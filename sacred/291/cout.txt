INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "291"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - preproces -- None
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
inputs Tensor("batch:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
net Tensor("siamese_fc/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
inputs Tensor("batch:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_1/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
net Tensor("siamese_fc_1/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
inputs Tensor("batch_1:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_2/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
net Tensor("siamese_fc_2/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("siamese_fc_2/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_2/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
inputs Tensor("batch_1:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_3/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
net Tensor("siamese_fc_3/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_3/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_3/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
Tensor("detection_1/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-16 03:37:58.722814: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 03:37:58.722878: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 03:37:58.722904: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 03:37:58.722919: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 03:37:58.722932: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 03:37:59.083111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-16 03:37:59.083192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-16 03:37:59.083220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-16 03:37:59.083240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-16 03:38:02.247687: step 0, loss = 2.28, batch loss = 2.23 (3.4 examples/sec; 2.352 sec/batch; 217h:15m:49s remains)
2017-12-16 03:38:02.898538: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289718 -4.428968 -4.4289627 -4.4289694 -4.4289784 -4.4289794 -4.4289775 -4.4289751 -4.4289536 -4.4289193 -4.428906 -4.4289174 -4.4289432 -4.4289718 -4.4289966][-4.4290152 -4.4290156 -4.4290128 -4.4290152 -4.429018 -4.4290142 -4.4290066 -4.4289985 -4.428977 -4.42895 -4.4289441 -4.4289579 -4.4289756 -4.4289918 -4.4290066][-4.4290333 -4.4290376 -4.4290361 -4.4290342 -4.4290295 -4.4290195 -4.4290051 -4.4289918 -4.4289742 -4.4289584 -4.4289618 -4.428977 -4.4289889 -4.4289966 -4.4290032][-4.4290233 -4.42903 -4.429028 -4.42902 -4.4290051 -4.4289832 -4.4289589 -4.4289427 -4.4289336 -4.4289322 -4.4289474 -4.42897 -4.4289823 -4.4289865 -4.4289889][-4.4289889 -4.4289966 -4.4289927 -4.4289756 -4.428947 -4.4289088 -4.4288764 -4.4288616 -4.4288611 -4.428874 -4.4289041 -4.4289408 -4.4289608 -4.4289665 -4.4289694][-4.42894 -4.4289503 -4.4289465 -4.4289227 -4.4288826 -4.4288335 -4.4287987 -4.4287848 -4.4287868 -4.4288096 -4.4288521 -4.4289045 -4.42894 -4.4289575 -4.4289675][-4.4289021 -4.4289179 -4.4289184 -4.4288969 -4.42886 -4.4288187 -4.4287891 -4.4287739 -4.4287715 -4.4287915 -4.4288311 -4.4288878 -4.4289341 -4.4289613 -4.428977][-4.4289074 -4.4289241 -4.4289279 -4.4289103 -4.4288826 -4.4288564 -4.4288392 -4.4288287 -4.4288273 -4.428843 -4.4288712 -4.4289141 -4.4289541 -4.428978 -4.428988][-4.4289412 -4.4289532 -4.4289565 -4.4289441 -4.428925 -4.42891 -4.4289036 -4.4289 -4.4289026 -4.4289179 -4.4289374 -4.4289637 -4.428988 -4.4289994 -4.428998][-4.428968 -4.4289761 -4.428977 -4.428966 -4.4289494 -4.4289408 -4.4289403 -4.4289412 -4.4289412 -4.4289546 -4.4289703 -4.428987 -4.4290028 -4.429008 -4.4289985][-4.4289694 -4.428977 -4.4289794 -4.4289703 -4.428956 -4.4289479 -4.4289508 -4.4289532 -4.4289484 -4.4289575 -4.4289732 -4.428988 -4.4290037 -4.4290104 -4.4290013][-4.4289556 -4.4289637 -4.4289675 -4.4289608 -4.4289479 -4.4289379 -4.4289389 -4.4289427 -4.4289389 -4.4289484 -4.428968 -4.428988 -4.4290066 -4.429018 -4.4290123][-4.4289455 -4.428956 -4.4289579 -4.42895 -4.4289336 -4.4289193 -4.4289179 -4.4289265 -4.42893 -4.4289436 -4.4289675 -4.4289894 -4.4290104 -4.4290252 -4.4290228][-4.4289322 -4.42895 -4.4289536 -4.4289441 -4.4289255 -4.4289083 -4.428906 -4.4289212 -4.4289346 -4.4289532 -4.4289751 -4.4289932 -4.429009 -4.4290209 -4.4290218][-4.428925 -4.4289503 -4.4289618 -4.4289551 -4.4289374 -4.4289193 -4.428916 -4.4289336 -4.4289522 -4.4289694 -4.4289832 -4.4289913 -4.4290009 -4.42901 -4.4290123]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 03:38:06.250893: step 10, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:32m:18s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 03:38:08.921105: step 20, loss = 2.27, batch loss = 2.22 (28.5 examples/sec; 0.281 sec/batch; 25h:55m:36s remains)
INFO - root - 2017-12-16 03:38:11.639024: step 30, loss = 2.17, batch loss = 2.11 (30.7 examples/sec; 0.261 sec/batch; 24h:06m:07s remains)
INFO - root - 2017-12-16 03:38:14.315071: step 40, loss = 1.36, batch loss = 1.30 (30.2 examples/sec; 0.265 sec/batch; 24h:27m:55s remains)
INFO - root - 2017-12-16 03:38:17.029801: step 50, loss = 0.75, batch loss = 0.69 (29.0 examples/sec; 0.276 sec/batch; 25h:29m:17s remains)
INFO - root - 2017-12-16 03:38:19.749079: step 60, loss = 0.65, batch loss = 0.59 (29.9 examples/sec; 0.267 sec/batch; 24h:40m:03s remains)
INFO - root - 2017-12-16 03:38:22.415898: step 70, loss = 0.56, batch loss = 0.51 (30.6 examples/sec; 0.262 sec/batch; 24h:10m:37s remains)
INFO - root - 2017-12-16 03:38:25.119364: step 80, loss = 0.59, batch loss = 0.53 (29.4 examples/sec; 0.272 sec/batch; 25h:08m:18s remains)
INFO - root - 2017-12-16 03:38:27.824443: step 90, loss = 0.55, batch loss = 0.49 (28.1 examples/sec; 0.285 sec/batch; 26h:16m:31s remains)
INFO - root - 2017-12-16 03:38:30.486839: step 100, loss = 0.58, batch loss = 0.52 (29.6 examples/sec; 0.270 sec/batch; 24h:57m:23s remains)
2017-12-16 03:38:30.931911: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.172327 -1.2021325 -1.218452 -1.2007086 -1.1625037 -1.1629643 -1.1888709 -1.1696258 -1.080009 -0.93727708 -0.76074839 -0.55007148 -0.34316349 -0.17777157 -0.045034885][-1.0606763 -1.0752714 -1.1001301 -1.0855682 -1.0461161 -1.0393059 -1.0460649 -1.0259547 -0.93689108 -0.78893185 -0.60805535 -0.39916325 -0.19731474 -0.048823357 0.034855843][-1.1329427 -1.1170983 -1.1444969 -1.1296225 -1.0853107 -1.062711 -1.0566926 -1.0314958 -0.94406652 -0.80367947 -0.63689208 -0.44867897 -0.26886511 -0.15631533 -0.12524033][-1.10134 -1.0482209 -1.0790799 -1.0757658 -1.0489407 -1.0380709 -1.0426567 -1.0280788 -0.95590734 -0.83539462 -0.68636346 -0.5136838 -0.34908819 -0.26718855 -0.28204918][-0.94515514 -0.8377502 -0.86678529 -0.88475657 -0.89554763 -0.92214227 -0.96072531 -0.97293448 -0.9249866 -0.825021 -0.686883 -0.52136254 -0.36390543 -0.29630375 -0.33236551][-0.76678038 -0.60800767 -0.6353426 -0.66991091 -0.71336842 -0.775012 -0.84441423 -0.87974882 -0.84962511 -0.76091266 -0.62065697 -0.4446044 -0.27779913 -0.20586061 -0.24638033][-0.66543007 -0.46516776 -0.48691463 -0.52103758 -0.57368374 -0.6487658 -0.73049378 -0.77813983 -0.76053834 -0.68722963 -0.555691 -0.37777758 -0.20736265 -0.13235903 -0.17857075][-0.62533355 -0.41393852 -0.42029476 -0.43691635 -0.47923851 -0.55083466 -0.63225365 -0.68808293 -0.69246793 -0.65287352 -0.55534458 -0.40164852 -0.24622011 -0.17491388 -0.22134781][-0.62539816 -0.39906597 -0.38719177 -0.38751745 -0.4213891 -0.49105096 -0.572958 -0.63990211 -0.67026973 -0.66779566 -0.61650324 -0.50927019 -0.38960505 -0.32741976 -0.36477089][-0.67913342 -0.43785214 -0.40982914 -0.40393972 -0.44092989 -0.51807 -0.60861659 -0.69144297 -0.74248743 -0.75950766 -0.739661 -0.6778338 -0.59656167 -0.53898764 -0.55396056][-0.79705787 -0.54352713 -0.50174737 -0.49199486 -0.53390837 -0.62312078 -0.72936845 -0.83164072 -0.902982 -0.93467355 -0.93588066 -0.9056325 -0.85212421 -0.79595637 -0.78337908][-0.94862628 -0.69006658 -0.63997912 -0.630316 -0.67631364 -0.77229524 -0.88800359 -1.0037022 -1.0941288 -1.1445484 -1.1668386 -1.1624987 -1.1318684 -1.0825579 -1.0501089][-1.1167638 -0.8801403 -0.83377123 -0.83352375 -0.88724923 -0.98477268 -1.0971575 -1.2088099 -1.3025529 -1.3589201 -1.3871348 -1.3906209 -1.3703992 -1.3276155 -1.286088][-1.3290927 -1.1352794 -1.1103015 -1.1321292 -1.2003412 -1.2994876 -1.4016664 -1.4942594 -1.570071 -1.6114268 -1.6259983 -1.6188939 -1.5940306 -1.5510581 -1.50652][-1.6080403 -1.4603562 -1.463084 -1.504508 -1.5797949 -1.6720476 -1.7565165 -1.8224111 -1.8715754 -1.8910134 -1.8874607 -1.866658 -1.8338878 -1.7903066 -1.747936]]...]
INFO - root - 2017-12-16 03:38:33.619982: step 110, loss = 0.56, batch loss = 0.50 (29.0 examples/sec; 0.276 sec/batch; 25h:30m:29s remains)
INFO - root - 2017-12-16 03:38:36.277581: step 120, loss = 0.52, batch loss = 0.46 (30.3 examples/sec; 0.264 sec/batch; 24h:24m:48s remains)
INFO - root - 2017-12-16 03:38:39.007538: step 130, loss = 0.61, batch loss = 0.55 (28.5 examples/sec; 0.281 sec/batch; 25h:55m:06s remains)
INFO - root - 2017-12-16 03:38:41.697672: step 140, loss = 0.54, batch loss = 0.48 (28.9 examples/sec; 0.277 sec/batch; 25h:35m:24s remains)
INFO - root - 2017-12-16 03:38:44.395064: step 150, loss = 0.50, batch loss = 0.44 (29.9 examples/sec; 0.268 sec/batch; 24h:42m:07s remains)
INFO - root - 2017-12-16 03:38:47.129791: step 160, loss = 0.48, batch loss = 0.42 (29.8 examples/sec; 0.269 sec/batch; 24h:48m:21s remains)
INFO - root - 2017-12-16 03:38:49.824509: step 170, loss = 0.61, batch loss = 0.55 (29.9 examples/sec; 0.268 sec/batch; 24h:42m:14s remains)
INFO - root - 2017-12-16 03:38:52.535775: step 180, loss = 0.48, batch loss = 0.42 (28.4 examples/sec; 0.281 sec/batch; 25h:58m:40s remains)
INFO - root - 2017-12-16 03:38:55.289481: step 190, loss = 0.59, batch loss = 0.53 (29.8 examples/sec; 0.268 sec/batch; 24h:46m:47s remains)
INFO - root - 2017-12-16 03:38:57.993781: step 200, loss = 0.50, batch loss = 0.44 (30.0 examples/sec; 0.267 sec/batch; 24h:37m:21s remains)
2017-12-16 03:38:58.438477: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8275981 -2.8579473 -2.7885714 -2.605562 -2.2755949 -1.8062499 -1.2583251 -0.82280588 -0.63852358 -0.74506569 -1.1195667 -1.6467249 -2.2044477 -2.6745903 -2.9984989][-2.5173874 -2.471549 -2.395489 -2.2221682 -1.9002669 -1.4375966 -0.92868948 -0.5411725 -0.40140057 -0.55763984 -0.98013639 -1.5505495 -2.1287394 -2.6102808 -2.9392014][-2.1843002 -2.0627449 -1.9932933 -1.858845 -1.5953188 -1.2102063 -0.8005178 -0.49926305 -0.41654444 -0.60939884 -1.0470304 -1.6182511 -2.1809216 -2.6431992 -2.9599013][-1.7554059 -1.5958593 -1.5545819 -1.4656482 -1.2579372 -0.94546223 -0.62962031 -0.42109919 -0.4086647 -0.65002012 -1.1108646 -1.6869774 -2.2360466 -2.6741858 -2.974525][-1.386843 -1.2200427 -1.2025461 -1.1410799 -0.96861029 -0.70085216 -0.44788647 -0.31253052 -0.36756849 -0.66051078 -1.1490078 -1.7345481 -2.2774708 -2.6981061 -2.9846795][-1.2702959 -1.0905168 -1.0641057 -0.985945 -0.80191851 -0.53635645 -0.30249357 -0.2003665 -0.29439783 -0.62228012 -1.1334288 -1.7316282 -2.2796645 -2.6996579 -2.9855781][-1.3643951 -1.195153 -1.1519892 -1.0345266 -0.80824924 -0.50566483 -0.24239874 -0.12987041 -0.23105001 -0.57102466 -1.092855 -1.7016971 -2.2627659 -2.6914766 -2.9827352][-1.528019 -1.3967993 -1.3460217 -1.1803265 -0.89034343 -0.52968478 -0.22120428 -0.083079338 -0.18171644 -0.5296917 -1.0649993 -1.6838174 -2.2550793 -2.6924686 -2.987289][-1.6326234 -1.5502677 -1.5063889 -1.3063948 -0.95937943 -0.53844285 -0.18395662 -0.029990673 -0.13891029 -0.50738025 -1.0696876 -1.7099547 -2.2923706 -2.7282677 -3.0150115][-1.6320858 -1.60043 -1.5840461 -1.3823874 -1.0173488 -0.57359481 -0.1999464 -0.0395422 -0.15480947 -0.53900933 -1.1183121 -1.7710741 -2.3571887 -2.7847657 -3.0565214][-1.537823 -1.5370493 -1.552315 -1.3768761 -1.0366731 -0.61731911 -0.26230097 -0.11188126 -0.22964764 -0.61245871 -1.1872075 -1.8331654 -2.410213 -2.8253322 -3.0835428][-1.3579934 -1.3779333 -1.4263229 -1.2930045 -0.99930978 -0.62678576 -0.31284714 -0.1899581 -0.31718159 -0.69222045 -1.2501562 -1.877511 -2.4375343 -2.838001 -3.0865479][-1.168376 -1.1961119 -1.2672005 -1.1721199 -0.92726851 -0.60570812 -0.33731937 -0.24628687 -0.38605356 -0.75401187 -1.2943702 -1.9035966 -2.4468927 -2.8368468 -3.0813267][-1.0583713 -1.0767343 -1.1561904 -1.093132 -0.899693 -0.62845588 -0.39577246 -0.32534122 -0.4682219 -0.82190156 -1.3362966 -1.9264534 -2.4573727 -2.8406355 -3.0824208][-1.1014593 -1.0974123 -1.1679435 -1.1230273 -0.96944547 -0.7424221 -0.5312345 -0.4568398 -0.57986569 -0.90035844 -1.37328 -1.9320164 -2.4502082 -2.8336971 -3.0781212]]...]
INFO - root - 2017-12-16 03:39:01.173818: step 210, loss = 0.44, batch loss = 0.38 (28.6 examples/sec; 0.280 sec/batch; 25h:50m:16s remains)
INFO - root - 2017-12-16 03:39:03.872830: step 220, loss = 0.51, batch loss = 0.46 (30.3 examples/sec; 0.264 sec/batch; 24h:20m:29s remains)
INFO - root - 2017-12-16 03:39:06.595135: step 230, loss = 0.53, batch loss = 0.47 (29.7 examples/sec; 0.269 sec/batch; 24h:50m:25s remains)
INFO - root - 2017-12-16 03:39:09.292398: step 240, loss = 0.46, batch loss = 0.40 (29.3 examples/sec; 0.273 sec/batch; 25h:10m:01s remains)
INFO - root - 2017-12-16 03:39:12.007566: step 250, loss = 0.51, batch loss = 0.45 (29.4 examples/sec; 0.272 sec/batch; 25h:06m:09s remains)
INFO - root - 2017-12-16 03:39:14.706932: step 260, loss = 0.40, batch loss = 0.34 (29.0 examples/sec; 0.276 sec/batch; 25h:28m:03s remains)
INFO - root - 2017-12-16 03:39:17.403016: step 270, loss = 0.52, batch loss = 0.47 (30.3 examples/sec; 0.264 sec/batch; 24h:20m:57s remains)
INFO - root - 2017-12-16 03:39:20.144058: step 280, loss = 0.42, batch loss = 0.36 (29.3 examples/sec; 0.273 sec/batch; 25h:11m:02s remains)
INFO - root - 2017-12-16 03:39:22.892474: step 290, loss = 0.50, batch loss = 0.44 (28.8 examples/sec; 0.278 sec/batch; 25h:40m:31s remains)
INFO - root - 2017-12-16 03:39:25.647812: step 300, loss = 0.45, batch loss = 0.40 (29.6 examples/sec; 0.271 sec/batch; 24h:57m:54s remains)
2017-12-16 03:39:26.122440: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.39192677 0.29665709 0.11676598 -0.059691429 -0.19099903 -0.23325396 -0.13903666 -0.12275267 -0.34241867 -0.79471731 -1.3011813 -1.7091069 -2.0048957 -2.2246635 -2.368607][0.61815119 0.52939224 0.3306613 0.15877533 0.052042961 0.034851551 0.11514235 0.10059643 -0.19055033 -0.743989 -1.3527098 -1.8187551 -2.1072762 -2.2830658 -2.3746991][0.47932482 0.4004488 0.19350481 0.0316844 -0.055175781 -0.058817863 0.0030994415 -0.032147408 -0.34729385 -0.93005848 -1.566349 -2.0309291 -2.2779284 -2.3820047 -2.4010384][0.35101557 0.31758595 0.12958193 -0.014411926 -0.09163332 -0.09841156 -0.0624485 -0.12070227 -0.43214917 -0.99850321 -1.6232996 -2.0785184 -2.3063443 -2.3755393 -2.359858][0.34630489 0.39760351 0.2612586 0.13929987 0.057637215 0.031027794 0.027075291 -0.05329895 -0.331501 -0.8436389 -1.437011 -1.9074006 -2.1749237 -2.2904379 -2.3158836][0.35643435 0.54981756 0.51251078 0.44203663 0.36499119 0.31570292 0.2696414 0.16941118 -0.067295551 -0.49703193 -1.0322251 -1.5208974 -1.8616083 -2.0809357 -2.2145112][0.30837011 0.6428051 0.73164749 0.73907042 0.69505739 0.648818 0.58670092 0.48449898 0.30276537 -0.017578602 -0.45798135 -0.92131424 -1.3039205 -1.6189761 -1.8788636][0.094997883 0.51896667 0.71564913 0.81753206 0.85696983 0.86871481 0.85447741 0.80169773 0.70371962 0.505486 0.1748476 -0.22442484 -0.60222006 -0.97514224 -1.3481946][-0.28346395 0.14699316 0.37783766 0.54010439 0.66906929 0.78807783 0.88638258 0.94409752 0.9613862 0.898294 0.69282484 0.3847518 0.059039116 -0.31177616 -0.74339962][-0.76143527 -0.41683388 -0.24639463 -0.094360828 0.088992596 0.30718517 0.52562666 0.71498728 0.86254454 0.9334836 0.8628664 0.68388462 0.47019339 0.170403 -0.23903084][-1.2973504 -1.0877807 -1.0302212 -0.93889832 -0.74486065 -0.46825171 -0.16275883 0.11902761 0.35050583 0.51257849 0.55490351 0.50284958 0.406744 0.21722221 -0.10641909][-1.7271988 -1.678632 -1.75734 -1.7616451 -1.6034994 -1.3362353 -1.015765 -0.71161127 -0.45889902 -0.27019835 -0.17580795 -0.15032244 -0.16595793 -0.26090288 -0.47275209][-1.9849176 -2.0695572 -2.2763836 -2.3919935 -2.3224187 -2.1310887 -1.8736849 -1.6131 -1.3868997 -1.2170968 -1.1185427 -1.0705714 -1.0541165 -1.0815051 -1.1810598][-2.10862 -2.2598543 -2.5356503 -2.7377584 -2.7684052 -2.6816673 -2.5235233 -2.3387918 -2.1633446 -2.0219524 -1.9338665 -1.8935399 -1.881062 -1.8813717 -1.9045472][-2.1563177 -2.3105211 -2.6000133 -2.8400931 -2.9370816 -2.9327626 -2.8567414 -2.733016 -2.5913692 -2.4690895 -2.39621 -2.3783433 -2.3991356 -2.4204969 -2.4259472]]...]
INFO - root - 2017-12-16 03:39:28.809259: step 310, loss = 0.44, batch loss = 0.39 (30.5 examples/sec; 0.262 sec/batch; 24h:12m:49s remains)
INFO - root - 2017-12-16 03:39:31.568442: step 320, loss = 0.37, batch loss = 0.31 (29.6 examples/sec; 0.270 sec/batch; 24h:54m:43s remains)
INFO - root - 2017-12-16 03:39:34.278102: step 330, loss = 0.49, batch loss = 0.43 (30.1 examples/sec; 0.266 sec/batch; 24h:31m:52s remains)
INFO - root - 2017-12-16 03:39:37.020186: step 340, loss = 0.38, batch loss = 0.32 (28.5 examples/sec; 0.281 sec/batch; 25h:54m:11s remains)
INFO - root - 2017-12-16 03:39:39.724463: step 350, loss = 0.45, batch loss = 0.40 (29.8 examples/sec; 0.269 sec/batch; 24h:46m:57s remains)
INFO - root - 2017-12-16 03:39:42.483838: step 360, loss = 0.45, batch loss = 0.40 (28.5 examples/sec; 0.280 sec/batch; 25h:52m:31s remains)
INFO - root - 2017-12-16 03:39:45.200929: step 370, loss = 0.49, batch loss = 0.43 (29.5 examples/sec; 0.271 sec/batch; 25h:01m:19s remains)
INFO - root - 2017-12-16 03:39:47.946853: step 380, loss = 0.55, batch loss = 0.49 (29.1 examples/sec; 0.274 sec/batch; 25h:19m:26s remains)
INFO - root - 2017-12-16 03:39:50.698156: step 390, loss = 0.41, batch loss = 0.35 (29.6 examples/sec; 0.270 sec/batch; 24h:54m:50s remains)
INFO - root - 2017-12-16 03:39:53.431822: step 400, loss = 0.37, batch loss = 0.31 (28.6 examples/sec; 0.279 sec/batch; 25h:46m:46s remains)
2017-12-16 03:39:53.917158: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5599375 -2.5404477 -2.5357413 -2.5650849 -2.5981669 -2.6357083 -2.6507604 -2.6642175 -2.690294 -2.7314911 -2.7930732 -2.8469424 -2.9025841 -2.9342656 -2.9410658][-2.3941524 -2.3669205 -2.3554978 -2.3949051 -2.4300694 -2.4541125 -2.4494553 -2.448195 -2.4721992 -2.5199809 -2.6047442 -2.7008247 -2.7985978 -2.8574157 -2.8703063][-2.3591776 -2.3288195 -2.3025632 -2.3207545 -2.3131454 -2.263454 -2.1922464 -2.1525681 -2.1728368 -2.2538762 -2.3960814 -2.5832303 -2.7627454 -2.8694344 -2.8998263][-2.3655057 -2.3170424 -2.2551317 -2.1937571 -2.0668135 -1.8655543 -1.6539631 -1.5280287 -1.5341032 -1.6890972 -1.9669042 -2.3255677 -2.6694555 -2.8848763 -2.9640076][-2.4030039 -2.3337417 -2.2130895 -2.0234125 -1.7227576 -1.3227079 -0.91999173 -0.68059278 -0.65773582 -0.90210962 -1.3599675 -1.9413893 -2.50564 -2.866837 -3.0229864][-2.4965596 -2.41044 -2.2101705 -1.8508575 -1.3235657 -0.68938684 -0.082183838 0.26863861 0.30804396 -0.05325079 -0.72191167 -1.544101 -2.329978 -2.8375833 -3.0736043][-2.6486981 -2.5720057 -2.3018191 -1.7671087 -0.99813247 -0.10811043 0.70830059 1.1694756 1.2028184 0.71911573 -0.15604496 -1.19713 -2.1673913 -2.8014603 -3.1062875][-2.8385515 -2.7988441 -2.4881787 -1.8319163 -0.88799429 0.19669676 1.1849842 1.7470851 1.7827158 1.2111197 0.19673109 -0.9895227 -2.0917382 -2.8289924 -3.1881022][-3.0190878 -3.0181723 -2.67338 -1.9345276 -0.8869431 0.29726934 1.3781824 2.0013714 2.0277319 1.3915381 0.29901171 -0.96499491 -2.1332827 -2.9334493 -3.3210955][-3.0897505 -3.0852647 -2.6971383 -1.9245934 -0.85300803 0.3189435 1.3688412 1.9678125 1.9447708 1.2660046 0.15473509 -1.1139348 -2.2759771 -3.085372 -3.4638281][-2.8695769 -2.8383622 -2.4330916 -1.7041266 -0.72498727 0.33921576 1.2761846 1.8094115 1.7144022 0.99926758 -0.091677189 -1.3298306 -2.4516778 -3.2490335 -3.5968966][-2.4236007 -2.3275349 -1.9261467 -1.295038 -0.49141121 0.3924346 1.1847305 1.6345615 1.4687071 0.72127676 -0.33906174 -1.5474963 -2.6404059 -3.4211493 -3.7370443][-1.8261497 -1.6632977 -1.3071699 -0.80853724 -0.2078557 0.47126389 1.1083932 1.4777069 1.2779832 0.50343227 -0.55016279 -1.7460585 -2.817106 -3.5668695 -3.8649161][-1.27883 -1.0776114 -0.79448223 -0.43515682 -0.0034885406 0.49990416 0.99224854 1.2846975 1.0643578 0.27877903 -0.78083277 -1.959708 -2.9913011 -3.6886985 -3.9557638][-1.0850399 -0.89277196 -0.7087214 -0.50132084 -0.23974991 0.092633247 0.44332266 0.66810751 0.47226334 -0.23548841 -1.2190187 -2.2983406 -3.2212906 -3.8196354 -4.0171127]]...]
INFO - root - 2017-12-16 03:39:56.674996: step 410, loss = 0.41, batch loss = 0.35 (28.0 examples/sec; 0.286 sec/batch; 26h:20m:13s remains)
INFO - root - 2017-12-16 03:39:59.417818: step 420, loss = 0.47, batch loss = 0.41 (29.0 examples/sec; 0.276 sec/batch; 25h:28m:10s remains)
INFO - root - 2017-12-16 03:40:02.129563: step 430, loss = 0.46, batch loss = 0.40 (29.1 examples/sec; 0.275 sec/batch; 25h:24m:03s remains)
INFO - root - 2017-12-16 03:40:04.853816: step 440, loss = 0.39, batch loss = 0.34 (30.0 examples/sec; 0.267 sec/batch; 24h:36m:26s remains)
INFO - root - 2017-12-16 03:40:07.615324: step 450, loss = 0.40, batch loss = 0.34 (29.3 examples/sec; 0.273 sec/batch; 25h:13m:28s remains)
INFO - root - 2017-12-16 03:40:10.403214: step 460, loss = 0.44, batch loss = 0.38 (29.0 examples/sec; 0.276 sec/batch; 25h:28m:16s remains)
INFO - root - 2017-12-16 03:40:13.181216: step 470, loss = 0.36, batch loss = 0.30 (26.6 examples/sec; 0.301 sec/batch; 27h:44m:25s remains)
INFO - root - 2017-12-16 03:40:15.921745: step 480, loss = 0.53, batch loss = 0.47 (29.1 examples/sec; 0.275 sec/batch; 25h:20m:37s remains)
INFO - root - 2017-12-16 03:40:18.661639: step 490, loss = 0.48, batch loss = 0.42 (30.3 examples/sec; 0.264 sec/batch; 24h:22m:34s remains)
INFO - root - 2017-12-16 03:40:21.473364: step 500, loss = 0.49, batch loss = 0.43 (27.6 examples/sec; 0.290 sec/batch; 26h:42m:07s remains)
2017-12-16 03:40:21.927371: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9691548 -1.994462 -2.2252264 -2.5236549 -2.7610502 -2.80197 -2.6546855 -2.4242907 -2.2545321 -2.2466018 -2.4321315 -2.7504008 -3.0932508 -3.2945867 -3.3331432][-1.0159881 -0.98659444 -1.2886877 -1.6999733 -2.0485036 -2.1744196 -2.0356336 -1.7641506 -1.5426166 -1.5422304 -1.7788098 -2.18386 -2.6592374 -3.0317044 -3.2575965][-0.1861701 -0.056098938 -0.36611319 -0.82161546 -1.2415621 -1.4289157 -1.3092358 -1.0097585 -0.73583961 -0.71960044 -0.96932435 -1.4205389 -2.0007849 -2.55729 -3.0224614][0.33974123 0.57791948 0.34032202 -0.034495354 -0.38857985 -0.55983639 -0.43297148 -0.13135576 0.14518356 0.12817478 -0.1815424 -0.70548892 -1.3807287 -2.0855327 -2.7420154][0.4588089 0.78372049 0.67105913 0.46985245 0.29405642 0.22336006 0.3841939 0.6737957 0.90137243 0.79596472 0.39140129 -0.19841814 -0.93998647 -1.7423813 -2.498574][0.19792271 0.584743 0.57962084 0.55253983 0.54342985 0.57846451 0.78882265 1.0703974 1.2491632 1.08009 0.617908 0.01178503 -0.73535085 -1.5612357 -2.3359833][0.0023736954 0.41899538 0.47606421 0.51626539 0.5392499 0.56745386 0.75439072 1.0197749 1.1858468 1.0478363 0.637547 0.087880611 -0.59947586 -1.3943317 -2.1401951][-0.17742825 0.22104692 0.27937269 0.29384995 0.25444126 0.1975565 0.30981159 0.55884075 0.74021959 0.68321657 0.39685011 -0.034195423 -0.61789846 -1.3267591 -1.9841375][-0.42258596 -0.077139854 -0.071997643 -0.16249275 -0.34072256 -0.51862073 -0.46993685 -0.21399641 0.022046566 0.081439495 -0.036323547 -0.32139778 -0.80221891 -1.4182405 -1.9989781][-0.84941864 -0.57472062 -0.66882706 -0.89906907 -1.2455294 -1.5460632 -1.5320449 -1.2076867 -0.83911586 -0.58577275 -0.47440886 -0.57670712 -0.94295931 -1.4842396 -2.0501456][-1.5797911 -1.3617468 -1.5204227 -1.8412609 -2.2984662 -2.6754956 -2.6918964 -2.3323119 -1.8498995 -1.4198625 -1.1017976 -1.019644 -1.2510409 -1.6882768 -2.2265968][-2.5173709 -2.33869 -2.4890902 -2.8063424 -3.261044 -3.6214657 -3.6299858 -3.2833104 -2.7826591 -2.2889717 -1.8794875 -1.6789699 -1.7900879 -2.1040502 -2.562016][-3.4419084 -3.32055 -3.4262135 -3.637228 -3.9516525 -4.1988282 -4.1607823 -3.8563468 -3.4266152 -2.9883704 -2.597353 -2.3563814 -2.3758504 -2.5733104 -2.9099317][-4.0315852 -3.9768007 -4.0466413 -4.1488504 -4.2967782 -4.3953142 -4.3126183 -4.072206 -3.757261 -3.4261122 -3.1164939 -2.9088979 -2.8875496 -2.9967022 -3.2114236][-4.1736627 -4.1540856 -4.1991224 -4.2374516 -4.2784266 -4.2886629 -4.2018085 -4.0442557 -3.8605926 -3.6651611 -3.4786162 -3.3317161 -3.284987 -3.3183947 -3.3852563]]...]
INFO - root - 2017-12-16 03:40:24.690590: step 510, loss = 0.39, batch loss = 0.33 (28.7 examples/sec; 0.279 sec/batch; 25h:42m:09s remains)
INFO - root - 2017-12-16 03:40:27.461759: step 520, loss = 0.45, batch loss = 0.39 (29.9 examples/sec; 0.268 sec/batch; 24h:40m:26s remains)
INFO - root - 2017-12-16 03:40:30.199702: step 530, loss = 0.43, batch loss = 0.37 (29.4 examples/sec; 0.272 sec/batch; 25h:04m:53s remains)
INFO - root - 2017-12-16 03:40:32.955014: step 540, loss = 0.34, batch loss = 0.28 (28.2 examples/sec; 0.284 sec/batch; 26h:10m:59s remains)
INFO - root - 2017-12-16 03:40:35.693193: step 550, loss = 0.46, batch loss = 0.40 (29.6 examples/sec; 0.270 sec/batch; 24h:55m:14s remains)
INFO - root - 2017-12-16 03:40:38.485635: step 560, loss = 0.42, batch loss = 0.36 (27.7 examples/sec; 0.288 sec/batch; 26h:35m:23s remains)
INFO - root - 2017-12-16 03:40:41.309665: step 570, loss = 0.40, batch loss = 0.35 (26.5 examples/sec; 0.302 sec/batch; 27h:51m:35s remains)
INFO - root - 2017-12-16 03:40:44.087569: step 580, loss = 0.42, batch loss = 0.36 (28.8 examples/sec; 0.278 sec/batch; 25h:38m:05s remains)
INFO - root - 2017-12-16 03:40:46.867768: step 590, loss = 0.44, batch loss = 0.38 (28.2 examples/sec; 0.284 sec/batch; 26h:11m:50s remains)
INFO - root - 2017-12-16 03:40:49.614861: step 600, loss = 0.38, batch loss = 0.32 (28.1 examples/sec; 0.285 sec/batch; 26h:15m:52s remains)
2017-12-16 03:40:50.082185: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2640202 -1.9257956 -1.4045198 -0.83251858 -0.38002443 -0.26843262 -0.54189253 -1.1350656 -1.8603601 -2.5758142 -3.1307356 -3.4154944 -3.4486117 -3.3678975 -3.3093445][-1.7516155 -1.4053168 -0.89654851 -0.3430233 0.031446934 0.036387444 -0.35590267 -1.0762064 -1.9083552 -2.6752238 -3.2257686 -3.4768004 -3.4889922 -3.3875322 -3.3120508][-1.2990401 -1.0271554 -0.65203667 -0.21893549 0.038039684 -0.04544735 -0.47655058 -1.2014759 -2.0049689 -2.7352848 -3.2468457 -3.4769042 -3.4909701 -3.3854456 -3.2969127][-1.0160143 -0.80714869 -0.55875659 -0.24575949 -0.0846262 -0.20300007 -0.59627128 -1.237617 -1.9463544 -2.6036522 -3.0744896 -3.2999592 -3.3401961 -3.2624297 -3.1594219][-0.99633074 -0.8138814 -0.68284678 -0.47725224 -0.38313293 -0.48096609 -0.78549695 -1.2786617 -1.8436644 -2.3918276 -2.7991881 -3.0190368 -3.0759344 -3.0024667 -2.8578751][-1.230377 -1.0065968 -0.90328526 -0.73621058 -0.65016913 -0.69625592 -0.89240718 -1.2256174 -1.6410754 -2.0813847 -2.4296098 -2.6238489 -2.6742663 -2.5897014 -2.3908691][-1.6038136 -1.2925704 -1.1150949 -0.90244365 -0.75972104 -0.72822285 -0.81053019 -1.0154259 -1.3232484 -1.6892962 -1.9985127 -2.166672 -2.1950676 -2.0713091 -1.7946422][-2.136759 -1.7482042 -1.4793417 -1.1869876 -0.957062 -0.82992887 -0.80252194 -0.89439225 -1.0963285 -1.3736029 -1.6134191 -1.7235944 -1.6883643 -1.491652 -1.12258][-2.7654943 -2.3494422 -2.0448837 -1.7351434 -1.4869003 -1.299777 -1.177603 -1.1592512 -1.2327566 -1.3720007 -1.4633229 -1.4470553 -1.2961493 -1.0039322 -0.55433178][-3.3505402 -2.9471841 -2.6593065 -2.381685 -2.1699333 -1.9998713 -1.856137 -1.7570052 -1.705889 -1.6993151 -1.6331379 -1.4803028 -1.2103946 -0.8317883 -0.35577106][-3.8219008 -3.4797072 -3.2388768 -3.0043764 -2.8325891 -2.6851568 -2.5390239 -2.4071097 -2.2883959 -2.1969242 -2.0482681 -1.8294485 -1.5130317 -1.1342404 -0.69354653][-4.0544186 -3.8370748 -3.6959577 -3.5462773 -3.4375198 -3.3265088 -3.1963499 -3.0414166 -2.8805366 -2.7439275 -2.5673902 -2.3637576 -2.0869548 -1.7842264 -1.4572558][-4.023077 -3.9286203 -3.9032197 -3.8647666 -3.8505249 -3.8073189 -3.7223768 -3.5802333 -3.41377 -3.2636516 -3.096601 -2.9376869 -2.7381668 -2.5458379 -2.3548636][-3.6840074 -3.6585433 -3.7127573 -3.7826164 -3.8788307 -3.9482598 -3.964915 -3.8866565 -3.766108 -3.644284 -3.5188828 -3.4167209 -3.3025837 -3.2124872 -3.1449037][-3.0923264 -3.0388827 -3.1012928 -3.2356272 -3.4307089 -3.6447997 -3.8056514 -3.8585711 -3.8325408 -3.7757649 -3.7176003 -3.6716452 -3.6289768 -3.610688 -3.6291156]]...]
INFO - root - 2017-12-16 03:40:52.859315: step 610, loss = 0.45, batch loss = 0.40 (28.5 examples/sec; 0.281 sec/batch; 25h:52m:44s remains)
INFO - root - 2017-12-16 03:40:55.722843: step 620, loss = 0.40, batch loss = 0.35 (28.1 examples/sec; 0.285 sec/batch; 26h:17m:25s remains)
INFO - root - 2017-12-16 03:40:58.464075: step 630, loss = 0.33, batch loss = 0.27 (28.7 examples/sec; 0.278 sec/batch; 25h:40m:09s remains)
INFO - root - 2017-12-16 03:41:01.227397: step 640, loss = 0.40, batch loss = 0.34 (29.7 examples/sec; 0.269 sec/batch; 24h:50m:22s remains)
INFO - root - 2017-12-16 03:41:03.989836: step 650, loss = 0.49, batch loss = 0.44 (28.4 examples/sec; 0.282 sec/batch; 26h:00m:37s remains)
INFO - root - 2017-12-16 03:41:06.690351: step 660, loss = 0.46, batch loss = 0.40 (28.2 examples/sec; 0.284 sec/batch; 26h:08m:09s remains)
INFO - root - 2017-12-16 03:41:09.507364: step 670, loss = 0.33, batch loss = 0.28 (27.8 examples/sec; 0.287 sec/batch; 26h:29m:07s remains)
INFO - root - 2017-12-16 03:41:12.263694: step 680, loss = 0.40, batch loss = 0.34 (29.6 examples/sec; 0.270 sec/batch; 24h:53m:06s remains)
INFO - root - 2017-12-16 03:41:15.076928: step 690, loss = 0.39, batch loss = 0.33 (28.4 examples/sec; 0.282 sec/batch; 25h:58m:32s remains)
INFO - root - 2017-12-16 03:41:17.862805: step 700, loss = 0.38, batch loss = 0.32 (26.8 examples/sec; 0.299 sec/batch; 27h:31m:19s remains)
2017-12-16 03:41:18.331210: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3439622 -3.3719506 -3.3934283 -3.3827333 -3.3724542 -3.3917203 -3.4098485 -3.4291272 -3.461205 -3.4872265 -3.5162878 -3.5108232 -3.4616189 -3.3718486 -3.2942643][-3.3899627 -3.4643447 -3.5413933 -3.5756018 -3.5918431 -3.6130459 -3.6492572 -3.6900389 -3.738606 -3.7812705 -3.8218613 -3.7986732 -3.6827388 -3.5023122 -3.3090281][-3.5417514 -3.6586044 -3.7639012 -3.799994 -3.7912636 -3.7820718 -3.7967629 -3.8398294 -3.9021778 -3.9614761 -4.0133624 -3.9829473 -3.8186233 -3.5341976 -3.2029915][-3.6492438 -3.755805 -3.8158076 -3.7499104 -3.62407 -3.5150604 -3.4681692 -3.4896622 -3.5563076 -3.6707289 -3.7968597 -3.8093252 -3.6534553 -3.3430009 -2.9413066][-3.6890786 -3.7142575 -3.6353877 -3.39788 -3.0977874 -2.8517621 -2.7330151 -2.7401166 -2.8294878 -3.0046196 -3.2163482 -3.3098419 -3.2044187 -2.9120526 -2.5226367][-3.5522311 -3.4552879 -3.1921785 -2.7490821 -2.2638738 -1.8883293 -1.7139056 -1.7273953 -1.879472 -2.1550202 -2.4672608 -2.6364567 -2.5830793 -2.3439977 -2.0108747][-3.3149612 -3.0851474 -2.6309161 -1.9962201 -1.3721409 -0.90973973 -0.70643234 -0.75020623 -0.98075652 -1.3715019 -1.7846146 -2.0202322 -1.9993558 -1.7913873 -1.504431][-3.0948343 -2.7928677 -2.2258196 -1.4603748 -0.76202273 -0.27802181 -0.069246769 -0.12509298 -0.42454243 -0.92969108 -1.4155042 -1.6746733 -1.6615798 -1.4729724 -1.2173605][-3.0185757 -2.7264633 -2.164957 -1.4530733 -0.85388947 -0.43381214 -0.25074339 -0.31352568 -0.6143434 -1.1216581 -1.5722706 -1.7674959 -1.6869769 -1.4726925 -1.2151814][-3.2004285 -2.9942255 -2.5309949 -1.9661019 -1.5429227 -1.2718334 -1.1795363 -1.262888 -1.5517745 -1.9789519 -2.2764246 -2.3287063 -2.1250575 -1.8338971 -1.539115][-3.6059918 -3.5070283 -3.1843863 -2.7905912 -2.5286424 -2.3795788 -2.3489344 -2.4388797 -2.6760051 -3.0001493 -3.1694446 -3.0814252 -2.7650704 -2.41458 -2.0795703][-3.9837744 -3.9819341 -3.7811666 -3.5323021 -3.3925056 -3.3179002 -3.3139102 -3.3837347 -3.5574162 -3.7573419 -3.7968302 -3.6378741 -3.3090692 -2.9773202 -2.6558447][-4.1487579 -4.1885037 -4.0820065 -3.9365566 -3.8506308 -3.8068681 -3.8096566 -3.8465354 -3.960742 -4.0703864 -4.0456548 -3.8835878 -3.6087408 -3.36282 -3.1157727][-4.0799417 -4.1010504 -4.0344992 -3.9594767 -3.9165826 -3.8847251 -3.8931129 -3.9146948 -3.9784741 -4.004787 -3.9402983 -3.8084195 -3.6189613 -3.479275 -3.3271251][-3.8630846 -3.8303103 -3.7674887 -3.6842911 -3.615289 -3.5751615 -3.5719421 -3.5765266 -3.6222823 -3.6327314 -3.5752769 -3.4827213 -3.36917 -3.3132892 -3.2364373]]...]
INFO - root - 2017-12-16 03:41:21.086949: step 710, loss = 0.56, batch loss = 0.50 (28.1 examples/sec; 0.284 sec/batch; 26h:12m:10s remains)
INFO - root - 2017-12-16 03:41:23.856984: step 720, loss = 0.44, batch loss = 0.38 (29.7 examples/sec; 0.270 sec/batch; 24h:50m:50s remains)
INFO - root - 2017-12-16 03:41:26.667266: step 730, loss = 0.44, batch loss = 0.38 (28.4 examples/sec; 0.282 sec/batch; 25h:59m:52s remains)
INFO - root - 2017-12-16 03:41:29.438992: step 740, loss = 0.38, batch loss = 0.32 (29.6 examples/sec; 0.270 sec/batch; 24h:52m:53s remains)
INFO - root - 2017-12-16 03:41:32.223537: step 750, loss = 0.47, batch loss = 0.41 (28.8 examples/sec; 0.277 sec/batch; 25h:34m:03s remains)
INFO - root - 2017-12-16 03:41:34.963545: step 760, loss = 0.32, batch loss = 0.26 (28.5 examples/sec; 0.280 sec/batch; 25h:49m:56s remains)
INFO - root - 2017-12-16 03:41:37.720360: step 770, loss = 0.32, batch loss = 0.26 (28.8 examples/sec; 0.278 sec/batch; 25h:35m:11s remains)
INFO - root - 2017-12-16 03:41:40.525339: step 780, loss = 0.41, batch loss = 0.35 (29.6 examples/sec; 0.270 sec/batch; 24h:54m:53s remains)
INFO - root - 2017-12-16 03:41:43.301912: step 790, loss = 0.49, batch loss = 0.43 (29.0 examples/sec; 0.276 sec/batch; 25h:24m:43s remains)
INFO - root - 2017-12-16 03:41:46.120007: step 800, loss = 0.35, batch loss = 0.29 (29.3 examples/sec; 0.273 sec/batch; 25h:10m:49s remains)
2017-12-16 03:41:46.587668: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6229897 -3.8660741 -3.9956493 -4.0030603 -3.9300089 -3.8567772 -3.8744903 -3.9836707 -4.166122 -4.3531189 -4.414309 -4.3107328 -4.0569239 -3.7587883 -3.515408][-3.5052567 -3.7333443 -3.794297 -3.6944211 -3.5055907 -3.3078279 -3.2260931 -3.3267632 -3.5973115 -3.9326584 -4.1334529 -4.1108875 -3.8900406 -3.5595708 -3.2506313][-3.3156052 -3.4644551 -3.4387436 -3.2246876 -2.8873296 -2.5521765 -2.3719988 -2.4362826 -2.7820637 -3.2523744 -3.5959356 -3.6624081 -3.4816272 -3.1493747 -2.8232415][-3.0705111 -3.1218429 -2.9988995 -2.686218 -2.2543156 -1.8099303 -1.5459495 -1.5954969 -1.9817047 -2.5354457 -2.9912038 -3.1303499 -2.9543731 -2.6071858 -2.284709][-2.7955875 -2.7546258 -2.5635612 -2.197161 -1.7348518 -1.2654741 -0.96390247 -0.98734593 -1.3717232 -1.9284997 -2.3933711 -2.5622981 -2.4027586 -2.0525343 -1.7526689][-2.5914726 -2.4541883 -2.2107759 -1.8366344 -1.3969314 -0.93890738 -0.62147331 -0.63101363 -0.97886634 -1.4721427 -1.8869157 -2.0390673 -1.8831232 -1.5398757 -1.2649117][-2.35994 -2.1561747 -1.9227006 -1.5751314 -1.1722605 -0.75593662 -0.459373 -0.440799 -0.71514034 -1.1228299 -1.4432936 -1.5450232 -1.3804665 -1.0695677 -0.8555243][-2.105226 -1.8188994 -1.5583379 -1.2316892 -0.87181926 -0.5031178 -0.2601099 -0.29118681 -0.56232381 -0.91082525 -1.1365404 -1.1517303 -0.93080544 -0.61540055 -0.45183277][-1.9404385 -1.5757692 -1.265281 -0.9577868 -0.64913249 -0.31311798 -0.11597633 -0.21592093 -0.51357007 -0.8164289 -0.92492652 -0.81953335 -0.52214122 -0.18856382 -0.10979366][-1.96155 -1.5679955 -1.2485554 -0.9551549 -0.69156551 -0.42957282 -0.29297018 -0.42245388 -0.72339821 -0.96713591 -0.94038892 -0.65851331 -0.23649549 0.078866959 0.061384678][-2.1976237 -1.8053145 -1.4967625 -1.2662807 -1.0838883 -0.94431281 -0.93374825 -1.1227336 -1.4366193 -1.5895336 -1.4018791 -0.92006969 -0.34621668 -0.016370773 -0.126194][-2.6070449 -2.3171144 -2.0899801 -1.9748018 -1.9587321 -1.9994094 -2.1518676 -2.4161453 -2.7288733 -2.8007946 -2.4833736 -1.8304172 -1.1279247 -0.75309992 -0.87113786][-3.2519732 -3.101058 -3.006974 -3.052228 -3.2222915 -3.4473915 -3.7491131 -4.0741916 -4.3750114 -4.3805108 -3.9949663 -3.3097262 -2.5774093 -2.1701107 -2.2282417][-3.8693457 -3.8865671 -3.9579031 -4.1675506 -4.4918542 -4.8540239 -5.2509685 -5.6155033 -5.9018788 -5.8637538 -5.4642067 -4.8247895 -4.1355786 -3.7234757 -3.721364][-4.2029343 -4.32826 -4.5310731 -4.8386493 -5.2320747 -5.66259 -6.1146684 -6.4789953 -6.7480154 -6.7438478 -6.4392586 -5.9075041 -5.3093371 -4.9097896 -4.8005672]]...]
INFO - root - 2017-12-16 03:41:49.324670: step 810, loss = 0.34, batch loss = 0.28 (28.7 examples/sec; 0.279 sec/batch; 25h:41m:42s remains)
INFO - root - 2017-12-16 03:41:52.092169: step 820, loss = 0.36, batch loss = 0.30 (29.8 examples/sec; 0.269 sec/batch; 24h:45m:10s remains)
INFO - root - 2017-12-16 03:41:54.887907: step 830, loss = 0.48, batch loss = 0.42 (30.3 examples/sec; 0.264 sec/batch; 24h:17m:37s remains)
2017-12-16 03:41:56.825693: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 226460 get requests, put_count=226459 evicted_count=1000 eviction_rate=0.00441581 and unsatisfied allocation rate=0.00486179
2017-12-16 03:41:56.825723: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
INFO - root - 2017-12-16 03:41:57.648334: step 840, loss = 0.38, batch loss = 0.32 (29.0 examples/sec; 0.276 sec/batch; 25h:27m:16s remains)
INFO - root - 2017-12-16 03:42:00.455611: step 850, loss = 0.29, batch loss = 0.23 (28.1 examples/sec; 0.284 sec/batch; 26h:12m:24s remains)
INFO - root - 2017-12-16 03:42:03.270295: step 860, loss = 0.52, batch loss = 0.46 (28.6 examples/sec; 0.280 sec/batch; 25h:46m:14s remains)
INFO - root - 2017-12-16 03:42:06.057759: step 870, loss = 0.37, batch loss = 0.32 (29.8 examples/sec; 0.269 sec/batch; 24h:44m:26s remains)
INFO - root - 2017-12-16 03:42:08.857674: step 880, loss = 0.37, batch loss = 0.31 (28.5 examples/sec; 0.281 sec/batch; 25h:52m:29s remains)
INFO - root - 2017-12-16 03:42:11.646680: step 890, loss = 0.36, batch loss = 0.30 (27.9 examples/sec; 0.287 sec/batch; 26h:24m:59s remains)
INFO - root - 2017-12-16 03:42:14.457816: step 900, loss = 0.38, batch loss = 0.32 (28.1 examples/sec; 0.285 sec/batch; 26h:12m:55s remains)
2017-12-16 03:42:14.896473: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6567779 -3.6820216 -3.4809518 -3.2414844 -3.1593094 -3.2569063 -3.5395207 -3.9035053 -4.2505231 -4.4846396 -4.4607644 -4.2576232 -3.9761894 -3.6220446 -3.3700066][-3.612777 -3.6504455 -3.4307489 -3.1539497 -3.0089 -3.0424874 -3.279856 -3.6083212 -3.9211984 -4.1056123 -3.989953 -3.6104894 -3.1144576 -2.578403 -2.2334661][-3.6955612 -3.7265275 -3.4911489 -3.1514564 -2.9224358 -2.8773494 -3.031086 -3.2961974 -3.5755057 -3.7320139 -3.5589528 -3.0342691 -2.3314934 -1.6111236 -1.166786][-3.7552993 -3.7496684 -3.4808044 -3.0655055 -2.6948836 -2.512392 -2.5419333 -2.7243891 -2.9792008 -3.1831765 -3.0740757 -2.5485399 -1.7837863 -0.99626827 -0.506691][-3.7766368 -3.6823103 -3.2785959 -2.6833577 -2.0887222 -1.6804602 -1.5188229 -1.6451013 -1.9605706 -2.2844837 -2.3850532 -2.0850143 -1.4593079 -0.75199246 -0.30916405][-3.8034549 -3.6062698 -3.0373487 -2.1780348 -1.2753823 -0.59521008 -0.24612617 -0.30754566 -0.70274115 -1.2511156 -1.6839256 -1.7482493 -1.450887 -0.98117352 -0.65860295][-3.7762527 -3.4410119 -2.6740093 -1.5536351 -0.34493113 0.63192034 1.1346326 1.062995 0.534894 -0.2551055 -1.0294764 -1.520931 -1.6351638 -1.5154626 -1.4329515][-3.7526844 -3.369118 -2.516151 -1.2280374 0.20510054 1.4215317 2.0940704 2.10571 1.5327549 0.56929159 -0.46118689 -1.3167205 -1.8180828 -2.063138 -2.2652249][-3.8038926 -3.4539492 -2.6615577 -1.4497395 -0.067622185 1.1835408 1.9274025 2.0314264 1.5632529 0.6702261 -0.37340164 -1.3710101 -2.0914006 -2.5722845 -2.9950535][-3.9400015 -3.7021458 -3.0922008 -2.1044025 -0.95121956 0.13106871 0.7889843 0.924335 0.57261419 -0.12853622 -0.97912145 -1.8502567 -2.5203152 -3.0111566 -3.4536705][-4.0091596 -3.943686 -3.6127427 -2.9895318 -2.2108245 -1.4228475 -0.93334985 -0.80045772 -1.0006638 -1.4541628 -2.0152426 -2.617348 -3.0737419 -3.4301696 -3.7752342][-3.9600527 -4.0305395 -3.9712672 -3.7379518 -3.3751831 -2.9267707 -2.6176887 -2.4956489 -2.5695808 -2.7908297 -3.0634069 -3.3799615 -3.6110039 -3.7994606 -4.0130229][-3.8248777 -3.9696913 -4.12608 -4.2088642 -4.1929851 -4.0769305 -3.9463878 -3.8469973 -3.809201 -3.8214178 -3.8505421 -3.9082158 -3.9406526 -3.9759977 -4.0602255][-3.5859647 -3.6839867 -3.8672974 -4.0746946 -4.2580919 -4.3795609 -4.4268093 -4.4277673 -4.3977737 -4.3282237 -4.2406182 -4.1511731 -4.0515952 -3.976326 -3.9541225][-3.3276749 -3.3560619 -3.4887156 -3.6547279 -3.8160059 -3.9614134 -4.0610666 -4.1197996 -4.1436176 -4.1251092 -4.0656285 -3.9717479 -3.8602819 -3.7663567 -3.6997304]]...]
INFO - root - 2017-12-16 03:42:17.702730: step 910, loss = 0.34, batch loss = 0.28 (29.0 examples/sec; 0.276 sec/batch; 25h:23m:32s remains)
INFO - root - 2017-12-16 03:42:20.460490: step 920, loss = 0.44, batch loss = 0.38 (28.8 examples/sec; 0.278 sec/batch; 25h:36m:50s remains)
INFO - root - 2017-12-16 03:42:23.227453: step 930, loss = 0.32, batch loss = 0.26 (27.9 examples/sec; 0.287 sec/batch; 26h:24m:48s remains)
INFO - root - 2017-12-16 03:42:26.066329: step 940, loss = 0.39, batch loss = 0.34 (28.8 examples/sec; 0.278 sec/batch; 25h:34m:15s remains)
INFO - root - 2017-12-16 03:42:28.913251: step 950, loss = 0.43, batch loss = 0.37 (28.1 examples/sec; 0.284 sec/batch; 26h:10m:48s remains)
INFO - root - 2017-12-16 03:42:31.753065: step 960, loss = 0.38, batch loss = 0.33 (29.6 examples/sec; 0.270 sec/batch; 24h:53m:13s remains)
INFO - root - 2017-12-16 03:42:34.545425: step 970, loss = 0.32, batch loss = 0.26 (28.9 examples/sec; 0.277 sec/batch; 25h:28m:40s remains)
INFO - root - 2017-12-16 03:42:37.373231: step 980, loss = 0.35, batch loss = 0.29 (28.1 examples/sec; 0.285 sec/batch; 26h:14m:49s remains)
INFO - root - 2017-12-16 03:42:40.152033: step 990, loss = 0.37, batch loss = 0.31 (28.6 examples/sec; 0.279 sec/batch; 25h:43m:23s remains)
INFO - root - 2017-12-16 03:42:42.911265: step 1000, loss = 0.40, batch loss = 0.34 (29.1 examples/sec; 0.275 sec/batch; 25h:20m:33s remains)
2017-12-16 03:42:43.358996: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9651546 -3.7976556 -3.6405945 -3.5269361 -3.4780753 -3.4519782 -3.4318666 -3.340363 -3.2089624 -3.0415277 -2.7583878 -2.4684787 -2.2793977 -2.2640276 -2.3133523][-3.8240023 -3.5729871 -3.3613296 -3.1752553 -3.0772367 -3.0373564 -3.0007062 -2.8761868 -2.7091618 -2.5060735 -2.1686287 -1.8096125 -1.5604053 -1.4776096 -1.491132][-3.4462 -3.1313999 -2.8922246 -2.6679807 -2.5358043 -2.5105939 -2.5158057 -2.4280729 -2.3016336 -2.1657853 -1.9071102 -1.6031375 -1.3463919 -1.1917584 -1.1122189][-2.6532063 -2.2731721 -2.0493131 -1.8537567 -1.772032 -1.8342702 -1.946677 -2.0175846 -2.0493281 -2.0816698 -2.0361893 -1.9024627 -1.7077293 -1.4949255 -1.3026242][-1.660841 -1.2491317 -1.0810421 -0.97268248 -0.97552204 -1.1335769 -1.3773439 -1.6161497 -1.8383338 -2.107307 -2.3359571 -2.44319 -2.3887372 -2.1660054 -1.8543129][-0.7726233 -0.3831234 -0.30858994 -0.29476404 -0.36210489 -0.58772182 -0.8994143 -1.2428839 -1.5890007 -2.0096414 -2.4556963 -2.7712274 -2.8782804 -2.7215071 -2.3901818][-0.19791651 0.11065483 0.097514153 0.038613319 -0.051332474 -0.26057148 -0.55768394 -0.91499782 -1.3041546 -1.8007972 -2.3560591 -2.8002596 -3.055243 -3.0414944 -2.8247161][-0.24489498 -0.059714317 -0.12354517 -0.17505074 -0.19757843 -0.31683445 -0.50052309 -0.77687979 -1.1383479 -1.6669664 -2.2769706 -2.7680478 -3.1014843 -3.207819 -3.1396852][-1.0395429 -0.97460771 -1.0577688 -1.0565317 -0.977113 -0.96491241 -1.0193429 -1.1995988 -1.4868147 -1.9632912 -2.5267951 -3.0089507 -3.3917537 -3.5836043 -3.6084151][-2.3520424 -2.4481103 -2.5376923 -2.45621 -2.2642567 -2.1533451 -2.1144025 -2.230969 -2.4923449 -2.9194515 -3.3776658 -3.7735729 -4.0999584 -4.234786 -4.2308979][-3.7766221 -3.9835539 -4.0921822 -3.9782255 -3.7130704 -3.5128753 -3.3910522 -3.4404626 -3.6404033 -3.9710903 -4.33221 -4.6080747 -4.8113089 -4.8485374 -4.754735][-4.743103 -5.0052137 -5.108017 -5.0124412 -4.7581062 -4.53064 -4.3893585 -4.3877268 -4.5033636 -4.6966619 -4.883091 -4.9988117 -5.0405278 -4.925056 -4.7108521][-5.1376276 -5.3905115 -5.5065613 -5.4400439 -5.2299895 -5.0401454 -4.9049072 -4.8621893 -4.8750949 -4.8945913 -4.889782 -4.8023376 -4.6426792 -4.3843317 -4.0478621][-4.9848971 -5.1421719 -5.21405 -5.1761069 -5.0405126 -4.8861184 -4.7597032 -4.6636915 -4.5722237 -4.4485517 -4.2834134 -4.0472779 -3.7528524 -3.4034872 -3.0327926][-4.5273628 -4.5499134 -4.513319 -4.4178133 -4.2687192 -4.1227784 -3.9833994 -3.8367324 -3.6907153 -3.5304966 -3.3282351 -3.0604665 -2.7518506 -2.4108009 -2.0618043]]...]
INFO - root - 2017-12-16 03:42:46.150702: step 1010, loss = 0.33, batch loss = 0.27 (27.0 examples/sec; 0.296 sec/batch; 27h:15m:51s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 03:42:48.941546: step 1020, loss = 0.29, batch loss = 0.23 (25.6 examples/sec; 0.313 sec/batch; 28h:48m:37s remains)
INFO - root - 2017-12-16 03:42:51.750311: step 1030, loss = 0.36, batch loss = 0.30 (28.7 examples/sec; 0.279 sec/batch; 25h:39m:27s remains)
INFO - root - 2017-12-16 03:42:54.560549: step 1040, loss = 0.47, batch loss = 0.42 (28.3 examples/sec; 0.283 sec/batch; 26h:02m:49s remains)
INFO - root - 2017-12-16 03:42:57.386613: step 1050, loss = 0.30, batch loss = 0.24 (28.7 examples/sec; 0.278 sec/batch; 25h:37m:50s remains)
INFO - root - 2017-12-16 03:43:00.165764: step 1060, loss = 0.31, batch loss = 0.26 (28.0 examples/sec; 0.285 sec/batch; 26h:16m:54s remains)
INFO - root - 2017-12-16 03:43:02.966278: step 1070, loss = 0.30, batch loss = 0.24 (28.1 examples/sec; 0.285 sec/batch; 26h:13m:19s remains)
INFO - root - 2017-12-16 03:43:05.759423: step 1080, loss = 0.40, batch loss = 0.34 (27.5 examples/sec; 0.291 sec/batch; 26h:44m:59s remains)
INFO - root - 2017-12-16 03:43:08.570130: step 1090, loss = 0.31, batch loss = 0.25 (29.9 examples/sec; 0.268 sec/batch; 24h:40m:14s remains)
INFO - root - 2017-12-16 03:43:11.388380: step 1100, loss = 0.37, batch loss = 0.31 (29.9 examples/sec; 0.267 sec/batch; 24h:36m:02s remains)
2017-12-16 03:43:11.847808: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1829443 -3.0337119 -2.8516302 -2.5992589 -2.1594014 -1.8000951 -1.7687767 -1.9046426 -2.0366931 -2.0965 -2.1441867 -2.1986864 -2.1490936 -2.0377564 -2.0603337][-2.7721033 -2.6613779 -2.5149179 -2.2592909 -1.7791655 -1.3776417 -1.3334026 -1.5486598 -1.8442962 -1.995472 -2.1177239 -2.2539375 -2.2228742 -2.0414016 -1.8814466][-2.506928 -2.4469709 -2.3195233 -2.0322981 -1.4968348 -1.0506408 -0.97702909 -1.217561 -1.633142 -1.9823015 -2.30443 -2.5475738 -2.5311139 -2.2852011 -1.982193][-2.3636007 -2.344101 -2.2154944 -1.8595095 -1.241488 -0.69967651 -0.56533623 -0.80630779 -1.3123362 -1.8564758 -2.4119635 -2.8550611 -2.9438436 -2.7011533 -2.3098228][-2.4061575 -2.3500628 -2.155679 -1.6693125 -0.90505242 -0.22251463 0.030925751 -0.19541693 -0.78936982 -1.5187418 -2.2978389 -2.958323 -3.2457039 -3.119206 -2.7601719][-2.7260673 -2.6099849 -2.2657516 -1.5852871 -0.60425019 0.27407694 0.67238188 0.46932316 -0.23000431 -1.1564472 -2.1828315 -3.0461481 -3.5341825 -3.6168609 -3.3908725][-3.209929 -3.0335026 -2.5240393 -1.6152229 -0.43100858 0.61138248 1.1262035 0.94385815 0.15460443 -0.94269061 -2.1611688 -3.2017016 -3.8435988 -4.0965509 -3.9997663][-3.7455127 -3.5059066 -2.8767967 -1.7953842 -0.477633 0.6668992 1.2481318 1.0502262 0.19677925 -0.9873836 -2.2917709 -3.448452 -4.2187524 -4.5800595 -4.5835571][-4.2723145 -4.0139151 -3.3300526 -2.2022662 -0.89317846 0.2178688 0.80039263 0.60791922 -0.24458933 -1.4383171 -2.7414596 -3.8576646 -4.636734 -5.041677 -5.0988126][-4.6845822 -4.4946804 -3.8773811 -2.8337564 -1.6415873 -0.65501404 -0.153265 -0.31489468 -1.0970833 -2.1946034 -3.3521302 -4.3216982 -4.9856529 -5.3277349 -5.3640943][-4.7768588 -4.7312088 -4.2881312 -3.4247022 -2.4005966 -1.5387995 -1.1304312 -1.2824299 -1.9372499 -2.8923435 -3.8779361 -4.6655703 -5.1540313 -5.3474927 -5.3091774][-4.591681 -4.6833754 -4.4277163 -3.7714527 -2.950613 -2.2103562 -1.8293173 -1.9204645 -2.4407029 -3.20889 -3.9942796 -4.6078644 -4.9603553 -5.0541248 -4.956656][-4.2641172 -4.4704385 -4.3495221 -3.8396864 -3.1330829 -2.4755893 -2.132735 -2.1561971 -2.5371647 -3.1378341 -3.7526374 -4.2173171 -4.4738975 -4.5232196 -4.4077172][-3.9532375 -4.2002411 -4.1371183 -3.6898148 -3.0283179 -2.3880019 -2.0226135 -1.9927685 -2.2896619 -2.7475843 -3.2316246 -3.6188626 -3.81386 -3.8474708 -3.7419219][-3.6504698 -3.9064999 -3.8468463 -3.4075384 -2.7428427 -2.100908 -1.6938605 -1.5866082 -1.775485 -2.1485715 -2.5576158 -2.881196 -3.0337152 -3.0713038 -3.0042074]]...]
INFO - root - 2017-12-16 03:43:14.598180: step 1110, loss = 0.38, batch loss = 0.33 (29.8 examples/sec; 0.268 sec/batch; 24h:42m:25s remains)
INFO - root - 2017-12-16 03:43:17.366962: step 1120, loss = 0.33, batch loss = 0.27 (28.7 examples/sec; 0.279 sec/batch; 25h:41m:16s remains)
INFO - root - 2017-12-16 03:43:20.115087: step 1130, loss = 0.28, batch loss = 0.22 (27.6 examples/sec; 0.290 sec/batch; 26h:39m:28s remains)
INFO - root - 2017-12-16 03:43:22.952413: step 1140, loss = 0.36, batch loss = 0.30 (28.7 examples/sec; 0.278 sec/batch; 25h:37m:48s remains)
INFO - root - 2017-12-16 03:43:25.842427: step 1150, loss = 0.31, batch loss = 0.25 (26.0 examples/sec; 0.308 sec/batch; 28h:19m:53s remains)
INFO - root - 2017-12-16 03:43:28.643035: step 1160, loss = 0.34, batch loss = 0.28 (29.3 examples/sec; 0.273 sec/batch; 25h:09m:48s remains)
INFO - root - 2017-12-16 03:43:31.428324: step 1170, loss = 0.30, batch loss = 0.24 (28.4 examples/sec; 0.282 sec/batch; 25h:54m:44s remains)
INFO - root - 2017-12-16 03:43:34.238803: step 1180, loss = 0.29, batch loss = 0.23 (27.4 examples/sec; 0.291 sec/batch; 26h:49m:37s remains)
INFO - root - 2017-12-16 03:43:37.043419: step 1190, loss = 0.33, batch loss = 0.27 (26.9 examples/sec; 0.297 sec/batch; 27h:20m:35s remains)
INFO - root - 2017-12-16 03:43:39.871112: step 1200, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 26h:18m:20s remains)
2017-12-16 03:43:40.348323: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.136272 -3.2766562 -3.4398015 -3.566649 -3.6213088 -3.5956988 -3.409848 -3.2240334 -3.095428 -3.0949669 -3.1665668 -3.1839795 -3.1997094 -3.0957541 -2.862987][-2.6249266 -2.7547579 -3.0094066 -3.2265959 -3.3192418 -3.3147769 -3.1231499 -2.9067979 -2.7614207 -2.7861071 -2.9297976 -3.0159252 -3.0982184 -3.048775 -2.8423073][-2.0899827 -2.1667786 -2.4585395 -2.7305393 -2.8204954 -2.7802188 -2.5636339 -2.3518171 -2.2443132 -2.3212039 -2.5700831 -2.7676544 -2.9186358 -2.8970485 -2.7287996][-1.6252384 -1.6836591 -1.9717233 -2.2283678 -2.2619288 -2.1291544 -1.8554361 -1.653137 -1.6202145 -1.7981081 -2.1592767 -2.4746232 -2.6654172 -2.6217928 -2.3992314][-1.5572779 -1.5855002 -1.770304 -1.9066062 -1.7830393 -1.4812622 -1.132575 -0.95580339 -1.0459011 -1.4062412 -1.9319978 -2.3738406 -2.6144819 -2.5857747 -2.3486867][-2.0805202 -1.9837809 -1.9402626 -1.7894969 -1.3781714 -0.87909174 -0.47781372 -0.39898729 -0.68552494 -1.2594166 -1.9487023 -2.4982495 -2.7853477 -2.7736235 -2.5442648][-2.856214 -2.6381583 -2.3492768 -1.885092 -1.1850085 -0.49239683 -0.090694427 -0.17588282 -0.687438 -1.4503455 -2.2446184 -2.8151002 -3.0685868 -3.0335481 -2.8251157][-3.6279981 -3.380259 -2.9326267 -2.2355714 -1.3621166 -0.5916388 -0.22209024 -0.44143534 -1.1097786 -2.0015233 -2.8304167 -3.3519516 -3.5140724 -3.3983526 -3.1735525][-4.2733707 -4.0661736 -3.5900269 -2.845484 -1.9730313 -1.2591407 -0.97864676 -1.2861028 -1.9826243 -2.8610902 -3.6075349 -4.0184216 -4.0761075 -3.8778718 -3.597563][-4.6946831 -4.5861659 -4.199894 -3.5428443 -2.7885551 -2.2023313 -2.0229235 -2.33293 -2.9564731 -3.6966748 -4.270164 -4.5287018 -4.5072136 -4.2711115 -3.9917817][-4.6329846 -4.6314 -4.4126334 -3.9495645 -3.4199698 -3.0229936 -2.9623208 -3.2606802 -3.75214 -4.2833858 -4.6326275 -4.7053404 -4.5712333 -4.2710652 -3.9766834][-4.3169947 -4.4205728 -4.3793511 -4.1266203 -3.827872 -3.6087279 -3.6216431 -3.8734815 -4.2210417 -4.54971 -4.7100439 -4.6566057 -4.4577913 -4.1433897 -3.8578923][-3.9140782 -4.0714803 -4.1638422 -4.0816574 -3.9601474 -3.8831997 -3.9348559 -4.1255078 -4.3434725 -4.5369167 -4.5936246 -4.4827662 -4.2767076 -3.9986949 -3.7517753][-3.4496744 -3.5652919 -3.6770961 -3.6881871 -3.6988809 -3.7145283 -3.7955503 -3.961493 -4.130971 -4.2548203 -4.26936 -4.1688318 -4.0143423 -3.8006577 -3.6141167][-3.1079698 -3.152092 -3.2183375 -3.2383423 -3.3036723 -3.3733697 -3.485908 -3.6300962 -3.7890441 -3.9102945 -3.9345696 -3.8647656 -3.7632337 -3.6181581 -3.4831347]]...]
INFO - root - 2017-12-16 03:43:43.154531: step 1210, loss = 0.40, batch loss = 0.34 (29.1 examples/sec; 0.275 sec/batch; 25h:18m:27s remains)
INFO - root - 2017-12-16 03:43:45.936603: step 1220, loss = 0.32, batch loss = 0.26 (29.5 examples/sec; 0.271 sec/batch; 24h:58m:00s remains)
INFO - root - 2017-12-16 03:43:48.709369: step 1230, loss = 0.43, batch loss = 0.37 (29.1 examples/sec; 0.275 sec/batch; 25h:15m:45s remains)
INFO - root - 2017-12-16 03:43:51.530216: step 1240, loss = 0.28, batch loss = 0.22 (28.7 examples/sec; 0.279 sec/batch; 25h:39m:52s remains)
INFO - root - 2017-12-16 03:43:54.349606: step 1250, loss = 0.36, batch loss = 0.31 (28.3 examples/sec; 0.283 sec/batch; 26h:02m:09s remains)
INFO - root - 2017-12-16 03:43:57.148623: step 1260, loss = 0.29, batch loss = 0.23 (28.5 examples/sec; 0.281 sec/batch; 25h:50m:32s remains)
INFO - root - 2017-12-16 03:43:59.924216: step 1270, loss = 0.40, batch loss = 0.34 (29.6 examples/sec; 0.270 sec/batch; 24h:51m:26s remains)
INFO - root - 2017-12-16 03:44:02.657192: step 1280, loss = 0.39, batch loss = 0.33 (29.5 examples/sec; 0.272 sec/batch; 24h:59m:09s remains)
INFO - root - 2017-12-16 03:44:05.425778: step 1290, loss = 0.43, batch loss = 0.37 (30.0 examples/sec; 0.267 sec/batch; 24h:31m:55s remains)
INFO - root - 2017-12-16 03:44:08.224700: step 1300, loss = 0.35, batch loss = 0.30 (29.4 examples/sec; 0.272 sec/batch; 25h:02m:22s remains)
2017-12-16 03:44:08.688087: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3666766 -1.0720706 -0.53556705 0.080017567 0.44966507 0.40132809 -0.051344395 -0.733526 -1.3485804 -1.5394819 -1.26932 -0.68838024 -0.08866787 0.26878023 0.22899771][-1.0289567 -0.82078457 -0.34780359 0.25379086 0.60809088 0.57405281 0.095977306 -0.66876245 -1.4173777 -1.7783573 -1.6270075 -1.0781736 -0.45833826 -0.0630784 -0.050622463][-1.3066659 -1.2476971 -0.92916322 -0.479084 -0.18766546 -0.20069075 -0.610363 -1.2627015 -1.8919647 -2.183943 -1.9693747 -1.3892469 -0.77587628 -0.46681762 -0.604326][-2.019155 -2.0429339 -1.8233821 -1.451654 -1.1511569 -1.0122952 -1.1756275 -1.5656741 -1.9478207 -2.0377669 -1.70579 -1.1168566 -0.60566425 -0.50164747 -0.86881471][-2.5775895 -2.6877971 -2.532706 -2.1327825 -1.6598029 -1.2287996 -1.0245867 -1.0583317 -1.2065101 -1.2122691 -0.92132545 -0.485121 -0.23222256 -0.42187643 -1.0469773][-2.5674186 -2.7531548 -2.6825609 -2.2245941 -1.560982 -0.81557846 -0.23690224 0.061132908 0.089321136 -0.011800766 -0.018076897 0.028189659 -0.10775518 -0.56668568 -1.3096852][-2.2149684 -2.484714 -2.4698634 -1.9800761 -1.1061139 -0.038100243 0.91508675 1.4876981 1.5400591 1.1882825 0.69459009 0.1950407 -0.37979889 -1.0349925 -1.735549][-1.7553592 -2.150728 -2.2446909 -1.763401 -0.78923464 0.48869991 1.7283301 2.5191426 2.5920949 1.9705849 0.99820709 -0.043224812 -1.0141487 -1.7738781 -2.3236904][-1.4708946 -1.9561558 -2.1249089 -1.7296121 -0.81652617 0.40838766 1.6669545 2.4946718 2.5187049 1.7442794 0.48408222 -0.90589094 -2.0976806 -2.8702619 -3.1802318][-1.4399211 -2.0196989 -2.3167553 -2.0866551 -1.3841496 -0.39589071 0.61644411 1.2617579 1.2060685 0.38355827 -0.92552257 -2.3682075 -3.5803864 -4.2513094 -4.313549][-1.6934237 -2.2946305 -2.6677313 -2.6579213 -2.2826815 -1.6831112 -1.019592 -0.60825992 -0.72822976 -1.4003947 -2.4984641 -3.7539968 -4.8031993 -5.3492036 -5.2633348][-1.8705487 -2.4143422 -2.8105311 -2.9801078 -2.8996916 -2.6854811 -2.4132612 -2.2370427 -2.3495195 -2.794127 -3.5422053 -4.4598722 -5.2686491 -5.702095 -5.6090894][-1.6089206 -2.0737426 -2.5267012 -2.8697557 -3.0749869 -3.1873677 -3.2044322 -3.1928015 -3.2510204 -3.41682 -3.7975109 -4.3800712 -4.9568987 -5.3803606 -5.4582582][-1.0594265 -1.4765506 -1.9638729 -2.4728072 -2.9195628 -3.2784984 -3.4687552 -3.5045414 -3.4742432 -3.4041686 -3.4679608 -3.7589636 -4.1587367 -4.5834856 -4.8622622][-0.52089429 -0.94586015 -1.4731648 -2.0483143 -2.5791874 -3.0641842 -3.3307276 -3.3450706 -3.17444 -2.9079239 -2.767838 -2.8379023 -3.1333566 -3.5696406 -4.0006576]]...]
INFO - root - 2017-12-16 03:44:11.499865: step 1310, loss = 0.45, batch loss = 0.39 (27.5 examples/sec; 0.291 sec/batch; 26h:44m:10s remains)
INFO - root - 2017-12-16 03:44:14.351504: step 1320, loss = 0.35, batch loss = 0.29 (29.0 examples/sec; 0.276 sec/batch; 25h:23m:13s remains)
INFO - root - 2017-12-16 03:44:17.184333: step 1330, loss = 0.39, batch loss = 0.33 (28.6 examples/sec; 0.280 sec/batch; 25h:45m:54s remains)
INFO - root - 2017-12-16 03:44:19.995662: step 1340, loss = 0.34, batch loss = 0.28 (28.7 examples/sec; 0.279 sec/batch; 25h:38m:16s remains)
INFO - root - 2017-12-16 03:44:22.850329: step 1350, loss = 0.33, batch loss = 0.27 (27.8 examples/sec; 0.288 sec/batch; 26h:27m:15s remains)
INFO - root - 2017-12-16 03:44:25.676632: step 1360, loss = 0.32, batch loss = 0.27 (29.8 examples/sec; 0.268 sec/batch; 24h:40m:31s remains)
INFO - root - 2017-12-16 03:44:28.472318: step 1370, loss = 0.40, batch loss = 0.35 (29.3 examples/sec; 0.273 sec/batch; 25h:08m:00s remains)
INFO - root - 2017-12-16 03:44:31.331744: step 1380, loss = 0.38, batch loss = 0.32 (28.4 examples/sec; 0.281 sec/batch; 25h:53m:18s remains)
INFO - root - 2017-12-16 03:44:34.118619: step 1390, loss = 0.31, batch loss = 0.25 (25.8 examples/sec; 0.310 sec/batch; 28h:32m:27s remains)
INFO - root - 2017-12-16 03:44:36.864194: step 1400, loss = 0.38, batch loss = 0.32 (29.7 examples/sec; 0.269 sec/batch; 24h:44m:30s remains)
2017-12-16 03:44:37.318039: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0873504 -1.2402503 -1.6251655 -2.1071019 -2.6298497 -3.0174623 -3.1905785 -3.0815086 -2.8602793 -2.6210468 -2.3984792 -2.2569172 -2.1938977 -2.2949944 -2.4171095][-1.1807809 -1.2759726 -1.6023262 -2.003432 -2.4630132 -2.8884621 -3.1309743 -3.1231749 -2.949523 -2.7325163 -2.519151 -2.3020451 -2.1317914 -2.1278765 -2.1639905][-1.5041776 -1.5000558 -1.6990535 -1.9237576 -2.2506292 -2.6302638 -2.9105697 -2.9825931 -2.8849857 -2.7478764 -2.5831876 -2.3966236 -2.2360337 -2.1837032 -2.1678383][-1.6949191 -1.5249469 -1.5732722 -1.5793025 -1.7138798 -1.9629738 -2.1624906 -2.2307639 -2.1556964 -2.0315895 -1.9219911 -1.9080319 -2.0286829 -2.2084534 -2.3736722][-1.7419651 -1.3612599 -1.1981063 -0.99314833 -0.92188287 -0.949497 -0.98799348 -0.98809385 -0.90814042 -0.83858657 -0.92608 -1.2354348 -1.7627189 -2.3185036 -2.7421312][-1.9176548 -1.3328445 -0.89577651 -0.51276493 -0.22979832 -0.053355217 0.11356544 0.27485085 0.39611578 0.34895515 -0.005218029 -0.7428968 -1.7770519 -2.7366006 -3.389781][-2.3104007 -1.5766001 -0.944705 -0.33555984 0.19835043 0.67308283 1.0912409 1.3234587 1.4035869 1.1559119 0.449996 -0.69750643 -2.095324 -3.3441355 -4.1187611][-3.2169046 -2.4926829 -1.7774227 -1.1023734 -0.39922476 0.35168123 1.0912232 1.5431347 1.6572123 1.20888 0.18510914 -1.2645402 -2.9008198 -4.2073965 -4.9374328][-4.3759522 -3.7985091 -3.1986785 -2.5747736 -1.8653591 -1.0906894 -0.32168627 0.21929359 0.37646389 -0.061882019 -1.1241035 -2.5310361 -3.9946244 -5.1132517 -5.6462526][-5.4468346 -5.1188831 -4.7176123 -4.2613444 -3.660713 -2.9429464 -2.2634401 -1.8219745 -1.7182448 -2.117193 -2.9934812 -4.1428447 -5.2247744 -5.8804073 -6.0817041][-5.7982664 -5.6142836 -5.39701 -5.1669073 -4.8257332 -4.321363 -3.7995892 -3.4846687 -3.4061875 -3.6535823 -4.2761035 -5.0643649 -5.68207 -5.9213905 -5.8072968][-5.576829 -5.5067463 -5.3812909 -5.2222652 -4.9835439 -4.6871943 -4.404583 -4.2344561 -4.1915321 -4.3479028 -4.7011213 -5.1595855 -5.4610558 -5.3775473 -5.033062][-4.9714017 -4.9756813 -4.9645443 -4.9520125 -4.880846 -4.7103581 -4.5346203 -4.4149523 -4.3489227 -4.3746047 -4.5347042 -4.7319174 -4.7738719 -4.5303078 -4.0670476][-4.2977924 -4.2569003 -4.241375 -4.286449 -4.3314366 -4.31067 -4.2835531 -4.2478366 -4.2083631 -4.1732745 -4.165484 -4.2109308 -4.1413174 -3.7802234 -3.2438707][-3.9363408 -3.8806229 -3.8324263 -3.8203821 -3.8269613 -3.8305621 -3.8382559 -3.83752 -3.8350549 -3.8184562 -3.7986581 -3.7896626 -3.6715593 -3.3002825 -2.7897363]]...]
INFO - root - 2017-12-16 03:44:40.143477: step 1410, loss = 0.35, batch loss = 0.29 (28.1 examples/sec; 0.285 sec/batch; 26h:09m:59s remains)
INFO - root - 2017-12-16 03:44:42.933986: step 1420, loss = 0.37, batch loss = 0.31 (28.3 examples/sec; 0.283 sec/batch; 25h:58m:50s remains)
INFO - root - 2017-12-16 03:44:45.676961: step 1430, loss = 0.33, batch loss = 0.27 (28.2 examples/sec; 0.283 sec/batch; 26h:04m:01s remains)
INFO - root - 2017-12-16 03:44:48.501163: step 1440, loss = 0.35, batch loss = 0.29 (29.2 examples/sec; 0.274 sec/batch; 25h:09m:14s remains)
INFO - root - 2017-12-16 03:44:51.284432: step 1450, loss = 0.49, batch loss = 0.43 (29.2 examples/sec; 0.274 sec/batch; 25h:09m:36s remains)
INFO - root - 2017-12-16 03:44:54.065732: step 1460, loss = 0.35, batch loss = 0.30 (28.3 examples/sec; 0.282 sec/batch; 25h:57m:52s remains)
INFO - root - 2017-12-16 03:44:56.864243: step 1470, loss = 0.31, batch loss = 0.25 (28.9 examples/sec; 0.277 sec/batch; 25h:26m:43s remains)
INFO - root - 2017-12-16 03:44:59.628919: step 1480, loss = 0.41, batch loss = 0.35 (29.4 examples/sec; 0.272 sec/batch; 25h:03m:03s remains)
INFO - root - 2017-12-16 03:45:02.443322: step 1490, loss = 0.42, batch loss = 0.36 (29.2 examples/sec; 0.274 sec/batch; 25h:09m:26s remains)
INFO - root - 2017-12-16 03:45:05.322320: step 1500, loss = 0.31, batch loss = 0.26 (28.2 examples/sec; 0.283 sec/batch; 26h:03m:21s remains)
2017-12-16 03:45:05.791250: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5488417 -3.6247797 -3.6704373 -3.7612152 -3.8943808 -4.0258203 -4.0464015 -3.9963117 -3.8409858 -3.4845796 -2.9058497 -2.2567675 -1.6568675 -1.2208478 -0.99926257][-3.5311239 -3.6543322 -3.7164137 -3.7922132 -3.8741121 -3.8918178 -3.7826061 -3.6284225 -3.3715138 -2.9200821 -2.244926 -1.526067 -0.89372492 -0.472461 -0.329926][-3.7060556 -3.8704123 -3.9262843 -3.949106 -3.9525156 -3.8315423 -3.6006708 -3.36276 -3.0090876 -2.490891 -1.7806706 -1.0857399 -0.54035592 -0.20019531 -0.14503431][-3.8804162 -4.0374932 -4.0305929 -3.8840642 -3.6673102 -3.3260484 -2.9689009 -2.6893682 -2.3495398 -1.8986447 -1.2661405 -0.6968751 -0.3288331 -0.20031834 -0.34660482][-4.1139917 -4.1934218 -3.9954531 -3.5321629 -2.9700911 -2.3398397 -1.8312733 -1.551971 -1.3537819 -1.161427 -0.78588223 -0.4382503 -0.31210852 -0.40100241 -0.75297332][-4.30917 -4.2582345 -3.8142719 -3.0073409 -2.0894656 -1.190804 -0.55609989 -0.32301855 -0.3349719 -0.45940804 -0.43133354 -0.3863678 -0.500824 -0.79011011 -1.2814529][-4.4132152 -4.2205954 -3.5419626 -2.4370236 -1.2381146 -0.13405037 0.61920023 0.79280663 0.53571415 0.072368145 -0.24855089 -0.51449132 -0.8795054 -1.3440034 -1.9075887][-4.3667097 -4.0441813 -3.1759598 -1.8739069 -0.52032208 0.62749434 1.360229 1.4566226 1.0031366 0.26434374 -0.36335373 -0.87050653 -1.4002171 -1.9795413 -2.5538955][-4.1972876 -3.7957582 -2.8295145 -1.468117 -0.11608267 0.92022943 1.4622626 1.3925204 0.79724979 -0.0715909 -0.87394166 -1.4960506 -2.0334082 -2.5729182 -3.0238638][-3.8912 -3.5081968 -2.6166143 -1.4152873 -0.23484087 0.62586308 0.94651651 0.70384932 0.020641327 -0.8685751 -1.6936338 -2.3081288 -2.7369204 -3.0733685 -3.2731705][-3.5681973 -3.2844 -2.6347375 -1.7706695 -0.91021919 -0.31649923 -0.22495222 -0.56509447 -1.2201822 -1.9991159 -2.6923587 -3.1646447 -3.3699861 -3.4078975 -3.2561617][-3.2471952 -3.078624 -2.729928 -2.2608993 -1.7676299 -1.416363 -1.4638026 -1.8083179 -2.3126786 -2.8799543 -3.3637877 -3.6608963 -3.6646514 -3.3828144 -2.8799152][-2.906594 -2.8463147 -2.8158941 -2.7467079 -2.6230905 -2.4741335 -2.5578551 -2.8080869 -3.1009676 -3.3732176 -3.5770578 -3.6436043 -3.4468164 -2.9303489 -2.1744442][-2.6847448 -2.6934052 -2.8731973 -3.060564 -3.1895533 -3.1618435 -3.1865015 -3.2726164 -3.3689775 -3.4576383 -3.4926856 -3.4432874 -3.1541116 -2.5102758 -1.6056108][-2.6490824 -2.7325931 -3.0380945 -3.3441119 -3.5511909 -3.5021355 -3.3635869 -3.2608185 -3.2168012 -3.2469106 -3.2746873 -3.2450881 -2.9730024 -2.2982168 -1.3725946]]...]
INFO - root - 2017-12-16 03:45:08.638701: step 1510, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.284 sec/batch; 26h:07m:46s remains)
INFO - root - 2017-12-16 03:45:11.425303: step 1520, loss = 0.45, batch loss = 0.39 (29.2 examples/sec; 0.274 sec/batch; 25h:13m:20s remains)
INFO - root - 2017-12-16 03:45:14.256573: step 1530, loss = 0.39, batch loss = 0.33 (28.2 examples/sec; 0.284 sec/batch; 26h:07m:03s remains)
INFO - root - 2017-12-16 03:45:17.081458: step 1540, loss = 0.29, batch loss = 0.23 (28.5 examples/sec; 0.280 sec/batch; 25h:46m:58s remains)
INFO - root - 2017-12-16 03:45:19.957374: step 1550, loss = 0.38, batch loss = 0.32 (27.6 examples/sec; 0.290 sec/batch; 26h:38m:57s remains)
INFO - root - 2017-12-16 03:45:22.732700: step 1560, loss = 0.38, batch loss = 0.32 (28.6 examples/sec; 0.280 sec/batch; 25h:44m:15s remains)
INFO - root - 2017-12-16 03:45:25.576995: step 1570, loss = 0.38, batch loss = 0.32 (28.7 examples/sec; 0.278 sec/batch; 25h:35m:08s remains)
INFO - root - 2017-12-16 03:45:28.371512: step 1580, loss = 0.36, batch loss = 0.30 (28.3 examples/sec; 0.283 sec/batch; 26h:00m:29s remains)
INFO - root - 2017-12-16 03:45:31.232870: step 1590, loss = 0.33, batch loss = 0.28 (28.4 examples/sec; 0.282 sec/batch; 25h:54m:01s remains)
INFO - root - 2017-12-16 03:45:34.045691: step 1600, loss = 0.56, batch loss = 0.50 (28.4 examples/sec; 0.282 sec/batch; 25h:53m:12s remains)
2017-12-16 03:45:34.541872: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0156355 -1.0652635 -1.3312135 -1.6761966 -1.7096643 -1.7222195 -1.7212908 -1.5825868 -1.4756868 -1.6667581 -2.3093083 -2.8883376 -3.2286363 -3.4983675 -3.7036326][-0.32326603 -0.52437663 -0.95109391 -1.4065838 -1.4763987 -1.4369957 -1.383369 -1.2666309 -1.2079632 -1.4893224 -2.2551014 -2.9671319 -3.3703976 -3.6262777 -3.8420331][-0.15959883 -0.55175567 -1.093029 -1.481617 -1.4521203 -1.3089581 -1.1915076 -1.0483727 -1.0408919 -1.5098772 -2.4111876 -3.2773175 -3.7951419 -4.0240908 -4.1836257][-0.66620779 -1.073148 -1.5378313 -1.7716208 -1.5371754 -1.0853264 -0.75795889 -0.62274432 -0.69580507 -1.3059299 -2.3884587 -3.455821 -4.1637168 -4.4573092 -4.6106095][-1.4556859 -1.7409608 -1.964097 -1.8299971 -1.2681181 -0.59740829 -0.13974571 -0.11135578 -0.40970182 -1.1791499 -2.3248672 -3.4810231 -4.2791 -4.6159372 -4.7621098][-2.1370845 -2.0776761 -1.9233518 -1.4011414 -0.53438354 0.3052721 0.800673 0.73258734 0.22331905 -0.83957648 -2.110384 -3.3129585 -4.1341577 -4.4654074 -4.5663142][-2.4492664 -2.074954 -1.5499592 -0.743804 0.25281286 1.2086339 1.8148694 1.7043867 1.024199 -0.23182726 -1.6289811 -2.8769879 -3.6635268 -3.8614 -3.7941623][-2.4707117 -1.8932722 -1.1680264 -0.25019073 0.76648426 1.6438494 2.1364021 1.9941492 1.3305979 0.036887646 -1.3500955 -2.4055343 -2.9433537 -2.8823915 -2.5535247][-2.4953766 -1.9429898 -1.2908869 -0.50842214 0.31538868 1.0110941 1.3751493 1.209825 0.62142944 -0.41244221 -1.4415252 -2.1215436 -2.1864326 -1.6682947 -0.99632144][-2.5738575 -2.2640281 -1.9000609 -1.3690877 -0.86068368 -0.47139478 -0.29087162 -0.44170666 -0.88028836 -1.5575702 -2.0984955 -2.1860921 -1.7092953 -0.80150127 0.16926479][-2.5804477 -2.5821471 -2.5692482 -2.3881075 -2.1766598 -2.0104628 -2.0004671 -2.200434 -2.5178394 -2.8998737 -3.0222502 -2.6576986 -1.8293214 -0.62233758 0.58748579][-2.7189374 -2.9682417 -3.2148192 -3.2739277 -3.3130665 -3.3715973 -3.5209951 -3.7721114 -4.0259504 -4.1644378 -3.9521029 -3.3694882 -2.4218798 -1.2193077 -0.018146992][-2.8715916 -3.1758428 -3.507463 -3.7078738 -3.9266462 -4.1697526 -4.5144992 -4.8796873 -5.1912274 -5.2556233 -4.8993268 -4.2122107 -3.2718754 -2.3022993 -1.4814999][-3.0281205 -3.221261 -3.3700628 -3.4971602 -3.724546 -4.0624733 -4.5834436 -5.1644154 -5.6882706 -5.9073629 -5.6457081 -5.0271816 -4.2425857 -3.560189 -3.0674882][-3.2941451 -3.3072915 -3.2335691 -3.1749837 -3.3176694 -3.6527529 -4.2419181 -5.0183558 -5.7549753 -6.1255765 -6.0103106 -5.57304 -5.0277033 -4.6478004 -4.4399962]]...]
INFO - root - 2017-12-16 03:45:37.356095: step 1610, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.285 sec/batch; 26h:09m:00s remains)
INFO - root - 2017-12-16 03:45:40.194291: step 1620, loss = 0.28, batch loss = 0.22 (28.6 examples/sec; 0.280 sec/batch; 25h:45m:03s remains)
INFO - root - 2017-12-16 03:45:42.982038: step 1630, loss = 0.41, batch loss = 0.35 (27.8 examples/sec; 0.288 sec/batch; 26h:29m:20s remains)
INFO - root - 2017-12-16 03:45:45.803946: step 1640, loss = 0.32, batch loss = 0.27 (29.1 examples/sec; 0.275 sec/batch; 25h:15m:16s remains)
INFO - root - 2017-12-16 03:45:48.596953: step 1650, loss = 0.32, batch loss = 0.26 (28.1 examples/sec; 0.285 sec/batch; 26h:10m:43s remains)
INFO - root - 2017-12-16 03:45:51.408808: step 1660, loss = 0.34, batch loss = 0.28 (29.0 examples/sec; 0.276 sec/batch; 25h:19m:58s remains)
INFO - root - 2017-12-16 03:45:54.190018: step 1670, loss = 0.42, batch loss = 0.36 (29.8 examples/sec; 0.268 sec/batch; 24h:39m:32s remains)
INFO - root - 2017-12-16 03:45:57.031111: step 1680, loss = 0.48, batch loss = 0.42 (28.1 examples/sec; 0.284 sec/batch; 26h:08m:19s remains)
INFO - root - 2017-12-16 03:45:59.824918: step 1690, loss = 0.33, batch loss = 0.27 (28.4 examples/sec; 0.281 sec/batch; 25h:51m:51s remains)
INFO - root - 2017-12-16 03:46:02.629300: step 1700, loss = 0.38, batch loss = 0.33 (27.9 examples/sec; 0.287 sec/batch; 26h:20m:57s remains)
2017-12-16 03:46:03.108304: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0356803 -4.15237 -4.3487349 -4.6407366 -4.9239392 -5.1178184 -5.2476144 -5.2840824 -5.3212137 -5.3500671 -5.4070215 -5.4726872 -5.4884658 -5.3952618 -5.1835718][-3.7101948 -3.8010194 -4.0885615 -4.5248456 -4.940855 -5.1566844 -5.225915 -5.2335296 -5.2950187 -5.3429866 -5.4661117 -5.6190166 -5.7147865 -5.68301 -5.4735875][-3.1080904 -3.1291103 -3.4390759 -3.8801775 -4.2886953 -4.4777513 -4.5031786 -4.4584503 -4.5690241 -4.7643657 -5.0629096 -5.400598 -5.6389723 -5.6910572 -5.5216236][-2.3622947 -2.2172048 -2.4228892 -2.7636466 -3.0144629 -2.9945557 -2.8613524 -2.7656794 -2.9401224 -3.3467262 -3.8916841 -4.50525 -4.96733 -5.1850986 -5.1283493][-1.5949929 -1.265784 -1.3203053 -1.4309099 -1.3803794 -1.0383143 -0.65083289 -0.46041083 -0.67702222 -1.3006971 -2.1029344 -2.9715805 -3.6723838 -4.12254 -4.2868729][-0.9342916 -0.43032146 -0.20265388 0.00487566 0.41986132 1.0524578 1.6521759 1.8667889 1.488061 0.63344526 -0.43108845 -1.5630631 -2.5001559 -3.1738238 -3.5223498][-0.42289925 0.16952705 0.62980461 1.2308798 2.010571 2.8612909 3.5493531 3.6339226 2.989501 1.8305264 0.49277925 -0.88411307 -2.0221643 -2.8128691 -3.2388964][-0.61002111 0.080225468 0.73973465 1.5986805 2.6088138 3.6760502 4.4633331 4.4623375 3.6707821 2.2884297 0.76724243 -0.72345495 -1.9847622 -2.8671834 -3.3396196][-1.5272017 -0.91568995 -0.20934868 0.81840754 1.9681015 3.0921798 3.9298182 4.0513496 3.377254 2.0451612 0.5372057 -0.92575312 -2.1876857 -3.1288092 -3.6480055][-2.93263 -2.437979 -1.7609982 -0.83375907 0.16890144 1.1508512 1.8560333 2.0159807 1.5008321 0.43998432 -0.7637713 -1.9684041 -3.0416737 -3.8708611 -4.3443418][-4.540545 -4.2674136 -3.7609327 -2.98581 -2.1873088 -1.4857156 -1.0188475 -0.9176774 -1.2860668 -2.07695 -2.958075 -3.7689524 -4.4528303 -5.0038729 -5.2744474][-6.14062 -6.0569043 -5.7438688 -5.2115879 -4.6864071 -4.1813765 -3.79456 -3.6893325 -3.8752713 -4.3512969 -4.8752136 -5.3205218 -5.6801643 -5.9499044 -6.0086427][-7.0123796 -7.1635628 -7.112421 -6.8461895 -6.548502 -6.2650347 -6.0418191 -5.8375568 -5.7595668 -5.8663626 -5.997612 -6.1026168 -6.1522427 -6.1808538 -6.09381][-6.9008384 -7.1433191 -7.2189336 -7.2055788 -7.1847057 -7.0791011 -6.9673667 -6.8051577 -6.6739268 -6.5425673 -6.3785009 -6.1963077 -5.9998522 -5.8438592 -5.6367383][-6.0784407 -6.3013334 -6.4321365 -6.5002289 -6.5456519 -6.5979767 -6.6458654 -6.5706363 -6.4512968 -6.2713909 -6.0222464 -5.7143569 -5.3951144 -5.1356654 -4.8679352]]...]
INFO - root - 2017-12-16 03:46:05.869214: step 1710, loss = 0.37, batch loss = 0.32 (29.1 examples/sec; 0.275 sec/batch; 25h:17m:46s remains)
INFO - root - 2017-12-16 03:46:08.688160: step 1720, loss = 0.31, batch loss = 0.26 (26.9 examples/sec; 0.297 sec/batch; 27h:18m:00s remains)
INFO - root - 2017-12-16 03:46:11.502069: step 1730, loss = 0.31, batch loss = 0.25 (29.7 examples/sec; 0.269 sec/batch; 24h:44m:34s remains)
INFO - root - 2017-12-16 03:46:14.316748: step 1740, loss = 0.35, batch loss = 0.30 (26.6 examples/sec; 0.301 sec/batch; 27h:39m:18s remains)
INFO - root - 2017-12-16 03:46:17.149530: step 1750, loss = 0.38, batch loss = 0.32 (28.6 examples/sec; 0.280 sec/batch; 25h:41m:40s remains)
INFO - root - 2017-12-16 03:46:19.966547: step 1760, loss = 0.26, batch loss = 0.20 (29.6 examples/sec; 0.271 sec/batch; 24h:51m:45s remains)
INFO - root - 2017-12-16 03:46:22.795007: step 1770, loss = 0.51, batch loss = 0.45 (28.5 examples/sec; 0.281 sec/batch; 25h:47m:11s remains)
INFO - root - 2017-12-16 03:46:25.559600: step 1780, loss = 0.31, batch loss = 0.25 (27.1 examples/sec; 0.295 sec/batch; 27h:06m:47s remains)
INFO - root - 2017-12-16 03:46:28.344325: step 1790, loss = 0.47, batch loss = 0.41 (29.7 examples/sec; 0.269 sec/batch; 24h:44m:55s remains)
INFO - root - 2017-12-16 03:46:31.097668: step 1800, loss = 0.32, batch loss = 0.26 (28.9 examples/sec; 0.277 sec/batch; 25h:28m:08s remains)
2017-12-16 03:46:31.524796: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5637007 -3.5341315 -3.60118 -3.7520862 -3.8949642 -3.9805944 -3.9985967 -3.9954295 -3.9534652 -3.888624 -3.8301773 -3.7981904 -3.7603216 -3.7328119 -3.7031224][-3.6698728 -3.5876439 -3.6497164 -3.9209731 -4.1758723 -4.3784623 -4.4292946 -4.4120488 -4.3252974 -4.2316647 -4.1551843 -4.0582366 -3.9857132 -3.9559186 -3.9085791][-3.6286569 -3.4572976 -3.462878 -3.6881433 -3.9383447 -4.1866636 -4.268908 -4.3395348 -4.2801571 -4.1052527 -3.9846065 -3.9136615 -3.9048538 -3.8850384 -3.929976][-3.3584774 -3.01589 -2.8991838 -2.9963877 -3.2087069 -3.3819451 -3.3890119 -3.4362857 -3.3947399 -3.286402 -3.189959 -3.1647615 -3.2360203 -3.3150053 -3.5369253][-2.9066639 -2.3128576 -1.9877925 -1.9026699 -1.9657474 -2.0063415 -1.9008222 -1.8881812 -1.8614237 -1.8510344 -1.8754923 -2.0217009 -2.2549469 -2.4224622 -2.8001332][-2.3073094 -1.4720321 -0.91399574 -0.639987 -0.50647855 -0.43297005 -0.309052 -0.20383501 -0.1233201 -0.18389654 -0.40342188 -0.762022 -1.1927242 -1.5231986 -2.0385373][-1.6052444 -0.65794754 0.00579834 0.42951632 0.67374563 0.86200953 0.96708679 1.0388227 1.146534 1.0950193 0.77413559 0.20582438 -0.41266966 -0.92865396 -1.5474133][-1.0623558 -0.079982758 0.60478115 1.1364932 1.4373426 1.5604448 1.5161705 1.5527215 1.6615868 1.5685863 1.2122922 0.61199 -0.04114151 -0.68006516 -1.3556964][-0.91456151 0.018865108 0.62324524 1.0986338 1.3075876 1.3202791 1.1042609 1.0773139 1.1602182 1.0583401 0.8431282 0.39901543 -0.11697578 -0.737489 -1.3926306][-1.1393676 -0.30845833 0.14615297 0.54965687 0.68831968 0.46817255 0.020683289 -0.069571495 0.011296749 -0.059117317 -0.14435911 -0.34237909 -0.65255857 -1.1731229 -1.7979338][-1.7742136 -1.0362492 -0.62716913 -0.35717916 -0.37758541 -0.68307638 -1.2070174 -1.3721056 -1.3061395 -1.2831693 -1.2929738 -1.3471808 -1.4856343 -1.8476624 -2.3733609][-2.529995 -1.8934655 -1.5683792 -1.3280942 -1.4231791 -1.8371863 -2.4173918 -2.5755033 -2.5181119 -2.4594498 -2.389704 -2.3191707 -2.3687861 -2.6578453 -3.0976629][-3.39993 -2.7481332 -2.36899 -2.1782682 -2.3184509 -2.71467 -3.2353706 -3.414619 -3.379436 -3.242363 -3.1044445 -3.0192647 -3.090215 -3.3644454 -3.797981][-4.2202048 -3.5450029 -3.1061502 -2.8517506 -2.9174812 -3.2120676 -3.6171691 -3.7578809 -3.7208602 -3.6147833 -3.5750506 -3.5890992 -3.730917 -4.0408497 -4.5114012][-4.9316387 -4.2148609 -3.7345257 -3.4750888 -3.4651525 -3.6233039 -3.8791265 -3.9873817 -3.9884512 -3.9634717 -4.0733523 -4.3017478 -4.5700932 -4.8657656 -5.2719889]]...]
INFO - root - 2017-12-16 03:46:34.303207: step 1810, loss = 0.39, batch loss = 0.33 (29.5 examples/sec; 0.271 sec/batch; 24h:52m:11s remains)
INFO - root - 2017-12-16 03:46:37.099466: step 1820, loss = 0.48, batch loss = 0.43 (27.6 examples/sec; 0.289 sec/batch; 26h:34m:37s remains)
INFO - root - 2017-12-16 03:46:39.910735: step 1830, loss = 0.34, batch loss = 0.28 (28.7 examples/sec; 0.278 sec/batch; 25h:34m:44s remains)
INFO - root - 2017-12-16 03:46:42.720455: step 1840, loss = 0.27, batch loss = 0.21 (29.0 examples/sec; 0.276 sec/batch; 25h:20m:41s remains)
INFO - root - 2017-12-16 03:46:45.497844: step 1850, loss = 0.31, batch loss = 0.25 (28.7 examples/sec; 0.279 sec/batch; 25h:36m:47s remains)
INFO - root - 2017-12-16 03:46:48.296296: step 1860, loss = 0.31, batch loss = 0.25 (28.4 examples/sec; 0.281 sec/batch; 25h:50m:44s remains)
INFO - root - 2017-12-16 03:46:51.114045: step 1870, loss = 0.42, batch loss = 0.36 (29.5 examples/sec; 0.271 sec/batch; 24h:54m:11s remains)
INFO - root - 2017-12-16 03:46:53.941823: step 1880, loss = 0.33, batch loss = 0.27 (27.0 examples/sec; 0.296 sec/batch; 27h:09m:53s remains)
INFO - root - 2017-12-16 03:46:56.732567: step 1890, loss = 0.46, batch loss = 0.41 (28.1 examples/sec; 0.284 sec/batch; 26h:06m:29s remains)
INFO - root - 2017-12-16 03:46:59.592438: step 1900, loss = 0.43, batch loss = 0.37 (25.8 examples/sec; 0.311 sec/batch; 28h:31m:01s remains)
2017-12-16 03:47:00.049538: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6026487 -2.832603 -3.352941 -3.9779825 -4.5660582 -4.9283376 -5.0258961 -4.942378 -4.6849694 -4.4075203 -4.0271587 -3.471014 -2.8058281 -2.2691708 -2.0026081][-2.5551989 -2.9613023 -3.6562307 -4.2826052 -4.8101296 -5.1775761 -5.264544 -5.1869178 -5.0150895 -4.8616481 -4.5906582 -4.1205783 -3.4974232 -2.8296738 -2.4155285][-2.5554724 -3.1028097 -3.8460138 -4.4482317 -4.829227 -4.9297633 -4.7691617 -4.5878091 -4.5359273 -4.6466441 -4.837533 -4.7342057 -4.2850475 -3.6735749 -3.0643578][-2.8664629 -3.3923156 -4.0727625 -4.4638195 -4.4552126 -4.13378 -3.6009934 -3.2406135 -3.2828112 -3.7715254 -4.4756269 -4.8955641 -4.7773371 -4.2496786 -3.554971][-3.1251044 -3.6371074 -4.1635447 -4.2157664 -3.6709983 -2.666697 -1.6647327 -1.1249328 -1.3839846 -2.2645173 -3.434227 -4.3138828 -4.5341878 -4.1664252 -3.4625216][-3.2517815 -3.6910889 -3.9596114 -3.5750325 -2.4232166 -0.80969882 0.48688316 1.078845 0.71534681 -0.50966406 -2.0998425 -3.3821621 -3.9200578 -3.7368658 -3.0843229][-3.2959027 -3.592437 -3.589344 -2.7627492 -1.1979625 0.73722363 2.2176967 2.7534609 2.1865253 0.81426 -0.93660212 -2.4681857 -3.2132988 -3.2550519 -2.7467113][-3.1961179 -3.3502507 -3.0518384 -1.93255 -0.21140528 1.651423 2.8973179 3.2626109 2.5843239 0.94700861 -0.90280151 -2.368119 -3.0477684 -3.0643063 -2.5565262][-3.3568408 -3.561537 -3.1862764 -2.0361748 -0.41723824 1.2682185 2.3413692 2.5214629 1.8210206 0.29672813 -1.4734099 -2.8258214 -3.3252873 -3.2226026 -2.5391631][-3.6696682 -4.0324383 -3.9247985 -3.1779099 -2.0104458 -0.74022341 0.16797256 0.3980298 -0.14596224 -1.453589 -2.74439 -3.7027071 -4.0073495 -3.6358218 -2.8400097][-4.4414134 -4.8592782 -4.9652023 -4.6374569 -4.0272193 -3.2533357 -2.6257312 -2.4997418 -2.8686631 -3.6793537 -4.5099115 -4.9567022 -4.8803291 -4.2995114 -3.4098737][-5.095274 -5.7718077 -6.2456441 -6.2557182 -5.9200864 -5.4855137 -5.1524186 -5.043313 -5.2389545 -5.6193566 -5.9330378 -6.0128341 -5.7175794 -4.92615 -3.9670596][-5.3963537 -6.0831575 -6.74011 -7.1231003 -7.2846317 -7.1928005 -6.9061422 -6.7027645 -6.5850477 -6.6215553 -6.6420755 -6.4107275 -5.9624205 -5.2457819 -4.4192567][-5.2341566 -5.7125859 -6.2260337 -6.6578026 -6.9629784 -7.1368628 -7.2234039 -7.2049847 -7.0201588 -6.7629633 -6.4596577 -6.0967627 -5.6519895 -5.0859275 -4.4542713][-4.7419839 -5.0553684 -5.3846736 -5.6940069 -5.9300303 -6.1241341 -6.23676 -6.2093043 -6.0835981 -5.8946843 -5.6695924 -5.3189569 -4.9251423 -4.4959121 -4.0733209]]...]
INFO - root - 2017-12-16 03:47:02.846072: step 1910, loss = 0.30, batch loss = 0.24 (29.7 examples/sec; 0.269 sec/batch; 24h:44m:14s remains)
INFO - root - 2017-12-16 03:47:05.676179: step 1920, loss = 0.40, batch loss = 0.35 (28.0 examples/sec; 0.286 sec/batch; 26h:13m:02s remains)
INFO - root - 2017-12-16 03:47:08.485011: step 1930, loss = 0.33, batch loss = 0.27 (28.7 examples/sec; 0.279 sec/batch; 25h:34m:34s remains)
INFO - root - 2017-12-16 03:47:11.341038: step 1940, loss = 0.27, batch loss = 0.21 (27.1 examples/sec; 0.295 sec/batch; 27h:04m:08s remains)
INFO - root - 2017-12-16 03:47:14.138330: step 1950, loss = 0.32, batch loss = 0.26 (28.6 examples/sec; 0.280 sec/batch; 25h:42m:31s remains)
INFO - root - 2017-12-16 03:47:16.939376: step 1960, loss = 0.29, batch loss = 0.23 (27.6 examples/sec; 0.290 sec/batch; 26h:36m:39s remains)
INFO - root - 2017-12-16 03:47:19.741983: step 1970, loss = 0.27, batch loss = 0.22 (29.4 examples/sec; 0.272 sec/batch; 24h:59m:52s remains)
INFO - root - 2017-12-16 03:47:22.563156: step 1980, loss = 0.37, batch loss = 0.31 (29.0 examples/sec; 0.276 sec/batch; 25h:20m:01s remains)
INFO - root - 2017-12-16 03:47:25.393383: step 1990, loss = 0.42, batch loss = 0.36 (28.6 examples/sec; 0.280 sec/batch; 25h:43m:03s remains)
INFO - root - 2017-12-16 03:47:28.177336: step 2000, loss = 0.39, batch loss = 0.33 (29.7 examples/sec; 0.269 sec/batch; 24h:42m:56s remains)
2017-12-16 03:47:28.627677: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8518553 -3.689182 -3.5831375 -3.6033893 -3.6413074 -3.6493423 -3.5796304 -3.4661407 -3.3626137 -3.2661469 -3.2212934 -3.1546159 -3.107317 -3.0944812 -3.0827284][-3.1756124 -3.0164952 -2.9280291 -2.9648981 -2.9470382 -2.9149659 -2.8603258 -2.8144464 -2.8406758 -2.9517732 -3.1016006 -3.1734223 -3.2219203 -3.2802815 -3.2710037][-2.1752555 -2.135392 -2.1523671 -2.2362392 -2.2079151 -2.1670887 -2.1117005 -2.1021507 -2.2779167 -2.6138844 -3.0090122 -3.3378549 -3.5583 -3.6823387 -3.7023][-1.2040296 -1.2766361 -1.4543009 -1.6222904 -1.61907 -1.5730443 -1.5352137 -1.6090667 -1.9136798 -2.4212866 -3.0092397 -3.5139878 -3.8708768 -4.0811458 -4.1218848][-0.5412097 -0.69154453 -0.9559803 -1.233295 -1.2687781 -1.2582135 -1.2252614 -1.2947125 -1.6012497 -2.1479504 -2.85606 -3.4622738 -3.9324551 -4.223783 -4.2998714][-0.37518978 -0.45945239 -0.74933362 -1.0473311 -1.10547 -1.1386917 -1.1036294 -1.1172879 -1.3423352 -1.8350334 -2.5452859 -3.2191267 -3.7854967 -4.1029639 -4.2020535][-0.67164326 -0.62270045 -0.86382937 -1.0898747 -1.1048038 -1.0657468 -0.98364019 -1.0030117 -1.1629186 -1.5248289 -2.1621521 -2.8384404 -3.4678452 -3.7948825 -3.9098544][-1.525861 -1.3725901 -1.4251711 -1.510891 -1.4209013 -1.2320602 -1.012361 -0.99782825 -1.1047249 -1.4025438 -1.9985671 -2.6469045 -3.23388 -3.5408516 -3.6455352][-2.6497664 -2.51295 -2.5008416 -2.4616871 -2.2729383 -2.0177596 -1.7494044 -1.6125872 -1.5996106 -1.799319 -2.2518671 -2.8233051 -3.360743 -3.6184318 -3.6969938][-3.7217054 -3.64895 -3.6209259 -3.5095453 -3.2551751 -2.9241021 -2.6213756 -2.5302818 -2.5641239 -2.7253928 -3.0964313 -3.5531447 -3.9822941 -4.1546535 -4.12765][-4.3472939 -4.4200654 -4.4414611 -4.3202925 -4.0812678 -3.7676044 -3.4795165 -3.3851869 -3.4482315 -3.6190534 -3.9312227 -4.3072267 -4.6154103 -4.7272863 -4.653645][-4.28552 -4.4574432 -4.5948381 -4.582809 -4.4861355 -4.2867279 -4.0658493 -4.0019035 -4.0691414 -4.2578845 -4.4817348 -4.7175283 -4.9044933 -4.9542131 -4.8828783][-3.7443752 -3.862834 -4.0035586 -4.0684104 -4.1112285 -4.0573606 -3.960716 -3.946012 -3.9970276 -4.1916347 -4.3759561 -4.4731131 -4.5481138 -4.5859437 -4.6021047][-2.925005 -2.8592863 -2.9231477 -3.0153437 -3.13886 -3.1686261 -3.0980251 -3.0513511 -3.0527434 -3.2056375 -3.3653111 -3.4507205 -3.5408726 -3.6140246 -3.7600164][-1.9037287 -1.6090457 -1.5699422 -1.7272899 -1.9345393 -2.0256956 -1.9866526 -1.9004033 -1.8764896 -1.9706213 -2.0690441 -2.1315856 -2.2555656 -2.4002392 -2.6523614]]...]
INFO - root - 2017-12-16 03:47:31.445225: step 2010, loss = 0.29, batch loss = 0.23 (29.2 examples/sec; 0.274 sec/batch; 25h:07m:39s remains)
INFO - root - 2017-12-16 03:47:34.274505: step 2020, loss = 0.28, batch loss = 0.22 (27.9 examples/sec; 0.287 sec/batch; 26h:18m:30s remains)
INFO - root - 2017-12-16 03:47:37.088702: step 2030, loss = 0.35, batch loss = 0.29 (28.8 examples/sec; 0.278 sec/batch; 25h:30m:37s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 03:47:39.908060: step 2040, loss = 0.23, batch loss = 0.17 (29.4 examples/sec; 0.272 sec/batch; 24h:59m:58s remains)
INFO - root - 2017-12-16 03:47:42.688832: step 2050, loss = 0.30, batch loss = 0.25 (28.5 examples/sec; 0.280 sec/batch; 25h:43m:34s remains)
INFO - root - 2017-12-16 03:47:45.515584: step 2060, loss = 0.35, batch loss = 0.30 (28.4 examples/sec; 0.282 sec/batch; 25h:52m:19s remains)
INFO - root - 2017-12-16 03:47:48.375470: step 2070, loss = 0.38, batch loss = 0.32 (28.1 examples/sec; 0.285 sec/batch; 26h:09m:49s remains)
INFO - root - 2017-12-16 03:47:51.148006: step 2080, loss = 0.28, batch loss = 0.22 (26.1 examples/sec; 0.307 sec/batch; 28h:08m:15s remains)
INFO - root - 2017-12-16 03:47:54.021430: step 2090, loss = 0.39, batch loss = 0.33 (27.5 examples/sec; 0.291 sec/batch; 26h:40m:36s remains)
INFO - root - 2017-12-16 03:47:56.791908: step 2100, loss = 0.27, batch loss = 0.21 (28.7 examples/sec; 0.279 sec/batch; 25h:34m:47s remains)
2017-12-16 03:47:57.254780: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6574259 -3.4916255 -3.399549 -3.5052886 -3.8581922 -4.4196887 -4.8781295 -5.0483422 -4.8750062 -4.5877237 -4.4333076 -4.4171996 -4.6509218 -5.0770526 -5.5064311][-3.113863 -2.95919 -2.7424316 -2.7418563 -3.0603435 -3.5856054 -4.0595121 -4.3361354 -4.3534069 -4.2176361 -4.1027451 -4.189877 -4.5580106 -5.0220137 -5.54924][-2.451014 -2.2198532 -1.9249656 -1.7758629 -1.8838665 -2.1897206 -2.6216283 -2.9821963 -3.1529436 -3.2487965 -3.4453547 -3.7267902 -4.1616573 -4.6571355 -5.189003][-1.8644598 -1.4546983 -0.96469855 -0.67526436 -0.75475407 -1.0063577 -1.3491516 -1.6105499 -1.8607757 -2.079736 -2.5026307 -3.0726473 -3.6538739 -4.1921244 -4.7610021][-1.4536018 -0.93051958 -0.36924076 0.096383572 0.23375845 0.23664808 0.15271854 0.01058197 -0.23361874 -0.6251471 -1.2878458 -2.104532 -2.9267228 -3.62486 -4.2722406][-1.2923679 -0.64858413 0.089517593 0.72286892 1.0804219 1.312326 1.5415983 1.5877833 1.4823751 1.0507665 0.21772289 -0.822819 -1.945189 -2.8909974 -3.7412009][-1.4992571 -0.8024261 0.0635438 0.94665051 1.7315183 2.2463956 2.6372461 2.8077435 2.6981416 2.0777068 1.0640759 -0.017607689 -1.1252155 -2.1583924 -3.1759515][-2.0817897 -1.4868376 -0.69843483 0.17108488 1.173737 2.1057725 2.8926005 3.2492666 3.1951456 2.5543752 1.3036404 0.030646324 -1.0853083 -2.0741849 -3.0517917][-2.8745527 -2.4927349 -1.9038723 -1.1601853 -0.05821991 1.0501356 2.1651182 3.0125771 3.182775 2.6399074 1.2428331 -0.2475214 -1.3904967 -2.359911 -3.2921994][-3.8658311 -3.7754009 -3.4531045 -2.8640251 -1.7111521 -0.4411056 0.72207403 1.8347006 2.3136077 1.9897118 0.63843393 -0.92108035 -2.1204655 -3.0869012 -3.8966773][-5.0037313 -5.1991634 -5.1819296 -4.7960734 -3.642396 -2.2291298 -1.1260648 -0.081439018 0.44768047 0.26068974 -0.89523745 -2.340642 -3.3240767 -4.1140947 -4.7256789][-6.4252496 -6.8506203 -7.0484586 -6.82198 -5.8399563 -4.4255838 -3.3061914 -2.3601823 -1.9456604 -2.1533616 -3.0949812 -4.2748661 -5.01374 -5.4969826 -5.7351646][-7.5993118 -8.236165 -8.6571007 -8.5765829 -7.8629627 -6.688899 -5.5848713 -4.619585 -4.1420608 -4.24539 -4.9602633 -5.8951597 -6.4478822 -6.716249 -6.65629][-8.0891161 -8.7027378 -9.1719055 -9.2450523 -8.8243885 -8.0639753 -7.2820382 -6.5404353 -6.0783081 -6.0522022 -6.5162764 -7.1040163 -7.457768 -7.4906697 -7.1741648][-7.8859577 -8.4038143 -8.7520018 -8.8758564 -8.7525349 -8.4499722 -8.1208992 -7.8655744 -7.6662278 -7.6030817 -7.7509155 -7.887289 -7.903326 -7.73292 -7.2543025]]...]
INFO - root - 2017-12-16 03:48:00.090537: step 2110, loss = 0.44, batch loss = 0.38 (29.0 examples/sec; 0.276 sec/batch; 25h:17m:49s remains)
INFO - root - 2017-12-16 03:48:02.920665: step 2120, loss = 0.45, batch loss = 0.40 (28.7 examples/sec; 0.279 sec/batch; 25h:37m:01s remains)
INFO - root - 2017-12-16 03:48:05.770124: step 2130, loss = 0.36, batch loss = 0.30 (27.2 examples/sec; 0.294 sec/batch; 26h:57m:42s remains)
INFO - root - 2017-12-16 03:48:08.611332: step 2140, loss = 0.31, batch loss = 0.26 (28.1 examples/sec; 0.284 sec/batch; 26h:05m:37s remains)
INFO - root - 2017-12-16 03:48:11.384962: step 2150, loss = 0.29, batch loss = 0.23 (29.5 examples/sec; 0.271 sec/batch; 24h:52m:34s remains)
INFO - root - 2017-12-16 03:48:14.151996: step 2160, loss = 0.29, batch loss = 0.23 (28.5 examples/sec; 0.281 sec/batch; 25h:47m:45s remains)
INFO - root - 2017-12-16 03:48:16.987617: step 2170, loss = 0.36, batch loss = 0.30 (27.5 examples/sec; 0.291 sec/batch; 26h:40m:11s remains)
INFO - root - 2017-12-16 03:48:19.801322: step 2180, loss = 0.30, batch loss = 0.25 (28.8 examples/sec; 0.278 sec/batch; 25h:31m:27s remains)
INFO - root - 2017-12-16 03:48:22.621818: step 2190, loss = 0.34, batch loss = 0.28 (26.2 examples/sec; 0.305 sec/batch; 27h:59m:58s remains)
INFO - root - 2017-12-16 03:48:25.490215: step 2200, loss = 0.33, batch loss = 0.27 (28.6 examples/sec; 0.280 sec/batch; 25h:42m:31s remains)
2017-12-16 03:48:25.947647: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9689155 -4.2436428 -4.4364033 -4.5505776 -4.6062093 -4.6574378 -4.7439356 -4.9152436 -5.0778 -5.1633568 -5.1353841 -5.0187306 -4.8356328 -4.6154881 -4.4626608][-4.14434 -4.3907895 -4.5298848 -4.5761633 -4.6167669 -4.706007 -4.8599634 -5.1407623 -5.3929734 -5.5810423 -5.6524997 -5.5196114 -5.3311796 -5.1168618 -4.9480109][-4.1802907 -4.3153839 -4.3002462 -4.1474957 -4.0493121 -4.0893707 -4.2471671 -4.5684333 -4.9123077 -5.2665 -5.5306854 -5.6005826 -5.5719204 -5.4757733 -5.3927732][-4.052279 -4.0320306 -3.8029215 -3.3979788 -3.0125906 -2.7764373 -2.7571802 -3.0393853 -3.4472218 -3.9980724 -4.526515 -4.9458785 -5.2235723 -5.4268284 -5.5875406][-3.8784809 -3.6620502 -3.174592 -2.4505172 -1.603296 -0.91691256 -0.53155088 -0.5562706 -0.97298026 -1.7943611 -2.7377014 -3.6039319 -4.3320742 -4.9618278 -5.4394922][-3.6850293 -3.3643484 -2.6774356 -1.6029038 -0.26337051 0.85810232 1.6392045 1.8944011 1.5772662 0.62512779 -0.67262411 -1.991822 -3.2145238 -4.2915339 -5.0853853][-3.697238 -3.32473 -2.5517879 -1.1845386 0.55199718 2.1243072 3.2718663 3.705018 3.4251175 2.4269009 0.98594809 -0.63898015 -2.2338998 -3.6239612 -4.6729555][-4.0967283 -3.8294768 -3.0505433 -1.5871043 0.27910328 2.1067529 3.5053015 4.197485 4.1192522 3.230547 1.8197899 0.11678648 -1.6171691 -3.2006242 -4.3689418][-4.7608695 -4.7266016 -4.1077046 -2.8395913 -1.1467826 0.62917233 2.126842 3.0094638 3.1942549 2.6264138 1.5367899 0.06505537 -1.52403 -3.0653787 -4.2685623][-5.5713458 -5.6897726 -5.3800268 -4.5298905 -3.1874104 -1.7286096 -0.37517262 0.61496305 1.0429087 0.78276968 0.10281658 -1.0404987 -2.36636 -3.649961 -4.7039828][-6.1368456 -6.380899 -6.3547282 -5.9365797 -5.1754651 -4.1948743 -3.1049964 -2.2622933 -1.7085292 -1.6148577 -1.9577918 -2.7291794 -3.6689587 -4.7024941 -5.5442634][-6.258975 -6.4849005 -6.63675 -6.617857 -6.4022994 -5.9053869 -5.2391038 -4.6084304 -4.0953903 -3.8525929 -3.857554 -4.2360015 -4.8166628 -5.4924355 -6.047236][-5.7592297 -5.9488916 -6.1700859 -6.34565 -6.4640789 -6.4046307 -6.1566467 -5.7492046 -5.34806 -5.1033392 -4.9669733 -5.0727816 -5.3419642 -5.6958628 -5.973187][-4.876503 -4.9414105 -5.0777431 -5.303309 -5.5662255 -5.7167716 -5.7372065 -5.6827893 -5.5224943 -5.3043404 -5.1044183 -5.0608454 -5.0920372 -5.183867 -5.2505331][-3.9260349 -3.9060848 -3.9748623 -4.1131868 -4.3074174 -4.5473146 -4.7290735 -4.7655921 -4.7042627 -4.6681056 -4.6005888 -4.5139928 -4.425056 -4.3873429 -4.3247809]]...]
INFO - root - 2017-12-16 03:48:28.747837: step 2210, loss = 0.30, batch loss = 0.24 (29.2 examples/sec; 0.274 sec/batch; 25h:06m:23s remains)
INFO - root - 2017-12-16 03:48:31.569552: step 2220, loss = 0.32, batch loss = 0.26 (29.0 examples/sec; 0.276 sec/batch; 25h:19m:09s remains)
INFO - root - 2017-12-16 03:48:34.338018: step 2230, loss = 0.33, batch loss = 0.27 (29.7 examples/sec; 0.269 sec/batch; 24h:42m:54s remains)
INFO - root - 2017-12-16 03:48:37.085532: step 2240, loss = 0.35, batch loss = 0.29 (29.9 examples/sec; 0.268 sec/batch; 24h:33m:49s remains)
INFO - root - 2017-12-16 03:48:39.912492: step 2250, loss = 0.29, batch loss = 0.23 (28.7 examples/sec; 0.278 sec/batch; 25h:32m:40s remains)
INFO - root - 2017-12-16 03:48:42.754753: step 2260, loss = 0.33, batch loss = 0.27 (28.7 examples/sec; 0.279 sec/batch; 25h:35m:33s remains)
INFO - root - 2017-12-16 03:48:45.560682: step 2270, loss = 0.33, batch loss = 0.27 (28.6 examples/sec; 0.280 sec/batch; 25h:42m:13s remains)
INFO - root - 2017-12-16 03:48:48.363178: step 2280, loss = 0.30, batch loss = 0.24 (29.5 examples/sec; 0.271 sec/batch; 24h:53m:35s remains)
INFO - root - 2017-12-16 03:48:51.168361: step 2290, loss = 0.55, batch loss = 0.49 (28.4 examples/sec; 0.282 sec/batch; 25h:49m:46s remains)
INFO - root - 2017-12-16 03:48:53.980008: step 2300, loss = 0.39, batch loss = 0.33 (28.2 examples/sec; 0.284 sec/batch; 26h:02m:29s remains)
2017-12-16 03:48:54.454250: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4131851 -2.1757612 -2.1099112 -2.3730335 -2.89966 -3.5683746 -4.1184797 -4.7640619 -5.2778807 -5.5064406 -5.6722641 -5.788137 -5.8140931 -5.6444588 -5.4282479][-1.7843928 -1.5287218 -1.561959 -1.9698811 -2.5673823 -3.429904 -4.2336354 -4.9239993 -5.4775448 -5.7784176 -5.8869882 -5.9213209 -5.962801 -5.7924023 -5.4541855][-1.2600241 -1.0859728 -1.2120647 -1.7307513 -2.3828623 -3.2376173 -4.0315018 -4.7728724 -5.2325568 -5.4220376 -5.58377 -5.6467566 -5.5858879 -5.3801875 -5.1013141][-1.1161389 -0.99034476 -1.1745174 -1.6178036 -2.2046771 -2.9368563 -3.5135818 -4.0355616 -4.4388337 -4.7114706 -4.8610067 -4.8614297 -4.8573704 -4.61075 -4.2095442][-1.4514937 -1.2078552 -1.2197587 -1.500355 -1.8584549 -2.2699792 -2.6178317 -2.9904258 -3.2691123 -3.5205476 -3.701602 -3.7392557 -3.7138708 -3.486414 -3.1152754][-2.0057287 -1.5434563 -1.3163426 -1.338413 -1.4041462 -1.6267776 -1.7659431 -2.0047185 -2.223717 -2.3507576 -2.4479818 -2.4886084 -2.4751787 -2.1911647 -1.7807095][-2.8002181 -2.1312571 -1.7201331 -1.4250984 -1.1362247 -1.0263646 -0.94976258 -1.1105735 -1.3229103 -1.4997869 -1.6390104 -1.5691507 -1.4069524 -1.0740407 -0.70042658][-3.5121269 -2.9230156 -2.5192909 -2.0951135 -1.6420016 -1.2328162 -0.87898612 -0.858341 -0.96139264 -1.1235807 -1.2180042 -1.1519964 -0.95620775 -0.51824427 -0.052531719][-4.1293316 -3.5974314 -3.2471638 -2.850379 -2.345474 -1.8435647 -1.4913836 -1.4020772 -1.4067297 -1.5643854 -1.6047347 -1.3956015 -1.1446531 -0.72671032 -0.19579983][-4.8768659 -4.40499 -4.0951672 -3.720823 -3.2445641 -2.7826877 -2.41131 -2.3379285 -2.4025338 -2.4885502 -2.4716644 -2.3040316 -2.059726 -1.5681267 -1.0150642][-5.180264 -4.8323464 -4.6472864 -4.3839312 -3.9809022 -3.6156113 -3.329031 -3.2763619 -3.3028536 -3.4077458 -3.4420776 -3.2521968 -2.9792776 -2.5137267 -1.9967525][-4.7743392 -4.6660452 -4.6840816 -4.6293406 -4.4893951 -4.2586136 -3.9823689 -3.8992476 -3.9151542 -3.9558926 -3.9446518 -3.8903556 -3.7376194 -3.3450255 -2.9765425][-3.9399705 -3.9517171 -4.1476059 -4.3342309 -4.4914865 -4.5410061 -4.4203205 -4.280961 -4.1684732 -4.1325736 -4.0604992 -4.0891256 -4.1680083 -4.0274968 -3.8219213][-3.2581196 -3.251863 -3.4533563 -3.745486 -4.003242 -4.1684351 -4.1307211 -4.0219154 -3.8332691 -3.6740572 -3.6399283 -3.77725 -4.0112457 -4.1445503 -4.2213364][-2.6490664 -2.6229348 -2.8699985 -3.196727 -3.4867852 -3.6718729 -3.6020746 -3.3669126 -3.0461917 -2.8322678 -2.7691407 -2.8548713 -3.1623929 -3.5160303 -3.8223238]]...]
INFO - root - 2017-12-16 03:48:57.238788: step 2310, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 25h:38m:19s remains)
INFO - root - 2017-12-16 03:49:00.042516: step 2320, loss = 0.45, batch loss = 0.39 (27.6 examples/sec; 0.290 sec/batch; 26h:37m:51s remains)
INFO - root - 2017-12-16 03:49:02.823723: step 2330, loss = 0.26, batch loss = 0.20 (29.0 examples/sec; 0.275 sec/batch; 25h:15m:58s remains)
INFO - root - 2017-12-16 03:49:05.657522: step 2340, loss = 0.33, batch loss = 0.27 (28.6 examples/sec; 0.279 sec/batch; 25h:37m:15s remains)
INFO - root - 2017-12-16 03:49:08.481538: step 2350, loss = 0.41, batch loss = 0.35 (28.2 examples/sec; 0.283 sec/batch; 25h:58m:20s remains)
INFO - root - 2017-12-16 03:49:11.255784: step 2360, loss = 0.28, batch loss = 0.22 (28.3 examples/sec; 0.283 sec/batch; 25h:57m:00s remains)
INFO - root - 2017-12-16 03:49:14.063667: step 2370, loss = 0.35, batch loss = 0.30 (28.9 examples/sec; 0.277 sec/batch; 25h:25m:21s remains)
INFO - root - 2017-12-16 03:49:16.916127: step 2380, loss = 0.33, batch loss = 0.27 (27.1 examples/sec; 0.295 sec/batch; 27h:02m:35s remains)
INFO - root - 2017-12-16 03:49:19.731579: step 2390, loss = 0.41, batch loss = 0.35 (28.8 examples/sec; 0.278 sec/batch; 25h:28m:09s remains)
INFO - root - 2017-12-16 03:49:22.561575: step 2400, loss = 0.37, batch loss = 0.31 (28.2 examples/sec; 0.284 sec/batch; 26h:03m:07s remains)
2017-12-16 03:49:23.085827: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4266872 -5.6714659 -5.9560513 -6.3416138 -6.7111626 -6.9742937 -7.2319779 -7.531086 -7.63281 -7.1745338 -6.2814312 -5.2971268 -4.4722438 -3.8930254 -3.5650711][-5.4826593 -5.6161261 -5.561511 -5.7027307 -5.9811234 -6.1882887 -6.445785 -6.7907734 -7.0790606 -6.8679457 -6.1532707 -5.2095222 -4.3209023 -3.7453651 -3.4427972][-5.4746218 -5.4178157 -5.0193195 -4.789505 -4.8207173 -4.9901567 -5.3646955 -5.7414575 -6.0260782 -5.9767036 -5.5938473 -4.955142 -4.2572122 -3.7345457 -3.4600248][-5.4076967 -5.1815572 -4.3174849 -3.5190616 -3.1806562 -3.130301 -3.4789953 -4.0329108 -4.5696568 -4.757009 -4.5626545 -4.2831907 -3.9734716 -3.711061 -3.5291235][-5.4546075 -5.0606804 -3.9788387 -2.6679916 -1.7061419 -1.2788899 -1.5160687 -2.1266332 -2.857765 -3.3452721 -3.5844245 -3.5742774 -3.5512428 -3.535995 -3.5197134][-5.6320887 -5.0866437 -3.8601382 -2.4035583 -1.213264 -0.4633913 -0.41156721 -0.89491415 -1.6742928 -2.40648 -2.8310924 -3.0546665 -3.2044072 -3.3277643 -3.373426][-5.4431367 -4.9691753 -3.8544798 -2.5250318 -1.1994464 -0.30936337 -0.060891628 -0.32082939 -0.90046 -1.5336618 -2.1369958 -2.5841751 -2.9159946 -3.1320028 -3.2466879][-5.0068474 -4.6510773 -3.7376282 -2.6310802 -1.5290985 -0.62200761 -0.10032749 -0.054774761 -0.38152695 -0.8270731 -1.3154304 -1.8412101 -2.3822238 -2.7907677 -3.005461][-4.4810057 -4.3545351 -3.6720676 -2.7958126 -1.8906047 -1.0250182 -0.48789787 -0.205513 -0.22899389 -0.51823068 -0.86846447 -1.3352225 -1.8651795 -2.3655429 -2.6928945][-4.1148872 -4.2824173 -4.0165262 -3.4636295 -2.7841716 -1.9656618 -1.3671808 -0.99479342 -0.83103561 -0.89757156 -1.0943048 -1.4368298 -1.8429258 -2.2437763 -2.5527635][-3.8898449 -4.076139 -3.9370825 -3.7430391 -3.4921911 -2.9623852 -2.4450536 -2.0699818 -1.8398645 -1.8288627 -1.9010959 -2.0209291 -2.1723893 -2.3843637 -2.5516067][-3.758754 -3.8493052 -3.7397964 -3.6441133 -3.6201057 -3.4275398 -3.1718934 -2.9567313 -2.7901254 -2.7132268 -2.6646903 -2.7454352 -2.7946067 -2.8477502 -2.8641052][-3.6935472 -3.6083257 -3.4040856 -3.2937562 -3.385747 -3.456002 -3.4602034 -3.4739761 -3.4572432 -3.4490914 -3.3944254 -3.3033798 -3.2002084 -3.1635895 -3.1164107][-3.6828585 -3.3715453 -3.0682416 -2.9196677 -3.0171988 -3.1607075 -3.3305588 -3.5192895 -3.7207177 -3.8428118 -3.7935534 -3.693949 -3.5165567 -3.3874564 -3.2739649][-3.9364882 -3.4546821 -2.9911649 -2.6845403 -2.6442323 -2.7222977 -2.8931217 -3.0595264 -3.308485 -3.4808426 -3.5518885 -3.5710988 -3.5180569 -3.4563792 -3.3902767]]...]
INFO - root - 2017-12-16 03:49:25.917204: step 2410, loss = 0.33, batch loss = 0.27 (28.9 examples/sec; 0.277 sec/batch; 25h:23m:27s remains)
INFO - root - 2017-12-16 03:49:28.755534: step 2420, loss = 0.29, batch loss = 0.23 (29.0 examples/sec; 0.276 sec/batch; 25h:18m:53s remains)
INFO - root - 2017-12-16 03:49:31.598822: step 2430, loss = 0.31, batch loss = 0.25 (28.6 examples/sec; 0.280 sec/batch; 25h:40m:43s remains)
INFO - root - 2017-12-16 03:49:34.370632: step 2440, loss = 0.39, batch loss = 0.33 (28.9 examples/sec; 0.277 sec/batch; 25h:21m:23s remains)
INFO - root - 2017-12-16 03:49:37.189290: step 2450, loss = 0.24, batch loss = 0.19 (28.9 examples/sec; 0.277 sec/batch; 25h:23m:09s remains)
INFO - root - 2017-12-16 03:49:39.979137: step 2460, loss = 0.28, batch loss = 0.22 (28.9 examples/sec; 0.276 sec/batch; 25h:20m:43s remains)
INFO - root - 2017-12-16 03:49:42.827547: step 2470, loss = 0.23, batch loss = 0.17 (29.1 examples/sec; 0.275 sec/batch; 25h:13m:07s remains)
INFO - root - 2017-12-16 03:49:45.641024: step 2480, loss = 0.38, batch loss = 0.32 (27.4 examples/sec; 0.292 sec/batch; 26h:44m:48s remains)
INFO - root - 2017-12-16 03:49:48.530622: step 2490, loss = 0.31, batch loss = 0.25 (28.4 examples/sec; 0.282 sec/batch; 25h:48m:19s remains)
INFO - root - 2017-12-16 03:49:51.284357: step 2500, loss = 0.27, batch loss = 0.21 (29.5 examples/sec; 0.271 sec/batch; 24h:50m:31s remains)
2017-12-16 03:49:51.742211: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0405693 -3.9155846 -3.7502046 -3.5735714 -3.4461679 -3.3013954 -3.2469583 -3.3262348 -3.2902694 -3.2035215 -3.05439 -2.8723998 -2.6771121 -2.516891 -2.3470297][-3.8774505 -3.7643595 -3.6692646 -3.5540681 -3.4872894 -3.4088387 -3.3672094 -3.34964 -3.2677898 -3.169137 -3.0473056 -3.0149965 -2.9599149 -2.8774457 -2.7128651][-3.5784264 -3.4902792 -3.4920311 -3.5022893 -3.5340595 -3.493046 -3.4437542 -3.3929844 -3.2869997 -3.2277493 -3.2111144 -3.2441139 -3.3160584 -3.3314641 -3.2156963][-3.115834 -3.09688 -3.2427814 -3.3912311 -3.4635253 -3.3439507 -3.1724176 -3.0396087 -2.9051621 -2.910742 -2.9885464 -3.1945934 -3.4389255 -3.5611191 -3.5724144][-2.4118578 -2.5126281 -2.7937374 -3.0016067 -3.0362773 -2.8556881 -2.539248 -2.3064098 -2.2305956 -2.3426442 -2.5543137 -2.9048963 -3.2691524 -3.5034194 -3.5618086][-1.7120857 -1.9743779 -2.3353667 -2.5781698 -2.5051377 -2.159807 -1.6992321 -1.4004717 -1.3462327 -1.4929938 -1.8048279 -2.2734215 -2.7448893 -3.0763271 -3.193542][-1.2332218 -1.5492384 -1.8865387 -2.0538332 -1.8690894 -1.4025435 -0.86733246 -0.55191565 -0.47594738 -0.64621353 -1.0026662 -1.5350876 -2.0424457 -2.3670678 -2.467226][-1.0425799 -1.2233987 -1.3437824 -1.2527332 -0.89301515 -0.41785049 0.032657146 0.20961523 0.15564775 -0.15073633 -0.63328218 -1.1693537 -1.6416008 -1.9256706 -2.0043139][-1.1493022 -1.1227355 -0.99144316 -0.63433075 -0.043275356 0.44927025 0.738287 0.69656515 0.42851353 -0.019318581 -0.62732291 -1.2636502 -1.7483003 -1.9579294 -1.981554][-1.6061819 -1.3041077 -0.87130904 -0.31267548 0.39393377 0.85255766 0.93664074 0.6568923 0.13923407 -0.52879405 -1.2928281 -1.9119284 -2.3060141 -2.4147892 -2.3147676][-2.2754304 -1.7661543 -1.1031511 -0.34305573 0.42670107 0.7902441 0.6518836 0.12151957 -0.61798024 -1.3610661 -2.164108 -2.7001781 -2.9013023 -2.8870418 -2.6727443][-2.9248493 -2.2538335 -1.4521387 -0.58739686 0.19842291 0.41811085 0.061645508 -0.62109971 -1.4976747 -2.327199 -3.0918744 -3.5163229 -3.5586138 -3.3649619 -3.0375476][-3.433743 -2.571485 -1.6344545 -0.82975507 -0.22998381 -0.11236334 -0.5802114 -1.4233682 -2.3622448 -3.0973828 -3.7311385 -4.0092378 -3.8740449 -3.5503902 -3.1838417][-3.7725854 -2.8871017 -2.0201952 -1.3560042 -0.96245694 -1.0366211 -1.6059325 -2.4248962 -3.2352815 -3.746603 -4.1230512 -4.2307205 -3.9713809 -3.5003362 -3.0548787][-4.0560412 -3.2267284 -2.4819741 -2.0230644 -1.8982859 -2.0986903 -2.6651897 -3.3676162 -3.933372 -4.1835566 -4.3557529 -4.254703 -3.8597164 -3.3466029 -2.889888]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-2500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-2500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 03:49:54.918455: step 2510, loss = 0.43, batch loss = 0.37 (29.6 examples/sec; 0.271 sec/batch; 24h:48m:28s remains)
INFO - root - 2017-12-16 03:49:57.708225: step 2520, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 25h:53m:51s remains)
INFO - root - 2017-12-16 03:50:00.490172: step 2530, loss = 0.33, batch loss = 0.27 (29.0 examples/sec; 0.276 sec/batch; 25h:18m:04s remains)
INFO - root - 2017-12-16 03:50:03.322113: step 2540, loss = 0.27, batch loss = 0.21 (28.3 examples/sec; 0.283 sec/batch; 25h:56m:36s remains)
INFO - root - 2017-12-16 03:50:06.112219: step 2550, loss = 0.39, batch loss = 0.33 (27.7 examples/sec; 0.288 sec/batch; 26h:26m:15s remains)
INFO - root - 2017-12-16 03:50:08.901956: step 2560, loss = 0.26, batch loss = 0.21 (29.2 examples/sec; 0.274 sec/batch; 25h:07m:33s remains)
INFO - root - 2017-12-16 03:50:11.706787: step 2570, loss = 0.30, batch loss = 0.24 (29.2 examples/sec; 0.274 sec/batch; 25h:07m:11s remains)
INFO - root - 2017-12-16 03:50:14.497358: step 2580, loss = 0.46, batch loss = 0.41 (28.3 examples/sec; 0.282 sec/batch; 25h:52m:15s remains)
INFO - root - 2017-12-16 03:50:17.291081: step 2590, loss = 0.29, batch loss = 0.23 (28.6 examples/sec; 0.280 sec/batch; 25h:37m:25s remains)
INFO - root - 2017-12-16 03:50:20.078721: step 2600, loss = 0.31, batch loss = 0.25 (29.2 examples/sec; 0.274 sec/batch; 25h:07m:20s remains)
2017-12-16 03:50:20.554511: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8998775 -3.9429862 -3.7994149 -3.6183374 -3.5348983 -3.5665491 -3.6341665 -3.6331103 -3.59836 -3.5421035 -3.4920855 -3.4913123 -3.5409892 -3.5007546 -3.3352432][-3.5722671 -3.5565758 -3.4230781 -3.3162844 -3.3386219 -3.4155283 -3.5428967 -3.68465 -3.737123 -3.6987128 -3.6284795 -3.5760424 -3.510884 -3.2847512 -2.9097369][-2.9597154 -2.9390655 -2.9023473 -2.9685171 -3.118865 -3.3378961 -3.5679977 -3.7570362 -3.820152 -3.8410268 -3.8365993 -3.7830358 -3.6209188 -3.2826552 -2.7688308][-2.3827708 -2.4708338 -2.5995548 -2.8601575 -3.1184058 -3.3477588 -3.5045755 -3.6203418 -3.7636833 -3.92163 -4.0446167 -4.0520449 -3.8951788 -3.5557082 -2.9903603][-1.8016765 -1.9880822 -2.2682142 -2.6588774 -2.936832 -3.0063136 -2.9017019 -2.8650689 -3.0326254 -3.3143239 -3.6539817 -3.8696828 -3.9099047 -3.7365603 -3.2645895][-1.2555788 -1.5498834 -1.9440436 -2.2661555 -2.273351 -2.0499725 -1.6969297 -1.4727395 -1.6123474 -2.0216305 -2.6282306 -3.2086821 -3.6033123 -3.7405629 -3.5519862][-1.2672877 -1.5695159 -1.8204212 -1.8610642 -1.4969928 -0.89768076 -0.27582026 0.15970469 0.060473442 -0.49049163 -1.2992318 -2.2268238 -2.9915986 -3.5388803 -3.7857933][-1.9288628 -2.1662509 -2.1777976 -1.8037815 -0.99138975 0.036337852 0.99986887 1.6691632 1.6938705 1.004446 -0.037500381 -1.2615881 -2.4082649 -3.3524156 -3.903511][-2.8670146 -2.9658666 -2.7951667 -2.132576 -0.90640473 0.48963833 1.6997428 2.5061436 2.6111422 1.9413543 0.792315 -0.60577226 -1.9914446 -3.1661177 -3.8818748][-3.4764605 -3.5668955 -3.3233495 -2.5757616 -1.2986395 0.16885805 1.4549265 2.3207483 2.5395527 2.0263891 0.95264959 -0.3768425 -1.7785168 -3.0858593 -3.9386382][-4.0874109 -4.2735915 -4.0320077 -3.2939715 -2.0949574 -0.6785078 0.55240631 1.4304743 1.7605672 1.3589091 0.49124193 -0.61794496 -1.8254459 -2.9207318 -3.6844754][-4.5132384 -4.7582288 -4.6186957 -4.0644054 -3.0196488 -1.6866481 -0.46629906 0.43294144 0.81112957 0.57011127 -0.12502432 -1.0080614 -1.9302783 -2.7091141 -3.1509571][-4.7242775 -5.18499 -5.2704177 -4.9500933 -4.0827088 -2.8372059 -1.5421202 -0.51991248 -0.044846535 -0.10225821 -0.5206244 -1.1661146 -1.8327565 -2.3343399 -2.6056862][-4.896987 -5.4758177 -5.7149076 -5.6197391 -4.89009 -3.6966963 -2.3627381 -1.2259007 -0.59446168 -0.47924757 -0.69947386 -1.138943 -1.67118 -1.9909046 -2.2271767][-4.9350662 -5.5247507 -5.8612823 -5.8804779 -5.2746429 -4.1069894 -2.728785 -1.5142024 -0.78261828 -0.52432728 -0.61971736 -0.9855969 -1.4774241 -1.7078061 -1.9091883]]...]
INFO - root - 2017-12-16 03:50:23.343206: step 2610, loss = 0.31, batch loss = 0.25 (29.1 examples/sec; 0.275 sec/batch; 25h:11m:21s remains)
INFO - root - 2017-12-16 03:50:26.172073: step 2620, loss = 0.29, batch loss = 0.23 (27.2 examples/sec; 0.295 sec/batch; 26h:59m:38s remains)
INFO - root - 2017-12-16 03:50:28.993730: step 2630, loss = 0.30, batch loss = 0.24 (29.5 examples/sec; 0.271 sec/batch; 24h:50m:43s remains)
INFO - root - 2017-12-16 03:50:31.785652: step 2640, loss = 0.30, batch loss = 0.24 (29.4 examples/sec; 0.272 sec/batch; 24h:56m:19s remains)
INFO - root - 2017-12-16 03:50:34.609864: step 2650, loss = 0.40, batch loss = 0.34 (29.0 examples/sec; 0.276 sec/batch; 25h:16m:01s remains)
INFO - root - 2017-12-16 03:50:37.389032: step 2660, loss = 0.31, batch loss = 0.25 (27.9 examples/sec; 0.287 sec/batch; 26h:15m:03s remains)
INFO - root - 2017-12-16 03:50:40.218949: step 2670, loss = 0.30, batch loss = 0.24 (28.1 examples/sec; 0.284 sec/batch; 26h:03m:51s remains)
INFO - root - 2017-12-16 03:50:43.061115: step 2680, loss = 0.28, batch loss = 0.23 (28.4 examples/sec; 0.282 sec/batch; 25h:50m:19s remains)
INFO - root - 2017-12-16 03:50:45.858181: step 2690, loss = 0.32, batch loss = 0.26 (28.8 examples/sec; 0.278 sec/batch; 25h:25m:38s remains)
INFO - root - 2017-12-16 03:50:48.703811: step 2700, loss = 0.37, batch loss = 0.31 (28.2 examples/sec; 0.283 sec/batch; 25h:57m:05s remains)
2017-12-16 03:50:49.161258: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6166863 -2.7215714 -2.8192303 -2.8670831 -2.8775949 -2.7989173 -2.7047648 -2.6191378 -2.5811617 -2.7130947 -2.95612 -3.2888527 -3.6439924 -3.972002 -4.260004][-2.885268 -3.0692348 -3.2004194 -3.2394638 -3.1839352 -3.0154068 -2.8496988 -2.7193646 -2.5825932 -2.6621819 -2.9631026 -3.3772097 -3.8471627 -4.2510467 -4.6176991][-3.409956 -3.6360762 -3.7540007 -3.6951268 -3.4884908 -3.1361585 -2.8064435 -2.620769 -2.4616346 -2.4877958 -2.7058496 -3.1759648 -3.7281501 -4.2119064 -4.7232656][-4.228621 -4.3583307 -4.33843 -4.1412082 -3.755491 -3.1670036 -2.6118436 -2.2060344 -1.9127998 -1.9079928 -2.0600188 -2.427567 -3.0100203 -3.6340592 -4.3319511][-4.8299603 -4.7887583 -4.5310431 -4.1634274 -3.5817609 -2.8101285 -2.1110227 -1.5498569 -1.094425 -0.92147183 -0.98619938 -1.2598541 -1.8755562 -2.6633735 -3.626287][-4.9372106 -4.6236439 -4.1119647 -3.5297759 -2.7841153 -1.9498899 -1.2367585 -0.76263809 -0.38147879 -0.14507198 -0.14691639 -0.46718574 -1.1350353 -2.0012276 -3.1079292][-4.4772086 -3.8863926 -3.1173744 -2.3722129 -1.5739822 -0.81464338 -0.20502472 0.093096256 0.24067068 0.25893354 0.074178696 -0.34877872 -1.054117 -2.0169272 -3.1475434][-3.6729903 -2.8551049 -1.9209692 -1.0581961 -0.29229498 0.27062511 0.56929636 0.57155657 0.40398216 0.10488796 -0.26185751 -0.72418 -1.4210985 -2.434391 -3.5439682][-2.6644535 -1.7529471 -0.80512857 0.0742898 0.749279 1.1139779 1.1419129 0.74531221 0.15347481 -0.52763796 -1.1669967 -1.7433641 -2.4149282 -3.2796736 -4.208168][-1.5928049 -0.69561291 0.18270874 0.92203903 1.3733087 1.5109558 1.2312756 0.52598715 -0.41253471 -1.3600838 -2.150717 -2.8099902 -3.402318 -4.0709572 -4.7885661][-0.79989815 0.013409615 0.81058311 1.3575945 1.5816221 1.4934554 0.97912979 0.047201157 -1.0842013 -2.1625352 -2.9808264 -3.5718093 -4.0194736 -4.4763765 -4.9898286][-0.26463127 0.501019 1.0300508 1.3001919 1.3242116 1.03579 0.3499794 -0.5557332 -1.6315253 -2.5347517 -3.1690073 -3.5989647 -3.88447 -4.1973124 -4.5909023][-0.093002319 0.8058486 1.2954597 1.3402314 1.0615964 0.60966492 -0.20936537 -1.1406229 -2.0376418 -2.6945486 -2.9974248 -3.1371965 -3.2383227 -3.381464 -3.6963704][0.33212805 1.3110671 1.7400002 1.6540899 1.2015386 0.58237839 -0.25938416 -1.1937966 -1.9907796 -2.4139328 -2.5719237 -2.5933495 -2.5633225 -2.6025205 -2.8522522][0.75085068 1.783752 2.1831326 2.0444598 1.59266 0.81377172 -0.20129251 -1.1282527 -1.8567085 -2.2163002 -2.2725496 -2.2938778 -2.220742 -2.206738 -2.3902814]]...]
INFO - root - 2017-12-16 03:50:52.000616: step 2710, loss = 0.32, batch loss = 0.26 (28.1 examples/sec; 0.284 sec/batch; 26h:02m:25s remains)
INFO - root - 2017-12-16 03:50:54.809917: step 2720, loss = 0.48, batch loss = 0.42 (28.3 examples/sec; 0.282 sec/batch; 25h:51m:53s remains)
INFO - root - 2017-12-16 03:50:57.658496: step 2730, loss = 0.26, batch loss = 0.20 (26.9 examples/sec; 0.297 sec/batch; 27h:14m:47s remains)
INFO - root - 2017-12-16 03:51:00.480864: step 2740, loss = 0.27, batch loss = 0.21 (28.6 examples/sec; 0.280 sec/batch; 25h:36m:39s remains)
INFO - root - 2017-12-16 03:51:03.253207: step 2750, loss = 0.67, batch loss = 0.61 (28.2 examples/sec; 0.284 sec/batch; 25h:58m:35s remains)
INFO - root - 2017-12-16 03:51:06.037069: step 2760, loss = 0.23, batch loss = 0.18 (29.5 examples/sec; 0.271 sec/batch; 24h:48m:04s remains)
INFO - root - 2017-12-16 03:51:08.876217: step 2770, loss = 0.24, batch loss = 0.19 (27.5 examples/sec; 0.291 sec/batch; 26h:40m:53s remains)
INFO - root - 2017-12-16 03:51:11.731684: step 2780, loss = 0.28, batch loss = 0.22 (28.3 examples/sec; 0.282 sec/batch; 25h:51m:26s remains)
INFO - root - 2017-12-16 03:51:14.592395: step 2790, loss = 0.24, batch loss = 0.19 (28.1 examples/sec; 0.284 sec/batch; 26h:02m:07s remains)
INFO - root - 2017-12-16 03:51:17.406688: step 2800, loss = 0.27, batch loss = 0.21 (29.4 examples/sec; 0.272 sec/batch; 24h:56m:29s remains)
2017-12-16 03:51:17.864427: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.97699 -4.0594044 -4.1602554 -4.499701 -5.0053492 -5.4728518 -5.6971846 -5.7261081 -5.4422956 -4.6600928 -3.8094604 -3.1084442 -2.5181923 -2.1915228 -2.3535769][-3.7902274 -4.1060419 -4.4858518 -4.8956237 -5.3143921 -5.9005914 -6.3568182 -6.4607048 -6.24113 -5.5366931 -4.6741624 -3.9042852 -3.3039727 -2.9690545 -2.8578763][-3.5129485 -3.9736164 -4.5079503 -5.0804448 -5.6098976 -5.9709468 -6.2206931 -6.5251989 -6.6820984 -6.3627625 -5.8213105 -5.1366582 -4.5100994 -4.103539 -3.793509][-3.2440131 -3.6631384 -4.184885 -4.6020517 -4.9355731 -5.2857847 -5.5728416 -5.8742132 -6.2184381 -6.4479518 -6.4418793 -6.1107969 -5.6828666 -5.1192975 -4.6378293][-3.0083272 -3.2254772 -3.4312787 -3.5249777 -3.5650983 -3.5698483 -3.727288 -4.2049785 -4.9029164 -5.6003246 -6.0983934 -6.14791 -5.8841128 -5.4036512 -4.9080257][-2.5427613 -2.6995316 -2.7145174 -2.4510016 -2.04378 -1.7099476 -1.6805711 -2.1168067 -2.9283788 -3.958147 -4.8591957 -5.3892035 -5.5050545 -5.1228342 -4.6503181][-1.9323151 -1.9307756 -1.8882322 -1.4688275 -0.8124578 -0.20386553 0.053995609 -0.36243105 -1.2175395 -2.3520067 -3.3894424 -4.214591 -4.5900884 -4.4760761 -4.0981607][-1.4906888 -1.2946947 -1.0419784 -0.51357841 0.22936773 0.98779297 1.2801833 1.0381327 0.19292068 -1.2547369 -2.4635997 -3.3456919 -3.8325176 -3.9397547 -3.6416352][-1.3792913 -1.0708528 -0.66405535 -0.050971985 0.64577341 1.3693309 1.7424326 1.4860458 0.63383961 -0.72845721 -2.1072915 -3.2364569 -3.7629423 -3.7726214 -3.3770957][-1.4172843 -1.1058371 -0.76142144 -0.1651001 0.59412479 1.2129946 1.3033485 0.972116 0.16806412 -1.2149274 -2.5666575 -3.5910354 -4.0700631 -4.0480919 -3.455883][-1.7668478 -1.3492169 -0.92559123 -0.39728832 0.19148302 0.75792646 0.78785324 0.18432617 -0.830883 -2.1229939 -3.3936515 -4.3087463 -4.5857639 -4.3000178 -3.539026][-2.2322943 -1.8380332 -1.3516705 -0.78285789 -0.22523832 0.1281271 0.088001251 -0.39390373 -1.4595451 -2.9329114 -4.1670408 -4.8855572 -5.0851741 -4.6476288 -3.7328432][-2.7652373 -2.4390755 -2.0129452 -1.4941022 -0.9381392 -0.52243423 -0.50837731 -1.0157361 -1.9367561 -3.1058059 -4.1852551 -4.8820877 -5.0122528 -4.6552663 -3.9357522][-3.2149286 -2.9999056 -2.6503725 -2.1855731 -1.621366 -1.149662 -1.0140195 -1.2962992 -1.9543407 -2.9020739 -3.7527862 -4.3156977 -4.5174007 -4.2651858 -3.718143][-3.5594964 -3.449228 -3.1371503 -2.7181606 -2.172116 -1.672781 -1.4069004 -1.4607673 -1.8361893 -2.4674304 -3.1085548 -3.5776377 -3.7998638 -3.77355 -3.5190549]]...]
INFO - root - 2017-12-16 03:51:20.717661: step 2810, loss = 0.36, batch loss = 0.30 (28.5 examples/sec; 0.280 sec/batch; 25h:41m:03s remains)
INFO - root - 2017-12-16 03:51:23.519522: step 2820, loss = 0.31, batch loss = 0.25 (27.5 examples/sec; 0.291 sec/batch; 26h:40m:23s remains)
INFO - root - 2017-12-16 03:51:26.324470: step 2830, loss = 0.30, batch loss = 0.24 (28.6 examples/sec; 0.280 sec/batch; 25h:37m:04s remains)
INFO - root - 2017-12-16 03:51:29.107664: step 2840, loss = 0.30, batch loss = 0.24 (28.5 examples/sec; 0.281 sec/batch; 25h:42m:32s remains)
INFO - root - 2017-12-16 03:51:31.930790: step 2850, loss = 0.23, batch loss = 0.17 (29.1 examples/sec; 0.275 sec/batch; 25h:09m:49s remains)
INFO - root - 2017-12-16 03:51:34.713617: step 2860, loss = 0.32, batch loss = 0.26 (29.3 examples/sec; 0.273 sec/batch; 25h:02m:12s remains)
INFO - root - 2017-12-16 03:51:37.539903: step 2870, loss = 0.43, batch loss = 0.37 (29.2 examples/sec; 0.274 sec/batch; 25h:03m:28s remains)
INFO - root - 2017-12-16 03:51:40.392357: step 2880, loss = 0.36, batch loss = 0.30 (26.4 examples/sec; 0.303 sec/batch; 27h:42m:22s remains)
INFO - root - 2017-12-16 03:51:43.236561: step 2890, loss = 0.25, batch loss = 0.19 (28.7 examples/sec; 0.279 sec/batch; 25h:31m:50s remains)
INFO - root - 2017-12-16 03:51:46.042913: step 2900, loss = 0.28, batch loss = 0.22 (29.7 examples/sec; 0.270 sec/batch; 24h:41m:08s remains)
2017-12-16 03:51:46.501452: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9308724 -3.0961809 -3.2272015 -3.3388391 -3.370743 -3.347435 -3.3060906 -3.2468176 -3.2121539 -3.2491534 -3.3084464 -3.3635941 -3.4042492 -3.339222 -3.2344396][-2.8442655 -3.026644 -3.1395047 -3.2290425 -3.2780876 -3.2484264 -3.1709952 -3.1606679 -3.1829727 -3.2948911 -3.4832492 -3.6062989 -3.6076851 -3.5051982 -3.3304915][-2.8579354 -3.0873647 -3.2278221 -3.2107573 -3.0978272 -3.0252934 -2.9199998 -2.867151 -2.9151385 -3.1487947 -3.4095719 -3.6845522 -3.8218868 -3.6790459 -3.4950495][-2.9637694 -3.1170542 -3.1369705 -2.9684732 -2.6802025 -2.3785067 -2.08135 -1.929908 -1.9541337 -2.3138561 -2.7880883 -3.2091365 -3.5008149 -3.5364044 -3.4413126][-2.9427519 -2.9715004 -2.792887 -2.455651 -1.9122744 -1.2590847 -0.60578036 -0.11892939 -0.062287331 -0.46957827 -1.1516917 -1.9614832 -2.6268945 -2.9928746 -3.1543493][-2.7436907 -2.6296172 -2.3761809 -1.7465951 -0.8526175 0.13823414 1.1540847 1.9336948 2.2026005 1.7597842 0.7608614 -0.4634378 -1.5454717 -2.2801678 -2.7018452][-2.604001 -2.3774753 -2.0135674 -1.2690628 -0.17652035 1.1963191 2.6904922 3.8355312 4.3500586 3.8715496 2.5820942 0.92505026 -0.57270813 -1.6502011 -2.3027556][-2.7319825 -2.4508286 -2.0966914 -1.370055 -0.093409538 1.5802007 3.3287349 4.7568712 5.5138073 5.0465822 3.5775824 1.6849332 -0.056328297 -1.3670902 -2.1169665][-2.8949564 -2.7692943 -2.600738 -2.1144013 -1.1021461 0.58550787 2.4477248 4.0290394 4.9026642 4.5212889 3.150219 1.1942582 -0.56270242 -1.7397072 -2.3672261][-3.3762503 -3.428863 -3.5215371 -3.3045669 -2.5500307 -1.1967225 0.39428711 1.8631821 2.7493672 2.4634218 1.2977214 -0.30259609 -1.6975431 -2.5858848 -2.9366851][-3.9010224 -4.0815945 -4.37616 -4.3913589 -3.9374225 -2.9366736 -1.6463096 -0.5450139 0.14487505 0.049775124 -0.7151804 -1.9115467 -2.8585048 -3.3365495 -3.3897738][-4.0362091 -4.2883134 -4.6614943 -4.8739028 -4.76492 -4.1584992 -3.283736 -2.4824409 -1.9273148 -1.8963358 -2.3344254 -3.1129189 -3.6656249 -3.8073454 -3.6210024][-3.7892692 -4.0044107 -4.30561 -4.5236239 -4.6593237 -4.4475274 -3.9913216 -3.5126762 -3.0812533 -2.9986131 -3.1089416 -3.4181061 -3.6476929 -3.6289408 -3.4267116][-3.3297534 -3.3852358 -3.5154607 -3.702178 -3.863308 -3.8170702 -3.6799359 -3.5281117 -3.3576798 -3.2547004 -3.2060151 -3.2733004 -3.2990456 -3.1783366 -2.9443536][-2.7448916 -2.63774 -2.5922549 -2.6894546 -2.7667007 -2.8167083 -2.8371971 -2.8192015 -2.8038096 -2.7668111 -2.7372708 -2.7245274 -2.6912527 -2.6256218 -2.4875991]]...]
INFO - root - 2017-12-16 03:51:49.267406: step 2910, loss = 0.29, batch loss = 0.23 (27.9 examples/sec; 0.287 sec/batch; 26h:16m:49s remains)
INFO - root - 2017-12-16 03:51:52.099337: step 2920, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 26h:00m:06s remains)
INFO - root - 2017-12-16 03:51:54.945989: step 2930, loss = 0.42, batch loss = 0.36 (29.1 examples/sec; 0.275 sec/batch; 25h:08m:02s remains)
INFO - root - 2017-12-16 03:51:57.770108: step 2940, loss = 0.31, batch loss = 0.25 (29.4 examples/sec; 0.272 sec/batch; 24h:53m:41s remains)
INFO - root - 2017-12-16 03:52:00.582464: step 2950, loss = 0.33, batch loss = 0.27 (28.5 examples/sec; 0.281 sec/batch; 25h:44m:19s remains)
INFO - root - 2017-12-16 03:52:03.396026: step 2960, loss = 0.33, batch loss = 0.27 (28.6 examples/sec; 0.279 sec/batch; 25h:34m:48s remains)
INFO - root - 2017-12-16 03:52:06.283812: step 2970, loss = 0.40, batch loss = 0.34 (24.2 examples/sec; 0.331 sec/batch; 30h:18m:36s remains)
INFO - root - 2017-12-16 03:52:09.109960: step 2980, loss = 0.34, batch loss = 0.28 (28.9 examples/sec; 0.277 sec/batch; 25h:19m:35s remains)
INFO - root - 2017-12-16 03:52:11.850477: step 2990, loss = 0.24, batch loss = 0.18 (29.3 examples/sec; 0.273 sec/batch; 24h:58m:47s remains)
INFO - root - 2017-12-16 03:52:14.632494: step 3000, loss = 0.46, batch loss = 0.40 (28.9 examples/sec; 0.277 sec/batch; 25h:21m:13s remains)
2017-12-16 03:52:15.109903: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5082018 -2.5619149 -2.7323308 -2.8913095 -3.1524012 -3.5096476 -3.6122468 -3.3587651 -3.0810146 -2.8996058 -2.71833 -2.631525 -2.6588025 -2.833699 -2.9698546][-3.0527778 -3.0677428 -3.1243737 -3.1458046 -3.2615845 -3.4490693 -3.4349413 -3.1349547 -2.8443584 -2.7018809 -2.6675129 -2.620151 -2.6276379 -2.7676911 -2.810811][-3.621284 -3.6761148 -3.6876378 -3.6210263 -3.5382526 -3.4506481 -3.1753926 -2.7249632 -2.4241567 -2.3603532 -2.4473746 -2.5999856 -2.7271302 -2.7436848 -2.5200748][-4.0157576 -4.2816734 -4.3623967 -4.1884327 -3.9035 -3.4850464 -2.9505181 -2.36897 -1.9972055 -2.0005383 -2.2019484 -2.4886012 -2.7111135 -2.6662359 -2.2131548][-4.2724252 -4.5757694 -4.6279616 -4.3411517 -3.8139105 -3.0196421 -2.187639 -1.5429125 -1.278178 -1.4088364 -1.8791912 -2.4153607 -2.7055516 -2.6755 -2.1141367][-3.891221 -4.2617626 -4.2686663 -3.9834747 -3.2790046 -2.2382421 -1.1744726 -0.29031181 -0.0207901 -0.35290575 -1.1550393 -2.1099298 -2.7381175 -2.7073989 -2.0470061][-3.6589789 -4.0522275 -4.0840964 -3.6410215 -2.7412128 -1.5101018 -0.23573399 0.727417 1.1428409 0.81397724 -0.1695199 -1.4799125 -2.5543749 -2.9322844 -2.4830732][-3.7849221 -4.0945783 -4.1402063 -3.751668 -2.7410855 -1.3423381 0.14854145 1.2599587 1.7350025 1.4908829 0.5386591 -0.88962173 -2.2469912 -2.9443913 -2.8672228][-4.2056155 -4.2961845 -4.2163363 -3.9139481 -3.0922089 -1.7200549 -0.19138622 1.0364285 1.6361737 1.4346991 0.479949 -0.97925544 -2.4230111 -3.3843706 -3.5615993][-4.2329392 -4.3659043 -4.2389755 -3.9381113 -3.3564827 -2.3665464 -1.0065143 0.19467306 0.90423012 0.92333269 0.15859556 -1.231571 -2.7134356 -3.6962054 -4.0848422][-3.9906139 -4.0423193 -4.0476527 -3.9753253 -3.5634913 -2.8078947 -1.8663197 -0.92406845 -0.14797068 0.12550688 -0.364892 -1.5396719 -2.9079256 -3.8358626 -4.2184386][-3.8341887 -3.7862463 -3.801054 -3.9554005 -3.905602 -3.4494617 -2.6519928 -1.9491425 -1.2749496 -0.89637041 -1.0880578 -1.9351771 -3.0181246 -3.8518097 -4.2148981][-3.3152494 -3.2789464 -3.3778691 -3.6186476 -3.8056009 -3.6694469 -3.1700063 -2.5805221 -1.9615781 -1.6291237 -1.714406 -2.2380466 -2.9631009 -3.5686967 -3.8640862][-2.6660414 -2.536736 -2.5467427 -2.821331 -3.1330445 -3.1512632 -2.8936322 -2.4543281 -1.915411 -1.5922663 -1.7027106 -2.218569 -2.8244889 -3.2199383 -3.4320819][-2.0330307 -1.8111839 -1.7518454 -1.9893806 -2.3433025 -2.4583549 -2.2671297 -1.8855228 -1.4355166 -1.2220933 -1.3424518 -1.9092424 -2.588933 -3.1087978 -3.3592913]]...]
INFO - root - 2017-12-16 03:52:17.924753: step 3010, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 25h:53m:47s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 03:52:20.752063: step 3020, loss = 0.28, batch loss = 0.22 (28.6 examples/sec; 0.280 sec/batch; 25h:36m:40s remains)
INFO - root - 2017-12-16 03:52:23.625257: step 3030, loss = 0.23, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 25h:54m:15s remains)
INFO - root - 2017-12-16 03:52:26.444540: step 3040, loss = 0.29, batch loss = 0.23 (28.6 examples/sec; 0.280 sec/batch; 25h:36m:21s remains)
INFO - root - 2017-12-16 03:52:29.272528: step 3050, loss = 0.33, batch loss = 0.27 (28.3 examples/sec; 0.283 sec/batch; 25h:54m:34s remains)
INFO - root - 2017-12-16 03:52:32.128385: step 3060, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.286 sec/batch; 26h:10m:27s remains)
INFO - root - 2017-12-16 03:52:34.927803: step 3070, loss = 0.30, batch loss = 0.24 (28.4 examples/sec; 0.282 sec/batch; 25h:48m:52s remains)
INFO - root - 2017-12-16 03:52:37.748633: step 3080, loss = 0.26, batch loss = 0.20 (25.6 examples/sec; 0.313 sec/batch; 28h:38m:58s remains)
INFO - root - 2017-12-16 03:52:40.541097: step 3090, loss = 0.24, batch loss = 0.18 (29.1 examples/sec; 0.275 sec/batch; 25h:11m:42s remains)
INFO - root - 2017-12-16 03:52:43.357507: step 3100, loss = 0.27, batch loss = 0.21 (28.1 examples/sec; 0.284 sec/batch; 26h:01m:23s remains)
2017-12-16 03:52:43.806021: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5892968 -2.8724549 -3.2954249 -3.8928554 -4.3913894 -4.6844845 -4.7871809 -4.7586279 -4.6541381 -4.4804239 -4.3514209 -4.2281489 -4.179873 -4.1943622 -4.2715378][-2.5253747 -2.8354323 -3.2280636 -3.7268889 -4.0403647 -4.1694183 -4.1294079 -4.0690427 -3.9954963 -4.0143962 -4.1268816 -4.2148767 -4.2932954 -4.3596973 -4.432467][-2.5009589 -2.7785854 -3.0715914 -3.4396086 -3.567848 -3.4845924 -3.2948537 -3.2484951 -3.2839432 -3.4525223 -3.7701602 -4.0589142 -4.2951574 -4.4094052 -4.4533372][-2.4302745 -2.72993 -2.9665384 -3.1616337 -3.1045015 -2.8704376 -2.5673981 -2.4761953 -2.6353703 -2.9728961 -3.4368041 -3.8838944 -4.2133865 -4.3192658 -4.2709084][-2.4236915 -2.7171764 -2.8733697 -2.9357843 -2.7356486 -2.3926795 -1.9973092 -1.8765576 -2.1114671 -2.5228004 -3.0291071 -3.5081198 -3.8192766 -3.9247236 -3.831212][-2.54205 -2.7649817 -2.8331451 -2.7744226 -2.4464402 -2.0525994 -1.6511905 -1.4348698 -1.6670833 -2.1126087 -2.6130223 -2.9944177 -3.2581177 -3.3302732 -3.2042515][-2.6484973 -2.8087192 -2.823221 -2.6385937 -2.2283268 -1.7039723 -1.248966 -0.95554376 -1.0594401 -1.4503093 -1.9500351 -2.3222322 -2.531806 -2.614161 -2.4608786][-2.6310225 -2.7353349 -2.6857247 -2.4600639 -2.0190074 -1.4560115 -0.86849904 -0.45791531 -0.37713194 -0.70932531 -1.2006848 -1.5688303 -1.7786515 -1.8291922 -1.635643][-2.6204717 -2.621655 -2.5624483 -2.3253164 -1.9430637 -1.4125187 -0.85965943 -0.39125919 -0.156466 -0.36264896 -0.74836874 -1.107553 -1.2953033 -1.2834358 -0.9870019][-2.5763903 -2.6728888 -2.8577659 -2.71009 -2.5278041 -2.0768 -1.5086799 -0.95241213 -0.63188124 -0.69418478 -0.95034409 -1.2108767 -1.2607191 -1.0802183 -0.65824485][-2.7686386 -3.0325692 -3.446779 -3.6835525 -3.8101261 -3.5771413 -3.0845156 -2.530443 -2.0701697 -1.9669678 -1.9589972 -2.0081604 -1.86603 -1.5146089 -0.96409297][-3.1618152 -3.7554827 -4.5886703 -5.1288934 -5.5094857 -5.4212656 -5.0269032 -4.5171661 -4.057704 -3.776989 -3.5109761 -3.2985325 -2.9058201 -2.4531178 -1.964112][-3.649303 -4.5334263 -5.5777369 -6.4873929 -7.12376 -7.1739931 -6.87089 -6.3492031 -5.8873515 -5.5202641 -5.1886368 -4.9153147 -4.5093589 -4.03742 -3.5653081][-4.1002455 -5.0858431 -6.1470737 -7.1167388 -7.7049904 -7.9302139 -7.7591677 -7.4186249 -7.0335178 -6.6850109 -6.4745932 -6.3174739 -5.9842095 -5.6572 -5.2807236][-4.22203 -5.1472392 -6.1503649 -6.989459 -7.46717 -7.683423 -7.6178689 -7.4478464 -7.2288933 -7.0726495 -7.0535593 -7.0936432 -7.0098705 -6.8739653 -6.5786915]]...]
INFO - root - 2017-12-16 03:52:46.634622: step 3110, loss = 0.40, batch loss = 0.34 (27.7 examples/sec; 0.289 sec/batch; 26h:26m:50s remains)
INFO - root - 2017-12-16 03:52:49.430578: step 3120, loss = 0.25, batch loss = 0.19 (28.8 examples/sec; 0.277 sec/batch; 25h:22m:26s remains)
INFO - root - 2017-12-16 03:52:52.205655: step 3130, loss = 0.46, batch loss = 0.41 (29.6 examples/sec; 0.270 sec/batch; 24h:42m:45s remains)
INFO - root - 2017-12-16 03:52:55.004494: step 3140, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 26h:04m:34s remains)
INFO - root - 2017-12-16 03:52:57.786644: step 3150, loss = 0.27, batch loss = 0.21 (28.7 examples/sec; 0.279 sec/batch; 25h:30m:45s remains)
INFO - root - 2017-12-16 03:53:00.626726: step 3160, loss = 0.26, batch loss = 0.20 (27.9 examples/sec; 0.287 sec/batch; 26h:14m:20s remains)
INFO - root - 2017-12-16 03:53:03.429496: step 3170, loss = 0.31, batch loss = 0.25 (28.5 examples/sec; 0.281 sec/batch; 25h:40m:37s remains)
INFO - root - 2017-12-16 03:53:06.216639: step 3180, loss = 0.36, batch loss = 0.31 (29.1 examples/sec; 0.275 sec/batch; 25h:09m:34s remains)
INFO - root - 2017-12-16 03:53:09.106635: step 3190, loss = 0.22, batch loss = 0.16 (29.5 examples/sec; 0.271 sec/batch; 24h:47m:19s remains)
INFO - root - 2017-12-16 03:53:11.886262: step 3200, loss = 0.32, batch loss = 0.26 (29.1 examples/sec; 0.275 sec/batch; 25h:08m:17s remains)
2017-12-16 03:53:12.345570: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7483978 -5.4517527 -4.9496827 -4.197732 -3.415247 -2.7330651 -2.3079712 -2.2319462 -2.61325 -3.30661 -3.9201825 -4.4095788 -4.4976692 -4.1292377 -3.4649208][-5.103497 -4.7880945 -4.3052139 -3.6381197 -2.9397752 -2.2716038 -1.7294641 -1.6373336 -2.0652876 -2.8143504 -3.6170073 -4.2114162 -4.3915529 -4.1526437 -3.56238][-4.1861897 -3.8877149 -3.5011013 -2.9853601 -2.3022108 -1.7049901 -1.2581975 -1.1755621 -1.7801118 -2.6957688 -3.6765418 -4.4391084 -4.726532 -4.4723678 -3.8425806][-3.1966486 -2.9153609 -2.6457281 -2.32715 -1.8223708 -1.2506011 -0.85864043 -0.79497242 -1.3763943 -2.4173963 -3.5874088 -4.4599104 -4.8202243 -4.5823936 -3.9749854][-2.242389 -2.0944455 -1.9906015 -1.8319705 -1.3322639 -0.68311954 -0.10452271 0.0050115585 -0.62748957 -1.7724919 -3.128772 -4.2406597 -4.8263755 -4.7570815 -4.2142677][-1.9578478 -1.9456494 -1.8875012 -1.7416129 -1.0593035 -0.18598413 0.6104784 0.88297462 0.37144613 -0.86215043 -2.4297798 -3.8678484 -4.7574954 -4.9636707 -4.5730424][-2.0230534 -2.0846281 -2.009093 -1.7840486 -0.97582221 0.071574211 1.0595732 1.4502254 0.94210815 -0.3407197 -2.08355 -3.7427273 -4.8623695 -5.2295685 -4.9380474][-2.3241861 -2.4191496 -2.2976441 -1.9359274 -0.97637653 0.15798426 1.1664705 1.6051893 1.1198153 -0.26160192 -2.1196859 -3.8226285 -5.0206738 -5.4902687 -5.2353759][-2.5852351 -2.7101655 -2.6171789 -2.1908729 -1.1344836 0.093921661 1.1675096 1.5793018 1.0810037 -0.29640818 -2.1859069 -4.0086761 -5.3270049 -5.8271456 -5.5758195][-2.6230216 -2.8626785 -2.8261 -2.4702706 -1.5657063 -0.35911655 0.704 1.1371298 0.67034531 -0.7125175 -2.5759087 -4.3627028 -5.6855464 -6.2108588 -5.9212627][-2.4719281 -2.8753281 -3.0415778 -2.7484961 -1.9072306 -0.79162812 0.1564784 0.59253407 0.14499998 -1.2191682 -3.020288 -4.7498422 -5.9594808 -6.4134674 -6.0858312][-2.4896579 -2.9474983 -3.1612082 -2.8936186 -2.0929425 -1.0673823 -0.27145863 -0.016535282 -0.57346296 -1.8915803 -3.5163634 -5.0895615 -6.1429133 -6.3631797 -5.8763046][-2.4947777 -2.9819961 -3.206948 -2.9419932 -2.2012384 -1.2841103 -0.67942309 -0.57637668 -1.1589344 -2.41962 -3.8875179 -5.1781449 -5.9685216 -6.099061 -5.5983667][-2.4756696 -3.0214984 -3.3147931 -3.0538945 -2.4134111 -1.5897498 -1.0416224 -1.0288846 -1.6020138 -2.7086642 -3.9897566 -5.1185741 -5.7501 -5.7584572 -5.2088985][-2.9800503 -3.415081 -3.5895143 -3.2424235 -2.5959539 -1.8388364 -1.3056784 -1.2819641 -1.850908 -2.9385724 -4.0642638 -5.0171118 -5.4518404 -5.3207984 -4.7387624]]...]
INFO - root - 2017-12-16 03:53:15.215724: step 3210, loss = 0.23, batch loss = 0.17 (27.7 examples/sec; 0.289 sec/batch; 26h:23m:28s remains)
INFO - root - 2017-12-16 03:53:18.068591: step 3220, loss = 0.27, batch loss = 0.22 (27.5 examples/sec; 0.291 sec/batch; 26h:36m:19s remains)
INFO - root - 2017-12-16 03:53:20.872313: step 3230, loss = 0.28, batch loss = 0.23 (28.9 examples/sec; 0.276 sec/batch; 25h:17m:14s remains)
INFO - root - 2017-12-16 03:53:23.728823: step 3240, loss = 0.30, batch loss = 0.25 (28.7 examples/sec; 0.278 sec/batch; 25h:27m:55s remains)
INFO - root - 2017-12-16 03:53:26.537272: step 3250, loss = 0.30, batch loss = 0.24 (28.4 examples/sec; 0.281 sec/batch; 25h:43m:49s remains)
INFO - root - 2017-12-16 03:53:29.343408: step 3260, loss = 0.28, batch loss = 0.22 (28.7 examples/sec; 0.279 sec/batch; 25h:31m:23s remains)
INFO - root - 2017-12-16 03:53:32.202005: step 3270, loss = 0.34, batch loss = 0.29 (27.8 examples/sec; 0.288 sec/batch; 26h:18m:37s remains)
INFO - root - 2017-12-16 03:53:35.050423: step 3280, loss = 0.22, batch loss = 0.16 (29.3 examples/sec; 0.273 sec/batch; 24h:58m:03s remains)
INFO - root - 2017-12-16 03:53:37.844972: step 3290, loss = 0.30, batch loss = 0.24 (28.0 examples/sec; 0.286 sec/batch; 26h:08m:56s remains)
INFO - root - 2017-12-16 03:53:40.671767: step 3300, loss = 0.32, batch loss = 0.27 (28.4 examples/sec; 0.281 sec/batch; 25h:42m:53s remains)
2017-12-16 03:53:41.138429: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8859248 -2.5639315 -1.8477104 -1.178411 -1.0416584 -1.1426914 -1.4806535 -1.7497895 -1.9688513 -1.9677467 -1.6322761 -1.1598525 -0.76893854 -0.71215296 -1.1245017][-2.2275038 -1.7279363 -0.79723477 -0.00032424927 0.22600079 0.162817 -0.28536415 -0.82836127 -1.4043214 -1.6798255 -1.4855142 -1.1053977 -0.69451141 -0.51443577 -0.74906993][-1.7995927 -1.1784713 -0.15574312 0.71214914 0.96987009 0.79196644 0.15227747 -0.66564846 -1.4781778 -2.0291784 -2.0939059 -1.7977054 -1.3719287 -1.0915039 -1.2408528][-1.3909514 -0.8761096 -0.032004356 0.7657938 1.0503278 0.90490723 0.26673508 -0.68879104 -1.7020104 -2.4931414 -2.8500481 -2.8877015 -2.741137 -2.500416 -2.4489274][-1.5676661 -1.1208875 -0.41738653 0.29256535 0.69256735 0.70862627 0.21219873 -0.66640162 -1.7552805 -2.77678 -3.5434608 -4.0059872 -4.1388774 -4.0980973 -4.0138526][-1.8097863 -1.5116467 -0.88171053 -0.15821552 0.48441029 0.76616573 0.57839251 -0.15243196 -1.1874924 -2.3628824 -3.5845976 -4.49151 -5.0193596 -5.1731977 -5.0734692][-2.0533681 -1.9036129 -1.4139616 -0.58971024 0.32396412 0.89034033 1.031486 0.58417034 -0.22027922 -1.4416766 -2.8874359 -4.2692537 -5.2486558 -5.6862783 -5.7226896][-2.2817371 -2.2195873 -1.8501217 -0.97307539 0.087199211 0.89058065 1.2564816 1.0619311 0.4185462 -0.8160193 -2.4037535 -3.9729548 -5.1380119 -5.8627152 -6.1154685][-2.6318393 -2.5214303 -2.1943827 -1.3866951 -0.30728054 0.62511683 1.1476593 1.0945683 0.47375202 -0.72113752 -2.2600975 -3.8815558 -5.2201929 -6.0217409 -6.2533064][-2.9862537 -2.7837095 -2.4576974 -1.7885954 -0.92384338 -0.16523933 0.18899488 0.15427828 -0.3767314 -1.4224055 -2.7809167 -4.1896577 -5.3490477 -6.0274687 -6.1338906][-3.0198238 -2.7385821 -2.4114985 -2.0350735 -1.5901375 -1.2538385 -1.1631875 -1.4099612 -1.9611566 -2.8069897 -3.8378043 -4.8641968 -5.6870551 -6.1406336 -6.21202][-3.0997024 -2.7104559 -2.383548 -2.1752131 -2.0700297 -2.1510115 -2.493 -2.9795561 -3.4844732 -4.1294751 -4.8951521 -5.6774311 -6.2447367 -6.523077 -6.4614887][-2.9952068 -2.592557 -2.2831981 -2.3067882 -2.607589 -3.0784831 -3.7224336 -4.38833 -4.9420581 -5.315887 -5.6809382 -6.0961361 -6.3840055 -6.5089207 -6.3578939][-2.8715076 -2.3854232 -2.0999885 -2.2059562 -2.6901579 -3.3693533 -4.1775184 -4.9785914 -5.581183 -5.8437233 -5.9163456 -6.038837 -6.08731 -5.9804878 -5.64586][-2.7698245 -2.2673759 -1.9841185 -2.0859485 -2.52361 -3.2308354 -4.0991888 -4.8904123 -5.376112 -5.5112176 -5.5054822 -5.4555492 -5.3237867 -5.1290464 -4.7834096]]...]
INFO - root - 2017-12-16 03:53:43.959435: step 3310, loss = 0.33, batch loss = 0.27 (26.4 examples/sec; 0.303 sec/batch; 27h:42m:16s remains)
INFO - root - 2017-12-16 03:53:46.783308: step 3320, loss = 0.29, batch loss = 0.23 (27.8 examples/sec; 0.288 sec/batch; 26h:17m:45s remains)
INFO - root - 2017-12-16 03:53:49.664457: step 3330, loss = 0.30, batch loss = 0.24 (28.5 examples/sec; 0.280 sec/batch; 25h:37m:29s remains)
INFO - root - 2017-12-16 03:53:52.501424: step 3340, loss = 0.41, batch loss = 0.35 (27.4 examples/sec; 0.292 sec/batch; 26h:41m:53s remains)
INFO - root - 2017-12-16 03:53:55.320993: step 3350, loss = 0.30, batch loss = 0.24 (28.3 examples/sec; 0.283 sec/batch; 25h:51m:50s remains)
INFO - root - 2017-12-16 03:53:58.172707: step 3360, loss = 0.41, batch loss = 0.35 (28.2 examples/sec; 0.284 sec/batch; 25h:55m:24s remains)
INFO - root - 2017-12-16 03:54:00.979362: step 3370, loss = 0.25, batch loss = 0.20 (29.6 examples/sec; 0.271 sec/batch; 24h:43m:54s remains)
INFO - root - 2017-12-16 03:54:03.795361: step 3380, loss = 0.40, batch loss = 0.34 (28.7 examples/sec; 0.279 sec/batch; 25h:27m:40s remains)
INFO - root - 2017-12-16 03:54:06.651459: step 3390, loss = 0.34, batch loss = 0.28 (27.0 examples/sec; 0.296 sec/batch; 27h:04m:51s remains)
INFO - root - 2017-12-16 03:54:09.482400: step 3400, loss = 0.32, batch loss = 0.26 (29.6 examples/sec; 0.271 sec/batch; 24h:44m:44s remains)
2017-12-16 03:54:09.929476: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1406965 -3.84214 -3.7233555 -3.7392626 -3.7177973 -3.7864075 -3.8643179 -3.9410727 -4.0688138 -4.2382026 -4.4284592 -4.6495852 -4.82386 -5.0004468 -5.099009][-3.9009256 -3.5828295 -3.4478047 -3.8217368 -4.1771569 -4.37838 -4.4136949 -4.4278889 -4.4321904 -4.592886 -4.9916191 -5.294435 -5.4562054 -5.4729877 -5.3814645][-2.9520745 -2.8724813 -3.1834817 -3.6906884 -4.3218751 -4.6459107 -4.60328 -4.5088887 -4.5634589 -4.7504826 -5.057035 -5.4750972 -5.7123628 -5.5462422 -5.1247487][-2.2479205 -2.1165745 -2.4761524 -3.2547407 -4.0235963 -4.3743663 -4.2734671 -3.8033738 -3.5284295 -3.8687577 -4.5460596 -5.0177093 -5.18879 -4.9318447 -4.357635][-1.4311297 -1.1890862 -1.7988651 -2.6342492 -3.3172712 -3.3813014 -2.787477 -2.3449593 -2.2300706 -2.5526159 -3.1467724 -3.9659815 -4.1100492 -3.6841621 -3.1847179][-0.59505987 -0.36231709 -0.82908177 -1.7199304 -2.1008177 -2.0350614 -1.3457947 -0.54740477 -0.42690039 -1.0576422 -2.0567172 -2.6213398 -2.5601902 -2.0619047 -1.6537566][0.289155 0.36206865 -0.25101757 -0.91703844 -1.2223592 -0.66346741 0.41527224 0.90139627 0.68828392 0.088891506 -0.76932549 -1.3756099 -1.1959133 -0.73620605 -0.53671384][0.18866968 0.20557451 -0.31016397 -0.75334644 -0.70179653 0.0031256676 1.1585689 1.5996695 1.358376 0.56471777 -0.15986538 -0.47126961 -0.38312387 -0.073839664 -0.24992561][-0.30869913 -0.41142654 -0.79259992 -1.2203875 -1.1088076 -0.22441149 0.80481625 1.2163868 1.0571589 0.42368937 -0.16643572 -0.31546974 -0.10255146 -0.14562321 -0.611876][-0.49883938 -0.91595507 -1.6166141 -2.04578 -1.8260896 -1.1406341 -0.54269052 -0.23774529 -0.41944981 -0.69510531 -0.94128036 -0.98088169 -0.72062325 -0.66319227 -0.98980451][-1.5512843 -1.7647054 -2.3374093 -3.1663065 -3.4416668 -2.951509 -2.3267112 -2.1747012 -2.2768021 -2.4197745 -2.4219985 -2.1903014 -1.8496804 -1.9354818 -2.3151302][-2.81534 -3.4155703 -4.2841296 -4.8416796 -4.93799 -4.8806791 -4.5798483 -4.2043142 -4.0648336 -4.1945238 -4.1987252 -3.9171708 -3.4702477 -3.2432232 -3.4717636][-4.0328274 -4.4277754 -5.097528 -5.8581486 -6.312398 -6.1227312 -5.8364778 -5.652442 -5.4766803 -5.2810297 -4.9771214 -4.95026 -4.7099724 -4.4472146 -4.5248327][-4.1154714 -4.5114069 -5.3627453 -5.9744744 -6.2858405 -6.283608 -6.2173343 -6.0565524 -5.9001412 -5.6711793 -5.4306588 -5.370275 -5.229239 -5.1216159 -5.2049246][-4.7060194 -4.7793078 -5.0843391 -5.657804 -6.1466856 -6.2093396 -6.2249284 -6.0796738 -5.9775524 -5.8202248 -5.64974 -5.4984741 -5.3789678 -5.2895107 -5.3153372]]...]
INFO - root - 2017-12-16 03:54:12.775841: step 3410, loss = 0.32, batch loss = 0.26 (28.2 examples/sec; 0.284 sec/batch; 25h:55m:47s remains)
INFO - root - 2017-12-16 03:54:15.662205: step 3420, loss = 0.30, batch loss = 0.25 (27.8 examples/sec; 0.288 sec/batch; 26h:18m:08s remains)
INFO - root - 2017-12-16 03:54:18.462335: step 3430, loss = 0.31, batch loss = 0.25 (28.3 examples/sec; 0.283 sec/batch; 25h:52m:01s remains)
INFO - root - 2017-12-16 03:54:21.253547: step 3440, loss = 0.31, batch loss = 0.25 (29.0 examples/sec; 0.276 sec/batch; 25h:12m:56s remains)
INFO - root - 2017-12-16 03:54:24.092831: step 3450, loss = 0.45, batch loss = 0.39 (27.5 examples/sec; 0.291 sec/batch; 26h:36m:03s remains)
INFO - root - 2017-12-16 03:54:26.915748: step 3460, loss = 0.33, batch loss = 0.27 (28.7 examples/sec; 0.279 sec/batch; 25h:29m:39s remains)
INFO - root - 2017-12-16 03:54:29.771955: step 3470, loss = 0.27, batch loss = 0.21 (27.5 examples/sec; 0.291 sec/batch; 26h:33m:23s remains)
INFO - root - 2017-12-16 03:54:32.559178: step 3480, loss = 0.25, batch loss = 0.19 (28.5 examples/sec; 0.281 sec/batch; 25h:41m:14s remains)
INFO - root - 2017-12-16 03:54:35.392406: step 3490, loss = 0.41, batch loss = 0.35 (28.3 examples/sec; 0.282 sec/batch; 25h:48m:12s remains)
INFO - root - 2017-12-16 03:54:38.278762: step 3500, loss = 0.47, batch loss = 0.41 (28.2 examples/sec; 0.283 sec/batch; 25h:52m:52s remains)
2017-12-16 03:54:38.748612: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7957268 -5.8096662 -5.8048167 -5.9418678 -6.0369444 -6.2122011 -6.3521061 -6.661417 -6.8556809 -6.9512806 -6.8494258 -6.5806484 -6.0862112 -5.401845 -4.6867757][-5.8819671 -5.7939749 -5.6804647 -5.668056 -5.7167654 -6.0905552 -6.3611503 -6.7837548 -7.0757036 -7.2472529 -7.21426 -6.9399309 -6.5492048 -5.95114 -5.2451243][-5.5958185 -5.2218008 -4.9073167 -4.70841 -4.6214371 -4.8699923 -5.1910052 -5.9769816 -6.4378147 -6.6272326 -6.6655369 -6.5564518 -6.4039383 -6.033927 -5.5216293][-4.8292694 -4.1289358 -3.4660118 -3.0459027 -2.8543303 -2.9751582 -3.2217579 -3.9522049 -4.5532393 -5.0895753 -5.3455091 -5.3970933 -5.5348072 -5.4879994 -5.37398][-3.8633175 -2.8101664 -1.7137465 -0.95354509 -0.48910928 -0.50087333 -0.84623766 -1.5214086 -2.1165588 -2.6785562 -3.2931709 -3.8620679 -4.4168873 -4.6494269 -4.9109416][-3.2536259 -1.9304447 -0.49043369 0.66744137 1.4955039 1.6668906 1.4257431 0.77203655 0.17034483 -0.37740946 -1.1791177 -2.102535 -3.2002206 -4.0035958 -4.456955][-3.5149534 -2.0724857 -0.41595507 0.99890041 2.1233072 2.6324434 2.686079 2.1317658 1.5643525 1.0431347 0.13204479 -0.949646 -2.1662352 -3.2275734 -3.8959756][-4.0509591 -2.7848458 -1.2343533 0.401299 1.7048855 2.4103265 2.7090378 2.4478612 2.1750889 1.6951046 0.7383914 -0.42974782 -1.7497396 -2.8203938 -3.5815823][-4.9367356 -4.0023394 -2.7547655 -1.3378997 0.020508766 1.0701141 1.6046095 1.5627117 1.5585146 1.2880263 0.53182411 -0.62719679 -1.9521258 -2.9390788 -3.6368508][-5.8962951 -5.4713316 -4.585041 -3.4651115 -2.2499671 -1.1853576 -0.39812326 -0.099476814 -0.035982609 -0.15089321 -0.71417809 -1.6286807 -2.6554737 -3.5439444 -4.1151247][-6.3279543 -6.0634265 -5.5610271 -4.9244261 -4.0195589 -3.060189 -2.1807473 -1.6493523 -1.4498336 -1.5028296 -1.9204304 -2.6388493 -3.3752353 -4.0700021 -4.5609674][-6.0559711 -6.0146551 -5.8433728 -5.3711224 -4.7733607 -4.1965384 -3.5150535 -2.992774 -2.5959449 -2.4939458 -2.7298837 -3.3285766 -3.88125 -4.2671337 -4.5543866][-5.0881958 -5.1184626 -5.1792035 -5.0457082 -4.78053 -4.4139314 -3.9231906 -3.6215851 -3.3277998 -3.1777837 -3.2150531 -3.5331764 -4.0145068 -4.3176579 -4.4097776][-4.0925479 -4.0332775 -4.015521 -4.01413 -3.9497643 -3.7851911 -3.6005681 -3.4226198 -3.3107848 -3.227489 -3.2637682 -3.4429588 -3.6376925 -3.7845545 -3.7866483][-3.1078153 -2.9425676 -2.8771372 -2.9042268 -2.9089875 -2.9024296 -2.9067497 -2.8462081 -2.8193932 -2.8107827 -2.8379869 -2.9189568 -2.980083 -3.1007752 -3.0971017]]...]
INFO - root - 2017-12-16 03:54:41.590458: step 3510, loss = 0.31, batch loss = 0.26 (28.4 examples/sec; 0.281 sec/batch; 25h:43m:25s remains)
INFO - root - 2017-12-16 03:54:44.360172: step 3520, loss = 0.28, batch loss = 0.23 (29.6 examples/sec; 0.270 sec/batch; 24h:42m:22s remains)
INFO - root - 2017-12-16 03:54:47.153907: step 3530, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.277 sec/batch; 25h:17m:57s remains)
INFO - root - 2017-12-16 03:54:49.998698: step 3540, loss = 0.34, batch loss = 0.28 (27.5 examples/sec; 0.291 sec/batch; 26h:37m:46s remains)
INFO - root - 2017-12-16 03:54:52.807494: step 3550, loss = 0.38, batch loss = 0.32 (28.2 examples/sec; 0.284 sec/batch; 25h:55m:31s remains)
INFO - root - 2017-12-16 03:54:55.654654: step 3560, loss = 0.36, batch loss = 0.30 (29.6 examples/sec; 0.271 sec/batch; 24h:43m:59s remains)
INFO - root - 2017-12-16 03:54:58.472948: step 3570, loss = 0.28, batch loss = 0.23 (28.4 examples/sec; 0.282 sec/batch; 25h:46m:42s remains)
INFO - root - 2017-12-16 03:55:01.251356: step 3580, loss = 0.36, batch loss = 0.30 (29.3 examples/sec; 0.273 sec/batch; 24h:59m:00s remains)
INFO - root - 2017-12-16 03:55:04.105029: step 3590, loss = 0.30, batch loss = 0.24 (27.3 examples/sec; 0.293 sec/batch; 26h:44m:57s remains)
INFO - root - 2017-12-16 03:55:06.913768: step 3600, loss = 0.39, batch loss = 0.33 (28.6 examples/sec; 0.280 sec/batch; 25h:32m:49s remains)
2017-12-16 03:55:07.339780: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4685457 -3.6203132 -3.6254151 -3.6226258 -3.7174225 -3.9067526 -4.1918082 -4.7355423 -5.4483356 -6.0140533 -6.4440026 -6.5558538 -6.3300476 -5.7807088 -4.9776053][-4.1005254 -4.1868033 -4.0989914 -4.0133061 -4.0062623 -4.2016177 -4.437458 -4.8598905 -5.5565329 -6.26794 -6.8621483 -6.9640636 -6.7341051 -6.1556792 -5.2770638][-4.65627 -4.743331 -4.5372248 -4.2370896 -3.9526286 -3.9153571 -3.9401436 -4.3898268 -5.1594586 -5.8680477 -6.5069942 -6.8269215 -6.7961149 -6.3358717 -5.5959349][-5.0220437 -4.9708986 -4.5645342 -3.9767182 -3.3003035 -2.9726214 -2.7535214 -3.0025377 -3.5546017 -4.2634096 -5.0843692 -5.7029886 -6.0869236 -6.0792689 -5.6720676][-5.132184 -4.7434964 -3.9796584 -3.0989335 -2.0044217 -1.1013985 -0.48616505 -0.53445292 -1.0555358 -1.819114 -2.8770804 -3.9991846 -4.9840755 -5.58006 -5.6399741][-4.8412189 -4.18426 -3.0317731 -1.6896327 -0.027334213 1.2521434 2.1769218 2.3969345 1.8591056 0.81748152 -0.69474292 -2.3086336 -3.8519566 -5.0632043 -5.61371][-3.7856107 -3.1948738 -1.9739451 -0.44441319 1.4970584 3.2741141 4.562953 4.8264217 4.3442831 3.2153668 1.3560829 -0.71201229 -2.6459985 -4.3070941 -5.3112178][-3.1306472 -2.6808052 -1.6315548 -0.1815958 1.8767319 3.8586855 5.2639284 5.7762852 5.478013 4.308136 2.382494 0.12246704 -2.1201108 -4.047009 -5.1149092][-2.9321413 -2.6909471 -2.0415823 -0.953804 0.69728708 2.5552993 4.2240372 4.8954573 4.598959 3.4802771 1.7512717 -0.37912846 -2.5673265 -4.3922968 -5.4126077][-3.0584841 -3.2289841 -2.9832392 -2.296344 -1.0896401 0.377995 1.6306739 2.3392982 2.3232412 1.3715997 -0.19241953 -2.132036 -3.9608862 -5.4588928 -6.2590938][-3.3564773 -3.6303191 -3.8962784 -3.9647629 -3.3371282 -2.2291746 -1.1999395 -0.68610263 -0.7203517 -1.1928337 -2.1699803 -3.6516006 -5.1649404 -6.2969804 -6.9065571][-3.5893741 -3.9771018 -4.6545568 -4.993011 -4.7814064 -4.316998 -3.7055917 -3.2927728 -3.147172 -3.4209847 -4.1287909 -5.1248164 -6.0998893 -6.913692 -7.3845682][-4.0245285 -4.2181072 -4.7243142 -5.2527943 -5.5236082 -5.3623819 -5.0714493 -4.8949566 -4.8135824 -4.842639 -5.3201718 -6.0871234 -6.7816181 -7.3069286 -7.582294][-4.1695929 -4.3182468 -4.6685319 -5.0342269 -5.33534 -5.4041653 -5.356 -5.2973208 -5.2951689 -5.3681178 -5.7891483 -6.3309708 -6.8528376 -7.1889706 -7.2901363][-4.5336337 -4.3850155 -4.4978318 -4.746501 -4.9913015 -5.06066 -5.0864525 -5.0900726 -5.1988292 -5.3236108 -5.6302881 -6.0153112 -6.3753047 -6.5620971 -6.5763426]]...]
INFO - root - 2017-12-16 03:55:10.172369: step 3610, loss = 0.30, batch loss = 0.24 (28.5 examples/sec; 0.280 sec/batch; 25h:37m:24s remains)
INFO - root - 2017-12-16 03:55:13.046049: step 3620, loss = 0.25, batch loss = 0.19 (28.9 examples/sec; 0.277 sec/batch; 25h:16m:38s remains)
INFO - root - 2017-12-16 03:55:15.872460: step 3630, loss = 0.28, batch loss = 0.22 (29.7 examples/sec; 0.269 sec/batch; 24h:37m:07s remains)
INFO - root - 2017-12-16 03:55:18.669187: step 3640, loss = 0.35, batch loss = 0.29 (29.7 examples/sec; 0.270 sec/batch; 24h:37m:17s remains)
INFO - root - 2017-12-16 03:55:21.462370: step 3650, loss = 0.27, batch loss = 0.21 (28.4 examples/sec; 0.282 sec/batch; 25h:43m:12s remains)
INFO - root - 2017-12-16 03:55:24.278672: step 3660, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 25h:42m:58s remains)
INFO - root - 2017-12-16 03:55:27.092375: step 3670, loss = 0.25, batch loss = 0.20 (29.3 examples/sec; 0.273 sec/batch; 24h:54m:43s remains)
INFO - root - 2017-12-16 03:55:29.914392: step 3680, loss = 0.28, batch loss = 0.22 (29.0 examples/sec; 0.276 sec/batch; 25h:11m:14s remains)
INFO - root - 2017-12-16 03:55:32.731194: step 3690, loss = 0.31, batch loss = 0.25 (28.7 examples/sec; 0.279 sec/batch; 25h:27m:18s remains)
INFO - root - 2017-12-16 03:55:35.594033: step 3700, loss = 0.29, batch loss = 0.23 (27.8 examples/sec; 0.287 sec/batch; 26h:14m:22s remains)
2017-12-16 03:55:36.045509: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1726034 -2.7726922 -2.4603081 -2.56954 -2.8910947 -3.2901347 -3.5175195 -3.4923658 -3.615921 -3.5661721 -3.4288321 -3.1992636 -3.067976 -3.032444 -2.8248043][-2.9237912 -2.2768366 -1.7677786 -1.7730818 -2.0954151 -2.5410595 -2.812489 -2.8836899 -3.1208475 -3.1792085 -3.1092372 -2.9166479 -2.7102823 -2.5672293 -2.3773704][-2.6723318 -2.0077167 -1.4263759 -1.2890439 -1.4399745 -1.8688669 -2.2316644 -2.3235867 -2.5013831 -2.7529116 -2.9306521 -2.8745413 -2.6944063 -2.5071449 -2.38517][-2.6084385 -1.8263381 -1.1234367 -0.80028152 -0.73326349 -0.93299389 -1.2162585 -1.446728 -1.7770102 -2.1486652 -2.5056953 -2.7699547 -2.8248773 -2.704092 -2.5146768][-2.5117118 -1.7928991 -1.0783217 -0.63013887 -0.32255983 -0.17534447 -0.2340126 -0.42345238 -0.91046548 -1.5552952 -2.2755544 -2.8224292 -3.1033325 -3.1738062 -3.1059198][-2.4344661 -1.7290821 -1.051429 -0.42798042 0.188869 0.65843296 0.79570293 0.69354391 0.20094728 -0.74096942 -1.8706565 -2.8024268 -3.3743663 -3.5662422 -3.5984764][-2.3270712 -1.7777469 -1.221843 -0.55389929 0.32048607 1.1706667 1.6431046 1.6590133 1.1964612 0.12156057 -1.3085287 -2.7293184 -3.6517391 -3.9573741 -4.0265889][-2.0976236 -1.7304862 -1.3185344 -0.73085237 0.14164066 1.1536736 1.7369795 1.8939266 1.544456 0.39076662 -1.247776 -2.9486923 -4.1110997 -4.6006346 -4.5467696][-1.9221334 -1.6761746 -1.5154705 -1.1441009 -0.47652888 0.49648 1.1020818 1.2731833 0.93988371 -0.089537621 -1.618449 -3.3572617 -4.5674338 -5.131712 -4.9780889][-1.6745877 -1.6086991 -1.8736804 -1.8725891 -1.5172718 -0.914202 -0.46928978 -0.13651943 -0.19579744 -1.0422542 -2.4162254 -4.0120468 -5.0695629 -5.3499508 -5.0743895][-1.5159121 -1.6117761 -2.2001963 -2.6351051 -2.6817799 -2.4029257 -2.0675871 -1.9156649 -1.8711569 -2.4047084 -3.4902339 -4.8669844 -5.7787151 -5.8272552 -5.3802872][-1.5554109 -1.7183738 -2.3591392 -3.0008705 -3.4327612 -3.4709222 -3.3328424 -3.3707824 -3.4449255 -3.8587532 -4.6787663 -5.7238474 -6.411695 -6.2564273 -5.6090636][-1.7901223 -1.8241413 -2.3753052 -3.0549622 -3.7984738 -4.1808805 -4.3291283 -4.4426718 -4.575264 -4.9154034 -5.5424747 -6.276957 -6.6799088 -6.4117131 -5.7310119][-2.1335709 -1.961267 -2.2823858 -2.7894711 -3.3862123 -3.8577254 -4.1981325 -4.3941207 -4.697794 -5.1377234 -5.7330952 -6.1767588 -6.3197885 -6.0508285 -5.3439813][-2.4457035 -2.031635 -1.9919147 -2.1906559 -2.636241 -3.0513978 -3.3492637 -3.5472283 -3.9054143 -4.5224261 -5.3414946 -5.9881415 -6.2622132 -6.0635042 -5.2914371]]...]
INFO - root - 2017-12-16 03:55:38.892350: step 3710, loss = 0.21, batch loss = 0.15 (28.0 examples/sec; 0.285 sec/batch; 26h:04m:13s remains)
INFO - root - 2017-12-16 03:55:41.706538: step 3720, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.281 sec/batch; 25h:39m:29s remains)
INFO - root - 2017-12-16 03:55:44.474664: step 3730, loss = 0.24, batch loss = 0.18 (29.4 examples/sec; 0.272 sec/batch; 24h:50m:23s remains)
INFO - root - 2017-12-16 03:55:47.337027: step 3740, loss = 0.32, batch loss = 0.26 (27.3 examples/sec; 0.294 sec/batch; 26h:48m:31s remains)
INFO - root - 2017-12-16 03:55:50.154607: step 3750, loss = 0.28, batch loss = 0.23 (29.0 examples/sec; 0.276 sec/batch; 25h:10m:55s remains)
INFO - root - 2017-12-16 03:55:53.016731: step 3760, loss = 0.27, batch loss = 0.21 (27.8 examples/sec; 0.288 sec/batch; 26h:15m:50s remains)
INFO - root - 2017-12-16 03:55:55.809785: step 3770, loss = 0.28, batch loss = 0.22 (28.2 examples/sec; 0.284 sec/batch; 25h:55m:29s remains)
INFO - root - 2017-12-16 03:55:58.620465: step 3780, loss = 0.29, batch loss = 0.23 (29.7 examples/sec; 0.269 sec/batch; 24h:34m:15s remains)
INFO - root - 2017-12-16 03:56:01.479548: step 3790, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.285 sec/batch; 26h:02m:45s remains)
INFO - root - 2017-12-16 03:56:04.301135: step 3800, loss = 0.31, batch loss = 0.25 (28.8 examples/sec; 0.278 sec/batch; 25h:24m:11s remains)
2017-12-16 03:56:04.762625: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9069085 -4.9430165 -4.8290763 -4.9939423 -5.0938406 -5.216444 -5.4067364 -5.4262824 -5.568326 -5.4109635 -5.2394996 -4.8313103 -4.4184465 -4.271944 -4.1823][-3.9935026 -4.0627356 -3.8762784 -3.8038194 -3.6685143 -3.4190238 -3.4278316 -3.6088419 -3.9407408 -4.0610089 -4.0750589 -3.8936903 -3.655338 -3.4295943 -3.2836275][-3.2427707 -3.2664518 -3.0295162 -2.7205913 -2.2803338 -1.803277 -1.6859162 -1.8577776 -2.3260882 -2.7825255 -3.2153592 -3.2598491 -3.0537009 -2.8317285 -2.6460609][-2.5094509 -2.5389047 -2.3435509 -1.8040369 -1.0411785 -0.19374943 0.209826 -0.095587254 -0.92490315 -1.7247384 -2.4633467 -2.9353375 -3.0656471 -2.7631121 -2.4306865][-2.0733533 -2.1298475 -1.9552927 -1.4062302 -0.47857189 0.53807116 1.1108651 0.94016981 0.10238695 -1.0930407 -2.2823572 -2.9773338 -3.2507577 -3.0246196 -2.6691585][-1.8893957 -2.0458207 -1.8721273 -1.1115358 0.10749578 1.299407 1.9873996 1.7598662 0.80159187 -0.64054346 -2.1181614 -3.1057053 -3.5713034 -3.3472326 -2.8995523][-1.6431577 -1.6908772 -1.4362819 -0.72126818 0.56844473 1.9306321 2.7223849 2.45751 1.3783894 -0.20309019 -1.8465736 -3.0367002 -3.6534104 -3.6042867 -3.2953529][-1.199934 -1.3381536 -1.1075814 -0.3321991 0.89907169 2.0787663 2.7187519 2.4791865 1.4500728 -0.14020157 -1.8227909 -2.9998102 -3.599834 -3.6851354 -3.4627309][-0.7320044 -0.90641475 -0.77845931 -0.23542786 0.76956224 1.8087468 2.3560109 2.1225076 1.1613317 -0.35630846 -2.0129294 -3.1683109 -3.7575505 -3.8426626 -3.6642604][-0.37003422 -0.76342154 -0.91515636 -0.60479903 0.15859699 1.0058813 1.4673972 1.3772631 0.6382637 -0.70052743 -2.2040145 -3.3028264 -3.8133266 -3.8904085 -3.6920705][-0.51249051 -1.0352721 -1.4123952 -1.3896363 -0.86135411 -0.19236851 0.14243746 0.14197731 -0.36748838 -1.396589 -2.6008105 -3.5278339 -3.9611478 -4.020092 -3.7544651][-0.97213435 -1.455584 -1.9711256 -2.0750604 -1.7551394 -1.3265862 -1.1528523 -1.1000247 -1.3892903 -2.0933428 -2.9989517 -3.7123666 -4.051322 -4.105278 -3.8039961][-1.6133492 -1.9985549 -2.4458861 -2.6964979 -2.6107798 -2.2668538 -2.097388 -2.0454378 -2.2299111 -2.6279426 -3.2322774 -3.8177402 -4.0966082 -4.2033768 -3.9245851][-2.1570446 -2.2194636 -2.4279358 -2.6103764 -2.6084905 -2.5048304 -2.5350616 -2.5347557 -2.6919065 -2.9548273 -3.3278825 -3.7559631 -4.0166917 -4.2067442 -4.0533314][-2.5492086 -2.2947273 -2.222868 -2.3308127 -2.4199309 -2.4821391 -2.6837583 -2.8417118 -3.0294681 -3.1576662 -3.392632 -3.7631159 -4.027967 -4.2805996 -4.2039146]]...]
INFO - root - 2017-12-16 03:56:07.589621: step 3810, loss = 0.38, batch loss = 0.32 (29.9 examples/sec; 0.268 sec/batch; 24h:27m:05s remains)
INFO - root - 2017-12-16 03:56:10.399224: step 3820, loss = 0.24, batch loss = 0.19 (28.9 examples/sec; 0.277 sec/batch; 25h:18m:42s remains)
INFO - root - 2017-12-16 03:56:13.233180: step 3830, loss = 0.30, batch loss = 0.24 (27.6 examples/sec; 0.289 sec/batch; 26h:25m:16s remains)
INFO - root - 2017-12-16 03:56:16.057107: step 3840, loss = 0.28, batch loss = 0.22 (29.6 examples/sec; 0.270 sec/batch; 24h:40m:39s remains)
INFO - root - 2017-12-16 03:56:18.879607: step 3850, loss = 0.27, batch loss = 0.21 (27.9 examples/sec; 0.287 sec/batch; 26h:11m:28s remains)
INFO - root - 2017-12-16 03:56:21.730935: step 3860, loss = 0.38, batch loss = 0.32 (27.2 examples/sec; 0.294 sec/batch; 26h:48m:04s remains)
INFO - root - 2017-12-16 03:56:24.560815: step 3870, loss = 0.27, batch loss = 0.22 (28.7 examples/sec; 0.279 sec/batch; 25h:25m:31s remains)
INFO - root - 2017-12-16 03:56:27.425804: step 3880, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 26h:04m:09s remains)
INFO - root - 2017-12-16 03:56:30.247306: step 3890, loss = 0.27, batch loss = 0.22 (26.7 examples/sec; 0.299 sec/batch; 27h:18m:45s remains)
INFO - root - 2017-12-16 03:56:33.089449: step 3900, loss = 0.33, batch loss = 0.28 (28.8 examples/sec; 0.277 sec/batch; 25h:19m:27s remains)
2017-12-16 03:56:33.543038: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4073348 -5.7882023 -6.0801191 -6.3553982 -6.3982568 -6.2287116 -6.0423732 -5.8927269 -5.7131424 -5.3060637 -4.867095 -4.4757667 -4.0489759 -3.6728678 -3.3978655][-5.26488 -5.7661824 -6.0227804 -6.1908154 -6.0906372 -6.0062151 -5.7774816 -5.522748 -5.3228087 -4.9631042 -4.668561 -4.2271929 -3.7647083 -3.3470144 -2.9329135][-4.6332603 -5.2948542 -5.5292964 -5.440156 -5.0394287 -4.8401122 -4.6559582 -4.5971265 -4.6122637 -4.3622861 -4.0956192 -3.7070055 -3.3226113 -2.9499798 -2.5503335][-3.7482779 -4.3688774 -4.483191 -4.1053815 -3.4537778 -2.9537134 -2.6765482 -2.6579952 -2.923717 -3.1665914 -3.2951837 -3.1510198 -2.9793618 -2.6917653 -2.4255846][-2.8416841 -3.081605 -2.7798073 -2.0393794 -0.90391278 -0.090168 0.24843168 0.041173935 -0.70525312 -1.5208123 -2.2135155 -2.4863157 -2.6334772 -2.6533186 -2.6010389][-1.9649525 -1.959012 -1.3192327 -0.098203659 1.4956765 2.4540796 2.7995143 2.5025034 1.3795419 0.093316078 -1.1622348 -1.98667 -2.4604192 -2.7841229 -2.8470993][-1.6267517 -1.2865148 -0.39176178 1.0927038 2.7593436 4.0664396 4.5950451 4.2371759 2.9786553 1.2692685 -0.47803378 -1.6885169 -2.4739227 -3.06518 -3.2864189][-2.0003586 -1.5819545 -0.62175632 1.1928802 3.0324478 4.491375 5.1699162 4.8637877 3.6992073 1.8190007 -0.13680363 -1.7404923 -2.8340352 -3.4048212 -3.6896966][-3.0123353 -2.6139772 -1.8787732 -0.33397007 1.4433308 3.1930556 4.145566 4.0917969 3.0772839 1.3822494 -0.3435235 -2.1009603 -3.2093782 -3.8220119 -4.0886517][-4.35948 -4.4269748 -4.0370684 -2.8442452 -1.4523137 0.22842598 1.3235011 1.7266183 1.2215309 -0.1250515 -1.6130955 -3.1729646 -4.093678 -4.5295992 -4.5969167][-5.9953861 -6.1480303 -6.0533195 -5.3828745 -4.4931269 -3.1032257 -1.9311931 -1.3652136 -1.3713615 -2.1260219 -3.1895657 -4.3934593 -5.1463842 -5.3503575 -5.3260441][-6.9203262 -7.4003062 -7.72766 -7.1604691 -6.4147749 -5.3714585 -4.3065271 -3.675828 -3.3693337 -3.7055759 -4.2460194 -5.222631 -5.7669058 -5.773581 -5.6561918][-6.9479847 -7.4107776 -7.7518845 -7.560874 -7.2043524 -6.3265162 -5.4755473 -4.8880367 -4.5846686 -4.7071037 -4.8322716 -5.435895 -5.6596026 -5.4894457 -5.2802954][-6.199625 -6.451992 -6.5963373 -6.5451784 -6.354434 -5.8401279 -5.3648787 -4.9827919 -4.7503614 -4.7378855 -4.7751455 -5.1045265 -5.1924295 -4.8436584 -4.5437732][-5.1360111 -5.0906882 -5.0597591 -4.9498949 -4.7907271 -4.5940638 -4.38542 -4.2456026 -4.1412263 -4.15542 -4.2080183 -4.3317647 -4.3356218 -4.0796394 -3.7985029]]...]
INFO - root - 2017-12-16 03:56:36.362950: step 3910, loss = 0.36, batch loss = 0.30 (26.4 examples/sec; 0.303 sec/batch; 27h:41m:01s remains)
INFO - root - 2017-12-16 03:56:39.241456: step 3920, loss = 0.26, batch loss = 0.20 (28.4 examples/sec; 0.282 sec/batch; 25h:44m:12s remains)
INFO - root - 2017-12-16 03:56:42.078663: step 3930, loss = 0.31, batch loss = 0.25 (28.3 examples/sec; 0.283 sec/batch; 25h:50m:23s remains)
INFO - root - 2017-12-16 03:56:44.871719: step 3940, loss = 0.30, batch loss = 0.24 (27.9 examples/sec; 0.287 sec/batch; 26h:11m:19s remains)
INFO - root - 2017-12-16 03:56:47.733517: step 3950, loss = 0.41, batch loss = 0.35 (28.0 examples/sec; 0.286 sec/batch; 26h:03m:32s remains)
INFO - root - 2017-12-16 03:56:50.557681: step 3960, loss = 0.36, batch loss = 0.30 (28.6 examples/sec; 0.280 sec/batch; 25h:32m:08s remains)
INFO - root - 2017-12-16 03:56:53.375568: step 3970, loss = 0.33, batch loss = 0.28 (29.6 examples/sec; 0.270 sec/batch; 24h:38m:11s remains)
INFO - root - 2017-12-16 03:56:56.222138: step 3980, loss = 0.35, batch loss = 0.29 (28.4 examples/sec; 0.282 sec/batch; 25h:43m:42s remains)
INFO - root - 2017-12-16 03:56:59.079688: step 3990, loss = 0.46, batch loss = 0.40 (28.5 examples/sec; 0.281 sec/batch; 25h:39m:05s remains)
INFO - root - 2017-12-16 03:57:01.877516: step 4000, loss = 0.27, batch loss = 0.21 (29.1 examples/sec; 0.275 sec/batch; 25h:03m:47s remains)
2017-12-16 03:57:02.318397: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2390313 -1.2080719 -1.36609 -1.4998708 -1.7500541 -2.0179315 -2.3021829 -2.3044374 -2.2828856 -2.5162797 -2.8481483 -3.3800516 -3.8799028 -4.3948956 -4.7538118][-1.5148611 -1.5759702 -1.8174427 -2.0997717 -2.428853 -2.6914287 -2.961772 -3.1114717 -3.2102444 -3.3205316 -3.5765605 -4.1048894 -4.5636525 -5.0389366 -5.2285972][-2.5535822 -2.5810783 -2.8415616 -3.1533217 -3.4318662 -3.5607932 -3.6862333 -3.8989511 -4.0954952 -4.4207826 -4.6931543 -5.0210848 -5.4402995 -5.6328254 -5.5536523][-3.8964548 -3.9052439 -3.90758 -3.9857938 -3.9982793 -3.8692324 -3.6832767 -3.6006258 -3.7430382 -4.1517758 -4.6323843 -4.9948444 -5.27597 -5.5556946 -5.6029143][-4.651372 -4.856235 -4.8545437 -4.4838266 -3.9328659 -3.3171036 -2.7412403 -2.3135352 -2.2283006 -2.5412929 -3.0487509 -3.7778616 -4.5236959 -4.8894534 -5.1681137][-4.9992657 -5.0017262 -4.8093953 -4.2742453 -3.4505897 -2.3687255 -1.2204468 -0.50350857 -0.27093554 -0.4297471 -0.99287224 -1.8403661 -2.8579369 -3.8453317 -4.5535693][-4.4040589 -4.3201056 -3.978435 -3.3850563 -2.4231191 -0.9934032 0.520946 1.5529299 2.0409851 1.9115806 1.275146 0.10564137 -1.3321824 -2.6109436 -3.70292][-3.5155888 -3.2998149 -2.7715364 -2.1861215 -1.2502944 0.17699051 1.8187575 3.1108341 3.8429918 3.6995983 2.7990508 1.407455 -0.24473715 -1.8728001 -3.2161431][-2.3184025 -2.3089364 -2.0969577 -1.5068827 -0.64489484 0.54094505 1.9241161 3.1394939 3.8700352 3.7458916 2.8958344 1.4537754 -0.3059659 -1.9352925 -3.3111298][-2.2618234 -1.8275359 -1.5123472 -1.2488108 -0.689137 0.21312523 1.2344565 2.037847 2.4390869 2.2851844 1.5680466 0.36301088 -1.148093 -2.521193 -3.6691046][-2.5737691 -1.9937975 -1.485867 -1.2357838 -0.88496327 -0.21542215 0.500402 0.9085536 0.9142909 0.63840628 0.0071249008 -0.93281174 -1.983216 -3.017837 -3.9217808][-2.9686399 -2.207716 -1.6257458 -1.2938166 -0.97408724 -0.65447426 -0.33765221 -0.21102667 -0.3513999 -0.67520165 -1.1852813 -1.9229453 -2.7155304 -3.2662005 -3.7786717][-3.467222 -2.4705591 -1.7364466 -1.2748542 -1.0530422 -0.914907 -0.89751649 -0.98635459 -1.2112067 -1.5396497 -1.8907659 -2.2519906 -2.6108274 -2.9207668 -3.2528315][-3.7916803 -2.5691104 -1.6240921 -0.969512 -0.61278987 -0.5344708 -0.55196834 -0.76498461 -1.03405 -1.209028 -1.4734178 -1.809387 -2.0719631 -2.1155059 -2.3188844][-3.6450644 -2.4419935 -1.3445587 -0.61051893 -0.1431632 0.0838027 0.13654661 0.0071396828 -0.15754128 -0.309834 -0.52824783 -0.72528982 -0.9292922 -1.195416 -1.521425]]...]
INFO - root - 2017-12-16 03:57:05.149288: step 4010, loss = 0.33, batch loss = 0.27 (29.0 examples/sec; 0.276 sec/batch; 25h:12m:21s remains)
INFO - root - 2017-12-16 03:57:07.985739: step 4020, loss = 0.33, batch loss = 0.27 (28.2 examples/sec; 0.283 sec/batch; 25h:50m:50s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 03:57:10.805503: step 4030, loss = 0.29, batch loss = 0.23 (28.9 examples/sec; 0.277 sec/batch; 25h:15m:50s remains)
INFO - root - 2017-12-16 03:57:13.603480: step 4040, loss = 0.29, batch loss = 0.23 (27.6 examples/sec; 0.290 sec/batch; 26h:26m:46s remains)
INFO - root - 2017-12-16 03:57:16.446225: step 4050, loss = 0.29, batch loss = 0.23 (28.0 examples/sec; 0.286 sec/batch; 26h:05m:43s remains)
INFO - root - 2017-12-16 03:57:19.219819: step 4060, loss = 0.31, batch loss = 0.25 (28.6 examples/sec; 0.280 sec/batch; 25h:32m:27s remains)
INFO - root - 2017-12-16 03:57:22.083785: step 4070, loss = 0.31, batch loss = 0.26 (28.8 examples/sec; 0.278 sec/batch; 25h:19m:12s remains)
INFO - root - 2017-12-16 03:57:24.940273: step 4080, loss = 0.31, batch loss = 0.25 (29.4 examples/sec; 0.272 sec/batch; 24h:51m:10s remains)
INFO - root - 2017-12-16 03:57:27.719004: step 4090, loss = 0.23, batch loss = 0.17 (27.8 examples/sec; 0.288 sec/batch; 26h:17m:24s remains)
INFO - root - 2017-12-16 03:57:30.514501: step 4100, loss = 0.32, batch loss = 0.26 (29.4 examples/sec; 0.273 sec/batch; 24h:51m:35s remains)
2017-12-16 03:57:30.993209: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1741903 -3.1536665 -3.1838572 -3.2753375 -3.4729996 -3.7382417 -4.0689764 -4.4412065 -4.7102203 -4.8070793 -4.7415295 -4.6058884 -4.336731 -3.9349203 -3.5616727][-3.1938586 -3.2271674 -3.3278742 -3.4958115 -3.7701561 -4.1270857 -4.5289021 -4.8405638 -5.0401731 -5.2125397 -5.2740684 -5.1149855 -4.7750683 -4.36388 -3.933073][-3.135715 -3.2470591 -3.4985561 -3.7876804 -4.0811472 -4.3411431 -4.5655136 -4.8745289 -5.1401834 -5.2334614 -5.2990069 -5.317812 -5.2452836 -4.9436436 -4.4782972][-3.1998229 -3.4345243 -3.7591994 -4.0766325 -4.2592278 -4.2849731 -4.1624126 -4.0961008 -4.1344051 -4.368896 -4.7163639 -4.9985037 -5.2334142 -5.2578621 -5.0148268][-3.501719 -3.6924236 -3.7844636 -3.7987092 -3.5452268 -3.0906012 -2.384649 -1.8452365 -1.8366077 -2.2221541 -2.9832046 -3.9778783 -4.8496051 -5.2806139 -5.290051][-3.5119946 -3.6708159 -3.5934911 -3.2293615 -2.3397412 -1.1990955 0.062574387 1.0168319 1.163085 0.49752522 -0.85659838 -2.4895768 -4.0430107 -5.0601311 -5.3927259][-3.2508037 -3.2749805 -2.9131639 -2.28459 -1.0986168 0.48202038 2.1872463 3.3171883 3.5387158 2.7582374 1.1113729 -1.0335658 -3.1083333 -4.4926229 -5.0624285][-2.8060699 -2.8467629 -2.4166977 -1.4512186 0.040420532 1.8795791 3.7952108 4.9846172 5.0380917 3.8438597 1.8734422 -0.33952713 -2.4388032 -4.0073819 -4.6843133][-2.556457 -2.7052889 -2.3263924 -1.4726572 -0.12476206 1.606463 3.3294897 4.380127 4.381897 3.2669415 1.3598514 -0.91095424 -2.8800197 -4.1336865 -4.5678282][-2.3453007 -2.6079521 -2.6926422 -2.0918548 -0.95532274 0.20055723 1.2028847 1.9569011 1.9573193 0.83596659 -0.71302462 -2.4525409 -3.8863471 -4.6548486 -4.809979][-2.5878885 -2.8423009 -3.08688 -2.9711585 -2.478821 -1.5618241 -0.81306314 -0.70416045 -1.0574782 -1.8446136 -2.8256292 -3.9603989 -4.7373877 -4.9728346 -4.752759][-3.0102396 -3.2331357 -3.552177 -3.6355684 -3.4679043 -3.3039396 -3.3025358 -3.4117627 -3.6680212 -4.1267738 -4.657042 -5.1764212 -5.2827215 -5.07684 -4.6041865][-3.2853308 -3.4144945 -3.6373174 -3.8526604 -4.1424556 -4.3559642 -4.5753841 -4.9973741 -5.3623266 -5.5931005 -5.6774521 -5.7069669 -5.4810266 -5.1060987 -4.5835328][-3.6319106 -3.6182528 -3.6812935 -3.8672769 -4.1529932 -4.4580092 -4.8909163 -5.2488103 -5.4189577 -5.5318537 -5.4432607 -5.3208351 -5.106946 -4.7362494 -4.2503672][-3.955667 -3.7768357 -3.6682675 -3.6269813 -3.6735539 -3.9737601 -4.3042793 -4.5196824 -4.6191578 -4.6493778 -4.55657 -4.3957233 -4.2268958 -4.1296029 -3.897851]]...]
INFO - root - 2017-12-16 03:57:33.811542: step 4110, loss = 0.30, batch loss = 0.24 (28.6 examples/sec; 0.279 sec/batch; 25h:28m:50s remains)
INFO - root - 2017-12-16 03:57:36.691725: step 4120, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.286 sec/batch; 26h:03m:47s remains)
INFO - root - 2017-12-16 03:57:39.478988: step 4130, loss = 0.25, batch loss = 0.19 (28.8 examples/sec; 0.278 sec/batch; 25h:22m:40s remains)
INFO - root - 2017-12-16 03:57:42.317929: step 4140, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.279 sec/batch; 25h:27m:13s remains)
INFO - root - 2017-12-16 03:57:45.133232: step 4150, loss = 0.25, batch loss = 0.20 (25.6 examples/sec; 0.313 sec/batch; 28h:31m:49s remains)
INFO - root - 2017-12-16 03:57:47.925701: step 4160, loss = 0.28, batch loss = 0.22 (29.2 examples/sec; 0.274 sec/batch; 24h:57m:45s remains)
INFO - root - 2017-12-16 03:57:50.763848: step 4170, loss = 0.27, batch loss = 0.21 (25.4 examples/sec; 0.314 sec/batch; 28h:40m:49s remains)
INFO - root - 2017-12-16 03:57:53.622757: step 4180, loss = 0.35, batch loss = 0.29 (28.1 examples/sec; 0.285 sec/batch; 25h:59m:38s remains)
INFO - root - 2017-12-16 03:57:56.454120: step 4190, loss = 0.24, batch loss = 0.18 (29.1 examples/sec; 0.275 sec/batch; 25h:03m:53s remains)
INFO - root - 2017-12-16 03:57:59.274460: step 4200, loss = 0.29, batch loss = 0.23 (28.5 examples/sec; 0.280 sec/batch; 25h:33m:52s remains)
2017-12-16 03:57:59.752581: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9638672 -4.0152984 -4.0178847 -4.1250911 -4.4257059 -4.6446657 -4.867435 -5.0079756 -5.0070271 -4.794342 -4.3824811 -4.0071654 -3.5863862 -3.2406802 -3.0362055][-4.14177 -4.1482563 -4.1023364 -4.2389064 -4.5671654 -4.8458238 -5.0864487 -5.2054963 -5.1720161 -4.9736114 -4.6145511 -4.2007923 -3.7648265 -3.3924651 -3.1139472][-3.9765921 -3.9591298 -3.9348829 -4.0688944 -4.3620653 -4.6057658 -4.8453631 -5.0555668 -5.0657263 -4.9592147 -4.7420359 -4.4498353 -4.1003957 -3.722553 -3.4115157][-3.4020419 -3.3604095 -3.3501143 -3.4711049 -3.7109032 -3.8605168 -3.9634047 -4.0716448 -4.0945492 -4.1085496 -4.1859779 -4.2691402 -4.2288513 -4.0457554 -3.7419057][-2.7871418 -2.5724063 -2.3346744 -2.3097355 -2.331296 -2.2733009 -2.3137898 -2.4069664 -2.5843146 -2.9216979 -3.4688716 -4.0336289 -4.337502 -4.3622179 -4.0851398][-1.7772155 -1.4456067 -1.0948381 -0.80125809 -0.58458281 -0.37886429 -0.29964972 -0.49175239 -0.91933179 -1.6910152 -2.7440064 -3.7944632 -4.401433 -4.5573044 -4.3260202][-1.1432831 -0.68644786 -0.32495308 0.12048197 0.60613489 1.0004454 1.2479229 1.1928864 0.7415266 -0.25124645 -1.6260514 -3.0701509 -4.0450726 -4.4403753 -4.3498373][-0.91357017 -0.48146796 -0.1420846 0.36503029 0.99058485 1.5837975 1.852716 1.7936749 1.4394112 0.44124794 -0.99255514 -2.4538927 -3.4758282 -4.0073776 -4.0797949][-1.3059518 -0.94128156 -0.69908428 -0.13684797 0.53499794 1.2032423 1.5836058 1.6272159 1.3700233 0.40487576 -0.92475772 -2.3331208 -3.3162136 -3.7911081 -3.9142592][-1.8566175 -1.6089721 -1.5471783 -1.1385987 -0.61467147 -0.17608786 0.11759996 0.23474693 0.11050463 -0.53236747 -1.4859142 -2.5683377 -3.3812943 -3.7900724 -3.8513694][-2.4370561 -2.392719 -2.4632623 -2.3578372 -2.0347953 -1.6085918 -1.3108196 -1.1194339 -1.0209279 -1.3358507 -1.9990222 -2.7780216 -3.3755803 -3.6883688 -3.7509716][-2.8881354 -2.9812903 -3.2476602 -3.3411031 -3.1960263 -2.854075 -2.4258149 -2.1215281 -1.8439562 -1.9075611 -2.3098059 -2.9533029 -3.5160365 -3.7528951 -3.7841091][-2.7775412 -2.8526332 -3.2119536 -3.5137036 -3.5516469 -3.4208407 -3.1096816 -2.7481554 -2.4570305 -2.4148462 -2.6618 -3.1237736 -3.5744684 -3.770154 -3.7246037][-2.0909722 -2.1628947 -2.5429108 -3.0922947 -3.3810644 -3.3803344 -3.1305294 -2.8588145 -2.58104 -2.4506912 -2.7172544 -3.1465914 -3.5453453 -3.7298362 -3.6710892][-1.5698113 -1.5968645 -2.0118074 -2.674427 -3.0744991 -3.1052079 -2.7451272 -2.331893 -2.0617902 -2.0140622 -2.3340158 -2.8928032 -3.3432121 -3.5368676 -3.5007787]]...]
INFO - root - 2017-12-16 03:58:02.553145: step 4210, loss = 0.30, batch loss = 0.25 (29.6 examples/sec; 0.270 sec/batch; 24h:38m:17s remains)
INFO - root - 2017-12-16 03:58:05.379117: step 4220, loss = 0.29, batch loss = 0.24 (25.9 examples/sec; 0.309 sec/batch; 28h:08m:21s remains)
INFO - root - 2017-12-16 03:58:08.226076: step 4230, loss = 0.25, batch loss = 0.19 (28.3 examples/sec; 0.283 sec/batch; 25h:47m:52s remains)
INFO - root - 2017-12-16 03:58:11.038106: step 4240, loss = 0.26, batch loss = 0.20 (28.2 examples/sec; 0.284 sec/batch; 25h:52m:57s remains)
INFO - root - 2017-12-16 03:58:13.838753: step 4250, loss = 0.32, batch loss = 0.26 (29.2 examples/sec; 0.274 sec/batch; 24h:57m:12s remains)
INFO - root - 2017-12-16 03:58:16.668793: step 4260, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.285 sec/batch; 25h:59m:27s remains)
INFO - root - 2017-12-16 03:58:19.496753: step 4270, loss = 0.27, batch loss = 0.21 (29.2 examples/sec; 0.274 sec/batch; 24h:56m:13s remains)
INFO - root - 2017-12-16 03:58:22.305236: step 4280, loss = 0.26, batch loss = 0.20 (29.0 examples/sec; 0.276 sec/batch; 25h:10m:14s remains)
INFO - root - 2017-12-16 03:58:25.146953: step 4290, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.280 sec/batch; 25h:33m:25s remains)
INFO - root - 2017-12-16 03:58:27.973121: step 4300, loss = 0.27, batch loss = 0.22 (27.4 examples/sec; 0.292 sec/batch; 26h:36m:21s remains)
2017-12-16 03:58:28.426000: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.093823 -3.5803251 -4.2180815 -4.549438 -4.7692418 -5.0321989 -5.138103 -5.18424 -5.1160707 -4.7659097 -4.3297038 -4.1064191 -3.8978944 -3.6584663 -3.4159977][-2.6759987 -2.8803158 -3.3203974 -3.9622829 -4.5659685 -4.9113593 -5.1121955 -5.1771631 -5.10587 -5.103961 -5.0094943 -4.6225181 -4.2345672 -4.0399823 -3.8501961][-1.0267577 -1.4716511 -2.2890816 -3.0061235 -3.7594678 -4.2919059 -4.6290064 -4.92899 -5.2125759 -5.21665 -5.19371 -5.1619296 -4.8750367 -4.5551605 -4.2052755][0.2555232 0.14271259 -0.56179857 -1.6158257 -2.5397782 -3.2607436 -3.8409674 -3.9625511 -4.0490184 -4.5123005 -4.8883548 -5.0984221 -5.1314445 -4.9177871 -4.5711107][1.9951324 1.8059783 0.85147381 -0.33051538 -1.4327796 -1.9870031 -2.2934139 -2.7067785 -3.1347084 -3.6037982 -4.1667047 -4.5948129 -4.6842427 -4.5663967 -4.2451591][2.9669156 2.7656264 1.7084336 0.64170694 -0.12693167 -0.66601682 -0.95262766 -1.2858284 -1.8257856 -2.6252139 -3.3520002 -4.0104079 -4.258235 -4.2001109 -4.0833154][2.6705213 2.5645471 1.8686986 1.0420127 0.53511715 0.47579956 0.53052044 0.32632351 -0.32394743 -1.4749134 -2.6501327 -3.5407071 -4.058466 -3.9754171 -3.5217004][1.5267854 1.3677216 0.90615511 0.77791643 0.7992568 1.2694287 1.6196561 1.3368883 0.66984177 -0.60167217 -2.052516 -3.1326818 -3.56388 -3.45544 -3.12421][-0.71541739 -0.46528912 -0.44380093 -0.30914974 0.21158266 0.86219931 1.3337526 1.3537245 0.74948025 -0.50942922 -1.7255135 -2.7391965 -3.1120083 -2.6575041 -2.0276175][-2.6984215 -2.7066092 -2.4215033 -1.8718603 -1.1813545 -0.25464106 0.41587734 0.32181931 -0.156044 -1.1618745 -2.2056224 -2.7850995 -2.6474702 -1.9726667 -1.2811534][-4.5201025 -4.4303594 -4.2569966 -3.56176 -2.447135 -1.5803273 -0.97693658 -0.89695477 -1.1474295 -1.8879094 -2.6596451 -2.925674 -2.4771714 -1.7657917 -1.1620581][-5.4339209 -5.6829944 -5.6149492 -5.17211 -4.4531851 -3.2403221 -2.1544681 -1.821151 -1.8899539 -2.2603049 -2.4770868 -2.5798333 -2.2837152 -1.6425924 -1.181004][-5.9053774 -6.3264346 -6.542861 -6.2814112 -5.3057179 -4.198885 -3.1818504 -2.2409284 -1.6748488 -1.8099213 -2.0744412 -1.8683558 -1.3416789 -0.89173651 -0.74331164][-5.4956455 -6.1544104 -6.5282497 -6.5005465 -5.7809868 -4.5851903 -3.1645203 -2.1545334 -1.5931244 -1.3053422 -1.3217313 -1.0340765 -0.55857348 -0.13475943 -0.24081135][-4.6205282 -5.5206652 -6.3754344 -6.5314198 -5.6784449 -4.6519732 -3.3971236 -1.9812295 -1.1010876 -0.82351732 -0.89851379 -0.80091572 -0.48643231 -0.15553093 -0.088653564]]...]
INFO - root - 2017-12-16 03:58:31.223002: step 4310, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.286 sec/batch; 26h:03m:46s remains)
INFO - root - 2017-12-16 03:58:33.997891: step 4320, loss = 0.32, batch loss = 0.26 (28.2 examples/sec; 0.283 sec/batch; 25h:49m:05s remains)
INFO - root - 2017-12-16 03:58:36.837819: step 4330, loss = 0.20, batch loss = 0.14 (29.3 examples/sec; 0.273 sec/batch; 24h:50m:53s remains)
INFO - root - 2017-12-16 03:58:39.644927: step 4340, loss = 0.33, batch loss = 0.27 (29.4 examples/sec; 0.272 sec/batch; 24h:48m:03s remains)
INFO - root - 2017-12-16 03:58:42.457645: step 4350, loss = 0.21, batch loss = 0.15 (29.6 examples/sec; 0.271 sec/batch; 24h:40m:28s remains)
INFO - root - 2017-12-16 03:58:45.247322: step 4360, loss = 0.23, batch loss = 0.17 (28.6 examples/sec; 0.280 sec/batch; 25h:31m:34s remains)
INFO - root - 2017-12-16 03:58:48.072844: step 4370, loss = 0.23, batch loss = 0.17 (29.6 examples/sec; 0.270 sec/batch; 24h:35m:37s remains)
INFO - root - 2017-12-16 03:58:50.879944: step 4380, loss = 0.33, batch loss = 0.28 (28.2 examples/sec; 0.284 sec/batch; 25h:51m:12s remains)
INFO - root - 2017-12-16 03:58:53.713192: step 4390, loss = 0.26, batch loss = 0.20 (27.8 examples/sec; 0.288 sec/batch; 26h:14m:42s remains)
INFO - root - 2017-12-16 03:58:56.584540: step 4400, loss = 0.30, batch loss = 0.24 (27.1 examples/sec; 0.295 sec/batch; 26h:53m:44s remains)
2017-12-16 03:58:57.046760: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.564127 -4.6940184 -4.8294821 -4.85295 -4.7400746 -4.586627 -4.5031371 -4.4548483 -4.5104589 -4.5106483 -4.3974833 -4.14205 -3.8397083 -3.7832556 -3.9170403][-4.5315967 -4.6938772 -4.8406568 -4.9665413 -4.9318643 -4.9334965 -4.9516745 -4.7278695 -4.5041776 -4.3696713 -4.2510004 -3.8079686 -3.2717009 -3.167284 -3.5162196][-4.5052123 -4.5310588 -4.5479116 -4.8007069 -4.8883219 -4.8140526 -4.6191196 -4.3081794 -4.0343108 -3.7283916 -3.4843302 -3.1375144 -2.8604412 -2.7789578 -3.0815308][-4.1995044 -4.1830072 -4.1601305 -4.2456918 -4.1693697 -4.0048985 -3.6842239 -3.0005221 -2.4374738 -2.1342318 -2.0301538 -1.8236389 -1.6598177 -1.7295866 -2.0947785][-3.4428375 -3.2484288 -3.0167499 -2.9732723 -2.6795006 -2.2767811 -1.7281985 -1.0310051 -0.54607058 -0.24766397 -0.27077341 -0.41761112 -0.59249878 -0.93815804 -1.2947152][-2.8456931 -2.5979524 -2.2626579 -1.9243512 -1.2007318 -0.56240845 0.20612764 0.82772207 1.1321588 1.1797452 0.81317329 0.39089489 0.014632225 -0.39472914 -0.61060667][-2.860147 -2.4562278 -1.8966434 -1.1609933 -0.16474915 0.67206907 1.471158 1.9920797 2.1573367 1.9880857 1.4586496 0.97553921 0.53172016 0.13164473 0.045165062][-2.9642706 -2.4447851 -1.7683265 -0.91371441 0.15568161 1.1782227 1.9942169 2.33817 2.3592238 2.0215797 1.440124 0.95684814 0.62672853 0.37089539 0.3749938][-3.1470127 -2.4885025 -1.804069 -1.0994184 -0.16170835 0.731122 1.4604745 1.8527131 1.8703842 1.544241 1.1403427 0.838181 0.6875577 0.54647923 0.6051836][-3.2257342 -2.9270539 -2.5428514 -1.9849329 -1.1675558 -0.5647459 -0.023350716 0.40008688 0.49588203 0.35699224 0.16683388 0.1505332 0.23880959 0.23789263 0.3432188][-3.6937962 -3.603982 -3.501615 -3.2536168 -2.7917013 -2.2779329 -1.6588049 -1.2222834 -1.0219162 -0.88236213 -0.80452847 -0.66904712 -0.44675803 -0.37545824 -0.33588743][-4.0035672 -4.2208481 -4.4693909 -4.5960321 -4.4682622 -4.20136 -3.8006859 -3.2006047 -2.6527524 -2.195864 -1.9155314 -1.6079526 -1.3482933 -1.2156844 -1.1945999][-4.3343964 -4.8925605 -5.3993096 -5.8641291 -6.0506153 -5.9999638 -5.7074604 -5.1059842 -4.512763 -3.8222806 -3.135087 -2.5989766 -2.2972953 -2.0496414 -1.9013801][-4.7035022 -5.3639684 -6.0206509 -6.7084637 -7.1218991 -7.3160124 -7.2071276 -6.5940228 -5.9236584 -5.1271496 -4.4212461 -3.7320981 -3.2083073 -2.9168282 -2.7591658][-4.6248631 -5.32743 -6.0662937 -6.7739458 -7.26939 -7.5382242 -7.5230746 -7.1167727 -6.6415606 -6.035984 -5.4556484 -4.7851729 -4.269031 -3.9499125 -3.7233834]]...]
INFO - root - 2017-12-16 03:58:59.853786: step 4410, loss = 0.29, batch loss = 0.23 (28.9 examples/sec; 0.277 sec/batch; 25h:13m:27s remains)
INFO - root - 2017-12-16 03:59:02.697170: step 4420, loss = 0.35, batch loss = 0.30 (26.2 examples/sec; 0.305 sec/batch; 27h:47m:57s remains)
INFO - root - 2017-12-16 03:59:05.536931: step 4430, loss = 0.30, batch loss = 0.25 (28.5 examples/sec; 0.280 sec/batch; 25h:32m:42s remains)
INFO - root - 2017-12-16 03:59:08.354778: step 4440, loss = 0.25, batch loss = 0.20 (29.5 examples/sec; 0.271 sec/batch; 24h:41m:37s remains)
INFO - root - 2017-12-16 03:59:11.232924: step 4450, loss = 0.33, batch loss = 0.27 (28.4 examples/sec; 0.282 sec/batch; 25h:40m:46s remains)
INFO - root - 2017-12-16 03:59:14.034569: step 4460, loss = 0.40, batch loss = 0.34 (29.2 examples/sec; 0.274 sec/batch; 24h:57m:02s remains)
INFO - root - 2017-12-16 03:59:16.865397: step 4470, loss = 0.30, batch loss = 0.24 (29.0 examples/sec; 0.276 sec/batch; 25h:06m:50s remains)
INFO - root - 2017-12-16 03:59:19.685516: step 4480, loss = 0.23, batch loss = 0.17 (27.2 examples/sec; 0.294 sec/batch; 26h:49m:20s remains)
INFO - root - 2017-12-16 03:59:22.499227: step 4490, loss = 0.24, batch loss = 0.19 (28.2 examples/sec; 0.283 sec/batch; 25h:48m:54s remains)
INFO - root - 2017-12-16 03:59:25.351532: step 4500, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.285 sec/batch; 26h:00m:30s remains)
2017-12-16 03:59:25.815305: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1791131 -3.7167737 -4.2952237 -4.8196793 -5.2553577 -5.5940838 -5.89771 -6.3690233 -6.7182369 -6.7845836 -6.4384546 -5.9339061 -5.3046088 -4.5683947 -4.1951041][-3.6227822 -4.3114247 -4.9458694 -5.32106 -5.509882 -5.6780615 -5.9026155 -6.1360292 -6.3016706 -6.5438442 -6.6250296 -6.2707038 -5.6539521 -4.8876514 -4.3567424][-4.2045789 -4.9944935 -5.5553641 -5.720243 -5.4947357 -5.103147 -4.8661203 -4.8936453 -5.1032934 -5.4319172 -5.750474 -5.86882 -5.4375248 -4.73914 -4.1913509][-4.568234 -5.3839097 -5.8695049 -5.6871581 -4.99466 -3.964905 -3.1022005 -2.7186089 -2.9041035 -3.6017828 -4.2611303 -4.7711921 -4.8390832 -4.4130368 -3.9293334][-4.5143433 -5.1641788 -5.2680821 -4.6152973 -3.4638329 -2.008662 -0.8444562 -0.28627062 -0.54660892 -1.4308884 -2.5190864 -3.3564641 -3.7409298 -3.7191482 -3.5036597][-4.3681126 -4.729516 -4.5454855 -3.4330688 -1.7338068 0.084608078 1.5454969 2.2020411 1.7663236 0.56326246 -0.92076993 -2.02468 -2.5368381 -2.6890001 -2.5409284][-4.1349335 -4.2604771 -3.9290283 -2.669585 -0.82271171 1.2958064 3.0000792 3.6779375 3.1439347 1.8406358 0.41199112 -0.80668664 -1.3733802 -1.5179081 -1.3376369][-3.7752168 -3.8011503 -3.5293002 -2.337456 -0.52618814 1.4861703 3.0196562 3.6413155 3.2677898 2.0793324 0.75648832 -0.1011939 -0.28702402 -0.16816902 0.13799429][-3.6329341 -3.7228532 -3.4817169 -2.5773511 -1.3396204 0.35667086 1.7212958 2.3851814 2.2077236 1.3985868 0.57614279 0.10439444 0.12718916 0.48173523 1.1371913][-3.8292732 -4.1317358 -4.4483223 -3.9441435 -3.00874 -1.7913957 -0.76898742 -0.044504166 0.088862419 -0.092310905 -0.21005678 -0.10723734 0.43682098 0.94557714 1.3631473][-4.5348177 -5.1208873 -5.6323347 -5.6465993 -5.3448963 -4.5279269 -3.7008424 -2.9700327 -2.5412331 -2.2134612 -1.7231681 -0.98350358 -0.096875668 0.49426603 0.79273462][-5.2032576 -6.0896826 -6.8931541 -7.2299595 -7.2902918 -6.8996296 -6.5050259 -5.9159336 -5.2304964 -4.4086556 -3.5165133 -2.5038271 -1.5305119 -0.91814089 -0.73896551][-5.670464 -6.7911816 -7.7917118 -8.3888273 -8.7427073 -8.7028065 -8.5764723 -8.1053123 -7.4582567 -6.61718 -5.557045 -4.4465714 -3.6089807 -3.1512229 -2.9414942][-5.8666129 -6.8643379 -7.80552 -8.6107273 -9.189249 -9.39584 -9.3461361 -9.1127625 -8.7636662 -8.1027727 -7.1986561 -6.27268 -5.4716311 -5.0165243 -4.9054704][-5.5894623 -6.3667097 -7.14647 -7.941658 -8.6026955 -9.04216 -9.2473783 -9.2585011 -9.0087547 -8.6196556 -8.0624084 -7.3935089 -6.8117156 -6.4381781 -6.0955954]]...]
INFO - root - 2017-12-16 03:59:28.608013: step 4510, loss = 0.27, batch loss = 0.22 (29.3 examples/sec; 0.273 sec/batch; 24h:51m:29s remains)
INFO - root - 2017-12-16 03:59:31.383788: step 4520, loss = 0.30, batch loss = 0.25 (29.2 examples/sec; 0.274 sec/batch; 24h:55m:26s remains)
INFO - root - 2017-12-16 03:59:34.208475: step 4530, loss = 0.33, batch loss = 0.27 (28.6 examples/sec; 0.280 sec/batch; 25h:30m:27s remains)
INFO - root - 2017-12-16 03:59:36.994016: step 4540, loss = 0.28, batch loss = 0.23 (27.9 examples/sec; 0.287 sec/batch; 26h:08m:10s remains)
INFO - root - 2017-12-16 03:59:39.798747: step 4550, loss = 0.25, batch loss = 0.19 (27.5 examples/sec; 0.291 sec/batch; 26h:30m:42s remains)
INFO - root - 2017-12-16 03:59:42.600349: step 4560, loss = 0.29, batch loss = 0.24 (28.5 examples/sec; 0.281 sec/batch; 25h:33m:20s remains)
INFO - root - 2017-12-16 03:59:45.384703: step 4570, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.283 sec/batch; 25h:49m:02s remains)
INFO - root - 2017-12-16 03:59:48.221544: step 4580, loss = 0.20, batch loss = 0.14 (27.5 examples/sec; 0.291 sec/batch; 26h:29m:01s remains)
INFO - root - 2017-12-16 03:59:50.998653: step 4590, loss = 0.30, batch loss = 0.24 (28.0 examples/sec; 0.286 sec/batch; 26h:00m:41s remains)
INFO - root - 2017-12-16 03:59:53.814770: step 4600, loss = 0.28, batch loss = 0.22 (28.9 examples/sec; 0.277 sec/batch; 25h:13m:36s remains)
2017-12-16 03:59:54.273519: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5651312 -3.6683156 -3.5472512 -3.3734255 -3.2091813 -2.9164698 -2.8423042 -2.6232872 -2.4677775 -2.5744658 -2.6802392 -2.7369776 -2.6682034 -2.6500218 -2.5600619][-2.4848356 -2.5746453 -2.5712042 -2.2829127 -1.9321315 -1.5790877 -1.3180044 -1.3683536 -1.4340682 -1.5422831 -1.8697875 -2.1511056 -2.2871017 -2.152009 -1.9086623][-1.50701 -1.3578713 -1.3428459 -1.0851531 -0.7766695 -0.5182066 -0.33845615 -0.10301876 -0.09922266 -0.75612712 -1.5927491 -2.0234008 -2.2381344 -2.2435374 -1.9337187][-0.70437384 -0.74526954 -1.0048962 -0.81123662 -0.3584137 0.1143446 0.45818853 0.45594025 0.20314121 -0.27720737 -1.0455575 -1.9692838 -2.4996245 -2.4720058 -2.177979][-0.75891876 -0.95534086 -1.1063335 -1.161577 -0.79447341 -0.24380207 0.323627 0.60685682 0.3750186 -0.33759737 -1.2954695 -2.1075749 -2.7087903 -3.0073762 -2.9257405][-1.180052 -1.6199534 -1.9292722 -1.7351401 -1.0700676 -0.3695364 0.37866211 0.86807489 0.81446457 0.010791779 -1.2048497 -2.2228215 -3.0759082 -3.5517294 -3.5815411][-1.7845924 -2.1235657 -2.5097671 -2.258909 -1.3864994 -0.43033195 0.49820995 1.1179061 1.1667204 0.46006823 -0.73302388 -1.8977234 -2.9195344 -3.5188553 -3.6472862][-2.4984188 -2.7711735 -2.9556279 -2.5625515 -1.6123502 -0.47267294 0.59227419 1.1080813 1.0947471 0.53198528 -0.5526557 -1.7852232 -2.7794087 -3.3112302 -3.3984575][-3.1435223 -3.2604222 -3.1490347 -2.6711264 -1.8810384 -0.92027712 0.10417938 0.58939886 0.46730328 -0.066441059 -0.79662371 -1.7945559 -2.5642791 -2.7765732 -2.4821758][-3.4631405 -3.5507765 -3.3577237 -2.8773012 -2.2043877 -1.4930682 -0.79145622 -0.35502148 -0.49365139 -1.1339419 -1.8215842 -2.3472655 -2.5657809 -2.5623631 -2.0673914][-3.6377268 -3.6548183 -3.4832473 -3.0001307 -2.4361725 -1.952271 -1.5103347 -1.3893242 -1.744487 -2.3985789 -2.9871931 -3.4228134 -3.5143125 -3.1897862 -2.3368413][-3.8101213 -3.6060877 -3.3644888 -3.0618792 -2.7498722 -2.5300744 -2.4199235 -2.5867958 -3.069562 -3.8010335 -4.3369212 -4.6339793 -4.6044664 -4.1281757 -3.0655456][-3.7350447 -3.4417677 -3.0747252 -2.8039224 -2.7333922 -2.773612 -2.9231167 -3.3348742 -4.0117416 -4.8128214 -5.4147348 -5.7104397 -5.5671477 -5.0752163 -3.9190683][-3.681756 -3.422807 -3.074012 -2.7536776 -2.6581616 -2.794632 -3.143307 -3.5942187 -4.3595481 -5.3203545 -6.1200409 -6.5053978 -6.4642324 -6.1833582 -5.279048][-3.833889 -3.4455395 -3.0469332 -2.7501707 -2.631222 -2.6364093 -2.9720201 -3.5904255 -4.4445496 -5.5326071 -6.550961 -7.1964159 -7.3892097 -7.3519506 -6.6594486]]...]
INFO - root - 2017-12-16 03:59:57.110282: step 4610, loss = 0.24, batch loss = 0.18 (27.5 examples/sec; 0.291 sec/batch; 26h:32m:19s remains)
INFO - root - 2017-12-16 03:59:59.956202: step 4620, loss = 0.32, batch loss = 0.26 (29.7 examples/sec; 0.270 sec/batch; 24h:32m:49s remains)
INFO - root - 2017-12-16 04:00:02.772744: step 4630, loss = 0.24, batch loss = 0.19 (29.0 examples/sec; 0.276 sec/batch; 25h:08m:17s remains)
INFO - root - 2017-12-16 04:00:05.639687: step 4640, loss = 0.22, batch loss = 0.16 (26.9 examples/sec; 0.297 sec/batch; 27h:04m:58s remains)
INFO - root - 2017-12-16 04:00:08.503069: step 4650, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 25h:49m:46s remains)
INFO - root - 2017-12-16 04:00:11.339028: step 4660, loss = 0.51, batch loss = 0.46 (28.2 examples/sec; 0.284 sec/batch; 25h:49m:29s remains)
INFO - root - 2017-12-16 04:00:14.143471: step 4670, loss = 0.31, batch loss = 0.25 (26.6 examples/sec; 0.301 sec/batch; 27h:23m:59s remains)
INFO - root - 2017-12-16 04:00:17.005577: step 4680, loss = 0.21, batch loss = 0.15 (28.6 examples/sec; 0.279 sec/batch; 25h:26m:24s remains)
INFO - root - 2017-12-16 04:00:19.819491: step 4690, loss = 0.25, batch loss = 0.19 (28.8 examples/sec; 0.278 sec/batch; 25h:18m:31s remains)
INFO - root - 2017-12-16 04:00:22.615972: step 4700, loss = 0.39, batch loss = 0.33 (28.8 examples/sec; 0.278 sec/batch; 25h:18m:01s remains)
2017-12-16 04:00:23.057326: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9799237 -4.8324709 -5.6739459 -6.0484533 -6.1355486 -6.1287165 -5.9704618 -5.6884117 -5.4541183 -5.2151113 -5.0742745 -5.0172377 -5.0798049 -5.1822653 -4.8931746][-3.445159 -4.0877252 -4.7061157 -5.2186971 -5.372201 -5.4070873 -5.1692429 -5.1062346 -5.10149 -4.8777666 -4.8996787 -5.0033717 -5.2579994 -5.2480259 -4.8280668][-2.8592577 -3.3429418 -3.6287916 -3.8700759 -3.806138 -3.7854786 -3.7698908 -3.7766538 -3.8245368 -3.8935366 -4.1681824 -4.6261215 -5.2333369 -5.4744215 -5.0865808][-2.1059418 -2.2057261 -2.2365589 -2.2210886 -2.0449743 -1.9464447 -1.9346058 -1.9920759 -2.0839565 -2.3857183 -2.9805536 -3.8685217 -4.9166021 -5.2720747 -4.8103786][-1.8272777 -1.3393216 -0.87876272 -0.76955891 -0.63480306 -0.37838936 -0.17581081 -0.3194232 -0.46712685 -0.76535559 -1.3777966 -2.279592 -3.3248215 -4.1193738 -4.1196012][-1.968164 -1.1134844 -0.32907152 0.18751621 0.6478014 0.89557028 1.2795658 1.4622841 1.5277448 1.2926102 0.5318222 -0.76357388 -2.1605227 -2.7301629 -2.9067125][-2.2657225 -1.1719737 -0.25012112 0.56772041 1.3919735 1.996489 2.8359609 3.3753223 3.5949402 3.1556458 2.0912519 0.7063117 -0.92481112 -2.057862 -2.5347342][-2.5525692 -1.3574007 -0.50371313 0.4884491 1.5883474 2.482408 3.6345396 4.3815308 4.7448673 4.3010206 3.03508 1.2992783 -0.28695583 -1.3972323 -2.1095881][-3.4688511 -2.3922331 -1.3550324 -0.51075411 0.5095439 1.7509604 3.1093078 3.9668188 4.3659477 3.8933945 2.7053604 0.9783926 -0.74533987 -1.7035334 -2.3995383][-4.1830287 -3.4967072 -2.8246188 -2.0423577 -0.89837337 0.32565403 1.5334492 2.4666424 2.8640976 2.3724871 1.1778955 -0.34779024 -1.8283305 -2.7819464 -3.502856][-4.4817486 -4.0455847 -3.6544719 -3.2244244 -2.3849807 -1.3799851 -0.35401773 0.19182014 0.29889202 -0.087371349 -1.0915523 -2.4904165 -3.7595031 -4.4573097 -4.8827229][-3.8476803 -3.6887107 -3.7704291 -3.7712445 -3.5455523 -2.9995031 -2.2482409 -1.8789074 -1.8205154 -2.1010013 -2.7081225 -3.7359695 -4.7249002 -5.2171063 -5.4564805][-3.0854506 -3.3391142 -4.0529418 -4.6110172 -5.0129676 -4.8559127 -4.3925772 -3.9817007 -3.589478 -3.4836512 -3.7452519 -4.2604284 -4.85368 -5.3266211 -5.6364355][-2.506269 -2.9043341 -3.9557977 -5.04059 -5.853518 -6.0308743 -5.7538848 -5.3651986 -4.8718596 -4.3776569 -4.1627784 -4.3089108 -4.6174226 -4.8788295 -5.1522913][-1.8509779 -2.4656296 -3.4974585 -4.6731358 -5.6577158 -5.9891295 -5.8603911 -5.3385568 -4.6550436 -4.1137671 -3.7838368 -3.7230809 -3.858911 -4.112617 -4.2503719]]...]
INFO - root - 2017-12-16 04:00:25.903376: step 4710, loss = 0.26, batch loss = 0.20 (27.5 examples/sec; 0.291 sec/batch; 26h:30m:02s remains)
INFO - root - 2017-12-16 04:00:28.690213: step 4720, loss = 0.26, batch loss = 0.20 (27.7 examples/sec; 0.289 sec/batch; 26h:17m:15s remains)
INFO - root - 2017-12-16 04:00:31.514932: step 4730, loss = 0.25, batch loss = 0.19 (27.5 examples/sec; 0.291 sec/batch; 26h:29m:42s remains)
INFO - root - 2017-12-16 04:00:34.364888: step 4740, loss = 0.37, batch loss = 0.31 (29.0 examples/sec; 0.276 sec/batch; 25h:07m:01s remains)
INFO - root - 2017-12-16 04:00:37.166842: step 4750, loss = 0.28, batch loss = 0.23 (29.5 examples/sec; 0.271 sec/batch; 24h:39m:11s remains)
INFO - root - 2017-12-16 04:00:40.022559: step 4760, loss = 0.32, batch loss = 0.27 (28.6 examples/sec; 0.280 sec/batch; 25h:27m:19s remains)
INFO - root - 2017-12-16 04:00:42.822497: step 4770, loss = 0.24, batch loss = 0.19 (29.0 examples/sec; 0.276 sec/batch; 25h:05m:05s remains)
INFO - root - 2017-12-16 04:00:45.657005: step 4780, loss = 0.18, batch loss = 0.12 (27.2 examples/sec; 0.295 sec/batch; 26h:48m:52s remains)
INFO - root - 2017-12-16 04:00:48.486564: step 4790, loss = 0.35, batch loss = 0.30 (28.7 examples/sec; 0.278 sec/batch; 25h:19m:57s remains)
INFO - root - 2017-12-16 04:00:51.305684: step 4800, loss = 0.27, batch loss = 0.22 (29.3 examples/sec; 0.273 sec/batch; 24h:53m:23s remains)
2017-12-16 04:00:51.774861: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4167585 -4.9450431 -4.5807476 -4.3539243 -4.343698 -4.6062407 -5.0229139 -5.3056593 -5.4229546 -5.3917522 -5.2655 -5.1361003 -4.9062266 -4.5182328 -3.9959648][-6.1960773 -5.8390427 -5.5009117 -5.4324632 -5.5546913 -5.7890635 -6.0937071 -6.2448578 -6.3063612 -6.1703615 -6.0683413 -5.864953 -5.7492981 -5.5120997 -5.0065646][-7.1520605 -6.9912605 -6.7136431 -6.6345568 -6.55326 -6.5974541 -6.6785431 -6.672729 -6.4736776 -6.2248516 -6.0687141 -5.9870443 -6.116447 -6.0866137 -5.6480451][-7.9292741 -7.9079571 -7.6861429 -7.38808 -7.0500875 -6.6588387 -6.1069493 -5.5261807 -4.9921532 -4.7767096 -4.87986 -5.2124953 -5.6511331 -5.8509359 -5.7670088][-7.9424171 -8.0429821 -7.8956394 -7.4801283 -6.6422281 -5.6365747 -4.5118284 -3.4855309 -2.77419 -2.5364115 -2.9239826 -3.8533802 -5.0036988 -5.601162 -5.4953523][-7.1995277 -7.3697114 -7.1191473 -6.539484 -5.3860388 -3.7627263 -2.0204959 -0.53196478 0.31018543 0.34347725 -0.65513349 -2.0099108 -3.3405092 -4.4676032 -4.75117][-6.0448785 -6.2913203 -6.1449146 -5.3615708 -3.8608737 -1.9234481 0.25723696 1.9655399 2.8310761 2.584209 1.3352118 -0.41727066 -1.9295535 -2.844274 -3.0026832][-5.0842552 -5.1482096 -4.8926878 -3.9457114 -2.3186047 -0.1300025 2.0619411 3.6157885 4.269846 3.6018515 2.1440568 0.60206938 -0.66352892 -1.3107693 -1.4970112][-3.6145411 -3.8429995 -3.8181996 -2.9176564 -1.4075203 0.64407206 2.6312704 3.9834681 4.2932262 3.3940911 1.8850284 0.56066608 -0.095646381 -0.35013866 -0.36461353][-2.3156283 -2.5893922 -2.6039732 -2.2490292 -1.0937839 0.523952 1.902854 2.8741269 2.9429722 2.0672712 0.76585674 -0.20982409 -0.31088591 -0.011423111 0.16313744][-1.4349315 -1.798094 -1.9571676 -1.9199603 -1.3368077 -0.23007441 0.832212 1.1862721 0.815681 0.14745188 -0.66419458 -1.1113133 -0.89222622 -0.41316843 -0.077341557][-1.220402 -1.3873158 -1.765367 -2.0480824 -1.7858841 -1.3009145 -0.87608171 -0.67769504 -0.89075708 -1.4655023 -2.2329607 -2.4163418 -2.0137348 -1.3941462 -1.0380328][-0.41459227 -0.65658617 -1.1998136 -1.9150436 -2.2392962 -2.019278 -1.6473567 -1.7504904 -2.2113812 -2.7315221 -3.3157637 -3.5338795 -3.1771679 -2.6986775 -2.5910323][0.20308161 0.054554462 -0.77900672 -1.8953083 -2.5313368 -2.5845785 -2.6200826 -2.7334378 -3.0196776 -3.4506695 -3.9179425 -4.0427632 -3.945066 -3.740114 -3.5827656][0.303442 0.064293861 -0.75084805 -2.0000427 -2.8035307 -3.1712568 -3.2901797 -3.3163595 -3.5583119 -3.8714449 -4.2458649 -4.2801867 -4.1163349 -4.1025229 -4.1874723]]...]
INFO - root - 2017-12-16 04:00:54.593334: step 4810, loss = 0.29, batch loss = 0.23 (28.6 examples/sec; 0.280 sec/batch; 25h:28m:19s remains)
INFO - root - 2017-12-16 04:00:57.382680: step 4820, loss = 0.27, batch loss = 0.21 (28.1 examples/sec; 0.284 sec/batch; 25h:53m:32s remains)
INFO - root - 2017-12-16 04:01:00.233621: step 4830, loss = 0.25, batch loss = 0.19 (29.6 examples/sec; 0.270 sec/batch; 24h:36m:53s remains)
INFO - root - 2017-12-16 04:01:03.016429: step 4840, loss = 0.28, batch loss = 0.23 (27.5 examples/sec; 0.290 sec/batch; 26h:25m:49s remains)
INFO - root - 2017-12-16 04:01:05.869584: step 4850, loss = 0.31, batch loss = 0.25 (28.5 examples/sec; 0.281 sec/batch; 25h:33m:03s remains)
INFO - root - 2017-12-16 04:01:08.712744: step 4860, loss = 0.28, batch loss = 0.22 (28.3 examples/sec; 0.282 sec/batch; 25h:41m:18s remains)
INFO - root - 2017-12-16 04:01:11.553869: step 4870, loss = 0.29, batch loss = 0.24 (25.8 examples/sec; 0.310 sec/batch; 28h:15m:00s remains)
INFO - root - 2017-12-16 04:01:14.403847: step 4880, loss = 0.23, batch loss = 0.18 (26.1 examples/sec; 0.307 sec/batch; 27h:55m:04s remains)
INFO - root - 2017-12-16 04:01:17.216734: step 4890, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 25h:45m:25s remains)
INFO - root - 2017-12-16 04:01:20.040508: step 4900, loss = 0.25, batch loss = 0.19 (27.9 examples/sec; 0.286 sec/batch; 26h:03m:12s remains)
2017-12-16 04:01:20.497438: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8374214 -4.0901146 -4.267076 -4.3450112 -4.3309765 -4.2533922 -4.1153917 -4.0593691 -4.0629592 -4.138361 -4.2554045 -4.3976946 -4.5403247 -4.6019249 -4.6979437][-3.9931822 -4.3052211 -4.4677706 -4.5209007 -4.496654 -4.4621754 -4.4015093 -4.3589115 -4.3277826 -4.4249463 -4.5747151 -4.823307 -5.1143045 -5.2485847 -5.2675653][-4.2333026 -4.5799522 -4.6776309 -4.5896664 -4.4344287 -4.3237543 -4.2617702 -4.353272 -4.4919782 -4.6093731 -4.694273 -4.9352088 -5.2260146 -5.4942327 -5.7474813][-4.5030513 -4.7534943 -4.6634622 -4.3600864 -4.0399022 -3.7467256 -3.5884829 -3.6946177 -3.8988326 -4.2102938 -4.4527478 -4.7415576 -5.03063 -5.3838482 -5.7462139][-4.935029 -4.9115124 -4.4677172 -3.8357344 -3.2190566 -2.7638884 -2.5046895 -2.5887475 -2.7521169 -3.0237837 -3.3332691 -3.8128922 -4.3075418 -4.8552747 -5.4146729][-5.2542853 -5.0664272 -4.4631066 -3.3756261 -2.3431787 -1.6323493 -1.2127321 -1.2339904 -1.4607282 -1.6754537 -1.7959924 -2.2332129 -2.8144073 -3.6711457 -4.579555][-5.2283278 -4.9539614 -4.2300735 -2.9802051 -1.6072786 -0.52054858 0.15515232 0.20330381 0.029941082 -0.23032618 -0.35252619 -0.6811583 -1.2232003 -2.1794648 -3.4479842][-5.5298314 -4.985621 -3.9948661 -2.5086627 -1.1409957 0.11338758 0.91152334 1.1034203 0.9795084 0.81884766 0.81810236 0.52863216 -0.084816933 -1.1343136 -2.5169082][-5.83176 -5.27884 -4.09983 -2.5490026 -1.137706 0.050545692 0.81355143 1.0573845 0.98386908 1.009594 1.1637812 0.96542931 0.36500549 -0.72978282 -2.2441905][-6.2589693 -5.5588894 -4.5000019 -3.002481 -1.6428051 -0.62115 0.026769161 0.33208561 0.40583563 0.55636406 0.76531696 0.64614248 0.05138731 -1.1470842 -2.8128824][-6.9198132 -6.2012634 -5.21552 -3.9203706 -2.7557852 -1.7218435 -1.059952 -0.87610722 -0.82571983 -0.54030061 -0.26995611 -0.37698174 -0.9730587 -2.0647795 -3.6965673][-7.0318961 -6.248951 -5.4578371 -4.69001 -4.1875486 -3.5068369 -2.8412058 -2.4485958 -2.1791725 -1.8593388 -1.7150855 -1.8921254 -2.418906 -3.4804115 -4.8899817][-7.2061996 -6.315948 -5.54146 -4.9363027 -4.6460166 -4.5088825 -4.4786797 -4.1952453 -3.7266314 -3.2086291 -2.87935 -3.0375605 -3.4663572 -4.472549 -5.7769756][-6.5454407 -5.97798 -5.5112963 -5.128531 -4.8969316 -4.7969913 -4.84625 -4.9344993 -4.8168564 -4.3430576 -4.0016518 -4.088697 -4.3914952 -5.0917 -5.9420176][-5.96057 -5.2317233 -4.6500821 -4.6977382 -4.8415895 -4.8456655 -4.9856544 -5.0178022 -5.04485 -4.8068175 -4.6601148 -4.7725692 -4.9413662 -5.3649039 -5.7490492]]...]
INFO - root - 2017-12-16 04:01:23.294576: step 4910, loss = 0.30, batch loss = 0.24 (26.7 examples/sec; 0.300 sec/batch; 27h:17m:48s remains)
INFO - root - 2017-12-16 04:01:26.095333: step 4920, loss = 0.32, batch loss = 0.26 (29.7 examples/sec; 0.269 sec/batch; 24h:29m:01s remains)
INFO - root - 2017-12-16 04:01:28.932171: step 4930, loss = 0.24, batch loss = 0.18 (29.0 examples/sec; 0.276 sec/batch; 25h:05m:58s remains)
INFO - root - 2017-12-16 04:01:31.820795: step 4940, loss = 0.37, batch loss = 0.31 (28.6 examples/sec; 0.280 sec/batch; 25h:28m:27s remains)
INFO - root - 2017-12-16 04:01:34.678826: step 4950, loss = 0.22, batch loss = 0.16 (28.4 examples/sec; 0.282 sec/batch; 25h:39m:46s remains)
INFO - root - 2017-12-16 04:01:37.484688: step 4960, loss = 0.49, batch loss = 0.43 (27.8 examples/sec; 0.288 sec/batch; 26h:12m:08s remains)
INFO - root - 2017-12-16 04:01:40.332346: step 4970, loss = 0.30, batch loss = 0.24 (29.3 examples/sec; 0.273 sec/batch; 24h:48m:23s remains)
INFO - root - 2017-12-16 04:01:43.110027: step 4980, loss = 0.25, batch loss = 0.19 (29.2 examples/sec; 0.274 sec/batch; 24h:54m:50s remains)
INFO - root - 2017-12-16 04:01:45.917209: step 4990, loss = 0.30, batch loss = 0.24 (29.3 examples/sec; 0.273 sec/batch; 24h:52m:37s remains)
INFO - root - 2017-12-16 04:01:48.747381: step 5000, loss = 0.39, batch loss = 0.33 (28.3 examples/sec; 0.283 sec/batch; 25h:43m:24s remains)
2017-12-16 04:01:49.185145: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9190459 -3.9507122 -3.7780621 -3.7775786 -3.5949547 -3.5274777 -3.5551548 -3.4851632 -3.3997808 -3.276392 -3.2705162 -3.3395064 -3.4565852 -3.5332079 -3.473536][-3.060174 -2.9020419 -2.7699277 -2.7363992 -2.4885988 -2.4005947 -2.4348433 -2.3962219 -2.3268709 -2.1798837 -2.2407854 -2.4454002 -2.6146388 -2.4874775 -2.2379739][-2.0441527 -1.9246349 -1.6208825 -1.5309894 -1.3758013 -1.1315439 -1.1163166 -1.2299821 -1.2188313 -1.3163934 -1.7059166 -1.9501791 -1.8647094 -1.6241598 -1.1501613][-0.93804765 -0.74838543 -0.58579326 -0.43997717 -0.17294741 -0.06110096 -0.11161089 -0.36571932 -0.76149726 -1.0771809 -1.384088 -1.797951 -1.8833089 -1.3441508 -0.61130595][-0.59145927 -0.47419715 -0.256701 -0.27384377 -0.28141117 -0.0329237 0.1430254 -0.1059289 -0.50709724 -0.97709394 -1.4875605 -1.7595911 -1.7903092 -1.4203384 -0.836632][-0.89688849 -0.92333531 -0.77176976 -0.61295485 -0.39457369 -0.2302866 -0.14009857 -0.16172123 -0.36447668 -0.98634505 -1.6600935 -2.0434666 -2.1124012 -1.8438561 -1.3497138][-1.54056 -1.6249123 -1.5909817 -1.364572 -0.90763259 -0.49229717 -0.1598177 -0.14054728 -0.30063391 -0.76739573 -1.4343719 -1.9796102 -2.2741306 -2.1533773 -1.738013][-2.1582696 -2.1625772 -2.0921781 -1.7870984 -1.2213547 -0.67397237 -0.31249142 -0.1377902 -0.13473845 -0.51117444 -1.0072615 -1.4311144 -1.6810567 -1.6821785 -1.5903761][-2.9537861 -3.0352576 -2.8372984 -2.3653674 -1.6776452 -1.0964139 -0.68913436 -0.52932835 -0.49462414 -0.66648507 -0.9130125 -1.1054704 -1.3329573 -1.3842347 -1.3138659][-3.7541385 -3.7480469 -3.5114563 -2.9074779 -2.1065843 -1.4973593 -1.2423043 -1.2611639 -1.394109 -1.5099754 -1.4880741 -1.410511 -1.425828 -1.487098 -1.575032][-3.9168499 -3.7719927 -3.4704731 -2.9719262 -2.4443891 -2.1645961 -2.1753068 -2.4428425 -2.8009627 -2.8942013 -2.7329049 -2.4267542 -2.1617258 -2.0610011 -2.1772332][-3.8877387 -3.5779796 -3.2329657 -2.8346875 -2.6359382 -2.7697756 -3.2792926 -3.9087691 -4.2843795 -4.4012876 -4.1001248 -3.5865016 -3.2267375 -3.0910606 -3.2183647][-3.7433939 -3.3626723 -3.0345817 -2.7706656 -2.832345 -3.3023455 -4.0408993 -4.9426389 -5.6104946 -5.6456575 -5.1553459 -4.6347771 -4.1920872 -4.0557704 -4.1239133][-3.3501639 -2.8840876 -2.5057962 -2.4191325 -2.73035 -3.423171 -4.4266729 -5.4922981 -6.1673918 -6.2528782 -5.9025893 -5.3256321 -4.7982225 -4.4929414 -4.3568816][-3.1735353 -2.6236143 -2.1841881 -2.07596 -2.4375677 -3.2325015 -4.3116574 -5.2945566 -5.9626741 -6.050354 -5.7278914 -5.27632 -4.7500563 -4.3404746 -4.1428041]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-5000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-5000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 04:01:52.590589: step 5010, loss = 0.28, batch loss = 0.22 (29.3 examples/sec; 0.273 sec/batch; 24h:49m:39s remains)
INFO - root - 2017-12-16 04:01:55.412306: step 5020, loss = 0.27, batch loss = 0.21 (27.5 examples/sec; 0.291 sec/batch; 26h:25m:45s remains)
INFO - root - 2017-12-16 04:01:58.276369: step 5030, loss = 0.28, batch loss = 0.22 (28.3 examples/sec; 0.282 sec/batch; 25h:41m:24s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 04:02:01.026821: step 5040, loss = 0.28, batch loss = 0.22 (29.0 examples/sec; 0.276 sec/batch; 25h:05m:43s remains)
INFO - root - 2017-12-16 04:02:03.895563: step 5050, loss = 0.33, batch loss = 0.27 (28.1 examples/sec; 0.285 sec/batch; 25h:54m:51s remains)
INFO - root - 2017-12-16 04:02:06.727519: step 5060, loss = 0.37, batch loss = 0.31 (28.8 examples/sec; 0.278 sec/batch; 25h:18m:03s remains)
INFO - root - 2017-12-16 04:02:09.560840: step 5070, loss = 0.23, batch loss = 0.17 (29.2 examples/sec; 0.274 sec/batch; 24h:57m:38s remains)
INFO - root - 2017-12-16 04:02:12.379506: step 5080, loss = 0.22, batch loss = 0.17 (28.6 examples/sec; 0.280 sec/batch; 25h:29m:01s remains)
INFO - root - 2017-12-16 04:02:15.189022: step 5090, loss = 0.30, batch loss = 0.24 (28.0 examples/sec; 0.286 sec/batch; 25h:59m:07s remains)
INFO - root - 2017-12-16 04:02:18.007805: step 5100, loss = 0.27, batch loss = 0.21 (28.8 examples/sec; 0.277 sec/batch; 25h:14m:04s remains)
2017-12-16 04:02:18.462738: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6629548 -3.2328882 -3.8272419 -4.3308759 -4.7620864 -4.8948183 -4.9245763 -5.0389485 -5.1789455 -5.1836138 -5.1025319 -4.939826 -4.6596055 -4.1688871 -3.6136923][-3.3743978 -4.1865892 -4.9372187 -5.3591032 -5.5970583 -5.6840034 -5.7014961 -5.482162 -5.2852979 -5.3435345 -5.6073427 -5.5958242 -5.2880745 -4.903522 -4.4603057][-4.2763252 -5.1963143 -5.9856749 -6.2839136 -6.1873903 -5.6554813 -5.0595703 -4.6657615 -4.4653058 -4.3526125 -4.5209656 -5.0431991 -5.49783 -5.4409366 -4.9275188][-4.7752571 -5.6516294 -6.3149252 -6.3731089 -5.9763131 -4.9326229 -3.762007 -2.6969907 -2.157347 -2.3940885 -3.1981292 -4.0048475 -4.7401557 -5.1619077 -5.1414452][-4.7568378 -5.3861737 -5.7373343 -5.4406157 -4.5353675 -2.9529729 -1.3278327 0.075832844 0.61590338 0.22803879 -1.1367512 -2.8868432 -4.3401704 -5.0582647 -5.2495022][-4.3235936 -4.604537 -4.5620203 -3.9555936 -2.6715462 -0.70113087 1.4111161 3.0427928 3.6408195 2.8645215 1.0726271 -1.0021262 -2.8709874 -4.293705 -5.0171604][-3.6801143 -3.7368541 -3.380168 -2.4237309 -0.86999822 1.2525997 3.3797522 5.0530624 5.5242453 4.6558275 2.7849822 0.43104172 -1.6599617 -3.2432854 -4.3479252][-3.1110909 -3.0808048 -2.6060588 -1.3851302 0.35776043 2.4337039 4.2911234 5.3390722 5.4257011 4.4945679 2.6918678 0.73194122 -1.1452513 -2.8187776 -4.0903544][-3.1467671 -3.0179005 -2.4442735 -1.2152946 0.48792744 2.2878489 3.6857176 4.3534327 4.1765509 3.2397208 1.9819112 0.51093721 -1.0422609 -2.6433291 -3.9583862][-3.8443294 -3.5502961 -2.9088948 -1.6242912 0.0062851906 1.434104 2.433445 2.8529735 2.7083445 1.9394808 1.0935202 0.040338993 -1.2218695 -2.6900926 -4.0382423][-4.7101593 -4.3087344 -3.5736568 -2.3165202 -1.0060291 0.029371262 0.66309547 1.0100231 1.0045409 0.53688908 0.018210888 -0.83188295 -1.8901927 -3.2054667 -4.4341083][-5.5302062 -5.0611272 -4.2124457 -3.1618266 -2.1150973 -1.3758087 -0.86174345 -0.53295422 -0.59310007 -0.80089188 -1.1393838 -1.718055 -2.7286415 -3.9085555 -4.9769506][-6.2875938 -5.8329029 -4.9747906 -3.8179293 -2.71915 -2.1101434 -1.6248016 -1.2367194 -1.0573807 -1.3634191 -1.9458568 -2.5686207 -3.5016496 -4.595171 -5.5286856][-6.2828412 -5.8039083 -5.0392265 -4.1225691 -2.9696703 -1.971072 -1.3691559 -1.1251619 -1.1152301 -1.3760486 -1.9543953 -2.9007533 -3.8587959 -4.7545366 -5.4621892][-5.6741047 -5.3206935 -4.7346621 -3.978919 -2.9317532 -1.8286111 -1.0741467 -0.71625352 -0.78997636 -1.0989077 -1.6321688 -2.4802675 -3.4076085 -4.3104291 -4.6855884]]...]
INFO - root - 2017-12-16 04:02:21.292873: step 5110, loss = 0.25, batch loss = 0.19 (26.5 examples/sec; 0.302 sec/batch; 27h:28m:58s remains)
INFO - root - 2017-12-16 04:02:24.142090: step 5120, loss = 0.22, batch loss = 0.16 (28.1 examples/sec; 0.285 sec/batch; 25h:53m:53s remains)
INFO - root - 2017-12-16 04:02:26.956840: step 5130, loss = 0.26, batch loss = 0.20 (27.4 examples/sec; 0.292 sec/batch; 26h:33m:52s remains)
INFO - root - 2017-12-16 04:02:29.764152: step 5140, loss = 0.28, batch loss = 0.22 (28.3 examples/sec; 0.283 sec/batch; 25h:44m:01s remains)
INFO - root - 2017-12-16 04:02:32.631279: step 5150, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.286 sec/batch; 26h:00m:40s remains)
INFO - root - 2017-12-16 04:02:35.428922: step 5160, loss = 0.27, batch loss = 0.21 (29.3 examples/sec; 0.273 sec/batch; 24h:51m:43s remains)
INFO - root - 2017-12-16 04:02:38.273971: step 5170, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.282 sec/batch; 25h:40m:57s remains)
INFO - root - 2017-12-16 04:02:41.080635: step 5180, loss = 0.29, batch loss = 0.23 (29.4 examples/sec; 0.272 sec/batch; 24h:42m:26s remains)
INFO - root - 2017-12-16 04:02:43.928273: step 5190, loss = 0.30, batch loss = 0.25 (28.4 examples/sec; 0.282 sec/batch; 25h:38m:10s remains)
INFO - root - 2017-12-16 04:02:46.769183: step 5200, loss = 0.31, batch loss = 0.25 (28.1 examples/sec; 0.285 sec/batch; 25h:54m:02s remains)
2017-12-16 04:02:47.246285: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0527153 -4.3082457 -4.4797626 -4.9902878 -5.7212858 -6.2347755 -6.9557486 -7.5246229 -7.9363985 -8.017169 -7.604743 -6.8952255 -5.9849215 -4.9254131 -4.0456619][-3.2645869 -3.7665997 -4.3442688 -4.5906558 -5.0629859 -5.9850726 -6.9473329 -7.521946 -7.8873215 -7.6479464 -7.2497196 -6.7981768 -6.0876145 -5.0917053 -4.1782317][-2.5113597 -2.9171681 -3.2664363 -3.9673612 -4.6891193 -5.0500913 -5.699048 -6.5089483 -6.965991 -6.9258842 -6.6306057 -6.1529918 -5.6958904 -4.956512 -3.9924071][-1.3539059 -2.0096397 -2.8205113 -3.2082002 -3.4335194 -3.6943326 -3.9987578 -4.2708449 -4.8479481 -5.165164 -5.2926097 -5.3856778 -5.2245331 -4.5932846 -3.7383928][-1.3173757 -1.6446009 -1.9649432 -2.3317318 -2.4928493 -1.9501233 -1.5217464 -1.4755876 -1.777364 -2.4067943 -3.2209232 -3.77353 -4.0873327 -4.0157928 -3.6369574][-1.3180301 -1.3745999 -1.2612121 -1.3148172 -0.8849802 -0.10514164 0.96413612 1.5811977 1.1270041 0.057122707 -1.2264864 -2.23901 -2.7924659 -3.0450089 -2.8925385][-1.1756442 -0.88254213 -0.91790056 -0.45881534 0.4065733 1.4230576 2.6120639 3.0808535 2.8002968 1.6522794 0.21029043 -1.0142 -2.0193841 -2.4839365 -2.5579982][-1.5662463 -1.3034253 -1.091471 -0.51562476 0.44087982 1.9128385 3.3233318 3.5637727 2.9784641 1.8087049 0.41746759 -0.75879073 -1.6658759 -2.3263896 -2.621969][-2.2787766 -1.9865787 -1.8833601 -1.4356999 -0.540591 0.73195124 1.9609337 2.5595036 2.469131 1.0650892 -0.30212164 -1.382298 -2.2848542 -2.7156234 -2.8678598][-2.385299 -2.731653 -3.3836985 -3.0541215 -2.3547361 -1.5114923 -0.681088 -0.075018883 0.0035057068 -0.72347975 -1.5316556 -2.3419468 -2.925446 -3.2796454 -3.5625155][-4.3087316 -4.1967211 -4.4035497 -4.6658483 -4.6704588 -3.9227357 -3.278779 -2.9829602 -2.6742229 -2.9099145 -3.3349519 -3.8793342 -4.2623205 -4.2696838 -4.2659006][-4.7953253 -5.4896035 -6.247508 -6.0431075 -5.7109947 -5.3353839 -5.0013967 -4.7456989 -4.53698 -4.5793743 -4.5675769 -4.9535351 -5.3020444 -5.2427173 -5.1515341][-4.9679079 -5.3449936 -5.9298964 -6.2902493 -6.3874421 -5.904037 -5.5053487 -5.2279315 -4.8759041 -4.9846883 -5.0857882 -5.1895447 -5.3011451 -5.3240981 -5.2649488][-4.657774 -4.80074 -5.0219 -5.2875328 -5.5829163 -5.3682432 -5.0373545 -4.9218726 -4.7965159 -4.5931959 -4.367517 -4.6770735 -5.0074005 -4.8224182 -4.6915307][-4.3053823 -4.2854004 -4.3647532 -4.4362655 -4.4378786 -4.3399405 -4.2289147 -4.0703306 -3.9014456 -4.0119243 -4.1022139 -4.1261511 -4.1801085 -4.2571492 -4.23009]]...]
INFO - root - 2017-12-16 04:02:50.140830: step 5210, loss = 0.26, batch loss = 0.20 (27.6 examples/sec; 0.289 sec/batch; 26h:18m:28s remains)
INFO - root - 2017-12-16 04:02:52.974774: step 5220, loss = 0.33, batch loss = 0.27 (28.1 examples/sec; 0.285 sec/batch; 25h:53m:58s remains)
INFO - root - 2017-12-16 04:02:55.818521: step 5230, loss = 0.35, batch loss = 0.29 (28.1 examples/sec; 0.285 sec/batch; 25h:55m:35s remains)
INFO - root - 2017-12-16 04:02:58.656853: step 5240, loss = 0.39, batch loss = 0.33 (28.5 examples/sec; 0.281 sec/batch; 25h:32m:40s remains)
INFO - root - 2017-12-16 04:03:01.462497: step 5250, loss = 0.34, batch loss = 0.28 (29.4 examples/sec; 0.272 sec/batch; 24h:45m:36s remains)
INFO - root - 2017-12-16 04:03:04.298599: step 5260, loss = 0.37, batch loss = 0.31 (29.1 examples/sec; 0.275 sec/batch; 24h:59m:27s remains)
INFO - root - 2017-12-16 04:03:07.098582: step 5270, loss = 0.28, batch loss = 0.23 (28.3 examples/sec; 0.282 sec/batch; 25h:39m:55s remains)
INFO - root - 2017-12-16 04:03:09.960342: step 5280, loss = 0.27, batch loss = 0.21 (27.7 examples/sec; 0.289 sec/batch; 26h:15m:41s remains)
INFO - root - 2017-12-16 04:03:12.759836: step 5290, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.279 sec/batch; 25h:22m:18s remains)
INFO - root - 2017-12-16 04:03:15.560274: step 5300, loss = 0.23, batch loss = 0.17 (27.3 examples/sec; 0.293 sec/batch; 26h:35m:08s remains)
2017-12-16 04:03:16.020808: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7392111 -5.0753441 -4.7340841 -4.7818346 -4.8359323 -5.0353804 -5.1390548 -5.2839842 -5.4368811 -5.3861208 -5.1795855 -4.7803788 -4.3822131 -3.9973619 -3.9496667][-6.1249485 -5.3734665 -4.7522068 -4.4967146 -4.2831445 -4.639503 -4.8226886 -5.1922445 -5.4422183 -5.506423 -5.5046682 -5.0294909 -4.5566225 -3.883251 -3.4614215][-6.1465654 -5.1554542 -4.476131 -4.1531 -3.6615527 -3.7607954 -3.780859 -4.2854586 -4.7741795 -5.2571349 -5.4405451 -5.1770859 -4.7804766 -4.0119214 -3.2865362][-5.6050124 -4.3749661 -3.3762653 -2.8732948 -2.4335587 -2.3774426 -2.528584 -3.1804934 -3.91567 -4.508894 -4.9677935 -4.9387765 -4.7135568 -4.1249962 -3.4414117][-4.6685815 -3.0581398 -1.4956629 -0.74521017 -0.17918682 -0.22269917 -0.67272854 -1.4164269 -2.5205383 -3.4471049 -4.2397571 -4.4092374 -4.2499046 -3.916039 -3.5250456][-3.8621714 -2.0490131 -0.33430052 1.0009904 2.1155858 2.1906328 1.967689 1.0219378 -0.25918484 -1.4782252 -2.748533 -3.2946763 -3.7394013 -3.6363587 -3.4096146][-3.372901 -1.6147954 0.10680342 1.5424337 2.7149677 3.3551273 3.7257566 3.002214 1.7522683 0.22599697 -1.3156447 -2.2965438 -3.1323516 -3.3197412 -3.4796839][-3.0885091 -1.8139021 -0.2108655 1.2608681 2.4903593 3.2770715 3.7423506 3.452457 2.4921799 1.0198436 -0.41321516 -1.4061809 -2.2842839 -2.8746743 -3.4939022][-2.9304128 -2.1926076 -1.271548 -0.073180676 0.92874908 2.0318613 2.8018928 2.7979336 2.1163044 0.82061195 -0.45618272 -1.5324733 -2.3914766 -3.0265608 -3.5628486][-3.241291 -3.1101551 -2.9163518 -2.1916008 -1.2072282 -0.059811115 0.95467949 1.36343 1.031095 -0.042301178 -1.2654324 -2.3963468 -3.2173626 -3.6639507 -4.0407667][-4.2164955 -4.059814 -4.2251058 -4.188365 -3.6205659 -2.5335412 -1.3166902 -0.67497253 -0.80582 -1.6598947 -2.665395 -3.4998887 -4.2202106 -4.4938383 -4.6119485][-5.0339127 -4.9652777 -5.2700129 -5.6031151 -5.4395819 -4.6542144 -3.6159616 -2.917809 -2.7311711 -3.1831958 -3.7957652 -4.2945518 -4.668077 -4.694521 -4.6113729][-5.7446208 -5.2826958 -5.541338 -5.9773979 -6.175107 -5.72275 -4.9653234 -4.3218608 -3.9117 -4.2384033 -4.515059 -4.6384721 -4.7855515 -4.7000713 -4.3319917][-6.2129006 -5.5360289 -5.4298019 -5.6570644 -6.0379381 -5.876462 -5.4713869 -5.0182905 -4.723505 -4.8031621 -4.7566524 -4.7197537 -4.6499228 -4.3990054 -3.9379978][-6.5910354 -5.6788349 -5.1612372 -5.1561832 -5.4618077 -5.5138478 -5.4109821 -5.1121187 -4.882165 -4.8158464 -4.6640306 -4.3506536 -4.0730262 -3.8000989 -3.4255271]]...]
INFO - root - 2017-12-16 04:03:18.849660: step 5310, loss = 0.20, batch loss = 0.15 (29.6 examples/sec; 0.270 sec/batch; 24h:33m:44s remains)
INFO - root - 2017-12-16 04:03:21.661274: step 5320, loss = 0.41, batch loss = 0.35 (28.0 examples/sec; 0.286 sec/batch; 25h:57m:34s remains)
INFO - root - 2017-12-16 04:03:24.520607: step 5330, loss = 0.33, batch loss = 0.27 (28.8 examples/sec; 0.278 sec/batch; 25h:15m:38s remains)
INFO - root - 2017-12-16 04:03:27.318181: step 5340, loss = 0.24, batch loss = 0.18 (27.7 examples/sec; 0.288 sec/batch; 26h:12m:27s remains)
INFO - root - 2017-12-16 04:03:30.118745: step 5350, loss = 0.24, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 24h:55m:36s remains)
INFO - root - 2017-12-16 04:03:32.949774: step 5360, loss = 0.26, batch loss = 0.20 (29.4 examples/sec; 0.272 sec/batch; 24h:43m:00s remains)
INFO - root - 2017-12-16 04:03:35.765187: step 5370, loss = 0.39, batch loss = 0.33 (28.7 examples/sec; 0.279 sec/batch; 25h:21m:12s remains)
INFO - root - 2017-12-16 04:03:38.635915: step 5380, loss = 0.30, batch loss = 0.24 (29.1 examples/sec; 0.275 sec/batch; 24h:59m:46s remains)
INFO - root - 2017-12-16 04:03:41.449449: step 5390, loss = 0.21, batch loss = 0.15 (28.1 examples/sec; 0.284 sec/batch; 25h:50m:35s remains)
INFO - root - 2017-12-16 04:03:44.321919: step 5400, loss = 0.26, batch loss = 0.20 (28.5 examples/sec; 0.281 sec/batch; 25h:29m:33s remains)
2017-12-16 04:03:44.782263: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7094083 -3.572324 -3.383008 -3.5659821 -3.8340864 -4.0810003 -4.1352057 -3.9270422 -3.5483732 -3.0304265 -2.6629131 -2.3808889 -2.1783235 -2.1083264 -2.0756755][-3.7263768 -3.5944395 -3.4499531 -3.5195231 -3.6569252 -4.0842819 -4.3733616 -4.2741294 -3.901937 -3.4921358 -3.1644454 -2.7791252 -2.4979577 -2.3321605 -2.1765573][-4.0996561 -3.6486959 -3.3034441 -3.2201762 -3.2499566 -3.5338924 -3.866838 -4.0533123 -4.0391531 -3.8953812 -3.7590268 -3.5745544 -3.3338387 -3.0486007 -2.7395248][-4.371788 -3.7409444 -3.0488439 -2.6597357 -2.4839773 -2.5551527 -2.7844543 -3.0709426 -3.4254036 -3.7494874 -3.9764404 -4.1010847 -4.0231724 -3.7798271 -3.4359581][-4.3387775 -3.4689093 -2.4684043 -1.6111512 -0.95930004 -0.67385769 -0.79310966 -1.1420155 -1.7110384 -2.5058265 -3.3111606 -3.8695755 -4.0549183 -4.0387797 -3.8179741][-4.0309572 -2.9894056 -1.7348545 -0.42851973 0.71386194 1.3945098 1.472332 1.0914431 0.30592299 -0.81677222 -2.0610209 -3.1499069 -3.7935159 -3.9550714 -3.7692871][-3.5605183 -2.4663796 -1.0276129 0.50691986 1.940742 3.0694551 3.5466318 3.2070746 2.240221 0.75353384 -0.85483909 -2.2921681 -3.1994977 -3.6411831 -3.6461139][-3.0400882 -2.0712712 -0.7672658 0.78027248 2.2893338 3.5984478 4.1721678 3.8664417 2.9960585 1.5547128 -0.14510155 -1.8426321 -2.9386954 -3.5649374 -3.7801127][-2.6776857 -1.8976877 -1.050056 0.01212883 1.2194929 2.4796562 3.0937452 2.9307146 2.1121473 0.71474361 -0.86139464 -2.462811 -3.5283809 -4.1166749 -4.2266397][-2.5529728 -2.0878117 -1.7809117 -1.2561443 -0.50635695 0.21483612 0.56293964 0.60627842 0.16254187 -0.9628458 -2.3866084 -3.7728069 -4.6684375 -5.0816979 -4.9332728][-2.5670485 -2.3441799 -2.4100468 -2.5196352 -2.4258077 -2.1082892 -1.8697531 -1.9981725 -2.3752708 -3.0917146 -4.19164 -5.2638221 -5.8945217 -6.2127657 -6.0904522][-2.6603923 -2.4966261 -2.8241444 -3.425915 -3.7436585 -3.9909365 -4.1804132 -4.2731566 -4.339818 -4.7275162 -5.438149 -6.2746658 -6.8433876 -7.1671982 -7.097908][-2.73211 -2.6535244 -3.134114 -3.7970495 -4.4220872 -4.90442 -5.2391944 -5.299449 -5.2775412 -5.4910345 -5.921854 -6.387289 -6.8533311 -7.2744236 -7.3100481][-2.9224858 -2.6970439 -3.0533404 -3.5512466 -4.007319 -4.4866867 -4.9337678 -5.07983 -5.014277 -4.9633393 -5.2490559 -5.5867119 -6.0774603 -6.4209404 -6.5263658][-3.152807 -2.5605369 -2.4381256 -2.8422079 -3.2898924 -3.7178204 -3.9820511 -4.0704446 -4.0842452 -4.0383611 -4.2854776 -4.3887715 -4.6473594 -5.007453 -5.2969036]]...]
INFO - root - 2017-12-16 04:03:47.559169: step 5410, loss = 0.26, batch loss = 0.20 (29.3 examples/sec; 0.273 sec/batch; 24h:47m:21s remains)
INFO - root - 2017-12-16 04:03:50.433136: step 5420, loss = 0.20, batch loss = 0.15 (27.9 examples/sec; 0.287 sec/batch; 26h:03m:28s remains)
INFO - root - 2017-12-16 04:03:53.230469: step 5430, loss = 0.31, batch loss = 0.25 (28.6 examples/sec; 0.280 sec/batch; 25h:26m:19s remains)
INFO - root - 2017-12-16 04:03:56.098602: step 5440, loss = 0.34, batch loss = 0.28 (27.8 examples/sec; 0.288 sec/batch; 26h:09m:46s remains)
INFO - root - 2017-12-16 04:03:58.908089: step 5450, loss = 0.22, batch loss = 0.17 (28.3 examples/sec; 0.283 sec/batch; 25h:41m:48s remains)
INFO - root - 2017-12-16 04:04:01.689471: step 5460, loss = 0.24, batch loss = 0.18 (27.3 examples/sec; 0.293 sec/batch; 26h:36m:06s remains)
INFO - root - 2017-12-16 04:04:04.526273: step 5470, loss = 0.24, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 24h:54m:12s remains)
INFO - root - 2017-12-16 04:04:07.402280: step 5480, loss = 0.29, batch loss = 0.23 (29.0 examples/sec; 0.276 sec/batch; 25h:05m:09s remains)
INFO - root - 2017-12-16 04:04:10.234413: step 5490, loss = 0.30, batch loss = 0.24 (28.2 examples/sec; 0.283 sec/batch; 25h:44m:16s remains)
INFO - root - 2017-12-16 04:04:13.030457: step 5500, loss = 0.26, batch loss = 0.20 (29.3 examples/sec; 0.273 sec/batch; 24h:50m:03s remains)
2017-12-16 04:04:13.455175: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3295362 -3.1343951 -2.9733782 -2.8153448 -2.6651397 -2.5405765 -2.5979278 -3.1159523 -3.4677229 -3.7394104 -4.0565495 -4.2953997 -4.2040696 -4.0506363 -4.1072412][-3.1182709 -3.0144813 -2.9610229 -2.86937 -2.6442008 -2.5738411 -2.5829306 -2.9777319 -3.2230554 -3.5980387 -4.0302868 -4.2890453 -4.3277569 -4.321198 -4.4063916][-2.9912457 -2.9287002 -2.9428515 -2.9233413 -2.6855197 -2.5975611 -2.5416541 -2.7491691 -2.8963652 -3.0785489 -3.4347427 -3.8076489 -4.1413374 -4.2904921 -4.4747915][-2.3112855 -2.4692879 -2.6395526 -2.7846866 -2.6657293 -2.4880509 -2.21218 -2.1716766 -2.1756232 -2.2330782 -2.4750504 -3.0100241 -3.4333706 -3.8622468 -4.1988645][-1.6944211 -1.9956629 -2.2576959 -2.4718719 -2.393255 -2.2298038 -1.8420126 -1.4084241 -1.193964 -1.3196511 -1.5964596 -2.03083 -2.4218442 -2.9735694 -3.33699][-1.1425319 -1.601141 -1.8203099 -1.998873 -1.864321 -1.6024702 -1.0701544 -0.49789524 -0.22916698 -0.31268072 -0.53873158 -1.0273092 -1.5064282 -2.0759151 -2.3947144][-0.66795468 -1.0741231 -1.2192321 -1.2845464 -1.0632503 -0.77466345 -0.29545259 0.21541119 0.49614334 0.43747568 0.24474669 -0.056403637 -0.40492678 -1.0168722 -1.4329262][-0.43612504 -0.72416925 -0.75364923 -0.63620687 -0.42959 -0.10020494 0.31948423 0.80257654 1.1060524 1.0214734 0.81246853 0.45018625 0.095053673 -0.50801873 -0.92493296][-0.53506851 -0.61136937 -0.48497438 -0.25188589 0.10179615 0.46884584 0.75423 1.0481062 1.2275429 1.0529804 0.86284542 0.45674992 0.028339863 -0.47289991 -0.85723662][-1.0171101 -0.779464 -0.49270725 -0.1918025 0.22742844 0.60396194 0.82749033 0.99563932 1.07483 0.80873203 0.43449116 -0.017528534 -0.47256947 -1.0558214 -1.4654226][-1.5550821 -1.1165645 -0.72665477 -0.28602648 0.19842291 0.55923986 0.70486879 0.57106543 0.3668766 0.035212994 -0.45567489 -0.9835546 -1.4399047 -1.8949666 -2.257488][-2.4781024 -1.8203688 -1.3615751 -0.99016333 -0.48982191 -0.077062607 -0.0029668808 -0.24958706 -0.56216 -1.0277903 -1.5959065 -2.0847752 -2.5098674 -2.8756793 -3.0357006][-3.2992382 -2.5075202 -1.90414 -1.4581854 -1.0298331 -0.5924418 -0.4746058 -0.92710781 -1.4054513 -1.8179178 -2.2141666 -2.7291946 -3.2031393 -3.4291544 -3.4611092][-3.9064407 -3.0470562 -2.2856841 -1.8683004 -1.5229814 -1.2209773 -1.2255335 -1.652036 -2.1768444 -2.6354995 -2.8593369 -3.030045 -3.2865508 -3.3355207 -3.2608225][-4.1642733 -3.3237572 -2.6232483 -2.1828034 -1.9067268 -1.7077935 -1.8538435 -2.4626889 -2.9822481 -3.2080331 -3.2858772 -3.1701803 -3.1337662 -3.0566092 -2.9225917]]...]
INFO - root - 2017-12-16 04:04:16.290394: step 5510, loss = 0.29, batch loss = 0.23 (28.3 examples/sec; 0.282 sec/batch; 25h:38m:16s remains)
INFO - root - 2017-12-16 04:04:19.105162: step 5520, loss = 0.27, batch loss = 0.22 (28.3 examples/sec; 0.283 sec/batch; 25h:42m:08s remains)
INFO - root - 2017-12-16 04:04:21.966551: step 5530, loss = 0.27, batch loss = 0.21 (27.2 examples/sec; 0.295 sec/batch; 26h:45m:00s remains)
INFO - root - 2017-12-16 04:04:24.760412: step 5540, loss = 0.24, batch loss = 0.18 (29.1 examples/sec; 0.275 sec/batch; 24h:59m:37s remains)
INFO - root - 2017-12-16 04:04:27.563913: step 5550, loss = 0.31, batch loss = 0.25 (27.8 examples/sec; 0.288 sec/batch; 26h:07m:28s remains)
INFO - root - 2017-12-16 04:04:30.371405: step 5560, loss = 0.22, batch loss = 0.17 (28.4 examples/sec; 0.281 sec/batch; 25h:32m:47s remains)
INFO - root - 2017-12-16 04:04:33.209514: step 5570, loss = 0.31, batch loss = 0.25 (27.9 examples/sec; 0.286 sec/batch; 26h:00m:54s remains)
INFO - root - 2017-12-16 04:04:36.035555: step 5580, loss = 0.29, batch loss = 0.24 (28.1 examples/sec; 0.284 sec/batch; 25h:49m:22s remains)
INFO - root - 2017-12-16 04:04:38.860090: step 5590, loss = 0.30, batch loss = 0.24 (29.4 examples/sec; 0.272 sec/batch; 24h:44m:23s remains)
INFO - root - 2017-12-16 04:04:41.660099: step 5600, loss = 0.23, batch loss = 0.17 (28.8 examples/sec; 0.278 sec/batch; 25h:14m:22s remains)
2017-12-16 04:04:42.122138: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0217595 -3.982806 -3.9356544 -3.6097512 -3.1953409 -2.9098282 -2.7079539 -2.7637112 -3.0342231 -3.3921208 -3.8335333 -3.9770947 -4.0710812 -3.991643 -3.761925][-4.1566396 -4.0925479 -3.8895292 -3.4290411 -2.853543 -2.4264808 -2.1461537 -2.2191844 -2.4176788 -2.6559131 -2.9750698 -3.1457238 -3.286953 -3.1190176 -2.8057685][-3.8958998 -3.6808383 -3.3558884 -3.0012114 -2.4919934 -1.9585564 -1.5134447 -1.4042594 -1.465281 -1.7278903 -1.9658244 -2.0191629 -2.1917553 -2.2123122 -2.1262937][-3.5617008 -3.2841492 -2.864368 -2.3331921 -1.7358232 -1.271718 -0.91697478 -0.93337679 -0.98802114 -1.0291481 -1.2529352 -1.6032917 -1.7665062 -1.5699224 -1.4821582][-2.5392671 -2.1217434 -1.6330056 -1.3915269 -1.0390668 -0.74434114 -0.54014754 -0.52109194 -0.60886884 -1.0207415 -1.3900771 -1.2935739 -1.2676234 -1.3739386 -1.4892747][-1.3542576 -0.85365605 -0.44240403 -0.3613348 -0.16329622 -0.052354813 -0.030870438 -0.43192339 -0.89758539 -1.2406023 -1.5676179 -1.9570193 -2.057066 -1.7262719 -1.6378572][-0.64935422 -0.19775915 -0.037644863 0.10259628 0.2984271 0.36957884 0.24387693 -0.20811796 -0.77128839 -1.5809803 -2.3107014 -2.5495465 -2.5161042 -2.4397097 -2.4311769][0.04074192 0.14876938 0.14195061 0.081069469 0.24266577 0.28850603 0.069919586 -0.61085963 -1.4098048 -2.1626611 -2.9133995 -3.4581218 -3.5035455 -3.1342015 -2.692359][-0.18973923 -0.078490257 1.3828278e-05 -0.20527744 -0.27861261 -0.318058 -0.57166123 -1.1884611 -2.0234787 -2.986366 -3.7999921 -4.1898 -4.0606675 -3.5468502 -2.8739285][-0.50472593 -0.61590862 -0.82285285 -1.0622072 -1.0702705 -1.07567 -1.3505538 -1.8853979 -2.5775735 -3.3714175 -4.1717339 -4.5607495 -4.2749557 -3.6536355 -2.7532182][-1.211688 -1.35939 -1.4730971 -1.6310368 -1.7476974 -1.8747294 -2.0192044 -2.3044281 -2.8554635 -3.5569458 -4.2223763 -4.4838939 -4.1714764 -3.3811107 -2.128257][-1.4759481 -1.7395263 -2.1856844 -2.4988422 -2.5437911 -2.4252141 -2.4667227 -2.6146746 -2.9589231 -3.4658234 -4.0168815 -4.4518337 -4.280107 -3.5038006 -2.19359][-1.9906037 -2.0686088 -2.1850159 -2.5535862 -2.7419267 -2.8387361 -2.894578 -2.8976517 -3.1253657 -3.460012 -4.0065246 -4.4470506 -4.406291 -3.802911 -2.5996187][-2.0530491 -2.1881137 -2.3564699 -2.5315061 -2.6373887 -2.5721412 -2.6184669 -2.7418852 -3.0025854 -3.4134812 -4.0685673 -4.7126184 -4.7939687 -4.3219709 -3.4075472][-2.2463009 -2.2030814 -2.1948521 -2.1512482 -2.1515079 -2.3574471 -2.5456114 -2.5431328 -2.7831507 -3.2771893 -3.9760437 -4.5249639 -4.7795386 -4.691299 -4.013608]]...]
INFO - root - 2017-12-16 04:04:44.906259: step 5610, loss = 0.21, batch loss = 0.16 (28.6 examples/sec; 0.280 sec/batch; 25h:26m:37s remains)
INFO - root - 2017-12-16 04:04:47.698658: step 5620, loss = 0.21, batch loss = 0.15 (29.1 examples/sec; 0.275 sec/batch; 25h:00m:14s remains)
INFO - root - 2017-12-16 04:04:50.522717: step 5630, loss = 0.22, batch loss = 0.16 (27.6 examples/sec; 0.290 sec/batch; 26h:20m:17s remains)
INFO - root - 2017-12-16 04:04:53.340909: step 5640, loss = 0.28, batch loss = 0.22 (28.6 examples/sec; 0.280 sec/batch; 25h:24m:23s remains)
INFO - root - 2017-12-16 04:04:56.156802: step 5650, loss = 0.34, batch loss = 0.28 (27.4 examples/sec; 0.292 sec/batch; 26h:28m:44s remains)
INFO - root - 2017-12-16 04:04:58.992703: step 5660, loss = 0.25, batch loss = 0.19 (28.7 examples/sec; 0.279 sec/batch; 25h:18m:09s remains)
INFO - root - 2017-12-16 04:05:01.785357: step 5670, loss = 0.39, batch loss = 0.34 (29.8 examples/sec; 0.268 sec/batch; 24h:20m:03s remains)
INFO - root - 2017-12-16 04:05:04.593038: step 5680, loss = 0.37, batch loss = 0.32 (28.9 examples/sec; 0.277 sec/batch; 25h:06m:34s remains)
INFO - root - 2017-12-16 04:05:07.368614: step 5690, loss = 0.26, batch loss = 0.20 (28.5 examples/sec; 0.281 sec/batch; 25h:30m:50s remains)
INFO - root - 2017-12-16 04:05:10.239933: step 5700, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.284 sec/batch; 25h:47m:19s remains)
2017-12-16 04:05:10.689198: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5364423 -3.8205886 -4.1418529 -4.4961195 -4.58621 -4.4574108 -4.1151567 -3.7401757 -3.4399045 -3.0940361 -2.960454 -3.1222577 -3.3334136 -3.335979 -3.2405567][-3.0846019 -3.3830824 -3.670892 -3.9953861 -4.1060505 -3.9962218 -3.7332549 -3.3677793 -3.0186286 -2.7274246 -2.6281862 -2.6659346 -2.7761354 -2.7489233 -2.5547233][-2.4384985 -2.767611 -3.1287718 -3.4405434 -3.4928436 -3.4613342 -3.1611686 -2.8935175 -2.6456685 -2.4302616 -2.3966577 -2.3499057 -2.4215956 -2.3687651 -2.172184][-1.6970377 -2.0094373 -2.336416 -2.6483171 -2.6567779 -2.488044 -2.1795604 -1.9894612 -1.8511629 -1.7525876 -1.7606564 -1.8019881 -1.8417308 -1.8400474 -1.8062186][-0.8429749 -1.145817 -1.3744831 -1.5032384 -1.3797433 -1.1755466 -1.0178099 -0.93194342 -0.93750811 -1.0648434 -1.2503114 -1.2980597 -1.3538139 -1.406199 -1.4631078][-0.77428722 -0.98118663 -1.0526438 -0.98788643 -0.60077119 -0.18580341 -0.012972355 0.046659946 -0.001147747 -0.21917772 -0.56864381 -0.81359386 -0.91961241 -1.075315 -1.2638702][-1.2784472 -1.497674 -1.5409729 -1.3457234 -0.7391324 -0.092576504 0.3554225 0.487947 0.37486506 0.19803715 -0.22611713 -0.53434062 -0.74233413 -0.84212542 -0.98671913][-2.113658 -2.4259665 -2.5237687 -2.2782564 -1.5703206 -0.89557266 -0.32893181 0.20678711 0.34314346 0.24849319 -0.048595905 -0.31364155 -0.56199288 -0.62525272 -0.6222868][-2.8098803 -3.3823693 -3.571516 -3.3975458 -2.796963 -1.9224169 -1.2013595 -0.82285595 -0.61697078 -0.49979162 -0.63126278 -0.67344356 -0.71198463 -0.66775107 -0.52181172][-3.6842842 -4.3556032 -4.716002 -4.653316 -4.1254082 -3.4007707 -2.6691382 -1.9217658 -1.5407679 -1.4420402 -1.3274846 -1.3765564 -1.3321404 -1.0860407 -0.84230042][-4.2438679 -4.9154596 -5.3732228 -5.4668231 -5.2139916 -4.6319714 -3.9421661 -3.3270957 -2.8134649 -2.5020971 -2.2532768 -2.0159762 -1.7769847 -1.4654334 -1.1509197][-4.7261882 -5.1453276 -5.3834562 -5.6364651 -5.6139221 -5.1963673 -4.7273197 -4.2653236 -3.8361666 -3.5368984 -3.2087886 -2.8437691 -2.403913 -1.8938663 -1.4148004][-5.19632 -5.2884231 -5.3960648 -5.3119712 -5.2335992 -5.0058661 -4.6270394 -4.4089146 -4.2164216 -4.1625953 -3.95047 -3.5019059 -2.7595649 -2.0225139 -1.471055][-5.5398903 -5.176156 -4.8388777 -4.69564 -4.6176271 -4.3652897 -4.2057252 -4.1557755 -4.0681605 -4.1240377 -4.0197816 -3.664603 -2.8001866 -1.8528478 -1.2232826][-5.3594713 -4.7385416 -4.222527 -3.879303 -3.6997144 -3.6238019 -3.6786976 -3.8498421 -4.03085 -4.2061386 -4.0896058 -3.5092146 -2.4931273 -1.2808836 -0.55940008]]...]
INFO - root - 2017-12-16 04:05:13.531819: step 5710, loss = 0.30, batch loss = 0.24 (25.4 examples/sec; 0.315 sec/batch; 28h:35m:37s remains)
INFO - root - 2017-12-16 04:05:16.372218: step 5720, loss = 0.23, batch loss = 0.17 (27.9 examples/sec; 0.287 sec/batch; 26h:00m:48s remains)
INFO - root - 2017-12-16 04:05:19.165152: step 5730, loss = 0.25, batch loss = 0.19 (28.9 examples/sec; 0.276 sec/batch; 25h:05m:50s remains)
INFO - root - 2017-12-16 04:05:22.070928: step 5740, loss = 0.34, batch loss = 0.28 (27.8 examples/sec; 0.287 sec/batch; 26h:05m:17s remains)
INFO - root - 2017-12-16 04:05:24.965119: step 5750, loss = 0.20, batch loss = 0.15 (29.3 examples/sec; 0.273 sec/batch; 24h:46m:08s remains)
INFO - root - 2017-12-16 04:05:27.769553: step 5760, loss = 0.23, batch loss = 0.17 (27.4 examples/sec; 0.292 sec/batch; 26h:29m:23s remains)
INFO - root - 2017-12-16 04:05:30.584373: step 5770, loss = 0.24, batch loss = 0.19 (27.1 examples/sec; 0.295 sec/batch; 26h:45m:07s remains)
INFO - root - 2017-12-16 04:05:33.427203: step 5780, loss = 0.26, batch loss = 0.20 (26.9 examples/sec; 0.297 sec/batch; 26h:59m:48s remains)
INFO - root - 2017-12-16 04:05:36.215006: step 5790, loss = 0.29, batch loss = 0.24 (29.6 examples/sec; 0.271 sec/batch; 24h:32m:55s remains)
INFO - root - 2017-12-16 04:05:39.044154: step 5800, loss = 0.38, batch loss = 0.32 (28.7 examples/sec; 0.279 sec/batch; 25h:19m:26s remains)
2017-12-16 04:05:39.491349: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7986407 -5.8840694 -5.7707758 -5.662262 -5.6541524 -5.644475 -5.6321244 -5.769434 -5.92006 -5.967082 -6.0109558 -5.863349 -5.5902853 -5.2582088 -4.9723706][-5.9577188 -5.8172541 -5.7541752 -5.7359872 -5.6922684 -5.9197693 -6.1606536 -6.1491485 -6.18718 -6.3943219 -6.4728432 -6.3086925 -6.1125546 -5.8449187 -5.5978422][-5.2908731 -5.2234187 -5.1763134 -5.1837387 -5.2752008 -5.4841018 -5.5025482 -5.6786118 -5.9163122 -6.0207472 -6.2366018 -6.4336166 -6.4409504 -6.1490488 -5.9477949][-4.4875131 -3.9875407 -3.7489438 -3.8491983 -3.9160256 -4.0793571 -4.0744781 -4.1461134 -4.3271637 -4.7541056 -5.1682224 -5.5864506 -5.8773828 -5.8624444 -5.7895465][-2.9259312 -2.1298594 -1.6950669 -1.8630643 -2.0528524 -2.0604637 -1.8540084 -1.8499846 -2.0131135 -2.5957475 -3.3575692 -4.1128554 -4.7018323 -4.8476539 -4.7229733][-1.7032754 -0.81873393 -0.29957962 -0.18218184 -0.094316483 0.03653717 0.509377 0.82245827 0.67038155 0.013849258 -0.92470431 -2.0938032 -3.04178 -3.4863935 -3.5400956][-0.81850266 -0.0821023 0.32602644 0.46050406 0.81995487 1.568665 2.6251044 3.0801482 2.86453 2.2021484 1.2589288 0.018562794 -1.1237717 -1.9344456 -2.3616385][-0.38899803 0.22067738 0.67616129 0.91809225 1.1986661 1.9354987 3.0999794 3.9414549 4.012785 3.3461895 2.3634109 1.1029348 0.002436161 -0.70328188 -1.3470678][-0.45700264 -0.083441734 0.1365428 0.342875 0.794549 1.5473108 2.5987411 3.2751317 3.383007 3.0176959 2.2867694 1.1975331 0.21683168 -0.4410603 -0.95535564][-0.77803731 -0.56900144 -0.61471653 -0.46565604 -0.10028458 0.47765112 1.3121881 1.9735913 2.0768585 1.7636118 1.282373 0.53303719 -0.1914506 -0.6371243 -1.0930862][-1.4641161 -1.1975408 -1.0718172 -1.0363436 -0.87495494 -0.53535271 -0.035439491 0.25517178 0.33583546 0.14401627 -0.23166418 -0.73464227 -1.1310287 -1.2901902 -1.5559273][-1.7663853 -1.3818748 -1.3204184 -1.3187733 -1.1771672 -1.0479054 -0.95215988 -0.96605277 -1.1030006 -1.4104624 -1.6842356 -1.8882575 -1.9974065 -1.8087595 -1.8003216][-1.8559172 -1.3853981 -1.1974123 -1.2031252 -1.2220299 -1.2239292 -1.2808006 -1.6609123 -1.9503663 -2.2871618 -2.5435445 -2.5218868 -2.5080211 -2.1920154 -1.9367077][-1.9130414 -1.2910752 -1.0015109 -0.871217 -0.83856034 -0.95435858 -1.0631979 -1.6287038 -2.2651224 -2.8734524 -3.185674 -3.0906689 -2.7258387 -2.2487547 -1.943758][-1.8626299 -1.2158961 -0.87140846 -0.73800254 -0.66539955 -0.64803886 -0.77821088 -1.4666555 -2.1798282 -2.8938389 -3.3395646 -3.3007512 -3.1164405 -2.5397425 -2.1442115]]...]
INFO - root - 2017-12-16 04:05:42.283251: step 5810, loss = 0.23, batch loss = 0.17 (29.2 examples/sec; 0.274 sec/batch; 24h:49m:50s remains)
INFO - root - 2017-12-16 04:05:45.116254: step 5820, loss = 0.23, batch loss = 0.17 (27.9 examples/sec; 0.287 sec/batch; 26h:01m:40s remains)
INFO - root - 2017-12-16 04:05:47.975104: step 5830, loss = 0.24, batch loss = 0.18 (28.7 examples/sec; 0.279 sec/batch; 25h:17m:55s remains)
INFO - root - 2017-12-16 04:05:50.793541: step 5840, loss = 0.22, batch loss = 0.16 (29.1 examples/sec; 0.275 sec/batch; 24h:58m:52s remains)
INFO - root - 2017-12-16 04:05:53.643376: step 5850, loss = 0.29, batch loss = 0.23 (27.9 examples/sec; 0.287 sec/batch; 26h:01m:35s remains)
INFO - root - 2017-12-16 04:05:56.481236: step 5860, loss = 0.30, batch loss = 0.24 (28.4 examples/sec; 0.281 sec/batch; 25h:31m:41s remains)
INFO - root - 2017-12-16 04:05:59.285973: step 5870, loss = 0.25, batch loss = 0.20 (29.4 examples/sec; 0.272 sec/batch; 24h:43m:09s remains)
INFO - root - 2017-12-16 04:06:02.111817: step 5880, loss = 0.25, batch loss = 0.20 (28.9 examples/sec; 0.276 sec/batch; 25h:04m:54s remains)
INFO - root - 2017-12-16 04:06:04.973938: step 5890, loss = 0.30, batch loss = 0.25 (28.9 examples/sec; 0.277 sec/batch; 25h:08m:47s remains)
INFO - root - 2017-12-16 04:06:07.806692: step 5900, loss = 0.34, batch loss = 0.28 (27.8 examples/sec; 0.288 sec/batch; 26h:07m:24s remains)
2017-12-16 04:06:08.265793: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6428947 -3.4506631 -3.22362 -2.957989 -2.7472122 -2.6814771 -2.6960328 -2.6758571 -2.7711334 -2.9679439 -3.1351852 -3.4111371 -3.606442 -3.7872994 -3.8495166][-4.101419 -3.6987548 -3.2547908 -2.9333963 -2.7280107 -2.5722041 -2.6185198 -2.6215792 -2.7596917 -3.0446515 -3.3858016 -3.717627 -3.9084394 -4.1962023 -4.3486085][-4.3104219 -3.7659748 -3.1993651 -2.7463512 -2.5036476 -2.3244958 -2.3932559 -2.3790078 -2.5353708 -2.8163671 -3.0962467 -3.631655 -4.0840569 -4.540513 -4.727603][-4.3013291 -3.6741543 -2.9599109 -2.4205277 -2.060271 -1.8086731 -1.8560677 -1.797215 -1.8987732 -2.2434638 -2.758014 -3.3603249 -3.7838852 -4.2595539 -4.5287137][-4.0941696 -3.4813764 -2.7674499 -2.1081698 -1.6321843 -1.279566 -1.0858665 -0.95398211 -1.1089187 -1.4711893 -1.949548 -2.6600661 -3.1859679 -3.6577723 -3.7204528][-3.571847 -2.9975619 -2.3192439 -1.5405197 -0.84818387 -0.2945385 0.056033134 0.28269386 0.15621662 -0.39884233 -1.1384182 -1.9328997 -2.3880758 -2.6304264 -2.5888495][-3.03068 -2.5302043 -1.9379616 -1.1337664 -0.35192394 0.21664 0.73653507 0.96499538 0.76963282 0.22723436 -0.47913003 -1.1203802 -1.3618367 -1.2533112 -0.786577][-3.0052114 -2.557178 -1.931577 -1.1447589 -0.41566086 0.34113455 0.96910238 1.1312118 0.91452885 0.40620995 -0.29546881 -0.63120556 -0.44848061 0.00032901764 0.770658][-3.1699486 -2.7968938 -2.3373983 -1.649087 -1.0250852 -0.32625198 0.19056368 0.32782602 0.22729635 -0.0952549 -0.46265697 -0.4345665 0.051402569 0.81273365 1.6701007][-3.4598205 -3.0840855 -2.7438288 -2.2948065 -1.9800427 -1.4118514 -1.0932839 -1.0585456 -1.0431025 -1.1086731 -1.068608 -0.80490589 -0.038354874 0.79358578 1.5378079][-3.7141974 -3.3305979 -3.137728 -2.9417543 -2.9039874 -2.5080845 -2.3223803 -2.5168276 -2.6139331 -2.4805684 -2.0982118 -1.5949733 -0.90003204 -0.11727762 0.67783546][-3.7855136 -3.4498811 -3.3933477 -3.3945746 -3.6697881 -3.444932 -3.4622159 -3.7380402 -3.8261061 -3.7261331 -3.1673794 -2.6542735 -2.1253743 -1.5132189 -0.82496953][-3.3664155 -3.0060568 -3.1332734 -3.2219672 -3.6755316 -3.7834439 -3.958703 -4.479569 -4.9243536 -5.026484 -4.3757977 -3.8378358 -3.357832 -3.0723727 -2.6363468][-2.9218204 -2.519268 -2.7499056 -2.9197223 -3.4393497 -3.7511139 -4.1712122 -4.9557619 -5.584311 -5.9312067 -5.4532237 -4.9000535 -4.4187307 -4.2911315 -4.0700965][-2.550432 -2.1072562 -2.098382 -2.3379221 -2.9290452 -3.5260835 -4.2242303 -5.2306638 -6.0883245 -6.7501507 -6.5373578 -5.8967028 -5.2249365 -4.9020758 -4.6117363]]...]
INFO - root - 2017-12-16 04:06:11.137037: step 5910, loss = 0.31, batch loss = 0.25 (28.0 examples/sec; 0.286 sec/batch; 25h:54m:19s remains)
INFO - root - 2017-12-16 04:06:13.938231: step 5920, loss = 0.31, batch loss = 0.25 (28.6 examples/sec; 0.279 sec/batch; 25h:20m:33s remains)
INFO - root - 2017-12-16 04:06:16.760121: step 5930, loss = 0.27, batch loss = 0.21 (28.9 examples/sec; 0.277 sec/batch; 25h:09m:02s remains)
INFO - root - 2017-12-16 04:06:19.563605: step 5940, loss = 0.31, batch loss = 0.25 (28.8 examples/sec; 0.278 sec/batch; 25h:12m:12s remains)
INFO - root - 2017-12-16 04:06:22.353432: step 5950, loss = 0.36, batch loss = 0.30 (28.5 examples/sec; 0.281 sec/batch; 25h:27m:19s remains)
INFO - root - 2017-12-16 04:06:25.228303: step 5960, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 25h:49m:37s remains)
INFO - root - 2017-12-16 04:06:28.069705: step 5970, loss = 0.32, batch loss = 0.27 (27.9 examples/sec; 0.287 sec/batch; 26h:00m:10s remains)
INFO - root - 2017-12-16 04:06:30.871624: step 5980, loss = 0.30, batch loss = 0.24 (29.6 examples/sec; 0.270 sec/batch; 24h:29m:24s remains)
INFO - root - 2017-12-16 04:06:33.676385: step 5990, loss = 0.24, batch loss = 0.19 (27.9 examples/sec; 0.287 sec/batch; 25h:59m:28s remains)
INFO - root - 2017-12-16 04:06:36.497117: step 6000, loss = 0.25, batch loss = 0.20 (29.0 examples/sec; 0.276 sec/batch; 24h:59m:53s remains)
2017-12-16 04:06:36.923214: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.678678 -4.977519 -5.2529726 -5.7818613 -6.1726751 -6.0996628 -6.0106368 -5.798943 -5.6798358 -5.381918 -5.0641689 -4.9334559 -4.5707178 -4.1849985 -4.1257663][-3.7374892 -4.2633057 -4.7590394 -5.2162051 -5.7695727 -6.2371392 -6.5257487 -6.3010669 -6.1088524 -5.8397098 -5.5702176 -5.2092156 -4.7637191 -4.1762552 -3.7898211][-3.3893785 -3.5926862 -3.8904006 -4.4962425 -5.208643 -5.7847943 -6.091558 -6.15875 -6.2288761 -5.7902236 -5.5364785 -5.4206271 -5.1043777 -4.3121767 -3.5659425][-2.6908596 -2.9633737 -3.2621422 -3.5437422 -3.9410303 -4.3343844 -4.6822739 -4.6523685 -4.852859 -5.1533647 -5.151895 -5.03453 -4.8760118 -4.1433411 -3.3359542][-2.7320676 -2.5126116 -2.2774103 -2.3120923 -2.5335884 -2.4014933 -2.2654486 -2.1571033 -2.389477 -3.0193424 -3.8438907 -4.3293033 -4.2329049 -3.8644297 -3.3735065][-3.1661005 -2.8966739 -2.4490256 -1.6710994 -0.55833912 0.28929472 0.65506983 0.9783988 0.52695036 -0.50581455 -1.7269976 -2.9063988 -3.7004561 -3.8006859 -3.43041][-3.5195189 -3.1901073 -2.6553347 -1.6153049 0.076759815 2.0056052 3.4416347 3.8219938 3.1520224 1.6535482 -0.29106808 -1.9341648 -3.0016179 -3.7730956 -4.0842609][-3.5615816 -3.2919111 -2.9604831 -1.8314223 0.1662631 2.5885673 4.3787851 4.9139404 4.295434 2.4849572 0.33067751 -1.4911041 -2.7972558 -3.7493317 -4.2901077][-4.6521826 -4.5888395 -4.1070051 -2.9609978 -1.0537262 1.4448905 3.6596155 4.4967842 3.9019041 1.992909 -0.31271648 -2.3825574 -3.8118649 -4.5908885 -4.7694211][-5.595767 -6.3033695 -6.5965395 -5.5995874 -3.5780859 -1.3027325 0.48476458 1.4618435 1.2603946 -0.32449722 -2.3141532 -4.171638 -5.5071545 -6.0405436 -5.9076815][-7.0441847 -7.8612065 -8.1821289 -7.7732711 -6.7072668 -4.9756632 -3.2668433 -2.482296 -2.421144 -3.3612051 -4.667202 -5.92618 -6.6959133 -6.8670077 -6.6037722][-7.2994471 -8.4349928 -9.0618305 -8.7062778 -7.6878963 -6.5936985 -5.5050659 -4.8528647 -4.7065926 -5.2598815 -6.0525794 -6.8593531 -7.285605 -7.1704245 -6.7583017][-6.929431 -7.8834686 -8.3792095 -8.27637 -7.8302526 -6.9399691 -5.9977536 -5.6085243 -5.3889604 -5.6110358 -6.2397804 -6.8928404 -7.1441636 -6.8632956 -6.3953924][-5.608654 -6.3613181 -6.775321 -6.746418 -6.6406965 -6.0656543 -5.4960747 -5.3076935 -5.2306371 -5.2866631 -5.4479179 -5.8618593 -6.1090245 -5.9534988 -5.5930538][-3.8502486 -4.3734722 -4.7904162 -4.94263 -4.8605165 -4.4981618 -4.1935778 -3.9845638 -3.9022985 -4.0384274 -4.2332125 -4.4863067 -4.6344514 -4.6989489 -4.5848379]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 04:06:39.789974: step 6010, loss = 0.25, batch loss = 0.19 (28.8 examples/sec; 0.277 sec/batch; 25h:09m:33s remains)
INFO - root - 2017-12-16 04:06:42.650488: step 6020, loss = 0.38, batch loss = 0.32 (28.8 examples/sec; 0.278 sec/batch; 25h:10m:49s remains)
INFO - root - 2017-12-16 04:06:45.468874: step 6030, loss = 0.35, batch loss = 0.29 (28.1 examples/sec; 0.285 sec/batch; 25h:50m:47s remains)
INFO - root - 2017-12-16 04:06:48.298338: step 6040, loss = 0.26, batch loss = 0.20 (27.6 examples/sec; 0.290 sec/batch; 26h:16m:07s remains)
INFO - root - 2017-12-16 04:06:51.176107: step 6050, loss = 0.27, batch loss = 0.21 (26.3 examples/sec; 0.304 sec/batch; 27h:32m:01s remains)
INFO - root - 2017-12-16 04:06:54.054173: step 6060, loss = 0.28, batch loss = 0.22 (27.5 examples/sec; 0.291 sec/batch; 26h:23m:43s remains)
INFO - root - 2017-12-16 04:06:56.901026: step 6070, loss = 0.31, batch loss = 0.25 (27.9 examples/sec; 0.287 sec/batch; 25h:59m:32s remains)
INFO - root - 2017-12-16 04:06:59.702852: step 6080, loss = 0.32, batch loss = 0.26 (27.5 examples/sec; 0.291 sec/batch; 26h:24m:12s remains)
INFO - root - 2017-12-16 04:07:02.540511: step 6090, loss = 0.29, batch loss = 0.23 (28.7 examples/sec; 0.279 sec/batch; 25h:18m:06s remains)
INFO - root - 2017-12-16 04:07:05.347775: step 6100, loss = 0.26, batch loss = 0.20 (28.4 examples/sec; 0.282 sec/batch; 25h:34m:30s remains)
2017-12-16 04:07:05.807860: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8901534 -2.1542487 -2.4268034 -2.5223253 -2.5717371 -2.7164657 -2.7862291 -2.9303777 -3.1803603 -3.5381274 -3.8150063 -3.8453979 -3.7938776 -3.4883623 -3.1655307][-1.8937535 -1.90025 -1.9435246 -1.8827727 -1.7879767 -1.9237938 -1.9598029 -2.0920932 -2.3423874 -2.8725247 -3.3546484 -3.4613638 -3.4374738 -3.0709517 -2.5664291][-1.8129146 -1.5869303 -1.4291768 -1.061511 -0.74409986 -0.70249987 -0.71911335 -0.976835 -1.3759842 -2.0006344 -2.6057177 -3.0044844 -3.1346087 -2.7329845 -2.0651665][-1.409508 -0.88568234 -0.53268123 0.032180309 0.6047225 0.7863307 0.65801048 0.10630274 -0.6696651 -1.4848006 -2.1179051 -2.5618877 -2.8273602 -2.5828905 -1.9916394][-1.4073927 -0.36560154 0.54841423 1.1652107 1.7150831 1.9503136 1.8264227 1.1979609 0.25627613 -0.648859 -1.4091189 -1.9000173 -2.1780765 -2.121516 -1.7969637][-1.5393612 -0.42685556 0.72897005 1.7440672 2.4819326 2.6994429 2.527041 1.9541817 1.0714269 0.1559391 -0.58427286 -1.2056787 -1.7537155 -1.9368978 -1.7765744][-1.842968 -0.5623312 0.77026606 1.9938898 2.8586888 3.2442088 3.1859455 2.4730558 1.3995519 0.46527958 -0.22864723 -0.9835608 -1.7345114 -2.4090092 -2.7741933][-2.7586632 -1.5547698 -0.043930531 1.2447038 2.2293262 2.6843657 2.7050672 2.2045751 1.1984558 0.19429398 -0.67591858 -1.5701959 -2.5231037 -3.4543171 -4.0516424][-4.0326347 -3.0245337 -1.7145767 -0.37691927 0.84771156 1.4491415 1.4739676 1.0949721 0.30046749 -0.58360052 -1.5003996 -2.4313703 -3.4229877 -4.4249172 -4.9100914][-5.2325468 -4.4522481 -3.3862934 -2.2075481 -1.0458679 -0.31309795 0.019933224 -0.31296587 -0.96916866 -1.6727815 -2.4099298 -3.1976662 -4.0390983 -4.8514 -5.2004023][-5.9905524 -5.3313932 -4.4735909 -3.5069625 -2.6431966 -2.0364168 -1.6540954 -1.8391149 -2.2949381 -2.8900201 -3.5154977 -4.2388148 -4.9213328 -5.3039427 -5.2976589][-6.4995527 -6.0650268 -5.4045873 -4.6514339 -3.9510202 -3.4101834 -3.0669756 -3.242455 -3.5703928 -4.0036931 -4.5118532 -5.0830274 -5.5544362 -5.6462817 -5.2617903][-6.1050191 -5.8493128 -5.5302653 -5.0558629 -4.5853205 -4.1910887 -3.9781327 -4.1498771 -4.4412618 -4.7071118 -4.9670343 -5.2976732 -5.5379333 -5.4580464 -4.9316621][-5.3755679 -5.0350246 -4.7510095 -4.5922894 -4.4162087 -4.2059407 -4.0955839 -4.2463751 -4.4541478 -4.6110096 -4.6671472 -4.7230606 -4.7311144 -4.5863838 -4.1815958][-4.726326 -4.4333539 -4.2114606 -4.1332469 -4.0563836 -3.9573698 -3.9285231 -4.0288796 -4.1619844 -4.279109 -4.3134136 -4.273385 -4.1874533 -4.0659018 -3.8078108]]...]
INFO - root - 2017-12-16 04:07:08.634652: step 6110, loss = 0.30, batch loss = 0.24 (28.9 examples/sec; 0.277 sec/batch; 25h:06m:48s remains)
INFO - root - 2017-12-16 04:07:11.467697: step 6120, loss = 0.34, batch loss = 0.28 (29.3 examples/sec; 0.273 sec/batch; 24h:42m:47s remains)
INFO - root - 2017-12-16 04:07:14.262466: step 6130, loss = 0.37, batch loss = 0.31 (28.2 examples/sec; 0.284 sec/batch; 25h:45m:37s remains)
INFO - root - 2017-12-16 04:07:17.100832: step 6140, loss = 0.28, batch loss = 0.23 (28.9 examples/sec; 0.277 sec/batch; 25h:08m:09s remains)
INFO - root - 2017-12-16 04:07:19.918150: step 6150, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 25h:43m:44s remains)
INFO - root - 2017-12-16 04:07:22.751217: step 6160, loss = 0.22, batch loss = 0.16 (28.2 examples/sec; 0.284 sec/batch; 25h:45m:03s remains)
INFO - root - 2017-12-16 04:07:25.560854: step 6170, loss = 0.28, batch loss = 0.22 (28.3 examples/sec; 0.283 sec/batch; 25h:39m:16s remains)
INFO - root - 2017-12-16 04:07:28.367273: step 6180, loss = 0.29, batch loss = 0.23 (28.9 examples/sec; 0.277 sec/batch; 25h:06m:44s remains)
INFO - root - 2017-12-16 04:07:31.201430: step 6190, loss = 0.41, batch loss = 0.35 (28.9 examples/sec; 0.277 sec/batch; 25h:05m:19s remains)
INFO - root - 2017-12-16 04:07:34.039342: step 6200, loss = 0.31, batch loss = 0.25 (27.9 examples/sec; 0.287 sec/batch; 26h:01m:28s remains)
2017-12-16 04:07:34.500672: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8589845 -5.9329362 -5.9734125 -5.9903741 -5.8040967 -5.5002012 -5.3516793 -5.7101026 -6.1525455 -6.6367741 -7.237052 -7.8553791 -8.1739483 -8.1326733 -7.6069727][-5.677701 -5.7909436 -5.913012 -5.605227 -5.154294 -5.0168495 -5.0587535 -5.358088 -5.8820767 -6.6014481 -7.4335117 -8.1149988 -8.4740505 -8.3952875 -7.7682147][-5.1009803 -4.8937378 -4.7674079 -4.4771538 -4.0050063 -3.5028591 -3.3599529 -3.899199 -4.70336 -5.6661744 -6.8602848 -7.8044171 -8.2713652 -8.3771868 -7.8577824][-4.117125 -3.75397 -3.4686806 -2.9270368 -2.171139 -1.5228908 -1.2865658 -1.8415241 -2.766046 -3.8997169 -5.1136913 -6.5012455 -7.5633125 -7.8786073 -7.4379482][-2.8952751 -2.2749343 -1.7808416 -1.1931951 -0.25972557 0.45796204 0.77192354 0.5172081 -0.24167347 -1.7268181 -3.518873 -5.0929093 -6.2459865 -7.1152692 -7.2583666][-1.7926409 -0.830549 -0.037751198 0.70123053 1.797821 2.688086 3.1553106 3.0393052 2.2188401 0.72257519 -1.1033943 -3.2677953 -5.1827188 -6.5296755 -6.9065161][-1.4099333 -0.32358789 0.74042225 1.7342668 2.9778514 4.12636 4.7967548 4.7139912 3.985342 2.3947792 0.31779671 -2.1019642 -4.20003 -5.8560147 -6.52824][-2.2152758 -0.98355651 0.055490971 1.314476 2.9861274 4.15108 4.8508663 4.8266172 3.9866343 2.3439989 0.39518356 -1.9885027 -4.1566558 -5.7554307 -6.2571836][-3.5860045 -2.7320542 -1.5360842 -0.086987019 1.3809896 2.6891775 3.548687 3.667449 2.9614353 1.4030342 -0.41991329 -2.5964375 -4.5269914 -5.9295969 -6.4094][-4.7299757 -4.289072 -3.7054787 -2.3337471 -0.840286 0.43196058 1.1088839 1.2665009 0.84020376 -0.39253378 -1.8547723 -3.7770829 -5.439538 -6.6231775 -7.0691433][-6.0838675 -5.6782494 -5.0318813 -4.1773376 -3.235507 -2.0349762 -1.2266996 -0.99797463 -1.3810914 -2.3916771 -3.578825 -5.1194119 -6.3382778 -7.1710572 -7.5251675][-6.2277637 -6.2217164 -6.1952772 -5.7171373 -5.0276937 -4.1671104 -3.5159037 -3.1840587 -3.27138 -3.9191933 -4.8664865 -6.1908431 -7.265399 -7.7664614 -7.8449392][-5.8329077 -5.9875073 -6.1760893 -6.2041531 -6.086647 -5.4736814 -4.8667927 -4.4247155 -4.2976837 -4.7468534 -5.5116606 -6.3999453 -7.0375347 -7.6717768 -7.9099536][-4.5207672 -4.7469473 -5.0634813 -5.4157162 -5.5582666 -5.2791615 -4.9402995 -4.6797523 -4.5818505 -4.7724991 -5.178153 -5.8541241 -6.38751 -6.6461973 -6.6268311][-4.0730071 -4.055613 -4.1715384 -4.4031558 -4.5888972 -4.5657229 -4.460093 -4.308856 -4.2235985 -4.3990879 -4.7357116 -4.9695344 -5.0874424 -5.1645155 -5.1640391]]...]
INFO - root - 2017-12-16 04:07:37.300270: step 6210, loss = 0.30, batch loss = 0.24 (28.7 examples/sec; 0.278 sec/batch; 25h:13m:51s remains)
INFO - root - 2017-12-16 04:07:40.112781: step 6220, loss = 0.23, batch loss = 0.17 (28.1 examples/sec; 0.284 sec/batch; 25h:45m:33s remains)
INFO - root - 2017-12-16 04:07:42.933691: step 6230, loss = 0.26, batch loss = 0.20 (28.4 examples/sec; 0.282 sec/batch; 25h:32m:50s remains)
INFO - root - 2017-12-16 04:07:45.822790: step 6240, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.278 sec/batch; 25h:13m:22s remains)
INFO - root - 2017-12-16 04:07:48.657660: step 6250, loss = 0.32, batch loss = 0.26 (28.9 examples/sec; 0.276 sec/batch; 25h:03m:22s remains)
INFO - root - 2017-12-16 04:07:51.505378: step 6260, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.284 sec/batch; 25h:41m:34s remains)
INFO - root - 2017-12-16 04:07:54.385312: step 6270, loss = 0.30, batch loss = 0.24 (29.5 examples/sec; 0.272 sec/batch; 24h:36m:17s remains)
INFO - root - 2017-12-16 04:07:57.243520: step 6280, loss = 0.33, batch loss = 0.27 (27.2 examples/sec; 0.294 sec/batch; 26h:36m:53s remains)
INFO - root - 2017-12-16 04:08:00.074355: step 6290, loss = 0.31, batch loss = 0.25 (28.6 examples/sec; 0.279 sec/batch; 25h:18m:38s remains)
INFO - root - 2017-12-16 04:08:02.903445: step 6300, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 25h:44m:00s remains)
2017-12-16 04:08:03.410138: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0483112 -3.0468559 -3.0639884 -3.0799408 -3.0937502 -3.062336 -3.0303159 -3.1801429 -3.4842775 -3.8371413 -4.4351087 -4.9972138 -5.3916821 -5.5537634 -5.3967524][-3.4455049 -3.5284064 -3.5808897 -3.4965811 -3.334641 -3.1987565 -3.0171862 -2.9817548 -3.3225682 -3.8246832 -4.5817966 -5.21506 -5.7488813 -5.997261 -5.7841043][-3.71613 -3.934087 -3.9969964 -3.7798018 -3.3652411 -2.9316165 -2.4912143 -2.2560954 -2.4534407 -3.1461678 -4.2223544 -5.0400233 -5.7491589 -6.0849104 -5.9596972][-4.0003414 -4.18088 -4.1672134 -3.6741619 -2.9412236 -2.2300549 -1.4699154 -1.1256628 -1.2739761 -1.9238145 -3.2366734 -4.4835992 -5.4732866 -6.0077858 -6.0930104][-4.1616464 -4.1113548 -3.8064594 -3.012866 -1.8086622 -0.68599916 0.29686022 0.75462961 0.59161472 -0.27741909 -1.8663642 -3.3335257 -4.6858773 -5.5991678 -5.8608418][-4.3022423 -4.0317435 -3.429018 -2.1680789 -0.51646495 0.88462305 2.1135011 2.6616659 2.3888922 1.2656808 -0.52489614 -2.2884226 -3.9134827 -5.1138339 -5.6581903][-4.4127808 -3.9083123 -3.00665 -1.4897521 0.49491882 2.1172395 3.5268831 4.0685396 3.7130909 2.4016037 0.37023258 -1.7098904 -3.5244868 -4.8605547 -5.562839][-4.501174 -4.0274997 -2.8922167 -1.3089349 0.53283834 2.2610664 3.7084875 4.2324791 3.8923855 2.5128975 0.52727127 -1.6408908 -3.6184049 -4.98103 -5.77309][-5.1086187 -4.5361042 -3.5483413 -2.0243864 -0.42785406 1.093966 2.4251747 2.9172978 2.6929979 1.5041485 -0.22818661 -2.2814152 -4.1094012 -5.4273024 -6.1808195][-5.7269382 -5.4880428 -4.8717093 -3.6988072 -2.2957897 -0.98084784 0.15288973 0.58193874 0.510839 -0.28498554 -1.4462669 -3.0667815 -4.6187067 -5.7222757 -6.428699][-6.156229 -5.98489 -5.481111 -4.7314115 -3.8485584 -2.6567929 -1.6628416 -1.3174684 -1.3923402 -1.9396048 -2.5996578 -3.6507382 -4.7033825 -5.5299187 -6.1911469][-6.2633648 -6.0681047 -5.6357489 -4.9918523 -4.392417 -3.5684092 -2.7624578 -2.4331057 -2.3469346 -2.5331087 -2.8944635 -3.6330123 -4.313725 -4.876411 -5.395381][-5.4011111 -5.0777216 -4.7412486 -4.1805449 -3.8967896 -3.5346165 -3.1920104 -2.9498906 -2.7546396 -2.809515 -2.8593442 -3.0878491 -3.5553319 -4.0620441 -4.4844422][-4.6738806 -4.093647 -3.5402572 -3.0262017 -2.7970603 -2.4585655 -2.4557133 -2.5542903 -2.4444835 -2.4544086 -2.3957462 -2.4308181 -2.6256425 -3.1083632 -3.6429439][-4.1783767 -3.3839345 -2.7226028 -2.1773148 -2.057112 -1.9413948 -2.0138216 -2.1518977 -2.1023455 -2.0355783 -1.9398515 -1.7525246 -1.7934995 -2.1850169 -2.6838982]]...]
INFO - root - 2017-12-16 04:08:06.236598: step 6310, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.284 sec/batch; 25h:41m:39s remains)
INFO - root - 2017-12-16 04:08:09.074110: step 6320, loss = 0.25, batch loss = 0.19 (28.5 examples/sec; 0.280 sec/batch; 25h:24m:13s remains)
INFO - root - 2017-12-16 04:08:11.893801: step 6330, loss = 0.36, batch loss = 0.31 (28.9 examples/sec; 0.277 sec/batch; 25h:03m:22s remains)
INFO - root - 2017-12-16 04:08:14.756632: step 6340, loss = 0.26, batch loss = 0.20 (26.4 examples/sec; 0.303 sec/batch; 27h:29m:29s remains)
INFO - root - 2017-12-16 04:08:17.604114: step 6350, loss = 0.25, batch loss = 0.20 (27.7 examples/sec; 0.288 sec/batch; 26h:08m:01s remains)
INFO - root - 2017-12-16 04:08:20.465922: step 6360, loss = 0.25, batch loss = 0.19 (26.9 examples/sec; 0.297 sec/batch; 26h:53m:39s remains)
INFO - root - 2017-12-16 04:08:23.289254: step 6370, loss = 0.28, batch loss = 0.22 (27.0 examples/sec; 0.297 sec/batch; 26h:53m:25s remains)
INFO - root - 2017-12-16 04:08:26.122095: step 6380, loss = 0.26, batch loss = 0.20 (27.2 examples/sec; 0.295 sec/batch; 26h:41m:12s remains)
INFO - root - 2017-12-16 04:08:28.936882: step 6390, loss = 0.23, batch loss = 0.18 (27.3 examples/sec; 0.293 sec/batch; 26h:33m:16s remains)
INFO - root - 2017-12-16 04:08:31.716964: step 6400, loss = 0.35, batch loss = 0.29 (29.6 examples/sec; 0.270 sec/batch; 24h:29m:34s remains)
2017-12-16 04:08:32.140302: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3070168 -3.0767131 -2.8952589 -2.6313772 -2.5921254 -2.5904102 -2.6220696 -2.8332038 -3.1277995 -3.3831835 -3.6746314 -3.8622351 -4.1747108 -4.450459 -4.8075528][-2.96983 -2.7978072 -2.591681 -2.4299586 -2.1961675 -2.04826 -1.8659089 -1.9186857 -2.360687 -2.7945333 -3.2938547 -3.7586737 -4.0641942 -4.2981997 -4.6193495][-2.2377894 -2.0889349 -1.7546263 -1.4850168 -1.3160808 -1.2502491 -1.1075599 -1.1714518 -1.6112716 -2.3194792 -2.9557147 -3.3933196 -3.7944324 -3.894067 -4.1381903][-1.5362062 -1.5397334 -1.4160554 -1.1626437 -1.0168293 -0.99864459 -0.86527109 -1.0035017 -1.4545789 -2.1788726 -2.6854634 -3.09079 -3.2425382 -3.3857646 -3.859982][-1.0238836 -0.99031734 -0.94295669 -0.71813488 -0.41393709 -0.2497921 -0.05073452 -0.18287611 -0.60187864 -1.3754876 -2.039809 -2.3991642 -2.4937525 -2.5310202 -3.0555491][-0.80874538 -0.64497757 -0.47503376 -0.10616875 0.49056149 0.90201235 1.1502576 1.086997 0.69027805 -0.20615816 -0.87436819 -1.3026497 -1.5163889 -1.8003683 -2.4522262][-0.49919105 -0.2727623 -0.26055002 0.0986433 0.6278739 1.0619287 1.4744906 1.6592994 1.6510234 0.98044538 0.27661133 -0.22696447 -0.57888436 -1.1304367 -1.864572][-0.82329845 -0.627285 -0.60194468 -0.36741114 0.085787773 0.46653748 0.978704 1.5369687 1.9766912 1.8683143 1.5245981 0.98177433 0.5584259 -0.19197083 -1.4790242][-1.5253398 -1.4773204 -1.6422846 -1.5498385 -1.0792472 -0.63198233 -0.073894024 0.76069832 1.6277218 1.9542909 1.9533758 1.7202902 1.1774116 0.36257267 -0.860559][-2.2032723 -2.2599394 -2.5008025 -2.6608377 -2.3982913 -2.0498714 -1.5112855 -0.63262129 0.26357889 1.0553083 1.5359845 1.5717692 1.3385739 0.53841019 -0.72889757][-2.8967161 -3.0192289 -3.3657129 -3.7303367 -3.8522036 -3.5774536 -3.0490072 -2.1178751 -1.0475488 -0.022892952 0.73391581 0.885231 0.69215488 -0.15382051 -1.4385827][-4.2448273 -4.3474708 -4.6551366 -5.19667 -5.5514922 -5.4961014 -5.0663657 -4.1127195 -2.8525233 -1.6783383 -0.83106136 -0.50579667 -0.57252336 -1.3533857 -2.6919661][-5.421052 -5.6084747 -6.1744576 -6.9384022 -7.6770973 -8.0142279 -7.8079128 -6.7944651 -5.2243886 -3.7907937 -2.7877054 -2.4372041 -2.7024486 -3.3556998 -4.1622019][-5.9785805 -6.30887 -7.081677 -8.2379913 -9.3982944 -9.9840565 -9.6863356 -8.8095884 -7.4627142 -5.7581687 -4.62476 -4.5569582 -5.003 -5.6357536 -6.3931379][-6.1255851 -6.5681543 -7.592483 -8.7482567 -9.8216743 -10.628819 -10.556381 -9.7647781 -8.4169636 -7.0299978 -6.2474842 -6.2443972 -6.7325792 -7.3412423 -7.8419604]]...]
INFO - root - 2017-12-16 04:08:34.964186: step 6410, loss = 0.28, batch loss = 0.22 (28.6 examples/sec; 0.280 sec/batch; 25h:19m:48s remains)
INFO - root - 2017-12-16 04:08:37.791723: step 6420, loss = 0.38, batch loss = 0.32 (27.3 examples/sec; 0.293 sec/batch; 26h:31m:22s remains)
INFO - root - 2017-12-16 04:08:40.640958: step 6430, loss = 0.36, batch loss = 0.30 (28.5 examples/sec; 0.281 sec/batch; 25h:24m:55s remains)
INFO - root - 2017-12-16 04:08:43.429546: step 6440, loss = 0.25, batch loss = 0.19 (29.0 examples/sec; 0.276 sec/batch; 25h:01m:33s remains)
INFO - root - 2017-12-16 04:08:46.277458: step 6450, loss = 0.35, batch loss = 0.30 (28.1 examples/sec; 0.285 sec/batch; 25h:48m:28s remains)
INFO - root - 2017-12-16 04:08:49.070224: step 6460, loss = 0.28, batch loss = 0.22 (28.6 examples/sec; 0.280 sec/batch; 25h:22m:38s remains)
INFO - root - 2017-12-16 04:08:51.878659: step 6470, loss = 0.31, batch loss = 0.25 (28.2 examples/sec; 0.284 sec/batch; 25h:42m:52s remains)
INFO - root - 2017-12-16 04:08:54.665164: step 6480, loss = 0.39, batch loss = 0.33 (28.4 examples/sec; 0.282 sec/batch; 25h:31m:39s remains)
INFO - root - 2017-12-16 04:08:57.466015: step 6490, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.280 sec/batch; 25h:18m:44s remains)
INFO - root - 2017-12-16 04:09:00.325722: step 6500, loss = 0.29, batch loss = 0.23 (27.6 examples/sec; 0.290 sec/batch; 26h:13m:18s remains)
2017-12-16 04:09:00.790418: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.441874 -5.4779992 -5.5255289 -5.36656 -5.1429768 -5.0984163 -4.9054813 -4.6912594 -4.5593567 -4.5426626 -4.6758294 -4.8578625 -5.0475831 -4.9297915 -4.7793446][-5.8081164 -5.623085 -5.4291196 -5.4768853 -5.4616051 -5.3782821 -5.254334 -5.200655 -5.0697222 -5.2223163 -5.4373388 -5.4722161 -5.57929 -5.2809272 -5.1266317][-5.4380493 -5.325851 -5.2176442 -4.9051247 -4.6195312 -4.8657923 -4.9333014 -5.0170069 -5.1569915 -5.3611507 -5.4697132 -5.5983362 -5.7433329 -5.3752594 -5.22791][-4.7390881 -4.2015095 -3.8605134 -3.6889892 -3.4522057 -3.3874631 -3.389998 -3.8742423 -4.3285217 -4.6997805 -4.9899673 -5.0016775 -4.8933759 -4.7250051 -4.8781486][-3.6138194 -2.8244104 -2.0743866 -1.5770812 -1.1177697 -0.958585 -1.0236955 -1.5933034 -2.2327483 -3.0317593 -3.5433135 -3.6233706 -3.7117095 -3.5842004 -3.8266306][-2.7072048 -1.4657228 -0.27734423 0.64351749 1.6158662 1.7598624 1.5252256 0.80564451 -0.028322697 -0.93406844 -1.5806677 -1.8041773 -1.9403863 -2.201349 -2.8249984][-2.3445437 -1.0979161 0.38767862 1.8602791 3.1651392 3.8073635 3.9598904 2.9555674 1.8968282 0.93121386 0.26133871 -0.13491392 -0.51167274 -0.99492979 -1.8519385][-2.3186615 -1.1752155 0.31661987 2.0064187 3.5501828 4.2893772 4.6392641 3.9614477 3.139173 2.1559558 1.4052186 1.1858106 0.7959609 -0.035746098 -1.1310675][-3.0026412 -2.0493073 -0.67833996 0.83488274 2.3742151 3.4290047 3.9094725 3.4008317 2.8937521 2.1920753 1.6387811 1.3376837 0.95542526 0.42090893 -0.67699194][-3.5117176 -3.0658927 -2.4095159 -1.0103095 0.46492958 1.3168697 1.8976526 1.7664132 1.6344438 1.1427588 0.84862804 0.68311691 0.33176661 -0.21272373 -1.2702131][-4.7430816 -4.3735161 -3.9041231 -3.148397 -2.3281262 -1.308208 -0.53367305 -0.525409 -0.38200569 -0.49950886 -0.47937655 -0.51160622 -0.63865638 -1.1960652 -2.2245545][-5.9472666 -5.4780178 -5.1197572 -4.6644816 -4.11137 -3.4520383 -3.0437498 -2.7298412 -2.2749233 -2.087513 -1.8758337 -1.8288119 -1.9045248 -2.3622341 -3.1353316][-6.7593837 -6.3008337 -5.8686943 -5.233633 -4.8104343 -4.3647408 -4.0706792 -3.8977447 -3.5686247 -3.147336 -2.7781935 -2.7519531 -2.8767715 -3.3528771 -3.9333558][-7.5957251 -6.8805647 -6.20995 -5.5238671 -4.9257722 -4.4371257 -4.3063426 -4.1922469 -4.0052614 -3.7085886 -3.4615269 -3.5116596 -3.8729715 -4.405077 -4.8654761][-8.4522257 -7.5333486 -6.5671072 -5.6327744 -4.9554911 -4.4118385 -4.1203585 -4.1459975 -4.2579818 -4.2389407 -4.2532721 -4.4078016 -4.9051046 -5.4251337 -5.5487909]]...]
INFO - root - 2017-12-16 04:09:03.617409: step 6510, loss = 0.19, batch loss = 0.13 (27.5 examples/sec; 0.291 sec/batch; 26h:22m:20s remains)
INFO - root - 2017-12-16 04:09:06.419978: step 6520, loss = 0.22, batch loss = 0.17 (27.9 examples/sec; 0.287 sec/batch; 25h:58m:49s remains)
INFO - root - 2017-12-16 04:09:09.232335: step 6530, loss = 0.27, batch loss = 0.21 (29.8 examples/sec; 0.269 sec/batch; 24h:19m:26s remains)
INFO - root - 2017-12-16 04:09:12.100508: step 6540, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.283 sec/batch; 25h:38m:52s remains)
INFO - root - 2017-12-16 04:09:14.939549: step 6550, loss = 0.24, batch loss = 0.18 (27.9 examples/sec; 0.287 sec/batch; 25h:59m:10s remains)
INFO - root - 2017-12-16 04:09:17.738223: step 6560, loss = 0.29, batch loss = 0.23 (28.7 examples/sec; 0.279 sec/batch; 25h:15m:36s remains)
INFO - root - 2017-12-16 04:09:20.619295: step 6570, loss = 0.24, batch loss = 0.18 (26.7 examples/sec; 0.299 sec/batch; 27h:04m:59s remains)
INFO - root - 2017-12-16 04:09:23.421218: step 6580, loss = 0.32, batch loss = 0.26 (28.9 examples/sec; 0.276 sec/batch; 25h:01m:26s remains)
INFO - root - 2017-12-16 04:09:26.287307: step 6590, loss = 0.35, batch loss = 0.29 (28.1 examples/sec; 0.284 sec/batch; 25h:43m:57s remains)
INFO - root - 2017-12-16 04:09:29.137455: step 6600, loss = 0.39, batch loss = 0.34 (27.3 examples/sec; 0.293 sec/batch; 26h:32m:00s remains)
2017-12-16 04:09:29.587736: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.506155 -5.1971283 -5.0898786 -5.1833515 -4.9175253 -4.7778988 -4.6276383 -4.4458237 -4.3507876 -4.2675748 -4.1930585 -4.0821757 -4.1667867 -4.4824443 -4.8263044][-5.1839046 -4.9357533 -5.0231738 -5.0377755 -4.926888 -4.897965 -4.7157626 -4.4946508 -4.36854 -4.2860107 -4.1716065 -3.9084377 -3.9677515 -4.3292117 -4.5813766][-3.9844158 -3.8290744 -4.1465077 -4.3872623 -4.3805866 -4.387886 -4.3885775 -4.1866093 -4.0395775 -4.1053753 -4.0263147 -3.8609924 -3.9446144 -4.3868928 -4.697063][-2.2430608 -2.0520008 -2.4148846 -2.8867421 -3.1150577 -3.2303016 -3.1425924 -3.0451882 -3.1477621 -3.438899 -3.5822012 -3.9080935 -4.3070006 -4.6950669 -5.0770679][-1.0501466 -0.321877 -0.46105146 -1.0091803 -1.2107282 -1.2976565 -1.1807094 -1.1480176 -1.3659685 -1.9286938 -2.5597043 -3.2375011 -3.9716003 -4.5853443 -4.894803][-0.48897529 0.42962885 0.5630765 0.40607309 0.53005838 0.65413189 1.0181456 1.0449219 0.68038559 -0.11464453 -1.3028598 -2.5073428 -3.5068121 -4.25495 -4.6814642][-0.62368631 0.35275745 0.44682026 0.56585741 1.1382504 1.8259625 2.6293135 2.7953477 2.229105 1.0992794 -0.23345089 -1.7415876 -3.0500326 -3.9539073 -4.6079173][-1.7266228 -0.95166492 -0.67010832 -0.19585276 0.58259535 1.8169227 3.1955528 3.490767 3.0028524 1.7230124 0.257205 -1.3719139 -2.8800302 -3.8312008 -4.4206834][-3.4958847 -2.964818 -2.6931219 -1.9792061 -0.76202154 0.80517387 2.2894921 2.7228026 2.3562794 1.3055425 -0.021148205 -1.6925504 -3.1225142 -4.0139685 -4.6383576][-5.0903845 -5.166163 -5.1767073 -4.4205852 -2.9565585 -1.2479229 0.29422045 0.84079266 0.62237358 -0.27393007 -1.42484 -2.8885865 -4.0620756 -4.6238513 -5.0041742][-6.4202962 -6.8502398 -6.9917612 -6.3025074 -5.1421161 -3.5711799 -2.1383557 -1.8238049 -1.9931419 -2.6419415 -3.4771354 -4.4134536 -5.0557575 -5.2499404 -5.2177005][-6.9084468 -7.2822146 -7.7828722 -7.436554 -6.5989285 -5.3624439 -4.2923775 -3.9827547 -3.9019687 -4.1411991 -4.6653266 -5.3580322 -5.6287088 -5.4123783 -5.3310647][-6.1175208 -6.4386539 -6.9399829 -6.8850956 -6.5478816 -5.8572636 -5.2730846 -5.0811176 -5.1262856 -5.2451692 -5.3376374 -5.7052917 -5.8387175 -5.4567966 -5.1290741][-5.5342379 -5.4375854 -5.5258923 -5.6695361 -5.6959753 -5.3430042 -5.0458784 -5.0549459 -5.0381742 -4.9038219 -4.8821344 -4.9912024 -4.9270444 -4.6982784 -4.5284843][-4.8207464 -4.2537413 -4.0848417 -4.093936 -4.2263551 -4.3322973 -4.3799052 -4.4425607 -4.2598519 -4.052496 -3.794677 -3.6385219 -3.3252666 -3.2220213 -3.3897529]]...]
INFO - root - 2017-12-16 04:09:32.417745: step 6610, loss = 0.35, batch loss = 0.29 (25.8 examples/sec; 0.310 sec/batch; 28h:04m:50s remains)
INFO - root - 2017-12-16 04:09:35.275134: step 6620, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.282 sec/batch; 25h:29m:52s remains)
INFO - root - 2017-12-16 04:09:38.089327: step 6630, loss = 0.28, batch loss = 0.23 (28.8 examples/sec; 0.278 sec/batch; 25h:07m:43s remains)
INFO - root - 2017-12-16 04:09:40.942469: step 6640, loss = 0.25, batch loss = 0.19 (28.9 examples/sec; 0.277 sec/batch; 25h:02m:58s remains)
INFO - root - 2017-12-16 04:09:43.755189: step 6650, loss = 0.33, batch loss = 0.28 (28.9 examples/sec; 0.277 sec/batch; 25h:03m:40s remains)
INFO - root - 2017-12-16 04:09:46.633610: step 6660, loss = 0.27, batch loss = 0.21 (26.7 examples/sec; 0.300 sec/batch; 27h:06m:36s remains)
INFO - root - 2017-12-16 04:09:49.440087: step 6670, loss = 0.28, batch loss = 0.22 (29.1 examples/sec; 0.275 sec/batch; 24h:53m:42s remains)
INFO - root - 2017-12-16 04:09:52.227723: step 6680, loss = 0.32, batch loss = 0.26 (28.3 examples/sec; 0.282 sec/batch; 25h:32m:58s remains)
INFO - root - 2017-12-16 04:09:55.088496: step 6690, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.281 sec/batch; 25h:26m:35s remains)
INFO - root - 2017-12-16 04:09:57.977390: step 6700, loss = 0.27, batch loss = 0.21 (29.4 examples/sec; 0.272 sec/batch; 24h:35m:49s remains)
2017-12-16 04:09:58.417737: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5216882 -3.1645937 -3.0626898 -3.1660018 -3.229718 -3.3371735 -3.5429626 -3.4841397 -3.3208947 -3.0751436 -2.8630486 -2.9280388 -3.0866876 -3.3912373 -3.581758][-2.4558492 -2.2176809 -2.1924865 -2.5180664 -2.907063 -3.2641921 -3.5931687 -3.6283808 -3.5277543 -3.4471924 -3.4053016 -3.3807135 -3.4035022 -3.6770277 -3.9751613][-1.3969219 -1.3124826 -1.4231777 -1.83831 -2.3304453 -2.96276 -3.5236242 -3.7232459 -3.824086 -3.897141 -4.0496945 -4.2007966 -4.3823743 -4.5673 -4.7754159][-0.79336095 -0.90088892 -1.0172951 -1.3001502 -1.6824532 -2.322017 -2.8671663 -3.2061827 -3.4858155 -3.7086089 -3.9961765 -4.3004065 -4.708921 -4.9793358 -5.198771][-0.9345696 -1.0343671 -0.98104811 -1.0838215 -1.2515955 -1.5494373 -1.6804309 -1.832366 -2.1965818 -2.6082449 -3.0719295 -3.644686 -4.2585382 -4.6385436 -4.9266357][-1.2226992 -1.1565726 -0.8319335 -0.61233211 -0.34706068 -0.31863213 -0.28772783 -0.2816515 -0.52350354 -1.0167181 -1.8015149 -2.5389645 -3.308641 -4.0044107 -4.4877915][-1.673368 -1.42432 -0.88635063 -0.37884569 0.24151325 0.74311924 1.2666368 1.5357175 1.37567 0.78322458 -0.14153337 -1.1581693 -2.1952746 -2.9431329 -3.5050907][-2.126255 -1.8260341 -1.280153 -0.57168961 0.32986879 1.0937119 1.8996038 2.4903526 2.6017704 2.0584269 1.138577 0.13310242 -0.93927169 -1.8572311 -2.5786281][-2.5679955 -2.1095741 -1.4487469 -0.779052 0.033015728 0.893116 1.6650958 2.1236835 2.1678686 1.6348338 0.82998514 -0.082166195 -0.95803833 -1.6081755 -2.0978856][-2.6971786 -2.2115018 -1.6782525 -1.1163611 -0.53623271 -0.0750618 0.27699709 0.4885087 0.40711689 -0.04295063 -0.65171885 -1.3603728 -1.8961403 -2.2392275 -2.4791398][-2.6384296 -2.0885665 -1.6453996 -1.3471422 -1.1796036 -1.0244336 -1.0078371 -1.3004429 -1.7734926 -2.258383 -2.6247764 -3.0379653 -3.33364 -3.4842565 -3.4442694][-2.6055081 -2.1737323 -1.9087029 -1.6995871 -1.753871 -2.1598608 -2.6837773 -3.2398598 -3.6859827 -4.0799718 -4.3122921 -4.5557194 -4.6135926 -4.5128589 -4.4046712][-2.5541966 -2.1119196 -1.8605065 -1.9071825 -2.382715 -3.0056958 -3.6764569 -4.3882475 -4.9646387 -5.3778892 -5.4389734 -5.3211875 -5.1865854 -5.1276402 -5.0814385][-2.4653928 -2.0417855 -1.9199421 -1.9462924 -2.3736489 -2.9647684 -3.6853423 -4.552494 -5.2010746 -5.4192448 -5.320961 -5.3367438 -5.3443007 -5.2438631 -5.1705027][-2.3098128 -1.9438341 -1.8590484 -1.9581883 -2.358851 -2.87264 -3.4200389 -4.0608644 -4.557323 -5.0047841 -5.1180329 -4.8951445 -4.6385407 -4.5661068 -4.6955719]]...]
INFO - root - 2017-12-16 04:10:01.267862: step 6710, loss = 0.38, batch loss = 0.32 (28.7 examples/sec; 0.279 sec/batch; 25h:12m:54s remains)
INFO - root - 2017-12-16 04:10:04.128172: step 6720, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 25h:46m:25s remains)
INFO - root - 2017-12-16 04:10:06.938878: step 6730, loss = 0.22, batch loss = 0.16 (28.7 examples/sec; 0.279 sec/batch; 25h:12m:19s remains)
INFO - root - 2017-12-16 04:10:09.780396: step 6740, loss = 0.21, batch loss = 0.15 (28.7 examples/sec; 0.278 sec/batch; 25h:11m:10s remains)
INFO - root - 2017-12-16 04:10:12.633275: step 6750, loss = 0.23, batch loss = 0.17 (29.3 examples/sec; 0.273 sec/batch; 24h:44m:33s remains)
INFO - root - 2017-12-16 04:10:15.462218: step 6760, loss = 0.23, batch loss = 0.17 (29.7 examples/sec; 0.270 sec/batch; 24h:24m:35s remains)
INFO - root - 2017-12-16 04:10:18.299513: step 6770, loss = 0.21, batch loss = 0.16 (27.5 examples/sec; 0.291 sec/batch; 26h:18m:15s remains)
INFO - root - 2017-12-16 04:10:21.129593: step 6780, loss = 0.32, batch loss = 0.27 (28.7 examples/sec; 0.279 sec/batch; 25h:15m:49s remains)
INFO - root - 2017-12-16 04:10:23.994926: step 6790, loss = 0.26, batch loss = 0.20 (27.1 examples/sec; 0.296 sec/batch; 26h:45m:26s remains)
INFO - root - 2017-12-16 04:10:26.803129: step 6800, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 25h:44m:53s remains)
2017-12-16 04:10:27.245033: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4602766 -4.5592685 -4.8723955 -5.2203865 -5.4820466 -5.5020461 -5.4656482 -5.4493208 -5.4028115 -5.1444755 -5.1063576 -5.1513944 -5.3875914 -5.6735754 -5.7080679][-3.3257608 -3.3422103 -3.5518703 -3.9666429 -4.4913621 -5.0139365 -5.3435907 -5.2376146 -5.1305666 -5.0786257 -5.14199 -5.2758384 -5.4995728 -5.8388834 -5.9120064][-1.8527839 -1.7889743 -2.1235051 -2.5898566 -3.1129546 -3.4537575 -3.8787296 -4.1403484 -4.30907 -4.3655596 -4.67538 -5.1439834 -5.5303917 -5.8986969 -5.8816271][-0.60920382 -0.40391016 -0.65325165 -1.080904 -1.688509 -2.141609 -2.4979331 -2.5452423 -2.6697547 -2.9845929 -3.551604 -4.2884884 -4.9491515 -5.4496717 -5.5854335][-0.034595013 0.38345098 0.31003284 0.0748992 -0.1672473 -0.30477 -0.41624403 -0.51600146 -0.67404556 -1.0913026 -1.983269 -3.1140332 -4.0454125 -4.6161785 -4.827383][-0.088824749 0.52003145 0.71884966 0.7781992 0.7950201 1.0122652 1.3230495 1.422586 1.273324 0.71158171 -0.36037779 -1.6544518 -2.9189982 -3.7178955 -4.1827369][-0.73043084 0.18537188 0.69628143 1.2006493 1.9513669 2.5257149 3.0697122 3.2849469 3.0871639 2.4281902 1.2607903 -0.10939646 -1.5125637 -2.6852252 -3.655767][-1.6396492 -0.87322521 -0.33981276 0.42335415 1.5656571 2.7746878 3.9649239 4.3606253 4.2621174 3.6133757 2.280746 0.7049737 -0.87967038 -2.1303411 -3.229341][-3.6516433 -2.9399943 -2.3520062 -1.552197 -0.20096397 1.4033761 2.8926797 3.5605555 3.8208618 3.4572644 2.303741 0.83036804 -0.83741975 -2.3483167 -3.5731881][-5.1493564 -5.0975361 -4.942771 -3.9854357 -2.4668317 -0.8960948 0.63052511 1.7616296 2.278738 2.036705 1.0579863 -0.12724209 -1.6521912 -3.1589146 -4.2517838][-6.4852114 -6.72703 -6.8405657 -6.405561 -5.1527567 -3.4506128 -1.9711673 -0.92163992 -0.22681475 -0.2281332 -1.0724244 -2.2995574 -3.4871173 -4.6888347 -5.5178218][-8.2564344 -8.7612648 -9.0656815 -8.7018747 -7.5975337 -6.1624327 -4.9469585 -3.6953893 -3.0422332 -3.1245604 -3.760206 -4.5734348 -5.3933449 -6.1279583 -6.4080205][-9.54027 -10.132853 -10.411154 -10.091868 -9.3842888 -8.29576 -7.2312784 -6.2013416 -5.8508534 -5.8525238 -6.1931467 -6.7729216 -7.1365371 -7.3550682 -7.0358062][-9.867259 -10.47191 -10.698133 -10.532047 -10.132254 -9.5976362 -9.0476418 -8.3426323 -8.0642252 -8.1377382 -8.3714542 -8.52495 -8.3191891 -8.06321 -7.3511558][-9.5791683 -10.002814 -10.196429 -10.04891 -9.7887945 -9.5040817 -9.4058352 -9.2818375 -9.449152 -9.64853 -9.7012959 -9.4483528 -8.9049339 -8.2702274 -7.1567659]]...]
INFO - root - 2017-12-16 04:10:30.023118: step 6810, loss = 0.39, batch loss = 0.33 (28.8 examples/sec; 0.278 sec/batch; 25h:06m:39s remains)
INFO - root - 2017-12-16 04:10:32.786642: step 6820, loss = 0.27, batch loss = 0.22 (27.7 examples/sec; 0.289 sec/batch; 26h:08m:23s remains)
INFO - root - 2017-12-16 04:10:35.653006: step 6830, loss = 0.28, batch loss = 0.22 (27.8 examples/sec; 0.288 sec/batch; 26h:02m:00s remains)
INFO - root - 2017-12-16 04:10:38.445166: step 6840, loss = 0.30, batch loss = 0.25 (28.1 examples/sec; 0.284 sec/batch; 25h:43m:41s remains)
INFO - root - 2017-12-16 04:10:41.332490: step 6850, loss = 0.40, batch loss = 0.34 (25.7 examples/sec; 0.312 sec/batch; 28h:12m:40s remains)
INFO - root - 2017-12-16 04:10:44.194201: step 6860, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 25h:30m:33s remains)
INFO - root - 2017-12-16 04:10:47.087857: step 6870, loss = 0.28, batch loss = 0.22 (26.0 examples/sec; 0.307 sec/batch; 27h:48m:42s remains)
INFO - root - 2017-12-16 04:10:49.890390: step 6880, loss = 0.30, batch loss = 0.25 (28.3 examples/sec; 0.283 sec/batch; 25h:33m:31s remains)
INFO - root - 2017-12-16 04:10:52.715194: step 6890, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 25h:44m:14s remains)
INFO - root - 2017-12-16 04:10:55.637683: step 6900, loss = 0.23, batch loss = 0.18 (27.4 examples/sec; 0.292 sec/batch; 26h:23m:07s remains)
2017-12-16 04:10:56.108407: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0622191 -4.8486991 -5.6425838 -6.2841682 -6.6780214 -6.7888279 -6.8664246 -7.0490723 -7.2329926 -7.1340179 -6.9318218 -6.5887704 -6.0478072 -5.502326 -5.083077][-3.3292608 -4.3284521 -5.2273259 -5.8511291 -6.3736491 -6.6839271 -6.8259873 -6.7981644 -6.8548574 -6.9628592 -6.9420366 -6.4768739 -5.85841 -5.3552556 -4.7726555][-2.663919 -3.6676004 -4.6661024 -5.4639978 -5.6437268 -5.7480583 -5.8342733 -5.9574461 -6.1215882 -6.2583928 -6.3039989 -6.2078276 -5.8668895 -5.2055721 -4.4988871][-2.2153962 -3.2266533 -4.042316 -4.2055554 -3.9596896 -3.9075158 -3.7059553 -3.6544583 -3.987654 -4.66715 -5.2546897 -5.4585733 -5.2838078 -4.9505591 -4.4113026][-2.0134633 -2.4246964 -2.5554252 -2.2092831 -1.5269158 -0.80178285 -0.30692577 -0.44428134 -1.1974394 -2.4251266 -3.6488292 -4.4266376 -4.7747645 -4.8007703 -4.31104][-2.11175 -2.0705388 -1.6051702 -0.36525869 1.2394991 2.3110805 2.7457938 2.642436 1.7426991 0.0012574196 -2.0550652 -3.5102637 -4.262053 -4.3259263 -3.9274039][-3.0423474 -2.4720964 -1.479641 0.26863909 2.4043946 4.2744923 5.47472 5.4024782 4.1216621 2.042542 -0.35428476 -2.2384958 -3.4710107 -3.9122949 -3.7271469][-4.2200851 -3.5442352 -2.2257733 0.078839779 2.6924853 4.9975357 6.4957485 6.6172037 5.4134712 3.052001 0.42063618 -1.7881517 -3.2030196 -3.773767 -3.6471477][-6.187706 -5.3560581 -3.9060562 -1.7361429 0.77480459 3.45708 5.3100252 5.8074379 4.7868366 2.5423031 0.047529697 -2.0770364 -3.4553242 -3.9868505 -3.8290508][-7.5559449 -7.3728104 -6.7613764 -4.7879105 -2.3242781 0.0937953 2.0188494 3.0890145 2.6925077 1.090126 -0.96850491 -2.8540916 -4.0564618 -4.5614676 -4.4245462][-9.0477924 -9.1155682 -8.69696 -7.4992428 -5.7429256 -3.4536886 -1.419178 -0.18723822 0.03142786 -0.97022367 -2.426923 -3.8708513 -4.74621 -5.0962467 -4.9511051][-9.249095 -9.8409538 -9.9837894 -9.1994667 -7.8766575 -6.0226226 -4.3030515 -3.1954651 -2.5469213 -2.8591533 -3.5818188 -4.63482 -5.196178 -5.2710094 -5.1163578][-8.0178719 -8.6005049 -8.8772116 -8.645648 -8.1069756 -7.0362005 -5.8262124 -4.7419639 -3.9854021 -3.9564462 -4.2771869 -4.8838162 -5.1892862 -5.1477523 -4.95443][-6.3992796 -6.7564678 -6.9418936 -6.8800144 -6.5454454 -5.9009995 -5.166791 -4.8520441 -4.6024623 -4.3804078 -4.3221226 -4.7793889 -5.1036186 -4.99488 -4.8156943][-5.0372519 -5.1499066 -5.0880113 -4.9224849 -4.7155447 -4.419466 -4.0081005 -3.7687554 -3.6973944 -3.7780874 -3.8360982 -4.0921936 -4.3357882 -4.4700456 -4.490643]]...]
INFO - root - 2017-12-16 04:10:58.861766: step 6910, loss = 0.33, batch loss = 0.27 (29.3 examples/sec; 0.273 sec/batch; 24h:39m:38s remains)
INFO - root - 2017-12-16 04:11:01.718384: step 6920, loss = 0.30, batch loss = 0.24 (29.4 examples/sec; 0.272 sec/batch; 24h:35m:04s remains)
INFO - root - 2017-12-16 04:11:04.601305: step 6930, loss = 0.34, batch loss = 0.28 (28.5 examples/sec; 0.281 sec/batch; 25h:22m:45s remains)
INFO - root - 2017-12-16 04:11:07.402796: step 6940, loss = 0.20, batch loss = 0.14 (29.6 examples/sec; 0.270 sec/batch; 24h:27m:38s remains)
INFO - root - 2017-12-16 04:11:10.231502: step 6950, loss = 0.24, batch loss = 0.18 (27.6 examples/sec; 0.290 sec/batch; 26h:13m:06s remains)
INFO - root - 2017-12-16 04:11:13.048868: step 6960, loss = 0.21, batch loss = 0.15 (29.0 examples/sec; 0.276 sec/batch; 24h:59m:14s remains)
INFO - root - 2017-12-16 04:11:15.835325: step 6970, loss = 0.40, batch loss = 0.34 (28.8 examples/sec; 0.277 sec/batch; 25h:05m:20s remains)
INFO - root - 2017-12-16 04:11:18.687635: step 6980, loss = 0.28, batch loss = 0.22 (28.8 examples/sec; 0.277 sec/batch; 25h:04m:58s remains)
INFO - root - 2017-12-16 04:11:21.475568: step 6990, loss = 0.37, batch loss = 0.32 (28.1 examples/sec; 0.285 sec/batch; 25h:47m:09s remains)
INFO - root - 2017-12-16 04:11:24.350017: step 7000, loss = 0.33, batch loss = 0.27 (26.3 examples/sec; 0.304 sec/batch; 27h:29m:21s remains)
2017-12-16 04:11:24.802493: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9090767 -2.8772752 -2.7527175 -2.6690037 -2.6536489 -2.7599545 -3.0262156 -3.2879109 -3.59685 -3.7947326 -3.8809719 -3.8866148 -3.7409351 -3.5306592 -3.2434702][-2.978807 -2.8901086 -2.7579002 -2.6335652 -2.5571213 -2.7079153 -2.936981 -3.0519347 -3.2467837 -3.3450906 -3.3870268 -3.3114436 -3.0361624 -2.77485 -2.5222631][-2.90973 -2.7672052 -2.6362529 -2.5144453 -2.4079428 -2.4065816 -2.5334232 -2.5533862 -2.545846 -2.6251643 -2.6717107 -2.5526435 -2.289382 -1.9753647 -1.7346404][-2.67948 -2.4349389 -2.2391751 -2.0952086 -1.9532669 -1.8727899 -1.8452342 -1.7248859 -1.60547 -1.4329138 -1.3238013 -1.3633177 -1.2922399 -1.1608269 -1.1315277][-2.3491306 -1.9952314 -1.6865497 -1.4280677 -1.2410612 -1.0613952 -0.87503433 -0.632782 -0.333138 -0.095700741 -0.0072197914 -0.0059728622 -0.19236231 -0.48126578 -0.813612][-2.1280916 -1.7221687 -1.3601208 -1.0177965 -0.6942091 -0.45660067 -0.18161917 0.28689194 0.73948336 0.95982313 1.0351458 0.84518623 0.34522152 -0.21497393 -0.89589858][-2.14045 -1.7039771 -1.3434491 -0.96353292 -0.54465246 -0.053934574 0.53809166 1.063499 1.527667 1.8021193 1.7538252 1.295867 0.4958272 -0.49997044 -1.5341649][-2.1755397 -1.8134525 -1.4620004 -0.99894786 -0.52061749 0.17075729 1.0637555 1.7310252 2.2574415 2.4283175 2.2557797 1.5844941 0.42981195 -0.89161134 -2.1273518][-2.0556858 -1.8239329 -1.6240861 -1.2721725 -0.75450993 0.01803875 0.94975662 1.7859058 2.4354973 2.5860167 2.2837973 1.4643621 0.18907833 -1.3709126 -2.7514334][-2.14905 -2.2005811 -2.315136 -2.1208971 -1.7547066 -1.0585706 -0.12412786 0.64125395 1.2497101 1.3718328 1.0846462 0.33630085 -0.82726407 -2.229861 -3.5073152][-2.6790652 -2.87949 -3.1240511 -3.1953607 -3.0467582 -2.4123149 -1.5580494 -0.9212122 -0.46925545 -0.40513849 -0.58943343 -1.3052981 -2.3057487 -3.3915584 -4.3139057][-3.058975 -3.3625941 -3.717617 -3.8943608 -3.9082327 -3.5019867 -2.8199987 -2.2889774 -1.9044144 -1.862216 -1.9997342 -2.5744309 -3.3603168 -4.2012596 -4.8193431][-3.1551206 -3.5735979 -3.9447892 -4.0829072 -4.1607747 -3.959476 -3.600132 -3.2599771 -3.0079708 -3.0753169 -3.2467551 -3.6989503 -4.2447948 -4.9046474 -5.366219][-3.2321663 -3.5929689 -3.8667295 -3.9515982 -3.9882751 -3.8393924 -3.6699882 -3.5288925 -3.5135038 -3.7244391 -4.0069222 -4.4497118 -4.8547788 -5.340857 -5.6656146][-3.3216672 -3.5367548 -3.6689768 -3.6550326 -3.5738447 -3.3851578 -3.2806163 -3.2358251 -3.3122797 -3.5785055 -3.9248879 -4.3472738 -4.716012 -5.1402612 -5.4162951]]...]
INFO - root - 2017-12-16 04:11:27.644009: step 7010, loss = 0.27, batch loss = 0.21 (28.6 examples/sec; 0.279 sec/batch; 25h:15m:07s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 04:11:30.513422: step 7020, loss = 0.24, batch loss = 0.18 (25.7 examples/sec; 0.312 sec/batch; 28h:11m:49s remains)
INFO - root - 2017-12-16 04:11:33.327420: step 7030, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.281 sec/batch; 25h:24m:30s remains)
INFO - root - 2017-12-16 04:11:36.191032: step 7040, loss = 0.22, batch loss = 0.17 (28.5 examples/sec; 0.281 sec/batch; 25h:24m:20s remains)
INFO - root - 2017-12-16 04:11:39.015852: step 7050, loss = 0.35, batch loss = 0.29 (28.6 examples/sec; 0.280 sec/batch; 25h:16m:15s remains)
INFO - root - 2017-12-16 04:11:41.837036: step 7060, loss = 0.21, batch loss = 0.15 (27.8 examples/sec; 0.288 sec/batch; 26h:01m:36s remains)
INFO - root - 2017-12-16 04:11:44.680085: step 7070, loss = 0.33, batch loss = 0.27 (28.4 examples/sec; 0.282 sec/batch; 25h:27m:52s remains)
INFO - root - 2017-12-16 04:11:47.518786: step 7080, loss = 0.31, batch loss = 0.26 (29.3 examples/sec; 0.273 sec/batch; 24h:39m:44s remains)
INFO - root - 2017-12-16 04:11:50.274652: step 7090, loss = 0.36, batch loss = 0.30 (28.8 examples/sec; 0.278 sec/batch; 25h:06m:49s remains)
INFO - root - 2017-12-16 04:11:53.130612: step 7100, loss = 0.27, batch loss = 0.21 (26.1 examples/sec; 0.306 sec/batch; 27h:41m:20s remains)
2017-12-16 04:11:53.598939: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5394025 -2.7057674 -2.7196832 -2.684247 -2.8176265 -3.0787058 -3.6847603 -4.1922631 -4.6541939 -4.8566875 -4.6203055 -4.2896276 -3.7098169 -3.1706958 -2.7046642][-3.2003179 -3.2909794 -3.1304321 -3.0319474 -3.2351992 -3.5065546 -3.9922261 -4.5754166 -5.0598645 -5.16053 -4.8429141 -4.32817 -3.6465964 -3.0388966 -2.5441594][-4.0210395 -4.0838594 -3.9319103 -3.6573575 -3.530117 -3.6024804 -3.971961 -4.4257445 -4.8390875 -4.97243 -4.7549267 -4.2699127 -3.6769979 -3.145535 -2.6869226][-4.863883 -4.9077134 -4.6835289 -4.2535834 -3.8238211 -3.4657357 -3.4444463 -3.6645474 -3.942349 -4.0723691 -4.0768456 -3.8722658 -3.5154858 -3.1920352 -2.8657866][-4.8875508 -4.9583817 -4.6246128 -3.92438 -3.1385756 -2.5040741 -2.2460968 -2.3662908 -2.7567694 -3.1296844 -3.4245906 -3.530185 -3.5133605 -3.2922778 -2.9877114][-4.568419 -4.4876585 -4.0009952 -3.0376246 -1.9555275 -1.0350056 -0.63288021 -0.75069785 -1.3036842 -2.1031992 -2.9407048 -3.4129543 -3.6071019 -3.5286403 -3.2132745][-4.3447404 -4.1661062 -3.4912179 -2.3867261 -1.0468626 0.0061283112 0.58153439 0.49410725 -0.15135765 -1.2175999 -2.338109 -3.1359484 -3.5567808 -3.5573192 -3.2277637][-3.9630873 -3.7293005 -3.1107006 -2.0428522 -0.58342981 0.44628954 1.1277866 1.0376358 0.38468742 -0.739445 -1.9537888 -2.8965893 -3.402307 -3.4375548 -3.1629333][-3.4229479 -3.2054272 -2.5748553 -1.7267656 -0.53327608 0.39763069 1.0113301 0.92991161 0.34754276 -0.67488456 -1.8310783 -2.7941284 -3.3716459 -3.4801061 -3.2103539][-3.1750233 -2.7566788 -2.3140862 -1.6294022 -0.75701284 -0.044442654 0.43320704 0.39512396 -0.13604975 -1.0405231 -2.0759382 -2.9385653 -3.455765 -3.588321 -3.3329835][-3.1419816 -2.770575 -2.4133749 -1.9521744 -1.3629527 -0.75869107 -0.32590294 -0.40698671 -0.90313244 -1.7191365 -2.6238146 -3.3944907 -3.8143849 -3.8574722 -3.5550027][-3.2992635 -2.9356568 -2.5562797 -2.2779632 -1.8323359 -1.393131 -1.136039 -1.2113118 -1.6351738 -2.3634748 -3.2346392 -3.910615 -4.2133937 -4.1661081 -3.8020082][-3.5593536 -2.7961183 -2.4245436 -2.1257963 -1.8622777 -1.632175 -1.5890174 -1.7651899 -2.2520041 -2.9117167 -3.6326594 -4.205595 -4.45692 -4.3203497 -3.9065268][-3.9568272 -2.9167373 -2.2670407 -1.9512122 -1.7509763 -1.7231474 -1.8475292 -2.1469789 -2.6546319 -3.2973461 -3.9614739 -4.383985 -4.4478335 -4.2390976 -3.8107767][-4.31133 -3.2987854 -2.5464356 -2.0490909 -1.8658884 -1.8381302 -1.8651099 -2.2312093 -2.8650723 -3.5572855 -4.1650524 -4.4342704 -4.4179873 -4.054 -3.5509505]]...]
INFO - root - 2017-12-16 04:11:56.394242: step 7110, loss = 0.25, batch loss = 0.19 (29.0 examples/sec; 0.276 sec/batch; 24h:54m:46s remains)
INFO - root - 2017-12-16 04:11:59.175875: step 7120, loss = 0.30, batch loss = 0.24 (28.8 examples/sec; 0.277 sec/batch; 25h:04m:05s remains)
INFO - root - 2017-12-16 04:12:02.045472: step 7130, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 25h:27m:03s remains)
INFO - root - 2017-12-16 04:12:04.858360: step 7140, loss = 0.29, batch loss = 0.23 (28.7 examples/sec; 0.278 sec/batch; 25h:09m:37s remains)
INFO - root - 2017-12-16 04:12:07.668658: step 7150, loss = 0.22, batch loss = 0.16 (28.1 examples/sec; 0.284 sec/batch; 25h:41m:09s remains)
INFO - root - 2017-12-16 04:12:10.550727: step 7160, loss = 0.22, batch loss = 0.16 (29.0 examples/sec; 0.276 sec/batch; 24h:54m:18s remains)
INFO - root - 2017-12-16 04:12:13.348151: step 7170, loss = 0.33, batch loss = 0.27 (28.0 examples/sec; 0.285 sec/batch; 25h:46m:42s remains)
INFO - root - 2017-12-16 04:12:16.230001: step 7180, loss = 0.52, batch loss = 0.47 (28.3 examples/sec; 0.282 sec/batch; 25h:31m:02s remains)
INFO - root - 2017-12-16 04:12:19.050060: step 7190, loss = 0.29, batch loss = 0.23 (29.5 examples/sec; 0.271 sec/batch; 24h:31m:25s remains)
INFO - root - 2017-12-16 04:12:21.880483: step 7200, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.286 sec/batch; 25h:49m:38s remains)
2017-12-16 04:12:22.328541: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1157055 -4.1086078 -4.1993847 -4.3114581 -4.630724 -4.7752113 -4.9669437 -5.1779628 -5.0488334 -4.6947532 -4.1893673 -3.7651038 -3.3092356 -2.9180334 -2.7348626][-4.3442254 -4.3559494 -4.4272647 -4.5611796 -4.863235 -5.0880871 -5.2938557 -5.4721251 -5.406651 -5.1101556 -4.6147003 -4.0907083 -3.5720112 -3.1136003 -2.8143079][-4.3019657 -4.3376379 -4.2577 -4.2960291 -4.5751591 -4.7098346 -4.8197346 -5.1446195 -5.3326859 -5.2165284 -4.9275107 -4.6172056 -4.1745715 -3.6556497 -3.2353277][-3.6779923 -3.6477242 -3.5356591 -3.4668212 -3.5275545 -3.5421915 -3.6339231 -3.8717313 -4.1387529 -4.2645988 -4.3847718 -4.48041 -4.350913 -4.0139046 -3.5762153][-2.8573556 -2.6507368 -2.1457441 -1.8986931 -1.7662807 -1.5448318 -1.4829218 -1.6913035 -2.1839755 -2.7768469 -3.4805269 -4.1437225 -4.4499993 -4.3467364 -3.9260778][-1.5931823 -1.3605983 -0.79896212 -0.12659359 0.41353798 0.83160925 1.0922618 0.85538578 0.10616207 -1.0838315 -2.4361644 -3.744319 -4.4764724 -4.5824132 -4.246666][-0.99954915 -0.56382084 0.062975407 0.88995028 1.6951447 2.3131886 2.7345409 2.6258712 1.9699483 0.582623 -1.1198747 -2.8482559 -4.0392823 -4.4589353 -4.2478271][-0.79576445 -0.38706398 0.091255665 0.92694426 1.9469423 2.7406263 3.09205 3.0714355 2.6739678 1.4061317 -0.399827 -2.1466749 -3.3153236 -3.91193 -4.0217509][-1.4070656 -1.0838106 -0.68014932 0.060489655 0.99239445 1.8835292 2.4987125 2.5926757 2.3266406 1.21802 -0.35100031 -2.1007013 -3.3248186 -3.8486638 -3.9848766][-2.09091 -1.9590993 -1.9407859 -1.3852351 -0.55894017 0.073125362 0.50155544 0.7051816 0.70814037 -0.041371822 -1.2169402 -2.4996076 -3.4359174 -3.9329324 -4.0686793][-2.6609263 -2.7227459 -2.848146 -2.7277942 -2.268363 -1.6341569 -1.2013216 -0.91294432 -0.69100356 -0.98568964 -1.8355105 -2.8495903 -3.5997429 -3.8929257 -3.9693086][-2.9632897 -3.1528585 -3.4866714 -3.5912828 -3.374012 -2.8613036 -2.2944338 -1.9288955 -1.6254139 -1.6812613 -2.2372596 -3.0004492 -3.6374738 -3.8681962 -3.9064918][-2.6937146 -2.7936654 -3.1836164 -3.486551 -3.3901048 -3.1798396 -2.7901123 -2.3852115 -2.1092837 -2.148994 -2.5800114 -3.1673269 -3.6719298 -3.7812686 -3.6933239][-1.7745597 -1.7529924 -2.0785403 -2.5869992 -2.7639225 -2.6539154 -2.3825355 -2.2068753 -2.0837829 -2.1665018 -2.6527264 -3.2095556 -3.6005671 -3.6565425 -3.5149298][-1.0364416 -0.81947923 -1.1792438 -1.7834618 -2.0401144 -2.0451331 -1.7193606 -1.4594057 -1.4469318 -1.6628206 -2.2298005 -2.8685117 -3.2949321 -3.3492112 -3.1923041]]...]
INFO - root - 2017-12-16 04:12:25.192021: step 7210, loss = 0.22, batch loss = 0.16 (28.5 examples/sec; 0.280 sec/batch; 25h:20m:06s remains)
INFO - root - 2017-12-16 04:12:28.029568: step 7220, loss = 0.26, batch loss = 0.20 (29.6 examples/sec; 0.270 sec/batch; 24h:24m:29s remains)
INFO - root - 2017-12-16 04:12:30.886272: step 7230, loss = 0.24, batch loss = 0.18 (27.8 examples/sec; 0.288 sec/batch; 26h:01m:45s remains)
INFO - root - 2017-12-16 04:12:33.735400: step 7240, loss = 0.28, batch loss = 0.22 (27.6 examples/sec; 0.290 sec/batch; 26h:12m:59s remains)
INFO - root - 2017-12-16 04:12:36.515048: step 7250, loss = 0.24, batch loss = 0.18 (29.4 examples/sec; 0.273 sec/batch; 24h:37m:26s remains)
INFO - root - 2017-12-16 04:12:39.367376: step 7260, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.284 sec/batch; 25h:40m:16s remains)
INFO - root - 2017-12-16 04:12:42.238832: step 7270, loss = 0.27, batch loss = 0.21 (29.4 examples/sec; 0.272 sec/batch; 24h:32m:46s remains)
INFO - root - 2017-12-16 04:12:45.056517: step 7280, loss = 0.50, batch loss = 0.44 (26.9 examples/sec; 0.297 sec/batch; 26h:51m:38s remains)
INFO - root - 2017-12-16 04:12:47.847916: step 7290, loss = 0.23, batch loss = 0.18 (28.0 examples/sec; 0.286 sec/batch; 25h:49m:36s remains)
INFO - root - 2017-12-16 04:12:50.734884: step 7300, loss = 0.21, batch loss = 0.15 (28.8 examples/sec; 0.278 sec/batch; 25h:06m:46s remains)
2017-12-16 04:12:51.195761: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.392858 -3.1742606 -3.1732502 -3.3773437 -3.551872 -3.9194064 -4.2513504 -4.4766054 -4.53653 -4.3633356 -4.0020494 -3.6289845 -3.36105 -3.112963 -2.7913978][-3.6137912 -3.4693508 -3.408658 -3.5515563 -3.8155487 -4.0863404 -4.4178376 -4.7262082 -4.9470124 -4.974699 -4.974896 -4.6992555 -4.1379676 -3.797039 -3.455353][-4.0463367 -3.9902887 -4.1223793 -4.116261 -4.0470243 -4.0305376 -4.1305313 -4.4697208 -5.0582318 -5.4566393 -5.5136123 -5.5427384 -5.3633447 -4.8331909 -4.2541327][-4.8680663 -4.8564806 -4.6850033 -4.2630506 -3.6666865 -3.2363405 -3.2425981 -3.4090343 -3.8677421 -4.8688664 -5.7717304 -6.1090035 -5.9532018 -5.5972123 -5.0195818][-5.1874733 -5.2004294 -4.8223505 -4.0205817 -2.6720872 -1.2909203 -0.68150711 -0.99152327 -2.2488673 -3.6149893 -5.041153 -6.0207357 -6.1690369 -5.9292226 -5.2854328][-5.4155941 -5.2866569 -4.5143447 -2.7491097 -0.56482482 1.2592468 2.1742215 1.9022059 0.42428493 -1.5423443 -3.6345346 -5.1731286 -5.8588505 -5.6342545 -5.1765261][-5.8469934 -5.3669124 -4.3023734 -2.2919192 0.30407238 2.8758469 4.3442039 4.2725115 2.5636654 0.0061292648 -2.4627433 -4.0188103 -4.8869557 -4.9657574 -4.2053065][-5.5630126 -5.341239 -4.5148907 -2.2891948 0.46943474 3.2419081 4.59719 4.3394003 2.8909359 0.73780155 -1.7806785 -3.6321058 -4.320766 -4.2923369 -3.6631184][-5.7671852 -5.4965172 -4.6370983 -3.0348973 -0.91824865 1.6079469 3.2922153 3.5258555 2.0264764 -0.11980867 -2.2374353 -4.0874238 -4.8813148 -4.44089 -3.3190839][-5.7749739 -6.0557079 -5.8716011 -4.5794306 -2.614943 -0.69775271 0.29257774 0.66800737 0.15996361 -1.3689873 -3.3074741 -4.6584797 -5.1694536 -5.0293093 -4.2751446][-5.9997659 -6.2595305 -6.2432561 -5.8194675 -5.033287 -3.7207184 -2.4314981 -2.170614 -2.7859192 -3.5627918 -4.5612669 -5.6620169 -5.8102045 -5.5493269 -4.8373961][-5.5371461 -6.0003405 -6.4436789 -6.3454494 -5.8384953 -5.4444051 -5.170989 -4.8422551 -4.6444278 -5.2460632 -5.6369276 -6.1893749 -6.6451874 -6.420435 -5.7041359][-4.8094769 -5.2530003 -5.6938043 -6.0495811 -6.3805947 -6.030189 -5.7088432 -5.6895494 -5.5933437 -5.4438949 -5.6970863 -5.9605694 -6.143641 -6.5444641 -6.487052][-4.2549243 -4.4525619 -4.7655554 -4.8676481 -5.0184746 -5.2907963 -5.5732145 -5.4141359 -5.2428608 -4.9621725 -4.6716347 -5.1656022 -5.8203459 -6.0912781 -6.2404747][-3.7479541 -3.7170241 -3.6318007 -3.7389264 -3.7874839 -3.8523533 -4.0728979 -4.1665626 -4.0179539 -3.8968945 -3.9425309 -4.0654321 -4.6073503 -5.5842853 -6.222003]]...]
INFO - root - 2017-12-16 04:12:53.978166: step 7310, loss = 0.20, batch loss = 0.14 (29.7 examples/sec; 0.270 sec/batch; 24h:22m:02s remains)
INFO - root - 2017-12-16 04:12:56.773404: step 7320, loss = 0.31, batch loss = 0.25 (27.8 examples/sec; 0.287 sec/batch; 25h:58m:06s remains)
INFO - root - 2017-12-16 04:12:59.557607: step 7330, loss = 0.27, batch loss = 0.22 (28.1 examples/sec; 0.285 sec/batch; 25h:44m:30s remains)
INFO - root - 2017-12-16 04:13:02.430895: step 7340, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 25h:43m:17s remains)
INFO - root - 2017-12-16 04:13:05.233568: step 7350, loss = 0.23, batch loss = 0.17 (28.8 examples/sec; 0.278 sec/batch; 25h:06m:13s remains)
INFO - root - 2017-12-16 04:13:08.127634: step 7360, loss = 0.24, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 25h:44m:39s remains)
INFO - root - 2017-12-16 04:13:10.971669: step 7370, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 25h:38m:37s remains)
INFO - root - 2017-12-16 04:13:13.832446: step 7380, loss = 0.42, batch loss = 0.36 (25.3 examples/sec; 0.316 sec/batch; 28h:32m:58s remains)
INFO - root - 2017-12-16 04:13:16.703849: step 7390, loss = 0.24, batch loss = 0.18 (29.1 examples/sec; 0.275 sec/batch; 24h:50m:32s remains)
INFO - root - 2017-12-16 04:13:19.564213: step 7400, loss = 0.30, batch loss = 0.25 (27.9 examples/sec; 0.287 sec/batch; 25h:52m:59s remains)
2017-12-16 04:13:20.026414: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8911314 -4.9507771 -4.9179425 -4.9452152 -4.8847222 -4.7791986 -4.7687612 -4.6852202 -4.6343093 -4.5368567 -4.5245161 -4.3860564 -4.0045528 -3.885772 -3.8551745][-4.66544 -4.8381338 -4.9215693 -5.022048 -5.0130515 -5.0338 -5.028697 -4.8653774 -4.7379375 -4.4913163 -4.3034081 -3.9413168 -3.5665455 -3.5538335 -3.5898991][-4.4554577 -4.5373988 -4.548161 -4.735188 -4.7139273 -4.6990366 -4.6758747 -4.4321928 -4.2613997 -3.9008546 -3.6022291 -3.2978344 -2.9956138 -3.0289435 -3.2733502][-4.062139 -4.0171447 -3.9627969 -4.071166 -4.0001407 -3.7829957 -3.5150118 -3.103359 -2.7869651 -2.4225345 -2.1652536 -1.9061372 -1.6563611 -1.846693 -2.2434847][-3.190109 -2.9356594 -2.7664247 -2.8574719 -2.5860064 -2.2238669 -1.7252262 -1.1064854 -0.65672183 -0.41007471 -0.44954991 -0.45899296 -0.5218215 -0.92526245 -1.32968][-2.678133 -2.4078615 -2.0732479 -1.8594401 -1.260422 -0.69883037 0.041921616 0.72964239 1.122036 1.2029085 0.84411764 0.43937159 0.035383224 -0.39814615 -0.65862274][-2.8616724 -2.3599992 -1.7692964 -1.1598527 -0.29048538 0.48649597 1.3387666 1.9123759 2.1521182 2.0814295 1.5875344 1.0407443 0.52220297 0.084649086 -0.073334217][-3.0325599 -2.4101815 -1.6854632 -0.92637658 -0.005718708 0.96779633 1.8787608 2.3303576 2.4898338 2.1939549 1.5597 1.0181308 0.61674929 0.23118305 0.19895601][-2.9505506 -2.4464564 -1.8894455 -1.1819279 -0.19190454 0.58790588 1.2989531 1.8082843 1.9996495 1.7943182 1.3695288 0.95863867 0.6557126 0.38971424 0.44431829][-3.0282497 -2.8311934 -2.5556517 -2.0823576 -1.3420339 -0.72406411 -0.10899639 0.32648611 0.51605177 0.51433372 0.39251137 0.31910515 0.32113552 0.22005272 0.31204557][-3.3540697 -3.3217032 -3.313798 -3.199965 -2.8296633 -2.3746946 -1.8730989 -1.4596825 -1.1218424 -0.82339716 -0.71202421 -0.55232191 -0.36019087 -0.34089518 -0.26399612][-3.5814824 -3.8352888 -4.2122445 -4.4903631 -4.4628148 -4.3276591 -4.0153656 -3.4277327 -2.9073715 -2.3846273 -1.9068868 -1.5307839 -1.302948 -1.191828 -1.1160209][-3.9564071 -4.54497 -5.1688366 -5.7200384 -5.9890022 -6.1046257 -5.9253287 -5.3441896 -4.7459283 -4.0154839 -3.346139 -2.72235 -2.3222182 -2.0725691 -1.8941972][-4.3452458 -5.0582848 -5.7525377 -6.5199614 -7.0848827 -7.3319039 -7.2595129 -6.66428 -5.9589067 -5.178545 -4.5220542 -3.8222079 -3.2377748 -2.9288244 -2.710639][-4.2882752 -4.994246 -5.73185 -6.4744539 -7.0207796 -7.445447 -7.57263 -7.1523266 -6.5791068 -5.9019556 -5.3834038 -4.8112473 -4.2874775 -4.0011358 -3.778482]]...]
INFO - root - 2017-12-16 04:13:22.852984: step 7410, loss = 0.28, batch loss = 0.22 (25.4 examples/sec; 0.315 sec/batch; 28h:29m:14s remains)
INFO - root - 2017-12-16 04:13:25.739734: step 7420, loss = 0.31, batch loss = 0.25 (28.6 examples/sec; 0.279 sec/batch; 25h:14m:04s remains)
INFO - root - 2017-12-16 04:13:28.568416: step 7430, loss = 0.22, batch loss = 0.16 (28.8 examples/sec; 0.278 sec/batch; 25h:05m:06s remains)
INFO - root - 2017-12-16 04:13:31.401731: step 7440, loss = 0.32, batch loss = 0.26 (28.4 examples/sec; 0.282 sec/batch; 25h:26m:58s remains)
INFO - root - 2017-12-16 04:13:34.184925: step 7450, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.284 sec/batch; 25h:36m:35s remains)
INFO - root - 2017-12-16 04:13:37.038339: step 7460, loss = 0.29, batch loss = 0.24 (28.4 examples/sec; 0.281 sec/batch; 25h:24m:30s remains)
INFO - root - 2017-12-16 04:13:39.893233: step 7470, loss = 0.21, batch loss = 0.15 (28.2 examples/sec; 0.283 sec/batch; 25h:35m:41s remains)
INFO - root - 2017-12-16 04:13:42.737144: step 7480, loss = 0.29, batch loss = 0.24 (28.5 examples/sec; 0.280 sec/batch; 25h:17m:58s remains)
INFO - root - 2017-12-16 04:13:45.535544: step 7490, loss = 0.40, batch loss = 0.34 (29.6 examples/sec; 0.271 sec/batch; 24h:25m:16s remains)
INFO - root - 2017-12-16 04:13:48.359382: step 7500, loss = 0.23, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 25h:32m:25s remains)
2017-12-16 04:13:48.797853: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7827392 -5.5135794 -5.3551617 -5.4041319 -5.4754186 -5.6700172 -5.6356988 -5.7240233 -5.7458124 -5.9458961 -6.3265281 -6.6624956 -6.9551811 -6.7962527 -6.5929947][-5.8392348 -5.6313725 -5.6156282 -5.6671381 -5.6469111 -5.6625552 -5.6914291 -5.902534 -6.1099792 -6.4364958 -6.740582 -6.8886795 -7.0175395 -6.67202 -6.2945218][-5.4836454 -5.2541118 -5.2240047 -5.445158 -5.2542219 -5.0398827 -4.9751129 -5.1481733 -5.5647869 -6.0986896 -6.5511408 -6.7178297 -6.6006041 -6.0516543 -5.5784431][-4.8193216 -4.4076676 -4.1226292 -4.0330105 -3.7589562 -3.5908625 -3.3214397 -3.5895371 -4.1713552 -4.8198514 -5.420217 -5.7101521 -5.6229911 -5.0413084 -4.8293586][-3.6589117 -3.0755098 -2.3680794 -1.8424332 -1.3584204 -1.0441284 -0.74223113 -1.2350085 -2.014853 -3.1376882 -4.0660152 -4.3534288 -4.4957809 -4.1050553 -4.0572848][-2.858593 -2.0263433 -0.97874141 0.13583326 1.3846955 1.989368 2.1426697 1.4711103 0.33342695 -1.2697756 -2.4954138 -3.138706 -3.50837 -3.4203582 -3.4840755][-2.1533804 -1.3164728 -0.16334963 1.2767715 2.7808728 3.902668 4.3926935 3.6534576 2.2285342 0.36170816 -1.1616571 -2.2899573 -3.0018926 -3.4302883 -3.8062143][-2.2205448 -1.2300541 -0.12786055 1.3745904 3.0177131 4.1989822 4.7976131 4.50323 3.4306483 1.5927048 -0.13571548 -1.615272 -2.8791153 -3.7678576 -4.3921981][-3.2786331 -2.2420464 -1.2888801 0.064058781 1.6726079 3.0430269 3.8009787 3.6824808 2.8066053 1.2051473 -0.36835337 -1.716018 -3.0374219 -4.1693335 -4.9556413][-4.7360029 -3.9544034 -3.1527028 -2.0927303 -0.9490037 0.24436522 1.0661674 1.3484206 0.99992085 -0.16662931 -1.4838164 -2.6457319 -3.7271247 -4.8058729 -5.6610613][-5.9978037 -5.5188417 -4.9647112 -4.2496634 -3.62983 -2.6602812 -1.7172892 -1.1828222 -0.92722821 -1.4416554 -2.3554578 -3.4236574 -4.3096104 -5.2662458 -6.1702604][-6.8565235 -6.3839645 -6.15843 -5.8591847 -5.6066313 -5.0354643 -4.424787 -3.6522536 -3.0362477 -3.0105274 -3.2383833 -3.9337294 -4.6575627 -5.5097551 -6.3071103][-6.9323416 -6.3602319 -6.3115163 -6.4973264 -6.6846352 -6.547924 -6.3188992 -5.6061349 -4.9618449 -4.4859262 -4.1862912 -4.5143876 -4.9973111 -5.7603636 -6.5007081][-6.7818909 -5.8994942 -5.6301 -5.9988728 -6.4726162 -6.7722507 -6.8160329 -6.413928 -6.01996 -5.3695774 -4.9392629 -5.0454531 -5.3303523 -5.929224 -6.4978733][-5.9004631 -5.0484986 -4.7771044 -5.2512946 -5.8437796 -6.3265114 -6.5367537 -6.3531833 -6.1095371 -5.5940475 -5.2110958 -5.1875257 -5.4004016 -5.8588786 -6.2116666]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-7500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-7500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 04:13:52.045924: step 7510, loss = 0.24, batch loss = 0.19 (29.1 examples/sec; 0.274 sec/batch; 24h:46m:44s remains)
INFO - root - 2017-12-16 04:13:54.840998: step 7520, loss = 0.36, batch loss = 0.31 (29.2 examples/sec; 0.274 sec/batch; 24h:45m:41s remains)
INFO - root - 2017-12-16 04:13:57.665091: step 7530, loss = 0.22, batch loss = 0.16 (28.9 examples/sec; 0.277 sec/batch; 24h:59m:55s remains)
INFO - root - 2017-12-16 04:14:00.501936: step 7540, loss = 0.27, batch loss = 0.21 (27.4 examples/sec; 0.292 sec/batch; 26h:21m:30s remains)
INFO - root - 2017-12-16 04:14:03.348015: step 7550, loss = 0.28, batch loss = 0.23 (29.5 examples/sec; 0.272 sec/batch; 24h:30m:35s remains)
INFO - root - 2017-12-16 04:14:06.166896: step 7560, loss = 0.28, batch loss = 0.23 (28.6 examples/sec; 0.280 sec/batch; 25h:14m:27s remains)
INFO - root - 2017-12-16 04:14:09.012983: step 7570, loss = 0.26, batch loss = 0.21 (28.4 examples/sec; 0.282 sec/batch; 25h:28m:09s remains)
INFO - root - 2017-12-16 04:14:11.838991: step 7580, loss = 0.33, batch loss = 0.27 (28.5 examples/sec; 0.281 sec/batch; 25h:21m:20s remains)
INFO - root - 2017-12-16 04:14:14.661942: step 7590, loss = 0.23, batch loss = 0.18 (29.8 examples/sec; 0.268 sec/batch; 24h:11m:42s remains)
INFO - root - 2017-12-16 04:14:17.504328: step 7600, loss = 0.33, batch loss = 0.27 (29.2 examples/sec; 0.274 sec/batch; 24h:44m:50s remains)
2017-12-16 04:14:17.944004: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.04275 -4.1015043 -4.0061855 -3.9058976 -3.9325819 -3.8885002 -3.8314898 -3.8609271 -3.9992585 -4.0298047 -4.1725011 -4.0944715 -3.8586104 -3.5342758 -3.1683879][-3.9741797 -3.952548 -3.8997931 -3.7566674 -3.6440175 -3.6143584 -3.55237 -3.5426331 -3.6642222 -3.7464483 -3.825088 -3.7186913 -3.5467494 -3.1651053 -2.7641826][-3.5517623 -3.6138167 -3.6195633 -3.4499359 -3.2413614 -3.0965586 -3.0404272 -3.0496187 -3.1273041 -3.2624598 -3.4934866 -3.4556158 -3.2517962 -2.9714749 -2.7450991][-2.9954233 -3.2686639 -3.4804749 -3.1601875 -2.66675 -2.2448552 -2.0736146 -2.0581796 -2.219403 -2.4697609 -2.6648984 -2.9521565 -3.1941202 -3.0193219 -2.7060738][-2.4246008 -2.8350134 -2.9861393 -2.6682599 -2.1693871 -1.473922 -0.9481554 -0.79648995 -0.98031521 -1.5706067 -2.3464839 -2.73012 -2.8117828 -3.0569034 -3.1873341][-1.873637 -2.2074754 -2.3377681 -1.8908243 -1.0069392 -0.08742857 0.60421324 0.84507942 0.41439819 -0.480721 -1.5835266 -2.6323118 -3.3988764 -3.566283 -3.451406][-1.7000055 -1.9458673 -2.0009053 -1.2201755 -0.040063381 1.3157744 2.3594007 2.6651573 2.2173367 0.94277239 -0.76877308 -2.2963274 -3.2562652 -3.6015034 -3.7262619][-1.8095906 -1.8648586 -2.0310555 -1.228936 0.055418015 1.6129608 2.8475409 3.2235937 2.7339487 1.2672238 -0.63792992 -2.4713302 -3.6906118 -4.2536116 -4.3928871][-2.0777435 -2.1168783 -2.1765301 -1.680299 -0.62617326 1.0173993 2.5049005 3.0533271 2.5248375 0.88515615 -1.1977496 -3.2960584 -4.7102647 -5.2285771 -5.0917749][-2.3443079 -2.4840536 -2.7912335 -2.6211848 -1.8280206 -0.66049623 0.51339054 1.332747 1.2094522 -0.16400385 -2.1602995 -4.0624018 -5.3208714 -5.78058 -5.6070843][-2.3346341 -2.5787237 -3.0822632 -3.4992342 -3.4605238 -2.7427173 -1.8098876 -1.442924 -1.6432869 -2.3927958 -3.8915334 -5.4643211 -6.4120393 -6.6189337 -6.2064142][-2.5221148 -2.8685079 -3.6534846 -4.2571964 -4.4418015 -4.4013262 -4.1867995 -3.9110091 -3.7866216 -4.355998 -5.4713511 -6.6936688 -7.349174 -7.3677454 -6.8676934][-2.5159688 -3.0436745 -3.9662912 -4.984601 -5.7336569 -6.0286818 -6.0194097 -5.8771324 -5.8005829 -6.0561905 -6.4855671 -7.0317373 -7.4085755 -7.4611 -7.0512409][-2.2820146 -2.6201777 -3.4406621 -4.3688326 -5.1296115 -5.7265148 -6.189817 -6.5096588 -6.6700549 -6.6175022 -6.7283249 -7.1044054 -7.3621945 -7.1482573 -6.7079754][-2.4779363 -2.4539633 -2.6929717 -3.1770082 -3.8836746 -4.4997859 -5.013875 -5.299943 -5.5324249 -5.7977552 -6.0894322 -6.2077646 -6.1682034 -6.1942253 -5.9952874]]...]
INFO - root - 2017-12-16 04:14:20.745650: step 7610, loss = 0.27, batch loss = 0.21 (29.0 examples/sec; 0.276 sec/batch; 24h:52m:40s remains)
INFO - root - 2017-12-16 04:14:23.601287: step 7620, loss = 0.28, batch loss = 0.22 (27.5 examples/sec; 0.291 sec/batch; 26h:17m:01s remains)
INFO - root - 2017-12-16 04:14:26.439653: step 7630, loss = 0.25, batch loss = 0.19 (27.8 examples/sec; 0.288 sec/batch; 25h:56m:55s remains)
INFO - root - 2017-12-16 04:14:29.260612: step 7640, loss = 0.21, batch loss = 0.16 (29.2 examples/sec; 0.274 sec/batch; 24h:41m:07s remains)
INFO - root - 2017-12-16 04:14:32.094036: step 7650, loss = 0.26, batch loss = 0.21 (27.9 examples/sec; 0.286 sec/batch; 25h:50m:35s remains)
INFO - root - 2017-12-16 04:14:34.893796: step 7660, loss = 0.25, batch loss = 0.20 (28.4 examples/sec; 0.282 sec/batch; 25h:27m:13s remains)
INFO - root - 2017-12-16 04:14:37.768504: step 7670, loss = 0.32, batch loss = 0.26 (28.4 examples/sec; 0.282 sec/batch; 25h:25m:55s remains)
INFO - root - 2017-12-16 04:14:40.648099: step 7680, loss = 0.25, batch loss = 0.19 (29.5 examples/sec; 0.271 sec/batch; 24h:26m:26s remains)
INFO - root - 2017-12-16 04:14:43.454097: step 7690, loss = 0.31, batch loss = 0.25 (29.2 examples/sec; 0.274 sec/batch; 24h:42m:57s remains)
INFO - root - 2017-12-16 04:14:46.353733: step 7700, loss = 0.27, batch loss = 0.21 (28.4 examples/sec; 0.281 sec/batch; 25h:23m:11s remains)
2017-12-16 04:14:46.803861: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4694052 -2.6953511 -3.0853541 -3.7102463 -4.0807791 -4.0494995 -3.9565437 -4.1563816 -4.5598927 -4.8561616 -5.0970144 -5.3267274 -5.3592458 -5.1916471 -4.8167596][-1.9111817 -2.2892368 -2.7058845 -3.0311022 -3.2293804 -3.3857524 -3.355119 -3.1587348 -3.0968947 -3.6013489 -4.1137166 -4.4508867 -4.6738014 -4.7465124 -4.5586834][-1.5834773 -1.9702289 -2.5874529 -2.9651761 -3.0367045 -2.6215997 -2.0575924 -1.9149172 -1.9817286 -2.2199001 -2.5731325 -3.1562183 -3.6542196 -4.1167336 -4.1735477][-1.8391359 -2.3677626 -3.0226679 -3.5190413 -3.3719642 -2.6067624 -1.6805327 -0.72844219 -0.24479532 -0.55530787 -1.2581232 -2.2356682 -3.2151337 -3.7425642 -3.7304337][-2.6035733 -3.3764963 -4.0539274 -4.2720933 -3.6866689 -2.3285477 -0.72078919 0.59487581 1.2856321 1.1071115 0.098470211 -1.2585447 -2.7706342 -3.6157815 -3.8013914][-3.3038034 -3.7118011 -4.0585675 -4.034873 -3.0440059 -1.2163758 0.65009546 2.1598873 3.0319939 2.7749238 1.5328164 -0.36409903 -2.0978174 -3.3138061 -3.928376][-3.8152816 -4.096046 -4.0320864 -3.4524732 -2.0189676 0.072589874 2.2872295 3.9261522 4.36936 3.740263 2.3825254 0.5567627 -1.4467943 -2.8263528 -3.1299438][-3.5345883 -3.9280305 -4.2272182 -3.4252064 -1.6818404 0.55161715 2.7105241 4.2353554 4.7669821 3.9482422 2.3422093 0.40568256 -1.4814329 -2.7146425 -3.1306319][-3.914448 -3.7320805 -3.2733722 -2.9284637 -1.6700275 0.39339447 2.0178156 3.0129628 3.3074093 2.6064353 1.2675672 -0.27306747 -1.9234915 -3.2331772 -3.8909044][-3.6117456 -3.857182 -3.9423151 -3.1864977 -2.0076165 -0.78943586 0.46459675 1.2338309 1.0269265 0.21012974 -0.39143848 -1.3537259 -2.5219245 -3.5842495 -4.2199473][-3.9553301 -3.923485 -4.0564661 -4.1203032 -3.7833965 -2.7932308 -1.9495258 -1.6837142 -1.662509 -1.9626639 -2.3959489 -2.7813272 -3.2707748 -3.892714 -4.275394][-4.4095931 -4.7949691 -4.877243 -4.8440557 -4.7441311 -4.3594275 -4.1141844 -3.8848658 -3.7493656 -3.7112136 -3.4420969 -3.6036005 -4.0313511 -4.47925 -4.706996][-4.2604737 -4.8108215 -5.1336904 -5.4070997 -5.5320377 -5.4174666 -5.355731 -5.2255783 -5.0621252 -4.8137703 -4.3355384 -3.9071341 -3.7855167 -4.3151155 -4.7196531][-3.9650409 -4.4486475 -4.8094797 -5.2622771 -5.4704957 -5.7184358 -6.0253 -5.8782296 -5.5991116 -5.1190348 -4.4376431 -4.2308145 -4.4052386 -4.5959263 -4.7979474][-3.2741716 -3.845046 -4.3547163 -4.7299805 -4.9935656 -5.2139606 -5.0766525 -4.8120279 -4.3467717 -3.659827 -3.2425182 -3.0388427 -3.2986507 -3.9649324 -4.4473686]]...]
INFO - root - 2017-12-16 04:14:49.669666: step 7710, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.286 sec/batch; 25h:48m:25s remains)
INFO - root - 2017-12-16 04:14:52.527871: step 7720, loss = 0.22, batch loss = 0.16 (28.7 examples/sec; 0.278 sec/batch; 25h:06m:25s remains)
INFO - root - 2017-12-16 04:14:55.399803: step 7730, loss = 0.37, batch loss = 0.32 (28.1 examples/sec; 0.284 sec/batch; 25h:39m:50s remains)
INFO - root - 2017-12-16 04:14:58.191676: step 7740, loss = 0.28, batch loss = 0.22 (29.7 examples/sec; 0.269 sec/batch; 24h:16m:22s remains)
INFO - root - 2017-12-16 04:15:01.061914: step 7750, loss = 0.32, batch loss = 0.26 (25.4 examples/sec; 0.315 sec/batch; 28h:22m:57s remains)
INFO - root - 2017-12-16 04:15:03.906424: step 7760, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.285 sec/batch; 25h:45m:06s remains)
INFO - root - 2017-12-16 04:15:06.756299: step 7770, loss = 0.24, batch loss = 0.19 (27.7 examples/sec; 0.288 sec/batch; 26h:01m:08s remains)
INFO - root - 2017-12-16 04:15:09.601540: step 7780, loss = 0.27, batch loss = 0.22 (27.4 examples/sec; 0.292 sec/batch; 26h:20m:10s remains)
INFO - root - 2017-12-16 04:15:12.422852: step 7790, loss = 0.28, batch loss = 0.23 (28.1 examples/sec; 0.284 sec/batch; 25h:39m:32s remains)
INFO - root - 2017-12-16 04:15:15.231203: step 7800, loss = 0.30, batch loss = 0.24 (28.2 examples/sec; 0.283 sec/batch; 25h:33m:35s remains)
2017-12-16 04:15:15.696076: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.23567 -4.9261756 -4.5973821 -4.3632503 -4.33406 -4.5133872 -4.7671995 -4.935452 -5.0925474 -5.110178 -5.0651603 -4.9807944 -4.7750015 -4.3666835 -3.6907239][-6.0831537 -5.803772 -5.4956179 -5.3843784 -5.4767046 -5.6089211 -5.7803507 -5.8210697 -5.9730172 -5.9017463 -5.938117 -5.7097554 -5.4087625 -5.052207 -4.4983621][-7.1405053 -6.9266129 -6.551475 -6.4048972 -6.2069612 -6.1138191 -6.027729 -5.905128 -5.7909622 -5.5684819 -5.4928751 -5.5176196 -5.6574945 -5.6379981 -5.1129241][-7.8763638 -7.8078079 -7.4869165 -7.0675268 -6.604743 -6.1147089 -5.4342284 -4.7907658 -4.3051381 -4.1119943 -4.2942958 -4.71059 -5.2146978 -5.3636174 -5.2059689][-7.8390007 -7.8176842 -7.5548549 -7.0240965 -5.9757094 -4.75458 -3.5136266 -2.4343362 -1.8477926 -1.7303312 -2.2854924 -3.3428912 -4.5013185 -5.1348414 -5.0204916][-6.6988993 -6.9343376 -6.5681458 -5.8421268 -4.4667654 -2.5819468 -0.77179623 0.60348415 1.2023845 1.0855298 -0.11709309 -1.6935012 -3.0332227 -4.0327458 -4.2616081][-5.35439 -5.7213449 -5.5659542 -4.7376847 -2.9722288 -0.77408123 1.4943523 3.1497555 3.8194628 3.3418789 1.758183 -0.21309566 -1.7515595 -2.64693 -2.6888931][-4.304574 -4.6127267 -4.3481264 -3.4542403 -1.6833744 0.68440437 2.8742094 4.316083 4.7377443 3.7909002 1.9909286 0.23107386 -1.0441206 -1.5918577 -1.5254006][-2.8516345 -3.2708237 -3.5440662 -2.7644348 -1.1925509 0.88785934 2.8860312 4.1017694 4.1682625 3.0194888 1.1441746 -0.41758204 -0.951107 -0.87283373 -0.64170122][-1.7200789 -2.2532864 -2.5957336 -2.4487171 -1.4276876 0.2082839 1.6252098 2.5505457 2.4432354 1.3544569 -0.21044254 -1.3759382 -1.3901863 -0.76533723 -0.14763498][-1.0221126 -1.7248144 -2.2695286 -2.5340793 -2.0625246 -0.99488831 0.067680359 0.56513929 0.20244551 -0.56556034 -1.6171629 -2.1988745 -1.8099513 -1.0185628 -0.29989195][-0.91264391 -1.4222653 -2.0880978 -2.7445965 -2.6470239 -2.20863 -1.7177873 -1.4149015 -1.642658 -2.2895842 -3.2794876 -3.4820149 -2.8658371 -1.9559579 -1.1949534][-0.35220623 -0.86522961 -1.7048712 -2.6342502 -3.0191443 -2.8559012 -2.5290308 -2.4753742 -2.9194596 -3.522882 -4.1616087 -4.4517565 -4.0155396 -3.2984157 -2.8123298][0.16815758 -0.16201448 -1.1769049 -2.4873657 -3.1986141 -3.2490594 -3.1842885 -3.1464097 -3.3834677 -3.9219694 -4.5218349 -4.655334 -4.4629517 -4.1581511 -3.6625252][0.28443718 0.0030236244 -0.93264127 -2.26212 -3.0697141 -3.4311891 -3.516367 -3.4492702 -3.6131396 -3.9953249 -4.3999829 -4.509912 -4.3192196 -4.1209545 -3.9895785]]...]
INFO - root - 2017-12-16 04:15:18.556472: step 7810, loss = 0.31, batch loss = 0.25 (25.8 examples/sec; 0.310 sec/batch; 27h:57m:26s remains)
INFO - root - 2017-12-16 04:15:21.361069: step 7820, loss = 0.30, batch loss = 0.24 (28.4 examples/sec; 0.282 sec/batch; 25h:25m:22s remains)
INFO - root - 2017-12-16 04:15:24.215897: step 7830, loss = 0.21, batch loss = 0.16 (28.7 examples/sec; 0.278 sec/batch; 25h:06m:47s remains)
INFO - root - 2017-12-16 04:15:27.019794: step 7840, loss = 0.47, batch loss = 0.41 (29.5 examples/sec; 0.271 sec/batch; 24h:28m:09s remains)
INFO - root - 2017-12-16 04:15:29.833588: step 7850, loss = 0.41, batch loss = 0.35 (29.5 examples/sec; 0.271 sec/batch; 24h:27m:05s remains)
INFO - root - 2017-12-16 04:15:32.618795: step 7860, loss = 0.21, batch loss = 0.15 (28.7 examples/sec; 0.279 sec/batch; 25h:08m:24s remains)
INFO - root - 2017-12-16 04:15:35.451833: step 7870, loss = 0.32, batch loss = 0.26 (27.9 examples/sec; 0.287 sec/batch; 25h:53m:35s remains)
INFO - root - 2017-12-16 04:15:38.238465: step 7880, loss = 0.28, batch loss = 0.22 (29.5 examples/sec; 0.271 sec/batch; 24h:27m:07s remains)
INFO - root - 2017-12-16 04:15:41.071479: step 7890, loss = 0.30, batch loss = 0.24 (29.0 examples/sec; 0.276 sec/batch; 24h:52m:46s remains)
INFO - root - 2017-12-16 04:15:43.877128: step 7900, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.281 sec/batch; 25h:22m:51s remains)
2017-12-16 04:15:44.336520: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8028629 -4.1386251 -4.499681 -4.8185306 -5.073349 -5.2627616 -5.391573 -5.5810075 -5.7208166 -5.7334495 -5.5604897 -5.3019857 -4.9398022 -4.4215794 -3.9345212][-3.5780404 -3.8805413 -4.2454472 -4.6662784 -4.9604392 -5.3149328 -5.5884795 -5.6557617 -5.6598024 -5.8055429 -5.8990111 -5.7506332 -5.3825326 -4.8159833 -4.2058816][-3.3641891 -3.6741769 -3.9784579 -4.2405982 -4.4833593 -4.7178278 -4.8415146 -5.0095134 -5.1500711 -5.3287497 -5.5484686 -5.6677575 -5.6174068 -5.1547608 -4.5038919][-2.9774823 -3.1561575 -3.2885222 -3.3069112 -3.2964449 -3.2354255 -3.1949632 -3.1950948 -3.3850002 -3.9427872 -4.5440111 -5.0208774 -5.2531123 -5.0694919 -4.6668997][-2.6417935 -2.5270019 -2.4300992 -2.0839946 -1.5942273 -1.124465 -0.73818326 -0.61864924 -0.87739038 -1.6260033 -2.6763811 -3.7235196 -4.4337754 -4.6339893 -4.47387][-2.6301117 -2.4156938 -1.8799884 -1.0258713 0.061903 1.0468884 1.8610988 2.2401466 1.8821702 0.91427326 -0.44321513 -1.9472661 -3.2380538 -3.9569037 -4.1381726][-2.8305688 -2.4723358 -1.831949 -0.64936185 1.054379 2.6675019 3.8862648 4.4220371 4.1251497 3.0173411 1.3884258 -0.4016571 -1.9644325 -3.0394902 -3.5784559][-3.3385863 -2.9277134 -2.2039433 -0.83599973 0.9737978 2.8612909 4.421257 5.1354837 4.9125633 3.8959188 2.3390093 0.40855932 -1.3559754 -2.5482707 -3.202183][-4.0619087 -3.8541832 -3.096468 -1.9194334 -0.30822086 1.5637736 3.2000518 4.1596546 4.2329884 3.4554148 2.0636544 0.40715837 -1.266259 -2.5642648 -3.3268504][-4.5966635 -4.6882181 -4.4696355 -3.743928 -2.3766654 -0.80812168 0.61325788 1.7740388 2.2603416 1.8579197 0.87749577 -0.55103636 -1.9813213 -3.076026 -3.7314773][-5.328815 -5.3454666 -5.1215143 -4.8636413 -4.236002 -3.1693664 -1.9598792 -0.9898212 -0.42390633 -0.33742142 -0.79335952 -1.8081646 -2.9208112 -3.8266478 -4.3125448][-5.708149 -5.8981438 -5.7901354 -5.5321665 -5.14645 -4.5807643 -3.8550997 -3.1083183 -2.5993748 -2.4076619 -2.5905359 -3.1390924 -3.8245142 -4.4351273 -4.7978177][-5.4481297 -5.7404594 -5.9132576 -5.9322853 -5.8323083 -5.4707623 -5.0163774 -4.4768262 -3.9899855 -3.6872506 -3.680078 -3.9979227 -4.3059273 -4.7031474 -4.9529657][-5.0855875 -5.2115307 -5.2868114 -5.3665042 -5.376997 -5.2356672 -5.1112766 -4.774312 -4.4510465 -4.1587267 -3.9243002 -3.9977305 -4.167459 -4.3994627 -4.4997072][-4.2955656 -4.2537222 -4.1945319 -4.3146996 -4.4110308 -4.3486118 -4.2991095 -4.1792212 -4.09198 -3.9209006 -3.7652545 -3.7135108 -3.6516852 -3.752852 -3.8352015]]...]
INFO - root - 2017-12-16 04:15:47.200333: step 7910, loss = 0.26, batch loss = 0.20 (28.8 examples/sec; 0.278 sec/batch; 25h:04m:58s remains)
INFO - root - 2017-12-16 04:15:50.065203: step 7920, loss = 0.25, batch loss = 0.19 (28.5 examples/sec; 0.281 sec/batch; 25h:19m:44s remains)
INFO - root - 2017-12-16 04:15:52.895466: step 7930, loss = 0.23, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 25h:31m:39s remains)
INFO - root - 2017-12-16 04:15:55.792841: step 7940, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 25h:25m:26s remains)
INFO - root - 2017-12-16 04:15:58.616341: step 7950, loss = 0.52, batch loss = 0.46 (28.1 examples/sec; 0.285 sec/batch; 25h:38m:56s remains)
INFO - root - 2017-12-16 04:16:01.423577: step 7960, loss = 0.23, batch loss = 0.17 (27.3 examples/sec; 0.293 sec/batch; 26h:26m:18s remains)
INFO - root - 2017-12-16 04:16:04.239663: step 7970, loss = 0.24, batch loss = 0.19 (27.9 examples/sec; 0.286 sec/batch; 25h:49m:30s remains)
INFO - root - 2017-12-16 04:16:07.042669: step 7980, loss = 0.33, batch loss = 0.27 (29.1 examples/sec; 0.275 sec/batch; 24h:46m:53s remains)
INFO - root - 2017-12-16 04:16:09.907983: step 7990, loss = 0.24, batch loss = 0.18 (28.3 examples/sec; 0.282 sec/batch; 25h:27m:23s remains)
INFO - root - 2017-12-16 04:16:12.755894: step 8000, loss = 0.21, batch loss = 0.15 (27.6 examples/sec; 0.290 sec/batch; 26h:09m:56s remains)
2017-12-16 04:16:13.204751: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6251965 -3.5580812 -3.5167103 -3.5467496 -3.4879036 -3.3932528 -3.2913246 -3.3195403 -3.3433542 -3.2864308 -3.505708 -4.0236073 -4.6437087 -5.0412788 -5.3960109][-3.7077713 -3.7169106 -3.6783302 -3.6424239 -3.5156178 -3.463695 -3.4068432 -3.3177319 -3.1568956 -3.1323752 -3.4302969 -3.9759483 -4.6689053 -5.219183 -5.6996059][-3.8804584 -3.950762 -3.954803 -3.7809625 -3.4456034 -3.1957774 -2.8710563 -2.8486798 -2.7755625 -2.9862432 -3.5284984 -4.1128235 -4.9756804 -5.6182342 -6.1755934][-3.9172022 -4.0586114 -4.0611796 -3.5212362 -2.6880593 -2.224154 -1.7818191 -1.6979654 -1.7057631 -2.1379557 -3.0375292 -4.1033163 -5.2884479 -6.05399 -6.4628363][-3.8622594 -3.8504403 -3.5616162 -2.8337407 -1.6908028 -0.69460368 0.10029793 0.32531309 0.069373608 -0.82852507 -2.0343058 -3.2916374 -4.5357985 -5.5570183 -6.3228536][-4.0954089 -3.7820847 -3.168201 -1.9356098 -0.18056154 1.0494404 2.00624 2.2102118 1.8985057 0.90528393 -0.54568768 -2.1606421 -3.70116 -4.8170877 -5.5376558][-4.5746622 -3.9538953 -2.7881994 -1.316721 0.71487474 2.5078988 3.8793926 4.0349483 3.4845715 2.3018141 0.46325779 -1.3503509 -3.0459988 -4.1760235 -5.008482][-4.765655 -4.0004964 -2.831037 -1.0791478 1.050281 3.0963526 4.5986662 4.865078 4.2630148 2.8622789 1.0313601 -1.0426238 -2.9035993 -4.1930923 -5.1003137][-5.2269845 -4.3496785 -3.1016264 -1.5260897 0.24790573 2.2669978 3.8490705 4.137619 3.5156503 2.3312478 0.64436579 -1.3725822 -3.055743 -4.178288 -5.0785041][-5.6797929 -5.0690126 -4.1561408 -2.7059245 -1.0704134 0.55736637 1.810288 2.1260271 1.7280302 0.76182604 -0.63278484 -2.1972048 -3.4196963 -4.3415036 -5.0297523][-6.0871911 -5.7441835 -5.0189071 -3.8476002 -2.6460767 -1.2969964 -0.22274351 0.12509251 -0.24104786 -1.0506198 -1.948962 -3.0877914 -4.0646634 -4.7089562 -5.2840366][-6.0734167 -5.8980045 -5.5817657 -4.842248 -3.8739824 -2.6965628 -1.8697786 -1.6807978 -1.6420059 -1.9283371 -2.6587589 -3.5469213 -4.2824473 -4.6839433 -5.1061192][-5.5582948 -5.6157403 -5.4387302 -4.9065909 -4.1375213 -3.2771697 -2.5944266 -2.3631651 -2.2751939 -2.5580416 -3.0417678 -3.4954796 -4.0087967 -4.4295554 -4.8268714][-5.2923789 -5.2830706 -5.0114894 -4.5760541 -4.0023284 -3.2488809 -2.5740016 -2.308409 -2.1606774 -2.417515 -2.8751721 -3.3139067 -3.6933742 -4.0986776 -4.4758515][-4.7009878 -4.7602177 -4.5975633 -4.2448912 -3.5791991 -2.7967095 -2.2064304 -1.8775711 -1.7484961 -1.897804 -2.2696753 -2.6575482 -3.0865159 -3.5052056 -3.8607402]]...]
INFO - root - 2017-12-16 04:16:16.024827: step 8010, loss = 0.27, batch loss = 0.21 (28.7 examples/sec; 0.279 sec/batch; 25h:06m:34s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 04:16:18.832462: step 8020, loss = 0.33, batch loss = 0.28 (29.2 examples/sec; 0.274 sec/batch; 24h:43m:34s remains)
INFO - root - 2017-12-16 04:16:21.641344: step 8030, loss = 0.30, batch loss = 0.24 (28.6 examples/sec; 0.280 sec/batch; 25h:12m:15s remains)
INFO - root - 2017-12-16 04:16:24.462791: step 8040, loss = 0.28, batch loss = 0.22 (29.1 examples/sec; 0.275 sec/batch; 24h:48m:03s remains)
INFO - root - 2017-12-16 04:16:27.335265: step 8050, loss = 0.26, batch loss = 0.20 (28.8 examples/sec; 0.278 sec/batch; 25h:02m:42s remains)
INFO - root - 2017-12-16 04:16:30.187229: step 8060, loss = 0.26, batch loss = 0.20 (28.5 examples/sec; 0.281 sec/batch; 25h:18m:58s remains)
INFO - root - 2017-12-16 04:16:32.993023: step 8070, loss = 0.32, batch loss = 0.26 (28.8 examples/sec; 0.278 sec/batch; 25h:00m:42s remains)
INFO - root - 2017-12-16 04:16:35.810651: step 8080, loss = 0.27, batch loss = 0.22 (28.5 examples/sec; 0.281 sec/batch; 25h:19m:58s remains)
INFO - root - 2017-12-16 04:16:38.587881: step 8090, loss = 0.27, batch loss = 0.21 (29.7 examples/sec; 0.269 sec/batch; 24h:16m:49s remains)
INFO - root - 2017-12-16 04:16:41.457478: step 8100, loss = 0.28, batch loss = 0.22 (25.3 examples/sec; 0.316 sec/batch; 28h:27m:47s remains)
2017-12-16 04:16:41.917914: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.473568 -2.7193265 -2.9636178 -3.3741827 -3.8813984 -4.30027 -4.6429443 -4.8694534 -5.0690088 -5.0002551 -4.7669487 -4.5598478 -4.2783623 -3.8346267 -3.4432569][-2.7631805 -3.1421247 -3.5562749 -3.9536896 -4.3313336 -4.69735 -5.0517192 -5.2122674 -5.3609009 -5.5428925 -5.6814218 -5.492106 -5.1214123 -4.5116596 -3.9579115][-3.5712121 -4.1808119 -4.4233565 -4.4527 -4.5783963 -4.6139288 -4.66774 -4.9695158 -5.3256946 -5.5537863 -5.7744474 -6.0364752 -5.8329763 -5.0324426 -4.1633129][-3.7718644 -4.6962514 -5.2584038 -5.10199 -4.4515748 -3.786552 -3.4276843 -3.3883395 -3.9007401 -4.8179021 -5.6521211 -6.1603003 -6.2304564 -5.5770292 -4.4121976][-4.2620554 -4.90121 -5.0957842 -4.6676702 -3.6133938 -2.1561005 -1.065568 -0.85624433 -1.5740988 -2.99755 -4.6545434 -5.8082557 -5.9895535 -5.4387722 -4.5654902][-4.4762845 -5.0670357 -5.0318794 -3.9887385 -2.0502052 0.21529818 1.9470968 2.4050503 1.3870826 -0.61774039 -2.8755426 -4.7607141 -5.5073662 -5.1530385 -4.1806612][-4.0621285 -4.5214186 -4.3463373 -3.0675335 -0.89100337 1.6041784 3.8292294 4.7427216 3.8006992 1.4141788 -1.1145091 -3.1404381 -4.2521696 -4.3422818 -3.7844262][-3.6485252 -3.8821344 -3.716352 -2.3089051 -0.006562233 2.7704668 5.0012388 5.4843845 4.4470196 2.100987 -0.47851872 -2.4654775 -3.2159019 -3.1487088 -2.7165923][-3.2947335 -3.4243493 -2.8069777 -1.6649866 0.29911804 2.8568764 4.6640739 5.1145287 3.9738359 1.7190695 -0.41865587 -2.2012525 -2.9676807 -2.7322295 -2.1643693][-3.2188723 -3.5221636 -3.4856651 -2.2646835 -0.30147076 1.4558125 2.7905359 2.951457 1.7715697 0.12154675 -1.1074929 -1.9871452 -2.2523224 -2.1451843 -1.6671414][-3.9900882 -4.2163377 -4.2304983 -3.5758913 -2.272182 -0.80529356 0.11412477 -0.075410366 -1.1514227 -2.3995681 -3.0088286 -2.7617474 -2.0494719 -1.5919197 -1.0108616][-4.4666462 -5.0666347 -5.329988 -4.7823267 -3.7173777 -2.9281349 -2.5629456 -2.8925114 -3.7587721 -4.5155182 -4.7124314 -3.9581404 -2.73248 -1.8365157 -1.2881973][-5.0299563 -5.8401914 -6.2302275 -5.9684572 -5.4963479 -4.9559779 -4.4866161 -4.5795665 -5.3150907 -5.8186421 -5.5016165 -4.535192 -3.5304766 -2.7597084 -2.0442326][-5.9000931 -6.5829649 -6.8521814 -6.8916283 -6.533596 -6.1828494 -6.0810261 -5.9268007 -5.9032764 -5.9382925 -5.7990246 -5.0438461 -4.0961566 -3.4717956 -2.9537723][-5.7613049 -6.5477548 -6.9464297 -7.029952 -6.7971435 -6.6775146 -6.4909 -6.302218 -6.3706994 -6.070127 -5.6595454 -5.2113442 -4.7768345 -4.4212179 -3.9648278]]...]
INFO - root - 2017-12-16 04:16:44.800271: step 8110, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.285 sec/batch; 25h:38m:23s remains)
INFO - root - 2017-12-16 04:16:47.648938: step 8120, loss = 0.25, batch loss = 0.19 (28.6 examples/sec; 0.279 sec/batch; 25h:11m:01s remains)
INFO - root - 2017-12-16 04:16:50.469914: step 8130, loss = 0.26, batch loss = 0.20 (27.0 examples/sec; 0.296 sec/batch; 26h:41m:57s remains)
INFO - root - 2017-12-16 04:16:53.289398: step 8140, loss = 0.32, batch loss = 0.26 (29.1 examples/sec; 0.275 sec/batch; 24h:44m:47s remains)
INFO - root - 2017-12-16 04:16:56.073789: step 8150, loss = 0.23, batch loss = 0.17 (28.8 examples/sec; 0.278 sec/batch; 25h:01m:43s remains)
INFO - root - 2017-12-16 04:16:58.910431: step 8160, loss = 0.31, batch loss = 0.25 (28.6 examples/sec; 0.280 sec/batch; 25h:13m:32s remains)
INFO - root - 2017-12-16 04:17:01.740281: step 8170, loss = 0.35, batch loss = 0.29 (27.6 examples/sec; 0.290 sec/batch; 26h:05m:12s remains)
INFO - root - 2017-12-16 04:17:04.577252: step 8180, loss = 0.24, batch loss = 0.19 (28.3 examples/sec; 0.282 sec/batch; 25h:25m:45s remains)
INFO - root - 2017-12-16 04:17:07.372663: step 8190, loss = 0.32, batch loss = 0.26 (28.6 examples/sec; 0.280 sec/batch; 25h:13m:57s remains)
INFO - root - 2017-12-16 04:17:10.218294: step 8200, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.284 sec/batch; 25h:37m:41s remains)
2017-12-16 04:17:10.667706: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.148797 -5.8897033 -6.7818246 -7.71456 -8.249342 -8.1475821 -7.7680311 -7.4191332 -6.8572769 -6.2285166 -5.5781231 -4.9329562 -4.3001719 -3.7561135 -3.3937261][-4.7261853 -5.4855809 -6.1812024 -6.9824762 -7.4307775 -7.8520279 -8.0925694 -7.8591676 -7.4537725 -6.9493484 -6.3175082 -5.6859832 -4.9927168 -4.2542048 -3.6869135][-3.3761086 -4.149158 -4.951221 -5.4417958 -5.707612 -6.0712395 -6.4116297 -6.8640718 -7.2217884 -7.1928067 -6.7945528 -6.260006 -5.5685682 -4.9160209 -4.4131246][-2.1973886 -2.4940071 -2.8483739 -3.2305303 -3.4483347 -3.6647146 -3.9152319 -4.4813175 -5.348012 -6.2975607 -6.6396122 -6.3998156 -6.0353246 -5.4192905 -4.8321753][-1.5728846 -1.4355524 -1.3400009 -1.2648556 -1.0554926 -1.010148 -1.2144666 -1.807209 -2.7830055 -4.2947526 -5.5720739 -6.2177525 -6.1967697 -5.7386103 -5.1387391][-0.859025 -0.34778357 0.037422657 0.53001881 1.2455349 1.5479498 1.4331055 0.816792 -0.454736 -2.3026469 -3.8690002 -5.1477861 -5.8136425 -5.4858437 -4.7093568][-0.69362521 0.51691961 1.3684773 1.9770031 2.8301935 3.540833 3.9351063 3.2412553 1.7185788 -0.51556253 -2.6285586 -4.0668745 -4.678535 -4.6359167 -4.1910934][-1.7777102 -0.52337694 0.67876959 1.9534612 3.2953792 4.2354631 4.6604939 4.1918535 2.8251395 0.48190022 -1.7633703 -3.3421879 -4.2491393 -4.0525041 -3.4747796][-3.5303233 -2.6167884 -1.442379 0.064315319 1.6652422 3.0733938 3.7536173 3.4218721 2.2990308 0.27064371 -1.7335172 -3.1262317 -3.8842678 -3.8865547 -3.4519856][-4.550034 -4.1292577 -3.6167092 -2.2707591 -0.69871259 0.69116592 1.4410577 1.413569 0.67824221 -0.82627368 -2.3550911 -3.3437228 -3.8875628 -3.9343863 -3.6489444][-6.0878153 -5.4503894 -4.8345008 -4.25131 -3.3731008 -2.1650631 -1.3412917 -1.1608045 -1.5168073 -2.2871933 -3.1525707 -3.8247809 -4.1755061 -4.0578494 -3.8153782][-6.7689948 -6.6128182 -6.6293969 -6.0895886 -5.2957492 -4.6087646 -4.1627626 -3.7787278 -3.6978617 -3.9436853 -4.156745 -4.3076367 -4.431066 -4.321198 -4.1409893][-6.5588427 -6.7987876 -7.0690784 -6.9443145 -6.7839351 -6.3226576 -5.8182735 -5.5308881 -5.3931689 -5.2347879 -4.9822903 -4.8002563 -4.7041368 -4.646934 -4.6348891][-5.4497452 -5.9432697 -6.5375962 -6.7472959 -6.8913431 -6.7757082 -6.5758753 -6.320416 -6.0263128 -5.8123732 -5.517168 -5.1422009 -4.8946161 -4.7860804 -4.7888589][-4.8060055 -5.0332103 -5.4610133 -5.9243116 -6.3200755 -6.5114126 -6.56338 -6.3481655 -6.0648646 -5.7814846 -5.4729652 -5.2027464 -4.9767327 -4.7336535 -4.4841218]]...]
INFO - root - 2017-12-16 04:17:13.491608: step 8210, loss = 0.28, batch loss = 0.23 (28.4 examples/sec; 0.282 sec/batch; 25h:22m:58s remains)
INFO - root - 2017-12-16 04:17:16.326592: step 8220, loss = 0.29, batch loss = 0.24 (26.4 examples/sec; 0.303 sec/batch; 27h:15m:07s remains)
INFO - root - 2017-12-16 04:17:19.200227: step 8230, loss = 0.31, batch loss = 0.25 (28.1 examples/sec; 0.285 sec/batch; 25h:39m:32s remains)
INFO - root - 2017-12-16 04:17:22.004845: step 8240, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 25h:12m:41s remains)
INFO - root - 2017-12-16 04:17:24.829705: step 8250, loss = 0.27, batch loss = 0.21 (29.5 examples/sec; 0.271 sec/batch; 24h:26m:17s remains)
INFO - root - 2017-12-16 04:17:27.654466: step 8260, loss = 0.45, batch loss = 0.40 (28.2 examples/sec; 0.283 sec/batch; 25h:31m:28s remains)
INFO - root - 2017-12-16 04:17:30.527003: step 8270, loss = 0.31, batch loss = 0.26 (28.3 examples/sec; 0.283 sec/batch; 25h:28m:25s remains)
INFO - root - 2017-12-16 04:17:33.377505: step 8280, loss = 0.30, batch loss = 0.24 (28.8 examples/sec; 0.277 sec/batch; 24h:59m:00s remains)
INFO - root - 2017-12-16 04:17:36.186080: step 8290, loss = 0.33, batch loss = 0.27 (29.2 examples/sec; 0.274 sec/batch; 24h:42m:22s remains)
INFO - root - 2017-12-16 04:17:39.039410: step 8300, loss = 0.28, batch loss = 0.22 (28.2 examples/sec; 0.284 sec/batch; 25h:35m:27s remains)
2017-12-16 04:17:39.473831: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0138545 -5.774869 -5.5377083 -5.4879389 -5.2716918 -5.1600761 -4.9884329 -4.9908633 -5.1632686 -5.3198719 -5.4339347 -5.4109416 -5.3255558 -5.0352774 -4.7411613][-6.2283688 -6.0584483 -5.8278627 -5.6248212 -5.3831835 -5.721941 -5.7719097 -6.0246944 -6.2179117 -6.5303497 -6.9378519 -6.905478 -6.7088118 -6.1816554 -5.7736044][-6.2991433 -5.805491 -5.4791121 -5.0491939 -4.6866117 -4.8703055 -5.1722617 -6.0499129 -6.5904884 -7.2518706 -7.6676593 -7.8989134 -7.9920483 -7.4513626 -6.80313][-5.6142211 -5.1473465 -4.5894761 -3.9211047 -3.3678634 -3.1573925 -3.1678019 -4.09474 -5.2395325 -6.5510597 -7.3754759 -7.8060784 -7.9026427 -7.5551581 -7.1647234][-4.4046922 -3.7505255 -2.916069 -1.9719963 -1.008625 -0.53144073 -0.45470881 -1.0985553 -2.1963303 -3.93695 -5.7792397 -6.7423649 -7.1050367 -6.8973246 -6.6506653][-3.5447383 -2.7594151 -1.538825 0.0047364235 1.7866735 2.8390441 3.2477779 2.3986363 0.8328681 -1.1595955 -3.0721245 -4.6671672 -5.8037968 -5.8819084 -5.7252636][-3.0251026 -2.415267 -1.2524686 0.58533907 2.8697891 4.9291706 6.2199211 5.5946321 3.93009 1.4547658 -0.91710258 -2.8695655 -4.2826538 -4.7772589 -4.7218208][-3.0326982 -2.7123976 -1.9566607 0.033139706 2.6692052 5.1931677 6.7859049 6.7805405 5.2712631 2.7643428 0.16136646 -1.9532695 -3.372781 -4.1508045 -4.3494372][-3.5890446 -3.6022656 -3.047153 -1.4247143 0.93972349 3.6425781 5.4726229 5.723197 4.5043259 2.1085029 -0.38783121 -2.3247602 -3.5091145 -4.0435052 -4.1677327][-4.3942318 -4.6696138 -4.7503948 -3.6515269 -1.8119617 0.44655657 2.0243464 2.6611691 2.0441642 0.28689051 -1.6412723 -3.2769339 -4.3008671 -4.7067013 -4.7880359][-5.9016027 -5.7494416 -5.65098 -5.344192 -4.6404638 -3.0448642 -1.7534938 -1.0841057 -1.2467296 -2.1619709 -3.2648215 -4.4596949 -5.0743914 -5.4371233 -5.6228724][-6.724699 -6.6237788 -6.5182667 -6.2692308 -5.9425631 -5.4499149 -5.0267391 -4.3472962 -3.8994777 -4.04965 -4.5354977 -5.4141569 -5.8249335 -6.0221705 -6.0821357][-6.941679 -6.6964126 -6.7375903 -6.5802422 -6.6670971 -6.4393797 -6.3321705 -6.106204 -5.6815844 -5.2706108 -5.2231131 -5.8071871 -6.2878428 -6.5619478 -6.5218916][-7.2442374 -6.565187 -6.343555 -6.3361969 -6.4881516 -6.46254 -6.5487227 -6.4529147 -6.3575621 -6.1259165 -5.8385553 -5.9965348 -6.2228756 -6.573802 -6.6120672][-6.9944654 -6.1773925 -5.6502724 -5.7362404 -6.0611734 -6.083087 -6.1042562 -6.1007476 -6.1031718 -6.1228666 -6.0850368 -6.147439 -6.2664537 -6.4635487 -6.4281516]]...]
INFO - root - 2017-12-16 04:17:42.318702: step 8310, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.285 sec/batch; 25h:41m:48s remains)
INFO - root - 2017-12-16 04:17:45.143093: step 8320, loss = 0.27, batch loss = 0.21 (27.9 examples/sec; 0.286 sec/batch; 25h:47m:47s remains)
INFO - root - 2017-12-16 04:17:47.954633: step 8330, loss = 0.27, batch loss = 0.22 (28.6 examples/sec; 0.280 sec/batch; 25h:10m:09s remains)
INFO - root - 2017-12-16 04:17:50.823356: step 8340, loss = 0.25, batch loss = 0.19 (28.3 examples/sec; 0.283 sec/batch; 25h:27m:51s remains)
INFO - root - 2017-12-16 04:17:53.675422: step 8350, loss = 0.22, batch loss = 0.16 (29.0 examples/sec; 0.276 sec/batch; 24h:51m:22s remains)
INFO - root - 2017-12-16 04:17:56.537729: step 8360, loss = 0.28, batch loss = 0.22 (29.4 examples/sec; 0.272 sec/batch; 24h:30m:26s remains)
INFO - root - 2017-12-16 04:17:59.337884: step 8370, loss = 0.21, batch loss = 0.16 (28.0 examples/sec; 0.286 sec/batch; 25h:44m:27s remains)
INFO - root - 2017-12-16 04:18:02.151901: step 8380, loss = 0.30, batch loss = 0.24 (29.4 examples/sec; 0.272 sec/batch; 24h:30m:35s remains)
INFO - root - 2017-12-16 04:18:05.024868: step 8390, loss = 0.38, batch loss = 0.33 (28.9 examples/sec; 0.276 sec/batch; 24h:52m:59s remains)
INFO - root - 2017-12-16 04:18:07.843187: step 8400, loss = 0.27, batch loss = 0.21 (27.8 examples/sec; 0.287 sec/batch; 25h:51m:44s remains)
2017-12-16 04:18:08.305421: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2619762 -3.2768335 -3.3837924 -3.490973 -3.5718646 -3.595593 -3.6034722 -3.5772152 -3.5246961 -3.4458458 -3.41922 -3.3636918 -3.2516856 -3.2115307 -3.2620163][-3.2169986 -3.3278918 -3.5724714 -3.7736244 -3.8666658 -3.8937376 -3.8558915 -3.7359481 -3.572042 -3.5153022 -3.5691817 -3.4586706 -3.3081539 -3.2603841 -3.3086848][-3.391113 -3.6287861 -3.9921405 -4.1924152 -4.1800089 -4.1237183 -3.9487674 -3.8078971 -3.6104939 -3.5223289 -3.625746 -3.5062742 -3.3799808 -3.361341 -3.401881][-3.522651 -3.8593082 -4.2893057 -4.3941846 -4.17524 -4.008759 -3.6431093 -3.3400459 -3.1479006 -3.169991 -3.4525926 -3.4562588 -3.4532406 -3.4401739 -3.4872696][-3.6437216 -3.9314885 -4.1719208 -4.0544314 -3.524555 -3.0536487 -2.4124489 -1.9332411 -1.6624463 -1.8042946 -2.2533622 -2.4548817 -2.7293963 -2.9995904 -3.1869149][-3.7194257 -3.914093 -3.9635789 -3.4476681 -2.5456378 -1.7120643 -0.71764374 -0.10639858 0.075377941 -0.18620539 -0.79887342 -1.3228395 -1.8603132 -2.2780454 -2.6524067][-3.4465284 -3.4807055 -3.284 -2.53324 -1.3556662 -0.20683336 1.0976796 1.8701997 2.039269 1.5188484 0.72528887 -0.11612701 -0.98557329 -1.5778539 -2.012557][-3.0772109 -3.1763918 -2.9731865 -2.1751065 -1.0173752 0.31485748 1.7667246 2.6687961 2.9321518 2.395257 1.5890012 0.63523722 -0.30118322 -1.0554473 -1.5841856][-2.9240294 -3.1053324 -2.9555411 -2.3242795 -1.3492196 0.018830776 1.5226669 2.396378 2.6071448 2.1716747 1.4628296 0.43965769 -0.55142641 -1.1893129 -1.7331929][-3.1823657 -3.5701661 -3.7589679 -3.4017119 -2.5587997 -1.3227816 -0.0063290596 0.928082 1.3036466 0.96923447 0.35434675 -0.59384489 -1.5325217 -2.1780682 -2.6430945][-3.6751173 -4.2928114 -4.6501107 -4.5420151 -4.1299329 -3.0978928 -1.9895227 -1.2043152 -0.91675639 -1.1428888 -1.6193266 -2.2757182 -2.8873353 -3.3908811 -3.7333145][-4.4965258 -5.0129809 -5.3265152 -5.3047013 -4.9509478 -4.1615009 -3.444767 -2.8501203 -2.56532 -2.6364422 -2.9857442 -3.6132662 -4.0910497 -4.3860331 -4.571631][-5.1419597 -5.416337 -5.6485982 -5.5813112 -5.2686448 -4.7355785 -4.309 -3.7641611 -3.4725823 -3.5155292 -3.6938453 -4.1458554 -4.5660996 -4.9017677 -5.0357523][-5.2800775 -5.2048779 -5.1034994 -5.0139031 -4.804594 -4.4029245 -4.097137 -3.8106115 -3.6333876 -3.5184684 -3.6179814 -4.0093875 -4.3844147 -4.6452212 -4.7671809][-5.1127295 -4.8050671 -4.5170646 -4.2772284 -3.9831929 -3.8104358 -3.7096157 -3.5420284 -3.4036434 -3.3325624 -3.3062587 -3.5784793 -3.8457115 -3.9857037 -4.0617127]]...]
INFO - root - 2017-12-16 04:18:11.123353: step 8410, loss = 0.23, batch loss = 0.18 (29.6 examples/sec; 0.270 sec/batch; 24h:20m:52s remains)
INFO - root - 2017-12-16 04:18:13.986602: step 8420, loss = 0.29, batch loss = 0.23 (28.1 examples/sec; 0.285 sec/batch; 25h:38m:32s remains)
INFO - root - 2017-12-16 04:18:16.827171: step 8430, loss = 0.38, batch loss = 0.32 (28.3 examples/sec; 0.282 sec/batch; 25h:24m:30s remains)
INFO - root - 2017-12-16 04:18:19.603734: step 8440, loss = 0.29, batch loss = 0.23 (27.8 examples/sec; 0.287 sec/batch; 25h:52m:10s remains)
INFO - root - 2017-12-16 04:18:22.412919: step 8450, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.280 sec/batch; 25h:14m:25s remains)
INFO - root - 2017-12-16 04:18:25.241604: step 8460, loss = 0.32, batch loss = 0.26 (28.3 examples/sec; 0.283 sec/batch; 25h:26m:57s remains)
INFO - root - 2017-12-16 04:18:28.121168: step 8470, loss = 0.26, batch loss = 0.21 (26.9 examples/sec; 0.298 sec/batch; 26h:48m:28s remains)
INFO - root - 2017-12-16 04:18:30.971623: step 8480, loss = 0.29, batch loss = 0.23 (28.5 examples/sec; 0.281 sec/batch; 25h:16m:00s remains)
INFO - root - 2017-12-16 04:18:33.835925: step 8490, loss = 0.35, batch loss = 0.30 (28.2 examples/sec; 0.283 sec/batch; 25h:30m:25s remains)
INFO - root - 2017-12-16 04:18:36.620399: step 8500, loss = 0.60, batch loss = 0.54 (28.4 examples/sec; 0.282 sec/batch; 25h:22m:45s remains)
2017-12-16 04:18:37.064882: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2906659 -3.2371187 -3.2471464 -3.2263279 -3.0596437 -2.9784193 -3.0294547 -3.5465975 -3.996676 -4.3133903 -4.6294341 -4.8533483 -4.86217 -4.7002573 -4.6145973][-2.8936267 -3.02093 -3.118144 -3.1021993 -2.9309645 -2.7664669 -2.7521133 -3.1573272 -3.5458434 -3.9154849 -4.3938479 -4.5765104 -4.5455832 -4.4531651 -4.4530926][-2.3001328 -2.5009642 -2.6797276 -2.7632525 -2.6261842 -2.5202029 -2.5540786 -2.6398909 -2.7637429 -2.9049435 -3.1635737 -3.4536431 -3.7217853 -3.8319414 -3.8715913][-1.5902727 -1.936718 -2.2221408 -2.3305173 -2.1484232 -1.8557756 -1.7360754 -1.6809452 -1.6878211 -1.6525123 -1.7919481 -2.2237489 -2.6580925 -3.038404 -3.3432946][-1.1220012 -1.4069445 -1.7375205 -1.8900013 -1.7539148 -1.412617 -1.1839185 -0.91074491 -0.72251368 -0.7227459 -0.88438082 -1.2624972 -1.6821578 -2.2162421 -2.67275][-0.90626359 -1.0472853 -1.1792827 -1.3588123 -1.3206658 -0.98615646 -0.67989182 -0.23950529 0.033720016 0.23341799 0.21288013 -0.19377947 -0.7968595 -1.4333763 -1.8272049][-0.79235315 -0.66083264 -0.65309143 -0.76268458 -0.75701046 -0.47138667 -0.099770546 0.38825178 0.73527813 0.95028114 0.93274069 0.57164955 -0.0011453629 -0.67508411 -1.1523459][-0.72103953 -0.41150951 -0.27050018 -0.2752161 -0.19992304 0.12424278 0.53583574 1.0916247 1.5341668 1.6727819 1.4867091 0.91872597 0.19493103 -0.55875659 -1.1021881][-0.88184 -0.47385144 -0.17348337 -0.059272766 0.17132092 0.56528807 0.97471428 1.4879861 1.8439488 1.9063163 1.5245223 0.7759614 -0.15622425 -0.90435767 -1.4358454][-1.089973 -0.43653512 -0.020329475 0.14465523 0.33303928 0.74636078 1.2064948 1.6077161 1.7406983 1.6243968 1.0929384 0.21929073 -0.84144735 -1.7333949 -2.2966647][-1.5697367 -0.84534931 -0.34794092 -0.019301891 0.22622442 0.62019825 1.0408335 1.2492962 1.0931902 0.79472733 0.097547531 -0.88677382 -1.9005442 -2.6917982 -3.2353973][-2.628756 -1.7857692 -1.1244524 -0.76098061 -0.57243967 -0.25984478 0.071746349 0.2406621 0.044333935 -0.49266648 -1.2862985 -2.1427836 -2.9796863 -3.5976589 -4.0371327][-3.5650654 -2.736371 -1.9019418 -1.4680729 -1.2281868 -0.87125683 -0.69732594 -0.82502508 -1.2069743 -1.6099911 -2.0855589 -2.76507 -3.4546328 -3.8101611 -4.0641003][-4.2028832 -3.282892 -2.3373694 -1.8792093 -1.8484132 -1.7642395 -1.7226331 -1.8849461 -2.3299062 -2.74298 -2.9695086 -3.1583104 -3.4248905 -3.5990505 -3.7904384][-4.5551152 -3.632592 -2.7088833 -2.3043871 -2.3398645 -2.3703084 -2.5064669 -2.8225369 -3.2907295 -3.5296693 -3.5099869 -3.2755432 -3.1093352 -3.2625546 -3.3950734]]...]
INFO - root - 2017-12-16 04:18:39.919821: step 8510, loss = 0.35, batch loss = 0.29 (28.4 examples/sec; 0.282 sec/batch; 25h:23m:31s remains)
INFO - root - 2017-12-16 04:18:42.703915: step 8520, loss = 0.24, batch loss = 0.18 (29.8 examples/sec; 0.269 sec/batch; 24h:11m:49s remains)
INFO - root - 2017-12-16 04:18:45.533512: step 8530, loss = 0.24, batch loss = 0.18 (25.6 examples/sec; 0.313 sec/batch; 28h:10m:24s remains)
INFO - root - 2017-12-16 04:18:48.360732: step 8540, loss = 0.24, batch loss = 0.18 (28.6 examples/sec; 0.279 sec/batch; 25h:08m:15s remains)
INFO - root - 2017-12-16 04:18:51.180304: step 8550, loss = 0.34, batch loss = 0.28 (29.1 examples/sec; 0.275 sec/batch; 24h:46m:01s remains)
INFO - root - 2017-12-16 04:18:54.055780: step 8560, loss = 0.32, batch loss = 0.26 (29.3 examples/sec; 0.273 sec/batch; 24h:31m:48s remains)
INFO - root - 2017-12-16 04:18:56.912227: step 8570, loss = 0.24, batch loss = 0.18 (27.2 examples/sec; 0.294 sec/batch; 26h:26m:31s remains)
INFO - root - 2017-12-16 04:18:59.760801: step 8580, loss = 0.31, batch loss = 0.25 (29.0 examples/sec; 0.276 sec/batch; 24h:50m:42s remains)
INFO - root - 2017-12-16 04:19:02.595875: step 8590, loss = 0.30, batch loss = 0.24 (29.5 examples/sec; 0.272 sec/batch; 24h:25m:46s remains)
INFO - root - 2017-12-16 04:19:05.420067: step 8600, loss = 0.24, batch loss = 0.18 (27.7 examples/sec; 0.289 sec/batch; 26h:00m:36s remains)
2017-12-16 04:19:05.883521: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9197774 -4.3415418 -4.0728331 -4.0413876 -4.1601958 -4.1539454 -4.2650714 -4.2643938 -3.9960978 -3.7217717 -3.4537735 -3.2726493 -3.1503985 -3.26298 -3.6217937][-4.8016953 -4.314332 -4.1724839 -4.0494804 -4.06573 -3.995851 -3.8762748 -3.6286936 -3.2681055 -2.9346118 -2.8302422 -2.6613355 -2.5507126 -2.5579021 -2.8115044][-5.103785 -4.8733659 -4.5171247 -4.2083645 -3.835027 -3.3689628 -2.9090204 -2.4970922 -2.1116061 -1.8529456 -1.9080408 -2.0917451 -2.082859 -1.9485791 -1.9701746][-5.0594788 -5.1932344 -5.0452352 -4.5044007 -3.6258082 -2.721734 -1.977134 -1.4654992 -1.0908659 -1.0681586 -1.2756755 -1.5750144 -1.5903206 -1.5041442 -1.5474498][-5.1734509 -4.9006548 -4.4649434 -3.7253079 -2.7345595 -1.4991534 -0.45387173 -0.018719673 -0.11070013 -0.37288046 -0.83767653 -1.4005139 -1.7289073 -1.7903168 -1.9369025][-5.544136 -4.934711 -3.9463134 -2.8608646 -1.4922616 -0.071824551 0.98728514 1.5130086 1.3632941 0.63128424 -0.45111632 -1.4246681 -2.1073081 -2.467494 -2.5830927][-5.4443331 -5.3396444 -4.2673864 -2.79326 -1.1023264 0.50766277 1.681664 2.0079513 1.810216 1.1521592 -0.037520885 -1.3138244 -2.2344069 -2.9223957 -3.3925631][-5.1031356 -5.0508337 -4.4227529 -3.1695323 -1.475234 0.18790913 1.5284657 2.0265808 1.8064642 1.2204638 0.11847734 -1.2742918 -2.4606235 -3.2734189 -3.9586318][-4.9061227 -4.737679 -4.0598521 -3.3264875 -2.1006076 -0.41940403 1.0789347 1.6897049 1.6762295 0.97238445 -0.29639292 -1.7467918 -3.0630279 -4.05999 -4.7713361][-4.4269147 -4.7618337 -4.4894142 -3.8469579 -2.9734821 -1.7647152 -0.33774376 0.73502636 1.0749073 0.502779 -0.664819 -2.1545007 -3.5767913 -4.72431 -5.5028749][-3.9832613 -4.8096142 -5.0563645 -4.6069212 -3.7301998 -2.645148 -1.7072284 -0.70734596 -0.18325949 -0.33651686 -1.2378938 -2.4300997 -3.7192311 -4.76456 -5.3926282][-3.9344771 -4.5966096 -5.0964594 -4.9880753 -4.2562332 -3.3419552 -2.5320187 -1.9192083 -1.5263922 -1.2770898 -1.7287183 -2.652745 -3.7521362 -4.6324015 -5.2267509][-4.0384488 -4.5480385 -4.9124231 -4.8208494 -4.3211069 -3.6583338 -3.0768087 -2.5932517 -2.2449622 -2.1861851 -2.3732657 -2.7749822 -3.4295015 -4.0267191 -4.530767][-3.6718764 -3.9930151 -4.1162271 -3.9338133 -3.6643128 -3.363785 -3.2154713 -2.9771802 -2.662416 -2.6008854 -2.6057625 -2.629014 -2.9032836 -3.2880609 -3.6150138][-3.4233246 -3.5216982 -3.5747657 -3.3289933 -3.097487 -2.8451653 -2.7661085 -2.7801116 -2.6939666 -2.5429268 -2.3669534 -2.3485818 -2.560235 -2.8261089 -3.0968189]]...]
INFO - root - 2017-12-16 04:19:08.683543: step 8610, loss = 0.26, batch loss = 0.20 (29.6 examples/sec; 0.270 sec/batch; 24h:17m:41s remains)
INFO - root - 2017-12-16 04:19:11.484634: step 8620, loss = 0.39, batch loss = 0.34 (28.8 examples/sec; 0.278 sec/batch; 25h:00m:58s remains)
INFO - root - 2017-12-16 04:19:14.318000: step 8630, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.281 sec/batch; 25h:15m:04s remains)
INFO - root - 2017-12-16 04:19:17.158186: step 8640, loss = 0.37, batch loss = 0.32 (26.2 examples/sec; 0.306 sec/batch; 27h:30m:37s remains)
INFO - root - 2017-12-16 04:19:20.046089: step 8650, loss = 0.29, batch loss = 0.23 (26.4 examples/sec; 0.303 sec/batch; 27h:14m:58s remains)
INFO - root - 2017-12-16 04:19:22.927203: step 8660, loss = 0.21, batch loss = 0.15 (26.2 examples/sec; 0.306 sec/batch; 27h:30m:52s remains)
INFO - root - 2017-12-16 04:19:25.745591: step 8670, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.281 sec/batch; 25h:14m:45s remains)
INFO - root - 2017-12-16 04:19:28.590233: step 8680, loss = 0.27, batch loss = 0.21 (27.6 examples/sec; 0.290 sec/batch; 26h:04m:48s remains)
INFO - root - 2017-12-16 04:19:31.398810: step 8690, loss = 0.32, batch loss = 0.27 (28.9 examples/sec; 0.277 sec/batch; 24h:53m:23s remains)
INFO - root - 2017-12-16 04:19:34.220184: step 8700, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.278 sec/batch; 25h:02m:29s remains)
2017-12-16 04:19:34.693750: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9150529 -2.3827229 -2.7950761 -3.1928711 -3.581954 -3.960537 -4.3106389 -4.3492012 -4.2234964 -4.0251379 -3.7126453 -3.3678024 -3.1481285 -3.2993519 -3.5669079][-2.2452092 -2.5583835 -3.0991075 -3.4419327 -3.7775197 -4.1664934 -4.445672 -4.8625436 -4.9348149 -4.7672524 -4.4002442 -3.9471195 -3.71481 -3.7181737 -4.0106297][-2.9913645 -3.3689857 -3.7623513 -3.6385834 -3.5584989 -3.7060535 -4.0585442 -4.5961657 -4.8102016 -5.0690331 -4.9836564 -4.4664836 -4.041965 -3.8911 -4.0121403][-4.1615376 -4.1304603 -3.820787 -3.2486184 -2.7872386 -2.5824666 -2.5299065 -3.0635157 -3.6609657 -4.3417397 -4.5184593 -4.0277228 -3.7411275 -3.6668313 -3.8736134][-5.2481127 -5.0538025 -4.2405715 -2.7272677 -1.1372147 -0.35226536 0.017597675 -0.37767839 -1.1400437 -2.2551224 -2.9122548 -2.8905091 -2.9070411 -2.7871182 -2.8460574][-5.6352944 -5.1565814 -3.9539752 -1.8286877 0.34836721 1.5376291 2.3036737 2.3131623 1.715692 0.33497143 -0.79045773 -0.97079325 -1.1030686 -1.2719209 -1.6030622][-5.2635074 -4.6874657 -3.3678741 -0.96525121 1.5820379 3.3306737 4.4698906 4.3367834 3.6897917 2.5441523 1.5546651 1.1193585 0.58549118 0.37357616 0.2434907][-5.0415196 -4.3531027 -2.7288902 -0.4110589 2.1953993 4.0958939 5.310277 5.5513573 5.1974926 4.1076479 3.0951805 2.6231251 2.0511875 1.6711192 1.3164287][-4.5542507 -4.17822 -3.1919284 -1.1569917 1.1829228 3.2376103 4.8170433 5.1341448 4.7947388 3.9573441 3.1691113 2.6064267 1.9842186 1.6790533 1.3736749][-4.8113341 -4.4592652 -3.5045426 -1.9679134 -0.30751371 1.1227851 2.2710223 2.8222208 2.9959283 2.5203247 1.988946 1.4183946 0.88530827 0.58654451 0.39476967][-5.7357893 -5.3040524 -4.4466047 -3.4328244 -2.1200354 -0.83622813 0.38392878 0.71165848 0.52530432 0.2495985 0.015387535 -0.44200945 -0.86850882 -1.0355589 -1.0641942][-6.8193541 -6.8112688 -6.0900726 -4.7717404 -3.7659931 -2.9056151 -2.1239593 -1.7546644 -1.528173 -1.7232955 -1.9427991 -2.3366654 -2.6503973 -2.7757244 -2.8573437][-7.4510117 -7.4721527 -6.9848709 -5.9867854 -4.8911328 -3.9528363 -3.4895272 -3.3065948 -3.2056262 -3.346319 -3.490411 -3.7443624 -3.9637754 -4.0870423 -3.9493866][-8.3836575 -8.2889872 -7.5333776 -6.3493986 -5.185431 -4.1945977 -3.6414075 -3.5863116 -3.7909384 -4.1442242 -4.3077979 -4.50837 -4.6598277 -4.6890883 -4.598042][-8.5997505 -8.6931477 -8.00168 -6.6786947 -5.3406286 -4.393239 -3.9206851 -3.7861915 -4.0137224 -4.4578686 -4.7636876 -4.9543996 -5.0257311 -5.02136 -4.9651542]]...]
INFO - root - 2017-12-16 04:19:37.542039: step 8710, loss = 0.34, batch loss = 0.28 (27.8 examples/sec; 0.287 sec/batch; 25h:50m:35s remains)
INFO - root - 2017-12-16 04:19:40.378947: step 8720, loss = 0.22, batch loss = 0.16 (29.1 examples/sec; 0.275 sec/batch; 24h:44m:36s remains)
INFO - root - 2017-12-16 04:19:43.188547: step 8730, loss = 0.32, batch loss = 0.26 (27.8 examples/sec; 0.288 sec/batch; 25h:53m:33s remains)
INFO - root - 2017-12-16 04:19:45.952779: step 8740, loss = 0.29, batch loss = 0.23 (28.5 examples/sec; 0.281 sec/batch; 25h:15m:54s remains)
INFO - root - 2017-12-16 04:19:48.813446: step 8750, loss = 0.21, batch loss = 0.15 (27.8 examples/sec; 0.288 sec/batch; 25h:55m:16s remains)
INFO - root - 2017-12-16 04:19:51.680211: step 8760, loss = 0.36, batch loss = 0.30 (26.5 examples/sec; 0.301 sec/batch; 27h:05m:48s remains)
INFO - root - 2017-12-16 04:19:54.506393: step 8770, loss = 0.30, batch loss = 0.25 (28.8 examples/sec; 0.278 sec/batch; 24h:59m:10s remains)
INFO - root - 2017-12-16 04:19:57.340658: step 8780, loss = 0.22, batch loss = 0.17 (28.2 examples/sec; 0.283 sec/batch; 25h:28m:16s remains)
INFO - root - 2017-12-16 04:20:00.148981: step 8790, loss = 0.41, batch loss = 0.35 (28.7 examples/sec; 0.279 sec/batch; 25h:04m:09s remains)
INFO - root - 2017-12-16 04:20:03.000808: step 8800, loss = 0.38, batch loss = 0.32 (29.2 examples/sec; 0.274 sec/batch; 24h:37m:44s remains)
2017-12-16 04:20:03.463457: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4988377 -1.7277625 -1.986933 -2.4171443 -2.7601283 -2.938581 -3.095773 -3.3219481 -3.6643815 -3.8065727 -3.7861688 -3.6003535 -3.2901366 -2.7710896 -2.309108][-1.2049379 -1.4931066 -1.8015199 -2.159059 -2.4434595 -2.8123655 -3.0583034 -3.2668881 -3.5274689 -3.6698043 -3.8132675 -3.7283096 -3.4694459 -2.9350705 -2.2857332][-1.4353199 -1.7759557 -2.0620096 -2.2643151 -2.3089085 -2.480864 -2.5956807 -2.8056374 -3.0957518 -3.313267 -3.6418071 -3.9333296 -3.8983889 -3.3405855 -2.5613165][-2.0415742 -2.5198593 -2.7617106 -2.7238135 -2.4616528 -2.1722434 -1.8764145 -1.8394098 -2.1346624 -2.6301079 -3.1658208 -3.7526543 -4.111805 -3.9521444 -3.1427841][-2.6600003 -3.17303 -3.2584229 -2.8591986 -2.0871878 -1.3453896 -0.67820358 -0.32904911 -0.6458087 -1.4542072 -2.4771047 -3.4537 -4.0292239 -4.1603913 -3.8827381][-3.3889461 -3.6996839 -3.4638286 -2.598815 -1.302736 -0.0075283051 1.1632986 1.7763453 1.4928017 0.37948132 -1.2294686 -2.8139067 -3.9740407 -4.4455686 -4.3561783][-3.6948235 -4.0326591 -3.4369473 -2.1549606 -0.46781754 1.3156934 2.9610987 3.7626276 3.3947077 2.0699267 0.13774776 -2.0111837 -3.7039568 -4.7003088 -4.8498049][-3.500267 -3.6841912 -3.2334218 -1.7744467 0.33865213 2.3373294 4.0389547 4.9147139 4.5799608 3.0325871 0.77917576 -1.5949028 -3.564584 -4.9223375 -5.3343768][-3.422133 -3.4222283 -2.9505606 -1.7109413 0.22866488 2.27566 3.9865742 4.8028145 4.3450155 2.767652 0.5406065 -1.8766344 -3.9039402 -5.1792212 -5.6766148][-3.3740225 -3.398818 -3.0352407 -2.1274674 -0.65328026 1.0838122 2.475729 3.025713 2.5353117 1.1282969 -0.76754355 -2.7955203 -4.4709435 -5.3983793 -5.730557][-3.5023513 -3.6839337 -3.5641894 -2.8415997 -1.8982885 -0.88341665 0.16864681 0.63106489 0.16036415 -1.0345061 -2.4702857 -3.8930368 -5.0724211 -5.6941404 -5.8295946][-3.503423 -3.8064904 -3.9206779 -3.605607 -2.9509284 -2.3059349 -1.9194469 -1.7329683 -2.0035837 -2.717649 -3.6684065 -4.6058927 -5.3172927 -5.6104269 -5.7327995][-3.8222346 -4.114954 -4.203505 -4.0953727 -3.7643921 -3.3653409 -3.1369648 -3.1387572 -3.3658555 -3.6939831 -4.041275 -4.448957 -4.8730774 -5.0987582 -5.2239237][-3.8901789 -4.0934911 -4.04115 -3.9280906 -3.6873376 -3.4164324 -3.2639489 -3.2314138 -3.3799496 -3.4783783 -3.5637643 -3.6777165 -3.8317389 -4.0420842 -4.2062135][-3.4493113 -3.5628951 -3.4091496 -3.1830103 -2.7813604 -2.4804759 -2.3008828 -2.2337615 -2.3455935 -2.5175374 -2.6053 -2.6311011 -2.6813145 -2.812156 -3.1139226]]...]
INFO - root - 2017-12-16 04:20:06.285818: step 8810, loss = 0.25, batch loss = 0.19 (28.8 examples/sec; 0.278 sec/batch; 24h:59m:27s remains)
INFO - root - 2017-12-16 04:20:09.177780: step 8820, loss = 0.31, batch loss = 0.26 (29.3 examples/sec; 0.273 sec/batch; 24h:33m:23s remains)
INFO - root - 2017-12-16 04:20:11.975820: step 8830, loss = 0.32, batch loss = 0.26 (28.9 examples/sec; 0.277 sec/batch; 24h:52m:51s remains)
INFO - root - 2017-12-16 04:20:14.868280: step 8840, loss = 0.29, batch loss = 0.23 (25.5 examples/sec; 0.314 sec/batch; 28h:14m:31s remains)
INFO - root - 2017-12-16 04:20:17.685530: step 8850, loss = 0.21, batch loss = 0.15 (28.6 examples/sec; 0.280 sec/batch; 25h:08m:23s remains)
INFO - root - 2017-12-16 04:20:20.511386: step 8860, loss = 0.24, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 25h:24m:26s remains)
INFO - root - 2017-12-16 04:20:23.364697: step 8870, loss = 0.22, batch loss = 0.16 (27.9 examples/sec; 0.286 sec/batch; 25h:44m:16s remains)
INFO - root - 2017-12-16 04:20:26.166643: step 8880, loss = 0.29, batch loss = 0.23 (29.0 examples/sec; 0.276 sec/batch; 24h:47m:21s remains)
INFO - root - 2017-12-16 04:20:28.995395: step 8890, loss = 0.26, batch loss = 0.20 (29.1 examples/sec; 0.275 sec/batch; 24h:43m:29s remains)
INFO - root - 2017-12-16 04:20:31.857041: step 8900, loss = 0.24, batch loss = 0.19 (27.6 examples/sec; 0.290 sec/batch; 26h:01m:39s remains)
2017-12-16 04:20:32.314346: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7679224 -3.0699847 -3.3883729 -3.4957266 -3.5688403 -3.5931544 -3.5934348 -3.7708962 -3.999953 -3.8466992 -3.3838511 -3.2515781 -3.3170934 -3.56179 -3.6121411][-3.0221846 -3.1285734 -3.3286469 -3.4572668 -3.3801646 -3.4424055 -3.5386281 -3.5628896 -3.7376671 -4.0107861 -4.0076566 -3.9187617 -3.9229305 -4.2587376 -4.4606485][-3.2152882 -3.421432 -3.6385858 -3.7351155 -3.6276875 -3.4518356 -3.2788582 -3.2695596 -3.3673477 -3.426656 -3.464648 -3.8104944 -4.1399803 -4.5954051 -4.82323][-3.3814769 -3.3459287 -3.4355216 -3.3479133 -3.0709515 -2.6661844 -2.2495515 -1.9501946 -1.9973109 -2.3542879 -2.6056466 -3.0420089 -3.5913091 -4.3677087 -4.6570635][-3.2808576 -3.3353095 -3.2855313 -2.9257505 -2.3189209 -1.6675184 -1.1497772 -0.59878874 -0.36602879 -0.6515224 -1.1035919 -1.8280773 -2.5141053 -3.3665047 -3.9204981][-3.543648 -3.5303254 -3.2934189 -2.6232092 -1.6478169 -0.61545324 0.16747856 0.81701469 1.0664463 0.87985849 0.4183979 -0.39018869 -1.221036 -2.3352854 -3.1846569][-3.5907154 -3.5252252 -3.1914315 -2.3028629 -0.98215961 0.16694832 1.2307792 2.0658979 2.515522 2.3701191 1.8952823 1.1492782 0.21493435 -1.0486364 -2.1969328][-3.1755431 -3.1522162 -2.9786267 -2.1273298 -0.86784387 0.2836628 1.3786845 2.3182793 2.9853525 3.084311 2.8348932 2.1541858 1.2814775 -0.0061221123 -1.3893282][-2.9826932 -2.8954918 -2.777617 -2.1142685 -1.1005259 -0.05089283 0.847877 1.7100334 2.4820008 2.790102 2.8543239 2.3905687 1.5512676 0.2614131 -1.2051477][-2.8550043 -2.809999 -2.7838693 -2.3995116 -1.6600087 -0.81850791 -0.012722969 0.63774776 1.2842121 1.6062684 1.7747965 1.5027575 0.782166 -0.45720458 -1.7740569][-2.9326358 -2.9844985 -3.0509057 -2.8551364 -2.3715882 -1.7688489 -1.1340115 -0.58216119 -0.085267544 0.24108171 0.4911871 0.3020153 -0.23300219 -1.2237165 -2.3315606][-3.1215048 -3.2080891 -3.2878685 -3.3232315 -3.1073349 -2.6905847 -2.2132397 -1.9658163 -1.691473 -1.4469435 -1.1773291 -0.9721067 -1.2982268 -1.9145424 -2.6773913][-3.1289907 -3.1392455 -3.0382965 -2.8355305 -2.6449184 -2.5095463 -2.3210688 -2.3562126 -2.3788233 -2.3280568 -2.2411931 -1.9587064 -1.9763455 -2.2695522 -2.6939216][-3.1365108 -3.0219381 -2.6280284 -2.2405164 -1.8536992 -1.6708326 -1.716203 -2.1153908 -2.522388 -2.6504216 -2.670393 -2.4156511 -2.3646972 -2.3503957 -2.5643215][-3.1175542 -2.5715318 -1.8522925 -1.3150852 -0.8926909 -0.79667163 -1.0459123 -1.7864954 -2.525435 -2.8062522 -2.8560872 -2.5469913 -2.4551554 -2.3068995 -2.39593]]...]
INFO - root - 2017-12-16 04:20:35.145182: step 8910, loss = 0.23, batch loss = 0.17 (25.5 examples/sec; 0.313 sec/batch; 28h:10m:27s remains)
INFO - root - 2017-12-16 04:20:37.961504: step 8920, loss = 0.44, batch loss = 0.38 (27.8 examples/sec; 0.288 sec/batch; 25h:53m:32s remains)
INFO - root - 2017-12-16 04:20:40.793135: step 8930, loss = 0.30, batch loss = 0.24 (28.7 examples/sec; 0.279 sec/batch; 25h:05m:30s remains)
INFO - root - 2017-12-16 04:20:43.581350: step 8940, loss = 0.32, batch loss = 0.26 (27.8 examples/sec; 0.287 sec/batch; 25h:49m:24s remains)
INFO - root - 2017-12-16 04:20:46.432839: step 8950, loss = 0.23, batch loss = 0.17 (29.7 examples/sec; 0.270 sec/batch; 24h:14m:12s remains)
INFO - root - 2017-12-16 04:20:49.261275: step 8960, loss = 0.30, batch loss = 0.24 (27.5 examples/sec; 0.291 sec/batch; 26h:09m:47s remains)
INFO - root - 2017-12-16 04:20:52.112268: step 8970, loss = 0.27, batch loss = 0.21 (27.5 examples/sec; 0.290 sec/batch; 26h:06m:02s remains)
INFO - root - 2017-12-16 04:20:55.035068: step 8980, loss = 0.26, batch loss = 0.20 (25.1 examples/sec; 0.318 sec/batch; 28h:35m:12s remains)
INFO - root - 2017-12-16 04:20:57.885127: step 8990, loss = 0.25, batch loss = 0.20 (28.5 examples/sec; 0.280 sec/batch; 25h:11m:20s remains)
INFO - root - 2017-12-16 04:21:00.662277: step 9000, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 25h:03m:09s remains)
2017-12-16 04:21:01.110053: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.53293777 -0.73472 -1.0461845 -1.5147433 -1.9096854 -1.9430666 -2.1408498 -2.1244323 -1.9113438 -1.4075103 -1.0592291 -1.0564032 -1.1037486 -1.2586207 -1.265455][-1.5425122 -1.7507522 -2.0144415 -2.4129515 -2.6520057 -2.4933026 -2.302726 -2.2919221 -2.478847 -2.1305246 -1.8678274 -1.8846459 -2.0099816 -2.1991284 -2.1499932][-3.1304476 -3.4045484 -3.6537957 -3.929343 -3.8770332 -3.5117576 -3.1736383 -2.8789241 -2.7385063 -2.6575112 -2.9766536 -3.1339135 -3.1103129 -3.2753067 -3.2467866][-4.9382234 -5.21731 -5.3241386 -5.2081647 -4.7548208 -3.9729218 -3.1163044 -2.5681396 -2.5848966 -2.7584581 -3.3235726 -3.8616605 -4.1471615 -4.3094144 -4.1011162][-5.9446344 -6.4129176 -6.4870472 -6.0186315 -5.0825815 -3.7311451 -2.3408914 -1.5256379 -1.5477974 -1.9636862 -2.8104551 -3.6760738 -4.3778944 -4.7148252 -4.4217553][-6.2256155 -6.618752 -6.65327 -6.0069256 -4.6554217 -2.8249722 -1.0089104 0.19064426 0.29324532 -0.33393383 -1.5177534 -2.7914321 -3.842453 -4.5958309 -4.8723145][-5.1575651 -5.7114305 -5.7358928 -4.9919949 -3.6237435 -1.6284289 0.41961908 1.6764598 1.7956023 0.855834 -0.59378982 -1.9131894 -3.2326369 -4.429328 -4.7731996][-3.7215588 -4.0552421 -4.0260329 -3.29391 -1.947485 -0.13805676 1.6883006 2.9418821 3.092855 1.9902763 0.2851963 -1.3719878 -2.7548609 -3.8637772 -4.4485621][-2.4346359 -2.5550072 -2.3814845 -1.7841129 -0.71720028 0.80711126 2.3188829 3.2594104 3.239296 2.200819 0.61185646 -1.0624721 -2.6702628 -3.881238 -4.2820234][-1.8296125 -1.9172354 -1.774209 -1.1955192 -0.34002829 0.61036825 1.6818924 2.4224634 2.414784 1.5649939 0.1665554 -1.559715 -3.3010736 -4.6134267 -5.2154264][-2.2574792 -2.2083721 -1.9840662 -1.5675764 -0.91524768 -0.20139551 0.61722851 1.0768995 0.93509865 0.15247679 -0.94992638 -2.3249547 -3.8527286 -5.2569222 -5.930562][-2.9444003 -2.9705539 -2.8558531 -2.3453004 -1.580097 -0.99826837 -0.47897387 -0.12712622 -0.17503881 -1.0042713 -2.094532 -3.1225266 -4.3036194 -5.5611033 -6.2602539][-3.8527379 -3.8637607 -3.6371415 -3.3258832 -2.5798967 -1.8489637 -1.1211867 -0.74228644 -0.87034583 -1.2495141 -2.028842 -3.0949907 -4.1600389 -5.2721977 -6.2560449][-4.4447131 -4.7447815 -4.6161771 -4.1936846 -3.3997216 -2.6245756 -1.8728478 -1.2983832 -1.1288781 -1.3586631 -1.9583356 -2.8260918 -3.7682884 -4.8196144 -5.9900255][-5.050756 -5.4027181 -5.5861921 -5.2704372 -4.4119768 -3.5413423 -2.5526226 -1.838635 -1.6856327 -1.7043643 -2.026551 -2.8231091 -3.857543 -5.0257778 -5.9959049]]...]
INFO - root - 2017-12-16 04:21:03.972463: step 9010, loss = 0.30, batch loss = 0.24 (28.4 examples/sec; 0.282 sec/batch; 25h:20m:01s remains)
INFO - root - 2017-12-16 04:21:06.819698: step 9020, loss = 0.24, batch loss = 0.18 (28.5 examples/sec; 0.281 sec/batch; 25h:15m:10s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 04:21:09.644475: step 9030, loss = 0.27, batch loss = 0.22 (29.5 examples/sec; 0.271 sec/batch; 24h:22m:16s remains)
INFO - root - 2017-12-16 04:21:12.515196: step 9040, loss = 0.25, batch loss = 0.19 (27.6 examples/sec; 0.290 sec/batch; 26h:02m:54s remains)
INFO - root - 2017-12-16 04:21:15.375026: step 9050, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 25h:36m:40s remains)
INFO - root - 2017-12-16 04:21:18.227270: step 9060, loss = 0.29, batch loss = 0.24 (28.7 examples/sec; 0.279 sec/batch; 25h:03m:16s remains)
INFO - root - 2017-12-16 04:21:21.089897: step 9070, loss = 0.25, batch loss = 0.19 (29.4 examples/sec; 0.273 sec/batch; 24h:29m:10s remains)
INFO - root - 2017-12-16 04:21:23.937990: step 9080, loss = 0.34, batch loss = 0.28 (27.0 examples/sec; 0.296 sec/batch; 26h:37m:41s remains)
INFO - root - 2017-12-16 04:21:26.812693: step 9090, loss = 0.36, batch loss = 0.30 (29.1 examples/sec; 0.275 sec/batch; 24h:40m:20s remains)
INFO - root - 2017-12-16 04:21:29.634429: step 9100, loss = 0.21, batch loss = 0.15 (28.9 examples/sec; 0.277 sec/batch; 24h:51m:51s remains)
2017-12-16 04:21:30.065834: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8741593 -3.6854503 -3.5681195 -3.7144456 -4.0461287 -4.0545745 -4.033864 -3.7747772 -3.4384289 -2.9687839 -2.5582147 -2.1693985 -2.2844687 -2.4265428 -2.5729821][-3.1703868 -3.0082173 -2.9051263 -3.1206617 -3.392746 -3.6075284 -3.3887131 -3.064198 -2.804492 -2.4261394 -2.0950441 -1.8371599 -1.836344 -1.8511281 -1.8955877][-2.2323139 -2.3653159 -2.3795233 -2.609019 -2.7574878 -3.0746019 -3.1792982 -2.7395134 -2.0783732 -1.7639987 -1.6543708 -1.5597675 -1.4629912 -1.3680143 -1.3791292][-0.79640841 -1.0220196 -1.2034607 -1.5218556 -1.9591782 -2.1818194 -2.1118999 -1.8675232 -1.613385 -1.2885804 -1.0832996 -1.029743 -1.2247164 -1.2842107 -1.313031][-0.17229891 -0.18070412 -0.17754364 -0.47704291 -0.81748104 -0.85679317 -0.663152 -0.51724339 -0.54777956 -0.72500014 -0.98175859 -1.1310203 -1.3942943 -1.5015829 -1.6430635][-0.64927649 -0.52684021 -0.35601854 -0.400517 -0.33988237 -0.25619459 -0.045322418 0.14298391 0.12938356 -0.1889267 -0.72743177 -1.2507062 -1.6864834 -1.8672421 -1.9553967][-1.5445538 -1.1158934 -0.948725 -0.7341969 -0.24095583 0.22348833 0.6535964 0.74440861 0.54085875 0.033616543 -0.6611712 -1.3982024 -2.0667572 -2.4631133 -2.729897][-2.5522742 -2.2090483 -1.9489524 -1.3953674 -0.75589061 0.065823078 0.83858871 1.1353498 0.95644283 0.28545713 -0.6195817 -1.5469437 -2.3511894 -2.8830636 -3.2176297][-4.0202508 -3.7809236 -3.4435563 -2.8903103 -2.2288456 -1.1252196 -0.25717878 0.17603111 0.34754753 -0.19886255 -1.0123794 -1.9474945 -2.7647703 -3.1621027 -3.4750774][-5.1850843 -5.3896928 -5.2920289 -4.8378882 -3.9977274 -2.8796597 -2.0074606 -1.4489489 -1.2784564 -1.6183977 -2.2213755 -3.0675383 -3.7673647 -3.9820278 -4.0666871][-5.7665415 -6.1300993 -6.1939974 -5.880939 -5.1924639 -4.1880765 -3.3240836 -2.8765368 -2.8360996 -3.133795 -3.630491 -4.2743845 -4.8646784 -4.996428 -4.8833666][-5.7380285 -6.21667 -6.6656113 -6.4854941 -5.9220772 -5.2400236 -4.5240569 -4.0756884 -3.8548641 -3.9638057 -4.4073896 -4.9046311 -5.1295366 -5.1280789 -5.0715837][-5.3617997 -5.9377522 -6.34252 -6.434516 -6.2792883 -5.7493691 -5.1709347 -4.8455443 -4.5470109 -4.5820351 -4.6785531 -4.7947721 -4.9326296 -4.8225231 -4.6354012][-4.8208094 -5.2599669 -5.4072456 -5.4491353 -5.3541441 -4.9900842 -4.7067561 -4.59917 -4.490447 -4.4773684 -4.4695621 -4.5132604 -4.4619164 -4.255651 -4.0640616][-4.1278834 -4.2700257 -4.1816092 -3.9748707 -3.6557517 -3.4735007 -3.4358373 -3.4524043 -3.5012939 -3.69087 -3.7298632 -3.4881904 -3.1736999 -3.1031618 -3.1187782]]...]
INFO - root - 2017-12-16 04:21:32.844083: step 9110, loss = 0.26, batch loss = 0.20 (28.2 examples/sec; 0.284 sec/batch; 25h:31m:21s remains)
INFO - root - 2017-12-16 04:21:35.693694: step 9120, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.285 sec/batch; 25h:38m:00s remains)
INFO - root - 2017-12-16 04:21:38.532334: step 9130, loss = 0.28, batch loss = 0.22 (27.9 examples/sec; 0.286 sec/batch; 25h:43m:07s remains)
INFO - root - 2017-12-16 04:21:41.325590: step 9140, loss = 0.25, batch loss = 0.19 (27.5 examples/sec; 0.290 sec/batch; 26h:05m:17s remains)
INFO - root - 2017-12-16 04:21:44.093655: step 9150, loss = 0.26, batch loss = 0.21 (28.6 examples/sec; 0.279 sec/batch; 25h:05m:12s remains)
INFO - root - 2017-12-16 04:21:46.882439: step 9160, loss = 0.28, batch loss = 0.22 (28.3 examples/sec; 0.282 sec/batch; 25h:21m:06s remains)
INFO - root - 2017-12-16 04:21:49.680847: step 9170, loss = 0.44, batch loss = 0.38 (29.4 examples/sec; 0.272 sec/batch; 24h:24m:02s remains)
INFO - root - 2017-12-16 04:21:52.542239: step 9180, loss = 0.41, batch loss = 0.35 (25.5 examples/sec; 0.313 sec/batch; 28h:08m:57s remains)
INFO - root - 2017-12-16 04:21:55.378696: step 9190, loss = 0.28, batch loss = 0.23 (28.4 examples/sec; 0.281 sec/batch; 25h:15m:46s remains)
INFO - root - 2017-12-16 04:21:58.172073: step 9200, loss = 0.23, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 25h:19m:21s remains)
2017-12-16 04:21:58.644982: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2888637 -4.6917534 -5.0772858 -5.1876793 -5.0903964 -4.9706211 -4.8360643 -4.9965482 -5.3210745 -5.4523349 -5.5343118 -5.528204 -5.1549053 -4.7270865 -4.3628359][-3.9428139 -4.1517215 -4.4272137 -4.5719614 -4.64818 -4.5745621 -4.7304583 -5.2468514 -5.7287407 -6.2320304 -6.5239553 -6.4859238 -6.1523452 -5.768888 -5.2726803][-3.8754413 -3.9297736 -4.0879745 -4.1737342 -4.0776415 -3.9914725 -4.1057711 -4.6146 -5.31809 -6.0515728 -6.4391723 -6.7261152 -6.7213845 -6.4121528 -5.9609284][-3.6884189 -3.6080244 -3.549283 -3.3922026 -3.062788 -2.6836898 -2.4768875 -2.9546828 -3.7306902 -4.7094564 -5.5111046 -6.272809 -6.5957575 -6.6352253 -6.202033][-2.9958396 -2.6636915 -2.3689246 -2.0478938 -1.5015659 -1.0720012 -0.70373511 -0.92528248 -1.5483396 -2.4935622 -3.6263034 -4.7219105 -5.4101338 -5.7301412 -5.3966722][-2.7586296 -2.3028483 -1.8521602 -1.4124186 -0.53830647 0.32517385 1.1331406 1.2891712 1.0452466 0.16405249 -1.2779787 -2.8265395 -4.0716448 -4.7497206 -4.6922965][-2.7096472 -2.0772362 -1.4874032 -0.73116088 0.36699677 1.6149063 2.9307404 3.4800186 3.5031972 2.6611915 1.2268729 -0.60733223 -2.2995002 -3.3300707 -3.778193][-2.6564932 -2.0894246 -1.427321 -0.5529778 0.66276407 2.0814552 3.4343672 4.1096354 4.3923283 3.8117666 2.5796218 0.69796085 -1.2264132 -2.5428109 -3.1177812][-2.8941312 -2.3147144 -1.5357182 -0.94509363 -0.063055515 1.3119612 2.5699167 3.1423874 3.2016816 2.8677726 2.0468011 0.33372164 -1.4003778 -2.635747 -3.1688159][-3.3557186 -2.8126404 -2.1786175 -1.7126598 -1.1330686 -0.309515 0.57799578 1.0446081 1.1964188 0.94549704 0.12626219 -1.2671473 -2.5838761 -3.4160712 -3.6530061][-3.5968392 -3.1251194 -2.6978211 -2.3961525 -2.1962245 -1.685385 -1.0426729 -0.93141031 -1.1405442 -1.5686939 -1.9989429 -2.7194161 -3.3736434 -3.785727 -3.9243186][-3.6058166 -3.2108335 -2.8482358 -2.7345638 -3.0492847 -2.9001732 -2.5218444 -2.6373005 -2.7557788 -3.1196938 -3.5171452 -3.9836457 -4.2981544 -4.3307419 -4.2149343][-3.799613 -3.653491 -3.4862223 -3.3449421 -3.5233757 -3.5907254 -3.5718608 -3.5188255 -3.481782 -3.7510073 -3.9824431 -4.1331954 -4.2409329 -4.4293833 -4.3874192][-4.2958412 -4.2268348 -3.8848381 -3.450614 -3.2866521 -3.2041736 -3.2791083 -3.4502506 -3.5194392 -3.7592282 -3.8474681 -3.8081274 -3.7856855 -3.6455688 -3.5406888][-4.514029 -4.4699326 -4.084868 -3.2370179 -2.5860574 -2.4296041 -2.5048892 -2.7039857 -2.9044173 -3.222167 -3.3838243 -3.2573195 -2.9774904 -2.750845 -2.7860351]]...]
INFO - root - 2017-12-16 04:22:01.405047: step 9210, loss = 0.19, batch loss = 0.13 (29.1 examples/sec; 0.275 sec/batch; 24h:41m:21s remains)
INFO - root - 2017-12-16 04:22:04.277060: step 9220, loss = 0.35, batch loss = 0.29 (29.5 examples/sec; 0.271 sec/batch; 24h:19m:14s remains)
INFO - root - 2017-12-16 04:22:07.088048: step 9230, loss = 0.21, batch loss = 0.15 (28.4 examples/sec; 0.282 sec/batch; 25h:19m:06s remains)
INFO - root - 2017-12-16 04:22:09.922979: step 9240, loss = 0.24, batch loss = 0.18 (28.5 examples/sec; 0.281 sec/batch; 25h:12m:46s remains)
INFO - root - 2017-12-16 04:22:12.719027: step 9250, loss = 0.27, batch loss = 0.21 (28.1 examples/sec; 0.285 sec/batch; 25h:33m:49s remains)
INFO - root - 2017-12-16 04:22:15.518674: step 9260, loss = 0.27, batch loss = 0.21 (29.3 examples/sec; 0.273 sec/batch; 24h:31m:17s remains)
INFO - root - 2017-12-16 04:22:18.371818: step 9270, loss = 0.24, batch loss = 0.18 (27.6 examples/sec; 0.290 sec/batch; 25h:59m:38s remains)
INFO - root - 2017-12-16 04:22:21.207401: step 9280, loss = 0.28, batch loss = 0.23 (28.9 examples/sec; 0.277 sec/batch; 24h:53m:27s remains)
INFO - root - 2017-12-16 04:22:24.053028: step 9290, loss = 0.22, batch loss = 0.16 (27.4 examples/sec; 0.292 sec/batch; 26h:11m:01s remains)
INFO - root - 2017-12-16 04:22:26.942323: step 9300, loss = 0.30, batch loss = 0.24 (28.3 examples/sec; 0.283 sec/batch; 25h:23m:55s remains)
2017-12-16 04:22:27.384142: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2644839 -4.5144029 -4.6770329 -4.528266 -4.2723842 -3.9284267 -3.6701407 -3.5338171 -3.6746893 -3.786844 -3.7233064 -3.4992785 -3.6451902 -3.9090686 -4.0091457][-3.5983324 -4.0497613 -4.2410216 -4.2571239 -4.2660279 -4.4283576 -4.4159985 -4.3170595 -4.4452281 -4.4818234 -4.3936105 -4.1217165 -4.0575385 -4.1216383 -4.1930423][-2.5161021 -3.1645808 -3.8534646 -4.2362833 -4.4037204 -4.5527053 -4.569427 -4.65442 -4.9131746 -4.882195 -4.6687069 -4.2971439 -4.1596608 -4.2239404 -4.0472569][-1.7063537 -2.5190501 -3.1354322 -3.6585712 -3.9118586 -4.1731033 -3.9661491 -3.9045477 -4.0295353 -4.1421351 -4.237534 -4.0469213 -4.0359769 -4.0209131 -4.0636721][-0.983104 -1.9568651 -2.6224427 -3.0166955 -2.9961534 -2.9585729 -2.6715736 -2.3827398 -2.3743277 -2.7532949 -3.2304265 -3.4989247 -3.6323178 -3.7916672 -3.9790392][-1.280515 -1.7467809 -2.1934988 -2.4899411 -2.0270486 -1.6131103 -0.88429856 -0.27230835 -0.22409534 -0.70248055 -1.4880476 -2.2349911 -2.8897114 -3.2822332 -3.33473][-1.7724774 -2.3840795 -2.3986897 -1.7378464 -0.39731073 0.70073557 1.867672 2.6905823 2.7554007 1.8289499 0.36885309 -0.82244658 -1.8978324 -2.39864 -2.5797396][-2.440557 -2.7198629 -2.5212655 -1.5107396 0.29646158 2.143786 3.7549105 4.4101763 4.1945295 3.0886707 1.4473162 -0.13376617 -1.5853944 -2.3290997 -2.6141958][-3.4935889 -3.519659 -3.0615673 -1.9879863 -0.29117584 1.548027 3.4296865 4.1932783 3.9613094 2.7172332 1.0414643 -0.59615588 -1.8679299 -2.5321221 -2.9132271][-4.19891 -4.6566625 -4.5889921 -3.4567249 -1.6690583 -0.014522076 1.6554942 2.5553484 2.5611997 1.5688934 0.12810755 -1.3587546 -2.455152 -2.8645191 -2.9226141][-4.9765515 -5.5017304 -5.4153357 -4.6720805 -3.3482666 -1.5611293 -0.07921505 0.34336233 0.082689285 -0.7226963 -1.6472194 -2.6849775 -3.4150157 -3.4732203 -3.2194886][-5.3405819 -5.8335943 -6.0382376 -5.5495186 -4.6308055 -3.5966609 -2.5325747 -2.2049372 -2.3366551 -2.8490288 -3.3676274 -3.8764198 -4.1606927 -3.9532537 -3.4440503][-5.0793462 -5.5524759 -5.89253 -5.649456 -5.028142 -4.3598413 -3.6704447 -3.4860232 -3.5900519 -3.7887473 -3.9161236 -4.0261903 -3.9297891 -3.5527167 -2.9828906][-4.8576488 -5.0502748 -4.9993277 -4.6099677 -4.1940045 -3.6355333 -3.2737556 -3.4019694 -3.6174588 -3.8674011 -3.8624845 -3.8076901 -3.5507214 -3.0803883 -2.4963918][-4.157187 -4.1023183 -3.9332938 -3.4556739 -3.0268126 -2.7700672 -2.7715797 -2.8679867 -3.0657964 -3.378423 -3.4122717 -3.1577208 -2.7341752 -2.4273703 -2.0353308]]...]
INFO - root - 2017-12-16 04:22:30.166307: step 9310, loss = 0.26, batch loss = 0.21 (28.1 examples/sec; 0.285 sec/batch; 25h:32m:57s remains)
INFO - root - 2017-12-16 04:22:33.015051: step 9320, loss = 0.22, batch loss = 0.16 (26.6 examples/sec; 0.301 sec/batch; 27h:00m:49s remains)
INFO - root - 2017-12-16 04:22:35.843391: step 9330, loss = 0.27, batch loss = 0.21 (28.3 examples/sec; 0.282 sec/batch; 25h:20m:28s remains)
INFO - root - 2017-12-16 04:22:38.703734: step 9340, loss = 0.26, batch loss = 0.20 (28.4 examples/sec; 0.281 sec/batch; 25h:15m:37s remains)
INFO - root - 2017-12-16 04:22:41.562592: step 9350, loss = 0.41, batch loss = 0.35 (28.2 examples/sec; 0.284 sec/batch; 25h:29m:40s remains)
INFO - root - 2017-12-16 04:22:44.439287: step 9360, loss = 0.36, batch loss = 0.30 (28.3 examples/sec; 0.283 sec/batch; 25h:22m:34s remains)
INFO - root - 2017-12-16 04:22:47.252285: step 9370, loss = 0.27, batch loss = 0.21 (28.7 examples/sec; 0.279 sec/batch; 25h:02m:01s remains)
INFO - root - 2017-12-16 04:22:50.072394: step 9380, loss = 0.21, batch loss = 0.15 (28.6 examples/sec; 0.280 sec/batch; 25h:08m:41s remains)
INFO - root - 2017-12-16 04:22:52.946284: step 9390, loss = 0.36, batch loss = 0.30 (28.4 examples/sec; 0.282 sec/batch; 25h:18m:03s remains)
INFO - root - 2017-12-16 04:22:55.781479: step 9400, loss = 0.28, batch loss = 0.22 (28.2 examples/sec; 0.283 sec/batch; 25h:25m:21s remains)
2017-12-16 04:22:56.254897: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.025579 -5.1231842 -5.0172358 -5.0803604 -5.1324487 -5.3224444 -5.8025331 -6.7686033 -7.505569 -7.4718437 -6.9713922 -5.9700336 -4.8740568 -3.7838509 -3.3148794][-5.5898681 -5.2715139 -4.6772361 -4.3689032 -4.2832723 -4.4571342 -4.9587951 -5.9183707 -7.1610069 -7.7578945 -7.6428757 -6.9304986 -6.0795007 -4.9485545 -4.2145834][-6.3843727 -5.9568114 -5.005456 -4.0427117 -3.3704844 -3.0919285 -3.3085055 -4.2712326 -5.6397324 -6.8812037 -7.6365881 -7.5534153 -6.9573278 -5.9598837 -5.0760069][-6.5029168 -5.9390631 -4.7851653 -3.552031 -2.348397 -1.5577908 -1.2617786 -1.863246 -3.1294515 -4.7486033 -6.1750031 -7.1007814 -7.2647161 -6.4202785 -5.5301285][-5.5283232 -4.7392588 -3.4096339 -2.0067835 -0.49184775 0.71267748 1.492857 1.2751513 0.075997353 -1.6958401 -3.856812 -5.4613342 -6.25016 -6.0789351 -5.2468419][-4.3931174 -3.3130989 -1.7427337 -0.031964302 1.6724706 3.1266294 4.1809998 4.4251776 3.5822659 1.5668564 -0.96727204 -3.2353919 -4.7866616 -5.34785 -4.8661284][-3.8618813 -2.706095 -1.111804 0.87063074 2.9187703 4.7618208 6.2318697 6.8946352 6.541172 4.5227346 1.554831 -1.4134395 -3.5605996 -4.6294045 -4.5465746][-3.9911561 -2.9487007 -1.4895329 0.49442244 2.6017385 4.6867285 6.4281216 7.3273478 7.3252935 5.5761871 2.6958356 -0.551301 -3.1294942 -4.3568625 -4.3705053][-4.6402273 -3.7745776 -2.5334969 -1.0502825 0.70518064 2.7530427 4.6776676 5.9080877 6.0212173 4.531002 1.990056 -0.92388153 -3.2537978 -4.5453625 -4.6131129][-6.0107703 -5.3821974 -4.4642525 -3.335381 -1.7796428 -0.12789631 1.3699894 2.5022693 2.7975488 1.6834493 -0.23544264 -2.4655325 -4.084084 -4.9226408 -4.9975023][-7.5152216 -7.181838 -6.5757256 -5.710546 -4.7895784 -3.451746 -2.0595803 -1.4694338 -1.44555 -1.9827161 -2.9945097 -4.3451176 -5.1879029 -5.4391327 -5.3468747][-8.4204588 -8.48901 -8.32696 -7.7357912 -7.1479673 -6.3568859 -5.5261326 -4.9663506 -4.8360519 -5.13823 -5.4189439 -5.771646 -5.9752092 -5.887187 -5.6598735][-8.0708008 -8.4267817 -8.5665216 -8.7444 -8.8977528 -8.5365973 -8.1702213 -7.9179454 -7.6025057 -7.2345657 -6.8083711 -6.40284 -5.9937496 -5.7553406 -5.5581651][-6.7461543 -7.1588163 -7.5100718 -7.9669852 -8.5182095 -8.83231 -9.0400009 -8.9780445 -8.6230927 -7.98667 -7.1233625 -6.38125 -5.7503805 -5.1962538 -4.9083395][-5.5295324 -5.7070217 -6.0670319 -6.5522633 -7.1021628 -7.6560464 -8.0370722 -8.12143 -7.8687072 -7.2878442 -6.4455309 -5.5655484 -4.8611937 -4.4614029 -4.1965122]]...]
INFO - root - 2017-12-16 04:22:59.067581: step 9410, loss = 0.29, batch loss = 0.23 (27.2 examples/sec; 0.294 sec/batch; 26h:22m:04s remains)
INFO - root - 2017-12-16 04:23:01.872609: step 9420, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.279 sec/batch; 25h:02m:58s remains)
INFO - root - 2017-12-16 04:23:04.746034: step 9430, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 25h:21m:27s remains)
INFO - root - 2017-12-16 04:23:07.591079: step 9440, loss = 0.24, batch loss = 0.18 (29.0 examples/sec; 0.276 sec/batch; 24h:44m:09s remains)
INFO - root - 2017-12-16 04:23:10.443216: step 9450, loss = 0.27, batch loss = 0.21 (28.8 examples/sec; 0.278 sec/batch; 24h:57m:16s remains)
INFO - root - 2017-12-16 04:23:13.301065: step 9460, loss = 0.41, batch loss = 0.35 (28.4 examples/sec; 0.282 sec/batch; 25h:19m:17s remains)
INFO - root - 2017-12-16 04:23:16.110277: step 9470, loss = 0.42, batch loss = 0.37 (27.3 examples/sec; 0.293 sec/batch; 26h:16m:44s remains)
INFO - root - 2017-12-16 04:23:18.906561: step 9480, loss = 0.23, batch loss = 0.18 (28.7 examples/sec; 0.279 sec/batch; 25h:00m:10s remains)
INFO - root - 2017-12-16 04:23:21.733117: step 9490, loss = 0.34, batch loss = 0.28 (29.5 examples/sec; 0.271 sec/batch; 24h:18m:37s remains)
INFO - root - 2017-12-16 04:23:24.661900: step 9500, loss = 0.30, batch loss = 0.25 (28.3 examples/sec; 0.283 sec/batch; 25h:22m:56s remains)
2017-12-16 04:23:25.115635: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3764243 -6.6284223 -6.54344 -6.03761 -5.5180221 -4.9268889 -4.4329414 -4.1351061 -4.3161211 -4.8910241 -5.4328647 -6.0486379 -6.7311287 -7.2509003 -7.4256163][-7.0155287 -7.4904346 -7.4696875 -6.8575764 -6.1784143 -5.4388342 -4.6787367 -4.0208898 -3.8724496 -4.4437618 -5.3400121 -6.1155906 -6.8050189 -7.2276516 -7.3340788][-7.1653409 -7.7231703 -7.6227531 -6.9516282 -6.0673304 -5.1148558 -4.236629 -3.6618867 -3.5483074 -3.8413935 -4.4263639 -5.2435083 -6.1410146 -6.7729506 -7.1104841][-6.2186279 -6.7061653 -6.4548664 -5.5814266 -4.5593524 -3.6515214 -2.7797375 -2.41585 -2.427557 -2.7905407 -3.2889993 -3.8826931 -4.5782361 -5.2980561 -5.7170019][-4.8377018 -4.9454126 -4.44486 -3.6876302 -2.8005929 -1.8605614 -1.2249255 -0.88949084 -0.78434014 -1.0040636 -1.4549661 -1.7029762 -2.3100736 -3.2618172 -4.0366769][-3.7270081 -3.3608048 -2.5390162 -1.5331814 -0.37690163 0.17347336 0.40419912 0.59234238 0.62769079 0.44608402 0.080736637 -0.18252373 -0.78011155 -1.5725067 -2.343641][-2.4783134 -1.9131117 -0.97033143 0.20217276 1.4728308 2.1955471 2.5356317 2.3569946 1.9750676 1.6437011 1.2472186 0.786788 0.11275196 -0.94906068 -1.8468802][-2.1242781 -1.4256363 -0.29287815 0.92419147 1.9910698 2.7831378 3.2092209 3.1874266 2.7698917 2.2185645 1.5666986 0.90635157 0.10928345 -0.99619842 -1.986995][-2.6293235 -2.1106906 -1.1103954 -0.06981945 0.92160225 1.7172475 2.2103009 2.4323015 2.1086035 1.4424834 0.72379684 -0.001701355 -0.74861836 -1.6404285 -2.7222407][-3.0004797 -2.9804902 -2.6202862 -1.9075353 -1.0293281 -0.19599152 0.3082037 0.52673721 0.35131645 -0.23129416 -0.80218482 -1.5115559 -2.2885833 -3.073307 -3.7344778][-4.2494478 -4.171916 -3.8052964 -3.504715 -3.2067013 -2.5636089 -1.9846156 -1.7323411 -1.7443967 -2.052062 -2.3420286 -2.8765159 -3.5094507 -3.9525971 -4.2589722][-5.3576112 -5.492775 -5.5673423 -5.2500768 -4.9176283 -4.6487374 -4.4906969 -4.16609 -3.8049579 -3.7119865 -3.6543846 -3.6928413 -3.8229494 -3.9469421 -4.0904741][-6.3025022 -6.4065018 -6.6526814 -6.6347194 -6.6954637 -6.5998783 -6.4103513 -6.1745257 -5.915607 -5.5119858 -5.0965891 -4.7390094 -4.4896307 -4.1218967 -3.8584669][-6.7965117 -6.717762 -6.7895927 -6.8942862 -7.0449696 -7.0628519 -6.9993958 -6.9338207 -6.6120391 -6.0811987 -5.7022977 -5.3124018 -4.9434814 -4.4905195 -4.0415974][-6.7059298 -6.3517847 -6.2720265 -6.4325867 -6.6488323 -6.8230658 -6.8403044 -6.7669983 -6.5075874 -6.1358824 -5.8462667 -5.3920417 -4.9460607 -4.5514746 -4.1734605]]...]
INFO - root - 2017-12-16 04:23:27.909132: step 9510, loss = 0.49, batch loss = 0.43 (29.0 examples/sec; 0.276 sec/batch; 24h:44m:43s remains)
INFO - root - 2017-12-16 04:23:30.697374: step 9520, loss = 0.23, batch loss = 0.17 (28.1 examples/sec; 0.284 sec/batch; 25h:31m:08s remains)
INFO - root - 2017-12-16 04:23:33.563374: step 9530, loss = 0.36, batch loss = 0.30 (27.3 examples/sec; 0.293 sec/batch; 26h:16m:57s remains)
INFO - root - 2017-12-16 04:23:36.417377: step 9540, loss = 0.33, batch loss = 0.27 (28.8 examples/sec; 0.278 sec/batch; 24h:55m:15s remains)
INFO - root - 2017-12-16 04:23:39.283133: step 9550, loss = 0.23, batch loss = 0.17 (27.9 examples/sec; 0.287 sec/batch; 25h:44m:45s remains)
INFO - root - 2017-12-16 04:23:42.123372: step 9560, loss = 0.22, batch loss = 0.16 (27.2 examples/sec; 0.294 sec/batch; 26h:24m:44s remains)
INFO - root - 2017-12-16 04:23:44.992278: step 9570, loss = 0.28, batch loss = 0.22 (26.8 examples/sec; 0.299 sec/batch; 26h:47m:22s remains)
INFO - root - 2017-12-16 04:23:47.850029: step 9580, loss = 0.35, batch loss = 0.29 (27.6 examples/sec; 0.290 sec/batch; 26h:02m:40s remains)
INFO - root - 2017-12-16 04:23:50.680215: step 9590, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.278 sec/batch; 24h:58m:35s remains)
INFO - root - 2017-12-16 04:23:53.499308: step 9600, loss = 0.23, batch loss = 0.17 (27.2 examples/sec; 0.294 sec/batch; 26h:24m:13s remains)
2017-12-16 04:23:53.964289: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4436398 -5.946218 -6.6763439 -6.7285042 -6.7429924 -6.3211117 -6.0418434 -6.1161447 -5.8947697 -5.7647138 -5.4550161 -4.5819368 -3.7154315 -3.1390398 -2.9809361][-6.6816339 -6.5183868 -6.5892477 -7.017025 -7.3082376 -7.3366566 -7.3402033 -7.0890951 -6.7821865 -6.5405936 -6.2617593 -5.9701138 -5.533288 -4.8873854 -4.4509797][-7.2344379 -7.1733656 -7.0649118 -7.1453662 -7.3109541 -6.7654424 -6.0492945 -6.3146362 -6.8111248 -6.8290515 -6.7648335 -6.5817957 -6.4058862 -6.0468731 -5.5912991][-7.1365328 -6.9146624 -6.8818774 -6.5659628 -6.090898 -5.3571267 -4.28057 -4.0185547 -4.552732 -5.7296982 -6.5963917 -6.8945756 -6.8315935 -6.5410962 -6.150774][-6.8093815 -6.350174 -6.0802865 -5.3345823 -4.5286136 -3.1115985 -1.6381922 -0.83695459 -0.92014241 -2.6343954 -4.5839944 -5.8854246 -6.2720566 -5.9342809 -5.248724][-5.5672789 -5.190865 -4.5186744 -3.2350903 -1.0698738 0.91368055 2.3705187 3.1184435 2.6437349 0.76122427 -1.7593553 -3.8169293 -4.8107224 -4.980269 -4.4401174][-5.0334992 -4.3728118 -3.2607594 -1.1031315 1.6358361 4.2680721 6.2889528 6.6650314 5.78407 3.5602775 0.85116196 -1.6300721 -3.154573 -3.4123425 -3.1820638][-4.9506192 -4.1867981 -3.1449904 -0.8839097 2.1147804 4.9064054 6.8644361 7.2522783 6.2129374 3.6761227 0.98035383 -0.91348696 -1.811053 -2.2643864 -2.4372702][-4.8877692 -4.4538121 -3.7619834 -1.9910488 0.30782413 3.0116115 4.9012032 5.3996449 4.59019 2.548677 0.30698824 -1.5990288 -2.4624395 -2.3470275 -1.8289056][-4.7105265 -4.7443671 -4.7245412 -3.83473 -2.265173 -0.30827332 1.0884638 1.8017869 1.363833 -0.33749676 -2.1309428 -3.4378128 -3.6845284 -3.1603336 -2.3893945][-5.7083077 -5.8515172 -5.83848 -5.6250725 -5.2176538 -4.3175116 -3.1300201 -2.5038705 -2.8400679 -3.7876382 -4.658977 -5.1922889 -5.1728835 -4.5055771 -3.5461669][-6.1145778 -6.6443071 -7.3216944 -7.2122755 -6.8580103 -6.6924763 -6.4037981 -6.0722141 -5.8681555 -6.0474815 -6.3353629 -6.7202263 -6.5619106 -5.5683346 -4.3532977][-5.3072338 -6.1679268 -7.1094017 -7.6940918 -8.0514946 -7.4983063 -6.9758687 -6.7995605 -6.715312 -6.5653133 -6.4642067 -6.6762838 -6.620369 -6.1219239 -5.21983][-5.0173264 -5.5262628 -6.0837364 -6.7901363 -7.2763462 -7.1240535 -6.7233253 -6.3478603 -6.3302374 -6.1860971 -5.8978729 -5.8002105 -5.8298006 -5.6551919 -5.05117][-4.8077679 -4.9432731 -5.2114587 -5.4920816 -5.70183 -5.7303147 -5.5734725 -5.2456346 -4.9779954 -5.0342617 -5.2077503 -5.0195618 -4.7893333 -4.7720685 -4.6457596]]...]
INFO - root - 2017-12-16 04:23:56.800023: step 9610, loss = 0.21, batch loss = 0.15 (29.1 examples/sec; 0.275 sec/batch; 24h:40m:42s remains)
INFO - root - 2017-12-16 04:23:59.664409: step 9620, loss = 0.22, batch loss = 0.16 (25.6 examples/sec; 0.312 sec/batch; 27h:59m:00s remains)
INFO - root - 2017-12-16 04:24:02.493676: step 9630, loss = 0.27, batch loss = 0.22 (27.5 examples/sec; 0.291 sec/batch; 26h:04m:44s remains)
INFO - root - 2017-12-16 04:24:05.300230: step 9640, loss = 0.28, batch loss = 0.22 (27.7 examples/sec; 0.289 sec/batch; 25h:53m:24s remains)
INFO - root - 2017-12-16 04:24:08.193897: step 9650, loss = 0.46, batch loss = 0.40 (24.3 examples/sec; 0.329 sec/batch; 29h:29m:50s remains)
INFO - root - 2017-12-16 04:24:11.041943: step 9660, loss = 0.25, batch loss = 0.19 (29.2 examples/sec; 0.274 sec/batch; 24h:34m:25s remains)
INFO - root - 2017-12-16 04:24:13.899285: step 9670, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.284 sec/batch; 25h:29m:29s remains)
INFO - root - 2017-12-16 04:24:16.720494: step 9680, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.283 sec/batch; 25h:22m:07s remains)
INFO - root - 2017-12-16 04:24:19.568919: step 9690, loss = 0.31, batch loss = 0.25 (25.6 examples/sec; 0.313 sec/batch; 28h:02m:35s remains)
INFO - root - 2017-12-16 04:24:22.395831: step 9700, loss = 0.27, batch loss = 0.22 (28.3 examples/sec; 0.283 sec/batch; 25h:23m:24s remains)
2017-12-16 04:24:22.872357: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.727581 -4.095561 -4.4625373 -4.4863729 -4.270597 -4.1369333 -4.1097465 -4.1317129 -4.0549994 -4.0356431 -4.0103374 -3.9563198 -3.7673125 -3.6927071 -3.89933][-3.9176321 -4.2005725 -4.3877254 -4.4157667 -4.0758839 -3.7513323 -3.602562 -3.5848663 -3.5554714 -3.5162203 -3.5311923 -3.3633542 -3.2594416 -3.191071 -3.3160777][-4.0374131 -4.1766725 -4.1977658 -3.8845954 -3.4389977 -3.2759407 -3.314225 -3.2790256 -3.2500734 -3.311846 -3.3378663 -3.3180313 -3.1911514 -3.0725582 -3.1705396][-4.0779123 -3.7598484 -3.2051582 -2.6121984 -2.0974026 -1.9586239 -2.2950945 -2.6981812 -2.8515229 -2.9645405 -2.9373569 -2.7819793 -2.6140118 -2.6964874 -2.8650365][-3.2478862 -2.262332 -1.1090515 -0.32871294 0.16132307 0.045565128 -0.46606326 -1.1399889 -1.6353092 -1.8083475 -1.7323704 -1.5057967 -1.4904034 -1.5281146 -1.7151566][-2.1410403 -0.87504554 0.5375247 1.5709901 2.046288 1.6664433 0.81403208 0.084962845 -0.33423758 -0.49399829 -0.42231607 -0.32385874 -0.40605211 -0.55320621 -0.7695148][-1.6489329 -0.20229959 1.2913527 2.3726974 2.786375 2.456099 1.8497128 1.2083135 0.77659082 0.61478281 0.634418 0.68226624 0.2670126 -0.14497805 -0.573195][-2.2811465 -0.49864531 1.0276885 1.968749 2.3961043 2.3121047 2.0236211 1.681211 1.4008307 1.276288 1.1699562 0.9113307 0.3603406 -0.15335226 -0.60053444][-3.3876805 -1.8329539 -0.25505114 0.74375343 1.1299453 1.1313481 1.0665689 1.1051898 1.1172752 1.0921602 0.93664551 0.46598577 -0.036885738 -0.39767408 -0.84027219][-4.9641862 -3.7682173 -2.479888 -1.3426576 -0.58552861 -0.36308908 -0.29725885 -0.22328043 -0.077392578 -0.020648479 -0.22898912 -0.71204782 -1.2795 -1.4935868 -1.5858634][-6.3542032 -5.2506337 -4.3268175 -3.4878728 -2.4698343 -1.7700086 -1.3729842 -1.4824178 -1.6584136 -1.6468382 -1.8099427 -2.2851126 -2.7489865 -2.834291 -3.0137134][-6.2132649 -5.50696 -4.8226995 -4.1883488 -3.6435976 -2.9232063 -2.4032171 -2.467742 -2.6407208 -2.7989345 -3.2281451 -3.8626885 -4.2319689 -4.0291848 -3.8409424][-5.0990138 -4.6340995 -4.3679481 -4.0913477 -3.8046951 -3.3948202 -2.9820318 -3.0226879 -3.3496566 -3.6977985 -4.08217 -4.5798874 -4.8123369 -4.474731 -4.1108136][-4.4439759 -3.9166102 -3.6841242 -3.58281 -3.6233063 -3.5162191 -3.2856889 -3.358613 -3.3669178 -3.535923 -3.933682 -4.354022 -4.4744687 -4.0074325 -3.4675112][-3.3855662 -2.9082642 -2.7346506 -2.8649006 -3.0765932 -2.9640837 -2.9655995 -3.0880995 -3.2446547 -3.3129306 -3.554461 -3.8595817 -3.6882663 -2.9008267 -2.1654224]]...]
INFO - root - 2017-12-16 04:24:25.709046: step 9710, loss = 0.18, batch loss = 0.13 (27.4 examples/sec; 0.292 sec/batch; 26h:11m:58s remains)
INFO - root - 2017-12-16 04:24:28.541854: step 9720, loss = 0.27, batch loss = 0.22 (27.7 examples/sec; 0.289 sec/batch; 25h:54m:21s remains)
INFO - root - 2017-12-16 04:24:31.402492: step 9730, loss = 0.36, batch loss = 0.30 (28.6 examples/sec; 0.280 sec/batch; 25h:05m:15s remains)
INFO - root - 2017-12-16 04:24:34.231495: step 9740, loss = 0.28, batch loss = 0.23 (28.6 examples/sec; 0.280 sec/batch; 25h:05m:59s remains)
INFO - root - 2017-12-16 04:24:37.078024: step 9750, loss = 0.28, batch loss = 0.22 (26.1 examples/sec; 0.306 sec/batch; 27h:26m:26s remains)
INFO - root - 2017-12-16 04:24:39.961933: step 9760, loss = 0.27, batch loss = 0.21 (27.5 examples/sec; 0.291 sec/batch; 26h:04m:39s remains)
INFO - root - 2017-12-16 04:24:42.802693: step 9770, loss = 0.31, batch loss = 0.25 (28.9 examples/sec; 0.276 sec/batch; 24h:46m:47s remains)
INFO - root - 2017-12-16 04:24:45.652008: step 9780, loss = 0.37, batch loss = 0.31 (28.2 examples/sec; 0.284 sec/batch; 25h:26m:58s remains)
INFO - root - 2017-12-16 04:24:48.484652: step 9790, loss = 0.27, batch loss = 0.22 (27.0 examples/sec; 0.296 sec/batch; 26h:33m:31s remains)
INFO - root - 2017-12-16 04:24:51.365501: step 9800, loss = 0.27, batch loss = 0.21 (29.4 examples/sec; 0.272 sec/batch; 24h:22m:14s remains)
2017-12-16 04:24:51.812560: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5481572 -4.9149518 -5.2950759 -5.5905981 -5.8088322 -5.8510652 -5.8346629 -5.9588861 -5.9098115 -5.6255555 -5.3362184 -5.1621532 -5.3038168 -5.6587963 -5.8599048][-3.4421964 -3.71282 -3.9766009 -4.3049126 -4.6377354 -5.09064 -5.4941692 -5.5337772 -5.5551643 -5.4389277 -5.3048897 -5.2246933 -5.4321842 -5.8624754 -6.104177][-1.7787008 -2.1482389 -2.5083852 -3.0027626 -3.327642 -3.5993025 -3.8985472 -4.2384152 -4.4813771 -4.4966145 -4.6696868 -4.9396081 -5.3874125 -5.9097385 -6.0881553][-0.57515144 -0.90579295 -1.1742473 -1.3924892 -1.7127755 -2.0257707 -2.2738757 -2.3890607 -2.6278992 -2.9926462 -3.5961287 -4.2525597 -4.9440813 -5.5396566 -5.7914281][-0.045115471 -0.10169697 -0.066097736 -0.028150558 0.071960926 0.13964558 0.12387323 -0.11679459 -0.47549391 -0.87549186 -1.7417619 -2.8914018 -3.9406366 -4.5649781 -4.8695989][-0.19276905 0.058585644 0.35262203 0.59931707 0.95586109 1.4840322 2.1226239 2.2928581 2.031508 1.3563318 0.20502663 -1.2221005 -2.6386786 -3.6624365 -4.2879615][-1.4045727 -0.94347239 -0.41486073 0.41576958 1.7755575 3.0939355 4.1029081 4.5593414 4.362854 3.469624 2.1203737 0.50175381 -1.1433182 -2.4696398 -3.5610332][-2.6660366 -2.1768153 -1.5917776 -0.53611255 1.224721 3.2739186 5.0386229 5.6690073 5.6648359 4.9899921 3.4429736 1.50245 -0.33891773 -1.7690632 -3.0534282][-4.5460987 -4.1310477 -3.5117433 -2.4823649 -0.69654894 1.636951 3.6859112 4.606986 5.0169821 4.5919981 3.3646541 1.6942177 -0.25111866 -2.1104128 -3.6018925][-6.0670772 -6.3591695 -6.3704391 -5.3568287 -3.628386 -1.4888124 0.63977718 2.1546431 2.8706231 2.6448164 1.811872 0.43428564 -1.2683184 -2.8958986 -4.2917061][-7.1774697 -7.9383974 -8.449935 -8.0552073 -6.8360949 -4.7949867 -2.7756057 -1.2825809 -0.23740625 -0.052137852 -0.74893427 -1.9821687 -3.2872367 -4.7297482 -5.8679323][-9.0142241 -10.021652 -10.648422 -10.433436 -9.4223061 -7.79558 -6.25628 -4.6832509 -3.7297568 -3.7754927 -4.0760527 -4.608645 -5.3749366 -6.3187 -6.906991][-10.145462 -11.478251 -12.238285 -11.994787 -11.412417 -10.25765 -8.9306679 -7.639328 -6.8926668 -6.7462969 -6.8996196 -7.2716866 -7.528234 -7.7981138 -7.7198095][-10.820189 -11.958008 -12.446684 -12.168077 -11.719167 -11.125354 -10.496599 -9.6223555 -9.0911446 -9.0812931 -9.1820927 -9.1955318 -9.032814 -8.8083363 -8.14646][-10.685711 -11.638788 -12.0235 -11.687813 -11.150887 -10.613937 -10.260756 -10.002661 -10.03549 -10.229044 -10.305173 -9.9581165 -9.4433336 -8.8734369 -7.8105011]]...]
INFO - root - 2017-12-16 04:24:54.601050: step 9810, loss = 0.21, batch loss = 0.16 (29.2 examples/sec; 0.274 sec/batch; 24h:32m:40s remains)
INFO - root - 2017-12-16 04:24:57.476470: step 9820, loss = 0.22, batch loss = 0.16 (27.8 examples/sec; 0.288 sec/batch; 25h:48m:41s remains)
INFO - root - 2017-12-16 04:25:00.306201: step 9830, loss = 0.36, batch loss = 0.30 (29.5 examples/sec; 0.271 sec/batch; 24h:18m:09s remains)
INFO - root - 2017-12-16 04:25:03.088121: step 9840, loss = 0.32, batch loss = 0.26 (28.2 examples/sec; 0.284 sec/batch; 25h:25m:24s remains)
INFO - root - 2017-12-16 04:25:05.864413: step 9850, loss = 0.26, batch loss = 0.20 (29.3 examples/sec; 0.273 sec/batch; 24h:28m:24s remains)
INFO - root - 2017-12-16 04:25:08.778590: step 9860, loss = 0.18, batch loss = 0.12 (27.8 examples/sec; 0.288 sec/batch; 25h:48m:58s remains)
INFO - root - 2017-12-16 04:25:11.608695: step 9870, loss = 0.28, batch loss = 0.22 (28.9 examples/sec; 0.277 sec/batch; 24h:50m:54s remains)
INFO - root - 2017-12-16 04:25:14.396936: step 9880, loss = 0.27, batch loss = 0.22 (28.8 examples/sec; 0.278 sec/batch; 24h:52m:32s remains)
INFO - root - 2017-12-16 04:25:17.216713: step 9890, loss = 0.27, batch loss = 0.21 (28.7 examples/sec; 0.278 sec/batch; 24h:56m:51s remains)
INFO - root - 2017-12-16 04:25:20.016944: step 9900, loss = 0.35, batch loss = 0.29 (28.2 examples/sec; 0.284 sec/batch; 25h:25m:10s remains)
2017-12-16 04:25:20.484394: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3217545 -5.9556513 -5.2762251 -4.7731261 -4.688385 -4.7119188 -4.5236626 -4.5710249 -4.6677766 -4.8533421 -5.0141768 -5.2594781 -5.5907073 -5.7219973 -5.8694558][-6.6305819 -6.2070618 -5.5364308 -4.9245386 -4.4572992 -4.4601417 -4.6948295 -4.8927736 -4.6616211 -4.7316079 -4.97722 -5.3273964 -5.5821228 -5.6681461 -5.9387445][-7.1969333 -6.4811616 -5.4924831 -4.5779123 -3.8838618 -3.5778668 -3.5494528 -4.18074 -4.4759307 -4.9494352 -4.9575086 -5.2490454 -5.6227007 -5.7817669 -5.8396177][-6.6792355 -6.1348491 -5.1916676 -3.7020767 -2.4582605 -1.7672102 -1.5222309 -2.1715124 -3.1453075 -4.4577351 -4.8687162 -5.2687745 -5.5996289 -5.9072075 -5.9592681][-6.2721338 -5.30075 -3.8239985 -1.9939606 -0.52010703 0.56334543 0.67028904 -0.15944481 -1.3147151 -2.8358822 -3.7260396 -4.6190491 -4.9757605 -5.3727117 -5.8565307][-6.5500674 -5.2807183 -3.2261667 -0.53545 1.6244197 2.8970304 2.8924432 1.8405313 0.40642357 -1.2834256 -2.0703089 -3.0009365 -3.869072 -4.479527 -4.940073][-6.6904345 -5.4564 -3.2636909 0.17393112 2.9321723 4.5070877 4.6636848 3.5337167 1.8196192 -0.20371532 -1.3813231 -2.1314554 -2.8747146 -3.8438044 -4.9213872][-7.3346744 -5.6289535 -2.9599705 0.22456741 2.6311855 4.4209414 4.7945452 3.6742096 1.9914246 0.34030056 -0.67952228 -1.6499748 -2.6797514 -3.7370951 -5.0751467][-7.9763727 -6.4195786 -3.9176216 -0.73225951 1.4841065 2.8334599 3.0227861 2.3258457 1.2060857 0.078373432 -0.56088591 -1.2648129 -2.2333045 -3.3473864 -4.8458896][-8.502018 -7.127728 -5.4933333 -2.9030471 -0.90783763 0.22817039 0.51198196 0.21504164 -0.42644215 -1.1311586 -1.6011972 -1.937016 -2.6683874 -3.6201007 -4.8881059][-9.60825 -7.9063668 -6.0730782 -4.3694935 -3.5197258 -2.4254687 -1.8068905 -2.0107942 -2.7088943 -3.0153906 -3.2047977 -3.4000397 -3.7551954 -4.3135633 -5.1881104][-9.6018963 -8.3804035 -6.8102579 -5.1921806 -4.3974504 -4.1738257 -4.1546583 -4.1907449 -4.2498589 -4.1203561 -4.0286551 -4.1458712 -4.3679953 -4.4478474 -4.7196097][-9.1446543 -7.9924049 -7.1247005 -5.9376497 -4.9129443 -4.4541082 -4.6839371 -5.2520218 -5.2993994 -5.0562553 -4.6558523 -4.5898085 -4.5635405 -4.5202746 -4.293119][-9.7730675 -8.1805353 -6.5129786 -5.5679789 -5.0425596 -4.5499845 -4.3297782 -5.1629267 -5.789319 -5.8922882 -5.2780428 -4.9155383 -4.9314775 -4.9641294 -4.4026155][-9.4745665 -8.2658186 -6.4315867 -5.203145 -4.7193089 -4.3995404 -4.1514359 -4.6632557 -5.3528967 -5.835814 -5.6928272 -5.1944857 -5.0391684 -5.0267563 -4.6366887]]...]
INFO - root - 2017-12-16 04:25:23.303285: step 9910, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.285 sec/batch; 25h:34m:32s remains)
INFO - root - 2017-12-16 04:25:26.124570: step 9920, loss = 0.33, batch loss = 0.28 (29.4 examples/sec; 0.272 sec/batch; 24h:20m:49s remains)
INFO - root - 2017-12-16 04:25:28.976903: step 9930, loss = 0.41, batch loss = 0.35 (26.9 examples/sec; 0.298 sec/batch; 26h:40m:10s remains)
INFO - root - 2017-12-16 04:25:31.829292: step 9940, loss = 0.24, batch loss = 0.18 (27.6 examples/sec; 0.290 sec/batch; 25h:59m:16s remains)
INFO - root - 2017-12-16 04:25:34.631070: step 9950, loss = 0.22, batch loss = 0.16 (29.6 examples/sec; 0.271 sec/batch; 24h:14m:37s remains)
INFO - root - 2017-12-16 04:25:37.488931: step 9960, loss = 0.26, batch loss = 0.20 (25.9 examples/sec; 0.309 sec/batch; 27h:42m:14s remains)
INFO - root - 2017-12-16 04:25:40.358022: step 9970, loss = 0.26, batch loss = 0.21 (27.4 examples/sec; 0.292 sec/batch; 26h:09m:51s remains)
INFO - root - 2017-12-16 04:25:43.189790: step 9980, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.281 sec/batch; 25h:12m:12s remains)
INFO - root - 2017-12-16 04:25:45.964837: step 9990, loss = 0.29, batch loss = 0.23 (28.3 examples/sec; 0.283 sec/batch; 25h:21m:18s remains)
INFO - root - 2017-12-16 04:25:48.827954: step 10000, loss = 0.19, batch loss = 0.14 (28.6 examples/sec; 0.280 sec/batch; 25h:04m:16s remains)
2017-12-16 04:25:49.278593: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3464513 -4.3848481 -4.1034689 -3.759402 -3.4345822 -3.4380441 -3.5413523 -3.90957 -4.2756462 -4.5864458 -4.845232 -4.9593859 -5.1084867 -5.002862 -4.9012971][-4.7549391 -4.5605764 -4.0767236 -3.4920337 -2.9049926 -2.8270645 -2.9513531 -3.2733746 -3.6312931 -4.0759573 -4.4824615 -4.61634 -4.8279457 -4.8117471 -4.8063178][-5.1618919 -4.7808585 -4.0872312 -3.1434004 -2.199085 -1.8482256 -1.855967 -2.2559638 -2.6967492 -3.1833415 -3.6463175 -3.9030681 -4.2474542 -4.3169212 -4.3750467][-5.0833273 -4.63364 -3.8244245 -2.5089102 -1.2203982 -0.54089642 -0.21436977 -0.49141288 -1.1250389 -1.838908 -2.3828521 -2.7667692 -3.2635617 -3.5368919 -3.6750557][-4.7069864 -4.0464969 -3.0312634 -1.4871073 0.066128254 1.0401306 1.5079708 1.2774162 0.59255838 -0.33118391 -1.0240285 -1.5209317 -2.0159183 -2.411052 -2.6229377][-4.4938636 -3.7174315 -2.398767 -0.63598251 1.1015587 2.3873281 3.0676398 2.7718954 1.9987059 1.0552859 0.44349051 -0.1506753 -0.66970754 -1.0066268 -1.1610122][-4.1912541 -3.4989171 -2.2950926 -0.42265224 1.4296417 2.8992324 3.690959 3.5193229 2.7110968 1.7176085 1.2605567 0.70407486 0.23260927 -0.13153744 -0.24170113][-4.3726516 -3.7107096 -2.5246143 -0.89018226 0.87295771 2.5275598 3.4957705 3.4279289 2.7504864 1.7689004 1.2790332 0.7291584 0.39925766 0.13009644 -0.031060696][-4.6991215 -4.2254934 -3.1073146 -1.6431012 -0.041800976 1.5585761 2.4125156 2.3041825 1.7211261 0.84857988 0.48052216 0.085630894 -0.15977526 -0.36915112 -0.47205925][-5.0982513 -4.8305407 -4.0843363 -2.8730249 -1.4913986 -0.1902318 0.53500271 0.45913363 -0.074723244 -0.79563594 -1.11759 -1.4488082 -1.6668334 -1.931015 -2.0258632][-5.3005395 -4.9802747 -4.5059409 -3.8957865 -2.9687307 -2.1229644 -1.7608216 -1.8855076 -2.3683875 -2.9958267 -3.3156652 -3.554848 -3.7124004 -4.046679 -4.174355][-5.1342125 -4.9833722 -4.6061521 -4.3051882 -4.0883656 -3.8268554 -3.7239618 -3.9048915 -4.3036356 -4.9038591 -5.2680378 -5.4899011 -5.671453 -6.0429692 -6.1655641][-4.7884951 -4.7300243 -4.5024791 -4.3302493 -4.224247 -4.4227228 -4.7225685 -4.9587297 -5.4108968 -5.9214129 -6.2400255 -6.4577675 -6.6329479 -7.0878696 -7.3857708][-4.3972878 -4.1666956 -3.8346543 -3.6395905 -3.61235 -4.06726 -4.7415133 -5.3227124 -5.9471812 -6.4257803 -6.6804595 -6.9098911 -7.0937109 -7.4957027 -7.8337259][-4.3565049 -4.1690354 -3.6345665 -3.2029173 -3.1640697 -3.6115434 -4.2627487 -4.8841715 -5.554997 -6.0880013 -6.3400025 -6.5438209 -6.5612812 -6.8331928 -7.1798592]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 04:25:52.670948: step 10010, loss = 0.31, batch loss = 0.25 (28.0 examples/sec; 0.286 sec/batch; 25h:35m:04s remains)
INFO - root - 2017-12-16 04:25:55.512289: step 10020, loss = 0.26, batch loss = 0.20 (27.0 examples/sec; 0.296 sec/batch; 26h:31m:19s remains)
INFO - root - 2017-12-16 04:25:58.329402: step 10030, loss = 0.38, batch loss = 0.32 (29.5 examples/sec; 0.271 sec/batch; 24h:15m:05s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 04:26:01.168039: step 10040, loss = 0.38, batch loss = 0.32 (27.5 examples/sec; 0.290 sec/batch; 26h:00m:54s remains)
INFO - root - 2017-12-16 04:26:04.015349: step 10050, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.285 sec/batch; 25h:32m:18s remains)
INFO - root - 2017-12-16 04:26:06.863392: step 10060, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.281 sec/batch; 25h:11m:32s remains)
INFO - root - 2017-12-16 04:26:09.691298: step 10070, loss = 0.38, batch loss = 0.33 (28.8 examples/sec; 0.278 sec/batch; 24h:51m:46s remains)
INFO - root - 2017-12-16 04:26:12.536797: step 10080, loss = 0.26, batch loss = 0.20 (27.0 examples/sec; 0.296 sec/batch; 26h:32m:41s remains)
INFO - root - 2017-12-16 04:26:15.373502: step 10090, loss = 0.23, batch loss = 0.18 (29.0 examples/sec; 0.276 sec/batch; 24h:43m:43s remains)
INFO - root - 2017-12-16 04:26:18.225539: step 10100, loss = 0.30, batch loss = 0.25 (28.8 examples/sec; 0.278 sec/batch; 24h:53m:06s remains)
2017-12-16 04:26:18.662654: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2356167 -5.3099284 -5.3408122 -5.4636545 -5.5290885 -5.4833317 -5.2204175 -5.1520057 -5.3492255 -5.9244947 -6.6840124 -7.5819874 -8.67479 -9.46971 -10.044962][-4.9596496 -5.2566209 -5.3780813 -5.387198 -5.4317017 -5.6694036 -5.4964123 -5.0523992 -4.7759843 -5.1068482 -5.9129925 -6.8703251 -7.9077711 -8.593092 -9.2316723][-4.3157458 -4.6825995 -4.67432 -4.5921745 -4.540823 -4.8534622 -5.0376067 -4.9929848 -4.8388815 -4.8995619 -5.2051926 -5.6969657 -6.7340574 -7.4699697 -8.2237988][-3.1739771 -3.7115488 -4.0056963 -3.4397573 -2.9554598 -3.0000248 -3.1050863 -3.4050848 -3.7545433 -4.0546722 -4.0414743 -4.4795108 -5.41549 -6.1324825 -7.2059][-2.2255642 -2.2767065 -2.0940607 -1.7495501 -1.3793535 -1.0174489 -0.85659981 -1.0504634 -1.2239437 -2.0687122 -2.6726356 -2.8446221 -3.2409325 -4.1949039 -5.6997223][-2.1302066 -1.4488103 -0.52328253 0.59531641 1.4538555 1.627913 1.5650492 1.400044 1.1189919 0.51391554 0.36473131 -0.27505255 -1.1647232 -1.8191488 -3.1018367][-2.5780306 -1.654161 -0.47774053 0.94233561 2.609436 3.6098576 3.8426352 3.40944 3.0421691 2.3838916 2.0003629 1.6488562 1.1621141 0.027630329 -1.7724154][-3.3527064 -2.3517947 -1.0470719 0.8129158 2.6077032 3.8273945 4.7110891 4.7702169 4.3139763 3.1770096 2.6486874 2.1898398 1.3927293 0.34081316 -1.0812716][-4.4773364 -3.8360753 -2.6932087 -1.0652645 0.67231941 2.0312772 2.9431152 2.9987941 3.0925531 2.6270456 2.2254949 1.5868955 0.90766764 0.21951342 -0.74053025][-4.9289122 -4.6037035 -4.1842394 -3.1753125 -1.8327591 -0.88589072 -0.054878235 0.26725435 0.21926165 0.0010085106 0.23928499 0.020102978 -0.47851276 -0.88095832 -1.5769103][-5.3429384 -5.0823708 -4.8139815 -4.5593104 -4.5017014 -3.8608837 -2.9776115 -2.8579993 -2.8437653 -2.8450637 -2.6705918 -2.8445787 -3.1786823 -3.0421319 -3.2192442][-5.604661 -5.3944378 -5.4224072 -5.5457644 -5.6103878 -5.7426839 -5.83601 -5.7787104 -5.5837297 -5.3488493 -5.0564156 -5.1272893 -5.0683126 -4.7257495 -4.7704682][-5.4358287 -5.4068136 -5.680203 -5.9267015 -6.2605591 -6.4073124 -6.4341569 -6.3804169 -6.2407722 -6.1462073 -5.9590988 -5.8699141 -5.8003082 -5.6187935 -5.4301357][-4.276742 -4.3796673 -4.6465435 -5.06947 -5.4961958 -5.7119141 -5.9329786 -5.995574 -6.0581779 -5.8951616 -5.654253 -5.9315062 -6.073679 -5.9104562 -5.7134266][-4.2651205 -4.05823 -3.9717128 -4.1681466 -4.422183 -4.6813369 -4.9478922 -4.9792881 -4.9019918 -5.0359406 -5.1721892 -5.0642524 -4.9147139 -5.0276289 -5.0912514]]...]
INFO - root - 2017-12-16 04:26:21.484948: step 10110, loss = 0.27, batch loss = 0.21 (29.3 examples/sec; 0.273 sec/batch; 24h:24m:48s remains)
INFO - root - 2017-12-16 04:26:24.366473: step 10120, loss = 0.33, batch loss = 0.27 (29.2 examples/sec; 0.274 sec/batch; 24h:33m:00s remains)
INFO - root - 2017-12-16 04:26:27.177088: step 10130, loss = 0.24, batch loss = 0.18 (28.9 examples/sec; 0.277 sec/batch; 24h:47m:28s remains)
INFO - root - 2017-12-16 04:26:29.976186: step 10140, loss = 0.24, batch loss = 0.18 (29.9 examples/sec; 0.268 sec/batch; 23h:59m:27s remains)
INFO - root - 2017-12-16 04:26:32.769365: step 10150, loss = 0.26, batch loss = 0.20 (28.4 examples/sec; 0.281 sec/batch; 25h:11m:28s remains)
INFO - root - 2017-12-16 04:26:35.578778: step 10160, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.284 sec/batch; 25h:25m:25s remains)
INFO - root - 2017-12-16 04:26:38.404623: step 10170, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 25h:12m:41s remains)
INFO - root - 2017-12-16 04:26:41.178364: step 10180, loss = 0.28, batch loss = 0.22 (28.6 examples/sec; 0.280 sec/batch; 25h:02m:42s remains)
INFO - root - 2017-12-16 04:26:44.018821: step 10190, loss = 0.23, batch loss = 0.17 (28.9 examples/sec; 0.277 sec/batch; 24h:45m:30s remains)
INFO - root - 2017-12-16 04:26:46.842057: step 10200, loss = 0.22, batch loss = 0.16 (29.9 examples/sec; 0.268 sec/batch; 23h:58m:10s remains)
2017-12-16 04:26:47.283761: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3970978 -2.7344255 -3.03896 -3.2827539 -3.5642309 -3.7282088 -3.69596 -3.5191355 -3.3461573 -3.3023512 -3.3342969 -3.4012585 -3.6293163 -3.7800448 -3.7533031][-2.9662142 -3.0610781 -3.2036057 -3.4488344 -3.7551322 -3.9257345 -3.9352188 -3.7171814 -3.4512372 -3.4565396 -3.5270367 -3.5879858 -3.6587896 -3.8082414 -3.8408453][-3.0309472 -3.0765145 -3.22713 -3.4264145 -3.6099467 -3.7713943 -3.7485166 -3.544117 -3.3556383 -3.2405384 -3.2413502 -3.3989339 -3.5669498 -3.7005939 -3.6936526][-3.0572567 -2.794142 -2.7512605 -2.9033132 -3.0405977 -3.2996955 -3.461915 -3.2938328 -3.0238442 -3.0082121 -3.0289435 -2.9565444 -2.9324419 -3.2138009 -3.3152261][-2.3324168 -1.8501222 -1.8176043 -2.0217404 -2.2113528 -2.5166943 -2.8387239 -2.9888601 -2.9221005 -2.7503288 -2.6010027 -2.5936282 -2.4626906 -2.3486829 -2.3379195][-1.2048824 -0.59907556 -0.46553111 -0.4599371 -0.64257479 -1.1940682 -1.7777407 -2.1664851 -2.3382735 -2.4123771 -2.2571921 -1.8763013 -1.4627845 -1.2760196 -1.2707629][-0.020270824 0.55450583 0.60121059 0.39329767 0.026239872 -0.37148142 -0.82684541 -1.2920978 -1.5996165 -1.648072 -1.5215878 -1.2846708 -0.77837181 -0.39066219 -0.27189445][0.65346527 0.9759717 0.91350603 0.73990726 0.36637831 -0.0040006638 -0.36491346 -0.73507738 -0.99030447 -1.0677879 -0.83892608 -0.47470665 0.0020194054 0.34887505 0.39880991][0.12686682 0.51968288 0.541811 0.23772049 -0.24390841 -0.37768793 -0.47330928 -0.59262395 -0.57300711 -0.4500525 -0.075847626 0.16316986 0.50995255 0.73798418 0.60904455][-0.64453149 -0.64678335 -0.82262278 -0.87972522 -1.1364484 -1.1627312 -1.0145612 -0.79812169 -0.596277 -0.30453825 0.18894148 0.57928228 0.83578205 0.71393442 0.35488129][-2.191071 -2.084368 -2.1484904 -2.3173695 -2.4597247 -2.2719808 -1.9110007 -1.4153838 -0.92431355 -0.44530392 0.082526207 0.44079065 0.44659472 0.11257124 -0.35834742][-3.1976929 -3.4948263 -3.8487463 -4.0107174 -4.209816 -3.9497108 -3.3348956 -2.608109 -1.8686619 -1.1331944 -0.51369476 -0.1581111 -0.24583149 -0.71126747 -1.3327606][-4.6515532 -4.8687191 -5.1257625 -5.33968 -5.4965105 -5.2400694 -4.6228142 -3.6855798 -2.7757838 -1.8168137 -1.1413445 -0.79801631 -1.0677962 -1.6275756 -2.2053275][-5.6287847 -5.917304 -6.0909324 -6.1908007 -6.2442083 -5.8284616 -5.1401343 -4.3013258 -3.4655404 -2.4953952 -1.9607601 -1.572319 -1.7497971 -2.4334919 -3.0574369][-5.9102254 -6.1932507 -6.342248 -6.3515434 -6.1762047 -5.9567661 -5.4553089 -4.6353674 -4.0471592 -3.2392125 -2.6781569 -2.246263 -2.5110843 -2.985569 -3.4045746]]...]
INFO - root - 2017-12-16 04:26:50.101655: step 10210, loss = 0.35, batch loss = 0.29 (28.9 examples/sec; 0.277 sec/batch; 24h:49m:14s remains)
INFO - root - 2017-12-16 04:26:52.934658: step 10220, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.277 sec/batch; 24h:50m:29s remains)
INFO - root - 2017-12-16 04:26:55.742715: step 10230, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.284 sec/batch; 25h:25m:54s remains)
INFO - root - 2017-12-16 04:26:58.531083: step 10240, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.285 sec/batch; 25h:33m:16s remains)
INFO - root - 2017-12-16 04:27:01.376757: step 10250, loss = 0.25, batch loss = 0.20 (27.6 examples/sec; 0.290 sec/batch; 25h:56m:42s remains)
INFO - root - 2017-12-16 04:27:04.232690: step 10260, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.286 sec/batch; 25h:36m:22s remains)
INFO - root - 2017-12-16 04:27:07.126932: step 10270, loss = 0.31, batch loss = 0.25 (27.9 examples/sec; 0.286 sec/batch; 25h:38m:15s remains)
INFO - root - 2017-12-16 04:27:09.962309: step 10280, loss = 0.21, batch loss = 0.15 (29.1 examples/sec; 0.275 sec/batch; 24h:37m:38s remains)
INFO - root - 2017-12-16 04:27:12.742394: step 10290, loss = 0.41, batch loss = 0.35 (28.1 examples/sec; 0.284 sec/batch; 25h:27m:42s remains)
INFO - root - 2017-12-16 04:27:15.538080: step 10300, loss = 0.29, batch loss = 0.23 (29.1 examples/sec; 0.275 sec/batch; 24h:37m:38s remains)
2017-12-16 04:27:16.045096: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7633324 -2.9638085 -3.1921248 -3.3470187 -3.4625456 -3.4921799 -3.5316143 -3.6756995 -3.7596684 -3.5642662 -3.4142652 -3.4707952 -3.2811995 -2.8668971 -2.6755331][-2.6395688 -2.9517117 -3.1538286 -3.196312 -3.2225676 -3.4798927 -3.7532854 -3.7457323 -3.7011545 -3.9209661 -4.0672827 -3.8423717 -3.4057639 -3.0655136 -2.8781126][-2.5857444 -2.9181106 -3.3331156 -3.4174433 -3.3494854 -3.25245 -3.2175758 -3.7093437 -4.1612663 -4.1359777 -3.9856474 -4.1576214 -4.0299845 -3.5572991 -3.150846][-2.9368377 -3.1677136 -3.4906666 -3.6257935 -3.3424811 -2.9805002 -2.7244811 -2.6790233 -3.0096347 -3.8732417 -4.4220304 -4.332592 -3.9922814 -3.8528812 -3.5312204][-3.1613331 -3.619374 -3.9754331 -3.8771982 -3.2873611 -2.0460675 -0.86898851 -0.90754223 -1.7856739 -2.7455785 -3.4495578 -4.1091433 -4.200779 -3.8093739 -3.3499146][-3.4075971 -3.8445432 -4.2801676 -3.7739847 -2.3227942 -0.64484453 0.92983484 1.4644794 0.50646448 -1.2308667 -2.8224494 -3.6266553 -3.6747499 -3.5086706 -3.0557017][-3.3887653 -3.6726623 -3.4637721 -2.7528577 -1.1529691 1.2458754 3.0608048 3.3189807 2.4206085 0.55344486 -1.4906549 -2.8991277 -3.1860514 -2.8073778 -2.2722888][-3.0937128 -3.4487448 -3.6683376 -2.3308175 -0.035055637 2.4677682 4.4705791 4.4881258 3.0343862 1.1675849 -0.56132579 -1.9676208 -2.4128513 -2.1389098 -1.531425][-3.5492649 -3.6592498 -3.2086177 -2.3545127 -0.6288259 1.8780007 3.419322 3.65162 2.648591 0.69535494 -0.92830157 -1.7959912 -2.0398591 -1.8278244 -1.3306077][-3.2162251 -3.7842593 -4.1851254 -3.2810316 -1.8634596 -0.042473316 1.3793974 1.7039495 0.751462 -0.539165 -1.643327 -2.3469198 -2.2863348 -1.9096529 -1.386843][-4.1209545 -4.5261683 -5.0082793 -5.07559 -4.361454 -2.838697 -1.933265 -1.290971 -1.4488807 -2.1589448 -2.5979338 -2.9908135 -2.8214428 -2.2912631 -1.7872708][-4.778614 -5.4141216 -6.1046181 -6.3556132 -6.3304749 -5.4703517 -4.4947758 -3.8981502 -3.5498776 -3.4620919 -3.6258075 -3.5519264 -3.0788813 -2.7402096 -2.6069217][-5.3940258 -6.0714664 -6.6089392 -6.8786678 -7.1160083 -6.4630065 -5.82679 -5.2174749 -4.8049335 -4.2934 -3.7389729 -3.7558558 -3.5265043 -3.0464387 -2.7831366][-5.8167253 -6.442616 -6.6984949 -6.8641434 -6.8677521 -6.2843323 -5.93731 -5.5833035 -5.3254585 -4.896215 -4.5012021 -4.0065284 -3.3438725 -3.1389902 -3.2341838][-5.7307339 -5.8727574 -5.9106879 -5.8818774 -5.7736878 -5.4148531 -5.292964 -5.1523986 -5.0701776 -4.8339024 -4.5463839 -4.2809238 -3.7629128 -3.2335982 -2.9596102]]...]
INFO - root - 2017-12-16 04:27:18.892377: step 10310, loss = 0.27, batch loss = 0.21 (28.9 examples/sec; 0.277 sec/batch; 24h:46m:45s remains)
INFO - root - 2017-12-16 04:27:21.714808: step 10320, loss = 0.31, batch loss = 0.26 (29.9 examples/sec; 0.267 sec/batch; 23h:55m:24s remains)
INFO - root - 2017-12-16 04:27:24.560132: step 10330, loss = 0.20, batch loss = 0.14 (27.6 examples/sec; 0.290 sec/batch; 25h:55m:22s remains)
INFO - root - 2017-12-16 04:27:27.416416: step 10340, loss = 0.31, batch loss = 0.25 (27.1 examples/sec; 0.295 sec/batch; 26h:22m:21s remains)
INFO - root - 2017-12-16 04:27:30.247268: step 10350, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.283 sec/batch; 25h:21m:38s remains)
INFO - root - 2017-12-16 04:27:33.026101: step 10360, loss = 0.23, batch loss = 0.17 (29.2 examples/sec; 0.274 sec/batch; 24h:28m:35s remains)
INFO - root - 2017-12-16 04:27:35.868883: step 10370, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.279 sec/batch; 25h:00m:26s remains)
INFO - root - 2017-12-16 04:27:38.685279: step 10380, loss = 0.22, batch loss = 0.16 (28.4 examples/sec; 0.282 sec/batch; 25h:13m:34s remains)
INFO - root - 2017-12-16 04:27:41.558089: step 10390, loss = 0.25, batch loss = 0.20 (27.4 examples/sec; 0.292 sec/batch; 26h:09m:28s remains)
INFO - root - 2017-12-16 04:27:44.390677: step 10400, loss = 0.32, batch loss = 0.26 (27.7 examples/sec; 0.288 sec/batch; 25h:48m:24s remains)
2017-12-16 04:27:44.872662: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9152226 -3.7376182 -3.1844425 -3.6214702 -4.3738236 -4.7009025 -5.056952 -5.2151752 -5.4101353 -5.7235594 -5.9482808 -5.9187412 -5.7469263 -5.3998442 -4.7832355][-4.2233 -3.4457526 -2.7617345 -2.6298709 -3.0854342 -4.037353 -4.6332679 -4.8174591 -4.9613595 -5.07288 -5.3474216 -5.3574677 -5.27589 -4.7702804 -4.1962891][-4.0758042 -2.8572845 -1.8103285 -1.6327217 -1.7822995 -2.5224986 -3.2815816 -3.9300549 -4.1792583 -4.3044271 -4.6410761 -4.6422873 -4.5077772 -4.1983275 -3.4711823][-3.1529629 -1.8620291 -0.88271475 -0.6005106 -1.2113137 -1.6005857 -1.9441652 -2.6118221 -3.0642767 -3.4431248 -3.6101825 -3.9156179 -4.0036097 -3.7142916 -3.1118279][-2.9595847 -1.0815685 0.30484295 0.73241472 0.53223515 -0.096821785 -0.80190492 -1.201268 -1.4302528 -2.1012092 -2.7691469 -2.9222932 -2.9799261 -2.9637456 -2.751523][-3.0047655 -0.960634 0.74384975 1.8457875 2.2643213 1.7469325 1.2241879 0.62544107 -0.1641717 -0.83713746 -1.6039422 -2.2531564 -2.5243955 -2.2315645 -1.9742775][-3.6446612 -1.4237394 0.43578911 1.6015339 2.2394133 2.3460927 2.1506815 1.471529 0.73293304 -0.18246603 -1.186533 -1.8908904 -2.1503286 -2.0422771 -2.1437087][-4.3957853 -2.5526042 -0.81243038 0.83286 1.67449 1.9188933 2.1685753 1.791574 0.98808336 -0.1449151 -1.2552152 -1.7946897 -2.114706 -2.3024657 -2.5289559][-5.1671844 -3.8138442 -2.3765883 -0.716465 0.66842175 1.5278497 1.9819469 1.497148 0.71197271 -0.37884521 -1.5602028 -2.3679173 -2.6700733 -2.8250318 -3.2000976][-5.901547 -5.087647 -4.2988725 -2.9913597 -1.44577 -0.22360373 0.5073595 0.51441383 -0.16618443 -1.5112684 -2.7522748 -3.5264211 -3.7834644 -3.7596734 -3.9024453][-6.90064 -6.1447582 -5.4908638 -4.8064542 -3.9428749 -2.5366538 -1.424356 -1.3820183 -1.8871722 -2.8726463 -3.8461208 -4.4728189 -4.6312618 -4.4945307 -4.3960357][-7.4293351 -6.6426096 -6.1590757 -5.6322112 -4.9750528 -4.3098965 -3.5902793 -3.0780518 -3.0215492 -3.5601096 -4.276546 -4.8430643 -4.9713449 -4.6391106 -4.5570946][-7.4954071 -6.4822006 -5.8812575 -5.5973072 -5.2704425 -4.8721561 -4.4065762 -4.3096194 -4.3133588 -4.4455142 -4.7046475 -4.8783231 -4.849793 -4.67358 -4.3505387][-7.6041441 -6.1817122 -5.5396891 -5.1641135 -5.01285 -4.9642668 -4.944272 -4.9517679 -4.88129 -4.9539862 -4.946156 -4.7526078 -4.6222839 -4.274435 -3.6470361][-7.7607589 -6.2277355 -5.0308037 -4.5522165 -4.6455975 -4.7838964 -4.7910552 -4.8645144 -4.8100228 -4.8827062 -4.8475165 -4.5028296 -4.1116514 -3.7004743 -3.252707]]...]
INFO - root - 2017-12-16 04:27:47.684350: step 10410, loss = 0.32, batch loss = 0.26 (29.3 examples/sec; 0.273 sec/batch; 24h:25m:43s remains)
INFO - root - 2017-12-16 04:27:50.498228: step 10420, loss = 0.38, batch loss = 0.32 (27.9 examples/sec; 0.287 sec/batch; 25h:39m:00s remains)
INFO - root - 2017-12-16 04:27:53.327490: step 10430, loss = 0.24, batch loss = 0.19 (28.9 examples/sec; 0.277 sec/batch; 24h:46m:33s remains)
INFO - root - 2017-12-16 04:27:56.182687: step 10440, loss = 0.28, batch loss = 0.22 (29.1 examples/sec; 0.275 sec/batch; 24h:36m:25s remains)
INFO - root - 2017-12-16 04:27:58.972731: step 10450, loss = 0.29, batch loss = 0.23 (29.6 examples/sec; 0.270 sec/batch; 24h:08m:50s remains)
INFO - root - 2017-12-16 04:28:01.805252: step 10460, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.280 sec/batch; 25h:03m:14s remains)
INFO - root - 2017-12-16 04:28:04.579714: step 10470, loss = 0.23, batch loss = 0.18 (29.6 examples/sec; 0.270 sec/batch; 24h:11m:04s remains)
INFO - root - 2017-12-16 04:28:07.368245: step 10480, loss = 0.24, batch loss = 0.18 (27.7 examples/sec; 0.289 sec/batch; 25h:49m:21s remains)
INFO - root - 2017-12-16 04:28:10.222514: step 10490, loss = 0.32, batch loss = 0.26 (29.0 examples/sec; 0.276 sec/batch; 24h:39m:54s remains)
INFO - root - 2017-12-16 04:28:13.077058: step 10500, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.278 sec/batch; 24h:54m:18s remains)
2017-12-16 04:28:13.516654: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2568278 -5.1905456 -5.196507 -5.5313807 -6.0976486 -6.2924318 -6.2551241 -6.1597266 -6.022047 -5.5867467 -4.8726606 -4.0165057 -3.1245022 -2.1585021 -1.6524844][-4.917789 -5.1483569 -5.7791991 -6.0582705 -6.4609838 -7.1398492 -7.5782471 -7.2395735 -6.5884905 -6.16002 -5.7463136 -4.7571406 -3.5730007 -2.5194173 -1.7038074][-4.5994544 -5.0040431 -5.5399547 -6.1538372 -6.9812889 -7.3699565 -7.3957076 -7.2604218 -6.9422541 -6.3198528 -5.7464132 -5.1456585 -4.3390474 -3.2357283 -2.4635253][-4.6775503 -5.1251879 -5.8013229 -6.3373246 -6.7692089 -6.882905 -6.7506266 -6.2813983 -5.8803825 -5.8026819 -5.6597137 -5.2062545 -4.7754588 -4.0748944 -3.2709785][-4.1554208 -4.7118015 -5.343874 -5.5410852 -5.3455439 -4.678381 -3.8782971 -3.4151154 -3.4797256 -3.7743037 -4.4795427 -4.9401426 -4.9302626 -4.2918677 -3.583905][-3.824707 -4.1288953 -4.4465714 -4.1830306 -3.015765 -1.6421618 -0.40035534 0.45054102 0.10070515 -1.0072026 -2.6970429 -3.8913016 -4.6849756 -4.5137625 -3.7765331][-3.7691846 -3.8249803 -3.4235978 -2.6356676 -1.2386148 0.64190865 2.7163768 3.6786032 2.9907002 1.5067534 -0.525856 -2.2798381 -3.5667834 -3.9056098 -3.672411][-3.4278479 -3.3441005 -2.6296585 -1.0901387 1.0149894 3.153286 4.9640608 5.6089497 4.5733089 2.6736951 0.58386326 -1.2907524 -2.795996 -3.5295615 -3.4338834][-3.0263834 -3.0658803 -2.3707557 -1.0269415 0.82608795 3.001112 4.7649469 5.3928185 4.1247072 1.9806514 -0.33721733 -2.0330873 -3.1777837 -3.8502958 -3.7605641][-2.7022281 -3.1292784 -3.2771246 -2.4073231 -1.0087233 0.27209759 1.4098921 2.1151347 1.4602556 -0.041635036 -1.8788652 -3.3011079 -4.2243533 -4.6567292 -4.4746933][-3.3955834 -3.7969892 -4.0819845 -4.193922 -3.7457185 -2.749424 -2.1422794 -2.1170971 -2.4873323 -3.1900783 -4.1089129 -4.7965021 -5.1607623 -5.2019339 -4.8382339][-4.0096207 -4.9164414 -5.923852 -6.0092645 -5.6989307 -5.6060667 -5.4920826 -5.4622126 -5.5658917 -5.8136725 -5.8787403 -5.8860087 -5.8477783 -5.6511669 -5.0714808][-4.6346736 -5.6335106 -6.543932 -7.2900047 -7.7782526 -7.3577733 -6.9208364 -6.8232441 -6.8053637 -6.464468 -5.901804 -5.6807222 -5.6075206 -5.5172458 -5.0687189][-5.2592669 -6.1421814 -6.763082 -7.2773151 -7.4300785 -7.6648455 -7.6139021 -7.0270128 -6.4218988 -5.8104777 -5.3638444 -4.9809432 -4.7622495 -4.7332935 -4.56548][-5.2971644 -5.9041882 -6.3296351 -6.5593176 -6.5778532 -6.5099974 -6.1844044 -5.813221 -5.2930751 -4.8403296 -4.4550571 -4.1909156 -4.0269918 -3.960212 -3.8452737]]...]
INFO - root - 2017-12-16 04:28:16.319423: step 10510, loss = 0.33, batch loss = 0.27 (28.4 examples/sec; 0.282 sec/batch; 25h:12m:52s remains)
INFO - root - 2017-12-16 04:28:19.135142: step 10520, loss = 0.32, batch loss = 0.26 (29.2 examples/sec; 0.274 sec/batch; 24h:27m:57s remains)
INFO - root - 2017-12-16 04:28:21.969769: step 10530, loss = 0.32, batch loss = 0.26 (28.4 examples/sec; 0.281 sec/batch; 25h:09m:39s remains)
INFO - root - 2017-12-16 04:28:24.833201: step 10540, loss = 0.23, batch loss = 0.17 (29.1 examples/sec; 0.275 sec/batch; 24h:34m:48s remains)
INFO - root - 2017-12-16 04:28:27.645373: step 10550, loss = 0.25, batch loss = 0.19 (27.9 examples/sec; 0.286 sec/batch; 25h:36m:43s remains)
INFO - root - 2017-12-16 04:28:30.468520: step 10560, loss = 0.28, batch loss = 0.22 (28.8 examples/sec; 0.278 sec/batch; 24h:51m:49s remains)
INFO - root - 2017-12-16 04:28:33.303045: step 10570, loss = 0.22, batch loss = 0.16 (28.0 examples/sec; 0.285 sec/batch; 25h:31m:46s remains)
INFO - root - 2017-12-16 04:28:36.167030: step 10580, loss = 0.24, batch loss = 0.18 (29.3 examples/sec; 0.273 sec/batch; 24h:24m:38s remains)
INFO - root - 2017-12-16 04:28:39.044994: step 10590, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.284 sec/batch; 25h:26m:20s remains)
INFO - root - 2017-12-16 04:28:41.897714: step 10600, loss = 0.27, batch loss = 0.21 (27.5 examples/sec; 0.291 sec/batch; 26h:00m:49s remains)
2017-12-16 04:28:42.334834: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9870162 -4.0678163 -4.0997005 -4.4064507 -4.7832041 -5.1650953 -5.6626444 -6.0037441 -6.282414 -6.1767092 -5.9175177 -5.2553225 -4.5370069 -4.025229 -3.8063371][-3.2293916 -3.1357272 -3.1806352 -3.4998536 -3.7831912 -4.383317 -4.9902453 -5.4505038 -5.793222 -5.8051653 -5.5789356 -4.9761186 -4.2853804 -3.6068051 -3.220865][-1.9057851 -1.7633715 -1.9177401 -2.2265754 -2.613184 -3.1690347 -3.7087121 -4.216866 -4.5670481 -4.86266 -4.8655853 -4.3725905 -3.669035 -2.9121404 -2.4571223][-0.55246449 -0.37546396 -0.69053149 -1.0611038 -1.4976044 -1.8512399 -2.307049 -2.9163232 -3.4237432 -3.8794513 -3.920877 -3.6639156 -3.217268 -2.4191632 -1.8752799][0.394876 0.59222555 0.20244741 -0.14346504 -0.37489176 -0.49093318 -0.69404864 -1.2739763 -2.0320258 -3.0131941 -3.6382895 -3.695286 -3.2548077 -2.7540853 -2.269654][0.28250074 0.53031158 0.45603514 0.58068037 1.0441847 1.382185 1.4951749 1.0218349 -0.01637888 -1.5415149 -2.8286655 -3.546649 -3.6370189 -3.3390837 -3.0034652][-0.52895331 -0.19455242 -0.095367432 0.55368233 1.5185332 2.6123052 3.2374406 2.8877158 1.9130554 0.29726171 -1.370049 -2.8056335 -3.5468247 -3.5526521 -3.3431239][-1.5685515 -1.1294591 -0.83014727 0.07119894 1.3807912 2.7788935 3.7533674 3.6792011 2.7651658 0.97525692 -0.9079299 -2.3900843 -3.4274404 -3.9412503 -3.9714496][-2.4420819 -1.9236095 -1.380924 -0.44610834 0.72337532 2.1130085 3.2192025 3.2380242 2.5156941 0.8506813 -1.2288976 -3.1410234 -4.4902515 -4.979321 -4.8925333][-3.0964622 -2.7508721 -2.3348172 -1.5228181 -0.40901232 0.83246469 1.7422843 1.9990835 1.525456 0.038244724 -1.7181904 -3.4510629 -4.855526 -5.4276581 -5.1762729][-3.4399161 -3.1446457 -3.07346 -2.7400911 -2.0372946 -1.0909426 -0.20740366 0.22882271 0.048503876 -0.88332939 -2.3380933 -3.5933626 -4.5804863 -4.8968153 -4.5439329][-2.6710126 -2.567668 -2.8122602 -3.0406349 -2.900167 -2.4006798 -1.8134503 -1.508795 -1.4560912 -1.8039129 -2.8068233 -3.8778653 -4.5542083 -4.453413 -3.9426882][-1.7862275 -1.8908284 -2.4102197 -3.0212107 -3.4136419 -3.3246102 -2.7730074 -2.3324988 -2.2037826 -2.4834766 -3.1018052 -3.8243985 -4.3319736 -4.2434516 -3.7116282][-1.2749963 -1.5351214 -2.1692054 -2.8715196 -3.2742209 -3.3139429 -2.9596329 -2.6293278 -2.3614216 -2.4262786 -3.041229 -3.5916927 -3.9830117 -3.9290581 -3.7027752][-0.99055386 -1.1337438 -1.7151442 -2.3833673 -2.7688947 -2.7863402 -2.486552 -2.0418015 -1.8397601 -1.9781442 -2.5782342 -3.1884193 -3.6492248 -3.6341007 -3.467984]]...]
INFO - root - 2017-12-16 04:28:45.196077: step 10610, loss = 0.28, batch loss = 0.22 (25.7 examples/sec; 0.311 sec/batch; 27h:49m:59s remains)
INFO - root - 2017-12-16 04:28:48.037144: step 10620, loss = 0.27, batch loss = 0.21 (26.9 examples/sec; 0.297 sec/batch; 26h:32m:41s remains)
INFO - root - 2017-12-16 04:28:50.829583: step 10630, loss = 0.29, batch loss = 0.23 (27.5 examples/sec; 0.291 sec/batch; 26h:00m:22s remains)
INFO - root - 2017-12-16 04:28:53.653066: step 10640, loss = 0.20, batch loss = 0.14 (28.0 examples/sec; 0.286 sec/batch; 25h:34m:46s remains)
INFO - root - 2017-12-16 04:28:56.446430: step 10650, loss = 0.26, batch loss = 0.20 (27.3 examples/sec; 0.293 sec/batch; 26h:12m:22s remains)
INFO - root - 2017-12-16 04:28:59.308086: step 10660, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.283 sec/batch; 25h:20m:05s remains)
INFO - root - 2017-12-16 04:29:02.152442: step 10670, loss = 0.41, batch loss = 0.35 (27.7 examples/sec; 0.289 sec/batch; 25h:50m:44s remains)
INFO - root - 2017-12-16 04:29:04.923535: step 10680, loss = 0.23, batch loss = 0.17 (29.2 examples/sec; 0.274 sec/batch; 24h:27m:29s remains)
INFO - root - 2017-12-16 04:29:07.803386: step 10690, loss = 0.32, batch loss = 0.27 (28.5 examples/sec; 0.280 sec/batch; 25h:03m:21s remains)
INFO - root - 2017-12-16 04:29:10.616376: step 10700, loss = 0.22, batch loss = 0.16 (28.9 examples/sec; 0.277 sec/batch; 24h:46m:24s remains)
2017-12-16 04:29:11.050770: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4605308 -5.8237343 -6.2565455 -6.5038118 -6.6892996 -6.587203 -6.5623608 -6.9632015 -7.3163247 -7.6282978 -7.8818445 -7.9100127 -7.6468248 -7.1204619 -6.3736191][-5.2266893 -5.7487679 -6.46265 -6.7428389 -6.7579927 -6.8282161 -6.8790388 -6.8614211 -7.1796732 -7.884232 -8.433732 -8.6368561 -8.4774971 -7.9746046 -7.1922607][-4.3934059 -4.9981642 -5.725697 -6.1227903 -6.0415354 -5.5016332 -5.101459 -5.2413659 -5.8627477 -6.8800249 -7.8658628 -8.4297752 -8.7464161 -8.4926243 -7.7135758][-3.7355661 -4.2527356 -4.7829328 -4.6628852 -4.1664171 -3.3889554 -2.6627078 -2.6650283 -3.3426955 -4.6738992 -5.9892936 -7.1478806 -8.0236387 -7.927599 -7.4577737][-3.0310979 -3.2954497 -3.6500249 -3.2405167 -2.0994086 -0.5593977 0.68156242 0.78023195 0.051037312 -1.6486712 -3.429661 -4.917901 -6.057023 -6.5894642 -6.6711192][-2.3612633 -2.3996985 -2.1887312 -1.1591477 0.3518424 2.1041989 3.6792936 4.1546078 3.4594584 1.4674888 -0.57805753 -2.4922242 -4.1694932 -5.2233553 -5.6975579][-2.5698943 -2.0155239 -1.1453757 0.27412891 2.3329792 4.5260239 6.254962 6.6532011 5.8834162 3.7521105 1.3634768 -0.98651576 -2.8557026 -4.1466141 -4.9841728][-3.1284537 -2.8961024 -2.2168334 -0.29584312 2.1475782 4.4801836 6.49527 7.2125454 6.5603962 4.4280853 2.0027728 -0.48430586 -2.6671505 -4.1527996 -4.9288392][-5.0642471 -4.7113576 -3.8715267 -2.3340974 -0.46483707 1.8959937 4.0917768 4.9012594 4.4832067 2.8500595 0.76164055 -1.4183249 -3.2491713 -4.6430888 -5.4578943][-5.4893656 -5.4692054 -5.4985824 -4.6155677 -3.1204257 -1.2429292 0.20794535 1.1548734 1.433414 0.36314249 -1.1242459 -2.7344022 -4.283536 -5.4224997 -6.0031028][-6.5771933 -6.8783793 -7.0690727 -6.5926518 -6.0892878 -4.9310527 -3.5616326 -2.6287284 -2.5356021 -3.0333786 -3.6982296 -4.7391057 -5.7996736 -6.5806489 -6.9091654][-7.1454763 -7.8807697 -8.6199808 -8.7537165 -8.4616556 -7.5273857 -6.792141 -6.1182156 -5.5577922 -5.5315843 -5.8923426 -6.4112577 -6.8107386 -7.0928183 -7.0930462][-7.3877549 -8.1887512 -8.9486008 -9.4616013 -9.5804176 -9.126955 -8.5907669 -7.6998777 -7.0045242 -6.8155236 -6.6838675 -6.5491595 -6.6317492 -6.8909731 -6.8089685][-6.3995671 -7.1355028 -7.892643 -8.4914112 -8.7747087 -8.7622147 -8.475069 -7.9084578 -7.4175582 -6.9957237 -6.74822 -6.8478003 -6.8371143 -6.6121922 -6.2365022][-5.946259 -6.3593025 -6.804935 -7.1834993 -7.3325911 -7.2802649 -7.1481457 -6.8573494 -6.5289297 -6.3660603 -6.2881613 -6.1246996 -5.8199062 -5.6433773 -5.3316231]]...]
INFO - root - 2017-12-16 04:29:13.834554: step 10710, loss = 0.24, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 24h:31m:16s remains)
INFO - root - 2017-12-16 04:29:16.663746: step 10720, loss = 0.23, batch loss = 0.17 (27.8 examples/sec; 0.287 sec/batch; 25h:41m:17s remains)
INFO - root - 2017-12-16 04:29:19.463569: step 10730, loss = 0.23, batch loss = 0.17 (29.1 examples/sec; 0.274 sec/batch; 24h:32m:03s remains)
INFO - root - 2017-12-16 04:29:22.297683: step 10740, loss = 0.21, batch loss = 0.15 (25.3 examples/sec; 0.316 sec/batch; 28h:12m:36s remains)
INFO - root - 2017-12-16 04:29:25.181158: step 10750, loss = 0.24, batch loss = 0.18 (28.5 examples/sec; 0.281 sec/batch; 25h:06m:56s remains)
INFO - root - 2017-12-16 04:29:27.981045: step 10760, loss = 0.19, batch loss = 0.13 (29.0 examples/sec; 0.276 sec/batch; 24h:40m:44s remains)
INFO - root - 2017-12-16 04:29:30.778742: step 10770, loss = 0.24, batch loss = 0.18 (28.9 examples/sec; 0.276 sec/batch; 24h:42m:14s remains)
INFO - root - 2017-12-16 04:29:33.542411: step 10780, loss = 0.33, batch loss = 0.28 (28.8 examples/sec; 0.278 sec/batch; 24h:48m:21s remains)
INFO - root - 2017-12-16 04:29:36.351872: step 10790, loss = 0.26, batch loss = 0.20 (29.5 examples/sec; 0.272 sec/batch; 24h:15m:53s remains)
INFO - root - 2017-12-16 04:29:39.168097: step 10800, loss = 0.35, batch loss = 0.30 (28.6 examples/sec; 0.280 sec/batch; 25h:00m:20s remains)
2017-12-16 04:29:39.616649: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4270153 -6.940752 -7.380558 -7.7021294 -7.7936239 -7.8450127 -7.8928809 -7.9712009 -8.1000385 -8.237196 -8.0121326 -7.736516 -7.2994957 -6.6276588 -5.8267646][-7.7365818 -8.1569014 -8.3356953 -8.3420105 -8.3704071 -8.6952543 -8.9497213 -9.2219563 -9.5638571 -9.8819733 -9.873497 -9.6086311 -8.8821 -8.0574169 -7.1788864][-8.1719475 -8.3553343 -8.372364 -8.0286713 -7.835268 -7.937006 -8.2192907 -9.1183586 -9.9132481 -10.586087 -10.961372 -10.843362 -10.333798 -9.5830259 -8.73864][-8.0439177 -7.8417978 -7.1667452 -6.5226536 -5.8801808 -5.6666679 -5.7995715 -6.6087208 -7.8477058 -9.5418472 -10.54265 -10.696476 -10.603032 -10.31403 -9.8550663][-7.0624094 -6.1158404 -4.6126671 -3.0980453 -1.7705617 -1.2252719 -1.5537639 -2.3787508 -3.751966 -5.7151713 -7.493495 -9.11383 -10.086795 -10.36236 -10.49472][-5.73563 -4.3388653 -2.3550129 0.21647644 2.4686255 3.6713448 3.8556852 2.6868348 0.78254652 -1.4391639 -3.6415567 -5.8928933 -8.0315332 -9.5470781 -10.312986][-5.398809 -3.8905873 -1.2989602 1.7402124 4.6798716 6.4696198 7.1803226 6.2976294 4.4300508 1.9824491 -0.63675237 -3.4382308 -6.11668 -7.9688063 -8.9981728][-5.9255905 -4.13665 -1.717365 1.4488196 4.8953352 7.2590256 8.2132053 7.561367 6.0165586 3.4770017 0.80735826 -2.0569177 -4.7548504 -6.7247458 -7.7847404][-6.8509703 -5.3434787 -3.1318846 -0.057565689 2.8203077 5.1412697 6.9927483 6.9895172 5.843545 3.5756493 1.0381794 -1.7907386 -4.4822283 -6.2533264 -6.8161626][-8.0407 -7.1509385 -5.4120917 -3.20264 -0.84986925 1.3318715 3.0008059 3.289216 2.8805695 1.2090235 -0.78074 -3.1210752 -5.1840329 -6.3396778 -6.7842159][-8.9346523 -8.530571 -7.3859596 -5.9313679 -4.4532337 -2.9956322 -1.2194414 -0.41173697 -0.6535337 -2.0420065 -3.3344734 -4.7788358 -6.09737 -6.9496546 -7.2832623][-9.0963764 -8.8953438 -8.5752516 -7.553998 -6.3063235 -5.440732 -4.2716227 -3.8298318 -3.571419 -4.0295291 -5.0758319 -6.4859247 -7.4068508 -7.56638 -7.4687762][-7.9587164 -7.8635578 -8.0182428 -7.5597014 -7.0194569 -6.1135821 -5.1731362 -5.1947608 -5.0371132 -5.2502627 -5.676857 -6.5484333 -7.2572155 -7.486062 -7.4871511][-6.365675 -6.3380423 -6.323266 -6.21629 -6.10769 -5.5182066 -4.9678206 -5.0756955 -5.1722922 -5.3115039 -5.3958693 -5.8168912 -6.3339834 -6.4514174 -6.2701979][-4.9040174 -4.649446 -4.4891558 -4.4604797 -4.4780426 -4.3570671 -4.2725329 -4.3331175 -4.3154316 -4.4886603 -4.5586476 -4.627749 -4.7585516 -4.8909154 -5.0105758]]...]
INFO - root - 2017-12-16 04:29:42.417631: step 10810, loss = 0.35, batch loss = 0.29 (28.9 examples/sec; 0.276 sec/batch; 24h:41m:39s remains)
INFO - root - 2017-12-16 04:29:45.213519: step 10820, loss = 0.25, batch loss = 0.19 (27.7 examples/sec; 0.289 sec/batch; 25h:50m:20s remains)
INFO - root - 2017-12-16 04:29:48.028312: step 10830, loss = 0.33, batch loss = 0.28 (28.1 examples/sec; 0.284 sec/batch; 25h:24m:30s remains)
INFO - root - 2017-12-16 04:29:50.917879: step 10840, loss = 0.34, batch loss = 0.28 (27.4 examples/sec; 0.292 sec/batch; 26h:06m:59s remains)
INFO - root - 2017-12-16 04:29:53.695279: step 10850, loss = 0.21, batch loss = 0.15 (29.5 examples/sec; 0.271 sec/batch; 24h:12m:06s remains)
INFO - root - 2017-12-16 04:29:56.518531: step 10860, loss = 0.32, batch loss = 0.26 (28.6 examples/sec; 0.279 sec/batch; 24h:56m:59s remains)
INFO - root - 2017-12-16 04:29:59.323303: step 10870, loss = 0.30, batch loss = 0.24 (28.9 examples/sec; 0.277 sec/batch; 24h:43m:10s remains)
INFO - root - 2017-12-16 04:30:02.158646: step 10880, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.281 sec/batch; 25h:07m:44s remains)
INFO - root - 2017-12-16 04:30:04.998636: step 10890, loss = 0.25, batch loss = 0.19 (30.2 examples/sec; 0.264 sec/batch; 23h:37m:35s remains)
INFO - root - 2017-12-16 04:30:07.860886: step 10900, loss = 0.29, batch loss = 0.23 (26.7 examples/sec; 0.300 sec/batch; 26h:45m:25s remains)
2017-12-16 04:30:08.313417: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7705667 -4.1456404 -4.454783 -4.8080025 -5.0930076 -5.2205524 -5.3172646 -5.5279608 -5.6615 -5.6008453 -5.3935924 -5.0842342 -4.6614332 -4.1721578 -3.7360637][-3.5547309 -3.9057033 -4.2334976 -4.6056576 -4.92285 -5.33219 -5.6079473 -5.675499 -5.6808987 -5.7737665 -5.7849946 -5.5835118 -5.1922364 -4.60166 -3.9999013][-3.3035979 -3.6332679 -3.9499655 -4.2000546 -4.3873348 -4.5629678 -4.7032633 -4.9510174 -5.1815138 -5.3876238 -5.5742254 -5.6172652 -5.4670291 -4.9576874 -4.3391638][-2.9581337 -3.0812979 -3.1645813 -3.1456594 -3.1037226 -2.9973793 -3.0029583 -3.1121678 -3.4041085 -3.9595633 -4.5888391 -5.0233068 -5.1754255 -4.9280653 -4.4781804][-2.5503979 -2.4347219 -2.1783245 -1.7521687 -1.2526097 -0.80770493 -0.55296779 -0.54352665 -0.93716192 -1.7433441 -2.8053679 -3.7614136 -4.3969426 -4.4974389 -4.2821083][-2.5513916 -2.2816699 -1.5902829 -0.54879069 0.61412382 1.5218244 2.1211205 2.2082472 1.622458 0.60223246 -0.77750063 -2.2122405 -3.3915575 -3.9184089 -4.0104375][-2.6627529 -2.1888998 -1.4647908 -0.11698437 1.6812568 3.2124853 4.1976347 4.4217482 3.8826351 2.7249093 1.0838075 -0.65624666 -2.1230969 -2.995841 -3.4309223][-3.1636736 -2.5773587 -1.6569352 -0.1298995 1.6230788 3.3341208 4.6343 5.0221291 4.5725584 3.5524087 2.0516462 0.24873924 -1.3483529 -2.4036715 -2.9900837][-3.9123855 -3.541748 -2.6857696 -1.3303149 0.30174017 1.9983621 3.3629746 3.9762621 3.8403368 3.0256343 1.7468548 0.24385262 -1.2253165 -2.3659983 -3.0232127][-4.5014138 -4.5166655 -4.2037005 -3.2975698 -1.869453 -0.36575031 0.90405369 1.7835732 2.0849781 1.6512666 0.67985487 -0.66957474 -1.9496901 -2.8870332 -3.4431129][-5.2742825 -5.1353397 -4.8462977 -4.6150212 -3.9785368 -2.9412045 -1.8155119 -0.95440841 -0.44444346 -0.40225077 -0.79294872 -1.7217381 -2.80567 -3.6380787 -4.0376287][-5.7314734 -5.7320647 -5.5583262 -5.2996011 -4.906281 -4.4121156 -3.7825358 -3.1414185 -2.6410232 -2.4323623 -2.5137234 -3.0290091 -3.6384928 -4.1811948 -4.520967][-5.504549 -5.6679316 -5.7591724 -5.6566238 -5.5078006 -5.181787 -4.7690916 -4.2916393 -3.8650086 -3.611347 -3.5792549 -3.8313041 -4.1309118 -4.524519 -4.7561421][-5.0712924 -5.2218838 -5.2427034 -5.238955 -5.1924081 -5.0001006 -4.8126159 -4.5442343 -4.2875609 -3.9906337 -3.771987 -3.8593402 -4.0155139 -4.2184854 -4.35973][-4.274271 -4.2259626 -4.1764631 -4.3288331 -4.3757424 -4.2472458 -4.1427155 -4.0073628 -3.8896046 -3.7485893 -3.6132231 -3.5455694 -3.4947803 -3.58656 -3.6910954]]...]
INFO - root - 2017-12-16 04:30:11.144017: step 10910, loss = 0.24, batch loss = 0.18 (27.9 examples/sec; 0.286 sec/batch; 25h:35m:23s remains)
INFO - root - 2017-12-16 04:30:13.960043: step 10920, loss = 0.22, batch loss = 0.16 (28.2 examples/sec; 0.283 sec/batch; 25h:18m:35s remains)
INFO - root - 2017-12-16 04:30:16.810833: step 10930, loss = 0.30, batch loss = 0.24 (29.4 examples/sec; 0.272 sec/batch; 24h:17m:22s remains)
INFO - root - 2017-12-16 04:30:19.646055: step 10940, loss = 0.24, batch loss = 0.19 (26.7 examples/sec; 0.300 sec/batch; 26h:45m:31s remains)
INFO - root - 2017-12-16 04:30:22.472315: step 10950, loss = 0.29, batch loss = 0.23 (28.5 examples/sec; 0.281 sec/batch; 25h:04m:53s remains)
INFO - root - 2017-12-16 04:30:25.324991: step 10960, loss = 0.25, batch loss = 0.20 (28.9 examples/sec; 0.276 sec/batch; 24h:41m:33s remains)
INFO - root - 2017-12-16 04:30:28.100937: step 10970, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.283 sec/batch; 25h:18m:56s remains)
INFO - root - 2017-12-16 04:30:30.913766: step 10980, loss = 0.22, batch loss = 0.17 (29.5 examples/sec; 0.271 sec/batch; 24h:14m:48s remains)
INFO - root - 2017-12-16 04:30:33.779858: step 10990, loss = 0.26, batch loss = 0.20 (29.3 examples/sec; 0.273 sec/batch; 24h:21m:08s remains)
INFO - root - 2017-12-16 04:30:36.597868: step 11000, loss = 0.23, batch loss = 0.17 (28.6 examples/sec; 0.280 sec/batch; 24h:58m:30s remains)
2017-12-16 04:30:37.067790: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3284516 -3.3360329 -3.4395895 -3.6744065 -3.8447375 -3.7833271 -3.7562032 -3.6669633 -3.6900027 -3.569874 -3.3438253 -3.3203404 -3.2964628 -3.2719989 -3.2792134][-2.6300492 -2.6702311 -2.8287349 -3.1828508 -3.5940545 -3.8714416 -4.1175313 -4.1160092 -4.1118541 -4.0792689 -3.871558 -3.5171969 -3.1420014 -3.0172629 -3.023258][-2.0595458 -2.0933444 -2.3153508 -2.8930962 -3.3651175 -3.6939571 -4.0156302 -4.2639003 -4.5665083 -4.4514308 -4.1803737 -3.7202158 -3.3042631 -2.9116707 -2.6011078][-1.7994635 -2.1680765 -2.3041644 -2.4090016 -2.7696342 -3.0981352 -3.2447374 -3.3476357 -3.6282268 -3.8351517 -3.9094069 -3.5924814 -3.2572212 -2.8461087 -2.502037][-1.6649947 -1.857353 -1.859601 -2.0553784 -2.2544801 -2.0052569 -1.7670989 -1.9007146 -2.2619119 -2.6573453 -2.8799534 -2.8744426 -2.823329 -2.9034481 -3.0779934][-1.4438596 -1.5137544 -1.5269852 -1.3037195 -0.90842056 -0.47421789 -0.007619381 0.045695305 -0.32438803 -0.97048068 -1.5419054 -2.0673127 -2.4627888 -2.6290326 -2.790251][-1.2637961 -1.2328079 -0.9828968 -0.494843 0.44211006 1.463449 2.460515 2.6768017 2.2131338 1.4300332 0.54341316 -0.39560509 -1.2008369 -1.9702086 -2.6759529][-1.4164968 -1.3223875 -0.98659897 -0.20740557 1.0171323 2.5518351 3.7895489 4.2174225 4.1824379 3.3637023 2.1447248 0.8487072 -0.32204962 -1.4157469 -2.3427482][-2.060473 -1.8757005 -1.5413797 -0.728317 0.43808889 1.9748902 3.3084712 3.9581509 4.0098619 3.224926 2.2426944 0.93789577 -0.427783 -1.6168923 -2.6429596][-2.7182999 -2.9703956 -3.0335803 -2.2753048 -1.1252794 0.16412592 1.3095231 2.1467667 2.4933581 1.9310856 1.072288 -0.047405243 -1.0957208 -1.9809546 -2.5975983][-3.8748066 -4.2932811 -4.4192824 -4.0906038 -3.2500248 -2.0535676 -0.79860568 0.13050175 0.491086 0.20373249 -0.41673183 -1.2586648 -1.9986992 -2.5589533 -2.8013334][-4.9809604 -5.6994276 -6.1439676 -5.8858266 -5.1095629 -4.1448007 -3.2023416 -2.182718 -1.7355447 -1.9792464 -2.4103539 -2.845407 -3.18673 -3.303992 -3.1159129][-6.1795387 -7.1125059 -7.5940342 -7.3443375 -6.7682295 -5.8317633 -4.7769313 -4.0188537 -3.7852225 -3.79952 -3.9815965 -4.1188903 -4.0520539 -3.7055395 -3.218461][-6.8924184 -7.76038 -8.1705666 -7.8016338 -7.1347089 -6.3661575 -5.5807571 -4.9240336 -4.5923486 -4.5312915 -4.6022043 -4.3202233 -4.0004396 -3.4566903 -2.8995433][-7.2534385 -7.9249563 -8.1187372 -7.655901 -6.8982716 -6.1397867 -5.4350462 -4.9940729 -4.858067 -4.7322388 -4.6357865 -4.2195611 -3.8100476 -3.273716 -2.6424134]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 04:30:39.924855: step 11010, loss = 0.35, batch loss = 0.30 (28.6 examples/sec; 0.280 sec/batch; 24h:59m:05s remains)
INFO - root - 2017-12-16 04:30:42.761883: step 11020, loss = 0.26, batch loss = 0.20 (27.8 examples/sec; 0.287 sec/batch; 25h:39m:12s remains)
INFO - root - 2017-12-16 04:30:45.550470: step 11030, loss = 0.41, batch loss = 0.36 (29.0 examples/sec; 0.276 sec/batch; 24h:37m:00s remains)
INFO - root - 2017-12-16 04:30:48.408702: step 11040, loss = 0.29, batch loss = 0.23 (26.0 examples/sec; 0.307 sec/batch; 27h:25m:22s remains)
INFO - root - 2017-12-16 04:30:51.276498: step 11050, loss = 0.23, batch loss = 0.18 (26.1 examples/sec; 0.306 sec/batch; 27h:20m:18s remains)
INFO - root - 2017-12-16 04:30:54.101522: step 11060, loss = 0.38, batch loss = 0.33 (28.2 examples/sec; 0.284 sec/batch; 25h:22m:08s remains)
INFO - root - 2017-12-16 04:30:56.907999: step 11070, loss = 0.33, batch loss = 0.27 (27.9 examples/sec; 0.286 sec/batch; 25h:34m:31s remains)
INFO - root - 2017-12-16 04:30:59.729408: step 11080, loss = 0.20, batch loss = 0.14 (29.2 examples/sec; 0.274 sec/batch; 24h:29m:02s remains)
INFO - root - 2017-12-16 04:31:02.572976: step 11090, loss = 0.27, batch loss = 0.21 (27.4 examples/sec; 0.291 sec/batch; 26h:01m:20s remains)
INFO - root - 2017-12-16 04:31:05.377604: step 11100, loss = 0.25, batch loss = 0.19 (27.8 examples/sec; 0.288 sec/batch; 25h:42m:58s remains)
2017-12-16 04:31:05.811762: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6706481 -4.7137809 -4.9078145 -5.1774044 -5.48145 -5.6237373 -5.7861242 -5.9794397 -6.2020254 -6.3378363 -6.2176657 -6.11495 -5.8295531 -5.4579883 -4.9448948][-4.9729953 -5.3277969 -5.6638904 -5.9677334 -6.2355261 -6.2631836 -6.101614 -6.1130309 -6.2330656 -6.3434391 -6.3571916 -6.3752527 -6.2398472 -6.0654249 -5.7349663][-4.9809804 -5.7648363 -6.2370906 -6.5715227 -6.55408 -6.1491833 -5.626688 -5.3009329 -5.1536751 -5.3597217 -5.60809 -6.0111742 -6.3272223 -6.4815388 -6.4485874][-4.6914921 -5.7566352 -6.3670759 -6.7098579 -6.4575782 -5.4347973 -4.229629 -3.3798795 -2.9388213 -3.1036115 -3.5833271 -4.5278578 -5.4564824 -6.2523108 -6.8160658][-4.2613134 -5.4629245 -6.1311188 -6.0984159 -5.3387136 -3.8490045 -2.1720176 -0.58620834 0.23940325 -0.042234421 -0.87335134 -2.412569 -3.9585929 -5.5309896 -6.7473855][-3.3392467 -4.6280494 -5.3308697 -5.13171 -3.8793418 -2.0659008 0.0139184 1.8525138 2.9688964 2.7780409 1.5761185 -0.42047024 -2.5209947 -4.5857749 -6.2271194][-2.4999018 -3.6107857 -4.2659864 -3.9164486 -2.4082258 -0.444901 1.7522984 3.383472 4.53409 4.5140018 3.3581581 1.2473516 -1.1063108 -3.3842502 -5.321826][-1.9586048 -3.1774812 -3.9240479 -3.5724573 -2.1402655 -0.054337978 1.9587421 3.5171876 4.610631 4.4895277 3.39296 1.3225613 -0.94919777 -3.1464286 -5.0648761][-1.9598377 -3.1366572 -3.8464122 -3.5900269 -2.5342567 -0.71814013 0.98616505 2.3238993 3.362637 3.199666 2.3373909 0.5957427 -1.4067836 -3.3051913 -5.1414943][-2.28275 -3.4598849 -4.2571921 -4.204381 -3.3858004 -2.0784347 -0.87629128 0.1095438 0.94297171 0.96567106 0.41905546 -1.0669065 -2.6686337 -4.2558994 -5.8262086][-2.726481 -3.9272156 -4.9043407 -5.1560555 -4.6904683 -3.7762067 -2.8511207 -2.1650951 -1.7067463 -1.5414207 -1.7572973 -2.8470249 -4.0245285 -5.2227473 -6.3262744][-3.687952 -4.7080956 -5.5282 -5.88409 -5.9582329 -5.4267073 -4.8405604 -4.3490539 -3.9533281 -3.8856735 -4.0447645 -4.5972424 -5.1984229 -5.9111714 -6.5599356][-4.6399364 -5.8231597 -6.6613927 -7.0286217 -7.1694927 -6.7535152 -6.2514553 -5.8116221 -5.3454537 -5.1692581 -5.2813792 -5.6845889 -6.1868596 -6.5232058 -6.6929865][-5.6246033 -6.4286294 -7.0553904 -7.5691557 -7.8247747 -7.5201769 -7.0656295 -6.55025 -6.1435127 -5.9416285 -5.9080596 -6.2127404 -6.4778767 -6.4485364 -6.3320208][-6.2927861 -6.7658873 -7.1714754 -7.6831932 -7.9538107 -7.7528582 -7.4665432 -6.9630256 -6.5672913 -6.3729067 -6.3017864 -6.3548379 -6.2223797 -5.95757 -5.5706754]]...]
INFO - root - 2017-12-16 04:31:08.657417: step 11110, loss = 0.27, batch loss = 0.21 (29.3 examples/sec; 0.273 sec/batch; 24h:23m:24s remains)
INFO - root - 2017-12-16 04:31:11.463335: step 11120, loss = 0.25, batch loss = 0.19 (28.3 examples/sec; 0.282 sec/batch; 25h:13m:01s remains)
INFO - root - 2017-12-16 04:31:14.315651: step 11130, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.285 sec/batch; 25h:24m:16s remains)
INFO - root - 2017-12-16 04:31:17.162728: step 11140, loss = 0.22, batch loss = 0.16 (28.1 examples/sec; 0.285 sec/batch; 25h:24m:11s remains)
INFO - root - 2017-12-16 04:31:19.993143: step 11150, loss = 0.32, batch loss = 0.26 (28.2 examples/sec; 0.284 sec/batch; 25h:20m:30s remains)
INFO - root - 2017-12-16 04:31:22.859550: step 11160, loss = 0.26, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 25h:31m:51s remains)
INFO - root - 2017-12-16 04:31:25.707703: step 11170, loss = 0.20, batch loss = 0.14 (27.6 examples/sec; 0.290 sec/batch; 25h:50m:36s remains)
INFO - root - 2017-12-16 04:31:28.508026: step 11180, loss = 0.25, batch loss = 0.19 (28.9 examples/sec; 0.277 sec/batch; 24h:43m:53s remains)
INFO - root - 2017-12-16 04:31:31.343896: step 11190, loss = 0.28, batch loss = 0.22 (29.4 examples/sec; 0.272 sec/batch; 24h:18m:45s remains)
INFO - root - 2017-12-16 04:31:34.206179: step 11200, loss = 0.37, batch loss = 0.32 (28.3 examples/sec; 0.283 sec/batch; 25h:15m:59s remains)
2017-12-16 04:31:34.666349: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2399859 -4.7010989 -5.0735679 -5.3224459 -5.4273629 -5.5231152 -5.6439862 -5.9074378 -6.1637478 -6.1401281 -5.9912834 -5.7886181 -5.4613223 -4.92952 -4.45435][-5.0088377 -5.6505136 -6.0939565 -6.3966341 -6.5123234 -6.7746916 -6.9655604 -7.1377592 -7.3953843 -7.7546911 -7.948205 -7.7640972 -7.3057818 -6.656085 -5.9518595][-5.7466469 -6.6048555 -7.1422067 -7.2747154 -7.17751 -7.056078 -6.9869108 -7.3260651 -7.8727322 -8.3629866 -8.8282642 -9.1842995 -9.1953468 -8.6780376 -7.8524485][-6.0713863 -6.9283838 -7.3431568 -7.2771325 -6.7952976 -6.1635513 -5.6879339 -5.5818415 -5.9891524 -7.0521054 -8.3600979 -9.3689709 -9.852972 -9.7896442 -9.2826052][-6.1088176 -6.7705069 -6.8177938 -6.2475748 -5.1508551 -3.7341132 -2.5124176 -1.9342749 -2.3963997 -3.7399225 -5.635766 -7.7740836 -9.5461884 -10.284495 -10.03459][-5.5941038 -5.8767109 -5.5285258 -4.4646945 -2.6706595 -0.55850077 1.4694314 2.7290244 2.6053157 0.85494423 -1.8628068 -4.86617 -7.3234921 -8.8766861 -9.4688425][-4.8228149 -4.8814616 -4.4033723 -3.1005414 -0.95073223 1.7472782 4.4600086 6.1207466 6.2954826 4.8989582 2.1338596 -1.4770234 -4.8788757 -7.061975 -7.9218273][-4.1605053 -4.0729537 -3.5871737 -2.2998841 -0.18113279 2.5146351 5.3958616 7.3778439 8.007267 6.7121058 3.9185896 0.56831455 -2.5127912 -4.9842677 -6.4264364][-3.6386633 -3.6276257 -3.3343847 -2.2902591 -0.45808005 2.0275435 4.6909323 6.476326 7.2468214 6.4265375 4.1646795 1.0683789 -1.9461505 -4.1308942 -5.3611956][-3.8375449 -3.9946451 -4.0793233 -3.5314839 -2.3548439 -0.48818231 1.6814842 3.4705272 4.309906 3.7019405 2.0793047 -0.1208334 -2.3642356 -4.3634996 -5.5642004][-4.641036 -4.9544697 -5.1790495 -5.0254927 -4.4099689 -3.3902473 -2.0690868 -0.9186492 -0.16704655 -0.23609495 -1.1037729 -2.7021937 -4.1984658 -5.3833752 -6.0809841][-5.1667514 -5.5496664 -5.9782014 -6.3146749 -6.4002752 -6.0718627 -5.3274975 -4.7552638 -4.4104509 -4.4183717 -4.7039366 -5.46944 -6.2411051 -6.9387512 -7.2222323][-5.7589278 -6.2754297 -6.8607483 -7.2221313 -7.5398188 -7.7643647 -7.6725464 -7.2630911 -6.7200923 -6.5082812 -6.4243765 -6.7416587 -7.0635653 -7.47567 -7.5798178][-5.4560046 -5.8329096 -6.3133912 -6.8171082 -7.36926 -7.572917 -7.6507564 -7.670167 -7.4885321 -7.0582733 -6.652029 -6.7443113 -6.9411669 -7.1000977 -7.09227][-5.2428055 -5.437459 -5.6397581 -6.0695591 -6.5477705 -6.8318172 -6.9991312 -6.8456035 -6.5551481 -6.3170753 -6.0700459 -5.8763175 -5.8150425 -6.0761089 -6.1358876]]...]
INFO - root - 2017-12-16 04:31:37.477336: step 11210, loss = 0.30, batch loss = 0.25 (28.9 examples/sec; 0.277 sec/batch; 24h:43m:06s remains)
INFO - root - 2017-12-16 04:31:40.323165: step 11220, loss = 0.28, batch loss = 0.22 (27.5 examples/sec; 0.291 sec/batch; 25h:57m:21s remains)
INFO - root - 2017-12-16 04:31:43.123254: step 11230, loss = 0.32, batch loss = 0.26 (29.2 examples/sec; 0.274 sec/batch; 24h:29m:06s remains)
INFO - root - 2017-12-16 04:31:45.978146: step 11240, loss = 0.38, batch loss = 0.32 (29.0 examples/sec; 0.275 sec/batch; 24h:34m:49s remains)
INFO - root - 2017-12-16 04:31:48.832590: step 11250, loss = 0.24, batch loss = 0.18 (27.1 examples/sec; 0.295 sec/batch; 26h:18m:51s remains)
INFO - root - 2017-12-16 04:31:51.722450: step 11260, loss = 0.21, batch loss = 0.15 (27.9 examples/sec; 0.287 sec/batch; 25h:34m:04s remains)
INFO - root - 2017-12-16 04:31:54.584345: step 11270, loss = 0.51, batch loss = 0.45 (28.5 examples/sec; 0.281 sec/batch; 25h:03m:52s remains)
INFO - root - 2017-12-16 04:31:57.412811: step 11280, loss = 0.20, batch loss = 0.15 (29.2 examples/sec; 0.274 sec/batch; 24h:27m:08s remains)
INFO - root - 2017-12-16 04:32:00.228600: step 11290, loss = 0.31, batch loss = 0.25 (27.9 examples/sec; 0.287 sec/batch; 25h:34m:37s remains)
INFO - root - 2017-12-16 04:32:03.049981: step 11300, loss = 0.41, batch loss = 0.35 (28.9 examples/sec; 0.277 sec/batch; 24h:40m:23s remains)
2017-12-16 04:32:03.495120: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3625402 -3.4143586 -3.2689338 -3.0461402 -2.8189404 -2.9415374 -3.2475758 -4.0419631 -4.9393826 -5.8223085 -6.53944 -6.9061604 -7.0183392 -6.5451522 -5.835341][-2.3635545 -2.4563468 -2.2249775 -2.0521452 -1.9521453 -2.2067661 -2.6447186 -3.4880948 -4.2185717 -5.1687069 -6.136219 -6.5349636 -6.7181582 -6.5556321 -6.1033988][-1.7223921 -1.6795206 -1.3893743 -1.1855667 -1.0821581 -1.2711077 -1.5774338 -2.497757 -3.4459767 -4.2565951 -4.9543977 -5.5787468 -6.1102 -5.8926349 -5.3833227][-1.2046516 -1.0055261 -0.42245913 0.0031685829 0.29343033 0.042418957 -0.50431848 -1.3427517 -1.9645131 -2.7101259 -3.522666 -4.0793247 -4.3496823 -4.3925381 -4.2996216][-1.2528353 -0.70391512 0.020445347 0.49395895 0.8166151 0.75666189 0.32941961 -0.35789442 -0.94825768 -1.4526694 -1.7344661 -2.1183186 -2.4690075 -2.5394201 -2.4342575][-1.8621078 -1.1197419 -0.23581886 0.44339943 1.0259509 1.0284543 0.77016354 0.30907536 -0.004928112 -0.16633892 -0.24429607 -0.41794538 -0.52031279 -0.651525 -0.85633421][-2.6839533 -1.6501794 -0.46931577 0.45183277 1.1729012 1.2309604 1.0591698 0.71952534 0.53666496 0.50215912 0.5823245 0.59869814 0.55305147 0.30785561 0.10297918][-3.4520884 -2.320189 -1.2290037 -0.18152046 0.68409491 1.0174932 1.0630684 0.76629353 0.62227154 0.67870331 0.89652872 1.0492177 1.0864015 0.94154072 0.71175861][-4.5021491 -3.4132662 -2.2729447 -1.247952 -0.42329311 -0.052639008 0.1611166 0.087506771 0.10541487 0.1712842 0.3421092 0.56251431 0.7064991 0.72349691 0.6208868][-5.1586008 -4.2033358 -3.275876 -2.3581388 -1.5865459 -1.2131131 -1.0214694 -1.0067849 -0.91850853 -0.8337605 -0.70611215 -0.538244 -0.4233861 -0.38662434 -0.33061886][-5.4515886 -4.8418856 -4.2329378 -3.5223022 -2.8169556 -2.4253237 -2.2507899 -2.283591 -2.2972302 -2.3695326 -2.3470819 -2.3411088 -2.3194947 -2.2126906 -1.906424][-5.6658711 -5.4402418 -5.217041 -4.7716002 -4.2799911 -3.855597 -3.5233078 -3.5028744 -3.590549 -3.7368608 -3.8539193 -3.9885502 -4.1393437 -4.0746441 -3.5362804][-5.3300095 -5.3840704 -5.3548574 -5.1552467 -4.94172 -4.6868486 -4.4898438 -4.3704648 -4.3149333 -4.6162109 -5.0163684 -5.2334118 -5.3531647 -5.5351 -5.0701385][-4.2438045 -4.3950686 -4.6596518 -4.6025105 -4.4821968 -4.3435416 -4.3559027 -4.51674 -4.84123 -5.1907372 -5.5300512 -5.9331303 -6.29938 -6.5189018 -6.0740528][-3.1649108 -3.2298241 -3.5167897 -3.7119837 -3.9167445 -3.9253988 -4.0083113 -4.0981388 -4.3420587 -4.8856969 -5.5785646 -6.0645795 -6.3466291 -6.6820121 -6.3423352]]...]
INFO - root - 2017-12-16 04:32:06.292298: step 11310, loss = 0.21, batch loss = 0.16 (28.3 examples/sec; 0.283 sec/batch; 25h:14m:46s remains)
INFO - root - 2017-12-16 04:32:09.128843: step 11320, loss = 0.28, batch loss = 0.22 (28.2 examples/sec; 0.284 sec/batch; 25h:17m:43s remains)
INFO - root - 2017-12-16 04:32:11.946849: step 11330, loss = 0.22, batch loss = 0.17 (26.1 examples/sec; 0.307 sec/batch; 27h:23m:45s remains)
INFO - root - 2017-12-16 04:32:14.753720: step 11340, loss = 0.24, batch loss = 0.19 (28.4 examples/sec; 0.281 sec/batch; 25h:05m:48s remains)
INFO - root - 2017-12-16 04:32:17.574014: step 11350, loss = 0.24, batch loss = 0.18 (26.6 examples/sec; 0.301 sec/batch; 26h:52m:19s remains)
INFO - root - 2017-12-16 04:32:20.432509: step 11360, loss = 0.31, batch loss = 0.25 (27.8 examples/sec; 0.288 sec/batch; 25h:41m:42s remains)
INFO - root - 2017-12-16 04:32:23.224948: step 11370, loss = 0.23, batch loss = 0.17 (29.0 examples/sec; 0.276 sec/batch; 24h:34m:49s remains)
INFO - root - 2017-12-16 04:32:26.078981: step 11380, loss = 0.25, batch loss = 0.19 (29.3 examples/sec; 0.273 sec/batch; 24h:21m:32s remains)
INFO - root - 2017-12-16 04:32:28.896941: step 11390, loss = 0.23, batch loss = 0.17 (29.5 examples/sec; 0.271 sec/batch; 24h:12m:56s remains)
INFO - root - 2017-12-16 04:32:31.684740: step 11400, loss = 0.34, batch loss = 0.28 (27.6 examples/sec; 0.290 sec/batch; 25h:50m:32s remains)
2017-12-16 04:32:32.138533: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7484016 -2.7459507 -2.7348342 -2.7498989 -2.7523282 -2.7901416 -2.834475 -2.7447729 -2.5905194 -2.3249602 -2.1243672 -2.1017606 -2.0122552 -1.9811223 -1.9828317][-2.6652493 -2.6456063 -2.6417718 -2.7601192 -2.8463593 -2.8557491 -2.8188257 -2.6922913 -2.5862691 -2.3774188 -2.1784153 -2.0990734 -1.989547 -2.0175829 -1.954567][-2.4926448 -2.4857101 -2.4910798 -2.4451096 -2.3425882 -2.3668664 -2.3884981 -2.4150584 -2.4905314 -2.4620953 -2.490283 -2.4938617 -2.3870332 -2.3332193 -2.1878057][-2.4686484 -2.3474622 -2.1300123 -1.969476 -1.7857423 -1.645926 -1.5623758 -1.6929076 -1.8855903 -2.1328709 -2.4071128 -2.6334305 -2.8162205 -2.8495626 -2.6966691][-2.0969841 -1.7923336 -1.4022014 -1.1874785 -0.86406875 -0.59266853 -0.44305205 -0.5030036 -0.7806375 -1.25595 -1.7887502 -2.3847094 -2.7929683 -3.0226755 -3.1118326][-1.7256174 -1.3284974 -0.77039313 -0.24246931 0.39792633 0.81238985 1.0570807 1.0766163 0.77138472 0.15482807 -0.59637618 -1.5840929 -2.3876305 -2.894773 -3.0599971][-1.4603422 -0.92163634 -0.23842049 0.4156146 1.1285338 1.7480984 2.3220711 2.444212 2.1254783 1.4579844 0.50418949 -0.63448119 -1.5926185 -2.3207996 -2.7728596][-1.3197956 -0.78689432 -0.22073889 0.44558191 1.256382 1.8351083 2.3132129 2.4700789 2.2533746 1.5574856 0.60486317 -0.46258163 -1.4588242 -2.222882 -2.6716173][-1.5660474 -1.1261685 -0.75009775 -0.51153135 -0.14714527 0.50138521 1.1686807 1.3469296 1.167192 0.54866695 -0.30854559 -1.41572 -2.2822237 -2.70466 -2.8804047][-1.6658771 -1.6520486 -1.7810254 -1.7418952 -1.5349157 -1.3390825 -1.138484 -0.98589563 -0.93451524 -1.3554552 -2.0604525 -2.9236803 -3.536921 -3.7178478 -3.5609531][-2.1197484 -1.9896872 -2.2570112 -2.8187547 -3.2137179 -3.1154261 -2.8615961 -3.0072088 -3.2016735 -3.6362903 -4.1942205 -4.7475772 -5.0833097 -5.0043983 -4.6242752][-2.3125076 -2.4747295 -3.112998 -3.6927304 -4.2084212 -4.6925397 -4.9852662 -5.0474687 -5.0893307 -5.2443128 -5.5204735 -6.104424 -6.4677086 -6.2663174 -5.7349305][-2.3117788 -2.3615236 -2.993598 -3.8767788 -4.7558541 -5.3970928 -5.7141166 -5.9247513 -6.0513492 -6.1630383 -6.3187876 -6.5222311 -6.64097 -6.5037894 -6.092833][-2.3276329 -2.3418174 -2.8041041 -3.4906411 -4.2818217 -4.8437519 -5.1132336 -5.4738913 -5.6862907 -5.5539927 -5.4700708 -5.8346024 -6.2362638 -6.2658868 -5.9979095][-2.2187793 -2.0179832 -2.1739292 -2.7998722 -3.529582 -4.2156167 -4.668644 -4.6885352 -4.5237203 -4.6104493 -4.7936368 -4.8342896 -4.9628277 -5.2381124 -5.283854]]...]
INFO - root - 2017-12-16 04:32:34.937152: step 11410, loss = 0.48, batch loss = 0.42 (27.3 examples/sec; 0.293 sec/batch; 26h:06m:10s remains)
INFO - root - 2017-12-16 04:32:37.759576: step 11420, loss = 0.30, batch loss = 0.25 (28.8 examples/sec; 0.277 sec/batch; 24h:44m:33s remains)
INFO - root - 2017-12-16 04:32:40.589694: step 11430, loss = 0.25, batch loss = 0.19 (28.5 examples/sec; 0.281 sec/batch; 25h:02m:10s remains)
INFO - root - 2017-12-16 04:32:43.468235: step 11440, loss = 0.28, batch loss = 0.22 (28.0 examples/sec; 0.285 sec/batch; 25h:26m:42s remains)
INFO - root - 2017-12-16 04:32:46.260447: step 11450, loss = 0.24, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 24h:25m:24s remains)
INFO - root - 2017-12-16 04:32:49.049971: step 11460, loss = 0.24, batch loss = 0.18 (29.3 examples/sec; 0.273 sec/batch; 24h:19m:19s remains)
INFO - root - 2017-12-16 04:32:51.918010: step 11470, loss = 0.32, batch loss = 0.26 (28.1 examples/sec; 0.285 sec/batch; 25h:25m:27s remains)
INFO - root - 2017-12-16 04:32:54.731469: step 11480, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.285 sec/batch; 25h:26m:49s remains)
INFO - root - 2017-12-16 04:32:57.568195: step 11490, loss = 0.38, batch loss = 0.32 (29.3 examples/sec; 0.273 sec/batch; 24h:21m:11s remains)
INFO - root - 2017-12-16 04:33:00.364776: step 11500, loss = 0.21, batch loss = 0.15 (28.9 examples/sec; 0.277 sec/batch; 24h:42m:20s remains)
2017-12-16 04:33:00.835967: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5872359 -4.5840268 -4.8374767 -5.5770111 -6.3147616 -6.7437677 -7.067111 -7.1676521 -6.9937754 -6.3883953 -5.6641517 -4.872551 -3.928411 -3.1534462 -2.6362579][-3.9679058 -3.9618247 -4.3505597 -4.9218712 -5.3391571 -5.4990516 -5.7245488 -5.8716021 -5.8760319 -5.5447569 -5.0556684 -4.3821182 -3.5540059 -2.6838331 -2.0747821][-3.1412282 -3.1354949 -3.4032407 -3.9438176 -4.2595387 -4.3287735 -4.5696425 -4.524229 -4.5364218 -4.7048621 -4.7578392 -4.3293991 -3.5383477 -2.7755961 -2.0798576][-2.2030387 -2.12385 -2.3248992 -2.4987288 -2.4748287 -2.4195733 -2.4910994 -2.7982903 -3.2380562 -3.5975058 -3.9396505 -4.1759677 -3.9520092 -3.1843672 -2.3075736][-1.2273016 -1.1719303 -1.4482846 -1.5188615 -1.3872402 -1.0875726 -0.91214609 -0.924072 -1.2391267 -2.2716527 -3.3395565 -3.8416288 -3.8010654 -3.5926569 -2.9206367][-1.0717795 -0.61058807 -0.50235152 -0.22184086 0.3949523 0.92434072 1.2667398 1.1332941 0.35990906 -0.86474729 -2.1842353 -3.3966479 -3.9863691 -3.7952287 -3.0301409][-1.2536061 -0.42557144 -0.046816349 0.57324266 1.4344969 2.3155203 3.1114497 3.0542536 2.2896266 0.50475025 -1.4068763 -2.8908048 -3.7554719 -3.9214659 -3.4252687][-1.5017495 -0.73539686 -0.4006114 0.41749 1.4048176 2.256259 2.9420247 2.9543691 2.3776302 0.73473215 -1.1683912 -2.8083987 -3.8129835 -4.218 -3.8958161][-2.3877926 -1.5120585 -1.1655099 -0.62208748 0.13649988 1.1782498 1.9873862 2.0073018 1.4189873 -0.11096907 -1.8126512 -3.2412 -4.1044569 -4.435246 -4.1900315][-3.0962133 -2.6541977 -2.7603536 -2.3740115 -1.703963 -1.0340447 -0.46330237 -0.20346689 -0.37701702 -1.3962541 -2.6940594 -3.8923414 -4.5900192 -4.7198033 -4.44114][-4.9232779 -4.1705704 -3.9498994 -4.2064867 -4.2767577 -3.8872962 -3.5766132 -3.458369 -3.3928757 -3.8451562 -4.51456 -5.118556 -5.4511538 -5.4122453 -5.1789613][-5.7186813 -5.5954046 -5.940546 -5.96318 -5.8104706 -5.9427648 -6.1166859 -5.9828634 -5.7819195 -5.8685074 -5.9916072 -6.2426419 -6.3294415 -6.1418304 -5.7195168][-6.1646857 -6.1159472 -6.4105053 -6.94318 -7.3485513 -7.32271 -7.3431034 -7.3232946 -7.1567459 -6.9571943 -6.7010713 -6.5342374 -6.3556809 -6.1582584 -5.7436843][-5.6453247 -5.5135741 -5.6596651 -6.0284581 -6.4625473 -6.9232793 -7.3745785 -7.3166103 -7.0847759 -6.7625456 -6.3567019 -6.1176262 -5.9509859 -5.7468624 -5.5134854][-5.2207823 -5.0200758 -5.0146823 -5.1667142 -5.4221296 -5.8285608 -6.223218 -6.2067413 -5.9873452 -5.8118863 -5.6402049 -5.3901129 -5.179328 -5.1524673 -5.0804672]]...]
INFO - root - 2017-12-16 04:33:03.676284: step 11510, loss = 0.27, batch loss = 0.22 (27.7 examples/sec; 0.288 sec/batch; 25h:42m:50s remains)
INFO - root - 2017-12-16 04:33:06.536227: step 11520, loss = 0.21, batch loss = 0.15 (28.8 examples/sec; 0.278 sec/batch; 24h:45m:03s remains)
INFO - root - 2017-12-16 04:33:09.394161: step 11530, loss = 0.35, batch loss = 0.29 (28.8 examples/sec; 0.278 sec/batch; 24h:48m:32s remains)
INFO - root - 2017-12-16 04:33:12.213940: step 11540, loss = 0.23, batch loss = 0.17 (27.6 examples/sec; 0.290 sec/batch; 25h:51m:06s remains)
INFO - root - 2017-12-16 04:33:15.056749: step 11550, loss = 0.30, batch loss = 0.25 (27.5 examples/sec; 0.291 sec/batch; 25h:54m:22s remains)
INFO - root - 2017-12-16 04:33:17.915310: step 11560, loss = 0.21, batch loss = 0.15 (27.6 examples/sec; 0.290 sec/batch; 25h:50m:10s remains)
INFO - root - 2017-12-16 04:33:20.740323: step 11570, loss = 0.28, batch loss = 0.22 (26.5 examples/sec; 0.302 sec/batch; 26h:55m:20s remains)
INFO - root - 2017-12-16 04:33:23.576386: step 11580, loss = 0.26, batch loss = 0.20 (27.0 examples/sec; 0.297 sec/batch; 26h:26m:35s remains)
INFO - root - 2017-12-16 04:33:26.415468: step 11590, loss = 0.27, batch loss = 0.21 (28.6 examples/sec; 0.279 sec/batch; 24h:54m:28s remains)
INFO - root - 2017-12-16 04:33:29.205036: step 11600, loss = 0.24, batch loss = 0.18 (28.5 examples/sec; 0.281 sec/batch; 25h:00m:46s remains)
2017-12-16 04:33:29.655009: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7355266 -4.3522797 -4.0574188 -3.562993 -3.3324981 -3.0038736 -2.9561224 -2.9804304 -3.0242682 -3.3633671 -3.7863891 -4.3652754 -4.5868759 -4.695096 -4.527781][-5.6734867 -5.0641685 -4.6056442 -4.1152415 -3.9043055 -3.5854349 -3.5147357 -3.5575974 -3.5498424 -3.8713062 -4.1745324 -4.5938721 -4.7305741 -4.6941996 -4.5186844][-5.4031849 -4.869194 -4.529891 -4.0368876 -3.8936331 -3.7153752 -3.6556544 -3.7124021 -3.8684978 -4.1517048 -4.4655833 -4.9793663 -5.2305293 -5.2536287 -4.9671893][-4.48508 -3.849901 -3.5855751 -3.4013681 -3.4193492 -3.3272595 -3.3729644 -3.4328475 -3.6238904 -3.9666932 -4.34534 -4.9403243 -5.2839603 -5.442153 -5.3012362][-2.8033972 -2.2457843 -2.1319261 -2.1982834 -2.2783382 -2.2055736 -2.1967635 -2.3911331 -2.6330538 -3.1222763 -3.7690489 -4.6038165 -5.1690311 -5.5896945 -5.4610572][-0.43753767 -0.036510944 -0.35556173 -0.69117641 -0.89045405 -0.92888737 -0.83995438 -0.89491177 -1.0147393 -1.5412292 -2.400058 -3.6556106 -4.786571 -5.4557805 -5.4969654][1.4029689 1.6567364 1.0377483 0.2795558 -0.094554424 0.10755253 0.538404 0.77804375 0.796453 0.21163702 -0.851588 -2.3523953 -3.8224733 -4.8157063 -4.9996767][1.5624256 1.6462207 1.0346456 0.495255 0.44615889 0.94422054 1.5289211 1.9965997 2.166326 1.5353446 0.31321669 -1.4720452 -3.2342944 -4.3808751 -4.6809697][0.14717293 0.48637342 0.023799419 -0.38162804 -0.11889505 0.73322248 1.6462617 2.4448533 2.7989049 2.334691 1.120502 -0.80061007 -2.6986423 -4.0202436 -4.4538674][-1.8316619 -1.6333885 -1.8564641 -1.9700706 -1.4038827 -0.22296906 0.83306217 1.8692608 2.389493 1.972549 0.80815315 -0.90345192 -2.6753788 -4.0943241 -4.7802467][-3.679908 -3.7161994 -3.8412414 -3.7776198 -3.2787814 -1.7650008 -0.39372683 0.65859747 1.2027802 0.93454933 -0.044566631 -1.6232255 -3.276742 -4.6248541 -5.3452215][-5.3844519 -5.5433307 -5.6588631 -5.4850855 -4.9023604 -3.4508476 -2.0815125 -0.88955092 -0.34263849 -0.47092533 -1.2422194 -2.5843363 -3.9356732 -5.0910978 -5.6570883][-6.3195806 -6.5605874 -6.8389025 -6.9302177 -6.6681623 -5.6599903 -4.4949327 -3.2707162 -2.6579008 -2.6834764 -3.1251173 -4.0631304 -5.0310087 -5.8165755 -6.0634003][-6.2662673 -6.6006622 -7.0442047 -7.42669 -7.4348903 -6.7957048 -5.7765326 -4.6918321 -4.10706 -3.951436 -4.2367525 -4.8835249 -5.5871687 -6.1242394 -6.0768566][-6.0619216 -6.525528 -7.0380206 -7.4120688 -7.5484591 -7.1691904 -6.3158617 -5.0591483 -4.2340336 -4.1333303 -4.3367782 -4.6462083 -5.1110969 -5.5560837 -5.6580291]]...]
INFO - root - 2017-12-16 04:33:32.487511: step 11610, loss = 0.22, batch loss = 0.17 (29.8 examples/sec; 0.268 sec/batch; 23h:55m:36s remains)
INFO - root - 2017-12-16 04:33:35.293486: step 11620, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 25h:06m:07s remains)
INFO - root - 2017-12-16 04:33:38.048299: step 11630, loss = 0.22, batch loss = 0.17 (27.9 examples/sec; 0.287 sec/batch; 25h:32m:30s remains)
INFO - root - 2017-12-16 04:33:40.846971: step 11640, loss = 0.26, batch loss = 0.20 (29.0 examples/sec; 0.276 sec/batch; 24h:36m:00s remains)
INFO - root - 2017-12-16 04:33:43.671590: step 11650, loss = 0.18, batch loss = 0.12 (28.6 examples/sec; 0.280 sec/batch; 24h:57m:10s remains)
INFO - root - 2017-12-16 04:33:46.480882: step 11660, loss = 0.27, batch loss = 0.21 (29.2 examples/sec; 0.274 sec/batch; 24h:24m:53s remains)
INFO - root - 2017-12-16 04:33:49.326830: step 11670, loss = 0.29, batch loss = 0.23 (29.4 examples/sec; 0.272 sec/batch; 24h:14m:33s remains)
INFO - root - 2017-12-16 04:33:52.184552: step 11680, loss = 0.28, batch loss = 0.23 (28.3 examples/sec; 0.283 sec/batch; 25h:11m:13s remains)
INFO - root - 2017-12-16 04:33:55.028348: step 11690, loss = 0.29, batch loss = 0.23 (28.9 examples/sec; 0.277 sec/batch; 24h:40m:30s remains)
INFO - root - 2017-12-16 04:33:57.869493: step 11700, loss = 0.30, batch loss = 0.24 (27.4 examples/sec; 0.292 sec/batch; 26h:01m:21s remains)
2017-12-16 04:33:58.317825: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3193874 -3.5588596 -3.8891032 -4.2263741 -4.4546351 -4.2537546 -3.9440806 -3.6632357 -3.4002733 -3.0789714 -2.9216886 -3.1143575 -3.3264329 -3.3211005 -3.2494171][-2.7210746 -2.9898853 -3.3144281 -3.6427832 -3.8616457 -3.8032389 -3.6347857 -3.2928107 -2.9512389 -2.7083349 -2.6290026 -2.6604309 -2.7551775 -2.7086756 -2.4745021][-1.8968742 -2.2447631 -2.5925217 -2.9652815 -3.1554284 -3.127327 -2.9193377 -2.7475743 -2.5511413 -2.3613818 -2.3155911 -2.2742586 -2.3407032 -2.253237 -2.0149322][-0.98876739 -1.3124757 -1.6502874 -1.9828846 -2.0692596 -2.0310178 -1.8313167 -1.7252128 -1.680975 -1.6585162 -1.6866527 -1.6790266 -1.7337265 -1.7111666 -1.6335938][-0.13603735 -0.42532873 -0.62108517 -0.75010276 -0.740921 -0.64154744 -0.55237746 -0.5803144 -0.66808081 -0.87028527 -1.072942 -1.0992038 -1.1682281 -1.2233543 -1.2813804][-0.28883219 -0.504045 -0.54555154 -0.44760513 -0.078183174 0.25500917 0.34845924 0.36866426 0.28435993 0.015187263 -0.33307743 -0.555846 -0.6793139 -0.85731077 -1.0488899][-0.87664986 -1.0922503 -1.1149666 -0.89566922 -0.30542803 0.28633976 0.70335436 0.8288641 0.69047785 0.50158882 0.083735943 -0.18137693 -0.41952896 -0.56864667 -0.79581833][-1.7633677 -2.0710008 -2.1055696 -1.8274329 -1.1797791 -0.5629735 -0.057287693 0.4663167 0.6101532 0.57283354 0.31154442 0.0907197 -0.16244888 -0.29879284 -0.40363455][-2.4581609 -3.0296888 -3.1720464 -2.9804339 -2.45443 -1.6550045 -0.96434045 -0.58338523 -0.39159775 -0.22321606 -0.27338505 -0.23065758 -0.30828094 -0.32896566 -0.35091066][-3.2610798 -3.9138565 -4.2991738 -4.2427025 -3.7510877 -3.115685 -2.4539847 -1.7405198 -1.3419569 -1.1726251 -1.0033164 -1.0281584 -1.0361462 -0.81341553 -0.72301912][-3.7411749 -4.401237 -4.8217087 -4.9235039 -4.83371 -4.31016 -3.6261172 -3.1062241 -2.6295707 -2.2885194 -1.9920409 -1.7850378 -1.6537707 -1.3340378 -1.1401999][-4.2060928 -4.5697751 -4.7984877 -5.0234332 -5.0477543 -4.6846266 -4.337904 -3.9353876 -3.5926113 -3.3825583 -3.0232372 -2.672091 -2.3543277 -1.9022131 -1.4817319][-4.6352105 -4.6514072 -4.7085061 -4.6399646 -4.6119809 -4.37842 -4.072969 -3.9594932 -3.843586 -3.9368372 -3.8213081 -3.3995767 -2.731854 -2.0263171 -1.5547612][-4.9293842 -4.4803782 -4.1452432 -4.011651 -4.00987 -3.78653 -3.6844935 -3.7072539 -3.6985381 -3.9309068 -3.8988888 -3.6044648 -2.7544613 -1.8163354 -1.2121103][-4.7881136 -4.063765 -3.5329518 -3.1882412 -3.1015811 -3.0714383 -3.2160149 -3.4897132 -3.7407212 -4.0542502 -4.0082 -3.4792879 -2.4421949 -1.2114019 -0.46433711]]...]
INFO - root - 2017-12-16 04:34:01.122862: step 11710, loss = 0.23, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 24h:24m:36s remains)
INFO - root - 2017-12-16 04:34:03.949074: step 11720, loss = 0.28, batch loss = 0.22 (28.3 examples/sec; 0.282 sec/batch; 25h:08m:52s remains)
INFO - root - 2017-12-16 04:34:06.787814: step 11730, loss = 0.37, batch loss = 0.31 (28.3 examples/sec; 0.283 sec/batch; 25h:12m:29s remains)
INFO - root - 2017-12-16 04:34:09.673077: step 11740, loss = 0.22, batch loss = 0.16 (28.3 examples/sec; 0.283 sec/batch; 25h:13m:39s remains)
INFO - root - 2017-12-16 04:34:12.519709: step 11750, loss = 0.27, batch loss = 0.21 (27.3 examples/sec; 0.293 sec/batch; 26h:06m:54s remains)
INFO - root - 2017-12-16 04:34:15.348116: step 11760, loss = 0.39, batch loss = 0.33 (26.9 examples/sec; 0.298 sec/batch; 26h:30m:33s remains)
INFO - root - 2017-12-16 04:34:18.223577: step 11770, loss = 0.27, batch loss = 0.21 (26.9 examples/sec; 0.297 sec/batch; 26h:29m:51s remains)
INFO - root - 2017-12-16 04:34:21.017840: step 11780, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 25h:10m:42s remains)
INFO - root - 2017-12-16 04:34:23.905933: step 11790, loss = 0.31, batch loss = 0.25 (26.6 examples/sec; 0.301 sec/batch; 26h:48m:50s remains)
INFO - root - 2017-12-16 04:34:26.767096: step 11800, loss = 0.25, batch loss = 0.20 (29.3 examples/sec; 0.273 sec/batch; 24h:19m:42s remains)
2017-12-16 04:34:27.213347: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5206673 -2.5500174 -2.6898813 -2.7785535 -2.8210049 -2.7371917 -2.729265 -2.8232145 -2.8227682 -2.6926918 -2.5087664 -2.1736751 -1.6508069 -1.3868418 -1.4609852][-1.8686717 -1.9830971 -2.2745237 -2.5744705 -2.5881646 -2.5949976 -2.6076899 -2.7582264 -2.836782 -2.6966696 -2.5697925 -2.2628946 -2.0422559 -1.6520464 -1.248688][-1.6923783 -1.9078836 -2.0549059 -2.2048793 -2.1626418 -2.0414598 -1.9567156 -2.1807594 -2.452579 -2.6781287 -2.9004004 -2.8691134 -2.6755433 -2.1225731 -1.6231143][-1.7442839 -2.1109672 -2.444814 -2.4939809 -2.2570891 -1.7456334 -1.2797546 -1.4039547 -1.7284114 -2.2266848 -2.7612023 -3.2885971 -3.382551 -2.8388846 -2.0083718][-2.0981648 -2.3961089 -2.5070934 -2.3506567 -1.804574 -1.0210936 -0.43683457 -0.43853474 -0.891232 -1.849385 -2.8717718 -3.5206242 -3.652365 -3.4047332 -2.852088][-2.3349564 -2.5493522 -2.4683371 -1.8180642 -0.65143824 0.46563053 1.1722612 1.2120352 0.51929855 -0.79022813 -2.3282259 -3.5526865 -4.2211618 -4.1595864 -3.4931765][-2.7103667 -2.6083593 -2.1587834 -1.1834078 0.22478104 1.7216458 2.804163 2.7397442 1.8215408 0.3958993 -1.2334704 -2.8514037 -3.9371004 -4.4511929 -4.220768][-2.898967 -2.6015573 -1.9384613 -0.67023611 0.88500977 2.4035339 3.3284912 3.25558 2.5003433 1.0039248 -0.86025953 -2.5331168 -3.61068 -4.3112125 -4.4314351][-3.1318288 -2.6158361 -1.799408 -0.5726676 0.79496 2.0893583 2.8896766 2.7698574 1.9835291 0.57693386 -1.1206629 -2.7350411 -3.8864763 -4.636312 -4.8304968][-3.4386139 -3.03844 -2.3661849 -1.3581159 -0.20105791 0.725122 1.25249 1.0689273 0.31373262 -0.64679122 -1.9073336 -3.3650331 -4.5314221 -5.1911635 -5.2676039][-3.7928338 -3.4186687 -2.8737116 -2.1498549 -1.487056 -0.93121839 -0.6550808 -0.90973377 -1.5391328 -2.4027088 -3.3146 -4.1426883 -4.7878103 -5.2211456 -5.2878714][-3.4520352 -3.2805064 -3.1674962 -2.7301359 -2.2048175 -1.86025 -1.8119347 -2.212404 -2.6228926 -3.1074834 -3.7940874 -4.4409904 -4.8631582 -5.1259165 -5.2066727][-3.1945043 -3.1945679 -3.12437 -2.9515285 -2.7047694 -2.520087 -2.4269893 -2.5896182 -2.9969106 -3.3849304 -3.7305124 -4.0452328 -4.30326 -4.6332536 -4.8871965][-3.1941941 -3.3396182 -3.2742829 -3.0549669 -2.7337594 -2.4104621 -2.3347902 -2.4081945 -2.535615 -2.6426816 -2.889782 -3.1971884 -3.451154 -3.6212564 -3.8259304][-2.7996135 -2.9883649 -3.0069413 -2.7069235 -2.1906228 -1.6737747 -1.4247229 -1.4112651 -1.4492478 -1.4875882 -1.6872792 -1.8827438 -2.1806135 -2.546139 -2.7851887]]...]
INFO - root - 2017-12-16 04:34:30.024246: step 11810, loss = 0.27, batch loss = 0.21 (29.2 examples/sec; 0.274 sec/batch; 24h:22m:41s remains)
INFO - root - 2017-12-16 04:34:32.828673: step 11820, loss = 0.35, batch loss = 0.29 (28.1 examples/sec; 0.285 sec/batch; 25h:20m:36s remains)
INFO - root - 2017-12-16 04:34:35.668288: step 11830, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.277 sec/batch; 24h:39m:19s remains)
INFO - root - 2017-12-16 04:34:38.478708: step 11840, loss = 0.25, batch loss = 0.19 (27.7 examples/sec; 0.288 sec/batch; 25h:41m:32s remains)
INFO - root - 2017-12-16 04:34:41.337056: step 11850, loss = 0.30, batch loss = 0.24 (28.9 examples/sec; 0.277 sec/batch; 24h:41m:08s remains)
INFO - root - 2017-12-16 04:34:44.133286: step 11860, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.285 sec/batch; 25h:22m:55s remains)
INFO - root - 2017-12-16 04:34:46.937950: step 11870, loss = 0.24, batch loss = 0.18 (29.0 examples/sec; 0.275 sec/batch; 24h:31m:51s remains)
INFO - root - 2017-12-16 04:34:49.747135: step 11880, loss = 0.25, batch loss = 0.19 (29.3 examples/sec; 0.273 sec/batch; 24h:20m:57s remains)
INFO - root - 2017-12-16 04:34:52.597578: step 11890, loss = 0.27, batch loss = 0.22 (27.8 examples/sec; 0.288 sec/batch; 25h:37m:10s remains)
INFO - root - 2017-12-16 04:34:55.397145: step 11900, loss = 0.25, batch loss = 0.19 (29.2 examples/sec; 0.274 sec/batch; 24h:21m:47s remains)
2017-12-16 04:34:55.874445: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8823664 -3.9210978 -3.9979653 -4.0684028 -4.0407982 -4.2470641 -4.477479 -4.7086911 -4.9599638 -5.0620441 -5.0235162 -4.731195 -4.3674364 -4.1266618 -4.0758157][-3.1712496 -3.094635 -3.144393 -3.2820928 -3.354949 -3.8363905 -4.2633576 -4.5310216 -4.7799916 -4.9081078 -4.827929 -4.5795016 -4.4054918 -4.1211143 -3.9878416][-2.401197 -2.1574671 -2.1841173 -2.4887078 -2.7526004 -3.1483026 -3.4743116 -3.8185313 -4.0463567 -4.1122742 -4.2276864 -4.1219072 -4.0498729 -4.1490626 -4.1068745][-1.4196699 -1.1884906 -1.2670877 -1.632864 -1.9780867 -2.3064046 -2.4713168 -2.493506 -2.5080891 -2.6510415 -2.9046297 -3.190167 -3.5261304 -3.5896368 -3.5206435][-0.13329029 0.040574551 -0.15133572 -0.60831356 -0.93730211 -0.96488595 -0.8322289 -0.75600338 -0.8145833 -1.0996046 -1.6884568 -2.2892797 -2.651998 -2.8854818 -2.9506085][-0.2156496 0.023144722 -0.14251947 -0.3617053 -0.18412924 0.19815731 0.68710089 0.97679186 0.80821085 0.054092407 -1.0900459 -2.1344321 -2.8028972 -3.0455053 -2.8501291][-1.554451 -1.1653695 -0.95168209 -0.83230948 0.029699802 1.1914353 2.2681856 2.6347356 2.2159266 1.2586036 -0.17026949 -1.641927 -2.5531402 -2.9728296 -2.9990373][-2.7258558 -2.3856065 -1.9535241 -1.1827562 -0.0016827583 1.3607779 2.6984296 3.1867876 2.7349873 1.4997916 -0.09202528 -1.4550812 -2.4594665 -3.0092638 -3.0635676][-3.845973 -3.5303779 -2.9549775 -1.992408 -0.59577274 0.9281888 2.1975265 2.520668 1.9911757 0.78872538 -0.78504992 -2.1640613 -3.1153893 -3.5730224 -3.5647287][-4.5726318 -4.3619919 -4.0205765 -3.2254181 -1.9074836 -0.5315125 0.47477055 0.89652681 0.57005739 -0.54241085 -1.8991399 -3.1762362 -3.9609373 -4.2163815 -4.0554252][-4.9369407 -4.9909945 -4.728159 -4.271903 -3.5444496 -2.54567 -1.8183928 -1.4755094 -1.6560295 -2.3401787 -3.1382833 -4.014648 -4.5337667 -4.4340668 -3.9668262][-4.68417 -4.9027505 -5.133554 -5.0056787 -4.622025 -4.0410991 -3.533818 -3.364738 -3.4063077 -3.6956649 -4.1796145 -4.7197409 -4.8800793 -4.5616379 -3.9603825][-4.4412975 -4.7095356 -5.0071898 -5.0456285 -4.8964 -4.5230765 -4.1327381 -4.1196203 -4.3181477 -4.5770149 -4.8727994 -5.150382 -5.0910068 -4.7085218 -4.0242171][-4.5232735 -4.7444911 -4.9444108 -4.9222126 -4.9077077 -4.5952067 -4.2604561 -4.284831 -4.38037 -4.5582981 -4.7353268 -4.8585091 -4.7423992 -4.3566689 -3.819869][-4.1895986 -4.2507782 -4.2267475 -4.1590738 -4.1127639 -3.9424772 -3.8869123 -4.0152874 -4.1344142 -4.3695984 -4.5998769 -4.5592642 -4.3545675 -4.1193891 -3.7516317]]...]
INFO - root - 2017-12-16 04:34:58.733224: step 11910, loss = 0.25, batch loss = 0.19 (27.8 examples/sec; 0.288 sec/batch; 25h:38m:32s remains)
INFO - root - 2017-12-16 04:35:01.545016: step 11920, loss = 0.23, batch loss = 0.18 (29.3 examples/sec; 0.273 sec/batch; 24h:17m:47s remains)
INFO - root - 2017-12-16 04:35:04.372502: step 11930, loss = 0.30, batch loss = 0.24 (28.9 examples/sec; 0.277 sec/batch; 24h:38m:15s remains)
INFO - root - 2017-12-16 04:35:07.169978: step 11940, loss = 0.26, batch loss = 0.21 (28.5 examples/sec; 0.281 sec/batch; 24h:58m:42s remains)
INFO - root - 2017-12-16 04:35:10.068430: step 11950, loss = 0.22, batch loss = 0.17 (28.9 examples/sec; 0.277 sec/batch; 24h:39m:22s remains)
INFO - root - 2017-12-16 04:35:12.888559: step 11960, loss = 0.31, batch loss = 0.25 (29.5 examples/sec; 0.272 sec/batch; 24h:10m:53s remains)
INFO - root - 2017-12-16 04:35:15.675505: step 11970, loss = 0.20, batch loss = 0.14 (27.9 examples/sec; 0.287 sec/batch; 25h:33m:42s remains)
INFO - root - 2017-12-16 04:35:18.478795: step 11980, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.285 sec/batch; 25h:24m:36s remains)
INFO - root - 2017-12-16 04:35:21.360703: step 11990, loss = 0.28, batch loss = 0.22 (28.0 examples/sec; 0.286 sec/batch; 25h:27m:06s remains)
INFO - root - 2017-12-16 04:35:24.219410: step 12000, loss = 0.29, batch loss = 0.24 (28.4 examples/sec; 0.282 sec/batch; 25h:05m:53s remains)
2017-12-16 04:35:24.666125: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8160915 -3.0644627 -3.0327373 -3.1912804 -3.4654658 -3.6005435 -3.679462 -4.0881562 -4.4762888 -4.54387 -4.3908606 -4.06385 -3.5593634 -3.1715317 -2.816154][-2.9615753 -3.3042347 -3.5492651 -3.5539134 -3.355556 -3.5000787 -3.5935676 -3.4899328 -3.6078038 -4.0088983 -4.1243329 -3.9264836 -3.6354094 -3.2777324 -2.939033][-3.236433 -3.728035 -3.8618526 -3.9055309 -3.6932633 -3.2661321 -2.7635746 -2.5908408 -2.6648655 -3.0456319 -3.5764282 -3.7861974 -3.6478927 -3.3974133 -3.1774158][-3.5041988 -4.1345863 -4.2696338 -3.8575459 -2.9810286 -2.0924807 -1.2095897 -0.80431843 -1.010473 -1.7239382 -2.4656835 -3.1039102 -3.3987179 -3.1118796 -2.740551][-4.0533876 -4.4859276 -4.2683959 -3.3571279 -1.7748592 -0.17355871 1.0099239 1.5587225 1.4573927 0.41385841 -1.0095818 -2.04201 -2.5711622 -2.7226675 -2.3978295][-4.5820475 -4.7164092 -4.162076 -2.9124942 -0.87124205 1.3674307 3.1243978 3.7283649 3.2054949 2.0323553 0.37683964 -1.3223288 -2.2797244 -2.4380357 -2.3106916][-4.5796185 -5.0154986 -4.3662062 -2.8495905 -0.67226744 1.552453 3.4118261 4.2371016 3.8699045 2.3426533 0.34528112 -1.2452447 -2.3426781 -2.6517982 -2.3298435][-4.8593545 -5.1557117 -4.8828182 -3.6208458 -1.3773978 0.84480667 2.6485949 3.3205638 2.8572249 1.6379199 -0.1410265 -1.7508454 -2.672554 -3.0205669 -2.7554359][-4.3346424 -4.9371185 -4.8491488 -3.9910662 -2.4427223 -0.50810981 1.2051692 1.959167 1.7267227 0.50583792 -1.175796 -2.5143435 -3.4091978 -3.5674269 -3.1839023][-3.4431603 -4.2827134 -4.8387442 -4.3711567 -3.2306712 -1.835716 -0.56063533 0.074657917 -0.0061426163 -0.69737935 -1.9071367 -3.1446095 -3.731662 -3.7726135 -3.4143481][-2.4358172 -3.7370389 -4.7776451 -4.8689528 -4.246088 -3.3582063 -2.4585104 -1.6572282 -1.3580968 -1.7740867 -2.5894644 -3.2763906 -3.5362906 -3.4726474 -3.0283823][-2.3730762 -3.2275696 -4.3952546 -5.1548314 -5.0409803 -4.3991318 -3.5743804 -2.9216142 -2.41154 -2.2379978 -2.6544156 -3.1095226 -3.2232325 -3.1149416 -2.8644774][-1.772826 -2.7856984 -4.073431 -4.7717361 -4.84952 -4.4313612 -3.7176106 -2.8775864 -2.1759059 -1.934216 -2.1907263 -2.5670719 -2.7159686 -2.5372181 -2.2022128][-1.2158029 -1.8539307 -3.2566717 -4.2927055 -4.4283843 -3.999712 -3.212595 -2.3625033 -1.5599535 -1.2585278 -1.5363357 -1.7057726 -1.6630595 -1.6280313 -1.616771][-1.6942434 -2.3680727 -3.4291434 -4.305378 -4.3410025 -3.8241842 -3.0074286 -2.0109472 -1.2166419 -0.74372172 -0.79215741 -1.080044 -1.3156667 -1.2662752 -1.1790814]]...]
INFO - root - 2017-12-16 04:35:27.526435: step 12010, loss = 0.24, batch loss = 0.18 (27.5 examples/sec; 0.291 sec/batch; 25h:54m:20s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 04:35:30.449245: step 12020, loss = 0.20, batch loss = 0.14 (25.4 examples/sec; 0.315 sec/batch; 28h:01m:20s remains)
INFO - root - 2017-12-16 04:35:33.256174: step 12030, loss = 0.39, batch loss = 0.33 (28.9 examples/sec; 0.277 sec/batch; 24h:37m:28s remains)
INFO - root - 2017-12-16 04:35:36.083931: step 12040, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 25h:06m:38s remains)
INFO - root - 2017-12-16 04:35:38.933485: step 12050, loss = 0.34, batch loss = 0.28 (28.8 examples/sec; 0.277 sec/batch; 24h:41m:41s remains)
INFO - root - 2017-12-16 04:35:41.766286: step 12060, loss = 0.26, batch loss = 0.20 (29.0 examples/sec; 0.276 sec/batch; 24h:35m:37s remains)
INFO - root - 2017-12-16 04:35:44.579013: step 12070, loss = 0.39, batch loss = 0.33 (27.6 examples/sec; 0.289 sec/batch; 25h:45m:57s remains)
INFO - root - 2017-12-16 04:35:47.377426: step 12080, loss = 0.29, batch loss = 0.23 (28.6 examples/sec; 0.280 sec/batch; 24h:53m:09s remains)
INFO - root - 2017-12-16 04:35:50.195339: step 12090, loss = 0.26, batch loss = 0.20 (28.8 examples/sec; 0.278 sec/batch; 24h:44m:04s remains)
INFO - root - 2017-12-16 04:35:53.061433: step 12100, loss = 0.21, batch loss = 0.16 (26.0 examples/sec; 0.307 sec/batch; 27h:21m:11s remains)
2017-12-16 04:35:53.515366: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6793034 -2.833919 -3.1035135 -3.4022808 -3.6695728 -3.7077343 -3.6489542 -3.5465741 -3.4151716 -3.3089767 -3.2701664 -3.3625264 -3.3476522 -3.1857638 -3.0043898][-2.4749274 -2.6252084 -3.0064454 -3.2956369 -3.4827466 -3.362699 -3.1054811 -2.9571707 -2.8059316 -2.7114735 -2.6273341 -2.6977291 -2.7608418 -2.5754287 -2.5414102][-2.1200042 -2.2881317 -2.6253023 -2.9708676 -3.1321387 -3.0402861 -2.7635503 -2.5209403 -2.2592449 -2.1013191 -2.1107855 -2.0541427 -1.9493144 -1.8010454 -1.8153548][-1.9147077 -2.1520851 -2.4953582 -2.683506 -2.6891642 -2.6568069 -2.3779554 -2.1205626 -1.9078755 -1.672267 -1.6057124 -1.6049509 -1.6623025 -1.7732975 -2.1201372][-1.6506078 -1.8847971 -2.2085648 -2.4286253 -2.4654441 -2.3104861 -2.004632 -1.8451228 -1.6875648 -1.454566 -1.3954155 -1.4003491 -1.5590003 -1.9111109 -2.5329719][-1.5748084 -1.8027132 -2.0486155 -2.1048183 -1.9502897 -1.7406013 -1.4731627 -1.2873311 -1.1356423 -1.0461032 -1.0712121 -1.1463902 -1.4516206 -2.0008349 -2.670702][-1.4381404 -1.5204132 -1.5950944 -1.4062083 -1.0840147 -0.76962805 -0.456573 -0.44209027 -0.48553681 -0.50281358 -0.59126234 -0.76143289 -1.1476638 -1.6337557 -2.3003914][-1.3830009 -1.339437 -1.2549865 -0.96755648 -0.68341684 -0.40693808 -0.16521025 -0.13420105 -0.14502382 -0.087369919 -0.070266247 -0.24407482 -0.59598613 -1.1857817 -1.9408445][-1.530931 -1.3466916 -1.2181053 -0.99937725 -0.73784781 -0.45338964 -0.2973156 -0.24969053 -0.23860168 -0.055202484 0.12739944 0.073130608 -0.21596766 -0.85745835 -1.6414099][-1.8791273 -1.7170236 -1.6223488 -1.4765568 -1.2751434 -1.1773796 -1.1207035 -0.89155149 -0.80870676 -0.49666095 -0.18082047 -0.1951437 -0.4557569 -1.0986252 -1.7940412][-2.5525584 -2.4362173 -2.3483639 -2.4462919 -2.4614546 -2.3319802 -2.3178489 -2.1123078 -2.0039158 -1.5251911 -1.0552366 -0.96956468 -1.1603248 -1.8849049 -2.4880118][-3.5238764 -3.5378678 -3.6209602 -3.815644 -3.9272742 -3.8565001 -3.8210115 -3.4121985 -3.1779563 -2.7491202 -2.3296962 -2.2144208 -2.3716352 -3.05827 -3.4809942][-4.8855381 -4.9860296 -5.1853838 -5.45901 -5.590682 -5.4405928 -5.2762957 -4.7623715 -4.5056887 -4.1023111 -3.7550764 -3.8254192 -4.0354743 -4.4536386 -4.5409188][-6.0656023 -6.0759344 -6.1463976 -6.4365788 -6.5217624 -6.346262 -6.1830835 -5.7495041 -5.5802889 -5.3381329 -5.1907539 -5.3277006 -5.4824352 -5.6229911 -5.3684011][-6.6698589 -6.6908298 -6.7941289 -6.9789362 -6.9829903 -6.8256764 -6.6922197 -6.3274431 -6.2305918 -6.1864595 -6.1972847 -6.342124 -6.3698812 -6.1679254 -5.5811148]]...]
INFO - root - 2017-12-16 04:35:56.303884: step 12110, loss = 0.29, batch loss = 0.23 (29.3 examples/sec; 0.273 sec/batch; 24h:18m:27s remains)
INFO - root - 2017-12-16 04:35:59.129126: step 12120, loss = 0.36, batch loss = 0.30 (28.2 examples/sec; 0.284 sec/batch; 25h:16m:37s remains)
INFO - root - 2017-12-16 04:36:01.900682: step 12130, loss = 0.22, batch loss = 0.16 (26.8 examples/sec; 0.299 sec/batch; 26h:35m:11s remains)
INFO - root - 2017-12-16 04:36:04.758782: step 12140, loss = 0.22, batch loss = 0.16 (28.3 examples/sec; 0.282 sec/batch; 25h:06m:45s remains)
INFO - root - 2017-12-16 04:36:07.575061: step 12150, loss = 0.28, batch loss = 0.22 (27.4 examples/sec; 0.292 sec/batch; 25h:58m:47s remains)
INFO - root - 2017-12-16 04:36:10.421805: step 12160, loss = 0.35, batch loss = 0.29 (29.5 examples/sec; 0.271 sec/batch; 24h:09m:24s remains)
INFO - root - 2017-12-16 04:36:13.278762: step 12170, loss = 0.30, batch loss = 0.24 (26.5 examples/sec; 0.302 sec/batch; 26h:51m:31s remains)
INFO - root - 2017-12-16 04:36:16.176748: step 12180, loss = 0.19, batch loss = 0.14 (28.2 examples/sec; 0.284 sec/batch; 25h:14m:29s remains)
INFO - root - 2017-12-16 04:36:18.982865: step 12190, loss = 0.23, batch loss = 0.17 (28.9 examples/sec; 0.277 sec/batch; 24h:38m:24s remains)
INFO - root - 2017-12-16 04:36:21.837148: step 12200, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.286 sec/batch; 25h:26m:23s remains)
2017-12-16 04:36:22.296928: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.77897 -2.8081594 -2.92973 -3.2681673 -3.5137482 -3.7619467 -4.0720091 -4.403275 -4.5634642 -4.395895 -4.0582967 -3.63967 -3.070806 -2.4879479 -2.0765336][-2.5850098 -2.7642379 -2.9584837 -3.3027112 -3.4758077 -3.5570085 -3.7032487 -3.8280632 -3.7918038 -3.5640154 -3.2671666 -2.7804236 -2.1767282 -1.6589439 -1.3648283][-2.6911561 -2.9726706 -3.2824795 -3.5388811 -3.4947758 -3.3283949 -3.2210879 -3.2859108 -3.2164736 -2.9550757 -2.7465496 -2.4274745 -1.9460714 -1.3412373 -1.060097][-2.8466363 -3.1485806 -3.3986213 -3.4062438 -3.0469527 -2.5240493 -2.0303404 -1.9101877 -1.895385 -1.9389832 -1.9356434 -1.8351221 -1.8342264 -1.5593097 -1.3999183][-3.3284335 -3.549912 -3.4121478 -2.9337144 -2.0687864 -1.0085945 -0.095085144 0.20486975 -0.045449734 -0.541832 -1.0601346 -1.3346457 -1.6201315 -1.6793911 -1.6770413][-3.4738536 -3.5887308 -3.1973038 -2.2538726 -0.94318438 0.57121897 1.8230224 2.2437391 1.8822398 1.0132174 0.048803329 -0.75084758 -1.4728 -1.6821744 -1.6755443][-3.4788544 -3.4049425 -2.6659379 -1.2820618 0.31879091 2.1479306 3.7693291 4.2012691 3.5959091 2.3021297 0.91755533 -0.28955603 -1.2752793 -1.594934 -1.6857274][-3.5529459 -3.3859832 -2.4925709 -0.80245376 1.0730882 2.9717197 4.5407772 5.0953579 4.5454006 2.8310289 1.0658641 -0.29279995 -1.2978001 -1.6894164 -1.7501023][-3.4144254 -3.3585844 -2.6105428 -0.9523387 0.90955782 2.718411 4.0883713 4.5218534 3.8456736 2.2184238 0.58306742 -0.79249239 -1.69504 -2.0275879 -1.9709349][-3.2844615 -3.3410215 -2.8282197 -1.5560753 -0.041293621 1.5762687 2.7261415 3.0078406 2.3420663 0.84402323 -0.68802404 -2.1104155 -2.9223607 -2.8873277 -2.6484294][-3.4594264 -3.603272 -3.2426853 -2.2578273 -1.1328044 0.14921093 0.95861578 0.94771814 0.34497166 -0.91455889 -2.3031585 -3.5539913 -4.231226 -4.094522 -3.7044215][-3.9378 -3.9947464 -3.8484581 -3.1462941 -2.2172713 -1.2566309 -0.89349484 -1.1556396 -1.7173176 -2.7519369 -3.9311793 -5.0467 -5.5543618 -5.3410473 -4.7635965][-4.133779 -4.037437 -3.9321041 -3.6087725 -3.1984959 -2.5084076 -2.16947 -2.4640784 -3.0666404 -3.9150078 -4.7665091 -5.7508707 -6.2588305 -6.0570765 -5.3876371][-4.3554096 -3.9985757 -3.6785607 -3.4948313 -3.2089839 -2.705575 -2.4558659 -2.8027859 -3.4282603 -4.1863937 -4.9503164 -5.7511263 -6.1648111 -5.6910877 -4.7640057][-4.4650836 -3.9735677 -3.5925469 -3.4350531 -3.210216 -2.748848 -2.4615507 -2.6243849 -2.9853382 -3.7247028 -4.5815439 -5.21019 -5.6280837 -5.30268 -4.2753077]]...]
INFO - root - 2017-12-16 04:36:25.105922: step 12210, loss = 0.29, batch loss = 0.23 (28.8 examples/sec; 0.278 sec/batch; 24h:41m:47s remains)
INFO - root - 2017-12-16 04:36:27.915903: step 12220, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.283 sec/batch; 25h:13m:06s remains)
INFO - root - 2017-12-16 04:36:30.744369: step 12230, loss = 0.48, batch loss = 0.42 (28.7 examples/sec; 0.278 sec/batch; 24h:45m:18s remains)
INFO - root - 2017-12-16 04:36:33.498453: step 12240, loss = 0.26, batch loss = 0.20 (29.1 examples/sec; 0.274 sec/batch; 24h:25m:02s remains)
INFO - root - 2017-12-16 04:36:36.379405: step 12250, loss = 0.46, batch loss = 0.40 (29.3 examples/sec; 0.273 sec/batch; 24h:18m:31s remains)
INFO - root - 2017-12-16 04:36:39.170088: step 12260, loss = 0.21, batch loss = 0.15 (28.0 examples/sec; 0.286 sec/batch; 25h:26m:07s remains)
INFO - root - 2017-12-16 04:36:41.996217: step 12270, loss = 0.32, batch loss = 0.26 (28.9 examples/sec; 0.276 sec/batch; 24h:34m:56s remains)
INFO - root - 2017-12-16 04:36:44.877176: step 12280, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.281 sec/batch; 25h:01m:00s remains)
INFO - root - 2017-12-16 04:36:47.731303: step 12290, loss = 0.30, batch loss = 0.24 (28.6 examples/sec; 0.279 sec/batch; 24h:51m:01s remains)
INFO - root - 2017-12-16 04:36:50.590754: step 12300, loss = 0.24, batch loss = 0.18 (25.7 examples/sec; 0.311 sec/batch; 27h:41m:42s remains)
2017-12-16 04:36:51.033292: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.89054561 -1.1863751 -1.7814982 -2.5501637 -3.3212523 -3.7326016 -4.2440491 -4.2825184 -4.335803 -4.0740266 -3.6480834 -3.2080367 -2.9279585 -3.0786247 -3.1777754][-0.32431459 -0.40271187 -0.71248126 -1.5721869 -2.4572191 -2.9945383 -3.4351711 -3.6091161 -3.8346093 -3.9037592 -3.6668839 -3.1342397 -2.8403003 -2.6229234 -2.7043962][0.14464426 -0.11668968 -0.46327472 -0.83086729 -1.3596492 -1.7199519 -2.0610051 -2.6316061 -3.1331515 -3.4200356 -3.7435987 -3.4931378 -3.1107936 -2.9108157 -2.8649931][-0.059628963 -0.1432333 -0.31278849 -0.85107923 -1.287734 -1.4459729 -1.396112 -1.3038473 -1.8721349 -2.5825982 -3.0772648 -3.4086223 -3.4496913 -3.146982 -2.8820519][-0.40893412 -0.4738934 -0.49894738 -0.51894569 -0.368968 -0.17315388 -0.005636692 -0.51190019 -1.377197 -2.3398056 -3.4400406 -3.80523 -3.7858958 -3.355907 -2.8506866][-1.7060816 -1.4627874 -1.0201969 -0.20125389 0.88029385 1.5281129 1.9799428 1.6890774 0.60052395 -0.88240767 -2.4781208 -3.3412693 -3.6107805 -3.3469 -2.9854002][-2.249408 -1.7169902 -1.0645325 -0.058958054 1.2627439 2.7076879 3.8153629 3.7186718 2.4675012 0.50051165 -1.244231 -2.316139 -2.8993618 -2.836441 -2.1704183][-2.4776797 -1.7493582 -0.94294691 0.36128998 1.8974652 3.3491974 4.3044138 3.9888611 2.7680731 0.95109463 -0.90335965 -2.1414866 -2.4363451 -2.2326484 -1.6038425][-3.0499635 -2.3008049 -1.3595083 -0.17199039 1.1036544 2.468 3.4104013 3.2383866 1.9855528 0.15583754 -1.3805261 -2.4159195 -2.6496856 -1.9345772 -0.667032][-3.3664389 -2.8434906 -2.3048365 -1.2350202 -0.29697275 0.55453587 1.0101113 0.87130022 -0.0246768 -1.5315404 -2.7069683 -3.2104597 -2.8132117 -2.2008049 -0.89760876][-3.4592528 -3.0326209 -2.6222239 -2.2242603 -1.9342308 -1.5208883 -1.3806052 -1.7049263 -2.6889219 -3.9981947 -4.8594155 -5.1793528 -4.4325714 -3.3575571 -1.8518922][-3.4360769 -3.5112207 -3.5667639 -3.560009 -3.7563546 -3.8682036 -4.0715179 -4.5278463 -5.2584896 -6.1695633 -6.8053942 -7.0641193 -6.3562202 -5.1108 -3.4357071][-3.4400196 -3.7508395 -4.1930456 -4.4100008 -4.7451878 -5.1624856 -5.718668 -6.1870618 -6.7926879 -7.6566868 -8.1414309 -8.2669535 -7.4997082 -6.3142471 -4.9565539][-3.1920311 -3.4334469 -3.8961804 -4.1733332 -4.5459847 -4.8796377 -5.565834 -6.3818531 -7.2876244 -8.3747673 -9.0498886 -9.1698875 -8.5334206 -7.6342878 -6.3853226][-2.9464588 -2.9949417 -3.1788445 -3.3930669 -3.7303782 -4.3851 -5.2590179 -6.2225032 -7.3555527 -8.5134811 -9.3507643 -9.588089 -9.144846 -8.3348408 -7.3138208]]...]
INFO - root - 2017-12-16 04:36:53.841916: step 12310, loss = 0.28, batch loss = 0.22 (28.8 examples/sec; 0.277 sec/batch; 24h:40m:27s remains)
INFO - root - 2017-12-16 04:36:56.653664: step 12320, loss = 0.28, batch loss = 0.22 (28.8 examples/sec; 0.277 sec/batch; 24h:40m:17s remains)
INFO - root - 2017-12-16 04:36:59.458703: step 12330, loss = 0.26, batch loss = 0.20 (29.5 examples/sec; 0.271 sec/batch; 24h:07m:45s remains)
INFO - root - 2017-12-16 04:37:02.279860: step 12340, loss = 0.30, batch loss = 0.24 (28.7 examples/sec; 0.279 sec/batch; 24h:49m:30s remains)
INFO - root - 2017-12-16 04:37:05.106666: step 12350, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.284 sec/batch; 25h:13m:16s remains)
INFO - root - 2017-12-16 04:37:07.969688: step 12360, loss = 0.31, batch loss = 0.25 (27.9 examples/sec; 0.287 sec/batch; 25h:32m:18s remains)
INFO - root - 2017-12-16 04:37:10.835286: step 12370, loss = 0.42, batch loss = 0.36 (26.9 examples/sec; 0.298 sec/batch; 26h:29m:30s remains)
INFO - root - 2017-12-16 04:37:13.678759: step 12380, loss = 0.34, batch loss = 0.28 (28.7 examples/sec; 0.279 sec/batch; 24h:49m:06s remains)
INFO - root - 2017-12-16 04:37:16.486958: step 12390, loss = 0.30, batch loss = 0.24 (29.0 examples/sec; 0.276 sec/batch; 24h:31m:15s remains)
INFO - root - 2017-12-16 04:37:19.314909: step 12400, loss = 0.20, batch loss = 0.14 (27.8 examples/sec; 0.287 sec/batch; 25h:32m:37s remains)
2017-12-16 04:37:19.772891: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1195168 -6.2611208 -6.2818842 -6.4265471 -6.5995879 -6.5794349 -6.3213768 -6.3284836 -6.3861885 -6.4882727 -6.7228427 -7.1259012 -7.5370378 -7.6331949 -7.5789409][-6.3010416 -6.509594 -6.57524 -6.5954418 -6.6079044 -6.8897696 -6.9420223 -6.8598604 -6.7469215 -7.044446 -7.60065 -8.0030861 -8.39833 -8.629961 -8.4803925][-5.9763694 -5.9650731 -5.8450522 -5.86039 -5.8889117 -5.9633503 -5.7946997 -5.903028 -6.1990619 -6.7135725 -7.4072351 -8.0321493 -8.6096277 -8.7524586 -8.6585617][-5.1172628 -4.9073577 -4.5206294 -4.08488 -3.7154784 -3.7339053 -3.4922812 -3.5602834 -3.92468 -4.9140749 -5.8671389 -6.7352972 -7.5971112 -7.9166765 -7.8784595][-4.1821141 -3.4778128 -2.5622759 -1.8533659 -1.1284418 -0.53805614 -0.062434196 -0.23221874 -0.68088913 -1.8549206 -3.1034276 -4.1808977 -5.1536574 -5.827611 -6.13443][-3.3356271 -2.4284639 -1.1750343 0.28183126 1.7295918 2.6561537 3.342546 3.2696805 2.6811504 1.2853856 -0.034497261 -1.3105257 -2.477757 -3.4466894 -4.1482821][-3.7330568 -2.4880104 -0.80995941 1.145689 3.1895838 4.7201138 5.8511114 5.9187031 5.2627792 3.7947588 2.2673526 0.70617485 -0.68586993 -1.8795784 -2.9403858][-4.6948986 -3.7088606 -2.0019855 0.28051996 2.599122 4.5192137 6.01618 6.3731718 5.9768162 4.6662445 3.1763344 1.5535479 0.048250675 -1.2772553 -2.5963173][-6.2435722 -5.4621992 -4.0136395 -1.8820031 0.31165314 2.3876266 4.2076178 4.8129883 4.7909946 3.9503031 2.8030119 1.34125 -0.11957312 -1.3172619 -2.599534][-7.58787 -7.1165552 -6.3666296 -4.7080379 -2.7097173 -0.78507042 0.89113522 1.9478531 2.4715724 2.0681238 1.2504363 0.063526154 -1.2919602 -2.4174836 -3.583838][-8.5908356 -8.6280985 -8.2159109 -7.1162119 -5.8425317 -4.3220243 -2.7059169 -1.5404861 -0.93822384 -0.98671007 -1.4579241 -2.3786314 -3.5379429 -4.4312997 -5.2191844][-8.8015938 -9.0695324 -9.0030317 -8.5683956 -7.848259 -6.8006859 -5.6105657 -4.6138239 -3.6724944 -3.3243241 -3.5165825 -4.27244 -5.1749964 -5.85834 -6.3928738][-8.3760281 -8.5799313 -8.7906265 -8.8565426 -8.6132994 -8.0617 -7.2387619 -6.1170378 -5.1229191 -4.6848555 -4.4888391 -4.785471 -5.4538279 -6.3071337 -6.8208733][-6.8659468 -7.0879498 -7.3016791 -7.5338712 -7.5808287 -7.4286909 -7.0266657 -6.3314905 -5.5835605 -4.9460826 -4.744916 -5.248692 -5.952188 -6.5304632 -6.7711568][-5.9939432 -6.0482974 -6.0695047 -6.222084 -6.3237586 -6.3582735 -6.2263927 -5.7976685 -5.3427162 -5.0064392 -4.9557185 -5.0873985 -5.4126072 -5.9077263 -6.1297421]]...]
INFO - root - 2017-12-16 04:37:22.597991: step 12410, loss = 0.25, batch loss = 0.19 (29.6 examples/sec; 0.271 sec/batch; 24h:03m:30s remains)
INFO - root - 2017-12-16 04:37:25.426305: step 12420, loss = 0.19, batch loss = 0.13 (28.5 examples/sec; 0.281 sec/batch; 24h:59m:05s remains)
INFO - root - 2017-12-16 04:37:28.261909: step 12430, loss = 0.30, batch loss = 0.24 (28.5 examples/sec; 0.281 sec/batch; 24h:59m:37s remains)
INFO - root - 2017-12-16 04:37:31.079878: step 12440, loss = 0.31, batch loss = 0.25 (27.6 examples/sec; 0.290 sec/batch; 25h:48m:52s remains)
INFO - root - 2017-12-16 04:37:33.936093: step 12450, loss = 0.21, batch loss = 0.15 (28.6 examples/sec; 0.280 sec/batch; 24h:53m:01s remains)
INFO - root - 2017-12-16 04:37:36.763578: step 12460, loss = 0.26, batch loss = 0.20 (29.4 examples/sec; 0.272 sec/batch; 24h:11m:00s remains)
INFO - root - 2017-12-16 04:37:39.670674: step 12470, loss = 0.27, batch loss = 0.21 (28.6 examples/sec; 0.280 sec/batch; 24h:52m:22s remains)
INFO - root - 2017-12-16 04:37:42.484750: step 12480, loss = 0.24, batch loss = 0.18 (26.9 examples/sec; 0.297 sec/batch; 26h:26m:25s remains)
INFO - root - 2017-12-16 04:37:45.385636: step 12490, loss = 0.22, batch loss = 0.17 (27.0 examples/sec; 0.296 sec/batch; 26h:19m:45s remains)
INFO - root - 2017-12-16 04:37:48.236789: step 12500, loss = 0.22, batch loss = 0.16 (27.9 examples/sec; 0.287 sec/batch; 25h:28m:10s remains)
2017-12-16 04:37:48.683237: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.9552288 -7.021812 -6.8885832 -6.3694463 -5.9763813 -5.4124703 -4.9385262 -4.6959314 -4.7323065 -5.1500654 -5.5809121 -6.12898 -6.7311993 -7.1796474 -7.02149][-7.6884232 -8.0062447 -7.9382629 -7.3999848 -6.7625322 -5.9288878 -5.071826 -4.4154282 -4.2717991 -4.8925853 -5.6090584 -6.1536112 -6.6526675 -7.1837769 -6.996398][-7.927702 -8.2483006 -8.0287056 -7.3187513 -6.5108004 -5.6079111 -4.6298337 -4.0007253 -3.7340093 -4.193903 -4.9180059 -5.5703163 -6.3174958 -6.8236 -6.6953769][-6.9850979 -7.3414721 -6.9759741 -5.9683213 -4.865314 -3.8812313 -2.9433479 -2.6468649 -2.7808204 -3.1460614 -3.6175044 -4.2022858 -4.7983871 -5.4554386 -5.576314][-5.3423133 -5.4420104 -4.9885225 -4.1397433 -3.1925864 -2.1239035 -1.1933293 -0.75787091 -0.75200963 -1.2215359 -1.9313419 -2.1556883 -2.7740967 -3.6634755 -4.1772952][-3.9960942 -3.5420299 -2.5357358 -1.3176546 -0.052789688 0.67979193 1.1386118 1.2391629 0.99071646 0.362957 -0.36698341 -0.93471265 -1.7106218 -2.5215578 -3.0932879][-2.8072929 -2.1036117 -1.0493095 0.40594912 2.0284986 2.9639502 3.3550777 3.0608287 2.5224628 1.6873446 0.69019032 -0.29150105 -1.2168591 -2.3327794 -3.1446714][-2.6727135 -1.8624835 -0.52671742 1.082057 2.5135326 3.4758635 3.9420929 3.7355995 2.9896731 1.9259343 0.71248341 -0.49239039 -1.6484783 -2.8906097 -3.7135015][-3.3265426 -2.58082 -1.4104087 -0.07623291 1.3754826 2.4632573 3.0100517 2.8046594 2.0041733 0.94049311 -0.28079128 -1.5817304 -2.7623522 -3.7898884 -4.4342427][-3.8391206 -3.6270568 -3.0889947 -2.0764515 -0.87653446 0.060971737 0.74690485 0.80460739 0.22644806 -0.681015 -1.7146165 -3.0101175 -4.1389031 -4.9080105 -5.1529789][-5.4448304 -5.21026 -4.5232396 -4.0474553 -3.4720459 -2.5841894 -1.8292899 -1.6484489 -1.8189695 -2.3934329 -3.0807555 -4.0535531 -4.8583865 -5.2718649 -5.1326532][-6.4929514 -6.4373426 -6.2669182 -5.8815532 -5.4724994 -4.9381933 -4.5221257 -4.1366506 -3.8987925 -4.003449 -4.1865611 -4.5916595 -4.9782991 -4.997334 -4.7396922][-7.1767063 -7.2294326 -7.3190656 -7.2227554 -7.08377 -6.7267075 -6.4496288 -6.1393032 -5.9138269 -5.6728535 -5.4322429 -5.3619318 -5.3224082 -5.0113578 -4.49132][-7.1590395 -7.090898 -7.1622462 -7.1490173 -7.088726 -6.9267516 -6.7348576 -6.603898 -6.4119763 -6.0976009 -5.8470993 -5.5724111 -5.3327165 -4.9637337 -4.4639411][-6.7401729 -6.3660622 -6.1903057 -6.2506323 -6.3521061 -6.3957219 -6.400712 -6.3223581 -6.1461391 -5.8756013 -5.6641464 -5.3854976 -5.1872129 -4.9045582 -4.5135679]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-12500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-12500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 04:37:51.884367: step 12510, loss = 0.24, batch loss = 0.18 (27.7 examples/sec; 0.289 sec/batch; 25h:40m:54s remains)
INFO - root - 2017-12-16 04:37:54.753231: step 12520, loss = 0.25, batch loss = 0.19 (27.8 examples/sec; 0.288 sec/batch; 25h:34m:34s remains)
INFO - root - 2017-12-16 04:37:57.561030: step 12530, loss = 0.26, batch loss = 0.20 (29.6 examples/sec; 0.270 sec/batch; 23h:59m:49s remains)
INFO - root - 2017-12-16 04:38:00.318681: step 12540, loss = 0.27, batch loss = 0.21 (29.0 examples/sec; 0.276 sec/batch; 24h:33m:25s remains)
INFO - root - 2017-12-16 04:38:03.156836: step 12550, loss = 0.32, batch loss = 0.26 (28.9 examples/sec; 0.277 sec/batch; 24h:34m:30s remains)
INFO - root - 2017-12-16 04:38:05.961010: step 12560, loss = 0.22, batch loss = 0.16 (28.8 examples/sec; 0.277 sec/batch; 24h:38m:43s remains)
INFO - root - 2017-12-16 04:38:08.801248: step 12570, loss = 0.18, batch loss = 0.12 (28.3 examples/sec; 0.282 sec/batch; 25h:04m:40s remains)
INFO - root - 2017-12-16 04:38:11.654364: step 12580, loss = 0.23, batch loss = 0.17 (28.9 examples/sec; 0.277 sec/batch; 24h:35m:40s remains)
INFO - root - 2017-12-16 04:38:14.496626: step 12590, loss = 0.20, batch loss = 0.14 (29.1 examples/sec; 0.275 sec/batch; 24h:28m:11s remains)
INFO - root - 2017-12-16 04:38:17.307526: step 12600, loss = 0.26, batch loss = 0.21 (28.9 examples/sec; 0.276 sec/batch; 24h:33m:58s remains)
2017-12-16 04:38:17.766192: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3088088 -3.6262507 -3.824542 -3.9926784 -4.067646 -4.0258574 -3.9125195 -3.8084681 -3.7564554 -3.7994463 -3.9631686 -4.1964426 -4.4595885 -4.5719461 -4.5755858][-3.4094629 -3.7944839 -3.987623 -4.1192384 -4.1510019 -4.1215363 -4.0783095 -4.031724 -3.9889712 -4.0244789 -4.1403813 -4.4071603 -4.779614 -5.0283937 -5.1068034][-3.6337359 -4.0227165 -4.1940446 -4.16521 -4.0368195 -3.8990111 -3.8012347 -3.8703396 -4.0166583 -4.1357455 -4.2124763 -4.4806638 -4.8756981 -5.1401834 -5.4260325][-4.1356254 -4.4154091 -4.3065448 -4.0023222 -3.6730714 -3.3127015 -3.0806823 -3.1236925 -3.2763608 -3.5860305 -3.9122617 -4.3120685 -4.7388167 -5.1012759 -5.389009][-4.7707663 -4.7814541 -4.3793278 -3.6869452 -2.9888716 -2.4339004 -2.0947771 -2.0841594 -2.2144115 -2.4908614 -2.7987018 -3.3560362 -3.952544 -4.5427122 -5.0766835][-5.1860394 -5.0404572 -4.516078 -3.3979113 -2.1524522 -1.3152766 -0.83102179 -0.77128363 -0.98035955 -1.248899 -1.4114616 -1.8537166 -2.4386411 -3.1789947 -4.0519681][-5.4135671 -5.0587306 -4.1922655 -2.8590057 -1.4802995 -0.25205183 0.58790541 0.6442318 0.35545588 0.0088386536 -0.18143654 -0.45167065 -0.84890604 -1.6237185 -2.6577418][-5.9320221 -5.3941822 -4.2380228 -2.5530453 -1.0400231 0.33226442 1.2744236 1.3999386 1.1937318 0.90043879 0.86227512 0.68861628 0.33215046 -0.5000813 -1.6470501][-6.4932423 -5.8687153 -4.6489592 -2.90698 -1.2365737 0.14387703 0.99942636 1.2159157 1.1653199 1.0510249 1.262629 1.1906204 0.85812187 -0.068820477 -1.4092238][-6.7626152 -6.1566391 -5.158927 -3.5090218 -1.9500721 -0.59902358 0.22032166 0.50678396 0.51208735 0.6353364 0.99378109 1.1015439 0.87612581 -0.069133759 -1.5627329][-6.9166408 -6.371892 -5.4107652 -4.2189808 -3.1068077 -1.8993766 -0.89712119 -0.50472045 -0.51338005 -0.25474548 0.13669443 0.22187042 -0.035318375 -0.89036012 -2.2560537][-7.0532513 -6.2878017 -5.33167 -4.5579853 -3.9907458 -3.3496423 -2.7293406 -2.1307547 -1.7548294 -1.4652638 -1.098141 -1.0887897 -1.3561585 -2.2092671 -3.3418989][-7.2657423 -6.4176722 -5.506506 -4.716918 -4.2304835 -4.0017281 -3.9520962 -3.6714208 -3.3148956 -2.7687907 -2.2694163 -2.2582982 -2.4367359 -3.1770134 -4.2639275][-6.407793 -5.82438 -5.181118 -4.7262211 -4.365099 -4.2402248 -4.2963324 -4.227345 -4.2118678 -3.7594056 -3.3729358 -3.3664989 -3.4415255 -3.8412275 -4.4473791][-5.7048454 -4.94786 -4.1639066 -4.0464735 -4.1013722 -4.0989265 -4.2517338 -4.3139195 -4.3884225 -4.0899763 -3.8563881 -3.8937693 -3.9312608 -4.1641164 -4.4302177]]...]
INFO - root - 2017-12-16 04:38:20.616825: step 12610, loss = 0.26, batch loss = 0.20 (26.3 examples/sec; 0.305 sec/batch; 27h:04m:31s remains)
INFO - root - 2017-12-16 04:38:23.478339: step 12620, loss = 0.30, batch loss = 0.24 (28.0 examples/sec; 0.285 sec/batch; 25h:21m:17s remains)
INFO - root - 2017-12-16 04:38:26.275958: step 12630, loss = 0.27, batch loss = 0.21 (28.8 examples/sec; 0.278 sec/batch; 24h:42m:45s remains)
INFO - root - 2017-12-16 04:38:29.099387: step 12640, loss = 0.31, batch loss = 0.26 (27.4 examples/sec; 0.292 sec/batch; 25h:54m:30s remains)
INFO - root - 2017-12-16 04:38:31.942025: step 12650, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.286 sec/batch; 25h:25m:26s remains)
INFO - root - 2017-12-16 04:38:34.791113: step 12660, loss = 0.24, batch loss = 0.19 (28.3 examples/sec; 0.282 sec/batch; 25h:05m:43s remains)
INFO - root - 2017-12-16 04:38:37.575750: step 12670, loss = 0.28, batch loss = 0.22 (29.2 examples/sec; 0.274 sec/batch; 24h:18m:28s remains)
INFO - root - 2017-12-16 04:38:40.384742: step 12680, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 25h:03m:05s remains)
INFO - root - 2017-12-16 04:38:43.213855: step 12690, loss = 0.30, batch loss = 0.24 (28.4 examples/sec; 0.282 sec/batch; 25h:02m:58s remains)
INFO - root - 2017-12-16 04:38:46.026325: step 12700, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 24h:47m:00s remains)
2017-12-16 04:38:46.466024: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5538638 -3.6193795 -3.6451473 -3.6566167 -3.708946 -3.6313992 -3.551095 -3.4413939 -3.2855291 -3.1339326 -2.8687658 -2.5740535 -2.3142595 -2.2057929 -2.0893364][-4.3031445 -4.3789144 -4.380147 -4.3703723 -4.3871303 -4.2857757 -4.1663661 -3.8870845 -3.6164064 -3.3978107 -3.0664706 -2.7703273 -2.4772816 -2.4517627 -2.3352625][-5.0189867 -5.0912266 -5.117774 -5.074367 -5.0109644 -4.77753 -4.5038209 -4.1156397 -3.7170682 -3.3634 -3.0180876 -2.8526428 -2.8222809 -2.918076 -2.8776114][-5.3483562 -5.3384752 -5.3099422 -5.2485166 -5.129746 -4.7724743 -4.3009095 -3.6886282 -3.2287047 -2.8367157 -2.5723076 -2.5822716 -2.8720295 -3.2700748 -3.4026198][-5.379261 -5.2182274 -5.0290079 -4.8464918 -4.584249 -4.173769 -3.5301707 -2.7001996 -2.1709816 -1.9130476 -1.9197655 -2.1452651 -2.7296345 -3.5323153 -3.8596845][-4.4985161 -4.3011618 -3.9752035 -3.6910884 -3.3144231 -2.8574529 -2.1908934 -1.4048538 -0.90347719 -0.78147244 -0.94844317 -1.4894481 -2.3945076 -3.3487434 -3.9363754][-2.9285066 -2.5904379 -2.3036218 -2.0594728 -1.7260828 -1.1901999 -0.49748254 0.064143181 0.4528904 0.51912785 0.25100565 -0.40670061 -1.498992 -2.6978498 -3.5096242][-1.8502614 -1.2585764 -0.99452186 -0.88544011 -0.68435621 -0.32859325 0.29061604 0.772254 1.0608611 0.92513084 0.53843832 -0.087908268 -1.1267478 -2.3320436 -3.2702684][-0.99058461 -0.60511851 -0.45121241 -0.26820517 -0.2195363 -0.10347271 0.29547071 0.6239872 0.93280888 0.859045 0.57091475 -0.094992638 -0.99806619 -2.0899763 -3.0664556][-0.66731358 -0.60986257 -0.40856695 -0.41097212 -0.63603187 -0.45981765 -0.36409807 -0.35176229 -0.18221951 -0.18803692 -0.23690176 -0.65217638 -1.3931155 -2.4130278 -3.2398105][-1.5934372 -1.4155183 -1.3914671 -1.4794905 -1.6462142 -1.6612747 -1.5342684 -1.5234141 -1.5679202 -1.5679777 -1.5550759 -1.8208778 -2.4314551 -3.2253852 -3.9587955][-2.5782669 -2.8684042 -3.0810485 -3.1561918 -3.339525 -3.3241506 -3.1205044 -3.0313325 -2.9176645 -2.7878876 -2.8257694 -3.0584311 -3.6266322 -4.2760544 -4.72671][-3.6466231 -4.1121688 -4.4287968 -4.7440996 -4.8761892 -4.7310185 -4.5197482 -4.2084332 -3.9412224 -3.7282028 -3.5583429 -3.6727309 -4.2317138 -4.9408894 -5.517581][-4.53061 -5.0198116 -5.3090715 -5.5668526 -5.7202168 -5.7077227 -5.3474379 -5.0340281 -4.7363095 -4.2678604 -3.9391847 -3.9112756 -4.308692 -4.9014235 -5.5318441][-4.8474765 -5.4822979 -5.9813242 -6.2454448 -6.1148982 -5.9557467 -5.6647372 -5.329257 -4.890624 -4.5604177 -4.3133206 -4.2167754 -4.50838 -4.9682326 -5.3560362]]...]
INFO - root - 2017-12-16 04:38:49.289953: step 12710, loss = 0.30, batch loss = 0.24 (28.7 examples/sec; 0.278 sec/batch; 24h:44m:03s remains)
INFO - root - 2017-12-16 04:38:52.089975: step 12720, loss = 0.24, batch loss = 0.18 (28.9 examples/sec; 0.276 sec/batch; 24h:33m:17s remains)
INFO - root - 2017-12-16 04:38:54.905989: step 12730, loss = 0.22, batch loss = 0.16 (29.5 examples/sec; 0.271 sec/batch; 24h:05m:02s remains)
INFO - root - 2017-12-16 04:38:57.722289: step 12740, loss = 0.61, batch loss = 0.55 (28.2 examples/sec; 0.284 sec/batch; 25h:14m:14s remains)
INFO - root - 2017-12-16 04:39:00.555337: step 12750, loss = 0.28, batch loss = 0.22 (27.9 examples/sec; 0.287 sec/batch; 25h:30m:30s remains)
INFO - root - 2017-12-16 04:39:03.373136: step 12760, loss = 0.20, batch loss = 0.14 (28.4 examples/sec; 0.281 sec/batch; 24h:59m:34s remains)
INFO - root - 2017-12-16 04:39:06.168980: step 12770, loss = 0.28, batch loss = 0.23 (28.6 examples/sec; 0.280 sec/batch; 24h:50m:12s remains)
INFO - root - 2017-12-16 04:39:08.998108: step 12780, loss = 0.21, batch loss = 0.15 (28.3 examples/sec; 0.283 sec/batch; 25h:08m:23s remains)
INFO - root - 2017-12-16 04:39:11.797500: step 12790, loss = 0.32, batch loss = 0.26 (28.4 examples/sec; 0.281 sec/batch; 24h:58m:23s remains)
INFO - root - 2017-12-16 04:39:14.585910: step 12800, loss = 0.29, batch loss = 0.24 (29.0 examples/sec; 0.275 sec/batch; 24h:27m:26s remains)
2017-12-16 04:39:15.029336: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5351231 -2.8028002 -3.1793756 -3.5382984 -3.9597435 -4.1766305 -4.2444105 -4.1180844 -3.8831975 -3.5735137 -3.4824142 -3.6812496 -3.9661131 -4.0410705 -4.0833511][-2.8527546 -3.2780848 -3.8771129 -4.432652 -4.6876411 -4.65458 -4.3967228 -3.8430133 -3.398242 -3.2780755 -3.4342933 -3.7087116 -4.0116544 -4.1698179 -4.2136106][-3.618561 -4.1984019 -4.7164946 -5.0772266 -5.0164928 -4.5781908 -3.7778988 -3.20367 -2.9352243 -2.7493896 -2.9392037 -3.5811982 -4.2690554 -4.5496178 -4.6341786][-4.4728518 -5.28463 -5.8738213 -6.0548077 -5.5770183 -4.38285 -3.0294681 -1.8452041 -1.3526502 -1.8786879 -2.6320114 -3.0667305 -3.4914875 -3.922272 -4.4731035][-5.3731928 -5.8635364 -6.0948753 -5.7845297 -4.7177196 -2.8922877 -1.2260211 -0.12532425 0.14190245 -0.15120316 -0.95673037 -2.1975329 -2.9981403 -3.2970304 -3.971498][-5.7146111 -5.8289552 -5.5386086 -4.6906085 -2.9274216 -0.80754447 1.135304 2.5542011 2.5687556 1.3984919 0.25582504 -0.40008545 -0.78068829 -1.8355129 -3.3525555][-5.4821706 -5.2682066 -4.4550071 -3.4088089 -1.566386 0.66714907 2.6539464 3.7543554 3.6363888 2.9404268 1.8377194 0.595489 -0.048156261 -0.77508235 -2.4104962][-4.7205849 -4.4987807 -3.8244355 -2.4175649 -0.38931561 1.7584119 3.3980694 4.0670824 3.7479334 2.7045927 1.9420128 1.6732488 1.1480408 -0.18317509 -2.3693302][-4.013958 -3.5205917 -2.7621641 -1.6941316 -0.33603239 1.5880947 3.0842166 3.56426 3.1992803 2.4491978 2.2725863 2.0266333 1.6336045 0.51204967 -1.724457][-3.4500268 -3.1504192 -2.6001856 -1.5630805 -0.22851944 0.77464437 1.2716722 1.7265897 1.6741462 1.4140134 1.6903682 1.628612 1.1613255 -0.24455976 -2.5246866][-3.3974252 -3.2507415 -3.0280209 -2.5898466 -1.5908887 -0.655565 -0.44820857 -0.80710578 -1.0565627 -0.67809248 0.18753004 0.42411804 0.0020036697 -1.3702798 -3.4876168][-4.05137 -3.5567973 -3.3606138 -3.1451774 -2.9301286 -2.9796438 -3.1670747 -3.1720986 -3.1166954 -2.9330146 -2.3482611 -1.6696243 -1.6777408 -2.9600792 -4.7029805][-4.033874 -3.998297 -4.0256276 -3.7757373 -4.018898 -4.1877174 -4.4382782 -4.7969484 -4.6845841 -3.9610295 -3.3322129 -3.2298503 -3.6189635 -4.5289154 -5.6862578][-4.540513 -4.3752928 -4.4096909 -4.7873855 -5.19324 -5.5782886 -6.13009 -6.1353593 -5.7774038 -5.4584246 -5.0645757 -4.8647904 -4.93691 -5.6747103 -6.5684862][-4.9224238 -4.7574925 -4.7292795 -5.0910153 -5.6459985 -6.1888003 -6.7114134 -7.0475864 -7.0891228 -6.54782 -5.9926929 -6.1047921 -6.5027275 -6.8569517 -7.1698804]]...]
INFO - root - 2017-12-16 04:39:17.884202: step 12810, loss = 0.28, batch loss = 0.22 (27.8 examples/sec; 0.287 sec/batch; 25h:31m:19s remains)
INFO - root - 2017-12-16 04:39:20.642232: step 12820, loss = 0.32, batch loss = 0.26 (29.2 examples/sec; 0.274 sec/batch; 24h:17m:30s remains)
INFO - root - 2017-12-16 04:39:23.480428: step 12830, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.285 sec/batch; 25h:19m:42s remains)
INFO - root - 2017-12-16 04:39:26.280944: step 12840, loss = 0.23, batch loss = 0.17 (28.1 examples/sec; 0.285 sec/batch; 25h:16m:58s remains)
INFO - root - 2017-12-16 04:39:29.125240: step 12850, loss = 0.29, batch loss = 0.23 (28.3 examples/sec; 0.283 sec/batch; 25h:07m:53s remains)
INFO - root - 2017-12-16 04:39:31.956712: step 12860, loss = 0.20, batch loss = 0.15 (28.3 examples/sec; 0.282 sec/batch; 25h:04m:38s remains)
INFO - root - 2017-12-16 04:39:34.787654: step 12870, loss = 0.36, batch loss = 0.30 (29.7 examples/sec; 0.269 sec/batch; 23h:53m:37s remains)
INFO - root - 2017-12-16 04:39:37.597889: step 12880, loss = 0.28, batch loss = 0.22 (28.8 examples/sec; 0.277 sec/batch; 24h:37m:57s remains)
INFO - root - 2017-12-16 04:39:40.420367: step 12890, loss = 0.29, batch loss = 0.23 (29.6 examples/sec; 0.270 sec/batch; 23h:58m:01s remains)
INFO - root - 2017-12-16 04:39:43.303010: step 12900, loss = 0.31, batch loss = 0.25 (26.9 examples/sec; 0.297 sec/batch; 26h:22m:08s remains)
2017-12-16 04:39:43.755840: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9602833 -5.0033994 -5.228075 -5.667635 -6.088088 -6.6170344 -6.8443403 -6.81756 -6.6936417 -6.2647314 -5.7832069 -5.4144669 -5.2444935 -5.2584133 -5.0927815][-4.4674554 -4.53887 -4.6713257 -4.9524956 -5.082056 -5.5371671 -5.7477326 -5.4893794 -5.1874652 -4.95587 -4.835494 -4.5547433 -4.3847728 -4.2502947 -4.0533352][-3.6695955 -3.6720874 -3.7752416 -3.8102665 -3.771513 -3.9084098 -3.8387246 -3.6496897 -3.48347 -3.2498183 -3.2469611 -3.3062823 -3.3686829 -3.1837354 -3.0643611][-2.7013202 -2.6893625 -2.7275712 -2.6346204 -2.2892716 -2.1743877 -1.9532709 -1.7194235 -1.6180515 -1.662514 -1.9884646 -2.2894869 -2.4449503 -2.3427556 -2.3097775][-2.0239878 -2.0326266 -1.8556886 -1.5313983 -0.88038778 -0.51047325 -0.18237877 0.018568039 -0.05739212 -0.37835264 -1.0456955 -1.5484977 -1.648196 -1.6981246 -1.7545588][-1.4504852 -1.5576096 -1.4116719 -0.90564346 0.1655612 0.84613848 1.3346453 1.619401 1.6560154 1.1842985 0.24338865 -0.55530047 -0.84344006 -0.86015892 -1.012358][-1.022234 -1.2043772 -1.1178219 -0.47859263 0.66161346 1.62012 2.3614721 2.591156 2.6606708 2.3775845 1.5382061 0.63592434 0.10752726 -0.12200642 -0.42206621][-0.92701578 -1.4180295 -1.5588367 -0.75721836 0.42289448 1.4730802 2.3348203 2.7867827 3.0834165 2.7337666 1.8959208 1.1555381 0.64447832 0.18113852 -0.33194828][-1.1042404 -1.5158279 -1.5916183 -1.0860291 -0.2243104 0.91110229 1.8517132 2.1998978 2.4391212 2.291131 1.7052813 0.98429203 0.3394537 -0.14622355 -0.66694903][-0.95666647 -1.6500125 -2.0408475 -1.4171107 -0.3280921 0.51788282 1.130156 1.4855156 1.7476583 1.5564942 1.0176721 0.3223443 -0.40349102 -1.042006 -1.6589119][-1.2812054 -1.7528937 -2.1128891 -1.9964659 -1.376848 -0.4284749 0.41754961 0.54009104 0.49164248 0.31611204 -0.093869209 -0.67082596 -1.4970171 -2.2257693 -2.7874603][-1.833719 -2.2314487 -2.7134082 -2.4965897 -1.9744387 -1.4503376 -1.1176302 -0.89116859 -0.54598355 -0.81212568 -1.4414945 -1.9355912 -2.6759455 -3.3308394 -3.7073302][-2.6517372 -2.8934991 -3.2474284 -3.3030844 -3.1003671 -2.4009819 -1.8346286 -1.685931 -1.6281612 -1.7156537 -1.8895724 -2.3317122 -3.008203 -3.5729644 -3.7567036][-2.6909766 -2.7736335 -2.9846659 -3.0415978 -2.8898315 -2.5166707 -2.1923187 -1.7990651 -1.5708606 -1.7786078 -2.0897489 -2.4124923 -2.8406935 -3.2988067 -3.3617935][-3.4207191 -3.4268785 -3.5896487 -3.5081847 -3.1863327 -2.8368673 -2.4276586 -2.0213182 -1.7745252 -1.8357317 -2.0217731 -2.2290866 -2.4833708 -2.7770004 -2.7829282]]...]
INFO - root - 2017-12-16 04:39:46.546925: step 12910, loss = 0.24, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 25h:22m:34s remains)
INFO - root - 2017-12-16 04:39:49.420356: step 12920, loss = 0.32, batch loss = 0.26 (28.7 examples/sec; 0.278 sec/batch; 24h:42m:58s remains)
INFO - root - 2017-12-16 04:39:52.296121: step 12930, loss = 0.34, batch loss = 0.28 (26.8 examples/sec; 0.298 sec/batch; 26h:29m:40s remains)
INFO - root - 2017-12-16 04:39:55.111959: step 12940, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.284 sec/batch; 25h:13m:45s remains)
INFO - root - 2017-12-16 04:39:57.975989: step 12950, loss = 0.22, batch loss = 0.17 (24.7 examples/sec; 0.324 sec/batch; 28h:45m:23s remains)
INFO - root - 2017-12-16 04:40:00.828215: step 12960, loss = 0.25, batch loss = 0.19 (29.5 examples/sec; 0.271 sec/batch; 24h:02m:01s remains)
INFO - root - 2017-12-16 04:40:03.691052: step 12970, loss = 0.37, batch loss = 0.31 (28.3 examples/sec; 0.282 sec/batch; 25h:02m:52s remains)
INFO - root - 2017-12-16 04:40:06.510552: step 12980, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.284 sec/batch; 25h:11m:56s remains)
INFO - root - 2017-12-16 04:40:09.397561: step 12990, loss = 0.26, batch loss = 0.20 (27.3 examples/sec; 0.294 sec/batch; 26h:03m:04s remains)
INFO - root - 2017-12-16 04:40:12.260347: step 13000, loss = 0.27, batch loss = 0.22 (28.0 examples/sec; 0.286 sec/batch; 25h:21m:34s remains)
2017-12-16 04:40:12.722497: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3625207 -2.2488511 -2.2426784 -2.4784417 -3.0770853 -3.655149 -4.0055628 -4.5057755 -4.8819652 -4.9354391 -5.0112972 -5.3647032 -5.4471426 -5.3487649 -5.1061692][-1.6292794 -1.4488578 -1.6199012 -2.1294119 -2.8324213 -3.7420754 -4.5000272 -4.8738704 -5.1225963 -5.4263988 -5.5034409 -5.4803152 -5.5934086 -5.796658 -5.5912871][-0.70085096 -0.40070581 -0.57388067 -1.3248358 -2.0794539 -2.8894267 -3.7475142 -4.6585937 -5.0121307 -4.9453735 -5.0597692 -5.3675609 -5.4046688 -5.4829884 -5.4014339][-0.02855587 0.11795521 -0.071032524 -0.62373257 -1.3624213 -2.1298053 -2.717083 -3.1006036 -3.5216036 -4.0840716 -4.5123682 -4.7106566 -4.95188 -5.2017331 -4.814116][-0.29203367 -0.10415363 -0.054736137 -0.32780933 -0.58105254 -0.85159683 -1.2092483 -1.5363016 -1.8661008 -2.3225389 -2.9079692 -3.5509334 -4.0789523 -4.1726875 -3.8143761][-0.9559412 -0.46763086 -0.01692152 -0.0671711 -0.042108059 -0.036521912 0.029848576 -0.17677832 -0.5356946 -0.83352709 -1.2453065 -1.8315628 -2.3135178 -2.330929 -1.8813355][-1.7889183 -1.0835891 -0.49669933 -0.20929766 0.16343498 0.479661 0.71089697 0.57060337 0.17902613 -0.085420132 -0.50076151 -0.8238771 -0.76046467 -0.58017492 -0.049961567][-2.52662 -1.8620954 -1.2534194 -0.7990737 -0.19842196 0.37202072 0.86754465 1.0369062 0.73362875 0.37225294 -0.10064936 -0.33255482 -0.19089508 0.25009537 0.97258282][-3.1885982 -2.539052 -1.9789076 -1.4822674 -0.8402524 -0.0058002472 0.60776377 0.59985638 0.35519361 -0.064498425 -0.46999574 -0.44136739 -0.23986769 0.32648611 1.0843759][-4.5119848 -3.8752756 -3.313787 -2.7171156 -1.9910467 -1.2131159 -0.54262257 -0.47444606 -0.64940929 -0.85103965 -0.94847083 -0.97952843 -0.78950953 -0.10101271 0.599576][-4.8119154 -4.2759767 -3.8204582 -3.3180723 -2.5703976 -1.971067 -1.6105735 -1.6397514 -1.7404277 -2.1501765 -2.393791 -2.1195154 -1.7304745 -1.0699697 -0.37712812][-4.918623 -4.61813 -4.2509465 -3.8862925 -3.3763542 -3.0135767 -2.6419191 -2.5458162 -2.6595373 -2.8931274 -3.0447807 -3.0980778 -2.9137392 -2.398001 -1.9656889][-4.2380738 -4.1095867 -4.0723333 -3.9445066 -3.7997332 -3.8333919 -3.6605773 -3.4217513 -3.419126 -3.5746272 -3.5480237 -3.5335307 -3.60474 -3.4756794 -3.1803589][-3.7180617 -3.4817448 -3.4035993 -3.3596282 -3.3787766 -3.3240662 -3.1604342 -3.1184669 -3.0148325 -2.9805946 -3.0796373 -3.2760262 -3.4972303 -3.6075206 -3.5863585][-3.5150495 -3.1819119 -3.0838106 -2.9507303 -2.9471943 -2.9705439 -2.8397098 -2.5493708 -2.1911194 -2.1050901 -2.1111686 -2.2005954 -2.5726521 -2.932303 -3.2019658]]...]
INFO - root - 2017-12-16 04:40:15.576730: step 13010, loss = 0.30, batch loss = 0.24 (28.6 examples/sec; 0.280 sec/batch; 24h:48m:28s remains)
INFO - root - 2017-12-16 04:40:18.396585: step 13020, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.279 sec/batch; 24h:45m:43s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 04:40:21.225001: step 13030, loss = 0.28, batch loss = 0.22 (27.0 examples/sec; 0.296 sec/batch; 26h:17m:40s remains)
INFO - root - 2017-12-16 04:40:24.076648: step 13040, loss = 0.29, batch loss = 0.23 (27.4 examples/sec; 0.292 sec/batch; 25h:54m:52s remains)
INFO - root - 2017-12-16 04:40:26.947016: step 13050, loss = 0.31, batch loss = 0.25 (28.2 examples/sec; 0.284 sec/batch; 25h:11m:03s remains)
INFO - root - 2017-12-16 04:40:29.792395: step 13060, loss = 0.25, batch loss = 0.19 (29.1 examples/sec; 0.275 sec/batch; 24h:25m:49s remains)
INFO - root - 2017-12-16 04:40:32.623150: step 13070, loss = 0.41, batch loss = 0.35 (27.8 examples/sec; 0.287 sec/batch; 25h:30m:21s remains)
INFO - root - 2017-12-16 04:40:35.473225: step 13080, loss = 0.21, batch loss = 0.15 (28.3 examples/sec; 0.283 sec/batch; 25h:07m:12s remains)
INFO - root - 2017-12-16 04:40:38.344546: step 13090, loss = 0.31, batch loss = 0.25 (29.2 examples/sec; 0.274 sec/batch; 24h:18m:48s remains)
INFO - root - 2017-12-16 04:40:41.175223: step 13100, loss = 0.28, batch loss = 0.22 (29.3 examples/sec; 0.273 sec/batch; 24h:12m:31s remains)
2017-12-16 04:40:41.631700: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9615164 -3.8932667 -3.7577353 -4.0748997 -4.3768229 -4.5919986 -4.5992675 -4.3791451 -4.1208677 -3.7105882 -3.3560004 -3.0134709 -2.7219515 -2.4778986 -2.2685206][-4.0461407 -3.9719298 -3.9201217 -4.0155897 -4.2296076 -4.7513881 -5.0504713 -4.9404073 -4.6913204 -4.3343391 -4.0211444 -3.654273 -3.3228478 -2.8701286 -2.434093][-4.3277049 -3.9352505 -3.6896415 -3.5828345 -3.6092215 -3.961195 -4.389411 -4.7968764 -4.9027591 -4.7906427 -4.625988 -4.3955708 -4.1240153 -3.6543686 -3.014029][-4.6388493 -4.06937 -3.4509869 -2.9077811 -2.6858919 -2.765183 -2.9738922 -3.35748 -3.8205373 -4.2819114 -4.5107141 -4.7248812 -4.7529883 -4.431901 -3.8069923][-4.5752535 -3.7870865 -2.8684883 -1.8025661 -1.0726619 -0.59779167 -0.62763977 -0.99928331 -1.6059158 -2.5867529 -3.5818722 -4.3055587 -4.6574388 -4.7329865 -4.3294506][-4.2263927 -3.2935872 -2.0102277 -0.48150706 0.90184069 1.7249131 1.9667654 1.6758027 0.96562338 -0.30287695 -1.7980442 -3.2604589 -4.3487749 -4.7187829 -4.4486742][-3.6027889 -2.655056 -1.3871388 0.33711958 2.0612617 3.34726 4.2191114 4.1688395 3.5425363 2.0254278 0.17509365 -1.6813221 -3.1519065 -4.0801396 -4.2512779][-3.0697403 -2.287173 -1.2777154 0.46487761 2.2948494 3.9137917 5.049758 5.1992817 4.6873236 3.1936798 1.3705392 -0.74784636 -2.5272141 -3.6718197 -4.2389674][-2.8100824 -2.2475057 -1.5330462 -0.17730284 1.2095528 2.7840586 3.9248228 4.2832174 3.9947186 2.572341 0.7563529 -1.3253019 -3.043823 -4.0363579 -4.4419837][-2.687531 -2.2963722 -2.1626358 -1.4154375 -0.46636581 0.44902229 1.2433271 1.6813784 1.6120944 0.60733557 -0.89180827 -2.6580667 -4.1505589 -4.93228 -5.08898][-3.1439223 -2.7475276 -2.6637571 -2.5821662 -2.3473036 -1.978313 -1.4354746 -1.3202484 -1.4899809 -1.9816346 -2.9876833 -4.4079971 -5.4988613 -5.9892969 -5.9760137][-3.8198206 -3.3522196 -3.2558541 -3.4348793 -3.6850808 -4.1469479 -4.3764806 -4.3579893 -4.2096338 -4.5156527 -4.9426928 -5.8299069 -6.6312647 -6.9822559 -6.9241896][-3.9522169 -3.3885818 -3.4690366 -3.8128004 -4.5227561 -5.2220092 -5.991004 -6.5076346 -6.4242134 -6.174509 -5.9954395 -6.2497225 -6.6732368 -6.9858503 -7.0334105][-4.5580206 -3.676492 -3.4489708 -3.592648 -4.282887 -5.2940493 -6.3171887 -6.9251337 -6.9008231 -6.3497477 -5.8661032 -5.7960491 -6.1076851 -6.2708721 -6.4038634][-4.663887 -3.4322391 -2.6435397 -2.8190866 -3.5593419 -4.2658668 -4.9748726 -5.8384495 -6.1996684 -5.7599583 -5.180511 -4.70076 -4.6347013 -4.9190507 -5.4543886]]...]
INFO - root - 2017-12-16 04:40:44.442238: step 13110, loss = 0.30, batch loss = 0.25 (29.0 examples/sec; 0.276 sec/batch; 24h:30m:07s remains)
INFO - root - 2017-12-16 04:40:47.319247: step 13120, loss = 0.20, batch loss = 0.15 (28.7 examples/sec; 0.279 sec/batch; 24h:44m:36s remains)
INFO - root - 2017-12-16 04:40:50.143668: step 13130, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.286 sec/batch; 25h:22m:08s remains)
INFO - root - 2017-12-16 04:40:52.971573: step 13140, loss = 0.28, batch loss = 0.22 (28.5 examples/sec; 0.281 sec/batch; 24h:54m:40s remains)
INFO - root - 2017-12-16 04:40:55.809743: step 13150, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 25h:17m:05s remains)
INFO - root - 2017-12-16 04:40:58.645017: step 13160, loss = 0.26, batch loss = 0.20 (27.5 examples/sec; 0.291 sec/batch; 25h:48m:22s remains)
INFO - root - 2017-12-16 04:41:01.438149: step 13170, loss = 0.24, batch loss = 0.19 (27.5 examples/sec; 0.291 sec/batch; 25h:49m:50s remains)
INFO - root - 2017-12-16 04:41:04.301179: step 13180, loss = 0.25, batch loss = 0.19 (26.5 examples/sec; 0.302 sec/batch; 26h:44m:50s remains)
INFO - root - 2017-12-16 04:41:07.160128: step 13190, loss = 0.31, batch loss = 0.25 (28.5 examples/sec; 0.280 sec/batch; 24h:51m:59s remains)
INFO - root - 2017-12-16 04:41:10.012166: step 13200, loss = 0.28, batch loss = 0.22 (29.0 examples/sec; 0.276 sec/batch; 24h:26m:18s remains)
2017-12-16 04:41:10.479827: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9853268 -3.9684815 -3.8137984 -3.5168419 -3.3809764 -3.3186564 -3.2990856 -3.3988614 -3.4516819 -3.3806241 -3.2419019 -3.0394626 -2.88843 -2.8615024 -2.9140902][-3.8591318 -3.8580658 -3.6864839 -3.3756537 -3.2067928 -3.2842741 -3.4507687 -3.4400277 -3.33445 -3.2460608 -3.1670632 -3.0124073 -2.9684818 -3.0528855 -3.1941535][-3.6301887 -3.3937764 -3.0810525 -2.8656731 -2.825315 -2.9769032 -3.1774487 -3.3126888 -3.3148713 -3.2119806 -3.1121912 -3.0111017 -3.0370893 -3.1960216 -3.3913302][-3.0360723 -2.7890203 -2.5841463 -2.5016236 -2.5950081 -2.7876773 -2.9979329 -2.9425645 -2.7312956 -2.5868552 -2.5031588 -2.6663179 -2.9403396 -3.3503041 -3.6357596][-2.0936635 -1.6425078 -1.5106218 -1.616848 -1.8391802 -2.0417926 -2.1616631 -2.0033917 -1.6860993 -1.490865 -1.4588096 -1.6398752 -1.9113982 -2.4840481 -2.9181211][-1.2693985 -0.82099366 -0.64076686 -0.80405092 -1.0730519 -1.2594192 -1.1393626 -0.82347178 -0.48367691 -0.30700207 -0.40770149 -0.86963892 -1.4061449 -1.9095056 -2.0585814][-0.98110127 -0.53068972 -0.40321827 -0.51158214 -0.57486343 -0.5026722 -0.17307997 0.26357508 0.58177042 0.44958353 0.071552277 -0.524853 -1.1392391 -1.5623555 -1.5651484][-0.97891927 -0.37465429 -0.20415163 -0.20390987 -0.018815041 0.44729376 0.92375374 1.3090034 1.4415741 0.94360495 0.079276085 -0.787369 -1.4255493 -1.7657564 -1.7428255][-1.3635905 -0.71310544 -0.29456806 0.01133585 0.41174507 1.0386286 1.6042538 1.7928739 1.5074186 0.77272129 -0.21940231 -1.3365083 -2.1272585 -2.2078049 -1.9743655][-1.781868 -1.1538618 -0.73196793 -0.31938219 0.31734753 1.0062985 1.4110675 1.3433738 0.74188948 -0.22488308 -1.312068 -2.2880685 -2.992784 -3.0187492 -2.6868138][-2.4522619 -1.9444821 -1.4752028 -0.96399236 -0.32824421 0.21156645 0.48157692 0.099487305 -0.88967991 -1.8734386 -2.714757 -3.3725736 -3.8100815 -3.6857948 -3.2713985][-3.4440634 -2.90032 -2.3450549 -1.765902 -1.2143826 -0.95490408 -1.1175036 -1.5027552 -2.20399 -3.2818806 -4.186595 -4.4701242 -4.3842874 -3.9638126 -3.5134861][-4.38641 -4.1265516 -3.5789704 -3.081871 -2.555377 -2.29972 -2.4561744 -3.0381656 -3.8105998 -4.2417793 -4.4678664 -4.5480118 -4.4022226 -3.8385572 -3.2998252][-5.7646785 -5.4870663 -4.8684216 -4.4234157 -3.8889942 -3.7028985 -3.8202386 -4.187551 -4.6062708 -4.8105092 -4.7652841 -4.4429712 -4.0552726 -3.3684459 -2.8486996][-6.4861846 -6.3739905 -5.9210567 -5.3943543 -4.9070783 -4.6283846 -4.5185094 -4.7500648 -5.0391436 -5.0846267 -4.791131 -4.2760458 -3.807601 -3.1947775 -2.7781363]]...]
INFO - root - 2017-12-16 04:41:13.302337: step 13210, loss = 0.24, batch loss = 0.18 (27.9 examples/sec; 0.286 sec/batch; 25h:23m:25s remains)
INFO - root - 2017-12-16 04:41:16.060387: step 13220, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 24h:59m:12s remains)
INFO - root - 2017-12-16 04:41:18.891358: step 13230, loss = 0.27, batch loss = 0.21 (29.0 examples/sec; 0.276 sec/batch; 24h:26m:44s remains)
INFO - root - 2017-12-16 04:41:21.715866: step 13240, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.284 sec/batch; 25h:11m:02s remains)
INFO - root - 2017-12-16 04:41:24.572910: step 13250, loss = 0.29, batch loss = 0.23 (27.2 examples/sec; 0.294 sec/batch; 26h:02m:09s remains)
INFO - root - 2017-12-16 04:41:27.396026: step 13260, loss = 0.23, batch loss = 0.17 (28.8 examples/sec; 0.278 sec/batch; 24h:37m:02s remains)
INFO - root - 2017-12-16 04:41:30.260376: step 13270, loss = 0.20, batch loss = 0.14 (27.9 examples/sec; 0.287 sec/batch; 25h:24m:19s remains)
INFO - root - 2017-12-16 04:41:33.037091: step 13280, loss = 0.38, batch loss = 0.32 (27.7 examples/sec; 0.289 sec/batch; 25h:38m:01s remains)
INFO - root - 2017-12-16 04:41:35.852287: step 13290, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.276 sec/batch; 24h:30m:33s remains)
INFO - root - 2017-12-16 04:41:38.726409: step 13300, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.277 sec/batch; 24h:35m:05s remains)
2017-12-16 04:41:39.215243: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3711143 -4.2618766 -4.2139397 -4.6240468 -5.279501 -5.9890513 -6.6022234 -6.6891365 -6.6627836 -6.1989794 -5.7029309 -5.381247 -4.7987742 -4.0989952 -3.4089453][-5.0102034 -5.507638 -5.9682446 -6.1766596 -6.9587831 -7.1328092 -7.1149907 -6.9658976 -6.5007043 -6.1239591 -5.9197865 -5.4741979 -4.7528839 -4.1560283 -3.6207528][-5.6253877 -6.2417846 -6.7315435 -6.9309559 -6.9006939 -6.5189953 -6.470439 -6.1033039 -5.7006855 -5.39579 -5.0713038 -5.08716 -5.0728045 -4.6430545 -4.0419216][-6.0668354 -6.990046 -7.3158236 -6.989903 -6.3476286 -5.2742867 -4.4103312 -3.7667484 -3.493438 -3.9586368 -4.6066704 -4.8364096 -4.8870816 -4.9327154 -4.6650381][-6.0350213 -6.9855013 -7.0491986 -6.187602 -4.6165857 -2.8200986 -1.5486321 -0.64638352 -0.72012234 -1.8129892 -3.0161157 -4.3182549 -5.2047658 -5.221313 -4.6961236][-4.6728516 -5.4101033 -5.4820542 -4.3431559 -1.9414628 0.58617258 2.4384904 3.2608385 2.5441484 0.75521326 -1.6458199 -3.6928241 -4.897387 -5.39234 -5.0731053][-3.3674173 -3.9885991 -3.626539 -2.0232117 0.30345488 2.9259257 5.0264378 5.7464867 4.7331 2.3005319 -0.54980063 -3.1037126 -4.8154879 -5.2690196 -4.7951684][-2.7120018 -3.4109588 -3.1949296 -1.4389319 1.2349238 4.0266562 5.9158964 6.0702715 4.8480415 2.2897072 -0.84455633 -3.3878148 -4.7815433 -5.1084404 -4.6673346][-2.6105487 -3.0907385 -2.5827918 -1.409745 0.26628685 2.6879296 4.5957518 4.85126 3.4426603 0.75314 -2.0956385 -4.3025274 -5.4938731 -5.52419 -4.8614635][-2.5815051 -3.2464223 -2.9535949 -1.9414189 -0.41166115 1.0173202 1.7626133 1.9688826 0.88187313 -1.4530752 -3.8892584 -5.70685 -6.3399634 -6.0908289 -5.3255277][-3.322659 -3.6276464 -3.4408374 -3.2336454 -2.8037834 -1.623579 -0.5010736 -0.65678024 -2.0122938 -3.5700803 -5.1806126 -6.5597324 -6.9943495 -6.4513612 -5.5346088][-4.3402557 -4.5676069 -4.6854348 -4.2737837 -3.702775 -3.4553294 -3.2562051 -3.2815545 -3.8561869 -5.0463886 -6.3235612 -6.9653392 -6.9126873 -6.3092036 -5.4785933][-5.2634931 -5.3637085 -5.4760857 -5.4137464 -5.3153334 -4.7361445 -4.3192935 -4.7077656 -5.3506947 -5.9623 -6.6872711 -7.0411654 -6.844141 -6.1599932 -5.4115629][-6.0330338 -5.9880362 -5.9647036 -5.8881435 -5.7525992 -5.343853 -5.2184944 -5.1353245 -5.359148 -5.9642854 -6.3909111 -6.3743138 -6.0866342 -5.4876471 -4.9067769][-6.6803045 -6.5870285 -6.4485865 -6.1928763 -5.8841386 -5.4920864 -5.2561016 -5.015573 -5.0590963 -5.3747392 -5.6059647 -5.4890447 -5.1010528 -4.5209026 -4.0122743]]...]
INFO - root - 2017-12-16 04:41:42.030079: step 13310, loss = 0.30, batch loss = 0.24 (27.9 examples/sec; 0.287 sec/batch; 25h:27m:22s remains)
INFO - root - 2017-12-16 04:41:44.817945: step 13320, loss = 0.25, batch loss = 0.19 (26.9 examples/sec; 0.297 sec/batch; 26h:19m:28s remains)
INFO - root - 2017-12-16 04:41:47.634286: step 13330, loss = 0.29, batch loss = 0.23 (29.0 examples/sec; 0.276 sec/batch; 24h:26m:20s remains)
INFO - root - 2017-12-16 04:41:50.464821: step 13340, loss = 0.23, batch loss = 0.17 (29.4 examples/sec; 0.272 sec/batch; 24h:05m:33s remains)
INFO - root - 2017-12-16 04:41:53.295318: step 13350, loss = 0.25, batch loss = 0.19 (27.1 examples/sec; 0.295 sec/batch; 26h:09m:03s remains)
INFO - root - 2017-12-16 04:41:56.108594: step 13360, loss = 0.25, batch loss = 0.20 (28.2 examples/sec; 0.284 sec/batch; 25h:09m:10s remains)
INFO - root - 2017-12-16 04:41:58.972323: step 13370, loss = 0.28, batch loss = 0.22 (27.4 examples/sec; 0.292 sec/batch; 25h:51m:59s remains)
INFO - root - 2017-12-16 04:42:01.840184: step 13380, loss = 0.25, batch loss = 0.19 (25.9 examples/sec; 0.309 sec/batch; 27h:22m:09s remains)
INFO - root - 2017-12-16 04:42:04.615628: step 13390, loss = 0.27, batch loss = 0.21 (29.4 examples/sec; 0.272 sec/batch; 24h:08m:53s remains)
INFO - root - 2017-12-16 04:42:07.466199: step 13400, loss = 0.27, batch loss = 0.21 (28.1 examples/sec; 0.285 sec/batch; 25h:13m:49s remains)
2017-12-16 04:42:07.944791: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1400743 -2.2943897 -2.4574947 -2.6162314 -2.6781816 -2.7007084 -2.7177515 -2.7123613 -2.6802945 -2.6365733 -2.5916028 -2.5771632 -2.6471686 -2.7646632 -2.8592203][-2.4159186 -2.668854 -2.8726778 -3.1404438 -3.3128591 -3.3795822 -3.298522 -3.0833519 -2.9214296 -2.8931713 -2.9059768 -2.9138522 -2.9980161 -3.1800411 -3.3379664][-3.0164037 -3.4757037 -3.766752 -3.9220719 -3.9296222 -3.906558 -3.7111464 -3.4676294 -3.2245221 -3.0804882 -3.0547571 -3.2136953 -3.4721203 -3.6273308 -3.7442756][-3.9363959 -4.2475519 -4.3531556 -4.3636918 -4.3042884 -3.9710381 -3.565599 -3.2064228 -2.9011273 -2.8753855 -2.8843699 -3.1430197 -3.5355983 -3.8962021 -4.0933576][-4.5014157 -4.7192354 -4.5816336 -4.2571087 -3.8284342 -3.3766189 -2.88702 -2.2620554 -1.9643891 -1.9748425 -2.101579 -2.5370586 -2.9797108 -3.5047443 -3.980108][-4.7603135 -4.6847334 -4.1391592 -3.4249268 -2.6853046 -2.1470218 -1.6069198 -1.0656736 -0.74607372 -0.66053915 -0.89419389 -1.631659 -2.4141414 -3.0484879 -3.6019802][-4.3770471 -4.0887661 -3.3755894 -2.3607967 -1.3797259 -0.43463945 0.29239225 0.59501553 0.68635511 0.44605875 -0.089373112 -0.79021645 -1.7136676 -2.8589277 -3.8121338][-3.9223349 -3.3962636 -2.3863785 -1.1913366 0.011967182 0.95577288 1.5313201 1.9356284 1.9231768 1.3151689 0.40554667 -0.64278293 -1.8235836 -2.9507055 -3.9679053][-2.8863683 -2.3621588 -1.5708287 -0.43063426 0.76263094 1.7739167 2.1970205 2.4168439 2.15875 1.3938298 0.32005548 -0.89759707 -2.011498 -3.366529 -4.4911618][-2.1279218 -1.8106301 -1.0842726 -0.21973944 0.892323 1.7924228 1.9581947 1.9706345 1.531939 0.53145313 -0.74544239 -1.981885 -2.9966927 -3.8934736 -4.6155887][-1.857578 -1.319078 -0.9185338 -0.46803546 0.43802834 1.1617413 1.4046092 1.2163625 0.583683 -0.49494123 -1.828342 -2.9246726 -3.7859845 -4.5021105 -5.0402269][-1.9763079 -1.4901609 -1.0566449 -0.60077167 -0.083824158 0.32684374 0.25226355 0.05362463 -0.43726683 -1.3292055 -2.5494657 -3.6000459 -4.230473 -4.6461477 -4.9363871][-1.7242723 -1.2312508 -1.1039474 -0.93095684 -0.73261285 -0.37519741 -0.42972136 -0.9406507 -1.6445024 -2.3802319 -3.2569969 -4.00343 -4.3530369 -4.3851962 -4.3860435][-1.7649007 -1.0881674 -0.78779244 -0.727839 -0.69789672 -0.73574448 -0.99888468 -1.4662554 -2.1036608 -2.9237792 -3.6650014 -4.0565743 -4.1249442 -3.9256568 -3.6607065][-1.4101429 -0.79729819 -0.38283348 -0.35364914 -0.31714392 -0.39526844 -0.74907351 -1.4263253 -2.1683393 -2.7791486 -3.3001914 -3.6694887 -3.74502 -3.5196705 -3.1968498]]...]
INFO - root - 2017-12-16 04:42:10.781593: step 13410, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.284 sec/batch; 25h:09m:01s remains)
INFO - root - 2017-12-16 04:42:13.647983: step 13420, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.277 sec/batch; 24h:31m:05s remains)
INFO - root - 2017-12-16 04:42:16.457069: step 13430, loss = 0.29, batch loss = 0.23 (27.3 examples/sec; 0.293 sec/batch; 25h:59m:46s remains)
INFO - root - 2017-12-16 04:42:19.244768: step 13440, loss = 0.29, batch loss = 0.23 (29.3 examples/sec; 0.273 sec/batch; 24h:14m:16s remains)
INFO - root - 2017-12-16 04:42:22.067434: step 13450, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.282 sec/batch; 24h:57m:06s remains)
INFO - root - 2017-12-16 04:42:24.897391: step 13460, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 24h:36m:58s remains)
INFO - root - 2017-12-16 04:42:27.708935: step 13470, loss = 0.31, batch loss = 0.25 (26.0 examples/sec; 0.307 sec/batch; 27h:14m:36s remains)
INFO - root - 2017-12-16 04:42:30.562605: step 13480, loss = 0.26, batch loss = 0.20 (27.8 examples/sec; 0.288 sec/batch; 25h:31m:47s remains)
INFO - root - 2017-12-16 04:42:33.391520: step 13490, loss = 0.26, batch loss = 0.20 (29.6 examples/sec; 0.270 sec/batch; 23h:55m:21s remains)
INFO - root - 2017-12-16 04:42:36.206662: step 13500, loss = 0.22, batch loss = 0.16 (27.7 examples/sec; 0.288 sec/batch; 25h:32m:47s remains)
2017-12-16 04:42:36.679212: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.697546 -5.7474489 -5.6009269 -5.5484195 -5.6511583 -5.6659217 -5.6512618 -5.7258658 -5.8078752 -5.703948 -5.6769733 -5.5918732 -5.4221663 -5.2224956 -4.9302664][-5.7071962 -5.5618658 -5.4808154 -5.5934153 -5.7087216 -5.9091344 -6.0631323 -6.0120516 -5.9658284 -6.078783 -6.1266122 -5.9754996 -5.7914457 -5.6183805 -5.3410182][-4.9438581 -4.9002562 -4.9018788 -4.9502711 -5.132638 -5.3599954 -5.3727379 -5.4114437 -5.6090813 -5.6549997 -5.7931533 -6.06701 -6.1701651 -5.8122625 -5.3231325][-4.1210146 -3.7012589 -3.4827588 -3.6423652 -3.819761 -3.9328406 -3.841013 -3.8835859 -4.0712795 -4.3698292 -4.6764836 -5.1705627 -5.4895406 -5.321764 -4.9302115][-2.5777857 -1.8054428 -1.4651301 -1.707181 -1.9362543 -1.9130933 -1.6620865 -1.6481087 -1.9426773 -2.6164298 -3.3383968 -4.0130482 -4.3824325 -4.2428575 -3.7451272][-1.5199909 -0.57034826 -0.04298687 0.011345387 0.03096962 0.21547127 0.636086 0.869534 0.53040314 -0.31533527 -1.2764058 -2.3696404 -3.0445209 -3.0334187 -2.6284859][-0.68055367 0.14289045 0.55693483 0.675406 1.0529108 1.8497443 2.7313662 2.9697094 2.4560175 1.4914513 0.42304993 -0.72455049 -1.5282321 -1.8334491 -1.913758][-0.21788979 0.43274403 0.866251 1.1084843 1.3640332 2.0942712 3.14402 3.8410339 3.6179781 2.5725145 1.218678 -0.044209003 -0.79096627 -1.0243907 -1.2109909][-0.28495502 0.11016703 0.27927446 0.43740988 0.87623692 1.7039404 2.5916805 3.1133466 3.0211649 2.4048409 1.3067884 0.14250946 -0.6026969 -0.84449863 -0.96007776][-0.539592 -0.4348762 -0.55190706 -0.43257475 -0.093770504 0.58050823 1.3087792 1.7710381 1.6058788 1.0597157 0.28115845 -0.435874 -0.89911127 -1.0328608 -1.1774406][-1.2473183 -1.1351442 -1.0987215 -1.0901995 -0.90478659 -0.54572368 -0.1382432 0.056691647 -0.076699734 -0.43912458 -0.96435213 -1.3157425 -1.4998283 -1.4976339 -1.513736][-1.6584265 -1.4324894 -1.4267991 -1.5425923 -1.4495199 -1.2793059 -1.1822526 -1.259346 -1.5519488 -1.9043751 -2.2046673 -2.2690535 -2.1511943 -1.911536 -1.7130799][-2.038305 -1.7110059 -1.6281571 -1.6637409 -1.6908479 -1.6793191 -1.8369656 -2.1721363 -2.7196302 -3.1682491 -3.3163557 -3.0733662 -2.7696948 -2.320817 -1.9726486][-2.1352766 -1.7019591 -1.5507948 -1.5258791 -1.5697761 -1.6097274 -1.7833726 -2.3068666 -3.031126 -3.7087827 -3.9768679 -3.7284751 -3.2086871 -2.6804473 -2.2920334][-2.2325606 -1.7847338 -1.5800602 -1.4952328 -1.420738 -1.4391477 -1.5355997 -2.082628 -2.8503356 -3.4966972 -3.8666766 -4.0033412 -3.8366914 -3.3665895 -2.9929595]]...]
INFO - root - 2017-12-16 04:42:39.545252: step 13510, loss = 0.20, batch loss = 0.14 (28.8 examples/sec; 0.278 sec/batch; 24h:39m:09s remains)
INFO - root - 2017-12-16 04:42:42.341096: step 13520, loss = 0.30, batch loss = 0.25 (29.1 examples/sec; 0.275 sec/batch; 24h:20m:09s remains)
INFO - root - 2017-12-16 04:42:45.166234: step 13530, loss = 0.25, batch loss = 0.19 (28.8 examples/sec; 0.278 sec/batch; 24h:35m:21s remains)
INFO - root - 2017-12-16 04:42:47.980994: step 13540, loss = 0.18, batch loss = 0.12 (28.4 examples/sec; 0.282 sec/batch; 24h:58m:08s remains)
INFO - root - 2017-12-16 04:42:50.808992: step 13550, loss = 0.26, batch loss = 0.20 (27.9 examples/sec; 0.287 sec/batch; 25h:23m:32s remains)
INFO - root - 2017-12-16 04:42:53.668931: step 13560, loss = 0.27, batch loss = 0.21 (28.6 examples/sec; 0.280 sec/batch; 24h:47m:49s remains)
INFO - root - 2017-12-16 04:42:56.493670: step 13570, loss = 0.23, batch loss = 0.18 (29.3 examples/sec; 0.273 sec/batch; 24h:13m:39s remains)
INFO - root - 2017-12-16 04:42:59.330563: step 13580, loss = 0.22, batch loss = 0.16 (26.7 examples/sec; 0.300 sec/batch; 26h:34m:00s remains)
INFO - root - 2017-12-16 04:43:02.157623: step 13590, loss = 0.25, batch loss = 0.19 (29.0 examples/sec; 0.276 sec/batch; 24h:25m:57s remains)
INFO - root - 2017-12-16 04:43:04.987069: step 13600, loss = 0.28, batch loss = 0.22 (27.1 examples/sec; 0.296 sec/batch; 26h:10m:58s remains)
2017-12-16 04:43:05.478911: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1323321 -3.2033377 -3.4035707 -4.0217609 -4.4733329 -4.6860518 -4.6598153 -4.5137019 -4.3138146 -4.044425 -3.8161147 -3.5601311 -3.6062465 -3.7525573 -3.8112309][-1.8912063 -2.0868616 -2.4421027 -2.8469977 -3.0508733 -3.6458445 -4.0322385 -3.9636908 -3.86015 -3.7855248 -3.8677151 -3.6179798 -3.2659836 -3.1487827 -3.1816201][-0.59364986 -0.48112178 -0.86689973 -1.3738031 -1.7950695 -2.3988953 -2.9264355 -3.3365755 -3.4392698 -3.4319184 -3.62379 -3.4626963 -3.268837 -2.9623375 -2.6762156][1.0283771 0.75681639 -0.11167049 -0.5028348 -0.69222879 -1.1614671 -1.5823205 -2.1584439 -2.8060484 -3.2863674 -3.4290261 -3.2657671 -2.91827 -2.5977745 -2.3782489][1.3855805 1.0658975 0.51350307 0.10072994 -0.24516726 -0.32749128 -0.31281567 -0.64331269 -1.3250856 -2.4172459 -3.2471528 -3.2850649 -2.8967028 -2.2547019 -1.7150066][1.044961 0.84605169 0.6742115 0.76524687 1.2733378 1.3466659 1.2045693 0.87205172 0.041208744 -1.094414 -2.122087 -2.7272468 -2.6573777 -2.2643185 -1.8424077][0.38191509 0.46758795 0.45355463 0.83642006 1.7380047 2.558713 2.9757771 2.55614 1.4073224 -0.041701317 -1.3869195 -2.2284856 -2.4121788 -2.2737768 -1.9210682][-0.60366583 -0.47589755 -0.3234272 0.45140886 1.3916483 2.4682794 3.2584996 3.0489326 1.9996915 0.40700579 -1.182688 -2.2169406 -2.7223711 -2.6997371 -2.3604681][-2.0918386 -1.7051134 -1.2355795 -0.34314585 0.79074907 1.9210672 2.8684659 2.7932611 1.6172776 -0.0064582825 -1.5521636 -2.6952031 -3.4266438 -3.513808 -3.1220505][-2.349468 -2.1538126 -1.8270781 -1.0554354 -0.041847229 0.89560652 1.5498419 1.6929598 0.90097 -0.58357644 -2.2285359 -3.3622136 -4.0254617 -4.3149872 -3.9771643][-2.897639 -2.7228308 -2.4870853 -1.9635239 -1.2468488 -0.38075924 0.15880251 0.06121254 -0.73747134 -1.6945179 -2.8576577 -3.8856301 -4.4988923 -4.7105665 -4.4064245][-2.9363542 -3.12891 -3.2336645 -2.9640293 -2.3396347 -1.8670845 -1.4933774 -1.3429546 -1.7713506 -2.6906967 -3.7488811 -4.4463968 -4.8268628 -4.896646 -4.6439981][-2.7139287 -2.9579954 -3.3464575 -3.4188643 -3.1778049 -2.8034282 -2.4903846 -2.4363916 -2.7765808 -3.3514235 -4.218935 -4.7662735 -4.9431396 -4.8611908 -4.3992381][-2.2757959 -2.8091059 -3.2926123 -3.6021352 -3.5596776 -3.46459 -3.3053012 -3.0349226 -3.1081285 -3.3858204 -3.8884029 -4.18081 -4.2269053 -4.1464291 -3.7769213][-1.9834678 -2.5242524 -3.160804 -3.6462631 -3.7530723 -3.4731045 -3.1869476 -3.073493 -3.054631 -3.266335 -3.4305279 -3.5500965 -3.5433006 -3.207051 -2.9101577]]...]
INFO - root - 2017-12-16 04:43:08.367194: step 13610, loss = 0.21, batch loss = 0.15 (28.0 examples/sec; 0.285 sec/batch; 25h:16m:28s remains)
INFO - root - 2017-12-16 04:43:11.216095: step 13620, loss = 0.28, batch loss = 0.22 (27.7 examples/sec; 0.289 sec/batch; 25h:33m:47s remains)
INFO - root - 2017-12-16 04:43:14.045545: step 13630, loss = 0.26, batch loss = 0.20 (28.2 examples/sec; 0.283 sec/batch; 25h:05m:10s remains)
INFO - root - 2017-12-16 04:43:16.848463: step 13640, loss = 0.25, batch loss = 0.19 (27.8 examples/sec; 0.288 sec/batch; 25h:31m:58s remains)
INFO - root - 2017-12-16 04:43:19.637349: step 13650, loss = 0.21, batch loss = 0.15 (27.9 examples/sec; 0.287 sec/batch; 25h:25m:17s remains)
INFO - root - 2017-12-16 04:43:22.502368: step 13660, loss = 0.23, batch loss = 0.18 (28.2 examples/sec; 0.283 sec/batch; 25h:04m:56s remains)
INFO - root - 2017-12-16 04:43:25.318705: step 13670, loss = 0.29, batch loss = 0.23 (29.0 examples/sec; 0.276 sec/batch; 24h:26m:43s remains)
INFO - root - 2017-12-16 04:43:28.191359: step 13680, loss = 0.27, batch loss = 0.21 (28.3 examples/sec; 0.282 sec/batch; 25h:00m:04s remains)
INFO - root - 2017-12-16 04:43:31.026313: step 13690, loss = 0.36, batch loss = 0.30 (27.8 examples/sec; 0.288 sec/batch; 25h:30m:50s remains)
INFO - root - 2017-12-16 04:43:33.823726: step 13700, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.281 sec/batch; 24h:52m:18s remains)
2017-12-16 04:43:34.283690: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5308015 -3.7303042 -3.7568161 -3.5879328 -3.4389503 -3.154695 -3.0435581 -3.3262908 -4.1068544 -4.889214 -5.7153778 -6.2074146 -6.0485487 -5.3558631 -4.7776556][-3.1364498 -3.2545176 -3.2841582 -3.339226 -3.4317689 -3.4066672 -3.4637017 -3.639046 -4.3282852 -5.2340341 -6.05921 -6.3996849 -5.9361706 -4.7501245 -3.8223107][-2.4231312 -2.5767179 -2.8320408 -2.9087934 -3.2444181 -3.3688741 -3.2992504 -3.5657258 -4.2703404 -5.0379596 -5.92382 -6.2534485 -6.0633864 -4.7802038 -3.2602592][-1.3422523 -1.5114105 -1.8649018 -2.1485796 -2.4155295 -2.5116093 -2.5433898 -2.52092 -3.1391761 -4.2696366 -5.2841392 -5.8420625 -5.6779494 -4.5540037 -3.0167346][-0.88167644 -1.1078997 -1.5534735 -1.8884377 -1.9675333 -1.5208232 -1.1144431 -1.0611947 -1.8373187 -3.0187778 -4.5003 -5.6231365 -5.5817971 -4.6468716 -3.036593][-0.8250134 -1.1205475 -1.7829425 -2.0438974 -1.4117639 -0.53514814 0.21143341 0.44209194 -0.438828 -2.1754758 -4.2331371 -5.5059404 -5.7459817 -5.0623832 -3.7210608][-1.3341072 -1.5663049 -1.7535298 -1.742806 -0.70213556 1.0722322 2.3954573 2.4007154 1.0360675 -1.1745152 -3.5732484 -5.1970758 -5.5175138 -4.8110161 -3.7099063][-2.1979053 -2.6233029 -2.8791709 -2.1426811 -0.24662638 2.0333276 3.5185728 3.605196 1.7323298 -0.90156031 -3.4454627 -4.875762 -5.1301012 -4.575295 -3.4569602][-3.7414093 -3.9584942 -3.773953 -2.9125905 -0.75836277 1.556509 3.011549 3.0738215 1.277535 -1.4730091 -4.3765378 -5.8111296 -5.7026348 -4.8219209 -3.4600852][-4.4135385 -4.7195616 -4.5866489 -3.5150828 -1.4271588 0.67443657 1.6812716 1.3483682 -0.19395351 -2.6826086 -5.161304 -6.6519909 -6.628089 -5.5353584 -3.9977505][-5.3052177 -5.4998565 -5.4759541 -4.679183 -2.9556186 -1.3295543 -0.54828453 -0.84497762 -2.5100522 -4.8748384 -7.0738888 -8.0496693 -7.9190416 -6.9697409 -5.1856413][-5.9153519 -6.0623746 -6.1221666 -5.8608842 -4.89837 -3.7829232 -3.5167909 -4.1278534 -5.4653945 -7.237792 -8.9381065 -9.7589378 -9.5109825 -8.6145353 -6.7492638][-5.6877871 -6.0789242 -6.5172133 -6.5687332 -6.2647781 -5.8087468 -5.44055 -5.8937531 -7.06467 -8.6436653 -9.7245455 -10.241007 -10.016289 -9.5008745 -8.0988712][-5.0219312 -5.421659 -5.9032569 -6.3354969 -6.521698 -6.6833858 -6.7503943 -7.0211277 -7.407896 -8.3573627 -9.2297049 -9.7656517 -9.7079487 -9.448246 -8.7070894][-4.626976 -4.95733 -5.3447819 -5.7227478 -5.9714308 -6.3584976 -6.6721134 -7.0488105 -7.4206815 -7.8381424 -8.3395758 -8.6976328 -8.7010479 -8.709856 -8.25984]]...]
INFO - root - 2017-12-16 04:43:37.068619: step 13710, loss = 0.29, batch loss = 0.23 (28.7 examples/sec; 0.279 sec/batch; 24h:42m:07s remains)
INFO - root - 2017-12-16 04:43:39.893086: step 13720, loss = 0.27, batch loss = 0.21 (29.1 examples/sec; 0.275 sec/batch; 24h:22m:29s remains)
INFO - root - 2017-12-16 04:43:42.706917: step 13730, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.284 sec/batch; 25h:10m:31s remains)
INFO - root - 2017-12-16 04:43:45.539761: step 13740, loss = 0.25, batch loss = 0.20 (28.8 examples/sec; 0.278 sec/batch; 24h:34m:37s remains)
INFO - root - 2017-12-16 04:43:48.312441: step 13750, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.279 sec/batch; 24h:43m:50s remains)
INFO - root - 2017-12-16 04:43:51.241014: step 13760, loss = 0.20, batch loss = 0.14 (26.8 examples/sec; 0.299 sec/batch; 26h:25m:57s remains)
INFO - root - 2017-12-16 04:43:54.073874: step 13770, loss = 0.24, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 25h:02m:49s remains)
INFO - root - 2017-12-16 04:43:56.877317: step 13780, loss = 0.25, batch loss = 0.19 (27.7 examples/sec; 0.289 sec/batch; 25h:32m:48s remains)
INFO - root - 2017-12-16 04:43:59.712003: step 13790, loss = 0.20, batch loss = 0.14 (29.0 examples/sec; 0.276 sec/batch; 24h:24m:55s remains)
INFO - root - 2017-12-16 04:44:02.555776: step 13800, loss = 0.25, batch loss = 0.19 (27.5 examples/sec; 0.291 sec/batch; 25h:47m:07s remains)
2017-12-16 04:44:03.007433: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0050735 -3.1727791 -3.4813845 -3.858808 -4.3167415 -4.6564989 -4.99548 -5.4269829 -5.6726484 -5.8167458 -5.7391973 -5.569912 -5.4289079 -5.1380219 -4.8794365][-2.6517811 -2.859442 -3.1597259 -3.3562045 -3.6331315 -4.0067854 -4.3124876 -4.5629549 -4.7628074 -4.9906573 -5.1164665 -4.9820857 -4.8495426 -4.6092105 -4.3453636][-2.5444245 -2.7817981 -3.0921204 -3.3319485 -3.3788671 -3.3810868 -3.3129897 -3.3382943 -3.510088 -3.7398624 -3.977917 -4.1134539 -4.1361728 -3.9547629 -3.7012205][-2.5494971 -2.9111316 -3.1667595 -3.3617144 -3.1554766 -2.611594 -1.9806399 -1.6338761 -1.666342 -1.9953358 -2.3952107 -2.7695317 -3.0740352 -3.0493872 -2.8335223][-2.6374915 -3.0658398 -3.2887287 -3.1823812 -2.601887 -1.5946369 -0.47494459 0.23338032 0.17062521 -0.35867262 -1.1215971 -1.8196316 -2.3544641 -2.5519192 -2.4494371][-2.8956122 -3.2136135 -3.2407615 -2.9191575 -1.924278 -0.49508 0.90115118 1.8092537 1.6981254 0.940444 -0.1783824 -1.215529 -1.8286204 -2.1074708 -2.0248392][-2.9869962 -3.2475653 -3.0556264 -2.4597161 -1.2072778 0.48664618 1.9933653 2.8366661 2.5741596 1.6700439 0.39618158 -0.81105137 -1.516006 -1.8413393 -1.8121073][-2.9593043 -2.9971495 -2.7672124 -1.9859114 -0.67160821 0.97217226 2.2688847 3.0471978 2.8353419 1.8076777 0.4455719 -0.68365979 -1.3115451 -1.6214294 -1.6048861][-2.8496368 -2.819478 -2.5119061 -1.7826219 -0.70393682 0.65058994 1.7062254 2.3234811 2.161243 1.3352013 0.14817715 -0.86228609 -1.3958757 -1.6630144 -1.5056994][-2.7369618 -2.8541472 -2.9268985 -2.5695865 -1.9024282 -0.955801 -0.25483704 0.45684052 0.42140579 -0.15919065 -0.87041616 -1.6055913 -1.9344044 -2.0444953 -1.7053025][-2.9514413 -3.2297184 -3.6188078 -3.9099061 -3.9643085 -3.480473 -2.8388753 -2.1617527 -1.8810515 -1.9645436 -2.2559388 -2.6085029 -2.7047062 -2.6653917 -2.2628956][-3.4467778 -3.9973066 -4.8160477 -5.5370245 -5.9313922 -5.9158459 -5.5641994 -4.9486284 -4.325623 -3.9993296 -3.8704786 -3.8334618 -3.7709243 -3.5788012 -3.0041966][-3.9867022 -4.9033356 -6.1071692 -7.1046653 -7.7059784 -7.8256607 -7.5823169 -7.0142555 -6.4579744 -5.9764032 -5.50199 -5.2914543 -5.0960712 -4.874311 -4.3399162][-4.4447212 -5.436749 -6.6257339 -7.7725368 -8.3670406 -8.4597034 -8.2302494 -7.8267717 -7.2875605 -6.8202128 -6.4304962 -6.1155581 -5.86389 -5.6559477 -5.3524704][-4.7704487 -5.6742325 -6.724556 -7.648838 -8.0903149 -8.185544 -7.923121 -7.629652 -7.2828884 -6.9539447 -6.653585 -6.4140921 -6.2617712 -6.0828414 -5.80637]]...]
INFO - root - 2017-12-16 04:44:05.816066: step 13810, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.285 sec/batch; 25h:15m:54s remains)
INFO - root - 2017-12-16 04:44:08.708458: step 13820, loss = 0.24, batch loss = 0.18 (27.2 examples/sec; 0.294 sec/batch; 26h:02m:57s remains)
INFO - root - 2017-12-16 04:44:11.529619: step 13830, loss = 0.24, batch loss = 0.18 (28.9 examples/sec; 0.277 sec/batch; 24h:31m:55s remains)
INFO - root - 2017-12-16 04:44:14.395777: step 13840, loss = 0.25, batch loss = 0.19 (25.6 examples/sec; 0.312 sec/batch; 27h:38m:03s remains)
INFO - root - 2017-12-16 04:44:17.274743: step 13850, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.283 sec/batch; 25h:04m:04s remains)
INFO - root - 2017-12-16 04:44:20.109396: step 13860, loss = 0.36, batch loss = 0.30 (28.8 examples/sec; 0.278 sec/batch; 24h:37m:32s remains)
INFO - root - 2017-12-16 04:44:22.952522: step 13870, loss = 0.35, batch loss = 0.29 (26.7 examples/sec; 0.300 sec/batch; 26h:31m:12s remains)
INFO - root - 2017-12-16 04:44:25.796474: step 13880, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.284 sec/batch; 25h:10m:06s remains)
INFO - root - 2017-12-16 04:44:28.675091: step 13890, loss = 0.33, batch loss = 0.27 (27.6 examples/sec; 0.290 sec/batch; 25h:37m:36s remains)
INFO - root - 2017-12-16 04:44:31.526217: step 13900, loss = 0.24, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 25h:11m:02s remains)
2017-12-16 04:44:31.990109: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8410144 -3.0546956 -3.147944 -3.173861 -3.4732132 -4.2400951 -4.8507705 -5.4217553 -6.0660572 -6.3714666 -6.2576323 -5.8807287 -5.2560773 -4.603528 -3.85041][-2.9022174 -2.831502 -2.6970298 -2.6275449 -2.7251558 -3.0900588 -3.98113 -4.4572268 -5.0311966 -5.4712629 -5.6858711 -5.3151808 -4.4330583 -3.8170979 -3.2024348][-2.8906758 -2.557919 -2.1712494 -1.8598492 -1.7149768 -1.8075261 -2.3959062 -3.2568519 -3.9546542 -4.5731854 -4.7945938 -4.4339156 -3.568717 -2.6986017 -1.8103113][-2.7623663 -2.0188 -1.5617259 -1.0560906 -0.56872725 -0.39050961 -0.81579375 -1.8622651 -2.9289207 -3.5976524 -3.6272697 -3.1529818 -2.4092808 -1.981544 -1.3112664][-2.7704442 -1.7807453 -0.86155462 -0.15393305 0.49144173 1.163353 1.12609 0.43528652 -0.84897709 -1.8437369 -2.4459271 -2.1056786 -1.3014414 -1.1692524 -1.0594146][-3.0626197 -2.080699 -0.79269409 0.67858696 2.1760225 3.2284884 3.3752069 2.804666 1.5331593 0.15039301 -0.928673 -1.2690279 -0.996387 -0.94159555 -1.1301775][-2.7383132 -1.823272 -0.62225842 1.0646081 2.7014217 4.0193911 4.4342594 3.767169 2.4602466 0.66285276 -0.834275 -1.2814977 -1.2338679 -0.98833132 -0.99742055][-2.9293861 -1.9163835 -0.49605846 1.2864265 3.0062675 3.9469995 4.0341759 3.2863259 1.8476439 0.2426672 -1.1801751 -1.728982 -1.548034 -1.4852312 -1.4443603][-3.1289909 -2.2204363 -0.84307408 0.79519367 2.3343759 3.304709 3.3912477 2.4874582 0.9919138 -0.60430193 -1.93907 -2.4638526 -2.2897239 -1.9491637 -1.5105705][-3.4194632 -2.5150108 -1.5210633 -0.12457228 1.1014137 1.8784819 1.8769689 1.1358252 -0.074964046 -1.4025149 -2.3398845 -2.8073378 -2.6640387 -2.0791118 -1.530534][-3.9158449 -3.286108 -2.5362983 -1.5822217 -0.70520711 -0.080119133 0.10073948 -0.39334154 -1.2900105 -2.1743581 -2.8376954 -2.9207249 -2.5749631 -1.9991076 -1.3988068][-4.0329165 -3.9215584 -3.5433538 -3.0719953 -2.534348 -2.0916057 -1.6701903 -1.6065397 -2.0189641 -2.528326 -2.8947883 -2.8694611 -2.5277052 -1.964654 -1.4323235][-3.6516359 -3.7142112 -3.5535648 -3.1547513 -2.7670572 -2.3788157 -1.9954705 -1.8355134 -1.9026301 -2.1344197 -2.2565236 -2.1673889 -1.9464455 -1.6463275 -1.27491][-3.0719197 -3.1866384 -3.1596217 -2.9016218 -2.6341555 -2.2247972 -1.6890714 -1.4894767 -1.5711327 -1.7173245 -1.7283845 -1.6240106 -1.5211885 -1.4377697 -1.2617922][-2.5446208 -2.8202693 -2.9397466 -2.7792583 -2.4960325 -2.2566659 -1.8078918 -1.3675301 -1.1498206 -1.1268201 -1.2174001 -1.2233665 -1.4143708 -1.4497349 -1.3280993]]...]
INFO - root - 2017-12-16 04:44:34.843494: step 13910, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.285 sec/batch; 25h:15m:08s remains)
INFO - root - 2017-12-16 04:44:37.641797: step 13920, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.281 sec/batch; 24h:49m:56s remains)
INFO - root - 2017-12-16 04:44:40.517423: step 13930, loss = 0.21, batch loss = 0.16 (28.1 examples/sec; 0.285 sec/batch; 25h:10m:35s remains)
INFO - root - 2017-12-16 04:44:43.352729: step 13940, loss = 0.28, batch loss = 0.22 (26.7 examples/sec; 0.300 sec/batch; 26h:31m:39s remains)
INFO - root - 2017-12-16 04:44:46.258847: step 13950, loss = 0.36, batch loss = 0.30 (27.8 examples/sec; 0.288 sec/batch; 25h:26m:23s remains)
INFO - root - 2017-12-16 04:44:49.072908: step 13960, loss = 0.34, batch loss = 0.29 (28.2 examples/sec; 0.284 sec/batch; 25h:07m:24s remains)
INFO - root - 2017-12-16 04:44:51.913576: step 13970, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 24h:55m:46s remains)
INFO - root - 2017-12-16 04:44:54.735386: step 13980, loss = 0.24, batch loss = 0.18 (29.3 examples/sec; 0.273 sec/batch; 24h:10m:42s remains)
INFO - root - 2017-12-16 04:44:57.604472: step 13990, loss = 0.30, batch loss = 0.24 (26.6 examples/sec; 0.300 sec/batch; 26h:34m:21s remains)
INFO - root - 2017-12-16 04:45:00.454945: step 14000, loss = 0.30, batch loss = 0.24 (28.0 examples/sec; 0.285 sec/batch; 25h:14m:44s remains)
2017-12-16 04:45:00.909853: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2906518 -3.6580577 -3.9552023 -4.3711586 -4.5831413 -4.8289309 -4.9667654 -4.8455157 -4.6751575 -4.5329771 -4.3275061 -3.9661899 -3.7425289 -3.635263 -3.4967384][-2.1606195 -2.7303848 -3.025342 -3.5128651 -4.0555124 -4.5399332 -4.8498616 -4.9500589 -4.8959684 -4.8541684 -4.8288679 -4.4519939 -3.91906 -3.6571474 -3.4072514][-1.8007751 -2.3156159 -2.6458228 -3.0263715 -3.5319159 -4.0208154 -4.3270617 -4.7043285 -4.9678011 -5.02531 -4.8915362 -4.552927 -4.3217616 -3.9596281 -3.4834914][-0.78383613 -1.4913383 -2.0565579 -2.4305983 -2.5923285 -2.9841986 -3.3709936 -3.7319412 -3.9041834 -4.2692132 -4.4570417 -4.2613592 -4.1615772 -3.9870791 -3.66216][-0.40081596 -0.76543975 -0.87175536 -1.0588882 -1.1729729 -1.3851204 -1.4178855 -1.7788379 -2.4438431 -3.2659416 -3.8460045 -4.13264 -4.2636518 -4.2658067 -4.2000508][-0.99133253 -0.91414618 -0.59095836 -0.049170971 0.68254852 0.85253811 0.89722157 0.44042158 -0.2588048 -1.4651701 -2.717155 -3.6042316 -4.2098918 -4.2736473 -4.0806813][-1.7952468 -1.5412068 -0.99299717 0.044614315 1.3427949 2.2390127 2.8805203 2.5091453 1.7093248 0.27125883 -1.3355148 -2.7740903 -3.7058976 -3.8994498 -3.7977087][-2.4870389 -2.1900702 -1.6579061 -0.50133085 1.0075541 2.3674169 3.3637757 3.1958447 2.3258662 0.94176531 -0.71093726 -2.3346746 -3.6480892 -4.1461849 -4.08403][-3.4379511 -3.2644653 -2.5674591 -1.3276792 0.10121679 1.7135849 2.999577 3.1270733 2.3738508 0.96187878 -0.62517834 -2.2391443 -3.4308028 -4.0611987 -4.1632915][-4.2488947 -4.2978635 -4.1197805 -2.9786239 -1.4156585 0.22475767 1.4206662 1.8286815 1.4542294 0.11304855 -1.4643958 -2.8315873 -3.6967585 -4.0491681 -3.8976064][-5.521265 -5.8579082 -5.6193724 -4.7751503 -3.6733847 -2.1896658 -0.89720535 -0.49667 -0.93615294 -1.918052 -3.0023627 -4.0425291 -4.5723591 -4.5574727 -4.122653][-6.21562 -6.8147554 -7.0052128 -6.3928518 -5.4130611 -4.284596 -3.2735548 -2.7930188 -2.9661522 -3.7028332 -4.4521575 -4.9666734 -5.1300149 -5.0255089 -4.3945045][-6.3596115 -7.1029048 -7.2478185 -6.7803249 -6.1488261 -5.2532516 -4.4286728 -3.9126351 -3.8124256 -4.2362137 -4.6243749 -4.9774466 -4.7947989 -4.4982624 -3.9201019][-6.2636213 -6.8692656 -6.9588413 -6.4820547 -5.8843203 -5.2420411 -4.7638764 -4.402091 -4.3035331 -4.4561858 -4.4759064 -4.4235535 -4.1803503 -3.934747 -3.4131336][-6.1251087 -6.4097848 -6.2283859 -5.7485561 -5.0853562 -4.5442972 -4.3367743 -4.1405473 -4.0267634 -4.1154966 -4.1142926 -3.9081454 -3.3072152 -2.9089251 -2.5884156]]...]
INFO - root - 2017-12-16 04:45:03.763974: step 14010, loss = 0.23, batch loss = 0.18 (28.6 examples/sec; 0.279 sec/batch; 24h:43m:35s remains)
INFO - root - 2017-12-16 04:45:06.598685: step 14020, loss = 0.30, batch loss = 0.25 (28.9 examples/sec; 0.277 sec/batch; 24h:27m:53s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 04:45:09.497864: step 14030, loss = 0.25, batch loss = 0.19 (29.1 examples/sec; 0.275 sec/batch; 24h:21m:37s remains)
INFO - root - 2017-12-16 04:45:12.316634: step 14040, loss = 0.32, batch loss = 0.26 (29.4 examples/sec; 0.272 sec/batch; 24h:03m:15s remains)
INFO - root - 2017-12-16 04:45:15.134873: step 14050, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 25h:17m:27s remains)
INFO - root - 2017-12-16 04:45:17.945719: step 14060, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.285 sec/batch; 25h:10m:41s remains)
INFO - root - 2017-12-16 04:45:20.758022: step 14070, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 24h:43m:46s remains)
INFO - root - 2017-12-16 04:45:23.573399: step 14080, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.284 sec/batch; 25h:06m:47s remains)
INFO - root - 2017-12-16 04:45:26.419723: step 14090, loss = 0.28, batch loss = 0.22 (27.8 examples/sec; 0.288 sec/batch; 25h:29m:35s remains)
INFO - root - 2017-12-16 04:45:29.247655: step 14100, loss = 0.22, batch loss = 0.16 (28.1 examples/sec; 0.285 sec/batch; 25h:11m:55s remains)
2017-12-16 04:45:29.703677: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6708441 -4.4863467 -5.3751292 -6.2163959 -6.8493118 -7.2040434 -7.5157361 -8.1230974 -8.6799641 -8.8921413 -9.1110029 -9.3647013 -9.2159672 -8.4763393 -7.7745113][-4.5636926 -5.6429362 -6.5800781 -7.5239325 -8.2291012 -8.8471842 -8.9936695 -9.1240549 -9.3418655 -9.8111839 -10.298318 -10.223711 -10.019083 -9.4007187 -8.4640484][-5.2951593 -6.5269175 -7.5402832 -8.3486624 -8.627758 -8.759532 -8.61294 -8.6675205 -8.8240814 -8.9620762 -9.2149963 -9.5663891 -9.7136669 -9.04184 -8.1744452][-5.6179247 -7.0086346 -8.1434565 -8.767684 -8.4192238 -7.6430349 -6.6608105 -6.0527167 -5.6999369 -6.2396965 -7.075223 -7.5365963 -7.8930235 -7.6325817 -6.9283228][-5.6250472 -6.6225519 -7.3346519 -7.3962736 -6.7775459 -5.4171333 -3.5951812 -2.2198894 -1.9821689 -2.3967292 -3.365119 -4.313982 -5.1472154 -5.2840834 -5.0860105][-5.0411363 -5.8050876 -6.0066075 -5.6271639 -3.9070363 -1.7202213 0.21549797 1.9029016 2.5715303 1.7369909 -0.05402565 -1.2139161 -1.6326432 -2.0610158 -2.4870849][-4.3293562 -4.6804094 -4.5331135 -3.6369634 -1.886359 0.51109791 3.1913142 5.1478558 5.3286858 4.4110489 3.001493 1.4457531 0.24632692 -0.28238583 -0.62985992][-3.9425805 -4.05217 -3.7546215 -2.5675113 -0.42040062 2.2722383 4.5289516 5.9387388 6.1146193 5.1984262 3.7583055 2.6113529 1.8896122 0.89249325 0.18363476][-3.4892421 -3.539427 -3.0703473 -1.842324 0.026700974 2.0826883 3.9932318 5.0999861 4.9809942 4.2634907 3.3600903 2.2133603 1.2549362 0.50532436 -0.035204887][-3.2889936 -3.4071994 -3.1692152 -2.2263799 -0.73189855 0.87347364 2.0700059 2.5447087 2.5844617 2.1666427 1.4360933 0.62200689 0.037460804 -0.58329225 -1.2031214][-3.7513735 -3.8926568 -3.7797537 -3.2878487 -2.2545688 -1.2914519 -0.4513917 -0.12946653 -0.34847498 -0.81803608 -1.34728 -1.7359605 -2.1682529 -2.6534448 -3.3018336][-4.149343 -4.4155216 -4.41994 -4.2530036 -4.0482764 -3.756216 -3.4057126 -3.3900166 -3.4830828 -3.9278679 -4.341939 -4.6548357 -4.8490276 -4.9186215 -5.23784][-4.732738 -5.0425372 -5.1745977 -5.3235111 -5.3081312 -5.498271 -5.6473546 -5.8338552 -5.9212222 -5.9714642 -6.1784506 -6.0997782 -6.1313047 -6.2325344 -6.2808075][-4.7402921 -5.1902952 -5.542666 -5.8068876 -5.9372616 -6.1434975 -6.3398619 -6.5445719 -6.6715374 -6.81799 -6.8540525 -6.9999814 -7.1054983 -6.8681068 -6.7820053][-4.7403073 -4.9556255 -5.1258574 -5.496 -5.769803 -6.1018577 -6.4431143 -6.6801672 -6.789165 -6.7582874 -6.8471394 -6.8110485 -6.6838951 -6.7837114 -6.6407046]]...]
INFO - root - 2017-12-16 04:45:32.604318: step 14110, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 25h:17m:50s remains)
INFO - root - 2017-12-16 04:45:35.439795: step 14120, loss = 0.23, batch loss = 0.17 (25.6 examples/sec; 0.313 sec/batch; 27h:38m:54s remains)
INFO - root - 2017-12-16 04:45:38.273555: step 14130, loss = 0.21, batch loss = 0.15 (27.7 examples/sec; 0.289 sec/batch; 25h:32m:22s remains)
INFO - root - 2017-12-16 04:45:41.122515: step 14140, loss = 0.31, batch loss = 0.25 (29.5 examples/sec; 0.272 sec/batch; 24h:00m:58s remains)
INFO - root - 2017-12-16 04:45:43.938069: step 14150, loss = 0.23, batch loss = 0.17 (29.6 examples/sec; 0.270 sec/batch; 23h:54m:19s remains)
INFO - root - 2017-12-16 04:45:46.792381: step 14160, loss = 0.24, batch loss = 0.18 (27.7 examples/sec; 0.289 sec/batch; 25h:31m:12s remains)
INFO - root - 2017-12-16 04:45:49.609919: step 14170, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 24h:36m:11s remains)
INFO - root - 2017-12-16 04:45:52.430406: step 14180, loss = 0.25, batch loss = 0.19 (27.4 examples/sec; 0.292 sec/batch; 25h:50m:37s remains)
INFO - root - 2017-12-16 04:45:55.273127: step 14190, loss = 0.34, batch loss = 0.28 (28.4 examples/sec; 0.282 sec/batch; 24h:54m:03s remains)
INFO - root - 2017-12-16 04:45:58.125491: step 14200, loss = 0.29, batch loss = 0.24 (28.9 examples/sec; 0.277 sec/batch; 24h:29m:11s remains)
2017-12-16 04:45:58.584281: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6160235 -5.4132457 -5.42115 -5.5120225 -5.4956274 -5.5656652 -5.6640244 -6.0100827 -6.4381714 -6.7694097 -6.8955231 -6.6408377 -5.9898472 -5.16456 -4.3925443][-4.98672 -4.7426224 -4.5924873 -4.9236469 -5.2128773 -5.6363158 -6.0882106 -6.4658165 -6.7563257 -7.1362371 -7.3797884 -7.2114153 -6.6599307 -5.8172803 -4.9387412][-3.237654 -3.0938225 -3.2523272 -3.5757313 -3.9622712 -4.5052314 -5.0978785 -5.7258768 -6.26409 -6.6722784 -6.9377346 -7.1048441 -6.9736519 -6.3425369 -5.4450946][-0.88217545 -0.50890756 -0.797858 -1.4970503 -2.2554226 -2.7816243 -3.1510146 -3.669579 -4.33043 -5.1388621 -5.77081 -6.2417603 -6.5113049 -6.3422279 -5.6863914][0.6315794 1.3683009 1.3476734 0.72543955 0.034478188 -0.60622287 -0.85352182 -1.1066544 -1.6387129 -2.721684 -3.9060714 -4.8523846 -5.4591265 -5.703361 -5.445179][0.99770212 2.1929598 2.5357423 2.2137003 1.9351268 1.4792528 1.589016 1.6840467 1.2830234 0.099606991 -1.538527 -3.2606533 -4.6248846 -5.1866245 -5.0990772][0.11741686 1.6368775 2.1098037 2.1760626 2.3545213 2.4590597 3.246182 3.6475534 3.3615394 2.2118516 0.49415159 -1.629416 -3.4885774 -4.3919086 -4.41839][-1.798826 -0.22193003 0.77116919 1.333693 1.7924433 2.4222474 3.6941071 4.3163052 4.2458887 2.9023418 0.99394417 -1.1065576 -2.8702917 -3.809139 -4.0666056][-4.090775 -2.6370487 -1.5839748 -0.83378196 0.09663105 1.2447681 2.7842093 3.5510249 3.7327633 2.5949345 0.63928032 -1.4650259 -3.1192622 -3.7843657 -3.9823618][-5.6603365 -4.5388017 -3.7842221 -2.8940613 -1.8346562 -0.64406276 0.82896471 1.5512261 1.6643138 0.8037982 -0.69868922 -2.5306671 -3.9235725 -4.2932758 -4.2231297][-6.6047454 -5.7373281 -5.0551381 -4.4549212 -3.6256638 -2.4621673 -1.1773186 -0.70029426 -0.74070907 -1.2799482 -2.2988713 -3.554039 -4.4280643 -4.6664433 -4.4904575][-6.2342334 -5.8099489 -5.5265336 -5.1170845 -4.5162544 -3.7049046 -2.873925 -2.5314803 -2.4636877 -2.9790804 -3.7519908 -4.4939618 -4.9463506 -4.8489361 -4.5702996][-5.24363 -5.0091472 -4.9870939 -4.9128747 -4.7514153 -4.2808723 -3.7107794 -3.623543 -3.721952 -3.9840074 -4.3564296 -4.7979932 -5.1086793 -4.9635024 -4.5522718][-3.8952906 -3.8823838 -3.9822752 -4.0675826 -4.1498528 -4.0317979 -3.8410959 -3.86202 -4.0264411 -4.182858 -4.3698773 -4.5126166 -4.4483924 -4.235116 -3.9366031][-3.0004945 -2.9450331 -2.9966817 -3.1182585 -3.1545095 -3.2002821 -3.2258706 -3.1519513 -3.2295768 -3.4133773 -3.5872426 -3.5810969 -3.4060256 -3.3262246 -3.1518359]]...]
INFO - root - 2017-12-16 04:46:01.409563: step 14210, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.285 sec/batch; 25h:13m:59s remains)
INFO - root - 2017-12-16 04:46:04.254266: step 14220, loss = 0.29, batch loss = 0.23 (28.2 examples/sec; 0.284 sec/batch; 25h:06m:36s remains)
INFO - root - 2017-12-16 04:46:07.090995: step 14230, loss = 0.28, batch loss = 0.22 (29.1 examples/sec; 0.275 sec/batch; 24h:20m:03s remains)
INFO - root - 2017-12-16 04:46:09.909759: step 14240, loss = 0.30, batch loss = 0.25 (28.7 examples/sec; 0.279 sec/batch; 24h:38m:07s remains)
INFO - root - 2017-12-16 04:46:12.744841: step 14250, loss = 0.37, batch loss = 0.32 (29.5 examples/sec; 0.272 sec/batch; 24h:00m:37s remains)
INFO - root - 2017-12-16 04:46:15.576933: step 14260, loss = 0.25, batch loss = 0.19 (27.4 examples/sec; 0.293 sec/batch; 25h:51m:25s remains)
INFO - root - 2017-12-16 04:46:18.408333: step 14270, loss = 0.23, batch loss = 0.17 (28.9 examples/sec; 0.277 sec/batch; 24h:29m:01s remains)
INFO - root - 2017-12-16 04:46:21.185087: step 14280, loss = 0.24, batch loss = 0.18 (29.1 examples/sec; 0.275 sec/batch; 24h:16m:10s remains)
INFO - root - 2017-12-16 04:46:24.028697: step 14290, loss = 0.28, batch loss = 0.23 (27.9 examples/sec; 0.287 sec/batch; 25h:20m:40s remains)
INFO - root - 2017-12-16 04:46:26.890660: step 14300, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.280 sec/batch; 24h:46m:06s remains)
2017-12-16 04:46:27.368277: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7124028 -4.5852847 -4.2781434 -3.9083533 -3.6273735 -3.6383598 -3.7759049 -4.0600057 -4.5563946 -5.0248013 -5.4020209 -5.5139837 -5.36655 -4.9596672 -4.3385429][-4.8899341 -4.4685569 -4.0036297 -3.7735548 -3.5945885 -3.6482515 -3.7790616 -4.0259166 -4.4855218 -4.898284 -5.3248806 -5.3071041 -5.0600662 -4.7559686 -4.2538986][-3.9274535 -3.706193 -3.4396589 -3.1567183 -2.9093547 -3.2324185 -3.5708365 -3.9271536 -4.3349919 -4.6437044 -4.8441811 -4.938777 -5.0682635 -4.9306035 -4.6399674][-2.5407581 -2.1074207 -1.7840588 -1.7499619 -1.8433542 -2.2294743 -2.5449581 -3.0234671 -3.2834842 -3.5688202 -3.9656525 -4.4020605 -4.9277711 -5.1255965 -5.0057411][-1.3412864 -0.34234619 0.15999508 0.06697464 0.065887451 -0.22541094 -0.51264191 -1.0715876 -1.5315266 -1.992084 -2.656055 -3.6699786 -4.755372 -5.5059533 -5.6494122][-0.416018 0.68588686 1.3385286 1.7291465 2.1324778 1.788415 1.4934349 1.2048926 0.91640806 0.11545277 -1.0849535 -2.83846 -4.4834161 -5.6401553 -5.9187918][-0.18258858 1.1639543 2.169251 2.6858616 3.1124249 3.2235556 3.3952432 3.1936316 2.9680548 1.9628329 0.40702105 -1.5896349 -3.5736241 -5.1819229 -5.9374466][-0.73913407 0.64551449 1.534204 2.3899102 3.2868156 3.918705 4.4058685 4.5195055 4.5234728 3.4243951 1.7088876 -0.90217948 -3.4154158 -5.170249 -5.7549386][-2.7641907 -1.5164359 -0.32742262 0.64705229 1.5834093 2.6836386 3.6931667 4.1794529 4.284955 3.4199657 1.8616228 -0.58246541 -3.0049634 -4.8986053 -5.683743][-3.9129357 -3.4572268 -2.8340716 -1.82936 -0.63866568 0.44395113 1.4138093 2.1351032 2.5443892 1.9727449 0.77471542 -1.3532805 -3.4629235 -4.981626 -5.7304983][-4.8954773 -4.7487965 -4.3269205 -3.7069726 -2.9762878 -1.9269817 -0.91841125 -0.3102994 -0.0094776154 -0.17578077 -0.8085568 -2.2399626 -3.8105798 -5.0518565 -5.6545219][-5.2585139 -5.4005156 -5.6059251 -5.311202 -4.761745 -4.0556974 -3.4853842 -2.9282515 -2.4180717 -2.4062173 -2.9576173 -3.7798252 -4.6064682 -5.4141579 -6.0138626][-4.9347072 -5.0717993 -5.4189882 -5.5630693 -5.56882 -5.2059317 -4.7950034 -4.5172186 -4.3047929 -4.1344867 -4.2033219 -4.4921694 -5.0300255 -5.5689073 -5.9435458][-4.34629 -4.3729434 -4.6691418 -4.9636636 -5.145329 -5.1211448 -5.0878778 -4.9030771 -4.6408696 -4.39786 -4.4700069 -4.4798684 -4.5615072 -4.8495879 -5.1744947][-4.4852033 -4.1510854 -4.1817446 -4.4564815 -4.7018785 -4.7774048 -4.8061581 -4.7974381 -4.8304195 -4.6539869 -4.5108948 -4.3782887 -4.3812318 -4.2858176 -4.2470112]]...]
INFO - root - 2017-12-16 04:46:30.243013: step 14310, loss = 0.30, batch loss = 0.24 (27.1 examples/sec; 0.296 sec/batch; 26h:07m:55s remains)
INFO - root - 2017-12-16 04:46:33.095098: step 14320, loss = 0.40, batch loss = 0.35 (28.2 examples/sec; 0.284 sec/batch; 25h:05m:41s remains)
INFO - root - 2017-12-16 04:46:35.947691: step 14330, loss = 0.22, batch loss = 0.16 (28.1 examples/sec; 0.285 sec/batch; 25h:09m:47s remains)
INFO - root - 2017-12-16 04:46:38.806054: step 14340, loss = 0.31, batch loss = 0.25 (28.4 examples/sec; 0.282 sec/batch; 24h:52m:49s remains)
INFO - root - 2017-12-16 04:46:41.629607: step 14350, loss = 0.35, batch loss = 0.29 (28.9 examples/sec; 0.276 sec/batch; 24h:26m:04s remains)
INFO - root - 2017-12-16 04:46:44.468504: step 14360, loss = 0.27, batch loss = 0.21 (28.1 examples/sec; 0.285 sec/batch; 25h:08m:36s remains)
INFO - root - 2017-12-16 04:46:47.268860: step 14370, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.284 sec/batch; 25h:05m:26s remains)
INFO - root - 2017-12-16 04:46:50.111572: step 14380, loss = 0.27, batch loss = 0.21 (28.3 examples/sec; 0.283 sec/batch; 24h:58m:13s remains)
INFO - root - 2017-12-16 04:46:52.912044: step 14390, loss = 0.29, batch loss = 0.23 (28.7 examples/sec; 0.279 sec/batch; 24h:37m:56s remains)
INFO - root - 2017-12-16 04:46:55.737678: step 14400, loss = 0.33, batch loss = 0.28 (27.6 examples/sec; 0.290 sec/batch; 25h:38m:47s remains)
2017-12-16 04:46:56.215763: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5978258 -3.7479932 -3.9543805 -3.9423594 -3.8504729 -3.643157 -3.5182734 -3.375041 -3.2091651 -2.9819705 -2.8105927 -2.7514462 -2.6744304 -2.5857005 -2.4829669][-3.5852144 -3.7223949 -4.0633655 -4.1431031 -4.0636764 -3.7843633 -3.7028048 -3.4380138 -3.1318998 -3.0108886 -2.9858348 -2.9158435 -2.8246036 -2.7611108 -2.6901393][-3.5095949 -3.6693821 -4.0527773 -4.0587034 -3.8943975 -3.6867235 -3.5508974 -3.3029232 -3.1745455 -3.2020485 -3.2416649 -3.3340316 -3.3234315 -3.2437537 -3.1194818][-3.4227397 -3.4642327 -3.7189209 -3.6583166 -3.3963423 -3.2477677 -3.0802186 -2.9128375 -2.8218346 -3.0051374 -3.2387891 -3.5942693 -3.718616 -3.7493694 -3.6326146][-2.9486463 -2.8544135 -2.8610463 -2.8191662 -2.5881159 -2.2689319 -1.8847117 -1.6984239 -1.7754347 -2.2451978 -2.8642497 -3.4006109 -3.616044 -3.7630062 -3.7661309][-2.2357361 -1.8961477 -1.7828031 -1.5230052 -1.0370505 -0.61385751 -0.20130587 -0.027916431 -0.11072493 -0.80217981 -1.7926934 -2.7261372 -3.2960448 -3.5975032 -3.7684689][-1.7605429 -1.476403 -1.167124 -0.55008149 0.19114637 0.81274176 1.3968492 1.6701374 1.4992843 0.5930357 -0.70728827 -1.7936063 -2.437021 -2.8867102 -3.1881979][-1.9289582 -1.466289 -0.86993337 -0.10411549 0.84421396 1.5333328 2.0612397 2.2881608 2.1339054 1.226037 -0.0048241615 -1.171762 -1.9525712 -2.3191741 -2.4317868][-2.2378597 -1.9337742 -1.2996192 -0.4180789 0.55284739 1.4017749 2.03021 2.2173095 1.8755836 1.147253 0.10025883 -0.901741 -1.4382672 -1.5986774 -1.3696828][-2.5283113 -2.3010368 -1.7526836 -0.91123652 0.050254345 0.6208396 0.85371161 0.9089179 0.69770336 0.17304945 -0.70298195 -1.3941331 -1.4992414 -1.2517059 -0.65808487][-2.9164228 -2.5914373 -2.3964634 -1.8745353 -1.1411541 -0.66934419 -0.49707437 -0.66777325 -1.1265991 -1.5749547 -2.0938504 -2.195785 -1.8138342 -1.2149563 -0.18317795][-3.1992843 -2.7617946 -2.5750537 -2.412679 -1.9917798 -1.9627454 -2.3853865 -2.8502131 -3.3633242 -3.5868726 -3.6093922 -3.3343148 -2.6406791 -1.7229831 -0.48434424][-3.0835831 -2.6861 -2.7108815 -2.7423396 -2.6485739 -2.9765685 -3.7290905 -4.4814048 -5.3650985 -5.5309973 -5.2373452 -4.4971018 -3.4898942 -2.5208774 -1.2370422][-2.8817754 -2.2228763 -2.1775212 -2.5127473 -2.7907264 -3.3260489 -4.3202105 -5.2738214 -6.2651658 -6.5175624 -6.1680784 -5.2030644 -4.0095282 -3.2062902 -2.3016806][-2.6654909 -2.0764287 -2.0882668 -2.4332545 -2.8230846 -3.5624671 -4.6783233 -5.6626406 -6.6754274 -7.1655807 -7.1492333 -6.3323007 -5.2413025 -4.5156436 -3.6720462]]...]
INFO - root - 2017-12-16 04:46:59.062405: step 14410, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 25h:15m:19s remains)
INFO - root - 2017-12-16 04:47:01.878555: step 14420, loss = 0.26, batch loss = 0.20 (29.4 examples/sec; 0.272 sec/batch; 24h:00m:37s remains)
INFO - root - 2017-12-16 04:47:04.752515: step 14430, loss = 0.30, batch loss = 0.24 (27.9 examples/sec; 0.287 sec/batch; 25h:19m:35s remains)
INFO - root - 2017-12-16 04:47:07.609837: step 14440, loss = 0.28, batch loss = 0.22 (27.9 examples/sec; 0.287 sec/batch; 25h:21m:43s remains)
INFO - root - 2017-12-16 04:47:10.429559: step 14450, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.281 sec/batch; 24h:48m:16s remains)
INFO - root - 2017-12-16 04:47:13.260766: step 14460, loss = 0.31, batch loss = 0.25 (29.6 examples/sec; 0.270 sec/batch; 23h:52m:17s remains)
INFO - root - 2017-12-16 04:47:16.090339: step 14470, loss = 0.27, batch loss = 0.21 (27.9 examples/sec; 0.287 sec/batch; 25h:18m:59s remains)
INFO - root - 2017-12-16 04:47:18.946166: step 14480, loss = 0.21, batch loss = 0.15 (29.1 examples/sec; 0.275 sec/batch; 24h:16m:50s remains)
INFO - root - 2017-12-16 04:47:21.820674: step 14490, loss = 0.23, batch loss = 0.17 (27.8 examples/sec; 0.288 sec/batch; 25h:24m:33s remains)
INFO - root - 2017-12-16 04:47:24.702229: step 14500, loss = 0.48, batch loss = 0.42 (29.7 examples/sec; 0.270 sec/batch; 23h:49m:12s remains)
2017-12-16 04:47:25.162786: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7161169 -4.7877541 -5.1291218 -5.9687963 -6.75629 -7.2127986 -7.4538264 -7.5502958 -7.2833438 -6.7463675 -6.1703887 -5.4219537 -4.3916521 -3.50749 -3.0010681][-4.2840652 -4.421432 -4.6827288 -5.254384 -5.7817841 -5.9293165 -6.0082493 -6.0920877 -6.2332659 -5.9118986 -5.4491343 -4.7979078 -3.9431655 -3.0216761 -2.3768413][-3.5460579 -3.5534713 -3.7229402 -4.0491204 -4.27308 -4.292984 -4.4555836 -4.5766659 -4.6437836 -4.8431096 -4.9348049 -4.4726052 -3.6381955 -2.7708783 -2.1889069][-2.5924451 -2.3718543 -2.2210665 -2.1897702 -2.0367737 -1.9464865 -2.193027 -2.5991974 -3.0678859 -3.6391203 -3.9624786 -4.0778589 -3.8312278 -2.9706671 -2.266366][-1.6558568 -1.4135356 -1.2575738 -1.0142322 -0.64325356 -0.19349623 -0.1144886 -0.50145721 -1.0408342 -2.0280156 -3.0855427 -3.6801019 -3.6430604 -3.3164015 -2.8296542][-1.4895222 -0.79498076 -0.31375885 0.34432507 1.2097507 1.8204513 1.9896307 1.6363249 0.79119825 -0.630491 -1.9818852 -3.0606141 -3.634181 -3.5540707 -2.9046698][-1.3795352 -0.53532052 0.26274967 1.0916162 2.2315774 3.2501388 3.8269262 3.3037534 2.2362332 0.50106049 -1.2212083 -2.7276061 -3.5204766 -3.6334479 -3.23595][-1.1283278 -0.31921339 0.17586899 0.96560907 2.1045079 3.03503 3.5151339 3.2228322 2.3561778 0.55480766 -1.1733224 -2.5969214 -3.4688091 -3.8494813 -3.605772][-1.7473469 -0.98251414 -0.62726259 0.09127903 0.90845633 1.8228903 2.4328909 2.2306709 1.4260287 -0.083224773 -1.5347247 -2.7707152 -3.5438221 -3.9426916 -3.7959375][-2.8401613 -2.3230093 -2.4516413 -1.8010952 -0.99202371 -0.30918169 0.0066056252 0.05764246 -0.32644892 -1.4028776 -2.4996834 -3.4849868 -4.1436243 -4.3949084 -4.2062893][-4.7656841 -4.3192525 -4.2574072 -3.9721229 -3.7247014 -3.1933973 -2.9470739 -2.9903994 -3.0301638 -3.5380731 -4.2197752 -4.8031883 -5.1299086 -5.2401123 -5.122107][-5.7170291 -5.7062397 -6.0835228 -6.0699186 -5.7304921 -5.4616876 -5.5028362 -5.4371319 -5.3460722 -5.5023828 -5.6676831 -5.9355974 -6.1966653 -6.2278476 -5.9862089][-5.9556527 -6.1046357 -6.4341888 -6.7146449 -6.8417106 -6.7521915 -6.7454338 -6.7218485 -6.7360897 -6.637948 -6.4286819 -6.3828373 -6.355113 -6.3273773 -5.9978085][-5.7483311 -5.6347671 -5.7468119 -6.008297 -6.2578397 -6.4345765 -6.7182322 -6.70405 -6.5189304 -6.3767834 -6.1680951 -6.091804 -6.0556006 -6.0000377 -5.7678041][-5.3646011 -5.1372175 -5.0869522 -5.1045184 -5.2514491 -5.5108213 -5.7812691 -5.7770529 -5.6655874 -5.6245756 -5.5542197 -5.5136795 -5.4413943 -5.5253782 -5.3941531]]...]
INFO - root - 2017-12-16 04:47:28.017141: step 14510, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.280 sec/batch; 24h:42m:26s remains)
INFO - root - 2017-12-16 04:47:30.852748: step 14520, loss = 0.26, batch loss = 0.21 (29.3 examples/sec; 0.273 sec/batch; 24h:09m:24s remains)
INFO - root - 2017-12-16 04:47:33.681103: step 14530, loss = 0.25, batch loss = 0.19 (29.3 examples/sec; 0.273 sec/batch; 24h:05m:03s remains)
INFO - root - 2017-12-16 04:47:36.480079: step 14540, loss = 0.27, batch loss = 0.21 (29.0 examples/sec; 0.276 sec/batch; 24h:23m:51s remains)
INFO - root - 2017-12-16 04:47:39.294690: step 14550, loss = 0.26, batch loss = 0.21 (28.6 examples/sec; 0.280 sec/batch; 24h:43m:57s remains)
INFO - root - 2017-12-16 04:47:42.112313: step 14560, loss = 0.22, batch loss = 0.17 (29.7 examples/sec; 0.270 sec/batch; 23h:49m:15s remains)
INFO - root - 2017-12-16 04:47:44.915577: step 14570, loss = 0.27, batch loss = 0.21 (27.5 examples/sec; 0.291 sec/batch; 25h:43m:37s remains)
INFO - root - 2017-12-16 04:47:47.799662: step 14580, loss = 0.30, batch loss = 0.24 (27.4 examples/sec; 0.292 sec/batch; 25h:45m:16s remains)
INFO - root - 2017-12-16 04:47:50.632859: step 14590, loss = 0.32, batch loss = 0.27 (28.1 examples/sec; 0.284 sec/batch; 25h:06m:54s remains)
INFO - root - 2017-12-16 04:47:53.497548: step 14600, loss = 0.25, batch loss = 0.19 (29.4 examples/sec; 0.272 sec/batch; 24h:03m:26s remains)
2017-12-16 04:47:53.954284: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1518788 -5.5904188 -4.8281317 -4.4020548 -3.9394236 -3.8392091 -3.7637384 -3.9268935 -4.2261195 -4.4961748 -4.7628484 -4.7205405 -4.4947233 -4.2965012 -4.5045042][-6.4819083 -5.9963427 -5.1500373 -4.2269082 -3.4908757 -3.6341295 -3.8019407 -3.8339581 -3.8412361 -4.11044 -4.5387549 -4.7062116 -4.8113384 -4.546361 -4.6399255][-6.7376404 -6.0201845 -4.8547554 -3.8756585 -2.819242 -2.5972433 -2.9098513 -3.2682295 -3.4786034 -4.0117755 -4.3750262 -4.5391731 -4.8815508 -4.8998933 -4.9099517][-6.2882867 -5.5518684 -4.4699616 -3.0432472 -1.583322 -1.0722733 -0.9180007 -1.3806641 -2.3987877 -3.599654 -4.266819 -4.8155236 -5.0845809 -5.0403738 -5.0444608][-6.27076 -4.995533 -3.4016676 -1.742224 -0.32252026 0.50753021 0.76307583 0.40857983 -0.54341221 -2.2551672 -3.5949359 -4.2866559 -4.5835123 -4.8805571 -4.882185][-6.1633596 -5.0001655 -3.0951443 -0.61745572 1.4496717 2.4476495 2.5354271 2.1718516 0.87022448 -0.900599 -2.1199808 -3.2112193 -4.0606775 -4.5087891 -4.463233][-6.1824975 -4.9312248 -3.0512927 -0.43715382 2.1418886 3.9115782 4.5666618 3.7631187 2.2221093 0.19685411 -1.5067108 -2.6524873 -3.1657796 -3.6867256 -4.2868514][-6.8302879 -5.5837264 -3.2629423 -0.370759 1.8524451 3.5295711 4.5422716 4.2393103 2.72052 0.66630363 -0.83266449 -2.08001 -2.7341473 -3.463088 -4.1503859][-7.9749694 -6.9417009 -4.7062697 -1.7670951 0.64223433 2.2614498 2.9560966 2.7663946 1.6732364 0.21632814 -0.973886 -1.9079037 -2.4689856 -3.0872757 -3.770009][-8.7351151 -7.89023 -6.4628706 -4.1556749 -1.7576931 0.033823967 0.78763676 0.92189169 0.26501989 -0.79707265 -1.8999026 -2.5461035 -2.6438475 -2.9918485 -3.5453105][-9.8913231 -8.87701 -7.1619625 -5.1639476 -3.6236048 -2.511627 -1.6320622 -1.553314 -2.0893459 -2.6874678 -3.2024391 -3.5047727 -3.5341184 -3.605907 -3.9227839][-10.158714 -9.1685371 -7.5461617 -5.9121618 -4.6534595 -3.9450045 -3.8664804 -3.8790407 -3.8632667 -4.1126223 -4.1973944 -4.3704844 -4.4330788 -4.2063723 -4.027184][-9.9150991 -8.645071 -7.565208 -6.3838921 -5.2037554 -4.6272306 -4.7855539 -5.173296 -5.1900191 -5.2283583 -5.0764637 -4.8606176 -4.6916924 -4.4772558 -4.0045924][-10.440985 -8.7214031 -6.9590554 -6.0182629 -5.3883376 -5.0770717 -5.2482729 -5.6537933 -5.9120483 -5.9420128 -5.5336785 -4.9507985 -4.7326355 -4.5446105 -4.0351233][-10.283377 -8.6436386 -6.7547321 -5.5497861 -4.9326105 -4.6241732 -4.6188712 -5.1852508 -5.6834717 -5.8023262 -5.5979605 -5.1361718 -4.77171 -4.5227971 -4.2533255]]...]
INFO - root - 2017-12-16 04:47:56.770784: step 14610, loss = 0.35, batch loss = 0.29 (28.1 examples/sec; 0.284 sec/batch; 25h:07m:03s remains)
INFO - root - 2017-12-16 04:47:59.617439: step 14620, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.285 sec/batch; 25h:11m:56s remains)
INFO - root - 2017-12-16 04:48:02.448937: step 14630, loss = 0.31, batch loss = 0.25 (29.3 examples/sec; 0.273 sec/batch; 24h:07m:58s remains)
INFO - root - 2017-12-16 04:48:05.193503: step 14640, loss = 0.38, batch loss = 0.33 (29.4 examples/sec; 0.272 sec/batch; 24h:03m:06s remains)
INFO - root - 2017-12-16 04:48:08.080065: step 14650, loss = 0.25, batch loss = 0.19 (26.7 examples/sec; 0.299 sec/batch; 26h:24m:51s remains)
INFO - root - 2017-12-16 04:48:10.887038: step 14660, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 24h:51m:15s remains)
INFO - root - 2017-12-16 04:48:13.732243: step 14670, loss = 0.25, batch loss = 0.19 (27.7 examples/sec; 0.288 sec/batch; 25h:28m:00s remains)
INFO - root - 2017-12-16 04:48:16.560157: step 14680, loss = 0.42, batch loss = 0.36 (27.4 examples/sec; 0.292 sec/batch; 25h:44m:15s remains)
INFO - root - 2017-12-16 04:48:19.375148: step 14690, loss = 0.30, batch loss = 0.24 (29.6 examples/sec; 0.270 sec/batch; 23h:50m:14s remains)
INFO - root - 2017-12-16 04:48:22.215113: step 14700, loss = 0.23, batch loss = 0.17 (26.9 examples/sec; 0.297 sec/batch; 26h:14m:58s remains)
2017-12-16 04:48:22.664808: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0219283 -2.8660736 -2.601243 -2.4471073 -2.4076269 -2.5768266 -2.971421 -3.4063942 -3.895839 -4.1898713 -4.3104806 -4.3827 -4.1635289 -3.8010049 -3.4629846][-3.1640339 -2.9184937 -2.6148658 -2.3804789 -2.2605536 -2.4188342 -2.7298126 -3.0513244 -3.49277 -3.8106604 -3.9353418 -3.8714 -3.4715042 -3.0197592 -2.6552062][-3.1271949 -2.8142803 -2.4969916 -2.2529569 -2.0996761 -2.1809027 -2.432251 -2.6827106 -2.898931 -3.0569446 -3.1016836 -3.0180116 -2.7617278 -2.2452066 -1.8506634][-3.0465002 -2.6453323 -2.2985408 -2.1034553 -1.9762032 -1.9902997 -2.0376525 -2.1568174 -2.1712332 -1.9642367 -1.7286565 -1.5521531 -1.3497756 -1.099582 -1.1540172][-3.0105438 -2.5788677 -2.2302847 -1.9739461 -1.8192263 -1.7785842 -1.6555197 -1.5543253 -1.2406154 -0.85270905 -0.39269924 -0.020543575 -0.04253149 -0.15286732 -0.53538513][-2.8836663 -2.4335747 -2.0796411 -1.8341918 -1.7189183 -1.7615726 -1.5939002 -1.2674944 -0.66365767 -0.0061831474 0.6637764 1.0922661 0.86941195 0.32302856 -0.68057323][-2.669584 -2.2954471 -1.9892497 -1.7306693 -1.585963 -1.5575414 -1.2780755 -0.86176825 -0.13805866 0.7021122 1.4234414 1.7810283 1.3364882 0.34259653 -1.0766802][-2.476088 -2.2069592 -1.8870046 -1.6384718 -1.4893968 -1.2878006 -0.69662428 -0.01628828 0.82858276 1.5840821 2.1491017 2.2937613 1.5913033 0.34198713 -1.2926416][-2.1493938 -2.0053558 -1.7634063 -1.4831047 -1.3002698 -1.1125841 -0.54034543 0.2827177 1.2174969 1.8933182 2.296577 2.22081 1.3679805 -0.048230648 -1.6964152][-1.9115248 -1.9907005 -1.953692 -1.7570221 -1.6673908 -1.5233884 -0.97329116 -0.27976274 0.4598546 0.91966963 1.2053447 1.0642099 0.26162434 -1.081897 -2.5896389][-2.1344864 -2.3744366 -2.5507684 -2.5954847 -2.6301012 -2.4574327 -1.9417362 -1.3512073 -0.78894353 -0.543241 -0.40293741 -0.71529794 -1.4804521 -2.5506134 -3.6058249][-2.3547516 -2.6685894 -3.0017118 -3.1843441 -3.3416972 -3.306519 -2.9612331 -2.5287905 -2.107461 -2.0723879 -2.0762994 -2.4266067 -3.0530279 -3.8768454 -4.5640764][-2.5603948 -2.8731937 -3.2157433 -3.4262867 -3.5862527 -3.6409032 -3.5657113 -3.3860111 -3.2220056 -3.4074607 -3.5398052 -3.8875656 -4.3446722 -4.9099936 -5.2705479][-2.7518594 -2.9056544 -3.1684968 -3.3245564 -3.503243 -3.5749106 -3.6244936 -3.6366358 -3.695009 -4.0445809 -4.3218088 -4.7281785 -5.0669012 -5.455492 -5.6543036][-2.8454237 -2.847533 -2.9486203 -3.0556581 -3.1919162 -3.2955542 -3.3947163 -3.4374619 -3.5539505 -3.8957181 -4.2094011 -4.6326103 -4.9507604 -5.2164855 -5.3678765]]...]
INFO - root - 2017-12-16 04:48:25.473585: step 14710, loss = 0.39, batch loss = 0.33 (28.9 examples/sec; 0.277 sec/batch; 24h:26m:23s remains)
INFO - root - 2017-12-16 04:48:28.381001: step 14720, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.285 sec/batch; 25h:09m:58s remains)
INFO - root - 2017-12-16 04:48:31.160081: step 14730, loss = 0.36, batch loss = 0.30 (28.5 examples/sec; 0.281 sec/batch; 24h:47m:07s remains)
INFO - root - 2017-12-16 04:48:33.985559: step 14740, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 24h:51m:30s remains)
INFO - root - 2017-12-16 04:48:36.832595: step 14750, loss = 0.27, batch loss = 0.21 (27.8 examples/sec; 0.287 sec/batch; 25h:22m:11s remains)
INFO - root - 2017-12-16 04:48:39.704726: step 14760, loss = 0.35, batch loss = 0.30 (27.0 examples/sec; 0.296 sec/batch; 26h:10m:05s remains)
INFO - root - 2017-12-16 04:48:42.543549: step 14770, loss = 0.21, batch loss = 0.15 (28.2 examples/sec; 0.284 sec/batch; 25h:01m:46s remains)
INFO - root - 2017-12-16 04:48:45.395481: step 14780, loss = 0.22, batch loss = 0.17 (29.5 examples/sec; 0.271 sec/batch; 23h:55m:56s remains)
INFO - root - 2017-12-16 04:48:48.249020: step 14790, loss = 0.30, batch loss = 0.24 (26.4 examples/sec; 0.303 sec/batch; 26h:46m:51s remains)
INFO - root - 2017-12-16 04:48:51.079518: step 14800, loss = 0.27, batch loss = 0.21 (28.7 examples/sec; 0.279 sec/batch; 24h:34m:47s remains)
2017-12-16 04:48:51.517632: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8466849 -4.5436254 -5.2474818 -5.9998264 -6.7921791 -7.3236265 -7.6554432 -7.292223 -6.858036 -6.0915852 -5.1579633 -4.3350334 -3.600709 -3.2752104 -3.0360661][-2.8555851 -3.4678786 -4.1458054 -4.8571334 -5.7346425 -6.6162167 -7.2572184 -7.1736073 -6.9650793 -6.3116045 -5.4735374 -4.6167665 -3.8021955 -3.3981624 -3.1653914][-1.9132044 -2.1380208 -2.4818 -3.0935681 -3.9968078 -5.0474052 -6.1117563 -6.4032068 -6.4111748 -6.1982975 -5.8233442 -5.0483866 -4.1953607 -3.7557464 -3.4181485][-1.292403 -1.4850383 -1.7616835 -1.9532716 -2.3681862 -3.1840143 -4.0764356 -4.4794631 -4.7969522 -4.9756784 -4.980865 -4.7471185 -4.4238491 -4.0108767 -3.4798222][-0.84416962 -0.91986895 -1.100174 -1.0448587 -0.97249341 -1.1295571 -1.4848747 -1.9314256 -2.4445214 -2.9084561 -3.430881 -3.8104806 -3.9790163 -3.9628811 -3.7172105][-0.66368151 -0.40021467 -0.10192299 0.44600582 0.99238253 1.2691641 1.3008838 0.94873476 0.31734371 -0.73809838 -1.8920553 -2.7969794 -3.5187151 -3.914247 -3.8671451][-0.73623276 -0.34779835 0.11495066 0.93080473 1.9464655 2.766336 3.2904296 3.1879807 2.5035663 1.1822882 -0.30084467 -1.726469 -2.9617109 -3.6142426 -3.7209949][-1.3023806 -0.87771273 -0.44450188 0.57103062 1.7824917 2.984623 4.1193657 4.3312044 3.8837385 2.6151013 0.91503334 -0.74363613 -2.2131112 -3.1405654 -3.5133066][-2.2829373 -1.9443152 -1.5303879 -0.71785474 0.54017544 2.0656505 3.2889647 3.716074 3.5583081 2.4617348 0.91376495 -0.77066541 -2.3165057 -3.2732368 -3.5861416][-2.7430341 -2.883358 -3.0204391 -2.4033303 -1.5250628 -0.45313811 0.62061739 1.3329048 1.3958101 0.68527412 -0.4619925 -1.9156389 -3.1083877 -3.793716 -4.0577269][-3.53956 -3.6788321 -3.9570589 -4.050034 -3.8926461 -2.9318287 -2.1115491 -1.8143339 -1.7281744 -2.0035458 -2.6346023 -3.5340352 -4.1671991 -4.456521 -4.2880139][-4.30963 -4.8681669 -5.40771 -5.5504637 -5.2863331 -4.9353251 -4.8426442 -4.4696136 -4.3907332 -4.6624484 -4.7490673 -5.0025887 -5.069643 -4.8728 -4.5124125][-4.7586021 -5.3472104 -6.0234523 -6.4506931 -6.6437454 -6.4886227 -6.3521852 -6.37724 -6.5948319 -6.4156117 -6.1569381 -6.0082622 -5.6563678 -5.2241344 -4.6452265][-4.6013923 -5.0123448 -5.459641 -6.0010452 -6.4194012 -6.6224508 -6.9303031 -7.1323452 -7.2735767 -7.1580777 -6.8923893 -6.4484577 -5.8196239 -5.1973729 -4.6496668][-4.3446369 -4.5368142 -4.7810688 -5.0552359 -5.3325582 -5.6402249 -6.0856786 -6.4413853 -6.7108736 -6.7165289 -6.4301157 -5.8319817 -5.2148738 -4.6908851 -4.0975308]]...]
INFO - root - 2017-12-16 04:48:54.333370: step 14810, loss = 0.23, batch loss = 0.17 (27.4 examples/sec; 0.292 sec/batch; 25h:48m:10s remains)
INFO - root - 2017-12-16 04:48:57.169771: step 14820, loss = 0.19, batch loss = 0.14 (28.9 examples/sec; 0.277 sec/batch; 24h:26m:58s remains)
INFO - root - 2017-12-16 04:49:00.032525: step 14830, loss = 0.24, batch loss = 0.18 (27.5 examples/sec; 0.291 sec/batch; 25h:41m:54s remains)
INFO - root - 2017-12-16 04:49:02.872995: step 14840, loss = 0.32, batch loss = 0.26 (29.2 examples/sec; 0.274 sec/batch; 24h:10m:41s remains)
INFO - root - 2017-12-16 04:49:05.726108: step 14850, loss = 0.24, batch loss = 0.18 (27.7 examples/sec; 0.288 sec/batch; 25h:27m:02s remains)
INFO - root - 2017-12-16 04:49:08.493708: step 14860, loss = 0.23, batch loss = 0.17 (29.6 examples/sec; 0.270 sec/batch; 23h:51m:26s remains)
INFO - root - 2017-12-16 04:49:11.329868: step 14870, loss = 0.29, batch loss = 0.23 (28.9 examples/sec; 0.277 sec/batch; 24h:25m:41s remains)
INFO - root - 2017-12-16 04:49:14.129168: step 14880, loss = 0.26, batch loss = 0.20 (29.4 examples/sec; 0.272 sec/batch; 24h:00m:12s remains)
INFO - root - 2017-12-16 04:49:16.932039: step 14890, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.286 sec/batch; 25h:12m:59s remains)
INFO - root - 2017-12-16 04:49:19.741489: step 14900, loss = 0.22, batch loss = 0.16 (29.7 examples/sec; 0.269 sec/batch; 23h:44m:47s remains)
2017-12-16 04:49:20.201057: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5770698 -2.5022731 -2.5688291 -3.0369344 -3.5058782 -4.0154366 -4.6426706 -5.196312 -5.6838784 -5.7944546 -5.8110037 -5.4295349 -4.8206468 -4.3104639 -4.0062604][-1.4799078 -1.4623523 -1.7145615 -2.2679079 -2.7533226 -3.4663773 -4.2794394 -4.9074187 -5.3108282 -5.4460335 -5.443872 -5.1128378 -4.6067839 -4.0967512 -3.696147][-0.40040922 -0.60132241 -1.0795381 -1.6818943 -2.2754266 -2.8192303 -3.2425809 -3.7878766 -4.1104503 -4.2178669 -4.28948 -4.1853232 -3.9416156 -3.70834 -3.3950706][0.62850666 0.12541151 -0.65253568 -1.2711258 -1.6857009 -1.8214631 -1.8831043 -2.0900686 -2.2315326 -2.5380845 -2.8371496 -3.1853261 -3.399663 -3.2538328 -2.7860651][0.70148325 -0.00085401535 -0.72643495 -1.1307406 -1.2259736 -0.83610129 -0.2813406 0.0014176369 -0.010799885 -0.65671778 -1.5720105 -2.4975305 -3.0261011 -3.0976682 -2.6886272][-0.644264 -1.0463133 -1.4910448 -1.3628104 -0.61807394 0.36162376 1.4018917 2.0767794 2.0028114 0.91140985 -0.60394883 -2.1044891 -3.154597 -3.4832447 -3.0819595][-2.1513216 -2.1028426 -1.9126701 -1.3322129 0.049726486 1.6802306 3.172698 3.8162346 3.4682975 2.0430846 0.010773659 -1.8551188 -3.0363526 -3.5631173 -3.400867][-3.0900469 -2.9912496 -2.5070248 -1.317064 0.49518728 2.1958156 3.6264486 4.2627192 3.6613016 1.9150987 -0.21427536 -2.0028942 -3.1847415 -3.7487071 -3.6127744][-3.8314881 -3.6587019 -3.0127487 -1.8655901 -0.37545395 1.2903523 2.6691008 3.0389776 2.3468227 0.79853249 -1.1680009 -2.8247075 -3.9027877 -4.331399 -4.0964732][-4.3476391 -4.2703271 -3.9989927 -3.1598592 -1.6604352 -0.35614157 0.54658079 0.97319984 0.55640793 -0.79776239 -2.3575792 -3.7075837 -4.5317163 -4.7077584 -4.3808818][-4.6978607 -4.97868 -4.8600955 -4.3732424 -3.5483136 -2.4892969 -1.7633338 -1.462606 -1.7861805 -2.5837891 -3.4186656 -4.2169294 -4.6744885 -4.5224175 -3.9464474][-4.7208977 -5.061758 -5.2922192 -5.0784431 -4.5408807 -3.9422565 -3.4758918 -3.2725134 -3.4251096 -3.8357749 -4.3692613 -4.7853746 -4.8640594 -4.481081 -3.8527093][-4.7245073 -4.9171929 -5.0765705 -4.8957777 -4.5712829 -4.28599 -3.8131752 -3.8293283 -3.9947639 -4.2790079 -4.5946984 -4.7869711 -4.7938967 -4.5155506 -4.066309][-4.6909423 -4.5918255 -4.5515366 -4.2956948 -4.0887508 -3.8756642 -3.5789304 -3.5870428 -3.5803061 -3.8858724 -4.2040248 -4.2624669 -4.2576275 -4.0496392 -3.7314725][-4.1235023 -3.7641633 -3.4971323 -3.1519208 -2.9046564 -2.7658477 -2.6502562 -2.6816905 -2.8275146 -3.2421789 -3.6146007 -3.8144469 -3.88021 -3.8146489 -3.7097178]]...]
INFO - root - 2017-12-16 04:49:23.014281: step 14910, loss = 0.27, batch loss = 0.21 (27.6 examples/sec; 0.290 sec/batch; 25h:36m:54s remains)
INFO - root - 2017-12-16 04:49:25.896936: step 14920, loss = 0.26, batch loss = 0.20 (29.4 examples/sec; 0.272 sec/batch; 23h:58m:15s remains)
INFO - root - 2017-12-16 04:49:28.780672: step 14930, loss = 0.38, batch loss = 0.32 (29.0 examples/sec; 0.276 sec/batch; 24h:21m:04s remains)
INFO - root - 2017-12-16 04:49:31.601745: step 14940, loss = 0.38, batch loss = 0.33 (28.1 examples/sec; 0.285 sec/batch; 25h:08m:22s remains)
INFO - root - 2017-12-16 04:49:34.400003: step 14950, loss = 0.32, batch loss = 0.26 (28.1 examples/sec; 0.285 sec/batch; 25h:07m:49s remains)
INFO - root - 2017-12-16 04:49:37.312367: step 14960, loss = 0.29, batch loss = 0.23 (27.5 examples/sec; 0.291 sec/batch; 25h:42m:01s remains)
INFO - root - 2017-12-16 04:49:40.147442: step 14970, loss = 0.20, batch loss = 0.15 (29.4 examples/sec; 0.272 sec/batch; 23h:59m:33s remains)
INFO - root - 2017-12-16 04:49:42.989826: step 14980, loss = 0.20, batch loss = 0.15 (27.7 examples/sec; 0.289 sec/batch; 25h:29m:49s remains)
INFO - root - 2017-12-16 04:49:45.825823: step 14990, loss = 0.32, batch loss = 0.26 (28.2 examples/sec; 0.284 sec/batch; 25h:00m:31s remains)
INFO - root - 2017-12-16 04:49:48.613078: step 15000, loss = 0.29, batch loss = 0.23 (28.5 examples/sec; 0.281 sec/batch; 24h:46m:58s remains)
2017-12-16 04:49:49.065125: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.901258 -2.8982682 -2.812464 -2.6375136 -2.612391 -2.7996175 -3.0289223 -3.1984026 -3.5203803 -3.6832981 -3.7434914 -3.8072996 -3.8779607 -3.7784274 -3.6180692][-2.6765137 -2.5856457 -2.5041151 -2.6102481 -2.7542353 -2.8386464 -3.1153884 -3.4172146 -3.6759324 -3.8934293 -4.1599255 -4.1274142 -4.008894 -3.9945719 -3.9249566][-1.5163634 -1.7669959 -2.0361667 -2.2318242 -2.5015471 -2.8371563 -3.1681812 -3.452188 -3.8810339 -4.0953197 -4.2926764 -4.5649037 -4.5820732 -4.3674264 -4.1336594][-0.41134214 -0.75224614 -1.182534 -1.6741586 -2.0095696 -2.2903023 -2.657537 -2.7507081 -2.8902307 -3.4175496 -3.9455991 -4.2205396 -4.3239865 -4.4057178 -4.2492294][0.64087009 0.049717426 -0.61885619 -1.2702787 -1.741972 -1.8985038 -1.8804002 -1.9896243 -2.1663654 -2.4928474 -3.0412962 -3.666708 -3.8986876 -3.9874642 -3.8774028][1.4424062 0.74370623 -0.15601969 -0.85718036 -1.0016837 -1.0536973 -1.0777409 -1.0226047 -1.2303095 -1.8465629 -2.5611758 -3.232857 -3.5670593 -3.7425239 -3.6896477][1.2346473 0.47166348 -0.0019807816 -0.31367016 -0.34688997 0.027675629 0.34921074 0.37491798 0.062517643 -0.81980038 -1.8858511 -2.727632 -3.1928086 -3.5053177 -3.5205574][0.34619951 -0.59435654 -1.0914695 -0.82202792 -0.082662106 0.71308756 1.2662311 1.4947071 1.2056036 0.27339745 -0.864887 -1.9851897 -2.6734672 -3.005352 -3.2184281][-1.8037395 -2.3133531 -2.2090521 -1.8691711 -1.011761 0.14491081 0.96829891 1.4368849 1.2211208 0.32356739 -0.6175499 -1.4925613 -2.1912386 -2.4576306 -2.530345][-3.3451571 -4.1552763 -4.1992369 -3.1689453 -1.8953621 -0.78946304 0.050386429 0.368711 0.35674381 -0.10935402 -1.049715 -1.7798939 -1.9635825 -1.9040689 -1.7766197][-4.8075562 -5.1902118 -5.1522913 -4.435173 -3.3149228 -2.2177374 -1.3326833 -0.85943627 -0.8741498 -1.1669207 -1.5725565 -1.9304466 -1.9448817 -1.7009845 -1.3407381][-5.2236166 -5.9132056 -6.2128491 -5.7396274 -5.1548262 -4.3833604 -3.4752724 -2.8282175 -2.2357852 -2.0303147 -1.9904923 -1.8666978 -1.7149339 -1.3580329 -1.0723004][-5.1888981 -5.8549833 -6.5939236 -6.7752252 -6.627306 -6.1281991 -5.1999912 -4.2556047 -3.2336645 -2.4059794 -2.0998328 -1.6857615 -1.343468 -1.2194831 -1.0474803][-4.4361672 -5.1451354 -6.0017238 -6.6048961 -7.0446949 -6.9873624 -6.0280685 -4.8020072 -3.6083698 -2.6039684 -2.0419412 -1.4820545 -0.92602968 -0.73683524 -0.89309454][-3.5485356 -4.0207081 -5.1801996 -6.1786475 -6.7600641 -7.1139908 -6.6613235 -5.4730892 -3.9772811 -2.8974671 -2.1320486 -1.3299992 -0.8316915 -0.62214088 -0.6003089]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-15000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-15000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 04:49:52.264723: step 15010, loss = 0.19, batch loss = 0.13 (29.5 examples/sec; 0.271 sec/batch; 23h:56m:02s remains)
INFO - root - 2017-12-16 04:49:55.129859: step 15020, loss = 0.28, batch loss = 0.23 (29.0 examples/sec; 0.276 sec/batch; 24h:20m:44s remains)
INFO - root - 2017-12-16 04:49:57.979561: step 15030, loss = 0.22, batch loss = 0.16 (28.1 examples/sec; 0.285 sec/batch; 25h:08m:01s remains)
INFO - root - 2017-12-16 04:50:00.761380: step 15040, loss = 0.19, batch loss = 0.14 (29.3 examples/sec; 0.273 sec/batch; 24h:07m:04s remains)
INFO - root - 2017-12-16 04:50:03.644146: step 15050, loss = 0.25, batch loss = 0.19 (26.8 examples/sec; 0.298 sec/batch; 26h:19m:07s remains)
INFO - root - 2017-12-16 04:50:06.494547: step 15060, loss = 0.28, batch loss = 0.22 (27.4 examples/sec; 0.292 sec/batch; 25h:45m:55s remains)
INFO - root - 2017-12-16 04:50:09.348309: step 15070, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.282 sec/batch; 24h:51m:28s remains)
INFO - root - 2017-12-16 04:50:12.145364: step 15080, loss = 0.24, batch loss = 0.18 (29.4 examples/sec; 0.272 sec/batch; 24h:00m:16s remains)
INFO - root - 2017-12-16 04:50:15.008255: step 15090, loss = 0.30, batch loss = 0.24 (27.3 examples/sec; 0.293 sec/batch; 25h:48m:39s remains)
INFO - root - 2017-12-16 04:50:17.860755: step 15100, loss = 0.24, batch loss = 0.18 (27.5 examples/sec; 0.291 sec/batch; 25h:40m:36s remains)
2017-12-16 04:50:18.321067: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1034923 -5.2173281 -5.407825 -5.3403783 -5.1846547 -4.8851137 -4.7052341 -4.5192194 -4.4657021 -4.49235 -4.420579 -4.3030663 -4.0650015 -3.8724313 -3.6800423][-5.3467402 -5.4088287 -5.4603963 -5.353797 -5.1413746 -4.952352 -4.8130965 -4.5293503 -4.3047838 -4.1569061 -4.0207295 -3.8608184 -3.7027986 -3.555573 -3.5064974][-5.3514991 -5.2800007 -5.1542153 -4.9618564 -4.7711244 -4.6106691 -4.3945317 -4.1800222 -3.9240541 -3.6701171 -3.4634883 -3.3849041 -3.3675895 -3.2987819 -3.3442934][-4.7246656 -4.6226873 -4.4748788 -4.3022571 -4.06156 -3.8306065 -3.5140376 -3.0820405 -2.6391792 -2.2703519 -1.972393 -1.9243522 -2.0193973 -2.2386734 -2.4845614][-3.8044138 -3.5045326 -3.2334728 -2.9693203 -2.5880094 -2.2791903 -1.9619348 -1.5141532 -1.0141413 -0.7827096 -0.69258857 -0.77778888 -0.99368405 -1.4320757 -1.8661125][-2.5507073 -2.1359196 -1.7551622 -1.4497294 -0.9881227 -0.63131094 -0.2718873 -0.027764797 0.1715827 0.33297443 0.25492907 -0.061057568 -0.42873669 -0.8626039 -1.2326932][-1.7540705 -1.2182074 -0.73814559 -0.37528086 0.018485069 0.40661335 0.76775026 0.96498632 1.0380926 0.95427036 0.774251 0.44426155 0.0542078 -0.40278196 -0.78513503][-1.5179725 -0.71532989 0.026771545 0.55423117 0.96714735 1.3101201 1.5211864 1.624413 1.5246239 1.1696305 0.81439781 0.38988733 0.025283813 -0.43609333 -0.83673549][-1.4403796 -0.52690339 0.30289841 0.98906374 1.494329 1.7517452 1.8382897 1.7777891 1.5105839 0.98939753 0.60175753 0.19470119 -0.17773867 -0.59844208 -1.0292547][-1.6339896 -0.6972971 0.14294815 0.81278324 1.243824 1.3768177 1.3380556 1.1339121 0.77892017 0.30192566 -0.048941612 -0.34046555 -0.60118508 -0.94858909 -1.3619697][-2.4241157 -1.4938993 -0.62485647 -0.05239296 0.21107197 0.31442547 0.29957438 0.023746014 -0.32445717 -0.66191983 -0.96100044 -1.2107415 -1.4591148 -1.6301899 -1.9020197][-3.561434 -2.8027444 -1.9903653 -1.4344327 -1.2800472 -1.286262 -1.2920637 -1.4271834 -1.6538963 -1.7766902 -1.8927462 -1.985904 -2.1724091 -2.24939 -2.45752][-4.8609018 -4.2748084 -3.6294491 -3.1138024 -2.9016504 -2.926506 -2.9745498 -2.9790359 -2.9920163 -2.7660511 -2.5371637 -2.3250825 -2.245753 -2.2570133 -2.4855812][-6.402329 -5.8368797 -5.2715359 -4.7278395 -4.44458 -4.3717546 -4.3600574 -4.3185186 -4.234508 -3.7424357 -3.2727504 -2.7831905 -2.3817453 -2.1515441 -2.2374387][-7.0596514 -6.487823 -5.9074373 -5.5005178 -5.3253865 -5.2820992 -5.3491826 -5.319417 -5.2174134 -4.6896954 -4.1188936 -3.4737115 -2.8986139 -2.4967666 -2.368906]]...]
INFO - root - 2017-12-16 04:50:21.153429: step 15110, loss = 0.30, batch loss = 0.24 (27.6 examples/sec; 0.290 sec/batch; 25h:33m:15s remains)
INFO - root - 2017-12-16 04:50:23.984350: step 15120, loss = 0.41, batch loss = 0.35 (27.8 examples/sec; 0.288 sec/batch; 25h:22m:39s remains)
INFO - root - 2017-12-16 04:50:26.824184: step 15130, loss = 0.30, batch loss = 0.24 (29.3 examples/sec; 0.273 sec/batch; 24h:01m:49s remains)
INFO - root - 2017-12-16 04:50:29.656765: step 15140, loss = 0.22, batch loss = 0.17 (28.1 examples/sec; 0.284 sec/batch; 25h:03m:27s remains)
INFO - root - 2017-12-16 04:50:32.443108: step 15150, loss = 0.21, batch loss = 0.15 (29.2 examples/sec; 0.274 sec/batch; 24h:10m:44s remains)
INFO - root - 2017-12-16 04:50:35.289508: step 15160, loss = 0.30, batch loss = 0.24 (26.3 examples/sec; 0.304 sec/batch; 26h:46m:21s remains)
INFO - root - 2017-12-16 04:50:38.110238: step 15170, loss = 0.20, batch loss = 0.14 (28.8 examples/sec; 0.278 sec/batch; 24h:31m:13s remains)
INFO - root - 2017-12-16 04:50:40.982718: step 15180, loss = 0.37, batch loss = 0.31 (28.6 examples/sec; 0.280 sec/batch; 24h:41m:33s remains)
INFO - root - 2017-12-16 04:50:43.797665: step 15190, loss = 0.49, batch loss = 0.44 (28.1 examples/sec; 0.285 sec/batch; 25h:06m:00s remains)
INFO - root - 2017-12-16 04:50:46.616419: step 15200, loss = 0.22, batch loss = 0.16 (29.1 examples/sec; 0.275 sec/batch; 24h:12m:54s remains)
2017-12-16 04:50:47.075795: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6986687 -2.8146882 -2.9675469 -3.2821968 -3.6234784 -4.0120649 -4.4014978 -4.9371977 -5.2920322 -5.1404729 -4.622345 -4.0209951 -3.437937 -2.7548006 -2.1714475][-2.3966072 -2.6834631 -2.9169846 -3.2833209 -3.6199925 -3.8592293 -4.1020546 -4.3346968 -4.3507838 -4.0381765 -3.4901922 -2.778193 -2.0784452 -1.5706334 -1.3271799][-2.4490337 -2.8562055 -3.1601949 -3.4360573 -3.5625377 -3.4455061 -3.4091835 -3.4895062 -3.4553466 -3.0740643 -2.6039944 -2.0997784 -1.6922519 -1.246453 -1.0927241][-2.7007685 -3.1261311 -3.3398619 -3.3305407 -3.0362682 -2.4996085 -1.9555681 -1.7301118 -1.7085199 -1.6061947 -1.4100289 -1.2831545 -1.3967125 -1.4401407 -1.3955188][-3.3428955 -3.6677554 -3.4959669 -2.8991032 -2.0521173 -0.93797493 0.10777807 0.6499238 0.51864481 0.18145132 -0.11076498 -0.45773983 -0.99654508 -1.4007504 -1.7397771][-3.6036263 -3.8152647 -3.3886204 -2.3089685 -0.80894136 0.89381075 2.3407888 3.1408114 3.0084991 2.223012 1.2312756 0.32899427 -0.66818857 -1.2989471 -1.6784089][-3.5767298 -3.5665879 -2.7766094 -1.1712313 0.62772655 2.7499208 4.5785532 5.3431187 4.8332882 3.6053381 2.1600008 0.73588467 -0.5481081 -1.2718527 -1.6493845][-3.5329351 -3.4894462 -2.5724759 -0.71769619 1.4071417 3.5353937 5.3212948 6.2797461 5.8594055 4.117506 2.1244392 0.51334763 -0.78292704 -1.5270426 -1.8247967][-3.4850557 -3.5418491 -2.8165665 -0.98250222 1.1769142 3.1849279 4.7415447 5.4427032 4.9170551 3.173624 1.119236 -0.51391864 -1.5185614 -2.0316298 -2.1500037][-3.3862848 -3.6067767 -3.202426 -1.8706801 -0.0844779 1.7129292 2.9278426 3.3899555 2.8304634 1.2676096 -0.57265353 -2.1340609 -3.0211506 -3.1602869 -3.0311399][-3.6404724 -3.9266443 -3.6275883 -2.5528007 -1.2185779 0.18299913 1.0882721 1.2407875 0.60579157 -0.67200184 -2.2471657 -3.6248353 -4.3741846 -4.470808 -4.2608786][-4.1920018 -4.4991436 -4.4472604 -3.6550872 -2.3991005 -1.1922016 -0.72084737 -0.873862 -1.4246013 -2.5251589 -3.840405 -5.004343 -5.7195368 -5.6898651 -5.3237829][-4.3502293 -4.5049639 -4.51839 -4.159389 -3.5550752 -2.5753629 -1.9322157 -2.0619779 -2.685277 -3.5779946 -4.5828772 -5.6158376 -6.2585411 -6.4735689 -6.11145][-4.3871994 -4.3123584 -4.1042237 -3.8931105 -3.4839461 -2.7881212 -2.2825394 -2.3468838 -3.0184498 -3.8246083 -4.6403909 -5.4383268 -6.0759463 -6.0307436 -5.4726334][-4.0472817 -3.8389 -3.6268432 -3.5040755 -3.3359768 -2.6881003 -2.1652324 -2.1062505 -2.4740956 -3.2106376 -4.0549779 -4.7797575 -5.2988582 -5.3412857 -4.7474265]]...]
INFO - root - 2017-12-16 04:50:49.902932: step 15210, loss = 0.31, batch loss = 0.25 (27.9 examples/sec; 0.287 sec/batch; 25h:18m:18s remains)
INFO - root - 2017-12-16 04:50:52.702861: step 15220, loss = 0.28, batch loss = 0.22 (27.6 examples/sec; 0.290 sec/batch; 25h:33m:04s remains)
INFO - root - 2017-12-16 04:50:55.587846: step 15230, loss = 0.21, batch loss = 0.15 (28.5 examples/sec; 0.281 sec/batch; 24h:44m:35s remains)
INFO - root - 2017-12-16 04:50:58.452263: step 15240, loss = 0.21, batch loss = 0.15 (28.5 examples/sec; 0.280 sec/batch; 24h:42m:21s remains)
INFO - root - 2017-12-16 04:51:01.284765: step 15250, loss = 0.21, batch loss = 0.15 (28.1 examples/sec; 0.285 sec/batch; 25h:04m:26s remains)
INFO - root - 2017-12-16 04:51:04.092320: step 15260, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.282 sec/batch; 24h:51m:54s remains)
INFO - root - 2017-12-16 04:51:06.925827: step 15270, loss = 0.27, batch loss = 0.22 (28.8 examples/sec; 0.278 sec/batch; 24h:29m:03s remains)
INFO - root - 2017-12-16 04:51:09.754614: step 15280, loss = 0.22, batch loss = 0.16 (29.1 examples/sec; 0.275 sec/batch; 24h:11m:17s remains)
INFO - root - 2017-12-16 04:51:12.571842: step 15290, loss = 0.34, batch loss = 0.28 (27.8 examples/sec; 0.288 sec/batch; 25h:22m:30s remains)
INFO - root - 2017-12-16 04:51:15.385239: step 15300, loss = 0.26, batch loss = 0.20 (28.5 examples/sec; 0.281 sec/batch; 24h:46m:22s remains)
2017-12-16 04:51:15.844303: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9643376 -3.9709558 -4.0175405 -4.2511654 -4.463769 -4.4134221 -4.1795974 -3.9303579 -3.5367715 -3.2716169 -3.1688251 -3.1866884 -3.0748386 -2.912519 -2.9761846][-2.9406672 -3.1211388 -3.3807454 -3.7655749 -3.9385738 -4.0026569 -3.8809414 -3.7076123 -3.6727097 -3.6041815 -3.5474443 -3.5805826 -3.4687161 -3.2736397 -3.158978][-2.1819403 -2.2329135 -2.4784536 -2.8562503 -3.2746205 -3.599544 -3.5745292 -3.6279774 -3.7092321 -3.9238889 -4.1386223 -4.1601405 -4.0415907 -3.7044044 -3.3838639][-1.3705859 -1.7772419 -2.2158816 -2.4910865 -2.7056403 -2.8928251 -2.8533156 -2.8612256 -3.0151663 -3.3833506 -3.8907354 -4.1578827 -4.2071609 -3.8583169 -3.5654941][-0.85089183 -1.4472277 -1.973067 -2.3365102 -2.3891339 -2.2866802 -1.9085605 -1.7061772 -1.7792385 -2.2701306 -2.7290301 -3.267319 -3.6051028 -3.6095755 -3.516552][-1.1561279 -1.5692322 -1.8730283 -2.0090749 -1.8777292 -1.5434113 -0.91762805 -0.35395145 -0.24879408 -0.68463635 -1.3772776 -2.2001619 -2.8162708 -3.1655998 -3.1804395][-1.6331427 -1.99984 -2.3633592 -2.1436765 -1.4363275 -0.55700064 0.50888395 1.3132854 1.6963692 1.2692676 0.33405161 -0.71796107 -1.6687024 -2.3215392 -2.7561474][-2.3218372 -2.743993 -2.9107192 -2.5001917 -1.5522645 -0.028309822 1.6529522 2.6198869 2.9545989 2.5474763 1.6108499 0.27374029 -1.057236 -1.9668162 -2.5186243][-3.27782 -3.6856678 -3.6972094 -3.2955041 -2.4643278 -0.89077973 0.9824295 2.0840216 2.5237217 2.0911007 1.1361957 -0.16521597 -1.4512751 -2.2721364 -2.7527697][-3.9835644 -4.5651169 -4.796001 -4.2866793 -3.2493255 -1.8679721 -0.39086103 0.67146397 1.2717042 0.92613649 0.032359123 -1.2539542 -2.3715339 -2.9573212 -3.1534805][-4.5663805 -4.987113 -5.1063786 -4.938396 -4.2099676 -2.7059503 -1.1874509 -0.54201865 -0.48077059 -0.84050989 -1.517863 -2.5414929 -3.5306196 -3.8099854 -3.7263026][-4.5620942 -5.0416317 -5.3760257 -5.2163959 -4.6775336 -3.9688315 -2.9733524 -2.2691677 -2.0434873 -2.3086648 -3.020154 -3.9033854 -4.5463495 -4.6086903 -4.1857805][-4.2083621 -4.5829782 -4.968576 -4.9550843 -4.5630126 -4.0905609 -3.4852128 -3.0591331 -2.8515682 -2.9271088 -3.4087141 -3.9612422 -4.4901271 -4.4770317 -4.2080622][-4.0229526 -4.0579624 -4.2628317 -4.2776084 -4.0046492 -3.3593621 -2.6261685 -2.3520734 -2.3066394 -2.3013804 -2.6165962 -3.3165383 -3.9314132 -4.0293484 -3.6779363][-3.7950807 -3.648982 -3.6366854 -3.6665852 -3.2874746 -2.6681533 -2.0879459 -1.7518327 -1.600215 -1.6827724 -2.0469773 -2.454289 -2.845161 -3.1206927 -3.0891073]]...]
INFO - root - 2017-12-16 04:51:18.666822: step 15310, loss = 0.27, batch loss = 0.22 (28.9 examples/sec; 0.276 sec/batch; 24h:21m:18s remains)
INFO - root - 2017-12-16 04:51:21.475228: step 15320, loss = 0.28, batch loss = 0.22 (28.0 examples/sec; 0.286 sec/batch; 25h:12m:35s remains)
INFO - root - 2017-12-16 04:51:24.278872: step 15330, loss = 0.27, batch loss = 0.21 (29.2 examples/sec; 0.274 sec/batch; 24h:08m:37s remains)
INFO - root - 2017-12-16 04:51:27.046579: step 15340, loss = 0.23, batch loss = 0.17 (27.9 examples/sec; 0.287 sec/batch; 25h:14m:50s remains)
INFO - root - 2017-12-16 04:51:29.863085: step 15350, loss = 0.26, batch loss = 0.20 (28.2 examples/sec; 0.284 sec/batch; 24h:59m:54s remains)
INFO - root - 2017-12-16 04:51:32.651813: step 15360, loss = 0.23, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 24h:05m:55s remains)
INFO - root - 2017-12-16 04:51:35.498587: step 15370, loss = 0.29, batch loss = 0.23 (26.5 examples/sec; 0.302 sec/batch; 26h:36m:19s remains)
INFO - root - 2017-12-16 04:51:38.349826: step 15380, loss = 0.46, batch loss = 0.40 (27.1 examples/sec; 0.295 sec/batch; 25h:57m:26s remains)
INFO - root - 2017-12-16 04:51:41.203808: step 15390, loss = 0.33, batch loss = 0.28 (28.3 examples/sec; 0.283 sec/batch; 24h:56m:24s remains)
INFO - root - 2017-12-16 04:51:44.008746: step 15400, loss = 0.37, batch loss = 0.31 (27.7 examples/sec; 0.289 sec/batch; 25h:27m:13s remains)
2017-12-16 04:51:44.462239: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.6491542 -6.5883608 -6.2228765 -5.7982049 -5.4743819 -5.3405266 -5.4080067 -5.5345449 -5.4742575 -5.1520538 -4.7633829 -4.5711803 -4.4837708 -4.5860019 -4.8859582][-5.8571682 -5.93929 -5.7242041 -5.3978748 -4.9982033 -4.6372547 -4.473515 -4.550631 -4.4706149 -4.1806841 -3.9911633 -4.0414376 -4.0873504 -4.2105818 -4.5630469][-5.1053648 -5.3272729 -5.1066236 -4.6913581 -4.2125421 -3.7279878 -3.3642426 -3.1126742 -3.1198406 -3.1010821 -3.1890607 -3.4348192 -3.7216911 -4.0297375 -4.3543148][-4.437891 -4.8716354 -4.949337 -4.5030308 -3.557987 -2.6019588 -1.953197 -1.5917525 -1.5705731 -1.8340883 -2.2322943 -2.7193885 -3.2182982 -3.5637312 -3.8811393][-3.8519392 -4.2524705 -4.1871109 -3.8186207 -2.8338685 -1.5276856 -0.35098314 0.19994068 0.17119408 -0.35353136 -1.1797976 -1.9670985 -2.6969669 -3.2586479 -3.7047541][-3.2244349 -3.5616374 -3.4632289 -2.9008846 -1.6124413 -0.17087126 1.2382936 1.8942966 1.7275686 0.90421772 -0.18264771 -1.2882221 -2.3378265 -3.0826626 -3.6034749][-2.7456803 -2.987555 -2.9673314 -2.3660676 -0.99764442 0.57606936 2.1081462 2.7834549 2.5556521 1.6566844 0.53998709 -0.66970015 -1.7565203 -2.5983529 -3.3203278][-2.3427765 -2.4671957 -2.3226225 -1.8844006 -0.74476981 0.762414 2.2258306 2.8066678 2.6991038 1.8838096 0.82291317 -0.31604147 -1.3654377 -2.2669325 -3.1015511][-1.8388274 -2.0722668 -2.0317254 -1.7438514 -0.88389444 0.16883469 1.4554625 2.0858536 2.0271983 1.5646858 0.87205029 -0.11075592 -1.0512266 -2.0018592 -3.0572054][-1.6018889 -1.7994447 -1.8437526 -1.947192 -1.5379381 -0.81103873 -0.13030195 0.38670444 0.59280634 0.56691408 0.22536898 -0.4345777 -1.1421962 -2.0981567 -3.2451158][-1.9055648 -2.1310251 -2.2496116 -2.5983047 -2.6487732 -2.3644412 -1.9246469 -1.5253599 -1.3537731 -1.0028348 -0.81661963 -1.1459479 -1.7659085 -2.6877713 -3.7213337][-3.1850405 -3.3338106 -3.4801567 -3.9358397 -4.2043309 -4.2022882 -4.239727 -3.873235 -3.3886211 -2.8582127 -2.5483742 -2.595077 -2.7214658 -3.5193291 -4.46939][-4.8593669 -4.9744916 -5.0923581 -5.5168056 -5.8584471 -5.9843464 -6.0081339 -5.8460312 -5.5588574 -4.8318052 -4.1468029 -4.0635195 -4.2535419 -4.7166123 -5.2833571][-6.662528 -6.6304541 -6.5894279 -6.8646994 -7.1461878 -7.300458 -7.5383005 -7.4495878 -7.2026362 -6.6608734 -6.1995087 -5.8510294 -5.6549339 -5.8887606 -6.1445494][-8.147027 -8.0383654 -7.9040833 -7.8143177 -7.91096 -8.1215925 -8.3879662 -8.50186 -8.5545063 -8.2793369 -7.8417406 -7.5100965 -7.3703361 -7.264308 -7.0767345]]...]
INFO - root - 2017-12-16 04:51:47.278282: step 15410, loss = 0.26, batch loss = 0.20 (29.3 examples/sec; 0.273 sec/batch; 24h:01m:09s remains)
INFO - root - 2017-12-16 04:51:50.085052: step 15420, loss = 0.26, batch loss = 0.20 (28.8 examples/sec; 0.278 sec/batch; 24h:27m:46s remains)
INFO - root - 2017-12-16 04:51:52.949844: step 15430, loss = 0.26, batch loss = 0.20 (28.2 examples/sec; 0.284 sec/batch; 25h:01m:46s remains)
INFO - root - 2017-12-16 04:51:55.759090: step 15440, loss = 0.34, batch loss = 0.28 (29.7 examples/sec; 0.270 sec/batch; 23h:44m:48s remains)
INFO - root - 2017-12-16 04:51:58.589936: step 15450, loss = 0.36, batch loss = 0.30 (28.7 examples/sec; 0.279 sec/batch; 24h:35m:22s remains)
INFO - root - 2017-12-16 04:52:01.467241: step 15460, loss = 0.25, batch loss = 0.19 (28.5 examples/sec; 0.281 sec/batch; 24h:44m:35s remains)
INFO - root - 2017-12-16 04:52:04.296688: step 15470, loss = 0.23, batch loss = 0.18 (26.0 examples/sec; 0.308 sec/batch; 27h:06m:46s remains)
INFO - root - 2017-12-16 04:52:07.114740: step 15480, loss = 0.27, batch loss = 0.21 (27.9 examples/sec; 0.286 sec/batch; 25h:13m:01s remains)
INFO - root - 2017-12-16 04:52:09.975465: step 15490, loss = 0.22, batch loss = 0.16 (29.0 examples/sec; 0.276 sec/batch; 24h:18m:03s remains)
INFO - root - 2017-12-16 04:52:12.762671: step 15500, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.284 sec/batch; 25h:00m:22s remains)
2017-12-16 04:52:13.211500: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0216584 -4.7733932 -4.6423278 -4.5275459 -4.538518 -4.6774569 -4.92417 -5.1315784 -5.3772836 -5.4320173 -5.4399137 -5.3499842 -5.1767612 -4.9510775 -4.3517518][-6.04221 -5.6787367 -5.3622389 -5.2736855 -5.3268247 -5.5553031 -5.8185863 -5.9149551 -6.023632 -6.1422834 -6.304903 -6.2102261 -6.0307865 -5.9965806 -5.2876954][-6.6417837 -6.498003 -6.3414321 -6.2474704 -6.1363926 -6.1050105 -6.1091719 -6.1297388 -6.2473726 -6.2997284 -6.3995676 -6.4265609 -6.4629397 -6.3894563 -5.6465573][-7.5086527 -7.3130951 -7.1718483 -6.9105825 -6.5852852 -6.2152267 -5.8196483 -5.2488256 -4.964417 -4.9509463 -5.350184 -5.8907337 -6.2149096 -6.2328486 -5.6605597][-7.3147326 -7.3770628 -7.2595186 -6.8549409 -6.04308 -4.9819736 -3.914778 -2.9838541 -2.7747631 -2.9886246 -3.622015 -4.5475698 -5.3443279 -5.60575 -5.1172919][-6.3249869 -6.5844703 -6.3760414 -5.7502265 -4.537509 -3.0195332 -1.4194095 -0.14009953 0.37772942 0.14360571 -1.1277649 -2.6629937 -3.7197886 -4.3423967 -4.2331624][-5.4333286 -5.7297859 -5.5555272 -4.7914262 -3.2413013 -1.3496754 0.57180357 2.0085154 2.4898572 2.2521071 1.0289297 -0.69029093 -2.1558976 -2.8754206 -2.6997042][-4.4358578 -4.7642908 -4.5445957 -3.7501411 -2.1789451 -0.075680733 1.9345722 3.3106151 3.7374496 3.1440449 1.7111583 0.06522131 -1.1539075 -1.8066852 -1.8707957][-3.326057 -3.6410489 -3.6490235 -2.9900317 -1.6378269 0.20103598 2.0064631 3.2212195 3.4685469 2.8835168 1.6193151 0.069879532 -1.0957439 -1.2500935 -0.75871992][-2.5179372 -2.8956296 -3.0133352 -2.7942224 -1.7293723 -0.21672487 1.1592278 2.1313663 2.302804 1.7024641 0.65686417 -0.42078781 -1.0885599 -1.010761 -0.12628412][-1.7902946 -2.1490581 -2.5191059 -2.7280562 -2.3332913 -1.2430396 -0.0060472488 0.768075 0.73316288 0.36287498 -0.3558526 -0.92799711 -1.000632 -0.4428196 0.47099495][-1.3557546 -1.6554632 -2.1033762 -2.5689657 -2.5319033 -2.1957219 -1.7277851 -1.211802 -1.1682425 -1.4536579 -1.9447834 -2.1085532 -1.8957026 -1.0910933 -0.099257946][-0.29195929 -0.58910227 -1.3279095 -2.2387791 -2.7957356 -2.8004022 -2.4945097 -2.3449447 -2.5188868 -2.8334217 -3.3073046 -3.4783924 -3.2701471 -2.6193738 -1.8580432][0.46991396 0.2770648 -0.57731509 -1.7849557 -2.7083378 -3.328054 -3.5807419 -3.5195882 -3.3641026 -3.5296831 -4.0784817 -4.3596797 -4.3408442 -4.1555362 -3.9536467][1.166779 0.86753082 -0.11722612 -1.4505928 -2.5852776 -3.4587455 -3.8195183 -4.01112 -4.1220169 -4.1319666 -4.4023604 -4.7174511 -4.9411478 -4.8092237 -4.8018875]]...]
INFO - root - 2017-12-16 04:52:16.026806: step 15510, loss = 0.28, batch loss = 0.23 (28.0 examples/sec; 0.286 sec/batch; 25h:11m:01s remains)
INFO - root - 2017-12-16 04:52:18.818949: step 15520, loss = 0.32, batch loss = 0.26 (28.4 examples/sec; 0.281 sec/batch; 24h:46m:51s remains)
INFO - root - 2017-12-16 04:52:21.686873: step 15530, loss = 0.40, batch loss = 0.34 (27.3 examples/sec; 0.293 sec/batch; 25h:47m:11s remains)
INFO - root - 2017-12-16 04:52:24.511933: step 15540, loss = 0.26, batch loss = 0.20 (26.3 examples/sec; 0.304 sec/batch; 26h:48m:28s remains)
INFO - root - 2017-12-16 04:52:27.313603: step 15550, loss = 0.34, batch loss = 0.28 (28.3 examples/sec; 0.283 sec/batch; 24h:54m:34s remains)
INFO - root - 2017-12-16 04:52:30.140961: step 15560, loss = 0.26, batch loss = 0.20 (27.2 examples/sec; 0.294 sec/batch; 25h:54m:55s remains)
INFO - root - 2017-12-16 04:52:32.993195: step 15570, loss = 0.45, batch loss = 0.39 (28.9 examples/sec; 0.277 sec/batch; 24h:24m:06s remains)
INFO - root - 2017-12-16 04:52:35.801801: step 15580, loss = 0.25, batch loss = 0.19 (28.7 examples/sec; 0.279 sec/batch; 24h:31m:30s remains)
INFO - root - 2017-12-16 04:52:38.618718: step 15590, loss = 0.28, batch loss = 0.22 (28.0 examples/sec; 0.286 sec/batch; 25h:11m:36s remains)
INFO - root - 2017-12-16 04:52:41.487482: step 15600, loss = 0.33, batch loss = 0.27 (28.5 examples/sec; 0.281 sec/batch; 24h:41m:39s remains)
2017-12-16 04:52:41.935083: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.967525 -5.2827253 -5.3533077 -5.3453326 -5.2783766 -5.1861706 -5.1081724 -5.2644854 -5.5788007 -5.9396605 -6.3662214 -6.6461077 -6.5835886 -6.1516004 -5.5768251][-5.1163111 -5.3209448 -5.2593179 -5.1295242 -4.9966269 -4.95923 -5.0616436 -5.3365693 -5.6919355 -6.195168 -6.6739631 -6.9905186 -7.1098194 -6.73835 -6.1787543][-5.1342473 -5.1453772 -4.800776 -4.3941183 -3.9516313 -3.7379787 -3.8194449 -4.3413138 -5.0089822 -5.7029829 -6.2608128 -6.7045355 -7.0593233 -7.0670757 -6.8756266][-4.7960043 -4.5387473 -3.8266315 -3.0787563 -2.3728337 -1.9696538 -1.8864193 -2.3646414 -3.1288724 -4.0732837 -5.0098639 -5.8920655 -6.6552606 -7.1115 -7.2732081][-4.4982715 -3.8486881 -2.7042828 -1.6884973 -0.7132585 -0.076236725 0.22377729 -0.0055918694 -0.58886957 -1.607064 -2.9357874 -4.5190754 -5.9885497 -6.9923611 -7.4615922][-4.5128503 -3.618413 -2.1942437 -0.86332273 0.57161522 1.6745348 2.357698 2.3733821 1.8562331 0.73478937 -0.78390646 -2.9292576 -5.0727549 -6.6213503 -7.3433309][-4.7431784 -3.894454 -2.4670947 -0.7907567 0.98938179 2.4633036 3.5825796 3.8265839 3.3635974 2.0839462 0.40788507 -1.9037025 -4.1017985 -5.8868971 -6.8654795][-5.5067477 -4.5326295 -3.0532916 -1.4658327 0.28899813 1.9983048 3.3923216 3.8728647 3.6206226 2.3939109 0.59451962 -1.8675487 -4.0250154 -5.4562583 -6.191309][-6.3698587 -5.5128565 -4.1764956 -2.6871457 -0.95638061 0.83070517 2.1961641 2.7053132 2.4750371 1.3485222 -0.21579742 -2.4962106 -4.5439219 -5.775835 -6.1809082][-7.0537033 -6.5442305 -5.5720315 -4.1463628 -2.3839312 -0.60057306 0.73086071 1.0777431 0.728045 -0.37795734 -1.7767971 -3.6349533 -5.2448711 -6.2177773 -6.5912461][-6.8773746 -6.5505056 -5.9631968 -4.9238939 -3.7368808 -2.3146043 -1.0814235 -0.80406332 -1.0689089 -2.0978122 -3.3194127 -4.763876 -5.9600554 -6.6656237 -7.0070581][-6.4263983 -6.0758557 -5.710578 -5.0452766 -4.2829957 -3.3140206 -2.4912529 -2.3878119 -2.5281775 -3.2258153 -4.2034912 -5.499404 -6.5823841 -6.9953804 -7.1705546][-5.3947554 -5.1875405 -4.9810848 -4.5387869 -4.0840259 -3.481101 -3.0284898 -3.0586417 -3.2126265 -3.7601 -4.3948727 -5.2490482 -6.0677934 -6.4330916 -6.5290871][-4.2560172 -4.1077352 -3.9674127 -3.7695975 -3.6746936 -3.3493481 -3.0568869 -3.0997858 -3.2481704 -3.5483136 -3.8361089 -4.4142632 -4.9741611 -5.2280459 -5.3356419][-3.5084653 -3.2794404 -3.1256628 -3.0425825 -3.0816913 -3.0056758 -2.9463074 -2.9683404 -2.9985843 -3.155642 -3.3290889 -3.5692449 -3.8015072 -3.9615378 -4.0618868]]...]
INFO - root - 2017-12-16 04:52:44.763942: step 15610, loss = 0.31, batch loss = 0.25 (28.2 examples/sec; 0.283 sec/batch; 24h:56m:10s remains)
INFO - root - 2017-12-16 04:52:47.638645: step 15620, loss = 0.28, batch loss = 0.22 (28.0 examples/sec; 0.286 sec/batch; 25h:10m:17s remains)
INFO - root - 2017-12-16 04:52:50.431869: step 15630, loss = 0.26, batch loss = 0.20 (29.6 examples/sec; 0.271 sec/batch; 23h:49m:25s remains)
INFO - root - 2017-12-16 04:52:53.260106: step 15640, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.286 sec/batch; 25h:09m:24s remains)
INFO - root - 2017-12-16 04:52:56.095009: step 15650, loss = 0.21, batch loss = 0.16 (27.9 examples/sec; 0.287 sec/batch; 25h:15m:54s remains)
INFO - root - 2017-12-16 04:52:58.937795: step 15660, loss = 0.26, batch loss = 0.20 (25.7 examples/sec; 0.311 sec/batch; 27h:23m:29s remains)
INFO - root - 2017-12-16 04:53:01.742511: step 15670, loss = 0.24, batch loss = 0.18 (28.5 examples/sec; 0.280 sec/batch; 24h:40m:39s remains)
INFO - root - 2017-12-16 04:53:04.619494: step 15680, loss = 0.25, batch loss = 0.20 (29.2 examples/sec; 0.274 sec/batch; 24h:08m:36s remains)
INFO - root - 2017-12-16 04:53:07.485360: step 15690, loss = 0.27, batch loss = 0.22 (28.3 examples/sec; 0.282 sec/batch; 24h:50m:50s remains)
INFO - root - 2017-12-16 04:53:10.322281: step 15700, loss = 0.28, batch loss = 0.22 (29.2 examples/sec; 0.274 sec/batch; 24h:06m:53s remains)
2017-12-16 04:53:10.766381: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3778033 -3.7090721 -3.7999203 -3.717242 -3.5621226 -3.4218161 -3.2315946 -3.0563192 -3.277808 -3.8258553 -4.4267683 -5.10524 -5.4667373 -5.4573011 -5.15295][-3.9245784 -4.1718707 -4.2858853 -4.1529093 -3.8093123 -3.4859738 -3.408432 -3.4113624 -3.7084572 -4.3351631 -5.229207 -5.816 -5.88422 -5.7010627 -5.1719661][-4.69833 -4.8731756 -4.8223524 -4.4867582 -3.9428644 -3.5828321 -3.418819 -3.587081 -4.152926 -4.7752409 -5.611052 -6.2695856 -6.5102391 -6.1719379 -5.5070095][-5.5204916 -5.6430316 -5.4340358 -4.8306375 -4.0400324 -3.3202572 -2.9915755 -3.0952919 -3.5230076 -4.1375589 -5.108407 -5.7744679 -6.0468273 -5.914114 -5.510169][-5.9561739 -6.02171 -5.5835352 -4.741715 -3.6324363 -2.646698 -2.0196385 -1.830354 -2.2444351 -2.8979073 -3.8373981 -4.714983 -5.35301 -5.5408363 -5.4137177][-5.4637942 -5.2819557 -4.6750779 -3.6971552 -2.4029608 -1.2296174 -0.33206749 0.1346488 -0.12517118 -0.83347487 -1.9715488 -3.139185 -4.2193055 -5.0657616 -5.4184732][-3.7830005 -3.6791663 -3.1668606 -2.2137206 -0.89023304 0.33528185 1.2941475 1.8304992 1.690383 0.91221666 -0.26766825 -1.6072841 -3.0281646 -4.2196164 -5.0428414][-2.035347 -2.076885 -1.7858727 -1.1178627 -0.19461155 0.87241983 1.9026408 2.5041962 2.4092112 1.7353597 0.72043562 -0.67083 -2.2277324 -3.6187036 -4.5960641][-0.936476 -1.1275105 -1.0155013 -0.76213145 -0.22517681 0.66463566 1.6456609 2.2320261 2.251277 1.6625338 0.75610065 -0.5390377 -2.0442913 -3.5125222 -4.6510086][-0.37780476 -0.79734659 -0.97973585 -0.89096594 -0.58408189 -0.095395565 0.44701147 0.90129137 1.068399 0.61101723 -0.096871376 -1.17872 -2.484118 -3.7931411 -4.7883639][-0.80794072 -1.3577092 -1.6019492 -1.781786 -2.0309436 -1.7143738 -1.0554106 -0.52761865 -0.28427935 -0.54977655 -1.0014834 -1.8060701 -2.86461 -3.9172635 -4.8283682][-1.6745789 -2.2166121 -2.7198553 -2.9781194 -3.1983938 -2.9096498 -2.4063568 -1.8628962 -1.4138854 -1.3479681 -1.4818571 -2.0498185 -2.8080802 -3.5974131 -4.4211335][-2.9932342 -3.4443088 -3.7484286 -3.8462961 -3.9792359 -3.6676173 -3.1933932 -2.4938025 -1.9537024 -1.7445083 -1.6090574 -1.8576384 -2.3097069 -2.9872293 -3.842659][-3.8564317 -4.0453944 -4.2568984 -4.3362927 -4.5353847 -4.1170645 -3.4741154 -2.8189197 -2.2076058 -1.5124738 -0.99590349 -1.1611662 -1.5510168 -2.1625106 -2.9778996][-4.4495153 -4.3594351 -4.3549166 -4.3844891 -4.4944606 -4.2482119 -3.8300471 -2.9141634 -1.9340923 -1.0447228 -0.37542295 -0.3840189 -0.74048662 -1.3732195 -2.2166517]]...]
INFO - root - 2017-12-16 04:53:13.612838: step 15710, loss = 0.25, batch loss = 0.19 (29.3 examples/sec; 0.273 sec/batch; 24h:03m:44s remains)
INFO - root - 2017-12-16 04:53:16.436065: step 15720, loss = 0.45, batch loss = 0.39 (28.7 examples/sec; 0.278 sec/batch; 24h:30m:16s remains)
INFO - root - 2017-12-16 04:53:19.265926: step 15730, loss = 0.29, batch loss = 0.23 (28.7 examples/sec; 0.278 sec/batch; 24h:29m:06s remains)
INFO - root - 2017-12-16 04:53:22.132572: step 15740, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 24h:47m:32s remains)
INFO - root - 2017-12-16 04:53:24.960408: step 15750, loss = 0.23, batch loss = 0.17 (29.3 examples/sec; 0.273 sec/batch; 24h:01m:18s remains)
INFO - root - 2017-12-16 04:53:27.744839: step 15760, loss = 0.22, batch loss = 0.16 (29.0 examples/sec; 0.276 sec/batch; 24h:15m:54s remains)
INFO - root - 2017-12-16 04:53:30.581507: step 15770, loss = 0.27, batch loss = 0.21 (27.5 examples/sec; 0.291 sec/batch; 25h:34m:21s remains)
INFO - root - 2017-12-16 04:53:33.397631: step 15780, loss = 0.26, batch loss = 0.21 (28.3 examples/sec; 0.283 sec/batch; 24h:52m:09s remains)
INFO - root - 2017-12-16 04:53:36.217345: step 15790, loss = 0.26, batch loss = 0.21 (27.4 examples/sec; 0.292 sec/batch; 25h:38m:49s remains)
INFO - root - 2017-12-16 04:53:39.078784: step 15800, loss = 0.27, batch loss = 0.21 (28.7 examples/sec; 0.278 sec/batch; 24h:29m:54s remains)
2017-12-16 04:53:39.527126: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5574722 -4.8250003 -5.1681767 -5.270184 -5.4192185 -5.4602594 -5.5575547 -5.7305994 -5.7578897 -5.6344824 -5.3334961 -5.2141061 -5.3719816 -5.6505928 -5.8043652][-3.3906536 -3.5289321 -3.7850206 -4.0755019 -4.4435687 -4.9749036 -5.3973727 -5.4843225 -5.4915338 -5.5386643 -5.52209 -5.3713756 -5.4497428 -5.8192534 -6.0183969][-1.6881516 -1.9572048 -2.4103241 -2.9662514 -3.3479877 -3.6881175 -4.0603971 -4.4189787 -4.54682 -4.5962515 -4.7718029 -5.0797381 -5.4358997 -5.9133716 -6.1186852][-0.46068859 -0.67450809 -1.0003893 -1.3429611 -1.7916439 -2.1152925 -2.4170866 -2.4911788 -2.6876354 -3.094893 -3.5630035 -4.2440395 -4.8824606 -5.5229125 -5.8221679][0.26180124 0.27966547 0.20494413 0.12705755 0.16105461 0.1704998 0.034104824 -0.20878124 -0.40415382 -0.84089541 -1.6925793 -2.8278055 -3.8827984 -4.5402341 -4.894454][0.0940485 0.41993618 0.58921623 0.747036 0.98990822 1.4064379 1.9900212 2.2086954 2.1263204 1.4670167 0.38790035 -1.030237 -2.4199605 -3.4712834 -4.150022][-1.1594615 -0.57398796 -0.033813477 0.70188665 1.8357716 2.9883275 4.0255775 4.671814 4.6635838 3.8282738 2.3994026 0.74807787 -0.86421871 -2.2725601 -3.4059029][-2.3706326 -1.7966135 -1.2549388 -0.30036354 1.3012109 3.3600855 5.213542 6.0553932 6.1100712 5.4680271 3.7737675 1.727035 -0.14319944 -1.5380957 -2.8767056][-4.372272 -3.9222758 -3.2970302 -2.3815131 -0.67686009 1.6853161 3.8377485 4.9645567 5.3770056 4.9721422 3.7470856 2.0337548 -0.043974876 -1.9673226 -3.5152855][-5.8545284 -6.2053556 -6.3468232 -5.4085531 -3.7585671 -1.5619359 0.68813086 2.410852 3.1962428 2.9991808 2.1324244 0.69022369 -1.0831652 -2.7882023 -4.232769][-6.9961615 -7.7636375 -8.43862 -8.2418041 -7.061655 -4.9460869 -2.8495481 -1.2422771 -0.16645718 0.14092922 -0.55171251 -1.8410857 -3.2383623 -4.7163429 -5.8034992][-8.72238 -9.7224979 -10.514582 -10.533457 -9.7336464 -8.1186714 -6.342556 -4.6463823 -3.6035533 -3.5353761 -3.9400382 -4.5961657 -5.4302826 -6.3679271 -6.9022913][-9.905695 -11.294773 -12.14171 -12.018213 -11.543908 -10.407587 -9.0396786 -7.6927853 -6.8587351 -6.6481719 -6.7936506 -7.2672396 -7.6211619 -7.8666024 -7.7450695][-10.757534 -11.922556 -12.517909 -12.354382 -12.016568 -11.423979 -10.762362 -9.82066 -9.2079277 -9.1452847 -9.1992426 -9.2613773 -9.1288967 -8.859561 -8.1369324][-10.740559 -11.674157 -12.102354 -11.857041 -11.389164 -10.866045 -10.539951 -10.32657 -10.317736 -10.460497 -10.449106 -10.100825 -9.5633354 -8.9924908 -7.9010181]]...]
INFO - root - 2017-12-16 04:53:42.364019: step 15810, loss = 0.43, batch loss = 0.37 (28.4 examples/sec; 0.282 sec/batch; 24h:47m:56s remains)
INFO - root - 2017-12-16 04:53:45.213000: step 15820, loss = 0.26, batch loss = 0.20 (27.5 examples/sec; 0.291 sec/batch; 25h:33m:26s remains)
INFO - root - 2017-12-16 04:53:48.046623: step 15830, loss = 0.31, batch loss = 0.25 (27.6 examples/sec; 0.290 sec/batch; 25h:29m:21s remains)
INFO - root - 2017-12-16 04:53:50.936848: step 15840, loss = 0.32, batch loss = 0.26 (27.6 examples/sec; 0.290 sec/batch; 25h:30m:59s remains)
INFO - root - 2017-12-16 04:53:53.822242: step 15850, loss = 0.32, batch loss = 0.26 (27.7 examples/sec; 0.289 sec/batch; 25h:26m:05s remains)
INFO - root - 2017-12-16 04:53:56.647742: step 15860, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.284 sec/batch; 24h:59m:02s remains)
INFO - root - 2017-12-16 04:53:59.432442: step 15870, loss = 0.27, batch loss = 0.21 (29.2 examples/sec; 0.274 sec/batch; 24h:07m:48s remains)
INFO - root - 2017-12-16 04:54:02.228098: step 15880, loss = 0.32, batch loss = 0.26 (28.0 examples/sec; 0.286 sec/batch; 25h:06m:36s remains)
INFO - root - 2017-12-16 04:54:05.076577: step 15890, loss = 0.20, batch loss = 0.14 (28.8 examples/sec; 0.278 sec/batch; 24h:24m:25s remains)
INFO - root - 2017-12-16 04:54:07.943283: step 15900, loss = 0.27, batch loss = 0.21 (27.8 examples/sec; 0.287 sec/batch; 25h:16m:06s remains)
2017-12-16 04:54:08.407696: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2843246 -4.6838489 -5.0740905 -5.6069613 -6.0263085 -6.2603 -6.6578169 -7.3963 -8.1072435 -8.47966 -8.3767014 -7.679173 -6.5861044 -5.3969789 -4.3817773][-3.7118585 -4.2213945 -4.7339172 -5.1708493 -5.6224723 -6.1854682 -6.7760077 -7.4676914 -8.09244 -8.5161419 -8.5247164 -8.032238 -7.2372065 -6.1348071 -4.8842745][-3.3487778 -3.7543139 -3.9192991 -4.5066752 -5.1007957 -5.4552927 -5.9060111 -6.5853119 -7.373517 -7.6945057 -7.5910316 -7.3013535 -6.931468 -6.1538458 -4.9351759][-2.9212403 -3.4150262 -3.7482505 -3.7308168 -3.8556519 -4.2750053 -4.4805346 -4.6434774 -5.1702023 -5.6487217 -5.8827429 -5.8738427 -5.7361436 -5.3222857 -4.4177442][-3.0052142 -3.0849922 -2.9493775 -2.6989245 -2.5593591 -2.0907502 -1.7034914 -1.4966843 -1.8335369 -2.5135407 -3.2195377 -3.7171481 -3.8654728 -3.8773327 -3.4170547][-2.1616497 -2.1396635 -1.9885654 -1.5042419 -0.82868528 -0.0024561882 1.0353689 2.1288166 2.0944581 1.0849171 -0.38152075 -1.6664007 -2.30123 -2.4811561 -2.15576][-1.7656708 -1.258096 -0.9276979 -0.31416893 0.58505535 1.7095308 2.8792772 3.8245888 3.9237671 3.1941805 1.8217149 0.10940647 -1.0358927 -1.6445107 -1.6743917][-1.9541614 -1.4043367 -0.95659757 -0.22155809 0.97139645 2.3685384 3.7067013 4.4569473 4.2151442 3.3454757 2.1412387 0.66299582 -0.4406383 -1.3880656 -1.7710066][-2.1948173 -1.8203449 -1.4455891 -1.1352513 -0.20326567 1.301651 2.8275266 3.6042032 3.4459386 2.5841579 1.4385915 0.011107445 -1.2425914 -1.9721618 -2.0262299][-2.4279327 -2.5714226 -2.8790359 -2.7578514 -1.9280691 -0.87110567 0.4113183 1.329257 1.3919411 0.811821 -0.043858051 -1.2322078 -2.3059931 -3.044821 -3.3272429][-4.032536 -4.0736957 -3.9658222 -4.1657166 -4.1287565 -3.3508315 -2.2236016 -1.3891726 -0.98120427 -1.2388275 -1.9062195 -2.842628 -3.6872373 -4.335412 -4.5609465][-4.8635826 -5.2653608 -5.5685053 -5.4488225 -5.0217886 -4.7046285 -4.423923 -3.8792682 -3.2383094 -3.0748284 -3.5135379 -4.5019026 -5.3148456 -5.5249257 -5.4061756][-5.025259 -5.3460684 -5.7279997 -5.8136458 -5.6813359 -5.1342821 -4.586628 -4.4914975 -4.3761687 -4.2136941 -4.2865214 -4.9006753 -5.6660438 -6.0541391 -5.9252968][-4.5115108 -4.6863351 -4.9959078 -5.1993217 -5.2980022 -4.9484396 -4.5139055 -4.3397079 -4.2160711 -4.20126 -4.2790484 -4.6927719 -5.3125086 -5.532928 -5.4170671][-4.1110406 -4.0724063 -4.1529427 -4.224061 -4.217134 -4.1480584 -4.0676222 -3.8844981 -3.6733174 -3.8087678 -4.0470815 -4.1951404 -4.4130316 -4.593091 -4.5855312]]...]
INFO - root - 2017-12-16 04:54:11.284101: step 15910, loss = 0.25, batch loss = 0.19 (29.3 examples/sec; 0.273 sec/batch; 24h:01m:23s remains)
INFO - root - 2017-12-16 04:54:14.161563: step 15920, loss = 0.29, batch loss = 0.23 (29.0 examples/sec; 0.276 sec/batch; 24h:14m:48s remains)
INFO - root - 2017-12-16 04:54:16.998285: step 15930, loss = 0.32, batch loss = 0.26 (28.2 examples/sec; 0.284 sec/batch; 24h:57m:49s remains)
INFO - root - 2017-12-16 04:54:19.887727: step 15940, loss = 0.26, batch loss = 0.20 (27.0 examples/sec; 0.296 sec/batch; 26h:02m:55s remains)
INFO - root - 2017-12-16 04:54:22.706282: step 15950, loss = 0.29, batch loss = 0.24 (27.8 examples/sec; 0.288 sec/batch; 25h:18m:43s remains)
INFO - root - 2017-12-16 04:54:25.551544: step 15960, loss = 0.31, batch loss = 0.25 (28.6 examples/sec; 0.279 sec/batch; 24h:33m:55s remains)
INFO - root - 2017-12-16 04:54:28.324814: step 15970, loss = 0.29, batch loss = 0.23 (28.9 examples/sec; 0.277 sec/batch; 24h:20m:05s remains)
INFO - root - 2017-12-16 04:54:31.155704: step 15980, loss = 0.25, batch loss = 0.20 (29.4 examples/sec; 0.272 sec/batch; 23h:54m:19s remains)
INFO - root - 2017-12-16 04:54:34.007675: step 15990, loss = 0.37, batch loss = 0.31 (27.8 examples/sec; 0.288 sec/batch; 25h:19m:38s remains)
INFO - root - 2017-12-16 04:54:36.838125: step 16000, loss = 0.41, batch loss = 0.35 (28.7 examples/sec; 0.279 sec/batch; 24h:29m:21s remains)
2017-12-16 04:54:37.286336: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6756415 -5.0493855 -4.6665945 -4.567771 -4.866343 -5.0618997 -5.0775537 -5.171124 -5.2768688 -5.3888211 -5.3126831 -5.1714344 -4.8351965 -4.3267617 -3.8160019][-6.0287457 -5.4226513 -4.8620305 -4.5784307 -4.439075 -4.8249135 -5.0010619 -5.1318345 -5.3648477 -5.5663757 -5.7187171 -5.6705565 -5.4070888 -4.6825428 -3.8141856][-6.2664819 -5.5724316 -4.8903923 -4.4023976 -4.0146418 -4.0561938 -3.8247757 -4.1064944 -4.5758615 -5.1688643 -5.7535343 -5.9637394 -6.0411119 -5.4089465 -4.5213842][-6.1909833 -5.3242807 -4.420763 -3.5868642 -3.0701308 -2.7430789 -2.4282918 -2.6851571 -2.9974761 -3.859467 -4.7241392 -5.7519026 -6.3063092 -5.8170714 -4.9976373][-5.4278197 -4.3854418 -3.1760063 -2.1461585 -1.3314655 -0.83940554 -0.46999741 -0.52632976 -1.015204 -2.0579996 -3.1760077 -4.44554 -5.2650881 -5.4343109 -4.9429288][-4.80032 -3.5962009 -2.09343 -0.54640245 0.83508158 1.7033019 2.3284054 2.2180829 1.4907012 0.42768526 -1.1122258 -2.9073997 -4.405479 -5.0799494 -4.7556825][-4.3232403 -3.0254722 -1.3810306 0.42424679 1.9819422 3.2964249 4.4159336 4.6201324 4.1186895 2.7465477 0.86731339 -1.2732635 -3.2673249 -4.3656673 -4.5395837][-3.605247 -2.61618 -1.2700491 0.40312862 2.008625 3.4595394 4.6660547 5.0373793 4.8390274 3.7315741 1.9846878 -0.21135473 -2.4285042 -3.9136667 -4.4323568][-3.1526494 -2.5984278 -1.8139772 -0.64231014 0.71073341 2.060957 3.4542537 4.0684118 4.0580149 3.2045741 1.6052275 -0.30253935 -2.2249403 -3.63288 -4.2469869][-3.580193 -3.2140512 -2.9319272 -2.1827595 -1.1938055 0.043283939 1.33073 2.1006212 2.3226261 1.5828466 0.23486996 -1.4096684 -3.2796297 -4.4628448 -4.9480853][-3.8812947 -3.5563307 -3.4143729 -3.2896256 -2.9984107 -2.16347 -0.9794805 -0.26331472 0.0645504 -0.58229661 -1.839515 -3.2488406 -4.6299305 -5.485867 -5.8250775][-4.6081471 -4.3328357 -4.3645349 -4.5271382 -4.6293678 -4.3995566 -3.4868352 -2.7719195 -2.3741043 -2.5898449 -3.5369844 -4.733 -5.6803241 -6.0288115 -5.9440756][-5.683228 -5.1427469 -4.9698853 -5.2363477 -5.493783 -5.6860414 -5.1235867 -4.4405575 -4.1026182 -4.1079817 -4.6651011 -5.331183 -5.9265003 -5.983696 -5.6502409][-6.7324066 -5.7498455 -5.2057962 -5.0326948 -5.1149817 -5.336122 -5.1234279 -4.685534 -4.2599115 -4.2341866 -4.5177059 -4.9516253 -5.4182925 -5.2990894 -4.8756695][-7.2005005 -5.851253 -4.9249582 -4.6136141 -4.7009692 -4.8932371 -4.9922152 -4.8630872 -4.4509635 -4.32351 -4.33097 -4.37192 -4.4848337 -4.2736573 -3.8929744]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 04:54:40.167898: step 16010, loss = 0.30, batch loss = 0.24 (27.6 examples/sec; 0.290 sec/batch; 25h:27m:53s remains)
INFO - root - 2017-12-16 04:54:42.962372: step 16020, loss = 0.28, batch loss = 0.23 (27.7 examples/sec; 0.289 sec/batch; 25h:22m:42s remains)
INFO - root - 2017-12-16 04:54:45.805565: step 16030, loss = 0.44, batch loss = 0.38 (27.2 examples/sec; 0.295 sec/batch; 25h:53m:36s remains)
INFO - root - 2017-12-16 04:54:48.656066: step 16040, loss = 0.31, batch loss = 0.25 (28.1 examples/sec; 0.284 sec/batch; 24h:59m:31s remains)
INFO - root - 2017-12-16 04:54:51.464032: step 16050, loss = 0.50, batch loss = 0.44 (27.8 examples/sec; 0.288 sec/batch; 25h:18m:02s remains)
INFO - root - 2017-12-16 04:54:54.338813: step 16060, loss = 0.25, batch loss = 0.19 (29.3 examples/sec; 0.273 sec/batch; 23h:58m:08s remains)
INFO - root - 2017-12-16 04:54:57.175746: step 16070, loss = 0.24, batch loss = 0.18 (27.0 examples/sec; 0.296 sec/batch; 26h:01m:07s remains)
INFO - root - 2017-12-16 04:54:59.980870: step 16080, loss = 0.26, batch loss = 0.21 (27.9 examples/sec; 0.287 sec/batch; 25h:12m:23s remains)
INFO - root - 2017-12-16 04:55:02.824114: step 16090, loss = 0.26, batch loss = 0.20 (26.9 examples/sec; 0.298 sec/batch; 26h:10m:35s remains)
INFO - root - 2017-12-16 04:55:05.637103: step 16100, loss = 0.20, batch loss = 0.14 (28.0 examples/sec; 0.286 sec/batch; 25h:08m:03s remains)
2017-12-16 04:55:06.088383: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5142725 -2.5747304 -2.7086725 -2.9682994 -3.2617559 -3.3279626 -3.3669376 -3.3139524 -3.2378798 -3.1105213 -2.9050899 -2.9368351 -3.0655303 -3.3302822 -3.490994][-2.2197762 -2.4763277 -2.6754813 -2.9814761 -3.2770133 -3.4495111 -3.5356889 -3.4995379 -3.5499325 -3.5142314 -3.3652554 -3.2046518 -3.2444439 -3.209054 -3.0210423][-2.4850721 -2.7216957 -2.9791021 -3.2507143 -3.459424 -3.45621 -3.4598098 -3.573983 -3.7720678 -3.8832614 -3.921375 -3.707926 -3.522104 -3.5165291 -3.1518271][-3.0937681 -3.3984213 -3.5519912 -3.5638473 -3.2640414 -2.9636497 -2.7967358 -2.9043257 -3.1718569 -3.4480143 -3.6431093 -3.7337837 -3.8583612 -3.5551944 -3.2071614][-3.5952361 -3.848814 -3.7885671 -3.5687623 -3.0017865 -2.2590306 -1.7237985 -1.6559124 -2.0384281 -2.471792 -2.9998374 -3.180099 -3.4423556 -3.5998707 -3.5355315][-3.9225736 -4.023705 -3.8876655 -3.2481132 -2.1961477 -1.1808357 -0.36101198 -0.1430006 -0.49689317 -1.0528781 -1.7997825 -2.5396404 -3.1358089 -3.4117351 -3.4696949][-3.7790427 -3.9275646 -3.3957863 -2.4035325 -1.1860173 -0.039080143 1.0028486 1.3928533 0.97016382 0.29632854 -0.804652 -1.8204553 -2.7919936 -3.2745981 -3.4958062][-3.0199571 -3.063086 -2.4857972 -1.6767473 -0.34753609 0.89292574 1.896656 2.1561875 1.6647091 0.91050386 -0.41241074 -1.7245584 -2.8455009 -3.3814106 -3.5657697][-2.1248283 -2.2176688 -1.7624056 -1.1324606 -0.27453613 0.62317276 1.4831777 1.6149111 1.1788473 0.48058081 -0.80830526 -2.1475077 -3.2785516 -3.65519 -3.6665876][-1.7815514 -1.7359924 -1.4178617 -1.017019 -0.36030769 0.0075860023 0.24026585 -0.022390842 -0.4831326 -1.0950511 -2.0802357 -3.0538094 -3.8694816 -3.9728577 -3.644453][-1.5480311 -1.529983 -1.2709913 -1.0075395 -0.62545729 -0.40478945 -0.41957426 -1.0402036 -1.8060246 -2.52133 -3.2437854 -3.8028257 -4.3541374 -4.2669463 -3.7573147][-1.4933524 -1.1688519 -0.91611886 -0.93478107 -0.93517566 -1.139945 -1.5372434 -2.2830775 -3.0546947 -3.5516915 -3.8544145 -4.1651869 -4.4659114 -4.2664657 -3.6952221][-1.5812662 -1.0820732 -0.86723328 -0.87366509 -0.85031056 -1.3486595 -1.8289826 -2.6679144 -3.5588999 -4.1251488 -4.1086 -3.9185376 -3.9079947 -3.6854568 -3.2122242][-2.0329545 -1.2150469 -0.70112967 -0.65680933 -0.73875809 -1.0360298 -1.4537733 -2.2007713 -3.00704 -3.3793077 -3.4588327 -3.23091 -3.1495156 -2.9509511 -2.7038808][-2.0466311 -1.1204581 -0.58302164 -0.41839552 -0.57867169 -1.0973482 -1.5292542 -2.0243874 -2.5733914 -2.918591 -2.9712613 -2.814095 -2.7270126 -2.6932859 -2.5669279]]...]
INFO - root - 2017-12-16 04:55:08.960060: step 16110, loss = 0.26, batch loss = 0.20 (25.8 examples/sec; 0.310 sec/batch; 27h:12m:03s remains)
INFO - root - 2017-12-16 04:55:11.747793: step 16120, loss = 0.41, batch loss = 0.35 (28.3 examples/sec; 0.283 sec/batch; 24h:50m:57s remains)
INFO - root - 2017-12-16 04:55:14.563144: step 16130, loss = 0.26, batch loss = 0.20 (29.5 examples/sec; 0.271 sec/batch; 23h:47m:45s remains)
INFO - root - 2017-12-16 04:55:17.371089: step 16140, loss = 0.40, batch loss = 0.35 (28.6 examples/sec; 0.279 sec/batch; 24h:32m:27s remains)
INFO - root - 2017-12-16 04:55:20.167205: step 16150, loss = 0.28, batch loss = 0.22 (27.9 examples/sec; 0.286 sec/batch; 25h:09m:11s remains)
INFO - root - 2017-12-16 04:55:22.972131: step 16160, loss = 0.31, batch loss = 0.25 (27.0 examples/sec; 0.296 sec/batch; 26h:00m:30s remains)
INFO - root - 2017-12-16 04:55:25.801488: step 16170, loss = 0.45, batch loss = 0.39 (29.3 examples/sec; 0.273 sec/batch; 24h:00m:05s remains)
INFO - root - 2017-12-16 04:55:28.643019: step 16180, loss = 0.22, batch loss = 0.16 (28.0 examples/sec; 0.286 sec/batch; 25h:06m:11s remains)
INFO - root - 2017-12-16 04:55:31.486214: step 16190, loss = 0.40, batch loss = 0.34 (28.9 examples/sec; 0.277 sec/batch; 24h:20m:09s remains)
INFO - root - 2017-12-16 04:55:34.283884: step 16200, loss = 0.30, batch loss = 0.25 (28.5 examples/sec; 0.281 sec/batch; 24h:40m:34s remains)
2017-12-16 04:55:34.744024: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5121717 -4.3798537 -4.3435869 -4.4248214 -4.7587895 -4.8680491 -5.0218964 -5.3372164 -5.3406882 -5.0536 -4.6647968 -4.346344 -3.8756812 -3.4257483 -3.1849499][-4.2205229 -4.154222 -4.2348185 -4.2998614 -4.5715923 -4.9499407 -5.3275609 -5.6685371 -5.75424 -5.582716 -5.1815152 -4.6853962 -4.087347 -3.5691681 -3.2301674][-3.5515745 -3.5182686 -3.5141716 -3.7207701 -4.0908494 -4.435564 -4.9039779 -5.3351536 -5.5078435 -5.4499888 -5.2422881 -5.0487194 -4.6103992 -4.0796428 -3.568408][-2.4459472 -2.3575225 -2.3048513 -2.5256078 -2.8750505 -3.2047179 -3.562957 -3.8499072 -4.108418 -4.2863274 -4.4828091 -4.7261686 -4.6176438 -4.28038 -3.8104658][-1.3495147 -1.0630226 -0.75138116 -0.80928874 -0.92087007 -1.0026355 -1.2110128 -1.5854197 -2.1159084 -2.7247007 -3.4349351 -4.2529593 -4.5953069 -4.5425496 -4.0659003][-0.52739239 -0.11238766 0.37603235 0.79180288 1.0609775 1.0877929 1.1048708 0.91496515 0.15748882 -1.0448787 -2.3882122 -3.8140583 -4.5589571 -4.7018104 -4.3333011][-0.33202791 0.27356052 0.82625866 1.3396659 1.894352 2.2371554 2.4870491 2.4704084 1.8746619 0.56366968 -1.0576444 -2.7728662 -3.9124155 -4.3362083 -4.1851439][-0.81633496 -0.34551668 0.24294424 1.0060997 1.7722383 2.3737822 2.8332672 2.8850842 2.4706335 1.2792468 -0.35917234 -2.0992556 -3.2685995 -3.7758107 -3.8971393][-1.7676957 -1.273962 -0.78068328 -0.10232306 0.61779404 1.220531 1.822814 2.0470672 1.8249259 0.73308229 -0.65448308 -2.1914086 -3.2724171 -3.6980925 -3.8663216][-2.5963082 -2.3348012 -2.1362338 -1.6377935 -0.92489171 -0.36969948 0.07115221 0.24874115 0.21881533 -0.42340803 -1.4709041 -2.5896363 -3.4172893 -3.8212388 -3.9163268][-3.2062364 -3.2078733 -3.1978827 -3.0367818 -2.5887027 -2.1145349 -1.6593006 -1.3509343 -1.1140926 -1.261374 -2.0047293 -2.920444 -3.593003 -3.8170991 -3.9291668][-3.4678514 -3.604804 -3.8757937 -3.9555063 -3.763972 -3.320581 -2.7570934 -2.4080055 -2.073817 -2.0194454 -2.4532359 -3.1112342 -3.7548337 -3.8985186 -3.9097719][-2.9128597 -3.1332059 -3.5739107 -3.7649188 -3.660717 -3.4201159 -3.1241238 -2.9109521 -2.6046877 -2.5345764 -2.8949127 -3.412333 -3.8618779 -3.9161837 -3.8088479][-2.0288868 -2.1957555 -2.6675639 -3.0969505 -3.2234802 -3.0671167 -2.8809068 -2.7323723 -2.5423894 -2.5415325 -2.9308147 -3.4472227 -3.8302212 -3.892138 -3.718509][-1.3761129 -1.5079591 -2.0514047 -2.4671555 -2.6063085 -2.4860959 -2.1563466 -1.9724824 -1.9308913 -2.0860975 -2.5583088 -3.1747823 -3.6150556 -3.6866193 -3.4596772]]...]
INFO - root - 2017-12-16 04:55:37.548699: step 16210, loss = 0.31, batch loss = 0.25 (28.6 examples/sec; 0.279 sec/batch; 24h:32m:57s remains)
INFO - root - 2017-12-16 04:55:40.321748: step 16220, loss = 0.40, batch loss = 0.34 (28.1 examples/sec; 0.285 sec/batch; 25h:02m:45s remains)
INFO - root - 2017-12-16 04:55:43.146526: step 16230, loss = 0.36, batch loss = 0.31 (28.6 examples/sec; 0.280 sec/batch; 24h:34m:57s remains)
INFO - root - 2017-12-16 04:55:45.945682: step 16240, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.279 sec/batch; 24h:29m:46s remains)
INFO - root - 2017-12-16 04:55:48.784206: step 16250, loss = 0.27, batch loss = 0.22 (28.7 examples/sec; 0.279 sec/batch; 24h:28m:46s remains)
INFO - root - 2017-12-16 04:55:51.646781: step 16260, loss = 0.21, batch loss = 0.15 (27.8 examples/sec; 0.288 sec/batch; 25h:18m:46s remains)
INFO - root - 2017-12-16 04:55:54.451201: step 16270, loss = 0.40, batch loss = 0.34 (29.4 examples/sec; 0.272 sec/batch; 23h:53m:05s remains)
INFO - root - 2017-12-16 04:55:57.262376: step 16280, loss = 0.31, batch loss = 0.25 (28.3 examples/sec; 0.283 sec/batch; 24h:50m:52s remains)
INFO - root - 2017-12-16 04:56:00.057173: step 16290, loss = 0.29, batch loss = 0.24 (29.1 examples/sec; 0.275 sec/batch; 24h:10m:13s remains)
INFO - root - 2017-12-16 04:56:02.913499: step 16300, loss = 0.22, batch loss = 0.16 (26.2 examples/sec; 0.305 sec/batch; 26h:47m:58s remains)
2017-12-16 04:56:03.366662: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6314566 -3.3548579 -4.0061569 -4.3245425 -4.3231015 -4.105751 -3.8334568 -3.4863191 -3.2736607 -3.2716494 -3.1702418 -3.1256204 -3.0454817 -3.0535045 -3.0556788][-2.9267287 -3.7210641 -4.4432406 -4.6114979 -4.4150362 -3.9952745 -3.542784 -3.2149503 -3.0669765 -3.077457 -2.9901924 -2.8160214 -2.6157041 -2.6104422 -2.7553668][-3.1339078 -3.77777 -4.2991223 -4.4107108 -4.2253671 -3.762536 -3.1856418 -2.7763619 -2.6200154 -2.7099848 -2.6660061 -2.5605755 -2.537951 -2.4368515 -2.389236][-3.1078544 -3.6665843 -4.0843048 -3.9903095 -3.5083857 -3.0480161 -2.5910788 -2.1363635 -1.7440169 -1.7048459 -1.8471096 -1.9561896 -2.0239878 -2.230875 -2.4197564][-2.9306488 -3.3511035 -3.5385671 -3.2928317 -2.5927396 -1.8348987 -1.1888571 -0.8916924 -0.76172137 -0.65619016 -0.70998859 -0.91481018 -1.1622076 -1.4207268 -1.6571426][-2.5265861 -2.6160398 -2.4672308 -1.9973869 -1.1562977 -0.40868807 0.23668242 0.679122 0.902915 0.71023083 0.32620478 -0.00045633316 -0.34669065 -0.7358222 -1.0736587][-2.0275931 -2.0269947 -1.785758 -1.1658192 -0.24225903 0.63012505 1.3569255 1.6791663 1.7704186 1.6418114 1.2445598 0.63888645 -0.014891148 -0.63347912 -1.1348991][-2.2613568 -1.9194944 -1.4670584 -0.7593863 0.18330431 1.097569 1.7832136 2.0407958 2.1726952 2.0307908 1.4750576 0.664536 -0.141891 -0.89891648 -1.5301485][-2.7793264 -2.26201 -1.6754646 -0.87942648 0.038050175 0.83202314 1.4894695 1.7445498 1.8191538 1.5697517 1.072948 0.42503929 -0.35210609 -1.1093912 -1.8338447][-3.4916334 -2.9083886 -2.1987174 -1.4569948 -0.79338789 -0.22046804 0.28281927 0.50157976 0.74974394 0.71506071 0.38052702 0.012803555 -0.49801016 -1.1283939 -1.9058869][-3.8549178 -3.4266126 -2.9444394 -2.1705358 -1.3145876 -0.723382 -0.441005 -0.58974171 -0.63025808 -0.41413498 -0.37502861 -0.52760863 -0.90497684 -1.4565082 -2.0474887][-4.7537227 -4.2197013 -3.3119369 -2.4714859 -1.8169899 -1.297374 -1.1422427 -1.308471 -1.5548604 -1.5955081 -1.5291286 -1.3686585 -1.296284 -1.6129141 -2.0806873][-5.4311266 -5.0906181 -4.1506944 -2.7625155 -1.584321 -1.139132 -1.3460841 -1.6919167 -2.0753696 -2.0805941 -1.9375098 -1.7601428 -1.4932497 -1.7207937 -2.1781778][-5.8408518 -5.3988137 -4.1938963 -2.8732352 -1.6168282 -0.72375751 -0.59587121 -1.1756637 -1.9658835 -2.0079677 -1.790529 -1.5433283 -1.2677295 -1.5428004 -2.2173913][-5.9721737 -5.3801975 -4.0318027 -2.5811 -1.1928756 -0.28433609 -0.24134207 -0.61886048 -1.3574486 -1.7756844 -1.8547049 -1.4668231 -1.0178065 -1.2892056 -1.9749308]]...]
INFO - root - 2017-12-16 04:56:06.188266: step 16310, loss = 0.28, batch loss = 0.22 (28.7 examples/sec; 0.279 sec/batch; 24h:31m:29s remains)
INFO - root - 2017-12-16 04:56:09.051300: step 16320, loss = 0.26, batch loss = 0.20 (26.7 examples/sec; 0.299 sec/batch; 26h:16m:44s remains)
INFO - root - 2017-12-16 04:56:11.925649: step 16330, loss = 0.19, batch loss = 0.13 (28.3 examples/sec; 0.283 sec/batch; 24h:51m:44s remains)
INFO - root - 2017-12-16 04:56:14.760651: step 16340, loss = 0.31, batch loss = 0.26 (28.4 examples/sec; 0.282 sec/batch; 24h:45m:51s remains)
INFO - root - 2017-12-16 04:56:17.594551: step 16350, loss = 0.24, batch loss = 0.18 (27.4 examples/sec; 0.292 sec/batch; 25h:39m:55s remains)
INFO - root - 2017-12-16 04:56:20.396249: step 16360, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.286 sec/batch; 25h:07m:18s remains)
INFO - root - 2017-12-16 04:56:23.226961: step 16370, loss = 0.20, batch loss = 0.14 (28.9 examples/sec; 0.277 sec/batch; 24h:20m:19s remains)
INFO - root - 2017-12-16 04:56:26.039602: step 16380, loss = 0.25, batch loss = 0.19 (29.3 examples/sec; 0.273 sec/batch; 23h:59m:42s remains)
INFO - root - 2017-12-16 04:56:28.851873: step 16390, loss = 0.40, batch loss = 0.34 (30.1 examples/sec; 0.266 sec/batch; 23h:22m:25s remains)
INFO - root - 2017-12-16 04:56:31.682855: step 16400, loss = 0.33, batch loss = 0.27 (25.7 examples/sec; 0.312 sec/batch; 27h:21m:38s remains)
2017-12-16 04:56:32.139333: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1377292 -3.3943245 -3.7326121 -4.09371 -4.288703 -4.2141838 -3.9038305 -3.5674236 -3.2484941 -2.9243226 -2.716543 -2.9050429 -3.179749 -3.1473498 -2.9914019][-2.5090222 -2.7665052 -3.0819163 -3.4784546 -3.6474156 -3.541096 -3.3400664 -3.1109998 -2.8682313 -2.5623481 -2.3916142 -2.4093738 -2.5316756 -2.5253832 -2.3407061][-1.6900666 -2.0150054 -2.4062097 -2.7677321 -3.0014267 -2.9796329 -2.7641811 -2.533603 -2.3863168 -2.2568231 -2.1279595 -2.0044274 -2.0619206 -2.0079856 -1.8885827][-0.939219 -1.2385964 -1.5311685 -1.8407531 -1.9707053 -1.9561477 -1.7972751 -1.6299222 -1.4978299 -1.4083006 -1.4538717 -1.4509056 -1.4907222 -1.4914367 -1.5057688][-0.00824213 -0.33614016 -0.59921265 -0.7318449 -0.63797522 -0.53999233 -0.51599622 -0.46460319 -0.46004033 -0.5703733 -0.81255531 -0.90013123 -0.96112561 -1.0206752 -1.1809494][-0.16835165 -0.33063889 -0.35704374 -0.370759 -0.17723465 0.25225782 0.55239725 0.6190505 0.52069092 0.31491947 -0.08765173 -0.32095861 -0.47106123 -0.64108109 -0.90802836][-0.68321705 -0.9121356 -0.92198086 -0.70789623 -0.20622635 0.30729866 0.69638062 0.96023273 0.92943859 0.71840143 0.27718163 -0.062515736 -0.26665354 -0.41583347 -0.65330338][-1.4981804 -1.8963168 -1.9983137 -1.7377532 -1.0786529 -0.44519877 0.041535854 0.5314703 0.8458581 0.85937881 0.47053385 0.14716959 -0.07519722 -0.13906813 -0.26821327][-2.1980932 -2.7484622 -2.9867229 -2.8722463 -2.3496583 -1.5232327 -0.82203531 -0.4265995 -0.21143532 0.0033621788 -0.044354916 -0.16276169 -0.36286354 -0.3182559 -0.25987148][-3.086307 -3.7580147 -4.0579185 -3.9905145 -3.5595798 -2.9615316 -2.3558607 -1.6068556 -1.0717826 -0.8808465 -0.8312788 -0.91165805 -1.1336281 -1.0097935 -0.70308089][-3.519161 -4.2359242 -4.7149334 -4.800035 -4.5229249 -4.0371962 -3.4714384 -2.9082642 -2.444808 -2.1719375 -1.8943288 -1.7300999 -1.6776435 -1.4397483 -1.2131295][-4.0112214 -4.4498439 -4.7130394 -4.96897 -5.0404758 -4.6326604 -4.1502266 -3.8780344 -3.5410047 -3.2828057 -3.0450408 -2.7763698 -2.4854274 -1.9313493 -1.4718263][-4.6376085 -4.7477679 -4.6963415 -4.632288 -4.5676565 -4.3867755 -4.105268 -3.9678214 -3.8820739 -3.9281194 -3.7615142 -3.4811363 -2.8905044 -2.1239963 -1.5490067][-4.968276 -4.6664586 -4.2772551 -4.0097342 -3.9891038 -3.7786145 -3.6520295 -3.7066908 -3.7858977 -3.9155726 -3.823432 -3.586086 -2.8504522 -1.9486561 -1.3543518][-4.8980446 -4.3492379 -3.7958117 -3.3769307 -3.1995606 -3.1250958 -3.15348 -3.3808627 -3.6704066 -3.9833643 -3.9108477 -3.3784881 -2.3826251 -1.2627957 -0.65061188]]...]
INFO - root - 2017-12-16 04:56:34.974961: step 16410, loss = 0.26, batch loss = 0.20 (28.5 examples/sec; 0.280 sec/batch; 24h:36m:47s remains)
INFO - root - 2017-12-16 04:56:37.849987: step 16420, loss = 0.32, batch loss = 0.26 (27.8 examples/sec; 0.287 sec/batch; 25h:14m:05s remains)
INFO - root - 2017-12-16 04:56:40.681493: step 16430, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.285 sec/batch; 25h:00m:23s remains)
INFO - root - 2017-12-16 04:56:43.506174: step 16440, loss = 0.24, batch loss = 0.18 (28.7 examples/sec; 0.279 sec/batch; 24h:29m:40s remains)
INFO - root - 2017-12-16 04:56:46.343999: step 16450, loss = 0.31, batch loss = 0.26 (29.3 examples/sec; 0.273 sec/batch; 23h:56m:35s remains)
INFO - root - 2017-12-16 04:56:49.212424: step 16460, loss = 0.27, batch loss = 0.21 (29.6 examples/sec; 0.270 sec/batch; 23h:43m:40s remains)
INFO - root - 2017-12-16 04:56:51.995103: step 16470, loss = 0.28, batch loss = 0.23 (28.5 examples/sec; 0.281 sec/batch; 24h:40m:14s remains)
INFO - root - 2017-12-16 04:56:54.835965: step 16480, loss = 0.28, batch loss = 0.22 (27.7 examples/sec; 0.289 sec/batch; 25h:21m:41s remains)
INFO - root - 2017-12-16 04:56:57.672838: step 16490, loss = 0.22, batch loss = 0.16 (29.0 examples/sec; 0.276 sec/batch; 24h:11m:50s remains)
INFO - root - 2017-12-16 04:57:00.553114: step 16500, loss = 0.21, batch loss = 0.15 (27.7 examples/sec; 0.288 sec/batch; 25h:18m:46s remains)
2017-12-16 04:57:01.019388: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.56926 -5.5779715 -5.4849272 -5.4702854 -5.5283628 -5.5520129 -5.5639539 -5.64819 -5.7375822 -5.6845188 -5.6165438 -5.520968 -5.3385139 -5.0764995 -4.7167439][-5.6023874 -5.45551 -5.3481889 -5.4689922 -5.5938635 -5.7963867 -5.979589 -5.9748979 -5.962533 -6.095643 -6.1267309 -5.9417276 -5.7244153 -5.5004683 -5.1848688][-4.8721046 -4.7972622 -4.7813716 -4.7811923 -4.9634104 -5.2587304 -5.2975564 -5.3855896 -5.636982 -5.6829824 -5.7991791 -6.0495882 -6.1333227 -5.7599425 -5.2173038][-3.9979482 -3.5275862 -3.2853339 -3.4560661 -3.71047 -3.8860145 -3.8381429 -3.8941231 -4.0665989 -4.365963 -4.7017512 -5.1566448 -5.4558096 -5.3075037 -4.8811522][-2.4814446 -1.6820316 -1.3135407 -1.5224783 -1.8099325 -1.8639433 -1.6242421 -1.65693 -1.9742346 -2.5793552 -3.2628043 -3.9467545 -4.3554783 -4.288372 -3.8371611][-1.4395676 -0.45557809 0.078474045 0.13554668 0.10807133 0.20512486 0.60767937 0.84402037 0.54981184 -0.26000595 -1.1985593 -2.3156569 -3.0246506 -3.0462074 -2.7131133][-0.66312122 0.23344707 0.6583128 0.74063158 1.0369987 1.7317061 2.5965075 2.8860683 2.4498549 1.586638 0.56697321 -0.61744595 -1.4820256 -1.8776662 -2.0350916][-0.22253323 0.48715925 0.85458803 1.0445294 1.268868 1.9319444 2.9141998 3.6421928 3.507381 2.5565429 1.296525 0.065001011 -0.7155571 -1.0405605 -1.3213191][-0.29343557 0.11946583 0.28127718 0.40149689 0.74972916 1.495831 2.3059602 2.8530974 2.804708 2.2692628 1.2614861 0.14815617 -0.59090495 -0.87173963 -1.0750456][-0.56994605 -0.46778727 -0.56383586 -0.48548007 -0.18671322 0.41532612 1.0684543 1.5221424 1.3674145 0.8628068 0.12627935 -0.56821609 -1.0353918 -1.1936316 -1.346638][-1.2741852 -1.1452508 -1.0684128 -1.0813577 -0.92048979 -0.61071515 -0.27647877 -0.10552549 -0.25152111 -0.64838958 -1.1711328 -1.527333 -1.7298973 -1.7571628 -1.7850065][-1.6721961 -1.4086792 -1.3608816 -1.4590588 -1.379992 -1.2556717 -1.2257395 -1.3431859 -1.6599176 -2.0808742 -2.4158754 -2.489995 -2.4076173 -2.1476905 -1.9485679][-1.9934084 -1.6345241 -1.5100873 -1.562537 -1.5956099 -1.6483314 -1.8651307 -2.209538 -2.7928174 -3.2980456 -3.4949865 -3.2694366 -2.9934328 -2.5235238 -2.1115353][-2.0670118 -1.60903 -1.3971183 -1.3906915 -1.4657502 -1.6003301 -1.7849748 -2.3094189 -3.10148 -3.8221257 -4.1046729 -3.8953152 -3.4058328 -2.840791 -2.4112458][-2.1320465 -1.6364877 -1.3843088 -1.3663504 -1.3471947 -1.4375076 -1.593076 -2.1631522 -2.9328911 -3.6253774 -4.0091748 -4.1124029 -3.9134259 -3.336875 -2.88236]]...]
INFO - root - 2017-12-16 04:57:03.829062: step 16510, loss = 0.22, batch loss = 0.16 (25.5 examples/sec; 0.314 sec/batch; 27h:32m:31s remains)
INFO - root - 2017-12-16 04:57:06.645445: step 16520, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.283 sec/batch; 24h:52m:57s remains)
INFO - root - 2017-12-16 04:57:09.546073: step 16530, loss = 0.24, batch loss = 0.18 (28.7 examples/sec; 0.279 sec/batch; 24h:29m:18s remains)
INFO - root - 2017-12-16 04:57:12.435846: step 16540, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.285 sec/batch; 25h:03m:17s remains)
INFO - root - 2017-12-16 04:57:15.262117: step 16550, loss = 0.32, batch loss = 0.26 (28.9 examples/sec; 0.277 sec/batch; 24h:17m:59s remains)
INFO - root - 2017-12-16 04:57:18.124600: step 16560, loss = 0.21, batch loss = 0.15 (26.9 examples/sec; 0.297 sec/batch; 26h:04m:30s remains)
INFO - root - 2017-12-16 04:57:20.998706: step 16570, loss = 0.26, batch loss = 0.20 (27.3 examples/sec; 0.293 sec/batch; 25h:42m:50s remains)
INFO - root - 2017-12-16 04:57:23.829467: step 16580, loss = 0.23, batch loss = 0.17 (27.3 examples/sec; 0.293 sec/batch; 25h:40m:26s remains)
INFO - root - 2017-12-16 04:57:26.702820: step 16590, loss = 0.25, batch loss = 0.19 (28.8 examples/sec; 0.278 sec/batch; 24h:24m:14s remains)
INFO - root - 2017-12-16 04:57:29.487152: step 16600, loss = 0.22, batch loss = 0.16 (28.3 examples/sec; 0.283 sec/batch; 24h:48m:25s remains)
2017-12-16 04:57:29.949732: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6719527 -2.933167 -3.2038693 -3.2495742 -3.1336913 -2.9642172 -3.0025403 -3.3239303 -3.6083083 -3.5879858 -3.4222565 -3.4231052 -3.3766508 -3.5077183 -3.5830386][-2.8868141 -2.9633369 -3.1714277 -3.2515349 -3.0140963 -2.9767923 -3.1105032 -3.200757 -3.5085304 -3.845926 -3.9936154 -3.9723127 -3.9490793 -4.150773 -4.3134756][-3.1029251 -3.2813618 -3.4350531 -3.518594 -3.2837906 -2.9851165 -2.9772859 -3.1231732 -3.3764653 -3.5385125 -3.8060331 -4.1686139 -4.3634572 -4.6796551 -4.8477573][-3.3078096 -3.4234669 -3.5999887 -3.518667 -3.1150665 -2.6362944 -2.2629819 -2.0074625 -2.1707873 -2.5991302 -3.0476117 -3.5377026 -3.9792137 -4.5949736 -4.7919712][-3.3431985 -3.5141897 -3.4492517 -3.1444991 -2.4984965 -1.7905612 -1.270442 -0.83582115 -0.72621942 -1.0418391 -1.6136503 -2.3543992 -3.0640423 -3.779304 -4.2472553][-3.618223 -3.6798048 -3.4414651 -2.7257795 -1.6991725 -0.72717071 0.021314144 0.58512545 0.758348 0.48627853 -0.048823357 -0.81749344 -1.6776197 -2.6913846 -3.4343619][-3.7234 -3.7028694 -3.3423476 -2.3628216 -0.96869159 0.14810228 1.1232009 1.86693 2.2888856 2.1165891 1.6162968 0.83847809 -0.11064959 -1.2714384 -2.33539][-3.2018847 -3.2892373 -3.0310667 -2.1489418 -0.78748965 0.41390324 1.4317894 2.2830029 2.9170332 2.9891667 2.7700291 2.0402002 1.134377 -0.15772915 -1.5203211][-2.9463911 -2.9482565 -2.7389548 -2.0892913 -1.0593932 0.020053387 0.85513353 1.7356005 2.5833879 2.9454064 3.1062379 2.6114798 1.7596717 0.40427685 -1.0945244][-2.804883 -2.854 -2.7931371 -2.4842548 -1.8254752 -1.0629656 -0.28131962 0.52777195 1.322741 1.7470059 2.1028008 1.8248515 1.0740047 -0.26146317 -1.6101434][-2.7256918 -2.8301144 -2.9626369 -2.903029 -2.6699038 -2.1959155 -1.5442019 -0.85748982 -0.053119659 0.35678196 0.78893757 0.63834906 0.057313442 -1.0873752 -2.2677338][-2.72937 -2.8649511 -3.1218011 -3.3477659 -3.3791108 -3.0403011 -2.5437651 -2.0835001 -1.487577 -1.203742 -0.8233161 -0.66776061 -1.1542854 -1.9646666 -2.8606446][-2.5861709 -2.6668587 -2.8120489 -2.8337126 -2.913868 -2.8348627 -2.5308423 -2.2651639 -1.9375458 -1.8906796 -1.721776 -1.5012517 -1.6991036 -2.2742586 -2.8708875][-2.5668631 -2.4596641 -2.2698019 -2.1758358 -2.03863 -1.9342105 -1.8341801 -1.9142175 -1.9330335 -2.091115 -2.0953035 -1.8440964 -1.7857327 -1.9649646 -2.4199162][-2.5597982 -2.072063 -1.4823203 -1.0665677 -0.73390913 -0.6431365 -0.73323154 -1.1769698 -1.6216819 -1.9841833 -2.191632 -1.9031148 -1.7338595 -1.6888118 -1.9892492]]...]
INFO - root - 2017-12-16 04:57:32.794243: step 16610, loss = 0.34, batch loss = 0.28 (29.2 examples/sec; 0.274 sec/batch; 24h:03m:18s remains)
INFO - root - 2017-12-16 04:57:35.655670: step 16620, loss = 0.30, batch loss = 0.24 (26.8 examples/sec; 0.298 sec/batch; 26h:09m:56s remains)
INFO - root - 2017-12-16 04:57:38.503794: step 16630, loss = 0.43, batch loss = 0.37 (28.2 examples/sec; 0.283 sec/batch; 24h:52m:03s remains)
INFO - root - 2017-12-16 04:57:41.304763: step 16640, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.277 sec/batch; 24h:18m:47s remains)
INFO - root - 2017-12-16 04:57:44.160650: step 16650, loss = 0.32, batch loss = 0.26 (26.2 examples/sec; 0.306 sec/batch; 26h:49m:03s remains)
INFO - root - 2017-12-16 04:57:47.007655: step 16660, loss = 0.23, batch loss = 0.17 (28.1 examples/sec; 0.284 sec/batch; 24h:57m:03s remains)
INFO - root - 2017-12-16 04:57:49.771061: step 16670, loss = 0.42, batch loss = 0.36 (28.9 examples/sec; 0.276 sec/batch; 24h:14m:53s remains)
INFO - root - 2017-12-16 04:57:52.647306: step 16680, loss = 0.28, batch loss = 0.23 (26.3 examples/sec; 0.305 sec/batch; 26h:42m:55s remains)
INFO - root - 2017-12-16 04:57:55.556165: step 16690, loss = 0.28, batch loss = 0.22 (26.7 examples/sec; 0.300 sec/batch; 26h:16m:48s remains)
INFO - root - 2017-12-16 04:57:58.418341: step 16700, loss = 0.27, batch loss = 0.22 (28.0 examples/sec; 0.286 sec/batch; 25h:03m:53s remains)
2017-12-16 04:57:58.876641: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8499236 -4.2198586 -4.6844072 -5.1293097 -5.3041606 -5.4012895 -5.4875512 -5.66959 -5.9067 -5.86851 -5.6723003 -5.222342 -4.7287607 -4.2299981 -3.8251958][-3.3445075 -3.9714639 -4.7646675 -5.1705284 -5.256846 -5.5495381 -5.7556152 -5.8259382 -5.8318677 -5.9144626 -5.937809 -5.4750519 -4.8388906 -3.93139 -3.4297967][-2.6643131 -3.6106186 -4.5551414 -4.9981556 -5.0438366 -5.0549083 -4.995923 -5.4920907 -5.7175789 -5.6897874 -5.5255675 -5.2510805 -4.7200389 -3.8155239 -3.1323962][-2.1141341 -3.271462 -4.261189 -4.4842734 -4.2526627 -3.977391 -3.7306514 -3.7923377 -3.9864511 -4.4637375 -4.6673284 -4.5351443 -4.165761 -3.3435924 -2.7510166][-2.1898997 -3.0961969 -3.7600381 -3.7991652 -2.92401 -1.8893926 -1.3078308 -1.293591 -1.7010732 -2.317945 -2.8383214 -3.1242015 -3.0254722 -2.6708612 -2.6580074][-2.4737554 -3.2552881 -3.2894657 -2.3555574 -1.0063276 0.41883993 1.5907059 1.8828073 1.231986 0.082857609 -1.0657339 -1.9001224 -2.5395212 -2.26701 -2.1646755][-3.1747508 -3.4622886 -3.0707297 -1.7993004 0.31162071 2.5159278 4.0073013 4.2440348 3.3868332 1.845654 0.27370262 -0.90441585 -1.8737557 -2.499193 -3.1183393][-3.7662487 -3.9351962 -3.528193 -1.7607672 0.72719717 2.9784842 4.81944 5.281558 4.3540297 2.6929679 0.89473867 -0.48289609 -1.6719508 -2.617089 -3.3981447][-4.9022322 -4.8030925 -4.0933819 -2.9196591 -0.78490186 1.8723559 3.91675 4.1684828 3.3822908 1.9283848 0.3042202 -1.0569804 -2.2250957 -2.880152 -3.3854351][-5.4720654 -6.3590016 -6.2030153 -4.8673091 -2.9965808 -1.3865519 0.13064241 1.2978582 1.1490273 -0.36195564 -1.8814969 -2.9743366 -3.6466281 -3.931083 -4.0660911][-6.4257245 -7.58101 -7.6982546 -7.203629 -5.9910851 -4.5055556 -3.289973 -3.0098691 -3.1240859 -3.3382378 -3.9269207 -4.6151266 -4.8885565 -4.8371382 -4.6274652][-7.2842417 -8.5417461 -9.4740925 -9.209343 -7.9488764 -7.0905437 -6.4898491 -5.8589487 -5.4174366 -5.4302835 -5.5772424 -5.650629 -5.5401697 -5.0354118 -4.42607][-7.4403086 -8.7423038 -9.5550213 -9.8092966 -9.5454712 -8.4925261 -7.458024 -7.2462692 -7.1127195 -6.7435837 -6.4837065 -6.1232347 -5.6431322 -5.159337 -4.8160009][-6.5203857 -7.9074211 -8.7958755 -8.8961725 -8.6057529 -8.1141052 -7.7743607 -7.37377 -6.8238616 -6.4167619 -5.9510703 -5.6737409 -5.605022 -5.0862131 -4.5768919][-5.4178171 -6.4820542 -7.1844692 -7.341291 -7.1809983 -6.8531208 -6.4468384 -6.1347346 -5.7695041 -5.2643023 -4.8175859 -4.5458717 -4.3866987 -4.3605161 -4.3782954]]...]
INFO - root - 2017-12-16 04:58:01.693827: step 16710, loss = 0.29, batch loss = 0.23 (28.1 examples/sec; 0.285 sec/batch; 24h:58m:21s remains)
INFO - root - 2017-12-16 04:58:04.549266: step 16720, loss = 0.31, batch loss = 0.25 (27.1 examples/sec; 0.295 sec/batch; 25h:54m:32s remains)
INFO - root - 2017-12-16 04:58:07.403645: step 16730, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.281 sec/batch; 24h:36m:53s remains)
INFO - root - 2017-12-16 04:58:10.242241: step 16740, loss = 0.30, batch loss = 0.24 (28.2 examples/sec; 0.284 sec/batch; 24h:52m:44s remains)
INFO - root - 2017-12-16 04:58:13.098135: step 16750, loss = 0.24, batch loss = 0.18 (27.8 examples/sec; 0.287 sec/batch; 25h:11m:48s remains)
INFO - root - 2017-12-16 04:58:15.923289: step 16760, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.285 sec/batch; 25h:01m:44s remains)
INFO - root - 2017-12-16 04:58:18.790736: step 16770, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 25h:04m:11s remains)
INFO - root - 2017-12-16 04:58:21.671915: step 16780, loss = 0.29, batch loss = 0.23 (28.7 examples/sec; 0.279 sec/batch; 24h:28m:27s remains)
INFO - root - 2017-12-16 04:58:24.532004: step 16790, loss = 0.21, batch loss = 0.15 (29.1 examples/sec; 0.274 sec/batch; 24h:04m:19s remains)
INFO - root - 2017-12-16 04:58:27.373120: step 16800, loss = 0.23, batch loss = 0.17 (27.6 examples/sec; 0.290 sec/batch; 25h:26m:02s remains)
2017-12-16 04:58:27.844070: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7848749 -3.3529444 -3.4752538 -3.6390851 -3.4458976 -3.4525633 -3.652828 -3.6597707 -3.7429824 -3.7732537 -3.6704347 -3.5506167 -3.4892168 -3.4273651 -3.2374115][-2.0801687 -2.5299506 -2.6595552 -2.9366529 -2.9086208 -2.9895175 -3.224545 -3.35424 -3.5724764 -3.6158271 -3.5217557 -3.3846269 -3.2161677 -2.9931054 -2.5935223][-1.2240605 -1.7131324 -1.9361668 -2.2790072 -2.386251 -2.4712162 -2.690908 -2.8854918 -3.3061528 -3.6009371 -3.619154 -3.486511 -3.243989 -2.6691298 -2.1069579][-0.4848361 -0.82414675 -1.1242218 -1.5545762 -1.7238927 -1.7843096 -1.9582069 -2.3208916 -2.900034 -3.3634105 -3.5894914 -3.46638 -3.1097102 -2.4439516 -1.7490697][-0.16766214 -0.30574131 -0.46343374 -0.83526444 -1.0101004 -1.0720344 -1.3124785 -1.7674227 -2.4806528 -3.0037642 -3.1821532 -3.1021957 -2.7439296 -2.0472085 -1.4289327][-0.16664648 -0.22184467 -0.45893669 -0.71613145 -0.7082994 -0.75616717 -0.99552393 -1.5635617 -2.3091094 -2.8197722 -2.9907293 -2.7893074 -2.3032691 -1.5491731 -0.870512][-0.652961 -0.61617231 -0.77772593 -0.94174695 -0.90039277 -0.81151438 -0.9184885 -1.3868284 -2.045831 -2.4102972 -2.4187465 -2.2094166 -1.8182003 -1.1460671 -0.48771548][-1.4261346 -1.4589114 -1.6273248 -1.6736887 -1.5562439 -1.264118 -1.1457434 -1.3857279 -1.7396533 -1.8473411 -1.6141927 -1.4479811 -1.1519544 -0.70781922 -0.26913834][-2.4013565 -2.3337834 -2.4310155 -2.4708486 -2.2892816 -1.8735983 -1.5088606 -1.3178782 -1.2995384 -1.1849773 -0.73368192 -0.52691054 -0.36760187 -0.21131802 0.068295956][-3.2913902 -3.463764 -3.5128675 -3.3140192 -3.0536463 -2.645524 -2.027097 -1.4999423 -1.2020173 -0.93317556 -0.48761487 -0.18548489 -0.03151083 -0.16994572 -0.14397287][-4.496572 -4.8614039 -4.805409 -4.5611811 -4.1636968 -3.5643258 -2.8232102 -2.0840511 -1.577884 -1.1624422 -0.80616212 -0.62778616 -0.51152897 -0.758029 -0.83931231][-5.6266541 -6.1639047 -6.1312213 -5.6791487 -5.0324297 -4.4136324 -3.7217817 -2.84267 -2.2909498 -2.0456066 -1.879688 -1.8158684 -1.8371184 -2.1435349 -2.2079098][-6.6594181 -7.28504 -7.3125181 -6.848175 -6.0847573 -5.4245834 -4.7699623 -4.0475912 -3.5728068 -3.3045216 -3.2016575 -3.3597996 -3.4871995 -3.7277124 -3.7338328][-7.4395218 -8.0592728 -8.1175137 -7.75671 -7.0540142 -6.4380703 -5.9346228 -5.3064909 -4.9384284 -4.7820554 -4.7894611 -4.9943852 -5.0976129 -5.3005419 -5.1971459][-7.8344574 -8.3632841 -8.4931 -8.1659451 -7.560266 -7.1222982 -6.7732124 -6.4037647 -6.2546539 -6.1948786 -6.253418 -6.4992123 -6.6280804 -6.6387992 -6.3350778]]...]
INFO - root - 2017-12-16 04:58:30.688759: step 16810, loss = 0.26, batch loss = 0.20 (27.6 examples/sec; 0.290 sec/batch; 25h:25m:42s remains)
INFO - root - 2017-12-16 04:58:33.530330: step 16820, loss = 0.27, batch loss = 0.21 (27.3 examples/sec; 0.293 sec/batch; 25h:42m:40s remains)
INFO - root - 2017-12-16 04:58:36.322048: step 16830, loss = 0.27, batch loss = 0.21 (29.2 examples/sec; 0.274 sec/batch; 24h:00m:08s remains)
INFO - root - 2017-12-16 04:58:39.144523: step 16840, loss = 0.33, batch loss = 0.27 (28.0 examples/sec; 0.286 sec/batch; 25h:04m:02s remains)
INFO - root - 2017-12-16 04:58:41.968689: step 16850, loss = 0.25, batch loss = 0.19 (28.7 examples/sec; 0.279 sec/batch; 24h:25m:22s remains)
INFO - root - 2017-12-16 04:58:44.786058: step 16860, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.280 sec/batch; 24h:30m:22s remains)
INFO - root - 2017-12-16 04:58:47.607944: step 16870, loss = 0.38, batch loss = 0.32 (28.2 examples/sec; 0.284 sec/batch; 24h:52m:39s remains)
INFO - root - 2017-12-16 04:58:50.451262: step 16880, loss = 0.26, batch loss = 0.20 (29.2 examples/sec; 0.274 sec/batch; 24h:00m:00s remains)
INFO - root - 2017-12-16 04:58:53.284018: step 16890, loss = 0.23, batch loss = 0.17 (26.2 examples/sec; 0.305 sec/batch; 26h:46m:29s remains)
INFO - root - 2017-12-16 04:58:56.091561: step 16900, loss = 0.21, batch loss = 0.15 (28.5 examples/sec; 0.281 sec/batch; 24h:37m:09s remains)
2017-12-16 04:58:56.539892: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8639121 -5.0757294 -3.9598308 -2.960247 -2.1387498 -1.9052038 -1.8090949 -2.0976629 -2.9212394 -3.6683483 -4.1490006 -4.4218841 -4.3813071 -3.9694428 -3.3594851][-5.1518879 -4.4096775 -3.4269404 -2.4791052 -1.7335281 -1.4067125 -1.3782866 -1.7788157 -2.5000548 -3.3041582 -3.9379749 -4.1896777 -4.1708961 -3.9035983 -3.3726859][-4.2332997 -3.5349374 -2.7293 -1.9097009 -1.3392909 -1.1205862 -1.1131988 -1.507925 -2.3170938 -3.3164062 -4.085011 -4.6052113 -4.6167521 -4.2718921 -3.6973939][-3.0547404 -2.5583758 -1.8527467 -1.3368168 -0.90699673 -0.71130419 -0.71225262 -1.1117682 -2.0488825 -3.1122909 -4.0493774 -4.6033416 -4.7291327 -4.31642 -3.6786501][-1.9846444 -1.6363997 -1.0645628 -0.72947 -0.13835669 0.3918066 0.62809515 0.39223337 -0.63543129 -1.9601209 -3.2805986 -4.2197394 -4.5597053 -4.4136815 -3.7560985][-1.6468425 -1.4607868 -1.0229104 -0.58909416 0.29367352 1.2552733 1.9306612 1.882112 1.0056882 -0.62176538 -2.3483264 -3.7431655 -4.5791073 -4.7212949 -4.2253385][-1.6673121 -1.6475923 -1.2980502 -0.75739479 0.33119583 1.6238565 2.5724912 2.6655712 1.7496796 0.012397289 -2.0324543 -3.8845503 -4.9578857 -5.2839165 -4.9238176][-2.0177834 -1.9283905 -1.5990398 -0.93708324 0.41725969 1.7270041 2.6234684 2.8863311 2.0720239 0.20657063 -2.0545871 -3.952158 -5.2183819 -5.6939483 -5.2999821][-2.3770778 -2.3665354 -2.011055 -1.4736252 -0.14706373 1.2594776 2.3415985 2.6656442 1.8395157 0.1733737 -2.0128822 -4.0951996 -5.5639119 -6.1184773 -5.7552509][-2.53203 -2.6442881 -2.413841 -1.9472969 -0.98007631 0.21613598 1.2774267 1.721282 1.1098714 -0.44420075 -2.5264611 -4.4629335 -5.9008856 -6.5453796 -6.1905317][-2.2431135 -2.6279488 -2.8855004 -2.6865931 -1.8510261 -0.71505904 0.22296429 0.64223862 0.18234348 -1.2602258 -3.1326585 -4.9600554 -6.2254248 -6.6980476 -6.3570323][-2.3205345 -2.8568079 -3.1441636 -3.0869091 -2.4675035 -1.4765515 -0.68308163 -0.34007835 -0.80183411 -2.0512161 -3.7368498 -5.371026 -6.3975792 -6.5961857 -6.080359][-2.3144207 -3.0303206 -3.4115064 -3.352469 -2.68013 -1.8384411 -1.2476935 -0.90230656 -1.4486482 -2.6015685 -4.0656939 -5.4076419 -6.0754023 -6.1610875 -5.6734347][-2.31681 -3.0351243 -3.5105987 -3.4222703 -2.8688562 -2.0410597 -1.4416921 -1.3769646 -1.8989189 -2.8901715 -4.1890531 -5.2153316 -5.7443032 -5.70126 -5.0853705][-2.8596945 -3.4885931 -3.7527065 -3.504144 -2.8810573 -2.0481741 -1.5584736 -1.5636804 -2.0601468 -3.1210806 -4.2456651 -5.131773 -5.494976 -5.2315993 -4.5315108]]...]
INFO - root - 2017-12-16 04:58:59.336439: step 16910, loss = 0.24, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 25h:00m:04s remains)
INFO - root - 2017-12-16 04:59:02.162418: step 16920, loss = 0.31, batch loss = 0.26 (28.4 examples/sec; 0.281 sec/batch; 24h:39m:52s remains)
INFO - root - 2017-12-16 04:59:05.009807: step 16930, loss = 0.49, batch loss = 0.43 (28.4 examples/sec; 0.282 sec/batch; 24h:41m:31s remains)
INFO - root - 2017-12-16 04:59:07.861675: step 16940, loss = 0.34, batch loss = 0.28 (27.1 examples/sec; 0.295 sec/batch; 25h:51m:20s remains)
INFO - root - 2017-12-16 04:59:10.672327: step 16950, loss = 0.39, batch loss = 0.33 (28.6 examples/sec; 0.279 sec/batch; 24h:29m:23s remains)
INFO - root - 2017-12-16 04:59:13.484354: step 16960, loss = 0.26, batch loss = 0.20 (27.8 examples/sec; 0.287 sec/batch; 25h:10m:54s remains)
INFO - root - 2017-12-16 04:59:16.309454: step 16970, loss = 0.46, batch loss = 0.40 (28.3 examples/sec; 0.283 sec/batch; 24h:48m:50s remains)
INFO - root - 2017-12-16 04:59:19.123177: step 16980, loss = 0.20, batch loss = 0.15 (29.7 examples/sec; 0.270 sec/batch; 23h:38m:41s remains)
INFO - root - 2017-12-16 04:59:21.982079: step 16990, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 24h:29m:59s remains)
INFO - root - 2017-12-16 04:59:24.777491: step 17000, loss = 0.27, batch loss = 0.22 (29.1 examples/sec; 0.275 sec/batch; 24h:04m:53s remains)
2017-12-16 04:59:25.229389: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7582197 -4.4877434 -4.1659684 -3.8534851 -3.5862203 -3.508214 -3.4786642 -3.390008 -3.2411757 -3.0278645 -2.7583489 -2.5407329 -2.4244955 -2.3725984 -2.4147103][-4.5060425 -4.2175007 -3.9710398 -3.8384404 -3.6702459 -3.5736098 -3.3886514 -3.2150204 -3.0875654 -2.9807291 -2.9012682 -2.7339625 -2.700031 -2.68765 -2.7226415][-3.7891614 -3.8034329 -3.721869 -3.5464983 -3.2682362 -3.1662946 -2.9774127 -2.8750563 -2.873508 -2.9846959 -3.1466308 -3.0977 -3.1314371 -3.1557932 -3.232955][-2.6453757 -2.8787258 -2.9612663 -2.7820015 -2.4182141 -2.3052728 -2.0917904 -2.1019013 -2.2094715 -2.4999728 -2.9452014 -3.2021704 -3.5418606 -3.662318 -3.6536684][-1.6087687 -1.9784431 -1.9676707 -1.7686887 -1.2437882 -0.91153383 -0.55513692 -0.47950792 -0.78823471 -1.4747474 -2.2414596 -2.7975149 -3.2534115 -3.623266 -3.7806726][-1.1343541 -1.344696 -1.1907082 -0.65780592 0.2698369 0.86300659 1.3603373 1.3981452 1.0472264 0.16065407 -0.92778349 -1.88891 -2.7694771 -3.3509321 -3.5282598][-1.0041692 -1.0145655 -0.60693645 0.25435448 1.4792657 2.3947392 3.1606278 3.1960411 2.6464214 1.4766455 0.054680347 -1.120595 -2.1486309 -2.7491751 -2.9387751][-1.4064288 -1.2964554 -0.71588755 0.30601072 1.6622944 2.7772546 3.6814356 3.7590437 3.234323 2.0856133 0.67735004 -0.59814787 -1.8056586 -2.5749364 -2.7904563][-2.0932217 -2.061969 -1.6719341 -0.81697083 0.36086226 1.6197882 2.6529951 2.8032646 2.4015918 1.3196034 0.038100719 -1.273025 -2.3497903 -2.7876306 -2.843004][-2.6341698 -2.949497 -2.92368 -2.3985085 -1.4175575 -0.344584 0.55131817 0.79354239 0.48289919 -0.381073 -1.5253224 -2.6258259 -3.5121105 -3.7300906 -3.4391844][-3.2950518 -3.5551877 -3.6518183 -3.7122633 -3.3536706 -2.5392601 -1.6426101 -1.4791398 -1.9407151 -2.6690173 -3.4207764 -4.0970621 -4.5439744 -4.4440589 -3.9364171][-3.4781258 -3.8040619 -4.2159209 -4.3589177 -4.2609119 -4.1108975 -3.7380748 -3.5628815 -3.5915949 -3.9953935 -4.5787225 -5.0129871 -5.18721 -4.8833995 -4.2895131][-3.4111593 -3.59022 -3.8677764 -4.1990376 -4.487143 -4.4980922 -4.2876873 -4.2902026 -4.5415263 -4.8465819 -4.9896269 -5.0735226 -5.0193224 -4.7091031 -4.2758417][-3.2561536 -3.1167047 -3.1127205 -3.229619 -3.4783709 -3.6772566 -3.7965808 -4.1195173 -4.4007826 -4.3708653 -4.3490858 -4.4078679 -4.3582683 -4.0428114 -3.6056056][-3.1516776 -2.7728195 -2.527858 -2.4475536 -2.5455325 -2.7465668 -3.0166731 -3.350904 -3.6558626 -3.856159 -3.7688603 -3.2717652 -2.7923398 -2.6346669 -2.7409847]]...]
INFO - root - 2017-12-16 04:59:28.072091: step 17010, loss = 0.25, batch loss = 0.19 (29.0 examples/sec; 0.275 sec/batch; 24h:08m:15s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 04:59:30.864253: step 17020, loss = 0.23, batch loss = 0.17 (28.6 examples/sec; 0.280 sec/batch; 24h:30m:55s remains)
INFO - root - 2017-12-16 04:59:33.769935: step 17030, loss = 0.24, batch loss = 0.18 (27.4 examples/sec; 0.292 sec/batch; 25h:32m:48s remains)
INFO - root - 2017-12-16 04:59:36.602762: step 17040, loss = 0.26, batch loss = 0.21 (29.0 examples/sec; 0.276 sec/batch; 24h:09m:55s remains)
INFO - root - 2017-12-16 04:59:39.456333: step 17050, loss = 0.25, batch loss = 0.19 (28.8 examples/sec; 0.278 sec/batch; 24h:21m:55s remains)
INFO - root - 2017-12-16 04:59:42.285109: step 17060, loss = 0.22, batch loss = 0.16 (27.7 examples/sec; 0.288 sec/batch; 25h:16m:27s remains)
INFO - root - 2017-12-16 04:59:45.106041: step 17070, loss = 0.30, batch loss = 0.24 (28.8 examples/sec; 0.278 sec/batch; 24h:18m:58s remains)
INFO - root - 2017-12-16 04:59:47.926016: step 17080, loss = 0.43, batch loss = 0.37 (28.9 examples/sec; 0.277 sec/batch; 24h:17m:10s remains)
INFO - root - 2017-12-16 04:59:50.792998: step 17090, loss = 0.24, batch loss = 0.18 (25.2 examples/sec; 0.318 sec/batch; 27h:49m:25s remains)
INFO - root - 2017-12-16 04:59:53.603251: step 17100, loss = 0.27, batch loss = 0.21 (28.6 examples/sec; 0.280 sec/batch; 24h:32m:26s remains)
2017-12-16 04:59:54.078324: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4094529 -4.114491 -4.889524 -5.6475677 -6.2066894 -6.4912634 -6.8084311 -7.5230761 -8.0966148 -8.4011354 -8.5922232 -8.7545652 -8.442852 -7.8168106 -7.2470179][-4.0560327 -5.0718117 -5.9880104 -6.8457885 -7.5430551 -8.1994486 -8.4568443 -8.74238 -8.9707909 -9.589344 -9.8976412 -9.7239342 -9.3900061 -8.7641983 -7.9989681][-4.5529437 -5.6595173 -6.6777821 -7.5004892 -7.8135591 -7.9407554 -7.8175488 -8.108036 -8.4478474 -8.7851324 -9.131753 -9.3848133 -9.2319832 -8.3626995 -7.575181][-4.8463845 -5.9464726 -6.8992128 -7.4596748 -7.2183003 -6.7152367 -5.9341855 -5.412044 -5.2691569 -6.0010653 -6.8795824 -7.3465371 -7.5506287 -7.2511783 -6.6397295][-4.9063964 -5.8235283 -6.359374 -6.35962 -5.7175732 -4.5374904 -2.9650283 -1.9482894 -1.8890917 -2.3744936 -3.3143811 -4.2490993 -4.8901439 -4.9533544 -4.8425126][-4.2616425 -5.0354848 -5.2780762 -4.88095 -3.2687292 -1.3609107 0.37833881 1.7594404 2.2448711 1.2483339 -0.36109924 -1.3500896 -1.7522435 -2.1352768 -2.4556153][-3.7164559 -3.9567585 -3.8610649 -3.0894673 -1.4400313 0.73213291 3.0281692 4.5331564 4.4095793 3.4817314 2.2477608 0.91288614 -0.030985355 -0.48104477 -0.8146677][-3.5680366 -3.6769781 -3.3346286 -2.157233 -0.1841836 2.1482458 4.043767 5.1933222 5.27223 4.3425922 3.0205579 2.039989 1.4337955 0.52135324 -0.21528149][-3.2850313 -3.3565598 -2.9107032 -1.7897995 -0.11240435 1.6264863 3.2782111 4.2003164 4.073288 3.4436297 2.6184454 1.5846348 0.67241669 -0.10392284 -0.78560448][-3.1608963 -3.2948275 -3.1257448 -2.2276852 -0.92474985 0.50922489 1.4920945 1.802587 1.7405958 1.3465843 0.76199579 0.086936474 -0.45571089 -1.0746317 -1.6130548][-3.511812 -3.671247 -3.65209 -3.1853521 -2.3482411 -1.5716052 -0.87159014 -0.62764144 -0.7374258 -1.1862595 -1.6835093 -2.1139753 -2.5278645 -2.9717607 -3.3268614][-3.8692844 -4.0001431 -3.9609225 -3.8471365 -3.7287996 -3.5502491 -3.3444066 -3.395077 -3.480303 -3.892457 -4.342936 -4.8397951 -5.1656609 -5.2902842 -5.3284965][-4.4468932 -4.7160783 -4.91312 -5.0498304 -5.0374584 -5.1587567 -5.3377571 -5.5457973 -5.6637611 -5.7586322 -5.942359 -5.9601355 -6.1486897 -6.4597673 -6.2251825][-4.3511305 -4.7334347 -5.0639009 -5.3281145 -5.4986272 -5.6761122 -5.8399267 -6.0759168 -6.2286925 -6.3504829 -6.3278317 -6.5640221 -6.6943984 -6.662921 -6.5159779][-4.4302406 -4.5674124 -4.7623706 -5.1165543 -5.4197626 -5.7329736 -6.0474529 -6.2138739 -6.2037692 -6.1811361 -6.25206 -6.2010388 -6.1373968 -6.3115215 -6.2593346]]...]
INFO - root - 2017-12-16 04:59:56.894516: step 17110, loss = 0.23, batch loss = 0.17 (28.1 examples/sec; 0.285 sec/batch; 24h:58m:41s remains)
INFO - root - 2017-12-16 04:59:59.730556: step 17120, loss = 0.21, batch loss = 0.15 (27.7 examples/sec; 0.289 sec/batch; 25h:17m:29s remains)
INFO - root - 2017-12-16 05:00:02.604095: step 17130, loss = 0.22, batch loss = 0.16 (28.5 examples/sec; 0.280 sec/batch; 24h:33m:27s remains)
INFO - root - 2017-12-16 05:00:05.401031: step 17140, loss = 0.22, batch loss = 0.16 (28.5 examples/sec; 0.281 sec/batch; 24h:35m:21s remains)
INFO - root - 2017-12-16 05:00:08.305819: step 17150, loss = 0.23, batch loss = 0.17 (27.9 examples/sec; 0.287 sec/batch; 25h:07m:37s remains)
INFO - root - 2017-12-16 05:00:11.140540: step 17160, loss = 0.26, batch loss = 0.21 (28.3 examples/sec; 0.283 sec/batch; 24h:48m:16s remains)
INFO - root - 2017-12-16 05:00:13.941943: step 17170, loss = 0.27, batch loss = 0.21 (29.1 examples/sec; 0.275 sec/batch; 24h:07m:00s remains)
INFO - root - 2017-12-16 05:00:16.769752: step 17180, loss = 0.28, batch loss = 0.22 (28.3 examples/sec; 0.283 sec/batch; 24h:45m:14s remains)
INFO - root - 2017-12-16 05:00:19.604182: step 17190, loss = 0.21, batch loss = 0.15 (27.2 examples/sec; 0.295 sec/batch; 25h:47m:42s remains)
INFO - root - 2017-12-16 05:00:22.462480: step 17200, loss = 0.38, batch loss = 0.33 (27.7 examples/sec; 0.289 sec/batch; 25h:19m:14s remains)
2017-12-16 05:00:22.934614: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7178307 -4.7741771 -5.0711036 -5.4819894 -6.1528082 -6.665473 -7.2680912 -7.7388477 -7.634099 -7.078721 -6.3374386 -5.8790493 -5.4813051 -4.8995767 -4.6549821][-4.1387973 -4.2039747 -4.2445536 -4.4286017 -5.0455585 -5.79875 -6.578928 -7.1121774 -7.0911579 -6.4980512 -5.6169653 -5.1000543 -4.7968907 -4.0605478 -3.8068557][-3.5342693 -3.0760407 -3.0057731 -3.1539478 -3.5452347 -4.3333578 -4.994308 -5.6767359 -5.8542061 -5.4541368 -4.4899778 -4.0568709 -3.9596105 -3.390573 -3.210012][-2.4747953 -1.944679 -1.6404881 -1.6342587 -2.1536872 -2.7772598 -3.1976738 -3.6945448 -3.8685417 -3.6257849 -2.9276652 -2.5423098 -2.6345329 -2.5312023 -2.3771813][-1.8113267 -1.2041066 -0.61688066 -0.37176657 -0.76287222 -1.4781983 -1.7451375 -2.0427756 -2.0466809 -1.9130452 -1.3894136 -1.1566203 -1.5760622 -1.8014205 -2.0367112][-1.8936424 -1.0215945 -0.52607679 -0.13160801 0.066114426 -0.24085569 -0.15205288 0.07802248 0.15046406 0.13128519 0.47633743 0.25180006 -0.39606142 -1.0909274 -1.5607848][-2.8851962 -2.0639958 -1.2165852 -0.52765989 -0.1781044 -0.00073862076 0.71998835 1.2584977 1.6055932 1.5398693 1.4375262 1.2668953 0.50822353 -0.32851744 -1.0957737][-3.8601074 -3.3844109 -2.6605651 -1.7130187 -0.89705205 -0.40246058 0.49180555 1.4180837 2.1709976 2.0875721 2.0405087 1.756074 1.0136209 0.057407856 -0.97413182][-4.9322529 -4.4066539 -3.9026761 -3.1657515 -2.2559886 -1.5079753 -0.55508995 0.33586502 1.1056719 1.5503788 1.7628331 1.6351528 1.0437379 0.17753601 -0.61196184][-5.7131453 -5.5075545 -4.9934111 -4.3357515 -3.6264544 -2.9690523 -2.1068347 -1.2551868 -0.38232517 0.16401052 0.44299316 0.33176374 -0.18705463 -0.63651705 -1.1630697][-5.8442826 -5.6023674 -5.3491664 -5.0784183 -4.5371494 -4.0040479 -3.5086677 -2.8129611 -1.9536934 -1.3587615 -1.0386188 -1.1213112 -1.3840234 -1.7660294 -2.263901][-5.9100695 -5.5353475 -5.36499 -5.2544003 -5.097291 -4.89038 -4.5887909 -4.2018623 -3.5965168 -2.9475663 -2.5336108 -2.5571737 -2.7963941 -2.9029527 -3.0434992][-5.1803312 -4.9308414 -4.8657622 -4.8045454 -4.804306 -4.837574 -4.9615021 -4.7413092 -4.1794176 -3.7274036 -3.4433277 -3.4283173 -3.5326471 -3.6955605 -3.810411][-4.5411949 -4.2255859 -4.1305389 -4.270721 -4.4292054 -4.4294248 -4.5502915 -4.4954705 -4.2273307 -3.8591578 -3.6225617 -3.7115722 -3.8260939 -3.9149029 -4.0141358][-3.9769151 -3.6525948 -3.5977683 -3.6432087 -3.7737851 -3.8915682 -4.0103388 -3.9478583 -3.7734289 -3.5940139 -3.4157705 -3.4089556 -3.5476274 -3.603018 -3.5658693]]...]
INFO - root - 2017-12-16 05:00:25.708079: step 17210, loss = 0.24, batch loss = 0.18 (28.7 examples/sec; 0.279 sec/batch; 24h:25m:41s remains)
INFO - root - 2017-12-16 05:00:28.548130: step 17220, loss = 0.38, batch loss = 0.32 (27.7 examples/sec; 0.289 sec/batch; 25h:17m:29s remains)
INFO - root - 2017-12-16 05:00:31.391439: step 17230, loss = 0.39, batch loss = 0.33 (28.8 examples/sec; 0.278 sec/batch; 24h:18m:16s remains)
INFO - root - 2017-12-16 05:00:34.191180: step 17240, loss = 0.32, batch loss = 0.26 (28.6 examples/sec; 0.280 sec/batch; 24h:32m:01s remains)
INFO - root - 2017-12-16 05:00:37.078544: step 17250, loss = 0.28, batch loss = 0.23 (27.0 examples/sec; 0.297 sec/batch; 25h:59m:27s remains)
INFO - root - 2017-12-16 05:00:39.885843: step 17260, loss = 0.24, batch loss = 0.18 (28.5 examples/sec; 0.281 sec/batch; 24h:34m:52s remains)
INFO - root - 2017-12-16 05:00:42.761240: step 17270, loss = 0.30, batch loss = 0.24 (28.1 examples/sec; 0.285 sec/batch; 24h:57m:02s remains)
INFO - root - 2017-12-16 05:00:45.588748: step 17280, loss = 0.34, batch loss = 0.29 (29.1 examples/sec; 0.275 sec/batch; 24h:03m:48s remains)
INFO - root - 2017-12-16 05:00:48.386188: step 17290, loss = 0.23, batch loss = 0.17 (29.3 examples/sec; 0.273 sec/batch; 23h:53m:51s remains)
INFO - root - 2017-12-16 05:00:51.202590: step 17300, loss = 0.24, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 23h:58m:12s remains)
2017-12-16 05:00:51.660354: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7611113 -6.2035484 -6.69871 -6.801836 -6.754034 -6.1812496 -5.92524 -5.9064636 -5.6176853 -5.1262369 -4.6913552 -4.1359916 -3.6515865 -3.4035037 -3.3574667][-7.0619316 -6.8635426 -6.9472694 -7.3880711 -7.7585578 -7.5964651 -7.1277227 -6.7326903 -6.3400269 -5.9483166 -5.6299067 -5.3167152 -5.1780224 -5.0496721 -4.9888434][-8.0190544 -8.0257893 -7.9046307 -7.8547034 -7.8146782 -7.1049709 -6.2838206 -6.2435522 -6.3212123 -6.1079769 -5.9024119 -5.8481541 -5.9179316 -6.1510363 -6.0947337][-8.369009 -7.975934 -7.8692265 -7.5488186 -6.9194393 -6.0914617 -4.9322491 -4.1317148 -4.0825315 -4.9564624 -5.9262118 -6.3750196 -6.5260696 -6.6459575 -6.5956936][-7.9071655 -7.5325127 -7.1477528 -6.2586489 -5.2172265 -3.7801681 -2.3474483 -1.3784211 -1.2250218 -2.395926 -3.9826612 -5.31366 -6.0928946 -6.47107 -6.1518173][-6.5872688 -6.100955 -5.3881063 -4.0508866 -1.9382217 0.10813141 1.6312995 2.4794197 2.2731109 0.71726847 -1.5967059 -3.5574367 -4.7133212 -5.3793783 -5.1973658][-5.6431651 -4.9097657 -3.97411 -2.0360861 0.66932058 3.3706064 5.53106 6.2691584 5.7136478 3.6029396 0.98547077 -1.4002116 -2.982718 -3.5451429 -3.6323237][-5.2566643 -4.6351266 -3.7424049 -1.6719663 1.1043983 4.1026955 6.3232365 7.0367556 6.2969418 3.9937754 1.3948808 -0.701684 -1.9843514 -2.7052164 -2.92411][-5.2066069 -4.8799734 -4.1062136 -2.3487909 -0.12485361 2.356976 4.3881893 5.32742 4.8876543 2.9443107 0.60415888 -1.4107676 -2.518496 -2.6055872 -2.2991478][-5.3264804 -5.3359156 -5.2117062 -4.1091456 -2.3543625 -0.57502031 0.96858072 1.7347021 1.4860349 -0.029311657 -1.6713767 -3.1925263 -3.8955908 -3.6221771 -3.1114845][-5.9994464 -6.4036131 -6.61638 -6.2211428 -5.3830028 -4.2384467 -2.9255428 -2.4144804 -2.6033134 -3.4520483 -4.2970738 -5.1562014 -5.5830832 -5.1614361 -4.2411685][-6.3826437 -6.9345155 -7.5673766 -7.6454329 -7.3391733 -6.7932854 -6.2494011 -5.8877554 -5.6184945 -5.8773332 -6.3880587 -6.9072962 -6.8753643 -6.1879759 -5.2252522][-5.57679 -6.4419208 -7.3480287 -7.9381151 -8.2373123 -7.7480516 -7.1289988 -6.8407478 -6.7259331 -6.73374 -6.6785326 -6.882781 -7.038846 -6.7283068 -5.8684363][-5.1383748 -5.6858058 -6.2580643 -6.9203386 -7.4258947 -7.3459096 -6.9325676 -6.5332747 -6.4068727 -6.3189793 -6.2435532 -6.3140535 -6.4428911 -6.2855234 -5.7160349][-4.8977103 -5.1049623 -5.3549962 -5.6488557 -5.8715038 -5.9265895 -5.8290906 -5.5234876 -5.1868014 -5.1982465 -5.4278731 -5.3841348 -5.2327366 -5.2943444 -5.2385859]]...]
INFO - root - 2017-12-16 05:00:54.478599: step 17310, loss = 0.32, batch loss = 0.26 (28.4 examples/sec; 0.282 sec/batch; 24h:40m:14s remains)
INFO - root - 2017-12-16 05:00:57.311260: step 17320, loss = 0.22, batch loss = 0.16 (27.9 examples/sec; 0.287 sec/batch; 25h:05m:31s remains)
INFO - root - 2017-12-16 05:01:00.180379: step 17330, loss = 0.23, batch loss = 0.17 (27.9 examples/sec; 0.287 sec/batch; 25h:05m:33s remains)
INFO - root - 2017-12-16 05:01:02.961097: step 17340, loss = 0.38, batch loss = 0.32 (28.4 examples/sec; 0.282 sec/batch; 24h:39m:40s remains)
INFO - root - 2017-12-16 05:01:05.788649: step 17350, loss = 0.37, batch loss = 0.31 (28.4 examples/sec; 0.282 sec/batch; 24h:39m:29s remains)
INFO - root - 2017-12-16 05:01:08.602939: step 17360, loss = 0.27, batch loss = 0.21 (28.9 examples/sec; 0.277 sec/batch; 24h:15m:46s remains)
INFO - root - 2017-12-16 05:01:11.367456: step 17370, loss = 0.40, batch loss = 0.34 (29.5 examples/sec; 0.271 sec/batch; 23h:45m:44s remains)
INFO - root - 2017-12-16 05:01:14.212490: step 17380, loss = 0.30, batch loss = 0.24 (27.8 examples/sec; 0.288 sec/batch; 25h:13m:15s remains)
INFO - root - 2017-12-16 05:01:17.013314: step 17390, loss = 0.23, batch loss = 0.17 (28.6 examples/sec; 0.280 sec/batch; 24h:30m:36s remains)
INFO - root - 2017-12-16 05:01:19.824245: step 17400, loss = 0.25, batch loss = 0.20 (29.3 examples/sec; 0.273 sec/batch; 23h:52m:52s remains)
2017-12-16 05:01:20.295212: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.954154 -2.922931 -3.0270655 -3.26956 -3.3418441 -3.4375644 -3.614393 -3.418757 -3.3517382 -3.097949 -2.9914978 -3.14261 -3.2913158 -3.4728825 -3.5148649][-2.2015486 -2.4088416 -2.4859438 -2.9603963 -3.485254 -3.6679809 -3.6794653 -3.5605488 -3.6886127 -3.5204754 -3.5132651 -3.636704 -3.7749169 -4.0856872 -4.1357274][-1.364527 -1.6288178 -1.9623928 -2.5527792 -3.0781066 -3.4999757 -3.8032374 -3.7874603 -3.9961338 -4.0625715 -4.3586183 -4.7113786 -4.8926311 -5.0037336 -4.8748455][-1.0937736 -1.4265041 -1.6908827 -2.0890281 -2.5064795 -2.9157429 -3.1757236 -3.3930926 -3.6540291 -3.860955 -4.3486953 -4.8279705 -5.2808342 -5.5245981 -5.4582982][-1.5020101 -1.5811844 -1.5530376 -1.7380006 -1.9484017 -2.0004563 -1.8938344 -2.2180228 -2.689137 -3.089216 -3.5060153 -4.0970793 -4.7441607 -5.1136465 -5.2114868][-1.7224166 -1.4091237 -1.0713747 -0.92675948 -0.81903958 -0.72686362 -0.64314032 -0.83058405 -1.2195685 -1.8415928 -2.4864759 -3.1284251 -3.714165 -4.2733955 -4.5419507][-1.9388673 -1.377053 -0.76742864 -0.21176291 0.25442743 0.63713074 1.0403142 0.95656538 0.57133484 -0.11319733 -0.85140181 -1.582355 -2.3319504 -2.9349618 -3.3228369][-2.1796021 -1.5326931 -0.82707596 -0.038249493 0.74576139 1.4156647 1.989809 2.1251826 1.9948478 1.3315821 0.53170013 -0.21729994 -0.9335134 -1.6468616 -2.2847807][-2.4820051 -1.75139 -0.90108585 -0.10524702 0.58127356 1.2277341 1.818193 1.8356133 1.5399985 0.99282312 0.40796852 -0.24552631 -0.88463211 -1.4025335 -1.8053045][-2.4379444 -1.8009162 -1.229986 -0.65500951 -0.17884016 0.14191866 0.29567575 0.2209444 -0.036334038 -0.51671886 -0.9517951 -1.4670815 -1.854248 -2.082675 -2.2360055][-2.4634547 -1.920217 -1.4417355 -1.1981859 -1.2444899 -1.1839085 -1.2274783 -1.7694013 -2.2914245 -2.6191475 -2.8796494 -3.1862559 -3.3757982 -3.3814564 -3.1899936][-2.5548859 -2.1693735 -1.9768636 -1.9593885 -2.0957417 -2.44067 -3.02071 -3.6039903 -4.0213056 -4.3335986 -4.5036063 -4.658905 -4.6628432 -4.5187273 -4.2903395][-2.5017872 -2.2276828 -2.0535667 -2.2847049 -2.9849415 -3.5393786 -4.0979648 -4.787446 -5.2811027 -5.5032268 -5.5490284 -5.48909 -5.3530126 -5.1946931 -4.8729272][-2.458138 -2.215225 -2.1731577 -2.3982031 -2.9369783 -3.622427 -4.3883266 -5.0691519 -5.62455 -5.7965221 -5.6854506 -5.63018 -5.564527 -5.3446379 -5.0446873][-2.3269296 -2.1489384 -2.1407607 -2.3130825 -2.7957468 -3.4056959 -4.0096951 -4.5576239 -4.8880177 -5.1966634 -5.4335489 -5.2159367 -4.8773146 -4.7572865 -4.6934848]]...]
INFO - root - 2017-12-16 05:01:23.171459: step 17410, loss = 0.22, batch loss = 0.17 (27.0 examples/sec; 0.296 sec/batch; 25h:56m:26s remains)
INFO - root - 2017-12-16 05:01:25.980732: step 17420, loss = 0.26, batch loss = 0.20 (29.1 examples/sec; 0.275 sec/batch; 24h:05m:10s remains)
INFO - root - 2017-12-16 05:01:28.842464: step 17430, loss = 0.28, batch loss = 0.22 (27.9 examples/sec; 0.286 sec/batch; 25h:03m:06s remains)
INFO - root - 2017-12-16 05:01:31.662583: step 17440, loss = 0.29, batch loss = 0.23 (28.6 examples/sec; 0.280 sec/batch; 24h:30m:17s remains)
INFO - root - 2017-12-16 05:01:34.482031: step 17450, loss = 0.31, batch loss = 0.26 (28.5 examples/sec; 0.281 sec/batch; 24h:33m:51s remains)
INFO - root - 2017-12-16 05:01:37.259371: step 17460, loss = 0.22, batch loss = 0.16 (28.2 examples/sec; 0.284 sec/batch; 24h:48m:55s remains)
INFO - root - 2017-12-16 05:01:40.088593: step 17470, loss = 0.28, batch loss = 0.22 (27.9 examples/sec; 0.286 sec/batch; 25h:03m:42s remains)
INFO - root - 2017-12-16 05:01:42.888951: step 17480, loss = 0.24, batch loss = 0.18 (28.6 examples/sec; 0.280 sec/batch; 24h:28m:40s remains)
INFO - root - 2017-12-16 05:01:45.717902: step 17490, loss = 0.38, batch loss = 0.32 (28.2 examples/sec; 0.283 sec/batch; 24h:47m:36s remains)
INFO - root - 2017-12-16 05:01:48.523161: step 17500, loss = 0.37, batch loss = 0.31 (27.7 examples/sec; 0.288 sec/batch; 25h:14m:28s remains)
2017-12-16 05:01:48.975818: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3601942 -5.6688461 -6.0154281 -6.468008 -6.91166 -7.0651531 -7.1226854 -7.539958 -8.1363983 -8.4083052 -8.2888746 -7.9985228 -7.3440585 -6.4339232 -5.3795376][-5.1082988 -5.5023875 -5.8568573 -6.117507 -6.2878342 -6.5790739 -6.8058672 -7.074698 -7.4005213 -7.7638674 -8.0013676 -7.7084837 -7.0252523 -6.1775236 -5.3023047][-4.5796962 -4.9357686 -5.1734009 -5.1581421 -5.0091047 -5.0193377 -4.9447317 -5.0438938 -5.4722476 -6.1477251 -6.73189 -6.7516289 -6.4428344 -5.8819647 -5.3302059][-3.9614916 -4.1513653 -4.0880032 -3.8585424 -3.4457111 -2.929853 -2.5357895 -2.6229444 -2.9973893 -3.5630691 -4.262743 -4.9876461 -5.45683 -5.4177423 -5.2352242][-2.9441009 -3.1066639 -2.8917875 -2.3091698 -1.5885265 -0.76662183 0.0052380562 0.21942186 -0.089202404 -0.98787761 -2.1525829 -3.3310847 -4.2747016 -4.9112034 -5.3217697][-2.056639 -1.922348 -1.3766768 -0.51489878 0.52018166 1.3964334 2.3221254 2.7298241 2.5022888 1.4721375 -0.012096405 -1.8682261 -3.6881361 -4.8755245 -5.4521][-1.3940456 -1.1588917 -0.71990848 0.23222446 1.6121531 2.8111539 3.8990097 4.1829176 3.839982 2.7364492 1.0020618 -1.2253127 -3.3361588 -4.9137325 -5.7402868][-1.3432498 -1.043539 -0.75281215 0.032525539 1.2445378 2.5397325 3.7490702 4.2195053 3.952445 2.8267007 1.1311464 -1.1603439 -3.4236524 -5.128593 -5.9085426][-1.5832391 -1.4466999 -1.2566717 -0.75308609 0.062117577 1.2947421 2.3876429 2.8874269 2.9009 1.9378777 0.47376728 -1.6281018 -3.786674 -5.3740563 -6.1988549][-1.9495425 -2.0789731 -2.4218774 -2.208199 -1.4332273 -0.44268465 0.46122408 0.94470692 1.054945 0.29511309 -0.94963479 -2.683064 -4.6404967 -6.0558691 -6.6874151][-3.1293354 -3.1862037 -3.3805666 -3.503366 -3.3284435 -2.6057763 -1.8058679 -1.422888 -1.5204768 -1.924629 -2.8500786 -4.0830646 -5.3671136 -6.3631454 -6.7331028][-3.9701853 -4.2256722 -4.6894636 -4.8790731 -4.7420931 -4.4620571 -4.2700372 -3.8484483 -3.6487389 -3.7250972 -4.31031 -5.0730457 -5.665453 -6.2584715 -6.5343151][-4.7965341 -5.1305881 -5.5772009 -6.02273 -6.3777089 -6.1573505 -5.8192005 -5.481833 -5.33403 -5.1402659 -5.1309566 -5.4738712 -5.98776 -6.2735734 -6.2655945][-5.8285646 -6.2892108 -6.6489868 -6.981524 -7.3904934 -7.4608088 -7.2792931 -6.7678075 -6.1484962 -5.5590262 -5.5195808 -5.5546575 -5.56989 -5.8481684 -5.9834805][-6.4473267 -6.939785 -7.4740644 -8.1132736 -8.4794044 -8.4833946 -8.1482229 -7.4584532 -6.8761058 -6.0673194 -5.5525746 -5.4715033 -5.6239986 -5.7781539 -5.5512047]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-17500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-17500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 05:01:52.331655: step 17510, loss = 0.32, batch loss = 0.26 (28.2 examples/sec; 0.284 sec/batch; 24h:50m:52s remains)
INFO - root - 2017-12-16 05:01:55.169505: step 17520, loss = 0.33, batch loss = 0.27 (29.4 examples/sec; 0.273 sec/batch; 23h:50m:42s remains)
INFO - root - 2017-12-16 05:01:57.967259: step 17530, loss = 0.33, batch loss = 0.27 (29.5 examples/sec; 0.271 sec/batch; 23h:43m:00s remains)
INFO - root - 2017-12-16 05:02:00.758683: step 17540, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 24h:55m:50s remains)
INFO - root - 2017-12-16 05:02:03.580929: step 17550, loss = 0.31, batch loss = 0.26 (29.5 examples/sec; 0.271 sec/batch; 23h:43m:58s remains)
INFO - root - 2017-12-16 05:02:06.406239: step 17560, loss = 0.27, batch loss = 0.22 (27.8 examples/sec; 0.288 sec/batch; 25h:12m:07s remains)
INFO - root - 2017-12-16 05:02:09.278663: step 17570, loss = 0.29, batch loss = 0.24 (28.4 examples/sec; 0.282 sec/batch; 24h:38m:00s remains)
INFO - root - 2017-12-16 05:02:12.154635: step 17580, loss = 0.41, batch loss = 0.35 (28.1 examples/sec; 0.284 sec/batch; 24h:53m:00s remains)
INFO - root - 2017-12-16 05:02:14.973593: step 17590, loss = 0.23, batch loss = 0.17 (27.3 examples/sec; 0.293 sec/batch; 25h:36m:09s remains)
INFO - root - 2017-12-16 05:02:17.802893: step 17600, loss = 0.31, batch loss = 0.25 (27.5 examples/sec; 0.291 sec/batch; 25h:29m:27s remains)
2017-12-16 05:02:18.264604: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3332672 -5.1733856 -3.8989897 -3.5992937 -3.9075208 -4.2151461 -4.2309027 -4.5059738 -4.8715949 -5.0840816 -5.2394595 -5.1283474 -4.8207622 -4.5621662 -4.5053897][-6.3295321 -5.2165856 -4.1703167 -3.5263238 -3.4772372 -4.1345968 -4.5169263 -4.5303111 -4.36496 -4.5842643 -4.9764886 -5.1077428 -5.062582 -4.6785717 -4.5085144][-6.1647935 -4.9542046 -3.6753142 -2.7994914 -2.5386355 -2.8512626 -3.351295 -3.754704 -3.9247868 -4.46438 -4.7277851 -4.8836884 -5.0957189 -4.9206839 -4.6472993][-5.257905 -4.1103392 -2.903883 -1.9202414 -1.2440915 -1.2923858 -1.3553708 -1.8669617 -2.737386 -3.8017249 -4.4838009 -5.1966934 -5.31803 -4.9426613 -4.5818968][-4.9472485 -3.2599261 -1.6988714 -0.42042923 0.21565914 0.46051264 0.56683254 0.037759781 -0.68928337 -2.3297172 -3.8819041 -4.7069039 -4.8434472 -4.7953033 -4.3589153][-4.95751 -3.3233948 -1.3897889 0.739985 2.4930267 2.8887706 2.6958337 2.1630483 1.0168738 -1.00636 -2.4147534 -3.6246779 -4.4387317 -4.3871064 -3.8291712][-5.25701 -3.6572304 -1.7992268 0.77754879 3.0274744 4.3525181 4.8679705 4.0836735 2.5128202 0.33748198 -1.6314085 -2.9562969 -3.4323401 -3.6559827 -3.6641006][-6.2653284 -4.816123 -2.5937328 0.2150135 2.446496 4.15606 4.9895048 4.4785213 3.1049638 0.90492582 -0.94002366 -2.328594 -2.9009781 -3.2129364 -3.3599432][-8.1716785 -6.931901 -4.7437 -1.7170823 0.79709864 2.5491371 3.433794 3.2367649 2.1465964 0.39453411 -1.1613231 -2.2222137 -2.7492504 -3.0308642 -3.1579404][-9.3373909 -8.4514751 -7.1293211 -4.4815097 -1.7343869 0.055026054 0.95951223 1.2089648 0.65268278 -0.63265538 -1.8811784 -2.752069 -3.0260153 -3.0707967 -3.3256254][-10.624928 -9.7878342 -8.3025627 -6.1838126 -4.4518356 -2.8169494 -1.6148865 -1.3968346 -1.5898952 -2.2732234 -3.0256982 -3.5986674 -3.8345656 -3.8121486 -3.7065439][-11.126196 -9.9085884 -8.57786 -7.0937052 -5.6953406 -4.7103186 -4.2067213 -3.88767 -3.6387064 -3.78574 -4.0823078 -4.5270596 -4.8433547 -4.5662742 -4.1950183][-10.332659 -9.0834417 -7.9529152 -6.7570095 -5.9650364 -5.6961732 -5.6679654 -5.5281429 -5.2730722 -5.1454515 -5.1655188 -5.1047468 -5.1910868 -4.8761787 -4.3353209][-10.110237 -8.4814186 -7.1454296 -6.3323274 -5.8662205 -5.8332663 -6.093627 -6.2239556 -6.2027044 -5.9980631 -5.5788217 -5.2285347 -5.2897172 -4.987421 -4.3349595][-9.5340519 -7.9041076 -6.4513397 -5.5173712 -5.2577972 -5.3880253 -5.620234 -5.9191351 -5.9953117 -5.8799987 -5.6696467 -5.3977175 -5.2087779 -4.9532123 -4.4800768]]...]
INFO - root - 2017-12-16 05:02:21.131695: step 17610, loss = 0.21, batch loss = 0.15 (28.5 examples/sec; 0.281 sec/batch; 24h:34m:24s remains)
INFO - root - 2017-12-16 05:02:23.962817: step 17620, loss = 0.29, batch loss = 0.23 (28.3 examples/sec; 0.283 sec/batch; 24h:44m:28s remains)
INFO - root - 2017-12-16 05:02:26.744544: step 17630, loss = 0.34, batch loss = 0.28 (28.9 examples/sec; 0.277 sec/batch; 24h:11m:39s remains)
INFO - root - 2017-12-16 05:02:29.535968: step 17640, loss = 0.24, batch loss = 0.19 (29.3 examples/sec; 0.273 sec/batch; 23h:52m:59s remains)
INFO - root - 2017-12-16 05:02:32.335725: step 17650, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 24h:39m:32s remains)
INFO - root - 2017-12-16 05:02:35.184973: step 17660, loss = 0.27, batch loss = 0.21 (26.3 examples/sec; 0.304 sec/batch; 26h:36m:11s remains)
INFO - root - 2017-12-16 05:02:38.043705: step 17670, loss = 0.35, batch loss = 0.29 (26.5 examples/sec; 0.302 sec/batch; 26h:23m:43s remains)
INFO - root - 2017-12-16 05:02:40.910888: step 17680, loss = 0.29, batch loss = 0.23 (27.9 examples/sec; 0.287 sec/batch; 25h:06m:17s remains)
INFO - root - 2017-12-16 05:02:43.711388: step 17690, loss = 0.28, batch loss = 0.22 (27.8 examples/sec; 0.288 sec/batch; 25h:10m:04s remains)
INFO - root - 2017-12-16 05:02:46.575764: step 17700, loss = 0.30, batch loss = 0.24 (28.1 examples/sec; 0.284 sec/batch; 24h:52m:32s remains)
2017-12-16 05:02:47.028498: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9577127 -2.8012524 -2.6076708 -2.5140638 -2.5341072 -2.7142825 -3.084764 -3.452301 -3.846627 -4.0512257 -4.1409464 -4.1107874 -3.9521291 -3.680429 -3.3225105][-3.0155206 -2.7354028 -2.525629 -2.4461746 -2.4819717 -2.73696 -3.1173587 -3.383009 -3.7029076 -3.8716121 -3.9259672 -3.770133 -3.4251595 -3.0309072 -2.7028856][-2.8941982 -2.6681857 -2.5335231 -2.4708843 -2.50824 -2.7008383 -2.9810731 -3.0971646 -3.1926363 -3.3430681 -3.3929899 -3.1873574 -2.8716009 -2.3554182 -1.9468527][-2.755188 -2.4566727 -2.3224249 -2.312367 -2.3440988 -2.4274864 -2.4638882 -2.4137464 -2.3245311 -2.1357241 -1.9958274 -1.8570228 -1.7036834 -1.4077752 -1.2968035][-2.6234169 -2.2103186 -1.965868 -1.8697019 -1.830229 -1.7807274 -1.5773387 -1.3393555 -0.97922039 -0.64406705 -0.43706036 -0.32328033 -0.43315339 -0.5433712 -0.86604762][-2.3562162 -1.9157615 -1.6044426 -1.3644655 -1.1735041 -1.1243405 -0.83130836 -0.29405355 0.28033495 0.74339151 1.0283122 0.98082018 0.53352356 0.050878048 -0.70268965][-2.2311995 -1.8189106 -1.5398805 -1.1829123 -0.83521914 -0.45931268 0.14462709 0.73953438 1.4569011 2.0311737 2.2159023 1.9823189 1.2386785 0.23457766 -0.95841908][-2.1920605 -1.7982526 -1.4310725 -0.99914718 -0.55752945 0.0808835 1.0755067 1.9055963 2.6514406 3.1307602 3.1635451 2.6553349 1.5453367 0.1526351 -1.2766857][-1.9936595 -1.6709235 -1.3608239 -0.93009615 -0.45203066 0.22313356 1.1521778 2.1126575 2.9584455 3.374711 3.3333559 2.6601014 1.4095039 -0.23196077 -1.7577333][-1.9213753 -1.8599179 -1.821192 -1.5249119 -1.2222776 -0.68417311 0.17593908 1.0178456 1.7286258 1.9953661 2.0147796 1.4507017 0.35472441 -1.1142936 -2.4764869][-2.3609867 -2.4384086 -2.6095383 -2.6150005 -2.5444822 -2.0977468 -1.3405817 -0.67293549 -0.15622091 -0.035289288 -0.024440765 -0.5509367 -1.4188519 -2.5000901 -3.416729][-2.6838088 -2.908385 -3.2287002 -3.3888259 -3.4917338 -3.2780936 -2.7589946 -2.251555 -1.8529418 -1.8619311 -1.9059398 -2.3505628 -2.9947143 -3.7628825 -4.3399363][-2.7843747 -3.1921175 -3.57974 -3.7455604 -3.8923283 -3.8596456 -3.6863451 -3.3818805 -3.1760659 -3.3384581 -3.4713888 -3.8613048 -4.2647052 -4.79234 -5.075171][-2.9379802 -3.2671704 -3.6113212 -3.7811542 -3.9189844 -3.8972812 -3.8610625 -3.7824512 -3.7676396 -4.0499392 -4.3162289 -4.781424 -5.0662918 -5.3544559 -5.510066][-2.9459925 -3.0921602 -3.2805407 -3.4138355 -3.4801848 -3.4182947 -3.427124 -3.4669018 -3.5935342 -3.9274631 -4.2822981 -4.789876 -5.0986266 -5.2978816 -5.4333348]]...]
INFO - root - 2017-12-16 05:02:49.889486: step 17710, loss = 0.26, batch loss = 0.20 (28.2 examples/sec; 0.283 sec/batch; 24h:46m:58s remains)
INFO - root - 2017-12-16 05:02:52.723574: step 17720, loss = 0.22, batch loss = 0.16 (28.9 examples/sec; 0.277 sec/batch; 24h:12m:40s remains)
INFO - root - 2017-12-16 05:02:55.537088: step 17730, loss = 0.29, batch loss = 0.23 (27.9 examples/sec; 0.287 sec/batch; 25h:05m:31s remains)
INFO - root - 2017-12-16 05:02:58.382252: step 17740, loss = 0.39, batch loss = 0.33 (28.5 examples/sec; 0.281 sec/batch; 24h:34m:13s remains)
INFO - root - 2017-12-16 05:03:01.162856: step 17750, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 24h:17m:59s remains)
INFO - root - 2017-12-16 05:03:04.069598: step 17760, loss = 0.20, batch loss = 0.14 (27.1 examples/sec; 0.295 sec/batch; 25h:46m:17s remains)
INFO - root - 2017-12-16 05:03:06.891756: step 17770, loss = 0.21, batch loss = 0.15 (28.4 examples/sec; 0.281 sec/batch; 24h:35m:10s remains)
INFO - root - 2017-12-16 05:03:09.743446: step 17780, loss = 0.23, batch loss = 0.17 (29.1 examples/sec; 0.275 sec/batch; 24h:04m:04s remains)
INFO - root - 2017-12-16 05:03:12.599316: step 17790, loss = 0.25, batch loss = 0.19 (26.6 examples/sec; 0.301 sec/batch; 26h:17m:34s remains)
INFO - root - 2017-12-16 05:03:15.408479: step 17800, loss = 0.21, batch loss = 0.16 (28.3 examples/sec; 0.283 sec/batch; 24h:43m:54s remains)
2017-12-16 05:03:15.865510: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0464826 -5.2328062 -5.2285166 -5.1843624 -5.0385714 -5.0213475 -4.9558873 -4.9641495 -5.4250145 -6.179317 -6.9389257 -7.788835 -8.7129765 -9.4349766 -9.7637386][-4.7337394 -5.0128279 -5.142415 -5.1981192 -5.1039324 -5.1894326 -4.9975905 -4.6650586 -4.572577 -5.1668072 -6.28061 -7.3254824 -8.24508 -8.8392582 -9.2761316][-4.4985924 -4.7009325 -4.6190581 -4.4827728 -4.2064452 -4.4390225 -4.663774 -4.6352305 -4.553688 -4.7864218 -5.3142414 -6.2287765 -7.3617115 -8.1002741 -8.4418964][-3.3399649 -3.8019569 -3.954206 -3.4950569 -2.8728266 -2.6691442 -2.7340102 -3.158402 -3.5804737 -3.9265265 -4.1624212 -4.70989 -5.8287973 -6.7650661 -7.8936439][-2.4893079 -2.3921523 -2.2295783 -2.0548944 -1.5996439 -1.3077464 -1.1442521 -1.2440207 -1.4084544 -2.1284409 -2.887857 -3.1977348 -3.6562178 -4.64663 -6.3601594][-2.4971516 -1.818383 -0.97186518 0.060112 0.88312578 1.1130128 1.0911436 0.71508169 0.51560926 0.375834 0.24820185 -0.39197254 -1.5619628 -2.4492862 -4.1351442][-2.6093242 -1.7499006 -0.79701018 0.12056017 1.612113 2.7340341 3.0530634 2.6343436 2.4070077 2.096911 1.9395967 1.6828318 0.8898921 -0.68418026 -2.9878869][-3.4048147 -2.5791144 -1.3705471 0.16238117 1.6241517 2.6682463 3.7095013 3.8517246 3.5459833 2.8868895 2.6750064 2.1410265 1.0049109 -0.51284122 -2.2744882][-4.1903443 -3.8611736 -2.9133558 -1.7539532 -0.10446787 1.2287145 2.1370029 2.3800197 2.6247606 2.3913469 2.1799421 1.5824871 0.70296764 -0.55553889 -1.9681633][-4.6253476 -4.4955268 -4.223866 -3.5178154 -2.4314895 -1.4469187 -0.57235861 -0.20916414 -0.12836266 -0.011948109 0.38948202 0.10489416 -0.57096934 -1.4834867 -2.5188389][-5.1718206 -4.99157 -4.7785478 -4.681211 -4.707901 -4.2356982 -3.461143 -3.2827449 -3.1542614 -2.9132359 -2.6594651 -2.7692409 -3.1846228 -3.3282182 -3.812567][-5.6506071 -5.4928865 -5.4811506 -5.5238171 -5.6439133 -5.859839 -6.0576391 -6.0629354 -5.8080053 -5.58584 -5.2692108 -5.2204275 -5.2224607 -5.1304336 -5.2951384][-5.7561216 -5.791831 -5.9913192 -6.2629724 -6.5684671 -6.7181358 -6.7773166 -6.7805071 -6.5696516 -6.4109979 -6.2423148 -6.0990176 -6.0928874 -6.0682507 -5.9417577][-4.5867195 -4.6744986 -4.9428244 -5.4196262 -5.8859468 -6.2257128 -6.4215059 -6.4170904 -6.404582 -6.2861571 -6.0795441 -6.2340207 -6.2702765 -6.1761093 -6.1484971][-4.6912017 -4.4849391 -4.4060478 -4.59005 -4.8507476 -5.1634474 -5.4459748 -5.5257149 -5.4093795 -5.5185461 -5.6186738 -5.5499315 -5.3005476 -5.4091473 -5.5368524]]...]
INFO - root - 2017-12-16 05:03:18.685705: step 17810, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.283 sec/batch; 24h:46m:00s remains)
INFO - root - 2017-12-16 05:03:21.510478: step 17820, loss = 0.22, batch loss = 0.16 (27.9 examples/sec; 0.287 sec/batch; 25h:04m:56s remains)
INFO - root - 2017-12-16 05:03:24.333559: step 17830, loss = 0.25, batch loss = 0.20 (27.7 examples/sec; 0.288 sec/batch; 25h:12m:58s remains)
INFO - root - 2017-12-16 05:03:27.185993: step 17840, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 24h:44m:02s remains)
INFO - root - 2017-12-16 05:03:30.024679: step 17850, loss = 0.27, batch loss = 0.21 (28.9 examples/sec; 0.277 sec/batch; 24h:13m:22s remains)
INFO - root - 2017-12-16 05:03:32.827952: step 17860, loss = 0.33, batch loss = 0.27 (27.8 examples/sec; 0.287 sec/batch; 25h:07m:03s remains)
INFO - root - 2017-12-16 05:03:35.638660: step 17870, loss = 0.26, batch loss = 0.20 (29.4 examples/sec; 0.272 sec/batch; 23h:46m:06s remains)
INFO - root - 2017-12-16 05:03:38.509807: step 17880, loss = 0.25, batch loss = 0.19 (29.2 examples/sec; 0.274 sec/batch; 23h:55m:39s remains)
INFO - root - 2017-12-16 05:03:41.386240: step 17890, loss = 0.23, batch loss = 0.17 (26.8 examples/sec; 0.298 sec/batch; 26h:04m:10s remains)
INFO - root - 2017-12-16 05:03:44.231354: step 17900, loss = 0.29, batch loss = 0.23 (27.5 examples/sec; 0.291 sec/batch; 25h:25m:00s remains)
2017-12-16 05:03:44.695193: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6642942 -4.5437379 -4.3927011 -4.3280187 -4.4027138 -4.4176493 -4.4733057 -4.6506186 -4.8028469 -4.6023245 -4.2691488 -3.9788311 -3.6068583 -3.1619234 -2.9462562][-4.44329 -4.4403305 -4.3805604 -4.325386 -4.3400702 -4.51252 -4.7998476 -5.0011687 -5.0771012 -4.9391403 -4.6185846 -4.1230869 -3.6377518 -3.1884267 -2.9157903][-3.9440441 -3.8830187 -3.7571237 -3.6796546 -3.8464131 -4.080163 -4.3186893 -4.7058449 -4.9379339 -4.8033609 -4.5090146 -4.2361979 -3.9149966 -3.470768 -3.132174][-2.9410062 -2.7394757 -2.5956523 -2.5639431 -2.6647229 -2.8655314 -3.1167135 -3.3823185 -3.5864508 -3.6789696 -3.7719064 -3.8714325 -3.9129457 -3.7391696 -3.4468369][-2.0726457 -1.6588264 -1.0885096 -0.7820878 -0.6802702 -0.74737477 -0.93074584 -1.3136213 -1.7605672 -2.2137179 -2.8285711 -3.4812877 -3.9272828 -4.0357819 -3.7985744][-1.1724234 -0.60470438 0.089660168 0.75034809 1.2633448 1.340837 1.2987328 0.97298527 0.31594658 -0.67053246 -1.9268064 -3.2176969 -4.0568738 -4.34168 -4.190836][-0.94202375 -0.23093748 0.58074903 1.2772493 1.9945874 2.3387198 2.4222913 2.190506 1.6050158 0.46946383 -1.00722 -2.5815077 -3.8289061 -4.3365908 -4.2374845][-1.125576 -0.604156 0.088989258 1.0298848 1.953455 2.430831 2.5640264 2.3320041 1.7787638 0.62562132 -0.90680575 -2.4723496 -3.6029589 -4.1337128 -4.1849608][-2.0177038 -1.3733752 -0.72358704 0.020259857 0.82666874 1.4270873 1.6760926 1.475584 1.0462456 -0.079101562 -1.4436371 -2.8464894 -3.9243231 -4.2720213 -4.3020453][-2.594933 -2.1519063 -1.7875054 -1.2081378 -0.549134 -0.1619072 0.072303772 -0.10565042 -0.43990541 -1.2331853 -2.1932204 -3.185627 -4.0239458 -4.3122587 -4.3103795][-3.096488 -2.850075 -2.7402968 -2.4861588 -2.1042879 -1.8376381 -1.6428258 -1.6160853 -1.6220965 -2.0154231 -2.641108 -3.393961 -4.0701165 -4.2270212 -4.1826482][-3.3978791 -3.1907177 -3.2461734 -3.2624073 -3.219676 -3.0446782 -2.6888237 -2.630156 -2.5088291 -2.609581 -2.9733295 -3.4766054 -3.9926467 -4.1055894 -4.0824142][-2.8771164 -2.643744 -2.7842426 -2.9662776 -2.9810247 -3.0125492 -2.8619452 -2.844027 -2.809072 -2.81633 -2.9689469 -3.3565536 -3.8858113 -3.9367058 -3.8247766][-2.0018613 -1.7403109 -1.9247935 -2.2607203 -2.4956894 -2.6107388 -2.5555034 -2.5599318 -2.4483411 -2.3812544 -2.63627 -3.0637994 -3.5102329 -3.6761947 -3.6113582][-1.2919636 -0.89447784 -1.116154 -1.6347077 -2.003751 -2.1592317 -2.0152557 -1.907789 -1.958472 -1.9521244 -2.2303708 -2.7838256 -3.2474289 -3.4403465 -3.3705325]]...]
INFO - root - 2017-12-16 05:03:47.491985: step 17910, loss = 0.32, batch loss = 0.26 (27.2 examples/sec; 0.294 sec/batch; 25h:43m:35s remains)
INFO - root - 2017-12-16 05:03:50.380642: step 17920, loss = 0.29, batch loss = 0.23 (27.8 examples/sec; 0.288 sec/batch; 25h:10m:10s remains)
INFO - root - 2017-12-16 05:03:53.185279: step 17930, loss = 0.29, batch loss = 0.24 (28.7 examples/sec; 0.279 sec/batch; 24h:20m:26s remains)
INFO - root - 2017-12-16 05:03:56.010037: step 17940, loss = 0.21, batch loss = 0.15 (26.7 examples/sec; 0.300 sec/batch; 26h:12m:14s remains)
INFO - root - 2017-12-16 05:03:58.834986: step 17950, loss = 0.25, batch loss = 0.19 (29.0 examples/sec; 0.275 sec/batch; 24h:03m:53s remains)
INFO - root - 2017-12-16 05:04:01.661230: step 17960, loss = 0.20, batch loss = 0.14 (28.8 examples/sec; 0.278 sec/batch; 24h:18m:20s remains)
INFO - root - 2017-12-16 05:04:04.492810: step 17970, loss = 0.37, batch loss = 0.31 (28.3 examples/sec; 0.282 sec/batch; 24h:39m:37s remains)
INFO - root - 2017-12-16 05:04:07.345536: step 17980, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.282 sec/batch; 24h:40m:26s remains)
INFO - root - 2017-12-16 05:04:10.289354: step 17990, loss = 0.22, batch loss = 0.16 (25.5 examples/sec; 0.313 sec/batch; 27h:22m:52s remains)
INFO - root - 2017-12-16 05:04:13.156112: step 18000, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.283 sec/batch; 24h:45m:42s remains)
2017-12-16 05:04:13.619080: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4321809 -2.7363269 -3.0714164 -3.3288028 -3.4181294 -3.428211 -3.4732261 -3.4898105 -3.4486625 -3.4472547 -3.3679144 -3.5118196 -3.820596 -4.0909643 -4.5297623][-2.3332794 -2.6572475 -2.8987281 -3.1778474 -3.3633847 -3.4540954 -3.4676154 -3.4474876 -3.47918 -3.4921231 -3.5239208 -3.6724987 -3.940958 -4.1629233 -4.4693456][-2.617197 -2.7730708 -2.9126954 -3.0056975 -2.9719546 -3.013885 -3.0650184 -3.1796272 -3.2707942 -3.3702633 -3.4153993 -3.6353362 -3.9849644 -4.185173 -4.3933244][-3.0811546 -2.8674846 -2.7003496 -2.5430574 -2.3613124 -2.3052089 -2.3460622 -2.3547513 -2.3922942 -2.6483822 -2.8335872 -2.9411459 -3.0194793 -3.3377769 -3.8842275][-2.7868605 -2.2711892 -1.8899794 -1.5757678 -1.1136198 -0.80885577 -0.74762511 -0.8976233 -1.0978091 -1.3252804 -1.4136193 -1.6433823 -1.737123 -1.9404645 -2.4730749][-2.398196 -1.6142225 -0.93286848 -0.36242151 0.26237583 0.68430042 0.88881731 0.80806541 0.41113186 -0.14886856 -0.38793707 -0.47062469 -0.42712593 -0.75823808 -1.4156275][-1.4509816 -0.87384844 -0.34617138 0.28986597 1.1588845 1.9490504 2.3952346 2.2483573 1.7731166 1.1122012 0.57094717 0.24839878 0.20587635 -0.19098043 -1.0978801][-0.78701735 -0.43977094 -0.14759111 0.40977907 1.4101062 2.4454961 3.0720253 3.1178808 2.7358418 1.9992914 1.3470368 0.89899492 0.5991354 -0.064181328 -1.2463233][-0.31292772 -0.35510349 -0.29153395 -0.023052692 0.72729397 1.965219 2.7709389 2.8748622 2.4834676 1.7883625 1.1844573 0.62906694 0.22638559 -0.40101767 -1.6066525][-0.21874714 -0.65998745 -1.0490129 -0.79633331 -0.052648544 0.90981865 1.6303058 1.8127542 1.4728956 0.65899324 -0.035757065 -0.5618875 -0.88608718 -1.4025984 -2.38076][-0.12327862 -0.85951781 -1.4832711 -1.563026 -1.1829166 -0.27523565 0.40857792 0.53306675 0.12684727 -0.6216886 -1.4169958 -2.1061985 -2.5188913 -2.8775616 -3.4944644][-0.53752303 -1.3891008 -2.2445886 -2.5439041 -2.172194 -1.4586344 -0.88779688 -0.84081912 -1.112133 -1.7820363 -2.578362 -3.2580004 -3.6875126 -3.9747546 -4.3242216][-0.95986891 -1.6796536 -2.399971 -2.7584467 -2.8548279 -2.3713412 -1.9773192 -1.9436605 -2.2095213 -2.8069849 -3.3497119 -3.8396106 -4.1324906 -4.3245668 -4.5130825][-1.3486056 -1.9019744 -2.360266 -2.5727711 -2.6512487 -2.6027699 -2.5094519 -2.6433988 -2.9395621 -3.4700427 -3.9269991 -4.1800408 -4.2343807 -4.2459168 -4.2381854][-1.9877632 -2.2167766 -2.3875029 -2.4883928 -2.4112711 -2.3634417 -2.4085722 -2.6929235 -3.0397062 -3.5597465 -3.9523106 -4.0622058 -3.9354162 -3.801126 -3.69971]]...]
INFO - root - 2017-12-16 05:04:16.485757: step 18010, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.284 sec/batch; 24h:50m:41s remains)
INFO - root - 2017-12-16 05:04:19.319934: step 18020, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 24h:37m:11s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 05:04:22.087378: step 18030, loss = 0.31, batch loss = 0.25 (28.7 examples/sec; 0.278 sec/batch; 24h:18m:29s remains)
INFO - root - 2017-12-16 05:04:24.943854: step 18040, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.280 sec/batch; 24h:28m:38s remains)
INFO - root - 2017-12-16 05:04:27.752962: step 18050, loss = 0.19, batch loss = 0.13 (28.3 examples/sec; 0.282 sec/batch; 24h:39m:44s remains)
INFO - root - 2017-12-16 05:04:30.567311: step 18060, loss = 0.24, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 23h:56m:08s remains)
INFO - root - 2017-12-16 05:04:33.374375: step 18070, loss = 0.22, batch loss = 0.16 (29.1 examples/sec; 0.275 sec/batch; 24h:01m:17s remains)
INFO - root - 2017-12-16 05:04:36.196417: step 18080, loss = 0.26, batch loss = 0.20 (29.2 examples/sec; 0.274 sec/batch; 23h:53m:40s remains)
INFO - root - 2017-12-16 05:04:39.034362: step 18090, loss = 0.28, batch loss = 0.23 (27.5 examples/sec; 0.291 sec/batch; 25h:27m:09s remains)
INFO - root - 2017-12-16 05:04:41.905875: step 18100, loss = 0.24, batch loss = 0.18 (28.9 examples/sec; 0.277 sec/batch; 24h:10m:31s remains)
2017-12-16 05:04:42.369635: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6449454 -2.6106966 -2.286097 -2.0000989 -1.8763602 -1.9893634 -1.9789515 -2.17585 -2.7910514 -3.524735 -3.9887283 -4.3610744 -4.6764879 -4.6823378 -4.5140657][-1.8350525 -1.7166359 -1.5707119 -1.5043097 -1.5847194 -1.7249863 -1.9735487 -2.516583 -3.2577693 -4.0434217 -4.6810188 -4.9739571 -5.1160674 -4.9122596 -4.6591048][-1.8005967 -1.8289998 -1.8469598 -1.7838523 -1.7476716 -1.8170373 -2.0926082 -2.7893362 -3.6662188 -4.5056348 -5.2836957 -5.6722527 -5.6503568 -5.1948261 -4.7676334][-2.3052301 -2.4876902 -2.4876752 -2.3535755 -2.1273897 -2.0726101 -2.198091 -2.7914581 -3.6270695 -4.5034876 -5.2731419 -5.6779284 -5.783453 -5.4035034 -4.9043574][-3.0255046 -3.335731 -3.3741953 -2.9382954 -2.3492279 -1.7802398 -1.4716816 -1.8636038 -2.668623 -3.5438762 -4.3090868 -4.838295 -5.0698309 -4.9439526 -4.6838369][-4.257565 -4.396524 -4.1012158 -3.2312391 -2.1506214 -1.1950047 -0.47156644 -0.39194775 -0.84800267 -1.7057707 -2.6317973 -3.4490318 -4.0370793 -4.2635775 -4.342968][-4.8968472 -4.958591 -4.2869945 -2.9972258 -1.5976093 -0.30191422 0.78330183 1.1479759 0.90289259 0.16891098 -0.8064363 -1.7418678 -2.62752 -3.2839575 -3.759706][-5.0608225 -4.9867396 -4.1705451 -2.6430473 -0.90545774 0.61535358 1.7641859 2.0619364 1.7820253 1.1523595 0.2568593 -0.73154092 -1.8466043 -2.6077275 -3.1562424][-4.7162838 -4.4511781 -3.8017092 -2.5334992 -1.315007 -0.0045108795 1.4306207 1.8571467 1.5546331 0.67022991 -0.34850836 -1.2348254 -2.2049956 -2.8615198 -3.3149533][-4.2515168 -3.9980164 -3.5264533 -2.6646307 -1.6374621 -0.66287541 0.0015740395 -0.099681377 -0.31661749 -1.004945 -1.9812393 -2.9047418 -3.6731884 -3.8825872 -3.899416][-3.4734659 -3.4318869 -3.1600633 -2.7083144 -2.3391693 -1.7080371 -0.99867892 -1.2281077 -2.0418918 -3.1264613 -4.0353165 -4.6432824 -5.0717912 -4.905479 -4.5938339][-3.1263011 -2.9714499 -2.9590821 -3.0298371 -3.0264769 -2.8076396 -2.6744893 -3.1362035 -3.8183191 -4.5499568 -5.2859831 -5.925797 -6.2006559 -5.7870131 -5.1901603][-2.2050877 -2.0999362 -2.2244177 -2.2642097 -2.4534311 -2.7708735 -2.9963188 -3.677242 -4.5003066 -5.29261 -5.9749384 -6.3656468 -6.4601851 -6.0381627 -5.5204296][-1.5592785 -1.1779933 -1.1451597 -1.4178162 -1.7748802 -1.9656229 -2.2023277 -3.2262921 -4.1412354 -4.6538029 -5.0504541 -5.5849085 -5.8556595 -5.4219413 -4.9881845][-0.73268819 -0.3383131 -0.17931318 -0.26577902 -0.6694181 -0.97258568 -1.3461759 -2.1580927 -3.0203459 -3.8051515 -4.3066573 -4.5773287 -4.6726303 -4.440742 -4.1050458]]...]
INFO - root - 2017-12-16 05:04:45.223448: step 18110, loss = 0.31, batch loss = 0.25 (27.6 examples/sec; 0.290 sec/batch; 25h:20m:49s remains)
INFO - root - 2017-12-16 05:04:48.033506: step 18120, loss = 0.30, batch loss = 0.24 (28.6 examples/sec; 0.280 sec/batch; 24h:25m:04s remains)
INFO - root - 2017-12-16 05:04:50.867155: step 18130, loss = 0.35, batch loss = 0.29 (28.2 examples/sec; 0.284 sec/batch; 24h:47m:51s remains)
INFO - root - 2017-12-16 05:04:53.721259: step 18140, loss = 0.27, batch loss = 0.21 (27.7 examples/sec; 0.289 sec/batch; 25h:12m:13s remains)
INFO - root - 2017-12-16 05:04:56.577005: step 18150, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.286 sec/batch; 24h:59m:21s remains)
INFO - root - 2017-12-16 05:04:59.386935: step 18160, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.279 sec/batch; 24h:22m:06s remains)
INFO - root - 2017-12-16 05:05:02.222410: step 18170, loss = 0.21, batch loss = 0.15 (27.7 examples/sec; 0.289 sec/batch; 25h:15m:35s remains)
INFO - root - 2017-12-16 05:05:05.022403: step 18180, loss = 0.22, batch loss = 0.16 (28.4 examples/sec; 0.282 sec/batch; 24h:34m:53s remains)
INFO - root - 2017-12-16 05:05:07.858921: step 18190, loss = 0.30, batch loss = 0.24 (28.0 examples/sec; 0.285 sec/batch; 24h:55m:15s remains)
INFO - root - 2017-12-16 05:05:10.727622: step 18200, loss = 0.27, batch loss = 0.21 (27.3 examples/sec; 0.293 sec/batch; 25h:33m:31s remains)
2017-12-16 05:05:11.184139: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3657217 -5.6580849 -5.0417519 -4.7158165 -4.6669884 -4.8918123 -5.0358315 -5.3255548 -5.6232185 -5.9244075 -6.3354068 -6.6440568 -6.8891382 -6.8236237 -6.6683717][-6.6684341 -5.7788186 -5.1363592 -4.8464894 -4.8260384 -4.9674888 -5.0842094 -5.3216443 -5.6633425 -6.0226574 -6.6517057 -6.9795256 -7.074234 -6.8545737 -6.6600685][-6.4293103 -5.425837 -4.6675324 -4.3435731 -4.2669888 -4.4782863 -4.4924607 -4.7604222 -5.1859288 -5.6245461 -6.3184404 -6.7280331 -6.8045492 -6.2404528 -5.8734636][-5.8994055 -4.721293 -3.7460368 -3.2026129 -2.8603439 -2.9058318 -3.0249982 -3.3989925 -3.96894 -4.6270032 -5.3312044 -5.6266522 -5.6876 -5.2759628 -4.9726877][-4.85842 -3.4482169 -2.1091762 -1.2222848 -0.67763114 -0.69965744 -0.87309289 -1.355607 -2.124501 -3.0629377 -4.0706587 -4.5786123 -4.705853 -4.3737869 -4.2312846][-3.8513405 -2.1818802 -0.63173938 0.65738916 1.6288767 1.7407918 1.5074415 0.85073996 -0.092702389 -1.4297001 -2.7251425 -3.4172988 -3.6020322 -3.6167207 -3.5167956][-3.0875769 -1.6082006 -0.21701908 1.39995 2.6805615 3.0786428 3.052618 2.2122421 0.98978329 -0.41788626 -1.7510352 -2.715529 -3.2161889 -3.3512619 -3.6226296][-3.2277584 -1.6745262 -0.25640726 1.1185007 2.4205546 2.9910316 2.9451451 2.2323303 1.2820177 -0.088225842 -1.4549763 -2.3124726 -2.9889112 -3.4954047 -4.0714083][-4.1975708 -2.8418012 -1.5009005 -0.061113358 1.0685744 1.6083012 1.6863322 1.3757014 0.722805 -0.41978979 -1.6056814 -2.4179118 -3.137074 -3.7821243 -4.4839392][-5.625855 -4.5493827 -3.4870737 -2.2786272 -1.302387 -0.66406655 -0.55090213 -0.646961 -0.86609077 -1.5573142 -2.3511894 -3.0143013 -3.6860845 -4.3683367 -5.1099157][-7.46898 -6.5703363 -5.7175083 -4.3076606 -3.3444161 -2.8859177 -2.6717567 -2.5693603 -2.4614086 -2.6063452 -2.939935 -3.4471097 -4.0078869 -4.8045812 -5.7429771][-8.6049013 -7.7960072 -6.9222584 -5.9459386 -5.3066964 -4.7328086 -4.4609804 -4.1391258 -3.7238879 -3.507206 -3.5353737 -3.9014328 -4.4577432 -5.3742461 -6.2135544][-8.8146706 -8.0274744 -7.3889065 -6.5938339 -6.1411705 -5.9411421 -5.8293018 -5.4401741 -4.9120007 -4.3775225 -4.1034651 -4.3597193 -4.8603091 -5.69615 -6.5432673][-8.59478 -7.535234 -6.7392044 -6.313736 -6.2444229 -6.1148138 -6.2187233 -6.09976 -5.7619057 -5.1744175 -4.7221947 -4.763267 -5.0965738 -5.8844309 -6.5624933][-7.5585318 -6.470005 -5.8170347 -5.6338758 -5.7898154 -5.9480667 -6.1221294 -6.1511822 -6.0650239 -5.6196275 -5.2427816 -5.3484421 -5.4876804 -5.8120146 -6.0870123]]...]
INFO - root - 2017-12-16 05:05:13.976712: step 18210, loss = 0.31, batch loss = 0.25 (27.9 examples/sec; 0.287 sec/batch; 25h:03m:38s remains)
INFO - root - 2017-12-16 05:05:16.800734: step 18220, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.278 sec/batch; 24h:18m:22s remains)
INFO - root - 2017-12-16 05:05:19.626695: step 18230, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.285 sec/batch; 24h:55m:08s remains)
INFO - root - 2017-12-16 05:05:22.483268: step 18240, loss = 0.34, batch loss = 0.29 (27.9 examples/sec; 0.287 sec/batch; 25h:02m:51s remains)
INFO - root - 2017-12-16 05:05:25.344168: step 18250, loss = 0.22, batch loss = 0.17 (26.7 examples/sec; 0.300 sec/batch; 26h:10m:25s remains)
INFO - root - 2017-12-16 05:05:28.188206: step 18260, loss = 0.22, batch loss = 0.16 (27.2 examples/sec; 0.294 sec/batch; 25h:42m:23s remains)
INFO - root - 2017-12-16 05:05:31.013282: step 18270, loss = 0.32, batch loss = 0.26 (28.5 examples/sec; 0.281 sec/batch; 24h:31m:47s remains)
INFO - root - 2017-12-16 05:05:33.844078: step 18280, loss = 0.26, batch loss = 0.21 (26.8 examples/sec; 0.298 sec/batch; 26h:00m:36s remains)
INFO - root - 2017-12-16 05:05:36.645879: step 18290, loss = 0.30, batch loss = 0.24 (29.3 examples/sec; 0.273 sec/batch; 23h:48m:38s remains)
INFO - root - 2017-12-16 05:05:39.473161: step 18300, loss = 0.24, batch loss = 0.18 (29.4 examples/sec; 0.272 sec/batch; 23h:45m:51s remains)
2017-12-16 05:05:39.922041: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.605787 -3.871419 -4.1271219 -4.1993227 -4.18857 -4.0067677 -3.803098 -3.4997735 -3.2549758 -3.0361581 -2.7773666 -2.7857692 -3.0802505 -3.0949879 -3.1653044][-3.8235559 -4.1829433 -4.4728026 -4.5599957 -4.5737138 -4.3841071 -4.0870752 -3.7307668 -3.5161021 -3.20029 -2.85357 -2.6411076 -2.6851621 -2.8024247 -3.105948][-4.1633959 -4.6925783 -5.1444993 -5.0568023 -4.7342353 -4.5425463 -4.3526273 -4.0287247 -3.6533046 -3.3274655 -3.1384788 -2.8846521 -2.8025196 -2.9325614 -3.2459176][-4.382369 -4.8313241 -5.1150422 -5.053369 -4.7271576 -4.0724125 -3.3979557 -3.0424261 -2.9676533 -3.0128798 -3.0623326 -3.1913822 -3.5065603 -3.64111 -3.8097396][-4.0332165 -4.545558 -4.7239566 -4.267611 -3.5018127 -2.6152806 -1.9500701 -1.4991574 -1.4365931 -1.758498 -2.22859 -2.7522352 -3.1536188 -3.7260234 -4.2237258][-3.4942102 -3.7475 -3.5833685 -3.0149455 -2.0950139 -1.1149881 -0.28091049 0.4301734 0.47671461 -0.0028409958 -0.75716615 -1.750906 -2.6624978 -3.281044 -3.6590838][-2.2474637 -2.4328737 -2.3603137 -1.6816683 -0.53537846 0.7615819 1.9163337 2.5067043 2.52906 1.8972793 0.87058592 -0.44766188 -1.6940072 -2.66319 -3.4051833][-1.412955 -1.4130969 -1.2902019 -0.74138165 0.19945002 1.6363091 3.13771 3.9491091 3.8726606 3.0495262 1.8540583 0.41464329 -0.98919344 -2.0728836 -2.8598423][-0.731868 -0.75405741 -0.79320621 -0.47426438 0.06044817 1.2037692 2.5750246 3.4995341 3.618578 2.8076787 1.6279311 0.25759506 -0.83019519 -1.670156 -2.5126066][-0.10686922 -0.37247086 -0.80116487 -0.8548336 -0.3810606 0.55729437 1.4368477 2.014039 2.073771 1.4009519 0.38760138 -0.72244406 -1.4256656 -1.8695905 -2.2708166][-0.39118624 -0.37061787 -0.77653909 -1.3393741 -1.5094609 -0.90789628 0.1452899 0.6306963 0.32057524 -0.35047245 -1.0507295 -1.7055163 -2.0231113 -2.0472102 -2.1313868][-1.1363976 -1.3643699 -1.787621 -2.19547 -2.2632616 -2.2291868 -1.9891527 -1.6094799 -1.6052239 -1.9748528 -2.5692613 -2.8297088 -2.5481477 -2.1259434 -1.9434187][-1.6977603 -2.0009062 -2.3842072 -2.840543 -3.1247501 -3.1372285 -2.8994024 -2.9098485 -3.2508633 -3.3030844 -3.2201633 -3.0892005 -2.7246723 -2.1501541 -1.8605726][-2.0733109 -2.1439476 -2.5870404 -3.0263898 -3.1559258 -3.2843466 -3.4294546 -3.3781471 -3.4688573 -3.7252154 -3.9284632 -3.4631486 -2.6141233 -1.8547127 -1.6125243][-2.9836414 -2.7688103 -2.8852854 -3.2161417 -3.4597449 -3.5779383 -3.6886878 -3.8692279 -4.0939631 -3.980175 -3.75015 -3.4566092 -2.7608166 -1.8806865 -1.3528645]]...]
INFO - root - 2017-12-16 05:05:42.764957: step 18310, loss = 0.23, batch loss = 0.17 (29.4 examples/sec; 0.272 sec/batch; 23h:42m:49s remains)
INFO - root - 2017-12-16 05:05:45.608179: step 18320, loss = 0.23, batch loss = 0.17 (29.1 examples/sec; 0.275 sec/batch; 24h:01m:08s remains)
INFO - root - 2017-12-16 05:05:48.414997: step 18330, loss = 0.30, batch loss = 0.24 (28.2 examples/sec; 0.284 sec/batch; 24h:46m:42s remains)
INFO - root - 2017-12-16 05:05:51.295916: step 18340, loss = 0.24, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 24h:41m:16s remains)
INFO - root - 2017-12-16 05:05:54.145969: step 18350, loss = 0.26, batch loss = 0.20 (29.5 examples/sec; 0.271 sec/batch; 23h:39m:24s remains)
INFO - root - 2017-12-16 05:05:56.976579: step 18360, loss = 0.51, batch loss = 0.45 (27.9 examples/sec; 0.287 sec/batch; 25h:00m:21s remains)
INFO - root - 2017-12-16 05:05:59.781288: step 18370, loss = 0.20, batch loss = 0.14 (29.5 examples/sec; 0.271 sec/batch; 23h:37m:40s remains)
INFO - root - 2017-12-16 05:06:02.629378: step 18380, loss = 0.23, batch loss = 0.17 (27.5 examples/sec; 0.291 sec/batch; 25h:22m:02s remains)
INFO - root - 2017-12-16 05:06:05.465882: step 18390, loss = 0.21, batch loss = 0.16 (27.1 examples/sec; 0.295 sec/batch; 25h:46m:34s remains)
INFO - root - 2017-12-16 05:06:08.373733: step 18400, loss = 0.24, batch loss = 0.18 (27.8 examples/sec; 0.288 sec/batch; 25h:07m:32s remains)
2017-12-16 05:06:08.847465: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2139368 -2.4591799 -3.0614033 -3.7300477 -4.3012795 -4.3748622 -4.5603762 -4.9763994 -5.5285006 -5.64338 -5.7283859 -5.8612118 -5.7936525 -5.4697495 -4.9297237][-1.7076077 -2.0864773 -2.5603437 -2.9867568 -3.5247424 -3.8424854 -4.0495291 -4.0237689 -4.0990262 -4.4527426 -4.8316989 -4.9928427 -4.9333472 -4.9288249 -4.6523829][-1.2796321 -1.7002389 -2.4092839 -3.0957108 -3.5427313 -3.4083014 -3.0497065 -2.8567948 -2.8716068 -2.9185734 -3.0947251 -3.6210308 -4.0948973 -4.4708934 -4.4224505][-1.7039614 -2.3512058 -3.1569529 -3.8350697 -3.9512327 -3.4753761 -2.6539791 -1.5068159 -0.74438763 -0.88141012 -1.5503802 -2.5646791 -3.6159992 -4.2403603 -4.3838243][-2.8039038 -3.5889978 -4.3498979 -4.8243203 -4.5320492 -3.2379961 -1.4965382 0.015426159 0.92210484 0.88061094 -0.03125 -1.6750271 -3.2662749 -4.1974382 -4.3645511][-3.2880788 -3.886616 -4.4386806 -4.6822643 -3.9589238 -2.2275457 -0.093675137 1.9243083 3.1121855 2.8933949 1.3151031 -0.78133368 -2.5545936 -3.7371283 -4.1764684][-3.6831081 -4.0742116 -4.2064033 -3.9507923 -2.8922968 -0.695132 1.9236112 4.0102911 5.0014963 4.5237131 2.723073 0.055708885 -2.1825707 -3.2146344 -3.1413083][-3.6233194 -4.0259142 -4.2475257 -3.6851885 -2.1946175 0.10898399 2.5868702 4.5884867 5.2755337 4.5497379 2.7214532 0.30819798 -1.8507581 -2.939527 -2.6487408][-3.7254376 -3.6960566 -3.3485141 -3.0392685 -1.8729184 0.3888092 2.3549619 3.6149006 3.9358721 3.1583409 1.5761795 -0.50132704 -2.031558 -3.1003366 -3.0423672][-3.2339454 -3.6373115 -3.7974358 -3.2019291 -2.1084402 -0.750412 0.66697168 1.7767191 1.7169089 0.83864975 -0.030677795 -1.336159 -2.546371 -3.0666432 -2.6677294][-3.813354 -3.8760343 -4.0711641 -4.4597297 -3.995805 -2.7879467 -1.915513 -1.6981728 -1.7079494 -1.9221663 -2.2361836 -2.6498554 -3.0631282 -3.6163659 -3.4430397][-4.5144472 -5.1680889 -5.4782877 -5.5904489 -5.2879038 -4.6735082 -4.142128 -4.0436754 -4.17652 -4.0516644 -3.5350323 -3.4301655 -3.5725682 -3.9651041 -4.0346408][-4.39327 -4.9513121 -5.3590403 -5.8616638 -6.0291452 -5.7850909 -5.5175576 -5.352572 -5.2833123 -5.1669745 -4.5912232 -3.9322052 -3.5373127 -3.6392021 -3.9515097][-4.2610126 -4.7595673 -5.0762177 -5.4052868 -5.61615 -5.9251947 -6.038475 -5.8839521 -5.6215763 -5.0496082 -4.1886573 -3.8621132 -3.966001 -4.2498503 -4.4604435][-3.4756327 -4.0656323 -4.5386353 -4.9183564 -5.1950336 -5.3492575 -5.256598 -4.9587903 -4.4601097 -3.8437805 -3.1243024 -2.5750883 -2.6788106 -3.4744158 -4.2115145]]...]
INFO - root - 2017-12-16 05:06:11.668794: step 18410, loss = 0.29, batch loss = 0.24 (29.4 examples/sec; 0.272 sec/batch; 23h:42m:41s remains)
INFO - root - 2017-12-16 05:06:14.472106: step 18420, loss = 0.43, batch loss = 0.38 (29.1 examples/sec; 0.275 sec/batch; 24h:00m:44s remains)
INFO - root - 2017-12-16 05:06:17.354333: step 18430, loss = 0.25, batch loss = 0.20 (28.8 examples/sec; 0.278 sec/batch; 24h:16m:00s remains)
INFO - root - 2017-12-16 05:06:20.170082: step 18440, loss = 0.24, batch loss = 0.19 (26.9 examples/sec; 0.298 sec/batch; 25h:57m:44s remains)
INFO - root - 2017-12-16 05:06:23.006191: step 18450, loss = 0.27, batch loss = 0.21 (28.7 examples/sec; 0.279 sec/batch; 24h:18m:37s remains)
INFO - root - 2017-12-16 05:06:25.870318: step 18460, loss = 0.41, batch loss = 0.35 (28.0 examples/sec; 0.286 sec/batch; 24h:55m:52s remains)
INFO - root - 2017-12-16 05:06:28.661356: step 18470, loss = 0.25, batch loss = 0.19 (28.9 examples/sec; 0.277 sec/batch; 24h:09m:39s remains)
INFO - root - 2017-12-16 05:06:31.533593: step 18480, loss = 0.26, batch loss = 0.20 (28.8 examples/sec; 0.278 sec/batch; 24h:13m:55s remains)
INFO - root - 2017-12-16 05:06:34.373980: step 18490, loss = 0.25, batch loss = 0.19 (28.7 examples/sec; 0.279 sec/batch; 24h:19m:58s remains)
INFO - root - 2017-12-16 05:06:37.205430: step 18500, loss = 0.30, batch loss = 0.24 (28.0 examples/sec; 0.286 sec/batch; 24h:54m:45s remains)
2017-12-16 05:06:37.677077: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5979023 -6.1884365 -6.7498665 -7.1356015 -7.2902975 -7.291853 -7.257791 -7.2769737 -7.3726721 -7.4700413 -7.2197313 -6.9577208 -6.5310688 -5.8517118 -5.085175][-6.8611441 -7.3590031 -7.6214137 -7.7149382 -7.760747 -7.9988642 -8.2617779 -8.5279408 -8.7577457 -9.0183353 -9.00477 -8.7544909 -8.0632763 -7.3009405 -6.4606485][-7.3098164 -7.5768447 -7.7033277 -7.3876934 -7.14819 -7.1600666 -7.3655858 -8.2752867 -9.1442366 -9.7536383 -9.9824619 -9.8753147 -9.52376 -8.8860989 -8.0882483][-7.338912 -7.1958184 -6.665905 -6.0930338 -5.4490762 -5.0795479 -5.0848 -5.8905845 -7.03123 -8.690444 -9.6886063 -9.8403854 -9.73719 -9.5716267 -9.2729969][-6.6663761 -5.767375 -4.3224845 -2.901114 -1.5826395 -0.98945665 -1.1926589 -1.9380417 -3.1418445 -5.031333 -6.8722563 -8.53838 -9.5243511 -9.8648663 -9.9534311][-5.4319229 -4.11362 -2.2416356 0.12222004 2.265048 3.5661387 3.899045 2.8520799 1.1586876 -0.98529029 -3.2842169 -5.5882845 -7.72429 -9.2470207 -9.8826675][-5.1368589 -3.7903676 -1.4366908 1.426991 4.2001562 5.908473 6.6110373 5.8784761 4.3694277 2.0991902 -0.53587031 -3.3521171 -5.9326291 -7.7072735 -8.62835][-5.7043934 -4.2084436 -2.1245692 0.64387083 3.9730892 6.4070845 7.472043 6.9362583 5.5291548 3.1861963 0.56421518 -2.1813595 -4.7029486 -6.5890379 -7.4426532][-6.6389427 -5.4611955 -3.599766 -0.78008938 2.0767241 4.3994179 6.2229214 6.3038359 5.3128281 3.1185522 0.60137177 -2.1573522 -4.7082977 -6.2496481 -6.4903479][-7.5756226 -6.9453106 -5.6586118 -3.769021 -1.4999621 0.82824087 2.5866385 2.8613029 2.4949327 0.83579588 -1.1295817 -3.4086823 -5.3823605 -6.40867 -6.6748919][-8.2933369 -8.0966854 -7.16352 -5.95785 -4.6839771 -3.2783906 -1.4577715 -0.56878829 -0.70569324 -2.1138444 -3.4384687 -4.8431764 -6.1602588 -6.938488 -7.1854019][-8.3697 -8.2581844 -8.031559 -7.1758385 -6.0062661 -5.1852183 -4.1218824 -3.7102702 -3.3894885 -3.8661761 -5.0107994 -6.4873791 -7.4170809 -7.432951 -7.2464828][-7.25183 -7.1934719 -7.2954822 -6.8505678 -6.3704109 -5.5469265 -4.6594415 -4.714416 -4.6043472 -4.9608412 -5.49098 -6.3824706 -7.1141682 -7.2267447 -7.1533756][-5.7744823 -5.783371 -5.7578645 -5.632483 -5.5407176 -4.9716177 -4.4279318 -4.5984936 -4.7637959 -4.9412055 -5.0191541 -5.4924688 -6.0836148 -6.1278553 -5.8823338][-4.4459319 -4.2026558 -4.0061097 -3.9320462 -3.9122412 -3.7954705 -3.7141514 -3.7871208 -3.8368075 -4.1065836 -4.2249231 -4.2455935 -4.3739281 -4.5509419 -4.6729193]]...]
INFO - root - 2017-12-16 05:06:40.507387: step 18510, loss = 0.25, batch loss = 0.19 (29.1 examples/sec; 0.275 sec/batch; 23h:56m:32s remains)
INFO - root - 2017-12-16 05:06:43.363892: step 18520, loss = 0.21, batch loss = 0.16 (29.4 examples/sec; 0.272 sec/batch; 23h:44m:23s remains)
INFO - root - 2017-12-16 05:06:46.212706: step 18530, loss = 0.29, batch loss = 0.23 (26.8 examples/sec; 0.298 sec/batch; 26h:01m:01s remains)
INFO - root - 2017-12-16 05:06:49.032256: step 18540, loss = 0.28, batch loss = 0.22 (27.8 examples/sec; 0.288 sec/batch; 25h:05m:05s remains)
INFO - root - 2017-12-16 05:06:51.881768: step 18550, loss = 0.29, batch loss = 0.23 (28.3 examples/sec; 0.282 sec/batch; 24h:36m:38s remains)
INFO - root - 2017-12-16 05:06:54.801400: step 18560, loss = 0.26, batch loss = 0.20 (27.7 examples/sec; 0.289 sec/batch; 25h:10m:52s remains)
INFO - root - 2017-12-16 05:06:57.664634: step 18570, loss = 0.27, batch loss = 0.21 (27.0 examples/sec; 0.297 sec/batch; 25h:52m:24s remains)
INFO - root - 2017-12-16 05:07:00.501297: step 18580, loss = 0.38, batch loss = 0.32 (29.0 examples/sec; 0.276 sec/batch; 24h:03m:59s remains)
INFO - root - 2017-12-16 05:07:03.348713: step 18590, loss = 0.25, batch loss = 0.20 (28.8 examples/sec; 0.278 sec/batch; 24h:12m:58s remains)
INFO - root - 2017-12-16 05:07:06.167923: step 18600, loss = 0.21, batch loss = 0.15 (27.0 examples/sec; 0.296 sec/batch; 25h:48m:14s remains)
2017-12-16 05:07:06.651853: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2351007 -4.5863223 -4.8287683 -4.9274673 -4.9211855 -4.8592548 -4.8132548 -4.8709335 -5.0088577 -5.1615925 -5.2930541 -5.3911462 -5.4001489 -5.2236261 -4.9127474][-4.377666 -4.7296839 -4.9104009 -4.97356 -4.9727049 -5.0170712 -5.0791578 -5.0891638 -5.2353625 -5.5658345 -5.9653468 -6.1925225 -6.2462149 -6.1283431 -5.8262019][-4.4555721 -4.7666521 -4.7756243 -4.5674872 -4.3225965 -4.2062893 -4.2428117 -4.4316974 -4.7346411 -5.1681619 -5.7056141 -6.287322 -6.7591467 -6.9132314 -6.7837191][-4.5595903 -4.6618176 -4.3565426 -3.7590427 -3.1671109 -2.7899141 -2.59823 -2.6560917 -2.9982786 -3.777739 -4.6248608 -5.5343323 -6.3645697 -7.06259 -7.4251332][-4.6661081 -4.3955169 -3.6722574 -2.701386 -1.7112713 -0.91665554 -0.37408829 -0.23116875 -0.3780055 -1.2079616 -2.3571732 -3.89784 -5.3171234 -6.5655956 -7.4287443][-4.7525611 -4.2146654 -3.1262045 -1.6874826 -0.23469496 0.95202875 1.8958688 2.3471432 2.306201 1.5062547 0.27362108 -1.6886871 -3.8048227 -5.7268391 -6.9156528][-5.0611649 -4.3486958 -3.0278566 -1.1675658 0.75269222 2.4066072 3.6344662 4.2910786 4.2890997 3.4232788 2.0242743 -0.25265074 -2.524689 -4.7895174 -6.2544956][-5.6408892 -4.7303715 -3.332279 -1.3408382 0.82093 2.7527585 4.2903881 5.0348415 4.9872561 3.9693251 2.502708 0.19560337 -2.2311549 -4.5939856 -5.9874129][-6.4161162 -5.5931177 -4.1190963 -2.0823419 -0.12853909 1.6096787 3.2756696 4.1551094 4.1391058 3.1690164 1.65133 -0.56366611 -2.896718 -4.9515486 -6.1452007][-6.9972382 -6.2333574 -5.2128143 -3.6101518 -1.9264603 -0.27326202 1.2073193 1.9977636 2.1757607 1.4284678 0.11827755 -1.9951661 -4.0786819 -5.9128876 -6.8589096][-7.1588554 -6.6803246 -5.8797464 -4.4993725 -3.1740942 -1.9706483 -0.83161592 -0.13241529 -0.17428637 -0.85046768 -1.843132 -3.4930916 -5.2035427 -6.740396 -7.468092][-6.9813209 -6.3674006 -5.7213297 -4.9853029 -4.1591659 -3.2062902 -2.5332265 -2.0537317 -2.1576126 -2.6203313 -3.458776 -4.8392324 -6.0765724 -7.0704541 -7.5230532][-6.3921995 -5.8001571 -5.2457538 -4.843884 -4.5488873 -4.0088854 -3.5812931 -3.21193 -3.2575338 -3.6003885 -4.2062078 -5.1919179 -6.1370125 -7.0354729 -7.406877][-5.037921 -4.74473 -4.4631696 -4.2527637 -4.037055 -3.7842937 -3.741271 -3.667136 -3.8044546 -3.9591973 -4.300878 -5.0352507 -5.7170258 -6.2921481 -6.3797517][-4.2720647 -3.7566416 -3.3519211 -3.4143944 -3.5144005 -3.4762726 -3.421515 -3.4390285 -3.6440439 -3.7926295 -4.0783372 -4.3147793 -4.4425616 -4.7083335 -4.8652835]]...]
INFO - root - 2017-12-16 05:07:09.454188: step 18610, loss = 0.23, batch loss = 0.18 (29.4 examples/sec; 0.272 sec/batch; 23h:44m:50s remains)
INFO - root - 2017-12-16 05:07:12.291578: step 18620, loss = 0.24, batch loss = 0.18 (28.5 examples/sec; 0.281 sec/batch; 24h:30m:58s remains)
INFO - root - 2017-12-16 05:07:15.147960: step 18630, loss = 0.25, batch loss = 0.19 (27.1 examples/sec; 0.295 sec/batch; 25h:45m:33s remains)
INFO - root - 2017-12-16 05:07:17.954868: step 18640, loss = 0.23, batch loss = 0.17 (28.6 examples/sec; 0.279 sec/batch; 24h:21m:57s remains)
INFO - root - 2017-12-16 05:07:20.815076: step 18650, loss = 0.25, batch loss = 0.19 (28.3 examples/sec; 0.283 sec/batch; 24h:38m:34s remains)
INFO - root - 2017-12-16 05:07:23.610480: step 18660, loss = 0.26, batch loss = 0.20 (27.7 examples/sec; 0.289 sec/batch; 25h:11m:04s remains)
INFO - root - 2017-12-16 05:07:26.441451: step 18670, loss = 0.26, batch loss = 0.20 (29.5 examples/sec; 0.271 sec/batch; 23h:38m:52s remains)
INFO - root - 2017-12-16 05:07:29.284396: step 18680, loss = 0.22, batch loss = 0.16 (25.9 examples/sec; 0.309 sec/batch; 26h:56m:06s remains)
INFO - root - 2017-12-16 05:07:32.125802: step 18690, loss = 0.23, batch loss = 0.17 (27.9 examples/sec; 0.287 sec/batch; 24h:58m:34s remains)
INFO - root - 2017-12-16 05:07:35.015332: step 18700, loss = 0.27, batch loss = 0.21 (26.8 examples/sec; 0.298 sec/batch; 25h:58m:38s remains)
2017-12-16 05:07:35.481364: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3478389 -3.3841782 -3.4536457 -3.4686177 -3.457629 -3.4122975 -3.3754761 -3.4032714 -3.4817319 -3.567102 -3.90478 -4.4004188 -4.9117742 -5.3079495 -5.4996362][-3.6752019 -3.8236594 -3.8407967 -3.7561667 -3.6255469 -3.614691 -3.5685894 -3.4190712 -3.355207 -3.3932157 -3.8235846 -4.428946 -5.1126842 -5.5762963 -5.8073082][-4.0869493 -4.26117 -4.2507653 -4.0006986 -3.6258254 -3.3280413 -3.0155163 -2.9587107 -2.9202425 -3.2796535 -3.9362707 -4.5477757 -5.424118 -5.9340448 -6.2930803][-4.2688627 -4.4514408 -4.3548231 -3.6748543 -2.8784103 -2.3606424 -1.8675961 -1.781368 -1.8639867 -2.4621866 -3.4098182 -4.5248408 -5.7051492 -6.3364387 -6.5524411][-4.3190832 -4.3146749 -3.9014468 -2.9735723 -1.7033298 -0.63687682 0.14557219 0.35696983 -0.07538414 -1.1387243 -2.400476 -3.7056315 -4.9161587 -5.9183521 -6.4281793][-4.3804259 -4.1618242 -3.4393265 -1.9090185 -0.078035355 1.2837071 2.2458568 2.4381967 1.9367704 0.69416237 -0.97913527 -2.6239991 -4.1481004 -5.28024 -5.8473206][-4.7075238 -4.10619 -3.0010214 -1.1409369 1.1488905 2.9735389 4.3188906 4.4426813 3.684907 2.1136374 0.084518433 -1.8410416 -3.5753822 -4.6709614 -5.4313297][-4.7360668 -4.0213661 -2.8850334 -0.84398389 1.5495596 3.7328873 5.1436472 5.3309574 4.6166716 2.8782892 0.75759697 -1.4742756 -3.4468715 -4.7044115 -5.4765053][-5.0959964 -4.43009 -3.2696571 -1.4544778 0.45181417 2.6620517 4.3492794 4.6367226 3.9733009 2.4167738 0.43971634 -1.7810686 -3.6314006 -4.8193111 -5.58204][-5.5629082 -5.1239567 -4.3814836 -2.9073846 -1.2208946 0.48637104 1.936245 2.4274621 2.1582599 1.0021915 -0.67867994 -2.5141339 -4.0545764 -5.0838223 -5.6598282][-6.1927814 -5.9309187 -5.3728046 -4.3184137 -3.0704005 -1.7234659 -0.46207929 0.13496876 -0.10474014 -0.86223412 -1.8451917 -3.2323587 -4.4030805 -5.2068295 -5.7561655][-6.3061066 -6.2675886 -5.9778824 -5.097795 -4.17604 -3.1022959 -2.1925325 -1.758213 -1.5714207 -1.8971701 -2.5389028 -3.5459785 -4.4154658 -4.9722681 -5.3550439][-5.9888773 -6.1537762 -5.9615359 -5.1289358 -4.2838364 -3.4578962 -2.809299 -2.5715628 -2.3760095 -2.5807409 -2.8245964 -3.235522 -3.8153572 -4.319694 -4.7208314][-5.6772685 -5.7303023 -5.3781466 -4.6075563 -3.8567915 -3.0063524 -2.4959767 -2.4161057 -2.5134113 -2.7295129 -2.8661742 -3.1512051 -3.4179635 -3.7280445 -4.0043483][-5.2647009 -5.2866039 -4.9385819 -4.0780649 -3.1305456 -2.271575 -1.8655319 -1.7965457 -1.9927733 -2.2608159 -2.5354385 -2.8040423 -2.9371591 -3.0598273 -3.2576747]]...]
INFO - root - 2017-12-16 05:07:38.314931: step 18710, loss = 0.30, batch loss = 0.24 (28.9 examples/sec; 0.277 sec/batch; 24h:08m:59s remains)
INFO - root - 2017-12-16 05:07:41.103356: step 18720, loss = 0.46, batch loss = 0.40 (29.6 examples/sec; 0.270 sec/batch; 23h:31m:10s remains)
INFO - root - 2017-12-16 05:07:43.917288: step 18730, loss = 0.26, batch loss = 0.20 (29.4 examples/sec; 0.272 sec/batch; 23h:42m:25s remains)
INFO - root - 2017-12-16 05:07:46.688422: step 18740, loss = 0.24, batch loss = 0.18 (28.5 examples/sec; 0.281 sec/batch; 24h:29m:58s remains)
INFO - root - 2017-12-16 05:07:49.527195: step 18750, loss = 0.35, batch loss = 0.29 (29.6 examples/sec; 0.270 sec/batch; 23h:31m:08s remains)
INFO - root - 2017-12-16 05:07:52.365263: step 18760, loss = 0.29, batch loss = 0.23 (27.7 examples/sec; 0.288 sec/batch; 25h:07m:42s remains)
INFO - root - 2017-12-16 05:07:55.226398: step 18770, loss = 0.30, batch loss = 0.24 (29.1 examples/sec; 0.275 sec/batch; 23h:59m:30s remains)
INFO - root - 2017-12-16 05:07:58.085126: step 18780, loss = 0.31, batch loss = 0.25 (28.1 examples/sec; 0.285 sec/batch; 24h:50m:27s remains)
INFO - root - 2017-12-16 05:08:00.934120: step 18790, loss = 0.30, batch loss = 0.24 (27.9 examples/sec; 0.286 sec/batch; 24h:56m:49s remains)
INFO - root - 2017-12-16 05:08:03.750517: step 18800, loss = 0.21, batch loss = 0.15 (28.0 examples/sec; 0.285 sec/batch; 24h:51m:29s remains)
2017-12-16 05:08:04.220033: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1379275 -2.2813501 -2.4744036 -2.7849174 -3.2601652 -3.6775665 -4.1648889 -4.7794456 -5.4037318 -5.913846 -5.969595 -5.6012492 -4.983511 -4.333952 -3.6242502][-2.7273664 -2.8950968 -3.082783 -3.1928267 -3.1697083 -3.4295583 -3.8805439 -4.3572955 -4.8113861 -5.1620317 -5.5273814 -5.6040564 -5.2397485 -4.5060697 -3.7847178][-3.5273826 -3.8677201 -3.9522998 -3.8678665 -3.7769332 -3.5438087 -3.1854153 -3.3520453 -3.752027 -4.16008 -4.5498543 -4.7113438 -4.7935739 -4.358407 -3.6248097][-3.7460074 -4.1949639 -4.3620195 -3.9921918 -3.410027 -2.7478535 -2.2864857 -1.9271648 -1.7799745 -2.2773957 -3.2016451 -3.7868013 -3.9865947 -3.8757408 -3.4428499][-4.0019531 -4.1776471 -4.0757442 -3.4674389 -2.4293771 -1.1335027 -0.042778969 0.52474213 0.57450104 0.12510347 -1.0531311 -2.5478749 -3.4845045 -3.5275455 -3.2566872][-4.1198788 -4.2387805 -3.8568711 -2.9791973 -1.7392893 0.036378384 1.5787392 2.4866185 2.6034389 1.7851486 0.3715148 -1.3447092 -2.7592955 -3.3942151 -3.2959323][-3.5024939 -3.6116343 -3.2367184 -2.102221 -0.62789583 0.92113304 2.3709941 3.2598486 3.2361722 2.2843356 0.62510395 -1.1798899 -2.5655036 -3.375118 -3.3747892][-3.0100527 -2.7967997 -2.3881195 -1.3491683 0.16320372 1.6871042 2.7476935 3.0523934 2.6505322 1.5323296 -0.016716003 -1.7732215 -3.1382015 -3.8274305 -3.8929324][-2.9125495 -2.6443081 -2.0548208 -1.0081656 0.12957668 1.356566 2.0397158 1.8201199 1.1011844 -0.041051865 -1.5274909 -2.9057355 -4.0373263 -4.5095906 -4.3958111][-3.2851562 -2.8129802 -2.239527 -1.1990788 -0.28737402 0.31104088 0.59726572 0.12396955 -0.78179669 -2.0416002 -3.1622982 -4.0688543 -4.7491183 -4.9379706 -4.7073526][-3.9656928 -3.3622096 -2.6905212 -1.8059425 -1.0384054 -0.80383229 -1.1537712 -1.7608392 -2.5471747 -3.5283647 -4.3350639 -4.8875279 -5.1458311 -4.9835162 -4.4829431][-4.5385 -3.9233086 -3.0509682 -2.28982 -1.799345 -1.7701619 -2.1632457 -3.0487115 -3.9896936 -4.70825 -5.3004756 -5.4836931 -5.1778612 -4.6821494 -4.1700773][-5.2385159 -4.3862939 -3.3958101 -2.3670473 -1.6815495 -1.8511891 -2.5133023 -3.3575218 -4.4037647 -5.2681608 -5.5602322 -5.4589105 -5.1269259 -4.40469 -3.6494541][-5.2838392 -4.3438854 -3.2907603 -2.2260332 -1.3805382 -1.233279 -1.8281355 -2.9805691 -4.0619264 -4.9196434 -5.5635662 -5.3523941 -4.5972123 -3.8844283 -3.3342075][-4.7723918 -3.8404346 -2.6484251 -1.4918408 -0.64875627 -0.43562174 -0.87415671 -2.0601535 -3.4874375 -4.564496 -4.971489 -4.9135709 -4.477253 -3.5637021 -2.8349919]]...]
INFO - root - 2017-12-16 05:08:07.055962: step 18810, loss = 0.24, batch loss = 0.19 (28.5 examples/sec; 0.281 sec/batch; 24h:27m:35s remains)
INFO - root - 2017-12-16 05:08:09.852693: step 18820, loss = 0.27, batch loss = 0.21 (29.6 examples/sec; 0.270 sec/batch; 23h:33m:07s remains)
INFO - root - 2017-12-16 05:08:12.665369: step 18830, loss = 0.28, batch loss = 0.22 (25.9 examples/sec; 0.309 sec/batch; 26h:57m:31s remains)
INFO - root - 2017-12-16 05:08:15.491926: step 18840, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.280 sec/batch; 24h:26m:11s remains)
INFO - root - 2017-12-16 05:08:18.309091: step 18850, loss = 0.21, batch loss = 0.16 (28.1 examples/sec; 0.284 sec/batch; 24h:47m:05s remains)
INFO - root - 2017-12-16 05:08:21.152708: step 18860, loss = 0.32, batch loss = 0.26 (29.0 examples/sec; 0.276 sec/batch; 24h:03m:43s remains)
INFO - root - 2017-12-16 05:08:24.015746: step 18870, loss = 0.23, batch loss = 0.17 (27.5 examples/sec; 0.291 sec/batch; 25h:20m:48s remains)
INFO - root - 2017-12-16 05:08:26.820745: step 18880, loss = 0.33, batch loss = 0.27 (28.5 examples/sec; 0.281 sec/batch; 24h:26m:35s remains)
INFO - root - 2017-12-16 05:08:29.613197: step 18890, loss = 0.32, batch loss = 0.26 (28.9 examples/sec; 0.277 sec/batch; 24h:08m:38s remains)
INFO - root - 2017-12-16 05:08:32.479585: step 18900, loss = 0.24, batch loss = 0.19 (29.2 examples/sec; 0.274 sec/batch; 23h:54m:19s remains)
2017-12-16 05:08:32.951056: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5577552 -3.5001173 -3.513082 -3.5237694 -3.6213262 -3.755096 -3.6669722 -3.4347749 -3.115118 -2.9278316 -3.1211572 -3.3065045 -3.6359019 -3.7036455 -3.5730152][-3.239512 -2.5371156 -2.2732251 -2.3834834 -2.325588 -2.2292469 -1.9534035 -1.8001611 -1.6595366 -1.5993791 -1.877486 -2.129987 -2.4626393 -2.5489345 -2.3782945][-2.6535869 -1.9643381 -1.5634134 -1.3130901 -1.1081841 -0.90616536 -0.65795708 -0.47079802 -0.22130203 -0.2104044 -0.48308611 -0.79575253 -1.2489891 -1.2970877 -1.1266584][-1.517221 -1.1494455 -0.96855736 -0.77084351 -0.41494226 -0.039825439 0.3438139 0.61241293 0.8971405 0.92525291 0.61989737 0.11466074 -0.37215805 -0.4856 -0.34580421][-1.3541789 -0.90715647 -0.7287693 -0.73283648 -0.34935236 0.28406429 0.91303968 1.2995133 1.5874352 1.5329776 1.048707 0.49889803 0.13757133 0.038929462 0.047922611][-0.89560771 -0.55228043 -0.4637785 -0.3142724 0.18335819 0.992043 1.7103543 2.1782122 2.5342817 2.2687206 1.5659041 0.83814526 0.47120857 0.43865395 0.39633656][-0.26294708 -0.24009085 -0.22185135 -0.16489697 0.32178211 1.2901635 2.091639 2.5016322 2.668623 2.1966853 1.5036545 0.61298418 0.23089027 0.17732906 0.054467678][0.42776966 0.38315105 -0.075008869 -0.12621593 0.39196873 1.2894654 2.0464568 2.3531666 2.4857092 1.8445997 0.904253 0.15672064 -0.30292845 -0.56237364 -0.698189][0.1674304 0.13536358 0.19140339 0.27838278 0.35714531 1.0668383 1.6237841 1.6764541 1.4975028 0.77794838 -0.048508167 -0.82846212 -1.1787989 -1.3554401 -1.4071906][-0.80436349 -0.744488 -1.0802658 -0.90147734 -0.25707197 0.35370398 0.4962368 0.40526533 0.18567896 -0.441962 -1.1662254 -1.8840671 -2.2105694 -2.1736848 -2.0077853][-1.972158 -2.1716032 -2.2708287 -2.0356314 -1.7816682 -1.066848 -0.39202642 -0.59334564 -0.9365828 -1.3590245 -1.8885453 -2.6461356 -3.0392332 -2.9143486 -2.6740279][-2.715601 -2.9229145 -3.2413828 -3.2565885 -2.9870963 -2.1997333 -1.7088411 -1.5458364 -1.5024455 -1.9018595 -2.5013461 -3.1899064 -3.5748589 -3.5288844 -3.3370728][-3.0039856 -3.3467629 -3.7201009 -3.6739945 -3.5013771 -3.1085343 -2.5765696 -2.1797397 -2.0034149 -2.1851916 -2.5816755 -3.0777817 -3.2896461 -3.3097718 -3.3314996][-2.7289233 -2.8796525 -3.0893679 -3.3542895 -3.3131866 -2.8851323 -2.542222 -2.4442792 -2.2165949 -2.1641166 -2.3950305 -2.6865487 -2.8981757 -2.856493 -2.7170219][-2.0889704 -2.2011487 -2.443501 -2.4528127 -2.3649402 -2.1985135 -2.0738966 -2.0152814 -1.9513876 -2.0114007 -2.1026568 -2.0906169 -1.9508076 -1.7677581 -1.8486822]]...]
INFO - root - 2017-12-16 05:08:35.790342: step 18910, loss = 0.24, batch loss = 0.18 (26.8 examples/sec; 0.298 sec/batch; 25h:59m:14s remains)
INFO - root - 2017-12-16 05:08:38.651627: step 18920, loss = 0.28, batch loss = 0.22 (27.8 examples/sec; 0.287 sec/batch; 25h:01m:44s remains)
INFO - root - 2017-12-16 05:08:41.499452: step 18930, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.285 sec/batch; 24h:50m:49s remains)
INFO - root - 2017-12-16 05:08:44.348951: step 18940, loss = 0.42, batch loss = 0.36 (29.5 examples/sec; 0.271 sec/batch; 23h:38m:07s remains)
INFO - root - 2017-12-16 05:08:47.172307: step 18950, loss = 0.25, batch loss = 0.20 (27.3 examples/sec; 0.293 sec/batch; 25h:29m:30s remains)
INFO - root - 2017-12-16 05:08:49.963903: step 18960, loss = 0.23, batch loss = 0.17 (28.1 examples/sec; 0.285 sec/batch; 24h:47m:44s remains)
INFO - root - 2017-12-16 05:08:52.820491: step 18970, loss = 0.28, batch loss = 0.22 (28.0 examples/sec; 0.286 sec/batch; 24h:52m:14s remains)
INFO - root - 2017-12-16 05:08:55.727540: step 18980, loss = 0.29, batch loss = 0.23 (25.6 examples/sec; 0.312 sec/batch; 27h:10m:14s remains)
INFO - root - 2017-12-16 05:08:58.560216: step 18990, loss = 0.20, batch loss = 0.14 (26.0 examples/sec; 0.308 sec/batch; 26h:50m:00s remains)
INFO - root - 2017-12-16 05:09:01.376454: step 19000, loss = 0.23, batch loss = 0.17 (28.1 examples/sec; 0.285 sec/batch; 24h:47m:58s remains)
2017-12-16 05:09:01.825446: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6946206 -2.9187219 -3.2168033 -3.5327795 -3.4160302 -3.3006806 -3.2715204 -3.4040592 -3.759583 -4.3470945 -5.1378212 -6.0502448 -6.8420296 -7.1648407 -7.15625][-1.7875905 -2.1481035 -2.5141683 -2.9153471 -2.898829 -2.8321977 -2.7752583 -2.9886703 -3.4354351 -4.1048412 -4.9821939 -5.8989954 -6.8323288 -7.3588896 -7.5398607][-1.3924003 -1.8823485 -2.2909813 -2.4917459 -2.4320402 -2.2647009 -2.095418 -2.3391819 -2.7779009 -3.4176002 -4.2490497 -5.3160772 -6.4484615 -7.064364 -7.2684989][-0.71362686 -1.104187 -1.3489354 -1.360096 -1.1486707 -0.93962288 -0.88038731 -1.1835589 -1.5967009 -2.1557615 -2.9270308 -4.1072969 -5.3528066 -6.1367545 -6.6096172][-0.89036751 -1.0107038 -0.93955731 -0.71474552 -0.37560749 -0.10805082 -0.051974773 -0.20624018 -0.48769832 -1.0548718 -1.6617489 -2.5170436 -3.5048232 -4.3431997 -4.9224014][-1.7556272 -1.4786227 -1.0125403 -0.66803384 -0.22217464 0.24847841 0.5402441 0.49857521 0.29943752 0.0480361 -0.36505508 -1.1172018 -1.9545259 -2.6642356 -3.1467857][-2.7139349 -2.1401565 -1.4427485 -0.77638531 -0.059541702 0.44438171 0.7986598 0.93717194 0.93906641 0.81204081 0.4636445 -0.021221161 -0.61903834 -1.3572073 -1.7941437][-3.49921 -2.7869172 -2.0204377 -1.2397277 -0.46957088 0.20475578 0.77445221 0.9369607 1.0191369 1.033102 0.81850481 0.491498 0.15847921 -0.21480942 -0.38045549][-4.3154306 -3.458581 -2.5939207 -1.825069 -1.1074603 -0.42982674 0.10427046 0.35535049 0.58878326 0.55609465 0.30688047 0.13492107 0.067729473 0.0074214935 0.092779636][-4.8841767 -3.9572642 -3.1059594 -2.3668153 -1.6860397 -1.2103925 -0.87344193 -0.62292504 -0.39939308 -0.34212017 -0.4585278 -0.72936916 -0.76217365 -0.66357636 -0.43805385][-5.0712976 -4.4847565 -3.9374523 -3.2559805 -2.5672326 -2.1311419 -1.7939732 -1.6670954 -1.6220155 -1.6697552 -1.8090975 -2.0521116 -2.0315039 -1.8285317 -1.4650071][-4.9754639 -4.739295 -4.5745416 -4.2838006 -3.9006319 -3.5546348 -3.1907332 -2.9952731 -2.9474711 -3.0567482 -3.2356784 -3.419992 -3.4422078 -3.2928295 -2.8885469][-4.6406293 -4.6904454 -4.83252 -4.8288717 -4.7350478 -4.6099834 -4.3961625 -4.1551166 -4.0345497 -4.1975713 -4.4877405 -4.6909432 -4.76867 -4.7453208 -4.4764953][-3.6006706 -3.8333552 -4.2295623 -4.469789 -4.6820173 -4.730052 -4.6782985 -4.53017 -4.5515795 -4.7925057 -5.1405458 -5.5439987 -5.8375039 -5.9671545 -5.7752786][-2.8570881 -3.0290904 -3.3279195 -3.6357162 -3.9794638 -4.2207117 -4.3778777 -4.4108224 -4.5477567 -4.819 -5.2380648 -5.6829357 -6.04471 -6.3126841 -6.2687006]]...]
INFO - root - 2017-12-16 05:09:04.670563: step 19010, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.282 sec/batch; 24h:34m:42s remains)
INFO - root - 2017-12-16 05:09:07.505146: step 19020, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.281 sec/batch; 24h:29m:51s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 05:09:10.363874: step 19030, loss = 0.26, batch loss = 0.21 (28.1 examples/sec; 0.285 sec/batch; 24h:47m:54s remains)
INFO - root - 2017-12-16 05:09:13.170741: step 19040, loss = 0.27, batch loss = 0.21 (28.8 examples/sec; 0.278 sec/batch; 24h:10m:13s remains)
INFO - root - 2017-12-16 05:09:15.967036: step 19050, loss = 0.39, batch loss = 0.33 (29.2 examples/sec; 0.274 sec/batch; 23h:50m:51s remains)
INFO - root - 2017-12-16 05:09:18.782533: step 19060, loss = 0.42, batch loss = 0.36 (28.2 examples/sec; 0.283 sec/batch; 24h:40m:37s remains)
INFO - root - 2017-12-16 05:09:21.612725: step 19070, loss = 0.24, batch loss = 0.19 (29.4 examples/sec; 0.272 sec/batch; 23h:40m:47s remains)
INFO - root - 2017-12-16 05:09:24.467182: step 19080, loss = 0.32, batch loss = 0.27 (26.1 examples/sec; 0.307 sec/batch; 26h:42m:12s remains)
INFO - root - 2017-12-16 05:09:27.295522: step 19090, loss = 0.32, batch loss = 0.26 (28.1 examples/sec; 0.285 sec/batch; 24h:48m:29s remains)
INFO - root - 2017-12-16 05:09:30.113646: step 19100, loss = 0.22, batch loss = 0.16 (29.8 examples/sec; 0.269 sec/batch; 23h:23m:17s remains)
2017-12-16 05:09:30.590171: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6647375 -3.8055894 -3.9960339 -3.9497333 -3.6606491 -3.560241 -3.4570527 -3.5553756 -3.5614953 -3.5854015 -3.4390225 -3.4308605 -3.60891 -3.9754536 -4.2898231][-3.1099229 -3.4369612 -3.7491822 -4.1016121 -4.2750049 -4.3416076 -4.1917186 -4.0751619 -4.0339994 -3.9945905 -3.8325782 -3.705615 -3.7406683 -3.7417302 -3.8065586][-2.8761654 -3.6094365 -4.420589 -4.7442722 -4.8195043 -4.8980994 -4.8243427 -4.7611403 -4.5824151 -4.3341532 -4.1943855 -3.9378483 -3.7829792 -3.628381 -3.7580397][-2.617841 -3.6396658 -4.4669647 -4.9795008 -5.08539 -4.8671074 -4.4121571 -4.1858759 -3.8791671 -3.68814 -3.5461168 -3.7235661 -4.2052135 -4.0262446 -3.7918181][-2.8988132 -4.0078883 -4.6369834 -4.7372169 -4.3736119 -3.6602683 -2.74817 -2.2892959 -2.0701663 -2.4350433 -2.8553481 -3.2418423 -3.6330881 -4.006608 -4.4020224][-3.3767889 -4.2122512 -4.6769872 -4.4239736 -3.31886 -1.9866457 -0.60108232 0.21134472 0.4071846 -0.098341465 -1.0142238 -2.2544124 -3.368439 -3.7560253 -3.8557632][-3.9910016 -4.4368377 -4.3546376 -3.5186548 -1.8848 0.057788849 1.8841786 2.8587098 3.0008612 2.1488562 0.67856455 -0.89150167 -2.1096849 -2.8318858 -3.2569578][-4.1533084 -4.433948 -4.2292495 -3.1053624 -1.1440489 1.2027278 3.399035 4.3773308 4.3713188 3.5278416 2.0579724 0.15235233 -1.6458611 -2.5561624 -2.9681475][-4.3788509 -4.7164354 -4.353879 -3.2664397 -1.6464901 0.72632504 3.1012793 4.2010584 4.2445774 3.1845493 1.5390821 -0.17361736 -1.4897239 -2.412075 -3.0289724][-4.6019988 -5.3904161 -5.5305004 -4.4790521 -2.8513384 -0.88173366 0.89261007 2.035553 2.4083276 1.7742157 0.4684391 -1.0947928 -2.3148069 -2.9742317 -3.1392856][-5.23895 -6.0452013 -6.1669378 -5.6315928 -4.5546074 -2.7645783 -1.0369799 -0.18656778 -0.15420628 -0.55109406 -1.3086514 -2.4740028 -3.2638855 -3.5758073 -3.6445718][-5.459918 -6.1027641 -6.396925 -6.1510048 -5.4121118 -4.2970405 -3.1917298 -2.4882236 -2.2422888 -2.4025509 -2.8582897 -3.6665397 -4.21823 -4.2320094 -3.9625208][-5.3555222 -5.8044071 -6.0328107 -5.7349005 -5.2932825 -4.6539178 -3.9683685 -3.5891249 -3.5026755 -3.5413883 -3.6630025 -4.0379257 -4.16309 -4.1003742 -3.8536847][-4.95877 -5.0206504 -4.9692121 -4.6713066 -4.4545889 -4.0170121 -3.6779563 -3.6228037 -3.7188592 -3.6871185 -3.6030583 -3.8372633 -3.8966572 -3.6070433 -3.2817216][-4.2485423 -3.9194286 -3.6580641 -3.3056123 -3.0704184 -2.9608092 -2.9965069 -3.0587323 -3.1631203 -3.2728951 -3.3420382 -3.363508 -3.2755127 -3.0956304 -2.8839841]]...]
INFO - root - 2017-12-16 05:09:33.456951: step 19110, loss = 0.25, batch loss = 0.19 (26.3 examples/sec; 0.304 sec/batch; 26h:28m:15s remains)
INFO - root - 2017-12-16 05:09:36.275154: step 19120, loss = 0.22, batch loss = 0.16 (27.9 examples/sec; 0.287 sec/batch; 24h:58m:30s remains)
INFO - root - 2017-12-16 05:09:39.119908: step 19130, loss = 0.36, batch loss = 0.30 (28.0 examples/sec; 0.286 sec/batch; 24h:54m:20s remains)
INFO - root - 2017-12-16 05:09:41.913946: step 19140, loss = 0.33, batch loss = 0.27 (29.6 examples/sec; 0.270 sec/batch; 23h:32m:07s remains)
INFO - root - 2017-12-16 05:09:44.732348: step 19150, loss = 0.20, batch loss = 0.15 (28.1 examples/sec; 0.285 sec/batch; 24h:49m:09s remains)
INFO - root - 2017-12-16 05:09:47.582368: step 19160, loss = 0.33, batch loss = 0.28 (26.9 examples/sec; 0.297 sec/batch; 25h:51m:19s remains)
INFO - root - 2017-12-16 05:09:50.392102: step 19170, loss = 0.39, batch loss = 0.33 (28.2 examples/sec; 0.284 sec/batch; 24h:43m:28s remains)
INFO - root - 2017-12-16 05:09:53.265326: step 19180, loss = 0.27, batch loss = 0.21 (26.5 examples/sec; 0.302 sec/batch; 26h:19m:21s remains)
INFO - root - 2017-12-16 05:09:56.126663: step 19190, loss = 0.33, batch loss = 0.27 (29.2 examples/sec; 0.274 sec/batch; 23h:49m:51s remains)
INFO - root - 2017-12-16 05:09:58.952251: step 19200, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 24h:31m:47s remains)
2017-12-16 05:09:59.422354: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6728845 -5.1157508 -5.41087 -5.5503683 -5.6378727 -5.5930896 -5.5877686 -5.7694521 -6.1722994 -6.7416945 -7.1673203 -7.5864215 -7.5706406 -6.9218674 -5.9876671][-5.5516977 -5.9942236 -6.1953573 -6.2061992 -6.19823 -6.3062754 -6.521955 -6.8017616 -7.2461042 -8.0440207 -8.9499092 -9.6027908 -9.6669006 -9.0013638 -7.8377781][-5.988812 -6.3991494 -6.4915648 -6.2434063 -5.9146891 -5.7949457 -5.8464155 -6.5147719 -7.45473 -8.5187092 -9.6232185 -10.573619 -11.165046 -10.783266 -9.5944843][-6.1347218 -6.3191018 -6.1067152 -5.4532456 -4.8702722 -4.4041619 -4.2049637 -4.6280622 -5.4879942 -7.1934428 -8.9528913 -10.367396 -11.304544 -11.482512 -10.722836][-5.5462608 -5.3830576 -4.7762785 -3.8549776 -2.7270269 -1.5602474 -0.9843092 -1.2050686 -2.1268988 -4.1146331 -6.4056244 -8.8228693 -10.616595 -11.400053 -10.964976][-4.7405629 -4.3053713 -3.2371914 -1.5940442 0.28857422 1.6555037 2.563951 2.8056006 2.0468106 -0.19370651 -3.2260501 -6.4550247 -9.0203381 -10.370928 -10.419024][-4.4410253 -3.6417959 -2.3204026 -0.48201895 2.0267735 4.3938627 6.0662022 6.247345 5.2910576 3.0597801 -0.18875408 -3.9702985 -7.1347713 -8.7247868 -8.8518162][-4.7236075 -3.8716946 -2.5704186 -0.36979914 2.2698917 4.8014927 7.0380678 7.7731905 6.9107008 4.4363871 1.1014366 -2.6772735 -5.7072344 -7.3430529 -7.535675][-5.286973 -4.8697929 -3.5715179 -1.464273 0.83404732 3.5027785 5.7160511 6.4123974 5.8391294 3.8432102 0.871325 -2.443578 -5.14676 -6.6808262 -6.7638626][-5.5068707 -5.7079916 -5.2753925 -3.5867226 -1.5047312 0.75936556 2.6471233 3.6870165 3.3408031 1.5895076 -0.59818816 -3.05399 -5.3416514 -6.6255455 -6.9061708][-6.4507065 -6.7366047 -6.4662557 -5.4317942 -4.0439024 -2.4151366 -1.0531199 -0.35819387 -0.38502455 -1.3351862 -2.9114614 -4.6232476 -6.029356 -6.8534946 -7.0939865][-6.8468256 -7.1525183 -7.3102608 -6.802309 -5.791491 -4.6450086 -3.7799635 -3.3528194 -3.44814 -4.1360245 -5.058043 -6.1412635 -6.9784174 -7.3221722 -7.2828374][-6.2365284 -6.6006441 -6.8907232 -6.793499 -6.6091032 -5.9432321 -5.1489615 -4.7599826 -4.7557068 -5.0307755 -5.669404 -6.4670205 -6.9329662 -7.2189989 -7.1363277][-5.3559752 -5.5094032 -5.6425819 -5.754477 -5.8879857 -5.6740646 -5.4797792 -5.3963218 -5.3767309 -5.4428244 -5.4734035 -5.7007 -6.0956473 -6.2746925 -6.1650629][-4.57325 -4.5156517 -4.3895774 -4.4094362 -4.5410385 -4.5854177 -4.6120358 -4.6366286 -4.6942987 -4.8759546 -5.010088 -4.8913512 -4.6825709 -4.7064357 -4.7657018]]...]
INFO - root - 2017-12-16 05:10:02.255481: step 19210, loss = 0.24, batch loss = 0.18 (27.9 examples/sec; 0.287 sec/batch; 24h:58m:33s remains)
INFO - root - 2017-12-16 05:10:05.072722: step 19220, loss = 0.25, batch loss = 0.19 (28.3 examples/sec; 0.283 sec/batch; 24h:38m:19s remains)
INFO - root - 2017-12-16 05:10:08.017009: step 19230, loss = 0.29, batch loss = 0.23 (24.6 examples/sec; 0.326 sec/batch; 28h:20m:58s remains)
INFO - root - 2017-12-16 05:10:10.867697: step 19240, loss = 0.33, batch loss = 0.27 (28.5 examples/sec; 0.281 sec/batch; 24h:26m:22s remains)
INFO - root - 2017-12-16 05:10:13.757072: step 19250, loss = 0.31, batch loss = 0.25 (28.7 examples/sec; 0.279 sec/batch; 24h:15m:45s remains)
INFO - root - 2017-12-16 05:10:16.598685: step 19260, loss = 0.26, batch loss = 0.20 (27.5 examples/sec; 0.291 sec/batch; 25h:20m:34s remains)
INFO - root - 2017-12-16 05:10:19.436436: step 19270, loss = 0.20, batch loss = 0.14 (29.6 examples/sec; 0.270 sec/batch; 23h:30m:05s remains)
INFO - root - 2017-12-16 05:10:22.309775: step 19280, loss = 0.21, batch loss = 0.15 (28.9 examples/sec; 0.277 sec/batch; 24h:04m:03s remains)
INFO - root - 2017-12-16 05:10:25.187144: step 19290, loss = 0.27, batch loss = 0.21 (27.8 examples/sec; 0.288 sec/batch; 25h:03m:03s remains)
INFO - root - 2017-12-16 05:10:27.984813: step 19300, loss = 0.28, batch loss = 0.22 (29.1 examples/sec; 0.275 sec/batch; 23h:54m:11s remains)
2017-12-16 05:10:28.430124: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8410583 -4.4067297 -3.8527899 -3.7841792 -3.791538 -3.6904469 -3.732043 -3.6416125 -3.2816994 -3.0669446 -3.1750188 -3.4153433 -3.7969277 -4.2758126 -4.7865109][-4.6830645 -4.2505207 -3.676877 -3.4097304 -3.4528058 -3.3752651 -3.4283023 -3.3648334 -2.954987 -2.6937346 -2.7394438 -2.8755112 -3.060822 -3.3462868 -3.8051007][-4.8914576 -4.5127573 -3.9004254 -3.3470294 -2.9980283 -2.8570976 -2.9327517 -2.8466744 -2.458034 -2.1706488 -2.0192902 -2.1723394 -2.3626502 -2.5757966 -2.9224737][-4.3789716 -3.9873102 -3.3815577 -3.072042 -2.840097 -2.421957 -2.3005228 -2.2400384 -1.8955638 -1.6126604 -1.5156546 -1.6543782 -1.847199 -1.9621668 -2.2544861][-4.0868158 -3.6320179 -2.9969032 -2.3918185 -1.8104868 -1.1942918 -0.82598805 -0.76089525 -0.71657062 -0.65142393 -0.72575259 -1.0608358 -1.3973205 -1.5850618 -1.9920423][-4.2259307 -3.7656169 -3.0203342 -2.0327721 -0.90303135 -0.092104435 0.38227797 0.67439413 0.78182125 0.47783756 -0.026889801 -0.672343 -1.3687658 -1.8147166 -2.20297][-4.2455697 -4.1058092 -3.4313712 -2.3304534 -0.86580515 0.46462488 1.3313532 1.4960008 1.6052718 1.2696266 0.43802309 -0.37633705 -1.0893767 -1.8269708 -2.5685427][-4.005476 -4.1995683 -3.7740896 -2.8599839 -1.2407746 0.17554283 1.241045 1.7197099 1.6589437 1.4825449 0.69812489 -0.35728931 -1.2336185 -2.0199125 -2.79884][-4.069386 -4.1714296 -3.9065969 -3.2277188 -1.8964465 -0.45430088 0.57503319 1.1580534 1.3665619 1.0784411 0.19139147 -0.87561464 -1.8780251 -2.7733324 -3.458353][-3.8106673 -4.4311695 -4.3711882 -3.7748165 -2.9343376 -1.7761271 -0.77673721 -0.18028736 0.23277617 0.2175622 -0.46588397 -1.3884263 -2.2777371 -3.0496614 -3.6548061][-3.2797446 -4.1317458 -4.3960853 -4.274044 -3.5573449 -2.7137856 -2.1019411 -1.5349166 -1.1212916 -1.035928 -1.4006577 -2.0194798 -2.6729112 -3.1647122 -3.4645212][-3.067132 -3.6686158 -3.9504859 -4.1154804 -3.8624978 -3.3217168 -2.9206865 -2.8145685 -2.466692 -2.0962415 -2.2940397 -2.6929455 -3.1686993 -3.492599 -3.6721823][-3.2609191 -3.4914579 -3.7002709 -3.6393194 -3.6818905 -3.4858489 -3.2910171 -3.186064 -3.0233045 -2.8360629 -2.6548123 -2.7534432 -3.0400584 -3.3446789 -3.5077546][-3.5062203 -3.5594578 -3.5540395 -3.2296438 -3.0400581 -3.0230892 -3.1915703 -3.2972496 -3.1680026 -3.1453385 -2.9132333 -2.7370925 -2.9005928 -3.1463442 -3.2749794][-3.6298704 -3.529732 -3.2986414 -2.9678926 -2.8965044 -2.8669128 -2.9893382 -3.1034844 -3.0017924 -2.7197335 -2.4058278 -2.2973201 -2.5953743 -3.0051935 -3.2005789]]...]
INFO - root - 2017-12-16 05:10:31.226657: step 19310, loss = 0.30, batch loss = 0.24 (27.4 examples/sec; 0.292 sec/batch; 25h:26m:04s remains)
INFO - root - 2017-12-16 05:10:34.057800: step 19320, loss = 0.32, batch loss = 0.26 (26.9 examples/sec; 0.297 sec/batch; 25h:50m:10s remains)
INFO - root - 2017-12-16 05:10:36.912484: step 19330, loss = 0.41, batch loss = 0.35 (27.6 examples/sec; 0.289 sec/batch; 25h:10m:15s remains)
INFO - root - 2017-12-16 05:10:39.727869: step 19340, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 24h:34m:39s remains)
INFO - root - 2017-12-16 05:10:42.496096: step 19350, loss = 0.28, batch loss = 0.22 (28.8 examples/sec; 0.277 sec/batch; 24h:08m:18s remains)
INFO - root - 2017-12-16 05:10:45.290364: step 19360, loss = 0.25, batch loss = 0.19 (27.3 examples/sec; 0.293 sec/batch; 25h:28m:45s remains)
INFO - root - 2017-12-16 05:10:48.124528: step 19370, loss = 0.30, batch loss = 0.24 (28.7 examples/sec; 0.278 sec/batch; 24h:13m:06s remains)
INFO - root - 2017-12-16 05:10:50.993484: step 19380, loss = 0.29, batch loss = 0.23 (28.4 examples/sec; 0.282 sec/batch; 24h:30m:22s remains)
INFO - root - 2017-12-16 05:10:53.848012: step 19390, loss = 0.28, batch loss = 0.22 (25.4 examples/sec; 0.315 sec/batch; 27h:25m:02s remains)
INFO - root - 2017-12-16 05:10:56.707657: step 19400, loss = 0.24, batch loss = 0.18 (29.1 examples/sec; 0.275 sec/batch; 23h:54m:47s remains)
2017-12-16 05:10:57.149773: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.075753 -3.3522429 -3.6840537 -3.9747658 -4.1447368 -4.1574593 -3.8135383 -3.441947 -3.1986296 -2.932646 -2.7180867 -2.9031916 -3.1872761 -3.1380143 -2.973474][-2.3586597 -2.6342726 -3.0090714 -3.3751636 -3.4240935 -3.3459048 -3.2095475 -3.0924923 -2.9114189 -2.6349726 -2.4981356 -2.5081663 -2.5828295 -2.5905261 -2.4141979][-1.6580195 -2.003562 -2.3803537 -2.7149487 -2.9367428 -2.9435062 -2.6411076 -2.4980731 -2.4680569 -2.4249935 -2.2720525 -2.1177161 -2.1261234 -2.1061995 -1.9628298][-1.081327 -1.4014835 -1.6516068 -1.8758371 -1.8911009 -1.896322 -1.8108134 -1.736444 -1.6026633 -1.6394181 -1.6910841 -1.5966232 -1.6026666 -1.6016126 -1.6197708][-0.27739286 -0.61143827 -0.84689951 -0.913368 -0.68603778 -0.52759504 -0.56476641 -0.62201452 -0.74334049 -0.91989183 -1.0757399 -1.0845823 -1.0802333 -1.1513095 -1.3338871][-0.49808717 -0.6213131 -0.629684 -0.64877844 -0.38855314 0.096816063 0.44762802 0.38945293 0.093026161 -0.15636063 -0.42129278 -0.52439475 -0.61310291 -0.74838877 -1.0125687][-1.0637193 -1.2971754 -1.2464397 -0.98471332 -0.471462 -0.089601517 0.26440954 0.49373007 0.46683168 0.22996759 -0.13090229 -0.31428003 -0.4004879 -0.54073262 -0.84223294][-1.8861828 -2.2520132 -2.3386397 -2.1129208 -1.4427674 -0.76382494 -0.3280592 0.010340691 0.30398417 0.27881336 0.0062789917 -0.11440754 -0.13343287 -0.13900948 -0.40093708][-2.5565338 -2.9941175 -3.2336071 -3.1133337 -2.6440475 -1.9427736 -1.1578777 -0.70538735 -0.6722405 -0.50794053 -0.43703675 -0.46959257 -0.47672462 -0.302114 -0.23905849][-3.5104747 -4.0786433 -4.2790184 -4.1898694 -3.7711689 -3.1809864 -2.5853534 -1.8214455 -1.2015216 -1.0815716 -1.1149132 -1.0752571 -1.1352382 -0.97794557 -0.6419909][-3.8792248 -4.4187746 -4.8734946 -4.9248466 -4.6265917 -4.1132178 -3.4487805 -2.8189621 -2.4609118 -2.23824 -1.9449089 -1.7570658 -1.5189338 -1.2663715 -1.0816486][-4.2340169 -4.5629215 -4.777504 -5.0243635 -5.0782194 -4.6594367 -4.1045508 -3.7692308 -3.3821156 -3.1241632 -2.993959 -2.6565261 -2.198844 -1.6177566 -1.1201267][-4.7953739 -4.7244568 -4.6168647 -4.62085 -4.58427 -4.3735881 -4.057703 -3.8414869 -3.697988 -3.7185051 -3.652478 -3.2935209 -2.5534031 -1.7316668 -1.1421652][-4.9147296 -4.52976 -4.1471858 -3.8985085 -3.9701438 -3.784549 -3.6275532 -3.650353 -3.7207696 -3.8549542 -3.7753246 -3.4457757 -2.6436086 -1.6203637 -1.0192981][-4.809413 -4.3015218 -3.6805792 -3.3274593 -3.1592956 -3.0695746 -3.0934858 -3.3112435 -3.634748 -3.9737473 -3.8423324 -3.2217236 -2.13962 -1.0167787 -0.42525816]]...]
INFO - root - 2017-12-16 05:10:59.976609: step 19410, loss = 0.40, batch loss = 0.35 (28.0 examples/sec; 0.285 sec/batch; 24h:49m:09s remains)
INFO - root - 2017-12-16 05:11:02.818810: step 19420, loss = 0.47, batch loss = 0.41 (29.1 examples/sec; 0.275 sec/batch; 23h:55m:02s remains)
INFO - root - 2017-12-16 05:11:05.577220: step 19430, loss = 0.25, batch loss = 0.19 (29.3 examples/sec; 0.273 sec/batch; 23h:43m:32s remains)
INFO - root - 2017-12-16 05:11:08.392361: step 19440, loss = 0.33, batch loss = 0.27 (28.0 examples/sec; 0.285 sec/batch; 24h:48m:36s remains)
INFO - root - 2017-12-16 05:11:11.211172: step 19450, loss = 0.22, batch loss = 0.17 (28.3 examples/sec; 0.283 sec/batch; 24h:36m:25s remains)
INFO - root - 2017-12-16 05:11:14.069871: step 19460, loss = 0.25, batch loss = 0.19 (29.5 examples/sec; 0.271 sec/batch; 23h:36m:25s remains)
INFO - root - 2017-12-16 05:11:16.918620: step 19470, loss = 0.33, batch loss = 0.27 (27.7 examples/sec; 0.289 sec/batch; 25h:08m:43s remains)
INFO - root - 2017-12-16 05:11:19.735766: step 19480, loss = 0.28, batch loss = 0.22 (27.8 examples/sec; 0.288 sec/batch; 25h:00m:39s remains)
INFO - root - 2017-12-16 05:11:22.570943: step 19490, loss = 0.25, batch loss = 0.19 (29.3 examples/sec; 0.273 sec/batch; 23h:44m:34s remains)
INFO - root - 2017-12-16 05:11:25.463061: step 19500, loss = 0.23, batch loss = 0.17 (25.6 examples/sec; 0.313 sec/batch; 27h:11m:31s remains)
2017-12-16 05:11:25.934556: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7548709 -2.5640302 -2.3740427 -2.1700754 -2.0673761 -2.2390783 -2.3852627 -2.5722136 -2.8407907 -3.0237336 -3.099359 -2.6514843 -2.2470584 -2.123497 -2.0785189][-2.0975347 -1.9405386 -1.8226686 -1.8131108 -1.8242176 -2.1319821 -2.4495511 -2.7635183 -3.074223 -3.2374334 -3.21143 -2.8019495 -2.2786806 -1.8887181 -1.5650148][-1.307528 -1.3763058 -1.4846032 -1.5787976 -1.7407537 -2.1123021 -2.5893478 -3.0782695 -3.4463813 -3.5238893 -3.3975768 -2.9499776 -2.4037485 -2.0373452 -1.4882767][-0.95901442 -1.0664232 -1.2207994 -1.4533288 -1.7268252 -2.0871687 -2.5404596 -2.8720169 -3.1681452 -3.3247371 -3.2739012 -2.9935563 -2.6708522 -2.3051414 -1.7546701][-0.72729325 -0.71012092 -0.72490978 -0.68735671 -0.74696946 -1.1295986 -1.6080022 -2.0202801 -2.3658109 -2.7341957 -3.0643907 -3.0801761 -2.9549911 -2.7083569 -2.3066576][-1.0101523 -0.37482119 0.17580175 0.57486725 0.77648783 0.60770845 0.20426655 -0.33465385 -0.82622814 -1.4812801 -2.2431448 -2.9543347 -3.5378981 -3.5607557 -3.1521733][-1.13432 -0.074027538 0.93066406 1.7993927 2.3301678 2.4794207 2.2833872 1.6987724 1.0073152 0.0936017 -1.0090213 -2.28536 -3.3479242 -4.0016994 -4.0943189][-1.2388937 -0.038280487 1.2058005 2.3447948 3.2666631 3.8133297 3.9156389 3.3818202 2.4471855 1.0911932 -0.55677271 -2.1865349 -3.4693174 -4.3831787 -4.5983667][-1.4356995 -0.14165306 1.0292807 1.9972606 3.0159059 3.8516836 4.0157795 3.5769749 2.621541 1.0645156 -0.88225341 -2.7464123 -4.1904359 -5.1077576 -5.328454][-1.7596149 -0.72435069 0.03984499 0.6407752 1.4100237 2.1480207 2.6501327 2.4190655 1.5514898 0.061471939 -1.7914119 -3.5849252 -4.9685597 -5.7809911 -5.9474831][-2.6929884 -1.9973512 -1.5370166 -1.3697391 -1.1447375 -0.59517741 -0.17560863 -0.13915539 -0.51835966 -1.4726908 -2.9098272 -4.3543572 -5.4603891 -6.0591583 -6.0994797][-3.4380989 -3.2121873 -3.3445044 -3.4016039 -3.3512528 -3.3176265 -3.2454386 -3.1981039 -3.2496569 -3.5674582 -4.2152157 -5.0564461 -5.7398739 -6.1862979 -6.2055893][-3.6975167 -3.6842132 -4.0247808 -4.570869 -5.1095119 -5.2626925 -5.3321638 -5.2992554 -5.1667056 -4.9579611 -4.9931831 -5.2457824 -5.6897535 -6.1702337 -6.2691345][-4.1982117 -3.9523077 -4.179738 -4.7809739 -5.4173393 -5.9740543 -6.2283487 -6.0979781 -5.7405624 -5.1988049 -4.7022791 -4.5495586 -4.8172593 -5.260478 -5.56516][-4.0578794 -3.620919 -3.6379716 -4.1581559 -4.8849587 -5.48678 -5.6903715 -5.5876083 -5.2114449 -4.5562706 -3.9948816 -3.6331081 -3.6720386 -4.182116 -4.5046177]]...]
INFO - root - 2017-12-16 05:11:28.777456: step 19510, loss = 0.41, batch loss = 0.35 (28.1 examples/sec; 0.285 sec/batch; 24h:45m:13s remains)
INFO - root - 2017-12-16 05:11:31.584598: step 19520, loss = 0.21, batch loss = 0.15 (29.4 examples/sec; 0.272 sec/batch; 23h:39m:42s remains)
INFO - root - 2017-12-16 05:11:34.394596: step 19530, loss = 0.21, batch loss = 0.16 (28.9 examples/sec; 0.277 sec/batch; 24h:03m:14s remains)
INFO - root - 2017-12-16 05:11:37.264262: step 19540, loss = 0.38, batch loss = 0.32 (27.3 examples/sec; 0.293 sec/batch; 25h:28m:11s remains)
INFO - root - 2017-12-16 05:11:40.126078: step 19550, loss = 0.31, batch loss = 0.25 (29.3 examples/sec; 0.273 sec/batch; 23h:41m:41s remains)
INFO - root - 2017-12-16 05:11:42.982102: step 19560, loss = 0.26, batch loss = 0.20 (28.5 examples/sec; 0.280 sec/batch; 24h:22m:30s remains)
INFO - root - 2017-12-16 05:11:45.790696: step 19570, loss = 0.27, batch loss = 0.21 (29.5 examples/sec; 0.271 sec/batch; 23h:35m:37s remains)
INFO - root - 2017-12-16 05:11:48.598667: step 19580, loss = 0.28, batch loss = 0.22 (26.8 examples/sec; 0.299 sec/batch; 25h:59m:33s remains)
INFO - root - 2017-12-16 05:11:51.441529: step 19590, loss = 0.30, batch loss = 0.24 (28.1 examples/sec; 0.285 sec/batch; 24h:46m:01s remains)
INFO - root - 2017-12-16 05:11:54.354467: step 19600, loss = 0.25, batch loss = 0.19 (25.7 examples/sec; 0.311 sec/batch; 27h:03m:31s remains)
2017-12-16 05:11:54.819208: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1960003 -2.8976455 -2.6978836 -2.496913 -2.4741006 -2.5917335 -2.7478914 -2.794117 -2.7641926 -2.7620206 -2.8941364 -3.1882224 -3.4344752 -3.7020383 -3.9307847][-3.6277595 -3.1155128 -2.7400165 -2.5947466 -2.5724878 -2.7402105 -3.0022881 -3.0609784 -3.1335938 -3.2943244 -3.512193 -3.7161229 -3.8990793 -4.2022071 -4.4100685][-3.8375721 -3.2897406 -2.7643373 -2.5799732 -2.5245466 -2.6022282 -2.7127469 -2.8616931 -3.1373625 -3.2511563 -3.4936662 -3.9283133 -4.2625904 -4.5425611 -4.6003342][-3.8461752 -3.259002 -2.7722192 -2.4264913 -2.1196165 -2.1783628 -2.2884827 -2.2185335 -2.2839084 -2.5845749 -3.1170659 -3.5715952 -3.8733287 -4.1758676 -4.2852564][-4.0765405 -3.5321548 -2.88546 -2.3613012 -1.8409791 -1.4947431 -1.1847081 -1.1473908 -1.3963807 -1.6975787 -2.1377561 -2.725491 -3.1754508 -3.4086366 -3.3465741][-3.8304546 -3.3654029 -2.7111535 -1.9517756 -1.0230563 -0.37411261 0.099420071 0.33491421 0.10675859 -0.44226766 -1.1941156 -1.841917 -2.1736224 -2.3713245 -2.3790462][-3.4421885 -3.0900755 -2.4793468 -1.5655882 -0.38608837 0.53263569 1.2239618 1.4147468 1.0729704 0.52471161 -0.16119337 -0.76912045 -1.0218382 -0.93132138 -0.66562414][-3.5497124 -3.2113247 -2.5215468 -1.5041893 -0.30323458 0.85360146 1.7287254 1.8735127 1.4980145 0.87446785 0.18780041 -0.13269997 -0.051343918 0.35308504 0.83184767][-3.7861795 -3.4973853 -2.8528409 -1.9413743 -0.92708039 0.13500834 0.893239 1.0754552 0.86090851 0.45203209 0.20784569 0.19947195 0.47276258 1.1274748 1.6805301][-3.8148634 -3.5691066 -3.1347933 -2.4912357 -1.7236135 -0.91544104 -0.43870378 -0.32810354 -0.36667395 -0.55953431 -0.44631457 -0.1079731 0.50536156 1.2565947 1.7592173][-3.9180381 -3.841001 -3.62121 -3.2504869 -2.8521876 -2.2089198 -1.8701246 -1.9133289 -1.9148436 -1.8533099 -1.4252086 -0.92274261 -0.18359137 0.53875971 0.91647625][-4.0437403 -3.8302665 -3.7627439 -3.6368952 -3.6171625 -3.3237753 -3.3420959 -3.4285226 -3.3164232 -3.1056261 -2.5565679 -1.9327474 -1.1682513 -0.58943367 -0.32671309][-3.6983643 -3.4693947 -3.4442294 -3.4002404 -3.6128693 -3.6387322 -3.9514031 -4.4434667 -4.5664353 -4.3883114 -3.7199793 -3.1489038 -2.5266156 -2.0410287 -1.7054529][-3.4234419 -3.1485951 -3.1650248 -3.1610272 -3.5680389 -3.8734205 -4.4585066 -5.2618618 -5.6781421 -5.6958046 -5.1710052 -4.6581082 -4.0301771 -3.6062162 -3.3984416][-3.0628328 -2.6462812 -2.5645444 -2.6376321 -3.2537785 -3.8798995 -4.8139191 -5.7722492 -6.3986135 -6.704524 -6.3851576 -5.8252182 -5.2152958 -4.7897725 -4.456614]]...]
INFO - root - 2017-12-16 05:11:57.667994: step 19610, loss = 0.36, batch loss = 0.30 (29.1 examples/sec; 0.275 sec/batch; 23h:54m:09s remains)
INFO - root - 2017-12-16 05:12:00.518763: step 19620, loss = 0.31, batch loss = 0.26 (26.4 examples/sec; 0.303 sec/batch; 26h:19m:52s remains)
INFO - root - 2017-12-16 05:12:03.346434: step 19630, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.281 sec/batch; 24h:24m:32s remains)
INFO - root - 2017-12-16 05:12:06.174733: step 19640, loss = 0.29, batch loss = 0.23 (28.6 examples/sec; 0.280 sec/batch; 24h:20m:52s remains)
INFO - root - 2017-12-16 05:12:09.059334: step 19650, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.279 sec/batch; 24h:16m:37s remains)
INFO - root - 2017-12-16 05:12:11.914697: step 19660, loss = 0.31, batch loss = 0.25 (29.1 examples/sec; 0.275 sec/batch; 23h:52m:58s remains)
INFO - root - 2017-12-16 05:12:14.738979: step 19670, loss = 0.24, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 23h:48m:17s remains)
INFO - root - 2017-12-16 05:12:17.590733: step 19680, loss = 0.21, batch loss = 0.16 (29.3 examples/sec; 0.273 sec/batch; 23h:45m:36s remains)
INFO - root - 2017-12-16 05:12:20.402284: step 19690, loss = 0.28, batch loss = 0.23 (27.7 examples/sec; 0.288 sec/batch; 25h:03m:00s remains)
INFO - root - 2017-12-16 05:12:23.306818: step 19700, loss = 0.19, batch loss = 0.13 (27.5 examples/sec; 0.291 sec/batch; 25h:16m:58s remains)
2017-12-16 05:12:23.782834: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.44158959 -0.47668672 -0.7375071 -0.85955834 -1.1834922 -1.7330148 -2.22293 -2.3337035 -2.3774109 -2.2365029 -1.9987342 -1.3640203 -0.94256473 -1.1037176 -1.3295476][-1.0940795 -1.1587677 -1.4144487 -1.6334202 -1.9174225 -2.1011503 -2.3369265 -2.5980806 -2.7665014 -2.5323155 -2.4196794 -2.0829964 -1.7719343 -1.7134626 -1.8436482][-2.3432028 -2.6053305 -2.8488507 -3.0474138 -3.0252419 -2.9491773 -2.9399157 -3.0431476 -3.2842498 -3.23248 -3.2528141 -3.1603122 -3.1521227 -3.1622519 -3.0406795][-3.5698688 -3.7534094 -3.9727726 -3.8556092 -3.3767588 -2.8848217 -2.5933871 -2.4391937 -2.5560322 -2.838434 -3.5259714 -4.0594773 -4.2601719 -4.3396416 -4.0757866][-4.6039729 -4.6779013 -4.6151261 -4.0504837 -3.0086169 -1.8943093 -1.1373992 -0.95991826 -1.2494934 -1.8922956 -3.0497684 -4.16548 -4.9462309 -5.2115545 -4.835104][-5.0726619 -4.9219151 -4.5890756 -3.6366668 -2.1231766 -0.67199588 0.45185328 0.82455206 0.56733656 -0.26522875 -1.8672297 -3.54253 -4.7835245 -5.3746419 -5.3521767][-4.655211 -4.2239542 -3.4423208 -2.1543918 -0.40374613 1.2807136 2.531621 2.8358874 2.3123565 0.86766386 -1.0032175 -2.8966165 -4.55689 -5.5491753 -5.6209598][-3.9478903 -3.4198675 -2.4970074 -0.99844241 1.0392499 2.9645534 4.2739983 4.6301508 4.0200586 2.3097568 0.028102398 -2.2762804 -3.9603233 -5.3273644 -5.8954391][-3.3893075 -2.8753414 -2.1916907 -0.86687922 1.0290217 2.9040403 4.206461 4.5883083 4.1635866 2.6237793 0.45450163 -1.8276365 -3.6431704 -4.816391 -5.4654312][-2.9049077 -2.6710708 -2.3546424 -1.7418938 -0.38870764 1.1135702 2.1907902 2.7059336 2.5695682 1.4824114 -0.23139477 -2.203243 -3.7934935 -4.9610152 -5.6360903][-2.093652 -2.2694333 -2.4884515 -2.4243245 -1.762589 -0.81456804 -0.074579239 0.48662186 0.5368886 -0.050792217 -1.2480984 -2.6254578 -3.7216904 -4.8338981 -5.5841403][-1.4156883 -1.6900456 -2.2907822 -2.6299634 -2.4735801 -2.1771035 -1.9482179 -1.4121985 -1.0219316 -1.1598489 -1.9179015 -2.8691275 -3.5858536 -4.3874531 -4.9978123][-0.94132924 -1.1794758 -1.803751 -2.5471659 -2.885896 -2.9701786 -2.9723268 -2.6181564 -2.1062703 -1.7538629 -1.9642386 -2.5466442 -3.177525 -3.9784179 -4.6616607][-0.13364172 -0.46430779 -1.1537137 -1.9622626 -2.499723 -3.0135288 -3.4103661 -3.1989932 -2.6148438 -2.0660565 -1.992615 -2.2433958 -2.6594274 -3.3018377 -3.8630898][0.50548124 0.50611496 -0.034477711 -0.99756408 -1.6471934 -2.2370524 -2.8647971 -3.0696893 -2.7053156 -2.1374514 -1.9387221 -2.003664 -2.2269065 -2.8454633 -3.3624105]]...]
INFO - root - 2017-12-16 05:12:26.605818: step 19710, loss = 0.28, batch loss = 0.23 (28.2 examples/sec; 0.284 sec/batch; 24h:40m:36s remains)
INFO - root - 2017-12-16 05:12:29.478105: step 19720, loss = 0.29, batch loss = 0.24 (29.1 examples/sec; 0.275 sec/batch; 23h:52m:13s remains)
INFO - root - 2017-12-16 05:12:32.318170: step 19730, loss = 0.37, batch loss = 0.31 (28.7 examples/sec; 0.279 sec/batch; 24h:14m:14s remains)
INFO - root - 2017-12-16 05:12:35.170578: step 19740, loss = 0.33, batch loss = 0.27 (28.2 examples/sec; 0.283 sec/batch; 24h:37m:12s remains)
INFO - root - 2017-12-16 05:12:38.033868: step 19750, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.284 sec/batch; 24h:41m:49s remains)
INFO - root - 2017-12-16 05:12:40.854631: step 19760, loss = 0.23, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 24h:44m:24s remains)
INFO - root - 2017-12-16 05:12:43.670961: step 19770, loss = 0.39, batch loss = 0.33 (28.3 examples/sec; 0.283 sec/batch; 24h:33m:42s remains)
INFO - root - 2017-12-16 05:12:46.517018: step 19780, loss = 0.24, batch loss = 0.19 (27.3 examples/sec; 0.293 sec/batch; 25h:27m:46s remains)
INFO - root - 2017-12-16 05:12:49.393886: step 19790, loss = 0.33, batch loss = 0.27 (28.5 examples/sec; 0.280 sec/batch; 24h:21m:39s remains)
INFO - root - 2017-12-16 05:12:52.218023: step 19800, loss = 0.39, batch loss = 0.33 (28.0 examples/sec; 0.285 sec/batch; 24h:46m:31s remains)
2017-12-16 05:12:52.677319: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.766046 -2.0148835 -2.6193004 -2.9212613 -3.2908468 -3.1795392 -2.9163017 -2.7250381 -2.5493498 -2.4618711 -2.4651618 -2.4581151 -2.4983191 -2.4251609 -2.6181693][-2.2648158 -2.1830931 -2.7900434 -3.615386 -4.1305318 -4.0747137 -3.7612998 -3.1742692 -2.8258276 -2.6667626 -2.5568972 -2.3656094 -2.2889431 -2.2005932 -2.3184183][-2.7241364 -3.0685959 -3.660306 -4.0579705 -4.5348244 -4.5329561 -4.1927757 -3.720432 -3.3250597 -2.9026732 -2.8689871 -2.8611498 -2.9264069 -2.7093093 -2.7497575][-3.5928502 -3.4599 -3.6841781 -3.8930736 -4.0653787 -3.8973529 -3.637068 -3.2022877 -2.8758161 -2.9293661 -3.0825109 -3.4127836 -3.9527047 -4.0676575 -4.0558963][-3.9421549 -3.7185664 -3.6788676 -3.2247734 -2.7769108 -2.0243406 -1.4431973 -1.3314018 -1.4862607 -1.8780429 -2.7927828 -3.8169396 -4.5265636 -5.0116396 -5.0425062][-4.2581692 -3.4531238 -2.7073395 -1.979841 -1.0147693 0.16309738 1.2022028 1.8545818 1.5197749 0.28860426 -1.4024022 -3.1824718 -4.6874814 -5.3737946 -5.297574][-4.1634035 -3.2230506 -2.2961249 -1.1109695 0.20793819 1.9163585 3.5540185 4.1966066 3.8728943 2.8187566 0.71179247 -1.7803411 -3.7907412 -4.980206 -5.1885319][-4.2429509 -3.4072194 -2.5885148 -1.1361246 0.35030365 2.3174715 4.2411776 5.0946455 4.7473869 3.3110285 1.3428631 -1.0214686 -3.2796459 -4.4174924 -4.6401415][-4.4672451 -4.0518651 -3.5904393 -2.3712449 -0.85749745 1.1038551 2.9914484 3.9575176 3.9473829 2.7406907 0.71962404 -1.5886559 -3.4995844 -4.5396242 -4.5474248][-4.5925756 -4.6333313 -4.7758412 -4.1734867 -2.8867025 -1.1611242 0.37821484 1.2314434 1.2572966 0.31734562 -1.2592983 -3.0584855 -4.4679708 -4.9921646 -4.88139][-5.241333 -4.9804912 -5.0130978 -5.0337577 -4.6502891 -3.3309565 -1.9134784 -1.4172881 -1.5523851 -2.356169 -3.589659 -4.9178462 -5.7528334 -5.5881367 -4.929388][-5.3789039 -5.0835485 -5.0930614 -5.0094333 -4.7182431 -4.3367605 -3.7112939 -3.3778863 -3.4882662 -3.9475408 -4.7646132 -5.6883373 -6.0203905 -5.47817 -4.5281358][-5.2280021 -4.6428595 -4.345221 -4.4322681 -4.522347 -4.316349 -4.0123315 -4.1835217 -4.3906164 -4.5520391 -4.8383961 -5.1264386 -5.0857639 -4.5556984 -3.7253318][-5.2406697 -4.1536818 -3.4718404 -3.3613591 -3.3295851 -3.56501 -3.6528873 -3.9401681 -4.2440677 -4.3257489 -4.30809 -4.366478 -4.1335182 -3.3986969 -2.645613][-5.2552671 -4.0169406 -3.031631 -2.7524252 -2.8002467 -3.0564852 -3.4160647 -3.6719465 -3.848078 -3.9911249 -4.09418 -3.9674726 -3.5910463 -3.0856252 -2.4694588]]...]
INFO - root - 2017-12-16 05:12:55.497288: step 19810, loss = 0.28, batch loss = 0.22 (29.5 examples/sec; 0.271 sec/batch; 23h:31m:51s remains)
INFO - root - 2017-12-16 05:12:58.365227: step 19820, loss = 0.22, batch loss = 0.16 (27.4 examples/sec; 0.291 sec/batch; 25h:18m:59s remains)
INFO - root - 2017-12-16 05:13:01.277271: step 19830, loss = 0.28, batch loss = 0.22 (27.2 examples/sec; 0.294 sec/batch; 25h:34m:27s remains)
INFO - root - 2017-12-16 05:13:04.072743: step 19840, loss = 0.27, batch loss = 0.22 (28.3 examples/sec; 0.282 sec/batch; 24h:30m:40s remains)
INFO - root - 2017-12-16 05:13:06.886080: step 19850, loss = 0.26, batch loss = 0.21 (26.3 examples/sec; 0.304 sec/batch; 26h:23m:57s remains)
INFO - root - 2017-12-16 05:13:09.709368: step 19860, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 24h:43m:56s remains)
INFO - root - 2017-12-16 05:13:12.548999: step 19870, loss = 0.38, batch loss = 0.32 (27.6 examples/sec; 0.290 sec/batch; 25h:11m:53s remains)
INFO - root - 2017-12-16 05:13:15.390987: step 19880, loss = 0.26, batch loss = 0.21 (29.6 examples/sec; 0.271 sec/batch; 23h:30m:19s remains)
INFO - root - 2017-12-16 05:13:18.245599: step 19890, loss = 0.29, batch loss = 0.23 (28.0 examples/sec; 0.285 sec/batch; 24h:46m:16s remains)
INFO - root - 2017-12-16 05:13:21.094333: step 19900, loss = 0.30, batch loss = 0.24 (28.2 examples/sec; 0.283 sec/batch; 24h:36m:33s remains)
2017-12-16 05:13:21.543423: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1149659 -3.058835 -3.2025094 -3.4288917 -3.7638457 -4.0853982 -4.2503724 -4.2533178 -4.1741457 -3.9077349 -3.504267 -3.0045567 -2.563132 -2.6082568 -2.9871125][-2.8500426 -2.8856263 -3.1083903 -3.4280276 -3.9540374 -4.1917162 -4.53375 -4.5551777 -4.4975209 -4.2349739 -3.6351976 -3.3034124 -3.1529613 -3.4939322 -3.9195848][-3.2200515 -3.1003251 -2.9195371 -3.0149345 -3.5484965 -4.010366 -4.5069451 -4.7679753 -4.7920661 -4.5072865 -3.942853 -3.5547171 -3.4144998 -3.8871198 -4.4301515][-2.8102655 -2.78583 -2.6617055 -2.6905961 -3.0034742 -3.734612 -4.4900761 -4.8362417 -4.99036 -4.8387561 -4.4272542 -4.132267 -3.8359995 -4.1869683 -4.773797][-2.9676208 -2.9535661 -2.9271424 -2.8921824 -3.1207869 -3.5044804 -4.0961032 -4.5543575 -4.7039833 -4.5417919 -4.1050162 -4.0394 -3.9069588 -4.2284012 -4.5176554][-2.834574 -2.7154632 -2.6014376 -2.6106458 -2.8318479 -2.98174 -3.2332373 -3.5426123 -3.6429117 -3.3920588 -3.1655459 -3.0281839 -2.8708615 -3.1884265 -3.2945404][-2.8485351 -2.6894598 -2.5662928 -2.4410758 -2.4693274 -2.6656847 -2.8203053 -2.8093965 -2.654958 -2.2861536 -2.0935574 -2.0170319 -1.9496241 -2.105824 -2.208132][-2.7113204 -2.4185042 -2.3015044 -2.1988912 -2.2284756 -2.2402029 -2.327724 -2.2340837 -1.8472166 -1.3181262 -0.78838062 -0.774194 -0.96587515 -1.0714431 -1.052994][-3.2151871 -2.8442593 -2.4304752 -2.1985979 -2.0223722 -1.8882523 -1.785923 -1.6725116 -1.4061887 -0.94802284 -0.43658018 -0.15771961 -0.19234753 -0.32559061 -0.30051231][-3.915884 -3.3532419 -2.7254343 -2.3788593 -2.2009807 -2.0113528 -1.7439351 -1.5438688 -1.3880134 -1.0877638 -0.70294261 -0.45488787 -0.29213572 -0.37674475 -0.35932398][-4.0130367 -3.6280479 -3.1001687 -2.7881126 -2.6614017 -2.4707634 -2.2345943 -1.9159255 -1.7084677 -1.5101795 -1.3268859 -1.1638644 -0.88490605 -0.70140553 -0.70254111][-3.764575 -3.3865671 -3.0271254 -2.7568715 -2.7341418 -2.8481355 -2.8238721 -2.6331987 -2.4487503 -2.1395462 -1.867532 -1.6656473 -1.4957469 -1.3638849 -1.3170872][-3.539355 -3.143497 -2.8003244 -2.5532455 -2.6828322 -3.0316067 -3.2993274 -3.2441931 -3.074738 -2.744442 -2.3026571 -1.9305186 -1.7600613 -1.7438014 -1.7583172][-3.0751503 -2.8414516 -2.6066613 -2.6285942 -2.9098146 -3.458358 -3.9300451 -4.0449805 -3.8629394 -3.3324022 -2.6694505 -2.2030842 -1.9007924 -1.8616519 -2.0771515][-2.4152272 -2.38445 -2.3712378 -2.565836 -2.9751997 -3.6309931 -4.1150761 -4.1880813 -3.87017 -3.3436894 -2.8269029 -2.4038482 -2.1527839 -2.0268049 -2.0817032]]...]
INFO - root - 2017-12-16 05:13:24.361867: step 19910, loss = 0.30, batch loss = 0.24 (28.5 examples/sec; 0.280 sec/batch; 24h:21m:06s remains)
INFO - root - 2017-12-16 05:13:27.197146: step 19920, loss = 0.25, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 24h:18m:12s remains)
INFO - root - 2017-12-16 05:13:30.086306: step 19930, loss = 0.22, batch loss = 0.16 (28.9 examples/sec; 0.277 sec/batch; 24h:02m:19s remains)
INFO - root - 2017-12-16 05:13:32.944291: step 19940, loss = 0.26, batch loss = 0.20 (28.2 examples/sec; 0.284 sec/batch; 24h:37m:42s remains)
INFO - root - 2017-12-16 05:13:35.780439: step 19950, loss = 0.29, batch loss = 0.23 (28.3 examples/sec; 0.283 sec/batch; 24h:31m:54s remains)
INFO - root - 2017-12-16 05:13:38.603200: step 19960, loss = 0.26, batch loss = 0.20 (27.8 examples/sec; 0.288 sec/batch; 24h:58m:49s remains)
INFO - root - 2017-12-16 05:13:41.410661: step 19970, loss = 0.31, batch loss = 0.25 (28.5 examples/sec; 0.280 sec/batch; 24h:19m:54s remains)
INFO - root - 2017-12-16 05:13:44.196557: step 19980, loss = 0.34, batch loss = 0.28 (28.9 examples/sec; 0.277 sec/batch; 24h:03m:41s remains)
INFO - root - 2017-12-16 05:13:47.084661: step 19990, loss = 0.24, batch loss = 0.18 (25.4 examples/sec; 0.315 sec/batch; 27h:20m:28s remains)
INFO - root - 2017-12-16 05:13:49.935290: step 20000, loss = 0.22, batch loss = 0.16 (28.5 examples/sec; 0.281 sec/batch; 24h:22m:39s remains)
2017-12-16 05:13:50.390834: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7539549 -2.8662615 -2.9566655 -3.3175542 -3.7408261 -4.2900038 -4.8010921 -4.8992562 -4.9256678 -4.5325561 -3.9182637 -3.3405502 -2.9075737 -2.8479033 -2.9279771][-1.6395321 -1.7715178 -1.9294302 -2.455168 -3.0752664 -3.3888321 -3.5497561 -3.8884444 -4.18602 -4.0084453 -3.5850706 -3.2115273 -2.9601889 -2.947731 -3.0883565][-1.55161 -1.7369404 -1.7255642 -1.7510145 -1.7423215 -1.8679338 -2.2594397 -2.5137739 -2.8050413 -3.3292608 -3.5349555 -3.4087996 -3.3841827 -3.5228927 -3.7328675][-1.3909194 -1.7422161 -1.9319837 -1.8508396 -1.6570594 -1.3964069 -0.9605794 -0.97667861 -1.3731294 -2.0005288 -2.6077135 -3.162941 -3.7881882 -4.1302238 -4.429945][-1.9015038 -2.1968415 -2.3239996 -2.1057396 -1.6779785 -1.1737938 -0.56575322 -0.28955984 -0.31014585 -0.94275069 -1.7503507 -2.7181664 -3.6677175 -4.2380333 -4.6208034][-2.7116313 -2.8454995 -2.8496618 -2.213794 -1.2044456 -0.16612244 0.74274015 1.120892 1.1337166 0.38249111 -0.80798292 -2.0825992 -3.0521011 -3.9420123 -4.681582][-3.0304348 -2.8762925 -2.6808693 -1.782181 -0.28612137 1.1001606 2.3923526 2.8545365 2.7499862 1.8565397 0.55386972 -0.97011662 -2.2538147 -2.8942571 -3.3312366][-2.8643384 -2.562541 -2.2349515 -1.4182858 0.13026237 1.6436348 2.7925539 3.1307149 3.1028132 2.3310924 0.95610476 -0.21522522 -0.92822146 -1.4647794 -1.650218][-2.8293042 -2.5299251 -2.32001 -1.7073169 -0.63225412 0.58131313 1.6915693 2.0983515 2.04253 1.4237185 0.42802858 -0.57794118 -0.78758788 -0.531862 -0.12431002][-2.8122549 -2.6609674 -2.7103522 -2.3039181 -1.472666 -0.65231419 0.031373978 0.25766802 0.23364544 -0.39725208 -1.0076354 -1.5025291 -1.3232388 -0.59234548 0.40591717][-2.9576275 -2.865726 -2.9435868 -2.8291376 -2.3447092 -1.7913892 -1.4231091 -1.5441511 -1.8296702 -2.4032917 -2.9271395 -2.8836362 -2.0614583 -0.97220731 0.30471373][-3.4974396 -3.4038639 -3.4637017 -3.6767752 -3.6765256 -3.5902922 -3.6407149 -3.7654643 -4.0886731 -4.5655127 -4.7163324 -4.3823528 -3.3726683 -2.0900364 -0.64492273][-4.1843181 -4.0069318 -4.0756245 -4.2057161 -4.3834686 -4.6255817 -4.9860344 -5.6166553 -6.3201385 -6.6393986 -6.4558783 -6.0900035 -5.125246 -3.9273922 -2.6807454][-4.7289219 -4.4431219 -4.3293509 -4.3175063 -4.3515372 -4.7367797 -5.5457125 -6.3920078 -7.1205816 -7.6419868 -7.6789761 -7.1693611 -6.1875029 -5.2499828 -4.3179393][-5.1287284 -4.7147975 -4.311471 -4.1399755 -4.3271661 -4.8181376 -5.7055063 -6.7605696 -7.7201166 -8.2059975 -8.1701469 -7.7150679 -6.9662223 -6.0899076 -5.220253]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 05:13:53.643967: step 20010, loss = 0.25, batch loss = 0.19 (28.8 examples/sec; 0.278 sec/batch; 24h:08m:07s remains)
INFO - root - 2017-12-16 05:13:56.449974: step 20020, loss = 0.24, batch loss = 0.18 (29.3 examples/sec; 0.273 sec/batch; 23h:41m:42s remains)
INFO - root - 2017-12-16 05:13:59.277799: step 20030, loss = 0.25, batch loss = 0.19 (28.5 examples/sec; 0.281 sec/batch; 24h:21m:05s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 05:14:02.101089: step 20040, loss = 0.23, batch loss = 0.17 (28.6 examples/sec; 0.280 sec/batch; 24h:18m:02s remains)
INFO - root - 2017-12-16 05:14:04.899611: step 20050, loss = 0.26, batch loss = 0.20 (28.4 examples/sec; 0.282 sec/batch; 24h:28m:27s remains)
INFO - root - 2017-12-16 05:14:07.680685: step 20060, loss = 0.37, batch loss = 0.31 (29.1 examples/sec; 0.275 sec/batch; 23h:51m:46s remains)
INFO - root - 2017-12-16 05:14:10.543539: step 20070, loss = 0.22, batch loss = 0.16 (25.9 examples/sec; 0.309 sec/batch; 26h:48m:07s remains)
INFO - root - 2017-12-16 05:14:13.363733: step 20080, loss = 0.25, batch loss = 0.19 (28.5 examples/sec; 0.281 sec/batch; 24h:22m:26s remains)
INFO - root - 2017-12-16 05:14:16.210530: step 20090, loss = 0.27, batch loss = 0.22 (27.1 examples/sec; 0.295 sec/batch; 25h:34m:28s remains)
INFO - root - 2017-12-16 05:14:19.016197: step 20100, loss = 0.22, batch loss = 0.16 (27.8 examples/sec; 0.288 sec/batch; 24h:57m:39s remains)
2017-12-16 05:14:19.488788: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.40394 -6.4881306 -6.5582705 -6.422833 -6.2599578 -6.1691685 -6.1519833 -6.2934837 -6.6608682 -6.831646 -6.893229 -7.0941544 -7.0816536 -6.8109355 -6.4336615][-6.8251896 -6.851243 -6.7536345 -6.6182518 -6.4275742 -6.3623786 -6.3702989 -6.5871067 -7.0277939 -7.65899 -8.3288593 -8.4886808 -8.3344965 -8.08028 -7.6161065][-6.6387596 -6.6044016 -6.4247451 -6.1005235 -5.6193819 -5.3618183 -5.4499488 -5.8499694 -6.4135003 -7.5058708 -8.3505135 -8.73621 -8.9373808 -8.7340136 -8.2863274][-6.1039228 -5.8386755 -5.4683213 -4.7627687 -3.8502045 -3.5087457 -3.2993417 -3.797544 -4.729054 -6.0251079 -7.0497723 -7.6647987 -7.9690046 -7.9734669 -7.96194][-5.1174822 -4.5395217 -3.7506659 -2.7743905 -1.383863 -0.4397316 -0.061341763 -0.512197 -1.525547 -3.2973504 -4.5882888 -5.3660111 -5.8723626 -6.3369794 -6.7149715][-3.9947226 -3.2751803 -2.2491903 -0.38194895 1.6189671 2.886714 3.5119414 3.0631876 1.8078456 -0.30646515 -1.9018033 -2.89098 -3.4456263 -4.2100959 -4.9921832][-4.0099578 -3.1168418 -1.462992 0.83666897 3.3436437 5.2696133 6.2095785 5.7831154 4.1878109 1.9188752 0.21511698 -0.884315 -1.6308825 -2.4445662 -3.3368864][-4.5293088 -3.9579093 -2.7803721 -0.15220737 2.8826885 5.1624966 6.2844114 6.2995958 5.0314951 2.899786 1.2014599 0.064107895 -0.69572473 -1.5792167 -2.659651][-6.1232715 -5.8475342 -4.6861496 -2.1531155 0.31548929 2.5083895 4.335969 4.6849518 3.735568 2.2702518 0.81725359 -0.29580212 -1.0739172 -1.9217279 -3.0070319][-7.1582246 -7.0988903 -6.6463795 -4.9563923 -2.7552905 -0.49011207 0.80617666 1.3427567 1.3302217 0.5375638 -0.47439623 -1.3580596 -2.1915085 -2.9675808 -3.8741035][-8.136344 -8.4546757 -8.371892 -7.1293459 -5.6088853 -3.99811 -2.7017283 -1.5878718 -1.2898538 -1.7065959 -2.1415184 -2.9094687 -3.7240176 -4.385448 -5.1644335][-8.5939789 -9.0464211 -9.4241581 -9.062149 -8.1598158 -6.7002125 -5.329587 -4.4159641 -3.7316353 -3.4276171 -3.7401218 -4.5802326 -5.3309269 -5.8557348 -6.3637414][-8.3589077 -8.8910255 -9.3354206 -9.4716434 -9.2007885 -8.2983913 -7.3834076 -6.2095637 -5.1711359 -4.9412203 -5.1298542 -5.5702534 -6.1160045 -6.7031465 -6.9182663][-7.2289662 -7.4576454 -7.7146473 -8.0296173 -8.0532608 -7.784543 -7.3748093 -6.6347804 -6.027966 -5.6493087 -5.5325618 -6.0662408 -6.6392589 -6.8509007 -6.7168512][-6.514698 -6.3679943 -6.3461351 -6.4748821 -6.498867 -6.4994211 -6.4155507 -6.14421 -5.8251452 -5.6779995 -5.8105135 -6.0417838 -6.175127 -6.2419257 -6.0722456]]...]
INFO - root - 2017-12-16 05:14:22.312990: step 20110, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 24h:10m:49s remains)
INFO - root - 2017-12-16 05:14:25.164574: step 20120, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 24h:41m:52s remains)
INFO - root - 2017-12-16 05:14:27.977242: step 20130, loss = 0.33, batch loss = 0.28 (29.1 examples/sec; 0.275 sec/batch; 23h:52m:42s remains)
INFO - root - 2017-12-16 05:14:30.801221: step 20140, loss = 0.32, batch loss = 0.26 (29.1 examples/sec; 0.275 sec/batch; 23h:52m:04s remains)
INFO - root - 2017-12-16 05:14:33.636639: step 20150, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.278 sec/batch; 24h:09m:37s remains)
INFO - root - 2017-12-16 05:14:36.440976: step 20160, loss = 0.19, batch loss = 0.13 (28.1 examples/sec; 0.284 sec/batch; 24h:39m:40s remains)
INFO - root - 2017-12-16 05:14:39.281700: step 20170, loss = 0.21, batch loss = 0.16 (28.1 examples/sec; 0.284 sec/batch; 24h:40m:16s remains)
INFO - root - 2017-12-16 05:14:42.131534: step 20180, loss = 0.26, batch loss = 0.20 (28.5 examples/sec; 0.281 sec/batch; 24h:21m:53s remains)
INFO - root - 2017-12-16 05:14:44.919122: step 20190, loss = 0.24, batch loss = 0.18 (29.5 examples/sec; 0.271 sec/batch; 23h:29m:27s remains)
INFO - root - 2017-12-16 05:14:47.807501: step 20200, loss = 0.23, batch loss = 0.17 (27.9 examples/sec; 0.287 sec/batch; 24h:52m:53s remains)
2017-12-16 05:14:48.261265: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2054868 -5.6316981 -5.3983812 -5.6882997 -6.4734397 -7.2904434 -7.890893 -8.0654488 -8.1425686 -8.0946846 -7.784236 -7.0489883 -6.14491 -5.294579 -4.7575316][-5.8816309 -5.1795669 -5.0240908 -5.3221369 -5.8540673 -6.8054752 -7.5887957 -7.8351474 -7.9511633 -7.9914861 -8.1017027 -7.8022957 -7.1591187 -6.2696586 -5.6363416][-4.754941 -3.8790998 -3.6361856 -4.0609069 -4.4090247 -4.9432487 -5.6300535 -6.4144793 -7.0076666 -7.26997 -7.6788387 -7.8316712 -7.4595551 -6.48363 -5.7049527][-3.132134 -2.1814861 -1.9443297 -2.1925144 -2.6109524 -2.8690238 -3.1708672 -4.0205708 -4.8717418 -5.5895853 -6.4827 -6.8335657 -6.7376728 -5.9624076 -5.159091][-2.088516 -0.97619653 -0.61506915 -0.53244257 -0.41786766 -0.28093576 -0.36984396 -0.84600425 -1.7460973 -3.0494273 -4.4634423 -5.2796879 -5.558404 -5.0961237 -4.5962644][-1.6090012 -0.38293362 0.13949966 0.72989941 1.7852912 2.5046611 2.9090614 2.4724503 1.3302898 -0.56116056 -2.5396333 -3.7500737 -4.3271427 -4.0250845 -3.5225024][-1.9437935 -0.57835245 0.35217285 1.5935373 3.0813279 4.3835897 5.187274 4.6937771 3.1765165 0.93574715 -1.1136348 -2.4813044 -3.1306524 -3.0946441 -2.7799237][-2.8342056 -1.4925375 -0.1830411 1.2968349 2.9221706 4.2621956 5.38167 5.265336 3.8353205 1.5183072 -0.71102023 -2.1878777 -2.8633561 -2.9907737 -2.8208261][-4.3231735 -3.3038211 -1.8065202 0.21639824 2.0475702 3.3823395 4.2779903 4.0054865 2.6728754 0.5979147 -1.5205171 -2.9619706 -3.3699875 -3.3249445 -3.2468886][-5.3785677 -4.421495 -3.1235826 -1.2864001 0.49496698 1.4804535 1.799778 1.3799939 0.25889921 -1.2739387 -2.9251475 -4.0161271 -4.2263603 -3.9224102 -3.5532787][-6.2919397 -5.4489527 -4.2723246 -3.0792856 -1.9683335 -1.3565681 -1.4939027 -2.026381 -2.9402676 -3.8042943 -4.636188 -5.2809749 -5.2981391 -4.8587265 -4.0018768][-6.2125072 -5.7978625 -5.2838993 -4.8022609 -4.3726878 -4.1988945 -4.4125228 -5.0123558 -5.5906825 -5.9403872 -6.1065426 -6.2182512 -6.1370168 -5.718327 -4.7351151][-5.6251283 -5.4315386 -5.3294172 -5.4211817 -5.466413 -5.7430463 -6.1485958 -6.5000238 -6.8630905 -7.1285515 -6.9537058 -6.6619873 -6.3403006 -6.170722 -5.5922422][-5.0673566 -4.9960179 -4.9431667 -5.1554995 -5.5119061 -5.98202 -6.4595184 -6.5924158 -6.5718536 -6.7318335 -6.6972985 -6.5503635 -6.3421211 -6.2145395 -6.0279751][-4.7810478 -4.6925058 -4.5959 -4.6585994 -4.8828921 -5.27723 -5.7067938 -5.8752308 -5.8748045 -5.8679571 -5.8203921 -5.7849612 -5.747736 -5.7430019 -5.5930324]]...]
INFO - root - 2017-12-16 05:14:51.083052: step 20210, loss = 0.24, batch loss = 0.18 (29.6 examples/sec; 0.270 sec/batch; 23h:25m:19s remains)
INFO - root - 2017-12-16 05:14:53.940868: step 20220, loss = 0.34, batch loss = 0.28 (26.7 examples/sec; 0.299 sec/batch; 25h:57m:31s remains)
INFO - root - 2017-12-16 05:14:56.750145: step 20230, loss = 0.31, batch loss = 0.25 (28.6 examples/sec; 0.280 sec/batch; 24h:16m:28s remains)
INFO - root - 2017-12-16 05:14:59.579712: step 20240, loss = 0.30, batch loss = 0.24 (29.4 examples/sec; 0.272 sec/batch; 23h:37m:41s remains)
INFO - root - 2017-12-16 05:15:02.401635: step 20250, loss = 0.27, batch loss = 0.21 (28.7 examples/sec; 0.279 sec/batch; 24h:09m:54s remains)
INFO - root - 2017-12-16 05:15:05.199425: step 20260, loss = 0.34, batch loss = 0.28 (28.6 examples/sec; 0.279 sec/batch; 24h:14m:30s remains)
INFO - root - 2017-12-16 05:15:08.025759: step 20270, loss = 0.29, batch loss = 0.24 (28.5 examples/sec; 0.280 sec/batch; 24h:19m:25s remains)
INFO - root - 2017-12-16 05:15:10.849448: step 20280, loss = 0.30, batch loss = 0.24 (29.2 examples/sec; 0.274 sec/batch; 23h:43m:34s remains)
INFO - root - 2017-12-16 05:15:13.715461: step 20290, loss = 0.28, batch loss = 0.23 (29.4 examples/sec; 0.272 sec/batch; 23h:33m:42s remains)
INFO - root - 2017-12-16 05:15:16.530653: step 20300, loss = 0.28, batch loss = 0.22 (28.9 examples/sec; 0.277 sec/batch; 24h:02m:32s remains)
2017-12-16 05:15:16.982589: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4499493 -4.2951956 -4.2858186 -4.4590034 -4.538476 -4.4621716 -4.24712 -4.0214891 -3.9925759 -4.0399747 -4.1868844 -4.2298613 -4.4795 -4.8768134 -5.0654564][-4.3049064 -4.2587423 -4.3140903 -4.3492112 -4.2541866 -4.0395956 -3.7217233 -3.4816077 -3.5266528 -3.6627929 -3.8950994 -3.9990232 -4.1937666 -4.4892316 -4.5389452][-4.1703138 -4.1402626 -4.177103 -4.1407747 -3.7871308 -3.3999562 -3.0222006 -2.6872478 -2.6957085 -2.9769876 -3.3236184 -3.4484191 -3.630877 -3.7376688 -3.6981401][-3.5497696 -3.6084361 -3.5985332 -3.4744318 -3.0136833 -2.4665651 -1.9596961 -1.7765882 -1.9511716 -2.2261364 -2.6484041 -2.7583911 -2.8299966 -2.7715888 -2.5867126][-2.6351483 -2.7254229 -2.7673104 -2.5994356 -1.9867661 -1.3942282 -0.96345639 -0.80483079 -1.0199201 -1.3588674 -1.8405848 -2.0175297 -1.9754109 -1.6806614 -1.4478772][-1.6567576 -1.6516736 -1.7210996 -1.4843705 -0.73516488 -0.078264713 0.37299252 0.31517982 -0.066006184 -0.54308653 -1.1144123 -1.2035384 -1.0975232 -0.69415212 -0.37691355][-0.58671021 -0.6314106 -0.86138153 -0.59406376 0.10927534 0.77734375 1.2336626 1.0579405 0.63904762 0.12011909 -0.40358973 -0.54951477 -0.55393314 -0.031549931 0.27317047][0.096088886 0.11742258 -0.22894192 -0.041484356 0.46903706 0.85688639 1.1962771 0.97817755 0.60149574 0.16204691 -0.31260014 -0.43861651 -0.34511471 0.18146181 0.47884989][0.51039028 0.44979811 -0.069141865 -0.066494942 0.1271944 0.35828876 0.63568449 0.44959259 0.14338255 -0.26911068 -0.70501971 -0.87109065 -0.90042734 -0.43405843 -0.10681438][0.61946678 0.45290709 -0.25241041 -0.43788314 -0.53399539 -0.53101969 -0.39265919 -0.45264077 -0.69674206 -0.96455836 -1.2501862 -1.59763 -1.7623363 -1.3399858 -0.96112561][0.17562675 0.060884 -0.59554362 -0.99707031 -1.2434871 -1.3817542 -1.395179 -1.490926 -1.6712048 -2.0120122 -2.4205339 -2.7484922 -2.825171 -2.4154708 -1.9694514][-0.22563076 -0.33203125 -0.88893485 -1.4528508 -1.8976152 -1.9776208 -1.9487531 -2.1767509 -2.5695102 -2.8948169 -3.4160361 -3.8880777 -4.009882 -3.5354905 -2.8370314][-0.46048307 -0.37867355 -0.71609879 -1.1954625 -1.6449969 -1.899575 -2.0553875 -2.2621224 -2.7241402 -3.2144742 -3.8590729 -4.37855 -4.5397105 -4.1220336 -3.3010819][-0.54240251 -0.3553977 -0.43926668 -0.89762926 -1.2901909 -1.5202224 -1.773433 -2.1137264 -2.6278734 -3.0878611 -3.6766777 -4.2048583 -4.301918 -3.7923923 -2.9511385][-0.77160716 -0.4797678 -0.42416048 -0.696115 -0.91465974 -1.1218178 -1.3606019 -1.6823678 -2.2024567 -2.7321534 -3.394433 -3.8201497 -3.8133898 -3.2745128 -2.3293109]]...]
INFO - root - 2017-12-16 05:15:19.819321: step 20310, loss = 0.23, batch loss = 0.17 (29.5 examples/sec; 0.271 sec/batch; 23h:31m:10s remains)
INFO - root - 2017-12-16 05:15:22.700461: step 20320, loss = 0.27, batch loss = 0.21 (26.0 examples/sec; 0.308 sec/batch; 26h:43m:53s remains)
INFO - root - 2017-12-16 05:15:25.532593: step 20330, loss = 0.21, batch loss = 0.15 (29.7 examples/sec; 0.269 sec/batch; 23h:22m:05s remains)
INFO - root - 2017-12-16 05:15:28.326048: step 20340, loss = 0.23, batch loss = 0.17 (26.5 examples/sec; 0.302 sec/batch; 26h:11m:42s remains)
INFO - root - 2017-12-16 05:15:31.200506: step 20350, loss = 0.30, batch loss = 0.24 (28.6 examples/sec; 0.279 sec/batch; 24h:13m:53s remains)
INFO - root - 2017-12-16 05:15:34.002707: step 20360, loss = 0.27, batch loss = 0.21 (27.4 examples/sec; 0.291 sec/batch; 25h:16m:14s remains)
INFO - root - 2017-12-16 05:15:36.851291: step 20370, loss = 0.25, batch loss = 0.19 (25.6 examples/sec; 0.313 sec/batch; 27h:07m:52s remains)
INFO - root - 2017-12-16 05:15:39.671448: step 20380, loss = 0.38, batch loss = 0.32 (28.9 examples/sec; 0.277 sec/batch; 24h:02m:19s remains)
INFO - root - 2017-12-16 05:15:42.501919: step 20390, loss = 0.28, batch loss = 0.22 (27.9 examples/sec; 0.287 sec/batch; 24h:53m:18s remains)
INFO - root - 2017-12-16 05:15:45.351726: step 20400, loss = 0.27, batch loss = 0.21 (26.9 examples/sec; 0.298 sec/batch; 25h:48m:36s remains)
2017-12-16 05:15:45.799959: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2850461 -4.6627264 -4.7541904 -4.9912715 -5.2823224 -5.6077766 -5.95362 -6.1242094 -6.1208386 -5.8476667 -5.587 -5.30739 -5.1706629 -5.10056 -4.9912066][-4.7907896 -5.0619512 -5.0055943 -5.1203451 -5.1635485 -5.2800775 -5.5855646 -5.86566 -5.9354186 -5.8074751 -5.5271649 -5.3034687 -5.4704576 -5.4960461 -5.3234205][-5.6461287 -5.61164 -5.2528529 -4.9570975 -4.6969056 -4.6730685 -4.6977224 -4.9890141 -5.2151 -5.2166109 -5.1331768 -5.0666227 -5.3366313 -5.5009656 -5.5760703][-6.1332974 -5.9055662 -5.22581 -4.3998952 -3.7487209 -3.2902255 -3.1008763 -3.3048849 -3.5869739 -4.0036259 -4.2980275 -4.5008574 -4.9302993 -5.064641 -5.0117903][-6.2418947 -5.8401003 -4.6744523 -3.2962933 -2.0717018 -1.3826921 -1.0669699 -1.0968575 -1.503346 -2.2198334 -2.9489465 -3.5702071 -4.0896492 -4.4310961 -4.4534345][-6.137485 -5.2760091 -3.8117323 -1.9689891 -0.16830873 1.0154004 1.5714979 1.3127651 0.55024672 -0.4728384 -1.5241268 -2.3967 -3.2790606 -3.7623558 -3.9618289][-5.7196546 -4.9118328 -3.2220142 -1.1429293 0.79803419 2.1792049 3.0668287 2.8744502 1.7716761 0.51692533 -0.645303 -1.7951374 -2.8093004 -3.38816 -3.7388163][-5.2084832 -4.34758 -2.8302577 -0.79180193 1.3115206 2.6813512 3.3785424 3.2269936 2.2723207 0.90738106 -0.55725169 -1.724015 -2.5663085 -3.2307286 -3.4387941][-5.0542264 -4.2357669 -2.7731514 -0.90759373 0.78524017 2.233068 3.1020141 2.8631063 1.9315505 0.601388 -0.8949039 -2.4074202 -3.6843326 -4.4044676 -4.4160404][-5.4689817 -4.7756286 -3.6025867 -2.1103818 -0.77574062 0.44428205 1.4393406 1.3691063 0.6799221 -0.52780342 -1.8665588 -3.3286493 -4.7739291 -5.6773214 -5.7524357][-6.2234545 -5.6132178 -4.584836 -3.4088926 -2.5227547 -1.6699147 -0.87454295 -0.69391131 -1.1113293 -2.1266332 -3.171129 -4.3535604 -5.4697208 -6.0898728 -6.0142679][-7.16565 -6.9576344 -6.2292371 -4.9801264 -3.9463985 -3.2264063 -2.7460766 -2.6137583 -2.9062676 -3.7246041 -4.5366817 -5.3317308 -5.9851723 -6.186945 -5.8430548][-7.9620409 -7.7948952 -7.2444439 -6.1716385 -5.3498497 -4.4874096 -3.8437281 -3.7741594 -4.1350651 -4.7398515 -5.3236318 -5.8842235 -6.1478939 -6.0431504 -5.5522861][-8.5902147 -8.2176332 -7.3187256 -6.228673 -5.6904469 -5.0717573 -4.7601004 -4.6225452 -4.8034973 -5.2923551 -5.5684743 -5.6908827 -5.701478 -5.4359646 -4.9750395][-8.7067537 -8.3501987 -7.391336 -6.1735821 -5.541142 -5.17341 -5.029644 -4.923389 -5.074419 -5.4260383 -5.526979 -5.4447665 -5.2953625 -5.0782714 -4.7951536]]...]
INFO - root - 2017-12-16 05:15:48.631960: step 20410, loss = 0.22, batch loss = 0.17 (28.9 examples/sec; 0.276 sec/batch; 23h:57m:48s remains)
INFO - root - 2017-12-16 05:15:51.495521: step 20420, loss = 0.29, batch loss = 0.24 (28.5 examples/sec; 0.281 sec/batch; 24h:21m:07s remains)
INFO - root - 2017-12-16 05:15:54.369552: step 20430, loss = 0.24, batch loss = 0.18 (27.1 examples/sec; 0.295 sec/batch; 25h:33m:52s remains)
INFO - root - 2017-12-16 05:15:57.195752: step 20440, loss = 0.25, batch loss = 0.19 (29.2 examples/sec; 0.274 sec/batch; 23h:45m:49s remains)
INFO - root - 2017-12-16 05:16:00.017805: step 20450, loss = 0.35, batch loss = 0.29 (28.8 examples/sec; 0.277 sec/batch; 24h:02m:13s remains)
INFO - root - 2017-12-16 05:16:02.798234: step 20460, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 24h:24m:33s remains)
INFO - root - 2017-12-16 05:16:05.608601: step 20470, loss = 0.32, batch loss = 0.26 (27.9 examples/sec; 0.287 sec/batch; 24h:50m:19s remains)
INFO - root - 2017-12-16 05:16:08.434909: step 20480, loss = 0.30, batch loss = 0.25 (28.5 examples/sec; 0.280 sec/batch; 24h:17m:32s remains)
INFO - root - 2017-12-16 05:16:11.280512: step 20490, loss = 0.31, batch loss = 0.26 (28.1 examples/sec; 0.284 sec/batch; 24h:38m:35s remains)
INFO - root - 2017-12-16 05:16:14.079803: step 20500, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 24h:11m:59s remains)
2017-12-16 05:16:14.526705: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2424889 -4.4236412 -4.6103244 -4.7270207 -4.9960661 -5.1906967 -5.3332019 -5.3440189 -5.2662063 -5.1809926 -4.999754 -4.8918095 -4.9784908 -5.198422 -5.3843741][-3.2054186 -3.2507648 -3.424022 -3.6089654 -3.9936271 -4.5891972 -5.06463 -5.1264753 -5.0315962 -4.9096036 -4.952055 -4.9344859 -5.0769811 -5.4232283 -5.6723542][-1.7491364 -1.8695896 -2.1519532 -2.5076017 -2.921514 -3.4582653 -3.8970871 -4.0854158 -4.1106892 -4.1137462 -4.3388004 -4.6408939 -5.02069 -5.4566216 -5.704978][-0.6499083 -0.69022274 -0.8511529 -1.0382352 -1.4919839 -1.9904006 -2.406554 -2.5061445 -2.5894561 -2.704648 -3.2019892 -3.9198701 -4.6076417 -5.1785908 -5.5211926][0.11009741 0.26277351 0.3086009 0.31922913 0.20868492 -0.066341877 -0.33703566 -0.53556156 -0.59243369 -0.80040884 -1.5676675 -2.6225095 -3.708425 -4.3547225 -4.7594128][-0.068893909 0.31805134 0.60770416 0.83410597 1.0351429 1.175447 1.3466816 1.4085431 1.5061464 1.2137041 0.24048948 -1.1206179 -2.5432737 -3.4447715 -4.078517][-1.2216313 -0.65691423 -0.06751442 0.68463087 1.5772862 2.3650627 3.0575304 3.413075 3.5723419 3.2171464 2.0474381 0.53285217 -1.0016835 -2.2394807 -3.2795033][-2.3813052 -1.7726216 -1.2734904 -0.45686364 0.85934353 2.3568921 3.7671413 4.4369 4.845953 4.562727 3.2313066 1.4845567 -0.18089676 -1.4934411 -2.6909575][-4.1640134 -3.6472871 -3.3436065 -2.5190506 -1.0055051 0.81413651 2.5670834 3.6852198 4.3863735 4.1588669 3.1296124 1.5999141 -0.10728979 -1.7346873 -3.1293132][-5.7594109 -5.7758169 -5.8599873 -5.1226568 -3.7113969 -1.8763444 -0.0071115494 1.4988894 2.5435848 2.4770985 1.6737342 0.33932114 -1.1119821 -2.653337 -3.9841793][-7.0066257 -7.4207087 -7.796402 -7.38122 -6.3566074 -4.6698952 -2.8951845 -1.3721087 -0.25641108 -0.076383114 -0.73543429 -1.9081597 -3.1441998 -4.5397553 -5.5414114][-8.7759533 -9.4851189 -9.9852982 -9.6430025 -8.7360554 -7.3482628 -5.9120975 -4.3366466 -3.4221957 -3.4031043 -3.7926235 -4.4774976 -5.294188 -6.209888 -6.6658058][-9.7969456 -10.946798 -11.522581 -11.196439 -10.630335 -9.5700893 -8.3334827 -6.9710217 -6.33173 -6.3181915 -6.5723658 -6.954114 -7.2860818 -7.6388254 -7.5382085][-10.499714 -11.609734 -11.997763 -11.598969 -11.181589 -10.513197 -9.7635593 -8.9206409 -8.590641 -8.6366768 -8.8316345 -8.882206 -8.7434464 -8.6095171 -8.014142][-10.389521 -11.296716 -11.590685 -11.244314 -10.737135 -10.173489 -9.8098488 -9.4850893 -9.5584145 -9.8773251 -10.016521 -9.6624451 -9.2192755 -8.6933517 -7.703763]]...]
INFO - root - 2017-12-16 05:16:17.422742: step 20510, loss = 0.24, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 24h:31m:54s remains)
INFO - root - 2017-12-16 05:16:20.219252: step 20520, loss = 0.22, batch loss = 0.16 (28.3 examples/sec; 0.283 sec/batch; 24h:30m:02s remains)
INFO - root - 2017-12-16 05:16:22.968356: step 20530, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.286 sec/batch; 24h:45m:52s remains)
INFO - root - 2017-12-16 05:16:25.751693: step 20540, loss = 0.28, batch loss = 0.22 (28.8 examples/sec; 0.277 sec/batch; 24h:01m:50s remains)
INFO - root - 2017-12-16 05:16:28.564270: step 20550, loss = 0.36, batch loss = 0.31 (29.2 examples/sec; 0.274 sec/batch; 23h:46m:11s remains)
INFO - root - 2017-12-16 05:16:31.397481: step 20560, loss = 0.24, batch loss = 0.19 (28.5 examples/sec; 0.281 sec/batch; 24h:20m:13s remains)
INFO - root - 2017-12-16 05:16:34.192750: step 20570, loss = 0.47, batch loss = 0.42 (27.3 examples/sec; 0.293 sec/batch; 25h:23m:38s remains)
INFO - root - 2017-12-16 05:16:37.020428: step 20580, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.281 sec/batch; 24h:20m:48s remains)
INFO - root - 2017-12-16 05:16:39.909005: step 20590, loss = 0.33, batch loss = 0.27 (28.7 examples/sec; 0.279 sec/batch; 24h:09m:12s remains)
INFO - root - 2017-12-16 05:16:42.732949: step 20600, loss = 0.23, batch loss = 0.17 (29.3 examples/sec; 0.273 sec/batch; 23h:39m:11s remains)
2017-12-16 05:16:43.183929: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4645429 -3.4663768 -3.552774 -3.5865133 -3.7657402 -3.7951937 -3.759989 -3.9852376 -4.1782579 -4.3397312 -4.40353 -4.521719 -4.5618668 -4.4639888 -4.2764463][-2.9927297 -2.9630888 -3.0452366 -3.1091127 -3.2691653 -3.3653259 -3.5736992 -3.8901482 -4.1644263 -4.4321942 -4.582912 -4.64054 -4.5449262 -4.2410259 -3.7563641][-2.1898046 -2.0819812 -2.1176846 -2.2323472 -2.3688769 -2.4470491 -2.7308178 -3.2199125 -3.6748493 -3.9541028 -4.3211775 -4.4957294 -4.563314 -4.092628 -3.4433289][-0.99158573 -0.98773527 -1.1315746 -1.1571853 -0.99619317 -1.1791112 -1.6144984 -2.0306096 -2.4081261 -2.9526541 -3.726784 -3.917845 -3.8312855 -3.4651003 -3.0155632][0.0099506378 -0.15478611 -0.13842535 -0.017889977 0.30526114 0.27955055 0.13603497 -0.053172112 -0.38107157 -0.89862442 -1.6076756 -2.051132 -2.2113385 -2.0698264 -1.9260187][0.54129553 0.3491745 0.37677717 0.72285032 1.4159217 1.6239257 1.751379 1.8309751 1.6139226 1.0791464 0.19232178 -0.45116806 -0.71940207 -0.79692054 -0.86527753][0.71589327 0.60164165 0.67478371 1.14042 2.1609802 2.7493715 3.1100063 3.3630209 3.2968078 2.6211119 1.381041 0.42250013 -0.16859579 -0.52168369 -0.77705431][0.77767563 0.5016036 0.35245895 0.80157566 1.8501277 2.7187238 3.4595542 3.896925 3.9485474 3.2819843 2.0762115 1.1216459 0.32038927 -0.443825 -1.0729728][0.10779285 -0.31790257 -0.24444437 0.22928572 0.967155 1.7541056 2.6096768 3.1893964 3.2958064 2.7117777 1.7020936 0.68505573 -0.22877645 -1.0508478 -1.8395832][-0.62376666 -1.1362734 -1.4479456 -1.0796716 -0.27324629 0.46388149 1.0033512 1.5317626 1.6568975 1.2064433 0.45715427 -0.52794719 -1.3341067 -2.1284153 -2.7919736][-1.8335252 -2.1320674 -2.31158 -2.2226584 -1.8346136 -1.161366 -0.59148312 -0.23471546 -0.22275066 -0.61685181 -1.3492405 -2.0746496 -2.6754951 -3.3369622 -3.6718206][-2.6825185 -3.0696466 -3.3217113 -3.2955866 -3.0892386 -2.6899905 -2.3602273 -2.0042965 -1.8807292 -2.2151971 -2.7594838 -3.1768575 -3.5235209 -3.9938941 -4.2100854][-3.2857046 -3.5695131 -3.8452494 -3.9933622 -4.1107039 -3.9449787 -3.7836251 -3.4820869 -3.2607765 -3.3121953 -3.3598547 -3.5903232 -3.7444167 -3.9614482 -3.9573956][-3.6007664 -3.6182554 -3.8278184 -4.0659914 -4.2791915 -4.2914906 -4.3311582 -4.20098 -3.9798717 -3.869328 -3.6802723 -3.5982254 -3.6324229 -3.7702718 -3.6553369][-3.5444174 -3.365201 -3.4067025 -3.5518634 -3.7557721 -3.9363134 -4.1279721 -4.119596 -3.9659619 -3.8742576 -3.7153788 -3.3929892 -3.012958 -3.0347643 -3.150156]]...]
INFO - root - 2017-12-16 05:16:46.018171: step 20610, loss = 0.19, batch loss = 0.14 (28.6 examples/sec; 0.279 sec/batch; 24h:11m:58s remains)
INFO - root - 2017-12-16 05:16:48.816418: step 20620, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.285 sec/batch; 24h:42m:37s remains)
INFO - root - 2017-12-16 05:16:51.581258: step 20630, loss = 0.53, batch loss = 0.47 (28.6 examples/sec; 0.279 sec/batch; 24h:12m:02s remains)
INFO - root - 2017-12-16 05:16:54.433335: step 20640, loss = 0.26, batch loss = 0.20 (28.4 examples/sec; 0.281 sec/batch; 24h:22m:34s remains)
INFO - root - 2017-12-16 05:16:57.230503: step 20650, loss = 0.28, batch loss = 0.22 (28.3 examples/sec; 0.283 sec/batch; 24h:28m:50s remains)
INFO - root - 2017-12-16 05:17:00.073700: step 20660, loss = 0.27, batch loss = 0.21 (27.2 examples/sec; 0.294 sec/batch; 25h:27m:51s remains)
INFO - root - 2017-12-16 05:17:02.965394: step 20670, loss = 0.49, batch loss = 0.43 (26.4 examples/sec; 0.303 sec/batch; 26h:16m:53s remains)
INFO - root - 2017-12-16 05:17:05.821993: step 20680, loss = 0.26, batch loss = 0.20 (28.8 examples/sec; 0.278 sec/batch; 24h:03m:50s remains)
INFO - root - 2017-12-16 05:17:08.676466: step 20690, loss = 0.31, batch loss = 0.26 (28.6 examples/sec; 0.280 sec/batch; 24h:15m:42s remains)
INFO - root - 2017-12-16 05:17:11.514918: step 20700, loss = 0.40, batch loss = 0.34 (27.3 examples/sec; 0.293 sec/batch; 25h:23m:34s remains)
2017-12-16 05:17:11.990659: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1130714 -4.3052053 -4.285953 -4.4126406 -4.2354379 -4.3537607 -4.61392 -5.0301847 -5.3408513 -5.5747318 -5.8421979 -5.9177132 -6.0067353 -5.7803955 -5.5021][-4.0856032 -4.3218541 -4.20315 -4.295876 -4.1800284 -4.0948954 -4.0342216 -4.4219093 -4.9984183 -5.3677 -5.71359 -5.9061227 -6.116961 -5.8499489 -5.43404][-4.3601232 -4.6462913 -4.3941245 -4.0008426 -3.5283775 -3.2653017 -2.9805295 -3.1727288 -3.8582892 -4.3794394 -4.8738108 -5.1780968 -5.4455471 -5.3020673 -5.0197711][-4.3021421 -4.5423474 -4.2997961 -3.6959822 -2.9835882 -2.2280998 -1.6420631 -1.6681159 -2.2142351 -2.9102528 -3.6343088 -4.0418863 -4.302115 -4.3029785 -4.2270951][-3.9364538 -3.9067345 -3.4028578 -2.6613855 -1.7328453 -0.7903738 -0.24615145 -0.20296478 -0.70218015 -1.3281343 -1.836736 -2.385319 -2.8641353 -3.1375682 -3.3514242][-3.5597801 -3.3228202 -2.5365088 -1.487864 -0.37271166 0.571074 1.1616435 1.3271623 0.95144367 0.31599426 -0.12613297 -0.6056664 -1.1892955 -1.8176475 -2.3714304][-3.3018839 -2.960469 -2.23505 -1.0802736 0.16654682 1.2364154 1.9179344 2.2709579 2.1905837 1.74653 1.3799834 0.83768988 0.16040564 -0.79999137 -1.5947502][-2.9467478 -2.5776248 -2.0645514 -1.2144406 -0.099181175 0.93367481 1.7480106 2.3636904 2.6457467 2.4051757 2.1695323 1.600472 0.75127745 -0.20996809 -0.98113942][-2.7605045 -2.53201 -2.2353895 -1.6787279 -0.80749679 0.0027894974 0.88769341 1.5401697 2.0657763 2.1120877 1.9430633 1.5983796 0.80957222 -0.052228451 -0.78973007][-3.2214339 -3.3678365 -3.3400025 -3.038316 -2.4511077 -1.8575399 -0.90065646 -0.098165035 0.52621174 0.79429007 0.72195768 0.32497931 -0.39009953 -1.001132 -1.4104888][-3.8042445 -4.1816597 -4.5112634 -4.5819488 -4.4001918 -4.0137157 -3.2363119 -2.5246112 -1.8304987 -1.3129246 -1.2251177 -1.6179531 -2.284009 -2.7114568 -2.7839022][-4.0922465 -4.3812995 -4.8588328 -5.3674068 -5.6740184 -5.6750813 -5.3579721 -5.113193 -4.5435019 -3.9165788 -3.7000346 -4.0178308 -4.5761833 -4.8287635 -4.6475477][-3.7142143 -4.0498867 -4.6108704 -5.24693 -5.8292074 -6.1313024 -6.2694507 -6.317893 -6.149579 -5.8085179 -5.7469258 -6.0307851 -6.4612622 -6.7395697 -6.5146751][-3.2060523 -3.4467192 -4.0154591 -4.8561654 -5.5870323 -5.9116769 -6.1143885 -6.4065065 -6.6263623 -6.53953 -6.651701 -7.1656094 -7.6444283 -7.9020848 -7.635911][-2.7961342 -2.8797889 -3.2161822 -3.9037323 -4.7457886 -5.3587747 -5.7606888 -6.0533009 -6.2612143 -6.5921679 -6.9787006 -7.4611974 -7.8844151 -8.1506748 -8.0495415]]...]
INFO - root - 2017-12-16 05:17:14.824844: step 20710, loss = 0.45, batch loss = 0.39 (28.1 examples/sec; 0.285 sec/batch; 24h:39m:10s remains)
INFO - root - 2017-12-16 05:17:17.649432: step 20720, loss = 0.37, batch loss = 0.31 (27.8 examples/sec; 0.287 sec/batch; 24h:53m:46s remains)
INFO - root - 2017-12-16 05:17:20.499845: step 20730, loss = 0.32, batch loss = 0.26 (27.1 examples/sec; 0.295 sec/batch; 25h:32m:39s remains)
INFO - root - 2017-12-16 05:17:23.336648: step 20740, loss = 0.21, batch loss = 0.15 (28.0 examples/sec; 0.286 sec/batch; 24h:44m:07s remains)
INFO - root - 2017-12-16 05:17:26.205828: step 20750, loss = 0.33, batch loss = 0.27 (28.5 examples/sec; 0.281 sec/batch; 24h:20m:40s remains)
INFO - root - 2017-12-16 05:17:29.069044: step 20760, loss = 0.30, batch loss = 0.25 (28.4 examples/sec; 0.282 sec/batch; 24h:23m:53s remains)
INFO - root - 2017-12-16 05:17:31.884776: step 20770, loss = 0.22, batch loss = 0.16 (28.5 examples/sec; 0.281 sec/batch; 24h:18m:01s remains)
INFO - root - 2017-12-16 05:17:34.724657: step 20780, loss = 0.28, batch loss = 0.22 (28.2 examples/sec; 0.283 sec/batch; 24h:31m:54s remains)
INFO - root - 2017-12-16 05:17:37.598451: step 20790, loss = 0.21, batch loss = 0.15 (28.1 examples/sec; 0.285 sec/batch; 24h:40m:13s remains)
INFO - root - 2017-12-16 05:17:40.417514: step 20800, loss = 0.27, batch loss = 0.21 (28.1 examples/sec; 0.284 sec/batch; 24h:37m:29s remains)
2017-12-16 05:17:40.877158: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9829516 -5.118773 -5.2298727 -5.3797646 -5.4095058 -5.3662353 -5.0445676 -4.8188739 -4.9508104 -5.4961658 -6.2422862 -7.1147256 -8.1638145 -9.0036907 -9.5764284][-4.6972876 -5.0164533 -5.1778316 -5.219677 -5.2650743 -5.5207763 -5.3447447 -4.9069748 -4.5804553 -4.8277507 -5.6085196 -6.5579081 -7.5860586 -8.2238522 -8.8216162][-4.0551572 -4.5156026 -4.5754237 -4.4319072 -4.3907051 -4.6162968 -4.8342047 -4.8481565 -4.7462106 -4.7705259 -5.0418358 -5.4515047 -6.4089193 -7.2442617 -7.869288][-2.9491656 -3.5135722 -3.8803945 -3.4193287 -2.9072762 -2.8387337 -2.9241633 -3.2027426 -3.6556511 -3.9983077 -4.0192442 -4.3332605 -5.1539321 -5.8845944 -6.9105225][-2.1035876 -2.1428204 -2.0713558 -1.7939496 -1.4011266 -1.0822868 -0.91449451 -1.0109313 -1.2625203 -1.9523485 -2.6770234 -2.851963 -3.1601477 -3.975194 -5.40553][-2.0182054 -1.4085045 -0.60561633 0.37175894 1.1786628 1.3639345 1.3230095 1.183444 0.92682219 0.45490694 0.16524506 -0.36510372 -1.2160461 -1.7588391 -2.9988294][-2.3510926 -1.5505953 -0.49082255 0.76479912 2.2576504 3.2247953 3.5022364 3.0925894 2.7122078 2.1521807 1.729413 1.3565788 0.77543354 -0.1505599 -1.8323359][-3.0363059 -2.1774518 -0.99843 0.64780474 2.2641521 3.3992147 4.2347288 4.3963232 4.0150785 2.9945645 2.3239231 1.8200331 1.0211787 0.013960361 -1.3645926][-3.9648542 -3.4808328 -2.4120653 -1.0722132 0.53019 1.7918606 2.6755152 2.7612877 2.8154688 2.4724908 2.0642023 1.3577957 0.60887861 -0.081151485 -1.0277519][-4.38711 -4.101459 -3.7011065 -2.9331169 -1.8341403 -0.92197156 -0.12585735 0.15556812 0.16837835 0.003745079 0.13042736 -0.072072506 -0.62538004 -1.0713127 -1.7501683][-4.8417563 -4.5130863 -4.2658706 -4.1491966 -4.1714315 -3.7859716 -2.9720452 -2.8072505 -2.794682 -2.7720168 -2.6183658 -2.6934977 -3.0482757 -3.0702076 -3.2712188][-5.28207 -5.0445752 -4.98802 -5.0684123 -5.2602396 -5.4432116 -5.528367 -5.5702882 -5.3885174 -5.1315966 -4.8422513 -4.8674707 -4.8652039 -4.6169162 -4.6384196][-5.1610022 -5.1362066 -5.3584781 -5.6252327 -5.9431911 -6.0667844 -6.1569843 -6.1441145 -5.9729652 -5.8289962 -5.6474032 -5.5074048 -5.4868565 -5.4177437 -5.302176][-4.066927 -4.1295629 -4.3874717 -4.8325834 -5.2847204 -5.5339556 -5.6879115 -5.7479405 -5.7830243 -5.640029 -5.3890505 -5.5706038 -5.7672653 -5.64585 -5.5098104][-4.1336961 -3.9063807 -3.8217015 -4.0019517 -4.2557421 -4.5297832 -4.7804832 -4.8499331 -4.7455788 -4.8555007 -4.9431543 -4.8523564 -4.7237606 -4.787312 -4.8966641]]...]
INFO - root - 2017-12-16 05:17:43.700125: step 20810, loss = 0.26, batch loss = 0.20 (27.5 examples/sec; 0.291 sec/batch; 25h:13m:46s remains)
INFO - root - 2017-12-16 05:17:46.535796: step 20820, loss = 0.29, batch loss = 0.23 (29.4 examples/sec; 0.272 sec/batch; 23h:35m:09s remains)
INFO - root - 2017-12-16 05:17:49.376609: step 20830, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 24h:43m:21s remains)
INFO - root - 2017-12-16 05:17:52.239068: step 20840, loss = 0.20, batch loss = 0.14 (27.9 examples/sec; 0.287 sec/batch; 24h:49m:03s remains)
INFO - root - 2017-12-16 05:17:55.086085: step 20850, loss = 0.19, batch loss = 0.13 (27.1 examples/sec; 0.295 sec/batch; 25h:33m:42s remains)
INFO - root - 2017-12-16 05:17:57.926739: step 20860, loss = 0.35, batch loss = 0.29 (26.3 examples/sec; 0.304 sec/batch; 26h:18m:58s remains)
INFO - root - 2017-12-16 05:18:00.798875: step 20870, loss = 0.24, batch loss = 0.19 (27.9 examples/sec; 0.287 sec/batch; 24h:49m:05s remains)
INFO - root - 2017-12-16 05:18:03.616984: step 20880, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 24h:22m:42s remains)
INFO - root - 2017-12-16 05:18:06.414958: step 20890, loss = 0.28, batch loss = 0.22 (27.7 examples/sec; 0.289 sec/batch; 25h:01m:43s remains)
INFO - root - 2017-12-16 05:18:09.235094: step 20900, loss = 0.18, batch loss = 0.12 (28.6 examples/sec; 0.280 sec/batch; 24h:12m:06s remains)
2017-12-16 05:18:09.704360: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0626273 -2.36371 -2.604722 -2.866713 -3.1823969 -3.3318734 -3.2868049 -3.247612 -3.169363 -3.1764672 -3.2634852 -3.2899241 -3.3627651 -3.4862633 -3.549072][-2.4928358 -2.6128011 -2.7992442 -3.0721803 -3.4220271 -3.5902011 -3.5619686 -3.3892784 -3.2170215 -3.2922211 -3.4253645 -3.4671068 -3.5750294 -3.6525311 -3.6189737][-2.6845751 -2.7508867 -2.9481468 -3.2128463 -3.4266548 -3.5701466 -3.5445712 -3.3046718 -3.0433459 -3.0764494 -3.231658 -3.3830256 -3.5665402 -3.6562884 -3.6278739][-2.8461704 -2.645072 -2.6463685 -2.8081918 -2.9953916 -3.2941172 -3.4540296 -3.2357016 -2.9141028 -2.8888288 -2.9260387 -3.0191469 -3.0889139 -3.2871575 -3.3375301][-2.4358542 -1.9820354 -1.9202127 -2.0843933 -2.2370462 -2.5743172 -2.9368212 -3.0185156 -2.9296963 -2.8504272 -2.7693925 -2.7202382 -2.5969472 -2.5536835 -2.5527649][-1.3518727 -0.80421019 -0.74299693 -0.73895907 -0.82597542 -1.2921169 -1.8475237 -2.3379734 -2.5307119 -2.5447726 -2.3863804 -2.1994021 -1.83284 -1.5337725 -1.4611416][-0.032540321 0.46583414 0.42842627 0.21490812 -0.10911846 -0.48867464 -0.95010948 -1.3758307 -1.7630057 -1.8996043 -1.8200448 -1.550777 -1.0578263 -0.71381378 -0.50305414][0.7262907 1.074779 1.021421 0.74486542 0.33564806 -0.076695442 -0.45114994 -0.89132285 -1.2131686 -1.2964063 -1.0949361 -0.76685548 -0.26975584 0.12203884 0.19154644][0.43035936 0.78174496 0.80157328 0.54654884 0.036563396 -0.21592808 -0.44019508 -0.69845128 -0.79918337 -0.76158118 -0.37505054 -0.015874863 0.43374014 0.68518448 0.55896044][-0.39819336 -0.27969074 -0.40215158 -0.49533963 -0.74639988 -0.80584288 -0.82893372 -0.85652232 -0.79052424 -0.52484441 -0.021231651 0.41697836 0.78942251 0.80463839 0.49515152][-1.6478882 -1.690491 -1.82617 -1.9012835 -2.0017 -1.8449738 -1.5617044 -1.2459664 -0.92910862 -0.57055068 -0.05389452 0.38549089 0.52259159 0.28068876 -0.12933016][-2.7559493 -2.9329419 -3.3019533 -3.5717757 -3.7648928 -3.5227652 -3.0170472 -2.4037075 -1.7249238 -1.0996723 -0.49694467 -0.12989378 -0.18692017 -0.62172627 -1.1782062][-4.0523844 -4.3565488 -4.669518 -4.8370438 -5.1064863 -4.9190445 -4.3299975 -3.4464588 -2.5425591 -1.8019276 -1.0969334 -0.77507019 -1.0067267 -1.5810134 -2.132277][-5.1867204 -5.4222775 -5.5879269 -5.7189827 -5.8601441 -5.5007114 -4.8859673 -4.0435967 -3.2694237 -2.5040174 -1.8762891 -1.5147882 -1.7046516 -2.3622708 -2.8782015][-5.5587521 -5.8368258 -5.9843035 -5.9347692 -5.8228922 -5.687151 -5.2490358 -4.4886169 -3.8329983 -3.1372547 -2.6603603 -2.2004919 -2.3417878 -2.8745565 -3.2116928]]...]
INFO - root - 2017-12-16 05:18:12.520249: step 20910, loss = 0.33, batch loss = 0.27 (29.3 examples/sec; 0.273 sec/batch; 23h:39m:47s remains)
INFO - root - 2017-12-16 05:18:15.385757: step 20920, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 24h:22m:41s remains)
INFO - root - 2017-12-16 05:18:18.237890: step 20930, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.276 sec/batch; 23h:55m:40s remains)
INFO - root - 2017-12-16 05:18:21.090836: step 20940, loss = 0.27, batch loss = 0.21 (26.1 examples/sec; 0.307 sec/batch; 26h:31m:34s remains)
INFO - root - 2017-12-16 05:18:23.911337: step 20950, loss = 0.26, batch loss = 0.20 (28.8 examples/sec; 0.278 sec/batch; 24h:01m:47s remains)
INFO - root - 2017-12-16 05:18:26.745203: step 20960, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 24h:03m:54s remains)
INFO - root - 2017-12-16 05:18:29.601177: step 20970, loss = 0.25, batch loss = 0.19 (26.8 examples/sec; 0.298 sec/batch; 25h:48m:05s remains)
INFO - root - 2017-12-16 05:18:32.413737: step 20980, loss = 0.22, batch loss = 0.16 (28.5 examples/sec; 0.281 sec/batch; 24h:18m:28s remains)
INFO - root - 2017-12-16 05:18:35.245030: step 20990, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 24h:00m:54s remains)
INFO - root - 2017-12-16 05:18:38.166081: step 21000, loss = 0.24, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 24h:27m:54s remains)
2017-12-16 05:18:38.661935: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4065456 -4.2125616 -4.0786524 -4.5172105 -5.2658944 -6.1675358 -6.7901115 -6.9000769 -6.8239746 -6.3241405 -5.890501 -5.6203647 -5.0090847 -4.3032765 -3.5573964][-5.0616655 -5.5201149 -5.8886566 -6.1402817 -6.9454355 -7.1745825 -7.3126678 -7.2379375 -6.747478 -6.3083935 -6.0590844 -5.6823735 -4.9787159 -4.32817 -3.7525568][-5.7127404 -6.248702 -6.7003517 -6.9369946 -6.9013119 -6.5803175 -6.6026578 -6.314568 -5.890883 -5.5464363 -5.344151 -5.3828683 -5.2445226 -4.8180542 -4.2193651][-5.9996223 -7.0278 -7.3976645 -7.1258125 -6.5359764 -5.4438868 -4.4560471 -3.8052979 -3.6294131 -4.1647196 -4.8274832 -5.0767188 -5.1283565 -5.1272821 -4.8519106][-5.8822041 -6.93981 -7.1857824 -6.4179764 -4.7521482 -2.9845605 -1.6452248 -0.59833074 -0.65898609 -1.7755032 -3.0676739 -4.5250125 -5.4193621 -5.3655748 -4.8745985][-4.4799547 -5.2979493 -5.6608386 -4.6382089 -2.1806664 0.41776371 2.4427471 3.4366989 2.8083739 0.91836023 -1.6211414 -3.81617 -5.0606818 -5.5894 -5.2647905][-3.20776 -3.9524086 -3.7773542 -2.2637789 0.043782234 2.6986465 5.016633 6.0975027 5.2300205 2.6640897 -0.43244982 -3.17173 -4.9757175 -5.4996862 -5.0167685][-2.5552609 -3.3362722 -3.2691445 -1.6944849 0.91396809 3.9153814 6.046751 6.369751 5.276392 2.7572384 -0.5416863 -3.3246589 -4.9191184 -5.3270364 -4.8816442][-2.4986932 -3.0156918 -2.6876891 -1.5557985 0.062916756 2.4791775 4.6537218 5.2967615 3.9832659 1.179162 -1.7968664 -4.2118769 -5.623004 -5.7248783 -5.08619][-2.516274 -3.172359 -3.0363259 -2.0446115 -0.50859976 0.88537216 1.8527079 2.3179612 1.2717791 -0.9937892 -3.5398364 -5.6648641 -6.4888144 -6.3146906 -5.5945878][-3.3219969 -3.5926776 -3.4965961 -3.2947004 -2.7815611 -1.5665894 -0.35170984 -0.39565849 -1.6875038 -3.2900774 -4.9980488 -6.5740929 -7.1775589 -6.7472491 -5.8333325][-4.3935895 -4.5930581 -4.6235404 -4.1739235 -3.6534336 -3.4601014 -3.1790147 -3.0920446 -3.5964694 -4.9144483 -6.3466763 -7.0843782 -7.1537094 -6.6597033 -5.8387628][-5.3491573 -5.3946133 -5.4286709 -5.3578992 -5.2107992 -4.6359439 -4.2002883 -4.6599746 -5.3176317 -5.9575729 -6.7496786 -7.2561803 -7.1458888 -6.4813509 -5.7010827][-6.187191 -6.0083418 -5.9188905 -5.839406 -5.67187 -5.3076334 -5.1752176 -5.052618 -5.3119874 -6.0310974 -6.5345583 -6.5916839 -6.3181219 -5.783318 -5.2137427][-6.8643236 -6.6918559 -6.4951735 -6.272316 -5.9316487 -5.499476 -5.2591267 -5.0246944 -5.0839829 -5.415545 -5.71825 -5.7094784 -5.3322644 -4.7285924 -4.2431993]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 05:18:41.505728: step 21010, loss = 0.27, batch loss = 0.21 (26.0 examples/sec; 0.307 sec/batch; 26h:35m:35s remains)
INFO - root - 2017-12-16 05:18:44.326026: step 21020, loss = 0.26, batch loss = 0.21 (28.9 examples/sec; 0.276 sec/batch; 23h:54m:54s remains)
INFO - root - 2017-12-16 05:18:47.224151: step 21030, loss = 0.29, batch loss = 0.23 (28.5 examples/sec; 0.281 sec/batch; 24h:17m:08s remains)
INFO - root - 2017-12-16 05:18:50.012026: step 21040, loss = 0.30, batch loss = 0.24 (29.4 examples/sec; 0.273 sec/batch; 23h:34m:51s remains)
INFO - root - 2017-12-16 05:18:52.885486: step 21050, loss = 0.20, batch loss = 0.14 (26.5 examples/sec; 0.302 sec/batch; 26h:05m:09s remains)
INFO - root - 2017-12-16 05:18:55.711620: step 21060, loss = 0.23, batch loss = 0.17 (28.6 examples/sec; 0.280 sec/batch; 24h:13m:51s remains)
INFO - root - 2017-12-16 05:18:58.513611: step 21070, loss = 0.26, batch loss = 0.20 (29.6 examples/sec; 0.271 sec/batch; 23h:24m:45s remains)
INFO - root - 2017-12-16 05:19:01.343001: step 21080, loss = 0.27, batch loss = 0.21 (29.0 examples/sec; 0.276 sec/batch; 23h:52m:08s remains)
INFO - root - 2017-12-16 05:19:04.218569: step 21090, loss = 0.29, batch loss = 0.23 (28.1 examples/sec; 0.285 sec/batch; 24h:36m:42s remains)
INFO - root - 2017-12-16 05:19:07.025336: step 21100, loss = 0.24, batch loss = 0.18 (29.5 examples/sec; 0.271 sec/batch; 23h:28m:46s remains)
2017-12-16 05:19:07.484797: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5024691 -6.0205832 -5.5187931 -5.2175961 -4.9081039 -4.7708125 -4.6810775 -4.7032447 -4.8290133 -5.1772661 -5.6111956 -5.8257132 -5.8920841 -5.7379646 -5.5078692][-7.1273789 -6.5436382 -5.7044039 -5.0790973 -4.6495681 -4.7042928 -4.6868396 -4.9143119 -5.1742678 -5.5502834 -6.0542831 -6.29269 -6.45043 -6.1893878 -5.8167667][-7.3173637 -6.5509491 -5.589282 -4.6301761 -3.9294562 -3.943604 -4.0196276 -4.4271388 -4.8358936 -5.4970608 -6.1509409 -6.6105032 -6.8240681 -6.4936728 -6.2003827][-6.8530016 -5.9024897 -4.8049769 -3.6484921 -2.8691285 -2.5969613 -2.4945302 -3.0194201 -3.6189976 -4.3742223 -5.1413655 -5.8097677 -6.2205815 -6.3541875 -6.6000938][-6.0554838 -4.809772 -3.3927083 -1.9879408 -0.8700912 -0.40528727 -0.43822002 -0.874485 -1.5092263 -2.5455189 -3.437813 -4.2499394 -5.04544 -5.8469768 -6.4814587][-5.5675974 -4.01233 -2.1622303 -0.14916039 1.5033751 2.0377626 2.0130095 1.3899279 0.53222132 -0.36680222 -1.3959007 -2.5109103 -3.8113441 -4.9380183 -6.0448747][-5.5503922 -3.7920284 -1.8138585 0.48091841 2.4271712 3.4074955 3.6782885 2.9556389 2.0290036 0.98925591 -0.02780962 -1.4080544 -3.1148849 -4.7094717 -6.1538339][-5.7387428 -4.3266058 -2.3407063 0.24973106 2.2487617 3.4793587 4.0492954 3.5896168 2.9128423 1.926291 0.73473024 -0.90929294 -2.851769 -4.7687798 -6.4115386][-5.9945111 -4.7432952 -3.1936328 -0.88491392 0.89143038 2.2242837 2.904583 2.8587275 2.4059052 1.5045595 0.49845695 -1.0750957 -2.8338151 -4.6788578 -6.2202215][-6.5915403 -5.9176216 -4.6930656 -2.7005675 -1.142761 -0.10655212 0.84793758 1.0591278 0.81234169 0.15240955 -0.61774445 -1.869293 -3.4482498 -4.9217587 -6.2371907][-7.9035196 -7.22581 -6.3325896 -4.9220872 -3.8273306 -2.6674807 -1.7086611 -1.2396107 -1.0674953 -1.4858854 -2.0489285 -3.1356108 -4.4298768 -5.6479454 -6.6458783][-8.608717 -8.0848293 -7.5601563 -6.5226727 -5.6867981 -5.0589342 -4.1407266 -3.4238331 -2.8905206 -2.9010239 -3.1828179 -3.9448874 -4.8416777 -5.6164331 -6.4957151][-8.5358944 -8.1049471 -7.7356853 -7.1418486 -6.708478 -6.2247639 -5.8059731 -5.1527305 -4.4184074 -4.0632391 -3.9426255 -4.2489328 -4.6847429 -5.14751 -5.6943421][-8.4362278 -7.5765142 -7.0333309 -6.6830468 -6.5448694 -6.37376 -6.2524838 -5.8660278 -5.3496075 -4.7069345 -4.2278762 -4.2574058 -4.3171887 -4.4710431 -4.7947259][-7.6425781 -6.6358805 -5.9121828 -5.7623849 -5.8212051 -5.7105389 -5.8833361 -5.8387418 -5.4952145 -4.9762311 -4.4446383 -4.2047749 -4.0464816 -4.0781412 -4.1301832]]...]
INFO - root - 2017-12-16 05:19:10.373703: step 21110, loss = 0.19, batch loss = 0.13 (27.9 examples/sec; 0.287 sec/batch; 24h:49m:47s remains)
INFO - root - 2017-12-16 05:19:13.185441: step 21120, loss = 0.27, batch loss = 0.21 (29.0 examples/sec; 0.276 sec/batch; 23h:53m:38s remains)
INFO - root - 2017-12-16 05:19:16.047220: step 21130, loss = 0.23, batch loss = 0.17 (26.9 examples/sec; 0.297 sec/batch; 25h:42m:37s remains)
INFO - root - 2017-12-16 05:19:18.910941: step 21140, loss = 0.21, batch loss = 0.15 (26.4 examples/sec; 0.303 sec/batch; 26h:14m:45s remains)
INFO - root - 2017-12-16 05:19:21.726198: step 21150, loss = 0.22, batch loss = 0.16 (29.4 examples/sec; 0.272 sec/batch; 23h:33m:38s remains)
INFO - root - 2017-12-16 05:19:24.603381: step 21160, loss = 0.27, batch loss = 0.22 (27.7 examples/sec; 0.289 sec/batch; 24h:58m:24s remains)
INFO - root - 2017-12-16 05:19:27.452914: step 21170, loss = 0.28, batch loss = 0.22 (29.3 examples/sec; 0.273 sec/batch; 23h:37m:41s remains)
INFO - root - 2017-12-16 05:19:30.308580: step 21180, loss = 0.23, batch loss = 0.17 (27.8 examples/sec; 0.287 sec/batch; 24h:50m:41s remains)
INFO - root - 2017-12-16 05:19:33.134832: step 21190, loss = 0.27, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 24h:20m:39s remains)
INFO - root - 2017-12-16 05:19:35.940066: step 21200, loss = 0.22, batch loss = 0.16 (28.4 examples/sec; 0.282 sec/batch; 24h:23m:14s remains)
2017-12-16 05:19:36.393325: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1182017 -4.2280107 -4.157515 -4.2570109 -4.402142 -4.3621025 -4.3640862 -4.51016 -4.6027455 -4.469511 -4.3803415 -4.2376766 -4.0312963 -3.8799472 -3.7606936][-3.4318628 -3.6710284 -3.9875135 -4.0203142 -4.1172123 -4.2238383 -4.0147057 -3.8124146 -3.7922754 -3.7368813 -3.7330058 -3.691005 -3.6537485 -3.5135159 -3.4085183][-2.9466004 -3.4561281 -3.9086645 -4.2292361 -4.1574306 -3.728735 -3.2029624 -2.7558141 -2.5493243 -2.7878342 -3.1393924 -3.2593923 -3.1273811 -3.0929356 -3.1637957][-2.3943865 -3.3135996 -4.1272311 -4.2482643 -3.7812309 -2.9895129 -2.0523729 -1.571769 -1.5671053 -1.8971102 -2.4150698 -2.9727969 -3.1569109 -2.9068131 -2.5194752][-2.6034284 -3.4977167 -4.233644 -4.1921477 -3.2896366 -2.1144998 -0.88917971 -0.25710678 -0.25511074 -1.0120726 -2.0855596 -2.8141618 -2.9708328 -2.9958732 -2.8241897][-3.0946088 -4.24508 -4.7077389 -4.2880945 -2.9666944 -1.1422677 0.57138872 1.302002 1.1143193 0.21721363 -0.97375989 -2.4128993 -3.5267444 -3.550137 -3.1274304][-3.2939506 -4.4441729 -4.7320266 -3.9966226 -2.4700441 -0.57317686 1.3677602 2.2674613 2.0295916 0.7796669 -0.77637434 -2.1455185 -3.1471822 -3.61952 -3.5628815][-2.8618226 -4.1716175 -4.862854 -4.18275 -2.3238385 -0.17602968 1.6256351 2.5996938 2.2786775 0.84412718 -1.0230441 -2.5112963 -3.3975296 -3.6818104 -3.6100433][-2.6589284 -3.9510622 -4.73388 -4.4242458 -3.0536003 -0.7037828 1.3583651 2.3405175 2.107152 0.85943937 -1.155839 -3.0822587 -4.2393651 -4.3677096 -3.8826129][-2.3811705 -3.7733254 -4.9199715 -4.8540382 -3.4879715 -1.4586823 0.3865571 1.5932155 1.4964075 0.21650505 -1.7516646 -3.6491022 -4.931334 -5.2542019 -4.7485361][-2.0145578 -3.3083425 -4.5870605 -4.668437 -3.8420954 -2.1194408 -0.41848135 0.43129206 0.2060113 -1.0212092 -2.7405133 -4.2640829 -5.2753425 -5.418488 -4.962914][-1.864867 -3.2280908 -4.5816112 -4.7473722 -4.0186629 -2.6515474 -1.5628855 -0.86706424 -1.1462691 -2.2077713 -3.6629443 -4.80384 -5.2398109 -4.9214091 -4.3235922][-2.1774948 -3.4721746 -4.8863268 -5.0846992 -4.4871607 -3.2737546 -2.3763368 -2.0861838 -2.51187 -3.3253729 -4.2501516 -5.0413876 -5.2786446 -4.958889 -4.25971][-2.2834661 -3.2562084 -4.3656087 -4.5904012 -4.3336225 -3.5314066 -2.9301572 -2.7808728 -3.2163572 -3.9395285 -4.5925322 -4.9217849 -4.9830112 -4.6144562 -4.1413603][-2.3949175 -2.9230528 -3.7814429 -3.9344277 -3.6224744 -3.176362 -2.947346 -3.070816 -3.5410216 -4.0951896 -4.4987617 -4.547183 -4.4306865 -4.0919275 -3.7872322]]...]
INFO - root - 2017-12-16 05:19:39.213314: step 21210, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.277 sec/batch; 23h:58m:49s remains)
INFO - root - 2017-12-16 05:19:42.060667: step 21220, loss = 0.25, batch loss = 0.19 (27.3 examples/sec; 0.294 sec/batch; 25h:23m:03s remains)
INFO - root - 2017-12-16 05:19:44.862824: step 21230, loss = 0.27, batch loss = 0.22 (28.9 examples/sec; 0.276 sec/batch; 23h:53m:50s remains)
INFO - root - 2017-12-16 05:19:47.743524: step 21240, loss = 0.31, batch loss = 0.25 (28.4 examples/sec; 0.282 sec/batch; 24h:21m:47s remains)
INFO - root - 2017-12-16 05:19:50.599846: step 21250, loss = 0.31, batch loss = 0.25 (26.8 examples/sec; 0.298 sec/batch; 25h:47m:52s remains)
INFO - root - 2017-12-16 05:19:53.465390: step 21260, loss = 0.44, batch loss = 0.38 (26.0 examples/sec; 0.308 sec/batch; 26h:36m:51s remains)
INFO - root - 2017-12-16 05:19:56.348620: step 21270, loss = 0.27, batch loss = 0.21 (28.1 examples/sec; 0.284 sec/batch; 24h:35m:24s remains)
INFO - root - 2017-12-16 05:19:59.255842: step 21280, loss = 0.25, batch loss = 0.19 (27.9 examples/sec; 0.287 sec/batch; 24h:49m:45s remains)
INFO - root - 2017-12-16 05:20:02.118202: step 21290, loss = 0.33, batch loss = 0.27 (28.6 examples/sec; 0.280 sec/batch; 24h:12m:05s remains)
INFO - root - 2017-12-16 05:20:04.953074: step 21300, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.282 sec/batch; 24h:22m:12s remains)
2017-12-16 05:20:05.441004: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.961237 -4.2463665 -4.3533659 -4.50873 -4.5521684 -4.3204527 -3.9721437 -3.7484505 -3.6770372 -3.727639 -3.8070478 -4.1025 -4.35287 -4.2480936 -3.9290078][-3.5824752 -4.1751442 -4.5983248 -4.7004776 -4.6467819 -4.5927196 -4.4201717 -4.1769633 -4.0811224 -4.2466736 -4.5455766 -4.8432088 -5.0957384 -5.1487293 -4.8660178][-3.3967564 -3.8293219 -4.1497631 -4.2735085 -4.2639918 -4.0334783 -3.7578681 -3.8026285 -3.9862726 -4.3654943 -4.8209019 -5.4419885 -5.9732008 -6.0410943 -5.6862135][-3.3699741 -3.8015718 -4.0142112 -3.6941228 -3.1376057 -2.6573315 -2.2283654 -1.9838622 -2.0568938 -2.9731536 -3.956742 -4.831 -5.69164 -6.2701969 -6.1652141][-2.9104605 -3.129426 -3.1686671 -2.6715767 -1.7397821 -0.55869079 0.57354259 1.1407881 1.081615 0.16803646 -1.2291358 -2.9023905 -4.3160305 -5.1758804 -5.4626637][-2.4042406 -2.1511919 -1.8645217 -1.2524979 -0.0080676079 1.5391212 3.0149169 4.1617126 4.4703865 3.4988055 1.6653633 -0.50619841 -2.5200984 -3.7289462 -4.0530224][-2.4349728 -1.8659353 -1.3363814 -0.29933834 1.2296128 2.98954 5.0722837 6.6687689 7.0740728 6.0289822 4.0198946 1.540803 -0.74932337 -2.3552151 -3.023855][-2.7844412 -2.14565 -1.624547 -0.56747484 1.1142449 3.344173 5.8040829 7.3869257 8.0264091 7.1723585 5.007246 2.4329772 0.22785234 -1.4014916 -2.2613196][-3.1582432 -2.8409922 -2.4157708 -1.714596 -0.49280143 1.5391102 3.9399376 5.8082609 6.701108 5.9271688 4.0952473 1.893549 -0.041901588 -1.2513406 -1.7965653][-3.3623495 -3.4814758 -3.7955556 -3.4964058 -2.5455351 -0.95110321 0.93208885 2.7090545 3.8213062 3.2634158 1.8548408 0.23278522 -1.1627874 -2.1222627 -2.5866551][-4.1678262 -4.2615709 -4.5663466 -4.8143444 -4.5279384 -3.335113 -1.7516685 -0.55143356 0.253942 0.22262764 -0.44026184 -1.6314294 -2.6110659 -3.0240469 -3.1267376][-4.2538285 -4.4846048 -4.8954368 -5.0704956 -4.9168525 -4.3628278 -3.4572885 -2.5775366 -1.8786442 -1.9433031 -2.3525813 -3.0135114 -3.5282369 -3.6842842 -3.6057868][-3.6463828 -3.8578646 -4.2475033 -4.4298434 -4.4965281 -4.1247482 -3.4908071 -2.9636755 -2.5767319 -2.6130939 -2.8320327 -3.1481013 -3.4031978 -3.5859756 -3.5811357][-3.3138876 -3.3277779 -3.5702465 -3.746747 -3.8645639 -3.6375327 -3.2179377 -2.9628279 -2.7803259 -2.7007034 -2.6990173 -2.8999152 -3.1051493 -3.0826125 -2.9661217][-2.8163619 -2.6008294 -2.540616 -2.6571093 -2.8277161 -2.7556763 -2.6155996 -2.5376997 -2.4475117 -2.4268916 -2.4798732 -2.4693995 -2.4553986 -2.5783114 -2.5994081]]...]
INFO - root - 2017-12-16 05:20:08.297088: step 21310, loss = 0.27, batch loss = 0.21 (29.5 examples/sec; 0.271 sec/batch; 23h:27m:47s remains)
INFO - root - 2017-12-16 05:20:11.163167: step 21320, loss = 0.25, batch loss = 0.19 (29.4 examples/sec; 0.272 sec/batch; 23h:32m:27s remains)
INFO - root - 2017-12-16 05:20:13.994590: step 21330, loss = 0.28, batch loss = 0.22 (27.9 examples/sec; 0.286 sec/batch; 24h:44m:51s remains)
INFO - root - 2017-12-16 05:20:16.867128: step 21340, loss = 0.42, batch loss = 0.36 (28.1 examples/sec; 0.285 sec/batch; 24h:37m:56s remains)
INFO - root - 2017-12-16 05:20:19.687100: step 21350, loss = 0.26, batch loss = 0.20 (27.8 examples/sec; 0.288 sec/batch; 24h:51m:13s remains)
INFO - root - 2017-12-16 05:20:22.575791: step 21360, loss = 0.28, batch loss = 0.22 (27.5 examples/sec; 0.291 sec/batch; 25h:08m:54s remains)
INFO - root - 2017-12-16 05:20:25.406350: step 21370, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.279 sec/batch; 24h:06m:32s remains)
INFO - root - 2017-12-16 05:20:28.194008: step 21380, loss = 0.21, batch loss = 0.15 (29.3 examples/sec; 0.273 sec/batch; 23h:36m:31s remains)
INFO - root - 2017-12-16 05:20:31.028710: step 21390, loss = 0.26, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 24h:31m:27s remains)
INFO - root - 2017-12-16 05:20:33.870234: step 21400, loss = 0.23, batch loss = 0.17 (27.6 examples/sec; 0.290 sec/batch; 25h:04m:28s remains)
2017-12-16 05:20:34.327250: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4548116 -4.8055825 -5.0208898 -5.0290203 -4.755177 -4.301424 -4.096714 -4.0565367 -3.9786789 -3.9261823 -3.8825469 -3.7732921 -3.6573062 -3.6770546 -3.7713263][-4.6286263 -5.0116348 -5.304801 -5.3703032 -5.0920458 -4.6655321 -4.279448 -3.8896337 -3.5172024 -3.3784466 -3.5520692 -3.4706655 -3.39085 -3.5045671 -3.6647859][-5.2213063 -5.6468997 -5.9573922 -5.9507408 -5.5092649 -4.7703905 -4.01706 -3.2567105 -2.7341716 -2.6011105 -2.8444498 -3.0552859 -3.1427667 -3.1204467 -3.2046511][-5.7874131 -6.3554 -6.6402655 -6.3791089 -5.619278 -4.5692134 -3.4989498 -2.4745836 -1.8210027 -1.6187379 -1.8095055 -2.2426164 -2.6656904 -2.7027903 -2.5814362][-5.62041 -6.3223224 -6.391696 -5.897419 -4.7825789 -3.2543831 -1.8348548 -0.86574197 -0.67986941 -0.927742 -1.4521012 -2.0725372 -2.6825275 -2.625309 -2.2519162][-5.3407249 -5.5911684 -5.2542911 -4.4369721 -3.0524454 -1.2752087 0.29551506 1.2854209 1.2127919 0.37770319 -1.0104411 -2.1458342 -2.9265614 -2.9582186 -2.5328183][-4.8543062 -5.3074102 -4.866889 -3.4771318 -1.7592666 0.10291004 1.8291221 2.6535821 2.4088006 1.2531085 -0.459069 -1.9902356 -3.0769224 -3.2527742 -2.7739105][-4.567522 -4.9645557 -4.5107293 -3.1542947 -1.3183279 0.8057251 2.4175754 2.8526325 2.3479772 1.2810469 -0.23957729 -2.0270233 -3.3099959 -3.5477886 -2.9997602][-4.3706341 -4.3797078 -3.7510922 -2.5035334 -1.04427 0.94438028 2.3914957 2.5117726 1.6905589 0.507247 -0.94397259 -2.7270584 -3.9691911 -4.1209283 -3.465822][-4.08861 -4.414247 -3.975343 -2.841701 -1.5089552 0.057999134 1.215116 1.6300001 0.90688562 -0.54217362 -1.9543631 -3.4363246 -4.5126662 -4.6083107 -3.8863106][-3.7571383 -4.3967042 -4.7248631 -3.8344071 -2.6354194 -1.5690889 -0.70523214 -0.31589937 -0.62649393 -1.8727126 -3.1993666 -4.1978455 -4.7729592 -4.6928658 -4.0309176][-4.0250216 -4.5499735 -5.0554185 -4.8727212 -4.3750038 -3.4168291 -2.6101794 -2.4722295 -2.5956678 -3.4251447 -4.3535266 -5.1011667 -5.3298635 -4.9725366 -4.1328254][-4.0280828 -4.7210865 -5.4449224 -5.702302 -5.5710287 -4.8261008 -4.12383 -3.8823903 -3.9338522 -4.4860878 -4.8974919 -5.3184047 -5.3557768 -4.9359684 -4.043314][-3.4683204 -4.2946677 -5.0240703 -5.2472925 -5.2696576 -5.0972214 -4.8360786 -4.6354442 -4.5529151 -4.9781208 -5.2597938 -5.2639489 -5.0750241 -4.6438961 -3.8955417][-2.9308734 -3.6953597 -4.2412992 -4.3946533 -4.3071394 -4.1075416 -3.914556 -3.8621528 -3.94643 -4.2997875 -4.6064906 -4.6509256 -4.334342 -3.7883077 -3.1959548]]...]
INFO - root - 2017-12-16 05:20:37.206557: step 21410, loss = 0.30, batch loss = 0.25 (26.2 examples/sec; 0.305 sec/batch; 26h:21m:27s remains)
INFO - root - 2017-12-16 05:20:40.087646: step 21420, loss = 0.26, batch loss = 0.20 (26.1 examples/sec; 0.307 sec/batch; 26h:31m:03s remains)
INFO - root - 2017-12-16 05:20:42.964990: step 21430, loss = 0.35, batch loss = 0.29 (26.9 examples/sec; 0.297 sec/batch; 25h:39m:12s remains)
INFO - root - 2017-12-16 05:20:45.796638: step 21440, loss = 0.33, batch loss = 0.27 (29.2 examples/sec; 0.274 sec/batch; 23h:41m:52s remains)
INFO - root - 2017-12-16 05:20:48.624710: step 21450, loss = 0.20, batch loss = 0.14 (28.0 examples/sec; 0.285 sec/batch; 24h:38m:59s remains)
INFO - root - 2017-12-16 05:20:51.439412: step 21460, loss = 0.25, batch loss = 0.19 (29.7 examples/sec; 0.269 sec/batch; 23h:14m:22s remains)
INFO - root - 2017-12-16 05:20:54.301474: step 21470, loss = 0.25, batch loss = 0.20 (28.6 examples/sec; 0.279 sec/batch; 24h:07m:36s remains)
INFO - root - 2017-12-16 05:20:57.131646: step 21480, loss = 0.29, batch loss = 0.23 (29.4 examples/sec; 0.272 sec/batch; 23h:31m:11s remains)
INFO - root - 2017-12-16 05:20:59.954660: step 21490, loss = 0.22, batch loss = 0.16 (29.0 examples/sec; 0.276 sec/batch; 23h:50m:02s remains)
INFO - root - 2017-12-16 05:21:02.770254: step 21500, loss = 0.31, batch loss = 0.25 (27.2 examples/sec; 0.295 sec/batch; 25h:26m:59s remains)
2017-12-16 05:21:03.212297: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7197876 -6.2710552 -6.7726011 -7.1446972 -7.2800708 -7.3289337 -7.3664255 -7.4341507 -7.5361814 -7.639492 -7.3992953 -7.1278744 -6.7087622 -6.06048 -5.2944703][-6.9892416 -7.4578934 -7.7084703 -7.77968 -7.8162746 -8.11298 -8.378006 -8.6285162 -8.9174595 -9.2175941 -9.19834 -8.9502506 -8.2574358 -7.4776244 -6.6172705][-7.4762836 -7.7012234 -7.7645044 -7.403717 -7.177907 -7.2389574 -7.5020847 -8.420702 -9.21879 -9.8264275 -10.114483 -10.033352 -9.6285505 -8.9676657 -8.15898][-7.4818249 -7.3079376 -6.6753349 -5.9927464 -5.3077984 -4.9816146 -5.0817943 -5.9214816 -7.10643 -8.7383661 -9.6945419 -9.8392515 -9.7898264 -9.6293774 -9.2982941][-6.7122068 -5.8154325 -4.3240204 -2.7904429 -1.4023345 -0.79522324 -1.0710237 -1.8598688 -3.13278 -4.9965825 -6.740613 -8.33222 -9.345645 -9.7085915 -9.872221][-5.5443735 -4.1523342 -2.21579 0.28235006 2.4887886 3.7494564 3.9984121 2.8978105 1.1044245 -0.95061564 -3.0595443 -5.3161335 -7.4557314 -8.9773474 -9.709137][-5.2769556 -3.7981639 -1.281373 1.6683507 4.4644051 6.200428 6.8240356 5.99518 4.3747044 2.2061739 -0.26921844 -2.9859312 -5.6177998 -7.4519167 -8.4230661][-5.8179884 -4.1852503 -1.8811324 1.0740738 4.3996611 6.7028666 7.6306105 7.0428724 5.6045628 3.3294835 0.880383 -1.7946403 -4.3585148 -6.3037353 -7.2604][-6.6675797 -5.3807378 -3.3572624 -0.46321464 2.3016462 4.542861 6.2770147 6.2946625 5.2600508 3.2027106 0.83731747 -1.8295462 -4.30998 -5.9172134 -6.3550758][-7.6126375 -6.94153 -5.4402084 -3.4571753 -1.2471306 0.91816807 2.538949 2.8337255 2.5084472 0.92630529 -0.93807244 -3.1400385 -5.0920181 -6.1417809 -6.5054169][-8.4079256 -8.09516 -7.0818377 -5.9130363 -4.575882 -3.1611447 -1.4269135 -0.58018064 -0.74028611 -2.0566428 -3.2817283 -4.6985273 -5.9886155 -6.7711725 -7.0534697][-8.5036583 -8.3447418 -8.0945549 -7.2219067 -6.0751324 -5.3090925 -4.1752253 -3.6873055 -3.382884 -3.838553 -4.9062967 -6.323441 -7.22759 -7.3148708 -7.170104][-7.3697023 -7.3049259 -7.435648 -6.9817019 -6.5069079 -5.7000661 -4.7782931 -4.8231883 -4.6825957 -4.97075 -5.4209361 -6.2520838 -6.9797239 -7.1464148 -7.0725393][-5.8653436 -5.8419247 -5.8150616 -5.7089925 -5.6113586 -5.0580206 -4.52851 -4.6572571 -4.768261 -4.9233546 -5.023664 -5.4501767 -5.9474821 -5.9981561 -5.7883768][-4.5302196 -4.2742944 -4.0864854 -4.047864 -4.0410638 -3.9254327 -3.8446913 -3.8958914 -3.9163256 -4.1219711 -4.193893 -4.2002134 -4.3035526 -4.4311447 -4.5392303]]...]
INFO - root - 2017-12-16 05:21:06.076298: step 21510, loss = 0.27, batch loss = 0.21 (28.3 examples/sec; 0.282 sec/batch; 24h:22m:51s remains)
INFO - root - 2017-12-16 05:21:08.932888: step 21520, loss = 0.23, batch loss = 0.17 (27.5 examples/sec; 0.291 sec/batch; 25h:10m:09s remains)
INFO - root - 2017-12-16 05:21:11.769172: step 21530, loss = 0.28, batch loss = 0.22 (29.3 examples/sec; 0.273 sec/batch; 23h:34m:58s remains)
INFO - root - 2017-12-16 05:21:14.588610: step 21540, loss = 0.26, batch loss = 0.20 (29.2 examples/sec; 0.274 sec/batch; 23h:38m:36s remains)
INFO - root - 2017-12-16 05:21:17.409728: step 21550, loss = 0.28, batch loss = 0.22 (28.9 examples/sec; 0.277 sec/batch; 23h:55m:34s remains)
INFO - root - 2017-12-16 05:21:20.257779: step 21560, loss = 0.29, batch loss = 0.24 (26.6 examples/sec; 0.300 sec/batch; 25h:56m:44s remains)
INFO - root - 2017-12-16 05:21:23.157927: step 21570, loss = 0.31, batch loss = 0.26 (26.3 examples/sec; 0.305 sec/batch; 26h:19m:14s remains)
INFO - root - 2017-12-16 05:21:25.965524: step 21580, loss = 0.25, batch loss = 0.19 (28.6 examples/sec; 0.280 sec/batch; 24h:11m:43s remains)
INFO - root - 2017-12-16 05:21:28.807033: step 21590, loss = 0.27, batch loss = 0.22 (26.9 examples/sec; 0.297 sec/batch; 25h:39m:27s remains)
INFO - root - 2017-12-16 05:21:31.673221: step 21600, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 24h:05m:53s remains)
2017-12-16 05:21:32.123832: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6747866 -4.8838916 -3.7945421 -2.8149188 -2.0033042 -1.7986414 -1.7262101 -2.0511901 -2.9037559 -3.6774232 -4.1769342 -4.4594011 -4.42369 -4.0068398 -3.3759322][-4.9417076 -4.2039671 -3.2453203 -2.3053136 -1.5853539 -1.2748988 -1.2686872 -1.7119286 -2.4555349 -3.2981937 -3.9455266 -4.2178831 -4.2074332 -3.9231236 -3.3796537][-4.0081077 -3.338131 -2.5551176 -1.7725573 -1.2410345 -1.0406508 -1.047148 -1.4599743 -2.282568 -3.2761786 -4.0377884 -4.5795436 -4.6165075 -4.2703595 -3.6899996][-2.8565938 -2.3949082 -1.730504 -1.2494416 -0.84603834 -0.66362524 -0.68094635 -1.091172 -2.0202136 -3.0654926 -3.992934 -4.54639 -4.7030497 -4.2967649 -3.6793976][-1.8585076 -1.5374069 -1.0008216 -0.70215154 -0.13815546 0.37296534 0.58708382 0.35523796 -0.66289711 -1.9718883 -3.2723494 -4.1944718 -4.5392423 -4.3934865 -3.7546349][-1.5672679 -1.4052405 -1.0071111 -0.60930634 0.25458765 1.1902728 1.8484678 1.7823806 0.919116 -0.67619181 -2.3725858 -3.7462578 -4.5702019 -4.7246323 -4.2470016][-1.6371622 -1.6399777 -1.3102508 -0.78434992 0.28398037 1.5295105 2.4353466 2.5238981 1.6252546 -0.079692841 -2.0780456 -3.8994176 -4.9518976 -5.293879 -4.9450636][-2.0212467 -1.94379 -1.6238098 -0.96607876 0.37044334 1.6328287 2.4899597 2.7244678 1.917923 0.092821121 -2.1182094 -3.9653025 -5.21063 -5.6889148 -5.3077669][-2.3738451 -2.3682947 -2.028547 -1.4950578 -0.17788792 1.1787734 2.2135549 2.5060019 1.6886573 0.048817158 -2.0925109 -4.11393 -5.5468044 -6.1048007 -5.7526288][-2.4990501 -2.6237917 -2.4166191 -1.9414625 -0.97630429 0.17926025 1.1988163 1.6123533 0.99266958 -0.54092216 -2.5862119 -4.4696507 -5.8676662 -6.5057125 -6.1569138][-2.2278802 -2.6121614 -2.866677 -2.6662457 -1.8324153 -0.71874881 0.17469263 0.56566238 0.10225773 -1.3350465 -3.1681857 -4.9535942 -6.1867127 -6.634553 -6.2919755][-2.2876761 -2.8387876 -3.1145034 -3.0377738 -2.4028099 -1.4455159 -0.6849587 -0.37385225 -0.85496593 -2.1029379 -3.7646642 -5.3572626 -6.3497486 -6.5255651 -5.9949632][-2.2666976 -2.9770074 -3.3474627 -3.2817056 -2.5970321 -1.7626359 -1.2013032 -0.89474821 -1.4650221 -2.6195343 -4.0695171 -5.3826137 -6.0260143 -6.0983367 -5.5890722][-2.2842746 -2.9852986 -3.4353127 -3.32655 -2.7664962 -1.9552636 -1.3795791 -1.3587086 -1.9038064 -2.904701 -4.1917748 -5.1952696 -5.7058864 -5.6459765 -5.0063286][-2.8337822 -3.4518027 -3.6850936 -3.4016244 -2.7776847 -1.95712 -1.4750476 -1.4943378 -2.0126643 -3.0810301 -4.1839352 -5.0521455 -5.4222031 -5.1713123 -4.4425964]]...]
INFO - root - 2017-12-16 05:21:34.921063: step 21610, loss = 0.26, batch loss = 0.20 (29.3 examples/sec; 0.273 sec/batch; 23h:36m:25s remains)
INFO - root - 2017-12-16 05:21:37.705338: step 21620, loss = 0.23, batch loss = 0.17 (28.6 examples/sec; 0.280 sec/batch; 24h:10m:00s remains)
INFO - root - 2017-12-16 05:21:40.589940: step 21630, loss = 0.20, batch loss = 0.14 (27.4 examples/sec; 0.291 sec/batch; 25h:10m:15s remains)
INFO - root - 2017-12-16 05:21:43.432597: step 21640, loss = 0.27, batch loss = 0.21 (26.1 examples/sec; 0.306 sec/batch; 26h:26m:00s remains)
INFO - root - 2017-12-16 05:21:46.296509: step 21650, loss = 0.27, batch loss = 0.22 (27.7 examples/sec; 0.289 sec/batch; 24h:58m:29s remains)
INFO - root - 2017-12-16 05:21:49.104600: step 21660, loss = 0.24, batch loss = 0.18 (29.1 examples/sec; 0.275 sec/batch; 23h:44m:13s remains)
INFO - root - 2017-12-16 05:21:51.937155: step 21670, loss = 0.22, batch loss = 0.16 (27.4 examples/sec; 0.292 sec/batch; 25h:12m:08s remains)
INFO - root - 2017-12-16 05:21:54.815875: step 21680, loss = 0.27, batch loss = 0.21 (27.5 examples/sec; 0.291 sec/batch; 25h:06m:11s remains)
INFO - root - 2017-12-16 05:21:57.722531: step 21690, loss = 0.26, batch loss = 0.20 (26.6 examples/sec; 0.300 sec/batch; 25h:55m:07s remains)
INFO - root - 2017-12-16 05:22:00.522533: step 21700, loss = 0.41, batch loss = 0.35 (28.1 examples/sec; 0.284 sec/batch; 24h:33m:33s remains)
2017-12-16 05:22:00.988183: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8017783 -3.9095511 -4.1040516 -4.2375689 -4.276463 -4.3951025 -4.37162 -4.2900038 -4.2558622 -4.0848846 -4.0877137 -4.1534138 -4.38435 -5.0481958 -5.6361876][-4.0782394 -4.3445463 -4.5396242 -4.4609389 -4.2863588 -4.3010654 -4.2204103 -4.0290365 -4.0500083 -4.0371571 -4.2409859 -4.4454403 -4.7672477 -5.3745971 -5.817677][-4.2611771 -4.5540915 -4.6597352 -4.5081606 -4.0237088 -3.7411969 -3.4299741 -3.2087748 -3.3621306 -3.5250928 -4.1997833 -4.53762 -5.0458064 -5.5763979 -5.8455715][-4.4627805 -4.77194 -4.7992053 -4.2158318 -3.0610061 -2.2924187 -1.7262089 -1.6276402 -2.0938044 -2.555181 -3.4841418 -4.0947704 -4.8008275 -5.4627619 -5.785552][-4.744328 -4.5952082 -3.9683704 -2.9787455 -1.4573605 -0.16563511 0.70283604 0.94708109 0.28421783 -0.63907623 -1.9177518 -2.9732714 -3.9575839 -4.7976937 -5.3331485][-4.8953586 -4.59103 -3.5058553 -1.8446121 0.27649403 1.8247266 2.8520217 3.0461121 2.3272662 1.2076759 -0.35818338 -1.4754353 -2.5016618 -3.5365171 -4.29322][-4.6196036 -4.3445897 -3.163969 -1.3033156 0.93488073 2.709908 4.1156349 4.443161 3.684145 2.4902878 0.815114 -0.47030115 -1.6350915 -2.5223441 -3.234334][-4.4593053 -4.1774263 -3.0197909 -1.2639174 0.961462 2.8137083 4.2297087 4.4400997 3.7149944 2.6051006 1.1654229 -0.077738285 -1.1784205 -1.931488 -2.7093983][-4.6290259 -4.4951978 -3.6184886 -2.1454389 -0.40074492 1.2962351 2.8076954 3.0564985 2.4932208 1.4742393 0.31214476 -0.71712852 -1.601109 -2.153085 -2.7650845][-5.0157809 -5.0255685 -4.27802 -3.1506402 -1.7394226 -0.46481776 0.62432194 0.909153 0.619061 -0.087260723 -0.90574193 -1.6612837 -2.3123455 -2.6507638 -3.0814762][-5.1120658 -5.1836467 -4.5733762 -3.7643642 -2.8267069 -1.9546204 -1.10038 -0.88303828 -1.1308043 -1.7000759 -2.1756678 -2.4367332 -2.7019715 -2.8782988 -3.1485653][-4.5103574 -4.5536075 -4.2690973 -3.6904421 -3.2797489 -2.9164414 -2.4833384 -2.4902029 -2.6169229 -2.8861449 -3.0657921 -3.1246405 -3.1469243 -3.0026011 -3.14018][-3.3615723 -3.1506371 -3.1011162 -2.8786035 -2.7649312 -2.8393817 -2.9764414 -3.2334058 -3.3911581 -3.5927 -3.4761789 -3.2008212 -3.0062666 -2.6809506 -2.681756][-2.940114 -2.4252045 -2.1262033 -1.9480958 -2.0695927 -2.314873 -2.6580205 -3.1792097 -3.5435071 -3.6813641 -3.3724115 -2.9380767 -2.5736027 -2.200706 -2.2171965][-2.4519281 -1.8007343 -1.3286595 -1.1009369 -1.3765192 -1.822783 -2.3902376 -2.9180293 -3.1783061 -3.2760296 -2.7830141 -2.214783 -1.7227273 -1.4481189 -1.7667191]]...]
INFO - root - 2017-12-16 05:22:03.800532: step 21710, loss = 0.22, batch loss = 0.16 (28.8 examples/sec; 0.278 sec/batch; 24h:01m:03s remains)
INFO - root - 2017-12-16 05:22:06.623163: step 21720, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.281 sec/batch; 24h:17m:25s remains)
INFO - root - 2017-12-16 05:22:09.505411: step 21730, loss = 0.25, batch loss = 0.19 (27.1 examples/sec; 0.295 sec/batch; 25h:28m:51s remains)
INFO - root - 2017-12-16 05:22:12.380343: step 21740, loss = 0.32, batch loss = 0.26 (25.5 examples/sec; 0.313 sec/batch; 27h:02m:40s remains)
INFO - root - 2017-12-16 05:22:15.169574: step 21750, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.285 sec/batch; 24h:38m:37s remains)
INFO - root - 2017-12-16 05:22:18.046968: step 21760, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 24h:31m:30s remains)
INFO - root - 2017-12-16 05:22:20.866566: step 21770, loss = 0.25, batch loss = 0.19 (27.8 examples/sec; 0.288 sec/batch; 24h:51m:49s remains)
INFO - root - 2017-12-16 05:22:23.726620: step 21780, loss = 0.20, batch loss = 0.14 (26.5 examples/sec; 0.302 sec/batch; 26h:03m:42s remains)
INFO - root - 2017-12-16 05:22:26.557152: step 21790, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.284 sec/batch; 24h:32m:25s remains)
INFO - root - 2017-12-16 05:22:29.353888: step 21800, loss = 0.29, batch loss = 0.23 (28.8 examples/sec; 0.278 sec/batch; 23h:57m:31s remains)
2017-12-16 05:22:29.816810: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1269739 -2.4469252 -2.7748199 -3.1173339 -3.5446861 -4.0092549 -4.4874249 -5.0351458 -5.6374488 -5.8792295 -5.73362 -5.41437 -5.0569153 -4.5940833 -3.8579526][-2.7994695 -3.1832139 -3.4953895 -3.6991465 -3.8812661 -4.2737408 -4.6867013 -5.0897484 -5.336401 -5.5147486 -5.7740655 -5.8333049 -5.4670029 -4.8572474 -4.3091936][-3.7391365 -4.4323845 -4.833395 -4.7957129 -4.6903434 -4.5625529 -4.3463674 -4.491909 -4.7715936 -5.0414386 -5.1185493 -5.2974062 -5.580792 -5.2024684 -4.3091807][-4.1388144 -4.9728832 -5.47389 -5.2686653 -4.7425189 -3.9865253 -3.5128417 -3.2347612 -2.9881461 -3.4484529 -4.2516127 -4.8358164 -5.0704784 -5.05333 -4.5057111][-4.70368 -5.1344414 -5.2716227 -4.8590541 -3.7206891 -2.1883569 -0.94935465 -0.3960495 -0.50130391 -1.0558617 -2.1664374 -3.8375542 -4.9992218 -4.9054451 -4.3941183][-4.9266138 -5.4371438 -5.0136919 -4.212955 -2.7543445 -0.68627453 1.2201495 2.304451 2.2835898 1.2064986 -0.39853954 -2.3253167 -4.0375032 -4.6952763 -4.3578839][-4.1509118 -4.5197964 -4.2533107 -2.9194942 -1.071964 0.79409361 2.6400332 3.74255 3.5972528 2.4089832 0.36676836 -1.6555452 -3.1171548 -4.1307211 -4.2672167][-3.580816 -3.3593655 -2.8768423 -1.7572794 0.36484814 2.5017734 3.7837143 4.1423025 3.6006956 2.3018875 0.33228874 -1.8053148 -3.4267251 -4.2111235 -4.3631315][-3.1927109 -3.0161209 -2.2272763 -1.0496578 0.31819296 2.1405859 3.3047638 3.0653963 2.0438385 0.64947176 -0.97350812 -2.6434479 -4.1927533 -5.0012751 -4.958559][-3.4008098 -3.0738094 -2.4065244 -1.2001543 0.067188263 0.92339659 1.5290332 1.2905793 0.11090612 -1.4060621 -2.7754159 -3.9451613 -4.8624654 -5.4693193 -5.4343719][-4.06812 -3.5678554 -2.777391 -1.8721716 -0.76498604 -0.12353516 -0.31494951 -0.73708129 -1.5710955 -2.7986579 -3.8697267 -4.6714129 -5.2178426 -5.4157763 -5.1432309][-4.5790987 -4.0823555 -3.2215583 -2.3346374 -1.700206 -1.4226067 -1.6063869 -2.4541869 -3.4015107 -4.123951 -4.916038 -5.3877583 -5.3766809 -5.0382295 -4.5625596][-5.4172778 -4.6371293 -3.8110371 -2.8095558 -2.1216781 -2.1499789 -2.597754 -3.1598763 -4.1486945 -5.1332664 -5.5407696 -5.6150312 -5.4411383 -4.7996049 -4.0554647][-5.6500454 -4.9802856 -4.0729747 -3.0475552 -2.3416395 -1.9622619 -2.2116678 -3.0315354 -3.9001107 -4.8486419 -5.6080089 -5.5354619 -4.8873677 -4.2213626 -3.6242816][-5.1524429 -4.5246158 -3.6662469 -2.6006105 -1.6983202 -1.335279 -1.4139369 -1.9961591 -3.1838412 -4.2287264 -4.8421645 -5.1051774 -4.6682844 -3.6645269 -2.9146104]]...]
INFO - root - 2017-12-16 05:22:32.675289: step 21810, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.281 sec/batch; 24h:16m:38s remains)
INFO - root - 2017-12-16 05:22:35.509010: step 21820, loss = 0.28, batch loss = 0.22 (25.8 examples/sec; 0.310 sec/batch; 26h:43m:19s remains)
INFO - root - 2017-12-16 05:22:38.379903: step 21830, loss = 0.26, batch loss = 0.21 (28.3 examples/sec; 0.283 sec/batch; 24h:25m:33s remains)
INFO - root - 2017-12-16 05:22:41.229307: step 21840, loss = 0.25, batch loss = 0.19 (28.7 examples/sec; 0.279 sec/batch; 24h:03m:27s remains)
INFO - root - 2017-12-16 05:22:44.082534: step 21850, loss = 0.21, batch loss = 0.15 (28.7 examples/sec; 0.279 sec/batch; 24h:05m:14s remains)
INFO - root - 2017-12-16 05:22:46.909654: step 21860, loss = 0.38, batch loss = 0.32 (26.3 examples/sec; 0.304 sec/batch; 26h:11m:55s remains)
INFO - root - 2017-12-16 05:22:49.732480: step 21870, loss = 0.29, batch loss = 0.23 (28.4 examples/sec; 0.282 sec/batch; 24h:17m:51s remains)
INFO - root - 2017-12-16 05:22:52.517175: step 21880, loss = 0.41, batch loss = 0.35 (29.4 examples/sec; 0.272 sec/batch; 23h:30m:01s remains)
INFO - root - 2017-12-16 05:22:55.378298: step 21890, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.284 sec/batch; 24h:32m:28s remains)
INFO - root - 2017-12-16 05:22:58.247992: step 21900, loss = 0.28, batch loss = 0.22 (27.6 examples/sec; 0.290 sec/batch; 25h:01m:45s remains)
2017-12-16 05:22:58.722968: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3442636 -6.4117036 -6.598525 -6.9523416 -7.4720449 -7.6871281 -7.722785 -7.861752 -8.0870934 -8.0336533 -7.7463675 -7.184238 -6.5066853 -5.7305474 -5.2874875][-5.5329852 -5.6771688 -6.1149268 -6.5732846 -6.9854722 -7.4597244 -7.8279066 -8.1375265 -8.5236788 -8.9364986 -9.0096 -8.7168179 -8.1987057 -7.5452976 -7.0637479][-4.0421305 -4.4960785 -5.0079927 -5.4836631 -5.7433496 -6.0080137 -6.4580178 -6.9738245 -7.6764393 -8.5126982 -9.1893816 -9.52128 -9.3389435 -8.9255886 -8.4177408][-2.4014432 -2.8915036 -3.4866552 -3.9330029 -4.0414691 -3.9473734 -3.9891438 -4.5377274 -5.6198993 -6.913147 -7.9317265 -8.67567 -9.0800886 -9.0698385 -8.9288836][-1.2215104 -1.6203518 -2.1716437 -2.2750916 -1.956656 -1.415406 -1.043123 -1.2664466 -2.2976701 -4.0548677 -5.7048483 -6.9640737 -7.7681522 -8.0610876 -7.8094044][-1.0929389 -1.0659242 -1.266679 -0.86032248 0.14458418 1.150466 1.8299007 1.8202596 0.79910421 -1.2038958 -3.2129831 -4.8752108 -5.8984127 -6.5201483 -6.3545518][-0.82163787 -0.72659135 -0.49105597 0.42249107 1.8040361 3.2911334 4.3363228 4.2577028 3.113183 0.8639102 -1.4582577 -3.3565888 -4.5119848 -4.8540306 -4.6362877][-1.152159 -1.1988807 -0.94194007 0.43070698 2.1190529 3.7688723 4.9966774 4.9293184 3.8876419 1.6429825 -0.74473166 -2.5909407 -3.6885412 -3.9685845 -3.7785492][-2.0848248 -2.1256082 -1.8833036 -0.76261568 0.66093826 2.3947906 3.6673679 3.70864 2.6981349 0.79537392 -1.1645558 -2.8371387 -3.8233185 -3.7636478 -3.2126908][-2.2902331 -2.5581498 -2.8011975 -2.1338696 -1.0142107 0.11388445 0.8066349 1.0210481 0.51185751 -0.88658428 -2.5055232 -3.8300161 -4.4185038 -3.9996676 -3.1682138][-3.3403108 -3.5402274 -3.5244036 -3.2878404 -3.05218 -2.3757396 -1.7981808 -1.9427483 -2.464201 -3.2609668 -4.1767769 -5.0585637 -5.3923593 -4.7693253 -3.8732586][-4.2131548 -4.600646 -4.9712715 -4.8978605 -4.6135721 -4.2551332 -4.220552 -4.3469992 -4.4513736 -4.8527007 -5.327178 -5.7982607 -5.8690586 -5.2949514 -4.4428086][-5.2275529 -5.5113788 -5.7853756 -6.073843 -6.3319077 -6.1406808 -5.9249744 -5.8300128 -5.8863544 -5.8919191 -5.6105366 -5.4811125 -5.3130746 -4.9523726 -4.3899083][-5.2782345 -5.3125763 -5.3042989 -5.5520616 -5.8826079 -6.1339855 -6.2701216 -6.3405676 -6.277781 -6.1757932 -5.9605074 -5.791152 -5.5445242 -5.1006937 -4.6455822][-5.2677426 -5.1760716 -4.9998732 -4.9760566 -5.1195135 -5.3779073 -5.6265426 -5.7351665 -5.7552643 -5.8170271 -5.7608547 -5.59424 -5.3511572 -5.0709224 -4.63452]]...]
INFO - root - 2017-12-16 05:23:01.534673: step 21910, loss = 0.28, batch loss = 0.22 (28.5 examples/sec; 0.280 sec/batch; 24h:11m:17s remains)
INFO - root - 2017-12-16 05:23:04.330256: step 21920, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.282 sec/batch; 24h:18m:35s remains)
INFO - root - 2017-12-16 05:23:07.204990: step 21930, loss = 0.36, batch loss = 0.30 (25.0 examples/sec; 0.320 sec/batch; 27h:36m:46s remains)
INFO - root - 2017-12-16 05:23:10.035132: step 21940, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 24h:03m:39s remains)
INFO - root - 2017-12-16 05:23:12.852196: step 21950, loss = 0.24, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 24h:23m:55s remains)
INFO - root - 2017-12-16 05:23:15.642336: step 21960, loss = 0.25, batch loss = 0.20 (27.8 examples/sec; 0.287 sec/batch; 24h:47m:40s remains)
INFO - root - 2017-12-16 05:23:18.494925: step 21970, loss = 0.22, batch loss = 0.16 (28.2 examples/sec; 0.284 sec/batch; 24h:29m:40s remains)
INFO - root - 2017-12-16 05:23:21.338574: step 21980, loss = 0.26, batch loss = 0.20 (28.5 examples/sec; 0.281 sec/batch; 24h:14m:21s remains)
INFO - root - 2017-12-16 05:23:24.142872: step 21990, loss = 0.33, batch loss = 0.27 (27.5 examples/sec; 0.291 sec/batch; 25h:03m:50s remains)
INFO - root - 2017-12-16 05:23:26.963076: step 22000, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.277 sec/batch; 23h:50m:54s remains)
2017-12-16 05:23:27.412836: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3902657 -2.5844057 -2.8925228 -3.2751508 -3.2640753 -3.1231613 -3.0971003 -3.2434647 -3.5829606 -4.1273909 -4.881124 -5.8430481 -6.7028027 -6.9676294 -6.9332962][-1.4986916 -1.8379083 -2.2142546 -2.6119356 -2.6335237 -2.6110568 -2.6501813 -2.8499403 -3.2220669 -3.8729954 -4.7301278 -5.6438279 -6.5740528 -7.1877251 -7.4681759][-1.1753864 -1.6135876 -2.0066168 -2.2038658 -2.1863489 -2.0087619 -1.8800774 -2.1483831 -2.5893979 -3.1837316 -3.9807737 -5.0803089 -6.2311339 -6.877286 -7.1535387][-0.46560311 -0.84248114 -1.1356401 -1.1623294 -0.9719553 -0.79300547 -0.74526525 -1.0065644 -1.3671408 -1.9270687 -2.7158732 -3.8865352 -5.1486721 -5.9919844 -6.5274911][-0.786052 -0.89757442 -0.8280592 -0.59839725 -0.26948833 -0.023077488 0.043447018 -0.068757057 -0.28114653 -0.7904191 -1.4037449 -2.3538957 -3.4608092 -4.3209515 -4.9044805][-1.6324997 -1.3565762 -0.884604 -0.53494692 -0.11308336 0.33737469 0.6654 0.67871284 0.52365303 0.31678104 -0.095821857 -0.87691116 -1.8158782 -2.6394305 -3.178051][-2.5665517 -1.9726136 -1.2732496 -0.59489489 0.12858772 0.64660454 1.0455704 1.2081146 1.2302117 1.1108241 0.71199322 0.18289614 -0.49083018 -1.260155 -1.6982191][-3.4022908 -2.6574583 -1.8485334 -1.0556686 -0.26858282 0.40936232 1.0010514 1.2061439 1.3155909 1.3330002 1.0633945 0.6894002 0.28387403 -0.12192106 -0.28140163][-4.1880198 -3.35399 -2.5068741 -1.7102873 -0.94625139 -0.25221109 0.3103919 0.56972742 0.81540537 0.82074881 0.54236317 0.31959105 0.19852877 0.12783098 0.21057749][-4.7116623 -3.8281126 -2.9922271 -2.2547848 -1.5638747 -1.0341585 -0.64829373 -0.40595627 -0.19107246 -0.13321447 -0.283751 -0.56564093 -0.62832546 -0.53500462 -0.3058219][-4.9208312 -4.3447165 -3.7762084 -3.1259408 -2.4445064 -1.9829586 -1.6402297 -1.4759767 -1.3989305 -1.4740183 -1.6882298 -1.945199 -1.931803 -1.7262511 -1.3848312][-4.8720665 -4.6998806 -4.5393419 -4.2232661 -3.7904351 -3.41162 -3.031816 -2.8317432 -2.7955184 -2.8752918 -3.0512304 -3.2853923 -3.3612266 -3.1585932 -2.7118647][-4.517695 -4.61635 -4.7423773 -4.7612104 -4.6799583 -4.52619 -4.2847838 -3.996731 -3.8431315 -4.0010886 -4.3286304 -4.5424719 -4.6250176 -4.6281309 -4.3911567][-3.5172589 -3.8020535 -4.2216439 -4.4882445 -4.6722364 -4.667666 -4.5623741 -4.3741693 -4.3922172 -4.65008 -5.0262814 -5.4250994 -5.7102556 -5.810276 -5.625123][-2.7126784 -2.8874707 -3.1896806 -3.5406561 -3.9094727 -4.1642761 -4.2878671 -4.26722 -4.3827662 -4.6567936 -5.0927815 -5.523591 -5.8956642 -6.1352091 -6.0754042]]...]
INFO - root - 2017-12-16 05:23:30.286844: step 22010, loss = 0.25, batch loss = 0.19 (26.7 examples/sec; 0.299 sec/batch; 25h:47m:50s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 05:23:33.094497: step 22020, loss = 0.31, batch loss = 0.25 (29.0 examples/sec; 0.276 sec/batch; 23h:46m:25s remains)
INFO - root - 2017-12-16 05:23:35.948928: step 22030, loss = 0.23, batch loss = 0.17 (25.9 examples/sec; 0.309 sec/batch; 26h:36m:43s remains)
INFO - root - 2017-12-16 05:23:38.800204: step 22040, loss = 0.22, batch loss = 0.16 (26.8 examples/sec; 0.298 sec/batch; 25h:41m:49s remains)
INFO - root - 2017-12-16 05:23:41.691502: step 22050, loss = 0.33, batch loss = 0.27 (26.3 examples/sec; 0.304 sec/batch; 26h:13m:56s remains)
INFO - root - 2017-12-16 05:23:44.501827: step 22060, loss = 0.32, batch loss = 0.27 (28.1 examples/sec; 0.284 sec/batch; 24h:31m:09s remains)
INFO - root - 2017-12-16 05:23:47.323017: step 22070, loss = 0.27, batch loss = 0.21 (29.4 examples/sec; 0.272 sec/batch; 23h:25m:54s remains)
INFO - root - 2017-12-16 05:23:50.169854: step 22080, loss = 0.23, batch loss = 0.17 (29.4 examples/sec; 0.272 sec/batch; 23h:28m:52s remains)
INFO - root - 2017-12-16 05:23:52.999901: step 22090, loss = 0.31, batch loss = 0.25 (27.7 examples/sec; 0.289 sec/batch; 24h:55m:04s remains)
INFO - root - 2017-12-16 05:23:55.812721: step 22100, loss = 0.25, batch loss = 0.20 (27.9 examples/sec; 0.287 sec/batch; 24h:44m:03s remains)
2017-12-16 05:23:56.279759: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3539882 -3.4640522 -3.4777584 -3.4356985 -3.4077878 -3.3555412 -3.28718 -3.1386495 -3.0530729 -2.8784804 -2.714663 -2.6660728 -2.5979729 -2.5632143 -2.5412951][-3.6030748 -3.7344103 -3.7951436 -3.8913088 -3.95529 -3.8767631 -3.6653392 -3.4380529 -3.239634 -2.9957714 -2.8800149 -2.7725844 -2.6993396 -2.7086267 -2.6659222][-3.6620297 -3.9733462 -4.2085104 -4.0983968 -3.866312 -3.7382309 -3.5336843 -3.3729258 -3.2046289 -3.1196904 -3.1637006 -3.1019361 -3.1100197 -3.140666 -3.0956168][-3.4652834 -3.6737 -3.8048675 -3.6430073 -3.2942803 -2.9294167 -2.6095772 -2.4941792 -2.4428198 -2.627275 -2.9464555 -3.1841369 -3.5019212 -3.5910313 -3.5102158][-2.8250551 -2.8781347 -2.8469708 -2.5109277 -1.8478806 -1.2378714 -0.75301719 -0.62776971 -0.8663342 -1.526036 -2.2650297 -2.8767438 -3.3338594 -3.6856017 -3.8910213][-2.211802 -2.1301734 -1.877871 -1.059777 0.13061714 0.90016413 1.3628983 1.4955487 1.1789775 0.33063745 -0.72248363 -1.8495383 -2.888464 -3.5112109 -3.6834905][-1.6704261 -1.2225749 -0.63966465 0.4085722 1.8558211 3.0303688 3.8723707 3.9518023 3.3878694 2.1483116 0.5829215 -0.80934644 -1.8623068 -2.6641231 -3.1619008][-1.4189329 -0.91246891 -0.20806932 1.0095496 2.6659684 4.056694 4.9104757 4.7603388 3.9944468 2.7735028 1.2413054 -0.39043951 -1.7913284 -2.5549111 -2.8602886][-1.9024944 -1.5253661 -1.0433433 -0.049168587 1.1245017 2.394495 3.5009465 3.5808525 2.8884249 1.6051087 0.04120779 -1.4653559 -2.4938886 -2.8659666 -2.9664226][-2.2481258 -2.2887011 -2.1641006 -1.7570882 -0.92115164 0.21597767 0.98433971 0.86194897 0.34771824 -0.63837528 -1.9933805 -3.2497425 -3.9626293 -3.9641469 -3.556437][-2.6855998 -2.8965759 -2.988899 -2.9058132 -2.7072735 -2.3098943 -1.6749125 -1.6906447 -2.2409484 -3.0441303 -4.1680903 -5.0511951 -5.3391733 -5.0086575 -4.3669109][-2.8572977 -3.135159 -3.5995157 -4.1036019 -4.2252731 -4.2593913 -4.3153253 -4.6939039 -4.8514085 -5.137454 -5.8417692 -6.4161468 -6.4990687 -5.8644366 -4.8018007][-2.4881425 -2.7129087 -3.268918 -3.9547691 -4.5461874 -5.0373697 -5.3559709 -5.8771768 -6.193903 -6.4445572 -6.5915704 -6.6239443 -6.3457794 -5.9469924 -5.387352][-2.2203066 -2.110553 -2.3798444 -3.0072324 -3.5852268 -4.1975241 -4.7672858 -5.4009471 -5.7222152 -5.7124672 -5.6119914 -5.6076875 -5.6977992 -5.477263 -5.086998][-1.9129944 -1.5049806 -1.5486543 -1.9108636 -2.2207453 -2.9027145 -3.60009 -4.2484121 -4.5492535 -4.8324375 -4.9748731 -4.648757 -4.1804647 -4.0475359 -4.3443656]]...]
INFO - root - 2017-12-16 05:23:59.126753: step 22110, loss = 0.31, batch loss = 0.25 (27.6 examples/sec; 0.290 sec/batch; 24h:59m:33s remains)
INFO - root - 2017-12-16 05:24:01.961095: step 22120, loss = 0.24, batch loss = 0.18 (26.9 examples/sec; 0.297 sec/batch; 25h:36m:03s remains)
INFO - root - 2017-12-16 05:24:04.850583: step 22130, loss = 0.23, batch loss = 0.17 (27.9 examples/sec; 0.287 sec/batch; 24h:42m:07s remains)
INFO - root - 2017-12-16 05:24:07.685978: step 22140, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 23h:59m:09s remains)
INFO - root - 2017-12-16 05:24:10.509722: step 22150, loss = 0.21, batch loss = 0.16 (28.5 examples/sec; 0.280 sec/batch; 24h:09m:50s remains)
INFO - root - 2017-12-16 05:24:13.327860: step 22160, loss = 0.24, batch loss = 0.18 (27.8 examples/sec; 0.287 sec/batch; 24h:46m:28s remains)
INFO - root - 2017-12-16 05:24:16.180659: step 22170, loss = 0.44, batch loss = 0.38 (27.9 examples/sec; 0.286 sec/batch; 24h:41m:18s remains)
INFO - root - 2017-12-16 05:24:18.985517: step 22180, loss = 0.34, batch loss = 0.28 (29.3 examples/sec; 0.273 sec/batch; 23h:33m:03s remains)
INFO - root - 2017-12-16 05:24:21.821315: step 22190, loss = 0.25, batch loss = 0.19 (28.3 examples/sec; 0.283 sec/batch; 24h:21m:37s remains)
INFO - root - 2017-12-16 05:24:24.637466: step 22200, loss = 0.37, batch loss = 0.31 (29.0 examples/sec; 0.276 sec/batch; 23h:45m:39s remains)
2017-12-16 05:24:25.107138: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2488832 -6.3373413 -6.6287255 -7.087811 -7.2174249 -6.9213462 -6.4249005 -6.2930131 -6.2442017 -6.4681921 -6.7658858 -7.2014246 -7.7176108 -7.7517505 -7.5800629][-6.0914426 -6.3608952 -6.6218538 -7.0015769 -7.2009621 -7.2772131 -6.96758 -6.6335959 -6.5729094 -6.7166305 -7.072269 -7.6544633 -8.1975231 -8.3715916 -8.32058][-5.475513 -5.7763443 -6.0241122 -6.2541642 -6.2567716 -6.2684345 -6.03202 -5.8597746 -5.7787275 -6.0226769 -6.8146048 -7.4177761 -8.2315512 -8.481102 -8.5648632][-4.4268117 -4.6853085 -4.5979519 -4.5057087 -4.2963295 -4.1378255 -3.8007481 -3.7622719 -3.7776065 -3.8877103 -4.4442263 -5.7417245 -7.3947415 -8.2985182 -8.9160786][-3.5885334 -3.1990328 -2.6402416 -2.3698471 -1.904639 -1.2344513 -0.73539495 -0.61922812 -0.80176282 -1.3486192 -2.3093731 -3.5078506 -5.3689208 -7.2729368 -8.6501236][-3.2556956 -2.5627465 -1.264554 -0.29535246 0.68865395 1.3167291 1.9804435 2.6127334 2.8383131 2.3164554 1.0106239 -1.3248415 -4.2247396 -6.5160155 -8.2340755][-3.4167702 -2.2399583 -0.79151392 0.58411741 2.2008171 3.3889766 4.4690657 5.1070213 5.3728552 4.873908 3.4397326 0.92778635 -2.2602732 -5.3949938 -7.7972116][-3.8140988 -2.4899492 -0.85022688 0.65506697 1.979373 3.3757505 4.9481831 5.8343897 6.3001728 5.6893435 4.1451035 1.5419507 -1.7429113 -4.4585433 -6.4646749][-4.7476163 -3.4944735 -2.1581268 -0.89626884 0.46630049 1.7963915 3.1147919 4.2076149 5.0283594 4.6801195 3.4922466 1.2469287 -1.5042586 -4.0151854 -5.7633247][-5.2535529 -4.7014055 -4.1802349 -3.202323 -2.242887 -1.3550849 -0.22953844 0.81971884 1.6007733 1.7715335 1.2048016 -0.1981926 -1.974474 -3.7829618 -5.0627756][-6.0504274 -5.541544 -5.2409248 -5.2387486 -5.0953612 -4.4141264 -3.5210898 -2.82301 -2.1760254 -1.9144924 -1.7689342 -2.2760663 -3.1998711 -4.0552483 -4.6734781][-5.9944224 -6.077673 -6.406786 -6.4452147 -6.5775709 -6.7305727 -6.2707424 -5.4974165 -4.749743 -4.1985979 -3.7371266 -3.8130472 -4.0902996 -4.6002994 -5.0318055][-5.4074512 -5.5608606 -6.0306931 -6.43106 -6.8927526 -7.0851235 -6.9596262 -6.7306862 -6.262558 -5.76705 -5.2399783 -4.8912196 -4.6870918 -4.8409491 -5.0508614][-4.8514323 -4.9080429 -5.1969109 -5.59522 -6.0471087 -6.2301311 -6.277586 -6.2717032 -6.0491009 -5.6720114 -5.2284517 -5.0573697 -4.8834052 -4.7619009 -4.7238817][-4.2612915 -4.2576709 -4.3865957 -4.667943 -4.9492893 -5.0666714 -5.07244 -4.9100847 -4.6401377 -4.4185305 -4.1412163 -3.8534408 -3.721931 -3.7988865 -3.8571901]]...]
INFO - root - 2017-12-16 05:24:27.956956: step 22210, loss = 0.39, batch loss = 0.33 (29.0 examples/sec; 0.276 sec/batch; 23h:48m:09s remains)
INFO - root - 2017-12-16 05:24:30.800460: step 22220, loss = 0.23, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 24h:22m:49s remains)
INFO - root - 2017-12-16 05:24:33.635674: step 22230, loss = 0.23, batch loss = 0.17 (26.6 examples/sec; 0.301 sec/batch; 25h:56m:02s remains)
INFO - root - 2017-12-16 05:24:36.495820: step 22240, loss = 0.25, batch loss = 0.19 (27.7 examples/sec; 0.289 sec/batch; 24h:54m:36s remains)
INFO - root - 2017-12-16 05:24:39.292353: step 22250, loss = 0.41, batch loss = 0.35 (27.4 examples/sec; 0.292 sec/batch; 25h:08m:59s remains)
INFO - root - 2017-12-16 05:24:42.137688: step 22260, loss = 0.18, batch loss = 0.13 (26.2 examples/sec; 0.305 sec/batch; 26h:17m:27s remains)
INFO - root - 2017-12-16 05:24:45.002813: step 22270, loss = 0.22, batch loss = 0.17 (27.0 examples/sec; 0.296 sec/batch; 25h:29m:10s remains)
INFO - root - 2017-12-16 05:24:47.826452: step 22280, loss = 0.25, batch loss = 0.19 (29.4 examples/sec; 0.272 sec/batch; 23h:24m:47s remains)
INFO - root - 2017-12-16 05:24:50.645338: step 22290, loss = 0.24, batch loss = 0.18 (27.3 examples/sec; 0.294 sec/batch; 25h:17m:39s remains)
INFO - root - 2017-12-16 05:24:53.471786: step 22300, loss = 0.37, batch loss = 0.32 (28.3 examples/sec; 0.283 sec/batch; 24h:21m:08s remains)
2017-12-16 05:24:53.948122: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7313323 -4.12832 -4.6024747 -5.1587529 -5.6167936 -5.9147024 -6.2152348 -6.5631628 -6.8872757 -6.8183689 -6.6763725 -6.3416481 -5.9518003 -5.4876318 -5.0481758][-3.8614821 -4.2193828 -4.5120668 -4.8075047 -5.0890713 -5.4519315 -5.7832584 -6.0954781 -6.3236866 -6.4117746 -6.3629189 -5.9270439 -5.5449595 -4.960186 -4.6421084][-4.0138149 -4.4407768 -4.7092466 -4.8157654 -4.7302837 -4.6723952 -4.6424217 -4.8275542 -4.9735613 -5.1842785 -5.3791075 -5.2359276 -5.04416 -4.6138687 -4.5696416][-4.1092925 -4.4818344 -4.6131968 -4.4993086 -4.0033092 -3.4862099 -3.2639549 -3.1593862 -3.2157516 -3.45857 -3.7517588 -4.0508723 -4.2480483 -4.1974115 -4.2519641][-3.845016 -3.9259996 -3.6243861 -3.1648273 -2.6185536 -1.8688436 -1.1651373 -0.84329367 -0.90760231 -1.4386339 -2.1624279 -2.8801465 -3.5415976 -4.0144024 -4.457406][-3.3233004 -3.1059344 -2.6138544 -1.7057457 -0.53521109 0.52025509 1.3110824 1.7474256 1.9052157 1.2387176 -0.016140938 -1.3924513 -2.5986762 -3.378737 -4.121325][-3.363348 -2.8686914 -2.1035154 -0.95803952 0.45039654 1.8932624 3.1876669 3.9401989 3.9911747 3.3075342 1.996171 0.15228319 -1.5865221 -2.5905352 -3.3987846][-3.5222075 -2.9888978 -2.1437 -0.90327549 0.49191332 2.0532742 3.5009055 4.3239746 4.6080446 3.9085941 2.4001417 0.47613382 -1.3981714 -2.7402339 -3.6881623][-3.3689084 -3.0783579 -2.5649734 -1.6727357 -0.47084451 0.98658466 2.4503174 3.3542066 3.6975031 3.0569534 1.6725812 -0.10955715 -1.8820555 -3.0971012 -3.8469334][-3.5760922 -3.5031252 -3.320349 -2.7517266 -1.895963 -0.74420333 0.55087185 1.3346462 1.5927677 1.0058432 -0.15526628 -1.7733355 -3.2881341 -4.0430346 -4.3218913][-4.2504673 -4.3226151 -4.2968435 -4.0752239 -3.6118574 -2.6916378 -1.6381559 -1.0144782 -0.81146121 -1.3442013 -2.4198835 -3.7399607 -4.9026651 -5.39985 -5.396935][-4.7043114 -4.8485718 -5.0713034 -5.1025372 -4.8784976 -4.341238 -3.57545 -3.1115308 -2.8542285 -3.1783752 -3.9569273 -5.073163 -6.0112348 -6.2508197 -6.0166874][-4.6590204 -4.8419542 -5.1293855 -5.23383 -5.2240734 -4.86641 -4.39557 -3.9952376 -3.7192707 -3.9439955 -4.470479 -5.2093167 -5.7454181 -6.0216727 -5.9299641][-4.178802 -4.171793 -4.3430581 -4.536056 -4.6400275 -4.5299373 -4.3120203 -4.1076045 -3.8993664 -3.8810205 -4.1008692 -4.6684804 -5.2255907 -5.4172044 -5.32201][-3.9376628 -3.7205262 -3.647687 -3.6787367 -3.7319388 -3.7498076 -3.7193105 -3.6103904 -3.5187893 -3.6060936 -3.7928913 -4.0314317 -4.2699757 -4.5258017 -4.5764332]]...]
INFO - root - 2017-12-16 05:24:56.813286: step 22310, loss = 0.24, batch loss = 0.18 (27.4 examples/sec; 0.292 sec/batch; 25h:11m:36s remains)
INFO - root - 2017-12-16 05:24:59.636404: step 22320, loss = 0.34, batch loss = 0.28 (28.1 examples/sec; 0.285 sec/batch; 24h:30m:57s remains)
INFO - root - 2017-12-16 05:25:02.465438: step 22330, loss = 0.23, batch loss = 0.17 (28.8 examples/sec; 0.278 sec/batch; 23h:58m:21s remains)
INFO - root - 2017-12-16 05:25:05.279293: step 22340, loss = 0.27, batch loss = 0.22 (29.3 examples/sec; 0.273 sec/batch; 23h:32m:40s remains)
INFO - root - 2017-12-16 05:25:08.077476: step 22350, loss = 0.24, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 23h:38m:33s remains)
INFO - root - 2017-12-16 05:25:10.875580: step 22360, loss = 0.30, batch loss = 0.24 (28.4 examples/sec; 0.281 sec/batch; 24h:14m:18s remains)
INFO - root - 2017-12-16 05:25:13.707131: step 22370, loss = 0.23, batch loss = 0.18 (28.5 examples/sec; 0.280 sec/batch; 24h:09m:10s remains)
INFO - root - 2017-12-16 05:25:16.553486: step 22380, loss = 0.24, batch loss = 0.18 (29.5 examples/sec; 0.271 sec/batch; 23h:22m:16s remains)
INFO - root - 2017-12-16 05:25:19.376058: step 22390, loss = 0.23, batch loss = 0.17 (28.6 examples/sec; 0.279 sec/batch; 24h:03m:23s remains)
INFO - root - 2017-12-16 05:25:22.186975: step 22400, loss = 0.37, batch loss = 0.31 (29.1 examples/sec; 0.275 sec/batch; 23h:39m:07s remains)
2017-12-16 05:25:22.687383: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0041442 -4.1030674 -4.2022872 -4.3561707 -4.4991055 -4.3482184 -4.0723419 -3.8668864 -3.7614045 -3.666286 -3.5097907 -3.3154817 -3.1328125 -3.0684738 -3.1885672][-4.1035123 -4.3043737 -4.448184 -4.462235 -4.4350619 -4.2832465 -3.9820206 -3.6040168 -3.3222389 -3.1730285 -3.0139832 -2.8125887 -2.8046865 -2.9744127 -3.1822057][-4.0664024 -4.2768788 -4.3140664 -4.1827993 -3.9583488 -3.6060567 -3.1820264 -2.8087554 -2.5600655 -2.4201124 -2.488647 -2.5512457 -2.7132325 -2.956841 -3.2760448][-3.6415412 -3.652724 -3.4966571 -3.2132225 -2.7888353 -2.2442541 -1.5974243 -1.1879084 -0.99551797 -1.1708572 -1.6013346 -2.018286 -2.6051102 -3.091157 -3.4950318][-3.05304 -2.7482393 -2.277904 -1.7494206 -1.0950363 -0.27103138 0.53498411 0.9958806 1.1435943 0.67981339 -0.18743372 -1.2021506 -2.2434702 -3.0889931 -3.6814132][-2.6173482 -1.9757512 -1.2471447 -0.29466867 0.76348162 1.7303476 2.6308479 3.2523751 3.3652492 2.6848674 1.3670449 -0.18336439 -1.5868244 -2.7625828 -3.562084][-2.3723133 -1.3213403 -0.26604748 0.77937222 1.9779506 3.2824054 4.4636097 4.8913393 4.7776117 4.0599031 2.6170826 0.65536356 -1.2094998 -2.577642 -3.4631174][-2.1984489 -1.1797502 -0.31272507 0.85015297 2.1029477 3.3968582 4.7389555 5.3664827 5.3240156 4.2001057 2.3948426 0.41048861 -1.568543 -3.0575557 -3.9241812][-2.5948064 -1.8319416 -1.2486024 -0.21740294 0.90837669 2.2672911 3.5849953 4.1771755 4.1645508 3.1347675 1.5028858 -0.59564328 -2.6107364 -3.97373 -4.7515464][-3.1532459 -2.8208556 -2.6755297 -1.9355421 -1.0671387 0.14647436 1.3512859 1.9042883 1.9167032 0.95809555 -0.39284039 -2.0733068 -3.6825035 -4.8080893 -5.3866272][-4.0683503 -3.8750577 -3.7407992 -3.3112326 -2.8029387 -1.8868637 -0.85010934 -0.44289804 -0.52779436 -1.314244 -2.3096209 -3.5195198 -4.6871171 -5.3361826 -5.6742043][-4.1757088 -4.3677006 -4.5668659 -4.2259536 -3.8299925 -3.2457356 -2.5035658 -2.1538904 -2.0401134 -2.4833035 -3.0978913 -3.9256732 -4.7749109 -5.0853181 -5.3004904][-3.7700379 -3.9296207 -4.1660447 -4.0870509 -3.9626608 -3.5861726 -3.0874202 -2.9584637 -2.8918667 -2.9568729 -3.0691218 -3.3855665 -3.8523426 -4.0209346 -4.2702951][-3.2535806 -3.3435569 -3.4411464 -3.4176891 -3.4714723 -3.2832437 -2.9639077 -2.8101358 -2.7109904 -2.5986421 -2.4712312 -2.5220513 -2.8036864 -2.9615693 -3.2561531][-2.7809229 -2.7710555 -2.7677341 -2.7179937 -2.736232 -2.6769176 -2.5022271 -2.287643 -2.1033015 -1.8987932 -1.7328591 -1.612504 -1.7811818 -2.0482326 -2.464551]]...]
INFO - root - 2017-12-16 05:25:25.576365: step 22410, loss = 0.28, batch loss = 0.22 (28.5 examples/sec; 0.281 sec/batch; 24h:10m:04s remains)
INFO - root - 2017-12-16 05:25:28.372946: step 22420, loss = 0.22, batch loss = 0.16 (28.1 examples/sec; 0.284 sec/batch; 24h:29m:51s remains)
INFO - root - 2017-12-16 05:25:31.223236: step 22430, loss = 0.32, batch loss = 0.26 (28.4 examples/sec; 0.282 sec/batch; 24h:18m:13s remains)
INFO - root - 2017-12-16 05:25:33.997494: step 22440, loss = 0.27, batch loss = 0.21 (28.4 examples/sec; 0.281 sec/batch; 24h:13m:09s remains)
INFO - root - 2017-12-16 05:25:36.859561: step 22450, loss = 0.33, batch loss = 0.27 (28.5 examples/sec; 0.281 sec/batch; 24h:09m:33s remains)
INFO - root - 2017-12-16 05:25:39.675923: step 22460, loss = 0.30, batch loss = 0.24 (29.5 examples/sec; 0.271 sec/batch; 23h:19m:08s remains)
INFO - root - 2017-12-16 05:25:42.507520: step 22470, loss = 0.24, batch loss = 0.18 (28.5 examples/sec; 0.281 sec/batch; 24h:12m:02s remains)
INFO - root - 2017-12-16 05:25:45.338492: step 22480, loss = 0.34, batch loss = 0.28 (27.0 examples/sec; 0.296 sec/batch; 25h:30m:29s remains)
INFO - root - 2017-12-16 05:25:48.149635: step 22490, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.286 sec/batch; 24h:35m:40s remains)
INFO - root - 2017-12-16 05:25:50.971585: step 22500, loss = 0.45, batch loss = 0.39 (28.5 examples/sec; 0.280 sec/batch; 24h:09m:13s remains)
2017-12-16 05:25:51.418649: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2495451 -2.2921679 -2.3548174 -2.4233 -2.422581 -2.4123755 -2.54126 -2.8615365 -3.2712417 -3.666539 -3.7505505 -3.6257212 -3.3358769 -3.098829 -2.882107][-1.8970306 -2.1298549 -2.2331955 -2.206568 -2.2453344 -2.505722 -2.8037887 -3.0748692 -3.3077602 -3.5227187 -3.5546017 -3.5031626 -3.3367281 -2.9816515 -2.6618106][-2.3506041 -2.5336773 -2.5490532 -2.3211877 -2.1058786 -2.0223923 -2.0983555 -2.4864793 -2.9025447 -3.043926 -3.0928197 -3.2890496 -3.4030278 -3.1911645 -2.9152369][-2.7234361 -2.7532914 -2.4927049 -2.1021056 -1.865051 -1.570287 -1.4374254 -1.6037474 -1.8539989 -2.1288445 -2.4357095 -2.9700825 -3.4232788 -3.4016004 -3.2547572][-3.0101748 -2.8201463 -2.3243816 -1.557405 -0.85336947 -0.25061131 0.088834286 -0.0837512 -0.45922184 -1.0320177 -1.7967975 -2.5573373 -3.156287 -3.411509 -3.4298515][-2.9832325 -2.5935211 -1.7464948 -0.74114966 0.27147722 1.1812744 1.8005667 1.8405795 1.4159236 0.50982761 -0.73310113 -2.1100502 -3.196023 -3.4632463 -3.3882678][-2.9302392 -2.2238898 -1.0386629 0.15958405 1.2747898 2.3919344 3.1668067 3.038938 2.2582946 1.1423759 -0.33417988 -1.8887331 -3.0897861 -3.479182 -3.3909831][-2.7168384 -1.9667592 -0.85712266 0.410244 1.7461472 2.9143558 3.5573144 3.2691917 2.31563 1.046082 -0.53167152 -2.1358509 -3.2628076 -3.6526144 -3.6887083][-2.7157841 -2.1749079 -1.2501187 -0.11192894 1.0570841 1.9920802 2.6550283 2.4923878 1.5758009 0.32864189 -1.0490961 -2.3708661 -3.3404961 -3.8793046 -4.0161123][-2.8052363 -2.5366883 -1.9646156 -1.1493063 -0.20413256 0.59449863 1.0194831 0.65644407 -0.17844486 -1.0995073 -2.089659 -3.2087021 -4.0785947 -4.4529228 -4.5236073][-3.2225466 -3.1433759 -2.553266 -2.0610106 -1.6888204 -1.1466386 -0.76927209 -1.0917521 -1.8525653 -2.5559292 -3.1908584 -4.0006714 -4.7357311 -5.0171671 -5.065927][-3.2120042 -3.2471027 -3.0513625 -2.730958 -2.3956761 -2.2124281 -2.2226577 -2.4691939 -2.8928854 -3.3480039 -3.9002213 -4.3965964 -4.7743621 -5.0817361 -5.3321652][-3.4095106 -3.5229626 -3.4128013 -3.00319 -2.6737771 -2.5720468 -2.5383382 -2.7439265 -3.1559057 -3.5845273 -3.9615107 -4.3060012 -4.5665717 -4.73566 -4.8752747][-3.6066351 -3.4600227 -3.1559346 -2.8642988 -2.6154594 -2.2878282 -2.1027386 -2.3820496 -2.7594452 -2.8954544 -3.15819 -3.511867 -3.687094 -3.7609639 -4.0069952][-3.1758914 -2.8736248 -2.4034421 -1.8867486 -1.3604567 -0.9599576 -0.75580025 -0.98212504 -1.4214978 -1.7880714 -2.1336923 -2.2816327 -2.3315549 -2.36839 -2.6236963]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-22500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-22500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 05:25:54.841868: step 22510, loss = 0.22, batch loss = 0.16 (29.0 examples/sec; 0.276 sec/batch; 23h:46m:13s remains)
INFO - root - 2017-12-16 05:25:57.708736: step 22520, loss = 0.23, batch loss = 0.17 (26.3 examples/sec; 0.304 sec/batch; 26h:09m:15s remains)
INFO - root - 2017-12-16 05:26:00.504562: step 22530, loss = 0.24, batch loss = 0.18 (28.7 examples/sec; 0.279 sec/batch; 24h:02m:23s remains)
INFO - root - 2017-12-16 05:26:03.330922: step 22540, loss = 0.35, batch loss = 0.30 (26.8 examples/sec; 0.299 sec/batch; 25h:42m:26s remains)
INFO - root - 2017-12-16 05:26:06.205784: step 22550, loss = 0.30, batch loss = 0.24 (27.5 examples/sec; 0.291 sec/batch; 25h:04m:39s remains)
INFO - root - 2017-12-16 05:26:09.057733: step 22560, loss = 0.23, batch loss = 0.17 (26.6 examples/sec; 0.300 sec/batch; 25h:50m:59s remains)
INFO - root - 2017-12-16 05:26:11.884397: step 22570, loss = 0.26, batch loss = 0.20 (28.5 examples/sec; 0.281 sec/batch; 24h:11m:25s remains)
INFO - root - 2017-12-16 05:26:14.721658: step 22580, loss = 0.23, batch loss = 0.17 (28.6 examples/sec; 0.280 sec/batch; 24h:05m:27s remains)
INFO - root - 2017-12-16 05:26:17.573450: step 22590, loss = 0.22, batch loss = 0.16 (27.8 examples/sec; 0.288 sec/batch; 24h:47m:34s remains)
INFO - root - 2017-12-16 05:26:20.403821: step 22600, loss = 0.33, batch loss = 0.27 (29.8 examples/sec; 0.269 sec/batch; 23h:08m:42s remains)
2017-12-16 05:26:20.875987: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4242876 -2.4818244 -2.7336619 -3.2279282 -3.7608478 -4.2300849 -4.6681929 -5.1243153 -5.4551706 -5.4538364 -5.3079963 -4.9188333 -4.3561196 -3.9348695 -3.8221011][-1.5456593 -1.6915057 -2.0834086 -2.5762606 -2.9325666 -3.4391479 -4.0340581 -4.553915 -4.9091835 -5.0592031 -5.032721 -4.7256937 -4.3210397 -3.8004785 -3.4035604][-0.75776672 -1.0725 -1.492028 -1.970598 -2.3228395 -2.7081764 -3.0515027 -3.5117164 -3.8061013 -3.8986592 -3.8585305 -3.7286596 -3.5436044 -3.3354635 -2.9758196][-0.13208818 -0.58569241 -1.1675713 -1.469367 -1.5299625 -1.5458424 -1.646126 -1.9422121 -2.1968815 -2.4946382 -2.6233754 -2.7989058 -3.0067194 -2.8484359 -2.3221748][-0.497077 -0.98903561 -1.2809672 -1.1928122 -0.90763783 -0.50736046 -0.13797951 -0.17047167 -0.47517014 -1.1084156 -1.8396428 -2.5887935 -2.9250622 -2.9207144 -2.5183008][-1.5352383 -1.8295331 -1.7849705 -1.140456 -0.20538378 0.69646263 1.5059648 1.813868 1.4471269 0.29717588 -0.97068381 -2.2401781 -3.1855302 -3.4818044 -3.0277114][-2.7357316 -2.4384518 -1.8192501 -0.81078887 0.61325359 1.9928737 3.1138935 3.3921313 2.8874969 1.5533414 -0.26730394 -1.9268677 -2.99542 -3.5591967 -3.4759736][-3.4226286 -2.9348674 -2.0868 -0.677439 0.98459911 2.392498 3.488955 3.8150959 3.0264635 1.3377686 -0.57246327 -2.2430165 -3.367784 -3.898499 -3.8187482][-3.7427077 -3.4178619 -2.4743769 -1.2845809 0.0061912537 1.4276023 2.420279 2.5321913 1.7124128 0.21103764 -1.566345 -3.1124353 -4.1935325 -4.59845 -4.4279861][-3.9991989 -3.8757837 -3.5878813 -2.7661717 -1.4390917 -0.4559474 0.19951725 0.50491 0.060072422 -1.1890686 -2.5868263 -3.8064091 -4.583168 -4.7671237 -4.5537171][-4.4906554 -4.7033968 -4.6101747 -4.186378 -3.5536852 -2.8177991 -2.2308598 -2.0082123 -2.2858205 -2.94984 -3.5812085 -4.2084336 -4.5737886 -4.4532094 -3.98231][-4.4655433 -4.8472795 -5.1281819 -4.9595766 -4.5590124 -4.1789932 -3.9419353 -3.8742533 -3.8860798 -4.1234183 -4.4855022 -4.7457981 -4.7592411 -4.4262323 -3.8536358][-4.5604229 -4.7116938 -4.89061 -4.738893 -4.5293369 -4.3382792 -4.1462393 -4.2428923 -4.350781 -4.4819717 -4.5699368 -4.6185632 -4.5868273 -4.4011259 -4.0596929][-4.5305228 -4.3905077 -4.3399544 -4.0865579 -3.9139204 -3.77931 -3.7289109 -3.8100765 -3.7981873 -4.0225339 -4.1076865 -4.0678668 -4.0468254 -3.9298413 -3.7199521][-4.0338187 -3.545084 -3.2285142 -2.8258042 -2.6995068 -2.7017322 -2.6861377 -2.8271737 -3.0412171 -3.3869612 -3.6184053 -3.7300858 -3.6954498 -3.6344347 -3.601687]]...]
INFO - root - 2017-12-16 05:26:23.675345: step 22610, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.278 sec/batch; 23h:57m:20s remains)
INFO - root - 2017-12-16 05:26:26.543144: step 22620, loss = 0.24, batch loss = 0.18 (28.7 examples/sec; 0.279 sec/batch; 24h:00m:31s remains)
INFO - root - 2017-12-16 05:26:29.309779: step 22630, loss = 0.22, batch loss = 0.16 (29.5 examples/sec; 0.271 sec/batch; 23h:22m:05s remains)
INFO - root - 2017-12-16 05:26:32.129877: step 22640, loss = 0.22, batch loss = 0.16 (28.8 examples/sec; 0.278 sec/batch; 23h:54m:56s remains)
INFO - root - 2017-12-16 05:26:34.974770: step 22650, loss = 0.31, batch loss = 0.25 (28.2 examples/sec; 0.284 sec/batch; 24h:27m:24s remains)
INFO - root - 2017-12-16 05:26:37.803894: step 22660, loss = 0.35, batch loss = 0.29 (28.8 examples/sec; 0.278 sec/batch; 23h:54m:37s remains)
INFO - root - 2017-12-16 05:26:40.642607: step 22670, loss = 0.32, batch loss = 0.26 (27.6 examples/sec; 0.290 sec/batch; 24h:55m:44s remains)
INFO - root - 2017-12-16 05:26:43.448027: step 22680, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 23h:53m:41s remains)
INFO - root - 2017-12-16 05:26:46.224075: step 22690, loss = 0.24, batch loss = 0.18 (29.0 examples/sec; 0.276 sec/batch; 23h:45m:13s remains)
INFO - root - 2017-12-16 05:26:49.057939: step 22700, loss = 0.58, batch loss = 0.53 (27.6 examples/sec; 0.290 sec/batch; 24h:54m:57s remains)
2017-12-16 05:26:49.537259: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.931035 -2.9094236 -2.7539592 -2.5931678 -2.4506979 -2.3577356 -2.4146297 -2.3557639 -2.2661595 -2.0322649 -1.9338152 -1.6188688 -1.3200788 -1.2438526 -1.0943737][-3.1856165 -3.2683892 -3.1178546 -2.9310446 -2.5938258 -2.2948215 -2.1729398 -2.0774739 -2.1618216 -2.0020289 -1.9658513 -1.8932128 -1.8277879 -1.7877293 -1.6350892][-3.1332121 -3.1548905 -2.906502 -2.5890477 -2.1550159 -1.7689021 -1.512197 -1.3017316 -1.3195782 -1.3192513 -1.6696861 -1.9283659 -2.2082984 -2.6128957 -2.795969][-3.0187414 -2.7915869 -2.3577852 -2.0582631 -1.6006639 -0.97038746 -0.47354913 -0.11265659 -0.0063591003 -0.12982321 -0.78342128 -1.7497883 -2.7894559 -3.3470311 -3.5406935][-2.6564178 -2.3338528 -1.8469589 -1.4301455 -0.90554476 -0.1603241 0.52422285 0.97125864 1.0858579 0.8292942 -0.22676754 -1.6388626 -2.8727822 -3.8127451 -4.2347941][-2.0794683 -1.5858991 -1.1380799 -0.61763549 0.02684164 0.79672956 1.5526581 2.1307521 2.4026675 1.9048171 0.62229872 -0.95794249 -2.5350654 -3.7312741 -4.3830171][-1.2043538 -0.653183 -0.299572 0.12330246 0.83691883 1.807611 2.7983484 3.363102 3.5203686 3.025229 1.7278738 -0.24125051 -2.2020512 -3.4272752 -4.0000858][-0.73208261 -0.44190717 -0.23057127 0.16388273 0.80343437 1.8611751 3.0092039 3.6934624 3.8841972 3.2752194 1.8641543 -0.10814095 -1.906769 -3.1636524 -3.8827522][-0.96649194 -0.90438294 -0.97543335 -0.71274281 -0.006166935 0.99714327 1.9720631 2.7353411 3.1541996 2.5054522 1.1962433 -0.43235755 -2.0555103 -3.0520844 -3.4376841][-1.7192931 -1.9244082 -2.3777192 -2.1170897 -1.3862038 -0.50270867 0.542058 1.2581811 1.5482006 1.1722765 0.3068924 -1.0237052 -2.3135555 -3.0230474 -3.1980712][-2.9833803 -2.9390292 -3.1604781 -3.2077312 -2.7679772 -1.9057219 -0.98674607 -0.43847585 -0.11878538 -0.32957029 -0.89312267 -1.7199478 -2.4537051 -2.8241527 -2.7752638][-3.8095112 -4.0052705 -4.19188 -3.9562798 -3.526479 -3.0390139 -2.4768944 -2.0106161 -1.6508324 -1.7135258 -1.9289837 -2.1926458 -2.4330697 -2.495739 -2.4068637][-4.3955021 -4.4225707 -4.4861693 -4.3229361 -3.9628773 -3.6689053 -3.3718681 -3.0373013 -2.629674 -2.4723577 -2.361696 -2.2515273 -2.1340427 -1.9190378 -1.7651258][-4.4380751 -4.4359021 -4.3549557 -4.1654634 -3.909755 -3.5393102 -3.2502546 -3.2058673 -2.9787452 -2.6388807 -2.2354865 -1.9699595 -1.7102628 -1.3554225 -1.1640701][-3.907979 -3.9474993 -3.9624612 -3.7132912 -3.4371939 -3.1983624 -3.0334091 -2.887511 -2.6688976 -2.4313498 -2.0007141 -1.6083913 -1.1932855 -0.84203744 -0.74758911]]...]
INFO - root - 2017-12-16 05:26:52.329807: step 22710, loss = 0.25, batch loss = 0.19 (29.3 examples/sec; 0.273 sec/batch; 23h:31m:03s remains)
INFO - root - 2017-12-16 05:26:55.159622: step 22720, loss = 0.27, batch loss = 0.22 (28.7 examples/sec; 0.279 sec/batch; 23h:59m:05s remains)
INFO - root - 2017-12-16 05:26:57.944108: step 22730, loss = 0.21, batch loss = 0.15 (27.7 examples/sec; 0.288 sec/batch; 24h:49m:19s remains)
INFO - root - 2017-12-16 05:27:00.758797: step 22740, loss = 0.24, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 23h:35m:39s remains)
INFO - root - 2017-12-16 05:27:03.585222: step 22750, loss = 0.23, batch loss = 0.17 (29.0 examples/sec; 0.276 sec/batch; 23h:44m:03s remains)
INFO - root - 2017-12-16 05:27:06.435808: step 22760, loss = 0.31, batch loss = 0.25 (29.6 examples/sec; 0.270 sec/batch; 23h:13m:23s remains)
INFO - root - 2017-12-16 05:27:09.333780: step 22770, loss = 0.45, batch loss = 0.39 (27.1 examples/sec; 0.295 sec/batch; 25h:22m:05s remains)
INFO - root - 2017-12-16 05:27:12.186721: step 22780, loss = 0.31, batch loss = 0.25 (27.8 examples/sec; 0.288 sec/batch; 24h:44m:32s remains)
INFO - root - 2017-12-16 05:27:15.004984: step 22790, loss = 0.27, batch loss = 0.21 (27.7 examples/sec; 0.289 sec/batch; 24h:52m:29s remains)
INFO - root - 2017-12-16 05:27:17.841573: step 22800, loss = 0.30, batch loss = 0.24 (29.1 examples/sec; 0.275 sec/batch; 23h:40m:44s remains)
2017-12-16 05:27:18.300340: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8832018 -2.2383962 -2.6651213 -2.890882 -2.9777989 -3.0982161 -3.2929778 -3.4203155 -3.52027 -3.6587238 -3.6673696 -3.5613306 -3.2328386 -2.8316035 -2.2264678][-1.2366576 -1.4459651 -1.8584225 -2.3126433 -2.6479115 -2.7100172 -2.9349985 -3.3600264 -3.6674697 -3.7831411 -3.7148962 -3.5622077 -3.1999879 -2.7856243 -2.2987659][-1.3604157 -1.5498257 -1.8807297 -2.321183 -2.5923562 -2.6700778 -2.8311377 -3.0578628 -3.3583369 -3.6168339 -3.8816466 -3.8092761 -3.3496752 -3.0435772 -2.7295187][-2.1115692 -2.1663592 -2.1918178 -2.3149719 -2.3144813 -2.1857653 -2.3944285 -2.6340652 -2.9003773 -3.2891183 -3.6450291 -3.8683827 -3.759855 -3.4929996 -3.2937915][-2.8983533 -2.6773973 -2.4208713 -1.9782419 -1.4276097 -1.0096476 -1.0734713 -1.5087614 -2.138324 -2.8205085 -3.4975705 -3.8987391 -4.0006518 -4.0506258 -3.953553][-3.6419344 -3.1840489 -2.5813556 -1.6763432 -0.71010995 0.18564749 0.6568718 0.44446564 -0.26356411 -1.264472 -2.3014445 -3.2066431 -3.9537556 -4.3346887 -4.3052931][-3.8804038 -3.0345404 -2.0110745 -0.60496259 0.66280413 1.7516222 2.2679191 2.0742712 1.4397769 0.2630105 -1.054204 -2.3844457 -3.4154582 -4.0515842 -4.306756][-3.873662 -2.9431796 -1.6532891 0.26481915 1.9255166 3.4488039 4.1989965 3.8905258 2.8828926 1.2562523 -0.26396322 -1.8971486 -3.2764158 -4.0240765 -4.2889295][-3.714092 -2.9379942 -1.8928909 -0.26686096 1.3087254 2.9291611 3.903307 3.8039398 2.9201851 1.4225678 -0.093415737 -1.8196836 -3.2539778 -4.1434946 -4.4973321][-3.9738259 -3.5033734 -2.8019207 -1.6170478 -0.4544096 0.80426264 1.7161007 1.8765635 1.2173386 -0.0800333 -1.3980105 -2.8075106 -3.9992113 -4.7210207 -4.9904089][-5.0455341 -4.5292072 -3.8728232 -3.0426235 -2.3122411 -1.5195985 -1.0337565 -0.9926424 -1.320507 -2.1585517 -3.0976868 -4.0737638 -4.8486614 -5.3331509 -5.5645161][-5.203011 -5.2660403 -5.10297 -4.4788871 -3.9664147 -3.5399446 -3.3611546 -3.4745479 -3.6492109 -4.0563841 -4.4593492 -5.1089029 -5.5699215 -5.8111281 -5.9048157][-4.9871511 -5.1068473 -5.19963 -5.1922708 -5.1368923 -4.8499494 -4.7421331 -4.8412275 -4.8734808 -4.9796867 -5.0104351 -5.2866583 -5.5052843 -5.69243 -5.8378911][-4.2354078 -4.3285279 -4.4931254 -4.5819607 -4.7371573 -4.9245639 -5.0374556 -5.0779557 -5.0689759 -4.9661746 -4.7201805 -4.5925651 -4.5462542 -4.6203728 -4.8432894][-3.5262895 -3.4852524 -3.5574992 -3.7977467 -4.0986466 -4.3565917 -4.527916 -4.4928646 -4.2801609 -4.1316676 -3.9761529 -3.6854653 -3.4372823 -3.4916439 -3.8055036]]...]
INFO - root - 2017-12-16 05:27:21.132390: step 22810, loss = 0.22, batch loss = 0.16 (28.4 examples/sec; 0.281 sec/batch; 24h:12m:12s remains)
INFO - root - 2017-12-16 05:27:24.002158: step 22820, loss = 0.28, batch loss = 0.22 (28.5 examples/sec; 0.281 sec/batch; 24h:09m:34s remains)
INFO - root - 2017-12-16 05:27:26.812646: step 22830, loss = 0.22, batch loss = 0.16 (27.8 examples/sec; 0.288 sec/batch; 24h:47m:35s remains)
INFO - root - 2017-12-16 05:27:29.638029: step 22840, loss = 0.24, batch loss = 0.18 (29.1 examples/sec; 0.275 sec/batch; 23h:38m:49s remains)
INFO - root - 2017-12-16 05:27:32.440490: step 22850, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.284 sec/batch; 24h:27m:12s remains)
INFO - root - 2017-12-16 05:27:35.235838: step 22860, loss = 0.25, batch loss = 0.19 (28.5 examples/sec; 0.281 sec/batch; 24h:08m:42s remains)
INFO - root - 2017-12-16 05:27:38.118521: step 22870, loss = 0.27, batch loss = 0.22 (26.5 examples/sec; 0.302 sec/batch; 25h:59m:23s remains)
INFO - root - 2017-12-16 05:27:40.943821: step 22880, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.281 sec/batch; 24h:12m:05s remains)
INFO - root - 2017-12-16 05:27:43.770661: step 22890, loss = 0.40, batch loss = 0.35 (28.1 examples/sec; 0.284 sec/batch; 24h:27m:06s remains)
INFO - root - 2017-12-16 05:27:46.640729: step 22900, loss = 0.27, batch loss = 0.21 (28.1 examples/sec; 0.285 sec/batch; 24h:28m:39s remains)
2017-12-16 05:27:47.142881: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3301458 -3.8770137 -4.4900479 -5.0045633 -5.4666538 -5.9192963 -6.3537755 -6.7033415 -6.795712 -6.796154 -6.6134019 -6.3382034 -5.8461242 -5.0315437 -4.4957132][-3.8756275 -4.527544 -5.1373196 -5.7046366 -5.9871273 -6.2600641 -6.6068373 -6.925952 -7.0539141 -6.9196558 -6.893239 -6.6088858 -5.9772034 -5.2812858 -4.7085838][-4.3862638 -5.2459216 -5.9763765 -6.2248807 -6.174067 -6.1491938 -6.0875235 -6.0949306 -6.0491834 -6.1218624 -6.19146 -5.99659 -5.5824218 -4.9794836 -4.4790578][-4.6958671 -5.5555253 -6.2742262 -6.4578381 -5.9947205 -5.2233677 -4.7732911 -4.5296679 -4.44468 -4.56879 -4.7554736 -4.960525 -4.7711153 -4.5293388 -4.2742677][-4.5619144 -5.2814274 -5.6616716 -5.3291316 -4.4235082 -3.2428534 -2.3290219 -1.9243226 -2.0668015 -2.7058084 -3.4354908 -3.8867745 -4.1009941 -3.9540827 -3.6529014][-4.6579609 -5.006701 -4.9642653 -4.21271 -2.805263 -1.0993829 0.3694129 1.0737753 0.77952576 -0.33107948 -1.5830472 -2.6114166 -3.14957 -3.276314 -2.9951546][-4.4882255 -4.7197 -4.5748873 -3.500442 -1.949682 -0.00089263916 1.8088069 2.72011 2.4986196 1.3489261 -0.025262833 -1.1861658 -1.8535964 -2.1596584 -1.957195][-4.2767978 -4.1476736 -3.7608273 -2.8330507 -1.3156776 0.57512188 2.1962419 2.95889 2.8614755 1.8298454 0.75643158 -0.16969633 -0.68786287 -0.693094 -0.36543703][-3.7196555 -3.6983359 -3.4167724 -2.4167511 -1.2493958 0.17933655 1.3815756 2.0384064 2.052618 1.306921 0.62129354 0.13415813 0.065162182 0.14742184 0.71058226][-3.9641278 -3.9162433 -3.8511045 -3.3371332 -2.3640938 -1.1920829 -0.32240057 0.14309454 0.073383808 -0.12687683 -0.0744338 -0.021720886 0.43203974 0.92505693 1.5564871][-4.2360978 -4.677587 -5.1311493 -4.8999114 -4.3886094 -3.5304883 -2.7037067 -2.2379622 -2.0387826 -1.8813515 -1.4314723 -0.73026681 0.24169254 0.92956448 1.5883203][-4.686645 -5.3518057 -6.1071978 -6.462975 -6.5073223 -6.0037317 -5.5128784 -4.9932432 -4.4708838 -3.8141088 -2.9231749 -2.0314291 -1.0771072 -0.30617332 0.43207121][-4.9771357 -5.9414177 -6.8897367 -7.5171061 -7.9822173 -7.9813089 -7.8161526 -7.3088207 -6.586071 -5.6468997 -4.6482544 -3.674238 -2.7850807 -2.2966566 -1.7398372][-5.4034686 -6.2340322 -7.026978 -7.7242517 -8.3275309 -8.642314 -8.7579269 -8.4964123 -7.9203386 -6.98774 -5.8761487 -4.9862556 -4.4390922 -4.063591 -3.5740595][-5.4046106 -6.0729947 -6.73224 -7.3670416 -7.8984518 -8.289135 -8.613307 -8.7447433 -8.5087814 -7.8565249 -7.0267849 -6.2206249 -5.535347 -5.2479749 -4.939209]]...]
INFO - root - 2017-12-16 05:27:49.961292: step 22910, loss = 0.26, batch loss = 0.20 (27.4 examples/sec; 0.292 sec/batch; 25h:07m:44s remains)
INFO - root - 2017-12-16 05:27:52.814604: step 22920, loss = 0.23, batch loss = 0.17 (28.6 examples/sec; 0.279 sec/batch; 24h:01m:42s remains)
INFO - root - 2017-12-16 05:27:55.665365: step 22930, loss = 0.31, batch loss = 0.25 (27.8 examples/sec; 0.288 sec/batch; 24h:44m:28s remains)
INFO - root - 2017-12-16 05:27:58.510472: step 22940, loss = 0.31, batch loss = 0.25 (28.9 examples/sec; 0.276 sec/batch; 23h:45m:55s remains)
INFO - root - 2017-12-16 05:28:01.353258: step 22950, loss = 0.29, batch loss = 0.23 (28.5 examples/sec; 0.280 sec/batch; 24h:06m:13s remains)
INFO - root - 2017-12-16 05:28:04.172874: step 22960, loss = 0.21, batch loss = 0.15 (28.9 examples/sec; 0.277 sec/batch; 23h:49m:12s remains)
INFO - root - 2017-12-16 05:28:06.998184: step 22970, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.278 sec/batch; 23h:55m:49s remains)
INFO - root - 2017-12-16 05:28:09.884926: step 22980, loss = 0.25, batch loss = 0.20 (28.1 examples/sec; 0.285 sec/batch; 24h:27m:39s remains)
INFO - root - 2017-12-16 05:28:12.705894: step 22990, loss = 0.21, batch loss = 0.15 (28.1 examples/sec; 0.284 sec/batch; 24h:26m:39s remains)
INFO - root - 2017-12-16 05:28:15.553217: step 23000, loss = 0.33, batch loss = 0.27 (29.6 examples/sec; 0.271 sec/batch; 23h:15m:39s remains)
2017-12-16 05:28:16.007132: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4218755 -2.7062387 -3.1560524 -3.6780798 -3.8577123 -4.3100944 -4.7333422 -4.8970051 -4.8017044 -4.4107985 -4.2687578 -3.7820392 -3.3449352 -3.309515 -3.3036556][-1.2600291 -1.6355982 -2.0875418 -2.75206 -3.0640402 -3.318634 -3.4198389 -3.7049417 -4.2872052 -4.3359942 -4.1184206 -3.7371564 -3.4958584 -3.2628832 -3.0842137][-0.75486541 -1.1290593 -1.439302 -1.8450823 -1.9768426 -2.1356146 -2.3635936 -2.8795176 -3.3175464 -3.5602138 -4.0808086 -3.968112 -3.6199133 -3.4546051 -3.4271293][-0.74686551 -1.0803628 -1.2693787 -1.4355612 -1.3170843 -1.3166893 -1.1866655 -1.4997435 -2.2997308 -3.1096835 -3.6697934 -3.7477105 -3.9265184 -3.7657652 -3.6763368][-1.197947 -1.4243805 -1.5134177 -1.43941 -0.98983955 -0.61469984 -0.36234713 -0.83511353 -1.6109431 -2.468946 -3.433846 -3.7014451 -3.7729111 -3.7355945 -3.5794182][-1.9679947 -1.956383 -1.8450189 -1.2033143 -0.084341049 0.64323235 0.97144556 0.76403141 -0.04001379 -1.3929863 -2.6026225 -3.251636 -3.4608431 -3.4998345 -3.7724228][-2.286835 -2.0132043 -1.6162872 -0.56878877 0.842113 2.0412283 2.8319988 2.5159407 1.3754725 -0.097668171 -1.3191569 -2.2827251 -2.8564191 -2.9143534 -2.7529287][-2.2481277 -1.8076575 -1.4025896 -0.31808424 1.3059454 2.6394019 3.2777057 3.0036268 2.067822 0.49983072 -0.92541909 -1.7853289 -2.0772321 -1.9873152 -1.5997121][-2.5640106 -2.0807366 -1.4898746 -0.46523857 0.7597928 1.9808154 2.677187 2.3238163 1.2418962 -0.16777277 -1.2853174 -1.8007734 -1.6150944 -1.1723099 -0.31533909][-2.9835887 -2.5396872 -2.1478982 -1.4136994 -0.52244163 0.396698 0.87162876 0.56065416 -0.31711864 -1.5116932 -2.3131044 -2.6095152 -2.1184323 -1.2014523 0.048352718][-3.2505202 -2.9956737 -2.829843 -2.3718078 -1.9220703 -1.5538192 -1.3814473 -1.7803617 -2.5862052 -3.5219011 -4.0698838 -3.9282091 -3.0163631 -1.8780394 -0.3752799][-3.5669451 -3.5131621 -3.6919627 -3.6447132 -3.519161 -3.4288909 -3.5463037 -4.041225 -4.6711755 -5.4197478 -5.7762861 -5.407248 -4.300519 -3.0783558 -1.5671608][-3.9390986 -3.9213223 -4.1239696 -4.1958785 -4.3217759 -4.6558881 -5.0742288 -5.6707087 -6.3633471 -6.9336061 -6.9223776 -6.5154476 -5.6051931 -4.5540729 -3.322228][-3.9146957 -3.8615751 -4.0345855 -4.0383635 -4.1400771 -4.49024 -5.174983 -6.1261029 -6.9549136 -7.6464853 -7.8486042 -7.5897202 -6.8034582 -5.9470763 -5.175323][-3.8648307 -3.7142553 -3.6199994 -3.5029423 -3.6749198 -4.1740966 -4.9683676 -6.1338987 -7.2318211 -8.0869312 -8.3668175 -8.1935635 -7.6259017 -6.9677334 -6.2344708]]...]
INFO - root - 2017-12-16 05:28:18.885881: step 23010, loss = 0.25, batch loss = 0.19 (26.9 examples/sec; 0.297 sec/batch; 25h:31m:26s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 05:28:21.662020: step 23020, loss = 0.23, batch loss = 0.17 (29.3 examples/sec; 0.273 sec/batch; 23h:30m:25s remains)
INFO - root - 2017-12-16 05:28:24.523822: step 23030, loss = 0.27, batch loss = 0.21 (27.8 examples/sec; 0.287 sec/batch; 24h:41m:56s remains)
INFO - root - 2017-12-16 05:28:27.325089: step 23040, loss = 0.41, batch loss = 0.35 (28.1 examples/sec; 0.284 sec/batch; 24h:26m:41s remains)
INFO - root - 2017-12-16 05:28:30.173349: step 23050, loss = 0.34, batch loss = 0.28 (25.6 examples/sec; 0.312 sec/batch; 26h:50m:51s remains)
INFO - root - 2017-12-16 05:28:32.991997: step 23060, loss = 0.45, batch loss = 0.39 (27.7 examples/sec; 0.289 sec/batch; 24h:50m:44s remains)
INFO - root - 2017-12-16 05:28:35.861375: step 23070, loss = 0.33, batch loss = 0.27 (28.4 examples/sec; 0.282 sec/batch; 24h:12m:21s remains)
INFO - root - 2017-12-16 05:28:38.653311: step 23080, loss = 0.28, batch loss = 0.23 (28.9 examples/sec; 0.277 sec/batch; 23h:46m:12s remains)
INFO - root - 2017-12-16 05:28:41.492852: step 23090, loss = 0.23, batch loss = 0.18 (28.2 examples/sec; 0.284 sec/batch; 24h:22m:32s remains)
INFO - root - 2017-12-16 05:28:44.332853: step 23100, loss = 0.27, batch loss = 0.21 (27.7 examples/sec; 0.289 sec/batch; 24h:50m:09s remains)
2017-12-16 05:28:44.791211: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.6766338 -6.6852951 -6.5744629 -6.6829777 -6.7893372 -6.6049976 -6.3790689 -6.3458519 -6.3874679 -6.6480966 -6.9531083 -7.3201141 -7.7482915 -7.9159079 -8.0441322][-6.507843 -6.7143269 -6.8166342 -6.6196108 -6.5326815 -6.8657002 -7.0953722 -6.9729743 -6.9842424 -7.2662249 -7.5587497 -7.972683 -8.24908 -8.3312178 -8.4103127][-6.1120124 -6.0956869 -5.8536396 -5.8947349 -6.05144 -5.96416 -5.7437925 -6.0339336 -6.6116409 -7.0988617 -7.6778603 -8.1821251 -8.5860481 -8.4605551 -8.1402073][-5.302062 -5.1154933 -4.8010926 -4.3385763 -4.102756 -4.0391979 -3.7647634 -3.9916773 -4.496635 -5.4433904 -6.5797768 -7.3995 -8.0418072 -8.0224276 -7.6506605][-4.0156531 -3.6454597 -3.1356254 -2.5862055 -1.9747674 -1.0620589 -0.48190689 -0.5933075 -1.0627229 -2.2593894 -3.6838574 -5.0976262 -5.8582034 -6.1877966 -6.2709088][-3.5632677 -2.7106447 -1.3764217 -0.2917285 0.7780447 1.8072195 2.7917972 3.0426435 2.5194697 1.2717247 -0.26814842 -1.7712035 -2.9841895 -3.6866064 -3.7931762][-3.85029 -2.6005454 -1.0541756 0.61741209 2.4298573 4.0293179 5.4163389 5.7760878 5.4600258 4.1803627 2.3081841 0.37614298 -0.98400259 -1.868624 -2.43011][-4.8282557 -3.6510408 -2.1574008 -0.051245689 2.3354053 4.3159018 5.9199524 6.6797562 6.5618639 5.3808146 3.6860609 1.5788035 -0.22801399 -1.5539558 -2.3444324][-6.3281941 -5.5446463 -3.9232295 -1.9687479 -0.011600971 2.3002057 4.4736261 5.4150629 5.4076347 4.5292015 3.0443449 0.99440527 -0.87413955 -2.1140158 -2.9861383][-7.3637629 -6.9321213 -6.14347 -4.6910906 -2.8867087 -1.0612004 0.73048019 2.0435028 2.4374795 1.9306622 0.98136234 -0.68193316 -2.1420031 -3.1983676 -3.955585][-8.0378647 -7.7545671 -7.3344936 -6.5726724 -5.6102271 -3.9961286 -2.5694273 -1.6807816 -1.3047838 -1.54599 -2.2950087 -3.4729562 -4.5810361 -5.3857732 -5.76564][-8.478816 -8.4133806 -8.2358332 -7.9375677 -7.3409319 -6.5515914 -5.8030791 -5.0879655 -4.6695461 -4.6862435 -5.1071081 -5.8516536 -6.3621321 -6.8449526 -7.1020985][-8.3422394 -8.506216 -8.6334944 -8.6473837 -8.5709343 -8.0685072 -7.3086681 -6.6096916 -6.0559111 -6.0903111 -6.2259479 -6.4208641 -6.6628113 -7.0670333 -7.2161813][-6.6844034 -7.0526981 -7.4029331 -7.7330589 -7.8762522 -7.6608844 -7.3457675 -6.8998475 -6.4060106 -6.1157866 -6.0643511 -6.5445924 -6.9555221 -7.2028875 -7.2006731][-6.1197281 -6.3503652 -6.5762711 -6.8920555 -7.040791 -7.0661187 -6.9775391 -6.4896417 -6.034472 -5.8604546 -5.964437 -6.0856791 -6.1139221 -6.3278294 -6.3835478]]...]
INFO - root - 2017-12-16 05:28:47.577881: step 23110, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.280 sec/batch; 24h:05m:39s remains)
INFO - root - 2017-12-16 05:28:50.397379: step 23120, loss = 0.38, batch loss = 0.32 (28.0 examples/sec; 0.285 sec/batch; 24h:31m:17s remains)
INFO - root - 2017-12-16 05:28:53.271145: step 23130, loss = 0.23, batch loss = 0.17 (27.9 examples/sec; 0.287 sec/batch; 24h:38m:41s remains)
INFO - root - 2017-12-16 05:28:56.078638: step 23140, loss = 0.25, batch loss = 0.20 (29.6 examples/sec; 0.270 sec/batch; 23h:13m:48s remains)
INFO - root - 2017-12-16 05:28:58.954391: step 23150, loss = 0.45, batch loss = 0.39 (28.9 examples/sec; 0.276 sec/batch; 23h:45m:09s remains)
INFO - root - 2017-12-16 05:29:01.808936: step 23160, loss = 0.23, batch loss = 0.18 (29.0 examples/sec; 0.276 sec/batch; 23h:40m:42s remains)
INFO - root - 2017-12-16 05:29:04.655349: step 23170, loss = 0.30, batch loss = 0.25 (28.2 examples/sec; 0.284 sec/batch; 24h:23m:40s remains)
INFO - root - 2017-12-16 05:29:07.472807: step 23180, loss = 0.35, batch loss = 0.29 (27.9 examples/sec; 0.287 sec/batch; 24h:38m:11s remains)
INFO - root - 2017-12-16 05:29:10.334076: step 23190, loss = 0.24, batch loss = 0.19 (29.1 examples/sec; 0.275 sec/batch; 23h:37m:17s remains)
INFO - root - 2017-12-16 05:29:13.239661: step 23200, loss = 0.26, batch loss = 0.20 (25.8 examples/sec; 0.310 sec/batch; 26h:36m:40s remains)
2017-12-16 05:29:13.705715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.0091991 -7.1763697 -7.1561284 -6.528512 -5.8917713 -5.4848852 -5.1522713 -4.9474015 -4.8142605 -5.09015 -5.5948663 -6.1951375 -6.9480314 -7.3047075 -7.1179848][-7.5357695 -7.9070706 -7.9181523 -7.3259945 -6.5333395 -5.9110727 -5.3432488 -5.1594 -5.0092163 -5.2485371 -5.903451 -6.620903 -7.467967 -7.7559452 -7.5381393][-7.4169059 -7.7089663 -7.6807532 -7.0784283 -6.2555356 -5.5482335 -4.8828611 -4.7342772 -4.69951 -4.8771029 -5.4239063 -6.2243671 -7.2589874 -7.5575628 -7.3231225][-6.3022342 -6.492445 -6.23873 -5.6172056 -4.6335096 -3.8928528 -3.4468741 -3.5687139 -3.7267478 -3.8741007 -4.4309011 -4.9658637 -5.8085351 -6.1336889 -6.1463][-4.8480463 -4.6772785 -4.2677832 -3.6406062 -2.7240949 -1.9584503 -1.3821373 -1.4645028 -1.4939539 -1.7704146 -2.3310478 -2.7502213 -3.3307955 -3.9069312 -4.36617][-3.67727 -3.0622325 -2.1402986 -1.1397667 -0.092804909 0.49491549 0.68018913 0.5431571 0.50362921 0.23062372 -0.24979162 -0.68999434 -1.3580372 -1.9727187 -2.6954234][-2.6136286 -1.7572799 -0.63170314 0.64598227 1.9472575 2.6865888 3.0786414 2.6779709 2.2438216 2.0000935 1.4947491 0.747417 -0.20777273 -1.1732502 -2.2696931][-2.3915644 -1.5314007 -0.25787592 1.1020484 2.4017968 3.4644322 4.0586872 3.8669081 3.4377532 2.8253016 2.1418304 1.1513462 -0.085194588 -1.3720369 -2.6253841][-2.9011025 -2.0809665 -0.8587532 0.37285757 1.4272423 2.4042363 3.19483 3.3216496 2.9661298 2.2122507 1.4026947 0.26628208 -0.94520378 -2.1365144 -3.3908789][-3.3741431 -3.0212498 -2.4129851 -1.4532697 -0.517159 0.3600235 0.97908497 1.297987 1.1251106 0.4077282 -0.30549479 -1.4616399 -2.6334116 -3.7049892 -4.6106496][-4.6967483 -4.5111876 -3.9302115 -3.4223342 -3.14968 -2.415642 -1.7281208 -1.3396833 -1.4035511 -1.8516171 -2.2148337 -3.1012232 -4.0249748 -4.7840986 -5.1723986][-5.917778 -5.9072008 -5.750968 -5.4148879 -5.114068 -4.7256556 -4.4689622 -4.0524054 -3.718595 -3.8192003 -3.8394327 -4.1768956 -4.5407362 -4.8022275 -4.904335][-6.8781676 -6.9672832 -7.0048013 -6.9021482 -6.9726377 -6.7645321 -6.5454111 -6.247324 -5.8824577 -5.6366329 -5.2508321 -5.1533837 -5.0985489 -4.9193664 -4.6431141][-7.453537 -7.186841 -7.2119532 -7.3293791 -7.3991704 -7.31682 -7.2152452 -7.068737 -6.8324814 -6.4732375 -6.0078483 -5.6622734 -5.416255 -5.0590796 -4.5723362][-7.28731 -6.8762593 -6.70915 -6.8016596 -7.0212555 -7.0049639 -6.9715042 -6.9318213 -6.6602297 -6.4034348 -6.1154957 -5.756938 -5.4118886 -4.9946232 -4.5835943]]...]
INFO - root - 2017-12-16 05:29:16.528554: step 23210, loss = 0.32, batch loss = 0.26 (29.6 examples/sec; 0.270 sec/batch; 23h:11m:20s remains)
INFO - root - 2017-12-16 05:29:19.370701: step 23220, loss = 0.24, batch loss = 0.19 (25.4 examples/sec; 0.315 sec/batch; 27h:05m:24s remains)
INFO - root - 2017-12-16 05:29:22.216487: step 23230, loss = 0.35, batch loss = 0.29 (26.5 examples/sec; 0.302 sec/batch; 25h:57m:22s remains)
INFO - root - 2017-12-16 05:29:25.106971: step 23240, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.277 sec/batch; 23h:48m:30s remains)
INFO - root - 2017-12-16 05:29:27.919536: step 23250, loss = 0.20, batch loss = 0.14 (28.9 examples/sec; 0.277 sec/batch; 23h:47m:01s remains)
INFO - root - 2017-12-16 05:29:30.745195: step 23260, loss = 0.37, batch loss = 0.31 (29.4 examples/sec; 0.272 sec/batch; 23h:24m:06s remains)
INFO - root - 2017-12-16 05:29:33.622750: step 23270, loss = 0.39, batch loss = 0.33 (28.4 examples/sec; 0.282 sec/batch; 24h:11m:23s remains)
INFO - root - 2017-12-16 05:29:36.418342: step 23280, loss = 0.26, batch loss = 0.20 (29.2 examples/sec; 0.274 sec/batch; 23h:33m:23s remains)
INFO - root - 2017-12-16 05:29:39.214990: step 23290, loss = 0.27, batch loss = 0.21 (29.1 examples/sec; 0.275 sec/batch; 23h:36m:42s remains)
INFO - root - 2017-12-16 05:29:42.035385: step 23300, loss = 0.28, batch loss = 0.22 (27.7 examples/sec; 0.288 sec/batch; 24h:45m:42s remains)
2017-12-16 05:29:42.497033: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7709136 -3.7427258 -3.9669371 -4.3202262 -4.6277 -4.6467128 -4.4210424 -4.2251248 -4.1857953 -4.0766506 -3.9740024 -3.8783588 -4.0425706 -4.4120727 -4.6220813][-3.3991797 -3.3867202 -3.6266181 -3.9988332 -4.2726488 -4.2667093 -4.0490818 -3.8882294 -3.84122 -3.7294664 -3.5794876 -3.4010327 -3.300807 -3.4632752 -3.6453032][-3.0513277 -3.0526421 -3.30717 -3.6013398 -3.7689443 -3.7378011 -3.5869434 -3.4219007 -3.2922902 -3.2045159 -2.9687128 -2.7027652 -2.5209169 -2.4673858 -2.5202427][-2.5571933 -2.5915265 -2.7297232 -3.0108595 -3.1081059 -2.9610806 -2.7744281 -2.6796408 -2.6068377 -2.4759362 -2.3263416 -2.0521834 -1.8066199 -1.6255467 -1.5143352][-1.9616766 -1.9894481 -2.1121414 -2.2175734 -2.1261573 -1.9419563 -1.7311692 -1.6656559 -1.699604 -1.6532292 -1.6070042 -1.36076 -0.99342895 -0.7238369 -0.61574054][-1.3121178 -1.2555234 -1.2851546 -1.1788838 -0.82252836 -0.52848268 -0.35136509 -0.4360981 -0.58858848 -0.73023677 -0.83486843 -0.6488843 -0.43725514 -0.057423592 0.10589743][-0.51323795 -0.44561172 -0.41295624 -0.24033976 0.14111805 0.5129137 0.72694635 0.51464176 0.28981256 0.0552516 -0.1597023 -0.14344931 -0.12598753 0.22287083 0.35377312][0.099643707 0.23600435 0.17642355 0.36889458 0.71270418 0.93342209 1.0871754 0.82713461 0.57365608 0.30892563 0.046640873 0.042491913 0.085784912 0.37982559 0.47528553][0.54989243 0.62988615 0.36681652 0.36837673 0.39110756 0.4867878 0.57010126 0.30900764 0.13984346 0.011363983 -0.1696744 -0.26724195 -0.33812761 -0.083200455 0.047109127][0.63829565 0.63058376 0.090042591 -0.15533161 -0.37228727 -0.45549822 -0.46241283 -0.67163491 -0.77321386 -0.771307 -0.81358361 -1.0702472 -1.2651687 -0.9285264 -0.67452407][0.31816196 0.26601362 -0.38218594 -0.85140944 -1.2753756 -1.476553 -1.5476239 -1.6882241 -1.6776857 -1.7102804 -1.9127719 -2.2132673 -2.3459358 -2.0832517 -1.7807961][-0.13982344 -0.24728727 -0.97921515 -1.6709983 -2.1665239 -2.2276185 -2.220279 -2.3896294 -2.5251608 -2.5692754 -2.9025373 -3.3067803 -3.5306711 -3.2166872 -2.7286358][-0.381958 -0.27792835 -0.70764518 -1.3382137 -1.907321 -2.0879185 -2.2000039 -2.2882819 -2.4906168 -2.7314625 -3.2270689 -3.8590138 -4.2081528 -3.9313464 -3.3173518][-0.58180833 -0.37189722 -0.52609396 -1.0412288 -1.4968128 -1.6785862 -1.8579564 -1.9987633 -2.3172536 -2.5597544 -2.9955878 -3.6544156 -4.0635867 -3.9033792 -3.2495236][-0.70814729 -0.40443754 -0.33340263 -0.59107542 -0.84800506 -0.95727277 -1.1064212 -1.2757001 -1.6612742 -2.0315993 -2.6250024 -3.1482067 -3.43998 -3.2650247 -2.633075]]...]
INFO - root - 2017-12-16 05:29:45.312142: step 23310, loss = 0.28, batch loss = 0.22 (29.2 examples/sec; 0.274 sec/batch; 23h:33m:56s remains)
INFO - root - 2017-12-16 05:29:48.134998: step 23320, loss = 0.32, batch loss = 0.26 (25.5 examples/sec; 0.314 sec/batch; 26h:59m:04s remains)
INFO - root - 2017-12-16 05:29:50.961280: step 23330, loss = 0.33, batch loss = 0.27 (27.7 examples/sec; 0.288 sec/batch; 24h:46m:22s remains)
INFO - root - 2017-12-16 05:29:53.804520: step 23340, loss = 0.20, batch loss = 0.14 (27.7 examples/sec; 0.289 sec/batch; 24h:48m:01s remains)
INFO - root - 2017-12-16 05:29:56.600829: step 23350, loss = 0.33, batch loss = 0.27 (29.3 examples/sec; 0.273 sec/batch; 23h:28m:25s remains)
INFO - root - 2017-12-16 05:29:59.405505: step 23360, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.283 sec/batch; 24h:19m:40s remains)
INFO - root - 2017-12-16 05:30:02.213715: step 23370, loss = 0.23, batch loss = 0.17 (29.5 examples/sec; 0.271 sec/batch; 23h:18m:36s remains)
INFO - root - 2017-12-16 05:30:05.061279: step 23380, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 24h:33m:09s remains)
INFO - root - 2017-12-16 05:30:07.947302: step 23390, loss = 0.30, batch loss = 0.24 (27.9 examples/sec; 0.287 sec/batch; 24h:38m:04s remains)
INFO - root - 2017-12-16 05:30:10.814860: step 23400, loss = 0.30, batch loss = 0.25 (27.7 examples/sec; 0.288 sec/batch; 24h:45m:15s remains)
2017-12-16 05:30:11.257478: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0974951 -4.7737646 -4.4922991 -4.2509294 -4.0977921 -4.0353847 -3.8850362 -3.831938 -4.0560889 -4.3888106 -4.8978982 -5.372828 -5.9155526 -6.7251692 -7.0822058][-4.9207659 -4.5442543 -4.1112103 -3.7138209 -3.4848022 -3.4732356 -3.3924456 -3.2141962 -3.3827419 -3.8988745 -4.8087111 -5.599071 -6.2263479 -6.9883671 -7.2764368][-4.2494349 -3.8146508 -3.3623977 -2.9284854 -2.5954924 -2.404346 -2.323729 -2.4362454 -2.7838297 -3.4043016 -4.4610987 -5.6404362 -6.6194406 -7.325593 -7.6030674][-2.8910236 -2.5752633 -2.2206273 -1.8166223 -1.4274139 -1.1481915 -1.0225103 -1.1898806 -1.7408373 -2.7941277 -4.0541372 -5.2726312 -6.3243742 -7.1875124 -7.542697][-1.8161345 -1.5410419 -1.0341532 -0.56941366 0.022114754 0.55120087 0.64400673 0.33998919 -0.32137537 -1.2615998 -2.5820594 -3.994745 -5.4070458 -6.4317732 -6.8443236][-0.85260725 -0.66566634 -0.1192441 0.478467 1.2723036 1.8957 2.1716218 2.0483632 1.4631329 0.53375912 -0.78646994 -2.1728661 -3.6180842 -5.1112838 -6.1542773][-1.0414298 -0.6813271 0.026641369 0.81548834 1.6457753 2.3338737 2.6883965 2.5818906 2.2250867 1.5337911 0.46723223 -0.89084244 -2.4888515 -4.1000009 -5.178503][-1.7388556 -1.3949645 -0.6473279 0.25229692 1.1858544 1.8571358 2.2397676 2.2672353 2.0408211 1.5625167 0.79617977 -0.34611559 -1.8985047 -3.4443846 -4.5970297][-3.0894268 -2.6028059 -1.6815026 -0.82351089 0.10679579 0.78089809 1.2752919 1.4953737 1.6344509 1.3477049 0.74699974 -0.25557423 -1.6223671 -3.1215234 -4.3528967][-3.9772406 -3.6307676 -3.0164878 -2.1564641 -1.2524226 -0.59397626 -0.067103863 0.26534414 0.47977066 0.40153074 -0.00393486 -0.86113071 -1.9765189 -3.1141312 -4.1370368][-4.7797809 -4.2794552 -3.4516993 -2.7731967 -2.1897511 -1.6545837 -1.0058851 -0.55805063 -0.39588213 -0.50714493 -0.772223 -1.3547528 -2.1464579 -3.1540272 -4.1380663][-4.8166442 -4.5199943 -3.8853967 -3.109621 -2.4136782 -2.0060339 -1.562659 -1.1350746 -0.86939192 -0.85890841 -0.98722005 -1.4156768 -2.0941269 -2.9229226 -3.7589629][-4.0936747 -3.7861419 -3.3762007 -2.8913374 -2.3551517 -1.9402332 -1.6028252 -1.2114804 -0.94595718 -0.88462853 -0.99256039 -1.2466707 -1.6859159 -2.3704031 -3.252902][-3.3902979 -3.0253773 -2.5275548 -2.2202671 -1.9399779 -1.6689425 -1.4477518 -1.1814206 -0.9468689 -0.66687846 -0.50101638 -0.68789268 -1.169435 -1.8139524 -2.5520012][-2.3600676 -2.0473723 -1.7177792 -1.564841 -1.4776087 -1.4386609 -1.3533156 -1.1378605 -0.823658 -0.39470434 -0.071449757 -0.0093250275 -0.33039904 -0.99615788 -1.8195095]]...]
INFO - root - 2017-12-16 05:30:14.109706: step 23410, loss = 0.21, batch loss = 0.15 (27.0 examples/sec; 0.296 sec/batch; 25h:24m:09s remains)
INFO - root - 2017-12-16 05:30:16.919176: step 23420, loss = 0.28, batch loss = 0.22 (28.0 examples/sec; 0.286 sec/batch; 24h:32m:46s remains)
INFO - root - 2017-12-16 05:30:19.763712: step 23430, loss = 0.23, batch loss = 0.17 (27.7 examples/sec; 0.288 sec/batch; 24h:45m:26s remains)
INFO - root - 2017-12-16 05:30:22.609795: step 23440, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.281 sec/batch; 24h:09m:27s remains)
INFO - root - 2017-12-16 05:30:25.490764: step 23450, loss = 0.29, batch loss = 0.24 (28.8 examples/sec; 0.278 sec/batch; 23h:53m:05s remains)
INFO - root - 2017-12-16 05:30:28.319973: step 23460, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.281 sec/batch; 24h:07m:59s remains)
INFO - root - 2017-12-16 05:30:31.128748: step 23470, loss = 0.22, batch loss = 0.16 (27.9 examples/sec; 0.287 sec/batch; 24h:37m:18s remains)
INFO - root - 2017-12-16 05:30:33.987161: step 23480, loss = 0.30, batch loss = 0.24 (28.5 examples/sec; 0.281 sec/batch; 24h:07m:46s remains)
INFO - root - 2017-12-16 05:30:36.773145: step 23490, loss = 0.23, batch loss = 0.17 (29.1 examples/sec; 0.275 sec/batch; 23h:34m:01s remains)
INFO - root - 2017-12-16 05:30:39.585704: step 23500, loss = 0.23, batch loss = 0.17 (29.3 examples/sec; 0.273 sec/batch; 23h:24m:58s remains)
2017-12-16 05:30:40.019239: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8297043 -3.3806226 -3.9421482 -4.4023418 -4.8181648 -4.9755316 -5.098351 -5.3407254 -5.6695328 -5.87615 -5.8434448 -5.6768856 -5.244894 -4.5155797 -3.7511196][-3.6569059 -4.3977976 -5.0511045 -5.3347783 -5.5064387 -5.6050467 -5.7709455 -5.6739054 -5.6274261 -5.8624134 -6.2720709 -6.3067389 -5.9339147 -5.3764954 -4.6608253][-4.519516 -5.4021764 -6.081883 -6.2349896 -6.02664 -5.4949417 -4.9660559 -4.7686338 -4.8775558 -4.9533677 -5.2221861 -5.6814742 -6.0316758 -5.8085208 -5.163341][-4.877326 -5.7610188 -6.306097 -6.1993184 -5.6592045 -4.5973592 -3.4999671 -2.6510985 -2.3480451 -2.7962461 -3.7711766 -4.5812583 -5.2093639 -5.4325285 -5.284965][-4.8230004 -5.3953128 -5.6100006 -5.1902237 -4.1193151 -2.4495807 -0.887835 0.26999712 0.54757452 -0.046967506 -1.4684105 -3.1223035 -4.5496545 -5.1436968 -5.2170296][-4.3475995 -4.6225843 -4.4244123 -3.6602521 -2.2147069 -0.18827057 1.9151373 3.3681231 3.6827707 2.7704959 0.90906715 -1.185286 -3.0534868 -4.3075352 -4.9203105][-3.6577709 -3.6845102 -3.1861377 -2.1066129 -0.41595411 1.6717305 3.6877937 5.1239729 5.3842869 4.3965063 2.5426722 0.28630543 -1.7897673 -3.3053052 -4.3298712][-3.0971079 -3.0677364 -2.3860788 -1.0761502 0.76208115 2.7185044 4.3779125 5.2151155 5.1695881 4.111764 2.2248416 0.31443071 -1.3991003 -2.99601 -4.1633577][-3.2133336 -3.0229554 -2.3181438 -0.97563887 0.84184217 2.5442033 3.7262955 4.1710997 3.8683586 2.8434539 1.5010047 0.029160023 -1.4868643 -2.9708948 -4.0903049][-4.0181727 -3.65093 -2.8615608 -1.5980408 0.0026988983 1.4309287 2.4074726 2.7111025 2.4042969 1.5275116 0.5862751 -0.48138928 -1.7659466 -3.1019669 -4.2777734][-4.8851957 -4.4898725 -3.70927 -2.4564297 -1.179589 -0.13743353 0.53577375 0.95665693 0.90706444 0.31134272 -0.31083012 -1.2686706 -2.4364557 -3.6933193 -4.7474594][-5.8262739 -5.3634915 -4.4518347 -3.3747501 -2.3355494 -1.5169852 -0.89823437 -0.53027153 -0.586169 -0.82284045 -1.2072396 -2.0063548 -3.1202035 -4.3392482 -5.3121576][-6.5355778 -6.1483521 -5.3181767 -4.07304 -2.8757226 -2.1311066 -1.5150936 -1.0823894 -0.97203684 -1.2331896 -1.7722886 -2.4965672 -3.5941787 -4.7634411 -5.6713104][-6.5502906 -6.0994682 -5.331708 -4.3586226 -3.1636724 -2.031872 -1.231087 -0.8181417 -0.78131533 -1.0068009 -1.592454 -2.5581598 -3.6272504 -4.6318874 -5.450038][-5.9566655 -5.607214 -4.9905858 -4.1865735 -3.0341141 -1.8097081 -0.94445729 -0.44089961 -0.35736036 -0.601007 -1.1409733 -2.0473268 -3.0538504 -3.9452152 -4.5348973]]...]
INFO - root - 2017-12-16 05:30:42.883859: step 23510, loss = 0.32, batch loss = 0.26 (27.7 examples/sec; 0.289 sec/batch; 24h:45m:47s remains)
INFO - root - 2017-12-16 05:30:45.709854: step 23520, loss = 0.29, batch loss = 0.23 (28.8 examples/sec; 0.277 sec/batch; 23h:48m:45s remains)
INFO - root - 2017-12-16 05:30:48.525481: step 23530, loss = 0.29, batch loss = 0.23 (28.0 examples/sec; 0.286 sec/batch; 24h:31m:50s remains)
INFO - root - 2017-12-16 05:30:51.343399: step 23540, loss = 0.32, batch loss = 0.26 (29.1 examples/sec; 0.275 sec/batch; 23h:34m:07s remains)
INFO - root - 2017-12-16 05:30:54.209197: step 23550, loss = 0.32, batch loss = 0.26 (28.7 examples/sec; 0.279 sec/batch; 23h:57m:35s remains)
INFO - root - 2017-12-16 05:30:57.020662: step 23560, loss = 0.28, batch loss = 0.23 (27.8 examples/sec; 0.288 sec/batch; 24h:41m:09s remains)
INFO - root - 2017-12-16 05:30:59.899391: step 23570, loss = 0.32, batch loss = 0.26 (27.7 examples/sec; 0.288 sec/batch; 24h:44m:28s remains)
INFO - root - 2017-12-16 05:31:02.737112: step 23580, loss = 0.27, batch loss = 0.21 (28.3 examples/sec; 0.283 sec/batch; 24h:15m:12s remains)
INFO - root - 2017-12-16 05:31:05.550933: step 23590, loss = 0.20, batch loss = 0.14 (29.4 examples/sec; 0.272 sec/batch; 23h:20m:12s remains)
INFO - root - 2017-12-16 05:31:08.379514: step 23600, loss = 0.30, batch loss = 0.24 (28.3 examples/sec; 0.282 sec/batch; 24h:13m:12s remains)
2017-12-16 05:31:08.869515: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5444717 -6.662364 -6.7017231 -6.663826 -6.6520367 -6.6260462 -6.5106525 -6.615808 -6.6243696 -6.6449785 -6.5221777 -6.3408046 -5.9580669 -5.4350843 -4.939734][-7.3827162 -7.3478708 -7.1671534 -7.000864 -6.8476868 -7.0156116 -7.3578405 -7.6367955 -7.7124252 -8.0378857 -8.14188 -7.8925071 -7.3921213 -6.7889347 -6.2145042][-7.58195 -7.2945156 -6.9308863 -6.4841022 -5.9517088 -5.9219408 -6.1405792 -6.8675156 -7.6926088 -8.4809456 -8.86778 -8.9793186 -8.6913967 -8.0832062 -7.5167084][-6.9319 -6.2861586 -5.4285455 -4.7364974 -4.1601315 -3.9573638 -3.9604151 -4.7696242 -5.76417 -7.1800761 -8.346117 -8.8975382 -8.9106283 -8.5976353 -8.2162247][-5.87803 -4.6507282 -3.2239277 -1.9253829 -0.75402856 -0.41368103 -0.6371088 -1.6674111 -2.8392122 -4.6445222 -6.2159834 -7.4064007 -8.1175232 -8.1656866 -8.0067291][-4.5996819 -3.1624229 -1.2519875 0.72070885 2.4011531 3.1234865 3.229907 2.0441532 0.34533596 -1.8217173 -3.6458135 -5.1818776 -6.3481512 -6.8789268 -7.1449223][-3.756669 -2.0777395 0.0053462982 2.2446589 4.1563787 5.3613462 5.8276405 4.8126268 3.139997 0.88387012 -1.0872931 -2.8734574 -4.2873311 -5.1482449 -5.6982965][-3.7468348 -2.1898537 -0.20575428 2.4026723 4.5679455 5.911664 6.5791368 5.95218 4.5238991 2.3432817 0.31303644 -1.2908769 -2.6109109 -3.6707573 -4.4819593][-4.8653846 -3.4726181 -1.7274601 0.46504498 2.4280181 4.0223188 5.0643778 4.8180761 3.9277325 2.3905926 0.74231052 -0.81315327 -2.1050286 -3.0482721 -3.7956326][-5.8371453 -4.8589716 -4.0044942 -2.2485979 -0.62773132 0.70529032 1.8140779 2.1419373 2.0127859 1.1390719 0.18245602 -0.82322121 -1.8927479 -2.9535737 -3.7839055][-7.1701946 -6.507226 -5.8520942 -4.8027129 -4.1453843 -3.0495615 -1.9595187 -1.403599 -0.95575452 -0.98172927 -1.2317986 -1.8211391 -2.4986815 -3.2580209 -3.9544704][-7.6604576 -7.4367576 -7.4105339 -7.0508647 -6.6756048 -6.0600758 -5.59922 -4.818512 -3.8599839 -3.3271666 -2.9881907 -3.0128999 -3.2456269 -3.7409356 -4.2269249][-7.5785341 -7.5942035 -7.7774897 -7.9666734 -8.244792 -8.0208607 -7.6071787 -6.863358 -6.1337562 -5.2538757 -4.3981495 -4.1236148 -4.0271373 -4.2206435 -4.5029545][-6.487083 -6.6129036 -7.0640903 -7.5322051 -8.0275478 -8.1704655 -8.1088142 -7.5421829 -6.8174977 -5.9876947 -5.276577 -4.92037 -4.5914121 -4.4318795 -4.3732719][-5.9777737 -5.8084064 -5.8678327 -6.4036207 -7.0559826 -7.3745327 -7.5011582 -7.1666765 -6.7125325 -6.1386633 -5.5486126 -5.0844207 -4.5589547 -4.2098408 -3.9346423]]...]
INFO - root - 2017-12-16 05:31:11.722401: step 23610, loss = 0.29, batch loss = 0.23 (28.9 examples/sec; 0.277 sec/batch; 23h:47m:12s remains)
INFO - root - 2017-12-16 05:31:14.543558: step 23620, loss = 0.35, batch loss = 0.29 (29.6 examples/sec; 0.270 sec/batch; 23h:11m:41s remains)
INFO - root - 2017-12-16 05:31:17.329646: step 23630, loss = 0.21, batch loss = 0.16 (29.4 examples/sec; 0.272 sec/batch; 23h:20m:00s remains)
INFO - root - 2017-12-16 05:31:20.175206: step 23640, loss = 0.21, batch loss = 0.15 (28.2 examples/sec; 0.284 sec/batch; 24h:20m:31s remains)
INFO - root - 2017-12-16 05:31:22.970889: step 23650, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.286 sec/batch; 24h:30m:28s remains)
INFO - root - 2017-12-16 05:31:25.797937: step 23660, loss = 0.23, batch loss = 0.17 (26.9 examples/sec; 0.297 sec/batch; 25h:28m:38s remains)
INFO - root - 2017-12-16 05:31:28.653974: step 23670, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.286 sec/batch; 24h:31m:05s remains)
INFO - root - 2017-12-16 05:31:31.444517: step 23680, loss = 0.27, batch loss = 0.21 (29.2 examples/sec; 0.274 sec/batch; 23h:32m:20s remains)
INFO - root - 2017-12-16 05:31:34.298719: step 23690, loss = 0.33, batch loss = 0.28 (27.6 examples/sec; 0.290 sec/batch; 24h:54m:23s remains)
INFO - root - 2017-12-16 05:31:37.132101: step 23700, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 23h:56m:08s remains)
2017-12-16 05:31:37.606339: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.18437 -2.8563676 -2.7996762 -2.7994785 -3.1356635 -3.6099193 -4.0643129 -4.5875578 -5.0871439 -5.1393995 -5.1320639 -5.5832562 -6.0594282 -6.0028753 -5.8294592][-2.5226245 -2.1722214 -1.8219514 -1.9231057 -2.414475 -3.0760808 -3.8953934 -4.6174316 -5.254797 -5.852561 -6.2435555 -6.276907 -6.4809923 -6.8877907 -6.83892][-1.0750148 -0.80936646 -0.4621613 -0.7791183 -1.4595139 -2.4284468 -3.4072194 -4.3353434 -5.1874113 -5.6068587 -5.8726177 -6.4560337 -6.7048655 -6.85298 -6.9212303][0.30767488 0.39673042 0.46692896 0.1073761 -0.72746015 -1.6122196 -2.4679036 -3.1329494 -3.8719523 -4.8740244 -5.58768 -5.8522663 -6.1688938 -6.8663492 -6.6848173][0.39971972 0.60514069 0.78351164 0.36121368 -0.25857735 -0.89628744 -1.5338526 -2.1745832 -2.9089415 -3.5380301 -4.3712907 -5.4277205 -6.1246891 -6.1439838 -6.036911][-0.081753254 0.39550114 0.81959963 0.68312979 0.40804958 0.054072857 -0.27189493 -0.95467091 -1.7653763 -2.4693556 -3.2810788 -4.0968881 -4.7375803 -4.8321509 -4.2848835][-0.6851182 -0.043780804 0.42282009 0.68753815 0.81373739 0.7116766 0.45469046 -0.0665493 -0.80443764 -1.6040409 -2.4448714 -3.1038406 -3.2162604 -2.8880758 -2.28948][-1.5526867 -0.9053123 -0.32027769 0.10710812 0.2202549 0.13345671 0.0037078857 -0.4356544 -1.1044593 -1.7531805 -2.2262323 -2.4796128 -2.1815846 -1.5635118 -0.55888033][-2.8636024 -2.0346422 -1.3278551 -0.71303868 -0.39375734 -0.25013113 -0.25339746 -0.66393018 -1.3875563 -2.0783813 -2.6682827 -2.5856385 -2.0755789 -1.1504936 0.019582748][-4.74567 -3.9199748 -3.0757375 -2.3466425 -1.6161945 -1.2237575 -1.0358591 -1.3472598 -1.895376 -2.4168212 -2.7690015 -2.7650309 -2.2248704 -1.160708 -0.14107323][-4.9884415 -4.3981771 -3.8623266 -3.183485 -2.5182071 -2.2569842 -2.1573029 -2.3714871 -2.5579762 -3.1243706 -3.5034003 -3.2624655 -2.6167905 -1.5352731 -0.50606823][-5.013268 -4.7045927 -4.3586459 -3.9063625 -3.532876 -3.3066921 -3.0701132 -3.1359358 -3.27248 -3.497602 -3.5759847 -3.6435962 -3.3260922 -2.5216317 -1.8492391][-4.5275364 -4.3179903 -4.2199063 -4.0838828 -3.921174 -3.8779821 -3.7270641 -3.7064505 -3.7765713 -3.9355786 -3.9093416 -3.7462673 -3.6740258 -3.3071415 -2.8578033][-4.1021972 -3.6432278 -3.4520655 -3.314137 -3.0719669 -2.8879669 -2.7670436 -2.8582067 -2.9415774 -2.905853 -3.068702 -3.2801352 -3.3680775 -3.3200221 -3.3681419][-3.7482319 -3.2717998 -2.9658747 -2.727695 -2.6211081 -2.5038648 -2.350491 -2.2965162 -2.1606023 -2.1093478 -2.1464312 -2.1832008 -2.4644279 -2.7515252 -3.0109396]]...]
INFO - root - 2017-12-16 05:31:40.423376: step 23710, loss = 0.29, batch loss = 0.23 (28.7 examples/sec; 0.279 sec/batch; 23h:55m:03s remains)
INFO - root - 2017-12-16 05:31:43.276964: step 23720, loss = 0.26, batch loss = 0.20 (27.1 examples/sec; 0.295 sec/batch; 25h:18m:48s remains)
INFO - root - 2017-12-16 05:31:46.124322: step 23730, loss = 0.21, batch loss = 0.15 (28.8 examples/sec; 0.277 sec/batch; 23h:47m:08s remains)
INFO - root - 2017-12-16 05:31:48.923416: step 23740, loss = 0.26, batch loss = 0.20 (27.9 examples/sec; 0.286 sec/batch; 24h:33m:09s remains)
INFO - root - 2017-12-16 05:31:51.790030: step 23750, loss = 0.30, batch loss = 0.24 (27.9 examples/sec; 0.286 sec/batch; 24h:33m:24s remains)
INFO - root - 2017-12-16 05:31:54.651172: step 23760, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 24h:08m:34s remains)
INFO - root - 2017-12-16 05:31:57.467210: step 23770, loss = 0.21, batch loss = 0.15 (29.0 examples/sec; 0.276 sec/batch; 23h:40m:08s remains)
INFO - root - 2017-12-16 05:32:00.323783: step 23780, loss = 0.29, batch loss = 0.23 (27.2 examples/sec; 0.294 sec/batch; 25h:13m:20s remains)
INFO - root - 2017-12-16 05:32:03.187673: step 23790, loss = 0.26, batch loss = 0.20 (28.4 examples/sec; 0.282 sec/batch; 24h:09m:22s remains)
INFO - root - 2017-12-16 05:32:06.036778: step 23800, loss = 0.28, batch loss = 0.22 (29.3 examples/sec; 0.273 sec/batch; 23h:24m:56s remains)
2017-12-16 05:32:06.513705: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0559597 -3.2950816 -3.6648028 -4.0420728 -4.2619629 -4.2917552 -4.2167315 -4.051846 -3.9404719 -3.91004 -3.868531 -3.780045 -3.6075842 -3.212739 -3.1794863][-2.1125169 -2.2863467 -2.79853 -3.331552 -3.626509 -3.7257032 -3.7777493 -3.6791029 -3.6302257 -3.5836961 -3.6346684 -3.548317 -3.1653347 -2.6653717 -2.5393071][-1.5778584 -1.7731311 -2.2245905 -2.6561975 -2.9994578 -3.1517658 -3.2832751 -3.3798912 -3.5716059 -3.6659126 -3.7136354 -3.4863939 -3.0404263 -2.5058265 -2.5688965][-1.6646557 -1.9435217 -2.255008 -2.3299105 -2.2194252 -2.2168832 -2.294559 -2.5051193 -2.9484537 -3.2613382 -3.4905603 -3.6558359 -3.5284176 -2.8836184 -2.729053][-2.303097 -2.3405735 -2.2511604 -2.1249373 -1.7814243 -1.1463084 -0.83912587 -1.0879276 -1.6672287 -2.2311757 -2.8356564 -3.3826852 -3.6328616 -3.5081191 -3.4243951][-3.2053771 -3.0655675 -2.7743745 -1.9274669 -0.85600424 0.0971508 0.67788267 0.54223919 -0.21366072 -1.1215198 -2.1088436 -3.0453572 -3.6423545 -3.7042139 -3.6836696][-4.2504869 -3.7970848 -3.0893474 -1.8983552 -0.48579264 0.83926249 1.9255347 2.0733867 1.3966675 0.17040205 -1.1805122 -2.489615 -3.4171791 -3.5656533 -3.5586381][-4.6317487 -4.1557603 -3.3642082 -2.1378863 -0.4037466 1.3533731 2.6696229 2.7513509 2.1006184 0.93429375 -0.45046306 -1.9039998 -2.9102087 -3.2484844 -3.2906122][-5.2253189 -4.6305394 -3.9783654 -2.8932829 -1.3148985 0.582706 2.1233683 2.426733 1.8179026 0.50335741 -0.78575993 -2.0938299 -2.9168859 -3.1572561 -3.1316671][-5.5926924 -5.4696264 -5.1445022 -3.9914582 -2.4962776 -1.0148571 0.31967688 0.83643436 0.43087196 -0.70676446 -1.8954368 -2.8944111 -3.3127418 -3.246357 -2.9292045][-5.6996331 -5.88585 -5.793437 -5.1492152 -3.8682063 -2.4730077 -1.3211586 -1.0470784 -1.398967 -2.2276351 -3.0396688 -3.6467266 -3.8196354 -3.4777002 -2.9671595][-5.7597461 -5.9381161 -6.0938768 -5.8315787 -5.0845776 -4.0881214 -3.1488321 -2.9647183 -3.1842911 -3.6521716 -3.9925623 -4.1555862 -3.9483912 -3.3726006 -3.0081267][-5.0938911 -5.5156369 -5.9021707 -5.7689438 -5.4456849 -4.9679794 -4.3952665 -4.1149445 -4.0982547 -4.19291 -4.18571 -3.9599798 -3.515177 -2.951972 -2.5288911][-4.4238267 -4.7543769 -5.0796642 -5.1814451 -5.1302257 -4.8186035 -4.5215626 -4.4153051 -4.2483211 -3.8682113 -3.5673957 -3.2268596 -2.7842185 -2.3052049 -2.1233439][-3.9315732 -4.1103053 -4.35103 -4.5052357 -4.5575085 -4.5149136 -4.2439232 -4.067749 -3.806242 -3.4168925 -3.0878823 -2.6332173 -2.2779727 -1.9926963 -2.0249784]]...]
INFO - root - 2017-12-16 05:32:09.331945: step 23810, loss = 0.24, batch loss = 0.18 (27.8 examples/sec; 0.288 sec/batch; 24h:40m:35s remains)
INFO - root - 2017-12-16 05:32:12.143604: step 23820, loss = 0.21, batch loss = 0.15 (28.3 examples/sec; 0.283 sec/batch; 24h:13m:27s remains)
INFO - root - 2017-12-16 05:32:15.033571: step 23830, loss = 0.22, batch loss = 0.17 (27.5 examples/sec; 0.291 sec/batch; 24h:56m:03s remains)
INFO - root - 2017-12-16 05:32:17.833844: step 23840, loss = 0.36, batch loss = 0.31 (27.9 examples/sec; 0.287 sec/batch; 24h:35m:43s remains)
INFO - root - 2017-12-16 05:32:20.654158: step 23850, loss = 0.22, batch loss = 0.16 (27.3 examples/sec; 0.293 sec/batch; 25h:05m:27s remains)
INFO - root - 2017-12-16 05:32:23.501481: step 23860, loss = 0.28, batch loss = 0.22 (27.4 examples/sec; 0.292 sec/batch; 25h:03m:08s remains)
INFO - root - 2017-12-16 05:32:26.322765: step 23870, loss = 0.33, batch loss = 0.27 (29.5 examples/sec; 0.271 sec/batch; 23h:16m:17s remains)
INFO - root - 2017-12-16 05:32:29.144826: step 23880, loss = 0.32, batch loss = 0.26 (29.5 examples/sec; 0.271 sec/batch; 23h:14m:22s remains)
INFO - root - 2017-12-16 05:32:32.006449: step 23890, loss = 0.36, batch loss = 0.30 (27.9 examples/sec; 0.286 sec/batch; 24h:33m:05s remains)
INFO - root - 2017-12-16 05:32:34.839720: step 23900, loss = 0.29, batch loss = 0.23 (27.8 examples/sec; 0.288 sec/batch; 24h:42m:12s remains)
2017-12-16 05:32:35.299128: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0207138 -3.8887434 -3.6734371 -3.3514972 -3.2040908 -3.2033787 -3.1132717 -3.2772589 -3.481118 -3.4477839 -3.2087488 -3.0651884 -2.8081424 -2.734633 -2.7033911][-3.8893461 -3.8252861 -3.6275232 -3.2671037 -3.0390663 -3.2123828 -3.3214426 -3.2575712 -3.18331 -3.1834064 -3.1778588 -3.1302788 -2.938257 -2.9730272 -2.9365249][-3.5620744 -3.3520303 -3.0014009 -2.7656686 -2.6932313 -2.8858619 -2.9954147 -3.1062031 -3.1408858 -3.0752478 -3.0261765 -2.9728565 -2.9825931 -3.085331 -3.1393318][-2.9370794 -2.6818783 -2.5084972 -2.4450595 -2.4960876 -2.6683993 -2.7151804 -2.6022105 -2.4180839 -2.3432312 -2.3432488 -2.574748 -2.8376193 -3.1607342 -3.3569279][-1.7847991 -1.3861706 -1.4019134 -1.5222821 -1.7146103 -1.850302 -1.8904557 -1.6131458 -1.2357194 -1.1021721 -1.1682224 -1.4299161 -1.754617 -2.2149837 -2.5674863][-0.77940845 -0.3214922 -0.37780094 -0.68317986 -0.97042608 -1.042259 -0.76872492 -0.3755846 -0.01714468 0.1733284 0.062802792 -0.53035164 -1.1103976 -1.4692616 -1.5821671][-0.2678175 0.063613892 0.037090778 -0.20144796 -0.357996 -0.19705391 0.27341461 0.77565813 1.0481067 0.92691183 0.51026011 -0.13136721 -0.70273304 -1.0355225 -1.0133476][-0.056389809 0.41481066 0.37573242 0.29588366 0.3961463 0.82644033 1.3694391 1.8450027 1.9520235 1.3696599 0.4185276 -0.46577024 -1.0233049 -1.3013394 -1.2936342][-0.39680195 0.21015692 0.42540216 0.584115 0.92888832 1.5426836 2.1101322 2.2440839 1.9242983 1.1438546 0.099917889 -1.0268426 -1.7478533 -1.8002057 -1.6152606][-0.99031091 -0.27339315 0.12118053 0.36851215 0.88823891 1.5454688 1.927669 1.8167462 1.2033362 0.16540241 -0.99689865 -1.9999835 -2.6178057 -2.6224575 -2.3301253][-1.8972034 -1.2802737 -0.71055841 -0.31443357 0.18645763 0.69706917 0.93942928 0.50992012 -0.48805737 -1.5017874 -2.3496878 -3.0346479 -3.4430375 -3.3288057 -2.9468589][-3.2540088 -2.5891685 -1.8200738 -1.2379916 -0.75783396 -0.618886 -0.8460238 -1.2582333 -1.9543726 -3.0060287 -3.913728 -4.1656365 -4.0471153 -3.6260076 -3.2489457][-4.5405221 -4.0165787 -3.2064607 -2.5696435 -2.1333807 -2.0205598 -2.26126 -2.8658447 -3.6491826 -4.0541506 -4.265274 -4.2523694 -4.0657563 -3.4794743 -2.9836349][-6.0502238 -5.546886 -4.6954765 -4.0354238 -3.444211 -3.3811159 -3.6686711 -4.1598916 -4.573266 -4.7284102 -4.62663 -4.2498322 -3.8006349 -3.0527613 -2.5046337][-6.9386687 -6.5212069 -5.7675076 -5.0834866 -4.4960451 -4.3107758 -4.3158083 -4.7046661 -5.0605879 -5.0821161 -4.7225947 -4.1311851 -3.5953622 -2.9429955 -2.4885249]]...]
INFO - root - 2017-12-16 05:32:38.125838: step 23910, loss = 0.33, batch loss = 0.27 (26.8 examples/sec; 0.298 sec/batch; 25h:33m:32s remains)
INFO - root - 2017-12-16 05:32:40.967163: step 23920, loss = 0.18, batch loss = 0.12 (28.2 examples/sec; 0.284 sec/batch; 24h:18m:53s remains)
INFO - root - 2017-12-16 05:32:43.783404: step 23930, loss = 0.23, batch loss = 0.17 (29.6 examples/sec; 0.270 sec/batch; 23h:09m:54s remains)
INFO - root - 2017-12-16 05:32:46.659276: step 23940, loss = 0.33, batch loss = 0.27 (28.1 examples/sec; 0.284 sec/batch; 24h:22m:41s remains)
INFO - root - 2017-12-16 05:32:49.459307: step 23950, loss = 0.30, batch loss = 0.24 (29.1 examples/sec; 0.275 sec/batch; 23h:35m:24s remains)
INFO - root - 2017-12-16 05:32:52.321813: step 23960, loss = 0.24, batch loss = 0.18 (28.7 examples/sec; 0.279 sec/batch; 23h:53m:35s remains)
INFO - root - 2017-12-16 05:32:55.160347: step 23970, loss = 0.24, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 23h:28m:40s remains)
INFO - root - 2017-12-16 05:32:57.980629: step 23980, loss = 0.27, batch loss = 0.21 (28.4 examples/sec; 0.281 sec/batch; 24h:06m:33s remains)
INFO - root - 2017-12-16 05:33:00.850006: step 23990, loss = 0.26, batch loss = 0.20 (29.0 examples/sec; 0.276 sec/batch; 23h:37m:54s remains)
INFO - root - 2017-12-16 05:33:03.685602: step 24000, loss = 0.20, batch loss = 0.14 (27.3 examples/sec; 0.293 sec/batch; 25h:04m:12s remains)
2017-12-16 05:33:04.142421: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3587074 -4.191618 -4.0967193 -4.5264406 -5.237174 -6.0120578 -6.68853 -6.7852812 -6.7507148 -6.2411385 -5.7756925 -5.5502033 -4.985755 -4.2578378 -3.5337629][-4.9669027 -5.4258418 -5.8919539 -6.1189275 -6.9766245 -7.2095995 -7.2211709 -7.0538244 -6.5406547 -6.1282091 -5.9441171 -5.5377083 -4.8657446 -4.3156095 -3.7267342][-5.6000566 -6.1736064 -6.6698513 -6.90831 -6.9266171 -6.5735331 -6.5441933 -6.1547351 -5.7366514 -5.39058 -5.1424804 -5.269721 -5.2262259 -4.7472262 -4.1066446][-6.0403366 -6.9708405 -7.3025107 -7.019206 -6.4294767 -5.3634992 -4.4702096 -3.7668655 -3.4531281 -3.9378891 -4.6808271 -4.9854326 -5.0509462 -5.1011462 -4.7813144][-5.9812093 -6.9655795 -7.0635862 -6.2406144 -4.6792722 -2.8541503 -1.5648177 -0.62012196 -0.68355608 -1.8038914 -3.0326812 -4.3962436 -5.3188648 -5.3150806 -4.7644114][-4.6024823 -5.4040318 -5.5317478 -4.4345975 -2.0122962 0.60123348 2.5292764 3.405673 2.6683993 0.79440975 -1.7008288 -3.8260632 -5.0532908 -5.5301275 -5.1808681][-3.258194 -3.9565711 -3.6530473 -2.0734096 0.29304552 2.9918065 5.1696568 5.9602051 4.919117 2.4067307 -0.56711245 -3.2352567 -4.9929934 -5.4373574 -4.920537][-2.6452699 -3.4033074 -3.2379072 -1.4795852 1.2775021 4.1554022 6.133894 6.3172264 5.0335741 2.4134912 -0.85916519 -3.5240157 -4.9694881 -5.284503 -4.8020654][-2.6054869 -3.1327016 -2.6556537 -1.4639304 0.2547102 2.7733054 4.783865 5.0883856 3.6938515 0.89242554 -2.1274281 -4.4349084 -5.7149282 -5.7456951 -5.031353][-2.6109579 -3.3276844 -3.0383024 -1.9879611 -0.38820744 1.1424294 1.981988 2.2309308 1.1378951 -1.2867205 -3.8622577 -5.8341451 -6.5579619 -6.3244486 -5.5368004][-3.3741007 -3.7251289 -3.530571 -3.3123498 -2.8269663 -1.5268161 -0.26937437 -0.35403442 -1.7498949 -3.4209867 -5.1797476 -6.7125378 -7.24807 -6.7085657 -5.75232][-4.3995523 -4.660202 -4.7701631 -4.348597 -3.7250042 -3.433964 -3.1497574 -3.0864964 -3.6690507 -4.9549184 -6.3636703 -7.1383462 -7.1673017 -6.5800152 -5.7308211][-5.3346992 -5.4674854 -5.5678883 -5.5024376 -5.3754954 -4.7491937 -4.2300019 -4.6047144 -5.2996516 -5.94796 -6.7672405 -7.2526855 -7.1264362 -6.4375963 -5.6412282][-6.0724945 -6.0670681 -6.0553684 -5.9968748 -5.8390837 -5.4084854 -5.2332077 -5.1179461 -5.3303065 -5.9528217 -6.464973 -6.5301571 -6.3117623 -5.7356582 -5.129756][-6.7205935 -6.6428843 -6.5174103 -6.2975082 -6.0042872 -5.6190729 -5.3573055 -5.0618143 -5.0831366 -5.3715816 -5.6302786 -5.5674567 -5.2352691 -4.6781793 -4.1768346]]...]
INFO - root - 2017-12-16 05:33:06.949783: step 24010, loss = 0.32, batch loss = 0.26 (29.2 examples/sec; 0.274 sec/batch; 23h:26m:58s remains)
INFO - root - 2017-12-16 05:33:09.771698: step 24020, loss = 0.24, batch loss = 0.19 (28.6 examples/sec; 0.280 sec/batch; 23h:57m:11s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 05:33:12.616487: step 24030, loss = 0.32, batch loss = 0.27 (28.7 examples/sec; 0.279 sec/batch; 23h:54m:51s remains)
INFO - root - 2017-12-16 05:33:15.402200: step 24040, loss = 0.30, batch loss = 0.24 (28.7 examples/sec; 0.279 sec/batch; 23h:54m:08s remains)
INFO - root - 2017-12-16 05:33:18.201757: step 24050, loss = 0.22, batch loss = 0.16 (29.7 examples/sec; 0.269 sec/batch; 23h:02m:48s remains)
INFO - root - 2017-12-16 05:33:21.025812: step 24060, loss = 0.22, batch loss = 0.16 (27.3 examples/sec; 0.293 sec/batch; 25h:06m:13s remains)
INFO - root - 2017-12-16 05:33:23.852842: step 24070, loss = 0.38, batch loss = 0.32 (28.5 examples/sec; 0.281 sec/batch; 24h:03m:42s remains)
INFO - root - 2017-12-16 05:33:26.739407: step 24080, loss = 0.36, batch loss = 0.30 (28.9 examples/sec; 0.276 sec/batch; 23h:40m:34s remains)
INFO - root - 2017-12-16 05:33:29.597328: step 24090, loss = 0.29, batch loss = 0.23 (28.4 examples/sec; 0.282 sec/batch; 24h:09m:43s remains)
INFO - root - 2017-12-16 05:33:32.389006: step 24100, loss = 0.30, batch loss = 0.24 (28.1 examples/sec; 0.285 sec/batch; 24h:24m:52s remains)
2017-12-16 05:33:32.862677: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.52350783 -0.86959291 -1.4111383 -1.8323026 -2.2223594 -2.5169973 -2.7243476 -2.6856287 -2.3711147 -2.2150517 -2.1952908 -1.7941296 -1.3746555 -1.4335442 -1.7763274][-0.71377707 -0.96043372 -1.3315074 -1.8065193 -2.1264856 -2.1926756 -2.2445126 -2.4031265 -2.5956674 -2.4333687 -2.3818815 -2.2048697 -2.0823493 -2.1105998 -2.3532639][-1.4608662 -1.6090214 -1.8291037 -2.1056995 -2.1905141 -2.0691793 -2.020613 -2.148689 -2.409014 -2.6828206 -3.0595357 -3.0750945 -3.0265722 -3.20932 -3.549608][-2.242347 -2.3215816 -2.4722853 -2.3208184 -1.9620383 -1.5830667 -1.3447282 -1.5248699 -1.9883118 -2.4559183 -3.0066438 -3.5907664 -4.0747132 -4.3839893 -4.4754362][-2.8944259 -2.8654325 -2.7690349 -2.238318 -1.5331755 -0.87732649 -0.36594629 -0.34588957 -0.85856557 -1.7824461 -2.8401656 -3.5751793 -4.1485891 -4.7939353 -4.96799][-3.3905666 -3.2556052 -2.7427974 -1.8607731 -0.84433842 0.22929573 1.0776367 1.2653189 0.71712875 -0.31665802 -1.5980537 -2.8582115 -4.0099378 -4.6522346 -4.8442345][-3.2118988 -3.0271828 -2.4434969 -1.4613855 -0.26710272 1.0365634 2.0170202 2.36794 1.8776593 0.71052837 -0.90588307 -2.4676838 -3.7793465 -4.6615191 -5.0635295][-2.8463113 -2.676755 -2.2084198 -1.137614 0.13386726 1.4348965 2.2880783 2.5976677 2.1323586 0.90782976 -0.63139629 -2.3761728 -3.9234519 -5.0212393 -5.5349059][-2.9322457 -2.7903457 -2.3056145 -1.3832972 -0.22331429 0.963552 1.864213 2.3021994 2.0136466 0.93204689 -0.59335279 -2.3081527 -3.8360376 -5.0361285 -5.5952225][-3.9309587 -3.7619815 -3.0859 -2.1229572 -1.1032195 -0.071777344 0.86742449 1.3587809 1.1555734 0.40760946 -0.77505732 -2.3558824 -3.8610103 -4.8215942 -5.2084403][-4.8957992 -4.9053025 -4.4213862 -3.4078841 -2.250571 -1.1757643 -0.28401327 0.40506554 0.52789164 0.080266 -0.86499572 -2.1682804 -3.4383075 -4.1228042 -4.226439][-5.5683117 -5.6890225 -5.45076 -4.6457357 -3.5158458 -2.2502203 -1.2070816 -0.52669144 -0.36588287 -0.60346127 -1.2193789 -2.1389418 -2.9717479 -3.4390588 -3.4168136][-6.1981134 -6.2960582 -5.9929357 -5.3310766 -4.1671562 -2.8451109 -1.6210208 -0.83912611 -0.63245249 -0.77040339 -1.2325621 -1.7890859 -2.2700915 -2.5186615 -2.4160118][-6.4170175 -6.6053944 -6.1389189 -5.3135357 -4.0164289 -2.4724355 -1.084223 -0.32965565 -0.11785316 -0.20191908 -0.51823354 -0.99159145 -1.3358524 -1.2677159 -1.0873442][-5.7396793 -5.9984488 -5.6589446 -4.7039981 -3.3014107 -1.7823162 -0.4090066 0.3074522 0.37192869 0.25122833 -0.056069374 -0.45494151 -0.87311983 -0.92613935 -0.78171253]]...]
INFO - root - 2017-12-16 05:33:35.700309: step 24110, loss = 0.25, batch loss = 0.19 (27.7 examples/sec; 0.289 sec/batch; 24h:43m:33s remains)
INFO - root - 2017-12-16 05:33:38.584864: step 24120, loss = 0.34, batch loss = 0.28 (29.1 examples/sec; 0.274 sec/batch; 23h:30m:43s remains)
INFO - root - 2017-12-16 05:33:41.428392: step 24130, loss = 0.32, batch loss = 0.26 (27.6 examples/sec; 0.290 sec/batch; 24h:50m:31s remains)
INFO - root - 2017-12-16 05:33:44.285562: step 24140, loss = 0.30, batch loss = 0.24 (29.1 examples/sec; 0.275 sec/batch; 23h:33m:55s remains)
INFO - root - 2017-12-16 05:33:47.110341: step 24150, loss = 0.37, batch loss = 0.31 (29.7 examples/sec; 0.270 sec/batch; 23h:05m:58s remains)
INFO - root - 2017-12-16 05:33:49.915730: step 24160, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.279 sec/batch; 23h:51m:46s remains)
INFO - root - 2017-12-16 05:33:52.780962: step 24170, loss = 0.29, batch loss = 0.23 (27.3 examples/sec; 0.293 sec/batch; 25h:03m:24s remains)
INFO - root - 2017-12-16 05:33:55.634314: step 24180, loss = 0.21, batch loss = 0.15 (28.5 examples/sec; 0.280 sec/batch; 24h:01m:18s remains)
INFO - root - 2017-12-16 05:33:58.468075: step 24190, loss = 0.29, batch loss = 0.23 (27.1 examples/sec; 0.295 sec/batch; 25h:15m:27s remains)
INFO - root - 2017-12-16 05:34:01.226263: step 24200, loss = 0.33, batch loss = 0.27 (28.5 examples/sec; 0.280 sec/batch; 24h:00m:50s remains)
2017-12-16 05:34:01.683831: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3814249 -4.56096 -4.6056528 -4.7980242 -4.8782854 -4.8672476 -4.7784939 -4.9514933 -5.0904379 -4.9355583 -4.7134905 -4.4995055 -4.3333216 -4.0578747 -3.7509022][-3.5535085 -3.9383042 -4.359519 -4.5121078 -4.4989953 -4.6902862 -4.5510612 -4.3142552 -4.2691169 -4.3251214 -4.2981291 -4.1069984 -3.8343034 -3.481935 -3.2344704][-2.7971179 -3.4264326 -4.0410905 -4.533041 -4.4031582 -3.9789991 -3.5011392 -3.1353388 -3.01503 -3.2876153 -3.5973823 -3.6379256 -3.4394114 -3.209507 -2.859993][-2.2396798 -3.1234679 -4.0716186 -4.36828 -3.903857 -3.1369133 -2.1934309 -1.7360256 -1.8512864 -2.0542223 -2.3880408 -2.9449835 -3.1982799 -2.8378854 -2.4002845][-2.2367702 -3.2003675 -4.0214624 -4.1266274 -3.31995 -2.1904669 -1.0141995 -0.32378626 -0.31501865 -1.0046334 -2.0059285 -2.5739775 -2.7951488 -2.8968887 -2.855159][-2.648057 -3.8754511 -4.3810105 -4.002038 -2.761816 -1.1454372 0.42481518 1.2959404 1.1385174 0.26281595 -0.77423406 -2.1288912 -3.3301885 -3.3916686 -3.0457888][-2.7289248 -3.9774878 -4.4787488 -3.7757411 -2.1794314 -0.37163019 1.4506831 2.4637508 2.4370584 1.2413344 -0.31051254 -1.7081082 -2.7448022 -3.456579 -3.7137856][-2.4473083 -3.7448318 -4.5056124 -3.988658 -2.14246 -0.029667854 1.8239493 3.0168281 2.9074259 1.5545158 -0.3230691 -1.9634693 -3.0396533 -3.5515759 -3.6377807][-2.1602409 -3.4619422 -4.1747389 -4.066783 -2.8200259 -0.55707932 1.549417 2.6677709 2.5310273 1.452311 -0.43372917 -2.4723184 -3.8775914 -4.1070852 -3.8194463][-1.8130057 -3.3097193 -4.5227323 -4.4210534 -3.1437888 -1.3167541 0.50707865 1.7388182 1.517077 0.2566309 -1.5416963 -3.3204317 -4.6372313 -5.1436915 -4.7468705][-1.6178813 -2.8401818 -4.1377854 -4.3381886 -3.6120682 -2.1813328 -0.68883705 0.04493618 -0.15233421 -1.3673453 -3.1402621 -4.5661192 -5.349082 -5.40135 -4.9527059][-1.4800384 -2.5442538 -3.83348 -4.253552 -3.8254216 -2.99023 -2.2929306 -1.7148855 -1.9455166 -3.105207 -4.5539265 -5.575696 -5.8180833 -5.2853231 -4.5393157][-1.4048712 -2.4737482 -3.8938522 -4.4604807 -4.5841684 -4.0200162 -3.3996694 -3.6035681 -4.2157936 -4.8408008 -5.4776869 -6.1074638 -6.1414728 -5.5870004 -4.6842394][-1.4715071 -2.2153916 -3.5343418 -4.2331004 -4.6924491 -4.9098139 -4.96103 -5.2035961 -5.6969881 -6.361865 -6.7617941 -6.7375855 -6.2495451 -5.5159979 -4.8489971][-1.5918107 -1.9275086 -2.9610453 -3.8111484 -4.6571679 -5.3416352 -5.8193226 -6.5040121 -6.988584 -7.068552 -6.8998184 -6.6689863 -6.2273488 -5.4375114 -4.6395922]]...]
INFO - root - 2017-12-16 05:34:04.464920: step 24210, loss = 0.26, batch loss = 0.20 (29.1 examples/sec; 0.275 sec/batch; 23h:30m:32s remains)
INFO - root - 2017-12-16 05:34:07.294906: step 24220, loss = 0.22, batch loss = 0.16 (28.3 examples/sec; 0.283 sec/batch; 24h:13m:09s remains)
INFO - root - 2017-12-16 05:34:10.152346: step 24230, loss = 0.30, batch loss = 0.24 (28.1 examples/sec; 0.284 sec/batch; 24h:20m:32s remains)
INFO - root - 2017-12-16 05:34:13.005671: step 24240, loss = 0.32, batch loss = 0.27 (28.0 examples/sec; 0.285 sec/batch; 24h:25m:20s remains)
INFO - root - 2017-12-16 05:34:15.816016: step 24250, loss = 0.29, batch loss = 0.23 (29.0 examples/sec; 0.276 sec/batch; 23h:39m:26s remains)
INFO - root - 2017-12-16 05:34:18.621191: step 24260, loss = 0.21, batch loss = 0.15 (28.9 examples/sec; 0.276 sec/batch; 23h:39m:49s remains)
INFO - root - 2017-12-16 05:34:21.460714: step 24270, loss = 0.28, batch loss = 0.22 (27.1 examples/sec; 0.295 sec/batch; 25h:15m:52s remains)
INFO - root - 2017-12-16 05:34:24.281453: step 24280, loss = 0.31, batch loss = 0.26 (29.1 examples/sec; 0.275 sec/batch; 23h:31m:23s remains)
INFO - root - 2017-12-16 05:34:27.096622: step 24290, loss = 0.24, batch loss = 0.18 (27.4 examples/sec; 0.292 sec/batch; 24h:59m:47s remains)
INFO - root - 2017-12-16 05:34:29.887790: step 24300, loss = 0.27, batch loss = 0.21 (29.1 examples/sec; 0.275 sec/batch; 23h:34m:03s remains)
2017-12-16 05:34:30.329366: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0171371 -3.1467128 -3.4328923 -3.9588556 -4.4821911 -4.8851943 -5.0853739 -5.0804811 -5.05978 -4.87381 -4.646369 -4.74216 -4.8082476 -4.5323105 -4.2149749][-1.7831903 -2.1471531 -2.5327623 -3.0709138 -3.5213294 -4.1102138 -4.4046974 -4.6489234 -4.6700125 -4.7311182 -4.692843 -4.4051828 -4.2879157 -4.0892806 -3.7609689][-0.49610376 -0.96124458 -1.4282119 -2.1372685 -2.7443151 -3.320776 -3.5482113 -3.7884707 -4.1328669 -4.242835 -4.3820763 -4.2866926 -3.9041195 -3.3650885 -2.8485765][0.66911888 0.1231823 -0.64566946 -1.2063098 -1.6797347 -2.2256396 -2.5366435 -2.7836199 -3.1862731 -3.5622611 -3.668952 -3.5862684 -3.4437149 -2.8929005 -2.5106575][1.0919309 0.49029303 -0.10777664 -0.61952567 -0.84488988 -0.95550823 -0.78679061 -1.1168718 -1.7702985 -2.4764445 -3.1150446 -3.249577 -2.8405969 -2.2855065 -2.0546277][0.76591682 0.40396261 0.096666813 0.012986183 0.39740133 0.79491186 1.167645 0.92192888 0.0663991 -0.99134445 -1.8807189 -2.3211396 -2.5329771 -2.2924442 -2.0961018][-0.014719486 -0.17436647 -0.09985733 0.33007669 1.1686697 2.0861483 2.8539062 2.6914816 1.6774921 0.39956141 -1.0556564 -2.0062158 -2.4121573 -2.4115918 -2.405741][-0.89542651 -1.0531919 -0.95068407 -0.17145967 0.96473312 2.3202276 3.4286661 3.5129404 2.6174884 1.1339254 -0.56534624 -1.8400037 -2.7538066 -2.9972217 -3.0650902][-2.087316 -2.1701565 -1.552316 -0.75347567 0.32294703 1.7366209 2.9193826 3.099679 2.1671157 0.71347523 -1.0558872 -2.5365765 -3.6131129 -3.9295969 -3.6878598][-2.5231318 -2.6046727 -2.3907137 -1.6276844 -0.67137575 0.32207918 1.072535 1.3726006 0.93956709 -0.32819176 -1.8281455 -3.1962323 -4.1932549 -4.5670123 -4.3821797][-3.0048177 -3.0964346 -3.0632248 -2.7587996 -2.3766434 -1.6456912 -0.92498517 -0.5470264 -0.85440373 -1.5595129 -2.5989649 -3.5648885 -4.4335442 -4.6565447 -4.3256311][-3.0231423 -3.2904177 -3.5644789 -3.7565866 -3.6864719 -3.2640512 -2.7361228 -2.3077273 -2.1912403 -2.5298982 -3.3121226 -4.0535922 -4.627327 -4.6165895 -4.2286849][-2.5465319 -3.030345 -3.5823898 -4.0618024 -4.34494 -4.2123189 -3.7627718 -3.3769956 -3.1602936 -3.3956888 -3.6451073 -4.0858932 -4.3408852 -4.2673607 -3.6508372][-2.3898489 -2.8691316 -3.5292959 -4.052772 -4.5145321 -4.6106215 -4.27033 -3.8624163 -3.461674 -3.3742366 -3.5162501 -3.5189667 -3.5964544 -3.4567556 -3.1066091][-2.7261767 -3.0839579 -3.5193059 -3.9482124 -4.2993522 -4.4349051 -4.1101646 -3.6747079 -3.3617506 -3.2579174 -3.2794278 -3.1970143 -3.1647358 -2.8469334 -2.5444121]]...]
INFO - root - 2017-12-16 05:34:33.126605: step 24310, loss = 0.29, batch loss = 0.24 (29.4 examples/sec; 0.272 sec/batch; 23h:17m:32s remains)
INFO - root - 2017-12-16 05:34:35.903265: step 24320, loss = 0.23, batch loss = 0.17 (28.9 examples/sec; 0.277 sec/batch; 23h:43m:42s remains)
INFO - root - 2017-12-16 05:34:38.783856: step 24330, loss = 0.34, batch loss = 0.28 (28.0 examples/sec; 0.286 sec/batch; 24h:28m:13s remains)
INFO - root - 2017-12-16 05:34:41.616981: step 24340, loss = 0.32, batch loss = 0.26 (29.3 examples/sec; 0.273 sec/batch; 23h:21m:28s remains)
INFO - root - 2017-12-16 05:34:44.420480: step 24350, loss = 0.23, batch loss = 0.18 (27.6 examples/sec; 0.290 sec/batch; 24h:47m:11s remains)
INFO - root - 2017-12-16 05:34:47.267701: step 24360, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.277 sec/batch; 23h:43m:42s remains)
INFO - root - 2017-12-16 05:34:50.093952: step 24370, loss = 0.21, batch loss = 0.16 (29.2 examples/sec; 0.274 sec/batch; 23h:25m:28s remains)
INFO - root - 2017-12-16 05:34:53.014618: step 24380, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.285 sec/batch; 24h:26m:06s remains)
INFO - root - 2017-12-16 05:34:55.840307: step 24390, loss = 0.25, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 23h:57m:54s remains)
INFO - root - 2017-12-16 05:34:58.702040: step 24400, loss = 0.27, batch loss = 0.21 (27.5 examples/sec; 0.291 sec/batch; 24h:52m:40s remains)
2017-12-16 05:34:59.170351: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.256516 -4.0659347 -3.9398742 -3.756552 -3.6401753 -3.5784652 -3.5622633 -3.6636641 -3.9984481 -4.3399906 -4.6903143 -5.0577612 -5.5660014 -5.78018 -5.6793003][-4.6135354 -4.46313 -4.480269 -4.3678203 -4.265471 -4.2317619 -4.2375345 -4.2091942 -4.2342315 -4.3898349 -4.6811094 -4.7778106 -4.99372 -4.9883561 -4.6451807][-4.7554994 -4.934545 -4.9398308 -4.8172669 -4.6327972 -4.4418883 -4.3174772 -4.2202158 -4.1524291 -4.0824795 -4.0040212 -3.8885999 -3.7479086 -3.31805 -2.9595537][-4.6094804 -4.8227606 -4.8756156 -4.6383662 -4.260149 -3.919384 -3.6414709 -3.4890878 -3.4105883 -3.2918749 -3.12012 -2.8305912 -2.5390677 -1.8893781 -1.2448344][-4.35589 -4.4663725 -4.4054928 -4.0882239 -3.6389928 -3.1573627 -2.8431044 -2.7366364 -2.60934 -2.6208968 -2.6780088 -2.2244656 -1.6253796 -0.87758923 -0.58305931][-3.7299485 -3.7362695 -3.4966414 -3.1957166 -2.7365451 -2.106039 -1.690304 -1.6490085 -1.8733947 -2.0099778 -1.7834704 -1.6139977 -1.4425237 -0.56119323 0.0731554][-3.1071825 -2.9860969 -2.6214232 -2.151376 -1.6814952 -1.0751247 -0.60882044 -0.40809011 -0.58414316 -1.1253986 -1.6377034 -1.5245886 -1.0332558 -0.70567465 -0.595664][-2.8231182 -2.6306572 -2.1828415 -1.7979732 -1.2559085 -0.41750479 0.24534798 0.38300276 -0.00013017654 -0.60940695 -1.3335187 -1.8951626 -2.0993893 -1.5773907 -1.275737][-2.4822118 -2.4215229 -2.0928338 -1.7439737 -1.3028004 -0.58423781 0.19453239 0.37379742 -0.11952448 -0.95241523 -1.7858133 -2.4388318 -2.7797244 -2.5212393 -2.1081102][-2.1855121 -2.0345597 -1.8088546 -1.6592679 -1.3489587 -0.81989479 -0.27476978 -0.23864841 -0.66522765 -1.5768204 -2.6477137 -3.3587708 -3.387279 -3.1100419 -2.6909368][-1.9213569 -1.8096719 -1.694967 -1.4549744 -1.2572951 -0.89867306 -0.51038551 -0.68317056 -1.365346 -2.3674426 -3.3296392 -4.0716 -4.2707071 -3.8472996 -3.1712446][-1.7351854 -1.6100256 -1.582617 -1.6806886 -1.7302995 -1.6121869 -1.373328 -1.4877555 -2.0658324 -2.9273355 -3.7954566 -4.4408789 -4.6853776 -4.3894706 -3.81148][-1.4987218 -1.2852259 -1.3809969 -1.7257581 -1.9978664 -2.1722102 -2.125843 -2.271369 -2.7113495 -3.3077056 -4.0645723 -4.7115464 -5.0940437 -4.89641 -4.4870553][-1.1300993 -0.91763735 -1.1434324 -1.6080828 -2.2780087 -2.6857932 -2.6417139 -2.732965 -3.040359 -3.4666953 -3.923522 -4.4884448 -5.0196824 -5.101408 -4.8015909][-1.0817194 -0.77141666 -0.99203682 -1.5239534 -2.3213522 -2.9732215 -3.2453523 -3.3076801 -3.3974209 -3.5573783 -3.9402385 -4.2720652 -4.5978479 -4.8698406 -4.8748918]]...]
INFO - root - 2017-12-16 05:35:02.004885: step 24410, loss = 0.26, batch loss = 0.20 (29.3 examples/sec; 0.273 sec/batch; 23h:20m:21s remains)
INFO - root - 2017-12-16 05:35:04.804724: step 24420, loss = 0.25, batch loss = 0.19 (28.3 examples/sec; 0.282 sec/batch; 24h:10m:15s remains)
INFO - root - 2017-12-16 05:35:07.679731: step 24430, loss = 0.35, batch loss = 0.29 (27.9 examples/sec; 0.286 sec/batch; 24h:30m:46s remains)
INFO - root - 2017-12-16 05:35:10.492458: step 24440, loss = 0.28, batch loss = 0.22 (28.2 examples/sec; 0.284 sec/batch; 24h:17m:36s remains)
INFO - root - 2017-12-16 05:35:13.327276: step 24450, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 24h:12m:18s remains)
INFO - root - 2017-12-16 05:35:16.133480: step 24460, loss = 0.26, batch loss = 0.20 (28.4 examples/sec; 0.282 sec/batch; 24h:08m:20s remains)
INFO - root - 2017-12-16 05:35:18.926009: step 24470, loss = 0.31, batch loss = 0.25 (27.9 examples/sec; 0.287 sec/batch; 24h:31m:38s remains)
INFO - root - 2017-12-16 05:35:21.790754: step 24480, loss = 0.22, batch loss = 0.16 (28.8 examples/sec; 0.278 sec/batch; 23h:46m:08s remains)
INFO - root - 2017-12-16 05:35:24.658833: step 24490, loss = 0.21, batch loss = 0.15 (28.4 examples/sec; 0.282 sec/batch; 24h:05m:53s remains)
INFO - root - 2017-12-16 05:35:27.487044: step 24500, loss = 0.26, batch loss = 0.20 (26.4 examples/sec; 0.303 sec/batch; 25h:53m:57s remains)
2017-12-16 05:35:27.942314: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5966082 -4.4993882 -4.4154325 -4.3900423 -4.4670186 -4.5828152 -4.7183642 -4.8540492 -4.8470387 -4.7253742 -4.5422244 -4.306932 -4.0172787 -3.7595623 -3.6063783][-5.42304 -5.43621 -5.4224305 -5.4827414 -5.5433178 -5.6106486 -5.7222266 -5.7045836 -5.575089 -5.55923 -5.4606919 -5.2717376 -5.0617657 -4.7851009 -4.494679][-6.1788955 -6.3196187 -6.3634095 -6.2227206 -6.04426 -5.9015741 -5.7247167 -5.6458154 -5.7603073 -5.9173989 -5.9761047 -6.041172 -5.97552 -5.6421161 -5.2935333][-6.5973125 -6.6835842 -6.4863749 -6.044364 -5.5437937 -5.0127277 -4.4081278 -4.1274266 -4.2132206 -4.7869878 -5.5248966 -6.0379438 -6.1856232 -5.9709635 -5.5691595][-6.4568892 -6.3282681 -5.5815229 -4.4200096 -3.194869 -2.2436173 -1.5035837 -1.2958543 -1.5471728 -2.5564303 -3.947067 -5.0121474 -5.5993633 -5.5013771 -5.0552073][-5.7502646 -5.4177837 -4.3696637 -2.5230331 -0.47866416 1.1526699 2.1869826 2.1262584 1.3408456 -0.27250338 -2.2140095 -3.7999482 -4.6863027 -4.6351628 -4.0444064][-4.72314 -4.180099 -3.0438352 -1.0198171 1.3698044 3.59208 5.0880241 4.93834 3.8028517 1.8375926 -0.33800602 -2.2067156 -3.3804109 -3.478765 -3.0325749][-3.8246722 -3.1613073 -2.0135779 -0.18068457 2.1769423 4.4372711 5.9677391 5.9454279 4.6638479 2.5280094 0.24072456 -1.4128113 -2.2384529 -2.3004608 -1.9206877][-3.233263 -2.7165346 -1.8963809 -0.53714514 1.2924795 3.3476858 4.8753281 4.88315 3.8307524 1.967093 -0.0053057671 -1.3507707 -1.9059794 -1.8133059 -1.3105168][-3.2102666 -2.9407811 -2.6079469 -1.7539442 -0.40033388 1.1459012 2.460216 2.7450223 2.057394 0.71028233 -0.72031903 -1.6509225 -1.8227215 -1.4016809 -0.94588995][-3.3518348 -3.280797 -3.0704575 -2.5436041 -1.8819807 -0.89800072 0.13613462 0.39287663 0.12990046 -0.60344529 -1.4688413 -2.0494831 -2.0122907 -1.4609253 -0.93357682][-3.4328456 -3.2749629 -3.3436928 -3.3055258 -3.0251174 -2.4172819 -1.8449504 -1.6398168 -1.6219578 -1.9342785 -2.3837352 -2.7341332 -2.5157652 -1.9006662 -1.4278347][-3.4174337 -3.0813498 -3.2282033 -3.3476419 -3.4760706 -3.130506 -2.8549767 -2.7103839 -2.5239725 -2.6429191 -2.8098903 -3.0571051 -2.9023886 -2.2702923 -1.7489622][-3.4526842 -2.8355517 -2.8075478 -2.9992609 -3.4132826 -3.2765074 -3.140276 -3.144865 -3.0612063 -2.9412646 -2.8401289 -3.0947099 -3.1664195 -2.7441435 -2.333745][-3.099586 -2.4884524 -2.3608902 -2.7351148 -3.2822371 -3.0760183 -2.748538 -2.555738 -2.4841096 -2.7037668 -2.8213325 -3.0927052 -3.1746678 -2.9706061 -2.7540922]]...]
INFO - root - 2017-12-16 05:35:30.796746: step 24510, loss = 0.39, batch loss = 0.34 (28.4 examples/sec; 0.282 sec/batch; 24h:05m:56s remains)
INFO - root - 2017-12-16 05:35:33.601653: step 24520, loss = 0.24, batch loss = 0.19 (29.6 examples/sec; 0.270 sec/batch; 23h:04m:58s remains)
INFO - root - 2017-12-16 05:35:36.484222: step 24530, loss = 0.22, batch loss = 0.16 (26.4 examples/sec; 0.303 sec/batch; 25h:54m:10s remains)
INFO - root - 2017-12-16 05:35:39.341613: step 24540, loss = 0.28, batch loss = 0.22 (27.4 examples/sec; 0.292 sec/batch; 25h:00m:45s remains)
INFO - root - 2017-12-16 05:35:42.179124: step 24550, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 24h:07m:29s remains)
INFO - root - 2017-12-16 05:35:45.023945: step 24560, loss = 0.27, batch loss = 0.21 (27.0 examples/sec; 0.296 sec/batch; 25h:20m:08s remains)
INFO - root - 2017-12-16 05:35:47.925611: step 24570, loss = 0.29, batch loss = 0.23 (27.8 examples/sec; 0.288 sec/batch; 24h:37m:27s remains)
INFO - root - 2017-12-16 05:35:50.751409: step 24580, loss = 0.34, batch loss = 0.28 (27.9 examples/sec; 0.287 sec/batch; 24h:33m:59s remains)
INFO - root - 2017-12-16 05:35:53.586198: step 24590, loss = 0.29, batch loss = 0.23 (27.0 examples/sec; 0.297 sec/batch; 25h:21m:47s remains)
INFO - root - 2017-12-16 05:35:56.423152: step 24600, loss = 0.23, batch loss = 0.17 (26.7 examples/sec; 0.300 sec/batch; 25h:37m:16s remains)
2017-12-16 05:35:56.894533: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0787024 -5.3142514 -4.1689172 -3.0559196 -2.1420784 -1.855859 -1.7143035 -1.995744 -2.77812 -3.5895011 -4.13608 -4.447288 -4.45732 -4.0626993 -3.4322062][-5.3467069 -4.5506258 -3.5115898 -2.5021343 -1.7442636 -1.3417118 -1.303261 -1.677402 -2.4260204 -3.2537985 -3.9308133 -4.28507 -4.3312716 -4.0515594 -3.4854374][-4.367384 -3.6502523 -2.833446 -2.021708 -1.4267883 -1.1709759 -1.1271617 -1.5204329 -2.3280296 -3.2895687 -4.1124148 -4.6392217 -4.6943355 -4.35026 -3.7481077][-2.9744325 -2.4847145 -1.8435979 -1.3855863 -0.99869466 -0.75203037 -0.70878053 -1.0476058 -1.9655666 -2.9886775 -3.9062047 -4.5444307 -4.7504969 -4.3695869 -3.7640643][-1.9198785 -1.5824928 -1.0994949 -0.79815888 -0.21783543 0.34616995 0.6606636 0.53072071 -0.36650467 -1.6879497 -3.0855641 -4.1147189 -4.5202165 -4.4696212 -3.850718][-1.5576141 -1.3629544 -1.015151 -0.68528295 0.18482494 1.2220821 2.0110917 2.0330806 1.2675772 -0.32183695 -2.1058283 -3.6695786 -4.6328111 -4.8242364 -4.3782229][-1.7293546 -1.6718276 -1.3508048 -0.775275 0.32254362 1.6286974 2.6941857 2.9041076 2.0547848 0.32110548 -1.7818336 -3.7307827 -4.9078779 -5.3717866 -5.0283203][-2.0753026 -1.9436302 -1.7009985 -1.0641878 0.30722332 1.6930857 2.6955891 3.0437646 2.3241854 0.480659 -1.7906132 -3.7584395 -5.1331139 -5.7035456 -5.3986707][-2.4916494 -2.4499164 -2.1491675 -1.5687664 -0.25947952 1.1744213 2.3588996 2.7419152 1.9581876 0.31054163 -1.889317 -4.0164928 -5.5390773 -6.184586 -5.877852][-2.5115576 -2.6695144 -2.5488105 -2.0926125 -1.1141305 0.089290619 1.1846094 1.7587948 1.2518878 -0.298975 -2.4516041 -4.4853578 -5.9997067 -6.7028275 -6.3924894][-2.1663852 -2.5830054 -2.9226675 -2.8007534 -1.9759822 -0.88047194 0.13110638 0.60778475 0.17254066 -1.1984251 -3.1092503 -5.0237317 -6.3811984 -6.8986621 -6.5657234][-2.1866374 -2.8265927 -3.1730008 -3.1489844 -2.541024 -1.5304344 -0.6465559 -0.25357389 -0.66705871 -1.9395576 -3.7092278 -5.4325285 -6.5503645 -6.7884378 -6.2505274][-2.1703103 -2.9711397 -3.4600155 -3.4661386 -2.8003733 -1.8893979 -1.2194722 -0.82401252 -1.3244572 -2.4819548 -3.9545887 -5.3644447 -6.1294889 -6.2635465 -5.7645125][-2.243289 -3.0612831 -3.6014142 -3.5335658 -3.0040717 -2.110863 -1.4394267 -1.3097649 -1.7881608 -2.8105187 -4.1609011 -5.210309 -5.7730207 -5.7569251 -5.1308651][-2.9070868 -3.6114538 -3.9222207 -3.6903269 -3.0163894 -2.1139505 -1.5011084 -1.3963037 -1.8851118 -2.9625008 -4.1302814 -5.0703545 -5.4771466 -5.241703 -4.55996]]...]
INFO - root - 2017-12-16 05:35:59.703174: step 24610, loss = 0.26, batch loss = 0.20 (27.6 examples/sec; 0.290 sec/batch; 24h:46m:59s remains)
INFO - root - 2017-12-16 05:36:02.542799: step 24620, loss = 0.22, batch loss = 0.16 (27.3 examples/sec; 0.293 sec/batch; 25h:06m:01s remains)
INFO - root - 2017-12-16 05:36:05.386230: step 24630, loss = 0.40, batch loss = 0.34 (28.7 examples/sec; 0.279 sec/batch; 23h:51m:01s remains)
INFO - root - 2017-12-16 05:36:08.244180: step 24640, loss = 0.39, batch loss = 0.33 (28.9 examples/sec; 0.277 sec/batch; 23h:41m:02s remains)
INFO - root - 2017-12-16 05:36:11.073824: step 24650, loss = 0.23, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 24h:19m:58s remains)
INFO - root - 2017-12-16 05:36:13.877756: step 24660, loss = 0.24, batch loss = 0.18 (28.7 examples/sec; 0.279 sec/batch; 23h:51m:04s remains)
INFO - root - 2017-12-16 05:36:16.708188: step 24670, loss = 0.26, batch loss = 0.20 (29.3 examples/sec; 0.273 sec/batch; 23h:21m:31s remains)
INFO - root - 2017-12-16 05:36:19.520956: step 24680, loss = 0.25, batch loss = 0.19 (28.3 examples/sec; 0.283 sec/batch; 24h:10m:29s remains)
INFO - root - 2017-12-16 05:36:22.385908: step 24690, loss = 0.27, batch loss = 0.21 (27.3 examples/sec; 0.293 sec/batch; 25h:04m:28s remains)
INFO - root - 2017-12-16 05:36:25.285765: step 24700, loss = 0.31, batch loss = 0.25 (28.6 examples/sec; 0.280 sec/batch; 23h:55m:03s remains)
2017-12-16 05:36:25.716867: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2774372 -3.7240696 -4.0458674 -4.4196267 -4.5497503 -4.6137886 -4.6459489 -4.4742236 -4.2566495 -4.1195369 -3.885428 -3.6134522 -3.3307295 -2.9299171 -2.6069598][-1.7941399 -2.3725276 -2.8016253 -3.54009 -4.0482044 -4.4648752 -4.6152725 -4.7075257 -4.6405983 -4.480454 -4.2410235 -3.7339661 -3.1789224 -2.5931792 -2.0803847][-0.83163095 -1.3457172 -1.8224967 -2.45157 -3.067893 -3.9070349 -4.3918033 -4.7868347 -4.84839 -4.6754336 -4.359303 -3.8236728 -3.2393141 -2.4940805 -1.9359169][-0.11755753 -0.5565815 -0.92466331 -1.3456256 -1.9875131 -2.6535168 -3.3628278 -4.0036883 -4.4038253 -4.6188407 -4.441257 -3.9470947 -3.3351955 -2.6060586 -2.0892665][0.33439875 0.277668 0.31346607 0.14144802 -0.1927247 -0.6917429 -1.5066195 -2.41737 -3.1702049 -3.7594466 -4.0566287 -4.016346 -3.6328032 -3.214016 -2.8287122][0.28257513 0.73629045 1.2397366 1.7085681 1.9795313 1.6054931 0.83074284 -0.31549835 -1.4923606 -2.4757671 -3.3138785 -3.8855634 -3.9727757 -3.6610198 -3.2640204][-0.24552155 0.51233339 1.4134445 2.5125475 3.3188391 3.5563445 3.0950007 1.89147 0.42007208 -1.004339 -2.2935703 -3.2002192 -3.7190359 -3.7216904 -3.5027785][-1.4113324 -0.54081893 0.64383078 2.2098427 3.4568067 4.2102556 4.1570272 3.0787911 1.5964003 -0.10304642 -1.7231643 -3.0535374 -3.8497157 -4.0289745 -3.8691201][-3.0989351 -2.3068082 -1.0363567 0.65609312 2.29886 3.5617657 3.7863245 2.8130274 1.3073573 -0.38834 -1.9155629 -3.1889262 -3.9417384 -4.1946211 -3.9708741][-4.2752986 -4.0907497 -3.2349334 -1.6434085 0.14926624 1.5575151 2.0356884 1.4269333 0.10885239 -1.5889373 -3.0763335 -4.0229368 -4.4909248 -4.4185953 -3.8908885][-5.6893263 -5.5968118 -4.9514117 -3.8583541 -2.480437 -1.0958903 -0.32628393 -0.66933155 -1.6751041 -3.0953748 -4.3763857 -4.9915833 -5.0403228 -4.6005769 -3.853359][-6.1659913 -6.5768261 -6.4501867 -5.5859423 -4.4331608 -3.3906446 -2.6605537 -2.7145195 -3.3842432 -4.327466 -5.1824875 -5.6332235 -5.4480696 -4.7829223 -3.7908731][-5.99314 -6.5252657 -6.4879007 -5.8858094 -5.0275011 -4.1903362 -3.6009059 -3.4856024 -3.8924005 -4.6141071 -5.2781825 -5.5628715 -5.1355419 -4.247983 -3.1942706][-5.2053795 -5.6626973 -5.5992527 -5.1027851 -4.4284015 -3.7032778 -3.2788982 -3.3283091 -3.7548831 -4.2375751 -4.636 -4.8446527 -4.4700718 -3.6314905 -2.6868169][-4.3925629 -4.760416 -4.6661792 -4.2210269 -3.6034141 -3.0383337 -2.764066 -2.8072681 -3.1627192 -3.6822705 -4.182363 -4.251802 -3.6660197 -2.7956209 -1.9174414]]...]
INFO - root - 2017-12-16 05:36:28.537199: step 24710, loss = 0.21, batch loss = 0.15 (28.1 examples/sec; 0.284 sec/batch; 24h:18m:08s remains)
INFO - root - 2017-12-16 05:36:31.369203: step 24720, loss = 0.27, batch loss = 0.21 (29.6 examples/sec; 0.271 sec/batch; 23h:08m:34s remains)
INFO - root - 2017-12-16 05:36:34.208159: step 24730, loss = 0.20, batch loss = 0.15 (27.4 examples/sec; 0.292 sec/batch; 24h:58m:30s remains)
INFO - root - 2017-12-16 05:36:37.033071: step 24740, loss = 0.25, batch loss = 0.19 (27.6 examples/sec; 0.290 sec/batch; 24h:45m:43s remains)
INFO - root - 2017-12-16 05:36:39.874021: step 24750, loss = 0.20, batch loss = 0.14 (27.9 examples/sec; 0.287 sec/batch; 24h:30m:50s remains)
INFO - root - 2017-12-16 05:36:42.653933: step 24760, loss = 0.27, batch loss = 0.21 (28.6 examples/sec; 0.280 sec/batch; 23h:55m:52s remains)
INFO - root - 2017-12-16 05:36:45.495649: step 24770, loss = 0.25, batch loss = 0.19 (28.5 examples/sec; 0.281 sec/batch; 23h:58m:52s remains)
INFO - root - 2017-12-16 05:36:48.335835: step 24780, loss = 0.33, batch loss = 0.27 (28.4 examples/sec; 0.281 sec/batch; 24h:02m:18s remains)
INFO - root - 2017-12-16 05:36:51.199611: step 24790, loss = 0.33, batch loss = 0.27 (28.8 examples/sec; 0.277 sec/batch; 23h:42m:33s remains)
INFO - root - 2017-12-16 05:36:54.022977: step 24800, loss = 0.25, batch loss = 0.19 (28.8 examples/sec; 0.278 sec/batch; 23h:44m:47s remains)
2017-12-16 05:36:54.485041: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8638272 -3.3692298 -3.863687 -4.3324294 -4.7613392 -5.1383619 -5.630208 -6.3588033 -7.0157881 -7.1576867 -6.8839264 -6.4610085 -5.6982069 -4.6236053 -3.7057388][-3.7070074 -4.3077536 -4.715086 -5.0539818 -5.3827643 -5.8612761 -6.3199081 -6.8670816 -7.4767761 -7.9171944 -7.9716139 -7.4236259 -6.6251497 -5.6802645 -4.6727262][-4.4798322 -5.2034764 -5.6490622 -5.6926389 -5.5437641 -5.4971566 -5.7018075 -6.2963133 -6.8990068 -7.2632103 -7.4921236 -7.4188342 -7.0881138 -6.3656521 -5.4551725][-5.2195034 -5.7370815 -6.0014839 -5.8544583 -5.3657932 -4.7429342 -4.4314132 -4.3706179 -4.5465465 -5.1246009 -5.8353224 -6.4534039 -6.8040047 -6.494524 -5.6995912][-5.623785 -5.7857466 -5.5446038 -4.8412461 -3.8088958 -2.6656914 -1.7051215 -1.1833997 -1.380425 -2.27361 -3.4629683 -4.7436914 -5.889183 -6.3262744 -5.9075685][-5.1167979 -5.0317926 -4.4512625 -3.3550353 -1.819391 -0.24517012 1.3618793 2.5181518 2.5521965 1.3313489 -0.727721 -2.9496648 -4.8181539 -5.7131019 -5.6145163][-4.3417072 -4.17486 -3.3387873 -1.8263509 0.025032043 1.9161344 3.7605639 5.064415 5.1718731 3.962 1.6504703 -1.021023 -3.3508041 -4.8328576 -5.1439991][-3.5716531 -3.1883616 -2.5071669 -0.9306941 1.2586713 3.5036526 5.28224 6.1345778 6.025712 4.7082796 2.3227291 -0.55845952 -2.9885225 -4.4184642 -4.8495669][-3.0172954 -2.487493 -1.5368032 -0.1670866 1.5065985 3.3233476 4.7403259 5.3354359 5.0397472 3.5455303 1.3263826 -1.1841509 -3.5141838 -4.8620195 -5.1483636][-3.2198877 -2.6429477 -1.7942061 -0.49086332 0.80315971 2.1165009 3.0601463 3.2451582 2.7677979 1.3519268 -0.61870909 -2.6500583 -4.2784805 -5.3002176 -5.4903927][-4.1712909 -3.4744072 -2.6500738 -1.6459942 -0.4817214 0.28238058 0.72016668 0.60661888 -0.15172434 -1.3182104 -2.6934409 -4.251965 -5.3411579 -5.6996374 -5.4587126][-4.8405871 -4.218442 -3.4552286 -2.6773467 -1.7167017 -1.2122047 -1.3213844 -1.9116943 -2.7579494 -3.7529633 -4.8127785 -5.6789813 -6.0800767 -5.82938 -5.2654347][-5.9302998 -5.128931 -4.1366849 -3.0826273 -2.454792 -2.1256738 -2.2047808 -3.1022091 -4.2306757 -5.080513 -5.6932292 -6.2354326 -6.3440905 -5.7086577 -4.7716541][-6.639997 -5.712522 -4.5014014 -3.2408028 -2.4427912 -2.2038791 -2.3134437 -3.102421 -4.1347513 -5.2087655 -5.7765951 -5.8446403 -5.6397047 -4.9983072 -4.0870619][-6.3986387 -5.4153123 -4.0979719 -2.7825389 -1.7221458 -1.3399529 -1.4528406 -2.20335 -3.2867622 -4.3383455 -4.9251728 -5.087348 -4.9863796 -4.212522 -3.3056822]]...]
INFO - root - 2017-12-16 05:36:57.293336: step 24810, loss = 0.40, batch loss = 0.34 (28.0 examples/sec; 0.286 sec/batch; 24h:25m:32s remains)
INFO - root - 2017-12-16 05:37:00.121314: step 24820, loss = 0.21, batch loss = 0.15 (28.3 examples/sec; 0.282 sec/batch; 24h:08m:16s remains)
INFO - root - 2017-12-16 05:37:02.964423: step 24830, loss = 0.25, batch loss = 0.20 (26.1 examples/sec; 0.307 sec/batch; 26h:12m:04s remains)
INFO - root - 2017-12-16 05:37:05.763035: step 24840, loss = 0.20, batch loss = 0.14 (28.4 examples/sec; 0.282 sec/batch; 24h:03m:53s remains)
INFO - root - 2017-12-16 05:37:08.592379: step 24850, loss = 0.28, batch loss = 0.22 (29.9 examples/sec; 0.268 sec/batch; 22h:53m:53s remains)
INFO - root - 2017-12-16 05:37:11.431254: step 24860, loss = 0.21, batch loss = 0.15 (28.1 examples/sec; 0.285 sec/batch; 24h:19m:34s remains)
INFO - root - 2017-12-16 05:37:14.248130: step 24870, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.283 sec/batch; 24h:13m:14s remains)
INFO - root - 2017-12-16 05:37:17.096001: step 24880, loss = 0.21, batch loss = 0.15 (27.0 examples/sec; 0.297 sec/batch; 25h:21m:34s remains)
INFO - root - 2017-12-16 05:37:19.944735: step 24890, loss = 0.25, batch loss = 0.19 (28.3 examples/sec; 0.283 sec/batch; 24h:10m:39s remains)
INFO - root - 2017-12-16 05:37:22.735741: step 24900, loss = 0.29, batch loss = 0.24 (29.5 examples/sec; 0.271 sec/batch; 23h:09m:49s remains)
2017-12-16 05:37:23.215418: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.6475182 -6.4123712 -5.9994831 -5.9326859 -5.8979397 -6.22904 -6.6264167 -7.3225632 -7.8649969 -8.1802273 -8.3829231 -8.1266174 -7.481102 -6.4549265 -5.4753451][-6.7954636 -6.4778624 -6.0861883 -5.7983532 -5.6101437 -5.9666624 -6.6003313 -7.4777675 -8.1540213 -8.5649118 -8.85186 -8.5555744 -7.8792524 -6.83312 -5.7268457][-6.1635122 -5.51497 -4.8524027 -4.5057092 -4.2856274 -4.5061994 -5.0467272 -6.2554297 -7.262023 -7.7478561 -8.0497379 -8.0112562 -7.6034222 -6.588129 -5.6849241][-4.631855 -3.8882668 -2.986661 -2.4175322 -2.1196179 -2.2381792 -2.6708984 -3.8652809 -4.9549484 -5.6375637 -6.2074738 -6.3401837 -6.2593775 -5.7427177 -5.1932836][-3.1315031 -2.0388129 -0.77249241 -0.15876675 0.14374876 0.17923927 -0.3206768 -1.3434143 -2.2480631 -2.9876754 -3.7382343 -4.283937 -4.7788138 -4.7470918 -4.623847][-2.1439993 -0.8044045 0.72997665 1.9698935 2.5314569 2.380795 2.153286 1.2998977 0.39804983 -0.17275095 -1.0440719 -2.0070765 -3.2215536 -3.9237571 -4.1717224][-2.4870353 -0.90553379 0.83778811 2.2003732 3.2263789 3.4947882 3.3740478 2.7826829 2.1007285 1.4022555 0.45297718 -0.58809328 -1.9440327 -2.932848 -3.468297][-3.0600276 -1.832695 -0.12121868 1.4537411 2.69554 3.1323318 3.3637404 3.1666174 2.7497406 2.0371552 0.909997 -0.33390617 -1.6424494 -2.5966406 -3.1897421][-4.3974128 -3.290484 -1.8864269 -0.50043058 0.8452692 1.7321339 2.1582861 2.2053733 2.2902341 1.6411319 0.49929953 -0.63570476 -2.0630972 -2.9628875 -3.3746552][-5.9091387 -5.2098384 -4.174829 -3.0246363 -1.7228746 -0.65108132 0.17955112 0.43721342 0.48108816 0.12337875 -0.75931382 -1.8327737 -2.7859676 -3.5063243 -3.9613261][-6.4878674 -6.0055 -5.3781013 -4.5602136 -3.5916314 -2.7430444 -1.6826463 -1.2501991 -1.0867832 -1.2978523 -1.9765999 -2.8238583 -3.5661931 -4.04639 -4.3049831][-6.4981861 -6.2582612 -5.7744589 -5.2127423 -4.5735035 -3.9134135 -3.125206 -2.7224274 -2.3820562 -2.411315 -2.6704333 -3.4659691 -4.2278681 -4.3912306 -4.4452438][-5.229599 -5.215992 -5.1484938 -4.8076897 -4.3422513 -3.9840436 -3.4293258 -3.2139916 -2.9868956 -2.9475822 -3.1153626 -3.4155195 -3.9850955 -4.2800593 -4.2831669][-3.6901853 -3.6485524 -3.6182117 -3.4507706 -3.3112741 -3.0676622 -2.8204305 -2.778913 -2.6684334 -2.6946964 -2.8328104 -3.0274434 -3.2891653 -3.3945441 -3.4475727][-2.5045726 -2.2775605 -2.1546578 -2.1605127 -2.1483586 -2.0954344 -2.0560062 -1.9790394 -1.895956 -1.9442403 -2.1086414 -2.2613835 -2.39612 -2.5543516 -2.6104686]]...]
INFO - root - 2017-12-16 05:37:26.049272: step 24910, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 24h:11m:03s remains)
INFO - root - 2017-12-16 05:37:28.893157: step 24920, loss = 0.20, batch loss = 0.14 (28.3 examples/sec; 0.282 sec/batch; 24h:07m:25s remains)
INFO - root - 2017-12-16 05:37:31.719361: step 24930, loss = 0.27, batch loss = 0.21 (28.8 examples/sec; 0.277 sec/batch; 23h:42m:22s remains)
INFO - root - 2017-12-16 05:37:34.527138: step 24940, loss = 0.27, batch loss = 0.21 (28.3 examples/sec; 0.283 sec/batch; 24h:09m:10s remains)
INFO - root - 2017-12-16 05:37:37.304030: step 24950, loss = 0.29, batch loss = 0.23 (28.2 examples/sec; 0.283 sec/batch; 24h:12m:06s remains)
INFO - root - 2017-12-16 05:37:40.137713: step 24960, loss = 0.31, batch loss = 0.25 (28.4 examples/sec; 0.281 sec/batch; 24h:01m:41s remains)
INFO - root - 2017-12-16 05:37:42.939951: step 24970, loss = 0.24, batch loss = 0.18 (27.6 examples/sec; 0.290 sec/batch; 24h:48m:01s remains)
INFO - root - 2017-12-16 05:37:45.844347: step 24980, loss = 0.42, batch loss = 0.36 (26.9 examples/sec; 0.297 sec/batch; 25h:22m:01s remains)
INFO - root - 2017-12-16 05:37:48.667382: step 24990, loss = 0.25, batch loss = 0.19 (29.6 examples/sec; 0.271 sec/batch; 23h:06m:24s remains)
INFO - root - 2017-12-16 05:37:51.456988: step 25000, loss = 0.22, batch loss = 0.16 (27.9 examples/sec; 0.286 sec/batch; 24h:27m:32s remains)
2017-12-16 05:37:51.925929: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7640314 -2.3692551 -2.5083194 -2.8861785 -3.3374972 -3.7961226 -4.0856371 -4.0912261 -4.205502 -4.5231404 -4.6250291 -4.3070326 -4.0054293 -3.905535 -4.1305575][-2.2521384 -1.903547 -1.8424964 -2.1944447 -2.5582395 -3.1751657 -3.2639673 -3.3318491 -3.4264054 -3.4702492 -3.4516897 -3.3190031 -3.3252754 -3.1294947 -3.1058116][-1.5957584 -1.1400361 -1.3665919 -1.8707473 -2.376847 -2.8421445 -2.771101 -2.6131725 -2.4098637 -2.5516706 -2.6754675 -2.4262884 -2.213006 -2.2208519 -2.3017385][-0.81089377 -0.45764518 -0.75909376 -1.3594923 -1.980433 -2.3410199 -2.0101831 -1.5607049 -1.5872126 -1.7316067 -1.6762185 -1.62797 -1.8396537 -2.0010753 -1.9233258][-0.8203485 -0.15429497 -0.24460649 -0.99134207 -1.6278605 -1.6831326 -1.2902045 -0.88683581 -0.60379815 -0.86453319 -1.2578328 -1.4088821 -1.4270782 -1.5766447 -1.8609693][-1.3260002 -0.56471014 -0.611382 -1.036134 -1.4390869 -1.499505 -1.0653908 -0.42496109 -0.11982059 -0.26944828 -0.59050846 -0.80064678 -1.0948825 -1.3643098 -1.5549412][-2.4473228 -1.2314911 -0.98059273 -1.4313488 -1.8327641 -1.5618172 -0.71626091 -0.083645821 0.16401529 0.07103014 -0.30930471 -0.78710675 -1.2479482 -1.4131188 -1.8512521][-3.3832622 -2.3643069 -2.0498762 -2.1562774 -2.1957018 -1.5831423 -0.47349072 0.39395571 0.68797874 0.41052723 -0.12018061 -0.80261874 -1.340651 -1.7890007 -2.2652109][-4.5732665 -3.7482491 -3.3130274 -3.0695791 -3.0613077 -2.2730334 -0.98941231 -0.10780191 0.23232841 -0.065512657 -0.65067148 -1.3713076 -2.0275507 -2.4204915 -2.9277487][-5.3071146 -4.8892031 -4.9641771 -4.6330767 -4.2924356 -3.3748345 -2.1172767 -1.2531033 -0.83029079 -1.068491 -1.6621575 -2.3600111 -2.8356094 -2.9973483 -3.4845738][-6.1167464 -5.95984 -6.1074276 -5.9508362 -5.5169125 -4.4623818 -3.1787786 -2.3294573 -1.9803586 -2.2021315 -2.7689548 -3.1686363 -3.5077317 -3.7198138 -4.1207113][-6.025681 -6.0966783 -6.389164 -6.4461432 -6.121675 -5.3080783 -4.2415314 -3.538655 -3.1870878 -3.160645 -3.4658322 -3.875005 -4.055243 -3.8823063 -4.0751758][-6.025773 -5.6745477 -5.9228587 -6.1845217 -6.2313161 -5.7805743 -5.0798941 -4.6804185 -4.4049525 -4.3291368 -4.3681893 -4.2980962 -4.2889962 -4.0487108 -3.9436996][-5.8717628 -5.2850118 -5.273417 -5.4102659 -5.60776 -5.6272974 -5.3557534 -5.0664287 -4.9374037 -4.8851423 -4.8184276 -4.5802822 -4.4467497 -4.1244617 -3.7525601][-6.0074143 -5.1480212 -4.7383003 -4.8398604 -5.0804343 -5.2653146 -5.1978588 -5.1518693 -5.1017995 -4.900166 -4.7318068 -4.39509 -4.1743126 -3.8516929 -3.6837807]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-25000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-25000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 05:37:55.160613: step 25010, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.285 sec/batch; 24h:22m:26s remains)
INFO - root - 2017-12-16 05:37:57.973936: step 25020, loss = 0.24, batch loss = 0.19 (27.4 examples/sec; 0.292 sec/batch; 24h:58m:01s remains)
INFO - root - 2017-12-16 05:38:00.863548: step 25030, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.284 sec/batch; 24h:13m:14s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 05:38:03.701822: step 25040, loss = 0.23, batch loss = 0.17 (27.6 examples/sec; 0.290 sec/batch; 24h:44m:37s remains)
INFO - root - 2017-12-16 05:38:06.487514: step 25050, loss = 0.24, batch loss = 0.18 (27.9 examples/sec; 0.286 sec/batch; 24h:27m:22s remains)
INFO - root - 2017-12-16 05:38:09.327879: step 25060, loss = 0.29, batch loss = 0.23 (27.2 examples/sec; 0.295 sec/batch; 25h:09m:03s remains)
INFO - root - 2017-12-16 05:38:12.163235: step 25070, loss = 0.27, batch loss = 0.21 (29.2 examples/sec; 0.274 sec/batch; 23h:24m:20s remains)
INFO - root - 2017-12-16 05:38:15.020492: step 25080, loss = 0.25, batch loss = 0.19 (29.0 examples/sec; 0.276 sec/batch; 23h:34m:31s remains)
INFO - root - 2017-12-16 05:38:17.847738: step 25090, loss = 0.27, batch loss = 0.21 (29.5 examples/sec; 0.271 sec/batch; 23h:09m:08s remains)
INFO - root - 2017-12-16 05:38:20.695391: step 25100, loss = 0.26, batch loss = 0.20 (27.4 examples/sec; 0.292 sec/batch; 24h:57m:53s remains)
2017-12-16 05:38:21.174826: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4873543 -2.9010744 -2.6153703 -2.5193164 -2.7257223 -3.1410637 -3.3895221 -3.4316311 -3.5260935 -3.5424113 -3.6072776 -3.4932475 -3.3994017 -3.1978989 -2.8947277][-3.5712216 -2.890017 -2.3799393 -2.3361874 -2.5254319 -2.8476505 -3.004591 -3.0458753 -3.2299919 -3.3665481 -3.5276756 -3.3977268 -3.2979498 -2.9228578 -2.4496498][-3.0351777 -2.4904242 -2.0795064 -1.9525721 -1.9720154 -2.3554714 -2.5147963 -2.5412791 -2.6600952 -2.9272354 -3.3753681 -3.3436761 -3.2397604 -2.811296 -2.4124217][-2.7451332 -2.1753781 -1.7238996 -1.5043147 -1.3806779 -1.536963 -1.593853 -1.7157743 -2.0400257 -2.4822731 -2.9532521 -3.2540932 -3.3737841 -2.9313941 -2.5445533][-2.1678085 -1.6020186 -1.073633 -0.8965807 -0.67688632 -0.58951879 -0.49929261 -0.57729721 -0.9452765 -1.7465715 -2.7227981 -3.2418704 -3.4709091 -3.2058194 -2.8272343][-1.8051178 -1.1772876 -0.55165005 -0.10160875 0.51543379 0.84722471 1.0168653 0.84602308 0.15845108 -0.96100116 -2.29716 -3.2475455 -3.6790206 -3.5313635 -3.1495018][-1.7225547 -1.2639749 -0.652802 0.054988861 0.95161915 1.6821504 2.143539 2.1287017 1.3734589 -0.012615204 -1.7016969 -3.0688934 -3.7144122 -3.5420442 -2.9627128][-1.721097 -1.3598328 -0.95480108 -0.30658865 0.68919945 1.6414003 2.1772819 2.1230927 1.4000478 -0.087353706 -1.9219024 -3.3705249 -4.1346784 -4.0248365 -3.4982922][-2.0905485 -1.8727429 -1.5392094 -0.99743247 -0.14406109 0.83557415 1.4982219 1.4751234 0.65026474 -0.80781031 -2.5934691 -4.1334352 -4.8925552 -4.5987654 -3.7374768][-2.3719125 -2.3455684 -2.4071262 -2.0501425 -1.4044065 -0.65414762 -0.11069345 -0.0074362755 -0.52930617 -1.7989571 -3.3915176 -4.713603 -5.316339 -5.0087366 -4.254118][-2.5631213 -2.661711 -2.7745945 -2.7515149 -2.4681406 -1.8906975 -1.5933013 -1.700779 -2.1432219 -3.1129127 -4.2585969 -5.2545371 -5.6960812 -5.1023722 -4.170414][-2.5041811 -2.65589 -3.0252643 -3.2314939 -3.2679553 -2.9855919 -2.8914194 -3.1538444 -3.6855547 -4.3147564 -5.08572 -5.8224134 -6.0486145 -5.3487835 -4.1504517][-2.3698046 -2.3574417 -2.642478 -3.1535919 -3.4601932 -3.5611582 -3.8688531 -4.2148728 -4.7309947 -5.2446117 -5.7521744 -6.2962141 -6.415297 -5.8495684 -4.8608541][-2.335284 -2.217072 -2.3049686 -2.7021205 -3.0424421 -3.3256376 -3.7087955 -4.2774587 -5.0910931 -5.6079197 -6.0644407 -6.4964595 -6.42965 -5.9197059 -4.8889332][-2.4108284 -2.0025456 -1.8049841 -2.0783732 -2.3603787 -2.6757991 -3.3256481 -4.0092006 -4.9178658 -5.5871139 -6.1351438 -6.6647749 -6.8242197 -6.391366 -5.1464095]]...]
INFO - root - 2017-12-16 05:38:24.059408: step 25110, loss = 0.22, batch loss = 0.16 (26.7 examples/sec; 0.300 sec/batch; 25h:36m:48s remains)
INFO - root - 2017-12-16 05:38:26.855468: step 25120, loss = 0.26, batch loss = 0.20 (27.3 examples/sec; 0.293 sec/batch; 25h:01m:09s remains)
INFO - root - 2017-12-16 05:38:29.723475: step 25130, loss = 0.25, batch loss = 0.19 (27.9 examples/sec; 0.287 sec/batch; 24h:30m:58s remains)
INFO - root - 2017-12-16 05:38:32.518753: step 25140, loss = 0.19, batch loss = 0.13 (28.9 examples/sec; 0.277 sec/batch; 23h:37m:04s remains)
INFO - root - 2017-12-16 05:38:35.370797: step 25150, loss = 0.23, batch loss = 0.17 (29.0 examples/sec; 0.276 sec/batch; 23h:34m:23s remains)
INFO - root - 2017-12-16 05:38:38.200366: step 25160, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.278 sec/batch; 23h:45m:57s remains)
INFO - root - 2017-12-16 05:38:41.025804: step 25170, loss = 0.28, batch loss = 0.22 (28.5 examples/sec; 0.280 sec/batch; 23h:55m:52s remains)
INFO - root - 2017-12-16 05:38:43.868555: step 25180, loss = 0.28, batch loss = 0.22 (27.5 examples/sec; 0.291 sec/batch; 24h:48m:54s remains)
INFO - root - 2017-12-16 05:38:46.658783: step 25190, loss = 0.42, batch loss = 0.36 (28.6 examples/sec; 0.280 sec/batch; 23h:53m:55s remains)
INFO - root - 2017-12-16 05:38:49.470661: step 25200, loss = 0.34, batch loss = 0.28 (28.0 examples/sec; 0.285 sec/batch; 24h:21m:54s remains)
2017-12-16 05:38:49.960822: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3871994 -4.8518891 -5.2208471 -5.4127388 -5.5164766 -5.4657836 -5.4337068 -5.4849367 -5.7675052 -6.2747555 -6.6844625 -7.1783981 -7.2840872 -6.6947765 -5.8018904][-5.1984572 -5.6900096 -5.9424372 -6.0129714 -6.051331 -6.212492 -6.4094791 -6.5374002 -6.8692789 -7.5473986 -8.4069862 -9.1710243 -9.394186 -8.8964586 -7.7733345][-5.6175003 -6.1322536 -6.2662983 -6.0685 -5.7804832 -5.6691165 -5.7032986 -6.3258162 -7.1985946 -8.0263081 -9.0806017 -10.145782 -10.861746 -10.681757 -9.6185465][-5.813735 -6.1174345 -5.9662628 -5.3888454 -4.7605886 -4.2531137 -4.0631375 -4.4160824 -5.18954 -6.8035297 -8.52596 -9.8912849 -10.949266 -11.391338 -10.856789][-5.2846971 -5.33839 -4.8682384 -3.9221342 -2.7523673 -1.54515 -0.89203763 -1.0417488 -1.8610737 -3.7245743 -5.9205132 -8.24986 -10.120764 -11.176668 -11.137098][-4.4984756 -4.2977543 -3.4797614 -1.9163406 0.062439442 1.610086 2.548131 2.7855978 2.1582632 0.13983107 -2.6752625 -5.802114 -8.4585171 -10.05625 -10.465765][-4.2735252 -3.6977177 -2.5218275 -0.88258624 1.6121583 4.1275063 5.9093733 6.1463823 5.2213297 3.3156757 0.28977394 -3.4238095 -6.6170082 -8.5456314 -9.0298176][-4.5812378 -3.9168549 -2.7404361 -0.67658114 1.9472976 4.5314341 6.8720961 7.7161055 6.9419622 4.7067842 1.4921012 -2.1391385 -5.3248835 -7.2577095 -7.6574192][-4.9219074 -4.732882 -3.6341507 -1.7050273 0.5379262 3.245295 5.6518755 6.528244 6.0967979 4.302701 1.422462 -1.8768382 -4.7449827 -6.4826417 -6.7643046][-4.9719744 -5.4182043 -5.0826011 -3.533386 -1.6231585 0.49185038 2.4284616 3.6605492 3.6406269 2.0399246 -0.11473227 -2.4993291 -4.868896 -6.4375019 -6.8721666][-5.8763466 -6.2414265 -6.1020112 -5.2274609 -3.8468623 -2.3445454 -1.1109357 -0.33228827 -0.22935486 -1.0250335 -2.5371456 -4.2503037 -5.6391535 -6.5534234 -6.942194][-6.3080406 -6.6619205 -6.8524175 -6.4258289 -5.4479837 -4.3951507 -3.5749443 -3.1463416 -3.2006278 -3.7624471 -4.6992145 -5.8354912 -6.724782 -7.1266246 -7.1173081][-5.835114 -6.2519107 -6.5561905 -6.47935 -6.2967606 -5.704742 -4.9152102 -4.5871024 -4.5864968 -4.7823224 -5.381115 -6.2189908 -6.7226276 -7.0860963 -7.0611982][-5.1696486 -5.3433323 -5.4596763 -5.5599632 -5.6757636 -5.5376329 -5.39739 -5.2858815 -5.2174826 -5.3269477 -5.3379011 -5.5032754 -5.9076705 -6.1121173 -6.0492883][-4.4522476 -4.4110765 -4.3061929 -4.3148828 -4.4521861 -4.5370555 -4.606905 -4.6146741 -4.6366467 -4.7404256 -4.8569961 -4.7351441 -4.5202346 -4.5518603 -4.6266131]]...]
INFO - root - 2017-12-16 05:38:52.805391: step 25210, loss = 0.30, batch loss = 0.24 (28.5 examples/sec; 0.281 sec/batch; 23h:59m:45s remains)
INFO - root - 2017-12-16 05:38:55.683245: step 25220, loss = 0.27, batch loss = 0.21 (27.3 examples/sec; 0.293 sec/batch; 25h:01m:36s remains)
INFO - root - 2017-12-16 05:38:58.531417: step 25230, loss = 0.20, batch loss = 0.15 (27.0 examples/sec; 0.296 sec/batch; 25h:15m:38s remains)
INFO - root - 2017-12-16 05:39:01.374813: step 25240, loss = 0.24, batch loss = 0.18 (27.8 examples/sec; 0.288 sec/batch; 24h:34m:16s remains)
INFO - root - 2017-12-16 05:39:04.218963: step 25250, loss = 0.30, batch loss = 0.24 (28.5 examples/sec; 0.281 sec/batch; 23h:58m:53s remains)
INFO - root - 2017-12-16 05:39:07.057918: step 25260, loss = 0.20, batch loss = 0.14 (28.2 examples/sec; 0.284 sec/batch; 24h:12m:37s remains)
INFO - root - 2017-12-16 05:39:09.857299: step 25270, loss = 0.33, batch loss = 0.27 (29.1 examples/sec; 0.275 sec/batch; 23h:29m:56s remains)
INFO - root - 2017-12-16 05:39:12.714298: step 25280, loss = 0.21, batch loss = 0.15 (28.8 examples/sec; 0.278 sec/batch; 23h:43m:53s remains)
INFO - root - 2017-12-16 05:39:15.569459: step 25290, loss = 0.29, batch loss = 0.23 (27.6 examples/sec; 0.290 sec/batch; 24h:42m:58s remains)
INFO - root - 2017-12-16 05:39:18.368274: step 25300, loss = 0.22, batch loss = 0.16 (29.3 examples/sec; 0.273 sec/batch; 23h:20m:01s remains)
2017-12-16 05:39:18.833369: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7117062 -4.3568 -4.0936279 -3.6309323 -3.496172 -3.22219 -3.146297 -3.1401036 -3.1873763 -3.4571412 -3.8489769 -4.2755904 -4.4219408 -4.4577155 -4.2253904][-5.7112932 -5.1336107 -4.7507486 -4.3112726 -4.1401429 -3.858928 -3.8265789 -3.8399997 -3.8034251 -4.0510635 -4.2486153 -4.4893336 -4.5461373 -4.4266887 -4.152935][-5.5297322 -5.0627127 -4.7580433 -4.3172994 -4.2120652 -4.042532 -4.0133152 -4.01336 -4.1382971 -4.3252258 -4.52758 -4.8806505 -5.0179071 -4.9144726 -4.5438437][-4.6876416 -4.1055765 -3.8888538 -3.7822118 -3.8063369 -3.6937528 -3.7167871 -3.72133 -3.8800895 -4.1137571 -4.367229 -4.8327079 -5.036653 -5.1185551 -4.9238582][-2.9809623 -2.4616082 -2.3858533 -2.5301142 -2.6479387 -2.5434632 -2.4738426 -2.5894551 -2.786787 -3.2047043 -3.7814231 -4.5285869 -4.9658284 -5.3187442 -5.09214][-0.65188 -0.25643587 -0.58051848 -0.99785328 -1.2362621 -1.2143302 -1.027946 -1.0019417 -1.0857804 -1.5914595 -2.4148161 -3.6289124 -4.629354 -5.1957817 -5.1678033][1.1602468 1.4547176 0.86196423 0.065610409 -0.35603476 -0.11613321 0.38349009 0.69424772 0.75393724 0.16407156 -0.8819921 -2.3336008 -3.6774776 -4.59298 -4.7054386][1.4028163 1.5128884 1.0466213 0.41473436 0.25989246 0.78628683 1.3992715 1.9526405 2.1378446 1.4909892 0.26400185 -1.4970965 -3.1482701 -4.1871371 -4.398519][0.12082195 0.42234468 0.0082688332 -0.439178 -0.17745495 0.67799091 1.5953503 2.4529333 2.7616777 2.2626514 1.0276227 -0.83561492 -2.6330292 -3.8453741 -4.2193494][-1.7881105 -1.6865728 -1.8812068 -1.9610453 -1.4766896 -0.33478832 0.79736519 1.8897343 2.3159337 1.8743277 0.71190262 -0.92874384 -2.6367245 -3.9366052 -4.5182724][-3.4848728 -3.5978956 -3.7969162 -3.7336774 -3.2875309 -1.7894959 -0.43414617 0.64338636 1.0933623 0.85452414 -0.16163778 -1.6917658 -3.2514277 -4.4564018 -5.1200509][-5.1821346 -5.414762 -5.6034708 -5.4498043 -4.8745546 -3.4445305 -2.1094568 -0.89826536 -0.3969574 -0.56638789 -1.3923848 -2.6481853 -3.9306226 -4.975533 -5.459816][-6.1977911 -6.4777288 -6.7985845 -6.8918209 -6.6178427 -5.5626736 -4.4099269 -3.2151217 -2.6384354 -2.6814172 -3.1430607 -4.0700831 -5.0247574 -5.6816163 -5.8535891][-6.0702586 -6.458097 -6.9743657 -7.3125648 -7.290452 -6.6081667 -5.6270776 -4.6073532 -4.0421734 -3.9204912 -4.2480364 -4.9143205 -5.5500488 -6.0291758 -6.0072045][-5.8874969 -6.4232759 -6.9749212 -7.3403263 -7.4088249 -6.9635038 -6.0925479 -4.9030266 -4.1688881 -4.1176186 -4.3458056 -4.7752829 -5.1966181 -5.7103739 -5.7091646]]...]
INFO - root - 2017-12-16 05:39:21.636573: step 25310, loss = 0.30, batch loss = 0.24 (27.7 examples/sec; 0.288 sec/batch; 24h:36m:03s remains)
INFO - root - 2017-12-16 05:39:24.526782: step 25320, loss = 0.23, batch loss = 0.17 (27.9 examples/sec; 0.287 sec/batch; 24h:27m:49s remains)
INFO - root - 2017-12-16 05:39:27.355828: step 25330, loss = 0.22, batch loss = 0.16 (28.9 examples/sec; 0.277 sec/batch; 23h:39m:34s remains)
INFO - root - 2017-12-16 05:39:30.182276: step 25340, loss = 0.26, batch loss = 0.20 (29.0 examples/sec; 0.276 sec/batch; 23h:30m:32s remains)
INFO - root - 2017-12-16 05:39:32.986577: step 25350, loss = 0.29, batch loss = 0.23 (28.1 examples/sec; 0.285 sec/batch; 24h:17m:14s remains)
INFO - root - 2017-12-16 05:39:35.853577: step 25360, loss = 0.27, batch loss = 0.21 (27.2 examples/sec; 0.295 sec/batch; 25h:07m:47s remains)
INFO - root - 2017-12-16 05:39:38.705191: step 25370, loss = 0.21, batch loss = 0.15 (28.2 examples/sec; 0.284 sec/batch; 24h:11m:59s remains)
INFO - root - 2017-12-16 05:39:41.528785: step 25380, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.282 sec/batch; 24h:04m:08s remains)
INFO - root - 2017-12-16 05:39:44.355766: step 25390, loss = 0.32, batch loss = 0.26 (27.6 examples/sec; 0.289 sec/batch; 24h:41m:26s remains)
INFO - root - 2017-12-16 05:39:47.188188: step 25400, loss = 0.29, batch loss = 0.23 (29.0 examples/sec; 0.276 sec/batch; 23h:31m:25s remains)
2017-12-16 05:39:47.643069: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9319863 -1.84499 -2.061096 -2.2599206 -2.4872499 -2.7034411 -2.9986422 -3.1115909 -3.05567 -3.0018668 -2.9775586 -3.2715116 -3.6551793 -4.043581 -4.7428794][-2.4471753 -2.56355 -2.8409534 -3.2368488 -3.6860127 -3.8089147 -3.9775045 -4.1316953 -4.2858429 -4.2512932 -4.1490474 -4.2484941 -4.4027009 -4.8713174 -5.3286066][-4.0276289 -4.2067523 -4.3532591 -4.4990273 -4.7124248 -4.8205991 -5.0292845 -5.0652218 -5.223834 -5.4277067 -5.5595202 -5.581953 -5.6504874 -5.7718372 -5.8760643][-5.4280844 -5.6183476 -5.6306329 -5.5354538 -5.3222666 -4.9362063 -4.6760426 -4.6816349 -4.9478321 -5.1780953 -5.4670315 -5.6563315 -5.7896938 -5.8631835 -5.8221121][-6.2024889 -6.5289612 -6.5590353 -6.1036448 -5.3693185 -4.4282441 -3.650542 -3.1181498 -2.9181437 -3.2226725 -3.7414632 -4.3363252 -5.0035148 -5.3037314 -5.4381657][-6.2979336 -6.345767 -6.244998 -5.66113 -4.5407424 -3.1549511 -1.7806063 -0.86235666 -0.52049232 -0.71722722 -1.3838868 -2.2598226 -3.3356571 -4.2697549 -4.9905486][-5.3210545 -5.3092966 -5.0759745 -4.3251867 -3.0905623 -1.4423575 0.4024148 1.7209587 2.3903646 2.1184802 1.1796265 -0.1110568 -1.6815653 -3.0180061 -4.1222076][-3.7679958 -3.8402174 -3.6491303 -3.0611043 -1.936986 -0.25051403 1.8205147 3.3885555 4.3061485 4.2189665 3.2207036 1.620872 -0.26352692 -2.111975 -3.6289721][-2.0929875 -2.3868811 -2.5425267 -1.9688923 -1.0771332 0.21386003 1.8909745 3.3536134 4.3103027 4.2936268 3.3491983 1.7894793 -0.10464191 -2.0074375 -3.6403546][-2.3744214 -2.0582547 -1.706048 -1.4661095 -0.93521237 0.17585373 1.4471278 2.3423543 2.7775831 2.654911 1.7985196 0.42583179 -1.1177931 -2.6605513 -3.9761331][-2.9222646 -2.44552 -1.9170666 -1.4411962 -0.81977344 -0.024791241 0.70629263 1.1780186 1.3000741 0.98848057 0.141469 -1.022717 -2.2669377 -3.3180122 -4.1237831][-4.0901394 -3.2508426 -2.348042 -1.7962859 -1.2161038 -0.67958951 -0.26751804 0.019162655 -0.090632915 -0.51492167 -1.216084 -2.1182873 -3.0494008 -3.7315853 -4.1050038][-5.0950708 -4.1497545 -3.0413647 -2.0540664 -1.3884759 -1.0815945 -1.0569162 -1.0110183 -1.2438157 -1.6146076 -2.0777128 -2.5720415 -3.1243505 -3.3560038 -3.4610586][-6.0015 -4.9265509 -3.5389106 -2.2599349 -1.3356531 -0.91575313 -0.86387277 -0.85544157 -1.1794002 -1.3959999 -1.5631895 -1.8946202 -2.2950022 -2.4951775 -2.6284328][-5.8917084 -4.9358916 -3.6879461 -2.3726883 -1.4046102 -0.78676009 -0.53331327 -0.34897709 -0.29107523 -0.35851049 -0.581501 -0.74658918 -1.0441194 -1.2380095 -1.472532]]...]
INFO - root - 2017-12-16 05:39:50.440169: step 25410, loss = 0.22, batch loss = 0.16 (28.0 examples/sec; 0.286 sec/batch; 24h:22m:25s remains)
INFO - root - 2017-12-16 05:39:53.286927: step 25420, loss = 0.21, batch loss = 0.15 (28.1 examples/sec; 0.285 sec/batch; 24h:16m:18s remains)
INFO - root - 2017-12-16 05:39:56.130076: step 25430, loss = 0.32, batch loss = 0.26 (28.0 examples/sec; 0.286 sec/batch; 24h:22m:14s remains)
INFO - root - 2017-12-16 05:39:58.945279: step 25440, loss = 0.28, batch loss = 0.22 (28.9 examples/sec; 0.276 sec/batch; 23h:34m:21s remains)
INFO - root - 2017-12-16 05:40:01.840142: step 25450, loss = 0.25, batch loss = 0.19 (25.8 examples/sec; 0.310 sec/batch; 26h:28m:51s remains)
INFO - root - 2017-12-16 05:40:04.699174: step 25460, loss = 0.24, batch loss = 0.18 (27.7 examples/sec; 0.288 sec/batch; 24h:35m:53s remains)
INFO - root - 2017-12-16 05:40:07.542594: step 25470, loss = 0.33, batch loss = 0.27 (28.3 examples/sec; 0.283 sec/batch; 24h:06m:53s remains)
INFO - root - 2017-12-16 05:40:10.388693: step 25480, loss = 0.23, batch loss = 0.18 (29.3 examples/sec; 0.273 sec/batch; 23h:18m:24s remains)
INFO - root - 2017-12-16 05:40:13.248564: step 25490, loss = 0.31, batch loss = 0.25 (26.1 examples/sec; 0.307 sec/batch; 26h:08m:52s remains)
INFO - root - 2017-12-16 05:40:16.097153: step 25500, loss = 0.26, batch loss = 0.20 (27.7 examples/sec; 0.289 sec/batch; 24h:38m:13s remains)
2017-12-16 05:40:16.556231: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4040675 -3.3830473 -3.4083805 -3.5511498 -3.8431826 -4.1828279 -4.5796332 -5.0292034 -5.4793243 -5.7739191 -5.6445494 -5.4139977 -4.9265571 -4.1435108 -3.4017744][-3.4951487 -3.5750184 -3.6982112 -3.8627393 -4.1567726 -4.6160326 -5.1723108 -5.5848722 -5.8539391 -6.1271844 -6.3972149 -6.2732944 -5.65512 -4.8757138 -4.0529094][-3.4527407 -3.6950209 -3.9715233 -4.2098112 -4.4081826 -4.6646185 -4.9792652 -5.4848475 -5.9756174 -6.15832 -6.2304678 -6.2770071 -6.2571483 -5.8003554 -5.0159817][-3.5795276 -3.9962287 -4.3931646 -4.6378822 -4.6592669 -4.4602733 -4.1511145 -4.0440359 -4.0514073 -4.52882 -5.3210783 -5.8073244 -6.0518203 -5.9999986 -5.6091881][-3.6921496 -4.0805473 -4.2466736 -4.1605659 -3.5410993 -2.5250547 -1.4294727 -0.70137024 -0.68279457 -1.403686 -2.6218042 -4.2111783 -5.5149159 -5.930799 -5.7147055][-3.491044 -3.845892 -3.9131222 -3.5086322 -2.1665788 -0.15731382 1.8908567 3.1569667 3.2731171 2.0805721 -0.085404396 -2.4440517 -4.3288074 -5.4802151 -5.7163968][-3.1032739 -3.3947515 -3.2542272 -2.5318577 -0.93638062 1.5218706 4.2505522 5.9196539 6.0644026 4.7134628 2.3430576 -0.59598565 -3.2116375 -4.7699981 -5.1715684][-2.6605701 -2.7703617 -2.6568933 -1.8061633 0.1599617 2.8028603 5.43157 7.0115156 6.955164 5.262332 2.6727476 -0.045927048 -2.3548424 -3.9601886 -4.5971909][-2.4634955 -2.7668102 -2.578943 -1.7246852 -0.39574718 1.7022667 4.0370445 5.4589558 5.1924486 3.5066786 1.0141163 -1.4804461 -3.3999228 -4.4078836 -4.5033269][-2.2371011 -2.6833715 -3.0561323 -2.6423879 -1.5241065 -0.062403679 0.96782494 1.7443848 1.7914143 0.35485935 -1.633491 -3.4976418 -4.8246202 -5.3329649 -5.1121674][-2.6121612 -3.005563 -3.4011846 -3.5557013 -3.3696461 -2.5907245 -1.7579088 -1.5286667 -1.9789491 -2.8065763 -3.7915502 -4.9844522 -5.6255784 -5.5673442 -5.1382632][-3.1275697 -3.4282169 -3.8340397 -4.0341163 -4.0675693 -4.2094178 -4.4434805 -4.5518336 -4.6785092 -5.0205936 -5.3986239 -5.7175941 -5.72751 -5.5561094 -5.0437446][-3.421361 -3.5574336 -3.7463064 -3.9892263 -4.3885412 -4.7095819 -4.9468384 -5.4392805 -5.84387 -5.8755422 -5.72253 -5.6689849 -5.3580179 -5.1618671 -4.8637362][-3.7605486 -3.7453754 -3.7039573 -3.83013 -4.143342 -4.48917 -4.8509564 -5.1880369 -5.2028208 -5.259429 -5.0733962 -4.8261514 -4.8006663 -4.7050872 -4.4271107][-4.1215477 -3.945081 -3.7626309 -3.6051414 -3.6019998 -3.7975769 -3.9862962 -4.1428795 -4.1659784 -4.0971026 -3.9383247 -3.7925587 -3.8051929 -3.9877558 -4.0497627]]...]
INFO - root - 2017-12-16 05:40:19.383994: step 25510, loss = 0.23, batch loss = 0.18 (28.9 examples/sec; 0.277 sec/batch; 23h:34m:55s remains)
INFO - root - 2017-12-16 05:40:22.181895: step 25520, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.281 sec/batch; 23h:59m:06s remains)
INFO - root - 2017-12-16 05:40:25.031333: step 25530, loss = 0.26, batch loss = 0.20 (28.8 examples/sec; 0.278 sec/batch; 23h:42m:30s remains)
INFO - root - 2017-12-16 05:40:27.839565: step 25540, loss = 0.28, batch loss = 0.22 (27.7 examples/sec; 0.289 sec/batch; 24h:36m:02s remains)
INFO - root - 2017-12-16 05:40:30.691033: step 25550, loss = 0.30, batch loss = 0.24 (28.8 examples/sec; 0.278 sec/batch; 23h:42m:29s remains)
INFO - root - 2017-12-16 05:40:33.563022: step 25560, loss = 0.30, batch loss = 0.24 (28.3 examples/sec; 0.283 sec/batch; 24h:08m:18s remains)
INFO - root - 2017-12-16 05:40:36.349891: step 25570, loss = 0.25, batch loss = 0.19 (28.6 examples/sec; 0.280 sec/batch; 23h:52m:24s remains)
INFO - root - 2017-12-16 05:40:39.183930: step 25580, loss = 0.21, batch loss = 0.15 (27.8 examples/sec; 0.288 sec/batch; 24h:31m:23s remains)
INFO - root - 2017-12-16 05:40:41.943058: step 25590, loss = 0.28, batch loss = 0.22 (29.0 examples/sec; 0.276 sec/batch; 23h:32m:03s remains)
INFO - root - 2017-12-16 05:40:44.764756: step 25600, loss = 0.28, batch loss = 0.22 (29.4 examples/sec; 0.272 sec/batch; 23h:12m:19s remains)
2017-12-16 05:40:45.243312: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6182079 -2.5655794 -2.7312095 -3.3010187 -3.9282222 -4.6437483 -5.3179069 -5.8718219 -6.1598134 -6.0608416 -5.8773346 -5.4097309 -4.8864236 -4.4190884 -4.218297][-1.6149166 -1.5830605 -1.9111483 -2.61049 -3.3798013 -4.3065381 -5.1012735 -5.6702347 -5.923203 -5.8196311 -5.7260413 -5.3471813 -4.8540316 -4.2995052 -3.9205098][-0.5290575 -0.72570419 -1.3309255 -2.0416491 -2.8217988 -3.6056838 -4.0861192 -4.5833087 -4.7997456 -4.7536125 -4.6801119 -4.4808788 -4.2365146 -3.8920853 -3.3986993][0.80348539 0.20634222 -0.68153834 -1.4497786 -2.0757868 -2.428936 -2.4968495 -2.6387668 -2.7566957 -3.0492873 -3.3165355 -3.5863433 -3.6553051 -3.3927107 -2.783875][0.93719959 0.13357544 -0.74648643 -1.2633662 -1.4215217 -1.1438496 -0.5824275 -0.27237129 -0.41409254 -1.1513093 -2.0456214 -2.8934908 -3.32266 -3.2850623 -2.6943238][-0.40554762 -0.91008615 -1.4684737 -1.4591656 -0.73245382 0.25321341 1.3663564 2.0487733 1.8342752 0.61808014 -1.0406675 -2.4898543 -3.3689513 -3.590507 -3.0993762][-1.935456 -1.9358292 -1.8731859 -1.2980988 0.094357967 1.8269353 3.4074512 4.0422411 3.5295682 1.9326191 -0.20681858 -2.1316342 -3.2622647 -3.7005997 -3.4648747][-3.0277853 -2.9249144 -2.3876669 -1.168371 0.4810338 2.4080276 4.0333843 4.6757011 3.9303436 2.0137777 -0.23208141 -2.1646404 -3.3501768 -3.8701179 -3.6865084][-3.7867367 -3.6609402 -3.0276399 -1.8264196 -0.22336054 1.5654874 3.0275888 3.5258527 2.812911 1.0830369 -0.99647832 -2.8055062 -3.9346402 -4.3716555 -4.1690969][-4.3943624 -4.3482633 -3.9301045 -3.0234022 -1.4863098 -0.10349369 0.94796228 1.4938202 1.0629315 -0.37087202 -2.011548 -3.4897952 -4.3717294 -4.6293945 -4.4036217][-4.6291332 -4.8210888 -4.7385225 -4.184864 -3.2747808 -2.22219 -1.4739144 -1.1012199 -1.3184752 -2.0906274 -3.048245 -3.9270139 -4.4195037 -4.3778248 -3.8766179][-4.4095106 -4.7545457 -5.0027156 -4.8278341 -4.3904185 -3.8754525 -3.45853 -3.2016649 -3.2401009 -3.5191009 -4.0511131 -4.496398 -4.6204553 -4.3290448 -3.7212243][-4.3178487 -4.6131039 -4.893033 -4.9146314 -4.7109389 -4.480793 -4.214663 -4.1425967 -4.1034369 -4.2291927 -4.4123378 -4.6371541 -4.6660509 -4.402698 -3.8945122][-4.1827693 -4.2702289 -4.4051642 -4.351171 -4.2767591 -4.0812573 -3.934993 -3.8878846 -3.8091948 -3.9777625 -4.1328058 -4.1943841 -4.1754289 -3.9770322 -3.6080184][-3.6279075 -3.4724457 -3.3732512 -3.2157707 -3.1090465 -3.01936 -2.9926558 -2.9622221 -3.0721688 -3.3839636 -3.6815939 -3.8643 -3.9446511 -3.8770785 -3.6668341]]...]
INFO - root - 2017-12-16 05:40:48.068313: step 25610, loss = 0.31, batch loss = 0.25 (27.7 examples/sec; 0.288 sec/batch; 24h:34m:35s remains)
INFO - root - 2017-12-16 05:40:50.916460: step 25620, loss = 0.23, batch loss = 0.17 (28.8 examples/sec; 0.278 sec/batch; 23h:40m:43s remains)
INFO - root - 2017-12-16 05:40:53.795551: step 25630, loss = 0.26, batch loss = 0.20 (27.6 examples/sec; 0.290 sec/batch; 24h:44m:11s remains)
INFO - root - 2017-12-16 05:40:56.609569: step 25640, loss = 0.27, batch loss = 0.21 (27.7 examples/sec; 0.288 sec/batch; 24h:35m:09s remains)
INFO - root - 2017-12-16 05:40:59.419689: step 25650, loss = 0.27, batch loss = 0.21 (29.0 examples/sec; 0.276 sec/batch; 23h:29m:05s remains)
INFO - root - 2017-12-16 05:41:02.224161: step 25660, loss = 0.22, batch loss = 0.16 (28.0 examples/sec; 0.286 sec/batch; 24h:20m:23s remains)
INFO - root - 2017-12-16 05:41:05.081128: step 25670, loss = 0.24, batch loss = 0.19 (27.9 examples/sec; 0.287 sec/batch; 24h:28m:09s remains)
INFO - root - 2017-12-16 05:41:07.921900: step 25680, loss = 0.25, batch loss = 0.19 (28.3 examples/sec; 0.283 sec/batch; 24h:05m:31s remains)
INFO - root - 2017-12-16 05:41:10.765815: step 25690, loss = 0.36, batch loss = 0.30 (29.1 examples/sec; 0.275 sec/batch; 23h:26m:23s remains)
INFO - root - 2017-12-16 05:41:13.587646: step 25700, loss = 0.28, batch loss = 0.23 (28.9 examples/sec; 0.277 sec/batch; 23h:33m:56s remains)
2017-12-16 05:41:14.048628: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.7747252 -1.0342569 -1.0963089 -1.2213683 -1.55162 -1.7150691 -1.8767517 -1.807662 -1.7790709 -1.502912 -1.3211775 -1.0667276 -0.89766407 -0.986686 -1.0472269][0.19171476 0.03331852 -0.15869141 -0.35632324 -0.83376288 -1.188365 -1.6458218 -1.7518132 -1.8281827 -1.6275306 -1.2917366 -0.9122653 -0.67816281 -0.67597675 -0.78252292][0.40785551 0.4686265 0.26814032 -0.18160582 -0.92435527 -1.4613767 -1.9721692 -2.24636 -2.1696332 -1.7417357 -1.2494578 -0.76571608 -0.41076565 -0.5695231 -0.93142653][-0.68626761 -0.52231312 -0.77837276 -1.1572316 -1.5655169 -2.0029216 -2.2539334 -2.4248319 -2.2406971 -1.564553 -1.072577 -0.71008134 -0.46580172 -0.50137472 -0.74555039][-1.347923 -1.4064796 -1.7896757 -2.2588727 -2.3705978 -2.41329 -2.3141506 -2.1659465 -1.7950528 -1.0010674 -0.54190755 -0.25260973 -0.079501152 -0.23343134 -0.58526492][-1.84411 -1.9930036 -2.4445593 -3.0858474 -3.1805882 -2.7076735 -2.1108921 -1.5341022 -0.92968488 -0.32070923 -0.11270428 0.00082063675 0.15188551 0.15119886 -0.18280792][-2.219028 -2.6236587 -3.3468983 -3.8323343 -3.4945812 -2.5984 -1.3322015 -0.49411631 0.12117958 0.56470013 0.35710859 -0.0061078072 -0.10027409 -0.006444931 0.0080151558][-2.3280687 -2.9514368 -3.857542 -4.3386827 -3.4860377 -1.8853157 -0.16113615 1.0610132 1.5607014 1.5957761 0.89708519 0.037881851 -0.40162039 -0.34058332 -0.12620497][-1.7349236 -2.5744271 -3.6038256 -4.0769038 -3.1033792 -1.3053794 0.56412268 1.8258667 1.9901123 1.699204 0.60723019 -0.557302 -0.97222161 -0.79680753 -0.17650509][-0.96965408 -2.0402429 -3.24971 -3.5540109 -2.7497268 -1.1535628 0.41678619 1.4853444 1.4536653 0.6880784 -0.59612036 -1.8212283 -2.3119578 -2.1056161 -1.2899375][-1.0280914 -1.8655429 -2.9350114 -3.3370128 -2.747273 -1.2872546 0.067223549 0.6788063 0.29222345 -0.72457862 -2.2290506 -3.6485276 -4.059926 -3.5641961 -2.4909723][-1.6612554 -2.4286456 -3.2887058 -3.5637167 -3.2496865 -2.2670794 -1.3441603 -1.0837204 -1.7143271 -2.6584432 -3.7787743 -5.0075779 -5.3383203 -4.5047398 -2.9974928][-2.595005 -3.2441573 -3.9998834 -4.1497726 -4.1523886 -3.609201 -3.0082421 -2.8301935 -3.3999577 -4.3710713 -5.2919984 -5.9204373 -5.8110414 -5.1542835 -3.73108][-2.8871431 -3.273397 -3.7748199 -3.8915896 -4.005847 -3.7772355 -3.6364148 -3.7972102 -4.4816394 -5.4276304 -6.1491404 -6.80638 -6.6482239 -5.6744766 -4.2835422][-3.1469665 -3.3260896 -3.524384 -3.6954746 -3.8949051 -3.673562 -3.6573179 -4.0727215 -4.7495909 -5.5892019 -6.2044811 -6.7022257 -6.52295 -5.61297 -4.3820562]]...]
INFO - root - 2017-12-16 05:41:16.901253: step 25710, loss = 0.29, batch loss = 0.23 (28.0 examples/sec; 0.286 sec/batch; 24h:21m:14s remains)
INFO - root - 2017-12-16 05:41:19.733680: step 25720, loss = 0.22, batch loss = 0.16 (28.7 examples/sec; 0.279 sec/batch; 23h:45m:24s remains)
INFO - root - 2017-12-16 05:41:22.560209: step 25730, loss = 0.23, batch loss = 0.17 (29.0 examples/sec; 0.276 sec/batch; 23h:32m:30s remains)
INFO - root - 2017-12-16 05:41:25.421804: step 25740, loss = 0.23, batch loss = 0.17 (29.0 examples/sec; 0.276 sec/batch; 23h:29m:43s remains)
INFO - root - 2017-12-16 05:41:28.255053: step 25750, loss = 0.24, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 24h:04m:42s remains)
INFO - root - 2017-12-16 05:41:31.076828: step 25760, loss = 0.38, batch loss = 0.32 (28.7 examples/sec; 0.278 sec/batch; 23h:43m:46s remains)
INFO - root - 2017-12-16 05:41:33.906870: step 25770, loss = 0.38, batch loss = 0.33 (28.3 examples/sec; 0.282 sec/batch; 24h:03m:08s remains)
INFO - root - 2017-12-16 05:41:36.711709: step 25780, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.284 sec/batch; 24h:11m:11s remains)
INFO - root - 2017-12-16 05:41:39.583242: step 25790, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.281 sec/batch; 23h:58m:14s remains)
INFO - root - 2017-12-16 05:41:42.387807: step 25800, loss = 0.28, batch loss = 0.22 (27.9 examples/sec; 0.286 sec/batch; 24h:23m:26s remains)
2017-12-16 05:41:42.836271: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7825003 -3.7365427 -3.6040285 -3.5173016 -3.5290558 -3.6250377 -3.5407698 -3.4617693 -3.571779 -3.5935802 -3.6583872 -3.4983943 -3.3309484 -2.9055543 -2.4474881][-3.2662549 -3.1231816 -3.0483599 -3.1794386 -3.2724347 -3.2373757 -2.9272442 -2.7564054 -2.6282835 -2.6317797 -2.7579987 -2.6551371 -2.5575454 -2.243037 -1.9212332][-2.1597641 -2.2709818 -2.5557189 -2.667788 -2.7554519 -2.7798982 -2.522269 -2.0467846 -1.7339966 -1.9699385 -2.0910208 -2.0855563 -1.9603772 -1.7452374 -1.5642605][-1.0070539 -1.1726482 -1.727577 -2.1232266 -2.2243338 -1.8553631 -1.3928192 -1.1344738 -0.997401 -1.0999138 -1.1698811 -1.576299 -1.8868926 -1.7642734 -1.5884373][-0.23817062 -0.5357995 -1.0185344 -1.3561289 -1.4508464 -0.94178724 -0.29332924 0.13602924 0.22715712 -0.35976124 -0.78055525 -1.2045188 -1.6511121 -2.058207 -2.15408][-0.39328575 -0.49911427 -0.87942004 -0.81283474 -0.49331951 -0.034065247 0.62666512 1.0565567 1.1347036 0.62737608 -0.24199438 -1.3352728 -2.1259558 -2.4058833 -2.351603][-0.92413068 -0.7447381 -1.1182046 -0.9568615 -0.3370595 0.63439035 1.4597216 1.7317543 1.6942587 1.0681639 0.0677762 -1.2111189 -2.2272334 -2.7646787 -2.8141334][-1.77739 -1.7397718 -1.9646986 -1.6248157 -1.1624088 -0.13509178 1.0841718 1.6236658 1.5481358 0.94649982 -0.0056123734 -1.2294421 -2.3416338 -2.8623133 -2.820421][-3.2151861 -3.2614737 -3.3819838 -3.150424 -2.7470336 -1.707988 -0.45369267 0.31319141 0.63057137 0.11473894 -0.83093309 -1.875324 -2.9725437 -3.4688854 -3.2745371][-4.0243692 -4.677496 -5.4189305 -5.3863535 -4.8341122 -3.7693353 -2.599113 -1.7225044 -1.3295004 -1.4743247 -2.1051087 -3.0305362 -3.7976556 -4.1216111 -3.8602493][-5.0106297 -5.673583 -6.2338424 -6.5961323 -6.5988646 -5.6317606 -4.3880997 -3.5699189 -3.1747236 -3.2260747 -3.5564449 -4.1553006 -4.5037947 -4.4632392 -4.1704946][-4.993598 -6.1336269 -7.013113 -7.1162143 -6.8870821 -6.2959986 -5.3178234 -4.5849929 -4.1804814 -4.0612783 -4.2479844 -4.64092 -4.7477708 -4.5594349 -4.3073325][-5.0575905 -5.9195724 -6.5063248 -6.61589 -6.399281 -5.762042 -4.9796047 -4.40528 -4.1147008 -4.2161264 -4.3169851 -4.25647 -4.2023559 -4.0582061 -3.9376762][-4.4716964 -5.2398486 -5.6040688 -5.366385 -4.9001336 -4.28677 -3.7802439 -3.6169779 -3.6612732 -3.4632616 -3.2413759 -3.2157192 -3.1686318 -3.0354123 -3.0702899][-3.6090608 -4.0468121 -4.3469925 -4.0152354 -3.3983979 -2.9083045 -2.6345587 -2.4938812 -2.433686 -2.4891021 -2.4103882 -2.00906 -1.7142448 -1.8581891 -2.1820767]]...]
INFO - root - 2017-12-16 05:41:45.669572: step 25810, loss = 0.26, batch loss = 0.21 (29.0 examples/sec; 0.276 sec/batch; 23h:28m:25s remains)
INFO - root - 2017-12-16 05:41:48.491981: step 25820, loss = 0.30, batch loss = 0.24 (27.6 examples/sec; 0.290 sec/batch; 24h:42m:59s remains)
INFO - root - 2017-12-16 05:41:51.267797: step 25830, loss = 0.27, batch loss = 0.21 (28.1 examples/sec; 0.285 sec/batch; 24h:17m:30s remains)
INFO - root - 2017-12-16 05:41:54.173159: step 25840, loss = 0.40, batch loss = 0.34 (27.7 examples/sec; 0.288 sec/batch; 24h:34m:17s remains)
INFO - root - 2017-12-16 05:41:56.998991: step 25850, loss = 0.30, batch loss = 0.25 (28.8 examples/sec; 0.278 sec/batch; 23h:38m:34s remains)
INFO - root - 2017-12-16 05:41:59.814386: step 25860, loss = 0.27, batch loss = 0.21 (28.4 examples/sec; 0.281 sec/batch; 23h:57m:48s remains)
INFO - root - 2017-12-16 05:42:02.602828: step 25870, loss = 0.22, batch loss = 0.16 (29.1 examples/sec; 0.275 sec/batch; 23h:26m:56s remains)
INFO - root - 2017-12-16 05:42:05.460783: step 25880, loss = 0.24, batch loss = 0.18 (29.1 examples/sec; 0.275 sec/batch; 23h:25m:43s remains)
INFO - root - 2017-12-16 05:42:08.315101: step 25890, loss = 0.30, batch loss = 0.24 (29.0 examples/sec; 0.276 sec/batch; 23h:28m:37s remains)
INFO - root - 2017-12-16 05:42:11.193404: step 25900, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.284 sec/batch; 24h:13m:30s remains)
2017-12-16 05:42:11.644976: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3390784 -4.8947124 -5.3382459 -5.6645417 -5.7617111 -5.8582997 -5.9065781 -6.0138512 -6.1372 -6.0776224 -6.0518875 -5.8971438 -5.785583 -5.3412123 -4.8717432][-5.0359755 -5.7885504 -6.2917891 -6.5578127 -6.4437451 -6.422307 -6.3691225 -6.3674159 -6.7032318 -6.8526115 -7.0712833 -7.0453196 -6.8274975 -6.1682391 -5.466332][-5.7990513 -6.6355095 -6.9728408 -6.8685656 -6.4227433 -6.0714378 -5.8648148 -5.9457278 -6.4456615 -6.8326893 -7.2425623 -7.3517752 -7.2217007 -6.5415125 -5.6823497][-6.1755519 -7.0098224 -7.0977774 -6.4762764 -5.4754972 -4.5308609 -3.9352763 -3.8386598 -4.4800754 -5.3603907 -6.2431207 -6.7256775 -6.7411318 -6.2361956 -5.48804][-6.1321521 -6.5717216 -6.2247543 -5.0675683 -3.4311311 -1.9582658 -1.1030107 -0.92643809 -1.7652228 -2.9800529 -4.0882192 -4.8295393 -5.1199107 -5.0046492 -4.6682439][-5.9155378 -6.023211 -5.1841464 -3.3459594 -1.0205619 1.0232821 2.1560898 2.2557716 1.1497278 -0.62504816 -2.1845694 -3.176275 -3.5318313 -3.5028825 -3.2482524][-5.3695693 -5.015481 -4.0678163 -2.0258124 0.63017464 3.0671043 4.6276894 4.6416054 3.3904161 1.5251579 -0.24156094 -1.6131916 -2.23576 -2.2064862 -1.8487673][-4.957531 -4.4507108 -3.3578811 -1.3847063 1.1235623 3.5585141 5.1265211 5.0250235 3.7166386 1.8683591 0.31239223 -0.67060161 -1.0200813 -0.71454668 -0.26530552][-4.5358276 -4.1269374 -3.1241198 -1.4674544 0.55567694 2.700717 4.1131411 4.054719 3.0613375 1.5697532 0.40504456 -0.22956467 -0.30055666 0.26199436 0.90130472][-4.5350838 -4.5134678 -4.0531874 -2.9092731 -1.3409879 0.20175743 1.2592974 1.3633504 0.85461426 0.092822075 -0.30529976 -0.15751362 0.37077856 1.004837 1.4304757][-5.1282449 -5.404685 -5.4568863 -4.954751 -4.0247655 -3.095861 -2.3312981 -1.9674509 -1.8408518 -1.7113361 -1.2462306 -0.55079055 0.31521893 1.0907459 1.4705186][-5.6907463 -6.397912 -6.9439735 -6.9738865 -6.5774708 -5.9995661 -5.4373174 -4.9060407 -4.3021688 -3.4919641 -2.4551497 -1.4228771 -0.46608067 0.08012867 0.22726154][-6.1412473 -7.0310812 -7.7974758 -8.2419319 -8.3419514 -8.1648226 -7.8026471 -7.3144407 -6.5641651 -5.3883033 -4.1148243 -3.1154251 -2.327348 -1.9475429 -1.9185882][-6.6223536 -7.390048 -8.0082016 -8.5376511 -8.7706051 -8.8175354 -8.6648655 -8.4270563 -7.9145365 -7.004652 -6.0228963 -5.0980716 -4.4676604 -4.3108311 -4.1956034][-6.6424637 -7.3343067 -7.8978596 -8.4297218 -8.6889954 -8.9018316 -8.940258 -8.828167 -8.4159708 -7.8219771 -7.2460604 -6.6886883 -6.329371 -6.11849 -5.9138656]]...]
INFO - root - 2017-12-16 05:42:14.450737: step 25910, loss = 0.28, batch loss = 0.22 (29.5 examples/sec; 0.271 sec/batch; 23h:07m:04s remains)
INFO - root - 2017-12-16 05:42:17.284229: step 25920, loss = 0.31, batch loss = 0.25 (28.2 examples/sec; 0.283 sec/batch; 24h:08m:20s remains)
INFO - root - 2017-12-16 05:42:20.081023: step 25930, loss = 0.33, batch loss = 0.27 (27.2 examples/sec; 0.295 sec/batch; 25h:05m:06s remains)
INFO - root - 2017-12-16 05:42:22.943639: step 25940, loss = 0.24, batch loss = 0.18 (27.8 examples/sec; 0.288 sec/batch; 24h:30m:07s remains)
INFO - root - 2017-12-16 05:42:25.736090: step 25950, loss = 0.25, batch loss = 0.19 (29.6 examples/sec; 0.270 sec/batch; 22h:58m:33s remains)
INFO - root - 2017-12-16 05:42:28.552987: step 25960, loss = 0.31, batch loss = 0.25 (26.5 examples/sec; 0.302 sec/batch; 25h:44m:50s remains)
INFO - root - 2017-12-16 05:42:31.378850: step 25970, loss = 0.28, batch loss = 0.22 (29.3 examples/sec; 0.273 sec/batch; 23h:16m:52s remains)
INFO - root - 2017-12-16 05:42:34.234491: step 25980, loss = 0.24, batch loss = 0.18 (29.1 examples/sec; 0.275 sec/batch; 23h:26m:12s remains)
INFO - root - 2017-12-16 05:42:37.046040: step 25990, loss = 0.28, batch loss = 0.22 (28.2 examples/sec; 0.284 sec/batch; 24h:08m:27s remains)
INFO - root - 2017-12-16 05:42:39.913687: step 26000, loss = 0.31, batch loss = 0.25 (28.3 examples/sec; 0.283 sec/batch; 24h:06m:06s remains)
2017-12-16 05:42:40.371166: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1652679 -5.0784025 -5.0387287 -4.8788233 -4.6874738 -4.3862276 -4.0392666 -3.8543949 -3.7840695 -3.7613997 -3.8092461 -3.9495354 -4.1419735 -4.1590724 -4.3051486][-4.9333906 -4.6921172 -4.6911783 -4.8222 -4.93829 -4.9240861 -4.6172304 -4.1057076 -3.7720296 -3.7498331 -3.867723 -3.6346183 -3.5389247 -3.7684212 -4.1277895][-3.846391 -4.0111136 -4.4213119 -4.7375689 -5.040225 -5.2285442 -5.0002 -4.6325226 -4.0740857 -3.8255565 -3.9081354 -3.7562819 -3.4571977 -3.1049738 -3.4159484][-2.6429665 -2.8155456 -3.3934612 -4.4242535 -5.0806656 -5.0058675 -4.4405956 -3.9528151 -3.6475167 -3.6165283 -3.7347031 -3.7816339 -3.9165909 -3.6778944 -3.5777962][-1.38803 -1.5197761 -2.1369655 -3.2024493 -3.9408715 -3.7212744 -2.8107359 -2.162581 -1.938693 -2.5291061 -3.2936878 -3.6450324 -3.7662768 -3.8362043 -3.9569695][-0.96949148 -1.0420814 -1.3018293 -1.7449796 -1.897681 -1.6912694 -0.893662 -0.025524616 0.13783979 -0.59091282 -1.709403 -2.7087972 -3.1782525 -3.2614329 -3.1854775][-1.163908 -1.4407594 -1.7000945 -1.5273964 -0.74351048 0.41011333 1.8239498 2.4533157 2.2857146 1.3152566 -0.072839737 -1.1013324 -1.924099 -2.2585862 -2.6823969][-1.8573804 -1.9140851 -1.998368 -1.4504886 -0.46248746 1.3136482 3.217926 4.1390333 3.9728765 2.679595 1.1173887 -0.350564 -1.3840036 -1.7220066 -2.2040217][-3.1574702 -3.2865896 -2.98187 -2.077307 -0.92799044 0.85915327 2.8128414 3.9990311 3.7679672 2.4587402 0.98760653 -0.36923552 -1.189821 -1.6697464 -2.2604508][-3.719681 -4.6308556 -5.1104941 -4.1447749 -2.5878031 -0.83875537 0.6588397 2.0308747 2.2096424 1.367044 0.082290649 -1.1978877 -1.6924219 -1.9519224 -2.3030033][-5.0222163 -5.5499821 -5.8594036 -5.5816193 -4.7131677 -3.0650015 -1.4364502 -0.40997934 -0.30875015 -0.94379163 -1.6430593 -2.3538764 -2.6925669 -2.7530875 -2.7089813][-5.4360256 -6.0541449 -6.5211673 -6.2713385 -5.6930289 -4.7872257 -3.8194706 -3.0501714 -2.5684776 -2.8827424 -3.3644876 -3.7127457 -3.525733 -3.2403359 -3.0279598][-5.3265185 -5.992475 -6.4141107 -6.3478322 -6.1137466 -5.3781214 -4.6909637 -4.3445559 -4.0034237 -3.9166095 -3.780869 -3.9626741 -3.7138202 -3.2579579 -2.8190911][-5.060617 -5.4316006 -5.538126 -5.467226 -5.3687239 -4.9686627 -4.5883827 -4.3489194 -4.22018 -4.1785808 -3.9020567 -3.6249557 -3.2781391 -2.9205041 -2.5283442][-4.3669128 -4.3974409 -4.3629608 -4.1860538 -4.0273862 -3.854846 -3.8101506 -3.7285645 -3.6807752 -3.6822224 -3.532414 -3.024806 -2.3835285 -2.1222034 -1.874747]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 05:42:43.211125: step 26010, loss = 0.20, batch loss = 0.14 (28.1 examples/sec; 0.285 sec/batch; 24h:13m:17s remains)
INFO - root - 2017-12-16 05:42:46.002250: step 26020, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.281 sec/batch; 23h:55m:00s remains)
INFO - root - 2017-12-16 05:42:48.792593: step 26030, loss = 0.27, batch loss = 0.21 (28.8 examples/sec; 0.278 sec/batch; 23h:38m:40s remains)
INFO - root - 2017-12-16 05:42:51.555587: step 26040, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 24h:11m:11s remains)
INFO - root - 2017-12-16 05:42:54.389710: step 26050, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.286 sec/batch; 24h:18m:30s remains)
INFO - root - 2017-12-16 05:42:57.236272: step 26060, loss = 0.27, batch loss = 0.21 (29.0 examples/sec; 0.276 sec/batch; 23h:28m:04s remains)
INFO - root - 2017-12-16 05:43:00.041571: step 26070, loss = 0.27, batch loss = 0.21 (27.4 examples/sec; 0.292 sec/batch; 24h:49m:34s remains)
INFO - root - 2017-12-16 05:43:02.838882: step 26080, loss = 0.25, batch loss = 0.19 (28.6 examples/sec; 0.280 sec/batch; 23h:50m:29s remains)
INFO - root - 2017-12-16 05:43:05.646809: step 26090, loss = 0.32, batch loss = 0.26 (27.4 examples/sec; 0.292 sec/batch; 24h:53m:31s remains)
INFO - root - 2017-12-16 05:43:08.485693: step 26100, loss = 0.32, batch loss = 0.26 (28.0 examples/sec; 0.286 sec/batch; 24h:21m:10s remains)
2017-12-16 05:43:08.947381: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3390813 -5.0823746 -5.3054528 -5.631155 -5.5292916 -5.527401 -5.8446388 -6.16908 -6.6369038 -6.8990307 -6.947454 -6.6817369 -6.3712149 -6.05729 -5.654284][-4.3010392 -4.8296695 -5.0115147 -4.7062807 -4.3556676 -4.6241593 -4.9594116 -5.4839749 -5.9980707 -6.4122949 -6.767652 -6.6814394 -6.563426 -6.1544824 -5.6588721][-4.5158858 -4.8694715 -4.8140287 -4.306375 -3.6194606 -3.4260311 -3.6348336 -4.2595477 -4.871861 -5.49257 -6.1485357 -6.5453482 -6.7921114 -6.4438763 -5.9592719][-4.1468329 -4.6982288 -4.649446 -3.7343397 -2.4224327 -1.7432883 -1.4986217 -1.9471049 -2.6902916 -3.7531018 -4.7177362 -5.5552964 -6.4441447 -6.7812476 -6.833806][-3.5383701 -3.938591 -3.4240596 -2.2813132 -0.90044069 0.5231266 1.2052207 1.0507507 0.51068163 -0.75309491 -2.2113762 -3.8638964 -5.1986427 -6.4331946 -7.1975956][-3.5981224 -3.4320588 -2.6091073 -1.0158079 1.0768466 2.5824733 3.6254864 4.0499163 3.5270319 2.2663355 0.60615396 -1.661185 -3.790235 -5.2809854 -6.3077393][-3.0493674 -2.898385 -2.0197277 -0.15355635 2.2573824 4.269557 5.5993233 6.0464706 5.7265997 4.5740414 2.4323173 -0.042348862 -2.4017737 -4.3577418 -5.7082152][-2.5693581 -2.50286 -2.1779788 -0.53048062 2.0155358 4.3637695 6.2448168 6.9117479 6.4763222 4.83587 2.5794435 0.29382658 -2.0810559 -4.0190029 -5.3995471][-2.5586209 -2.9352598 -2.562798 -1.5397248 0.17823124 2.5827732 4.7356892 5.7004776 5.5563612 4.0091438 1.787015 -0.7059586 -2.8637469 -4.1276927 -5.2907753][-2.3690248 -3.2824159 -3.9556248 -3.3104067 -1.553535 0.51098061 2.0748916 3.1180773 3.3639874 2.1790147 0.41855669 -1.6835163 -3.5295014 -4.5886569 -5.52283][-3.205174 -3.8701916 -4.4772005 -4.6349368 -4.0089107 -2.3279021 -0.39913034 0.51076937 0.41422176 -0.52269053 -1.9012313 -3.6488347 -4.9047346 -5.7396793 -6.375176][-3.7848949 -4.911561 -5.7121034 -5.83545 -5.1918855 -4.2369294 -3.1624374 -2.2164431 -1.9109905 -2.7010939 -3.89607 -5.3026166 -6.193924 -6.740243 -7.0352039][-4.1800447 -5.0657072 -6.0310774 -6.5230331 -6.3021131 -5.41635 -4.3591251 -3.850842 -3.76063 -4.1329103 -4.8670774 -5.6810923 -6.1799984 -6.5204988 -6.689395][-4.4718089 -4.9240165 -5.5580578 -5.9270492 -5.8910365 -5.3634167 -4.7051396 -4.2499266 -4.029448 -4.2254114 -4.6200242 -5.166049 -5.6881557 -5.97169 -5.9645309][-5.0621061 -5.0694714 -5.2168665 -5.3806071 -5.2660313 -4.93997 -4.517314 -4.1139235 -3.8337922 -3.8667793 -4.1275067 -4.3193583 -4.5252032 -4.9641929 -5.2012763]]...]
INFO - root - 2017-12-16 05:43:11.768819: step 26110, loss = 0.22, batch loss = 0.16 (28.7 examples/sec; 0.279 sec/batch; 23h:42m:15s remains)
INFO - root - 2017-12-16 05:43:14.618501: step 26120, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.284 sec/batch; 24h:12m:36s remains)
INFO - root - 2017-12-16 05:43:17.384226: step 26130, loss = 0.29, batch loss = 0.23 (29.7 examples/sec; 0.269 sec/batch; 22h:55m:39s remains)
INFO - root - 2017-12-16 05:43:20.254753: step 26140, loss = 0.25, batch loss = 0.19 (27.2 examples/sec; 0.294 sec/batch; 25h:01m:45s remains)
INFO - root - 2017-12-16 05:43:23.072963: step 26150, loss = 0.22, batch loss = 0.16 (29.1 examples/sec; 0.275 sec/batch; 23h:22m:56s remains)
INFO - root - 2017-12-16 05:43:25.871368: step 26160, loss = 0.38, batch loss = 0.32 (27.8 examples/sec; 0.288 sec/batch; 24h:30m:21s remains)
INFO - root - 2017-12-16 05:43:28.688211: step 26170, loss = 0.21, batch loss = 0.15 (27.1 examples/sec; 0.295 sec/batch; 25h:05m:22s remains)
INFO - root - 2017-12-16 05:43:31.532624: step 26180, loss = 0.29, batch loss = 0.23 (27.9 examples/sec; 0.287 sec/batch; 24h:25m:34s remains)
INFO - root - 2017-12-16 05:43:34.367030: step 26190, loss = 0.23, batch loss = 0.17 (28.8 examples/sec; 0.278 sec/batch; 23h:39m:28s remains)
INFO - root - 2017-12-16 05:43:37.160407: step 26200, loss = 0.22, batch loss = 0.16 (29.0 examples/sec; 0.276 sec/batch; 23h:29m:02s remains)
2017-12-16 05:43:37.628304: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8127396 -1.8858976 -1.8825929 -2.0246971 -2.2679796 -2.5725977 -3.0767488 -3.5588694 -3.9648836 -4.032578 -3.9668531 -3.630322 -3.0283492 -2.4402823 -1.8269451][-2.3168287 -2.5235813 -2.5357814 -2.3901989 -2.335737 -2.4526985 -2.6846259 -3.0166416 -3.5246861 -3.7311881 -3.842207 -3.6966324 -3.4359522 -2.954972 -2.1630094][-3.4411001 -3.5047598 -3.2471933 -2.9512286 -2.6388755 -2.4482577 -2.4621186 -2.4682937 -2.6546769 -3.0314462 -3.6302257 -3.8788853 -3.7669785 -3.4087815 -2.8930893][-5.0411139 -5.0672078 -4.8099113 -4.3324113 -3.6126642 -2.7542753 -2.0593405 -1.7881286 -2.0375888 -2.2683361 -2.6458182 -3.3414755 -3.9854527 -4.0155873 -3.494318][-6.1565905 -6.3942242 -6.1166506 -5.3901386 -4.2338476 -2.9268425 -1.8473024 -1.0755496 -1.0269694 -1.5342641 -2.5934777 -3.5720065 -4.2732396 -4.665709 -4.7646][-6.3328252 -6.7225409 -6.3552103 -5.5531492 -4.1455026 -2.2514806 -0.56243539 0.54125738 0.637373 -0.20492172 -1.5708735 -2.9417479 -4.1251364 -4.630044 -4.5248184][-5.5200319 -5.9833879 -5.8864098 -5.1226935 -3.5165987 -1.2231793 0.79154682 1.9406953 1.90693 1.0520606 -0.36346769 -1.855155 -2.9552755 -3.3599494 -3.4007502][-4.0828753 -4.49884 -4.3489389 -3.8643179 -2.68267 -0.565886 1.5020204 2.7500291 2.7635021 1.8555694 0.4101882 -0.83857417 -1.5634 -1.9175906 -1.9590051][-2.1702302 -2.5419068 -3.0736995 -2.854691 -1.7806685 -0.0021133423 1.7363286 2.8685408 2.8770089 1.9062929 0.67614937 -0.17518854 -0.53590655 -0.6117928 -0.6754334][-0.42156124 -0.82220817 -1.4928858 -1.9070206 -1.6064005 -0.12009573 1.3260064 2.1860938 2.125988 1.1899776 0.0034899712 -0.42750502 -0.087165356 0.21833086 0.43723059][0.40595818 -0.078969 -1.0481613 -1.6555667 -1.5760534 -0.74365473 0.29602146 0.92625093 0.58183384 -0.25067854 -0.91033077 -0.93795609 -0.34953022 0.18023109 0.44922829][0.24992704 -0.083551407 -0.85655189 -1.7062306 -1.8819005 -1.4703851 -1.0982268 -0.75858855 -0.89954257 -1.4144871 -1.9186938 -1.7586117 -1.0325959 -0.61875224 -0.49909902][-0.2283721 -0.49315786 -1.3442602 -2.2435267 -2.6229224 -2.3231142 -1.8729377 -1.7007492 -2.1589572 -2.5962305 -2.5649228 -2.2542229 -1.697557 -1.4551101 -1.6198704][-1.0697527 -0.99997234 -1.4830363 -2.2111244 -2.6055942 -2.4658916 -2.284308 -2.0877502 -2.2397366 -2.5189738 -2.5674086 -2.3409262 -2.0387487 -2.1009262 -2.2118552][-2.0665653 -1.7856216 -1.8732884 -2.2221284 -2.3166182 -2.3382356 -2.2341454 -2.1135945 -2.4053473 -2.5717044 -2.5321355 -2.4029417 -2.256875 -2.3433578 -2.6012163]]...]
INFO - root - 2017-12-16 05:43:40.437365: step 26210, loss = 0.24, batch loss = 0.19 (28.2 examples/sec; 0.284 sec/batch; 24h:09m:54s remains)
INFO - root - 2017-12-16 05:43:43.270428: step 26220, loss = 0.43, batch loss = 0.37 (29.1 examples/sec; 0.275 sec/batch; 23h:21m:23s remains)
INFO - root - 2017-12-16 05:43:46.174404: step 26230, loss = 0.22, batch loss = 0.17 (27.0 examples/sec; 0.297 sec/batch; 25h:13m:29s remains)
INFO - root - 2017-12-16 05:43:49.013242: step 26240, loss = 0.22, batch loss = 0.16 (28.0 examples/sec; 0.285 sec/batch; 24h:16m:24s remains)
INFO - root - 2017-12-16 05:43:51.799964: step 26250, loss = 0.27, batch loss = 0.21 (29.1 examples/sec; 0.274 sec/batch; 23h:21m:00s remains)
INFO - root - 2017-12-16 05:43:54.663657: step 26260, loss = 0.24, batch loss = 0.18 (27.8 examples/sec; 0.288 sec/batch; 24h:27m:24s remains)
INFO - root - 2017-12-16 05:43:57.449152: step 26270, loss = 0.26, batch loss = 0.20 (27.5 examples/sec; 0.291 sec/batch; 24h:45m:32s remains)
INFO - root - 2017-12-16 05:44:00.280073: step 26280, loss = 0.21, batch loss = 0.15 (28.5 examples/sec; 0.281 sec/batch; 23h:53m:00s remains)
INFO - root - 2017-12-16 05:44:03.115809: step 26290, loss = 0.33, batch loss = 0.27 (28.1 examples/sec; 0.285 sec/batch; 24h:15m:32s remains)
INFO - root - 2017-12-16 05:44:06.021700: step 26300, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 24h:14m:17s remains)
2017-12-16 05:44:06.472255: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8563161 -3.0998707 -3.1840322 -3.2236953 -3.5802786 -4.3445792 -4.9897623 -5.6185551 -6.3064308 -6.6181679 -6.5220127 -6.1795282 -5.5431986 -4.8537149 -4.1114087][-2.8802154 -2.7727416 -2.6718111 -2.6573234 -2.8124568 -3.1846261 -4.1235423 -4.6489587 -5.260324 -5.7165375 -5.9547815 -5.5777178 -4.65996 -4.0363593 -3.4473879][-2.8936062 -2.5448961 -2.1518223 -1.8540292 -1.7540216 -1.9214768 -2.4930389 -3.4522715 -4.1980605 -4.8313942 -5.0214329 -4.6104336 -3.7302918 -2.8565052 -1.9551785][-2.7335849 -1.9623339 -1.5184739 -1.0760603 -0.66126108 -0.52355886 -0.97544503 -2.0326622 -3.1713266 -3.8362641 -3.8146381 -3.2799337 -2.4986043 -2.0557375 -1.4535091][-2.71695 -1.7032471 -0.80068755 -0.15128946 0.36242151 0.96949577 0.957582 0.2708478 -1.0216801 -2.0324252 -2.6385407 -2.2084944 -1.2986999 -1.1823981 -1.1615229][-3.0079179 -2.0268059 -0.78136992 0.65855217 2.1314497 3.1369786 3.2572446 2.6516142 1.4240088 0.017629623 -1.059056 -1.3765593 -1.0839305 -0.99016714 -1.1974652][-2.7144067 -1.790113 -0.66829157 0.996335 2.5972242 3.956255 4.4327784 3.7645102 2.3829241 0.57037497 -0.9735322 -1.4036384 -1.3200183 -1.0625317 -1.0780685][-3.0008297 -1.9542143 -0.547935 1.1981945 2.9363165 3.8864393 3.982666 3.2572994 1.819787 0.18508244 -1.2367477 -1.8122199 -1.6497562 -1.5960305 -1.5794466][-3.2034047 -2.3157985 -0.94874334 0.67640924 2.233798 3.2577786 3.3763027 2.4527197 0.93658161 -0.69661689 -2.0473835 -2.5931022 -2.4423847 -2.0974338 -1.67535][-3.5232375 -2.6089525 -1.6581366 -0.27030897 0.98306417 1.7819304 1.7845311 1.0280743 -0.18934441 -1.5361907 -2.4782834 -2.9649892 -2.8529425 -2.2761595 -1.7374225][-4.14525 -3.4320815 -2.6620851 -1.7399826 -0.86251736 -0.22994947 -0.043332577 -0.55994558 -1.4964323 -2.378283 -3.011457 -3.0949662 -2.7430203 -2.182725 -1.586405][-4.2730045 -4.0770464 -3.6984549 -3.23564 -2.720592 -2.275089 -1.8687401 -1.7980134 -2.1992235 -2.7329211 -3.0832419 -3.0554881 -2.7078912 -2.1297736 -1.6012049][-3.8375261 -3.8476284 -3.7116265 -3.3181534 -2.960371 -2.5592549 -2.1740868 -2.0206726 -2.08703 -2.2925043 -2.4126692 -2.3209417 -2.1028037 -1.809797 -1.4360642][-3.2451406 -3.327029 -3.3002048 -3.0648355 -2.8060732 -2.3904221 -1.8056936 -1.6271086 -1.7502401 -1.8696795 -1.8816438 -1.7666998 -1.6625569 -1.5734284 -1.4537678][-2.7262678 -2.9268312 -3.0645876 -2.907377 -2.5890348 -2.3463156 -1.8856082 -1.4448776 -1.2675197 -1.2630832 -1.3812332 -1.3918328 -1.594301 -1.6120441 -1.5133939]]...]
INFO - root - 2017-12-16 05:44:09.307847: step 26310, loss = 0.30, batch loss = 0.24 (28.0 examples/sec; 0.286 sec/batch; 24h:19m:06s remains)
INFO - root - 2017-12-16 05:44:12.140208: step 26320, loss = 0.25, batch loss = 0.19 (28.6 examples/sec; 0.280 sec/batch; 23h:49m:33s remains)
INFO - root - 2017-12-16 05:44:14.970147: step 26330, loss = 0.43, batch loss = 0.37 (29.4 examples/sec; 0.272 sec/batch; 23h:08m:39s remains)
INFO - root - 2017-12-16 05:44:17.817185: step 26340, loss = 0.24, batch loss = 0.18 (26.2 examples/sec; 0.305 sec/batch; 25h:57m:47s remains)
INFO - root - 2017-12-16 05:44:20.675562: step 26350, loss = 0.29, batch loss = 0.23 (28.0 examples/sec; 0.285 sec/batch; 24h:15m:33s remains)
INFO - root - 2017-12-16 05:44:23.516812: step 26360, loss = 0.33, batch loss = 0.28 (28.6 examples/sec; 0.280 sec/batch; 23h:47m:57s remains)
INFO - root - 2017-12-16 05:44:26.360360: step 26370, loss = 0.25, batch loss = 0.19 (27.4 examples/sec; 0.292 sec/batch; 24h:49m:23s remains)
INFO - root - 2017-12-16 05:44:29.156198: step 26380, loss = 0.25, batch loss = 0.19 (28.7 examples/sec; 0.279 sec/batch; 23h:43m:23s remains)
INFO - root - 2017-12-16 05:44:31.965442: step 26390, loss = 0.27, batch loss = 0.21 (29.2 examples/sec; 0.274 sec/batch; 23h:18m:29s remains)
INFO - root - 2017-12-16 05:44:34.792127: step 26400, loss = 0.25, batch loss = 0.20 (28.8 examples/sec; 0.278 sec/batch; 23h:36m:28s remains)
2017-12-16 05:44:35.251142: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9229412 -3.0545354 -3.1502264 -3.2195172 -3.1496673 -2.9757149 -3.0404005 -3.3493223 -3.994664 -4.682075 -5.232564 -5.5146809 -5.5061636 -5.1532421 -4.4646254][-3.3987088 -3.5719543 -3.5890312 -3.4524088 -3.2368498 -2.9891453 -2.8762317 -3.2081933 -4.0178585 -4.9294982 -5.7989578 -6.1274357 -6.0444961 -5.6352787 -4.99105][-4.0645614 -4.1418109 -3.9659417 -3.5382948 -2.9783268 -2.5361547 -2.2746439 -2.5331547 -3.4319122 -4.5742354 -5.7939615 -6.3854308 -6.4895816 -6.1043444 -5.4690843][-4.5312676 -4.4207735 -3.9364691 -3.216979 -2.3477333 -1.5330844 -1.0309255 -1.3168464 -2.28383 -3.5924993 -4.9908228 -5.9029412 -6.4526367 -6.3996515 -5.9126492][-4.6884217 -4.3135762 -3.4185972 -2.2017303 -0.9282639 0.18949556 0.87709475 0.67924881 -0.33027506 -1.968097 -3.7933648 -5.2091084 -6.0927491 -6.4104757 -6.1362133][-4.4840317 -3.8650761 -2.7966728 -1.1046369 0.70775175 2.0055032 2.7874651 2.5980315 1.5520506 -0.28118467 -2.4509521 -4.3395934 -5.6530981 -6.2870741 -6.2213483][-4.059866 -3.237545 -2.0381672 -0.25140429 1.8253341 3.3680892 4.407752 4.1707096 2.9548192 0.95343256 -1.4795902 -3.5365071 -5.0965433 -5.9435463 -5.9751043][-3.7550263 -3.0394068 -1.9471974 -0.21330738 1.7803898 3.4412656 4.4965382 4.3706579 3.1855288 1.1364207 -1.2593088 -3.4354162 -5.0599146 -5.9881291 -6.0371156][-3.8202016 -3.3650765 -2.4924755 -1.032799 0.45100689 2.095715 3.1487255 2.9496222 1.9992619 0.1861949 -1.9262438 -3.9897976 -5.5206928 -6.3132429 -6.298512][-4.084929 -3.9781432 -3.7464128 -2.7707396 -1.3803144 -0.15461636 0.72458315 0.67083359 0.04353714 -1.2924829 -3.0072496 -4.776093 -6.0881681 -6.6453629 -6.5888405][-4.4978251 -4.4952536 -4.4676604 -4.203114 -3.3827682 -2.2596512 -1.3094673 -1.2260082 -1.7478323 -2.6412838 -3.7556725 -5.1023231 -6.1186523 -6.5662475 -6.5733809][-4.5300097 -4.5618472 -4.6631041 -4.509625 -4.0888653 -3.3280919 -2.5336494 -2.3529603 -2.5707181 -3.2190151 -4.0273232 -5.0981379 -5.8003917 -6.1108403 -6.2460938][-3.6522548 -3.4672184 -3.5047235 -3.5357907 -3.4115293 -3.1326256 -2.7335691 -2.6762381 -2.6720243 -3.0713151 -3.6945872 -4.4734349 -5.0552292 -5.3756762 -5.6302481][-2.7911386 -2.2388675 -1.9510183 -1.8612702 -1.9388547 -2.0884264 -2.130666 -2.2734027 -2.3538768 -2.4195421 -2.6336598 -3.2351284 -3.9068992 -4.3969445 -4.7233105][-2.0935054 -1.1506853 -0.56282735 -0.36210823 -0.44912386 -0.65332532 -0.91854072 -1.0818594 -1.1869464 -1.353761 -1.4956608 -1.9030869 -2.5334611 -3.286629 -3.9665787]]...]
INFO - root - 2017-12-16 05:44:38.078655: step 26410, loss = 0.35, batch loss = 0.29 (29.3 examples/sec; 0.273 sec/batch; 23h:13m:44s remains)
INFO - root - 2017-12-16 05:44:40.955390: step 26420, loss = 0.21, batch loss = 0.15 (27.9 examples/sec; 0.287 sec/batch; 24h:22m:46s remains)
INFO - root - 2017-12-16 05:44:43.808478: step 26430, loss = 0.30, batch loss = 0.24 (29.2 examples/sec; 0.274 sec/batch; 23h:19m:33s remains)
INFO - root - 2017-12-16 05:44:46.597455: step 26440, loss = 0.29, batch loss = 0.23 (28.7 examples/sec; 0.279 sec/batch; 23h:41m:44s remains)
INFO - root - 2017-12-16 05:44:49.456708: step 26450, loss = 0.26, batch loss = 0.21 (26.5 examples/sec; 0.302 sec/batch; 25h:42m:26s remains)
INFO - root - 2017-12-16 05:44:52.249852: step 26460, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 24h:17m:41s remains)
INFO - root - 2017-12-16 05:44:55.125186: step 26470, loss = 0.29, batch loss = 0.23 (27.4 examples/sec; 0.292 sec/batch; 24h:51m:27s remains)
INFO - root - 2017-12-16 05:44:57.922454: step 26480, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.279 sec/batch; 23h:42m:57s remains)
INFO - root - 2017-12-16 05:45:00.746246: step 26490, loss = 0.28, batch loss = 0.23 (29.5 examples/sec; 0.271 sec/batch; 23h:04m:29s remains)
INFO - root - 2017-12-16 05:45:03.597877: step 26500, loss = 0.27, batch loss = 0.21 (29.5 examples/sec; 0.271 sec/batch; 23h:01m:36s remains)
2017-12-16 05:45:04.060820: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1988511 -2.5664277 -3.0309997 -3.5125086 -3.9827111 -4.4311786 -4.9147229 -5.5420275 -6.0748472 -6.2043233 -6.0766654 -5.9274378 -5.4800096 -4.6281948 -3.6618912][-3.0126038 -3.5440536 -4.0067534 -4.3566346 -4.5520735 -4.7340794 -5.0532513 -5.4344134 -5.8200145 -6.237175 -6.54253 -6.3345246 -5.8940639 -5.3795013 -4.6164527][-4.0567913 -4.6452484 -4.9386945 -4.8684816 -4.5968833 -4.3481369 -4.2024326 -4.428617 -4.8664994 -5.324286 -5.7532072 -6.1219664 -6.1560965 -5.5867648 -4.7834482][-4.7550659 -5.3397188 -5.4794927 -5.0444031 -4.1922793 -3.2039638 -2.5594301 -2.375596 -2.6886714 -3.4979811 -4.5332131 -5.2918892 -5.7074046 -5.6153383 -5.0246973][-5.2745981 -5.4536047 -5.2201385 -4.3320093 -2.7980263 -1.1715031 -0.011909485 0.44837379 0.22241974 -0.87364769 -2.6112158 -4.3768992 -5.3515363 -5.4544883 -5.1107779][-4.9804845 -4.6276832 -3.9202714 -2.6964374 -0.84529614 1.2099285 2.6070275 3.1140175 2.7518711 1.4132295 -0.66733265 -2.8430018 -4.4648108 -5.1923079 -5.0821366][-4.004755 -3.4633727 -2.346395 -0.85454178 1.0364456 2.9649515 4.4842129 4.8538227 4.1215591 2.5789475 0.4757247 -1.8044832 -3.5903776 -4.6327977 -4.95536][-3.2518513 -2.4970307 -1.3165975 0.33318949 2.1652188 3.9083576 4.9833927 5.0566797 4.2521276 2.5442238 0.4173007 -1.732568 -3.4852047 -4.5895057 -5.0494757][-2.9495897 -2.2385247 -1.0646951 0.48710918 2.0261378 3.4677439 4.3028517 4.0549383 3.1902361 1.8215184 0.0030202866 -1.9965632 -3.6339517 -4.7383776 -5.3433189][-3.40468 -2.6190929 -1.6061399 -0.23872662 1.0450377 1.9191465 2.4369311 2.353394 1.6687579 0.4599371 -0.80037141 -2.2905238 -3.7568517 -4.9233842 -5.5140243][-4.2907419 -3.4032888 -2.3814952 -1.2587106 -0.32350683 0.32710743 0.53317451 0.53405666 0.25751829 -0.64344 -1.9024577 -3.1960154 -4.3955655 -5.3302627 -5.8213639][-5.249249 -4.3258023 -3.2153363 -2.1318974 -1.3251345 -0.85771203 -0.59450316 -0.82612133 -1.2620692 -1.842953 -2.7318416 -4.0177541 -5.1847992 -5.8900824 -6.1236296][-6.2207355 -5.2176027 -4.0035334 -2.7400441 -1.8679001 -1.4066248 -1.2433577 -1.2778296 -1.6686292 -2.496047 -3.4116168 -4.4018207 -5.4635911 -6.244339 -6.2817087][-6.4198623 -5.3344178 -3.9949074 -2.667376 -1.6317983 -1.1177597 -1.0791972 -1.2687666 -1.6717458 -2.4411316 -3.4554167 -4.528182 -5.3827815 -5.9909706 -6.1098075][-5.8197937 -4.656527 -3.3147755 -1.9252527 -0.8555553 -0.41072798 -0.48056626 -0.75386 -1.2213573 -1.977365 -2.8513203 -3.8652585 -4.8115311 -5.31049 -5.0964203]]...]
INFO - root - 2017-12-16 05:45:06.854168: step 26510, loss = 0.27, batch loss = 0.22 (29.0 examples/sec; 0.276 sec/batch; 23h:28m:33s remains)
INFO - root - 2017-12-16 05:45:09.748388: step 26520, loss = 0.33, batch loss = 0.27 (28.1 examples/sec; 0.285 sec/batch; 24h:12m:59s remains)
INFO - root - 2017-12-16 05:45:12.587829: step 26530, loss = 0.32, batch loss = 0.26 (26.5 examples/sec; 0.302 sec/batch; 25h:38m:11s remains)
INFO - root - 2017-12-16 05:45:15.413078: step 26540, loss = 0.25, batch loss = 0.19 (28.7 examples/sec; 0.279 sec/batch; 23h:43m:44s remains)
INFO - root - 2017-12-16 05:45:18.228983: step 26550, loss = 0.33, batch loss = 0.27 (28.7 examples/sec; 0.279 sec/batch; 23h:41m:05s remains)
INFO - root - 2017-12-16 05:45:21.107498: step 26560, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.282 sec/batch; 23h:57m:33s remains)
INFO - root - 2017-12-16 05:45:23.941097: step 26570, loss = 0.29, batch loss = 0.23 (28.2 examples/sec; 0.283 sec/batch; 24h:04m:08s remains)
INFO - root - 2017-12-16 05:45:26.733268: step 26580, loss = 0.29, batch loss = 0.23 (29.2 examples/sec; 0.274 sec/batch; 23h:18m:46s remains)
INFO - root - 2017-12-16 05:45:29.522504: step 26590, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 24h:08m:04s remains)
INFO - root - 2017-12-16 05:45:32.377245: step 26600, loss = 0.30, batch loss = 0.25 (27.5 examples/sec; 0.291 sec/batch; 24h:43m:08s remains)
2017-12-16 05:45:32.832723: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9816537 -4.747056 -4.7725496 -5.2246876 -5.9246597 -6.4088068 -6.9633136 -8.0268736 -8.6867018 -8.8802681 -8.4068956 -7.6125536 -6.5662012 -5.1547232 -4.0568333][-3.8028424 -3.7709768 -3.9844341 -4.1595941 -4.4466062 -5.7058582 -7.1649761 -7.9681807 -8.3962212 -8.3375816 -8.0401735 -7.4781675 -6.22011 -4.8949003 -4.0112281][-2.4756293 -2.39467 -2.5790133 -3.0323031 -3.7852097 -4.2542443 -5.0926962 -6.8226609 -7.65082 -7.6257653 -7.2322369 -6.6501169 -5.752368 -4.5215678 -3.5506363][-1.199734 -1.1987023 -1.5183485 -1.8127706 -2.1539431 -2.8868704 -3.6865158 -4.3677835 -5.1774755 -5.9760971 -6.0530009 -5.8944664 -5.223248 -4.2286186 -3.2921739][-0.344532 -0.066251278 -0.060070038 -0.53550839 -1.0079503 -0.75299191 -0.80133557 -1.5889652 -2.1647429 -3.0637832 -3.9382665 -4.3630776 -4.0890946 -3.7131033 -3.2162824][-0.605922 0.14003372 0.7050128 0.69154072 0.7469964 1.2311692 1.7515612 1.7006111 0.72644138 -0.63074064 -1.5997615 -2.6599097 -3.0234647 -2.6704502 -2.2078381][-1.0716448 0.033874035 0.64197445 1.2633066 1.9601049 2.6131029 3.4976025 3.3419819 2.6037803 1.1123934 -0.21248007 -1.2235034 -1.9087284 -2.1535287 -1.8873308][-1.510571 -0.33223152 0.27579021 1.0086932 1.7688212 2.9747968 4.2134066 3.894927 3.029336 1.6189294 0.46770477 -0.65058517 -1.3482828 -1.8402803 -2.0792592][-2.1132991 -1.20241 -0.76289415 -0.18758917 0.48617935 1.6957645 2.8225136 3.0582852 2.6973867 1.1781588 -0.12449026 -1.144089 -1.842551 -2.2387288 -2.2873435][-2.5208306 -2.1536465 -2.4446161 -2.1917305 -1.5734768 -0.66347528 0.078316689 0.49175167 0.59296751 -0.20107222 -1.1108229 -2.0506129 -2.6984096 -3.0330546 -3.1561964][-4.0157895 -3.7471819 -3.8312247 -4.0191431 -4.0755725 -3.2824302 -2.4975629 -2.1990569 -2.0209157 -2.3554847 -2.9182687 -3.6674764 -4.1258693 -4.1652074 -4.114398][-4.3295183 -4.7576056 -5.5911722 -5.4815369 -5.2899718 -4.9244556 -4.5539966 -4.1602988 -3.8492279 -3.9306989 -4.2162423 -4.9818759 -5.442997 -5.3028469 -4.9785948][-4.4300179 -4.6896148 -5.2236624 -5.5954905 -5.9055576 -5.4652033 -5.0796051 -4.7973938 -4.4648995 -4.5657353 -4.856473 -5.2528133 -5.4728332 -5.4155474 -5.1770954][-4.3000546 -4.5165277 -4.7664762 -4.9394646 -5.1704936 -4.8615451 -4.5280204 -4.4972968 -4.5156302 -4.4403811 -4.3198004 -4.7114367 -5.0984707 -4.8377485 -4.5029459][-3.8293469 -3.8877735 -4.0494561 -4.1732173 -4.2154694 -4.0719371 -3.9386666 -3.7485564 -3.613071 -3.8112369 -3.9239693 -3.8553214 -3.8240256 -3.9193258 -3.9424024]]...]
INFO - root - 2017-12-16 05:45:35.636867: step 26610, loss = 0.22, batch loss = 0.17 (28.6 examples/sec; 0.279 sec/batch; 23h:43m:48s remains)
INFO - root - 2017-12-16 05:45:38.496802: step 26620, loss = 0.47, batch loss = 0.41 (26.7 examples/sec; 0.300 sec/batch; 25h:29m:49s remains)
INFO - root - 2017-12-16 05:45:41.293208: step 26630, loss = 0.29, batch loss = 0.23 (28.5 examples/sec; 0.281 sec/batch; 23h:52m:53s remains)
INFO - root - 2017-12-16 05:45:44.101930: step 26640, loss = 0.30, batch loss = 0.25 (29.0 examples/sec; 0.276 sec/batch; 23h:27m:20s remains)
INFO - root - 2017-12-16 05:45:46.924914: step 26650, loss = 0.20, batch loss = 0.15 (28.7 examples/sec; 0.279 sec/batch; 23h:41m:06s remains)
INFO - root - 2017-12-16 05:45:49.756885: step 26660, loss = 0.36, batch loss = 0.30 (29.0 examples/sec; 0.276 sec/batch; 23h:27m:15s remains)
INFO - root - 2017-12-16 05:45:52.638214: step 26670, loss = 0.23, batch loss = 0.17 (29.1 examples/sec; 0.274 sec/batch; 23h:18m:55s remains)
INFO - root - 2017-12-16 05:45:55.497201: step 26680, loss = 0.23, batch loss = 0.17 (28.8 examples/sec; 0.278 sec/batch; 23h:35m:20s remains)
INFO - root - 2017-12-16 05:45:58.310784: step 26690, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 24h:17m:12s remains)
INFO - root - 2017-12-16 05:46:01.117981: step 26700, loss = 0.22, batch loss = 0.16 (28.9 examples/sec; 0.277 sec/batch; 23h:30m:39s remains)
2017-12-16 05:46:01.570060: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4686127 -4.7538533 -4.3237319 -4.3604875 -4.6002564 -5.0047684 -5.2883277 -5.2540712 -5.3891087 -5.1278305 -4.9508772 -4.4562731 -4.1809063 -4.0888638 -4.1475906][-5.7326059 -4.8788743 -4.2235212 -3.970078 -3.8603439 -4.3697033 -4.7325478 -5.0221496 -5.2134557 -4.9694076 -4.8208723 -4.2257042 -3.9436951 -3.5382187 -3.3778634][-5.4485459 -4.5839753 -3.9462378 -3.5943208 -3.3797021 -3.6781125 -3.9422541 -4.3143225 -4.6311398 -4.6413641 -4.5704556 -4.1547976 -3.8867757 -3.4331653 -3.0332658][-4.7248707 -3.5527813 -2.6203511 -2.3181558 -2.4106572 -2.5222106 -2.8674388 -3.4687362 -4.0162487 -4.1542525 -3.9658437 -3.7690957 -3.7316864 -3.5547082 -3.188282][-3.6662297 -2.0625646 -0.59644818 -0.15888453 -0.13820934 -0.42073631 -1.0768754 -1.7756178 -2.67419 -3.1029418 -3.3795772 -3.4663184 -3.5205574 -3.5104203 -3.2751775][-2.8037939 -1.0395753 0.42629528 1.2566247 1.6708875 1.536829 1.1965156 0.26822042 -0.75826526 -1.438977 -2.1775053 -2.5377424 -3.0420361 -3.2722847 -3.2267957][-2.5105705 -0.55337429 0.7822361 1.608407 2.1488981 2.5046535 2.610589 1.747539 0.58989906 -0.30334997 -1.1956282 -2.1490548 -3.0111408 -3.3768651 -3.5338445][-2.5509853 -0.86031651 0.37564898 1.2840066 1.8878875 2.4595838 2.7707944 2.213213 1.1252332 0.08462286 -0.76307583 -1.6673923 -2.5266724 -3.1338897 -3.6297607][-2.6249657 -1.6446452 -0.77268672 -0.0771904 0.49301147 1.443428 2.008697 1.6882467 0.74374104 -0.32950497 -1.2193944 -2.1130731 -2.9088683 -3.3977647 -3.8011217][-3.1484289 -3.0291967 -2.7338762 -2.2224855 -1.4555638 -0.33475256 0.4266715 0.43860626 -0.32810068 -1.3449116 -2.3354456 -3.1395302 -3.700254 -3.9627516 -4.1126957][-3.9703233 -4.083982 -4.368494 -4.1160078 -3.4330115 -2.1671219 -1.1458108 -0.99195957 -1.715246 -2.6482444 -3.4576955 -4.0751243 -4.5284042 -4.6996827 -4.6556349][-4.2479715 -4.5653625 -5.2169418 -5.5294304 -5.0859461 -4.0918808 -3.0624683 -2.6575642 -2.8788223 -3.468267 -4.0367794 -4.4457641 -4.6441293 -4.5625772 -4.542172][-4.953413 -4.9678035 -5.5069537 -5.9361544 -5.8613405 -5.1729717 -4.4793282 -4.0364504 -3.9551 -4.3504772 -4.5508718 -4.6465659 -4.691906 -4.5181289 -4.19159][-5.4315343 -5.1446366 -5.4976673 -5.7939858 -5.8954129 -5.5259275 -5.1214437 -4.7945194 -4.7094774 -4.76271 -4.7920136 -4.7410016 -4.7126875 -4.406858 -3.9817886][-6.0254059 -5.4048262 -5.19151 -5.3418612 -5.4961643 -5.3252292 -5.1485543 -5.0068836 -4.874928 -4.7824583 -4.6356111 -4.4062963 -4.2665577 -3.946244 -3.7107954]]...]
INFO - root - 2017-12-16 05:46:04.415529: step 26710, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.283 sec/batch; 24h:01m:44s remains)
INFO - root - 2017-12-16 05:46:07.202368: step 26720, loss = 0.26, batch loss = 0.20 (29.5 examples/sec; 0.271 sec/batch; 23h:02m:04s remains)
INFO - root - 2017-12-16 05:46:10.024836: step 26730, loss = 0.30, batch loss = 0.24 (29.2 examples/sec; 0.274 sec/batch; 23h:18m:19s remains)
INFO - root - 2017-12-16 05:46:12.853575: step 26740, loss = 0.27, batch loss = 0.21 (27.7 examples/sec; 0.289 sec/batch; 24h:33m:15s remains)
INFO - root - 2017-12-16 05:46:15.701374: step 26750, loss = 0.28, batch loss = 0.22 (29.3 examples/sec; 0.273 sec/batch; 23h:09m:23s remains)
INFO - root - 2017-12-16 05:46:18.485565: step 26760, loss = 0.27, batch loss = 0.21 (27.3 examples/sec; 0.293 sec/batch; 24h:55m:32s remains)
INFO - root - 2017-12-16 05:46:21.315142: step 26770, loss = 0.22, batch loss = 0.16 (29.0 examples/sec; 0.276 sec/batch; 23h:24m:32s remains)
INFO - root - 2017-12-16 05:46:24.183867: step 26780, loss = 0.31, batch loss = 0.25 (27.5 examples/sec; 0.291 sec/batch; 24h:40m:50s remains)
INFO - root - 2017-12-16 05:46:27.007845: step 26790, loss = 0.26, batch loss = 0.20 (29.1 examples/sec; 0.275 sec/batch; 23h:21m:16s remains)
INFO - root - 2017-12-16 05:46:29.794800: step 26800, loss = 0.26, batch loss = 0.20 (29.5 examples/sec; 0.271 sec/batch; 23h:02m:33s remains)
2017-12-16 05:46:30.233497: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5897412 -6.1759892 -6.6059952 -7.5392656 -8.5031052 -9.4259319 -9.9172983 -9.6438608 -8.7398548 -7.1474743 -5.5403285 -4.408392 -3.7096252 -3.2986915 -3.0347753][-5.8553963 -6.3043041 -6.65174 -7.2462659 -7.6076775 -8.1153183 -8.5033531 -8.2445717 -7.4349222 -6.1671982 -4.9637022 -4.0198932 -3.3485017 -3.0265381 -2.7688751][-6.529604 -6.672925 -6.1791983 -6.06417 -6.0377693 -5.947618 -5.93174 -5.7957878 -5.5627251 -5.0383849 -4.3203049 -3.6595354 -3.1681051 -2.8579788 -2.6692739][-6.7825842 -6.6480684 -5.5990124 -4.6719346 -3.89556 -3.4488862 -3.1605363 -3.0807991 -3.2678943 -3.4208579 -3.4203882 -3.1165347 -2.9308114 -2.8790956 -2.9800158][-6.499186 -5.8289628 -4.3058167 -2.8861058 -1.5533028 -0.58473349 -0.30502748 -0.56935358 -1.3418345 -1.9537077 -2.3780174 -2.6493533 -2.7737966 -2.8714046 -3.107594][-5.6377339 -4.7150235 -2.8816221 -1.0452075 0.5605526 1.6465631 1.7291594 1.1938357 0.28780556 -0.66292405 -1.6137292 -2.3301346 -2.5361061 -2.6487424 -2.9043186][-4.358355 -3.6127763 -2.0914478 -0.30747032 1.3902197 2.4385977 2.6189785 2.1778116 1.2348251 0.24366713 -0.71553969 -1.6157334 -2.213484 -2.4179056 -2.4581444][-3.2567692 -2.8904388 -1.9380593 -0.56138587 0.85520411 2.0588851 2.6739736 2.4858084 1.8080659 0.84872675 -0.20417213 -1.1756694 -1.8614461 -2.0693924 -2.105572][-2.6311522 -2.5140543 -2.1005177 -1.2639832 -0.065096378 1.1753917 1.8684363 2.0406618 1.702188 0.70927811 -0.28153133 -0.94742393 -1.3948333 -1.6459906 -1.7488685][-2.82277 -3.079334 -3.0779824 -2.5142655 -1.4859807 -0.21027851 0.55248737 0.87196589 0.69107008 0.088869572 -0.53440833 -1.1516979 -1.4241271 -1.5111699 -1.5771337][-3.4420209 -3.8282666 -3.7748027 -3.2195759 -2.4453325 -1.46738 -0.83807826 -0.67687249 -0.88466573 -1.2152932 -1.5129027 -1.6231654 -1.6566429 -1.7135963 -1.7918606][-4.3057652 -4.5255775 -4.2666373 -3.6385167 -2.8799517 -2.2664568 -2.0152183 -2.1002915 -2.3012743 -2.5715306 -2.6644058 -2.5981088 -2.5517387 -2.5044432 -2.5031972][-5.3488808 -5.3223968 -4.9285569 -4.1219382 -3.3983517 -2.9925811 -2.8983548 -3.1124973 -3.3517246 -3.4800761 -3.4329047 -3.3683615 -3.2482476 -3.13542 -3.1298018][-6.0147085 -5.6906867 -5.0059471 -4.2910371 -3.7792733 -3.4817948 -3.5632913 -3.7841995 -4.010963 -4.1136637 -4.0259309 -3.855376 -3.7285624 -3.6011209 -3.5635722][-6.2729917 -5.8101754 -5.0428171 -4.3304148 -3.7669606 -3.4528186 -3.4469471 -3.6924968 -3.9413855 -3.9951155 -3.8891921 -3.6923294 -3.6960621 -3.6624122 -3.7191455]]...]
INFO - root - 2017-12-16 05:46:33.069941: step 26810, loss = 0.24, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 24h:13m:01s remains)
INFO - root - 2017-12-16 05:46:35.880331: step 26820, loss = 0.19, batch loss = 0.13 (29.0 examples/sec; 0.276 sec/batch; 23h:25m:23s remains)
INFO - root - 2017-12-16 05:46:38.750789: step 26830, loss = 0.22, batch loss = 0.16 (29.1 examples/sec; 0.275 sec/batch; 23h:22m:22s remains)
INFO - root - 2017-12-16 05:46:41.593345: step 26840, loss = 0.25, batch loss = 0.20 (28.0 examples/sec; 0.286 sec/batch; 24h:17m:34s remains)
INFO - root - 2017-12-16 05:46:44.364308: step 26850, loss = 0.28, batch loss = 0.23 (29.6 examples/sec; 0.270 sec/batch; 22h:55m:59s remains)
INFO - root - 2017-12-16 05:46:47.174788: step 26860, loss = 0.25, batch loss = 0.19 (28.7 examples/sec; 0.279 sec/batch; 23h:41m:23s remains)
INFO - root - 2017-12-16 05:46:50.014123: step 26870, loss = 0.22, batch loss = 0.16 (27.7 examples/sec; 0.289 sec/batch; 24h:31m:04s remains)
INFO - root - 2017-12-16 05:46:52.863384: step 26880, loss = 0.25, batch loss = 0.19 (27.5 examples/sec; 0.290 sec/batch; 24h:39m:36s remains)
INFO - root - 2017-12-16 05:46:55.687405: step 26890, loss = 0.20, batch loss = 0.14 (29.0 examples/sec; 0.276 sec/batch; 23h:24m:55s remains)
INFO - root - 2017-12-16 05:46:58.496218: step 26900, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 24h:16m:39s remains)
2017-12-16 05:46:58.962581: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4421451 -3.2396615 -3.024292 -2.653666 -2.2894285 -1.9536839 -1.9822679 -2.23707 -2.512661 -2.684926 -2.90847 -3.025773 -2.7946029 -2.7324014 -2.8737659][-3.4931142 -3.337966 -3.1721456 -2.789494 -2.371995 -2.0095925 -2.0742762 -2.202744 -2.2499566 -2.4494393 -2.7975359 -3.0310345 -2.9257052 -3.0361249 -3.3464582][-3.6377509 -3.4132013 -3.1536169 -2.7508039 -2.3722916 -1.9504006 -1.8522036 -1.8888044 -1.9735327 -2.0771987 -2.3196535 -2.8226824 -3.0993581 -3.2955933 -3.6463096][-3.187541 -3.1201739 -3.13548 -2.9924364 -2.8081651 -2.3834236 -2.1504252 -1.8840642 -1.6501462 -1.6947184 -2.0101933 -2.5083864 -2.8983326 -3.4118652 -3.9406395][-2.5844231 -2.6409221 -2.7996011 -2.7048268 -2.5214152 -2.1607678 -1.8069682 -1.3216927 -0.94907761 -0.95943046 -1.0423155 -1.4713185 -1.9520864 -2.5601535 -3.0943408][-1.6572444 -1.9292629 -2.3013239 -2.4272995 -2.3394914 -1.9543939 -1.4105239 -0.82560849 -0.40723276 -0.32432652 -0.29102564 -0.59138155 -0.99191689 -1.5320611 -2.0651863][-0.82908535 -1.259099 -1.7141247 -1.9338303 -1.9684577 -1.6132443 -1.0873461 -0.47228789 -0.050105095 0.061800957 0.15805054 0.054795742 -0.26132059 -0.82074761 -1.1847143][-0.21987772 -0.66568995 -1.0734415 -1.3126292 -1.4053595 -1.0366209 -0.58862638 -0.20913458 -0.027028084 0.12095833 0.34928036 0.29120779 0.0843668 -0.31128836 -0.43783426][0.063884258 -0.29980183 -0.56542206 -0.7173214 -0.71043062 -0.47774291 -0.21655226 0.050749779 0.14260912 0.28149796 0.58976364 0.55806446 0.38904 0.083455563 -0.061074734][-0.17605114 -0.15183926 -0.14337111 -0.22168159 -0.14777136 0.10288572 0.162189 0.20378304 0.1992383 0.14311504 0.30528641 0.30935526 0.25502634 -0.033682823 -0.16279459][-0.77979088 -0.49043131 -0.31145287 -0.22527981 0.011775017 0.30151844 0.26630831 0.0071816444 -0.28680658 -0.44555831 -0.39822292 -0.52999473 -0.60006475 -0.71672773 -0.72256279][-1.7915223 -1.1131384 -0.52180433 -0.29022264 -0.029258251 0.28436995 0.19243622 -0.27726412 -0.74587584 -1.0998745 -1.331444 -1.5514524 -1.5826166 -1.6210063 -1.5795407][-2.5352101 -1.5326245 -0.76272964 -0.31501484 0.1230607 0.44183826 0.3308835 -0.32623816 -1.0869031 -1.5069046 -1.815181 -2.2514598 -2.469075 -2.41462 -2.2681606][-3.3257694 -2.1120119 -1.0048802 -0.31047678 0.15086651 0.40492964 0.1203084 -0.68445587 -1.4344633 -2.0881019 -2.6506045 -2.9549375 -2.9181561 -2.7890964 -2.6241684][-3.6679349 -2.3512397 -1.3074081 -0.55207777 -0.10562944 0.031813145 -0.34160566 -1.2725563 -2.2224293 -2.8304462 -3.2660346 -3.3759561 -3.2669494 -2.9597802 -2.6244326]]...]
INFO - root - 2017-12-16 05:47:01.787131: step 26910, loss = 0.25, batch loss = 0.19 (29.0 examples/sec; 0.276 sec/batch; 23h:25m:04s remains)
INFO - root - 2017-12-16 05:47:04.626229: step 26920, loss = 0.32, batch loss = 0.26 (25.7 examples/sec; 0.311 sec/batch; 26h:22m:42s remains)
INFO - root - 2017-12-16 05:47:07.443758: step 26930, loss = 0.20, batch loss = 0.15 (28.1 examples/sec; 0.285 sec/batch; 24h:10m:54s remains)
INFO - root - 2017-12-16 05:47:10.354276: step 26940, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.286 sec/batch; 24h:15m:49s remains)
INFO - root - 2017-12-16 05:47:13.127581: step 26950, loss = 0.22, batch loss = 0.17 (29.5 examples/sec; 0.271 sec/batch; 22h:59m:11s remains)
INFO - root - 2017-12-16 05:47:15.968405: step 26960, loss = 0.25, batch loss = 0.19 (27.6 examples/sec; 0.289 sec/batch; 24h:33m:39s remains)
INFO - root - 2017-12-16 05:47:18.814673: step 26970, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.285 sec/batch; 24h:12m:44s remains)
INFO - root - 2017-12-16 05:47:21.646988: step 26980, loss = 0.22, batch loss = 0.17 (25.6 examples/sec; 0.313 sec/batch; 26h:33m:37s remains)
INFO - root - 2017-12-16 05:47:24.491228: step 26990, loss = 0.32, batch loss = 0.26 (29.2 examples/sec; 0.274 sec/batch; 23h:15m:58s remains)
INFO - root - 2017-12-16 05:47:27.286972: step 27000, loss = 0.26, batch loss = 0.21 (28.2 examples/sec; 0.283 sec/batch; 24h:03m:19s remains)
2017-12-16 05:47:27.759698: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4488931 -4.2280207 -4.0840449 -4.4767036 -5.134831 -6.0450172 -6.6872153 -6.8457694 -6.8279982 -6.3813686 -5.9658065 -5.7306695 -5.1448727 -4.4473505 -3.7139094][-5.1241446 -5.5422692 -5.9036174 -6.1280789 -6.8933916 -7.1332974 -7.2851572 -7.2321644 -6.7728472 -6.406168 -6.1651292 -5.7781572 -5.097837 -4.4724855 -3.8816764][-5.7643719 -6.2954583 -6.7157297 -6.9613104 -6.9322186 -6.6146336 -6.6369715 -6.3338833 -5.9585867 -5.6229954 -5.4313922 -5.5132494 -5.4267883 -4.97737 -4.3749986][-6.0744648 -7.0855837 -7.4631057 -7.1762033 -6.56149 -5.5007753 -4.5356317 -3.9242749 -3.7166126 -4.2705069 -4.9700603 -5.2317944 -5.3111563 -5.3555946 -5.0842566][-5.9697223 -7.0587463 -7.2801504 -6.5318241 -4.8960719 -3.094337 -1.7509675 -0.73102903 -0.81058526 -1.9341502 -3.2281995 -4.6792006 -5.5943546 -5.5752459 -5.1049771][-4.5613136 -5.432672 -5.7565289 -4.7120185 -2.29108 0.32357502 2.3459234 3.2970557 2.6501212 0.76110935 -1.7710803 -3.9652476 -5.217207 -5.7994642 -5.5030937][-3.289042 -4.041997 -3.8872645 -2.3854439 -0.03753233 2.6719646 4.9856234 6.00749 5.0649519 2.5153236 -0.55884886 -3.31778 -5.1474447 -5.6952782 -5.2564621][-2.65509 -3.4331677 -3.3471689 -1.6885934 0.9559617 3.9264669 6.0299673 6.3504934 5.2158165 2.6840711 -0.65883231 -3.4446459 -5.066144 -5.51415 -5.1176777][-2.5708477 -3.0947285 -2.7339458 -1.5583396 0.12494612 2.6013331 4.77382 5.2773409 3.9251175 1.1411815 -1.8516653 -4.2948689 -5.7495794 -5.8994379 -5.2968621][-2.5526767 -3.2241859 -3.0480227 -2.0315607 -0.46294069 0.94910717 1.9121337 2.3498516 1.2923965 -1.0529463 -3.6275387 -5.7372665 -6.6045542 -6.4975758 -5.8068857][-3.2873862 -3.594779 -3.4955361 -3.3124847 -2.7810779 -1.5448339 -0.33436012 -0.38650751 -1.7003453 -3.3243153 -5.0634308 -6.6857133 -7.3211956 -6.9158812 -6.0209556][-4.34388 -4.5807085 -4.6520529 -4.2416215 -3.7082 -3.4994895 -3.2159529 -3.1163659 -3.63032 -4.9577732 -6.3940558 -7.1831427 -7.289022 -6.8203526 -6.0099773][-5.2485075 -5.3610425 -5.4484429 -5.419044 -5.2902031 -4.70899 -4.2613239 -4.7010932 -5.3434486 -5.9802842 -6.7692842 -7.2940636 -7.2246675 -6.619504 -5.8661771][-6.0765123 -5.9838457 -5.9666104 -5.9162383 -5.7654333 -5.3979492 -5.254271 -5.1175327 -5.3602118 -6.0528827 -6.536458 -6.6073914 -6.3848839 -5.8918514 -5.3496981][-6.80389 -6.6821632 -6.5499854 -6.3523631 -6.0512819 -5.6182508 -5.3603206 -5.0992222 -5.1336093 -5.4361038 -5.7222767 -5.6984653 -5.3478217 -4.810113 -4.3682775]]...]
INFO - root - 2017-12-16 05:47:30.605925: step 27010, loss = 0.35, batch loss = 0.29 (27.8 examples/sec; 0.288 sec/batch; 24h:24m:07s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 05:47:33.403526: step 27020, loss = 0.34, batch loss = 0.28 (29.6 examples/sec; 0.270 sec/batch; 22h:55m:19s remains)
INFO - root - 2017-12-16 05:47:36.217369: step 27030, loss = 0.31, batch loss = 0.25 (29.3 examples/sec; 0.273 sec/batch; 23h:11m:27s remains)
INFO - root - 2017-12-16 05:47:39.039794: step 27040, loss = 0.36, batch loss = 0.30 (28.8 examples/sec; 0.278 sec/batch; 23h:33m:47s remains)
INFO - root - 2017-12-16 05:47:41.865047: step 27050, loss = 0.26, batch loss = 0.20 (27.8 examples/sec; 0.288 sec/batch; 24h:26m:57s remains)
INFO - root - 2017-12-16 05:47:44.731942: step 27060, loss = 0.30, batch loss = 0.24 (27.6 examples/sec; 0.290 sec/batch; 24h:34m:28s remains)
INFO - root - 2017-12-16 05:47:47.560545: step 27070, loss = 0.27, batch loss = 0.21 (28.4 examples/sec; 0.282 sec/batch; 23h:54m:31s remains)
INFO - root - 2017-12-16 05:47:50.353530: step 27080, loss = 0.28, batch loss = 0.22 (28.3 examples/sec; 0.283 sec/batch; 23h:58m:28s remains)
INFO - root - 2017-12-16 05:47:53.183018: step 27090, loss = 0.33, batch loss = 0.27 (28.2 examples/sec; 0.284 sec/batch; 24h:04m:59s remains)
INFO - root - 2017-12-16 05:47:56.002007: step 27100, loss = 0.36, batch loss = 0.30 (28.2 examples/sec; 0.283 sec/batch; 24h:02m:32s remains)
2017-12-16 05:47:56.461252: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0222602 -3.4614451 -3.9156597 -4.2112074 -4.29953 -4.0780311 -3.7237742 -3.2735851 -3.0450304 -2.7586813 -2.5695724 -2.8000336 -2.9779305 -2.8341069 -2.5085959][-2.3403797 -2.770102 -3.2473407 -3.5788796 -3.591361 -3.3249521 -3.1118429 -2.8727612 -2.6218047 -2.3463411 -2.2926428 -2.3730268 -2.35411 -2.1344662 -1.7621586][-1.6681008 -2.0776203 -2.4959075 -2.8161631 -2.9907291 -2.8684373 -2.5626116 -2.363853 -2.1978111 -2.1313803 -2.0847733 -2.0164752 -1.9028101 -1.7442448 -1.4772291][-1.1654592 -1.38532 -1.6506751 -1.8518233 -1.8487859 -1.7685382 -1.7218611 -1.593241 -1.3853605 -1.3516181 -1.3758948 -1.3450572 -1.3725464 -1.341526 -1.2417221][-0.64454293 -0.80581164 -0.92661142 -0.85433125 -0.6439743 -0.596061 -0.6072669 -0.52177811 -0.59876895 -0.72996354 -0.8318243 -0.86086679 -0.94218278 -1.0494428 -1.190511][-0.96383 -0.94037175 -0.82700944 -0.75988817 -0.48720813 -0.077700138 0.21344471 0.20277214 0.042777061 -0.10361814 -0.2897296 -0.36134577 -0.47075939 -0.71061683 -1.0884817][-1.6009696 -1.7324696 -1.605834 -1.2477055 -0.74894238 -0.43344188 -0.10755444 0.17889738 0.30202627 0.14446402 -0.087502 -0.1329422 -0.32999802 -0.67969251 -0.92244506][-2.2481687 -2.646122 -2.7266331 -2.4472663 -1.7939699 -1.2013035 -0.61804008 -0.12240267 0.090579033 0.10131931 0.029612541 0.042443275 -0.0371933 -0.24798346 -0.68864131][-2.7912903 -3.3331661 -3.6384275 -3.5211117 -3.0187771 -2.263309 -1.414115 -0.924392 -0.65487218 -0.41698074 -0.40723467 -0.38534069 -0.3444066 -0.29297829 -0.49643993][-3.7027364 -4.4145889 -4.7279344 -4.6320839 -4.1306243 -3.4555416 -2.7522602 -1.9369514 -1.2397671 -0.948488 -0.87367296 -0.90342021 -0.91070533 -0.71468472 -0.6130383][-3.8771091 -4.7502975 -5.2611237 -5.2209783 -4.8003397 -4.1836009 -3.3972073 -2.7115884 -2.2823641 -1.9251173 -1.5822606 -1.3857512 -1.1867073 -0.99794173 -0.94011688][-4.1821623 -4.8000803 -5.24429 -5.4246006 -5.203095 -4.5709205 -3.8959394 -3.4570134 -2.9874063 -2.6249373 -2.3442338 -1.9650736 -1.5337048 -1.0750167 -0.84661841][-4.5301638 -4.7247515 -4.9360118 -4.9454103 -4.7375283 -4.320756 -3.8851671 -3.5067868 -3.2048614 -3.0903993 -2.8599749 -2.2882226 -1.6509705 -1.0603693 -0.82286406][-4.6572647 -4.4914856 -4.2568822 -4.100709 -4.0391774 -3.7569075 -3.5658662 -3.5016754 -3.4662423 -3.3637068 -2.920249 -2.2899559 -1.5538046 -0.94801521 -0.66729665][-4.4195409 -4.0098906 -3.6651771 -3.394043 -3.3400269 -3.2162323 -3.232089 -3.338973 -3.5509138 -3.4622869 -2.9479747 -2.1393855 -1.0880132 -0.41776466 -0.2469902]]...]
INFO - root - 2017-12-16 05:47:59.285018: step 27110, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 24h:10m:15s remains)
INFO - root - 2017-12-16 05:48:02.115579: step 27120, loss = 0.29, batch loss = 0.24 (28.8 examples/sec; 0.278 sec/batch; 23h:35m:09s remains)
INFO - root - 2017-12-16 05:48:05.001550: step 27130, loss = 0.24, batch loss = 0.19 (27.7 examples/sec; 0.289 sec/batch; 24h:29m:13s remains)
INFO - root - 2017-12-16 05:48:07.822390: step 27140, loss = 0.30, batch loss = 0.25 (27.8 examples/sec; 0.288 sec/batch; 24h:24m:33s remains)
INFO - root - 2017-12-16 05:48:10.664401: step 27150, loss = 0.24, batch loss = 0.19 (28.5 examples/sec; 0.280 sec/batch; 23h:47m:19s remains)
INFO - root - 2017-12-16 05:48:13.442965: step 27160, loss = 0.25, batch loss = 0.19 (27.9 examples/sec; 0.287 sec/batch; 24h:18m:23s remains)
INFO - root - 2017-12-16 05:48:16.259690: step 27170, loss = 0.27, batch loss = 0.22 (27.8 examples/sec; 0.287 sec/batch; 24h:22m:50s remains)
INFO - root - 2017-12-16 05:48:19.077303: step 27180, loss = 0.22, batch loss = 0.17 (28.8 examples/sec; 0.278 sec/batch; 23h:35m:04s remains)
INFO - root - 2017-12-16 05:48:21.958454: step 27190, loss = 0.22, batch loss = 0.16 (27.4 examples/sec; 0.292 sec/batch; 24h:47m:57s remains)
INFO - root - 2017-12-16 05:48:24.807049: step 27200, loss = 0.30, batch loss = 0.24 (29.0 examples/sec; 0.276 sec/batch; 23h:23m:49s remains)
2017-12-16 05:48:25.274564: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0094609 -6.1791592 -6.1088023 -6.0490503 -6.016181 -5.9746189 -5.944901 -6.046607 -6.1700792 -6.1913166 -6.1904483 -6.074625 -5.8163581 -5.4247265 -5.0810709][-6.0495563 -6.0002232 -5.90728 -5.9886041 -6.0131984 -6.2381182 -6.4221182 -6.3680673 -6.3951015 -6.6093006 -6.6993752 -6.4661045 -6.157835 -5.8265657 -5.5579348][-5.2757578 -5.2279019 -5.230566 -5.2386618 -5.3315387 -5.56934 -5.6161442 -5.8407831 -6.1320806 -6.2663126 -6.3905449 -6.5009642 -6.4256649 -6.0371332 -5.758975][-4.3058429 -3.8820031 -3.6487689 -3.7299278 -3.8686781 -4.0550113 -4.08521 -4.2407064 -4.4382644 -4.8501496 -5.215837 -5.4804306 -5.6611748 -5.57287 -5.4944792][-2.6387324 -1.9130216 -1.544651 -1.695658 -1.918514 -1.9625685 -1.7847657 -1.9051032 -2.1803339 -2.7303743 -3.3509007 -3.9089756 -4.333487 -4.4420743 -4.4129024][-1.4689326 -0.6679554 -0.12676764 0.09214735 0.15081549 0.21140194 0.61160707 0.814991 0.599452 -0.039948463 -0.8812418 -1.9486017 -2.8039026 -3.1627045 -3.2881124][-0.67439127 0.081176758 0.51117468 0.70853758 1.0801425 1.7743444 2.719418 3.0828428 2.7662821 2.1414862 1.2842565 0.13640213 -0.92260265 -1.692868 -2.1741657][-0.2511816 0.36797333 0.78679276 1.0761542 1.419682 2.0764041 3.114337 3.849494 3.8487225 3.1637206 2.1668024 0.98248005 -0.072341919 -0.6829493 -1.2103605][-0.40002346 -0.083503246 0.133183 0.39194918 0.87763262 1.6154232 2.5405803 3.0338087 3.0795155 2.6892071 1.9366035 0.90590858 -0.035038471 -0.64016438 -1.0480731][-0.735698 -0.61465263 -0.65222549 -0.50834394 -0.13178158 0.44076347 1.2052383 1.6594276 1.6761556 1.2623496 0.65808392 -0.026376247 -0.628742 -0.94609118 -1.1769381][-1.4407473 -1.2423193 -1.1477029 -1.1222575 -0.90049052 -0.58896089 -0.15497494 -0.016113281 -0.081558704 -0.41123629 -0.82414126 -1.2777109 -1.617465 -1.646091 -1.6934953][-1.7470422 -1.4765799 -1.3999851 -1.4128957 -1.2637537 -1.0921891 -0.953825 -1.1385729 -1.441364 -1.910475 -2.3048737 -2.4029763 -2.4458506 -2.1044281 -1.8346694][-1.9161978 -1.5638514 -1.3626678 -1.3966658 -1.3932221 -1.3609092 -1.302202 -1.731998 -2.2714875 -2.8606358 -3.2567468 -3.2702446 -3.1731429 -2.6238127 -2.0580347][-1.9893639 -1.5099146 -1.2289462 -1.1092184 -1.0115752 -1.0997889 -1.074193 -1.6691267 -2.5101264 -3.3349433 -3.8862336 -3.795507 -3.470664 -2.8740115 -2.3362219][-2.043819 -1.5140975 -1.125802 -1.0206251 -0.89882278 -0.81437564 -0.76232791 -1.3752718 -2.2906752 -3.2477064 -3.9623165 -4.1062303 -3.9729156 -3.2943206 -2.6833425]]...]
INFO - root - 2017-12-16 05:48:28.132269: step 27210, loss = 0.31, batch loss = 0.25 (28.0 examples/sec; 0.286 sec/batch; 24h:13m:51s remains)
INFO - root - 2017-12-16 05:48:30.919379: step 27220, loss = 0.25, batch loss = 0.19 (28.7 examples/sec; 0.279 sec/batch; 23h:40m:05s remains)
INFO - root - 2017-12-16 05:48:33.804164: step 27230, loss = 0.24, batch loss = 0.18 (27.8 examples/sec; 0.288 sec/batch; 24h:25m:38s remains)
INFO - root - 2017-12-16 05:48:36.673503: step 27240, loss = 0.27, batch loss = 0.21 (28.7 examples/sec; 0.278 sec/batch; 23h:36m:54s remains)
INFO - root - 2017-12-16 05:48:39.467192: step 27250, loss = 0.23, batch loss = 0.17 (28.8 examples/sec; 0.277 sec/batch; 23h:31m:26s remains)
INFO - root - 2017-12-16 05:48:42.355263: step 27260, loss = 0.29, batch loss = 0.23 (28.6 examples/sec; 0.280 sec/batch; 23h:43m:56s remains)
INFO - root - 2017-12-16 05:48:45.191463: step 27270, loss = 0.20, batch loss = 0.14 (28.3 examples/sec; 0.283 sec/batch; 23h:58m:05s remains)
INFO - root - 2017-12-16 05:48:48.065927: step 27280, loss = 0.43, batch loss = 0.37 (27.1 examples/sec; 0.296 sec/batch; 25h:04m:18s remains)
INFO - root - 2017-12-16 05:48:50.914097: step 27290, loss = 0.21, batch loss = 0.15 (28.1 examples/sec; 0.285 sec/batch; 24h:09m:46s remains)
INFO - root - 2017-12-16 05:48:53.776013: step 27300, loss = 0.34, batch loss = 0.28 (27.2 examples/sec; 0.294 sec/batch; 24h:57m:28s remains)
2017-12-16 05:48:54.244855: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.704134 -3.0175414 -3.5345702 -3.774756 -3.9919956 -3.9774892 -4.1102047 -4.3084855 -4.473165 -4.4049954 -4.0025015 -3.8654473 -3.8537455 -3.9617486 -4.0089545][-3.0223346 -3.0276258 -3.4540191 -3.846494 -3.8566132 -3.9484029 -4.2591686 -4.1771398 -4.1867275 -4.3630586 -4.4766574 -4.3713489 -4.2240033 -4.5479183 -4.8808975][-3.1952877 -3.3606822 -3.6214614 -3.8745708 -3.769253 -3.4776492 -3.5078063 -3.8076978 -4.0257154 -4.024457 -4.0387926 -4.4027672 -4.6163111 -4.980391 -5.243022][-3.4228258 -3.4187982 -3.6265788 -3.5045581 -3.15198 -2.7358718 -2.37467 -2.2110655 -2.3728783 -2.8890195 -3.3196719 -3.6554022 -3.9553816 -4.6253095 -5.0030885][-3.4605656 -3.5238554 -3.523375 -3.1560674 -2.5018895 -1.8181577 -1.3822732 -1.0161645 -0.89791918 -1.1138279 -1.6668384 -2.3935013 -3.0161648 -3.5952454 -4.2286177][-3.7699881 -3.6771474 -3.4681211 -2.7853966 -1.7616105 -0.74888182 0.023590088 0.59617567 0.8044982 0.59212637 -0.018270969 -0.634434 -1.3846822 -2.351238 -3.2632575][-3.5376492 -3.5132577 -3.2283628 -2.2613096 -0.98702574 0.062508583 1.00281 1.7895985 2.3239484 2.2801862 1.773066 1.0768609 0.36700153 -0.63824892 -1.8869855][-2.918189 -2.9534864 -2.7157972 -1.866184 -0.55341268 0.512445 1.4018388 2.1814165 2.7662158 2.972259 2.7214346 2.30938 1.7613273 0.72368431 -0.67050433][-2.5476947 -2.4757166 -2.301132 -1.6557858 -0.6001122 0.35696173 0.96999311 1.723248 2.4641333 2.8349996 2.8649507 2.7109833 2.2213087 1.3317776 -0.096822739][-2.3806517 -2.2699163 -2.0866957 -1.724721 -0.99410009 -0.3108449 0.19223928 0.63289642 1.1402206 1.5700865 1.7445488 1.8296285 1.579946 0.71024227 -0.43422532][-2.0491807 -2.0763533 -2.1465569 -1.9863815 -1.5542526 -1.2070744 -0.8507421 -0.58073378 -0.24844646 0.068499088 0.4189949 0.55489826 0.45195675 -0.19952059 -1.1543605][-1.955488 -1.9693387 -2.1291213 -2.38321 -2.3110585 -2.0176249 -1.9050777 -1.7910695 -1.6385098 -1.5777366 -1.3304596 -0.98447323 -0.96150017 -1.452852 -2.1387649][-1.8903964 -2.0066109 -2.211241 -2.3055546 -2.4199781 -2.4067688 -2.3457565 -2.4002619 -2.4521947 -2.5979183 -2.6986363 -2.5152178 -2.3795457 -2.6061096 -2.9263117][-2.1980071 -2.2137816 -2.28157 -2.3805282 -2.3581457 -2.3322713 -2.4986784 -2.6555061 -2.8192327 -3.2164731 -3.520534 -3.5726635 -3.395539 -3.3058896 -3.4449389][-2.5642071 -2.4874721 -2.3466291 -2.2598283 -2.3177273 -2.3405933 -2.5129979 -2.8652472 -3.33114 -3.8704996 -4.2155256 -4.2714167 -4.1744885 -3.8879931 -3.6739526]]...]
INFO - root - 2017-12-16 05:48:57.088444: step 27310, loss = 0.24, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 24h:15m:01s remains)
INFO - root - 2017-12-16 05:48:59.954818: step 27320, loss = 0.24, batch loss = 0.18 (26.8 examples/sec; 0.299 sec/batch; 25h:18m:18s remains)
INFO - root - 2017-12-16 05:49:02.757154: step 27330, loss = 0.25, batch loss = 0.20 (28.9 examples/sec; 0.277 sec/batch; 23h:27m:12s remains)
INFO - root - 2017-12-16 05:49:05.581530: step 27340, loss = 0.28, batch loss = 0.22 (26.7 examples/sec; 0.300 sec/batch; 25h:26m:27s remains)
INFO - root - 2017-12-16 05:49:08.380408: step 27350, loss = 0.23, batch loss = 0.17 (28.6 examples/sec; 0.280 sec/batch; 23h:44m:14s remains)
INFO - root - 2017-12-16 05:49:11.204029: step 27360, loss = 0.36, batch loss = 0.30 (28.3 examples/sec; 0.283 sec/batch; 23h:59m:36s remains)
INFO - root - 2017-12-16 05:49:14.035511: step 27370, loss = 0.39, batch loss = 0.33 (28.7 examples/sec; 0.279 sec/batch; 23h:37m:38s remains)
INFO - root - 2017-12-16 05:49:16.892067: step 27380, loss = 0.25, batch loss = 0.20 (28.8 examples/sec; 0.278 sec/batch; 23h:31m:46s remains)
INFO - root - 2017-12-16 05:49:19.712881: step 27390, loss = 0.25, batch loss = 0.20 (28.2 examples/sec; 0.284 sec/batch; 24h:03m:23s remains)
INFO - root - 2017-12-16 05:49:22.520122: step 27400, loss = 0.34, batch loss = 0.29 (29.3 examples/sec; 0.273 sec/batch; 23h:08m:03s remains)
2017-12-16 05:49:23.001716: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2257171 -5.5261011 -5.6038232 -5.6448298 -5.6885395 -5.3458419 -5.125936 -4.8749743 -4.9304366 -4.8623252 -4.9819279 -4.9938388 -4.9484944 -4.7571888 -4.5050206][-4.4840827 -4.6741428 -5.0630069 -4.9684014 -4.7196217 -4.5613875 -4.4951081 -4.283998 -4.4478173 -4.5257082 -4.7286987 -4.8366528 -4.7809005 -4.3808093 -3.9378712][-4.1182122 -4.3740888 -4.48062 -4.3372426 -4.2376885 -3.7924738 -3.4213223 -3.3112512 -3.5457852 -3.6888344 -4.0933256 -4.3849578 -4.3401227 -3.9219341 -3.4729724][-2.6023729 -3.2241349 -3.9223881 -3.9215474 -3.3416743 -2.6665406 -2.2431159 -2.0239675 -2.201154 -2.5831161 -3.025712 -3.5759082 -3.8861985 -3.5762568 -3.1753917][-1.2050664 -2.0416121 -2.6200743 -2.858005 -2.3663425 -1.2542002 -0.49726033 -0.20088005 -0.47008872 -0.96713448 -1.7992339 -2.6290858 -3.0616643 -3.5261688 -3.6616626][-0.62770724 -1.5944114 -2.3457336 -2.2195945 -1.071686 0.35345173 1.3535547 1.8957977 1.822382 1.0930643 -0.033873081 -1.5660598 -2.8178251 -3.26854 -3.4056902][-1.3884504 -2.0778804 -2.0969791 -1.5885332 -0.30330563 1.4335427 2.8389225 3.5284128 3.6148548 2.7647023 1.1828671 -0.80226541 -2.377492 -3.3764782 -3.8280108][-1.9796946 -2.6249781 -2.8959761 -1.9168282 -0.078970909 2.0099444 3.5474038 4.30408 4.3393316 3.2893848 1.5799069 -0.51254892 -2.2183485 -3.2480061 -3.9167328][-4.2445312 -4.23841 -3.5504591 -2.4917927 -0.80518579 1.2953486 3.2602558 4.1901331 4.0833607 2.7486711 0.88117933 -1.2105491 -2.8279169 -3.64737 -3.8866539][-5.1626329 -5.6832824 -5.6684351 -4.3560185 -2.2820957 -0.36310387 1.1245341 1.993588 2.0774298 0.74054193 -1.0622199 -2.78671 -4.005558 -4.5324535 -4.4624057][-7.3396397 -7.2557354 -6.9718657 -6.3443542 -4.986474 -3.3611612 -1.8680608 -1.2748969 -1.4078119 -2.2051997 -3.4183054 -4.6350327 -5.262475 -5.3817992 -5.0752888][-7.6347885 -8.2486448 -8.39972 -7.6491356 -6.4104252 -5.3724718 -4.398376 -3.902559 -3.7864239 -4.2917385 -4.9243422 -5.559813 -5.897449 -5.9164162 -5.5599685][-7.2225609 -7.878808 -7.9812922 -7.6200895 -6.94825 -6.1419787 -5.2625484 -5.0382719 -5.052475 -5.2326956 -5.5502892 -5.8462119 -5.8857746 -5.8012238 -5.4353185][-6.153482 -6.9248285 -7.1635342 -6.9820461 -6.5727234 -5.9710946 -5.4762697 -5.3990421 -5.2683253 -5.2130961 -5.0748887 -5.0651855 -5.1249042 -5.0771675 -4.914124][-4.8268666 -5.348835 -5.5624504 -5.6538916 -5.381505 -4.9329276 -4.5889807 -4.464613 -4.3914042 -4.3171639 -4.1738153 -4.0855851 -4.0909791 -4.2121334 -4.3080659]]...]
INFO - root - 2017-12-16 05:49:25.860856: step 27410, loss = 0.24, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 24h:13m:58s remains)
INFO - root - 2017-12-16 05:49:28.659794: step 27420, loss = 0.29, batch loss = 0.23 (28.1 examples/sec; 0.284 sec/batch; 24h:05m:12s remains)
INFO - root - 2017-12-16 05:49:31.494877: step 27430, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.280 sec/batch; 23h:41m:27s remains)
INFO - root - 2017-12-16 05:49:34.295027: step 27440, loss = 0.34, batch loss = 0.28 (28.7 examples/sec; 0.279 sec/batch; 23h:38m:18s remains)
INFO - root - 2017-12-16 05:49:37.172432: step 27450, loss = 0.27, batch loss = 0.22 (28.0 examples/sec; 0.286 sec/batch; 24h:13m:46s remains)
INFO - root - 2017-12-16 05:49:40.035783: step 27460, loss = 0.25, batch loss = 0.20 (28.5 examples/sec; 0.281 sec/batch; 23h:48m:25s remains)
INFO - root - 2017-12-16 05:49:42.878296: step 27470, loss = 0.23, batch loss = 0.17 (26.3 examples/sec; 0.304 sec/batch; 25h:45m:06s remains)
INFO - root - 2017-12-16 05:49:45.702938: step 27480, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 23h:53m:57s remains)
INFO - root - 2017-12-16 05:49:48.541602: step 27490, loss = 0.21, batch loss = 0.15 (27.4 examples/sec; 0.292 sec/batch; 24h:43m:43s remains)
INFO - root - 2017-12-16 05:49:51.356039: step 27500, loss = 0.29, batch loss = 0.23 (27.4 examples/sec; 0.292 sec/batch; 24h:44m:21s remains)
2017-12-16 05:49:51.820727: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9611185 -3.6475172 -3.7759414 -3.8271832 -3.5483334 -3.5077267 -3.7422874 -3.7251346 -3.8570781 -3.8978958 -3.8098991 -3.6866934 -3.6041608 -3.5873954 -3.4135203][-2.3296053 -2.8253512 -2.9763043 -3.1713161 -3.0353494 -3.0114317 -3.2636547 -3.3916326 -3.6726053 -3.7375839 -3.6830025 -3.6249447 -3.4176619 -3.1844311 -2.8533807][-1.3480432 -1.9698825 -2.1989088 -2.469624 -2.5295711 -2.5235848 -2.73916 -2.8919117 -3.3245349 -3.7074156 -3.7723293 -3.7186983 -3.5965331 -3.1493189 -2.5419192][-0.58589482 -1.0446346 -1.3622913 -1.7370336 -1.8759866 -1.8617513 -2.0197589 -2.3374259 -2.9321012 -3.4376791 -3.790014 -3.8470795 -3.5394833 -2.9614623 -2.2038398][-0.20392227 -0.2556529 -0.5031774 -0.93479037 -1.0397811 -1.0053127 -1.2128637 -1.6558795 -2.4426389 -3.0928297 -3.4209032 -3.4959664 -3.1314182 -2.4470768 -1.7375793][-0.24146128 -0.20655394 -0.45691967 -0.72132778 -0.720695 -0.68233204 -0.8460803 -1.3595607 -2.193743 -2.9139619 -3.2437825 -3.0978787 -2.5966291 -1.7117424 -0.89475083][-0.78910804 -0.6393292 -0.72243667 -0.94898891 -0.87451673 -0.74063635 -0.75729656 -1.1388185 -1.8445935 -2.4149804 -2.6467276 -2.4692569 -2.0069113 -1.2711165 -0.53406572][-1.6350183 -1.4715602 -1.6157866 -1.7704146 -1.5991862 -1.2704768 -1.1090305 -1.2016511 -1.5701454 -1.8613596 -1.7898107 -1.6373811 -1.2810421 -0.731339 -0.27461529][-2.6303148 -2.4503937 -2.5254188 -2.6040006 -2.44881 -2.035069 -1.556535 -1.2908967 -1.2620769 -1.2032678 -0.84862089 -0.64025879 -0.41607761 -0.17074299 0.086671829][-3.5459037 -3.6433854 -3.6811497 -3.6193249 -3.3628016 -2.9045911 -2.2624431 -1.6822038 -1.2892926 -0.98590589 -0.60602808 -0.2672677 -0.0010957718 -0.086544514 -0.066506863][-4.7614956 -5.0845308 -5.1223445 -4.9858689 -4.5805564 -3.9503331 -3.0881541 -2.2820816 -1.7187097 -1.2509537 -0.84365606 -0.54834652 -0.40868998 -0.65095139 -0.75695181][-5.8821073 -6.4475746 -6.5456438 -6.2345233 -5.6271191 -4.8453012 -3.9696453 -3.0280485 -2.3901269 -2.0447047 -1.7700627 -1.6266732 -1.6220891 -1.9708979 -2.1110756][-6.900198 -7.6616993 -7.8622971 -7.47696 -6.6650767 -5.8682637 -5.0445061 -4.2119017 -3.6763711 -3.326335 -3.1643381 -3.2230461 -3.3180923 -3.6444457 -3.6948714][-7.7708244 -8.5605221 -8.7738771 -8.4361029 -7.690383 -6.9258957 -6.238204 -5.5613675 -5.1516752 -4.9470072 -4.8991957 -4.9878125 -5.089148 -5.3658352 -5.3066139][-8.3100538 -9.0458088 -9.2814426 -8.9470034 -8.2730808 -7.6672339 -7.2110338 -6.7598224 -6.5852365 -6.5605354 -6.6167526 -6.7868004 -6.8717175 -6.9436622 -6.6952858]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-27500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-27500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 05:49:55.184566: step 27510, loss = 0.22, batch loss = 0.16 (29.0 examples/sec; 0.276 sec/batch; 23h:21m:16s remains)
INFO - root - 2017-12-16 05:49:58.008838: step 27520, loss = 0.27, batch loss = 0.22 (27.3 examples/sec; 0.293 sec/batch; 24h:50m:04s remains)
INFO - root - 2017-12-16 05:50:00.874069: step 27530, loss = 0.31, batch loss = 0.26 (27.9 examples/sec; 0.287 sec/batch; 24h:18m:59s remains)
INFO - root - 2017-12-16 05:50:03.740665: step 27540, loss = 0.29, batch loss = 0.23 (27.5 examples/sec; 0.291 sec/batch; 24h:36m:40s remains)
INFO - root - 2017-12-16 05:50:06.594248: step 27550, loss = 0.20, batch loss = 0.15 (28.3 examples/sec; 0.282 sec/batch; 23h:54m:49s remains)
INFO - root - 2017-12-16 05:50:09.423523: step 27560, loss = 0.25, batch loss = 0.19 (29.1 examples/sec; 0.275 sec/batch; 23h:18m:50s remains)
INFO - root - 2017-12-16 05:50:12.279128: step 27570, loss = 0.29, batch loss = 0.23 (28.1 examples/sec; 0.285 sec/batch; 24h:06m:47s remains)
INFO - root - 2017-12-16 05:50:15.070722: step 27580, loss = 0.30, batch loss = 0.24 (28.5 examples/sec; 0.281 sec/batch; 23h:46m:20s remains)
INFO - root - 2017-12-16 05:50:17.884250: step 27590, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 23h:30m:55s remains)
INFO - root - 2017-12-16 05:50:20.726353: step 27600, loss = 0.34, batch loss = 0.28 (28.0 examples/sec; 0.285 sec/batch; 24h:09m:48s remains)
2017-12-16 05:50:21.198655: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6039562 -5.1697392 -4.7801476 -4.5647354 -4.5664897 -4.9892368 -5.7114391 -6.2405443 -6.3560505 -6.3368311 -6.3204603 -6.369462 -6.1456752 -6.0821738 -5.9250288][-5.0050182 -4.435183 -3.89003 -3.8827069 -4.0504165 -4.5525446 -5.2377248 -5.6861744 -5.7911005 -5.8971453 -6.1441827 -6.2064819 -6.0895348 -6.0315995 -5.82705][-4.597362 -3.8262193 -3.1194196 -2.714406 -2.6441202 -3.197669 -3.9493344 -4.3601379 -4.5625577 -4.8979192 -5.3641315 -5.8718367 -6.1658082 -6.2122908 -6.0763459][-3.5645764 -2.8965063 -2.1832473 -1.7087281 -1.6083446 -1.5919826 -1.7024593 -2.200619 -2.9937849 -3.5810492 -4.2608242 -5.043345 -5.6624918 -6.0967526 -6.1930227][-2.3933432 -1.4335687 -0.43352938 0.046458721 0.27676439 0.26072168 0.14341021 -0.28825283 -0.99943042 -2.0616262 -3.2831872 -4.4345522 -5.3289127 -5.88149 -6.1010914][-1.6070824 -0.55592656 0.64194918 1.6657472 2.4135723 2.4940834 2.3835392 1.9130635 1.1252174 -0.027237415 -1.2768445 -2.8168354 -4.1798968 -5.2613144 -5.8883371][-1.0796981 -0.12817955 1.00004 2.3212748 3.2695036 3.7581816 3.9744825 3.5864515 2.7648377 1.4904432 0.051178455 -1.7418759 -3.4429784 -4.6803293 -5.3507562][-1.3610866 -0.45903873 0.73054886 2.1322656 3.1549144 3.802659 4.09665 3.818716 3.073576 1.7618728 0.26331329 -1.6105783 -3.3803639 -4.7656274 -5.4856977][-2.6336744 -1.9658372 -0.8387804 0.4874382 1.6571436 2.4955535 2.9811954 2.8341703 2.1308222 0.8980546 -0.53576732 -2.3090663 -3.8843956 -5.0219316 -5.4764957][-3.6889663 -3.320045 -2.9312711 -1.8146775 -0.63357759 0.25602531 0.83366346 0.89970922 0.54475975 -0.27236462 -1.4334371 -3.0627725 -4.6430078 -5.9022903 -6.29065][-4.7676306 -4.7426953 -4.3622646 -3.6025293 -2.9473581 -2.1378422 -1.4989743 -1.364615 -1.5606258 -2.1641266 -2.9308114 -4.0931258 -5.305542 -6.4995995 -7.0592][-5.416544 -5.4412675 -5.4874735 -5.1755033 -4.8377686 -4.2371159 -3.8645542 -3.5284586 -3.3527682 -3.7249637 -4.3176556 -5.3069487 -6.1592817 -6.8227234 -7.2485161][-5.787591 -6.0500717 -6.2573805 -6.140655 -6.0979252 -5.7179451 -5.3506637 -4.7890172 -4.4226165 -4.4672036 -4.7587132 -5.4071693 -6.0293474 -6.6772628 -7.0710235][-4.9920878 -5.5124254 -5.9948912 -6.252183 -6.3742433 -6.0412097 -5.6807361 -5.2214513 -4.86223 -4.7506866 -4.8457575 -5.285182 -5.7087951 -6.1308928 -6.4109035][-4.6658421 -4.9446411 -5.2899642 -5.6774249 -5.9500227 -5.8255463 -5.5773778 -5.1203632 -4.7785974 -4.7433848 -4.8814354 -4.9946637 -5.0086126 -5.1708364 -5.2456436]]...]
INFO - root - 2017-12-16 05:50:24.035718: step 27610, loss = 0.25, batch loss = 0.19 (27.7 examples/sec; 0.289 sec/batch; 24h:28m:55s remains)
INFO - root - 2017-12-16 05:50:26.868322: step 27620, loss = 0.22, batch loss = 0.16 (27.7 examples/sec; 0.289 sec/batch; 24h:29m:04s remains)
INFO - root - 2017-12-16 05:50:29.739859: step 27630, loss = 0.29, batch loss = 0.23 (28.3 examples/sec; 0.283 sec/batch; 23h:58m:46s remains)
INFO - root - 2017-12-16 05:50:32.569486: step 27640, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.281 sec/batch; 23h:48m:32s remains)
INFO - root - 2017-12-16 05:50:35.377297: step 27650, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.277 sec/batch; 23h:27m:55s remains)
INFO - root - 2017-12-16 05:50:38.238701: step 27660, loss = 0.22, batch loss = 0.16 (25.5 examples/sec; 0.314 sec/batch; 26h:34m:42s remains)
INFO - root - 2017-12-16 05:50:41.071133: step 27670, loss = 0.27, batch loss = 0.21 (26.8 examples/sec; 0.299 sec/batch; 25h:17m:12s remains)
INFO - root - 2017-12-16 05:50:43.943259: step 27680, loss = 0.40, batch loss = 0.34 (28.4 examples/sec; 0.282 sec/batch; 23h:53m:19s remains)
INFO - root - 2017-12-16 05:50:46.727225: step 27690, loss = 0.31, batch loss = 0.25 (28.8 examples/sec; 0.278 sec/batch; 23h:29m:44s remains)
INFO - root - 2017-12-16 05:50:49.571182: step 27700, loss = 0.25, batch loss = 0.19 (27.5 examples/sec; 0.291 sec/batch; 24h:39m:33s remains)
2017-12-16 05:50:50.029280: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4601994 -3.8485117 -4.2118416 -4.3234611 -4.4724436 -4.3661513 -4.1978335 -4.0918531 -3.9201748 -3.7790406 -3.6251488 -3.81363 -3.7960982 -4.1387691 -4.3509703][-2.7105339 -3.1218154 -3.6736183 -4.1991115 -4.7637305 -4.9813275 -4.9642358 -4.82047 -4.6427197 -4.2606845 -4.0578337 -4.1399784 -4.1699963 -4.5012975 -4.6909084][-2.0363977 -2.5952187 -3.3880341 -4.0064993 -4.48736 -4.7535987 -4.66652 -4.6309457 -4.5521803 -4.3551879 -4.2142439 -4.1217971 -4.2350092 -4.6555333 -4.9535055][-1.9281888 -2.2463851 -2.797143 -3.3298769 -3.672893 -3.805722 -3.6952868 -3.5400367 -3.4928114 -3.4934013 -3.5962286 -3.8571587 -4.409656 -4.8983192 -5.1511521][-2.1961658 -2.3465557 -2.6856813 -2.8653903 -2.798084 -2.5963874 -2.1637897 -1.8517883 -1.7763724 -1.9986086 -2.4110525 -3.0987642 -3.8154395 -4.4647307 -5.0108285][-2.2649145 -2.3014295 -2.2381845 -1.9282644 -1.3501077 -0.69326353 0.22725344 0.64655447 0.55949354 0.068594933 -0.80055833 -1.7021174 -2.7668786 -3.6572537 -4.0006957][-2.60759 -2.1918402 -1.5958185 -0.67666125 0.52571392 1.6549416 2.9059324 3.3162293 3.2224436 2.3876414 1.0202041 -0.32047224 -1.5991442 -2.3474486 -2.7428026][-2.8024731 -2.3266492 -1.5530345 -0.1221981 1.2971172 2.8434405 4.4219646 5.0435839 4.9802895 3.8159122 2.4077048 0.95219135 -0.56660771 -1.4957516 -2.0275135][-3.4215877 -2.9937534 -2.2979019 -0.97780704 0.45964336 2.3071041 3.9104776 4.5834551 4.7623186 3.8134985 2.4917097 0.95781136 -0.3097105 -1.0133851 -1.5231416][-4.0196233 -4.0066385 -3.784163 -2.6606226 -1.2404325 0.32282495 1.7728953 2.7965021 3.215507 2.5466528 1.5559382 0.23036766 -0.84214306 -1.3667581 -1.6952617][-5.0413289 -5.1462703 -5.1234174 -4.4577932 -3.5051403 -2.1090503 -0.79620266 0.14614248 0.67531347 0.36794329 -0.13759804 -1.0005953 -1.6876407 -1.9267516 -2.0103352][-5.2244353 -5.8039227 -6.3518605 -5.9167376 -5.4103012 -4.4963965 -3.4766304 -2.6427121 -1.793865 -1.619555 -1.7757728 -2.2777216 -2.6059644 -2.556988 -2.5407143][-5.0995483 -5.8118825 -6.5566216 -6.6100974 -6.420939 -5.6892538 -4.8219166 -4.0028973 -3.0997593 -2.6864843 -2.5473466 -2.6672652 -2.7368302 -2.6908224 -2.6982656][-5.1219029 -5.6283212 -6.1276665 -6.3230085 -6.2156448 -5.5338178 -4.8164291 -4.0867786 -3.3407621 -2.8712358 -2.4994626 -2.4681005 -2.4289324 -2.3637068 -2.3737631][-4.9688969 -5.28957 -5.5878367 -5.4728765 -5.1157455 -4.4624949 -3.7896144 -3.1432781 -2.5484397 -2.2339251 -2.0573361 -1.9617941 -1.8054564 -1.8022847 -1.9293561]]...]
INFO - root - 2017-12-16 05:50:52.871173: step 27710, loss = 0.24, batch loss = 0.19 (28.8 examples/sec; 0.278 sec/batch; 23h:30m:14s remains)
INFO - root - 2017-12-16 05:50:55.727506: step 27720, loss = 0.26, batch loss = 0.20 (27.0 examples/sec; 0.296 sec/batch; 25h:04m:02s remains)
INFO - root - 2017-12-16 05:50:58.584019: step 27730, loss = 0.25, batch loss = 0.19 (26.4 examples/sec; 0.303 sec/batch; 25h:39m:22s remains)
INFO - root - 2017-12-16 05:51:01.386228: step 27740, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 23h:52m:34s remains)
INFO - root - 2017-12-16 05:51:04.206347: step 27750, loss = 0.28, batch loss = 0.22 (27.6 examples/sec; 0.290 sec/batch; 24h:31m:17s remains)
INFO - root - 2017-12-16 05:51:07.102819: step 27760, loss = 0.32, batch loss = 0.26 (25.7 examples/sec; 0.312 sec/batch; 26h:23m:24s remains)
INFO - root - 2017-12-16 05:51:09.952267: step 27770, loss = 0.34, batch loss = 0.29 (28.7 examples/sec; 0.279 sec/batch; 23h:35m:59s remains)
INFO - root - 2017-12-16 05:51:12.771572: step 27780, loss = 0.24, batch loss = 0.18 (29.6 examples/sec; 0.270 sec/batch; 22h:51m:13s remains)
INFO - root - 2017-12-16 05:51:15.595584: step 27790, loss = 0.31, batch loss = 0.25 (28.8 examples/sec; 0.278 sec/batch; 23h:32m:49s remains)
INFO - root - 2017-12-16 05:51:18.425193: step 27800, loss = 0.39, batch loss = 0.33 (28.7 examples/sec; 0.279 sec/batch; 23h:35m:42s remains)
2017-12-16 05:51:18.875052: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.753963 -4.5850735 -5.404748 -6.2365551 -6.7841787 -7.0066977 -7.3342619 -8.1562653 -8.9786158 -9.3290844 -9.5400372 -9.59157 -9.2140675 -8.6594067 -8.302145][-4.2807817 -5.3681769 -6.3289895 -7.161725 -7.7381606 -8.1118488 -8.4582615 -8.6270866 -8.9259281 -9.6453953 -10.048601 -9.934948 -9.547286 -9.0739918 -8.6303864][-4.7106047 -5.9512863 -6.8346 -7.414629 -7.5632596 -7.4920807 -7.4328508 -7.4865828 -7.8448424 -8.0834208 -8.3196278 -8.6539841 -8.7232666 -8.5306606 -8.2313423][-4.9690804 -6.1859426 -7.1063657 -7.3483057 -6.8629475 -5.965487 -5.20502 -4.7271671 -4.6160717 -5.1220865 -5.6697955 -6.3421297 -6.8242993 -6.8397183 -7.0133705][-4.846127 -5.7297668 -6.23307 -6.0896382 -5.3328943 -3.9152772 -2.3791854 -1.3516433 -1.2004051 -1.4128625 -2.1656837 -3.3189151 -4.0227032 -4.6467113 -5.1322355][-4.5355043 -5.1805019 -5.2180538 -4.4167638 -2.819459 -0.92325091 0.68101549 1.9559736 2.4614763 1.7844381 0.82382345 -0.20330191 -1.3120849 -2.2352068 -2.9157624][-4.0799341 -4.2913146 -4.1921306 -3.1701458 -1.2743638 0.989583 3.2876678 4.5024786 4.390336 3.8920603 3.1712041 1.8751249 0.62662268 -0.46629095 -1.1335716][-3.9068944 -4.0077982 -3.5774846 -2.1547217 -0.20929956 1.9959555 3.8357782 4.9405766 5.2240629 4.5750418 3.7438431 2.7811852 1.6586404 0.62415314 -0.026903629][-3.7383549 -3.7137616 -3.2599397 -2.005487 -0.17300987 1.6227331 3.2000456 4.0023012 4.0667639 3.7052851 3.1006794 2.1696424 1.1371417 0.043102741 -0.61438036][-3.7003577 -3.6878598 -3.2891879 -2.3899248 -1.213702 0.18234825 1.22854 1.7629895 2.0611873 1.655911 1.1440687 0.39141178 -0.61265135 -1.4662247 -1.9836609][-4.0770345 -4.0763535 -4.0429578 -3.5334482 -2.6725631 -1.9850214 -1.2871842 -1.0675919 -1.0307083 -1.1126516 -1.6922164 -2.4006596 -3.2265162 -3.898834 -4.3153095][-4.5563169 -4.7247524 -4.5846586 -4.5620656 -4.5771003 -4.2364063 -3.9778352 -3.91769 -3.8453779 -4.2828746 -4.786447 -5.3926716 -6.1245837 -6.4471865 -6.5753422][-5.1072531 -5.2920971 -5.3991747 -5.5312762 -5.5533648 -5.6858716 -5.7301927 -5.7861705 -5.8358574 -5.9603224 -6.3197112 -6.6185536 -7.0975332 -7.5760365 -7.6351948][-4.6577549 -4.9022617 -5.0884972 -5.3094721 -5.5627556 -5.7430167 -5.9474955 -6.0951257 -6.2127161 -6.3411617 -6.5342183 -7.1467323 -7.7328415 -7.7228889 -7.5551853][-4.6709452 -4.6494393 -4.6965733 -4.992877 -5.2696991 -5.5665007 -5.8217459 -5.9413147 -5.9701719 -6.0087366 -6.3127046 -6.5139618 -6.6902051 -6.98055 -6.9472532]]...]
INFO - root - 2017-12-16 05:51:21.759190: step 27810, loss = 0.31, batch loss = 0.26 (28.0 examples/sec; 0.286 sec/batch; 24h:12m:24s remains)
INFO - root - 2017-12-16 05:51:24.586816: step 27820, loss = 0.22, batch loss = 0.16 (29.2 examples/sec; 0.274 sec/batch; 23h:11m:14s remains)
INFO - root - 2017-12-16 05:51:27.426541: step 27830, loss = 0.23, batch loss = 0.17 (28.6 examples/sec; 0.280 sec/batch; 23h:40m:42s remains)
INFO - root - 2017-12-16 05:51:30.260849: step 27840, loss = 0.28, batch loss = 0.22 (29.4 examples/sec; 0.272 sec/batch; 23h:01m:41s remains)
INFO - root - 2017-12-16 05:51:33.063965: step 27850, loss = 0.24, batch loss = 0.18 (29.3 examples/sec; 0.273 sec/batch; 23h:06m:20s remains)
INFO - root - 2017-12-16 05:51:35.886434: step 27860, loss = 0.23, batch loss = 0.17 (29.3 examples/sec; 0.273 sec/batch; 23h:06m:50s remains)
INFO - root - 2017-12-16 05:51:38.706823: step 27870, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.285 sec/batch; 24h:05m:48s remains)
INFO - root - 2017-12-16 05:51:41.529626: step 27880, loss = 0.26, batch loss = 0.20 (29.2 examples/sec; 0.274 sec/batch; 23h:08m:55s remains)
INFO - root - 2017-12-16 05:51:44.393193: step 27890, loss = 0.23, batch loss = 0.17 (28.8 examples/sec; 0.277 sec/batch; 23h:28m:16s remains)
INFO - root - 2017-12-16 05:51:47.153168: step 27900, loss = 0.38, batch loss = 0.32 (29.3 examples/sec; 0.273 sec/batch; 23h:07m:59s remains)
2017-12-16 05:51:47.654939: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9987783 -4.7685823 -4.7730722 -5.2030869 -5.8333874 -6.370954 -6.9365711 -7.9674673 -8.7380638 -8.9925346 -8.5195189 -7.6632786 -6.522789 -5.1263704 -4.0448952][-3.7284155 -3.6861105 -3.9095874 -4.1528978 -4.4255748 -5.6772385 -7.05756 -7.9022284 -8.3328562 -8.34909 -8.1838064 -7.5520105 -6.2134342 -4.8803945 -3.9935148][-2.38268 -2.2912865 -2.4489374 -2.9355187 -3.7244916 -4.3748164 -5.1499634 -6.713027 -7.5444684 -7.5013018 -7.2322993 -6.6560283 -5.7707787 -4.5336041 -3.4930754][-1.172261 -1.1844518 -1.4733779 -1.7707427 -2.1365 -2.925041 -3.7939708 -4.3763394 -4.9771318 -5.6924706 -5.9219828 -5.823266 -5.2484288 -4.2300286 -3.1983337][-0.31258869 -0.032932281 -0.0171237 -0.50783825 -1.0206351 -0.86243773 -0.8589499 -1.5010686 -2.0242312 -2.7998421 -3.693131 -4.2142715 -3.9935222 -3.5967815 -3.0418115][-0.6262424 0.16318464 0.700459 0.67388344 0.73357391 1.2075486 1.7114382 1.7786574 1.024168 -0.30761242 -1.3642752 -2.4856811 -2.9112458 -2.5592749 -2.0086937][-1.0548501 0.032910824 0.63201857 1.2265577 1.8549032 2.4907379 3.417799 3.4359064 2.9046502 1.5125608 0.042927742 -1.1504734 -1.8813586 -2.0864937 -1.7897623][-1.4019318 -0.24248505 0.34145927 0.9891777 1.6006708 2.7859306 4.0499763 3.9113531 3.2725644 1.9525285 0.71382618 -0.54360151 -1.3880525 -1.8314807 -1.9988182][-1.9674647 -1.1510832 -0.71943283 -0.23273277 0.34038639 1.4729171 2.57474 2.9613523 2.7559958 1.3582697 0.0060276985 -1.0961149 -1.8659275 -2.2751205 -2.3145871][-2.3826795 -2.0805042 -2.4252455 -2.3832145 -1.8130291 -0.94340014 -0.18076324 0.33047628 0.4702096 -0.24286032 -1.1388938 -2.0689094 -2.7734547 -3.1427512 -3.2375245][-3.8482749 -3.6354611 -3.7873702 -4.1226149 -4.2752542 -3.5940485 -2.7165971 -2.3534226 -2.1642485 -2.4042537 -2.9686177 -3.7100458 -4.1953788 -4.3031569 -4.233809][-4.2230196 -4.6616025 -5.5511856 -5.5722437 -5.4033256 -5.1512175 -4.7984533 -4.3370008 -3.9158096 -3.9824255 -4.3082829 -4.9907966 -5.4311686 -5.3850341 -5.096653][-4.3861403 -4.6393194 -5.1810341 -5.59497 -6.0002856 -5.649272 -5.2146626 -4.9043341 -4.5080452 -4.5636678 -4.871664 -5.2477961 -5.4455628 -5.4259171 -5.239779][-4.2812138 -4.4549041 -4.7310619 -4.9314032 -5.1658654 -4.9107981 -4.6086354 -4.5667038 -4.5150766 -4.4206934 -4.2933407 -4.6673589 -5.0897884 -4.8546491 -4.5262432][-3.7836614 -3.8371387 -4.0000057 -4.1037292 -4.1674294 -4.0577087 -3.940398 -3.7612739 -3.5886617 -3.7577684 -3.8883784 -3.8329852 -3.7816279 -3.8457904 -3.8948121]]...]
INFO - root - 2017-12-16 05:51:50.484522: step 27910, loss = 0.30, batch loss = 0.24 (27.9 examples/sec; 0.287 sec/batch; 24h:14m:58s remains)
INFO - root - 2017-12-16 05:51:53.276112: step 27920, loss = 0.24, batch loss = 0.18 (29.0 examples/sec; 0.276 sec/batch; 23h:19m:25s remains)
INFO - root - 2017-12-16 05:51:56.116015: step 27930, loss = 0.32, batch loss = 0.26 (27.9 examples/sec; 0.287 sec/batch; 24h:17m:10s remains)
INFO - root - 2017-12-16 05:51:58.929392: step 27940, loss = 0.31, batch loss = 0.25 (28.4 examples/sec; 0.282 sec/batch; 23h:51m:46s remains)
INFO - root - 2017-12-16 05:52:01.762087: step 27950, loss = 0.27, batch loss = 0.21 (26.2 examples/sec; 0.305 sec/batch; 25h:49m:48s remains)
INFO - root - 2017-12-16 05:52:04.592643: step 27960, loss = 0.26, batch loss = 0.20 (28.2 examples/sec; 0.283 sec/batch; 23h:57m:45s remains)
INFO - root - 2017-12-16 05:52:07.451249: step 27970, loss = 0.27, batch loss = 0.21 (28.4 examples/sec; 0.282 sec/batch; 23h:50m:36s remains)
INFO - root - 2017-12-16 05:52:10.366383: step 27980, loss = 0.32, batch loss = 0.26 (28.8 examples/sec; 0.277 sec/batch; 23h:28m:01s remains)
INFO - root - 2017-12-16 05:52:13.193189: step 27990, loss = 0.26, batch loss = 0.20 (28.5 examples/sec; 0.281 sec/batch; 23h:45m:15s remains)
INFO - root - 2017-12-16 05:52:16.035319: step 28000, loss = 0.34, batch loss = 0.28 (29.6 examples/sec; 0.271 sec/batch; 22h:53m:21s remains)
2017-12-16 05:52:16.493417: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7473817 -4.6616087 -4.611177 -4.6290069 -4.4312148 -4.3620906 -4.3933692 -4.4250546 -4.4987888 -4.5382633 -4.45346 -4.2757106 -4.3352818 -4.4421039 -4.5771136][-4.4057813 -4.2293782 -4.275413 -4.3375783 -4.2540755 -4.2598515 -4.2423878 -4.1833162 -4.1973205 -4.2225595 -4.1253796 -3.8330593 -3.8245492 -4.1086559 -4.3030119][-3.2843289 -3.1721182 -3.4140859 -3.4662306 -3.485146 -3.5667849 -3.6005852 -3.6434438 -3.6406183 -3.7454181 -3.6912618 -3.5220461 -3.5734863 -3.8216095 -4.0067897][-1.7286081 -1.5373011 -1.9576776 -2.270931 -2.4975948 -2.688468 -2.6898994 -2.657233 -2.7269597 -3.0481954 -3.3084707 -3.41842 -3.6774836 -3.8654785 -4.10443][-0.75573349 -0.13942862 -0.36679792 -0.87449574 -1.066262 -1.1560247 -1.0817254 -1.0941362 -1.3421082 -1.9983485 -2.7086263 -3.0899289 -3.4541936 -3.8364942 -4.0766578][-0.4791 0.463655 0.4466114 0.24905634 0.36616802 0.4617548 0.77105856 0.77743244 0.44035578 -0.44971609 -1.5589993 -2.3712873 -2.938921 -3.3546038 -3.6752453][-0.826926 0.29643822 0.35152817 0.43835497 0.99786425 1.5150409 2.0804396 2.2049823 1.7738271 0.43078232 -1.0450201 -1.9291146 -2.5515635 -2.9374046 -3.2687392][-1.8708479 -0.96264505 -0.86005831 -0.47417426 0.35647249 1.3264012 2.3503075 2.5339351 2.0433803 0.74627352 -0.58364868 -1.6158509 -2.4120886 -2.9598398 -3.1821749][-3.6211805 -2.7474446 -2.5826144 -2.0287561 -0.91702366 0.35275555 1.5798359 1.9264688 1.5109167 0.40180349 -0.85612845 -1.8866234 -2.5631318 -2.9566073 -3.2657161][-4.8000121 -4.7368865 -4.877738 -4.0522857 -2.7460918 -1.4401293 -0.19753504 0.23366737 -0.10392427 -1.1493692 -2.1155126 -2.924571 -3.4675789 -3.6000202 -3.7294085][-6.2657542 -6.5357876 -6.7004976 -5.9896755 -4.7809591 -3.3787537 -2.0305448 -1.7140622 -1.9751844 -2.851254 -3.7199094 -4.3956456 -4.64266 -4.5749359 -4.3975148][-6.252346 -6.8046613 -7.5099268 -7.0877581 -6.170157 -5.0737381 -3.973495 -3.5806966 -3.6051998 -4.0485797 -4.5630407 -5.1278825 -5.2349572 -4.9769115 -4.7979445][-5.4559546 -6.0365429 -6.8063607 -6.8259039 -6.4156446 -5.6804419 -4.917593 -4.5993209 -4.4722495 -4.6551189 -4.8640079 -5.2048125 -5.2328053 -5.0463266 -4.8866024][-4.8302412 -4.9968805 -5.3646822 -5.7509227 -5.7545915 -5.3120222 -4.8908186 -4.7763543 -4.6665659 -4.4833722 -4.3738346 -4.569736 -4.6039066 -4.4994168 -4.5443926][-4.3316483 -3.9882677 -4.03375 -4.1130881 -4.1203594 -4.0969319 -4.0098619 -3.9868357 -3.8395247 -3.7664886 -3.6020334 -3.4526029 -3.2493556 -3.2998011 -3.5286055]]...]
INFO - root - 2017-12-16 05:52:19.328645: step 28010, loss = 0.36, batch loss = 0.30 (28.5 examples/sec; 0.281 sec/batch; 23h:45m:48s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 05:52:22.144778: step 28020, loss = 0.47, batch loss = 0.41 (28.5 examples/sec; 0.281 sec/batch; 23h:45m:22s remains)
INFO - root - 2017-12-16 05:52:25.000975: step 28030, loss = 0.20, batch loss = 0.14 (27.8 examples/sec; 0.288 sec/batch; 24h:19m:06s remains)
INFO - root - 2017-12-16 05:52:27.828363: step 28040, loss = 0.24, batch loss = 0.18 (29.3 examples/sec; 0.273 sec/batch; 23h:03m:41s remains)
INFO - root - 2017-12-16 05:52:30.646792: step 28050, loss = 0.21, batch loss = 0.15 (28.1 examples/sec; 0.285 sec/batch; 24h:04m:34s remains)
INFO - root - 2017-12-16 05:52:33.484730: step 28060, loss = 0.20, batch loss = 0.15 (28.6 examples/sec; 0.280 sec/batch; 23h:41m:35s remains)
INFO - root - 2017-12-16 05:52:36.337401: step 28070, loss = 0.29, batch loss = 0.23 (29.3 examples/sec; 0.273 sec/batch; 23h:05m:08s remains)
INFO - root - 2017-12-16 05:52:39.196836: step 28080, loss = 0.30, batch loss = 0.24 (29.1 examples/sec; 0.275 sec/batch; 23h:16m:04s remains)
INFO - root - 2017-12-16 05:52:42.018672: step 28090, loss = 0.35, batch loss = 0.29 (28.4 examples/sec; 0.281 sec/batch; 23h:47m:25s remains)
INFO - root - 2017-12-16 05:52:44.868958: step 28100, loss = 0.41, batch loss = 0.35 (29.3 examples/sec; 0.273 sec/batch; 23h:03m:31s remains)
2017-12-16 05:52:45.341749: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2271116 -2.2315509 -2.2645178 -2.2849574 -2.3100605 -2.346092 -2.3205218 -2.2008841 -2.0841269 -1.9259412 -1.8500304 -1.8439915 -1.8357761 -1.8958182 -1.9406302][-2.2287645 -2.3198664 -2.4531677 -2.5622568 -2.575027 -2.475466 -2.3239295 -2.1461556 -2.0048304 -1.8560448 -1.8079891 -1.7838044 -1.8216882 -1.9062114 -1.929368][-2.4528637 -2.6776524 -2.9000227 -2.9439082 -2.8293729 -2.633141 -2.36101 -2.1071901 -1.9533415 -1.9356389 -2.1109021 -2.2225032 -2.325803 -2.4158041 -2.4242535][-2.708154 -2.9241166 -3.1484394 -3.1308017 -2.8302188 -2.3492537 -1.7956126 -1.4584372 -1.3813577 -1.6205018 -2.0082102 -2.3640351 -2.7558331 -2.9840472 -3.0034149][-2.8034377 -2.9760585 -3.1215224 -2.899848 -2.3006408 -1.5484347 -0.828573 -0.4306798 -0.34806156 -0.75982952 -1.474735 -2.1975579 -2.8134894 -3.2308173 -3.420429][-2.5426984 -2.6439562 -2.6732035 -2.159482 -1.1549346 -0.19524527 0.63289118 1.1982346 1.3068614 0.59863281 -0.5100081 -1.5253093 -2.4685912 -3.1819413 -3.4920273][-2.2813041 -2.1481855 -1.9411161 -1.284445 -0.10305452 1.1320653 2.2218518 2.7273707 2.6727705 1.9848056 0.69617462 -0.70672822 -1.953712 -2.755806 -3.1924105][-1.9847972 -1.8067756 -1.5503392 -0.83337092 0.34801483 1.6776323 2.9390459 3.503377 3.4080138 2.5527062 1.1969624 -0.26770115 -1.7162459 -2.6480396 -3.0963068][-2.0988636 -1.9034145 -1.6483197 -1.1846945 -0.31715584 0.96439171 2.2161622 2.7713137 2.8353086 2.1245146 0.81917143 -0.76297188 -2.1363461 -2.8373094 -3.136363][-2.2678225 -2.3328478 -2.3649871 -2.1055107 -1.5066261 -0.61050773 0.29694223 0.79353714 0.83985043 0.27318335 -0.76640058 -2.0796645 -3.21743 -3.6637838 -3.619195][-2.8082895 -2.7439165 -2.6538463 -2.6323366 -2.4306114 -1.8666131 -1.3096242 -1.1387854 -1.2996526 -1.8879611 -2.7862811 -3.831254 -4.5876813 -4.585042 -4.1768589][-3.2484066 -3.2452796 -3.3153977 -3.2988167 -3.1903443 -3.0792942 -2.9619935 -3.0383716 -3.3799331 -4.0429935 -4.7242823 -5.4147444 -5.7381258 -5.3225818 -4.6386924][-3.3645589 -3.2701972 -3.2859106 -3.4396224 -3.723345 -3.8672674 -3.9587967 -4.3311434 -4.7979064 -5.4001613 -5.8919764 -6.1639156 -5.9740715 -5.273819 -4.32267][-3.2089732 -2.9912763 -2.9047837 -3.0466168 -3.3555686 -3.6429045 -4.065814 -4.733685 -5.3477154 -5.8252983 -6.0828381 -6.20337 -5.7991762 -4.7653112 -3.6191216][-2.9622796 -2.696552 -2.552186 -2.6508296 -2.9022486 -3.244298 -3.8157368 -4.4621711 -5.0030365 -5.5959625 -5.9560213 -5.7557983 -5.0118661 -3.8233824 -2.6063983]]...]
INFO - root - 2017-12-16 05:52:48.114432: step 28110, loss = 0.30, batch loss = 0.25 (28.7 examples/sec; 0.278 sec/batch; 23h:32m:19s remains)
INFO - root - 2017-12-16 05:52:50.976921: step 28120, loss = 0.19, batch loss = 0.13 (28.4 examples/sec; 0.282 sec/batch; 23h:48m:13s remains)
INFO - root - 2017-12-16 05:52:53.840029: step 28130, loss = 0.39, batch loss = 0.34 (26.7 examples/sec; 0.300 sec/batch; 25h:21m:01s remains)
INFO - root - 2017-12-16 05:52:56.684269: step 28140, loss = 0.26, batch loss = 0.21 (27.6 examples/sec; 0.290 sec/batch; 24h:30m:05s remains)
INFO - root - 2017-12-16 05:52:59.528586: step 28150, loss = 0.27, batch loss = 0.22 (29.6 examples/sec; 0.270 sec/batch; 22h:51m:42s remains)
INFO - root - 2017-12-16 05:53:02.307227: step 28160, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.284 sec/batch; 24h:02m:57s remains)
INFO - root - 2017-12-16 05:53:05.212603: step 28170, loss = 0.26, batch loss = 0.20 (26.8 examples/sec; 0.299 sec/batch; 25h:15m:56s remains)
INFO - root - 2017-12-16 05:53:08.103360: step 28180, loss = 0.30, batch loss = 0.25 (26.9 examples/sec; 0.297 sec/batch; 25h:07m:17s remains)
INFO - root - 2017-12-16 05:53:10.904782: step 28190, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.284 sec/batch; 24h:01m:58s remains)
INFO - root - 2017-12-16 05:53:13.779188: step 28200, loss = 0.20, batch loss = 0.14 (29.4 examples/sec; 0.272 sec/batch; 22h:58m:23s remains)
2017-12-16 05:53:14.249898: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.810976 -5.4117274 -6.0025043 -6.6707745 -7.2903376 -7.5495405 -7.9643955 -8.6144638 -9.0126371 -8.66461 -7.9093189 -7.1976376 -6.2672844 -5.1964064 -4.3958311][-4.2353382 -4.993391 -5.7823448 -6.483552 -7.0757279 -7.4809065 -7.8969493 -8.2940216 -8.3940639 -8.0207977 -7.3476548 -6.6958675 -5.885725 -5.0881343 -4.3875642][-3.8261297 -4.484653 -5.1034021 -5.5907278 -6.022944 -6.0956964 -6.0969973 -6.2734261 -6.3923068 -6.2944889 -6.0575686 -5.725637 -5.4205389 -5.1202855 -4.7232547][-3.1012807 -3.6538453 -4.0853596 -4.3175278 -4.44899 -4.0663528 -3.5995035 -3.3546362 -3.2628634 -3.3131213 -3.5970225 -4.21884 -4.719377 -4.9654708 -4.8400173][-2.5348873 -2.8056507 -2.954159 -2.8552303 -2.5856509 -1.7998776 -0.86410236 -0.16716528 0.13070679 -0.2243185 -1.1660018 -2.4576488 -3.5796013 -4.4985933 -4.9891653][-1.8157532 -1.8466325 -1.5595555 -1.1098104 -0.43726158 0.49395704 1.8190117 2.8855052 3.1668692 2.4279542 0.96689796 -1.1428139 -3.1377435 -4.4601078 -4.9888849][-1.4159245 -1.3008194 -0.75523329 0.097474575 1.2242565 2.4781885 3.7853661 4.7914534 4.9430075 3.9589338 1.9524913 -0.54894257 -2.956933 -4.5814667 -5.2803726][-1.419847 -1.4115849 -1.1238875 -0.23575544 0.8509407 2.2371826 3.7599163 4.867527 5.0138454 3.9892502 2.0386796 -0.6312058 -3.1813855 -4.8425651 -5.5580115][-2.3205588 -2.4473486 -2.270474 -1.7809587 -0.82274246 0.64161825 1.90729 3.0541987 3.4215469 2.6224532 0.92073631 -1.4583602 -3.8199203 -5.3947616 -6.0301976][-3.0994277 -3.6490359 -4.044776 -3.7821751 -2.7911558 -1.7972567 -0.87930155 0.26408434 0.72721815 0.21825743 -1.0229099 -3.024477 -5.0454597 -6.294055 -6.6055412][-4.5050969 -5.06789 -5.5001583 -5.6840324 -5.3930664 -4.4235287 -3.52626 -2.7138004 -2.226228 -2.2670643 -3.1666114 -4.6024747 -6.0046711 -6.9194036 -6.943861][-5.527822 -6.3630528 -7.2842293 -7.7442064 -7.4577451 -6.9024482 -6.3132539 -5.1905165 -4.2995319 -4.1375213 -4.7255883 -5.7309532 -6.5983219 -7.1371217 -6.9245014][-6.7595849 -7.8675904 -8.8380566 -9.46187 -9.6718788 -8.9437256 -7.959301 -6.8308344 -5.8809724 -5.3481317 -5.4697509 -6.104001 -6.6577344 -6.9358664 -6.593225][-7.6548715 -8.8323078 -10.04861 -11.025946 -11.347672 -10.780167 -9.9231939 -8.5710535 -7.3124542 -6.7437162 -6.7078457 -6.7819939 -6.6756144 -6.4791975 -6.1308918][-8.14875 -9.3205528 -10.488856 -11.243953 -11.472949 -10.946314 -9.955905 -8.8351135 -8.0279512 -7.4284325 -7.1058235 -7.0044794 -6.7686253 -6.4018087 -5.7334876]]...]
INFO - root - 2017-12-16 05:53:17.110440: step 28210, loss = 0.23, batch loss = 0.17 (27.9 examples/sec; 0.286 sec/batch; 24h:12m:11s remains)
INFO - root - 2017-12-16 05:53:19.917774: step 28220, loss = 0.23, batch loss = 0.18 (29.7 examples/sec; 0.269 sec/batch; 22h:44m:38s remains)
INFO - root - 2017-12-16 05:53:22.773834: step 28230, loss = 0.38, batch loss = 0.32 (28.2 examples/sec; 0.284 sec/batch; 23h:59m:30s remains)
INFO - root - 2017-12-16 05:53:25.618123: step 28240, loss = 0.36, batch loss = 0.31 (28.4 examples/sec; 0.281 sec/batch; 23h:46m:11s remains)
INFO - root - 2017-12-16 05:53:28.450961: step 28250, loss = 0.23, batch loss = 0.17 (28.1 examples/sec; 0.284 sec/batch; 24h:01m:39s remains)
INFO - root - 2017-12-16 05:53:31.333521: step 28260, loss = 0.23, batch loss = 0.17 (28.6 examples/sec; 0.280 sec/batch; 23h:40m:15s remains)
INFO - root - 2017-12-16 05:53:34.125258: step 28270, loss = 0.20, batch loss = 0.14 (28.6 examples/sec; 0.280 sec/batch; 23h:39m:10s remains)
INFO - root - 2017-12-16 05:53:37.010988: step 28280, loss = 0.29, batch loss = 0.23 (28.3 examples/sec; 0.283 sec/batch; 23h:53m:33s remains)
INFO - root - 2017-12-16 05:53:39.845084: step 28290, loss = 0.25, batch loss = 0.19 (27.9 examples/sec; 0.287 sec/batch; 24h:13m:37s remains)
INFO - root - 2017-12-16 05:53:42.702369: step 28300, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.276 sec/batch; 23h:21m:33s remains)
2017-12-16 05:53:43.191109: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9226918 -6.0184078 -5.4737124 -5.0146976 -4.7335787 -4.7982011 -4.7062364 -4.7077508 -4.8094215 -5.050992 -5.2084813 -5.3967981 -5.4996614 -5.5033226 -5.4501734][-6.5008698 -6.3359375 -5.9057431 -5.5446053 -4.8828554 -4.7347708 -4.7831893 -5.0150485 -5.0240245 -5.1761956 -5.3353786 -5.5060635 -5.550909 -5.606225 -5.7055364][-7.33101 -6.913476 -6.1996388 -5.4539504 -4.810739 -4.6651192 -4.6183558 -4.87471 -5.2103367 -5.4729276 -5.4427338 -5.6384954 -5.9887829 -6.00651 -6.177712][-6.8439112 -6.8050003 -6.1287818 -4.9828911 -3.8279004 -3.5363846 -3.4445148 -3.7296381 -4.4159179 -4.9183517 -5.0882092 -5.4112496 -5.8656268 -6.2426109 -6.6447859][-6.4018412 -6.0591044 -5.0636606 -3.9239295 -2.3529451 -1.5992448 -1.2598534 -1.6431613 -2.3367023 -2.9686747 -3.4465554 -4.2007432 -4.8204222 -5.7361469 -6.7451782][-6.4810095 -6.0943546 -4.7871504 -2.6825676 -0.77451324 0.13065338 0.82830763 0.66976833 -0.082261086 -1.0714459 -1.5686555 -2.3970344 -3.5736895 -4.9845438 -6.2481947][-6.5944757 -6.0003638 -4.5353866 -2.024857 0.26338863 2.0251942 2.8155537 2.3740187 1.4067626 0.39793158 -0.40521049 -1.3980551 -2.9625993 -4.4948797 -6.1249619][-6.9118214 -6.0626554 -4.1732454 -1.5776026 0.70133209 2.4138494 3.1324854 3.0694151 2.3617206 1.543354 0.78468466 -0.52428246 -2.420481 -4.3013525 -6.2549586][-7.7864676 -6.8714314 -4.71713 -2.1414354 0.089848518 1.590744 2.3288841 2.4013658 2.0562387 1.5530505 1.2730632 0.46387863 -1.1323788 -3.1957955 -5.410399][-8.3403587 -7.473279 -5.8617826 -3.4771609 -1.2257233 -0.11256504 0.4741354 0.6446805 0.62652922 0.33323526 0.25788069 -0.13346958 -1.3763239 -3.048882 -4.8922796][-8.8482246 -7.7165375 -6.3351722 -4.9027338 -3.65425 -2.4156637 -1.5522265 -1.7001948 -1.8029952 -1.8224216 -1.631361 -1.7544506 -2.5623593 -3.7558398 -5.2395697][-9.0443516 -8.4476833 -7.2920761 -6.153326 -5.4422975 -4.8150291 -4.2226486 -4.172534 -3.9071717 -3.6844907 -3.3380709 -3.2592556 -3.6699326 -4.37889 -5.4438314][-9.6493082 -9.0086136 -8.1411438 -7.3115969 -6.4232159 -6.0330491 -5.7483759 -5.610292 -5.0168753 -4.7581758 -4.495821 -4.245234 -4.2861943 -4.656261 -5.3064046][-9.6790714 -8.9197731 -8.0167122 -7.179492 -6.52421 -6.0219293 -5.7041831 -5.9987469 -5.92005 -5.6038017 -5.0931163 -4.9607882 -5.0688405 -5.0606055 -4.8822379][-8.6681652 -7.9470649 -6.9059806 -6.2612104 -6.0385132 -5.76242 -5.4922218 -5.5554881 -5.8100204 -5.842649 -5.4492 -4.9882536 -4.9123359 -4.85636 -4.6810131]]...]
INFO - root - 2017-12-16 05:53:46.031582: step 28310, loss = 0.37, batch loss = 0.31 (28.7 examples/sec; 0.279 sec/batch; 23h:35m:05s remains)
INFO - root - 2017-12-16 05:53:48.874058: step 28320, loss = 0.24, batch loss = 0.18 (27.5 examples/sec; 0.291 sec/batch; 24h:34m:02s remains)
INFO - root - 2017-12-16 05:53:51.730556: step 28330, loss = 0.22, batch loss = 0.16 (29.5 examples/sec; 0.271 sec/batch; 22h:54m:38s remains)
INFO - root - 2017-12-16 05:53:54.543801: step 28340, loss = 0.28, batch loss = 0.22 (28.5 examples/sec; 0.280 sec/batch; 23h:41m:48s remains)
INFO - root - 2017-12-16 05:53:57.347965: step 28350, loss = 0.33, batch loss = 0.27 (28.9 examples/sec; 0.277 sec/batch; 23h:25m:37s remains)
INFO - root - 2017-12-16 05:54:00.154099: step 28360, loss = 0.25, batch loss = 0.19 (28.9 examples/sec; 0.277 sec/batch; 23h:21m:56s remains)
INFO - root - 2017-12-16 05:54:02.977300: step 28370, loss = 0.33, batch loss = 0.27 (28.9 examples/sec; 0.277 sec/batch; 23h:22m:58s remains)
INFO - root - 2017-12-16 05:54:05.847895: step 28380, loss = 0.21, batch loss = 0.15 (29.7 examples/sec; 0.269 sec/batch; 22h:44m:33s remains)
INFO - root - 2017-12-16 05:54:08.638220: step 28390, loss = 0.32, batch loss = 0.26 (29.2 examples/sec; 0.274 sec/batch; 23h:06m:16s remains)
INFO - root - 2017-12-16 05:54:11.437734: step 28400, loss = 0.28, batch loss = 0.22 (28.8 examples/sec; 0.277 sec/batch; 23h:26m:14s remains)
2017-12-16 05:54:11.893880: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7296219 -2.6282887 -2.4863245 -2.3993981 -2.4161975 -2.562953 -2.8916192 -3.204257 -3.5764487 -3.8070068 -3.9088838 -3.872606 -3.6782911 -3.4569645 -3.1526575][-2.7328157 -2.5889802 -2.4319675 -2.3247492 -2.2884216 -2.4758348 -2.7621708 -2.9437833 -3.2105813 -3.3804402 -3.4428606 -3.303982 -2.9512429 -2.7008553 -2.4303613][-2.6302581 -2.4703603 -2.3426316 -2.223237 -2.1463411 -2.2164714 -2.4073141 -2.4289179 -2.495369 -2.6459687 -2.6798172 -2.5487869 -2.3109794 -1.9688704 -1.7253342][-2.4240882 -2.1559618 -1.9730337 -1.8433065 -1.7379138 -1.7015159 -1.7304416 -1.6428719 -1.5555103 -1.4205561 -1.3112776 -1.3096178 -1.2190945 -1.0745966 -1.0793831][-2.2306359 -1.8258493 -1.5119228 -1.28092 -1.1024983 -0.92303443 -0.77019477 -0.55956817 -0.28972149 -0.076103687 0.0055708885 -0.046188354 -0.21050119 -0.45012474 -0.82205248][-2.0436602 -1.571192 -1.2156985 -0.88727951 -0.58128977 -0.37958765 -0.14208364 0.31374407 0.75132227 0.962749 1.0253797 0.80963993 0.32882643 -0.19219971 -0.90115738][-2.0796502 -1.6262987 -1.2489178 -0.84459162 -0.43832803 0.0030994415 0.5294714 1.0455465 1.5091796 1.7766781 1.7181225 1.2779331 0.48063278 -0.49805689 -1.522023][-2.1098564 -1.7192357 -1.3308318 -0.88317394 -0.39415264 0.27128077 1.1022091 1.7296171 2.2465339 2.4101071 2.2252789 1.5572286 0.40778351 -0.89751554 -2.1226604][-2.0106497 -1.7020023 -1.463048 -1.1024611 -0.56208467 0.15513611 1.0133924 1.7886648 2.4281769 2.5458775 2.2308912 1.4353089 0.18559837 -1.3357096 -2.6977811][-2.0961208 -2.0813122 -2.1625519 -1.9245853 -1.5600164 -0.92416048 -0.047925949 0.66353559 1.2519369 1.3580351 1.0515513 0.34247637 -0.792243 -2.1764548 -3.4320593][-2.6867993 -2.7920821 -2.9938364 -3.0297103 -2.884141 -2.2778752 -1.4710653 -0.8991127 -0.46300793 -0.42268991 -0.61483312 -1.2813621 -2.258281 -3.2949491 -4.1844845][-3.0289457 -3.2599814 -3.5685267 -3.7100835 -3.7250438 -3.3250709 -2.7004592 -2.2499862 -1.8825655 -1.8812964 -2.0206254 -2.5638013 -3.30553 -4.0959539 -4.7007236][-3.0664186 -3.4285345 -3.7646773 -3.8691816 -3.9520872 -3.7504606 -3.4419227 -3.1661518 -2.9354346 -3.0487857 -3.2020693 -3.6377239 -4.1529274 -4.73951 -5.1664968][-3.1369941 -3.4268422 -3.6659741 -3.7341526 -3.7773981 -3.6122656 -3.4759345 -3.3850312 -3.3881364 -3.6174326 -3.8715985 -4.2847576 -4.6503811 -5.0789509 -5.3937626][-3.23425 -3.3669648 -3.4643109 -3.4358222 -3.3775868 -3.1981339 -3.1184566 -3.091542 -3.1647897 -3.4239216 -3.7271872 -4.1162534 -4.4372458 -4.7829275 -5.0568562]]...]
INFO - root - 2017-12-16 05:54:14.723155: step 28410, loss = 0.29, batch loss = 0.23 (27.5 examples/sec; 0.291 sec/batch; 24h:32m:34s remains)
INFO - root - 2017-12-16 05:54:17.571068: step 28420, loss = 0.32, batch loss = 0.26 (28.9 examples/sec; 0.277 sec/batch; 23h:25m:12s remains)
INFO - root - 2017-12-16 05:54:20.384820: step 28430, loss = 0.25, batch loss = 0.19 (29.5 examples/sec; 0.271 sec/batch; 22h:54m:08s remains)
INFO - root - 2017-12-16 05:54:23.231581: step 28440, loss = 0.24, batch loss = 0.18 (25.6 examples/sec; 0.312 sec/batch; 26h:21m:07s remains)
INFO - root - 2017-12-16 05:54:26.069205: step 28450, loss = 0.33, batch loss = 0.27 (29.0 examples/sec; 0.276 sec/batch; 23h:16m:19s remains)
INFO - root - 2017-12-16 05:54:28.896726: step 28460, loss = 0.22, batch loss = 0.16 (29.1 examples/sec; 0.275 sec/batch; 23h:12m:53s remains)
INFO - root - 2017-12-16 05:54:31.770167: step 28470, loss = 0.48, batch loss = 0.42 (28.1 examples/sec; 0.285 sec/batch; 24h:02m:53s remains)
INFO - root - 2017-12-16 05:54:34.550957: step 28480, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.279 sec/batch; 23h:32m:45s remains)
INFO - root - 2017-12-16 05:54:37.359228: step 28490, loss = 0.23, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 24h:02m:50s remains)
INFO - root - 2017-12-16 05:54:40.201516: step 28500, loss = 0.30, batch loss = 0.25 (27.9 examples/sec; 0.287 sec/batch; 24h:11m:38s remains)
2017-12-16 05:54:40.669187: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5507598 -3.1790047 -2.9747329 -2.8287563 -3.0210757 -3.2723298 -3.8443086 -4.34619 -4.7479162 -4.7183518 -4.3171439 -3.9191244 -3.3514948 -2.8954902 -2.5573602][-4.2459254 -3.7919483 -3.3644485 -3.1522737 -3.2105293 -3.4861567 -4.0471325 -4.47719 -4.850512 -4.827188 -4.4170532 -3.8960183 -3.2481921 -2.7210016 -2.3765254][-4.9168158 -4.5070291 -4.0621495 -3.5920119 -3.3480053 -3.4882963 -3.9418826 -4.3368139 -4.6957688 -4.8082809 -4.4402471 -3.9328849 -3.3216107 -2.8067195 -2.4307683][-5.3277116 -4.7476344 -4.1289244 -3.4876521 -3.1138058 -3.0275297 -3.2364008 -3.6947787 -4.1562819 -4.3266311 -4.1944962 -3.8775687 -3.3775196 -2.9111333 -2.5529733][-5.1818652 -4.5180407 -3.6813705 -2.7253277 -2.1787858 -1.9987962 -2.1509559 -2.627409 -3.1606464 -3.6800642 -3.8890138 -3.8040187 -3.5671442 -3.2129555 -2.7985477][-4.4482222 -3.7026172 -2.7480311 -1.5328753 -0.67400074 -0.42898035 -0.54047346 -1.1256702 -1.9617598 -2.9293261 -3.6074157 -3.8583155 -3.8395977 -3.562964 -3.0800924][-3.9828684 -3.1872134 -2.0835702 -0.65073991 0.41138744 0.83911705 0.8291502 0.16495943 -0.9072454 -2.2446492 -3.2663479 -3.8237214 -4.015274 -3.7926118 -3.2784276][-3.7334418 -3.0151803 -1.9093654 -0.49980879 0.60046673 1.13694 1.1978645 0.56057692 -0.4874177 -1.8052056 -2.9879527 -3.6999707 -3.9458933 -3.7678056 -3.3080981][-3.5593944 -2.8927751 -1.8500748 -0.59456754 0.45448112 0.89051962 0.92517471 0.38648272 -0.59930062 -1.7783308 -2.8205631 -3.5326352 -3.8989298 -3.7990451 -3.356118][-3.4075 -2.9164116 -2.2559924 -1.2284787 -0.3153038 0.080056667 0.20127439 -0.1913805 -1.0026908 -1.9239321 -2.8523564 -3.4910979 -3.802505 -3.7574105 -3.3929777][-3.4967341 -3.0553861 -2.4253371 -1.8149705 -1.2943645 -0.88545489 -0.80192804 -1.0284567 -1.6145003 -2.4079361 -3.17491 -3.6937616 -3.902271 -3.8151779 -3.46477][-3.661413 -3.1832159 -2.7790384 -2.3528466 -1.9082713 -1.639271 -1.6181126 -1.7594965 -2.2286015 -2.8983161 -3.5838544 -4.058795 -4.2073 -4.0325165 -3.6216056][-3.1081271 -2.6139369 -2.3596745 -2.0509548 -1.8952854 -1.819052 -1.8589573 -2.0516491 -2.5113504 -3.1442981 -3.7562351 -4.2047958 -4.3557386 -4.1333895 -3.7506771][-3.164607 -2.302237 -1.8132174 -1.6907675 -1.5964854 -1.6900582 -1.965378 -2.1710854 -2.5831022 -3.2278643 -3.8943892 -4.266923 -4.247952 -4.0358062 -3.6772609][-3.1639578 -2.3954399 -1.921056 -1.6775322 -1.4891531 -1.5429265 -1.6855648 -2.0124404 -2.6195326 -3.2539251 -3.8067679 -4.1702533 -4.2058349 -3.8633165 -3.4094932]]...]
INFO - root - 2017-12-16 05:54:43.505895: step 28510, loss = 0.33, batch loss = 0.28 (28.5 examples/sec; 0.281 sec/batch; 23h:44m:17s remains)
INFO - root - 2017-12-16 05:54:46.349276: step 28520, loss = 0.40, batch loss = 0.34 (28.6 examples/sec; 0.280 sec/batch; 23h:37m:39s remains)
INFO - root - 2017-12-16 05:54:49.154967: step 28530, loss = 0.29, batch loss = 0.23 (28.6 examples/sec; 0.279 sec/batch; 23h:35m:46s remains)
INFO - root - 2017-12-16 05:54:51.993769: step 28540, loss = 0.31, batch loss = 0.25 (29.4 examples/sec; 0.272 sec/batch; 22h:59m:41s remains)
INFO - root - 2017-12-16 05:54:54.870172: step 28550, loss = 0.29, batch loss = 0.24 (28.4 examples/sec; 0.282 sec/batch; 23h:46m:48s remains)
INFO - root - 2017-12-16 05:54:57.699521: step 28560, loss = 0.28, batch loss = 0.22 (28.2 examples/sec; 0.283 sec/batch; 23h:54m:57s remains)
INFO - root - 2017-12-16 05:55:00.490144: step 28570, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.280 sec/batch; 23h:40m:08s remains)
INFO - root - 2017-12-16 05:55:03.346743: step 28580, loss = 0.22, batch loss = 0.16 (27.2 examples/sec; 0.294 sec/batch; 24h:49m:56s remains)
INFO - root - 2017-12-16 05:55:06.259498: step 28590, loss = 0.23, batch loss = 0.18 (25.3 examples/sec; 0.316 sec/batch; 26h:39m:03s remains)
INFO - root - 2017-12-16 05:55:09.119532: step 28600, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.280 sec/batch; 23h:39m:00s remains)
2017-12-16 05:55:09.579250: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5014257 -4.3550215 -4.1965857 -4.2451091 -4.5433536 -4.6092653 -4.8530941 -5.2520413 -5.3554616 -5.1490889 -4.7133527 -4.3675156 -3.8448272 -3.362175 -3.0879779][-4.2222939 -4.1591983 -4.1407051 -4.1879315 -4.4100332 -4.7854466 -5.22501 -5.5993347 -5.7584643 -5.6090441 -5.1603122 -4.6118345 -3.9767134 -3.4384604 -3.0743766][-3.7371094 -3.6875834 -3.6299396 -3.8155913 -4.1594243 -4.48474 -4.88513 -5.3091631 -5.4758615 -5.3790536 -5.1542187 -4.9250503 -4.4669228 -3.9284737 -3.4065044][-2.6489863 -2.5662565 -2.4675488 -2.6485624 -2.9846845 -3.2563095 -3.5397098 -3.7975569 -4.0953531 -4.2701354 -4.4404383 -4.6518478 -4.5280232 -4.133244 -3.6055202][-1.5731435 -1.2718675 -0.89808464 -0.91089463 -1.0035083 -1.0216632 -1.211632 -1.5858092 -2.1153791 -2.7336421 -3.4302135 -4.1658669 -4.453227 -4.3489156 -3.8163707][-0.69805932 -0.27552938 0.24406052 0.71609831 0.98042631 1.0698133 1.1654587 0.976563 0.19570684 -0.96188235 -2.300487 -3.6715703 -4.3602104 -4.4790916 -4.0682478][-0.54173136 0.0845685 0.65159178 1.2175684 1.8565283 2.2579703 2.5533257 2.5735469 1.9397411 0.6052227 -1.0048151 -2.6947839 -3.8046522 -4.1797705 -3.959564][-1.0676539 -0.57234836 0.13298225 0.98023558 1.7679815 2.4195294 2.8871121 2.9216919 2.4559598 1.2190552 -0.43847322 -2.1068635 -3.1932998 -3.6749771 -3.707207][-1.9463186 -1.4281883 -0.87221432 -0.085744381 0.63831806 1.2220969 1.8562617 2.0984073 1.8202343 0.7281065 -0.65203738 -2.1487136 -3.1779604 -3.5982304 -3.7101045][-2.7578483 -2.4448915 -2.1992381 -1.6533525 -0.93160176 -0.35826397 0.090941429 0.36590004 0.35275173 -0.33966494 -1.3907268 -2.4972043 -3.3223364 -3.7329435 -3.7852569][-3.2496119 -3.2708492 -3.2439692 -3.0313299 -2.5760932 -2.0355403 -1.5609465 -1.1749291 -0.90105438 -1.1257751 -1.9466219 -2.8907909 -3.5383687 -3.7806268 -3.848531][-3.3832436 -3.5546012 -3.8287997 -3.87245 -3.6473336 -3.1404052 -2.5406227 -2.1440759 -1.7999802 -1.8442492 -2.3716028 -3.0939054 -3.7220063 -3.8527346 -3.818274][-2.8007572 -3.0225713 -3.4954462 -3.6540439 -3.5146303 -3.2280545 -2.9010053 -2.6315341 -2.2931955 -2.3112888 -2.7343297 -3.3171616 -3.7730894 -3.8291526 -3.6800742][-1.8827374 -2.0677145 -2.5715895 -2.9685783 -3.0666957 -2.8330371 -2.6196465 -2.4439955 -2.202158 -2.2916625 -2.7530961 -3.3174472 -3.7237127 -3.7835526 -3.5879002][-1.3439698 -1.4940102 -2.0539634 -2.3792074 -2.4851716 -2.2690964 -1.886234 -1.6956682 -1.6244626 -1.8423157 -2.3711956 -3.0284982 -3.4687188 -3.5348685 -3.305717]]...]
INFO - root - 2017-12-16 05:55:12.382427: step 28610, loss = 0.29, batch loss = 0.23 (29.2 examples/sec; 0.274 sec/batch; 23h:07m:50s remains)
INFO - root - 2017-12-16 05:55:15.246031: step 28620, loss = 0.38, batch loss = 0.33 (26.2 examples/sec; 0.306 sec/batch; 25h:48m:26s remains)
INFO - root - 2017-12-16 05:55:18.086723: step 28630, loss = 0.27, batch loss = 0.21 (27.9 examples/sec; 0.287 sec/batch; 24h:12m:12s remains)
INFO - root - 2017-12-16 05:55:20.947076: step 28640, loss = 0.31, batch loss = 0.26 (28.5 examples/sec; 0.281 sec/batch; 23h:40m:53s remains)
INFO - root - 2017-12-16 05:55:23.804606: step 28650, loss = 0.28, batch loss = 0.22 (27.0 examples/sec; 0.297 sec/batch; 25h:02m:37s remains)
INFO - root - 2017-12-16 05:55:26.641901: step 28660, loss = 0.24, batch loss = 0.18 (28.5 examples/sec; 0.281 sec/batch; 23h:43m:34s remains)
INFO - root - 2017-12-16 05:55:29.472017: step 28670, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 24h:03m:38s remains)
INFO - root - 2017-12-16 05:55:32.279109: step 28680, loss = 0.28, batch loss = 0.22 (28.7 examples/sec; 0.278 sec/batch; 23h:29m:43s remains)
INFO - root - 2017-12-16 05:55:35.141494: step 28690, loss = 0.25, batch loss = 0.19 (27.6 examples/sec; 0.289 sec/batch; 24h:25m:36s remains)
INFO - root - 2017-12-16 05:55:37.986895: step 28700, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 23h:36m:28s remains)
2017-12-16 05:55:38.432016: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7113304 -2.4229414 -2.5610516 -2.8274457 -3.128686 -3.4982944 -3.7388158 -3.7959375 -3.7255862 -3.440897 -3.1233459 -2.9374919 -2.729445 -2.4439042 -2.1962962][-3.0531387 -2.7214873 -2.5862024 -3.0718663 -3.5962133 -4.0635085 -4.2995625 -4.3831778 -4.3766389 -4.2036271 -3.9823062 -3.7002909 -3.4123769 -3.1686103 -2.6897845][-3.5932209 -3.4039402 -3.5565355 -3.6882353 -3.7683752 -4.2058034 -4.3785629 -4.5697527 -4.6733446 -4.6563025 -4.6562247 -4.6229887 -4.3960195 -4.0182781 -3.4679661][-4.773901 -4.5915689 -4.5229497 -4.4606028 -4.238512 -3.9816256 -3.7532492 -3.6441126 -3.719517 -4.4621468 -5.2150545 -5.3962636 -5.2453318 -5.0119796 -4.5521121][-5.622715 -5.5594239 -5.4501266 -4.770535 -3.6329465 -2.5573668 -1.6975977 -1.5463035 -2.2198236 -3.3437886 -4.4745865 -5.4867992 -5.9752264 -5.6621418 -5.0071554][-6.0992203 -5.9251809 -5.3357725 -3.8834743 -1.8197451 0.12079668 1.3390799 1.5448909 0.66275787 -1.136821 -3.2203619 -4.8862906 -5.6515884 -5.8206034 -5.481636][-6.67577 -6.3649421 -5.4993029 -3.5078018 -0.43915534 2.2962031 4.1198244 4.4403219 3.0919952 0.87993431 -1.4417424 -3.3451681 -4.6293235 -5.0218444 -4.4079509][-6.784606 -6.5222216 -5.7685633 -3.4334657 -0.33633757 2.9351444 5.265008 5.6187963 4.379777 1.9868312 -0.71507716 -2.4956508 -3.3635952 -3.8589013 -3.6122265][-6.6747236 -6.2142339 -5.3179736 -3.5162983 -0.99075651 2.3591337 4.7594767 5.2402477 3.9693213 1.6186347 -0.93083382 -2.9338994 -3.8337541 -3.455966 -2.4023845][-5.9382539 -6.1233811 -6.0548286 -4.5840364 -2.5089288 -0.0470047 1.7265348 2.5699611 2.1140146 0.32304859 -1.8866405 -3.7397914 -4.5470281 -4.1703825 -2.9568913][-6.23701 -6.4638076 -6.4178534 -5.99407 -5.1311154 -3.4026451 -1.9041913 -1.057044 -1.2568879 -2.211993 -3.4675519 -4.6590762 -5.2221236 -5.034699 -3.8818409][-6.1082764 -6.5257406 -7.0375462 -6.8743143 -6.407567 -5.8300238 -5.3254142 -4.6208339 -4.200428 -4.4825382 -5.1480412 -5.7913465 -5.9644861 -5.7248993 -4.9932075][-5.4877 -6.1278019 -6.7076159 -7.0406065 -7.2776027 -7.0280313 -6.5874557 -6.167058 -5.7573261 -5.6115475 -5.7690806 -6.0560179 -6.1261435 -6.2518034 -6.0637903][-5.1156931 -5.394629 -5.7979507 -6.1872959 -6.5487156 -6.6683817 -6.6025596 -6.3209791 -5.7986064 -5.295073 -5.0261526 -5.46022 -6.0546622 -6.21038 -6.1481605][-4.6214738 -4.6492949 -4.6182904 -4.7496085 -5.00917 -5.1310148 -5.157228 -5.0521183 -4.8219895 -4.5509791 -4.486721 -4.7043853 -5.3744206 -6.1118145 -6.4353161]]...]
INFO - root - 2017-12-16 05:55:41.212201: step 28710, loss = 0.28, batch loss = 0.22 (28.6 examples/sec; 0.279 sec/batch; 23h:33m:52s remains)
INFO - root - 2017-12-16 05:55:43.993116: step 28720, loss = 0.24, batch loss = 0.18 (27.8 examples/sec; 0.288 sec/batch; 24h:16m:59s remains)
INFO - root - 2017-12-16 05:55:46.799331: step 28730, loss = 0.27, batch loss = 0.21 (28.3 examples/sec; 0.283 sec/batch; 23h:50m:25s remains)
INFO - root - 2017-12-16 05:55:49.642403: step 28740, loss = 0.39, batch loss = 0.33 (26.4 examples/sec; 0.304 sec/batch; 25h:36m:44s remains)
INFO - root - 2017-12-16 05:55:52.469373: step 28750, loss = 0.23, batch loss = 0.17 (29.1 examples/sec; 0.275 sec/batch; 23h:12m:29s remains)
INFO - root - 2017-12-16 05:55:55.340310: step 28760, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.284 sec/batch; 23h:58m:48s remains)
INFO - root - 2017-12-16 05:55:58.171249: step 28770, loss = 0.27, batch loss = 0.22 (27.3 examples/sec; 0.293 sec/batch; 24h:43m:03s remains)
INFO - root - 2017-12-16 05:56:01.008166: step 28780, loss = 0.27, batch loss = 0.21 (27.7 examples/sec; 0.288 sec/batch; 24h:19m:35s remains)
INFO - root - 2017-12-16 05:56:03.851623: step 28790, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 23h:30m:09s remains)
INFO - root - 2017-12-16 05:56:06.657598: step 28800, loss = 0.23, batch loss = 0.17 (29.0 examples/sec; 0.276 sec/batch; 23h:16m:02s remains)
2017-12-16 05:56:07.122963: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1431189 -5.274024 -5.4916778 -5.4130893 -5.2577085 -5.0025578 -4.8841295 -4.6555095 -4.4817548 -4.4878917 -4.3867807 -4.3285728 -4.1057973 -3.8585758 -3.7373929][-5.4611397 -5.5168033 -5.5894756 -5.4461465 -5.2635856 -5.1348248 -4.977798 -4.6388121 -4.3192186 -4.1837392 -3.9900095 -3.8313775 -3.6782625 -3.5400138 -3.5519166][-5.4084325 -5.3870196 -5.2894888 -5.1227636 -4.8660583 -4.7040157 -4.5106359 -4.2776341 -3.9382634 -3.6167169 -3.4104934 -3.3050983 -3.2351727 -3.207135 -3.3397393][-4.8232169 -4.7392974 -4.576911 -4.414485 -4.1784744 -3.9178617 -3.5783443 -3.1059785 -2.548974 -2.1697855 -1.8957021 -1.8047478 -1.8618717 -2.0819046 -2.4307437][-3.8739076 -3.6195416 -3.3407037 -3.0500541 -2.6448603 -2.3421159 -2.0020087 -1.5358517 -1.0237384 -0.71291041 -0.53110409 -0.6382184 -0.89291525 -1.2968676 -1.6976199][-2.7741027 -2.3377938 -1.8449283 -1.491493 -1.0085282 -0.67041326 -0.35618019 -0.086393833 0.20277023 0.43536472 0.42481661 0.14512205 -0.21945667 -0.67564535 -1.0627344][-2.0223954 -1.449712 -0.91087484 -0.48547053 0.013201714 0.43258476 0.76737404 0.95362282 1.080018 1.1081843 0.98752546 0.69702339 0.29309702 -0.24554873 -0.65145421][-1.7531736 -0.89154577 -0.14064598 0.42520714 0.941226 1.3546124 1.6016884 1.7278953 1.6674929 1.3441525 1.0267296 0.63096142 0.24115753 -0.30778646 -0.75821424][-1.6279597 -0.68855 0.13744211 0.83382988 1.3932614 1.7685165 1.9550171 1.9198966 1.6931076 1.2084737 0.83635712 0.41125107 -0.031650543 -0.55342078 -1.0325291][-1.8873336 -0.92539048 0.021183491 0.69113779 1.1111779 1.3021288 1.3596463 1.2550025 0.96829939 0.51461029 0.18498373 -0.11616325 -0.43692327 -0.88313174 -1.3780544][-2.6824813 -1.7085576 -0.72700167 -0.109056 0.12129784 0.19302845 0.22058678 0.06199789 -0.23378944 -0.53984237 -0.79048204 -1.0332589 -1.2977259 -1.508301 -1.8512087][-3.8086648 -3.0436583 -2.2250769 -1.5419989 -1.2846196 -1.3728235 -1.4506857 -1.5215325 -1.7026787 -1.7821159 -1.8566849 -1.9172633 -2.0501819 -2.1277943 -2.3729906][-5.13563 -4.4982185 -3.8714209 -3.3458762 -3.0561142 -3.0325708 -3.0716302 -3.1116407 -3.1531615 -2.8932755 -2.6367993 -2.3948727 -2.2599955 -2.2218533 -2.4241505][-6.7145996 -6.1434984 -5.5259657 -4.9609494 -4.6577144 -4.5026765 -4.4542646 -4.4306412 -4.3429737 -3.8959723 -3.4610624 -2.9798505 -2.5417004 -2.2412832 -2.26838][-7.3133349 -6.7354689 -6.1761456 -5.80319 -5.6256008 -5.5114651 -5.4736395 -5.4186263 -5.3536696 -4.8695135 -4.31921 -3.7054327 -3.1421294 -2.6250834 -2.4032662]]...]
INFO - root - 2017-12-16 05:56:09.961893: step 28810, loss = 0.32, batch loss = 0.26 (28.1 examples/sec; 0.284 sec/batch; 23h:58m:26s remains)
INFO - root - 2017-12-16 05:56:12.850381: step 28820, loss = 0.43, batch loss = 0.37 (29.3 examples/sec; 0.273 sec/batch; 23h:01m:33s remains)
INFO - root - 2017-12-16 05:56:15.647314: step 28830, loss = 0.29, batch loss = 0.23 (28.4 examples/sec; 0.282 sec/batch; 23h:46m:17s remains)
INFO - root - 2017-12-16 05:56:18.428761: step 28840, loss = 0.26, batch loss = 0.20 (29.3 examples/sec; 0.273 sec/batch; 23h:02m:40s remains)
INFO - root - 2017-12-16 05:56:21.224592: step 28850, loss = 0.25, batch loss = 0.20 (29.1 examples/sec; 0.275 sec/batch; 23h:12m:50s remains)
INFO - root - 2017-12-16 05:56:24.062485: step 28860, loss = 0.24, batch loss = 0.18 (27.7 examples/sec; 0.289 sec/batch; 24h:22m:58s remains)
INFO - root - 2017-12-16 05:56:26.855081: step 28870, loss = 0.24, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 23h:06m:41s remains)
INFO - root - 2017-12-16 05:56:29.725325: step 28880, loss = 0.30, batch loss = 0.24 (28.9 examples/sec; 0.277 sec/batch; 23h:22m:29s remains)
INFO - root - 2017-12-16 05:56:32.564254: step 28890, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.283 sec/batch; 23h:53m:09s remains)
INFO - root - 2017-12-16 05:56:35.359327: step 28900, loss = 0.30, batch loss = 0.24 (29.8 examples/sec; 0.268 sec/batch; 22h:38m:35s remains)
2017-12-16 05:56:35.820096: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2250414 -5.862298 -5.5212216 -5.3687959 -5.2865782 -5.2871618 -5.2208152 -5.3011346 -5.5997667 -5.847168 -6.1828423 -6.6691933 -7.0542307 -7.0718088 -6.9304657][-6.45418 -5.9657679 -5.5541773 -5.3928723 -5.3148832 -5.2902956 -5.1593838 -5.1981754 -5.4389935 -5.806448 -6.3391566 -6.8079624 -7.0115852 -6.9137573 -6.6974783][-6.4372282 -5.8630438 -5.2475309 -4.8591952 -4.659481 -4.6357007 -4.428297 -4.5709534 -4.8760839 -5.1585417 -5.6372404 -6.1701803 -6.4947605 -6.158186 -5.7796049][-6.1721053 -5.4175253 -4.5712914 -3.889451 -3.4735873 -3.1385188 -2.8923573 -3.193027 -3.6240816 -4.1929355 -4.6915822 -5.1428275 -5.3445187 -5.0920696 -4.8959217][-5.7463241 -4.7289076 -3.4513652 -2.3405342 -1.431859 -0.87523985 -0.61802197 -0.92420912 -1.5985391 -2.48428 -3.3458624 -3.9225 -4.1952395 -4.1011105 -4.0841556][-5.3178167 -4.0437865 -2.5287859 -0.92075467 0.54521751 1.3771749 1.7869744 1.4812799 0.65214252 -0.56380987 -1.7748065 -2.5424781 -3.0207872 -3.2123137 -3.3422017][-4.614327 -3.4120705 -1.9864447 -0.076581 1.679873 2.723495 3.3769569 3.1039114 2.1792598 0.80818558 -0.57289767 -1.5788624 -2.376745 -2.8805764 -3.4348843][-4.3707051 -3.0756915 -1.6974819 0.0060582161 1.7284889 3.0225496 3.8358192 3.7449684 3.0186367 1.7346172 0.33895779 -0.81784582 -1.9633296 -2.8934031 -3.8525934][-4.8879757 -3.8041692 -2.6486468 -1.0923297 0.50283861 1.8352451 2.7171035 3.0167928 2.5831308 1.4202609 0.11337137 -1.0596042 -2.17364 -3.2026517 -4.2745466][-6.2078896 -5.4431343 -4.4605088 -3.0104198 -1.6651833 -0.38629055 0.47881031 1.0837536 1.0821013 0.35412931 -0.66615605 -1.85269 -2.9952221 -4.055953 -5.1285925][-7.7785592 -7.3397875 -6.7224851 -5.5035458 -4.2356825 -2.9123902 -1.8311071 -1.0060277 -0.74325585 -0.92259836 -1.5160356 -2.6254983 -3.8096139 -4.9048705 -5.9232583][-8.7141876 -8.2781219 -7.9186878 -7.2605209 -6.5371647 -5.3100371 -4.2366352 -3.2545171 -2.635344 -2.4443483 -2.6882234 -3.5090113 -4.4155536 -5.549407 -6.5433064][-9.085474 -8.6018553 -8.2838 -7.8754735 -7.495698 -6.831728 -6.0343947 -5.1114635 -4.361671 -3.8009727 -3.7047439 -4.2870049 -4.9600468 -5.8623433 -6.7394247][-8.7026949 -7.9872246 -7.5710859 -7.3454127 -7.3602872 -7.0546732 -6.7715645 -6.2581749 -5.6578574 -5.0768933 -4.7862005 -4.9999614 -5.3500156 -6.008822 -6.6997118][-7.8126979 -6.8828983 -6.2821922 -6.2441769 -6.4814959 -6.6184916 -6.7260237 -6.5490103 -6.2505722 -5.7890625 -5.4504266 -5.5020151 -5.6222143 -5.9640994 -6.32797]]...]
INFO - root - 2017-12-16 05:56:38.666676: step 28910, loss = 0.33, batch loss = 0.27 (28.8 examples/sec; 0.278 sec/batch; 23h:25m:02s remains)
INFO - root - 2017-12-16 05:56:41.502610: step 28920, loss = 0.28, batch loss = 0.22 (28.7 examples/sec; 0.278 sec/batch; 23h:28m:19s remains)
INFO - root - 2017-12-16 05:56:44.338462: step 28930, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 23h:46m:24s remains)
INFO - root - 2017-12-16 05:56:47.176666: step 28940, loss = 0.26, batch loss = 0.20 (28.4 examples/sec; 0.281 sec/batch; 23h:43m:53s remains)
INFO - root - 2017-12-16 05:56:50.024621: step 28950, loss = 0.27, batch loss = 0.21 (27.9 examples/sec; 0.287 sec/batch; 24h:10m:29s remains)
INFO - root - 2017-12-16 05:56:52.845962: step 28960, loss = 0.27, batch loss = 0.21 (27.0 examples/sec; 0.297 sec/batch; 25h:00m:05s remains)
INFO - root - 2017-12-16 05:56:55.712512: step 28970, loss = 0.32, batch loss = 0.26 (27.5 examples/sec; 0.291 sec/batch; 24h:33m:56s remains)
INFO - root - 2017-12-16 05:56:58.587077: step 28980, loss = 0.19, batch loss = 0.13 (25.6 examples/sec; 0.313 sec/batch; 26h:23m:16s remains)
INFO - root - 2017-12-16 05:57:01.438059: step 28990, loss = 0.29, batch loss = 0.24 (27.9 examples/sec; 0.287 sec/batch; 24h:10m:02s remains)
INFO - root - 2017-12-16 05:57:04.234049: step 29000, loss = 0.22, batch loss = 0.16 (28.8 examples/sec; 0.278 sec/batch; 23h:23m:57s remains)
2017-12-16 05:57:04.699070: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3055739 -3.4276342 -3.7967176 -4.1239595 -3.7789068 -3.4939733 -3.2165608 -2.9873803 -2.8299327 -2.5955219 -2.3966167 -2.3481038 -2.3752415 -2.3642337 -2.3331623][-3.1176078 -3.2707779 -3.712131 -4.1570954 -4.1657605 -3.7882662 -3.2679319 -2.9526258 -2.7084222 -2.6397281 -2.6176186 -2.4637289 -2.4632359 -2.4118764 -2.3765597][-2.1706982 -2.7380679 -3.5488758 -3.8547044 -3.7395988 -3.5534947 -3.2111356 -2.95884 -2.7405968 -2.7385468 -2.8434317 -2.8652105 -2.9120517 -2.8348813 -2.8726344][-1.4948401 -1.9401686 -2.6866918 -3.3013525 -3.4011803 -2.9994934 -2.4716265 -2.3517206 -2.3587737 -2.7022839 -2.9805887 -3.3119736 -3.5810015 -3.396754 -3.1926894][-1.1732066 -1.591753 -2.124552 -2.4199672 -2.2456014 -1.6921883 -1.099262 -1.0849187 -1.3170192 -2.0550637 -2.7547622 -3.2240329 -3.3931763 -3.3904984 -3.384202][-1.0565565 -1.0101697 -1.4007468 -1.2860248 -0.5763433 0.21284199 0.86124754 0.83110619 0.33086014 -0.61304903 -1.5766184 -2.4097362 -2.86582 -2.9498854 -3.0206065][-1.1708205 -0.67219543 -0.567981 -0.16138315 0.822907 1.89119 2.6601396 2.6329079 1.9291639 0.67862892 -0.54817748 -1.2871809 -1.8894787 -2.0771768 -2.1497111][-1.4449656 -0.88031936 -0.48126531 0.19484472 1.3021836 2.5477753 3.4997702 3.4416351 2.7651658 1.5835314 0.38377953 -0.49493289 -1.0791624 -1.1601779 -1.2015545][-1.6160045 -1.4467273 -0.96023154 -0.20992279 0.71619415 1.8880677 2.8055682 2.9045811 2.4369931 1.4924989 0.45922089 -0.47188449 -0.94005203 -0.58904076 0.053352356][-1.5629656 -1.5179119 -1.4867816 -1.1218734 -0.28288603 0.58798885 1.1387939 1.267272 0.85329533 0.12071609 -0.80472732 -1.3646848 -1.2830586 -0.99504614 -0.078524113][-1.8452795 -1.6823437 -1.6501219 -1.6253645 -1.2767975 -0.79677558 -0.53973985 -0.52672315 -1.0351412 -1.8022463 -2.7128491 -3.0203319 -2.4473443 -1.5558138 -0.075870514][-2.2047698 -2.0509934 -2.1276741 -2.2814806 -2.1248627 -2.1931405 -2.2852633 -2.6224854 -3.2831674 -4.0630603 -4.7666388 -4.6783628 -3.6372094 -2.2190909 -0.41083145][-2.8108382 -2.4484942 -2.4633722 -2.579946 -2.60813 -2.7073355 -3.267153 -4.324892 -5.5207996 -6.2815871 -6.5725083 -6.1138577 -4.7900953 -3.1782117 -1.3984678][-3.2394938 -2.7775202 -2.6384251 -2.5922709 -2.6092293 -2.8725355 -3.8282452 -4.8734517 -6.252965 -7.4325948 -8.0093193 -7.4560022 -5.871377 -4.2657676 -2.7847235][-3.8790767 -3.2055364 -2.9148417 -2.8620739 -2.895987 -3.2838094 -4.2370005 -5.4621882 -6.8946557 -7.9203949 -8.3148174 -7.9730444 -6.8861895 -5.3809414 -3.7573755]]...]
INFO - root - 2017-12-16 05:57:07.538113: step 29010, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.284 sec/batch; 23h:55m:00s remains)
INFO - root - 2017-12-16 05:57:10.405781: step 29020, loss = 0.38, batch loss = 0.33 (28.3 examples/sec; 0.283 sec/batch; 23h:50m:00s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 05:57:13.219863: step 29030, loss = 0.40, batch loss = 0.34 (29.7 examples/sec; 0.269 sec/batch; 22h:41m:42s remains)
INFO - root - 2017-12-16 05:57:16.116636: step 29040, loss = 0.28, batch loss = 0.22 (29.4 examples/sec; 0.272 sec/batch; 22h:54m:33s remains)
INFO - root - 2017-12-16 05:57:18.965819: step 29050, loss = 0.27, batch loss = 0.21 (27.5 examples/sec; 0.291 sec/batch; 24h:33m:08s remains)
INFO - root - 2017-12-16 05:57:21.751653: step 29060, loss = 0.22, batch loss = 0.16 (28.5 examples/sec; 0.280 sec/batch; 23h:37m:13s remains)
INFO - root - 2017-12-16 05:57:24.569284: step 29070, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.279 sec/batch; 23h:33m:23s remains)
INFO - root - 2017-12-16 05:57:27.407455: step 29080, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.278 sec/batch; 23h:27m:46s remains)
INFO - root - 2017-12-16 05:57:30.269180: step 29090, loss = 0.31, batch loss = 0.25 (27.3 examples/sec; 0.293 sec/batch; 24h:43m:18s remains)
INFO - root - 2017-12-16 05:57:33.057845: step 29100, loss = 0.28, batch loss = 0.22 (29.1 examples/sec; 0.275 sec/batch; 23h:08m:36s remains)
2017-12-16 05:57:33.515909: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.603364 -2.00899 -2.6933973 -3.2839384 -3.9149191 -4.5207105 -5.0025868 -5.1685891 -5.19104 -5.0670371 -4.9232349 -4.8922319 -4.8293061 -4.9050593 -5.0098825][-0.95536232 -1.2451255 -1.8709941 -2.649684 -3.3960311 -3.7912238 -4.0001464 -4.0292077 -3.9596884 -4.0125914 -4.1454387 -4.15114 -4.1044722 -4.2234755 -4.4076657][-1.0529783 -1.4356613 -1.9703534 -2.4476073 -2.7718816 -2.8595738 -2.7104316 -2.494102 -2.3705261 -2.4121623 -2.7482715 -3.1706381 -3.4664762 -3.761785 -4.0426469][-1.5837123 -1.9700789 -2.5693517 -2.9878702 -2.9628136 -2.3280227 -1.3899603 -0.60920382 -0.40802336 -0.79795623 -1.5581844 -2.3926191 -2.9181461 -3.3721585 -3.5635467][-2.8715844 -3.3899236 -3.7956312 -3.8962617 -3.4209337 -2.2536318 -0.63910961 0.65622854 1.0605025 0.50542307 -0.74039865 -2.283201 -3.0774317 -2.9939642 -2.5791125][-3.1626723 -3.7347479 -4.1608186 -3.979296 -2.8053572 -1.1291139 0.83528423 2.4697256 2.8790793 1.6063399 -0.43297911 -2.1019845 -2.6417489 -2.5853112 -1.9027796][-3.3821216 -3.5092304 -3.5728931 -3.2242694 -1.7744622 0.53987026 3.017787 4.6291618 4.8054762 3.3019724 0.981894 -1.3811574 -2.4172506 -1.8797309 -0.43583465][-3.7365258 -3.5983844 -3.4870834 -2.7769728 -1.1257501 1.2947645 3.8991289 5.2553482 5.0633211 3.6550121 1.291955 -0.86573625 -1.4646082 -0.95090318 0.30832672][-3.5286889 -3.286448 -2.9778481 -2.6756606 -1.6617782 0.31175613 2.4874072 3.8992052 3.9330559 2.2527127 0.042591095 -1.3556311 -1.406908 -0.30503035 1.0887685][-3.17978 -3.1681666 -3.2333107 -2.8446643 -2.152025 -1.2922618 0.097676277 1.3213949 1.304647 0.094464779 -1.2644012 -2.0475957 -1.646857 -0.50181222 0.56540632][-3.6356666 -3.394264 -3.3347073 -3.5635982 -3.3519115 -2.6062264 -1.9476824 -1.6257873 -1.6309087 -2.2006595 -3.0876875 -3.3031266 -2.5596681 -1.2922952 -0.46598983][-3.7000978 -3.9553602 -4.3659616 -4.6508837 -4.55199 -4.3157969 -3.9006822 -3.456423 -3.4580462 -3.5815458 -3.6881032 -3.8478062 -3.5701313 -2.781697 -2.4951375][-3.5047147 -3.8688025 -4.265059 -4.765522 -5.2302146 -5.0934567 -4.6277304 -4.1182432 -3.6756 -3.684876 -3.7725651 -3.7064564 -3.7717574 -3.9085402 -4.1052551][-3.427402 -3.8715549 -4.2381759 -4.7200451 -5.1492271 -5.2421417 -5.0681076 -4.7269917 -4.0365934 -3.3142579 -2.8687425 -3.072382 -3.6723356 -4.2345867 -4.6267838][-2.7115028 -3.2910347 -3.9210873 -4.5410643 -4.9785175 -4.8620663 -4.2807775 -3.6751571 -2.9712014 -2.4928679 -2.2852561 -2.361758 -2.897531 -3.8053191 -4.5072203]]...]
INFO - root - 2017-12-16 05:57:36.360970: step 29110, loss = 0.32, batch loss = 0.26 (27.8 examples/sec; 0.287 sec/batch; 24h:13m:34s remains)
INFO - root - 2017-12-16 05:57:39.207440: step 29120, loss = 0.22, batch loss = 0.16 (29.2 examples/sec; 0.274 sec/batch; 23h:05m:03s remains)
INFO - root - 2017-12-16 05:57:42.038617: step 29130, loss = 0.35, batch loss = 0.29 (29.4 examples/sec; 0.272 sec/batch; 22h:55m:30s remains)
INFO - root - 2017-12-16 05:57:44.863211: step 29140, loss = 0.33, batch loss = 0.27 (28.5 examples/sec; 0.280 sec/batch; 23h:37m:03s remains)
INFO - root - 2017-12-16 05:57:47.701007: step 29150, loss = 0.24, batch loss = 0.18 (28.7 examples/sec; 0.279 sec/batch; 23h:29m:20s remains)
INFO - root - 2017-12-16 05:57:50.503886: step 29160, loss = 0.31, batch loss = 0.25 (29.8 examples/sec; 0.268 sec/batch; 22h:37m:04s remains)
INFO - root - 2017-12-16 05:57:53.306290: step 29170, loss = 0.23, batch loss = 0.17 (29.3 examples/sec; 0.273 sec/batch; 23h:00m:05s remains)
INFO - root - 2017-12-16 05:57:56.138900: step 29180, loss = 0.31, batch loss = 0.25 (27.6 examples/sec; 0.290 sec/batch; 24h:27m:40s remains)
INFO - root - 2017-12-16 05:57:58.996669: step 29190, loss = 0.23, batch loss = 0.17 (29.8 examples/sec; 0.268 sec/batch; 22h:37m:03s remains)
INFO - root - 2017-12-16 05:58:01.802631: step 29200, loss = 0.26, batch loss = 0.20 (29.1 examples/sec; 0.275 sec/batch; 23h:09m:54s remains)
2017-12-16 05:58:02.274407: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9146292 -3.2261467 -3.5328851 -3.9638627 -4.4037724 -4.7885227 -5.1383581 -5.6556149 -6.14339 -6.3775887 -6.3338094 -6.158051 -5.8715706 -5.3307056 -5.030539][-2.6419148 -2.8869028 -3.2041709 -3.6638021 -4.0878859 -4.7255168 -5.2830577 -5.8313365 -6.3342 -6.7419615 -7.0147829 -6.8514042 -6.4913635 -5.8501911 -5.1884141][-2.2890725 -2.354867 -2.6250656 -3.1143923 -3.5469651 -4.116437 -4.5531979 -5.145843 -5.6857324 -6.2055149 -6.68196 -6.8417344 -6.65057 -5.8262186 -5.0560207][-1.788034 -1.7802939 -1.9423645 -2.3493221 -2.7979765 -3.2045302 -3.4382679 -3.6878402 -3.9961963 -4.60611 -5.3140635 -5.9237218 -6.0735278 -5.4684935 -4.5878491][-1.3391683 -1.1412327 -1.2269843 -1.5786462 -1.8593938 -1.8096025 -1.5179148 -1.3961632 -1.7102761 -2.3012135 -3.2266116 -4.2879758 -4.9231596 -4.8037925 -4.206634][-1.2076414 -0.90706563 -0.81003833 -0.81614208 -0.61365008 -0.19547081 0.48311424 1.1806278 1.1826758 0.27232218 -1.3508756 -2.8579791 -3.8218732 -4.0410166 -3.7573729][-1.2130349 -0.90410113 -0.81711054 -0.61857486 -0.09536171 0.86234093 2.2212849 3.166358 3.1607285 2.2620211 0.69094038 -1.2056367 -2.8103013 -3.3402281 -3.1543739][-1.1789618 -0.989583 -1.0586345 -0.707927 0.060119152 1.3774605 2.9796181 4.1562786 4.5384359 3.6504192 1.9399242 0.14351225 -1.4467337 -2.4912946 -2.7699466][-1.7554119 -1.4076159 -1.1965563 -1.0306716 -0.47452807 0.89583683 2.4844236 3.8169031 4.3399591 3.6227713 2.1066446 0.303411 -1.2773347 -2.3328929 -2.5270977][-2.2377841 -2.2905693 -2.3636889 -2.1567013 -1.5072732 -0.61036396 0.43170261 1.6823092 2.3512206 2.0675564 1.0366817 -0.41009045 -1.7022147 -2.6317883 -2.8489366][-3.3802068 -3.1690164 -3.1713121 -3.322437 -3.1445251 -2.5523071 -1.6705756 -0.79299068 -0.29309988 -0.24783134 -0.63549948 -1.4386263 -2.3164904 -2.9660089 -2.8602772][-4.4361534 -4.3448811 -4.4053311 -4.4560323 -4.4108353 -4.3472562 -3.9922333 -3.3080697 -2.6870961 -2.3882105 -2.5044951 -2.9299479 -3.3007383 -3.5618026 -3.2601445][-5.4463873 -5.4930696 -5.6616793 -5.941546 -6.0981541 -6.0909925 -5.8995886 -5.5648603 -5.1256156 -4.6701965 -4.337091 -4.379077 -4.4727879 -4.5473223 -4.174633][-6.4966345 -6.5476551 -6.5940733 -6.7343283 -6.9706483 -7.0956807 -7.120255 -6.93868 -6.6196795 -6.15889 -5.7203264 -5.5613217 -5.4250846 -5.2861929 -4.7997365][-6.6058521 -6.6237049 -6.6163177 -6.7965183 -6.9836221 -7.0004635 -7.03401 -7.0519452 -6.9824057 -6.6107912 -6.1959648 -5.9088736 -5.6982532 -5.4711852 -5.0650482]]...]
INFO - root - 2017-12-16 05:58:05.136581: step 29210, loss = 0.28, batch loss = 0.22 (29.3 examples/sec; 0.273 sec/batch; 22h:58m:08s remains)
INFO - root - 2017-12-16 05:58:08.006162: step 29220, loss = 0.28, batch loss = 0.22 (26.7 examples/sec; 0.300 sec/batch; 25h:16m:34s remains)
INFO - root - 2017-12-16 05:58:10.843819: step 29230, loss = 0.19, batch loss = 0.14 (27.9 examples/sec; 0.286 sec/batch; 24h:07m:26s remains)
INFO - root - 2017-12-16 05:58:13.696133: step 29240, loss = 0.28, batch loss = 0.22 (28.5 examples/sec; 0.281 sec/batch; 23h:39m:37s remains)
INFO - root - 2017-12-16 05:58:16.485212: step 29250, loss = 0.30, batch loss = 0.24 (27.8 examples/sec; 0.287 sec/batch; 24h:11m:50s remains)
INFO - root - 2017-12-16 05:58:19.315427: step 29260, loss = 0.23, batch loss = 0.18 (29.0 examples/sec; 0.276 sec/batch; 23h:15m:58s remains)
INFO - root - 2017-12-16 05:58:22.132826: step 29270, loss = 0.26, batch loss = 0.20 (27.4 examples/sec; 0.292 sec/batch; 24h:33m:50s remains)
INFO - root - 2017-12-16 05:58:24.972825: step 29280, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 23h:45m:18s remains)
INFO - root - 2017-12-16 05:58:27.759779: step 29290, loss = 0.27, batch loss = 0.21 (29.4 examples/sec; 0.272 sec/batch; 22h:55m:07s remains)
INFO - root - 2017-12-16 05:58:30.571474: step 29300, loss = 0.23, batch loss = 0.18 (29.6 examples/sec; 0.270 sec/batch; 22h:45m:29s remains)
2017-12-16 05:58:31.019895: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7777941 -4.1429219 -4.5024829 -4.8347783 -5.0983777 -5.2636228 -5.3926287 -5.6165118 -5.8040218 -5.743381 -5.43649 -5.14402 -4.7178955 -4.1679878 -3.7001841][-3.6288047 -3.9711018 -4.3348756 -4.7029619 -4.9840121 -5.3163238 -5.6020622 -5.6762815 -5.7125683 -5.8645191 -5.9320974 -5.6960926 -5.20094 -4.6105051 -3.9837453][-3.4233503 -3.7460856 -4.0725574 -4.322052 -4.515439 -4.6580696 -4.7204256 -4.9549646 -5.2030511 -5.4068789 -5.60803 -5.6691833 -5.5368156 -4.9762068 -4.2896485][-3.0258868 -3.2101912 -3.3225284 -3.3002903 -3.2526414 -3.124505 -3.0345728 -3.1246104 -3.4084315 -4.0364318 -4.6260786 -5.0449405 -5.2270126 -4.9643722 -4.5055771][-2.6475668 -2.5587697 -2.3726869 -1.9442575 -1.3938539 -0.86425209 -0.48234439 -0.42589092 -0.8563509 -1.7547545 -2.8397825 -3.8866487 -4.5143223 -4.5833263 -4.3612132][-2.5599246 -2.3086364 -1.6721582 -0.72914433 0.39680862 1.385396 2.1580176 2.4082718 1.8530321 0.756825 -0.70527148 -2.2238064 -3.4435115 -4.0445876 -4.1154833][-2.6776061 -2.2709603 -1.522886 -0.23760319 1.4960332 3.084866 4.1875811 4.5249729 4.0403671 2.857162 1.1294022 -0.6775229 -2.1560628 -3.1191444 -3.5546145][-3.0849693 -2.6656227 -1.8542702 -0.38790464 1.3860416 3.1864681 4.6317949 5.1385584 4.7200708 3.6416492 2.0603013 0.16400051 -1.5164151 -2.5840158 -3.1234064][-3.8455949 -3.5842094 -2.7608414 -1.5498641 -0.012318134 1.7684989 3.2939682 4.0767269 4.0652609 3.2250051 1.8107815 0.17800903 -1.4168055 -2.6208668 -3.2957134][-4.3923659 -4.4282789 -4.1855989 -3.4429603 -2.0547702 -0.569993 0.68981028 1.7088656 2.1526651 1.7341771 0.7511816 -0.68163443 -2.0622237 -3.0929313 -3.6681457][-5.1603847 -5.0969791 -4.9059877 -4.6767483 -4.1022768 -3.0521216 -1.8648658 -0.92935777 -0.46336126 -0.37128735 -0.79742479 -1.7949524 -2.9083557 -3.7899275 -4.2284122][-5.6217709 -5.7542543 -5.6528816 -5.4096012 -5.0652547 -4.5529585 -3.8587263 -3.1456003 -2.6207972 -2.3757002 -2.5629971 -3.089608 -3.7766337 -4.3547468 -4.6777563][-5.434474 -5.7003803 -5.8920379 -5.8748908 -5.7727637 -5.4301472 -4.9464383 -4.4278083 -3.9312987 -3.6285849 -3.6046929 -3.8795779 -4.2137489 -4.6066203 -4.8365192][-4.9975319 -5.11141 -5.1871109 -5.2732239 -5.3026209 -5.1769543 -5.0667706 -4.7198296 -4.3833914 -4.08405 -3.8353069 -3.9009585 -4.0893116 -4.2959657 -4.4040074][-4.2816119 -4.2199521 -4.1539383 -4.2890029 -4.4011264 -4.3451352 -4.2866058 -4.1424751 -4.0388036 -3.8366728 -3.6617227 -3.6109807 -3.5455623 -3.6260159 -3.7406878]]...]
INFO - root - 2017-12-16 05:58:33.865097: step 29310, loss = 0.23, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 23h:57m:59s remains)
INFO - root - 2017-12-16 05:58:36.642614: step 29320, loss = 0.31, batch loss = 0.25 (28.4 examples/sec; 0.281 sec/batch; 23h:41m:28s remains)
INFO - root - 2017-12-16 05:58:39.495502: step 29330, loss = 0.30, batch loss = 0.24 (28.4 examples/sec; 0.282 sec/batch; 23h:44m:42s remains)
INFO - root - 2017-12-16 05:58:42.351813: step 29340, loss = 0.24, batch loss = 0.18 (27.7 examples/sec; 0.289 sec/batch; 24h:20m:55s remains)
INFO - root - 2017-12-16 05:58:45.124156: step 29350, loss = 0.35, batch loss = 0.30 (29.4 examples/sec; 0.272 sec/batch; 22h:54m:38s remains)
INFO - root - 2017-12-16 05:58:47.915137: step 29360, loss = 0.37, batch loss = 0.31 (28.4 examples/sec; 0.282 sec/batch; 23h:43m:27s remains)
INFO - root - 2017-12-16 05:58:50.719975: step 29370, loss = 0.24, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 24h:04m:36s remains)
INFO - root - 2017-12-16 05:58:53.569958: step 29380, loss = 0.32, batch loss = 0.26 (26.4 examples/sec; 0.304 sec/batch; 25h:33m:46s remains)
INFO - root - 2017-12-16 05:58:56.410060: step 29390, loss = 0.25, batch loss = 0.20 (28.4 examples/sec; 0.282 sec/batch; 23h:44m:05s remains)
INFO - root - 2017-12-16 05:58:59.235446: step 29400, loss = 0.30, batch loss = 0.24 (29.0 examples/sec; 0.275 sec/batch; 23h:11m:14s remains)
2017-12-16 05:58:59.694161: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2998958 -3.1422281 -2.9243755 -2.5564518 -2.3247421 -2.1707067 -2.1017241 -1.9646952 -1.7587223 -1.4847779 -1.3305383 -1.2573478 -1.1730747 -1.0817537 -1.1324959][-2.000212 -2.0550373 -1.8194954 -1.436759 -1.0832992 -0.77523804 -0.72380137 -0.72067 -0.65858769 -0.513819 -0.40095139 -0.43749452 -0.43486357 -0.33007336 -0.325418][-1.5293467 -1.2455828 -0.81160331 -0.52309322 -0.18748379 -0.070795059 -0.11579037 -0.23949814 -0.37656021 -0.41214895 -0.51775718 -0.50252461 -0.24038887 -0.11137295 -0.21970177][-1.0966506 -0.89419222 -0.61545086 -0.22526407 0.1123395 0.37247658 0.25602722 -0.11962557 -0.39831018 -0.49218464 -0.60541797 -0.77937794 -0.82498884 -0.66713309 -0.66381121][-0.749104 -0.6977253 -0.44657397 -0.2799325 -0.033302784 0.2322402 0.097285748 -0.26773739 -0.75381541 -0.89404225 -1.1628776 -1.2546837 -1.3418312 -1.6376197 -1.8444071][-0.83790183 -0.7201817 -0.4988265 -0.27165651 -0.034526348 0.25633097 0.30201674 0.071953773 -0.25130844 -0.35764742 -0.71608853 -1.2890565 -1.8263454 -2.0457313 -2.2376082][-1.1690416 -1.113157 -0.89897895 -0.57812643 -0.32620859 0.14383268 0.40571165 0.42959929 0.37695646 0.35925817 0.29294205 -0.27263117 -1.0888681 -1.8820632 -2.4864321][-1.4521847 -1.4350822 -1.3168216 -0.91124344 -0.58339262 -0.062180996 0.45014048 0.6425786 0.74835682 0.84786224 0.79135132 0.24021769 -0.59280086 -1.5188842 -2.2816536][-2.0819392 -2.0247495 -1.8193314 -1.4097331 -1.0249386 -0.38528585 0.27068043 0.7290926 1.0874395 1.4348836 1.5198231 0.89718056 -0.04404211 -1.1122081 -1.8824849][-2.75627 -2.6620052 -2.4211779 -1.9657731 -1.5377402 -0.90902376 -0.25073385 0.28724909 0.711391 1.0605602 1.2712646 1.002512 0.20017576 -0.766006 -1.3979101][-3.0460095 -2.9490387 -2.7155023 -2.3709812 -2.0545974 -1.5674641 -0.9978292 -0.52801991 -0.0759964 0.35126066 0.55445194 0.39627934 -0.12068558 -0.746541 -1.297364][-3.3443213 -3.1809523 -3.0044351 -2.6438143 -2.4490418 -2.3929138 -2.173142 -1.7803905 -1.3412642 -0.863816 -0.53810763 -0.47517085 -0.68249035 -1.0730364 -1.3224344][-3.2792552 -3.1365042 -3.0870533 -2.9726186 -3.0758057 -3.1072176 -3.16394 -3.0218492 -2.7142415 -2.3902295 -1.859792 -1.572113 -1.450161 -1.4923489 -1.6158385][-3.013166 -2.8175712 -2.7642622 -2.9058452 -3.1936579 -3.5238752 -3.80421 -3.6693554 -3.4715335 -3.1358633 -2.610348 -2.2616341 -2.0120099 -1.8026822 -1.6623313][-2.9300745 -2.7364321 -2.7027962 -2.8515038 -3.1956277 -3.7965267 -4.1735506 -4.1785159 -4.0923553 -3.7229934 -3.2722869 -2.6157286 -2.1010208 -1.785924 -1.4795392]]...]
INFO - root - 2017-12-16 05:59:02.489280: step 29410, loss = 0.29, batch loss = 0.23 (26.9 examples/sec; 0.297 sec/batch; 25h:01m:35s remains)
INFO - root - 2017-12-16 05:59:05.323020: step 29420, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.285 sec/batch; 23h:58m:28s remains)
INFO - root - 2017-12-16 05:59:08.153891: step 29430, loss = 0.23, batch loss = 0.17 (27.4 examples/sec; 0.292 sec/batch; 24h:36m:40s remains)
INFO - root - 2017-12-16 05:59:10.970157: step 29440, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.283 sec/batch; 23h:48m:47s remains)
INFO - root - 2017-12-16 05:59:13.866539: step 29450, loss = 0.28, batch loss = 0.23 (27.3 examples/sec; 0.293 sec/batch; 24h:38m:43s remains)
INFO - root - 2017-12-16 05:59:16.697850: step 29460, loss = 0.42, batch loss = 0.37 (28.7 examples/sec; 0.279 sec/batch; 23h:29m:39s remains)
INFO - root - 2017-12-16 05:59:19.548876: step 29470, loss = 0.26, batch loss = 0.21 (28.9 examples/sec; 0.277 sec/batch; 23h:16m:58s remains)
INFO - root - 2017-12-16 05:59:22.368582: step 29480, loss = 0.35, batch loss = 0.29 (27.2 examples/sec; 0.295 sec/batch; 24h:47m:35s remains)
INFO - root - 2017-12-16 05:59:25.155764: step 29490, loss = 0.36, batch loss = 0.31 (28.9 examples/sec; 0.277 sec/batch; 23h:16m:41s remains)
INFO - root - 2017-12-16 05:59:27.935744: step 29500, loss = 0.34, batch loss = 0.28 (28.8 examples/sec; 0.278 sec/batch; 23h:23m:14s remains)
2017-12-16 05:59:28.390057: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3988109 -3.9303112 -4.4630346 -4.9237556 -5.256465 -5.4760461 -5.5614405 -5.5090504 -5.3781328 -5.0852323 -4.7496243 -4.5827241 -4.6590552 -4.7295132 -4.8284793][-3.665252 -4.151495 -4.7004342 -5.239078 -5.4704123 -5.7817287 -5.99184 -6.1147156 -6.1576271 -6.1564875 -6.0376587 -5.7162633 -5.4896417 -5.4274931 -5.3658524][-4.2004771 -4.6033068 -4.9373646 -5.13167 -5.196198 -5.3058376 -5.410152 -5.7249794 -6.0973377 -6.3265004 -6.4871616 -6.4383292 -6.192574 -5.7932606 -5.6436472][-4.4174304 -4.8184719 -4.9514942 -4.7601995 -4.3105783 -3.9012709 -3.7370872 -3.7964461 -4.1789861 -5.0269403 -5.9127378 -6.2452097 -6.0932288 -5.7717247 -5.5422192][-4.430016 -4.4933949 -4.1031356 -3.4698248 -2.53682 -1.3731301 -0.72647572 -0.76285958 -1.3783622 -2.3512521 -3.5008016 -4.6889157 -5.0382986 -4.9279747 -4.918714][-4.31624 -4.1380734 -3.4891319 -2.2380054 -0.29898453 1.5158763 2.6945062 3.0253768 2.212667 0.51181412 -1.4454587 -2.6815457 -2.9280214 -3.0682271 -3.1831012][-4.1546478 -3.7643547 -2.8474445 -1.3316853 0.93235826 3.6030931 5.6474047 6.1925631 5.2651386 3.5298734 1.3874764 -0.64863324 -1.1494083 -1.0314419 -1.2713504][-3.7709045 -3.5280008 -2.8641109 -1.00014 1.4642987 4.2075491 6.3830948 7.3041115 6.639863 4.7214041 2.8086886 1.3023396 0.88454962 1.0248065 0.75882721][-3.8426495 -3.4162474 -2.4266155 -1.1026208 0.57247257 2.8903399 4.9173822 5.7668028 5.4097691 4.2567558 2.9250431 1.831605 1.7133174 1.7861366 1.503283][-4.2654295 -4.1434245 -3.6687851 -2.6082926 -1.3280692 0.1097846 1.4486589 2.4316154 2.6971469 2.3144383 1.9348555 1.6231618 2.1050134 2.3436561 1.9769082][-5.1228881 -4.6590662 -4.2518625 -4.0078254 -3.7318609 -3.1107469 -2.3173301 -1.513943 -0.79840136 -0.39185047 0.079807281 0.46138906 1.212574 1.542665 1.3199325][-5.6605868 -5.3531537 -5.1733279 -5.0482454 -5.2132173 -5.4427862 -5.4477749 -5.0660205 -4.3753395 -3.3870006 -2.2990677 -1.3894918 -0.5323689 -0.22961569 -0.65869379][-6.4004631 -6.0079718 -5.7262678 -5.8823023 -6.1224012 -6.3603878 -6.8381972 -6.9637461 -6.3445082 -5.3020048 -4.1038518 -3.1224198 -2.5277009 -2.320771 -2.3513002][-6.9742956 -6.2011843 -5.5716543 -5.3823886 -5.5897689 -5.8863606 -6.4112821 -6.9799976 -7.0352173 -6.2627897 -5.0390563 -4.2061443 -3.8584516 -3.9607408 -4.0888753][-6.3097057 -5.698688 -4.9142976 -4.6041284 -4.8536844 -5.212924 -5.8578773 -6.2965422 -6.5835867 -6.4306288 -5.7032924 -5.1142464 -4.8320069 -4.9347272 -4.7520657]]...]
INFO - root - 2017-12-16 05:59:31.222707: step 29510, loss = 0.20, batch loss = 0.14 (28.0 examples/sec; 0.285 sec/batch; 24h:00m:16s remains)
INFO - root - 2017-12-16 05:59:34.048956: step 29520, loss = 0.28, batch loss = 0.22 (28.2 examples/sec; 0.284 sec/batch; 23h:52m:53s remains)
INFO - root - 2017-12-16 05:59:36.881538: step 29530, loss = 0.35, batch loss = 0.29 (28.4 examples/sec; 0.282 sec/batch; 23h:43m:17s remains)
INFO - root - 2017-12-16 05:59:39.733588: step 29540, loss = 0.26, batch loss = 0.20 (29.3 examples/sec; 0.273 sec/batch; 22h:56m:47s remains)
INFO - root - 2017-12-16 05:59:42.562491: step 29550, loss = 0.27, batch loss = 0.21 (28.8 examples/sec; 0.278 sec/batch; 23h:21m:22s remains)
INFO - root - 2017-12-16 05:59:45.365957: step 29560, loss = 0.30, batch loss = 0.24 (28.0 examples/sec; 0.286 sec/batch; 24h:04m:14s remains)
INFO - root - 2017-12-16 05:59:48.131374: step 29570, loss = 0.29, batch loss = 0.23 (28.2 examples/sec; 0.283 sec/batch; 23h:51m:17s remains)
INFO - root - 2017-12-16 05:59:50.966524: step 29580, loss = 0.30, batch loss = 0.24 (27.3 examples/sec; 0.293 sec/batch; 24h:37m:54s remains)
INFO - root - 2017-12-16 05:59:53.817609: step 29590, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.286 sec/batch; 24h:01m:33s remains)
INFO - root - 2017-12-16 05:59:56.641645: step 29600, loss = 0.27, batch loss = 0.21 (27.4 examples/sec; 0.292 sec/batch; 24h:34m:40s remains)
2017-12-16 05:59:57.108953: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0751977 -4.5241432 -4.9362955 -5.2589149 -5.368329 -5.4056063 -5.425282 -5.4275565 -5.4192905 -5.3803062 -5.1993446 -5.057127 -4.8546576 -4.4689703 -4.0845757][-4.621366 -5.2191634 -5.7284412 -6.103652 -6.2532325 -6.4575567 -6.5850668 -6.6436806 -6.7957273 -6.9665165 -7.0443649 -6.9687777 -6.6443005 -6.190495 -5.6920457][-5.1021614 -5.9754133 -6.5840654 -6.8183203 -6.9133615 -6.9657965 -6.9563408 -7.2018633 -7.5023813 -7.8079953 -8.16125 -8.4811974 -8.5302715 -8.2354794 -7.5910654][-5.2728329 -6.1988916 -6.8525486 -7.0764813 -6.97406 -6.6463604 -6.3667078 -6.2356443 -6.3487387 -7.000679 -7.9469576 -8.788765 -9.3765869 -9.6179638 -9.2839317][-5.1629782 -5.9584403 -6.3621755 -6.3145571 -5.7726536 -4.9198565 -4.0500264 -3.5361662 -3.5602453 -4.2798047 -5.574636 -7.3110285 -8.9358635 -9.9170218 -10.02687][-4.8514442 -5.1723204 -5.2575645 -4.9085808 -3.8090839 -2.4107668 -0.86825156 0.33260918 0.78247786 -0.02021122 -1.801405 -4.2667236 -6.7482595 -8.703577 -9.7183027][-4.6796031 -4.553596 -4.0927486 -3.3667169 -2.0952406 -0.22006655 1.9219618 3.4714103 4.2652178 3.9680519 2.2399812 -0.61936569 -3.8195257 -6.4585361 -8.0477276][-4.3593559 -4.2376676 -3.7895126 -2.6659539 -0.936075 0.96154118 3.1502051 5.2240076 6.6834612 6.4168663 4.69366 1.9682322 -1.160049 -4.1278787 -6.1589122][-3.9660587 -4.0224013 -3.7696958 -2.8882675 -1.3655076 0.8683567 3.225173 5.2191429 6.5702991 6.8624783 5.633029 2.9925895 -0.16759443 -2.9261293 -4.7685366][-4.1003909 -4.3210378 -4.4899712 -4.2380576 -3.1722298 -1.3639443 0.8614378 3.0202284 4.641283 4.7896147 3.7424097 1.6343875 -0.85509872 -3.184185 -4.7656856][-5.0189552 -5.2298436 -5.3549414 -5.3112006 -5.0517807 -4.12852 -2.5131807 -0.82781911 0.44771385 0.74433231 0.35009241 -1.1211941 -2.9121065 -4.539495 -5.5949154][-5.6874876 -6.0075955 -6.35122 -6.4099469 -6.3109865 -5.9327173 -5.2971563 -4.5214524 -3.733103 -3.5668063 -3.8433607 -4.6683621 -5.370646 -6.0936813 -6.5981817][-6.1064477 -6.6141815 -7.1641407 -7.3685694 -7.6903305 -7.6855226 -7.5148816 -7.237257 -6.9948397 -6.8145022 -6.6476073 -6.7875137 -6.9001474 -6.9794817 -6.9986334][-5.9028444 -6.4063478 -6.993288 -7.5073776 -8.1191368 -8.3285036 -8.511405 -8.5129986 -8.438324 -8.1679459 -7.7633882 -7.4233513 -7.1082039 -6.840672 -6.5084934][-5.4540377 -5.736639 -6.1075397 -6.6901755 -7.3272977 -7.7567463 -8.1012831 -8.0681009 -7.8990507 -7.6207314 -7.2049894 -6.6452241 -6.1306677 -5.8857975 -5.6670241]]...]
INFO - root - 2017-12-16 05:59:59.933084: step 29610, loss = 0.25, batch loss = 0.19 (27.7 examples/sec; 0.289 sec/batch; 24h:18m:55s remains)
INFO - root - 2017-12-16 06:00:02.796075: step 29620, loss = 0.35, batch loss = 0.29 (28.2 examples/sec; 0.284 sec/batch; 23h:52m:40s remains)
INFO - root - 2017-12-16 06:00:05.596428: step 29630, loss = 0.22, batch loss = 0.16 (29.4 examples/sec; 0.272 sec/batch; 22h:53m:44s remains)
INFO - root - 2017-12-16 06:00:08.428833: step 29640, loss = 0.20, batch loss = 0.14 (28.5 examples/sec; 0.281 sec/batch; 23h:37m:51s remains)
INFO - root - 2017-12-16 06:00:11.238463: step 29650, loss = 0.23, batch loss = 0.18 (29.3 examples/sec; 0.273 sec/batch; 22h:56m:35s remains)
INFO - root - 2017-12-16 06:00:14.060194: step 29660, loss = 0.28, batch loss = 0.22 (28.9 examples/sec; 0.277 sec/batch; 23h:18m:53s remains)
INFO - root - 2017-12-16 06:00:16.901523: step 29670, loss = 0.31, batch loss = 0.25 (29.2 examples/sec; 0.274 sec/batch; 23h:01m:46s remains)
INFO - root - 2017-12-16 06:00:19.742517: step 29680, loss = 0.27, batch loss = 0.21 (28.8 examples/sec; 0.278 sec/batch; 23h:21m:58s remains)
INFO - root - 2017-12-16 06:00:22.565768: step 29690, loss = 0.24, batch loss = 0.18 (29.0 examples/sec; 0.276 sec/batch; 23h:12m:58s remains)
INFO - root - 2017-12-16 06:00:25.393491: step 29700, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 23h:58m:43s remains)
2017-12-16 06:00:25.861701: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1875944 -5.3491693 -5.5069308 -5.6874323 -5.7344155 -5.5122375 -5.0802407 -4.8005834 -4.9051785 -5.2709656 -5.8500514 -6.1300774 -6.464385 -6.4726996 -6.3203826][-4.43964 -4.6824675 -4.9688411 -5.0680208 -5.0635991 -5.1828852 -5.0202856 -4.7412586 -4.6135273 -4.8177376 -5.3157592 -5.5457706 -5.8856211 -5.592433 -5.3642683][-3.47454 -3.5690877 -3.8299975 -4.0017686 -4.1440163 -4.2106781 -4.0644627 -3.9882228 -4.06973 -4.15716 -4.3210945 -4.5248985 -4.7678843 -4.5717416 -4.4921675][-2.305701 -2.2880082 -2.4144249 -2.5395594 -2.5799608 -2.7547178 -2.6866183 -2.5894041 -2.5075469 -2.6643314 -2.9757357 -3.2317724 -3.58047 -3.5135479 -3.5127397][-1.013423 -0.64473891 -0.62123919 -0.80969477 -1.0329845 -1.0774744 -0.92755771 -0.96270752 -1.0452144 -1.1272199 -1.3548763 -1.617866 -1.9095476 -2.1805716 -2.554945][-0.036528111 0.51795197 0.60701323 0.60830259 0.80290222 0.65602207 0.48177671 0.54642391 0.7390728 0.64994431 0.24551821 -0.25364304 -0.626024 -0.90602016 -1.2641511][0.46043777 1.126184 1.2628679 1.1983476 1.3782578 1.7974553 2.3948703 2.4016395 2.1931791 2.0423846 1.5992384 0.86836433 0.25883293 -0.29328346 -0.9209168][0.6004529 0.86608505 0.91617775 0.9849658 1.4456501 1.967319 2.5943542 2.9980898 3.1619158 2.8819609 2.1297703 1.225162 0.45549107 -0.070183754 -0.51100016][-0.22369719 -0.07827282 -0.10590887 -0.13886118 0.18788385 0.98440504 1.7927747 2.2432299 2.5391378 2.5544896 2.1870055 1.3457413 0.63718367 0.24237585 -0.20362902][-1.349788 -1.582046 -1.8725774 -1.7503939 -1.2755115 -0.64443326 -0.18164921 0.45398426 1.0367208 1.146184 1.0163126 0.55446959 0.24449635 -0.13261747 -0.53645587][-2.8302579 -2.9732852 -2.9106443 -3.0235691 -2.8909445 -2.1930459 -1.8196657 -1.7661984 -1.5671437 -0.95190096 -0.86717749 -1.3293602 -1.5667195 -1.4889016 -1.4023325][-3.9559317 -4.1011677 -4.1041594 -3.9224553 -3.5201278 -3.2930102 -3.4080923 -3.5160375 -3.4297402 -3.3923931 -3.3852098 -3.4193447 -3.4464431 -3.3474092 -3.0294459][-5.0996251 -5.0929356 -4.7797074 -4.6746292 -4.6066985 -4.2205844 -4.1221337 -4.7791038 -5.30721 -5.396946 -5.2497416 -5.2682004 -5.2216473 -5.1105638 -4.5584149][-5.5303731 -5.6511068 -5.4649591 -5.1924653 -4.9437175 -4.7579737 -5.0652514 -5.5104065 -6.0951023 -6.7458677 -7.060504 -6.9973669 -6.863596 -6.6843472 -5.8991089][-5.9936829 -6.0501533 -5.5922942 -5.0975189 -4.6863818 -4.4503627 -4.513485 -4.9794006 -5.9174004 -6.6800914 -7.0789595 -7.3353562 -7.4501133 -7.3870058 -6.5624094]]...]
INFO - root - 2017-12-16 06:00:28.679501: step 29710, loss = 0.20, batch loss = 0.14 (28.9 examples/sec; 0.277 sec/batch; 23h:17m:49s remains)
INFO - root - 2017-12-16 06:00:31.516757: step 29720, loss = 0.21, batch loss = 0.15 (27.5 examples/sec; 0.291 sec/batch; 24h:30m:19s remains)
INFO - root - 2017-12-16 06:00:34.393189: step 29730, loss = 0.27, batch loss = 0.21 (27.7 examples/sec; 0.288 sec/batch; 24h:14m:55s remains)
INFO - root - 2017-12-16 06:00:37.185453: step 29740, loss = 0.24, batch loss = 0.18 (28.9 examples/sec; 0.277 sec/batch; 23h:18m:41s remains)
INFO - root - 2017-12-16 06:00:40.056804: step 29750, loss = 0.22, batch loss = 0.16 (28.0 examples/sec; 0.285 sec/batch; 24h:00m:06s remains)
INFO - root - 2017-12-16 06:00:42.877638: step 29760, loss = 0.26, batch loss = 0.20 (29.0 examples/sec; 0.276 sec/batch; 23h:12m:59s remains)
INFO - root - 2017-12-16 06:00:45.733500: step 29770, loss = 0.24, batch loss = 0.18 (27.2 examples/sec; 0.295 sec/batch; 24h:46m:06s remains)
INFO - root - 2017-12-16 06:00:48.537062: step 29780, loss = 0.24, batch loss = 0.19 (28.9 examples/sec; 0.277 sec/batch; 23h:16m:07s remains)
INFO - root - 2017-12-16 06:00:51.365182: step 29790, loss = 0.52, batch loss = 0.46 (28.3 examples/sec; 0.282 sec/batch; 23h:45m:03s remains)
INFO - root - 2017-12-16 06:00:54.166349: step 29800, loss = 0.28, batch loss = 0.22 (28.5 examples/sec; 0.280 sec/batch; 23h:33m:52s remains)
2017-12-16 06:00:54.620695: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.62014 -3.4611311 -3.3781929 -3.4143009 -3.4079156 -3.5248179 -3.7056768 -3.874913 -3.8510861 -3.7177634 -3.4803734 -3.2335868 -2.9864378 -2.8502464 -3.0283444][-2.794579 -2.6452134 -2.6601915 -2.9180181 -3.1306932 -3.457062 -3.7252059 -4.0018759 -4.231216 -4.25006 -4.0168657 -3.6552832 -3.3194904 -3.11119 -3.1298261][-1.7720745 -1.7437458 -1.8832669 -2.2778704 -2.6537714 -3.0158706 -3.4333279 -3.9874852 -4.4254355 -4.5690188 -4.5658813 -4.2825818 -3.8781061 -3.5498579 -3.4910841][-0.9138031 -0.99657631 -1.195616 -1.5349643 -1.7441654 -2.1085563 -2.6435757 -3.2241681 -3.6467104 -4.1289654 -4.4501495 -4.3487396 -4.0191512 -3.7850804 -3.7844553][-1.0538359 -1.0822542 -1.1564746 -1.2542338 -1.1871352 -1.1720607 -1.3889499 -1.9191992 -2.6431592 -3.2268682 -3.6027868 -3.8671837 -3.7990346 -3.6440284 -3.6100004][-1.4761758 -1.3932357 -1.1473396 -0.81050563 -0.36250448 0.017045021 -0.011171818 -0.43133712 -1.1608512 -1.9746761 -2.7027521 -3.0305691 -2.9544461 -2.9695199 -3.0501733][-2.1386325 -1.6681128 -0.96125293 -0.15837431 0.73051071 1.5232048 1.6853681 1.2819948 0.54908562 -0.39688873 -1.2949555 -1.8062329 -1.8938262 -1.8013308 -1.8065565][-2.4827101 -1.8076959 -0.99769974 0.24581289 1.4649444 2.4045205 2.7737288 2.520009 1.8965626 0.99539614 0.14226341 -0.39102173 -0.43512392 -0.29037476 -0.40379953][-2.9550757 -2.1654222 -1.1756437 0.056463242 1.3297091 2.5627146 3.023613 2.9027987 2.4882951 1.832623 1.2791328 0.92528009 1.0168037 1.1105056 0.93629265][-3.3118191 -2.5761018 -1.6610379 -0.39619493 0.78246069 1.7351713 2.1238804 2.2676349 2.0682111 1.6652331 1.3873358 1.2307763 1.4260039 1.5492487 1.3815761][-4.1819224 -3.3525457 -2.2977061 -1.3040648 -0.54291129 0.40841293 0.77245808 0.75523233 0.66188 0.71489096 0.82971478 0.98431015 1.4199948 1.703146 1.5350223][-5.371655 -4.7294931 -3.8559856 -2.8178682 -1.9778919 -1.4093146 -1.1563604 -0.852947 -0.73477793 -0.41250324 0.03211689 0.47139168 0.96264172 1.1449814 0.92918062][-6.8222952 -6.3384681 -5.5160236 -4.6482348 -3.8831072 -3.1718628 -2.8123136 -2.534934 -2.2898602 -1.7550042 -1.1752396 -0.84101534 -0.52380824 -0.42302561 -0.73787618][-7.83755 -7.3652639 -6.6024723 -5.761528 -4.975337 -4.3874016 -4.0578575 -3.6540074 -3.4156032 -2.9955304 -2.5574243 -2.3990402 -2.2862589 -2.3476112 -2.6315703][-8.236166 -7.8131251 -7.1128454 -6.5024462 -5.8981805 -5.20713 -4.6399746 -4.2529769 -4.1437306 -3.8208442 -3.5981641 -3.6678834 -3.8085983 -3.876159 -3.8741405]]...]
INFO - root - 2017-12-16 06:00:57.442111: step 29810, loss = 0.25, batch loss = 0.19 (28.6 examples/sec; 0.279 sec/batch; 23h:29m:20s remains)
INFO - root - 2017-12-16 06:01:00.228841: step 29820, loss = 0.30, batch loss = 0.24 (29.4 examples/sec; 0.272 sec/batch; 22h:54m:25s remains)
INFO - root - 2017-12-16 06:01:03.015661: step 29830, loss = 0.36, batch loss = 0.30 (27.1 examples/sec; 0.295 sec/batch; 24h:46m:38s remains)
INFO - root - 2017-12-16 06:01:05.871217: step 29840, loss = 0.30, batch loss = 0.24 (25.2 examples/sec; 0.317 sec/batch; 26h:39m:12s remains)
INFO - root - 2017-12-16 06:01:08.689211: step 29850, loss = 0.23, batch loss = 0.17 (27.7 examples/sec; 0.289 sec/batch; 24h:18m:49s remains)
INFO - root - 2017-12-16 06:01:11.522403: step 29860, loss = 0.28, batch loss = 0.22 (28.3 examples/sec; 0.282 sec/batch; 23h:43m:40s remains)
INFO - root - 2017-12-16 06:01:14.386641: step 29870, loss = 0.24, batch loss = 0.19 (25.6 examples/sec; 0.312 sec/batch; 26h:14m:43s remains)
INFO - root - 2017-12-16 06:01:17.197502: step 29880, loss = 0.27, batch loss = 0.21 (28.9 examples/sec; 0.276 sec/batch; 23h:14m:11s remains)
INFO - root - 2017-12-16 06:01:19.985526: step 29890, loss = 0.32, batch loss = 0.26 (29.2 examples/sec; 0.274 sec/batch; 23h:03m:16s remains)
INFO - root - 2017-12-16 06:01:22.841743: step 29900, loss = 0.28, batch loss = 0.23 (27.6 examples/sec; 0.290 sec/batch; 24h:22m:15s remains)
2017-12-16 06:01:23.307719: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2121525 -3.0292583 -2.9007177 -2.7752752 -2.5602565 -2.5276668 -2.6131766 -3.1851826 -3.677206 -4.0094471 -4.3093696 -4.5558467 -4.4529748 -4.1947012 -4.1877046][-2.9387488 -2.8504138 -2.832459 -2.743032 -2.4873328 -2.4724994 -2.5446851 -2.9930115 -3.3227673 -3.7412376 -4.2635369 -4.4713273 -4.3369727 -4.2423368 -4.3304691][-2.7045779 -2.6253159 -2.7011027 -2.6662853 -2.4187958 -2.3874793 -2.4107625 -2.6108174 -2.8296323 -3.0719907 -3.4416549 -3.7988889 -4.0674286 -4.1591568 -4.2562895][-1.9074631 -2.0466833 -2.3154278 -2.4598036 -2.3047752 -2.1036043 -1.8828678 -1.8599267 -1.9633362 -2.0756955 -2.3710003 -2.9025369 -3.3450747 -3.72602 -4.022058][-1.2745669 -1.6158197 -2.0268784 -2.2303929 -2.1394041 -1.9438329 -1.5903809 -1.188195 -1.0357242 -1.2418919 -1.570344 -1.9968119 -2.3736775 -2.8915215 -3.2817807][-0.8118937 -1.3391461 -1.710012 -1.8864684 -1.8222315 -1.5769272 -1.0916998 -0.53294945 -0.25491762 -0.33678007 -0.59114814 -1.1012757 -1.6390266 -2.140898 -2.395112][-0.38744688 -0.80786276 -1.1424406 -1.249614 -1.0774646 -0.78616786 -0.35135365 0.099302769 0.43005848 0.3585906 0.058232307 -0.32528162 -0.68024635 -1.1940832 -1.5178671][-0.11892319 -0.46755886 -0.75949717 -0.70809245 -0.48790956 -0.13758469 0.25513458 0.68426371 0.97913933 0.86217642 0.51275778 0.043018341 -0.37684584 -0.917938 -1.2298086][-0.30777884 -0.42191982 -0.4994204 -0.35231256 -0.048983574 0.4289403 0.7285018 1.0030742 1.1580553 0.97499228 0.61272812 0.011232376 -0.52240825 -1.0456812 -1.3853705][-0.66492033 -0.42343569 -0.34464598 -0.16377354 0.15091324 0.60898066 0.93442631 1.1631484 1.2001719 0.83462763 0.32877254 -0.31524372 -0.99248838 -1.6251466 -2.0039909][-1.1391277 -0.68598104 -0.53703284 -0.30593061 0.08155632 0.5421648 0.84660673 0.77338362 0.50082111 0.094779491 -0.56732678 -1.2566302 -1.9008636 -2.4593935 -2.7964082][-2.1690123 -1.5141716 -1.216403 -1.003885 -0.63999724 -0.14565086 0.03147316 -0.11129141 -0.45309973 -1.003654 -1.7623012 -2.3842869 -2.8961849 -3.3577843 -3.6116927][-3.1502736 -2.4133003 -1.9086781 -1.6268356 -1.3493288 -0.91133428 -0.73602319 -1.0558243 -1.5208454 -1.9503555 -2.305671 -2.8130174 -3.4007068 -3.7091651 -3.78389][-3.945272 -3.0849009 -2.4260452 -2.1586418 -2.0582595 -1.7988532 -1.6859367 -1.9815943 -2.4477663 -2.8607433 -2.9935358 -3.114146 -3.3137102 -3.4250855 -3.4563508][-4.413238 -3.591923 -2.9046121 -2.5869117 -2.4740758 -2.3889222 -2.4928823 -2.8360395 -3.2919879 -3.489661 -3.4262023 -3.2143533 -3.1552653 -3.1633549 -3.1156931]]...]
INFO - root - 2017-12-16 06:01:26.155216: step 29910, loss = 0.36, batch loss = 0.30 (28.6 examples/sec; 0.280 sec/batch; 23h:32m:01s remains)
INFO - root - 2017-12-16 06:01:28.967888: step 29920, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.281 sec/batch; 23h:38m:53s remains)
INFO - root - 2017-12-16 06:01:31.826076: step 29930, loss = 0.32, batch loss = 0.26 (28.3 examples/sec; 0.283 sec/batch; 23h:45m:58s remains)
INFO - root - 2017-12-16 06:01:34.628378: step 29940, loss = 0.32, batch loss = 0.26 (29.7 examples/sec; 0.269 sec/batch; 22h:38m:25s remains)
INFO - root - 2017-12-16 06:01:37.458425: step 29950, loss = 0.20, batch loss = 0.14 (27.4 examples/sec; 0.292 sec/batch; 24h:33m:12s remains)
INFO - root - 2017-12-16 06:01:40.345677: step 29960, loss = 0.22, batch loss = 0.16 (28.0 examples/sec; 0.285 sec/batch; 23h:59m:02s remains)
INFO - root - 2017-12-16 06:01:43.129717: step 29970, loss = 0.27, batch loss = 0.21 (29.2 examples/sec; 0.274 sec/batch; 23h:03m:11s remains)
INFO - root - 2017-12-16 06:01:45.964006: step 29980, loss = 0.26, batch loss = 0.21 (28.1 examples/sec; 0.285 sec/batch; 23h:56m:34s remains)
INFO - root - 2017-12-16 06:01:48.769068: step 29990, loss = 0.21, batch loss = 0.15 (27.9 examples/sec; 0.287 sec/batch; 24h:04m:53s remains)
INFO - root - 2017-12-16 06:01:51.642951: step 30000, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.285 sec/batch; 23h:59m:18s remains)
2017-12-16 06:01:52.097562: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.291285 -4.2417541 -4.3561835 -4.1975951 -3.9173565 -3.7742796 -3.7885437 -3.8853941 -3.9966059 -4.0101972 -4.00785 -3.724896 -3.224983 -2.7103491 -2.5025578][-3.1172183 -3.1226289 -3.1791902 -3.3246584 -3.3620856 -3.4525869 -3.5755725 -3.6387169 -3.4374714 -3.440644 -3.2973602 -2.8395574 -2.4274564 -1.9589796 -1.7848098][-2.4932544 -2.3306365 -2.3852746 -2.6296866 -2.7922754 -3.0265861 -2.9378481 -2.8486946 -2.7625494 -2.597971 -2.500176 -2.1843967 -1.9169495 -1.6965685 -1.6574662][-1.5218296 -1.2888033 -1.4795313 -2.0755076 -2.5756636 -2.8420658 -2.7143826 -2.1076233 -1.3819065 -1.1642494 -1.2140474 -1.338485 -1.516093 -1.4533191 -1.1824641][-0.45596933 -0.23937321 -0.46600747 -1.0937378 -1.58055 -1.6754243 -1.5035157 -1.1128416 -0.44017315 -0.0657711 -0.41601706 -0.83291125 -1.0789142 -1.3502512 -1.3959174][-0.18378639 0.0068221092 -0.27687311 -0.72263765 -1.0108578 -0.8308928 -0.087769508 0.62695312 0.93568563 0.85376263 0.31573009 -0.46213341 -1.2864909 -1.5626731 -1.4613159][-1.1801169 -0.92589855 -0.81334329 -0.92660618 -0.64654946 0.18605757 1.0890965 1.6552429 1.7640162 1.2318621 0.15901852 -0.75470686 -1.4699512 -1.9684453 -2.1887443][-1.9853673 -1.9085937 -1.8208137 -1.3984036 -0.6868 0.19546747 1.3772078 2.0859551 1.8582439 0.98612118 -0.20018053 -1.3678527 -2.2650726 -2.6972871 -2.691292][-2.7572651 -2.7778392 -2.6629128 -2.1406226 -1.0177302 0.40030003 1.3170972 1.6365666 1.4865956 0.64985943 -0.87829185 -2.168906 -2.8674655 -3.128396 -3.0762768][-3.3288877 -3.737613 -3.4056077 -2.4365232 -1.3992126 -0.31633806 0.74546671 1.275485 0.48226023 -0.69937229 -1.9520955 -2.9799852 -3.5841756 -3.7642958 -3.4556565][-3.9283562 -4.3065829 -4.1824203 -3.4762387 -2.3521321 -1.2739975 -0.66660786 -0.43702555 -1.0041277 -2.3518445 -3.5473449 -4.0712895 -4.1571693 -3.8946598 -3.2895942][-4.4009104 -4.791328 -4.7103543 -3.9967284 -3.028101 -2.2078424 -1.7369678 -1.8536122 -2.6539278 -3.6831253 -4.4465065 -4.8700366 -4.8079948 -4.3014355 -3.58592][-4.1759858 -4.5201468 -4.5723624 -3.8530648 -3.2006121 -2.5877542 -2.3051283 -2.617358 -3.320116 -4.2769814 -4.7609839 -4.8193865 -4.5020604 -3.9687941 -3.2983146][-3.97979 -4.2373743 -4.1018453 -3.43073 -2.9168854 -2.64532 -2.844687 -3.28967 -3.8996847 -4.5110912 -4.6852021 -4.5630245 -4.1715722 -3.6726027 -3.1573894][-3.8736343 -3.959527 -3.6982512 -3.1163507 -2.6960912 -2.489872 -2.7499392 -3.3103108 -3.8206911 -4.2518439 -4.4039559 -4.2226028 -3.9498584 -3.5831678 -3.321876]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 06:01:55.314252: step 30010, loss = 0.28, batch loss = 0.22 (28.5 examples/sec; 0.281 sec/batch; 23h:35m:34s remains)
INFO - root - 2017-12-16 06:01:58.150794: step 30020, loss = 0.33, batch loss = 0.27 (28.9 examples/sec; 0.277 sec/batch; 23h:16m:52s remains)
INFO - root - 2017-12-16 06:02:00.968725: step 30030, loss = 0.31, batch loss = 0.25 (28.7 examples/sec; 0.279 sec/batch; 23h:25m:34s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 06:02:03.877442: step 30040, loss = 0.26, batch loss = 0.20 (26.9 examples/sec; 0.298 sec/batch; 25h:01m:44s remains)
INFO - root - 2017-12-16 06:02:06.703128: step 30050, loss = 0.32, batch loss = 0.26 (28.7 examples/sec; 0.279 sec/batch; 23h:24m:10s remains)
INFO - root - 2017-12-16 06:02:09.552662: step 30060, loss = 0.28, batch loss = 0.22 (28.5 examples/sec; 0.281 sec/batch; 23h:34m:41s remains)
INFO - root - 2017-12-16 06:02:12.430465: step 30070, loss = 0.34, batch loss = 0.28 (28.7 examples/sec; 0.278 sec/batch; 23h:23m:32s remains)
INFO - root - 2017-12-16 06:02:15.239736: step 30080, loss = 0.25, batch loss = 0.19 (29.2 examples/sec; 0.274 sec/batch; 23h:00m:55s remains)
INFO - root - 2017-12-16 06:02:18.061370: step 30090, loss = 0.21, batch loss = 0.15 (27.5 examples/sec; 0.291 sec/batch; 24h:28m:42s remains)
INFO - root - 2017-12-16 06:02:20.901405: step 30100, loss = 0.32, batch loss = 0.26 (29.1 examples/sec; 0.275 sec/batch; 23h:04m:37s remains)
2017-12-16 06:02:21.345762: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1509118 -5.0025854 -4.687109 -4.5155959 -4.3514047 -4.2262526 -4.0665631 -4.1023979 -4.3459935 -4.7134 -5.1597366 -5.5645952 -5.7744274 -5.67537 -5.20762][-5.5908017 -5.4775505 -5.031744 -4.4980054 -3.9460144 -4.0578389 -4.2978969 -4.4426079 -4.7425733 -5.2138186 -5.7788782 -6.2194419 -6.4116826 -6.3101611 -5.9267154][-6.1643181 -5.8135409 -5.0512776 -4.2516618 -3.4862242 -3.2568974 -3.3192701 -3.9355042 -4.650806 -5.2332115 -5.95459 -6.5911369 -6.994544 -6.9964151 -6.6496506][-5.7250161 -5.3609138 -4.3682151 -3.0459063 -1.8732255 -1.5058398 -1.4545794 -2.115726 -3.2147832 -4.3694649 -5.33466 -6.0031939 -6.7144208 -7.224823 -7.2412171][-5.2575626 -4.3772507 -2.8630219 -1.1856632 0.42532682 1.3183222 1.35637 0.50655651 -0.70829654 -2.0189524 -3.4401271 -4.682354 -5.7135134 -6.6092119 -7.1335812][-5.3236017 -4.2508378 -2.3299243 0.29239607 2.6857758 3.9739952 4.3277922 3.6192675 2.0923409 0.46672773 -1.0294607 -2.5882354 -4.1344986 -5.6258335 -6.7121139][-5.4752469 -4.2950954 -1.9725578 0.92080259 3.8325195 5.8944197 6.7713423 5.8408651 4.2745075 2.671113 0.94590616 -0.9631927 -2.8482881 -4.5509853 -5.8427572][-5.5497894 -4.4701228 -2.3478315 0.798542 3.9438848 6.1901579 7.3495922 6.7892141 5.3553104 3.5984335 1.8101492 -0.10338306 -2.1229343 -4.0911627 -5.636683][-6.1681161 -5.0834789 -3.28621 -0.619426 2.1218829 4.5963154 6.1297483 5.9481335 4.9244595 3.4688516 1.7876096 -0.14361286 -2.0716972 -4.00462 -5.5990672][-7.2580118 -6.4484167 -4.9713655 -2.790812 -0.51105976 1.5529795 2.9978042 3.2413692 2.8149314 1.8115816 0.58250809 -0.98910809 -2.7392592 -4.4578495 -5.8369136][-8.5323429 -7.8989582 -6.7514563 -5.1452947 -3.5612395 -1.8991456 -0.63180566 -0.10300303 -0.033029556 -0.69329667 -1.6557395 -2.7952223 -3.9693079 -5.1238546 -6.2484946][-8.9817715 -8.5770063 -7.7788429 -6.6763349 -5.5894003 -4.556951 -3.7746611 -3.1606297 -2.7925739 -3.0057592 -3.4799252 -4.3809528 -5.2620516 -6.1349182 -6.7784534][-8.9359455 -8.5032387 -7.9025965 -7.1755128 -6.3868675 -5.7398672 -5.3099313 -4.9205642 -4.51441 -4.435523 -4.6522236 -5.1846991 -5.6742129 -6.3307667 -6.843997][-8.2634678 -7.8882809 -7.4491129 -6.9289484 -6.3644357 -5.7933297 -5.4976311 -5.45486 -5.2323809 -5.0546694 -4.9623547 -5.3026781 -5.6187544 -5.8748217 -5.9706659][-6.9157429 -6.4094706 -5.7687693 -5.5636964 -5.5284081 -5.2377467 -4.9338455 -4.8273387 -4.8661814 -4.8856983 -4.7944088 -4.8134165 -4.80875 -4.9246783 -4.9858594]]...]
INFO - root - 2017-12-16 06:02:24.218106: step 30110, loss = 0.36, batch loss = 0.31 (28.2 examples/sec; 0.283 sec/batch; 23h:48m:15s remains)
INFO - root - 2017-12-16 06:02:27.021799: step 30120, loss = 0.25, batch loss = 0.19 (29.6 examples/sec; 0.270 sec/batch; 22h:42m:03s remains)
INFO - root - 2017-12-16 06:02:29.821857: step 30130, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.285 sec/batch; 23h:56m:01s remains)
INFO - root - 2017-12-16 06:02:32.674587: step 30140, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 23h:53m:55s remains)
INFO - root - 2017-12-16 06:02:35.530427: step 30150, loss = 0.22, batch loss = 0.16 (29.4 examples/sec; 0.272 sec/batch; 22h:50m:07s remains)
INFO - root - 2017-12-16 06:02:38.403210: step 30160, loss = 0.25, batch loss = 0.20 (28.4 examples/sec; 0.282 sec/batch; 23h:40m:16s remains)
INFO - root - 2017-12-16 06:02:41.252189: step 30170, loss = 0.25, batch loss = 0.19 (28.3 examples/sec; 0.283 sec/batch; 23h:44m:39s remains)
INFO - root - 2017-12-16 06:02:44.087121: step 30180, loss = 0.31, batch loss = 0.25 (27.3 examples/sec; 0.293 sec/batch; 24h:34m:54s remains)
INFO - root - 2017-12-16 06:02:46.920090: step 30190, loss = 0.25, batch loss = 0.20 (27.3 examples/sec; 0.293 sec/batch; 24h:34m:22s remains)
INFO - root - 2017-12-16 06:02:49.782219: step 30200, loss = 0.31, batch loss = 0.25 (27.9 examples/sec; 0.287 sec/batch; 24h:06m:01s remains)
2017-12-16 06:02:50.246254: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3577392 -1.4367483 -1.6722767 -2.1975698 -2.6404209 -2.719903 -2.8089998 -3.0269103 -3.4056373 -3.6064806 -3.5579996 -3.3247643 -2.931283 -2.4398708 -1.9980848][-0.83562088 -1.1953924 -1.5060253 -1.9281406 -2.2278543 -2.3960218 -2.6194816 -2.8070002 -3.1520991 -3.3614008 -3.468482 -3.365273 -3.1481314 -2.6797624 -2.0625865][-1.1342909 -1.5426018 -1.8147094 -2.0249619 -2.0774217 -2.0235381 -1.9606736 -2.1117358 -2.5397658 -3.0167952 -3.4715521 -3.6674612 -3.5760775 -3.1707215 -2.4313188][-1.9221132 -2.4732192 -2.7480435 -2.6130774 -2.1954274 -1.6368051 -1.1231859 -1.034883 -1.4708586 -2.2471678 -3.0664756 -3.7704213 -4.0524836 -3.69665 -2.8898404][-2.6697793 -3.3496764 -3.5339887 -3.0157404 -1.9923968 -0.84311104 0.11854792 0.57466888 0.20042801 -0.90503287 -2.314301 -3.4952042 -4.0995741 -4.1946478 -3.7317691][-3.341711 -3.9929526 -3.839484 -2.8800964 -1.2331419 0.61412621 2.1690283 2.80626 2.2932801 0.85163736 -1.0368969 -2.8596478 -4.0583162 -4.3799729 -4.1978545][-3.8893323 -4.2835321 -3.8697422 -2.5537786 -0.39056921 2.0301208 4.0248652 4.8560028 4.228775 2.5339632 0.26565981 -1.9802334 -3.6346574 -4.5469174 -4.6844006][-3.7729163 -4.1112881 -3.6257911 -1.9811161 0.39798498 2.9813232 5.1566753 6.0697613 5.3815184 3.4190474 0.864511 -1.5763113 -3.4656711 -4.7031584 -5.1283894][-3.7076626 -3.75951 -3.1710525 -1.7487993 0.44163036 2.8544846 4.8474054 5.762784 5.1960163 3.2154832 0.5758729 -2.0027087 -3.9691327 -5.1248527 -5.5356445][-3.4930747 -3.7267377 -3.4362466 -2.1841927 -0.38426304 1.3583465 2.7946777 3.4962382 3.0145488 1.4199977 -0.70442581 -2.848052 -4.5563841 -5.5256338 -5.8189583][-3.5789428 -3.8796244 -3.7688122 -3.043313 -1.9234717 -0.67838264 0.31631756 0.63718319 0.097875118 -1.0598307 -2.5244079 -3.9917445 -5.1852112 -5.7402258 -5.8339119][-3.4786115 -3.9241626 -4.1781416 -3.7509632 -2.8494191 -2.2074211 -1.9418888 -1.9479427 -2.2208381 -2.9198143 -3.8431873 -4.6782622 -5.2684221 -5.5847969 -5.7427988][-3.7231109 -4.0782952 -4.2390709 -4.1258283 -3.734592 -3.2209039 -2.9236975 -3.02288 -3.4821754 -3.907001 -4.1327357 -4.3821697 -4.7407422 -4.9754229 -5.1017537][-3.8976765 -4.1555872 -4.1145024 -3.91777 -3.6472232 -3.3161016 -3.1777363 -3.2256851 -3.358386 -3.4655583 -3.5268338 -3.5296967 -3.6319704 -3.77104 -4.0136137][-3.5360689 -3.6916523 -3.554244 -3.2052929 -2.6520109 -2.2173119 -2.0678983 -2.1499183 -2.2775655 -2.3768709 -2.4064264 -2.2816687 -2.2869842 -2.5100546 -2.7986882]]...]
INFO - root - 2017-12-16 06:02:53.066969: step 30210, loss = 0.29, batch loss = 0.23 (26.7 examples/sec; 0.299 sec/batch; 25h:08m:54s remains)
INFO - root - 2017-12-16 06:02:55.902299: step 30220, loss = 0.27, batch loss = 0.22 (28.2 examples/sec; 0.283 sec/batch; 23h:46m:46s remains)
INFO - root - 2017-12-16 06:02:58.734907: step 30230, loss = 0.35, batch loss = 0.29 (27.2 examples/sec; 0.294 sec/batch; 24h:40m:20s remains)
INFO - root - 2017-12-16 06:03:01.617260: step 30240, loss = 0.21, batch loss = 0.15 (29.3 examples/sec; 0.273 sec/batch; 22h:55m:47s remains)
INFO - root - 2017-12-16 06:03:04.459309: step 30250, loss = 0.27, batch loss = 0.21 (29.0 examples/sec; 0.276 sec/batch; 23h:11m:46s remains)
INFO - root - 2017-12-16 06:03:07.284735: step 30260, loss = 0.24, batch loss = 0.18 (28.6 examples/sec; 0.280 sec/batch; 23h:30m:57s remains)
INFO - root - 2017-12-16 06:03:10.097058: step 30270, loss = 0.29, batch loss = 0.23 (26.6 examples/sec; 0.301 sec/batch; 25h:15m:06s remains)
INFO - root - 2017-12-16 06:03:12.956572: step 30280, loss = 0.40, batch loss = 0.35 (28.8 examples/sec; 0.277 sec/batch; 23h:17m:05s remains)
INFO - root - 2017-12-16 06:03:15.753912: step 30290, loss = 0.29, batch loss = 0.24 (28.6 examples/sec; 0.280 sec/batch; 23h:30m:02s remains)
INFO - root - 2017-12-16 06:03:18.591608: step 30300, loss = 0.25, batch loss = 0.19 (26.2 examples/sec; 0.305 sec/batch; 25h:35m:29s remains)
2017-12-16 06:03:19.050825: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.658869 -1.6896296 -1.6574969 -1.5452995 -1.6937528 -1.838526 -1.9250102 -1.9309876 -2.1071918 -2.2372572 -2.1417792 -2.1185076 -2.2851517 -2.5643673 -2.696614][-0.97546935 -1.1076686 -1.3002656 -1.376991 -1.6949177 -1.9986706 -2.2209876 -2.2767787 -2.3209169 -2.4187284 -2.3791258 -2.3115032 -2.3899786 -2.5833397 -2.7763357][-0.78109527 -1.1001894 -1.5000477 -1.8654866 -2.2517705 -2.6246653 -2.8089964 -2.7631905 -2.8229856 -3.0073791 -2.9979615 -2.8924122 -2.8346436 -2.8131447 -2.8825994][-0.84942842 -1.4320452 -2.0317788 -2.3401129 -2.5477569 -2.6469808 -2.6734123 -2.6507292 -2.7773838 -2.9940603 -3.0519729 -3.053987 -3.0701365 -2.9692793 -2.9078443][-1.1094882 -2.0095713 -2.7044394 -2.8113437 -2.8447971 -2.6368883 -2.3365948 -2.0717332 -2.0908716 -2.4445992 -2.6734152 -2.7807059 -2.8167133 -2.6199374 -2.4535832][-1.314887 -2.0193055 -2.753768 -2.8013322 -2.5108733 -1.9169424 -1.2831159 -0.84025955 -0.86984777 -1.2962117 -1.6790578 -1.8467073 -1.9487002 -2.0457265 -2.0775464][-1.604156 -2.09582 -2.5584555 -2.5857711 -2.1195476 -1.0738406 -0.023111343 0.60267115 0.74237394 0.29354 -0.4052701 -0.93817186 -1.206316 -1.2548819 -1.3127248][-1.2879176 -1.9192288 -2.4018428 -2.1016183 -1.4642346 -0.46596479 0.6226387 1.5636902 1.9565716 1.4728894 0.76461554 0.10809088 -0.42361546 -0.65501285 -0.80691743][-0.5881989 -1.3093419 -2.0367584 -1.9861746 -1.3224857 -0.026367188 1.2154818 2.1666574 2.5831866 2.1760454 1.4504466 0.72187185 0.19280958 -0.10435772 -0.38439512][-0.036426544 -0.76795435 -1.8710144 -2.0804317 -1.6387677 -0.47314024 0.75739813 1.6415477 2.1794052 1.9580674 1.4287157 0.80500555 0.33478308 0.024727821 -0.19175339][0.1666379 -0.4028945 -1.4369516 -2.0789685 -2.1978753 -1.2635441 0.012985229 0.98458052 1.4608736 1.2504392 0.77480268 0.32620764 0.012928486 -0.14544535 -0.38101149][0.2254138 -0.65112352 -1.8549378 -2.3982983 -2.2825115 -1.7803066 -0.99469709 -0.20909977 0.38241196 0.43107176 0.06292963 -0.332376 -0.52318907 -0.59950757 -0.75792527][0.10379457 -0.68882632 -1.7052033 -2.2162461 -2.3596 -2.0675182 -1.5805192 -1.0063298 -0.50382519 -0.373343 -0.57315493 -0.90393567 -1.0438433 -0.88426423 -0.73116422][0.099295616 -0.391572 -1.1377587 -1.7175403 -1.8832819 -1.6865366 -1.4379535 -1.2364626 -0.96607828 -0.81504369 -1.0117691 -1.2362185 -1.2271597 -1.0176449 -0.779217][0.055925846 -0.1260252 -0.51653934 -0.82528329 -0.86336446 -0.83267522 -0.8206315 -0.85570884 -0.87690783 -0.934824 -1.2760689 -1.5606203 -1.5771072 -1.0913401 -0.4028883]]...]
INFO - root - 2017-12-16 06:03:21.865023: step 30310, loss = 0.29, batch loss = 0.23 (28.8 examples/sec; 0.278 sec/batch; 23h:19m:28s remains)
INFO - root - 2017-12-16 06:03:24.735437: step 30320, loss = 0.28, batch loss = 0.22 (28.9 examples/sec; 0.277 sec/batch; 23h:13m:47s remains)
INFO - root - 2017-12-16 06:03:27.535444: step 30330, loss = 0.23, batch loss = 0.17 (29.2 examples/sec; 0.274 sec/batch; 23h:00m:46s remains)
INFO - root - 2017-12-16 06:03:30.364029: step 30340, loss = 0.30, batch loss = 0.24 (29.5 examples/sec; 0.271 sec/batch; 22h:46m:44s remains)
INFO - root - 2017-12-16 06:03:33.230069: step 30350, loss = 0.33, batch loss = 0.27 (26.8 examples/sec; 0.298 sec/batch; 25h:03m:09s remains)
INFO - root - 2017-12-16 06:03:36.054873: step 30360, loss = 0.24, batch loss = 0.19 (28.8 examples/sec; 0.278 sec/batch; 23h:19m:35s remains)
INFO - root - 2017-12-16 06:03:38.892940: step 30370, loss = 0.30, batch loss = 0.24 (27.1 examples/sec; 0.295 sec/batch; 24h:45m:23s remains)
INFO - root - 2017-12-16 06:03:41.678942: step 30380, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.285 sec/batch; 23h:56m:26s remains)
INFO - root - 2017-12-16 06:03:44.480263: step 30390, loss = 0.24, batch loss = 0.18 (28.7 examples/sec; 0.279 sec/batch; 23h:22m:55s remains)
INFO - root - 2017-12-16 06:03:47.307421: step 30400, loss = 0.27, batch loss = 0.21 (28.1 examples/sec; 0.285 sec/batch; 23h:53m:33s remains)
2017-12-16 06:03:47.772020: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0928335 -2.8969431 -2.4005017 -1.972641 -1.7236288 -1.6995866 -2.1364906 -2.4382353 -2.7798915 -2.9709339 -3.0498106 -3.1925282 -3.3253524 -3.5902348 -3.6577668][-3.7357564 -3.5164845 -2.9396815 -2.3772359 -2.064888 -1.9353819 -2.2694664 -2.8523679 -3.6233106 -4.0084448 -4.2807961 -4.4677987 -4.5230913 -4.7405047 -4.8127637][-4.763495 -4.3743911 -3.5937293 -2.8194475 -2.2183888 -1.8345199 -1.960393 -2.4495597 -3.2769594 -4.1898775 -4.7893291 -5.2140522 -5.538393 -5.7429008 -5.5865207][-5.6270761 -5.0335526 -3.9160767 -2.7263608 -1.7670565 -1.1851091 -1.1152184 -1.4436827 -2.3400135 -3.5863092 -4.7433157 -5.5665121 -5.9027352 -6.1822591 -6.264185][-5.8831434 -5.1091971 -3.8380659 -2.3809912 -1.0683341 -0.20892286 0.022550106 -0.38111258 -1.3024735 -2.4695513 -3.8281734 -5.0981812 -5.791904 -5.9041939 -5.6026516][-5.6102252 -4.7442441 -3.2510445 -1.4497654 0.13698149 1.2059011 1.6010256 1.1776285 0.16572905 -1.1440253 -2.7370503 -4.1102767 -4.9429045 -5.2505364 -4.8342404][-4.1195269 -3.3729479 -2.0903096 -0.33344507 1.3199739 2.4425435 2.9271388 2.5154638 1.4616981 0.1129303 -1.2452714 -2.5661914 -3.4595277 -3.6901128 -3.3275747][-2.8782346 -2.3115425 -1.1596584 0.38225365 1.8118696 2.6976271 2.914681 2.4766307 1.704092 0.56573009 -0.72136354 -1.4701247 -1.6987402 -1.7734339 -1.3906412][-2.1821811 -1.6683733 -0.73129129 0.4718976 1.4245467 2.1176023 2.2293024 1.6499352 0.74303865 -0.10824442 -0.76007342 -1.1032069 -0.92566562 -0.42887783 0.1443162][-1.8835638 -1.4756954 -1.0464396 -0.28554392 0.42030668 0.76617384 0.36554193 -0.15639687 -0.7220974 -1.2971468 -1.4603064 -1.1738126 -0.39496613 0.51734161 1.4544148][-2.4636462 -1.9832919 -1.4368391 -0.93141913 -0.74190545 -0.79518223 -1.2594221 -2.0584683 -2.8444741 -3.0425141 -2.6800175 -1.7589352 -0.36827421 1.059155 2.2293468][-2.9699845 -2.5073996 -2.0790677 -1.5273755 -1.2421067 -1.6350849 -2.5765572 -3.6116564 -4.4298172 -4.7126579 -4.2486954 -2.9374642 -1.1301994 0.55666494 1.6816182][-3.808991 -3.0977216 -2.2497933 -1.6552248 -1.6571989 -1.967227 -3.1118822 -4.6161146 -5.6738753 -6.1110363 -5.6928954 -4.3142223 -2.386215 -0.39927769 0.86432695][-4.1184626 -3.5865378 -2.7307358 -1.9081209 -1.6528592 -2.0871019 -3.5588226 -5.1513114 -6.585063 -7.2656865 -6.8077555 -5.5325375 -3.5622025 -1.6306849 -0.40805626][-4.0107365 -3.6529565 -2.9883914 -2.2264462 -2.0009539 -2.4777138 -3.8366385 -5.6973896 -7.3202991 -8.1094875 -7.7640972 -6.4208908 -4.5552688 -2.7864108 -1.7441044]]...]
INFO - root - 2017-12-16 06:03:50.636297: step 30410, loss = 0.29, batch loss = 0.23 (28.6 examples/sec; 0.280 sec/batch; 23h:30m:39s remains)
INFO - root - 2017-12-16 06:03:53.506914: step 30420, loss = 0.25, batch loss = 0.19 (28.3 examples/sec; 0.283 sec/batch; 23h:44m:01s remains)
INFO - root - 2017-12-16 06:03:56.345308: step 30430, loss = 0.27, batch loss = 0.22 (27.3 examples/sec; 0.293 sec/batch; 24h:36m:47s remains)
INFO - root - 2017-12-16 06:03:59.164412: step 30440, loss = 0.26, batch loss = 0.20 (27.9 examples/sec; 0.287 sec/batch; 24h:04m:53s remains)
INFO - root - 2017-12-16 06:04:01.984898: step 30450, loss = 0.31, batch loss = 0.25 (27.6 examples/sec; 0.290 sec/batch; 24h:21m:21s remains)
INFO - root - 2017-12-16 06:04:04.835142: step 30460, loss = 0.21, batch loss = 0.15 (28.2 examples/sec; 0.284 sec/batch; 23h:49m:25s remains)
INFO - root - 2017-12-16 06:04:07.678797: step 30470, loss = 0.23, batch loss = 0.18 (28.2 examples/sec; 0.283 sec/batch; 23h:46m:44s remains)
INFO - root - 2017-12-16 06:04:10.496242: step 30480, loss = 0.36, batch loss = 0.30 (27.0 examples/sec; 0.296 sec/batch; 24h:49m:32s remains)
INFO - root - 2017-12-16 06:04:13.318943: step 30490, loss = 0.29, batch loss = 0.23 (28.8 examples/sec; 0.278 sec/batch; 23h:17m:22s remains)
INFO - root - 2017-12-16 06:04:16.169421: step 30500, loss = 0.34, batch loss = 0.28 (28.4 examples/sec; 0.282 sec/batch; 23h:38m:42s remains)
2017-12-16 06:04:16.617047: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7636468 -3.5366058 -3.4292159 -3.5800829 -3.9626029 -3.9555259 -3.8523057 -3.4787223 -3.086133 -2.5982995 -2.1220152 -1.836761 -2.0632315 -2.3012009 -2.5262823][-3.0418413 -2.8370395 -2.7729578 -2.9612596 -3.3026066 -3.5124826 -3.1439478 -2.812901 -2.4929261 -2.0337586 -1.6465569 -1.4983478 -1.7101173 -1.7317331 -1.8423474][-2.11965 -2.2006516 -2.258198 -2.4974232 -2.616765 -2.9947534 -2.9255316 -2.4073052 -1.7037058 -1.3850646 -1.3259304 -1.2631867 -1.3180807 -1.3030119 -1.3927274][-0.792593 -1.0629451 -1.2470753 -1.5419865 -1.97893 -2.2021942 -2.0379827 -1.7061193 -1.4456003 -1.119746 -0.87444592 -0.860631 -1.1584032 -1.2838774 -1.3860264][-0.27473927 -0.31215572 -0.33412981 -0.57387805 -0.91579914 -0.949846 -0.75242257 -0.60243177 -0.59455252 -0.73285818 -0.98245955 -1.1646621 -1.4161587 -1.5067458 -1.7227893][-0.84033751 -0.75754118 -0.56236196 -0.56598306 -0.48750567 -0.37607384 -0.16628742 0.034912586 0.068728924 -0.20712757 -0.72532773 -1.2364948 -1.6880896 -1.8807585 -1.9942405][-1.8377781 -1.4107001 -1.141088 -0.92593265 -0.41329002 0.07318449 0.53641415 0.62549591 0.43811464 -0.03082037 -0.67636991 -1.3970525 -2.0509613 -2.4423838 -2.7215285][-2.8394039 -2.4792254 -2.2105322 -1.6017375 -0.92734981 -0.10037327 0.69376755 1.0042057 0.83820868 0.18259478 -0.72404051 -1.6094115 -2.4018979 -2.9031186 -3.28791][-4.2949247 -3.9891918 -3.6118636 -3.0300984 -2.3867297 -1.2558496 -0.35105038 0.080367088 0.22694492 -0.32027674 -1.1147132 -2.0210788 -2.8129668 -3.2059114 -3.5820742][-5.2760663 -5.4530745 -5.40377 -4.9498167 -4.0812149 -2.93417 -2.0329115 -1.4743619 -1.3062162 -1.6641252 -2.2531035 -3.0771194 -3.7579913 -3.9495151 -4.1233816][-5.8195076 -6.1471586 -6.198864 -5.8440065 -5.162128 -4.1510029 -3.2810597 -2.8340108 -2.8031485 -3.1183255 -3.5922143 -4.2329092 -4.8053737 -4.9218693 -4.8715987][-5.731123 -6.1582956 -6.5799046 -6.3760328 -5.7971272 -5.085587 -4.387569 -3.9579778 -3.778904 -3.9255838 -4.3611269 -4.8607054 -5.09331 -5.0673532 -5.1289954][-5.3222651 -5.8743138 -6.2490005 -6.3106427 -6.1206913 -5.5647793 -4.9755731 -4.6455641 -4.4212132 -4.4858179 -4.5783319 -4.6813178 -4.8169584 -4.6837716 -4.625792][-4.6800628 -5.1462922 -5.3056269 -5.3384733 -5.2142472 -4.8220387 -4.5050535 -4.40637 -4.3267584 -4.3429074 -4.3394151 -4.3516188 -4.3129616 -4.0968537 -3.9280593][-3.9875362 -4.1682992 -4.1037951 -3.9094305 -3.6084986 -3.4250679 -3.3660643 -3.378629 -3.4290578 -3.6336141 -3.6865144 -3.3915968 -3.028574 -2.9916191 -3.0719149]]...]
INFO - root - 2017-12-16 06:04:19.473757: step 30510, loss = 0.31, batch loss = 0.25 (27.3 examples/sec; 0.293 sec/batch; 24h:35m:19s remains)
INFO - root - 2017-12-16 06:04:22.331745: step 30520, loss = 0.36, batch loss = 0.30 (28.4 examples/sec; 0.282 sec/batch; 23h:39m:02s remains)
INFO - root - 2017-12-16 06:04:25.170700: step 30530, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.281 sec/batch; 23h:36m:12s remains)
INFO - root - 2017-12-16 06:04:28.022740: step 30540, loss = 0.41, batch loss = 0.35 (27.3 examples/sec; 0.293 sec/batch; 24h:35m:44s remains)
INFO - root - 2017-12-16 06:04:30.809052: step 30550, loss = 0.27, batch loss = 0.21 (28.3 examples/sec; 0.283 sec/batch; 23h:44m:06s remains)
INFO - root - 2017-12-16 06:04:33.618452: step 30560, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 23h:56m:46s remains)
INFO - root - 2017-12-16 06:04:36.483825: step 30570, loss = 0.26, batch loss = 0.20 (27.9 examples/sec; 0.286 sec/batch; 24h:01m:42s remains)
INFO - root - 2017-12-16 06:04:39.359585: step 30580, loss = 0.29, batch loss = 0.23 (26.4 examples/sec; 0.303 sec/batch; 25h:27m:02s remains)
INFO - root - 2017-12-16 06:04:42.164252: step 30590, loss = 0.29, batch loss = 0.23 (27.2 examples/sec; 0.294 sec/batch; 24h:38m:03s remains)
INFO - root - 2017-12-16 06:04:44.976051: step 30600, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.284 sec/batch; 23h:50m:03s remains)
2017-12-16 06:04:45.444405: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4831169 -3.1654239 -3.100884 -2.9605508 -2.7778902 -2.6642528 -2.5735135 -2.3458588 -2.1908431 -2.2773206 -2.5978856 -3.3299053 -4.135242 -5.1661577 -6.1298237][-3.5356359 -3.061183 -2.9202819 -2.9137869 -2.8827794 -2.812346 -2.8410525 -2.6848969 -2.4002221 -2.2970412 -2.494678 -3.2578824 -4.253696 -5.4794497 -6.5263186][-3.680758 -3.0523415 -2.7039373 -2.4394636 -2.3051093 -2.2672045 -2.4293501 -2.3751287 -2.1482062 -2.0443518 -2.1908264 -3.0434752 -4.2207074 -5.5275617 -6.744215][-3.6072955 -2.7786493 -2.4400406 -2.0230098 -1.5758059 -1.3804529 -1.4893165 -1.5724909 -1.5521781 -1.5669842 -1.7972512 -2.5264449 -3.720448 -5.3310885 -6.6363697][-3.4920135 -2.8348644 -2.5531497 -2.4912915 -2.3015769 -1.9850271 -1.6391366 -1.3567786 -1.1146359 -0.92936397 -1.0479517 -1.6509318 -2.7491107 -4.1360497 -5.4149003][-3.1744332 -2.7006605 -2.6080906 -2.5965309 -2.520622 -2.3312435 -1.939353 -1.31583 -0.68012428 -0.11010933 0.086970329 -0.41229868 -1.5392826 -2.9557786 -4.212317][-3.0269 -2.5178781 -2.2530472 -2.2503328 -2.2157006 -2.0482855 -1.6290715 -0.91450167 -0.133605 0.38679981 0.43858337 0.037173271 -0.87439966 -2.1846838 -3.1403923][-2.952105 -2.6560001 -2.4568446 -2.1411052 -1.8361986 -1.6054382 -1.1693673 -0.60890579 -0.061871052 0.31954527 0.17062187 -0.36638021 -1.301599 -2.2683883 -3.0363479][-2.9630284 -2.7483535 -2.7310715 -2.6582789 -2.4669077 -2.0142307 -1.338598 -0.7998426 -0.44076967 -0.28614569 -0.54601216 -1.1509383 -2.0554397 -2.9262519 -3.681313][-3.0596123 -2.991426 -2.9489479 -3.0341105 -2.9905591 -2.6310935 -2.0459139 -1.5184529 -1.1400766 -1.0968916 -1.3659236 -2.047158 -2.9326751 -3.3810785 -3.7300086][-3.0154185 -2.9097877 -2.8524189 -2.9303384 -2.8573637 -2.7595563 -2.4984097 -2.187139 -1.8797722 -1.7105818 -1.9257755 -2.6807933 -3.3968399 -3.8397162 -4.0680451][-2.8256588 -2.7957644 -2.7652655 -2.697449 -2.5828567 -2.578331 -2.5053966 -2.5904365 -2.716105 -2.654588 -2.9155228 -3.5283523 -4.1377478 -4.3605585 -4.3433022][-2.6265173 -2.5429561 -2.6309733 -2.6931477 -2.7263403 -2.7263591 -2.7777987 -3.0610065 -3.2691557 -3.3291633 -3.6415071 -4.0204325 -4.3494945 -4.52714 -4.461925][-3.0463796 -2.7300153 -2.4658771 -2.524456 -2.7322197 -2.73163 -2.7535338 -2.932097 -3.0820568 -3.1474798 -3.3349946 -3.6928151 -4.0819173 -4.0623693 -3.8444366][-3.0611224 -2.6791334 -2.3457005 -2.2064636 -2.2644782 -2.4637895 -2.6805813 -2.8620236 -3.0504649 -3.0272851 -2.9750919 -3.1594973 -3.340003 -3.3978052 -3.3656707]]...]
INFO - root - 2017-12-16 06:04:48.263356: step 30610, loss = 0.30, batch loss = 0.24 (28.5 examples/sec; 0.281 sec/batch; 23h:32m:58s remains)
INFO - root - 2017-12-16 06:04:51.115232: step 30620, loss = 0.26, batch loss = 0.20 (27.3 examples/sec; 0.293 sec/batch; 24h:31m:43s remains)
INFO - root - 2017-12-16 06:04:53.906145: step 30630, loss = 0.32, batch loss = 0.26 (27.9 examples/sec; 0.287 sec/batch; 24h:01m:56s remains)
INFO - root - 2017-12-16 06:04:56.726390: step 30640, loss = 0.21, batch loss = 0.16 (27.8 examples/sec; 0.287 sec/batch; 24h:06m:22s remains)
INFO - root - 2017-12-16 06:04:59.545407: step 30650, loss = 0.28, batch loss = 0.22 (28.6 examples/sec; 0.279 sec/batch; 23h:25m:06s remains)
INFO - root - 2017-12-16 06:05:02.332559: step 30660, loss = 0.20, batch loss = 0.14 (28.4 examples/sec; 0.282 sec/batch; 23h:38m:54s remains)
INFO - root - 2017-12-16 06:05:05.189567: step 30670, loss = 0.43, batch loss = 0.37 (28.1 examples/sec; 0.284 sec/batch; 23h:50m:05s remains)
INFO - root - 2017-12-16 06:05:08.034850: step 30680, loss = 0.36, batch loss = 0.31 (27.6 examples/sec; 0.290 sec/batch; 24h:16m:25s remains)
INFO - root - 2017-12-16 06:05:10.909608: step 30690, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 23h:56m:44s remains)
INFO - root - 2017-12-16 06:05:13.750031: step 30700, loss = 0.33, batch loss = 0.27 (28.8 examples/sec; 0.278 sec/batch; 23h:18m:56s remains)
2017-12-16 06:05:14.208112: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0754182 -2.794632 -2.4094584 -2.1365905 -2.3752694 -2.323056 -2.4280279 -2.3474619 -2.3796711 -2.0477335 -1.7299247 -1.608887 -1.7772288 -2.3407102 -3.0961523][-2.1188567 -1.5549247 -1.0044432 -0.57141232 -0.70835137 -0.90560222 -1.2511804 -1.4744735 -1.6732948 -1.5154715 -1.2498753 -1.0954752 -1.1232443 -1.4213443 -2.0910208][-1.3127522 -0.64878368 0.10399103 0.45413733 0.35145283 0.08481741 -0.53964758 -0.9882133 -1.1023126 -1.133075 -1.2297673 -1.2979217 -1.4209077 -1.4376414 -1.6736727][-0.68976784 -0.22294474 0.2699666 0.62663364 0.68983793 0.55791759 -0.056970119 -0.85560822 -1.3952351 -1.657444 -1.7115157 -1.845397 -2.1442814 -2.3721819 -2.271323][-1.2522206 -0.65692306 0.14717627 0.59223795 0.96479559 0.99126339 0.40400076 -0.49611306 -1.4144826 -2.2703464 -2.8762383 -3.005383 -2.9703007 -3.1411693 -3.1571236][-1.9231942 -1.3341556 -0.43285656 0.43297958 1.0825558 1.2998796 1.0126328 0.36670876 -0.44630384 -1.5308733 -2.7584317 -3.56002 -3.8483329 -3.8164432 -3.5647559][-2.4158807 -1.7011428 -0.76836109 0.26574421 1.0035787 1.4224062 1.4628048 0.95157433 0.38689709 -0.53066969 -1.9609382 -3.3485928 -4.2396588 -4.4033365 -4.1976423][-2.7991896 -1.9995732 -1.1014202 0.020750046 0.7898674 1.4401269 1.7440157 1.272162 0.50935173 -0.534544 -1.9205966 -3.4251976 -4.5492735 -4.944334 -4.7057915][-3.2972226 -2.5433395 -1.7608242 -0.77867794 -0.0010666847 0.7069211 1.1438231 1.00945 0.37723112 -0.75669169 -2.1610017 -3.7468114 -5.0057774 -5.4420056 -5.1922421][-3.24984 -2.6550412 -2.157979 -1.6129584 -1.1265085 -0.53491187 -0.18113708 -0.20012188 -0.69344068 -1.684155 -3.0510571 -4.399075 -5.4533606 -5.7464871 -5.4115448][-3.4162776 -2.8338695 -2.5137112 -2.2771702 -1.9850414 -1.6646602 -1.4816351 -1.7501385 -2.3135688 -3.0767832 -4.2377415 -5.48206 -6.3727231 -6.472971 -5.9819932][-3.5473876 -3.0400934 -2.8928533 -2.8837967 -2.7706118 -2.633698 -2.7361188 -3.2235992 -3.6605208 -4.2080512 -5.1901493 -6.209126 -7.0456276 -7.1437259 -6.59791][-3.253469 -2.9626684 -2.8450441 -3.0506442 -3.2762296 -3.2490845 -3.2108326 -3.718426 -4.29611 -4.88222 -5.7601957 -6.52643 -7.0365591 -6.91063 -6.5164566][-2.983685 -2.6957159 -2.6186924 -2.7808962 -2.8693423 -2.8347173 -2.844944 -3.3003931 -3.8654685 -4.4779983 -5.3803492 -6.0039153 -6.3320303 -5.8610992 -5.1861963][-2.6988597 -2.3943076 -2.284332 -2.382977 -2.3206303 -2.2433212 -2.2178943 -2.6363363 -3.1946251 -3.7628355 -4.6111026 -4.9814496 -5.1221972 -4.6717873 -4.0598431]]...]
INFO - root - 2017-12-16 06:05:17.069301: step 30710, loss = 0.24, batch loss = 0.18 (27.9 examples/sec; 0.286 sec/batch; 24h:00m:51s remains)
INFO - root - 2017-12-16 06:05:19.868959: step 30720, loss = 0.28, batch loss = 0.22 (28.0 examples/sec; 0.286 sec/batch; 23h:57m:05s remains)
INFO - root - 2017-12-16 06:05:22.717685: step 30730, loss = 0.24, batch loss = 0.18 (27.9 examples/sec; 0.287 sec/batch; 24h:01m:22s remains)
INFO - root - 2017-12-16 06:05:25.611360: step 30740, loss = 0.32, batch loss = 0.26 (29.1 examples/sec; 0.275 sec/batch; 23h:03m:40s remains)
INFO - root - 2017-12-16 06:05:28.397599: step 30750, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.284 sec/batch; 23h:45m:53s remains)
INFO - root - 2017-12-16 06:05:31.189570: step 30760, loss = 0.29, batch loss = 0.23 (27.4 examples/sec; 0.292 sec/batch; 24h:29m:10s remains)
INFO - root - 2017-12-16 06:05:34.036221: step 30770, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.286 sec/batch; 23h:59m:03s remains)
INFO - root - 2017-12-16 06:05:36.854192: step 30780, loss = 0.28, batch loss = 0.23 (28.5 examples/sec; 0.281 sec/batch; 23h:31m:08s remains)
INFO - root - 2017-12-16 06:05:39.655889: step 30790, loss = 0.22, batch loss = 0.16 (27.9 examples/sec; 0.287 sec/batch; 24h:03m:33s remains)
INFO - root - 2017-12-16 06:05:42.508242: step 30800, loss = 0.29, batch loss = 0.23 (28.2 examples/sec; 0.284 sec/batch; 23h:45m:45s remains)
2017-12-16 06:05:42.956493: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4251089 -4.7269773 -4.8280163 -4.8449097 -5.09071 -5.5973749 -6.4134912 -7.4292097 -8.043951 -8.0382118 -7.5983424 -6.7312174 -5.2972069 -3.8794138 -2.9009309][-5.1635165 -5.0097423 -4.6651464 -4.4350929 -4.44075 -4.77044 -5.5885978 -6.7327385 -7.8281441 -8.3560057 -8.1415272 -7.2681065 -5.8955975 -4.4662447 -3.3565798][-6.4911723 -5.9959083 -5.1343145 -4.2708907 -3.7380729 -3.7382088 -4.4076314 -5.4996886 -6.7070675 -7.6778522 -8.0294809 -7.5046473 -6.2289491 -4.8768334 -3.7719004][-6.9248667 -6.1810818 -5.14067 -3.9340174 -2.9275658 -2.3809996 -2.4967737 -3.3482344 -4.5766025 -5.8354764 -6.74754 -7.1289978 -6.3253083 -5.0500846 -4.0591803][-5.9712715 -5.0487008 -3.8262165 -2.5042911 -1.2065117 -0.25987434 0.084853172 -0.46407342 -1.6611853 -3.256577 -4.8435988 -5.82845 -5.7461243 -4.9692917 -3.7879608][-4.2941504 -3.1472511 -1.6983306 -0.31485653 1.1306291 2.3427958 3.0283995 2.8566895 1.7619328 -0.17881441 -2.3258929 -3.9871268 -4.6992841 -4.406199 -3.3601694][-3.2456493 -1.9891357 -0.53545618 1.1437979 2.8014083 4.358943 5.4427061 5.54578 4.6156788 2.4465919 -0.18573475 -2.3858743 -3.66153 -3.8260949 -3.1511745][-2.842391 -1.6492424 -0.38379574 1.3879209 3.09621 4.6961231 6.0278091 6.502634 5.8004789 3.648015 0.98121977 -1.6324766 -3.2898397 -3.6222348 -3.1904073][-2.9996042 -1.9676714 -0.68463778 0.53264713 1.8591628 3.5341115 4.8980188 5.426383 4.8248491 2.8609862 0.40635777 -1.9540668 -3.5884781 -4.0860248 -3.6621304][-4.1517882 -3.1966503 -2.3584344 -1.4936814 -0.29364061 0.86539841 1.8579307 2.4698148 2.0474215 0.60824203 -1.3269684 -3.0815382 -4.2805347 -4.6540642 -4.366365][-6.0552793 -5.1815267 -4.5280695 -4.0586209 -3.5510232 -2.5392051 -1.4633458 -1.0892496 -1.4385936 -2.2764845 -3.3140373 -4.2692394 -4.993413 -5.1460123 -4.9029422][-7.3181109 -7.061861 -7.0931392 -6.7065773 -6.2516537 -5.724782 -5.246563 -4.7065039 -4.3812919 -4.4853044 -4.928 -5.2736354 -5.4531312 -5.3947911 -5.1986251][-7.7650633 -7.789959 -7.9616809 -8.3989267 -8.6083012 -8.2842121 -7.9044666 -7.5224414 -7.14258 -6.5663166 -6.0184832 -5.6297293 -5.5568151 -5.4944692 -5.14176][-7.0779018 -7.3542194 -7.6789327 -8.2616243 -8.7531395 -9.0200481 -9.0488262 -8.5875568 -7.8407974 -7.0169621 -6.2808933 -5.6392565 -5.2734251 -4.9793978 -4.7366996][-6.0181417 -6.2527466 -6.7163162 -7.2938085 -7.7917595 -8.136899 -8.2184048 -7.8813486 -7.2437515 -6.442935 -5.6793532 -5.0205574 -4.5662284 -4.3142681 -4.132473]]...]
INFO - root - 2017-12-16 06:05:45.793273: step 30810, loss = 0.37, batch loss = 0.31 (26.4 examples/sec; 0.303 sec/batch; 25h:23m:03s remains)
INFO - root - 2017-12-16 06:05:48.631248: step 30820, loss = 0.29, batch loss = 0.23 (27.8 examples/sec; 0.288 sec/batch; 24h:09m:08s remains)
INFO - root - 2017-12-16 06:05:51.454191: step 30830, loss = 0.26, batch loss = 0.21 (29.1 examples/sec; 0.275 sec/batch; 23h:03m:13s remains)
INFO - root - 2017-12-16 06:05:54.318086: step 30840, loss = 0.31, batch loss = 0.25 (29.3 examples/sec; 0.273 sec/batch; 22h:53m:18s remains)
INFO - root - 2017-12-16 06:05:57.133969: step 30850, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.276 sec/batch; 23h:09m:51s remains)
INFO - root - 2017-12-16 06:05:59.987412: step 30860, loss = 0.39, batch loss = 0.33 (28.4 examples/sec; 0.282 sec/batch; 23h:35m:49s remains)
INFO - root - 2017-12-16 06:06:02.798201: step 30870, loss = 0.25, batch loss = 0.19 (26.8 examples/sec; 0.298 sec/batch; 24h:58m:25s remains)
INFO - root - 2017-12-16 06:06:05.642386: step 30880, loss = 0.21, batch loss = 0.15 (27.8 examples/sec; 0.288 sec/batch; 24h:05m:16s remains)
INFO - root - 2017-12-16 06:06:08.486808: step 30890, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.282 sec/batch; 23h:37m:46s remains)
INFO - root - 2017-12-16 06:06:11.314839: step 30900, loss = 0.26, batch loss = 0.20 (27.4 examples/sec; 0.292 sec/batch; 24h:29m:35s remains)
2017-12-16 06:06:11.786452: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1953373 -5.3571167 -5.5307274 -5.6373758 -5.6109562 -5.45369 -5.19016 -4.9963446 -4.8131652 -4.7486715 -4.7234864 -4.7454381 -4.8472629 -4.6817183 -4.6098981][-5.0477247 -5.1189365 -5.124166 -5.3280315 -5.4579358 -5.510416 -5.4149675 -5.2351522 -5.09458 -5.1525745 -5.1886396 -5.1303091 -5.1109896 -4.8406239 -4.7469831][-4.4253283 -4.3705096 -4.3617191 -4.381258 -4.33996 -4.5370626 -4.6434045 -4.7161193 -4.8999338 -5.0079379 -5.0091767 -5.0438747 -5.0534611 -4.705637 -4.6943583][-3.6040659 -3.2066989 -2.8452482 -2.6634693 -2.5391874 -2.572978 -2.7000611 -3.0670228 -3.581881 -3.9873683 -4.2104397 -4.2705727 -4.2383561 -4.0395803 -4.2499385][-2.7851906 -2.1300511 -1.3519478 -0.72633696 -0.26883984 2.2888184e-05 -0.031280518 -0.62911296 -1.3982794 -2.1460009 -2.5399256 -2.8173542 -2.9348917 -2.8961129 -3.2098689][-2.5689526 -1.638072 -0.40604782 0.822875 1.9047432 2.4465947 2.4544678 1.9440427 1.0497394 0.063546658 -0.46496511 -0.85797954 -1.0911593 -1.2855828 -1.8517425][-2.4656005 -1.6090531 -0.18486023 1.5885868 3.1509237 4.1455059 4.5832987 4.0041246 2.9014606 1.9192567 1.4194841 0.95272446 0.56395197 0.141325 -0.54404521][-2.4018435 -1.6973069 -0.38919353 1.4139023 3.1463389 4.4903173 5.2242994 4.9055061 4.0881891 3.2222433 2.6304154 2.2415981 1.8328838 1.2262096 0.48439598][-2.8768611 -2.3194058 -1.2092321 0.28542805 1.8116541 3.1366739 3.9876404 3.9939394 3.5219493 2.895575 2.5975718 2.4507875 2.0937271 1.5009003 0.8623476][-3.4073188 -3.2075715 -2.6034846 -1.3897786 -0.078137875 1.0753627 1.8747969 2.0147872 1.8305669 1.5463448 1.5055313 1.5983706 1.4030671 1.0052948 0.42467546][-4.31817 -4.0758843 -3.7469156 -3.1288395 -2.4525967 -1.4453285 -0.69686532 -0.48389816 -0.44000649 -0.51528668 -0.19628859 0.18668604 0.13080263 -0.19304895 -0.55271173][-5.6258025 -5.2220874 -4.8901062 -4.4826221 -4.008081 -3.3749542 -3.0222731 -2.7489209 -2.48569 -2.412492 -1.8663604 -1.2865472 -1.1755655 -1.3858087 -1.5217571][-6.7238474 -6.3681149 -6.1375837 -5.6552396 -5.1416574 -4.6661825 -4.2965965 -4.0542893 -3.9153891 -3.7224646 -3.1049359 -2.4867716 -2.2695603 -2.3738911 -2.4000413][-7.9900026 -7.4585576 -6.98318 -6.3720093 -5.8480444 -5.2584214 -4.8211403 -4.5868659 -4.4687181 -4.4088902 -3.9959998 -3.4142163 -3.1412947 -3.247992 -3.1573722][-8.9582558 -8.3585234 -7.6874352 -6.9419613 -6.1770244 -5.4902649 -5.0286107 -4.7942758 -4.6887584 -4.7547321 -4.58896 -4.2140622 -4.0748544 -4.1785259 -4.0561843]]...]
INFO - root - 2017-12-16 06:06:14.656117: step 30910, loss = 0.21, batch loss = 0.15 (28.3 examples/sec; 0.283 sec/batch; 23h:42m:19s remains)
INFO - root - 2017-12-16 06:06:17.453807: step 30920, loss = 0.27, batch loss = 0.21 (29.2 examples/sec; 0.274 sec/batch; 22h:57m:17s remains)
INFO - root - 2017-12-16 06:06:20.347034: step 30930, loss = 0.21, batch loss = 0.15 (27.4 examples/sec; 0.292 sec/batch; 24h:27m:21s remains)
INFO - root - 2017-12-16 06:06:23.191045: step 30940, loss = 0.24, batch loss = 0.18 (27.4 examples/sec; 0.292 sec/batch; 24h:25m:27s remains)
INFO - root - 2017-12-16 06:06:26.015488: step 30950, loss = 0.30, batch loss = 0.24 (28.4 examples/sec; 0.282 sec/batch; 23h:37m:42s remains)
INFO - root - 2017-12-16 06:06:28.856434: step 30960, loss = 0.27, batch loss = 0.21 (25.5 examples/sec; 0.314 sec/batch; 26h:17m:44s remains)
INFO - root - 2017-12-16 06:06:31.742750: step 30970, loss = 0.25, batch loss = 0.20 (26.0 examples/sec; 0.308 sec/batch; 25h:45m:55s remains)
INFO - root - 2017-12-16 06:06:34.570831: step 30980, loss = 0.21, batch loss = 0.16 (28.6 examples/sec; 0.279 sec/batch; 23h:24m:16s remains)
INFO - root - 2017-12-16 06:06:37.394431: step 30990, loss = 0.41, batch loss = 0.36 (28.7 examples/sec; 0.278 sec/batch; 23h:18m:54s remains)
INFO - root - 2017-12-16 06:06:40.257479: step 31000, loss = 0.23, batch loss = 0.17 (25.4 examples/sec; 0.315 sec/batch; 26h:24m:33s remains)
2017-12-16 06:06:40.735137: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5764437 -5.7891521 -5.8143296 -6.0229893 -6.3079743 -6.5089521 -6.7986164 -7.6554842 -8.2466927 -8.565238 -8.436718 -7.8096924 -6.8563929 -5.9344444 -5.3989549][-5.97578 -6.1390347 -6.3316855 -6.2782669 -6.2077127 -6.6935039 -7.4987588 -8.2815495 -8.5978584 -8.9719486 -9.176302 -9.0803785 -8.62467 -7.6668615 -6.8353148][-6.333056 -6.2881451 -6.081718 -5.7880273 -5.6542654 -5.5890255 -5.8229918 -6.8981743 -7.8928413 -8.6439972 -8.9509773 -8.9955292 -9.1106091 -8.680706 -7.9297562][-6.5823374 -6.0392485 -5.5638404 -4.8181853 -3.9196873 -3.7982624 -3.8321598 -4.1765952 -4.8600249 -6.0692015 -7.6235409 -8.4034309 -8.7934933 -8.6286736 -8.1726332][-5.8835106 -5.2836971 -4.4505143 -3.3117232 -2.1521752 -1.3158088 -0.49799776 -0.51516175 -1.058322 -2.4587588 -4.51451 -6.4969268 -8.0424042 -8.525466 -8.0867109][-4.5988603 -3.9808002 -3.1129084 -1.526696 0.24139261 1.6412477 2.9752817 3.4949484 2.872118 0.81456995 -1.8824544 -4.3565617 -6.3761129 -7.7582083 -7.7809706][-3.9233174 -3.0653925 -1.8686526 -0.21169615 2.0629134 3.9979725 5.3853092 6.0432625 5.4281607 3.3198848 -0.13418484 -3.0280643 -4.9546418 -6.1588793 -6.5901604][-4.3987179 -3.4798596 -2.3568656 -0.60213828 1.7726526 3.9038143 5.6680183 6.4466162 5.8370228 3.788868 0.70761919 -2.3134654 -4.3143997 -5.3763695 -5.6460085][-5.3715682 -4.7749853 -4.1316271 -2.7676175 -0.37081385 1.9556422 3.5710773 4.0477066 3.7198753 2.1331053 -0.28668976 -3.0268354 -4.8521838 -5.4675817 -5.1766167][-5.804203 -5.5734172 -5.6041169 -4.9286919 -3.4711897 -1.5201237 0.30171394 0.9896102 0.66778755 -0.739151 -2.4786406 -4.3618832 -5.5246921 -5.8847332 -5.7490358][-6.8247643 -6.7968407 -6.7643528 -6.6608849 -6.2696271 -4.9437022 -3.4339361 -2.5511479 -2.4693441 -3.2912452 -4.4234734 -5.6636577 -6.4738493 -6.5567961 -6.2698007][-7.0331826 -7.2412205 -7.5289993 -7.39713 -7.0083227 -6.5020351 -5.9072566 -5.2858524 -4.8173752 -5.0683694 -5.6967168 -6.7071037 -7.2436228 -7.1131773 -6.6820745][-5.9000626 -6.442174 -6.9712334 -7.066565 -6.8611021 -6.1147342 -5.5371504 -5.6578512 -5.7336664 -5.8711662 -6.0139475 -6.5329442 -6.9812126 -6.9024134 -6.4841738][-4.7680564 -5.1371145 -5.6407137 -5.8893561 -6.0656514 -5.5787764 -4.8881431 -4.636714 -4.8844924 -5.2916284 -5.4549146 -5.7384892 -5.9537191 -5.8427219 -5.505363][-3.9757707 -3.9855309 -4.0598116 -4.2011485 -4.3817115 -4.3248272 -4.2007113 -3.9761305 -3.7580819 -4.0429058 -4.4845695 -4.5866032 -4.4758334 -4.56421 -4.5259848]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 06:06:43.534890: step 31010, loss = 0.33, batch loss = 0.27 (28.2 examples/sec; 0.284 sec/batch; 23h:46m:12s remains)
INFO - root - 2017-12-16 06:06:46.315999: step 31020, loss = 0.26, batch loss = 0.20 (27.8 examples/sec; 0.287 sec/batch; 24h:03m:23s remains)
INFO - root - 2017-12-16 06:06:49.152685: step 31030, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.279 sec/batch; 23h:24m:17s remains)
INFO - root - 2017-12-16 06:06:51.951385: step 31040, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.284 sec/batch; 23h:48m:08s remains)
INFO - root - 2017-12-16 06:06:54.748979: step 31050, loss = 0.29, batch loss = 0.23 (29.4 examples/sec; 0.272 sec/batch; 22h:46m:28s remains)
INFO - root - 2017-12-16 06:06:57.598224: step 31060, loss = 0.27, batch loss = 0.21 (26.8 examples/sec; 0.298 sec/batch; 24h:57m:13s remains)
INFO - root - 2017-12-16 06:07:00.433923: step 31070, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.283 sec/batch; 23h:43m:51s remains)
INFO - root - 2017-12-16 06:07:03.268843: step 31080, loss = 0.20, batch loss = 0.14 (28.6 examples/sec; 0.280 sec/batch; 23h:24m:47s remains)
INFO - root - 2017-12-16 06:07:06.108070: step 31090, loss = 0.27, batch loss = 0.21 (27.3 examples/sec; 0.293 sec/batch; 24h:32m:58s remains)
INFO - root - 2017-12-16 06:07:08.915514: step 31100, loss = 0.27, batch loss = 0.22 (29.5 examples/sec; 0.271 sec/batch; 22h:41m:11s remains)
2017-12-16 06:07:09.368154: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3508868 -3.0950918 -2.9400532 -2.5504818 -2.1990912 -2.1155188 -2.3230746 -2.4591517 -2.6272366 -2.5256882 -2.4537506 -2.330837 -1.9392242 -1.7676675 -1.5771446][-2.5892458 -2.2283194 -1.915797 -1.9235995 -2.0984972 -1.900939 -2.1803722 -2.7029257 -2.8042183 -2.6166716 -2.3953984 -2.2861819 -2.0280833 -1.8194518 -1.6258805][-1.9294584 -1.8196397 -1.7417009 -1.5987756 -1.7126577 -2.0706854 -2.5379095 -2.7996254 -2.8757405 -3.051568 -2.8979583 -2.5983503 -2.4202454 -2.2535975 -2.1131015][-1.4425631 -1.3013189 -1.3864272 -1.6783311 -2.0086341 -2.169446 -2.1751158 -2.6509089 -2.8846722 -2.8905077 -2.9319792 -2.7628558 -2.6430953 -2.5714917 -2.3981581][-1.6858971 -1.4963839 -1.3117945 -1.3349857 -1.3520975 -1.6177156 -1.7591355 -1.7450593 -1.8522801 -2.1489828 -2.3113093 -2.4776487 -2.5439634 -2.5107312 -2.3792217][-1.5337014 -1.5055163 -1.1864352 -0.81155157 -0.49900413 -0.381773 -0.332798 -0.51865458 -0.75492811 -1.1692624 -1.5640526 -1.8451869 -2.0225539 -2.2133923 -2.2804542][-1.549778 -1.4172919 -1.0985296 -0.46870136 0.241292 0.682889 1.0001378 0.9756403 0.77301455 0.17515516 -0.49598908 -0.93545413 -1.3430686 -1.3598068 -1.36781][-2.0521841 -1.6494212 -1.2399335 -0.56029654 0.26931334 1.0181451 1.6623983 1.780129 1.5989928 1.1324415 0.49167967 0.020958424 -0.40149546 -0.54736233 -0.37525177][-2.6690445 -2.32493 -1.7595878 -0.90595436 -0.10011053 0.7807107 1.5469151 1.8194985 1.7510281 1.3994274 0.99324608 0.37103415 -0.10066414 0.010891914 0.29948664][-3.0213113 -2.911005 -2.4783936 -1.6021852 -0.73422289 0.052288055 0.60882425 0.9407177 0.95727825 0.7159605 0.5088172 0.26523018 0.028874874 0.034421921 0.2406311][-3.1495252 -2.9668703 -2.5735397 -2.0033023 -1.5143576 -0.86590362 -0.3579855 -0.27634 -0.33159971 -0.34354591 -0.44902658 -0.50609207 -0.45637012 -0.22823334 0.10961771][-3.3826776 -3.0626471 -2.8057323 -2.2654471 -1.8136795 -1.6533997 -1.6292667 -1.7024093 -1.8015926 -1.7868478 -1.5400243 -1.3675306 -1.2998068 -0.99219632 -0.64563608][-3.4372172 -3.1784246 -2.9544053 -2.6125197 -2.4039018 -2.3045022 -2.4312358 -2.941807 -3.2412376 -3.195667 -2.8218417 -2.374989 -2.0514455 -1.6636457 -1.3032603][-3.258637 -2.9296494 -2.7383575 -2.4762626 -2.3784249 -2.5547948 -3.0147405 -3.54304 -3.9105055 -4.0694838 -3.87477 -3.360599 -2.7248859 -2.1736822 -1.7584281][-3.0837412 -2.6040025 -2.2828865 -2.1005316 -2.1266277 -2.4168205 -3.1335163 -3.9369705 -4.5018344 -4.6034236 -4.3945837 -3.9851513 -3.3653772 -2.7589822 -2.2793365]]...]
INFO - root - 2017-12-16 06:07:12.142523: step 31110, loss = 0.26, batch loss = 0.21 (28.9 examples/sec; 0.277 sec/batch; 23h:10m:51s remains)
INFO - root - 2017-12-16 06:07:14.952695: step 31120, loss = 0.31, batch loss = 0.25 (28.9 examples/sec; 0.276 sec/batch; 23h:08m:23s remains)
INFO - root - 2017-12-16 06:07:17.739431: step 31130, loss = 0.21, batch loss = 0.15 (28.8 examples/sec; 0.278 sec/batch; 23h:15m:38s remains)
INFO - root - 2017-12-16 06:07:20.579541: step 31140, loss = 0.39, batch loss = 0.33 (29.3 examples/sec; 0.273 sec/batch; 22h:52m:06s remains)
INFO - root - 2017-12-16 06:07:23.454899: step 31150, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.282 sec/batch; 23h:35m:17s remains)
INFO - root - 2017-12-16 06:07:26.269449: step 31160, loss = 0.25, batch loss = 0.19 (29.1 examples/sec; 0.275 sec/batch; 22h:59m:08s remains)
INFO - root - 2017-12-16 06:07:29.080678: step 31170, loss = 0.25, batch loss = 0.19 (29.4 examples/sec; 0.273 sec/batch; 22h:48m:54s remains)
INFO - root - 2017-12-16 06:07:31.905982: step 31180, loss = 0.32, batch loss = 0.26 (28.0 examples/sec; 0.286 sec/batch; 23h:53m:53s remains)
INFO - root - 2017-12-16 06:07:34.728700: step 31190, loss = 0.32, batch loss = 0.26 (28.9 examples/sec; 0.277 sec/batch; 23h:11m:21s remains)
INFO - root - 2017-12-16 06:07:37.562291: step 31200, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.284 sec/batch; 23h:48m:11s remains)
2017-12-16 06:07:38.069615: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6867132 -4.7757034 -5.0278478 -5.1833439 -5.3136315 -5.3825917 -5.449091 -5.5783443 -5.535697 -5.2996678 -4.9557343 -4.8089647 -4.9378786 -5.1485863 -5.316628][-3.6014597 -3.6168504 -3.7427604 -3.9577227 -4.2921438 -4.7776513 -5.2209015 -5.2434583 -5.1887288 -5.1239762 -4.9582286 -4.9106321 -5.0660043 -5.355382 -5.5362496][-2.0373626 -2.0904858 -2.3135951 -2.7207661 -3.0603065 -3.3443182 -3.706486 -4.1558533 -4.3690138 -4.3661752 -4.4744182 -4.8050594 -5.1140933 -5.417366 -5.5553017][-0.83460808 -0.78812718 -0.87456965 -1.0761232 -1.4558058 -1.8243167 -2.1815715 -2.3474379 -2.6692181 -3.0447836 -3.4033344 -4.0642328 -4.6960864 -5.1541839 -5.3388014][0.070474625 0.32329082 0.48065042 0.50585079 0.46584702 0.34529114 0.087255478 -0.26356602 -0.60299444 -1.0106511 -1.745527 -2.8528781 -3.7714055 -4.3308353 -4.6160083][0.029713631 0.49099255 0.90828037 1.1185927 1.3361707 1.6200061 1.8864455 1.8703213 1.5852017 0.97732496 0.022754669 -1.3266501 -2.6609936 -3.5175045 -4.0251727][-1.0701013 -0.34922695 0.43998718 1.1998863 2.1529746 2.9541759 3.5367398 3.8415604 3.7183428 3.0236192 1.8261833 0.40777397 -1.0858302 -2.3568556 -3.363039][-2.1572576 -1.5031683 -0.80141854 0.17866135 1.6436043 3.227231 4.5311995 5.0572128 5.0269604 4.4868832 3.0507474 1.3617306 -0.2968154 -1.6156535 -2.8249402][-4.0369544 -3.5559461 -2.9476032 -2.0114641 -0.43431258 1.5774035 3.3329191 4.2498531 4.5558529 4.2151461 3.1420569 1.631041 -0.16503382 -1.8772907 -3.3023074][-5.6951866 -5.88722 -5.784286 -4.8073077 -3.2215381 -1.2608683 0.60673285 2.0385833 2.7288718 2.5873947 1.7284384 0.50490427 -1.0235815 -2.6491714 -4.0253234][-6.9167271 -7.4597521 -7.8546524 -7.4021993 -6.0972252 -4.1797757 -2.398684 -1.0036325 -0.10248375 0.08881855 -0.56707954 -1.7624056 -3.0596519 -4.4191985 -5.4666157][-8.67295 -9.464365 -9.8733406 -9.5370541 -8.5649509 -6.9704771 -5.4793282 -4.0256586 -3.16294 -3.1537876 -3.5677469 -4.2343836 -5.0676179 -6.0117016 -6.5195827][-9.6942472 -10.876069 -11.41915 -11.049981 -10.348893 -9.1527958 -7.8398085 -6.6379452 -6.0343924 -5.941247 -6.1642032 -6.6852436 -7.0214043 -7.3143167 -7.2295737][-10.399519 -11.429837 -11.788773 -11.456772 -10.912716 -10.255703 -9.5838966 -8.6999493 -8.2229767 -8.3037987 -8.49921 -8.5873938 -8.4797115 -8.2627831 -7.612711][-10.249815 -11.079276 -11.344954 -11.002017 -10.457628 -9.9078178 -9.5951462 -9.4019423 -9.4590263 -9.6105862 -9.6600647 -9.4203 -8.9315414 -8.3864536 -7.4028516]]...]
INFO - root - 2017-12-16 06:07:40.942356: step 31210, loss = 0.27, batch loss = 0.22 (28.0 examples/sec; 0.285 sec/batch; 23h:53m:21s remains)
INFO - root - 2017-12-16 06:07:43.768069: step 31220, loss = 0.30, batch loss = 0.24 (28.2 examples/sec; 0.284 sec/batch; 23h:44m:30s remains)
INFO - root - 2017-12-16 06:07:46.640024: step 31230, loss = 0.30, batch loss = 0.25 (27.1 examples/sec; 0.296 sec/batch; 24h:43m:57s remains)
INFO - root - 2017-12-16 06:07:49.425688: step 31240, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 23h:25m:45s remains)
INFO - root - 2017-12-16 06:07:52.284688: step 31250, loss = 0.24, batch loss = 0.18 (28.6 examples/sec; 0.280 sec/batch; 23h:25m:28s remains)
INFO - root - 2017-12-16 06:07:55.111984: step 31260, loss = 0.20, batch loss = 0.14 (28.0 examples/sec; 0.286 sec/batch; 23h:53m:43s remains)
INFO - root - 2017-12-16 06:07:57.933828: step 31270, loss = 0.29, batch loss = 0.24 (28.7 examples/sec; 0.278 sec/batch; 23h:18m:03s remains)
INFO - root - 2017-12-16 06:08:00.771627: step 31280, loss = 0.28, batch loss = 0.22 (27.5 examples/sec; 0.291 sec/batch; 24h:21m:29s remains)
INFO - root - 2017-12-16 06:08:03.659349: step 31290, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 23h:56m:08s remains)
INFO - root - 2017-12-16 06:08:06.513102: step 31300, loss = 0.20, batch loss = 0.14 (27.2 examples/sec; 0.294 sec/batch; 24h:35m:04s remains)
2017-12-16 06:08:06.958582: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.862535 -5.1851091 -4.622602 -4.4691024 -4.2825294 -4.2588067 -4.1865606 -4.2684436 -4.354353 -4.5270352 -4.8071127 -4.98464 -5.1719508 -5.248096 -5.5617547][-5.9797039 -5.6182747 -5.1336374 -4.5374684 -3.9065986 -4.0871248 -4.3013983 -4.5022712 -4.3360567 -4.5484118 -4.9315085 -5.2023487 -5.4325185 -5.4144573 -5.5878806][-6.4893456 -5.6534762 -4.7770739 -4.1936445 -3.559418 -3.2546225 -3.4730542 -3.9760203 -4.0626683 -4.5714431 -4.93935 -5.2438803 -5.7392387 -5.8462858 -5.7827559][-6.1060486 -5.2527728 -4.4579973 -3.2843716 -2.07726 -1.7777469 -1.9364398 -2.37009 -3.0322433 -4.1878715 -4.9909935 -5.5624309 -6.0381718 -6.2110443 -6.1179986][-5.904839 -4.7082682 -3.2770891 -1.7520335 -0.663744 -0.020629406 -0.11353731 -0.43201089 -1.2528479 -2.9083753 -4.0802131 -4.8848972 -5.5093966 -6.1014285 -6.1994934][-6.1177478 -4.6190987 -2.6633792 -0.57230234 1.2770772 2.0472031 1.9132566 1.309166 0.15234661 -1.3370993 -2.4202619 -3.4756303 -4.6178503 -5.2119446 -5.1235142][-6.2501173 -4.8613062 -2.921361 -0.061375141 2.4961495 3.773488 3.9873571 3.2748785 1.8803825 -0.15405703 -1.7816057 -2.7924628 -3.4412887 -4.2691069 -5.0638762][-6.5995312 -5.1675987 -2.9084477 0.04554081 2.3272324 3.660511 4.5158615 3.9849768 2.34163 0.30281448 -0.9903214 -2.164196 -3.3252745 -4.1248322 -5.117878][-7.5198269 -5.9220204 -3.363178 -0.61248207 1.2923937 2.5111151 3.1583056 2.776166 1.6601171 0.24530268 -0.58357167 -1.5390673 -2.3397217 -3.4043531 -4.7651434][-8.2748547 -7.1996527 -5.3488088 -2.7696271 -0.53488946 0.53282976 0.86913776 0.8596015 0.19481897 -0.7192688 -1.491051 -1.994029 -2.4463992 -3.2472887 -4.4067731][-9.30987 -8.1489086 -6.3064575 -4.3188577 -3.060169 -2.1082058 -1.280731 -1.3167238 -2.1210666 -2.703382 -3.12435 -3.4210067 -3.6636033 -4.2080345 -4.9221678][-9.5893383 -8.4189234 -6.61884 -5.0571885 -4.154912 -3.7433438 -3.618659 -3.7444396 -3.9259365 -4.0271797 -4.0613036 -4.17546 -4.3676505 -4.605526 -4.9411764][-9.113802 -8.0602665 -6.9864311 -5.8801432 -4.8130412 -4.3333554 -4.4941492 -4.88856 -5.0422125 -5.1559381 -5.00245 -4.692862 -4.5264859 -4.5840173 -4.5755677][-10.023523 -8.4069157 -6.6082077 -5.5320139 -5.0881 -4.8510838 -4.6737905 -5.1257615 -5.6705661 -5.7855535 -5.43427 -4.9745946 -4.8286409 -4.8577819 -4.4458246][-9.7406044 -8.3103981 -6.4321194 -5.284132 -4.7949438 -4.5565405 -4.5118175 -4.9171886 -5.4634852 -5.7984591 -5.6402683 -5.1607304 -4.8439946 -4.8390813 -4.6561079]]...]
INFO - root - 2017-12-16 06:08:09.819519: step 31310, loss = 0.28, batch loss = 0.22 (29.1 examples/sec; 0.275 sec/batch; 22h:59m:57s remains)
INFO - root - 2017-12-16 06:08:12.622027: step 31320, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 23h:13m:34s remains)
INFO - root - 2017-12-16 06:08:15.456324: step 31330, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.282 sec/batch; 23h:36m:50s remains)
INFO - root - 2017-12-16 06:08:18.269634: step 31340, loss = 0.27, batch loss = 0.21 (27.4 examples/sec; 0.292 sec/batch; 24h:27m:29s remains)
INFO - root - 2017-12-16 06:08:21.092518: step 31350, loss = 0.21, batch loss = 0.15 (28.9 examples/sec; 0.277 sec/batch; 23h:11m:07s remains)
INFO - root - 2017-12-16 06:08:24.007209: step 31360, loss = 0.25, batch loss = 0.20 (27.0 examples/sec; 0.296 sec/batch; 24h:45m:53s remains)
INFO - root - 2017-12-16 06:08:26.785203: step 31370, loss = 0.27, batch loss = 0.21 (29.3 examples/sec; 0.273 sec/batch; 22h:48m:09s remains)
INFO - root - 2017-12-16 06:08:29.646213: step 31380, loss = 0.26, batch loss = 0.20 (27.8 examples/sec; 0.288 sec/batch; 24h:06m:15s remains)
INFO - root - 2017-12-16 06:08:32.453795: step 31390, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.284 sec/batch; 23h:44m:06s remains)
INFO - root - 2017-12-16 06:08:35.287224: step 31400, loss = 0.26, batch loss = 0.20 (29.2 examples/sec; 0.274 sec/batch; 22h:55m:50s remains)
2017-12-16 06:08:35.737142: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.507472 -5.4458995 -5.1369505 -4.7518754 -4.2128744 -4.0478187 -4.059082 -4.4922433 -4.8624368 -5.2086992 -5.531846 -5.6236219 -5.7951779 -5.7633429 -5.6948962][-5.8244557 -5.6290836 -5.32738 -4.7387624 -3.9139507 -3.7070959 -3.6818066 -4.0406084 -4.4460354 -4.9585986 -5.3530064 -5.447576 -5.7535548 -5.7771244 -5.7104988][-6.475934 -6.1785297 -5.5849037 -4.668869 -3.5290306 -2.9657242 -2.805685 -3.152926 -3.585777 -4.1843719 -4.6939082 -4.9188557 -5.3519478 -5.4560156 -5.4849997][-6.4096513 -6.1627398 -5.4827852 -4.2435994 -2.7904572 -1.817239 -1.2881234 -1.4512005 -2.0224521 -2.8316545 -3.446645 -3.8344262 -4.3553929 -4.6782403 -4.8003111][-6.10255 -5.6330881 -4.7489309 -3.2865119 -1.5636106 -0.24899149 0.4958477 0.46237326 -0.14295435 -1.0593534 -1.8448765 -2.3921275 -2.9763033 -3.4667697 -3.6298673][-5.7815733 -5.2551093 -4.2088103 -2.4589963 -0.38780928 1.3199716 2.2872863 2.3230271 1.6266842 0.69580126 -0.045663357 -0.77842283 -1.4321406 -1.9611301 -2.0938737][-5.3606191 -4.8400722 -3.8766356 -2.0384192 0.20992041 2.2479005 3.5373292 3.5883417 2.7866144 1.7116971 1.1106238 0.37372828 -0.31560755 -0.84189487 -0.914387][-5.2755294 -4.7839708 -3.8321338 -2.0667162 0.17492628 2.352529 3.7976007 4.141428 3.3771796 2.3185191 1.6617351 0.81393528 0.32687378 -0.15569735 -0.26352644][-5.4650259 -4.9834585 -4.0207458 -2.4000654 -0.40255213 1.7522168 3.1438007 3.2736306 2.6701856 1.7174282 1.2142644 0.59327173 0.15825224 -0.13871193 -0.12218857][-5.7973537 -5.6153927 -4.7987909 -3.5020843 -1.7781398 0.064137459 1.297996 1.4090343 0.84935904 0.19253111 -0.16145802 -0.68264389 -0.92878175 -1.2234266 -1.2655628][-5.9625463 -5.7907629 -5.3563251 -4.5768337 -3.3237247 -2.0246077 -1.2133172 -1.138134 -1.5783954 -2.1672776 -2.3755126 -2.6568947 -2.7837939 -3.1883821 -3.3190413][-6.0825882 -5.9798675 -5.48468 -5.03285 -4.3733044 -3.812089 -3.5396247 -3.5980344 -3.9162977 -4.3529925 -4.7607117 -5.00139 -5.1007528 -5.4632511 -5.6235938][-5.6001291 -5.5817919 -5.4291139 -5.154058 -4.8242407 -4.7138863 -4.7899351 -5.0567536 -5.5004082 -5.9498463 -6.3266964 -6.4441519 -6.62398 -7.0654917 -7.2883697][-5.1599894 -4.9757986 -4.8040843 -4.5943227 -4.4256916 -4.8366261 -5.3081493 -5.7972627 -6.4374003 -7.0398989 -7.4006834 -7.5109344 -7.5952606 -8.0753956 -8.259201][-5.013 -4.9580278 -4.4929447 -4.14065 -3.9837439 -4.3176932 -4.9636688 -5.5205975 -6.1928639 -6.9397163 -7.3757238 -7.50594 -7.4357948 -7.6869383 -8.0005713]]...]
INFO - root - 2017-12-16 06:08:38.540522: step 31410, loss = 0.35, batch loss = 0.29 (29.2 examples/sec; 0.274 sec/batch; 22h:55m:54s remains)
INFO - root - 2017-12-16 06:08:41.370483: step 31420, loss = 0.24, batch loss = 0.18 (27.8 examples/sec; 0.287 sec/batch; 24h:02m:12s remains)
INFO - root - 2017-12-16 06:08:44.213325: step 31430, loss = 0.34, batch loss = 0.29 (28.8 examples/sec; 0.278 sec/batch; 23h:15m:07s remains)
INFO - root - 2017-12-16 06:08:47.005673: step 31440, loss = 0.24, batch loss = 0.18 (27.3 examples/sec; 0.293 sec/batch; 24h:31m:55s remains)
INFO - root - 2017-12-16 06:08:49.844139: step 31450, loss = 0.30, batch loss = 0.25 (27.5 examples/sec; 0.291 sec/batch; 24h:18m:35s remains)
INFO - root - 2017-12-16 06:08:52.692882: step 31460, loss = 0.21, batch loss = 0.15 (29.3 examples/sec; 0.273 sec/batch; 22h:50m:53s remains)
INFO - root - 2017-12-16 06:08:55.542934: step 31470, loss = 0.35, batch loss = 0.29 (26.3 examples/sec; 0.305 sec/batch; 25h:28m:59s remains)
INFO - root - 2017-12-16 06:08:58.373167: step 31480, loss = 0.22, batch loss = 0.16 (28.7 examples/sec; 0.279 sec/batch; 23h:17m:15s remains)
INFO - root - 2017-12-16 06:09:01.189681: step 31490, loss = 0.23, batch loss = 0.17 (29.6 examples/sec; 0.271 sec/batch; 22h:37m:57s remains)
INFO - root - 2017-12-16 06:09:04.014204: step 31500, loss = 0.31, batch loss = 0.25 (28.0 examples/sec; 0.286 sec/batch; 23h:52m:36s remains)
2017-12-16 06:09:04.503392: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8506846 -4.9226141 -4.9544358 -4.9956126 -4.9132051 -4.6790133 -4.3983607 -4.5591555 -4.9328713 -5.4585209 -6.2073517 -7.1297636 -8.2062378 -8.849206 -9.2376041][-4.5072393 -4.8678222 -4.9401588 -4.8584838 -4.78448 -4.8538532 -4.5886674 -4.2595863 -4.2404184 -4.686502 -5.5545979 -6.4689856 -7.3971796 -7.9458628 -8.4682178][-4.0693893 -4.282732 -4.2121534 -4.1002197 -3.9295826 -4.0353003 -4.0836105 -4.1363988 -4.1788669 -4.4336042 -4.9143362 -5.4414773 -6.3476009 -6.8752146 -7.315073][-3.1378379 -3.4357052 -3.458039 -2.8479271 -2.3915327 -2.3451996 -2.3317635 -2.7364202 -3.2507172 -3.4742098 -3.5775042 -4.2233858 -5.2011495 -5.7596049 -6.5431051][-2.0091734 -1.9427345 -1.5912111 -1.1616595 -0.67558455 -0.34762383 -0.21140718 -0.49861073 -0.77888894 -1.6685531 -2.3148751 -2.5384836 -3.0300393 -4.0290551 -5.38278][-1.6308658 -0.940356 0.067468643 1.2690578 2.040411 2.1917949 2.1610174 1.8524275 1.3981953 0.78372383 0.52530575 -0.15631914 -1.0884318 -1.9015627 -2.9708519][-2.0000134 -1.0138199 0.27509737 1.6971464 3.1409736 4.0333672 4.2539997 3.6783524 3.1742406 2.5182238 2.0806551 1.5063066 0.81979418 -0.33939886 -1.9636726][-2.7344198 -1.6907437 -0.28986406 1.5695858 3.162046 4.199811 4.9488144 4.8465481 4.2404051 3.1442404 2.5258751 1.8459511 0.92327929 -0.30375385 -1.6523576][-3.9396818 -3.1836472 -1.8891268 -0.30317402 1.1760378 2.4084573 3.1894736 3.1067481 2.9790268 2.3515673 1.7845783 1.0020418 0.24981117 -0.55561543 -1.3821487][-4.3970194 -4.0124025 -3.5273418 -2.472888 -1.2962234 -0.47400856 0.17082071 0.38849211 0.28711128 -0.093186855 -0.19706488 -0.67152667 -1.2187705 -1.6823659 -2.2974391][-4.7925091 -4.6104717 -4.3302417 -3.9492292 -3.865025 -3.2560749 -2.5375443 -2.4918957 -2.6929364 -2.9402523 -3.0515254 -3.4221067 -3.8971839 -3.8610177 -3.8590713][-5.0663714 -4.9788632 -5.0416665 -5.1109242 -5.1291084 -5.2306895 -5.3148117 -5.3835936 -5.3173556 -5.3221817 -5.3443475 -5.6327639 -5.7328882 -5.4929142 -5.3692584][-4.8251662 -4.866775 -5.1801996 -5.4252658 -5.8013725 -5.9370956 -5.9743042 -5.894628 -5.8042293 -5.9893579 -6.0523152 -6.0746741 -6.1406097 -6.1703243 -5.9566736][-3.8637252 -3.9835334 -4.2133031 -4.6033978 -4.9883556 -5.1872063 -5.3750911 -5.4778357 -5.5950894 -5.4991527 -5.4052439 -5.8584185 -6.2382588 -6.0768151 -5.7493315][-3.8437042 -3.66749 -3.6230631 -3.7808888 -3.9664226 -4.2183971 -4.4832177 -4.5440717 -4.5059242 -4.7024584 -4.9109173 -4.8771725 -4.8031178 -4.99277 -5.0776625]]...]
INFO - root - 2017-12-16 06:09:07.273571: step 31510, loss = 0.32, batch loss = 0.26 (29.5 examples/sec; 0.272 sec/batch; 22h:42m:15s remains)
INFO - root - 2017-12-16 06:09:10.129652: step 31520, loss = 0.23, batch loss = 0.17 (28.8 examples/sec; 0.278 sec/batch; 23h:12m:50s remains)
INFO - root - 2017-12-16 06:09:12.961113: step 31530, loss = 0.23, batch loss = 0.17 (27.9 examples/sec; 0.287 sec/batch; 23h:58m:25s remains)
INFO - root - 2017-12-16 06:09:15.791433: step 31540, loss = 0.30, batch loss = 0.24 (26.9 examples/sec; 0.297 sec/batch; 24h:50m:40s remains)
INFO - root - 2017-12-16 06:09:18.582485: step 31550, loss = 0.21, batch loss = 0.16 (29.3 examples/sec; 0.273 sec/batch; 22h:48m:10s remains)
INFO - root - 2017-12-16 06:09:21.419214: step 31560, loss = 0.27, batch loss = 0.21 (28.9 examples/sec; 0.277 sec/batch; 23h:07m:11s remains)
INFO - root - 2017-12-16 06:09:24.275527: step 31570, loss = 0.39, batch loss = 0.33 (28.6 examples/sec; 0.280 sec/batch; 23h:23m:51s remains)
INFO - root - 2017-12-16 06:09:27.109421: step 31580, loss = 0.34, batch loss = 0.28 (28.9 examples/sec; 0.277 sec/batch; 23h:07m:49s remains)
INFO - root - 2017-12-16 06:09:29.949072: step 31590, loss = 0.25, batch loss = 0.20 (27.2 examples/sec; 0.294 sec/batch; 24h:35m:50s remains)
INFO - root - 2017-12-16 06:09:32.790375: step 31600, loss = 0.29, batch loss = 0.23 (29.6 examples/sec; 0.271 sec/batch; 22h:37m:38s remains)
2017-12-16 06:09:33.264704: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4625373 -4.4343233 -4.3171196 -4.2356958 -4.4387259 -4.51207 -4.6917624 -4.9122357 -4.8660493 -4.5469055 -4.1511889 -3.7986124 -3.3730946 -3.0041339 -2.7904892][-4.2242951 -4.2753334 -4.2702832 -4.2447605 -4.3867588 -4.659246 -4.8409023 -4.9994149 -4.995409 -4.7244854 -4.4587708 -4.0298109 -3.5150454 -3.0924931 -2.8156402][-3.74069 -3.8316679 -3.8553936 -3.9013124 -4.1295872 -4.3580337 -4.588366 -4.8681874 -4.8585258 -4.6565766 -4.4179277 -4.2170792 -3.8899777 -3.4091823 -2.973279][-2.7355652 -2.7656603 -2.7688088 -2.9162667 -3.2072263 -3.3644922 -3.5747006 -3.7325592 -3.7934871 -3.8149378 -3.8700235 -4.0017915 -3.916275 -3.5919032 -3.176244][-1.830431 -1.6555305 -1.3265555 -1.3233314 -1.4319768 -1.4525628 -1.5355642 -1.7703714 -2.0318615 -2.408138 -2.9676423 -3.6194136 -3.8807669 -3.7985897 -3.3610148][-1.0079229 -0.71049833 -0.29368877 0.0962615 0.36549187 0.47138977 0.66971064 0.58268976 0.057691574 -0.83494258 -1.9698906 -3.1734939 -3.7556574 -3.8518977 -3.4913831][-0.78211308 -0.3659296 -0.022040844 0.38537216 0.98143911 1.4166689 1.8243976 1.9484806 1.6042771 0.64081335 -0.74253321 -2.2556617 -3.242789 -3.5934949 -3.3815169][-1.0354671 -0.7575686 -0.40060997 0.19297457 0.90330935 1.6211157 2.1328549 2.336699 2.1472759 1.1795182 -0.23502636 -1.6619797 -2.6672468 -3.1204805 -3.1600175][-1.8834605 -1.5947971 -1.3683004 -0.72415876 0.0093131065 0.71530867 1.3312702 1.6549249 1.7169228 0.87068844 -0.40994835 -1.7330897 -2.6630692 -3.0634627 -3.2033734][-2.7618828 -2.549665 -2.4982953 -2.0889797 -1.3196011 -0.70255375 -0.19703436 0.21162605 0.41388178 -0.080114365 -0.95115519 -1.9901955 -2.797035 -3.1701355 -3.2737505][-3.2145977 -3.2337306 -3.3053932 -3.1594372 -2.6927047 -2.1340764 -1.606113 -1.0705719 -0.67986631 -0.80800772 -1.3886385 -2.2332838 -2.8779063 -3.1925735 -3.2958956][-3.433136 -3.4805589 -3.7638948 -3.7659674 -3.5150557 -3.0126777 -2.3805256 -1.9172723 -1.4716985 -1.4749348 -1.8648958 -2.5169706 -3.0858908 -3.2982633 -3.3470573][-2.8911533 -2.9080253 -3.3511081 -3.4685988 -3.401814 -3.2273684 -2.7500219 -2.3593841 -1.9414437 -1.8908904 -2.2470911 -2.7498975 -3.2124834 -3.3083253 -3.2391903][-1.9205759 -1.8376105 -2.2848148 -2.7845836 -2.9701033 -2.8727884 -2.5861201 -2.2729006 -1.9507303 -1.9417186 -2.3028669 -2.7922711 -3.2213631 -3.3051791 -3.175751][-1.1175489 -1.0241218 -1.5217395 -2.1345384 -2.4694147 -2.3705044 -1.9474905 -1.5798836 -1.3439157 -1.4205661 -1.8416474 -2.4081936 -2.8532481 -2.9374161 -2.7870622]]...]
INFO - root - 2017-12-16 06:09:36.077351: step 31610, loss = 0.29, batch loss = 0.23 (28.9 examples/sec; 0.277 sec/batch; 23h:06m:45s remains)
INFO - root - 2017-12-16 06:09:38.892056: step 31620, loss = 0.29, batch loss = 0.23 (27.0 examples/sec; 0.296 sec/batch; 24h:44m:09s remains)
INFO - root - 2017-12-16 06:09:41.671472: step 31630, loss = 0.26, batch loss = 0.20 (29.3 examples/sec; 0.273 sec/batch; 22h:50m:35s remains)
INFO - root - 2017-12-16 06:09:44.475184: step 31640, loss = 0.21, batch loss = 0.15 (29.0 examples/sec; 0.276 sec/batch; 23h:04m:03s remains)
INFO - root - 2017-12-16 06:09:47.301158: step 31650, loss = 0.24, batch loss = 0.19 (28.9 examples/sec; 0.277 sec/batch; 23h:09m:16s remains)
INFO - root - 2017-12-16 06:09:50.212932: step 31660, loss = 0.25, batch loss = 0.19 (26.7 examples/sec; 0.299 sec/batch; 25h:00m:25s remains)
INFO - root - 2017-12-16 06:09:53.061630: step 31670, loss = 0.22, batch loss = 0.17 (28.0 examples/sec; 0.286 sec/batch; 23h:52m:47s remains)
INFO - root - 2017-12-16 06:09:55.874428: step 31680, loss = 0.23, batch loss = 0.17 (28.9 examples/sec; 0.277 sec/batch; 23h:07m:08s remains)
INFO - root - 2017-12-16 06:09:58.733761: step 31690, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.281 sec/batch; 23h:30m:34s remains)
INFO - root - 2017-12-16 06:10:01.554767: step 31700, loss = 0.33, batch loss = 0.27 (29.2 examples/sec; 0.274 sec/batch; 22h:51m:44s remains)
2017-12-16 06:10:02.004378: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4800892 -3.2739072 -3.3300462 -3.6037064 -3.8999803 -4.2129254 -4.4928946 -4.5422726 -4.5788412 -4.2685614 -3.9123259 -3.6468468 -3.2931087 -2.8492405 -2.3567996][-3.7649605 -3.6697197 -3.6423335 -3.9633107 -4.4026537 -4.7268519 -4.9828892 -5.1518593 -5.15601 -4.9557228 -4.788065 -4.4151311 -4.0531497 -3.64308 -3.0405719][-4.2009559 -4.3530436 -4.6334472 -4.7491179 -4.722455 -4.7690225 -4.9303432 -5.1218805 -5.3062935 -5.361402 -5.378026 -5.3523283 -5.091805 -4.5409341 -3.9320645][-5.3125677 -5.4334884 -5.3566761 -5.1106706 -4.7187929 -4.2294736 -3.9826944 -3.9338219 -4.2144132 -5.0491462 -5.7556281 -6.0007091 -5.86771 -5.4842205 -4.89424][-5.9533844 -6.009491 -5.7477903 -4.8483539 -3.423933 -2.0412614 -1.2600613 -1.3493087 -2.28039 -3.5452776 -4.8688664 -5.9372759 -6.3718123 -6.0011568 -5.1251712][-6.3578739 -6.1127424 -5.22964 -3.2334125 -0.70086861 1.3669958 2.4223132 2.21145 0.85174894 -1.1976471 -3.5184011 -5.2265253 -5.9733047 -5.9265018 -5.364944][-6.8455348 -6.294559 -5.0417285 -2.4325552 1.0229402 3.7932425 5.4126797 5.2339287 3.3061819 0.63320732 -1.870213 -3.7211368 -4.8163166 -4.9988337 -4.2325025][-6.6835756 -6.2196512 -4.976007 -2.1040785 1.2657762 4.47334 6.1775961 5.8351374 4.1563845 1.4679675 -1.2497602 -3.0462539 -3.6988127 -3.8051727 -3.2579708][-6.6856804 -6.0363445 -4.759191 -2.6093719 0.26951742 3.3779454 5.1962318 5.0391188 3.2044015 0.6646862 -1.7319524 -3.7083251 -4.353415 -3.627589 -2.3136237][-6.3569126 -6.3377495 -5.8469791 -4.1577663 -1.8401406 0.45159578 1.6872029 1.9936171 1.1995621 -0.727963 -2.7389388 -4.4538484 -5.1123962 -4.6149697 -3.2416959][-6.6210918 -6.7884951 -6.5509796 -5.9110661 -4.8138533 -3.2972291 -1.9974005 -1.6694188 -2.1548381 -3.0553038 -4.2713594 -5.4495649 -5.6694131 -5.3830214 -4.3475814][-6.4781857 -6.8717041 -7.1579905 -6.8234482 -6.2352991 -5.7724032 -5.339047 -4.9082079 -4.758153 -5.2832794 -5.7781973 -6.3991604 -6.5091419 -6.1829324 -5.4368863][-5.6361647 -6.133883 -6.7068124 -6.9970508 -7.0808773 -6.6929646 -6.4016519 -6.2489934 -5.902699 -5.8530636 -6.0988979 -6.3698182 -6.5390821 -6.8045177 -6.5008612][-5.1078687 -5.3734112 -5.6814108 -5.9263349 -6.254725 -6.3674974 -6.3290091 -6.1571059 -5.8493309 -5.4242506 -5.1961784 -5.7551718 -6.3828754 -6.6828737 -6.7004943][-4.6306825 -4.5521269 -4.5141525 -4.5512738 -4.6625509 -4.6952887 -4.8933558 -4.8739066 -4.6480546 -4.4175811 -4.3894081 -4.7865386 -5.5687695 -6.4106574 -6.7334337]]...]
INFO - root - 2017-12-16 06:10:04.816924: step 31710, loss = 0.21, batch loss = 0.15 (29.0 examples/sec; 0.276 sec/batch; 23h:04m:08s remains)
INFO - root - 2017-12-16 06:10:07.676442: step 31720, loss = 0.27, batch loss = 0.21 (27.0 examples/sec; 0.297 sec/batch; 24h:47m:04s remains)
INFO - root - 2017-12-16 06:10:10.501665: step 31730, loss = 0.23, batch loss = 0.17 (28.9 examples/sec; 0.277 sec/batch; 23h:08m:25s remains)
INFO - root - 2017-12-16 06:10:13.332016: step 31740, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.276 sec/batch; 23h:05m:52s remains)
INFO - root - 2017-12-16 06:10:16.138151: step 31750, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.279 sec/batch; 23h:20m:34s remains)
INFO - root - 2017-12-16 06:10:19.058268: step 31760, loss = 0.22, batch loss = 0.16 (27.9 examples/sec; 0.287 sec/batch; 23h:57m:31s remains)
INFO - root - 2017-12-16 06:10:21.862747: step 31770, loss = 0.27, batch loss = 0.21 (28.3 examples/sec; 0.282 sec/batch; 23h:35m:45s remains)
INFO - root - 2017-12-16 06:10:24.748681: step 31780, loss = 0.26, batch loss = 0.20 (27.6 examples/sec; 0.290 sec/batch; 24h:11m:13s remains)
INFO - root - 2017-12-16 06:10:27.612414: step 31790, loss = 0.28, batch loss = 0.22 (29.6 examples/sec; 0.270 sec/batch; 22h:33m:27s remains)
INFO - root - 2017-12-16 06:10:30.432706: step 31800, loss = 0.23, batch loss = 0.18 (28.9 examples/sec; 0.277 sec/batch; 23h:09m:32s remains)
2017-12-16 06:10:30.904935: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.135241 -5.2257915 -4.4378409 -4.1970444 -4.0974402 -4.4355574 -4.9466233 -5.4463272 -5.7562404 -5.8781924 -5.9711866 -5.8659234 -5.8147907 -5.784236 -5.6862068][-6.4359255 -5.46447 -4.53334 -3.9832923 -3.6976402 -4.110178 -4.7496538 -5.3616195 -5.6217985 -5.6824827 -5.7233214 -5.6853609 -5.7286468 -5.3770013 -5.010788][-6.4177666 -5.3778019 -4.3652287 -3.4669673 -2.9318404 -3.3272069 -4.0584154 -4.7175312 -5.01733 -5.2768321 -5.3122196 -5.1964383 -5.3518076 -5.0233164 -4.4799509][-6.1165934 -4.6887627 -3.4003139 -2.6667466 -2.2597725 -2.3352509 -2.754895 -3.4788091 -3.9630928 -4.1202 -4.1827559 -4.3470464 -4.5643125 -4.2625766 -3.72323][-5.6454644 -4.0212426 -2.3939345 -1.4205725 -0.84603977 -1.0472662 -1.4864826 -1.883863 -2.2215018 -2.6501341 -3.2139549 -3.4657888 -3.7002971 -3.5520709 -3.1175938][-4.9415855 -3.3267617 -1.7051141 -0.403687 0.539659 0.59515524 0.3681736 0.0025401115 -0.35598183 -0.99756074 -1.8754065 -2.7549148 -3.620091 -3.5096927 -3.175719][-4.4186711 -2.8636932 -1.3910265 -0.099894047 0.91968727 1.4034209 1.6500964 1.4529014 1.2494736 0.60457563 -0.32330084 -1.5715241 -2.8270643 -3.3733459 -3.6284833][-4.2542372 -2.8067946 -1.3476043 -0.21260071 0.84718895 1.5232081 1.9856367 2.1434932 2.0384569 1.3703742 0.32708216 -0.92954254 -2.153985 -3.1158295 -3.7525635][-4.0376496 -2.9818807 -1.7424586 -0.684819 0.34462881 1.2009587 1.8410544 2.1820674 2.1105156 1.5768924 0.50771761 -0.97595263 -2.4582984 -3.3219771 -3.8708959][-3.9504566 -3.0862525 -2.2493041 -1.3557794 -0.36841202 0.43936872 1.1217728 1.5436921 1.5633054 0.8762126 -0.31209993 -1.6922121 -2.9644949 -3.9442589 -4.5589504][-4.4789076 -3.5888312 -2.8362527 -2.2531633 -1.6681924 -1.0625434 -0.47487283 -0.15827465 -0.17743921 -0.71916771 -1.7260022 -2.9247777 -4.0906491 -4.8182311 -5.1267676][-5.3269429 -4.4764748 -3.717463 -3.3181634 -2.9792819 -2.8466139 -2.6440167 -2.4048548 -2.433743 -2.9077318 -3.58876 -4.2405019 -4.9764786 -5.2635279 -5.192595][-6.5176086 -5.497993 -4.6297579 -4.182229 -4.0614767 -4.109642 -4.1070027 -4.1772847 -4.3977323 -4.67018 -5.1099925 -5.3782372 -5.5001059 -5.3277068 -5.0073967][-7.7225156 -6.5640731 -5.5879855 -4.851685 -4.5550604 -4.9290686 -5.4095907 -5.644269 -5.7758288 -5.9670691 -6.1513381 -5.9862604 -5.5771666 -4.9356446 -4.2881103][-8.497983 -7.2634869 -6.1927943 -5.5220366 -5.2489271 -5.5463009 -6.0862093 -6.5370793 -6.886117 -7.0115252 -6.9956179 -6.5213966 -5.9088192 -5.003684 -4.1604948]]...]
INFO - root - 2017-12-16 06:10:33.728024: step 31810, loss = 0.20, batch loss = 0.14 (26.8 examples/sec; 0.298 sec/batch; 24h:55m:23s remains)
INFO - root - 2017-12-16 06:10:36.509163: step 31820, loss = 0.30, batch loss = 0.24 (29.0 examples/sec; 0.276 sec/batch; 23h:03m:55s remains)
INFO - root - 2017-12-16 06:10:39.346582: step 31830, loss = 0.29, batch loss = 0.23 (27.9 examples/sec; 0.287 sec/batch; 23h:59m:27s remains)
INFO - root - 2017-12-16 06:10:42.170836: step 31840, loss = 0.29, batch loss = 0.23 (28.9 examples/sec; 0.276 sec/batch; 23h:04m:51s remains)
INFO - root - 2017-12-16 06:10:45.023464: step 31850, loss = 0.34, batch loss = 0.29 (28.6 examples/sec; 0.280 sec/batch; 23h:23m:07s remains)
INFO - root - 2017-12-16 06:10:47.802446: step 31860, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.283 sec/batch; 23h:39m:59s remains)
INFO - root - 2017-12-16 06:10:50.631584: step 31870, loss = 0.31, batch loss = 0.25 (29.0 examples/sec; 0.276 sec/batch; 23h:03m:31s remains)
INFO - root - 2017-12-16 06:10:53.500696: step 31880, loss = 0.25, batch loss = 0.19 (29.5 examples/sec; 0.272 sec/batch; 22h:40m:42s remains)
INFO - root - 2017-12-16 06:10:56.377882: step 31890, loss = 0.28, batch loss = 0.22 (27.3 examples/sec; 0.293 sec/batch; 24h:30m:01s remains)
INFO - root - 2017-12-16 06:10:59.167297: step 31900, loss = 0.23, batch loss = 0.17 (28.6 examples/sec; 0.279 sec/batch; 23h:19m:40s remains)
2017-12-16 06:10:59.632427: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0476828 -6.3536229 -6.3344131 -6.3722534 -6.3269334 -6.1317148 -6.1673865 -6.7114048 -7.1860685 -7.1317439 -6.8548746 -6.3231816 -5.5873561 -4.866631 -4.401906][-6.3911786 -6.9610381 -7.1191063 -6.9418936 -6.5127478 -6.2792578 -6.2105503 -6.2286797 -6.5525322 -6.954052 -7.0611029 -6.6826439 -5.9703403 -5.2338567 -4.5933356][-6.5030069 -7.1345587 -7.3309641 -6.9964809 -6.3306246 -5.5883055 -5.0892916 -5.05891 -5.347383 -5.6781344 -6.0457697 -6.2561464 -5.9613156 -5.266716 -4.6261482][-6.7628088 -7.509769 -7.6338625 -6.9834042 -5.9541512 -4.623806 -3.4427867 -2.9099064 -2.9459326 -3.5723696 -4.2680812 -4.7935514 -4.9840422 -4.850975 -4.5447025][-6.4974422 -7.2005329 -7.2584977 -6.3087749 -4.6830859 -2.7048826 -0.95388985 0.063799381 0.11547661 -0.74592233 -1.911557 -3.0376954 -3.6689918 -3.7513411 -3.7168126][-5.8202744 -6.28319 -6.1309133 -4.97825 -2.7803028 -0.033438683 2.3946505 3.7475967 3.6484909 2.3140974 0.46749496 -1.2844617 -2.5085058 -2.9920342 -2.9871209][-5.03703 -5.4694886 -4.9899454 -3.420995 -0.96994209 2.1189861 5.1259918 6.64664 6.1204758 4.3290024 2.0261731 -0.12998295 -1.6987703 -2.413115 -2.5469522][-4.6324639 -4.8611488 -4.386898 -2.5756741 0.21505642 3.4389253 6.2745695 7.6350307 6.9577122 4.6933851 1.9903326 -0.15565872 -1.5405979 -2.1903236 -2.2398133][-4.1395559 -4.6380258 -4.0510125 -2.5961323 -0.40118837 2.4184308 4.7562132 5.9187679 5.295476 3.3021479 0.86275148 -1.1115141 -2.1436195 -2.3915577 -2.1246219][-3.9128921 -4.7420135 -5.0334396 -4.0636277 -2.2254725 -0.28100348 1.1582198 1.8583851 1.3394651 0.074937344 -1.3576849 -2.4690714 -2.8917098 -2.7927105 -2.3395746][-4.2913384 -5.1108346 -5.6037035 -5.6678247 -4.97095 -3.6534429 -2.7568102 -2.4578092 -2.824719 -3.4559073 -3.8387504 -3.9275713 -3.6158562 -3.1108871 -2.5561814][-4.7983704 -5.8187032 -6.6297283 -6.8537531 -6.6440458 -6.3351736 -6.1729612 -6.0975566 -5.982862 -5.7843342 -5.3700976 -4.756074 -3.9059131 -3.2601757 -2.8084083][-5.3102603 -6.5811529 -7.6168537 -8.202424 -8.4369411 -8.0239935 -7.6373577 -7.4637709 -7.3319826 -6.8625631 -5.8495817 -4.8393507 -3.968586 -3.2898245 -2.9021745][-5.6660953 -6.7467875 -7.517179 -8.0082655 -8.2351332 -8.3265066 -8.1923666 -7.6560717 -7.0902882 -6.6198168 -5.9347286 -4.9713697 -4.0217485 -3.3445592 -2.989785][-5.3895164 -6.2689924 -6.8751183 -7.0998478 -7.1185312 -6.9999604 -6.8641562 -6.6292768 -6.13974 -5.6687727 -5.1898007 -4.5580106 -3.798363 -3.1219349 -2.7659452]]...]
INFO - root - 2017-12-16 06:11:02.474682: step 31910, loss = 0.29, batch loss = 0.24 (26.2 examples/sec; 0.305 sec/batch; 25h:29m:02s remains)
INFO - root - 2017-12-16 06:11:05.325113: step 31920, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 23h:43m:21s remains)
INFO - root - 2017-12-16 06:11:08.109266: step 31930, loss = 0.20, batch loss = 0.15 (29.5 examples/sec; 0.271 sec/batch; 22h:36m:19s remains)
INFO - root - 2017-12-16 06:11:10.982241: step 31940, loss = 0.26, batch loss = 0.20 (27.4 examples/sec; 0.292 sec/batch; 24h:22m:48s remains)
INFO - root - 2017-12-16 06:11:13.787502: step 31950, loss = 0.24, batch loss = 0.18 (29.0 examples/sec; 0.275 sec/batch; 22h:59m:45s remains)
INFO - root - 2017-12-16 06:11:16.625472: step 31960, loss = 0.24, batch loss = 0.18 (29.1 examples/sec; 0.275 sec/batch; 22h:56m:04s remains)
INFO - root - 2017-12-16 06:11:19.463041: step 31970, loss = 0.23, batch loss = 0.17 (28.1 examples/sec; 0.285 sec/batch; 23h:45m:07s remains)
INFO - root - 2017-12-16 06:11:22.298440: step 31980, loss = 0.24, batch loss = 0.18 (27.6 examples/sec; 0.289 sec/batch; 24h:10m:00s remains)
INFO - root - 2017-12-16 06:11:25.187504: step 31990, loss = 0.21, batch loss = 0.15 (27.4 examples/sec; 0.292 sec/batch; 24h:24m:35s remains)
INFO - root - 2017-12-16 06:11:28.005704: step 32000, loss = 0.34, batch loss = 0.28 (27.7 examples/sec; 0.289 sec/batch; 24h:05m:31s remains)
2017-12-16 06:11:28.462023: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9376242 -4.3102446 -4.5055404 -4.5693488 -4.557034 -4.4299545 -4.2056832 -4.0591073 -4.0251741 -4.0497818 -4.1097059 -4.30821 -4.3932562 -4.3300953 -4.1380777][-3.9009845 -4.3780036 -4.7252769 -4.7820563 -4.5992489 -4.4418211 -4.3394537 -4.2257915 -4.1636939 -4.3401084 -4.5873 -4.8135314 -4.9704022 -4.9713778 -4.716167][-3.9119115 -4.3752751 -4.5371604 -4.4663515 -4.1876111 -3.9104748 -3.5884404 -3.4874673 -3.5826223 -4.0553493 -4.5503116 -5.0209436 -5.4195542 -5.5622425 -5.356431][-3.8678331 -4.27928 -4.2604718 -3.8705602 -3.1723514 -2.5398035 -1.9643602 -1.6729264 -1.7910762 -2.5465384 -3.5136108 -4.4332466 -5.2498093 -5.7201738 -5.6470842][-3.3923025 -3.5485988 -3.31466 -2.6152296 -1.5391598 -0.45477533 0.50747156 1.1154771 0.90716457 -0.063365459 -1.4846556 -3.0282817 -4.27502 -5.0051465 -5.1870742][-2.7662685 -2.4995449 -2.0670807 -1.0652714 0.46199179 1.973701 3.2037892 3.8824568 3.7645645 2.5150456 0.69327736 -1.3906748 -3.0004833 -4.0044622 -4.2223043][-2.7594137 -2.2505257 -1.4297605 -0.098865032 1.6593528 3.5729847 5.5775375 6.3577709 6.1288481 4.5870819 2.4555273 0.11402893 -1.7745173 -2.978555 -3.3488526][-2.9744439 -2.4682717 -1.8506067 -0.50883555 1.6348238 3.8627853 6.031311 7.0893497 7.0011444 5.4934473 3.1669598 0.9374609 -0.88198829 -2.1357663 -2.6264572][-3.2450767 -3.1349189 -2.8116169 -1.9487967 -0.38694668 1.9847693 4.2194881 5.5415239 5.6489363 4.4258051 2.6833458 0.71445847 -0.8673203 -1.8195589 -2.2694247][-3.7621982 -3.9481349 -4.3695059 -3.8686526 -2.5500674 -0.54960465 1.3476934 2.6007476 3.0806866 2.1563234 0.87709 -0.58907151 -1.8067291 -2.4793835 -2.7386277][-4.7000265 -5.0002694 -5.4076295 -5.2110748 -4.4772215 -2.953197 -1.1828554 -0.08355093 0.29224682 -0.079803944 -1.0166805 -2.1366355 -2.8579915 -3.183598 -3.2220626][-4.7786865 -5.2855811 -5.8637381 -5.8215961 -5.2173319 -4.0517831 -2.8746414 -1.9466932 -1.638062 -1.8441634 -2.4085698 -3.1404123 -3.560185 -3.6956737 -3.5913706][-4.2341809 -4.6617131 -5.0884056 -5.21012 -5.0844026 -4.3268747 -3.3877606 -2.7825511 -2.430166 -2.5846725 -2.8586521 -3.2873 -3.5018392 -3.5493751 -3.4788308][-3.7676322 -3.8960238 -4.0674124 -4.1840482 -4.1891966 -3.8298676 -3.310976 -3.0227838 -2.7866516 -2.8226151 -2.9741361 -3.1101041 -3.22003 -3.2120872 -3.0294943][-3.0904956 -2.866334 -2.7802389 -2.772382 -2.8563809 -2.7562258 -2.6489964 -2.5435891 -2.4993305 -2.5376735 -2.6387837 -2.6813569 -2.6655192 -2.7423897 -2.7156029]]...]
INFO - root - 2017-12-16 06:11:31.270001: step 32010, loss = 0.32, batch loss = 0.26 (29.1 examples/sec; 0.275 sec/batch; 22h:58m:28s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 06:11:34.114286: step 32020, loss = 0.19, batch loss = 0.14 (27.5 examples/sec; 0.291 sec/batch; 24h:15m:03s remains)
INFO - root - 2017-12-16 06:11:36.928367: step 32030, loss = 0.24, batch loss = 0.18 (28.9 examples/sec; 0.277 sec/batch; 23h:08m:09s remains)
INFO - root - 2017-12-16 06:11:39.781775: step 32040, loss = 0.27, batch loss = 0.22 (28.8 examples/sec; 0.278 sec/batch; 23h:10m:14s remains)
INFO - root - 2017-12-16 06:11:42.651438: step 32050, loss = 0.33, batch loss = 0.27 (28.7 examples/sec; 0.279 sec/batch; 23h:15m:53s remains)
INFO - root - 2017-12-16 06:11:45.473247: step 32060, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.282 sec/batch; 23h:31m:37s remains)
INFO - root - 2017-12-16 06:11:48.326449: step 32070, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.283 sec/batch; 23h:36m:10s remains)
INFO - root - 2017-12-16 06:11:51.139852: step 32080, loss = 0.23, batch loss = 0.17 (27.7 examples/sec; 0.289 sec/batch; 24h:04m:34s remains)
INFO - root - 2017-12-16 06:11:54.005685: step 32090, loss = 0.29, batch loss = 0.23 (28.5 examples/sec; 0.281 sec/batch; 23h:27m:50s remains)
INFO - root - 2017-12-16 06:11:56.856620: step 32100, loss = 0.27, batch loss = 0.21 (28.1 examples/sec; 0.284 sec/batch; 23h:43m:28s remains)
2017-12-16 06:11:57.306682: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6643679 -2.7899132 -3.007617 -3.5830455 -4.2723155 -4.62881 -4.6338229 -4.2413483 -3.8060529 -3.30895 -2.8698516 -2.859695 -2.9293139 -3.1176324 -3.4379573][-2.8519652 -3.027257 -3.2647998 -3.5678205 -4.043786 -4.4169602 -4.2740655 -3.6661649 -3.2535911 -2.8849206 -2.7202477 -2.7346992 -2.8961382 -3.154036 -3.3655763][-2.8618588 -2.8825407 -3.0654857 -3.5063095 -3.8458254 -3.7455606 -3.2844398 -2.6457067 -2.2051291 -2.0178206 -2.0860753 -2.4599617 -2.8597398 -3.0458894 -3.1815357][-3.1939762 -3.5001633 -3.6952076 -3.7295868 -3.5515244 -3.1785316 -2.5588114 -1.7264407 -1.2028601 -1.1846743 -1.5351775 -2.0992265 -2.6411448 -2.7351456 -2.33773][-3.8325715 -3.8997319 -3.9172454 -3.6802197 -3.2136636 -2.3638759 -1.2685366 -0.42766428 -0.19118881 -0.31585741 -0.8742075 -1.7174914 -2.2342103 -2.3890407 -1.8989422][-3.6401186 -3.5263386 -3.2410178 -2.6789138 -1.7297492 -0.68919134 0.35352993 1.3771706 1.6083255 0.96214342 -0.27983093 -1.5113714 -2.3884063 -2.2099915 -1.4186442][-3.552063 -3.3125443 -2.8579836 -1.9557097 -0.75385356 0.66579485 1.8455625 2.7398643 3.0019836 2.4625406 1.0526443 -0.77473211 -2.0731275 -2.4277372 -1.8770049][-4.210711 -3.6655467 -3.1042664 -2.3005745 -0.85526252 0.70159912 2.1966171 3.2397947 3.4932375 3.0417085 1.7067451 -0.19836664 -1.6534777 -2.2548118 -2.1395926][-4.941349 -4.342597 -3.4697237 -2.674088 -1.735723 -0.19698238 1.3649783 2.3878098 2.77416 2.3757238 1.0759244 -0.66722536 -2.1367548 -2.8810039 -2.8790278][-4.4922423 -4.3993058 -3.9676769 -3.1919277 -2.2887633 -1.1838577 -0.11921215 1.0161943 1.6850295 1.4716606 0.39664841 -1.3120608 -2.8052621 -3.6073785 -3.76911][-4.034554 -3.9816029 -3.7357373 -3.4024563 -2.7449415 -1.8919055 -1.1497462 -0.40555382 0.21049023 0.41213655 -0.41598368 -1.9187112 -3.2623243 -3.9731994 -4.0518241][-4.0182157 -3.8194807 -3.5476923 -3.6794796 -3.611846 -2.9579296 -2.1097207 -1.6250291 -1.2185593 -1.0361238 -1.5580726 -2.5668283 -3.5739398 -4.2206612 -4.312326][-3.5676026 -3.4404871 -3.4970665 -3.6408868 -3.8281765 -3.5991192 -3.079946 -2.6524649 -2.273237 -2.1879094 -2.5253749 -3.2964852 -3.9868112 -4.3963208 -4.3625903][-2.4142919 -2.363203 -2.4946976 -2.879447 -3.3546796 -3.4172816 -3.2096915 -2.8632107 -2.5206687 -2.3716474 -2.6808929 -3.310185 -3.8846021 -4.1523275 -4.0564051][-1.4792056 -1.3381042 -1.3885663 -1.769748 -2.308188 -2.610285 -2.5462885 -2.2786219 -2.0589657 -2.1162863 -2.404496 -3.1009035 -3.8363266 -4.1837831 -4.0282168]]...]
INFO - root - 2017-12-16 06:12:00.104815: step 32110, loss = 0.28, batch loss = 0.22 (29.0 examples/sec; 0.276 sec/batch; 23h:02m:49s remains)
INFO - root - 2017-12-16 06:12:02.926631: step 32120, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.285 sec/batch; 23h:44m:37s remains)
INFO - root - 2017-12-16 06:12:05.825105: step 32130, loss = 0.24, batch loss = 0.18 (26.9 examples/sec; 0.298 sec/batch; 24h:49m:35s remains)
INFO - root - 2017-12-16 06:12:08.692860: step 32140, loss = 0.33, batch loss = 0.27 (28.5 examples/sec; 0.281 sec/batch; 23h:27m:11s remains)
INFO - root - 2017-12-16 06:12:11.521040: step 32150, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.283 sec/batch; 23h:38m:13s remains)
INFO - root - 2017-12-16 06:12:14.347909: step 32160, loss = 0.32, batch loss = 0.26 (25.2 examples/sec; 0.318 sec/batch; 26h:31m:10s remains)
INFO - root - 2017-12-16 06:12:17.199743: step 32170, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.283 sec/batch; 23h:38m:39s remains)
INFO - root - 2017-12-16 06:12:20.023059: step 32180, loss = 0.33, batch loss = 0.27 (28.3 examples/sec; 0.283 sec/batch; 23h:35m:10s remains)
INFO - root - 2017-12-16 06:12:22.835438: step 32190, loss = 0.25, batch loss = 0.20 (29.3 examples/sec; 0.273 sec/batch; 22h:44m:20s remains)
INFO - root - 2017-12-16 06:12:25.669629: step 32200, loss = 0.23, batch loss = 0.17 (28.9 examples/sec; 0.277 sec/batch; 23h:05m:51s remains)
2017-12-16 06:12:26.134777: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9178848 -3.2855191 -3.6128192 -4.0706768 -4.5305409 -4.8900275 -5.2089033 -5.5593691 -5.662447 -5.4428582 -5.1278849 -4.7061191 -4.3726439 -4.0634322 -3.8965378][-2.7100251 -3.0423722 -3.352221 -3.7568045 -4.0694251 -4.492609 -4.7933455 -4.9740629 -5.1420984 -5.1447358 -4.9247603 -4.5236797 -4.136333 -3.8287766 -3.6442525][-2.8562355 -3.1539993 -3.339613 -3.436013 -3.3583384 -3.3988261 -3.6025531 -3.8596125 -4.2451096 -4.4347453 -4.4269795 -4.2697463 -4.049973 -3.6794453 -3.4177265][-3.0739923 -3.2992454 -3.3776414 -3.05244 -2.3811176 -1.8133738 -1.7367086 -1.9503024 -2.5413227 -3.1201224 -3.6510034 -3.8583984 -3.6961989 -3.3411932 -3.0620508][-3.2228909 -3.3315334 -3.0014982 -2.1479406 -1.1017487 -0.10098934 0.30444527 0.088754654 -0.70805168 -1.7443039 -2.6359787 -3.1718459 -3.2270339 -3.0681434 -2.7734036][-3.4295366 -3.1859207 -2.6683564 -1.422776 0.33432579 1.6759033 2.0909657 1.7970486 0.9137001 -0.38411283 -1.6372361 -2.5181127 -2.8134127 -2.6965775 -2.4726791][-3.529623 -3.1301479 -2.2683563 -0.785934 1.0056853 2.6967626 3.4473038 3.102797 1.9865994 0.5726552 -0.78214622 -1.8953583 -2.3551643 -2.3903806 -2.2978542][-3.3208451 -2.8736322 -1.9602258 -0.33312798 1.4305515 2.9230075 3.6572733 3.4420938 2.5399733 1.1263256 -0.35122204 -1.5009089 -1.9832907 -2.1912141 -2.1412547][-3.2669296 -2.728322 -1.7721076 -0.35812759 1.2556443 2.5649862 3.105547 2.8839998 2.1571102 0.96297789 -0.26014996 -1.241713 -1.8164704 -2.1731579 -2.1549811][-3.2870107 -2.9823527 -2.3369915 -1.0853913 0.20471478 1.2372451 1.764245 1.636579 1.0110359 0.14022493 -0.72794986 -1.5506589 -2.0414407 -2.2823894 -2.2131293][-3.93741 -3.7580171 -3.299696 -2.586761 -1.6131763 -0.79862571 -0.46505618 -0.40365553 -0.80679655 -1.3680017 -1.8542874 -2.1883152 -2.4435148 -2.5682981 -2.3414483][-4.5474844 -4.8134222 -4.85516 -4.4749479 -3.8185053 -3.0685325 -2.6458368 -2.6234283 -2.919836 -3.101687 -3.2158422 -3.1585479 -3.057924 -2.9306474 -2.5970378][-5.2885027 -5.8802471 -6.194356 -6.1130219 -5.7850704 -5.2277489 -4.7032375 -4.3418474 -4.5288224 -4.6320062 -4.4961476 -4.2680717 -3.9814172 -3.5733173 -3.0759532][-6.0154004 -6.88364 -7.4590168 -7.5687904 -7.3260565 -6.7036295 -6.1258879 -5.660562 -5.5404167 -5.3606887 -5.1912208 -4.9396768 -4.5272965 -4.0560842 -3.7457292][-6.3319159 -7.5537543 -8.458704 -8.67691 -8.43148 -7.7240915 -7.0227604 -6.4254637 -6.0928574 -5.788413 -5.4477472 -5.1885386 -4.9684439 -4.6544476 -4.3363819]]...]
INFO - root - 2017-12-16 06:12:28.952130: step 32210, loss = 0.33, batch loss = 0.27 (28.3 examples/sec; 0.283 sec/batch; 23h:34m:37s remains)
INFO - root - 2017-12-16 06:12:31.793083: step 32220, loss = 0.28, batch loss = 0.22 (28.7 examples/sec; 0.279 sec/batch; 23h:17m:08s remains)
INFO - root - 2017-12-16 06:12:34.582684: step 32230, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 23h:09m:44s remains)
INFO - root - 2017-12-16 06:12:37.423818: step 32240, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 23h:34m:25s remains)
INFO - root - 2017-12-16 06:12:40.244417: step 32250, loss = 0.28, batch loss = 0.22 (27.9 examples/sec; 0.286 sec/batch; 23h:52m:24s remains)
INFO - root - 2017-12-16 06:12:43.108587: step 32260, loss = 0.28, batch loss = 0.22 (29.0 examples/sec; 0.276 sec/batch; 22h:59m:25s remains)
INFO - root - 2017-12-16 06:12:45.941668: step 32270, loss = 0.26, batch loss = 0.20 (27.6 examples/sec; 0.289 sec/batch; 24h:08m:30s remains)
INFO - root - 2017-12-16 06:12:48.742851: step 32280, loss = 0.28, batch loss = 0.22 (29.2 examples/sec; 0.274 sec/batch; 22h:52m:59s remains)
INFO - root - 2017-12-16 06:12:51.594416: step 32290, loss = 0.23, batch loss = 0.17 (29.6 examples/sec; 0.270 sec/batch; 22h:31m:19s remains)
INFO - root - 2017-12-16 06:12:54.423483: step 32300, loss = 0.32, batch loss = 0.26 (28.8 examples/sec; 0.278 sec/batch; 23h:11m:42s remains)
2017-12-16 06:12:54.888931: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0985584 -4.2844152 -4.4629383 -4.7350607 -4.9547534 -5.02839 -5.0746717 -5.1951694 -5.2322593 -5.0720625 -4.7548075 -4.4649963 -4.01696 -3.5820045 -3.1834421][-3.5556586 -3.7474895 -4.00509 -4.3113875 -4.5845327 -4.9837327 -5.2102261 -5.215837 -5.2026129 -5.2357016 -5.1804843 -4.9066544 -4.3495674 -3.7725973 -3.25074][-2.9786739 -3.118962 -3.4512484 -3.7050366 -3.8948274 -4.1270151 -4.3320279 -4.6432233 -4.8671446 -5.0323553 -5.1336493 -4.9949694 -4.6746936 -4.1208067 -3.5288534][-2.2765613 -2.2534208 -2.3119366 -2.40549 -2.5350661 -2.600698 -2.6592338 -2.9102974 -3.4084044 -4.0028081 -4.4332485 -4.6250091 -4.5623088 -4.1736708 -3.7017105][-1.7276087 -1.4476326 -1.1142433 -0.84461975 -0.59175014 -0.41960144 -0.37750673 -0.56843758 -1.1296916 -2.1115355 -3.1280749 -3.8283908 -4.096076 -3.9762931 -3.6863475][-1.77145 -1.173322 -0.34626818 0.52581739 1.3453288 1.7940683 2.0154061 1.7795177 0.99485064 -0.13788223 -1.4318485 -2.6742043 -3.5325503 -3.6914508 -3.55473][-2.08294 -1.3534274 -0.14377403 1.2341485 2.5185037 3.4643974 3.8952541 3.6999855 2.9847093 1.6338897 0.019825935 -1.458015 -2.5325866 -3.0838084 -3.2485917][-2.7548466 -1.7097707 -0.41221046 1.1113982 2.5351634 3.6434193 4.3063869 4.2334909 3.6370888 2.4214005 0.85802364 -0.80104733 -2.0486858 -2.7286551 -2.9849539][-3.7301939 -2.7402968 -1.405134 -0.0043225288 1.2238059 2.4947257 3.2543416 3.3872991 3.0826545 2.0464296 0.64212847 -0.8377049 -2.0472834 -2.7948658 -3.0694454][-4.3757987 -3.7317889 -2.9285338 -1.7993467 -0.64003491 0.32893562 0.97169971 1.5163341 1.5664725 0.81251955 -0.35951424 -1.636296 -2.5897653 -3.1725757 -3.42265][-4.7756109 -4.1217003 -3.6478794 -3.2135136 -2.754715 -2.0012071 -1.3055446 -0.82465434 -0.61108613 -0.84205651 -1.5039303 -2.4823966 -3.3291163 -3.7659841 -3.8428478][-5.1204066 -4.5967822 -4.4307928 -4.1127038 -3.7427678 -3.5256882 -3.2931862 -2.8291841 -2.4057553 -2.3936975 -2.6995335 -3.270896 -3.812093 -4.0912209 -4.2081828][-4.9083824 -4.7125578 -4.6436543 -4.7828026 -4.9388809 -4.6984072 -4.3554144 -3.9762769 -3.6059284 -3.4453826 -3.4941542 -3.7047892 -3.9548163 -4.2063742 -4.3165164][-4.3824 -4.3586335 -4.33561 -4.4864392 -4.6058192 -4.64892 -4.6152945 -4.30905 -3.9245596 -3.6219819 -3.5097742 -3.6091566 -3.7631698 -3.886225 -3.9746997][-3.6356759 -3.4932032 -3.5224063 -3.7737746 -3.9809959 -4.0520325 -4.0298657 -3.8535185 -3.5580578 -3.3878574 -3.2974558 -3.2094035 -3.1735296 -3.28893 -3.4276991]]...]
INFO - root - 2017-12-16 06:12:57.715387: step 32310, loss = 0.49, batch loss = 0.43 (27.9 examples/sec; 0.287 sec/batch; 23h:55m:30s remains)
INFO - root - 2017-12-16 06:13:00.538088: step 32320, loss = 0.26, batch loss = 0.20 (29.1 examples/sec; 0.275 sec/batch; 22h:55m:32s remains)
INFO - root - 2017-12-16 06:13:03.364410: step 32330, loss = 0.28, batch loss = 0.22 (28.9 examples/sec; 0.277 sec/batch; 23h:06m:45s remains)
INFO - root - 2017-12-16 06:13:06.174578: step 32340, loss = 0.27, batch loss = 0.22 (28.6 examples/sec; 0.280 sec/batch; 23h:18m:23s remains)
INFO - root - 2017-12-16 06:13:09.086579: step 32350, loss = 0.23, batch loss = 0.18 (27.4 examples/sec; 0.292 sec/batch; 24h:21m:54s remains)
INFO - root - 2017-12-16 06:13:11.905425: step 32360, loss = 0.30, batch loss = 0.25 (28.5 examples/sec; 0.281 sec/batch; 23h:25m:45s remains)
INFO - root - 2017-12-16 06:13:14.739642: step 32370, loss = 0.23, batch loss = 0.17 (26.4 examples/sec; 0.303 sec/batch; 25h:14m:45s remains)
INFO - root - 2017-12-16 06:13:17.537972: step 32380, loss = 0.28, batch loss = 0.22 (29.5 examples/sec; 0.271 sec/batch; 22h:35m:27s remains)
INFO - root - 2017-12-16 06:13:20.367408: step 32390, loss = 0.24, batch loss = 0.18 (25.5 examples/sec; 0.314 sec/batch; 26h:09m:48s remains)
INFO - root - 2017-12-16 06:13:23.253010: step 32400, loss = 0.58, batch loss = 0.52 (27.3 examples/sec; 0.293 sec/batch; 24h:23m:10s remains)
2017-12-16 06:13:23.749127: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7740893 -3.9956553 -4.2927985 -4.3244343 -4.3981824 -4.5565305 -4.5578928 -4.4229689 -4.319489 -4.1526275 -4.0097947 -4.1195879 -4.4468517 -5.2367597 -6.1448374][-3.8728902 -4.341898 -4.6374826 -4.6985354 -4.8440442 -4.97894 -4.9167242 -4.6596246 -4.4105258 -4.2221546 -4.296051 -4.6801291 -5.295702 -6.0792713 -6.625772][-4.302907 -4.8092313 -5.2674341 -5.3121004 -4.8825936 -4.6961393 -4.526422 -4.2095623 -4.1660995 -4.1468706 -4.486692 -4.8749762 -5.4761972 -6.2279396 -6.8037739][-4.6486988 -5.2468843 -5.7428122 -5.6116023 -4.6431928 -3.7091949 -2.9413495 -2.6291642 -2.8481507 -3.2340684 -3.9912877 -4.5823021 -5.2035437 -5.9762831 -6.5950909][-5.3485966 -5.4701524 -5.034399 -4.4570918 -3.3219168 -1.895946 -0.55948257 0.069502831 -0.19899273 -0.98668432 -2.3394876 -3.4790211 -4.4081149 -5.2728071 -5.9367862][-5.7374535 -5.7187328 -4.8095078 -3.3619776 -1.1391463 0.61363888 1.8043213 2.5532889 2.2936687 1.2297683 -0.42626715 -1.801749 -2.8332958 -3.8800108 -4.8105807][-5.31716 -5.2774816 -4.309042 -2.5120063 0.054350853 2.2916927 3.9001217 4.4869823 4.1029978 3.0631261 1.3175726 -0.35672569 -1.7269158 -2.7101436 -3.509654][-5.1344662 -4.8401575 -3.5644131 -1.7714202 0.5390625 2.8849535 4.6896734 5.1016254 4.3567476 3.1971574 1.7962351 0.33069134 -0.93374109 -1.8213761 -2.735209][-5.2540841 -5.1443815 -3.8454156 -2.2251019 -0.37272644 1.5513206 3.2061782 3.7993126 3.3295283 2.1449347 0.82649183 -0.3798542 -1.4025598 -1.913034 -2.562408][-4.9326692 -5.3394103 -4.76573 -3.3989546 -1.7784352 -0.41893053 0.72689629 1.080894 0.86322117 0.1429081 -0.73630381 -1.5438821 -2.1723304 -2.5946457 -3.2696652][-5.3386331 -5.6449118 -5.1338124 -4.2759285 -3.3047848 -2.4038978 -1.6039104 -1.3953726 -1.6572878 -2.3434 -2.8646493 -3.0779347 -3.1840856 -3.2069602 -3.63169][-5.618248 -5.8307853 -5.3297477 -4.6355815 -4.0074191 -3.5965774 -3.4440618 -3.5268216 -3.5675936 -3.8986125 -4.1761966 -4.2587862 -4.2537026 -3.8540425 -3.8585343][-4.4796648 -4.5455232 -4.5959744 -4.0712037 -3.5599177 -3.5584521 -3.8133874 -4.2063913 -4.5594287 -4.7994447 -4.5623775 -4.313859 -4.0280061 -3.6498656 -3.5387864][-4.1969008 -3.6290698 -3.1386187 -2.8211861 -2.7834032 -2.9249356 -3.2775128 -3.9077413 -4.5013819 -4.8702517 -4.6226373 -4.1241426 -3.6796467 -3.1885991 -2.8861847][-3.6749039 -3.0539927 -2.4248867 -1.9070234 -1.9712214 -2.4202557 -3.0697689 -3.6969194 -4.2205958 -4.5057664 -4.1183124 -3.5327981 -3.0760477 -2.7449665 -2.7131689]]...]
INFO - root - 2017-12-16 06:13:26.608869: step 32410, loss = 0.25, batch loss = 0.19 (27.8 examples/sec; 0.287 sec/batch; 23h:56m:46s remains)
INFO - root - 2017-12-16 06:13:29.448987: step 32420, loss = 0.27, batch loss = 0.21 (28.6 examples/sec; 0.280 sec/batch; 23h:18m:45s remains)
INFO - root - 2017-12-16 06:13:32.258345: step 32430, loss = 0.27, batch loss = 0.22 (27.0 examples/sec; 0.296 sec/batch; 24h:39m:56s remains)
INFO - root - 2017-12-16 06:13:35.082379: step 32440, loss = 0.22, batch loss = 0.16 (28.8 examples/sec; 0.278 sec/batch; 23h:09m:39s remains)
INFO - root - 2017-12-16 06:13:37.901227: step 32450, loss = 0.29, batch loss = 0.23 (28.8 examples/sec; 0.277 sec/batch; 23h:07m:06s remains)
INFO - root - 2017-12-16 06:13:40.751452: step 32460, loss = 0.29, batch loss = 0.23 (26.7 examples/sec; 0.299 sec/batch; 24h:55m:53s remains)
INFO - root - 2017-12-16 06:13:43.568120: step 32470, loss = 0.24, batch loss = 0.18 (29.5 examples/sec; 0.271 sec/batch; 22h:35m:12s remains)
INFO - root - 2017-12-16 06:13:46.384850: step 32480, loss = 0.27, batch loss = 0.22 (28.6 examples/sec; 0.280 sec/batch; 23h:18m:38s remains)
INFO - root - 2017-12-16 06:13:49.255790: step 32490, loss = 0.20, batch loss = 0.14 (28.1 examples/sec; 0.285 sec/batch; 23h:42m:55s remains)
INFO - root - 2017-12-16 06:13:52.067880: step 32500, loss = 0.26, batch loss = 0.21 (28.7 examples/sec; 0.278 sec/batch; 23h:11m:32s remains)
2017-12-16 06:13:52.509364: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1966031 -2.4094207 -2.6186476 -3.0584934 -3.6025329 -3.9599288 -4.3806753 -4.984375 -5.575645 -5.9791961 -5.7980194 -5.5260243 -5.1145968 -4.5070109 -3.7792315][-2.866364 -3.1722436 -3.4385781 -3.6202717 -3.7543292 -4.1442595 -4.550961 -4.8534727 -5.07335 -5.4080892 -5.837142 -5.9325294 -5.4873447 -4.8307858 -4.177989][-3.8681061 -4.3590956 -4.5960226 -4.6239166 -4.5688062 -4.3459845 -3.9628172 -4.0776267 -4.3099933 -4.6001673 -4.7942891 -5.1684189 -5.4466972 -4.9465747 -4.0989337][-4.2637329 -4.9534221 -5.2584562 -4.96901 -4.4061556 -3.6342697 -2.9744127 -2.4630346 -2.2870109 -2.8277421 -3.6792276 -4.3143086 -4.6074929 -4.6313391 -4.0939794][-4.5040817 -4.87535 -4.8801131 -4.2730103 -3.1343827 -1.6609993 -0.39495134 0.21289206 0.23264933 -0.28987551 -1.5151331 -3.1251936 -4.2033882 -4.2427526 -3.9189143][-4.5768075 -4.8642292 -4.5013981 -3.5782638 -2.0753362 -0.10239363 1.6716514 2.647984 2.6253915 1.6102514 0.070720673 -1.8644671 -3.4761608 -4.0677891 -3.8572145][-3.831785 -4.063365 -3.5546703 -2.3367462 -0.62931132 1.210896 2.9504566 3.9099627 3.7397947 2.5549755 0.574873 -1.4796407 -3.0173411 -3.8658788 -3.8451307][-3.2323346 -3.0519295 -2.4904792 -1.2727902 0.57494879 2.3752227 3.656127 3.9927549 3.4496088 2.0892491 0.20780563 -1.8368714 -3.3595061 -4.1393251 -4.2248368][-3.1010747 -2.8102951 -1.9718382 -0.77154565 0.5375905 1.9831901 2.7963433 2.5537038 1.5890069 0.33002949 -1.2839053 -2.8802228 -4.2537293 -4.8373117 -4.7419896][-3.31842 -2.9029019 -2.2328131 -1.0112524 0.0060381889 0.68955517 0.9796052 0.47609472 -0.5066247 -1.8596525 -3.0974498 -4.0951052 -4.8571134 -5.2191 -5.0812211][-4.0516033 -3.4983339 -2.749424 -1.7540276 -0.84409261 -0.49810696 -0.88250852 -1.505481 -2.4366903 -3.3695898 -4.195817 -4.8228216 -5.1815252 -5.1500897 -4.7362418][-4.619463 -4.0569124 -3.1920152 -2.4197772 -1.8526711 -1.6903663 -2.10566 -2.9710345 -3.9641898 -4.7090454 -5.2564082 -5.4284067 -5.1792011 -4.7226796 -4.2613759][-5.3581281 -4.5760307 -3.6574988 -2.5584221 -1.9235814 -2.1011257 -2.6974545 -3.4681549 -4.4970484 -5.3360424 -5.6164308 -5.4967189 -5.1830864 -4.433248 -3.6785982][-5.5128846 -4.6502748 -3.6375146 -2.5667567 -1.6922853 -1.4801776 -2.0160193 -2.9910438 -3.9550827 -4.8991852 -5.6238217 -5.3888388 -4.611608 -3.9172487 -3.3442061][-4.9700556 -4.1920404 -3.1225321 -1.918036 -0.96887469 -0.52345419 -0.78086758 -1.8722205 -3.2431395 -4.3239422 -4.8472857 -4.9634247 -4.5446997 -3.5571113 -2.8255553]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-32500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-32500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 06:13:55.901053: step 32510, loss = 0.31, batch loss = 0.25 (26.3 examples/sec; 0.304 sec/batch; 25h:21m:57s remains)
INFO - root - 2017-12-16 06:13:58.670696: step 32520, loss = 0.26, batch loss = 0.20 (29.4 examples/sec; 0.272 sec/batch; 22h:38m:19s remains)
INFO - root - 2017-12-16 06:14:01.577193: step 32530, loss = 0.26, batch loss = 0.20 (27.8 examples/sec; 0.288 sec/batch; 24h:00m:27s remains)
INFO - root - 2017-12-16 06:14:04.367563: step 32540, loss = 0.24, batch loss = 0.18 (28.5 examples/sec; 0.281 sec/batch; 23h:22m:22s remains)
INFO - root - 2017-12-16 06:14:07.231525: step 32550, loss = 0.28, batch loss = 0.22 (29.6 examples/sec; 0.270 sec/batch; 22h:29m:38s remains)
INFO - root - 2017-12-16 06:14:10.100534: step 32560, loss = 0.45, batch loss = 0.39 (28.6 examples/sec; 0.280 sec/batch; 23h:18m:24s remains)
INFO - root - 2017-12-16 06:14:12.928775: step 32570, loss = 0.21, batch loss = 0.16 (29.6 examples/sec; 0.270 sec/batch; 22h:29m:48s remains)
INFO - root - 2017-12-16 06:14:15.761043: step 32580, loss = 0.29, batch loss = 0.23 (28.3 examples/sec; 0.283 sec/batch; 23h:32m:17s remains)
INFO - root - 2017-12-16 06:14:18.597457: step 32590, loss = 0.24, batch loss = 0.18 (28.7 examples/sec; 0.279 sec/batch; 23h:15m:36s remains)
INFO - root - 2017-12-16 06:14:21.437125: step 32600, loss = 0.24, batch loss = 0.18 (26.6 examples/sec; 0.301 sec/batch; 25h:05m:22s remains)
2017-12-16 06:14:21.902884: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8715417 -4.4451814 -4.8857794 -5.2387829 -5.4012442 -5.5168772 -5.6490278 -5.7955122 -5.8978086 -5.9522395 -5.7296481 -5.5424557 -5.2970004 -4.8318787 -4.3455958][-4.558558 -5.2612915 -5.701036 -5.9970837 -6.0690403 -6.3962126 -6.6711512 -6.8809528 -7.2602363 -7.5446138 -7.7087197 -7.6267185 -7.245575 -6.7006922 -6.0511312][-5.1685314 -6.0423141 -6.4667606 -6.5137157 -6.3438439 -6.3283048 -6.3155718 -6.8097649 -7.3896894 -7.9596562 -8.5406837 -8.9579754 -9.03153 -8.6300011 -7.8481951][-5.4617596 -6.2324471 -6.4681683 -6.276547 -5.6975 -5.1399088 -4.8198414 -4.9988794 -5.47299 -6.6254525 -7.908463 -8.9169722 -9.5693932 -9.6981335 -9.2135286][-5.417388 -6.0012112 -5.8175473 -5.0204449 -3.8052006 -2.542438 -1.7271912 -1.5043459 -2.0086331 -3.3286362 -5.1384997 -7.12938 -8.8336878 -9.7586594 -9.8191242][-5.0253811 -5.1602902 -4.5973039 -3.3790779 -1.4423971 0.59158468 2.1094642 2.8693891 2.6225781 1.0694284 -1.2764542 -4.014833 -6.4597692 -8.2376022 -9.1087][-4.4745936 -4.3404584 -3.5056918 -2.0119781 0.05450058 2.4961386 4.7444668 5.8298969 5.798852 4.6583261 2.4161024 -0.67172289 -3.7801557 -6.1380463 -7.4509025][-3.9633703 -3.7067783 -2.9993186 -1.5363011 0.715889 3.1833329 5.3056459 6.7085838 7.1279764 6.1674414 4.0380621 1.4038644 -1.4380386 -4.0593648 -5.7875695][-3.3767724 -3.2573364 -2.7199979 -1.4426873 0.27722168 2.498961 4.6526613 5.9298344 6.3567715 5.9822273 4.5342932 2.1833258 -0.51882672 -2.9185605 -4.5601735][-3.2667513 -3.3416162 -3.189301 -2.4533887 -1.2768049 0.38681746 1.9694333 3.2123389 3.9942932 3.8430147 2.8119645 1.1541686 -0.8050127 -2.8548372 -4.47463][-3.9338784 -4.0963969 -4.0634336 -3.7724 -3.2983594 -2.4197435 -1.491385 -0.43133116 0.18442154 0.31588125 -0.053992271 -1.1057029 -2.5483067 -4.0340528 -5.2224708][-4.60211 -4.8428812 -4.9699039 -5.0298719 -4.9725013 -4.659018 -4.2127051 -3.6066113 -3.1213918 -2.8476448 -3.1015608 -3.8186295 -4.6471691 -5.5289378 -6.2343121][-4.9076934 -5.3328385 -5.7815962 -5.946383 -6.0260935 -6.0750079 -6.0043926 -5.8005991 -5.4391422 -5.1948376 -5.1963511 -5.44567 -5.9083576 -6.5370655 -6.9692125][-4.7976637 -5.2138491 -5.6826134 -6.0991068 -6.5089588 -6.6475964 -6.634654 -6.6685853 -6.6613512 -6.5153651 -6.2900543 -6.2457294 -6.3733783 -6.4883184 -6.5029583][-4.5237355 -4.7079182 -4.9336677 -5.3875828 -5.8587103 -6.125555 -6.3240666 -6.2863116 -6.2196274 -6.1902094 -6.1229362 -5.8949475 -5.620996 -5.6092992 -5.521841]]...]
INFO - root - 2017-12-16 06:14:24.719698: step 32610, loss = 0.24, batch loss = 0.18 (27.6 examples/sec; 0.290 sec/batch; 24h:07m:18s remains)
INFO - root - 2017-12-16 06:14:27.517735: step 32620, loss = 0.23, batch loss = 0.17 (29.0 examples/sec; 0.276 sec/batch; 22h:58m:29s remains)
INFO - root - 2017-12-16 06:14:30.394025: step 32630, loss = 0.22, batch loss = 0.16 (28.1 examples/sec; 0.285 sec/batch; 23h:42m:30s remains)
INFO - root - 2017-12-16 06:14:33.211720: step 32640, loss = 0.73, batch loss = 0.67 (25.6 examples/sec; 0.312 sec/batch; 25h:59m:47s remains)
INFO - root - 2017-12-16 06:14:36.024172: step 32650, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.281 sec/batch; 23h:26m:41s remains)
INFO - root - 2017-12-16 06:14:38.841851: step 32660, loss = 0.25, batch loss = 0.19 (28.5 examples/sec; 0.281 sec/batch; 23h:23m:21s remains)
INFO - root - 2017-12-16 06:14:41.632380: step 32670, loss = 0.33, batch loss = 0.27 (29.2 examples/sec; 0.274 sec/batch; 22h:50m:03s remains)
INFO - root - 2017-12-16 06:14:44.461504: step 32680, loss = 0.37, batch loss = 0.31 (29.1 examples/sec; 0.275 sec/batch; 22h:55m:53s remains)
INFO - root - 2017-12-16 06:14:47.263972: step 32690, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.281 sec/batch; 23h:26m:14s remains)
INFO - root - 2017-12-16 06:14:50.115201: step 32700, loss = 0.23, batch loss = 0.17 (29.2 examples/sec; 0.274 sec/batch; 22h:51m:05s remains)
2017-12-16 06:14:50.570384: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4274402 -3.4895275 -3.3278718 -3.0920353 -2.8557649 -2.9467793 -3.217679 -3.9997354 -4.8873572 -5.763248 -6.4702959 -6.8423886 -7.0418744 -6.5983658 -5.8690948][-2.4630077 -2.5728812 -2.3178275 -2.1276298 -2.0064323 -2.2485013 -2.668704 -3.488205 -4.1878319 -5.1322374 -6.10402 -6.5080972 -6.731884 -6.6046858 -6.2064295][-1.8297369 -1.7971106 -1.4888992 -1.2640672 -1.1597698 -1.325974 -1.6259034 -2.5400496 -3.4836671 -4.2739382 -4.9295444 -5.5447712 -6.1372237 -5.9510484 -5.4752083][-1.2917967 -1.1053107 -0.52896667 -0.0822649 0.22198391 -0.024156094 -0.56866 -1.4031091 -2.0268743 -2.7558923 -3.5487204 -4.0859709 -4.38279 -4.4526749 -4.4124274][-1.3394423 -0.80125189 -0.054893494 0.42965508 0.73779774 0.66635752 0.24037075 -0.42683887 -1.0048954 -1.4916923 -1.7522211 -2.1088006 -2.4513388 -2.5448523 -2.4710052][-1.868798 -1.1378591 -0.27609253 0.39787102 0.94838715 0.94068813 0.67324877 0.21699142 -0.087702274 -0.21580458 -0.25055695 -0.40498924 -0.51200724 -0.66491103 -0.906065][-2.6815968 -1.6540844 -0.47880983 0.43154478 1.1035938 1.1353507 0.9508028 0.61810446 0.44619513 0.44612074 0.56925678 0.61081839 0.56180668 0.3054657 0.066486359][-3.4372289 -2.3186915 -1.231133 -0.19214296 0.63519812 0.94913816 0.9750514 0.69749641 0.57664537 0.66611433 0.90983772 1.0642352 1.091197 0.93218851 0.68144321][-4.4708071 -3.3985772 -2.2637112 -1.2457674 -0.43459582 -0.083052635 0.10710764 0.046927452 0.07989645 0.1735692 0.37331247 0.58619308 0.71814156 0.7232914 0.60897923][-5.1103697 -4.172368 -3.2643905 -2.3538728 -1.5862024 -1.221298 -1.0255895 -1.0117908 -0.93922639 -0.82938719 -0.66625762 -0.49912047 -0.39392424 -0.36065435 -0.32593966][-5.3719945 -4.7874389 -4.1833577 -3.4900126 -2.8100548 -2.4267373 -2.2476511 -2.273164 -2.2737522 -2.3386414 -2.3287365 -2.3113258 -2.2694905 -2.1664126 -1.866488][-5.603096 -5.3686447 -5.1439552 -4.7027707 -4.2154427 -3.8031583 -3.4906301 -3.4766026 -3.5718615 -3.7043359 -3.8085327 -3.9362285 -4.0983019 -4.0275679 -3.4832442][-5.2597055 -5.2866597 -5.2488465 -5.0633893 -4.8674436 -4.6267681 -4.44788 -4.3372679 -4.2870922 -4.583992 -4.9883637 -5.2009211 -5.3083563 -5.4782414 -5.0252895][-4.1709056 -4.3214211 -4.5591617 -4.5035324 -4.392405 -4.2699461 -4.2926393 -4.4710088 -4.8202238 -5.1736412 -5.4985132 -5.8922906 -6.2612071 -6.4703655 -6.0358715][-3.1424499 -3.2011085 -3.471221 -3.6635578 -3.8660016 -3.874244 -3.9590833 -4.0758715 -4.3402786 -4.8750973 -5.5494089 -6.0288582 -6.3126974 -6.6342697 -6.2989273]]...]
INFO - root - 2017-12-16 06:14:53.402906: step 32710, loss = 0.43, batch loss = 0.37 (27.5 examples/sec; 0.291 sec/batch; 24h:16m:04s remains)
INFO - root - 2017-12-16 06:14:56.218366: step 32720, loss = 0.27, batch loss = 0.21 (27.3 examples/sec; 0.293 sec/batch; 24h:22m:42s remains)
INFO - root - 2017-12-16 06:14:59.057385: step 32730, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.284 sec/batch; 23h:39m:22s remains)
INFO - root - 2017-12-16 06:15:01.935573: step 32740, loss = 0.22, batch loss = 0.17 (27.9 examples/sec; 0.286 sec/batch; 23h:50m:53s remains)
INFO - root - 2017-12-16 06:15:04.812976: step 32750, loss = 0.24, batch loss = 0.18 (26.3 examples/sec; 0.305 sec/batch; 25h:21m:19s remains)
INFO - root - 2017-12-16 06:15:07.638920: step 32760, loss = 0.22, batch loss = 0.16 (29.7 examples/sec; 0.270 sec/batch; 22h:27m:43s remains)
INFO - root - 2017-12-16 06:15:10.491214: step 32770, loss = 0.31, batch loss = 0.25 (28.7 examples/sec; 0.279 sec/batch; 23h:14m:28s remains)
INFO - root - 2017-12-16 06:15:13.348855: step 32780, loss = 0.29, batch loss = 0.24 (27.8 examples/sec; 0.288 sec/batch; 23h:59m:21s remains)
INFO - root - 2017-12-16 06:15:16.181510: step 32790, loss = 0.35, batch loss = 0.29 (28.0 examples/sec; 0.286 sec/batch; 23h:49m:20s remains)
INFO - root - 2017-12-16 06:15:18.995236: step 32800, loss = 0.36, batch loss = 0.30 (28.6 examples/sec; 0.280 sec/batch; 23h:18m:42s remains)
2017-12-16 06:15:19.464593: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6566761 -2.6938605 -2.6974959 -2.7266617 -2.7875962 -2.8707075 -2.9337611 -2.9318111 -2.9469271 -2.8097954 -2.6456923 -2.6184275 -2.5597367 -2.5816984 -2.5261698][-2.5310535 -2.4694424 -2.4810162 -2.6354659 -2.7502408 -2.7418427 -2.7430491 -2.7745497 -2.831193 -2.7764883 -2.7059639 -2.6491914 -2.4972634 -2.4812095 -2.3876619][-2.44734 -2.2634938 -2.1611488 -2.2705474 -2.4158018 -2.432888 -2.4470718 -2.3959622 -2.4247477 -2.5170856 -2.5915642 -2.6672134 -2.7310405 -2.7261677 -2.5369825][-2.3146136 -2.1767635 -2.1221807 -2.1274769 -2.12066 -2.0512264 -1.9108093 -1.7024119 -1.7852366 -1.9633367 -2.1111302 -2.3646016 -2.679316 -2.8717918 -2.8657808][-2.2599127 -1.9347601 -1.5948749 -1.4852171 -1.3928015 -1.2177796 -0.96166921 -0.86205673 -0.99424481 -1.0797362 -1.2002449 -1.5427122 -2.057493 -2.5524096 -2.765264][-2.1398187 -1.6537452 -1.1607473 -0.85747814 -0.46368575 -0.031678677 0.39546394 0.62509155 0.61055183 0.48389339 0.0351696 -0.47444654 -1.2185662 -1.9216576 -2.3028579][-1.781467 -1.3615375 -0.91441178 -0.35134459 0.36915207 0.92184734 1.5706182 1.9211755 1.9740586 1.735641 1.1313944 0.41697264 -0.66601729 -1.4638832 -1.9098911][-1.8140485 -1.4375472 -0.92112565 -0.2578001 0.62138367 1.3455915 2.0301275 2.1513214 2.107142 1.7908154 1.1097422 0.34647465 -0.74544692 -1.6074066 -2.233732][-1.9245124 -1.5069852 -0.95618439 -0.43529439 0.20727444 0.81919193 1.3134236 1.381145 1.2400994 0.84489822 0.200737 -0.57818747 -1.6684194 -2.4109981 -2.8449421][-2.2690012 -1.8204007 -1.3297353 -0.91262007 -0.41910696 -0.16639662 -0.1246357 -0.45633483 -0.79712629 -1.0830948 -1.5614369 -2.1785719 -2.9014826 -3.2229528 -3.34881][-2.5163722 -2.1548305 -1.722656 -1.3797898 -1.1898894 -1.2895081 -1.4217503 -2.1752083 -2.9030035 -3.4273732 -3.9380839 -4.2871828 -4.5434256 -4.5182524 -4.3345323][-2.7564759 -2.3637657 -2.0873735 -1.9463558 -2.1227219 -2.4448028 -2.9503856 -3.9555848 -4.8286958 -5.4558616 -5.8679614 -6.0156741 -6.0033145 -5.7240644 -5.333674][-2.8644633 -2.4920282 -2.2302527 -2.1500247 -2.455842 -2.9649172 -3.6913061 -4.6993418 -5.5929766 -6.2240214 -6.5168762 -6.5304985 -6.3903561 -6.0611458 -5.673224][-2.8728633 -2.4396658 -2.0930126 -1.9660037 -2.2527952 -2.8050532 -3.4547706 -4.3171225 -5.085546 -5.5499439 -5.7662 -5.8432479 -5.8700981 -5.708003 -5.6107965][-2.871666 -2.368618 -2.0037425 -1.8454573 -2.0789165 -2.6477838 -3.2833071 -3.9937565 -4.555553 -4.875946 -4.9560528 -4.9356365 -4.9461875 -4.9453769 -5.1115432]]...]
INFO - root - 2017-12-16 06:15:22.278209: step 32810, loss = 0.21, batch loss = 0.15 (27.8 examples/sec; 0.288 sec/batch; 23h:58m:39s remains)
INFO - root - 2017-12-16 06:15:25.122408: step 32820, loss = 0.25, batch loss = 0.19 (29.3 examples/sec; 0.273 sec/batch; 22h:43m:53s remains)
INFO - root - 2017-12-16 06:15:27.982485: step 32830, loss = 0.25, batch loss = 0.19 (29.2 examples/sec; 0.274 sec/batch; 22h:47m:11s remains)
INFO - root - 2017-12-16 06:15:30.811884: step 32840, loss = 0.28, batch loss = 0.22 (28.9 examples/sec; 0.277 sec/batch; 23h:04m:42s remains)
INFO - root - 2017-12-16 06:15:33.652869: step 32850, loss = 0.32, batch loss = 0.26 (28.9 examples/sec; 0.277 sec/batch; 23h:03m:56s remains)
INFO - root - 2017-12-16 06:15:36.460093: step 32860, loss = 0.28, batch loss = 0.22 (28.6 examples/sec; 0.280 sec/batch; 23h:17m:43s remains)
INFO - root - 2017-12-16 06:15:39.308635: step 32870, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.282 sec/batch; 23h:30m:31s remains)
INFO - root - 2017-12-16 06:15:42.203175: step 32880, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.284 sec/batch; 23h:38m:04s remains)
INFO - root - 2017-12-16 06:15:45.057225: step 32890, loss = 0.24, batch loss = 0.19 (29.0 examples/sec; 0.276 sec/batch; 22h:58m:30s remains)
INFO - root - 2017-12-16 06:15:47.925472: step 32900, loss = 0.27, batch loss = 0.21 (27.5 examples/sec; 0.291 sec/batch; 24h:11m:30s remains)
2017-12-16 06:15:48.392986: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.398634 -4.2440872 -4.05097 -3.9719424 -4.0672078 -4.1326761 -4.3079767 -4.6479659 -4.8563709 -4.758347 -4.4504619 -4.0942349 -3.6134317 -3.0719037 -2.7637291][-4.1113162 -4.0185056 -3.8444579 -3.79054 -3.8946655 -4.1947551 -4.5438128 -4.9115453 -5.1916733 -5.1243038 -4.7277231 -4.1268744 -3.5310063 -3.0145545 -2.6983366][-3.5433993 -3.3799672 -3.1101842 -2.9921198 -3.201674 -3.6328926 -4.1550093 -4.6374245 -4.905633 -4.8424106 -4.5756612 -4.1772542 -3.7038383 -3.1661191 -2.7652411][-2.5892177 -2.2249436 -1.8302631 -1.7300804 -1.9352567 -2.3205438 -2.7874706 -3.30126 -3.6828575 -3.8082914 -3.7696264 -3.7044878 -3.5774264 -3.2572458 -2.9382439][-1.597542 -0.92285466 -0.2294488 0.049320221 0.062321186 -0.28820419 -0.74125814 -1.2517891 -1.802608 -2.3365378 -2.9253683 -3.3529646 -3.571743 -3.4661341 -3.1183829][-0.61136317 0.197793 1.0393877 1.6583176 1.948452 1.7013721 1.3405466 0.76843119 -0.0030508041 -0.96505165 -2.0769105 -3.0219295 -3.5629172 -3.6332352 -3.3872759][-0.29463434 0.66977882 1.5080032 2.1334844 2.5873718 2.595849 2.3646307 1.8511524 1.0670094 -0.068065166 -1.4196291 -2.6740851 -3.4828169 -3.6683698 -3.4654007][-0.53235579 0.19718266 0.93273687 1.7948885 2.4075747 2.4643683 2.2195888 1.7179012 0.98069334 -0.19708824 -1.5950191 -2.7904458 -3.5603137 -3.8242919 -3.66564][-1.432225 -0.73539853 -0.12377357 0.52657223 1.0631428 1.2890468 1.1633272 0.69031811 0.10332012 -1.0296977 -2.388552 -3.4283767 -4.021234 -4.1547909 -3.9755685][-2.2890365 -1.893368 -1.582633 -1.0815351 -0.62122154 -0.45020652 -0.54007459 -0.90512872 -1.3971999 -2.1247766 -2.9658086 -3.7590079 -4.2758889 -4.3175945 -4.0986376][-2.8827446 -2.7881534 -2.7724981 -2.5560832 -2.2991307 -2.270057 -2.3365667 -2.3667316 -2.4096637 -2.8934062 -3.5053422 -3.9843273 -4.3403964 -4.3171072 -4.0815334][-3.231092 -3.2806625 -3.4776278 -3.6088626 -3.6203976 -3.445544 -3.2719972 -3.2590399 -3.1609206 -3.2878807 -3.6177418 -3.9233208 -4.18013 -4.1094527 -3.9281683][-2.8538027 -2.8508489 -3.2311668 -3.4675121 -3.4936807 -3.4611695 -3.2524924 -3.1503315 -2.9911819 -3.0284023 -3.2038157 -3.4746184 -3.8174815 -3.7673995 -3.5849724][-1.971621 -1.9417505 -2.3270094 -2.7643495 -2.85731 -2.8754315 -2.6580992 -2.4866505 -2.2506795 -2.2502248 -2.593143 -2.9711981 -3.3469718 -3.4579933 -3.3470831][-1.2429254 -1.1284862 -1.5556707 -2.1168606 -2.3331106 -2.2419353 -1.8122904 -1.6002028 -1.5090852 -1.6089239 -2.0377727 -2.6220577 -3.0933533 -3.2397614 -3.1656246]]...]
INFO - root - 2017-12-16 06:15:51.190003: step 32910, loss = 0.30, batch loss = 0.24 (29.8 examples/sec; 0.268 sec/batch; 22h:18m:44s remains)
INFO - root - 2017-12-16 06:15:54.022124: step 32920, loss = 0.26, batch loss = 0.20 (28.4 examples/sec; 0.281 sec/batch; 23h:25m:00s remains)
INFO - root - 2017-12-16 06:15:56.864289: step 32930, loss = 0.40, batch loss = 0.34 (28.1 examples/sec; 0.284 sec/batch; 23h:39m:48s remains)
INFO - root - 2017-12-16 06:15:59.673614: step 32940, loss = 0.30, batch loss = 0.24 (28.6 examples/sec; 0.280 sec/batch; 23h:18m:02s remains)
INFO - root - 2017-12-16 06:16:02.517668: step 32950, loss = 0.28, batch loss = 0.22 (27.1 examples/sec; 0.295 sec/batch; 24h:33m:05s remains)
INFO - root - 2017-12-16 06:16:05.351421: step 32960, loss = 0.29, batch loss = 0.23 (28.6 examples/sec; 0.280 sec/batch; 23h:15m:28s remains)
INFO - root - 2017-12-16 06:16:08.229898: step 32970, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 23h:45m:48s remains)
INFO - root - 2017-12-16 06:16:11.091917: step 32980, loss = 0.28, batch loss = 0.22 (27.8 examples/sec; 0.288 sec/batch; 23h:58m:54s remains)
INFO - root - 2017-12-16 06:16:13.933309: step 32990, loss = 0.24, batch loss = 0.18 (27.7 examples/sec; 0.289 sec/batch; 24h:01m:20s remains)
INFO - root - 2017-12-16 06:16:16.760024: step 33000, loss = 0.23, batch loss = 0.18 (29.8 examples/sec; 0.268 sec/batch; 22h:20m:01s remains)
2017-12-16 06:16:17.220736: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5269952 -4.56308 -4.5012612 -4.2507114 -4.0354629 -3.9449103 -3.9312816 -3.9980035 -4.1232247 -4.198597 -4.1386337 -3.8949018 -3.492837 -2.9053333 -2.5756741][-3.263793 -3.3206859 -3.2797298 -3.1879723 -3.302834 -3.5643663 -3.680598 -3.6614008 -3.5457597 -3.5426979 -3.4308996 -2.9100871 -2.3397245 -1.9021797 -1.7977843][-2.7348528 -2.4252234 -2.2871604 -2.2924218 -2.4718902 -2.9096689 -3.17038 -3.1534505 -2.8570602 -2.5975261 -2.3915606 -2.1043897 -1.8315241 -1.6294007 -1.5826461][-1.7644563 -1.3428469 -1.3062227 -1.7619712 -2.2635331 -2.5712934 -2.6243682 -2.3032289 -1.6327803 -1.2059193 -1.0800042 -1.1390977 -1.3253617 -1.3569775 -1.3040695][-0.67643356 -0.26318407 -0.26214266 -0.86651039 -1.5711515 -1.8513095 -1.5307093 -1.0500176 -0.46710563 0.0036802292 -0.12182331 -0.43776917 -0.78207922 -1.1477039 -1.366653][-0.42496252 -0.099748135 -0.1186142 -0.58097839 -1.0772653 -1.0953479 -0.417624 0.59066391 1.2409549 1.2710371 0.69131136 -0.14362192 -1.0024953 -1.4331193 -1.4683273][-1.1751366 -0.92652988 -0.88232732 -0.96622682 -0.74581981 -0.18352365 0.60193682 1.4016037 1.9350467 1.7527547 0.7695899 -0.33540964 -1.215174 -1.8754442 -2.2283616][-1.7168479 -1.6599352 -1.9037712 -1.8247607 -1.3510847 -0.30426073 1.0951629 1.8852973 1.9449453 1.3688788 0.25479507 -1.0319891 -2.030493 -2.5700243 -2.6739292][-2.5165763 -2.6638412 -2.6423504 -2.4496703 -1.740773 -0.36546803 0.91102791 1.6224265 1.6434755 0.79842854 -0.62871933 -1.9567881 -2.83618 -3.2422771 -3.2341275][-2.9624004 -3.5435951 -3.4777331 -2.7736053 -1.8593857 -0.97604918 0.052217484 0.88886309 0.62487459 -0.47672009 -1.7993889 -2.9663956 -3.7244658 -3.9452431 -3.6603642][-3.4845822 -4.1516714 -4.3409538 -3.8151257 -2.7217638 -1.6488254 -1.0385447 -0.80356145 -1.1478274 -2.16557 -3.3790503 -4.1545391 -4.3829274 -4.1447091 -3.558584][-4.0167947 -4.591351 -4.7265391 -4.1881018 -3.3541589 -2.4742444 -1.7979636 -1.8263631 -2.6085205 -3.6224372 -4.42047 -4.9425354 -4.9795742 -4.5389576 -3.7527273][-4.0052085 -4.4708476 -4.6359725 -4.0514641 -3.3705506 -2.6999459 -2.325495 -2.4681306 -3.1379781 -4.2188978 -4.8347874 -4.9188251 -4.6088133 -4.0907249 -3.403687][-3.8102944 -4.2263155 -4.2758636 -3.7651281 -3.1698546 -2.7021067 -2.6708217 -3.0613365 -3.6907716 -4.4125924 -4.7277017 -4.6796389 -4.28709 -3.7200058 -3.1776402][-3.6814365 -3.85877 -3.762013 -3.3425443 -2.8970551 -2.5198488 -2.5389242 -3.0361021 -3.6780803 -4.28224 -4.5202146 -4.3262782 -3.913728 -3.5058622 -3.2311714]]...]
INFO - root - 2017-12-16 06:16:20.048005: step 33010, loss = 0.24, batch loss = 0.18 (29.5 examples/sec; 0.271 sec/batch; 22h:33m:17s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 06:16:22.911788: step 33020, loss = 0.26, batch loss = 0.20 (26.1 examples/sec; 0.307 sec/batch; 25h:32m:35s remains)
INFO - root - 2017-12-16 06:16:25.747544: step 33030, loss = 0.26, batch loss = 0.20 (29.6 examples/sec; 0.270 sec/batch; 22h:27m:30s remains)
INFO - root - 2017-12-16 06:16:28.569356: step 33040, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.286 sec/batch; 23h:47m:06s remains)
INFO - root - 2017-12-16 06:16:31.396024: step 33050, loss = 0.28, batch loss = 0.22 (28.9 examples/sec; 0.276 sec/batch; 22h:59m:19s remains)
INFO - root - 2017-12-16 06:16:34.219371: step 33060, loss = 0.22, batch loss = 0.16 (29.2 examples/sec; 0.274 sec/batch; 22h:46m:47s remains)
INFO - root - 2017-12-16 06:16:37.066877: step 33070, loss = 0.24, batch loss = 0.18 (29.7 examples/sec; 0.269 sec/batch; 22h:24m:55s remains)
INFO - root - 2017-12-16 06:16:39.949952: step 33080, loss = 0.27, batch loss = 0.21 (27.2 examples/sec; 0.294 sec/batch; 24h:26m:46s remains)
INFO - root - 2017-12-16 06:16:42.800990: step 33090, loss = 0.28, batch loss = 0.23 (29.0 examples/sec; 0.276 sec/batch; 22h:57m:51s remains)
INFO - root - 2017-12-16 06:16:45.632539: step 33100, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 23h:26m:16s remains)
2017-12-16 06:16:46.099351: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8383749 -4.210464 -4.702354 -5.1757364 -5.1805897 -5.3107104 -5.3373165 -5.3685932 -5.3280878 -5.1099372 -5.0040913 -4.8450236 -4.7628756 -4.7017035 -4.6266665][-3.8139925 -4.295392 -4.6468258 -4.878582 -4.9063482 -5.2157683 -5.348299 -5.2261395 -4.9826488 -4.9060035 -4.8794713 -4.4702053 -4.148252 -3.7463632 -3.480376][-3.8116422 -3.980952 -4.0658317 -4.1879935 -4.2511578 -4.38255 -4.4018674 -4.539196 -4.6109786 -4.6136518 -4.5685754 -4.2220674 -3.935935 -3.5220127 -3.1105239][-2.8620057 -3.0190225 -3.1047082 -2.9532244 -2.5983982 -2.6787884 -2.7718182 -2.9241986 -3.1679804 -3.4845328 -3.748682 -3.5586085 -3.4080536 -3.1673458 -2.849442][-1.7941997 -1.7272952 -1.3942213 -1.1377606 -0.85298514 -0.70385313 -0.713583 -0.91989708 -1.3543558 -1.948137 -2.6109552 -2.8592234 -3.1174886 -3.2077241 -3.0661492][-1.5347886 -1.2081037 -0.7705915 -0.028831959 0.95914745 1.4844594 1.5936003 1.4068007 0.817719 -0.086763859 -1.2409427 -2.2849109 -3.2236257 -3.5583408 -3.5396206][-1.5332179 -1.0972161 -0.6174159 0.5365386 1.8729124 2.8901439 3.593173 3.4299955 2.7800016 1.6694765 0.24246454 -1.1401229 -2.5214815 -3.3376143 -3.7813835][-2.0166898 -1.7972209 -1.3240261 -0.0081782341 1.5168662 3.0305343 4.0092678 4.1039896 3.6168404 2.3464732 0.74626732 -0.71707821 -2.1541557 -3.1210372 -3.8515341][-3.0946836 -3.1252546 -2.7553177 -1.5016448 0.1545186 1.7881374 2.9374628 3.4653955 3.3139834 2.2794523 0.75685453 -0.8785665 -2.3982441 -3.366677 -4.141202][-4.1986523 -4.6186237 -4.7491245 -3.5265598 -1.718075 -0.059625626 1.0875454 1.4876194 1.3760839 0.65385914 -0.48961687 -1.9091151 -3.2037857 -3.85322 -4.296113][-5.5124192 -5.668304 -5.6193433 -4.9052634 -3.69594 -2.2911534 -1.2387938 -0.95018625 -1.0875428 -1.810638 -2.9097855 -3.9653342 -4.61873 -4.8157649 -4.6788173][-5.8142629 -5.98204 -6.2300282 -5.8124342 -4.9388037 -4.1167397 -3.5542426 -3.3854849 -3.3670535 -3.8270166 -4.4986973 -5.1555171 -5.3888431 -5.1773167 -4.6582408][-5.2343659 -5.1843948 -5.4334083 -5.4513354 -5.2554016 -4.98862 -4.772121 -4.7991471 -4.8167677 -5.0161452 -5.2311172 -5.551053 -5.6137276 -5.0656314 -4.4015136][-4.6067009 -4.2992544 -4.2851839 -4.5371447 -4.7090139 -4.7602186 -4.82579 -5.0227871 -5.1146574 -5.089612 -5.0067744 -4.9352217 -4.6862712 -4.0566211 -3.4997694][-4.2319221 -3.7057934 -3.5005705 -3.6965826 -4.0581579 -4.5268745 -4.8713784 -4.8767653 -4.7237296 -4.6063433 -4.4301229 -4.1098347 -3.6859183 -3.0553846 -2.5054629]]...]
INFO - root - 2017-12-16 06:16:48.958438: step 33110, loss = 0.22, batch loss = 0.16 (27.8 examples/sec; 0.288 sec/batch; 23h:55m:21s remains)
INFO - root - 2017-12-16 06:16:51.757774: step 33120, loss = 0.31, batch loss = 0.25 (28.9 examples/sec; 0.277 sec/batch; 23h:00m:49s remains)
INFO - root - 2017-12-16 06:16:54.605562: step 33130, loss = 0.25, batch loss = 0.20 (29.3 examples/sec; 0.273 sec/batch; 22h:42m:31s remains)
INFO - root - 2017-12-16 06:16:57.438327: step 33140, loss = 0.25, batch loss = 0.20 (28.2 examples/sec; 0.283 sec/batch; 23h:32m:56s remains)
INFO - root - 2017-12-16 06:17:00.287234: step 33150, loss = 0.31, batch loss = 0.25 (28.0 examples/sec; 0.286 sec/batch; 23h:45m:37s remains)
INFO - root - 2017-12-16 06:17:03.089604: step 33160, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.283 sec/batch; 23h:33m:52s remains)
INFO - root - 2017-12-16 06:17:05.867858: step 33170, loss = 0.25, batch loss = 0.20 (29.1 examples/sec; 0.275 sec/batch; 22h:50m:05s remains)
INFO - root - 2017-12-16 06:17:08.737994: step 33180, loss = 0.32, batch loss = 0.26 (29.0 examples/sec; 0.276 sec/batch; 22h:54m:27s remains)
INFO - root - 2017-12-16 06:17:11.529188: step 33190, loss = 0.34, batch loss = 0.28 (29.7 examples/sec; 0.269 sec/batch; 22h:22m:21s remains)
INFO - root - 2017-12-16 06:17:14.374674: step 33200, loss = 0.20, batch loss = 0.14 (28.9 examples/sec; 0.277 sec/batch; 22h:59m:24s remains)
2017-12-16 06:17:14.845280: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9485552 -2.0130715 -2.1831243 -2.4943128 -2.7522488 -2.8321838 -2.8623095 -2.9555807 -3.3039005 -3.71004 -3.9245136 -3.9682157 -3.6977494 -3.3235948 -2.8337941][-1.4320033 -1.6541262 -1.9259701 -2.1091902 -2.2916868 -2.4337082 -2.6472626 -2.7408164 -3.0102658 -3.4298668 -3.6839952 -3.7948632 -3.6393228 -3.256227 -2.7252903][-1.7922297 -2.0725138 -2.2867224 -2.2794108 -2.1729705 -1.9493592 -1.9604816 -1.9820411 -2.2134764 -2.7161839 -3.1532693 -3.4587684 -3.5033765 -3.3042932 -2.8486242][-2.5188317 -2.8945642 -3.0295944 -2.7202675 -2.2548676 -1.6818082 -1.2495315 -1.0287805 -1.111413 -1.6968932 -2.3374231 -2.9623775 -3.4311967 -3.4769692 -3.19978][-3.1832836 -3.4158564 -3.2561555 -2.5397072 -1.6575487 -0.72369051 -0.048308849 0.37933159 0.29251957 -0.43528771 -1.4121361 -2.4609854 -3.2810311 -3.6856503 -3.7369761][-3.4651461 -3.4800148 -2.9919519 -1.8913522 -0.65884852 0.59807682 1.5583048 2.1103463 2.0529046 1.1080537 -0.26659489 -1.8356285 -3.1944184 -3.8869309 -4.05867][-3.5524812 -3.3045573 -2.6144924 -1.3152318 0.12450504 1.5208693 2.7052341 3.3406563 3.2610974 2.226696 0.61781216 -1.2359996 -2.8785493 -3.9201794 -4.3802085][-3.3827667 -3.0229862 -2.3989615 -1.1698596 0.34749842 1.8826489 3.0789127 3.5904436 3.42238 2.2539449 0.46685934 -1.4929392 -3.1923344 -4.2873316 -4.8242168][-2.9703188 -2.7143903 -2.0992079 -1.1334639 -0.074212074 1.2625413 2.3852239 2.8721557 2.6462088 1.4454756 -0.22427082 -2.0646315 -3.6475604 -4.78194 -5.3458862][-2.737637 -2.7163148 -2.5851088 -1.8463924 -0.856987 0.077866077 0.84555912 1.2510457 1.0806456 0.022763252 -1.3343351 -2.7480896 -3.9710963 -4.8371391 -5.3010559][-2.8185225 -2.8255744 -2.8548102 -2.6957111 -2.1120474 -1.2183223 -0.55561733 -0.53682446 -0.85018039 -1.5278714 -2.4782424 -3.5101025 -4.4071937 -4.9926596 -5.3511381][-2.967001 -3.1145563 -3.2106733 -3.1224434 -2.7534957 -2.2047997 -1.7272263 -1.6747253 -2.0968928 -2.6685648 -3.2981248 -3.9298596 -4.4819088 -4.9419184 -5.2489061][-2.9439597 -3.2014241 -3.3387451 -3.2027507 -2.8808684 -2.5464807 -2.3756516 -2.3425474 -2.4991183 -2.8178995 -3.2822294 -3.6651607 -3.8632498 -4.1497078 -4.4731884][-3.1293468 -3.2056179 -3.11098 -3.0521975 -2.7471771 -2.3053265 -2.1041994 -2.2284818 -2.422755 -2.4647498 -2.6049428 -2.8219573 -2.9970832 -3.1852882 -3.3748055][-2.8411317 -2.8289628 -2.6748514 -2.4956672 -1.9991345 -1.4536004 -1.1635144 -1.1903765 -1.3795984 -1.5029564 -1.7225654 -1.8790581 -1.9574699 -2.131835 -2.332382]]...]
INFO - root - 2017-12-16 06:17:17.633944: step 33210, loss = 0.28, batch loss = 0.23 (28.4 examples/sec; 0.282 sec/batch; 23h:26m:07s remains)
INFO - root - 2017-12-16 06:17:20.434869: step 33220, loss = 0.35, batch loss = 0.29 (29.6 examples/sec; 0.271 sec/batch; 22h:29m:38s remains)
INFO - root - 2017-12-16 06:17:23.281902: step 33230, loss = 0.27, batch loss = 0.21 (27.1 examples/sec; 0.295 sec/batch; 24h:31m:43s remains)
INFO - root - 2017-12-16 06:17:26.088930: step 33240, loss = 0.28, batch loss = 0.22 (25.4 examples/sec; 0.315 sec/batch; 26h:09m:47s remains)
INFO - root - 2017-12-16 06:17:28.975710: step 33250, loss = 0.22, batch loss = 0.17 (28.3 examples/sec; 0.283 sec/batch; 23h:29m:14s remains)
INFO - root - 2017-12-16 06:17:31.802058: step 33260, loss = 0.20, batch loss = 0.15 (29.3 examples/sec; 0.273 sec/batch; 22h:41m:04s remains)
INFO - root - 2017-12-16 06:17:34.612132: step 33270, loss = 0.25, batch loss = 0.19 (29.3 examples/sec; 0.273 sec/batch; 22h:40m:23s remains)
INFO - root - 2017-12-16 06:17:37.425805: step 33280, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.283 sec/batch; 23h:33m:31s remains)
INFO - root - 2017-12-16 06:17:40.268308: step 33290, loss = 0.35, batch loss = 0.29 (28.3 examples/sec; 0.283 sec/batch; 23h:29m:34s remains)
INFO - root - 2017-12-16 06:17:43.073257: step 33300, loss = 0.22, batch loss = 0.16 (28.1 examples/sec; 0.285 sec/batch; 23h:38m:56s remains)
2017-12-16 06:17:43.535094: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3032136 -4.1111436 -3.9109926 -3.774102 -3.7466073 -3.6264341 -3.3882823 -3.2219687 -2.9977756 -2.9133506 -2.8143575 -2.8782353 -3.1536565 -3.4683251 -3.7017236][-4.0905623 -3.8724902 -3.7965894 -3.9208057 -4.1454487 -4.1197777 -3.9077048 -3.63721 -3.2469072 -3.0640678 -2.8461182 -2.9211593 -2.9537537 -2.8089085 -2.7697325][-3.3709385 -3.526392 -3.9064517 -4.3127613 -4.7135968 -4.6461134 -4.3854065 -4.1747875 -3.8799419 -3.5294993 -3.075078 -2.842309 -2.7410269 -2.7172875 -2.4899375][-2.7097979 -3.2231913 -3.9282143 -4.6867895 -5.245543 -5.1209364 -4.6105194 -4.1622276 -3.7361548 -3.3752265 -2.9870787 -3.0150275 -3.0872302 -2.841536 -2.4953618][-1.9937747 -2.7811718 -3.7179048 -4.5773048 -4.8819723 -4.3606834 -3.6172307 -2.9723399 -2.6522698 -2.7089496 -2.7969224 -2.935266 -2.9840264 -3.1745212 -3.2339916][-2.0598185 -2.7985432 -3.4873469 -3.8642683 -3.5494273 -2.7152619 -1.7208962 -0.87350082 -0.5062151 -0.76627326 -1.4269602 -2.439095 -3.2654352 -3.4684806 -3.1870565][-2.3120687 -2.8173242 -3.1812882 -3.1926188 -2.2953274 -0.77456379 0.81736469 1.7692881 1.906877 1.2640443 0.13796377 -1.3032093 -2.4821448 -3.2428279 -3.4624302][-2.4280756 -3.0768843 -3.0712152 -2.4065406 -0.93242764 0.94747066 2.8126678 4.0003004 4.0315485 3.0131369 1.4582248 -0.43450832 -2.1769569 -3.1740494 -3.3725049][-3.1688516 -3.7644961 -3.492353 -2.4997044 -0.94509697 1.1847897 3.260922 4.4503708 4.4972677 3.2254615 1.4065218 -0.47700524 -2.0663445 -3.0109997 -3.3192458][-3.6672552 -4.4150786 -4.70898 -3.7361887 -1.974299 -0.28942776 1.27246 2.3909078 2.6293373 1.7491298 0.26540613 -1.4530051 -2.7141266 -3.1494775 -3.0836177][-4.5655618 -5.3738637 -5.629571 -5.0413666 -3.8021665 -1.9914603 -0.40226889 0.18457794 0.17021179 -0.55527616 -1.5911121 -2.6418347 -3.2902002 -3.370348 -3.1923475][-5.333137 -6.0237122 -6.3698735 -6.106689 -5.2308459 -4.0347843 -3.1311636 -2.6472583 -2.3209357 -2.5827055 -3.0816779 -3.6174803 -3.8722925 -3.7086711 -3.3551791][-5.0120759 -5.7487564 -6.2611785 -6.0890508 -5.4188023 -4.6585064 -3.9912813 -3.9324589 -3.9549844 -3.8602209 -3.6705732 -3.6293597 -3.4871542 -3.1246858 -2.7748775][-4.831852 -5.1945076 -5.2508788 -4.9594011 -4.5531425 -4.0601592 -3.7755804 -3.8008351 -3.8781638 -3.9273028 -3.6380253 -3.334543 -3.0589328 -2.649128 -2.2634847][-4.3066349 -4.3198776 -4.1781 -3.7323589 -3.2758756 -3.0075011 -2.982826 -3.0636914 -3.2658875 -3.4348197 -3.3284378 -3.0514646 -2.785202 -2.6302824 -2.372726]]...]
INFO - root - 2017-12-16 06:17:46.368446: step 33310, loss = 0.24, batch loss = 0.18 (29.3 examples/sec; 0.273 sec/batch; 22h:40m:18s remains)
INFO - root - 2017-12-16 06:17:49.226894: step 33320, loss = 0.23, batch loss = 0.17 (29.1 examples/sec; 0.275 sec/batch; 22h:50m:50s remains)
INFO - root - 2017-12-16 06:17:52.086094: step 33330, loss = 0.25, batch loss = 0.20 (26.8 examples/sec; 0.299 sec/batch; 24h:48m:33s remains)
INFO - root - 2017-12-16 06:17:54.925657: step 33340, loss = 0.23, batch loss = 0.18 (29.1 examples/sec; 0.275 sec/batch; 22h:49m:06s remains)
INFO - root - 2017-12-16 06:17:57.791689: step 33350, loss = 0.24, batch loss = 0.18 (28.7 examples/sec; 0.279 sec/batch; 23h:08m:36s remains)
INFO - root - 2017-12-16 06:18:00.559735: step 33360, loss = 0.24, batch loss = 0.19 (29.2 examples/sec; 0.274 sec/batch; 22h:47m:45s remains)
INFO - root - 2017-12-16 06:18:03.368042: step 33370, loss = 0.25, batch loss = 0.20 (27.9 examples/sec; 0.287 sec/batch; 23h:50m:12s remains)
INFO - root - 2017-12-16 06:18:06.249104: step 33380, loss = 0.28, batch loss = 0.22 (26.8 examples/sec; 0.299 sec/batch; 24h:48m:47s remains)
INFO - root - 2017-12-16 06:18:09.134422: step 33390, loss = 0.24, batch loss = 0.18 (27.7 examples/sec; 0.288 sec/batch; 23h:57m:14s remains)
INFO - root - 2017-12-16 06:18:11.917112: step 33400, loss = 0.33, batch loss = 0.27 (28.2 examples/sec; 0.283 sec/batch; 23h:32m:45s remains)
2017-12-16 06:18:12.346108: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6756558 -2.8416648 -3.201838 -3.5112786 -3.627836 -3.5637927 -3.4664462 -3.2423072 -3.0186462 -2.9655209 -2.9449885 -2.979774 -2.9231699 -2.9467874 -2.8537743][-3.0728383 -3.2528043 -3.6576734 -4.0480127 -4.2344918 -4.0660219 -3.759537 -3.5666113 -3.3685942 -3.1583381 -3.0909579 -3.12692 -2.9400783 -2.8734818 -2.7727172][-4.4752455 -4.3837733 -4.3720031 -4.353003 -4.0038133 -3.484035 -3.109623 -3.0253239 -3.0239623 -3.0301647 -3.0725498 -3.0617862 -2.8926475 -2.5884919 -2.3786893][-6.074234 -5.8748283 -5.1335583 -4.3568134 -3.5355248 -2.6218626 -1.904819 -1.8144636 -2.2093308 -2.613132 -2.7277274 -2.5407386 -2.2340066 -1.9921474 -1.8499024][-6.7426305 -6.2248583 -5.2229033 -3.8041577 -2.3992271 -1.2987168 -0.75458908 -0.83835435 -1.3602853 -2.0125787 -2.337714 -2.1538465 -1.5033047 -0.95841765 -0.8093195][-6.6664667 -5.8393469 -4.5022368 -2.7716351 -0.99265218 0.34478521 0.813252 0.36784077 -0.6027739 -1.2583187 -1.4322507 -1.4455862 -1.0621622 -0.45787573 -0.12427759][-5.7347322 -5.1511912 -3.49292 -1.3672247 0.542161 1.8301725 2.1943741 1.6235704 0.34988594 -0.76315928 -1.3064601 -1.0999675 -0.44544697 0.097716331 0.30105209][-5.2051282 -4.7023549 -2.8043113 -0.43455434 1.4948931 2.6922307 2.8577633 2.0731044 0.69153023 -0.54790926 -1.258796 -1.3372033 -0.97065449 -0.19325924 0.3871336][-4.5182085 -3.9591961 -2.5624108 -0.39487791 1.3569646 2.4431796 2.5511522 1.6736503 0.19386911 -1.0688629 -1.8134806 -2.0617592 -1.9172001 -1.3583543 -0.59617162][-3.69658 -3.1931996 -1.7855532 -0.22616291 1.0286288 1.7874775 1.4946923 0.51305008 -0.96942163 -2.3140712 -3.1406217 -3.389636 -3.2112703 -2.6509116 -1.7338502][-3.2425103 -2.9006467 -1.7732027 -0.39819813 0.43359852 0.6457119 0.22807026 -0.78168058 -2.2829328 -3.5735965 -4.4995885 -4.8577509 -4.6609817 -3.91877 -2.7039385][-4.0551596 -3.360158 -2.0256844 -0.88443351 -0.19265556 -0.29150105 -1.0966585 -2.2506809 -3.5760436 -4.7617617 -5.5456181 -5.8126235 -5.5733147 -4.6328473 -3.1344953][-5.0923886 -4.0840282 -2.7485676 -1.5549691 -0.88574767 -0.91502976 -1.6729603 -2.7870343 -4.1471567 -5.2445307 -5.9333696 -6.2656088 -5.9899173 -5.0923371 -3.6931744][-5.6824155 -4.4544067 -2.8403332 -1.6179328 -1.1094701 -1.2949402 -2.0885029 -3.2117434 -4.4307303 -5.3357434 -5.8499594 -6.0099964 -5.707149 -4.846488 -3.548686][-6.1285467 -4.5394382 -2.8193712 -1.6556232 -1.1918879 -1.4681971 -2.2753441 -3.373868 -4.5017238 -5.2614994 -5.5378575 -5.4910507 -5.1626616 -4.2866321 -3.0304735]]...]
INFO - root - 2017-12-16 06:18:15.125493: step 33410, loss = 0.33, batch loss = 0.27 (28.6 examples/sec; 0.279 sec/batch; 23h:12m:08s remains)
INFO - root - 2017-12-16 06:18:17.955369: step 33420, loss = 0.23, batch loss = 0.17 (28.1 examples/sec; 0.285 sec/batch; 23h:39m:43s remains)
INFO - root - 2017-12-16 06:18:20.767585: step 33430, loss = 0.24, batch loss = 0.18 (25.5 examples/sec; 0.313 sec/batch; 26h:02m:28s remains)
INFO - root - 2017-12-16 06:18:23.592617: step 33440, loss = 0.24, batch loss = 0.18 (27.0 examples/sec; 0.296 sec/batch; 24h:36m:08s remains)
INFO - root - 2017-12-16 06:18:26.386542: step 33450, loss = 0.21, batch loss = 0.15 (28.1 examples/sec; 0.284 sec/batch; 23h:37m:06s remains)
INFO - root - 2017-12-16 06:18:29.211837: step 33460, loss = 0.21, batch loss = 0.15 (26.9 examples/sec; 0.297 sec/batch; 24h:40m:35s remains)
INFO - root - 2017-12-16 06:18:32.062963: step 33470, loss = 0.20, batch loss = 0.14 (27.6 examples/sec; 0.290 sec/batch; 24h:04m:11s remains)
INFO - root - 2017-12-16 06:18:34.897253: step 33480, loss = 0.29, batch loss = 0.24 (27.8 examples/sec; 0.288 sec/batch; 23h:56m:16s remains)
INFO - root - 2017-12-16 06:18:37.726621: step 33490, loss = 0.26, batch loss = 0.20 (27.1 examples/sec; 0.295 sec/batch; 24h:29m:25s remains)
INFO - root - 2017-12-16 06:18:40.581356: step 33500, loss = 0.30, batch loss = 0.24 (26.5 examples/sec; 0.302 sec/batch; 25h:05m:37s remains)
2017-12-16 06:18:41.053513: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0095625 -2.3055017 -2.7629395 -2.98347 -3.0387444 -3.0960915 -3.2039156 -3.305439 -3.4173634 -3.6265678 -3.6603181 -3.3010316 -2.9121537 -2.3202751 -1.8265612][-1.2513208 -1.5837078 -2.0549114 -2.4403048 -2.7681384 -2.8389759 -2.9632826 -3.2084146 -3.4555192 -3.652472 -3.6464784 -3.38943 -3.0128536 -2.3421094 -1.807915][-1.2394748 -1.6205969 -2.0988271 -2.4257002 -2.6186538 -2.7128277 -2.952713 -3.2192407 -3.5157664 -3.6687379 -3.7242086 -3.4700055 -2.9892833 -2.4957895 -2.2109966][-2.0457108 -2.124485 -2.160722 -2.2332938 -2.2161126 -2.1234748 -2.3575354 -2.679 -3.1429698 -3.6059783 -3.9233222 -3.8347483 -3.5988324 -3.2322125 -2.9255574][-2.778317 -2.6885872 -2.3732362 -1.7714729 -1.1835833 -0.83062458 -0.95595694 -1.3598919 -2.0044544 -2.7640054 -3.5638256 -3.9486926 -3.9716866 -3.9744203 -3.8708446][-3.488234 -2.9581344 -2.2542369 -1.225493 -0.22846413 0.53155565 0.748013 0.33740139 -0.44027805 -1.4181204 -2.4069431 -3.1887574 -3.8126533 -4.1578484 -4.1263647][-3.9861355 -3.1004553 -1.8363545 -0.19338894 1.1656885 2.1975732 2.4293184 1.9493003 1.0822453 -0.17617226 -1.546222 -2.7602615 -3.6133921 -4.0920053 -4.2233787][-3.7484908 -2.9520364 -1.605315 0.55880356 2.4466615 4.0248985 4.4987364 4.0213995 2.8048973 1.028698 -0.68174815 -2.3319182 -3.5737114 -4.16317 -4.2866068][-3.6320162 -2.7340837 -1.4484842 0.21541214 1.815691 3.453495 4.2451134 4.1134996 3.0149388 1.3325329 -0.454525 -2.2626872 -3.6039248 -4.3399153 -4.6191669][-4.0130682 -3.4640269 -2.5247288 -1.1216741 0.21436882 1.3428373 2.0271955 2.0509672 1.2002072 -0.18473148 -1.6503928 -3.0734668 -4.2030425 -4.90255 -5.1169872][-5.0343666 -4.4746637 -3.6417866 -2.7134209 -1.8007982 -0.91370654 -0.34690142 -0.4838078 -1.0010822 -1.9747832 -3.1305788 -4.2384419 -4.987452 -5.4633722 -5.6630096][-5.1455336 -5.16399 -4.8828249 -4.0813928 -3.5204825 -3.1428485 -2.9129133 -2.9726186 -3.2173934 -3.768223 -4.3307686 -5.0902915 -5.6596155 -5.9458675 -5.9891067][-4.9148865 -4.9322686 -4.914453 -4.8545585 -4.8486538 -4.5559697 -4.4118624 -4.490787 -4.5699177 -4.72561 -4.8615961 -5.1506476 -5.4233584 -5.7442818 -5.9486][-3.9969718 -4.0863738 -4.2259483 -4.2763915 -4.4558635 -4.669806 -4.8162546 -4.8173532 -4.7220683 -4.5505748 -4.400846 -4.4538541 -4.5512795 -4.6408839 -4.8998117][-3.2397509 -3.1917191 -3.2467155 -3.4733706 -3.8681197 -4.1972051 -4.380228 -4.3732958 -4.1589427 -3.9091492 -3.7039196 -3.4316022 -3.3074551 -3.4457984 -3.8358727]]...]
INFO - root - 2017-12-16 06:18:43.893593: step 33510, loss = 0.25, batch loss = 0.20 (28.6 examples/sec; 0.279 sec/batch; 23h:11m:52s remains)
INFO - root - 2017-12-16 06:18:46.708064: step 33520, loss = 0.40, batch loss = 0.34 (28.3 examples/sec; 0.283 sec/batch; 23h:30m:16s remains)
INFO - root - 2017-12-16 06:18:49.485094: step 33530, loss = 0.40, batch loss = 0.34 (28.5 examples/sec; 0.281 sec/batch; 23h:19m:50s remains)
INFO - root - 2017-12-16 06:18:52.339582: step 33540, loss = 0.26, batch loss = 0.20 (27.8 examples/sec; 0.288 sec/batch; 23h:52m:53s remains)
INFO - root - 2017-12-16 06:18:55.202943: step 33550, loss = 0.25, batch loss = 0.19 (28.7 examples/sec; 0.279 sec/batch; 23h:08m:49s remains)
INFO - root - 2017-12-16 06:18:58.009159: step 33560, loss = 0.24, batch loss = 0.18 (27.6 examples/sec; 0.290 sec/batch; 24h:02m:31s remains)
INFO - root - 2017-12-16 06:19:00.850621: step 33570, loss = 0.25, batch loss = 0.19 (29.4 examples/sec; 0.272 sec/batch; 22h:35m:55s remains)
INFO - root - 2017-12-16 06:19:03.700736: step 33580, loss = 0.22, batch loss = 0.16 (28.4 examples/sec; 0.282 sec/batch; 23h:24m:18s remains)
INFO - root - 2017-12-16 06:19:06.487678: step 33590, loss = 0.21, batch loss = 0.16 (28.1 examples/sec; 0.285 sec/batch; 23h:38m:40s remains)
INFO - root - 2017-12-16 06:19:09.339082: step 33600, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.286 sec/batch; 23h:45m:38s remains)
2017-12-16 06:19:09.807781: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7371106 -2.7305131 -2.7752919 -3.0955019 -3.3814974 -3.64811 -3.8766026 -4.2648692 -4.487855 -4.39517 -4.0786242 -3.6840959 -3.2364187 -2.6423328 -2.176728][-2.3255773 -2.4452267 -2.572706 -2.9205766 -3.1936455 -3.3039827 -3.4890888 -3.6706734 -3.7336986 -3.5766366 -3.2698979 -2.8309488 -2.266634 -1.742799 -1.3260689][-2.2132223 -2.4113848 -2.6277804 -2.9559152 -3.1118307 -2.9883995 -2.9532104 -3.1101716 -3.2044334 -3.0593371 -2.8343818 -2.5234084 -2.0182803 -1.3744049 -0.97091794][-2.2172322 -2.4745193 -2.6734838 -2.7628427 -2.6239119 -2.2102447 -1.7787707 -1.7485003 -1.9252489 -2.1044424 -2.0761175 -1.9128647 -1.8315084 -1.5285158 -1.2784102][-2.6824751 -2.8873355 -2.751652 -2.3780487 -1.7350218 -0.73819494 0.11656713 0.3441329 -0.0023889542 -0.58321357 -1.045507 -1.2597227 -1.5370457 -1.5870497 -1.516681][-2.9745283 -3.0807834 -2.6853614 -1.794858 -0.63141274 0.835989 2.08427 2.4853468 2.0334926 1.1107898 0.15458536 -0.55689025 -1.2986641 -1.6027606 -1.5762062][-3.0951962 -3.0502849 -2.3724031 -1.0253983 0.50346851 2.2764344 3.9375992 4.4504461 3.7934647 2.4460788 1.0620341 -0.082937241 -1.1314816 -1.5515912 -1.6174405][-3.2586405 -3.119916 -2.2847283 -0.66180062 1.1934509 3.137599 4.743844 5.3430719 4.7915258 3.1064625 1.2928553 -0.10938454 -1.2174768 -1.6687839 -1.7177532][-3.2566485 -3.1928902 -2.5115967 -0.9002502 0.98903036 2.8786759 4.3700819 4.8833017 4.265625 2.6359162 0.84827614 -0.63259006 -1.6798546 -2.0797033 -2.0089281][-3.1821377 -3.2232795 -2.7388263 -1.4945679 0.033181667 1.7585773 2.9556518 3.2727628 2.6937423 1.1765141 -0.51812124 -2.0481389 -2.9982915 -3.0082426 -2.710988][-3.3718257 -3.4783187 -3.1717689 -2.2182312 -1.0370934 0.35027361 1.2225046 1.2367206 0.62284613 -0.67044306 -2.2131438 -3.5898547 -4.3716869 -4.2691088 -3.8644218][-3.9129789 -3.9528167 -3.8146911 -3.1335719 -2.164984 -1.0842986 -0.64259744 -0.87072134 -1.453228 -2.5203218 -3.7842002 -5.0292373 -5.71832 -5.6000218 -5.02913][-4.2519536 -4.1227584 -4.0011368 -3.6598356 -3.1836147 -2.414948 -2.0049148 -2.2145061 -2.7783647 -3.6553283 -4.6178541 -5.6632185 -6.3040962 -6.2928391 -5.7272239][-4.5373898 -4.1373358 -3.7435219 -3.5242519 -3.1743829 -2.6584368 -2.361752 -2.5831022 -3.1986368 -3.9664841 -4.7955146 -5.6685009 -6.2712054 -6.0279074 -5.2214179][-4.5477738 -4.0741329 -3.5899067 -3.3521066 -3.1442575 -2.7100472 -2.3965535 -2.4587293 -2.8238409 -3.5659351 -4.4625373 -5.13958 -5.6845884 -5.6233206 -4.7996411]]...]
INFO - root - 2017-12-16 06:19:12.639692: step 33610, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.284 sec/batch; 23h:35m:24s remains)
INFO - root - 2017-12-16 06:19:15.485446: step 33620, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 23h:23m:39s remains)
INFO - root - 2017-12-16 06:19:18.322987: step 33630, loss = 0.31, batch loss = 0.25 (28.6 examples/sec; 0.280 sec/batch; 23h:12m:39s remains)
INFO - root - 2017-12-16 06:19:21.125830: step 33640, loss = 0.18, batch loss = 0.12 (29.5 examples/sec; 0.271 sec/batch; 22h:31m:25s remains)
INFO - root - 2017-12-16 06:19:23.983737: step 33650, loss = 0.22, batch loss = 0.16 (26.5 examples/sec; 0.302 sec/batch; 25h:05m:50s remains)
INFO - root - 2017-12-16 06:19:26.837974: step 33660, loss = 0.25, batch loss = 0.19 (27.9 examples/sec; 0.286 sec/batch; 23h:45m:59s remains)
INFO - root - 2017-12-16 06:19:29.699177: step 33670, loss = 0.31, batch loss = 0.25 (28.2 examples/sec; 0.284 sec/batch; 23h:35m:19s remains)
INFO - root - 2017-12-16 06:19:32.530252: step 33680, loss = 0.25, batch loss = 0.19 (29.2 examples/sec; 0.274 sec/batch; 22h:42m:16s remains)
INFO - root - 2017-12-16 06:19:35.384830: step 33690, loss = 0.28, batch loss = 0.22 (26.8 examples/sec; 0.298 sec/batch; 24h:45m:28s remains)
INFO - root - 2017-12-16 06:19:38.245731: step 33700, loss = 0.50, batch loss = 0.44 (26.6 examples/sec; 0.301 sec/batch; 24h:59m:21s remains)
2017-12-16 06:19:38.731313: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6871035 -2.7386246 -2.776957 -3.0955019 -3.5858176 -4.1264863 -4.5393777 -4.5672436 -4.5084553 -4.3264451 -3.8956153 -3.423779 -3.1878376 -3.2613649 -3.4258513][-1.8802621 -2.0848694 -2.3273909 -2.6287303 -3.0473309 -3.1854327 -3.3710179 -3.7373002 -4.122395 -4.0172672 -3.6800966 -3.4744704 -3.3963008 -3.4279804 -3.6337576][-1.9170105 -2.1548841 -2.1723156 -2.1869621 -2.1003561 -2.289264 -2.571553 -2.6442509 -2.8326383 -3.2579005 -3.604012 -3.6613796 -3.8311605 -4.0674996 -4.2850571][-1.9931746 -2.2904937 -2.4538808 -2.2785013 -1.9197457 -1.6661344 -1.3102176 -1.4350116 -1.7060039 -2.25143 -2.7604527 -3.4099629 -4.0646272 -4.4642148 -4.7360678][-2.414248 -2.6550977 -2.7759936 -2.4645042 -1.9699917 -1.3991108 -0.86010981 -0.60944653 -0.76907563 -1.2514894 -2.0653381 -2.9681556 -3.8302116 -4.2459784 -4.6369104][-3.1651649 -3.2611084 -3.0865006 -2.4669094 -1.4040241 -0.37594557 0.4509697 0.71051884 0.63443041 -0.074520588 -1.1985154 -2.2775965 -3.0209241 -3.8223629 -4.3870206][-3.5573828 -3.3525743 -3.0406814 -2.0406194 -0.61405039 0.73260736 1.9817772 2.3638368 2.1542988 1.3127522 0.094497681 -1.2665784 -2.2723203 -2.7024403 -3.0023801][-3.3550487 -3.0792384 -2.7020245 -1.8345785 -0.38108587 1.0126719 2.0281343 2.3379745 2.2446833 1.54738 0.34294558 -0.6017735 -1.3002834 -1.6028295 -1.5951495][-3.2172809 -2.9922314 -2.7454619 -2.191175 -1.1937325 -0.0079016685 0.94347239 1.1249208 0.96122885 0.35961437 -0.42020941 -1.1207459 -1.1868362 -0.91045618 -0.42068052][-3.2645316 -3.2025294 -3.2127347 -2.8571129 -2.1250117 -1.4048147 -0.98081017 -0.81346321 -0.87638164 -1.4202967 -1.9037292 -2.2483993 -1.8991735 -1.1258118 -0.0423584][-3.4315264 -3.4086196 -3.4473023 -3.2998981 -2.8513498 -2.4363794 -2.2276692 -2.3713422 -2.7198625 -3.2320271 -3.5619195 -3.3493223 -2.63337 -1.5982566 -0.39199686][-3.92433 -3.8289762 -3.9400418 -4.1501026 -4.0895753 -4.0576925 -4.0774727 -4.35569 -4.7644606 -5.2512126 -5.3325095 -4.8583684 -3.8110027 -2.6029143 -1.3453302][-4.6003041 -4.4689064 -4.5343766 -4.6361246 -4.8684335 -5.0904136 -5.4672103 -6.1462622 -6.8166475 -7.0853906 -6.876708 -6.4962773 -5.5223 -4.321166 -3.1427245][-5.1025562 -4.8298774 -4.734437 -4.6796246 -4.7905741 -5.2322235 -6.0770431 -6.86094 -7.6088762 -8.0952082 -8.1024694 -7.5230327 -6.5759821 -5.6768675 -4.94312][-5.422986 -5.0484471 -4.6479859 -4.5139737 -4.7429123 -5.3563652 -6.2535982 -7.2579384 -8.1304626 -8.6417217 -8.4863062 -7.9685211 -7.2334275 -6.4618144 -5.7686663]]...]
INFO - root - 2017-12-16 06:19:41.575239: step 33710, loss = 0.29, batch loss = 0.23 (28.8 examples/sec; 0.278 sec/batch; 23h:01m:54s remains)
INFO - root - 2017-12-16 06:19:44.382290: step 33720, loss = 0.29, batch loss = 0.24 (28.6 examples/sec; 0.280 sec/batch; 23h:14m:23s remains)
INFO - root - 2017-12-16 06:19:47.187460: step 33730, loss = 0.20, batch loss = 0.15 (29.0 examples/sec; 0.276 sec/batch; 22h:55m:07s remains)
INFO - root - 2017-12-16 06:19:50.049835: step 33740, loss = 0.26, batch loss = 0.20 (29.0 examples/sec; 0.276 sec/batch; 22h:52m:58s remains)
INFO - root - 2017-12-16 06:19:52.887682: step 33750, loss = 0.24, batch loss = 0.19 (28.5 examples/sec; 0.280 sec/batch; 23h:15m:55s remains)
INFO - root - 2017-12-16 06:19:55.703462: step 33760, loss = 0.21, batch loss = 0.15 (27.3 examples/sec; 0.293 sec/batch; 24h:21m:08s remains)
INFO - root - 2017-12-16 06:19:58.556089: step 33770, loss = 0.24, batch loss = 0.18 (29.3 examples/sec; 0.273 sec/batch; 22h:38m:54s remains)
INFO - root - 2017-12-16 06:20:01.415558: step 33780, loss = 0.32, batch loss = 0.26 (28.3 examples/sec; 0.283 sec/batch; 23h:28m:34s remains)
INFO - root - 2017-12-16 06:20:04.222753: step 33790, loss = 0.26, batch loss = 0.20 (29.1 examples/sec; 0.275 sec/batch; 22h:50m:54s remains)
INFO - root - 2017-12-16 06:20:07.071563: step 33800, loss = 0.43, batch loss = 0.37 (26.9 examples/sec; 0.297 sec/batch; 24h:38m:48s remains)
2017-12-16 06:20:07.531077: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6149526 -5.0780506 -5.284687 -5.2774463 -5.1623087 -4.8241916 -4.6195664 -4.4461465 -4.1911349 -3.8078322 -3.4354692 -3.1570368 -3.210474 -3.4996064 -3.94211][-3.7548728 -4.3105373 -4.615232 -4.6287861 -4.3679814 -4.0911965 -3.5630877 -3.1499257 -2.9756165 -2.6694484 -2.3927903 -2.3473387 -2.6218524 -2.8500381 -3.2262554][-2.769176 -3.5085375 -4.0486784 -4.0797353 -3.7288077 -3.2775893 -2.7578821 -2.1306741 -1.8058846 -1.6233549 -1.7633564 -1.8602073 -2.0974777 -2.5114996 -2.8966117][-2.3151813 -3.0066922 -3.6657934 -3.8699899 -3.5121107 -2.9531376 -2.1470897 -1.4951668 -1.3017414 -1.2816799 -1.3149977 -1.6234422 -2.1706867 -2.5490518 -2.8469675][-2.0496097 -2.8094635 -3.5426223 -3.8467486 -3.5208063 -2.8234305 -1.7405281 -0.88893723 -0.55990171 -0.68001342 -1.2543137 -1.7617526 -2.3129258 -2.7311676 -3.0861337][-1.9856691 -2.6086719 -3.2243631 -3.494175 -3.0629458 -2.330924 -1.3438885 -0.40370846 -0.018455029 -0.20424652 -0.7435379 -1.4980319 -2.292783 -2.8659194 -3.2648654][-1.8749654 -2.2331071 -2.7741933 -3.1714478 -2.9411664 -2.0689569 -0.97529864 -0.11392021 0.32261276 0.16563511 -0.43155742 -1.3066921 -2.148283 -2.7836728 -3.2678511][-1.5532908 -1.7946014 -2.1751907 -2.5605264 -2.562706 -2.0539138 -1.0865285 -0.2035985 0.28040218 0.23226738 -0.30970478 -1.1280184 -1.9845853 -2.6917214 -3.2725368][-1.1089833 -1.149754 -1.4404476 -1.9780715 -2.0916581 -1.7644653 -1.0844355 -0.32231808 0.14340115 0.15320396 -0.28792524 -1.0035026 -1.7445376 -2.4489408 -3.1640902][-0.76996064 -0.63482976 -0.83956504 -1.3648589 -1.7504861 -1.7309866 -1.2985106 -0.69546247 -0.25445509 -0.14300632 -0.35860062 -0.87173891 -1.5235593 -2.2406549 -2.9561958][-0.85565376 -0.66234589 -0.791137 -1.2815909 -1.7727067 -1.90114 -1.7035282 -1.2781625 -0.85073352 -0.711359 -0.76909971 -1.0589404 -1.5421121 -2.153002 -2.8260522][-1.7442684 -1.4364345 -1.4267352 -1.8034313 -2.2516041 -2.5418098 -2.4418898 -2.1192284 -1.7691939 -1.5255322 -1.5363948 -1.6730349 -1.9888065 -2.3570616 -2.9354906][-2.6548829 -2.3583834 -2.3089571 -2.5531235 -2.9244838 -3.1427474 -3.118453 -2.8215356 -2.5821116 -2.3482802 -2.3349504 -2.2268612 -2.4680092 -2.6804883 -3.0293341][-3.7730145 -3.4052258 -3.3119025 -3.3113708 -3.5231147 -3.7289858 -3.6650996 -3.4991317 -3.2856979 -3.0590596 -3.0435419 -2.9932413 -2.8835487 -2.9413886 -3.2447329][-4.8649378 -4.744493 -4.5808563 -4.4601512 -4.4144578 -4.4780493 -4.3407645 -4.1600151 -3.9878035 -3.7988033 -3.6784234 -3.4878759 -3.4894955 -3.464169 -3.5698957]]...]
INFO - root - 2017-12-16 06:20:10.370960: step 33810, loss = 0.24, batch loss = 0.18 (28.9 examples/sec; 0.277 sec/batch; 22h:56m:54s remains)
INFO - root - 2017-12-16 06:20:13.208638: step 33820, loss = 0.29, batch loss = 0.23 (29.0 examples/sec; 0.276 sec/batch; 22h:53m:55s remains)
INFO - root - 2017-12-16 06:20:16.020469: step 33830, loss = 0.29, batch loss = 0.23 (27.9 examples/sec; 0.287 sec/batch; 23h:49m:46s remains)
INFO - root - 2017-12-16 06:20:18.862744: step 33840, loss = 0.20, batch loss = 0.14 (28.6 examples/sec; 0.280 sec/batch; 23h:11m:36s remains)
INFO - root - 2017-12-16 06:20:21.683570: step 33850, loss = 0.29, batch loss = 0.23 (28.1 examples/sec; 0.285 sec/batch; 23h:38m:22s remains)
INFO - root - 2017-12-16 06:20:24.495168: step 33860, loss = 0.22, batch loss = 0.17 (28.8 examples/sec; 0.277 sec/batch; 23h:00m:45s remains)
INFO - root - 2017-12-16 06:20:27.332978: step 33870, loss = 0.23, batch loss = 0.18 (28.1 examples/sec; 0.284 sec/batch; 23h:34m:35s remains)
INFO - root - 2017-12-16 06:20:30.167478: step 33880, loss = 0.26, batch loss = 0.20 (27.8 examples/sec; 0.288 sec/batch; 23h:52m:21s remains)
INFO - root - 2017-12-16 06:20:32.991680: step 33890, loss = 0.22, batch loss = 0.16 (29.0 examples/sec; 0.276 sec/batch; 22h:53m:21s remains)
INFO - root - 2017-12-16 06:20:35.786945: step 33900, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 23h:36m:35s remains)
2017-12-16 06:20:36.239582: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3173208 -6.5519657 -6.4784646 -5.991437 -5.4701405 -4.8841782 -4.3604779 -4.0538387 -4.1498384 -4.6654577 -5.2091694 -5.8191609 -6.6138096 -7.2393737 -7.4099531][-6.8455687 -7.3065386 -7.3655939 -6.7843218 -6.1601648 -5.4079156 -4.6651244 -3.966378 -3.7199023 -4.3066516 -5.2265053 -5.9560165 -6.6937551 -7.1877327 -7.3493528][-6.8894291 -7.4023619 -7.3358974 -6.7240191 -5.8924265 -5.0036407 -4.1439848 -3.5932834 -3.413095 -3.6944711 -4.3837862 -5.2318521 -6.0734076 -6.6791778 -7.08418][-5.8834085 -6.3441629 -6.1502995 -5.2693396 -4.31051 -3.4369378 -2.6293128 -2.3375726 -2.3460085 -2.7101636 -3.3139937 -3.9510648 -4.6599288 -5.3696504 -5.7395539][-4.5401435 -4.6351042 -4.1770339 -3.3730426 -2.4518342 -1.5434294 -0.97215605 -0.80241251 -0.8100481 -1.1438105 -1.6596708 -1.9365263 -2.5787387 -3.5059423 -4.1867905][-3.50199 -3.1133788 -2.2527394 -1.1756678 -0.043333054 0.45842838 0.57546425 0.67516184 0.56765985 0.18310738 -0.35790682 -0.67205834 -1.2834713 -2.0114133 -2.6877012][-2.3800077 -1.7794993 -0.82554674 0.35055447 1.6321678 2.3456297 2.5877461 2.2570453 1.6871567 1.2057695 0.59646463 0.055902958 -0.63395953 -1.5681682 -2.2722952][-2.1734128 -1.4271858 -0.26433134 0.91603994 1.9446993 2.6871591 3.0594158 2.8904209 2.3142567 1.5806465 0.74247313 -0.025274754 -0.81912255 -1.762234 -2.664865][-2.8127542 -2.202147 -1.1307671 -0.079747677 0.83381605 1.5956888 1.9918432 2.0486856 1.5960431 0.75304174 -0.12216616 -0.9089241 -1.6571271 -2.5124311 -3.3878493][-3.1169066 -3.2107673 -2.8481591 -2.037493 -1.2173824 -0.41957664 -0.012454987 0.097026348 -0.11276531 -0.78677511 -1.4731605 -2.2244287 -3.0082076 -3.7793279 -4.3886657][-4.3685627 -4.3302593 -3.9662988 -3.7733965 -3.4772758 -2.7327123 -2.1478879 -2.0353003 -2.1256003 -2.4452481 -2.7400608 -3.3660021 -4.084538 -4.5451527 -4.8405046][-5.497818 -5.5567722 -5.6155868 -5.3764496 -5.0480695 -4.8025551 -4.5880461 -4.2774186 -3.9447811 -3.9010205 -3.9154506 -3.9876711 -4.2009616 -4.3631296 -4.4990726][-6.3131967 -6.3821816 -6.57056 -6.5200319 -6.6263261 -6.5711713 -6.3043861 -6.1109104 -5.8685527 -5.4949632 -5.1089063 -4.8334 -4.736218 -4.5155005 -4.2641821][-6.7269955 -6.6386032 -6.6644182 -6.680984 -6.7972183 -6.8047152 -6.7408953 -6.7500234 -6.5124159 -5.9902887 -5.5856891 -5.2811689 -5.0412264 -4.7492237 -4.3947196][-6.5195045 -6.16195 -6.05546 -6.144876 -6.3138552 -6.4284177 -6.4416943 -6.4012461 -6.2304573 -5.9546881 -5.7199469 -5.3641663 -5.0475168 -4.72919 -4.4110332]]...]
INFO - root - 2017-12-16 06:20:39.069495: step 33910, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 23h:28m:37s remains)
INFO - root - 2017-12-16 06:20:41.899287: step 33920, loss = 0.32, batch loss = 0.26 (28.2 examples/sec; 0.284 sec/batch; 23h:32m:49s remains)
INFO - root - 2017-12-16 06:20:44.724486: step 33930, loss = 0.22, batch loss = 0.16 (28.2 examples/sec; 0.284 sec/batch; 23h:33m:12s remains)
INFO - root - 2017-12-16 06:20:47.562059: step 33940, loss = 0.22, batch loss = 0.17 (29.3 examples/sec; 0.273 sec/batch; 22h:39m:01s remains)
INFO - root - 2017-12-16 06:20:50.391968: step 33950, loss = 0.34, batch loss = 0.29 (29.0 examples/sec; 0.276 sec/batch; 22h:54m:19s remains)
INFO - root - 2017-12-16 06:20:53.259185: step 33960, loss = 0.27, batch loss = 0.22 (27.8 examples/sec; 0.288 sec/batch; 23h:54m:05s remains)
INFO - root - 2017-12-16 06:20:56.118413: step 33970, loss = 0.34, batch loss = 0.28 (27.8 examples/sec; 0.288 sec/batch; 23h:53m:24s remains)
INFO - root - 2017-12-16 06:20:58.909296: step 33980, loss = 0.37, batch loss = 0.31 (28.5 examples/sec; 0.281 sec/batch; 23h:17m:37s remains)
INFO - root - 2017-12-16 06:21:01.796826: step 33990, loss = 0.23, batch loss = 0.17 (27.5 examples/sec; 0.291 sec/batch; 24h:09m:50s remains)
INFO - root - 2017-12-16 06:21:04.584916: step 34000, loss = 0.31, batch loss = 0.25 (27.9 examples/sec; 0.287 sec/batch; 23h:46m:22s remains)
2017-12-16 06:21:05.044637: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6937447 -3.8138521 -4.0607471 -4.4350104 -4.6186714 -4.637331 -4.5322471 -4.4645243 -4.5477953 -4.3989477 -4.2425547 -4.1178875 -4.2381358 -4.3506603 -4.2634549][-3.4184744 -3.6396916 -3.8842268 -4.11487 -4.1748214 -4.2858472 -4.2085185 -4.0955367 -4.1485639 -4.0486469 -3.955 -3.6763268 -3.5149012 -3.5831878 -3.5386562][-3.2511458 -3.356534 -3.5877504 -3.7421544 -3.6972384 -3.6980624 -3.598922 -3.4625025 -3.4146459 -3.3272433 -3.2088752 -2.9269967 -2.7671914 -2.6121979 -2.487184][-2.8306451 -2.9146667 -3.0819244 -3.19245 -3.0604088 -2.8654172 -2.6861572 -2.6636541 -2.6652536 -2.6030159 -2.4845567 -2.1260192 -1.9406757 -1.7377312 -1.4768786][-2.1478419 -2.2211993 -2.30797 -2.3193927 -2.1040988 -1.9201796 -1.679234 -1.6529825 -1.7801926 -1.7482975 -1.6219294 -1.2834387 -1.0091391 -0.80463147 -0.65821123][-1.3626535 -1.3781278 -1.3789177 -1.2887609 -0.99427176 -0.78991437 -0.62586832 -0.55816913 -0.72607231 -0.92200661 -0.96112752 -0.72265887 -0.34595251 -0.070257187 -0.083974838][-0.70970368 -0.679173 -0.63581538 -0.52780247 -0.25893784 -0.031612873 0.098408222 -0.025531292 -0.12948942 -0.29553843 -0.42451763 -0.36090517 -0.17623472 0.15399504 0.24434996][-0.23128033 -0.20577908 -0.32859993 -0.1591773 0.091191292 0.18967724 0.28821993 0.050999641 -0.084373474 -0.2750001 -0.47520638 -0.33112907 -0.1922555 0.043156147 0.16262865][-0.024675369 0.10832977 -0.038819313 -0.18387222 -0.32241011 -0.25775909 -0.20417023 -0.39734364 -0.43443036 -0.58668089 -0.75364256 -0.84759545 -0.78035569 -0.55638742 -0.51365709][0.2105484 0.042122364 -0.37585878 -0.6225729 -0.84187675 -0.92202067 -0.93863869 -1.0737231 -1.1378508 -1.1827106 -1.1810877 -1.4639852 -1.6408143 -1.406328 -1.2173517][-0.12928534 -0.23263884 -0.75284338 -1.262943 -1.6306214 -1.6853445 -1.6684558 -1.7612085 -1.7967606 -1.9182217 -2.1506033 -2.41927 -2.5424452 -2.3983307 -2.1617987][-0.79696655 -0.98198009 -1.430655 -1.9436939 -2.25122 -2.2189143 -2.1849089 -2.2260492 -2.334981 -2.4876552 -2.8145537 -3.2878108 -3.5331192 -3.3538313 -2.8619628][-0.8708365 -0.85217047 -1.2809722 -1.6938415 -1.9577909 -2.0335076 -2.0423195 -1.9627054 -2.0544271 -2.405786 -3.0019188 -3.5804148 -3.8460793 -3.787925 -3.3454244][-0.96591616 -0.78138018 -1.112843 -1.403074 -1.6790602 -1.6749089 -1.7069433 -1.7969592 -2.0687439 -2.1863065 -2.5641942 -3.2314062 -3.710829 -3.5927818 -2.8902733][-1.0581834 -0.86287427 -0.90049434 -0.91101241 -0.96303725 -0.92118192 -1.0460482 -1.2355154 -1.517102 -1.8546951 -2.3281734 -2.8366241 -3.1831589 -3.0949974 -2.5160978]]...]
INFO - root - 2017-12-16 06:21:07.866697: step 34010, loss = 0.35, batch loss = 0.29 (29.4 examples/sec; 0.272 sec/batch; 22h:32m:12s remains)
INFO - root - 2017-12-16 06:21:10.684394: step 34020, loss = 0.27, batch loss = 0.21 (28.6 examples/sec; 0.280 sec/batch; 23h:11m:58s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 06:21:13.533908: step 34030, loss = 0.22, batch loss = 0.17 (28.1 examples/sec; 0.285 sec/batch; 23h:36m:33s remains)
INFO - root - 2017-12-16 06:21:16.385085: step 34040, loss = 0.32, batch loss = 0.26 (28.8 examples/sec; 0.278 sec/batch; 23h:03m:03s remains)
INFO - root - 2017-12-16 06:21:19.218936: step 34050, loss = 0.20, batch loss = 0.14 (27.7 examples/sec; 0.289 sec/batch; 23h:56m:01s remains)
INFO - root - 2017-12-16 06:21:22.071429: step 34060, loss = 0.29, batch loss = 0.23 (27.9 examples/sec; 0.287 sec/batch; 23h:46m:52s remains)
INFO - root - 2017-12-16 06:21:24.925885: step 34070, loss = 0.25, batch loss = 0.20 (28.1 examples/sec; 0.284 sec/batch; 23h:33m:32s remains)
INFO - root - 2017-12-16 06:21:27.736666: step 34080, loss = 0.25, batch loss = 0.19 (29.1 examples/sec; 0.275 sec/batch; 22h:46m:56s remains)
INFO - root - 2017-12-16 06:21:30.523746: step 34090, loss = 0.30, batch loss = 0.24 (28.2 examples/sec; 0.284 sec/batch; 23h:32m:08s remains)
INFO - root - 2017-12-16 06:21:33.325554: step 34100, loss = 0.21, batch loss = 0.16 (27.9 examples/sec; 0.287 sec/batch; 23h:46m:41s remains)
2017-12-16 06:21:33.771857: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1448557 -3.1342463 -3.2007821 -3.290453 -3.4322076 -3.3973703 -3.3210988 -3.4261255 -3.4528837 -3.2752614 -3.0416203 -2.8057902 -2.698072 -2.6871419 -2.5776324][-2.3931909 -2.2540841 -2.3047819 -2.5903053 -2.8477345 -2.9448731 -2.8459458 -2.8744988 -2.8902884 -3.0068138 -3.0968995 -3.0524011 -2.9973254 -3.0376167 -3.0375857][-1.8957582 -1.9322915 -1.9811471 -2.1185756 -2.2602139 -2.4087479 -2.5785 -2.7331085 -2.8143861 -3.0526652 -3.2628331 -3.3939328 -3.430758 -3.3897455 -3.3617425][-1.829037 -2.2338276 -2.4784384 -2.2354696 -2.0747225 -1.8842142 -1.8170431 -2.1956511 -2.6464896 -3.0246048 -3.35654 -3.5821118 -3.9275153 -3.985379 -3.7723277][-2.3409629 -2.65834 -2.7183146 -2.525418 -2.3205125 -1.8865936 -1.3838327 -1.2380891 -1.6340158 -2.2228816 -2.7576647 -3.3658571 -3.9537511 -4.1341257 -4.0544815][-3.2016611 -3.3455048 -3.1621647 -2.5051661 -1.8302376 -1.1784647 -0.50288486 -0.22727108 -0.47884703 -1.0966008 -1.8301849 -2.6230528 -3.3499379 -3.8250809 -3.9620125][-3.3600669 -3.3602595 -3.0469508 -2.077527 -0.99579263 -0.079555988 0.57083464 0.85523367 0.79810381 0.088323116 -0.88364577 -1.9530365 -2.9763203 -3.4605703 -3.5617905][-3.4811721 -3.095314 -2.6546288 -1.7352755 -0.64453626 0.5314455 1.4240942 1.669827 1.6466513 1.1924195 0.296628 -0.89525485 -2.1897194 -2.939528 -3.093605][-3.4714596 -2.9214711 -2.4322248 -1.6852102 -0.74943709 0.51814079 1.6757507 2.0363131 1.8155103 1.2182546 0.33922815 -0.87964869 -2.028172 -2.7442293 -2.9370909][-3.5599823 -2.8816066 -2.4220426 -1.8515534 -1.0882096 0.36976528 1.4930429 1.6708107 1.4303455 0.68465948 -0.038008213 -0.97623849 -1.9649971 -2.5634389 -2.6381905][-3.5667186 -2.8962574 -2.2091856 -1.8673759 -1.6187773 -0.7357564 0.20222902 0.35686684 0.24330425 -0.15893698 -0.72436261 -1.2975147 -1.8888295 -2.2995257 -2.2654557][-3.3288257 -2.9372087 -2.6230581 -2.3039467 -2.0720696 -1.6524074 -1.0969515 -0.97775173 -1.1340992 -1.2797825 -1.3740284 -1.8323739 -2.2953084 -2.5232983 -2.521266][-2.7939816 -2.490973 -2.464293 -2.6877327 -2.8774107 -2.5319824 -2.0656455 -2.0117717 -2.0049479 -2.0119324 -1.922951 -1.9798372 -2.3129056 -2.5570011 -2.5635071][-2.7553291 -2.3252513 -2.0813687 -2.3287556 -2.7914236 -2.9138064 -2.6834459 -2.5510888 -2.5331221 -2.5994289 -2.5242124 -2.429831 -2.4801664 -2.8403039 -3.2102189][-2.5937896 -2.4159143 -2.3355904 -2.3531096 -2.5049713 -2.7228136 -2.7788472 -2.7382636 -2.8137107 -2.8016698 -2.6520767 -2.8040314 -3.2025862 -3.5518017 -3.6992111]]...]
INFO - root - 2017-12-16 06:21:36.581380: step 34110, loss = 0.31, batch loss = 0.26 (27.7 examples/sec; 0.289 sec/batch; 23h:56m:52s remains)
INFO - root - 2017-12-16 06:21:39.389830: step 34120, loss = 0.25, batch loss = 0.19 (28.6 examples/sec; 0.280 sec/batch; 23h:10m:47s remains)
INFO - root - 2017-12-16 06:21:42.192962: step 34130, loss = 0.26, batch loss = 0.20 (28.5 examples/sec; 0.281 sec/batch; 23h:15m:41s remains)
INFO - root - 2017-12-16 06:21:44.996346: step 34140, loss = 0.22, batch loss = 0.16 (28.5 examples/sec; 0.280 sec/batch; 23h:13m:42s remains)
INFO - root - 2017-12-16 06:21:47.832054: step 34150, loss = 0.30, batch loss = 0.24 (29.8 examples/sec; 0.268 sec/batch; 22h:12m:46s remains)
INFO - root - 2017-12-16 06:21:50.626873: step 34160, loss = 0.22, batch loss = 0.16 (29.6 examples/sec; 0.270 sec/batch; 22h:22m:13s remains)
INFO - root - 2017-12-16 06:21:53.460413: step 34170, loss = 0.44, batch loss = 0.38 (28.9 examples/sec; 0.277 sec/batch; 22h:57m:27s remains)
INFO - root - 2017-12-16 06:21:56.323199: step 34180, loss = 0.22, batch loss = 0.16 (26.7 examples/sec; 0.300 sec/batch; 24h:52m:20s remains)
INFO - root - 2017-12-16 06:21:59.103867: step 34190, loss = 0.27, batch loss = 0.21 (29.1 examples/sec; 0.275 sec/batch; 22h:46m:47s remains)
INFO - root - 2017-12-16 06:22:01.942069: step 34200, loss = 0.25, batch loss = 0.19 (26.6 examples/sec; 0.301 sec/batch; 24h:57m:56s remains)
2017-12-16 06:22:02.422513: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8003383 -3.8742561 -3.8465765 -3.5910995 -3.3349843 -3.4470513 -3.599267 -3.7836118 -4.2259388 -4.4899464 -4.5805597 -4.5846567 -4.6843653 -4.9632072 -5.2648911][-2.6657443 -2.8816323 -2.7846389 -2.8660209 -2.7197537 -2.8311825 -2.7714009 -2.9814625 -3.5495636 -3.9404926 -4.305459 -4.5023923 -4.7465286 -4.8579974 -5.11155][-1.7095389 -1.7616236 -1.7064846 -1.7318223 -1.6247723 -1.7318447 -1.7378566 -1.8939352 -2.5526125 -3.2636018 -3.8870516 -4.2644033 -4.6593161 -4.670332 -4.6775522][-0.94509292 -1.0114691 -1.1554563 -1.2041161 -1.1757324 -1.1123405 -0.89687872 -0.96308613 -1.6941667 -2.8052635 -3.6397665 -4.14259 -4.4769216 -4.4963646 -4.4767685][-0.54799867 -0.66552305 -0.72345877 -0.81473017 -0.73186445 -0.51087117 -0.12680387 0.014010906 -0.52660894 -1.4903488 -2.3364363 -3.1918859 -3.6331325 -3.7192011 -3.8974948][-0.38918924 -0.48473239 -0.60669279 -0.6331315 -0.30715942 0.34270477 1.104322 1.4004483 1.1940832 0.35032272 -0.6282227 -1.4288914 -1.8619452 -2.3660393 -2.9940019][-0.33658266 -0.31309509 -0.50425911 -0.52389717 -0.2530179 0.59655857 1.4893556 2.1717634 2.4356122 1.934999 1.0962801 0.20791483 -0.30713415 -1.0104339 -1.9484079][-0.54927731 -0.64022803 -0.91899037 -0.86741877 -0.55520535 0.33038473 1.409081 2.4616642 3.2720528 3.3666344 3.07226 2.2472315 1.4739285 0.31424189 -1.2517123][-1.5675123 -1.6926789 -2.0028257 -2.037 -1.6653144 -0.68240738 0.59064722 2.0526471 3.4869647 3.9449339 3.9959354 3.3949161 2.5735078 1.0985117 -0.89222741][-2.597548 -3.0617592 -3.5817807 -3.5532537 -3.1831312 -2.4629116 -1.4338396 0.35117292 2.1480689 3.0955534 3.5827703 3.3342061 2.7476549 1.2373157 -0.83084726][-3.461762 -3.9845634 -4.5949974 -4.8669949 -4.9555311 -4.1716084 -3.0327568 -1.2909572 0.29301262 1.5724883 2.4421449 2.1833806 1.5277262 0.12589455 -1.7599988][-4.805686 -5.3916802 -6.0971465 -6.4488592 -6.5765204 -5.931529 -5.2260466 -3.7805274 -2.1594584 -0.87000275 -0.20966482 -0.30439568 -0.71585059 -1.8494217 -3.435925][-6.33354 -6.8955154 -7.6562681 -8.3622169 -8.7063456 -8.295783 -7.8261194 -6.8257833 -5.3654156 -4.0870519 -3.2642474 -3.2965393 -3.5386119 -4.2081523 -5.1895065][-7.1118746 -7.6894331 -8.5649929 -9.5611839 -10.193502 -10.168036 -9.8277817 -9.023715 -7.9004135 -6.8646994 -6.0986757 -6.1074486 -6.175436 -6.5435691 -7.0443659][-7.3388743 -7.9021111 -8.7887115 -9.7367783 -10.410336 -10.563496 -10.380571 -10.010685 -9.4236031 -8.7109795 -8.1587334 -8.2503347 -8.3311291 -8.4384995 -8.48967]]...]
INFO - root - 2017-12-16 06:22:05.296895: step 34210, loss = 0.22, batch loss = 0.16 (28.0 examples/sec; 0.285 sec/batch; 23h:38m:08s remains)
INFO - root - 2017-12-16 06:22:08.138043: step 34220, loss = 0.31, batch loss = 0.26 (28.5 examples/sec; 0.280 sec/batch; 23h:14m:09s remains)
INFO - root - 2017-12-16 06:22:10.965803: step 34230, loss = 0.26, batch loss = 0.20 (27.9 examples/sec; 0.286 sec/batch; 23h:43m:29s remains)
INFO - root - 2017-12-16 06:22:13.760405: step 34240, loss = 0.32, batch loss = 0.27 (29.3 examples/sec; 0.273 sec/batch; 22h:37m:55s remains)
INFO - root - 2017-12-16 06:22:16.618848: step 34250, loss = 0.39, batch loss = 0.33 (25.5 examples/sec; 0.314 sec/batch; 26h:00m:42s remains)
INFO - root - 2017-12-16 06:22:19.405062: step 34260, loss = 0.23, batch loss = 0.18 (29.7 examples/sec; 0.269 sec/batch; 22h:18m:13s remains)
INFO - root - 2017-12-16 06:22:22.254823: step 34270, loss = 0.30, batch loss = 0.24 (27.5 examples/sec; 0.290 sec/batch; 24h:03m:54s remains)
INFO - root - 2017-12-16 06:22:25.119552: step 34280, loss = 0.31, batch loss = 0.26 (27.6 examples/sec; 0.290 sec/batch; 23h:59m:06s remains)
INFO - root - 2017-12-16 06:22:27.909799: step 34290, loss = 0.38, batch loss = 0.32 (28.8 examples/sec; 0.278 sec/batch; 23h:01m:41s remains)
INFO - root - 2017-12-16 06:22:30.705713: step 34300, loss = 0.31, batch loss = 0.25 (27.9 examples/sec; 0.286 sec/batch; 23h:43m:40s remains)
2017-12-16 06:22:31.180083: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0224752 -4.0666995 -4.1597052 -4.5955954 -5.0669165 -5.6077538 -5.9265714 -5.9082222 -5.7784967 -5.2450891 -4.4396396 -3.7008421 -3.4537404 -3.4489951 -3.4505744][-3.1006835 -3.0382738 -3.1293535 -3.4559429 -3.8248839 -4.3057542 -4.538753 -4.4837394 -4.3013115 -3.9169116 -3.4054503 -2.8499656 -2.4528913 -2.2300692 -2.0649555][-2.1138663 -1.9975762 -2.1078577 -2.26726 -2.4817584 -2.7218902 -2.8464124 -2.8274796 -2.5731592 -2.4566317 -2.3572972 -2.0517328 -1.7514279 -1.3452828 -1.0554154][-0.95859861 -0.8797605 -1.0751519 -1.1782267 -0.98437428 -1.0606077 -1.140748 -1.2930605 -1.281647 -1.309077 -1.3204963 -1.2632577 -1.1806881 -0.79663396 -0.67496848][-0.51435947 -0.59982824 -0.69790673 -0.68954825 -0.47942829 -0.25671482 0.11724663 0.048370838 -0.22019958 -0.47520208 -0.69319367 -0.6115222 -0.65446019 -0.531857 -0.60246062][-0.49924874 -0.79923558 -0.8790102 -0.61712623 0.098823071 0.6146307 1.0313559 1.1301875 1.0975847 0.88025761 0.33361244 0.044370174 -0.12099123 -0.071224689 -0.18251753][-0.61167836 -0.95938039 -1.1205294 -0.62032104 0.37569809 1.2018213 1.786129 1.8337069 1.7100334 1.5385947 1.0886493 0.52699375 0.00074052811 -0.043219566 -0.12487698][-0.86901355 -1.4159384 -1.5398805 -1.0028226 -0.14009094 0.88615131 1.6743474 1.8196044 1.6400995 1.4231162 0.8795681 0.39375544 0.053910255 -0.082128525 -0.36725569][-1.136565 -1.7477367 -1.7626438 -1.3700793 -0.62359524 0.47384167 1.3217764 1.4943194 1.2837653 0.99625826 0.49321938 -0.12038898 -0.60991788 -0.72338605 -0.7870779][-0.96978259 -1.9586599 -2.3636553 -1.8127699 -0.89995241 -0.15479517 0.45942974 0.80876732 0.70710135 0.3043294 -0.14336109 -0.70996833 -1.2619298 -1.5652444 -1.7145078][-1.3758636 -2.0877366 -2.3745854 -2.351759 -2.006556 -1.107173 -0.11177826 0.056453228 -0.23604727 -0.60683727 -1.0288014 -1.6438875 -2.2145092 -2.595057 -2.7574644][-1.8357279 -2.4672868 -2.9718027 -2.9039783 -2.4957614 -2.0201306 -1.6427023 -1.1986578 -0.8849628 -1.3440597 -2.0430212 -2.6820302 -3.1735072 -3.4881816 -3.5628829][-2.5694895 -3.1811848 -3.5991049 -3.7241344 -3.6491222 -3.0422728 -2.3005009 -1.9432571 -1.870213 -1.9830918 -2.217169 -2.8655605 -3.4730403 -3.7407846 -3.6829572][-2.7449303 -2.949327 -3.1426582 -3.2618847 -3.2003808 -2.9508095 -2.4017687 -1.7496922 -1.4146614 -1.6124275 -2.0321419 -2.6735997 -3.182539 -3.5204389 -3.6193571][-3.1088281 -3.1349163 -3.3109689 -3.3470082 -3.1110811 -2.8669114 -2.379642 -1.7871847 -1.3082001 -1.2609525 -1.575243 -2.0730145 -2.4546769 -2.7989678 -2.9716854]]...]
INFO - root - 2017-12-16 06:22:34.027962: step 34310, loss = 0.25, batch loss = 0.19 (29.4 examples/sec; 0.272 sec/batch; 22h:33m:56s remains)
INFO - root - 2017-12-16 06:22:36.832528: step 34320, loss = 0.41, batch loss = 0.35 (28.6 examples/sec; 0.279 sec/batch; 23h:08m:56s remains)
INFO - root - 2017-12-16 06:22:39.692652: step 34330, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.283 sec/batch; 23h:28m:16s remains)
INFO - root - 2017-12-16 06:22:42.498278: step 34340, loss = 0.21, batch loss = 0.15 (29.4 examples/sec; 0.272 sec/batch; 22h:33m:29s remains)
INFO - root - 2017-12-16 06:22:45.281887: step 34350, loss = 0.24, batch loss = 0.18 (28.6 examples/sec; 0.279 sec/batch; 23h:08m:22s remains)
INFO - root - 2017-12-16 06:22:48.082790: step 34360, loss = 0.27, batch loss = 0.21 (27.8 examples/sec; 0.288 sec/batch; 23h:50m:47s remains)
INFO - root - 2017-12-16 06:22:50.894297: step 34370, loss = 0.31, batch loss = 0.25 (28.5 examples/sec; 0.280 sec/batch; 23h:13m:37s remains)
INFO - root - 2017-12-16 06:22:53.708255: step 34380, loss = 0.32, batch loss = 0.26 (29.2 examples/sec; 0.274 sec/batch; 22h:39m:31s remains)
INFO - root - 2017-12-16 06:22:56.552050: step 34390, loss = 0.29, batch loss = 0.23 (28.3 examples/sec; 0.282 sec/batch; 23h:22m:31s remains)
INFO - root - 2017-12-16 06:22:59.367120: step 34400, loss = 0.31, batch loss = 0.25 (27.4 examples/sec; 0.292 sec/batch; 24h:10m:41s remains)
2017-12-16 06:22:59.835056: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9840107 -5.5043707 -5.0772519 -4.8252544 -4.9048243 -5.1989789 -5.1698718 -5.2693377 -5.3195858 -5.3882389 -5.3392119 -5.1294937 -4.8507986 -4.3481426 -3.8177738][-6.2739425 -5.8006115 -5.3609972 -5.0746417 -4.9772606 -5.2737923 -5.330411 -5.4473791 -5.4823713 -5.6552596 -5.7865052 -5.611721 -5.2405448 -4.5394063 -3.8304965][-6.4495354 -5.8843694 -5.2710819 -4.8089223 -4.5169783 -4.6790738 -4.6306238 -4.8411641 -5.2189145 -5.543263 -5.72087 -5.8819151 -5.9894562 -5.3149276 -4.4227209][-6.1609988 -5.4439754 -4.5784307 -3.8840683 -3.468405 -3.2746749 -3.0768762 -3.4201164 -3.8552024 -4.5375834 -5.2358813 -5.9048028 -6.2141509 -5.7903533 -5.0275354][-5.339705 -4.4635324 -3.3840184 -2.405951 -1.6472347 -1.1697249 -0.79502106 -1.0700519 -1.6973152 -2.654964 -3.6718688 -4.863235 -5.6948872 -5.7138042 -5.2115636][-4.6315856 -3.5462506 -2.1052418 -0.6934135 0.5909853 1.4500704 1.9706206 1.8046765 1.1700234 -0.042685509 -1.5212028 -3.2358227 -4.6886029 -5.2456079 -4.8130226][-3.8982377 -2.7865834 -1.3776593 0.33461666 1.8925791 3.1846318 4.2500725 4.3345184 3.7108278 2.3874283 0.75033188 -1.3202913 -3.1253424 -4.1953106 -4.394453][-3.3289661 -2.2045493 -0.79086447 0.70220804 2.0772238 3.4646258 4.6272383 4.9894705 4.7809124 3.5993671 1.826426 -0.25509453 -2.1923909 -3.5024261 -3.949017][-3.1179445 -2.3064461 -1.3751497 -0.25637197 1.0170116 2.1501279 3.3834143 3.9964628 4.0344944 3.1514492 1.4874854 -0.43788934 -2.2601936 -3.5782843 -4.1693635][-3.5374727 -3.1361923 -2.6261206 -1.846282 -1.0230365 -0.081946373 1.0533743 1.7285733 1.8710294 1.1437654 -0.31630516 -2.0026479 -3.7567897 -4.8077326 -5.1255126][-4.2549438 -4.0365014 -3.8022227 -3.5557883 -3.2028022 -2.6906557 -1.6216722 -1.0270052 -0.8655417 -1.3815455 -2.6318226 -4.1482658 -5.5670471 -6.2317352 -6.3545761][-5.2453427 -5.126101 -5.0294476 -4.9882612 -4.9756584 -4.7558117 -4.0863061 -3.6406159 -3.2306786 -3.5213199 -4.5535607 -5.6741905 -6.6424351 -6.9453692 -6.6497846][-6.0050874 -5.8528461 -5.8140783 -5.770072 -5.7603288 -5.8684444 -5.4417415 -5.0385823 -4.8266053 -4.8644218 -5.2773552 -5.9342055 -6.594243 -6.5548067 -6.1350555][-6.9343176 -6.1879 -5.7104387 -5.5291433 -5.4985762 -5.5023727 -5.3417859 -5.12231 -4.8575077 -4.8239021 -4.9710112 -5.2202291 -5.6449318 -5.5803 -5.1370974][-7.3638992 -6.2621393 -5.5269175 -5.1762567 -5.0434961 -5.0425987 -5.0543547 -4.9757905 -4.7089734 -4.6015124 -4.5955853 -4.4664063 -4.4296169 -4.2275648 -3.9174819]]...]
INFO - root - 2017-12-16 06:23:02.641958: step 34410, loss = 0.23, batch loss = 0.18 (28.5 examples/sec; 0.281 sec/batch; 23h:16m:04s remains)
INFO - root - 2017-12-16 06:23:05.455684: step 34420, loss = 0.23, batch loss = 0.17 (29.3 examples/sec; 0.273 sec/batch; 22h:37m:05s remains)
INFO - root - 2017-12-16 06:23:08.342510: step 34430, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 23h:20m:21s remains)
INFO - root - 2017-12-16 06:23:11.149491: step 34440, loss = 0.23, batch loss = 0.17 (28.1 examples/sec; 0.284 sec/batch; 23h:32m:57s remains)
INFO - root - 2017-12-16 06:23:13.995840: step 34450, loss = 0.32, batch loss = 0.26 (29.1 examples/sec; 0.275 sec/batch; 22h:44m:14s remains)
INFO - root - 2017-12-16 06:23:16.822326: step 34460, loss = 0.31, batch loss = 0.25 (28.8 examples/sec; 0.278 sec/batch; 23h:01m:35s remains)
INFO - root - 2017-12-16 06:23:19.608181: step 34470, loss = 0.36, batch loss = 0.30 (28.0 examples/sec; 0.286 sec/batch; 23h:38m:41s remains)
INFO - root - 2017-12-16 06:23:22.450713: step 34480, loss = 0.37, batch loss = 0.31 (28.9 examples/sec; 0.277 sec/batch; 22h:53m:24s remains)
INFO - root - 2017-12-16 06:23:25.291866: step 34490, loss = 0.30, batch loss = 0.24 (28.2 examples/sec; 0.284 sec/batch; 23h:30m:50s remains)
INFO - root - 2017-12-16 06:23:28.160040: step 34500, loss = 0.24, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 23h:34m:19s remains)
2017-12-16 06:23:28.608349: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9034281 -3.8619099 -3.8461161 -4.1123958 -4.4381723 -4.4577231 -4.3500109 -4.1656113 -3.959146 -3.4675708 -2.8944495 -2.4526381 -2.11588 -1.9573319 -1.881973][-3.3690262 -3.4028311 -3.4780447 -3.643353 -3.8540683 -4.1435413 -4.3014517 -4.04099 -3.751832 -3.4037991 -3.0787544 -2.6723502 -2.4457808 -2.2925189 -2.0818379][-2.7722962 -3.0257986 -3.224051 -3.2592554 -3.2583797 -3.2892051 -3.2169495 -3.1966403 -3.3174829 -3.4255519 -3.4727345 -3.2102995 -3.0785506 -2.9100657 -2.7151234][-2.243221 -2.6770148 -2.8858409 -2.7011533 -2.2454565 -2.0026307 -1.8040516 -1.7493625 -2.0476282 -2.6027875 -3.0151467 -3.3001127 -3.560787 -3.4986153 -3.315167][-2.1729779 -2.4647965 -2.2656167 -1.6680124 -0.93930292 -0.15968752 0.41906452 0.35068846 -0.19107628 -1.1672494 -2.0551116 -2.7514851 -3.2564797 -3.5924876 -3.5435591][-2.3759232 -2.4924483 -2.1622233 -1.0886655 0.43462563 1.575603 2.191802 2.2552695 1.744771 0.62831974 -0.7238214 -1.821197 -2.6594205 -3.3018785 -3.4783487][-2.7752371 -2.7108123 -2.1021507 -0.70454025 1.1150899 2.5885553 3.7095528 3.8217363 3.030973 1.7775497 0.22851372 -1.0347176 -2.1021442 -2.7406585 -2.9893847][-2.9536128 -2.9091411 -2.4035692 -1.0155485 0.87323141 2.6478338 3.8374252 4.0451937 3.4606643 2.1960907 0.69575262 -0.62208366 -1.8243961 -2.5209532 -2.8151021][-3.4365973 -3.372592 -2.9835205 -1.9579623 -0.607388 1.0822854 2.4548025 2.804564 2.4010496 1.230711 -0.19354486 -1.4555349 -2.4348185 -2.7843814 -2.772898][-3.6100225 -4.1019497 -4.3081555 -3.4957261 -2.1301849 -0.68212461 0.43699932 0.70842981 0.43891716 -0.39890718 -1.6387761 -2.9268489 -3.7340817 -3.8654463 -3.6535847][-4.1631541 -4.5306125 -4.607007 -4.5841212 -4.0135117 -2.6808026 -1.2817104 -1.0139616 -1.4386313 -2.3610463 -3.4636297 -4.3206697 -4.808219 -4.8469796 -4.4795375][-3.9942527 -4.675478 -5.1815858 -5.0907431 -4.5635381 -4.0595546 -3.3969271 -2.8261273 -2.7017748 -3.1833582 -4.0292921 -4.895607 -5.4924173 -5.288353 -4.7732959][-3.9697554 -4.513814 -4.88037 -4.9689465 -4.8181486 -4.3355823 -3.7532418 -3.3329017 -3.2419262 -3.4905128 -4.0005517 -4.4953361 -4.9742327 -4.9344935 -4.605113][-3.7089691 -4.1257386 -4.3002429 -4.2942486 -4.1470408 -3.618825 -3.0186543 -2.7215176 -2.5954852 -2.3971083 -2.5453141 -3.2618158 -3.9922147 -4.1002626 -3.9455929][-3.1381238 -3.1049881 -2.9813251 -3.1144924 -3.1098492 -2.751924 -2.3025377 -1.9241884 -1.6382527 -1.5487618 -1.7579038 -2.0050569 -2.4381361 -3.0007508 -3.2291565]]...]
INFO - root - 2017-12-16 06:23:31.429572: step 34510, loss = 0.28, batch loss = 0.22 (28.5 examples/sec; 0.281 sec/batch; 23h:15m:47s remains)
INFO - root - 2017-12-16 06:23:34.268387: step 34520, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 23h:19m:00s remains)
INFO - root - 2017-12-16 06:23:37.162704: step 34530, loss = 0.29, batch loss = 0.23 (28.8 examples/sec; 0.278 sec/batch; 23h:01m:08s remains)
INFO - root - 2017-12-16 06:23:40.033047: step 34540, loss = 0.23, batch loss = 0.17 (28.9 examples/sec; 0.277 sec/batch; 22h:56m:08s remains)
INFO - root - 2017-12-16 06:23:42.880177: step 34550, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.277 sec/batch; 22h:54m:39s remains)
INFO - root - 2017-12-16 06:23:45.667375: step 34560, loss = 0.35, batch loss = 0.29 (29.3 examples/sec; 0.273 sec/batch; 22h:34m:25s remains)
INFO - root - 2017-12-16 06:23:48.458472: step 34570, loss = 0.24, batch loss = 0.19 (29.8 examples/sec; 0.269 sec/batch; 22h:14m:57s remains)
INFO - root - 2017-12-16 06:23:51.283998: step 34580, loss = 0.30, batch loss = 0.24 (28.7 examples/sec; 0.279 sec/batch; 23h:04m:10s remains)
INFO - root - 2017-12-16 06:23:54.142105: step 34590, loss = 0.41, batch loss = 0.35 (29.3 examples/sec; 0.273 sec/batch; 22h:35m:27s remains)
INFO - root - 2017-12-16 06:23:56.991989: step 34600, loss = 0.21, batch loss = 0.15 (27.7 examples/sec; 0.289 sec/batch; 23h:53m:39s remains)
2017-12-16 06:23:57.460867: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0858111 -2.8933043 -2.7512431 -2.5195026 -2.3951182 -2.2309909 -2.2432117 -2.5278759 -2.7058313 -2.8275926 -2.8591387 -2.7124343 -2.3599052 -2.3349562 -2.5858068][-3.198905 -3.1020837 -3.03291 -2.790463 -2.530467 -2.2651634 -2.2221434 -2.3641644 -2.4171476 -2.6536112 -2.918838 -2.8922324 -2.6496248 -2.738616 -3.0836964][-3.3856995 -3.2764368 -3.1049109 -2.8387017 -2.5224478 -2.2059708 -2.0835407 -2.1068048 -2.2269349 -2.3472073 -2.4779911 -2.7162459 -2.9312959 -3.1478276 -3.476964][-3.199779 -3.3007653 -3.4033356 -3.296607 -3.0669737 -2.6349106 -2.3124983 -2.0655026 -1.9105334 -2.0466533 -2.3329306 -2.6889579 -2.9876485 -3.4331658 -3.8781354][-2.762845 -3.016252 -3.2297938 -3.1352448 -2.8356762 -2.4021688 -1.9526751 -1.5195916 -1.2680869 -1.3669248 -1.5413191 -1.9497874 -2.3548107 -2.8644338 -3.2325253][-2.0250628 -2.5284724 -2.9067953 -2.93194 -2.6134839 -2.0594034 -1.4219005 -0.89496017 -0.712404 -0.73035574 -0.81754518 -1.1613245 -1.5253863 -2.0088472 -2.3250825][-1.4293299 -1.9741011 -2.3464661 -2.4308033 -2.2207024 -1.7374372 -1.1092803 -0.49637032 -0.26177645 -0.23817253 -0.23628521 -0.39653158 -0.71762323 -1.2008898 -1.3989158][-0.91955161 -1.4023118 -1.7665696 -1.9007604 -1.8265491 -1.3933065 -0.82173371 -0.35682726 -0.16573858 -0.075227737 0.12252617 0.08491993 -0.16095352 -0.56864429 -0.67470241][-0.66065216 -1.0664015 -1.300972 -1.3864529 -1.3719516 -1.0708513 -0.66463828 -0.32157326 -0.13349724 0.049833298 0.4666667 0.51426268 0.29902506 -0.062335968 -0.29237652][-0.77851629 -0.8619895 -0.89313507 -0.95522666 -0.90790343 -0.72190928 -0.48268723 -0.3272562 -0.22019529 -0.0927124 0.28155088 0.43315554 0.38336658 0.0546937 -0.25026226][-1.3843064 -1.1878581 -1.0703034 -0.94573379 -0.80183864 -0.611197 -0.451046 -0.49107647 -0.63954163 -0.57279611 -0.26782227 -0.22659206 -0.31416655 -0.50828004 -0.74374485][-2.3479202 -1.8089173 -1.3374264 -1.0634983 -0.84470963 -0.62168193 -0.6118958 -0.80579543 -0.96732306 -1.0628693 -1.0024524 -1.0368268 -1.0744491 -1.271492 -1.517056][-2.9535418 -2.167629 -1.5044549 -1.0885944 -0.648031 -0.43095183 -0.43282223 -0.87427211 -1.287452 -1.4734123 -1.4898958 -1.6938868 -1.8837922 -2.01461 -2.1629694][-3.5784523 -2.4817443 -1.4677417 -0.99622083 -0.62521052 -0.4510951 -0.64172912 -1.2484279 -1.7188146 -2.0824108 -2.3665245 -2.4600029 -2.3443763 -2.3530266 -2.4463654][-3.7276824 -2.5739484 -1.5689592 -0.90782356 -0.5334239 -0.52428842 -1.00933 -1.7887414 -2.4556947 -2.830009 -2.9990695 -2.9311962 -2.722326 -2.4994144 -2.4138093]]...]
INFO - root - 2017-12-16 06:24:00.291494: step 34610, loss = 0.29, batch loss = 0.23 (28.7 examples/sec; 0.279 sec/batch; 23h:04m:19s remains)
INFO - root - 2017-12-16 06:24:03.129080: step 34620, loss = 0.26, batch loss = 0.20 (27.7 examples/sec; 0.289 sec/batch; 23h:53m:13s remains)
INFO - root - 2017-12-16 06:24:06.004171: step 34630, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 23h:38m:50s remains)
INFO - root - 2017-12-16 06:24:08.856924: step 34640, loss = 0.32, batch loss = 0.26 (29.5 examples/sec; 0.271 sec/batch; 22h:24m:21s remains)
INFO - root - 2017-12-16 06:24:11.702925: step 34650, loss = 0.25, batch loss = 0.19 (28.9 examples/sec; 0.277 sec/batch; 22h:55m:23s remains)
INFO - root - 2017-12-16 06:24:14.573916: step 34660, loss = 0.22, batch loss = 0.16 (29.2 examples/sec; 0.274 sec/batch; 22h:38m:27s remains)
INFO - root - 2017-12-16 06:24:17.386848: step 34670, loss = 0.23, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 23h:01m:06s remains)
INFO - root - 2017-12-16 06:24:20.210661: step 34680, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 23h:39m:52s remains)
INFO - root - 2017-12-16 06:24:23.077381: step 34690, loss = 0.38, batch loss = 0.33 (25.7 examples/sec; 0.312 sec/batch; 25h:46m:20s remains)
INFO - root - 2017-12-16 06:24:25.957546: step 34700, loss = 0.25, batch loss = 0.19 (26.9 examples/sec; 0.298 sec/batch; 24h:38m:20s remains)
2017-12-16 06:24:26.423976: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2247381 -4.120141 -4.1029911 -4.5170712 -5.1448402 -5.8450136 -6.447464 -6.5371494 -6.6138096 -6.1688151 -5.6397161 -5.26727 -4.6610003 -3.9181533 -3.2128425][-4.84532 -5.3219047 -5.7878485 -6.0246053 -6.7914195 -6.9489417 -6.9629717 -6.8353195 -6.4420576 -6.0627408 -5.8406005 -5.3368554 -4.53507 -3.8951325 -3.3545151][-5.4575896 -6.0837693 -6.5934982 -6.7871094 -6.7582884 -6.4064169 -6.3837404 -6.0283561 -5.6382933 -5.3242106 -4.9417238 -4.8832459 -4.7992125 -4.3324451 -3.7295425][-5.8992057 -6.8224783 -7.1817789 -6.9066839 -6.3041897 -5.2532277 -4.413579 -3.771265 -3.4995213 -3.9186907 -4.4827971 -4.6269064 -4.6117792 -4.6282744 -4.3573127][-5.9006953 -6.8740745 -6.9720497 -6.1489282 -4.6153817 -2.8675094 -1.624424 -0.72533035 -0.77826023 -1.8021491 -2.9084883 -4.1182566 -4.9447045 -4.9301119 -4.4152451][-4.6076818 -5.3822517 -5.4820728 -4.38989 -2.0625212 0.43959045 2.3016338 3.1496572 2.4743562 0.75983906 -1.5439529 -3.5055902 -4.6534114 -5.1202154 -4.8027868][-3.3044858 -3.9552124 -3.6457891 -2.1014695 0.14661884 2.742425 4.8364105 5.6014433 4.6542521 2.3034692 -0.44971156 -2.9306569 -4.600112 -5.030097 -4.5528812][-2.656209 -3.3546023 -3.1653924 -1.4642663 1.135612 3.8895121 5.7822113 5.9934959 4.8115206 2.3247523 -0.72900057 -3.2240505 -4.5906682 -4.9036527 -4.4583192][-2.5194335 -2.9924383 -2.5406742 -1.4106369 0.23581839 2.6332674 4.5448189 4.8517256 3.5048094 0.86299419 -1.9453528 -4.1207633 -5.3154578 -5.3471341 -4.6741128][-2.4656768 -3.107142 -2.8622522 -1.8871677 -0.39020729 1.0268974 1.8095222 2.061327 1.0183563 -1.2643311 -3.666872 -5.4885311 -6.1407557 -5.9056597 -5.1469965][-3.1832724 -3.4853046 -3.3273849 -3.1216128 -2.7065094 -1.5410459 -0.40440512 -0.51247048 -1.7941833 -3.3336358 -4.9480343 -6.3305049 -6.7843552 -6.2661858 -5.3577952][-4.1892781 -4.4075561 -4.522758 -4.1439595 -3.5904171 -3.3240609 -3.0900629 -3.0886202 -3.6323547 -4.7892485 -6.0483513 -6.7226448 -6.7166557 -6.1335125 -5.3053622][-5.1063437 -5.1997509 -5.3032131 -5.246635 -5.1439476 -4.5646229 -4.1197639 -4.4726644 -5.0823312 -5.6718707 -6.4031835 -6.7769017 -6.6115656 -5.9590454 -5.2175341][-5.8627272 -5.8109021 -5.7717462 -5.6964006 -5.5466366 -5.1385088 -4.9908528 -4.8865352 -5.0815477 -5.653388 -6.0683813 -6.0801644 -5.8285995 -5.2540011 -4.687171][-6.4567137 -6.3626246 -6.2182593 -5.9650702 -5.6562977 -5.2661057 -5.0144844 -4.7520823 -4.7777867 -5.0675588 -5.2912011 -5.1863766 -4.822515 -4.2661977 -3.7844517]]...]
INFO - root - 2017-12-16 06:24:29.194937: step 34710, loss = 0.37, batch loss = 0.31 (29.3 examples/sec; 0.273 sec/batch; 22h:33m:45s remains)
INFO - root - 2017-12-16 06:24:32.025827: step 34720, loss = 0.29, batch loss = 0.23 (28.4 examples/sec; 0.282 sec/batch; 23h:17m:35s remains)
INFO - root - 2017-12-16 06:24:34.835534: step 34730, loss = 0.27, batch loss = 0.21 (27.9 examples/sec; 0.287 sec/batch; 23h:44m:18s remains)
INFO - root - 2017-12-16 06:24:37.643872: step 34740, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.285 sec/batch; 23h:35m:05s remains)
INFO - root - 2017-12-16 06:24:40.508766: step 34750, loss = 0.32, batch loss = 0.26 (28.4 examples/sec; 0.282 sec/batch; 23h:18m:49s remains)
INFO - root - 2017-12-16 06:24:43.339608: step 34760, loss = 0.45, batch loss = 0.39 (28.6 examples/sec; 0.280 sec/batch; 23h:07m:33s remains)
INFO - root - 2017-12-16 06:24:46.120076: step 34770, loss = 0.34, batch loss = 0.29 (28.6 examples/sec; 0.279 sec/batch; 23h:06m:53s remains)
INFO - root - 2017-12-16 06:24:48.944794: step 34780, loss = 0.22, batch loss = 0.16 (28.1 examples/sec; 0.285 sec/batch; 23h:31m:53s remains)
INFO - root - 2017-12-16 06:24:51.770207: step 34790, loss = 0.19, batch loss = 0.14 (29.0 examples/sec; 0.276 sec/batch; 22h:47m:24s remains)
INFO - root - 2017-12-16 06:24:54.609417: step 34800, loss = 0.23, batch loss = 0.17 (29.1 examples/sec; 0.275 sec/batch; 22h:45m:23s remains)
2017-12-16 06:24:55.047182: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2429311 -2.3462408 -2.4574111 -2.5211411 -2.5811765 -2.565577 -2.5387 -2.5171833 -2.5449166 -2.5010884 -2.391499 -2.3656142 -2.277231 -2.2253034 -2.141674][-2.50188 -2.6733341 -2.8508821 -3.0153079 -3.1223989 -3.1418304 -3.125896 -3.0260434 -2.9338832 -2.8720918 -2.8090758 -2.7171321 -2.5928 -2.5264401 -2.398288][-2.9580789 -3.3070598 -3.56422 -3.7227285 -3.840363 -3.8262916 -3.7709837 -3.767869 -3.6275666 -3.4287784 -3.3154285 -3.2564158 -3.169291 -3.0182958 -2.8998408][-3.47336 -3.7838061 -4.0157089 -4.1089244 -4.1487007 -4.0593743 -3.8985922 -3.7602181 -3.6501465 -3.625088 -3.4789238 -3.4688725 -3.5051143 -3.4798024 -3.372262][-4.0090985 -4.1808109 -4.2728553 -4.1444588 -3.9243944 -3.6953225 -3.4510958 -3.1453056 -2.9376888 -2.9788036 -2.97964 -3.1101298 -3.2336526 -3.5360434 -3.6925755][-4.0016341 -4.1605382 -4.0192122 -3.6816916 -3.1993403 -2.7037368 -2.3536382 -2.1607387 -2.0696869 -1.917444 -1.8888869 -2.2667975 -2.7458277 -3.1922774 -3.4299147][-3.6465166 -3.6458304 -3.3662915 -2.7330356 -1.9158533 -1.1203089 -0.6533165 -0.4601593 -0.50490117 -0.67512608 -0.84772921 -1.2559192 -1.9524045 -2.807364 -3.5533676][-3.2416906 -3.0763559 -2.5219793 -1.7103205 -0.65323853 0.27094984 0.8044591 1.0662999 1.102603 0.8180089 0.39422035 -0.48126221 -1.5190041 -2.4966817 -3.338655][-2.2534955 -2.1835277 -1.634491 -0.798686 -0.029035091 0.86237144 1.566535 1.8723793 1.723742 1.5231538 1.1336813 0.059933662 -1.3218009 -2.5916848 -3.4812503][-1.7016625 -1.6124554 -1.3175907 -0.71154523 0.0044255257 0.82159758 1.5064335 1.9675603 1.9667554 1.7426329 1.1245518 -0.12242985 -1.5303152 -2.7799559 -3.7015202][-1.2633815 -1.107594 -0.9642427 -0.79696488 -0.43933177 0.32478285 1.0540171 1.6320367 1.6133165 1.3500347 0.71609163 -0.65728736 -2.1494429 -3.2185321 -3.9168305][-0.99422932 -0.78830242 -0.75621557 -0.7647512 -0.75747323 -0.41873169 0.021890163 0.59290504 0.84912729 0.59503126 -0.15753317 -1.2751355 -2.455792 -3.3872933 -3.8992596][-1.0380895 -0.80820107 -0.93917394 -1.2240794 -1.3373988 -1.0504727 -0.43096805 -0.059161186 0.054071903 0.07980442 -0.35067987 -1.2033317 -2.1274178 -2.7645547 -3.0642822][-1.1625323 -0.84412384 -0.90188026 -1.2195101 -1.2667534 -1.0609438 -0.69750953 -0.242877 -0.02511549 -0.00873661 -0.44927073 -0.97310567 -1.6313367 -2.0112185 -2.2380817][-0.87097859 -0.40428495 -0.40785742 -0.7648406 -0.92092061 -0.73501778 -0.40426922 -0.18922853 -0.16997004 -0.062068462 -0.26021481 -0.75464368 -1.3334939 -1.5271068 -1.5454211]]...]
INFO - root - 2017-12-16 06:24:57.926597: step 34810, loss = 0.36, batch loss = 0.31 (26.0 examples/sec; 0.308 sec/batch; 25h:26m:04s remains)
INFO - root - 2017-12-16 06:25:00.806362: step 34820, loss = 0.29, batch loss = 0.23 (27.9 examples/sec; 0.287 sec/batch; 23h:41m:33s remains)
INFO - root - 2017-12-16 06:25:03.642636: step 34830, loss = 0.20, batch loss = 0.14 (27.9 examples/sec; 0.286 sec/batch; 23h:40m:54s remains)
INFO - root - 2017-12-16 06:25:06.554261: step 34840, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.280 sec/batch; 23h:08m:42s remains)
INFO - root - 2017-12-16 06:25:09.393051: step 34850, loss = 0.25, batch loss = 0.19 (29.1 examples/sec; 0.275 sec/batch; 22h:46m:05s remains)
INFO - root - 2017-12-16 06:25:12.150401: step 34860, loss = 0.34, batch loss = 0.28 (29.1 examples/sec; 0.275 sec/batch; 22h:45m:17s remains)
INFO - root - 2017-12-16 06:25:15.010597: step 34870, loss = 0.46, batch loss = 0.40 (28.8 examples/sec; 0.278 sec/batch; 22h:59m:08s remains)
INFO - root - 2017-12-16 06:25:17.796758: step 34880, loss = 0.22, batch loss = 0.16 (28.0 examples/sec; 0.285 sec/batch; 23h:35m:42s remains)
INFO - root - 2017-12-16 06:25:20.623614: step 34890, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 23h:21m:20s remains)
INFO - root - 2017-12-16 06:25:23.482393: step 34900, loss = 0.23, batch loss = 0.17 (25.6 examples/sec; 0.313 sec/batch; 25h:50m:24s remains)
2017-12-16 06:25:23.953954: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2265611 -4.1220016 -4.125874 -4.1966009 -4.357244 -4.3334751 -4.2584453 -4.4226475 -4.5528584 -4.3778687 -4.1688213 -3.9804785 -3.701925 -3.412061 -3.1733055][-3.8680689 -3.7479973 -4.0980186 -4.3675179 -4.3991542 -4.3612947 -4.1266093 -3.8619676 -3.7918079 -3.8833842 -3.8828621 -3.6499157 -3.4561167 -3.2329082 -3.1011691][-3.5472903 -3.8150299 -4.180511 -4.6737342 -4.7700224 -4.2783165 -3.6863623 -3.1294065 -2.7675309 -2.9426467 -3.2295637 -3.2972977 -3.2252786 -3.2327719 -3.4235497][-3.1417329 -3.6251488 -4.51449 -4.95041 -4.5952764 -3.8192425 -2.7704792 -2.1716282 -2.00726 -2.1354508 -2.5104795 -3.1509969 -3.5961325 -3.3901896 -3.2219472][-2.8967595 -3.5255437 -4.2298794 -4.6414003 -4.1408677 -2.9689984 -1.5387506 -0.69461727 -0.37949181 -1.013638 -2.0773928 -2.9625852 -3.4060235 -3.6739712 -3.8198659][-2.9024162 -3.966217 -4.5688848 -4.5305634 -3.5989542 -1.8967764 0.0699625 1.2104712 1.4655085 0.57036686 -0.72328234 -2.4349155 -3.966769 -4.3223577 -4.1833906][-2.7019987 -4.0362267 -4.6959143 -4.2652941 -2.7863917 -0.76618433 1.4262767 2.7009583 2.8896122 1.7612853 -0.039325237 -1.8984451 -3.3656437 -4.183836 -4.4577408][-2.089175 -3.5398879 -4.7468863 -4.512249 -2.6849542 -0.068270683 2.2158651 3.5377755 3.6147652 2.1904187 0.0994277 -1.7626514 -3.0693457 -3.7037435 -4.0932245][-2.3709345 -3.5674157 -4.3209524 -4.3752751 -3.1521835 -0.68962383 1.8389363 3.1041088 3.1502957 1.9223084 -0.20751572 -2.2597942 -3.6530633 -3.976742 -3.7328854][-2.1263294 -3.5554705 -4.7526975 -4.5890641 -3.237931 -1.2720661 0.72551441 2.0478759 1.8942428 0.45099831 -1.4421167 -3.3225336 -4.6062708 -4.9027305 -4.5088596][-2.0868225 -3.2503557 -4.5237055 -4.4728875 -3.6710687 -2.028553 -0.38407803 0.09719038 -0.28780174 -1.4418747 -3.1220255 -4.4869776 -5.2509727 -5.3303785 -4.7862196][-1.7234447 -2.862401 -4.1743326 -4.4968209 -4.0933938 -3.016705 -2.3073041 -1.8667338 -2.1947753 -3.5150914 -4.7489448 -5.5882635 -5.6764522 -5.1603503 -4.4456382][-1.4734025 -2.5020981 -4.1523619 -4.8123336 -4.8855309 -4.2842784 -3.5780251 -3.8952003 -4.6328053 -5.2874937 -5.9944096 -6.5725584 -6.4984641 -5.8053346 -4.6259727][-1.3861291 -2.3061843 -3.83493 -4.71435 -5.3106637 -5.249351 -5.3114262 -5.5687284 -5.9116659 -6.5005207 -6.9381924 -7.1061306 -6.7809162 -6.0515752 -5.1222982][-1.6741834 -2.1254964 -3.4491534 -4.4667497 -5.30437 -5.8541355 -6.0881319 -6.5482903 -6.9213219 -7.0322132 -6.9726744 -6.7209039 -6.3182454 -5.7333317 -4.7619348]]...]
INFO - root - 2017-12-16 06:25:26.743793: step 34910, loss = 0.26, batch loss = 0.20 (29.3 examples/sec; 0.273 sec/batch; 22h:32m:53s remains)
INFO - root - 2017-12-16 06:25:29.590216: step 34920, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.284 sec/batch; 23h:29m:02s remains)
INFO - root - 2017-12-16 06:25:32.426873: step 34930, loss = 0.28, batch loss = 0.22 (28.8 examples/sec; 0.277 sec/batch; 22h:55m:32s remains)
INFO - root - 2017-12-16 06:25:35.245920: step 34940, loss = 0.26, batch loss = 0.20 (29.5 examples/sec; 0.271 sec/batch; 22h:24m:58s remains)
INFO - root - 2017-12-16 06:25:38.109356: step 34950, loss = 0.27, batch loss = 0.22 (28.0 examples/sec; 0.286 sec/batch; 23h:38m:32s remains)
INFO - root - 2017-12-16 06:25:40.968614: step 34960, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.285 sec/batch; 23h:35m:23s remains)
INFO - root - 2017-12-16 06:25:43.773305: step 34970, loss = 0.22, batch loss = 0.16 (29.0 examples/sec; 0.276 sec/batch; 22h:47m:09s remains)
INFO - root - 2017-12-16 06:25:46.604772: step 34980, loss = 0.31, batch loss = 0.25 (26.3 examples/sec; 0.305 sec/batch; 25h:10m:13s remains)
INFO - root - 2017-12-16 06:25:49.439545: step 34990, loss = 0.27, batch loss = 0.21 (28.6 examples/sec; 0.279 sec/batch; 23h:05m:37s remains)
INFO - root - 2017-12-16 06:25:52.261281: step 35000, loss = 0.29, batch loss = 0.23 (28.3 examples/sec; 0.282 sec/batch; 23h:20m:24s remains)
2017-12-16 06:25:52.765969: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0951123 -2.9673886 -2.8348889 -2.6563864 -2.6098366 -2.6195865 -2.8423526 -2.8951092 -2.8094206 -2.7523611 -2.6784883 -2.8297892 -2.9398975 -3.1024525 -3.4254937][-3.6655326 -3.31678 -2.991468 -2.7758746 -2.7622304 -2.7751138 -3.004014 -3.118995 -3.2046857 -3.2897477 -3.2763815 -3.3892405 -3.4333732 -3.6153388 -3.8279862][-4.1370115 -3.6346087 -3.0723128 -2.7044625 -2.5785995 -2.5974519 -2.8003035 -2.9863758 -3.236506 -3.3941529 -3.4460452 -3.6866815 -3.8943567 -4.0509334 -4.0061278][-4.5833745 -3.8964884 -3.1545005 -2.5628061 -2.1689062 -2.0915613 -2.2545452 -2.3429036 -2.4803309 -2.8069212 -3.244319 -3.5643017 -3.6903069 -3.8438525 -3.8245168][-4.9185729 -4.2453942 -3.4557033 -2.5579898 -1.8520188 -1.5252469 -1.3111989 -1.3119659 -1.6434641 -2.1042414 -2.4423728 -2.77951 -3.005635 -3.1066651 -2.889004][-4.6550522 -4.1072054 -3.2176359 -2.1211097 -1.0856419 -0.46328712 -0.076569557 -0.023115635 -0.32812309 -0.92078829 -1.5438404 -1.9239564 -1.8976464 -1.8383298 -1.5771656][-4.19096 -3.7439666 -2.9759178 -1.7618098 -0.47830176 0.38279486 0.96072149 1.0217161 0.559886 -0.029779434 -0.53298306 -0.83236194 -0.72416973 -0.36540747 0.13171577][-3.9880371 -3.6235409 -2.9553752 -1.735162 -0.50113392 0.55620575 1.3515129 1.3552294 0.80677223 0.25886631 -0.047440529 -0.023099422 0.34741306 0.96937943 1.4081707][-4.0456305 -3.9282413 -3.3522077 -2.1988149 -1.1330156 -0.11730671 0.6119585 0.6335988 0.32378197 -0.053981781 -0.03842926 0.30829096 0.87493467 1.6236238 1.9820871][-4.0659533 -3.9626653 -3.6044738 -2.7918615 -2.0466433 -1.2236736 -0.7028966 -0.71991086 -0.795362 -0.95137954 -0.69172621 -0.22736454 0.5603056 1.2929358 1.5262585][-4.2336206 -4.2695289 -4.1671886 -3.6990724 -3.0842483 -2.4341338 -2.0783074 -2.2019076 -2.3302014 -2.3514714 -1.8991971 -1.4024498 -0.53358769 0.18514633 0.30754852][-4.4798508 -4.4878573 -4.3655777 -4.0157595 -3.8027554 -3.4346824 -3.4215727 -3.6857243 -3.7134662 -3.5634596 -3.1666131 -2.6511121 -1.7856686 -1.1404161 -1.0474176][-4.3318419 -4.2069716 -4.1747985 -3.9046779 -3.8604894 -3.8071043 -4.1061239 -4.7354465 -5.0533843 -5.0073261 -4.5283208 -3.9811454 -3.3508377 -2.7937279 -2.4801083][-4.0168171 -3.7743268 -3.667789 -3.5048594 -3.6734781 -3.914618 -4.6715732 -5.6622953 -6.3195496 -6.3873286 -5.8848166 -5.3704495 -4.834023 -4.398737 -4.0481811][-3.4342597 -3.0206633 -2.7887135 -2.7892461 -3.2790887 -3.8836496 -4.9656615 -6.1289625 -7.0032787 -7.2332797 -6.8338404 -6.1647744 -5.4145937 -4.9277563 -4.6550765]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-35000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-35000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 06:25:56.084063: step 35010, loss = 0.21, batch loss = 0.15 (28.9 examples/sec; 0.277 sec/batch; 22h:52m:10s remains)
INFO - root - 2017-12-16 06:25:58.929545: step 35020, loss = 0.38, batch loss = 0.32 (28.1 examples/sec; 0.285 sec/batch; 23h:32m:41s remains)
INFO - root - 2017-12-16 06:26:01.770814: step 35030, loss = 0.19, batch loss = 0.14 (25.5 examples/sec; 0.314 sec/batch; 25h:55m:17s remains)
INFO - root - 2017-12-16 06:26:04.594596: step 35040, loss = 0.35, batch loss = 0.29 (28.8 examples/sec; 0.278 sec/batch; 22h:57m:16s remains)
INFO - root - 2017-12-16 06:26:07.405176: step 35050, loss = 0.27, batch loss = 0.22 (28.6 examples/sec; 0.280 sec/batch; 23h:08m:57s remains)
INFO - root - 2017-12-16 06:26:10.245864: step 35060, loss = 0.30, batch loss = 0.25 (27.5 examples/sec; 0.291 sec/batch; 24h:03m:53s remains)
INFO - root - 2017-12-16 06:26:13.103521: step 35070, loss = 0.30, batch loss = 0.24 (28.5 examples/sec; 0.281 sec/batch; 23h:11m:09s remains)
INFO - root - 2017-12-16 06:26:15.928270: step 35080, loss = 0.34, batch loss = 0.28 (27.4 examples/sec; 0.292 sec/batch; 24h:09m:19s remains)
INFO - root - 2017-12-16 06:26:18.734891: step 35090, loss = 0.22, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 23h:02m:03s remains)
INFO - root - 2017-12-16 06:26:21.590420: step 35100, loss = 0.27, batch loss = 0.21 (29.4 examples/sec; 0.272 sec/batch; 22h:30m:28s remains)
2017-12-16 06:26:22.058601: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3700366 -2.9655714 -2.587903 -2.5634236 -2.8624554 -3.4291239 -4.2295494 -4.9106083 -5.6260338 -5.9498348 -6.0618992 -5.9163556 -5.6057029 -5.4855552 -5.0460448][-3.3949623 -2.96025 -2.4077005 -2.2081444 -2.46712 -3.0185416 -3.8851361 -4.61885 -5.3814478 -5.8213139 -5.9767828 -5.8709469 -5.6386328 -5.6480556 -5.1883087][-3.4662738 -2.9406948 -2.2016609 -1.7582235 -1.8556185 -2.2811546 -3.1892271 -3.97571 -4.7927942 -5.4077148 -5.804832 -5.9509888 -5.7238216 -5.5821786 -5.1134605][-3.1966746 -2.6295915 -1.7612112 -1.2343976 -1.2444763 -1.3008332 -1.9571531 -2.7773376 -3.7261446 -4.4709997 -5.0452714 -5.53193 -5.5609326 -5.3100567 -4.6725321][-2.6850114 -1.8437266 -0.69241738 0.035685539 0.33465624 0.51779795 0.16111612 -0.57502317 -1.7294378 -2.8618357 -3.9024611 -4.6772184 -4.8019409 -4.5702682 -3.9226663][-2.2615707 -1.2862701 0.024456978 0.99279785 1.7474489 2.3659773 2.3177528 1.5927334 0.24467659 -1.1516623 -2.5476823 -3.8365767 -4.2666335 -4.0494175 -3.5278482][-2.009604 -1.0004241 0.32268715 1.3107085 2.1642976 3.0408406 3.4110098 3.0446429 1.870872 0.12715149 -1.6895609 -3.2594314 -3.918237 -3.8010559 -3.4435024][-2.005163 -1.2886593 -0.2402072 0.84955835 1.9879675 2.982491 3.4433947 3.074863 1.9335499 0.22694874 -1.6249447 -3.1641695 -3.8342392 -3.9045756 -3.5453136][-2.166786 -1.6739669 -0.9771986 -0.092707157 0.88751554 1.8982348 2.5542479 2.2455835 1.1188598 -0.43438601 -2.0154676 -3.5811799 -4.3181906 -4.2310262 -3.7367291][-2.8099489 -2.4290543 -2.0510957 -1.4306061 -0.637274 0.12929583 0.5574708 0.44406319 -0.36929035 -1.8251619 -3.219522 -4.421279 -4.97509 -4.8724742 -4.3006744][-3.5581546 -3.2999651 -2.9743359 -2.4406302 -1.8959591 -1.3455255 -0.98496985 -1.1653657 -1.8318107 -3.0093417 -4.1863031 -5.1642237 -5.5974069 -5.4648337 -4.9233055][-3.92576 -3.6062827 -3.2150497 -2.743319 -2.4306059 -2.1997168 -2.045547 -2.2226307 -2.796114 -3.7492533 -4.606389 -5.4835849 -5.9817171 -5.953681 -5.3978672][-3.9913113 -3.686233 -3.3453546 -2.8661966 -2.5777192 -2.4499016 -2.549417 -2.7106721 -3.073149 -3.8614938 -4.4923296 -5.0506668 -5.3485456 -5.389729 -5.0029912][-4.043891 -3.7512636 -3.3464308 -2.8659039 -2.6680036 -2.3906262 -2.3636429 -2.5726905 -2.9519582 -3.3624346 -3.6092494 -4.1053939 -4.4030628 -4.4080348 -4.1562052][-4.0224805 -3.6083837 -3.2131124 -2.815304 -2.5136886 -2.248487 -2.1087952 -2.0968449 -2.2833898 -2.6526198 -2.9172039 -3.2437639 -3.533937 -3.6165943 -3.43114]]...]
INFO - root - 2017-12-16 06:26:24.937217: step 35110, loss = 0.35, batch loss = 0.29 (28.4 examples/sec; 0.281 sec/batch; 23h:14m:02s remains)
INFO - root - 2017-12-16 06:26:27.805600: step 35120, loss = 0.29, batch loss = 0.23 (27.9 examples/sec; 0.287 sec/batch; 23h:43m:04s remains)
INFO - root - 2017-12-16 06:26:30.683539: step 35130, loss = 0.31, batch loss = 0.25 (25.6 examples/sec; 0.313 sec/batch; 25h:49m:41s remains)
INFO - root - 2017-12-16 06:26:33.546906: step 35140, loss = 0.25, batch loss = 0.19 (27.6 examples/sec; 0.290 sec/batch; 23h:55m:55s remains)
INFO - root - 2017-12-16 06:26:36.400095: step 35150, loss = 0.44, batch loss = 0.39 (29.2 examples/sec; 0.274 sec/batch; 22h:37m:56s remains)
INFO - root - 2017-12-16 06:26:39.266400: step 35160, loss = 0.41, batch loss = 0.35 (27.9 examples/sec; 0.287 sec/batch; 23h:41m:32s remains)
INFO - root - 2017-12-16 06:26:42.093200: step 35170, loss = 0.25, batch loss = 0.19 (25.6 examples/sec; 0.313 sec/batch; 25h:50m:52s remains)
INFO - root - 2017-12-16 06:26:44.913915: step 35180, loss = 0.32, batch loss = 0.26 (28.8 examples/sec; 0.278 sec/batch; 22h:57m:47s remains)
INFO - root - 2017-12-16 06:26:47.710764: step 35190, loss = 0.33, batch loss = 0.27 (28.6 examples/sec; 0.280 sec/batch; 23h:06m:46s remains)
INFO - root - 2017-12-16 06:26:50.560894: step 35200, loss = 0.32, batch loss = 0.26 (26.7 examples/sec; 0.300 sec/batch; 24h:45m:11s remains)
2017-12-16 06:26:51.024112: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0114198 -3.1985183 -3.530746 -3.8966625 -4.2602563 -4.5254312 -4.6424694 -4.8582821 -4.9779339 -5.13319 -5.2138171 -5.2279334 -5.2755389 -5.0883946 -4.9069281][-2.6114631 -2.7827127 -3.1163626 -3.3494265 -3.5449586 -3.7259014 -3.8674119 -3.9932477 -3.993767 -4.27876 -4.5756769 -4.710268 -4.8358822 -4.6386285 -4.4277196][-2.4450736 -2.6683922 -2.9336863 -3.1839457 -3.2699332 -3.1271195 -2.7917795 -2.7925584 -2.9315584 -3.1071081 -3.5049746 -3.8628211 -4.1430926 -4.1195583 -3.8590209][-2.3385618 -2.6263318 -2.8548353 -2.962698 -2.7830298 -2.3630431 -1.7582195 -1.4005911 -1.4970665 -1.9467394 -2.4385736 -2.8623719 -3.224457 -3.1811564 -2.9623666][-2.3743403 -2.7381711 -2.9539733 -2.8474281 -2.4155507 -1.5767872 -0.65661836 -0.14580154 -0.28565264 -0.72732973 -1.4120076 -2.0300822 -2.388247 -2.4987671 -2.4006763][-2.7426214 -3.0415721 -3.0724463 -2.7164052 -1.8850057 -0.74330544 0.28807497 0.91058493 0.79396105 0.14707804 -0.72212076 -1.4357829 -1.8013768 -1.9069989 -1.7590501][-2.9436848 -3.1593332 -3.1056149 -2.5775704 -1.4589386 -0.13722181 0.90587139 1.5238047 1.3160586 0.68397951 -0.10200596 -0.87350154 -1.3043416 -1.4471903 -1.3765185][-3.0128574 -3.0622435 -2.9434986 -2.3584201 -1.4256756 -0.025481224 1.0516462 1.6452198 1.5368495 0.9175477 0.11619997 -0.46296215 -0.81465507 -1.0636311 -1.13539][-2.9416397 -2.906352 -2.6455886 -2.091223 -1.4042149 -0.43486381 0.41940498 1.0190506 1.0608835 0.70334148 0.064406872 -0.44923329 -0.77176523 -1.0275133 -1.0747604][-2.7776756 -2.7488635 -2.9221368 -2.6871805 -2.0851572 -1.5314245 -1.0307407 -0.37196779 -0.17907524 -0.39571571 -0.67360282 -1.053911 -1.2705927 -1.455658 -1.3840089][-2.8665912 -3.1096163 -3.4808238 -3.6667249 -3.8889618 -3.5037038 -2.8706198 -2.3316145 -2.0670466 -1.9917157 -1.9867117 -2.1408041 -2.1762512 -2.1968365 -1.9752104][-3.339071 -3.8350432 -4.5838494 -5.2762637 -5.6025634 -5.6045685 -5.3732944 -4.6546245 -4.0394807 -3.7227466 -3.5557611 -3.4961429 -3.4288859 -3.3461328 -2.9640102][-3.8373494 -4.682888 -5.8421116 -6.702394 -7.2452893 -7.33939 -7.0567503 -6.55113 -5.9863038 -5.4868131 -5.1735148 -5.0624051 -4.9572067 -4.8791451 -4.4618587][-4.3126593 -5.2494812 -6.3319731 -7.304368 -7.8962479 -8.0289364 -7.789237 -7.3513441 -6.9087071 -6.5811634 -6.2246118 -6.1360216 -6.0000219 -5.8520169 -5.50726][-4.6054282 -5.4870682 -6.4294825 -7.2204933 -7.6421118 -7.7702236 -7.5921516 -7.3739681 -7.051003 -6.8764486 -6.663661 -6.5494232 -6.4708118 -6.4060926 -6.1329656]]...]
INFO - root - 2017-12-16 06:26:53.842201: step 35210, loss = 0.21, batch loss = 0.16 (28.1 examples/sec; 0.285 sec/batch; 23h:32m:10s remains)
INFO - root - 2017-12-16 06:26:56.716124: step 35220, loss = 0.28, batch loss = 0.22 (27.4 examples/sec; 0.292 sec/batch; 24h:08m:36s remains)
INFO - root - 2017-12-16 06:26:59.493032: step 35230, loss = 0.22, batch loss = 0.16 (28.1 examples/sec; 0.285 sec/batch; 23h:31m:23s remains)
INFO - root - 2017-12-16 06:27:02.340707: step 35240, loss = 0.21, batch loss = 0.15 (29.2 examples/sec; 0.274 sec/batch; 22h:37m:00s remains)
INFO - root - 2017-12-16 06:27:05.111128: step 35250, loss = 0.22, batch loss = 0.16 (29.7 examples/sec; 0.269 sec/batch; 22h:13m:13s remains)
INFO - root - 2017-12-16 06:27:07.945061: step 35260, loss = 0.23, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 23h:16m:56s remains)
INFO - root - 2017-12-16 06:27:10.771845: step 35270, loss = 0.24, batch loss = 0.18 (28.6 examples/sec; 0.279 sec/batch; 23h:03m:46s remains)
INFO - root - 2017-12-16 06:27:13.595484: step 35280, loss = 0.25, batch loss = 0.20 (29.3 examples/sec; 0.273 sec/batch; 22h:31m:31s remains)
INFO - root - 2017-12-16 06:27:16.390193: step 35290, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 23h:04m:47s remains)
INFO - root - 2017-12-16 06:27:19.211140: step 35300, loss = 0.49, batch loss = 0.43 (28.2 examples/sec; 0.284 sec/batch; 23h:26m:18s remains)
2017-12-16 06:27:19.696975: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7062 -5.3084846 -5.6906614 -5.784472 -5.510428 -5.5734816 -5.8094954 -6.436326 -7.2762055 -7.8374815 -8.1910086 -8.4522953 -8.4438362 -7.9366183 -7.1703386][-4.6690822 -5.1285377 -5.3134789 -5.387229 -5.26481 -5.4182935 -5.6789432 -6.1286545 -6.6487417 -7.4549932 -8.2654095 -8.4875612 -8.412466 -7.9536266 -7.1780758][-3.6147833 -4.3833718 -4.7794232 -4.6362228 -4.4011354 -4.3320527 -4.33518 -4.8622541 -5.5626526 -6.251193 -6.8462105 -7.2835231 -7.6226234 -7.6055727 -7.262167][-2.8366904 -3.149987 -3.3680882 -3.2403636 -2.7245476 -2.4565315 -2.4627178 -2.7787051 -3.085043 -3.7830691 -4.7383461 -5.6320667 -6.407764 -6.7240434 -6.6585436][-2.065449 -1.9893553 -1.8202653 -1.5028167 -0.84657383 -0.39806795 0.077456474 0.14147329 -0.23912907 -1.0542278 -2.1195226 -3.3148756 -4.5354414 -5.80812 -6.6133237][-1.2277038 -0.44573188 0.0982461 0.56163168 1.4244595 2.1192594 2.6538668 2.9216819 2.7267456 1.9461474 0.57445574 -1.4721296 -3.6355805 -5.4616165 -6.3526459][-1.2020383 0.0018024445 1.0469337 1.9589577 3.1585016 4.1501446 4.9360838 5.2740173 4.8020582 3.5933065 1.9502077 -0.41239023 -2.8470607 -5.1283593 -6.395216][-2.43502 -0.96499062 0.14298105 1.4331326 3.3781629 4.7782478 5.6618166 5.781908 5.1541605 3.6033649 1.5464425 -1.056962 -3.5276389 -5.5412855 -6.3258171][-4.1699309 -2.9367363 -1.7781267 -0.53271151 1.1618681 2.8943663 4.1822605 4.4621897 3.7115974 2.1946306 0.2322135 -2.3174751 -4.5441084 -6.2253428 -6.8930225][-5.3994241 -4.9731445 -4.5595455 -3.4556389 -1.7532599 0.17489529 1.3572974 1.7774158 1.5003085 0.24186325 -1.5597262 -3.9555001 -5.874486 -7.188693 -7.6145635][-6.6928606 -6.3901234 -6.0618868 -5.6226206 -4.9545507 -3.3534479 -1.9061687 -1.2160101 -1.4098098 -2.3368912 -3.5086784 -5.2909722 -6.7742062 -7.7602191 -8.0942726][-6.9529819 -7.1544867 -7.4825821 -7.3944035 -6.7821741 -5.51921 -4.5617275 -3.8929074 -3.6359625 -4.0591235 -5.0259962 -6.599041 -7.7054167 -8.20241 -8.2332821][-6.2720022 -6.6144867 -7.0884309 -7.4231539 -7.5298281 -6.6610694 -5.8497791 -5.1208115 -4.8001871 -5.1630435 -5.8218575 -6.7758117 -7.5223579 -8.1270866 -8.2914581][-4.7981234 -5.3173923 -5.841712 -6.3307428 -6.5618892 -6.1536403 -5.6629667 -5.13927 -4.9403796 -5.1128097 -5.5219145 -6.2218637 -6.76544 -6.9165478 -6.7337646][-4.0939231 -4.2573233 -4.5889049 -4.9805808 -5.2321548 -5.1606488 -4.99191 -4.7003503 -4.4829082 -4.6087952 -4.9630775 -5.1993356 -5.2491088 -5.2844996 -5.2587948]]...]
INFO - root - 2017-12-16 06:27:22.543459: step 35310, loss = 0.25, batch loss = 0.19 (27.9 examples/sec; 0.287 sec/batch; 23h:39m:41s remains)
INFO - root - 2017-12-16 06:27:25.401568: step 35320, loss = 0.22, batch loss = 0.17 (28.8 examples/sec; 0.277 sec/batch; 22h:53m:50s remains)
INFO - root - 2017-12-16 06:27:28.207533: step 35330, loss = 0.19, batch loss = 0.13 (28.1 examples/sec; 0.284 sec/batch; 23h:27m:39s remains)
INFO - root - 2017-12-16 06:27:30.991830: step 35340, loss = 0.30, batch loss = 0.24 (29.3 examples/sec; 0.273 sec/batch; 22h:34m:07s remains)
INFO - root - 2017-12-16 06:27:33.816499: step 35350, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.286 sec/batch; 23h:34m:56s remains)
INFO - root - 2017-12-16 06:27:36.626222: step 35360, loss = 0.24, batch loss = 0.18 (29.0 examples/sec; 0.276 sec/batch; 22h:45m:38s remains)
INFO - root - 2017-12-16 06:27:39.416914: step 35370, loss = 0.22, batch loss = 0.16 (29.3 examples/sec; 0.273 sec/batch; 22h:30m:29s remains)
INFO - root - 2017-12-16 06:27:42.253486: step 35380, loss = 0.24, batch loss = 0.18 (28.7 examples/sec; 0.279 sec/batch; 23h:00m:12s remains)
INFO - root - 2017-12-16 06:27:45.084482: step 35390, loss = 0.21, batch loss = 0.15 (27.2 examples/sec; 0.294 sec/batch; 24h:13m:56s remains)
INFO - root - 2017-12-16 06:27:47.940590: step 35400, loss = 0.33, batch loss = 0.27 (29.2 examples/sec; 0.274 sec/batch; 22h:37m:58s remains)
2017-12-16 06:27:48.418332: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8681769 -1.3566303 -1.1342034 -1.3006582 -1.4244545 -1.7247391 -2.1827641 -2.2501366 -2.2604954 -2.044975 -2.1312413 -2.3558295 -2.4820993 -2.7365739 -3.0780144][-1.0711272 -0.58613038 -0.28997517 -0.36400604 -0.81746793 -1.1172218 -1.3459373 -1.5236218 -1.7771449 -1.7076609 -1.8308768 -2.1232722 -2.3319263 -2.611408 -2.8658772][-0.47900248 -0.061732769 0.25087738 0.27629375 -0.027249336 -0.46626139 -1.088474 -1.411093 -1.7570031 -1.9617879 -2.1551008 -2.4124913 -2.6762924 -2.9430392 -3.201575][-0.03312397 0.3448329 0.65367556 0.737463 0.47202921 -0.056935787 -0.58191538 -1.1223376 -1.8859804 -2.3999438 -2.9920325 -3.39991 -3.5995359 -3.629601 -3.56949][-0.46488881 0.086611271 0.65828705 1.0900698 1.1512508 0.87419224 0.30308867 -0.57284021 -1.4784319 -2.3179843 -3.3593917 -4.1754522 -4.4979205 -4.361537 -4.0934715][-1.164784 -0.46797466 0.30465317 1.1009588 1.6636438 1.8560929 1.6939888 0.91773558 -0.13356352 -1.4992468 -2.8789556 -3.8155332 -4.647922 -4.724483 -4.3523417][-1.8544974 -1.0495586 -0.07332325 0.93398428 1.7801251 2.2999907 2.5351677 2.0735259 1.114265 -0.56747985 -2.4024317 -3.7621193 -4.5768318 -4.6658916 -4.4945536][-2.3236213 -1.6291897 -0.90766716 0.15513468 1.4208984 2.3486686 2.6584277 2.3304276 1.5670643 0.01196909 -2.0343361 -3.7793665 -4.7327142 -4.7370515 -4.340457][-2.9818425 -2.3661625 -1.6427042 -0.78069258 0.25747108 1.2192793 1.7867036 1.4809365 0.63630629 -0.64953637 -2.3033333 -3.7406702 -4.7015042 -4.8061938 -4.3604074][-3.3905654 -2.9716907 -2.4828835 -1.8439512 -0.98779869 -0.19788694 0.29210663 0.19642353 -0.43313503 -1.662384 -3.1886048 -4.4126706 -5.1232548 -5.0276837 -4.6252718][-3.5813444 -3.3660293 -3.1286345 -2.6785786 -2.1068058 -1.6234453 -1.3518126 -1.6372316 -2.1482463 -2.9297857 -4.1646423 -5.2004671 -5.7995896 -5.5824533 -5.1531734][-3.6252508 -3.5724392 -3.6033723 -3.3962581 -3.107873 -2.8232608 -2.6324 -3.0172091 -3.6788907 -4.3009367 -4.9530935 -5.5195618 -5.8449335 -5.5386057 -5.1299057][-3.5388136 -3.5590096 -3.549542 -3.4736009 -3.5631132 -3.6679256 -3.8575044 -4.1961579 -4.4762177 -4.7408218 -5.1160331 -5.1878657 -5.0528274 -4.8155665 -4.5189219][-3.3046165 -3.1709654 -3.0372603 -2.9296179 -2.9930372 -3.1765766 -3.4762282 -3.9497495 -4.3838859 -4.5530224 -4.3318458 -3.9356248 -3.5388007 -3.2391338 -3.181757][-3.131279 -2.9461708 -2.7302694 -2.4033337 -2.3534749 -2.623656 -3.0183942 -3.3192275 -3.3921366 -3.3270731 -3.0909512 -2.5074492 -1.9742441 -1.681829 -1.6114979]]...]
INFO - root - 2017-12-16 06:27:51.318892: step 35410, loss = 0.50, batch loss = 0.44 (27.5 examples/sec; 0.291 sec/batch; 24h:01m:34s remains)
INFO - root - 2017-12-16 06:27:54.205319: step 35420, loss = 0.24, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 23h:20m:59s remains)
INFO - root - 2017-12-16 06:27:57.052088: step 35430, loss = 0.27, batch loss = 0.22 (27.8 examples/sec; 0.288 sec/batch; 23h:47m:14s remains)
INFO - root - 2017-12-16 06:27:59.863718: step 35440, loss = 0.26, batch loss = 0.20 (27.1 examples/sec; 0.295 sec/batch; 24h:22m:35s remains)
INFO - root - 2017-12-16 06:28:02.733465: step 35450, loss = 0.27, batch loss = 0.21 (28.8 examples/sec; 0.278 sec/batch; 22h:55m:32s remains)
INFO - root - 2017-12-16 06:28:05.517794: step 35460, loss = 0.25, batch loss = 0.20 (29.4 examples/sec; 0.272 sec/batch; 22h:25m:02s remains)
INFO - root - 2017-12-16 06:28:08.355226: step 35470, loss = 0.27, batch loss = 0.21 (27.3 examples/sec; 0.293 sec/batch; 24h:10m:02s remains)
INFO - root - 2017-12-16 06:28:11.200410: step 35480, loss = 0.24, batch loss = 0.18 (27.4 examples/sec; 0.292 sec/batch; 24h:05m:21s remains)
INFO - root - 2017-12-16 06:28:14.019850: step 35490, loss = 0.28, batch loss = 0.22 (27.2 examples/sec; 0.294 sec/batch; 24h:13m:18s remains)
INFO - root - 2017-12-16 06:28:16.860028: step 35500, loss = 0.25, batch loss = 0.19 (26.9 examples/sec; 0.297 sec/batch; 24h:31m:44s remains)
2017-12-16 06:28:17.316976: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4906545 -4.5910058 -4.8548255 -5.0785317 -5.4908271 -5.5979972 -5.63373 -5.8843737 -6.3135939 -6.3865185 -6.3173943 -6.3053646 -6.0775747 -5.7323942 -5.2272081][-4.8109107 -5.2209816 -5.6559443 -5.886642 -5.9223032 -6.1190882 -6.2840214 -6.0970678 -5.8609581 -6.0766635 -6.4236236 -6.3396816 -6.2325916 -6.155283 -5.8891973][-4.8596215 -5.6635828 -6.2227654 -6.5461826 -6.4939909 -5.8664732 -5.2441177 -5.2814722 -5.3276138 -5.3535752 -5.2953548 -5.765986 -6.2166471 -6.4217634 -6.45621][-4.5044193 -5.7568913 -6.6204624 -6.7859888 -6.193089 -5.1648979 -4.0487013 -3.1121769 -2.7417474 -3.0866904 -3.4272344 -4.1533675 -4.9793458 -5.8694468 -6.5109][-4.1989217 -5.5715361 -6.4178309 -6.4005008 -5.533433 -3.7984731 -1.9640572 -0.53201151 0.20013285 -0.1511507 -1.0507975 -2.4284973 -3.8156533 -5.3711119 -6.5425076][-3.264601 -4.7682915 -5.6002026 -5.4763155 -4.0671334 -1.9928637 0.17833471 1.9547458 2.9319181 2.6490035 1.5318103 -0.48463774 -2.5615907 -4.5147486 -5.9836807][-2.3824322 -3.6341231 -4.4486661 -4.2342386 -2.7807317 -0.55697584 1.928896 3.4554915 4.31756 4.0815763 3.06137 1.0275054 -1.3147142 -3.579577 -5.4629836][-1.8801787 -3.4128916 -4.1933231 -3.5959229 -2.2015042 -0.17498636 1.8869295 3.5903387 4.5796251 4.0253792 2.836833 1.0894117 -0.86320043 -3.1021528 -5.2030606][-2.4399323 -3.4228618 -3.7580507 -3.5654726 -2.5089769 -0.589504 1.1120062 2.3826437 3.1876669 2.8474445 1.8315973 0.26151085 -1.4350958 -3.2446604 -5.1986141][-2.2694309 -3.7612453 -4.8074946 -4.3703713 -3.1274264 -1.7565241 -0.62040472 0.34984732 0.90893221 0.78382015 0.40086603 -0.98857141 -2.6742115 -4.3334222 -5.8783011][-3.1488051 -4.1420059 -4.7119517 -5.0658865 -4.8624063 -3.7091866 -2.441349 -1.8004138 -1.6224349 -1.52634 -1.7805998 -2.6742058 -3.7983096 -5.1140022 -6.4208374][-3.8884766 -5.0678349 -5.8370118 -5.8339362 -5.7456093 -5.4422855 -5.096601 -4.463676 -3.7647541 -3.5958092 -3.9154062 -4.3992252 -5.1701088 -6.0738096 -6.7933254][-4.6402149 -5.7864647 -6.6631393 -6.8698397 -6.897171 -6.5157919 -6.2006412 -5.7345419 -5.3126731 -4.9442825 -4.71236 -5.1845746 -6.0839834 -6.5705204 -6.8342586][-5.6468573 -6.4386959 -7.0647902 -7.4551225 -7.6914387 -7.3878489 -7.0469408 -6.5748558 -6.1670961 -5.9223638 -5.8605051 -6.0147171 -6.2530518 -6.4618053 -6.6120934][-6.3042383 -6.8349071 -7.3479772 -7.709909 -7.9502921 -7.8559027 -7.5839128 -7.02078 -6.6465278 -6.4267621 -6.2838244 -6.3292027 -6.3878136 -6.2854147 -5.9483232]]...]
INFO - root - 2017-12-16 06:28:20.122603: step 35510, loss = 0.27, batch loss = 0.21 (29.4 examples/sec; 0.272 sec/batch; 22h:26m:13s remains)
INFO - root - 2017-12-16 06:28:22.951852: step 35520, loss = 0.24, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 22h:36m:48s remains)
INFO - root - 2017-12-16 06:28:25.764255: step 35530, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.282 sec/batch; 23h:16m:51s remains)
INFO - root - 2017-12-16 06:28:28.673898: step 35540, loss = 0.36, batch loss = 0.30 (26.9 examples/sec; 0.297 sec/batch; 24h:29m:38s remains)
INFO - root - 2017-12-16 06:28:31.458675: step 35550, loss = 0.25, batch loss = 0.20 (27.5 examples/sec; 0.291 sec/batch; 24h:00m:39s remains)
INFO - root - 2017-12-16 06:28:34.304027: step 35560, loss = 0.26, batch loss = 0.20 (28.5 examples/sec; 0.281 sec/batch; 23h:11m:14s remains)
INFO - root - 2017-12-16 06:28:37.117082: step 35570, loss = 0.25, batch loss = 0.19 (29.2 examples/sec; 0.274 sec/batch; 22h:36m:08s remains)
INFO - root - 2017-12-16 06:28:39.949045: step 35580, loss = 0.31, batch loss = 0.26 (28.8 examples/sec; 0.278 sec/batch; 22h:53m:51s remains)
INFO - root - 2017-12-16 06:28:42.759434: step 35590, loss = 0.22, batch loss = 0.16 (28.9 examples/sec; 0.276 sec/batch; 22h:47m:56s remains)
INFO - root - 2017-12-16 06:28:45.597907: step 35600, loss = 0.54, batch loss = 0.48 (28.3 examples/sec; 0.283 sec/batch; 23h:19m:22s remains)
2017-12-16 06:28:46.059636: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3042989 -6.0991716 -5.7046585 -5.7790833 -5.9481354 -6.2285967 -6.5461731 -7.1853032 -7.7832088 -8.2326756 -8.4334087 -8.1251659 -7.4387012 -6.4453053 -5.4968343][-6.2252855 -5.9477963 -5.6019111 -5.4824505 -5.42231 -5.9187465 -6.5802717 -7.4088821 -8.104805 -8.5425892 -8.7933273 -8.60486 -7.9928665 -6.9220409 -5.8414679][-5.7028904 -5.043529 -4.4255633 -4.0958662 -3.9445162 -4.2663288 -4.7937269 -6.09489 -7.2119617 -7.6528749 -7.9282322 -7.8983397 -7.5979004 -6.7058277 -5.8402214][-4.2998762 -3.7265115 -2.9166214 -2.3850098 -2.1644461 -2.2305114 -2.5954931 -3.8497372 -4.9846396 -5.8227758 -6.3218932 -6.3731055 -6.2567997 -5.742538 -5.2733531][-2.8137593 -1.9801936 -0.8666048 -0.26858521 0.058644772 0.019277096 -0.4582057 -1.4416873 -2.2967083 -3.1339512 -3.9956272 -4.5937462 -5.0379791 -4.8782663 -4.7259536][-1.8654037 -0.66414142 0.68717432 1.7563081 2.3692183 2.2946634 1.8804522 1.0494328 0.27927017 -0.3737812 -1.2476046 -2.3140156 -3.5187643 -4.1748753 -4.4390249][-2.4563785 -0.98670936 0.67890549 1.8992686 2.808918 3.0911412 3.0508838 2.4271002 1.6755404 0.93305826 -0.0059447289 -1.0372341 -2.2593446 -3.2902174 -3.8787909][-3.2919955 -1.8724632 -0.17236567 1.2714763 2.345499 2.7678475 3.0691056 2.8278742 2.4497619 1.7205167 0.46725845 -0.78451729 -2.059973 -2.9801502 -3.5216336][-4.7345829 -3.5794916 -2.1619489 -0.70883179 0.49417257 1.3115649 1.7891397 1.7958193 1.8059659 1.2321162 0.18811655 -1.1490877 -2.6017 -3.3394208 -3.6598258][-5.9350934 -5.3043327 -4.4054279 -3.1465845 -1.9056501 -0.90245056 -0.07576561 0.068136692 -0.018623829 -0.39963818 -1.1805325 -2.20399 -3.2792685 -3.8972156 -4.1987734][-6.3329992 -5.8986025 -5.3189383 -4.4980469 -3.5846171 -2.6385446 -1.7509129 -1.3153572 -1.2106407 -1.661329 -2.3413231 -3.0876038 -3.7356133 -4.2131376 -4.5087051][-6.4082346 -6.0904226 -5.613091 -5.1207213 -4.4417486 -3.7416277 -3.0065594 -2.602067 -2.3290088 -2.4416189 -2.8220375 -3.5944548 -4.2324438 -4.4410639 -4.5323663][-5.12809 -5.0897961 -5.0072675 -4.6541204 -4.2190881 -3.8735516 -3.3308015 -3.193176 -3.00853 -2.9653349 -3.168808 -3.5455079 -4.0504456 -4.2855825 -4.348299][-3.7369895 -3.7164764 -3.7022715 -3.5207078 -3.4057071 -3.1408119 -2.8628983 -2.847384 -2.7775693 -2.7703376 -2.8898034 -3.1410885 -3.4361155 -3.5596926 -3.5623689][-2.6008024 -2.3832264 -2.2641044 -2.2707486 -2.2576501 -2.2086174 -2.1504531 -2.068886 -1.9928563 -2.0303919 -2.2120998 -2.3897309 -2.5579276 -2.6996803 -2.7262506]]...]
INFO - root - 2017-12-16 06:28:48.905525: step 35610, loss = 0.27, batch loss = 0.21 (29.4 examples/sec; 0.272 sec/batch; 22h:25m:55s remains)
INFO - root - 2017-12-16 06:28:51.715866: step 35620, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.281 sec/batch; 23h:11m:54s remains)
INFO - root - 2017-12-16 06:28:54.588693: step 35630, loss = 0.21, batch loss = 0.15 (28.6 examples/sec; 0.279 sec/batch; 23h:01m:43s remains)
INFO - root - 2017-12-16 06:28:57.404434: step 35640, loss = 0.23, batch loss = 0.17 (27.6 examples/sec; 0.290 sec/batch; 23h:56m:27s remains)
INFO - root - 2017-12-16 06:29:00.275636: step 35650, loss = 0.26, batch loss = 0.20 (28.2 examples/sec; 0.284 sec/batch; 23h:24m:28s remains)
INFO - root - 2017-12-16 06:29:03.098236: step 35660, loss = 0.22, batch loss = 0.16 (28.4 examples/sec; 0.282 sec/batch; 23h:13m:16s remains)
INFO - root - 2017-12-16 06:29:05.933854: step 35670, loss = 0.21, batch loss = 0.15 (27.7 examples/sec; 0.289 sec/batch; 23h:48m:22s remains)
INFO - root - 2017-12-16 06:29:08.808051: step 35680, loss = 0.33, batch loss = 0.27 (25.4 examples/sec; 0.314 sec/batch; 25h:55m:46s remains)
INFO - root - 2017-12-16 06:29:11.633539: step 35690, loss = 0.24, batch loss = 0.18 (27.6 examples/sec; 0.289 sec/batch; 23h:51m:23s remains)
INFO - root - 2017-12-16 06:29:14.466812: step 35700, loss = 0.43, batch loss = 0.37 (27.0 examples/sec; 0.296 sec/batch; 24h:25m:25s remains)
2017-12-16 06:29:14.928384: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9201412 -4.0814009 -4.2525215 -4.3024697 -4.1813025 -4.20712 -4.1927061 -4.2002339 -4.5449409 -4.8487039 -5.1128683 -5.4469757 -5.529911 -5.2256832 -4.6356316][-3.8961391 -4.0282612 -4.1423864 -4.3470826 -4.3794713 -4.4234509 -4.4490213 -4.3971872 -4.5177507 -4.7392969 -5.0509458 -5.2193666 -5.219202 -5.0671992 -4.5772119][-3.3131533 -3.5666721 -3.648958 -3.7988162 -3.8599548 -4.0775266 -4.2037635 -4.2304244 -4.3995132 -4.52627 -4.7014012 -4.8899031 -5.0859542 -5.1163292 -4.9365492][-2.2057586 -2.3123176 -2.2927179 -2.4409087 -2.6948023 -3.069627 -3.2084322 -3.2384133 -3.2698798 -3.5694709 -3.8997834 -4.30995 -4.8870368 -5.1546807 -5.2274523][-1.5185785 -0.93455243 -0.71000791 -0.89604807 -1.0105863 -1.1093385 -1.0393932 -1.1466427 -1.3124118 -1.7981453 -2.3900158 -3.3740535 -4.4170322 -5.3175378 -5.78769][-1.1141682 -0.28260756 0.20590019 0.3601141 0.57738781 0.62522936 0.94421673 1.1403184 1.0674782 0.51021957 -0.39020348 -2.1390336 -3.9128933 -5.2119493 -5.86028][-1.453656 -0.30791807 0.4976244 0.87110233 1.2996602 1.7527905 2.3875165 2.7627659 2.9172587 2.2396989 1.073751 -0.86084509 -2.9708071 -4.7964668 -5.784409][-2.1412096 -1.204386 -0.66562438 0.22112465 1.1393747 1.9572687 2.8940387 3.4841027 3.8146057 3.2360792 2.028995 -0.31323051 -2.78601 -4.7284184 -5.6560493][-3.6765308 -3.0455241 -2.3673828 -1.6271472 -0.69707894 0.59091949 1.8209205 2.5973024 3.0414658 2.6560469 1.6383038 -0.48734045 -2.8613839 -4.8883014 -5.9236546][-4.82593 -4.5928965 -4.2965941 -3.6890152 -2.7473419 -1.5867352 -0.56444383 0.38395214 1.0744967 0.8181076 0.028053284 -1.6390853 -3.5212767 -5.2167649 -6.2370353][-5.4963384 -5.514492 -5.4050159 -5.0525937 -4.4582844 -3.4307601 -2.4083869 -1.6538498 -1.2769735 -1.1913519 -1.5107062 -2.7248206 -4.1665177 -5.3598385 -6.0607677][-5.621038 -5.848002 -6.1125107 -6.0098968 -5.5623827 -4.9188113 -4.3253279 -3.7337525 -3.256937 -3.092607 -3.4059453 -4.064899 -4.7882361 -5.5580482 -6.085393][-5.2602973 -5.4389191 -5.8002129 -6.035181 -6.0383635 -5.6646204 -5.2549472 -4.9171014 -4.6876936 -4.412323 -4.3135033 -4.4948459 -4.9279928 -5.3411665 -5.6259036][-4.7497778 -4.7890296 -5.0222006 -5.2989569 -5.4691644 -5.4635596 -5.36265 -5.1665368 -4.9731359 -4.7100196 -4.6527576 -4.5069389 -4.5109358 -4.6652122 -4.8969359][-4.9652419 -4.5955281 -4.5917263 -4.8392639 -5.0554104 -5.166965 -5.1782846 -5.1232567 -5.1398621 -4.9874063 -4.8351817 -4.5639067 -4.4219422 -4.3120518 -4.3236079]]...]
INFO - root - 2017-12-16 06:29:17.834990: step 35710, loss = 0.30, batch loss = 0.24 (27.5 examples/sec; 0.291 sec/batch; 24h:01m:05s remains)
INFO - root - 2017-12-16 06:29:20.681575: step 35720, loss = 0.28, batch loss = 0.22 (26.1 examples/sec; 0.306 sec/batch; 25h:15m:17s remains)
INFO - root - 2017-12-16 06:29:23.515286: step 35730, loss = 0.26, batch loss = 0.21 (28.6 examples/sec; 0.279 sec/batch; 23h:01m:20s remains)
INFO - root - 2017-12-16 06:29:26.361955: step 35740, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.285 sec/batch; 23h:31m:05s remains)
INFO - root - 2017-12-16 06:29:29.189669: step 35750, loss = 0.21, batch loss = 0.15 (26.2 examples/sec; 0.305 sec/batch; 25h:10m:32s remains)
INFO - root - 2017-12-16 06:29:31.993898: step 35760, loss = 0.24, batch loss = 0.18 (29.0 examples/sec; 0.276 sec/batch; 22h:42m:32s remains)
INFO - root - 2017-12-16 06:29:34.840159: step 35770, loss = 0.20, batch loss = 0.14 (29.4 examples/sec; 0.272 sec/batch; 22h:27m:36s remains)
INFO - root - 2017-12-16 06:29:37.677125: step 35780, loss = 0.24, batch loss = 0.18 (26.9 examples/sec; 0.298 sec/batch; 24h:31m:46s remains)
INFO - root - 2017-12-16 06:29:40.541607: step 35790, loss = 0.35, batch loss = 0.30 (27.9 examples/sec; 0.286 sec/batch; 23h:35m:38s remains)
INFO - root - 2017-12-16 06:29:43.347164: step 35800, loss = 0.23, batch loss = 0.17 (29.4 examples/sec; 0.272 sec/batch; 22h:27m:23s remains)
2017-12-16 06:29:43.812902: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4575653 -3.5969045 -3.7483106 -3.9788404 -4.091702 -4.0901175 -3.9817991 -3.8214059 -3.7177925 -3.6151969 -3.481715 -3.4668639 -3.4249611 -3.3377979 -3.2925913][-3.529387 -3.8558812 -4.1441922 -4.4505372 -4.4963346 -4.4343796 -4.2835722 -4.0266728 -3.7998052 -3.7077298 -3.7065763 -3.5997279 -3.4457822 -3.4801176 -3.5719066][-3.6614816 -4.0982604 -4.4825869 -4.695127 -4.5263162 -4.2613058 -3.9659624 -3.7705224 -3.6599536 -3.5394185 -3.6183169 -3.6362884 -3.5180697 -3.5733156 -3.6899421][-3.6756825 -4.1720514 -4.5073953 -4.55232 -4.0992413 -3.6071682 -3.1248896 -2.7960548 -2.7656441 -2.8950396 -3.1084127 -3.3072493 -3.3879361 -3.4983773 -3.5550284][-3.5944176 -4.0163841 -4.1218491 -3.800245 -2.8734422 -1.9197264 -1.102423 -0.587492 -0.49666953 -0.79173565 -1.3481722 -1.8521528 -2.2121868 -2.6355653 -2.9245241][-3.4470115 -3.67421 -3.6844671 -2.8439598 -1.3797107 -0.19290495 0.94780588 1.6814308 1.8249736 1.4226189 0.57265425 -0.25511646 -0.94454 -1.671515 -2.2974944][-3.0981321 -2.9977131 -2.7418251 -1.7051682 -0.15704012 1.3618631 2.8195438 3.6479044 3.7466688 3.1705542 2.1097898 1.0418553 0.048708439 -0.93759394 -1.6061301][-2.7878847 -2.8543417 -2.6449528 -1.4499207 0.1884675 1.8321276 3.3020449 4.2518768 4.4634008 3.9354362 2.8327785 1.6600294 0.6172061 -0.334435 -1.0578249][-2.5663333 -2.7606373 -2.8632002 -1.938225 -0.47725368 1.10284 2.6246362 3.6357145 3.8647118 3.4900985 2.6451602 1.4057794 0.18640184 -0.65007019 -1.3441129][-2.9508862 -3.3224058 -3.5827231 -3.0648918 -2.0149662 -0.7199192 0.63822079 1.7777119 2.3348742 2.0549746 1.1106987 -0.012061119 -1.0208044 -1.8777587 -2.5291986][-3.6355712 -4.2176604 -4.5296168 -4.2188196 -3.5664151 -2.5937948 -1.4898729 -0.69216681 -0.31372261 -0.47005486 -1.1160934 -2.026154 -2.7450171 -3.5051684 -3.9815223][-4.3656049 -4.8113327 -5.2091074 -5.1327486 -4.63949 -3.8489263 -3.09483 -2.486886 -2.1519587 -2.1313953 -2.4005477 -3.2920811 -4.0645766 -4.5889726 -4.7582788][-4.8855934 -5.13479 -5.3868966 -5.4201646 -5.0678439 -4.53396 -3.9916193 -3.3932028 -2.9899521 -2.9286113 -3.0841155 -3.7646976 -4.4188342 -4.9202332 -5.1722879][-5.0829697 -5.0101466 -4.9422483 -4.8548579 -4.5630331 -4.1932464 -3.9139378 -3.5962672 -3.2378631 -3.0288069 -3.0685313 -3.6605418 -4.2273259 -4.6070642 -4.7338829][-4.9365339 -4.7087903 -4.4609127 -4.2265511 -3.9138157 -3.6638665 -3.4965298 -3.2354946 -2.9439831 -2.812048 -2.7377615 -3.0715036 -3.4548235 -3.7471933 -3.8843267]]...]
INFO - root - 2017-12-16 06:29:46.590578: step 35810, loss = 0.27, batch loss = 0.21 (27.6 examples/sec; 0.289 sec/batch; 23h:50m:52s remains)
INFO - root - 2017-12-16 06:29:49.472661: step 35820, loss = 0.35, batch loss = 0.30 (29.7 examples/sec; 0.269 sec/batch; 22h:12m:11s remains)
INFO - root - 2017-12-16 06:29:52.328475: step 35830, loss = 0.22, batch loss = 0.16 (27.4 examples/sec; 0.292 sec/batch; 24h:03m:40s remains)
INFO - root - 2017-12-16 06:29:55.162423: step 35840, loss = 0.23, batch loss = 0.18 (28.7 examples/sec; 0.279 sec/batch; 22h:57m:31s remains)
INFO - root - 2017-12-16 06:29:57.992513: step 35850, loss = 0.35, batch loss = 0.29 (27.6 examples/sec; 0.290 sec/batch; 23h:54m:22s remains)
INFO - root - 2017-12-16 06:30:00.825517: step 35860, loss = 0.23, batch loss = 0.17 (27.4 examples/sec; 0.292 sec/batch; 24h:04m:43s remains)
INFO - root - 2017-12-16 06:30:03.630059: step 35870, loss = 0.26, batch loss = 0.20 (27.7 examples/sec; 0.288 sec/batch; 23h:46m:13s remains)
INFO - root - 2017-12-16 06:30:06.477471: step 35880, loss = 0.24, batch loss = 0.18 (25.3 examples/sec; 0.316 sec/batch; 26h:01m:35s remains)
INFO - root - 2017-12-16 06:30:09.314262: step 35890, loss = 0.20, batch loss = 0.14 (27.6 examples/sec; 0.290 sec/batch; 23h:53m:59s remains)
INFO - root - 2017-12-16 06:30:12.169653: step 35900, loss = 0.22, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 22h:57m:03s remains)
2017-12-16 06:30:12.645052: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8077083 -6.2443476 -6.7315497 -7.237587 -7.6436167 -7.8362222 -7.9694948 -8.334919 -8.9154911 -8.8967133 -8.4664555 -8.1126184 -7.4966984 -6.5121837 -5.289824][-5.4510932 -5.85814 -6.2011957 -6.6799812 -7.0521212 -7.5057759 -7.812799 -7.9474392 -8.1499281 -8.3420591 -8.3326988 -7.8329144 -7.0593204 -6.1997194 -5.2036672][-4.848454 -5.3129716 -5.6480637 -5.8756423 -5.9074326 -5.9399967 -5.8771911 -5.9506865 -6.1929107 -6.6407566 -6.9646454 -6.789094 -6.3546247 -5.7066607 -5.04498][-3.9156995 -4.4111595 -4.6695981 -4.7222 -4.4845729 -3.9585552 -3.5027597 -3.3139737 -3.5235095 -3.8925989 -4.3710141 -4.9804969 -5.3338733 -5.1990271 -4.8454971][-2.8913429 -3.2660797 -3.2165408 -2.9763527 -2.4198575 -1.488328 -0.563884 -0.053506851 -0.18993998 -0.92348266 -1.833694 -2.9057996 -3.8627791 -4.6375251 -5.1091819][-1.900522 -1.9040844 -1.4715405 -0.75880337 0.19804192 1.1960664 2.397418 3.1512284 3.0922661 2.07866 0.5866766 -1.3596761 -3.16852 -4.5504436 -5.3054914][-1.172219 -1.0458097 -0.78715396 0.16073322 1.584321 2.9534287 4.2996559 4.9474258 4.8568497 3.7511225 1.9576602 -0.47849917 -2.7366877 -4.5617452 -5.5412469][-0.86103392 -0.76086545 -0.598073 0.091877937 1.2556152 2.788466 4.3074722 5.0283556 4.9740191 3.9111156 2.1412382 -0.32450724 -2.7818894 -4.6945553 -5.6017165][-1.0502694 -0.99174047 -0.86396742 -0.49091911 0.28794003 1.6245389 2.8945303 3.7044449 3.9048462 2.9489522 1.3337479 -0.93091035 -3.2784534 -5.1203971 -6.1184492][-1.657424 -1.8750246 -2.1843984 -1.8823683 -1.082885 -0.13533258 0.80892467 1.4945731 1.6583486 0.90646982 -0.31343079 -2.3046603 -4.4187908 -5.9050646 -6.575232][-2.9058166 -3.0889969 -3.3348031 -3.465703 -3.2522759 -2.4630725 -1.5853841 -1.1821311 -1.1999512 -1.6084476 -2.4610448 -3.783463 -5.1830373 -6.2331142 -6.6455393][-3.954211 -4.3150692 -4.9125929 -5.0910807 -4.9853649 -4.6864538 -4.3848219 -3.8600249 -3.508471 -3.5829842 -4.157927 -4.8569946 -5.6465282 -6.3111429 -6.4875522][-5.1112485 -5.7332954 -6.3579888 -6.6803656 -6.8947787 -6.6978188 -6.1746335 -5.7119212 -5.417882 -5.0507741 -4.9157996 -5.2681723 -5.82759 -6.1645551 -6.2630777][-6.2779474 -6.9079189 -7.5293036 -7.9871287 -8.3092823 -8.2686062 -8.1338339 -7.6266618 -6.8976979 -6.1094661 -5.8940206 -5.6847525 -5.5294542 -5.7502637 -6.0209808][-7.09999 -7.67986 -8.38241 -8.84436 -9.2291756 -9.26314 -8.9552116 -8.2664452 -7.59441 -6.8166952 -6.2008939 -5.9167442 -5.98099 -6.090291 -5.8402243]]...]
INFO - root - 2017-12-16 06:30:15.480671: step 35910, loss = 0.31, batch loss = 0.25 (28.3 examples/sec; 0.282 sec/batch; 23h:15m:35s remains)
INFO - root - 2017-12-16 06:30:18.312798: step 35920, loss = 0.30, batch loss = 0.24 (28.8 examples/sec; 0.278 sec/batch; 22h:54m:24s remains)
INFO - root - 2017-12-16 06:30:21.151429: step 35930, loss = 0.41, batch loss = 0.35 (28.1 examples/sec; 0.285 sec/batch; 23h:29m:08s remains)
INFO - root - 2017-12-16 06:30:23.962533: step 35940, loss = 0.27, batch loss = 0.21 (28.7 examples/sec; 0.278 sec/batch; 22h:55m:34s remains)
INFO - root - 2017-12-16 06:30:26.855531: step 35950, loss = 0.20, batch loss = 0.14 (27.4 examples/sec; 0.292 sec/batch; 24h:05m:26s remains)
INFO - root - 2017-12-16 06:30:29.678207: step 35960, loss = 0.28, batch loss = 0.22 (28.7 examples/sec; 0.279 sec/batch; 22h:59m:50s remains)
INFO - root - 2017-12-16 06:30:32.521022: step 35970, loss = 0.19, batch loss = 0.13 (29.1 examples/sec; 0.275 sec/batch; 22h:39m:37s remains)
INFO - root - 2017-12-16 06:30:35.379157: step 35980, loss = 0.32, batch loss = 0.27 (26.2 examples/sec; 0.306 sec/batch; 25h:10m:34s remains)
INFO - root - 2017-12-16 06:30:38.239802: step 35990, loss = 0.20, batch loss = 0.15 (28.3 examples/sec; 0.282 sec/batch; 23h:14m:57s remains)
INFO - root - 2017-12-16 06:30:41.060903: step 36000, loss = 0.29, batch loss = 0.23 (27.3 examples/sec; 0.293 sec/batch; 24h:06m:46s remains)
2017-12-16 06:30:41.556599: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6801224 -4.0626025 -4.55548 -5.004869 -5.3208108 -5.4650903 -5.3874755 -5.3166542 -5.3931422 -5.3419647 -5.5363584 -5.60889 -5.6053905 -5.300283 -4.6916032][-3.6459389 -4.0952792 -4.510376 -4.9806452 -5.165226 -5.2004375 -5.0767393 -5.0143347 -5.0854425 -5.10015 -5.2968822 -5.276053 -5.0471716 -4.7572303 -4.3224311][-3.5979238 -4.2034836 -4.6239376 -4.9150419 -4.8353276 -4.6224623 -4.352839 -4.23643 -4.4100637 -4.6477151 -4.8775353 -4.9359307 -4.8452044 -4.5755181 -4.3277173][-3.5729184 -4.1344423 -4.4610186 -4.4503746 -3.9867156 -3.3428268 -2.9426103 -2.8925185 -3.1321993 -3.3499742 -3.7503738 -4.1041317 -4.3025985 -4.1283035 -4.0649962][-2.9872353 -3.1792502 -3.1329031 -2.9390879 -2.2533624 -1.3856468 -1.0599437 -1.1838171 -1.4803214 -1.7752833 -2.2123828 -2.7203207 -3.3062358 -3.8144159 -4.0973077][-2.7104168 -2.572119 -2.0699444 -1.2174606 -0.21094084 0.57260323 0.85949659 0.73001146 0.44254017 0.034066677 -0.42876911 -0.92387271 -1.6786287 -2.5917375 -3.5050251][-2.9146705 -2.5026965 -1.6611636 -0.39117718 1.0092478 2.0170388 2.2588339 2.0333915 1.7784181 1.5516677 1.1113324 0.42057037 -0.64195991 -1.6564348 -2.634531][-3.1291461 -2.5233691 -1.6360714 -0.21962309 1.2024126 2.2757196 2.5651755 2.3490968 2.2234249 1.9430342 1.5005827 0.83610106 -0.24412346 -1.5710087 -2.6797757][-3.3997865 -2.7942324 -1.9870095 -0.90376711 0.10173512 0.88465118 1.2786651 1.3658547 1.5433202 1.4884109 1.1242175 0.53819656 -0.3759551 -1.551008 -2.5837617][-3.9106555 -3.5805287 -3.0328388 -2.1076345 -1.2404289 -0.8086586 -0.68366814 -0.45726323 -0.12081099 -0.15075874 -0.4263463 -0.94585204 -1.7467835 -2.6607957 -3.3243592][-4.5823569 -4.4236221 -4.0889587 -3.5595467 -2.9948623 -2.4970639 -2.2226989 -2.2621698 -2.0257971 -1.9820552 -2.3937266 -2.9433455 -3.5734434 -4.1534896 -4.568675][-5.0595632 -4.9711008 -4.9618444 -4.7335715 -4.4089909 -4.0283661 -3.6502624 -3.4937131 -3.3406272 -3.3882575 -3.72146 -4.2935762 -4.9666319 -5.4153643 -5.3796024][-5.092885 -5.1323891 -5.2127652 -5.1531162 -5.0038495 -4.7351 -4.4568257 -4.1005707 -3.8853228 -4.0064831 -4.240109 -4.6112733 -4.9983191 -5.4941163 -5.5791216][-4.6811767 -4.5674434 -4.5668845 -4.6374307 -4.6436081 -4.551857 -4.398747 -4.310267 -4.0518928 -3.9259617 -4.08134 -4.5248737 -5.021256 -5.2446342 -5.2237291][-4.56709 -4.3091679 -4.0767064 -3.9602418 -3.898124 -3.860568 -3.8303063 -3.7596102 -3.7183342 -3.6971784 -3.7145324 -3.9518919 -4.2643847 -4.5760865 -4.601944]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 06:30:44.446850: step 36010, loss = 0.24, batch loss = 0.18 (25.8 examples/sec; 0.310 sec/batch; 25h:31m:23s remains)
INFO - root - 2017-12-16 06:30:47.248735: step 36020, loss = 0.24, batch loss = 0.18 (28.6 examples/sec; 0.279 sec/batch; 23h:00m:07s remains)
INFO - root - 2017-12-16 06:30:50.085648: step 36030, loss = 0.31, batch loss = 0.25 (27.3 examples/sec; 0.293 sec/batch; 24h:07m:21s remains)
INFO - root - 2017-12-16 06:30:52.923842: step 36040, loss = 0.45, batch loss = 0.39 (27.4 examples/sec; 0.292 sec/batch; 24h:04m:37s remains)
INFO - root - 2017-12-16 06:30:55.748101: step 36050, loss = 0.30, batch loss = 0.25 (28.7 examples/sec; 0.278 sec/batch; 22h:55m:44s remains)
INFO - root - 2017-12-16 06:30:58.574853: step 36060, loss = 0.44, batch loss = 0.38 (29.2 examples/sec; 0.274 sec/batch; 22h:34m:28s remains)
INFO - root - 2017-12-16 06:31:01.462773: step 36070, loss = 0.28, batch loss = 0.22 (27.9 examples/sec; 0.286 sec/batch; 23h:34m:24s remains)
INFO - root - 2017-12-16 06:31:04.332927: step 36080, loss = 0.30, batch loss = 0.24 (28.8 examples/sec; 0.277 sec/batch; 22h:50m:30s remains)
INFO - root - 2017-12-16 06:31:07.193586: step 36090, loss = 0.22, batch loss = 0.16 (28.2 examples/sec; 0.283 sec/batch; 23h:20m:23s remains)
INFO - root - 2017-12-16 06:31:10.069237: step 36100, loss = 0.21, batch loss = 0.15 (28.1 examples/sec; 0.285 sec/batch; 23h:27m:57s remains)
2017-12-16 06:31:10.536428: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7760527 -1.6592004 -1.9106026 -2.1749823 -2.4337382 -2.6537313 -2.9403465 -3.0769913 -3.0166125 -2.9311008 -2.8838069 -3.14044 -3.5054624 -3.8786781 -4.5917411][-2.2341037 -2.3715715 -2.6810064 -3.123507 -3.5730844 -3.7365291 -3.9031153 -4.0673018 -4.211482 -4.1661911 -4.0533638 -4.1267409 -4.2696276 -4.7577004 -5.22092][-3.7054977 -3.8990777 -4.1085925 -4.3208895 -4.55491 -4.6829276 -4.8783607 -4.9612236 -5.1247563 -5.3511257 -5.4607773 -5.4595642 -5.5143909 -5.6442051 -5.7581391][-5.0703921 -5.3146219 -5.4067817 -5.3862219 -5.1900334 -4.808105 -4.5735545 -4.5659814 -4.8243523 -5.07924 -5.370471 -5.5697584 -5.6827049 -5.7422695 -5.7205076][-5.865284 -6.268971 -6.3746419 -5.9877429 -5.3073425 -4.3582778 -3.5864339 -3.0409729 -2.8617628 -3.1650689 -3.6636903 -4.2519445 -4.8918676 -5.1754522 -5.3105845][-6.0534973 -6.1550789 -6.1059408 -5.583919 -4.4942203 -3.130147 -1.7688015 -0.83511639 -0.50815868 -0.69176388 -1.3315413 -2.1873958 -3.1935172 -4.0914078 -4.7883849][-5.1493874 -5.1838026 -4.9951763 -4.2914639 -3.0996318 -1.4771395 0.35062408 1.6996417 2.3988047 2.1601386 1.2318192 -0.023263931 -1.5351305 -2.8194804 -3.8790054][-3.6361732 -3.7802675 -3.6060753 -3.0044396 -1.9246125 -0.32591057 1.6871648 3.2773829 4.224184 4.2101 3.2606311 1.6964335 -0.15388012 -1.9505308 -3.410574][-2.0991623 -2.3498554 -2.4691265 -1.9211857 -1.0816352 0.15169621 1.7845297 3.2422872 4.21212 4.2423334 3.3520417 1.8452325 -0.026081562 -1.8887064 -3.439393][-2.3799582 -2.0395854 -1.6242929 -1.3710661 -0.86646366 0.17613077 1.370707 2.2647567 2.71846 2.619174 1.796 0.46741962 -1.0369234 -2.5588861 -3.8312173][-2.9202197 -2.3741558 -1.8295159 -1.3302069 -0.73847461 -0.030162811 0.64758348 1.1214743 1.2638259 0.96405172 0.12527132 -1.0075626 -2.2226405 -3.2556806 -4.0209041][-4.0374465 -3.1555638 -2.2373149 -1.6510301 -1.0933836 -0.65620279 -0.322505 -0.027761936 -0.12106371 -0.52202392 -1.2057858 -2.1054881 -3.0114222 -3.6810098 -4.0431852][-5.0915055 -4.06957 -2.9051561 -1.8986447 -1.2698317 -1.0434926 -1.0502038 -1.0320973 -1.2575943 -1.5861163 -2.0397532 -2.5674081 -3.0843461 -3.3384681 -3.4421847][-5.9176168 -4.7679434 -3.3382473 -2.1032684 -1.2233586 -0.85765982 -0.84115386 -0.81602311 -1.1184266 -1.3129225 -1.48932 -1.8676302 -2.2863145 -2.4774122 -2.5889931][-5.8460035 -4.8864403 -3.6144869 -2.2763162 -1.3531306 -0.76802826 -0.50663304 -0.31160688 -0.25143671 -0.3216877 -0.52100372 -0.70110822 -1.015506 -1.2252724 -1.4241974]]...]
INFO - root - 2017-12-16 06:31:13.356756: step 36110, loss = 0.19, batch loss = 0.13 (29.0 examples/sec; 0.276 sec/batch; 22h:43m:02s remains)
INFO - root - 2017-12-16 06:31:16.163257: step 36120, loss = 0.36, batch loss = 0.30 (28.9 examples/sec; 0.277 sec/batch; 22h:46m:29s remains)
INFO - root - 2017-12-16 06:31:18.983096: step 36130, loss = 0.20, batch loss = 0.15 (25.9 examples/sec; 0.309 sec/batch; 25h:27m:22s remains)
INFO - root - 2017-12-16 06:31:21.807263: step 36140, loss = 0.38, batch loss = 0.32 (28.9 examples/sec; 0.276 sec/batch; 22h:45m:37s remains)
INFO - root - 2017-12-16 06:31:24.626041: step 36150, loss = 0.20, batch loss = 0.15 (29.4 examples/sec; 0.272 sec/batch; 22h:24m:27s remains)
INFO - root - 2017-12-16 06:31:27.487708: step 36160, loss = 0.22, batch loss = 0.16 (27.8 examples/sec; 0.288 sec/batch; 23h:40m:43s remains)
INFO - root - 2017-12-16 06:31:30.294080: step 36170, loss = 0.26, batch loss = 0.20 (29.7 examples/sec; 0.269 sec/batch; 22h:10m:52s remains)
INFO - root - 2017-12-16 06:31:33.083969: step 36180, loss = 0.36, batch loss = 0.30 (27.9 examples/sec; 0.287 sec/batch; 23h:36m:11s remains)
INFO - root - 2017-12-16 06:31:35.951186: step 36190, loss = 0.36, batch loss = 0.30 (27.7 examples/sec; 0.289 sec/batch; 23h:47m:22s remains)
INFO - root - 2017-12-16 06:31:38.758217: step 36200, loss = 0.22, batch loss = 0.16 (29.1 examples/sec; 0.275 sec/batch; 22h:39m:45s remains)
2017-12-16 06:31:39.252341: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6674776 -2.8388329 -3.1161833 -3.4692733 -3.9462044 -4.4104805 -4.8535333 -5.2934294 -5.6897225 -5.8194456 -5.5212331 -5.088418 -4.5525875 -3.8952165 -3.2781801][-2.6386886 -2.8717086 -3.2516513 -3.8012276 -4.3921113 -4.9854579 -5.5399513 -5.713346 -5.7146444 -5.7766476 -5.8516469 -5.6897063 -5.1496 -4.4661832 -3.7749081][-2.6959226 -3.213275 -3.7783728 -4.3876505 -4.8721285 -5.1698723 -5.2508554 -5.3800521 -5.5688939 -5.4006047 -5.2660313 -5.3550997 -5.4002295 -5.0371618 -4.3661103][-2.5579753 -3.3038306 -4.1821613 -4.9694929 -5.2500038 -4.9720259 -4.3990374 -3.8793788 -3.5086575 -3.6479228 -4.166182 -4.5973706 -4.939878 -4.9462266 -4.5765591][-2.5345521 -3.330658 -4.0940289 -4.5727282 -4.3901024 -3.3363371 -1.8861175 -0.73164845 -0.33339643 -0.81369495 -1.7499108 -3.1798067 -4.3553085 -4.843483 -4.6689844][-2.6916752 -3.5128665 -4.1731706 -4.1940665 -3.3172588 -1.4092548 0.77115726 2.5928826 3.3450303 2.5142565 0.68586874 -1.4973936 -3.2900357 -4.4203215 -4.5279875][-2.8584948 -3.4863048 -3.8737307 -3.3917282 -1.8931615 0.27938271 2.8369813 4.8780651 5.5265388 4.6806307 2.7355409 0.11359262 -2.2093098 -3.6027246 -3.9804816][-2.9475408 -3.3087718 -3.4124141 -2.3910694 -0.51212573 1.9581671 4.3784113 5.9118671 6.3254395 5.1048737 2.9869781 0.66370392 -1.4202077 -2.8272333 -3.3242936][-3.4291754 -3.85142 -3.7465913 -2.5045214 -0.79288626 1.4678917 3.71587 5.0130711 5.0313025 3.6903715 1.703455 -0.37545729 -2.1068962 -3.0864954 -3.3262014][-3.6878018 -4.0436912 -4.4687824 -3.8892567 -2.1588752 -0.240839 0.98013163 1.886631 2.103066 0.95274782 -0.65338445 -2.1679382 -3.3022442 -3.8433542 -3.7970312][-4.2784653 -4.41114 -4.6065822 -4.5740118 -4.1322846 -2.9711957 -1.6648107 -1.3346298 -1.6017241 -2.1919849 -2.8887725 -3.8257391 -4.3736353 -4.392571 -4.0272827][-4.6240034 -5.0722876 -5.5068326 -5.5080829 -5.121098 -4.8625965 -4.71133 -4.4684582 -4.4598 -4.7741222 -5.1099157 -5.3371015 -5.3150716 -5.0498767 -4.4637141][-4.5371404 -5.1947618 -5.8806186 -6.2024593 -6.3125067 -6.2129984 -5.9596925 -6.1001997 -6.2600465 -6.1726527 -6.1447597 -5.9948521 -5.6088929 -5.2046576 -4.7350416][-4.1119275 -4.6572003 -5.2574973 -5.8408585 -6.27818 -6.3212428 -6.2774911 -6.3056655 -6.1147327 -6.0194874 -5.8047523 -5.4745317 -5.2761841 -4.9329414 -4.5078549][-3.6068373 -3.916079 -4.3487716 -4.7836785 -5.1386018 -5.4166064 -5.5501943 -5.4501772 -5.2672935 -5.08977 -4.8166556 -4.5776548 -4.4204855 -4.4185514 -4.362659]]...]
INFO - root - 2017-12-16 06:31:42.091902: step 36210, loss = 0.23, batch loss = 0.18 (28.9 examples/sec; 0.277 sec/batch; 22h:46m:10s remains)
INFO - root - 2017-12-16 06:31:44.900691: step 36220, loss = 0.20, batch loss = 0.14 (29.2 examples/sec; 0.274 sec/batch; 22h:31m:52s remains)
INFO - root - 2017-12-16 06:31:47.739131: step 36230, loss = 0.24, batch loss = 0.18 (26.8 examples/sec; 0.298 sec/batch; 24h:32m:27s remains)
INFO - root - 2017-12-16 06:31:50.532330: step 36240, loss = 0.23, batch loss = 0.18 (29.3 examples/sec; 0.273 sec/batch; 22h:29m:59s remains)
INFO - root - 2017-12-16 06:31:53.344352: step 36250, loss = 0.29, batch loss = 0.23 (28.1 examples/sec; 0.284 sec/batch; 23h:23m:49s remains)
INFO - root - 2017-12-16 06:31:56.150660: step 36260, loss = 0.26, batch loss = 0.21 (28.9 examples/sec; 0.277 sec/batch; 22h:45m:57s remains)
INFO - root - 2017-12-16 06:31:58.955814: step 36270, loss = 0.23, batch loss = 0.17 (29.2 examples/sec; 0.274 sec/batch; 22h:32m:26s remains)
INFO - root - 2017-12-16 06:32:01.779199: step 36280, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 23h:00m:07s remains)
INFO - root - 2017-12-16 06:32:04.609251: step 36290, loss = 0.33, batch loss = 0.27 (28.3 examples/sec; 0.283 sec/batch; 23h:16m:02s remains)
INFO - root - 2017-12-16 06:32:07.518025: step 36300, loss = 0.28, batch loss = 0.22 (26.8 examples/sec; 0.298 sec/batch; 24h:31m:14s remains)
2017-12-16 06:32:07.999068: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9269557 -2.6343269 -2.7463598 -3.2261567 -3.8227034 -4.4358935 -5.0522823 -5.5472903 -5.7853107 -5.8524461 -5.8807263 -5.5964465 -5.0769267 -4.6282926 -4.448195][-1.8143244 -1.571676 -1.7838879 -2.3611636 -3.0314128 -3.8231785 -4.5958266 -5.1710114 -5.6185122 -5.8163781 -5.8491158 -5.681675 -5.197773 -4.5664616 -4.2323842][-0.59411311 -0.43708944 -0.84383965 -1.5479927 -2.3023434 -2.9988086 -3.7023275 -4.3383 -4.7049012 -4.9453735 -5.0730762 -4.8787551 -4.5188804 -4.0770721 -3.6988387][0.84399891 0.77225685 0.07271862 -0.77084947 -1.5410891 -2.0490749 -2.3829143 -2.7434392 -2.9139311 -3.2873697 -3.6363702 -3.9222882 -3.839885 -3.4201522 -2.9642296][1.3395133 1.0529227 0.26996469 -0.45898795 -0.93101168 -0.96218681 -0.77688694 -0.71563792 -0.76402831 -1.3475654 -2.1107965 -2.906332 -3.258215 -3.2182651 -2.7368937][0.27787638 0.097053051 -0.36513948 -0.576179 -0.32857609 0.31119823 1.0592928 1.5017858 1.4533148 0.48209381 -0.95373654 -2.3424163 -3.1456523 -3.3551235 -2.937314][-1.1911137 -1.0522263 -0.99684834 -0.68553472 0.3311348 1.6871142 2.9890223 3.5684972 3.2177472 1.9105258 0.057965279 -1.7886434 -2.9363797 -3.4017067 -3.2331877][-2.2557724 -2.0810018 -1.6994636 -0.72462821 0.59792948 2.2271051 3.6404161 4.3283567 3.8226662 2.1774859 0.12268639 -1.7147417 -2.9182155 -3.5480883 -3.5569956][-3.3771753 -3.168026 -2.6894803 -1.6743128 -0.21764565 1.4888239 2.8320622 3.3659739 2.7924776 1.2340727 -0.73285675 -2.5148768 -3.6260476 -4.1446195 -4.172174][-4.1163731 -4.1712623 -3.809206 -2.9371777 -1.5870006 -0.20040751 0.90558004 1.48809 1.1024704 -0.26064253 -1.8222392 -3.3176744 -4.2534752 -4.5997591 -4.5099263][-4.5658507 -4.8887858 -4.8137851 -4.35697 -3.5209997 -2.3740501 -1.5461538 -1.0613868 -1.2116888 -2.0157852 -2.9957075 -3.9303379 -4.4902472 -4.5390234 -4.200696][-4.746377 -5.2793221 -5.6008811 -5.3870192 -4.8692045 -4.11667 -3.49962 -3.0804453 -3.0923193 -3.4924664 -3.9572716 -4.4636869 -4.6890283 -4.4856615 -4.04514][-4.5694828 -4.9834986 -5.3208909 -5.2735777 -5.0241919 -4.5571051 -4.1343303 -3.9416831 -3.9413805 -4.175108 -4.391511 -4.5646067 -4.656652 -4.4422631 -4.0494556][-4.3747993 -4.53313 -4.6596065 -4.5028353 -4.3888459 -4.1015267 -3.7869315 -3.6966493 -3.626574 -3.8682206 -4.0402355 -4.1317344 -4.186542 -4.0973816 -3.9308639][-3.788868 -3.7464323 -3.7891808 -3.5659692 -3.4397283 -3.2574453 -3.0639482 -3.0208094 -3.0435185 -3.3163712 -3.6058631 -3.7546933 -3.9239922 -3.9899123 -3.9891868]]...]
INFO - root - 2017-12-16 06:32:10.804334: step 36310, loss = 0.28, batch loss = 0.22 (29.2 examples/sec; 0.274 sec/batch; 22h:34m:01s remains)
INFO - root - 2017-12-16 06:32:13.679686: step 36320, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.286 sec/batch; 23h:30m:00s remains)
INFO - root - 2017-12-16 06:32:16.484141: step 36330, loss = 0.26, batch loss = 0.20 (29.6 examples/sec; 0.270 sec/batch; 22h:13m:30s remains)
INFO - root - 2017-12-16 06:32:19.318104: step 36340, loss = 0.23, batch loss = 0.17 (28.8 examples/sec; 0.278 sec/batch; 22h:51m:15s remains)
INFO - root - 2017-12-16 06:32:22.235550: step 36350, loss = 0.36, batch loss = 0.31 (27.5 examples/sec; 0.291 sec/batch; 23h:57m:26s remains)
INFO - root - 2017-12-16 06:32:25.065890: step 36360, loss = 0.23, batch loss = 0.17 (28.9 examples/sec; 0.277 sec/batch; 22h:44m:45s remains)
INFO - root - 2017-12-16 06:32:27.876415: step 36370, loss = 0.31, batch loss = 0.25 (28.3 examples/sec; 0.283 sec/batch; 23h:16m:23s remains)
INFO - root - 2017-12-16 06:32:30.731374: step 36380, loss = 0.33, batch loss = 0.27 (28.8 examples/sec; 0.278 sec/batch; 22h:49m:40s remains)
INFO - root - 2017-12-16 06:32:33.603466: step 36390, loss = 0.30, batch loss = 0.24 (28.1 examples/sec; 0.284 sec/batch; 23h:24m:02s remains)
INFO - root - 2017-12-16 06:32:36.404719: step 36400, loss = 0.28, batch loss = 0.22 (26.2 examples/sec; 0.306 sec/batch; 25h:07m:47s remains)
2017-12-16 06:32:36.889217: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.803937 -4.0101147 -4.259213 -4.4937863 -4.7046795 -4.8157024 -4.7563767 -4.6127586 -4.5580034 -4.5014892 -4.3187566 -4.0435758 -3.7080965 -3.3077748 -3.1010118][-3.2935977 -3.5348172 -3.7921338 -4.0741463 -4.4449687 -4.8032441 -4.878664 -4.9032445 -4.79747 -4.6771722 -4.51396 -4.2858272 -3.9689364 -3.6277108 -3.4317534][-1.7193885 -1.9891331 -2.7328529 -3.3519614 -3.8468733 -4.2855058 -4.6678481 -4.949553 -5.1023283 -5.02766 -4.8148155 -4.5978379 -4.3593416 -4.0263557 -3.752388][-0.12147474 -0.053778648 -0.51755285 -1.6312263 -2.7904096 -3.4100046 -3.7406318 -3.8873458 -4.1004715 -4.5758615 -4.9119148 -4.9894824 -4.9672804 -4.7004089 -4.3396149][2.0985584 2.2035079 1.2654839 -0.067717075 -1.1861351 -1.9632976 -2.3049417 -2.524797 -2.8748436 -3.4917409 -4.0700903 -4.5713124 -4.8535271 -4.7235465 -4.3186269][3.0050201 3.396349 2.5785713 1.28721 0.2175889 -0.65861154 -1.1787486 -1.2579937 -1.5980501 -2.5096231 -3.4835992 -4.1184616 -4.4823675 -4.5548749 -4.2690129][3.5103674 3.649374 2.8478642 1.7663651 1.173326 0.79789734 0.57298565 0.20168352 -0.38208866 -1.4507833 -2.7206478 -3.5972435 -4.0419497 -4.0905094 -3.793828][2.4833937 2.7462215 2.1937981 1.6338348 1.5264425 1.8641701 2.0982099 1.6190281 0.85456371 -0.48670602 -2.0583165 -3.1574912 -3.7028286 -3.6594276 -3.2107227][0.30799866 0.42352152 0.43846369 0.56450939 0.93708611 1.5802264 2.2294149 2.0672951 1.2567329 -0.11838198 -1.5834577 -2.697356 -3.305779 -3.0095391 -2.3497977][-2.085346 -2.0446901 -1.8991468 -1.4260619 -0.40634251 0.56769371 1.116363 1.0268631 0.30080366 -0.82806993 -2.0327895 -2.8432279 -2.9063239 -2.5075803 -1.8224685][-4.6629953 -4.4194551 -3.9446023 -3.2711389 -2.1679881 -1.2308786 -0.36350441 -0.24542904 -0.89877653 -1.9314041 -2.858609 -3.1083226 -2.7541919 -2.000591 -1.1455863][-5.7868032 -6.0490341 -5.9930687 -5.3517818 -4.4191303 -3.4955368 -2.6727567 -2.2811849 -2.237551 -2.6081629 -3.1421051 -3.1915376 -2.6163888 -1.7005887 -0.97082138][-6.3067703 -6.825418 -7.1718349 -7.0657187 -6.3346448 -5.395977 -4.4280038 -3.5286758 -2.8981426 -2.6875372 -2.7705016 -2.6466346 -2.0940394 -1.3374426 -0.76559973][-5.7499828 -6.6471405 -7.4137869 -7.7362776 -7.2433267 -6.1847324 -4.77499 -3.5007973 -2.7070837 -2.1785638 -1.8503942 -1.5726697 -1.2619443 -0.78422165 -0.56261349][-5.1705647 -6.0609636 -7.0510464 -7.6000314 -7.1984076 -6.1701822 -4.7752314 -3.2452307 -2.054903 -1.4888761 -1.3052673 -0.99071956 -0.70909476 -0.47419453 -0.38381958]]...]
INFO - root - 2017-12-16 06:32:39.700563: step 36410, loss = 0.32, batch loss = 0.26 (27.0 examples/sec; 0.296 sec/batch; 24h:20m:21s remains)
INFO - root - 2017-12-16 06:32:42.526534: step 36420, loss = 0.23, batch loss = 0.17 (28.9 examples/sec; 0.277 sec/batch; 22h:46m:12s remains)
INFO - root - 2017-12-16 06:32:45.352979: step 36430, loss = 0.29, batch loss = 0.23 (28.8 examples/sec; 0.278 sec/batch; 22h:50m:28s remains)
INFO - root - 2017-12-16 06:32:48.192947: step 36440, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.279 sec/batch; 22h:56m:13s remains)
INFO - root - 2017-12-16 06:32:51.011814: step 36450, loss = 0.23, batch loss = 0.17 (27.6 examples/sec; 0.290 sec/batch; 23h:48m:38s remains)
INFO - root - 2017-12-16 06:32:53.832899: step 36460, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.283 sec/batch; 23h:14m:11s remains)
INFO - root - 2017-12-16 06:32:56.690113: step 36470, loss = 0.28, batch loss = 0.22 (28.0 examples/sec; 0.285 sec/batch; 23h:28m:21s remains)
INFO - root - 2017-12-16 06:32:59.555079: step 36480, loss = 0.35, batch loss = 0.29 (27.8 examples/sec; 0.288 sec/batch; 23h:40m:19s remains)
INFO - root - 2017-12-16 06:33:02.364638: step 36490, loss = 0.27, batch loss = 0.22 (27.9 examples/sec; 0.286 sec/batch; 23h:32m:57s remains)
INFO - root - 2017-12-16 06:33:05.198138: step 36500, loss = 0.34, batch loss = 0.28 (28.1 examples/sec; 0.285 sec/batch; 23h:23m:55s remains)
2017-12-16 06:33:05.674141: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5622041 -4.0997972 -4.5378723 -4.5525794 -4.4903455 -4.353766 -4.1689391 -3.8395505 -3.5573812 -3.2789 -3.2694612 -3.6169386 -3.6941736 -3.4711471 -3.0508585][-4.1827316 -4.6312728 -4.9405828 -5.0593715 -5.0482965 -4.7855344 -4.5014873 -4.2867432 -4.056911 -3.9560182 -3.9768982 -4.0284066 -4.0125456 -3.9244931 -3.5246456][-4.4956512 -4.7859278 -4.8845997 -4.7243757 -4.5135903 -4.2820415 -3.9859102 -3.8999786 -3.9756873 -4.12582 -4.2305193 -4.4473186 -4.5645084 -4.3678842 -3.9382539][-4.170155 -4.0956211 -3.8429637 -3.3973827 -3.0524552 -2.8240061 -2.7032795 -2.808249 -3.0573502 -3.6293094 -4.1312914 -4.3855181 -4.4448595 -4.4800668 -4.3053851][-3.2757752 -2.7080488 -2.139322 -1.4882288 -0.78856754 -0.38655281 -0.33875465 -0.627882 -1.169205 -2.0719612 -3.0195014 -3.9040942 -4.4309564 -4.6236143 -4.4981465][-2.4054053 -1.4664412 -0.62894773 0.45596838 1.4688916 1.9421144 2.1396565 1.7991762 1.0629988 -0.17529392 -1.5852516 -2.9093633 -4.0242581 -4.6881714 -4.6816683][-2.0079787 -0.77534795 0.41035843 1.8111367 2.9534569 3.8856211 4.478694 4.18069 3.3166957 1.9951315 0.31483078 -1.5143943 -3.0672607 -3.969317 -4.2283125][-1.5925145 -0.86871362 0.098151207 1.6864228 3.1337862 4.3665352 5.0939207 5.1216936 4.5995369 3.2606235 1.2936602 -0.67838764 -2.3242288 -3.4879045 -3.9830003][-2.3560736 -1.8853948 -1.1617534 -0.014204502 1.3123813 2.8616576 3.9258785 4.2652731 3.8559532 2.6757369 1.0946279 -0.75147343 -2.492064 -3.6020639 -4.0449877][-3.39113 -3.48702 -3.4084399 -2.4515376 -1.173512 0.28221083 1.546629 2.334487 2.2023482 1.2860494 -0.035954475 -1.6212244 -3.0335777 -3.9643703 -4.3457007][-4.9360285 -5.1143436 -5.0555539 -4.6670356 -3.9073412 -2.4370651 -0.91938186 -0.0066699982 0.11696434 -0.4053936 -1.3643456 -2.5764639 -3.6544852 -4.3279238 -4.5082388][-5.621973 -6.1531725 -6.4768295 -6.1217213 -5.4200644 -4.3492742 -3.1308222 -2.129359 -1.6273797 -1.8770075 -2.6135252 -3.5427008 -4.2711091 -4.6176338 -4.5824795][-5.670774 -6.1199656 -6.3481159 -6.061348 -5.5024714 -4.6851778 -3.7655342 -2.9820294 -2.4028025 -2.4742928 -2.9314992 -3.5909908 -4.1904674 -4.4628725 -4.377871][-5.1811743 -5.437274 -5.4473796 -5.1216807 -4.7185912 -3.9928608 -3.1472778 -2.6489439 -2.2619939 -2.1968875 -2.3895953 -2.9256141 -3.363544 -3.5972226 -3.674516][-4.1059217 -3.9625704 -3.7656507 -3.6128523 -3.3393946 -2.771749 -2.1433616 -1.5988 -1.1809003 -1.1525843 -1.3816428 -1.7668519 -2.2008104 -2.5893126 -2.8175273]]...]
INFO - root - 2017-12-16 06:33:08.543238: step 36510, loss = 0.26, batch loss = 0.20 (27.9 examples/sec; 0.287 sec/batch; 23h:33m:44s remains)
INFO - root - 2017-12-16 06:33:11.364273: step 36520, loss = 0.26, batch loss = 0.20 (29.6 examples/sec; 0.270 sec/batch; 22h:11m:47s remains)
INFO - root - 2017-12-16 06:33:14.155283: step 36530, loss = 0.23, batch loss = 0.18 (28.0 examples/sec; 0.286 sec/batch; 23h:29m:21s remains)
INFO - root - 2017-12-16 06:33:17.011456: step 36540, loss = 0.27, batch loss = 0.21 (28.4 examples/sec; 0.281 sec/batch; 23h:07m:09s remains)
INFO - root - 2017-12-16 06:33:19.897964: step 36550, loss = 0.34, batch loss = 0.28 (28.3 examples/sec; 0.282 sec/batch; 23h:12m:24s remains)
INFO - root - 2017-12-16 06:33:22.747271: step 36560, loss = 0.21, batch loss = 0.15 (28.1 examples/sec; 0.285 sec/batch; 23h:26m:33s remains)
INFO - root - 2017-12-16 06:33:25.587369: step 36570, loss = 0.39, batch loss = 0.33 (28.0 examples/sec; 0.286 sec/batch; 23h:30m:37s remains)
INFO - root - 2017-12-16 06:33:28.413113: step 36580, loss = 0.30, batch loss = 0.25 (28.5 examples/sec; 0.281 sec/batch; 23h:04m:30s remains)
INFO - root - 2017-12-16 06:33:31.254986: step 36590, loss = 0.29, batch loss = 0.23 (27.4 examples/sec; 0.292 sec/batch; 23h:58m:51s remains)
INFO - root - 2017-12-16 06:33:34.125108: step 36600, loss = 0.20, batch loss = 0.14 (27.8 examples/sec; 0.288 sec/batch; 23h:38m:17s remains)
2017-12-16 06:33:34.585422: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7647567 -4.316381 -4.8952165 -5.4513474 -5.8265882 -6.2519736 -6.8343163 -7.2850065 -7.2934823 -7.0176535 -6.694284 -6.34006 -5.713377 -4.97733 -4.5840645][-4.2692471 -4.9462156 -5.4672432 -5.9303684 -6.204514 -6.577621 -6.9374609 -7.3252096 -7.5367727 -7.4596534 -7.2071047 -6.7077837 -5.9979382 -5.1473989 -4.5070405][-4.726244 -5.5237212 -6.1126604 -6.3008547 -6.1918111 -6.121695 -6.2196646 -6.4033051 -6.5597138 -6.5819068 -6.5355091 -6.2456236 -5.7526016 -5.0824518 -4.5006189][-5.0565367 -5.8253717 -6.2921991 -6.2704625 -5.74747 -5.051878 -4.6870928 -4.5444236 -4.8030124 -5.1455445 -5.3953323 -5.4789715 -5.0818892 -4.6609397 -4.4071164][-4.9492984 -5.5574107 -5.6841431 -5.1475759 -4.1788497 -2.9246526 -2.0625906 -1.8241274 -2.2361956 -3.1035595 -3.7852364 -4.2728977 -4.362988 -4.1136203 -3.9053507][-4.8103094 -5.0208797 -4.8547964 -3.7769136 -2.1936288 -0.60536146 0.5970192 1.0682087 0.51493549 -0.71077466 -2.0035541 -2.8716998 -3.1159644 -3.3059027 -3.2469776][-4.73887 -4.5449572 -4.0328193 -2.8458207 -1.1539226 0.81482887 2.3779392 2.8473878 2.2977557 1.0337882 -0.28954697 -1.4597182 -1.9531212 -2.0962744 -1.9011452][-4.29934 -4.0577931 -3.4818234 -1.9671605 -0.19929981 1.467247 2.8502049 3.3039179 2.8869071 1.5764041 0.48360157 -0.26405573 -0.54461193 -0.59818387 -0.3845911][-3.9674971 -3.7138505 -3.1640353 -1.9704523 -0.63367844 0.90650988 2.1152439 2.4152384 2.08925 1.2442684 0.53991652 0.16854858 0.23537254 0.44491386 0.90901566][-4.1024408 -4.1154189 -3.9315805 -3.0552292 -1.9971755 -0.92153215 -0.089499474 0.29001331 0.25617743 -0.030404568 -0.10705137 0.071209908 0.69750452 1.0832772 1.5370927][-4.5046296 -4.8870597 -5.158186 -4.8672457 -4.2983384 -3.3634696 -2.6300082 -2.1705413 -1.8543434 -1.6507905 -1.066503 -0.38379097 0.41080666 0.95582485 1.4968982][-4.9367981 -5.575613 -6.2133441 -6.46096 -6.4286928 -5.9698648 -5.5331368 -5.0127945 -4.4092493 -3.6582704 -2.6129556 -1.7002785 -0.76464629 -0.27228451 0.16811752][-5.2863073 -6.1208668 -6.9335942 -7.5134988 -7.9535007 -7.93917 -7.7219763 -7.2045507 -6.4873443 -5.5828466 -4.4258113 -3.4867759 -2.7573066 -2.4456801 -1.925909][-5.6705117 -6.385973 -7.070487 -7.7373371 -8.3232365 -8.6039333 -8.6739082 -8.4733906 -7.893476 -7.0257168 -5.8968863 -5.0275235 -4.4293308 -4.2822137 -3.9390957][-5.67953 -6.3392291 -6.9455252 -7.530757 -8.02276 -8.4233532 -8.7003794 -8.7392826 -8.4902859 -7.9627924 -7.2174497 -6.4848967 -5.9061265 -5.6531053 -5.3416462]]...]
INFO - root - 2017-12-16 06:33:37.420201: step 36610, loss = 0.20, batch loss = 0.15 (28.7 examples/sec; 0.278 sec/batch; 22h:53m:07s remains)
INFO - root - 2017-12-16 06:33:40.228828: step 36620, loss = 0.25, batch loss = 0.19 (29.6 examples/sec; 0.271 sec/batch; 22h:14m:19s remains)
INFO - root - 2017-12-16 06:33:43.135250: step 36630, loss = 0.24, batch loss = 0.18 (25.4 examples/sec; 0.315 sec/batch; 25h:52m:06s remains)
INFO - root - 2017-12-16 06:33:45.936365: step 36640, loss = 0.28, batch loss = 0.23 (28.5 examples/sec; 0.281 sec/batch; 23h:03m:15s remains)
INFO - root - 2017-12-16 06:33:48.809757: step 36650, loss = 0.22, batch loss = 0.16 (28.9 examples/sec; 0.277 sec/batch; 22h:43m:39s remains)
INFO - root - 2017-12-16 06:33:51.641364: step 36660, loss = 0.25, batch loss = 0.19 (27.5 examples/sec; 0.291 sec/batch; 23h:53m:56s remains)
INFO - root - 2017-12-16 06:33:54.478518: step 36670, loss = 0.32, batch loss = 0.27 (27.9 examples/sec; 0.287 sec/batch; 23h:35m:09s remains)
INFO - root - 2017-12-16 06:33:57.312827: step 36680, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.281 sec/batch; 23h:04m:31s remains)
INFO - root - 2017-12-16 06:34:00.131438: step 36690, loss = 0.21, batch loss = 0.16 (26.2 examples/sec; 0.305 sec/batch; 25h:05m:06s remains)
INFO - root - 2017-12-16 06:34:02.952994: step 36700, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.277 sec/batch; 22h:45m:16s remains)
2017-12-16 06:34:03.420021: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9619961 -3.8006275 -3.710464 -3.6607034 -3.6416636 -3.5604753 -3.348594 -3.3029428 -3.1746192 -3.1775336 -3.0670662 -3.0758991 -3.3237469 -3.5546732 -3.7634435][-3.8018577 -3.6229341 -3.616116 -3.7784681 -4.0320883 -4.062705 -3.8743386 -3.6381865 -3.3179724 -3.2596655 -3.0723038 -3.1355939 -3.1706448 -3.032825 -3.0334873][-3.1231399 -3.3215964 -3.7258079 -4.1034436 -4.4295869 -4.4580016 -4.2305703 -4.0818686 -3.8763916 -3.6565843 -3.2866483 -3.12259 -3.0298851 -2.7775457 -2.4484239][-2.3969591 -2.8899865 -3.6272767 -4.3296013 -4.8608222 -4.7185149 -4.189393 -3.8708706 -3.6511 -3.3812542 -2.9757395 -2.9801261 -3.088788 -2.8547337 -2.5883398][-1.9024365 -2.7492776 -3.6483328 -4.3364758 -4.484745 -3.941659 -3.1567886 -2.4443016 -2.2655036 -2.4673972 -2.6309628 -2.7436132 -2.8338752 -2.9765539 -3.1887889][-2.0597949 -2.6229615 -3.12727 -3.5558276 -3.2410703 -2.3023357 -1.2848949 -0.54611754 -0.45303822 -0.91554761 -1.6163414 -2.4202483 -2.9389849 -2.9742241 -2.9814036][-2.4070885 -2.7147574 -2.9144807 -2.7807262 -1.8585517 -0.65291214 0.72719812 1.6183147 1.6635113 0.94280052 -0.18610573 -1.501801 -2.5539742 -3.0010831 -3.1739726][-2.701056 -3.1492524 -2.8573523 -2.2401419 -0.68281889 1.068943 2.5324821 3.2496386 3.0543318 2.0658407 0.61172295 -0.9528811 -2.3795404 -3.0471334 -3.0619354][-3.1293297 -3.51617 -3.0508714 -2.2474172 -0.848346 1.0591073 2.7768703 3.514729 3.1235352 1.6815381 0.09265089 -1.2600353 -2.4474704 -3.1336565 -3.20677][-3.4850814 -4.1432066 -4.3713155 -3.2402568 -1.6235311 -0.52121687 0.7308321 1.608202 1.6678424 0.67554 -0.85402894 -2.2132342 -3.0919795 -3.2493715 -3.05727][-4.320065 -4.8436437 -4.91607 -4.3541613 -3.2097411 -1.7687893 -0.7841177 -0.55707049 -0.67136073 -1.3578303 -2.2652972 -3.0669065 -3.5193448 -3.4572327 -3.2110963][-5.1798429 -5.546392 -5.6502314 -5.2686443 -4.5667124 -3.6569238 -3.0695567 -2.8636913 -2.612093 -3.0172713 -3.6077518 -4.0010538 -4.1472006 -3.8952599 -3.4486883][-5.045464 -5.4997582 -5.8232074 -5.5234785 -4.8031111 -4.2617788 -3.9489849 -4.1536255 -4.1509748 -3.8947396 -3.6794438 -3.8403938 -3.8428707 -3.496892 -3.2138867][-4.8390112 -5.0786285 -5.0167994 -4.778059 -4.4866261 -3.9387779 -3.6598978 -3.8324819 -3.9371576 -3.9606333 -3.6518011 -3.4512942 -3.2614167 -2.8981292 -2.7147374][-4.3656411 -4.4254851 -4.2836995 -3.7812169 -3.3411114 -3.0887644 -3.0461845 -3.1151123 -3.296097 -3.405292 -3.2783532 -3.1759305 -3.0509226 -2.9259844 -2.7156792]]...]
INFO - root - 2017-12-16 06:34:06.266671: step 36710, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 23h:10m:30s remains)
INFO - root - 2017-12-16 06:34:09.132193: step 36720, loss = 0.39, batch loss = 0.33 (27.2 examples/sec; 0.294 sec/batch; 24h:10m:46s remains)
INFO - root - 2017-12-16 06:34:11.935662: step 36730, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.280 sec/batch; 23h:02m:33s remains)
INFO - root - 2017-12-16 06:34:14.780208: step 36740, loss = 0.25, batch loss = 0.19 (27.1 examples/sec; 0.296 sec/batch; 24h:16m:38s remains)
INFO - root - 2017-12-16 06:34:17.617583: step 36750, loss = 0.22, batch loss = 0.16 (28.4 examples/sec; 0.282 sec/batch; 23h:10m:52s remains)
INFO - root - 2017-12-16 06:34:20.515372: step 36760, loss = 0.22, batch loss = 0.16 (28.1 examples/sec; 0.284 sec/batch; 23h:20m:55s remains)
INFO - root - 2017-12-16 06:34:23.352536: step 36770, loss = 0.25, batch loss = 0.19 (28.5 examples/sec; 0.281 sec/batch; 23h:04m:10s remains)
INFO - root - 2017-12-16 06:34:26.177112: step 36780, loss = 0.27, batch loss = 0.21 (29.1 examples/sec; 0.275 sec/batch; 22h:33m:22s remains)
INFO - root - 2017-12-16 06:34:28.997665: step 36790, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.282 sec/batch; 23h:11m:17s remains)
INFO - root - 2017-12-16 06:34:31.791105: step 36800, loss = 0.32, batch loss = 0.26 (26.4 examples/sec; 0.303 sec/batch; 24h:53m:12s remains)
2017-12-16 06:34:32.247613: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7242885 -3.4142475 -4.1617823 -4.7999792 -5.26773 -5.5454507 -5.8215103 -5.8733435 -5.8116083 -5.7531695 -5.4983721 -5.1371922 -5.0076156 -4.8887219 -4.7458382][-1.8895531 -2.5867963 -3.2821212 -3.941853 -4.4605784 -4.7734718 -4.9172983 -4.8206377 -4.7441273 -4.6708665 -4.4850292 -4.2075539 -4.1388421 -4.1266832 -4.1574864][-1.6023324 -2.3526521 -3.1468396 -3.5810444 -3.7757721 -3.7740626 -3.5716949 -3.381187 -3.2887282 -3.1622443 -3.1014383 -3.1852334 -3.3779249 -3.616643 -3.9732184][-1.9095583 -2.6634173 -3.4936049 -3.8862255 -3.7670915 -3.2235646 -2.4113393 -1.7067235 -1.3012493 -1.3238771 -1.664098 -1.9409356 -2.4597125 -3.127974 -3.6619377][-2.6462891 -3.5146296 -4.2712374 -4.5146194 -4.12853 -2.9472551 -1.408947 -0.069075108 0.59162855 0.490242 0.0076141357 -0.85155439 -1.6133747 -2.3471687 -3.0456171][-3.3424401 -4.1666884 -4.7434955 -4.6463017 -3.8070092 -2.2104495 -0.13193512 1.7015667 2.5327992 2.1082263 1.1999226 0.13737965 -0.69740534 -1.3738801 -2.0749445][-3.7933567 -4.2340117 -4.5754418 -4.2966118 -2.9552803 -0.91262984 1.4788094 3.5620642 4.41934 3.8826542 2.5249648 1.1230731 0.24433374 -0.39320135 -0.85257864][-3.8128603 -3.9079905 -4.1106963 -3.5719104 -2.2559071 -0.11311817 2.4593167 4.3602781 5.1673737 4.746604 3.5312653 1.9347172 0.97001839 0.33973837 -0.094061852][-3.6237016 -3.4558461 -3.1564572 -2.75146 -1.9924023 -0.14786911 2.098628 3.7239246 4.5832853 3.888875 2.6505332 1.6816483 1.1399374 0.68380356 0.19062662][-3.0175114 -2.9907415 -3.1631553 -3.0091121 -2.3339005 -1.2495141 0.19840717 1.7089844 2.4816508 2.064908 1.2011571 0.54602623 0.295815 0.0036444664 -0.5680635][-3.1961534 -2.9448438 -3.063729 -3.4400406 -3.4095318 -2.72259 -1.7756724 -0.94067979 -0.41297722 -0.61736369 -1.1092129 -1.4067092 -1.5695214 -1.7468688 -2.0253139][-3.0047545 -3.139955 -3.6215715 -4.0429258 -4.0963621 -4.0003982 -3.4731858 -2.7304688 -2.3987229 -2.6380413 -2.7917747 -3.123425 -3.5146642 -3.7112403 -4.0591068][-2.9305942 -3.3170738 -3.8150148 -4.361743 -4.8983617 -4.9018388 -4.3934941 -3.8492677 -3.4639239 -3.5788324 -3.684478 -3.685257 -3.8506234 -4.3034134 -4.8542671][-3.0406075 -3.4685204 -3.8752935 -4.491601 -5.0687418 -5.1499457 -4.9337215 -4.6433234 -4.1694722 -3.7195282 -3.4741433 -3.8077145 -4.3893309 -4.6961675 -4.8059268][-2.4816561 -2.7822237 -3.3355675 -4.149169 -4.7940879 -4.816401 -4.440752 -4.0245461 -3.4971488 -3.1108232 -3.1086545 -3.2867246 -3.7333424 -4.3622808 -4.6752658]]...]
INFO - root - 2017-12-16 06:34:35.090679: step 36810, loss = 0.27, batch loss = 0.21 (28.7 examples/sec; 0.279 sec/batch; 22h:53m:55s remains)
INFO - root - 2017-12-16 06:34:37.933994: step 36820, loss = 0.27, batch loss = 0.21 (29.4 examples/sec; 0.272 sec/batch; 22h:22m:18s remains)
INFO - root - 2017-12-16 06:34:40.766322: step 36830, loss = 0.27, batch loss = 0.21 (28.3 examples/sec; 0.283 sec/batch; 23h:14m:33s remains)
INFO - root - 2017-12-16 06:34:43.591746: step 36840, loss = 0.26, batch loss = 0.20 (29.7 examples/sec; 0.269 sec/batch; 22h:07m:49s remains)
INFO - root - 2017-12-16 06:34:46.451983: step 36850, loss = 0.25, batch loss = 0.20 (27.7 examples/sec; 0.289 sec/batch; 23h:43m:34s remains)
INFO - root - 2017-12-16 06:34:49.283134: step 36860, loss = 0.25, batch loss = 0.19 (27.7 examples/sec; 0.288 sec/batch; 23h:41m:12s remains)
INFO - root - 2017-12-16 06:34:52.150442: step 36870, loss = 0.23, batch loss = 0.17 (27.7 examples/sec; 0.289 sec/batch; 23h:43m:31s remains)
INFO - root - 2017-12-16 06:34:55.073747: step 36880, loss = 0.27, batch loss = 0.21 (28.4 examples/sec; 0.282 sec/batch; 23h:07m:54s remains)
INFO - root - 2017-12-16 06:34:57.870935: step 36890, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.281 sec/batch; 23h:04m:23s remains)
INFO - root - 2017-12-16 06:35:00.736290: step 36900, loss = 0.31, batch loss = 0.25 (28.3 examples/sec; 0.282 sec/batch; 23h:10m:39s remains)
2017-12-16 06:35:01.209614: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3478351 -5.1669674 -4.8570685 -4.4641285 -4.2985892 -4.3893456 -4.6775956 -4.7964463 -4.8297868 -4.64518 -4.4060993 -4.2189097 -3.9522281 -3.8199334 -3.4120355][-6.6135931 -6.2411 -5.7894125 -5.6493835 -5.6762619 -5.7256756 -5.8555827 -5.8064117 -5.7361193 -5.4805837 -5.2849827 -4.9358053 -4.7017441 -4.6760874 -4.3078551][-7.660151 -7.5193515 -7.1885862 -6.8826327 -6.6861758 -6.6337385 -6.6036949 -6.3945155 -6.0397148 -5.625392 -5.2807708 -5.2255139 -5.3583355 -5.3608327 -5.0881839][-8.581377 -8.5136757 -8.17968 -7.9399538 -7.6330681 -7.1330566 -6.4360228 -5.5148077 -4.732316 -4.1564364 -4.0657444 -4.3493767 -4.8185916 -5.2346778 -5.4265919][-8.1743059 -8.33922 -8.1443977 -7.724123 -6.9111981 -5.7962675 -4.6105275 -3.487042 -2.6053002 -2.182863 -2.2105818 -2.9727507 -4.028409 -4.7873855 -4.9349813][-6.9195127 -7.1546364 -7.0529013 -6.5261927 -5.4879422 -3.818069 -1.8480306 -0.16713572 0.79792643 0.98135042 0.22877455 -0.8923521 -2.0206807 -3.2996693 -3.8505096][-5.27784 -5.453773 -5.4893689 -4.9076285 -3.5503807 -1.6511586 0.5210166 2.3102856 3.3398185 3.26362 2.3280625 0.74929476 -0.86206269 -1.9247932 -2.3139737][-3.9402497 -4.0993986 -3.967741 -3.2357578 -1.9191065 0.19527626 2.455297 4.0699587 4.6928358 3.9656935 2.5990391 1.2307801 0.16982412 -0.6538713 -1.0655611][-2.4361291 -2.6781249 -3.0139544 -2.5245104 -1.3856122 0.51831722 2.3920665 3.68536 4.1072826 3.2627726 1.7523556 0.42005634 -0.24818516 -0.38104582 -0.41301107][-1.2034597 -1.6558328 -2.3160603 -2.2785413 -1.4350607 0.04475069 1.2946787 2.3430438 2.4244094 1.59271 0.40944672 -0.6514051 -0.85992432 -0.49049234 -0.31929398][-0.56858468 -1.1945574 -1.9684157 -2.4616132 -2.3803461 -1.367538 -0.13612318 0.527823 0.18947315 -0.5276401 -1.3317053 -1.7755451 -1.6216252 -1.24249 -0.91179991][-0.23258114 -0.94703627 -1.9025412 -2.6028545 -2.52807 -2.1950154 -1.9260995 -1.5137615 -1.5774689 -2.0568426 -2.7078555 -2.9576013 -2.5558724 -1.966639 -1.6948977][-0.4453733 -0.85884356 -1.747674 -2.7927628 -3.3183451 -3.0228491 -2.5141187 -2.409744 -2.8018935 -3.3062053 -3.6746669 -3.8101301 -3.5042655 -3.0957644 -2.8453727][-0.64777875 -0.91035342 -1.6653283 -2.6376331 -3.1841524 -3.286428 -3.1749768 -2.9958739 -3.1837192 -3.5150466 -3.7220814 -3.7785754 -3.7757576 -3.8314302 -3.7560134][-1.3597605 -1.3613844 -1.8160627 -2.6723957 -3.2882218 -3.5518241 -3.7311797 -3.6537833 -3.6206057 -3.7868886 -4.052247 -3.9867465 -3.6370704 -3.6708457 -3.6973913]]...]
INFO - root - 2017-12-16 06:35:04.089179: step 36910, loss = 0.25, batch loss = 0.19 (27.8 examples/sec; 0.288 sec/batch; 23h:38m:13s remains)
INFO - root - 2017-12-16 06:35:06.946542: step 36920, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.282 sec/batch; 23h:09m:45s remains)
INFO - root - 2017-12-16 06:35:09.794239: step 36930, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.280 sec/batch; 22h:57m:56s remains)
INFO - root - 2017-12-16 06:35:12.656700: step 36940, loss = 0.24, batch loss = 0.18 (27.8 examples/sec; 0.288 sec/batch; 23h:37m:07s remains)
INFO - root - 2017-12-16 06:35:15.437827: step 36950, loss = 0.25, batch loss = 0.19 (28.7 examples/sec; 0.278 sec/batch; 22h:51m:34s remains)
INFO - root - 2017-12-16 06:35:18.264222: step 36960, loss = 0.22, batch loss = 0.16 (29.2 examples/sec; 0.274 sec/batch; 22h:30m:09s remains)
INFO - root - 2017-12-16 06:35:21.049678: step 36970, loss = 0.28, batch loss = 0.22 (29.9 examples/sec; 0.268 sec/batch; 21h:58m:29s remains)
INFO - root - 2017-12-16 06:35:23.891981: step 36980, loss = 0.34, batch loss = 0.28 (27.0 examples/sec; 0.297 sec/batch; 24h:21m:55s remains)
INFO - root - 2017-12-16 06:35:26.721852: step 36990, loss = 0.34, batch loss = 0.28 (28.1 examples/sec; 0.285 sec/batch; 23h:23m:50s remains)
INFO - root - 2017-12-16 06:35:29.497778: step 37000, loss = 0.39, batch loss = 0.34 (29.2 examples/sec; 0.274 sec/batch; 22h:29m:09s remains)
2017-12-16 06:35:29.953074: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4674048 -4.811523 -5.0290694 -5.0866861 -5.069519 -4.9786448 -4.8625913 -4.83228 -4.8900857 -5.0607314 -5.3095145 -5.4316998 -5.4054594 -5.2469072 -4.9529428][-4.7545586 -5.1214452 -5.2779822 -5.3002658 -5.2663283 -5.2635312 -5.2620091 -5.2482405 -5.3137522 -5.5808611 -5.8996944 -6.1961136 -6.3978786 -6.3310575 -6.0194616][-4.953568 -5.3560429 -5.4328537 -5.1646075 -4.845767 -4.6768413 -4.617341 -4.7861013 -5.1094289 -5.5308223 -5.9513683 -6.3959007 -6.7603626 -6.9853716 -7.035862][-5.1283174 -5.2689009 -5.00077 -4.4520507 -3.9013886 -3.4658108 -3.2273674 -3.3003073 -3.6267834 -4.2984819 -5.0231152 -5.8324251 -6.5788908 -7.2134829 -7.5681562][-5.3788075 -5.1083012 -4.2988281 -3.3166637 -2.3979421 -1.651273 -1.1245203 -0.95390344 -1.0991619 -1.8324857 -2.8606081 -4.2887597 -5.6928983 -6.9096375 -7.6863728][-5.3046813 -4.9434228 -4.0208755 -2.528605 -0.90396309 0.3183856 1.2102294 1.6500106 1.5507441 0.87303543 -0.23399353 -2.1502156 -4.1791811 -6.08705 -7.2974043][-5.5069442 -4.8869276 -3.6934803 -1.8586776 0.076330662 1.7049356 3.0162892 3.7068768 3.8195381 2.9842272 1.6950731 -0.40921879 -2.7283933 -5.1060467 -6.6604681][-6.2844219 -5.2675347 -3.6389632 -1.6534567 0.37762117 2.2022 3.7205467 4.4568634 4.5181351 3.6318665 2.2128978 0.02879858 -2.2803972 -4.5376325 -6.0569692][-6.8515387 -5.9642572 -4.4835954 -2.4712093 -0.44403577 1.3592882 2.8414569 3.6434517 3.76587 3.0167465 1.7980142 -0.40378189 -2.7242565 -4.8740573 -6.11907][-7.3371658 -6.626318 -5.5407372 -3.7938867 -1.8725657 -0.14369106 1.253263 2.0119743 2.0675998 1.431705 0.2458005 -1.7189932 -3.7893229 -5.7884483 -6.8450885][-7.47811 -6.9931273 -6.1864643 -4.9157763 -3.6127646 -2.1277621 -0.74628234 -0.040500641 -0.21286297 -0.81400061 -1.6905017 -3.2604036 -4.8105874 -6.3416367 -7.3074827][-7.2878337 -6.6762657 -6.0495739 -5.322567 -4.6083465 -3.7476983 -2.8476086 -2.1240442 -1.9816544 -2.4286258 -3.1995358 -4.5141296 -5.7936096 -6.8432703 -7.3778419][-6.7650108 -6.239893 -5.7238169 -5.260262 -4.8849821 -4.3655272 -4.0320807 -3.6704571 -3.532202 -3.6899076 -4.1492972 -5.1620045 -6.1168928 -7.0506282 -7.4571171][-5.4914317 -5.2195668 -5.0194368 -4.8813672 -4.6871414 -4.3814664 -4.2295785 -4.0984373 -4.2217641 -4.2453079 -4.4406719 -5.07321 -5.6869254 -6.2860966 -6.5256248][-4.6126943 -4.1110125 -3.7220578 -3.7961512 -3.8818028 -3.7990992 -3.8826299 -3.8601782 -3.984812 -4.1084795 -4.280158 -4.5321178 -4.7330132 -5.0356708 -5.127018]]...]
INFO - root - 2017-12-16 06:35:32.793815: step 37010, loss = 0.25, batch loss = 0.19 (28.9 examples/sec; 0.277 sec/batch; 22h:44m:25s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 06:35:35.624599: step 37020, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 23h:29m:11s remains)
INFO - root - 2017-12-16 06:35:38.440152: step 37030, loss = 0.22, batch loss = 0.16 (27.8 examples/sec; 0.288 sec/batch; 23h:38m:02s remains)
INFO - root - 2017-12-16 06:35:41.291873: step 37040, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 23h:18m:20s remains)
INFO - root - 2017-12-16 06:35:44.142594: step 37050, loss = 0.20, batch loss = 0.14 (27.9 examples/sec; 0.286 sec/batch; 23h:29m:36s remains)
INFO - root - 2017-12-16 06:35:46.936842: step 37060, loss = 0.28, batch loss = 0.22 (29.5 examples/sec; 0.272 sec/batch; 22h:16m:59s remains)
INFO - root - 2017-12-16 06:35:49.751769: step 37070, loss = 0.25, batch loss = 0.19 (27.2 examples/sec; 0.294 sec/batch; 24h:09m:10s remains)
INFO - root - 2017-12-16 06:35:52.555746: step 37080, loss = 0.21, batch loss = 0.15 (29.0 examples/sec; 0.275 sec/batch; 22h:36m:01s remains)
INFO - root - 2017-12-16 06:35:55.380711: step 37090, loss = 0.30, batch loss = 0.24 (29.3 examples/sec; 0.273 sec/batch; 22h:24m:15s remains)
INFO - root - 2017-12-16 06:35:58.219965: step 37100, loss = 0.25, batch loss = 0.19 (27.9 examples/sec; 0.287 sec/batch; 23h:33m:19s remains)
2017-12-16 06:35:58.674435: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3126321 -3.9621036 -3.6944609 -3.4734411 -3.2193015 -3.0553746 -2.9706864 -2.9200144 -3.0031071 -3.0401871 -3.016906 -2.7972555 -2.5490086 -2.3800528 -2.1821721][-3.8600883 -3.705822 -3.5897431 -3.4457619 -3.3032293 -3.2094312 -3.1116309 -2.9573164 -2.842236 -2.8886786 -2.9945359 -2.81699 -2.5430741 -2.3763542 -2.200794][-3.3317049 -3.3396382 -3.4355388 -3.3304372 -3.169373 -3.0713723 -2.9000039 -2.6387744 -2.4715843 -2.6125386 -2.8082023 -2.7664223 -2.6588206 -2.5883324 -2.5641043][-2.2571862 -2.4880083 -2.7899566 -2.7596228 -2.5565214 -2.3374627 -2.0027366 -1.7017908 -1.50035 -1.7525828 -2.1153908 -2.3990757 -2.6294909 -2.7029138 -2.7832613][-1.2593055 -1.4490299 -1.7292421 -1.66295 -1.4047284 -1.111944 -0.58550549 -0.21548796 -0.14437246 -0.70827889 -1.4294813 -1.9409437 -2.3394618 -2.5796642 -2.7828951][-1.1834314 -1.0688577 -0.97682142 -0.63338113 -0.042253971 0.44251823 0.99667883 1.4159217 1.4401431 0.68527126 -0.33344173 -1.1186039 -1.691613 -2.1389689 -2.4336836][-1.4162326 -1.1421795 -0.82540846 -0.15657902 0.77454948 1.4962363 2.2612276 2.6824131 2.6326342 1.7259793 0.50986338 -0.50881457 -1.1819353 -1.597512 -1.8646023][-1.7722418 -1.3801203 -0.93074727 -0.15896606 0.76086235 1.5773392 2.490705 2.9372969 2.9039412 1.8890529 0.59793139 -0.41745186 -1.0596807 -1.3337531 -1.3758063][-2.3694115 -2.0860627 -1.725925 -1.1494703 -0.34349155 0.51295042 1.5381494 2.0546632 2.0594559 1.1791005 0.037733078 -0.87208462 -1.3355541 -1.3619652 -1.036443][-3.0594106 -2.9808078 -2.943378 -2.6802888 -2.0992374 -1.4268703 -0.58402658 -0.040492058 0.060421944 -0.59623408 -1.4413533 -2.2455695 -2.5240188 -2.2660832 -1.6321392][-3.4215136 -3.6272798 -3.9883902 -4.0529265 -3.9440062 -3.5970426 -2.922009 -2.5193765 -2.5178571 -3.0526872 -3.6819139 -4.105444 -4.1497936 -3.7332618 -2.8371801][-3.4727521 -3.833113 -4.4819074 -4.95615 -5.2344728 -5.1999187 -4.9070654 -4.8229322 -4.9124894 -5.2503281 -5.6586804 -5.8310618 -5.63873 -5.0547132 -4.0364518][-3.4376788 -3.7860236 -4.4171371 -5.0345445 -5.6680055 -6.0565481 -6.162631 -6.2979531 -6.5416183 -6.8343935 -6.9352627 -6.8388252 -6.53866 -5.9333372 -4.9644728][-2.9896145 -3.2390871 -3.7791674 -4.4535666 -5.2717185 -5.8936634 -6.3648105 -6.716125 -7.0936632 -7.3181629 -7.322938 -7.1252794 -6.7555227 -6.3556242 -5.6619015][-2.4142663 -2.5425673 -3.0435257 -3.6617188 -4.4902587 -5.3282681 -6.0434675 -6.4956641 -6.7888489 -6.8633413 -6.8872023 -6.6091175 -6.1446304 -5.8241897 -5.4398694]]...]
INFO - root - 2017-12-16 06:36:01.478234: step 37110, loss = 0.34, batch loss = 0.28 (28.9 examples/sec; 0.276 sec/batch; 22h:40m:36s remains)
INFO - root - 2017-12-16 06:36:04.341483: step 37120, loss = 0.25, batch loss = 0.19 (27.5 examples/sec; 0.291 sec/batch; 23h:54m:34s remains)
INFO - root - 2017-12-16 06:36:07.155945: step 37130, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.285 sec/batch; 23h:21m:02s remains)
INFO - root - 2017-12-16 06:36:09.984782: step 37140, loss = 0.25, batch loss = 0.20 (27.6 examples/sec; 0.290 sec/batch; 23h:46m:58s remains)
INFO - root - 2017-12-16 06:36:12.842648: step 37150, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 22h:52m:10s remains)
INFO - root - 2017-12-16 06:36:15.618804: step 37160, loss = 0.22, batch loss = 0.16 (29.0 examples/sec; 0.275 sec/batch; 22h:35m:43s remains)
INFO - root - 2017-12-16 06:36:18.462418: step 37170, loss = 0.29, batch loss = 0.23 (27.8 examples/sec; 0.288 sec/batch; 23h:36m:38s remains)
INFO - root - 2017-12-16 06:36:21.269330: step 37180, loss = 0.32, batch loss = 0.26 (28.6 examples/sec; 0.280 sec/batch; 22h:58m:15s remains)
INFO - root - 2017-12-16 06:36:24.059287: step 37190, loss = 0.20, batch loss = 0.14 (29.1 examples/sec; 0.275 sec/batch; 22h:34m:36s remains)
INFO - root - 2017-12-16 06:36:26.904429: step 37200, loss = 0.26, batch loss = 0.21 (28.9 examples/sec; 0.277 sec/batch; 22h:42m:19s remains)
2017-12-16 06:36:27.347890: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1500473 -5.8364615 -6.0866127 -6.3755755 -6.1890173 -6.2148981 -5.9326925 -5.7156858 -5.5784616 -5.1028857 -4.862422 -4.6288676 -4.7301989 -5.056365 -5.5141983][-3.6562412 -4.1635227 -4.3986735 -4.7189679 -4.7199244 -4.7748833 -4.6277971 -4.6071835 -4.6240406 -4.3707542 -4.5227852 -4.6236348 -4.7771916 -5.0593791 -5.2840204][-2.1459019 -2.3819971 -2.6787438 -2.8808122 -2.7886705 -2.9490619 -2.908679 -3.1612959 -3.4238286 -3.6378956 -4.0782256 -4.5655646 -4.9171057 -4.9075189 -4.8060012][-0.33408976 -0.4988544 -0.89617658 -1.4197249 -1.5147288 -1.5328422 -1.3424313 -1.5053937 -2.1084583 -2.941853 -3.6939054 -4.3031864 -4.7026119 -4.6530919 -4.5778565][0.72223425 0.68371296 0.41332293 -0.037923336 -0.0027980804 0.2167387 0.71451139 0.63921452 -0.10585403 -1.1621685 -2.4072564 -3.422965 -4.0135942 -4.0419645 -4.1604743][1.2120733 1.4331393 1.3367548 1.2358909 1.8627334 2.7158723 3.5546765 3.6488266 2.8596182 1.4201241 -0.17043734 -1.4681196 -2.2318742 -2.8084531 -3.5233827][1.0263214 1.3783517 1.3677616 1.647202 2.5745606 3.8659658 5.0123281 5.1777773 4.6048594 3.374959 1.8230214 0.46658659 -0.52478933 -1.3812466 -2.3349791][0.35459328 0.55857515 0.28127527 0.779119 1.9382782 3.40092 4.9630175 5.3806953 5.346488 4.703619 3.6435509 2.4145575 1.1102686 -0.1054492 -1.7308176][-0.92966127 -1.0743272 -1.2994285 -0.91112566 -0.021129131 1.5654902 3.4121332 4.4316292 5.0188437 4.8394632 4.1612539 3.2476892 2.053864 0.46663618 -1.615787][-1.8810484 -2.3541198 -3.0810194 -2.8972096 -2.0836084 -0.74463916 0.60987282 1.931355 3.0462041 3.4123425 3.6635437 3.1239004 2.2106714 0.59178686 -1.7280254][-2.7246065 -3.3123155 -4.1654305 -4.4153056 -4.1926842 -3.245739 -1.9068723 -0.66106963 0.57237387 1.4818373 2.1290312 2.0877023 1.2560749 -0.28867483 -2.4529772][-4.2667103 -5.1480727 -6.2522483 -6.7203174 -6.6055694 -6.0412745 -5.3261423 -4.2143431 -2.9086251 -1.5755434 -0.85396266 -0.669883 -1.0332482 -2.2641902 -3.7471814][-5.4926147 -6.64701 -7.7919044 -8.52751 -8.912159 -8.4899025 -7.9006262 -6.8799715 -5.7295609 -4.6271896 -3.8014688 -3.5665143 -3.8940144 -4.583869 -5.5379782][-6.1530657 -7.0846996 -8.0860567 -8.886713 -9.456068 -9.4534311 -9.2231979 -8.6907425 -7.9235506 -7.2086296 -6.7076111 -6.56834 -6.5670033 -6.6661768 -6.9603539][-6.2344437 -6.95595 -7.6859236 -8.4044046 -8.9713192 -9.1934681 -9.3631287 -9.2865915 -9.324192 -9.2407608 -8.9688177 -8.7997 -8.53298 -8.2852983 -7.8443117]]...]
INFO - root - 2017-12-16 06:36:30.173356: step 37210, loss = 0.23, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 23h:21m:04s remains)
INFO - root - 2017-12-16 06:36:33.071238: step 37220, loss = 0.28, batch loss = 0.22 (28.0 examples/sec; 0.286 sec/batch; 23h:27m:52s remains)
INFO - root - 2017-12-16 06:36:35.892592: step 37230, loss = 0.21, batch loss = 0.16 (29.1 examples/sec; 0.274 sec/batch; 22h:30m:43s remains)
INFO - root - 2017-12-16 06:36:38.750694: step 37240, loss = 0.26, batch loss = 0.20 (26.0 examples/sec; 0.307 sec/batch; 25h:12m:31s remains)
INFO - root - 2017-12-16 06:36:41.611828: step 37250, loss = 0.25, batch loss = 0.20 (26.7 examples/sec; 0.299 sec/batch; 24h:33m:16s remains)
INFO - root - 2017-12-16 06:36:44.429359: step 37260, loss = 0.24, batch loss = 0.19 (28.7 examples/sec; 0.279 sec/batch; 22h:52m:05s remains)
INFO - root - 2017-12-16 06:36:47.269789: step 37270, loss = 0.31, batch loss = 0.26 (29.1 examples/sec; 0.275 sec/batch; 22h:30m:59s remains)
INFO - root - 2017-12-16 06:36:50.083369: step 37280, loss = 0.34, batch loss = 0.28 (28.4 examples/sec; 0.282 sec/batch; 23h:07m:36s remains)
INFO - root - 2017-12-16 06:36:52.926731: step 37290, loss = 0.25, batch loss = 0.19 (27.0 examples/sec; 0.296 sec/batch; 24h:18m:38s remains)
INFO - root - 2017-12-16 06:36:55.770836: step 37300, loss = 0.29, batch loss = 0.24 (29.1 examples/sec; 0.275 sec/batch; 22h:30m:48s remains)
2017-12-16 06:36:56.230607: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0819845 -6.1963091 -6.6113386 -7.4112349 -8.252367 -8.5767231 -8.5747719 -8.23388 -7.855545 -7.1870232 -6.3960333 -5.4978886 -4.5285487 -3.8046958 -3.3327584][-5.8551826 -6.1817884 -6.6381626 -7.3391628 -7.8859115 -8.5835133 -8.8291016 -8.7443676 -8.538538 -8.07397 -7.4738646 -6.6058331 -5.6595464 -4.9579577 -4.2851157][-4.9689946 -5.1216578 -5.4527059 -6.0811448 -6.2996345 -6.7872748 -7.2103863 -7.6268587 -8.0322609 -8.0645924 -7.97649 -7.4438076 -6.7115221 -6.0195785 -5.2416067][-3.511323 -3.3260393 -3.2485416 -3.4798675 -3.6958811 -4.1124721 -4.4544554 -5.1586404 -6.1409612 -6.984601 -7.5627728 -7.4495993 -7.110755 -6.3318396 -5.4727154][-2.4243464 -1.7991033 -1.083045 -0.64132953 -0.39213228 -0.7181046 -1.1480925 -2.0119514 -3.3733313 -5.0493121 -6.3530164 -6.9833827 -6.8621721 -5.9662514 -5.0751772][-1.7644229 -0.720526 0.4793787 1.6022921 2.6513834 2.7701197 2.2906404 1.1463141 -0.4737401 -2.698678 -4.53989 -5.7264071 -6.0324178 -5.4211388 -4.424448][-1.473031 -0.074691772 1.5393362 3.0057654 4.3797188 5.0920897 5.1884451 4.0036449 1.9635849 -0.46604705 -2.7688041 -4.2966557 -4.7906513 -4.3498273 -3.7715416][-1.530041 -0.44661784 0.8757782 2.7209377 4.4702435 5.4014769 5.7326012 4.8879824 3.1215138 0.54290247 -1.8501942 -3.3148069 -4.0415854 -3.802918 -3.3718786][-2.964277 -2.063338 -0.89839959 0.60779905 2.04071 3.4737406 4.2370806 3.6549091 2.2512422 0.0041394234 -2.085551 -3.4470727 -3.9826806 -3.8461576 -3.5825279][-4.2873087 -3.7821877 -3.2011559 -1.9838328 -0.50598741 0.83054161 1.2969909 1.1345634 0.26595402 -1.3990066 -3.0554972 -4.2748041 -4.7303529 -4.4664507 -4.1358738][-5.8722005 -5.6799445 -5.1764784 -4.401391 -3.6202998 -2.4314826 -1.6986976 -1.687722 -2.3948092 -3.256629 -4.1568112 -4.9827089 -5.2821269 -5.0075669 -4.6103363][-7.2669115 -7.4522848 -7.5138931 -7.0054684 -6.0934105 -5.2029929 -4.8020973 -4.5912113 -4.6172733 -4.8902221 -5.1894732 -5.4318471 -5.4649439 -5.17232 -4.7178278][-7.46857 -7.8444767 -8.1286449 -8.103672 -7.9489112 -7.2642212 -6.652185 -6.3154588 -6.1850939 -6.0514193 -5.8306222 -5.6146693 -5.3879929 -5.1188889 -4.7553453][-6.66206 -6.9498243 -7.2189875 -7.4228191 -7.5204744 -7.3397436 -7.1842566 -6.77967 -6.3680654 -6.1272864 -5.8537264 -5.57097 -5.2328906 -4.8624868 -4.4665136][-6.0734825 -6.1776657 -6.2780991 -6.4766607 -6.6469941 -6.6842346 -6.6821647 -6.4959307 -6.140883 -5.777648 -5.4426546 -5.1806579 -4.870398 -4.5347285 -4.1630607]]...]
INFO - root - 2017-12-16 06:36:59.007653: step 37310, loss = 0.25, batch loss = 0.20 (29.4 examples/sec; 0.272 sec/batch; 22h:19m:38s remains)
INFO - root - 2017-12-16 06:37:01.857234: step 37320, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.284 sec/batch; 23h:19m:26s remains)
INFO - root - 2017-12-16 06:37:04.716048: step 37330, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.277 sec/batch; 22h:44m:23s remains)
INFO - root - 2017-12-16 06:37:07.539151: step 37340, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.281 sec/batch; 23h:00m:37s remains)
INFO - root - 2017-12-16 06:37:10.432615: step 37350, loss = 0.25, batch loss = 0.19 (27.1 examples/sec; 0.295 sec/batch; 24h:11m:45s remains)
INFO - root - 2017-12-16 06:37:13.294114: step 37360, loss = 0.22, batch loss = 0.16 (27.3 examples/sec; 0.293 sec/batch; 24h:01m:37s remains)
INFO - root - 2017-12-16 06:37:16.119966: step 37370, loss = 0.30, batch loss = 0.24 (29.8 examples/sec; 0.269 sec/batch; 22h:01m:58s remains)
INFO - root - 2017-12-16 06:37:18.963770: step 37380, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.283 sec/batch; 23h:13m:33s remains)
INFO - root - 2017-12-16 06:37:21.823524: step 37390, loss = 0.30, batch loss = 0.24 (29.2 examples/sec; 0.274 sec/batch; 22h:25m:40s remains)
INFO - root - 2017-12-16 06:37:24.653483: step 37400, loss = 0.24, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 23h:12m:19s remains)
2017-12-16 06:37:25.104994: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.38704 -4.7658582 -4.434701 -4.560008 -4.7132182 -4.6593189 -4.5222259 -4.4710279 -4.4982033 -4.5748363 -4.7975569 -5.0819254 -5.3311086 -5.4653707 -5.8064108][-5.2844625 -4.9623222 -4.5922546 -4.362433 -4.4564118 -5.0256715 -4.9778409 -4.7584991 -4.6337752 -4.7867966 -5.0315037 -5.2594161 -5.7154608 -5.6942616 -5.5578294][-5.239397 -4.6615429 -4.2324185 -3.7973647 -3.5508304 -3.8894248 -4.2274981 -4.6941824 -4.886899 -5.027945 -5.3031726 -5.5690179 -5.9246769 -5.9594879 -5.8229041][-4.5326176 -3.918051 -3.4903004 -2.9027452 -2.3965421 -2.3199878 -2.3332391 -2.8313813 -3.5540864 -4.4777193 -5.2330022 -5.7205667 -6.0792127 -6.0484495 -5.9697065][-3.6698058 -2.6795964 -1.7553077 -1.1448288 -0.610579 -0.15769529 0.11051273 -0.17817688 -0.91999912 -2.2748668 -3.6764891 -4.8289952 -5.6180654 -6.00482 -5.9080215][-3.2131467 -2.1780586 -1.0519242 0.62975121 2.0626707 2.5274119 2.5973887 2.5113068 1.6698132 0.10691404 -1.4446962 -3.011657 -4.32454 -4.9649887 -5.3802857][-3.001471 -2.0500381 -1.1845353 0.51980162 2.5669208 4.0859346 4.7822027 4.5117636 3.7971191 2.1667061 0.18865395 -1.4414656 -2.8177607 -3.8456054 -4.6976027][-3.8201613 -2.7574091 -1.6200569 0.16431522 1.9358535 3.5793 4.7977943 5.1309452 4.6012306 2.9740076 1.324718 -0.50771451 -2.095197 -3.1985111 -4.2884092][-4.8618188 -4.2191682 -3.2072549 -1.3326097 0.37192106 1.9773946 3.2780514 3.715766 3.4598322 2.2401476 1.0403728 -0.43379664 -1.9500566 -3.045032 -3.968492][-5.6615391 -5.4484868 -5.0287809 -3.5529461 -1.9577696 -0.82739735 0.31813383 0.91427422 0.86936808 0.046800613 -0.73389053 -1.6140115 -2.5582409 -3.4548573 -4.4645152][-6.9433031 -6.2095337 -5.5800667 -4.942657 -4.3908653 -3.2842231 -2.3384364 -1.9783802 -1.6972432 -2.1490448 -2.6954536 -3.2013168 -3.7387862 -4.1616344 -4.6946793][-7.558012 -6.7935047 -6.2445197 -5.319942 -4.9136477 -4.8909073 -4.4279947 -3.9325163 -3.595818 -3.6631823 -3.6448102 -3.9850612 -4.4023633 -4.4483294 -4.628552][-7.3170338 -6.6135197 -6.1604652 -5.518631 -5.3106384 -5.2257514 -5.2449079 -5.2027731 -4.8014364 -4.5362368 -4.1735678 -4.1772008 -4.4393549 -4.4017153 -4.2434807][-7.4272642 -6.1263657 -5.6223483 -5.1723061 -4.9735923 -4.930439 -5.094202 -5.2477989 -5.1478505 -4.9657779 -4.5949669 -4.420002 -4.3774333 -4.15793 -3.8571351][-7.1106472 -5.9054866 -5.1178317 -4.8100576 -4.6634259 -4.5105968 -4.6147079 -4.761466 -4.6622233 -4.69373 -4.6242423 -4.3930163 -4.1713095 -3.9415548 -3.611877]]...]
INFO - root - 2017-12-16 06:37:27.931302: step 37410, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 22h:44m:57s remains)
INFO - root - 2017-12-16 06:37:30.777402: step 37420, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.281 sec/batch; 23h:03m:56s remains)
INFO - root - 2017-12-16 06:37:33.554566: step 37430, loss = 0.21, batch loss = 0.15 (28.9 examples/sec; 0.277 sec/batch; 22h:40m:58s remains)
INFO - root - 2017-12-16 06:37:36.384413: step 37440, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 23h:22m:19s remains)
INFO - root - 2017-12-16 06:37:39.214978: step 37450, loss = 0.27, batch loss = 0.21 (28.8 examples/sec; 0.278 sec/batch; 22h:46m:15s remains)
INFO - root - 2017-12-16 06:37:42.066856: step 37460, loss = 0.32, batch loss = 0.26 (27.4 examples/sec; 0.292 sec/batch; 23h:57m:00s remains)
INFO - root - 2017-12-16 06:37:44.889632: step 37470, loss = 0.35, batch loss = 0.29 (28.6 examples/sec; 0.280 sec/batch; 22h:55m:59s remains)
INFO - root - 2017-12-16 06:37:47.761905: step 37480, loss = 0.23, batch loss = 0.17 (29.3 examples/sec; 0.273 sec/batch; 22h:22m:04s remains)
INFO - root - 2017-12-16 06:37:50.627165: step 37490, loss = 0.21, batch loss = 0.15 (27.8 examples/sec; 0.288 sec/batch; 23h:35m:59s remains)
INFO - root - 2017-12-16 06:37:53.473409: step 37500, loss = 0.29, batch loss = 0.23 (29.0 examples/sec; 0.276 sec/batch; 22h:37m:22s remains)
2017-12-16 06:37:53.949343: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8029075 -2.8434563 -3.1436274 -3.1650696 -3.0642056 -2.8901253 -2.6438007 -2.4005601 -2.3040874 -2.2324786 -2.1601214 -2.0976353 -2.0141907 -1.9855814 -1.8918724][-2.5445266 -2.735023 -3.11864 -3.3087997 -3.2806325 -3.0707519 -2.792532 -2.5547223 -2.4685659 -2.4626036 -2.4052327 -2.3378034 -2.1896472 -2.0158544 -1.8996811][-2.2738538 -2.628448 -3.192678 -3.3049481 -3.1587145 -2.9858217 -2.7938232 -2.6323071 -2.5488944 -2.6830411 -2.8421192 -2.9051867 -2.7987504 -2.5351014 -2.3173146][-2.2238 -2.4742684 -2.8202047 -2.8416462 -2.6439407 -2.3394163 -2.0504484 -2.1021805 -2.3400931 -2.6854968 -3.0169463 -3.3314016 -3.426435 -3.222502 -2.9278417][-1.8488967 -1.9346502 -1.9458027 -1.9134562 -1.5174294 -0.91915894 -0.55714607 -0.75109339 -1.2593782 -2.0718098 -2.8401766 -3.4239075 -3.7329612 -3.5488942 -3.2480855][-1.375664 -1.2209973 -0.93072438 -0.49305677 0.27371979 0.99163771 1.4000916 1.1657057 0.44596386 -0.67783642 -1.901804 -2.92877 -3.546741 -3.5455141 -3.2793593][-1.1289852 -0.66914034 -0.16175222 0.63137865 1.6629882 2.6633658 3.30234 3.0410128 2.1443381 0.70807505 -0.87217903 -2.1936364 -3.0525224 -3.1501298 -3.0023215][-1.4286928 -0.92499876 -0.28040123 0.79987812 2.1688395 3.3717732 4.0712891 3.7587147 2.7444491 1.2037754 -0.46701527 -1.8675461 -2.8392365 -3.0193148 -2.8453896][-1.9899917 -1.7592719 -1.1614895 -0.21073008 1.1296697 2.5092068 3.3622184 3.1459246 2.1500669 0.63914156 -0.98137069 -2.3100054 -3.0713494 -3.0235653 -2.7074394][-2.6224675 -2.8122418 -2.6099787 -1.7472095 -0.56894636 0.51561117 1.1604891 1.0660214 0.32774782 -0.8616724 -2.13574 -3.1655438 -3.6668181 -3.4298725 -2.9798384][-3.4496634 -3.6594536 -3.6042163 -3.1066465 -2.4639452 -1.6893194 -1.2775238 -1.4074697 -1.9847982 -2.908036 -3.7187216 -4.2838311 -4.3109636 -3.796257 -3.1879959][-3.6433866 -3.9467542 -4.2140684 -4.1957836 -3.9488878 -3.4660532 -3.2517481 -3.450942 -3.9758875 -4.5503187 -4.9446039 -5.1188745 -4.7447567 -3.9505103 -3.2532587][-3.4820635 -3.8387144 -4.1956825 -4.2561893 -4.4032612 -4.5572453 -4.7400846 -4.684989 -4.7267375 -4.9742794 -5.2014484 -5.035121 -4.3073 -3.4559679 -2.9099][-2.7545059 -2.9472809 -3.3603272 -3.7285757 -4.1563106 -4.339623 -4.581212 -5.0127335 -5.3124428 -4.957531 -4.2533875 -3.7316337 -3.1934137 -2.5223241 -2.0907357][-2.1421437 -2.1156163 -2.490675 -2.9072704 -3.4387889 -3.8088441 -4.1953273 -4.3266406 -4.2287469 -4.0864377 -3.7028499 -2.8255463 -1.757215 -1.1294167 -1.0152521]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-37500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-37500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 06:37:57.311708: step 37510, loss = 0.27, batch loss = 0.21 (28.6 examples/sec; 0.280 sec/batch; 22h:56m:03s remains)
INFO - root - 2017-12-16 06:38:00.086904: step 37520, loss = 0.27, batch loss = 0.22 (29.3 examples/sec; 0.273 sec/batch; 22h:20m:18s remains)
INFO - root - 2017-12-16 06:38:02.911165: step 37530, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 23h:12m:03s remains)
INFO - root - 2017-12-16 06:38:05.755797: step 37540, loss = 0.25, batch loss = 0.19 (29.3 examples/sec; 0.273 sec/batch; 22h:23m:20s remains)
INFO - root - 2017-12-16 06:38:08.606749: step 37550, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 22h:45m:09s remains)
INFO - root - 2017-12-16 06:38:11.418396: step 37560, loss = 0.25, batch loss = 0.19 (28.9 examples/sec; 0.277 sec/batch; 22h:40m:37s remains)
INFO - root - 2017-12-16 06:38:14.297945: step 37570, loss = 0.27, batch loss = 0.22 (28.2 examples/sec; 0.283 sec/batch; 23h:12m:27s remains)
INFO - root - 2017-12-16 06:38:17.145638: step 37580, loss = 0.23, batch loss = 0.17 (29.5 examples/sec; 0.271 sec/batch; 22h:14m:03s remains)
INFO - root - 2017-12-16 06:38:19.984805: step 37590, loss = 0.26, batch loss = 0.20 (26.1 examples/sec; 0.307 sec/batch; 25h:06m:52s remains)
INFO - root - 2017-12-16 06:38:22.866083: step 37600, loss = 0.20, batch loss = 0.14 (27.7 examples/sec; 0.289 sec/batch; 23h:41m:26s remains)
2017-12-16 06:38:23.332630: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3341231 -3.3389058 -3.1807065 -2.896801 -2.6456647 -2.3362396 -2.0975325 -2.2294569 -2.576607 -2.9502022 -3.173872 -3.5221796 -3.6691275 -3.6763368 -3.7612689][-3.30014 -3.37462 -3.3440773 -3.0817444 -2.806344 -2.4840651 -2.0921533 -2.0481639 -2.2489002 -2.7041278 -3.0759788 -3.5105393 -3.7428474 -3.9492931 -4.1650329][-3.2736676 -3.3006048 -3.283855 -3.0213883 -2.659914 -2.2877758 -1.9298282 -1.8887517 -2.0656908 -2.3519516 -2.5523458 -3.1166844 -3.7474496 -4.0854659 -4.328145][-2.6970043 -2.8737998 -3.1107159 -3.0461531 -2.7697735 -2.39631 -2.04564 -1.8024783 -1.7199574 -1.9053266 -2.1846526 -2.7099633 -3.2066762 -3.8128204 -4.375432][-1.916224 -2.1554406 -2.4879117 -2.4816728 -2.23412 -1.9545197 -1.7072139 -1.3871737 -1.116693 -1.1589758 -1.2944832 -1.7045813 -2.1999276 -2.8164141 -3.3663871][-0.96966672 -1.4366264 -1.8221943 -1.84776 -1.6657243 -1.4280105 -1.1335254 -0.76316476 -0.50213456 -0.47460938 -0.56629252 -0.87230849 -1.3016 -1.8006265 -2.2438567][-0.31822205 -0.70049119 -1.0954518 -1.1136732 -0.92682385 -0.78080344 -0.60781622 -0.34731245 -0.10844946 -0.087174416 -0.16936636 -0.18850231 -0.35928774 -0.80324888 -1.1878674][0.12000275 -0.24873066 -0.5647893 -0.54404306 -0.43732858 -0.39119387 -0.27308655 -0.071572304 0.010041714 -0.00056934357 0.0295825 0.13767624 0.047424793 -0.27383232 -0.51513076][0.00034046173 -0.16374111 -0.21291637 -0.095394611 -0.02200985 0.035883427 0.12675858 0.28699398 0.36893415 0.3398838 0.37983036 0.45082092 0.32942486 0.099484921 -0.12118101][-0.622283 -0.46253991 -0.30716419 -0.019322395 0.22654009 0.28395605 0.39054394 0.52234316 0.47913361 0.37135172 0.26085615 0.32777262 0.1244421 -0.22938156 -0.44307446][-1.6830359 -1.3229749 -0.9115231 -0.42616177 -0.058605671 0.18108892 0.37018824 0.4416647 0.16744995 -0.10062027 -0.31604004 -0.32333183 -0.42719078 -0.83509445 -1.1206095][-2.8339767 -2.2366993 -1.5671985 -0.96074963 -0.54906225 -0.21398163 0.024802685 0.079416752 -0.21214485 -0.60506845 -0.95008707 -1.0928068 -1.319634 -1.6611891 -1.9326034][-3.6703107 -3.0020723 -2.1738141 -1.3770528 -0.74393272 -0.35900831 -0.11275244 -0.19391012 -0.70202565 -1.1374152 -1.4140728 -1.6941683 -2.1053221 -2.3757529 -2.5157416][-4.5463161 -3.6335361 -2.5941629 -1.6543534 -1.0982702 -0.77982926 -0.60290146 -0.70120645 -1.2269278 -1.9398754 -2.4336867 -2.5310373 -2.5865805 -2.7558842 -2.7995529][-4.8132691 -4.0104103 -2.9942083 -1.9278858 -1.2918265 -1.0842204 -1.0804241 -1.3800249 -2.0661142 -2.755656 -3.0775123 -2.9896483 -2.8098774 -2.7914271 -2.709938]]...]
INFO - root - 2017-12-16 06:38:26.230099: step 37610, loss = 0.24, batch loss = 0.18 (27.4 examples/sec; 0.292 sec/batch; 23h:54m:52s remains)
INFO - root - 2017-12-16 06:38:29.059710: step 37620, loss = 0.30, batch loss = 0.24 (28.4 examples/sec; 0.282 sec/batch; 23h:06m:18s remains)
INFO - root - 2017-12-16 06:38:31.881430: step 37630, loss = 0.28, batch loss = 0.22 (28.5 examples/sec; 0.280 sec/batch; 22h:57m:23s remains)
INFO - root - 2017-12-16 06:38:34.700107: step 37640, loss = 0.23, batch loss = 0.17 (28.1 examples/sec; 0.285 sec/batch; 23h:19m:22s remains)
INFO - root - 2017-12-16 06:38:37.538672: step 37650, loss = 0.39, batch loss = 0.33 (26.4 examples/sec; 0.303 sec/batch; 24h:48m:35s remains)
INFO - root - 2017-12-16 06:38:40.356795: step 37660, loss = 0.25, batch loss = 0.20 (29.4 examples/sec; 0.272 sec/batch; 22h:15m:28s remains)
INFO - root - 2017-12-16 06:38:43.157828: step 37670, loss = 0.18, batch loss = 0.12 (29.0 examples/sec; 0.275 sec/batch; 22h:33m:31s remains)
INFO - root - 2017-12-16 06:38:45.986075: step 37680, loss = 0.22, batch loss = 0.16 (28.8 examples/sec; 0.278 sec/batch; 22h:44m:48s remains)
INFO - root - 2017-12-16 06:38:48.900456: step 37690, loss = 0.33, batch loss = 0.27 (27.4 examples/sec; 0.292 sec/batch; 23h:54m:26s remains)
INFO - root - 2017-12-16 06:38:51.732964: step 37700, loss = 0.38, batch loss = 0.32 (27.6 examples/sec; 0.290 sec/batch; 23h:45m:35s remains)
2017-12-16 06:38:52.206145: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3477297 -4.1721215 -4.0657673 -4.503262 -5.2257648 -6.0286784 -6.7111473 -6.8142562 -6.7815075 -6.2950621 -5.83434 -5.5617843 -4.9466529 -4.1918659 -3.4925508][-4.9606013 -5.4048653 -5.8472204 -6.0933638 -6.9559951 -7.186625 -7.1982832 -7.0493574 -6.5547676 -6.1554961 -5.980907 -5.5556517 -4.8622775 -4.2602816 -3.6791017][-5.54932 -6.0985451 -6.6132755 -6.8597207 -6.8770323 -6.5354948 -6.4969854 -6.1054153 -5.693614 -5.3718057 -5.1554637 -5.2103176 -5.1770692 -4.7305007 -4.117178][-5.97412 -6.895277 -7.240315 -6.9516191 -6.360158 -5.2861452 -4.3814459 -3.7049191 -3.4144154 -3.9053371 -4.6741729 -4.9467535 -5.0325584 -5.0906005 -4.780344][-5.8857694 -6.8613091 -6.9723778 -6.1615953 -4.6165166 -2.7798514 -1.4691601 -0.529593 -0.6121428 -1.7658777 -3.0347812 -4.4182706 -5.3538775 -5.3696623 -4.8221803][-4.5053611 -5.3045797 -5.445981 -4.3468046 -1.934891 0.6602087 2.5881805 3.4653487 2.7231708 0.82702351 -1.703249 -3.8539314 -5.0953364 -5.584806 -5.2244072][-3.2094259 -3.8994522 -3.5972466 -2.0336571 0.32018518 3.0070076 5.1905251 5.9836874 4.9477482 2.4165621 -0.57863641 -3.2738652 -5.0469341 -5.4911489 -4.9487481][-2.5860252 -3.3323545 -3.1829138 -1.4561336 1.2712493 4.1212091 6.0987988 6.3103428 5.059432 2.4232602 -0.86553526 -3.557225 -5.0181942 -5.3307648 -4.8297219][-2.550462 -3.0727367 -2.601048 -1.432878 0.25948524 2.7458768 4.7509527 5.0479984 3.6627254 0.86709309 -2.1607649 -4.4939494 -5.7639732 -5.7840862 -5.0527763][-2.5934858 -3.2873678 -2.9920111 -1.9669724 -0.39491463 1.1057563 1.9522595 2.200408 1.0889363 -1.3490591 -3.9360743 -5.9090834 -6.6137581 -6.3593273 -5.5494704][-3.3739629 -3.7057006 -3.4906616 -3.2709689 -2.7939496 -1.5179265 -0.29944849 -0.41240263 -1.819638 -3.4964986 -5.2565074 -6.77534 -7.2789464 -6.7296352 -5.7626743][-4.4446855 -4.6653557 -4.7354116 -4.2911887 -3.6704197 -3.3837709 -3.1121728 -3.1077151 -3.7317209 -5.033247 -6.433198 -7.1864276 -7.1946793 -6.5906277 -5.7262096][-5.4013348 -5.4954586 -5.5569367 -5.4619312 -5.3033495 -4.6821251 -4.2091322 -4.6278849 -5.3451037 -6.0141873 -6.8262897 -7.2811384 -7.134861 -6.4461985 -5.6481686][-6.1666493 -6.1152625 -6.058919 -5.965189 -5.7899694 -5.3686924 -5.21753 -5.1398616 -5.3939185 -6.03811 -6.5341921 -6.5724611 -6.3296714 -5.74479 -5.1405516][-6.8293123 -6.7176771 -6.553834 -6.2983284 -5.9736209 -5.5937395 -5.3551335 -5.0864391 -5.1349616 -5.4431925 -5.70699 -5.63076 -5.283608 -4.7233291 -4.225246]]...]
INFO - root - 2017-12-16 06:38:55.086374: step 37710, loss = 0.22, batch loss = 0.16 (25.5 examples/sec; 0.313 sec/batch; 25h:39m:01s remains)
INFO - root - 2017-12-16 06:38:57.965792: step 37720, loss = 0.29, batch loss = 0.24 (27.3 examples/sec; 0.293 sec/batch; 24h:00m:00s remains)
INFO - root - 2017-12-16 06:39:00.749188: step 37730, loss = 0.25, batch loss = 0.19 (28.3 examples/sec; 0.282 sec/batch; 23h:07m:22s remains)
INFO - root - 2017-12-16 06:39:03.581870: step 37740, loss = 0.32, batch loss = 0.26 (25.6 examples/sec; 0.313 sec/batch; 25h:36m:24s remains)
INFO - root - 2017-12-16 06:39:06.399194: step 37750, loss = 0.44, batch loss = 0.38 (29.0 examples/sec; 0.275 sec/batch; 22h:33m:22s remains)
INFO - root - 2017-12-16 06:39:09.293685: step 37760, loss = 0.28, batch loss = 0.22 (28.3 examples/sec; 0.283 sec/batch; 23h:08m:43s remains)
INFO - root - 2017-12-16 06:39:12.150144: step 37770, loss = 0.34, batch loss = 0.28 (28.7 examples/sec; 0.279 sec/batch; 22h:50m:32s remains)
INFO - root - 2017-12-16 06:39:14.989088: step 37780, loss = 0.26, batch loss = 0.20 (29.2 examples/sec; 0.274 sec/batch; 22h:26m:46s remains)
INFO - root - 2017-12-16 06:39:17.823231: step 37790, loss = 0.22, batch loss = 0.16 (27.9 examples/sec; 0.286 sec/batch; 23h:27m:10s remains)
INFO - root - 2017-12-16 06:39:20.615707: step 37800, loss = 0.34, batch loss = 0.28 (29.3 examples/sec; 0.273 sec/batch; 22h:20m:45s remains)
2017-12-16 06:39:21.093549: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0152559 -3.30696 -3.648453 -3.949059 -4.1722674 -4.1938577 -3.8385544 -3.4517732 -3.2409019 -2.9564083 -2.6956625 -2.8868828 -3.1638968 -3.1104691 -2.9500551][-2.27599 -2.5513844 -2.9328847 -3.3132844 -3.3973894 -3.3434412 -3.2006388 -3.061326 -2.846077 -2.563458 -2.483686 -2.4922 -2.5448847 -2.5442343 -2.3735135][-1.5777502 -1.909107 -2.2931695 -2.6553528 -2.9007854 -2.9085479 -2.5893633 -2.428412 -2.3783669 -2.3038201 -2.1683865 -2.0249884 -2.0448024 -2.0242352 -1.9005132][-0.98660278 -1.2978809 -1.5710945 -1.8011708 -1.8193064 -1.8144221 -1.6986539 -1.5787864 -1.4482009 -1.4881744 -1.5342758 -1.446928 -1.4913073 -1.4902577 -1.5089569][-0.19159555 -0.55026722 -0.79643917 -0.85417366 -0.63876534 -0.45125198 -0.44924879 -0.44854355 -0.54307628 -0.72346425 -0.89367366 -0.9308362 -0.93248177 -1.0333614 -1.22248][-0.40309143 -0.57907486 -0.56347919 -0.57303238 -0.30474329 0.22109461 0.54374218 0.50917387 0.290318 0.055468559 -0.25299644 -0.3798275 -0.47503686 -0.61657238 -0.87890124][-0.96374941 -1.205611 -1.1945872 -0.93280292 -0.39999771 -0.00470829 0.3647728 0.60064363 0.50480556 0.28594923 -0.0290761 -0.19373322 -0.27530432 -0.40456724 -0.68703485][-1.8009665 -2.2004859 -2.2755363 -2.0507369 -1.3871412 -0.70137334 -0.29706955 0.025725365 0.36089563 0.32068777 0.010954857 -0.04577589 -0.063788414 -0.05159235 -0.26148844][-2.4594045 -2.9466443 -3.1948295 -3.0671883 -2.6026535 -1.8982854 -1.1239593 -0.678221 -0.67504239 -0.52875853 -0.39078665 -0.40563107 -0.38930798 -0.18789673 -0.16531038][-3.4438767 -4.0369086 -4.2345319 -4.1233625 -3.7064729 -3.1223464 -2.5600014 -1.827493 -1.2256985 -1.1002953 -1.0731194 -1.0131974 -1.0584381 -0.84665656 -0.5533092][-3.8369353 -4.3832731 -4.7896137 -4.8376627 -4.5427256 -4.0410419 -3.3917751 -2.7933879 -2.4813795 -2.2600002 -1.9433186 -1.7239902 -1.4528694 -1.1618457 -1.0083339][-4.1807122 -4.4867835 -4.7085609 -4.9150491 -4.9897909 -4.5925884 -4.0706511 -3.7651436 -3.391202 -3.1487937 -3.0077357 -2.6249652 -2.1642771 -1.5689738 -1.0738497][-4.7341228 -4.6521082 -4.5375476 -4.5411243 -4.4743237 -4.2815142 -3.9992485 -3.8229432 -3.710227 -3.7447448 -3.6499829 -3.2654471 -2.4617522 -1.632931 -1.0741618][-4.8142767 -4.40079 -4.0346365 -3.7833819 -3.8475077 -3.6688385 -3.5350666 -3.5986667 -3.7109995 -3.8551264 -3.777164 -3.4408393 -2.5854845 -1.5170114 -0.90928316][-4.6887817 -4.1445518 -3.536561 -3.200614 -3.0505731 -2.9770007 -3.0357189 -3.2799764 -3.6129761 -3.9594007 -3.8377442 -3.214972 -2.0958645 -0.90549827 -0.28745031]]...]
INFO - root - 2017-12-16 06:39:23.909888: step 37810, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.281 sec/batch; 22h:59m:40s remains)
INFO - root - 2017-12-16 06:39:26.686149: step 37820, loss = 0.26, batch loss = 0.20 (28.2 examples/sec; 0.283 sec/batch; 23h:11m:42s remains)
INFO - root - 2017-12-16 06:39:29.492780: step 37830, loss = 0.29, batch loss = 0.23 (27.2 examples/sec; 0.294 sec/batch; 24h:03m:58s remains)
INFO - root - 2017-12-16 06:39:32.356804: step 37840, loss = 0.23, batch loss = 0.17 (26.7 examples/sec; 0.300 sec/batch; 24h:32m:10s remains)
INFO - root - 2017-12-16 06:39:35.192574: step 37850, loss = 0.35, batch loss = 0.29 (28.4 examples/sec; 0.282 sec/batch; 23h:03m:14s remains)
INFO - root - 2017-12-16 06:39:37.989362: step 37860, loss = 0.23, batch loss = 0.17 (29.3 examples/sec; 0.273 sec/batch; 22h:19m:35s remains)
INFO - root - 2017-12-16 06:39:40.792043: step 37870, loss = 0.40, batch loss = 0.34 (29.0 examples/sec; 0.276 sec/batch; 22h:34m:20s remains)
INFO - root - 2017-12-16 06:39:43.658128: step 37880, loss = 0.43, batch loss = 0.37 (29.6 examples/sec; 0.271 sec/batch; 22h:08m:45s remains)
INFO - root - 2017-12-16 06:39:46.476152: step 37890, loss = 0.36, batch loss = 0.30 (26.3 examples/sec; 0.304 sec/batch; 24h:53m:35s remains)
INFO - root - 2017-12-16 06:39:49.298624: step 37900, loss = 0.39, batch loss = 0.33 (28.3 examples/sec; 0.283 sec/batch; 23h:09m:00s remains)
2017-12-16 06:39:49.761895: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6177697 -5.7392888 -5.66652 -5.5948715 -5.5636392 -5.5511718 -5.584796 -5.7468681 -5.8842659 -5.9268661 -5.9637761 -5.8387222 -5.5507522 -5.1899972 -4.8733878][-5.7201719 -5.6336527 -5.5718775 -5.6355562 -5.6343555 -5.8522663 -6.0910006 -6.0899539 -6.1085033 -6.3733559 -6.4609566 -6.2294569 -5.9801474 -5.6930876 -5.4436951][-5.0270777 -5.0441332 -5.0548019 -5.0226059 -5.1009178 -5.3378205 -5.3785434 -5.5713253 -5.8350887 -5.9483409 -6.0992851 -6.3078365 -6.3088446 -5.9663815 -5.7234583][-4.2063646 -3.7971618 -3.5723248 -3.6704776 -3.7614684 -3.8933642 -3.8656096 -3.954308 -4.1227536 -4.58243 -5.0157762 -5.3764648 -5.6244116 -5.6006327 -5.5431032][-2.6845102 -1.928057 -1.514447 -1.6473598 -1.8152497 -1.7904098 -1.5310023 -1.5765626 -1.7893202 -2.39537 -3.1380918 -3.848109 -4.4074626 -4.5684261 -4.4945865][-1.5978951 -0.70944357 -0.15146589 0.025001526 0.18284416 0.38003397 0.93020678 1.2128878 1.0223427 0.28496742 -0.68882918 -1.8861372 -2.8285103 -3.2435446 -3.3319907][-0.82225943 0.04158926 0.50941372 0.67196465 1.0895381 1.8955383 3.0062609 3.4673576 3.22504 2.4697289 1.4851809 0.21457624 -0.98404789 -1.802942 -2.2276623][-0.35287619 0.34899855 0.80540323 1.1261201 1.524663 2.3155656 3.435967 4.2095242 4.2044554 3.4376769 2.3846922 1.0770655 -0.036317348 -0.71976519 -1.3260839][-0.39739847 0.0011925697 0.22979164 0.4835372 0.96588564 1.8117433 2.8938046 3.48696 3.4674568 2.9817138 2.1930857 1.0511346 0.013151169 -0.6159656 -1.079066][-0.66368055 -0.50583124 -0.58100748 -0.41600466 -0.04147625 0.58398533 1.3911433 1.952549 1.986289 1.5529957 0.97517157 0.18161488 -0.54267883 -0.94120336 -1.3366022][-1.3494935 -1.1768973 -1.085917 -1.049777 -0.85033917 -0.49966264 -0.031469345 0.14440632 0.12188435 -0.193964 -0.61199784 -1.1711094 -1.6253347 -1.7288597 -1.9399433][-1.6590676 -1.4185576 -1.4043899 -1.4212868 -1.2856596 -1.1452081 -0.99659586 -1.0853367 -1.262296 -1.6825478 -2.0350983 -2.2431457 -2.427388 -2.2229681 -2.185276][-1.7693305 -1.4543602 -1.3552229 -1.4044561 -1.4373755 -1.393532 -1.3882091 -1.7653584 -2.1334124 -2.5385721 -2.7948422 -2.8353453 -2.854327 -2.51257 -2.2700422][-1.8582463 -1.4269817 -1.1767235 -1.097472 -1.0558655 -1.1226907 -1.1373942 -1.6357985 -2.3053689 -3.0164027 -3.258914 -3.1749961 -2.9438148 -2.4851127 -2.1710162][-1.8039134 -1.3435459 -1.067569 -0.9946847 -0.88413215 -0.80160666 -0.79543281 -1.4264448 -2.1343164 -2.9032276 -3.4099145 -3.419416 -3.2324538 -2.6200085 -2.2272537]]...]
INFO - root - 2017-12-16 06:39:52.576523: step 37910, loss = 0.28, batch loss = 0.22 (26.7 examples/sec; 0.300 sec/batch; 24h:32m:52s remains)
INFO - root - 2017-12-16 06:39:55.419662: step 37920, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.279 sec/batch; 22h:51m:06s remains)
INFO - root - 2017-12-16 06:39:58.242869: step 37930, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 23h:03m:11s remains)
INFO - root - 2017-12-16 06:40:01.065190: step 37940, loss = 0.27, batch loss = 0.21 (27.4 examples/sec; 0.292 sec/batch; 23h:53m:21s remains)
INFO - root - 2017-12-16 06:40:03.877660: step 37950, loss = 0.29, batch loss = 0.23 (29.4 examples/sec; 0.272 sec/batch; 22h:16m:11s remains)
INFO - root - 2017-12-16 06:40:06.741276: step 37960, loss = 0.25, batch loss = 0.19 (25.5 examples/sec; 0.314 sec/batch; 25h:42m:18s remains)
INFO - root - 2017-12-16 06:40:09.589592: step 37970, loss = 0.40, batch loss = 0.34 (28.1 examples/sec; 0.284 sec/batch; 23h:16m:28s remains)
INFO - root - 2017-12-16 06:40:12.385886: step 37980, loss = 0.22, batch loss = 0.16 (28.3 examples/sec; 0.282 sec/batch; 23h:06m:26s remains)
INFO - root - 2017-12-16 06:40:15.263985: step 37990, loss = 0.26, batch loss = 0.20 (29.0 examples/sec; 0.276 sec/batch; 22h:34m:17s remains)
INFO - root - 2017-12-16 06:40:18.114969: step 38000, loss = 0.26, batch loss = 0.20 (27.6 examples/sec; 0.290 sec/batch; 23h:42m:26s remains)
2017-12-16 06:40:18.570207: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.026948 -2.9350967 -2.9221697 -2.6796207 -2.4210181 -2.3312259 -2.3692145 -2.3038011 -2.3462951 -2.2685983 -2.1430686 -2.190881 -2.2215638 -2.301115 -2.4069881][-3.9402258 -3.7332942 -3.4755814 -3.1474278 -2.8878613 -2.542861 -2.3197248 -2.3480756 -2.6055422 -2.6243658 -2.6296091 -2.73556 -2.7807894 -2.9632626 -3.1191823][-4.7796984 -4.4958267 -4.10323 -3.4536886 -2.8366532 -2.4467683 -2.2916672 -2.2636507 -2.4328995 -2.616375 -2.9116654 -3.1826453 -3.3358998 -3.436413 -3.5477316][-5.4763203 -5.0911145 -4.441927 -3.5655222 -2.783886 -2.1735427 -1.8254211 -1.8324966 -2.0142419 -2.3299944 -2.7629497 -3.1502056 -3.441031 -3.6953404 -3.7183621][-5.9910841 -5.6156077 -4.917953 -3.8758533 -2.8252308 -2.0165865 -1.4198146 -1.2079792 -1.3184888 -1.5306678 -1.9832907 -2.7107103 -3.1714315 -3.4035845 -3.3781309][-5.7833862 -5.4883232 -4.8098741 -3.6609743 -2.429903 -1.4112194 -0.61791468 -0.08245945 0.14738274 -0.12206125 -0.90784836 -1.6259623 -2.0781448 -2.436564 -2.4953203][-4.8355489 -4.6778522 -4.0863118 -2.9617057 -1.6770425 -0.73820543 0.22801828 0.96236229 1.3102555 1.2152529 0.60941648 -0.33868361 -1.0910919 -1.2728562 -1.0058167][-4.1913476 -4.1818109 -3.6650429 -2.7547805 -1.5809131 -0.457582 0.77706766 1.5772166 1.9016581 1.7974253 1.1101098 0.29515696 -0.026847839 0.066168785 0.3977623][-3.9992549 -4.1585088 -3.7582958 -2.9185236 -1.8614254 -0.88234162 0.18968153 1.0311823 1.5004959 1.4060187 0.82711792 0.23397732 0.1733222 0.74644947 1.5156364][-4.1987166 -4.3980002 -3.9303231 -3.1580582 -2.2991316 -1.4786377 -0.70045567 -0.19843054 0.0093932152 -0.11448717 -0.4147296 -0.6356647 -0.33051586 0.32632017 1.1775494][-4.5730491 -4.6854167 -4.2953267 -3.7160742 -3.1636522 -2.4674976 -1.8947105 -1.6785223 -1.7275538 -1.9305935 -2.0988128 -2.0191269 -1.4869485 -0.67483115 0.1620965][-4.7970376 -4.8099165 -4.3219581 -3.9783051 -3.7902086 -3.5080163 -3.3171499 -3.2601781 -3.3375752 -3.6804533 -3.8273237 -3.6023824 -2.9754477 -2.0656641 -1.1837072][-4.9070835 -4.7621608 -4.2068062 -3.7644553 -3.6385179 -3.6438482 -3.8570645 -4.068119 -4.5138412 -4.9940128 -5.113081 -4.902873 -4.3938937 -3.6676202 -2.7568598][-4.2618752 -4.1315951 -3.674808 -3.3211675 -3.3063998 -3.2918327 -3.6192088 -4.1513038 -5.06789 -5.9052396 -6.3742461 -6.4305468 -6.0189 -5.348526 -4.496593][-3.2939067 -3.3194141 -3.060986 -2.8462129 -2.901336 -3.0233023 -3.553514 -4.2615409 -5.280302 -6.4449234 -7.3851013 -7.4488258 -6.8839569 -6.2128048 -5.2650242]]...]
INFO - root - 2017-12-16 06:40:21.380636: step 38010, loss = 0.22, batch loss = 0.16 (29.7 examples/sec; 0.269 sec/batch; 22h:00m:28s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 06:40:24.222686: step 38020, loss = 0.30, batch loss = 0.24 (27.9 examples/sec; 0.287 sec/batch; 23h:27m:06s remains)
INFO - root - 2017-12-16 06:40:27.043459: step 38030, loss = 0.20, batch loss = 0.14 (28.9 examples/sec; 0.277 sec/batch; 22h:38m:33s remains)
INFO - root - 2017-12-16 06:40:29.871415: step 38040, loss = 0.22, batch loss = 0.16 (27.0 examples/sec; 0.297 sec/batch; 24h:15m:53s remains)
INFO - root - 2017-12-16 06:40:32.747455: step 38050, loss = 0.30, batch loss = 0.24 (27.4 examples/sec; 0.292 sec/batch; 23h:54m:37s remains)
INFO - root - 2017-12-16 06:40:35.573822: step 38060, loss = 0.33, batch loss = 0.27 (29.4 examples/sec; 0.273 sec/batch; 22h:17m:19s remains)
INFO - root - 2017-12-16 06:40:38.363678: step 38070, loss = 0.35, batch loss = 0.29 (29.4 examples/sec; 0.273 sec/batch; 22h:17m:31s remains)
INFO - root - 2017-12-16 06:40:41.168111: step 38080, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.277 sec/batch; 22h:40m:49s remains)
INFO - root - 2017-12-16 06:40:43.992794: step 38090, loss = 0.30, batch loss = 0.24 (26.2 examples/sec; 0.305 sec/batch; 24h:56m:30s remains)
INFO - root - 2017-12-16 06:40:46.848663: step 38100, loss = 0.25, batch loss = 0.19 (27.0 examples/sec; 0.296 sec/batch; 24h:13m:13s remains)
2017-12-16 06:40:47.315624: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9872823 -2.5109615 -2.0367219 -1.7314539 -1.6754549 -1.8093293 -2.092422 -2.1190014 -1.9567041 -1.886492 -1.7358627 -1.4954491 -1.1294 -0.78969979 -0.44792843][-2.9182563 -2.7539067 -2.5651903 -2.4182475 -2.3998377 -2.4301887 -2.4244895 -2.4443722 -2.5253921 -2.3283269 -1.9805193 -1.6037478 -1.1711042 -0.74493074 -0.36988163][-2.979012 -3.1369629 -3.3186235 -3.4910846 -3.5455427 -3.559212 -3.5655992 -3.2934253 -2.959645 -2.7753468 -2.6669686 -2.2575 -1.7574008 -1.2483342 -0.94739962][-3.1384182 -3.5708752 -4.080584 -4.5368781 -4.7039294 -4.4563775 -3.9896593 -3.4694328 -3.033114 -2.7006545 -2.4721885 -2.3000853 -1.8820112 -1.4430988 -1.2414367][-3.2480884 -3.9782681 -4.6323886 -5.0771937 -5.0013418 -4.3608813 -3.3357794 -2.369473 -1.8482096 -1.5548086 -1.5567234 -1.6808944 -1.6538455 -1.5629296 -1.5326862][-3.6780541 -4.4501729 -5.0838308 -5.3198071 -4.8311219 -3.7777882 -2.4626286 -1.2023773 -0.41848469 -0.24260283 -0.5758996 -0.97433066 -1.2687838 -1.5317211 -1.9003348][-3.8538928 -4.4754748 -4.8681607 -4.7427053 -3.9871631 -2.5984159 -0.92510104 0.36351109 1.0004144 0.97156334 0.41688108 -0.25326109 -0.99634695 -1.6259756 -2.0994875][-3.8161342 -4.1983047 -4.3776207 -3.9475203 -2.7448854 -1.0090077 0.69282913 1.9778996 2.5845113 2.3590627 1.6107955 0.62246418 -0.19515991 -1.0269103 -1.7994456][-3.7723489 -4.0822444 -3.8791816 -3.2885008 -2.2620378 -0.6091013 0.99516582 2.1766 2.6393886 2.4038186 1.5556922 0.64671373 -0.082448959 -0.82842636 -1.5062175][-3.4374075 -3.8682981 -3.9364066 -3.3466961 -2.21447 -0.93231893 0.40837717 1.6473603 2.0805984 1.8037143 1.148438 0.4252429 -0.45853257 -1.2243092 -1.7269616][-3.6261432 -3.7803669 -3.8544509 -3.7598257 -3.0217595 -1.8228018 -0.66813993 0.1890955 0.72879267 0.86944151 0.33338022 -0.45021749 -1.0503528 -1.5251462 -2.0577869][-3.7821238 -4.0044012 -4.2066131 -4.2404456 -4.0027771 -3.345243 -2.505873 -1.5761354 -0.78405 -0.65392494 -1.0110555 -1.3663671 -1.8222861 -2.2270646 -2.4212847][-4.587862 -4.8454766 -5.0554714 -5.3255043 -5.5377808 -5.20441 -4.3286834 -3.265552 -2.3041627 -1.5415204 -1.3252587 -1.5901594 -2.1099253 -2.3254504 -2.4648266][-5.0441046 -5.5818253 -5.9450541 -6.3261957 -6.4727459 -6.3448148 -5.5427275 -4.1304064 -2.5672522 -1.4109285 -1.0359542 -1.1000118 -1.6071768 -2.0927176 -2.4693506][-5.2809963 -5.8948245 -6.519105 -7.10639 -7.1414542 -6.9138947 -5.8801489 -4.3797197 -2.7221572 -1.3489978 -0.6708436 -0.60201406 -1.3616512 -1.9961071 -2.4080157]]...]
INFO - root - 2017-12-16 06:40:50.138334: step 38110, loss = 0.30, batch loss = 0.24 (28.1 examples/sec; 0.285 sec/batch; 23h:17m:40s remains)
INFO - root - 2017-12-16 06:40:52.958554: step 38120, loss = 0.22, batch loss = 0.16 (27.3 examples/sec; 0.293 sec/batch; 23h:59m:43s remains)
INFO - root - 2017-12-16 06:40:55.758323: step 38130, loss = 0.23, batch loss = 0.17 (29.3 examples/sec; 0.273 sec/batch; 22h:18m:31s remains)
INFO - root - 2017-12-16 06:40:58.599398: step 38140, loss = 0.32, batch loss = 0.27 (26.2 examples/sec; 0.305 sec/batch; 24h:55m:34s remains)
INFO - root - 2017-12-16 06:41:01.444617: step 38150, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.285 sec/batch; 23h:19m:28s remains)
INFO - root - 2017-12-16 06:41:04.267927: step 38160, loss = 0.22, batch loss = 0.16 (28.8 examples/sec; 0.278 sec/batch; 22h:42m:41s remains)
INFO - root - 2017-12-16 06:41:07.083529: step 38170, loss = 0.28, batch loss = 0.23 (28.0 examples/sec; 0.286 sec/batch; 23h:22m:18s remains)
INFO - root - 2017-12-16 06:41:09.861699: step 38180, loss = 0.22, batch loss = 0.16 (29.4 examples/sec; 0.272 sec/batch; 22h:14m:49s remains)
INFO - root - 2017-12-16 06:41:12.695672: step 38190, loss = 0.24, batch loss = 0.19 (28.2 examples/sec; 0.283 sec/batch; 23h:09m:48s remains)
INFO - root - 2017-12-16 06:41:15.545951: step 38200, loss = 0.22, batch loss = 0.16 (27.9 examples/sec; 0.287 sec/batch; 23h:28m:15s remains)
2017-12-16 06:41:15.991680: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0814123 -4.1284204 -4.2510419 -4.2509623 -4.2022781 -4.1188583 -4.0422616 -4.2581911 -4.4899092 -4.6531487 -4.9827561 -5.3590765 -5.4887104 -5.3128333 -5.0136466][-4.4164968 -4.5965781 -4.8035178 -4.9781837 -4.9984908 -4.9339137 -4.9331036 -4.9987645 -5.0464654 -5.3367033 -5.6088338 -5.8546038 -6.1523867 -6.1684542 -5.9283686][-4.642128 -5.0455751 -5.4211082 -5.537261 -5.4543877 -5.2513523 -5.0215731 -5.0088973 -5.1723967 -5.3288679 -5.421382 -5.7832046 -6.316689 -6.5900116 -6.4965229][-4.8231053 -5.3088031 -5.6672192 -5.7156487 -5.531641 -5.1298571 -4.6579714 -4.2159543 -3.8484764 -4.0868163 -4.4382586 -4.97677 -5.5884023 -6.0511742 -6.3004026][-4.7170892 -5.1270742 -5.38432 -5.2355952 -4.6323338 -3.7656994 -2.9866114 -2.2307003 -1.4529009 -1.3160353 -1.860785 -2.8089519 -3.8144717 -4.7858057 -5.4115386][-4.4096479 -4.6046934 -4.6432772 -4.29551 -3.3920767 -2.114094 -0.82380462 0.31138706 1.2934828 1.4672165 0.90893936 -0.48182416 -2.3079557 -3.7033281 -4.3710594][-3.948653 -3.9664886 -3.7515411 -3.2510433 -2.2233236 -0.63410187 1.0977244 2.48869 3.5692921 3.7892637 3.2230349 1.7434163 -0.34171152 -2.1975646 -3.4113441][-3.4331086 -3.5232992 -3.2117944 -2.5074439 -1.4188795 0.15167904 2.0140696 3.7055368 4.9004936 4.7939205 3.8603039 2.4347 0.63976669 -1.1776955 -2.5424747][-2.9985476 -3.2569103 -3.0216513 -2.3946168 -1.4773421 -0.20539951 1.5220184 3.052527 4.0875196 4.0816574 3.2015705 1.8069587 0.20391273 -0.99350715 -1.8051023][-2.8233423 -3.1091547 -3.1559608 -2.699024 -1.9671061 -1.1720705 -0.025392532 1.2576652 2.0902829 2.0524325 1.4103551 0.31260347 -0.80357957 -1.5233259 -1.9860492][-3.132401 -3.278327 -3.3593788 -3.2646794 -2.8965349 -2.2966003 -1.4532068 -0.8374269 -0.41698265 -0.2515521 -0.69129038 -1.4059525 -1.9549365 -2.2472296 -2.3394063][-3.5100608 -3.1729248 -3.0175891 -3.2347002 -3.1636658 -2.9505868 -2.7418709 -2.4545164 -2.0582049 -1.9203224 -2.3207963 -2.7986584 -2.9156575 -2.7977109 -2.6110537][-3.4691932 -3.0251598 -2.7756171 -2.9711394 -3.193248 -3.1556146 -3.0503051 -3.1684372 -3.1036718 -2.8215442 -2.7877412 -2.9983573 -3.1801355 -3.1214371 -2.8216066][-3.663311 -2.8238826 -2.1729934 -2.3149624 -2.6123185 -2.6889987 -2.6067958 -2.5520287 -2.5239282 -2.3085945 -2.1298621 -2.342042 -2.6014357 -2.65504 -2.5498128][-3.4060817 -2.3864229 -1.6113627 -1.5763631 -1.8702703 -2.0408397 -2.0943904 -1.9232619 -1.6348803 -1.5514264 -1.5858839 -1.6717708 -1.871465 -2.1115339 -2.1147528]]...]
INFO - root - 2017-12-16 06:41:18.802200: step 38210, loss = 0.23, batch loss = 0.17 (28.9 examples/sec; 0.277 sec/batch; 22h:38m:22s remains)
INFO - root - 2017-12-16 06:41:21.685625: step 38220, loss = 0.40, batch loss = 0.34 (28.7 examples/sec; 0.279 sec/batch; 22h:46m:36s remains)
INFO - root - 2017-12-16 06:41:24.544482: step 38230, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.279 sec/batch; 22h:49m:30s remains)
INFO - root - 2017-12-16 06:41:27.398966: step 38240, loss = 0.23, batch loss = 0.17 (26.1 examples/sec; 0.307 sec/batch; 25h:03m:53s remains)
INFO - root - 2017-12-16 06:41:30.213775: step 38250, loss = 0.26, batch loss = 0.20 (28.4 examples/sec; 0.282 sec/batch; 23h:03m:23s remains)
INFO - root - 2017-12-16 06:41:33.015779: step 38260, loss = 0.47, batch loss = 0.41 (29.4 examples/sec; 0.272 sec/batch; 22h:12m:36s remains)
INFO - root - 2017-12-16 06:41:35.840428: step 38270, loss = 0.31, batch loss = 0.25 (28.7 examples/sec; 0.279 sec/batch; 22h:45m:50s remains)
INFO - root - 2017-12-16 06:41:38.728677: step 38280, loss = 0.24, batch loss = 0.18 (27.8 examples/sec; 0.287 sec/batch; 23h:29m:46s remains)
INFO - root - 2017-12-16 06:41:41.511999: step 38290, loss = 0.22, batch loss = 0.17 (28.6 examples/sec; 0.279 sec/batch; 22h:50m:21s remains)
INFO - root - 2017-12-16 06:41:44.393883: step 38300, loss = 0.34, batch loss = 0.28 (27.3 examples/sec; 0.293 sec/batch; 23h:57m:19s remains)
2017-12-16 06:41:44.863481: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8355584 -5.061667 -3.9553294 -2.91827 -2.0567629 -1.8159399 -1.7020493 -2.03223 -2.8986645 -3.6968727 -4.2250552 -4.5412378 -4.5337429 -4.0906467 -3.458673][-5.1013174 -4.3676138 -3.3944867 -2.4444327 -1.6834946 -1.3438716 -1.3292949 -1.7118351 -2.469835 -3.2975054 -3.9645581 -4.2533264 -4.2738209 -4.0185127 -3.481406][-4.1833472 -3.499248 -2.71986 -1.9277375 -1.3597584 -1.0897963 -1.0511129 -1.4634764 -2.2760813 -3.2794256 -4.0856161 -4.6299915 -4.6910172 -4.3691654 -3.7972717][-3.0203767 -2.5224757 -1.8551707 -1.3613467 -0.9550221 -0.73305392 -0.71378922 -1.0674407 -2.0071628 -3.0810895 -4.0394287 -4.6464767 -4.8048306 -4.4063911 -3.7853713][-1.9740784 -1.5993176 -1.0842624 -0.77581191 -0.19756937 0.33554363 0.57861805 0.36210918 -0.63667345 -1.9336832 -3.2756112 -4.2553115 -4.636734 -4.5093389 -3.871418][-1.6444988 -1.4264011 -1.0101256 -0.63693976 0.21994781 1.184473 1.8509274 1.8067455 0.95755434 -0.60876083 -2.2999041 -3.7289085 -4.6039596 -4.7814708 -4.3315725][-1.677845 -1.6477809 -1.3096766 -0.775197 0.28737068 1.5553308 2.5070138 2.6104746 1.727653 0.027151108 -1.9995551 -3.8415363 -4.9445381 -5.34864 -5.025722][-2.0584297 -1.9181468 -1.6300542 -0.971148 0.35726595 1.661653 2.5661688 2.8348866 2.0539861 0.21960258 -2.0156667 -3.9151993 -5.2070732 -5.7236509 -5.3757877][-2.4122126 -2.3493073 -2.02805 -1.4659648 -0.16673374 1.18754 2.275557 2.5962625 1.8008118 0.1598835 -2.0084417 -4.0850329 -5.5702648 -6.1392784 -5.8075519][-2.4752502 -2.600358 -2.3908825 -1.9406693 -0.98347235 0.20166683 1.2394934 1.7002764 1.0977063 -0.45738053 -2.5290511 -4.4789743 -5.9304724 -6.5851846 -6.2536736][-2.217772 -2.5907059 -2.8584166 -2.6793692 -1.8441322 -0.73792505 0.19142056 0.61102533 0.15544033 -1.2635901 -3.132679 -4.97066 -6.2758789 -6.7697096 -6.4362879][-2.2850938 -2.851007 -3.1397343 -3.0584011 -2.4318519 -1.45348 -0.65147519 -0.34817123 -0.81072044 -2.0602365 -3.7445271 -5.4007235 -6.4425049 -6.6530323 -6.1463919][-2.2776208 -2.9985061 -3.3899105 -3.323287 -2.630589 -1.7891843 -1.2113082 -0.89288306 -1.44647 -2.60718 -4.0636487 -5.4135938 -6.1063285 -6.2088909 -5.7178078][-2.2926376 -3.0072403 -3.4762244 -3.3908906 -2.8390589 -2.00178 -1.4205418 -1.3421166 -1.862606 -2.8748565 -4.1762166 -5.1988058 -5.7437172 -5.7325659 -5.1226139][-2.8648214 -3.5064583 -3.7526283 -3.4983463 -2.8828528 -2.0695999 -1.5371037 -1.5197067 -1.9954779 -3.0428679 -4.1441 -5.0489073 -5.4610214 -5.2445211 -4.5471053]]...]
INFO - root - 2017-12-16 06:41:47.731838: step 38310, loss = 0.25, batch loss = 0.19 (27.8 examples/sec; 0.288 sec/batch; 23h:32m:28s remains)
INFO - root - 2017-12-16 06:41:50.572739: step 38320, loss = 0.23, batch loss = 0.17 (29.6 examples/sec; 0.271 sec/batch; 22h:06m:27s remains)
INFO - root - 2017-12-16 06:41:53.424021: step 38330, loss = 0.36, batch loss = 0.30 (27.2 examples/sec; 0.294 sec/batch; 24h:01m:03s remains)
INFO - root - 2017-12-16 06:41:56.233355: step 38340, loss = 0.50, batch loss = 0.44 (29.3 examples/sec; 0.273 sec/batch; 22h:16m:36s remains)
INFO - root - 2017-12-16 06:41:59.051314: step 38350, loss = 0.27, batch loss = 0.21 (26.7 examples/sec; 0.300 sec/batch; 24h:28m:20s remains)
INFO - root - 2017-12-16 06:42:01.852223: step 38360, loss = 0.22, batch loss = 0.16 (28.3 examples/sec; 0.283 sec/batch; 23h:05m:59s remains)
INFO - root - 2017-12-16 06:42:04.682753: step 38370, loss = 0.31, batch loss = 0.25 (28.7 examples/sec; 0.279 sec/batch; 22h:46m:21s remains)
INFO - root - 2017-12-16 06:42:07.522923: step 38380, loss = 0.22, batch loss = 0.16 (27.9 examples/sec; 0.287 sec/batch; 23h:24m:58s remains)
INFO - root - 2017-12-16 06:42:10.345474: step 38390, loss = 0.30, batch loss = 0.24 (27.8 examples/sec; 0.288 sec/batch; 23h:31m:29s remains)
INFO - root - 2017-12-16 06:42:13.177070: step 38400, loss = 0.44, batch loss = 0.38 (29.0 examples/sec; 0.276 sec/batch; 22h:33m:50s remains)
2017-12-16 06:42:13.635828: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.155457 -3.4767075 -3.9520311 -4.2022281 -4.3130479 -4.17797 -4.0177774 -3.8664222 -3.8173184 -3.7734144 -3.7345638 -3.8361092 -3.8042164 -4.12994 -4.2843757][-2.2833002 -2.6556308 -3.382401 -4.031805 -4.5190153 -4.72266 -4.7559133 -4.7276831 -4.6114974 -4.2718639 -4.11069 -4.0506496 -4.16708 -4.4912219 -4.7723804][-1.5603583 -1.9752424 -2.8378444 -3.5914705 -4.170023 -4.56593 -4.581223 -4.6680408 -4.6038966 -4.3952436 -4.2235827 -4.10593 -4.2926693 -4.6707325 -5.0378752][-1.5228257 -1.5661814 -2.0271924 -2.5471048 -2.9982748 -3.3271804 -3.3313475 -3.3049273 -3.3769059 -3.4635139 -3.5025327 -3.7468333 -4.3153129 -4.8648224 -5.2246904][-1.8132269 -1.6581278 -1.8065519 -2.0370531 -2.1440399 -1.9697981 -1.6635573 -1.492342 -1.5332687 -1.7695763 -2.1399136 -2.821909 -3.4704638 -4.1762652 -4.6420655][-1.8818886 -1.6178582 -1.4412093 -1.1161621 -0.62135291 -0.14465761 0.51624775 0.68803883 0.62722063 0.22216702 -0.56764126 -1.4481614 -2.4912884 -3.4288087 -3.7727747][-2.3436708 -1.6607075 -0.7997849 0.13663244 1.12603 2.1735225 3.1667476 3.427546 3.3195043 2.5194988 1.322763 0.0087761879 -1.3250804 -2.1455872 -2.5810568][-2.5889149 -1.8681922 -0.88121796 0.47366619 1.7078757 3.2026739 4.484663 4.9722023 4.906188 3.8840303 2.6590338 1.2554269 -0.31528091 -1.3651638 -2.04224][-3.2628713 -2.6319928 -1.8691161 -0.71518493 0.57005739 2.3645926 3.8302107 4.5518026 4.7078924 3.9718704 2.7672791 1.2061367 -0.1875782 -1.08359 -1.6716614][-4.1941524 -3.9461191 -3.6517539 -2.7046561 -1.3952529 0.24710941 1.7802072 2.8190026 3.2370148 2.7926993 1.8536677 0.46151924 -0.73386717 -1.4932599 -1.864742][-5.2914224 -5.4454226 -5.3981256 -4.7218528 -3.7729533 -2.2078223 -0.67607784 0.35259008 0.93240643 0.77045536 0.13616991 -0.87001038 -1.6587694 -1.9889557 -2.0401866][-5.5563536 -6.0782728 -6.4922056 -6.1771078 -5.7219262 -4.5655904 -3.244791 -2.239578 -1.3374929 -1.1430831 -1.4988933 -2.133919 -2.5780773 -2.6455235 -2.53507][-5.282526 -5.9359961 -6.5010242 -6.4858446 -6.1973624 -5.3864484 -4.4053483 -3.499408 -2.5724297 -2.2015691 -2.1422167 -2.3781466 -2.5654092 -2.6016989 -2.5655303][-5.174562 -5.6960006 -6.0501223 -6.0734172 -5.7751884 -4.9747276 -4.2018189 -3.4840758 -2.7886541 -2.4203947 -2.131115 -2.2075579 -2.2080407 -2.1863027 -2.2131662][-4.7592564 -5.1636229 -5.3336205 -5.0612025 -4.5532303 -3.7779043 -3.1108408 -2.5259809 -2.0389016 -1.8703074 -1.8153172 -1.8042092 -1.7085154 -1.6973517 -1.7309816]]...]
INFO - root - 2017-12-16 06:42:16.506289: step 38410, loss = 0.28, batch loss = 0.23 (28.4 examples/sec; 0.281 sec/batch; 22h:58m:36s remains)
INFO - root - 2017-12-16 06:42:19.312098: step 38420, loss = 0.38, batch loss = 0.32 (29.4 examples/sec; 0.272 sec/batch; 22h:14m:37s remains)
INFO - root - 2017-12-16 06:42:22.181901: step 38430, loss = 0.26, batch loss = 0.20 (27.7 examples/sec; 0.288 sec/batch; 23h:33m:12s remains)
INFO - root - 2017-12-16 06:42:25.078382: step 38440, loss = 0.33, batch loss = 0.27 (26.9 examples/sec; 0.297 sec/batch; 24h:17m:23s remains)
INFO - root - 2017-12-16 06:42:27.991377: step 38450, loss = 0.29, batch loss = 0.23 (27.6 examples/sec; 0.290 sec/batch; 23h:42m:26s remains)
INFO - root - 2017-12-16 06:42:30.818503: step 38460, loss = 0.33, batch loss = 0.27 (28.1 examples/sec; 0.285 sec/batch; 23h:14m:27s remains)
INFO - root - 2017-12-16 06:42:33.642669: step 38470, loss = 0.32, batch loss = 0.26 (28.8 examples/sec; 0.278 sec/batch; 22h:40m:32s remains)
INFO - root - 2017-12-16 06:42:36.522806: step 38480, loss = 0.33, batch loss = 0.27 (28.3 examples/sec; 0.283 sec/batch; 23h:05m:13s remains)
INFO - root - 2017-12-16 06:42:39.423206: step 38490, loss = 0.24, batch loss = 0.18 (27.7 examples/sec; 0.289 sec/batch; 23h:34m:32s remains)
INFO - root - 2017-12-16 06:42:42.251468: step 38500, loss = 0.31, batch loss = 0.25 (27.8 examples/sec; 0.288 sec/batch; 23h:28m:48s remains)
2017-12-16 06:42:42.722325: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5960741 -4.863399 -4.8754654 -4.9559774 -5.3102965 -5.875679 -6.7526226 -7.7788181 -8.4298973 -8.50301 -7.9729381 -6.8836808 -5.3060856 -3.7808874 -2.8113718][-5.2795076 -5.0868335 -4.6332145 -4.4826097 -4.5948572 -5.0201807 -5.9576454 -7.0813413 -8.2549582 -8.8074474 -8.569994 -7.6032329 -6.0758843 -4.4392643 -3.2874627][-6.5128775 -5.9649787 -5.0831261 -4.2503958 -3.7398491 -3.8719447 -4.6070766 -5.775969 -7.0846972 -8.0669556 -8.4717731 -7.8813362 -6.5528316 -5.0746984 -3.8172822][-6.9267874 -6.1902275 -5.1072922 -3.9258473 -2.875484 -2.3408391 -2.5192096 -3.4374924 -4.7655344 -6.1542621 -7.1405067 -7.4168935 -6.6375346 -5.2830496 -4.167613][-5.8636193 -5.0172396 -3.7503893 -2.4166758 -1.1071932 -0.14471102 0.20119047 -0.32779503 -1.6100562 -3.2595873 -5.0355849 -6.07619 -5.9975538 -5.2020597 -3.9153993][-4.1033764 -2.9736109 -1.4946942 -0.088871479 1.3942575 2.5796933 3.3601022 3.2442336 2.0524907 -0.050630093 -2.397666 -4.1555395 -4.8736992 -4.501493 -3.3998704][-3.0257804 -1.804445 -0.17600298 1.5261946 3.2553515 4.8307543 5.9950409 6.1188774 5.129158 2.7544489 -0.14968014 -2.4924557 -3.823597 -3.9699364 -3.1527402][-2.6834888 -1.3729749 -0.11479998 1.6603465 3.4881659 5.2136936 6.6978941 7.1771135 6.4073677 4.04685 1.1150417 -1.7258861 -3.4355249 -3.7363367 -3.2577643][-2.8881512 -1.812675 -0.49206209 0.89783621 2.2180929 3.9051056 5.3953075 5.9809361 5.3429451 3.1466436 0.4480691 -2.0927563 -3.7801826 -4.2295947 -3.7619033][-4.055335 -3.0403743 -2.196413 -1.3003411 -0.05751133 1.25738 2.2302823 2.8048306 2.2618256 0.75962353 -1.3749504 -3.2638578 -4.5108771 -4.8248043 -4.4430938][-6.0678658 -5.1612663 -4.442224 -3.9327531 -3.3685358 -2.3908117 -1.2748787 -0.95069551 -1.3676274 -2.3231516 -3.486624 -4.4573503 -5.1824722 -5.33201 -5.0104532][-7.4148455 -7.0979261 -7.098115 -6.7112408 -6.2169 -5.6153727 -5.1335106 -4.6715431 -4.4441204 -4.6443448 -5.1097469 -5.453342 -5.6031818 -5.5097179 -5.299305][-7.8262663 -7.8788805 -8.0646782 -8.4647846 -8.6827078 -8.3617306 -7.9885592 -7.6055775 -7.2539225 -6.7266359 -6.2187405 -5.8731647 -5.700346 -5.6120453 -5.2102938][-7.0927958 -7.3799152 -7.7108793 -8.3082962 -8.8418846 -9.1177444 -9.2015572 -8.7606926 -8.049737 -7.2212625 -6.4516716 -5.8216114 -5.4694896 -5.1412892 -4.8605995][-5.9567394 -6.2164793 -6.7007866 -7.2962122 -7.8153229 -8.20509 -8.3237944 -8.01668 -7.3793268 -6.5613203 -5.7878609 -5.1354351 -4.6977825 -4.424561 -4.2415032]]...]
INFO - root - 2017-12-16 06:42:45.507071: step 38510, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.286 sec/batch; 23h:18m:59s remains)
INFO - root - 2017-12-16 06:42:48.297611: step 38520, loss = 0.22, batch loss = 0.17 (28.5 examples/sec; 0.281 sec/batch; 22h:56m:19s remains)
INFO - root - 2017-12-16 06:42:51.112693: step 38530, loss = 0.33, batch loss = 0.28 (27.7 examples/sec; 0.289 sec/batch; 23h:34m:02s remains)
INFO - root - 2017-12-16 06:42:53.951755: step 38540, loss = 0.27, batch loss = 0.22 (27.8 examples/sec; 0.287 sec/batch; 23h:28m:02s remains)
INFO - root - 2017-12-16 06:42:56.812004: step 38550, loss = 0.22, batch loss = 0.16 (28.4 examples/sec; 0.282 sec/batch; 22h:59m:21s remains)
INFO - root - 2017-12-16 06:42:59.660581: step 38560, loss = 0.24, batch loss = 0.18 (27.7 examples/sec; 0.289 sec/batch; 23h:34m:01s remains)
INFO - root - 2017-12-16 06:43:02.505377: step 38570, loss = 0.31, batch loss = 0.25 (26.1 examples/sec; 0.306 sec/batch; 25h:00m:36s remains)
INFO - root - 2017-12-16 06:43:05.346408: step 38580, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.285 sec/batch; 23h:17m:49s remains)
INFO - root - 2017-12-16 06:43:08.228108: step 38590, loss = 0.41, batch loss = 0.35 (27.5 examples/sec; 0.291 sec/batch; 23h:44m:13s remains)
INFO - root - 2017-12-16 06:43:11.070413: step 38600, loss = 0.31, batch loss = 0.25 (27.4 examples/sec; 0.292 sec/batch; 23h:50m:17s remains)
2017-12-16 06:43:11.523929: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6984854 -4.5746322 -4.76674 -5.2050753 -6.223063 -6.8558292 -7.5302672 -7.6033392 -7.5207744 -7.1406069 -6.591423 -6.335536 -6.078043 -5.6193972 -5.7217407][-3.8316021 -3.6797686 -3.8668153 -4.2305913 -4.9095678 -6.0011649 -6.7385159 -7.0236769 -6.9465551 -6.6607094 -5.9230981 -5.7156286 -5.9389992 -5.810873 -5.7918963][-2.8867183 -2.2245095 -2.2653906 -2.6211367 -3.4631505 -4.328989 -5.0519228 -5.550065 -5.4393406 -5.4200573 -4.8600512 -4.6779666 -4.9891958 -5.1832137 -5.375721][-1.810766 -1.1153209 -0.98155928 -1.0168543 -1.8380096 -2.8089023 -3.2651324 -3.4415975 -3.3957238 -3.4248581 -3.3243685 -3.3236537 -3.9703679 -4.3334379 -4.8803215][-1.0512185 -0.33554077 0.027135372 0.21546364 -0.48696518 -1.3823006 -1.5982313 -1.774081 -1.6555135 -1.8440223 -1.7372143 -2.0450456 -2.7785654 -3.5033116 -4.2827516][-1.3109503 -0.47774124 -0.00339365 0.57932234 0.40570593 -0.27283525 -0.063788414 0.38953114 0.72186327 0.42085218 0.24758005 -0.17340565 -1.0024607 -2.0423298 -3.0087476][-2.426847 -1.5537832 -0.972435 -0.22391272 0.34476233 0.21469498 0.70210743 1.43543 2.1737061 2.1253743 1.8690653 1.5385079 0.81024981 -0.27103853 -1.9365213][-3.8268976 -3.0752354 -2.3023758 -1.5206237 -0.77772737 -0.24407244 0.63320589 1.5612502 2.3981915 2.5360994 2.5138683 2.25071 1.6990037 0.55962324 -1.0544605][-5.1699762 -4.5069394 -3.7033172 -2.9646409 -2.2484367 -1.5905874 -0.60399485 0.51628828 1.5017924 1.932487 2.1394939 2.1276608 1.815804 0.99239588 -0.43131161][-5.8711805 -5.6228094 -5.1456556 -4.3980432 -3.5656736 -2.9669619 -2.1442521 -1.1463695 -0.1146493 0.60697556 1.0326619 1.1432862 0.96878386 0.53122568 -0.4627912][-6.031364 -5.6200042 -5.5163684 -5.2455163 -4.666574 -3.9520149 -3.2545595 -2.5445375 -1.7201908 -1.2332492 -0.8772254 -0.61820912 -0.61000562 -0.80712152 -1.4756379][-5.87844 -5.5994787 -5.5104384 -5.4411931 -5.3142867 -5.0003633 -4.5562229 -4.0703034 -3.4486742 -2.8821239 -2.5861979 -2.5144067 -2.4165642 -2.2518959 -2.5363152][-5.089365 -4.9253359 -4.9842196 -5.0207925 -5.0843248 -4.9979506 -5.055953 -4.8103123 -4.2952724 -3.8703978 -3.5417309 -3.4274702 -3.448231 -3.4250464 -3.3388186][-4.2084694 -4.022151 -4.0921211 -4.30983 -4.5303073 -4.4080257 -4.427299 -4.37583 -4.2266269 -4.0133524 -3.7603648 -3.874799 -3.9582093 -3.9007893 -3.8209269][-3.5781403 -3.3415575 -3.4280696 -3.5743661 -3.7639794 -3.8409476 -3.8612881 -3.7492387 -3.5880833 -3.504879 -3.38028 -3.4048285 -3.4753942 -3.4845021 -3.4105482]]...]
INFO - root - 2017-12-16 06:43:14.379314: step 38610, loss = 0.27, batch loss = 0.21 (28.1 examples/sec; 0.285 sec/batch; 23h:14m:28s remains)
INFO - root - 2017-12-16 06:43:17.264512: step 38620, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 23h:01m:56s remains)
INFO - root - 2017-12-16 06:43:20.104405: step 38630, loss = 0.32, batch loss = 0.27 (27.4 examples/sec; 0.291 sec/batch; 23h:47m:27s remains)
INFO - root - 2017-12-16 06:43:22.945594: step 38640, loss = 0.29, batch loss = 0.23 (29.0 examples/sec; 0.276 sec/batch; 22h:32m:35s remains)
INFO - root - 2017-12-16 06:43:25.795924: step 38650, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 23h:14m:40s remains)
INFO - root - 2017-12-16 06:43:28.602398: step 38660, loss = 0.25, batch loss = 0.19 (29.0 examples/sec; 0.276 sec/batch; 22h:30m:13s remains)
INFO - root - 2017-12-16 06:43:31.384630: step 38670, loss = 0.28, batch loss = 0.22 (28.2 examples/sec; 0.284 sec/batch; 23h:09m:13s remains)
INFO - root - 2017-12-16 06:43:34.257819: step 38680, loss = 0.33, batch loss = 0.27 (27.6 examples/sec; 0.290 sec/batch; 23h:40m:11s remains)
INFO - root - 2017-12-16 06:43:37.075505: step 38690, loss = 0.20, batch loss = 0.15 (29.2 examples/sec; 0.274 sec/batch; 22h:19m:47s remains)
INFO - root - 2017-12-16 06:43:39.881606: step 38700, loss = 0.27, batch loss = 0.21 (27.9 examples/sec; 0.287 sec/batch; 23h:25m:20s remains)
2017-12-16 06:43:40.332129: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.05776 -4.9473515 -4.829874 -4.6675377 -4.4760375 -4.382947 -4.3892016 -4.3234568 -4.336802 -4.3753157 -4.2517781 -4.1128521 -4.1723275 -4.2760744 -4.36343][-4.8642454 -4.499898 -4.335 -4.3057542 -4.272016 -4.3207378 -4.3288174 -4.2162981 -4.2150755 -4.26256 -4.1359472 -3.8507946 -3.7781854 -4.0258207 -4.136806][-3.4355 -3.2615762 -3.4150598 -3.5177743 -3.5964375 -3.7059114 -3.7274895 -3.7505121 -3.8279297 -3.888032 -3.731144 -3.6026652 -3.6371875 -3.8739471 -4.0046062][-1.7931468 -1.5775406 -1.9598098 -2.4246445 -2.7849193 -2.9323707 -2.8926105 -2.7552276 -2.7597654 -3.0995984 -3.3022513 -3.4503431 -3.6712964 -3.9249113 -4.0448966][-0.69709253 -0.16515732 -0.44368339 -1.1695182 -1.540612 -1.6028585 -1.53158 -1.4564736 -1.5066097 -1.9273753 -2.4582837 -2.967253 -3.4055572 -3.8669415 -4.1450467][-0.38985682 0.35367155 0.13003206 -0.36758471 -0.34470367 -0.25864363 0.097268581 0.29564619 0.094761372 -0.59923983 -1.5044827 -2.4060726 -3.0781333 -3.58506 -3.8535433][-0.92429423 -0.054824829 -0.10138369 -0.34209108 0.12865019 0.79491186 1.4586043 1.7003598 1.3888435 0.32111835 -1.0433342 -2.081727 -2.6792231 -3.1141849 -3.4475718][-1.8598211 -1.339843 -1.5935197 -1.4440548 -0.50835657 0.65408993 1.8001151 2.200428 1.8940372 0.75695133 -0.70948792 -1.8737731 -2.7083628 -3.2055488 -3.4389765][-3.5918767 -3.1357398 -3.1614883 -2.8271255 -1.77618 -0.31374598 1.1186204 1.6115732 1.3366704 0.29526663 -0.96621108 -2.1808293 -2.9075012 -3.2888131 -3.6001854][-4.6591496 -4.8958392 -5.24804 -4.6899872 -3.3357277 -1.8363562 -0.58446026 -0.049053669 -0.1885848 -1.1737618 -2.2819986 -3.18266 -3.7660797 -3.9144115 -4.0689349][-5.9108624 -6.3847923 -6.6022797 -6.1444798 -4.9792938 -3.4984756 -2.2701077 -1.8673799 -2.1210141 -2.9583254 -3.8034909 -4.5062513 -4.7104831 -4.6153808 -4.5426474][-6.2608042 -6.9646339 -7.5343885 -7.25803 -6.2872934 -5.1273494 -3.9831431 -3.5863709 -3.6966832 -4.1433134 -4.6736388 -5.2378521 -5.3661823 -5.0865507 -4.9430928][-5.7645063 -6.3369589 -6.9889736 -7.0225124 -6.4765067 -5.7042942 -4.8857021 -4.5589771 -4.5304203 -4.767653 -5.0961637 -5.3860316 -5.3932152 -5.1651807 -5.1478477][-5.1436844 -5.3704624 -5.7575788 -6.0703764 -6.0347352 -5.4967971 -4.9760556 -4.8456306 -4.7477469 -4.6368747 -4.6815844 -4.8012462 -4.7882733 -4.6749272 -4.6607656][-4.7897596 -4.506197 -4.4887586 -4.6039519 -4.5602403 -4.4284663 -4.3193188 -4.2574811 -4.1596584 -4.0747461 -3.9733744 -3.7858934 -3.5670245 -3.53911 -3.6786366]]...]
INFO - root - 2017-12-16 06:43:43.170555: step 38710, loss = 0.24, batch loss = 0.18 (28.6 examples/sec; 0.280 sec/batch; 22h:49m:11s remains)
INFO - root - 2017-12-16 06:43:45.999485: step 38720, loss = 0.27, batch loss = 0.21 (29.3 examples/sec; 0.273 sec/batch; 22h:15m:04s remains)
INFO - root - 2017-12-16 06:43:48.904196: step 38730, loss = 0.26, batch loss = 0.20 (27.5 examples/sec; 0.291 sec/batch; 23h:46m:22s remains)
INFO - root - 2017-12-16 06:43:51.754021: step 38740, loss = 0.36, batch loss = 0.31 (28.6 examples/sec; 0.280 sec/batch; 22h:51m:36s remains)
INFO - root - 2017-12-16 06:43:54.558418: step 38750, loss = 0.29, batch loss = 0.23 (28.5 examples/sec; 0.280 sec/batch; 22h:52m:10s remains)
INFO - root - 2017-12-16 06:43:57.387854: step 38760, loss = 0.21, batch loss = 0.15 (28.0 examples/sec; 0.286 sec/batch; 23h:19m:04s remains)
INFO - root - 2017-12-16 06:44:00.221955: step 38770, loss = 0.25, batch loss = 0.19 (27.2 examples/sec; 0.295 sec/batch; 24h:01m:47s remains)
INFO - root - 2017-12-16 06:44:03.127687: step 38780, loss = 0.24, batch loss = 0.18 (25.4 examples/sec; 0.315 sec/batch; 25h:44m:19s remains)
INFO - root - 2017-12-16 06:44:05.962076: step 38790, loss = 0.29, batch loss = 0.23 (28.2 examples/sec; 0.284 sec/batch; 23h:08m:28s remains)
INFO - root - 2017-12-16 06:44:08.793843: step 38800, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.281 sec/batch; 22h:53m:07s remains)
2017-12-16 06:44:09.229349: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4853802 -3.1833324 -3.0695124 -3.1528492 -3.2881436 -3.5339112 -3.8363123 -3.8198617 -3.6797061 -3.4338088 -3.2077751 -3.273005 -3.3906488 -3.5713005 -3.6676602][-2.5233989 -2.2954919 -2.2450523 -2.5606208 -2.9971237 -3.3885379 -3.8020263 -3.9862368 -4.0046444 -3.8285575 -3.6634393 -3.7294312 -3.8688138 -4.0551581 -4.1738319][-1.5266078 -1.4171593 -1.4796679 -1.9091098 -2.4524813 -3.0667655 -3.6137934 -3.8586445 -4.0461607 -4.1847172 -4.3360653 -4.50066 -4.7389622 -4.9188013 -5.0353832][-0.95719934 -0.94478917 -1.0343211 -1.3220687 -1.697634 -2.355257 -2.9448683 -3.2053621 -3.3750834 -3.6099806 -4.00196 -4.4214373 -4.88066 -5.1961503 -5.4123483][-1.0264657 -1.065979 -0.95357943 -1.0243483 -1.2030823 -1.4726753 -1.6254048 -1.7842643 -2.1425 -2.4835618 -2.9165111 -3.5811861 -4.2482548 -4.6648436 -4.9077296][-1.245116 -1.116071 -0.77084875 -0.50834632 -0.23899698 -0.17592001 -0.13073826 -0.079677105 -0.36444092 -0.90157914 -1.7688549 -2.5792623 -3.4073319 -4.0905638 -4.4472704][-1.7206702 -1.3779864 -0.80913782 -0.25704193 0.39116669 0.94512892 1.4303937 1.6959295 1.5154366 0.82990408 -0.24127197 -1.3163977 -2.3206429 -2.9902937 -3.48139][-2.1311736 -1.85326 -1.282129 -0.53846359 0.39646673 1.2621627 2.117116 2.6637607 2.7220225 2.100769 1.0507374 -0.06948328 -1.1737297 -2.0020027 -2.6217847][-2.4643626 -2.1025491 -1.4929826 -0.81204653 0.036664486 0.99118853 1.8117676 2.2666445 2.209002 1.5625587 0.6770463 -0.30102587 -1.1961119 -1.7992253 -2.2395082][-2.6785064 -2.1906617 -1.65662 -1.1176519 -0.54153967 -0.068539143 0.29661179 0.56251049 0.46498871 -0.16627932 -0.94974589 -1.6262796 -2.1427352 -2.4493449 -2.6512854][-2.6492546 -2.1271784 -1.6616745 -1.3622952 -1.1691027 -0.99108171 -0.97919106 -1.3035247 -1.8305812 -2.3112609 -2.7404113 -3.2439518 -3.5886168 -3.7143118 -3.6584058][-2.5013652 -2.1421366 -1.9512205 -1.7609644 -1.8403752 -2.2308993 -2.7085197 -3.2644663 -3.7633879 -4.2235289 -4.4981108 -4.6975336 -4.8179612 -4.7510939 -4.6437659][-2.3943741 -2.03322 -1.8983614 -2.043365 -2.5577326 -3.1808934 -3.813252 -4.470017 -5.004004 -5.4939933 -5.6466923 -5.5172138 -5.3471174 -5.3208389 -5.3390083][-2.3097155 -1.938663 -1.8346808 -1.9055421 -2.4772129 -3.1575572 -3.8954804 -4.6808562 -5.2440381 -5.5117188 -5.541173 -5.6333942 -5.6611881 -5.4941597 -5.3439593][-2.1471076 -1.8220553 -1.7933059 -1.9192913 -2.3578711 -2.9462578 -3.5498412 -4.2631612 -4.80066 -5.2656665 -5.3388882 -5.1028566 -4.9027977 -4.8726635 -4.9453855]]...]
INFO - root - 2017-12-16 06:44:12.096250: step 38810, loss = 0.25, batch loss = 0.19 (28.6 examples/sec; 0.280 sec/batch; 22h:49m:14s remains)
INFO - root - 2017-12-16 06:44:14.971447: step 38820, loss = 0.22, batch loss = 0.16 (27.7 examples/sec; 0.289 sec/batch; 23h:34m:29s remains)
INFO - root - 2017-12-16 06:44:17.801251: step 38830, loss = 0.29, batch loss = 0.23 (28.5 examples/sec; 0.281 sec/batch; 22h:53m:06s remains)
INFO - root - 2017-12-16 06:44:20.634777: step 38840, loss = 0.24, batch loss = 0.18 (27.7 examples/sec; 0.289 sec/batch; 23h:35m:20s remains)
INFO - root - 2017-12-16 06:44:23.461032: step 38850, loss = 0.31, batch loss = 0.25 (28.2 examples/sec; 0.283 sec/batch; 23h:06m:44s remains)
INFO - root - 2017-12-16 06:44:26.341064: step 38860, loss = 0.20, batch loss = 0.14 (28.4 examples/sec; 0.282 sec/batch; 22h:58m:03s remains)
INFO - root - 2017-12-16 06:44:29.139368: step 38870, loss = 0.26, batch loss = 0.20 (29.3 examples/sec; 0.273 sec/batch; 22h:14m:26s remains)
INFO - root - 2017-12-16 06:44:31.946805: step 38880, loss = 0.25, batch loss = 0.20 (29.8 examples/sec; 0.268 sec/batch; 21h:53m:53s remains)
INFO - root - 2017-12-16 06:44:34.678768: step 38890, loss = 0.20, batch loss = 0.14 (29.9 examples/sec; 0.268 sec/batch; 21h:51m:08s remains)
INFO - root - 2017-12-16 06:44:37.526104: step 38900, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.284 sec/batch; 23h:09m:00s remains)
2017-12-16 06:44:38.024308: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8756742 -4.9940987 -5.2192492 -5.3293667 -5.4828329 -5.6177063 -5.6664162 -5.7970128 -5.7815557 -5.6078043 -5.3797164 -5.2932558 -5.4770842 -5.7131162 -5.8203135][-3.684047 -3.7582152 -3.8924491 -4.19259 -4.503521 -5.0297236 -5.557147 -5.6806536 -5.6167121 -5.5690937 -5.5051727 -5.4029174 -5.5177507 -5.8201065 -5.912611][-1.9072118 -2.0938807 -2.4920449 -2.966218 -3.3099961 -3.7118073 -4.1709294 -4.5610132 -4.6861267 -4.7488918 -4.8619986 -5.0840187 -5.4462337 -5.8226066 -5.933342][-0.70221615 -0.78231835 -1.0969992 -1.4177923 -1.8462853 -2.2672603 -2.7031522 -2.903949 -3.0721409 -3.3862576 -3.8129773 -4.3748846 -4.9524856 -5.4613671 -5.6870594][0.075602055 0.22136974 0.18346739 0.11540174 0.0702467 -0.13289785 -0.37136126 -0.70703125 -0.94648385 -1.3840399 -2.1248074 -3.0656018 -3.982183 -4.5518541 -4.8092589][0.0309062 0.44494104 0.57069397 0.69417906 0.91470194 1.2073555 1.5623136 1.5885196 1.4508572 0.83217 -0.18676901 -1.3666854 -2.5689251 -3.368104 -3.9486582][-1.2328205 -0.51719689 -0.034261227 0.64771557 1.6820841 2.663311 3.5165114 3.8189449 3.68009 2.925632 1.7342501 0.37562275 -1.0709572 -2.2414451 -3.2575564][-2.5073147 -1.841542 -1.3107531 -0.37094164 1.1404376 3.0317426 4.6336975 5.1952171 5.1562853 4.544302 3.0442629 1.3309546 -0.32096338 -1.5518825 -2.7395525][-4.573245 -4.0315809 -3.4357235 -2.4326982 -0.66576934 1.5254245 3.4402857 4.3184872 4.6454353 4.232151 3.0604177 1.5145712 -0.27371645 -1.957283 -3.3307271][-6.3130302 -6.4162092 -6.4237347 -5.3937211 -3.6236186 -1.4949329 0.58294582 2.0425692 2.6615548 2.5366855 1.6126752 0.29649878 -1.2779953 -2.7766681 -4.01369][-7.5770531 -8.2720528 -8.7265892 -8.195612 -6.895164 -4.8255668 -2.8304131 -1.3299479 -0.43415451 -0.16462708 -0.85312891 -2.0490575 -3.2801633 -4.5888848 -5.5589771][-9.4178162 -10.396597 -10.929667 -10.567827 -9.466177 -7.7421026 -6.1395221 -4.6553841 -3.7115936 -3.6921945 -4.0841503 -4.6498013 -5.32845 -6.171917 -6.6209917][-10.564784 -11.846181 -12.480593 -12.117785 -11.362398 -10.08045 -8.7944155 -7.5207677 -6.9074564 -6.7710009 -6.8832922 -7.1830091 -7.4246149 -7.6000395 -7.4335976][-11.284058 -12.326272 -12.681442 -12.327852 -11.775972 -11.147049 -10.523749 -9.696125 -9.2459354 -9.162056 -9.21176 -9.2072983 -8.9130764 -8.5937634 -7.8856382][-11.200045 -11.982059 -12.195324 -11.777885 -11.204275 -10.725192 -10.422787 -10.245136 -10.335814 -10.455265 -10.453176 -10.062386 -9.4198818 -8.7686386 -7.6630044]]...]
INFO - root - 2017-12-16 06:44:40.894780: step 38910, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 23h:12m:25s remains)
INFO - root - 2017-12-16 06:44:43.749600: step 38920, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.282 sec/batch; 23h:01m:13s remains)
INFO - root - 2017-12-16 06:44:46.558930: step 38930, loss = 0.39, batch loss = 0.33 (27.9 examples/sec; 0.287 sec/batch; 23h:25m:27s remains)
INFO - root - 2017-12-16 06:44:49.403181: step 38940, loss = 0.25, batch loss = 0.19 (28.9 examples/sec; 0.277 sec/batch; 22h:33m:57s remains)
INFO - root - 2017-12-16 06:44:52.191700: step 38950, loss = 0.31, batch loss = 0.25 (28.8 examples/sec; 0.278 sec/batch; 22h:39m:52s remains)
INFO - root - 2017-12-16 06:44:55.057174: step 38960, loss = 0.27, batch loss = 0.21 (28.6 examples/sec; 0.279 sec/batch; 22h:47m:19s remains)
INFO - root - 2017-12-16 06:44:57.879717: step 38970, loss = 0.29, batch loss = 0.23 (27.0 examples/sec; 0.296 sec/batch; 24h:09m:39s remains)
INFO - root - 2017-12-16 06:45:00.680731: step 38980, loss = 0.30, batch loss = 0.24 (27.3 examples/sec; 0.293 sec/batch; 23h:53m:32s remains)
INFO - root - 2017-12-16 06:45:03.480356: step 38990, loss = 0.28, batch loss = 0.22 (28.5 examples/sec; 0.281 sec/batch; 22h:55m:11s remains)
INFO - root - 2017-12-16 06:45:06.274764: step 39000, loss = 0.32, batch loss = 0.26 (29.1 examples/sec; 0.275 sec/batch; 22h:24m:01s remains)
2017-12-16 06:45:06.735352: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.56364 -3.8707349 -4.4433579 -4.8130083 -5.2588363 -5.6210642 -5.7404938 -5.8664227 -5.5229006 -5.06791 -4.6236386 -4.5194364 -4.6933517 -4.7703571 -4.6766825][-2.8777645 -3.4139924 -4.0814042 -4.5344477 -4.9792871 -5.2524366 -5.0807877 -4.8957253 -4.5422225 -4.3092141 -4.1003408 -3.8398168 -3.9138615 -4.0522366 -3.9424348][-2.1516483 -2.8744535 -3.5489149 -3.8886492 -4.1012607 -4.1050611 -3.7853136 -3.3496275 -2.8679857 -2.7755504 -2.8738964 -2.9628739 -3.1776323 -3.1040151 -2.9772058][-1.6428747 -2.5407591 -3.3598228 -3.5932798 -3.4355507 -2.8692112 -2.1233685 -1.6096301 -1.2680311 -1.3427134 -1.5328078 -1.9170072 -2.3723788 -2.4148316 -2.2839146][-1.5776668 -2.4008894 -2.8414559 -2.7942665 -2.338845 -1.4242072 -0.40164709 0.16309214 0.40830851 0.17772627 -0.18264532 -0.70598006 -1.3056929 -1.6140995 -1.7200296][-1.084336 -1.709564 -1.7834663 -1.1345901 0.0013532639 1.15594 2.1625056 2.6699862 2.517139 1.8568912 0.92315865 0.23441935 -0.39704037 -0.81692934 -1.0205944][-0.27889681 -0.6473 -0.67727995 0.22597837 1.5852032 2.971561 4.0616617 4.3557892 3.9465389 3.0052652 1.9802814 1.0638151 0.15548182 -0.27295732 -0.52575326][0.38512707 0.13423157 -0.0030040741 0.56280565 1.6234913 2.876123 3.9950914 4.2071486 3.7922573 3.0558348 2.2085361 1.5052519 0.85612535 0.2499094 -0.27565622][1.1495318 0.71241 0.51839113 0.669157 1.2498555 2.0334091 2.6752667 2.8751488 2.8110762 2.5216985 2.0863814 1.4833322 0.87742949 0.54676247 0.24452448][1.8761406 1.1278248 0.26172018 0.22094107 0.73787737 1.1932378 1.5004902 1.6196413 1.5681858 1.5270538 1.4846811 1.1859536 0.69625711 0.25252151 -0.25316143][1.5284319 0.61587811 -0.69980788 -1.3812854 -1.3981841 -0.84671593 -0.23010683 0.024367332 0.065475941 0.086922169 0.12306309 0.059229851 -0.17712021 -0.594301 -1.0726748][-0.27962446 -1.2057414 -2.3241625 -3.0991011 -3.25842 -3.0264938 -2.691632 -2.0365305 -1.4078557 -1.2309005 -1.1426048 -1.2178416 -1.3736582 -1.6633 -1.998493][-1.7483397 -2.7580681 -3.8089314 -4.617547 -4.8327341 -4.5435567 -4.2459841 -3.7071881 -3.0624423 -2.450815 -1.9689026 -2.0174274 -2.1653783 -2.354316 -2.5631099][-3.1545219 -3.8258798 -4.6006889 -5.1833324 -5.4653192 -5.2269745 -4.9129081 -4.415092 -3.9502115 -3.6097684 -3.2304859 -3.0473533 -3.0017157 -3.1898994 -3.2389791][-4.88301 -5.196723 -5.5466523 -5.71125 -5.6988521 -5.4832363 -5.2443027 -4.8343096 -4.4567881 -4.2660508 -4.0226502 -3.7771037 -3.586848 -3.6344237 -3.5525365]]...]
INFO - root - 2017-12-16 06:45:09.572556: step 39010, loss = 0.27, batch loss = 0.21 (28.6 examples/sec; 0.280 sec/batch; 22h:49m:44s remains)
INFO - root - 2017-12-16 06:45:12.451846: step 39020, loss = 0.22, batch loss = 0.16 (28.1 examples/sec; 0.284 sec/batch; 23h:10m:45s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 06:45:15.268429: step 39030, loss = 0.27, batch loss = 0.21 (28.7 examples/sec; 0.278 sec/batch; 22h:41m:35s remains)
INFO - root - 2017-12-16 06:45:18.076265: step 39040, loss = 0.28, batch loss = 0.23 (28.5 examples/sec; 0.281 sec/batch; 22h:52m:03s remains)
INFO - root - 2017-12-16 06:45:20.918055: step 39050, loss = 0.27, batch loss = 0.21 (27.5 examples/sec; 0.291 sec/batch; 23h:41m:03s remains)
INFO - root - 2017-12-16 06:45:23.756012: step 39060, loss = 0.25, batch loss = 0.19 (26.3 examples/sec; 0.304 sec/batch; 24h:45m:04s remains)
INFO - root - 2017-12-16 06:45:26.568283: step 39070, loss = 0.22, batch loss = 0.16 (29.8 examples/sec; 0.268 sec/batch; 21h:52m:18s remains)
INFO - root - 2017-12-16 06:45:29.426342: step 39080, loss = 0.28, batch loss = 0.23 (28.7 examples/sec; 0.279 sec/batch; 22h:44m:51s remains)
INFO - root - 2017-12-16 06:45:32.288850: step 39090, loss = 0.25, batch loss = 0.19 (27.1 examples/sec; 0.295 sec/batch; 24h:02m:14s remains)
INFO - root - 2017-12-16 06:45:35.113935: step 39100, loss = 0.34, batch loss = 0.28 (29.0 examples/sec; 0.276 sec/batch; 22h:28m:14s remains)
2017-12-16 06:45:35.567859: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2238026 -5.1446137 -4.6682768 -4.0028291 -3.4975245 -3.5601335 -3.9282427 -4.3702283 -4.7540073 -5.0811772 -5.2720551 -5.4512148 -5.6623783 -5.6610565 -5.559773][-5.4547219 -5.3538518 -4.9041352 -4.1113839 -3.2767696 -3.0922785 -3.2419188 -3.7634223 -4.2750721 -4.6769657 -5.0315504 -5.2282853 -5.5268397 -5.5819111 -5.4866509][-6.0170579 -5.7971582 -4.9344091 -3.7957931 -2.6768177 -2.2095475 -2.1807146 -2.6009724 -3.1812043 -3.7967761 -4.2858191 -4.5790243 -5.0128117 -5.0995426 -5.0598636][-5.9543991 -5.7064033 -4.7397351 -3.2020397 -1.6145127 -0.72334743 -0.50563741 -0.8659544 -1.4420583 -2.2091396 -2.9755788 -3.4066422 -3.8724635 -4.1567783 -4.3091726][-5.60984 -5.1702824 -3.9167092 -2.0916085 -0.22911072 1.048203 1.4930182 1.1798301 0.44582129 -0.39247847 -1.1920524 -1.8901422 -2.4964609 -2.8419895 -3.0403748][-5.2321634 -4.7190971 -3.430162 -1.1639297 1.2136064 2.68897 3.2864347 3.0676503 2.2866631 1.2548895 0.44424677 -0.23042965 -0.83409405 -1.2768052 -1.4505286][-4.8768048 -4.4374952 -3.12186 -0.79805112 1.6253333 3.5107684 4.4424686 4.1834526 3.2621274 2.2488198 1.4799109 0.75065565 0.26964712 -0.0055913925 -0.047541142][-4.867178 -4.4004335 -3.3319559 -1.2725818 1.1497955 3.1958938 4.4122 4.4818649 3.661027 2.5150909 1.7833486 1.0524535 0.6356082 0.45806551 0.4231329][-5.001646 -4.7957606 -3.7261398 -1.913877 0.0678854 1.9038806 3.0921412 3.2583585 2.7213821 1.8430786 1.1841512 0.54898024 0.29286003 0.1461277 0.17605686][-5.6889811 -5.4039016 -4.4281416 -3.0954514 -1.463722 0.10783911 0.93034935 0.93437862 0.72224522 0.17810583 -0.29176521 -0.84004116 -1.1233513 -1.2482009 -1.3162062][-5.8984289 -5.7115 -5.1133089 -4.1393223 -3.0537057 -2.0725818 -1.5716496 -1.6526127 -2.0040669 -2.4153533 -2.6124992 -2.9280381 -3.2621036 -3.6317303 -3.781646][-6.0243144 -5.8354387 -5.2231789 -4.6024637 -3.9987028 -3.6866665 -3.7955112 -4.1263142 -4.4231982 -4.9157057 -5.1514053 -5.3295956 -5.5226355 -5.8589954 -6.1343246][-5.3361831 -5.3205452 -5.1318579 -4.83914 -4.426322 -4.3615952 -4.6911411 -5.3398123 -6.0655136 -6.5531321 -6.7360744 -6.7520428 -6.8431511 -7.3843718 -7.6605744][-4.7582259 -4.6565638 -4.3320556 -4.0916553 -4.0476842 -4.4049058 -4.8441114 -5.4992 -6.4535656 -7.2229271 -7.4932709 -7.4995852 -7.6179838 -8.0012045 -8.2657356][-4.7003317 -4.4517727 -3.9514184 -3.5373006 -3.3982947 -3.6839738 -4.2636533 -4.89506 -5.8070421 -6.6940107 -7.1611776 -7.227025 -7.1270685 -7.3361559 -7.7580681]]...]
INFO - root - 2017-12-16 06:45:38.395729: step 39110, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.285 sec/batch; 23h:15m:39s remains)
INFO - root - 2017-12-16 06:45:41.199023: step 39120, loss = 0.29, batch loss = 0.23 (28.8 examples/sec; 0.277 sec/batch; 22h:35m:56s remains)
INFO - root - 2017-12-16 06:45:44.044561: step 39130, loss = 0.31, batch loss = 0.25 (28.7 examples/sec; 0.279 sec/batch; 22h:41m:53s remains)
INFO - root - 2017-12-16 06:45:46.864277: step 39140, loss = 0.30, batch loss = 0.24 (28.2 examples/sec; 0.283 sec/batch; 23h:05m:09s remains)
INFO - root - 2017-12-16 06:45:49.659708: step 39150, loss = 0.28, batch loss = 0.22 (28.2 examples/sec; 0.284 sec/batch; 23h:08m:26s remains)
INFO - root - 2017-12-16 06:45:52.485387: step 39160, loss = 0.29, batch loss = 0.23 (28.0 examples/sec; 0.285 sec/batch; 23h:15m:12s remains)
INFO - root - 2017-12-16 06:45:55.304080: step 39170, loss = 0.31, batch loss = 0.25 (29.2 examples/sec; 0.274 sec/batch; 22h:18m:46s remains)
INFO - root - 2017-12-16 06:45:58.155951: step 39180, loss = 0.27, batch loss = 0.21 (27.2 examples/sec; 0.294 sec/batch; 23h:57m:36s remains)
INFO - root - 2017-12-16 06:46:00.971984: step 39190, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.283 sec/batch; 23h:04m:32s remains)
INFO - root - 2017-12-16 06:46:03.797250: step 39200, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.281 sec/batch; 22h:52m:14s remains)
2017-12-16 06:46:04.266647: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1390996 -5.710866 -5.6841264 -5.7283845 -5.983182 -6.1554017 -6.5192022 -7.0216689 -7.428401 -7.3465753 -6.84445 -6.0359869 -5.0592947 -4.125556 -3.3447208][-5.1511021 -5.7699461 -5.917779 -6.1434383 -6.4011269 -6.667068 -7.058023 -7.3641233 -7.7114124 -7.9252367 -7.6024876 -6.7919207 -5.9180169 -4.9684968 -4.0705462][-4.1412988 -5.0132046 -5.4241028 -5.4965429 -5.5927744 -5.8050942 -6.0531716 -6.3877869 -6.8867273 -7.46122 -7.6774111 -7.3254647 -6.7459011 -5.7851133 -4.8345346][-2.9399834 -3.6237712 -3.9345253 -3.9452672 -3.8426213 -3.7954597 -3.8637421 -4.3225808 -5.1408105 -5.9404907 -6.6518307 -7.0675812 -7.1313581 -6.4162645 -5.4803114][-2.2759168 -2.4725487 -2.3012712 -2.0128644 -1.6770577 -1.1043184 -0.85382247 -1.325779 -2.213474 -3.7771566 -5.3829865 -6.3290291 -6.8837485 -6.6772656 -5.9605479][-1.6381733 -1.4226983 -1.0822282 -0.40324068 0.67741585 1.6255913 2.1238828 1.9907284 1.084703 -0.910239 -3.2046475 -5.1677251 -6.5194426 -6.7274151 -6.2102394][-1.1585751 -0.5807395 0.18229389 1.3013854 2.5903077 4.0136404 5.0542555 4.8696194 3.701457 1.3429232 -1.344949 -3.8549426 -5.7025342 -6.177021 -5.8995938][-1.2638438 -0.81085849 -0.13707876 1.1019335 2.6901722 4.3770218 5.6163359 5.6473246 4.6176233 2.2485528 -0.55328321 -3.2325091 -5.2084827 -5.9262595 -5.7878294][-2.06921 -1.73019 -1.3589478 -0.38633156 0.94368124 2.6243615 3.9479942 4.1806154 3.5118127 1.2971358 -1.2829165 -3.6418109 -5.3860908 -6.0105605 -5.8473816][-2.2091055 -2.6516814 -2.9942966 -2.155118 -1.0403945 0.064254761 1.0916581 1.5554385 1.1589646 -0.55189466 -2.5224061 -4.5744948 -5.9187913 -6.313633 -6.168026][-2.8566155 -3.4514771 -3.9740698 -3.8934722 -3.6425743 -2.7330737 -1.8259063 -1.737675 -2.0202703 -2.9743638 -4.1590209 -5.4867697 -6.3252592 -6.5879269 -6.2630463][-3.4546165 -4.1685276 -4.9391732 -5.2264934 -5.1178579 -4.7276592 -4.51562 -4.3123331 -4.2195263 -4.869741 -5.3410654 -6.0306592 -6.4731016 -6.442276 -6.1121788][-3.6549637 -4.1906586 -4.914505 -5.3595057 -5.5493455 -5.3823624 -5.1343312 -5.1473508 -5.2741489 -5.5122385 -5.6635151 -6.1023712 -6.2264142 -6.1873178 -5.9809575][-3.5713847 -3.8958268 -4.3611031 -4.7967281 -5.0764585 -4.9570265 -4.9126439 -4.9816489 -5.0171371 -5.1481137 -5.14942 -5.3471866 -5.4222965 -5.4299922 -5.2446842][-3.5984545 -3.6826808 -3.9362433 -4.1537185 -4.219604 -4.2176266 -4.2166424 -4.243813 -4.2383327 -4.3344436 -4.3525972 -4.2354817 -4.1038141 -4.1167994 -4.0826769]]...]
INFO - root - 2017-12-16 06:46:07.095293: step 39210, loss = 0.29, batch loss = 0.23 (27.9 examples/sec; 0.287 sec/batch; 23h:23m:12s remains)
INFO - root - 2017-12-16 06:46:09.909032: step 39220, loss = 0.24, batch loss = 0.18 (29.4 examples/sec; 0.272 sec/batch; 22h:11m:46s remains)
INFO - root - 2017-12-16 06:46:12.684600: step 39230, loss = 0.24, batch loss = 0.18 (29.3 examples/sec; 0.273 sec/batch; 22h:13m:41s remains)
INFO - root - 2017-12-16 06:46:15.502723: step 39240, loss = 0.28, batch loss = 0.22 (28.6 examples/sec; 0.280 sec/batch; 22h:48m:19s remains)
INFO - root - 2017-12-16 06:46:18.332654: step 39250, loss = 0.26, batch loss = 0.20 (28.8 examples/sec; 0.277 sec/batch; 22h:35m:53s remains)
INFO - root - 2017-12-16 06:46:21.171761: step 39260, loss = 0.25, batch loss = 0.20 (28.7 examples/sec; 0.279 sec/batch; 22h:42m:52s remains)
INFO - root - 2017-12-16 06:46:24.009885: step 39270, loss = 0.26, batch loss = 0.20 (26.2 examples/sec; 0.305 sec/batch; 24h:49m:25s remains)
INFO - root - 2017-12-16 06:46:26.847015: step 39280, loss = 0.28, batch loss = 0.22 (28.8 examples/sec; 0.278 sec/batch; 22h:36m:41s remains)
INFO - root - 2017-12-16 06:46:29.666773: step 39290, loss = 0.33, batch loss = 0.27 (29.1 examples/sec; 0.275 sec/batch; 22h:23m:56s remains)
INFO - root - 2017-12-16 06:46:32.522767: step 39300, loss = 0.26, batch loss = 0.20 (26.4 examples/sec; 0.304 sec/batch; 24h:43m:10s remains)
2017-12-16 06:46:32.978125: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5061228 -3.5525548 -3.6090717 -3.6332364 -3.6218567 -3.60204 -3.614253 -3.9228559 -4.1012139 -4.3584166 -4.5001497 -4.5111694 -4.346487 -3.8596592 -3.6582546][-3.2261381 -3.2432323 -3.2432156 -3.309068 -3.31225 -3.5357475 -3.7017851 -3.7806952 -3.8365083 -4.0821381 -4.2412295 -4.3070769 -4.2669358 -4.0449934 -3.9723349][-2.670012 -2.5521412 -2.5791547 -2.7463524 -2.9074566 -3.0795364 -3.2111218 -3.4556339 -3.4635339 -3.5237234 -3.6678543 -3.8584738 -3.9367614 -3.9489837 -4.0289607][-1.6070423 -1.6479514 -1.8846927 -2.1360257 -2.4591579 -2.7194343 -2.829556 -2.8805256 -2.7813866 -2.7897561 -2.7892268 -3.1143136 -3.41296 -3.7236731 -4.0048232][-0.81011057 -0.68900537 -0.91762233 -1.3525574 -1.7544155 -1.9768012 -2.06143 -1.9438851 -1.5693381 -1.631907 -1.753449 -2.0414193 -2.3548899 -2.8348894 -3.2876363][-0.14996719 -0.024159431 -0.30435467 -0.76259947 -1.1591735 -1.3003056 -1.1843855 -0.88202119 -0.45517683 -0.35108805 -0.46599007 -1.0182724 -1.5172102 -2.0462909 -2.4636841][-0.23545742 -0.055971622 -0.21185446 -0.6026783 -0.84118462 -0.80213332 -0.49705148 0.037904263 0.60938692 0.59111977 0.20284796 -0.44341612 -1.1306839 -1.6682575 -2.0080533][-0.93700409 -0.58311939 -0.52370143 -0.67296195 -0.67022681 -0.41906023 0.10962963 0.77784634 1.3375778 1.3255496 0.73396683 -0.19380283 -1.0875716 -1.8413279 -2.2748132][-1.7943332 -1.4074275 -1.1650662 -1.1296904 -0.860754 -0.45881748 0.13896084 0.82316971 1.2698231 1.2724776 0.68074894 -0.29891253 -1.3157637 -2.0935006 -2.4918447][-2.7700162 -2.2942555 -1.8631864 -1.7325969 -1.3969977 -0.89198112 -0.3514719 0.20505238 0.55922127 0.53493547 -0.14579773 -1.0356653 -1.9366593 -2.7055266 -3.0505733][-3.3670454 -3.1476903 -2.7965474 -2.5923085 -2.2847371 -1.7686155 -1.3359721 -1.0329123 -0.8750906 -0.94570637 -1.4207051 -2.0926673 -2.7605982 -3.2458987 -3.4864254][-4.3582373 -4.2547221 -4.0295362 -3.7430041 -3.3507388 -3.0832787 -2.8199303 -2.5337813 -2.4629664 -2.5473595 -2.9393673 -3.2952874 -3.5366712 -3.775105 -3.8054621][-5.0801263 -5.3087807 -5.2879262 -5.0714693 -4.7152672 -4.2791681 -4.0041213 -3.9539204 -3.9752121 -3.9457638 -4.053071 -4.0893564 -3.9607186 -3.7910211 -3.6112223][-5.8170934 -6.2251468 -6.3431268 -6.1967316 -5.831718 -5.53079 -5.2824345 -5.1035228 -5.0469179 -4.9695349 -4.8771753 -4.6465263 -4.2814822 -3.8839779 -3.4818337][-6.3737125 -6.932766 -7.1392231 -7.0646849 -6.7132721 -6.1769924 -5.7324433 -5.4452729 -5.261405 -5.2381887 -5.2131295 -4.8933253 -4.4259629 -3.9541192 -3.5315552]]...]
INFO - root - 2017-12-16 06:46:35.812883: step 39310, loss = 0.31, batch loss = 0.26 (29.0 examples/sec; 0.276 sec/batch; 22h:27m:36s remains)
INFO - root - 2017-12-16 06:46:38.615910: step 39320, loss = 0.44, batch loss = 0.38 (27.2 examples/sec; 0.294 sec/batch; 23h:56m:49s remains)
INFO - root - 2017-12-16 06:46:41.423943: step 39330, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.285 sec/batch; 23h:12m:10s remains)
INFO - root - 2017-12-16 06:46:44.261504: step 39340, loss = 0.30, batch loss = 0.24 (27.9 examples/sec; 0.286 sec/batch; 23h:19m:03s remains)
INFO - root - 2017-12-16 06:46:47.078137: step 39350, loss = 0.30, batch loss = 0.25 (29.5 examples/sec; 0.271 sec/batch; 22h:05m:36s remains)
INFO - root - 2017-12-16 06:46:49.897539: step 39360, loss = 0.30, batch loss = 0.25 (29.1 examples/sec; 0.275 sec/batch; 22h:24m:01s remains)
INFO - root - 2017-12-16 06:46:52.722681: step 39370, loss = 0.27, batch loss = 0.21 (28.9 examples/sec; 0.277 sec/batch; 22h:32m:09s remains)
INFO - root - 2017-12-16 06:46:55.567832: step 39380, loss = 0.23, batch loss = 0.18 (29.1 examples/sec; 0.275 sec/batch; 22h:22m:53s remains)
INFO - root - 2017-12-16 06:46:58.415279: step 39390, loss = 0.30, batch loss = 0.24 (25.5 examples/sec; 0.313 sec/batch; 25h:29m:47s remains)
INFO - root - 2017-12-16 06:47:01.240779: step 39400, loss = 0.21, batch loss = 0.15 (26.3 examples/sec; 0.304 sec/batch; 24h:43m:07s remains)
2017-12-16 06:47:01.723605: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.394701 -3.224472 -3.36143 -3.5908391 -3.8669319 -4.0925164 -4.2624097 -4.4157748 -4.4524026 -4.2701254 -3.9351573 -3.6835105 -3.5014391 -3.1672039 -2.8155494][-3.6875763 -3.5753536 -3.5294719 -3.6947267 -4.0508976 -4.4871955 -4.7636752 -4.9208107 -4.863307 -4.8342752 -4.804925 -4.4821181 -4.0890288 -3.7604525 -3.3462639][-4.1278949 -3.9806945 -4.0521207 -4.0745797 -4.0850086 -4.2651076 -4.4721737 -4.9724298 -5.4079766 -5.5474958 -5.47178 -5.2557807 -5.0064769 -4.5055404 -4.0663457][-4.8065376 -4.7470837 -4.4177761 -3.9146495 -3.3574691 -3.2239573 -3.7125461 -4.3283467 -4.8862238 -5.5109839 -5.921587 -5.9845715 -5.6949911 -5.1752529 -4.6797194][-5.264842 -4.8743029 -4.203814 -3.3071098 -2.296113 -1.5293355 -1.4442554 -2.2669096 -3.5429311 -4.8823075 -5.8267326 -6.1724777 -6.0304961 -5.4893637 -4.8584023][-5.4240727 -4.9407406 -3.7219949 -1.7357395 0.17771292 1.3231664 1.1975369 0.041702271 -1.6214578 -3.3216648 -4.758575 -5.6706619 -5.8693385 -5.4699969 -4.6996169][-5.7172208 -4.7147017 -3.0292382 -0.69386005 1.6580763 3.3375406 3.8368673 2.7789202 0.6153717 -1.7435288 -3.742419 -4.7356777 -5.1440616 -4.974844 -3.9500456][-5.2314172 -4.529563 -2.9124026 -0.2130847 2.637701 4.7020626 4.7847795 3.488338 1.6626406 -0.42290974 -2.4248981 -3.8015018 -4.2040095 -4.0672932 -3.4544177][-5.4620171 -4.7350216 -3.2539206 -0.95562577 1.4154186 3.4473929 4.292593 3.3664002 1.332408 -0.71972013 -2.5660286 -3.9417129 -4.4698086 -3.9501739 -2.6870642][-5.474906 -5.4177427 -4.68163 -2.9191499 -0.62454796 1.1547351 1.5337791 1.0730624 0.08943224 -1.7222676 -3.4081349 -4.6602049 -5.1540251 -4.7997546 -3.7766616][-5.788095 -5.7794771 -5.41863 -4.6044207 -3.6105669 -2.2494552 -1.1181502 -1.4299583 -2.3925068 -3.5437248 -4.691256 -5.67075 -5.9344354 -5.6824589 -4.6940684][-5.56897 -5.8028479 -5.9749489 -5.5206981 -4.8146014 -4.4130721 -4.1951904 -4.0993128 -4.34047 -5.0358238 -5.6711211 -6.5212808 -7.097805 -6.76789 -5.788075][-4.8049417 -5.0877962 -5.4022179 -5.6086917 -5.7189116 -5.2800412 -4.9699764 -5.0889783 -5.0992246 -5.3161817 -5.788837 -6.3394375 -6.8946829 -7.3018565 -6.9393854][-4.1066813 -4.21864 -4.3571219 -4.5151734 -4.5638008 -4.7322807 -4.9964318 -5.0727482 -4.9728251 -4.8133612 -4.7048297 -5.4377122 -6.3545952 -6.8865328 -6.8727484][-3.7012486 -3.5363307 -3.3300371 -3.4268045 -3.5088894 -3.5602932 -3.6828375 -3.8394113 -3.7894914 -3.8011961 -3.9178298 -4.1814604 -4.9024105 -6.0063567 -6.5590963]]...]
INFO - root - 2017-12-16 06:47:04.586700: step 39410, loss = 0.21, batch loss = 0.15 (28.4 examples/sec; 0.281 sec/batch; 22h:54m:33s remains)
INFO - root - 2017-12-16 06:47:07.410448: step 39420, loss = 0.29, batch loss = 0.24 (28.3 examples/sec; 0.283 sec/batch; 23h:01m:58s remains)
INFO - root - 2017-12-16 06:47:10.264640: step 39430, loss = 0.39, batch loss = 0.33 (28.0 examples/sec; 0.285 sec/batch; 23h:13m:32s remains)
INFO - root - 2017-12-16 06:47:13.068963: step 39440, loss = 0.48, batch loss = 0.42 (29.3 examples/sec; 0.273 sec/batch; 22h:15m:14s remains)
INFO - root - 2017-12-16 06:47:15.930312: step 39450, loss = 0.21, batch loss = 0.15 (27.1 examples/sec; 0.296 sec/batch; 24h:04m:27s remains)
INFO - root - 2017-12-16 06:47:18.754537: step 39460, loss = 0.41, batch loss = 0.35 (28.6 examples/sec; 0.280 sec/batch; 22h:45m:12s remains)
INFO - root - 2017-12-16 06:47:21.594642: step 39470, loss = 0.25, batch loss = 0.19 (27.2 examples/sec; 0.294 sec/batch; 23h:58m:11s remains)
INFO - root - 2017-12-16 06:47:24.433925: step 39480, loss = 0.30, batch loss = 0.24 (27.0 examples/sec; 0.296 sec/batch; 24h:07m:54s remains)
INFO - root - 2017-12-16 06:47:27.267193: step 39490, loss = 0.31, batch loss = 0.25 (28.7 examples/sec; 0.279 sec/batch; 22h:42m:30s remains)
INFO - root - 2017-12-16 06:47:30.084908: step 39500, loss = 0.29, batch loss = 0.23 (28.6 examples/sec; 0.279 sec/batch; 22h:44m:49s remains)
2017-12-16 06:47:30.567662: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2836013 -5.2959547 -5.4085941 -5.3256922 -5.2453403 -5.1979752 -5.2044735 -5.0239029 -4.6620154 -4.5428562 -4.381741 -4.2179408 -4.0933714 -3.9037046 -3.9109521][-5.3172436 -5.4422216 -5.564198 -5.4506536 -5.3203545 -5.2834415 -5.2676568 -5.0488148 -4.6757469 -4.4796228 -4.2271047 -3.9519744 -3.8594556 -3.7884908 -3.8496082][-5.1826696 -5.1985273 -5.2816954 -5.1531844 -4.8413544 -4.6972737 -4.596457 -4.4288917 -4.1262593 -3.8665411 -3.6185622 -3.4405715 -3.3731766 -3.3661602 -3.5085053][-4.61022 -4.5026093 -4.4528189 -4.3659406 -4.0977077 -3.8276026 -3.5471539 -3.22429 -2.8134007 -2.5988512 -2.3772912 -2.0977108 -2.0243073 -2.0953877 -2.2963645][-3.8532114 -3.5653887 -3.2531545 -2.9886856 -2.4929557 -2.021595 -1.6085823 -1.2911751 -1.117605 -1.0685086 -1.0313935 -1.0200021 -1.0913429 -1.2066658 -1.3151042][-3.1001225 -2.6394403 -2.148448 -1.6843967 -0.94495153 -0.34516859 0.20164871 0.40670204 0.233037 0.12745094 0.076781273 -0.080465794 -0.19668102 -0.38079071 -0.52640986][-2.8376846 -2.055151 -1.2577288 -0.5388701 0.25910425 0.94398642 1.5434122 1.6257548 1.3504357 0.99898529 0.75335932 0.59715414 0.512136 0.35960245 0.20540333][-2.7922354 -1.8081729 -0.71023464 0.23064709 1.0482674 1.727582 2.2476521 2.361455 2.1527672 1.5936856 1.1366806 0.87273788 0.71459246 0.51379251 0.27779055][-2.5448294 -1.7044547 -0.752831 0.26247358 1.1657205 1.7730284 2.2343211 2.350708 2.1493554 1.6547608 1.2137237 0.88535738 0.71427155 0.50777817 0.19228315][-2.6719632 -2.1290379 -1.3121026 -0.37117958 0.49653196 1.0196676 1.3290215 1.4373994 1.2728562 0.871624 0.562696 0.40951729 0.36187887 0.21486282 -0.073318481][-3.3780165 -2.9539638 -2.3206396 -1.5555398 -0.78214574 -0.31643963 -0.17432833 -0.068005562 -0.081511021 -0.31246614 -0.4939568 -0.53698659 -0.58959937 -0.65155935 -0.84089184][-3.8841712 -3.7541022 -3.5743282 -3.0800347 -2.45859 -2.1872065 -2.1411562 -2.007344 -1.8832741 -1.8247886 -1.8209674 -1.6977816 -1.6458757 -1.6182671 -1.703016][-4.7986908 -5.0769968 -5.2221065 -5.0657506 -4.5906711 -4.1806316 -3.9333444 -3.7642195 -3.6190131 -3.2104454 -2.8672574 -2.5179317 -2.2653768 -2.0654025 -1.9651568][-6.1129656 -6.5382938 -6.6635437 -6.6251297 -6.2192011 -5.7735577 -5.4387736 -5.135601 -4.8914375 -4.3644257 -3.9314113 -3.4017859 -2.962852 -2.5435944 -2.2643738][-6.628191 -7.0115457 -7.1274061 -7.0652404 -6.7559361 -6.4639978 -6.2535315 -6.0164347 -5.8091784 -5.3190475 -4.9005532 -4.3895383 -3.9079483 -3.3105638 -2.8080368]]...]
INFO - root - 2017-12-16 06:47:33.392131: step 39510, loss = 0.32, batch loss = 0.26 (27.7 examples/sec; 0.289 sec/batch; 23h:29m:12s remains)
INFO - root - 2017-12-16 06:47:36.252647: step 39520, loss = 0.22, batch loss = 0.16 (28.8 examples/sec; 0.278 sec/batch; 22h:37m:34s remains)
INFO - root - 2017-12-16 06:47:39.080173: step 39530, loss = 0.42, batch loss = 0.36 (28.6 examples/sec; 0.280 sec/batch; 22h:46m:55s remains)
INFO - root - 2017-12-16 06:47:41.884099: step 39540, loss = 0.27, batch loss = 0.21 (29.1 examples/sec; 0.275 sec/batch; 22h:23m:41s remains)
INFO - root - 2017-12-16 06:47:44.699154: step 39550, loss = 0.25, batch loss = 0.19 (29.3 examples/sec; 0.273 sec/batch; 22h:11m:23s remains)
INFO - root - 2017-12-16 06:47:47.526156: step 39560, loss = 0.43, batch loss = 0.37 (28.8 examples/sec; 0.278 sec/batch; 22h:37m:45s remains)
INFO - root - 2017-12-16 06:47:50.395484: step 39570, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.284 sec/batch; 23h:04m:10s remains)
INFO - root - 2017-12-16 06:47:53.207708: step 39580, loss = 0.28, batch loss = 0.22 (26.0 examples/sec; 0.308 sec/batch; 25h:01m:45s remains)
INFO - root - 2017-12-16 06:47:56.019745: step 39590, loss = 0.23, batch loss = 0.17 (29.2 examples/sec; 0.274 sec/batch; 22h:19m:30s remains)
INFO - root - 2017-12-16 06:47:58.799351: step 39600, loss = 0.31, batch loss = 0.25 (27.5 examples/sec; 0.291 sec/batch; 23h:39m:31s remains)
2017-12-16 06:47:59.267832: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.885437 -5.4216371 -5.0605531 -4.9950781 -5.1001415 -5.2952294 -5.2041311 -5.0335836 -5.1042829 -5.1473622 -5.4828238 -5.8685813 -6.3115783 -6.579484 -6.8408194][-5.9916563 -5.4140711 -4.9978266 -4.9008923 -4.8581638 -4.9295459 -4.8306665 -4.712605 -4.8843989 -4.957345 -5.3683171 -5.8085747 -6.0936928 -6.0729771 -6.30212][-5.6090927 -5.0010285 -4.4632993 -4.3117943 -4.1050763 -3.9161649 -3.7123482 -3.7941494 -4.1471839 -4.418591 -4.9832473 -5.4172034 -5.618329 -5.3426118 -5.4025025][-4.9442554 -4.051682 -3.3121738 -3.0433607 -2.8014281 -2.5263705 -2.3511481 -2.5390608 -3.0170043 -3.4968255 -4.1163111 -4.510088 -4.64725 -4.4034767 -4.5030947][-3.9596615 -2.6675754 -1.5415289 -1.0682883 -0.72794509 -0.50797224 -0.39515686 -0.73092008 -1.3784807 -2.1903923 -3.1305857 -3.741559 -3.9627314 -3.7476077 -3.944916][-3.1494775 -1.616925 -0.22160387 0.83178329 1.6985316 1.9827404 1.958456 1.328239 0.40450668 -0.86436033 -1.9768147 -2.6809347 -3.0667326 -3.0470142 -3.3015227][-2.4580679 -1.0595548 0.25850868 1.6361027 2.6659074 3.1251836 3.2888026 2.5916739 1.540863 0.1952672 -1.0057814 -2.0335827 -2.7477174 -3.00393 -3.4611368][-2.3867824 -1.0053873 0.15140152 1.4942636 2.5992236 3.1098838 3.3224249 2.8228016 2.0157442 0.67555857 -0.66507196 -1.7494171 -2.7175503 -3.3236337 -4.0524583][-3.2950382 -2.0917253 -1.1060402 0.10000849 1.111804 1.8383875 2.3399796 2.0989237 1.5335507 0.49811649 -0.61545849 -1.6925232 -2.76584 -3.666115 -4.6127257][-4.8614092 -3.7144079 -2.9720218 -2.091893 -1.2094746 -0.37485695 0.19808006 0.35229921 0.20905352 -0.49750876 -1.4006307 -2.36688 -3.3805542 -4.3566074 -5.4053555][-6.6864395 -5.7641635 -5.1570239 -4.3314548 -3.6026077 -2.7862532 -2.0124607 -1.5706024 -1.41026 -1.6774108 -2.2526307 -3.0898266 -3.990591 -4.9696255 -6.1168022][-7.9540882 -7.1136303 -6.5907955 -6.0549192 -5.6267004 -4.9480529 -4.2034764 -3.5046873 -3.0448008 -2.9187913 -3.0869484 -3.7433691 -4.5296969 -5.6014438 -6.6807866][-8.2703419 -7.5962543 -7.3132429 -6.9317236 -6.6882267 -6.2601752 -5.7521486 -5.0638742 -4.5929031 -4.1998076 -4.0247397 -4.4780655 -5.0343995 -6.0154777 -6.956811][-8.1423092 -7.2861185 -6.8506794 -6.7567167 -6.7507935 -6.5624352 -6.3383589 -5.8782115 -5.5609055 -5.1698532 -4.8845506 -5.1221066 -5.4562826 -6.1465645 -6.8213863][-7.0280342 -6.1951809 -5.7530785 -5.823 -6.0233159 -6.1712008 -6.2734814 -6.0840306 -5.9889293 -5.642755 -5.3908296 -5.518723 -5.6361651 -5.9289255 -6.2591429]]...]
INFO - root - 2017-12-16 06:48:02.062203: step 39610, loss = 0.24, batch loss = 0.18 (27.0 examples/sec; 0.297 sec/batch; 24h:07m:35s remains)
INFO - root - 2017-12-16 06:48:04.873791: step 39620, loss = 0.21, batch loss = 0.15 (28.9 examples/sec; 0.276 sec/batch; 22h:29m:09s remains)
INFO - root - 2017-12-16 06:48:07.683542: step 39630, loss = 0.25, batch loss = 0.19 (29.0 examples/sec; 0.275 sec/batch; 22h:24m:30s remains)
INFO - root - 2017-12-16 06:48:10.564962: step 39640, loss = 0.36, batch loss = 0.30 (26.6 examples/sec; 0.301 sec/batch; 24h:29m:14s remains)
INFO - root - 2017-12-16 06:48:13.414749: step 39650, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 23h:08m:43s remains)
INFO - root - 2017-12-16 06:48:16.262073: step 39660, loss = 0.31, batch loss = 0.25 (29.1 examples/sec; 0.275 sec/batch; 22h:23m:20s remains)
INFO - root - 2017-12-16 06:48:19.060441: step 39670, loss = 0.39, batch loss = 0.33 (26.9 examples/sec; 0.298 sec/batch; 24h:12m:48s remains)
INFO - root - 2017-12-16 06:48:21.931041: step 39680, loss = 0.36, batch loss = 0.30 (27.4 examples/sec; 0.292 sec/batch; 23h:43m:33s remains)
INFO - root - 2017-12-16 06:48:24.770176: step 39690, loss = 0.25, batch loss = 0.19 (29.0 examples/sec; 0.276 sec/batch; 22h:26m:07s remains)
INFO - root - 2017-12-16 06:48:27.597556: step 39700, loss = 0.37, batch loss = 0.31 (29.4 examples/sec; 0.272 sec/batch; 22h:05m:52s remains)
2017-12-16 06:48:28.065448: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.132206 -3.1960709 -3.2693162 -3.4308274 -3.8570609 -4.3437271 -4.8425994 -5.2883668 -5.6072655 -5.6142354 -5.2493515 -4.7550135 -4.1564832 -3.6621647 -3.2973342][-3.1796165 -3.3458238 -3.5058234 -3.7350755 -3.9298203 -4.2918892 -4.9275255 -5.5523334 -5.8788853 -5.9739757 -5.7759857 -5.30626 -4.6552205 -3.9598923 -3.4461467][-3.3890405 -3.4875388 -3.5873613 -3.701829 -3.8847036 -4.1169438 -4.589756 -5.0855341 -5.5305929 -5.8298655 -5.7709532 -5.6528454 -5.3041096 -4.6547861 -3.9522424][-3.4219627 -3.4911475 -3.4886332 -3.4482582 -3.52846 -3.4384956 -3.4625776 -3.6065168 -3.8574882 -4.3119116 -4.6856508 -5.226788 -5.3867378 -5.0052443 -4.3962469][-3.1656861 -3.1548138 -3.0285587 -2.7430887 -2.220489 -1.6916301 -1.1510651 -0.89012814 -0.96633029 -1.4151208 -2.2926176 -3.6284142 -4.4770784 -4.7117414 -4.1795554][-2.5768721 -2.3264146 -2.0553479 -1.5416338 -0.74996161 0.41474104 1.628746 2.4763246 2.7477908 1.9304643 0.36782789 -1.6515877 -3.0732365 -3.7742538 -3.5217607][-2.3179474 -1.8162661 -1.4974735 -0.69598389 0.3750701 2.0720782 3.9876957 5.2504187 5.6515255 4.6735449 2.7793584 0.24614525 -1.8033895 -2.8893781 -2.9321723][-2.1857419 -1.8263505 -1.4748838 -0.6461916 0.6890769 2.7590537 4.9598255 6.609292 7.1925716 6.1289797 3.9882574 1.3862777 -0.80185485 -2.0714486 -2.3843069][-2.2987738 -2.1479671 -2.0551631 -1.5832765 -0.47084904 1.6745176 3.8563538 5.6841679 6.2471294 5.3055658 3.4902425 1.1697974 -0.71902585 -1.9524505 -2.3645134][-2.8498697 -2.804862 -3.1652322 -3.0882497 -2.3829265 -0.84697032 0.96618557 2.6792674 3.2698073 2.6009874 1.3460646 -0.37214613 -1.8068612 -2.7219336 -2.9753442][-3.5043743 -3.7325695 -4.200624 -4.3459697 -4.2064967 -3.2687371 -1.8326771 -0.54127693 0.011223316 -0.028641224 -0.85375571 -1.9932857 -2.8850174 -3.4418006 -3.5287147][-3.6949692 -4.0087166 -4.59643 -5.016305 -5.1438823 -4.7043533 -4.0266881 -3.1298099 -2.4352255 -2.2531898 -2.6170454 -3.1394553 -3.6239126 -3.8871083 -3.8241711][-3.5252674 -3.7283401 -4.1031556 -4.5598516 -5.0452003 -5.0308385 -4.7137837 -4.1526117 -3.7013009 -3.4448776 -3.3549221 -3.5618858 -3.7436726 -3.8780091 -3.7263014][-3.1457319 -3.1508334 -3.3318493 -3.6776388 -4.0644493 -4.2057948 -4.2800283 -3.9889884 -3.6071854 -3.2962928 -3.1541808 -3.2090206 -3.3088915 -3.3590648 -3.1778064][-2.733012 -2.5325687 -2.5404222 -2.666163 -2.9668293 -3.1722941 -3.2806959 -3.2376008 -3.0820498 -2.8350713 -2.6916625 -2.6307745 -2.626085 -2.7218785 -2.6814184]]...]
INFO - root - 2017-12-16 06:48:30.904617: step 39710, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.285 sec/batch; 23h:08m:36s remains)
INFO - root - 2017-12-16 06:48:33.747921: step 39720, loss = 0.28, batch loss = 0.23 (28.6 examples/sec; 0.279 sec/batch; 22h:42m:45s remains)
INFO - root - 2017-12-16 06:48:36.592748: step 39730, loss = 0.35, batch loss = 0.29 (28.8 examples/sec; 0.278 sec/batch; 22h:36m:59s remains)
INFO - root - 2017-12-16 06:48:39.418788: step 39740, loss = 0.25, batch loss = 0.20 (28.7 examples/sec; 0.279 sec/batch; 22h:41m:27s remains)
INFO - root - 2017-12-16 06:48:42.257191: step 39750, loss = 0.26, batch loss = 0.20 (27.4 examples/sec; 0.292 sec/batch; 23h:46m:10s remains)
INFO - root - 2017-12-16 06:48:45.076486: step 39760, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.285 sec/batch; 23h:12m:00s remains)
INFO - root - 2017-12-16 06:48:47.925403: step 39770, loss = 0.22, batch loss = 0.17 (28.3 examples/sec; 0.283 sec/batch; 22h:58m:23s remains)
INFO - root - 2017-12-16 06:48:50.730682: step 39780, loss = 0.27, batch loss = 0.21 (29.5 examples/sec; 0.271 sec/batch; 22h:03m:56s remains)
INFO - root - 2017-12-16 06:48:53.537548: step 39790, loss = 0.25, batch loss = 0.19 (28.9 examples/sec; 0.277 sec/batch; 22h:32m:44s remains)
INFO - root - 2017-12-16 06:48:56.378132: step 39800, loss = 0.23, batch loss = 0.17 (27.3 examples/sec; 0.293 sec/batch; 23h:49m:29s remains)
2017-12-16 06:48:56.840297: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8424258 -4.7173958 -5.7490339 -6.3514447 -6.4101639 -6.3219852 -6.2730923 -6.2114983 -6.2163677 -6.1677771 -6.1482253 -6.1875744 -6.2733393 -6.1496463 -5.72517][-3.3041365 -4.1485715 -5.0313072 -5.4779186 -5.4106731 -5.2044 -4.968925 -5.07968 -5.3152766 -5.5716243 -5.8866124 -6.0686355 -6.1187587 -5.8866186 -5.3524203][-2.8227262 -3.6767459 -4.4673443 -4.9284782 -4.6313915 -4.2324514 -4.0084033 -4.017868 -4.2505822 -4.9228468 -5.680748 -6.1903372 -6.5337791 -6.4134927 -5.6958246][-1.6166511 -2.7840958 -3.9339793 -4.1498437 -3.4023552 -2.6142764 -1.9954872 -2.0045536 -2.4844742 -3.2897162 -4.32041 -5.3362122 -6.0748224 -6.0904155 -5.5728712][-1.3127649 -2.3279319 -2.9520688 -3.0452027 -2.1739833 -0.88544917 0.40716219 0.91008615 0.56254911 -0.4778018 -1.9721105 -3.4230099 -4.5234356 -5.27381 -5.3577485][-1.4564538 -2.2041688 -2.6106591 -2.2335026 -0.7917161 0.89475679 2.4413366 3.2756996 3.115088 1.8410707 -0.090288162 -1.8717883 -3.4088345 -4.2863631 -4.5923753][-1.5222659 -1.9924703 -2.2783432 -1.7340739 -0.073724747 2.1019711 4.0745659 5.2676249 5.2724094 3.8532877 1.574861 -0.68953276 -2.4877555 -3.6855009 -4.2764373][-1.7844372 -1.9465697 -2.0893524 -1.292552 0.38186169 2.6329403 4.9270449 6.1830578 5.96251 4.6451454 2.5664678 0.27937746 -1.6463087 -3.0282311 -3.7199345][-2.5081816 -2.2901404 -1.9720728 -1.4229958 -0.093945026 2.0337057 4.2554808 5.6341934 5.78539 4.3759289 1.9622087 -0.35102463 -2.1617024 -3.4184022 -3.9002621][-2.9677978 -2.9809585 -3.0883195 -2.380471 -1.1222343 0.42695379 1.9271083 3.1591687 3.4065304 2.3341112 0.45833254 -1.644886 -3.3928092 -4.4988489 -4.915246][-3.4439988 -3.442153 -3.444689 -3.3107133 -2.792254 -1.4871931 0.025615215 0.536571 0.30431175 -0.39214516 -1.8336902 -3.6718719 -5.0543 -5.6910553 -5.7193213][-3.4251108 -3.5475941 -3.9357486 -4.1470447 -3.9980145 -3.4918876 -2.8829885 -2.0855715 -1.6403403 -2.3373055 -3.5548975 -4.7928071 -5.8196697 -6.5038548 -6.5965309][-3.2216496 -3.3944373 -4.2920675 -5.1711135 -5.5437 -5.1242929 -4.4314804 -3.9731369 -3.6736319 -3.715724 -4.2735696 -5.2015204 -6.0757942 -6.6454954 -6.6754036][-2.8909233 -3.1361642 -3.9211779 -4.9698234 -5.7894034 -5.8394289 -5.4884739 -4.9486752 -4.3901496 -4.0991993 -4.2083569 -4.8436089 -5.5679531 -5.9186826 -6.0026755][-3.3609068 -3.3417935 -4.1046243 -5.2119393 -6.0330734 -6.244297 -5.9538212 -5.2957125 -4.580617 -4.1530552 -4.0713854 -4.0678248 -4.2573223 -4.6901984 -4.9361296]]...]
INFO - root - 2017-12-16 06:48:59.649314: step 39810, loss = 0.23, batch loss = 0.17 (29.5 examples/sec; 0.272 sec/batch; 22h:04m:32s remains)
INFO - root - 2017-12-16 06:49:02.483666: step 39820, loss = 0.24, batch loss = 0.18 (29.1 examples/sec; 0.275 sec/batch; 22h:22m:13s remains)
INFO - root - 2017-12-16 06:49:05.311612: step 39830, loss = 0.43, batch loss = 0.37 (28.0 examples/sec; 0.285 sec/batch; 23h:12m:19s remains)
INFO - root - 2017-12-16 06:49:08.110482: step 39840, loss = 0.34, batch loss = 0.28 (27.5 examples/sec; 0.291 sec/batch; 23h:37m:29s remains)
INFO - root - 2017-12-16 06:49:10.976905: step 39850, loss = 0.31, batch loss = 0.25 (25.8 examples/sec; 0.310 sec/batch; 25h:10m:18s remains)
INFO - root - 2017-12-16 06:49:13.806244: step 39860, loss = 0.36, batch loss = 0.30 (27.9 examples/sec; 0.287 sec/batch; 23h:17m:45s remains)
INFO - root - 2017-12-16 06:49:16.653064: step 39870, loss = 0.23, batch loss = 0.17 (29.2 examples/sec; 0.274 sec/batch; 22h:17m:41s remains)
INFO - root - 2017-12-16 06:49:19.481799: step 39880, loss = 0.41, batch loss = 0.35 (28.9 examples/sec; 0.277 sec/batch; 22h:29m:05s remains)
INFO - root - 2017-12-16 06:49:22.274357: step 39890, loss = 0.34, batch loss = 0.29 (28.4 examples/sec; 0.281 sec/batch; 22h:51m:42s remains)
INFO - root - 2017-12-16 06:49:25.098555: step 39900, loss = 0.24, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 22h:16m:12s remains)
2017-12-16 06:49:25.561773: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8660111 -3.1816945 -3.4938226 -3.9329743 -4.381649 -4.76285 -5.1067514 -5.6394062 -6.1032934 -6.3063669 -6.2248592 -6.0304489 -5.7663903 -5.225688 -4.9424849][-2.5999012 -2.8484302 -3.1626787 -3.6447473 -4.0752087 -4.7228961 -5.2825661 -5.8009348 -6.2819405 -6.6591353 -6.8976355 -6.7114239 -6.3744421 -5.75418 -5.1050529][-2.2591121 -2.3206127 -2.5816398 -3.0722318 -3.4934807 -4.0919595 -4.526588 -5.1150646 -5.6342 -6.0873461 -6.5433159 -6.7106881 -6.5783176 -5.7567244 -4.9725127][-1.7761936 -1.7572649 -1.8986077 -2.2941704 -2.7322483 -3.1317451 -3.3452377 -3.6092298 -3.896771 -4.481874 -5.1935735 -5.8001051 -6.0320616 -5.4427938 -4.5497732][-1.3056591 -1.0980484 -1.1662846 -1.5113864 -1.7796969 -1.7402086 -1.4372785 -1.3028805 -1.6073751 -2.202153 -3.1390076 -4.2267871 -4.9412031 -4.8536253 -4.1973481][-1.1405988 -0.7986424 -0.697855 -0.72537565 -0.50735474 -0.11894655 0.54464626 1.2487831 1.2398496 0.31452894 -1.3405867 -2.8721998 -3.8499608 -4.0793743 -3.7883017][-1.1141515 -0.78084779 -0.67288256 -0.48137331 0.025642395 0.9696312 2.3127036 3.2271161 3.1798806 2.2644506 0.6539588 -1.2978659 -2.8931086 -3.3997846 -3.1756406][-1.0354331 -0.82972789 -0.88037324 -0.56312609 0.19078159 1.4924746 3.0705047 4.2003841 4.5058069 3.5755882 1.8398504 0.027143955 -1.5624192 -2.6028333 -2.873064][-1.6387579 -1.2609649 -1.0504982 -0.87790632 -0.3234396 1.0127673 2.575891 3.840621 4.2707176 3.4994087 1.9636121 0.14774609 -1.4357028 -2.4562776 -2.6292768][-2.1128323 -2.1923079 -2.2479644 -2.0031722 -1.3633895 -0.46500349 0.525743 1.6951766 2.2803006 1.9591165 0.90834427 -0.56116676 -1.8630726 -2.8098011 -3.0177214][-3.2448449 -3.0380611 -3.0375772 -3.2178683 -3.0105269 -2.3713651 -1.5415022 -0.74701977 -0.32409811 -0.32641315 -0.72094727 -1.5369773 -2.4389644 -3.1315994 -3.0318182][-4.3613644 -4.2520051 -4.3013115 -4.3208761 -4.26247 -4.20225 -3.8628962 -3.2230701 -2.6735313 -2.4213464 -2.5766311 -3.0270102 -3.4172621 -3.6684587 -3.3250732][-5.34632 -5.4164972 -5.5645037 -5.8140726 -5.9393191 -5.9127293 -5.7220092 -5.4392514 -5.0933237 -4.6813254 -4.3824506 -4.4341459 -4.5398493 -4.5637217 -4.182816][-6.3670821 -6.402072 -6.4244404 -6.55223 -6.7806492 -6.9046907 -6.9270096 -6.7841864 -6.5239115 -6.1253934 -5.7095022 -5.5546942 -5.4170561 -5.1052604 -4.5434823][-6.5093107 -6.5044041 -6.4695916 -6.6385651 -6.8287029 -6.8533258 -6.9045677 -6.9235053 -6.8656693 -6.5302267 -6.1381865 -5.847959 -5.6348667 -5.3456388 -4.8397932]]...]
INFO - root - 2017-12-16 06:49:28.379098: step 39910, loss = 0.24, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 22h:58m:15s remains)
INFO - root - 2017-12-16 06:49:31.250862: step 39920, loss = 0.22, batch loss = 0.16 (28.7 examples/sec; 0.279 sec/batch; 22h:38m:05s remains)
INFO - root - 2017-12-16 06:49:34.071322: step 39930, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 22h:59m:30s remains)
INFO - root - 2017-12-16 06:49:36.925795: step 39940, loss = 0.38, batch loss = 0.33 (28.8 examples/sec; 0.278 sec/batch; 22h:35m:36s remains)
INFO - root - 2017-12-16 06:49:39.728244: step 39950, loss = 0.26, batch loss = 0.20 (29.8 examples/sec; 0.269 sec/batch; 21h:50m:28s remains)
INFO - root - 2017-12-16 06:49:42.536106: step 39960, loss = 0.30, batch loss = 0.24 (29.2 examples/sec; 0.274 sec/batch; 22h:15m:12s remains)
INFO - root - 2017-12-16 06:49:45.370723: step 39970, loss = 0.45, batch loss = 0.39 (27.7 examples/sec; 0.289 sec/batch; 23h:26m:35s remains)
INFO - root - 2017-12-16 06:49:48.231316: step 39980, loss = 0.23, batch loss = 0.17 (26.8 examples/sec; 0.299 sec/batch; 24h:15m:38s remains)
INFO - root - 2017-12-16 06:49:51.050277: step 39990, loss = 0.24, batch loss = 0.19 (29.5 examples/sec; 0.271 sec/batch; 22h:00m:51s remains)
INFO - root - 2017-12-16 06:49:53.896250: step 40000, loss = 0.28, batch loss = 0.23 (27.2 examples/sec; 0.294 sec/batch; 23h:52m:59s remains)
2017-12-16 06:49:54.377288: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9015703 -2.5672677 -2.90037 -3.5174174 -4.4298992 -5.3741965 -6.2374043 -6.6058903 -6.771452 -6.3792963 -5.5695014 -4.5880423 -3.5316434 -2.8571353 -2.6846888][-1.7674224 -2.3994489 -3.0447397 -3.597014 -4.2451658 -4.9255142 -5.6471148 -5.7220497 -5.3142195 -5.0735693 -4.5961108 -3.5564342 -2.3688624 -1.7906022 -1.7290375][-2.2958305 -3.0358353 -3.5914226 -3.8972764 -3.9398787 -4.0022616 -3.9504805 -3.8524683 -3.5862076 -3.3686156 -3.1458154 -2.6767497 -1.7527916 -0.8957181 -0.67070341][-3.2118535 -4.0652351 -4.7533565 -4.8032713 -4.0083313 -3.0072465 -2.2167943 -1.7548442 -1.701396 -1.6663597 -1.5724442 -1.415797 -1.0338523 -0.4468801 -0.21251678][-3.9664969 -4.6665578 -5.0002041 -4.6428142 -3.4337134 -1.8285022 -0.67286181 0.17063761 0.19397736 -0.3542819 -0.97069025 -1.1584656 -0.96975708 -0.75062966 -0.73374939][-4.7789125 -5.1757526 -5.2148252 -4.2467513 -2.2330959 -0.10013485 1.4738526 2.0948968 1.72364 0.60060549 -0.52780318 -1.2598369 -1.4965093 -1.5159659 -1.4168231][-4.4046512 -4.8400426 -4.5755382 -3.3725142 -1.3753655 0.83531618 2.4706235 3.0386281 2.3262639 0.83929157 -0.48104405 -1.345516 -1.8532014 -1.9336159 -1.7384334][-4.025629 -4.3287649 -3.9031239 -2.7461534 -1.0504959 0.8695302 2.1729803 2.5681825 1.8920603 0.49553919 -0.95758271 -1.8237708 -2.2882552 -2.3722475 -2.2344084][-2.9880357 -3.5004902 -3.3098054 -2.3898187 -1.064842 0.31363821 1.311944 1.6103749 1.1093917 -0.060087204 -1.4068954 -2.3259218 -2.8893805 -3.1121707 -2.9476762][-2.1941338 -2.8849998 -3.1507175 -2.7004566 -1.692806 -0.692405 0.036152363 0.31959867 -0.0060839653 -0.89676976 -1.9192116 -2.8159361 -3.3186116 -3.5018439 -3.4883301][-2.2188606 -2.92737 -3.2624571 -3.1398363 -2.5254161 -1.675343 -1.0491569 -0.84594655 -1.055043 -1.6602538 -2.4909124 -3.1714373 -3.5441961 -3.8243015 -4.0569363][-2.4141951 -3.0283518 -3.6106272 -3.6239367 -3.2520761 -2.6135836 -2.1301873 -1.922977 -2.0676625 -2.4502761 -3.1328769 -3.7868862 -4.0248342 -3.9747503 -4.0429044][-2.4732585 -3.1082921 -3.5946777 -3.7621987 -3.5044515 -3.1465869 -2.6711783 -2.6141353 -3.0101852 -3.1707468 -3.3968062 -3.7389495 -3.68538 -3.4411187 -3.2290924][-2.1779492 -2.6568582 -2.9077277 -3.1035125 -3.1399784 -2.9538569 -2.7075868 -2.8117948 -3.1993592 -3.3820529 -3.3522005 -3.2899709 -2.913851 -2.5372992 -2.4011464][-2.1120338 -2.391891 -2.78897 -2.7406631 -2.7725949 -2.8287828 -2.8676188 -2.9840646 -3.291085 -3.413662 -3.3108416 -3.1628907 -2.8033509 -2.3804998 -2.0679843]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 06:49:57.646215: step 40010, loss = 0.27, batch loss = 0.21 (28.8 examples/sec; 0.278 sec/batch; 22h:33m:52s remains)
INFO - root - 2017-12-16 06:50:00.473094: step 40020, loss = 0.21, batch loss = 0.15 (26.7 examples/sec; 0.300 sec/batch; 24h:20m:06s remains)
INFO - root - 2017-12-16 06:50:03.353324: step 40030, loss = 0.31, batch loss = 0.25 (28.4 examples/sec; 0.282 sec/batch; 22h:52m:52s remains)
INFO - root - 2017-12-16 06:50:06.146609: step 40040, loss = 0.29, batch loss = 0.23 (28.5 examples/sec; 0.281 sec/batch; 22h:48m:00s remains)
INFO - root - 2017-12-16 06:50:09.000577: step 40050, loss = 0.27, batch loss = 0.21 (27.7 examples/sec; 0.289 sec/batch; 23h:30m:01s remains)
INFO - root - 2017-12-16 06:50:11.823178: step 40060, loss = 0.26, batch loss = 0.20 (28.5 examples/sec; 0.280 sec/batch; 22h:45m:56s remains)
INFO - root - 2017-12-16 06:50:14.617806: step 40070, loss = 0.32, batch loss = 0.26 (29.4 examples/sec; 0.272 sec/batch; 22h:07m:44s remains)
INFO - root - 2017-12-16 06:50:17.446451: step 40080, loss = 0.21, batch loss = 0.16 (28.5 examples/sec; 0.281 sec/batch; 22h:50m:16s remains)
INFO - root - 2017-12-16 06:50:20.332838: step 40090, loss = 0.30, batch loss = 0.24 (28.4 examples/sec; 0.282 sec/batch; 22h:52m:28s remains)
INFO - root - 2017-12-16 06:50:23.180843: step 40100, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.286 sec/batch; 23h:11m:56s remains)
2017-12-16 06:50:23.644515: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.861378 -2.9910665 -3.0752633 -3.138324 -3.1412015 -2.9641168 -3.0053453 -3.3058589 -3.9801283 -4.6410594 -5.1628737 -5.442235 -5.4061394 -5.0201221 -4.2926936][-3.323103 -3.4532232 -3.4484699 -3.3172517 -3.0987048 -2.8760273 -2.8001482 -3.1054676 -3.8446412 -4.7236938 -5.5983672 -5.9570293 -6.0576911 -5.6715689 -4.9362736][-3.9334519 -3.9689329 -3.7545452 -3.3366303 -2.8246799 -2.4190898 -2.1581385 -2.4462376 -3.2963066 -4.4217067 -5.6124558 -6.309051 -6.5208945 -6.1534357 -5.4828653][-4.4401817 -4.2962384 -3.7724698 -2.9854884 -2.1017988 -1.3063033 -0.84726405 -1.2269208 -2.2169344 -3.4860239 -4.8881869 -5.8780823 -6.5051103 -6.4534664 -5.9295249][-4.6793847 -4.3729424 -3.4808359 -2.2161055 -0.89375806 0.2393651 0.90069437 0.705668 -0.27195549 -1.9691756 -3.8654504 -5.3045783 -6.1762176 -6.478569 -6.1623497][-4.5507522 -3.9970937 -2.9556561 -1.2375247 0.54993343 1.9041281 2.7374244 2.6100826 1.563241 -0.3023448 -2.4105909 -4.2669 -5.619493 -6.3317828 -6.2283964][-4.3000116 -3.4825425 -2.2498569 -0.47331953 1.6326075 3.3632069 4.4237928 4.2735939 3.0908384 1.1022239 -1.3063569 -3.3808866 -4.9733796 -5.851964 -5.9453716][-3.9545212 -3.3096142 -2.3007905 -0.48425698 1.6186075 3.4562511 4.5814915 4.5605116 3.4449949 1.3941154 -1.0837805 -3.2831545 -4.9629908 -5.9649138 -5.9814773][-3.9349852 -3.4215431 -2.6301184 -1.2231574 0.23101759 2.0864177 3.3116202 3.274107 2.3044491 0.46137333 -1.7309692 -3.9153674 -5.4642124 -6.2666969 -6.2596869][-3.9795318 -3.9057622 -3.7820117 -2.8855729 -1.4434605 -0.024564743 0.80855656 0.845171 0.44308519 -0.89544368 -2.8651686 -4.6671925 -5.9416618 -6.5271425 -6.5229549][-4.4629521 -4.4900455 -4.3434339 -4.088 -3.4102197 -2.2414317 -1.1420984 -0.86635041 -1.4010134 -2.320055 -3.4439285 -4.8867126 -5.975594 -6.4822073 -6.474741][-4.3564363 -4.3449845 -4.438107 -4.3523808 -3.9886627 -3.2119555 -2.5450034 -2.347254 -2.3383427 -2.901439 -3.8128262 -5.0310225 -5.8188562 -6.1551852 -6.2403922][-3.4408846 -3.2280197 -3.241137 -3.3023276 -3.2833605 -3.1269889 -2.7741876 -2.6841702 -2.7029648 -3.0414195 -3.6655681 -4.4943576 -5.028801 -5.4055858 -5.7483525][-2.7666912 -2.229363 -1.9102159 -1.8386834 -1.9910247 -2.1629949 -2.2407842 -2.4130173 -2.4477336 -2.4372289 -2.5986485 -3.2257328 -3.941385 -4.4695368 -4.7763643][-2.0138412 -0.99172878 -0.37630749 -0.20002651 -0.3131814 -0.5437727 -0.77725458 -0.94938731 -1.0639548 -1.2714469 -1.5165975 -1.9090774 -2.4854178 -3.2224331 -3.9205651]]...]
INFO - root - 2017-12-16 06:50:26.446856: step 40110, loss = 0.25, batch loss = 0.19 (29.5 examples/sec; 0.271 sec/batch; 22h:02m:59s remains)
INFO - root - 2017-12-16 06:50:29.304484: step 40120, loss = 0.24, batch loss = 0.19 (28.2 examples/sec; 0.283 sec/batch; 23h:00m:34s remains)
INFO - root - 2017-12-16 06:50:32.139336: step 40130, loss = 0.30, batch loss = 0.24 (29.5 examples/sec; 0.272 sec/batch; 22h:03m:01s remains)
INFO - root - 2017-12-16 06:50:34.945582: step 40140, loss = 0.47, batch loss = 0.41 (29.5 examples/sec; 0.271 sec/batch; 21h:59m:33s remains)
INFO - root - 2017-12-16 06:50:37.787120: step 40150, loss = 0.29, batch loss = 0.23 (28.6 examples/sec; 0.280 sec/batch; 22h:42m:42s remains)
INFO - root - 2017-12-16 06:50:40.616015: step 40160, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.281 sec/batch; 22h:47m:17s remains)
INFO - root - 2017-12-16 06:50:43.418652: step 40170, loss = 0.29, batch loss = 0.23 (29.3 examples/sec; 0.273 sec/batch; 22h:09m:51s remains)
INFO - root - 2017-12-16 06:50:46.244559: step 40180, loss = 0.22, batch loss = 0.16 (28.2 examples/sec; 0.284 sec/batch; 23h:01m:44s remains)
INFO - root - 2017-12-16 06:50:49.072705: step 40190, loss = 0.38, batch loss = 0.33 (28.6 examples/sec; 0.279 sec/batch; 22h:41m:07s remains)
INFO - root - 2017-12-16 06:50:51.885615: step 40200, loss = 0.20, batch loss = 0.15 (27.2 examples/sec; 0.294 sec/batch; 23h:50m:48s remains)
2017-12-16 06:50:52.345507: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4800639 -2.7540982 -3.0098972 -3.4262567 -3.8547578 -4.2616205 -4.6237755 -4.8880491 -5.0757427 -4.9527593 -4.7224159 -4.5926375 -4.40164 -4.0838847 -3.7658005][-2.8526721 -3.228374 -3.648237 -4.0516114 -4.329031 -4.6745968 -4.9939475 -5.0840364 -5.2217054 -5.4439754 -5.5844707 -5.4788814 -5.2608037 -4.7735925 -4.3548765][-3.6724944 -4.2512264 -4.4794364 -4.5483141 -4.6388969 -4.6094861 -4.5629053 -4.7309928 -5.011694 -5.27969 -5.65246 -6.0842009 -5.9989085 -5.3135567 -4.5007448][-3.8160336 -4.7725439 -5.3238955 -5.1384792 -4.3810596 -3.6461883 -3.1955118 -3.0261917 -3.4989269 -4.4014659 -5.3926539 -6.2075162 -6.4258766 -5.7895441 -4.5941958][-4.2758913 -4.9287457 -5.116672 -4.6883016 -3.4925451 -1.8733077 -0.64641285 -0.31238604 -1.0079579 -2.5179224 -4.389298 -5.77267 -6.0874271 -5.6369438 -4.7931094][-4.5647159 -5.1776938 -5.1336 -4.0299373 -1.9726028 0.4509635 2.3235011 2.8803115 1.8603725 -0.19403553 -2.6538901 -4.7060595 -5.5324612 -5.2882881 -4.3600793][-4.1502676 -4.64138 -4.5280995 -3.1891258 -0.90081811 1.7864151 4.1388988 5.0987453 4.1019716 1.5660596 -1.1206608 -3.2467339 -4.38655 -4.5153437 -3.8430171][-3.6715829 -3.9878416 -3.8761621 -2.3713944 -0.031899452 2.8777871 5.1988735 5.737711 4.6092691 2.0571179 -0.67647409 -2.6465211 -3.3370843 -3.3690681 -2.9272094][-3.3580968 -3.5245166 -2.9614773 -1.8769696 0.16622162 2.897891 4.7281408 5.1545916 3.9006586 1.5939279 -0.6548512 -2.4860554 -3.1905162 -2.8752708 -2.3141451][-3.2955098 -3.6919494 -3.7290502 -2.5158844 -0.58544755 1.3274765 2.6975732 2.782732 1.5255857 -0.21105099 -1.5251353 -2.3866045 -2.5925691 -2.4092724 -1.8344798][-4.0947475 -4.4101062 -4.5586658 -3.9682472 -2.6107473 -1.0913317 -0.1599617 -0.34677553 -1.5158508 -2.8053558 -3.4701958 -3.2551479 -2.4712558 -1.9039197 -1.2273445][-4.6307864 -5.3187561 -5.7018919 -5.1992836 -4.1035671 -3.2441669 -2.9154255 -3.2662046 -4.023881 -4.7602935 -5.0437713 -4.329895 -3.0890226 -2.2011893 -1.647965][-5.2425003 -6.2022266 -6.7165594 -6.4568377 -5.8919983 -5.2785845 -4.7938533 -4.9141088 -5.5985584 -5.9521403 -5.6348691 -4.710217 -3.7954521 -3.1142673 -2.3882313][-6.1395469 -6.964334 -7.3805847 -7.4552488 -7.06563 -6.6235166 -6.3721085 -6.1777539 -6.1374464 -6.1522818 -6.0061226 -5.216228 -4.3648024 -3.8400097 -3.3439741][-6.0194149 -6.8534584 -7.3157763 -7.4822254 -7.2848778 -7.0876265 -6.778038 -6.5314627 -6.4689045 -6.1414862 -5.8192978 -5.3965635 -5.0503359 -4.7897325 -4.343739]]...]
INFO - root - 2017-12-16 06:50:55.199000: step 40210, loss = 0.24, batch loss = 0.18 (28.3 examples/sec; 0.282 sec/batch; 22h:55m:01s remains)
INFO - root - 2017-12-16 06:50:58.049470: step 40220, loss = 0.26, batch loss = 0.20 (27.8 examples/sec; 0.288 sec/batch; 23h:20m:50s remains)
INFO - root - 2017-12-16 06:51:00.815525: step 40230, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.282 sec/batch; 22h:52m:59s remains)
INFO - root - 2017-12-16 06:51:03.665970: step 40240, loss = 0.25, batch loss = 0.19 (28.8 examples/sec; 0.278 sec/batch; 22h:32m:24s remains)
INFO - root - 2017-12-16 06:51:06.511398: step 40250, loss = 0.28, batch loss = 0.22 (29.1 examples/sec; 0.275 sec/batch; 22h:20m:33s remains)
INFO - root - 2017-12-16 06:51:09.310025: step 40260, loss = 0.26, batch loss = 0.21 (29.5 examples/sec; 0.271 sec/batch; 21h:58m:59s remains)
INFO - root - 2017-12-16 06:51:12.144318: step 40270, loss = 0.23, batch loss = 0.17 (28.1 examples/sec; 0.285 sec/batch; 23h:07m:15s remains)
INFO - root - 2017-12-16 06:51:14.992824: step 40280, loss = 0.39, batch loss = 0.33 (27.0 examples/sec; 0.296 sec/batch; 24h:01m:14s remains)
INFO - root - 2017-12-16 06:51:17.813936: step 40290, loss = 0.24, batch loss = 0.18 (29.3 examples/sec; 0.273 sec/batch; 22h:11m:51s remains)
INFO - root - 2017-12-16 06:51:20.646585: step 40300, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.284 sec/batch; 23h:01m:12s remains)
2017-12-16 06:51:21.133800: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2112894 -6.0612926 -6.0704327 -6.7162848 -7.7884521 -8.6119461 -9.072588 -9.0298023 -8.8768883 -8.2010822 -7.2110643 -6.1268373 -5.2821765 -4.6048265 -4.1442523][-6.3450642 -6.2440224 -6.3511596 -6.718914 -7.3225245 -8.3405952 -9.0024757 -9.1528053 -9.1932106 -8.7621393 -8.1827488 -7.2395997 -6.3736167 -5.6620035 -5.1490688][-6.0854559 -5.787416 -5.4553227 -5.7390924 -5.9249363 -6.4804254 -7.0321321 -7.686367 -8.1805124 -8.424572 -8.4465933 -7.8166981 -7.1346931 -6.5183878 -5.9984927][-5.2021589 -4.5319018 -3.9448173 -3.5630021 -3.3848684 -3.7235134 -4.1172423 -4.8949528 -5.8074684 -6.8099003 -7.5198078 -7.6036811 -7.3961897 -6.6874943 -6.1214514][-4.5890536 -3.2859 -1.8834379 -0.84196353 -0.052672863 -0.13879919 -0.5638485 -1.3999138 -2.711802 -4.4807134 -6.0335436 -6.6899424 -6.8167295 -6.2903934 -5.8747892][-3.8292022 -2.3824537 -0.67106128 1.2385378 3.0236096 3.4988031 3.3096528 2.1744752 0.41563749 -1.9911408 -4.0780072 -5.4572215 -6.0846739 -5.7115026 -5.1157551][-3.7720222 -1.9352829 0.37602854 2.6046686 4.5548592 5.6600447 6.0432596 4.8371096 2.7027726 -0.056307793 -2.5779843 -4.3720264 -5.0132976 -4.8563504 -4.3641124][-3.3477581 -1.7550094 0.092372894 2.5039587 4.670887 5.88126 6.2769327 5.2731915 3.4675951 0.76004982 -1.6795764 -3.4295847 -4.30492 -4.2564006 -3.7824798][-4.4789119 -2.8378613 -0.87806296 0.89238453 2.3160229 3.7909021 4.7345877 3.995903 2.418611 0.08244133 -1.9644241 -3.4432406 -4.2348213 -4.0907426 -3.6768229][-5.4332342 -4.2199912 -3.2255437 -1.8069022 -0.46223187 0.731544 1.2368398 1.118166 0.36928797 -1.3111804 -2.9507585 -4.0492249 -4.3726363 -4.1130362 -3.7344704][-6.3992777 -5.5893884 -4.9286361 -4.28515 -3.7880247 -2.8279395 -2.09408 -1.8615839 -2.387495 -3.2082634 -4.0369782 -4.7094755 -4.8124862 -4.3509293 -3.8337524][-7.0722485 -7.0772462 -7.1005392 -6.8145666 -6.3183484 -5.6979132 -5.2037721 -4.7922292 -4.6183095 -4.7603388 -5.0412145 -5.1992993 -4.9849792 -4.4319806 -3.8783164][-7.0969872 -7.4965029 -7.916255 -8.1105452 -8.1359711 -7.7296419 -7.0314322 -6.437521 -6.0194149 -5.844214 -5.6422739 -5.411356 -4.9727693 -4.45507 -3.9375007][-6.0028958 -6.3096423 -6.7691174 -7.2098856 -7.4944739 -7.476747 -7.2708197 -6.7965918 -6.350656 -6.0880518 -5.7807374 -5.54806 -5.2058229 -4.6456857 -4.0032825][-5.491828 -5.6185508 -5.823298 -6.1541977 -6.423759 -6.5534625 -6.5612249 -6.2763858 -5.9684753 -5.68536 -5.4725528 -5.2376814 -4.906251 -4.467958 -4.0112648]]...]
INFO - root - 2017-12-16 06:51:23.985471: step 40310, loss = 0.23, batch loss = 0.17 (28.8 examples/sec; 0.278 sec/batch; 22h:32m:46s remains)
INFO - root - 2017-12-16 06:51:26.818486: step 40320, loss = 0.24, batch loss = 0.18 (28.5 examples/sec; 0.281 sec/batch; 22h:46m:33s remains)
INFO - root - 2017-12-16 06:51:29.678174: step 40330, loss = 0.22, batch loss = 0.16 (29.3 examples/sec; 0.273 sec/batch; 22h:07m:27s remains)
INFO - root - 2017-12-16 06:51:32.472364: step 40340, loss = 0.25, batch loss = 0.19 (29.2 examples/sec; 0.274 sec/batch; 22h:15m:42s remains)
INFO - root - 2017-12-16 06:51:35.312637: step 40350, loss = 0.34, batch loss = 0.28 (28.9 examples/sec; 0.277 sec/batch; 22h:28m:06s remains)
INFO - root - 2017-12-16 06:51:38.214997: step 40360, loss = 0.32, batch loss = 0.26 (26.2 examples/sec; 0.305 sec/batch; 24h:46m:13s remains)
INFO - root - 2017-12-16 06:51:41.044592: step 40370, loss = 0.29, batch loss = 0.23 (28.3 examples/sec; 0.282 sec/batch; 22h:55m:07s remains)
INFO - root - 2017-12-16 06:51:43.850746: step 40380, loss = 0.29, batch loss = 0.23 (28.1 examples/sec; 0.284 sec/batch; 23h:04m:04s remains)
INFO - root - 2017-12-16 06:51:46.672289: step 40390, loss = 0.24, batch loss = 0.18 (29.7 examples/sec; 0.269 sec/batch; 21h:50m:57s remains)
INFO - root - 2017-12-16 06:51:49.456971: step 40400, loss = 0.28, batch loss = 0.22 (27.9 examples/sec; 0.286 sec/batch; 23h:14m:32s remains)
2017-12-16 06:51:49.917586: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0215645 -2.7013211 -2.7047443 -2.7108736 -3.1679027 -3.6109254 -4.1377668 -4.3010807 -4.0669813 -3.9635646 -3.8136795 -3.8442698 -3.9492667 -4.238431 -4.615201][-1.6308103 -1.4476297 -1.578506 -1.947046 -2.4854515 -2.93075 -3.6190717 -4.056776 -4.1935573 -4.0880837 -3.8821495 -4.0516381 -4.2760897 -4.6638656 -5.199656][-0.42153072 -0.26137304 -0.3143816 -0.80271888 -1.5980115 -2.3093147 -3.0268931 -3.5521975 -3.6125507 -3.6588562 -3.639993 -3.8551798 -4.3109465 -4.7947173 -5.167459][0.60670185 0.6374445 0.35553122 0.0093102455 -0.54422379 -1.2542818 -1.8988371 -2.4090145 -2.6606169 -2.9039741 -3.0988379 -3.3760967 -3.7375066 -4.379694 -5.091639][0.7554493 0.9044981 0.7354784 0.34226227 -0.18895912 -0.67809582 -1.1127934 -1.4515297 -1.63712 -1.843097 -1.9586675 -2.3486309 -2.9271269 -3.5236905 -3.9017675][-0.060535908 0.35811329 0.66253948 0.70105982 0.6107378 0.40723085 0.15231514 -0.21664381 -0.47348 -0.83316326 -1.1714222 -1.6339886 -1.9956825 -2.5275593 -2.94318][-1.5692413 -0.60082269 0.10754108 0.49890757 0.681406 0.62674713 0.53222513 0.291512 0.12279177 -0.19139147 -0.40499973 -0.62526083 -0.85867882 -1.0749011 -1.0189383][-2.7540655 -1.7029104 -0.95824933 -0.28352547 0.18227673 0.37669182 0.57177448 0.53631449 0.56270313 0.48809624 0.47483349 0.43366671 0.53232622 0.63366747 0.73629332][-4.1375651 -3.0431962 -2.184871 -1.4044704 -0.83696818 -0.39436388 -0.0054311752 0.25096464 0.63596392 0.78185749 0.93327332 1.0261579 1.2781978 1.4439349 1.6724439][-5.1766715 -4.19067 -3.3218865 -2.5500236 -1.9964602 -1.4900784 -1.010638 -0.7169764 -0.31276369 0.04142952 0.39493036 0.56488466 0.824492 1.0823045 1.3019366][-5.6887937 -4.9655137 -4.3420143 -3.7021711 -3.078203 -2.647892 -2.2270012 -1.9917316 -1.6525352 -1.3340349 -1.0158517 -0.7837615 -0.50363946 -0.29489422 -0.033712864][-5.6944852 -5.3219795 -5.0369358 -4.665864 -4.3563867 -4.1471066 -3.8557138 -3.6334577 -3.3069124 -3.0944819 -2.8717065 -2.6389668 -2.3990579 -2.2490544 -2.1009886][-5.3638515 -5.2486215 -5.1885591 -5.2181444 -5.2645912 -5.2839913 -5.2454 -5.2665682 -5.0833917 -4.9203253 -4.6633649 -4.4831367 -4.3797445 -4.3711529 -4.227643][-4.4075084 -4.4339771 -4.6927819 -5.1271191 -5.4912138 -5.9510345 -6.2397008 -6.4439974 -6.47423 -6.355576 -6.137888 -6.0054393 -6.0238667 -5.9525418 -5.7326183][-3.6699266 -3.8058443 -4.0916438 -4.7199354 -5.3148384 -5.9950428 -6.4151697 -6.7854567 -7.0385709 -7.064806 -6.9325294 -6.7980928 -6.6986728 -6.5160542 -6.2476296]]...]
INFO - root - 2017-12-16 06:51:52.735232: step 40410, loss = 0.28, batch loss = 0.22 (27.8 examples/sec; 0.288 sec/batch; 23h:21m:15s remains)
INFO - root - 2017-12-16 06:51:55.572256: step 40420, loss = 0.37, batch loss = 0.31 (27.6 examples/sec; 0.290 sec/batch; 23h:32m:16s remains)
INFO - root - 2017-12-16 06:51:58.433050: step 40430, loss = 0.29, batch loss = 0.23 (27.7 examples/sec; 0.289 sec/batch; 23h:24m:29s remains)
INFO - root - 2017-12-16 06:52:01.258738: step 40440, loss = 0.27, batch loss = 0.22 (28.7 examples/sec; 0.279 sec/batch; 22h:38m:52s remains)
INFO - root - 2017-12-16 06:52:04.129865: step 40450, loss = 0.27, batch loss = 0.21 (26.5 examples/sec; 0.302 sec/batch; 24h:29m:13s remains)
INFO - root - 2017-12-16 06:52:06.960347: step 40460, loss = 0.29, batch loss = 0.23 (29.5 examples/sec; 0.271 sec/batch; 21h:59m:57s remains)
INFO - root - 2017-12-16 06:52:09.798910: step 40470, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.285 sec/batch; 23h:07m:08s remains)
INFO - root - 2017-12-16 06:52:12.609771: step 40480, loss = 0.25, batch loss = 0.19 (27.1 examples/sec; 0.295 sec/batch; 23h:57m:46s remains)
INFO - root - 2017-12-16 06:52:15.433098: step 40490, loss = 0.26, batch loss = 0.21 (28.1 examples/sec; 0.285 sec/batch; 23h:06m:52s remains)
INFO - root - 2017-12-16 06:52:18.232543: step 40500, loss = 0.21, batch loss = 0.15 (29.3 examples/sec; 0.273 sec/batch; 22h:10m:05s remains)
2017-12-16 06:52:18.685282: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7947254 -3.2811894 -3.7743919 -4.0112367 -3.7722673 -3.6799951 -3.7295291 -3.8942852 -4.1508183 -4.3171034 -4.4271488 -4.4239283 -4.241498 -3.8428833 -3.4075458][-2.4371133 -3.1802354 -4.0084772 -4.37765 -4.436408 -4.2629018 -4.2193093 -4.2764597 -4.2890224 -4.2944636 -4.4878035 -4.3644438 -4.2064705 -3.8759859 -3.4382231][-2.1378093 -3.1883783 -4.5404234 -5.1127915 -5.070065 -4.8997755 -4.5801735 -4.2796173 -4.1734176 -4.3159089 -4.5913944 -4.5519896 -4.4997988 -4.0486941 -3.663517][-1.9977982 -3.0616422 -4.2670321 -4.8617721 -4.9263768 -4.6147947 -4.0035057 -3.5543246 -3.2039788 -3.3369834 -3.8094661 -4.1219468 -4.3479872 -3.9995046 -3.676754][-1.7913706 -2.6924217 -3.7516689 -4.1340628 -3.8406 -3.0434341 -1.9913089 -1.3945549 -1.2369146 -1.6551595 -2.498353 -3.218214 -3.6482525 -3.6488044 -3.5245194][-2.2246263 -2.7299588 -3.4329469 -3.5720763 -2.7118936 -1.3365643 0.16725683 1.0891409 1.3898935 0.51550627 -0.93187737 -2.1063793 -3.10002 -3.2329681 -3.1969666][-2.4752474 -2.8107712 -3.2024016 -2.6314912 -1.3527248 0.47632122 2.5967832 3.660985 3.8099298 2.6839123 1.0577154 -0.69035935 -1.8813488 -2.3269114 -2.630836][-2.5823088 -2.9088931 -3.0122385 -2.3375769 -0.78305483 1.472199 3.789279 4.9394751 5.1499739 3.9201584 2.1553741 0.24560356 -1.3709226 -2.2359695 -2.5000422][-3.112318 -3.3540952 -3.4534774 -2.6687405 -1.1564808 1.0409126 3.2644095 4.3677607 4.5404005 3.3992195 1.7554092 -0.26061249 -1.7895486 -2.444833 -2.6798906][-3.830286 -4.233223 -4.8463278 -4.1229072 -2.579062 -0.67710662 1.2853827 2.4408593 2.7399893 1.8155518 0.27187014 -1.4772723 -2.7402878 -3.0696425 -3.0428905][-4.4903054 -5.10974 -5.65231 -5.2345004 -4.1906338 -2.4893041 -0.68622851 0.13602018 0.32155657 -0.4092865 -1.5988386 -2.9173746 -3.8286257 -3.8584337 -3.4931555][-4.2312937 -4.6959529 -5.5410047 -5.69182 -5.1084251 -3.824455 -2.6673803 -2.1352756 -1.8768802 -2.478569 -3.3770227 -4.3213215 -4.7444134 -4.4809961 -4.0056806][-3.6915538 -4.0067945 -4.8504844 -5.1320314 -4.9721346 -4.440083 -3.8158917 -3.4043267 -3.3688469 -3.7197206 -4.1412182 -4.6635432 -4.8680868 -4.4860282 -3.8037195][-3.287663 -3.3012633 -3.7460816 -4.2610826 -4.3000445 -4.1088 -3.8499722 -3.770359 -3.7783937 -3.8914857 -4.0326738 -4.2785239 -4.2542377 -3.7037849 -2.9942417][-3.2144632 -2.818675 -3.0241652 -3.3202102 -3.5484085 -3.6258707 -3.5797973 -3.6459808 -3.772213 -3.9059544 -3.9673343 -3.9143975 -3.6237462 -3.1135502 -2.4964147]]...]
INFO - root - 2017-12-16 06:52:21.525687: step 40510, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.282 sec/batch; 22h:50m:52s remains)
INFO - root - 2017-12-16 06:52:24.370766: step 40520, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.280 sec/batch; 22h:44m:30s remains)
INFO - root - 2017-12-16 06:52:27.167469: step 40530, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.283 sec/batch; 22h:55m:51s remains)
INFO - root - 2017-12-16 06:52:29.962228: step 40540, loss = 0.30, batch loss = 0.24 (28.7 examples/sec; 0.279 sec/batch; 22h:36m:43s remains)
INFO - root - 2017-12-16 06:52:32.795000: step 40550, loss = 0.33, batch loss = 0.27 (28.1 examples/sec; 0.284 sec/batch; 23h:03m:08s remains)
INFO - root - 2017-12-16 06:52:35.570385: step 40560, loss = 0.20, batch loss = 0.15 (28.7 examples/sec; 0.279 sec/batch; 22h:35m:43s remains)
INFO - root - 2017-12-16 06:52:38.404850: step 40570, loss = 0.24, batch loss = 0.19 (28.6 examples/sec; 0.280 sec/batch; 22h:41m:08s remains)
INFO - root - 2017-12-16 06:52:41.242671: step 40580, loss = 0.27, batch loss = 0.21 (28.3 examples/sec; 0.283 sec/batch; 22h:55m:14s remains)
INFO - root - 2017-12-16 06:52:44.040804: step 40590, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 23h:04m:31s remains)
INFO - root - 2017-12-16 06:52:46.877356: step 40600, loss = 0.28, batch loss = 0.22 (27.3 examples/sec; 0.293 sec/batch; 23h:45m:15s remains)
2017-12-16 06:52:47.340889: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1287179 -5.1203656 -5.2575736 -5.8289161 -6.1380482 -6.1207376 -5.7377949 -5.45544 -5.4445615 -5.6076384 -5.9859142 -6.3593254 -6.9388595 -6.9792638 -6.8693433][-4.7878852 -5.1763096 -5.4770651 -5.7733908 -5.8781905 -6.2964334 -6.1556129 -5.8009338 -5.6848497 -5.6991587 -6.1331859 -6.6120186 -7.2324395 -7.4782319 -7.3927622][-4.3592529 -4.6874328 -4.9702454 -5.1491451 -5.2010975 -5.3107476 -4.9919534 -4.8928652 -4.7535706 -4.9200974 -5.7044239 -6.3663559 -6.9837751 -7.1321974 -7.2828274][-3.4283109 -3.8737254 -3.8498018 -3.6411192 -3.3484211 -3.1009436 -2.6545744 -2.6680851 -2.6893249 -2.9817867 -3.5169988 -4.5869722 -6.2643948 -7.1901731 -7.4740553][-2.8986163 -2.7636523 -2.3068287 -1.9199374 -1.2252274 -0.39478254 0.3535099 0.58355141 0.3560133 -0.35926819 -1.4980459 -2.718751 -4.3464756 -6.0780077 -7.4541311][-3.2512746 -2.8235309 -1.6160989 -0.44409251 0.8395195 1.7877951 2.8131261 3.5326996 3.5288348 2.8774667 1.5028877 -0.61342168 -3.3981495 -5.615994 -7.0690117][-4.0914755 -3.1074014 -1.7254035 -0.11072683 1.7199731 3.1345925 4.699955 5.4791956 5.7438612 4.9638386 3.3057485 0.92808104 -1.8649983 -4.5930734 -6.9785385][-4.7217093 -3.6704888 -2.3744488 -0.64175844 1.1472979 2.884582 4.708149 5.6155043 6.1667833 5.5052052 4.028904 1.5104284 -1.4835744 -4.000248 -5.9131885][-5.8084297 -4.8778229 -3.6550698 -2.1948442 -0.68091726 1.0307059 2.7811561 3.8539629 4.5760431 4.1078568 2.9232635 0.92917776 -1.4274058 -3.6736739 -5.2866325][-6.2354383 -5.7949896 -5.3277755 -4.2445951 -2.9527249 -1.7118649 -0.58451724 0.39865685 1.2290411 1.1450958 0.43038559 -0.80433464 -2.3689177 -3.8279214 -4.8053503][-6.56938 -6.2450213 -5.9500513 -5.6101108 -5.172688 -4.2113013 -3.1703815 -2.610446 -2.237427 -2.234055 -2.1713629 -2.7990375 -3.7323194 -4.3229423 -4.7458277][-5.9940925 -5.9199395 -6.1464071 -5.9712682 -5.8136363 -5.7082815 -5.2543325 -4.6790524 -4.1601009 -4.0606265 -3.9227309 -4.1923738 -4.5064497 -4.8482881 -5.2257104][-4.8009186 -4.8293529 -5.1139379 -5.1413369 -5.4352226 -5.5178766 -5.4220386 -5.3851709 -5.1532555 -5.0153508 -4.8719826 -4.8914008 -4.8384237 -4.9276237 -5.1954622][-4.0514445 -3.9885306 -4.1417766 -4.2908611 -4.5837541 -4.6086025 -4.6609731 -4.8071656 -4.7813859 -4.6321621 -4.4285483 -4.5436225 -4.6775351 -4.613811 -4.5805645][-3.3436089 -3.2228427 -3.2401447 -3.3718362 -3.5651116 -3.6428607 -3.68388 -3.6063013 -3.4666555 -3.4887729 -3.5109637 -3.3935215 -3.367918 -3.5002847 -3.641784]]...]
INFO - root - 2017-12-16 06:52:50.214254: step 40610, loss = 0.22, batch loss = 0.16 (28.5 examples/sec; 0.281 sec/batch; 22h:44m:43s remains)
INFO - root - 2017-12-16 06:52:53.077177: step 40620, loss = 0.25, batch loss = 0.19 (29.7 examples/sec; 0.269 sec/batch; 21h:48m:36s remains)
INFO - root - 2017-12-16 06:52:55.889874: step 40630, loss = 0.39, batch loss = 0.33 (28.6 examples/sec; 0.280 sec/batch; 22h:42m:51s remains)
INFO - root - 2017-12-16 06:52:58.712664: step 40640, loss = 0.22, batch loss = 0.16 (29.2 examples/sec; 0.274 sec/batch; 22h:11m:23s remains)
INFO - root - 2017-12-16 06:53:01.556023: step 40650, loss = 0.34, batch loss = 0.28 (26.7 examples/sec; 0.300 sec/batch; 24h:19m:57s remains)
INFO - root - 2017-12-16 06:53:04.420397: step 40660, loss = 0.24, batch loss = 0.18 (26.9 examples/sec; 0.297 sec/batch; 24h:04m:43s remains)
INFO - root - 2017-12-16 06:53:07.231591: step 40670, loss = 0.24, batch loss = 0.19 (27.8 examples/sec; 0.288 sec/batch; 23h:20m:21s remains)
INFO - root - 2017-12-16 06:53:10.054669: step 40680, loss = 0.34, batch loss = 0.28 (27.4 examples/sec; 0.292 sec/batch; 23h:38m:33s remains)
INFO - root - 2017-12-16 06:53:12.850406: step 40690, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.278 sec/batch; 22h:34m:27s remains)
INFO - root - 2017-12-16 06:53:15.686449: step 40700, loss = 0.24, batch loss = 0.18 (27.9 examples/sec; 0.287 sec/batch; 23h:14m:57s remains)
2017-12-16 06:53:16.169175: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2694592 -5.5133786 -4.975781 -4.6645379 -4.5632906 -4.418005 -4.4433956 -4.3272948 -4.0723329 -3.8534667 -3.6100569 -3.4762082 -3.5188141 -3.8839939 -4.2661285][-5.9628267 -5.4517312 -5.11539 -4.7511096 -4.4148245 -4.0609012 -3.8280811 -3.5083594 -3.1946173 -2.8292937 -2.6867723 -2.6106906 -2.726069 -2.9228764 -3.2885602][-6.095645 -5.6818843 -5.0877156 -4.6422057 -4.0719767 -3.3816042 -2.8393359 -2.3722339 -1.9899495 -1.7605317 -1.7852948 -1.9047713 -2.0572853 -2.221324 -2.3956895][-5.7385139 -5.6686411 -5.2917542 -4.612606 -3.6958506 -2.7882264 -2.0296965 -1.4052017 -1.026757 -1.0033073 -1.2040119 -1.4759774 -1.7369871 -1.7621415 -1.9512322][-5.6639071 -5.4211173 -4.8233256 -3.9254518 -2.9496377 -1.7009108 -0.65469909 -0.20105076 -0.23334694 -0.45062947 -0.79712534 -1.2686081 -1.6713982 -1.9914064 -2.2621765][-6.049036 -5.521677 -4.585135 -3.4819796 -2.0688057 -0.61687469 0.40082693 0.95660543 0.97858858 0.39941931 -0.50115275 -1.3990359 -2.0433104 -2.5142026 -2.6281192][-6.0655413 -5.7019873 -4.7207232 -3.4021666 -1.9184837 -0.31210136 0.94622421 1.5091577 1.4431658 0.84746361 -0.21308041 -1.3899481 -2.3527303 -3.0411386 -3.5085638][-5.7203517 -5.467608 -4.7316771 -3.5703282 -2.1579075 -0.60065508 0.74843645 1.535213 1.5098033 0.95665646 -0.16280556 -1.517092 -2.6823502 -3.5111921 -4.1017118][-5.4149117 -5.373198 -4.7137504 -3.90883 -2.724319 -1.1996257 0.29989052 1.1107969 1.3176103 0.78505945 -0.42369413 -1.8924534 -3.2363555 -4.276391 -4.9823294][-5.0255604 -5.3936138 -5.2947278 -4.6863236 -3.6121645 -2.3100336 -0.94211912 0.13985157 0.67662382 0.19899797 -0.87137961 -2.278465 -3.6850767 -4.812737 -5.5608368][-4.500042 -5.280364 -5.6126857 -5.3322821 -4.4683337 -3.266356 -2.0995388 -1.0577598 -0.51639676 -0.67501044 -1.5420632 -2.691983 -3.8936744 -4.8909068 -5.428453][-4.3980618 -5.1405511 -5.5161538 -5.3440156 -4.7107582 -3.6843953 -2.7696316 -2.0878317 -1.731657 -1.7608175 -2.2551076 -3.1319046 -4.0930943 -4.7476358 -5.0897236][-4.3018456 -4.8648071 -5.072679 -4.7944112 -4.2743626 -3.5787342 -3.0473762 -2.6461458 -2.4403915 -2.5013506 -2.8061135 -3.2762151 -3.7800562 -4.1535797 -4.3355303][-4.0121665 -4.3475623 -4.3495679 -3.9203792 -3.3859198 -2.97527 -2.8578639 -2.8204644 -2.8023005 -2.9302125 -3.0704975 -3.1122911 -3.2761011 -3.4376497 -3.5574694][-3.8908679 -3.8791428 -3.6730099 -3.2500327 -2.9034705 -2.6427898 -2.6012688 -2.721437 -2.8825684 -2.9363918 -2.8814559 -2.8521631 -2.9673853 -2.9865839 -3.1132774]]...]
INFO - root - 2017-12-16 06:53:18.998057: step 40710, loss = 0.24, batch loss = 0.18 (27.9 examples/sec; 0.287 sec/batch; 23h:13m:25s remains)
INFO - root - 2017-12-16 06:53:21.879449: step 40720, loss = 0.24, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 22h:14m:05s remains)
INFO - root - 2017-12-16 06:53:24.779261: step 40730, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.284 sec/batch; 23h:00m:06s remains)
INFO - root - 2017-12-16 06:53:27.615109: step 40740, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 23h:03m:51s remains)
INFO - root - 2017-12-16 06:53:30.468603: step 40750, loss = 0.20, batch loss = 0.14 (27.6 examples/sec; 0.289 sec/batch; 23h:27m:01s remains)
INFO - root - 2017-12-16 06:53:33.299269: step 40760, loss = 0.26, batch loss = 0.20 (27.6 examples/sec; 0.290 sec/batch; 23h:30m:39s remains)
INFO - root - 2017-12-16 06:53:36.117037: step 40770, loss = 0.23, batch loss = 0.17 (29.3 examples/sec; 0.273 sec/batch; 22h:08m:47s remains)
INFO - root - 2017-12-16 06:53:38.929570: step 40780, loss = 0.45, batch loss = 0.40 (28.1 examples/sec; 0.285 sec/batch; 23h:06m:22s remains)
INFO - root - 2017-12-16 06:53:41.771993: step 40790, loss = 0.22, batch loss = 0.17 (28.4 examples/sec; 0.282 sec/batch; 22h:50m:43s remains)
INFO - root - 2017-12-16 06:53:44.537031: step 40800, loss = 0.24, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 22h:09m:49s remains)
2017-12-16 06:53:44.993413: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8081441 -4.9702888 -4.9072342 -4.599422 -4.3467088 -4.2402568 -4.2174644 -4.129797 -3.9504242 -3.913523 -4.0883069 -4.3247476 -4.5500479 -4.720624 -5.1702013][-4.6353378 -4.755301 -4.6683683 -4.5199189 -4.3467145 -4.2867074 -4.3916154 -4.2432318 -4.0224791 -3.9775958 -4.2015395 -4.5016246 -4.737452 -4.8142781 -5.0587888][-3.990355 -4.3350749 -4.4160438 -4.1722879 -3.8775048 -3.8294144 -3.6899452 -3.6961439 -3.6234586 -3.6857872 -4.0401282 -4.45575 -4.9090056 -5.2217045 -5.4371276][-3.1381624 -3.4373767 -3.6075077 -3.4006481 -2.9811356 -2.8050551 -2.5812206 -2.4699836 -2.4292116 -2.7001607 -3.2246969 -3.9770494 -4.8368063 -5.4269333 -5.7561536][-2.145828 -2.314364 -2.4349773 -2.2563989 -1.8308191 -1.2266858 -0.55295038 -0.23620033 -0.32065821 -1.0746408 -2.006489 -3.1190429 -4.2973385 -5.2807221 -5.9236903][-1.0516798 -1.1535258 -1.4784636 -1.0159349 -0.070904732 0.70971394 1.3296571 1.8446298 1.8991861 1.1010389 -0.25823307 -1.9627063 -3.6289544 -4.9937387 -5.914175][-0.72076225 -0.60048175 -0.31690121 0.087982178 0.74766159 1.909348 3.1600671 3.5625119 3.3905849 2.5351024 1.0915403 -0.97311497 -2.8708119 -4.453938 -5.5095005][-0.705843 -0.79918432 -0.83581066 0.13115978 1.4443221 2.592031 3.6700897 4.184391 4.05663 3.0416961 1.3873825 -0.7086463 -2.5691547 -4.0789027 -5.2233977][-1.4922037 -1.7320311 -1.5906024 -0.86202168 0.03965044 1.6640329 3.1151161 3.5257339 3.2420774 2.2873702 0.92464924 -1.0649309 -2.7189884 -4.1451635 -5.1204977][-1.7983499 -2.4874859 -2.9895244 -2.2938802 -1.1976047 -0.07897377 0.76298761 1.5079093 1.7398548 0.94658661 -0.35753727 -2.0215695 -3.3521979 -4.30922 -5.11365][-2.9123018 -3.6406872 -3.811928 -3.7421956 -3.3507628 -2.2281096 -1.1631856 -0.67373323 -0.62468839 -0.99884105 -1.8131008 -2.9510968 -4.066586 -5.0872889 -5.6356483][-3.6323104 -4.600594 -5.3155832 -5.3261051 -4.757844 -4.0750527 -3.5871384 -2.9197383 -2.5843444 -2.6067276 -3.4009576 -4.3729224 -5.0033197 -5.6694922 -6.1190281][-3.8917136 -4.8621049 -5.7928605 -6.267405 -6.329638 -5.765707 -5.1361713 -4.5316367 -4.4125495 -4.2064819 -4.4381146 -4.9668741 -5.4941797 -6.0537472 -6.3073678][-4.4811106 -4.9240737 -5.7714949 -6.4845133 -6.6617546 -6.4921217 -6.1231585 -5.5125403 -5.0846152 -4.5263896 -4.6789432 -4.9885111 -5.4437575 -5.9350109 -6.3559847][-5.2856989 -5.349278 -5.773345 -6.3799868 -6.706934 -6.6109486 -6.2711143 -5.6538558 -5.2287445 -4.7782145 -4.7248435 -4.9270191 -5.526762 -5.9943933 -6.0911512]]...]
INFO - root - 2017-12-16 06:53:47.844768: step 40810, loss = 0.23, batch loss = 0.17 (27.2 examples/sec; 0.295 sec/batch; 23h:52m:28s remains)
INFO - root - 2017-12-16 06:53:50.631775: step 40820, loss = 0.32, batch loss = 0.26 (29.1 examples/sec; 0.275 sec/batch; 22h:18m:34s remains)
INFO - root - 2017-12-16 06:53:53.506369: step 40830, loss = 0.28, batch loss = 0.22 (28.7 examples/sec; 0.279 sec/batch; 22h:35m:32s remains)
INFO - root - 2017-12-16 06:53:56.355098: step 40840, loss = 0.51, batch loss = 0.45 (26.2 examples/sec; 0.305 sec/batch; 24h:44m:57s remains)
INFO - root - 2017-12-16 06:53:59.131883: step 40850, loss = 0.48, batch loss = 0.42 (28.7 examples/sec; 0.278 sec/batch; 22h:33m:19s remains)
INFO - root - 2017-12-16 06:54:01.906001: step 40860, loss = 0.30, batch loss = 0.24 (29.6 examples/sec; 0.270 sec/batch; 21h:53m:30s remains)
INFO - root - 2017-12-16 06:54:04.721459: step 40870, loss = 0.22, batch loss = 0.17 (29.5 examples/sec; 0.271 sec/batch; 21h:56m:08s remains)
INFO - root - 2017-12-16 06:54:07.557133: step 40880, loss = 0.37, batch loss = 0.31 (28.3 examples/sec; 0.283 sec/batch; 22h:56m:22s remains)
INFO - root - 2017-12-16 06:54:10.461301: step 40890, loss = 0.23, batch loss = 0.17 (27.7 examples/sec; 0.289 sec/batch; 23h:22m:28s remains)
INFO - root - 2017-12-16 06:54:13.293121: step 40900, loss = 0.37, batch loss = 0.31 (28.2 examples/sec; 0.283 sec/batch; 22h:56m:51s remains)
2017-12-16 06:54:13.748799: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2799296 -2.4216745 -2.6592546 -3.052844 -3.391469 -3.5816383 -3.7510016 -3.9693978 -4.0476975 -3.8083224 -3.3930678 -2.9074349 -2.3891139 -1.9891226 -1.6914306][-1.985811 -2.304147 -2.6731882 -3.0885186 -3.3549113 -3.552403 -3.7116489 -3.6432407 -3.3744483 -2.9546053 -2.5227056 -2.0186906 -1.5387044 -1.2151806 -1.0877664][-2.1409869 -2.4589572 -2.8302393 -3.2420497 -3.34326 -3.2314708 -3.0235071 -2.7934163 -2.5646143 -2.0674016 -1.7487833 -1.4384441 -1.1707902 -0.98881817 -0.95031309][-2.493886 -2.8319211 -3.1197684 -3.2875471 -3.0375118 -2.4663184 -1.8010471 -1.1125093 -0.840276 -0.63634491 -0.53608894 -0.77478433 -1.1767364 -1.2861986 -1.2033331][-2.9502277 -3.3211689 -3.5028973 -3.2668614 -2.5500283 -1.4550152 -0.21183395 0.74162626 0.98859167 0.97662258 0.52198267 -0.24239588 -1.0808256 -1.6319714 -1.8735187][-3.1174164 -3.4837155 -3.4076276 -2.8956602 -1.7114589 -0.081223965 1.547298 2.8250842 3.170094 2.7270837 1.5528173 0.18391991 -1.0595365 -1.7917423 -2.0257628][-2.9878342 -3.2628031 -2.94727 -2.04902 -0.54756856 1.5953741 3.7804928 5.0676003 5.1067963 4.2351074 2.5364823 0.54694128 -1.0122993 -1.8239963 -2.0345516][-3.1971583 -3.4185224 -2.9506907 -1.735024 0.1043067 2.4480176 4.5848455 5.9513206 6.0559473 4.6587648 2.4787879 0.39719582 -1.2527096 -2.1247098 -2.2401204][-3.256391 -3.5606039 -3.2727644 -1.9008191 0.067136288 2.3613634 4.2428083 5.3085022 5.071208 3.6045485 1.4410443 -0.47691274 -1.7886353 -2.5583544 -2.76102][-3.3718419 -3.7315173 -3.750525 -2.740273 -0.93374324 1.1406093 2.7110176 3.6331511 3.2844424 1.8832588 -0.12014866 -1.8909369 -3.0341661 -3.477201 -3.5001984][-3.9232719 -4.2475915 -4.0580783 -3.1954679 -1.8744137 -0.1677928 1.1264877 1.6083817 1.2833772 0.19868374 -1.4425793 -2.9345572 -3.8230629 -4.2700281 -4.382093][-4.4381976 -4.8179274 -4.8313727 -3.9103255 -2.4767404 -1.2432415 -0.63352609 -0.41851377 -0.7583375 -1.7841833 -3.03718 -4.1930275 -4.8802752 -5.1839271 -5.1751304][-4.5592928 -4.8574996 -4.9149728 -4.4905558 -3.8166373 -2.7878082 -2.0199714 -1.9519353 -2.3155878 -2.9625812 -3.9963737 -4.9962869 -5.5482874 -5.9023213 -5.8055792][-4.6045771 -4.6033216 -4.4469509 -4.1660495 -3.597729 -2.9042583 -2.4276247 -2.3314598 -2.7968907 -3.4334021 -4.1349545 -4.9572439 -5.5128064 -5.5910482 -5.3377404][-4.1346879 -4.0913377 -3.9159288 -3.7048 -3.3687577 -2.6967974 -2.12028 -2.0788474 -2.4627881 -3.1274114 -3.8446329 -4.540731 -4.989295 -5.0886664 -4.667695]]...]
INFO - root - 2017-12-16 06:54:16.586287: step 40910, loss = 0.21, batch loss = 0.15 (28.0 examples/sec; 0.286 sec/batch; 23h:07m:31s remains)
INFO - root - 2017-12-16 06:54:19.380560: step 40920, loss = 0.38, batch loss = 0.32 (28.7 examples/sec; 0.278 sec/batch; 22h:33m:09s remains)
INFO - root - 2017-12-16 06:54:22.216856: step 40930, loss = 0.26, batch loss = 0.20 (27.9 examples/sec; 0.286 sec/batch; 23h:10m:54s remains)
INFO - root - 2017-12-16 06:54:25.096789: step 40940, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.278 sec/batch; 22h:33m:11s remains)
INFO - root - 2017-12-16 06:54:27.942551: step 40950, loss = 0.32, batch loss = 0.26 (27.8 examples/sec; 0.288 sec/batch; 23h:17m:41s remains)
INFO - root - 2017-12-16 06:54:30.755168: step 40960, loss = 0.24, batch loss = 0.18 (29.3 examples/sec; 0.273 sec/batch; 22h:08m:38s remains)
INFO - root - 2017-12-16 06:54:33.598163: step 40970, loss = 0.27, batch loss = 0.21 (28.1 examples/sec; 0.285 sec/batch; 23h:05m:24s remains)
INFO - root - 2017-12-16 06:54:36.408033: step 40980, loss = 0.24, batch loss = 0.19 (27.5 examples/sec; 0.291 sec/batch; 23h:35m:12s remains)
INFO - root - 2017-12-16 06:54:39.244057: step 40990, loss = 0.22, batch loss = 0.16 (28.5 examples/sec; 0.281 sec/batch; 22h:45m:13s remains)
INFO - root - 2017-12-16 06:54:42.055184: step 41000, loss = 0.25, batch loss = 0.19 (27.9 examples/sec; 0.287 sec/batch; 23h:14m:41s remains)
2017-12-16 06:54:42.530324: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8909416 -3.1776428 -3.2702398 -3.33665 -3.3178406 -3.2423544 -3.2993207 -3.6570501 -3.9170871 -3.8900096 -3.7979569 -3.8233092 -3.9537764 -4.2423186 -4.3969][-3.1014895 -3.3192825 -3.3413167 -3.1907377 -2.9453382 -3.0174322 -3.2236943 -3.3817639 -3.6701295 -4.0083275 -4.27401 -4.3147416 -4.4313741 -4.8551216 -5.0714259][-3.4389963 -3.6123872 -3.674762 -3.5675964 -3.2181034 -2.9297667 -2.9498878 -3.142689 -3.4487305 -3.6083224 -3.7814889 -4.0937614 -4.4475327 -5.0146036 -5.3161016][-3.6697359 -3.7190197 -3.7460389 -3.5255151 -3.0667775 -2.5858965 -2.2398102 -2.1374328 -2.3791366 -2.7244048 -2.9885163 -3.3100989 -3.7086134 -4.4592457 -4.90943][-3.6301818 -3.6429827 -3.5278869 -3.0574913 -2.3793635 -1.8233066 -1.3972423 -1.0217283 -0.98900747 -1.2048748 -1.5558279 -2.0023837 -2.5380847 -3.279855 -3.9757547][-3.9208465 -3.786907 -3.3604407 -2.5850818 -1.5862062 -0.69449878 -0.012602329 0.3529315 0.42476845 0.37083387 0.043024063 -0.35434437 -0.97200179 -1.8985071 -2.8098698][-3.6916091 -3.6146338 -3.1825275 -2.1904998 -0.89677238 0.12468433 0.96235943 1.5263734 1.8064132 1.9543014 1.6458988 1.3877001 0.85599518 -0.12118769 -1.3590696][-3.0432382 -3.0854769 -2.8970809 -2.0201907 -0.73307633 0.32593346 1.1837387 1.8146911 2.3003798 2.6755118 2.7066579 2.5258965 2.0777783 1.1473241 -0.28844786][-2.7611163 -2.7329178 -2.5929153 -1.9883711 -0.95586538 0.014436245 0.7414794 1.3167939 1.9856014 2.6234455 3.0509214 3.0713387 2.7049441 1.8041563 0.31769371][-2.7333179 -2.7850308 -2.7259173 -2.423938 -1.8191097 -1.0229812 -0.294127 0.32839203 1.0210347 1.6457744 2.1087294 2.292799 2.0138702 1.1899734 -0.14240551][-2.6599426 -2.8143566 -3.0064545 -2.927489 -2.5759215 -2.1002958 -1.4985261 -0.86652517 -0.12908888 0.48455477 0.95901346 1.1006403 0.92345285 0.13986778 -1.0344586][-2.8774705 -3.0717888 -3.2841702 -3.4026513 -3.3019142 -2.9627373 -2.4114261 -1.8604319 -1.2409689 -0.73914814 -0.36777306 -0.22845173 -0.43289089 -1.1015084 -1.9392242][-2.7423844 -2.9605985 -3.2427096 -3.2993479 -3.2134013 -3.0085714 -2.6678455 -2.4208338 -2.0310557 -1.834861 -1.7384529 -1.6203904 -1.7475822 -2.0652528 -2.592802][-2.6363142 -2.7800422 -2.8849087 -2.8668633 -2.7648418 -2.5716071 -2.363013 -2.2884109 -2.198591 -2.280673 -2.3698199 -2.3916795 -2.3872912 -2.3962595 -2.6111574][-2.6456847 -2.5437846 -2.3195286 -2.0674126 -1.8694007 -1.7359996 -1.7206876 -1.8928583 -2.1843641 -2.5595648 -2.8118141 -2.7978334 -2.8190846 -2.7421961 -2.6267006]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 06:54:45.393324: step 41010, loss = 0.30, batch loss = 0.24 (28.0 examples/sec; 0.285 sec/batch; 23h:06m:40s remains)
INFO - root - 2017-12-16 06:54:48.242115: step 41020, loss = 0.23, batch loss = 0.18 (28.0 examples/sec; 0.286 sec/batch; 23h:09m:11s remains)
INFO - root - 2017-12-16 06:54:51.069830: step 41030, loss = 0.26, batch loss = 0.20 (27.8 examples/sec; 0.287 sec/batch; 23h:16m:21s remains)
INFO - root - 2017-12-16 06:54:53.933477: step 41040, loss = 0.32, batch loss = 0.26 (28.2 examples/sec; 0.284 sec/batch; 22h:59m:10s remains)
INFO - root - 2017-12-16 06:54:56.766708: step 41050, loss = 0.30, batch loss = 0.24 (27.9 examples/sec; 0.287 sec/batch; 23h:14m:06s remains)
INFO - root - 2017-12-16 06:54:59.554334: step 41060, loss = 0.24, batch loss = 0.19 (28.3 examples/sec; 0.283 sec/batch; 22h:53m:26s remains)
INFO - root - 2017-12-16 06:55:02.384103: step 41070, loss = 0.23, batch loss = 0.17 (27.3 examples/sec; 0.293 sec/batch; 23h:41m:27s remains)
INFO - root - 2017-12-16 06:55:05.185005: step 41080, loss = 0.30, batch loss = 0.24 (28.9 examples/sec; 0.277 sec/batch; 22h:24m:30s remains)
INFO - root - 2017-12-16 06:55:08.041280: step 41090, loss = 0.26, batch loss = 0.20 (25.4 examples/sec; 0.316 sec/batch; 25h:32m:30s remains)
INFO - root - 2017-12-16 06:55:10.908081: step 41100, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 23h:09m:24s remains)
2017-12-16 06:55:11.376374: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6661067 -2.9650884 -2.9443865 -2.5058148 -2.2257533 -2.2375104 -2.3053834 -2.2629912 -2.4288375 -2.6457982 -2.809989 -2.9203744 -2.9407744 -3.0866089 -3.1748405][-3.2982783 -3.5001972 -3.2547915 -2.8903923 -2.5532641 -2.3558664 -2.4353807 -2.5708785 -2.8393455 -2.9481859 -3.2592497 -3.6287985 -3.7500935 -3.9574637 -4.1303687][-4.30653 -4.3212948 -3.9473398 -3.3784692 -2.6884003 -2.2840312 -2.2530942 -2.4422359 -2.7316504 -3.087178 -3.5670831 -4.1017494 -4.4838495 -4.8537831 -5.0493116][-5.0436864 -4.7829885 -4.2268453 -3.3625536 -2.4737225 -1.8434737 -1.650321 -1.8075299 -2.1775768 -2.8823354 -3.6339726 -4.2575188 -4.7537909 -5.2878861 -5.5448594][-5.6948209 -5.223907 -4.42784 -3.2628498 -2.051738 -1.0424786 -0.59093094 -0.69684458 -1.1697261 -1.8943758 -2.7481298 -3.7159026 -4.6308236 -5.2531142 -5.5325727][-5.3738647 -4.8145013 -3.8754272 -2.5435655 -1.0517848 0.18161345 0.87299538 0.897377 0.47333717 -0.38099146 -1.6243303 -2.8448911 -3.9423792 -4.7361574 -5.1222467][-4.3241935 -3.6592336 -2.7921143 -1.3889074 0.1748414 1.4182458 2.280252 2.3985639 2.0559034 1.0842876 -0.24852657 -1.7187028 -2.9865937 -3.8122978 -3.9991124][-3.0373554 -2.3820522 -1.6410992 -0.53389716 0.72841644 1.8360772 2.5574565 2.5445089 2.132967 1.1665077 -0.0068078041 -1.2438807 -2.1848879 -2.8620219 -2.8171906][-2.5643435 -1.9999125 -1.2708006 -0.30251265 0.73696089 1.6036081 2.1345763 1.9599929 1.4111972 0.49202156 -0.59810662 -1.7549841 -2.5544436 -2.6292095 -2.2396839][-2.5640526 -2.27787 -1.7141826 -0.96696639 -0.13887024 0.56383753 0.93139505 0.73111916 0.10537004 -0.79131794 -1.7035921 -2.4994206 -2.8097913 -2.5109894 -1.7375526][-3.0420971 -2.8695908 -2.4720182 -1.8616083 -1.2971916 -0.78725576 -0.54570723 -0.88684487 -1.6446211 -2.6074228 -3.3920341 -3.8233047 -3.6529713 -2.8087261 -1.5468709][-3.5384748 -3.4906 -3.0931115 -2.565114 -2.0972214 -1.7181497 -1.7720511 -2.1237328 -3.0762217 -4.242805 -5.0125632 -5.32367 -4.9052105 -3.699434 -2.0815022][-4.1195521 -3.9935331 -3.6113436 -3.0541263 -2.5106392 -2.1335371 -2.1645613 -2.6998956 -3.8077624 -5.0889955 -6.1446295 -6.50828 -6.0603714 -4.8563113 -3.2373972][-4.6312008 -4.5080514 -3.8464689 -3.2095556 -2.6057723 -2.1533782 -2.2701921 -2.8056393 -4.0504785 -5.5108404 -6.6518908 -7.2410231 -6.8396893 -5.7700338 -4.3442988][-4.6107259 -4.6614504 -4.2202158 -3.3802655 -2.6422081 -2.1738279 -2.3228731 -2.896152 -4.1849537 -5.80966 -7.1928816 -7.7452965 -7.3434706 -6.3174257 -4.8671989]]...]
INFO - root - 2017-12-16 06:55:14.170054: step 41110, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.276 sec/batch; 22h:22m:16s remains)
INFO - root - 2017-12-16 06:55:17.038535: step 41120, loss = 0.35, batch loss = 0.29 (29.4 examples/sec; 0.272 sec/batch; 22h:00m:17s remains)
INFO - root - 2017-12-16 06:55:19.878878: step 41130, loss = 0.45, batch loss = 0.39 (29.0 examples/sec; 0.276 sec/batch; 22h:19m:27s remains)
INFO - root - 2017-12-16 06:55:22.694619: step 41140, loss = 0.23, batch loss = 0.17 (28.8 examples/sec; 0.277 sec/batch; 22h:26m:51s remains)
INFO - root - 2017-12-16 06:55:25.565715: step 41150, loss = 0.32, batch loss = 0.26 (28.0 examples/sec; 0.285 sec/batch; 23h:05m:14s remains)
INFO - root - 2017-12-16 06:55:28.402574: step 41160, loss = 0.24, batch loss = 0.19 (28.0 examples/sec; 0.285 sec/batch; 23h:05m:28s remains)
INFO - root - 2017-12-16 06:55:31.240350: step 41170, loss = 0.27, batch loss = 0.21 (28.3 examples/sec; 0.283 sec/batch; 22h:53m:28s remains)
INFO - root - 2017-12-16 06:55:34.054534: step 41180, loss = 0.20, batch loss = 0.14 (27.9 examples/sec; 0.286 sec/batch; 23h:09m:51s remains)
INFO - root - 2017-12-16 06:55:36.873930: step 41190, loss = 0.23, batch loss = 0.17 (29.0 examples/sec; 0.276 sec/batch; 22h:21m:12s remains)
INFO - root - 2017-12-16 06:55:39.713026: step 41200, loss = 0.35, batch loss = 0.29 (28.4 examples/sec; 0.281 sec/batch; 22h:45m:30s remains)
2017-12-16 06:55:40.182677: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.845865 -2.1840212 -2.3767867 -2.4388051 -2.4855461 -2.656189 -2.7256334 -2.8903365 -3.1634641 -3.4449489 -3.5259533 -3.4027104 -3.1927729 -2.8913436 -2.6339402][-1.8069103 -1.9528024 -1.9614265 -1.8772836 -1.8241203 -2.0386887 -2.1550281 -2.2378025 -2.4107122 -2.7974279 -3.088563 -3.0795379 -2.984868 -2.5169859 -1.979531][-1.5876136 -1.5838687 -1.5047224 -1.1829946 -0.8981185 -0.85397434 -0.84990954 -1.1133065 -1.4519997 -1.8473647 -2.2460973 -2.6419883 -2.711242 -2.2349603 -1.6458001][-1.1714611 -0.95416379 -0.76976585 -0.34002304 0.22177887 0.54125118 0.58202887 0.21240234 -0.43705869 -1.167248 -1.74592 -2.2312965 -2.4644227 -2.1894846 -1.5618684][-1.2235837 -0.6918087 0.017133713 0.4222331 1.0453916 1.6161833 1.9220753 1.6034298 0.79281616 -0.054868221 -0.88550615 -1.6141939 -1.9237776 -1.862272 -1.6565213][-1.5256588 -1.0027347 -0.11456537 0.80625391 1.8430672 2.5534744 2.9660296 2.8132696 2.079885 1.1497765 0.15985632 -0.7585125 -1.5085332 -1.7642176 -1.7546957][-1.6155109 -1.0037558 -0.15018415 0.96488 2.2696629 3.2753534 3.9538908 3.7504902 2.843926 1.8721414 0.83985949 -0.28035688 -1.3750627 -2.21366 -2.8292007][-2.2527215 -1.7445662 -0.88638878 0.25665188 1.7007136 2.9022322 3.6297293 3.7178764 3.0324111 1.9084206 0.75558853 -0.42573786 -1.6557627 -2.9325452 -3.8510797][-3.5902598 -3.3442137 -2.4487333 -1.1635666 0.30847502 1.6638694 2.5591617 2.7277565 2.2393608 1.2889318 0.19317579 -1.0318961 -2.3199317 -3.6021609 -4.4353657][-4.8215475 -4.7050958 -4.0731859 -2.8361197 -1.2866063 -0.0094413757 1.0057807 1.2822628 0.85942459 0.057916641 -0.86122346 -1.8413398 -2.9377565 -4.0410957 -4.6705685][-5.6264458 -5.5042214 -4.9492464 -3.9335527 -2.8408327 -1.9152577 -1.0237556 -0.58709383 -0.77085567 -1.4308822 -2.2185636 -3.096674 -3.948441 -4.5853148 -4.7889905][-6.0680771 -6.1356812 -5.88463 -5.0700417 -4.174747 -3.4170709 -2.8084464 -2.5724146 -2.5455589 -2.9439692 -3.5356126 -4.2431149 -4.8144174 -5.1057343 -4.8497152][-5.9511294 -5.928534 -5.8412294 -5.3563294 -4.772944 -4.2124257 -3.749861 -3.7680812 -3.8777182 -4.0841513 -4.3592463 -4.7279687 -5.0717888 -5.0765638 -4.6046391][-5.2299004 -4.9529147 -4.7433753 -4.5634027 -4.3651338 -4.0981464 -3.8970876 -4.00247 -4.1636081 -4.302166 -4.3883662 -4.4124737 -4.4310622 -4.2904606 -3.8606346][-4.4681435 -4.1978927 -4.0558324 -3.9390781 -3.8810363 -3.8003349 -3.7422459 -3.8447933 -3.9776032 -4.0938892 -4.1035542 -4.011971 -3.9208083 -3.7644584 -3.4417622]]...]
INFO - root - 2017-12-16 06:55:43.033060: step 41210, loss = 0.29, batch loss = 0.23 (27.7 examples/sec; 0.289 sec/batch; 23h:20m:51s remains)
INFO - root - 2017-12-16 06:55:45.924234: step 41220, loss = 0.25, batch loss = 0.19 (28.7 examples/sec; 0.279 sec/batch; 22h:32m:33s remains)
INFO - root - 2017-12-16 06:55:48.716878: step 41230, loss = 0.35, batch loss = 0.29 (29.1 examples/sec; 0.275 sec/batch; 22h:14m:48s remains)
INFO - root - 2017-12-16 06:55:51.531369: step 41240, loss = 0.25, batch loss = 0.19 (28.7 examples/sec; 0.278 sec/batch; 22h:31m:38s remains)
INFO - root - 2017-12-16 06:55:54.396286: step 41250, loss = 0.31, batch loss = 0.25 (29.1 examples/sec; 0.275 sec/batch; 22h:14m:37s remains)
INFO - root - 2017-12-16 06:55:57.231111: step 41260, loss = 0.22, batch loss = 0.16 (28.8 examples/sec; 0.278 sec/batch; 22h:29m:56s remains)
INFO - root - 2017-12-16 06:56:00.030578: step 41270, loss = 0.47, batch loss = 0.42 (28.3 examples/sec; 0.283 sec/batch; 22h:52m:58s remains)
INFO - root - 2017-12-16 06:56:02.869790: step 41280, loss = 0.23, batch loss = 0.18 (27.9 examples/sec; 0.287 sec/batch; 23h:10m:40s remains)
INFO - root - 2017-12-16 06:56:05.704277: step 41290, loss = 0.30, batch loss = 0.24 (27.5 examples/sec; 0.291 sec/batch; 23h:32m:52s remains)
INFO - root - 2017-12-16 06:56:08.536050: step 41300, loss = 0.28, batch loss = 0.22 (28.8 examples/sec; 0.277 sec/batch; 22h:26m:34s remains)
2017-12-16 06:56:08.970352: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7148366 -2.6076469 -2.5652397 -2.7842808 -2.9997578 -3.305608 -3.7274387 -4.2008753 -4.5466838 -4.5506897 -4.2083421 -3.80272 -3.3137856 -2.7807903 -2.3103039][-2.3681021 -2.4697943 -2.528007 -2.8168464 -2.9945648 -3.0888348 -3.3407445 -3.541465 -3.6970417 -3.6266828 -3.3225079 -2.88468 -2.2974696 -1.8244231 -1.4531116][-2.4161754 -2.6595025 -2.7922263 -3.0818751 -3.1313655 -3.0315723 -3.0157466 -3.1293545 -3.2306013 -3.0527234 -2.7964816 -2.4730659 -1.9982879 -1.411592 -1.0616233][-2.6864352 -3.1072493 -3.2903192 -3.3116088 -3.0292592 -2.462347 -1.9364691 -1.8511469 -1.9582331 -2.0043538 -1.9098802 -1.7648776 -1.8008773 -1.5754728 -1.3715215][-3.2898083 -3.6081696 -3.5100579 -3.08039 -2.238657 -1.1209388 -0.15444469 0.20377874 -0.068241596 -0.50884175 -0.90652514 -1.1180959 -1.4446292 -1.5816898 -1.6190128][-3.519187 -3.7064276 -3.3082914 -2.3804026 -1.0807507 0.47930145 1.7841902 2.3071485 1.957438 1.1969423 0.31358194 -0.3887291 -1.1305139 -1.5106027 -1.5790353][-3.518573 -3.5094838 -2.7394152 -1.3274651 0.27892971 2.1341352 3.8028708 4.3909454 3.7969294 2.5978131 1.274035 0.14410162 -0.89387107 -1.4146354 -1.5798109][-3.5591834 -3.4343104 -2.4951634 -0.72701359 1.22433 3.1800528 4.832674 5.5783691 5.0194082 3.353744 1.5254827 0.12708473 -0.98637009 -1.5407522 -1.677484][-3.4421074 -3.3636036 -2.5457211 -0.78580141 1.1641946 3.0944424 4.5863352 5.143836 4.5229483 2.8916626 1.0872197 -0.34868431 -1.3671618 -1.8717873 -1.9036598][-3.3097672 -3.3682957 -2.8303249 -1.4610572 0.11593723 1.8757 3.1273494 3.5260253 2.9264741 1.4614491 -0.2541523 -1.7160125 -2.5781674 -2.7321491 -2.6019943][-3.5010424 -3.6558769 -3.271698 -2.2371175 -0.98977828 0.37872076 1.3009725 1.4808145 0.94781828 -0.31273937 -1.8233814 -3.0969696 -3.8728142 -3.9156694 -3.6561296][-3.9450173 -4.1298928 -4.0163465 -3.2072232 -2.1272118 -1.0403612 -0.54696894 -0.69483185 -1.1770422 -2.1951404 -3.4202368 -4.5060892 -5.117023 -5.1079421 -4.6586361][-4.1609888 -4.24548 -4.2096405 -3.9005742 -3.4150562 -2.5140347 -1.9727933 -2.1097214 -2.6106641 -3.3843384 -4.3069525 -5.2234826 -5.8002982 -5.8660455 -5.3869405][-4.2700248 -4.1101961 -3.8864751 -3.7151272 -3.3894529 -2.8730993 -2.5148697 -2.6676168 -3.226151 -3.845355 -4.5776873 -5.3487606 -5.8289633 -5.5432253 -4.8450174][-4.1740379 -3.9338181 -3.6753266 -3.5413508 -3.3040838 -2.8484778 -2.5159879 -2.5972555 -2.9526861 -3.6260755 -4.3946147 -4.9212389 -5.263875 -5.0495968 -4.2701478]]...]
INFO - root - 2017-12-16 06:56:11.769778: step 41310, loss = 0.25, batch loss = 0.19 (27.7 examples/sec; 0.289 sec/batch; 23h:21m:40s remains)
INFO - root - 2017-12-16 06:56:14.603012: step 41320, loss = 0.35, batch loss = 0.29 (28.9 examples/sec; 0.277 sec/batch; 22h:25m:20s remains)
INFO - root - 2017-12-16 06:56:17.379010: step 41330, loss = 0.29, batch loss = 0.23 (29.2 examples/sec; 0.274 sec/batch; 22h:07m:52s remains)
INFO - root - 2017-12-16 06:56:20.170481: step 41340, loss = 0.33, batch loss = 0.27 (29.3 examples/sec; 0.273 sec/batch; 22h:06m:35s remains)
INFO - root - 2017-12-16 06:56:23.012807: step 41350, loss = 0.23, batch loss = 0.17 (28.6 examples/sec; 0.279 sec/batch; 22h:35m:51s remains)
INFO - root - 2017-12-16 06:56:25.864868: step 41360, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 23h:07m:30s remains)
INFO - root - 2017-12-16 06:56:28.715174: step 41370, loss = 0.31, batch loss = 0.25 (28.5 examples/sec; 0.280 sec/batch; 22h:39m:49s remains)
INFO - root - 2017-12-16 06:56:31.511870: step 41380, loss = 0.27, batch loss = 0.21 (29.0 examples/sec; 0.275 sec/batch; 22h:16m:24s remains)
INFO - root - 2017-12-16 06:56:34.350065: step 41390, loss = 0.34, batch loss = 0.28 (29.0 examples/sec; 0.276 sec/batch; 22h:20m:29s remains)
INFO - root - 2017-12-16 06:56:37.171243: step 41400, loss = 0.30, batch loss = 0.24 (28.6 examples/sec; 0.280 sec/batch; 22h:36m:39s remains)
2017-12-16 06:56:37.631626: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.848242 -2.3585749 -2.8457153 -3.1872292 -3.6243379 -3.9964085 -4.3365602 -4.4765339 -4.2394729 -4.1592045 -3.8134363 -3.253325 -2.6914191 -2.2781768 -2.1426706][-2.6497872 -3.1571498 -3.7210071 -3.9450002 -4.2228642 -4.5791264 -4.6812057 -4.792315 -4.6766214 -4.5673351 -4.2587981 -3.6086793 -2.9766009 -2.4920897 -2.2325251][-4.4466987 -4.8324089 -4.9987969 -4.9000225 -4.7178655 -4.5576444 -4.5352335 -4.7126422 -4.661263 -4.6153126 -4.4567175 -3.8877902 -3.3236308 -2.8708839 -2.6448102][-6.1134353 -6.50039 -6.5211 -5.9407234 -5.0902081 -4.32581 -3.7910023 -3.5696943 -3.4783096 -3.6404293 -3.71456 -3.4595914 -3.1809492 -2.8954902 -2.8085597][-7.2843971 -7.6356807 -7.4792519 -6.6506624 -5.2844114 -3.9482462 -2.8863769 -2.3711746 -2.1242294 -2.2847061 -2.5956101 -2.7230718 -2.9366279 -3.0605223 -3.1064794][-7.116971 -7.6209064 -7.4690065 -6.4210391 -4.5448694 -2.6688027 -1.3217468 -0.7922287 -0.52574825 -0.82335019 -1.4676111 -1.9718738 -2.5100572 -2.9395628 -3.2155557][-6.115592 -6.6727448 -6.5874434 -5.4678178 -3.3610418 -1.2032697 0.41155958 1.1271152 1.2938037 0.71569824 -0.25901842 -1.1836793 -2.0876341 -2.7178662 -3.0456691][-4.8962188 -5.37202 -5.2925458 -4.2911682 -2.3860087 -0.33088779 1.3492098 2.1279778 2.2503142 1.6547155 0.61672354 -0.5233345 -1.5900228 -2.445529 -2.9197626][-3.9791853 -4.4350533 -4.4580536 -3.6992741 -2.2521234 -0.43885326 1.0174284 1.5817266 1.6599331 1.0235214 -0.013279438 -1.1629996 -2.1009018 -2.7438822 -3.0888634][-3.0909498 -3.7800217 -4.0689654 -3.5138402 -2.4967852 -1.2622712 -0.31801462 0.097238064 0.1502409 -0.50444078 -1.5220013 -2.5823712 -3.343523 -3.570349 -3.4734893][-2.4324784 -3.1951196 -3.7454381 -3.4764271 -2.8137515 -2.0020308 -1.3707049 -1.3484323 -1.7498841 -2.4047456 -3.193682 -4.0663557 -4.480022 -4.2957473 -3.8419623][-2.1136739 -2.7668898 -3.3596721 -3.5219536 -3.393537 -2.9393249 -2.6217384 -2.7667556 -3.1904061 -3.823884 -4.5756793 -5.4036059 -5.6166654 -5.0989537 -4.3410954][-1.5208941 -2.1763742 -2.9822047 -3.4254053 -3.59992 -3.503705 -3.3411937 -3.3399148 -3.5274239 -4.3414388 -5.240293 -5.8624411 -5.9934311 -5.4190769 -4.3876772][-1.146071 -1.5180275 -2.3200788 -2.9097977 -3.1216846 -2.9601812 -2.7441015 -2.7353694 -2.9878566 -3.6575029 -4.5727506 -5.643559 -6.13307 -5.5063639 -4.291605][-1.0628431 -1.0737605 -1.6354485 -2.2489374 -2.4271975 -2.2391648 -1.9621959 -1.9790456 -2.2812984 -2.759366 -3.5953321 -4.6139784 -5.0587926 -4.77572 -3.8703609]]...]
INFO - root - 2017-12-16 06:56:40.466359: step 41410, loss = 0.31, batch loss = 0.25 (29.4 examples/sec; 0.272 sec/batch; 22h:01m:35s remains)
INFO - root - 2017-12-16 06:56:43.291188: step 41420, loss = 0.20, batch loss = 0.14 (29.3 examples/sec; 0.273 sec/batch; 22h:05m:55s remains)
INFO - root - 2017-12-16 06:56:46.123486: step 41430, loss = 0.25, batch loss = 0.19 (28.6 examples/sec; 0.279 sec/batch; 22h:34m:43s remains)
INFO - root - 2017-12-16 06:56:48.957655: step 41440, loss = 0.25, batch loss = 0.19 (29.4 examples/sec; 0.272 sec/batch; 22h:00m:27s remains)
INFO - root - 2017-12-16 06:56:51.793740: step 41450, loss = 0.18, batch loss = 0.12 (28.1 examples/sec; 0.284 sec/batch; 22h:58m:50s remains)
INFO - root - 2017-12-16 06:56:54.682677: step 41460, loss = 0.22, batch loss = 0.16 (28.1 examples/sec; 0.284 sec/batch; 22h:59m:24s remains)
INFO - root - 2017-12-16 06:56:57.485019: step 41470, loss = 0.24, batch loss = 0.18 (29.0 examples/sec; 0.276 sec/batch; 22h:16m:55s remains)
INFO - root - 2017-12-16 06:57:00.334131: step 41480, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.283 sec/batch; 22h:55m:00s remains)
INFO - root - 2017-12-16 06:57:03.173979: step 41490, loss = 0.33, batch loss = 0.27 (28.2 examples/sec; 0.283 sec/batch; 22h:54m:02s remains)
INFO - root - 2017-12-16 06:57:06.006190: step 41500, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.285 sec/batch; 22h:59m:49s remains)
2017-12-16 06:57:06.467452: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8148727 -5.034524 -5.1229157 -5.2987018 -5.3765774 -5.6682873 -6.28997 -7.1568055 -7.7970276 -7.7007775 -7.150466 -6.0832973 -4.7625823 -3.564373 -2.9995909][-5.31248 -5.0947084 -4.7500157 -4.5930529 -4.6137838 -4.9250617 -5.5158687 -6.5154285 -7.6008906 -8.0952387 -7.915833 -7.0881562 -6.0541253 -4.8189497 -4.109736][-6.33602 -5.904489 -5.0457258 -4.1944084 -3.6456764 -3.5347414 -3.8620749 -4.8393121 -6.085928 -7.3223848 -7.9648061 -7.7138739 -7.0714436 -5.9162345 -5.0423331][-6.7851238 -6.0906925 -4.9253569 -3.7342563 -2.5552721 -1.8651061 -1.7073076 -2.3224549 -3.5588193 -5.192955 -6.6510334 -7.5809183 -7.643343 -6.6870661 -5.6440706][-5.8888497 -4.9688492 -3.6007569 -2.1779311 -0.65596151 0.55408478 1.1984005 0.92990637 -0.27589226 -2.140198 -4.3414388 -6.0529776 -6.8250079 -6.560092 -5.567719][-4.6512332 -3.4118142 -1.7946136 -0.059843063 1.6568189 3.1439934 4.226038 4.2761469 3.3401017 1.1777911 -1.4957991 -3.8963428 -5.4974718 -5.84854 -5.1601954][-4.0777326 -2.7286367 -0.98325491 1.0631232 3.1230335 4.949975 6.4438906 6.9370842 6.3729849 4.1228142 0.94399548 -2.0533283 -4.2079878 -5.0743828 -4.7719955][-4.2530117 -3.0045583 -1.4051743 0.65470982 2.8568263 4.9138517 6.6386423 7.4474249 7.2367964 5.2338047 2.0892606 -1.246033 -3.7698085 -4.8164487 -4.58421][-4.9549894 -3.8591208 -2.4357576 -0.83567595 0.94912338 3.0342441 4.9668026 6.0274944 5.9589224 4.1600075 1.340364 -1.6962366 -3.9957507 -5.1303458 -4.9881926][-6.2436061 -5.4681654 -4.4341664 -3.1940644 -1.5525041 0.12669802 1.6091876 2.7376504 2.8517289 1.4054275 -0.755183 -3.1178331 -4.7815571 -5.5549951 -5.4590788][-7.8408985 -7.3107524 -6.55303 -5.6570296 -4.6457348 -3.2448809 -1.8468702 -1.2830558 -1.3198524 -2.0418797 -3.3070798 -4.7934818 -5.7135277 -5.9899364 -5.7962685][-8.803997 -8.7009687 -8.4542236 -7.7066517 -6.9797993 -6.2111058 -5.3216529 -4.7358451 -4.6401205 -5.0015497 -5.4256096 -5.959166 -6.2904005 -6.2726588 -6.0354948][-8.47789 -8.6669979 -8.6821518 -8.778635 -8.8560171 -8.4191723 -7.9782591 -7.6493435 -7.3639264 -7.064 -6.75872 -6.4334445 -6.1649661 -6.0653028 -5.8625908][-6.9486976 -7.3431516 -7.6344757 -8.04204 -8.544199 -8.805027 -8.963335 -8.8031988 -8.3814459 -7.7200761 -6.9817123 -6.3739839 -5.8539934 -5.3968863 -5.1279864][-5.609128 -5.7629328 -6.1398234 -6.6617527 -7.2375569 -7.7559228 -8.0474186 -8.0008745 -7.65003 -7.0273762 -6.2511749 -5.4693537 -4.8970022 -4.5787044 -4.3134065]]...]
INFO - root - 2017-12-16 06:57:09.325754: step 41510, loss = 0.29, batch loss = 0.23 (28.8 examples/sec; 0.278 sec/batch; 22h:27m:24s remains)
INFO - root - 2017-12-16 06:57:12.186156: step 41520, loss = 0.22, batch loss = 0.17 (27.9 examples/sec; 0.287 sec/batch; 23h:10m:58s remains)
INFO - root - 2017-12-16 06:57:15.018749: step 41530, loss = 0.22, batch loss = 0.16 (27.3 examples/sec; 0.293 sec/batch; 23h:40m:10s remains)
INFO - root - 2017-12-16 06:57:17.824862: step 41540, loss = 0.28, batch loss = 0.22 (29.1 examples/sec; 0.275 sec/batch; 22h:11m:25s remains)
INFO - root - 2017-12-16 06:57:20.664908: step 41550, loss = 0.33, batch loss = 0.27 (28.0 examples/sec; 0.286 sec/batch; 23h:05m:09s remains)
INFO - root - 2017-12-16 06:57:23.489126: step 41560, loss = 0.27, batch loss = 0.21 (29.4 examples/sec; 0.272 sec/batch; 21h:57m:32s remains)
INFO - root - 2017-12-16 06:57:26.319534: step 41570, loss = 0.17, batch loss = 0.11 (28.5 examples/sec; 0.281 sec/batch; 22h:43m:14s remains)
INFO - root - 2017-12-16 06:57:29.101909: step 41580, loss = 0.25, batch loss = 0.20 (26.8 examples/sec; 0.298 sec/batch; 24h:07m:09s remains)
INFO - root - 2017-12-16 06:57:31.933703: step 41590, loss = 0.25, batch loss = 0.20 (28.8 examples/sec; 0.277 sec/batch; 22h:25m:18s remains)
INFO - root - 2017-12-16 06:57:34.759149: step 41600, loss = 0.31, batch loss = 0.25 (28.9 examples/sec; 0.277 sec/batch; 22h:20m:35s remains)
2017-12-16 06:57:35.232917: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9023423 -6.0956392 -6.2036209 -5.8524437 -5.5257554 -4.9808035 -4.5780582 -4.4595656 -4.5987811 -5.212451 -5.8671179 -6.7146645 -7.4726639 -7.9701519 -7.7367954][-6.7088771 -7.1161547 -7.2510123 -6.854598 -6.3006554 -5.4744177 -4.6445379 -4.1680446 -4.2145419 -5.143878 -6.0932264 -6.8271017 -7.5095072 -8.0057278 -7.6843758][-6.9752426 -7.3126364 -7.19798 -6.6292224 -5.9256525 -5.0311284 -4.0114665 -3.6485727 -3.6371779 -4.2948146 -5.0896425 -6.0478473 -7.037231 -7.375658 -7.1234217][-6.2210236 -6.5198307 -6.1782908 -5.2166429 -4.2019424 -3.2227936 -2.2562873 -2.247489 -2.5958953 -3.0911703 -3.5466909 -4.26316 -5.0219536 -5.8093758 -5.9113159][-4.9049239 -4.8779292 -4.2444158 -3.3473768 -2.4773748 -1.4673958 -0.55125928 -0.27630997 -0.45928669 -1.0620511 -1.7204857 -1.9798083 -2.7358546 -3.7232878 -4.2759933][-3.7988131 -3.1339688 -1.8667634 -0.48950744 0.72470808 1.2704258 1.6495652 1.5934739 1.2944899 0.71676254 0.082930088 -0.55091143 -1.5703201 -2.5461102 -3.2082925][-2.8050656 -1.8862054 -0.54185414 1.0313516 2.5545392 3.3400974 3.6373196 3.1747813 2.520153 1.7684903 1.0639853 0.12943268 -0.94118023 -2.2230964 -3.0953269][-2.7690289 -1.6595569 -0.12757826 1.4808364 2.7534542 3.52812 3.9134941 3.6005707 2.8558426 1.8668909 0.79687643 -0.26288414 -1.4321268 -2.741437 -3.5997148][-3.5081396 -2.667479 -1.3760173 0.080814362 1.5288963 2.4083924 2.8120365 2.581707 1.9059496 0.90244865 -0.20186853 -1.3481271 -2.5339594 -3.6312544 -4.2901587][-4.1023812 -3.8420894 -3.3066566 -2.2744594 -1.0182769 -0.045472622 0.64717054 0.62330389 0.043987274 -0.81434393 -1.7703314 -2.9392314 -4.0608759 -4.84231 -5.0286078][-5.7978568 -5.394012 -4.6927238 -4.2037373 -3.6210475 -2.7531998 -1.9617503 -1.753886 -1.9283578 -2.5274122 -3.2421467 -4.0725765 -4.8090577 -5.1755071 -5.0185509][-6.6344337 -6.5753207 -6.372263 -5.9069924 -5.3717918 -4.8519206 -4.4848156 -4.1213317 -3.8850131 -4.0064797 -4.2379193 -4.6168127 -4.9532461 -4.9623857 -4.69119][-7.1895504 -7.1264925 -7.1685338 -7.0064483 -6.7629957 -6.3712759 -6.0872631 -5.8346958 -5.7056494 -5.5154247 -5.3125768 -5.21998 -5.1684623 -4.8680329 -4.3954296][-6.8348808 -6.6787357 -6.7322016 -6.7192974 -6.6642418 -6.4932823 -6.3251228 -6.2470284 -6.1065903 -5.8592615 -5.620327 -5.3382835 -5.023994 -4.713335 -4.2500038][-6.1791029 -5.76894 -5.61394 -5.6969166 -5.8412275 -5.9233556 -5.9459348 -5.8881378 -5.7453432 -5.5283952 -5.3044338 -4.9960923 -4.7474 -4.4847188 -4.0883317]]...]
INFO - root - 2017-12-16 06:57:38.088242: step 41610, loss = 0.23, batch loss = 0.17 (27.5 examples/sec; 0.291 sec/batch; 23h:32m:27s remains)
INFO - root - 2017-12-16 06:57:40.953089: step 41620, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 23h:04m:55s remains)
INFO - root - 2017-12-16 06:57:43.764001: step 41630, loss = 0.24, batch loss = 0.18 (29.0 examples/sec; 0.276 sec/batch; 22h:18m:51s remains)
INFO - root - 2017-12-16 06:57:46.631619: step 41640, loss = 0.28, batch loss = 0.22 (27.0 examples/sec; 0.296 sec/batch; 23h:54m:49s remains)
INFO - root - 2017-12-16 06:57:49.453213: step 41650, loss = 0.26, batch loss = 0.20 (28.2 examples/sec; 0.284 sec/batch; 22h:56m:00s remains)
INFO - root - 2017-12-16 06:57:52.336025: step 41660, loss = 0.25, batch loss = 0.19 (28.9 examples/sec; 0.277 sec/batch; 22h:23m:12s remains)
INFO - root - 2017-12-16 06:57:55.179116: step 41670, loss = 0.25, batch loss = 0.19 (29.1 examples/sec; 0.275 sec/batch; 22h:14m:42s remains)
INFO - root - 2017-12-16 06:57:58.036132: step 41680, loss = 0.29, batch loss = 0.23 (26.3 examples/sec; 0.304 sec/batch; 24h:32m:51s remains)
INFO - root - 2017-12-16 06:58:00.868585: step 41690, loss = 0.20, batch loss = 0.14 (29.0 examples/sec; 0.276 sec/batch; 22h:17m:46s remains)
INFO - root - 2017-12-16 06:58:03.731568: step 41700, loss = 0.22, batch loss = 0.17 (28.6 examples/sec; 0.280 sec/batch; 22h:37m:45s remains)
2017-12-16 06:58:04.178515: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0274253 -6.1348114 -6.113369 -6.2826605 -6.5754786 -6.6536112 -6.533247 -7.0826936 -7.3439312 -7.2898045 -6.8900881 -6.2738237 -5.6034822 -4.7812486 -4.6235089][-7.0837288 -6.6958323 -6.6103497 -6.5641885 -6.601706 -7.2287731 -7.6885152 -8.0026646 -8.0411167 -8.1816788 -8.2989044 -8.1314754 -7.744339 -6.9710617 -6.4507608][-7.2741489 -7.1631393 -6.6024542 -6.3361669 -6.515934 -6.4619775 -6.0752382 -6.6459017 -7.3822632 -8.081378 -8.3347416 -8.6869354 -8.8990049 -8.6325569 -8.07126][-7.4503069 -6.899353 -6.4251113 -5.7728109 -4.9072008 -4.6758652 -4.3017921 -4.1928592 -4.6189532 -5.9099674 -7.5802541 -8.7139988 -9.2743149 -9.2760658 -8.755846][-6.5901155 -6.1371183 -5.4141865 -4.2209511 -3.2468925 -2.2519259 -1.0911975 -0.57126379 -0.90338111 -2.7509308 -5.0099449 -7.0406022 -8.5166759 -8.9596634 -8.345046][-5.0425849 -4.2907257 -3.3365302 -1.9175806 -0.18199778 1.2964888 2.7464051 3.589159 3.1112432 0.87672949 -2.0104437 -4.5900183 -6.4154787 -7.691319 -7.5972977][-4.2266459 -3.7091217 -2.4760506 -0.40564346 2.0820246 4.1478825 5.8722258 6.8704853 6.4463873 3.9181404 0.57109404 -2.3702614 -4.4236279 -5.5637851 -5.7167][-4.3021364 -3.7841973 -2.9880548 -0.96750069 1.8110938 4.3790512 6.5382519 7.4673615 6.8886747 4.5011387 1.4591074 -1.2364821 -2.9509168 -4.0579748 -4.5197878][-4.6235433 -4.3686004 -3.853585 -2.432332 -0.18127966 2.2227263 4.2874203 5.2524605 4.9567366 3.27958 0.8211236 -1.5691111 -3.2045698 -3.7369585 -3.6295593][-5.1664748 -5.0985432 -5.0893245 -4.2395077 -2.8147626 -1.0214093 0.56264114 1.475327 1.4318447 0.14962244 -1.5497384 -3.2983613 -4.24867 -4.3397517 -4.25596][-5.97892 -6.345293 -6.5998306 -6.475132 -5.8645339 -4.6691861 -3.3093004 -2.6072812 -2.5761497 -3.221766 -4.1523123 -5.1172848 -5.7042847 -5.6769943 -5.33695][-6.3307614 -6.8780322 -7.5733981 -7.7040124 -7.5183773 -6.9561148 -6.3331614 -5.6629477 -5.3856411 -5.4919486 -5.8723459 -6.6082373 -6.9136486 -6.5018034 -6.0380616][-5.5037594 -6.259285 -7.1102328 -7.6757803 -7.8438044 -7.3092117 -6.7240028 -6.3946114 -6.15316 -6.141202 -6.2973266 -6.6274405 -6.8184028 -6.7279296 -6.463273][-4.9729366 -5.4366121 -6.0491729 -6.5807571 -6.8870344 -6.5498714 -6.0582056 -5.7334704 -5.7035513 -5.7127929 -5.6419311 -5.8398671 -6.0866961 -6.0001178 -5.6923132][-4.5213904 -4.6827888 -4.8166523 -5.0903096 -5.2828441 -5.195282 -4.9894185 -4.711185 -4.4674087 -4.5731368 -4.8830523 -4.9420338 -4.8628592 -4.9623079 -4.996851]]...]
INFO - root - 2017-12-16 06:58:06.975883: step 41710, loss = 0.26, batch loss = 0.21 (28.9 examples/sec; 0.277 sec/batch; 22h:21m:58s remains)
INFO - root - 2017-12-16 06:58:09.804839: step 41720, loss = 0.29, batch loss = 0.24 (29.1 examples/sec; 0.275 sec/batch; 22h:13m:42s remains)
INFO - root - 2017-12-16 06:58:12.655528: step 41730, loss = 0.24, batch loss = 0.18 (29.1 examples/sec; 0.275 sec/batch; 22h:11m:19s remains)
INFO - root - 2017-12-16 06:58:15.561760: step 41740, loss = 0.24, batch loss = 0.18 (26.8 examples/sec; 0.299 sec/batch; 24h:08m:45s remains)
INFO - root - 2017-12-16 06:58:18.382125: step 41750, loss = 0.29, batch loss = 0.23 (28.8 examples/sec; 0.278 sec/batch; 22h:25m:08s remains)
INFO - root - 2017-12-16 06:58:21.245639: step 41760, loss = 0.23, batch loss = 0.17 (28.8 examples/sec; 0.278 sec/batch; 22h:28m:18s remains)
INFO - root - 2017-12-16 06:58:24.011673: step 41770, loss = 0.27, batch loss = 0.21 (29.3 examples/sec; 0.273 sec/batch; 22h:03m:29s remains)
INFO - root - 2017-12-16 06:58:26.857765: step 41780, loss = 0.21, batch loss = 0.15 (29.6 examples/sec; 0.271 sec/batch; 21h:51m:18s remains)
INFO - root - 2017-12-16 06:58:29.685868: step 41790, loss = 0.26, batch loss = 0.21 (29.5 examples/sec; 0.271 sec/batch; 21h:54m:10s remains)
INFO - root - 2017-12-16 06:58:32.536613: step 41800, loss = 0.34, batch loss = 0.28 (28.1 examples/sec; 0.285 sec/batch; 23h:01m:37s remains)
2017-12-16 06:58:33.009374: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.233345 -3.1957145 -3.019918 -2.6192317 -2.2912889 -2.2111647 -2.5243654 -2.6499181 -2.7158225 -2.7491183 -2.5392518 -2.222605 -1.9589746 -1.6684551 -1.4671824][-2.6335759 -2.38929 -2.0781252 -2.0698137 -2.3333659 -2.3290653 -2.5291057 -3.0855765 -3.2536912 -2.8623126 -2.5704679 -2.3145204 -2.0484049 -1.830987 -1.5848908][-2.0651855 -1.9105513 -1.7993646 -1.8178089 -2.0285964 -2.547349 -3.109931 -3.4024363 -3.4708965 -3.4991589 -3.153301 -2.6865454 -2.4597704 -2.2921727 -2.0706687][-1.5737331 -1.5301847 -1.5976002 -1.8376682 -2.2087126 -2.4901519 -2.5781255 -3.0157328 -3.2780075 -3.2953758 -3.3145876 -3.1227913 -2.8982978 -2.7567508 -2.6053863][-1.7383382 -1.5380549 -1.3927693 -1.4490144 -1.5661349 -1.8177903 -1.9440272 -2.0518556 -2.1717122 -2.5315704 -2.7809134 -2.8432255 -2.8963232 -2.7761877 -2.6394129][-1.6956184 -1.6401947 -1.2938466 -0.85861135 -0.57463193 -0.45890164 -0.345675 -0.56177449 -0.87948132 -1.2977381 -1.6953654 -1.9642861 -2.1556714 -2.277715 -2.3322718][-1.7951531 -1.6527572 -1.2234168 -0.5259757 0.26911497 0.80126953 1.0983105 1.0184374 0.8060379 0.20816278 -0.42546368 -0.89586592 -1.2802751 -1.3453693 -1.3792946][-2.2209365 -1.8085744 -1.3301785 -0.57681084 0.38253546 1.1714478 1.7913408 1.9541941 1.7279801 1.2238979 0.59506845 0.048683643 -0.33835745 -0.45627928 -0.4126482][-2.7995534 -2.368243 -1.6917462 -0.80807376 0.005317688 0.909214 1.750886 1.9998889 1.8952794 1.594708 1.0948758 0.44646168 -0.057220936 -0.036677837 0.20455456][-3.099977 -2.9465051 -2.4926326 -1.5729122 -0.71541095 0.044552803 0.67020988 1.0206327 1.0318542 0.78474903 0.62455654 0.34587336 -0.034062386 -0.07559824 0.15752745][-3.1564775 -3.0624151 -2.7398715 -2.1418118 -1.6634908 -1.0742586 -0.53221893 -0.37536383 -0.33815956 -0.34731913 -0.39167881 -0.43452907 -0.4129982 -0.25990725 0.015946388][-3.3844261 -3.0773485 -2.8026228 -2.3151674 -1.9223695 -1.7855268 -1.8099079 -1.8641634 -1.8429792 -1.7762537 -1.5681977 -1.3856688 -1.2451282 -0.98038054 -0.66699672][-3.3632891 -3.0798576 -2.8263617 -2.4402134 -2.2706292 -2.2349689 -2.447823 -3.020401 -3.3540177 -3.2465868 -2.8616037 -2.4287238 -2.0942736 -1.7037232 -1.3523386][-3.2122583 -2.810679 -2.6008458 -2.3477645 -2.2452562 -2.4415245 -2.9554341 -3.5806279 -4.03675 -4.2267113 -4.0545349 -3.5542269 -2.9772794 -2.4207909 -1.9923167][-3.0711784 -2.5581145 -2.2529252 -2.1287816 -2.1632237 -2.4658675 -3.1839309 -4.01016 -4.569571 -4.74547 -4.6071668 -4.1823473 -3.6530375 -3.0417752 -2.4480939]]...]
INFO - root - 2017-12-16 06:58:35.816004: step 41810, loss = 0.20, batch loss = 0.14 (29.3 examples/sec; 0.273 sec/batch; 22h:01m:30s remains)
INFO - root - 2017-12-16 06:58:38.641103: step 41820, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.284 sec/batch; 22h:56m:37s remains)
INFO - root - 2017-12-16 06:58:41.511343: step 41830, loss = 0.32, batch loss = 0.26 (28.4 examples/sec; 0.281 sec/batch; 22h:43m:38s remains)
INFO - root - 2017-12-16 06:58:44.324101: step 41840, loss = 0.28, batch loss = 0.22 (26.9 examples/sec; 0.297 sec/batch; 23h:58m:43s remains)
INFO - root - 2017-12-16 06:58:47.123280: step 41850, loss = 0.25, batch loss = 0.19 (29.1 examples/sec; 0.275 sec/batch; 22h:12m:27s remains)
INFO - root - 2017-12-16 06:58:49.951335: step 41860, loss = 0.20, batch loss = 0.14 (28.6 examples/sec; 0.280 sec/batch; 22h:34m:03s remains)
INFO - root - 2017-12-16 06:58:52.766829: step 41870, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.281 sec/batch; 22h:42m:17s remains)
INFO - root - 2017-12-16 06:58:55.628872: step 41880, loss = 0.38, batch loss = 0.33 (29.0 examples/sec; 0.276 sec/batch; 22h:18m:15s remains)
INFO - root - 2017-12-16 06:58:58.437182: step 41890, loss = 0.26, batch loss = 0.21 (27.6 examples/sec; 0.290 sec/batch; 23h:24m:53s remains)
INFO - root - 2017-12-16 06:59:01.237323: step 41900, loss = 0.25, batch loss = 0.20 (28.8 examples/sec; 0.278 sec/batch; 22h:25m:57s remains)
2017-12-16 06:59:01.686555: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1902304 -3.5040584 -4.0447831 -4.6857858 -5.3310676 -5.6459742 -5.9056907 -5.9963222 -5.9712954 -5.6389751 -5.3655519 -5.3096247 -5.5414529 -5.6999645 -5.5987496][-2.42517 -2.7044234 -3.3013191 -4.0532255 -4.7994075 -5.3914042 -5.6859412 -5.48914 -5.2318006 -5.2517853 -5.3299079 -5.4798722 -5.6655278 -5.8203964 -5.7631388][-1.6877801 -1.9038215 -2.4648166 -3.3026109 -3.9246712 -4.101738 -4.2306819 -4.3364615 -4.2680974 -4.2049637 -4.6122637 -5.2873664 -5.8295164 -5.9608603 -5.7824535][-0.62564349 -0.92414188 -1.4440696 -2.1000364 -2.6948824 -2.8688297 -2.6800447 -2.3814478 -2.3613739 -2.774539 -3.4685526 -4.3872414 -5.172431 -5.6042995 -5.581233][0.10758686 0.00076818466 -0.46111679 -1.0258229 -1.2346234 -0.96574974 -0.60301924 -0.22230196 -0.098998547 -0.75543523 -1.9513478 -3.2994025 -4.2241406 -4.7562537 -4.8984394][0.22721052 0.37883282 0.24110079 -0.07749939 -0.21256065 0.34136868 1.077632 1.5593162 1.6642127 1.0903821 -0.23991823 -1.8921535 -3.2592759 -4.0019326 -4.3898659][0.37755823 0.57226944 0.58045387 0.64537191 1.170073 1.9657879 2.79702 3.2069969 3.2337217 2.5533028 1.1188927 -0.42802238 -1.8520541 -3.0820522 -3.8688514][0.011487007 0.2366209 0.10242844 0.29686117 1.2281795 2.4226308 3.56454 4.0836973 3.9520798 3.186502 1.6432586 0.011169434 -1.4894114 -2.7006841 -3.642921][-1.9775071 -1.6782928 -1.3782685 -1.0216062 0.040953636 1.4407878 2.7660289 3.3368292 3.5382214 2.9422431 1.6998277 0.058769226 -1.5685549 -2.9479613 -4.0168481][-3.6048727 -3.8590136 -3.8995154 -3.0464706 -1.6757076 -0.29479933 0.82476854 1.5678568 1.874382 1.4782615 0.39809656 -0.83774137 -2.3651013 -3.7151124 -4.7177391][-5.470284 -5.5195847 -5.6103706 -5.1701112 -3.9842052 -2.6567249 -1.5180862 -0.87290049 -0.66371346 -0.8166461 -1.6858053 -2.7344966 -3.9685595 -5.097578 -5.8656511][-7.2143908 -7.472208 -7.60406 -7.1400003 -6.1648107 -5.1481709 -4.3959994 -3.5610065 -3.1757879 -3.3216536 -4.0333881 -4.7758918 -5.6029868 -6.3229012 -6.6370249][-8.4317636 -9.0681038 -9.3346634 -8.9756784 -8.3466082 -7.4425364 -6.7365227 -6.0615153 -5.798769 -5.7581525 -6.1200495 -6.6917896 -7.1094656 -7.425169 -7.2555985][-8.9099436 -9.6239643 -10.123107 -10.211521 -9.9317255 -9.4118176 -8.8703251 -8.05004 -7.762167 -7.8412571 -8.0573559 -8.0783587 -7.9713225 -7.9694433 -7.552762][-8.5838766 -9.3232536 -9.9773655 -10.252255 -10.226902 -10.063105 -9.9350452 -9.5942593 -9.4369621 -9.2620687 -9.2299786 -9.0113688 -8.61639 -8.1999388 -7.3616829]]...]
INFO - root - 2017-12-16 06:59:04.493013: step 41910, loss = 0.23, batch loss = 0.17 (28.6 examples/sec; 0.280 sec/batch; 22h:36m:47s remains)
INFO - root - 2017-12-16 06:59:07.324042: step 41920, loss = 0.22, batch loss = 0.16 (29.2 examples/sec; 0.274 sec/batch; 22h:06m:23s remains)
INFO - root - 2017-12-16 06:59:10.157000: step 41930, loss = 0.27, batch loss = 0.21 (28.1 examples/sec; 0.285 sec/batch; 22h:59m:45s remains)
INFO - root - 2017-12-16 06:59:13.017082: step 41940, loss = 0.27, batch loss = 0.21 (28.7 examples/sec; 0.279 sec/batch; 22h:31m:49s remains)
INFO - root - 2017-12-16 06:59:15.842495: step 41950, loss = 0.35, batch loss = 0.29 (29.1 examples/sec; 0.275 sec/batch; 22h:10m:54s remains)
INFO - root - 2017-12-16 06:59:18.705464: step 41960, loss = 0.44, batch loss = 0.38 (29.1 examples/sec; 0.275 sec/batch; 22h:13m:13s remains)
INFO - root - 2017-12-16 06:59:21.543730: step 41970, loss = 0.25, batch loss = 0.19 (29.0 examples/sec; 0.276 sec/batch; 22h:16m:13s remains)
INFO - root - 2017-12-16 06:59:24.389457: step 41980, loss = 0.23, batch loss = 0.18 (28.0 examples/sec; 0.286 sec/batch; 23h:03m:34s remains)
INFO - root - 2017-12-16 06:59:27.196623: step 41990, loss = 0.34, batch loss = 0.28 (28.4 examples/sec; 0.282 sec/batch; 22h:44m:08s remains)
INFO - root - 2017-12-16 06:59:30.056062: step 42000, loss = 0.30, batch loss = 0.24 (28.3 examples/sec; 0.283 sec/batch; 22h:49m:38s remains)
2017-12-16 06:59:30.506168: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7472327 -3.9114647 -4.3134809 -4.7942986 -5.3269086 -5.7975779 -6.040668 -6.1392732 -6.3440323 -6.4128885 -6.3432035 -6.2843537 -6.0707955 -5.6004534 -4.9591122][-2.9237854 -3.0701814 -3.4044409 -3.8504958 -4.4174223 -5.0060606 -5.2815289 -5.4227753 -5.6021409 -5.8972983 -6.1891518 -6.1120043 -5.8963933 -5.5484419 -4.8781066][-2.3151712 -2.3828683 -2.5541134 -2.9594402 -3.2835996 -3.7771664 -4.1872582 -4.545188 -4.749331 -4.9150157 -5.1358552 -5.4585509 -5.6771975 -5.3787103 -4.7001319][-1.7919664 -1.8876975 -2.0273492 -1.9820418 -1.6958153 -1.6585858 -1.8073652 -2.2186012 -2.8277164 -3.5877583 -4.1403413 -4.5506744 -4.8359785 -4.7652178 -4.416687][-1.9369628 -1.5474887 -0.87466431 -0.39120626 0.16051769 0.82971954 1.2069783 0.85896397 -0.17021322 -1.4271755 -2.6887636 -3.7140336 -4.3459115 -4.4077573 -4.0776815][-2.4931493 -1.7280672 -0.583472 0.93584442 2.48838 3.2159944 3.455265 3.2323117 2.2461057 0.71856594 -1.0945992 -2.6349311 -3.6404748 -3.940577 -3.7657874][-3.3865767 -2.3250711 -0.93711305 1.1000228 3.1661658 4.6714973 5.5065403 5.0033121 3.6850319 2.0316124 0.24329472 -1.4340947 -2.8315668 -3.4728758 -3.5614433][-4.75795 -3.5911603 -1.9004807 0.3676219 2.5283213 4.5282269 5.756835 5.6042833 4.4272842 2.4810338 0.526453 -1.2537637 -2.6436522 -3.1850443 -3.2844081][-6.5039315 -5.6335659 -4.0000353 -1.7863889 0.47451162 2.6658163 4.1467667 4.450779 3.453701 1.7087197 0.085421085 -1.5301995 -2.8493385 -3.4104908 -3.4389932][-7.5911431 -7.5274487 -6.8070841 -4.8932943 -2.6332011 -0.56464124 1.1617107 2.0145102 1.6395712 0.46398973 -0.89162707 -2.3382914 -3.5321076 -4.07828 -4.1553836][-8.50806 -8.4368591 -7.99148 -6.9866228 -5.5767508 -3.6842487 -1.7893703 -0.75253057 -0.58942151 -1.3297431 -2.3391731 -3.4547133 -4.4269223 -4.8128514 -4.68658][-8.2555141 -8.6013336 -8.5129776 -7.691926 -6.6594625 -5.4349275 -4.1118536 -3.2176971 -2.6851759 -2.9164307 -3.4706285 -4.2677794 -4.7538414 -4.8814545 -4.7233515][-7.2458892 -7.7414856 -7.8160067 -7.3464003 -6.6729603 -5.7826486 -4.854938 -4.22471 -3.6498265 -3.6303875 -3.8583479 -4.4057021 -4.8394856 -4.9464312 -4.7074337][-6.0785437 -6.2219143 -6.2127547 -5.9455061 -5.5343227 -4.7508316 -4.0014687 -3.8392248 -3.6579494 -3.5558915 -3.6602693 -4.2101989 -4.6775918 -4.8257151 -4.6900582][-5.0169315 -4.8427978 -4.6212258 -4.31914 -4.0254884 -3.6162477 -3.2168138 -3.0357661 -2.9485846 -3.1039286 -3.3628974 -3.7528534 -4.0534797 -4.2425389 -4.2191467]]...]
INFO - root - 2017-12-16 06:59:33.315585: step 42010, loss = 0.23, batch loss = 0.17 (27.9 examples/sec; 0.287 sec/batch; 23h:09m:18s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 06:59:36.148605: step 42020, loss = 0.32, batch loss = 0.26 (28.5 examples/sec; 0.280 sec/batch; 22h:37m:23s remains)
INFO - root - 2017-12-16 06:59:39.027316: step 42030, loss = 0.19, batch loss = 0.14 (27.8 examples/sec; 0.287 sec/batch; 23h:10m:40s remains)
INFO - root - 2017-12-16 06:59:41.863453: step 42040, loss = 0.28, batch loss = 0.22 (27.1 examples/sec; 0.296 sec/batch; 23h:51m:15s remains)
INFO - root - 2017-12-16 06:59:44.746047: step 42050, loss = 0.28, batch loss = 0.22 (26.8 examples/sec; 0.298 sec/batch; 24h:03m:17s remains)
INFO - root - 2017-12-16 06:59:47.565024: step 42060, loss = 0.37, batch loss = 0.31 (28.2 examples/sec; 0.284 sec/batch; 22h:54m:50s remains)
INFO - root - 2017-12-16 06:59:50.429775: step 42070, loss = 0.30, batch loss = 0.24 (28.1 examples/sec; 0.284 sec/batch; 22h:55m:59s remains)
INFO - root - 2017-12-16 06:59:53.288961: step 42080, loss = 0.20, batch loss = 0.14 (25.3 examples/sec; 0.316 sec/batch; 25h:27m:53s remains)
INFO - root - 2017-12-16 06:59:56.117722: step 42090, loss = 0.28, batch loss = 0.22 (27.6 examples/sec; 0.290 sec/batch; 23h:25m:12s remains)
INFO - root - 2017-12-16 06:59:58.919688: step 42100, loss = 0.23, batch loss = 0.17 (27.3 examples/sec; 0.293 sec/batch; 23h:36m:47s remains)
2017-12-16 06:59:59.386887: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.441853 -4.2455 -4.1416197 -4.031352 -4.0049696 -4.0233507 -4.1646271 -4.315959 -4.4636087 -4.484405 -4.4032383 -4.3071446 -4.2439833 -4.0639143 -3.824754][-4.6743131 -4.4743533 -4.2711549 -4.0812454 -3.9890003 -3.8707693 -3.9544582 -4.1324754 -4.2059836 -4.1844726 -4.1197929 -4.0637641 -4.0211835 -3.8191876 -3.5616682][-4.8648233 -4.7032375 -4.5069547 -4.182467 -3.8899062 -3.6661692 -3.6779184 -3.7846553 -3.8692265 -3.8099334 -3.6460977 -3.607367 -3.6605494 -3.5669031 -3.3498721][-4.8225188 -4.6836863 -4.5084381 -4.1184621 -3.754096 -3.485806 -3.5007937 -3.5627184 -3.4707644 -3.2148056 -3.0197678 -3.146126 -3.341074 -3.3167863 -2.9874334][-4.50098 -4.3010015 -3.9677815 -3.5415075 -3.3122053 -3.0753756 -3.0821569 -3.1060832 -2.9196215 -2.6699967 -2.5247846 -2.6725812 -3.0141821 -3.1298985 -2.9108572][-4.05651 -3.6803288 -3.1445837 -2.8801579 -2.8280687 -2.7354598 -2.7338204 -2.7379444 -2.4940391 -2.1319411 -1.9798076 -2.2734013 -2.7645512 -2.9560456 -2.7646408][-3.3696618 -2.8173385 -2.3682163 -2.1281459 -2.0806935 -2.1027517 -2.1388314 -2.1309929 -1.895576 -1.5694625 -1.5206907 -2.0192826 -2.6309829 -2.9220028 -2.8858724][-2.6272955 -2.0954442 -1.6218097 -1.3657682 -1.4346735 -1.5522757 -1.7405164 -1.7329974 -1.4022694 -1.1251116 -1.194381 -1.6421964 -2.317409 -2.8209417 -2.8213694][-2.1895397 -1.5082359 -1.0272 -1.0236301 -1.3024247 -1.4883633 -1.6562233 -1.6316898 -1.2742095 -0.68210125 -0.57882094 -1.2586985 -2.1332328 -2.6461682 -2.6859875][-2.0267434 -1.4157014 -0.9957962 -1.0443697 -1.3746772 -1.6155281 -1.8100588 -1.4901497 -1.0133903 -0.54222035 -0.52248645 -0.996192 -1.8849342 -2.6508703 -2.9437237][-1.9514344 -1.4640958 -1.2961726 -1.5286102 -1.9662883 -2.2910769 -2.3570156 -2.0312476 -1.5891502 -1.0617304 -1.1014392 -1.4936974 -2.1847069 -2.826102 -2.8736014][-2.2395036 -1.8367772 -1.7186315 -2.1357894 -2.8197041 -3.3964767 -3.8548002 -3.5126572 -2.8786411 -2.4601903 -2.4206834 -2.5595968 -2.96047 -3.2644258 -3.0570927][-2.6814775 -2.4795761 -2.56574 -3.0397925 -3.7758071 -4.4710717 -4.8751268 -4.7070808 -4.431922 -3.8891883 -3.6107221 -3.8178728 -4.1814547 -4.077014 -3.3499618][-3.0659895 -2.8319597 -2.8548996 -3.4228487 -4.2776351 -5.0000381 -5.523932 -5.66201 -5.6201553 -5.3450074 -5.0387025 -4.9794621 -4.9704576 -4.7475109 -4.1081247][-3.7630637 -3.6061068 -3.7253869 -4.1359053 -4.89725 -5.7968307 -6.4566936 -6.6565008 -6.7799106 -6.7640634 -6.6577106 -6.31898 -6.0534182 -5.6592445 -4.7739506]]...]
INFO - root - 2017-12-16 07:00:02.181840: step 42110, loss = 0.32, batch loss = 0.26 (28.0 examples/sec; 0.286 sec/batch; 23h:05m:09s remains)
INFO - root - 2017-12-16 07:00:05.006624: step 42120, loss = 0.26, batch loss = 0.20 (29.2 examples/sec; 0.274 sec/batch; 22h:07m:38s remains)
INFO - root - 2017-12-16 07:00:07.823116: step 42130, loss = 0.29, batch loss = 0.23 (28.7 examples/sec; 0.278 sec/batch; 22h:27m:19s remains)
INFO - root - 2017-12-16 07:00:10.647472: step 42140, loss = 0.21, batch loss = 0.15 (29.1 examples/sec; 0.275 sec/batch; 22h:10m:54s remains)
INFO - root - 2017-12-16 07:00:13.430437: step 42150, loss = 0.23, batch loss = 0.18 (29.1 examples/sec; 0.275 sec/batch; 22h:08m:59s remains)
INFO - root - 2017-12-16 07:00:16.329252: step 42160, loss = 0.25, batch loss = 0.19 (27.7 examples/sec; 0.289 sec/batch; 23h:19m:57s remains)
INFO - root - 2017-12-16 07:00:19.150793: step 42170, loss = 0.30, batch loss = 0.24 (25.2 examples/sec; 0.317 sec/batch; 25h:35m:21s remains)
INFO - root - 2017-12-16 07:00:21.980067: step 42180, loss = 0.27, batch loss = 0.21 (27.9 examples/sec; 0.287 sec/batch; 23h:07m:52s remains)
INFO - root - 2017-12-16 07:00:24.813641: step 42190, loss = 0.40, batch loss = 0.35 (28.4 examples/sec; 0.282 sec/batch; 22h:42m:47s remains)
INFO - root - 2017-12-16 07:00:27.654429: step 42200, loss = 0.25, batch loss = 0.20 (27.5 examples/sec; 0.291 sec/batch; 23h:27m:07s remains)
2017-12-16 07:00:28.124345: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7928772 -4.8742881 -4.9188223 -4.9778633 -4.957695 -4.7327886 -4.4078126 -4.5589972 -4.9790435 -5.5388827 -6.2509975 -7.2568727 -8.3612823 -9.0954838 -9.4775419][-4.500432 -4.8563242 -4.9153385 -4.82606 -4.8050451 -4.8856168 -4.6378279 -4.285511 -4.2602072 -4.8043103 -5.6851268 -6.6117578 -7.6143818 -8.2383881 -8.7491951][-4.0914121 -4.2896619 -4.2001476 -4.077662 -3.8967724 -4.0168185 -4.0828867 -4.1094375 -4.1727743 -4.4788432 -4.9861608 -5.6040282 -6.5441351 -7.1369605 -7.56007][-3.2355466 -3.5105209 -3.5104604 -2.8908055 -2.406781 -2.336731 -2.2912669 -2.6979833 -3.2427216 -3.5432816 -3.6825438 -4.341589 -5.3837123 -6.0397997 -6.8461485][-2.223906 -2.1380761 -1.768398 -1.2947307 -0.75832534 -0.39924669 -0.20984602 -0.45618129 -0.75533557 -1.7457347 -2.3972709 -2.6265762 -3.1413596 -4.2434955 -5.6173525][-1.9091461 -1.1933708 -0.13186741 1.107996 1.9232593 2.1279445 2.1636653 1.9179363 1.438365 0.74684477 0.47023916 -0.20958567 -1.1895282 -1.9989624 -3.0849426][-2.2792246 -1.3111696 0.0090403557 1.4969649 3.0120082 3.9793606 4.2761841 3.7324266 3.1775808 2.4918289 2.0497575 1.4311905 0.71895933 -0.47417474 -2.1458497][-3.0337746 -2.0186868 -0.60848212 1.3484411 3.0360966 4.1875296 5.0005474 4.9274549 4.2790709 3.1053495 2.436708 1.7842674 0.85957766 -0.41590691 -1.7991061][-4.2552466 -3.5084453 -2.200788 -0.56916189 1.0315714 2.353622 3.1795588 3.1680498 3.0399189 2.3492751 1.7737641 0.9546032 0.20809078 -0.61529851 -1.4811809][-4.6881123 -4.3431864 -3.8702371 -2.7226605 -1.4447169 -0.59141636 0.067071438 0.31091738 0.20482397 -0.18428946 -0.27806377 -0.71379757 -1.2555711 -1.7452514 -2.387903][-5.1350055 -4.9642344 -4.6530323 -4.2478991 -4.1299691 -3.4571724 -2.7247844 -2.6951094 -2.9258842 -3.1610732 -3.2165411 -3.5770626 -4.0241194 -3.9988291 -4.01205][-5.4663029 -5.4030485 -5.4706612 -5.5021033 -5.4742308 -5.5691433 -5.6984248 -5.7388973 -5.6570692 -5.6122169 -5.5872231 -5.8650846 -5.9368033 -5.7032075 -5.5813656][-5.2903414 -5.3848019 -5.7111068 -5.9518251 -6.3138089 -6.4090657 -6.435616 -6.3455844 -6.2750859 -6.4159074 -6.4186211 -6.4022465 -6.4604073 -6.5061073 -6.2961717][-4.3171268 -4.4632497 -4.69835 -5.1098595 -5.5141892 -5.7309427 -5.9240537 -6.02186 -6.1193018 -5.9938264 -5.8812981 -6.3062363 -6.6416211 -6.4755831 -6.1517172][-4.31324 -4.1713142 -4.1350093 -4.2869706 -4.4730091 -4.7445641 -5.0260849 -5.0903325 -5.042304 -5.2190275 -5.4003468 -5.35949 -5.2914705 -5.487783 -5.5502853]]...]
INFO - root - 2017-12-16 07:00:30.966813: step 42210, loss = 0.19, batch loss = 0.13 (27.9 examples/sec; 0.286 sec/batch; 23h:05m:11s remains)
INFO - root - 2017-12-16 07:00:33.785251: step 42220, loss = 0.24, batch loss = 0.19 (29.0 examples/sec; 0.276 sec/batch; 22h:14m:02s remains)
INFO - root - 2017-12-16 07:00:36.648500: step 42230, loss = 0.22, batch loss = 0.16 (29.3 examples/sec; 0.273 sec/batch; 21h:59m:30s remains)
INFO - root - 2017-12-16 07:00:39.485709: step 42240, loss = 0.25, batch loss = 0.19 (28.6 examples/sec; 0.280 sec/batch; 22h:32m:16s remains)
INFO - root - 2017-12-16 07:00:42.352038: step 42250, loss = 0.24, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 22h:06m:15s remains)
INFO - root - 2017-12-16 07:00:45.167206: step 42260, loss = 0.26, batch loss = 0.20 (29.2 examples/sec; 0.274 sec/batch; 22h:06m:33s remains)
INFO - root - 2017-12-16 07:00:47.999853: step 42270, loss = 0.28, batch loss = 0.22 (28.6 examples/sec; 0.280 sec/batch; 22h:34m:38s remains)
INFO - root - 2017-12-16 07:00:50.833070: step 42280, loss = 0.25, batch loss = 0.19 (27.3 examples/sec; 0.293 sec/batch; 23h:39m:27s remains)
INFO - root - 2017-12-16 07:00:53.732281: step 42290, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.285 sec/batch; 22h:59m:34s remains)
INFO - root - 2017-12-16 07:00:56.574105: step 42300, loss = 0.26, batch loss = 0.20 (26.2 examples/sec; 0.305 sec/batch; 24h:35m:43s remains)
2017-12-16 07:00:57.035563: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4821639 -4.2677011 -4.1685047 -4.1772952 -4.3651562 -4.5331144 -4.7337747 -5.0289259 -5.0724611 -4.8840656 -4.6037035 -4.2647743 -3.7838526 -3.3228407 -3.0512147][-4.2606106 -4.2040215 -4.2252631 -4.2838383 -4.4712744 -4.6771874 -4.9211121 -5.2875733 -5.3811355 -5.192872 -4.9249892 -4.5038576 -3.9473002 -3.4141908 -3.0569801][-3.7834852 -3.773046 -3.8100591 -3.9218745 -4.126801 -4.3679609 -4.6807089 -4.9981003 -5.1137648 -5.0898228 -4.9583397 -4.7563705 -4.3841047 -3.8275893 -3.326324][-2.8828294 -2.7471128 -2.6564691 -2.8615565 -3.1353505 -3.3031392 -3.5561719 -3.828516 -4.0216451 -4.1126 -4.2823424 -4.4589772 -4.3467593 -3.9809132 -3.4997194][-1.7059453 -1.427578 -1.1354525 -1.1408498 -1.149492 -1.2062776 -1.3676474 -1.6498699 -2.0894115 -2.6845956 -3.3870904 -4.0907688 -4.3404074 -4.202064 -3.7204394][-0.69930744 -0.2608633 0.2371397 0.62251949 0.89436054 1.0132031 1.0255165 0.70708179 -0.0027360916 -1.0784268 -2.3737326 -3.694777 -4.298553 -4.376936 -3.9877872][-0.1666441 0.31896353 0.75963116 1.3170905 1.8890042 2.1787019 2.429893 2.4433694 1.8981423 0.58272982 -1.0216663 -2.6869926 -3.7495675 -4.1465669 -3.983362][-0.48994184 -0.046782494 0.45541477 1.1480346 1.8947344 2.5084982 2.9338293 2.9985824 2.6273623 1.3916698 -0.24279833 -1.9662757 -3.0668178 -3.555202 -3.6660218][-1.2140169 -0.87335372 -0.48899102 0.1905427 0.90712547 1.5525322 2.098207 2.3045254 2.1435466 1.0227947 -0.42828512 -1.9757149 -3.0074797 -3.4671745 -3.678467][-2.1830432 -1.8744304 -1.7367184 -1.273526 -0.58632565 -0.0059885979 0.47139645 0.71467686 0.6789422 -0.061248302 -1.143863 -2.35423 -3.2427287 -3.5723312 -3.6773736][-2.7149997 -2.7456737 -2.8131464 -2.6470394 -2.1738288 -1.6798286 -1.2070715 -0.86947775 -0.567219 -0.84639525 -1.6960931 -2.6297116 -3.3155811 -3.60293 -3.7061355][-3.1308918 -3.2611542 -3.5535302 -3.6142776 -3.3725142 -2.8898106 -2.3244789 -1.9670198 -1.551806 -1.5256228 -2.10607 -2.898602 -3.5447361 -3.702462 -3.7486479][-2.5998728 -2.7856197 -3.2459412 -3.507628 -3.4239869 -3.1106 -2.7501922 -2.524848 -2.1949093 -2.1936004 -2.6218214 -3.1824508 -3.6654043 -3.7401063 -3.6329675][-1.7167082 -1.8894341 -2.4448009 -2.8938737 -3.0449317 -2.8695951 -2.6134737 -2.4307013 -2.2079284 -2.256851 -2.7134695 -3.2826738 -3.6664076 -3.7218046 -3.5173368][-1.1331203 -1.2681837 -1.8091786 -2.3871338 -2.5771966 -2.4075925 -2.014554 -1.6902578 -1.5604408 -1.7279592 -2.2511327 -2.9262078 -3.3406181 -3.4183331 -3.1970294]]...]
INFO - root - 2017-12-16 07:00:59.839677: step 42310, loss = 0.24, batch loss = 0.18 (28.9 examples/sec; 0.276 sec/batch; 22h:17m:11s remains)
INFO - root - 2017-12-16 07:01:02.705371: step 42320, loss = 0.34, batch loss = 0.28 (27.8 examples/sec; 0.287 sec/batch; 23h:09m:38s remains)
INFO - root - 2017-12-16 07:01:05.553316: step 42330, loss = 0.25, batch loss = 0.19 (27.9 examples/sec; 0.287 sec/batch; 23h:06m:18s remains)
INFO - root - 2017-12-16 07:01:08.394087: step 42340, loss = 0.28, batch loss = 0.22 (28.3 examples/sec; 0.282 sec/batch; 22h:45m:57s remains)
INFO - root - 2017-12-16 07:01:11.258664: step 42350, loss = 0.28, batch loss = 0.22 (28.3 examples/sec; 0.283 sec/batch; 22h:48m:41s remains)
INFO - root - 2017-12-16 07:01:14.008657: step 42360, loss = 0.36, batch loss = 0.30 (29.6 examples/sec; 0.271 sec/batch; 21h:48m:16s remains)
INFO - root - 2017-12-16 07:01:16.861832: step 42370, loss = 0.23, batch loss = 0.17 (27.4 examples/sec; 0.291 sec/batch; 23h:29m:25s remains)
INFO - root - 2017-12-16 07:01:19.648778: step 42380, loss = 0.21, batch loss = 0.16 (29.2 examples/sec; 0.274 sec/batch; 22h:05m:58s remains)
INFO - root - 2017-12-16 07:01:22.497054: step 42390, loss = 0.22, batch loss = 0.16 (27.5 examples/sec; 0.291 sec/batch; 23h:26m:33s remains)
INFO - root - 2017-12-16 07:01:25.329506: step 42400, loss = 0.33, batch loss = 0.27 (29.9 examples/sec; 0.268 sec/batch; 21h:34m:55s remains)
2017-12-16 07:01:25.788997: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4234924 -4.2858562 -4.2519131 -4.66789 -5.3669367 -6.1408634 -6.8543024 -6.9730906 -6.9843512 -6.4879603 -6.0713921 -5.7646232 -5.1217113 -4.3816428 -3.6418905][-5.0640345 -5.5817623 -6.0544748 -6.330718 -7.1809053 -7.4332561 -7.5167456 -7.418623 -7.0232935 -6.615078 -6.3057146 -5.79341 -5.0964584 -4.4314709 -3.8216386][-5.6698489 -6.3240547 -6.8686056 -7.1766014 -7.2389107 -6.9686861 -7.0122051 -6.5989103 -6.1831255 -5.7871585 -5.5072422 -5.4530392 -5.3703232 -4.8987265 -4.25196][-6.1053128 -7.1033821 -7.5183592 -7.2595439 -6.6610641 -5.6779222 -4.8290977 -4.1793962 -3.9672248 -4.3451695 -4.9515624 -5.1343226 -5.2182374 -5.242538 -4.9131989][-6.0076041 -7.0482082 -7.199605 -6.4555254 -5.0176644 -3.28188 -2.0015736 -0.99244285 -1.0617673 -2.1028666 -3.3039567 -4.60533 -5.4802375 -5.4952374 -4.945158][-4.6403823 -5.5617418 -5.7018151 -4.5994277 -2.2601566 0.20610905 2.0624881 2.9830956 2.3376098 0.57651091 -1.8174152 -3.9024088 -5.1457763 -5.670939 -5.3489318][-3.3235021 -4.0837975 -3.7845044 -2.2509708 0.083396912 2.7800565 4.9531727 5.7487316 4.7501 2.2972765 -0.63010383 -3.2819612 -5.0585117 -5.5476418 -5.0789051][-2.7057083 -3.4401169 -3.2718778 -1.5604749 1.1259646 3.9550524 5.9130468 6.130682 4.8929729 2.3094444 -0.89504361 -3.5354853 -5.009347 -5.3816414 -4.9416037][-2.6726446 -3.1834474 -2.7256117 -1.5703409 0.11398792 2.5765476 4.5672035 4.8986635 3.5441866 0.81859064 -2.135886 -4.4230609 -5.7024665 -5.7981539 -5.138535][-2.6863651 -3.3839941 -3.1329603 -2.1369808 -0.58556294 0.90766287 1.7437911 2.0234427 0.9745841 -1.3634131 -3.8802152 -5.81046 -6.5468283 -6.3668995 -5.6289096][-3.4771943 -3.8020535 -3.6190741 -3.4311552 -2.9987431 -1.7492394 -0.51659775 -0.57450747 -1.9085169 -3.5081875 -5.1995749 -6.69112 -7.22519 -6.743721 -5.837338][-4.4883084 -4.7381372 -4.8702455 -4.4725485 -3.8912103 -3.6140776 -3.3546705 -3.2763374 -3.7991471 -5.0170555 -6.3684583 -7.116961 -7.1587539 -6.6130328 -5.8173132][-5.4384336 -5.5512047 -5.6621113 -5.6267762 -5.545269 -4.9569545 -4.4604111 -4.7989335 -5.423346 -6.0195975 -6.7833819 -7.237545 -7.113903 -6.4773841 -5.7404108][-6.2095613 -6.1800251 -6.1780066 -6.1320658 -6.006711 -5.601234 -5.4361253 -5.31513 -5.4989519 -6.075912 -6.5274973 -6.5656824 -6.3483133 -5.8024759 -5.2389951][-6.8426938 -6.7656784 -6.6534195 -6.4346228 -6.158813 -5.7745571 -5.5085893 -5.2185516 -5.2281537 -5.5085135 -5.7333255 -5.6464572 -5.3029509 -4.7693276 -4.3009515]]...]
INFO - root - 2017-12-16 07:01:28.607234: step 42410, loss = 0.27, batch loss = 0.21 (28.9 examples/sec; 0.277 sec/batch; 22h:16m:59s remains)
INFO - root - 2017-12-16 07:01:31.460206: step 42420, loss = 0.21, batch loss = 0.15 (27.1 examples/sec; 0.295 sec/batch; 23h:45m:15s remains)
INFO - root - 2017-12-16 07:01:34.287006: step 42430, loss = 0.29, batch loss = 0.23 (28.7 examples/sec; 0.279 sec/batch; 22h:28m:10s remains)
INFO - root - 2017-12-16 07:01:37.121862: step 42440, loss = 0.37, batch loss = 0.32 (28.2 examples/sec; 0.283 sec/batch; 22h:50m:29s remains)
INFO - root - 2017-12-16 07:01:39.979199: step 42450, loss = 0.27, batch loss = 0.21 (26.4 examples/sec; 0.303 sec/batch; 24h:23m:06s remains)
INFO - root - 2017-12-16 07:01:42.795671: step 42460, loss = 0.30, batch loss = 0.25 (29.3 examples/sec; 0.273 sec/batch; 21h:58m:56s remains)
INFO - root - 2017-12-16 07:01:45.650519: step 42470, loss = 0.25, batch loss = 0.19 (29.1 examples/sec; 0.275 sec/batch; 22h:07m:15s remains)
INFO - root - 2017-12-16 07:01:48.476126: step 42480, loss = 0.32, batch loss = 0.26 (26.8 examples/sec; 0.299 sec/batch; 24h:03m:15s remains)
INFO - root - 2017-12-16 07:01:51.306948: step 42490, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.278 sec/batch; 22h:25m:36s remains)
INFO - root - 2017-12-16 07:01:54.181721: step 42500, loss = 0.31, batch loss = 0.25 (28.6 examples/sec; 0.280 sec/batch; 22h:31m:03s remains)
2017-12-16 07:01:54.621282: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7113771 -5.2149405 -4.6440473 -4.3214974 -4.2558537 -4.2175956 -4.2396154 -4.3916945 -4.5972285 -4.8634667 -5.1574478 -5.5112066 -5.7394977 -5.7393894 -5.6294684][-6.0551305 -5.3681841 -4.6514807 -4.0460534 -3.7328181 -3.7741141 -3.8043494 -4.1211181 -4.6518164 -5.077776 -5.5686464 -5.852385 -5.9908929 -5.8990221 -5.6747365][-6.324533 -5.3362741 -4.3617916 -3.4718621 -2.9138477 -2.853487 -2.8634949 -3.3835645 -3.8558683 -4.5192533 -5.3794971 -5.7724419 -6.11528 -6.0868378 -5.822773][-5.9852405 -4.79067 -3.5925751 -2.4926424 -1.7662024 -1.5259225 -1.6371303 -2.3479154 -2.9726076 -3.7061164 -4.2747111 -4.8926778 -5.5508857 -5.8273849 -6.0004807][-5.2001314 -3.7154858 -2.1207888 -0.72507668 0.10749006 0.3279295 0.011499405 -0.71708894 -1.5352647 -2.3656294 -3.0108147 -3.6699681 -4.2720017 -5.0597157 -5.8121691][-4.7609367 -3.0033069 -1.0153985 0.93905067 2.344625 2.5637617 2.0195165 1.0373178 0.065796852 -0.73246789 -1.4095685 -2.187192 -3.2170217 -4.3030858 -5.2825313][-4.607151 -2.8398347 -0.76435709 1.314044 2.9839473 3.5747719 3.3267751 2.3915896 1.294898 0.3591156 -0.45713782 -1.4805603 -2.9482839 -4.2126431 -5.46195][-4.6118522 -3.0531831 -1.3242409 0.73013973 2.2812767 3.0669103 3.1848712 2.6224523 2.0149503 1.0300107 0.040329933 -1.2759817 -3.0728278 -4.7058058 -6.120369][-4.964653 -3.6840651 -1.9997838 -0.24051332 0.788795 1.551959 1.9785643 1.7673635 1.3669682 0.62862349 -0.19367409 -1.4122949 -3.1149271 -4.7602158 -6.1463828][-5.6484127 -4.6533928 -3.3981924 -1.86657 -0.74549747 -0.26814508 -0.016200542 0.15790319 0.021285057 -0.47667003 -1.0787556 -2.1500788 -3.5835981 -4.899055 -6.0357571][-6.7266321 -5.9245205 -4.9620562 -3.9028234 -3.0493684 -2.3845518 -1.778296 -1.5508528 -1.6220696 -1.9118862 -2.2663391 -3.0843585 -4.21837 -5.3426576 -6.3577662][-7.4209752 -6.7547617 -6.0100384 -5.1607757 -4.6596217 -4.1420879 -3.5634384 -3.1797929 -2.9353032 -2.9085066 -3.1501913 -3.6970346 -4.4467216 -5.2276525 -6.0524669][-7.4833326 -6.9393787 -6.3750815 -5.833765 -5.6690469 -5.4549217 -5.0491009 -4.5564661 -4.1076975 -3.8118093 -3.7285213 -3.9007468 -4.2425327 -4.7426963 -5.3598223][-7.6389022 -6.7746849 -6.2106161 -5.8354673 -5.772913 -5.8077097 -5.7789669 -5.4689236 -4.9605889 -4.4328485 -3.9821451 -3.9437916 -4.1458216 -4.3764205 -4.6287689][-7.19695 -6.0121317 -5.4004979 -5.2400651 -5.2736568 -5.2944736 -5.3525224 -5.2667251 -5.0086675 -4.5877023 -4.1442585 -3.9489427 -3.8362498 -3.9302161 -4.1565776]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-42500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-42500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 07:01:57.988164: step 42510, loss = 0.47, batch loss = 0.41 (28.1 examples/sec; 0.285 sec/batch; 22h:56m:20s remains)
INFO - root - 2017-12-16 07:02:00.787698: step 42520, loss = 0.30, batch loss = 0.24 (29.1 examples/sec; 0.275 sec/batch; 22h:10m:17s remains)
INFO - root - 2017-12-16 07:02:03.612196: step 42530, loss = 0.28, batch loss = 0.22 (28.3 examples/sec; 0.282 sec/batch; 22h:44m:46s remains)
INFO - root - 2017-12-16 07:02:06.426067: step 42540, loss = 0.21, batch loss = 0.15 (28.5 examples/sec; 0.280 sec/batch; 22h:35m:05s remains)
INFO - root - 2017-12-16 07:02:09.233585: step 42550, loss = 0.20, batch loss = 0.14 (28.7 examples/sec; 0.278 sec/batch; 22h:25m:29s remains)
INFO - root - 2017-12-16 07:02:12.037046: step 42560, loss = 0.37, batch loss = 0.31 (29.5 examples/sec; 0.271 sec/batch; 21h:48m:25s remains)
INFO - root - 2017-12-16 07:02:14.828318: step 42570, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.284 sec/batch; 22h:54m:34s remains)
INFO - root - 2017-12-16 07:02:17.625912: step 42580, loss = 0.20, batch loss = 0.14 (29.4 examples/sec; 0.272 sec/batch; 21h:52m:59s remains)
INFO - root - 2017-12-16 07:02:20.430949: step 42590, loss = 0.28, batch loss = 0.22 (28.7 examples/sec; 0.279 sec/batch; 22h:26m:01s remains)
INFO - root - 2017-12-16 07:02:23.221387: step 42600, loss = 0.32, batch loss = 0.26 (28.1 examples/sec; 0.285 sec/batch; 22h:55m:09s remains)
2017-12-16 07:02:23.694580: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.84763 -4.6200275 -5.2504025 -6.0228772 -6.5984254 -6.6356916 -6.3894243 -6.3140593 -6.1062655 -5.7906809 -5.3322115 -4.7928395 -4.1719141 -3.5937619 -3.3156123][-4.3817644 -5.1308322 -5.70129 -6.2783456 -6.6119776 -6.8258057 -6.7311935 -6.3629737 -6.1058083 -5.9751554 -5.7604351 -5.2538738 -4.6471395 -4.0904679 -3.7355375][-5.1125579 -6.0547032 -6.5363636 -6.5313554 -6.2919216 -6.133554 -5.9613786 -5.9284143 -5.7362328 -5.548068 -5.6050482 -5.6379557 -5.2996497 -4.7819614 -4.3418169][-5.3113818 -6.2743998 -6.6867971 -6.1730537 -5.1541934 -4.3152885 -3.7705896 -3.7156632 -3.8629563 -4.3045945 -4.7423506 -5.2557235 -5.3542562 -5.0959282 -4.7450428][-5.8848181 -6.4179893 -6.2782426 -5.4750195 -4.0138178 -2.0310383 -0.61939 -0.38292027 -0.96409249 -2.1591423 -3.3694305 -4.5054712 -4.9901819 -4.987052 -4.8235397][-6.399044 -6.6799545 -5.9034939 -4.2622809 -2.0192485 0.34956121 2.1373878 2.8674202 2.1165233 0.209167 -1.7395763 -3.5519924 -4.6995707 -4.9067078 -4.4943075][-6.1274185 -5.9875317 -4.860436 -2.8424225 -0.16731834 2.4407363 4.3514462 4.8567524 3.9673843 1.9309773 -0.42962837 -2.5825872 -3.9683864 -4.5120387 -4.3302979][-5.5793328 -5.1228685 -3.6192203 -1.4223518 1.1862211 3.6277075 5.544776 5.9515018 4.8533239 2.4111991 -0.16175985 -2.1783671 -3.5947008 -4.3222046 -4.27082][-4.8741245 -4.5460348 -3.008702 -0.94455934 1.3177171 3.489193 4.869545 5.1094046 4.1010857 1.9011416 -0.58219218 -2.6854835 -3.9914753 -4.5064068 -4.434525][-4.273005 -4.2009134 -3.4704103 -2.0422385 -0.3740139 1.1551623 2.158906 2.2454543 1.2840586 -0.3285799 -2.144388 -3.6690173 -4.6204739 -4.9788642 -4.808773][-4.6318903 -4.6404524 -4.2206759 -3.6334288 -2.6193132 -1.6351912 -1.2258351 -1.354739 -2.1896024 -3.3920612 -4.4831061 -5.1000252 -5.4165039 -5.371212 -5.0245504][-5.4313722 -5.7567792 -5.7819939 -5.3208971 -4.524539 -4.181016 -3.9690228 -4.0756197 -4.5424361 -5.1758618 -5.8670788 -6.0330672 -5.7026038 -5.1811023 -4.710999][-6.8020949 -7.241333 -7.4282966 -7.482852 -7.3008633 -6.3579388 -5.7149935 -5.71984 -6.081306 -6.4166307 -6.2051296 -5.76265 -5.265327 -4.7211571 -4.1873164][-7.4208407 -8.6201611 -9.1765575 -8.7586155 -8.0518484 -7.7073908 -7.6971483 -7.0679345 -6.4624891 -6.1341257 -5.9810758 -5.656786 -4.8717647 -4.1887732 -3.6897814][-8.0632019 -8.80942 -9.2202644 -9.3431034 -8.828516 -7.58873 -6.6313248 -6.4281249 -6.1522131 -5.4814973 -4.80397 -4.421804 -4.1862049 -3.8250263 -3.3392913]]...]
INFO - root - 2017-12-16 07:02:26.557283: step 42610, loss = 0.21, batch loss = 0.15 (28.2 examples/sec; 0.284 sec/batch; 22h:52m:32s remains)
INFO - root - 2017-12-16 07:02:29.369108: step 42620, loss = 0.21, batch loss = 0.15 (29.3 examples/sec; 0.273 sec/batch; 22h:00m:00s remains)
INFO - root - 2017-12-16 07:02:32.204413: step 42630, loss = 0.32, batch loss = 0.26 (27.5 examples/sec; 0.291 sec/batch; 23h:24m:04s remains)
INFO - root - 2017-12-16 07:02:35.024466: step 42640, loss = 0.23, batch loss = 0.17 (28.9 examples/sec; 0.277 sec/batch; 22h:18m:27s remains)
INFO - root - 2017-12-16 07:02:37.897863: step 42650, loss = 0.32, batch loss = 0.27 (27.7 examples/sec; 0.289 sec/batch; 23h:15m:35s remains)
INFO - root - 2017-12-16 07:02:40.721467: step 42660, loss = 0.51, batch loss = 0.45 (28.6 examples/sec; 0.280 sec/batch; 22h:33m:28s remains)
INFO - root - 2017-12-16 07:02:43.552123: step 42670, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.283 sec/batch; 22h:47m:36s remains)
INFO - root - 2017-12-16 07:02:46.388996: step 42680, loss = 0.30, batch loss = 0.24 (28.8 examples/sec; 0.278 sec/batch; 22h:21m:31s remains)
INFO - root - 2017-12-16 07:02:49.235800: step 42690, loss = 0.23, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 22h:55m:56s remains)
INFO - root - 2017-12-16 07:02:52.029609: step 42700, loss = 0.23, batch loss = 0.17 (29.0 examples/sec; 0.276 sec/batch; 22h:13m:33s remains)
2017-12-16 07:02:52.478154: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1573696 -4.4299164 -4.5261893 -4.5393543 -4.4751487 -4.2441444 -3.969208 -3.8347616 -3.8458519 -3.869359 -3.8469279 -4.0452967 -4.1763978 -4.0982056 -3.857904][-4.3043323 -4.8452258 -5.0632706 -5.0285239 -4.75725 -4.492981 -4.3171024 -4.1488595 -4.1474738 -4.34594 -4.5065613 -4.6414003 -4.7310314 -4.7617645 -4.5274644][-4.1691422 -4.6863523 -4.8484268 -4.7687345 -4.4769335 -4.2046032 -3.8744617 -3.8354325 -4.0207663 -4.4450068 -4.87101 -5.2306876 -5.4219151 -5.4775276 -5.1584787][-4.0626588 -4.4926658 -4.4691558 -3.8992603 -3.1211681 -2.575655 -2.1624813 -2.1177635 -2.3982363 -3.2478845 -4.188014 -4.9332066 -5.7049484 -6.0489941 -5.8839507][-3.3404541 -3.4044833 -3.0354497 -2.1080697 -0.98942757 0.017468929 0.71875334 0.93525457 0.47140789 -0.87504125 -2.3175607 -3.6627474 -4.6551938 -5.4125018 -5.6421919][-2.7038174 -2.4145763 -1.6055384 -0.23395157 1.5841403 3.0446591 3.9383535 4.2652311 3.7346153 2.1435127 0.32657194 -1.7508841 -3.3934565 -4.3676777 -4.5957851][-2.6846683 -1.8856473 -0.74076104 1.0527997 3.2552042 5.1390095 6.7260103 7.0673323 6.3382893 4.628006 2.3380828 0.00023174286 -1.7891681 -3.0203073 -3.4283848][-2.8145437 -2.1978581 -1.2615395 0.71632624 3.2356076 5.5964375 7.5155621 8.1492052 7.5235519 5.7147961 3.2431068 1.0120983 -0.84154487 -2.1532321 -2.6856017][-3.7838631 -3.3228116 -2.4497275 -0.98459911 1.020772 3.5035391 5.5346632 6.4694986 6.3417931 4.9110184 2.9134183 0.7283864 -0.88086677 -1.8287978 -2.1958768][-4.2208214 -4.6701231 -4.8235259 -3.6459746 -1.8420696 0.40634441 2.2799363 3.477716 3.7209129 2.5353675 1.0095925 -0.54708171 -1.7979054 -2.5494385 -2.8610325][-5.6876974 -6.1368742 -6.1586385 -5.7083464 -4.6000924 -2.7149632 -0.9784739 0.19539261 0.56330538 0.10803318 -1.0230346 -2.3140941 -3.0809364 -3.3837132 -3.3318276][-5.5432992 -6.1650305 -6.6576233 -6.4987969 -5.5693979 -4.3397403 -3.0137129 -2.0395954 -1.7014446 -1.8685334 -2.4190872 -3.3316545 -3.8960223 -3.9632444 -3.7526245][-4.8809052 -5.3373723 -5.6605177 -5.6550198 -5.3493385 -4.4996805 -3.4539754 -2.9148948 -2.5747347 -2.7678151 -3.1998391 -3.6725271 -3.8174865 -3.8379045 -3.6157734][-3.8809571 -4.0383325 -4.2139611 -4.2157946 -4.1974297 -3.671164 -3.0744722 -2.8845186 -2.7399716 -2.8059025 -2.9480834 -3.2355418 -3.4643011 -3.4137716 -3.1486917][-3.0115628 -2.7625856 -2.6190696 -2.6035831 -2.66892 -2.4932504 -2.3887451 -2.3150277 -2.3088894 -2.4222403 -2.5981529 -2.6033478 -2.5902119 -2.7121902 -2.6785827]]...]
INFO - root - 2017-12-16 07:02:55.295477: step 42710, loss = 0.35, batch loss = 0.29 (28.8 examples/sec; 0.278 sec/batch; 22h:22m:50s remains)
INFO - root - 2017-12-16 07:02:58.122723: step 42720, loss = 0.30, batch loss = 0.24 (24.5 examples/sec; 0.326 sec/batch; 26h:14m:29s remains)
INFO - root - 2017-12-16 07:03:00.965620: step 42730, loss = 0.32, batch loss = 0.26 (28.4 examples/sec; 0.282 sec/batch; 22h:41m:13s remains)
INFO - root - 2017-12-16 07:03:03.767317: step 42740, loss = 0.27, batch loss = 0.21 (28.7 examples/sec; 0.279 sec/batch; 22h:28m:28s remains)
INFO - root - 2017-12-16 07:03:06.578048: step 42750, loss = 0.37, batch loss = 0.31 (27.7 examples/sec; 0.289 sec/batch; 23h:16m:55s remains)
INFO - root - 2017-12-16 07:03:09.418633: step 42760, loss = 0.26, batch loss = 0.21 (28.1 examples/sec; 0.285 sec/batch; 22h:56m:41s remains)
INFO - root - 2017-12-16 07:03:12.291934: step 42770, loss = 0.24, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 22h:45m:04s remains)
INFO - root - 2017-12-16 07:03:15.139872: step 42780, loss = 0.42, batch loss = 0.36 (28.9 examples/sec; 0.276 sec/batch; 22h:14m:36s remains)
INFO - root - 2017-12-16 07:03:17.971232: step 42790, loss = 0.30, batch loss = 0.25 (28.1 examples/sec; 0.285 sec/batch; 22h:55m:52s remains)
INFO - root - 2017-12-16 07:03:20.761562: step 42800, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.280 sec/batch; 22h:32m:05s remains)
2017-12-16 07:03:21.232809: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6091275 -3.5626626 -3.5358858 -3.5672636 -3.45228 -3.194356 -2.8934875 -3.0888028 -3.5991356 -4.1816163 -4.4982815 -4.6097336 -4.1292629 -3.263864 -2.3980014][-4.18872 -4.2097754 -4.224854 -4.3010726 -4.1151423 -3.7943516 -3.462765 -3.3874362 -3.3736763 -3.7121613 -4.0851307 -4.0895829 -3.5619276 -2.9001994 -2.113534][-4.5870214 -4.8174624 -4.9555469 -4.8850741 -4.5679255 -4.0946879 -3.4517179 -3.0256071 -2.8050914 -2.8534057 -2.8741403 -2.8617883 -2.584486 -2.2309861 -1.7439413][-4.98235 -5.3736572 -5.5602875 -5.3121262 -4.7316437 -3.8211908 -2.7382355 -1.785289 -1.1110959 -0.95320868 -0.79570246 -1.1642237 -1.5798688 -1.6285744 -1.5831261][-5.2816668 -5.8265452 -5.8924041 -5.4529409 -4.5860462 -3.3315792 -1.8311713 -0.32923937 0.7890172 1.0599422 0.79629469 0.27188587 -0.19212055 -1.1796453 -2.1902056][-4.9735413 -5.5753074 -5.6196318 -4.8568516 -3.5594234 -1.9832718 -0.25223827 1.4044385 2.4586701 2.9169512 2.742763 1.6101222 0.16134691 -1.0226276 -2.0212781][-4.4401011 -5.0586429 -4.9810009 -4.0569596 -2.4833879 -0.385602 1.779633 3.1946054 3.9872894 3.7742481 2.8967905 1.3976307 -0.12275505 -1.5303566 -2.6342897][-3.9865186 -4.75481 -4.8516183 -3.7994089 -1.8412619 0.60988569 3.03165 4.4651213 4.6207733 3.8166189 2.4326758 0.43596745 -1.2145438 -2.5660872 -3.5199666][-3.2255349 -4.1643949 -4.5079184 -3.7858603 -2.1278031 0.38481712 2.7708116 4.0302134 4.1383419 3.3127551 1.6756191 -0.21914768 -1.874342 -3.1176724 -3.8999357][-2.1401844 -3.2340703 -4.1410565 -3.8779271 -2.5891275 -0.76194787 1.19729 2.5500903 2.57685 1.1761904 -0.574631 -2.079674 -3.2082009 -3.96602 -4.3587084][-1.3622775 -2.567909 -3.8249953 -4.0759411 -3.2894492 -1.7452285 -0.26785851 0.4515233 0.16067648 -1.1391649 -2.870651 -4.2710609 -5.1394525 -5.4306126 -5.3837266][-1.2337294 -2.2915335 -3.4943008 -4.1074481 -3.8630769 -2.9322705 -2.0603075 -1.5905006 -1.7385929 -2.6955602 -4.0331421 -5.3106847 -6.1073661 -6.3098326 -6.2092223][-1.2126629 -2.2513905 -3.4479551 -3.9494154 -4.1724477 -3.8494997 -3.4692144 -3.2463703 -3.4507802 -4.2816396 -5.3956532 -6.1982374 -6.4310522 -6.5121326 -6.3807316][-1.1755252 -2.1544561 -3.2368388 -3.8479786 -4.1760492 -4.1075172 -4.1328683 -4.4125652 -4.9179983 -5.4686151 -6.1376286 -6.6988955 -6.8460836 -6.352046 -5.7400494][-1.8591251 -2.5509608 -3.3199973 -3.8241441 -4.315928 -4.4590416 -4.5334406 -4.7243342 -4.9789886 -5.3429923 -5.6707487 -5.9156022 -5.8976364 -5.6652112 -5.2178807]]...]
INFO - root - 2017-12-16 07:03:24.041610: step 42810, loss = 0.22, batch loss = 0.16 (27.7 examples/sec; 0.289 sec/batch; 23h:13m:45s remains)
INFO - root - 2017-12-16 07:03:26.826528: step 42820, loss = 0.26, batch loss = 0.20 (27.9 examples/sec; 0.287 sec/batch; 23h:05m:32s remains)
INFO - root - 2017-12-16 07:03:29.630945: step 42830, loss = 0.33, batch loss = 0.28 (28.8 examples/sec; 0.278 sec/batch; 22h:22m:45s remains)
INFO - root - 2017-12-16 07:03:32.417963: step 42840, loss = 0.32, batch loss = 0.26 (28.0 examples/sec; 0.286 sec/batch; 23h:00m:21s remains)
INFO - root - 2017-12-16 07:03:35.224663: step 42850, loss = 0.21, batch loss = 0.15 (29.4 examples/sec; 0.272 sec/batch; 21h:54m:04s remains)
INFO - root - 2017-12-16 07:03:38.091978: step 42860, loss = 0.33, batch loss = 0.27 (27.2 examples/sec; 0.294 sec/batch; 23h:40m:01s remains)
INFO - root - 2017-12-16 07:03:40.957638: step 42870, loss = 0.46, batch loss = 0.41 (27.2 examples/sec; 0.295 sec/batch; 23h:42m:11s remains)
INFO - root - 2017-12-16 07:03:43.822425: step 42880, loss = 0.30, batch loss = 0.24 (28.2 examples/sec; 0.284 sec/batch; 22h:50m:02s remains)
INFO - root - 2017-12-16 07:03:46.591932: step 42890, loss = 0.20, batch loss = 0.14 (29.0 examples/sec; 0.276 sec/batch; 22h:09m:55s remains)
INFO - root - 2017-12-16 07:03:49.428796: step 42900, loss = 0.24, batch loss = 0.18 (26.9 examples/sec; 0.298 sec/batch; 23h:56m:29s remains)
2017-12-16 07:03:49.885945: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2599959 -6.4873538 -6.6591258 -6.3451638 -6.1137381 -5.600904 -5.1138115 -4.8546782 -4.745389 -5.1678982 -5.6749897 -6.496274 -7.351222 -7.8666863 -7.7397213][-7.0305033 -7.5046339 -7.7332382 -7.4424763 -6.9588938 -6.2105017 -5.3258162 -4.6980548 -4.4321489 -5.1564631 -5.9125485 -6.6181307 -7.431675 -8.08386 -7.9301682][-7.5103436 -7.8889036 -7.833878 -7.3298178 -6.6467447 -5.6976981 -4.4983997 -3.9587529 -3.7866964 -4.2907438 -5.0529337 -6.0136609 -7.1091833 -7.5726843 -7.485693][-6.6570387 -7.1003113 -6.8798375 -5.966083 -4.9128613 -3.8016467 -2.6218348 -2.3260467 -2.4014921 -2.8519278 -3.4997168 -4.3248115 -5.2039804 -6.0218315 -6.1809521][-5.3717184 -5.4672313 -4.9441953 -3.9985247 -2.9956398 -1.7693336 -0.57667732 -0.10810757 -0.17481184 -0.78232 -1.583606 -2.040199 -2.9038815 -3.9199948 -4.5217347][-4.2936249 -3.7371445 -2.4890795 -1.0053377 0.42754841 1.2698431 1.9257188 1.9977627 1.7077312 0.94729233 0.058009624 -0.68082452 -1.6655316 -2.5822086 -3.2491608][-3.2276802 -2.4306722 -1.0964115 0.60517979 2.4200802 3.5240107 4.0705032 3.680891 3.0006633 2.1101923 1.1965785 0.13877249 -0.917248 -2.2198615 -3.1123672][-2.9568808 -1.9787288 -0.5083859 1.1377797 2.6951356 3.7586613 4.4007263 4.2133179 3.3732443 2.2563338 1.0510087 -0.11319637 -1.2531924 -2.5662327 -3.5427697][-3.4919229 -2.6713591 -1.318361 0.097093105 1.5341272 2.6177235 3.2945714 3.1787519 2.452384 1.3527379 0.16332293 -1.0877702 -2.2880747 -3.3939104 -4.1691093][-3.9670143 -3.8002515 -3.1406655 -2.1168118 -0.90074611 0.099108219 0.83438778 0.9159503 0.41429806 -0.46053267 -1.4281921 -2.6776609 -3.8065064 -4.6312923 -4.9356937][-5.56022 -5.2499352 -4.569839 -4.1221938 -3.5863166 -2.6675477 -1.8556418 -1.6202006 -1.7430499 -2.2681835 -2.8971345 -3.8100727 -4.6371045 -5.0436583 -4.9793458][-6.6340284 -6.561574 -6.3672252 -5.9132876 -5.3653741 -4.8143764 -4.4681253 -4.0999365 -3.8699093 -3.9886541 -4.1404133 -4.4712868 -4.8201785 -4.8290267 -4.5892262][-7.3378496 -7.3184371 -7.3866892 -7.27147 -7.1042919 -6.6758738 -6.3257427 -6.0657825 -5.9224272 -5.6572938 -5.3876619 -5.2661152 -5.1978388 -4.8619738 -4.348834][-7.2038736 -7.0791683 -7.1250868 -7.1303439 -7.1001472 -6.9405522 -6.7572021 -6.6543989 -6.475626 -6.1405821 -5.827497 -5.4880795 -5.1755896 -4.811306 -4.3172569][-6.6979828 -6.2574277 -6.0743961 -6.1697383 -6.3183436 -6.4044914 -6.4240842 -6.3480191 -6.1800995 -5.9024124 -5.59861 -5.1972752 -4.9104505 -4.6021214 -4.2003174]]...]
INFO - root - 2017-12-16 07:03:52.708821: step 42910, loss = 0.27, batch loss = 0.21 (26.1 examples/sec; 0.307 sec/batch; 24h:39m:50s remains)
INFO - root - 2017-12-16 07:03:55.556107: step 42920, loss = 0.25, batch loss = 0.19 (28.8 examples/sec; 0.278 sec/batch; 22h:22m:08s remains)
INFO - root - 2017-12-16 07:03:58.327298: step 42930, loss = 0.23, batch loss = 0.18 (29.4 examples/sec; 0.272 sec/batch; 21h:51m:22s remains)
INFO - root - 2017-12-16 07:04:01.178193: step 42940, loss = 0.23, batch loss = 0.17 (27.1 examples/sec; 0.295 sec/batch; 23h:43m:42s remains)
INFO - root - 2017-12-16 07:04:04.046026: step 42950, loss = 0.27, batch loss = 0.21 (28.9 examples/sec; 0.277 sec/batch; 22h:17m:14s remains)
INFO - root - 2017-12-16 07:04:06.928201: step 42960, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.284 sec/batch; 22h:48m:18s remains)
INFO - root - 2017-12-16 07:04:09.779842: step 42970, loss = 0.27, batch loss = 0.21 (28.4 examples/sec; 0.282 sec/batch; 22h:39m:00s remains)
INFO - root - 2017-12-16 07:04:12.614835: step 42980, loss = 0.30, batch loss = 0.24 (26.1 examples/sec; 0.307 sec/batch; 24h:40m:19s remains)
INFO - root - 2017-12-16 07:04:15.448091: step 42990, loss = 0.20, batch loss = 0.14 (28.4 examples/sec; 0.282 sec/batch; 22h:38m:33s remains)
INFO - root - 2017-12-16 07:04:18.227075: step 43000, loss = 0.23, batch loss = 0.17 (29.1 examples/sec; 0.275 sec/batch; 22h:07m:17s remains)
2017-12-16 07:04:18.690959: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1053772 -4.3100505 -4.5196409 -4.8315306 -5.0400434 -5.1314611 -5.1284032 -5.1963682 -5.2169662 -5.03047 -4.773912 -4.4867215 -4.0619826 -3.6278927 -3.2587504][-3.5776222 -3.7740777 -4.0341372 -4.4045997 -4.7227798 -5.1789727 -5.4743881 -5.480689 -5.37373 -5.34949 -5.2519445 -4.9154835 -4.404449 -3.8505797 -3.3420706][-2.9649346 -3.2169037 -3.6154966 -3.9139473 -4.1344881 -4.4379048 -4.6724095 -4.9867864 -5.2226005 -5.3098674 -5.323328 -5.1541495 -4.7727661 -4.1915216 -3.6144686][-2.3647132 -2.3948014 -2.5470464 -2.8054268 -3.015018 -3.1532192 -3.2362952 -3.4333892 -3.8541908 -4.3177156 -4.7058382 -4.85096 -4.7294636 -4.3221822 -3.8220897][-1.7516894 -1.5583425 -1.352 -1.226305 -1.121738 -1.1010845 -1.1204863 -1.2508068 -1.6662087 -2.4629796 -3.3820279 -3.9727869 -4.2174616 -4.072526 -3.776819][-1.8152146 -1.3436584 -0.63524652 0.18619013 0.87879896 1.1953135 1.3734074 1.209321 0.59677839 -0.36439371 -1.5005074 -2.7035518 -3.6102009 -3.7617545 -3.619735][-2.0929539 -1.4205773 -0.37774229 0.90826845 2.0414691 2.8982086 3.3863473 3.2817006 2.7495885 1.5947452 0.13137388 -1.3156614 -2.4388952 -3.0332842 -3.293685][-2.6496096 -1.6842117 -0.45100164 1.0306392 2.2824883 3.4045224 4.0315466 3.9971619 3.6137094 2.619102 1.2002411 -0.464607 -1.7696033 -2.5194702 -2.893796][-3.5617654 -2.5695467 -1.3435009 0.027177334 1.1791778 2.4546623 3.2040925 3.3996191 3.2314014 2.3237586 1.0693378 -0.39095831 -1.6452439 -2.4965305 -2.9055815][-4.4368305 -3.7118146 -2.9146276 -1.746779 -0.58200407 0.504776 1.192462 1.729948 1.9072361 1.2693181 0.14731598 -1.1400902 -2.1662495 -2.8558321 -3.209341][-4.8001623 -4.0495439 -3.5510311 -3.094394 -2.6937137 -1.8536451 -1.1504686 -0.63391781 -0.27502632 -0.39453983 -0.99783516 -2.037771 -2.9839802 -3.5170484 -3.6914132][-5.1778216 -4.538353 -4.3837943 -4.0540218 -3.6256218 -3.3753746 -3.1983352 -2.6857915 -2.1978705 -2.0904105 -2.2867546 -2.8804355 -3.4977889 -3.8898396 -4.0751677][-4.9850845 -4.6670837 -4.5587769 -4.673708 -4.7999344 -4.5199208 -4.18006 -3.8009 -3.4468281 -3.2328 -3.222986 -3.433924 -3.7364576 -4.071023 -4.2482014][-4.563715 -4.4903245 -4.4409885 -4.53039 -4.594974 -4.599937 -4.5404077 -4.2727718 -3.862221 -3.513715 -3.3947668 -3.4933217 -3.6377811 -3.7952116 -3.9394822][-3.7324033 -3.6005054 -3.6088786 -3.8506117 -4.0408297 -4.0928144 -4.0762439 -3.8910236 -3.5547347 -3.376761 -3.2865772 -3.1716242 -3.1357682 -3.2654705 -3.3920288]]...]
INFO - root - 2017-12-16 07:04:21.507402: step 43010, loss = 0.31, batch loss = 0.25 (29.1 examples/sec; 0.274 sec/batch; 22h:04m:22s remains)
INFO - root - 2017-12-16 07:04:24.399367: step 43020, loss = 0.21, batch loss = 0.15 (27.8 examples/sec; 0.287 sec/batch; 23h:06m:32s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 07:04:27.280724: step 43030, loss = 0.27, batch loss = 0.21 (27.6 examples/sec; 0.290 sec/batch; 23h:20m:48s remains)
INFO - root - 2017-12-16 07:04:30.112513: step 43040, loss = 0.27, batch loss = 0.21 (28.7 examples/sec; 0.279 sec/batch; 22h:25m:41s remains)
INFO - root - 2017-12-16 07:04:32.943885: step 43050, loss = 0.30, batch loss = 0.24 (28.9 examples/sec; 0.277 sec/batch; 22h:14m:27s remains)
INFO - root - 2017-12-16 07:04:35.755555: step 43060, loss = 0.22, batch loss = 0.16 (27.9 examples/sec; 0.286 sec/batch; 23h:01m:45s remains)
INFO - root - 2017-12-16 07:04:38.651055: step 43070, loss = 0.24, batch loss = 0.18 (28.5 examples/sec; 0.281 sec/batch; 22h:36m:21s remains)
INFO - root - 2017-12-16 07:04:41.479952: step 43080, loss = 0.30, batch loss = 0.24 (28.5 examples/sec; 0.281 sec/batch; 22h:33m:53s remains)
INFO - root - 2017-12-16 07:04:44.345192: step 43090, loss = 0.31, batch loss = 0.25 (28.5 examples/sec; 0.281 sec/batch; 22h:34m:26s remains)
INFO - root - 2017-12-16 07:04:47.162841: step 43100, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 22h:57m:23s remains)
2017-12-16 07:04:47.636622: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0439732 -1.4982555 -1.3344767 -1.6141551 -1.9883947 -2.1597066 -2.3714821 -2.1944442 -2.0811281 -1.7793977 -1.8055406 -2.1129007 -2.4217086 -2.9057617 -3.402534][-1.0489707 -0.56019855 -0.2725091 -0.57942843 -1.077054 -1.3491278 -1.6226377 -1.8018727 -1.7390096 -1.5963092 -1.7336009 -1.8052142 -2.0388129 -2.3337343 -2.7437665][-0.3417778 0.072227478 0.25218964 0.10967684 -0.34866905 -0.94697332 -1.3829389 -1.6457925 -1.9599643 -2.0190682 -2.2428329 -2.5310178 -2.7392726 -2.7645674 -2.9281764][-0.098671913 0.28833246 0.60334349 0.57036114 0.13676167 -0.4781301 -1.0440016 -1.6893151 -2.3634734 -2.8088717 -3.1328683 -3.566818 -3.7933867 -3.5832672 -3.5048187][-0.64950085 -0.056535244 0.5598917 0.89763069 0.92449903 0.49570894 -0.15680027 -1.0844021 -2.0568669 -3.0308628 -3.864063 -4.3682256 -4.5387526 -4.4763274 -4.1334682][-1.2664726 -0.58271861 0.26290131 1.0277271 1.4808483 1.4990654 1.3200164 0.56618977 -0.53010964 -1.9995043 -3.4557164 -4.5159907 -5.0466137 -4.9487543 -4.5367675][-1.8083773 -0.93292212 0.032786846 0.94445705 1.7882061 2.3750515 2.4955559 1.8625469 0.87368107 -0.75583315 -2.6968942 -4.2037816 -5.0563221 -5.137887 -4.738287][-2.3861792 -1.5038469 -0.6259861 0.50607014 1.6919093 2.6778007 3.125555 2.6903477 1.5540605 -0.19927263 -2.2367678 -3.8224828 -4.761076 -4.866076 -4.4395084][-3.0145965 -2.3063822 -1.5106695 -0.55717254 0.59790373 1.7511621 2.4214025 2.2425857 1.2760587 -0.42068958 -2.4047339 -3.9307961 -4.7693591 -4.7951961 -4.3550038][-3.4033833 -2.9862368 -2.5524662 -1.8088245 -0.81454325 0.14226961 0.70460987 0.62256956 -0.092524529 -1.4707785 -3.1770754 -4.5358152 -5.3430986 -5.2395492 -4.8075414][-3.6910722 -3.4650774 -3.2887411 -2.9348996 -2.344995 -1.6536906 -1.2256534 -1.5044274 -2.1852283 -3.2572641 -4.5848532 -5.5327787 -6.0498767 -5.8427539 -5.3272214][-3.668288 -3.6641188 -3.7007723 -3.5502791 -3.3699756 -3.1671143 -2.9554996 -3.2786207 -3.9464796 -4.5740261 -5.3889971 -6.1875067 -6.3677988 -6.0797305 -5.6111512][-3.5677524 -3.5426245 -3.5343549 -3.52007 -3.6565633 -3.7972438 -4.0050712 -4.2799273 -4.6911411 -5.0713282 -5.392415 -5.63117 -5.6764374 -5.4574828 -5.1604958][-3.2292087 -3.1544576 -3.0729353 -2.905241 -3.0232625 -3.304791 -3.6542838 -4.1553521 -4.5301518 -4.5139422 -4.4131217 -4.1883693 -3.9883039 -3.8248143 -3.7614551][-3.0944214 -2.8891578 -2.6816587 -2.3722818 -2.2830551 -2.6202731 -3.1413331 -3.5476084 -3.719002 -3.5642402 -3.1604147 -2.5124354 -2.1327934 -2.0312786 -2.1061018]]...]
INFO - root - 2017-12-16 07:04:50.443277: step 43110, loss = 0.35, batch loss = 0.29 (28.3 examples/sec; 0.282 sec/batch; 22h:41m:54s remains)
INFO - root - 2017-12-16 07:04:53.246596: step 43120, loss = 0.22, batch loss = 0.16 (28.1 examples/sec; 0.285 sec/batch; 22h:52m:13s remains)
INFO - root - 2017-12-16 07:04:56.075645: step 43130, loss = 0.29, batch loss = 0.23 (28.2 examples/sec; 0.283 sec/batch; 22h:47m:02s remains)
INFO - root - 2017-12-16 07:04:58.888925: step 43140, loss = 0.23, batch loss = 0.18 (29.4 examples/sec; 0.272 sec/batch; 21h:54m:10s remains)
INFO - root - 2017-12-16 07:05:01.757373: step 43150, loss = 0.25, batch loss = 0.20 (26.8 examples/sec; 0.298 sec/batch; 23h:58m:28s remains)
INFO - root - 2017-12-16 07:05:04.578578: step 43160, loss = 0.23, batch loss = 0.17 (28.9 examples/sec; 0.277 sec/batch; 22h:16m:44s remains)
INFO - root - 2017-12-16 07:05:07.408193: step 43170, loss = 0.26, batch loss = 0.20 (26.5 examples/sec; 0.302 sec/batch; 24h:17m:35s remains)
INFO - root - 2017-12-16 07:05:10.262867: step 43180, loss = 0.34, batch loss = 0.28 (28.3 examples/sec; 0.283 sec/batch; 22h:44m:30s remains)
INFO - root - 2017-12-16 07:05:13.116957: step 43190, loss = 0.23, batch loss = 0.17 (28.1 examples/sec; 0.285 sec/batch; 22h:53m:23s remains)
INFO - root - 2017-12-16 07:05:15.918068: step 43200, loss = 0.22, batch loss = 0.16 (29.8 examples/sec; 0.268 sec/batch; 21h:34m:01s remains)
2017-12-16 07:05:16.387612: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5766153 -2.8208017 -3.2584844 -3.7516894 -4.1733508 -4.4317508 -4.5902548 -4.7248983 -4.7111616 -4.4331803 -3.9904327 -3.6274087 -3.2856603 -2.9547138 -2.6842141][-2.8487723 -3.2032251 -3.582679 -4.0668097 -4.5640025 -4.98375 -5.1548171 -5.0522866 -4.8832183 -4.7511826 -4.6316295 -4.3032918 -3.881073 -3.48427 -3.1268616][-3.2116737 -3.8293 -4.3941226 -4.7051172 -4.8100238 -4.9041805 -4.8475103 -4.7934165 -4.584198 -4.215271 -3.9916162 -4.1417346 -4.3202715 -4.0400839 -3.5811846][-3.5825813 -4.2313046 -4.7669082 -5.0397563 -4.8275728 -4.1720533 -3.5546196 -3.0880427 -2.720242 -2.8667722 -3.1723692 -3.4500351 -3.803782 -4.1072917 -3.9781678][-3.8084109 -4.288681 -4.5341339 -4.3938303 -3.7184441 -2.5565162 -1.3592622 -0.56272888 -0.43396783 -0.82870984 -1.6017327 -2.7312956 -3.6503127 -4.1305285 -4.1256347][-3.587852 -4.0046663 -4.0586004 -3.592237 -2.3290243 -0.67079163 0.89921761 2.1394181 2.3456092 1.3187165 -0.25388575 -1.9327562 -3.3282404 -4.1701713 -4.1317492][-2.9454746 -3.2085824 -3.0745788 -2.4097915 -0.98556423 0.91180563 2.8523936 4.0825253 4.2720995 3.1488557 1.1980829 -1.0053504 -2.9087782 -4.1055145 -4.3280711][-2.5498624 -2.3674662 -2.2035191 -1.3751628 0.055929661 1.8376179 3.6854534 4.7556305 4.7430019 3.497745 1.430099 -0.93195772 -3.0168231 -4.1506753 -4.3870378][-2.5351977 -2.1186912 -1.6033018 -1.0077381 0.0038299561 1.2889528 2.7649674 3.6359177 3.5779839 2.3740788 0.48048639 -1.6721048 -3.6304717 -4.7971063 -4.8935938][-2.5858743 -2.3196011 -1.7917557 -1.2075021 -0.551388 0.25712204 0.99870253 1.6879716 1.6513019 0.65052223 -1.0689845 -2.935823 -4.4627986 -5.411592 -5.4990764][-2.9767771 -2.5568275 -2.1601551 -1.7874982 -1.3759282 -0.82757688 -0.38064718 -0.29645824 -0.5807271 -1.4246907 -2.6392043 -4.0817637 -5.3411055 -5.8185582 -5.5641789][-3.284883 -2.4922252 -2.0328166 -1.9174914 -1.7760112 -1.7356925 -1.5798836 -1.7517214 -2.1876912 -3.103693 -4.2574973 -5.1483264 -5.7921357 -5.906476 -5.4854016][-3.4783897 -2.4955633 -1.8495896 -1.7119403 -1.8132954 -1.9229507 -2.0365427 -2.5321019 -3.3767967 -4.2110715 -5.0410395 -5.8295412 -6.1655879 -5.9233446 -5.1883979][-3.1768274 -2.0381277 -1.3402367 -1.017575 -1.189491 -1.6052113 -2.1903074 -2.584805 -3.4162493 -4.4857745 -5.430727 -5.8740168 -5.9189463 -5.6372457 -4.9067707][-2.2675188 -1.1994896 -0.52081776 -0.19119072 -0.21870708 -0.69589138 -1.4126241 -2.3034704 -3.3215246 -4.3265076 -5.0843811 -5.533546 -5.5579472 -4.93884 -4.1392422]]...]
INFO - root - 2017-12-16 07:05:19.191122: step 43210, loss = 0.22, batch loss = 0.16 (28.2 examples/sec; 0.284 sec/batch; 22h:48m:56s remains)
INFO - root - 2017-12-16 07:05:22.052373: step 43220, loss = 0.24, batch loss = 0.18 (25.5 examples/sec; 0.314 sec/batch; 25h:14m:44s remains)
INFO - root - 2017-12-16 07:05:24.936670: step 43230, loss = 0.24, batch loss = 0.18 (26.5 examples/sec; 0.302 sec/batch; 24h:17m:56s remains)
INFO - root - 2017-12-16 07:05:27.775315: step 43240, loss = 0.28, batch loss = 0.22 (27.9 examples/sec; 0.287 sec/batch; 23h:01m:24s remains)
INFO - root - 2017-12-16 07:05:30.611832: step 43250, loss = 0.33, batch loss = 0.28 (28.6 examples/sec; 0.280 sec/batch; 22h:30m:06s remains)
INFO - root - 2017-12-16 07:05:33.466417: step 43260, loss = 0.40, batch loss = 0.34 (25.4 examples/sec; 0.315 sec/batch; 25h:18m:02s remains)
INFO - root - 2017-12-16 07:05:36.321934: step 43270, loss = 0.25, batch loss = 0.19 (26.8 examples/sec; 0.299 sec/batch; 23h:59m:39s remains)
INFO - root - 2017-12-16 07:05:39.121624: step 43280, loss = 0.22, batch loss = 0.17 (28.0 examples/sec; 0.285 sec/batch; 22h:55m:56s remains)
INFO - root - 2017-12-16 07:05:41.924068: step 43290, loss = 0.35, batch loss = 0.30 (28.2 examples/sec; 0.284 sec/batch; 22h:48m:47s remains)
INFO - root - 2017-12-16 07:05:44.782883: step 43300, loss = 0.31, batch loss = 0.25 (28.9 examples/sec; 0.277 sec/batch; 22h:13m:08s remains)
2017-12-16 07:05:45.253588: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2713604 -4.8655624 -5.3763847 -5.7929497 -5.9826922 -6.1646914 -6.3231454 -6.5350313 -6.6683588 -6.6150651 -6.3736019 -6.2534342 -6.0006609 -5.439055 -4.9467096][-4.9001665 -5.6013036 -6.1037846 -6.5381002 -6.6257648 -6.9181218 -7.1287141 -7.23339 -7.5926666 -8.0457916 -8.3506613 -8.3793631 -8.0603094 -7.5276437 -6.9562244][-5.5143185 -6.4241033 -6.9062104 -6.9940844 -6.72713 -6.5238218 -6.3639941 -6.7487221 -7.362113 -7.897068 -8.7213879 -9.4983912 -9.7911463 -9.5304756 -8.9351263][-5.7443666 -6.5285664 -6.9134 -6.64933 -5.9908638 -5.2701564 -4.5734367 -4.4264174 -4.8610435 -6.2089987 -7.8212166 -9.1514292 -10.168354 -10.47429 -10.157166][-5.5936365 -6.2358217 -6.1505179 -5.304204 -4.061357 -2.4633617 -1.1784647 -0.83061576 -1.2195261 -2.652662 -4.7184963 -7.1049762 -9.3159218 -10.484034 -10.685663][-5.2514725 -5.4006429 -4.8816862 -3.5822461 -1.5643146 0.59447241 2.4948006 3.679965 3.6651049 1.9650474 -0.65155721 -3.6822464 -6.6322985 -8.8053532 -9.8659515][-4.9017124 -4.7096868 -3.7236285 -2.1532545 0.072088242 2.6951132 5.3026857 6.6567736 6.785059 5.7910395 3.4727159 0.028300762 -3.6055999 -6.3403397 -7.797256][-4.5687022 -4.27635 -3.5448532 -1.8460994 0.694839 3.0905085 5.472043 7.4030352 8.2356014 7.371748 5.0948477 2.137826 -1.1569118 -4.2539558 -6.1985931][-4.2307568 -4.0548973 -3.4770644 -2.1081798 -0.21391487 2.1817546 4.6572437 6.2828369 7.0771551 6.8998203 5.37132 2.6994753 -0.46106124 -3.203352 -5.092062][-4.3786025 -4.4187779 -4.1497655 -3.34731 -2.1182532 -0.366786 1.4864287 3.2726221 4.5222149 4.4394913 3.3558583 1.3688779 -0.97103882 -3.3218036 -5.1178203][-5.260397 -5.4903297 -5.3684516 -4.9369855 -4.271307 -3.3520894 -2.193682 -0.80561256 0.19855928 0.46484613 0.096023083 -1.2668817 -2.9572663 -4.6001639 -5.8569984][-5.8847013 -6.1131248 -6.0755939 -6.258688 -6.1816325 -5.6660404 -5.0008941 -4.2197061 -3.5099742 -3.0939574 -3.2456088 -4.12956 -5.1349077 -6.2197828 -6.8915787][-6.1432328 -6.536869 -6.8284206 -6.9700475 -7.063138 -7.1191063 -6.8331366 -6.2838869 -5.8838105 -5.6284413 -5.6041379 -5.9488335 -6.4400711 -7.1196194 -7.5157394][-5.8367152 -6.1803117 -6.5383024 -6.9249744 -7.3432055 -7.494307 -7.4073353 -7.2981291 -7.1303124 -6.8349633 -6.6709366 -6.835104 -7.0453019 -7.1819882 -7.136971][-5.2391095 -5.3276539 -5.4849515 -5.8963766 -6.4706788 -6.810317 -7.0073586 -6.9179945 -6.7584219 -6.7140589 -6.6933603 -6.4537821 -6.1749048 -6.1804996 -6.0301356]]...]
INFO - root - 2017-12-16 07:05:48.094739: step 43310, loss = 0.39, batch loss = 0.33 (27.8 examples/sec; 0.288 sec/batch; 23h:06m:30s remains)
INFO - root - 2017-12-16 07:05:50.893864: step 43320, loss = 0.22, batch loss = 0.16 (28.7 examples/sec; 0.279 sec/batch; 22h:24m:48s remains)
INFO - root - 2017-12-16 07:05:53.769601: step 43330, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.284 sec/batch; 22h:48m:38s remains)
INFO - root - 2017-12-16 07:05:56.579968: step 43340, loss = 0.38, batch loss = 0.33 (27.9 examples/sec; 0.286 sec/batch; 22h:59m:40s remains)
INFO - root - 2017-12-16 07:05:59.441178: step 43350, loss = 0.31, batch loss = 0.25 (28.0 examples/sec; 0.286 sec/batch; 22h:56m:24s remains)
INFO - root - 2017-12-16 07:06:02.284421: step 43360, loss = 0.26, batch loss = 0.20 (29.2 examples/sec; 0.274 sec/batch; 22h:00m:06s remains)
INFO - root - 2017-12-16 07:06:05.157081: step 43370, loss = 0.21, batch loss = 0.15 (28.9 examples/sec; 0.277 sec/batch; 22h:15m:28s remains)
INFO - root - 2017-12-16 07:06:07.993925: step 43380, loss = 0.34, batch loss = 0.28 (28.1 examples/sec; 0.285 sec/batch; 22h:51m:14s remains)
INFO - root - 2017-12-16 07:06:10.860890: step 43390, loss = 0.43, batch loss = 0.37 (26.4 examples/sec; 0.303 sec/batch; 24h:21m:26s remains)
INFO - root - 2017-12-16 07:06:13.717033: step 43400, loss = 0.29, batch loss = 0.23 (28.5 examples/sec; 0.281 sec/batch; 22h:33m:50s remains)
2017-12-16 07:06:14.182980: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1975536 -3.3144217 -3.3624711 -3.6014748 -3.9492965 -4.0445786 -3.8346591 -3.7082739 -3.7358255 -3.7881837 -4.0990591 -4.7148418 -5.4063287 -6.0127807 -6.4364228][-2.2795608 -2.6371269 -2.8170376 -3.1789131 -3.5861993 -3.7922986 -3.6719494 -3.5125856 -3.5432677 -3.7220442 -4.1013308 -4.840497 -5.8396711 -6.7012596 -7.3440108][-1.4831829 -1.8552873 -2.1199598 -2.4459171 -2.8229206 -3.1009245 -3.0905862 -3.0034611 -3.0870948 -3.3869822 -3.8592756 -4.7139912 -5.7837253 -6.6986771 -7.2966604][-0.70163345 -1.185581 -1.49785 -1.6778996 -1.8671062 -1.9800594 -1.9869955 -1.8793213 -1.9796753 -2.4783792 -3.1299753 -4.1493011 -5.2879725 -6.416132 -7.1683645][-0.57705355 -0.75315523 -0.7488296 -0.73865557 -0.66768813 -0.72329259 -0.81157494 -0.84408021 -1.1348963 -1.6928396 -2.3252151 -3.1941419 -4.0856977 -5.0564885 -5.6499352][-1.1603987 -0.75549626 -0.25013113 -0.013947487 0.2616725 0.4265852 0.42470503 0.14088774 -0.38913965 -0.97152424 -1.5305896 -2.3915179 -3.0875821 -3.5862234 -3.7019968][-2.1566534 -1.32301 -0.55648112 -0.017081738 0.47495127 0.6940136 0.74376917 0.50909662 0.080355167 -0.437649 -0.8405509 -1.223614 -1.341907 -1.4911468 -1.5328279][-2.9919407 -2.0633738 -1.198076 -0.49927998 0.046663761 0.46596956 0.72812986 0.57732391 0.22752333 -0.06595993 -0.15502071 -0.22731066 0.035885811 0.42051458 0.77426434][-3.8981991 -2.8413584 -1.8635197 -1.1256502 -0.61183429 -0.21113777 0.0924201 0.17906904 0.159626 -0.021715164 0.022716522 0.23155594 0.767879 1.3078194 1.7114053][-4.8016324 -3.6894667 -2.6981492 -1.9218326 -1.3382273 -1.0208199 -0.77134776 -0.725698 -0.6667695 -0.62377524 -0.42659807 -0.2418623 0.22273016 0.87691069 1.3678789][-5.0796657 -4.3558869 -3.7290573 -3.0610695 -2.5383186 -2.2102103 -1.8878098 -1.8819206 -1.7841086 -1.6576271 -1.4702449 -1.2102985 -0.73904824 -0.20034838 0.11944103][-5.258687 -4.8159156 -4.5147772 -4.1499343 -3.862788 -3.6793852 -3.4884253 -3.4753196 -3.3090959 -3.179945 -3.0341754 -2.8726125 -2.5884674 -2.2149386 -1.978339][-4.7961879 -4.6619048 -4.7159252 -4.7140417 -4.7109313 -4.7871132 -4.8596034 -4.947341 -4.8913684 -4.7936525 -4.7279687 -4.6480207 -4.613759 -4.3531036 -4.1628809][-3.825315 -3.9187655 -4.2741237 -4.6931219 -5.0157061 -5.3655295 -5.6953182 -6.045742 -6.287137 -6.1767673 -5.9915447 -5.864954 -5.835608 -5.6826124 -5.5834727][-3.1425059 -3.1246843 -3.5083103 -4.0320535 -4.5757937 -5.2174578 -5.838964 -6.4030118 -6.7934036 -6.8353653 -6.7566342 -6.5575786 -6.4203105 -6.2153883 -5.9816132]]...]
INFO - root - 2017-12-16 07:06:17.020965: step 43410, loss = 0.33, batch loss = 0.27 (29.4 examples/sec; 0.272 sec/batch; 21h:52m:41s remains)
INFO - root - 2017-12-16 07:06:19.824094: step 43420, loss = 0.30, batch loss = 0.24 (28.8 examples/sec; 0.278 sec/batch; 22h:18m:07s remains)
INFO - root - 2017-12-16 07:06:22.700991: step 43430, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 22h:53m:13s remains)
INFO - root - 2017-12-16 07:06:25.501947: step 43440, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.283 sec/batch; 22h:42m:53s remains)
INFO - root - 2017-12-16 07:06:28.387963: step 43450, loss = 0.24, batch loss = 0.19 (26.1 examples/sec; 0.306 sec/batch; 24h:34m:55s remains)
INFO - root - 2017-12-16 07:06:31.209881: step 43460, loss = 0.34, batch loss = 0.28 (28.5 examples/sec; 0.281 sec/batch; 22h:34m:04s remains)
INFO - root - 2017-12-16 07:06:33.966387: step 43470, loss = 0.21, batch loss = 0.16 (29.2 examples/sec; 0.274 sec/batch; 22h:00m:17s remains)
INFO - root - 2017-12-16 07:06:36.788250: step 43480, loss = 0.27, batch loss = 0.21 (28.6 examples/sec; 0.280 sec/batch; 22h:28m:35s remains)
INFO - root - 2017-12-16 07:06:39.636938: step 43490, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 22h:47m:03s remains)
INFO - root - 2017-12-16 07:06:42.466731: step 43500, loss = 0.28, batch loss = 0.22 (28.3 examples/sec; 0.282 sec/batch; 22h:39m:39s remains)
2017-12-16 07:06:42.964210: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5602298 -2.5950756 -2.6615176 -2.7438192 -2.8020558 -2.8916218 -2.961143 -2.8464837 -2.7391806 -2.5806279 -2.3616116 -2.3298805 -2.2595489 -2.2276304 -2.1804984][-2.45036 -2.389647 -2.411263 -2.6296558 -2.8412964 -2.8280392 -2.8038619 -2.7413075 -2.728816 -2.6284223 -2.4421306 -2.4044986 -2.2750556 -2.3031271 -2.2880254][-2.3071558 -2.1976655 -2.162838 -2.149884 -2.2359538 -2.2975295 -2.2525172 -2.233655 -2.2645397 -2.3316488 -2.4537349 -2.5348191 -2.5434122 -2.5096517 -2.3198235][-2.2435985 -2.15244 -2.0483367 -1.8658152 -1.7146785 -1.5945628 -1.4902678 -1.4790921 -1.5452292 -1.8240712 -2.176379 -2.2658031 -2.5047045 -2.6757483 -2.713026][-2.3488376 -1.9279976 -1.4450734 -1.2804739 -1.0977936 -0.79115081 -0.51252484 -0.42468643 -0.57134533 -0.93102646 -1.4214904 -1.9173689 -2.4384098 -2.7121496 -2.9292188][-2.2522953 -1.7980111 -1.1070983 -0.58139372 0.0023641586 0.42894077 0.80079365 1.0341272 0.86747551 0.36570406 -0.18090582 -0.819788 -1.6844733 -2.339056 -2.7404571][-1.8675075 -1.3204279 -0.62507868 0.13144875 1.0213299 1.5552726 1.9662886 2.0675159 1.8928781 1.4740839 0.71207094 -0.16081858 -1.109117 -1.8367941 -2.3284383][-2.0158112 -1.3857617 -0.6598649 0.18937588 1.1494942 1.7816377 2.3036222 2.2194171 1.912797 1.2848868 0.37914371 -0.3678627 -1.2915418 -2.0585043 -2.5336115][-2.2197688 -1.6439238 -0.97686291 -0.31765985 0.34641314 0.81661654 1.2825809 1.1662755 0.92640972 0.33030653 -0.37651634 -1.2059472 -2.2142467 -2.6993337 -2.9360852][-2.2638018 -1.768286 -1.1952798 -0.69144678 -0.29559755 -0.02967453 -0.086660385 -0.50124907 -0.875864 -1.308882 -1.8495178 -2.4993467 -3.1212654 -3.3643708 -3.4093504][-2.5182538 -2.0503411 -1.52195 -1.3003368 -1.2457812 -1.2560747 -1.3494539 -2.0537453 -2.8302572 -3.3387914 -3.6790626 -4.0130758 -4.3391767 -4.3279161 -4.1813078][-2.9053593 -2.4646091 -2.1296318 -2.1041062 -2.3108058 -2.8268256 -3.3700526 -4.1374178 -4.7614312 -5.1909328 -5.4731278 -5.5948038 -5.5002766 -5.2496052 -4.926024][-2.8896983 -2.459583 -2.2142098 -2.3734412 -2.8872755 -3.4406428 -4.0292678 -4.9924822 -5.8307357 -6.2339 -6.3498173 -6.3047314 -6.083209 -5.7955952 -5.4743805][-2.6952269 -2.3123109 -2.1139905 -2.137285 -2.5897622 -3.2691374 -3.9148688 -4.6404 -5.3074975 -5.697711 -5.8607149 -5.976737 -6.0080576 -5.7661119 -5.3766952][-2.6568005 -2.1235857 -1.8013518 -1.9414098 -2.3653376 -2.9689798 -3.6133311 -4.2326384 -4.7308044 -5.1103849 -5.3684616 -5.2552938 -5.134964 -5.0034294 -4.8373027]]...]
INFO - root - 2017-12-16 07:06:45.793336: step 43510, loss = 0.23, batch loss = 0.18 (28.2 examples/sec; 0.284 sec/batch; 22h:48m:26s remains)
INFO - root - 2017-12-16 07:06:48.623641: step 43520, loss = 0.40, batch loss = 0.34 (28.2 examples/sec; 0.283 sec/batch; 22h:45m:07s remains)
INFO - root - 2017-12-16 07:06:51.438022: step 43530, loss = 0.28, batch loss = 0.22 (29.5 examples/sec; 0.271 sec/batch; 21h:46m:31s remains)
INFO - root - 2017-12-16 07:06:54.293284: step 43540, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.279 sec/batch; 22h:23m:59s remains)
INFO - root - 2017-12-16 07:06:57.090635: step 43550, loss = 0.28, batch loss = 0.22 (29.8 examples/sec; 0.269 sec/batch; 21h:34m:04s remains)
INFO - root - 2017-12-16 07:06:59.910017: step 43560, loss = 0.40, batch loss = 0.35 (29.1 examples/sec; 0.275 sec/batch; 22h:05m:38s remains)
INFO - root - 2017-12-16 07:07:02.751167: step 43570, loss = 0.22, batch loss = 0.16 (28.3 examples/sec; 0.283 sec/batch; 22h:41m:45s remains)
INFO - root - 2017-12-16 07:07:05.577367: step 43580, loss = 0.28, batch loss = 0.22 (29.1 examples/sec; 0.275 sec/batch; 22h:04m:08s remains)
INFO - root - 2017-12-16 07:07:08.406601: step 43590, loss = 0.25, batch loss = 0.20 (29.1 examples/sec; 0.275 sec/batch; 22h:05m:34s remains)
INFO - root - 2017-12-16 07:07:11.262059: step 43600, loss = 0.26, batch loss = 0.20 (27.8 examples/sec; 0.287 sec/batch; 23h:04m:01s remains)
2017-12-16 07:07:11.732548: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0738425 -4.5104017 -5.2205458 -6.2802658 -7.2371211 -7.9051714 -8.3429127 -8.712142 -8.97057 -8.747942 -8.3782454 -8.0361624 -7.4222507 -6.8697128 -6.4402685][-3.2272892 -3.9905577 -4.8459435 -5.599998 -6.3750224 -7.1334248 -7.8138642 -7.816401 -7.945292 -7.9947395 -7.9667659 -7.7831707 -7.3328571 -6.837821 -6.4724245][-2.3301568 -2.9119263 -3.6927104 -4.3372536 -4.7816639 -5.2321172 -5.6064053 -5.889822 -6.1069107 -6.1620011 -6.4834805 -6.6727252 -6.5243196 -6.122767 -6.0688381][-1.8925741 -2.3791184 -2.8465424 -3.1273265 -3.0564256 -3.0892248 -3.1240284 -3.5105126 -3.8961496 -4.346683 -4.9028926 -5.3325982 -5.4311733 -5.2044249 -5.2408085][-1.8857713 -2.0516369 -2.0326376 -2.0753651 -1.8352211 -1.1901784 -0.95839691 -1.0596907 -1.5406828 -2.3192055 -3.0096905 -3.6696937 -3.9441361 -4.0741615 -4.284348][-1.8819242 -1.8651507 -1.841363 -1.3640983 -0.52337432 0.084880352 0.33116007 0.46278906 0.14515734 -0.60603476 -1.4454298 -2.0008113 -2.3421032 -2.4752731 -2.6287489][-2.0125127 -1.7855639 -1.6415076 -0.96005654 -0.081505775 0.87503862 1.5003562 1.397254 0.7920661 0.21457195 -0.20597267 -0.64859009 -0.72407269 -1.0485506 -1.2290564][-1.9041731 -1.8884652 -1.7832921 -1.1547761 -0.30111265 0.9214406 1.8744922 2.2013202 1.9246736 1.2438226 0.89309645 0.59498119 0.5802412 0.44555044 0.24022627][-2.0965271 -1.8109832 -1.7518849 -1.532187 -1.0804684 0.099567413 1.093864 1.7334619 2.0495553 1.8573904 1.6454687 1.3579159 1.453454 1.3641925 0.76971579][-1.9521444 -2.1705923 -2.5249748 -2.402472 -2.0097938 -1.1100285 -0.41045713 0.4199295 1.0432448 1.1956215 1.4877119 1.5589695 1.8231602 1.5384793 0.83331633][-2.848618 -2.8144748 -3.1216474 -3.393677 -3.421937 -2.784122 -1.9244177 -0.98977327 -0.25398827 0.21327829 0.71209764 1.1937966 1.6218271 1.3691669 0.455276][-3.1091046 -3.3990622 -4.1077886 -4.4205227 -4.4839525 -4.2271209 -4.1049981 -3.2219582 -2.239404 -1.4523373 -0.64376688 0.144773 0.65293932 0.26904202 -0.96866465][-3.6389709 -3.7618027 -4.2699447 -4.7809639 -5.6587381 -6.0427275 -6.09316 -5.595964 -5.0250134 -3.9244137 -2.8079243 -2.0532868 -1.4946604 -1.8374109 -2.614671][-3.9246106 -4.1830587 -4.7760062 -5.2536993 -5.9169321 -6.5338778 -7.1697764 -7.3447256 -7.1503429 -6.1840448 -5.0105376 -4.0415435 -3.666491 -4.0034661 -4.3156934][-4.5542479 -4.4207482 -4.6116171 -5.4556279 -6.3642883 -7.0397816 -7.6001177 -7.7877331 -7.7518578 -7.4726 -6.7909813 -6.0991182 -5.7094469 -5.8843737 -5.9893508]]...]
INFO - root - 2017-12-16 07:07:14.518356: step 43610, loss = 0.24, batch loss = 0.18 (28.7 examples/sec; 0.279 sec/batch; 22h:23m:57s remains)
INFO - root - 2017-12-16 07:07:17.406121: step 43620, loss = 0.25, batch loss = 0.19 (26.3 examples/sec; 0.305 sec/batch; 24h:26m:28s remains)
INFO - root - 2017-12-16 07:07:20.194311: step 43630, loss = 0.33, batch loss = 0.27 (29.1 examples/sec; 0.275 sec/batch; 22h:04m:49s remains)
INFO - root - 2017-12-16 07:07:23.050411: step 43640, loss = 0.28, batch loss = 0.22 (28.6 examples/sec; 0.280 sec/batch; 22h:26m:15s remains)
INFO - root - 2017-12-16 07:07:25.882632: step 43650, loss = 0.24, batch loss = 0.18 (28.7 examples/sec; 0.279 sec/batch; 22h:23m:47s remains)
INFO - root - 2017-12-16 07:07:28.666999: step 43660, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.282 sec/batch; 22h:37m:19s remains)
INFO - root - 2017-12-16 07:07:31.470543: step 43670, loss = 0.24, batch loss = 0.19 (28.4 examples/sec; 0.282 sec/batch; 22h:36m:07s remains)
INFO - root - 2017-12-16 07:07:34.320733: step 43680, loss = 0.32, batch loss = 0.26 (28.1 examples/sec; 0.285 sec/batch; 22h:51m:21s remains)
INFO - root - 2017-12-16 07:07:37.174301: step 43690, loss = 0.35, batch loss = 0.29 (28.0 examples/sec; 0.286 sec/batch; 22h:57m:33s remains)
INFO - root - 2017-12-16 07:07:40.037133: step 43700, loss = 0.23, batch loss = 0.18 (27.5 examples/sec; 0.291 sec/batch; 23h:18m:16s remains)
2017-12-16 07:07:40.499363: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4673796 -5.2774444 -4.9618025 -4.270833 -3.8028512 -3.3353362 -3.1721048 -3.1914759 -3.3207374 -3.7181969 -4.0783472 -4.2348471 -4.2035942 -4.2043371 -3.9873118][-6.3435125 -6.0079279 -5.5568628 -4.7770367 -4.2297931 -3.8863871 -3.6883488 -3.545495 -3.5579369 -3.949435 -4.213232 -4.3079209 -4.2927341 -4.1861176 -3.927249][-6.2110214 -5.7869387 -5.3640356 -4.6165986 -4.1518512 -3.9167385 -3.7230935 -3.5122719 -3.4406571 -3.735023 -4.0414748 -4.326396 -4.4814425 -4.4974394 -4.2979684][-5.2992654 -4.7578521 -4.3016114 -3.7866068 -3.5704184 -3.3546708 -3.0725491 -2.7765136 -2.7121067 -3.0817809 -3.5070381 -4.0238495 -4.3610053 -4.5725651 -4.574193][-3.3354785 -2.7731607 -2.4771082 -2.1991558 -2.1563847 -2.0813429 -1.783345 -1.6003757 -1.5701962 -2.0876853 -2.7708826 -3.5649 -4.1706314 -4.6705074 -4.7260146][-1.2281575 -0.24644136 -0.080785275 -0.087427139 -0.33233118 -0.48134732 -0.13104391 -0.0032968521 0.014309406 -0.5889132 -1.5058358 -2.7250743 -3.8602309 -4.6064091 -4.78108][0.76530266 1.6410675 1.6705351 1.3468266 0.74577522 0.67264938 1.244328 1.630619 1.755794 1.0719628 -0.080443859 -1.5813663 -3.0480409 -4.1276584 -4.4006982][0.86019611 1.7951288 1.9895682 1.7006011 1.2713933 1.4781289 2.2499099 2.8501406 3.0588145 2.2677388 0.88561392 -0.90024924 -2.6390553 -3.7943313 -4.1458979][-0.47239232 0.32539129 0.59269571 0.43511486 0.46446228 1.0808349 2.1007528 3.0446348 3.3935871 2.7628717 1.3747206 -0.47878838 -2.2687423 -3.5985742 -4.0900035][-2.3246775 -1.9431443 -1.6950424 -1.5319836 -1.31111 -0.37072182 0.95703554 2.1357574 2.7067804 2.2457876 0.96598291 -0.71533418 -2.3946381 -3.7220068 -4.3984256][-4.0417418 -3.7996154 -3.7299252 -3.6067605 -3.4146705 -2.1445866 -0.69283938 0.55039787 1.203805 1.0628858 0.10366583 -1.4219313 -2.8793378 -4.1700363 -4.9367704][-5.6552744 -5.7105889 -5.7994728 -5.5720472 -5.1531367 -3.8572202 -2.5761151 -1.2037995 -0.46731544 -0.50048423 -1.1259458 -2.4338057 -3.5829561 -4.5959177 -5.2234383][-6.5318251 -6.5369854 -6.5374866 -6.5816269 -6.5417156 -5.7341356 -4.8412209 -3.6704359 -2.837224 -2.7077606 -2.9281082 -3.6973145 -4.5238123 -5.256578 -5.5800819][-6.4984455 -6.4004936 -6.45091 -6.6154118 -6.8466063 -6.5372686 -6.1344824 -5.3941607 -4.6898346 -4.1851721 -4.0178432 -4.4424973 -4.8210473 -5.1728678 -5.35014][-5.981945 -5.9683285 -6.1612716 -6.4139609 -6.7337446 -6.7525015 -6.5798688 -5.8660612 -4.9566574 -4.4333787 -4.2110009 -4.070312 -4.1250105 -4.4966469 -4.6225634]]...]
INFO - root - 2017-12-16 07:07:43.301993: step 43710, loss = 0.27, batch loss = 0.21 (29.9 examples/sec; 0.268 sec/batch; 21h:27m:51s remains)
INFO - root - 2017-12-16 07:07:46.108788: step 43720, loss = 0.31, batch loss = 0.25 (28.6 examples/sec; 0.280 sec/batch; 22h:25m:59s remains)
INFO - root - 2017-12-16 07:07:48.991624: step 43730, loss = 0.21, batch loss = 0.15 (28.0 examples/sec; 0.286 sec/batch; 22h:55m:11s remains)
INFO - root - 2017-12-16 07:07:51.797136: step 43740, loss = 0.24, batch loss = 0.18 (29.0 examples/sec; 0.276 sec/batch; 22h:09m:27s remains)
INFO - root - 2017-12-16 07:07:54.620882: step 43750, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.279 sec/batch; 22h:25m:00s remains)
INFO - root - 2017-12-16 07:07:57.515954: step 43760, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.284 sec/batch; 22h:48m:37s remains)
INFO - root - 2017-12-16 07:08:00.325990: step 43770, loss = 0.31, batch loss = 0.25 (28.1 examples/sec; 0.284 sec/batch; 22h:48m:24s remains)
INFO - root - 2017-12-16 07:08:03.191110: step 43780, loss = 0.28, batch loss = 0.22 (26.5 examples/sec; 0.302 sec/batch; 24h:11m:27s remains)
INFO - root - 2017-12-16 07:08:06.019965: step 43790, loss = 0.28, batch loss = 0.22 (28.6 examples/sec; 0.279 sec/batch; 22h:23m:52s remains)
INFO - root - 2017-12-16 07:08:08.940562: step 43800, loss = 0.21, batch loss = 0.15 (27.5 examples/sec; 0.291 sec/batch; 23h:17m:50s remains)
2017-12-16 07:08:09.396766: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7421083 -6.0072608 -6.0123916 -5.9165468 -5.7484803 -5.6824527 -5.5385532 -5.3939037 -5.4476252 -5.3866539 -5.1574516 -4.8044329 -4.4855247 -3.9901807 -3.6082082][-5.8271723 -5.9932747 -5.8529987 -5.6819363 -5.448432 -5.5057783 -5.2061663 -5.1029372 -5.1244268 -5.060986 -4.6922169 -4.0216646 -3.4629464 -2.7079206 -2.2128685][-5.6469383 -5.6775322 -5.3668184 -4.819376 -4.3330579 -4.1807504 -4.0462737 -4.0314755 -4.2411518 -4.4289045 -3.9898345 -3.2513032 -2.5558896 -1.7703934 -1.0395842][-4.3287396 -4.2914629 -3.6911507 -2.837204 -2.3186462 -2.2482731 -2.2512403 -2.3923666 -3.0745606 -3.4357362 -3.2265992 -2.8050079 -2.3064387 -1.7389491 -1.1049979][-3.0901165 -2.590591 -1.4446416 -0.60229087 0.016923428 -0.024802208 -0.15161657 -0.52115512 -1.4850709 -2.1596797 -2.4458723 -2.4341259 -2.3810704 -2.2771626 -1.8431563][-2.5454328 -1.6780307 -0.25398779 1.2758789 2.3612413 2.4187574 2.275125 1.6193151 0.51264334 -0.5917244 -1.3513732 -1.9101024 -2.5559871 -2.6870441 -2.5508142][-2.4580145 -1.259326 0.23486376 2.0971785 3.4908171 3.9772615 3.9925737 3.1207051 1.7711973 0.49424696 -0.35543728 -1.1990771 -2.1216428 -2.7440476 -2.9695289][-3.2913973 -2.010896 0.076901436 2.1209245 3.2699442 3.9280939 4.1064882 3.5011044 2.2597976 0.90761518 -0.029564381 -1.0319302 -1.9370379 -2.7809658 -3.3397336][-4.373714 -3.42235 -1.7060213 0.28311062 1.8971252 2.8793974 3.1421475 2.7694812 2.0090857 0.77264166 -0.32630205 -1.380234 -2.4874778 -3.3336954 -3.9171367][-5.599988 -5.1148338 -3.9722981 -2.137037 -0.31238413 0.90641451 1.5738506 1.2701106 0.61743307 -0.41595507 -1.3495877 -2.3697143 -3.3321886 -3.9334788 -4.377882][-6.8361311 -6.518909 -5.8620939 -4.5037408 -3.0155802 -1.8679085 -0.92682028 -0.81752276 -1.2704866 -2.2925386 -3.1272345 -3.9378946 -4.5030074 -4.7180223 -4.7333422][-7.0555735 -7.1266918 -6.7419682 -5.7885184 -4.75681 -3.8387311 -3.1621568 -2.9505553 -2.9648547 -3.5024734 -4.198719 -4.7639203 -5.0768394 -5.0581732 -4.8795753][-5.9880691 -6.4222431 -6.5896683 -6.1282873 -5.6702867 -5.1287417 -4.47222 -4.1709552 -4.1858263 -4.4376073 -4.5868249 -5.0223594 -5.4083252 -5.0909958 -4.8204393][-4.9522576 -5.2213874 -5.5809445 -5.6339526 -5.6406584 -5.4239655 -5.0711913 -4.9702291 -4.7746143 -4.7046318 -4.6042361 -4.6641974 -4.7223554 -4.397861 -4.0693994][-4.0875778 -4.2785139 -4.4953933 -4.6776981 -4.8393154 -4.9776754 -4.93487 -4.7606435 -4.4393559 -4.1488271 -4.0221376 -3.967561 -3.6018045 -3.1693423 -2.7384212]]...]
INFO - root - 2017-12-16 07:08:12.216451: step 43810, loss = 0.31, batch loss = 0.25 (28.1 examples/sec; 0.285 sec/batch; 22h:49m:27s remains)
INFO - root - 2017-12-16 07:08:15.020957: step 43820, loss = 0.27, batch loss = 0.21 (28.1 examples/sec; 0.285 sec/batch; 22h:49m:01s remains)
INFO - root - 2017-12-16 07:08:17.850503: step 43830, loss = 0.26, batch loss = 0.20 (27.3 examples/sec; 0.293 sec/batch; 23h:31m:01s remains)
INFO - root - 2017-12-16 07:08:20.708358: step 43840, loss = 0.39, batch loss = 0.33 (26.0 examples/sec; 0.308 sec/batch; 24h:42m:59s remains)
INFO - root - 2017-12-16 07:08:23.542485: step 43850, loss = 0.23, batch loss = 0.17 (27.8 examples/sec; 0.288 sec/batch; 23h:05m:19s remains)
INFO - root - 2017-12-16 07:08:26.424557: step 43860, loss = 0.34, batch loss = 0.28 (28.1 examples/sec; 0.285 sec/batch; 22h:49m:54s remains)
INFO - root - 2017-12-16 07:08:29.253146: step 43870, loss = 0.25, batch loss = 0.19 (27.0 examples/sec; 0.296 sec/batch; 23h:44m:29s remains)
INFO - root - 2017-12-16 07:08:32.102513: step 43880, loss = 0.26, batch loss = 0.20 (28.4 examples/sec; 0.282 sec/batch; 22h:34m:49s remains)
INFO - root - 2017-12-16 07:08:34.919796: step 43890, loss = 0.27, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 22h:35m:17s remains)
INFO - root - 2017-12-16 07:08:37.748789: step 43900, loss = 0.37, batch loss = 0.31 (29.3 examples/sec; 0.273 sec/batch; 21h:53m:50s remains)
2017-12-16 07:08:38.210765: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4648211 -2.5410132 -2.5086675 -2.412607 -2.4122291 -2.3618515 -2.5233054 -2.8174486 -3.3134866 -3.6102057 -3.5359716 -3.5172288 -3.3433785 -3.1037102 -2.7088761][-1.9087322 -2.1109228 -2.2254813 -2.1081049 -2.1906922 -2.3136637 -2.5291882 -2.7839456 -3.015214 -3.2402263 -3.2595489 -3.3448095 -3.1986895 -2.833602 -2.5123005][-2.1142178 -2.2447865 -2.2346594 -1.937953 -1.7229412 -1.5987909 -1.7123907 -2.1615417 -2.5800302 -2.8155046 -2.8857074 -3.1728294 -3.3970468 -3.1783767 -2.7186162][-2.4033716 -2.461555 -2.2645118 -1.8157432 -1.4359949 -1.0452554 -0.90922141 -1.2270832 -1.6405652 -2.0273449 -2.3608577 -2.9350624 -3.394628 -3.3548872 -3.1256824][-2.7914972 -2.6487789 -2.1921403 -1.3395956 -0.52901912 0.1596427 0.51353121 0.17002916 -0.36222935 -1.0205567 -1.746722 -2.4689634 -3.1063552 -3.3100705 -3.259973][-2.9541268 -2.5683413 -1.7435203 -0.65972567 0.39655638 1.3952017 2.0059423 1.8580709 1.3292489 0.4422946 -0.74457932 -2.0106306 -3.0282011 -3.2722483 -3.1788478][-2.9363074 -2.2987974 -1.1964655 0.10329485 1.2599139 2.3133445 2.9206953 2.6596742 1.8661866 0.85553074 -0.40139866 -1.7872145 -2.8724184 -3.2449665 -3.1400526][-2.7765918 -2.0482697 -1.0117078 0.25202894 1.5220804 2.6030335 3.1596236 2.7208066 1.7500978 0.62826872 -0.676821 -2.0330288 -3.0672059 -3.5300577 -3.592402][-2.8622737 -2.2626283 -1.3565695 -0.23265266 0.84122372 1.60567 2.1398759 1.9006348 1.0309796 -0.084226608 -1.3029456 -2.4049838 -3.2722707 -3.8800895 -4.0317421][-2.9681587 -2.5492353 -1.9211278 -1.1658909 -0.33850718 0.34472322 0.65688419 0.23444033 -0.57416034 -1.4679317 -2.420481 -3.4101269 -4.1871781 -4.5934129 -4.6435313][-3.14508 -3.0099356 -2.4368758 -1.9533789 -1.5664713 -1.1647971 -0.99769759 -1.3524485 -2.0825183 -2.7783828 -3.424109 -4.1885366 -4.8547497 -5.1591339 -5.1872849][-3.0665636 -3.0402606 -2.8799462 -2.6132331 -2.1979907 -2.0282903 -2.1705086 -2.5138843 -3.0227573 -3.4965329 -4.0482774 -4.520083 -4.8801088 -5.2391434 -5.41173][-3.2291746 -3.2739491 -3.1615105 -2.802125 -2.4174621 -2.3104324 -2.3867323 -2.6427581 -3.0995965 -3.6011829 -4.0375881 -4.3554463 -4.5690422 -4.75298 -4.8337965][-3.4227483 -3.3067565 -3.00987 -2.7016075 -2.3920016 -2.0510578 -1.9827571 -2.2193251 -2.5749493 -2.7777081 -3.0675302 -3.3253059 -3.3988979 -3.433285 -3.6252244][-2.9641318 -2.7339778 -2.319155 -1.8311746 -1.2898777 -0.82923126 -0.68870878 -0.83790112 -1.2031844 -1.4713833 -1.7720976 -1.890065 -1.8522222 -1.8874369 -2.113157]]...]
INFO - root - 2017-12-16 07:08:41.069543: step 43910, loss = 0.29, batch loss = 0.23 (28.9 examples/sec; 0.276 sec/batch; 22h:09m:28s remains)
INFO - root - 2017-12-16 07:08:43.924448: step 43920, loss = 0.22, batch loss = 0.16 (29.3 examples/sec; 0.273 sec/batch; 21h:54m:44s remains)
INFO - root - 2017-12-16 07:08:46.821298: step 43930, loss = 0.25, batch loss = 0.20 (27.1 examples/sec; 0.295 sec/batch; 23h:37m:31s remains)
INFO - root - 2017-12-16 07:08:49.605303: step 43940, loss = 0.23, batch loss = 0.18 (27.9 examples/sec; 0.287 sec/batch; 22h:59m:46s remains)
INFO - root - 2017-12-16 07:08:52.427074: step 43950, loss = 0.28, batch loss = 0.22 (28.2 examples/sec; 0.284 sec/batch; 22h:46m:23s remains)
INFO - root - 2017-12-16 07:08:55.278802: step 43960, loss = 0.35, batch loss = 0.30 (29.4 examples/sec; 0.272 sec/batch; 21h:46m:41s remains)
INFO - root - 2017-12-16 07:08:58.128788: step 43970, loss = 0.23, batch loss = 0.17 (26.4 examples/sec; 0.303 sec/batch; 24h:18m:17s remains)
INFO - root - 2017-12-16 07:09:00.938686: step 43980, loss = 0.28, batch loss = 0.22 (29.4 examples/sec; 0.272 sec/batch; 21h:47m:23s remains)
INFO - root - 2017-12-16 07:09:03.785149: step 43990, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.286 sec/batch; 22h:53m:21s remains)
INFO - root - 2017-12-16 07:09:06.666371: step 44000, loss = 0.28, batch loss = 0.22 (26.8 examples/sec; 0.298 sec/batch; 23h:55m:07s remains)
2017-12-16 07:09:07.121713: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4783809 -1.5764973 -1.6401086 -1.6328211 -1.6026165 -1.4112363 -1.4099674 -1.4733853 -1.7007997 -2.0133338 -2.1153786 -2.2852054 -2.409349 -2.6784046 -2.808507][-0.97417712 -1.2853589 -1.6466205 -1.7408071 -1.8369226 -1.7735591 -1.8051405 -1.6476564 -1.6427727 -1.9502428 -2.2027574 -2.2854235 -2.2530186 -2.4813061 -2.8088074][-1.286402 -1.7992258 -2.3414793 -2.5879729 -2.4924955 -2.2155218 -1.9838083 -1.7667229 -1.6502471 -1.7792375 -2.0807171 -2.4052253 -2.589088 -2.755527 -2.9328587][-1.9183786 -2.5683067 -3.1807494 -3.3991127 -3.0981934 -2.4390836 -1.7439003 -1.1990807 -0.90419745 -1.1056507 -1.5577648 -2.0874758 -2.4600148 -2.64319 -2.8191288][-2.0897396 -2.9652574 -3.7894967 -3.8367951 -3.2696924 -2.3623295 -1.3957839 -0.51238227 -0.012585163 -0.24909067 -0.78811336 -1.4148848 -1.8626795 -1.991189 -2.1121743][-2.5049851 -3.0271418 -3.7335927 -3.7477388 -3.1093655 -1.9527707 -0.577682 0.66358948 1.313139 1.01899 0.17466068 -0.53901815 -0.93947005 -1.1354625 -1.304841][-2.1917124 -2.694617 -3.1874657 -3.1293325 -2.4164593 -1.0199592 0.66374588 1.8362236 2.4504623 2.1522489 1.1758561 0.19796181 -0.32723761 -0.44838643 -0.52463245][-1.1145632 -1.6607223 -2.393198 -2.2952802 -1.4892609 -0.14275599 1.4710331 2.6512947 3.133039 2.5784616 1.5631804 0.75884676 0.27776527 0.14739418 0.011497974][-0.19581127 -0.8841486 -1.7735426 -1.8947504 -1.2955911 0.0338068 1.4420929 2.4819474 2.9246297 2.433866 1.5569153 0.76066971 0.36264133 0.36237812 0.28247404][0.28363371 -0.43054152 -1.6956575 -1.9936028 -1.6580129 -0.55439472 0.62837887 1.4538708 1.7943907 1.4551249 0.87155581 0.34668446 0.050442219 -0.0083928108 -0.034542084][0.10889912 -0.46841121 -1.5164864 -2.1128104 -2.2493386 -1.4356184 -0.38716364 0.32310057 0.72741795 0.44090414 -0.16130209 -0.59446168 -0.6664381 -0.49857068 -0.52742553][-0.10269833 -0.78816533 -1.8460145 -2.3535857 -2.3478103 -1.8395653 -1.2349806 -0.87639046 -0.61977839 -0.67662549 -1.0842187 -1.5113044 -1.5306561 -1.1632476 -0.86383438][-0.19358587 -0.75650549 -1.5726564 -1.9284637 -1.9912891 -1.7852538 -1.6139534 -1.4389408 -1.3832278 -1.5476301 -1.883152 -2.2198241 -2.1994605 -1.6837287 -1.0998867][-0.2495575 -0.39485407 -0.743649 -1.1064138 -1.3026249 -1.2010112 -1.1596978 -1.4723186 -1.7093234 -1.9262309 -2.3433552 -2.8069417 -2.6591368 -1.8618877 -1.0088501][-0.08302784 0.01859808 -0.11838388 -0.13949299 -0.12889481 -0.03824234 -0.41796637 -0.97723126 -1.5237591 -2.0505207 -2.6582296 -3.1117663 -2.9371877 -2.0110161 -0.75381422]]...]
INFO - root - 2017-12-16 07:09:09.953592: step 44010, loss = 0.33, batch loss = 0.27 (28.2 examples/sec; 0.284 sec/batch; 22h:46m:03s remains)
INFO - root - 2017-12-16 07:09:12.804662: step 44020, loss = 0.24, batch loss = 0.18 (26.2 examples/sec; 0.305 sec/batch; 24h:27m:44s remains)
INFO - root - 2017-12-16 07:09:15.666922: step 44030, loss = 0.20, batch loss = 0.14 (27.8 examples/sec; 0.288 sec/batch; 23h:05m:53s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 07:09:18.532361: step 44040, loss = 0.25, batch loss = 0.19 (27.7 examples/sec; 0.288 sec/batch; 23h:06m:56s remains)
INFO - root - 2017-12-16 07:09:21.331711: step 44050, loss = 0.24, batch loss = 0.18 (29.6 examples/sec; 0.270 sec/batch; 21h:37m:56s remains)
INFO - root - 2017-12-16 07:09:24.187973: step 44060, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.285 sec/batch; 22h:49m:15s remains)
INFO - root - 2017-12-16 07:09:27.031767: step 44070, loss = 0.23, batch loss = 0.17 (27.8 examples/sec; 0.287 sec/batch; 23h:01m:02s remains)
INFO - root - 2017-12-16 07:09:29.808728: step 44080, loss = 0.26, batch loss = 0.20 (29.2 examples/sec; 0.274 sec/batch; 21h:55m:08s remains)
INFO - root - 2017-12-16 07:09:32.590473: step 44090, loss = 0.28, batch loss = 0.22 (28.0 examples/sec; 0.286 sec/batch; 22h:54m:01s remains)
INFO - root - 2017-12-16 07:09:35.437225: step 44100, loss = 0.30, batch loss = 0.24 (28.0 examples/sec; 0.286 sec/batch; 22h:52m:43s remains)
2017-12-16 07:09:35.908791: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1173265 -2.415997 -2.8298597 -3.4108334 -3.8265803 -4.1543636 -4.2394195 -4.319036 -4.4099846 -4.2613125 -4.045702 -3.8519058 -3.5222278 -3.22306 -3.0095778][-1.6108301 -1.8445935 -1.9848976 -2.4186401 -3.0190196 -3.7037978 -4.0304546 -4.1945882 -4.2303224 -4.2322431 -4.2720351 -4.1138 -3.7976589 -3.5979466 -3.2800503][-0.61294556 -0.50856113 -0.82135868 -1.2659855 -1.9395902 -2.9389954 -3.7200756 -4.1080241 -4.3291211 -4.5313468 -4.6011147 -4.5764556 -4.4738693 -4.12462 -3.7246718][0.64728689 1.2468262 1.1086903 0.21535349 -0.86947608 -1.8299088 -2.5290856 -2.9703698 -3.3917665 -4.00146 -4.4819703 -4.6588326 -4.8137107 -4.6426568 -4.2823057][2.3987489 2.9939179 2.6807833 1.7389503 0.6592555 -0.41357946 -0.99270821 -1.3717673 -1.9871418 -2.7491684 -3.6036854 -4.2628903 -4.4997897 -4.31699 -3.9737222][2.877336 3.9459839 3.7819815 2.7258849 1.8276529 0.87875652 0.27773428 -0.016366482 -0.57582068 -1.6932552 -2.9415555 -3.6869917 -4.0693817 -4.0314503 -3.6729131][2.7891402 3.7872133 3.547966 2.9935493 2.5846639 2.1233554 1.894906 1.3319697 0.60870695 -0.6438241 -2.21526 -3.2078552 -3.7580774 -3.724041 -3.3508725][1.8099022 2.7449069 2.5317841 2.3299704 2.4751563 2.8050084 3.0802484 2.5697331 1.6475644 0.14135456 -1.4221444 -2.4574375 -3.0632024 -2.9445109 -2.6318827][-0.81527996 0.026633739 0.57009268 0.89319611 1.4535165 2.2047267 2.8666387 2.7066717 1.7747664 0.40484953 -0.95995426 -1.9504383 -2.4547918 -2.1940436 -1.7133689][-2.8962767 -2.4806592 -1.9579716 -1.0417292 0.12139225 0.84767342 1.4369154 1.3297367 0.64139557 -0.40385628 -1.5669055 -2.2211819 -2.272279 -1.8927076 -1.2848427][-5.0306025 -4.6731534 -4.1898875 -3.3437943 -2.0474207 -1.1029112 -0.34248543 -0.32092762 -0.88252497 -1.7005277 -2.4330864 -2.5604625 -2.1875556 -1.4566863 -0.775481][-6.1117983 -6.3489265 -6.1165328 -5.4661303 -4.7588058 -3.8138943 -2.8041167 -2.3688586 -2.3420293 -2.5344191 -2.7500029 -2.5972321 -1.9814923 -1.2431319 -0.76219106][-6.4519634 -7.0298834 -7.3341208 -7.3119373 -6.7288842 -6.0225687 -5.0970421 -3.9249086 -3.0653286 -2.6415582 -2.5560436 -2.2150612 -1.6236699 -1.1269748 -0.77611113][-5.7788687 -6.7748442 -7.6307068 -8.1554747 -7.8882551 -6.9007072 -5.5263233 -4.1381373 -2.9967611 -1.9897952 -1.5378597 -1.4172668 -1.1408963 -0.77256584 -0.66079521][-4.949955 -5.9186163 -7.09908 -7.967248 -7.8656244 -7.0250292 -5.637289 -3.7360489 -2.1569796 -1.1293814 -0.68967104 -0.5634973 -0.45461583 -0.38811207 -0.40345383]]...]
INFO - root - 2017-12-16 07:09:38.712837: step 44110, loss = 0.24, batch loss = 0.18 (28.9 examples/sec; 0.276 sec/batch; 22h:08m:21s remains)
INFO - root - 2017-12-16 07:09:41.521284: step 44120, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.284 sec/batch; 22h:45m:56s remains)
INFO - root - 2017-12-16 07:09:44.349372: step 44130, loss = 0.36, batch loss = 0.30 (28.4 examples/sec; 0.282 sec/batch; 22h:35m:11s remains)
INFO - root - 2017-12-16 07:09:47.193300: step 44140, loss = 0.30, batch loss = 0.24 (29.2 examples/sec; 0.274 sec/batch; 21h:55m:01s remains)
INFO - root - 2017-12-16 07:09:50.063409: step 44150, loss = 0.27, batch loss = 0.21 (27.8 examples/sec; 0.287 sec/batch; 23h:00m:47s remains)
INFO - root - 2017-12-16 07:09:52.850516: step 44160, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.281 sec/batch; 22h:31m:37s remains)
INFO - root - 2017-12-16 07:09:55.673796: step 44170, loss = 0.25, batch loss = 0.19 (27.2 examples/sec; 0.294 sec/batch; 23h:33m:17s remains)
INFO - root - 2017-12-16 07:09:58.474641: step 44180, loss = 0.29, batch loss = 0.23 (29.3 examples/sec; 0.273 sec/batch; 21h:50m:22s remains)
INFO - root - 2017-12-16 07:10:01.306549: step 44190, loss = 0.36, batch loss = 0.30 (27.3 examples/sec; 0.293 sec/batch; 23h:28m:06s remains)
INFO - root - 2017-12-16 07:10:04.110768: step 44200, loss = 0.27, batch loss = 0.21 (28.9 examples/sec; 0.276 sec/batch; 22h:08m:11s remains)
2017-12-16 07:10:04.552475: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1024132 -3.0518026 -2.8536611 -2.9768469 -3.1717861 -3.1708798 -3.2009339 -3.1829557 -3.2445664 -3.3435619 -3.3885322 -3.3072679 -3.1246078 -3.1858296 -3.490423][-2.5960023 -2.5220134 -2.4931104 -2.5591602 -2.5296121 -2.6580575 -2.7187927 -2.5918865 -2.7109673 -2.8645773 -2.8572659 -2.8962226 -2.88103 -2.8828092 -3.0504832][-1.8635862 -1.7510886 -1.8232467 -1.8026135 -1.7874227 -1.9974434 -1.9868116 -1.8862982 -1.9417796 -2.3092835 -2.5801196 -2.6202812 -2.5391068 -2.6723514 -2.9466014][-0.59156394 -0.60247874 -0.74921513 -0.73732615 -0.91977191 -0.89187956 -0.82002687 -0.99996996 -1.3317804 -1.8251793 -2.1121716 -2.5432158 -2.8262539 -2.8428574 -2.8880548][-0.12115717 0.15634155 0.29884338 0.21560621 0.21146679 0.31742859 0.49826431 0.40841103 -0.012640476 -0.99597692 -1.7626872 -2.2372346 -2.5000081 -2.8058915 -3.0553112][-0.65349174 -0.13511896 0.28034496 0.71040821 1.163075 1.2529864 1.3281984 1.2588377 0.7787075 -0.33715725 -1.4710398 -2.2611725 -2.6981814 -2.824554 -2.8671155][-1.8127601 -1.0401924 -0.20858717 0.47243261 1.2534299 1.875073 2.4074821 2.0955968 1.389544 0.16835928 -1.1880014 -2.1570532 -2.8087525 -3.0443156 -3.2563429][-2.893198 -2.1191986 -1.1545718 -0.27879524 0.8298974 1.7093172 2.5420156 2.4453292 1.5949464 0.36680651 -1.1637738 -2.4312708 -3.2447338 -3.540061 -3.7127385][-4.482461 -3.8894739 -2.9338617 -1.8569541 -0.5861814 0.6352973 1.6271935 1.5519218 0.86676168 -0.23065567 -1.6672025 -2.9326739 -3.7883067 -4.1781921 -4.2328811][-6.1339865 -5.9414229 -5.2770376 -4.0912886 -2.5624132 -1.1237233 -0.11404276 -0.012399197 -0.34577036 -1.0213108 -2.1804714 -3.4205606 -4.32334 -4.5937185 -4.4425216][-6.5780048 -6.6117325 -6.0590329 -5.2725415 -3.9280343 -2.5454602 -1.6454797 -1.4036565 -1.6439989 -2.1879458 -3.1347656 -4.0426879 -4.5538158 -4.6020126 -4.41799][-6.4181647 -6.3109827 -5.9795651 -5.3531542 -4.4731259 -3.567874 -2.77104 -2.525631 -2.5556273 -2.8189802 -3.4723871 -4.2328444 -4.5102706 -4.3741837 -4.14193][-5.3683834 -5.1998343 -4.92538 -4.57857 -4.0477695 -3.497335 -2.9777896 -3.000026 -3.0543489 -3.0044909 -3.2504644 -3.772994 -4.1026115 -3.9938846 -3.66578][-4.1245337 -3.8615475 -3.5873103 -3.3016396 -3.0502844 -2.706811 -2.4341578 -2.5863662 -2.6425354 -2.6390419 -2.6748149 -2.9047794 -3.2163439 -3.2217712 -2.9479914][-2.7271481 -2.1750565 -1.8368702 -1.6638284 -1.619118 -1.6908123 -1.7086194 -1.8101516 -1.8502502 -1.8481433 -1.8892515 -2.0311599 -2.3082731 -2.5042515 -2.4250145]]...]
INFO - root - 2017-12-16 07:10:07.416436: step 44210, loss = 0.34, batch loss = 0.28 (27.8 examples/sec; 0.288 sec/batch; 23h:04m:15s remains)
INFO - root - 2017-12-16 07:10:10.266732: step 44220, loss = 0.19, batch loss = 0.13 (28.1 examples/sec; 0.285 sec/batch; 22h:47m:40s remains)
INFO - root - 2017-12-16 07:10:13.070421: step 44230, loss = 0.27, batch loss = 0.21 (27.8 examples/sec; 0.287 sec/batch; 23h:01m:12s remains)
INFO - root - 2017-12-16 07:10:15.859909: step 44240, loss = 0.29, batch loss = 0.23 (28.1 examples/sec; 0.284 sec/batch; 22h:45m:24s remains)
INFO - root - 2017-12-16 07:10:18.680061: step 44250, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 22h:46m:59s remains)
INFO - root - 2017-12-16 07:10:21.534025: step 44260, loss = 0.44, batch loss = 0.39 (28.5 examples/sec; 0.280 sec/batch; 22h:26m:13s remains)
INFO - root - 2017-12-16 07:10:24.403029: step 44270, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 22h:44m:40s remains)
INFO - root - 2017-12-16 07:10:27.232243: step 44280, loss = 0.23, batch loss = 0.17 (28.1 examples/sec; 0.285 sec/batch; 22h:48m:16s remains)
INFO - root - 2017-12-16 07:10:30.115296: step 44290, loss = 0.30, batch loss = 0.25 (29.4 examples/sec; 0.272 sec/batch; 21h:48m:37s remains)
INFO - root - 2017-12-16 07:10:32.991909: step 44300, loss = 0.31, batch loss = 0.25 (26.4 examples/sec; 0.303 sec/batch; 24h:15m:28s remains)
2017-12-16 07:10:33.457121: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8544743 -2.9387875 -3.0788019 -3.3798747 -3.5866683 -3.8688068 -4.19608 -4.6392365 -4.9708591 -4.741735 -4.4241638 -4.0130177 -3.2743268 -2.5375686 -1.9967711][-2.6324773 -2.8129961 -2.9742284 -3.3269563 -3.4687693 -3.5566435 -3.7252975 -3.9048989 -4.1391387 -3.8412836 -3.4978285 -2.9569945 -2.229146 -1.5368907 -1.049531][-2.7639635 -3.0852058 -3.2727628 -3.500303 -3.444212 -3.2808216 -3.2402663 -3.3405626 -3.3944252 -3.1974139 -3.0495243 -2.5467923 -1.7939954 -1.0546057 -0.68292594][-2.8674984 -3.2299156 -3.4074192 -3.4111092 -3.0317428 -2.4628954 -2.1347649 -2.1499169 -2.2658529 -2.32141 -2.3120315 -2.1040568 -1.6861322 -1.1942267 -0.98099828][-3.296839 -3.4083669 -3.2440186 -2.779254 -1.9963837 -1.026438 -0.37168026 -0.37462616 -0.79141665 -1.34726 -1.7667303 -1.8193903 -1.6408591 -1.3084934 -1.2449973][-3.491107 -3.4508402 -2.9919877 -1.9626272 -0.6362195 0.72882605 1.6226015 1.6409521 1.020071 0.046067715 -0.86757874 -1.2963738 -1.4903371 -1.3830168 -1.2896347][-3.4534342 -3.3193758 -2.5506954 -1.1300192 0.48035431 2.1557889 3.4459629 3.5054612 2.5234962 0.99269533 -0.33457184 -1.0542266 -1.4033439 -1.4013963 -1.4471326][-3.4801729 -3.2407355 -2.3013947 -0.5623014 1.3002095 2.9866838 4.2137594 4.428853 3.4197078 1.5235739 -0.14832783 -1.1889744 -1.6527574 -1.5547259 -1.4830773][-3.3524525 -3.2356668 -2.3791523 -0.64097762 1.230514 2.8670163 3.9641466 4.006361 2.9194393 1.0530224 -0.59905457 -1.6643684 -2.1090281 -1.9660656 -1.7397077][-3.2325695 -3.2506671 -2.6094949 -1.2083442 0.34726095 1.8302898 2.7128682 2.5859523 1.5388546 -0.19069147 -1.7399075 -2.9628434 -3.3149786 -2.9142027 -2.6240034][-3.3547621 -3.3757095 -2.8820481 -1.7430716 -0.55737376 0.45822239 0.92464304 0.63276625 -0.29434681 -1.8242469 -3.2462368 -4.3831234 -4.7399158 -4.21088 -3.7532592][-3.8462882 -3.8443878 -3.5631387 -2.6610498 -1.6354783 -0.88036537 -0.84502697 -1.4425509 -2.2156482 -3.4415307 -4.6060147 -5.6633015 -5.9984035 -5.3337603 -4.518302][-4.2946358 -4.0592852 -3.8618023 -3.4677114 -2.981741 -2.3527825 -2.25578 -2.8695536 -3.6025317 -4.4654675 -5.257751 -6.200861 -6.4186625 -5.8381038 -4.868814][-4.6838665 -4.1631765 -3.7846849 -3.5338898 -3.2700324 -2.813736 -2.7281463 -3.3530223 -4.1445808 -4.7741981 -5.3977036 -6.1205525 -6.2305927 -5.4514766 -4.491035][-4.8263879 -4.1813345 -3.7324052 -3.6064336 -3.4715972 -3.1386445 -2.9860404 -3.2266207 -3.752104 -4.4724612 -5.1489434 -5.8377461 -6.0939288 -5.3379741 -4.165278]]...]
INFO - root - 2017-12-16 07:10:36.249621: step 44310, loss = 0.38, batch loss = 0.32 (28.7 examples/sec; 0.279 sec/batch; 22h:20m:11s remains)
INFO - root - 2017-12-16 07:10:39.075289: step 44320, loss = 0.22, batch loss = 0.16 (27.8 examples/sec; 0.287 sec/batch; 23h:00m:05s remains)
INFO - root - 2017-12-16 07:10:41.868307: step 44330, loss = 0.30, batch loss = 0.25 (28.0 examples/sec; 0.285 sec/batch; 22h:49m:52s remains)
INFO - root - 2017-12-16 07:10:44.673276: step 44340, loss = 0.36, batch loss = 0.30 (28.7 examples/sec; 0.279 sec/batch; 22h:19m:30s remains)
INFO - root - 2017-12-16 07:10:47.545862: step 44350, loss = 0.48, batch loss = 0.42 (27.9 examples/sec; 0.286 sec/batch; 22h:55m:07s remains)
INFO - root - 2017-12-16 07:10:50.349335: step 44360, loss = 0.30, batch loss = 0.24 (29.6 examples/sec; 0.270 sec/batch; 21h:36m:28s remains)
INFO - root - 2017-12-16 07:10:53.214536: step 44370, loss = 0.40, batch loss = 0.34 (26.0 examples/sec; 0.308 sec/batch; 24h:38m:51s remains)
INFO - root - 2017-12-16 07:10:56.080401: step 44380, loss = 0.30, batch loss = 0.25 (28.6 examples/sec; 0.279 sec/batch; 22h:20m:53s remains)
INFO - root - 2017-12-16 07:10:58.914839: step 44390, loss = 0.22, batch loss = 0.16 (28.2 examples/sec; 0.284 sec/batch; 22h:42m:02s remains)
INFO - root - 2017-12-16 07:11:01.761762: step 44400, loss = 0.33, batch loss = 0.27 (25.5 examples/sec; 0.314 sec/batch; 25h:06m:32s remains)
2017-12-16 07:11:02.225850: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0592232 -2.781944 -2.3460147 -2.0921774 -2.2642076 -2.2445612 -2.2534461 -2.0647302 -1.9584925 -1.5861688 -1.0393562 -0.74410176 -0.70506239 -1.2178378 -1.8634155][-1.9308374 -1.519954 -0.93179893 -0.48672962 -0.5708127 -0.61444449 -0.88682103 -1.0889683 -1.1783063 -1.0639455 -0.618104 -0.51082253 -0.48735356 -0.88884783 -1.3146465][-1.5458851 -0.834069 -0.062178135 0.43570089 0.47779036 0.36402035 -0.22369337 -0.67849731 -1.0268385 -1.112298 -1.0459816 -1.0546267 -0.90109062 -1.1749802 -1.4405298][-1.1806564 -0.70499086 -0.041776657 0.54490423 0.75692415 0.93647242 0.526701 -0.22640944 -0.93856859 -1.4281373 -1.6952419 -2.0520959 -2.1996834 -2.2525277 -2.1457467][-1.6658807 -1.131551 -0.33059263 0.30465412 0.86100388 1.165545 0.71375656 0.01634407 -1.0139396 -1.8405433 -2.6161361 -3.1641455 -3.341043 -3.59613 -3.4655912][-2.2447493 -1.6929998 -0.8418 0.14739513 1.0974488 1.6551008 1.5842834 0.82846308 -0.25392532 -1.5263031 -2.821609 -3.7884326 -4.3029785 -4.3406868 -4.1138453][-2.8978624 -2.2144394 -1.3553143 -0.11619234 1.0506964 1.8300838 2.0436687 1.5861678 0.81827164 -0.48962474 -2.0764875 -3.6643944 -4.7850704 -5.0585256 -4.8284311][-3.2313271 -2.5500202 -1.7204266 -0.46968007 0.78542423 1.6604238 1.9892898 1.5821791 0.79829168 -0.50277925 -2.1046686 -3.7709036 -5.0488977 -5.6074276 -5.481998][-3.5328031 -2.9085658 -2.1075563 -1.0715683 -0.027776718 0.91445637 1.455637 1.2556992 0.42192459 -0.93654037 -2.5518541 -4.3758483 -5.8445578 -6.4952264 -6.3470535][-3.6789455 -3.1172714 -2.5254831 -1.8492239 -1.0999804 -0.42208004 -0.092544079 -0.221385 -0.85084057 -2.0111103 -3.5268416 -5.1628852 -6.4997978 -7.0313678 -6.6903605][-3.5313125 -2.9879594 -2.5257833 -2.281508 -2.0775537 -1.8381433 -1.6688221 -2.00872 -2.6836858 -3.5762653 -4.81844 -6.212409 -7.2662282 -7.6629839 -7.3450294][-3.5891957 -3.0912573 -2.8623557 -2.8034716 -2.8324947 -2.9610214 -3.1481051 -3.5120602 -4.0429196 -4.7137003 -5.6323524 -6.8583198 -7.864244 -8.2648211 -7.895956][-3.1488807 -2.8857346 -2.8053579 -3.10823 -3.5624259 -3.7765667 -4.0557 -4.4392061 -4.8413877 -5.3406067 -6.0509219 -6.915441 -7.6757779 -7.9833508 -7.5490379][-2.8455498 -2.639339 -2.6486201 -3.0100956 -3.4436905 -3.7930241 -4.0514436 -4.2552385 -4.6205339 -5.0154619 -5.7163091 -6.4807539 -6.9339285 -6.93798 -6.412477][-2.6719875 -2.4575834 -2.4712443 -2.7405844 -2.9666235 -3.1787114 -3.4229214 -3.6480594 -3.783468 -4.0980911 -4.6940951 -5.3442655 -5.7087235 -5.5894804 -4.9337645]]...]
INFO - root - 2017-12-16 07:11:05.016644: step 44410, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.284 sec/batch; 22h:42m:15s remains)
INFO - root - 2017-12-16 07:11:07.878571: step 44420, loss = 0.28, batch loss = 0.22 (29.2 examples/sec; 0.274 sec/batch; 21h:53m:25s remains)
INFO - root - 2017-12-16 07:11:10.782115: step 44430, loss = 0.26, batch loss = 0.20 (25.4 examples/sec; 0.315 sec/batch; 25h:12m:22s remains)
INFO - root - 2017-12-16 07:11:13.594534: step 44440, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.280 sec/batch; 22h:22m:46s remains)
INFO - root - 2017-12-16 07:11:16.435245: step 44450, loss = 0.27, batch loss = 0.21 (27.6 examples/sec; 0.290 sec/batch; 23h:13m:41s remains)
INFO - root - 2017-12-16 07:11:19.273962: step 44460, loss = 0.27, batch loss = 0.21 (26.4 examples/sec; 0.303 sec/batch; 24h:15m:56s remains)
INFO - root - 2017-12-16 07:11:22.089646: step 44470, loss = 0.30, batch loss = 0.24 (28.4 examples/sec; 0.281 sec/batch; 22h:30m:27s remains)
INFO - root - 2017-12-16 07:11:24.926544: step 44480, loss = 0.31, batch loss = 0.25 (28.0 examples/sec; 0.286 sec/batch; 22h:53m:33s remains)
INFO - root - 2017-12-16 07:11:27.781028: step 44490, loss = 0.24, batch loss = 0.18 (29.3 examples/sec; 0.273 sec/batch; 21h:52m:42s remains)
INFO - root - 2017-12-16 07:11:30.604019: step 44500, loss = 0.21, batch loss = 0.15 (28.7 examples/sec; 0.279 sec/batch; 22h:19m:35s remains)
2017-12-16 07:11:31.066017: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4737749 -5.5675182 -5.8532643 -6.2572756 -6.5800734 -6.6245508 -6.6181808 -6.9380207 -7.3936853 -7.7802305 -7.962399 -7.9234524 -7.6302814 -7.0803542 -6.2980366][-5.2564425 -5.4815607 -6.0884929 -6.5303373 -6.7429304 -7.0940337 -7.3446035 -7.3372068 -7.4379 -8.0288525 -8.6240015 -8.7722349 -8.5373764 -7.9857168 -7.1706696][-4.147769 -4.6705933 -5.5675197 -6.2589574 -6.5907764 -6.2531905 -5.8210511 -5.9580173 -6.472086 -7.1494565 -7.9453168 -8.5546579 -8.8703423 -8.5446281 -7.8431344][-3.469286 -3.8306305 -4.5707211 -5.0440855 -4.9998364 -4.5413 -3.8286395 -3.4364438 -3.7390687 -4.9173832 -6.290781 -7.4137869 -8.3159723 -8.2015362 -7.6573849][-2.9125834 -3.1361561 -3.7009568 -3.9398737 -3.4340768 -2.0278234 -0.43520236 0.054363251 -0.20371914 -1.6127741 -3.3780284 -5.1113868 -6.5329137 -7.0857658 -7.0101461][-2.436265 -2.4511185 -2.6064911 -2.3223612 -1.2755919 0.47155666 2.4610252 3.5501719 3.4583488 1.8881478 -0.13610697 -2.4211876 -4.3574471 -5.4876609 -5.9467163][-2.274559 -1.9645607 -1.4556916 -0.68497896 0.91646385 3.00914 5.3175116 6.2955275 6.179533 4.40394 2.0442934 -0.38797426 -2.5368676 -3.965528 -4.7897992][-2.7729902 -2.1979084 -1.7794635 -0.41050863 1.6706948 3.8189392 6.308424 7.439002 7.4534531 5.5246563 3.0936823 0.37941122 -2.0305057 -3.599216 -4.4798203][-4.4448633 -3.8654652 -3.0982919 -1.5543187 0.17396498 2.3702359 4.6521416 5.6924438 5.8116951 4.1550789 1.905139 -0.51963377 -2.5520844 -4.1008415 -5.0292382][-4.5701847 -4.5175166 -4.4985685 -3.4344125 -2.1198421 -0.282032 1.2728353 2.2116113 2.4345274 1.4246435 -0.0070090294 -1.9306037 -3.6821449 -4.9444 -5.547472][-5.5852962 -5.4418159 -5.3990293 -5.1025143 -4.6608505 -3.5737724 -2.6019893 -1.8935013 -1.8052263 -2.3605468 -3.0666783 -4.0690732 -5.1762757 -6.0670719 -6.3833361][-5.8537378 -6.2036095 -6.8580108 -7.005023 -6.8250518 -6.3660755 -5.9242969 -5.6042581 -5.252295 -5.2438674 -5.4542179 -5.8705378 -6.3106127 -6.6340446 -6.6370516][-6.503984 -7.0546589 -7.829793 -8.2060108 -8.491991 -8.2934608 -7.8241091 -7.2757082 -6.7143178 -6.4123683 -6.217607 -6.1607985 -6.1828542 -6.4722214 -6.4670782][-5.6318874 -6.4199247 -7.3138895 -7.9876328 -8.3716908 -8.2933922 -8.1245136 -7.7110567 -7.1608734 -6.7949996 -6.5515203 -6.4943466 -6.45988 -6.35422 -6.0113039][-5.597384 -5.9812007 -6.5589213 -7.12015 -7.406795 -7.3935423 -7.2182341 -6.8755121 -6.53876 -6.4487581 -6.3328085 -6.0600243 -5.7174096 -5.4857907 -5.2018085]]...]
INFO - root - 2017-12-16 07:11:33.931735: step 44510, loss = 0.39, batch loss = 0.33 (25.6 examples/sec; 0.313 sec/batch; 25h:02m:31s remains)
INFO - root - 2017-12-16 07:11:36.753404: step 44520, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.281 sec/batch; 22h:29m:39s remains)
INFO - root - 2017-12-16 07:11:39.589463: step 44530, loss = 0.27, batch loss = 0.21 (27.7 examples/sec; 0.288 sec/batch; 23h:04m:20s remains)
INFO - root - 2017-12-16 07:11:42.392365: step 44540, loss = 0.34, batch loss = 0.28 (27.9 examples/sec; 0.287 sec/batch; 22h:55m:16s remains)
INFO - root - 2017-12-16 07:11:45.279500: step 44550, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.281 sec/batch; 22h:26m:14s remains)
INFO - root - 2017-12-16 07:11:48.107228: step 44560, loss = 0.31, batch loss = 0.25 (28.5 examples/sec; 0.280 sec/batch; 22h:25m:47s remains)
INFO - root - 2017-12-16 07:11:50.939871: step 44570, loss = 0.26, batch loss = 0.21 (29.2 examples/sec; 0.274 sec/batch; 21h:54m:39s remains)
INFO - root - 2017-12-16 07:11:53.799939: step 44580, loss = 0.32, batch loss = 0.26 (27.5 examples/sec; 0.291 sec/batch; 23h:16m:44s remains)
INFO - root - 2017-12-16 07:11:56.720502: step 44590, loss = 0.37, batch loss = 0.31 (26.1 examples/sec; 0.307 sec/batch; 24h:33m:00s remains)
INFO - root - 2017-12-16 07:11:59.518987: step 44600, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.282 sec/batch; 22h:32m:46s remains)
2017-12-16 07:11:59.967094: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.109046 -5.2977028 -5.3445821 -5.235548 -5.0545921 -5.0041475 -4.8734779 -4.6765685 -4.5738974 -4.5943937 -4.7261548 -4.9393663 -5.1586466 -5.0772896 -4.91061][-5.4757781 -5.3912544 -5.1847887 -5.2953353 -5.291472 -5.1626396 -5.0328283 -5.002491 -4.9571266 -5.1697016 -5.4030461 -5.5127749 -5.6666861 -5.4379077 -5.2760305][-5.2018852 -5.1681175 -5.023303 -4.6949754 -4.3816123 -4.5906181 -4.6380692 -4.7367439 -4.9146152 -5.1679125 -5.3624873 -5.6266785 -5.8902292 -5.5921397 -5.4018564][-4.6294494 -4.201623 -3.797437 -3.5602012 -3.3023326 -3.1212888 -3.0420153 -3.5398655 -3.9574556 -4.4078145 -4.7730837 -4.9476018 -5.0331335 -4.9631858 -5.1128578][-3.8732913 -3.0583897 -2.2135382 -1.6719587 -1.0236905 -0.69126487 -0.70802951 -1.2307293 -1.8204103 -2.6133032 -3.1802287 -3.4617481 -3.7469971 -3.7724876 -4.0580287][-3.2813272 -1.9013374 -0.65368271 0.3893199 1.5523543 1.8658137 1.7587218 1.1216202 0.38589096 -0.4571867 -1.1354065 -1.5209453 -1.89695 -2.2913632 -3.1198144][-3.1245434 -1.6771255 -0.098487854 1.4613819 2.9627967 3.7946014 4.0151482 3.0919962 2.2426562 1.4190087 0.648777 0.13632536 -0.4447608 -1.1193132 -2.1297898][-3.0341856 -1.6594205 -0.14759398 1.6111465 3.3631015 4.3028126 4.75659 4.1849232 3.5527945 2.6616883 1.8644609 1.5432992 0.91230345 -0.17333221 -1.4492302][-3.736691 -2.4973409 -1.0673835 0.64557362 2.4135551 3.5582528 4.1361752 3.8184748 3.4446983 2.7843404 2.1085272 1.7348504 1.139369 0.33432722 -0.99966431][-4.0100012 -3.3858871 -2.6212497 -1.0400417 0.70630455 1.6762314 2.2676349 2.2793245 2.3068814 1.7926297 1.324748 1.0710626 0.48651886 -0.33735132 -1.6191306][-5.1000409 -4.5542912 -3.9666104 -2.9943466 -1.8822613 -0.79180837 0.026682377 0.16871023 0.35718536 0.12667847 -0.016899109 -0.1709466 -0.561759 -1.374433 -2.6539831][-6.1533651 -5.5728841 -5.1292987 -4.5001054 -3.7063537 -2.9918523 -2.5499649 -2.1794274 -1.6399047 -1.5929739 -1.583333 -1.6376936 -1.9026985 -2.514729 -3.5157912][-6.8424873 -6.2262616 -5.7094231 -5.0265961 -4.5047064 -4.0054126 -3.6749489 -3.5026207 -3.2077105 -2.8483541 -2.5569139 -2.6449203 -2.9595876 -3.4078155 -4.1655579][-7.4927034 -6.7063475 -6.0153089 -5.2994232 -4.708118 -4.2746577 -4.1697383 -4.1089206 -3.9179916 -3.6549985 -3.4404993 -3.5851674 -3.996743 -4.400991 -4.9891243][-8.1987667 -7.2777309 -6.3087511 -5.4207826 -4.8256416 -4.3674321 -4.1623087 -4.2469115 -4.4031658 -4.39555 -4.3945293 -4.593266 -5.0531359 -5.4060621 -5.5673819]]...]
INFO - root - 2017-12-16 07:12:02.793894: step 44610, loss = 0.27, batch loss = 0.21 (28.4 examples/sec; 0.282 sec/batch; 22h:32m:32s remains)
INFO - root - 2017-12-16 07:12:05.646584: step 44620, loss = 0.41, batch loss = 0.35 (27.8 examples/sec; 0.287 sec/batch; 22h:58m:19s remains)
INFO - root - 2017-12-16 07:12:08.442156: step 44630, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.286 sec/batch; 22h:50m:30s remains)
INFO - root - 2017-12-16 07:12:11.276926: step 44640, loss = 0.25, batch loss = 0.19 (28.7 examples/sec; 0.279 sec/batch; 22h:19m:15s remains)
INFO - root - 2017-12-16 07:12:14.100557: step 44650, loss = 0.30, batch loss = 0.24 (28.1 examples/sec; 0.285 sec/batch; 22h:44m:58s remains)
INFO - root - 2017-12-16 07:12:16.906538: step 44660, loss = 0.25, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 22h:36m:01s remains)
INFO - root - 2017-12-16 07:12:19.751966: step 44670, loss = 0.33, batch loss = 0.27 (27.7 examples/sec; 0.289 sec/batch; 23h:07m:19s remains)
INFO - root - 2017-12-16 07:12:22.604822: step 44680, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.282 sec/batch; 22h:32m:04s remains)
INFO - root - 2017-12-16 07:12:25.457334: step 44690, loss = 0.25, batch loss = 0.20 (28.4 examples/sec; 0.282 sec/batch; 22h:30m:46s remains)
INFO - root - 2017-12-16 07:12:28.332818: step 44700, loss = 0.30, batch loss = 0.24 (28.1 examples/sec; 0.285 sec/batch; 22h:44m:44s remains)
2017-12-16 07:12:28.796993: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9392729 -3.9016314 -3.8901925 -4.1248479 -4.3102493 -4.1765532 -3.8945804 -3.7485304 -3.9279704 -3.8585787 -3.6675756 -3.5561554 -3.5575113 -3.7989254 -3.9078619][-3.6439803 -3.7282553 -3.6902337 -3.6894364 -3.7773926 -3.8050311 -3.6220076 -3.3676634 -3.35197 -3.386178 -3.3200212 -3.1187398 -2.9188461 -2.9971333 -3.0689278][-3.3928285 -3.399039 -3.3993585 -3.4079285 -3.4118793 -3.3182707 -3.0785823 -2.8850431 -2.7652516 -2.7872131 -2.7829053 -2.5455244 -2.355974 -2.1504695 -1.9531395][-2.9566584 -2.9487598 -2.9102354 -2.8228321 -2.7500877 -2.6136594 -2.4157426 -2.3387816 -2.3039205 -2.3043756 -2.2711663 -2.0743594 -1.8244967 -1.4313431 -1.1684411][-2.3281739 -2.1591048 -2.103857 -2.1347995 -2.0451443 -1.8663654 -1.6527426 -1.6273308 -1.8147542 -1.9489357 -1.9424708 -1.6928596 -1.2422464 -0.78764105 -0.59850359][-1.5176506 -1.4700549 -1.517422 -1.404552 -1.0845551 -0.83954883 -0.70983434 -0.8290081 -1.0859528 -1.440449 -1.5743821 -1.4270568 -1.0255759 -0.51840019 -0.1430068][-0.656235 -0.68287539 -0.86907506 -0.788867 -0.42600632 -0.1288166 0.05041647 -0.25906658 -0.63120723 -0.92664957 -1.2586901 -1.2284815 -0.99390364 -0.49447083 -0.093054295][-0.10847521 -0.16888571 -0.45068741 -0.41026211 -0.088116169 0.095593929 0.15869427 -0.17215729 -0.63199377 -0.99772406 -1.3010685 -1.1912632 -0.95701003 -0.57543731 -0.29485083][0.22865629 0.13477993 -0.17313433 -0.2804327 -0.26752281 -0.2827549 -0.29784918 -0.56969309 -0.92257285 -1.3275824 -1.5678513 -1.5783014 -1.5163803 -1.152739 -0.8959496][0.309052 0.19113731 -0.41424894 -0.69129539 -0.74234009 -0.94509244 -1.1516218 -1.2974386 -1.500885 -1.7257833 -1.8461916 -2.0059712 -2.1978984 -2.0139222 -1.7390325][0.12486887 -0.0301857 -0.59480715 -1.1043346 -1.5534966 -1.8385286 -1.9124577 -2.00441 -2.3133543 -2.52836 -2.709 -2.9320192 -3.101378 -2.9641316 -2.6463623][-0.51168394 -0.602715 -1.2284548 -1.7785497 -2.1336257 -2.5050659 -2.5834963 -2.6668692 -2.9634213 -3.1744552 -3.5185854 -3.8576646 -4.072825 -3.8450541 -3.3004336][-0.75776052 -0.568233 -1.019594 -1.6413007 -2.1367083 -2.5146923 -2.5648355 -2.536901 -2.8111856 -3.2680347 -3.8759198 -4.3241963 -4.5622578 -4.410253 -3.8524404][-0.94722581 -0.82639766 -1.096143 -1.4360316 -1.9011555 -2.161983 -2.3182659 -2.5121322 -2.8387265 -3.1553242 -3.5189028 -4.0895476 -4.5048027 -4.3347416 -3.6527078][-0.94292188 -0.586334 -0.70255685 -0.92596459 -1.2341359 -1.4043424 -1.5965798 -1.8508983 -2.1834176 -2.5810523 -3.0802629 -3.6433477 -3.9376149 -3.7512543 -3.1786709]]...]
INFO - root - 2017-12-16 07:12:31.663153: step 44710, loss = 0.26, batch loss = 0.20 (28.4 examples/sec; 0.282 sec/batch; 22h:33m:09s remains)
INFO - root - 2017-12-16 07:12:34.506697: step 44720, loss = 0.22, batch loss = 0.16 (27.1 examples/sec; 0.295 sec/batch; 23h:36m:58s remains)
INFO - root - 2017-12-16 07:12:37.346070: step 44730, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.282 sec/batch; 22h:34m:16s remains)
INFO - root - 2017-12-16 07:12:40.210137: step 44740, loss = 0.29, batch loss = 0.24 (28.3 examples/sec; 0.283 sec/batch; 22h:35m:47s remains)
INFO - root - 2017-12-16 07:12:43.040964: step 44750, loss = 0.22, batch loss = 0.16 (28.7 examples/sec; 0.279 sec/batch; 22h:17m:02s remains)
INFO - root - 2017-12-16 07:12:45.862251: step 44760, loss = 0.24, batch loss = 0.19 (28.5 examples/sec; 0.281 sec/batch; 22h:25m:38s remains)
INFO - root - 2017-12-16 07:12:48.689761: step 44770, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 22h:47m:21s remains)
INFO - root - 2017-12-16 07:12:51.535945: step 44780, loss = 0.21, batch loss = 0.15 (28.7 examples/sec; 0.279 sec/batch; 22h:15m:34s remains)
INFO - root - 2017-12-16 07:12:54.350520: step 44790, loss = 0.26, batch loss = 0.20 (28.5 examples/sec; 0.281 sec/batch; 22h:27m:42s remains)
INFO - root - 2017-12-16 07:12:57.175144: step 44800, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.283 sec/batch; 22h:37m:52s remains)
2017-12-16 07:12:57.654680: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3038712 -4.0775909 -3.9148769 -3.9093058 -3.9617624 -3.9325614 -3.920527 -3.8214729 -4.0317216 -4.2786021 -4.3762918 -4.6023903 -4.7734556 -4.9518876 -4.8672762][-4.469892 -4.1978106 -3.9749808 -3.9149842 -3.8767076 -3.7084932 -3.5170302 -3.3214972 -3.4702063 -3.84619 -4.2949767 -4.637084 -4.7629046 -5.1183066 -5.0215368][-4.4684849 -4.2295413 -4.0404639 -3.8794422 -3.62565 -3.407577 -3.1093135 -2.663151 -2.7445343 -3.2869871 -4.0863013 -4.6907668 -5.2158351 -5.6366568 -5.4472375][-4.29146 -4.170496 -4.1037135 -3.9256074 -3.4356041 -2.8633466 -2.3467112 -1.916995 -1.9150712 -2.5937579 -3.7408881 -4.8245964 -5.4923515 -5.8950224 -5.847271][-4.214633 -4.0885658 -3.9922807 -3.6845334 -2.8922789 -1.8866127 -1.0111413 -0.40531254 -0.43763375 -1.3491704 -2.73433 -4.4206071 -5.5946341 -6.1787596 -6.1323004][-3.8169532 -3.6670434 -3.4964004 -3.0379171 -1.6949236 -0.17077065 1.2395768 2.0432415 1.8848515 0.63038874 -1.2803674 -3.4119825 -5.057631 -6.148159 -6.3189025][-3.3307538 -3.014854 -2.6465213 -1.8034265 -0.038157463 1.7175684 3.2871037 4.1260233 3.9045134 2.4104905 0.091389656 -2.339498 -4.2271333 -5.4642458 -5.786767][-3.2668679 -2.854394 -2.3366406 -1.191258 0.72871685 2.6825686 4.3472166 5.0640936 4.7370996 3.0539336 0.63263416 -1.883652 -3.9239974 -5.1561313 -5.4953651][-3.6826959 -3.2973528 -2.5250828 -1.3276584 0.4430418 2.3099685 3.9179125 4.6402445 4.36248 2.7482166 0.46430445 -2.0120959 -3.9606469 -5.0970535 -5.5045261][-3.9043825 -3.6773946 -3.17913 -2.0500934 -0.37368059 1.1915245 2.3928866 2.8904843 2.6666217 1.3835235 -0.448462 -2.5242057 -4.2170672 -5.1172175 -5.3676424][-4.5680723 -4.4226174 -3.9683526 -3.0837631 -1.8222983 -0.47387481 0.58128262 0.88166428 0.46269464 -0.47967243 -1.7699754 -3.3249395 -4.6076827 -5.3005729 -5.5627012][-4.951694 -4.9660163 -4.7224126 -4.0336022 -3.161993 -2.1968257 -1.4022634 -1.0840585 -1.2496228 -1.9554696 -2.9340825 -4.1022673 -5.0855517 -5.653101 -5.8701553][-4.8596687 -5.1141319 -5.1263342 -4.6423678 -3.9315312 -3.080004 -2.3932641 -2.0659871 -2.2400165 -2.6851029 -3.4233208 -4.3760195 -5.1297088 -5.7135429 -6.004684][-4.7346311 -4.8449564 -4.7311139 -4.3956108 -3.939522 -3.1385353 -2.3982108 -2.0119753 -2.0667145 -2.3260818 -2.677906 -3.4973023 -4.3784428 -4.9820051 -5.201407][-4.2737093 -4.2072344 -4.0133824 -3.6417913 -3.2752452 -2.656327 -2.02355 -1.5176692 -1.300432 -1.3819623 -1.7083681 -2.3251908 -3.0100095 -3.6736081 -4.1976867]]...]
INFO - root - 2017-12-16 07:13:00.457559: step 44810, loss = 0.31, batch loss = 0.25 (28.3 examples/sec; 0.283 sec/batch; 22h:37m:18s remains)
INFO - root - 2017-12-16 07:13:03.334032: step 44820, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.282 sec/batch; 22h:29m:51s remains)
INFO - root - 2017-12-16 07:13:06.139801: step 44830, loss = 0.34, batch loss = 0.28 (29.2 examples/sec; 0.274 sec/batch; 21h:52m:39s remains)
INFO - root - 2017-12-16 07:13:09.015805: step 44840, loss = 0.24, batch loss = 0.18 (27.1 examples/sec; 0.295 sec/batch; 23h:33m:21s remains)
INFO - root - 2017-12-16 07:13:11.859397: step 44850, loss = 0.34, batch loss = 0.28 (28.6 examples/sec; 0.280 sec/batch; 22h:23m:10s remains)
INFO - root - 2017-12-16 07:13:14.737297: step 44860, loss = 0.24, batch loss = 0.18 (28.6 examples/sec; 0.280 sec/batch; 22h:20m:18s remains)
INFO - root - 2017-12-16 07:13:17.550747: step 44870, loss = 0.23, batch loss = 0.17 (29.3 examples/sec; 0.273 sec/batch; 21h:48m:02s remains)
INFO - root - 2017-12-16 07:13:20.381529: step 44880, loss = 0.33, batch loss = 0.27 (28.4 examples/sec; 0.282 sec/batch; 22h:30m:24s remains)
INFO - root - 2017-12-16 07:13:23.209708: step 44890, loss = 0.28, batch loss = 0.23 (25.6 examples/sec; 0.313 sec/batch; 25h:00m:19s remains)
INFO - root - 2017-12-16 07:13:26.068336: step 44900, loss = 0.30, batch loss = 0.25 (29.0 examples/sec; 0.276 sec/batch; 22h:00m:47s remains)
2017-12-16 07:13:26.530893: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4574106 -3.9779549 -4.5249753 -5.0007558 -5.4971056 -5.5382152 -5.47574 -5.6985154 -5.939949 -5.7493172 -5.3785849 -5.1449575 -4.7537427 -4.3756242 -3.9853187][-4.0163326 -4.7148266 -5.3090768 -5.5172739 -5.623723 -5.9482207 -6.0936813 -5.7960119 -5.792944 -6.153307 -6.4976597 -6.2618618 -5.7890019 -5.3991156 -4.9773][-4.9870009 -5.5760584 -6.0603409 -5.9803886 -5.579298 -5.0992618 -4.7820139 -5.1433816 -5.6870022 -5.9797783 -6.2327809 -6.6419406 -6.7066865 -6.2922139 -5.8224311][-5.614326 -6.2602 -6.3725057 -5.7050352 -4.7268591 -3.7278969 -3.1071925 -2.8905756 -3.3409815 -4.741138 -5.8273764 -6.3292446 -6.6601176 -6.6885118 -6.5149455][-5.579916 -5.9052315 -5.5577641 -4.3036261 -2.5203354 -0.8318181 0.19026947 0.22159863 -0.57086635 -2.092912 -3.6768355 -5.123127 -5.9452944 -6.3694534 -6.5478511][-5.1426759 -5.2013793 -4.3838248 -2.5957098 -0.10159779 2.2046709 3.6359663 3.857378 2.8466072 0.84610844 -1.3011448 -3.0064108 -4.3240738 -5.4031706 -6.0543146][-4.1910849 -4.0386467 -2.8814402 -1.0411444 1.384325 3.7731781 5.401227 5.547822 4.6026897 2.9004292 0.82565165 -1.1370485 -2.7776198 -4.1099319 -5.202548][-3.3455005 -2.8742323 -1.8811784 -0.046441555 1.9672027 3.9045076 5.3203125 5.468524 4.6697054 3.1018219 1.394146 -0.28047371 -1.7734261 -3.2830009 -4.6714287][-2.466495 -1.9542155 -0.98171544 0.27313948 1.6270986 3.0095649 4.0826254 4.273715 3.6963243 2.5419793 1.3384366 -0.08190918 -1.567142 -2.9801061 -4.3061476][-2.3654606 -1.7568178 -0.93907285 0.06798172 0.8615036 1.533752 2.1386728 2.3458371 2.1741576 1.4295049 0.64428711 -0.46216321 -1.6777308 -2.9253623 -4.2502408][-2.7127686 -2.1016107 -1.6359754 -1.0151742 -0.43736982 -0.029228687 0.18599176 0.36510229 0.40622091 0.32466078 0.021919727 -0.83803248 -1.9789581 -3.2137337 -4.3888879][-3.0657239 -2.1333122 -1.4555998 -1.3828104 -1.1977391 -1.1091068 -1.0971851 -0.90769506 -0.61828685 -0.41402245 -0.47728491 -1.2049518 -2.3484361 -3.63596 -4.7942348][-3.1470771 -2.0551159 -1.2243176 -1.1077917 -1.34432 -1.5091777 -1.5232816 -1.3548291 -1.0419333 -0.767575 -0.82808185 -1.529382 -2.6849568 -4.0104651 -5.1347156][-3.2055702 -2.0304654 -1.0718172 -0.76247311 -0.89781809 -1.320893 -1.6215475 -1.5114851 -1.2359123 -0.95464182 -0.80739069 -1.4577608 -2.6416345 -3.9745 -5.1448035][-3.0808361 -1.7206023 -0.62753534 -0.39290905 -0.62137818 -0.95559049 -1.2266567 -1.2407858 -1.1283715 -0.94036841 -0.74411869 -1.0357659 -2.0344427 -3.2884607 -4.4669662]]...]
INFO - root - 2017-12-16 07:13:29.342034: step 44910, loss = 0.34, batch loss = 0.28 (29.1 examples/sec; 0.275 sec/batch; 21h:58m:35s remains)
INFO - root - 2017-12-16 07:13:32.121065: step 44920, loss = 0.31, batch loss = 0.25 (28.5 examples/sec; 0.281 sec/batch; 22h:26m:51s remains)
INFO - root - 2017-12-16 07:13:34.948476: step 44930, loss = 0.21, batch loss = 0.15 (28.2 examples/sec; 0.283 sec/batch; 22h:37m:52s remains)
INFO - root - 2017-12-16 07:13:37.834545: step 44940, loss = 0.23, batch loss = 0.17 (27.9 examples/sec; 0.287 sec/batch; 22h:54m:52s remains)
INFO - root - 2017-12-16 07:13:40.663712: step 44950, loss = 0.32, batch loss = 0.26 (27.9 examples/sec; 0.287 sec/batch; 22h:54m:02s remains)
INFO - root - 2017-12-16 07:13:43.505704: step 44960, loss = 0.24, batch loss = 0.18 (29.1 examples/sec; 0.275 sec/batch; 21h:57m:48s remains)
INFO - root - 2017-12-16 07:13:46.372670: step 44970, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.282 sec/batch; 22h:29m:39s remains)
INFO - root - 2017-12-16 07:13:49.163084: step 44980, loss = 0.23, batch loss = 0.17 (27.9 examples/sec; 0.287 sec/batch; 22h:54m:33s remains)
INFO - root - 2017-12-16 07:13:52.011672: step 44990, loss = 0.35, batch loss = 0.29 (28.1 examples/sec; 0.285 sec/batch; 22h:45m:15s remains)
INFO - root - 2017-12-16 07:13:54.859665: step 45000, loss = 0.28, batch loss = 0.22 (28.3 examples/sec; 0.282 sec/batch; 22h:32m:45s remains)
2017-12-16 07:13:55.347953: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0248246 -5.562789 -6.0051346 -6.3697205 -6.686657 -6.8287745 -6.8922424 -7.1102629 -7.2939358 -7.4196558 -7.3172827 -7.1653566 -6.9967666 -6.4541912 -5.7611737][-5.8745608 -6.3758659 -6.6728344 -6.7921286 -6.8954883 -7.1964693 -7.5792542 -7.9387426 -8.1936216 -8.5381374 -8.8082123 -8.7999573 -8.5533857 -8.0152607 -7.382195][-6.0299468 -6.4040594 -6.43015 -6.0742087 -5.8257 -5.8647528 -5.9963365 -6.7901468 -7.7031255 -8.4400578 -8.9596634 -9.4205475 -9.737339 -9.3737679 -8.7551317][-5.9603343 -6.0028172 -5.5241594 -4.7126684 -4.0116291 -3.5678511 -3.3804507 -3.9324923 -4.7518845 -6.3440194 -7.8190703 -8.5961208 -9.255374 -9.6363659 -9.5968361][-5.299943 -5.0749655 -4.1277533 -2.670691 -1.1958172 -0.16957617 0.26856279 -0.13754797 -1.0948331 -2.7865453 -4.6042037 -6.4947243 -8.1966667 -9.0673485 -9.5622358][-4.2070518 -3.5584886 -2.40685 -0.43817592 1.7802434 3.1662889 4.0436764 4.1161194 3.2091064 1.1497297 -1.1520233 -3.3532472 -5.6990094 -7.7576256 -9.0518436][-4.1051846 -3.1206775 -1.4507856 0.753119 3.4066377 5.3613291 6.605526 6.5365353 5.756937 4.2336931 2.0422096 -0.68407869 -3.572341 -5.90563 -7.5483942][-4.5897932 -3.7001929 -2.0897264 0.28446865 3.1192255 5.31524 6.9824629 7.4097052 6.7583838 4.9967613 2.8565383 0.55693245 -2.1240733 -4.5406704 -6.2245674][-5.3637924 -4.7659373 -3.6067398 -1.7090797 0.71906424 2.8733358 4.8092661 5.6331186 5.58039 4.5171566 2.6768732 0.28985023 -2.2076809 -4.1803775 -5.4824467][-6.0490379 -5.9693022 -5.3229985 -4.1617131 -2.5346448 -0.75740457 1.0161877 2.1782312 2.6863008 2.0563064 0.751317 -0.98531413 -2.9633284 -4.7523837 -5.9439726][-6.7684631 -7.1560793 -6.7945595 -6.0282803 -5.1778316 -3.9667587 -2.4767284 -1.4440489 -0.91929221 -1.121489 -1.8307078 -3.0569959 -4.3588023 -5.3072376 -6.0501642][-7.1321449 -7.4162493 -7.3765383 -7.0643926 -6.2992911 -5.5659142 -4.9241357 -4.1457205 -3.520896 -3.4904077 -3.9132757 -4.8404126 -5.63319 -6.025116 -6.3424616][-5.9698911 -6.339489 -6.5976553 -6.4877496 -6.2461114 -5.742084 -5.0603151 -4.6890483 -4.4307332 -4.4241228 -4.6055675 -5.15427 -5.7292867 -6.2498856 -6.5272112][-4.6763525 -4.9852228 -5.2733903 -5.3373523 -5.3470268 -5.0907454 -4.7384028 -4.5858769 -4.4706674 -4.4415283 -4.3746252 -4.6924462 -5.1423464 -5.4080782 -5.4825792][-3.7772515 -3.6281419 -3.5817933 -3.7368765 -3.9531984 -3.9348335 -3.9225795 -3.8660409 -3.77019 -3.8720868 -3.9124634 -3.7691681 -3.7284203 -3.9642713 -4.1793818]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-45000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-45000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 07:13:58.581203: step 45010, loss = 0.26, batch loss = 0.20 (25.6 examples/sec; 0.313 sec/batch; 24h:59m:43s remains)
INFO - root - 2017-12-16 07:14:01.367325: step 45020, loss = 0.23, batch loss = 0.17 (28.8 examples/sec; 0.278 sec/batch; 22h:10m:09s remains)
INFO - root - 2017-12-16 07:14:04.210836: step 45030, loss = 0.25, batch loss = 0.19 (27.5 examples/sec; 0.291 sec/batch; 23h:12m:51s remains)
INFO - root - 2017-12-16 07:14:07.018735: step 45040, loss = 0.27, batch loss = 0.21 (28.3 examples/sec; 0.283 sec/batch; 22h:34m:46s remains)
INFO - root - 2017-12-16 07:14:09.827912: step 45050, loss = 0.29, batch loss = 0.23 (29.1 examples/sec; 0.275 sec/batch; 21h:55m:56s remains)
INFO - root - 2017-12-16 07:14:12.673248: step 45060, loss = 0.23, batch loss = 0.18 (29.6 examples/sec; 0.270 sec/batch; 21h:32m:44s remains)
INFO - root - 2017-12-16 07:14:15.491797: step 45070, loss = 0.34, batch loss = 0.29 (29.7 examples/sec; 0.269 sec/batch; 21h:30m:19s remains)
INFO - root - 2017-12-16 07:14:18.313412: step 45080, loss = 0.27, batch loss = 0.21 (27.8 examples/sec; 0.287 sec/batch; 22h:57m:06s remains)
INFO - root - 2017-12-16 07:14:21.181437: step 45090, loss = 0.21, batch loss = 0.15 (27.5 examples/sec; 0.291 sec/batch; 23h:15m:30s remains)
INFO - root - 2017-12-16 07:14:24.035081: step 45100, loss = 0.28, batch loss = 0.22 (29.3 examples/sec; 0.273 sec/batch; 21h:46m:27s remains)
2017-12-16 07:14:24.501122: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8460169 -5.351778 -4.9318132 -4.7883387 -4.8939476 -5.0972748 -5.0684495 -5.1843715 -5.3115315 -5.3783326 -5.3346786 -5.2157884 -4.9659867 -4.4769659 -3.9413817][-6.0529227 -5.6289515 -5.2686176 -4.9833803 -4.8116179 -5.2176366 -5.2749519 -5.3088946 -5.3587184 -5.6027484 -5.8185983 -5.6909542 -5.3868504 -4.6645222 -3.9419119][-6.3609877 -5.7586365 -5.1467738 -4.7389693 -4.4663997 -4.5096226 -4.3470964 -4.657958 -5.0799322 -5.4952011 -5.7554631 -5.9467144 -6.0547256 -5.4247856 -4.5722427][-6.061307 -5.36194 -4.5550447 -3.8201406 -3.3719602 -3.1286726 -2.9428945 -3.2094266 -3.6356554 -4.4361243 -5.1545353 -5.9229212 -6.2112389 -5.7657104 -4.9597154][-5.2809806 -4.3716083 -3.3562555 -2.434165 -1.6870353 -1.1532197 -0.72469068 -0.91488838 -1.5555189 -2.547905 -3.5849531 -4.7045617 -5.5069251 -5.6122503 -5.0445437][-4.6098828 -3.4994559 -2.1392338 -0.70940638 0.5495944 1.4036808 1.9789181 1.8194137 1.2000437 0.061497688 -1.4120114 -3.1049106 -4.5413222 -5.0282078 -4.6059079][-3.9004269 -2.7584059 -1.411298 0.27216578 1.7812285 3.0925074 4.2330942 4.3142338 3.6969252 2.3086357 0.66056252 -1.2873783 -3.0575943 -4.0371523 -4.2588854][-3.3026519 -2.1901929 -0.92427707 0.59522915 1.9481931 3.3247724 4.5177736 4.8574409 4.6674318 3.4892149 1.7050719 -0.35983992 -2.2692757 -3.5106697 -3.8573875][-3.1325927 -2.3361869 -1.4827332 -0.39280367 0.82160664 1.9698529 3.2758255 3.907589 3.9680452 3.0578313 1.393692 -0.47998571 -2.2587526 -3.5251522 -4.0315156][-3.55833 -3.0873179 -2.6326542 -1.8840454 -1.0092349 -0.065318108 1.03199 1.7416954 1.9244385 1.1441193 -0.24345255 -1.9036803 -3.6355495 -4.6589065 -4.938756][-4.1074586 -3.8624079 -3.6091278 -3.4133122 -3.0614872 -2.4735465 -1.4391732 -0.797354 -0.61340904 -1.2085485 -2.445271 -3.8757248 -5.1782441 -5.8915315 -6.0298328][-4.9705606 -4.8860683 -4.8399529 -4.7791018 -4.6748996 -4.4702263 -3.8709459 -3.3462229 -2.9741361 -3.2363188 -4.1687641 -5.3074708 -6.2488146 -6.5415068 -6.2038178][-5.7119856 -5.5469532 -5.5596542 -5.597878 -5.5019546 -5.5402694 -5.0524549 -4.6303864 -4.4384646 -4.5118427 -4.9456754 -5.5406303 -6.1246443 -6.1461635 -5.7309151][-6.5676441 -5.8493724 -5.4047074 -5.2437987 -5.2014437 -5.1477151 -4.9455409 -4.7369394 -4.4781857 -4.4722714 -4.6978188 -4.9688168 -5.3244038 -5.1978703 -4.7488785][-6.9594307 -5.9002728 -5.1922421 -4.9295158 -4.8283591 -4.7588196 -4.7119455 -4.6165709 -4.3863721 -4.3043613 -4.361043 -4.273973 -4.2067838 -4.0206332 -3.7213991]]...]
INFO - root - 2017-12-16 07:14:27.300314: step 45110, loss = 0.20, batch loss = 0.14 (28.3 examples/sec; 0.282 sec/batch; 22h:32m:10s remains)
INFO - root - 2017-12-16 07:14:30.101249: step 45120, loss = 0.21, batch loss = 0.15 (28.7 examples/sec; 0.279 sec/batch; 22h:13m:56s remains)
INFO - root - 2017-12-16 07:14:32.945291: step 45130, loss = 0.27, batch loss = 0.21 (29.4 examples/sec; 0.272 sec/batch; 21h:43m:17s remains)
INFO - root - 2017-12-16 07:14:35.756061: step 45140, loss = 0.29, batch loss = 0.24 (30.0 examples/sec; 0.267 sec/batch; 21h:18m:23s remains)
INFO - root - 2017-12-16 07:14:38.610111: step 45150, loss = 0.22, batch loss = 0.16 (27.0 examples/sec; 0.296 sec/batch; 23h:37m:49s remains)
INFO - root - 2017-12-16 07:14:41.466049: step 45160, loss = 0.24, batch loss = 0.18 (27.2 examples/sec; 0.294 sec/batch; 23h:29m:15s remains)
INFO - root - 2017-12-16 07:14:44.310659: step 45170, loss = 0.39, batch loss = 0.33 (27.9 examples/sec; 0.286 sec/batch; 22h:51m:16s remains)
INFO - root - 2017-12-16 07:14:47.110307: step 45180, loss = 0.30, batch loss = 0.24 (28.0 examples/sec; 0.286 sec/batch; 22h:48m:55s remains)
INFO - root - 2017-12-16 07:14:49.968120: step 45190, loss = 0.25, batch loss = 0.19 (25.5 examples/sec; 0.314 sec/batch; 25h:03m:21s remains)
INFO - root - 2017-12-16 07:14:52.775768: step 45200, loss = 0.24, batch loss = 0.18 (28.6 examples/sec; 0.279 sec/batch; 22h:18m:15s remains)
2017-12-16 07:14:53.275375: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2176237 -5.643487 -6.4702864 -8.1062107 -9.892168 -11.220247 -11.904696 -11.74239 -11.149609 -9.3913136 -7.3725471 -5.5555639 -4.2630992 -3.5437322 -3.2067976][-5.543189 -6.0605621 -6.7251072 -8.1117239 -9.435 -10.510494 -11.197565 -10.876251 -9.9953756 -8.494709 -6.72054 -5.1296844 -3.8787487 -3.2920833 -3.0119238][-6.0611858 -6.8306441 -6.9824247 -7.5240393 -8.2692385 -8.8298922 -8.9785538 -8.6607819 -7.7886558 -6.762104 -5.7497993 -4.608954 -3.7186956 -3.2755771 -2.9604115][-6.0456877 -6.608757 -6.4732828 -6.0062032 -5.5485573 -5.6292257 -5.7728658 -5.4176745 -5.0560732 -4.6370778 -4.2518916 -3.7561808 -3.3637905 -3.085288 -3.0323725][-5.6481161 -5.8162584 -4.9463425 -3.6925559 -2.38771 -1.6116004 -1.5892425 -1.6967771 -2.1835246 -2.5567212 -2.7229142 -2.75881 -2.8232963 -2.8694606 -2.9794066][-5.1455936 -4.8366342 -3.477953 -1.5091648 0.53702545 1.7688437 1.802537 1.1288376 0.038627148 -0.96844625 -1.6703386 -2.1045442 -2.2164242 -2.4091539 -2.5834713][-4.2029366 -3.6651468 -2.3313272 -0.46680117 1.6107812 2.8213048 2.9696546 2.1145091 0.76720095 -0.28608131 -0.91680384 -1.1676726 -1.6844437 -2.0432155 -2.159488][-3.5265131 -2.9729934 -1.8578808 -0.35384274 1.340538 2.4200954 2.8467159 2.1811013 1.0256519 0.013137817 -0.66197133 -1.0084913 -1.352885 -1.6807792 -1.9006658][-3.2989576 -3.2530091 -2.5414567 -1.4976592 -0.1825676 0.94752884 1.455503 1.2638073 0.63645411 -0.1118474 -0.6549232 -0.96149015 -1.270252 -1.5380483 -1.7371902][-3.8099694 -4.4282064 -4.2005086 -3.3874602 -2.4429808 -1.4142556 -0.68552971 -0.41177177 -0.61909485 -1.0166674 -1.3135207 -1.4638441 -1.5501602 -1.6852591 -1.7546387][-4.4768434 -5.1314921 -5.1207733 -4.5896149 -3.8685236 -3.0291 -2.5196581 -2.2798655 -2.3838985 -2.3137817 -2.2336593 -2.1188133 -2.0271707 -2.0059183 -2.0583394][-5.0937858 -5.4831762 -5.2045131 -4.6518192 -4.1968193 -3.7090263 -3.3693843 -3.3385353 -3.3604536 -3.2684526 -3.2662048 -3.1211505 -2.9365849 -2.7455087 -2.6544838][-5.6850653 -5.4740143 -4.9946156 -4.1969471 -3.6035962 -3.2466674 -3.1551828 -3.3049245 -3.5272548 -3.6262369 -3.6549478 -3.5644941 -3.4303267 -3.276876 -3.3314693][-6.0140657 -5.4138093 -4.573112 -3.8214834 -3.4393048 -3.1348872 -3.0755243 -3.155612 -3.3326802 -3.5322018 -3.6982148 -3.6509295 -3.6230626 -3.5604491 -3.6012669][-6.2449484 -5.3940253 -4.4725432 -3.6062222 -3.0967803 -2.8519266 -2.7071352 -2.7863545 -2.9781473 -3.132091 -3.2622881 -3.264467 -3.4156399 -3.5675449 -3.7326291]]...]
INFO - root - 2017-12-16 07:14:56.117774: step 45210, loss = 0.33, batch loss = 0.27 (29.1 examples/sec; 0.275 sec/batch; 21h:58m:15s remains)
INFO - root - 2017-12-16 07:14:58.958257: step 45220, loss = 0.36, batch loss = 0.30 (29.2 examples/sec; 0.274 sec/batch; 21h:50m:28s remains)
INFO - root - 2017-12-16 07:15:01.774890: step 45230, loss = 0.30, batch loss = 0.24 (28.4 examples/sec; 0.282 sec/batch; 22h:29m:12s remains)
INFO - root - 2017-12-16 07:15:04.608558: step 45240, loss = 0.27, batch loss = 0.22 (29.3 examples/sec; 0.273 sec/batch; 21h:46m:11s remains)
INFO - root - 2017-12-16 07:15:07.404226: step 45250, loss = 0.28, batch loss = 0.23 (29.2 examples/sec; 0.274 sec/batch; 21h:52m:44s remains)
INFO - root - 2017-12-16 07:15:10.249802: step 45260, loss = 0.28, batch loss = 0.23 (29.5 examples/sec; 0.271 sec/batch; 21h:37m:19s remains)
INFO - root - 2017-12-16 07:15:13.109088: step 45270, loss = 0.20, batch loss = 0.14 (27.3 examples/sec; 0.294 sec/batch; 23h:25m:12s remains)
INFO - root - 2017-12-16 07:15:15.907817: step 45280, loss = 0.25, batch loss = 0.20 (28.5 examples/sec; 0.281 sec/batch; 22h:24m:04s remains)
INFO - root - 2017-12-16 07:15:18.715193: step 45290, loss = 0.28, batch loss = 0.22 (29.0 examples/sec; 0.276 sec/batch; 22h:01m:22s remains)
INFO - root - 2017-12-16 07:15:21.571647: step 45300, loss = 0.28, batch loss = 0.22 (28.2 examples/sec; 0.284 sec/batch; 22h:37m:55s remains)
2017-12-16 07:15:22.035763: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0607214 -4.0839357 -3.9068182 -3.6008613 -3.4813147 -3.4602168 -3.4071016 -3.6812956 -3.7979059 -3.8631313 -3.63767 -3.3821805 -3.1396551 -2.9303548 -2.9677358][-3.9571714 -4.0240235 -3.9890749 -3.7733128 -3.5951521 -3.6653607 -3.799062 -3.9064832 -3.7632484 -3.7869139 -3.6876278 -3.4742858 -3.3288882 -3.1707749 -3.2881429][-3.8120675 -3.676012 -3.5126731 -3.5035214 -3.5293813 -3.6064126 -3.6680794 -3.7941701 -3.7024343 -3.5180662 -3.3176007 -3.2142971 -3.2314956 -3.2605669 -3.369319][-3.3337541 -3.3749285 -3.3412266 -3.443222 -3.5471325 -3.6285005 -3.5542865 -3.3039184 -2.972424 -2.8181367 -2.6645331 -2.7839198 -3.0165646 -3.3889937 -3.6771252][-2.5069678 -2.4355273 -2.5189147 -2.7490282 -2.8702211 -2.8173909 -2.5541382 -2.0515275 -1.4620419 -1.2323377 -1.2783475 -1.5874422 -1.9276652 -2.5348234 -3.0058041][-1.7924531 -1.6794879 -1.7824376 -1.9753983 -2.0389056 -1.8597727 -1.2398515 -0.47077107 0.15428209 0.39858294 0.16252899 -0.52841306 -1.2498894 -1.8622918 -2.1623933][-1.5314119 -1.4872308 -1.4272718 -1.5304723 -1.352113 -0.83444309 0.015741348 0.94692564 1.5391445 1.4884162 0.86822462 0.054389954 -0.793386 -1.4549918 -1.5795832][-1.3936207 -1.2663786 -1.1678579 -0.99920154 -0.5148747 0.35690832 1.3601289 2.2091694 2.5862045 2.118681 0.92824364 -0.27211571 -1.1593952 -1.7710025 -1.9188509][-1.9193728 -1.5506876 -1.186434 -0.73813105 0.076196194 1.0381026 2.0460463 2.662509 2.6092949 1.8481011 0.55841351 -0.88472342 -2.0082486 -2.4502232 -2.4210372][-2.3492925 -1.822613 -1.3552971 -0.95486426 -0.028738022 1.0278854 1.7697783 1.9953098 1.5484414 0.5748086 -0.80666614 -2.0582998 -3.0064707 -3.3710351 -3.2341576][-3.0157025 -2.6560466 -2.1097209 -1.590631 -0.69462204 0.0079169273 0.42078209 0.34564734 -0.44001889 -1.4517462 -2.5062869 -3.3258967 -3.9288642 -4.1079063 -3.8937554][-4.1889372 -3.7939043 -3.1176372 -2.5849843 -1.8308229 -1.3101013 -1.4280415 -1.7264442 -2.2705991 -3.1880245 -4.2414966 -4.66195 -4.6967335 -4.5272775 -4.2468438][-5.2113385 -5.0786929 -4.531858 -3.910485 -3.1807861 -2.7941649 -2.9551694 -3.4325528 -4.188303 -4.5726452 -4.7735863 -4.8388233 -4.7331042 -4.4278164 -4.0868731][-6.6509266 -6.423336 -5.776979 -5.2806935 -4.71042 -4.3915281 -4.4670315 -4.7057605 -5.1127672 -5.3459969 -5.2513332 -4.8179955 -4.3716946 -3.8662286 -3.4596686][-7.38122 -7.3591032 -6.8385906 -6.1760836 -5.548521 -5.2060156 -5.182445 -5.3511472 -5.6069117 -5.6217914 -5.3108816 -4.6663041 -4.0421157 -3.503933 -3.1988039]]...]
INFO - root - 2017-12-16 07:15:24.844540: step 45310, loss = 0.28, batch loss = 0.23 (29.4 examples/sec; 0.272 sec/batch; 21h:42m:59s remains)
INFO - root - 2017-12-16 07:15:27.694731: step 45320, loss = 0.25, batch loss = 0.19 (27.1 examples/sec; 0.295 sec/batch; 23h:31m:13s remains)
INFO - root - 2017-12-16 07:15:30.528655: step 45330, loss = 0.28, batch loss = 0.22 (28.2 examples/sec; 0.284 sec/batch; 22h:37m:25s remains)
INFO - root - 2017-12-16 07:15:33.362101: step 45340, loss = 0.20, batch loss = 0.15 (28.2 examples/sec; 0.284 sec/batch; 22h:39m:41s remains)
INFO - root - 2017-12-16 07:15:36.173833: step 45350, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.281 sec/batch; 22h:23m:58s remains)
INFO - root - 2017-12-16 07:15:39.096739: step 45360, loss = 0.26, batch loss = 0.21 (28.1 examples/sec; 0.284 sec/batch; 22h:40m:14s remains)
INFO - root - 2017-12-16 07:15:41.901510: step 45370, loss = 0.33, batch loss = 0.27 (27.5 examples/sec; 0.291 sec/batch; 23h:14m:29s remains)
INFO - root - 2017-12-16 07:15:44.695355: step 45380, loss = 0.22, batch loss = 0.16 (28.4 examples/sec; 0.282 sec/batch; 22h:28m:30s remains)
INFO - root - 2017-12-16 07:15:47.540269: step 45390, loss = 0.31, batch loss = 0.25 (27.2 examples/sec; 0.294 sec/batch; 23h:26m:29s remains)
INFO - root - 2017-12-16 07:15:50.347208: step 45400, loss = 0.32, batch loss = 0.26 (29.0 examples/sec; 0.276 sec/batch; 21h:59m:59s remains)
2017-12-16 07:15:50.824412: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.571259 -3.6607327 -3.6045034 -3.4697666 -3.1974611 -2.9443128 -2.7530565 -2.9110641 -3.1307838 -3.4015589 -3.9103146 -4.1584268 -4.1928267 -4.036674 -3.8156962][-3.6565626 -3.6588466 -3.5240479 -3.1214049 -2.5151 -2.1525872 -1.9516397 -2.0989974 -2.3450289 -2.6256981 -3.0327544 -3.1622519 -3.3592212 -3.2916505 -2.911355][-3.4971385 -3.2611732 -2.9221592 -2.4848306 -1.9956439 -1.5690398 -1.2413099 -1.2653239 -1.3324025 -1.6451261 -2.014164 -2.0530658 -2.192534 -2.1443086 -2.1780689][-3.1538382 -2.7726264 -2.3235145 -1.8024166 -1.2795203 -0.90915108 -0.55564666 -0.62237763 -0.81664324 -1.0173087 -1.1824152 -1.4002147 -1.6781662 -1.4138296 -1.3464358][-2.3416624 -1.7196565 -1.1342909 -0.84337282 -0.55577421 -0.2895236 -0.15699053 -0.27714729 -0.37539291 -0.74789071 -1.1517322 -1.2223811 -1.1301823 -1.1152403 -1.3708279][-1.0141273 -0.3782196 0.11303425 0.3188653 0.45052004 0.42481804 0.40173149 0.13547564 -0.34566402 -0.81781912 -1.1598895 -1.480535 -1.7728751 -1.5745392 -1.4258215][-0.01270628 0.32464266 0.62169695 0.75064421 0.84485292 0.88581276 0.82636881 0.40526342 -0.1173687 -0.78963351 -1.5526946 -2.0615349 -2.2349803 -2.3546853 -2.4816141][0.58767796 0.62661076 0.62231255 0.63805389 0.79128313 0.83597946 0.77449703 0.29126453 -0.50239587 -1.3962817 -2.2365806 -2.8127251 -3.2334697 -3.2072856 -2.9802079][0.37840939 0.32381248 0.436666 0.24196291 0.28115082 0.43318892 0.37557697 -0.15508318 -1.0405746 -2.0882072 -3.1086745 -3.7856221 -4.1192865 -3.9322579 -3.4765809][-0.39948177 -0.36285353 -0.46636248 -0.57054257 -0.42673063 -0.37125397 -0.46990395 -0.98532462 -1.8208549 -2.769742 -3.7257812 -4.4158115 -4.6779885 -4.2840815 -3.6646442][-1.1169174 -1.2177143 -1.1834998 -1.2565916 -1.2876775 -1.2904885 -1.3993416 -1.7518919 -2.4328666 -3.2914708 -4.1190858 -4.6818147 -4.7315006 -4.3259716 -3.5448141][-1.5683944 -1.7485123 -1.8122575 -1.9961562 -2.0966525 -2.0910969 -2.1677868 -2.4290164 -3.0168262 -3.6424673 -4.2311764 -4.8054848 -4.9001408 -4.4019909 -3.4268789][-1.8362117 -1.9749913 -1.9283366 -2.2271438 -2.4834125 -2.686048 -2.9968319 -3.1599216 -3.4173012 -3.8595538 -4.3585725 -4.7747617 -4.7532506 -4.3291016 -3.4230521][-1.9908371 -1.936461 -1.872061 -2.1050124 -2.397295 -2.5872951 -2.8179021 -3.1042485 -3.4635394 -3.7704482 -4.2077885 -4.70968 -4.7914629 -4.4691525 -3.6609035][-2.020401 -1.8403666 -1.7390473 -1.7761676 -1.9315686 -2.2171538 -2.6065645 -2.8495712 -3.1373663 -3.5749238 -4.0339561 -4.5200949 -4.7195225 -4.5912576 -3.9839389]]...]
INFO - root - 2017-12-16 07:15:53.717977: step 45410, loss = 0.24, batch loss = 0.18 (27.8 examples/sec; 0.287 sec/batch; 22h:55m:13s remains)
INFO - root - 2017-12-16 07:15:56.542798: step 45420, loss = 0.40, batch loss = 0.34 (28.9 examples/sec; 0.277 sec/batch; 22h:03m:44s remains)
INFO - root - 2017-12-16 07:15:59.395238: step 45430, loss = 0.26, batch loss = 0.20 (29.6 examples/sec; 0.271 sec/batch; 21h:35m:01s remains)
INFO - root - 2017-12-16 07:16:02.224027: step 45440, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.283 sec/batch; 22h:34m:04s remains)
INFO - root - 2017-12-16 07:16:05.092073: step 45450, loss = 0.20, batch loss = 0.14 (28.7 examples/sec; 0.279 sec/batch; 22h:15m:47s remains)
INFO - root - 2017-12-16 07:16:07.906059: step 45460, loss = 0.34, batch loss = 0.29 (29.0 examples/sec; 0.276 sec/batch; 21h:59m:24s remains)
INFO - root - 2017-12-16 07:16:10.753378: step 45470, loss = 0.29, batch loss = 0.23 (28.2 examples/sec; 0.284 sec/batch; 22h:38m:09s remains)
INFO - root - 2017-12-16 07:16:13.596201: step 45480, loss = 0.27, batch loss = 0.21 (27.5 examples/sec; 0.291 sec/batch; 23h:11m:55s remains)
INFO - root - 2017-12-16 07:16:16.429669: step 45490, loss = 0.28, batch loss = 0.22 (27.3 examples/sec; 0.293 sec/batch; 23h:22m:14s remains)
INFO - root - 2017-12-16 07:16:19.231724: step 45500, loss = 0.29, batch loss = 0.23 (29.1 examples/sec; 0.275 sec/batch; 21h:55m:08s remains)
2017-12-16 07:16:19.729732: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0970616 -3.3704345 -3.7435436 -4.0845013 -4.249373 -4.1735177 -3.8504574 -3.4641433 -3.1749592 -2.8834605 -2.6704259 -2.8608861 -3.1844544 -3.1779099 -3.0215497][-2.3791244 -2.6481223 -3.0261531 -3.4348567 -3.5377676 -3.4046702 -3.2094684 -2.9980216 -2.8168516 -2.5356569 -2.3892105 -2.4559944 -2.5193563 -2.5286307 -2.3786013][-1.6408117 -1.9820712 -2.3775079 -2.7274227 -2.9777493 -2.9614251 -2.6407688 -2.4255991 -2.3210533 -2.2586081 -2.193738 -2.0645032 -2.0951748 -2.059665 -1.906028][-1.0048344 -1.3297195 -1.6409378 -1.9368567 -1.973906 -1.933368 -1.8158147 -1.7010975 -1.5287712 -1.5037055 -1.5683689 -1.5113482 -1.5292387 -1.5467386 -1.5732813][-0.17792416 -0.51111984 -0.80198669 -0.92262316 -0.73241425 -0.55653 -0.51745629 -0.47554421 -0.55518961 -0.73108339 -0.89923453 -0.96412587 -1.025089 -1.0988538 -1.2688055][-0.33353472 -0.53481936 -0.58269596 -0.5567596 -0.29992104 0.16129827 0.56071568 0.6067791 0.3805666 0.13380432 -0.19471025 -0.3749094 -0.50589871 -0.67917442 -0.95974874][-0.89649868 -1.1660154 -1.1291034 -0.864825 -0.31525421 0.19420862 0.54677534 0.77499151 0.80488825 0.56857443 0.11727715 -0.16965485 -0.3187151 -0.47609854 -0.75478935][-1.6765294 -2.0389166 -2.1654766 -1.9135778 -1.215008 -0.55357361 -0.0198493 0.40989208 0.59048605 0.55537224 0.26159811 0.020586967 -0.097311974 -0.1288271 -0.35803795][-2.3598907 -2.8131366 -3.0212591 -2.8850212 -2.4047284 -1.6679766 -0.88685966 -0.48212528 -0.37154913 -0.23274565 -0.28937864 -0.35296202 -0.46107841 -0.35371304 -0.29108429][-3.3027921 -3.8722336 -4.0772057 -3.9533265 -3.51392 -2.8909526 -2.2879584 -1.6108706 -1.0524142 -0.93542886 -0.97466111 -1.0208366 -1.1354392 -0.99308372 -0.72968841][-3.7053561 -4.2473493 -4.6918006 -4.7235355 -4.4305186 -3.9076729 -3.2506127 -2.6533127 -2.3389447 -2.148015 -1.8851368 -1.7553585 -1.590307 -1.3146639 -1.1488688][-4.161438 -4.4840593 -4.6332431 -4.8686829 -4.9181356 -4.5235558 -3.9941609 -3.7097259 -3.3649137 -3.0913365 -2.9983373 -2.7220924 -2.3313761 -1.7506688 -1.2635036][-4.72009 -4.6542211 -4.538826 -4.47104 -4.4426508 -4.2488766 -3.94555 -3.7952533 -3.6897175 -3.7672751 -3.7049608 -3.3803482 -2.7328663 -1.9082026 -1.2873044][-4.8846498 -4.5116391 -4.1143427 -3.8397737 -3.8396053 -3.6602235 -3.510644 -3.5399754 -3.6424794 -3.8321872 -3.7547948 -3.4748938 -2.7209387 -1.7391825 -1.0970778][-4.7744966 -4.2504253 -3.6284304 -3.2578201 -3.0755272 -2.9988623 -3.0123861 -3.215517 -3.5312655 -3.8554544 -3.8051684 -3.270606 -2.2356536 -1.1156559 -0.49243927]]...]
INFO - root - 2017-12-16 07:16:22.556652: step 45510, loss = 0.34, batch loss = 0.28 (26.9 examples/sec; 0.297 sec/batch; 23h:41m:27s remains)
INFO - root - 2017-12-16 07:16:25.425733: step 45520, loss = 0.31, batch loss = 0.25 (27.5 examples/sec; 0.290 sec/batch; 23h:09m:01s remains)
INFO - root - 2017-12-16 07:16:28.307465: step 45530, loss = 0.22, batch loss = 0.17 (27.5 examples/sec; 0.291 sec/batch; 23h:10m:22s remains)
INFO - root - 2017-12-16 07:16:31.145460: step 45540, loss = 0.23, batch loss = 0.17 (29.0 examples/sec; 0.276 sec/batch; 21h:58m:47s remains)
INFO - root - 2017-12-16 07:16:33.991413: step 45550, loss = 0.20, batch loss = 0.14 (29.4 examples/sec; 0.272 sec/batch; 21h:40m:25s remains)
INFO - root - 2017-12-16 07:16:36.849900: step 45560, loss = 0.26, batch loss = 0.21 (26.2 examples/sec; 0.305 sec/batch; 24h:19m:33s remains)
INFO - root - 2017-12-16 07:16:39.735289: step 45570, loss = 0.21, batch loss = 0.15 (26.3 examples/sec; 0.304 sec/batch; 24h:15m:57s remains)
INFO - root - 2017-12-16 07:16:42.563476: step 45580, loss = 0.30, batch loss = 0.24 (28.6 examples/sec; 0.279 sec/batch; 22h:15m:30s remains)
INFO - root - 2017-12-16 07:16:45.383750: step 45590, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 22h:17m:13s remains)
INFO - root - 2017-12-16 07:16:48.185181: step 45600, loss = 0.45, batch loss = 0.40 (28.5 examples/sec; 0.281 sec/batch; 22h:23m:11s remains)
2017-12-16 07:16:48.660284: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3805027 -5.5036397 -5.4369469 -5.3649459 -5.3108168 -5.2755928 -5.3134031 -5.475018 -5.6576104 -5.7602572 -5.8128572 -5.7422891 -5.5107584 -5.1633477 -4.8471956][-5.39457 -5.2971487 -5.2004032 -5.2662387 -5.3145995 -5.5022435 -5.6865635 -5.7449465 -5.7982111 -6.0575209 -6.2357316 -6.0893459 -5.8395653 -5.5733919 -5.3441939][-4.7503519 -4.6749845 -4.7025819 -4.6899662 -4.747972 -5.0254335 -5.0836506 -5.2465506 -5.5075483 -5.6687336 -5.8309369 -6.0470834 -6.076653 -5.7762427 -5.5320711][-3.908232 -3.4423714 -3.2618914 -3.3760772 -3.4586535 -3.6079185 -3.5858228 -3.7008908 -3.9223313 -4.3729558 -4.7905879 -5.1317391 -5.3796663 -5.4001408 -5.3462706][-2.3550408 -1.5621517 -1.2416592 -1.4322941 -1.629951 -1.6206326 -1.3715103 -1.4210839 -1.6325872 -2.2413542 -2.9467232 -3.6048677 -4.1081495 -4.2991109 -4.3032656][-1.2189138 -0.35581589 0.048298359 0.10647011 0.17495251 0.38211489 0.86763906 1.1487684 0.98437405 0.29025221 -0.59689951 -1.69385 -2.5813885 -3.0166907 -3.1311109][-0.55716944 0.16837263 0.48549366 0.60100126 0.94292116 1.7011619 2.7182398 3.2265716 3.0243621 2.3121538 1.4075975 0.23659658 -0.86768031 -1.6478465 -2.0931165][-0.11722708 0.46608448 0.75557804 0.97502422 1.2523193 1.9745879 3.0788908 3.847682 3.8460312 3.1504197 2.1580753 0.91937304 -0.14000273 -0.76819348 -1.3367133][-0.19431162 0.07521677 0.15172052 0.33108377 0.73935938 1.5051179 2.4809766 3.0313759 3.0333238 2.568954 1.8020663 0.71013784 -0.32034302 -0.90585947 -1.2692893][-0.57362461 -0.44345737 -0.61396408 -0.52933717 -0.20666647 0.35672235 1.1214118 1.623353 1.6647358 1.1963592 0.55413437 -0.21643019 -0.95405555 -1.3320198 -1.5852652][-1.2492068 -1.0861409 -1.1391575 -1.1197565 -0.94124866 -0.62026286 -0.16240072 0.047686577 0.0023808479 -0.35229683 -0.84819841 -1.4556091 -1.9421086 -2.0334427 -2.1310427][-1.5433345 -1.3350527 -1.3795245 -1.4540617 -1.335815 -1.1010613 -0.87443733 -0.96951389 -1.2169616 -1.6705396 -2.1227109 -2.4445567 -2.7106547 -2.520256 -2.3640003][-1.682133 -1.3799295 -1.3066759 -1.398402 -1.4373302 -1.3273079 -1.2218328 -1.5702231 -2.0086875 -2.4773159 -2.8913708 -3.0335832 -3.1422114 -2.8149137 -2.416465][-1.8101521 -1.3853228 -1.1951754 -1.1710989 -1.1271877 -1.1200037 -1.0247207 -1.4353147 -2.1107075 -2.8869314 -3.3356478 -3.3147261 -3.1867065 -2.7627485 -2.3554006][-1.8438168 -1.3663015 -1.1323161 -1.0663638 -0.9614706 -0.85128951 -0.70392156 -1.1666994 -1.8905261 -2.7305868 -3.3064985 -3.3909974 -3.258796 -2.7683752 -2.3571253]]...]
INFO - root - 2017-12-16 07:16:51.490234: step 45610, loss = 0.22, batch loss = 0.17 (25.5 examples/sec; 0.314 sec/batch; 24h:59m:12s remains)
INFO - root - 2017-12-16 07:16:54.363214: step 45620, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.277 sec/batch; 22h:04m:11s remains)
INFO - root - 2017-12-16 07:16:57.219967: step 45630, loss = 0.20, batch loss = 0.14 (29.2 examples/sec; 0.274 sec/batch; 21h:48m:18s remains)
INFO - root - 2017-12-16 07:17:00.041676: step 45640, loss = 0.24, batch loss = 0.18 (27.2 examples/sec; 0.294 sec/batch; 23h:26m:39s remains)
INFO - root - 2017-12-16 07:17:02.917476: step 45650, loss = 0.28, batch loss = 0.23 (28.2 examples/sec; 0.284 sec/batch; 22h:35m:26s remains)
INFO - root - 2017-12-16 07:17:05.745189: step 45660, loss = 0.35, batch loss = 0.29 (26.4 examples/sec; 0.303 sec/batch; 24h:06m:16s remains)
INFO - root - 2017-12-16 07:17:08.556790: step 45670, loss = 0.25, batch loss = 0.20 (28.4 examples/sec; 0.281 sec/batch; 22h:24m:28s remains)
INFO - root - 2017-12-16 07:17:11.390207: step 45680, loss = 0.24, batch loss = 0.18 (27.1 examples/sec; 0.296 sec/batch; 23h:32m:48s remains)
INFO - root - 2017-12-16 07:17:14.256395: step 45690, loss = 0.22, batch loss = 0.16 (28.1 examples/sec; 0.285 sec/batch; 22h:41m:52s remains)
INFO - root - 2017-12-16 07:17:17.040621: step 45700, loss = 0.27, batch loss = 0.21 (29.4 examples/sec; 0.272 sec/batch; 21h:41m:10s remains)
2017-12-16 07:17:17.531002: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8326321 -2.8954139 -3.0770233 -3.5934019 -4.0260334 -4.4685769 -4.779788 -4.9138784 -5.0194335 -5.060071 -4.9972262 -4.8928738 -4.9570069 -4.8028445 -4.5480332][-1.6435027 -1.7707858 -2.1405594 -2.6597252 -2.919343 -3.569376 -3.9876649 -4.1265926 -4.3983488 -4.6199946 -4.7821388 -4.644351 -4.4344645 -4.2148514 -3.9442372][-0.057198048 -0.24313402 -0.793658 -1.3212597 -1.9027963 -2.563041 -2.8867235 -3.2026772 -3.5559196 -3.862745 -4.3450294 -4.3324728 -4.0407557 -3.5562212 -3.0549746][1.3973651 0.95553589 0.2052536 -0.41483498 -1.0213819 -1.5875335 -1.9691148 -2.1310012 -2.4906754 -3.0882397 -3.6641741 -3.8017943 -3.6171627 -2.9859829 -2.3720148][1.8540678 1.2772312 0.47763968 -0.13770819 -0.54550481 -0.83041573 -0.78937483 -0.58128643 -1.0115011 -1.9746063 -2.9174013 -3.2578797 -3.1092486 -2.5377371 -2.0465627][1.7257996 1.1941595 0.58560514 0.27325583 0.32697964 0.35271549 0.62928057 0.98420906 0.67175388 -0.35797787 -1.5315347 -2.347487 -2.5692053 -2.4398563 -2.0244334][1.127142 0.82052994 0.34153652 0.29560328 0.71622658 1.2593837 2.0298877 2.4924903 2.1757393 1.1798353 -0.25354242 -1.527241 -2.1620405 -2.3424788 -2.19852][0.20018482 -0.047462463 -0.31705952 -0.015873432 0.693748 1.6063395 2.8326955 3.6010828 3.3483214 2.0349369 0.27075577 -1.3665428 -2.531456 -3.0221694 -2.8667159][-0.98929572 -1.1617239 -1.1012263 -0.57084179 0.31423807 1.438992 2.6291494 3.4900975 3.2434931 1.9227247 0.10438251 -1.6072721 -2.9272375 -3.685684 -3.5278852][-1.8921041 -1.8574777 -1.8003004 -1.2115533 -0.16723728 0.91229296 1.9173474 2.6032166 2.5039191 1.193471 -0.62752175 -2.306401 -3.5567014 -4.2485642 -4.2075529][-2.5086162 -2.4568534 -2.4500866 -2.0321095 -1.2829754 -0.47320414 0.30916786 0.71415281 0.48534346 -0.41516113 -1.7120144 -3.13316 -4.0647478 -4.5844216 -4.4834991][-2.8199935 -3.1203144 -3.2984295 -3.090704 -2.5436535 -2.0271688 -1.4580123 -0.94665146 -1.1475148 -1.9138176 -2.982471 -3.9513974 -4.6005564 -4.894053 -4.704638][-2.8936377 -3.2596602 -3.8359292 -4.0161347 -3.7394662 -3.327632 -2.9601583 -2.5076027 -2.4786487 -3.0493407 -3.6803551 -4.2759328 -4.6019316 -4.7949076 -4.4178395][-2.7789869 -3.4177904 -4.2030697 -4.6175718 -4.679575 -4.4680095 -4.1392241 -3.5813692 -3.4120522 -3.5723212 -3.7705061 -3.9586916 -3.8750653 -3.8767374 -3.6175735][-2.8632379 -3.5748062 -4.2386246 -4.6507773 -4.8147836 -4.81174 -4.4497972 -3.9271505 -3.6649065 -3.620841 -3.5295169 -3.508328 -3.2994947 -3.2073734 -2.9731638]]...]
INFO - root - 2017-12-16 07:17:20.336773: step 45710, loss = 0.26, batch loss = 0.20 (29.0 examples/sec; 0.276 sec/batch; 22h:00m:48s remains)
INFO - root - 2017-12-16 07:17:23.213402: step 45720, loss = 0.28, batch loss = 0.22 (28.0 examples/sec; 0.286 sec/batch; 22h:47m:34s remains)
INFO - root - 2017-12-16 07:17:26.009662: step 45730, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 22h:13m:52s remains)
INFO - root - 2017-12-16 07:17:28.834320: step 45740, loss = 0.22, batch loss = 0.16 (28.7 examples/sec; 0.279 sec/batch; 22h:11m:05s remains)
INFO - root - 2017-12-16 07:17:31.670164: step 45750, loss = 0.28, batch loss = 0.22 (28.5 examples/sec; 0.281 sec/batch; 22h:22m:18s remains)
INFO - root - 2017-12-16 07:17:34.525192: step 45760, loss = 0.28, batch loss = 0.22 (27.8 examples/sec; 0.288 sec/batch; 22h:56m:35s remains)
INFO - root - 2017-12-16 07:17:37.405592: step 45770, loss = 0.29, batch loss = 0.23 (27.8 examples/sec; 0.288 sec/batch; 22h:53m:58s remains)
INFO - root - 2017-12-16 07:17:40.239494: step 45780, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.283 sec/batch; 22h:33m:35s remains)
INFO - root - 2017-12-16 07:17:43.041587: step 45790, loss = 0.32, batch loss = 0.26 (29.1 examples/sec; 0.275 sec/batch; 21h:55m:37s remains)
INFO - root - 2017-12-16 07:17:45.947946: step 45800, loss = 0.23, batch loss = 0.17 (27.7 examples/sec; 0.289 sec/batch; 22h:58m:53s remains)
2017-12-16 07:17:46.435065: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.385673 -4.4323158 -4.3864589 -4.3560781 -4.3279247 -4.1340461 -4.1485147 -4.5429678 -4.854907 -4.8365731 -4.6567388 -4.0277095 -3.2616785 -2.4215982 -1.8210053][-4.902647 -5.0390925 -5.2241583 -5.0426526 -4.6841941 -4.4264174 -4.2208819 -4.2844744 -4.3868032 -4.2330279 -4.0508428 -3.4612052 -2.7974327 -1.8623343 -0.99805284][-5.5339413 -5.7688575 -5.72763 -5.3648696 -4.793951 -4.0619049 -3.3576608 -3.0843506 -2.8444018 -2.7426145 -2.904314 -2.6735239 -2.2290769 -1.5502634 -0.89176035][-6.2237921 -6.5204582 -6.4262037 -5.7182088 -4.6191139 -3.1890213 -1.6959457 -0.82946587 -0.5223186 -0.46796346 -0.57988095 -1.0129068 -1.4033606 -1.2864652 -0.98104262][-6.5732775 -6.9226933 -6.5778265 -5.4665442 -3.9036555 -1.9116848 0.020541668 1.3921323 2.0271983 1.6392126 0.69729471 -0.20360613 -0.9947567 -1.6313868 -1.904227][-6.6247931 -6.7982445 -6.0818291 -4.4343204 -2.265862 0.18257523 2.5077629 3.9066505 4.2624121 3.6059666 2.0967851 0.28563213 -1.2994764 -2.1920104 -2.4477673][-6.4352126 -6.5136862 -5.4334688 -3.5204585 -1.010668 1.8403206 4.1851206 5.5347719 5.5289021 4.3131933 2.2996192 0.097295761 -1.6705837 -2.6599398 -2.9920728][-6.295135 -6.4649129 -5.4474034 -3.1786268 -0.48482823 2.4944372 4.8800049 5.884511 5.5115023 3.9368887 1.7247758 -0.77494645 -2.6325948 -3.6215324 -3.7850285][-5.7165723 -6.0338411 -5.3629689 -3.5283475 -1.048655 1.8533602 4.1149569 5.0398655 4.6117887 2.9716187 0.75853062 -1.608428 -3.242712 -4.0839252 -4.138957][-4.7858019 -5.5314803 -5.3850226 -4.0582824 -2.1407111 0.18125486 2.0535727 2.8271241 2.4635878 0.9310627 -1.1305819 -3.029449 -4.3400812 -4.8223505 -4.6300812][-4.2475433 -5.2356496 -5.5553827 -4.8701372 -3.4785864 -1.6816516 -0.24887228 0.29840612 -0.11470604 -1.4389436 -3.0995679 -4.6880655 -5.6990671 -5.9216413 -5.5392332][-4.0116329 -4.9424109 -5.6205606 -5.3804793 -4.5946913 -3.3580022 -2.4154923 -2.1203082 -2.2958186 -3.2326651 -4.4680696 -5.8183193 -6.6074724 -6.5643129 -6.0794225][-3.6418939 -4.4895353 -5.2607226 -5.51519 -5.4139581 -4.5982037 -3.8388863 -3.7055938 -4.1105042 -4.8641949 -5.5452595 -6.2530565 -6.6693158 -6.51707 -5.9660635][-3.5131407 -4.1009097 -4.7049246 -5.0458808 -5.1554942 -4.9360714 -4.7042632 -4.7947736 -5.1421065 -5.6943412 -6.1457257 -6.4764562 -6.4194031 -5.8209853 -5.0924482][-3.7964382 -4.153542 -4.57706 -4.7941689 -4.983954 -4.808876 -4.6617708 -4.7831779 -5.0312481 -5.4062681 -5.6715651 -5.7901382 -5.6141109 -5.0470004 -4.4167862]]...]
INFO - root - 2017-12-16 07:17:49.291876: step 45810, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.286 sec/batch; 22h:46m:56s remains)
INFO - root - 2017-12-16 07:17:52.081573: step 45820, loss = 0.25, batch loss = 0.19 (29.4 examples/sec; 0.272 sec/batch; 21h:38m:03s remains)
INFO - root - 2017-12-16 07:17:54.991624: step 45830, loss = 0.27, batch loss = 0.21 (27.2 examples/sec; 0.294 sec/batch; 23h:24m:15s remains)
INFO - root - 2017-12-16 07:17:57.809806: step 45840, loss = 0.27, batch loss = 0.22 (28.0 examples/sec; 0.286 sec/batch; 22h:44m:51s remains)
INFO - root - 2017-12-16 07:18:00.611118: step 45850, loss = 0.20, batch loss = 0.14 (28.0 examples/sec; 0.285 sec/batch; 22h:43m:03s remains)
INFO - root - 2017-12-16 07:18:03.456251: step 45860, loss = 0.32, batch loss = 0.27 (29.2 examples/sec; 0.274 sec/batch; 21h:47m:46s remains)
INFO - root - 2017-12-16 07:18:06.341247: step 45870, loss = 0.30, batch loss = 0.25 (27.7 examples/sec; 0.288 sec/batch; 22h:58m:05s remains)
INFO - root - 2017-12-16 07:18:09.201396: step 45880, loss = 0.22, batch loss = 0.16 (28.4 examples/sec; 0.281 sec/batch; 22h:23m:56s remains)
INFO - root - 2017-12-16 07:18:12.029552: step 45890, loss = 0.29, batch loss = 0.23 (28.7 examples/sec; 0.278 sec/batch; 22h:09m:46s remains)
INFO - root - 2017-12-16 07:18:14.826124: step 45900, loss = 0.27, batch loss = 0.21 (27.5 examples/sec; 0.291 sec/batch; 23h:11m:48s remains)
2017-12-16 07:18:15.299001: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1696076 -6.507637 -6.7614241 -6.8987179 -6.8925514 -6.9074688 -6.9468527 -7.1867023 -7.3948736 -7.6233158 -7.5547905 -7.2948523 -6.9109664 -6.3412504 -5.6742549][-7.37195 -7.5815105 -7.6088705 -7.505836 -7.4464178 -7.8418465 -8.1250772 -8.4206123 -8.6687994 -9.077816 -9.2376947 -9.0252819 -8.4938869 -7.7151337 -6.8618388][-7.8511276 -7.8336639 -7.67964 -7.2646503 -6.9366093 -7.0725975 -7.4732413 -8.3456516 -9.0937681 -9.7801781 -10.15223 -10.106607 -9.76305 -9.0495205 -8.2547646][-7.5577 -7.1413445 -6.3735104 -5.7320647 -5.1185117 -4.8811646 -5.0225277 -5.8814392 -7.1110439 -8.6890621 -9.6535511 -9.8707418 -9.8154526 -9.4800549 -9.1261082][-6.6002507 -5.4512558 -3.8637171 -2.4507971 -1.2295346 -0.87680745 -1.11533 -1.9430623 -3.2878633 -5.1856341 -6.8776083 -8.1601667 -8.9946728 -9.1965075 -9.3279991][-5.3181877 -3.7666514 -1.7887852 0.46009827 2.5765758 3.5319033 3.6100016 2.4861622 0.73798037 -1.2812572 -3.1665635 -5.1595149 -7.1020246 -8.292345 -8.9550123][-4.9124293 -3.2311671 -0.81612062 1.8858924 4.4457207 6.0198946 6.7649746 5.8710842 4.2053967 2.0655065 -0.12981462 -2.4700842 -4.8630843 -6.6911745 -7.8307943][-5.3037071 -3.5072465 -1.0660825 1.7646599 4.632287 6.6443644 7.6084471 7.0727577 5.8791895 3.6588173 1.371253 -0.9772594 -3.4408042 -5.3683138 -6.5551534][-6.1326962 -4.525147 -2.3990548 0.24505424 2.8354516 4.871192 6.3989286 6.5510454 5.763793 3.9081173 1.7914424 -0.67063 -3.1449256 -4.8319573 -5.6566062][-7.4120445 -6.3690805 -4.777884 -2.6520009 -0.35470057 1.5029116 3.0556011 3.4343977 3.1719666 1.7873869 0.14990664 -1.8909016 -3.9420092 -5.2001309 -5.72188][-8.2542839 -7.7461967 -6.7654324 -5.3568573 -3.8712597 -2.3488748 -0.64840055 0.1508503 0.11218262 -1.0939894 -2.2799296 -3.7314088 -5.0596409 -5.91809 -6.3415089][-8.4887981 -8.1962729 -7.9417109 -6.9513054 -5.8284125 -4.8792791 -3.6809955 -3.0227392 -2.6581879 -3.0854084 -3.9828892 -5.3806567 -6.439496 -6.7698236 -6.7004805][-7.5470762 -7.4232349 -7.5445528 -6.9869189 -6.4336948 -5.5610914 -4.6322117 -4.4638748 -4.257524 -4.4822483 -4.891263 -5.7588677 -6.5507679 -6.8381824 -6.8229065][-5.9243169 -5.89733 -5.9413576 -5.799809 -5.6280637 -4.9781 -4.4084787 -4.3860765 -4.4560213 -4.6741652 -4.8372021 -5.3159614 -5.7654562 -5.8715429 -5.7572374][-4.625391 -4.3046665 -4.1478596 -4.1602631 -4.1698232 -4.009665 -3.8921072 -3.892277 -3.9157498 -4.1132259 -4.1975646 -4.2349811 -4.3080931 -4.3829708 -4.4046273]]...]
INFO - root - 2017-12-16 07:18:18.130151: step 45910, loss = 0.29, batch loss = 0.23 (29.2 examples/sec; 0.274 sec/batch; 21h:49m:53s remains)
INFO - root - 2017-12-16 07:18:20.987661: step 45920, loss = 0.33, batch loss = 0.27 (28.3 examples/sec; 0.283 sec/batch; 22h:32m:15s remains)
INFO - root - 2017-12-16 07:18:23.852941: step 45930, loss = 0.30, batch loss = 0.24 (26.8 examples/sec; 0.298 sec/batch; 23h:45m:21s remains)
INFO - root - 2017-12-16 07:18:26.714665: step 45940, loss = 0.21, batch loss = 0.15 (28.4 examples/sec; 0.281 sec/batch; 22h:24m:03s remains)
INFO - root - 2017-12-16 07:18:29.522087: step 45950, loss = 0.26, batch loss = 0.20 (29.3 examples/sec; 0.273 sec/batch; 21h:42m:47s remains)
INFO - root - 2017-12-16 07:18:32.389818: step 45960, loss = 0.26, batch loss = 0.21 (25.5 examples/sec; 0.314 sec/batch; 25h:00m:48s remains)
INFO - root - 2017-12-16 07:18:35.211832: step 45970, loss = 0.23, batch loss = 0.17 (28.6 examples/sec; 0.280 sec/batch; 22h:15m:57s remains)
INFO - root - 2017-12-16 07:18:38.050863: step 45980, loss = 0.32, batch loss = 0.26 (28.8 examples/sec; 0.277 sec/batch; 22h:04m:52s remains)
INFO - root - 2017-12-16 07:18:40.886482: step 45990, loss = 0.34, batch loss = 0.29 (26.4 examples/sec; 0.304 sec/batch; 24h:09m:41s remains)
INFO - root - 2017-12-16 07:18:43.753870: step 46000, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.280 sec/batch; 22h:18m:21s remains)
2017-12-16 07:18:44.211685: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7823949 -5.0977211 -4.0660357 -3.0648909 -2.2865286 -1.9789896 -1.8558147 -2.1207249 -2.978476 -3.8179359 -4.3177695 -4.6392021 -4.5919514 -4.1115732 -3.4417238][-5.0059137 -4.3577704 -3.4519122 -2.5417237 -1.833461 -1.45209 -1.3475852 -1.7408893 -2.6082935 -3.470803 -4.141057 -4.4379373 -4.4069853 -4.1017976 -3.467576][-4.1069131 -3.5189559 -2.7907689 -1.9973192 -1.3791392 -1.0529401 -0.98590326 -1.3770292 -2.3022444 -3.3684003 -4.2037053 -4.7827492 -4.8334308 -4.459229 -3.7880762][-3.0037978 -2.5922818 -1.9312456 -1.4199247 -0.92616439 -0.60768962 -0.50366664 -0.87311888 -1.8364978 -2.9812131 -4.0181484 -4.651607 -4.8262849 -4.4141116 -3.7705045][-1.9325364 -1.6885722 -1.1773143 -0.83644652 -0.16912317 0.45052481 0.743238 0.55075455 -0.49672365 -1.871696 -3.2656584 -4.25974 -4.636076 -4.4928951 -3.8288693][-1.6539476 -1.5419424 -1.1505833 -0.70159936 0.29080629 1.3627844 2.0958257 2.0702453 1.1825752 -0.50782204 -2.3253152 -3.7930126 -4.6641555 -4.7989836 -4.3101125][-1.6924539 -1.7273045 -1.4369981 -0.89935231 0.29610443 1.6882439 2.7396278 2.8786278 1.9567642 0.14178181 -2.0090065 -3.9373777 -5.0554285 -5.3901567 -5.0320587][-2.0116749 -2.0290959 -1.7164731 -1.0257473 0.41672611 1.8278718 2.8055997 3.0964503 2.2828379 0.3486948 -2.0101082 -3.9926615 -5.3050995 -5.8010225 -5.4188638][-2.2277391 -2.3240573 -2.025233 -1.4872098 -0.0702734 1.404449 2.5066152 2.8264093 1.9735107 0.22086811 -2.0537362 -4.1719661 -5.6480618 -6.1954808 -5.8247857][-2.2640228 -2.4839225 -2.3570402 -1.9016688 -0.86417532 0.3807559 1.45118 1.8815508 1.1882262 -0.48554373 -2.6513541 -4.6178265 -6.0262995 -6.6222382 -6.2542291][-1.9748077 -2.4601598 -2.7863042 -2.5748575 -1.7325723 -0.56672907 0.38092661 0.75204182 0.22920513 -1.3060558 -3.2314618 -5.0939436 -6.3239307 -6.7556448 -6.3891644][-2.1197503 -2.7450771 -3.0970898 -3.0150266 -2.33943 -1.3601716 -0.60649991 -0.32970142 -0.88235569 -2.2367578 -3.928592 -5.5172544 -6.4940424 -6.6193409 -6.0616312][-2.2603538 -3.0045991 -3.4132969 -3.2695556 -2.5671539 -1.7234395 -1.1827021 -0.92311335 -1.5652292 -2.8168378 -4.2778254 -5.5673614 -6.1610742 -6.1591353 -5.6282244][-2.4279418 -3.1843572 -3.6380455 -3.4422932 -2.8378878 -2.0067732 -1.4248435 -1.4078538 -2.0313537 -3.0789213 -4.3709 -5.3653173 -5.8154531 -5.7008204 -5.0487952][-3.2304969 -3.8166397 -3.9991143 -3.6142938 -2.9471292 -2.1191087 -1.6122556 -1.6181681 -2.1716015 -3.2784078 -4.3837886 -5.1993742 -5.5067749 -5.2072616 -4.47683]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 07:18:47.039469: step 46010, loss = 0.18, batch loss = 0.12 (28.3 examples/sec; 0.283 sec/batch; 22h:30m:52s remains)
INFO - root - 2017-12-16 07:18:49.904262: step 46020, loss = 0.34, batch loss = 0.28 (26.3 examples/sec; 0.305 sec/batch; 24h:15m:03s remains)
INFO - root - 2017-12-16 07:18:52.743190: step 46030, loss = 0.33, batch loss = 0.27 (28.4 examples/sec; 0.282 sec/batch; 22h:24m:08s remains)
INFO - root - 2017-12-16 07:18:55.578295: step 46040, loss = 0.33, batch loss = 0.27 (27.1 examples/sec; 0.295 sec/batch; 23h:27m:53s remains)
INFO - root - 2017-12-16 07:18:58.433581: step 46050, loss = 0.25, batch loss = 0.20 (29.4 examples/sec; 0.272 sec/batch; 21h:38m:05s remains)
INFO - root - 2017-12-16 07:19:01.308050: step 46060, loss = 0.30, batch loss = 0.24 (27.1 examples/sec; 0.296 sec/batch; 23h:31m:03s remains)
INFO - root - 2017-12-16 07:19:04.139388: step 46070, loss = 0.23, batch loss = 0.17 (27.6 examples/sec; 0.290 sec/batch; 23h:03m:43s remains)
INFO - root - 2017-12-16 07:19:06.967562: step 46080, loss = 0.31, batch loss = 0.26 (29.3 examples/sec; 0.273 sec/batch; 21h:43m:46s remains)
INFO - root - 2017-12-16 07:19:09.831562: step 46090, loss = 0.34, batch loss = 0.28 (27.0 examples/sec; 0.296 sec/batch; 23h:33m:13s remains)
INFO - root - 2017-12-16 07:19:12.656050: step 46100, loss = 0.25, batch loss = 0.20 (28.6 examples/sec; 0.279 sec/batch; 22h:13m:35s remains)
2017-12-16 07:19:13.104185: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9736986 -4.6581864 -4.374444 -4.2362971 -4.2417469 -4.1644325 -4.1906695 -4.2460766 -4.2330775 -3.9095025 -3.4260776 -3.021575 -2.728756 -2.5383992 -2.5965605][-3.8681159 -3.617198 -3.4393871 -3.4096851 -3.560406 -3.6357949 -3.7793896 -3.8591065 -3.9227161 -3.8877041 -3.6463103 -3.3626611 -3.0734003 -2.8870068 -2.9370582][-2.4104066 -2.3408802 -2.4346526 -2.5225055 -2.7291241 -2.9714613 -3.1911421 -3.3467984 -3.5893087 -3.8854511 -4.0075078 -3.8516531 -3.6266973 -3.5142176 -3.5553665][-0.93145037 -1.1893663 -1.5392153 -1.6364853 -1.7812703 -2.010716 -2.2160745 -2.5354862 -2.8460555 -3.3475237 -3.8132732 -4.0241904 -4.1429396 -4.0988336 -4.0457826][0.1881299 -0.24778032 -0.73869061 -1.0945656 -1.1360574 -1.1547678 -1.0793278 -1.0255418 -1.2542231 -2.0570483 -2.8911319 -3.5083003 -3.8978987 -4.1421905 -4.16043][0.013049603 -0.29728889 -0.69475031 -0.74826241 -0.35009193 -0.017019749 0.32145548 0.524282 0.40702343 -0.42929769 -1.5508275 -2.4900522 -3.1870306 -3.6652694 -3.7336807][-0.43769765 -0.63120437 -0.81403351 -0.57503414 0.28797054 1.1732087 2.0309305 2.3833127 2.1528673 1.215198 -0.23568487 -1.5072863 -2.5116391 -3.0174034 -3.0840564][-1.1438675 -1.3265688 -1.2651882 -0.79600477 0.28906965 1.5830331 2.819634 3.4245977 3.2055206 2.2690945 0.90128136 -0.56276035 -1.8213341 -2.5204923 -2.6849229][-2.1586747 -2.2557983 -2.232142 -1.718173 -0.73155594 0.71728325 2.0844445 2.7570324 2.7729011 1.7969675 0.41342258 -1.0599046 -2.1142147 -2.5687523 -2.6356902][-2.8176315 -3.1722026 -3.4023347 -2.9204526 -1.9125705 -0.74349642 0.31663084 0.87603951 0.92262506 0.21135902 -1.001611 -2.4069123 -3.3008027 -3.4432225 -3.221621][-3.423243 -3.6327558 -3.8281732 -3.7565019 -3.1778874 -2.1027765 -1.1245348 -1.0000753 -1.332406 -1.932234 -2.7298779 -3.7029128 -4.399209 -4.4047484 -3.8741283][-3.7318382 -3.9531081 -4.2270141 -4.0941896 -3.7486429 -3.252821 -2.7330225 -2.6602402 -2.7480173 -3.0718887 -3.6767094 -4.5504427 -5.1168079 -4.9415178 -4.4045391][-3.5215433 -3.6124811 -3.9481854 -4.1947002 -4.2439442 -3.9161651 -3.4983945 -3.5928798 -3.7841015 -4.0034165 -4.1753821 -4.4449654 -4.7530031 -4.7052994 -4.4315763][-3.6133804 -3.2855568 -3.1895952 -3.3279977 -3.5305798 -3.4068136 -3.1579776 -3.3296168 -3.4507399 -3.3096828 -3.3327622 -3.6869452 -4.0963559 -4.1737757 -4.0408912][-3.5609756 -3.1116762 -2.8284907 -2.7636042 -2.7006698 -2.7065501 -2.7160718 -2.7807579 -2.6702414 -2.4784806 -2.3733408 -2.1489797 -2.2291942 -2.7577815 -3.3732722]]...]
INFO - root - 2017-12-16 07:19:15.900093: step 46110, loss = 0.35, batch loss = 0.29 (27.3 examples/sec; 0.293 sec/batch; 23h:18m:59s remains)
INFO - root - 2017-12-16 07:19:18.705051: step 46120, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.283 sec/batch; 22h:29m:06s remains)
INFO - root - 2017-12-16 07:19:21.527844: step 46130, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 22h:34m:29s remains)
INFO - root - 2017-12-16 07:19:24.320908: step 46140, loss = 0.23, batch loss = 0.17 (29.4 examples/sec; 0.272 sec/batch; 21h:37m:30s remains)
INFO - root - 2017-12-16 07:19:27.153747: step 46150, loss = 0.22, batch loss = 0.16 (28.3 examples/sec; 0.283 sec/batch; 22h:30m:36s remains)
INFO - root - 2017-12-16 07:19:29.946334: step 46160, loss = 0.21, batch loss = 0.16 (28.4 examples/sec; 0.281 sec/batch; 22h:22m:06s remains)
INFO - root - 2017-12-16 07:19:32.782599: step 46170, loss = 0.36, batch loss = 0.30 (26.4 examples/sec; 0.303 sec/batch; 24h:07m:44s remains)
INFO - root - 2017-12-16 07:19:35.578073: step 46180, loss = 0.27, batch loss = 0.22 (28.6 examples/sec; 0.280 sec/batch; 22h:17m:07s remains)
INFO - root - 2017-12-16 07:19:38.399994: step 46190, loss = 0.30, batch loss = 0.24 (27.7 examples/sec; 0.289 sec/batch; 22h:59m:39s remains)
INFO - root - 2017-12-16 07:19:41.203387: step 46200, loss = 0.27, batch loss = 0.21 (29.7 examples/sec; 0.269 sec/batch; 21h:23m:22s remains)
2017-12-16 07:19:41.661331: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0017142 -2.9088225 -2.8080862 -2.8138943 -3.057693 -3.4240789 -3.8361957 -4.3705721 -4.7348533 -4.93768 -4.8860049 -4.5396624 -3.912384 -3.168514 -2.8021178][-3.4119315 -3.0200281 -2.8844419 -2.7474947 -2.7477078 -3.3027453 -3.9552724 -4.4338 -4.6570315 -4.9077954 -4.8530369 -4.5259809 -3.9834752 -3.2790291 -2.9729567][-3.7856541 -3.486141 -3.1810541 -2.893297 -2.963223 -3.2689862 -3.7968812 -4.6146569 -4.7442474 -4.6601934 -4.6128273 -4.5350246 -4.1791377 -3.594347 -3.3093231][-4.1816969 -3.6701746 -3.361433 -3.2444487 -3.31353 -3.5728889 -3.8909266 -4.2698307 -4.3210735 -4.201879 -3.9728944 -4.0823817 -4.1910157 -3.8071685 -3.542505][-4.2916412 -3.5650835 -3.1676033 -3.0788219 -3.2501481 -3.4694543 -3.6626358 -4.0125413 -3.825623 -3.5720217 -3.5132844 -3.7653971 -3.9479604 -3.820653 -3.6736794][-3.763602 -3.0936298 -2.9038029 -2.905128 -3.3076031 -3.4939158 -3.4231412 -3.4309342 -3.0317292 -2.8159537 -2.8391857 -3.5594652 -4.022985 -3.8684945 -3.692296][-2.645932 -2.1502397 -2.114599 -2.6230061 -3.1986566 -3.3648705 -3.2210922 -2.9124541 -2.3646181 -2.2483289 -2.5118279 -3.185082 -3.6839724 -3.8743286 -3.6866245][-1.529736 -1.0175939 -1.1479797 -1.8867278 -2.5852222 -2.8165619 -2.5733154 -2.1737962 -1.8149002 -1.8054657 -2.2639647 -3.0577292 -3.6193092 -3.6802425 -3.6029737][-0.66011405 -0.47286153 -0.72117734 -1.4040377 -2.1746533 -2.476522 -2.2994621 -1.9796774 -1.7770324 -1.9071043 -2.3043296 -3.0993838 -3.7226586 -3.8346491 -3.7224936][-0.30450678 -0.15680695 -0.46319652 -1.2767785 -1.9046721 -2.2304866 -2.1956599 -1.838702 -1.6350093 -1.9464808 -2.4950457 -3.1035166 -3.6246734 -3.9719598 -3.8991199][-0.45795298 -0.50988126 -0.7693429 -1.2674792 -1.8071189 -2.0976188 -2.1444461 -2.0349622 -2.1952233 -2.6788282 -3.2334828 -3.74174 -4.1236382 -4.1618023 -3.964407][-1.1362333 -0.93789172 -1.0263326 -1.3049729 -1.6020501 -2.006362 -2.359153 -2.4922335 -2.8502197 -3.4415755 -3.9678996 -4.3172469 -4.4933205 -4.3517413 -3.9725327][-1.7667179 -1.5656891 -1.3145785 -1.4481196 -1.7993536 -2.2511716 -2.7395477 -3.1954629 -3.5398183 -4.0216522 -4.4129958 -4.6151295 -4.5200915 -4.1104794 -3.7093041][-2.4083104 -1.8457882 -1.4938097 -1.3596048 -1.5539274 -2.099508 -2.712121 -3.3141022 -3.960284 -4.4943848 -4.8514705 -4.8719192 -4.5690689 -3.9687996 -3.4372535][-2.9044504 -2.0401332 -1.5050378 -1.3967388 -1.5843635 -1.9760125 -2.4792094 -3.1697702 -3.8056328 -4.3285565 -4.5895619 -4.45689 -4.0608439 -3.375432 -2.8527391]]...]
INFO - root - 2017-12-16 07:19:44.515859: step 46210, loss = 0.45, batch loss = 0.39 (27.4 examples/sec; 0.292 sec/batch; 23h:11m:19s remains)
INFO - root - 2017-12-16 07:19:47.325186: step 46220, loss = 0.35, batch loss = 0.29 (29.3 examples/sec; 0.273 sec/batch; 21h:44m:14s remains)
INFO - root - 2017-12-16 07:19:50.166203: step 46230, loss = 0.35, batch loss = 0.29 (28.5 examples/sec; 0.281 sec/batch; 22h:21m:18s remains)
INFO - root - 2017-12-16 07:19:52.977407: step 46240, loss = 0.34, batch loss = 0.28 (27.8 examples/sec; 0.288 sec/batch; 22h:55m:15s remains)
INFO - root - 2017-12-16 07:19:55.807789: step 46250, loss = 0.42, batch loss = 0.36 (28.0 examples/sec; 0.286 sec/batch; 22h:42m:06s remains)
INFO - root - 2017-12-16 07:19:58.661962: step 46260, loss = 0.29, batch loss = 0.23 (29.0 examples/sec; 0.276 sec/batch; 21h:56m:25s remains)
INFO - root - 2017-12-16 07:20:01.444218: step 46270, loss = 0.23, batch loss = 0.17 (29.2 examples/sec; 0.274 sec/batch; 21h:49m:08s remains)
INFO - root - 2017-12-16 07:20:04.291684: step 46280, loss = 0.25, batch loss = 0.19 (29.2 examples/sec; 0.274 sec/batch; 21h:45m:46s remains)
INFO - root - 2017-12-16 07:20:07.136559: step 46290, loss = 0.29, batch loss = 0.24 (29.2 examples/sec; 0.274 sec/batch; 21h:44m:55s remains)
INFO - root - 2017-12-16 07:20:09.966649: step 46300, loss = 0.29, batch loss = 0.23 (28.7 examples/sec; 0.279 sec/batch; 22h:11m:11s remains)
2017-12-16 07:20:10.424046: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.11507 -7.0820427 -6.85814 -6.6755323 -6.5073686 -6.5455322 -6.6224203 -7.0471153 -7.3736906 -7.4393911 -7.3557911 -6.9353213 -6.2235084 -5.2150831 -4.2781215][-7.3864284 -7.3554955 -7.0699887 -6.8662624 -6.8615608 -7.185111 -7.5485659 -8.0686884 -8.4597387 -8.69416 -8.5455971 -8.0258579 -7.33097 -6.2341833 -5.1665621][-6.7061844 -6.4471045 -6.077003 -5.8567619 -5.84917 -6.2797985 -6.8349857 -7.80574 -8.4543676 -8.89894 -8.8739719 -8.5626268 -8.1479759 -7.2040291 -6.1475458][-5.3106008 -4.5906305 -3.895946 -3.5629056 -3.5744681 -4.08295 -4.5582643 -5.5388165 -6.4204335 -7.3412914 -7.9737253 -8.0875473 -8.0846128 -7.645586 -6.7814665][-3.5141866 -2.3076084 -0.98040223 -0.41095114 -0.29914188 -0.76618004 -1.5638416 -2.5529377 -3.407331 -4.3442793 -5.5280795 -6.5879526 -7.2055387 -7.2127514 -6.8204365][-2.4150057 -0.85986042 0.739799 2.0448856 2.7590508 2.4703603 1.9392691 1.0402184 0.070033073 -1.0911219 -2.7038927 -4.4755931 -5.9610577 -6.6271229 -6.4679794][-2.7430606 -0.88399053 0.93513012 2.3665624 3.6104984 4.055088 4.0914822 3.5443234 2.8862166 1.6478944 -0.28966665 -2.6556957 -4.6368728 -5.6029158 -5.635849][-3.4505348 -1.6821208 0.12577438 1.8296857 3.3785048 4.1925507 4.6931887 4.3355665 3.815299 2.532578 0.40849447 -1.9111383 -3.75375 -4.8399544 -4.9739704][-4.7684155 -3.2349157 -1.6519153 -0.24781179 1.4402356 2.7494726 3.6391535 3.6838684 3.3806729 2.3774929 0.5928688 -1.6812942 -3.5339241 -4.4833817 -4.5941548][-5.9692135 -5.0123062 -3.9236517 -2.7073121 -1.2416751 0.080708981 1.1158233 1.5571532 1.4616208 0.59671974 -0.81190729 -2.6438498 -4.11947 -4.8076329 -4.8159256][-6.5593033 -5.8978353 -5.1340494 -4.296524 -3.263998 -2.062222 -1.0258303 -0.64705706 -0.78773165 -1.362993 -2.3404298 -3.6440744 -4.6994724 -5.1702194 -5.0858469][-6.2085857 -5.9460497 -5.6266794 -5.0029964 -4.2921977 -3.5730305 -2.8939776 -2.6100736 -2.6323545 -3.1218553 -3.7136149 -4.4770036 -5.03346 -5.1244192 -4.9167237][-5.0851932 -4.8757534 -4.7548523 -4.4917436 -4.2583084 -3.9482379 -3.6726685 -3.6526394 -3.6703131 -3.9465864 -4.1604233 -4.6071777 -5.0051374 -5.05149 -4.7466569][-3.7644911 -3.6433764 -3.5707116 -3.4632607 -3.4877441 -3.3460112 -3.3144181 -3.4459567 -3.6236384 -3.8724184 -3.9455688 -4.0895271 -4.2008295 -4.127707 -3.9254873][-2.7312675 -2.494915 -2.3817596 -2.4110725 -2.4736097 -2.5397213 -2.6577997 -2.7377248 -2.8522499 -3.0055697 -3.116797 -3.0621157 -2.9649553 -2.93627 -2.8452752]]...]
INFO - root - 2017-12-16 07:20:13.219963: step 46310, loss = 0.32, batch loss = 0.26 (29.0 examples/sec; 0.276 sec/batch; 21h:55m:00s remains)
INFO - root - 2017-12-16 07:20:16.040709: step 46320, loss = 0.24, batch loss = 0.18 (29.6 examples/sec; 0.271 sec/batch; 21h:31m:03s remains)
INFO - root - 2017-12-16 07:20:18.918125: step 46330, loss = 0.35, batch loss = 0.29 (28.9 examples/sec; 0.277 sec/batch; 22h:00m:08s remains)
INFO - root - 2017-12-16 07:20:21.770692: step 46340, loss = 0.23, batch loss = 0.18 (26.5 examples/sec; 0.302 sec/batch; 24h:02m:24s remains)
INFO - root - 2017-12-16 07:20:24.631397: step 46350, loss = 0.21, batch loss = 0.15 (28.5 examples/sec; 0.281 sec/batch; 22h:20m:40s remains)
INFO - root - 2017-12-16 07:20:27.497376: step 46360, loss = 0.35, batch loss = 0.29 (27.7 examples/sec; 0.289 sec/batch; 22h:58m:07s remains)
INFO - root - 2017-12-16 07:20:30.291474: step 46370, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 22h:43m:51s remains)
INFO - root - 2017-12-16 07:20:33.147073: step 46380, loss = 0.24, batch loss = 0.19 (27.5 examples/sec; 0.291 sec/batch; 23h:08m:25s remains)
INFO - root - 2017-12-16 07:20:35.985990: step 46390, loss = 0.21, batch loss = 0.15 (28.4 examples/sec; 0.282 sec/batch; 22h:25m:02s remains)
INFO - root - 2017-12-16 07:20:38.850433: step 46400, loss = 0.20, batch loss = 0.14 (28.1 examples/sec; 0.284 sec/batch; 22h:36m:13s remains)
2017-12-16 07:20:39.313732: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8450937 -4.359261 -4.0088763 -4.1170812 -4.5860076 -4.8720531 -4.9333234 -4.9831805 -4.9630308 -4.8809781 -4.800642 -4.6798735 -4.55431 -4.447762 -4.4099236][-4.7853031 -4.0662603 -3.623925 -3.7256029 -4.087584 -4.6855712 -4.7991033 -4.6973934 -4.6143041 -4.6414838 -4.6516023 -4.5002036 -4.3886461 -4.3512859 -4.1603227][-4.5260673 -3.6926742 -3.0966306 -2.9553335 -3.2293446 -3.8504043 -4.1794796 -4.2132363 -4.0621514 -4.0189962 -4.1677523 -4.3096786 -4.3190985 -4.3761334 -4.168364][-3.7135336 -2.6509302 -1.7562742 -1.6670604 -2.1893358 -2.5487981 -2.8479743 -2.9304552 -2.9723215 -3.0805912 -3.3115234 -3.7589622 -4.1069345 -4.3453383 -4.3270903][-3.2999654 -1.8956296 -0.63227081 -0.19340086 -0.38531923 -0.74465156 -1.0913587 -1.1920207 -1.3321009 -1.6991765 -2.2062888 -2.7901688 -3.4528322 -3.9791892 -4.0719051][-3.1623137 -1.6571076 -0.32702827 0.66150379 1.0422187 0.77807713 0.61890888 0.62271452 0.45785713 -0.12309313 -1.0620973 -2.0198498 -2.8581073 -3.5348911 -3.8587341][-3.4119172 -1.8824439 -0.6620934 0.37237883 1.0361466 1.4344053 1.7313809 1.8450789 1.7333136 0.998651 -0.057193756 -1.3672707 -2.4116871 -3.1958885 -3.7011137][-4.0934448 -2.7563725 -1.4754598 -0.41932869 0.27785921 1.0875368 1.7774806 2.1441088 2.0737886 1.2987695 0.24129152 -0.98466682 -2.1081219 -3.0753529 -3.7623081][-5.2678 -4.3625178 -3.1521263 -1.9200583 -0.81325221 0.18045378 1.0608935 1.5516667 1.578001 0.88518333 -0.13110352 -1.3427951 -2.4018824 -3.1431694 -3.8121514][-6.3111725 -5.9290085 -5.0478187 -4.0053768 -2.5462713 -1.3590636 -0.30291605 0.19798326 0.095509052 -0.61128473 -1.4491222 -2.4310908 -3.2061145 -3.6324506 -3.9663007][-7.5694857 -7.1305027 -6.492341 -5.6927114 -4.3898168 -3.0707159 -1.8191149 -1.4536126 -1.7214944 -2.4259918 -3.1867094 -3.9255013 -4.4537024 -4.6584525 -4.6193004][-8.0915985 -7.6955013 -7.3047619 -6.6097221 -5.5890055 -4.6682539 -3.7085419 -3.3073425 -3.4146585 -3.8893168 -4.4990168 -5.0206165 -5.1600204 -4.9715028 -4.7430096][-7.900774 -7.1952696 -6.7728524 -6.287199 -5.6602092 -5.1444964 -4.6093698 -4.480031 -4.4701848 -4.6689882 -4.94881 -5.187881 -5.3303285 -5.0011115 -4.603672][-7.509778 -6.4990563 -5.8580575 -5.298831 -5.0830421 -4.8494277 -4.7195477 -4.7961822 -4.8030057 -4.7753735 -4.7493229 -4.8789825 -4.8078856 -4.5063939 -4.0751529][-7.614975 -6.140913 -5.0126357 -4.4153175 -4.26958 -4.4239521 -4.6375451 -4.7774162 -4.8877549 -4.8063126 -4.6271787 -4.3619356 -3.974838 -3.7213135 -3.3608897]]...]
INFO - root - 2017-12-16 07:20:42.152779: step 46410, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.284 sec/batch; 22h:35m:01s remains)
INFO - root - 2017-12-16 07:20:44.978149: step 46420, loss = 0.21, batch loss = 0.15 (28.5 examples/sec; 0.280 sec/batch; 22h:16m:14s remains)
INFO - root - 2017-12-16 07:20:47.832522: step 46430, loss = 0.27, batch loss = 0.21 (30.0 examples/sec; 0.267 sec/batch; 21h:12m:13s remains)
INFO - root - 2017-12-16 07:20:50.692782: step 46440, loss = 0.27, batch loss = 0.21 (26.4 examples/sec; 0.303 sec/batch; 24h:02m:19s remains)
INFO - root - 2017-12-16 07:20:53.509596: step 46450, loss = 0.24, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 22h:27m:37s remains)
INFO - root - 2017-12-16 07:20:56.382841: step 46460, loss = 0.30, batch loss = 0.24 (28.9 examples/sec; 0.277 sec/batch; 22h:01m:04s remains)
INFO - root - 2017-12-16 07:20:59.205460: step 46470, loss = 0.25, batch loss = 0.20 (29.7 examples/sec; 0.269 sec/batch; 21h:23m:23s remains)
INFO - root - 2017-12-16 07:21:01.988756: step 46480, loss = 0.39, batch loss = 0.33 (29.2 examples/sec; 0.274 sec/batch; 21h:46m:06s remains)
INFO - root - 2017-12-16 07:21:04.854331: step 46490, loss = 0.38, batch loss = 0.32 (25.5 examples/sec; 0.313 sec/batch; 24h:54m:06s remains)
INFO - root - 2017-12-16 07:21:07.714561: step 46500, loss = 0.21, batch loss = 0.15 (27.2 examples/sec; 0.294 sec/batch; 23h:20m:11s remains)
2017-12-16 07:21:08.179535: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1908269 -3.1885753 -2.3802574 -2.427623 -2.7642195 -3.1630669 -3.3671153 -3.4443896 -3.5988412 -3.6236806 -3.70926 -3.8303945 -3.8191543 -3.6132574 -3.2538698][-3.9134731 -2.7948403 -1.9413431 -1.9538138 -2.3173792 -2.69977 -2.9019604 -3.0856133 -3.3102741 -3.4539595 -3.6488056 -3.7485762 -3.7275484 -3.3019886 -2.7961302][-3.4546988 -2.3191035 -1.4993048 -1.2642326 -1.2736697 -1.7695045 -2.2197921 -2.4151928 -2.6063871 -2.9593248 -3.362216 -3.4461112 -3.2912776 -2.8897164 -2.6180992][-3.116574 -1.9580519 -1.0604348 -0.74493289 -0.7088294 -0.825439 -0.91196895 -1.1907341 -1.6214693 -2.0851784 -2.5166998 -2.9760804 -3.1388824 -2.7273996 -2.3508651][-2.4233162 -1.3405402 -0.53040695 -0.21751595 0.050923824 0.22316217 0.2356658 0.034779072 -0.37763643 -1.2627039 -2.2613313 -2.8079147 -2.8876827 -2.8735576 -2.8346727][-1.8482361 -0.95386338 -0.23934126 0.31648207 1.0112915 1.4769678 1.6881218 1.5663238 0.93004751 -0.17140007 -1.4105201 -2.5507581 -3.2369928 -3.2810407 -2.9766917][-1.5152192 -0.97271228 -0.45519161 0.27891302 1.2348289 2.1139188 2.7260871 2.8349152 2.3288369 0.95954275 -0.877388 -2.4037657 -3.1857238 -3.3150778 -3.1668286][-1.3107624 -1.0360491 -0.77241182 -0.19264984 0.88085175 2.0854602 2.7578249 2.917788 2.4476085 0.98961496 -0.96499872 -2.813117 -3.9926348 -4.2811012 -3.8520033][-1.2717245 -1.2470024 -1.2823794 -0.971596 -0.17859888 1.0072813 1.8582029 2.1873102 1.6462464 0.17354965 -1.8892355 -3.9586458 -5.1906414 -5.4447918 -4.7165909][-1.3690841 -1.5506284 -2.0491061 -2.1335821 -1.6525612 -0.86350274 -0.31917667 0.24592543 0.085207939 -1.2307942 -3.1698167 -5.1697989 -6.259367 -6.3244133 -5.5551219][-1.4639351 -1.8696506 -2.5514684 -3.00262 -3.1682057 -2.6011715 -2.0135896 -1.734782 -1.8663273 -2.8531947 -4.4986844 -6.2344394 -7.1876841 -6.9722085 -5.9155059][-1.7238724 -1.8959904 -2.7680073 -3.4549422 -3.735121 -3.5965919 -3.4532542 -3.3100982 -3.4289217 -4.13546 -5.4612083 -7.01188 -7.7351532 -7.3829083 -6.3021407][-2.0961509 -2.0384617 -2.5404396 -3.2321129 -3.8486748 -3.9369681 -3.9087982 -3.9513841 -4.3457627 -5.0756674 -6.119978 -7.1934347 -7.5749917 -7.2091551 -6.2182918][-2.4036658 -2.1239152 -2.4209232 -2.7360272 -3.0573874 -3.3545325 -3.5840847 -3.7458656 -4.2720828 -5.0369081 -5.9380856 -6.7965708 -7.0861974 -6.7843418 -5.8837357][-2.7271237 -1.8737423 -1.5052655 -1.6766942 -2.1402059 -2.3590105 -2.6450086 -3.0823417 -3.6712108 -4.529788 -5.6888342 -6.6169968 -6.9968915 -6.8789959 -5.8140969]]...]
INFO - root - 2017-12-16 07:21:10.987087: step 46510, loss = 0.22, batch loss = 0.16 (28.5 examples/sec; 0.280 sec/batch; 22h:15m:42s remains)
INFO - root - 2017-12-16 07:21:13.771874: step 46520, loss = 0.23, batch loss = 0.17 (28.6 examples/sec; 0.279 sec/batch; 22h:11m:47s remains)
INFO - root - 2017-12-16 07:21:16.654729: step 46530, loss = 0.22, batch loss = 0.16 (28.5 examples/sec; 0.281 sec/batch; 22h:18m:08s remains)
INFO - root - 2017-12-16 07:21:19.468230: step 46540, loss = 0.25, batch loss = 0.19 (27.9 examples/sec; 0.287 sec/batch; 22h:48m:36s remains)
INFO - root - 2017-12-16 07:21:22.258984: step 46550, loss = 0.25, batch loss = 0.19 (28.8 examples/sec; 0.278 sec/batch; 22h:03m:44s remains)
INFO - root - 2017-12-16 07:21:25.116641: step 46560, loss = 0.31, batch loss = 0.26 (29.3 examples/sec; 0.273 sec/batch; 21h:40m:10s remains)
INFO - root - 2017-12-16 07:21:27.935560: step 46570, loss = 0.34, batch loss = 0.28 (28.3 examples/sec; 0.283 sec/batch; 22h:28m:35s remains)
INFO - root - 2017-12-16 07:21:30.772216: step 46580, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 22h:42m:12s remains)
INFO - root - 2017-12-16 07:21:33.522006: step 46590, loss = 0.17, batch loss = 0.11 (28.7 examples/sec; 0.279 sec/batch; 22h:07m:27s remains)
INFO - root - 2017-12-16 07:21:36.358564: step 46600, loss = 0.25, batch loss = 0.19 (29.5 examples/sec; 0.272 sec/batch; 21h:34m:17s remains)
2017-12-16 07:21:36.840688: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8436246 -4.8047447 -5.0660648 -5.7856436 -6.5107155 -6.8900118 -7.2831221 -7.280302 -7.1129427 -6.5206466 -5.7815447 -5.0263214 -4.02463 -3.2460005 -2.6517715][-4.2105789 -4.1900992 -4.5864158 -5.1189947 -5.4880285 -5.5760555 -5.8122964 -5.9306321 -5.9205685 -5.579452 -5.1377158 -4.5039887 -3.6403241 -2.7898965 -2.1411233][-3.4227514 -3.3547034 -3.6295412 -4.1190777 -4.3745818 -4.35048 -4.5624237 -4.4699721 -4.5003872 -4.7305279 -4.8091063 -4.4180532 -3.6251674 -2.8751109 -2.1195967][-2.462491 -2.2864468 -2.4530678 -2.5617895 -2.4745462 -2.3934202 -2.4644535 -2.7260339 -3.1498477 -3.5616798 -3.9824507 -4.2623816 -4.0310149 -3.2840238 -2.3791194][-1.4110794 -1.2413294 -1.4870946 -1.4822025 -1.2737095 -0.92614961 -0.73067021 -0.74752212 -1.088532 -2.1683202 -3.3021336 -3.8795352 -3.8856637 -3.6792164 -3.0425534][-1.107265 -0.52918911 -0.40565348 -0.059810638 0.6159296 1.1722298 1.5267391 1.3850508 0.60796165 -0.6664536 -2.0952942 -3.3889761 -4.016377 -3.8457091 -3.144186][-1.1500027 -0.23043776 0.14534855 0.84597635 1.7765808 2.6818528 3.4891973 3.3966708 2.5717769 0.71547842 -1.2984049 -2.8679533 -3.7768884 -3.9735112 -3.580081][-1.275851 -0.4673903 -0.15030956 0.7101965 1.7262769 2.6186938 3.3154445 3.2984619 2.6791916 0.93870592 -1.1077902 -2.8434567 -3.8793039 -4.2898507 -4.0273552][-2.0888147 -1.2436185 -0.95296144 -0.37375593 0.42081404 1.4728036 2.2506247 2.2578506 1.6200933 0.011853218 -1.7944114 -3.3054292 -4.1957817 -4.5248003 -4.3435822][-2.8415251 -2.4245226 -2.5962677 -2.2019153 -1.5187185 -0.82006097 -0.25860691 -0.04137516 -0.29285002 -1.3627863 -2.740766 -3.988565 -4.6895952 -4.7900014 -4.5482116][-4.7407422 -4.01274 -3.8554873 -4.1261883 -4.2002625 -3.8107681 -3.511389 -3.4095821 -3.3656669 -3.84995 -4.5871639 -5.2242064 -5.5499425 -5.4860611 -5.2100248][-5.6281319 -5.5668812 -5.9479518 -5.9749484 -5.8294954 -5.973917 -6.1752186 -6.0583081 -5.86511 -5.9578438 -6.1031137 -6.3583651 -6.4234934 -6.2088137 -5.7551508][-6.1026754 -6.1075048 -6.4461541 -7.0319815 -7.4748993 -7.453845 -7.4912667 -7.5108595 -7.3606634 -7.1402345 -6.8692503 -6.6913538 -6.48744 -6.2482119 -5.7945657][-5.629168 -5.551487 -5.7217703 -6.1372623 -6.6296272 -7.1406336 -7.6197376 -7.5658274 -7.3356962 -6.995913 -6.5604334 -6.2925882 -6.1043863 -5.8792305 -5.6206403][-5.2513208 -5.1128058 -5.1462584 -5.3381863 -5.6325774 -6.0702672 -6.489295 -6.4910069 -6.2739153 -6.0637913 -5.8644156 -5.5959063 -5.3686209 -5.3286195 -5.2337012]]...]
INFO - root - 2017-12-16 07:21:39.687525: step 46610, loss = 0.30, batch loss = 0.24 (27.5 examples/sec; 0.290 sec/batch; 23h:03m:38s remains)
INFO - root - 2017-12-16 07:21:42.443412: step 46620, loss = 0.43, batch loss = 0.37 (29.3 examples/sec; 0.273 sec/batch; 21h:40m:22s remains)
INFO - root - 2017-12-16 07:21:45.265784: step 46630, loss = 0.20, batch loss = 0.14 (28.5 examples/sec; 0.280 sec/batch; 22h:16m:11s remains)
INFO - root - 2017-12-16 07:21:48.102639: step 46640, loss = 0.24, batch loss = 0.18 (27.2 examples/sec; 0.294 sec/batch; 23h:20m:11s remains)
INFO - root - 2017-12-16 07:21:50.948051: step 46650, loss = 0.23, batch loss = 0.17 (26.6 examples/sec; 0.301 sec/batch; 23h:52m:34s remains)
INFO - root - 2017-12-16 07:21:53.767033: step 46660, loss = 0.29, batch loss = 0.23 (28.2 examples/sec; 0.284 sec/batch; 22h:33m:10s remains)
INFO - root - 2017-12-16 07:21:56.582893: step 46670, loss = 0.34, batch loss = 0.28 (28.8 examples/sec; 0.278 sec/batch; 22h:04m:49s remains)
INFO - root - 2017-12-16 07:21:59.425172: step 46680, loss = 0.22, batch loss = 0.16 (27.5 examples/sec; 0.291 sec/batch; 23h:07m:09s remains)
INFO - root - 2017-12-16 07:22:02.250914: step 46690, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.285 sec/batch; 22h:38m:34s remains)
INFO - root - 2017-12-16 07:22:05.089619: step 46700, loss = 0.39, batch loss = 0.33 (28.7 examples/sec; 0.278 sec/batch; 22h:05m:34s remains)
2017-12-16 07:22:05.561785: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9459057 -4.0955229 -4.3711572 -4.9782538 -5.6371565 -6.0257025 -6.1411037 -6.353579 -6.6853237 -6.8263741 -6.6062336 -6.1229162 -5.3419728 -4.298821 -3.2450352][-2.8930125 -3.3018031 -3.8527327 -4.3872023 -4.9547482 -5.7767267 -6.3547459 -6.1884522 -6.0885444 -6.2606897 -6.3490644 -5.8420563 -4.91384 -4.017241 -3.0751033][-2.1321132 -2.3607261 -2.9371071 -3.867295 -4.4415593 -4.6753531 -4.745245 -4.8529673 -4.8475418 -4.5839095 -4.4238381 -4.2456675 -3.7407212 -3.059958 -2.5653083][-2.0643249 -2.3141224 -2.7521429 -3.3598256 -3.774523 -3.5883439 -3.1814623 -2.6700163 -2.2249846 -2.0245142 -2.1365433 -2.3753576 -2.528615 -2.4853692 -2.3758776][-2.4909937 -2.7512841 -3.2544994 -3.3443308 -3.1366973 -2.4220173 -1.6151953 -0.6866107 -0.017935753 0.014154911 -0.4166503 -1.0553586 -1.3792553 -2.0426521 -2.5104842][-2.845335 -2.9247971 -3.0169582 -2.7431026 -1.9131749 -0.68548417 0.46586704 1.5123515 2.1623554 2.1333241 1.4051089 0.22262955 -0.77562809 -1.7314236 -2.4633341][-3.4018769 -3.3303905 -3.0428748 -2.2743864 -1.0579903 0.64731455 2.0300779 2.9602942 3.2981119 3.1048064 2.3223886 1.1198025 -0.091621876 -1.4037611 -2.4823213][-3.7373002 -3.4346318 -2.9406176 -1.8595216 -0.28098917 1.4447813 2.7309403 3.4873424 3.8635588 3.631773 2.9231973 1.8949041 0.8191843 -0.59948349 -2.0497][-3.42555 -3.0782504 -2.5264552 -1.4198778 0.096025467 1.5637364 2.6237803 3.2527199 3.4305439 3.3214006 3.0105448 2.236167 1.1931219 -0.24927092 -1.7759635][-2.8736053 -2.6409259 -2.1710012 -1.4038434 -0.26595592 0.93285704 1.6394539 2.0257568 2.24195 2.3258219 2.3330598 1.8917899 1.1441112 -0.14509392 -1.5867832][-2.4084206 -2.2162967 -2.1137679 -1.6652317 -1.0188463 -0.14990664 0.45440578 0.76447916 0.93612766 1.2386942 1.4513025 1.2695599 0.74298143 -0.268795 -1.5747077][-2.4772205 -2.489975 -2.3672223 -2.1532321 -1.8289938 -1.3756256 -1.1439536 -0.82538271 -0.43634295 -0.067729473 0.16787291 0.21387768 -0.026335716 -0.72137284 -1.7136731][-3.0008175 -3.0402918 -3.1167321 -3.0691013 -2.9743748 -2.63125 -2.3466585 -2.1777053 -1.985631 -1.6540301 -1.2636299 -1.16201 -1.263202 -1.6496444 -2.3222787][-3.6757386 -3.615103 -3.5735042 -3.5423803 -3.5885937 -3.5206118 -3.5487144 -3.426908 -3.1687646 -2.9223638 -2.7171745 -2.4784698 -2.2609921 -2.4267457 -2.8893757][-4.070549 -4.1065869 -4.0733647 -4.0210166 -3.9917889 -3.9278851 -4.0272493 -4.175458 -4.2581186 -4.0644689 -3.6921287 -3.3562384 -3.0001135 -2.9278507 -3.1734538]]...]
INFO - root - 2017-12-16 07:22:08.393144: step 46710, loss = 0.26, batch loss = 0.20 (27.4 examples/sec; 0.292 sec/batch; 23h:10m:29s remains)
INFO - root - 2017-12-16 07:22:11.292460: step 46720, loss = 0.28, batch loss = 0.23 (28.7 examples/sec; 0.279 sec/batch; 22h:09m:05s remains)
INFO - root - 2017-12-16 07:22:14.158067: step 46730, loss = 0.28, batch loss = 0.22 (28.7 examples/sec; 0.279 sec/batch; 22h:06m:34s remains)
INFO - root - 2017-12-16 07:22:16.997156: step 46740, loss = 0.27, batch loss = 0.21 (25.5 examples/sec; 0.314 sec/batch; 24h:54m:17s remains)
INFO - root - 2017-12-16 07:22:19.839225: step 46750, loss = 0.29, batch loss = 0.23 (28.0 examples/sec; 0.286 sec/batch; 22h:40m:15s remains)
INFO - root - 2017-12-16 07:22:22.667822: step 46760, loss = 0.30, batch loss = 0.24 (29.1 examples/sec; 0.275 sec/batch; 21h:51m:00s remains)
INFO - root - 2017-12-16 07:22:25.537596: step 46770, loss = 0.36, batch loss = 0.30 (29.4 examples/sec; 0.272 sec/batch; 21h:34m:39s remains)
INFO - root - 2017-12-16 07:22:28.401131: step 46780, loss = 0.24, batch loss = 0.19 (27.0 examples/sec; 0.296 sec/batch; 23h:29m:23s remains)
INFO - root - 2017-12-16 07:22:31.206100: step 46790, loss = 0.27, batch loss = 0.21 (28.4 examples/sec; 0.282 sec/batch; 22h:22m:06s remains)
INFO - root - 2017-12-16 07:22:34.008739: step 46800, loss = 0.29, batch loss = 0.23 (29.0 examples/sec; 0.276 sec/batch; 21h:54m:46s remains)
2017-12-16 07:22:34.468517: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3612607 -1.3857858 -1.683991 -1.924659 -2.238728 -2.5640984 -2.9379621 -2.8730812 -2.7685142 -2.8883803 -3.0716929 -3.6219125 -4.0883036 -4.6029391 -5.0651321][-1.8861375 -2.0304048 -2.4010489 -2.8712373 -3.3745079 -3.6313667 -3.9199574 -4.0387707 -4.202 -4.1731162 -4.242384 -4.5964904 -4.9412408 -5.3561778 -5.4558754][-3.2086549 -3.3958654 -3.7238326 -4.0557284 -4.43309 -4.6535583 -4.8667 -5.0500927 -5.3223591 -5.5634642 -5.777514 -5.8981094 -6.1291351 -6.2635365 -6.0325193][-4.5461559 -4.6929469 -4.8048172 -4.8504143 -4.8194876 -4.643539 -4.4805579 -4.5058918 -4.7881203 -5.2556605 -5.6820035 -5.8858852 -6.124135 -6.2832508 -6.0502882][-5.3620319 -5.6703486 -5.7580671 -5.3482866 -4.6643705 -3.83619 -3.1277432 -2.72833 -2.7216759 -3.178102 -3.6996109 -4.3565168 -5.0853543 -5.430943 -5.4902463][-5.6314778 -5.619072 -5.4124584 -4.74702 -3.6977854 -2.3587937 -0.99209642 -0.27217674 -0.17123699 -0.49510527 -1.1348133 -1.9845212 -3.052758 -4.11734 -4.8006625][-4.7154193 -4.61963 -4.2798672 -3.4400046 -2.0428147 -0.37962341 1.3391914 2.4676747 2.9058428 2.5983157 1.6734748 0.38086128 -1.1613386 -2.5696585 -3.6873832][-3.5024755 -3.3864822 -2.8851655 -2.0832219 -0.96315503 0.70404625 2.7191176 4.1728363 4.9460812 4.8022051 3.7764416 2.2628303 0.3539257 -1.5792313 -3.0824661][-1.9521329 -2.0278559 -1.9062502 -1.2984102 -0.35405588 0.94521952 2.4993563 3.8160238 4.6768541 4.6932325 3.8456345 2.2890177 0.26342869 -1.585783 -3.2201123][-2.085166 -1.5473793 -1.1188326 -0.878639 -0.40305281 0.49871159 1.6271691 2.4395084 2.8846498 2.7897696 2.0506682 0.80047083 -0.879256 -2.418637 -3.7457922][-2.3362889 -1.6995034 -1.2917268 -0.89340711 -0.51830983 0.071674824 0.79446745 1.2085876 1.2791162 0.88247156 0.062398911 -0.95014024 -2.1501276 -3.1597819 -4.0839224][-3.0738432 -2.1678464 -1.5879984 -1.1917284 -0.93468142 -0.71736526 -0.46567702 -0.32122183 -0.42311335 -0.85700893 -1.4902558 -2.1762786 -3.0062995 -3.54244 -4.0685849][-3.8707967 -2.7916212 -2.0230989 -1.4922531 -1.315089 -1.2654114 -1.2837462 -1.3727267 -1.5207229 -1.8747065 -2.2957315 -2.604219 -2.9795716 -3.2322512 -3.5799179][-4.546195 -3.2240047 -2.1587274 -1.3840182 -1.0086288 -0.927279 -0.93373442 -1.0940428 -1.2791941 -1.4426188 -1.7068734 -2.005446 -2.2629981 -2.3899679 -2.7442617][-4.4653158 -3.1696956 -1.9916024 -1.1321526 -0.63197255 -0.36818218 -0.22159243 -0.17456961 -0.20957947 -0.35299826 -0.50587511 -0.72763634 -0.93464708 -1.2620285 -1.8914714]]...]
INFO - root - 2017-12-16 07:22:37.249331: step 46810, loss = 0.35, batch loss = 0.29 (29.5 examples/sec; 0.272 sec/batch; 21h:32m:57s remains)
INFO - root - 2017-12-16 07:22:40.123279: step 46820, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.285 sec/batch; 22h:35m:07s remains)
INFO - root - 2017-12-16 07:22:42.896174: step 46830, loss = 0.32, batch loss = 0.26 (29.5 examples/sec; 0.271 sec/batch; 21h:31m:29s remains)
INFO - root - 2017-12-16 07:22:45.780176: step 46840, loss = 0.20, batch loss = 0.14 (27.5 examples/sec; 0.291 sec/batch; 23h:05m:14s remains)
INFO - root - 2017-12-16 07:22:48.618714: step 46850, loss = 0.29, batch loss = 0.23 (26.2 examples/sec; 0.305 sec/batch; 24h:12m:05s remains)
INFO - root - 2017-12-16 07:22:51.441895: step 46860, loss = 0.24, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 21h:44m:25s remains)
INFO - root - 2017-12-16 07:22:54.295370: step 46870, loss = 0.35, batch loss = 0.29 (28.2 examples/sec; 0.284 sec/batch; 22h:32m:19s remains)
INFO - root - 2017-12-16 07:22:57.106021: step 46880, loss = 0.31, batch loss = 0.25 (29.0 examples/sec; 0.276 sec/batch; 21h:52m:04s remains)
INFO - root - 2017-12-16 07:22:59.921662: step 46890, loss = 0.24, batch loss = 0.18 (28.6 examples/sec; 0.280 sec/batch; 22h:11m:29s remains)
INFO - root - 2017-12-16 07:23:02.753726: step 46900, loss = 0.39, batch loss = 0.33 (27.1 examples/sec; 0.295 sec/batch; 23h:26m:33s remains)
2017-12-16 07:23:03.216018: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9547353 -2.9883466 -3.0783355 -3.1932564 -3.1431468 -3.0778427 -3.09584 -3.0053399 -2.9066005 -2.6778474 -2.3251395 -2.0161829 -1.7133179 -1.5673456 -1.5510895][-2.0856578 -2.3368 -2.4859581 -2.669719 -2.728924 -2.6406298 -2.5477319 -2.4572566 -2.4151053 -2.2475994 -2.0095825 -1.872992 -1.7658055 -1.5370159 -1.2296731][-1.7610869 -2.105366 -2.3615043 -2.3689702 -2.3093507 -2.12963 -1.8887303 -1.8211358 -1.8901043 -2.0595608 -2.173702 -2.3023078 -2.2659216 -1.9356909 -1.3794076][-1.7414749 -2.2176201 -2.5599976 -2.4959996 -2.1886582 -1.5115302 -0.94283366 -0.75632334 -0.97970366 -1.4953127 -2.1643 -2.8951719 -3.0327954 -2.6002281 -1.8644679][-2.029367 -2.4664693 -2.5730824 -2.4089448 -1.7710571 -0.769011 0.016667843 0.28662777 -0.0972271 -1.024298 -2.234437 -3.0741334 -3.5307426 -3.4515738 -2.7442226][-2.6608777 -2.8971913 -2.7187834 -1.9553211 -0.74278879 0.62409496 1.727891 1.9516826 1.2240124 -0.27390623 -2.0587344 -3.4968507 -4.3136654 -4.3400607 -3.6959507][-3.3489547 -3.1642115 -2.6254826 -1.4980323 0.14411402 2.074986 3.3925552 3.5297928 2.56425 0.768312 -1.3658724 -3.357728 -4.611763 -5.0131316 -4.5683427][-3.674494 -3.223434 -2.3207414 -0.82037473 0.99706221 2.9935431 4.3012714 4.3330297 3.2731657 1.279129 -1.1099694 -3.2097647 -4.5643721 -5.3178158 -5.1847467][-3.9247174 -3.2458253 -2.157645 -0.55789518 1.3265276 2.9690027 3.9769173 3.976861 2.7820387 0.68842745 -1.6973758 -3.7431922 -5.2388096 -6.0791969 -5.9166222][-4.2375717 -3.6966038 -2.6775787 -1.3490949 0.25927734 1.5680599 2.3465457 2.2444773 1.2280617 -0.63603187 -2.8510666 -4.7546372 -6.1555576 -6.8549461 -6.6659536][-4.2245226 -3.8444979 -3.2172856 -2.2316623 -1.0967326 -0.25382614 0.24398279 0.093521595 -0.73394346 -2.1686316 -3.8284538 -5.3504295 -6.4467859 -6.9732008 -6.7264977][-3.6486535 -3.5818067 -3.5173604 -3.0334516 -2.2243402 -1.6002474 -1.3170676 -1.3441794 -1.8946295 -2.9624944 -4.1798034 -5.2673078 -6.0431004 -6.587574 -6.4404612][-3.2399411 -3.5039883 -3.645123 -3.6427867 -3.2436597 -2.7420325 -2.2964907 -2.0731981 -2.4758587 -3.1424894 -3.9314384 -4.7062736 -5.2665992 -5.6703238 -5.6841555][-3.0097716 -3.4385676 -3.7352591 -3.8255897 -3.5499828 -3.1504831 -2.6653705 -2.1416576 -2.0184486 -2.3188174 -2.8221273 -3.3886375 -3.8292181 -4.2137165 -4.410996][-2.5478995 -3.1135077 -3.5774379 -3.6416769 -3.2548032 -2.5259051 -1.9076457 -1.2691796 -0.88959479 -0.848001 -1.211741 -1.7517989 -2.2020025 -2.7904749 -3.070116]]...]
INFO - root - 2017-12-16 07:23:06.059512: step 46910, loss = 0.30, batch loss = 0.24 (28.3 examples/sec; 0.283 sec/batch; 22h:27m:50s remains)
INFO - root - 2017-12-16 07:23:08.902123: step 46920, loss = 0.35, batch loss = 0.29 (26.5 examples/sec; 0.302 sec/batch; 23h:58m:24s remains)
INFO - root - 2017-12-16 07:23:11.700954: step 46930, loss = 0.27, batch loss = 0.21 (29.0 examples/sec; 0.275 sec/batch; 21h:51m:03s remains)
INFO - root - 2017-12-16 07:23:14.531101: step 46940, loss = 0.27, batch loss = 0.21 (29.0 examples/sec; 0.276 sec/batch; 21h:54m:02s remains)
INFO - root - 2017-12-16 07:23:17.345065: step 46950, loss = 0.24, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 22h:27m:35s remains)
INFO - root - 2017-12-16 07:23:20.151383: step 46960, loss = 0.31, batch loss = 0.25 (28.8 examples/sec; 0.278 sec/batch; 22h:03m:36s remains)
INFO - root - 2017-12-16 07:23:22.979538: step 46970, loss = 0.23, batch loss = 0.17 (27.9 examples/sec; 0.286 sec/batch; 22h:42m:18s remains)
INFO - root - 2017-12-16 07:23:25.830026: step 46980, loss = 0.36, batch loss = 0.30 (28.0 examples/sec; 0.286 sec/batch; 22h:39m:04s remains)
INFO - root - 2017-12-16 07:23:28.685055: step 46990, loss = 0.35, batch loss = 0.29 (28.5 examples/sec; 0.281 sec/batch; 22h:17m:42s remains)
INFO - root - 2017-12-16 07:23:31.518535: step 47000, loss = 0.27, batch loss = 0.21 (27.3 examples/sec; 0.293 sec/batch; 23h:13m:58s remains)
2017-12-16 07:23:31.979723: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2255855 -4.2996216 -4.3757496 -4.5292649 -4.8079734 -4.8160434 -4.8208776 -4.8147955 -4.6650066 -4.306303 -3.9438779 -3.6587563 -3.2942128 -3.0095279 -2.8581452][-4.5569396 -4.6457391 -4.7460203 -4.9513278 -5.1940985 -5.3046193 -5.3074389 -5.2930665 -5.1463027 -4.7821827 -4.3570595 -3.9186461 -3.4863622 -3.0953703 -2.8546638][-4.6718736 -4.7089829 -4.7090759 -4.8204455 -5.0343065 -5.1351328 -5.1223927 -5.3220081 -5.3357348 -5.1114235 -4.8265228 -4.4431648 -4.037394 -3.5659995 -3.178143][-4.0874014 -4.0619025 -4.0300422 -4.0605254 -4.1589561 -4.190589 -4.169405 -4.3403568 -4.441246 -4.4177728 -4.4181671 -4.4038172 -4.2256961 -3.8793645 -3.4642141][-3.3177505 -3.1222496 -2.8118038 -2.6625562 -2.5165339 -2.3372869 -2.3379421 -2.5516853 -2.9332905 -3.3979263 -3.9090366 -4.3270149 -4.3934226 -4.1872935 -3.729682][-2.028969 -1.7403955 -1.2941244 -0.74279571 -0.24978065 0.0840497 0.19279766 -0.17990828 -0.97146583 -2.0524447 -3.1968889 -4.1769624 -4.573751 -4.5032668 -4.0243583][-1.2686493 -0.77486634 -0.19480467 0.57014036 1.3652501 1.9394011 2.1979637 1.8261523 0.91675091 -0.4787786 -2.0655432 -3.472425 -4.3069067 -4.4698138 -4.124938][-0.86358476 -0.39177465 0.19570684 1.0429978 1.9595556 2.6900148 2.9249711 2.5680566 1.7651601 0.29687786 -1.401449 -2.8650148 -3.7592115 -4.06874 -3.9221327][-1.4811552 -1.1073728 -0.6525178 0.18971348 1.12075 2.0023065 2.4579835 2.2642045 1.6586571 0.27413464 -1.2618275 -2.7440376 -3.6214104 -3.8945074 -3.8398621][-2.2998018 -2.1040311 -1.8372307 -1.1207514 -0.25614882 0.47522926 0.83927631 0.80381823 0.4328084 -0.61147475 -1.7990272 -2.9464383 -3.6616075 -3.9349561 -3.8765023][-2.9895382 -2.8965945 -2.8134139 -2.5313601 -1.8675354 -1.171154 -0.80769849 -0.69777513 -0.75593519 -1.3128893 -2.2619934 -3.1502395 -3.6747093 -3.8622477 -3.8345902][-3.3446336 -3.3770311 -3.5640776 -3.5375166 -3.1752968 -2.6570387 -2.1570768 -1.9243498 -1.744112 -1.9352868 -2.5425544 -3.2501922 -3.7211423 -3.8389943 -3.8059196][-3.0807345 -3.0713139 -3.4377098 -3.5961156 -3.4990137 -3.2985904 -2.9767029 -2.7375031 -2.4943728 -2.5370364 -2.9171777 -3.3967323 -3.752748 -3.788667 -3.6738091][-2.3311164 -2.1817968 -2.4111609 -2.8590937 -3.0763929 -3.0632839 -2.954452 -2.834559 -2.6604514 -2.5932665 -2.9210472 -3.3472395 -3.6241934 -3.6833186 -3.5327997][-1.5207112 -1.1703897 -1.403322 -2.0237868 -2.4908352 -2.6969128 -2.5740168 -2.3782282 -2.2477102 -2.1939285 -2.5074348 -2.9994435 -3.3348618 -3.4143627 -3.2787619]]...]
INFO - root - 2017-12-16 07:23:34.892162: step 47010, loss = 0.27, batch loss = 0.22 (27.7 examples/sec; 0.288 sec/batch; 22h:52m:34s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 07:23:37.707730: step 47020, loss = 0.28, batch loss = 0.23 (29.1 examples/sec; 0.275 sec/batch; 21h:49m:53s remains)
INFO - root - 2017-12-16 07:23:40.578099: step 47030, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 22h:12m:29s remains)
INFO - root - 2017-12-16 07:23:43.378526: step 47040, loss = 0.24, batch loss = 0.18 (28.7 examples/sec; 0.279 sec/batch; 22h:08m:07s remains)
INFO - root - 2017-12-16 07:23:46.190876: step 47050, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.284 sec/batch; 22h:32m:02s remains)
INFO - root - 2017-12-16 07:23:49.043792: step 47060, loss = 0.37, batch loss = 0.31 (26.9 examples/sec; 0.297 sec/batch; 23h:32m:40s remains)
INFO - root - 2017-12-16 07:23:51.880399: step 47070, loss = 0.29, batch loss = 0.23 (28.5 examples/sec; 0.281 sec/batch; 22h:16m:48s remains)
INFO - root - 2017-12-16 07:23:54.723365: step 47080, loss = 0.35, batch loss = 0.29 (28.6 examples/sec; 0.280 sec/batch; 22h:10m:05s remains)
INFO - root - 2017-12-16 07:23:57.524829: step 47090, loss = 0.25, batch loss = 0.19 (27.0 examples/sec; 0.296 sec/batch; 23h:29m:51s remains)
INFO - root - 2017-12-16 07:24:00.340925: step 47100, loss = 0.29, batch loss = 0.23 (29.5 examples/sec; 0.271 sec/batch; 21h:30m:40s remains)
2017-12-16 07:24:00.813363: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6264906 -2.6651258 -2.6449859 -2.6476276 -2.7066722 -2.9991188 -3.3276052 -3.5974352 -3.9868479 -4.1665111 -4.173306 -4.1303959 -3.9564304 -3.7212567 -3.4716554][-2.2531316 -2.2912655 -2.3483348 -2.553103 -2.7560058 -3.005446 -3.4682369 -3.9062047 -4.2030649 -4.4362764 -4.6650567 -4.4139514 -4.1379433 -3.9734564 -3.7316935][-1.2158601 -1.5638664 -1.9129074 -2.2390189 -2.5668831 -3.0572515 -3.532069 -4.009768 -4.4836993 -4.735117 -4.8443618 -4.8258276 -4.6074424 -4.2465348 -3.9512033][-0.10897541 -0.65076637 -1.3013382 -1.805413 -2.1297426 -2.450943 -2.8786991 -3.1283383 -3.4480698 -4.0233121 -4.425262 -4.450213 -4.3929582 -4.267118 -3.960156][0.70580912 -0.016018391 -0.80171609 -1.477742 -1.9369264 -2.0996935 -2.1146569 -2.2923074 -2.6768928 -3.0383458 -3.5060546 -3.8582678 -3.953826 -3.9827175 -3.7620344][1.4679041 0.48890591 -0.51525617 -1.1549101 -1.2360675 -1.3047314 -1.3893132 -1.3617785 -1.6945703 -2.3218427 -2.9827738 -3.4345946 -3.6692097 -3.7331252 -3.6812725][1.0528407 0.24286985 -0.33192444 -0.59311891 -0.51159453 -0.19016123 0.090945244 0.053309441 -0.38396692 -1.2927897 -2.293149 -2.9159927 -3.2909417 -3.536026 -3.52573][0.23578596 -0.78082895 -1.1993091 -0.86017561 -0.14712763 0.53536892 1.0590281 1.1529803 0.73130751 -0.24114084 -1.3856525 -2.2757909 -2.814997 -3.119854 -3.27696][-1.9730968 -2.4964037 -2.3747673 -1.9411633 -0.85594296 0.15326452 0.81225491 1.1211247 0.74927616 -0.074754715 -0.9544723 -1.7064333 -2.3298523 -2.546339 -2.6285946][-3.4904099 -4.3195734 -4.1043477 -3.1077895 -1.8872266 -0.73661733 0.080813408 0.27463245 0.12807703 -0.41712284 -1.2999251 -1.8377993 -1.9847887 -1.9825668 -1.8490314][-4.91326 -5.3536925 -5.2065482 -4.4013515 -3.3031898 -2.3085024 -1.3488009 -0.84199 -0.92454863 -1.2885563 -1.7502959 -2.008007 -1.9749813 -1.721694 -1.4533186][-5.5031538 -6.2414713 -6.2964187 -5.9214725 -5.4327278 -4.4822168 -3.4495993 -2.5338726 -1.9592576 -1.7893305 -1.8091052 -1.7783747 -1.5415437 -1.2537639 -1.1498711][-5.56874 -6.4172497 -7.0619326 -7.2300711 -6.9916148 -6.2492871 -5.099 -3.63275 -2.3561141 -1.7073274 -1.6054714 -1.3337209 -1.0121515 -0.94845915 -0.93423915][-4.8269682 -5.7587018 -6.6430626 -7.2980909 -7.6776991 -7.0481615 -5.6815042 -4.1896257 -2.7718515 -1.624182 -1.2143016 -0.91918468 -0.45181751 -0.4414494 -0.76366282][-3.8096726 -4.7365341 -6.09757 -7.0932751 -7.49949 -7.4195423 -6.4168253 -4.6435318 -2.9898677 -1.9628015 -1.1517656 -0.53140879 -0.19828939 -0.15159655 -0.33227587]]...]
INFO - root - 2017-12-16 07:24:03.647381: step 47110, loss = 0.27, batch loss = 0.21 (28.1 examples/sec; 0.284 sec/batch; 22h:32m:35s remains)
INFO - root - 2017-12-16 07:24:06.445994: step 47120, loss = 0.34, batch loss = 0.28 (28.9 examples/sec; 0.277 sec/batch; 21h:56m:24s remains)
INFO - root - 2017-12-16 07:24:09.280844: step 47130, loss = 0.28, batch loss = 0.22 (27.6 examples/sec; 0.290 sec/batch; 23h:00m:28s remains)
INFO - root - 2017-12-16 07:24:12.112947: step 47140, loss = 0.22, batch loss = 0.17 (28.7 examples/sec; 0.278 sec/batch; 22h:03m:53s remains)
INFO - root - 2017-12-16 07:24:14.930772: step 47150, loss = 0.34, batch loss = 0.29 (29.4 examples/sec; 0.272 sec/batch; 21h:33m:51s remains)
INFO - root - 2017-12-16 07:24:17.762150: step 47160, loss = 0.41, batch loss = 0.36 (28.8 examples/sec; 0.278 sec/batch; 22h:02m:15s remains)
INFO - root - 2017-12-16 07:24:20.580643: step 47170, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.284 sec/batch; 22h:29m:26s remains)
INFO - root - 2017-12-16 07:24:23.439931: step 47180, loss = 0.37, batch loss = 0.31 (28.0 examples/sec; 0.286 sec/batch; 22h:38m:18s remains)
INFO - root - 2017-12-16 07:24:26.262691: step 47190, loss = 0.33, batch loss = 0.27 (28.9 examples/sec; 0.277 sec/batch; 21h:54m:53s remains)
INFO - root - 2017-12-16 07:24:29.102691: step 47200, loss = 0.28, batch loss = 0.22 (27.6 examples/sec; 0.290 sec/batch; 22h:57m:33s remains)
2017-12-16 07:24:29.570638: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2829313 -3.0210853 -2.8655705 -2.4934907 -2.2783172 -1.9873354 -2.0359709 -1.9949896 -1.9070661 -2.0355132 -2.3956301 -2.9452429 -3.2492905 -3.7406826 -4.3127389][-2.4084547 -2.3840277 -2.4913073 -2.4379392 -2.4581976 -2.1958628 -2.066489 -2.2165985 -2.4821277 -2.8448906 -3.4807985 -3.9907508 -4.2147408 -4.4458051 -4.6776891][-2.5008307 -2.6092553 -2.8853602 -3.0049808 -3.0981679 -2.9790554 -2.8400002 -2.752636 -2.9681506 -3.4456029 -4.1958046 -4.75569 -5.0754504 -5.1330237 -5.0134597][-3.0559685 -3.4372802 -3.81542 -3.990397 -3.9783165 -3.409595 -2.9502356 -2.8963265 -3.2852945 -3.8947039 -4.6716485 -5.2182713 -5.5338864 -5.4734988 -5.1639452][-3.9452834 -4.3723054 -4.7718468 -4.4568019 -3.9673476 -3.2217972 -2.4475255 -2.0367196 -2.2806413 -3.0062542 -3.9404106 -4.72847 -5.2509103 -5.2859764 -4.9361143][-4.7506046 -5.0517125 -5.0824008 -4.4983835 -3.6494513 -2.414259 -1.3213103 -0.69544005 -0.71130753 -1.3244364 -2.3105354 -3.1606989 -3.8331404 -4.3226585 -4.4066892][-4.607203 -4.8615532 -4.859417 -3.8534098 -2.6083245 -1.1545198 0.10285139 0.82624483 0.97083092 0.32637405 -0.67573977 -1.7712071 -2.7867107 -3.4184003 -3.6223166][-4.3418608 -4.193521 -3.8789363 -2.9958987 -1.6706681 -0.077589989 1.3207855 2.0289826 2.0227165 1.2173038 0.14168835 -0.98400354 -2.0279472 -2.8301711 -3.2646756][-3.2400022 -2.9723203 -2.6943498 -1.9045548 -0.91606569 0.06742382 1.0996308 1.6939387 1.7102838 0.91247368 -0.34936047 -1.6699305 -2.7113538 -3.3050871 -3.5372024][-2.7406995 -1.9795041 -1.5940681 -1.2578883 -0.73509455 -0.065522671 0.48149014 0.54224491 0.29701138 -0.562942 -1.7555423 -3.0630155 -4.1519313 -4.5964088 -4.5723529][-2.1542645 -1.659097 -1.4901614 -1.3019085 -1.2888014 -1.1930926 -0.88407779 -0.95024991 -1.356111 -2.2397571 -3.3207917 -4.4119964 -5.2718272 -5.5603557 -5.4581842][-1.8019338 -1.3636506 -1.3527079 -1.6225219 -1.9370863 -2.0623188 -2.1219647 -2.46132 -2.9203987 -3.6355684 -4.3602366 -5.2491651 -6.0172825 -6.1704636 -6.0507932][-1.5286176 -1.0098195 -0.99932671 -1.5025954 -2.0681868 -2.5594916 -2.8751001 -2.9888029 -3.2502718 -3.7566967 -4.3374777 -4.8983464 -5.4694958 -5.8796358 -6.1200576][-1.1416864 -0.68444657 -0.61305046 -0.98997831 -1.5317762 -1.9281235 -2.1425793 -2.5799537 -3.1086283 -3.3171291 -3.360477 -3.7534513 -4.4173112 -4.9352074 -5.4151707][-0.54823804 -0.10671425 -0.051381111 -0.22783279 -0.57474589 -1.2143419 -1.7143047 -1.8267908 -1.8883958 -2.2443123 -2.5675406 -2.7705741 -3.2223461 -4.0103712 -4.7779131]]...]
INFO - root - 2017-12-16 07:24:32.368372: step 47210, loss = 0.28, batch loss = 0.22 (29.6 examples/sec; 0.270 sec/batch; 21h:23m:17s remains)
INFO - root - 2017-12-16 07:24:35.191533: step 47220, loss = 0.24, batch loss = 0.19 (27.8 examples/sec; 0.288 sec/batch; 22h:50m:40s remains)
INFO - root - 2017-12-16 07:24:38.098232: step 47230, loss = 0.19, batch loss = 0.14 (26.4 examples/sec; 0.303 sec/batch; 24h:02m:27s remains)
INFO - root - 2017-12-16 07:24:40.961689: step 47240, loss = 0.31, batch loss = 0.25 (27.9 examples/sec; 0.286 sec/batch; 22h:41m:00s remains)
INFO - root - 2017-12-16 07:24:43.780566: step 47250, loss = 0.35, batch loss = 0.29 (28.1 examples/sec; 0.285 sec/batch; 22h:35m:28s remains)
INFO - root - 2017-12-16 07:24:46.631886: step 47260, loss = 0.32, batch loss = 0.26 (28.6 examples/sec; 0.280 sec/batch; 22h:10m:30s remains)
INFO - root - 2017-12-16 07:24:49.459638: step 47270, loss = 0.22, batch loss = 0.17 (29.3 examples/sec; 0.273 sec/batch; 21h:38m:21s remains)
INFO - root - 2017-12-16 07:24:52.263503: step 47280, loss = 0.29, batch loss = 0.24 (28.5 examples/sec; 0.281 sec/batch; 22h:14m:17s remains)
INFO - root - 2017-12-16 07:24:55.126647: step 47290, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.282 sec/batch; 22h:18m:35s remains)
INFO - root - 2017-12-16 07:24:57.943952: step 47300, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.284 sec/batch; 22h:28m:31s remains)
2017-12-16 07:24:58.398616: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.394526 -3.8733866 -4.4814162 -5.0176339 -5.4257536 -5.9405274 -6.497323 -7.0657167 -7.2818847 -7.1887712 -6.8979187 -6.303247 -5.4915991 -4.6786876 -4.2839947][-3.8182826 -4.5065961 -5.1527591 -5.6994853 -6.0472517 -6.4229417 -6.9023514 -7.403676 -7.597311 -7.5600376 -7.3785925 -6.8579607 -5.9952068 -4.951458 -4.3364611][-4.3166432 -5.0695415 -5.65304 -5.9611154 -5.923419 -5.9391451 -6.1268921 -6.4841757 -6.7700891 -6.9180036 -6.929718 -6.5750313 -5.8311634 -4.94767 -4.2764711][-4.633729 -5.4409533 -6.0004864 -6.0594153 -5.6377792 -4.9704204 -4.553483 -4.4556952 -4.6905656 -5.115705 -5.5263066 -5.691247 -5.3475447 -4.7664924 -4.2959018][-4.5705376 -5.2273736 -5.4318633 -4.9952478 -4.0255523 -2.8840241 -2.1013284 -1.7938137 -2.1379738 -2.8872659 -3.6290407 -4.174149 -4.2623515 -4.16547 -3.9949257][-4.4269662 -4.8452191 -4.7404819 -3.7287302 -2.160912 -0.46992898 0.82815218 1.3539343 0.95499229 -0.27428818 -1.6183584 -2.59939 -3.0532713 -3.256731 -3.1232104][-4.2849731 -4.2775979 -3.9290056 -2.7149734 -0.85444283 1.1551085 2.792922 3.4260807 2.9557376 1.6900997 0.31617546 -0.93690014 -1.6396391 -1.9719422 -1.9071314][-3.8616104 -3.8104305 -3.3455949 -1.9081993 -0.042654514 1.9446826 3.6381607 4.1816292 3.7425213 2.4123712 1.1522937 0.24010706 -0.21304893 -0.37166309 -0.12200403][-3.5384998 -3.4009624 -2.9660783 -1.838896 -0.44303918 1.2833886 2.7879138 3.3427625 3.1205945 2.0964537 1.2118683 0.64288521 0.54568338 0.65643644 1.1888061][-3.6354821 -3.7345841 -3.6877666 -3.0072842 -2.0351486 -0.73877335 0.35840082 0.98642588 1.1364574 0.76558113 0.5940299 0.6458745 1.0658026 1.4052434 1.9460654][-4.0798922 -4.4861512 -4.8581886 -4.7294464 -4.3278828 -3.4807832 -2.678822 -1.9792418 -1.4980528 -1.2040515 -0.57824516 0.052409172 0.82062292 1.3403835 1.9344363][-4.5235209 -5.219048 -6.0242825 -6.473711 -6.5846486 -6.1232028 -5.6635871 -5.0485148 -4.3343463 -3.431906 -2.2943013 -1.3028128 -0.28684092 0.25057554 0.73829174][-4.9244237 -5.8213539 -6.8114948 -7.5478549 -8.0598087 -8.131011 -8.0054207 -7.500567 -6.805625 -5.7278671 -4.3655863 -3.252564 -2.3631854 -1.9288254 -1.4192927][-5.3486853 -6.2116165 -7.0704145 -7.8767033 -8.5790043 -8.899456 -8.95343 -8.7405605 -8.2421217 -7.3084912 -6.165678 -5.1346383 -4.3386683 -4.0038104 -3.5595591][-5.3543205 -6.131669 -6.8836327 -7.5897059 -8.2036819 -8.71386 -9.0699854 -9.151125 -8.856348 -8.2358007 -7.4118109 -6.580061 -5.9558673 -5.6084213 -5.1619449]]...]
INFO - root - 2017-12-16 07:25:01.210283: step 47310, loss = 0.25, batch loss = 0.19 (29.3 examples/sec; 0.273 sec/batch; 21h:39m:34s remains)
INFO - root - 2017-12-16 07:25:04.062258: step 47320, loss = 0.26, batch loss = 0.20 (27.9 examples/sec; 0.287 sec/batch; 22h:41m:46s remains)
INFO - root - 2017-12-16 07:25:06.894335: step 47330, loss = 0.26, batch loss = 0.20 (28.8 examples/sec; 0.278 sec/batch; 22h:00m:44s remains)
INFO - root - 2017-12-16 07:25:09.736282: step 47340, loss = 0.25, batch loss = 0.19 (29.6 examples/sec; 0.270 sec/batch; 21h:24m:09s remains)
INFO - root - 2017-12-16 07:25:12.622021: step 47350, loss = 0.30, batch loss = 0.24 (26.9 examples/sec; 0.298 sec/batch; 23h:35m:38s remains)
INFO - root - 2017-12-16 07:25:15.455082: step 47360, loss = 0.31, batch loss = 0.25 (28.6 examples/sec; 0.280 sec/batch; 22h:11m:21s remains)
INFO - root - 2017-12-16 07:25:18.272573: step 47370, loss = 0.41, batch loss = 0.35 (28.7 examples/sec; 0.278 sec/batch; 22h:02m:37s remains)
INFO - root - 2017-12-16 07:25:21.147176: step 47380, loss = 0.35, batch loss = 0.29 (29.1 examples/sec; 0.275 sec/batch; 21h:45m:56s remains)
INFO - root - 2017-12-16 07:25:24.030199: step 47390, loss = 0.27, batch loss = 0.21 (26.5 examples/sec; 0.302 sec/batch; 23h:53m:59s remains)
INFO - root - 2017-12-16 07:25:26.858422: step 47400, loss = 0.37, batch loss = 0.31 (28.9 examples/sec; 0.277 sec/batch; 21h:56m:11s remains)
2017-12-16 07:25:27.339861: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2225604 -3.9314225 -3.8574467 -3.9578404 -4.0128326 -3.9421213 -3.8075979 -3.6011558 -3.3020532 -2.9410083 -2.5954945 -2.3081427 -2.1771076 -2.1675432 -2.2419286][-3.7739913 -3.5270541 -3.468698 -3.4877129 -3.5626965 -3.6305525 -3.5108957 -3.2507679 -2.96989 -2.7566068 -2.5660126 -2.3375885 -2.2432318 -2.1703014 -2.0432539][-3.0540013 -2.8756626 -3.0198057 -3.0514917 -3.0843472 -3.1309004 -3.0722508 -2.8584259 -2.6401978 -2.696301 -2.7577362 -2.6420369 -2.5424733 -2.520185 -2.3691185][-2.1973391 -2.1952095 -2.453258 -2.5313809 -2.4286087 -2.3535144 -2.2207708 -2.0822177 -2.1098361 -2.3243356 -2.5907419 -2.8457894 -3.1273584 -3.0723081 -2.8215647][-1.3131769 -1.5125461 -1.7722077 -1.8029065 -1.464757 -0.94464016 -0.4223423 -0.42056465 -0.84377813 -1.3604288 -1.8630948 -2.4084778 -2.9492011 -3.2443438 -3.1544971][-0.76074886 -0.99618769 -1.1244211 -0.86466932 -0.13326979 0.67529392 1.4804559 1.6899128 1.1447129 0.24829388 -0.79637742 -1.8187783 -2.6154022 -3.0670271 -3.1216226][-0.74046016 -0.95758557 -0.95427895 -0.38879108 0.76385975 2.0394688 3.2224722 3.60326 3.0103889 1.8288779 0.31954956 -1.0925877 -2.144191 -2.7904749 -3.0482125][-1.0431042 -1.2904232 -1.3030765 -0.55229354 0.97831726 2.6529989 4.0444946 4.5437889 4.0666466 2.7937508 1.0679607 -0.65648675 -1.968468 -2.682148 -2.917433][-1.6139603 -1.9804254 -1.8774815 -1.1985466 0.061609268 1.7654476 3.3869243 3.9888668 3.4865613 2.14543 0.53002834 -1.1238825 -2.4402213 -2.9657207 -3.0167][-2.1146779 -2.8797126 -3.2426138 -2.5646114 -1.3042686 0.029620647 1.2358193 1.8788624 1.7233639 0.47139025 -1.2066615 -2.7009575 -3.6105289 -3.6324496 -3.3024793][-2.8638134 -3.5398877 -4.0039711 -3.9303093 -3.1217642 -1.6882992 -0.3293438 -0.017804146 -0.41552019 -1.3391387 -2.7143106 -3.9949365 -4.6660414 -4.3648953 -3.8150711][-3.2496333 -3.8384814 -4.3847866 -4.4642892 -4.087358 -3.2198009 -2.1796482 -1.6895661 -1.7629871 -2.4034677 -3.5400159 -4.5506859 -5.1172819 -4.8794808 -4.2542963][-2.9931526 -3.4038048 -3.8484926 -4.1545157 -4.2042527 -3.7411132 -3.0565548 -2.6303849 -2.5752454 -3.1009941 -3.9165876 -4.5255828 -4.9119697 -4.8156023 -4.5743895][-3.1636274 -3.1356459 -3.2985353 -3.6247258 -3.8292613 -3.5128903 -3.0272813 -2.7244625 -2.5857148 -2.7391317 -3.191216 -3.8816609 -4.43693 -4.4270406 -4.3162766][-3.3298984 -3.1074357 -3.0252233 -3.10613 -3.1047068 -2.9624112 -2.6043863 -2.13741 -1.8920422 -1.8194919 -2.0558057 -2.5364676 -3.1083593 -3.5615783 -3.7897217]]...]
INFO - root - 2017-12-16 07:25:30.193311: step 47410, loss = 0.31, batch loss = 0.25 (28.0 examples/sec; 0.286 sec/batch; 22h:36m:37s remains)
INFO - root - 2017-12-16 07:25:33.041030: step 47420, loss = 0.50, batch loss = 0.44 (29.5 examples/sec; 0.271 sec/batch; 21h:26m:35s remains)
INFO - root - 2017-12-16 07:25:35.820772: step 47430, loss = 0.22, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 22h:04m:39s remains)
INFO - root - 2017-12-16 07:25:38.610473: step 47440, loss = 0.20, batch loss = 0.14 (29.4 examples/sec; 0.272 sec/batch; 21h:31m:41s remains)
INFO - root - 2017-12-16 07:25:41.480529: step 47450, loss = 0.22, batch loss = 0.16 (27.9 examples/sec; 0.287 sec/batch; 22h:44m:15s remains)
INFO - root - 2017-12-16 07:25:44.338281: step 47460, loss = 0.33, batch loss = 0.27 (27.9 examples/sec; 0.287 sec/batch; 22h:44m:32s remains)
INFO - root - 2017-12-16 07:25:47.187334: step 47470, loss = 0.33, batch loss = 0.27 (28.8 examples/sec; 0.278 sec/batch; 21h:58m:29s remains)
INFO - root - 2017-12-16 07:25:50.012115: step 47480, loss = 0.25, batch loss = 0.20 (28.6 examples/sec; 0.279 sec/batch; 22h:07m:19s remains)
INFO - root - 2017-12-16 07:25:52.856378: step 47490, loss = 0.22, batch loss = 0.16 (27.6 examples/sec; 0.290 sec/batch; 22h:55m:29s remains)
INFO - root - 2017-12-16 07:25:55.684150: step 47500, loss = 0.24, batch loss = 0.18 (29.9 examples/sec; 0.268 sec/batch; 21h:11m:06s remains)
2017-12-16 07:25:56.162030: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0741444 -2.5651155 -3.1809134 -3.3915229 -3.5669765 -3.6044471 -3.5228481 -3.3181798 -3.0367436 -2.9851739 -3.0031602 -2.8982534 -3.1175117 -3.5835185 -4.1325355][-0.8829987 -1.562427 -2.3218853 -2.6688232 -2.8931527 -2.9671402 -2.6083682 -2.0532644 -1.820009 -1.8337278 -1.9294891 -2.1471286 -2.4473581 -2.7937698 -3.3010817][-0.42088127 -1.3391507 -2.2576749 -2.6856248 -2.9008198 -2.8846786 -2.4787343 -1.7310989 -0.99035072 -0.94849825 -1.2222414 -1.5850849 -2.0569696 -2.4526815 -2.871244][-0.57914424 -1.4652321 -2.4773512 -2.967303 -3.1694655 -2.9390497 -2.3217123 -1.5110817 -0.841655 -0.64460683 -0.76218486 -1.2740602 -1.8795106 -2.277396 -2.6634893][-0.80423188 -1.7180541 -2.7354884 -3.3272753 -3.5656126 -3.253834 -2.4943285 -1.5874124 -0.75669432 -0.55531263 -0.92873526 -1.5135014 -2.091604 -2.4635429 -2.8863916][-1.1558118 -1.8128181 -2.834775 -3.5803823 -3.679925 -3.3628271 -2.5583725 -1.5938206 -0.74723887 -0.36992836 -0.5928812 -1.2403092 -1.96721 -2.507328 -2.9732413][-1.3184571 -1.8806834 -2.7623854 -3.6913173 -3.9542756 -3.5262532 -2.6117172 -1.6652551 -0.72417831 -0.20579529 -0.31226397 -0.93650913 -1.616925 -2.2832496 -2.9459367][-1.0195627 -1.4709976 -2.3102086 -3.2471807 -3.802181 -3.5827668 -2.7902765 -1.7841351 -0.80760741 -0.24557257 -0.2566185 -0.79295015 -1.4246395 -2.14257 -2.7743189][-0.55487227 -0.87227511 -1.6569891 -2.7755921 -3.3195677 -3.2750278 -2.7069223 -1.7866659 -0.89313865 -0.32842016 -0.28564835 -0.7210207 -1.2443714 -1.9449472 -2.6623228][-0.37215948 -0.34958792 -0.91026449 -2.1819782 -3.0498857 -3.1728048 -2.6817069 -1.9464724 -1.1608629 -0.60102463 -0.47932696 -0.76612568 -1.1785579 -1.822911 -2.4991779][-0.26647234 -0.18655205 -0.62812185 -1.53303 -2.3156693 -2.6737385 -2.448144 -1.8377757 -1.277514 -0.91317558 -0.79304671 -0.96750855 -1.2605901 -1.8112576 -2.4456959][-0.75298262 -0.29557133 -0.32065678 -1.1292045 -2.0448532 -2.5630054 -2.3799589 -1.9259408 -1.584192 -1.2672567 -1.101754 -1.1903787 -1.4069633 -1.8426428 -2.3630087][-0.83013153 -0.22370434 -0.059828758 -0.65499139 -1.4292011 -2.0711088 -2.2126479 -1.9531713 -1.7096584 -1.5448434 -1.3814838 -1.4172664 -1.5737064 -1.9268889 -2.3964717][-1.3284631 -0.59705448 -0.23019361 -0.48165298 -1.2765815 -1.9679844 -2.1876824 -2.1079152 -1.9503963 -1.7630208 -1.5695918 -1.5966356 -1.6769252 -2.0148168 -2.5049438][-1.5497065 -0.90176725 -0.61281967 -0.7572093 -1.3118269 -2.0030897 -2.4782844 -2.3969641 -2.2377517 -1.9860961 -1.7004123 -1.7025392 -1.77391 -2.1288948 -2.5258982]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-47500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-47500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 07:25:59.509403: step 47510, loss = 0.23, batch loss = 0.17 (27.5 examples/sec; 0.291 sec/batch; 23h:01m:19s remains)
INFO - root - 2017-12-16 07:26:02.293276: step 47520, loss = 0.23, batch loss = 0.17 (27.5 examples/sec; 0.291 sec/batch; 23h:01m:43s remains)
INFO - root - 2017-12-16 07:26:05.145348: step 47530, loss = 0.27, batch loss = 0.21 (29.3 examples/sec; 0.273 sec/batch; 21h:37m:32s remains)
INFO - root - 2017-12-16 07:26:08.017598: step 47540, loss = 0.52, batch loss = 0.46 (27.8 examples/sec; 0.287 sec/batch; 22h:44m:50s remains)
INFO - root - 2017-12-16 07:26:10.892057: step 47550, loss = 0.35, batch loss = 0.29 (28.5 examples/sec; 0.280 sec/batch; 22h:10m:56s remains)
INFO - root - 2017-12-16 07:26:13.717399: step 47560, loss = 0.28, batch loss = 0.22 (26.1 examples/sec; 0.306 sec/batch; 24h:14m:44s remains)
INFO - root - 2017-12-16 07:26:16.564599: step 47570, loss = 0.19, batch loss = 0.14 (27.8 examples/sec; 0.288 sec/batch; 22h:49m:01s remains)
INFO - root - 2017-12-16 07:26:19.379949: step 47580, loss = 0.28, batch loss = 0.22 (27.3 examples/sec; 0.293 sec/batch; 23h:09m:37s remains)
INFO - root - 2017-12-16 07:26:22.227220: step 47590, loss = 0.28, batch loss = 0.22 (26.0 examples/sec; 0.308 sec/batch; 24h:21m:12s remains)
INFO - root - 2017-12-16 07:26:25.048597: step 47600, loss = 0.24, batch loss = 0.19 (29.2 examples/sec; 0.274 sec/batch; 21h:41m:58s remains)
2017-12-16 07:26:25.532241: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1258841 -5.0043821 -4.6978536 -4.2894435 -4.1193419 -4.2761564 -4.5518928 -4.6407423 -4.6762147 -4.5994534 -4.3979316 -4.1384525 -3.9459529 -3.8460782 -3.4730635][-6.3812895 -6.0625644 -5.6328974 -5.4698243 -5.5139761 -5.5504885 -5.6745749 -5.6486869 -5.62703 -5.4784288 -5.317503 -5.07006 -4.8219175 -4.6856613 -4.2713561][-7.497982 -7.4083171 -7.0991545 -6.7726197 -6.574791 -6.5378962 -6.5426936 -6.3480406 -6.0221634 -5.6540523 -5.3077621 -5.2035689 -5.3081822 -5.3042016 -4.9541006][-8.4667921 -8.3715687 -8.0549393 -7.7989216 -7.5229483 -7.1211777 -6.4969139 -5.6237559 -4.8740516 -4.4065928 -4.3094077 -4.509016 -4.8944716 -5.1495485 -5.2564135][-8.3408184 -8.4444389 -8.2595024 -7.7240419 -6.9446368 -5.8711309 -4.6981473 -3.5932302 -2.7023215 -2.1985605 -2.2116175 -2.9629622 -3.9962993 -4.6509156 -4.7402358][-7.2684603 -7.4169421 -7.212781 -6.5573735 -5.4775481 -3.8625827 -1.9059322 -0.17329979 0.82367468 0.9951 0.20646286 -0.9367907 -2.0643082 -3.3532014 -3.815928][-5.6348248 -5.7961969 -5.7696447 -4.99367 -3.5839438 -1.665278 0.52733994 2.371078 3.5185871 3.5455256 2.5436172 0.80451059 -0.83294106 -1.9020536 -2.2518668][-4.3262749 -4.43353 -4.1586027 -3.393856 -1.9508445 0.2601037 2.5561204 4.2380838 4.9475584 4.2657928 2.8210597 1.3291183 0.056968212 -0.87814808 -1.1391711][-2.6869206 -2.9822659 -3.2521262 -2.6487405 -1.3030379 0.7099247 2.7027931 4.1253357 4.6401205 3.8331461 2.1543937 0.54284573 -0.26832151 -0.40686131 -0.32245731][-1.5455759 -1.9519038 -2.5121753 -2.4122119 -1.3955142 0.21169233 1.6647305 2.7388234 2.9082446 2.0090985 0.66675568 -0.51760721 -0.8527267 -0.45226455 0.0055446625][-0.61646461 -1.2550595 -2.0743914 -2.5514719 -2.3168473 -1.1317618 0.16512299 0.80380726 0.50165319 -0.19863701 -1.0919349 -1.6112175 -1.4018376 -0.8095336 -0.23057222][-0.31653547 -1.0170944 -1.9595556 -2.6714869 -2.6007957 -2.2473781 -1.8482585 -1.3716731 -1.4620452 -1.9524572 -2.6533384 -2.8859563 -2.3783827 -1.4823611 -0.89573908][-0.26757669 -0.75467968 -1.7130423 -2.8285995 -3.391705 -3.0950928 -2.5426056 -2.484313 -2.8667741 -3.3669727 -3.8108749 -3.843575 -3.2840619 -2.6420655 -2.2623351][-0.31479406 -0.65155029 -1.5600514 -2.6322598 -3.3129172 -3.5301416 -3.3933859 -3.211942 -3.3569269 -3.6519566 -3.8545861 -3.8980598 -3.7952332 -3.531131 -3.229368][-0.52582383 -0.76465178 -1.4869058 -2.5573041 -3.348773 -3.7128863 -3.870841 -3.7893596 -3.7647128 -3.9269094 -4.1825371 -3.9946148 -3.5213149 -3.4184391 -3.4780436]]...]
INFO - root - 2017-12-16 07:26:28.353034: step 47610, loss = 0.41, batch loss = 0.35 (28.9 examples/sec; 0.277 sec/batch; 21h:52m:59s remains)
INFO - root - 2017-12-16 07:26:31.163970: step 47620, loss = 0.34, batch loss = 0.28 (29.1 examples/sec; 0.275 sec/batch; 21h:46m:57s remains)
INFO - root - 2017-12-16 07:26:34.008262: step 47630, loss = 0.24, batch loss = 0.18 (27.2 examples/sec; 0.294 sec/batch; 23h:14m:27s remains)
INFO - root - 2017-12-16 07:26:36.864106: step 47640, loss = 0.24, batch loss = 0.19 (26.6 examples/sec; 0.301 sec/batch; 23h:49m:10s remains)
INFO - root - 2017-12-16 07:26:39.707998: step 47650, loss = 0.40, batch loss = 0.34 (28.3 examples/sec; 0.283 sec/batch; 22h:23m:42s remains)
INFO - root - 2017-12-16 07:26:42.571951: step 47660, loss = 0.39, batch loss = 0.33 (28.2 examples/sec; 0.283 sec/batch; 22h:24m:42s remains)
INFO - root - 2017-12-16 07:26:45.418013: step 47670, loss = 0.26, batch loss = 0.20 (29.3 examples/sec; 0.273 sec/batch; 21h:36m:26s remains)
INFO - root - 2017-12-16 07:26:48.272567: step 47680, loss = 0.21, batch loss = 0.15 (27.8 examples/sec; 0.287 sec/batch; 22h:43m:48s remains)
INFO - root - 2017-12-16 07:26:51.100209: step 47690, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 21h:59m:23s remains)
INFO - root - 2017-12-16 07:26:53.933849: step 47700, loss = 0.27, batch loss = 0.21 (27.2 examples/sec; 0.295 sec/batch; 23h:18m:06s remains)
2017-12-16 07:26:54.397189: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2150307 -3.5033636 -3.7115824 -3.8489261 -3.8659568 -3.876641 -3.8324203 -3.7832427 -3.7318149 -3.6399636 -3.5957789 -3.5744097 -3.6345706 -3.592603 -3.6263144][-3.062685 -3.3925381 -3.5937841 -3.7750509 -3.8385026 -3.8983245 -3.9009647 -3.859441 -3.7743304 -3.6834445 -3.6699176 -3.6900146 -3.8134379 -3.7954156 -3.828038][-3.0590084 -3.3826327 -3.6221707 -3.7588391 -3.7057397 -3.647197 -3.567857 -3.5701025 -3.594115 -3.6012347 -3.6440914 -3.747849 -3.9222162 -3.937536 -4.0311341][-3.3179505 -3.6099777 -3.7475042 -3.7057521 -3.4642115 -3.1786768 -2.9148309 -2.8645496 -2.9338248 -3.1204405 -3.3805323 -3.6955514 -3.9871137 -4.0073209 -4.0587764][-3.8129296 -3.9649632 -3.9430633 -3.6730914 -3.1384509 -2.6198158 -2.2115998 -2.0673406 -2.1052532 -2.3166625 -2.6392241 -3.0936923 -3.5169196 -3.722544 -3.8519323][-4.2038007 -4.3117452 -4.1055894 -3.6955023 -2.9417932 -2.0873644 -1.4586625 -1.2130237 -1.222671 -1.4389675 -1.7545798 -2.2178621 -2.7258096 -3.1410851 -3.4433818][-4.1610985 -4.2564268 -4.0809689 -3.5170226 -2.6961608 -1.6146359 -0.70723772 -0.31545687 -0.25636244 -0.538383 -0.96725965 -1.4614687 -1.9914296 -2.4810343 -2.8365839][-3.9548318 -3.8565645 -3.5189633 -2.889055 -2.0079315 -1.0566664 -0.30909014 0.20253038 0.41223431 0.17303181 -0.193789 -0.72192836 -1.2753186 -1.6732237 -1.919697][-3.3592286 -3.1159444 -2.7836633 -2.2024593 -1.4243362 -0.57691383 -0.020516872 0.21951342 0.30063772 0.24397087 0.086900711 -0.31210852 -0.73668075 -0.95257378 -0.93409467][-3.3421135 -2.8949285 -2.4940109 -2.1010158 -1.6264694 -0.93728852 -0.469249 -0.1181283 -0.18153572 -0.17402554 -0.18881035 -0.38894272 -0.56740832 -0.74536419 -0.58930945][-4.1807756 -3.760211 -3.1461966 -2.7241695 -2.3087878 -1.7752006 -1.4291277 -1.120322 -1.0812991 -0.93813586 -0.82101965 -0.74337792 -0.64903045 -0.58955145 -0.16935158][-5.6293116 -5.0997887 -4.3175459 -3.6850345 -3.2109175 -2.8336196 -2.7865989 -2.4866583 -2.3467677 -1.9672947 -1.608865 -1.3223679 -1.0528972 -0.80392981 -0.1431632][-7.3465152 -6.8265591 -5.8376684 -5.115787 -4.4276438 -3.9843106 -3.8689408 -3.7967186 -3.8244948 -3.3141835 -2.6799474 -2.3026867 -1.9532259 -1.7121301 -1.1274354][-8.9251213 -8.4810352 -7.39872 -6.6727791 -5.9340615 -5.2280784 -4.9598451 -4.8175426 -5.0171947 -4.7320766 -4.2833328 -3.7280521 -3.252852 -3.0124278 -2.4172137][-9.7193565 -9.2562027 -8.17687 -7.4706273 -6.7573328 -6.0879869 -5.6184983 -5.3888793 -5.4722586 -5.4310317 -5.305963 -4.8998218 -4.5465665 -4.32719 -3.814414]]...]
INFO - root - 2017-12-16 07:26:57.253708: step 47710, loss = 0.33, batch loss = 0.27 (27.9 examples/sec; 0.287 sec/batch; 22h:41m:52s remains)
INFO - root - 2017-12-16 07:27:00.080919: step 47720, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.282 sec/batch; 22h:18m:32s remains)
INFO - root - 2017-12-16 07:27:02.940993: step 47730, loss = 0.30, batch loss = 0.24 (28.4 examples/sec; 0.281 sec/batch; 22h:15m:18s remains)
INFO - root - 2017-12-16 07:27:05.773439: step 47740, loss = 0.41, batch loss = 0.35 (28.5 examples/sec; 0.281 sec/batch; 22h:12m:40s remains)
INFO - root - 2017-12-16 07:27:08.676341: step 47750, loss = 0.27, batch loss = 0.21 (27.7 examples/sec; 0.289 sec/batch; 22h:52m:12s remains)
INFO - root - 2017-12-16 07:27:11.502406: step 47760, loss = 0.23, batch loss = 0.17 (29.0 examples/sec; 0.276 sec/batch; 21h:48m:23s remains)
INFO - root - 2017-12-16 07:27:14.323061: step 47770, loss = 0.23, batch loss = 0.17 (29.3 examples/sec; 0.273 sec/batch; 21h:33m:50s remains)
INFO - root - 2017-12-16 07:27:17.149031: step 47780, loss = 0.46, batch loss = 0.40 (27.2 examples/sec; 0.294 sec/batch; 23h:17m:12s remains)
INFO - root - 2017-12-16 07:27:20.020441: step 47790, loss = 0.38, batch loss = 0.32 (28.0 examples/sec; 0.286 sec/batch; 22h:36m:54s remains)
INFO - root - 2017-12-16 07:27:22.854387: step 47800, loss = 0.26, batch loss = 0.21 (28.9 examples/sec; 0.277 sec/batch; 21h:53m:54s remains)
2017-12-16 07:27:23.308064: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1987634 -4.3760905 -4.5527844 -4.6569862 -4.6171551 -4.44617 -4.2027807 -3.8233142 -3.4844661 -3.2103047 -3.0731676 -2.8664689 -2.6709039 -2.528825 -2.3799932][-4.9264064 -5.1536155 -5.335537 -5.43213 -5.3629465 -5.0441413 -4.5952606 -4.1318154 -3.8369522 -3.5479484 -3.4315054 -3.2608964 -3.0620492 -2.9137232 -2.7324445][-5.6712832 -5.875638 -6.0158882 -5.9917889 -5.6952276 -5.1391377 -4.5554342 -3.9689245 -3.6357617 -3.4803174 -3.6334021 -3.805728 -3.855608 -3.7217569 -3.4752705][-6.0032845 -6.1565714 -6.1723037 -5.9525404 -5.3937397 -4.5458622 -3.6736231 -2.9644032 -2.7125998 -2.8709514 -3.4024801 -4.0008512 -4.4308295 -4.4272585 -4.1114092][-5.7788663 -5.7489471 -5.5016141 -5.100183 -4.3415008 -3.1815896 -2.1578119 -1.5535376 -1.4455614 -1.9206367 -2.8375397 -3.8327763 -4.5581579 -4.7543159 -4.5586257][-4.8041754 -4.6322894 -4.2559543 -3.5278549 -2.4238672 -1.2039299 -0.17659998 0.26755667 0.25163555 -0.40861368 -1.6130056 -2.9978914 -4.143208 -4.5855641 -4.47455][-3.269377 -2.879034 -2.5201077 -1.7882051 -0.7182796 0.5502758 1.6651769 1.9707832 1.841783 1.0477109 -0.2452178 -1.8751221 -3.2844427 -3.9298446 -4.0048618][-1.7000804 -1.3583527 -1.1314569 -0.66265345 0.11893702 1.1787615 2.0952239 2.2687464 2.1432567 1.4010906 0.22885942 -1.3929763 -2.9197364 -3.6499572 -3.7634134][-0.5071497 -0.11121368 0.045035362 0.07407093 0.32929134 1.0261889 1.6927352 1.7663832 1.6054201 1.0582576 0.14883232 -1.3461628 -2.7825024 -3.5187359 -3.6387558][0.19195557 0.52907991 0.30875587 0.089332104 0.19156408 0.31335258 0.35181618 0.369668 0.36669111 -0.034250259 -0.7553091 -1.9210877 -3.2052 -3.8790035 -3.9281447][-0.39753628 -0.12722969 -0.20411396 -0.78531933 -1.2970648 -1.1038289 -0.81915545 -1.1121042 -1.3503537 -1.604099 -2.0747144 -3.010426 -3.9900849 -4.5914483 -4.7668667][-1.6729262 -1.6615217 -2.1260219 -2.60352 -2.7061424 -2.5973732 -2.6248155 -2.6222835 -2.5939445 -2.7609837 -3.1515255 -3.9584789 -4.7626495 -5.3037872 -5.4169917][-3.2549496 -3.3657794 -3.649416 -4.2038975 -4.4030051 -4.1002173 -3.6637692 -3.5345342 -3.5234365 -3.5370712 -3.7615881 -4.4308152 -5.1631093 -5.801054 -5.9691744][-4.675426 -4.7753034 -5.06196 -5.415216 -5.3308325 -4.9462647 -4.5067291 -4.1967206 -3.9408362 -3.7252522 -3.795789 -4.3134151 -4.9853315 -5.6498208 -5.9482241][-5.5356436 -5.8605781 -6.1433015 -6.179852 -5.8621125 -5.4716473 -4.8902769 -4.3438468 -4.0471892 -3.9211612 -3.9263332 -4.2102365 -4.7122464 -5.3738952 -5.6349983]]...]
INFO - root - 2017-12-16 07:27:26.155551: step 47810, loss = 0.33, batch loss = 0.27 (28.2 examples/sec; 0.284 sec/batch; 22h:26m:11s remains)
INFO - root - 2017-12-16 07:27:28.960434: step 47820, loss = 0.36, batch loss = 0.30 (28.9 examples/sec; 0.277 sec/batch; 21h:53m:10s remains)
INFO - root - 2017-12-16 07:27:31.770604: step 47830, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 22h:30m:00s remains)
INFO - root - 2017-12-16 07:27:34.648471: step 47840, loss = 0.31, batch loss = 0.25 (27.9 examples/sec; 0.287 sec/batch; 22h:41m:12s remains)
INFO - root - 2017-12-16 07:27:37.509623: step 47850, loss = 0.38, batch loss = 0.32 (28.0 examples/sec; 0.285 sec/batch; 22h:34m:04s remains)
INFO - root - 2017-12-16 07:27:40.415435: step 47860, loss = 0.24, batch loss = 0.18 (29.1 examples/sec; 0.275 sec/batch; 21h:44m:04s remains)
INFO - root - 2017-12-16 07:27:43.237758: step 47870, loss = 0.32, batch loss = 0.27 (29.7 examples/sec; 0.269 sec/batch; 21h:18m:26s remains)
INFO - root - 2017-12-16 07:27:46.069405: step 47880, loss = 0.27, batch loss = 0.22 (29.1 examples/sec; 0.275 sec/batch; 21h:44m:57s remains)
INFO - root - 2017-12-16 07:27:48.889237: step 47890, loss = 0.25, batch loss = 0.19 (27.2 examples/sec; 0.294 sec/batch; 23h:14m:34s remains)
INFO - root - 2017-12-16 07:27:51.727997: step 47900, loss = 0.47, batch loss = 0.41 (27.9 examples/sec; 0.287 sec/batch; 22h:40m:57s remains)
2017-12-16 07:27:52.189826: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9533346 -2.4216089 -3.0651426 -3.6982677 -4.1714578 -4.4775815 -4.6514359 -4.8546863 -4.9105225 -4.6415105 -4.2369843 -3.9413478 -3.7027588 -3.289381 -2.8685403][-2.3129163 -2.9754493 -3.6458108 -4.2288032 -4.6184759 -4.8635573 -4.921803 -4.7924385 -4.6844568 -4.668118 -4.6152725 -4.364892 -4.0808077 -3.7211165 -3.3337421][-2.897753 -3.628911 -4.3196754 -4.691658 -4.6372395 -4.4762893 -4.2185287 -4.0651159 -4.0284629 -4.1369915 -4.4824638 -4.7019458 -4.6804814 -4.2594671 -3.6320302][-3.2029653 -4.0900097 -4.8237734 -4.9828014 -4.5148067 -3.7028723 -2.8805368 -2.2590423 -2.2105522 -2.8184538 -3.7872381 -4.7201223 -5.1991978 -4.9196191 -4.2257981][-3.4723184 -4.2069249 -4.6107459 -4.3683596 -3.4633224 -2.0352933 -0.74728584 0.016600132 -0.080039978 -1.1188643 -2.7164564 -4.255034 -5.2481003 -5.3531928 -4.7481532][-3.6563449 -4.0850582 -4.1396413 -3.4509597 -1.9223499 0.16828442 1.8906302 2.5502863 1.9661746 0.42450953 -1.7182653 -3.6985919 -4.9713993 -5.3978705 -4.9237304][-3.4120028 -3.5660288 -3.2988129 -2.2345457 -0.50701976 1.5347099 3.3671174 4.2237797 3.4628935 1.3146114 -1.1827197 -3.3053527 -4.5287552 -4.8851094 -4.69542][-2.9688582 -3.0453303 -2.7020111 -1.4617488 0.39422274 2.6188426 4.2134581 4.5643063 3.6595926 1.5449142 -1.0232673 -3.14456 -4.3497844 -4.6341019 -4.117981][-2.5487015 -2.3657708 -1.9627471 -0.97721791 0.68034172 2.5502639 3.8359032 4.0655355 3.0015626 1.1177988 -0.89212322 -2.6809521 -3.6447158 -3.8082359 -3.2018812][-2.3968027 -2.4752655 -2.16506 -1.1541681 0.097437859 1.3731151 2.3348126 2.37646 1.2294564 -0.34719896 -1.6082621 -2.4738152 -2.6306114 -2.4089797 -1.915905][-2.7360253 -2.8960979 -3.0218976 -2.8297172 -2.0444677 -1.052011 -0.25189829 -0.10219097 -0.7331419 -1.865104 -2.9600277 -3.1123767 -2.4622416 -1.6380806 -0.693403][-3.2074804 -3.7644744 -4.358036 -4.3876319 -3.9613602 -3.3713305 -2.822361 -2.3310423 -2.3304598 -2.9486094 -3.631042 -3.7660813 -3.0345237 -1.8670921 -0.63038778][-3.7894757 -4.8106232 -5.6058688 -5.804646 -5.5853558 -4.94878 -4.0801044 -3.479285 -3.445024 -3.7269595 -3.8870909 -3.864223 -3.3962243 -2.4806921 -1.3699663][-4.7292094 -5.8111539 -6.7212672 -7.2362614 -7.0804439 -6.3795552 -5.5181432 -4.7175474 -4.3313408 -4.4372907 -4.6561952 -4.4708424 -3.9370179 -3.3531427 -2.815618][-5.11317 -6.5333838 -7.8089066 -8.401886 -8.1872692 -7.4824171 -6.717679 -5.9583364 -5.5288377 -5.4039311 -5.4803281 -5.5669413 -5.3591251 -5.0313783 -4.6793122]]...]
INFO - root - 2017-12-16 07:27:55.039478: step 47910, loss = 0.46, batch loss = 0.40 (28.2 examples/sec; 0.284 sec/batch; 22h:25m:53s remains)
INFO - root - 2017-12-16 07:27:57.890663: step 47920, loss = 0.27, batch loss = 0.21 (24.9 examples/sec; 0.322 sec/batch; 25h:26m:22s remains)
INFO - root - 2017-12-16 07:28:00.698354: step 47930, loss = 0.26, batch loss = 0.20 (29.0 examples/sec; 0.276 sec/batch; 21h:49m:56s remains)
INFO - root - 2017-12-16 07:28:03.541478: step 47940, loss = 0.18, batch loss = 0.13 (27.9 examples/sec; 0.287 sec/batch; 22h:39m:48s remains)
INFO - root - 2017-12-16 07:28:06.311580: step 47950, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 22h:01m:44s remains)
INFO - root - 2017-12-16 07:28:09.186590: step 47960, loss = 0.21, batch loss = 0.15 (27.6 examples/sec; 0.289 sec/batch; 22h:52m:27s remains)
INFO - root - 2017-12-16 07:28:11.967888: step 47970, loss = 0.33, batch loss = 0.27 (28.1 examples/sec; 0.285 sec/batch; 22h:30m:16s remains)
INFO - root - 2017-12-16 07:28:14.730550: step 47980, loss = 0.29, batch loss = 0.23 (29.3 examples/sec; 0.273 sec/batch; 21h:34m:25s remains)
INFO - root - 2017-12-16 07:28:17.554104: step 47990, loss = 0.35, batch loss = 0.29 (28.1 examples/sec; 0.285 sec/batch; 22h:29m:11s remains)
INFO - root - 2017-12-16 07:28:20.381936: step 48000, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.284 sec/batch; 22h:27m:13s remains)
2017-12-16 07:28:20.856248: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.248065 -5.190794 -5.1779938 -5.6401215 -6.174221 -6.6938448 -7.0745564 -7.4628544 -7.5469208 -7.0901031 -6.2565241 -5.1659436 -4.0686364 -3.0287757 -2.4787035][-4.0514226 -4.2392836 -4.5489378 -4.8549643 -5.21858 -6.1281881 -6.873353 -7.0338831 -6.7980762 -6.2110753 -5.5376463 -4.6818428 -3.8184977 -3.016773 -2.4268353][-3.5247819 -3.5599127 -3.6489618 -4.0959997 -4.5593395 -5.0376954 -5.5338035 -5.9536433 -5.955049 -5.4510908 -5.0786691 -4.5244904 -3.7938778 -2.9396324 -2.3163443][-2.8841991 -3.1343555 -3.1748953 -3.2031379 -3.3944156 -3.7576077 -4.0053639 -3.9999471 -4.0571761 -4.0929756 -4.2531929 -4.272615 -4.018075 -3.1412745 -2.2307668][-2.5497007 -2.653954 -2.4629455 -2.2735691 -2.0951483 -2.0284812 -2.1939881 -2.2030764 -2.3587694 -2.7571006 -3.2998133 -3.4921582 -3.2370772 -2.8394885 -2.3366892][-2.2084436 -2.1841619 -1.8107147 -1.4445603 -1.0516834 -0.39701509 0.089574814 0.11623144 -0.600317 -1.2516785 -1.9641781 -2.6201828 -2.5827489 -2.1765893 -1.6114116][-1.9666145 -1.8522971 -1.5279412 -0.87643194 -0.030553818 0.76478195 1.3200507 1.4685535 0.87519932 0.1183176 -0.76853633 -1.3715563 -1.5469005 -1.6299875 -1.3202932][-1.7382746 -1.586473 -1.2728479 -0.71102524 -0.14388704 0.76055336 1.6653614 1.6392002 0.90533686 0.15591621 -0.62028432 -1.0555267 -0.97905993 -1.1823187 -1.2445583][-1.8595483 -1.6875474 -1.1954734 -0.93597293 -0.61035895 0.24336004 0.77226543 0.8539443 0.50566816 -0.34549284 -1.0776851 -1.6269379 -1.827966 -1.638639 -1.1586304][-1.7179022 -1.9497392 -2.1639173 -1.9414406 -1.3571687 -0.83127236 -0.67002463 -0.38629532 -0.68962932 -1.4653497 -1.9877379 -2.3906255 -2.2974298 -2.2231297 -2.0106091][-2.5294178 -2.4735491 -2.5263696 -2.8127313 -2.9153926 -2.5391273 -2.222554 -2.187016 -2.5616736 -2.9374671 -3.2295079 -3.3994207 -2.9730854 -2.5797491 -2.0899184][-3.4969296 -3.5157359 -3.7345519 -3.6641741 -3.5621181 -3.7407885 -3.9085772 -3.8747663 -3.8687038 -4.077179 -4.3144889 -4.3894444 -3.9215124 -3.2372203 -2.6429405][-4.5124664 -4.5520077 -4.7025676 -4.6285095 -4.5224352 -4.2948928 -4.3222532 -4.6473804 -4.7537031 -4.7167358 -4.7231522 -4.9192104 -4.7366066 -4.1655807 -3.5391848][-5.1202607 -4.9971471 -5.0798264 -5.0908017 -5.1014 -4.8705254 -4.8298421 -5.0327988 -5.1195207 -5.2107081 -5.2849913 -5.4332705 -5.3172259 -5.1693759 -4.753541][-5.2979221 -4.903512 -4.7859473 -4.9367485 -5.0980496 -5.0831785 -5.1230059 -5.2649717 -5.4100952 -5.5050464 -5.5193172 -5.7172632 -5.829392 -5.680439 -5.1411214]]...]
INFO - root - 2017-12-16 07:28:23.767838: step 48010, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.282 sec/batch; 22h:17m:26s remains)
INFO - root - 2017-12-16 07:28:26.637129: step 48020, loss = 0.39, batch loss = 0.33 (26.5 examples/sec; 0.302 sec/batch; 23h:50m:51s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 07:28:29.541403: step 48030, loss = 0.23, batch loss = 0.17 (25.6 examples/sec; 0.312 sec/batch; 24h:41m:01s remains)
INFO - root - 2017-12-16 07:28:32.326933: step 48040, loss = 0.31, batch loss = 0.25 (28.5 examples/sec; 0.281 sec/batch; 22h:10m:31s remains)
INFO - root - 2017-12-16 07:28:35.210087: step 48050, loss = 0.29, batch loss = 0.23 (28.3 examples/sec; 0.282 sec/batch; 22h:18m:15s remains)
INFO - root - 2017-12-16 07:28:38.035727: step 48060, loss = 0.33, batch loss = 0.28 (28.2 examples/sec; 0.283 sec/batch; 22h:23m:57s remains)
INFO - root - 2017-12-16 07:28:40.903707: step 48070, loss = 0.26, batch loss = 0.20 (28.2 examples/sec; 0.284 sec/batch; 22h:25m:41s remains)
INFO - root - 2017-12-16 07:28:43.695705: step 48080, loss = 0.23, batch loss = 0.17 (28.9 examples/sec; 0.277 sec/batch; 21h:52m:37s remains)
INFO - root - 2017-12-16 07:28:46.540719: step 48090, loss = 0.26, batch loss = 0.20 (28.8 examples/sec; 0.278 sec/batch; 21h:55m:25s remains)
INFO - root - 2017-12-16 07:28:49.384043: step 48100, loss = 0.30, batch loss = 0.24 (28.1 examples/sec; 0.285 sec/batch; 22h:29m:06s remains)
2017-12-16 07:28:49.850881: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7322607 -5.2479467 -4.6854134 -4.8739519 -5.1089778 -5.2441339 -5.1703019 -4.8891878 -4.6085005 -4.0455151 -3.5037005 -2.5574059 -1.6089306 -0.99569154 -0.76119256][-4.531848 -4.3594723 -4.1043782 -3.8904798 -3.8982806 -4.249321 -4.2988129 -4.2763891 -4.1029911 -3.4240344 -2.8687491 -2.1601236 -1.5834866 -0.70489788 0.0059523582][-3.7428493 -3.2797732 -2.9454341 -2.8187847 -2.7073953 -2.9331479 -3.2748754 -3.5444822 -3.3449383 -3.0448081 -2.7977748 -2.150146 -1.3707464 -0.77032161 -0.35966396][-2.5447931 -2.0585122 -1.5538538 -1.4007742 -1.4945374 -1.4056365 -1.4844596 -2.314904 -3.1379809 -3.1112959 -2.6889386 -2.4531746 -1.9984877 -1.3655477 -0.81696963][-1.6020164 -0.82344985 -0.081115723 0.32859945 0.50822163 0.35440588 -0.032344818 -0.77839088 -1.5311506 -2.3109956 -2.8690615 -2.6981914 -2.3803949 -2.1535776 -2.1103213][-1.2532468 -0.38826895 0.36316872 1.265789 1.8867264 1.861124 1.6990671 0.777195 -0.210886 -1.0748591 -1.6509981 -2.2753139 -2.7655668 -2.644275 -2.4713583][-1.4583597 -0.39960289 0.78850889 1.8038764 2.4204149 2.7043815 2.8144755 1.8845744 0.71679926 -0.42822886 -1.2404377 -1.9178474 -2.7271941 -3.2170253 -3.3143573][-1.1798394 -0.33122158 0.52140427 1.7124872 2.6467834 2.921412 2.912992 2.2116504 1.0439754 -0.38016844 -1.6129251 -2.5395446 -3.447453 -3.9364119 -4.028935][-2.0831833 -1.3296816 -0.31767797 0.60682583 1.2885103 1.9383378 2.1876183 1.2441211 0.022217751 -1.2242208 -2.3839431 -3.4110646 -4.3497076 -4.7045069 -4.6174][-2.2331109 -2.0334337 -1.7910886 -0.97275662 -0.21771908 0.178864 0.25036 -0.22409344 -1.1005585 -2.4436417 -3.5649517 -4.4643135 -5.2652478 -5.5734286 -5.3009605][-3.3357592 -2.8889675 -2.3172827 -2.1804225 -2.147898 -1.7148981 -1.3582919 -2.0998983 -3.2761707 -4.2485566 -5.020093 -5.6871538 -6.3689971 -6.5214758 -6.144783][-4.2689362 -4.3667755 -4.1922083 -3.7002358 -3.4724581 -3.6906757 -3.8784685 -4.1690068 -4.7880387 -5.7674046 -6.2832565 -6.514039 -6.7998228 -6.7334027 -6.3074312][-5.2566867 -5.3731251 -5.2956147 -5.3156166 -5.4526896 -5.3400474 -5.1525822 -5.5299921 -6.028059 -6.460988 -6.5108929 -6.4029346 -6.256587 -6.0550265 -5.8557653][-5.2558432 -5.3867359 -5.5278382 -5.5338135 -5.6510592 -5.9429 -5.9014897 -5.7515545 -5.730473 -5.976759 -5.9924908 -5.7639756 -5.6642857 -5.3354139 -4.9714675][-5.0456967 -5.1522694 -5.3423243 -5.4864454 -5.553731 -5.4884129 -5.1688495 -4.8594012 -4.62038 -4.7047987 -4.9105568 -4.6699986 -4.35211 -4.2029781 -4.1355228]]...]
INFO - root - 2017-12-16 07:28:52.677169: step 48110, loss = 0.32, batch loss = 0.27 (29.2 examples/sec; 0.274 sec/batch; 21h:37m:45s remains)
INFO - root - 2017-12-16 07:28:55.571575: step 48120, loss = 0.20, batch loss = 0.14 (25.6 examples/sec; 0.312 sec/batch; 24h:39m:25s remains)
INFO - root - 2017-12-16 07:28:58.398820: step 48130, loss = 0.19, batch loss = 0.14 (28.1 examples/sec; 0.285 sec/batch; 22h:28m:34s remains)
INFO - root - 2017-12-16 07:29:01.266249: step 48140, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.284 sec/batch; 22h:27m:18s remains)
INFO - root - 2017-12-16 07:29:04.063845: step 48150, loss = 0.27, batch loss = 0.21 (28.7 examples/sec; 0.279 sec/batch; 22h:01m:40s remains)
INFO - root - 2017-12-16 07:29:06.909042: step 48160, loss = 0.39, batch loss = 0.33 (28.2 examples/sec; 0.284 sec/batch; 22h:24m:27s remains)
INFO - root - 2017-12-16 07:29:09.738888: step 48170, loss = 0.28, batch loss = 0.22 (28.0 examples/sec; 0.286 sec/batch; 22h:34m:46s remains)
INFO - root - 2017-12-16 07:29:12.571146: step 48180, loss = 0.38, batch loss = 0.32 (29.0 examples/sec; 0.276 sec/batch; 21h:45m:35s remains)
INFO - root - 2017-12-16 07:29:15.421182: step 48190, loss = 0.28, batch loss = 0.23 (28.5 examples/sec; 0.281 sec/batch; 22h:11m:51s remains)
INFO - root - 2017-12-16 07:29:18.294886: step 48200, loss = 0.22, batch loss = 0.16 (27.7 examples/sec; 0.289 sec/batch; 22h:49m:18s remains)
2017-12-16 07:29:18.764008: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7532136 -2.8942225 -3.2826533 -3.6084845 -3.5877852 -3.3636031 -3.1100419 -2.7203872 -2.4033015 -2.188664 -2.0760126 -2.0429766 -1.9833355 -1.9615211 -1.8823483][-2.5117385 -2.7727907 -3.2125864 -3.6959484 -3.8673704 -3.6914592 -3.3533282 -2.9798627 -2.7763224 -2.5922091 -2.5387387 -2.5021834 -2.4076095 -2.2899687 -2.1105037][-2.2698271 -2.5984716 -3.2187581 -3.4896059 -3.4848974 -3.3702176 -3.1742857 -3.0247569 -2.9679785 -3.0460553 -3.1933827 -3.280005 -3.2970624 -3.1146054 -2.8377185][-2.1074293 -2.3123827 -2.7293255 -2.9097548 -2.7999415 -2.496093 -2.1695814 -2.1951628 -2.5430703 -3.1209044 -3.5557208 -3.8626444 -4.0572076 -3.8985617 -3.5423934][-1.7245913 -1.7663584 -1.838877 -1.7669787 -1.4176726 -0.81791806 -0.40500641 -0.45608377 -1.1406279 -2.2610495 -3.2623916 -3.9966815 -4.3282361 -4.3317204 -3.9761143][-1.2996821 -0.99025989 -0.73002124 -0.22558165 0.59795713 1.4075122 1.8747959 1.7639694 0.90811825 -0.49505234 -2.0368314 -3.3997598 -4.2182455 -4.3117881 -3.9315436][-1.0658672 -0.51130223 -0.0036425591 0.92667961 2.1980009 3.345459 4.0830708 3.8796234 2.8247108 1.0052829 -0.9792273 -2.4864326 -3.4928739 -3.787498 -3.5513556][-1.2570422 -0.69143486 -0.1353941 0.91634178 2.4591784 3.8993092 4.7370615 4.499959 3.3861456 1.5865507 -0.43239045 -2.2498386 -3.3497095 -3.6041803 -3.3522873][-1.5541174 -1.5342262 -1.0301695 -0.3164196 0.86267853 2.4872098 3.689187 3.5794792 2.5344067 0.77979851 -1.0866363 -2.7326076 -3.7349567 -3.8827381 -3.3983848][-2.138906 -2.5331507 -2.6655898 -2.17836 -1.0966737 0.083303452 0.94909954 1.0963817 0.46246386 -0.97754097 -2.6090312 -3.8822484 -4.5116963 -4.3598514 -3.73047][-3.040576 -3.3881304 -3.6141834 -3.7279773 -3.3998845 -2.4275339 -1.5069902 -1.4596663 -2.044806 -3.19537 -4.4694476 -5.4074283 -5.6548953 -5.1267376 -4.2793927][-3.4333258 -3.9453509 -4.5331254 -4.7753825 -4.633451 -4.4627047 -3.9825897 -3.7064326 -3.8555069 -4.5825229 -5.5688062 -6.4502831 -6.4483314 -5.7037344 -4.8176727][-3.3536565 -3.6748285 -4.1310425 -4.7147756 -4.989255 -4.9575295 -4.9282932 -4.8126764 -4.7294288 -5.0339437 -5.6762218 -6.2494593 -6.1362233 -5.4636078 -4.6538877][-2.928287 -2.8617377 -3.0547276 -3.5014081 -4.0621233 -4.16554 -4.0996823 -4.4552803 -4.829175 -4.7264395 -4.6852264 -4.9279652 -4.9266148 -4.3248415 -3.7194304][-2.3917887 -2.0190146 -1.9634397 -2.2656991 -2.8916702 -3.4338229 -3.7713203 -3.6617322 -3.5644314 -3.7215583 -3.9627273 -3.899147 -3.5734444 -2.9764276 -2.3814008]]...]
INFO - root - 2017-12-16 07:29:21.606571: step 48210, loss = 0.24, batch loss = 0.18 (27.6 examples/sec; 0.290 sec/batch; 22h:52m:03s remains)
INFO - root - 2017-12-16 07:29:24.470642: step 48220, loss = 0.28, batch loss = 0.22 (28.5 examples/sec; 0.281 sec/batch; 22h:10m:21s remains)
INFO - root - 2017-12-16 07:29:27.294850: step 48230, loss = 0.28, batch loss = 0.22 (28.6 examples/sec; 0.280 sec/batch; 22h:06m:26s remains)
INFO - root - 2017-12-16 07:29:30.116912: step 48240, loss = 0.20, batch loss = 0.14 (28.1 examples/sec; 0.284 sec/batch; 22h:27m:48s remains)
INFO - root - 2017-12-16 07:29:32.919002: step 48250, loss = 0.34, batch loss = 0.28 (28.7 examples/sec; 0.279 sec/batch; 22h:02m:23s remains)
INFO - root - 2017-12-16 07:29:35.741808: step 48260, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.285 sec/batch; 22h:31m:40s remains)
INFO - root - 2017-12-16 07:29:38.537469: step 48270, loss = 0.25, batch loss = 0.20 (29.0 examples/sec; 0.276 sec/batch; 21h:47m:37s remains)
INFO - root - 2017-12-16 07:29:41.410068: step 48280, loss = 0.38, batch loss = 0.32 (29.3 examples/sec; 0.273 sec/batch; 21h:34m:20s remains)
INFO - root - 2017-12-16 07:29:44.240519: step 48290, loss = 0.20, batch loss = 0.14 (28.9 examples/sec; 0.277 sec/batch; 21h:50m:19s remains)
INFO - root - 2017-12-16 07:29:47.139743: step 48300, loss = 0.24, batch loss = 0.18 (27.9 examples/sec; 0.286 sec/batch; 22h:36m:27s remains)
2017-12-16 07:29:47.601876: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3108358 -4.758646 -5.0955763 -5.2800169 -5.3671346 -5.3386374 -5.3174248 -5.3675432 -5.6488371 -6.1290541 -6.4938231 -6.9111905 -6.9704533 -6.4232578 -5.6034374][-5.0392642 -5.532733 -5.782764 -5.828301 -5.8534269 -5.9683108 -6.130044 -6.3145027 -6.6185417 -7.2612247 -8.0788879 -8.7749062 -8.9758472 -8.50083 -7.4852266][-5.4153709 -5.9330921 -6.0703249 -5.8330622 -5.5084405 -5.4097161 -5.4410696 -6.0104108 -6.8366432 -7.6753607 -8.6776276 -9.6872425 -10.407887 -10.260931 -9.2892418][-5.60799 -5.8876944 -5.7583356 -5.1865892 -4.5264983 -4.0010219 -3.7987239 -4.1689792 -4.9034348 -6.4103456 -8.0719414 -9.41753 -10.462313 -10.944052 -10.503873][-5.1151257 -5.1038218 -4.5817561 -3.66566 -2.4921913 -1.3038657 -0.65493345 -0.78639269 -1.6115646 -3.3924534 -5.5423584 -7.8668652 -9.7534513 -10.783506 -10.735364][-4.4442682 -4.1654787 -3.22187 -1.630882 0.27639627 1.7502046 2.7099137 2.9396172 2.29782 0.35305834 -2.3915505 -5.4693413 -8.1486883 -9.7558289 -10.150876][-4.1959491 -3.6223693 -2.4305847 -0.75112963 1.7651567 4.1899471 5.8876762 6.1226521 5.2114935 3.3403358 0.44488859 -3.1198997 -6.2721648 -8.22621 -8.7670383][-4.4906158 -3.8060019 -2.6134648 -0.60780287 1.9740615 4.4825706 6.7575703 7.5818844 6.8419628 4.6686554 1.5631523 -1.9741282 -5.0723467 -6.9806576 -7.4373827][-4.8722634 -4.6400275 -3.487185 -1.596699 0.57956314 3.1505361 5.4544125 6.242404 5.8543167 4.1682739 1.3885956 -1.8117168 -4.6007395 -6.3340225 -6.6305866][-4.9089313 -5.2979083 -4.9660587 -3.4619274 -1.5940857 0.42040157 2.304739 3.4898944 3.4275489 1.8875408 -0.16371775 -2.5461702 -4.8689146 -6.3320613 -6.742981][-5.7462454 -6.1109123 -5.9709949 -5.1495304 -3.8579273 -2.4265835 -1.1751397 -0.37357616 -0.24333286 -1.0183053 -2.4817023 -4.1879177 -5.6407571 -6.5659676 -6.866375][-6.1483212 -6.4973645 -6.6896038 -6.3307157 -5.4594922 -4.4549026 -3.655899 -3.2133269 -3.1930046 -3.7331185 -4.6159534 -5.7116928 -6.6062312 -7.0324273 -7.0676775][-5.7005186 -6.103312 -6.4128027 -6.3558884 -6.2164631 -5.6818991 -4.9237452 -4.5908613 -4.5685921 -4.7419014 -5.3286982 -6.165822 -6.6594992 -6.9817524 -6.9404974][-5.0439544 -5.1873007 -5.3122334 -5.4355869 -5.5757208 -5.4093609 -5.2426906 -5.1561527 -5.1135712 -5.2149215 -5.2396441 -5.4514608 -5.8820333 -6.0875373 -6.0098863][-4.3354673 -4.2743549 -4.1462317 -4.1570792 -4.3071094 -4.3709254 -4.4280658 -4.450048 -4.4890013 -4.6240335 -4.7597 -4.6610603 -4.456584 -4.5133433 -4.6135011]]...]
INFO - root - 2017-12-16 07:29:50.460481: step 48310, loss = 0.21, batch loss = 0.15 (28.0 examples/sec; 0.286 sec/batch; 22h:32m:16s remains)
INFO - root - 2017-12-16 07:29:53.278490: step 48320, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 22h:27m:42s remains)
INFO - root - 2017-12-16 07:29:56.100257: step 48330, loss = 0.27, batch loss = 0.21 (28.3 examples/sec; 0.282 sec/batch; 22h:17m:12s remains)
INFO - root - 2017-12-16 07:29:58.884517: step 48340, loss = 0.28, batch loss = 0.22 (29.5 examples/sec; 0.271 sec/batch; 21h:22m:45s remains)
INFO - root - 2017-12-16 07:30:01.684001: step 48350, loss = 0.28, batch loss = 0.22 (29.6 examples/sec; 0.270 sec/batch; 21h:21m:00s remains)
INFO - root - 2017-12-16 07:30:04.499536: step 48360, loss = 0.23, batch loss = 0.17 (26.8 examples/sec; 0.299 sec/batch; 23h:33m:55s remains)
INFO - root - 2017-12-16 07:30:07.347796: step 48370, loss = 0.27, batch loss = 0.21 (27.5 examples/sec; 0.291 sec/batch; 22h:55m:42s remains)
INFO - root - 2017-12-16 07:30:10.214742: step 48380, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.278 sec/batch; 21h:58m:06s remains)
INFO - root - 2017-12-16 07:30:13.003837: step 48390, loss = 0.23, batch loss = 0.18 (28.7 examples/sec; 0.279 sec/batch; 21h:58m:59s remains)
INFO - root - 2017-12-16 07:30:15.799476: step 48400, loss = 0.21, batch loss = 0.15 (28.4 examples/sec; 0.282 sec/batch; 22h:12m:59s remains)
2017-12-16 07:30:16.265109: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5135417 -4.3123341 -4.1527205 -4.5198345 -5.1898022 -6.0256987 -6.625226 -6.7662759 -6.7537503 -6.268384 -5.7720184 -5.4808979 -4.8867111 -4.1966953 -3.5351255][-5.1653438 -5.5954738 -5.9149036 -6.093369 -6.8023067 -6.9797688 -7.1225538 -7.0712214 -6.6349869 -6.2469397 -5.9942608 -5.5714731 -4.8568072 -4.2412362 -3.7244573][-5.7789741 -6.3281384 -6.7213488 -6.8828259 -6.7689648 -6.3849549 -6.3946109 -6.1264219 -5.7676749 -5.420166 -5.1825218 -5.2753358 -5.1625228 -4.7309027 -4.1791716][-6.0394478 -7.0691514 -7.4167728 -7.0645156 -6.392664 -5.2547407 -4.2857938 -3.6745372 -3.5447242 -4.1022906 -4.749505 -5.0068779 -5.0483537 -5.0999475 -4.8682017][-5.9160857 -6.9678388 -7.1803174 -6.36428 -4.6488161 -2.8515825 -1.5088065 -0.51244211 -0.65115976 -1.8098061 -3.1131725 -4.5476093 -5.4356327 -5.3922634 -4.9273849][-4.5137339 -5.3302875 -5.6351204 -4.5471244 -2.0592041 0.53007221 2.5257816 3.4422421 2.7419705 0.85164356 -1.6631281 -3.8239863 -5.0529275 -5.5930119 -5.2951326][-3.2820983 -4.0067053 -3.7967434 -2.2380867 0.11949968 2.7783837 5.0276661 6.020709 5.0846043 2.5012555 -0.53160691 -3.196558 -4.9672327 -5.495717 -5.0597286][-2.6267152 -3.374063 -3.2571859 -1.6432316 0.95545387 3.9300623 5.9928207 6.2289886 5.0924606 2.5518541 -0.67878103 -3.3694127 -4.9193864 -5.3293815 -4.9254255][-2.500078 -2.9940484 -2.6324947 -1.4745135 0.17036009 2.5417929 4.6433525 5.1808796 3.7914438 1.0061436 -1.8887672 -4.2348933 -5.5978804 -5.6992645 -5.10093][-2.4714632 -3.1014812 -2.9291759 -1.9557922 -0.42227983 0.96401453 1.8703566 2.2539282 1.1811318 -1.0954227 -3.6080582 -5.6357369 -6.4129543 -6.2525225 -5.571219][-3.1944613 -3.4816663 -3.3851333 -3.1658828 -2.6272471 -1.445816 -0.28335238 -0.38052797 -1.6996932 -3.2922988 -4.9540386 -6.4861808 -7.0623136 -6.641438 -5.7807717][-4.2171893 -4.4426804 -4.4756703 -4.0385704 -3.5067744 -3.2749848 -3.0228894 -2.9828362 -3.4961243 -4.8012843 -6.2170591 -6.9470892 -7.0042915 -6.528914 -5.7559228][-5.11213 -5.1979976 -5.2663889 -5.1961727 -5.0243716 -4.43453 -4.0252037 -4.4619222 -5.1149888 -5.7613487 -6.546772 -7.0539637 -6.9593244 -6.3487544 -5.6242828][-5.924511 -5.7949357 -5.7483339 -5.6676722 -5.482327 -5.1031985 -4.957521 -4.8271723 -5.0841703 -5.805696 -6.3149991 -6.3918352 -6.1536183 -5.66522 -5.1502819][-6.5723124 -6.4543033 -6.3011522 -6.0946054 -5.763567 -5.3201981 -5.0564518 -4.8279366 -4.8801556 -5.2131047 -5.526392 -5.5470371 -5.2186642 -4.660183 -4.2134776]]...]
INFO - root - 2017-12-16 07:30:19.025906: step 48410, loss = 0.23, batch loss = 0.17 (29.2 examples/sec; 0.274 sec/batch; 21h:38m:13s remains)
INFO - root - 2017-12-16 07:30:21.902198: step 48420, loss = 0.20, batch loss = 0.14 (29.0 examples/sec; 0.276 sec/batch; 21h:46m:29s remains)
INFO - root - 2017-12-16 07:30:24.764860: step 48430, loss = 0.25, batch loss = 0.19 (29.1 examples/sec; 0.275 sec/batch; 21h:42m:46s remains)
INFO - root - 2017-12-16 07:30:27.593747: step 48440, loss = 0.29, batch loss = 0.23 (28.5 examples/sec; 0.281 sec/batch; 22h:09m:09s remains)
INFO - root - 2017-12-16 07:30:30.421119: step 48450, loss = 0.23, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 22h:17m:54s remains)
INFO - root - 2017-12-16 07:30:33.271262: step 48460, loss = 0.24, batch loss = 0.19 (26.8 examples/sec; 0.298 sec/batch; 23h:31m:06s remains)
INFO - root - 2017-12-16 07:30:36.120553: step 48470, loss = 0.45, batch loss = 0.39 (27.5 examples/sec; 0.291 sec/batch; 22h:57m:00s remains)
INFO - root - 2017-12-16 07:30:38.933284: step 48480, loss = 0.24, batch loss = 0.18 (27.3 examples/sec; 0.293 sec/batch; 23h:06m:11s remains)
INFO - root - 2017-12-16 07:30:41.774371: step 48490, loss = 0.22, batch loss = 0.16 (28.3 examples/sec; 0.283 sec/batch; 22h:19m:15s remains)
INFO - root - 2017-12-16 07:30:44.615964: step 48500, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.283 sec/batch; 22h:17m:34s remains)
2017-12-16 07:30:45.089911: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0385351 -5.4032917 -5.4000411 -5.2741222 -5.083746 -4.9648943 -4.9031606 -4.9959836 -5.0973587 -5.2710652 -5.4429827 -5.6489115 -5.8523092 -5.7753959 -5.5501866][-5.5827093 -6.0379791 -5.9583292 -5.7780066 -5.2841268 -5.1770067 -5.188405 -5.2844625 -5.5681133 -5.8701735 -6.1960039 -6.491519 -6.7356935 -6.6766348 -6.4436741][-5.8521032 -6.2198343 -6.1412411 -5.7015929 -4.9934363 -4.6709738 -4.3367739 -4.6000972 -5.076251 -5.5506659 -6.202734 -6.7587957 -7.3178415 -7.4138761 -7.2768912][-5.4617224 -5.935164 -5.7060409 -4.7047696 -3.5069919 -2.76272 -2.371778 -2.6221306 -3.1557479 -4.1416225 -5.2384892 -6.1907511 -7.064291 -7.5720863 -7.884757][-4.6418724 -4.7822323 -4.1750417 -3.0472186 -1.4230039 -0.0038990974 0.82163429 0.70162535 -0.10860586 -1.5120144 -2.9057734 -4.3973351 -5.7833381 -6.7694697 -7.5194483][-3.9763517 -3.9611795 -3.2668676 -1.5754876 0.70249081 2.6494226 3.8809633 4.07215 3.3685102 1.7477841 -0.053628445 -1.9385788 -3.6817906 -5.2641864 -6.4117427][-3.7966573 -3.5726256 -2.5957522 -0.68452954 1.8430457 4.2293282 6.1653366 6.69285 5.9726858 4.3042469 2.2716374 0.17037868 -2.0648358 -3.843802 -5.1496778][-3.9178166 -3.6736097 -2.9158425 -0.93730187 1.9499741 4.6311369 6.724822 7.334753 6.9611292 5.3337622 3.2577677 1.1560974 -1.0536976 -2.9364009 -4.6051364][-4.5841603 -4.4064751 -3.694154 -2.0731738 0.18748283 2.8048105 5.178503 6.0326347 5.7807522 4.5090647 2.988996 1.1104903 -0.97985625 -2.7359774 -4.3173785][-5.7209969 -5.8829017 -5.3738508 -4.0285587 -2.2256925 0.057436943 2.031045 2.9658504 3.0534062 2.2993402 1.1593394 -0.2113986 -1.8421168 -3.2944181 -4.5533481][-7.037838 -7.2565632 -6.9708056 -6.1322651 -4.9526682 -3.3177652 -1.5238922 -0.52573419 -0.28397655 -0.71517587 -1.3610671 -2.0858119 -3.2253566 -4.2239108 -5.3097382][-7.9702387 -8.2235 -8.1066523 -7.5904961 -6.8433161 -5.83757 -4.69074 -3.9185836 -3.426851 -3.3679056 -3.4812052 -4.0506415 -4.7962141 -5.4180546 -6.1733541][-8.4788342 -8.7239866 -8.5803585 -8.1213493 -7.6278982 -7.102334 -6.43169 -5.8172383 -5.3309574 -5.0176988 -4.9565806 -5.1668382 -5.3954144 -5.9086571 -6.5621452][-8.4622078 -8.595335 -8.4141846 -8.0134926 -7.6566877 -7.1942906 -6.7219586 -6.4909024 -6.1604929 -5.6938677 -5.3275051 -5.4346886 -5.8162675 -5.9988184 -6.1750717][-7.416019 -7.498014 -7.1980991 -6.8936682 -6.7059259 -6.3761368 -6.1601591 -6.024827 -5.9069681 -5.6978641 -5.3288536 -5.2296958 -5.2984796 -5.4153252 -5.5769515]]...]
INFO - root - 2017-12-16 07:30:47.930933: step 48510, loss = 0.27, batch loss = 0.21 (26.1 examples/sec; 0.307 sec/batch; 24h:11m:53s remains)
INFO - root - 2017-12-16 07:30:50.809287: step 48520, loss = 0.30, batch loss = 0.24 (27.4 examples/sec; 0.292 sec/batch; 23h:03m:03s remains)
INFO - root - 2017-12-16 07:30:53.640038: step 48530, loss = 0.22, batch loss = 0.16 (27.8 examples/sec; 0.288 sec/batch; 22h:42m:51s remains)
INFO - root - 2017-12-16 07:30:56.486517: step 48540, loss = 0.33, batch loss = 0.27 (28.2 examples/sec; 0.284 sec/batch; 22h:23m:06s remains)
INFO - root - 2017-12-16 07:30:59.380354: step 48550, loss = 0.26, batch loss = 0.21 (28.1 examples/sec; 0.285 sec/batch; 22h:27m:27s remains)
INFO - root - 2017-12-16 07:31:02.281348: step 48560, loss = 0.21, batch loss = 0.15 (27.7 examples/sec; 0.288 sec/batch; 22h:45m:05s remains)
INFO - root - 2017-12-16 07:31:05.125956: step 48570, loss = 0.22, batch loss = 0.16 (26.6 examples/sec; 0.300 sec/batch; 23h:40m:55s remains)
INFO - root - 2017-12-16 07:31:07.977752: step 48580, loss = 0.28, batch loss = 0.22 (27.9 examples/sec; 0.287 sec/batch; 22h:39m:04s remains)
INFO - root - 2017-12-16 07:31:10.825597: step 48590, loss = 0.29, batch loss = 0.23 (28.3 examples/sec; 0.283 sec/batch; 22h:16m:53s remains)
INFO - root - 2017-12-16 07:31:13.657040: step 48600, loss = 0.25, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 22h:03m:31s remains)
2017-12-16 07:31:14.134358: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5653558 -5.702055 -5.6077495 -5.5338407 -5.5029116 -5.4821305 -5.4714622 -5.5885887 -5.7301416 -5.77864 -5.7917562 -5.6581182 -5.393724 -5.0104856 -4.6914511][-5.649982 -5.5771818 -5.5010796 -5.5440569 -5.5541563 -5.7809095 -5.9735804 -5.962523 -5.9924078 -6.2281318 -6.3210735 -6.0698767 -5.7578111 -5.4133315 -5.1599083][-4.9542642 -4.92563 -4.9318733 -4.9114485 -5.0042715 -5.2591419 -5.2966547 -5.5119867 -5.7850108 -5.8947182 -6.0264311 -6.1498694 -6.0662503 -5.644927 -5.3566713][-4.0625033 -3.6289456 -3.4395092 -3.5510204 -3.673049 -3.8750331 -3.8997521 -4.0363984 -4.224112 -4.5961895 -4.9335957 -5.2067108 -5.3762875 -5.2770743 -5.1629415][-2.4942396 -1.726862 -1.42764 -1.6189094 -1.8569655 -1.9190397 -1.728807 -1.8183992 -2.0164206 -2.530241 -3.1467054 -3.7034533 -4.124145 -4.2396774 -4.1714616][-1.4250593 -0.53419852 -0.079402447 0.022660732 0.052631855 0.14740992 0.58229351 0.81416178 0.6541338 0.070501328 -0.70872188 -1.7677383 -2.6251097 -3.0130968 -3.0952263][-0.661072 0.17909098 0.52378321 0.62443352 0.94719124 1.6648574 2.6331468 3.049109 2.850565 2.224586 1.3846517 0.26477528 -0.7741847 -1.5846899 -2.0583045][-0.21354818 0.47474909 0.79142046 1.0134587 1.2821131 1.9664402 3.03583 3.8355923 3.9074469 3.262526 2.2853117 1.0833545 0.060731411 -0.58507252 -1.1072309][-0.32824755 0.033436775 0.15951395 0.33696651 0.75590372 1.5294156 2.5212607 3.0983443 3.1799345 2.808507 2.0822215 1.0268106 0.0804863 -0.5370729 -0.930655][-0.64895177 -0.52745771 -0.65473795 -0.56433654 -0.24047709 0.35338354 1.1906209 1.7193294 1.8080177 1.4104786 0.804029 0.1046114 -0.52179718 -0.85869789 -1.0698395][-1.3119853 -1.1606662 -1.1516547 -1.1767869 -0.986619 -0.646338 -0.15001202 0.059214592 0.06185627 -0.27001143 -0.71091914 -1.2066474 -1.5404265 -1.5633075 -1.5856094][-1.6281149 -1.4076433 -1.4225354 -1.4885097 -1.3531492 -1.1593509 -0.97457886 -1.0763376 -1.2981145 -1.8074176 -2.2381659 -2.4009082 -2.4556007 -2.0714741 -1.7720978][-1.8141863 -1.4878564 -1.3722095 -1.4214149 -1.4435902 -1.3688617 -1.2844441 -1.6787679 -2.2070527 -2.7853103 -3.2055974 -3.2773447 -3.2188344 -2.6677005 -2.0504313][-1.8923171 -1.4324689 -1.2003901 -1.1463919 -1.0747302 -1.0891254 -1.0302956 -1.5867596 -2.416754 -3.2605894 -3.7782071 -3.7871075 -3.5109689 -2.9117825 -2.3069222][-1.9605031 -1.4364138 -1.1078587 -1.0667453 -0.9681468 -0.82355332 -0.72026587 -1.3264184 -2.2476995 -3.1801221 -3.8724675 -4.0439987 -3.9445446 -3.3365488 -2.71358]]...]
INFO - root - 2017-12-16 07:31:16.963509: step 48610, loss = 0.30, batch loss = 0.24 (28.0 examples/sec; 0.286 sec/batch; 22h:32m:07s remains)
INFO - root - 2017-12-16 07:31:19.820099: step 48620, loss = 0.28, batch loss = 0.23 (27.5 examples/sec; 0.291 sec/batch; 22h:58m:34s remains)
INFO - root - 2017-12-16 07:31:22.599209: step 48630, loss = 0.37, batch loss = 0.31 (27.7 examples/sec; 0.289 sec/batch; 22h:47m:52s remains)
INFO - root - 2017-12-16 07:31:25.433378: step 48640, loss = 0.36, batch loss = 0.30 (28.8 examples/sec; 0.278 sec/batch; 21h:55m:07s remains)
INFO - root - 2017-12-16 07:31:28.272483: step 48650, loss = 0.33, batch loss = 0.27 (25.6 examples/sec; 0.312 sec/batch; 24h:36m:08s remains)
INFO - root - 2017-12-16 07:31:31.132497: step 48660, loss = 0.35, batch loss = 0.29 (27.9 examples/sec; 0.287 sec/batch; 22h:35m:40s remains)
INFO - root - 2017-12-16 07:31:33.920577: step 48670, loss = 0.21, batch loss = 0.15 (28.8 examples/sec; 0.278 sec/batch; 21h:54m:59s remains)
INFO - root - 2017-12-16 07:31:36.772824: step 48680, loss = 0.25, batch loss = 0.19 (26.2 examples/sec; 0.305 sec/batch; 24h:04m:50s remains)
INFO - root - 2017-12-16 07:31:39.637925: step 48690, loss = 0.40, batch loss = 0.34 (26.8 examples/sec; 0.299 sec/batch; 23h:32m:30s remains)
INFO - root - 2017-12-16 07:31:42.420490: step 48700, loss = 0.36, batch loss = 0.30 (29.4 examples/sec; 0.272 sec/batch; 21h:28m:15s remains)
2017-12-16 07:31:42.901315: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7671533 -3.0371451 -3.2124231 -3.1866834 -3.0607278 -2.8296368 -2.7585852 -3.0279436 -3.3278017 -3.4266353 -3.3671727 -3.5639329 -3.7901044 -4.0430784 -4.1281161][-3.0163188 -3.0516272 -3.0598288 -3.0400357 -2.7563777 -2.6871705 -2.8999386 -2.96243 -3.2036233 -3.5384216 -3.9278221 -4.2241125 -4.3687682 -4.8363142 -5.204792][-3.2686327 -3.3192911 -3.2663393 -3.2275379 -2.9976139 -2.6600528 -2.653969 -2.9715874 -3.4494307 -3.612108 -3.8191218 -4.2833877 -4.6532087 -5.2239609 -5.4519372][-3.4385285 -3.5139875 -3.460932 -3.129642 -2.6516171 -2.4054587 -2.2260227 -2.166641 -2.4094925 -2.9556346 -3.5143268 -3.8928196 -4.1454039 -4.7963037 -5.1634269][-3.4405198 -3.4758787 -3.2839241 -2.8629842 -2.2332878 -1.672296 -1.2835324 -1.1486964 -1.2052963 -1.4659688 -1.9885328 -2.6201825 -3.2074928 -3.6461856 -3.9740818][-3.765799 -3.613112 -3.2343025 -2.5465083 -1.6298313 -0.82068133 -0.252357 0.10486221 0.2170949 0.10622072 -0.39556122 -0.8369112 -1.2281697 -1.9108562 -2.5690746][-3.4858334 -3.4593446 -3.0866094 -2.2150583 -1.0358572 -0.22649336 0.54669333 1.0984364 1.4215789 1.5767136 1.3031273 0.9970212 0.63285637 0.094244957 -0.55997372][-3.0903661 -3.0472145 -2.7151337 -2.0492864 -0.95741868 -0.080953121 0.66570807 1.2766356 1.8003497 2.2517648 2.4264841 2.3401828 2.2100506 1.7484412 0.83875847][-2.914427 -2.840523 -2.6557577 -2.1237841 -1.2573738 -0.48025441 0.13768435 0.7436657 1.4614162 2.1098447 2.5254846 2.774991 2.809145 2.3715181 1.6077704][-3.1001956 -3.0113764 -2.9868174 -2.7333894 -2.2042139 -1.6331096 -0.97465277 -0.44615746 0.20739555 0.898993 1.4179854 1.7113466 1.7401505 1.4445667 0.7053647][-3.134562 -3.2645104 -3.4771214 -3.3225603 -3.0346575 -2.6930888 -2.092556 -1.592314 -0.97436047 -0.50481033 -0.0940299 0.1365037 0.0954442 -0.15451002 -0.66100287][-3.4723442 -3.4902208 -3.6347768 -3.8597589 -3.7848535 -3.3932071 -2.9034243 -2.604212 -2.0937307 -1.7912169 -1.7393005 -1.667639 -1.7772653 -2.0349827 -2.311141][-3.3973758 -3.4241295 -3.4322994 -3.3667374 -3.2430129 -3.1185527 -2.8274937 -2.7354579 -2.6189241 -2.749506 -3.153028 -3.3041804 -3.6290998 -3.7705085 -3.8548241][-3.3214192 -3.1377468 -2.9063883 -2.7198451 -2.4207766 -2.2213726 -2.1568792 -2.3969233 -2.6650224 -3.100034 -3.8165631 -4.3733735 -4.7965226 -4.7865043 -4.7059255][-2.9161978 -2.5068798 -1.9209847 -1.5794349 -1.2625232 -1.1399982 -1.0880194 -1.6164191 -2.3433928 -3.1815109 -4.1263161 -4.7109718 -5.1899595 -5.2045841 -4.8495831]]...]
INFO - root - 2017-12-16 07:31:45.717126: step 48710, loss = 0.28, batch loss = 0.22 (28.8 examples/sec; 0.278 sec/batch; 21h:54m:18s remains)
INFO - root - 2017-12-16 07:31:48.553616: step 48720, loss = 0.22, batch loss = 0.16 (27.4 examples/sec; 0.292 sec/batch; 23h:03m:02s remains)
INFO - root - 2017-12-16 07:31:51.422022: step 48730, loss = 0.30, batch loss = 0.24 (26.4 examples/sec; 0.303 sec/batch; 23h:53m:59s remains)
INFO - root - 2017-12-16 07:31:54.297152: step 48740, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.286 sec/batch; 22h:33m:14s remains)
INFO - root - 2017-12-16 07:31:57.145357: step 48750, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 22h:18m:01s remains)
INFO - root - 2017-12-16 07:31:59.957093: step 48760, loss = 0.25, batch loss = 0.19 (28.5 examples/sec; 0.281 sec/batch; 22h:08m:56s remains)
INFO - root - 2017-12-16 07:32:02.791643: step 48770, loss = 0.27, batch loss = 0.21 (27.0 examples/sec; 0.296 sec/batch; 23h:19m:38s remains)
INFO - root - 2017-12-16 07:32:05.592487: step 48780, loss = 0.27, batch loss = 0.21 (28.4 examples/sec; 0.282 sec/batch; 22h:12m:21s remains)
INFO - root - 2017-12-16 07:32:08.431252: step 48790, loss = 0.36, batch loss = 0.30 (27.1 examples/sec; 0.295 sec/batch; 23h:16m:36s remains)
INFO - root - 2017-12-16 07:32:11.256378: step 48800, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.277 sec/batch; 21h:51m:18s remains)
2017-12-16 07:32:11.711163: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2329025 -2.0937181 -1.9804947 -1.8446078 -1.8058407 -1.9534564 -2.0347102 -1.870594 -1.7697258 -1.7791498 -1.7787235 -1.5088346 -1.0980814 -0.7386446 -0.51293254][-2.3309686 -2.3028858 -2.2844641 -2.2700472 -2.2670798 -2.351037 -2.3814814 -2.2175508 -1.9975934 -1.8282537 -1.8283732 -1.5664308 -1.1026478 -0.5375061 -0.12898779][-2.4077623 -2.5563684 -2.7332523 -2.9441435 -3.1297932 -3.2373285 -3.2291536 -2.9140298 -2.6444731 -2.4157436 -2.2505569 -2.0885251 -1.7870858 -1.1610432 -0.69887543][-2.6973426 -3.1161251 -3.6141095 -4.0173221 -4.2510371 -4.0807981 -3.7109563 -3.2177267 -2.8612189 -2.574254 -2.4182122 -2.3900149 -2.1312091 -1.5687287 -1.0812156][-3.2161503 -3.8533192 -4.495048 -4.8265176 -4.8071055 -4.2948337 -3.4702296 -2.6457219 -2.2275946 -2.0226033 -2.081125 -2.3682029 -2.402312 -2.1418009 -1.8051312][-3.8128562 -4.5367646 -5.0357566 -5.0902619 -4.682158 -3.7570853 -2.5708809 -1.3337052 -0.73106194 -0.68545985 -1.0955496 -1.7730865 -2.2983866 -2.4613252 -2.3303802][-3.8637695 -4.5890007 -4.8561697 -4.5827961 -3.8517432 -2.4846635 -0.84585333 0.40465212 0.91778517 0.75769424 0.015468597 -1.0767386 -1.9908473 -2.4022872 -2.3666403][-3.616466 -4.1282334 -4.2570448 -3.5738153 -2.3653455 -0.68853927 1.0219555 2.2904716 2.6203418 2.1198401 1.0568576 -0.33021832 -1.3612404 -1.9542272 -2.1247942][-3.4682767 -3.6881442 -3.5819614 -2.8175812 -1.466187 0.22085762 1.6816034 2.7863908 3.0108047 2.393189 1.3271923 0.016954899 -0.97044706 -1.5633495 -1.8042734][-3.3724046 -3.4688463 -3.2146096 -2.5238883 -1.3883531 0.042878628 1.2679963 2.0328131 2.1616244 1.7466183 0.81303835 -0.32685757 -1.2661974 -1.7479136 -1.9204473][-3.6701658 -3.7018461 -3.5299568 -2.9816866 -2.0259831 -0.83107495 0.067613125 0.4753623 0.72117472 0.55349016 -0.054469109 -0.85136747 -1.4945278 -1.9066143 -2.043608][-4.0767136 -4.1383715 -4.0150051 -3.7193789 -3.3002806 -2.4636755 -1.7619131 -1.5130637 -1.2899518 -1.1919541 -1.2432621 -1.5305452 -1.8824823 -2.0646827 -2.1104012][-4.5412121 -4.6497221 -4.6797676 -4.6324325 -4.5755696 -4.1612716 -3.546273 -3.0930629 -2.5587444 -2.0284417 -1.7175276 -1.5243468 -1.6867409 -1.9203796 -2.0321431][-4.9126763 -5.0185461 -5.1330881 -5.3033414 -5.40806 -5.3571386 -4.7741776 -3.9918938 -3.0855062 -2.2135034 -1.6311252 -1.158993 -1.3126209 -1.6580427 -2.0430081][-5.026525 -5.1558261 -5.4355478 -5.783112 -5.8612518 -5.9925408 -5.4992347 -4.5210423 -3.2431889 -2.2041316 -1.5361123 -0.9898572 -1.094533 -1.5736327 -2.0718348]]...]
INFO - root - 2017-12-16 07:32:14.547901: step 48810, loss = 0.24, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 22h:17m:43s remains)
INFO - root - 2017-12-16 07:32:17.335708: step 48820, loss = 0.23, batch loss = 0.17 (28.8 examples/sec; 0.278 sec/batch; 21h:54m:44s remains)
INFO - root - 2017-12-16 07:32:20.206521: step 48830, loss = 0.30, batch loss = 0.25 (29.2 examples/sec; 0.274 sec/batch; 21h:34m:39s remains)
INFO - root - 2017-12-16 07:32:23.001294: step 48840, loss = 0.22, batch loss = 0.16 (29.4 examples/sec; 0.272 sec/batch; 21h:27m:53s remains)
INFO - root - 2017-12-16 07:32:25.819262: step 48850, loss = 0.32, batch loss = 0.26 (29.2 examples/sec; 0.274 sec/batch; 21h:34m:00s remains)
INFO - root - 2017-12-16 07:32:28.649034: step 48860, loss = 0.29, batch loss = 0.23 (28.0 examples/sec; 0.286 sec/batch; 22h:30m:25s remains)
INFO - root - 2017-12-16 07:32:31.432198: step 48870, loss = 0.31, batch loss = 0.25 (29.4 examples/sec; 0.272 sec/batch; 21h:24m:17s remains)
INFO - root - 2017-12-16 07:32:34.263038: step 48880, loss = 0.27, batch loss = 0.21 (27.9 examples/sec; 0.286 sec/batch; 22h:33m:46s remains)
INFO - root - 2017-12-16 07:32:37.083806: step 48890, loss = 0.24, batch loss = 0.18 (28.3 examples/sec; 0.282 sec/batch; 22h:14m:43s remains)
INFO - root - 2017-12-16 07:32:39.905704: step 48900, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 22h:20m:07s remains)
2017-12-16 07:32:40.374056: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2239306 -3.3114605 -3.4740651 -3.4775991 -3.4257655 -3.1762686 -3.1206498 -3.033716 -2.9121752 -2.9757609 -2.8774557 -2.8788805 -2.9687195 -3.2034597 -3.3736629][-2.8658495 -3.0729299 -3.3319244 -3.2741876 -3.2173285 -2.8484874 -2.5823174 -2.4198914 -2.3473482 -2.3001218 -2.2075467 -2.2361307 -2.4553075 -2.7190571 -2.9659829][-2.5288947 -2.7742469 -3.0722928 -3.1055682 -3.0411448 -2.6085598 -2.1451073 -1.8348866 -1.6746283 -1.5201619 -1.389323 -1.4598548 -1.8441262 -2.2770531 -2.5799994][-2.2951353 -2.7666402 -3.2751064 -3.3432431 -3.1483626 -2.7417679 -2.2015924 -1.7624862 -1.4710538 -1.2123957 -1.0241771 -1.0713117 -1.5620372 -2.0968556 -2.5513091][-2.2456131 -2.9048834 -3.4896679 -3.6943769 -3.5930357 -3.0831769 -2.3339477 -1.8140407 -1.4787371 -1.1705351 -1.0289023 -1.1122885 -1.5658064 -2.2102911 -2.7837806][-2.4061046 -3.1040063 -3.6811559 -3.891855 -3.667531 -3.0875709 -2.3233562 -1.6850927 -1.299444 -1.0117023 -0.83749342 -0.89253569 -1.4060941 -2.2609906 -3.0906022][-2.3650682 -2.9623766 -3.3695536 -3.5401692 -3.33072 -2.5801597 -1.7122905 -1.1160543 -0.78410316 -0.59202051 -0.48477983 -0.65920544 -1.2737777 -2.2411323 -3.3647661][-1.9798694 -2.5510747 -2.9831192 -3.2052102 -3.031611 -2.3028896 -1.5131326 -0.84811592 -0.36983633 -0.18341303 -0.1986475 -0.48465419 -1.1864543 -2.3168967 -3.5791829][-1.8665295 -2.3993678 -2.7846808 -2.970067 -2.8195748 -2.2275174 -1.5340846 -0.86082983 -0.47830153 -0.24835873 -0.24499273 -0.75792289 -1.4943249 -2.5909443 -3.8907928][-2.1287661 -2.5747941 -2.9392381 -3.0705056 -2.8825293 -2.3264577 -1.8133595 -1.2660115 -0.788316 -0.57335687 -0.69778085 -1.1177671 -1.7576275 -2.8530335 -4.0673614][-2.722435 -3.1382608 -3.4642904 -3.5557768 -3.4874618 -3.0582347 -2.6346436 -2.2402387 -1.8664823 -1.5425036 -1.5258327 -1.8664939 -2.3316674 -3.2574744 -4.256784][-3.7025855 -4.1881795 -4.5241466 -4.6110406 -4.4546237 -4.1116438 -3.871196 -3.4271848 -3.1008124 -2.8382633 -2.7856474 -2.9592168 -3.2355452 -3.9140749 -4.613019][-4.6877623 -5.1769991 -5.54156 -5.7837648 -5.7673612 -5.5561714 -5.4289579 -5.0035682 -4.5774145 -4.0677071 -3.8521135 -3.873914 -3.9323678 -4.370286 -4.9361448][-5.6965137 -6.0627017 -6.2554255 -6.4743137 -6.5519085 -6.4667234 -6.3274512 -5.9210567 -5.5070457 -4.9563365 -4.6105423 -4.4465356 -4.38116 -4.7711706 -5.2203126][-6.2487936 -6.4370127 -6.5351005 -6.6356792 -6.6012459 -6.5418091 -6.4144258 -5.9035158 -5.4037695 -4.917738 -4.5535059 -4.4104013 -4.3953681 -4.8561325 -5.3813062]]...]
INFO - root - 2017-12-16 07:32:43.249579: step 48910, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.281 sec/batch; 22h:09m:54s remains)
INFO - root - 2017-12-16 07:32:46.100390: step 48920, loss = 0.27, batch loss = 0.22 (29.5 examples/sec; 0.272 sec/batch; 21h:23m:46s remains)
INFO - root - 2017-12-16 07:32:48.963487: step 48930, loss = 0.29, batch loss = 0.23 (26.9 examples/sec; 0.297 sec/batch; 23h:22m:58s remains)
INFO - root - 2017-12-16 07:32:51.822078: step 48940, loss = 0.27, batch loss = 0.21 (27.2 examples/sec; 0.294 sec/batch; 23h:07m:28s remains)
INFO - root - 2017-12-16 07:32:54.659534: step 48950, loss = 0.22, batch loss = 0.16 (28.1 examples/sec; 0.285 sec/batch; 22h:26m:24s remains)
INFO - root - 2017-12-16 07:32:57.507104: step 48960, loss = 0.37, batch loss = 0.32 (29.2 examples/sec; 0.274 sec/batch; 21h:34m:47s remains)
INFO - root - 2017-12-16 07:33:00.371467: step 48970, loss = 0.29, batch loss = 0.23 (26.6 examples/sec; 0.301 sec/batch; 23h:41m:21s remains)
INFO - root - 2017-12-16 07:33:03.268959: step 48980, loss = 0.29, batch loss = 0.23 (28.0 examples/sec; 0.286 sec/batch; 22h:30m:35s remains)
INFO - root - 2017-12-16 07:33:06.112566: step 48990, loss = 0.27, batch loss = 0.21 (28.4 examples/sec; 0.281 sec/batch; 22h:08m:42s remains)
INFO - root - 2017-12-16 07:33:08.959932: step 49000, loss = 0.29, batch loss = 0.23 (28.1 examples/sec; 0.285 sec/batch; 22h:25m:44s remains)
2017-12-16 07:33:09.424638: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2101407 -3.8464992 -4.9045897 -5.8932815 -6.9508038 -7.6195421 -7.8754263 -7.7392716 -7.3813286 -6.8790951 -6.4332457 -6.2449765 -5.6889987 -4.9605589 -4.4370828][-3.2509716 -4.2442856 -5.6033173 -6.778872 -7.8971691 -8.2952223 -8.22135 -7.7685623 -7.09464 -6.7739506 -6.5069218 -6.2024374 -5.6951237 -4.9989576 -4.2962728][-3.9846783 -5.2288895 -6.6060133 -7.5863905 -8.2526417 -8.1526089 -7.6115108 -6.8742609 -6.1774964 -5.8420062 -5.726985 -5.9088521 -5.6181345 -4.7548265 -4.0418057][-4.7364736 -6.1220455 -7.2691975 -7.6870613 -7.4118652 -6.4117928 -5.204493 -4.2059331 -3.6495669 -3.9056609 -4.6166329 -5.211904 -5.1877446 -4.7287045 -4.1111178][-5.4536638 -6.3522882 -6.7901239 -6.4732838 -5.2530737 -3.4850996 -1.6922824 -0.63238668 -0.61331367 -1.5322766 -2.9333267 -4.2335358 -4.832 -4.5618629 -3.8932829][-5.8077383 -6.0514665 -5.8806009 -4.9111643 -2.9506962 -0.58562517 1.5029683 2.4856091 2.173574 0.68059969 -1.4173391 -3.1338019 -3.98364 -4.0430737 -3.4856169][-5.2496262 -5.1294127 -4.6038127 -3.4186614 -1.3102574 1.3167825 3.6319933 4.6013908 3.9189024 2.1041675 -0.095687389 -2.0159729 -3.0694342 -3.2096767 -2.6860797][-4.5308323 -4.0825992 -3.2813635 -2.1468461 -0.26804209 2.1288037 4.2555666 5.1990595 4.6612186 2.9066858 0.71502876 -1.0477996 -1.9995642 -2.4169159 -2.1266384][-3.5775137 -3.1057305 -2.414186 -1.5827215 -0.2237792 1.6965947 3.4395285 4.4175491 4.1767712 2.7754073 0.98993254 -0.51951909 -1.4266028 -1.7146935 -1.3186896][-2.9792674 -2.6439 -2.2477279 -1.5737545 -0.53295708 0.6216116 1.7120252 2.5529079 2.5767288 1.7910876 0.69008732 -0.41830254 -1.0912762 -1.4207394 -1.2809026][-2.8427625 -2.6457634 -2.4614029 -2.0602763 -1.5913725 -0.83866096 -0.15117025 0.29102182 0.46828222 0.24211979 -0.30082226 -0.89903617 -1.2900078 -1.597451 -1.4774334][-3.0495043 -2.7800469 -2.7341952 -2.7476931 -2.6141582 -2.3757927 -2.3617156 -2.1109598 -1.8233914 -1.9086111 -2.0852931 -2.2231946 -2.4013095 -2.5420232 -2.2442992][-3.1716383 -2.9346433 -3.0813265 -3.3323483 -3.6063643 -3.8501992 -4.0655355 -4.0715308 -3.9439578 -3.6491656 -3.3526344 -3.4496975 -3.6403875 -3.6526189 -3.1905146][-3.5257239 -3.4012775 -3.5659242 -4.032536 -4.6742997 -5.2004137 -5.5973206 -5.6901631 -5.492547 -5.2814336 -5.1499476 -5.2047362 -5.3030305 -5.2563066 -4.7751689][-4.3991718 -4.4860582 -4.7814827 -5.2467747 -5.7543411 -6.307395 -6.6981726 -6.8511815 -6.78747 -6.5585642 -6.3911576 -6.6001463 -6.8306875 -6.678936 -5.9464083]]...]
INFO - root - 2017-12-16 07:33:12.240851: step 49010, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 22h:11m:32s remains)
INFO - root - 2017-12-16 07:33:15.106284: step 49020, loss = 0.25, batch loss = 0.19 (28.3 examples/sec; 0.283 sec/batch; 22h:17m:25s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 07:33:17.934888: step 49030, loss = 0.32, batch loss = 0.26 (27.1 examples/sec; 0.295 sec/batch; 23h:14m:22s remains)
INFO - root - 2017-12-16 07:33:20.770785: step 49040, loss = 0.21, batch loss = 0.15 (26.4 examples/sec; 0.303 sec/batch; 23h:51m:26s remains)
INFO - root - 2017-12-16 07:33:23.644974: step 49050, loss = 0.21, batch loss = 0.15 (27.6 examples/sec; 0.289 sec/batch; 22h:47m:18s remains)
INFO - root - 2017-12-16 07:33:26.473008: step 49060, loss = 0.28, batch loss = 0.22 (27.9 examples/sec; 0.287 sec/batch; 22h:36m:39s remains)
INFO - root - 2017-12-16 07:33:29.319545: step 49070, loss = 0.24, batch loss = 0.18 (28.9 examples/sec; 0.277 sec/batch; 21h:47m:47s remains)
INFO - root - 2017-12-16 07:33:32.105163: step 49080, loss = 0.21, batch loss = 0.15 (28.8 examples/sec; 0.278 sec/batch; 21h:52m:08s remains)
INFO - root - 2017-12-16 07:33:34.975543: step 49090, loss = 0.21, batch loss = 0.15 (28.5 examples/sec; 0.281 sec/batch; 22h:06m:47s remains)
INFO - root - 2017-12-16 07:33:37.798888: step 49100, loss = 0.22, batch loss = 0.17 (28.1 examples/sec; 0.284 sec/batch; 22h:22m:55s remains)
2017-12-16 07:33:38.268873: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2643971 -2.3966877 -2.4283319 -2.5044613 -2.4342766 -2.3846974 -2.6523211 -2.6235704 -2.5849724 -2.652889 -2.8243961 -2.9302437 -2.9005735 -2.997184 -3.183053][-0.9376018 -1.0296748 -0.95123363 -1.1779919 -1.3761482 -1.4225109 -1.6010015 -1.8038816 -2.3493333 -2.7612476 -2.9312422 -3.0653455 -3.09575 -3.1665812 -3.2729366][-0.49058652 -0.50994825 -0.44713974 -0.49692154 -0.53565073 -0.73560405 -1.0625424 -1.2689865 -1.8330288 -2.5890393 -3.2407877 -3.5026972 -3.4873044 -3.4646091 -3.3803153][-0.040357113 0.031143188 0.090423584 0.037930489 0.061954021 -0.065321445 -0.41741419 -0.89792562 -1.5488036 -2.2225492 -2.8779869 -3.2267945 -3.3936534 -3.3588431 -3.2954447][0.22939491 0.34399414 0.57842064 0.72475481 0.8548522 0.79903984 0.60266304 -0.057300568 -1.0056882 -1.7944868 -2.4888909 -2.9562354 -3.2138557 -3.2454486 -3.0559738][-0.051701546 0.25470304 0.75491476 1.2989273 1.8671575 2.016952 1.8146048 1.1577058 0.011906147 -1.1811006 -2.2241874 -2.7951016 -3.0465903 -3.1283126 -2.9572756][-0.8707366 -0.35575533 0.38937283 1.3532028 2.2970471 2.7934465 2.8854237 2.180202 0.97346973 -0.40062857 -1.7883368 -2.7050772 -3.1452694 -3.2263556 -2.9758573][-1.5393903 -0.957134 -0.22088194 0.73738194 1.8550382 2.5517101 2.8029003 2.3020649 1.3102918 0.11452723 -1.2030234 -2.2296712 -2.9790461 -3.2500558 -3.1457367][-2.2078662 -1.5872219 -0.87050271 -0.055593967 0.80923653 1.4933906 1.8638749 1.4641738 0.60361242 -0.44065928 -1.5080173 -2.4462962 -3.2632031 -3.6325252 -3.6032302][-2.7555754 -2.2947283 -1.8303928 -1.2173057 -0.45921087 0.091864109 0.43300438 0.28273535 -0.41045094 -1.5427589 -2.5814106 -3.3906617 -3.9937487 -4.1933947 -4.1197815][-2.9995513 -2.6593618 -2.3243747 -2.0031428 -1.65112 -1.2929509 -0.897372 -1.1216936 -1.6275759 -2.3542387 -3.2107477 -4.0997357 -4.6771612 -4.746664 -4.6408973][-2.8807569 -2.6810145 -2.5740294 -2.6056716 -2.5218568 -2.1586988 -1.7665794 -2.0681262 -2.6128068 -3.0596633 -3.4878216 -4.2102919 -4.7789803 -4.9161806 -4.8794827][-2.0952659 -1.8457692 -1.9259965 -2.2178502 -2.5081291 -2.5025597 -2.3215623 -2.4179039 -2.6482782 -3.1335595 -3.5049038 -3.9533765 -4.3024445 -4.5819697 -4.737309][-1.278374 -0.94718862 -0.92055154 -1.2150149 -1.5293348 -1.5367558 -1.5220523 -1.7424114 -2.1243083 -2.6651578 -3.0201635 -3.4271619 -3.6979778 -3.976228 -4.1655788][-0.3475337 -0.063533306 -0.1544776 -0.48633075 -0.64791727 -0.41051674 -0.29299355 -0.54856491 -0.956069 -1.4493225 -1.8994739 -2.4750724 -2.8201168 -2.9987922 -3.1260948]]...]
INFO - root - 2017-12-16 07:33:41.051392: step 49110, loss = 0.24, batch loss = 0.18 (27.4 examples/sec; 0.292 sec/batch; 22h:59m:56s remains)
INFO - root - 2017-12-16 07:33:43.919488: step 49120, loss = 0.25, batch loss = 0.19 (26.9 examples/sec; 0.297 sec/batch; 23h:22m:13s remains)
INFO - root - 2017-12-16 07:33:46.721940: step 49130, loss = 0.30, batch loss = 0.24 (29.6 examples/sec; 0.270 sec/batch; 21h:15m:13s remains)
INFO - root - 2017-12-16 07:33:49.537168: step 49140, loss = 0.31, batch loss = 0.25 (29.7 examples/sec; 0.269 sec/batch; 21h:11m:52s remains)
INFO - root - 2017-12-16 07:33:52.374131: step 49150, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.285 sec/batch; 22h:24m:07s remains)
INFO - root - 2017-12-16 07:33:55.187311: step 49160, loss = 0.36, batch loss = 0.31 (28.9 examples/sec; 0.276 sec/batch; 21h:45m:05s remains)
INFO - root - 2017-12-16 07:33:58.043197: step 49170, loss = 0.27, batch loss = 0.21 (29.7 examples/sec; 0.270 sec/batch; 21h:12m:49s remains)
INFO - root - 2017-12-16 07:34:00.864324: step 49180, loss = 0.21, batch loss = 0.15 (29.3 examples/sec; 0.273 sec/batch; 21h:27m:20s remains)
INFO - root - 2017-12-16 07:34:03.722428: step 49190, loss = 0.24, batch loss = 0.18 (28.9 examples/sec; 0.277 sec/batch; 21h:47m:29s remains)
INFO - root - 2017-12-16 07:34:06.537120: step 49200, loss = 0.30, batch loss = 0.24 (28.1 examples/sec; 0.285 sec/batch; 22h:25m:51s remains)
2017-12-16 07:34:07.007992: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7114441 -4.4046583 -5.049262 -5.6913166 -6.1372395 -6.3700724 -6.7814937 -7.6584563 -8.4285679 -8.7252007 -8.736022 -8.6493216 -8.1577835 -7.6624308 -7.1956539][-4.2054048 -5.1818056 -5.951045 -6.6360035 -7.0327568 -7.5489573 -8.0716314 -8.4466171 -8.7387094 -9.258461 -9.3958778 -9.1176548 -8.5566177 -7.9463358 -7.4513407][-4.5238667 -5.6450062 -6.4074392 -6.8010526 -6.8727369 -6.83084 -6.9296632 -7.2795362 -7.8576245 -8.0876322 -8.1642332 -8.22622 -8.0425358 -7.6003041 -7.2010593][-4.9116197 -5.9640718 -6.6341391 -6.7513056 -6.2212806 -5.4233603 -4.8936558 -4.608573 -4.6831808 -5.2495403 -5.8304491 -6.2899585 -6.4265919 -6.3181586 -6.29657][-4.8592486 -5.6885009 -6.0261059 -5.7112556 -4.8461494 -3.5194035 -2.2538941 -1.5884752 -1.7096891 -2.1460826 -2.8477292 -3.6687799 -4.154397 -4.5288358 -4.8380375][-4.4417987 -5.1444159 -5.1693115 -4.26837 -2.5242271 -0.69443011 0.69703627 1.6016841 1.6238647 0.68271685 -0.34831715 -1.2410219 -1.9983826 -2.5650351 -2.9871039][-4.0059061 -4.2084589 -4.0796986 -3.0193639 -1.1663234 1.1073298 3.0376215 3.8472004 3.3312373 2.6104422 1.7553935 0.54375982 -0.42490053 -1.12219 -1.4882267][-3.8814502 -3.9579904 -3.4445119 -1.9872668 -0.093049049 2.0349956 3.5795021 4.2884283 4.1125546 3.2626967 2.3937187 1.443913 0.53740931 -0.13984203 -0.51649976][-3.6041126 -3.63052 -3.0858896 -1.7439754 0.057323456 1.761652 3.0076418 3.5193515 3.1751962 2.6582136 1.9777646 1.06778 0.16682816 -0.6685946 -1.1055872][-3.3988457 -3.4420745 -3.0995936 -2.0766413 -0.95873952 0.43198442 1.2962189 1.5759001 1.5089245 0.94655561 0.37320948 -0.29470921 -1.0874739 -1.7654924 -2.1779497][-3.6024156 -3.687315 -3.6142259 -3.0532322 -2.2500007 -1.5368536 -0.97071028 -0.9407351 -1.036675 -1.2666173 -1.8187981 -2.427695 -3.1137271 -3.6626704 -3.9345956][-4.0561466 -4.141026 -3.9945905 -3.9113836 -3.8258367 -3.5828593 -3.4558043 -3.498615 -3.4658465 -3.8621094 -4.4237633 -5.0489783 -5.7650824 -6.0168152 -5.9865136][-4.6120973 -4.8037295 -4.9072819 -5.0528135 -5.0561733 -5.03349 -5.1259317 -5.2465596 -5.3957391 -5.5031695 -5.8475709 -6.08035 -6.5120583 -7.1823554 -7.1604977][-4.1933928 -4.4128184 -4.583137 -4.8036704 -4.9851408 -5.1051679 -5.28321 -5.4882722 -5.5373349 -5.6894712 -5.8692479 -6.4096627 -6.8950396 -7.1034222 -6.9417753][-4.2752213 -4.24423 -4.2518096 -4.51425 -4.7549005 -5.04708 -5.3059154 -5.375391 -5.3681211 -5.4225845 -5.6664052 -5.8544188 -6.0965223 -6.5160484 -6.5119843]]...]
INFO - root - 2017-12-16 07:34:09.830910: step 49210, loss = 0.29, batch loss = 0.23 (27.3 examples/sec; 0.293 sec/batch; 23h:03m:27s remains)
INFO - root - 2017-12-16 07:34:12.686271: step 49220, loss = 0.22, batch loss = 0.17 (25.3 examples/sec; 0.316 sec/batch; 24h:50m:26s remains)
INFO - root - 2017-12-16 07:34:15.540962: step 49230, loss = 0.24, batch loss = 0.18 (26.8 examples/sec; 0.299 sec/batch; 23h:29m:28s remains)
INFO - root - 2017-12-16 07:34:18.374834: step 49240, loss = 0.34, batch loss = 0.28 (28.4 examples/sec; 0.282 sec/batch; 22h:10m:24s remains)
INFO - root - 2017-12-16 07:34:21.215259: step 49250, loss = 0.25, batch loss = 0.19 (27.3 examples/sec; 0.293 sec/batch; 23h:01m:18s remains)
INFO - root - 2017-12-16 07:34:24.074521: step 49260, loss = 0.20, batch loss = 0.14 (28.3 examples/sec; 0.283 sec/batch; 22h:16m:04s remains)
INFO - root - 2017-12-16 07:34:26.864048: step 49270, loss = 0.20, batch loss = 0.14 (27.8 examples/sec; 0.288 sec/batch; 22h:38m:08s remains)
INFO - root - 2017-12-16 07:34:29.665843: step 49280, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 21h:59m:57s remains)
INFO - root - 2017-12-16 07:34:32.484471: step 49290, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 22h:27m:54s remains)
INFO - root - 2017-12-16 07:34:35.296662: step 49300, loss = 0.22, batch loss = 0.16 (27.7 examples/sec; 0.289 sec/batch; 22h:43m:10s remains)
2017-12-16 07:34:35.765385: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.518115 -4.7740636 -5.1315651 -5.546104 -6.2508283 -6.8469992 -7.358418 -7.4619579 -7.1585588 -6.650486 -5.9600239 -5.6107035 -5.423357 -4.9399328 -4.9687304][-3.8046622 -3.9762907 -4.3103976 -4.7062187 -5.2841387 -5.8572197 -6.3641453 -6.7215996 -6.4150906 -5.7424569 -4.9529161 -4.5037041 -4.4906068 -4.0472317 -3.87625][-3.342057 -2.9138906 -2.8297958 -3.0443952 -3.5471039 -4.1547623 -4.4703693 -4.9691906 -5.0812197 -4.6227713 -3.7762012 -3.366688 -3.388191 -3.1317348 -3.0415931][-2.4309525 -1.9287927 -1.5544195 -1.4103067 -1.8719454 -2.547873 -2.7921872 -2.97608 -2.886487 -2.6839423 -2.1326165 -1.8657279 -2.1691418 -2.1606517 -2.250411][-1.7516215 -0.91500044 -0.32413149 -0.19837713 -0.5842464 -1.1283915 -1.2071075 -1.4469602 -1.4366975 -1.2790105 -0.84760022 -0.83274984 -1.2949622 -1.4530661 -1.5615637][-1.944906 -0.99733686 -0.15446472 0.32765198 0.38507032 -0.0021748543 0.14829302 0.45914936 0.63462877 0.66053772 0.77984858 0.37696123 -0.46987224 -0.89737535 -1.117589][-2.6458821 -1.7015734 -0.83146119 -0.1008687 0.33748436 0.36786318 0.993515 1.5142627 1.8290997 1.8964276 1.90908 1.5969186 0.69653177 -0.072113514 -0.62138343][-3.5812802 -2.789835 -1.9849865 -1.1831901 -0.49802995 -0.037365437 0.74206448 1.6337996 2.4383368 2.4981728 2.4940577 2.0623436 1.1724453 0.19934368 -0.67835259][-4.7943292 -4.1285696 -3.2478776 -2.4629941 -1.8870306 -1.3069606 -0.40969896 0.60131931 1.438457 1.8403015 2.0661306 1.8774052 1.1463017 0.36190796 -0.419528][-5.1933274 -5.0352855 -4.6925182 -3.9299262 -3.0130303 -2.4044802 -1.7274542 -0.9119916 -0.07614994 0.48797274 0.75435829 0.41930676 -0.28263044 -0.68537569 -1.036773][-5.628427 -5.1055312 -4.7689962 -4.7338853 -4.3190131 -3.5650601 -2.8850851 -2.2636321 -1.6202886 -1.202409 -0.94759655 -1.155746 -1.5643797 -2.0199749 -2.3595622][-5.5216842 -5.2291884 -5.1142159 -4.8465009 -4.7029738 -4.6443787 -4.3760943 -3.80305 -3.057467 -2.6703844 -2.6150212 -2.8340173 -3.0761251 -3.0635619 -3.1211457][-4.7601867 -4.5761666 -4.5545006 -4.5884061 -4.6955762 -4.5455446 -4.5052533 -4.3703828 -3.9203281 -3.5867419 -3.3936207 -3.440577 -3.5985453 -3.7918978 -3.8993924][-4.0059872 -3.7708547 -3.8068576 -4.0371308 -4.2703834 -4.2173653 -4.2661777 -4.1355438 -3.9149017 -3.7182131 -3.5051284 -3.7132063 -3.9880612 -3.9516881 -3.9182825][-3.5561063 -3.2272191 -3.1742966 -3.3140385 -3.5991683 -3.75673 -3.837981 -3.664814 -3.3860698 -3.3060799 -3.2699604 -3.2361248 -3.2879028 -3.4111538 -3.4857948]]...]
INFO - root - 2017-12-16 07:34:38.562377: step 49310, loss = 0.20, batch loss = 0.14 (29.3 examples/sec; 0.273 sec/batch; 21h:28m:28s remains)
INFO - root - 2017-12-16 07:34:41.441408: step 49320, loss = 0.30, batch loss = 0.24 (28.1 examples/sec; 0.285 sec/batch; 22h:25m:23s remains)
INFO - root - 2017-12-16 07:34:44.258961: step 49330, loss = 0.41, batch loss = 0.35 (28.6 examples/sec; 0.280 sec/batch; 22h:01m:57s remains)
INFO - root - 2017-12-16 07:34:47.112687: step 49340, loss = 0.40, batch loss = 0.34 (29.2 examples/sec; 0.274 sec/batch; 21h:33m:04s remains)
INFO - root - 2017-12-16 07:34:49.948074: step 49350, loss = 0.32, batch loss = 0.26 (26.3 examples/sec; 0.305 sec/batch; 23h:57m:05s remains)
INFO - root - 2017-12-16 07:34:52.746237: step 49360, loss = 0.22, batch loss = 0.17 (28.9 examples/sec; 0.277 sec/batch; 21h:45m:40s remains)
INFO - root - 2017-12-16 07:34:55.563650: step 49370, loss = 0.20, batch loss = 0.14 (28.0 examples/sec; 0.286 sec/batch; 22h:29m:33s remains)
INFO - root - 2017-12-16 07:34:58.393748: step 49380, loss = 0.28, batch loss = 0.22 (28.9 examples/sec; 0.277 sec/batch; 21h:47m:23s remains)
INFO - root - 2017-12-16 07:35:01.224301: step 49390, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.283 sec/batch; 22h:13m:19s remains)
INFO - root - 2017-12-16 07:35:04.067413: step 49400, loss = 0.24, batch loss = 0.18 (29.6 examples/sec; 0.270 sec/batch; 21h:14m:07s remains)
2017-12-16 07:35:04.537672: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0210381 -4.8409333 -4.7891197 -4.7558441 -4.5205469 -4.3519535 -4.1573043 -3.9930494 -3.8972726 -3.8460264 -3.8163683 -3.8032799 -4.0565085 -4.48566 -4.7751536][-4.8110189 -4.5874805 -4.6370406 -4.5957627 -4.4059057 -4.2716513 -4.1284919 -3.9929554 -3.896708 -3.8160481 -3.6587467 -3.4615197 -3.6695762 -4.2261672 -4.496686][-3.7185414 -3.6240454 -3.8073528 -3.8946893 -3.8164315 -3.7517376 -3.6720266 -3.4892082 -3.3607483 -3.3495758 -3.334198 -3.3600547 -3.6645837 -4.1870723 -4.3650894][-1.9579287 -1.8204153 -2.1997283 -2.6355939 -2.8246462 -2.9157543 -2.8106909 -2.5074635 -2.3423357 -2.6457677 -2.9418614 -3.3852909 -3.9063654 -4.4376907 -4.7220984][-0.77492881 -0.13009787 -0.41315079 -1.088258 -1.41832 -1.5556257 -1.5088391 -1.2812932 -1.0149293 -1.3596632 -2.0408401 -2.8208909 -3.6036973 -4.340034 -4.7707186][0.058591366 0.899817 0.55305958 0.011096954 -0.11247683 -0.084002972 0.30516243 0.69827509 0.85332012 0.38996077 -0.65267825 -1.9644034 -3.0817509 -3.8197217 -4.3745384][0.32762623 1.1863513 0.81595182 0.3140173 0.44876528 0.95669031 1.7903666 2.4001765 2.4656005 1.7504597 0.44948721 -1.1383648 -2.5312905 -3.4511044 -4.1443448][-0.32129717 0.25681639 -0.15866995 -0.46735644 -0.0748744 0.93168354 2.2162781 3.0553546 3.2565861 2.5136805 1.0701265 -0.7785151 -2.4727306 -3.5820496 -4.3143592][-1.9696114 -1.6277332 -2.050416 -2.1238456 -1.3527093 -0.0053391457 1.5500565 2.4839602 2.7226915 2.1365433 0.91769266 -0.90066981 -2.5880935 -3.7229872 -4.600585][-3.2314627 -3.4763422 -4.1517515 -3.9996219 -2.9516792 -1.4156814 0.10990143 0.96788645 1.1771622 0.6698885 -0.4473331 -2.0384383 -3.4468355 -4.3531122 -5.0105839][-4.7839756 -5.2485275 -5.79711 -5.584619 -4.702529 -3.2562923 -1.9098637 -1.4557419 -1.416405 -1.7373731 -2.6089859 -3.7403684 -4.5910435 -5.1024837 -5.4336009][-5.4411378 -5.9675565 -6.6945076 -6.5984373 -5.9432268 -4.8299136 -3.8089783 -3.5467374 -3.5137613 -3.6155803 -4.1331768 -4.9295473 -5.4292293 -5.5173159 -5.6178102][-5.1213694 -5.4890652 -6.2018094 -6.2603154 -5.9894152 -5.3418794 -4.7357326 -4.7230363 -4.7943697 -4.8492289 -5.0616307 -5.4620504 -5.7127247 -5.6099472 -5.4568038][-4.62515 -4.5874205 -4.8818116 -5.1639261 -5.3004208 -4.95406 -4.6808167 -4.8467712 -4.8957725 -4.6878905 -4.6232791 -4.8035316 -4.8133507 -4.6929255 -4.7383003][-4.1621919 -3.594244 -3.4936004 -3.6473844 -3.8990207 -4.012713 -4.1088128 -4.2696619 -4.2041311 -3.9198296 -3.6049364 -3.3748803 -3.1578631 -3.2707152 -3.5251579]]...]
INFO - root - 2017-12-16 07:35:07.372562: step 49410, loss = 0.21, batch loss = 0.15 (28.1 examples/sec; 0.284 sec/batch; 22h:22m:14s remains)
INFO - root - 2017-12-16 07:35:10.235504: step 49420, loss = 0.32, batch loss = 0.26 (27.6 examples/sec; 0.290 sec/batch; 22h:46m:57s remains)
INFO - root - 2017-12-16 07:35:13.034905: step 49430, loss = 0.31, batch loss = 0.25 (28.7 examples/sec; 0.278 sec/batch; 21h:53m:47s remains)
INFO - root - 2017-12-16 07:35:15.870375: step 49440, loss = 0.22, batch loss = 0.17 (28.3 examples/sec; 0.283 sec/batch; 22h:13m:39s remains)
INFO - root - 2017-12-16 07:35:18.687228: step 49450, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 22h:23m:24s remains)
INFO - root - 2017-12-16 07:35:21.488586: step 49460, loss = 0.22, batch loss = 0.16 (29.2 examples/sec; 0.274 sec/batch; 21h:32m:58s remains)
INFO - root - 2017-12-16 07:35:24.382286: step 49470, loss = 0.25, batch loss = 0.19 (27.6 examples/sec; 0.290 sec/batch; 22h:45m:54s remains)
INFO - root - 2017-12-16 07:35:27.227509: step 49480, loss = 0.30, batch loss = 0.25 (26.2 examples/sec; 0.306 sec/batch; 24h:01m:29s remains)
INFO - root - 2017-12-16 07:35:30.047471: step 49490, loss = 0.26, batch loss = 0.20 (28.5 examples/sec; 0.281 sec/batch; 22h:04m:16s remains)
INFO - root - 2017-12-16 07:35:32.924223: step 49500, loss = 0.21, batch loss = 0.15 (29.0 examples/sec; 0.276 sec/batch; 21h:42m:06s remains)
2017-12-16 07:35:33.392487: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8553591 -2.9550724 -3.0554428 -2.9799986 -2.901237 -2.7623134 -2.6777296 -2.5394616 -2.2907903 -2.0052454 -1.7811768 -1.5892878 -1.4650707 -1.524528 -1.7259767][-2.8418345 -3.0083051 -3.1020598 -3.1678309 -3.1605957 -3.078125 -3.0102463 -2.8192511 -2.6283908 -2.4152377 -2.1940207 -1.993736 -1.8421056 -1.7155275 -1.6309431][-2.7337961 -3.1485856 -3.5459557 -3.6476426 -3.6171069 -3.48099 -3.369031 -3.19459 -3.0173202 -2.8618684 -2.7488303 -2.612123 -2.4595728 -2.3397548 -2.1896646][-2.7688205 -3.469152 -4.0540309 -4.3169394 -4.3404746 -4.0366917 -3.6029208 -3.2336578 -2.9659123 -2.8838706 -2.8486137 -2.9580557 -3.017499 -2.8541803 -2.6405497][-2.9554739 -3.7513151 -4.4063487 -4.7332768 -4.6436815 -4.0368924 -3.3838582 -2.8661218 -2.4131544 -2.3189104 -2.3579357 -2.5691276 -2.8436909 -3.1192074 -3.3331685][-3.101944 -3.9023829 -4.5363045 -4.7835388 -4.5134349 -3.6393309 -2.6818106 -1.835196 -1.170608 -0.90318632 -0.93805432 -1.4932432 -2.2632542 -2.7957685 -3.1942456][-3.2056479 -3.801229 -4.3703389 -4.5171466 -4.0172405 -2.8738291 -1.5370989 -0.45320034 0.24967194 0.47466564 0.38470888 -0.17728853 -1.1593058 -2.2623811 -3.2182727][-3.0797408 -3.3489785 -3.6264691 -3.59135 -3.0220084 -1.9103899 -0.54275703 0.60831928 1.5716515 1.9638786 1.6963587 0.67176151 -0.73571897 -2.0020885 -3.1212492][-2.5093913 -2.712611 -2.8806751 -2.6856852 -2.0660396 -1.0961254 0.076932907 1.0510373 1.793045 2.1624374 1.90868 0.96732521 -0.571301 -2.1321425 -3.4309604][-2.4517336 -2.3316267 -2.2606926 -2.1471531 -1.5898767 -0.70633245 0.19615602 1.0089431 1.5846543 1.7454581 1.3953485 0.47877121 -0.90730238 -2.3438518 -3.3464863][-2.2536142 -2.171885 -2.106473 -1.9639335 -1.6195176 -0.94704962 -0.15502739 0.37648678 0.42048931 0.60732889 0.34759808 -0.46205115 -1.5393515 -2.5774908 -3.2845852][-2.219691 -1.8726268 -1.7212915 -1.6804085 -1.3571644 -1.0690413 -0.87089968 -0.64160395 -0.39753246 -0.45579219 -0.84752226 -1.363472 -2.1211672 -2.7176981 -3.0551133][-1.859221 -1.4200399 -1.4871898 -1.6235311 -1.5481131 -1.2248144 -0.82081366 -0.98427033 -1.3369031 -1.3627312 -1.4235823 -1.6117291 -2.1168163 -2.4206438 -2.6424563][-1.9384208 -1.2080131 -0.92462373 -1.1825566 -1.3379724 -1.401257 -1.4845991 -1.4845541 -1.5238979 -1.7173128 -1.879591 -1.8161955 -1.8713124 -1.9480741 -1.9505558][-1.1699543 -0.65757942 -0.72262692 -0.99958968 -1.2432418 -1.2732108 -1.3426569 -1.6212468 -2.033783 -2.0195801 -1.753037 -1.5611439 -1.3990798 -1.2165096 -1.1426864]]...]
INFO - root - 2017-12-16 07:35:36.248738: step 49510, loss = 0.24, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 21h:33m:26s remains)
INFO - root - 2017-12-16 07:35:39.091985: step 49520, loss = 0.30, batch loss = 0.25 (28.6 examples/sec; 0.279 sec/batch; 21h:57m:20s remains)
INFO - root - 2017-12-16 07:35:41.882936: step 49530, loss = 0.28, batch loss = 0.22 (28.6 examples/sec; 0.280 sec/batch; 21h:59m:00s remains)
INFO - root - 2017-12-16 07:35:44.711482: step 49540, loss = 0.25, batch loss = 0.19 (29.1 examples/sec; 0.275 sec/batch; 21h:35m:22s remains)
INFO - root - 2017-12-16 07:35:47.535189: step 49550, loss = 0.27, batch loss = 0.21 (28.7 examples/sec; 0.279 sec/batch; 21h:54m:30s remains)
INFO - root - 2017-12-16 07:35:50.398635: step 49560, loss = 0.27, batch loss = 0.21 (29.1 examples/sec; 0.275 sec/batch; 21h:34m:49s remains)
INFO - root - 2017-12-16 07:35:53.222398: step 49570, loss = 0.28, batch loss = 0.22 (28.3 examples/sec; 0.283 sec/batch; 22h:14m:22s remains)
INFO - root - 2017-12-16 07:35:56.096987: step 49580, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.282 sec/batch; 22h:07m:38s remains)
INFO - root - 2017-12-16 07:35:58.883873: step 49590, loss = 0.28, batch loss = 0.22 (29.8 examples/sec; 0.268 sec/batch; 21h:03m:48s remains)
INFO - root - 2017-12-16 07:36:01.715400: step 49600, loss = 0.26, batch loss = 0.20 (28.8 examples/sec; 0.278 sec/batch; 21h:48m:36s remains)
2017-12-16 07:36:02.194897: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3435497 -5.5965805 -5.7435789 -6.0448804 -6.2785854 -6.3332829 -6.3864074 -6.60627 -6.9975014 -7.164938 -7.2618322 -7.2136207 -6.8465571 -6.1168227 -5.3255873][-4.7863054 -5.028142 -5.1972508 -5.4247909 -5.3914032 -5.44745 -5.5722747 -5.7332406 -5.9964161 -6.3327503 -6.7031078 -6.6244287 -6.1854792 -5.4970551 -4.7739525][-4.1164379 -4.3379803 -4.4132462 -4.4649916 -4.2475953 -4.0176735 -3.8815086 -3.9465346 -4.220489 -4.73775 -5.3943315 -5.6216631 -5.378665 -4.8649883 -4.3473644][-3.4066808 -3.6007156 -3.5930102 -3.334353 -2.9660325 -2.3944421 -1.907548 -1.8407788 -2.0917611 -2.6841488 -3.3710282 -3.8613164 -4.1273003 -4.1178908 -4.0752048][-2.63787 -2.7510324 -2.6108794 -2.0895255 -1.4154072 -0.64380383 0.047110081 0.20148516 -0.057085037 -0.66083646 -1.3911388 -2.1597645 -2.7926497 -3.3123369 -3.8714848][-1.9423966 -1.7547238 -1.2737007 -0.54140759 0.43593264 1.2645135 1.8329453 1.9724512 1.7194118 1.1156521 0.39366102 -0.49527478 -1.5507812 -2.6522932 -3.604959][-1.3295217 -1.140152 -0.64760089 0.3050909 1.3622656 2.1699405 2.6533389 2.7735858 2.5665684 1.9846287 1.4352803 0.45710802 -0.72630739 -2.0911438 -3.4679866][-1.045845 -0.82606149 -0.51538706 0.29372072 1.1982832 2.00811 2.5090418 2.4781437 2.3941493 2.0422773 1.6467395 0.77434254 -0.4624567 -1.9312663 -3.3868513][-1.1614416 -0.80590987 -0.52472091 -0.0191226 0.5334444 1.1693182 1.5277328 1.6288824 1.6883669 1.5719786 1.5063791 0.89779568 -0.40752745 -2.0721471 -3.750438][-1.6123292 -1.4033117 -1.4743681 -1.2043524 -0.55935526 -0.030902386 0.08487463 0.13999653 0.41916895 0.54779291 0.66111135 0.17559862 -1.0397282 -2.7784872 -4.4855604][-2.7528839 -2.6278923 -2.524693 -2.4098692 -2.3331912 -1.9215109 -1.614841 -1.5344069 -1.4132116 -1.1597173 -1.0381529 -1.3961413 -2.3483603 -3.832685 -5.2544346][-3.8314927 -3.9262242 -4.105844 -4.1608806 -3.9200883 -3.7877007 -3.8381248 -3.5259175 -3.0309901 -2.6660237 -2.6230106 -2.9463036 -3.6923232 -4.8301797 -5.7639413][-4.9786763 -5.1807551 -5.3371882 -5.5975723 -5.7499676 -5.4907589 -5.1651559 -4.9436378 -4.7992234 -4.3144288 -3.9945776 -4.3847446 -5.0715714 -5.7859468 -6.2859578][-6.0817566 -6.41117 -6.528986 -6.59893 -6.706687 -6.7016368 -6.5468893 -6.1506162 -5.6107383 -5.1095028 -5.1007056 -5.3730278 -5.7051148 -6.2464752 -6.6994219][-6.8290844 -7.1471872 -7.415391 -7.7704248 -7.9094706 -7.6944528 -7.3162127 -6.7823305 -6.5177269 -6.176404 -5.87597 -6.0139971 -6.4196076 -6.8199058 -6.7346373]]...]
INFO - root - 2017-12-16 07:36:05.072328: step 49610, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 22h:29m:08s remains)
INFO - root - 2017-12-16 07:36:07.875104: step 49620, loss = 0.25, batch loss = 0.19 (26.4 examples/sec; 0.302 sec/batch; 23h:46m:02s remains)
INFO - root - 2017-12-16 07:36:10.751586: step 49630, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 21h:56m:24s remains)
INFO - root - 2017-12-16 07:36:13.553338: step 49640, loss = 0.32, batch loss = 0.26 (28.5 examples/sec; 0.281 sec/batch; 22h:05m:37s remains)
INFO - root - 2017-12-16 07:36:16.406874: step 49650, loss = 0.23, batch loss = 0.17 (29.1 examples/sec; 0.275 sec/batch; 21h:35m:25s remains)
INFO - root - 2017-12-16 07:36:19.241822: step 49660, loss = 0.23, batch loss = 0.17 (26.5 examples/sec; 0.302 sec/batch; 23h:43m:55s remains)
INFO - root - 2017-12-16 07:36:22.039134: step 49670, loss = 0.22, batch loss = 0.17 (29.8 examples/sec; 0.269 sec/batch; 21h:05m:39s remains)
INFO - root - 2017-12-16 07:36:24.897626: step 49680, loss = 0.25, batch loss = 0.19 (27.6 examples/sec; 0.289 sec/batch; 22h:44m:27s remains)
INFO - root - 2017-12-16 07:36:27.763344: step 49690, loss = 0.26, batch loss = 0.20 (27.9 examples/sec; 0.286 sec/batch; 22h:30m:23s remains)
INFO - root - 2017-12-16 07:36:30.652300: step 49700, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 22h:26m:23s remains)
2017-12-16 07:36:31.108311: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4381614 -5.7977629 -6.0311637 -6.3958158 -6.5535212 -6.4403639 -6.17295 -5.9618711 -5.6544933 -5.2008705 -4.8110828 -4.3453326 -3.7396071 -3.1509566 -2.9402528][-5.4753752 -5.9523263 -6.10062 -6.1097155 -6.0055189 -6.0645084 -6.0184574 -5.68771 -5.0969725 -4.6632118 -4.4344282 -3.9757471 -3.3326211 -2.8461273 -2.5866218][-4.7966523 -5.4321566 -5.6167369 -5.53802 -5.1067643 -4.8360868 -4.5950866 -4.5787139 -4.5151334 -4.1221709 -3.6901038 -3.3645649 -2.9543915 -2.4981332 -2.2676334][-3.9322224 -4.4721522 -4.5228314 -4.1585612 -3.4919412 -2.8339188 -2.3710077 -2.3134987 -2.5111387 -2.7113867 -2.8457727 -2.8903885 -2.822737 -2.5643473 -2.4353352][-3.1139293 -2.9371512 -2.4444852 -1.759799 -0.84704804 0.16407871 0.83595181 0.6993866 -0.0013594627 -0.85593581 -1.7186656 -2.2839117 -2.7013781 -2.9566693 -2.9409747][-2.0502694 -1.738323 -0.89932442 0.49218845 2.1214981 3.1772504 3.7096825 3.5123906 2.483254 1.0974755 -0.33950567 -1.6331174 -2.6151786 -3.2083151 -3.3448658][-2.0973241 -1.2581873 0.11014843 1.5604568 3.1726141 4.8560638 5.9370022 5.6527081 4.2716312 2.2554173 0.30274391 -1.3154004 -2.4575605 -3.3837621 -3.7033725][-2.4478712 -1.7985909 -0.57989 1.4267869 3.3938828 5.1297092 6.2329082 6.2477007 5.1432877 2.9775906 0.69280767 -1.3868859 -2.8840818 -3.7657104 -4.1009588][-3.7915363 -3.1424928 -2.0494592 -0.44773674 1.4041796 3.5392098 5.012475 5.2473822 4.3624916 2.4938164 0.4052372 -1.6734028 -3.0942216 -3.8652961 -4.0951662][-4.8503389 -4.9550924 -4.727704 -3.5018988 -1.922545 -0.10124111 1.4342852 2.2424746 2.0184884 0.62367344 -1.0798893 -2.8753045 -3.9771612 -4.4610405 -4.5440726][-6.9153614 -6.9168124 -6.4675031 -5.9100556 -5.290298 -3.7870784 -2.2142956 -1.2376838 -0.81639433 -1.5663826 -2.8976109 -4.2860384 -5.131453 -5.4491982 -5.4460893][-7.4535751 -8.0520487 -8.4862709 -8.0179071 -7.1582928 -5.9281039 -4.8076873 -3.8822937 -3.1661391 -3.38021 -3.9400885 -5.1347604 -5.9035177 -6.0212994 -6.039031][-7.4513578 -8.0615025 -8.3439932 -8.3635321 -8.1217871 -7.079165 -5.9437695 -5.0528226 -4.5138121 -4.5146232 -4.8678365 -5.7649317 -6.1454983 -6.0776281 -5.93753][-6.5513525 -6.8379211 -7.01125 -7.2575827 -7.1392555 -6.48446 -5.736259 -5.2705526 -4.9904656 -4.7501378 -4.8263369 -5.4777031 -5.9740963 -5.7577505 -5.4892445][-5.4035792 -5.2631655 -5.1695309 -5.0953889 -4.94174 -4.743269 -4.4092445 -4.2234535 -4.1397085 -4.2302079 -4.5189977 -4.8772244 -5.0907903 -4.9745231 -4.8306637]]...]
INFO - root - 2017-12-16 07:36:33.925380: step 49710, loss = 0.30, batch loss = 0.24 (28.5 examples/sec; 0.280 sec/batch; 22h:01m:32s remains)
INFO - root - 2017-12-16 07:36:36.769482: step 49720, loss = 0.27, batch loss = 0.22 (28.3 examples/sec; 0.282 sec/batch; 22h:10m:09s remains)
INFO - root - 2017-12-16 07:36:39.582113: step 49730, loss = 0.27, batch loss = 0.21 (27.2 examples/sec; 0.294 sec/batch; 23h:03m:51s remains)
INFO - root - 2017-12-16 07:36:42.436589: step 49740, loss = 0.31, batch loss = 0.25 (28.7 examples/sec; 0.278 sec/batch; 21h:52m:17s remains)
INFO - root - 2017-12-16 07:36:45.256523: step 49750, loss = 0.27, batch loss = 0.21 (29.5 examples/sec; 0.271 sec/batch; 21h:17m:14s remains)
INFO - root - 2017-12-16 07:36:48.088630: step 49760, loss = 0.34, batch loss = 0.28 (28.5 examples/sec; 0.281 sec/batch; 22h:02m:49s remains)
INFO - root - 2017-12-16 07:36:50.881683: step 49770, loss = 0.24, batch loss = 0.18 (28.7 examples/sec; 0.279 sec/batch; 21h:52m:28s remains)
INFO - root - 2017-12-16 07:36:53.737187: step 49780, loss = 0.43, batch loss = 0.38 (28.6 examples/sec; 0.280 sec/batch; 21h:57m:57s remains)
INFO - root - 2017-12-16 07:36:56.557248: step 49790, loss = 0.34, batch loss = 0.28 (27.8 examples/sec; 0.288 sec/batch; 22h:36m:09s remains)
INFO - root - 2017-12-16 07:36:59.374772: step 49800, loss = 0.22, batch loss = 0.16 (28.9 examples/sec; 0.277 sec/batch; 21h:45m:08s remains)
2017-12-16 07:36:59.861831: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9687409 -2.7876029 -2.6106699 -2.4995451 -2.5146236 -2.6721036 -3.002686 -3.3287823 -3.6993213 -3.915792 -4.0475 -4.030261 -3.8899257 -3.6106498 -3.2534189][-2.9736257 -2.7176907 -2.5414548 -2.4560156 -2.4552965 -2.6751184 -3.0388212 -3.2843585 -3.5937819 -3.7717414 -3.8425956 -3.7040429 -3.3891106 -3.0243983 -2.6688795][-2.8648896 -2.6614814 -2.5342488 -2.4668717 -2.5133324 -2.674706 -2.9085941 -3.0149789 -3.162034 -3.3212645 -3.3859673 -3.2013295 -2.8895397 -2.3814387 -2.0061872][-2.7153704 -2.4339762 -2.31091 -2.297576 -2.3253384 -2.4100935 -2.4534149 -2.386045 -2.2997508 -2.1616602 -2.0506985 -1.9219873 -1.7900965 -1.5127521 -1.3857868][-2.6042123 -2.2058322 -1.973345 -1.8643215 -1.8304353 -1.7906063 -1.608582 -1.3941152 -1.0496466 -0.73231983 -0.52624679 -0.41508293 -0.50862265 -0.61894369 -0.95593882][-2.3635013 -1.9341683 -1.6408045 -1.4080606 -1.2327302 -1.2189255 -0.95180297 -0.46384883 0.074851513 0.50707817 0.79529047 0.80310297 0.41026258 -0.047841549 -0.79727912][-2.3045409 -1.9133303 -1.5936172 -1.2630496 -0.96497321 -0.65106893 -0.089690685 0.46657896 1.1273503 1.656702 1.8589101 1.6976161 1.0142603 0.074923515 -1.0752072][-2.2995472 -1.9244177 -1.5691564 -1.1593578 -0.72672105 -0.17408323 0.72501993 1.487896 2.2271795 2.6779389 2.7528911 2.3290334 1.2912989 -0.033079624 -1.4241121][-2.1444027 -1.8308237 -1.5467372 -1.1331692 -0.65130043 -0.05912447 0.812377 1.7025156 2.5261483 2.9309726 2.9239883 2.3438597 1.1870809 -0.37660789 -1.8909171][-2.0975847 -2.0204947 -1.9785023 -1.6859474 -1.3912024 -0.90750647 -0.095805645 0.67252159 1.3560991 1.630321 1.6532431 1.1598215 0.14451265 -1.2437868 -2.5675528][-2.5208869 -2.5771589 -2.7238731 -2.6989808 -2.6307642 -2.2141795 -1.5025146 -0.89535236 -0.39376783 -0.26948309 -0.25665426 -0.73054171 -1.538111 -2.5678248 -3.4506493][-2.8012404 -2.9980435 -3.2895052 -3.4216471 -3.5128012 -3.3035526 -2.8169384 -2.3496 -1.9636116 -1.968611 -1.976306 -2.3860466 -3.0159531 -3.7410662 -4.2836995][-2.874836 -3.2428951 -3.5914805 -3.7343702 -3.8781736 -3.8573861 -3.6858191 -3.4078774 -3.2058296 -3.3621774 -3.4777453 -3.8340769 -4.1973691 -4.6920967 -4.9503756][-3.0144508 -3.2963514 -3.5982814 -3.742414 -3.8656437 -3.8500457 -3.8282802 -3.7614529 -3.744169 -4.0118823 -4.2675495 -4.6869931 -4.9414053 -5.2059207 -5.3106556][-3.0079832 -3.1193156 -3.2742975 -3.3829222 -3.4352822 -3.3821595 -3.39472 -3.4275484 -3.5483084 -3.8541484 -4.19053 -4.6456246 -4.9098272 -5.0814295 -5.1832089]]...]
INFO - root - 2017-12-16 07:37:02.698819: step 49810, loss = 0.19, batch loss = 0.13 (26.0 examples/sec; 0.308 sec/batch; 24h:11m:08s remains)
INFO - root - 2017-12-16 07:37:05.530237: step 49820, loss = 0.29, batch loss = 0.23 (28.8 examples/sec; 0.278 sec/batch; 21h:49m:46s remains)
INFO - root - 2017-12-16 07:37:08.351017: step 49830, loss = 0.28, batch loss = 0.22 (29.4 examples/sec; 0.272 sec/batch; 21h:20m:45s remains)
INFO - root - 2017-12-16 07:37:11.187824: step 49840, loss = 0.20, batch loss = 0.14 (29.8 examples/sec; 0.269 sec/batch; 21h:05m:56s remains)
INFO - root - 2017-12-16 07:37:14.066912: step 49850, loss = 0.29, batch loss = 0.23 (27.8 examples/sec; 0.288 sec/batch; 22h:37m:40s remains)
INFO - root - 2017-12-16 07:37:16.853267: step 49860, loss = 0.44, batch loss = 0.38 (28.2 examples/sec; 0.284 sec/batch; 22h:16m:02s remains)
INFO - root - 2017-12-16 07:37:19.693442: step 49870, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.286 sec/batch; 22h:25m:03s remains)
INFO - root - 2017-12-16 07:37:22.516976: step 49880, loss = 0.34, batch loss = 0.28 (29.0 examples/sec; 0.276 sec/batch; 21h:40m:09s remains)
INFO - root - 2017-12-16 07:37:25.318085: step 49890, loss = 0.25, batch loss = 0.19 (25.7 examples/sec; 0.311 sec/batch; 24h:24m:04s remains)
INFO - root - 2017-12-16 07:37:28.141691: step 49900, loss = 0.24, batch loss = 0.18 (29.9 examples/sec; 0.268 sec/batch; 21h:00m:35s remains)
2017-12-16 07:37:28.579833: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1248181 -1.3111117 -1.3675489 -1.5210261 -1.718133 -2.0856872 -2.642036 -3.0349469 -3.49976 -4.0182767 -4.1892362 -4.2780509 -4.1849914 -3.9221876 -3.6403184][-0.75514865 -0.90178657 -0.99272871 -1.0909259 -1.3635187 -1.9022667 -2.6307142 -3.3841338 -4.1217384 -4.5514336 -4.7676435 -4.6726465 -4.2217865 -3.8999915 -3.5408783][-0.87775755 -0.99947381 -1.0556192 -1.0938458 -1.3763597 -1.9200819 -2.7928276 -3.5079126 -4.1814427 -4.9868965 -5.3200274 -5.2646461 -4.9674339 -4.4348307 -3.8727098][-1.1856823 -1.2159696 -1.2804949 -1.3709295 -1.7377529 -2.3134336 -3.086236 -3.8502953 -4.5993142 -5.157259 -5.3925877 -5.4894133 -5.2533174 -4.8644557 -4.2984667][-1.9929276 -2.0327296 -2.1975837 -2.1980169 -2.431046 -2.7903805 -3.3686924 -3.9163914 -4.3990154 -4.9600425 -5.3283443 -5.5341816 -5.5151997 -5.1587453 -4.6461916][-2.5373564 -2.6384776 -2.8947525 -2.9465027 -3.058846 -3.1669071 -3.3762877 -3.5969644 -3.8451352 -4.2720771 -4.7934012 -5.2604156 -5.5222244 -5.4666018 -5.0304842][-3.1953115 -3.2791603 -3.4076264 -3.3819661 -3.3873363 -3.284178 -3.1903248 -3.168776 -3.2754583 -3.6307142 -4.161447 -4.7385674 -5.2745414 -5.3333464 -4.9104328][-3.8266037 -3.9006455 -3.9514279 -3.6818364 -3.4252882 -3.0182705 -2.7638836 -2.6518292 -2.6798909 -2.9474931 -3.3893771 -3.94281 -4.4435763 -4.5772009 -4.3931613][-4.1258478 -4.1201634 -3.8899858 -3.42303 -2.9708385 -2.4154041 -2.0840428 -1.8887253 -1.9447889 -2.1714771 -2.6436496 -3.2259569 -3.7484124 -4.0183444 -3.9396038][-3.5640512 -3.5589304 -3.3376765 -2.9448626 -2.579618 -1.8843768 -1.3693428 -1.1409934 -1.1678157 -1.4602704 -1.9779437 -2.5884681 -3.2074287 -3.6003127 -3.7406812][-2.5313959 -2.57224 -2.7802377 -2.6935143 -2.33522 -1.669843 -1.1691761 -0.76901674 -0.74670243 -1.0597522 -1.6722145 -2.5048132 -3.1796048 -3.535284 -3.7424095][-1.9676964 -2.3086541 -2.7910194 -2.919616 -2.6703033 -1.9741433 -1.3532248 -1.0030866 -0.95066142 -1.1378307 -1.7314065 -2.5702252 -3.2756386 -3.6530168 -3.8276467][-1.9015222 -2.4202373 -3.2689743 -3.7296937 -3.6075866 -2.8174448 -1.9458804 -1.4316745 -1.1465931 -1.3357022 -1.9124615 -2.6681337 -3.3053153 -3.6597857 -3.8032207][-2.1533136 -3.0212595 -4.2659035 -4.8445096 -4.7728944 -3.9848547 -3.0403328 -2.2333317 -1.8061759 -1.8269262 -2.231282 -2.8512187 -3.3564901 -3.652132 -3.720078][-2.998879 -4.015974 -5.2014365 -5.850831 -5.7270904 -4.7168508 -3.4423089 -2.5477238 -2.0283051 -2.0457878 -2.4891076 -2.98363 -3.3448362 -3.5219676 -3.4516556]]...]
INFO - root - 2017-12-16 07:37:31.363708: step 49910, loss = 0.30, batch loss = 0.24 (29.0 examples/sec; 0.276 sec/batch; 21h:41m:19s remains)
INFO - root - 2017-12-16 07:37:34.222396: step 49920, loss = 0.22, batch loss = 0.16 (27.6 examples/sec; 0.290 sec/batch; 22h:43m:47s remains)
INFO - root - 2017-12-16 07:37:37.032678: step 49930, loss = 0.22, batch loss = 0.16 (27.7 examples/sec; 0.289 sec/batch; 22h:40m:09s remains)
INFO - root - 2017-12-16 07:37:39.862038: step 49940, loss = 0.27, batch loss = 0.21 (29.0 examples/sec; 0.275 sec/batch; 21h:37m:17s remains)
INFO - root - 2017-12-16 07:37:42.676029: step 49950, loss = 0.30, batch loss = 0.24 (29.2 examples/sec; 0.274 sec/batch; 21h:32m:14s remains)
INFO - root - 2017-12-16 07:37:45.491481: step 49960, loss = 0.24, batch loss = 0.18 (29.7 examples/sec; 0.269 sec/batch; 21h:07m:22s remains)
INFO - root - 2017-12-16 07:37:48.295171: step 49970, loss = 0.26, batch loss = 0.20 (29.0 examples/sec; 0.275 sec/batch; 21h:36m:46s remains)
INFO - root - 2017-12-16 07:37:51.103589: step 49980, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.278 sec/batch; 21h:51m:11s remains)
INFO - root - 2017-12-16 07:37:53.905041: step 49990, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.282 sec/batch; 22h:06m:27s remains)
INFO - root - 2017-12-16 07:37:56.721407: step 50000, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.279 sec/batch; 21h:54m:07s remains)
2017-12-16 07:37:57.178535: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9480073 -3.4904549 -3.1099875 -2.9186165 -2.9734056 -3.5357096 -4.31532 -5.1891909 -5.8114853 -5.9475594 -5.8275895 -5.2813406 -4.519372 -3.8175483 -3.3740094][-2.9852345 -2.5926704 -2.2039611 -2.0982268 -2.1496277 -2.775506 -3.6861296 -4.5863976 -5.2760744 -5.5293803 -5.4485655 -4.9268541 -4.1636844 -3.5728822 -3.18787][-1.6776788 -1.375504 -1.2286296 -1.0705249 -1.1405809 -1.6265848 -2.49944 -3.4357712 -4.0465508 -4.3774657 -4.569488 -4.4938889 -4.1248212 -3.6166854 -3.1926413][-0.54548264 -0.15228939 -0.078263283 -0.23202467 -0.44815946 -0.85593271 -1.3135605 -2.0240955 -2.611474 -3.0035243 -3.5323403 -3.8871021 -3.9048262 -3.613477 -3.2869339][0.26502705 0.60135031 0.73735809 0.47191429 0.37015724 0.20939541 0.13143349 -0.23546934 -0.80579448 -1.3660774 -2.1639621 -3.0855706 -3.6020398 -3.6440082 -3.3756709][0.47579956 0.6637578 0.74282503 0.62126923 0.63771677 0.92390871 1.3639789 1.4580116 1.1037965 0.29886627 -1.0070012 -2.5547495 -3.5314014 -3.802928 -3.5896966][-0.18543625 0.0367136 0.070249081 0.10675192 0.38928986 1.1172452 2.1691222 2.6489754 2.3737531 1.3705711 -0.24490643 -2.0470674 -3.3505244 -3.8498647 -3.6794915][-0.88586164 -0.62732244 -0.50663877 -0.22757244 0.37577343 1.3252335 2.4822116 3.0853181 2.8242784 1.6565528 -0.049944878 -1.8185616 -3.171845 -3.7826409 -3.7127237][-1.8164945 -1.7016475 -1.4435112 -1.1293886 -0.45818734 0.691988 1.9095798 2.5713882 2.3985605 1.3528881 -0.24245644 -1.9454393 -3.3225946 -3.9524186 -3.999676][-2.6121402 -2.6177857 -2.5533791 -2.1979456 -1.4494007 -0.40406513 0.80726337 1.5383711 1.4449363 0.48336506 -0.90561438 -2.4315033 -3.72227 -4.2934337 -4.2841525][-2.7522783 -2.8542862 -2.8798676 -2.62881 -1.9634516 -1.0702386 -0.15627575 0.36534786 0.18728733 -0.7317555 -1.9121883 -3.2003183 -4.1776266 -4.6109452 -4.5559196][-2.5048027 -2.7423077 -2.9516327 -2.8038161 -2.3511541 -1.6015809 -0.88704348 -0.69365144 -0.93389368 -1.7166169 -2.761023 -3.7943368 -4.4981041 -4.6465812 -4.5158229][-1.9205818 -2.3839169 -2.8522844 -2.8090425 -2.5208843 -2.0769098 -1.6017449 -1.4833441 -1.7702699 -2.4692845 -3.2979379 -4.0529184 -4.5478258 -4.5480824 -4.32464][-1.488261 -2.0502579 -2.6416106 -2.8767946 -2.5979548 -2.0797863 -1.6338906 -1.6192267 -1.865696 -2.5300646 -3.4175611 -4.0580287 -4.398591 -4.3046045 -3.9649119][-1.4214406 -1.9999597 -2.5769246 -2.7331028 -2.4261327 -1.8420935 -1.2427883 -1.2208056 -1.5653644 -2.2650969 -3.0431156 -3.6416264 -3.9081278 -3.7538438 -3.4406862]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 07:38:00.400181: step 50010, loss = 0.27, batch loss = 0.21 (28.9 examples/sec; 0.277 sec/batch; 21h:43m:45s remains)
INFO - root - 2017-12-16 07:38:03.184359: step 50020, loss = 0.35, batch loss = 0.29 (29.1 examples/sec; 0.275 sec/batch; 21h:36m:05s remains)
INFO - root - 2017-12-16 07:38:06.056156: step 50030, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 22h:09m:59s remains)
INFO - root - 2017-12-16 07:38:08.928927: step 50040, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.284 sec/batch; 22h:17m:12s remains)
INFO - root - 2017-12-16 07:38:11.776887: step 50050, loss = 0.28, batch loss = 0.22 (29.0 examples/sec; 0.276 sec/batch; 21h:40m:34s remains)
INFO - root - 2017-12-16 07:38:14.615938: step 50060, loss = 0.29, batch loss = 0.23 (28.0 examples/sec; 0.286 sec/batch; 22h:26m:47s remains)
INFO - root - 2017-12-16 07:38:17.410985: step 50070, loss = 0.26, batch loss = 0.21 (29.3 examples/sec; 0.273 sec/batch; 21h:26m:59s remains)
INFO - root - 2017-12-16 07:38:20.202065: step 50080, loss = 0.26, batch loss = 0.20 (29.1 examples/sec; 0.275 sec/batch; 21h:32m:04s remains)
INFO - root - 2017-12-16 07:38:23.043130: step 50090, loss = 0.22, batch loss = 0.17 (27.6 examples/sec; 0.290 sec/batch; 22h:44m:15s remains)
INFO - root - 2017-12-16 07:38:25.836031: step 50100, loss = 0.25, batch loss = 0.19 (28.7 examples/sec; 0.279 sec/batch; 21h:54m:09s remains)
2017-12-16 07:38:26.312889: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3433352 -3.4087481 -3.4020014 -3.4762135 -3.7392902 -4.0761185 -4.3082294 -4.1737428 -4.0896592 -4.0016909 -4.0603242 -4.2192841 -4.6380358 -5.0864873 -5.3825445][-3.5099518 -3.4451237 -3.3278811 -3.3451643 -3.5526214 -3.8929479 -3.9966426 -3.8687708 -3.7252426 -3.6386955 -3.780549 -4.147058 -4.5728769 -5.1347051 -5.45434][-3.5711396 -3.1846964 -2.9272137 -2.7639844 -2.7909365 -3.0052028 -3.1739378 -3.1989269 -3.1561694 -3.1105328 -3.1825252 -3.6365418 -4.3252687 -4.8480582 -5.0481691][-3.0496054 -2.3765147 -1.8260276 -1.4019051 -1.2941113 -1.4645114 -1.7801111 -1.8907166 -1.8997293 -2.0441713 -2.2859075 -2.5843115 -3.0391145 -3.6335669 -4.0799327][-2.0599546 -1.0508769 -0.34123611 0.073829174 0.24565935 0.0090241432 -0.4054451 -0.60214853 -0.69401479 -0.75118685 -0.89138389 -1.2267172 -1.5276372 -1.8827758 -2.3213556][-0.9810791 0.2333827 1.0389152 1.4241571 1.5120487 1.2983279 0.95358658 0.59930754 0.26092386 0.098047733 0.04594183 -0.12295818 -0.20492268 -0.45553231 -0.81781554][0.18275166 1.1536102 1.9900646 2.55761 2.7607155 2.6956954 2.3986154 1.9975958 1.6152439 1.1207175 0.78837395 0.45979166 0.36773634 0.084750652 -0.52558994][0.654243 1.5429068 2.0447187 2.6157413 3.0145869 3.1937194 3.1219425 2.637249 2.1351666 1.4658818 0.86173487 0.49686146 0.47778797 0.1700573 -0.61188936][-0.13279676 0.48043346 1.0211115 1.6150107 2.0101376 2.4006357 2.5752048 2.2113495 1.562077 0.77600956 0.18495464 -0.35919666 -0.42520285 -0.54291558 -1.0791497][-0.82743716 -0.86355329 -0.47245574 0.23086834 0.934659 1.4409237 1.4333897 1.1827831 0.50428295 -0.48391414 -1.1966872 -1.6419821 -1.6803575 -1.7733736 -1.9831049][-1.9450006 -1.9525564 -1.601598 -1.1519861 -0.5156467 0.083771229 0.23140955 -0.26381159 -1.0429769 -1.8436639 -2.5739503 -3.0261757 -2.998384 -2.7344608 -2.7226248][-2.8096814 -3.081388 -3.2113829 -2.660661 -2.0662928 -1.6302989 -1.5414264 -1.8382123 -2.3749959 -3.0741291 -3.6329806 -3.907155 -3.8070531 -3.3914056 -3.0142283][-3.1763587 -3.403398 -3.6774006 -3.5825853 -3.347733 -2.9712467 -2.8848104 -3.0533929 -3.3327069 -3.7501101 -4.087646 -4.1391983 -3.8659425 -3.4469607 -2.8863726][-3.2725039 -3.4594233 -3.5662942 -3.6501198 -3.653832 -3.5692089 -3.5449259 -3.6886859 -3.9053493 -4.193953 -4.3269486 -4.2343121 -3.7923546 -3.1060479 -2.4423008][-3.511343 -3.6282458 -3.7483876 -3.7822418 -3.679213 -3.6155241 -3.6902289 -3.8557758 -4.046216 -4.3603678 -4.5818205 -4.3932271 -3.7348461 -2.8407159 -1.8477671]]...]
INFO - root - 2017-12-16 07:38:29.169341: step 50110, loss = 0.27, batch loss = 0.21 (28.8 examples/sec; 0.277 sec/batch; 21h:45m:11s remains)
INFO - root - 2017-12-16 07:38:32.026758: step 50120, loss = 0.28, batch loss = 0.22 (25.6 examples/sec; 0.312 sec/batch; 24h:30m:06s remains)
INFO - root - 2017-12-16 07:38:34.887417: step 50130, loss = 0.28, batch loss = 0.23 (27.4 examples/sec; 0.292 sec/batch; 22h:52m:15s remains)
INFO - root - 2017-12-16 07:38:37.719243: step 50140, loss = 0.24, batch loss = 0.19 (27.8 examples/sec; 0.288 sec/batch; 22h:36m:04s remains)
INFO - root - 2017-12-16 07:38:40.574512: step 50150, loss = 0.22, batch loss = 0.16 (28.3 examples/sec; 0.282 sec/batch; 22h:08m:03s remains)
INFO - root - 2017-12-16 07:38:43.397879: step 50160, loss = 0.28, batch loss = 0.22 (27.5 examples/sec; 0.291 sec/batch; 22h:50m:17s remains)
INFO - root - 2017-12-16 07:38:46.253176: step 50170, loss = 0.27, batch loss = 0.22 (28.3 examples/sec; 0.283 sec/batch; 22h:10m:40s remains)
INFO - root - 2017-12-16 07:38:49.114146: step 50180, loss = 0.35, batch loss = 0.29 (28.7 examples/sec; 0.279 sec/batch; 21h:50m:44s remains)
INFO - root - 2017-12-16 07:38:51.966781: step 50190, loss = 0.29, batch loss = 0.23 (28.5 examples/sec; 0.281 sec/batch; 22h:02m:38s remains)
INFO - root - 2017-12-16 07:38:54.841279: step 50200, loss = 0.26, batch loss = 0.20 (28.2 examples/sec; 0.284 sec/batch; 22h:15m:09s remains)
2017-12-16 07:38:55.322635: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2989812 -5.3430209 -5.43882 -5.3596897 -5.2993531 -5.1802564 -5.0925188 -4.9410472 -4.6499863 -4.5325518 -4.3985605 -4.165659 -4.0090733 -3.8947175 -3.9615331][-5.3578949 -5.4755707 -5.5538688 -5.4678283 -5.3145213 -5.2314939 -5.1703529 -4.9431267 -4.6064844 -4.4423256 -4.2273693 -3.9269462 -3.8471618 -3.7839298 -3.8751457][-5.3373842 -5.2913222 -5.2507162 -5.1403484 -4.8419275 -4.6339345 -4.4701095 -4.284287 -4.0578523 -3.8800561 -3.6197348 -3.3924022 -3.360045 -3.3737476 -3.5860112][-4.7429552 -4.5855379 -4.4537067 -4.3157206 -3.9902089 -3.6842251 -3.3928356 -3.0948586 -2.8350325 -2.6501844 -2.3776114 -2.1197941 -2.01359 -2.0726285 -2.3292158][-3.9696362 -3.6179996 -3.2437143 -2.8944836 -2.3311539 -1.8499074 -1.4581091 -1.1552453 -1.0526555 -1.0969987 -1.1065342 -1.0598145 -1.0257156 -1.1818132 -1.4035003][-2.9868541 -2.5393147 -1.9986703 -1.45276 -0.71626091 -0.12821579 0.37364149 0.52321053 0.31679106 0.13924694 0.024438858 -0.13634109 -0.21139097 -0.36880159 -0.54339623][-2.5824571 -1.8262985 -0.98637557 -0.16135454 0.63103247 1.2591438 1.7443328 1.735723 1.4123087 0.9819355 0.68554163 0.52931643 0.49789238 0.3755827 0.14838362][-2.5218091 -1.511116 -0.370121 0.60321 1.4759979 2.1301641 2.495244 2.4679904 2.1596432 1.5549035 1.1101551 0.85064125 0.71827316 0.54256916 0.25251865][-2.3928595 -1.4266019 -0.38984108 0.617774 1.6052208 2.1975665 2.471334 2.4735174 2.187541 1.6191378 1.1853914 0.90643835 0.77528906 0.57627058 0.23537588][-2.5273952 -1.8484643 -1.0340815 -0.11902285 0.81044054 1.3613224 1.6556931 1.6243572 1.3116961 0.87633419 0.54740286 0.39003992 0.36207342 0.25110245 -0.066213131][-3.298532 -2.8189142 -2.143358 -1.2890971 -0.56124091 -0.075135708 0.16635513 0.17888737 0.03873682 -0.23989964 -0.47126436 -0.53195429 -0.57497 -0.6492703 -0.89496851][-3.8778119 -3.6997979 -3.3904579 -2.8370023 -2.2602196 -1.9600832 -1.8508103 -1.7531247 -1.6868138 -1.7017829 -1.7250006 -1.6077745 -1.624155 -1.6253421 -1.7869568][-4.8330994 -5.052774 -5.0846872 -4.8544173 -4.3528614 -4.0060263 -3.861166 -3.6320622 -3.4174271 -3.0726795 -2.7311053 -2.3583217 -2.1697137 -2.045517 -2.0452843][-6.2302837 -6.5580988 -6.6377754 -6.5699763 -6.1207428 -5.7038116 -5.3983731 -5.1212416 -4.902153 -4.3300157 -3.8257961 -3.2573385 -2.8320036 -2.4961562 -2.3339841][-6.7520957 -7.0992789 -7.1764317 -7.0613632 -6.7118015 -6.4170966 -6.2180276 -6.0116215 -5.8411531 -5.3228178 -4.8911657 -4.3320169 -3.8055563 -3.3104568 -2.9218531]]...]
INFO - root - 2017-12-16 07:38:58.131500: step 50210, loss = 0.27, batch loss = 0.21 (28.4 examples/sec; 0.282 sec/batch; 22h:05m:34s remains)
INFO - root - 2017-12-16 07:39:00.932520: step 50220, loss = 0.28, batch loss = 0.22 (27.9 examples/sec; 0.287 sec/batch; 22h:31m:18s remains)
INFO - root - 2017-12-16 07:39:03.792515: step 50230, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 22h:26m:23s remains)
INFO - root - 2017-12-16 07:39:06.604746: step 50240, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 21h:55m:08s remains)
INFO - root - 2017-12-16 07:39:09.385548: step 50250, loss = 0.23, batch loss = 0.17 (29.1 examples/sec; 0.275 sec/batch; 21h:34m:57s remains)
INFO - root - 2017-12-16 07:39:12.240792: step 50260, loss = 0.25, batch loss = 0.19 (29.0 examples/sec; 0.275 sec/batch; 21h:35m:47s remains)
INFO - root - 2017-12-16 07:39:15.070106: step 50270, loss = 0.30, batch loss = 0.25 (26.3 examples/sec; 0.304 sec/batch; 23h:48m:34s remains)
INFO - root - 2017-12-16 07:39:17.925896: step 50280, loss = 0.22, batch loss = 0.16 (27.1 examples/sec; 0.295 sec/batch; 23h:08m:37s remains)
INFO - root - 2017-12-16 07:39:20.738160: step 50290, loss = 0.26, batch loss = 0.20 (27.2 examples/sec; 0.295 sec/batch; 23h:05m:22s remains)
INFO - root - 2017-12-16 07:39:23.583121: step 50300, loss = 0.28, batch loss = 0.23 (28.8 examples/sec; 0.278 sec/batch; 21h:46m:43s remains)
2017-12-16 07:39:24.042203: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2705846 -5.5672364 -4.9826794 -4.7806277 -4.8189416 -5.0250425 -5.1056967 -5.3035855 -5.55042 -5.7896085 -6.1523128 -6.500721 -6.7657857 -6.6283851 -6.400568][-6.4998617 -5.6284428 -4.9020877 -4.637991 -4.6098962 -4.8623528 -4.9685903 -5.2243519 -5.5244255 -5.7569213 -6.1918211 -6.540803 -6.6531014 -6.394958 -6.1321096][-6.2523079 -5.1937218 -4.3521295 -4.0202408 -3.9066987 -4.0765471 -4.0438294 -4.368978 -4.8273749 -5.1601105 -5.7435102 -6.1165485 -6.2260914 -5.7517347 -5.40221][-5.4462504 -4.2597041 -3.2522511 -2.7831645 -2.5583191 -2.6112804 -2.62483 -2.9791989 -3.4550207 -4.0569205 -4.7109089 -5.1722193 -5.3314924 -5.032196 -4.7637539][-4.4574375 -2.9923208 -1.5992973 -0.842659 -0.44512486 -0.47553515 -0.58600879 -1.0030558 -1.6177404 -2.4623032 -3.347513 -3.9936953 -4.2880144 -4.1555977 -4.0966835][-3.3996625 -1.6966581 -0.16463184 0.93352842 1.6910167 1.6725726 1.4476728 0.91954565 0.20244598 -0.870095 -2.0349283 -2.8101749 -3.2856865 -3.4249811 -3.4690349][-2.7227967 -1.230505 0.13094473 1.4241014 2.3969579 2.6998386 2.6225104 2.0128026 1.133997 -0.045938492 -1.2980199 -2.3969884 -3.163794 -3.4921622 -3.8080461][-2.9726994 -1.4748995 -0.21917248 0.96477795 1.9626193 2.413331 2.4438357 2.0455232 1.4554191 0.3092165 -1.0278499 -2.1411877 -3.1723094 -3.8191216 -4.3856535][-3.9199128 -2.6942413 -1.655468 -0.54814649 0.41021156 1.0505428 1.2721758 1.1524148 0.78253126 -0.055328369 -1.1248882 -2.1966941 -3.2845488 -4.0895982 -4.8771982][-5.5855207 -4.4928064 -3.5889339 -2.5617027 -1.7656312 -1.0945284 -0.84919786 -0.62598681 -0.59126973 -1.0894611 -1.845619 -2.7119403 -3.7053623 -4.5712986 -5.4199252][-7.1705775 -6.29357 -5.5620313 -4.5328903 -3.7186625 -3.0909982 -2.6722736 -2.35263 -2.2009227 -2.2312665 -2.5000653 -3.2188606 -4.1314378 -5.0026627 -5.9274826][-8.2087812 -7.2426739 -6.3250766 -5.4011889 -5.0119905 -4.5541997 -4.325182 -3.9077582 -3.4456065 -3.1070504 -3.0250735 -3.5035028 -4.223031 -5.2574525 -6.1991119][-8.5145607 -7.5048213 -6.7349396 -5.9494905 -5.5792775 -5.3439078 -5.1468334 -4.8837943 -4.52373 -4.0079451 -3.6366708 -3.9512858 -4.4908123 -5.3969321 -6.2470932][-8.4852715 -7.1752176 -6.1996145 -5.6735268 -5.728951 -5.6738129 -5.7520833 -5.6736856 -5.3053308 -4.7784328 -4.3825378 -4.4806204 -4.8195591 -5.4917984 -6.1737909][-7.3620243 -6.0603733 -5.2982159 -5.0942097 -5.3662081 -5.5830932 -5.8002586 -5.8455124 -5.7262325 -5.2982044 -4.8966112 -4.994729 -5.1520238 -5.4764814 -5.7273989]]...]
INFO - root - 2017-12-16 07:39:26.917663: step 50310, loss = 0.28, batch loss = 0.22 (27.4 examples/sec; 0.292 sec/batch; 22h:52m:32s remains)
INFO - root - 2017-12-16 07:39:29.691748: step 50320, loss = 0.38, batch loss = 0.33 (29.1 examples/sec; 0.275 sec/batch; 21h:33m:08s remains)
INFO - root - 2017-12-16 07:39:32.581899: step 50330, loss = 0.27, batch loss = 0.21 (26.7 examples/sec; 0.300 sec/batch; 23h:29m:09s remains)
INFO - root - 2017-12-16 07:39:35.424805: step 50340, loss = 0.31, batch loss = 0.25 (28.8 examples/sec; 0.278 sec/batch; 21h:46m:28s remains)
INFO - root - 2017-12-16 07:39:38.300335: step 50350, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.283 sec/batch; 22h:13m:00s remains)
INFO - root - 2017-12-16 07:39:41.106647: step 50360, loss = 0.24, batch loss = 0.18 (27.9 examples/sec; 0.287 sec/batch; 22h:28m:38s remains)
INFO - root - 2017-12-16 07:39:43.977944: step 50370, loss = 0.24, batch loss = 0.18 (26.5 examples/sec; 0.301 sec/batch; 23h:37m:36s remains)
INFO - root - 2017-12-16 07:39:46.820950: step 50380, loss = 0.23, batch loss = 0.17 (27.6 examples/sec; 0.290 sec/batch; 22h:43m:31s remains)
INFO - root - 2017-12-16 07:39:49.673318: step 50390, loss = 0.22, batch loss = 0.16 (28.3 examples/sec; 0.283 sec/batch; 22h:09m:42s remains)
INFO - root - 2017-12-16 07:39:52.497326: step 50400, loss = 0.23, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 22h:19m:55s remains)
2017-12-16 07:39:52.993852: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9724381 -4.3212543 -4.6247983 -4.748702 -4.7132549 -4.5131993 -4.2726936 -4.0912323 -3.9720042 -3.8112063 -3.6463814 -3.6002436 -3.54295 -3.4477391 -3.3356805][-4.1736088 -4.7281923 -5.1198754 -5.2878256 -5.09894 -4.8988776 -4.5695744 -4.3368111 -4.0556707 -3.8107927 -3.779434 -3.6693521 -3.5841208 -3.5317881 -3.4144173][-4.3176856 -5.0046382 -5.5688815 -5.7064662 -5.4467688 -5.0125284 -4.4542751 -4.0780311 -3.7409449 -3.7034826 -3.7421317 -3.8095148 -3.9821255 -3.9459763 -3.8017561][-4.551 -5.2074437 -5.5551434 -5.5505748 -5.1116743 -4.2813559 -3.4675169 -2.854455 -2.6204729 -2.857368 -3.2368526 -3.7403347 -4.2325654 -4.4509282 -4.3762965][-4.2454791 -4.7603455 -4.9106727 -4.4726381 -3.5074542 -2.3125319 -1.1020179 -0.34938478 -0.36655855 -0.91591644 -1.9664147 -3.1370692 -3.8604925 -4.3601894 -4.3937283][-3.9906039 -4.0625105 -3.8322217 -2.9030688 -1.2386034 0.53070974 1.9466105 2.6388292 2.5104175 1.4176984 -0.2057085 -1.857733 -3.1586919 -3.871532 -3.8016322][-3.8189745 -3.7155054 -3.027813 -1.6109211 0.31989431 2.7117586 4.8165741 5.6423979 5.12002 3.5516729 1.5266504 -0.59128475 -2.0274665 -2.8828743 -3.0343471][-3.5153766 -3.2320852 -2.7588801 -1.1258972 1.1517863 3.7749529 5.8807697 6.9124517 6.5344219 4.8650665 2.6176634 0.28872442 -1.3318889 -2.2790499 -2.4376364][-3.5429575 -3.4752095 -2.8410361 -1.5427268 0.067468643 2.5984612 4.7674608 5.9414825 5.6677227 4.1880083 2.2894125 0.13947248 -1.3810842 -2.2226291 -2.2673352][-3.8036268 -3.9562914 -4.136652 -3.3470221 -1.8877742 0.009853363 1.5128684 2.8175168 2.9811358 2.049005 0.41526556 -1.2626457 -2.4586272 -3.0665636 -2.965332][-4.6221747 -5.0205765 -5.2503362 -4.8805513 -4.27383 -2.9901602 -1.5556042 -0.36185074 -0.17792845 -0.476676 -1.6098936 -2.8534966 -3.6816678 -4.0168486 -3.8014765][-4.93939 -5.4566669 -5.9154282 -6.0011206 -5.6909885 -4.8289714 -4.1224575 -3.2553675 -2.607724 -2.5586743 -3.0791106 -3.8131361 -4.4230742 -4.592309 -4.2195415][-4.8550968 -5.3275642 -5.6519842 -5.8797607 -6.0372567 -5.6960897 -5.2116985 -4.5570784 -4.1131988 -3.8506093 -3.7643023 -3.983252 -4.179502 -4.3005767 -4.1030641][-4.4737844 -4.6104851 -4.71335 -4.8667946 -4.9847193 -4.9910874 -5.015007 -4.6975641 -4.291955 -3.9643495 -3.731 -3.6553488 -3.686168 -3.7131572 -3.5304437][-4.0187025 -3.8447487 -3.7150371 -3.6216273 -3.6405253 -3.7047465 -3.7231445 -3.6451175 -3.5742869 -3.3059611 -3.0851464 -2.971869 -2.9214287 -3.0810091 -3.0873346]]...]
INFO - root - 2017-12-16 07:39:55.859104: step 50410, loss = 0.25, batch loss = 0.19 (28.5 examples/sec; 0.281 sec/batch; 21h:58m:58s remains)
INFO - root - 2017-12-16 07:39:58.698401: step 50420, loss = 0.24, batch loss = 0.18 (29.1 examples/sec; 0.275 sec/batch; 21h:33m:12s remains)
INFO - root - 2017-12-16 07:40:01.500671: step 50430, loss = 0.29, batch loss = 0.23 (28.6 examples/sec; 0.280 sec/batch; 21h:55m:40s remains)
INFO - root - 2017-12-16 07:40:04.278704: step 50440, loss = 0.27, batch loss = 0.22 (29.2 examples/sec; 0.274 sec/batch; 21h:27m:08s remains)
INFO - root - 2017-12-16 07:40:07.075910: step 50450, loss = 0.46, batch loss = 0.40 (29.1 examples/sec; 0.275 sec/batch; 21h:31m:13s remains)
INFO - root - 2017-12-16 07:40:09.952701: step 50460, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 21h:46m:12s remains)
INFO - root - 2017-12-16 07:40:12.827618: step 50470, loss = 0.24, batch loss = 0.18 (28.5 examples/sec; 0.280 sec/batch; 21h:58m:06s remains)
INFO - root - 2017-12-16 07:40:15.650253: step 50480, loss = 0.21, batch loss = 0.15 (28.2 examples/sec; 0.284 sec/batch; 22h:15m:43s remains)
INFO - root - 2017-12-16 07:40:18.467500: step 50490, loss = 0.26, batch loss = 0.20 (29.2 examples/sec; 0.274 sec/batch; 21h:27m:06s remains)
INFO - root - 2017-12-16 07:40:21.278273: step 50500, loss = 0.54, batch loss = 0.48 (29.3 examples/sec; 0.273 sec/batch; 21h:24m:14s remains)
2017-12-16 07:40:21.749609: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.146136 -2.7927299 -3.5566943 -4.1792879 -4.7385073 -5.0840211 -5.3865309 -5.464922 -5.4762921 -5.45409 -5.4057927 -5.2921152 -5.3513441 -5.3039041 -5.2153368][-1.225116 -1.8357103 -2.6250534 -3.2636681 -3.8852532 -4.2184882 -4.3208427 -4.2550521 -4.1376715 -4.0495443 -3.9968376 -3.9213073 -4.0266905 -4.1605268 -4.37159][-0.94837713 -1.5502131 -2.3145697 -2.8032427 -3.1363235 -3.2796855 -3.0196352 -2.6409874 -2.470016 -2.3804471 -2.3429275 -2.5687747 -3.0460963 -3.46599 -3.9702663][-1.1554446 -1.8073742 -2.591413 -3.0514483 -3.043931 -2.6657264 -1.8362207 -0.96472096 -0.47479844 -0.54489231 -0.77456856 -1.1703377 -1.8669612 -2.6721547 -3.518786][-2.0721622 -2.8908858 -3.6125712 -3.8690972 -3.4701314 -2.4439216 -0.99242735 0.35304451 1.1229734 1.0852695 0.70260096 -0.14206982 -0.921633 -1.7569764 -2.5998502][-2.9512906 -3.5430896 -4.084898 -4.0287642 -3.1131995 -1.5848207 0.37151623 2.0696774 3.025578 2.7968006 1.9469509 1.0052323 0.13297558 -0.71060753 -1.2832942][-3.3549194 -3.6003482 -3.7609797 -3.4391708 -2.2915778 -0.36522388 1.981461 3.918623 4.89268 4.5747719 3.4673338 2.2573972 1.2875981 0.55106544 0.069105148][-3.3836038 -3.410006 -3.4287519 -2.921803 -1.7025537 0.32623816 2.6873555 4.4572582 5.4573421 5.1323376 4.0894642 2.9642754 1.980526 1.2311444 0.8645587][-3.0908775 -3.0083494 -2.8061659 -2.4314237 -1.6494868 -0.036885738 1.9637041 3.5708361 4.4210663 3.9962282 3.1178575 2.346159 1.748394 1.2954926 0.94777632][-2.7489552 -2.7653744 -2.954798 -2.6977878 -2.0648057 -1.2881908 0.025835991 1.5566087 2.3167949 2.0183134 1.464993 1.0220342 0.67356539 0.56542253 0.32085609][-2.9494839 -2.8445318 -2.9682589 -3.2291436 -3.1989141 -2.5775225 -1.7002332 -0.95592117 -0.44077682 -0.45765543 -0.71688032 -0.97657824 -1.1646104 -1.1460257 -1.2200441][-2.8732216 -3.0524075 -3.4476128 -3.9283762 -4.1087747 -3.8966732 -3.4119811 -2.9383492 -2.5154982 -2.37162 -2.382221 -2.6456089 -3.0502453 -3.1647174 -3.2344823][-2.6877961 -2.9468224 -3.4498787 -4.1702747 -4.7099633 -4.7024736 -4.3880787 -4.0376978 -3.6214375 -3.4353609 -3.3943138 -3.4293203 -3.7079654 -4.0393915 -4.3501244][-2.6327298 -2.9665351 -3.5202579 -4.3527422 -5.0261235 -5.2079048 -5.0034895 -4.6940126 -4.2699556 -3.7863002 -3.356916 -3.4958005 -4.0898781 -4.4852448 -4.6236362][-2.0535238 -2.4175005 -3.0995359 -3.9837029 -4.7702904 -4.9138269 -4.4633856 -3.9982455 -3.4096174 -2.8155537 -2.558454 -2.7574911 -3.3696685 -4.1237125 -4.5754128]]...]
INFO - root - 2017-12-16 07:40:24.590508: step 50510, loss = 0.25, batch loss = 0.19 (29.1 examples/sec; 0.275 sec/batch; 21h:33m:54s remains)
INFO - root - 2017-12-16 07:40:27.421370: step 50520, loss = 0.26, batch loss = 0.20 (28.8 examples/sec; 0.278 sec/batch; 21h:45m:14s remains)
INFO - root - 2017-12-16 07:40:30.222619: step 50530, loss = 0.22, batch loss = 0.16 (28.9 examples/sec; 0.277 sec/batch; 21h:40m:13s remains)
INFO - root - 2017-12-16 07:40:33.083360: step 50540, loss = 0.26, batch loss = 0.20 (28.5 examples/sec; 0.280 sec/batch; 21h:56m:50s remains)
INFO - root - 2017-12-16 07:40:35.908671: step 50550, loss = 0.24, batch loss = 0.19 (29.4 examples/sec; 0.272 sec/batch; 21h:18m:00s remains)
INFO - root - 2017-12-16 07:40:38.814791: step 50560, loss = 0.20, batch loss = 0.14 (25.8 examples/sec; 0.310 sec/batch; 24h:14m:25s remains)
INFO - root - 2017-12-16 07:40:41.655419: step 50570, loss = 0.48, batch loss = 0.42 (27.7 examples/sec; 0.289 sec/batch; 22h:36m:09s remains)
INFO - root - 2017-12-16 07:40:44.510368: step 50580, loss = 0.28, batch loss = 0.22 (27.9 examples/sec; 0.286 sec/batch; 22h:25m:26s remains)
INFO - root - 2017-12-16 07:40:47.321776: step 50590, loss = 0.24, batch loss = 0.18 (28.9 examples/sec; 0.277 sec/batch; 21h:39m:51s remains)
INFO - root - 2017-12-16 07:40:50.088610: step 50600, loss = 0.43, batch loss = 0.38 (29.7 examples/sec; 0.269 sec/batch; 21h:05m:28s remains)
2017-12-16 07:40:50.568535: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8141007 -3.0894346 -3.4148765 -3.8121266 -4.1667905 -4.4860535 -4.7990284 -5.23494 -5.6356678 -5.8622332 -5.87107 -5.7670813 -5.5647659 -5.1333661 -4.9138846][-2.4827819 -2.7004085 -2.9969907 -3.4568281 -3.8695745 -4.4413314 -4.9384146 -5.4096837 -5.8523755 -6.2483759 -6.5676041 -6.5053835 -6.2437592 -5.6476822 -5.0946407][-2.1240613 -2.2124553 -2.5219793 -3.0056493 -3.3640552 -3.8842516 -4.3213606 -4.8482232 -5.3776832 -5.8714285 -6.4067116 -6.5833521 -6.4963174 -5.7557888 -5.0501947][-1.675622 -1.7095003 -1.8887146 -2.3077693 -2.7042956 -3.0512309 -3.2293341 -3.4236176 -3.7546375 -4.3933382 -5.1545739 -5.7605314 -5.958777 -5.3690977 -4.5504637][-1.2578914 -1.0821662 -1.2052689 -1.5829 -1.7663374 -1.6363235 -1.3574972 -1.2896206 -1.6321893 -2.2975056 -3.2492168 -4.2725987 -4.863986 -4.7100186 -4.1292048][-1.1179833 -0.79382205 -0.73814726 -0.74717689 -0.48289323 -0.047214985 0.57121944 1.1860352 1.127667 0.17005062 -1.3950086 -2.8194377 -3.6842144 -3.8325467 -3.5079238][-1.0507092 -0.76951289 -0.69656849 -0.46554995 0.089116573 1.0788088 2.3723211 3.1918459 3.0564346 2.134676 0.63051224 -1.121243 -2.5279899 -3.0110886 -2.8220968][-1.0731647 -0.85762286 -0.84407878 -0.50589633 0.30074453 1.6109772 3.1257205 4.2046385 4.4448357 3.500555 1.8282061 0.12115574 -1.3236067 -2.2570281 -2.479672][-1.5327308 -1.1638484 -1.0541003 -0.8274951 -0.17725849 1.1860919 2.6973433 3.9442034 4.3762741 3.6148453 2.1440845 0.401906 -1.0978978 -2.0823226 -2.2605157][-2.000319 -2.0412526 -2.1627152 -1.9243336 -1.2774556 -0.33751631 0.70031452 1.9161267 2.5263062 2.2294216 1.1939354 -0.20814657 -1.4564273 -2.378643 -2.571353][-3.2343225 -3.0115561 -3.051466 -3.2347589 -3.0281124 -2.4034777 -1.5270202 -0.65879059 -0.12973452 -0.034454346 -0.42063665 -1.1721375 -2.056541 -2.7583756 -2.6751966][-4.3405838 -4.2345066 -4.3013363 -4.3411169 -4.3457165 -4.2299123 -3.8209019 -3.1428294 -2.4804196 -2.1488552 -2.284065 -2.6605196 -3.0686123 -3.3247881 -3.0589273][-5.2536478 -5.3124142 -5.5096264 -5.8535829 -6.0252953 -5.9264507 -5.6716909 -5.2898488 -4.8127646 -4.2958074 -3.9601667 -4.0463943 -4.2293324 -4.3460846 -4.0159512][-6.264677 -6.32985 -6.3897934 -6.6077371 -6.8946953 -7.014801 -6.9515305 -6.6613979 -6.2535486 -5.7635574 -5.3234305 -5.2052603 -5.1193628 -5.0671325 -4.6495786][-6.3682318 -6.4111729 -6.4085822 -6.6746578 -6.9178553 -6.9599833 -6.95277 -6.8939619 -6.73553 -6.2879248 -5.8158956 -5.5649862 -5.4300327 -5.2745585 -4.8997793]]...]
INFO - root - 2017-12-16 07:40:53.398368: step 50610, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.286 sec/batch; 22h:22m:59s remains)
INFO - root - 2017-12-16 07:40:56.266617: step 50620, loss = 0.28, batch loss = 0.22 (27.3 examples/sec; 0.294 sec/batch; 22h:59m:04s remains)
INFO - root - 2017-12-16 07:40:59.091459: step 50630, loss = 0.24, batch loss = 0.18 (27.5 examples/sec; 0.291 sec/batch; 22h:45m:50s remains)
INFO - root - 2017-12-16 07:41:01.929755: step 50640, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.284 sec/batch; 22h:12m:53s remains)
INFO - root - 2017-12-16 07:41:04.733873: step 50650, loss = 0.38, batch loss = 0.32 (29.2 examples/sec; 0.274 sec/batch; 21h:26m:50s remains)
INFO - root - 2017-12-16 07:41:07.525509: step 50660, loss = 0.38, batch loss = 0.32 (28.5 examples/sec; 0.281 sec/batch; 21h:58m:13s remains)
INFO - root - 2017-12-16 07:41:10.414324: step 50670, loss = 0.30, batch loss = 0.24 (28.7 examples/sec; 0.279 sec/batch; 21h:49m:01s remains)
INFO - root - 2017-12-16 07:41:13.242861: step 50680, loss = 0.25, batch loss = 0.20 (28.2 examples/sec; 0.284 sec/batch; 22h:14m:05s remains)
INFO - root - 2017-12-16 07:41:16.102364: step 50690, loss = 0.23, batch loss = 0.17 (25.1 examples/sec; 0.319 sec/batch; 24h:56m:26s remains)
INFO - root - 2017-12-16 07:41:18.955974: step 50700, loss = 0.29, batch loss = 0.23 (27.9 examples/sec; 0.286 sec/batch; 22h:24m:28s remains)
2017-12-16 07:41:19.430275: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0230272 -2.573781 -3.0570977 -3.7377582 -4.6397114 -5.77061 -6.7407184 -7.3248167 -7.4781685 -6.7812743 -5.6372871 -4.6373076 -3.5635197 -2.8900011 -2.6761947][-1.9846404 -2.4261649 -2.7842793 -3.3081799 -3.8849206 -4.6004729 -5.4428759 -5.7573624 -5.5511703 -5.0705175 -4.4506807 -3.516643 -2.4607654 -1.9907672 -1.8957381][-2.4815173 -2.9039688 -3.2071857 -3.3138542 -3.1187222 -3.143549 -3.3743858 -3.6688218 -3.838479 -3.7160919 -3.3685646 -2.8209872 -1.9101167 -1.1247983 -0.68092704][-3.3130944 -3.6878023 -3.8018034 -3.5927954 -2.6822894 -1.8025372 -1.5509245 -1.6477556 -1.9154618 -1.9527607 -1.8316822 -1.6581378 -1.2815404 -0.73345327 -0.3956604][-4.0818887 -4.262301 -4.1498036 -3.323416 -2.0047088 -0.51953888 0.38315821 0.54079485 0.093700886 -0.59181666 -1.169172 -1.302593 -1.1850786 -1.0591671 -1.1939158][-5.1134286 -5.0922623 -4.5965924 -3.2072787 -1.0861435 1.0101728 2.2484121 2.6141448 1.9893322 0.7117672 -0.56547689 -1.2662163 -1.6845334 -1.7652414 -1.7354217][-5.0137305 -5.1706791 -4.5357127 -2.79674 -0.51701045 1.7349763 3.1988149 3.3971558 2.3911281 0.80176783 -0.65884042 -1.5362737 -2.0922084 -2.1862381 -2.0731916][-4.6091309 -4.5624237 -3.7820077 -2.1739295 -0.20761728 1.7684412 2.8742089 2.861145 1.8141637 0.22819614 -1.2465603 -2.0243995 -2.4053829 -2.4972334 -2.4937139][-3.6544995 -3.8598795 -3.285655 -1.9357612 -0.28476572 1.1157193 1.9409833 1.8714805 0.99271727 -0.38342905 -1.6580887 -2.4410524 -2.7096195 -2.7057161 -2.5724568][-2.7776935 -3.2995095 -3.2006731 -2.3211768 -1.0505936 0.15756798 0.74812508 0.6228199 -0.091954231 -1.1197355 -1.9603179 -2.5102382 -2.7438428 -2.6740541 -2.6005051][-2.6941121 -3.3444636 -3.4903417 -2.9635651 -2.0758588 -1.1671317 -0.59653497 -0.72379565 -1.2140086 -1.9010103 -2.5262194 -2.7895937 -2.8794622 -2.8048022 -2.8182702][-3.1640186 -3.6912062 -4.0111761 -3.7930212 -3.0999694 -2.3519268 -1.8628612 -1.7787917 -1.9087751 -2.3133924 -2.8245778 -2.9915204 -2.9634781 -2.8897195 -2.8854957][-3.6877756 -4.2816105 -4.5711861 -4.2625084 -3.576822 -2.8540606 -2.4069285 -2.3322806 -2.5707746 -2.83518 -2.9346218 -2.9222374 -2.8245254 -2.5360801 -2.4483562][-3.6469367 -4.0414958 -4.1741061 -3.9218307 -3.3941803 -2.8133483 -2.4207354 -2.4233902 -2.6105175 -2.8748026 -3.01602 -2.5894449 -2.1254747 -1.8001623 -1.7719822][-3.3914413 -3.6084163 -3.6177046 -3.190825 -2.6307626 -2.358063 -2.3446836 -2.3788965 -2.5971098 -2.6720176 -2.6102424 -2.0089457 -1.417702 -1.0059979 -0.95362115]]...]
INFO - root - 2017-12-16 07:41:22.267282: step 50710, loss = 0.20, batch loss = 0.14 (28.0 examples/sec; 0.285 sec/batch; 22h:20m:09s remains)
INFO - root - 2017-12-16 07:41:25.142036: step 50720, loss = 0.37, batch loss = 0.32 (27.4 examples/sec; 0.292 sec/batch; 22h:53m:38s remains)
INFO - root - 2017-12-16 07:41:27.941577: step 50730, loss = 0.23, batch loss = 0.17 (29.0 examples/sec; 0.276 sec/batch; 21h:37m:00s remains)
INFO - root - 2017-12-16 07:41:30.798077: step 50740, loss = 0.28, batch loss = 0.22 (29.5 examples/sec; 0.271 sec/batch; 21h:13m:33s remains)
INFO - root - 2017-12-16 07:41:33.647504: step 50750, loss = 0.31, batch loss = 0.25 (27.7 examples/sec; 0.289 sec/batch; 22h:37m:42s remains)
INFO - root - 2017-12-16 07:41:36.439951: step 50760, loss = 0.23, batch loss = 0.17 (29.1 examples/sec; 0.275 sec/batch; 21h:29m:36s remains)
INFO - root - 2017-12-16 07:41:39.308439: step 50770, loss = 0.25, batch loss = 0.20 (28.4 examples/sec; 0.282 sec/batch; 22h:03m:25s remains)
INFO - root - 2017-12-16 07:41:42.141845: step 50780, loss = 0.32, batch loss = 0.26 (27.2 examples/sec; 0.294 sec/batch; 22h:59m:13s remains)
INFO - root - 2017-12-16 07:41:44.953286: step 50790, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 22h:17m:23s remains)
INFO - root - 2017-12-16 07:41:47.803431: step 50800, loss = 0.26, batch loss = 0.20 (27.9 examples/sec; 0.287 sec/batch; 22h:25m:45s remains)
2017-12-16 07:41:48.278261: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4545321 -3.4391923 -3.468982 -3.4902337 -3.4836688 -3.4584908 -3.4078293 -3.4319825 -3.5490036 -3.7375367 -4.2483068 -4.7900362 -5.2348433 -5.4385319 -5.4143968][-3.8924291 -3.9643602 -3.9672532 -3.8277438 -3.6498668 -3.5464671 -3.4075665 -3.2936158 -3.3821361 -3.7239261 -4.4052963 -5.0749598 -5.6160951 -5.8720579 -5.8103056][-4.1152058 -4.222558 -4.1353335 -3.7962937 -3.3370724 -3.0210063 -2.687192 -2.5159998 -2.5465698 -3.2461994 -4.2822342 -5.1208863 -5.830821 -6.0786757 -6.0715203][-4.2967329 -4.3460469 -4.1284823 -3.3494854 -2.4830315 -1.8884513 -1.372344 -1.2622273 -1.5231354 -2.280916 -3.5735979 -4.8266191 -5.8409815 -6.2031608 -6.2345366][-4.347188 -4.0634403 -3.4835157 -2.41197 -1.0834041 -0.10285187 0.62143135 0.77951479 0.35907888 -0.76401663 -2.3102913 -3.728982 -4.9767532 -5.7774677 -5.9625969][-4.6046672 -3.9507875 -2.8506875 -1.2528272 0.51205683 1.77986 2.6875381 2.7116919 2.0490637 0.71442556 -1.0523548 -2.7620728 -4.2468967 -5.2843685 -5.7524505][-4.8201337 -3.8393269 -2.4112902 -0.49417472 1.5984716 3.1483412 4.2963009 4.2417459 3.4344764 1.8843622 -0.17419672 -2.2098706 -3.9221613 -5.0029349 -5.5749755][-4.9482355 -3.9422812 -2.4165094 -0.42048597 1.5848751 3.3373895 4.5054274 4.5074148 3.7346916 2.1122851 0.11884642 -1.9843631 -3.9060287 -5.0887065 -5.7015634][-5.451611 -4.4726958 -3.0653346 -1.2893901 0.37061453 2.0666261 3.2702703 3.2717543 2.6196866 1.1321721 -0.56755352 -2.5914729 -4.307559 -5.429039 -6.0608325][-6.1049862 -5.4665966 -4.5855803 -3.125391 -1.5646858 -0.11272049 0.9606123 1.1316576 0.68687296 -0.405509 -1.6797996 -3.220695 -4.5992079 -5.5675058 -6.1178222][-6.408411 -6.0640779 -5.4090834 -4.4443164 -3.3895829 -2.0472274 -0.92444706 -0.68420148 -1.0593493 -1.9579241 -2.8085284 -3.8952231 -4.7828164 -5.4102707 -5.8991747][-6.4655786 -6.2370677 -5.9095812 -5.060132 -4.268888 -3.3001199 -2.3697536 -2.0341053 -2.0916898 -2.5921581 -3.1183381 -3.9231195 -4.5851054 -4.978632 -5.255053][-5.8162174 -5.6798744 -5.5052047 -4.74255 -4.0560212 -3.4068003 -2.8154936 -2.6730347 -2.5694275 -2.9251764 -3.1898642 -3.5245891 -3.9199698 -4.283916 -4.5234127][-5.3396144 -4.9858017 -4.506166 -3.8888547 -3.3806221 -2.7101874 -2.3203702 -2.38396 -2.5320907 -2.7791183 -2.9436979 -3.2019391 -3.3443627 -3.6588221 -3.9435623][-4.9141016 -4.4099383 -3.7948673 -3.0450933 -2.4890919 -2.0980399 -1.8778741 -1.9929178 -2.2316716 -2.5211744 -2.6151257 -2.743206 -2.737412 -2.9208152 -3.1267533]]...]
INFO - root - 2017-12-16 07:41:51.087976: step 50810, loss = 0.35, batch loss = 0.30 (28.1 examples/sec; 0.285 sec/batch; 22h:16m:19s remains)
INFO - root - 2017-12-16 07:41:53.930668: step 50820, loss = 0.22, batch loss = 0.16 (28.5 examples/sec; 0.281 sec/batch; 21h:57m:17s remains)
INFO - root - 2017-12-16 07:41:56.746496: step 50830, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.281 sec/batch; 21h:57m:12s remains)
INFO - root - 2017-12-16 07:41:59.525607: step 50840, loss = 0.30, batch loss = 0.25 (29.1 examples/sec; 0.275 sec/batch; 21h:32m:28s remains)
INFO - root - 2017-12-16 07:42:02.379265: step 50850, loss = 0.25, batch loss = 0.19 (26.2 examples/sec; 0.305 sec/batch; 23h:52m:03s remains)
INFO - root - 2017-12-16 07:42:05.212816: step 50860, loss = 0.31, batch loss = 0.25 (28.9 examples/sec; 0.277 sec/batch; 21h:38m:22s remains)
INFO - root - 2017-12-16 07:42:08.046114: step 50870, loss = 0.29, batch loss = 0.23 (28.6 examples/sec; 0.279 sec/batch; 21h:51m:26s remains)
INFO - root - 2017-12-16 07:42:10.889929: step 50880, loss = 0.22, batch loss = 0.16 (29.9 examples/sec; 0.268 sec/batch; 20h:57m:17s remains)
INFO - root - 2017-12-16 07:42:13.745382: step 50890, loss = 0.23, batch loss = 0.17 (27.9 examples/sec; 0.287 sec/batch; 22h:24m:47s remains)
INFO - root - 2017-12-16 07:42:16.564440: step 50900, loss = 0.35, batch loss = 0.30 (27.8 examples/sec; 0.287 sec/batch; 22h:28m:15s remains)
2017-12-16 07:42:17.010430: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1890047 -2.4836268 -2.8117235 -3.2127905 -3.73205 -4.1485529 -4.6003432 -5.0728083 -5.5346947 -5.7565365 -5.5087814 -5.2063613 -4.9349442 -4.4941025 -3.8560808][-2.7765646 -3.1824346 -3.5776372 -3.8379955 -4.1453571 -4.6114674 -5.0237474 -5.2917137 -5.4804511 -5.5762539 -5.7893744 -5.7474427 -5.2645469 -4.6997533 -4.1704979][-3.7914448 -4.4347043 -4.85992 -4.9760652 -4.9122972 -4.7950392 -4.5818133 -4.6885452 -4.9246941 -5.0669289 -5.05068 -5.26761 -5.4832487 -5.1117368 -4.2820644][-4.2949414 -5.1171079 -5.6131268 -5.5149508 -5.1371427 -4.4229093 -3.7404661 -3.3230753 -3.0449052 -3.4055386 -4.1081295 -4.6509147 -4.88142 -4.9199343 -4.4085689][-4.9173346 -5.3523569 -5.4201927 -5.0376487 -3.9239056 -2.436621 -1.122925 -0.46404791 -0.43545127 -0.97144628 -2.1097331 -3.656435 -4.7511134 -4.6778822 -4.2268953][-5.1088014 -5.6007414 -5.1789627 -4.30075 -2.7783222 -0.64220524 1.2789578 2.3979325 2.4092283 1.3250661 -0.36638832 -2.3110344 -3.92994 -4.5007534 -4.1906614][-4.3262725 -4.6436992 -4.2004561 -2.8631306 -0.99540687 0.98258591 2.7887168 3.8867464 3.6465435 2.4484558 0.34750938 -1.7463112 -3.2629964 -4.1281385 -4.1638813][-3.706655 -3.4846776 -2.8845835 -1.5855749 0.51913834 2.6759863 4.0686893 4.3448458 3.6837044 2.1554623 0.14941502 -2.0218637 -3.5462403 -4.2810473 -4.4179854][-3.2720952 -3.1362829 -2.2329051 -0.95448589 0.57201767 2.3378849 3.4888906 3.2537928 2.1750369 0.614038 -1.1880367 -2.8385396 -4.2955923 -5.0793505 -4.9994283][-3.3556104 -3.1064117 -2.3856058 -1.0989316 0.26171255 1.1381826 1.6795197 1.3111143 0.13482618 -1.4578488 -2.9169912 -4.1283484 -4.9919677 -5.5063691 -5.4289236][-3.9958076 -3.5812311 -2.8051851 -1.7576911 -0.65115643 0.017889977 -0.17523766 -0.699378 -1.6549923 -2.8561583 -3.9750202 -4.868351 -5.4355211 -5.5684562 -5.2103539][-4.5454326 -4.0407166 -3.2085614 -2.3437014 -1.6996419 -1.3959055 -1.6282554 -2.3676097 -3.3342237 -4.1466417 -4.9390168 -5.4611592 -5.5312405 -5.1736221 -4.66554][-5.3725662 -4.654726 -3.7570848 -2.7398989 -2.1437557 -2.1993933 -2.6396217 -3.2183957 -4.0658231 -4.95298 -5.4328642 -5.5858412 -5.5140767 -4.9078221 -4.1273341][-5.5499015 -4.8946667 -3.999788 -2.9874837 -2.2071786 -1.9714837 -2.2768879 -2.9645553 -3.8171318 -4.6615505 -5.4152369 -5.4748068 -4.923595 -4.26612 -3.67651][-5.0158253 -4.3807778 -3.5382955 -2.5165629 -1.656368 -1.2172816 -1.3048928 -1.9683387 -3.0265346 -4.01931 -4.6355214 -4.8676014 -4.5943036 -3.6746569 -2.9848709]]...]
INFO - root - 2017-12-16 07:42:19.848683: step 50910, loss = 0.25, batch loss = 0.20 (28.9 examples/sec; 0.277 sec/batch; 21h:41m:14s remains)
INFO - root - 2017-12-16 07:42:22.655238: step 50920, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 21h:53m:07s remains)
INFO - root - 2017-12-16 07:42:25.493238: step 50930, loss = 0.30, batch loss = 0.24 (28.5 examples/sec; 0.281 sec/batch; 21h:57m:43s remains)
INFO - root - 2017-12-16 07:42:28.348950: step 50940, loss = 0.31, batch loss = 0.26 (28.1 examples/sec; 0.285 sec/batch; 22h:15m:42s remains)
INFO - root - 2017-12-16 07:42:31.194914: step 50950, loss = 0.26, batch loss = 0.20 (29.2 examples/sec; 0.274 sec/batch; 21h:23m:29s remains)
INFO - root - 2017-12-16 07:42:34.044364: step 50960, loss = 0.28, batch loss = 0.22 (29.3 examples/sec; 0.273 sec/batch; 21h:21m:06s remains)
INFO - root - 2017-12-16 07:42:36.958617: step 50970, loss = 0.33, batch loss = 0.28 (27.2 examples/sec; 0.294 sec/batch; 23h:00m:09s remains)
INFO - root - 2017-12-16 07:42:39.840853: step 50980, loss = 0.25, batch loss = 0.20 (29.4 examples/sec; 0.272 sec/batch; 21h:16m:41s remains)
INFO - root - 2017-12-16 07:42:42.640575: step 50990, loss = 0.36, batch loss = 0.31 (28.4 examples/sec; 0.281 sec/batch; 21h:59m:36s remains)
INFO - root - 2017-12-16 07:42:45.519563: step 51000, loss = 0.29, batch loss = 0.23 (29.0 examples/sec; 0.276 sec/batch; 21h:32m:45s remains)
2017-12-16 07:42:45.966252: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5223851 -5.162406 -5.7731824 -6.2221 -6.5858212 -6.7564831 -6.7827063 -6.8762879 -7.0423117 -7.0463543 -6.766408 -6.5627961 -6.0907116 -5.2991104 -4.4660907][-5.5433831 -6.136117 -6.5796633 -6.925128 -7.1948986 -7.5477457 -7.8897567 -8.050148 -8.1115074 -8.3348379 -8.4906807 -8.2037983 -7.5187263 -6.7665958 -5.8809452][-5.65442 -6.2820477 -6.6671362 -6.6115084 -6.5220962 -6.6401148 -6.8224435 -7.5196738 -8.2699461 -8.6168995 -8.7615681 -8.968174 -8.8606243 -8.2363548 -7.330719][-5.6172533 -5.7516255 -5.4994955 -5.1714554 -4.8878951 -4.5970216 -4.5300193 -4.9897823 -5.6548972 -6.9989581 -8.1532307 -8.5584393 -8.7452412 -8.824357 -8.3918877][-5.0973473 -4.9165473 -4.1086364 -3.0288858 -1.84045 -1.0247145 -0.79445982 -1.1947923 -2.0730917 -3.5275867 -5.1220074 -6.9829226 -8.3444433 -8.7865591 -8.7140408][-3.9422932 -3.4045506 -2.158011 -0.64926147 1.1498446 2.4584079 3.260747 3.3499365 2.394733 0.41334534 -1.9054279 -4.2477036 -6.4717569 -8.169817 -8.6418142][-3.7238665 -2.7202859 -1.0756614 0.88408566 3.1616364 4.9436493 6.0831423 6.1237068 5.2685709 3.605938 1.1797032 -1.8468962 -4.6098123 -6.5052166 -7.3828382][-4.3277154 -3.2364225 -1.7745862 0.35883474 3.0155249 5.10268 6.7795935 7.219553 6.5502758 4.5022917 1.9147949 -0.72625184 -3.2496963 -5.1762228 -6.0057898][-5.1477437 -4.6561 -3.4000945 -1.5690379 0.53084278 2.7313643 4.6688375 5.3773928 5.2812738 3.9599724 1.7301445 -1.0274656 -3.4599123 -4.9292088 -5.4619489][-5.6699171 -5.7555847 -5.3276157 -4.2691226 -2.6352136 -0.76554418 0.8610487 1.8330097 2.2056084 1.2502775 -0.20114994 -2.0846913 -4.0404706 -5.5059433 -6.1401696][-6.5813141 -6.9566154 -6.6665964 -6.17204 -5.3822918 -4.0328522 -2.4912577 -1.8350437 -1.5439854 -1.9376917 -2.6602812 -3.86231 -5.0563416 -5.928586 -6.4819837][-6.968648 -7.25393 -7.5323396 -7.3360538 -6.5585155 -5.905139 -5.2224436 -4.5507526 -4.04437 -4.2757783 -4.79709 -5.6290975 -6.3455472 -6.6020508 -6.703701][-5.9276867 -6.2938733 -6.6625338 -6.6391821 -6.6389685 -6.1498733 -5.4047704 -5.272233 -5.0361214 -4.9696465 -5.1582842 -5.7268763 -6.3264937 -6.7722406 -6.9730921][-4.6860671 -4.9501328 -5.2781291 -5.3783503 -5.4967117 -5.2604742 -5.0110178 -4.9463091 -4.822248 -4.7885003 -4.6352582 -4.8858566 -5.4323578 -5.6782322 -5.7593937][-3.8889608 -3.7495489 -3.6789432 -3.8526103 -4.0783858 -4.0726104 -4.0806351 -4.0679383 -3.913784 -3.9940181 -4.0706978 -3.9809222 -3.9738858 -4.1689734 -4.3826056]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 07:42:48.774398: step 51010, loss = 0.29, batch loss = 0.23 (28.9 examples/sec; 0.277 sec/batch; 21h:39m:14s remains)
INFO - root - 2017-12-16 07:42:51.611423: step 51020, loss = 0.27, batch loss = 0.22 (27.2 examples/sec; 0.294 sec/batch; 23h:01m:34s remains)
INFO - root - 2017-12-16 07:42:54.465466: step 51030, loss = 0.27, batch loss = 0.21 (29.0 examples/sec; 0.276 sec/batch; 21h:32m:59s remains)
INFO - root - 2017-12-16 07:42:57.294389: step 51040, loss = 0.25, batch loss = 0.19 (28.5 examples/sec; 0.280 sec/batch; 21h:55m:01s remains)
INFO - root - 2017-12-16 07:43:00.111265: step 51050, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 22h:14m:48s remains)
INFO - root - 2017-12-16 07:43:02.994625: step 51060, loss = 0.28, batch loss = 0.22 (28.3 examples/sec; 0.282 sec/batch; 22h:04m:16s remains)
INFO - root - 2017-12-16 07:43:05.906340: step 51070, loss = 0.22, batch loss = 0.16 (27.5 examples/sec; 0.291 sec/batch; 22h:44m:46s remains)
INFO - root - 2017-12-16 07:43:08.770722: step 51080, loss = 0.21, batch loss = 0.15 (26.9 examples/sec; 0.297 sec/batch; 23h:14m:37s remains)
INFO - root - 2017-12-16 07:43:11.567619: step 51090, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.283 sec/batch; 22h:06m:01s remains)
INFO - root - 2017-12-16 07:43:14.439458: step 51100, loss = 0.21, batch loss = 0.15 (28.7 examples/sec; 0.278 sec/batch; 21h:45m:24s remains)
2017-12-16 07:43:14.933319: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1988978 -4.1099806 -4.35972 -4.940248 -5.0641141 -4.9517369 -4.6666031 -4.6480985 -4.6146588 -4.5569468 -4.6406865 -4.4922371 -4.4849811 -4.25147 -4.2723479][-4.1969676 -3.9988348 -4.1438966 -4.4978356 -4.6892743 -5.0485859 -5.0024986 -4.8748646 -4.77191 -4.6615977 -4.6137958 -4.3556294 -4.3441525 -3.9722574 -3.6744523][-3.7932112 -3.4642935 -3.4270327 -3.5367005 -3.7603555 -3.9690385 -4.0699272 -4.57019 -4.7345114 -4.8592906 -4.892272 -4.5969477 -4.4764404 -4.0428729 -3.6645436][-2.7718287 -2.4353242 -2.4633121 -2.1682484 -2.0313225 -2.1654415 -2.2350271 -2.6322651 -3.228121 -4.1135397 -4.6248479 -4.7046146 -4.7866507 -4.3818645 -3.9113045][-1.8896809 -1.0657434 -0.48407006 -0.18113899 -0.072128296 0.2732501 0.31182957 -0.2753768 -0.909076 -2.1639271 -3.4733982 -4.1701531 -4.4304752 -4.3214421 -3.9763918][-1.6943693 -0.61018872 0.22088146 1.3138523 2.5223136 2.9195943 2.9238987 2.4831796 1.4626126 -0.11160851 -1.6219046 -2.9868605 -3.8085327 -3.9989126 -3.8106773][-2.1043253 -0.8722899 0.20674944 1.4616914 3.0329123 4.3340855 5.058363 4.3796749 3.4328852 1.8375974 -0.14101076 -1.7496262 -2.8323026 -3.275454 -3.7375379][-3.0533874 -2.0211663 -0.765749 0.82935667 2.5003395 3.9878225 5.0881233 4.7865715 3.8797951 2.3346052 0.5666008 -1.0514021 -2.3295615 -3.03953 -3.6869955][-4.2103891 -3.7572081 -2.7555509 -1.174674 0.68552732 2.4621716 3.7104683 3.6860046 3.0799327 1.6963682 0.06157589 -1.4158998 -2.5466301 -3.1469429 -3.5339003][-5.1495166 -5.2122064 -4.9194565 -3.538913 -1.7723081 -0.1966691 1.0951338 1.3298712 0.823102 -0.29371548 -1.5908961 -2.5891814 -3.2857323 -3.5081415 -3.7294104][-6.5346465 -6.1669159 -5.6023054 -5.1281061 -4.1994262 -2.8335462 -1.5303898 -1.2909899 -1.6855941 -2.4760303 -3.5273652 -4.1337647 -4.3840122 -4.30102 -4.0892649][-6.6284351 -6.5303268 -6.4199085 -5.75805 -5.0231938 -4.4520335 -3.8296671 -3.5133152 -3.5896344 -4.01827 -4.557219 -4.9484081 -5.0759745 -4.6502471 -4.1045918][-6.7710943 -6.1918187 -6.009119 -5.8993511 -5.5912495 -4.9755659 -4.4565587 -4.5414047 -4.8144569 -4.8920612 -5.0300813 -4.9507561 -4.8721266 -4.57717 -3.8095794][-6.6995006 -5.7676849 -5.29609 -5.01325 -4.9590869 -4.87707 -4.6879363 -4.6554103 -4.8072505 -4.9499264 -4.9692445 -4.7676988 -4.6029954 -4.1135182 -3.2724283][-6.532064 -5.4945517 -4.7859044 -4.4607306 -4.4011688 -4.4022741 -4.4170237 -4.3643336 -4.3907285 -4.4751263 -4.5775623 -4.3522758 -4.1665964 -4.0143976 -3.3275728]]...]
INFO - root - 2017-12-16 07:43:17.732844: step 51110, loss = 0.21, batch loss = 0.15 (28.0 examples/sec; 0.286 sec/batch; 22h:19m:07s remains)
INFO - root - 2017-12-16 07:43:20.599979: step 51120, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.280 sec/batch; 21h:52m:03s remains)
INFO - root - 2017-12-16 07:43:23.455553: step 51130, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 22h:18m:58s remains)
INFO - root - 2017-12-16 07:43:26.273882: step 51140, loss = 0.32, batch loss = 0.26 (27.2 examples/sec; 0.294 sec/batch; 23h:00m:06s remains)
INFO - root - 2017-12-16 07:43:29.077218: step 51150, loss = 0.20, batch loss = 0.14 (28.4 examples/sec; 0.282 sec/batch; 22h:02m:37s remains)
INFO - root - 2017-12-16 07:43:31.857402: step 51160, loss = 0.21, batch loss = 0.16 (28.2 examples/sec; 0.284 sec/batch; 22h:11m:10s remains)
INFO - root - 2017-12-16 07:43:34.679034: step 51170, loss = 0.27, batch loss = 0.22 (27.2 examples/sec; 0.294 sec/batch; 22h:56m:46s remains)
INFO - root - 2017-12-16 07:43:37.550665: step 51180, loss = 0.22, batch loss = 0.16 (28.0 examples/sec; 0.286 sec/batch; 22h:19m:39s remains)
INFO - root - 2017-12-16 07:43:40.426323: step 51190, loss = 0.24, batch loss = 0.19 (25.7 examples/sec; 0.311 sec/batch; 24h:18m:46s remains)
INFO - root - 2017-12-16 07:43:43.240437: step 51200, loss = 0.33, batch loss = 0.27 (29.1 examples/sec; 0.275 sec/batch; 21h:30m:50s remains)
2017-12-16 07:43:43.682117: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6369886 -2.7885935 -3.0991726 -3.1240401 -3.0265505 -2.8407252 -2.6585817 -2.4900775 -2.3514192 -2.2574234 -2.1454239 -2.0916834 -1.9440613 -1.8749187 -1.825532][-2.5599504 -2.8416734 -3.2510669 -3.3192129 -3.2368345 -3.0675364 -2.819859 -2.5841069 -2.4040842 -2.3517108 -2.3078341 -2.2295115 -2.0743113 -1.9694369 -1.8607152][-2.6063275 -2.9534624 -3.4307778 -3.4878118 -3.2824159 -3.023644 -2.711267 -2.4961047 -2.3695374 -2.4962163 -2.6281543 -2.70607 -2.6270936 -2.5318851 -2.4267516][-2.4736071 -2.76187 -3.1629052 -3.1961455 -2.8394823 -2.4477913 -2.1107557 -2.0117226 -2.117471 -2.4803481 -2.8698468 -3.1921573 -3.3175006 -3.1904616 -3.0232992][-2.0110173 -2.1653225 -2.2823739 -2.1758389 -1.6409738 -1.0015149 -0.59494996 -0.62225652 -1.0206006 -1.7816362 -2.6169817 -3.2614837 -3.5742393 -3.5343695 -3.43896][-1.3543475 -1.2860968 -1.2280452 -0.76556015 0.11108828 0.97010326 1.5045691 1.4372077 0.79783058 -0.33152008 -1.6056106 -2.6575339 -3.3338208 -3.4720817 -3.488543][-1.0333166 -0.71202683 -0.41538239 0.34281158 1.5699768 2.797194 3.5202742 3.3689885 2.6011343 1.2503815 -0.41818142 -1.8433776 -2.8037503 -3.1236968 -3.2477424][-1.1084323 -0.77931619 -0.34440184 0.63093185 2.0735168 3.4183455 4.261816 4.0845118 3.2229352 1.7149148 -0.03922987 -1.490032 -2.5156348 -2.8832581 -3.0652313][-1.6409822 -1.4773836 -1.1113088 -0.24986076 1.1218162 2.4783826 3.4291377 3.3216114 2.5140133 1.0489883 -0.64617753 -1.9188895 -2.7029548 -2.8528008 -2.8607407][-2.4006681 -2.4623306 -2.3291497 -1.6438744 -0.49581671 0.47999573 1.1951547 1.1172442 0.55276442 -0.4979043 -1.770098 -2.8342748 -3.3800409 -3.1703308 -2.8289073][-3.15313 -3.3850522 -3.3747966 -3.0738986 -2.4071665 -1.8397448 -1.3093655 -1.3747547 -1.8054709 -2.626265 -3.424964 -3.8187344 -3.8622963 -3.4798386 -2.9859304][-3.3135455 -3.7158761 -4.0381193 -4.1437373 -4.007946 -3.8065927 -3.5801063 -3.6990502 -4.0321794 -4.4024315 -4.6480031 -4.6465907 -4.2284379 -3.4151185 -2.6982322][-3.2984405 -3.7038164 -4.0797086 -4.5163221 -4.7801409 -4.9767661 -5.0935955 -5.1052432 -5.0289636 -4.8310003 -4.63705 -4.2551341 -3.6458807 -2.8235452 -2.2180378][-2.8528123 -3.0333309 -3.3442678 -3.8981 -4.4858766 -4.8539891 -5.0958815 -5.1159382 -5.0563078 -4.4300566 -3.5351052 -2.7893343 -2.1849129 -1.654443 -1.3029261][-2.1784103 -2.2596955 -2.4815457 -3.1198504 -3.7431259 -4.1518126 -4.4899111 -4.4506755 -4.285738 -3.6355317 -2.7220664 -1.8475988 -1.112874 -0.70427895 -0.57803488]]...]
INFO - root - 2017-12-16 07:43:46.469925: step 51210, loss = 0.24, batch loss = 0.18 (28.6 examples/sec; 0.280 sec/batch; 21h:50m:51s remains)
INFO - root - 2017-12-16 07:43:49.301749: step 51220, loss = 0.22, batch loss = 0.16 (27.5 examples/sec; 0.291 sec/batch; 22h:44m:57s remains)
INFO - root - 2017-12-16 07:43:52.151459: step 51230, loss = 0.23, batch loss = 0.17 (27.0 examples/sec; 0.296 sec/batch; 23h:07m:20s remains)
INFO - root - 2017-12-16 07:43:54.995971: step 51240, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 22h:15m:24s remains)
INFO - root - 2017-12-16 07:43:57.781755: step 51250, loss = 0.27, batch loss = 0.22 (29.2 examples/sec; 0.274 sec/batch; 21h:24m:49s remains)
INFO - root - 2017-12-16 07:44:00.651665: step 51260, loss = 0.26, batch loss = 0.20 (29.0 examples/sec; 0.276 sec/batch; 21h:32m:08s remains)
INFO - root - 2017-12-16 07:44:03.468528: step 51270, loss = 0.22, batch loss = 0.16 (29.7 examples/sec; 0.270 sec/batch; 21h:04m:22s remains)
INFO - root - 2017-12-16 07:44:06.320470: step 51280, loss = 0.30, batch loss = 0.24 (28.2 examples/sec; 0.284 sec/batch; 22h:10m:24s remains)
INFO - root - 2017-12-16 07:44:09.167858: step 51290, loss = 0.33, batch loss = 0.27 (27.9 examples/sec; 0.287 sec/batch; 22h:25m:39s remains)
INFO - root - 2017-12-16 07:44:11.959698: step 51300, loss = 0.20, batch loss = 0.14 (29.6 examples/sec; 0.270 sec/batch; 21h:06m:18s remains)
2017-12-16 07:44:12.396755: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3077266 -3.2142041 -3.0661626 -2.85525 -2.6449904 -2.3339093 -2.0953093 -2.2739818 -2.6139219 -2.9965239 -3.2579195 -3.480711 -3.459249 -3.5176315 -3.7052][-3.2585723 -3.2253954 -3.2324569 -2.9942374 -2.6998439 -2.4118481 -2.0982425 -2.0747764 -2.2390327 -2.7088215 -3.0177522 -3.2915356 -3.4851911 -3.7225902 -3.8273315][-3.1445363 -3.184906 -3.2095559 -3.0440655 -2.725194 -2.2705133 -1.7881801 -1.7416151 -2.0394838 -2.3309464 -2.4852529 -3.0156784 -3.643513 -3.9383984 -4.0899606][-2.6645658 -2.8844059 -3.1651785 -3.1774988 -2.9111001 -2.4326525 -1.9773216 -1.6708407 -1.5926545 -1.7448773 -2.0537519 -2.5665932 -3.1340773 -3.7302494 -4.2004871][-2.1294274 -2.5465317 -2.9714818 -3.006721 -2.6989095 -2.2175391 -1.6898553 -1.2352054 -0.96798468 -1.0550418 -1.2374167 -1.6505768 -2.177259 -2.7596097 -3.2187443][-1.4055309 -2.0198298 -2.4726529 -2.5529757 -2.2542267 -1.7361932 -1.1420574 -0.64420819 -0.30555058 -0.2710743 -0.47985196 -0.92412519 -1.4185019 -1.8773465 -2.278198][-0.78674293 -1.3639739 -1.8581197 -1.8500855 -1.5065877 -1.1230946 -0.66386962 -0.26635313 0.055777073 0.098731518 -0.072011948 -0.31910086 -0.68095732 -1.0990872 -1.3934577][-0.33684826 -0.8616457 -1.2360153 -1.1959534 -0.93120766 -0.64884543 -0.23829985 0.13519049 0.32739449 0.27503586 0.14548635 0.050394058 -0.17199039 -0.49438047 -0.73280692][-0.35910606 -0.67036533 -0.73989081 -0.55854464 -0.32424021 -0.098452568 0.22057581 0.5338974 0.69420385 0.6236639 0.51228094 0.39570379 0.1777215 -0.096268654 -0.33520126][-0.77323508 -0.75531626 -0.65464187 -0.30693722 0.1394577 0.4110837 0.72026205 0.78273916 0.6338501 0.54693556 0.40636063 0.29642057 -0.051997185 -0.41542482 -0.59367442][-1.7620609 -1.3839197 -0.95634747 -0.51807117 -0.075248241 0.2974143 0.51985931 0.54913521 0.31063843 0.054869175 -0.29807234 -0.44896984 -0.62681961 -0.97439027 -1.1919603][-2.8261871 -2.1966362 -1.5178304 -0.87956166 -0.44104266 -0.13775873 0.040498257 0.071403027 -0.24094439 -0.62394071 -0.981601 -1.1232541 -1.435111 -1.7734649 -1.9456933][-3.5215368 -2.8018234 -2.0318258 -1.3234446 -0.75437331 -0.29124689 -0.025028229 -0.12231445 -0.68941474 -1.1770706 -1.5782075 -1.814569 -2.228724 -2.4220452 -2.4348073][-4.4342966 -3.5347488 -2.4549212 -1.5631905 -1.0032511 -0.73662257 -0.66713786 -0.8742094 -1.4911819 -2.0846648 -2.5375667 -2.6460772 -2.7680573 -2.8621416 -2.8071642][-4.6881161 -3.8954 -2.9497347 -1.9141805 -1.2688744 -1.043757 -1.0642986 -1.4845822 -2.3002264 -2.9409251 -3.26763 -3.167665 -2.9468923 -2.8810778 -2.6910753]]...]
INFO - root - 2017-12-16 07:44:15.236698: step 51310, loss = 0.35, batch loss = 0.30 (27.8 examples/sec; 0.287 sec/batch; 22h:27m:14s remains)
INFO - root - 2017-12-16 07:44:18.072679: step 51320, loss = 0.42, batch loss = 0.36 (29.0 examples/sec; 0.276 sec/batch; 21h:34m:05s remains)
INFO - root - 2017-12-16 07:44:20.892742: step 51330, loss = 0.28, batch loss = 0.22 (29.8 examples/sec; 0.269 sec/batch; 20h:59m:37s remains)
INFO - root - 2017-12-16 07:44:23.699671: step 51340, loss = 0.21, batch loss = 0.16 (29.1 examples/sec; 0.275 sec/batch; 21h:27m:32s remains)
INFO - root - 2017-12-16 07:44:26.527773: step 51350, loss = 0.21, batch loss = 0.15 (29.6 examples/sec; 0.270 sec/batch; 21h:06m:03s remains)
INFO - root - 2017-12-16 07:44:29.335719: step 51360, loss = 0.20, batch loss = 0.14 (28.4 examples/sec; 0.282 sec/batch; 22h:02m:01s remains)
INFO - root - 2017-12-16 07:44:32.126649: step 51370, loss = 0.27, batch loss = 0.21 (27.8 examples/sec; 0.288 sec/batch; 22h:27m:22s remains)
INFO - root - 2017-12-16 07:44:34.957961: step 51380, loss = 0.26, batch loss = 0.20 (29.4 examples/sec; 0.272 sec/batch; 21h:13m:08s remains)
INFO - root - 2017-12-16 07:44:37.782034: step 51390, loss = 0.26, batch loss = 0.20 (28.2 examples/sec; 0.284 sec/batch; 22h:10m:16s remains)
INFO - root - 2017-12-16 07:44:40.626924: step 51400, loss = 0.28, batch loss = 0.22 (28.7 examples/sec; 0.279 sec/batch; 21h:44m:55s remains)
2017-12-16 07:44:41.091717: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5173087 -2.9962249 -3.4434891 -3.5964775 -3.6432714 -3.8040075 -4.163866 -4.345572 -4.4713974 -4.5852852 -4.7500463 -4.8177953 -4.6127677 -4.1551881 -3.6615076][-2.219034 -2.4820328 -2.8262715 -2.9851344 -3.0437784 -3.2044542 -3.5699081 -3.784914 -4.0081644 -4.1061263 -4.3116126 -4.4033017 -4.2194486 -3.8103247 -3.3183992][-2.0607755 -2.3334117 -2.5273526 -2.7420242 -2.8688283 -2.9186151 -3.0493662 -3.1233149 -3.3020051 -3.3090544 -3.4713633 -3.6536937 -3.515327 -3.1808791 -2.8068943][-1.7686729 -2.0069342 -2.1717689 -2.2641883 -2.2143841 -2.2438908 -2.2116892 -1.946821 -1.683701 -1.5644147 -1.7091234 -1.9028635 -2.0644014 -1.9828906 -1.6530941][-2.0911748 -2.2157071 -2.1914673 -2.1057918 -1.7203894 -1.3450465 -0.9813025 -0.57855058 -0.40406132 -0.25485563 -0.3736186 -0.86381459 -1.1432974 -1.234884 -1.1938074][-2.4828525 -2.666852 -2.6075232 -2.398067 -1.8237913 -1.0459547 -0.14692354 0.64655638 0.97650671 0.90474606 0.42883253 -0.17978716 -0.68795133 -1.0575118 -0.99308276][-2.8978019 -3.2014658 -3.1324666 -2.664484 -1.7222359 -0.76156688 0.23300838 1.1396804 1.6106172 1.5510421 1.042737 0.12852097 -0.66746068 -1.1300669 -1.333884][-3.3304467 -3.72669 -3.7983098 -3.3265131 -2.0821002 -0.64260268 0.51779461 1.3849025 1.8952928 1.7884083 1.0998678 -0.024078369 -1.0842214 -1.7587559 -2.0591125][-3.6755495 -4.1463685 -4.1639991 -3.6065869 -2.471736 -0.90515018 0.61989546 1.5705304 1.8269906 1.5305076 0.85930061 -0.33032179 -1.5536149 -2.2834158 -2.6300259][-3.7473376 -4.4649277 -4.7326436 -4.2634873 -3.1147118 -1.4589617 0.04744339 1.114912 1.5778093 1.1454968 0.16284847 -1.1010494 -2.2728822 -2.9940119 -3.3244402][-4.023643 -4.69602 -4.9060731 -4.5191617 -3.48554 -1.8696711 -0.40339279 0.40764809 0.5629015 0.22190905 -0.65630054 -2.0467293 -3.207859 -3.7227609 -3.8480675][-4.1486993 -4.8934131 -5.0163612 -4.4862952 -3.52372 -2.08724 -0.7692883 -0.16161013 -0.16076994 -0.65348268 -1.5580058 -2.6832032 -3.700417 -4.1538754 -4.1905003][-4.3654981 -5.0862327 -5.3222227 -4.7699308 -3.8026686 -2.3931713 -1.074069 -0.51820517 -0.69350457 -1.3282082 -2.1142883 -3.0602064 -3.9027267 -4.281673 -4.2701154][-4.3017693 -4.9293661 -5.0825353 -4.5746622 -3.6638057 -2.2268119 -1.0012434 -0.42959547 -0.5729003 -1.244621 -2.1221046 -2.9942307 -3.6993036 -4.0260305 -4.0105681][-4.3301477 -4.8838391 -4.9875922 -4.2260556 -3.105319 -1.8063624 -0.66803288 -0.057804108 -0.25440454 -1.0004125 -1.9119973 -2.7933042 -3.5337906 -3.8920317 -3.8713181]]...]
INFO - root - 2017-12-16 07:44:43.911426: step 51410, loss = 0.29, batch loss = 0.23 (25.3 examples/sec; 0.317 sec/batch; 24h:43m:48s remains)
INFO - root - 2017-12-16 07:44:46.760334: step 51420, loss = 0.25, batch loss = 0.20 (26.3 examples/sec; 0.304 sec/batch; 23h:42m:20s remains)
INFO - root - 2017-12-16 07:44:49.553168: step 51430, loss = 0.30, batch loss = 0.24 (29.6 examples/sec; 0.271 sec/batch; 21h:07m:28s remains)
INFO - root - 2017-12-16 07:44:52.385544: step 51440, loss = 0.35, batch loss = 0.30 (26.7 examples/sec; 0.300 sec/batch; 23h:23m:37s remains)
INFO - root - 2017-12-16 07:44:55.177962: step 51450, loss = 0.24, batch loss = 0.18 (27.2 examples/sec; 0.294 sec/batch; 22h:56m:31s remains)
INFO - root - 2017-12-16 07:44:58.027487: step 51460, loss = 0.36, batch loss = 0.30 (29.3 examples/sec; 0.273 sec/batch; 21h:17m:43s remains)
INFO - root - 2017-12-16 07:45:00.926451: step 51470, loss = 0.25, batch loss = 0.19 (26.8 examples/sec; 0.298 sec/batch; 23h:17m:52s remains)
INFO - root - 2017-12-16 07:45:03.732785: step 51480, loss = 0.31, batch loss = 0.25 (28.5 examples/sec; 0.281 sec/batch; 21h:54m:22s remains)
INFO - root - 2017-12-16 07:45:06.591206: step 51490, loss = 0.29, batch loss = 0.23 (27.9 examples/sec; 0.287 sec/batch; 22h:22m:57s remains)
INFO - root - 2017-12-16 07:45:09.416394: step 51500, loss = 0.25, batch loss = 0.19 (27.6 examples/sec; 0.290 sec/batch; 22h:38m:08s remains)
2017-12-16 07:45:09.880937: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0813551 -5.5652165 -5.7085652 -5.736414 -5.6392756 -5.542829 -5.4697676 -5.6181254 -5.7907996 -5.989078 -6.0868325 -5.9882512 -5.7134767 -5.0891519 -4.3937087][-5.7039165 -6.1905265 -6.2144656 -5.9972506 -5.5942936 -5.5025277 -5.5418415 -5.7294135 -6.0253248 -6.56522 -7.0378342 -7.0632339 -6.8536921 -6.2746572 -5.5525789][-6.1663179 -6.539341 -6.3035493 -5.7192378 -5.0292997 -4.69563 -4.526772 -4.9518042 -5.6187882 -6.4534416 -7.24337 -7.6446095 -7.7857 -7.3047228 -6.568325][-6.07744 -6.2815495 -5.7287774 -4.5633869 -3.4012051 -2.7246623 -2.5258818 -3.0039651 -3.8919952 -5.2966986 -6.6746869 -7.4572282 -7.8732376 -7.77175 -7.2958789][-5.5514536 -5.2888389 -4.1838245 -2.5497322 -0.87495852 0.21173716 0.553493 0.0705595 -1.0766387 -2.7964697 -4.6335449 -6.0834284 -7.1206903 -7.5036335 -7.3549595][-4.8304033 -4.3822803 -2.9927616 -0.77130008 1.5084071 3.0548716 3.7353697 3.3577528 2.119 0.058486938 -2.1906791 -4.0172896 -5.6748319 -6.7303581 -7.0183039][-4.4855256 -3.7682648 -2.173136 0.16108942 2.588294 4.3891306 5.4980469 5.2973967 4.1441545 2.1179533 -0.10951853 -2.1832058 -4.12838 -5.5534358 -6.181057][-4.3972626 -3.7431452 -2.3764234 -0.025731564 2.6559405 4.5624876 5.7264042 5.67519 4.6183596 2.5602851 0.40364027 -1.5740275 -3.5311813 -5.0041065 -5.5732903][-4.6604948 -4.213789 -3.1805 -1.2174733 1.08813 3.075068 4.655426 4.90442 4.1765642 2.3815517 0.40280867 -1.5958972 -3.5332296 -4.9496088 -5.56757][-5.4392796 -5.3656983 -4.6763873 -3.1828866 -1.2972023 0.43906021 1.9031525 2.5405817 2.3095593 0.96362209 -0.56952548 -2.271807 -3.9740014 -5.3578744 -5.961937][-6.5004578 -6.6355243 -6.0615754 -5.1089678 -3.8612878 -2.4439816 -0.87204671 -0.086966515 -0.14462948 -1.099884 -2.2549002 -3.4758139 -4.7171702 -5.7923708 -6.2723713][-7.2030897 -7.3632421 -7.1050224 -6.3986473 -5.350656 -4.4308934 -3.2557683 -2.4126678 -2.1279929 -2.8114061 -3.7464116 -4.8267832 -5.7819705 -6.4325523 -6.6640997][-7.4573135 -7.6254215 -7.4193749 -7.0042152 -6.3703117 -5.5991979 -4.758883 -4.034018 -3.6469989 -3.9450495 -4.5279837 -5.3169336 -5.98882 -6.5907755 -6.79953][-7.1274681 -7.2608557 -7.13315 -6.7428474 -6.2815676 -5.7731423 -5.2462668 -4.7337828 -4.4108062 -4.3623796 -4.5753956 -5.0843534 -5.45763 -5.7371645 -5.8209443][-5.952879 -5.78063 -5.4769473 -5.2804232 -5.0764022 -4.7619514 -4.5149488 -4.243289 -4.1474886 -4.2113218 -4.3311586 -4.464808 -4.5459948 -4.6714396 -4.7068939]]...]
INFO - root - 2017-12-16 07:45:12.702478: step 51510, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 21h:59m:44s remains)
INFO - root - 2017-12-16 07:45:15.587643: step 51520, loss = 0.26, batch loss = 0.20 (27.1 examples/sec; 0.295 sec/batch; 23h:00m:56s remains)
INFO - root - 2017-12-16 07:45:18.387563: step 51530, loss = 0.26, batch loss = 0.20 (28.2 examples/sec; 0.284 sec/batch; 22h:08m:53s remains)
INFO - root - 2017-12-16 07:45:21.249796: step 51540, loss = 0.23, batch loss = 0.17 (29.0 examples/sec; 0.276 sec/batch; 21h:32m:20s remains)
INFO - root - 2017-12-16 07:45:24.147894: step 51550, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.281 sec/batch; 21h:57m:19s remains)
INFO - root - 2017-12-16 07:45:26.982180: step 51560, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.285 sec/batch; 22h:15m:27s remains)
INFO - root - 2017-12-16 07:45:29.833412: step 51570, loss = 0.27, batch loss = 0.21 (27.3 examples/sec; 0.293 sec/batch; 22h:53m:31s remains)
INFO - root - 2017-12-16 07:45:32.650144: step 51580, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.281 sec/batch; 21h:55m:24s remains)
INFO - root - 2017-12-16 07:45:35.546362: step 51590, loss = 0.28, batch loss = 0.22 (28.0 examples/sec; 0.286 sec/batch; 22h:16m:55s remains)
INFO - root - 2017-12-16 07:45:38.411749: step 51600, loss = 0.29, batch loss = 0.23 (27.2 examples/sec; 0.295 sec/batch; 22h:58m:53s remains)
2017-12-16 07:45:38.880817: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5373125 -2.5233688 -2.5991154 -2.7522237 -2.7249455 -2.6210492 -2.5691175 -2.5063975 -2.4046271 -2.2979026 -2.0596716 -1.7025037 -1.384439 -1.3524573 -1.4835618][-1.7910075 -1.9719167 -2.141577 -2.2981639 -2.2815437 -2.2535291 -2.1872675 -2.1343665 -2.2351224 -2.1313846 -1.9250979 -1.785892 -1.6731598 -1.3798106 -1.0524776][-1.7103338 -1.8612409 -1.9864488 -2.0359206 -1.8911076 -1.6430435 -1.4441726 -1.4841554 -1.7429266 -2.0064206 -2.2657804 -2.3102405 -2.226187 -1.9410844 -1.4519949][-1.6965106 -2.0806739 -2.3290987 -2.1635194 -1.8219314 -1.169548 -0.625149 -0.5538094 -0.83065128 -1.4644587 -2.0725269 -2.7177629 -2.951879 -2.4960566 -1.6863809][-2.071949 -2.30359 -2.2627301 -1.9996448 -1.3301053 -0.36047316 0.29887676 0.39248323 -0.080774784 -1.1303411 -2.2485311 -2.9097066 -3.200532 -3.1272259 -2.4565954][-2.5390468 -2.6681273 -2.3263178 -1.3949509 -0.14504957 1.0657411 1.9720979 1.9643669 1.039288 -0.48718476 -2.188863 -3.4824786 -4.1349931 -3.9527593 -3.2702684][-3.2742658 -2.9166024 -2.1888227 -0.92057681 0.69213581 2.3419042 3.3492885 3.1840358 2.115531 0.29926634 -1.6420655 -3.330617 -4.3889337 -4.7278724 -4.238975][-3.4031553 -2.9378061 -2.1143689 -0.57292771 1.2421308 2.9176593 3.8984318 3.6931286 2.510191 0.5351882 -1.6092508 -3.4242916 -4.5823188 -5.1099558 -4.8534045][-3.6328232 -2.8210783 -1.8006654 -0.38032675 1.2753816 2.7058582 3.4711213 3.2334228 1.9909558 -0.036505222 -2.192965 -3.9637446 -5.1892262 -5.8321118 -5.5816183][-3.8436835 -3.3587322 -2.4188623 -1.1651888 0.23693657 1.2998152 1.8492479 1.5908456 0.50396872 -1.1387115 -3.08153 -4.7962542 -5.9528694 -6.4736013 -6.2126155][-4.0265875 -3.6427827 -3.0067554 -2.0992737 -1.0458846 -0.42491817 -0.041331768 -0.26455688 -1.1785266 -2.5599389 -3.994524 -5.2043724 -6.0821042 -6.4671459 -6.1050653][-3.409976 -3.3564241 -3.2115078 -2.7103076 -1.9238157 -1.3346591 -1.1180778 -1.3156207 -1.8140142 -2.7699478 -3.9330666 -4.9250307 -5.5213833 -5.8844457 -5.7160025][-3.0449638 -3.2050586 -3.2166891 -3.0888557 -2.702949 -2.1655617 -1.7183909 -1.5976052 -2.0482771 -2.6322694 -3.2109418 -3.9383621 -4.5707459 -5.0775161 -5.0774794][-3.0111036 -3.4722579 -3.6200035 -3.5274169 -3.1259568 -2.6531138 -2.1894019 -1.7891023 -1.761507 -1.9876258 -2.4003656 -2.8503306 -3.2289219 -3.7140718 -3.9725471][-2.7397094 -3.3142133 -3.6447258 -3.5390358 -2.9934688 -2.1993532 -1.5957637 -1.1183522 -0.90621877 -0.95022368 -1.2206159 -1.649138 -2.0117157 -2.5137148 -2.8384054]]...]
INFO - root - 2017-12-16 07:45:41.735641: step 51610, loss = 0.33, batch loss = 0.27 (28.8 examples/sec; 0.277 sec/batch; 21h:38m:31s remains)
INFO - root - 2017-12-16 07:45:44.582334: step 51620, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.279 sec/batch; 21h:44m:18s remains)
INFO - root - 2017-12-16 07:45:47.390846: step 51630, loss = 0.19, batch loss = 0.13 (28.1 examples/sec; 0.285 sec/batch; 22h:13m:27s remains)
INFO - root - 2017-12-16 07:45:50.224931: step 51640, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 22h:19m:16s remains)
INFO - root - 2017-12-16 07:45:53.049461: step 51650, loss = 0.22, batch loss = 0.16 (28.1 examples/sec; 0.284 sec/batch; 22h:10m:23s remains)
INFO - root - 2017-12-16 07:45:55.910093: step 51660, loss = 0.36, batch loss = 0.30 (26.5 examples/sec; 0.301 sec/batch; 23h:31m:03s remains)
INFO - root - 2017-12-16 07:45:58.812240: step 51670, loss = 0.24, batch loss = 0.18 (29.5 examples/sec; 0.271 sec/batch; 21h:10m:12s remains)
INFO - root - 2017-12-16 07:46:01.666676: step 51680, loss = 0.36, batch loss = 0.30 (28.5 examples/sec; 0.281 sec/batch; 21h:54m:46s remains)
INFO - root - 2017-12-16 07:46:04.574880: step 51690, loss = 0.27, batch loss = 0.21 (28.4 examples/sec; 0.282 sec/batch; 21h:59m:21s remains)
INFO - root - 2017-12-16 07:46:07.461491: step 51700, loss = 0.31, batch loss = 0.26 (26.9 examples/sec; 0.297 sec/batch; 23h:10m:43s remains)
2017-12-16 07:46:08.022163: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8245859 -2.8807492 -3.2990928 -3.5098393 -3.6918371 -3.7255557 -3.9344659 -4.11924 -4.1299677 -4.0025277 -3.715095 -3.6114743 -3.5367036 -3.6980107 -3.9348979][-3.1665745 -2.9803586 -3.2886724 -3.5950143 -3.5191896 -3.653718 -4.0055695 -4.0848961 -4.0561457 -4.2059569 -4.1956267 -4.1055579 -4.0786028 -4.4693437 -4.9190383][-3.3730369 -3.2161815 -3.3108611 -3.5105066 -3.3878446 -3.2355828 -3.3818324 -3.7826846 -3.8097806 -3.8657618 -3.9636462 -4.2951541 -4.4834328 -4.911459 -5.3706675][-3.6037118 -3.3587275 -3.374337 -3.2529266 -2.8808589 -2.4310284 -2.2330706 -2.250968 -2.3889923 -2.8530736 -3.2595353 -3.6679087 -4.0484014 -4.7278757 -5.2130075][-3.5785248 -3.3819952 -3.1769679 -2.8262177 -2.2567046 -1.6892052 -1.1846926 -0.79107976 -0.67749906 -0.98868918 -1.5269167 -2.3006663 -3.0463934 -3.7720873 -4.5174031][-3.6806004 -3.46904 -3.02283 -2.335676 -1.3752611 -0.45531607 0.29170418 0.79784346 0.96047592 0.68905449 0.11778641 -0.57088995 -1.4531751 -2.5723736 -3.5070593][-3.5672498 -3.3859186 -2.9720891 -1.9339874 -0.53829145 0.55538082 1.5712819 2.2755284 2.62891 2.5035973 1.9509773 1.2645469 0.39643574 -0.76141286 -2.048945][-3.0814698 -2.9985676 -2.6215639 -1.5941141 -0.17856073 0.94766521 2.0086737 2.7795568 3.2274795 3.3770947 3.152545 2.6299138 1.8330488 0.70108461 -0.79996657][-2.7855072 -2.6237283 -2.2593787 -1.4511175 -0.25357151 0.72221375 1.5196257 2.2709961 2.935864 3.2106786 3.3016944 3.0652957 2.3356466 1.2212272 -0.256907][-2.7830482 -2.6391435 -2.3121147 -1.7782497 -0.8690598 -0.12385845 0.47595787 0.93146658 1.4055839 1.7627559 2.0386534 2.056551 1.5463953 0.47032404 -0.84429336][-2.6224868 -2.5986261 -2.4652908 -2.146327 -1.613827 -1.2321203 -0.86399245 -0.56053138 -0.23779631 0.06855154 0.44003105 0.560421 0.25982475 -0.54399705 -1.6347864][-2.3404295 -2.3964648 -2.4119282 -2.59659 -2.47608 -2.3150837 -2.2129517 -2.1221259 -1.9137118 -1.8154564 -1.5555773 -1.1953664 -1.3491635 -1.9521399 -2.6073194][-2.085588 -2.168237 -2.2624249 -2.426918 -2.5304618 -2.6908197 -2.7788057 -2.8636284 -2.8611202 -3.0474963 -3.0661767 -2.7247508 -2.6757913 -3.0599022 -3.3540409][-2.2748232 -2.2091362 -2.189033 -2.3236239 -2.4220483 -2.5952559 -2.9151173 -3.225091 -3.427381 -3.8414941 -4.080153 -3.8516784 -3.6467736 -3.5948193 -3.7394059][-2.4286852 -2.2610133 -2.0551336 -2.0851374 -2.2191117 -2.4515243 -2.8252141 -3.2921705 -3.7425704 -4.3375354 -4.6289 -4.401597 -4.1598072 -3.8749547 -3.6529043]]...]
INFO - root - 2017-12-16 07:46:10.914679: step 51710, loss = 0.22, batch loss = 0.17 (27.6 examples/sec; 0.290 sec/batch; 22h:35m:39s remains)
INFO - root - 2017-12-16 07:46:13.841550: step 51720, loss = 0.43, batch loss = 0.37 (28.4 examples/sec; 0.282 sec/batch; 21h:58m:02s remains)
INFO - root - 2017-12-16 07:46:16.727929: step 51730, loss = 0.36, batch loss = 0.30 (28.1 examples/sec; 0.285 sec/batch; 22h:12m:55s remains)
INFO - root - 2017-12-16 07:46:19.604907: step 51740, loss = 0.30, batch loss = 0.24 (27.1 examples/sec; 0.296 sec/batch; 23h:02m:52s remains)
INFO - root - 2017-12-16 07:46:22.447754: step 51750, loss = 0.31, batch loss = 0.26 (26.8 examples/sec; 0.299 sec/batch; 23h:19m:14s remains)
INFO - root - 2017-12-16 07:46:25.398682: step 51760, loss = 0.25, batch loss = 0.20 (27.8 examples/sec; 0.288 sec/batch; 22h:28m:18s remains)
INFO - root - 2017-12-16 07:46:28.277495: step 51770, loss = 0.20, batch loss = 0.15 (26.5 examples/sec; 0.302 sec/batch; 23h:33m:36s remains)
INFO - root - 2017-12-16 07:46:31.198760: step 51780, loss = 0.27, batch loss = 0.21 (27.4 examples/sec; 0.292 sec/batch; 22h:46m:42s remains)
INFO - root - 2017-12-16 07:46:34.052446: step 51790, loss = 0.24, batch loss = 0.19 (28.4 examples/sec; 0.282 sec/batch; 21h:58m:22s remains)
INFO - root - 2017-12-16 07:46:36.962404: step 51800, loss = 0.30, batch loss = 0.24 (27.4 examples/sec; 0.292 sec/batch; 22h:47m:15s remains)
2017-12-16 07:46:37.492258: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.97260094 -1.325767 -1.6912358 -1.9982808 -2.3458171 -2.3772991 -2.4890451 -2.4081855 -2.2171297 -1.7375355 -1.2915275 -1.1190286 -1.2400069 -1.591151 -1.7523987][-1.9306056 -2.2611017 -2.5692892 -2.8915868 -3.1615124 -3.032599 -2.7903092 -2.5973043 -2.6207652 -2.3017824 -1.9923313 -1.9475975 -2.0075483 -2.3840032 -2.6640747][-3.2678187 -3.5297987 -3.7576199 -3.9606633 -3.9412992 -3.6760581 -3.3726439 -3.0637062 -2.8724093 -2.7044828 -2.8705952 -2.942008 -3.027684 -3.4577513 -3.7135851][-4.5616555 -4.7107611 -4.6685004 -4.4667792 -4.030911 -3.4317603 -2.8890224 -2.5110903 -2.5667839 -2.6860583 -3.1246409 -3.5019052 -3.7342305 -4.1654587 -4.3027954][-5.3707242 -5.5119548 -5.2724967 -4.6304679 -3.7321744 -2.6443329 -1.7213511 -1.3129563 -1.5092409 -1.9685736 -2.7627006 -3.5022528 -4.0535345 -4.4860826 -4.5509868][-5.5971351 -5.6234803 -5.32152 -4.462254 -3.1882412 -1.7211132 -0.53930449 0.044241905 -0.19689131 -0.85245085 -1.8567762 -2.9238839 -3.764179 -4.3931403 -4.7312427][-5.0348258 -5.0228925 -4.5063491 -3.4584146 -2.0799534 -0.55098224 0.69301081 1.1998463 0.86079168 -0.15441322 -1.4055021 -2.4882383 -3.3994522 -4.3862472 -4.7808042][-3.7442348 -3.6406829 -3.0377927 -1.9323416 -0.55984521 0.81101942 1.7913232 2.1930923 1.8488097 0.75588989 -0.70790339 -1.9932582 -3.0767539 -4.0049171 -4.527679][-2.7922029 -2.4925413 -1.8931837 -0.92510366 0.33709002 1.5098863 2.2381415 2.4413533 1.9905748 1.0407124 -0.30282164 -1.7011857 -2.9790854 -4.08998 -4.5725112][-2.4194434 -2.1639013 -1.5896759 -0.78805184 0.24909687 1.0530114 1.5795436 1.6974392 1.254878 0.5022049 -0.62331581 -2.0871921 -3.5675435 -4.7526894 -5.3924751][-2.6257353 -2.4407053 -1.9317875 -1.2656114 -0.5180192 0.03643322 0.50999022 0.58120012 0.24156332 -0.5184629 -1.4820082 -2.591543 -3.9906559 -5.3454037 -6.0240526][-2.8983626 -2.783536 -2.6422734 -1.9731145 -1.2425408 -0.81180763 -0.49735355 -0.39632797 -0.53369522 -1.2122767 -2.108429 -2.9894009 -4.0592117 -5.4882646 -6.1258535][-3.7585702 -3.6879582 -3.2642746 -2.8576713 -2.1992376 -1.5885212 -1.1137559 -0.84479809 -0.95084691 -1.2274382 -1.9114759 -2.8904347 -3.882251 -5.0614557 -6.0046506][-4.1819272 -4.480895 -4.3178854 -3.7912049 -3.0554214 -2.3831763 -1.8676679 -1.4435563 -1.264164 -1.4362941 -1.8778331 -2.6293073 -3.5721946 -4.5946822 -5.4783597][-4.6202431 -4.9927869 -5.1026611 -4.6778903 -3.8907781 -3.1457474 -2.3296013 -1.7536843 -1.5937805 -1.6621633 -1.8431532 -2.4615693 -3.414638 -4.5617151 -5.3833504]]...]
INFO - root - 2017-12-16 07:46:40.389516: step 51810, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.285 sec/batch; 22h:11m:18s remains)
INFO - root - 2017-12-16 07:46:43.319554: step 51820, loss = 0.40, batch loss = 0.34 (27.1 examples/sec; 0.295 sec/batch; 23h:02m:12s remains)
INFO - root - 2017-12-16 07:46:46.196327: step 51830, loss = 0.35, batch loss = 0.29 (27.8 examples/sec; 0.288 sec/batch; 22h:25m:05s remains)
INFO - root - 2017-12-16 07:46:49.052652: step 51840, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.281 sec/batch; 21h:56m:39s remains)
INFO - root - 2017-12-16 07:46:51.950937: step 51850, loss = 0.45, batch loss = 0.39 (28.0 examples/sec; 0.286 sec/batch; 22h:17m:23s remains)
INFO - root - 2017-12-16 07:46:54.876803: step 51860, loss = 0.23, batch loss = 0.18 (27.6 examples/sec; 0.290 sec/batch; 22h:35m:32s remains)
INFO - root - 2017-12-16 07:46:57.774956: step 51870, loss = 0.25, batch loss = 0.20 (28.0 examples/sec; 0.285 sec/batch; 22h:13m:59s remains)
INFO - root - 2017-12-16 07:47:00.678923: step 51880, loss = 0.27, batch loss = 0.21 (26.8 examples/sec; 0.299 sec/batch; 23h:16m:05s remains)
INFO - root - 2017-12-16 07:47:03.533026: step 51890, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 22h:15m:19s remains)
INFO - root - 2017-12-16 07:47:06.404626: step 51900, loss = 0.27, batch loss = 0.21 (28.1 examples/sec; 0.285 sec/batch; 22h:11m:35s remains)
2017-12-16 07:47:06.908561: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.590302 -3.3870225 -3.0596273 -2.917645 -2.7649832 -2.6926613 -2.8815155 -3.0082264 -3.2231565 -3.4897163 -3.5194683 -3.4489424 -3.3622091 -3.0678291 -2.8640735][-3.0031781 -2.7781177 -2.7460127 -2.5512428 -2.2838957 -2.1426408 -2.0131078 -2.0258598 -2.1666765 -2.4297786 -2.6518717 -2.6745567 -2.6154494 -2.3918834 -2.2614722][-2.3407645 -2.2468996 -2.2350776 -2.0192564 -1.7915092 -1.7159402 -1.6653271 -1.646584 -1.5201409 -1.5382936 -1.6581819 -1.871474 -1.9401307 -1.9959548 -2.1300588][-1.5664 -1.860249 -2.3335471 -2.2528019 -1.8338423 -1.2335105 -0.96337342 -0.9086175 -1.0673344 -1.0748434 -0.90411258 -1.2334723 -1.5377069 -1.7413321 -1.9612856][-1.3053348 -1.3668382 -1.4331532 -1.6492889 -1.7400482 -1.1482625 -0.5965457 -0.29412794 -0.2701683 -0.51235509 -0.79107904 -0.92156339 -1.2500887 -1.7242634 -2.1244504][-1.5753834 -1.5370753 -1.3373029 -0.98966408 -0.59377694 -0.19588089 0.0228858 0.086454868 0.15169573 -0.061036587 -0.54207516 -1.2427151 -1.77881 -2.0540988 -2.4895349][-2.1059847 -1.6700706 -1.6007433 -1.119571 -0.30579758 0.65716457 1.168488 1.0565095 0.79639673 0.26899481 -0.43526483 -1.2558727 -1.798238 -2.2887831 -2.7243919][-3.1754284 -2.8482585 -2.1801517 -1.5230682 -0.69993997 0.60454988 1.5683537 1.694828 1.3082309 0.5880661 -0.18066502 -1.2944577 -2.0933695 -2.353555 -2.4609666][-4.3326907 -4.1464 -3.7471051 -2.9899096 -1.8217709 -0.35245228 0.79190493 1.3090777 1.2932687 0.57026243 -0.44150805 -1.7532864 -2.6944218 -2.9854732 -2.9107289][-4.8434906 -5.4627957 -5.6722794 -5.0963545 -3.8694797 -2.2643025 -0.92704225 -0.29832029 -0.15243387 -0.55806947 -1.3815989 -2.5681353 -3.4391851 -3.6405005 -3.5112715][-6.0760117 -6.5826144 -6.7450104 -6.5611858 -5.8264265 -4.3622422 -2.9103255 -2.2450275 -1.9062941 -2.0936015 -2.7017655 -3.4090946 -3.9568877 -4.0895333 -3.8150439][-6.0727825 -6.8249006 -7.3400135 -7.1274457 -6.3961945 -5.2540555 -4.0659595 -3.5582168 -3.3281684 -3.2544284 -3.3905363 -3.9680073 -4.3623567 -4.3879995 -4.3121667][-5.8833776 -6.4513741 -6.7005448 -6.5681076 -6.082859 -5.23547 -4.314568 -3.7595239 -3.3709977 -3.3978577 -3.6238546 -3.9338155 -4.2740932 -4.2108622 -4.0342803][-5.0858097 -5.4496226 -5.4253373 -5.0038877 -4.518445 -4.0528326 -3.5570745 -3.3561997 -3.2549188 -2.94319 -2.7435248 -2.8300638 -3.0836954 -3.304534 -3.4476047][-4.1222653 -4.0051708 -3.8121426 -3.441407 -2.9968967 -2.7494557 -2.5229006 -2.2952058 -2.1423392 -2.189513 -2.1971395 -1.9549916 -1.9315462 -2.2767665 -2.5756974]]...]
INFO - root - 2017-12-16 07:47:09.848420: step 51910, loss = 0.23, batch loss = 0.18 (27.8 examples/sec; 0.288 sec/batch; 22h:27m:37s remains)
INFO - root - 2017-12-16 07:47:12.721062: step 51920, loss = 0.28, batch loss = 0.22 (28.9 examples/sec; 0.277 sec/batch; 21h:33m:01s remains)
INFO - root - 2017-12-16 07:47:15.606195: step 51930, loss = 0.25, batch loss = 0.19 (27.4 examples/sec; 0.292 sec/batch; 22h:47m:42s remains)
INFO - root - 2017-12-16 07:47:18.441488: step 51940, loss = 0.20, batch loss = 0.14 (28.1 examples/sec; 0.285 sec/batch; 22h:13m:25s remains)
INFO - root - 2017-12-16 07:47:21.265532: step 51950, loss = 0.20, batch loss = 0.15 (26.8 examples/sec; 0.299 sec/batch; 23h:18m:13s remains)
INFO - root - 2017-12-16 07:47:24.061458: step 51960, loss = 0.26, batch loss = 0.21 (28.3 examples/sec; 0.283 sec/batch; 22h:03m:44s remains)
INFO - root - 2017-12-16 07:47:26.882814: step 51970, loss = 0.25, batch loss = 0.20 (29.8 examples/sec; 0.269 sec/batch; 20h:55m:23s remains)
INFO - root - 2017-12-16 07:47:29.680509: step 51980, loss = 0.40, batch loss = 0.35 (28.8 examples/sec; 0.278 sec/batch; 21h:38m:56s remains)
INFO - root - 2017-12-16 07:47:32.515387: step 51990, loss = 0.24, batch loss = 0.19 (27.6 examples/sec; 0.290 sec/batch; 22h:36m:53s remains)
INFO - root - 2017-12-16 07:47:35.334495: step 52000, loss = 0.25, batch loss = 0.19 (28.8 examples/sec; 0.278 sec/batch; 21h:40m:18s remains)
2017-12-16 07:47:35.802473: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1978064 -5.4216394 -5.580061 -5.7208014 -5.6316409 -5.4140167 -5.1424255 -5.1020908 -5.1393838 -4.9207535 -4.772253 -4.6521978 -4.326201 -4.0037389 -3.760056][-4.9725132 -5.0151129 -5.0523214 -5.0995111 -5.0805006 -5.0032558 -5.0583181 -5.2143531 -5.3044634 -5.2853947 -5.2409868 -5.1967096 -4.9082079 -4.6195717 -4.3121414][-4.6509805 -4.4301558 -4.3358912 -4.3192058 -4.102344 -3.981108 -4.0661716 -4.3417907 -4.7462029 -4.9705968 -5.1222725 -5.2300248 -5.1015444 -4.9586568 -4.7434349][-4.1863122 -3.7715054 -3.3613446 -3.0539529 -2.69666 -2.5492668 -2.5454545 -2.828824 -3.3794217 -3.7812645 -4.1482906 -4.7022829 -4.9633431 -5.1395645 -5.0121851][-3.3151689 -2.7875161 -2.0804055 -1.4589245 -0.91537619 -0.63044262 -0.48942637 -0.90769458 -1.6035824 -2.1536539 -2.7900758 -3.5758324 -4.1705394 -4.59997 -4.6383147][-2.77634 -2.1317728 -1.2281959 -0.37730122 0.39243364 0.95018244 1.3492107 1.2255445 0.72541666 -0.018200397 -0.94219708 -2.0649059 -3.0622234 -3.6977024 -3.8722453][-2.6635208 -1.751554 -0.60203028 0.57221413 1.5686755 2.38869 3.1286349 3.1976204 2.8593321 2.1312928 1.1019907 -0.23683882 -1.5268645 -2.3984308 -2.8940926][-2.5842986 -1.6491082 -0.40919209 0.83893061 1.8402152 2.7523956 3.4380717 3.5475578 3.4582758 2.837872 1.9408507 0.64158154 -0.8675735 -1.981138 -2.5950716][-2.6373937 -1.8494563 -0.83412576 0.13221121 0.88591862 1.6085429 2.3120594 2.4845171 2.4474659 2.0076318 1.3943691 0.28047562 -1.0255072 -2.0739045 -2.7414708][-2.9469724 -2.3783081 -1.6520994 -1.0625114 -0.60370374 -0.10654163 0.34061098 0.45055962 0.53145504 0.34982157 -0.0594182 -1.041646 -2.1376367 -2.7765636 -3.113138][-2.9713223 -2.5246849 -2.2532873 -1.9566698 -1.8210115 -1.569994 -1.0720291 -1.1695318 -1.3177567 -1.5616775 -1.8098786 -2.2606189 -2.8001838 -3.2088711 -3.5141408][-3.0100675 -2.7221644 -2.5909853 -2.6431499 -2.8471684 -2.6607566 -2.1556666 -2.2525549 -2.2930026 -2.6232285 -2.9122682 -3.180306 -3.4737325 -3.5541887 -3.6242206][-3.3488522 -3.2639861 -3.3357005 -3.4013619 -3.6074574 -3.5389748 -3.2483416 -3.2736635 -3.1827774 -3.3092763 -3.3857732 -3.4072027 -3.5070484 -3.6205862 -3.7424855][-3.9182434 -3.8339777 -3.6210737 -3.3964789 -3.2746964 -3.1841464 -3.128031 -3.2294736 -3.1590748 -3.2722139 -3.2825775 -3.149961 -3.0708544 -2.8772497 -2.8580317][-4.2258649 -4.1450295 -3.8033357 -3.1868544 -2.6320062 -2.3285823 -2.2584198 -2.4267881 -2.5615182 -2.6379733 -2.642822 -2.5227447 -2.250344 -1.9796042 -1.9565678]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 07:47:38.635908: step 52010, loss = 0.23, batch loss = 0.17 (29.6 examples/sec; 0.270 sec/batch; 21h:03m:55s remains)
INFO - root - 2017-12-16 07:47:41.493400: step 52020, loss = 0.23, batch loss = 0.17 (28.1 examples/sec; 0.284 sec/batch; 22h:09m:27s remains)
INFO - root - 2017-12-16 07:47:44.277131: step 52030, loss = 0.22, batch loss = 0.16 (29.0 examples/sec; 0.276 sec/batch; 21h:30m:18s remains)
INFO - root - 2017-12-16 07:47:47.087132: step 52040, loss = 0.21, batch loss = 0.15 (28.8 examples/sec; 0.277 sec/batch; 21h:36m:14s remains)
INFO - root - 2017-12-16 07:47:49.907953: step 52050, loss = 0.32, batch loss = 0.26 (28.8 examples/sec; 0.277 sec/batch; 21h:36m:27s remains)
INFO - root - 2017-12-16 07:47:52.682465: step 52060, loss = 0.34, batch loss = 0.28 (29.6 examples/sec; 0.271 sec/batch; 21h:05m:13s remains)
INFO - root - 2017-12-16 07:47:55.534328: step 52070, loss = 0.27, batch loss = 0.21 (27.7 examples/sec; 0.289 sec/batch; 22h:30m:21s remains)
INFO - root - 2017-12-16 07:47:58.398837: step 52080, loss = 0.27, batch loss = 0.21 (28.8 examples/sec; 0.278 sec/batch; 21h:38m:32s remains)
INFO - root - 2017-12-16 07:48:01.242452: step 52090, loss = 0.23, batch loss = 0.17 (28.1 examples/sec; 0.285 sec/batch; 22h:12m:42s remains)
INFO - root - 2017-12-16 07:48:04.060156: step 52100, loss = 0.27, batch loss = 0.21 (28.1 examples/sec; 0.284 sec/batch; 22h:08m:13s remains)
2017-12-16 07:48:04.541124: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8195958 -4.7275667 -4.5451217 -4.817688 -5.20103 -5.3618426 -5.4003472 -5.30523 -5.2341304 -5.02289 -4.9235563 -4.8892488 -5.1453671 -5.4128246 -5.8769722][-5.1780987 -4.9212375 -4.8243561 -4.8370552 -4.8969617 -5.07566 -5.2198071 -4.9466629 -4.8362446 -4.75571 -4.7962422 -4.9350729 -5.2998104 -5.7391052 -6.4000931][-5.2108488 -4.8894815 -4.6651082 -4.4614682 -4.3308911 -4.4180627 -4.4474077 -4.3514352 -4.1543536 -4.2718081 -4.4788237 -4.6058359 -5.1834035 -5.8199997 -6.3206673][-4.8962221 -4.4266148 -4.026319 -3.4752166 -3.0587659 -3.051703 -3.0887103 -2.9690716 -2.8471735 -3.35004 -3.9732947 -4.3724666 -5.03621 -5.7536478 -6.42459][-4.7144608 -3.9093328 -3.0761051 -2.1911955 -1.5264192 -1.1888769 -1.1724646 -1.0447268 -1.0681162 -1.9048502 -2.8096313 -3.4663956 -4.3428411 -5.4578614 -6.3586473][-4.8841362 -3.8868973 -2.6347494 -1.0633874 0.17612267 0.59326649 0.740499 0.98888683 0.98496056 -0.019702435 -1.2495918 -2.0994012 -2.8732219 -4.0387869 -4.8274279][-5.249486 -3.9817429 -2.4302201 -0.43785763 1.4337187 2.2905245 2.6921191 2.6195731 2.3284259 1.276351 0.038951874 -0.7998395 -1.6791573 -2.7434089 -3.7876222][-5.633853 -4.177784 -2.3845861 -0.088022709 1.8095369 2.8795381 3.4304085 3.2029405 2.7884946 1.6492653 0.50393677 -0.34233141 -1.1624062 -1.9918964 -3.1853998][-6.561451 -5.1271372 -3.1457675 -0.80293131 0.80357409 1.8254199 2.6260829 2.4738145 1.9932661 1.0749774 0.39614439 -0.260509 -0.95846415 -1.6801383 -2.6031978][-7.4229307 -6.2714448 -4.7239108 -2.5356421 -0.90701294 0.21711588 0.72717476 0.76417351 0.3976717 -0.2038517 -0.56701875 -1.0074229 -1.2557116 -1.6749675 -2.5431762][-8.0889988 -7.3322449 -6.2120109 -4.5627017 -3.0126956 -2.0050924 -1.3433316 -1.3520412 -1.6782 -2.1505842 -2.2579422 -2.4458916 -2.6986198 -2.9593475 -3.6561661][-8.6114721 -8.126255 -7.2753439 -6.1795082 -5.1669955 -4.3955216 -3.7095909 -3.6205812 -3.7758868 -4.0264359 -4.0914769 -4.25759 -4.4113593 -4.5023866 -4.9949255][-8.2351189 -8.0043917 -7.5022988 -6.9779377 -6.3100481 -5.9365454 -5.4041758 -5.0969377 -4.8711443 -4.8433208 -4.7635713 -4.938416 -5.03778 -5.0145378 -5.2202711][-6.9127512 -6.5191755 -6.3956442 -6.1951222 -5.9820819 -5.6951218 -5.6304436 -5.4539208 -5.1151266 -4.8408632 -4.7322392 -5.0465894 -5.4432855 -5.5286551 -5.46666][-5.6211419 -5.0525022 -4.7070861 -4.6965957 -4.8041339 -4.7970486 -4.9601436 -4.9557133 -4.9557419 -4.8153896 -4.8260088 -5.123508 -5.4834876 -5.5377426 -5.5648217]]...]
INFO - root - 2017-12-16 07:48:07.362584: step 52110, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.285 sec/batch; 22h:12m:37s remains)
INFO - root - 2017-12-16 07:48:10.188621: step 52120, loss = 0.26, batch loss = 0.20 (27.5 examples/sec; 0.291 sec/batch; 22h:41m:01s remains)
INFO - root - 2017-12-16 07:48:12.991755: step 52130, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.284 sec/batch; 22h:07m:49s remains)
INFO - root - 2017-12-16 07:48:15.840473: step 52140, loss = 0.33, batch loss = 0.27 (29.0 examples/sec; 0.276 sec/batch; 21h:27m:33s remains)
INFO - root - 2017-12-16 07:48:18.630424: step 52150, loss = 0.48, batch loss = 0.42 (28.5 examples/sec; 0.281 sec/batch; 21h:53m:42s remains)
INFO - root - 2017-12-16 07:48:21.480306: step 52160, loss = 0.32, batch loss = 0.26 (27.9 examples/sec; 0.287 sec/batch; 22h:21m:10s remains)
INFO - root - 2017-12-16 07:48:24.334781: step 52170, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.285 sec/batch; 22h:13m:22s remains)
INFO - root - 2017-12-16 07:48:27.202148: step 52180, loss = 0.27, batch loss = 0.22 (27.5 examples/sec; 0.291 sec/batch; 22h:40m:44s remains)
INFO - root - 2017-12-16 07:48:30.076516: step 52190, loss = 0.26, batch loss = 0.20 (28.5 examples/sec; 0.280 sec/batch; 21h:49m:55s remains)
INFO - root - 2017-12-16 07:48:32.974045: step 52200, loss = 0.22, batch loss = 0.17 (26.5 examples/sec; 0.302 sec/batch; 23h:30m:08s remains)
2017-12-16 07:48:33.493988: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7695894 -7.1379671 -6.9698224 -7.0204215 -7.2335873 -7.0825782 -6.727036 -6.5518112 -6.567194 -6.6992378 -6.8243484 -7.1832972 -7.5637836 -7.6290388 -7.5849586][-7.2769804 -7.614748 -7.5935526 -7.6313429 -7.4037132 -7.3668852 -7.2967176 -7.2383223 -7.3762512 -7.7407928 -8.2619228 -8.5521374 -8.7061548 -8.72357 -8.5195827][-7.4399548 -7.5843716 -7.1664376 -6.9972458 -6.8215008 -6.5926228 -6.1451659 -6.2324266 -6.7257318 -7.5103955 -8.3983021 -9.034893 -9.4625092 -9.2858639 -8.8501225][-6.7466297 -6.8294964 -6.3623605 -5.6213856 -4.6931534 -4.2835741 -3.7939477 -3.8082087 -4.5140934 -5.81228 -7.064795 -8.0418968 -8.6134644 -8.7403126 -8.4428263][-5.705761 -5.4156733 -4.4803042 -3.3224573 -2.1720479 -0.99177432 -0.0016341209 -0.23104334 -1.0366139 -2.5943789 -4.2594795 -5.6351066 -6.38039 -6.7054238 -6.730504][-4.8812675 -4.1156011 -2.8577161 -1.1950941 0.86779213 2.3898377 3.3779187 3.5497837 2.7453523 0.69628 -1.3035975 -2.8856711 -3.7697868 -4.3199058 -4.5821285][-5.0399137 -4.0546842 -2.3524442 -0.072510719 2.3783045 4.6555433 6.4768314 6.7224035 5.8460665 3.8619461 1.563252 -0.54296994 -1.7529123 -2.452924 -3.008601][-5.6894855 -5.0325146 -3.6155844 -1.111948 1.9602828 4.6701794 6.6199875 7.4242268 6.9929123 5.0936852 2.7828255 0.67915106 -0.77222037 -1.8516693 -2.715591][-6.9720688 -6.6040773 -5.4026985 -3.243139 -0.65347648 2.1901274 4.7854252 5.8223772 5.7315664 4.2542439 2.2185402 0.17297363 -1.2013969 -1.9568689 -2.7401714][-8.15287 -8.0447521 -7.4342961 -5.8025656 -3.5519097 -1.0402164 1.1434121 2.5140648 3.0841923 2.2160482 0.73822737 -0.93763232 -2.1249831 -2.78819 -3.50188][-9.0259571 -9.2410583 -8.9454412 -7.7713366 -6.2676034 -4.3108664 -2.4573212 -1.1924167 -0.67704439 -1.1506782 -2.1902838 -3.4147315 -4.4559021 -4.9217558 -5.2516813][-9.4071522 -9.792511 -9.64603 -9.0635576 -8.0562992 -6.7611079 -5.5818319 -4.6701112 -4.1075873 -4.1477833 -4.6085739 -5.4110045 -6.0717125 -6.3919272 -6.6284618][-9.0346518 -9.3205547 -9.4914284 -9.4331627 -9.0237656 -8.2440214 -7.370759 -6.438365 -5.7587533 -5.5779481 -5.5875177 -5.8592844 -6.1191635 -6.5755048 -6.8655939][-7.3231392 -7.5293903 -7.696939 -7.891634 -7.8779345 -7.6587839 -7.3263206 -6.8156033 -6.3459311 -5.90583 -5.7171488 -6.0771713 -6.5166588 -6.8619237 -6.8618393][-6.4526682 -6.5150862 -6.4753051 -6.5749531 -6.6763992 -6.771843 -6.7481995 -6.4758482 -6.1048961 -5.8003511 -5.7224946 -5.7394323 -5.7939153 -6.1296573 -6.2474413]]...]
INFO - root - 2017-12-16 07:48:36.355195: step 52210, loss = 0.28, batch loss = 0.22 (28.7 examples/sec; 0.279 sec/batch; 21h:42m:02s remains)
INFO - root - 2017-12-16 07:48:39.281664: step 52220, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 21h:44m:06s remains)
INFO - root - 2017-12-16 07:48:42.154133: step 52230, loss = 0.26, batch loss = 0.20 (28.5 examples/sec; 0.281 sec/batch; 21h:50m:17s remains)
INFO - root - 2017-12-16 07:48:45.026615: step 52240, loss = 0.24, batch loss = 0.18 (27.0 examples/sec; 0.297 sec/batch; 23h:06m:28s remains)
INFO - root - 2017-12-16 07:48:47.908585: step 52250, loss = 0.25, batch loss = 0.20 (26.9 examples/sec; 0.298 sec/batch; 23h:09m:49s remains)
INFO - root - 2017-12-16 07:48:50.771957: step 52260, loss = 0.42, batch loss = 0.36 (27.7 examples/sec; 0.288 sec/batch; 22h:26m:41s remains)
INFO - root - 2017-12-16 07:48:53.646510: step 52270, loss = 0.32, batch loss = 0.26 (27.9 examples/sec; 0.286 sec/batch; 22h:18m:05s remains)
INFO - root - 2017-12-16 07:48:56.519418: step 52280, loss = 0.23, batch loss = 0.18 (28.2 examples/sec; 0.284 sec/batch; 22h:07m:04s remains)
INFO - root - 2017-12-16 07:48:59.402634: step 52290, loss = 0.25, batch loss = 0.20 (27.6 examples/sec; 0.290 sec/batch; 22h:32m:04s remains)
INFO - root - 2017-12-16 07:49:02.252413: step 52300, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.282 sec/batch; 21h:58m:37s remains)
2017-12-16 07:49:02.796808: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9691744 -5.1727653 -5.4774342 -6.1293788 -7.0324936 -7.7257986 -7.9994545 -7.7859454 -7.5360613 -7.1087861 -6.88046 -6.6728158 -6.3904676 -5.9926972 -5.856698][-4.2011294 -4.5100832 -4.9472284 -5.6337118 -6.1581397 -6.9422827 -7.2823343 -7.1064186 -6.5821638 -6.1860738 -6.054718 -6.0740061 -6.2586346 -5.9908605 -5.9404354][-3.4230821 -3.2152867 -3.4945383 -4.2591867 -4.9340611 -5.4859128 -5.6975322 -5.6499224 -5.3197746 -4.9702897 -4.8934083 -4.9091897 -5.1052561 -5.2190409 -5.1905818][-2.2638865 -2.1094153 -2.3615661 -2.994653 -3.5970144 -3.9371636 -3.7501793 -3.5617456 -3.2475433 -3.1974003 -3.2436905 -3.6524634 -4.0606656 -4.2853775 -4.4514961][-1.5827224 -1.3376606 -1.272181 -1.8573596 -2.5528417 -2.7488232 -2.2257745 -1.9914026 -1.5902088 -1.5483742 -1.5662639 -2.1825438 -2.9229226 -3.5014448 -3.8371124][-1.886029 -1.5310612 -1.0829136 -1.0573592 -1.1418447 -1.0879147 -0.48752904 0.18033266 0.50477457 0.39853382 0.2474184 -0.37379646 -1.5395057 -2.5308776 -3.078146][-3.0607429 -2.60265 -1.9136643 -1.4548283 -1.0982106 -0.43587041 0.74498987 1.7005048 2.1199598 1.9411254 1.7077427 0.9277277 -0.15540552 -1.3513844 -2.4175415][-4.4799342 -3.6987348 -2.9731336 -2.2842362 -1.5249496 -0.40808916 0.92564344 2.1507187 2.8607392 2.7016845 2.3493328 1.4363694 0.30690622 -0.90225077 -1.9017897][-5.8291483 -5.0291281 -3.9937568 -3.0858679 -2.3244569 -1.2295995 0.28674412 1.6036978 2.3739119 2.51129 2.3243542 1.562645 0.49606991 -0.59945631 -1.7032402][-6.2974167 -5.9496169 -5.2878289 -4.2396626 -3.0895553 -2.1048462 -0.9746089 0.25037193 1.057507 1.335115 1.1608443 0.64298916 -0.038359642 -0.83752561 -1.7099621][-6.4577456 -5.9017344 -5.3346744 -4.8275838 -4.1541014 -3.3130887 -2.3242097 -1.235867 -0.67133665 -0.47159386 -0.49649334 -0.73791075 -1.1585495 -1.7179482 -2.3343289][-6.2619534 -5.9459567 -5.6240597 -5.31957 -4.9989591 -4.691834 -4.1709151 -3.3818717 -2.7540679 -2.4810839 -2.4299951 -2.4345398 -2.5023232 -2.6331334 -2.998234][-5.5628834 -5.4014373 -5.408556 -5.4257 -5.5207767 -5.6517515 -5.64032 -5.1914334 -4.5411925 -4.1760035 -3.8687654 -3.7280235 -3.6773 -3.6419959 -3.6525474][-4.9139509 -4.7175188 -4.8437691 -5.0672221 -5.3023944 -5.5314407 -5.7783051 -5.6808944 -5.43948 -5.07811 -4.6739669 -4.4420094 -4.3681512 -4.2806511 -4.1442585][-4.2908068 -4.0255909 -4.0626192 -4.3907804 -4.7448683 -5.0989804 -5.3013344 -5.2587514 -5.1745224 -4.9506984 -4.7430172 -4.4769897 -4.3193378 -4.2312822 -4.1230736]]...]
INFO - root - 2017-12-16 07:49:05.667356: step 52310, loss = 0.25, batch loss = 0.20 (28.8 examples/sec; 0.278 sec/batch; 21h:38m:01s remains)
INFO - root - 2017-12-16 07:49:08.514848: step 52320, loss = 0.25, batch loss = 0.19 (28.5 examples/sec; 0.281 sec/batch; 21h:52m:43s remains)
INFO - root - 2017-12-16 07:49:11.368166: step 52330, loss = 0.29, batch loss = 0.23 (28.7 examples/sec; 0.279 sec/batch; 21h:43m:33s remains)
INFO - root - 2017-12-16 07:49:14.285983: step 52340, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.281 sec/batch; 21h:50m:20s remains)
INFO - root - 2017-12-16 07:49:17.192509: step 52350, loss = 0.19, batch loss = 0.13 (27.1 examples/sec; 0.296 sec/batch; 22h:59m:49s remains)
INFO - root - 2017-12-16 07:49:20.061348: step 52360, loss = 0.20, batch loss = 0.14 (27.8 examples/sec; 0.288 sec/batch; 22h:23m:25s remains)
INFO - root - 2017-12-16 07:49:22.904321: step 52370, loss = 0.33, batch loss = 0.27 (27.4 examples/sec; 0.292 sec/batch; 22h:45m:11s remains)
INFO - root - 2017-12-16 07:49:25.758881: step 52380, loss = 0.23, batch loss = 0.17 (29.5 examples/sec; 0.272 sec/batch; 21h:07m:53s remains)
INFO - root - 2017-12-16 07:49:28.624443: step 52390, loss = 0.21, batch loss = 0.15 (26.9 examples/sec; 0.298 sec/batch; 23h:10m:29s remains)
INFO - root - 2017-12-16 07:49:31.493053: step 52400, loss = 0.31, batch loss = 0.25 (26.9 examples/sec; 0.297 sec/batch; 23h:06m:45s remains)
2017-12-16 07:49:32.037276: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1715446 -4.1851726 -4.2756262 -4.4105406 -4.3884091 -4.2040815 -4.0045671 -3.869741 -3.8751888 -3.9096849 -3.9843454 -3.9776087 -4.1333432 -4.3683252 -4.4265594][-3.9112008 -4.0199475 -4.1281743 -4.175087 -4.1337433 -3.9164128 -3.6487424 -3.5351429 -3.6438992 -3.7215838 -3.7455597 -3.6959612 -3.7059398 -3.8869576 -3.8390365][-3.5510433 -3.7069409 -3.8596416 -3.8290017 -3.5806189 -3.3423033 -3.1381693 -2.921526 -2.9538465 -3.1125197 -3.1305285 -3.0138779 -3.0064387 -3.0888555 -3.0208211][-3.1364613 -3.3120365 -3.3888054 -3.2948742 -3.01656 -2.6270635 -2.2409215 -2.114625 -2.2269108 -2.3557656 -2.4361653 -2.3215013 -2.277204 -2.3318737 -2.287854][-2.6423783 -2.799639 -2.7862403 -2.5531151 -2.0717077 -1.6367385 -1.2344599 -1.049377 -1.1604249 -1.4052651 -1.6030397 -1.6029985 -1.5279274 -1.5141077 -1.5401192][-2.1718142 -2.065975 -1.9291184 -1.6059177 -0.98182297 -0.46827698 -0.099171162 -0.030103683 -0.14545727 -0.45501232 -0.76881814 -0.8644383 -0.88372517 -0.72399831 -0.76175117][-1.3324568 -1.2447619 -1.1084845 -0.70103812 -0.055870056 0.45529366 0.87817287 0.81013632 0.67771339 0.37706137 0.018193245 -0.23669243 -0.34703016 -0.16807652 -0.27713871][-0.56248212 -0.43493557 -0.33207941 -0.052423954 0.38822556 0.74400759 1.0995789 0.96261454 0.8096137 0.48372412 0.10199213 -0.046510696 -0.035128593 0.16060877 0.16013861][-0.19680691 -0.045078754 -0.11883926 0.017186642 0.18912077 0.30215263 0.4369688 0.305336 0.27597427 0.06814146 -0.20171404 -0.37363005 -0.39979124 -0.1501236 -0.042812824][0.055339813 0.1180768 -0.28540564 -0.48630095 -0.64515972 -0.64518309 -0.57498884 -0.76392746 -0.78634453 -0.79435396 -0.85921288 -1.0399516 -1.0860143 -0.79835653 -0.53600335][-0.2410717 -0.12472773 -0.64982057 -1.0698435 -1.4776964 -1.72752 -1.8319569 -1.8556483 -1.6881027 -1.6413262 -1.7424402 -1.8749774 -1.8718951 -1.6345358 -1.3108575][-0.43568516 -0.43861341 -1.081229 -1.6737111 -2.1404047 -2.3033009 -2.3872206 -2.4111826 -2.3263025 -2.2199 -2.3598249 -2.5682716 -2.6595979 -2.4778597 -2.0906074][-0.67247772 -0.42705631 -0.93663764 -1.5560849 -1.9831102 -2.1428158 -2.2096245 -2.1750281 -2.20203 -2.4274094 -2.8103609 -3.2727196 -3.4713166 -3.23445 -2.6467485][-0.538234 -0.19885063 -0.50743937 -0.98406577 -1.4108398 -1.59782 -1.7652278 -1.8577571 -1.9723961 -2.1309466 -2.4969373 -3.0426922 -3.3417172 -3.2649655 -2.7928171][-0.41538525 -0.055166245 -0.22861528 -0.52847791 -0.78729296 -0.97719312 -1.114285 -1.2704179 -1.4918356 -1.8503895 -2.3308792 -2.7359593 -2.9251919 -2.7333803 -2.19338]]...]
INFO - root - 2017-12-16 07:49:34.878403: step 52410, loss = 0.27, batch loss = 0.21 (27.9 examples/sec; 0.287 sec/batch; 22h:19m:32s remains)
INFO - root - 2017-12-16 07:49:37.751172: step 52420, loss = 0.21, batch loss = 0.15 (27.4 examples/sec; 0.292 sec/batch; 22h:41m:03s remains)
INFO - root - 2017-12-16 07:49:40.659008: step 52430, loss = 0.28, batch loss = 0.22 (27.2 examples/sec; 0.294 sec/batch; 22h:52m:52s remains)
INFO - root - 2017-12-16 07:49:43.511020: step 52440, loss = 0.34, batch loss = 0.28 (28.3 examples/sec; 0.283 sec/batch; 21h:59m:08s remains)
INFO - root - 2017-12-16 07:49:46.413802: step 52450, loss = 0.21, batch loss = 0.15 (27.4 examples/sec; 0.292 sec/batch; 22h:42m:43s remains)
INFO - root - 2017-12-16 07:49:49.190365: step 52460, loss = 0.29, batch loss = 0.23 (29.3 examples/sec; 0.273 sec/batch; 21h:14m:59s remains)
INFO - root - 2017-12-16 07:49:52.026110: step 52470, loss = 0.48, batch loss = 0.42 (26.4 examples/sec; 0.304 sec/batch; 23h:36m:47s remains)
INFO - root - 2017-12-16 07:49:54.904183: step 52480, loss = 0.31, batch loss = 0.25 (27.8 examples/sec; 0.288 sec/batch; 22h:24m:27s remains)
INFO - root - 2017-12-16 07:49:57.707958: step 52490, loss = 0.23, batch loss = 0.17 (28.8 examples/sec; 0.278 sec/batch; 21h:36m:30s remains)
INFO - root - 2017-12-16 07:50:00.621165: step 52500, loss = 0.41, batch loss = 0.35 (28.1 examples/sec; 0.285 sec/batch; 22h:09m:52s remains)
2017-12-16 07:50:01.159526: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8788095 -2.7742951 -2.9539156 -2.7318175 -2.816273 -3.2790904 -3.8525417 -4.0065608 -3.865181 -3.6578393 -3.1320751 -2.5170629 -2.143028 -1.9199858 -1.3965459][-2.181653 -1.9623508 -2.2449405 -2.4648509 -2.733017 -3.1662416 -3.8142495 -4.1743364 -4.3015609 -3.9855881 -3.4812384 -3.0763741 -2.56103 -2.3247797 -1.6329315][-1.5502696 -1.6777794 -2.0345697 -2.1313448 -2.1103778 -2.6655517 -3.6493838 -4.0751681 -4.086308 -4.0997815 -3.8757048 -3.4660833 -2.6965375 -2.4670589 -2.1223083][-1.0211556 -1.197859 -1.5365152 -1.8426204 -2.0472763 -2.3583884 -2.746906 -3.0085807 -3.1640608 -3.523941 -3.4749241 -3.3521602 -3.2224391 -3.0089171 -2.5871124][-1.4359162 -1.4470487 -1.49332 -1.4636381 -1.168329 -1.2152398 -1.3886204 -1.7262533 -2.0690994 -2.4451938 -2.8518586 -3.0527153 -3.1505318 -3.0915685 -2.6708293][-1.6293588 -1.6227725 -1.318826 -0.91444039 -0.28625154 0.10480785 0.35451698 0.25791311 -0.076253891 -0.79900241 -1.6461761 -2.1976681 -2.3714993 -2.5797517 -2.3410189][-2.2230668 -2.0456965 -1.3519175 -0.49105787 0.5496192 1.2945957 1.8905759 1.9883761 1.6185536 0.81526661 0.024497509 -0.8821671 -1.3675916 -1.4532189 -1.2275672][-2.8279929 -2.5375924 -1.7624924 -0.64561057 0.58599997 1.5738416 2.423202 2.6347313 2.420198 1.7427716 0.96289253 0.27726793 -0.10970926 -0.17587614 -0.18017912][-3.5276656 -3.1308186 -2.2726979 -1.2459388 -0.054273605 1.0299134 1.8573484 2.1097937 1.9616122 1.6423922 1.2671375 0.66591072 0.30121088 0.40590429 0.67234182][-3.9716384 -3.5960789 -2.6944561 -1.752378 -0.77748704 -0.0033788681 0.56251669 0.70844746 0.48960924 0.41919088 0.43791676 0.15543938 0.0662446 0.265419 0.6123805][-3.8748493 -3.4559882 -2.741745 -2.0896823 -1.4657791 -1.1382136 -0.93860412 -1.1341286 -1.5304456 -1.393234 -1.1233289 -0.80938458 -0.45810461 -0.1518898 0.20916891][-3.7227068 -3.3191288 -2.8616607 -2.4573951 -2.1333296 -2.2691324 -2.592998 -2.955368 -3.2345912 -3.098345 -2.6849985 -2.0825593 -1.3826301 -0.87125421 -0.36474895][-3.254374 -2.8769832 -2.4353919 -2.3429441 -2.5305841 -3.0995145 -3.8800271 -4.545866 -5.031673 -4.9388061 -4.279377 -3.4603572 -2.651566 -1.8523552 -1.220037][-2.9126253 -2.5618806 -2.288955 -2.2060981 -2.5244398 -3.2918453 -4.4801617 -5.4786882 -6.0370445 -6.0436487 -5.3663678 -4.3683605 -3.3224616 -2.4791236 -1.903358][-2.801672 -2.2556307 -2.060694 -2.0369921 -2.3979251 -3.3296638 -4.58863 -5.7658362 -6.4612637 -6.5027027 -6.0822415 -5.1069894 -3.9511304 -3.1653616 -2.5860405]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-52500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-52500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 07:50:04.837030: step 52510, loss = 0.32, batch loss = 0.27 (29.2 examples/sec; 0.274 sec/batch; 21h:18m:41s remains)
INFO - root - 2017-12-16 07:50:07.669478: step 52520, loss = 0.36, batch loss = 0.30 (28.2 examples/sec; 0.284 sec/batch; 22h:03m:39s remains)
INFO - root - 2017-12-16 07:50:10.496898: step 52530, loss = 0.33, batch loss = 0.27 (28.6 examples/sec; 0.280 sec/batch; 21h:46m:27s remains)
INFO - root - 2017-12-16 07:50:13.325472: step 52540, loss = 0.33, batch loss = 0.27 (28.5 examples/sec; 0.280 sec/batch; 21h:48m:13s remains)
INFO - root - 2017-12-16 07:50:16.191879: step 52550, loss = 0.26, batch loss = 0.20 (27.4 examples/sec; 0.292 sec/batch; 22h:43m:47s remains)
INFO - root - 2017-12-16 07:50:19.047195: step 52560, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 22h:08m:46s remains)
INFO - root - 2017-12-16 07:50:21.857508: step 52570, loss = 0.23, batch loss = 0.17 (28.9 examples/sec; 0.277 sec/batch; 21h:31m:33s remains)
INFO - root - 2017-12-16 07:50:24.686925: step 52580, loss = 0.21, batch loss = 0.16 (27.1 examples/sec; 0.296 sec/batch; 22h:58m:41s remains)
INFO - root - 2017-12-16 07:50:27.537358: step 52590, loss = 0.32, batch loss = 0.26 (28.2 examples/sec; 0.284 sec/batch; 22h:05m:28s remains)
INFO - root - 2017-12-16 07:50:30.371626: step 52600, loss = 0.35, batch loss = 0.29 (27.4 examples/sec; 0.292 sec/batch; 22h:44m:21s remains)
2017-12-16 07:50:30.913226: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4155271 -3.4165318 -3.445478 -3.2304983 -3.1163909 -3.0977945 -3.0842113 -3.2804675 -3.5156715 -3.7203693 -4.0014448 -4.03516 -4.2616305 -4.6159081 -5.1725216][-3.1977713 -3.2167425 -3.0955691 -2.9153185 -2.6664004 -2.4444623 -2.2344911 -2.3265569 -2.743187 -3.1206648 -3.5931878 -3.9564736 -4.1619754 -4.4971495 -4.90889][-2.2815733 -2.322479 -2.2022936 -1.9710314 -1.7927928 -1.5713968 -1.3935139 -1.4683714 -1.9517326 -2.6712933 -3.3120611 -3.6860278 -4.0542312 -4.1480322 -4.4707322][-1.5022824 -1.694406 -1.845402 -1.6962299 -1.5062025 -1.3077605 -1.0109112 -1.2293825 -1.6193304 -2.4357502 -2.9311645 -3.4372263 -3.6321158 -3.6415977 -4.0695848][-1.0782542 -1.1545525 -1.1770413 -1.0555856 -0.722903 -0.31558371 0.073322773 0.092023849 -0.51511812 -1.2941091 -2.030966 -2.5565128 -2.6989269 -2.8382468 -3.3301511][-1.0209417 -0.88524437 -0.6135447 -0.28060865 0.35846996 0.93395519 1.3747067 1.3642597 0.84895563 -0.038890362 -0.89775372 -1.2976792 -1.5537188 -1.9929016 -2.6483943][-0.7445848 -0.50516081 -0.34825945 -0.021978378 0.56018639 1.1413369 1.675787 1.8882704 1.7823248 1.0575824 0.30507421 -0.13794374 -0.52837467 -1.16557 -2.0011752][-1.0596635 -0.80675054 -0.74787855 -0.51732206 -0.024911404 0.51722527 1.1925411 1.6699948 2.1056089 2.0269256 1.712997 1.2317514 0.61932135 -0.26162052 -1.6712844][-1.9155343 -1.8314641 -1.8588724 -1.7472229 -1.2650752 -0.69189692 -0.050688744 0.84209538 1.7660341 2.0823989 2.2172079 1.9632215 1.2658992 0.26995516 -1.2331359][-2.5781898 -2.6972554 -2.7916198 -2.98217 -2.7216644 -2.2706459 -1.6753914 -0.72187757 0.21846008 1.0852823 1.6914682 1.702621 1.2421288 0.27883482 -1.1002886][-3.1359389 -3.338789 -3.67783 -3.9910457 -4.1018286 -3.8625283 -3.3748312 -2.3629711 -1.2325506 -0.14992094 0.66776323 0.66889143 0.40667677 -0.55823088 -1.9446332][-4.5348053 -4.7342787 -5.1487393 -5.5900493 -5.8761883 -5.8473835 -5.5525265 -4.6523228 -3.2079363 -1.8826802 -1.1168914 -0.83797908 -0.99858284 -1.8640285 -3.3636131][-5.8446732 -6.2010226 -6.7851105 -7.4824991 -8.2345276 -8.6685925 -8.4786415 -7.577126 -5.9347038 -4.4248242 -3.2657127 -2.7990274 -3.0911069 -3.9032817 -4.9185104][-6.3156376 -6.7706575 -7.6020346 -8.88385 -10.134385 -10.638575 -10.357843 -9.5695124 -8.2865229 -6.5395226 -5.2211246 -4.9393625 -5.52916 -6.2149916 -7.0497055][-6.2904811 -6.8173513 -7.9460316 -9.2834873 -10.456627 -11.204923 -11.168328 -10.304127 -9.0115595 -7.725234 -6.8009653 -6.8009863 -7.2898316 -7.9455576 -8.60063]]...]
INFO - root - 2017-12-16 07:50:33.756784: step 52610, loss = 0.30, batch loss = 0.24 (29.1 examples/sec; 0.275 sec/batch; 21h:22m:03s remains)
INFO - root - 2017-12-16 07:50:36.592657: step 52620, loss = 0.29, batch loss = 0.23 (27.8 examples/sec; 0.288 sec/batch; 22h:22m:08s remains)
INFO - root - 2017-12-16 07:50:39.395480: step 52630, loss = 0.31, batch loss = 0.25 (29.5 examples/sec; 0.271 sec/batch; 21h:06m:08s remains)
INFO - root - 2017-12-16 07:50:42.305829: step 52640, loss = 0.24, batch loss = 0.18 (28.6 examples/sec; 0.279 sec/batch; 21h:43m:39s remains)
INFO - root - 2017-12-16 07:50:45.154621: step 52650, loss = 0.28, batch loss = 0.22 (28.6 examples/sec; 0.279 sec/batch; 21h:42m:57s remains)
INFO - root - 2017-12-16 07:50:47.997871: step 52660, loss = 0.24, batch loss = 0.18 (29.4 examples/sec; 0.272 sec/batch; 21h:09m:08s remains)
INFO - root - 2017-12-16 07:50:50.774633: step 52670, loss = 0.32, batch loss = 0.27 (28.5 examples/sec; 0.281 sec/batch; 21h:51m:22s remains)
INFO - root - 2017-12-16 07:50:53.636578: step 52680, loss = 0.37, batch loss = 0.31 (27.7 examples/sec; 0.289 sec/batch; 22h:29m:06s remains)
INFO - root - 2017-12-16 07:50:56.491173: step 52690, loss = 0.23, batch loss = 0.18 (27.0 examples/sec; 0.296 sec/batch; 23h:01m:15s remains)
INFO - root - 2017-12-16 07:50:59.320181: step 52700, loss = 0.30, batch loss = 0.25 (28.6 examples/sec; 0.280 sec/batch; 21h:46m:17s remains)
2017-12-16 07:50:59.852424: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5736694 -5.5757966 -5.1793251 -4.9631081 -4.7973623 -4.879755 -4.9040833 -4.9967403 -5.0636973 -5.1074572 -5.3074074 -5.665103 -6.1200476 -6.342494 -6.4404697][-6.0002217 -5.7604971 -5.3465257 -5.1094031 -4.8382478 -4.8689852 -4.9860697 -5.2057285 -5.0933433 -5.0769267 -5.3960233 -5.9323807 -6.57681 -6.5644865 -6.3844557][-6.6354055 -6.251585 -5.5146332 -4.8684092 -4.416718 -4.2370296 -4.2953639 -4.5364385 -4.6618028 -4.9147058 -5.2050295 -5.5661249 -6.1168971 -6.2099075 -5.9351382][-6.3184557 -6.1838379 -5.6555591 -4.7148438 -3.7613945 -3.124536 -2.7562752 -2.8419302 -3.4548337 -4.1418571 -4.496954 -4.8792334 -5.3561354 -5.3403831 -5.0823731][-6.0509081 -5.7226524 -4.8287096 -3.7446246 -2.4767365 -1.4144924 -0.89664459 -1.0413761 -1.5794373 -2.3140314 -2.9773703 -3.3981557 -3.7217526 -3.9682705 -4.1994619][-5.7631483 -5.5502453 -4.57162 -2.9197757 -1.0123706 0.37731934 1.104363 1.0802293 0.47408676 -0.39947081 -0.94481277 -1.3704317 -2.014992 -2.6834264 -3.0942271][-5.6185207 -5.3597941 -4.373333 -2.5302167 -0.34815264 1.4940691 2.5959816 2.6255069 1.8759117 1.0011845 0.50776434 -0.16440439 -0.84601021 -1.8790751 -2.9939041][-5.5415282 -5.3691349 -4.3908215 -2.4156446 -0.26318169 1.6446276 2.8640108 3.2970953 2.8959746 1.9759893 1.3024874 0.55295467 -0.14950514 -1.4544604 -2.8467267][-6.0327692 -5.6071544 -4.5429339 -2.8829699 -0.97402239 0.92666054 2.1288218 2.6014533 2.311656 1.7228699 1.1437941 0.45432711 0.088991165 -0.90529084 -2.5478241][-6.5938444 -6.3583717 -5.6105466 -4.1243477 -2.2946796 -0.68104839 0.41122818 0.96031523 0.88569307 0.42112112 -0.0033321381 -0.41730165 -0.6955924 -1.4342253 -2.5768929][-7.8965969 -7.5388522 -6.7690182 -5.608036 -4.4771137 -3.2073112 -2.0613434 -1.4974942 -1.4910851 -1.86762 -1.9781816 -2.0929203 -2.1699746 -2.4888887 -3.2144415][-8.6144562 -8.2934818 -7.6165018 -6.7191005 -5.7923908 -4.8617892 -4.270185 -4.052783 -3.8150778 -3.8901305 -4.0228629 -4.173367 -4.1583185 -4.0849094 -4.2543173][-9.2007351 -8.6946678 -8.04058 -7.3390093 -6.5975304 -5.9357781 -5.50486 -5.4495554 -5.318356 -5.2902193 -5.3094125 -5.3318281 -5.292407 -5.2048836 -4.9533682][-9.7699337 -9.2942066 -8.3154287 -7.4982324 -6.9447832 -6.4856882 -6.0868134 -6.1868129 -6.2769623 -6.394886 -6.3117876 -6.0966339 -5.789854 -5.6042442 -5.2791753][-9.3698063 -9.05419 -8.2086964 -7.2033205 -6.5631137 -6.2031951 -5.8398008 -5.860662 -6.0164428 -6.3720541 -6.4426723 -6.1754107 -5.781652 -5.6113353 -5.3177505]]...]
INFO - root - 2017-12-16 07:51:02.611705: step 52710, loss = 0.23, batch loss = 0.17 (29.0 examples/sec; 0.276 sec/batch; 21h:28m:09s remains)
INFO - root - 2017-12-16 07:51:05.418184: step 52720, loss = 0.20, batch loss = 0.14 (29.2 examples/sec; 0.274 sec/batch; 21h:17m:21s remains)
INFO - root - 2017-12-16 07:51:08.249121: step 52730, loss = 0.27, batch loss = 0.21 (27.9 examples/sec; 0.287 sec/batch; 22h:15m:55s remains)
INFO - root - 2017-12-16 07:51:11.083977: step 52740, loss = 0.21, batch loss = 0.15 (28.0 examples/sec; 0.285 sec/batch; 22h:11m:01s remains)
INFO - root - 2017-12-16 07:51:13.870732: step 52750, loss = 0.23, batch loss = 0.17 (27.7 examples/sec; 0.289 sec/batch; 22h:26m:48s remains)
INFO - root - 2017-12-16 07:51:16.704357: step 52760, loss = 0.37, batch loss = 0.31 (30.1 examples/sec; 0.266 sec/batch; 20h:39m:16s remains)
INFO - root - 2017-12-16 07:51:19.521217: step 52770, loss = 0.20, batch loss = 0.14 (29.3 examples/sec; 0.273 sec/batch; 21h:11m:57s remains)
INFO - root - 2017-12-16 07:51:22.351696: step 52780, loss = 0.27, batch loss = 0.21 (27.4 examples/sec; 0.292 sec/batch; 22h:43m:20s remains)
INFO - root - 2017-12-16 07:51:25.138119: step 52790, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.280 sec/batch; 21h:46m:22s remains)
INFO - root - 2017-12-16 07:51:27.977775: step 52800, loss = 0.30, batch loss = 0.24 (27.7 examples/sec; 0.289 sec/batch; 22h:27m:43s remains)
2017-12-16 07:51:28.495324: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6312737 -4.8510041 -4.5679436 -4.2490354 -3.6263366 -3.8085454 -4.3986769 -4.8650136 -5.1713376 -5.5285115 -5.7785835 -5.9797373 -6.2573557 -6.3795395 -6.3662591][-4.571774 -4.8440461 -4.6457591 -4.282825 -3.6911368 -3.6281695 -3.781884 -4.218524 -4.7907233 -5.2032771 -5.526669 -5.7806787 -6.0560918 -6.1996546 -6.124033][-4.725563 -4.9628391 -4.6565094 -3.9942524 -3.1189098 -2.8873088 -2.8956718 -3.1093512 -3.5687854 -4.019712 -4.5073175 -4.9197793 -5.2815795 -5.389679 -5.2979708][-4.4661512 -4.7553506 -4.3904734 -3.416142 -2.2551248 -1.5437949 -1.2682252 -1.3715739 -1.7959383 -2.4171963 -3.0907745 -3.6020641 -3.9351575 -4.0929856 -4.2107959][-4.0344405 -4.0268722 -3.4526772 -2.347337 -0.98809648 0.1967411 0.8263135 0.75109863 0.12016392 -0.518877 -1.2710812 -2.0528259 -2.6022415 -2.9584286 -2.8883235][-3.8569846 -3.7371106 -3.029458 -1.4982879 0.30807018 1.738658 2.5410538 2.7369003 2.2563324 1.236062 0.21335554 -0.55688548 -1.0038812 -1.4114873 -1.4993382][-3.7612731 -3.5479906 -2.8424702 -1.3159811 0.67751026 2.5414062 3.8200941 4.0838556 3.4689832 2.4401903 1.3221211 0.40554667 -0.05610466 -0.27942228 -0.18687344][-3.5230472 -3.2352507 -2.7727742 -1.5135527 0.31960297 2.1685262 3.6386909 4.2660656 3.9620018 2.8145418 1.6553445 0.73078871 0.30347681 0.23116684 0.41568375][-3.6581721 -3.6022241 -3.2526324 -2.1807067 -0.82181287 0.80253935 2.3836527 3.1014743 2.9567103 2.0249352 1.0828829 0.27764845 -0.038135052 0.11219645 0.528162][-4.426578 -4.6403136 -4.4525771 -3.6911376 -2.524755 -1.2675261 -0.064172745 0.72266054 0.86826563 0.40454578 -0.18021679 -0.97028279 -1.2514367 -1.0947204 -0.71498609][-5.0535736 -5.5036435 -5.6285753 -5.1931119 -4.420785 -3.5011821 -2.5559998 -2.2038572 -2.2172601 -2.4643307 -2.6006985 -2.9799027 -3.2068954 -3.1235981 -2.7984867][-5.4351664 -5.8171496 -5.8534417 -5.810442 -5.6350136 -5.3297572 -4.9731412 -4.9443603 -4.9508648 -5.1506767 -5.1767278 -5.4154549 -5.5489683 -5.4412761 -5.1950207][-4.9910135 -5.3717375 -5.6931777 -5.74869 -5.7260938 -5.8060913 -5.8500624 -6.2405281 -6.6941795 -7.0215063 -7.1066055 -7.017518 -6.9574218 -7.1916809 -7.1109123][-4.6511607 -4.8383293 -4.8056145 -4.9091244 -5.2331486 -5.6029272 -5.8742032 -6.1670747 -6.7786245 -7.426034 -7.7127323 -7.746747 -7.7882977 -7.9983735 -8.0488768][-4.2899928 -4.2261539 -4.1186113 -3.9870248 -4.0219688 -4.4854712 -5.1256518 -5.5941782 -6.0444479 -6.6510344 -7.2097025 -7.403656 -7.3735209 -7.5881977 -7.8186111]]...]
INFO - root - 2017-12-16 07:51:31.285395: step 52810, loss = 0.22, batch loss = 0.17 (29.3 examples/sec; 0.273 sec/batch; 21h:13m:42s remains)
INFO - root - 2017-12-16 07:51:34.075971: step 52820, loss = 0.29, batch loss = 0.23 (28.5 examples/sec; 0.281 sec/batch; 21h:50m:15s remains)
INFO - root - 2017-12-16 07:51:36.935614: step 52830, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.277 sec/batch; 21h:32m:09s remains)
INFO - root - 2017-12-16 07:51:39.802111: step 52840, loss = 0.22, batch loss = 0.16 (28.9 examples/sec; 0.277 sec/batch; 21h:30m:54s remains)
INFO - root - 2017-12-16 07:51:42.622365: step 52850, loss = 0.21, batch loss = 0.15 (28.5 examples/sec; 0.281 sec/batch; 21h:48m:06s remains)
INFO - root - 2017-12-16 07:51:45.476237: step 52860, loss = 0.27, batch loss = 0.21 (28.4 examples/sec; 0.282 sec/batch; 21h:53m:24s remains)
INFO - root - 2017-12-16 07:51:48.348060: step 52870, loss = 0.22, batch loss = 0.17 (28.1 examples/sec; 0.285 sec/batch; 22h:06m:33s remains)
INFO - root - 2017-12-16 07:51:51.199933: step 52880, loss = 0.36, batch loss = 0.30 (29.2 examples/sec; 0.274 sec/batch; 21h:16m:35s remains)
INFO - root - 2017-12-16 07:51:54.045919: step 52890, loss = 0.30, batch loss = 0.25 (28.4 examples/sec; 0.281 sec/batch; 21h:50m:44s remains)
INFO - root - 2017-12-16 07:51:56.887238: step 52900, loss = 0.24, batch loss = 0.18 (27.2 examples/sec; 0.294 sec/batch; 22h:48m:39s remains)
2017-12-16 07:51:57.394630: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.45981 -6.1817522 -6.7884293 -8.1070986 -9.4410152 -10.273076 -10.512024 -9.9399948 -8.6561222 -7.0020428 -5.2641091 -4.0074482 -3.2410233 -2.9418659 -2.77137][-5.6553316 -6.454906 -7.1731911 -8.0735474 -8.69467 -9.1004171 -9.0525246 -8.3941793 -7.3072944 -5.8242168 -4.657177 -3.7747681 -3.0653524 -2.7096143 -2.5218933][-6.3262954 -6.7230921 -6.7265558 -7.0200787 -7.2198863 -6.9167509 -6.5136843 -5.9923978 -5.3858995 -4.6975341 -4.0902224 -3.4608803 -2.9014654 -2.5775819 -2.4603641][-6.6733217 -6.914506 -6.1115427 -5.2055635 -4.5379286 -4.0385981 -3.7609878 -3.5032344 -3.2365072 -3.0217953 -2.9109681 -2.8080864 -2.7869711 -2.6399968 -2.78695][-6.2264404 -5.9343429 -4.7573714 -3.3321939 -1.9111004 -1.0132985 -0.84520721 -1.1335773 -1.7399883 -2.0429509 -1.9751546 -2.125303 -2.4524479 -2.6762938 -2.924484][-5.5498271 -4.734633 -3.20842 -1.5184727 0.063918591 0.96938658 0.99873114 0.42524385 -0.41349697 -1.1498675 -1.7332854 -2.0159218 -2.0947313 -2.4288044 -2.8074646][-4.567585 -3.5100255 -1.7695723 -0.14284515 1.2435865 1.9812207 1.8747988 1.2285109 0.50801468 -0.09470129 -0.76211262 -1.4254651 -2.0184579 -2.1777103 -2.2429173][-3.4503822 -2.9763827 -1.7236307 -0.32494164 0.93144989 1.8074765 2.2128129 1.949667 1.3854523 0.74243832 0.19028902 -0.59166765 -1.4527278 -1.8453264 -2.0182993][-2.9316177 -2.7251575 -2.220911 -1.3241949 -0.217556 0.81034184 1.4224696 1.7395077 1.7549381 1.2200418 0.60088587 -0.037699223 -0.67269254 -1.2238874 -1.5605543][-2.8542862 -3.1410022 -3.0812802 -2.6044345 -1.7288511 -0.45774913 0.40647268 1.0420589 1.1285133 0.87081337 0.38507318 -0.22977638 -0.7972858 -1.1586621 -1.3823602][-3.2945549 -3.8613653 -3.9647903 -3.568666 -2.8664696 -1.7312191 -0.62474132 -0.1403079 -0.10533524 -0.17748451 -0.40937853 -0.79574084 -1.1951468 -1.4735959 -1.6905012][-4.5528693 -4.9325957 -4.8796535 -4.4286156 -3.7264807 -2.832396 -2.0589926 -1.6447308 -1.535511 -1.768822 -1.857656 -1.9478066 -2.1062484 -2.1882586 -2.2435946][-5.6853437 -5.9750004 -5.8908825 -5.2336226 -4.4243298 -3.7486882 -3.203779 -3.0863512 -3.1493292 -3.3446999 -3.2822514 -3.1400659 -2.9407048 -2.9209576 -2.9335196][-6.4995451 -6.3818231 -6.00504 -5.5242157 -4.9649734 -4.3698072 -4.0429173 -3.9447193 -4.0499949 -4.2578797 -4.2712207 -4.0443745 -3.7049828 -3.5159566 -3.4443934][-7.1221218 -6.9073591 -6.3574576 -5.763464 -5.215239 -4.8544283 -4.6539679 -4.6405106 -4.6922836 -4.6579661 -4.415906 -4.1481485 -3.9922423 -3.7564712 -3.8408608]]...]
INFO - root - 2017-12-16 07:52:00.204537: step 52910, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.280 sec/batch; 21h:45m:59s remains)
INFO - root - 2017-12-16 07:52:03.047739: step 52920, loss = 0.29, batch loss = 0.23 (27.8 examples/sec; 0.288 sec/batch; 22h:21m:08s remains)
INFO - root - 2017-12-16 07:52:05.887733: step 52930, loss = 0.20, batch loss = 0.14 (29.2 examples/sec; 0.274 sec/batch; 21h:17m:47s remains)
INFO - root - 2017-12-16 07:52:08.723754: step 52940, loss = 0.20, batch loss = 0.14 (29.0 examples/sec; 0.276 sec/batch; 21h:25m:25s remains)
INFO - root - 2017-12-16 07:52:11.574769: step 52950, loss = 0.28, batch loss = 0.22 (26.6 examples/sec; 0.300 sec/batch; 23h:19m:58s remains)
INFO - root - 2017-12-16 07:52:14.375419: step 52960, loss = 0.24, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 21h:15m:03s remains)
INFO - root - 2017-12-16 07:52:17.218243: step 52970, loss = 0.29, batch loss = 0.24 (28.5 examples/sec; 0.280 sec/batch; 21h:46m:15s remains)
INFO - root - 2017-12-16 07:52:20.072740: step 52980, loss = 0.36, batch loss = 0.30 (28.5 examples/sec; 0.281 sec/batch; 21h:48m:41s remains)
INFO - root - 2017-12-16 07:52:22.926897: step 52990, loss = 0.39, batch loss = 0.33 (27.3 examples/sec; 0.293 sec/batch; 22h:46m:33s remains)
INFO - root - 2017-12-16 07:52:25.799806: step 53000, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 21h:51m:54s remains)
2017-12-16 07:52:26.325868: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9696989 -3.7562008 -3.5867407 -3.4050837 -3.3010404 -3.1947856 -3.3144741 -3.378746 -3.3393984 -3.096632 -2.6708822 -2.2658846 -1.9964929 -2.077122 -2.1313028][-4.0042725 -3.8672967 -3.7020361 -3.4604185 -3.336956 -3.3389831 -3.4923563 -3.3564978 -3.123275 -3.0007677 -2.7751794 -2.5353246 -2.4589751 -2.595192 -2.6315031][-3.8880904 -3.7170148 -3.6516867 -3.5739861 -3.5042558 -3.432874 -3.4022117 -3.2269201 -2.968127 -2.8102484 -2.6681318 -2.6998167 -2.8366184 -3.040272 -3.1858342][-3.4117498 -3.4345183 -3.5618103 -3.6344488 -3.6956744 -3.6580009 -3.5601463 -3.3525972 -3.0626419 -2.9187479 -2.8673368 -3.0100226 -3.2321291 -3.4538772 -3.6140561][-2.6982985 -2.7578826 -2.9505038 -3.0964026 -3.1598563 -3.0845556 -2.9224176 -2.752337 -2.4888811 -2.4201455 -2.5085106 -2.7039344 -2.955235 -3.1843543 -3.2733333][-1.9240828 -2.2074471 -2.5843549 -2.6795397 -2.5079665 -2.211256 -1.8365695 -1.6419623 -1.4432142 -1.4530113 -1.6071537 -1.8270457 -2.0814831 -2.3124373 -2.434804][-1.3824964 -1.7280703 -2.0820611 -2.2532818 -2.0484509 -1.6140056 -1.0494583 -0.69375205 -0.50042295 -0.50022554 -0.68381 -0.96218204 -1.2625759 -1.4286568 -1.4470758][-0.73921323 -1.0167441 -1.2875524 -1.3586488 -1.1508121 -0.70362425 -0.22409678 0.093908787 0.26975965 0.18310118 -0.13142729 -0.5210197 -0.82903624 -1.0635569 -1.0798647][-0.45810628 -0.49714231 -0.58430719 -0.45638204 -0.12628841 0.23757219 0.56950855 0.77474594 0.84465837 0.70364428 0.28528976 -0.33240175 -0.8809824 -1.1925895 -1.2853251][-0.55627346 -0.39255238 -0.25706148 -0.032926559 0.23323536 0.51854467 0.73176527 0.8285408 0.67389917 0.25158167 -0.43658638 -1.0501223 -1.5324223 -1.8858581 -1.9310985][-1.0426862 -0.76089954 -0.51218772 -0.13011742 0.20705938 0.43414736 0.40841293 0.18215132 -0.26081896 -0.71502519 -1.4310164 -2.1255934 -2.481158 -2.6396227 -2.547611][-2.0712461 -1.4921439 -1.0410256 -0.54875779 -0.19570637 -0.13793993 -0.40807009 -0.62439656 -1.0066359 -1.7346747 -2.6822505 -3.197084 -3.3674746 -3.4152174 -3.1997628][-3.1282244 -2.3261545 -1.7237854 -1.2782125 -0.94221878 -0.99796295 -1.311996 -1.8262401 -2.3663454 -2.7355642 -3.3161271 -3.744978 -3.7434871 -3.5319691 -3.291877][-4.0498729 -3.1207387 -2.42213 -2.0007195 -1.8194773 -2.1714554 -2.7247779 -3.0631728 -3.305264 -3.5478482 -3.8467927 -3.850831 -3.5502372 -3.1607256 -2.8242998][-4.827539 -3.9432058 -3.3245745 -2.9270585 -2.8007665 -3.0381966 -3.5226438 -3.9423654 -4.2074614 -4.1575742 -4.0491958 -3.8746886 -3.4246931 -2.909503 -2.4614296]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 07:52:29.177684: step 53010, loss = 0.27, batch loss = 0.22 (28.2 examples/sec; 0.284 sec/batch; 22h:01m:11s remains)
INFO - root - 2017-12-16 07:52:32.029493: step 53020, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.283 sec/batch; 21h:56m:49s remains)
INFO - root - 2017-12-16 07:52:34.882147: step 53030, loss = 0.31, batch loss = 0.25 (26.0 examples/sec; 0.308 sec/batch; 23h:52m:49s remains)
INFO - root - 2017-12-16 07:52:37.710391: step 53040, loss = 0.23, batch loss = 0.17 (28.1 examples/sec; 0.285 sec/batch; 22h:06m:11s remains)
INFO - root - 2017-12-16 07:52:40.584065: step 53050, loss = 0.31, batch loss = 0.25 (28.2 examples/sec; 0.284 sec/batch; 22h:00m:36s remains)
INFO - root - 2017-12-16 07:52:43.437038: step 53060, loss = 0.22, batch loss = 0.16 (28.1 examples/sec; 0.284 sec/batch; 22h:04m:49s remains)
INFO - root - 2017-12-16 07:52:46.261444: step 53070, loss = 0.22, batch loss = 0.16 (26.2 examples/sec; 0.305 sec/batch; 23h:41m:12s remains)
INFO - root - 2017-12-16 07:52:49.125238: step 53080, loss = 0.23, batch loss = 0.18 (27.6 examples/sec; 0.290 sec/batch; 22h:28m:19s remains)
INFO - root - 2017-12-16 07:52:51.976845: step 53090, loss = 0.24, batch loss = 0.18 (29.0 examples/sec; 0.276 sec/batch; 21h:23m:23s remains)
INFO - root - 2017-12-16 07:52:54.861846: step 53100, loss = 0.26, batch loss = 0.20 (28.8 examples/sec; 0.277 sec/batch; 21h:31m:50s remains)
2017-12-16 07:52:55.404697: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.305665 -4.1294322 -3.9959929 -4.4181733 -5.11291 -5.8990717 -6.5921793 -6.7281971 -6.7205477 -6.242897 -5.7410583 -5.413166 -4.8251114 -4.1196761 -3.4269745][-4.8984966 -5.3393712 -5.76018 -5.9557757 -6.7663813 -7.0142441 -7.055171 -6.9474683 -6.5061812 -6.0996108 -5.9038439 -5.4692655 -4.7378712 -4.1339579 -3.6092587][-5.4929667 -6.0136576 -6.4910812 -6.7035971 -6.6835833 -6.3508773 -6.3497 -6.0157065 -5.637104 -5.3238149 -5.0076137 -5.0391164 -5.0407176 -4.6216183 -4.0290594][-5.9229822 -6.8109069 -7.1143179 -6.7850657 -6.16873 -5.1054153 -4.22917 -3.594646 -3.3399398 -3.8529205 -4.5502253 -4.8043528 -4.8720326 -4.9531474 -4.7025924][-5.8584614 -6.7862096 -6.8482103 -6.0181103 -4.4630609 -2.6490102 -1.3442547 -0.41510153 -0.494071 -1.6493025 -2.9171362 -4.2987485 -5.2392445 -5.2707486 -4.752409][-4.4668 -5.2215734 -5.3096638 -4.1921353 -1.7758286 0.78764486 2.689714 3.555984 2.8197556 0.94118166 -1.5678937 -3.7157004 -4.9640403 -5.4761553 -5.1552429][-3.2005062 -3.8517048 -3.505959 -1.9227777 0.44074392 3.1324897 5.3228054 6.0904884 5.0621872 2.5277305 -0.46615863 -3.1527054 -4.9343157 -5.3992887 -4.8899302][-2.5541906 -3.2893558 -3.0986371 -1.3562918 1.372642 4.22867 6.19228 6.3990993 5.1481943 2.513196 -0.7492702 -3.4317493 -4.8955512 -5.2380924 -4.7791653][-2.4780536 -2.9853153 -2.492888 -1.31002 0.39402723 2.8839917 4.8662167 5.13488 3.7322178 0.96274614 -2.02906 -4.3511434 -5.622385 -5.6730227 -4.9858618][-2.4968927 -3.1643803 -2.8309269 -1.7903795 -0.24191904 1.231636 2.0429902 2.2755136 1.1617155 -1.2479661 -3.7860043 -5.7430129 -6.4639344 -6.2477579 -5.4776297][-3.2679133 -3.5545971 -3.3148458 -3.0566077 -2.5855055 -1.359602 -0.19003725 -0.34852076 -1.742918 -3.3912904 -5.114305 -6.61294 -7.1235914 -6.6068439 -5.6841993][-4.3358831 -4.5192833 -4.5424 -4.0764709 -3.4840248 -3.218797 -2.9798827 -3.0003726 -3.6172767 -4.90516 -6.2813225 -7.0359774 -7.0498757 -6.4649448 -5.6323056][-5.284081 -5.3314681 -5.372335 -5.2490778 -5.1044693 -4.5123067 -4.0781894 -4.4982009 -5.2026486 -5.8779559 -6.6778703 -7.1204357 -6.9770441 -6.3120065 -5.5465288][-6.0568218 -5.9519444 -5.8579926 -5.7328939 -5.5705013 -5.1495628 -5.027205 -4.9716272 -5.2536554 -5.9209394 -6.4177775 -6.457468 -6.1985912 -5.6133194 -5.0288339][-6.7068176 -6.5596375 -6.3793745 -6.0927868 -5.761127 -5.3622971 -5.1339192 -4.9153967 -4.9995193 -5.3561616 -5.6268525 -5.5408626 -5.1825871 -4.6200671 -4.1295104]]...]
INFO - root - 2017-12-16 07:52:58.287958: step 53110, loss = 0.24, batch loss = 0.19 (28.5 examples/sec; 0.281 sec/batch; 21h:49m:08s remains)
INFO - root - 2017-12-16 07:53:01.155772: step 53120, loss = 0.28, batch loss = 0.22 (29.4 examples/sec; 0.272 sec/batch; 21h:05m:34s remains)
INFO - root - 2017-12-16 07:53:04.038542: step 53130, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 22h:10m:53s remains)
INFO - root - 2017-12-16 07:53:06.963606: step 53140, loss = 0.32, batch loss = 0.26 (29.1 examples/sec; 0.275 sec/batch; 21h:21m:13s remains)
INFO - root - 2017-12-16 07:53:09.836588: step 53150, loss = 0.30, batch loss = 0.24 (28.9 examples/sec; 0.276 sec/batch; 21h:27m:12s remains)
INFO - root - 2017-12-16 07:53:12.690616: step 53160, loss = 0.29, batch loss = 0.23 (28.6 examples/sec; 0.279 sec/batch; 21h:41m:10s remains)
INFO - root - 2017-12-16 07:53:15.534140: step 53170, loss = 0.29, batch loss = 0.23 (28.6 examples/sec; 0.280 sec/batch; 21h:41m:28s remains)
INFO - root - 2017-12-16 07:53:18.342406: step 53180, loss = 0.30, batch loss = 0.24 (29.0 examples/sec; 0.276 sec/batch; 21h:25m:07s remains)
INFO - root - 2017-12-16 07:53:21.206454: step 53190, loss = 0.22, batch loss = 0.16 (28.7 examples/sec; 0.279 sec/batch; 21h:38m:57s remains)
INFO - root - 2017-12-16 07:53:24.027603: step 53200, loss = 0.24, batch loss = 0.18 (28.6 examples/sec; 0.280 sec/batch; 21h:43m:24s remains)
2017-12-16 07:53:24.582943: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4490204 -5.5350657 -4.6433053 -4.40583 -4.5387182 -5.0797939 -5.6648607 -5.9642696 -6.1600466 -6.2117996 -6.3615994 -6.3389482 -6.2176733 -6.184772 -6.0346966][-6.4511232 -5.53942 -4.5387616 -3.9876847 -3.8221006 -4.421649 -5.1274714 -5.4862905 -5.6362553 -5.4948215 -5.4453936 -5.2685423 -5.1797132 -4.9003639 -4.5253177][-5.99347 -4.9779787 -3.967648 -3.1961207 -2.7327628 -3.2787156 -4.194664 -4.6755462 -4.6880331 -4.6013556 -4.5766993 -4.3207779 -4.1626897 -3.8153448 -3.3790352][-5.6928797 -4.1724553 -2.8137212 -2.0305052 -1.7821913 -2.1598933 -2.8968344 -3.6264074 -3.9066992 -3.5841188 -3.234858 -3.304234 -3.4733169 -3.0574079 -2.2752168][-5.4644146 -3.587574 -1.7532709 -0.56235313 -0.091947556 -0.62637138 -1.4735091 -2.0663476 -2.3613193 -2.4560351 -2.7078176 -2.767545 -2.7602677 -2.5609317 -2.0630603][-5.1182203 -3.01231 -0.84041238 0.67615986 1.3254013 0.88962412 0.13473892 -0.55984282 -1.0284142 -1.342051 -1.865416 -2.4251065 -3.0093167 -2.7603564 -2.087723][-4.79765 -2.6393366 -0.61017156 0.91203403 1.6288066 1.5544381 1.0136323 0.29015303 -0.1950388 -0.58364367 -1.1454129 -2.0217106 -3.014874 -3.1947443 -2.827981][-4.7467356 -2.495008 -0.28078794 1.1277518 1.8342528 1.7628331 1.1704316 0.41368151 -0.045762062 -0.56054211 -1.2226541 -2.2056096 -3.2520728 -3.768877 -3.7212114][-4.3666592 -2.421684 -0.63257527 0.78372431 1.3578677 1.1719599 0.72942972 0.0038113594 -0.65768218 -1.2476301 -2.0095026 -3.1212893 -4.17132 -4.5059228 -4.1679888][-4.1634746 -2.4141512 -0.76833558 0.12094402 0.46172857 0.26407528 -0.36981153 -0.96974492 -1.53226 -2.217701 -3.1538973 -4.2605934 -5.4265413 -5.7320514 -5.3329892][-4.0587635 -2.614152 -1.535007 -1.0699768 -1.3095136 -1.5392675 -1.7243528 -2.1568549 -2.7093992 -3.3469698 -4.2625842 -5.2793632 -6.2926321 -6.58499 -6.199646][-4.9931092 -3.502347 -2.5825603 -2.4479361 -2.9139967 -3.3866241 -3.8158076 -3.8929691 -3.9702945 -4.3328648 -4.9899163 -5.9141703 -6.8304005 -7.0735173 -6.5634389][-5.9035325 -4.4992323 -3.622577 -3.5596719 -4.140151 -4.7551718 -4.9562922 -4.9211435 -4.8505259 -4.9613118 -5.3246384 -5.9791985 -6.5724592 -6.7712226 -6.4017496][-7.3405466 -5.7362866 -4.7036028 -4.4252949 -4.7504745 -5.3676739 -5.5267067 -5.2826867 -4.9223294 -4.7713695 -4.7359791 -5.050168 -5.5757623 -5.7163978 -5.4774351][-8.3279076 -6.7891579 -5.580102 -5.1507549 -5.3060284 -5.65499 -5.8318915 -5.6143208 -5.1622105 -4.7796803 -4.6097584 -4.5863638 -4.7192221 -4.6736937 -4.510129]]...]
INFO - root - 2017-12-16 07:53:27.398859: step 53210, loss = 0.43, batch loss = 0.37 (28.8 examples/sec; 0.278 sec/batch; 21h:32m:18s remains)
INFO - root - 2017-12-16 07:53:30.186163: step 53220, loss = 0.23, batch loss = 0.17 (27.7 examples/sec; 0.288 sec/batch; 22h:22m:47s remains)
INFO - root - 2017-12-16 07:53:33.000286: step 53230, loss = 0.31, batch loss = 0.25 (27.0 examples/sec; 0.296 sec/batch; 22h:59m:24s remains)
INFO - root - 2017-12-16 07:53:35.843511: step 53240, loss = 0.45, batch loss = 0.39 (28.0 examples/sec; 0.286 sec/batch; 22h:10m:49s remains)
INFO - root - 2017-12-16 07:53:38.647835: step 53250, loss = 0.28, batch loss = 0.22 (28.9 examples/sec; 0.277 sec/batch; 21h:30m:19s remains)
INFO - root - 2017-12-16 07:53:41.455419: step 53260, loss = 0.25, batch loss = 0.19 (29.1 examples/sec; 0.275 sec/batch; 21h:19m:16s remains)
INFO - root - 2017-12-16 07:53:44.252217: step 53270, loss = 0.28, batch loss = 0.22 (28.0 examples/sec; 0.286 sec/batch; 22h:11m:29s remains)
INFO - root - 2017-12-16 07:53:47.134371: step 53280, loss = 0.39, batch loss = 0.33 (27.0 examples/sec; 0.296 sec/batch; 22h:58m:40s remains)
INFO - root - 2017-12-16 07:53:50.018941: step 53290, loss = 0.33, batch loss = 0.27 (28.0 examples/sec; 0.286 sec/batch; 22h:08m:55s remains)
INFO - root - 2017-12-16 07:53:52.878126: step 53300, loss = 0.25, batch loss = 0.20 (27.5 examples/sec; 0.291 sec/batch; 22h:35m:12s remains)
2017-12-16 07:53:53.359425: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8346543 -6.0327148 -6.0438461 -6.1733985 -6.2092805 -6.0602164 -6.0849562 -6.5883904 -6.9028072 -6.8401518 -6.4689913 -6.0472722 -5.4549356 -4.69825 -4.3951426][-6.2455792 -6.7239447 -6.8025432 -6.6287074 -6.3133974 -6.1174383 -6.0715189 -6.08202 -6.3530135 -6.6783867 -6.7923107 -6.401001 -5.7905116 -5.1729527 -4.662878][-6.2879324 -6.8175726 -6.9271474 -6.5412512 -5.8597841 -5.1900282 -4.7721639 -4.7175627 -4.9982681 -5.3277826 -5.7862506 -5.9849591 -5.6752424 -5.1308346 -4.63008][-6.6161289 -7.2348666 -7.3089285 -6.5684166 -5.5076876 -4.21403 -3.1581867 -2.7334852 -2.7811258 -3.4427345 -4.1375637 -4.7308846 -4.9495444 -4.7435384 -4.4791245][-6.3796153 -7.0415635 -7.0523887 -6.0876589 -4.4982986 -2.5318961 -0.81769109 -0.055575848 -0.15138531 -1.0268137 -2.1500983 -3.2776504 -3.9414885 -3.9160004 -3.8110609][-5.7527723 -6.1150637 -5.9059634 -4.7535124 -2.5701528 0.10762215 2.3373156 3.5486646 3.2889724 1.923996 0.045013428 -1.6971824 -2.7979286 -3.2380462 -3.1870446][-5.1656752 -5.4628391 -4.8096313 -3.2719507 -0.92936563 2.1072702 4.9920683 6.4169025 5.7528706 3.9123297 1.679534 -0.47645664 -2.0651484 -2.7050381 -2.7603636][-4.738409 -5.0362377 -4.5316405 -2.6128545 0.20531702 3.3346334 6.1066389 7.390996 6.685524 4.44606 1.7714238 -0.36091328 -1.7870452 -2.4295387 -2.4450903][-4.2297773 -4.6336451 -4.0932803 -2.5956712 -0.48084188 2.3082504 4.7539349 5.928256 5.3198986 3.270833 0.76536894 -1.2534626 -2.3285246 -2.5722768 -2.292434][-3.9939735 -4.7899027 -5.0102706 -3.9624426 -2.0491509 -0.10418415 1.1631856 1.943706 1.5617757 0.23362589 -1.3174026 -2.5637865 -3.066721 -2.9745035 -2.498044][-4.25858 -5.10995 -5.5623789 -5.5459027 -4.7127619 -3.2124739 -2.1792459 -1.9366324 -2.5112915 -3.2229314 -3.6949444 -3.9309309 -3.7977507 -3.3200324 -2.7294366][-4.7712393 -5.743083 -6.4904318 -6.6335716 -6.3159323 -5.86884 -5.6090612 -5.4447293 -5.3621583 -5.401382 -5.2312512 -4.7853527 -4.0201564 -3.4052334 -3.0025239][-5.2969794 -6.6028824 -7.6770606 -8.2940931 -8.4491444 -7.9432268 -7.4136086 -7.1948032 -7.1375694 -6.727901 -5.7869034 -4.8471851 -3.9898114 -3.4278967 -3.0240126][-5.6188717 -6.8083882 -7.626029 -8.1460218 -8.4131165 -8.505991 -8.3456631 -7.7766771 -7.1835527 -6.7335572 -6.0653515 -5.1993723 -4.2683897 -3.5103698 -3.0842669][-5.3583994 -6.3086429 -6.9380369 -7.1751308 -7.2019138 -7.0675182 -6.94486 -6.7416515 -6.3303847 -5.9262214 -5.442667 -4.7550521 -3.9775651 -3.3611245 -3.0196953]]...]
INFO - root - 2017-12-16 07:53:56.206472: step 53310, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.279 sec/batch; 21h:39m:20s remains)
INFO - root - 2017-12-16 07:53:59.023911: step 53320, loss = 0.24, batch loss = 0.18 (29.1 examples/sec; 0.275 sec/batch; 21h:21m:16s remains)
INFO - root - 2017-12-16 07:54:01.915721: step 53330, loss = 0.36, batch loss = 0.30 (27.5 examples/sec; 0.290 sec/batch; 22h:31m:19s remains)
INFO - root - 2017-12-16 07:54:04.741972: step 53340, loss = 0.32, batch loss = 0.27 (27.1 examples/sec; 0.295 sec/batch; 22h:53m:53s remains)
INFO - root - 2017-12-16 07:54:07.657837: step 53350, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.286 sec/batch; 22h:10m:34s remains)
INFO - root - 2017-12-16 07:54:10.535132: step 53360, loss = 0.30, batch loss = 0.24 (28.6 examples/sec; 0.280 sec/batch; 21h:42m:05s remains)
INFO - root - 2017-12-16 07:54:13.342661: step 53370, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.285 sec/batch; 22h:07m:25s remains)
INFO - root - 2017-12-16 07:54:16.176709: step 53380, loss = 0.34, batch loss = 0.29 (26.6 examples/sec; 0.301 sec/batch; 23h:18m:45s remains)
INFO - root - 2017-12-16 07:54:18.994478: step 53390, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 22h:01m:05s remains)
INFO - root - 2017-12-16 07:54:21.803882: step 53400, loss = 0.20, batch loss = 0.15 (28.2 examples/sec; 0.284 sec/batch; 22h:00m:03s remains)
2017-12-16 07:54:22.330336: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.496912 -4.8758178 -5.1708937 -5.2741961 -5.3693948 -5.2769842 -5.0558219 -4.8434834 -4.5983872 -4.1235089 -3.7160604 -3.390882 -3.1890321 -2.9895995 -2.8158138][-4.6899076 -5.2642751 -5.6452575 -5.7150588 -5.7286472 -5.7292142 -5.7500854 -5.52254 -5.2240987 -4.8474317 -4.5960193 -4.1044168 -3.7275074 -3.538265 -3.2430089][-4.9768624 -5.4800382 -5.8040638 -5.9509306 -5.9031458 -5.7855148 -5.655447 -5.5703449 -5.3840485 -5.1906981 -4.9804935 -4.8044987 -4.6986942 -4.3057156 -3.8797462][-4.5728126 -4.9264317 -4.96435 -4.91643 -4.7328253 -4.6563668 -4.5802612 -4.616673 -4.5303192 -4.6434264 -4.86688 -4.950695 -5.1760464 -4.907023 -4.3214664][-3.4161639 -3.2862818 -2.8985443 -2.4787998 -2.0571578 -1.9014549 -1.7982678 -1.9817066 -2.3817298 -2.9215021 -3.8222439 -4.5625877 -4.9140062 -4.850317 -4.2947731][-2.7446566 -2.109113 -1.271729 -0.36885118 0.61989641 1.2764349 1.6855178 1.4167628 0.81012106 -0.2572403 -1.6302083 -3.2950914 -4.2761912 -4.3831906 -3.7811093][-2.2201507 -1.4788699 -0.47577786 0.73149347 2.13983 3.385293 4.5099859 4.5656624 3.8721046 2.3578224 0.39263868 -1.612787 -2.8986301 -3.3812652 -3.0121067][-1.8430252 -1.0903733 -0.33647871 0.83388329 2.2792535 3.8766212 5.2642727 5.6999979 5.3018637 3.9512129 1.8220549 -0.47265506 -2.03468 -2.6700835 -2.5493007][-1.8890882 -1.353653 -0.77955151 0.0029430389 1.0215187 2.6682224 4.2205858 4.9158421 4.81818 3.622613 1.7729754 -0.35905743 -1.9825177 -2.7189231 -2.639081][-2.3605201 -2.0424578 -2.0345628 -1.6012383 -0.74563479 0.38970709 1.5861177 2.4224486 2.5653429 1.6892943 0.22435808 -1.5157232 -2.8491688 -3.50177 -3.3615725][-3.1115065 -2.9871089 -3.1655347 -3.2332323 -3.0696905 -2.2745781 -1.4232442 -0.79904485 -0.62326431 -0.94770503 -1.8650446 -2.949295 -3.716433 -3.9224472 -3.6723242][-3.6909246 -3.6417587 -3.8556354 -4.1191406 -4.4229107 -4.2572613 -3.898159 -3.3919425 -3.1842189 -3.1584098 -3.4617167 -4.0264316 -4.4383678 -4.2836089 -3.8109961][-3.9225645 -3.7414489 -3.8806708 -4.0424857 -4.517314 -4.7994604 -4.89449 -4.7117648 -4.545249 -4.4887071 -4.4341068 -4.3074718 -4.1661129 -3.8859098 -3.3867497][-4.4040027 -3.8575077 -3.519393 -3.5262904 -3.7990086 -4.2711573 -4.6066494 -4.769177 -4.6677289 -4.4271674 -4.1381044 -3.876977 -3.4081593 -2.8634381 -2.4584742][-4.7580528 -4.05048 -3.3493338 -3.1013951 -3.1221976 -3.4143226 -3.6891222 -4.045218 -4.1364622 -3.9505367 -3.5876248 -3.2595954 -2.8833003 -2.4511142 -2.0809646]]...]
INFO - root - 2017-12-16 07:54:25.211676: step 53410, loss = 0.27, batch loss = 0.21 (27.8 examples/sec; 0.287 sec/batch; 22h:17m:16s remains)
INFO - root - 2017-12-16 07:54:28.064825: step 53420, loss = 0.27, batch loss = 0.21 (27.7 examples/sec; 0.289 sec/batch; 22h:22m:44s remains)
INFO - root - 2017-12-16 07:54:30.918867: step 53430, loss = 0.24, batch loss = 0.18 (29.6 examples/sec; 0.270 sec/batch; 20h:57m:58s remains)
INFO - root - 2017-12-16 07:54:33.744588: step 53440, loss = 0.32, batch loss = 0.26 (27.6 examples/sec; 0.290 sec/batch; 22h:27m:28s remains)
INFO - root - 2017-12-16 07:54:36.600372: step 53450, loss = 0.22, batch loss = 0.16 (28.1 examples/sec; 0.284 sec/batch; 22h:03m:09s remains)
INFO - root - 2017-12-16 07:54:39.439802: step 53460, loss = 0.40, batch loss = 0.34 (27.6 examples/sec; 0.290 sec/batch; 22h:27m:15s remains)
INFO - root - 2017-12-16 07:54:42.288715: step 53470, loss = 0.34, batch loss = 0.28 (27.8 examples/sec; 0.288 sec/batch; 22h:18m:25s remains)
INFO - root - 2017-12-16 07:54:45.140622: step 53480, loss = 0.22, batch loss = 0.16 (28.4 examples/sec; 0.282 sec/batch; 21h:50m:28s remains)
INFO - root - 2017-12-16 07:54:48.019273: step 53490, loss = 0.27, batch loss = 0.21 (27.9 examples/sec; 0.286 sec/batch; 22h:11m:59s remains)
INFO - root - 2017-12-16 07:54:50.877709: step 53500, loss = 0.24, batch loss = 0.18 (27.6 examples/sec; 0.290 sec/batch; 22h:28m:52s remains)
2017-12-16 07:54:51.379221: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1831717 -2.4421668 -2.817925 -3.3146975 -3.882669 -4.3349724 -4.4637871 -4.2476721 -3.9239316 -3.7177262 -3.5382438 -3.4789317 -3.5199225 -3.4596694 -3.5611429][-2.2530158 -2.4242072 -2.7529581 -3.1931224 -3.7156155 -4.1870818 -4.2286186 -3.9609497 -3.5672264 -3.5089791 -3.4894764 -3.54862 -3.6686313 -3.6897287 -3.7117405][-2.2982574 -2.4023042 -2.6848454 -2.9771485 -3.2828517 -3.5637693 -3.576364 -3.3539703 -3.0297928 -3.1323507 -3.3413553 -3.6297715 -3.8519509 -3.9083421 -3.9841352][-2.5760477 -2.6985302 -2.9585814 -3.1552486 -3.2257285 -3.1899364 -3.0309982 -2.5672984 -2.0924299 -2.3971441 -2.9308095 -3.5696921 -4.0275803 -4.0385895 -3.6949425][-2.80364 -2.8893631 -3.0899453 -3.0666502 -2.9501126 -2.614217 -1.9551907 -1.5164676 -1.3394246 -1.6408553 -2.3435776 -3.3080332 -3.9749901 -4.0492668 -3.5669122][-2.9891567 -2.9430451 -2.8461547 -2.6478109 -2.001121 -1.1580861 -0.25174332 0.18761063 0.13244104 -0.76229715 -1.9510977 -3.140928 -3.9705453 -3.9652658 -3.3354974][-3.2657478 -3.2545555 -2.7686658 -1.9336491 -0.87953472 0.39556694 1.6647911 2.1845145 1.8980155 0.7521143 -0.8218894 -2.6488914 -3.8231506 -3.8165245 -3.3018136][-3.8474073 -3.8771071 -3.4350526 -2.1532321 -0.53403521 1.1271381 2.5303659 2.9680271 2.610055 1.4715195 -0.23909092 -2.0086708 -3.2651172 -3.4608409 -2.9249449][-5.0401216 -4.7422109 -3.7061543 -2.4788308 -0.86718869 0.93213654 2.2809677 2.7178779 2.1857166 1.0668063 -0.5644784 -2.5012035 -3.8284612 -3.9778724 -3.4543686][-5.42722 -5.0629625 -4.2650809 -2.9502683 -1.3144586 0.27989674 1.3860469 1.7825398 1.2917209 0.15250254 -1.3324246 -2.9748826 -4.2117391 -4.5687 -4.2792983][-5.9928989 -5.6639371 -4.7805109 -3.6511736 -2.5709748 -1.2731793 -0.1686058 0.22913837 -0.014364719 -0.74681687 -1.9628017 -3.3349864 -4.2534695 -4.5463266 -4.3618684][-5.9152966 -5.4095554 -4.8978004 -4.3483744 -3.6840153 -2.7368631 -1.9151299 -1.495769 -1.5977764 -2.095789 -2.851773 -3.6612525 -4.3586259 -4.7334471 -4.5690885][-5.0792117 -4.9421015 -4.66843 -4.4704208 -4.3956151 -4.0586362 -3.629343 -3.2368994 -3.096597 -3.1551433 -3.4181604 -3.9771307 -4.4678879 -4.6434169 -4.5750232][-4.170733 -3.9775782 -3.98904 -4.161531 -4.4705591 -4.52933 -4.2950397 -3.9873583 -3.6141915 -3.3616724 -3.4185643 -3.5626209 -3.8156126 -4.0765481 -4.150517][-2.8265595 -2.6737125 -2.848069 -3.3084741 -3.79016 -4.1382852 -4.023665 -3.65055 -3.2108381 -2.8360682 -2.7201929 -2.9333711 -3.4140475 -3.7266991 -3.8025346]]...]
INFO - root - 2017-12-16 07:54:54.195511: step 53510, loss = 0.23, batch loss = 0.17 (27.4 examples/sec; 0.292 sec/batch; 22h:36m:56s remains)
INFO - root - 2017-12-16 07:54:57.041889: step 53520, loss = 0.42, batch loss = 0.37 (29.2 examples/sec; 0.274 sec/batch; 21h:12m:58s remains)
INFO - root - 2017-12-16 07:54:59.895544: step 53530, loss = 0.30, batch loss = 0.24 (27.2 examples/sec; 0.294 sec/batch; 22h:47m:11s remains)
INFO - root - 2017-12-16 07:55:02.742662: step 53540, loss = 0.37, batch loss = 0.32 (29.2 examples/sec; 0.274 sec/batch; 21h:13m:58s remains)
INFO - root - 2017-12-16 07:55:05.566356: step 53550, loss = 0.25, batch loss = 0.19 (28.3 examples/sec; 0.283 sec/batch; 21h:54m:17s remains)
INFO - root - 2017-12-16 07:55:08.444264: step 53560, loss = 0.22, batch loss = 0.16 (27.4 examples/sec; 0.292 sec/batch; 22h:39m:17s remains)
INFO - root - 2017-12-16 07:55:11.293563: step 53570, loss = 0.28, batch loss = 0.22 (26.4 examples/sec; 0.303 sec/batch; 23h:29m:13s remains)
INFO - root - 2017-12-16 07:55:14.076862: step 53580, loss = 0.21, batch loss = 0.15 (29.3 examples/sec; 0.273 sec/batch; 21h:10m:21s remains)
INFO - root - 2017-12-16 07:55:16.944619: step 53590, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.283 sec/batch; 21h:56m:56s remains)
INFO - root - 2017-12-16 07:55:19.781355: step 53600, loss = 0.23, batch loss = 0.17 (28.6 examples/sec; 0.280 sec/batch; 21h:39m:33s remains)
2017-12-16 07:55:20.301266: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.603652 -6.18442 -6.7418337 -7.1049151 -7.2411919 -7.2190557 -7.160531 -7.1167231 -7.1919169 -7.2582827 -7.0202847 -6.7972665 -6.334393 -5.6771636 -4.9780927][-6.8811007 -7.4008856 -7.6954365 -7.7990961 -7.8571734 -8.0161953 -8.1759434 -8.3600254 -8.53409 -8.7469463 -8.7481794 -8.4895 -7.8751531 -7.1501951 -6.3277946][-7.3968344 -7.7272058 -7.9022884 -7.6368141 -7.3929358 -7.3924122 -7.4813557 -8.278264 -9.0296516 -9.5725822 -9.742898 -9.5439177 -9.2100239 -8.6660166 -7.9825][-7.4242706 -7.4278984 -6.9665194 -6.4781713 -5.8859425 -5.5124006 -5.4823427 -6.1488466 -7.1340866 -8.715374 -9.6192341 -9.7930508 -9.6563015 -9.4602528 -9.2207174][-6.858366 -6.0445485 -4.750741 -3.3912568 -2.1538079 -1.52547 -1.63114 -2.305573 -3.4521618 -5.2329397 -6.9370179 -8.4600658 -9.396903 -9.8232441 -9.9742527][-5.730751 -4.5483537 -2.7223344 -0.41681576 1.7845802 3.0622764 3.4464874 2.5184617 0.83838844 -1.2893157 -3.6581275 -5.9108481 -7.8812695 -9.2887554 -9.9046936][-5.3181372 -4.0601358 -1.7794685 1.0087156 3.7605362 5.4864006 6.1257143 5.4831333 3.9830093 1.7784629 -0.82448173 -3.6877167 -6.1942768 -7.8486137 -8.6916733][-5.9350424 -4.45148 -2.5185866 0.35570383 3.6665602 6.0591526 7.11767 6.68472 5.2903719 2.9143491 0.23420572 -2.4948652 -4.9413252 -6.7351217 -7.5258551][-6.816287 -5.5992684 -3.7591763 -0.99362135 1.8189807 4.1684332 5.882885 5.98195 5.0274534 2.9016271 0.43314075 -2.3478293 -4.8520875 -6.3726468 -6.6152096][-7.7853661 -7.1134148 -5.7620373 -3.8295903 -1.6079216 0.81841135 2.5507908 2.8760653 2.2836118 0.58110189 -1.2835402 -3.5017083 -5.4267755 -6.4768362 -6.76252][-8.53119 -8.3264618 -7.3338585 -6.0889387 -4.7231817 -3.2620728 -1.4477725 -0.57931113 -0.85958147 -2.2089672 -3.5886869 -4.9813333 -6.1728668 -6.9232483 -7.1068048][-8.467041 -8.3304968 -8.0751829 -7.2016335 -6.0117373 -5.08207 -3.9700363 -3.6534057 -3.4595919 -3.9137104 -5.0574179 -6.4900975 -7.2982945 -7.3228769 -7.2163715][-7.3511205 -7.3213363 -7.4489608 -7.0000162 -6.4872026 -5.6375284 -4.7633743 -4.8304105 -4.7810726 -5.0200949 -5.5212173 -6.3942528 -7.0907574 -7.2109013 -7.0884681][-5.8611488 -5.8809781 -5.8428631 -5.731246 -5.6529484 -5.0824833 -4.5580153 -4.7323003 -4.8696866 -4.885644 -5.0162625 -5.5630727 -6.1083546 -6.1573973 -5.9051013][-4.550292 -4.2703991 -4.0802016 -4.0057325 -3.9939947 -3.9032004 -3.8179939 -3.9124446 -3.9135802 -4.1199436 -4.2666326 -4.3294582 -4.4494748 -4.6065531 -4.7417555]]...]
INFO - root - 2017-12-16 07:55:23.144431: step 53610, loss = 0.22, batch loss = 0.16 (27.4 examples/sec; 0.292 sec/batch; 22h:38m:33s remains)
INFO - root - 2017-12-16 07:55:26.046147: step 53620, loss = 0.25, batch loss = 0.19 (25.9 examples/sec; 0.308 sec/batch; 23h:53m:27s remains)
INFO - root - 2017-12-16 07:55:28.865284: step 53630, loss = 0.25, batch loss = 0.20 (29.3 examples/sec; 0.273 sec/batch; 21h:07m:11s remains)
INFO - root - 2017-12-16 07:55:31.738675: step 53640, loss = 0.26, batch loss = 0.20 (26.4 examples/sec; 0.303 sec/batch; 23h:30m:22s remains)
INFO - root - 2017-12-16 07:55:34.547577: step 53650, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.286 sec/batch; 22h:07m:51s remains)
INFO - root - 2017-12-16 07:55:37.381949: step 53660, loss = 0.39, batch loss = 0.34 (28.3 examples/sec; 0.283 sec/batch; 21h:53m:29s remains)
INFO - root - 2017-12-16 07:55:40.230339: step 53670, loss = 0.25, batch loss = 0.20 (28.2 examples/sec; 0.283 sec/batch; 21h:56m:39s remains)
INFO - root - 2017-12-16 07:55:43.056888: step 53680, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.276 sec/batch; 21h:24m:23s remains)
INFO - root - 2017-12-16 07:55:45.866456: step 53690, loss = 0.17, batch loss = 0.12 (29.8 examples/sec; 0.268 sec/batch; 20h:47m:29s remains)
INFO - root - 2017-12-16 07:55:48.662882: step 53700, loss = 0.22, batch loss = 0.16 (28.8 examples/sec; 0.278 sec/batch; 21h:32m:28s remains)
2017-12-16 07:55:49.166381: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2685232 -4.7611551 -5.1778045 -5.2577672 -5.2463055 -5.1753216 -5.1084814 -5.3965526 -5.4646597 -5.4383535 -5.4555297 -5.2843103 -5.06957 -4.7546663 -4.59344][-3.8138108 -4.2262716 -4.702178 -4.8322563 -4.7455668 -4.8045287 -4.8732557 -4.9506021 -4.8126903 -4.8173442 -4.9433203 -4.8759689 -4.8299203 -4.5039034 -4.2245817][-3.0626836 -3.4260817 -3.8672121 -4.0783086 -4.0609508 -3.9306636 -3.7246776 -3.8378127 -3.8500352 -3.8756361 -4.0503683 -4.1762843 -4.3680282 -4.3343244 -4.3382912][-2.2189624 -2.3241467 -2.5908442 -2.6471863 -2.4397721 -2.2240765 -2.0036576 -1.9694672 -1.8879659 -2.1485715 -2.6309397 -3.2219477 -3.8763695 -4.2373967 -4.5051713][-1.2096984 -1.0910075 -1.050339 -0.99069 -0.77341986 -0.36318541 0.024968624 0.0807147 0.15888643 -0.14612293 -0.90200996 -1.8338318 -2.7202585 -3.5728936 -4.2884564][-0.72343779 -0.53209853 -0.25938797 0.10415316 0.75872469 1.3443031 1.7787862 2.0179152 2.0819864 1.761487 1.0337615 -0.17290258 -1.5127294 -2.6836638 -3.643342][-0.38094139 -0.2620101 0.078764439 0.59597206 1.4414082 2.4686141 3.401031 3.7407131 3.7541676 3.3488755 2.4356561 1.0978274 -0.35087013 -1.7873263 -3.0052881][-0.78411841 -0.68546844 -0.20829344 0.40964508 1.4098535 2.6030726 3.6068993 4.1360235 4.274291 3.7488022 2.8331804 1.4986243 0.049629211 -1.3689358 -2.4938624][-1.2793548 -1.506753 -1.3288231 -0.87115 0.12049389 1.2730889 2.3390236 2.7929697 2.8558621 2.4680567 1.6650534 0.36591864 -0.95007634 -1.8868392 -2.5078902][-2.3636882 -2.7035184 -2.8607779 -2.6064827 -1.7811854 -0.72438097 0.24040031 0.67464447 0.67741013 0.091691971 -0.6620276 -1.6765518 -2.6922369 -3.2242968 -3.4757814][-2.8061695 -3.2771606 -3.64465 -3.6451011 -3.31954 -2.5143571 -1.7088263 -1.5585806 -1.6742105 -2.3283632 -3.1683941 -3.8413923 -4.3633318 -4.6301022 -4.4710708][-2.842047 -3.222096 -3.6252 -3.8287742 -3.7641022 -3.4693825 -3.2134929 -3.1052084 -3.2894423 -4.109868 -4.8555889 -5.5456214 -6.0364966 -5.7609396 -5.1271596][-2.7727876 -2.8832736 -3.321569 -3.6153407 -3.845906 -3.9085207 -3.8714879 -4.0044837 -4.3385992 -4.9799967 -5.579761 -6.1604662 -6.4583826 -6.1231213 -5.4730878][-3.033926 -3.0064368 -3.201416 -3.4229751 -3.7178226 -3.8903883 -3.9443753 -4.1191421 -4.4671021 -4.8043542 -5.0879526 -5.488771 -5.7158852 -5.5207162 -5.0624461][-3.2637057 -3.0851207 -3.1791656 -3.4064512 -3.8187888 -4.0923619 -4.1604462 -4.2577586 -4.3783693 -4.494945 -4.7015052 -4.7170725 -4.6121163 -4.5488119 -4.3839989]]...]
INFO - root - 2017-12-16 07:55:51.976638: step 53710, loss = 0.29, batch loss = 0.23 (28.3 examples/sec; 0.283 sec/batch; 21h:54m:36s remains)
INFO - root - 2017-12-16 07:55:54.770392: step 53720, loss = 0.22, batch loss = 0.17 (27.8 examples/sec; 0.288 sec/batch; 22h:16m:40s remains)
INFO - root - 2017-12-16 07:55:57.633864: step 53730, loss = 0.27, batch loss = 0.22 (28.2 examples/sec; 0.283 sec/batch; 21h:55m:59s remains)
INFO - root - 2017-12-16 07:56:00.488171: step 53740, loss = 0.29, batch loss = 0.23 (28.0 examples/sec; 0.286 sec/batch; 22h:09m:35s remains)
INFO - root - 2017-12-16 07:56:03.382943: step 53750, loss = 0.25, batch loss = 0.19 (28.6 examples/sec; 0.280 sec/batch; 21h:40m:16s remains)
INFO - root - 2017-12-16 07:56:06.200412: step 53760, loss = 0.27, batch loss = 0.21 (28.6 examples/sec; 0.280 sec/batch; 21h:41m:13s remains)
INFO - root - 2017-12-16 07:56:09.104227: step 53770, loss = 0.24, batch loss = 0.19 (26.8 examples/sec; 0.298 sec/batch; 23h:05m:31s remains)
INFO - root - 2017-12-16 07:56:11.928312: step 53780, loss = 0.25, batch loss = 0.19 (28.5 examples/sec; 0.281 sec/batch; 21h:46m:11s remains)
INFO - root - 2017-12-16 07:56:14.806492: step 53790, loss = 0.27, batch loss = 0.21 (28.4 examples/sec; 0.282 sec/batch; 21h:49m:09s remains)
INFO - root - 2017-12-16 07:56:17.675715: step 53800, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.286 sec/batch; 22h:07m:26s remains)
2017-12-16 07:56:18.246137: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3287346 -2.7040815 -3.1687791 -3.642796 -4.0087271 -4.1760044 -4.3450217 -4.3818297 -4.2916102 -4.0950942 -3.9344769 -3.7357337 -3.505372 -3.1993752 -2.8636487][-1.0335126 -1.3499343 -1.8149967 -2.5396957 -3.3379846 -3.6644273 -3.8648562 -4.0730457 -4.4008613 -4.4742303 -4.4392176 -4.1562562 -3.7131424 -3.11139 -2.5918837][-0.30997467 -0.57086015 -1.0393989 -1.6167521 -2.237911 -2.9132271 -3.5256279 -3.7924261 -4.0427027 -4.3285179 -4.4764481 -4.30744 -3.9270964 -3.2424707 -2.4377055][0.070414066 -0.34202194 -0.75675893 -0.96388841 -1.2765822 -1.7551835 -2.4293714 -3.0264697 -3.5455995 -3.9916329 -4.1997786 -4.1025829 -3.6652474 -3.0787692 -2.4990025][0.15210533 0.019307613 -0.047923088 -0.063848972 -0.15590525 -0.25464773 -0.66284943 -1.3710577 -2.3952804 -3.1548762 -3.602263 -3.8763056 -3.7455072 -3.2198751 -2.6722832][-0.10846996 0.14020205 0.65350962 1.0767398 1.4812737 1.4928517 1.0666838 0.25675535 -0.81012154 -1.8450856 -2.7385745 -3.484071 -3.759593 -3.5481462 -2.985353][-0.7631321 -0.28800392 0.43097782 1.4281306 2.4958115 3.1101298 3.0729942 2.1799607 0.93603373 -0.41695881 -1.6148069 -2.6856341 -3.2730641 -3.39262 -3.1070185][-1.5989373 -1.0000973 -0.17414379 0.91010904 2.1806111 3.2172403 3.7566128 3.0927 1.9067149 0.43766451 -1.04809 -2.3917451 -3.2456779 -3.5214963 -3.3241305][-2.8000536 -2.3747182 -1.4959064 -0.36541939 0.95653582 2.2170324 3.0042696 2.5590148 1.5569763 0.20997429 -1.1817257 -2.5723891 -3.5220394 -3.8665271 -3.7059526][-3.850389 -3.8271537 -3.5072913 -2.429939 -0.79424667 0.64732122 1.433105 1.1672363 0.33561182 -0.99231052 -2.4156976 -3.6437614 -4.2291441 -4.2160935 -3.76761][-4.6773839 -4.9408674 -4.8172827 -4.2209182 -3.2375886 -1.8698685 -0.720979 -0.82859445 -1.5026963 -2.4975739 -3.5795827 -4.568603 -4.9636989 -4.6329203 -3.9665518][-4.8241911 -5.5365977 -5.9445486 -5.652133 -4.9053497 -3.9643 -3.0569539 -2.741334 -2.864923 -3.6669807 -4.6110754 -5.3272104 -5.4946556 -5.0650544 -4.3335013][-4.1512351 -5.1307516 -5.8890409 -5.9158907 -5.4690495 -4.771873 -3.9962366 -3.5004158 -3.3355992 -3.8741841 -4.6058807 -5.2463441 -5.4274149 -4.9616203 -4.0823107][-2.9987469 -3.8895624 -4.7766528 -5.1093659 -4.8730745 -4.1398063 -3.5212791 -3.1944323 -3.0471883 -3.4510942 -3.9517488 -4.4897876 -4.7064672 -4.2956467 -3.5163612][-1.5585515 -2.4393263 -3.460434 -4.0251803 -4.0361924 -3.5832896 -3.0524201 -2.6160893 -2.3521831 -2.6598754 -3.2160583 -3.7063746 -3.7740004 -3.3980732 -2.6865273]]...]
INFO - root - 2017-12-16 07:56:21.073348: step 53810, loss = 0.20, batch loss = 0.14 (27.5 examples/sec; 0.290 sec/batch; 22h:29m:01s remains)
INFO - root - 2017-12-16 07:56:23.899390: step 53820, loss = 0.22, batch loss = 0.16 (28.7 examples/sec; 0.279 sec/batch; 21h:33m:48s remains)
INFO - root - 2017-12-16 07:56:26.810355: step 53830, loss = 0.29, batch loss = 0.24 (25.9 examples/sec; 0.308 sec/batch; 23h:51m:59s remains)
INFO - root - 2017-12-16 07:56:29.609789: step 53840, loss = 0.21, batch loss = 0.15 (28.9 examples/sec; 0.277 sec/batch; 21h:25m:30s remains)
INFO - root - 2017-12-16 07:56:32.407375: step 53850, loss = 0.31, batch loss = 0.25 (28.1 examples/sec; 0.285 sec/batch; 22h:02m:18s remains)
INFO - root - 2017-12-16 07:56:35.219523: step 53860, loss = 0.29, batch loss = 0.23 (28.5 examples/sec; 0.281 sec/batch; 21h:44m:01s remains)
INFO - root - 2017-12-16 07:56:38.057465: step 53870, loss = 0.26, batch loss = 0.20 (27.2 examples/sec; 0.295 sec/batch; 22h:48m:00s remains)
INFO - root - 2017-12-16 07:56:40.926833: step 53880, loss = 0.25, batch loss = 0.19 (27.6 examples/sec; 0.290 sec/batch; 22h:25m:25s remains)
INFO - root - 2017-12-16 07:56:43.787015: step 53890, loss = 0.21, batch loss = 0.15 (26.7 examples/sec; 0.300 sec/batch; 23h:12m:35s remains)
INFO - root - 2017-12-16 07:56:46.597025: step 53900, loss = 0.24, batch loss = 0.18 (29.7 examples/sec; 0.269 sec/batch; 20h:49m:02s remains)
2017-12-16 07:56:47.113765: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1983271 -2.2826629 -2.4418931 -2.6941209 -3.1367867 -3.5404077 -4.1215472 -4.8829064 -5.5220895 -5.8017154 -5.81155 -5.4634552 -4.6615248 -3.824667 -3.2303567][-2.699367 -2.9097767 -3.0289426 -3.1152167 -3.2627614 -3.5734639 -4.1055713 -4.670186 -5.1618295 -5.6380153 -5.8867574 -5.7446728 -5.2155304 -4.2960563 -3.4730139][-3.4146385 -3.810364 -3.9425356 -3.7755933 -3.6056972 -3.4889951 -3.4718852 -3.8030634 -4.2654829 -4.6840239 -5.0413203 -5.2621212 -5.149889 -4.5110292 -3.8033764][-3.8625388 -4.2810578 -4.3202081 -3.9330769 -3.375973 -2.5544658 -2.0705519 -1.9734402 -2.0880423 -2.562623 -3.3971298 -4.2695127 -4.5861616 -4.2568994 -3.7778525][-4.1981997 -4.4258704 -4.1445646 -3.3298411 -2.2445517 -0.84158564 0.41024446 0.86582422 0.58945704 -0.03841877 -1.1503417 -2.7306807 -3.8946004 -4.1473827 -3.9299498][-4.1802158 -4.1621165 -3.5149972 -2.4100461 -0.77770233 1.1664495 2.7838469 3.4961939 3.2043304 2.0310507 0.2735343 -1.7751136 -3.3531964 -3.8947499 -3.8092737][-3.4018195 -3.1786118 -2.1384256 -0.7519908 0.86378813 2.7100177 4.4093571 5.01742 4.533555 3.2480721 1.1989832 -1.3062496 -3.1412411 -3.8729031 -3.8378606][-2.8664863 -2.3191252 -1.2407634 0.26519537 2.0169024 3.7792177 4.9643984 5.0406265 4.108078 2.6120348 0.66813612 -1.699327 -3.5042446 -4.2454195 -4.208672][-2.6067138 -1.8858488 -0.56443143 0.68272924 1.7418637 3.0936904 3.8728704 3.6370878 2.4649196 0.74411297 -1.1255538 -2.9880762 -4.4573994 -4.9356451 -4.6618166][-2.4679024 -1.8130398 -0.86107588 0.56599808 1.4047608 1.6150303 1.4986229 0.96624708 -0.2526927 -1.7786372 -3.189733 -4.4009948 -5.1466732 -5.3359661 -4.9940205][-3.2137704 -2.404202 -1.5145552 -0.71557188 -0.095892429 -0.15177774 -0.995214 -2.0214431 -3.031405 -4.1091309 -4.9475374 -5.5896239 -5.729033 -5.3896246 -4.7918296][-3.6512973 -2.9041605 -2.1525526 -1.5162375 -1.372643 -1.7357581 -2.5778036 -3.8026452 -4.9849524 -5.9049225 -6.2985897 -6.4495358 -6.0876317 -5.3383403 -4.5173473][-4.099853 -3.0132837 -2.1437109 -1.4918649 -1.5030665 -2.0364559 -3.1063066 -4.3381472 -5.5767632 -6.6245122 -6.8551455 -6.5675836 -5.9543395 -4.9931421 -4.0444736][-3.8809092 -2.7022 -1.7103205 -1.0120957 -0.93745375 -1.5244803 -2.6536732 -3.9661167 -5.0628734 -6.0526514 -6.5889835 -6.3128424 -5.3933821 -4.3354845 -3.4857681][-3.1011133 -1.8269742 -0.83459306 -0.16339493 0.08861208 -0.29598951 -1.2857096 -2.7921562 -4.1897182 -5.132165 -5.484818 -5.553072 -4.96636 -3.8459721 -2.8980222]]...]
INFO - root - 2017-12-16 07:56:49.948062: step 53910, loss = 0.22, batch loss = 0.17 (28.2 examples/sec; 0.284 sec/batch; 21h:56m:54s remains)
INFO - root - 2017-12-16 07:56:52.812004: step 53920, loss = 0.22, batch loss = 0.16 (27.7 examples/sec; 0.288 sec/batch; 22h:18m:59s remains)
INFO - root - 2017-12-16 07:56:55.704757: step 53930, loss = 0.29, batch loss = 0.23 (27.6 examples/sec; 0.290 sec/batch; 22h:24m:29s remains)
INFO - root - 2017-12-16 07:56:58.568032: step 53940, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.284 sec/batch; 21h:59m:55s remains)
INFO - root - 2017-12-16 07:57:01.418553: step 53950, loss = 0.27, batch loss = 0.21 (28.9 examples/sec; 0.277 sec/batch; 21h:24m:39s remains)
INFO - root - 2017-12-16 07:57:04.292484: step 53960, loss = 0.26, batch loss = 0.20 (29.4 examples/sec; 0.272 sec/batch; 21h:04m:44s remains)
INFO - root - 2017-12-16 07:57:07.115038: step 53970, loss = 0.23, batch loss = 0.18 (27.9 examples/sec; 0.287 sec/batch; 22h:11m:10s remains)
INFO - root - 2017-12-16 07:57:10.026542: step 53980, loss = 0.22, batch loss = 0.16 (26.6 examples/sec; 0.301 sec/batch; 23h:16m:55s remains)
INFO - root - 2017-12-16 07:57:12.885594: step 53990, loss = 0.25, batch loss = 0.19 (29.3 examples/sec; 0.273 sec/batch; 21h:05m:21s remains)
INFO - root - 2017-12-16 07:57:15.759807: step 54000, loss = 0.50, batch loss = 0.44 (28.2 examples/sec; 0.283 sec/batch; 21h:55m:10s remains)
2017-12-16 07:57:16.303016: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0876093 -4.5400062 -4.9585371 -5.3517985 -5.5179386 -5.679142 -5.95601 -6.3959322 -6.7249231 -6.7391596 -6.5814381 -6.5232172 -6.2596579 -5.5648746 -4.9024105][-4.7515316 -5.4179225 -5.8751807 -6.19312 -6.2970805 -6.6851597 -6.9267182 -7.134726 -7.6502447 -8.3412542 -8.7733717 -8.7132969 -8.3418579 -7.6281195 -6.8562555][-5.3884878 -6.242568 -6.7862043 -6.8783 -6.6614017 -6.4452834 -6.3302526 -6.9177322 -7.6552458 -8.36444 -9.2475014 -10.006012 -10.231464 -9.8134556 -8.8869991][-5.6965809 -6.4846678 -6.8083849 -6.57199 -5.9659047 -5.1754827 -4.4986587 -4.413023 -5.1187925 -6.6237531 -8.3373423 -9.7323513 -10.597233 -10.604021 -9.9770689][-5.6107311 -6.1828523 -6.0765905 -5.3380866 -3.9556911 -2.2017844 -0.85895085 -0.53361368 -1.2357647 -2.9018188 -5.0510545 -7.4941363 -9.5473137 -10.429051 -10.140169][-5.23487 -5.3603239 -4.8361006 -3.5393143 -1.4107454 0.9352107 3.0336986 4.2221222 3.9596071 1.9714608 -1.0002496 -4.2685447 -6.9891348 -8.6454582 -9.21137][-4.8053637 -4.5895543 -3.6638947 -1.9639361 0.42439938 3.3883867 6.1680765 7.45747 7.4469042 5.9761782 3.1511974 -0.60898089 -4.1167812 -6.4394379 -7.3954835][-4.1597176 -4.0479474 -3.5672207 -1.6046076 1.1134276 3.9684343 6.7751961 8.7240515 9.3525209 7.8612366 4.9720554 1.4962206 -1.8358605 -4.4009132 -5.8898325][-4.1345029 -4.0754533 -3.4690018 -2.0593548 -0.15580702 2.7659936 5.6435795 7.2627258 8.0441313 7.2214842 5.0807629 1.7744336 -1.5924401 -3.9157112 -5.1297097][-4.3275857 -4.572329 -4.7283521 -3.8614769 -2.4156697 -0.38056517 1.7939978 3.6650171 4.7797813 4.1616735 2.5963426 0.23720789 -2.2684095 -4.3485165 -5.5839691][-5.457449 -5.8433704 -5.9675059 -5.6262927 -5.0117226 -3.8921394 -2.4620063 -1.1642809 -0.40844536 -0.46706915 -0.98116636 -2.5245433 -4.1985483 -5.4665308 -6.0859995][-6.18529 -6.7164049 -7.2675047 -7.39546 -7.2300663 -6.7444563 -6.0731339 -5.4448743 -4.7902679 -4.6117539 -4.8790808 -5.6373682 -6.2691059 -6.8617544 -6.9911757][-6.6938725 -7.3922806 -8.0530882 -8.3539762 -8.6840029 -8.7929516 -8.6023159 -8.1069088 -7.6120052 -7.28716 -6.9744291 -6.9814587 -7.0630727 -7.3588448 -7.2422371][-6.5017138 -7.1326752 -7.6670609 -8.1319256 -8.6363869 -8.8933392 -9.1772461 -9.1782665 -8.7631235 -8.106163 -7.475039 -7.2190008 -7.0779209 -6.9550462 -6.6290579][-6.0861254 -6.3777385 -6.5823116 -7.029757 -7.4854546 -7.8550949 -8.20084 -8.171524 -7.932909 -7.51175 -6.9981594 -6.4203348 -5.9966426 -6.0299473 -5.9396048]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 07:57:19.152152: step 54010, loss = 0.25, batch loss = 0.19 (28.3 examples/sec; 0.283 sec/batch; 21h:52m:51s remains)
INFO - root - 2017-12-16 07:57:21.983006: step 54020, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.284 sec/batch; 21h:56m:47s remains)
INFO - root - 2017-12-16 07:57:24.863297: step 54030, loss = 0.32, batch loss = 0.27 (29.0 examples/sec; 0.276 sec/batch; 21h:21m:14s remains)
INFO - root - 2017-12-16 07:57:27.746995: step 54040, loss = 0.24, batch loss = 0.19 (28.9 examples/sec; 0.277 sec/batch; 21h:25m:20s remains)
INFO - root - 2017-12-16 07:57:30.648306: step 54050, loss = 0.22, batch loss = 0.17 (27.9 examples/sec; 0.287 sec/batch; 22h:11m:10s remains)
INFO - root - 2017-12-16 07:57:33.518130: step 54060, loss = 0.32, batch loss = 0.26 (27.9 examples/sec; 0.287 sec/batch; 22h:11m:36s remains)
INFO - root - 2017-12-16 07:57:36.405527: step 54070, loss = 0.23, batch loss = 0.17 (28.9 examples/sec; 0.277 sec/batch; 21h:23m:17s remains)
INFO - root - 2017-12-16 07:57:39.284520: step 54080, loss = 0.23, batch loss = 0.17 (26.9 examples/sec; 0.297 sec/batch; 22h:58m:27s remains)
INFO - root - 2017-12-16 07:57:42.115354: step 54090, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 21h:53m:20s remains)
INFO - root - 2017-12-16 07:57:44.993087: step 54100, loss = 0.21, batch loss = 0.15 (27.4 examples/sec; 0.292 sec/batch; 22h:33m:33s remains)
2017-12-16 07:57:45.529693: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1081963 -2.8835549 -2.8010998 -2.6530695 -3.0342395 -3.4437294 -3.8792634 -4.0947104 -3.9784381 -3.9769316 -3.9109597 -3.9739919 -4.0870686 -4.2782693 -4.6153522][-1.8174329 -1.631093 -1.7013235 -1.9986501 -2.4646235 -2.8904011 -3.5092711 -4.0299897 -4.2791352 -4.2912178 -4.1546044 -4.3076959 -4.4344249 -4.7556081 -5.2655983][-0.54560995 -0.40721369 -0.4114213 -0.83001518 -1.5550423 -2.2626028 -3.0154543 -3.6442912 -3.8138769 -3.939368 -3.9568517 -4.18338 -4.5970592 -4.9386435 -5.2612023][0.5526948 0.49930906 0.27487469 -0.034834385 -0.54244995 -1.2644441 -1.9395669 -2.5764284 -2.9019251 -3.2021136 -3.4308193 -3.7064013 -4.005764 -4.5808411 -5.2566185][0.721951 0.90255642 0.76601267 0.36607122 -0.13336229 -0.6771667 -1.1668048 -1.5887887 -1.8295496 -2.0619063 -2.2102811 -2.6168172 -3.1956906 -3.7560186 -4.1003356][-0.03386879 0.40138149 0.6866498 0.73417711 0.63217545 0.3700161 0.062471867 -0.34803057 -0.63952804 -1.0239558 -1.3688898 -1.8494074 -2.2320549 -2.758594 -3.1712241][-1.4811721 -0.51910758 0.16235733 0.52059031 0.70030165 0.59037876 0.4537878 0.18032169 -0.0044016838 -0.34656096 -0.60057044 -0.83011389 -1.0727882 -1.2947586 -1.2535281][-2.6681843 -1.612901 -0.86886549 -0.21367168 0.21224451 0.3396287 0.49856806 0.42205811 0.42047834 0.32730532 0.27807379 0.22573137 0.29779673 0.4107666 0.53265667][-4.0723577 -2.9815958 -2.107759 -1.3495739 -0.80236149 -0.41414547 -0.073282242 0.1375947 0.48129416 0.59921885 0.71809053 0.81363058 1.0614228 1.2600107 1.5279465][-5.1589637 -4.1674285 -3.3002105 -2.5234818 -1.9937294 -1.5370309 -1.0682092 -0.79567409 -0.41974211 -0.10878229 0.20354223 0.38353109 0.66074467 0.9683733 1.2228613][-5.7330885 -5.022841 -4.3788633 -3.7287176 -3.1262276 -2.7310269 -2.3256097 -2.0965238 -1.7417557 -1.4310641 -1.1107416 -0.8636241 -0.54216313 -0.27543831 0.011874199][-5.7589436 -5.3999805 -5.1215744 -4.749073 -4.4636931 -4.2801571 -3.9860311 -3.7708218 -3.4271274 -3.187376 -2.9190416 -2.6422081 -2.3644402 -2.1623478 -2.042249][-5.4198523 -5.3570614 -5.3100333 -5.3468857 -5.4138932 -5.469595 -5.4382625 -5.4309683 -5.2047811 -5.000093 -4.6841836 -4.4383831 -4.2755771 -4.2354627 -4.199111][-4.3922062 -4.4850068 -4.8076658 -5.2463522 -5.6360478 -6.1293907 -6.4556322 -6.6504412 -6.6196127 -6.4277425 -6.1387806 -5.9456396 -5.9204226 -5.8490057 -5.7249908][-3.6351919 -3.8187606 -4.166059 -4.8273764 -5.4679079 -6.18212 -6.6369762 -7.019248 -7.2486124 -7.2013316 -6.9918427 -6.7982736 -6.6584849 -6.4789276 -6.332037]]...]
INFO - root - 2017-12-16 07:57:48.406425: step 54110, loss = 0.30, batch loss = 0.25 (28.9 examples/sec; 0.277 sec/batch; 21h:24m:07s remains)
INFO - root - 2017-12-16 07:57:51.208060: step 54120, loss = 0.29, batch loss = 0.23 (27.0 examples/sec; 0.296 sec/batch; 22h:53m:44s remains)
INFO - root - 2017-12-16 07:57:54.083024: step 54130, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.284 sec/batch; 21h:57m:22s remains)
INFO - root - 2017-12-16 07:57:56.969933: step 54140, loss = 0.32, batch loss = 0.26 (27.2 examples/sec; 0.294 sec/batch; 22h:44m:12s remains)
INFO - root - 2017-12-16 07:57:59.831052: step 54150, loss = 0.26, batch loss = 0.20 (28.8 examples/sec; 0.277 sec/batch; 21h:26m:41s remains)
INFO - root - 2017-12-16 07:58:02.646247: step 54160, loss = 0.47, batch loss = 0.41 (27.3 examples/sec; 0.293 sec/batch; 22h:37m:47s remains)
INFO - root - 2017-12-16 07:58:05.518578: step 54170, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 21h:32m:41s remains)
INFO - root - 2017-12-16 07:58:08.393700: step 54180, loss = 0.37, batch loss = 0.31 (26.2 examples/sec; 0.305 sec/batch; 23h:34m:27s remains)
INFO - root - 2017-12-16 07:58:11.300730: step 54190, loss = 0.27, batch loss = 0.22 (28.9 examples/sec; 0.277 sec/batch; 21h:25m:21s remains)
INFO - root - 2017-12-16 07:58:14.124894: step 54200, loss = 0.30, batch loss = 0.24 (28.6 examples/sec; 0.280 sec/batch; 21h:37m:10s remains)
2017-12-16 07:58:14.634568: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6982007 -2.7475624 -2.8057718 -2.8783028 -2.9760685 -2.9801164 -2.9369087 -2.8642986 -2.8078582 -2.6478233 -2.5194039 -2.5197229 -2.4135487 -2.3913088 -2.3295214][-2.6231904 -2.6797333 -2.7382195 -2.8687119 -3.0189037 -3.0159564 -2.9758024 -2.882365 -2.7962546 -2.6783462 -2.6112256 -2.5573339 -2.4707828 -2.4681048 -2.3741083][-2.5068614 -2.501087 -2.5170159 -2.646246 -2.6988015 -2.7194982 -2.7370133 -2.6880217 -2.6387002 -2.6807487 -2.800158 -2.8249633 -2.8149834 -2.7691092 -2.6539223][-2.6005583 -2.5416341 -2.4000018 -2.4024589 -2.2776034 -2.1119502 -2.0388339 -2.0740263 -2.2593832 -2.489306 -2.6888952 -2.8372741 -3.003758 -3.0862138 -3.1504376][-2.5637517 -2.1889884 -1.6809251 -1.6170013 -1.4596341 -1.2882798 -1.156275 -1.0934174 -1.3440008 -1.7151189 -2.140492 -2.5972962 -2.9538157 -3.1642516 -3.2866955][-2.2340574 -1.6869078 -0.97904325 -0.56564665 -0.07033062 0.17054462 0.32506323 0.42007828 0.18605042 -0.28749371 -0.95337629 -1.6975398 -2.458468 -2.9584608 -3.1365933][-1.6789992 -1.0473874 -0.30225992 0.35037279 1.0627732 1.4292822 1.7514811 1.7285128 1.4546151 0.961112 0.18856049 -0.66722274 -1.6497769 -2.3518698 -2.7929254][-1.6294501 -0.86638165 0.0035705566 0.76996756 1.418035 1.8227429 2.1265349 2.0180883 1.6938477 1.0547771 0.20909643 -0.6404407 -1.573287 -2.2924266 -2.8102024][-1.7292736 -1.028264 -0.21192074 0.28718328 0.73773527 1.0725317 1.3297782 1.1212759 0.8272295 0.19401932 -0.55488276 -1.4904778 -2.4416871 -2.8688006 -3.0877812][-1.8454778 -1.286864 -0.80622935 -0.57862782 -0.36299181 -0.28223276 -0.38128328 -0.8344593 -1.2635028 -1.7270179 -2.1424925 -2.8514595 -3.521692 -3.7301884 -3.6822155][-2.1808727 -1.61536 -1.376116 -1.5941732 -1.838773 -1.9757087 -2.1364212 -2.9143624 -3.5574312 -4.0935488 -4.457108 -4.7173619 -4.913588 -4.831533 -4.6228228][-2.4960868 -2.1320934 -2.0783539 -2.448627 -3.0579357 -3.5566795 -4.0166316 -4.8450217 -5.4358888 -5.7321377 -5.9041581 -6.0380821 -6.0172663 -5.7675085 -5.3829765][-2.6169562 -2.2799804 -2.3471928 -2.8463669 -3.6222324 -4.1952782 -4.7964039 -5.6405764 -6.2581487 -6.5147495 -6.4345064 -6.3171229 -6.2357779 -6.02348 -5.6282263][-2.6844406 -2.3727367 -2.4413905 -2.8238854 -3.5487442 -4.0954614 -4.510778 -5.2240758 -5.7027774 -5.9611111 -5.9189086 -5.8389649 -5.8934669 -5.7473917 -5.5434346][-2.710309 -2.3669491 -2.3817551 -2.6879773 -3.2561507 -3.8438041 -4.3126221 -4.7782741 -5.1122413 -5.3896208 -5.3508282 -5.1990657 -5.0673037 -4.9213943 -4.8440204]]...]
INFO - root - 2017-12-16 07:58:17.518473: step 54210, loss = 0.28, batch loss = 0.22 (27.7 examples/sec; 0.289 sec/batch; 22h:18m:25s remains)
INFO - root - 2017-12-16 07:58:20.365882: step 54220, loss = 0.61, batch loss = 0.55 (27.4 examples/sec; 0.292 sec/batch; 22h:35m:02s remains)
INFO - root - 2017-12-16 07:58:23.218685: step 54230, loss = 0.20, batch loss = 0.14 (28.4 examples/sec; 0.281 sec/batch; 21h:44m:31s remains)
INFO - root - 2017-12-16 07:58:26.040517: step 54240, loss = 0.22, batch loss = 0.17 (28.3 examples/sec; 0.283 sec/batch; 21h:50m:50s remains)
INFO - root - 2017-12-16 07:58:28.935131: step 54250, loss = 0.27, batch loss = 0.21 (27.6 examples/sec; 0.290 sec/batch; 22h:23m:48s remains)
INFO - root - 2017-12-16 07:58:31.782814: step 54260, loss = 0.29, batch loss = 0.23 (28.4 examples/sec; 0.281 sec/batch; 21h:44m:34s remains)
INFO - root - 2017-12-16 07:58:34.674785: step 54270, loss = 0.23, batch loss = 0.18 (28.2 examples/sec; 0.284 sec/batch; 21h:57m:48s remains)
INFO - root - 2017-12-16 07:58:37.518738: step 54280, loss = 0.27, batch loss = 0.21 (29.2 examples/sec; 0.274 sec/batch; 21h:09m:35s remains)
INFO - root - 2017-12-16 07:58:40.363902: step 54290, loss = 0.47, batch loss = 0.41 (27.9 examples/sec; 0.287 sec/batch; 22h:08m:42s remains)
INFO - root - 2017-12-16 07:58:43.165337: step 54300, loss = 0.27, batch loss = 0.22 (28.1 examples/sec; 0.284 sec/batch; 21h:58m:04s remains)
2017-12-16 07:58:43.688690: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.37028 -4.777452 -5.0784116 -5.2393088 -5.3590412 -5.3346167 -5.3335867 -5.4588132 -5.8151932 -6.3802338 -6.8285966 -7.2642288 -7.3055124 -6.6811409 -5.7417197][-5.1243367 -5.5353279 -5.7233973 -5.7578797 -5.780839 -5.93268 -6.1722984 -6.405138 -6.8252072 -7.5881963 -8.474493 -9.1796513 -9.3117676 -8.6650791 -7.503582][-5.5014143 -5.9243073 -5.9787049 -5.75233 -5.4867015 -5.3910642 -5.4480863 -6.0851889 -7.0373678 -8.000206 -9.1013355 -10.098772 -10.69582 -10.356663 -9.2104464][-5.6326809 -5.8662305 -5.6495023 -5.034493 -4.4402556 -3.9829071 -3.8611114 -4.2651639 -5.11086 -6.7458239 -8.5243893 -9.9220667 -10.844012 -11.061552 -10.339806][-5.0545864 -4.9952407 -4.4576941 -3.5098963 -2.3719954 -1.264092 -0.73510456 -0.97284436 -1.9036355 -3.8466213 -6.1021128 -8.3778362 -10.115099 -10.950237 -10.636457][-4.3054028 -3.983197 -3.0806663 -1.4987986 0.424098 1.8508477 2.6598153 2.7979918 2.023159 -0.18030119 -3.1188564 -6.1939898 -8.6608334 -9.9915886 -10.074801][-4.1095572 -3.4719577 -2.2231445 -0.53098559 1.9088368 4.2991886 5.9655304 6.0614033 5.0039845 2.8534403 -0.31021786 -3.975209 -6.9647589 -8.5504808 -8.7078114][-4.4214664 -3.6836357 -2.4802728 -0.43178463 2.1378164 4.6449137 6.7976351 7.4598989 6.5213432 4.1069145 0.84146357 -2.7441382 -5.6657009 -7.2307053 -7.361928][-4.8422961 -4.5706382 -3.4168146 -1.4833307 0.71716595 3.3119969 5.5186892 6.2133961 5.5688734 3.5968323 0.71008396 -2.4816015 -5.0896688 -6.5226474 -6.5560122][-5.0017843 -5.3282862 -4.9504776 -3.3938994 -1.4619644 0.66539717 2.4692478 3.4669852 3.1865892 1.4844294 -0.69909143 -3.0289273 -5.1756516 -6.4518185 -6.70084][-5.901866 -6.234479 -6.0528612 -5.1231313 -3.8081589 -2.3129227 -1.081027 -0.39095736 -0.44767833 -1.3992462 -2.9271514 -4.5300889 -5.8191991 -6.593483 -6.8187385][-6.2869205 -6.6181431 -6.7927861 -6.3510108 -5.3974185 -4.3314362 -3.5522857 -3.1495323 -3.263248 -3.9150982 -4.8372612 -5.9272904 -6.7514343 -7.0795283 -7.01691][-5.7261729 -6.1086884 -6.4032359 -6.3293419 -6.1545291 -5.5605617 -4.8069406 -4.4841385 -4.4996719 -4.7529902 -5.3724127 -6.1963596 -6.6810732 -6.9776516 -6.9196253][-4.9912839 -5.1317129 -5.2563376 -5.3580956 -5.4702239 -5.3086176 -5.1499486 -5.0861235 -5.067296 -5.1641374 -5.1934462 -5.4019322 -5.8169212 -6.0166235 -5.9174757][-4.2551527 -4.1797791 -4.072269 -4.08606 -4.2019062 -4.2517152 -4.2823172 -4.306848 -4.36886 -4.5426788 -4.6997018 -4.6082811 -4.4285688 -4.4922142 -4.5560517]]...]
INFO - root - 2017-12-16 07:58:46.509961: step 54310, loss = 0.27, batch loss = 0.22 (29.1 examples/sec; 0.275 sec/batch; 21h:16m:17s remains)
INFO - root - 2017-12-16 07:58:49.360692: step 54320, loss = 0.24, batch loss = 0.18 (27.7 examples/sec; 0.289 sec/batch; 22h:19m:07s remains)
INFO - root - 2017-12-16 07:58:52.212376: step 54330, loss = 0.33, batch loss = 0.27 (28.7 examples/sec; 0.279 sec/batch; 21h:31m:52s remains)
INFO - root - 2017-12-16 07:58:55.047903: step 54340, loss = 0.25, batch loss = 0.19 (28.9 examples/sec; 0.277 sec/batch; 21h:23m:57s remains)
INFO - root - 2017-12-16 07:58:57.866851: step 54350, loss = 0.29, batch loss = 0.23 (28.6 examples/sec; 0.280 sec/batch; 21h:35m:47s remains)
INFO - root - 2017-12-16 07:59:00.775461: step 54360, loss = 0.44, batch loss = 0.38 (27.9 examples/sec; 0.287 sec/batch; 22h:08m:51s remains)
INFO - root - 2017-12-16 07:59:03.645523: step 54370, loss = 0.23, batch loss = 0.17 (28.1 examples/sec; 0.284 sec/batch; 21h:57m:46s remains)
INFO - root - 2017-12-16 07:59:06.513628: step 54380, loss = 0.18, batch loss = 0.12 (28.1 examples/sec; 0.285 sec/batch; 22h:00m:31s remains)
INFO - root - 2017-12-16 07:59:09.334225: step 54390, loss = 0.42, batch loss = 0.36 (29.6 examples/sec; 0.270 sec/batch; 20h:53m:07s remains)
INFO - root - 2017-12-16 07:59:12.193402: step 54400, loss = 0.27, batch loss = 0.21 (28.6 examples/sec; 0.279 sec/batch; 21h:34m:29s remains)
2017-12-16 07:59:12.698990: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2887788 -5.1216197 -4.8305364 -4.1782112 -3.7649877 -3.3550596 -3.2197766 -3.2400866 -3.337863 -3.7167089 -4.0664592 -4.2454286 -4.2377434 -4.2483926 -4.0574412][-6.199234 -5.8339119 -5.3906817 -4.6429567 -4.1387739 -3.8572998 -3.7074718 -3.5842412 -3.5757852 -3.9354234 -4.1912923 -4.3005052 -4.3099146 -4.231163 -3.9987311][-6.0848837 -5.6277676 -5.2109323 -4.4744163 -4.0378284 -3.8628988 -3.7171054 -3.5351505 -3.4763432 -3.7457676 -4.0407839 -4.3474054 -4.5149193 -4.5419636 -4.3638611][-5.163219 -4.5942574 -4.1489682 -3.65462 -3.469852 -3.296221 -3.0600448 -2.7881739 -2.7409875 -3.1008053 -3.5226951 -4.0495534 -4.3945594 -4.6105723 -4.6239066][-3.2850709 -2.6914673 -2.3760948 -2.0862761 -2.060091 -2.0265515 -1.7721148 -1.6270175 -1.6089807 -2.1231108 -2.8104982 -3.6057742 -4.2002797 -4.69191 -4.7549453][-1.1687648 -0.18555689 -0.011834145 -0.033753395 -0.29703522 -0.45912361 -0.13927412 -0.031023502 -0.015624046 -0.61650276 -1.5477617 -2.7773125 -3.89573 -4.6162562 -4.7817068][0.864202 1.6987586 1.6980309 1.3460369 0.73118877 0.65049505 1.2068882 1.61345 1.7650428 1.0914216 -0.083137035 -1.6066098 -3.067997 -4.1234679 -4.396656][0.94569921 1.8434343 1.9986529 1.6894975 1.2710495 1.4780874 2.2580338 2.8976836 3.1271582 2.3326683 0.91338062 -0.88656712 -2.6135697 -3.7543497 -4.1230154][-0.33582211 0.41232777 0.65065432 0.49098921 0.54508972 1.1738124 2.2061419 3.1799054 3.5476036 2.9007554 1.4722648 -0.39679956 -2.1953151 -3.5121474 -4.0199394][-2.1246884 -1.7855282 -1.5934889 -1.4244859 -1.1704719 -0.23575497 1.1017089 2.3057723 2.8978634 2.4318738 1.1148982 -0.58318686 -2.2736852 -3.5956054 -4.2693119][-3.8576269 -3.6539052 -3.6205857 -3.5113559 -3.2988181 -1.9955456 -0.54853582 0.72176456 1.3984785 1.2513094 0.26449585 -1.2618918 -2.730092 -4.0262237 -4.7923326][-5.5737295 -5.6393738 -5.7649255 -5.5044951 -5.0460715 -3.7362568 -2.4415145 -1.029917 -0.3024826 -0.34731913 -0.99953866 -2.300112 -3.4648249 -4.4730887 -5.0783072][-6.5807018 -6.5627747 -6.5515471 -6.5832496 -6.5200434 -5.6950297 -4.7782269 -3.5636849 -2.7287054 -2.6047683 -2.8413668 -3.610213 -4.4379878 -5.1613946 -5.4646254][-6.6269803 -6.5017738 -6.5354967 -6.6768813 -6.8846488 -6.558434 -6.1376863 -5.3685069 -4.6601214 -4.1567483 -3.9942706 -4.4224005 -4.8101087 -5.173388 -5.3405547][-6.1585994 -6.1328082 -6.3158159 -6.5491157 -6.8551841 -6.8493729 -6.6520348 -5.909945 -5.0029678 -4.4910731 -4.3001342 -4.1743269 -4.2385206 -4.6188493 -4.7230616]]...]
INFO - root - 2017-12-16 07:59:15.532169: step 54410, loss = 0.31, batch loss = 0.25 (28.5 examples/sec; 0.281 sec/batch; 21h:40m:25s remains)
INFO - root - 2017-12-16 07:59:18.395830: step 54420, loss = 0.27, batch loss = 0.21 (28.4 examples/sec; 0.282 sec/batch; 21h:47m:48s remains)
INFO - root - 2017-12-16 07:59:21.312400: step 54430, loss = 0.26, batch loss = 0.20 (27.1 examples/sec; 0.296 sec/batch; 22h:50m:03s remains)
INFO - root - 2017-12-16 07:59:24.197656: step 54440, loss = 0.26, batch loss = 0.21 (27.6 examples/sec; 0.290 sec/batch; 22h:23m:54s remains)
INFO - root - 2017-12-16 07:59:27.032077: step 54450, loss = 0.21, batch loss = 0.15 (29.0 examples/sec; 0.276 sec/batch; 21h:19m:24s remains)
INFO - root - 2017-12-16 07:59:29.894653: step 54460, loss = 0.23, batch loss = 0.18 (27.2 examples/sec; 0.294 sec/batch; 22h:41m:47s remains)
INFO - root - 2017-12-16 07:59:32.700412: step 54470, loss = 0.27, batch loss = 0.21 (28.1 examples/sec; 0.285 sec/batch; 22h:01m:32s remains)
INFO - root - 2017-12-16 07:59:35.545543: step 54480, loss = 0.27, batch loss = 0.21 (29.4 examples/sec; 0.272 sec/batch; 21h:01m:13s remains)
INFO - root - 2017-12-16 07:59:38.413029: step 54490, loss = 0.20, batch loss = 0.14 (28.6 examples/sec; 0.279 sec/batch; 21h:35m:03s remains)
INFO - root - 2017-12-16 07:59:41.269459: step 54500, loss = 0.30, batch loss = 0.25 (27.4 examples/sec; 0.292 sec/batch; 22h:32m:23s remains)
2017-12-16 07:59:41.763881: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.24109 -5.6478477 -5.7827473 -5.7462535 -5.5530934 -5.3282032 -5.1540184 -4.950767 -4.94054 -5.0553336 -5.1803536 -5.4107003 -5.5629454 -5.3817825 -4.9641366][-5.7020073 -6.0511255 -6.1411881 -6.0569563 -5.8887281 -5.8472791 -5.7526422 -5.558218 -5.5035329 -5.6316853 -6.0651827 -6.406003 -6.68384 -6.682869 -6.3630867][-5.5909185 -5.9349718 -6.0345221 -5.7417307 -5.4392252 -5.3855262 -5.2784472 -5.3572493 -5.4145265 -5.628139 -6.2887321 -6.8930717 -7.5852327 -7.9894619 -7.9417677][-4.9598274 -5.173347 -5.0273328 -4.4855518 -3.9564636 -3.6002512 -3.3146768 -3.4291892 -3.6582546 -4.2919006 -5.3396821 -6.5590267 -7.9355407 -8.7562275 -9.1288328][-4.2398853 -4.1461797 -3.7239647 -2.9432688 -1.8821208 -0.96367049 -0.5022316 -0.41681671 -0.6124332 -1.5582321 -3.039767 -4.8685727 -7.0060711 -8.6013012 -9.5287266][-3.5260055 -3.40049 -2.7986951 -1.6117196 -0.15754318 1.1819592 2.2075863 2.5598388 2.2610583 1.2960143 -0.36477089 -2.8399296 -5.679574 -7.8803487 -9.1868362][-3.2791934 -2.9442158 -2.1382775 -0.65795088 1.3174167 3.0856729 4.4430733 4.9400845 4.7241125 3.631958 1.787209 -1.0951097 -4.3635139 -6.9210777 -8.41892][-3.4229145 -3.0949178 -2.3211658 -0.6096158 1.5480771 3.6439447 5.353034 6.005084 5.742609 4.6123047 2.8290391 -0.094443321 -3.4544542 -6.122098 -7.5712185][-4.2746339 -4.1010251 -3.390378 -1.9050944 -0.21588516 1.7993593 3.6829243 4.6091509 4.5525904 3.8367748 2.4197502 -0.15044546 -3.2530398 -5.8416834 -7.0652623][-5.5602627 -5.7281837 -5.2678585 -4.0491357 -2.4990768 -0.77101183 0.80123806 1.9291582 2.3225069 1.8357778 0.82926607 -1.121635 -3.6115968 -5.8714218 -7.0143738][-7.1057329 -7.1607819 -6.7147946 -5.9285 -4.8669176 -3.3065045 -1.8637476 -0.87561846 -0.40366077 -0.51971269 -1.0168405 -2.349896 -4.2285991 -6.0097165 -6.9176321][-8.442215 -8.3966389 -7.8239412 -7.1492472 -6.3585739 -5.4273672 -4.4909511 -3.5855138 -3.0692587 -2.8279 -2.988059 -3.8178546 -4.9717512 -6.1452217 -6.7705307][-9.3440819 -9.2173729 -8.6032791 -7.7700663 -7.0530725 -6.4314823 -5.7183771 -5.0314751 -4.672791 -4.3279877 -4.2051196 -4.5546803 -5.22053 -6.1000228 -6.6288357][-9.4842873 -9.2984056 -8.627615 -7.7073407 -6.9327536 -6.2815933 -5.9082918 -5.602035 -5.2294106 -4.9195523 -4.8135757 -5.0694604 -5.4469519 -5.8773985 -6.1294694][-8.6960564 -8.2355928 -7.4381595 -6.8363371 -6.3103819 -5.7929149 -5.5052166 -5.4555054 -5.4145331 -5.1675377 -5.0188718 -5.0030041 -5.0448265 -5.2704492 -5.39799]]...]
INFO - root - 2017-12-16 07:59:44.575304: step 54510, loss = 0.35, batch loss = 0.29 (25.8 examples/sec; 0.311 sec/batch; 23h:59m:23s remains)
INFO - root - 2017-12-16 07:59:47.463450: step 54520, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 21h:44m:18s remains)
INFO - root - 2017-12-16 07:59:50.309087: step 54530, loss = 0.40, batch loss = 0.34 (27.5 examples/sec; 0.291 sec/batch; 22h:27m:01s remains)
INFO - root - 2017-12-16 07:59:53.159182: step 54540, loss = 0.21, batch loss = 0.15 (27.9 examples/sec; 0.287 sec/batch; 22h:09m:43s remains)
INFO - root - 2017-12-16 07:59:56.031387: step 54550, loss = 0.31, batch loss = 0.25 (28.9 examples/sec; 0.277 sec/batch; 21h:23m:45s remains)
INFO - root - 2017-12-16 07:59:58.880451: step 54560, loss = 0.22, batch loss = 0.16 (28.2 examples/sec; 0.283 sec/batch; 21h:52m:44s remains)
INFO - root - 2017-12-16 08:00:01.711196: step 54570, loss = 0.23, batch loss = 0.18 (28.0 examples/sec; 0.286 sec/batch; 22h:05m:41s remains)
INFO - root - 2017-12-16 08:00:04.533470: step 54580, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.277 sec/batch; 21h:22m:19s remains)
INFO - root - 2017-12-16 08:00:07.433083: step 54590, loss = 0.28, batch loss = 0.22 (28.3 examples/sec; 0.283 sec/batch; 21h:49m:48s remains)
INFO - root - 2017-12-16 08:00:10.221031: step 54600, loss = 0.25, batch loss = 0.19 (29.3 examples/sec; 0.273 sec/batch; 21h:05m:29s remains)
2017-12-16 08:00:10.718730: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5477495 -5.7095213 -5.6346245 -5.557559 -5.5409594 -5.52155 -5.5112343 -5.6428404 -5.8080697 -5.8828087 -5.9197731 -5.836729 -5.5729027 -5.1885295 -4.756093][-5.6234951 -5.5710239 -5.48106 -5.55096 -5.579402 -5.8340197 -6.0561161 -6.00809 -6.0085964 -6.2725811 -6.3877168 -6.191968 -5.9485431 -5.660182 -5.36606][-4.9340105 -4.9097939 -4.8959708 -4.8807435 -4.98845 -5.2582064 -5.301774 -5.5313044 -5.8181772 -5.9179888 -6.0395689 -6.2181044 -6.1871152 -5.8805828 -5.5825157][-4.0456605 -3.6137927 -3.4043598 -3.5337329 -3.6781058 -3.8628521 -3.8903203 -3.9894283 -4.1468916 -4.6030283 -4.9915662 -5.2746181 -5.4769211 -5.480464 -5.3805408][-2.5010223 -1.7368555 -1.3651543 -1.5466368 -1.813904 -1.8566508 -1.6178114 -1.6468301 -1.8687179 -2.4321413 -3.0787637 -3.7186599 -4.2096457 -4.3767357 -4.34509][-1.4247577 -0.55467558 -0.083507061 0.070131779 0.14791584 0.26155567 0.74406528 1.0364499 0.8873086 0.21759272 -0.63060331 -1.7556505 -2.653985 -3.0622413 -3.169364][-0.68150187 0.12956858 0.49876547 0.63857889 0.9610467 1.6687894 2.7245884 3.2454643 3.0362859 2.3881292 1.5001397 0.30575848 -0.84540939 -1.6747625 -2.0842605][-0.25520897 0.39818048 0.71976757 0.9701972 1.2865005 1.9724073 3.0708361 3.9227047 4.0140448 3.3605666 2.4181776 1.1577263 0.031637192 -0.66332674 -1.2713425][-0.42659903 -0.072713375 0.052176952 0.24640322 0.66776371 1.4327068 2.4500866 3.1000752 3.1925383 2.818954 2.1109724 1.027247 -0.019609451 -0.67583847 -1.1002977][-0.743278 -0.60711622 -0.74081349 -0.65615559 -0.36987925 0.17731762 0.95925 1.5834842 1.6959014 1.3199935 0.8002243 0.045697212 -0.66647196 -1.0822015 -1.4537537][-1.4104142 -1.2249963 -1.2102866 -1.267801 -1.1407962 -0.85568047 -0.44029617 -0.19435596 -0.15772057 -0.41729116 -0.821295 -1.3980885 -1.8474314 -1.9664438 -2.1294241][-1.6840882 -1.4402566 -1.4703264 -1.5282753 -1.4789605 -1.3853753 -1.268527 -1.3233469 -1.4909415 -1.8635154 -2.2153108 -2.439889 -2.7165256 -2.610682 -2.5311649][-1.8077502 -1.4853418 -1.3743336 -1.4715061 -1.558578 -1.5643904 -1.5667543 -1.8900073 -2.2430847 -2.6468954 -2.9704609 -3.0755734 -3.1577773 -2.9091134 -2.6412354][-1.9031475 -1.4546502 -1.2446156 -1.1951854 -1.1866963 -1.2899733 -1.3011622 -1.748106 -2.3782575 -3.0741427 -3.4623511 -3.409606 -3.2397184 -2.8791468 -2.5975664][-1.8789 -1.3873501 -1.1071017 -1.0717392 -1.0160573 -0.97520423 -0.96975565 -1.5134926 -2.162358 -2.9063408 -3.4793837 -3.5766115 -3.506587 -2.9757047 -2.5941463]]...]
INFO - root - 2017-12-16 08:00:13.553181: step 54610, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 21h:49m:18s remains)
INFO - root - 2017-12-16 08:00:16.395049: step 54620, loss = 0.25, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 21h:50m:10s remains)
INFO - root - 2017-12-16 08:00:19.266542: step 54630, loss = 0.26, batch loss = 0.20 (28.5 examples/sec; 0.281 sec/batch; 21h:41m:42s remains)
INFO - root - 2017-12-16 08:00:22.068969: step 54640, loss = 0.35, batch loss = 0.29 (28.4 examples/sec; 0.281 sec/batch; 21h:42m:50s remains)
INFO - root - 2017-12-16 08:00:24.905879: step 54650, loss = 0.29, batch loss = 0.23 (28.4 examples/sec; 0.281 sec/batch; 21h:42m:50s remains)
INFO - root - 2017-12-16 08:00:27.773404: step 54660, loss = 0.27, batch loss = 0.21 (26.8 examples/sec; 0.299 sec/batch; 23h:02m:34s remains)
INFO - root - 2017-12-16 08:00:30.646317: step 54670, loss = 0.24, batch loss = 0.18 (26.2 examples/sec; 0.306 sec/batch; 23h:35m:45s remains)
INFO - root - 2017-12-16 08:00:33.512033: step 54680, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.285 sec/batch; 22h:01m:00s remains)
INFO - root - 2017-12-16 08:00:36.354912: step 54690, loss = 0.26, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 21h:54m:52s remains)
INFO - root - 2017-12-16 08:00:39.225321: step 54700, loss = 0.23, batch loss = 0.17 (29.0 examples/sec; 0.275 sec/batch; 21h:15m:31s remains)
2017-12-16 08:00:39.726628: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0680087 -2.346735 -2.6806622 -2.9467201 -3.0014567 -2.9061136 -2.7956004 -2.7121224 -2.7085571 -2.7990603 -2.9065318 -3.0996614 -3.2552829 -3.3662829 -3.387717][-2.0481863 -2.470746 -2.8867531 -3.1144392 -3.1447792 -2.980372 -2.7659676 -2.4662871 -2.2723646 -2.389071 -2.6149578 -2.763829 -2.9859228 -3.2866089 -3.5100265][-2.3838892 -2.9588668 -3.3122234 -3.4707127 -3.3947368 -3.0786304 -2.6368973 -2.1662571 -1.9182765 -1.982677 -2.2605422 -2.566751 -3.0100794 -3.3881795 -3.6770706][-2.819268 -3.2961655 -3.3733945 -3.3062422 -3.0168238 -2.4539919 -1.7896392 -1.2776389 -1.0690696 -1.3439894 -1.8252923 -2.2986071 -2.8829765 -3.4252348 -3.8687534][-2.8510776 -3.2937422 -3.1889973 -2.8334894 -2.3055317 -1.5392094 -0.75165415 -0.20867205 -0.097619057 -0.59070754 -1.2037845 -1.8543115 -2.6001587 -3.2251027 -3.7827165][-2.366323 -2.666389 -2.4235351 -1.76805 -0.87249017 0.023571014 0.90344095 1.2531185 1.0865245 0.44347143 -0.48224449 -1.3437014 -2.1324303 -2.875164 -3.5528896][-1.5743871 -1.7336154 -1.4085116 -0.66241527 0.35712767 1.3505483 2.2481532 2.4658308 2.2145505 1.3618274 0.27536821 -0.78487372 -1.7292547 -2.5547853 -3.2802858][-0.97645831 -1.281569 -1.0176332 -0.20472431 0.8504796 1.9098654 2.758244 2.8486218 2.499198 1.7280993 0.68178988 -0.3787446 -1.224508 -2.0015242 -2.7813601][-0.88149381 -1.299011 -1.2002337 -0.48490357 0.45786619 1.3435655 2.1364689 2.261241 2.0149784 1.3586531 0.46257019 -0.35808945 -0.97234988 -1.57058 -2.2530229][-0.8701334 -1.3742034 -1.42909 -0.85170531 -0.22241306 0.34307909 0.89839458 0.94465828 0.70866013 0.36736917 -0.16164589 -0.61737752 -0.98482871 -1.382633 -1.8811164][-1.0450261 -1.5812008 -1.7313387 -1.3637488 -0.97347045 -0.73228145 -0.44375324 -0.33456135 -0.44294333 -0.63440418 -0.77082705 -0.87136245 -0.943213 -1.189147 -1.6010025][-1.4465947 -1.8398871 -2.1287189 -1.9471965 -1.7322645 -1.5658958 -1.4792385 -1.4077487 -1.3917842 -1.3164265 -1.1412072 -0.96141386 -0.76861191 -0.71899962 -1.025583][-1.4125562 -1.5655441 -1.6782885 -1.6825449 -1.6734471 -1.6982877 -1.8379571 -1.8726954 -1.8147385 -1.6055868 -1.2207019 -0.76512623 -0.32855272 -0.09346962 -0.29381657][-0.99043489 -0.88179612 -0.67010665 -0.7331717 -1.05429 -1.317929 -1.6749294 -1.9654748 -2.1605034 -1.8944566 -1.2376003 -0.57983017 0.024154186 0.39315557 0.32580948][-0.81122732 -0.51693296 -0.18214655 -0.12609768 -0.43550324 -0.83630538 -1.3866425 -1.7829909 -2.0336251 -1.8348477 -1.1579216 -0.28740835 0.4666605 0.93759918 0.93666267]]...]
INFO - root - 2017-12-16 08:00:42.553725: step 54710, loss = 0.27, batch loss = 0.21 (28.6 examples/sec; 0.279 sec/batch; 21h:33m:20s remains)
INFO - root - 2017-12-16 08:00:45.387035: step 54720, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.285 sec/batch; 22h:00m:43s remains)
INFO - root - 2017-12-16 08:00:48.179935: step 54730, loss = 0.51, batch loss = 0.45 (28.5 examples/sec; 0.281 sec/batch; 21h:39m:12s remains)
INFO - root - 2017-12-16 08:00:51.018166: step 54740, loss = 0.27, batch loss = 0.21 (28.7 examples/sec; 0.279 sec/batch; 21h:29m:38s remains)
INFO - root - 2017-12-16 08:00:53.878173: step 54750, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 21h:31m:11s remains)
INFO - root - 2017-12-16 08:00:56.743408: step 54760, loss = 0.32, batch loss = 0.26 (27.9 examples/sec; 0.287 sec/batch; 22h:06m:43s remains)
INFO - root - 2017-12-16 08:00:59.545759: step 54770, loss = 0.25, batch loss = 0.20 (28.9 examples/sec; 0.276 sec/batch; 21h:19m:23s remains)
INFO - root - 2017-12-16 08:01:02.372615: step 54780, loss = 0.32, batch loss = 0.26 (29.3 examples/sec; 0.273 sec/batch; 21h:02m:22s remains)
INFO - root - 2017-12-16 08:01:05.264615: step 54790, loss = 0.39, batch loss = 0.34 (27.5 examples/sec; 0.291 sec/batch; 22h:27m:40s remains)
INFO - root - 2017-12-16 08:01:08.129330: step 54800, loss = 0.25, batch loss = 0.19 (28.6 examples/sec; 0.280 sec/batch; 21h:34m:35s remains)
2017-12-16 08:01:08.720243: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.76047087 -0.77653837 -0.83215451 -1.3316307 -1.8672795 -2.2210395 -2.4090436 -2.3448129 -2.1138632 -1.8066926 -1.6181779 -1.616842 -1.9817297 -2.3600576 -2.7613955][-0.15845156 -0.4306879 -0.91830349 -1.4590151 -2.2871802 -2.8472347 -3.0333171 -2.8642364 -2.602881 -2.3242292 -2.094552 -1.9907024 -2.1056321 -2.4001973 -2.8346505][-0.62918568 -0.80814123 -1.3329437 -2.1457894 -2.9444468 -3.4268322 -3.3921728 -3.140717 -2.7560875 -2.467525 -2.2264023 -2.0505486 -2.1709886 -2.2631767 -2.5918822][-1.9659622 -2.2491205 -2.6359086 -3.085587 -3.5753605 -3.588316 -3.2484827 -2.7165031 -2.2677631 -2.1903062 -2.2375672 -2.177531 -1.9850287 -1.8363566 -2.0119276][-2.8376038 -3.3192129 -3.8470609 -4.1728377 -4.0082464 -3.5063725 -2.6655526 -1.9534693 -1.5518792 -1.4535954 -1.6966579 -1.8875666 -1.7444088 -1.3913331 -1.3030844][-3.4117837 -3.8707712 -4.3719954 -4.7010927 -4.2301955 -3.1334496 -1.972291 -1.0859942 -0.67419219 -0.91938806 -1.3583548 -1.5012162 -1.217278 -0.74582314 -0.45971203][-3.8822074 -4.5745335 -4.9413052 -4.5929084 -3.5691519 -2.1506052 -0.58914185 0.32104874 0.41528988 -0.046114445 -0.7108469 -1.0894077 -0.86642003 -0.2523303 0.15786695][-3.9118764 -4.5098157 -4.8717875 -4.5425739 -3.1219275 -1.2440898 0.26259947 1.1039076 1.1832519 0.46668768 -0.39391136 -0.65481281 -0.34276438 0.20176029 0.55067968][-3.6832881 -4.3346996 -4.6831031 -4.3466339 -3.0722644 -1.268688 0.34206343 0.97144508 0.76181841 0.11582613 -0.55112481 -0.949697 -0.68936992 -0.088194847 0.42518473][-2.9067497 -3.7990868 -4.20135 -3.8382 -2.8302889 -1.506696 -0.49952269 0.063637257 -0.18186045 -0.94667792 -1.595701 -1.7385294 -1.4401603 -0.79741 -0.36730289][-2.4049938 -3.1137943 -3.7012317 -3.4461474 -2.610342 -1.7569387 -1.199821 -1.1830637 -1.6335618 -2.3765736 -2.839561 -2.8119993 -2.3652637 -1.6857526 -1.2663746][-2.8743773 -3.3088133 -3.6318512 -3.7729154 -3.5214055 -2.9780521 -2.6904402 -2.891027 -3.4349282 -3.8985479 -4.0837197 -3.7562964 -3.0271745 -2.1928358 -1.8181663][-3.1050704 -3.5678205 -3.9795535 -4.1276903 -4.1564946 -4.1229248 -4.133935 -4.3716283 -4.849699 -5.2406325 -5.1133485 -4.4501204 -3.4373293 -2.509202 -2.0080724][-2.8787956 -3.3078291 -3.6867476 -3.8980122 -4.022131 -4.2870111 -4.7866731 -5.2659426 -5.8165779 -6.03214 -5.670989 -4.9273548 -3.8786952 -2.797163 -2.2859328][-2.8983483 -3.0423613 -3.2912545 -3.6960664 -4.1471534 -4.5353475 -5.0237489 -5.6551251 -6.2096882 -6.3730693 -6.0013065 -5.11388 -4.0464354 -3.2532611 -2.9898567]]...]
INFO - root - 2017-12-16 08:01:11.591135: step 54810, loss = 0.39, batch loss = 0.33 (27.5 examples/sec; 0.291 sec/batch; 22h:25m:30s remains)
INFO - root - 2017-12-16 08:01:14.452495: step 54820, loss = 0.29, batch loss = 0.23 (28.5 examples/sec; 0.281 sec/batch; 21h:38m:38s remains)
INFO - root - 2017-12-16 08:01:17.295328: step 54830, loss = 0.29, batch loss = 0.23 (28.4 examples/sec; 0.282 sec/batch; 21h:44m:58s remains)
INFO - root - 2017-12-16 08:01:20.148942: step 54840, loss = 0.24, batch loss = 0.18 (26.1 examples/sec; 0.306 sec/batch; 23h:37m:53s remains)
INFO - root - 2017-12-16 08:01:22.954333: step 54850, loss = 0.24, batch loss = 0.18 (27.7 examples/sec; 0.288 sec/batch; 22h:14m:49s remains)
INFO - root - 2017-12-16 08:01:25.805950: step 54860, loss = 0.28, batch loss = 0.22 (28.0 examples/sec; 0.285 sec/batch; 22h:00m:28s remains)
INFO - root - 2017-12-16 08:01:28.646170: step 54870, loss = 0.44, batch loss = 0.38 (28.6 examples/sec; 0.280 sec/batch; 21h:34m:17s remains)
INFO - root - 2017-12-16 08:01:31.485969: step 54880, loss = 0.25, batch loss = 0.20 (27.5 examples/sec; 0.291 sec/batch; 22h:24m:48s remains)
INFO - root - 2017-12-16 08:01:34.321257: step 54890, loss = 0.33, batch loss = 0.27 (28.8 examples/sec; 0.277 sec/batch; 21h:23m:22s remains)
INFO - root - 2017-12-16 08:01:37.140989: step 54900, loss = 0.22, batch loss = 0.16 (28.3 examples/sec; 0.282 sec/batch; 21h:46m:00s remains)
2017-12-16 08:01:37.629826: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5772781 -3.3314662 -3.1520998 -2.9748573 -2.8967476 -3.0089397 -3.1242409 -3.2570028 -3.5411408 -3.6014411 -3.3959985 -3.0430572 -2.6758885 -2.374428 -1.9863322][-3.7483382 -3.3102064 -2.8032312 -2.5045135 -2.356307 -2.3755476 -2.4346013 -2.5741072 -2.7820449 -2.6260912 -2.4783583 -2.3704963 -2.1640995 -1.8368196 -1.5123305][-3.7091074 -3.1280146 -2.5630012 -2.0056326 -1.7788522 -1.9230139 -2.1343191 -2.0849969 -1.9609284 -1.9795966 -1.8356721 -1.6847639 -1.8050976 -1.7791815 -1.6759632][-3.0947897 -2.5709405 -2.1397295 -1.6695361 -1.2887077 -0.99119306 -0.99197769 -1.2367301 -1.2508028 -1.149591 -1.1416686 -1.5836444 -2.2313216 -2.4678621 -2.3862066][-2.4028704 -1.6185782 -0.85584259 -0.42145491 -0.21056747 -0.037443638 0.14651251 0.15427876 0.073216915 -0.096887112 -0.52732778 -1.20264 -2.0703635 -3.0499234 -3.3602467][-1.5670109 -0.67586756 0.20576477 0.75497389 1.2144589 1.4698529 1.5786948 1.5389442 1.4176893 1.1276422 0.52476645 -0.74087167 -2.1567736 -3.0326052 -3.5134454][-1.4399722 -0.20356846 0.87269974 1.6449471 2.2020984 2.5720105 2.9539504 2.8710027 2.6817226 2.1799989 1.2410684 -0.19685888 -2.0016615 -3.2336555 -3.8124423][-1.9589629 -0.69197631 0.5333271 1.4233084 2.0369263 2.6624928 3.4394951 3.4602981 3.1168389 2.5133319 1.2805347 -0.54731369 -2.5000415 -3.6234951 -3.9927447][-3.4209807 -2.4835157 -1.3180959 -0.25012827 0.68915796 1.6454773 2.602879 2.9242148 2.7961936 1.9676404 0.55566883 -1.3905022 -3.412745 -4.4452486 -4.6749845][-4.9080076 -4.50094 -3.7915688 -2.8202739 -1.6612511 -0.26406431 1.0386391 1.464942 1.4435787 0.6656723 -0.89193583 -2.9161839 -4.8383789 -5.5788555 -5.4076881][-5.8861666 -5.8032246 -5.4911966 -4.8834124 -4.0277843 -2.8102636 -1.4797473 -0.94375014 -1.0973549 -1.9614017 -3.3606687 -4.879087 -6.1556249 -6.5927286 -6.18627][-6.3879857 -6.4518862 -6.3098617 -6.0327778 -5.5258441 -4.5692358 -3.5434375 -3.1701069 -3.290493 -3.8370647 -4.8530602 -6.0827594 -6.9334764 -6.9379721 -6.3794427][-6.0304136 -6.2970352 -6.4416785 -6.2612505 -5.8761368 -5.3332295 -4.7611265 -4.4701858 -4.50542 -4.9228559 -5.5331941 -6.1101847 -6.470726 -6.2371345 -5.6731334][-5.5871415 -5.673543 -5.7883515 -5.8043289 -5.6900806 -5.3700442 -4.9442835 -4.8750415 -5.05043 -5.1306391 -5.3049479 -5.5372143 -5.5007296 -4.9959059 -4.3242598][-4.9749985 -4.954586 -4.9558978 -5.0304089 -5.08025 -5.0232515 -4.8887658 -4.9065642 -4.9914093 -4.9221692 -4.8687115 -4.7336984 -4.44414 -3.9483004 -3.3866026]]...]
INFO - root - 2017-12-16 08:01:40.441508: step 54910, loss = 0.23, batch loss = 0.18 (28.3 examples/sec; 0.282 sec/batch; 21h:46m:12s remains)
INFO - root - 2017-12-16 08:01:43.221034: step 54920, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 21h:33m:33s remains)
INFO - root - 2017-12-16 08:01:45.994153: step 54930, loss = 0.40, batch loss = 0.34 (28.7 examples/sec; 0.279 sec/batch; 21h:31m:17s remains)
INFO - root - 2017-12-16 08:01:48.834137: step 54940, loss = 0.30, batch loss = 0.25 (28.7 examples/sec; 0.279 sec/batch; 21h:28m:40s remains)
INFO - root - 2017-12-16 08:01:51.685188: step 54950, loss = 0.22, batch loss = 0.17 (29.0 examples/sec; 0.276 sec/batch; 21h:18m:13s remains)
INFO - root - 2017-12-16 08:01:54.562215: step 54960, loss = 0.24, batch loss = 0.18 (27.8 examples/sec; 0.288 sec/batch; 22h:12m:25s remains)
INFO - root - 2017-12-16 08:01:57.420009: step 54970, loss = 0.28, batch loss = 0.23 (25.4 examples/sec; 0.315 sec/batch; 24h:15m:29s remains)
INFO - root - 2017-12-16 08:02:00.281880: step 54980, loss = 0.32, batch loss = 0.26 (28.6 examples/sec; 0.280 sec/batch; 21h:35m:26s remains)
INFO - root - 2017-12-16 08:02:03.145832: step 54990, loss = 0.20, batch loss = 0.14 (28.4 examples/sec; 0.281 sec/batch; 21h:40m:56s remains)
INFO - root - 2017-12-16 08:02:05.965146: step 55000, loss = 0.31, batch loss = 0.25 (29.1 examples/sec; 0.275 sec/batch; 21h:10m:34s remains)
2017-12-16 08:02:06.518974: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9883127 -5.2272439 -4.092792 -2.9913929 -2.0882187 -1.800869 -1.6629539 -1.9432578 -2.7291911 -3.5462725 -4.0960917 -4.3999052 -4.3861361 -3.9628637 -3.3117735][-5.2473607 -4.45544 -3.4301853 -2.4201849 -1.6747878 -1.2714038 -1.2328763 -1.6026623 -2.3514864 -3.1887708 -3.8602381 -4.2128506 -4.250845 -3.9397287 -3.360394][-4.2712049 -3.5499473 -2.7461507 -1.9478924 -1.3565216 -1.1009459 -1.0607996 -1.4429431 -2.2358294 -3.2008982 -4.0306649 -4.553659 -4.5995631 -4.2400613 -3.6255355][-2.8964109 -2.4047477 -1.7637467 -1.3160734 -0.94267344 -0.69508433 -0.65051031 -0.98468137 -1.8862028 -2.8987594 -3.8181317 -4.4428897 -4.6534557 -4.2413616 -3.6358609][-1.8595414 -1.5201364 -1.0375247 -0.74175072 -0.17765665 0.38809061 0.70699978 0.58507919 -0.29347467 -1.5997391 -2.9873796 -4.0057349 -4.3827267 -4.3173766 -3.7056324][-1.5025253 -1.3043978 -0.95902252 -0.6417048 0.20933867 1.2249246 2.0222273 2.0608454 1.3254032 -0.23709774 -1.9879475 -3.515769 -4.4718041 -4.6671486 -4.2351747][-1.6566877 -1.6031487 -1.3005593 -0.76017213 0.31892347 1.6064034 2.6591759 2.8819852 2.0676885 0.38026762 -1.6694264 -3.5781684 -4.7277746 -5.1918249 -4.8734679][-1.9865205 -1.8436792 -1.6324699 -1.0352321 0.2986517 1.6564775 2.6430683 3.0006719 2.3147459 0.52331877 -1.6874614 -3.601486 -4.9458556 -5.5132313 -5.2131472][-2.4086924 -2.3584175 -2.0823774 -1.5166814 -0.23028564 1.1652994 2.3231492 2.7245774 1.9692059 0.36651564 -1.7717907 -3.8481126 -5.3387504 -5.9709606 -5.6718278][-2.4413235 -2.5788271 -2.45752 -2.0204012 -1.0687058 0.10107565 1.179915 1.7547169 1.2736592 -0.22566891 -2.3161428 -4.2952008 -5.7690592 -6.4516029 -6.1517706][-2.1064479 -2.4908075 -2.8173325 -2.6966095 -1.8888094 -0.82606244 0.15623236 0.62047386 0.21531439 -1.1204064 -2.9728174 -4.82463 -6.133584 -6.6310725 -6.30116][-2.1568234 -2.7457731 -3.0625234 -3.0277736 -2.4383183 -1.4494383 -0.58346653 -0.19919252 -0.58969045 -1.8234885 -3.5536163 -5.2288194 -6.3044243 -6.5234003 -5.9868026][-2.1665051 -2.9072812 -3.357748 -3.3513265 -2.6894128 -1.7915721 -1.1427066 -0.75034595 -1.231215 -2.36203 -3.7943435 -5.16702 -5.8981881 -6.014082 -5.5197248][-2.2222736 -2.9886115 -3.5008311 -3.4081347 -2.8832006 -2.0186479 -1.3588929 -1.2279651 -1.698977 -2.6974688 -4.0163946 -5.0325222 -5.5652709 -5.54298 -4.9147468][-2.8631506 -3.5224597 -3.8034573 -3.5697377 -2.9062152 -2.0125611 -1.4150362 -1.3042445 -1.7733493 -2.828999 -3.9702833 -4.8881431 -5.2864623 -5.0446625 -4.3721566]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-55000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-55000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 08:02:10.044536: step 55010, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.284 sec/batch; 21h:54m:06s remains)
INFO - root - 2017-12-16 08:02:12.898921: step 55020, loss = 0.31, batch loss = 0.25 (27.1 examples/sec; 0.295 sec/batch; 22h:45m:58s remains)
INFO - root - 2017-12-16 08:02:15.719197: step 55030, loss = 0.21, batch loss = 0.15 (28.5 examples/sec; 0.280 sec/batch; 21h:36m:11s remains)
INFO - root - 2017-12-16 08:02:18.591075: step 55040, loss = 0.36, batch loss = 0.30 (28.7 examples/sec; 0.279 sec/batch; 21h:28m:10s remains)
INFO - root - 2017-12-16 08:02:21.388708: step 55050, loss = 0.27, batch loss = 0.21 (29.5 examples/sec; 0.271 sec/batch; 20h:54m:46s remains)
INFO - root - 2017-12-16 08:02:24.245171: step 55060, loss = 0.23, batch loss = 0.17 (26.5 examples/sec; 0.302 sec/batch; 23h:16m:46s remains)
INFO - root - 2017-12-16 08:02:27.100279: step 55070, loss = 0.29, batch loss = 0.24 (28.0 examples/sec; 0.285 sec/batch; 21h:59m:14s remains)
INFO - root - 2017-12-16 08:02:29.946837: step 55080, loss = 0.22, batch loss = 0.16 (29.0 examples/sec; 0.276 sec/batch; 21h:15m:12s remains)
INFO - root - 2017-12-16 08:02:32.752244: step 55090, loss = 0.27, batch loss = 0.22 (27.7 examples/sec; 0.288 sec/batch; 22h:13m:41s remains)
INFO - root - 2017-12-16 08:02:35.579959: step 55100, loss = 0.28, batch loss = 0.22 (28.2 examples/sec; 0.284 sec/batch; 21h:51m:06s remains)
2017-12-16 08:02:36.080150: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4449964 -4.2059035 -4.07043 -4.1452756 -4.3015065 -4.27772 -4.041431 -3.7140307 -3.3472657 -3.0094364 -2.6071615 -2.372473 -2.3205805 -2.3483326 -2.3626301][-4.0029578 -3.8119593 -3.7773576 -3.77041 -3.8287811 -3.9124267 -3.8638947 -3.5671678 -3.0938294 -2.7411938 -2.5138073 -2.3484983 -2.2690623 -2.1965406 -2.1301532][-3.2410843 -3.0586743 -3.2876897 -3.5057359 -3.5689173 -3.448813 -3.3704348 -3.2326736 -2.9330182 -2.8275466 -2.6911585 -2.5133176 -2.4397743 -2.3851097 -2.1780241][-2.3188207 -2.3616233 -2.7644858 -3.0676744 -3.1997538 -3.0729818 -2.7074018 -2.4522097 -2.5351682 -2.6880999 -2.7122526 -2.8563466 -3.0418026 -2.9157085 -2.5449934][-1.4450598 -1.6498153 -2.0430262 -2.3657448 -2.386061 -2.0246589 -1.4969893 -1.2134848 -1.3158786 -1.8068454 -2.2397277 -2.5629396 -2.956713 -3.1413903 -2.9655302][-0.936239 -1.2099292 -1.6244779 -1.6165617 -1.0737271 -0.54035759 0.037581444 0.42257786 0.37068939 -0.17636967 -1.0611527 -2.0314074 -2.7977509 -3.1465378 -3.0362444][-0.92013979 -1.1017487 -1.4769964 -1.3951068 -0.50011921 0.81284618 2.0894756 2.3416333 1.9679036 1.3785849 0.33669424 -1.0257795 -2.1654863 -2.8169484 -3.0253592][-1.1822987 -1.4278524 -1.5037043 -1.0098071 0.090274334 1.5941324 3.1587605 3.8922348 3.6034374 2.5914059 1.2655554 -0.36093521 -1.8298094 -2.6650338 -2.9650555][-1.8509719 -2.0373652 -1.9307742 -1.4200563 -0.22982025 1.4341755 3.0620475 3.9462757 3.8298206 2.782661 1.2662029 -0.44784331 -1.9196429 -2.7964668 -3.2233939][-2.2713454 -2.7848153 -3.1080849 -2.5554547 -1.4046292 -0.038998127 1.2597885 2.153656 2.3238878 1.4038587 -0.096311092 -1.7430353 -2.9472542 -3.4364185 -3.5012186][-3.1726437 -3.6065836 -3.7067611 -3.4497609 -2.8531685 -1.5930037 -0.26960945 0.34032917 0.13042974 -0.76428628 -1.8693573 -3.0689962 -4.056087 -4.2176132 -3.9570365][-3.6163325 -4.2352004 -4.7349577 -4.44762 -3.7681222 -2.9805803 -2.187763 -1.6590762 -1.5476046 -2.1055756 -3.040611 -4.1297235 -4.8341241 -4.7041044 -4.2764754][-3.2776425 -3.8739204 -4.3645554 -4.4740973 -4.391427 -3.764509 -3.1021814 -2.8686433 -2.8829975 -3.126025 -3.6624312 -4.3413382 -4.8043189 -4.6025906 -4.2823443][-3.2785826 -3.3891964 -3.6412902 -3.9210372 -3.9801617 -3.4999156 -3.0025477 -2.8987277 -2.9304805 -2.9640436 -3.1237116 -3.7284889 -4.3439546 -4.2652168 -4.0489235][-3.2609034 -3.0796652 -3.1022632 -3.2912176 -3.3306384 -2.9819686 -2.5428 -2.2611232 -2.1350617 -2.0715814 -2.2817082 -2.5843134 -3.0080004 -3.3406343 -3.52174]]...]
INFO - root - 2017-12-16 08:02:38.879992: step 55110, loss = 0.28, batch loss = 0.22 (28.6 examples/sec; 0.280 sec/batch; 21h:32m:52s remains)
INFO - root - 2017-12-16 08:02:41.691232: step 55120, loss = 0.36, batch loss = 0.30 (28.5 examples/sec; 0.281 sec/batch; 21h:39m:02s remains)
INFO - root - 2017-12-16 08:02:44.458140: step 55130, loss = 0.22, batch loss = 0.16 (29.1 examples/sec; 0.275 sec/batch; 21h:09m:09s remains)
INFO - root - 2017-12-16 08:02:47.360720: step 55140, loss = 0.26, batch loss = 0.21 (28.3 examples/sec; 0.283 sec/batch; 21h:48m:29s remains)
INFO - root - 2017-12-16 08:02:50.200529: step 55150, loss = 0.31, batch loss = 0.25 (28.5 examples/sec; 0.281 sec/batch; 21h:39m:03s remains)
INFO - root - 2017-12-16 08:02:53.065896: step 55160, loss = 0.21, batch loss = 0.15 (28.1 examples/sec; 0.285 sec/batch; 21h:58m:13s remains)
INFO - root - 2017-12-16 08:02:55.919934: step 55170, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.283 sec/batch; 21h:50m:17s remains)
INFO - root - 2017-12-16 08:02:58.712557: step 55180, loss = 0.32, batch loss = 0.26 (28.2 examples/sec; 0.284 sec/batch; 21h:53m:14s remains)
INFO - root - 2017-12-16 08:03:01.545204: step 55190, loss = 0.22, batch loss = 0.17 (27.7 examples/sec; 0.289 sec/batch; 22h:16m:11s remains)
INFO - root - 2017-12-16 08:03:04.411305: step 55200, loss = 0.29, batch loss = 0.23 (28.9 examples/sec; 0.277 sec/batch; 21h:18m:02s remains)
2017-12-16 08:03:04.957661: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6894376 -4.4578533 -5.1890407 -5.8836384 -6.4115677 -6.7326765 -7.2630234 -8.2561646 -9.1380177 -9.4140406 -9.3657494 -9.1306877 -8.4783125 -7.7035489 -7.2523785][-4.236177 -5.2581258 -6.1345539 -6.8541842 -7.3786674 -7.9642425 -8.5815334 -8.9521713 -9.2860584 -9.7518215 -9.7695122 -9.3419666 -8.6062546 -7.8361621 -7.3673868][-4.5860076 -5.7291479 -6.4804335 -6.92989 -6.9943309 -7.0510364 -7.2387314 -7.6460156 -8.0410452 -8.1135159 -8.1821623 -8.2264137 -7.8595858 -7.1865139 -6.772522][-4.9610448 -5.9249392 -6.6426153 -6.7524438 -6.2029104 -5.4522057 -4.8180466 -4.529902 -4.5703025 -5.0231152 -5.5013828 -5.9631853 -6.1359186 -5.7802029 -5.6067643][-4.7828236 -5.5523844 -5.8871455 -5.5245934 -4.6812177 -3.3174376 -1.978538 -1.2319245 -1.2208228 -1.5599821 -2.23343 -3.0891409 -3.471981 -3.8160863 -4.0687943][-4.3946724 -4.9022756 -4.8666267 -4.0282192 -2.218411 -0.20025015 1.2343512 2.1349416 2.2417865 1.3603983 0.24622059 -0.67548203 -1.3538377 -1.7709208 -2.0710945][-3.8987043 -4.0582113 -3.8665283 -2.743654 -0.86092448 1.4969678 3.5410624 4.342535 3.8202524 3.0860333 2.2410498 1.008944 0.15709114 -0.39990187 -0.62842464][-3.7887635 -3.6696126 -3.0939136 -1.7889168 0.19118881 2.4239168 3.9613037 4.6510372 4.466486 3.5176611 2.6657882 1.8788247 1.1783504 0.55246353 0.21438551][-3.6380403 -3.5353966 -2.8700728 -1.5083458 0.20216465 1.9555254 3.2585392 3.7111597 3.3649502 2.8129892 2.1851563 1.3242669 0.5331254 -0.253345 -0.74616528][-3.4805493 -3.4613647 -3.0586123 -2.0959911 -0.85639238 0.53557873 1.2843404 1.4472032 1.3172655 0.77678204 0.35923052 -0.2085228 -0.94748569 -1.6870265 -2.1721296][-3.6577184 -3.6684897 -3.5397692 -2.9963017 -2.205004 -1.5085924 -0.95980382 -0.94378066 -1.1627812 -1.4988389 -1.9442444 -2.5035939 -3.1828818 -3.813308 -4.0797696][-4.116087 -4.1318808 -3.9057047 -3.8097353 -3.7397456 -3.4958673 -3.4441805 -3.5818889 -3.6807046 -4.1914005 -4.7280331 -5.3563728 -5.9775872 -6.3229003 -6.3565903][-4.6338515 -4.8314819 -4.9131327 -4.9716821 -4.9388008 -5.1508842 -5.392345 -5.6566682 -5.8903565 -6.0494037 -6.3781281 -6.6367884 -7.0312319 -7.702548 -7.6023917][-4.2810912 -4.5009031 -4.66019 -4.8454952 -5.0272932 -5.2255487 -5.5905852 -5.892231 -6.0889568 -6.3122296 -6.4853477 -6.9430389 -7.3689089 -7.5969563 -7.4327545][-4.2792654 -4.28692 -4.2999616 -4.5351248 -4.8216243 -5.2395496 -5.670805 -5.9154387 -5.9811864 -5.9929862 -6.2498913 -6.3929367 -6.49043 -6.8825922 -6.8401661]]...]
INFO - root - 2017-12-16 08:03:07.775336: step 55210, loss = 0.24, batch loss = 0.18 (27.9 examples/sec; 0.286 sec/batch; 22h:02m:57s remains)
INFO - root - 2017-12-16 08:03:10.586590: step 55220, loss = 0.35, batch loss = 0.30 (28.9 examples/sec; 0.277 sec/batch; 21h:18m:02s remains)
INFO - root - 2017-12-16 08:03:13.412320: step 55230, loss = 0.25, batch loss = 0.19 (27.5 examples/sec; 0.291 sec/batch; 22h:24m:51s remains)
INFO - root - 2017-12-16 08:03:16.267069: step 55240, loss = 0.41, batch loss = 0.35 (28.9 examples/sec; 0.276 sec/batch; 21h:17m:36s remains)
INFO - root - 2017-12-16 08:03:19.071347: step 55250, loss = 0.30, batch loss = 0.24 (28.4 examples/sec; 0.282 sec/batch; 21h:43m:29s remains)
INFO - root - 2017-12-16 08:03:21.947415: step 55260, loss = 0.27, batch loss = 0.22 (27.1 examples/sec; 0.296 sec/batch; 22h:45m:31s remains)
INFO - root - 2017-12-16 08:03:24.736417: step 55270, loss = 0.36, batch loss = 0.30 (29.7 examples/sec; 0.269 sec/batch; 20h:43m:08s remains)
INFO - root - 2017-12-16 08:03:27.583587: step 55280, loss = 0.23, batch loss = 0.17 (26.9 examples/sec; 0.297 sec/batch; 22h:52m:23s remains)
INFO - root - 2017-12-16 08:03:30.412523: step 55290, loss = 0.23, batch loss = 0.18 (28.6 examples/sec; 0.279 sec/batch; 21h:30m:47s remains)
INFO - root - 2017-12-16 08:03:33.240957: step 55300, loss = 0.22, batch loss = 0.16 (27.9 examples/sec; 0.287 sec/batch; 22h:04m:41s remains)
2017-12-16 08:03:33.748306: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.2025633 -7.4284482 -7.3671794 -6.7530146 -6.1730552 -5.708663 -5.1932459 -4.7617679 -4.50784 -4.7982955 -5.3473082 -5.9451227 -6.7613769 -7.2760544 -7.33039][-7.8747187 -8.3196049 -8.3917713 -7.8721943 -7.15891 -6.4520206 -5.6578665 -5.0484118 -4.6891575 -5.0285497 -5.7472134 -6.5366664 -7.2923751 -7.707099 -7.601366][-7.8112812 -8.2456207 -8.2536268 -7.6450262 -6.9917836 -6.2535219 -5.3819017 -4.9315319 -4.6432619 -4.6102076 -5.170105 -6.1315727 -7.175498 -7.5508261 -7.3768959][-6.7004023 -7.0663819 -6.8081627 -6.014504 -5.081316 -4.4139218 -3.7831614 -3.7106175 -3.7610087 -3.9241724 -4.4015203 -5.0564852 -5.77221 -6.1867805 -6.3008118][-5.1663804 -5.2068162 -4.8854022 -4.2503457 -3.4306827 -2.4617481 -1.744957 -1.6602869 -1.6225924 -1.8633733 -2.4939489 -2.8522077 -3.43286 -4.0615296 -4.6699109][-3.9670007 -3.4674997 -2.485543 -1.343781 -0.28679037 0.25926065 0.47958612 0.50307846 0.34079313 0.020226955 -0.6401701 -1.1746597 -1.9297888 -2.5647781 -3.3429527][-2.9236083 -2.1047328 -0.98203826 0.43925762 1.9991045 2.948441 3.2560039 2.667089 2.1996965 1.7795734 1.0079179 0.0082154274 -1.0710392 -2.2682922 -3.2614348][-2.8564062 -1.9517393 -0.6627717 0.91327333 2.4518995 3.6135778 4.2159653 4.01116 3.3879695 2.6351748 1.7095466 0.53610611 -0.82528973 -2.297509 -3.48856][-3.5143142 -2.7595625 -1.5269439 -0.13547564 1.2439547 2.524538 3.38415 3.4320683 2.8957438 2.0331726 0.97471571 -0.4034009 -1.7784214 -3.073483 -4.2669005][-4.0315762 -3.9427826 -3.3658586 -2.2346666 -1.0213192 0.048221111 0.97377157 1.3497624 1.0884314 0.2872653 -0.65283227 -1.9858 -3.2918549 -4.3472271 -5.0215864][-5.6390133 -5.410099 -4.81671 -4.4116807 -3.9059885 -2.8568072 -1.8932161 -1.4477839 -1.3932302 -1.8539784 -2.3885379 -3.4400978 -4.4443455 -5.0495977 -5.2806778][-6.8279824 -6.8836613 -6.7768869 -6.2781811 -5.8673759 -5.3664351 -4.8331738 -4.212111 -3.7376575 -3.8024802 -3.9432967 -4.3847427 -4.852387 -5.0473042 -5.003726][-7.6676111 -7.7922373 -7.9072962 -7.7235489 -7.554882 -7.1703844 -6.8807259 -6.4581852 -6.04432 -5.7897377 -5.4902935 -5.4298515 -5.4435186 -5.1077275 -4.6713572][-7.7949357 -7.6780043 -7.7874169 -7.7577267 -7.7024517 -7.4645376 -7.1799355 -7.0168638 -6.8242416 -6.5138073 -6.1375132 -5.8485465 -5.6031504 -5.1673965 -4.666925][-7.2046666 -6.7587075 -6.5530152 -6.7001448 -6.8818316 -6.87062 -6.8376522 -6.6735115 -6.4154272 -6.1689548 -5.9243803 -5.6042962 -5.3068175 -4.9533262 -4.581779]]...]
INFO - root - 2017-12-16 08:03:36.593623: step 55310, loss = 0.26, batch loss = 0.20 (28.8 examples/sec; 0.278 sec/batch; 21h:25m:30s remains)
INFO - root - 2017-12-16 08:03:39.494761: step 55320, loss = 0.34, batch loss = 0.28 (27.1 examples/sec; 0.296 sec/batch; 22h:46m:11s remains)
INFO - root - 2017-12-16 08:03:42.330028: step 55330, loss = 0.24, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 21h:46m:39s remains)
INFO - root - 2017-12-16 08:03:45.122698: step 55340, loss = 0.22, batch loss = 0.16 (28.2 examples/sec; 0.283 sec/batch; 21h:48m:20s remains)
INFO - root - 2017-12-16 08:03:47.929291: step 55350, loss = 0.25, batch loss = 0.19 (28.3 examples/sec; 0.283 sec/batch; 21h:46m:14s remains)
INFO - root - 2017-12-16 08:03:50.764975: step 55360, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.285 sec/batch; 21h:58m:12s remains)
INFO - root - 2017-12-16 08:03:53.620848: step 55370, loss = 0.30, batch loss = 0.24 (27.5 examples/sec; 0.291 sec/batch; 22h:24m:56s remains)
INFO - root - 2017-12-16 08:03:56.451722: step 55380, loss = 0.20, batch loss = 0.14 (28.3 examples/sec; 0.282 sec/batch; 21h:44m:00s remains)
INFO - root - 2017-12-16 08:03:59.286998: step 55390, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.280 sec/batch; 21h:35m:00s remains)
INFO - root - 2017-12-16 08:04:02.134367: step 55400, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 21h:42m:22s remains)
2017-12-16 08:04:02.685115: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.06113 -3.9362855 -3.855787 -4.1035705 -4.2956781 -4.3853745 -4.4339414 -4.3235412 -4.3405151 -4.0674272 -3.652396 -3.2911615 -3.2870383 -3.5806665 -3.642988][-3.4224029 -3.3393354 -3.2712622 -3.4523144 -3.6470068 -3.846087 -3.8362651 -3.8010204 -3.8071256 -3.5230803 -3.2004356 -2.7716641 -2.4823961 -2.5973768 -2.663511][-2.8303094 -2.7852402 -2.7732248 -2.936121 -3.0813951 -3.2250085 -3.2819247 -3.2488298 -3.1792345 -2.9524276 -2.6404219 -2.1680863 -1.8401291 -1.6133473 -1.5054424][-1.9670076 -1.9930675 -2.0717642 -2.1616406 -2.2457836 -2.3852551 -2.4667935 -2.5959206 -2.6042547 -2.4607987 -2.2924154 -1.7296164 -1.2866082 -0.90451574 -0.53538084][-1.1206901 -1.1827097 -1.339514 -1.4780505 -1.4370432 -1.4639149 -1.4760411 -1.5950198 -1.7512722 -1.7607996 -1.7200906 -1.3002942 -0.79091954 -0.34849358 -0.028198719][-0.42574644 -0.63211632 -0.75930333 -0.82938647 -0.6299572 -0.40971994 -0.25759935 -0.3231864 -0.53899884 -0.84454083 -0.99045491 -0.7466917 -0.39045668 -0.0039753914 0.19135666][-0.269794 -0.37728167 -0.38117027 -0.28620577 0.087205887 0.33447933 0.62629557 0.57751894 0.34115028 -0.045445919 -0.50033522 -0.49578643 -0.31685305 0.055633545 0.39869738][-0.1734767 -0.22861671 -0.23471975 -0.0787344 0.28519773 0.51704168 0.75344753 0.69261074 0.44208288 0.027209759 -0.4615798 -0.47006559 -0.4121418 -0.12743425 0.21779871][-0.15450096 -0.13140774 -0.15856457 -0.097930908 -0.024273396 0.10175228 0.31176853 0.33088064 0.1143856 -0.30816174 -0.69840837 -0.8799293 -1.0011883 -0.76898837 -0.50844741][-0.14252567 -0.22542095 -0.57877135 -0.61206484 -0.59741521 -0.66256762 -0.6324544 -0.56990671 -0.71975613 -0.92587757 -1.1093955 -1.4307232 -1.7141094 -1.5646496 -1.2303183][-0.20605373 -0.25700474 -0.74140763 -1.0974042 -1.358948 -1.487186 -1.4797058 -1.5014975 -1.7407374 -1.9544406 -2.21259 -2.4888887 -2.6204221 -2.4476414 -2.0448108][-0.61491895 -0.74740243 -1.1242199 -1.555295 -1.9602594 -2.1304553 -2.1924911 -2.3119764 -2.6321158 -2.8683228 -3.2378669 -3.6308279 -3.8219924 -3.5185857 -2.8719292][-0.72330332 -0.71660805 -1.0418918 -1.5580585 -1.9371817 -2.2068837 -2.364491 -2.4214754 -2.7931876 -3.1418686 -3.7012558 -4.1918497 -4.3622694 -4.0583539 -3.4113235][-0.75885177 -0.676754 -0.93685412 -1.2203259 -1.6066432 -1.7928324 -2.0242419 -2.3431385 -2.7522464 -2.9829068 -3.3917325 -3.9619634 -4.2737422 -3.9033787 -3.0451629][-0.92777967 -0.77727461 -0.91805887 -0.95217109 -1.0429947 -1.0599518 -1.2507365 -1.6302884 -2.0841537 -2.5252709 -3.0683851 -3.5427489 -3.7990425 -3.4967766 -2.7136798]]...]
INFO - root - 2017-12-16 08:04:05.480605: step 55410, loss = 0.28, batch loss = 0.23 (27.9 examples/sec; 0.287 sec/batch; 22h:06m:11s remains)
INFO - root - 2017-12-16 08:04:08.266388: step 55420, loss = 0.28, batch loss = 0.22 (28.5 examples/sec; 0.280 sec/batch; 21h:34m:05s remains)
INFO - root - 2017-12-16 08:04:11.140195: step 55430, loss = 0.26, batch loss = 0.20 (28.5 examples/sec; 0.281 sec/batch; 21h:35m:58s remains)
INFO - root - 2017-12-16 08:04:13.986176: step 55440, loss = 0.25, batch loss = 0.20 (28.4 examples/sec; 0.282 sec/batch; 21h:41m:44s remains)
INFO - root - 2017-12-16 08:04:16.832085: step 55450, loss = 0.31, batch loss = 0.26 (28.9 examples/sec; 0.277 sec/batch; 21h:16m:50s remains)
INFO - root - 2017-12-16 08:04:19.659097: step 55460, loss = 0.30, batch loss = 0.24 (29.1 examples/sec; 0.275 sec/batch; 21h:07m:54s remains)
INFO - root - 2017-12-16 08:04:22.523076: step 55470, loss = 0.30, batch loss = 0.24 (28.4 examples/sec; 0.282 sec/batch; 21h:40m:22s remains)
INFO - root - 2017-12-16 08:04:25.367234: step 55480, loss = 0.24, batch loss = 0.18 (29.6 examples/sec; 0.270 sec/batch; 20h:48m:00s remains)
INFO - root - 2017-12-16 08:04:28.218909: step 55490, loss = 0.22, batch loss = 0.17 (28.5 examples/sec; 0.281 sec/batch; 21h:36m:57s remains)
INFO - root - 2017-12-16 08:04:31.050271: step 55500, loss = 0.36, batch loss = 0.30 (28.1 examples/sec; 0.285 sec/batch; 21h:55m:31s remains)
2017-12-16 08:04:31.583769: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1482959 -3.1970248 -3.2359443 -3.2899308 -3.3123498 -3.2571931 -3.1524403 -3.0011592 -2.9957943 -3.1689382 -3.4319184 -3.7977204 -4.2579727 -4.7340088 -5.3621974][-3.4394855 -3.5640893 -3.6608126 -3.7180829 -3.6501584 -3.431417 -3.2278585 -3.0063596 -2.7747836 -2.6907883 -2.8639452 -3.12264 -3.482852 -3.9418182 -4.6359062][-3.6797316 -3.9518173 -4.0737028 -4.0345516 -3.9393375 -3.6202595 -3.1272407 -2.6828988 -2.4117448 -2.085845 -2.0418386 -2.3282723 -2.7902484 -3.2777264 -3.9148498][-3.5969229 -4.0614028 -4.2420783 -4.2004485 -3.8216941 -3.1491356 -2.4013727 -1.8953226 -1.5676043 -1.3243163 -1.456908 -1.6839542 -2.2238142 -2.8495843 -3.57223][-3.7685614 -4.1647391 -4.1499772 -3.9252882 -3.3074865 -2.3315325 -1.1637397 -0.46897578 -0.21148252 -0.12362623 -0.49272442 -1.1304793 -1.8804383 -2.6296592 -3.4680922][-4.041069 -4.3764672 -4.1568518 -3.5591102 -2.4279127 -1.0579007 0.36621523 1.2230549 1.5277877 1.3184848 0.63171816 -0.32208014 -1.4844749 -2.4901085 -3.3648081][-3.9232717 -4.0999813 -3.624784 -2.7372079 -1.2671888 0.21439552 1.8788981 2.7105355 2.8185205 2.4483094 1.5572433 0.25269794 -1.2278001 -2.547904 -3.6492016][-3.8477731 -3.9846137 -3.5537641 -2.557539 -0.94681072 0.81940365 2.6008105 3.3411374 3.3301787 2.7496896 1.6234884 0.12386656 -1.5516312 -2.9186597 -4.024467][-4.7242937 -4.6776586 -3.9726787 -2.8929248 -1.4306638 0.099374771 1.7226639 2.48097 2.5487027 1.9564266 0.91357708 -0.5065639 -1.9965398 -3.3596153 -4.5013123][-5.1868534 -5.1230392 -4.6862073 -3.6625171 -2.2850471 -1.0623667 0.15770388 0.7964282 0.93060541 0.41906738 -0.36465311 -1.4158044 -2.6116672 -3.7425337 -4.6830335][-5.9161072 -5.812664 -5.50026 -4.6124763 -3.3749878 -2.2785919 -1.2624366 -0.83299494 -0.7421186 -1.0068059 -1.4357371 -2.1409466 -3.0130944 -3.8223395 -4.6380758][-6.1444983 -6.0375042 -5.8228865 -5.1713 -4.33887 -3.3912578 -2.4750438 -2.0976419 -1.8862405 -1.9731028 -2.1619475 -2.4215302 -2.9212949 -3.5448573 -4.2077866][-5.8560648 -5.8240137 -5.62245 -5.06955 -4.3457265 -3.7134056 -3.161552 -2.783325 -2.3566213 -2.1429992 -2.1399398 -2.3212631 -2.6852636 -3.0951288 -3.724658][-5.5175977 -5.5095325 -5.2956386 -4.65899 -4.0621705 -3.4541967 -2.8403254 -2.4599891 -2.0331991 -1.6769753 -1.5487525 -1.6767194 -2.0212231 -2.4306884 -3.0563369][-4.8909836 -4.8058252 -4.5573711 -4.1451144 -3.6215327 -3.0737407 -2.5130239 -2.0425642 -1.5819793 -1.1526995 -0.91567659 -0.98621607 -1.3569186 -1.7566729 -2.3002825]]...]
INFO - root - 2017-12-16 08:04:34.400345: step 55510, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.282 sec/batch; 21h:43m:30s remains)
INFO - root - 2017-12-16 08:04:37.225041: step 55520, loss = 0.32, batch loss = 0.26 (28.0 examples/sec; 0.285 sec/batch; 21h:56m:51s remains)
INFO - root - 2017-12-16 08:04:39.999709: step 55530, loss = 0.23, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 21h:22m:41s remains)
INFO - root - 2017-12-16 08:04:42.846122: step 55540, loss = 0.31, batch loss = 0.26 (27.6 examples/sec; 0.290 sec/batch; 22h:17m:15s remains)
INFO - root - 2017-12-16 08:04:45.702393: step 55550, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 21h:55m:55s remains)
INFO - root - 2017-12-16 08:04:48.542259: step 55560, loss = 0.23, batch loss = 0.17 (27.6 examples/sec; 0.290 sec/batch; 22h:19m:15s remains)
INFO - root - 2017-12-16 08:04:51.363863: step 55570, loss = 0.37, batch loss = 0.31 (28.2 examples/sec; 0.284 sec/batch; 21h:51m:26s remains)
INFO - root - 2017-12-16 08:04:54.212715: step 55580, loss = 0.21, batch loss = 0.15 (27.8 examples/sec; 0.288 sec/batch; 22h:08m:56s remains)
INFO - root - 2017-12-16 08:04:57.060047: step 55590, loss = 0.23, batch loss = 0.17 (27.5 examples/sec; 0.290 sec/batch; 22h:20m:36s remains)
INFO - root - 2017-12-16 08:04:59.939943: step 55600, loss = 0.30, batch loss = 0.24 (28.1 examples/sec; 0.284 sec/batch; 21h:52m:57s remains)
2017-12-16 08:05:00.487935: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2103753 -3.2095666 -3.3738823 -3.602731 -3.9374506 -4.2509284 -4.6565428 -5.2146449 -5.6741152 -5.6451235 -5.2373095 -4.6852918 -4.0268965 -3.3406286 -2.7336667][-3.793685 -3.7674613 -3.9475119 -4.187634 -4.4890594 -4.7738848 -5.0083013 -5.2211337 -5.5421143 -5.8642325 -5.7538404 -5.2044659 -4.5220909 -3.8224139 -3.1713686][-4.1291151 -4.3370066 -4.5709953 -4.5937614 -4.5266728 -4.4977674 -4.5774136 -4.7817817 -4.8935013 -5.2580643 -5.5956774 -5.4471712 -4.873714 -4.182117 -3.5220501][-3.9637871 -3.9685965 -4.2068038 -4.3591366 -4.2318583 -3.7267487 -3.2709594 -3.3310211 -3.7774024 -4.3257155 -4.8234777 -5.0743608 -5.0342603 -4.4087281 -3.7234237][-3.3448453 -3.2319443 -3.285645 -3.263021 -2.8548234 -2.1084909 -1.5116634 -1.4681637 -1.8144095 -2.9907486 -4.1782088 -4.8427529 -4.8990936 -4.4971895 -3.9178791][-1.9555056 -1.8824761 -2.0113831 -1.7879817 -1.0099704 0.012260914 0.69960737 0.55910921 -0.14586878 -1.5774403 -3.156898 -4.2447925 -4.6629348 -4.4477482 -3.9897099][-0.76311636 -0.579571 -0.57292247 -0.32397461 0.5246706 1.7202392 2.5308642 2.2615371 1.200048 -0.51062465 -2.2799382 -3.4358025 -4.049943 -4.14758 -3.8894606][-0.16132164 0.037660122 0.23237658 0.65073633 1.4163203 2.4894276 3.2036085 2.7874403 1.6306543 -0.16885471 -1.8838718 -3.0461764 -3.7799466 -3.9772327 -3.869576][0.050948143 0.14088678 0.40298367 0.99672842 1.6404309 2.3702402 2.8310714 2.5003633 1.4390068 -0.12395048 -1.626785 -2.6671181 -3.4387848 -3.9085796 -4.0842285][0.11754656 0.05836153 0.15654516 0.69919825 1.2609296 1.5833068 1.7133732 1.5091767 0.78560495 -0.32159185 -1.3821106 -2.2637715 -3.0668101 -3.7922761 -4.2529011][0.0076789856 -0.12169456 -0.1326623 0.15039968 0.53131151 0.68193054 0.61895561 0.55965424 0.13117933 -0.70326495 -1.6066966 -2.4486766 -3.3569059 -4.1182594 -4.6410732][-0.22461271 -0.24987507 -0.2448597 -0.1989255 -0.12015724 -0.16484976 -0.2739954 -0.2809062 -0.47716165 -1.0179353 -1.774801 -2.640893 -3.5984726 -4.5554295 -5.2252693][-0.095334053 -0.17928839 -0.46147084 -0.56401134 -0.54495811 -0.62592149 -0.827502 -0.79900742 -0.84460616 -1.2388482 -1.9929099 -2.8794451 -3.9312007 -4.7878971 -5.3522596][-0.095225334 -0.049582958 -0.27342033 -0.611989 -0.84042978 -0.83808374 -0.88601184 -0.95604253 -1.0070612 -1.3286579 -2.010674 -2.9219036 -3.8700643 -4.8062987 -5.4576693][0.0020389557 -0.021670341 -0.31014585 -0.64469123 -0.84940267 -0.97442555 -1.0091588 -0.96429753 -0.99492335 -1.2088578 -1.7888937 -2.7655787 -3.8779557 -4.6714845 -5.0776858]]...]
INFO - root - 2017-12-16 08:05:03.339987: step 55610, loss = 0.20, batch loss = 0.15 (28.2 examples/sec; 0.284 sec/batch; 21h:48m:38s remains)
INFO - root - 2017-12-16 08:05:06.156807: step 55620, loss = 0.35, batch loss = 0.29 (29.3 examples/sec; 0.273 sec/batch; 20h:58m:39s remains)
INFO - root - 2017-12-16 08:05:09.024727: step 55630, loss = 0.32, batch loss = 0.27 (29.1 examples/sec; 0.275 sec/batch; 21h:10m:22s remains)
INFO - root - 2017-12-16 08:05:11.921432: step 55640, loss = 0.25, batch loss = 0.19 (27.8 examples/sec; 0.288 sec/batch; 22h:08m:55s remains)
INFO - root - 2017-12-16 08:05:14.692785: step 55650, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.280 sec/batch; 21h:32m:09s remains)
INFO - root - 2017-12-16 08:05:17.534545: step 55660, loss = 0.25, batch loss = 0.20 (26.4 examples/sec; 0.303 sec/batch; 23h:16m:38s remains)
INFO - root - 2017-12-16 08:05:20.371739: step 55670, loss = 0.41, batch loss = 0.35 (27.7 examples/sec; 0.288 sec/batch; 22h:10m:21s remains)
INFO - root - 2017-12-16 08:05:23.184584: step 55680, loss = 0.44, batch loss = 0.38 (28.9 examples/sec; 0.277 sec/batch; 21h:17m:00s remains)
INFO - root - 2017-12-16 08:05:26.036276: step 55690, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 21h:57m:54s remains)
INFO - root - 2017-12-16 08:05:28.909701: step 55700, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 21h:55m:34s remains)
2017-12-16 08:05:29.432026: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3279238 -3.0794616 -2.9997888 -3.1033509 -3.3231168 -3.5911958 -3.9621284 -4.1992021 -4.3188119 -4.3793955 -4.1590972 -3.8277259 -3.4746671 -3.3011508 -3.3244758][-3.0638719 -2.72248 -2.5895491 -2.698693 -3.0220315 -3.3699431 -3.7178769 -3.7867842 -3.7180147 -3.6629305 -3.5279484 -3.2811432 -3.0277128 -2.8675535 -2.7855732][-2.6917424 -2.2983859 -2.2786961 -2.3919315 -2.544693 -2.5994837 -2.8207304 -2.9516177 -2.9130449 -2.8370457 -2.796082 -2.8016768 -2.6921611 -2.4768929 -2.2550085][-2.1309152 -1.7063062 -1.6677907 -1.7813187 -1.940341 -1.8267844 -1.7209134 -1.5693264 -1.5265515 -1.648695 -1.7306385 -2.0062947 -2.1236598 -1.9103563 -1.6773264][-1.6175416 -1.1362605 -0.93687344 -0.95081139 -0.94387317 -0.41526508 0.16862774 0.50512981 0.5197525 0.16623783 -0.22099447 -0.79477215 -1.0878751 -1.0306191 -0.81946874][-1.3523622 -0.8474474 -0.5674 -0.35785532 0.14956617 0.90093756 1.7576437 2.2902327 2.2303438 1.6164532 0.84439325 0.12242603 -0.21008778 -0.26394653 -0.086136818][-0.7896266 -0.27781773 -0.043637753 0.18396091 0.83376932 1.9434724 3.2044992 3.7552519 3.5565066 2.9264998 2.1432061 1.1814065 0.52511215 0.32656717 0.19115639][-0.57964277 -0.20709705 -0.0023670197 0.30789852 0.96576738 2.0797954 3.3130412 4.0516253 4.0904703 3.4846954 2.7660294 2.1813693 1.6523089 0.96524954 0.39264584][-1.0363259 -0.93061066 -0.78703594 -0.62846303 -0.056529522 1.0776944 2.237493 3.0946021 3.2479806 2.7540078 2.3319855 1.7911844 1.2103701 0.53361607 -0.067799091][-1.5990102 -1.8347557 -2.027858 -2.0420697 -1.5968015 -0.72417641 0.273746 1.1736817 1.4382019 1.2026157 1.0328307 0.58732414 0.129457 -0.76642561 -1.4814372][-2.5853734 -2.962924 -3.20259 -3.5822287 -3.6132841 -2.8811836 -2.0531447 -1.3937206 -1.0117218 -0.97580528 -1.0453587 -1.2999821 -1.5787313 -2.4946709 -3.3512044][-4.0031004 -4.7350116 -5.2233348 -5.4626455 -5.4365568 -5.0455217 -4.5891919 -3.7883015 -3.2055852 -3.1513727 -3.104568 -3.1733737 -3.3663459 -4.0952015 -4.7156653][-5.5799518 -6.4766412 -7.1181231 -7.4717369 -7.5113316 -7.1407604 -6.6481447 -5.9008837 -5.300457 -4.9407749 -4.5553226 -4.4343724 -4.4599481 -4.8979807 -5.1155157][-6.8332634 -7.5480433 -7.958724 -8.2524052 -8.1786337 -7.7811422 -7.3933392 -6.867919 -6.3313651 -5.8786774 -5.49498 -5.3291092 -5.16569 -5.3265362 -5.4230232][-7.0298853 -7.6301861 -7.8907471 -8.0058308 -7.755064 -7.3274832 -6.9551463 -6.5270305 -6.0771494 -5.7192698 -5.5008359 -5.1793633 -4.7825255 -4.8272109 -4.8460546]]...]
INFO - root - 2017-12-16 08:05:32.240206: step 55710, loss = 0.26, batch loss = 0.21 (28.2 examples/sec; 0.283 sec/batch; 21h:46m:27s remains)
INFO - root - 2017-12-16 08:05:35.098441: step 55720, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.286 sec/batch; 21h:59m:38s remains)
INFO - root - 2017-12-16 08:05:38.006183: step 55730, loss = 0.22, batch loss = 0.16 (29.0 examples/sec; 0.276 sec/batch; 21h:11m:37s remains)
INFO - root - 2017-12-16 08:05:40.794036: step 55740, loss = 0.32, batch loss = 0.26 (27.5 examples/sec; 0.291 sec/batch; 22h:21m:32s remains)
INFO - root - 2017-12-16 08:05:43.633301: step 55750, loss = 0.23, batch loss = 0.17 (27.6 examples/sec; 0.290 sec/batch; 22h:17m:35s remains)
INFO - root - 2017-12-16 08:05:46.430722: step 55760, loss = 0.20, batch loss = 0.14 (28.0 examples/sec; 0.286 sec/batch; 21h:57m:45s remains)
INFO - root - 2017-12-16 08:05:49.196564: step 55770, loss = 0.25, batch loss = 0.19 (29.3 examples/sec; 0.273 sec/batch; 21h:00m:35s remains)
INFO - root - 2017-12-16 08:05:51.995682: step 55780, loss = 0.27, batch loss = 0.22 (28.6 examples/sec; 0.280 sec/batch; 21h:31m:54s remains)
INFO - root - 2017-12-16 08:05:54.803696: step 55790, loss = 0.23, batch loss = 0.17 (29.4 examples/sec; 0.272 sec/batch; 20h:54m:20s remains)
INFO - root - 2017-12-16 08:05:57.600341: step 55800, loss = 0.28, batch loss = 0.22 (28.9 examples/sec; 0.276 sec/batch; 21h:14m:59s remains)
2017-12-16 08:05:58.098564: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3074622 -2.2309363 -2.251955 -2.4772267 -3.0779977 -3.7055721 -4.090826 -4.583147 -4.896297 -4.9093561 -5.0291176 -5.4227629 -5.5329075 -5.4867716 -5.3455071][-1.4919665 -1.3442469 -1.5481384 -2.0813856 -2.8258488 -3.7947783 -4.538835 -4.876545 -5.1576905 -5.4398232 -5.5087204 -5.5287757 -5.7745848 -6.0967336 -5.8602743][-0.5802083 -0.30347061 -0.49148798 -1.2495351 -2.0297735 -2.8790278 -3.7854006 -4.6592221 -4.9775715 -4.9087839 -5.1129851 -5.4628844 -5.620472 -5.7791405 -5.6975861][0.12738371 0.21576834 0.0021138191 -0.55159593 -1.2924244 -2.0510142 -2.6628551 -3.046433 -3.4553258 -4.0196781 -4.4666562 -4.700417 -5.1455636 -5.4709144 -5.103035][-0.1256547 0.047944069 0.071801662 -0.20551252 -0.47025013 -0.74886489 -1.1303914 -1.4655061 -1.8274636 -2.2858231 -2.8705628 -3.5436637 -4.1319242 -4.26404 -3.9380877][-0.79308581 -0.28475666 0.17176676 0.14883471 0.15755606 0.13855362 0.18950844 -0.049852371 -0.43214345 -0.75507 -1.2079391 -1.8326943 -2.3310475 -2.3696675 -1.9189479][-1.5942447 -0.86919856 -0.26239252 0.054359913 0.41687298 0.71729231 0.89289236 0.70980835 0.299191 -0.011599541 -0.44992471 -0.76644111 -0.69641232 -0.52117157 0.04502821][-2.2834339 -1.5774505 -0.9394021 -0.46735477 0.072982311 0.62444115 1.1130214 1.199337 0.84558344 0.46080065 0.0004658699 -0.19084406 -0.015771389 0.43337011 1.1543741][-3.0076256 -2.307971 -1.7052512 -1.1668348 -0.53581262 0.24239254 0.77955961 0.69934082 0.44412565 0.029864311 -0.32936954 -0.24376488 0.022564411 0.60503006 1.3760219][-4.3381481 -3.6670251 -3.0314455 -2.4274204 -1.7285078 -0.99649739 -0.39961958 -0.41309214 -0.5752902 -0.7724793 -0.87520528 -0.8249054 -0.53965306 0.22696161 0.9677062][-4.70492 -4.1251931 -3.6352098 -3.1274877 -2.4122055 -1.8570993 -1.5441499 -1.6153178 -1.7614484 -2.1814194 -2.3108189 -1.9519141 -1.4888752 -0.76561308 -0.048814774][-4.8216968 -4.4767895 -4.074173 -3.683929 -3.1866322 -2.8648143 -2.5351543 -2.4997931 -2.6580648 -2.8974223 -3.0333052 -3.0452025 -2.7478867 -2.1547139 -1.6887248][-4.2221632 -4.038837 -3.9602652 -3.7902474 -3.6361694 -3.6565056 -3.4698472 -3.2955127 -3.3673458 -3.5528226 -3.5355697 -3.5002444 -3.4987836 -3.2880173 -2.9207447][-3.7618513 -3.4704077 -3.3378363 -3.2145028 -3.174628 -3.0963621 -2.9443121 -2.9635653 -2.9231334 -2.9336276 -3.0582125 -3.2430906 -3.4108207 -3.4349737 -3.3468227][-3.5643594 -3.2160754 -3.06363 -2.8353231 -2.7672546 -2.7797885 -2.6745768 -2.4210827 -2.11963 -2.0440388 -2.0610805 -2.1661382 -2.4870558 -2.730207 -2.9288044]]...]
INFO - root - 2017-12-16 08:06:00.919384: step 55810, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.283 sec/batch; 21h:46m:39s remains)
INFO - root - 2017-12-16 08:06:03.788232: step 55820, loss = 0.28, batch loss = 0.22 (26.6 examples/sec; 0.301 sec/batch; 23h:06m:51s remains)
INFO - root - 2017-12-16 08:06:06.676515: step 55830, loss = 0.25, batch loss = 0.19 (28.6 examples/sec; 0.279 sec/batch; 21h:28m:12s remains)
INFO - root - 2017-12-16 08:06:09.472530: step 55840, loss = 0.28, batch loss = 0.22 (27.6 examples/sec; 0.290 sec/batch; 22h:18m:52s remains)
INFO - root - 2017-12-16 08:06:12.353790: step 55850, loss = 0.23, batch loss = 0.17 (26.9 examples/sec; 0.297 sec/batch; 22h:51m:30s remains)
INFO - root - 2017-12-16 08:06:15.207004: step 55860, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.281 sec/batch; 21h:36m:23s remains)
INFO - root - 2017-12-16 08:06:17.992097: step 55870, loss = 0.26, batch loss = 0.21 (29.0 examples/sec; 0.276 sec/batch; 21h:11m:07s remains)
INFO - root - 2017-12-16 08:06:20.839116: step 55880, loss = 0.24, batch loss = 0.18 (27.5 examples/sec; 0.290 sec/batch; 22h:19m:07s remains)
INFO - root - 2017-12-16 08:06:23.666123: step 55890, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.279 sec/batch; 21h:27m:53s remains)
INFO - root - 2017-12-16 08:06:26.525610: step 55900, loss = 0.24, batch loss = 0.18 (27.1 examples/sec; 0.295 sec/batch; 22h:40m:29s remains)
2017-12-16 08:06:27.107139: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3848443 -3.3247786 -3.4058681 -3.7964389 -4.0760355 -4.2908382 -4.3165083 -4.1069584 -3.8714941 -3.5022569 -3.1570494 -2.8233995 -2.5676119 -2.3693314 -2.1747141][-3.5593786 -3.499929 -3.4934778 -3.7083073 -4.0487752 -4.4712605 -4.675251 -4.5734739 -4.3886089 -4.0692887 -3.7439139 -3.4014704 -3.0920944 -2.7271614 -2.3006921][-3.7077293 -3.4211764 -3.3084464 -3.3138294 -3.4601002 -3.7367065 -4.172677 -4.4406538 -4.5623851 -4.4326973 -4.3200655 -4.1422005 -3.8237565 -3.4428804 -2.8555987][-4.0286226 -3.4818649 -2.9864964 -2.6337385 -2.4360309 -2.4503465 -2.7115822 -3.1084423 -3.58875 -4.0127921 -4.2717204 -4.489821 -4.5221391 -4.171804 -3.5359924][-3.9638784 -3.2133498 -2.3646858 -1.5125756 -0.85449386 -0.49516296 -0.53534579 -0.89733005 -1.5027952 -2.4409041 -3.3945413 -4.0235958 -4.3759155 -4.4333529 -4.0411625][-3.5736887 -2.5866508 -1.5364583 -0.19566822 0.93900537 1.6880565 1.9020891 1.5851288 0.890965 -0.30821896 -1.636049 -2.9412928 -3.8798809 -4.1641026 -4.0729094][-3.0078855 -2.0415778 -0.91703439 0.56312275 2.0329552 3.1879849 3.9087172 3.7590218 3.1490908 1.7406631 0.10157871 -1.506202 -2.8567948 -3.6624827 -3.7554519][-2.5143664 -1.663408 -0.79670858 0.65659046 2.2666731 3.6491394 4.5434618 4.6229162 4.0321684 2.5984402 0.93807411 -0.87588167 -2.3357813 -3.3414316 -3.7963829][-2.2813282 -1.7603476 -1.0714304 -0.018105507 1.2443357 2.5437927 3.4900327 3.6203871 3.1445971 1.819356 0.20508051 -1.5662925 -3.0233026 -3.7913702 -4.0853372][-2.2868025 -1.9715238 -1.7895916 -1.3124082 -0.51492715 0.21912336 0.91872835 1.109417 0.94298649 -0.14313221 -1.5013366 -2.9896851 -4.2039852 -4.8036942 -4.7864494][-2.7619438 -2.4007373 -2.5178287 -2.5705996 -2.3511934 -2.0812259 -1.693953 -1.8006284 -2.0247669 -2.6228857 -3.5236964 -4.6232691 -5.4713078 -5.7987385 -5.7046537][-3.4663403 -3.0333767 -3.1474829 -3.3830872 -3.7139857 -4.1138759 -4.3364577 -4.3518381 -4.4637179 -4.7930832 -5.1026592 -5.776145 -6.4053459 -6.5788827 -6.4203835][-3.6592937 -3.2139387 -3.3257961 -3.7397442 -4.6067343 -5.218678 -5.7229304 -6.1679382 -6.1393476 -5.9727278 -5.885251 -6.03461 -6.2572184 -6.4877849 -6.5368457][-4.0123291 -3.3120089 -3.2311943 -3.499651 -4.2129951 -5.1577644 -5.9152422 -6.3058958 -6.3951812 -5.9772997 -5.5190005 -5.4396219 -5.6140261 -5.7010913 -5.8311286][-4.2497654 -3.1194212 -2.5469089 -2.8935816 -3.5326252 -4.1753144 -4.85545 -5.42003 -5.5039797 -5.2838373 -4.777256 -4.3441825 -4.2702456 -4.5837393 -4.9393544]]...]
INFO - root - 2017-12-16 08:06:29.910372: step 55910, loss = 0.33, batch loss = 0.27 (28.7 examples/sec; 0.279 sec/batch; 21h:24m:09s remains)
INFO - root - 2017-12-16 08:06:32.751131: step 55920, loss = 0.32, batch loss = 0.26 (29.2 examples/sec; 0.274 sec/batch; 21h:04m:32s remains)
INFO - root - 2017-12-16 08:06:35.657509: step 55930, loss = 0.43, batch loss = 0.37 (25.9 examples/sec; 0.309 sec/batch; 23h:44m:14s remains)
INFO - root - 2017-12-16 08:06:38.435933: step 55940, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.280 sec/batch; 21h:32m:29s remains)
INFO - root - 2017-12-16 08:06:41.270567: step 55950, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.280 sec/batch; 21h:29m:03s remains)
INFO - root - 2017-12-16 08:06:44.137450: step 55960, loss = 0.25, batch loss = 0.19 (28.7 examples/sec; 0.279 sec/batch; 21h:25m:21s remains)
INFO - root - 2017-12-16 08:06:46.942547: step 55970, loss = 0.33, batch loss = 0.27 (29.0 examples/sec; 0.276 sec/batch; 21h:09m:56s remains)
INFO - root - 2017-12-16 08:06:49.815710: step 55980, loss = 0.37, batch loss = 0.31 (28.4 examples/sec; 0.281 sec/batch; 21h:36m:17s remains)
INFO - root - 2017-12-16 08:06:52.662306: step 55990, loss = 0.22, batch loss = 0.16 (28.0 examples/sec; 0.286 sec/batch; 21h:56m:40s remains)
INFO - root - 2017-12-16 08:06:55.487796: step 56000, loss = 0.28, batch loss = 0.23 (29.0 examples/sec; 0.276 sec/batch; 21h:10m:23s remains)
2017-12-16 08:06:56.016749: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4931252 -3.3678584 -3.2269139 -3.1805935 -3.2744815 -3.3758755 -3.4504313 -3.5187664 -3.4943025 -3.0810075 -2.57793 -2.1600492 -1.9258451 -1.9659405 -2.221446][-3.5365546 -3.5561049 -3.5515459 -3.4762824 -3.5523977 -3.6965261 -3.8509908 -3.6756513 -3.3665085 -3.0197635 -2.721673 -2.398752 -2.1858804 -2.2548704 -2.5340378][-3.498013 -3.4537988 -3.4844167 -3.5560565 -3.6596224 -3.7136102 -3.730921 -3.5599148 -3.3685098 -2.8998146 -2.518728 -2.4749994 -2.5008342 -2.69804 -2.8872428][-3.4671957 -3.6235943 -3.7315338 -3.7663116 -3.7513185 -3.5994465 -3.3302689 -3.0108531 -2.8402133 -2.5709155 -2.4258435 -2.5282936 -2.6482098 -2.9753802 -3.1497719][-2.8773088 -3.0983839 -3.2711492 -3.2742469 -3.081923 -2.7697697 -2.4129419 -1.8818095 -1.5741658 -1.5211418 -1.6035309 -1.9165404 -2.1502068 -2.5273778 -2.7239356][-2.2367332 -2.5380492 -2.7025223 -2.5760179 -2.1411791 -1.4676054 -0.71766996 -0.16146612 0.044617176 -0.063807964 -0.4557662 -1.0653586 -1.4500663 -1.8687856 -2.0588963][-1.8872166 -2.1710236 -2.2091517 -1.9320238 -1.2654574 -0.33014584 0.66378975 1.3084402 1.3709726 0.97714138 0.31657124 -0.38136673 -0.84094405 -1.168467 -1.1677544][-1.2373395 -1.5459611 -1.4921985 -1.0017555 -0.16468859 0.90255022 1.7884512 2.270009 2.0928035 1.2945399 0.22582102 -0.547884 -0.94702458 -1.218116 -1.2387326][-1.3792429 -1.4126222 -1.1175001 -0.41708374 0.54225969 1.5743065 2.3126521 2.4156504 1.8792772 1.0425315 -0.034667969 -1.0685327 -1.6119485 -1.6831989 -1.5757644][-1.6200089 -1.2773581 -0.90615129 -0.37766123 0.5029645 1.3121486 1.7169189 1.611239 0.85709858 -0.16081095 -1.1663337 -2.0479927 -2.5802412 -2.6686211 -2.4368982][-2.1889391 -1.8548965 -1.3504994 -0.77906775 -0.11835384 0.31379175 0.42280102 -0.0082607269 -0.94297028 -1.761939 -2.5369992 -3.1264644 -3.372273 -3.3176599 -3.0445015][-3.1611443 -2.7380946 -2.1047478 -1.5247087 -1.0215502 -0.96500921 -1.3742647 -1.7970417 -2.3914251 -3.1717472 -3.8987539 -4.1011486 -3.9673641 -3.7309487 -3.5011618][-4.1664038 -3.8394685 -3.2239604 -2.7004509 -2.2577615 -2.1659725 -2.503438 -3.1693339 -3.820003 -3.9363081 -4.0770316 -4.2029839 -4.0928802 -3.7281919 -3.4401579][-5.6097536 -5.1806145 -4.4418707 -3.9291315 -3.4942641 -3.4748402 -3.7328234 -4.1613312 -4.5493279 -4.5392251 -4.2689219 -4.0323744 -3.7454948 -3.3338151 -2.9776292][-6.3361363 -6.01702 -5.4659576 -4.9030142 -4.4388342 -4.3279777 -4.4182291 -4.6526237 -4.9149623 -4.8344703 -4.4205432 -3.9275851 -3.4509487 -3.0658069 -2.6593361]]...]
INFO - root - 2017-12-16 08:06:58.879764: step 56010, loss = 0.36, batch loss = 0.30 (29.4 examples/sec; 0.272 sec/batch; 20h:55m:19s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 08:07:01.708902: step 56020, loss = 0.20, batch loss = 0.15 (27.5 examples/sec; 0.291 sec/batch; 22h:20m:33s remains)
INFO - root - 2017-12-16 08:07:04.556750: step 56030, loss = 0.31, batch loss = 0.26 (28.2 examples/sec; 0.283 sec/batch; 21h:45m:28s remains)
INFO - root - 2017-12-16 08:07:07.418044: step 56040, loss = 0.43, batch loss = 0.37 (28.3 examples/sec; 0.282 sec/batch; 21h:40m:46s remains)
INFO - root - 2017-12-16 08:07:10.289262: step 56050, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.285 sec/batch; 21h:51m:55s remains)
INFO - root - 2017-12-16 08:07:13.143664: step 56060, loss = 0.23, batch loss = 0.17 (29.3 examples/sec; 0.273 sec/batch; 20h:59m:06s remains)
INFO - root - 2017-12-16 08:07:16.003095: step 56070, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 21h:46m:52s remains)
INFO - root - 2017-12-16 08:07:18.829915: step 56080, loss = 0.22, batch loss = 0.16 (27.4 examples/sec; 0.292 sec/batch; 22h:26m:24s remains)
INFO - root - 2017-12-16 08:07:21.682336: step 56090, loss = 0.22, batch loss = 0.16 (28.5 examples/sec; 0.281 sec/batch; 21h:33m:10s remains)
INFO - root - 2017-12-16 08:07:24.488656: step 56100, loss = 0.30, batch loss = 0.24 (28.4 examples/sec; 0.282 sec/batch; 21h:37m:40s remains)
2017-12-16 08:07:24.996707: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3612976 -4.2085195 -4.1399946 -4.546082 -5.2026935 -5.9472246 -6.6122141 -6.7268209 -6.7124228 -6.2645493 -5.7531939 -5.4422026 -4.861917 -4.1785655 -3.5010827][-5.0029845 -5.4745011 -5.8927789 -6.1210537 -6.9079752 -7.0975051 -7.1297226 -7.008101 -6.5353813 -6.1533365 -5.943357 -5.5006924 -4.773005 -4.2044449 -3.6728516][-5.5838695 -6.1991205 -6.7188997 -6.923275 -6.8790669 -6.5467114 -6.513464 -6.1206651 -5.7211676 -5.4077287 -5.0697246 -5.095448 -5.0807309 -4.6687236 -4.0834956][-6.0279255 -6.9623518 -7.3303714 -7.0465364 -6.4327936 -5.3453045 -4.4452624 -3.7733858 -3.4680748 -3.9364758 -4.5979714 -4.8251333 -4.8832622 -4.96827 -4.719173][-5.9496279 -6.9284 -7.0670528 -6.2650619 -4.7332149 -2.9152761 -1.5948102 -0.63892794 -0.66625643 -1.7473779 -2.9705982 -4.3067689 -5.2167892 -5.2623177 -4.7750807][-4.5942683 -5.3855844 -5.5329971 -4.4734154 -2.1179085 0.44220018 2.3815718 3.2970228 2.6381893 0.84244537 -1.5954807 -3.6723003 -4.9140873 -5.4495459 -5.154727][-3.2902768 -3.9527595 -3.6729698 -2.1537826 0.11310339 2.7636328 4.9530764 5.7656488 4.83416 2.4135036 -0.46324492 -3.0916233 -4.8630409 -5.3457093 -4.8958235][-2.6076112 -3.3286881 -3.1913571 -1.5166571 1.1290154 3.9227343 5.8983183 6.18561 5.0063305 2.452456 -0.70896626 -3.3315384 -4.8010087 -5.1898255 -4.789937][-2.4955845 -2.9843593 -2.5622952 -1.4583235 0.18381596 2.6152411 4.6139317 4.9732485 3.6552038 0.98073626 -1.9258885 -4.209671 -5.4906411 -5.6098895 -4.9997339][-2.4882205 -3.1553655 -2.9190822 -1.9606478 -0.46706915 0.97752047 1.829608 2.1309648 1.0852079 -1.233252 -3.6962814 -5.5853586 -6.3328018 -6.1863604 -5.477984][-3.2808006 -3.5710115 -3.4107437 -3.2359343 -2.8277683 -1.6264927 -0.45095396 -0.54446507 -1.8227992 -3.3784382 -5.0576749 -6.5011473 -7.0263538 -6.5572453 -5.6908393][-4.346148 -4.5496759 -4.6608148 -4.2719455 -3.7151361 -3.4522347 -3.2192678 -3.2026172 -3.756309 -4.9597359 -6.2304716 -6.9355946 -6.9808755 -6.4337673 -5.6407981][-5.3480468 -5.4114776 -5.509553 -5.4519687 -5.3414469 -4.767674 -4.3266191 -4.6902437 -5.3199291 -5.9159369 -6.6397867 -7.0407147 -6.8822188 -6.2563028 -5.5443444][-6.1799812 -6.0943432 -6.0401487 -5.9615116 -5.8192596 -5.4358425 -5.2932096 -5.1828179 -5.3871269 -5.9635029 -6.3765574 -6.3748145 -6.114357 -5.5599232 -5.01137][-6.8220468 -6.6943426 -6.5338764 -6.2797093 -5.9785128 -5.6014462 -5.3572111 -5.1039462 -5.1260896 -5.393775 -5.5984764 -5.4838338 -5.1263542 -4.600904 -4.1396251]]...]
INFO - root - 2017-12-16 08:07:27.824745: step 56110, loss = 0.24, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 21h:44m:13s remains)
INFO - root - 2017-12-16 08:07:30.676992: step 56120, loss = 0.26, batch loss = 0.20 (29.1 examples/sec; 0.274 sec/batch; 21h:04m:25s remains)
INFO - root - 2017-12-16 08:07:33.457342: step 56130, loss = 0.30, batch loss = 0.24 (27.4 examples/sec; 0.292 sec/batch; 22h:22m:48s remains)
INFO - root - 2017-12-16 08:07:36.287758: step 56140, loss = 0.31, batch loss = 0.25 (28.4 examples/sec; 0.282 sec/batch; 21h:39m:34s remains)
INFO - root - 2017-12-16 08:07:39.144755: step 56150, loss = 0.39, batch loss = 0.33 (28.3 examples/sec; 0.283 sec/batch; 21h:41m:58s remains)
INFO - root - 2017-12-16 08:07:42.008074: step 56160, loss = 0.19, batch loss = 0.13 (27.6 examples/sec; 0.290 sec/batch; 22h:15m:22s remains)
INFO - root - 2017-12-16 08:07:44.860153: step 56170, loss = 0.28, batch loss = 0.23 (28.3 examples/sec; 0.283 sec/batch; 21h:42m:04s remains)
INFO - root - 2017-12-16 08:07:47.728808: step 56180, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 21h:38m:17s remains)
INFO - root - 2017-12-16 08:07:50.515322: step 56190, loss = 0.25, batch loss = 0.19 (29.6 examples/sec; 0.271 sec/batch; 20h:46m:00s remains)
INFO - root - 2017-12-16 08:07:53.335053: step 56200, loss = 0.21, batch loss = 0.15 (28.5 examples/sec; 0.280 sec/batch; 21h:30m:50s remains)
2017-12-16 08:07:53.933361: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.14764 -2.2800379 -2.3922758 -2.4955587 -2.597342 -2.6539345 -2.6992886 -2.7127397 -2.7389903 -2.6585336 -2.5853591 -2.5782852 -2.5768981 -2.5351887 -2.5089068][-2.2762151 -2.4416542 -2.6413803 -2.8514361 -3.0761781 -3.21154 -3.2843304 -3.2473154 -3.1499166 -3.0675724 -3.0332365 -2.955626 -2.9189358 -2.9472032 -2.9599683][-2.7090683 -3.0360708 -3.3392415 -3.5855236 -3.7815475 -3.8385434 -3.8371572 -3.7397316 -3.6009386 -3.4984689 -3.4755301 -3.4418244 -3.4356623 -3.458077 -3.4699242][-3.5275037 -3.9122691 -4.1396475 -4.2185564 -4.2311115 -4.0667872 -3.898581 -3.6653795 -3.4762008 -3.4609909 -3.4469705 -3.5054271 -3.6551456 -3.8053126 -3.9070508][-4.3703623 -4.7377253 -4.87157 -4.6375771 -4.1737661 -3.6806235 -3.2893567 -2.9122257 -2.6437817 -2.6287591 -2.6663456 -2.8691311 -3.1769152 -3.5763462 -3.9808335][-4.7188087 -5.0662336 -4.9448757 -4.3523493 -3.4155228 -2.5818033 -2.0256703 -1.7001567 -1.5925765 -1.5165329 -1.5120788 -1.7891028 -2.3297117 -2.9054403 -3.4042754][-4.55149 -4.7363119 -4.3161569 -3.3147664 -2.1105642 -0.790153 -0.05164957 0.23036909 0.18054485 -0.032938957 -0.38918257 -0.84360075 -1.3197603 -2.2682116 -3.24559][-4.0871482 -3.9194627 -3.3617318 -2.0335517 -0.55234385 0.81964159 1.6553044 2.0863485 2.0311112 1.5251136 0.86220551 0.044428349 -0.86508632 -2.0086184 -3.0661533][-2.9706841 -2.829 -2.1272488 -1.0768473 0.28059816 1.6721182 2.5414867 2.9793434 2.9115562 2.3839579 1.5571418 0.36221886 -0.77080774 -2.0614197 -3.3590965][-2.1227806 -2.0356131 -1.4994979 -0.57026958 0.51745892 1.697701 2.4135294 2.8461623 2.6950994 2.0960197 1.1191282 -0.159091 -1.4506924 -2.7990332 -3.8128238][-1.6702347 -1.443686 -1.0646119 -0.50904822 0.1575985 1.1570597 1.7541938 2.0481625 1.8358436 1.2310977 0.20887852 -1.2579451 -2.4223716 -3.5506885 -4.4579053][-1.5576618 -1.2567959 -0.85980678 -0.46094084 0.017921925 0.49085045 0.83212328 1.0060506 0.86118555 0.21254158 -0.66536379 -1.755744 -2.8110931 -3.6698678 -4.3792391][-1.3018727 -1.030889 -0.81978393 -0.82085872 -0.63621306 -0.19087029 0.14814806 0.23685932 -0.090257645 -0.52346206 -1.2168536 -2.0487306 -2.8440886 -3.3971317 -3.7508605][-1.2065356 -0.80060887 -0.49926639 -0.54272461 -0.52680039 -0.30479336 -0.30480576 -0.096711159 -0.27822208 -0.7387557 -1.3051622 -1.9219041 -2.3869445 -2.8398967 -3.0097613][-0.63488126 -0.23738194 -0.15438271 -0.18644333 -0.202178 -0.054367065 -0.02814579 -0.064757347 -0.29814386 -0.76248956 -1.1907394 -1.6473944 -1.9823918 -2.2959123 -2.4987962]]...]
INFO - root - 2017-12-16 08:07:56.751002: step 56210, loss = 0.26, batch loss = 0.20 (27.2 examples/sec; 0.294 sec/batch; 22h:33m:20s remains)
INFO - root - 2017-12-16 08:07:59.591837: step 56220, loss = 0.20, batch loss = 0.14 (28.8 examples/sec; 0.278 sec/batch; 21h:20m:00s remains)
INFO - root - 2017-12-16 08:08:02.462351: step 56230, loss = 0.20, batch loss = 0.15 (25.4 examples/sec; 0.315 sec/batch; 24h:09m:02s remains)
INFO - root - 2017-12-16 08:08:05.335877: step 56240, loss = 0.28, batch loss = 0.23 (27.8 examples/sec; 0.287 sec/batch; 22h:02m:40s remains)
INFO - root - 2017-12-16 08:08:08.164081: step 56250, loss = 0.24, batch loss = 0.18 (26.1 examples/sec; 0.307 sec/batch; 23h:33m:25s remains)
INFO - root - 2017-12-16 08:08:11.002857: step 56260, loss = 0.31, batch loss = 0.25 (27.3 examples/sec; 0.293 sec/batch; 22h:28m:18s remains)
INFO - root - 2017-12-16 08:08:13.859698: step 56270, loss = 0.29, batch loss = 0.23 (24.9 examples/sec; 0.321 sec/batch; 24h:39m:53s remains)
INFO - root - 2017-12-16 08:08:16.747424: step 56280, loss = 0.29, batch loss = 0.23 (27.6 examples/sec; 0.289 sec/batch; 22h:12m:41s remains)
INFO - root - 2017-12-16 08:08:19.594599: step 56290, loss = 0.24, batch loss = 0.18 (27.1 examples/sec; 0.295 sec/batch; 22h:39m:54s remains)
INFO - root - 2017-12-16 08:08:22.423669: step 56300, loss = 0.27, batch loss = 0.21 (29.0 examples/sec; 0.276 sec/batch; 21h:09m:54s remains)
2017-12-16 08:08:22.998377: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5141726 -4.5859094 -4.6748362 -4.7889504 -5.0052505 -5.2870383 -5.3419752 -5.4863138 -5.5856524 -5.4672527 -5.3519559 -5.4423738 -5.6839628 -5.5265284 -5.2822576][-4.0137119 -4.0740995 -4.4052649 -4.8434758 -5.0012007 -5.3009424 -5.3794122 -5.4595513 -5.436029 -5.4685764 -5.6816955 -5.8790159 -6.1340375 -5.8891373 -5.4700189][-3.1520486 -3.4020185 -3.9621592 -4.529737 -4.8862219 -5.0804563 -4.9053707 -4.7415504 -4.7653527 -5.0645132 -5.7091866 -6.0218453 -6.1721058 -5.7175388 -5.1121931][-2.6033471 -2.7099814 -3.4634039 -4.3410087 -4.569797 -4.242763 -3.8314278 -3.680583 -3.7570965 -4.23201 -5.14104 -5.7447753 -5.9169316 -5.1745782 -4.2097874][-1.8349736 -2.1700926 -3.1058266 -3.9595454 -3.8744431 -3.1740637 -2.1548171 -1.8004861 -2.0769777 -2.9385662 -4.0864062 -4.8233519 -4.8889627 -4.3119731 -3.3710265][-1.3466084 -1.7310934 -2.5862875 -3.4129775 -3.0360866 -1.7484281 -0.28181648 0.26899385 -0.35502291 -1.6821775 -3.1961865 -4.1233292 -4.2379527 -3.5273936 -2.4880261][-1.311657 -1.8331878 -2.6428843 -3.0034208 -2.1263545 -0.46655059 1.1831093 1.7677073 1.0172186 -0.60884714 -2.4223142 -3.4673734 -3.7348995 -3.0692325 -2.0346086][-1.0099862 -1.6032152 -2.6665273 -3.0394669 -1.9646525 0.095934391 2.0473566 2.6683731 1.8475027 0.11910677 -1.877178 -3.0919614 -3.3907368 -2.9835513 -1.9846761][-1.0736022 -1.8625214 -2.8542023 -3.09057 -2.3519554 -0.39358807 1.4606113 2.3121982 1.5888448 -0.26780939 -2.4379656 -3.7843337 -4.1348948 -3.8863707 -2.7291288][-1.6965594 -2.5790555 -3.9817138 -4.2960491 -3.3061457 -1.4897676 -0.10553837 0.59971952 0.13771105 -1.558176 -3.4425347 -4.8593912 -5.3287158 -5.1477289 -4.0398078][-2.6871498 -3.75124 -4.9015222 -5.3638945 -4.8387976 -3.3147302 -1.5487189 -0.80043483 -1.5791898 -3.21199 -4.8992867 -6.0310307 -6.3452921 -6.0590029 -4.9903479][-3.7820776 -4.708416 -5.9958353 -6.5435557 -6.029366 -4.8734541 -3.9363158 -3.315299 -3.671068 -4.7040663 -6.027616 -6.8945532 -6.9094934 -6.2590194 -5.1884665][-4.9399934 -5.9444857 -7.1686535 -7.7488241 -7.7973175 -6.7853937 -5.84147 -5.4614396 -5.860857 -6.4491687 -6.8886328 -7.3066273 -7.1218252 -6.4305592 -5.3997841][-5.4667115 -6.1829472 -7.0809956 -7.7129045 -7.9529076 -7.431469 -6.9652166 -6.75572 -6.87646 -7.1349888 -7.422924 -7.4078264 -6.9560428 -6.2391558 -5.2110548][-5.5188661 -5.9205217 -6.4401445 -6.8514719 -7.0288482 -6.8727 -6.7963705 -6.8140945 -7.0955868 -7.1181374 -7.0186715 -6.8385735 -6.3443661 -5.6213551 -4.8027396]]...]
INFO - root - 2017-12-16 08:08:25.854562: step 56310, loss = 0.28, batch loss = 0.22 (27.6 examples/sec; 0.290 sec/batch; 22h:12m:37s remains)
INFO - root - 2017-12-16 08:08:28.740734: step 56320, loss = 0.27, batch loss = 0.21 (27.9 examples/sec; 0.287 sec/batch; 22h:00m:47s remains)
INFO - root - 2017-12-16 08:08:31.606043: step 56330, loss = 0.23, batch loss = 0.17 (26.7 examples/sec; 0.300 sec/batch; 23h:01m:33s remains)
INFO - root - 2017-12-16 08:08:34.438149: step 56340, loss = 0.32, batch loss = 0.27 (28.6 examples/sec; 0.280 sec/batch; 21h:28m:45s remains)
INFO - root - 2017-12-16 08:08:37.252516: step 56350, loss = 0.38, batch loss = 0.32 (27.8 examples/sec; 0.288 sec/batch; 22h:05m:02s remains)
INFO - root - 2017-12-16 08:08:40.154442: step 56360, loss = 0.25, batch loss = 0.19 (28.6 examples/sec; 0.280 sec/batch; 21h:28m:44s remains)
INFO - root - 2017-12-16 08:08:43.000940: step 56370, loss = 0.30, batch loss = 0.24 (28.5 examples/sec; 0.281 sec/batch; 21h:34m:05s remains)
INFO - root - 2017-12-16 08:08:45.840159: step 56380, loss = 0.21, batch loss = 0.16 (28.3 examples/sec; 0.283 sec/batch; 21h:41m:12s remains)
INFO - root - 2017-12-16 08:08:48.732772: step 56390, loss = 0.19, batch loss = 0.14 (28.1 examples/sec; 0.285 sec/batch; 21h:51m:12s remains)
INFO - root - 2017-12-16 08:08:51.563658: step 56400, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.282 sec/batch; 21h:35m:42s remains)
2017-12-16 08:08:52.066239: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4540391 -3.457093 -3.6613216 -4.2857847 -4.7085872 -4.9496908 -4.9103556 -4.66027 -4.4086862 -4.26681 -4.0890985 -3.9985206 -4.1819048 -4.1812682 -4.0518432][-2.1393929 -2.17276 -2.4179816 -3.0371242 -3.5675735 -4.187304 -4.3014636 -4.1794167 -4.2074552 -4.1155424 -4.0827069 -3.9002218 -3.9720063 -3.9344473 -3.8439763][-1.0016127 -0.79524708 -1.053478 -1.6966772 -2.3055716 -2.9484901 -3.3381376 -3.5712001 -3.6826723 -3.79651 -3.8406725 -3.777343 -3.7650933 -3.45018 -3.0901132][0.66749859 0.55685806 -0.10418797 -0.59855056 -1.0419281 -1.6887913 -2.1588748 -2.6735797 -3.3033738 -3.6803613 -3.5867252 -3.4013615 -3.15984 -2.8639374 -2.6690562][1.4530244 1.3384886 0.98580742 0.24957943 -0.3158021 -0.56781316 -0.84939623 -1.4711194 -2.2635925 -3.1468205 -3.5789809 -3.4491606 -3.0877995 -2.415561 -1.8422067][1.4238415 1.4345841 1.3878636 1.2619696 1.2104716 0.75501204 0.35091829 -0.17857742 -1.2457001 -2.1613309 -2.7010818 -2.9188743 -2.8131204 -2.4340463 -1.9462419][0.81118345 0.92960024 1.1175027 1.4607177 1.8384318 2.0656347 2.021584 1.211226 -0.03799963 -1.2788885 -2.206661 -2.8161397 -3.0065453 -2.714654 -2.3920183][-0.2795887 0.011387348 0.23551846 0.91886139 1.6535888 2.3618722 2.6376429 2.0159774 0.84178066 -0.73689413 -2.0456798 -2.7901249 -2.999366 -2.8881736 -2.8180432][-1.5013051 -1.1152654 -0.59033513 0.15299702 1.0285368 1.9213467 2.4399257 1.7613301 0.31921864 -1.166615 -2.4261265 -3.4307518 -3.8710909 -3.6411548 -3.2129195][-1.8239422 -1.6524494 -1.1487482 -0.47363353 0.26426935 1.0151815 1.3526831 0.96927071 -0.16883802 -1.8358245 -3.1486311 -4.0046759 -4.4933534 -4.5960975 -4.3926582][-2.508812 -2.3128409 -2.0205779 -1.5802596 -1.0380147 -0.44567585 0.0055437088 -0.44488358 -1.3780308 -2.3391716 -3.2920902 -4.0556016 -4.4704437 -4.4373441 -4.2587605][-2.9673295 -3.0586066 -3.0164328 -2.6343431 -2.0541654 -1.8453507 -1.736526 -1.8338258 -2.3303015 -3.1206136 -3.9511449 -4.3604479 -4.4563956 -4.419476 -4.1276422][-2.7616763 -3.002912 -3.2727003 -3.2572145 -2.9761171 -2.714828 -2.6350231 -2.7996287 -3.124855 -3.496325 -4.0663457 -4.4895577 -4.4421139 -4.1421394 -3.7005329][-2.7965202 -3.2708595 -3.6194966 -3.7669272 -3.7220373 -3.6283979 -3.5705066 -3.3114939 -3.2655568 -3.3877637 -3.5444989 -3.6132183 -3.508152 -3.3486466 -3.0696406][-3.0487962 -3.4818521 -3.8179693 -4.0382404 -4.04384 -3.8554258 -3.7137768 -3.4664147 -3.3260984 -3.2738521 -3.2211354 -3.1894641 -3.0471525 -2.7291021 -2.4455347]]...]
INFO - root - 2017-12-16 08:08:54.884373: step 56410, loss = 0.35, batch loss = 0.30 (29.9 examples/sec; 0.268 sec/batch; 20h:33m:08s remains)
INFO - root - 2017-12-16 08:08:57.744424: step 56420, loss = 0.21, batch loss = 0.16 (28.1 examples/sec; 0.285 sec/batch; 21h:50m:15s remains)
INFO - root - 2017-12-16 08:09:00.572088: step 56430, loss = 0.20, batch loss = 0.15 (27.2 examples/sec; 0.295 sec/batch; 22h:35m:23s remains)
INFO - root - 2017-12-16 08:09:03.403871: step 56440, loss = 0.24, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 21h:50m:51s remains)
INFO - root - 2017-12-16 08:09:06.241454: step 56450, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 21h:51m:46s remains)
INFO - root - 2017-12-16 08:09:09.071363: step 56460, loss = 0.20, batch loss = 0.14 (28.7 examples/sec; 0.278 sec/batch; 21h:20m:20s remains)
INFO - root - 2017-12-16 08:09:11.962361: step 56470, loss = 0.25, batch loss = 0.19 (27.7 examples/sec; 0.289 sec/batch; 22h:10m:09s remains)
INFO - root - 2017-12-16 08:09:14.793098: step 56480, loss = 0.21, batch loss = 0.15 (28.8 examples/sec; 0.278 sec/batch; 21h:16m:38s remains)
INFO - root - 2017-12-16 08:09:17.658344: step 56490, loss = 0.22, batch loss = 0.16 (29.0 examples/sec; 0.276 sec/batch; 21h:09m:55s remains)
INFO - root - 2017-12-16 08:09:20.531420: step 56500, loss = 0.29, batch loss = 0.23 (28.8 examples/sec; 0.277 sec/batch; 21h:15m:46s remains)
2017-12-16 08:09:21.079274: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8350072 -4.2929168 -4.7497621 -5.16357 -5.5251894 -5.5348043 -5.5707569 -5.7171063 -6.0168133 -6.0820017 -5.8823295 -5.4143267 -4.8357553 -4.1909776 -3.764185][-3.1197157 -3.9656491 -4.8332648 -5.2437143 -5.32263 -5.6413131 -5.8807611 -5.9571819 -5.9279795 -5.9632964 -6.1293936 -5.6462493 -4.9482827 -4.0192013 -3.3454661][-2.443351 -3.4612646 -4.5074487 -4.9552512 -5.0760775 -5.08574 -5.0638151 -5.5682526 -5.7327356 -5.8526511 -5.7364583 -5.3892522 -4.8899326 -3.8487754 -3.1743994][-2.0514135 -3.1766996 -4.2159972 -4.53415 -4.3561006 -3.9372747 -3.610755 -3.7635939 -4.033452 -4.5278926 -4.7005844 -4.5540137 -4.1889019 -3.4777126 -2.9119322][-2.0806143 -3.1740723 -3.8156927 -3.8518696 -3.0795937 -1.8805809 -1.1176188 -1.0913565 -1.5729015 -2.3084779 -2.8744264 -3.0698566 -2.9433436 -2.6345043 -2.6295393][-2.2593045 -3.2641854 -3.5002217 -2.6052516 -0.93245149 0.56541395 1.6503749 2.0989385 1.6033912 0.42361259 -0.86983848 -1.8100009 -2.4544103 -2.2223568 -2.0974972][-3.0260282 -3.5545998 -3.2672272 -2.0251012 0.18014622 2.6734 4.3132343 4.6917706 3.9252186 2.4033504 0.715178 -0.651824 -1.769172 -2.383533 -2.941453][-3.7983704 -3.9090991 -3.6362524 -2.1323328 0.42411232 3.1336532 5.2493105 5.8012066 4.998313 3.34874 1.4017949 -0.052883148 -1.4244468 -2.4597743 -3.2972345][-5.0370927 -4.9092846 -4.2680902 -3.1832414 -1.2696078 1.5320549 4.2173729 4.9547014 4.0993357 2.5466785 0.89548826 -0.63832021 -1.9232931 -2.6408577 -3.2364132][-5.5965195 -6.4416351 -6.4450917 -5.3450289 -3.6054678 -1.8971021 -0.084811211 1.4518585 1.71527 0.23120213 -1.4487591 -2.6352553 -3.5090287 -3.908335 -4.0335646][-6.4245958 -7.7228346 -7.9791765 -7.5472894 -6.4819846 -5.2158046 -3.8981838 -3.1540916 -2.9754319 -3.1719332 -3.7590766 -4.6256118 -4.9436407 -4.9394412 -4.7591739][-7.4053612 -8.626976 -9.7090206 -9.5852089 -8.3907175 -7.5646009 -6.9758911 -6.3973079 -5.6808491 -5.4192748 -5.6593513 -5.8253622 -5.7653074 -5.331028 -4.6861668][-7.6290832 -8.9473457 -9.83312 -10.082644 -9.8296423 -8.8665571 -7.8619404 -7.516077 -7.354908 -7.0594268 -6.7731752 -6.4927878 -6.0105042 -5.4490547 -5.1418309][-6.6384506 -8.1630812 -9.0583858 -9.1446629 -8.902113 -8.32709 -7.9301586 -7.657063 -7.2094784 -6.7705073 -6.2734151 -5.9109826 -5.8269215 -5.381011 -4.8875208][-5.48232 -6.6729512 -7.3891392 -7.5237713 -7.3880515 -7.0626016 -6.577117 -6.2589331 -5.9141531 -5.4446449 -5.0285187 -4.7498837 -4.6087308 -4.5181723 -4.4536114]]...]
INFO - root - 2017-12-16 08:09:23.896424: step 56510, loss = 0.28, batch loss = 0.22 (27.0 examples/sec; 0.297 sec/batch; 22h:44m:26s remains)
INFO - root - 2017-12-16 08:09:26.739828: step 56520, loss = 0.32, batch loss = 0.26 (27.9 examples/sec; 0.287 sec/batch; 21h:57m:51s remains)
INFO - root - 2017-12-16 08:09:29.554024: step 56530, loss = 0.28, batch loss = 0.22 (29.1 examples/sec; 0.275 sec/batch; 21h:05m:35s remains)
INFO - root - 2017-12-16 08:09:32.411285: step 56540, loss = 0.25, batch loss = 0.19 (27.5 examples/sec; 0.291 sec/batch; 22h:16m:39s remains)
INFO - root - 2017-12-16 08:09:35.309445: step 56550, loss = 0.32, batch loss = 0.26 (25.8 examples/sec; 0.310 sec/batch; 23h:47m:58s remains)
INFO - root - 2017-12-16 08:09:38.158041: step 56560, loss = 0.31, batch loss = 0.25 (28.2 examples/sec; 0.283 sec/batch; 21h:43m:48s remains)
INFO - root - 2017-12-16 08:09:41.011195: step 56570, loss = 0.21, batch loss = 0.15 (27.9 examples/sec; 0.287 sec/batch; 21h:59m:34s remains)
INFO - root - 2017-12-16 08:09:43.843902: step 56580, loss = 0.26, batch loss = 0.20 (26.6 examples/sec; 0.301 sec/batch; 23h:03m:45s remains)
INFO - root - 2017-12-16 08:09:46.690669: step 56590, loss = 0.33, batch loss = 0.28 (27.9 examples/sec; 0.287 sec/batch; 21h:59m:56s remains)
INFO - root - 2017-12-16 08:09:49.550092: step 56600, loss = 0.30, batch loss = 0.24 (28.4 examples/sec; 0.282 sec/batch; 21h:36m:42s remains)
2017-12-16 08:09:50.058963: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8215292 -3.1776648 -3.6209426 -4.036356 -4.3921995 -4.6300759 -4.8256555 -4.9167647 -4.9054513 -4.6971765 -4.4640808 -4.3257508 -4.1684575 -4.0757866 -4.1210532][-2.3381732 -2.6851878 -3.0452824 -3.4436202 -3.764101 -4.1338539 -4.3047781 -4.4577551 -4.5856543 -4.715065 -4.6439734 -4.4284439 -4.2381358 -4.1231775 -4.1074][-2.2970641 -2.563941 -2.782371 -2.9132085 -3.0112097 -3.2253082 -3.3940337 -3.7057848 -3.9549198 -4.2121224 -4.5865073 -4.6536679 -4.4793916 -4.14849 -3.9411933][-2.3356023 -2.6237948 -2.7944603 -2.5735075 -2.1291885 -1.8695195 -1.9874833 -2.4106808 -2.9542322 -3.5841084 -4.2626677 -4.48223 -4.3170447 -4.0411191 -3.8493292][-2.3252592 -2.4523568 -2.2946644 -1.7354407 -0.89840794 -0.23290348 -0.097432613 -0.60677457 -1.58255 -2.6525426 -3.4388714 -3.9028616 -3.916723 -3.5470269 -3.2640026][-2.4898343 -2.3630834 -1.8697507 -0.83601761 0.53883505 1.5228343 1.5890698 0.81868267 -0.26155233 -1.5664201 -2.8336535 -3.3857622 -3.3064961 -2.9682336 -2.7370274][-2.7794991 -2.5162251 -1.7451112 -0.37780428 1.2663674 2.5696111 2.9130416 2.1010041 0.70722008 -0.58491564 -1.6345525 -2.2895186 -2.453521 -2.2814124 -2.0416732][-2.9216118 -2.5725932 -1.761939 -0.37571764 1.1796007 2.458127 2.8983388 2.4131641 1.4260964 0.10285997 -1.0731256 -1.6451476 -1.6885264 -1.6046667 -1.540911][-3.0205634 -2.6975164 -1.9471505 -0.81305003 0.43053389 1.5502105 1.8031697 1.4397755 0.80061817 -0.0567708 -0.7760973 -1.2697463 -1.4374285 -1.2775304 -1.0691578][-3.1863339 -3.0549765 -2.6496005 -1.9192276 -1.0250189 -0.20663595 0.16221762 -0.024786472 -0.4625392 -0.91434073 -1.2054679 -1.3553009 -1.3500006 -1.1479678 -0.80009294][-3.9104798 -4.0208621 -3.9497483 -3.5036995 -2.920773 -2.4663324 -2.1528637 -2.0649858 -2.23876 -2.3197262 -2.260638 -2.0159798 -1.5860026 -1.1161821 -0.57996655][-4.5640612 -5.1430626 -5.5715494 -5.5591865 -5.1791773 -4.70605 -4.4733615 -4.3006816 -4.2396994 -3.9338565 -3.6956074 -3.1803298 -2.3954043 -1.5605431 -0.68981075][-5.47099 -6.2323341 -6.8576288 -7.1287003 -7.1101093 -6.7726526 -6.303298 -5.9834132 -5.9209619 -5.6890974 -5.1907406 -4.5468917 -3.7607803 -2.7994328 -1.8070071][-6.3609247 -7.2649574 -7.7876391 -8.0238018 -7.981822 -7.7472982 -7.5537839 -7.3035879 -7.0310183 -6.8284903 -6.5541811 -5.9762793 -5.1414847 -4.2790523 -3.3891549][-6.6667023 -7.6697469 -8.3473663 -8.5866795 -8.4263191 -8.1527939 -8.0107632 -7.7320118 -7.6360421 -7.51054 -7.3947983 -7.1563873 -6.4840465 -5.7358923 -5.1472826]]...]
INFO - root - 2017-12-16 08:09:52.865900: step 56610, loss = 0.28, batch loss = 0.22 (27.9 examples/sec; 0.287 sec/batch; 21h:59m:13s remains)
INFO - root - 2017-12-16 08:09:55.695644: step 56620, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 21h:39m:20s remains)
INFO - root - 2017-12-16 08:09:58.612401: step 56630, loss = 0.23, batch loss = 0.18 (26.4 examples/sec; 0.303 sec/batch; 23h:13m:27s remains)
INFO - root - 2017-12-16 08:10:01.449409: step 56640, loss = 0.22, batch loss = 0.16 (28.3 examples/sec; 0.282 sec/batch; 21h:38m:29s remains)
INFO - root - 2017-12-16 08:10:04.277077: step 56650, loss = 0.27, batch loss = 0.21 (27.8 examples/sec; 0.288 sec/batch; 22h:02m:27s remains)
INFO - root - 2017-12-16 08:10:07.138244: step 56660, loss = 0.29, batch loss = 0.23 (26.5 examples/sec; 0.302 sec/batch; 23h:09m:58s remains)
INFO - root - 2017-12-16 08:10:10.013350: step 56670, loss = 0.36, batch loss = 0.30 (28.0 examples/sec; 0.286 sec/batch; 21h:52m:54s remains)
INFO - root - 2017-12-16 08:10:12.898106: step 56680, loss = 0.27, batch loss = 0.21 (28.4 examples/sec; 0.281 sec/batch; 21h:33m:14s remains)
INFO - root - 2017-12-16 08:10:15.776008: step 56690, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.281 sec/batch; 21h:32m:27s remains)
INFO - root - 2017-12-16 08:10:18.649581: step 56700, loss = 0.34, batch loss = 0.29 (27.9 examples/sec; 0.287 sec/batch; 21h:57m:27s remains)
2017-12-16 08:10:19.184941: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0072556 -5.1623535 -4.0251207 -3.0131838 -2.2289529 -1.9840183 -1.9287443 -2.2778938 -3.1854224 -4.0855041 -4.5821042 -4.8988447 -4.8675809 -4.3437409 -3.6364703][-5.2366147 -4.4653645 -3.4698887 -2.5005236 -1.8347869 -1.4961152 -1.4923158 -1.9196899 -2.8032556 -3.6625628 -4.3471189 -4.63869 -4.5895824 -4.2993813 -3.6691527][-4.3017249 -3.5783408 -2.7732003 -1.9592421 -1.3889735 -1.1383159 -1.1306503 -1.5852687 -2.5315981 -3.5949831 -4.4384031 -4.9990425 -5.0533695 -4.6532116 -3.9757125][-3.0643611 -2.5321498 -1.8496196 -1.3458798 -0.943243 -0.74966812 -0.7808373 -1.1815302 -2.1728563 -3.3089876 -4.3156075 -4.9311261 -5.0647421 -4.6294842 -3.9585173][-1.9936833 -1.5778799 -1.0579059 -0.75261521 -0.17426682 0.34205866 0.53837585 0.27153778 -0.7976253 -2.146842 -3.5211554 -4.5064721 -4.8869758 -4.7451215 -4.0639472][-1.648325 -1.3989072 -0.97105074 -0.5789597 0.27391195 1.2182183 1.8425269 1.7202096 0.81685829 -0.83115196 -2.57085 -4.011508 -4.8639479 -5.0286961 -4.5707345][-1.6317992 -1.5699482 -1.2464402 -0.71306181 0.32794333 1.5583687 2.4601917 2.5019498 1.5429082 -0.22763014 -2.3099124 -4.1691051 -5.2512469 -5.6406574 -5.2912407][-1.9951916 -1.8563821 -1.589721 -0.94864464 0.36056423 1.6277261 2.4910183 2.7067237 1.8473263 -0.044825554 -2.3461962 -4.271946 -5.5514312 -6.0411034 -5.6417732][-2.3452415 -2.2998955 -2.01329 -1.5029435 -0.2373085 1.0833945 2.1405749 2.4251704 1.5823612 -0.13668251 -2.3588362 -4.4688296 -5.9526534 -6.5025377 -6.1265507][-2.40203 -2.5799575 -2.4367871 -2.039217 -1.1254089 0.046433449 1.0755959 1.5106187 0.83543873 -0.80082393 -2.9375851 -4.9093742 -6.356297 -7.0124884 -6.6285787][-2.1862814 -2.6236739 -2.9663463 -2.844368 -2.0452561 -0.92609882 -0.0077962875 0.38622046 -0.12550306 -1.6236989 -3.5555556 -5.4257374 -6.7217665 -7.1877489 -6.7915535][-2.3523128 -2.9991164 -3.3333755 -3.2776313 -2.652283 -1.6642923 -0.86771107 -0.5653336 -1.1091692 -2.4472983 -4.1861744 -5.8682566 -6.8779345 -7.0329642 -6.4591613][-2.4218051 -3.2067709 -3.6410351 -3.5939553 -2.9022882 -2.0429857 -1.4719455 -1.1671641 -1.8163171 -3.0507364 -4.5603437 -5.9196687 -6.5610304 -6.6129937 -6.0744123][-2.4866793 -3.2524886 -3.7262478 -3.631012 -3.0872617 -2.2348468 -1.6597395 -1.6309323 -2.2409074 -3.3228755 -4.6605315 -5.6739416 -6.191813 -6.1296287 -5.4788332][-3.1167383 -3.814081 -4.0426512 -3.7266452 -3.0892048 -2.254566 -1.7363675 -1.8120437 -2.4046896 -3.5330081 -4.672204 -5.5856667 -5.9163151 -5.6007214 -4.860178]]...]
INFO - root - 2017-12-16 08:10:22.074257: step 56710, loss = 0.26, batch loss = 0.21 (27.4 examples/sec; 0.292 sec/batch; 22h:22m:05s remains)
INFO - root - 2017-12-16 08:10:24.898934: step 56720, loss = 0.18, batch loss = 0.12 (28.9 examples/sec; 0.277 sec/batch; 21h:10m:57s remains)
INFO - root - 2017-12-16 08:10:27.727113: step 56730, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.284 sec/batch; 21h:46m:11s remains)
INFO - root - 2017-12-16 08:10:30.594605: step 56740, loss = 0.28, batch loss = 0.22 (27.2 examples/sec; 0.294 sec/batch; 22h:29m:33s remains)
INFO - root - 2017-12-16 08:10:33.415886: step 56750, loss = 0.27, batch loss = 0.21 (29.3 examples/sec; 0.273 sec/batch; 20h:55m:26s remains)
INFO - root - 2017-12-16 08:10:36.275946: step 56760, loss = 0.32, batch loss = 0.27 (28.4 examples/sec; 0.282 sec/batch; 21h:35m:20s remains)
INFO - root - 2017-12-16 08:10:39.083788: step 56770, loss = 0.40, batch loss = 0.34 (28.0 examples/sec; 0.285 sec/batch; 21h:51m:33s remains)
INFO - root - 2017-12-16 08:10:41.987905: step 56780, loss = 0.27, batch loss = 0.22 (27.8 examples/sec; 0.288 sec/batch; 22h:01m:51s remains)
INFO - root - 2017-12-16 08:10:44.840344: step 56790, loss = 0.22, batch loss = 0.16 (26.8 examples/sec; 0.298 sec/batch; 22h:50m:25s remains)
INFO - root - 2017-12-16 08:10:47.694255: step 56800, loss = 0.36, batch loss = 0.30 (29.4 examples/sec; 0.272 sec/batch; 20h:49m:50s remains)
2017-12-16 08:10:48.194342: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2269716 -5.3900204 -5.3058395 -4.7818651 -4.0484715 -3.6262803 -3.5258279 -3.6816025 -3.8353097 -4.1159296 -4.437706 -4.7777839 -5.0761256 -5.52135 -5.6277208][-4.6038094 -4.7029219 -4.5701361 -4.5741768 -4.4822383 -4.5855045 -4.5990191 -4.6422567 -4.6215596 -4.7608132 -4.9635439 -5.2154408 -5.4147773 -5.5714173 -5.58165][-3.1261806 -3.549602 -4.0867233 -4.5289955 -4.7147503 -4.9777169 -4.9683414 -5.0714288 -5.134491 -5.1628876 -5.3456521 -5.405447 -5.5898714 -5.7090688 -5.6105042][-1.9033661 -2.2309988 -2.736474 -3.4964421 -4.0533457 -4.5120959 -4.5150623 -4.5561242 -4.5939121 -4.7962961 -5.1259971 -5.4499621 -5.886076 -5.8354616 -5.7287617][-0.60198307 -1.0172379 -1.8912284 -2.7448545 -3.1962645 -3.4682105 -3.2152524 -3.0468857 -2.9395504 -3.45313 -4.200717 -4.7107134 -5.0446663 -5.1637974 -5.34654][-0.87716341 -1.0442307 -1.4447813 -1.9920664 -2.0477071 -2.0741551 -1.4969993 -0.91984987 -0.80080295 -1.2317066 -1.9680598 -2.9082336 -3.9047058 -4.1390381 -4.1476855][-1.5465379 -1.8027956 -1.9148726 -1.6808672 -0.86386251 0.056222439 1.2772455 2.0489535 2.1559691 1.4574385 0.17917967 -1.065717 -2.3905132 -2.9837711 -3.1599579][-2.1076849 -2.2096102 -2.1166778 -1.5844812 -0.571785 1.0291719 2.8620915 3.6698904 3.6048517 2.753427 1.5244498 0.019105434 -1.6430776 -2.446141 -2.8741121][-3.4421458 -3.6011324 -3.4257169 -2.6059618 -1.3957136 0.21797037 2.195859 3.3335395 3.5911655 2.6813631 1.2791162 -0.175529 -1.4892306 -2.2867119 -2.812989][-4.402566 -5.1822519 -5.529767 -4.6886411 -3.2821436 -1.5338612 0.21896267 1.4693999 1.9580307 1.4798818 0.50380754 -0.94933367 -2.2243068 -2.8434043 -3.12542][-5.6961589 -6.3593335 -6.4972148 -5.9974189 -5.0453873 -3.2840257 -1.5938575 -0.72928286 -0.4731288 -0.79832029 -1.5803928 -2.6310773 -3.4396069 -3.7326765 -3.8308821][-6.1577015 -6.6466913 -6.887702 -6.59194 -5.9562259 -4.9326262 -3.8548436 -3.0836439 -2.4980011 -2.5733662 -3.0175891 -3.8163593 -4.3917613 -4.4451585 -4.1698813][-5.999773 -6.5360279 -6.918807 -6.675828 -6.1843743 -5.4155021 -4.6577039 -4.2296443 -3.8135428 -3.4955006 -3.39074 -3.7947209 -4.0840349 -4.1597452 -4.0031309][-5.7652364 -5.9094315 -5.9992189 -5.7887259 -5.5165763 -4.9565873 -4.3847241 -3.9706554 -3.5643904 -3.2816539 -3.129086 -3.29637 -3.348295 -3.2192287 -3.1156][-5.1583943 -4.9941745 -4.89246 -4.6777949 -4.3709483 -4.0227218 -3.6741371 -3.2369528 -2.7725959 -2.4255803 -2.2098155 -2.2013571 -2.1882529 -2.2775061 -2.2474854]]...]
INFO - root - 2017-12-16 08:10:51.041040: step 56810, loss = 0.27, batch loss = 0.22 (28.6 examples/sec; 0.280 sec/batch; 21h:25m:29s remains)
INFO - root - 2017-12-16 08:10:53.878637: step 56820, loss = 0.27, batch loss = 0.22 (27.7 examples/sec; 0.289 sec/batch; 22h:07m:38s remains)
INFO - root - 2017-12-16 08:10:56.744226: step 56830, loss = 0.34, batch loss = 0.28 (27.9 examples/sec; 0.287 sec/batch; 21h:56m:53s remains)
INFO - root - 2017-12-16 08:10:59.622141: step 56840, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 21h:36m:16s remains)
INFO - root - 2017-12-16 08:11:02.466532: step 56850, loss = 0.23, batch loss = 0.17 (27.8 examples/sec; 0.287 sec/batch; 22h:00m:16s remains)
INFO - root - 2017-12-16 08:11:05.334382: step 56860, loss = 0.20, batch loss = 0.14 (27.9 examples/sec; 0.286 sec/batch; 21h:55m:20s remains)
INFO - root - 2017-12-16 08:11:08.182123: step 56870, loss = 0.24, batch loss = 0.19 (28.2 examples/sec; 0.284 sec/batch; 21h:44m:37s remains)
INFO - root - 2017-12-16 08:11:11.021455: step 56880, loss = 0.23, batch loss = 0.18 (27.9 examples/sec; 0.287 sec/batch; 21h:57m:07s remains)
INFO - root - 2017-12-16 08:11:13.879861: step 56890, loss = 0.46, batch loss = 0.40 (27.9 examples/sec; 0.287 sec/batch; 21h:57m:58s remains)
INFO - root - 2017-12-16 08:11:16.720789: step 56900, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.281 sec/batch; 21h:31m:35s remains)
2017-12-16 08:11:17.218543: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6519079 -3.0767322 -3.6565301 -4.2557459 -4.77577 -5.2080765 -5.6600795 -6.2025223 -6.6086469 -6.6050739 -6.2470174 -5.7654748 -5.2239823 -4.5859165 -3.9416537][-3.1938307 -3.7473958 -4.4088073 -5.0832176 -5.6983685 -6.1433249 -6.5291886 -6.9544311 -7.2777948 -7.3458471 -7.0647964 -6.461812 -5.7820716 -5.1033573 -4.41586][-3.947742 -4.6359868 -5.3978019 -5.959712 -6.2489147 -6.4368382 -6.52564 -6.6371074 -6.8317866 -7.0384297 -7.0177546 -6.774826 -6.3511691 -5.7186651 -4.9270358][-4.6776848 -5.3183236 -6.0022206 -6.5165205 -6.6150761 -6.26202 -5.8014064 -5.5594091 -5.5881639 -5.7190833 -5.8313923 -6.0595036 -6.1692066 -5.8324251 -5.1838007][-5.3760109 -5.833334 -6.0255566 -6.0425868 -5.6903243 -4.8873706 -3.8726897 -3.0731292 -3.020129 -3.4635594 -4.1114421 -4.8262267 -5.4164753 -5.5514154 -5.19911][-5.4861212 -5.9530172 -5.8147764 -5.2088552 -4.1801972 -2.962142 -1.5185404 -0.22332573 0.040504456 -0.6889 -1.9695022 -3.4193449 -4.7258058 -5.3673048 -5.2499824][-4.7799253 -5.2137756 -4.9704933 -4.01293 -2.5610991 -1.0573924 0.47112083 1.7573752 2.1796713 1.362493 -0.2620821 -2.1289988 -3.9245577 -4.9283042 -5.0183182][-4.0474072 -4.1609645 -3.901814 -2.880085 -1.0841124 0.80990267 2.4405136 3.2450891 3.1674266 2.1446218 0.23421192 -1.9422834 -3.9089215 -4.8644319 -5.0287366][-3.5818739 -3.4045632 -2.9245582 -2.1759202 -0.84523845 0.819819 2.3106685 2.9826326 2.6663446 1.4269352 -0.39140654 -2.4237049 -4.312923 -5.2850518 -5.37272][-3.5903296 -3.3794465 -2.7357368 -1.8912585 -1.0220196 0.0065674782 1.0094109 1.4929247 0.989748 -0.37597942 -2.0015521 -3.6482732 -5.0933785 -5.8266196 -5.73822][-3.9582839 -3.6219578 -3.1431029 -2.3784633 -1.6051097 -1.0796618 -0.68649483 -0.52562857 -0.92610121 -2.1796584 -3.6268039 -4.8647246 -5.7739997 -6.0870724 -5.8060422][-4.4938874 -4.0127726 -3.5067194 -3.0548863 -2.5248327 -2.214781 -2.1914868 -2.577641 -3.1697078 -4.045001 -4.9495254 -5.7555723 -6.2328916 -6.0552735 -5.4152174][-5.0236378 -4.3619089 -3.5998664 -3.1797135 -2.9684067 -2.9617524 -3.2040172 -3.7236738 -4.5113239 -5.3127904 -5.9139628 -6.2622328 -6.3182144 -5.8442268 -5.0236783][-5.1554914 -4.52887 -3.7523046 -3.3125334 -3.2534065 -3.3807969 -3.638505 -4.0025482 -4.7032561 -5.421855 -5.7444105 -5.8515964 -5.75599 -5.2750678 -4.5329027][-4.4833441 -3.8477192 -3.1235309 -2.7533231 -2.5869932 -2.7289424 -3.0488482 -3.392534 -3.9924812 -4.7075963 -5.1214595 -5.1546812 -4.9495668 -4.4709992 -3.881022]]...]
INFO - root - 2017-12-16 08:11:20.069892: step 56910, loss = 0.21, batch loss = 0.15 (27.1 examples/sec; 0.296 sec/batch; 22h:38m:03s remains)
INFO - root - 2017-12-16 08:11:22.899979: step 56920, loss = 0.21, batch loss = 0.15 (29.4 examples/sec; 0.272 sec/batch; 20h:49m:41s remains)
INFO - root - 2017-12-16 08:11:25.782260: step 56930, loss = 0.33, batch loss = 0.27 (27.2 examples/sec; 0.294 sec/batch; 22h:30m:05s remains)
INFO - root - 2017-12-16 08:11:28.616273: step 56940, loss = 0.26, batch loss = 0.20 (29.5 examples/sec; 0.271 sec/batch; 20h:44m:33s remains)
INFO - root - 2017-12-16 08:11:31.454433: step 56950, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.284 sec/batch; 21h:44m:32s remains)
INFO - root - 2017-12-16 08:11:34.331052: step 56960, loss = 0.26, batch loss = 0.20 (27.6 examples/sec; 0.289 sec/batch; 22h:09m:20s remains)
INFO - root - 2017-12-16 08:11:37.162283: step 56970, loss = 0.24, batch loss = 0.18 (28.7 examples/sec; 0.278 sec/batch; 21h:18m:47s remains)
INFO - root - 2017-12-16 08:11:40.003991: step 56980, loss = 0.36, batch loss = 0.30 (28.6 examples/sec; 0.280 sec/batch; 21h:26m:21s remains)
INFO - root - 2017-12-16 08:11:42.876723: step 56990, loss = 0.22, batch loss = 0.17 (29.1 examples/sec; 0.275 sec/batch; 21h:01m:09s remains)
INFO - root - 2017-12-16 08:11:45.684503: step 57000, loss = 0.24, batch loss = 0.18 (27.6 examples/sec; 0.290 sec/batch; 22h:11m:44s remains)
2017-12-16 08:11:46.252587: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.32087 -5.6442194 -5.2616873 -5.5731211 -6.1618872 -6.6572285 -7.0316377 -7.5716729 -8.1877928 -8.55489 -8.5381165 -7.9585848 -7.2059813 -6.2982707 -5.553596][-5.8767138 -5.1753259 -4.8896928 -5.1106358 -5.5198464 -6.2808609 -6.9520326 -7.6107264 -8.0044794 -8.5088778 -9.0746813 -8.9710579 -8.5572681 -7.5800009 -6.7387094][-4.7002449 -3.9318798 -3.6484606 -3.8776307 -4.2145038 -4.6663928 -5.2134194 -6.0632415 -6.8085265 -7.5198345 -8.3442516 -8.8006935 -8.926754 -8.2577925 -7.3335624][-3.5655444 -2.5227292 -2.2563353 -2.4495564 -2.7643661 -3.0650663 -3.0691276 -3.6248159 -4.3609147 -5.3183336 -6.470274 -7.4921637 -8.1769762 -7.8019009 -6.9139223][-2.7304382 -1.559139 -0.93621612 -0.99218822 -1.1474566 -1.1019571 -0.81113458 -0.90578747 -1.3118117 -2.3758242 -4.00741 -5.4142118 -6.5002832 -6.675169 -6.1148028][-1.9895172 -0.90764046 -0.13981676 0.39971733 0.93847561 1.2062402 1.6384969 1.9305725 1.6177621 0.48032713 -1.4665079 -3.3999066 -4.8597512 -5.233602 -4.6938996][-2.4157076 -1.0495486 -0.040513992 0.88507795 1.866991 2.9282951 3.8971071 4.209424 3.7046595 2.28226 0.12536955 -2.1090097 -3.6835155 -4.2066336 -3.7585638][-3.7112632 -2.3394744 -1.1531618 0.21273613 1.5944042 2.8898649 4.1589022 4.8078146 4.4185467 2.7681293 0.45535707 -1.8459067 -3.3813033 -3.8669593 -3.4977379][-5.4132891 -4.228756 -3.001997 -1.5161984 0.030077934 1.7073278 3.1763515 3.8183069 3.464334 2.0345645 -0.10599899 -2.3762925 -3.7254212 -4.1382265 -3.749989][-6.2911477 -5.3815632 -4.5206127 -3.1020088 -1.6551194 -0.19014931 0.97831297 1.61724 1.4446764 0.14085054 -1.5870335 -3.4938416 -4.8195281 -5.0923724 -4.5007496][-7.3715687 -6.61481 -5.8230066 -4.8704982 -3.9015412 -2.7240474 -1.8306961 -1.4726322 -1.5221596 -2.2594461 -3.4526267 -4.7555251 -5.5905066 -5.853868 -5.2076473][-7.5922804 -7.3059225 -7.031579 -6.4315395 -5.6652994 -4.9079227 -4.35812 -3.9910047 -3.9755106 -4.4066224 -5.0005293 -5.8268328 -6.4184184 -6.3126545 -5.4525633][-6.8485107 -6.8274608 -6.8634567 -6.8253326 -6.7040081 -6.3556495 -5.9729805 -5.746068 -5.6782446 -5.7762113 -5.9799461 -6.2953715 -6.3587294 -6.241261 -5.5655437][-5.8779511 -5.9653916 -6.1262927 -6.3100824 -6.4522514 -6.5346928 -6.592618 -6.4555869 -6.2500396 -6.1437907 -6.1603003 -6.2311244 -6.1661205 -5.9104195 -5.2867532][-5.0423255 -5.1416621 -5.3020697 -5.4543138 -5.5777454 -5.7673454 -5.90514 -5.8365746 -5.7801785 -5.6870646 -5.6037488 -5.5797253 -5.4669881 -5.2366014 -4.785624]]...]
INFO - root - 2017-12-16 08:11:49.089065: step 57010, loss = 0.20, batch loss = 0.14 (28.7 examples/sec; 0.279 sec/batch; 21h:21m:02s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 08:11:51.930491: step 57020, loss = 0.42, batch loss = 0.36 (28.3 examples/sec; 0.282 sec/batch; 21h:36m:15s remains)
INFO - root - 2017-12-16 08:11:54.748248: step 57030, loss = 0.21, batch loss = 0.15 (29.1 examples/sec; 0.275 sec/batch; 21h:01m:15s remains)
INFO - root - 2017-12-16 08:11:57.609258: step 57040, loss = 0.30, batch loss = 0.24 (27.4 examples/sec; 0.292 sec/batch; 22h:20m:15s remains)
INFO - root - 2017-12-16 08:12:00.449902: step 57050, loss = 0.29, batch loss = 0.23 (27.7 examples/sec; 0.289 sec/batch; 22h:06m:29s remains)
INFO - root - 2017-12-16 08:12:03.267149: step 57060, loss = 0.40, batch loss = 0.34 (27.5 examples/sec; 0.291 sec/batch; 22h:15m:57s remains)
INFO - root - 2017-12-16 08:12:06.094842: step 57070, loss = 0.22, batch loss = 0.16 (28.8 examples/sec; 0.278 sec/batch; 21h:15m:38s remains)
INFO - root - 2017-12-16 08:12:08.928907: step 57080, loss = 0.25, batch loss = 0.19 (29.0 examples/sec; 0.276 sec/batch; 21h:06m:28s remains)
INFO - root - 2017-12-16 08:12:11.734262: step 57090, loss = 0.25, batch loss = 0.19 (27.8 examples/sec; 0.288 sec/batch; 22h:02m:14s remains)
INFO - root - 2017-12-16 08:12:14.566543: step 57100, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.283 sec/batch; 21h:40m:34s remains)
2017-12-16 08:12:15.103714: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0629592 -5.0672536 -5.0688691 -5.2682381 -5.3190622 -5.130826 -5.0506053 -5.1299505 -5.1802597 -5.0482259 -4.8315496 -4.2146916 -3.5792725 -3.1541061 -2.8130894][-4.6100349 -4.6799307 -4.6198483 -4.4312816 -4.1691165 -4.494935 -4.82859 -4.9802618 -5.0347576 -4.8907809 -4.6734762 -4.1818709 -3.700042 -2.7810998 -1.8585856][-3.9206977 -3.6970727 -3.4736967 -3.2203135 -3.0966625 -3.1550698 -3.3641696 -3.9266367 -4.2353516 -4.5312729 -4.7864313 -4.5555873 -4.0225191 -3.2536993 -2.5659926][-2.3362389 -2.1197863 -1.9772272 -1.7272804 -1.6530735 -1.5696642 -1.809078 -2.5463078 -3.301481 -3.9072402 -4.2560987 -4.7374473 -4.8074527 -4.1468515 -3.3480964][-0.96665716 -0.38615847 -0.017498493 0.22528362 0.49341774 0.49515009 0.077421188 -0.61731577 -1.4318023 -2.6536469 -3.9075525 -4.6147056 -4.6471505 -4.3755035 -3.98139][-0.15693665 0.57273531 1.1578145 1.9439883 2.6971693 2.8357139 2.5795302 1.6321449 0.48862982 -0.74815536 -2.1086488 -3.5651863 -4.5706496 -4.6967449 -4.3252883][-0.12219477 0.73589134 1.5273299 2.511044 3.4721451 4.1700554 4.20866 3.3994193 2.414619 0.94447279 -0.92264724 -2.6073604 -3.7892923 -4.3197379 -4.4189467][-0.58739758 0.26702976 1.0671816 2.2776308 3.5469112 4.3325119 4.398756 3.9053268 3.0274873 1.3926134 -0.51623678 -2.3883584 -3.6447043 -4.3878031 -4.7869225][-1.8474696 -1.2223263 -0.27885294 0.9131856 2.1627469 3.2958746 3.7287855 3.1476059 2.1098895 0.77037096 -0.87654114 -2.6391468 -4.1087232 -4.8121104 -4.8914452][-2.8334844 -2.811728 -2.3197186 -1.0731268 0.24963379 1.1282644 1.5095077 1.2767892 0.63048792 -0.67388535 -2.2614012 -3.8861425 -5.3503704 -5.9248433 -6.0291085][-3.5149484 -3.6391711 -3.4160161 -2.9618154 -2.354182 -1.3493204 -0.72489715 -0.99886489 -1.8003061 -3.0536718 -4.3184996 -5.5778174 -6.6151533 -7.2430406 -7.2463574][-3.6239402 -4.0033326 -4.28079 -4.1811595 -3.7034898 -3.3511872 -3.0826383 -3.1811564 -3.5918581 -4.507195 -5.6615081 -6.8356662 -7.6558733 -7.7114506 -7.3027215][-3.576005 -3.9167295 -4.3096271 -4.6046181 -4.612617 -4.4655008 -4.3444653 -4.4048047 -4.6369729 -5.2123971 -5.994071 -6.6632557 -7.0750723 -7.0397735 -6.8149328][-3.5473754 -3.7050314 -4.2195511 -4.595418 -4.8294916 -4.6328921 -4.5340652 -4.7684369 -5.0364561 -5.1147761 -5.3258886 -5.8981991 -6.3771863 -6.1907678 -5.660759][-3.7768314 -3.729934 -3.9581754 -4.2646041 -4.5891881 -4.6450944 -4.6596637 -4.6248088 -4.5533037 -4.694829 -4.9142056 -4.963191 -4.9694271 -4.9157648 -4.7332463]]...]
INFO - root - 2017-12-16 08:12:17.901834: step 57110, loss = 0.30, batch loss = 0.24 (29.3 examples/sec; 0.273 sec/batch; 20h:53m:17s remains)
INFO - root - 2017-12-16 08:12:20.735003: step 57120, loss = 0.26, batch loss = 0.20 (29.1 examples/sec; 0.275 sec/batch; 21h:02m:07s remains)
INFO - root - 2017-12-16 08:12:23.586483: step 57130, loss = 0.27, batch loss = 0.22 (28.5 examples/sec; 0.281 sec/batch; 21h:28m:57s remains)
INFO - root - 2017-12-16 08:12:26.383862: step 57140, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 21h:39m:07s remains)
INFO - root - 2017-12-16 08:12:29.183295: step 57150, loss = 0.27, batch loss = 0.21 (28.1 examples/sec; 0.285 sec/batch; 21h:46m:08s remains)
INFO - root - 2017-12-16 08:12:32.053008: step 57160, loss = 0.31, batch loss = 0.25 (28.7 examples/sec; 0.279 sec/batch; 21h:20m:45s remains)
INFO - root - 2017-12-16 08:12:34.862536: step 57170, loss = 0.22, batch loss = 0.17 (28.4 examples/sec; 0.282 sec/batch; 21h:34m:19s remains)
INFO - root - 2017-12-16 08:12:37.682238: step 57180, loss = 0.26, batch loss = 0.20 (28.8 examples/sec; 0.278 sec/batch; 21h:15m:29s remains)
INFO - root - 2017-12-16 08:12:40.481057: step 57190, loss = 0.23, batch loss = 0.17 (27.6 examples/sec; 0.290 sec/batch; 22h:09m:34s remains)
INFO - root - 2017-12-16 08:12:43.346207: step 57200, loss = 0.22, batch loss = 0.16 (27.7 examples/sec; 0.289 sec/batch; 22h:04m:11s remains)
2017-12-16 08:12:43.836613: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3750424 -4.3625903 -4.2664642 -4.1856031 -4.1257634 -4.1017423 -3.9286509 -3.7245932 -3.5577264 -3.4499555 -3.3720145 -3.2967072 -3.1543825 -3.011013 -2.9541171][-3.5728245 -3.7156138 -3.8904343 -4.2560449 -4.5562458 -4.5660553 -4.2528195 -4.0254707 -3.8646071 -3.8498549 -3.8535962 -3.6938007 -3.4737198 -3.1359339 -2.9549384][-2.477864 -3.0315976 -3.7527382 -4.4534354 -4.90206 -4.943749 -4.7043848 -4.2191749 -3.8887262 -3.979744 -4.1716609 -4.136364 -3.9395406 -3.4145255 -3.0541327][-1.7759268 -2.8105819 -3.6457829 -4.1944919 -4.3774228 -4.3561521 -4.0448184 -3.662658 -3.351897 -3.4707577 -3.6896298 -3.9214482 -4.0700107 -3.702095 -3.3082123][-1.3220026 -2.1379216 -2.9395936 -3.4744902 -3.3101091 -2.7786269 -1.9778454 -1.6772118 -1.6888959 -2.1410031 -2.7400842 -3.3004847 -3.6357486 -3.5304325 -3.3432956][-1.8284352 -2.2685642 -2.6968436 -2.9059825 -2.3948045 -1.4130023 -0.14850521 0.62706947 0.76685858 -0.071766853 -1.2943976 -2.3105843 -3.0854421 -3.3351736 -3.3365712][-2.704103 -3.1033688 -3.22923 -2.7055223 -1.4548807 0.057631493 1.7101417 2.8126936 3.1368265 2.241652 0.6503129 -0.93865991 -2.2675457 -2.9120479 -3.2194514][-3.2658892 -3.558491 -3.533834 -2.860507 -1.237957 0.89966106 3.0404353 3.9889269 4.1154709 3.1937833 1.6234007 -0.17761755 -1.8363156 -2.8023691 -3.2351816][-3.97418 -4.2087574 -3.9331343 -3.1998754 -1.7422431 0.44338083 2.7163177 3.7279596 3.824295 2.7857928 1.1711183 -0.62400055 -2.0883336 -2.9012518 -3.2765636][-4.5029912 -4.8763556 -4.9084692 -4.1000018 -2.5577102 -0.81602812 0.845994 1.9925194 2.3235373 1.5805645 0.141994 -1.5600491 -2.8198318 -3.3190856 -3.2739096][-4.755127 -5.3500972 -5.54043 -4.9306393 -3.8363702 -2.4631581 -0.924721 -0.15826893 -0.10268068 -0.648891 -1.6915886 -2.8479962 -3.5318637 -3.6627367 -3.3760214][-4.6037264 -5.0521684 -5.3974996 -5.3124995 -4.7169232 -3.577693 -2.3971071 -1.858151 -1.7878408 -2.3777184 -3.29779 -3.9741879 -4.2746434 -4.0310612 -3.5121276][-4.3602519 -4.7770691 -5.1374812 -4.9945159 -4.5873508 -3.954829 -3.337606 -2.9182014 -2.8823161 -3.3437467 -3.8916578 -4.3129234 -4.4268284 -3.955795 -3.2039356][-4.0794 -4.1652613 -4.3067355 -4.121882 -3.675699 -3.1503086 -2.7865763 -2.6070275 -2.6493459 -2.9621038 -3.3346462 -3.6824324 -3.6896522 -3.4139078 -2.8048809][-3.6756816 -3.6623964 -3.6748109 -3.5079312 -3.1333284 -2.7291632 -2.4330378 -2.1484709 -2.1293211 -2.3568182 -2.7699642 -3.0245092 -2.99619 -2.8009937 -2.4547548]]...]
INFO - root - 2017-12-16 08:12:46.657644: step 57210, loss = 0.36, batch loss = 0.30 (28.9 examples/sec; 0.277 sec/batch; 21h:10m:26s remains)
INFO - root - 2017-12-16 08:12:49.517299: step 57220, loss = 0.24, batch loss = 0.18 (27.9 examples/sec; 0.287 sec/batch; 21h:55m:46s remains)
INFO - root - 2017-12-16 08:12:52.347391: step 57230, loss = 0.22, batch loss = 0.16 (29.1 examples/sec; 0.275 sec/batch; 21h:02m:34s remains)
INFO - root - 2017-12-16 08:12:55.161407: step 57240, loss = 0.19, batch loss = 0.13 (29.2 examples/sec; 0.274 sec/batch; 20h:57m:54s remains)
INFO - root - 2017-12-16 08:12:58.000539: step 57250, loss = 0.34, batch loss = 0.28 (29.5 examples/sec; 0.271 sec/batch; 20h:45m:23s remains)
INFO - root - 2017-12-16 08:13:00.858299: step 57260, loss = 0.26, batch loss = 0.20 (28.4 examples/sec; 0.282 sec/batch; 21h:32m:32s remains)
INFO - root - 2017-12-16 08:13:03.689318: step 57270, loss = 0.26, batch loss = 0.21 (27.9 examples/sec; 0.287 sec/batch; 21h:54m:57s remains)
INFO - root - 2017-12-16 08:13:06.493965: step 57280, loss = 0.33, batch loss = 0.27 (27.5 examples/sec; 0.290 sec/batch; 22h:12m:05s remains)
INFO - root - 2017-12-16 08:13:09.320823: step 57290, loss = 0.21, batch loss = 0.15 (27.9 examples/sec; 0.287 sec/batch; 21h:56m:42s remains)
INFO - root - 2017-12-16 08:13:12.142656: step 57300, loss = 0.24, batch loss = 0.19 (26.4 examples/sec; 0.303 sec/batch; 23h:07m:39s remains)
2017-12-16 08:13:12.720743: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0948963 -5.2756104 -5.3990006 -6.0190573 -6.4544225 -6.5915146 -6.3040361 -6.013443 -5.7542567 -5.6430492 -5.6715174 -5.9370966 -6.4311991 -6.8096905 -7.3397908][-4.6703658 -5.1963949 -5.5200891 -5.9736261 -6.4266925 -6.9417338 -6.7137928 -6.139967 -5.7589135 -5.6889009 -5.7198792 -6.0694838 -6.6439013 -7.1517935 -7.6083612][-4.0185509 -4.5305238 -4.879631 -5.226018 -5.5659146 -5.8785915 -5.7986588 -5.3918338 -5.1009736 -4.9321818 -5.4401674 -5.8311329 -6.2240105 -6.70186 -7.316184][-3.033483 -3.5941405 -3.7053258 -3.7497952 -3.7458258 -3.8700283 -3.6605418 -3.4082432 -3.2145648 -3.2080965 -3.5271297 -4.1685081 -5.3777552 -6.0314841 -6.7471194][-2.6367311 -2.689827 -2.3051319 -2.1357658 -1.8234484 -1.3789215 -0.94532728 -0.8107655 -0.74917746 -1.0561414 -1.4009485 -2.0628157 -3.0709333 -4.4268732 -6.0152969][-2.7814522 -2.4804487 -1.6004696 -0.7025125 0.24095964 0.87591934 1.4269333 2.0339561 2.0890408 1.8407745 1.4749961 0.34542561 -1.3802605 -3.0773342 -4.72069][-3.4681506 -2.7121534 -1.4253211 0.0002040863 1.4396982 2.4613166 3.4048047 4.0015354 4.373313 4.3430719 3.8840532 2.5098934 0.42675924 -2.0087454 -4.5175972][-4.446497 -3.4158003 -2.0181408 -0.40748358 1.235147 2.6082458 3.8420219 4.6559744 5.3723097 5.37401 4.9870605 3.3161502 1.0148425 -1.4401536 -3.6534123][-5.722105 -4.5937185 -3.2133756 -1.8369079 -0.41736269 1.1412449 2.7408643 3.741889 4.701148 4.9227266 4.643055 3.3243933 1.236918 -0.94026613 -2.9704509][-6.3818974 -5.6997643 -4.8446941 -3.544836 -2.3653369 -1.1859307 0.040435314 1.071701 1.948585 2.3958554 2.3858962 1.5188413 -0.015681744 -1.624326 -2.8546739][-6.7052751 -6.1551814 -5.5372591 -5.15851 -4.5560923 -3.479773 -2.4788046 -1.8873775 -1.4392586 -1.1520247 -0.90548444 -1.2144079 -1.9703228 -2.7250438 -3.2387459][-6.261137 -6.1139317 -6.0904474 -5.8854346 -5.6796179 -5.3960075 -4.8231525 -4.2564893 -3.7036414 -3.316395 -3.0830503 -3.109221 -3.2316368 -3.4928851 -3.7109632][-5.0486212 -5.0407457 -5.2122917 -5.2801595 -5.6269493 -5.6856694 -5.5190778 -5.3656244 -5.0632334 -4.8015118 -4.5495505 -4.330421 -4.2299633 -4.1272182 -4.1559668][-4.1010971 -4.10524 -4.2315292 -4.4292483 -4.782362 -4.872366 -4.9376779 -5.0954452 -5.1000171 -4.9071221 -4.5800138 -4.5730133 -4.5783668 -4.3755341 -4.127975][-3.4088888 -3.3568239 -3.441767 -3.6540918 -3.91531 -4.014605 -4.0360966 -3.9616108 -3.8473737 -3.8047304 -3.7530828 -3.5798433 -3.4551744 -3.5100789 -3.5031385]]...]
INFO - root - 2017-12-16 08:13:15.560799: step 57310, loss = 0.20, batch loss = 0.14 (28.6 examples/sec; 0.280 sec/batch; 21h:23m:05s remains)
INFO - root - 2017-12-16 08:13:18.407765: step 57320, loss = 0.25, batch loss = 0.19 (28.5 examples/sec; 0.281 sec/batch; 21h:27m:33s remains)
INFO - root - 2017-12-16 08:13:21.263507: step 57330, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.281 sec/batch; 21h:30m:35s remains)
INFO - root - 2017-12-16 08:13:24.143156: step 57340, loss = 0.27, batch loss = 0.21 (27.2 examples/sec; 0.294 sec/batch; 22h:29m:17s remains)
INFO - root - 2017-12-16 08:13:26.926204: step 57350, loss = 0.23, batch loss = 0.17 (29.8 examples/sec; 0.268 sec/batch; 20h:30m:05s remains)
INFO - root - 2017-12-16 08:13:29.809205: step 57360, loss = 0.20, batch loss = 0.14 (27.0 examples/sec; 0.297 sec/batch; 22h:40m:56s remains)
INFO - root - 2017-12-16 08:13:32.642312: step 57370, loss = 0.41, batch loss = 0.35 (27.3 examples/sec; 0.293 sec/batch; 22h:21m:51s remains)
INFO - root - 2017-12-16 08:13:35.479991: step 57380, loss = 0.24, batch loss = 0.19 (29.0 examples/sec; 0.276 sec/batch; 21h:05m:50s remains)
INFO - root - 2017-12-16 08:13:38.335891: step 57390, loss = 0.25, batch loss = 0.19 (28.6 examples/sec; 0.280 sec/batch; 21h:24m:17s remains)
INFO - root - 2017-12-16 08:13:41.160981: step 57400, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.281 sec/batch; 21h:29m:32s remains)
2017-12-16 08:13:41.697427: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3911777 -6.206305 -5.5085678 -5.0038328 -4.7808008 -5.0100064 -5.0274282 -5.1527214 -5.478951 -5.7945328 -6.1376019 -6.5012736 -6.5365486 -6.4671192 -6.2823992][-6.7704639 -6.1791887 -5.4528341 -4.9449081 -4.6899128 -4.6843796 -5.0456276 -5.4830022 -5.5921183 -5.8691254 -6.3401651 -6.6555386 -6.8180509 -6.93138 -6.6825871][-7.1366463 -6.2974648 -5.1147146 -4.3304806 -3.9740558 -4.0271921 -4.4124012 -4.86872 -5.2411165 -5.6770291 -5.9214678 -6.2019854 -6.6377239 -6.7988997 -6.6814842][-6.1630611 -5.5308495 -4.4974518 -3.2061439 -2.39661 -2.3343532 -2.6138206 -3.1637473 -3.9195962 -4.6884422 -4.8093 -5.21216 -5.9498787 -6.5951042 -6.81941][-5.7792044 -4.9360518 -3.4447093 -1.7468147 -0.64296818 -0.085288525 -0.11911201 -1.0325329 -1.8650544 -2.5109055 -2.9223142 -3.8741393 -4.7348647 -5.9215841 -7.0347614][-5.6028495 -4.6911798 -3.1092663 -0.82182455 1.0537663 1.9186311 1.9659686 1.4115372 0.6229229 -0.32650805 -0.85170436 -1.7792897 -3.2326021 -5.042469 -6.4521918][-6.403461 -5.1967535 -2.926861 -0.28035688 1.6847324 2.992981 3.5171862 2.8139176 1.7767963 0.918488 0.5559392 -0.7487092 -2.828198 -4.7824488 -6.5251417][-6.8261919 -5.878736 -3.5948758 -0.64738441 1.6666636 3.0815291 3.7097549 3.4711313 2.7917538 1.7498798 1.0232072 -0.4775548 -2.7039294 -5.0704618 -6.8741689][-7.5707054 -6.426754 -4.1414671 -1.644402 0.41647625 2.0207939 2.8967643 3.0110312 2.5134869 1.7737527 1.1601963 -0.31364775 -2.3983698 -4.8131356 -6.6801329][-8.1953144 -7.2484074 -5.4229746 -2.9439807 -1.107096 0.11595249 0.9611125 1.2380733 0.74775791 0.097882271 -0.65328145 -1.8766613 -3.6254268 -5.4041739 -6.9735885][-9.2552757 -8.2858343 -6.443284 -4.5855217 -3.2748737 -1.8854413 -0.82995462 -0.57263517 -0.90504718 -1.6906302 -2.5244293 -3.6248109 -5.0143223 -6.397923 -7.58745][-9.4053431 -8.6850681 -7.3576794 -5.9624176 -4.8726835 -3.9112163 -3.2031727 -2.9383259 -2.8631241 -3.2776024 -4.1125593 -5.178308 -6.1129136 -6.8860207 -7.6323433][-9.749897 -8.78217 -7.7171621 -6.7664552 -5.9007936 -5.2274857 -4.8520408 -4.6816111 -4.4529467 -4.566854 -4.8750725 -5.3979712 -6.0512371 -6.5859685 -6.9697838][-9.7530918 -8.9318924 -7.8829465 -7.002264 -6.4665289 -5.8885784 -5.4421854 -5.4068241 -5.4014039 -5.2237492 -5.1835613 -5.5044861 -5.8504958 -5.9689822 -6.0304775][-8.8122578 -8.0230961 -7.0671368 -6.515934 -6.3646069 -6.0867205 -5.707366 -5.6094222 -5.58463 -5.3740649 -5.24433 -5.1415172 -5.2750111 -5.4080644 -5.5335121]]...]
INFO - root - 2017-12-16 08:13:44.582701: step 57410, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.280 sec/batch; 21h:24m:26s remains)
INFO - root - 2017-12-16 08:13:47.451695: step 57420, loss = 0.25, batch loss = 0.20 (28.1 examples/sec; 0.285 sec/batch; 21h:44m:26s remains)
INFO - root - 2017-12-16 08:13:50.265800: step 57430, loss = 0.38, batch loss = 0.32 (27.2 examples/sec; 0.294 sec/batch; 22h:29m:15s remains)
INFO - root - 2017-12-16 08:13:53.144081: step 57440, loss = 0.35, batch loss = 0.29 (26.5 examples/sec; 0.302 sec/batch; 23h:03m:20s remains)
INFO - root - 2017-12-16 08:13:55.973002: step 57450, loss = 0.22, batch loss = 0.16 (28.0 examples/sec; 0.286 sec/batch; 21h:49m:31s remains)
INFO - root - 2017-12-16 08:13:58.826355: step 57460, loss = 0.25, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 21h:22m:40s remains)
INFO - root - 2017-12-16 08:14:01.655918: step 57470, loss = 0.28, batch loss = 0.22 (29.4 examples/sec; 0.272 sec/batch; 20h:47m:07s remains)
INFO - root - 2017-12-16 08:14:04.502728: step 57480, loss = 0.25, batch loss = 0.19 (29.2 examples/sec; 0.274 sec/batch; 20h:54m:25s remains)
INFO - root - 2017-12-16 08:14:07.293418: step 57490, loss = 0.30, batch loss = 0.24 (29.3 examples/sec; 0.273 sec/batch; 20h:50m:59s remains)
INFO - root - 2017-12-16 08:14:10.159266: step 57500, loss = 0.23, batch loss = 0.17 (28.9 examples/sec; 0.276 sec/batch; 21h:06m:35s remains)
2017-12-16 08:14:10.697286: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0028226 -1.9970841 -2.1728129 -2.3515286 -2.617497 -2.8317337 -3.2299671 -3.2394402 -3.1611602 -3.0261223 -3.1091394 -3.5772047 -3.9458911 -4.2486196 -4.8558388][-2.5643442 -2.7366965 -2.9886305 -3.3399997 -3.8738282 -4.0744939 -4.315309 -4.4314337 -4.5192113 -4.4238148 -4.3842616 -4.4607906 -4.6536312 -5.0624781 -5.3217859][-4.0947409 -4.2357531 -4.360908 -4.560504 -4.7988482 -5.0225387 -5.2873478 -5.3817644 -5.4810658 -5.6399755 -5.7650948 -5.7559996 -5.8490496 -5.904088 -5.8492184][-5.5569715 -5.7703228 -5.6487837 -5.455677 -5.2754989 -4.9514318 -4.7592778 -4.8821154 -5.1187272 -5.3734303 -5.6860065 -5.7968197 -5.9866304 -6.002182 -5.8433037][-6.1975245 -6.4593754 -6.4552503 -5.8743114 -5.0943847 -4.2449265 -3.5986962 -3.1916246 -3.0341802 -3.3527296 -3.8381472 -4.3954368 -5.1085 -5.413178 -5.5189257][-6.2441692 -6.1758118 -5.9241691 -5.1506472 -4.0033841 -2.6911874 -1.4414666 -0.7746973 -0.62591386 -0.93037271 -1.516845 -2.3024037 -3.2769194 -4.2400117 -4.9222555][-5.1881394 -5.1179876 -4.7691455 -3.8760831 -2.5125456 -0.9117794 0.7419157 1.7780108 2.2405052 1.9011745 0.98569441 -0.23774624 -1.6920345 -3.007762 -4.0515714][-3.6329391 -3.7399812 -3.4783802 -2.7550247 -1.6014955 0.041948318 2.0189729 3.3550019 4.0193672 3.8833065 2.8843799 1.3524551 -0.4714551 -2.274905 -3.6758783][-1.9670496 -2.3322515 -2.4393344 -1.8821187 -0.92716432 0.33742094 1.8277798 3.0754266 3.818264 3.7738075 2.8554945 1.3541579 -0.47553062 -2.2913008 -3.7480431][-2.3238909 -2.0427649 -1.6483202 -1.4476871 -0.8614862 0.19410086 1.3185911 2.0294504 2.2528133 2.0786328 1.2333488 -0.067809105 -1.545717 -2.9835792 -4.0807252][-2.7163644 -2.3233085 -1.909487 -1.4607584 -0.76928473 -0.026927948 0.53765249 0.92494822 0.8520937 0.40211105 -0.42091131 -1.5022438 -2.6847618 -3.5578797 -4.2270455][-3.7644424 -3.0247192 -2.2806869 -1.7616212 -1.2117312 -0.69189715 -0.45515656 -0.27098083 -0.50166464 -0.9532218 -1.5595374 -2.3482294 -3.2568102 -3.8893094 -4.1908674][-4.7555351 -3.855876 -2.8664455 -2.0442951 -1.4580891 -1.16941 -1.1727331 -1.1551805 -1.4745629 -1.8419554 -2.27534 -2.7614536 -3.2558582 -3.4232373 -3.5002081][-5.5184355 -4.4660926 -3.2981076 -2.2024653 -1.4154058 -1.0448627 -0.99461985 -0.98756385 -1.2904134 -1.450721 -1.6615965 -1.9784875 -2.4130268 -2.5823369 -2.7558577][-5.5526533 -4.6405826 -3.5078125 -2.2865148 -1.433697 -0.93350077 -0.66217256 -0.4427321 -0.37486696 -0.4389205 -0.70249486 -0.86257696 -1.1417696 -1.3076892 -1.5277169]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-57500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-57500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 08:14:14.379375: step 57510, loss = 0.32, batch loss = 0.27 (27.7 examples/sec; 0.289 sec/batch; 22h:04m:12s remains)
INFO - root - 2017-12-16 08:14:17.163891: step 57520, loss = 0.28, batch loss = 0.22 (29.1 examples/sec; 0.275 sec/batch; 20h:58m:25s remains)
INFO - root - 2017-12-16 08:14:20.030302: step 57530, loss = 0.25, batch loss = 0.19 (27.8 examples/sec; 0.288 sec/batch; 21h:59m:10s remains)
INFO - root - 2017-12-16 08:14:22.924735: step 57540, loss = 0.27, batch loss = 0.21 (25.5 examples/sec; 0.313 sec/batch; 23h:55m:11s remains)
INFO - root - 2017-12-16 08:14:25.750257: step 57550, loss = 0.25, batch loss = 0.20 (27.7 examples/sec; 0.289 sec/batch; 22h:04m:54s remains)
INFO - root - 2017-12-16 08:14:28.551278: step 57560, loss = 0.31, batch loss = 0.25 (28.6 examples/sec; 0.280 sec/batch; 21h:22m:06s remains)
INFO - root - 2017-12-16 08:14:31.402557: step 57570, loss = 0.29, batch loss = 0.23 (28.0 examples/sec; 0.286 sec/batch; 21h:49m:22s remains)
INFO - root - 2017-12-16 08:14:34.188463: step 57580, loss = 0.24, batch loss = 0.18 (28.9 examples/sec; 0.277 sec/batch; 21h:07m:13s remains)
INFO - root - 2017-12-16 08:14:37.057645: step 57590, loss = 0.29, batch loss = 0.23 (27.7 examples/sec; 0.288 sec/batch; 22h:00m:55s remains)
INFO - root - 2017-12-16 08:14:39.954239: step 57600, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.284 sec/batch; 21h:40m:41s remains)
2017-12-16 08:14:40.461176: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5128179 -3.7600107 -4.0654979 -4.3843212 -4.7555866 -5.1418548 -5.5035906 -5.8351917 -5.9633446 -5.8275452 -5.3874006 -4.9114404 -4.3937163 -3.8100767 -3.2926431][-3.6147592 -3.961858 -4.4114614 -4.9109588 -5.4439573 -5.9873848 -6.4535265 -6.6576471 -6.6872816 -6.7024031 -6.5217495 -6.11307 -5.4602084 -4.6976509 -3.9398556][-3.9783506 -4.4577141 -4.9485245 -5.3687291 -5.8598366 -6.2587562 -6.5721407 -6.9331646 -7.0633345 -6.9754124 -6.7614212 -6.5471487 -6.208106 -5.535984 -4.7141404][-4.3151412 -4.9871449 -5.5791264 -5.85909 -5.9818082 -5.7626958 -5.5528078 -5.393352 -5.2373271 -5.5376577 -6.00992 -6.3948817 -6.4912724 -6.0568562 -5.31065][-4.4069858 -4.81854 -5.0945673 -4.9859824 -4.4295235 -3.4675264 -2.4810209 -1.9990897 -2.0356555 -2.6395082 -3.69709 -5.0248218 -6.018436 -6.2391291 -5.717639][-4.4540768 -4.6381893 -4.4208412 -3.6489763 -2.1060061 -0.15167189 1.6052113 2.5527596 2.4469686 1.0659814 -1.0742383 -3.2790208 -5.0667372 -5.8495893 -5.7141266][-4.1683626 -4.2366924 -3.769855 -2.4405742 -0.31323957 2.2524223 4.7447777 6.0477877 5.9173727 4.3639069 1.7305965 -1.2667429 -3.7845473 -5.076499 -5.2690859][-3.7134283 -3.6635685 -3.1443079 -1.5479081 1.052618 3.9082832 6.52925 7.6693459 7.2489395 5.3085833 2.4341736 -0.52648115 -3.0079036 -4.4776859 -4.8309441][-3.5072751 -3.4646182 -2.9019103 -1.6764212 0.37195587 2.8530116 5.22744 6.3292055 5.86767 3.8619509 1.0940027 -1.5506887 -3.7241988 -4.7790055 -4.8914971][-3.0624869 -3.4203227 -3.4207907 -2.6283288 -1.1533267 0.27276945 1.5835962 2.2444487 1.9142032 0.37296438 -1.6411624 -3.5565937 -5.0461226 -5.5836725 -5.412745][-3.4538076 -3.8336391 -3.9760036 -3.8292112 -3.3975179 -2.5922427 -1.7942078 -1.717016 -2.2915623 -3.3741529 -4.5585151 -5.6406927 -6.1571217 -6.0215273 -5.5121841][-3.4785185 -4.0266471 -4.610692 -4.904675 -4.9109426 -5.0995469 -5.1746225 -5.2462416 -5.594655 -6.0470443 -6.4591727 -6.7316651 -6.6853952 -6.1842403 -5.4887276][-3.0743322 -3.5828195 -4.1695948 -4.7897458 -5.4690156 -5.9074717 -6.1581554 -6.61434 -6.8788033 -7.0276651 -6.9657221 -6.7015538 -6.28548 -5.7067442 -5.1317954][-2.6620159 -2.9641447 -3.34289 -3.9435258 -4.6770978 -5.2664218 -5.8032656 -6.1275983 -6.19588 -6.0336552 -5.6995006 -5.4046926 -4.9770594 -4.5657816 -4.2254038][-2.3038445 -2.419575 -2.6703157 -3.0869718 -3.5809512 -4.1126828 -4.5365295 -4.6394615 -4.5812993 -4.3269749 -4.0034232 -3.6203661 -3.3639908 -3.2491913 -3.1554842]]...]
INFO - root - 2017-12-16 08:14:43.265672: step 57610, loss = 0.27, batch loss = 0.21 (27.7 examples/sec; 0.288 sec/batch; 22h:01m:36s remains)
INFO - root - 2017-12-16 08:14:46.099532: step 57620, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.279 sec/batch; 21h:17m:17s remains)
INFO - root - 2017-12-16 08:14:48.976016: step 57630, loss = 0.27, batch loss = 0.21 (27.8 examples/sec; 0.288 sec/batch; 21h:58m:09s remains)
INFO - root - 2017-12-16 08:14:51.817526: step 57640, loss = 0.24, batch loss = 0.18 (28.5 examples/sec; 0.281 sec/batch; 21h:26m:26s remains)
INFO - root - 2017-12-16 08:14:54.674233: step 57650, loss = 0.24, batch loss = 0.18 (27.2 examples/sec; 0.294 sec/batch; 22h:26m:56s remains)
INFO - root - 2017-12-16 08:14:57.463949: step 57660, loss = 0.23, batch loss = 0.17 (29.1 examples/sec; 0.275 sec/batch; 20h:57m:36s remains)
INFO - root - 2017-12-16 08:15:00.326579: step 57670, loss = 0.28, batch loss = 0.22 (27.8 examples/sec; 0.288 sec/batch; 22h:00m:14s remains)
INFO - root - 2017-12-16 08:15:03.178223: step 57680, loss = 0.23, batch loss = 0.17 (27.2 examples/sec; 0.294 sec/batch; 22h:25m:39s remains)
INFO - root - 2017-12-16 08:15:06.010781: step 57690, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 21h:48m:50s remains)
INFO - root - 2017-12-16 08:15:08.863916: step 57700, loss = 0.33, batch loss = 0.27 (27.2 examples/sec; 0.294 sec/batch; 22h:25m:58s remains)
2017-12-16 08:15:09.390628: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8430734 -2.5704169 -2.3378606 -2.2717562 -2.2950716 -2.285145 -2.3619473 -2.4338548 -2.5823603 -2.9224029 -3.2333474 -3.4774723 -3.6359127 -3.8863535 -4.1229668][-2.7028952 -2.3901684 -2.2015002 -2.2369523 -2.439641 -2.6942344 -2.7687216 -2.8363068 -3.0936708 -3.3383002 -3.6046164 -3.9022439 -4.2518358 -4.6136465 -4.7532029][-2.6724153 -2.2482758 -1.9451249 -2.0176361 -2.294826 -2.6692481 -2.8851018 -3.007148 -3.3081636 -3.6159449 -3.9978063 -4.2403851 -4.5919023 -4.9919939 -5.3037462][-2.0800295 -1.5908298 -1.2237175 -1.2034419 -1.4043543 -1.8162684 -2.0877893 -2.2893329 -2.6500854 -3.0234494 -3.4752769 -3.8006678 -4.3172927 -4.7602262 -5.2772908][-1.871443 -1.2631614 -0.80256748 -0.64278173 -0.64643979 -0.79343033 -0.86554909 -1.1937993 -1.6387415 -2.0774179 -2.6868982 -3.1308842 -3.6297479 -4.1479473 -4.7752633][-1.5728557 -0.73929286 -0.13352871 0.13912773 0.28452682 0.35389757 0.4254303 0.21557999 -0.27669954 -0.848984 -1.5659173 -2.0399158 -2.6107311 -3.3774323 -4.0680704][-0.99936366 -0.21312857 0.35979176 0.80227423 1.2849503 1.7357969 2.1219511 2.1015429 1.5963879 0.97219992 0.043405533 -0.74710178 -1.6238644 -2.5634329 -3.3134232][-0.51066589 0.17096758 0.60299206 1.0194759 1.5840068 2.0183582 2.4620047 2.6029773 2.3920536 2.0254216 1.2482276 0.44965124 -0.58254218 -1.7667289 -2.8403788][-0.13534784 0.36020565 0.566072 0.76876163 1.0639672 1.4785218 1.9211364 2.1925769 2.1622791 1.8054991 1.3337908 0.91573048 0.085538387 -1.0269401 -2.1740139][0.13573933 0.49066496 0.5063405 0.43959951 0.66222143 0.97208357 1.2250605 1.3525271 1.3505039 1.2130041 0.9293952 0.5696969 0.089371204 -0.73243666 -1.6909401][0.30302811 0.61611319 0.41976643 0.26379681 0.2596035 0.46532297 0.64217091 0.75802135 0.84772253 0.72405052 0.58115625 0.38911295 0.0040884018 -0.76596 -1.5625751][-0.30881405 -0.027328968 -0.33732843 -0.60871696 -0.79509878 -0.74868774 -0.54945469 -0.49532056 -0.46146941 -0.42525721 -0.34617758 -0.14611483 -0.37307262 -0.875618 -1.5241935][-1.0268366 -0.73873806 -0.94252276 -1.1950197 -1.3187554 -1.3368299 -1.2902751 -1.2789092 -1.2624645 -1.1700208 -0.82024693 -0.36332226 -0.2426405 -0.41127396 -1.0031784][-1.3139586 -1.1331422 -1.2911947 -1.3565657 -1.4412673 -1.3082361 -1.0209913 -1.0451777 -1.1452029 -1.0037379 -0.62696695 -0.091528893 0.23172235 0.15181732 -0.53062725][-2.0409925 -1.7534277 -1.8396389 -1.8707945 -1.8258328 -1.6520989 -1.2830982 -1.2078638 -1.062567 -0.84328985 -0.29643297 0.37338352 0.72132683 0.5345788 -0.089797974]]...]
INFO - root - 2017-12-16 08:15:12.215937: step 57710, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.282 sec/batch; 21h:33m:44s remains)
INFO - root - 2017-12-16 08:15:15.014171: step 57720, loss = 0.25, batch loss = 0.19 (29.3 examples/sec; 0.273 sec/batch; 20h:50m:45s remains)
INFO - root - 2017-12-16 08:15:17.837992: step 57730, loss = 0.38, batch loss = 0.32 (27.6 examples/sec; 0.290 sec/batch; 22h:09m:18s remains)
INFO - root - 2017-12-16 08:15:20.691818: step 57740, loss = 0.32, batch loss = 0.26 (28.4 examples/sec; 0.281 sec/batch; 21h:28m:18s remains)
INFO - root - 2017-12-16 08:15:23.510761: step 57750, loss = 0.30, batch loss = 0.24 (29.0 examples/sec; 0.276 sec/batch; 21h:03m:56s remains)
INFO - root - 2017-12-16 08:15:26.361258: step 57760, loss = 0.31, batch loss = 0.26 (27.0 examples/sec; 0.296 sec/batch; 22h:34m:35s remains)
INFO - root - 2017-12-16 08:15:29.263718: step 57770, loss = 0.33, batch loss = 0.27 (26.2 examples/sec; 0.305 sec/batch; 23h:18m:27s remains)
INFO - root - 2017-12-16 08:15:32.078809: step 57780, loss = 0.31, batch loss = 0.26 (29.1 examples/sec; 0.275 sec/batch; 20h:58m:24s remains)
INFO - root - 2017-12-16 08:15:34.915658: step 57790, loss = 0.22, batch loss = 0.16 (29.1 examples/sec; 0.275 sec/batch; 21h:00m:16s remains)
INFO - root - 2017-12-16 08:15:37.770036: step 57800, loss = 0.22, batch loss = 0.17 (29.4 examples/sec; 0.272 sec/batch; 20h:46m:29s remains)
2017-12-16 08:15:38.357284: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0301335 -3.130744 -3.1802378 -2.9962258 -2.8760891 -2.9604 -3.0961542 -3.3790021 -3.7903581 -3.8478272 -3.7703791 -3.7829504 -3.6568413 -3.3581531 -3.0584025][-2.6630814 -2.6839273 -2.7593927 -2.8266869 -2.773808 -2.8628089 -3.170471 -3.3474123 -3.4458992 -3.7223823 -3.984237 -3.8113816 -3.5695786 -3.360971 -3.1667395][-1.4736087 -1.8635027 -2.2140794 -2.4449935 -2.643471 -2.8770211 -3.0111594 -3.3189421 -3.6525354 -3.8202155 -3.9741395 -4.0719032 -3.895947 -3.5144079 -3.1657383][-0.2615633 -0.81992292 -1.5256999 -2.0568202 -2.2581069 -2.3647971 -2.4558282 -2.4955626 -2.7020607 -3.2546427 -3.7122059 -3.9176145 -3.9427302 -3.7269077 -3.3897915][0.62038374 -0.17260218 -0.94744849 -1.7163134 -2.1097536 -2.0113237 -1.7136033 -1.697031 -2.0030081 -2.480619 -3.1071072 -3.551563 -3.6527781 -3.6382935 -3.4164791][1.4304776 0.41099596 -0.67180967 -1.3148654 -1.2898159 -1.1300242 -0.92848682 -0.80963683 -1.1432991 -1.8315229 -2.6127248 -3.237524 -3.5116444 -3.5733361 -3.5060463][0.90759087 0.12282515 -0.46264577 -0.74027658 -0.55912185 -0.01057148 0.46040249 0.44964933 -0.1009593 -1.1308815 -2.15413 -2.8404317 -3.2926018 -3.3885708 -3.2860432][0.051623821 -0.87474728 -1.2779882 -0.89933848 -0.086325645 0.74790907 1.2855463 1.2211728 0.65706873 -0.40571785 -1.6307003 -2.5200839 -2.9782479 -3.1820998 -3.2442768][-2.1902945 -2.5125751 -2.2878866 -1.7742217 -0.69400692 0.39639521 1.1001201 1.2380886 0.62767744 -0.37339211 -1.3227971 -2.1282723 -2.5939198 -2.6618848 -2.6481385][-3.6432295 -4.1968374 -3.8738554 -2.9018629 -1.633944 -0.47810912 0.2507782 0.28082418 -0.048817635 -0.8225801 -1.714777 -2.1477718 -2.1934092 -2.0774286 -1.9453924][-5.0361042 -5.2561221 -4.9602728 -4.16774 -3.0234714 -1.986367 -1.1357696 -0.84353352 -1.0712802 -1.6105564 -2.153492 -2.3537703 -2.1727679 -1.8942947 -1.6794167][-5.6620326 -6.19088 -6.1690016 -5.8152761 -5.2911587 -4.3817697 -3.2647865 -2.5673752 -2.1934495 -2.2098987 -2.266804 -2.0914528 -1.8146989 -1.479609 -1.43185][-5.6166739 -6.3742304 -7.0273371 -7.2257595 -6.9223576 -6.2276068 -5.1478915 -3.904479 -2.7781432 -2.1916904 -1.993937 -1.653738 -1.3471708 -1.2333317 -1.2145939][-4.8770833 -5.7695513 -6.59455 -7.2446156 -7.680922 -7.1724129 -5.7945127 -4.31863 -2.9874005 -1.9642859 -1.4632423 -1.0278332 -0.60996842 -0.59394217 -0.95197177][-3.8214464 -4.8635902 -6.1873527 -7.1468949 -7.5385113 -7.4689722 -6.43853 -4.7827 -3.2078636 -1.9986277 -1.245991 -0.63392544 -0.41297626 -0.4313314 -0.63533568]]...]
INFO - root - 2017-12-16 08:15:41.179929: step 57810, loss = 0.28, batch loss = 0.23 (28.1 examples/sec; 0.285 sec/batch; 21h:45m:29s remains)
INFO - root - 2017-12-16 08:15:44.016010: step 57820, loss = 0.48, batch loss = 0.42 (28.9 examples/sec; 0.276 sec/batch; 21h:05m:24s remains)
INFO - root - 2017-12-16 08:15:46.872965: step 57830, loss = 0.24, batch loss = 0.18 (27.7 examples/sec; 0.289 sec/batch; 22h:03m:44s remains)
INFO - root - 2017-12-16 08:15:49.820236: step 57840, loss = 0.24, batch loss = 0.18 (27.5 examples/sec; 0.291 sec/batch; 22h:12m:11s remains)
INFO - root - 2017-12-16 08:15:52.626504: step 57850, loss = 0.23, batch loss = 0.17 (27.8 examples/sec; 0.288 sec/batch; 21h:56m:07s remains)
INFO - root - 2017-12-16 08:15:55.440680: step 57860, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.285 sec/batch; 21h:44m:55s remains)
INFO - root - 2017-12-16 08:15:58.292625: step 57870, loss = 0.23, batch loss = 0.17 (28.8 examples/sec; 0.277 sec/batch; 21h:10m:09s remains)
INFO - root - 2017-12-16 08:16:01.186092: step 57880, loss = 0.29, batch loss = 0.23 (28.7 examples/sec; 0.279 sec/batch; 21h:17m:27s remains)
INFO - root - 2017-12-16 08:16:04.074937: step 57890, loss = 0.27, batch loss = 0.21 (27.3 examples/sec; 0.293 sec/batch; 22h:23m:00s remains)
INFO - root - 2017-12-16 08:16:06.890748: step 57900, loss = 0.23, batch loss = 0.17 (28.6 examples/sec; 0.279 sec/batch; 21h:17m:58s remains)
2017-12-16 08:16:07.416541: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3677826 -3.6288903 -3.809113 -4.0097942 -4.0171995 -3.9972296 -3.9031177 -3.8305366 -3.6278672 -3.2389669 -3.0673957 -3.0333338 -3.0495186 -2.9546804 -3.0045393][-3.206748 -3.4743636 -3.6035721 -3.7507646 -3.8444526 -3.7535546 -3.5478857 -3.3185091 -3.1318557 -2.8943162 -2.8100023 -2.7230911 -2.6130452 -2.5277679 -2.5028052][-2.9556007 -3.1287317 -3.179791 -3.0485058 -2.8997979 -2.8416829 -2.8257003 -2.7560787 -2.5163636 -2.4163852 -2.4013238 -2.3787303 -2.4543595 -2.2767165 -2.0331287][-2.3052177 -2.6112804 -2.6740854 -2.2875571 -1.7781272 -1.4501188 -1.289185 -1.3642962 -1.4760134 -1.4937067 -1.6198268 -1.80143 -2.05731 -2.0832419 -2.0351756][-1.8336575 -1.6923854 -1.4330065 -1.0668249 -0.38260651 0.32295227 0.66046906 0.67615223 0.38595581 -0.12003374 -0.65660739 -1.1547761 -1.810843 -2.382236 -2.5054445][-2.408082 -1.8513441 -1.0268579 -0.036144257 1.122076 1.9717331 2.6361232 2.8887491 2.3845754 1.4285212 0.42635059 -0.87964416 -2.1978111 -3.0334966 -3.201915][-2.8708289 -2.1783173 -1.336679 0.20147753 1.797308 3.09583 4.1905909 4.4552765 3.8541269 2.6725926 1.1887822 -0.46217608 -2.1463671 -3.3679271 -3.7851243][-3.6991651 -2.9556565 -1.979048 -0.45700955 1.1658216 2.9069071 4.3556366 4.7986917 4.2218122 2.815949 1.103447 -0.7211194 -2.5391107 -3.8940594 -4.3905716][-5.2501163 -4.57426 -3.4822402 -2.091948 -0.48408604 1.2625117 2.8341856 3.5543704 3.2077122 1.9247332 0.2352953 -1.7886341 -3.6670861 -4.900054 -5.2610321][-6.6450453 -6.4423857 -5.6191568 -4.2059283 -2.4569221 -0.78744292 0.65339231 1.3229237 1.2469258 0.21714497 -1.2531118 -3.0094078 -4.6482167 -5.624938 -5.806262][-7.2472372 -7.1259623 -6.5669708 -5.5370917 -4.1650891 -2.7918663 -1.5255907 -1.1074464 -1.1709926 -1.878293 -2.9634554 -4.2567749 -5.3943019 -5.8698244 -5.7928152][-7.303813 -7.2270346 -6.905633 -6.1318545 -5.1783257 -4.2449918 -3.3258436 -2.984102 -2.8221653 -3.1684389 -3.9862638 -4.8735876 -5.5606251 -5.8140306 -5.5794253][-6.5944977 -6.6903009 -6.5192127 -5.9083719 -5.2857046 -4.621357 -3.9657023 -3.689712 -3.5323067 -3.7393909 -4.2103662 -4.7724342 -5.2513895 -5.39427 -5.0794506][-5.6410666 -5.4664474 -5.2219839 -4.9188738 -4.5973506 -4.0334377 -3.5617101 -3.4783018 -3.4529409 -3.4583211 -3.6366997 -3.9553411 -4.2412581 -4.218647 -3.9114623][-4.2147307 -3.9429462 -3.6907241 -3.5147183 -3.4048896 -3.1314626 -2.8974607 -2.7683673 -2.6641154 -2.6694527 -2.7421036 -2.7663074 -2.8168588 -2.8996382 -2.7784905]]...]
INFO - root - 2017-12-16 08:16:10.286554: step 57910, loss = 0.20, batch loss = 0.14 (29.1 examples/sec; 0.275 sec/batch; 20h:59m:54s remains)
INFO - root - 2017-12-16 08:16:13.136977: step 57920, loss = 0.27, batch loss = 0.22 (27.4 examples/sec; 0.292 sec/batch; 22h:14m:23s remains)
INFO - root - 2017-12-16 08:16:15.961195: step 57930, loss = 0.24, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 20h:54m:13s remains)
INFO - root - 2017-12-16 08:16:18.762446: step 57940, loss = 0.30, batch loss = 0.24 (27.8 examples/sec; 0.288 sec/batch; 21h:58m:16s remains)
INFO - root - 2017-12-16 08:16:21.675059: step 57950, loss = 0.28, batch loss = 0.22 (28.6 examples/sec; 0.280 sec/batch; 21h:21m:28s remains)
INFO - root - 2017-12-16 08:16:24.540045: step 57960, loss = 0.21, batch loss = 0.16 (28.3 examples/sec; 0.282 sec/batch; 21h:31m:13s remains)
INFO - root - 2017-12-16 08:16:27.402023: step 57970, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 21h:46m:51s remains)
INFO - root - 2017-12-16 08:16:30.232592: step 57980, loss = 0.20, batch loss = 0.14 (27.8 examples/sec; 0.288 sec/batch; 21h:58m:15s remains)
INFO - root - 2017-12-16 08:16:33.064765: step 57990, loss = 0.30, batch loss = 0.24 (27.7 examples/sec; 0.288 sec/batch; 21h:59m:06s remains)
INFO - root - 2017-12-16 08:16:35.925149: step 58000, loss = 0.27, batch loss = 0.21 (28.3 examples/sec; 0.283 sec/batch; 21h:35m:07s remains)
2017-12-16 08:16:36.451012: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1449046 -6.0031481 -5.9741416 -6.3156919 -6.5083809 -6.3626404 -6.0667772 -5.7278395 -5.4156146 -5.1749015 -4.9682465 -4.8376055 -4.6927366 -4.4662023 -4.31686][-5.4233475 -5.2884974 -5.21393 -5.4741921 -5.7123508 -5.7872572 -5.7329068 -5.3823795 -5.13046 -5.0144372 -4.8875074 -4.7977676 -4.6328454 -4.3362088 -4.0868092][-3.6947575 -3.5751998 -3.5352492 -3.7754736 -4.1905932 -4.335638 -4.3900251 -4.3303633 -4.3336711 -4.3746147 -4.5102057 -4.4903474 -4.283721 -3.9494262 -3.6318049][-1.635462 -1.322216 -1.3029089 -1.7083354 -2.2595618 -2.6168842 -2.8992252 -2.8635826 -2.8701432 -3.0714884 -3.4017208 -3.661972 -3.7071378 -3.4517431 -3.141382][0.26636982 0.74133921 0.62542868 0.11304808 -0.52279949 -0.94822431 -1.1941364 -1.1372414 -1.1315506 -1.3868957 -1.8058105 -2.2482395 -2.5407617 -2.7413154 -2.6971948][1.0842781 1.7472901 1.6788845 1.0555153 0.4952693 0.18759632 0.1748395 0.58560276 0.83316422 0.60187483 0.089119911 -0.730917 -1.4720664 -1.9713924 -2.1274407][1.3196893 1.8489051 1.6323714 1.1639423 0.8966589 0.77429628 1.1987839 1.9359365 2.4558415 2.3937078 1.8052349 0.7957983 -0.36167431 -1.4110334 -1.9753854][0.96256161 1.1636753 0.79553556 0.35895872 0.30057621 0.62692738 1.6301455 2.7610922 3.50883 3.5217891 2.8498535 1.5320964 0.054683208 -1.389993 -2.2777946][-0.24081278 -0.2590313 -0.59843373 -1.0037584 -0.90449238 -0.16978836 1.191247 2.4504967 3.3648152 3.4409227 2.7426171 1.2204833 -0.51810169 -2.1688337 -3.2596991][-1.4778178 -1.8458679 -2.3124533 -2.6144807 -2.402808 -1.3659909 0.092549324 1.45328 2.3973989 2.3380055 1.5999513 0.063201904 -1.7952311 -3.5106788 -4.586482][-2.5752087 -2.9836507 -3.3655813 -3.6771894 -3.6855664 -2.6684623 -1.2108474 0.0042510033 0.76003981 0.62982512 -0.17099142 -1.6563225 -3.3016558 -4.9206018 -5.9535093][-3.5291834 -3.8802364 -4.3567724 -4.4796405 -4.335145 -3.4737253 -2.4509039 -1.4979758 -0.938081 -1.153785 -1.7868223 -3.2491033 -4.6391592 -5.8401289 -6.6280503][-3.9491963 -4.0863562 -4.6433449 -4.8496666 -4.78826 -4.1324682 -3.4839375 -2.8117995 -2.3241704 -2.5401464 -3.2029197 -4.3313837 -5.3314943 -6.2394581 -6.6898155][-3.9426484 -3.66039 -3.9076331 -4.1669326 -4.2435107 -3.85607 -3.5920355 -3.3868761 -3.32402 -3.4087853 -3.7203104 -4.6880789 -5.4619217 -5.9953122 -6.1248131][-3.8349683 -3.3017592 -3.2299147 -3.3813756 -3.5154898 -3.42845 -3.4247327 -3.3642216 -3.389904 -3.555172 -3.8341904 -4.4377546 -4.8697195 -5.2465644 -5.09271]]...]
INFO - root - 2017-12-16 08:16:39.292722: step 58010, loss = 0.24, batch loss = 0.18 (29.5 examples/sec; 0.271 sec/batch; 20h:42m:00s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 08:16:42.152770: step 58020, loss = 0.23, batch loss = 0.17 (27.7 examples/sec; 0.289 sec/batch; 21h:59m:58s remains)
INFO - root - 2017-12-16 08:16:45.013099: step 58030, loss = 0.24, batch loss = 0.19 (29.1 examples/sec; 0.275 sec/batch; 20h:56m:25s remains)
INFO - root - 2017-12-16 08:16:47.864377: step 58040, loss = 0.23, batch loss = 0.18 (27.3 examples/sec; 0.293 sec/batch; 22h:22m:13s remains)
INFO - root - 2017-12-16 08:16:50.732341: step 58050, loss = 0.28, batch loss = 0.22 (27.5 examples/sec; 0.291 sec/batch; 22h:11m:34s remains)
INFO - root - 2017-12-16 08:16:53.523315: step 58060, loss = 0.24, batch loss = 0.18 (29.5 examples/sec; 0.271 sec/batch; 20h:39m:32s remains)
INFO - root - 2017-12-16 08:16:56.373438: step 58070, loss = 0.19, batch loss = 0.14 (28.5 examples/sec; 0.281 sec/batch; 21h:24m:27s remains)
INFO - root - 2017-12-16 08:16:59.214854: step 58080, loss = 0.47, batch loss = 0.41 (27.3 examples/sec; 0.293 sec/batch; 22h:18m:39s remains)
INFO - root - 2017-12-16 08:17:02.012824: step 58090, loss = 0.25, batch loss = 0.19 (27.0 examples/sec; 0.296 sec/batch; 22h:33m:39s remains)
INFO - root - 2017-12-16 08:17:04.849187: step 58100, loss = 0.24, batch loss = 0.19 (27.9 examples/sec; 0.286 sec/batch; 21h:49m:03s remains)
2017-12-16 08:17:05.418008: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2292564 -3.4097102 -3.7259269 -3.7885063 -3.71302 -3.4843578 -3.2994337 -3.2033548 -2.9987042 -2.8584166 -2.7300439 -2.6330595 -2.561532 -2.5064592 -2.5049739][-2.8038335 -3.1511829 -3.6095977 -3.8096962 -3.8090618 -3.5226529 -3.2782717 -3.1263139 -2.9392161 -2.9329071 -2.8405747 -2.7553825 -2.6813316 -2.593101 -2.5775833][-2.2188673 -2.6885328 -3.4920988 -3.8644524 -3.8208332 -3.5051515 -3.143167 -2.9184494 -2.7836719 -2.8474941 -2.9998233 -3.0242088 -2.9706368 -2.9841635 -3.0024936][-1.7221141 -2.3156583 -3.1746349 -3.710211 -3.6538644 -3.1734288 -2.6292467 -2.2556703 -2.2284222 -2.5375314 -2.8398936 -3.1223571 -3.2562194 -3.2606463 -3.3697848][-1.5413859 -2.1601102 -2.9975252 -3.3611159 -3.0640817 -2.3266194 -1.5306907 -1.2365637 -1.2928107 -1.8736241 -2.5116553 -2.876092 -3.1336637 -3.3002508 -3.4177182][-1.6752899 -2.0628283 -2.7374907 -2.6781888 -1.8948276 -0.91826725 0.059636116 0.35020876 0.095499039 -0.72714353 -1.568753 -2.1831968 -2.5133762 -2.7971148 -3.2061567][-1.6222467 -1.9581394 -2.2917008 -1.8837876 -0.81695127 0.41075611 1.4772601 1.8342218 1.469574 0.49785757 -0.4654851 -1.2914827 -1.8779888 -2.2682641 -2.5103257][-1.5767274 -1.8727493 -2.0919802 -1.4622109 -0.42671204 0.94712114 1.9656906 2.2740431 2.0169473 0.97668362 0.0023107529 -0.699424 -1.1424196 -1.4267132 -1.4377246][-1.6270065 -1.7913976 -1.9719281 -1.3369651 -0.40302896 0.60446453 1.5422783 1.8174887 1.5155377 0.78809023 -0.05826664 -0.55801797 -0.810853 -0.79711986 -0.2764616][-1.4532681 -1.5964725 -1.9514215 -1.4755404 -0.848233 -0.28035593 0.16674805 0.33311796 -0.0045003891 -0.46435785 -0.89090586 -1.0924177 -0.78927016 -0.29939365 0.25592566][-1.586206 -1.697562 -1.9772103 -1.702343 -1.4848087 -1.258801 -1.1892455 -1.3893747 -1.8243394 -2.3424368 -2.5610938 -1.9389241 -1.0046816 -0.16537762 0.73242092][-1.923223 -2.0168045 -2.310708 -2.2988131 -2.3715775 -2.5287056 -3.0209761 -3.4060445 -3.9228816 -4.2405634 -4.0537524 -3.0311096 -1.571321 -0.39383936 0.63221455][-2.4788177 -2.4415534 -2.6377814 -2.8338938 -3.1208105 -3.7493579 -4.6172338 -5.4132681 -6.1507092 -6.2094684 -5.4368281 -4.0492039 -2.4597335 -1.0847702 -0.19758606][-3.0451589 -2.820951 -2.8496079 -3.0818691 -3.518569 -4.4323225 -5.6105261 -6.5585728 -7.21231 -7.2119446 -6.5604115 -5.2239313 -3.6313038 -2.3966684 -1.738234][-3.7809849 -3.3317294 -3.1745467 -3.396246 -4.0390806 -5.0870514 -6.4656892 -7.3916082 -7.8083172 -7.65973 -6.9203587 -5.6527066 -4.591682 -3.7042155 -3.0545902]]...]
INFO - root - 2017-12-16 08:17:08.277728: step 58110, loss = 0.23, batch loss = 0.18 (28.7 examples/sec; 0.279 sec/batch; 21h:14m:56s remains)
INFO - root - 2017-12-16 08:17:11.103906: step 58120, loss = 0.46, batch loss = 0.40 (28.7 examples/sec; 0.279 sec/batch; 21h:16m:48s remains)
INFO - root - 2017-12-16 08:17:13.977522: step 58130, loss = 0.29, batch loss = 0.23 (27.7 examples/sec; 0.289 sec/batch; 22h:02m:37s remains)
INFO - root - 2017-12-16 08:17:16.807160: step 58140, loss = 0.28, batch loss = 0.22 (29.1 examples/sec; 0.275 sec/batch; 20h:58m:43s remains)
INFO - root - 2017-12-16 08:17:19.628432: step 58150, loss = 0.24, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 20h:53m:29s remains)
INFO - root - 2017-12-16 08:17:22.423320: step 58160, loss = 0.24, batch loss = 0.18 (28.7 examples/sec; 0.279 sec/batch; 21h:15m:48s remains)
INFO - root - 2017-12-16 08:17:25.249755: step 58170, loss = 0.23, batch loss = 0.17 (29.1 examples/sec; 0.275 sec/batch; 20h:56m:07s remains)
INFO - root - 2017-12-16 08:17:28.109412: step 58180, loss = 0.23, batch loss = 0.17 (28.9 examples/sec; 0.277 sec/batch; 21h:07m:03s remains)
INFO - root - 2017-12-16 08:17:30.913787: step 58190, loss = 0.21, batch loss = 0.15 (27.7 examples/sec; 0.289 sec/batch; 21h:59m:46s remains)
INFO - root - 2017-12-16 08:17:33.748882: step 58200, loss = 0.27, batch loss = 0.22 (27.8 examples/sec; 0.288 sec/batch; 21h:57m:31s remains)
2017-12-16 08:17:34.256463: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.351696 -5.8724465 -5.96383 -5.9436178 -5.76604 -5.2446895 -4.9285545 -4.6041813 -4.4035749 -4.1756864 -3.9275141 -3.7028322 -3.5915985 -3.6635065 -3.8293977][-4.5507531 -5.1331558 -5.3857841 -5.3359318 -4.9665265 -4.3531895 -3.6985841 -3.365202 -3.2500589 -2.9265232 -2.7925181 -2.8370135 -2.9568055 -3.0526929 -3.2327843][-3.5679648 -4.3289361 -4.7101092 -4.5561132 -4.1337247 -3.4157119 -2.7835116 -2.1843412 -2.0142233 -1.9902639 -2.0230374 -2.10916 -2.2903709 -2.5441096 -2.7737546][-2.9409022 -3.7666998 -4.3447695 -4.3750534 -3.8615303 -3.1309748 -2.2828186 -1.5403776 -1.3652174 -1.401829 -1.6453431 -1.9211202 -2.2269819 -2.4945374 -2.6631687][-2.6643586 -3.5399041 -4.3295774 -4.38901 -3.8404133 -2.9089949 -1.8180642 -1.0437264 -0.76900458 -0.820843 -1.3734565 -1.9345 -2.3882217 -2.650754 -2.8529685][-2.5602317 -3.1799917 -3.7285922 -3.8089008 -3.1256936 -2.3052347 -1.2547326 -0.38364172 -0.15022087 -0.40073681 -1.0359621 -1.6723173 -2.2848032 -2.7514489 -3.0490868][-2.30559 -2.7466755 -3.2787294 -3.3116548 -2.7733254 -1.9193406 -0.96356177 -0.24122524 0.043884754 -0.18957233 -0.78942347 -1.5013776 -2.1889102 -2.6874223 -3.0196738][-2.0456352 -2.2370903 -2.615355 -2.747076 -2.4573307 -1.8460519 -1.0926852 -0.47026157 -0.13778067 -0.28787994 -0.76456165 -1.4185188 -2.0542855 -2.5840876 -3.0104613][-1.5787683 -1.6070054 -1.7414198 -1.8034739 -1.7338049 -1.4180019 -0.98251152 -0.51299596 -0.25287247 -0.3226676 -0.63401055 -1.2229505 -1.8276017 -2.3583472 -2.9032121][-1.3314812 -1.025167 -1.0247133 -1.1027124 -1.1530483 -1.1789048 -1.0916252 -0.83928633 -0.56261587 -0.53248549 -0.65922713 -0.992888 -1.5042481 -2.0662825 -2.7063208][-1.5155046 -1.1850412 -0.97998643 -0.96646857 -1.1765289 -1.3182693 -1.4983256 -1.4409802 -1.238348 -1.1373491 -1.0784779 -1.2662978 -1.6649601 -2.1104629 -2.751987][-2.3200369 -1.8773856 -1.6528025 -1.5631521 -1.684649 -2.042155 -2.2974262 -2.388552 -2.3156629 -2.192853 -2.0674367 -2.043745 -2.2161424 -2.4812679 -2.9988489][-3.3633437 -2.8775928 -2.5567479 -2.3184319 -2.4615273 -2.7965631 -3.1116476 -3.3072937 -3.3149495 -3.1144776 -2.9318357 -2.795733 -2.7570028 -2.7880607 -3.1552808][-4.4494238 -4.1310835 -3.8828528 -3.5678713 -3.4765456 -3.6811595 -3.8995807 -4.0527339 -4.060246 -3.8673553 -3.7066441 -3.5405927 -3.406949 -3.3606012 -3.5958023][-5.8262138 -5.6574874 -5.4507 -5.07299 -4.8224535 -4.81174 -4.8293 -4.8788362 -4.914115 -4.8090148 -4.6697955 -4.4722219 -4.3736949 -4.3023982 -4.2719092]]...]
INFO - root - 2017-12-16 08:17:37.043044: step 58210, loss = 0.21, batch loss = 0.15 (29.1 examples/sec; 0.275 sec/batch; 20h:57m:17s remains)
INFO - root - 2017-12-16 08:17:39.865580: step 58220, loss = 0.48, batch loss = 0.42 (29.8 examples/sec; 0.268 sec/batch; 20h:26m:29s remains)
INFO - root - 2017-12-16 08:17:42.695661: step 58230, loss = 0.29, batch loss = 0.23 (29.1 examples/sec; 0.275 sec/batch; 20h:58m:11s remains)
INFO - root - 2017-12-16 08:17:45.565977: step 58240, loss = 0.26, batch loss = 0.20 (26.7 examples/sec; 0.300 sec/batch; 22h:49m:41s remains)
INFO - root - 2017-12-16 08:17:48.449310: step 58250, loss = 0.37, batch loss = 0.31 (26.6 examples/sec; 0.301 sec/batch; 22h:56m:11s remains)
INFO - root - 2017-12-16 08:17:51.331738: step 58260, loss = 0.25, batch loss = 0.19 (29.6 examples/sec; 0.270 sec/batch; 20h:35m:13s remains)
INFO - root - 2017-12-16 08:17:54.192637: step 58270, loss = 0.22, batch loss = 0.16 (27.8 examples/sec; 0.288 sec/batch; 21h:56m:23s remains)
INFO - root - 2017-12-16 08:17:57.074827: step 58280, loss = 0.28, batch loss = 0.22 (28.3 examples/sec; 0.283 sec/batch; 21h:33m:01s remains)
INFO - root - 2017-12-16 08:17:59.888626: step 58290, loss = 0.27, batch loss = 0.21 (28.6 examples/sec; 0.280 sec/batch; 21h:17m:30s remains)
INFO - root - 2017-12-16 08:18:02.721131: step 58300, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 21h:29m:30s remains)
2017-12-16 08:18:03.264880: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1378667 -2.153954 -2.0850735 -1.9887807 -2.1313338 -2.386672 -2.6068363 -2.8403668 -3.03715 -3.0916698 -2.9699254 -2.65117 -2.3926325 -1.8643684 -1.3896966][-2.2279537 -2.1302788 -2.1166556 -1.9665563 -1.8805296 -1.9799833 -2.1107278 -2.2713923 -2.5768933 -2.7178373 -2.7185087 -2.5362921 -2.4688883 -2.0755339 -1.4885378][-3.3855405 -3.1171517 -2.8252547 -2.4829431 -2.21464 -2.1454544 -2.1761057 -2.1205738 -2.1771247 -2.4208465 -2.9082012 -3.0413167 -2.860497 -2.6066482 -2.2390072][-4.7887721 -4.6771049 -4.5020547 -4.2066512 -3.6451652 -2.9099755 -2.2345335 -1.9856448 -2.2481976 -2.4831738 -2.7166204 -3.1538506 -3.6770229 -3.7246609 -3.1236711][-6.2846971 -6.371675 -6.1790428 -5.6354604 -4.7318015 -3.5639832 -2.4236887 -1.6610935 -1.6563675 -2.278779 -3.1169302 -3.7670958 -4.1512918 -4.4107981 -4.3677325][-6.6554337 -6.8980217 -6.7578683 -6.1600862 -4.9197555 -3.1482606 -1.5979128 -0.62962723 -0.48260975 -1.1437407 -2.1882033 -3.3852091 -4.3998947 -5.0161867 -5.0363331][-5.7881918 -6.1395159 -6.2151985 -5.666111 -4.353548 -2.4543047 -0.51185083 0.719996 0.85724497 0.14406729 -1.0427659 -2.3516834 -3.3816576 -4.082613 -4.4123974][-4.5830879 -4.9557562 -4.7542596 -4.3599143 -3.2853732 -1.4423552 0.44500875 1.7922239 2.0315509 1.2701726 -0.0049052238 -1.3420267 -2.3593371 -3.1587198 -3.4850078][-2.83741 -2.9679379 -3.2026985 -2.9821892 -2.21761 -0.76681161 0.78800583 2.1193748 2.4352889 1.6774735 0.49386597 -0.68606949 -1.4465399 -2.018636 -2.3724909][-1.3522513 -1.4499183 -1.809442 -2.2323239 -2.2106307 -1.2500134 -0.011896133 1.0316048 1.4172015 0.90624237 -0.1454215 -1.0534821 -1.2465456 -1.1691337 -0.97464895][-0.070709229 -0.23927259 -1.0602839 -1.9238811 -2.3187933 -2.0056136 -1.0688963 -0.14112616 -0.093477726 -0.60415244 -1.1311803 -1.4420223 -1.2361512 -0.81401682 -0.43060374][0.087357044 0.13303185 -0.69234872 -1.983865 -2.6912134 -2.5871906 -2.4534092 -2.0505383 -1.7426057 -1.8684728 -2.2643578 -2.2768259 -1.6850765 -1.1596808 -0.773999][-0.017969131 0.24517965 -0.58129859 -1.9124999 -3.117471 -3.4457195 -3.1067343 -2.8745241 -3.2408972 -3.5382233 -3.3450232 -3.0255191 -2.4522111 -1.9708886 -1.8033278][-0.40109444 0.026216507 -0.60860515 -1.7433913 -2.8022103 -3.4209003 -3.6417162 -3.4466479 -3.4525752 -3.7708037 -3.7897406 -3.5132477 -3.0603147 -2.7740469 -2.7008781][-0.49106598 -0.079314709 -0.47981691 -1.4868228 -2.4738345 -3.1554022 -3.3298769 -3.3198786 -3.5673163 -3.6633821 -3.5898831 -3.445868 -3.1538 -3.1256628 -3.3843455]]...]
INFO - root - 2017-12-16 08:18:06.106515: step 58310, loss = 0.23, batch loss = 0.17 (27.7 examples/sec; 0.289 sec/batch; 21h:59m:58s remains)
INFO - root - 2017-12-16 08:18:08.945271: step 58320, loss = 0.42, batch loss = 0.36 (27.8 examples/sec; 0.288 sec/batch; 21h:55m:12s remains)
INFO - root - 2017-12-16 08:18:11.778855: step 58330, loss = 0.25, batch loss = 0.20 (28.0 examples/sec; 0.286 sec/batch; 21h:46m:00s remains)
INFO - root - 2017-12-16 08:18:14.619231: step 58340, loss = 0.22, batch loss = 0.16 (26.6 examples/sec; 0.301 sec/batch; 22h:53m:40s remains)
INFO - root - 2017-12-16 08:18:17.522019: step 58350, loss = 0.27, batch loss = 0.21 (26.4 examples/sec; 0.304 sec/batch; 23h:07m:02s remains)
INFO - root - 2017-12-16 08:18:20.379096: step 58360, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 21h:47m:31s remains)
INFO - root - 2017-12-16 08:18:23.229940: step 58370, loss = 0.31, batch loss = 0.25 (26.7 examples/sec; 0.299 sec/batch; 22h:47m:18s remains)
INFO - root - 2017-12-16 08:18:26.059763: step 58380, loss = 0.19, batch loss = 0.14 (28.1 examples/sec; 0.285 sec/batch; 21h:40m:19s remains)
INFO - root - 2017-12-16 08:18:28.907460: step 58390, loss = 0.22, batch loss = 0.17 (28.4 examples/sec; 0.281 sec/batch; 21h:25m:55s remains)
INFO - root - 2017-12-16 08:18:31.734500: step 58400, loss = 0.30, batch loss = 0.24 (28.0 examples/sec; 0.286 sec/batch; 21h:46m:47s remains)
2017-12-16 08:18:32.235282: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7663178 -3.017591 -3.1808131 -3.2705703 -3.4877176 -4.0783787 -4.6402211 -5.1683865 -5.9036717 -6.3287029 -6.4590225 -6.1939454 -5.6892715 -4.8443265 -4.0022082][-3.0697515 -2.9072747 -2.7753963 -2.7894089 -2.7209277 -3.0561531 -3.8529849 -4.36071 -4.9949818 -5.4150677 -5.7671852 -5.725492 -5.0952191 -4.2700939 -3.5478315][-3.1593447 -2.9424646 -2.5903113 -2.261168 -1.8896675 -1.7725143 -2.2012284 -3.0512547 -3.7691283 -4.3415904 -4.6525655 -4.5407505 -3.9605544 -3.0132606 -1.9632952][-3.5485954 -2.8242292 -2.1931286 -1.6740501 -1.0304129 -0.52785611 -0.678179 -1.5062938 -2.3774526 -2.9062829 -3.185225 -3.1261759 -2.6798949 -2.0985775 -1.4583523][-3.7746761 -3.0538437 -2.1407802 -1.1995075 -0.23147345 0.82741165 1.2950616 0.773623 -0.24667358 -1.0971904 -1.8696818 -1.9113097 -1.5216193 -1.2576149 -1.1194725][-4.3798223 -3.4100869 -2.0235744 -0.59419656 1.0561943 2.3649278 2.8766522 2.5761685 1.5668874 0.25391102 -0.7857604 -1.3249052 -1.2925148 -1.1762097 -1.2039251][-4.054142 -3.3085365 -2.0526483 -0.35952044 1.3893776 2.8306131 3.4553409 3.1216035 2.0094953 0.51506805 -0.75502062 -1.331902 -1.3532474 -1.1789453 -1.0952747][-4.00485 -3.006696 -1.6741033 0.014464378 1.7252002 2.8214707 3.019412 2.6215081 1.4895473 0.059080124 -1.0880563 -1.5678277 -1.6008685 -1.4271152 -1.2520602][-3.7340765 -2.949214 -1.5716329 -0.042950153 1.3514023 2.0690799 2.0925069 1.6114841 0.6022296 -0.55727172 -1.4613461 -1.8664443 -1.795491 -1.6910794 -1.4615319][-3.6073232 -3.0879726 -1.9687178 -0.72173643 0.37368822 0.966321 0.92428875 0.45427132 -0.25621986 -1.1244912 -1.6978493 -1.9562082 -1.9135265 -1.6647327 -1.3345068][-3.4250507 -3.1532755 -2.4695477 -1.7367878 -1.1119087 -0.85801291 -0.8235569 -0.95846605 -1.304848 -1.6684902 -1.9144361 -1.9030905 -1.7464924 -1.5500355 -1.2479837][-3.2803035 -3.3448267 -3.0159326 -2.680532 -2.3550282 -2.0494053 -1.8900173 -1.8098884 -1.7951076 -1.7881193 -1.8582878 -1.8276389 -1.7151101 -1.6207824 -1.3703954][-2.8453262 -2.9240737 -3.0049715 -2.9590263 -2.7440577 -2.5408247 -2.3392205 -2.176626 -1.9971502 -1.8440974 -1.6104562 -1.5783353 -1.5398161 -1.6445863 -1.7054992][-2.6346989 -2.8212581 -2.9820662 -3.1539083 -3.2585053 -3.18961 -2.9524174 -2.7790639 -2.4903798 -2.1632411 -1.8244197 -1.6737716 -1.6803806 -1.8682284 -2.126981][-2.5442002 -2.6170416 -2.8565388 -3.0145688 -3.1610355 -3.381701 -3.545459 -3.3792698 -3.0036614 -2.582129 -2.1500425 -1.8637507 -1.9409339 -2.1754353 -2.5721445]]...]
INFO - root - 2017-12-16 08:18:35.041410: step 58410, loss = 0.23, batch loss = 0.17 (27.5 examples/sec; 0.291 sec/batch; 22h:10m:13s remains)
INFO - root - 2017-12-16 08:18:37.864422: step 58420, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 21h:41m:33s remains)
INFO - root - 2017-12-16 08:18:40.705117: step 58430, loss = 0.41, batch loss = 0.35 (28.8 examples/sec; 0.277 sec/batch; 21h:06m:41s remains)
INFO - root - 2017-12-16 08:18:43.547947: step 58440, loss = 0.49, batch loss = 0.43 (29.0 examples/sec; 0.276 sec/batch; 21h:00m:48s remains)
INFO - root - 2017-12-16 08:18:46.454531: step 58450, loss = 0.27, batch loss = 0.21 (26.9 examples/sec; 0.298 sec/batch; 22h:39m:28s remains)
INFO - root - 2017-12-16 08:18:49.349647: step 58460, loss = 0.26, batch loss = 0.20 (27.8 examples/sec; 0.288 sec/batch; 21h:54m:43s remains)
INFO - root - 2017-12-16 08:18:52.227576: step 58470, loss = 0.21, batch loss = 0.15 (28.5 examples/sec; 0.281 sec/batch; 21h:21m:41s remains)
INFO - root - 2017-12-16 08:18:55.053048: step 58480, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 21h:41m:40s remains)
INFO - root - 2017-12-16 08:18:57.886600: step 58490, loss = 0.23, batch loss = 0.17 (27.7 examples/sec; 0.289 sec/batch; 22h:01m:02s remains)
INFO - root - 2017-12-16 08:19:00.728598: step 58500, loss = 0.23, batch loss = 0.17 (28.1 examples/sec; 0.285 sec/batch; 21h:39m:53s remains)
2017-12-16 08:19:01.218245: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3087773 -4.5461111 -4.4997659 -4.834177 -5.1834383 -5.5987768 -5.7289052 -5.8950529 -6.0004053 -5.7620497 -5.4040389 -5.1571689 -5.1487279 -4.7305136 -4.1813517][-4.8022304 -5.0197468 -4.8309507 -4.970768 -5.1626186 -5.3865747 -5.5553102 -5.8531675 -5.8794937 -5.6565318 -5.4370537 -5.2507563 -5.3095989 -4.8601904 -4.1946192][-5.653 -5.6916976 -5.2771559 -4.9445896 -4.68539 -4.6089358 -4.6870737 -4.967381 -5.1182032 -5.111557 -5.0372372 -4.9894114 -5.2129183 -4.82848 -4.0481534][-6.0029345 -5.8870029 -5.2205882 -4.459938 -3.7599046 -3.3158987 -3.21551 -3.4828715 -4.0524588 -4.3799129 -4.4700913 -4.5421443 -4.7205729 -4.312377 -3.5979638][-5.8693867 -5.43473 -4.3426375 -3.1180868 -1.9298913 -1.2561188 -1.1694033 -1.4788239 -2.3443964 -2.9965563 -3.6356616 -3.8206029 -3.9178958 -3.5543787 -2.9484482][-5.6566429 -4.8334694 -3.3947897 -1.6413317 0.1291132 1.0841346 1.2607355 0.71087933 -0.36851454 -1.3740876 -2.4231715 -2.9161973 -3.3406487 -3.1804547 -2.7252328][-5.2464452 -4.2971177 -2.718267 -0.81593871 0.9572897 2.1051631 2.5371337 1.9640956 0.70082617 -0.41319704 -1.4572687 -2.2835462 -2.9280014 -2.8837194 -2.4370573][-4.7146039 -3.7294395 -2.0544193 -0.22964716 1.3363357 2.4555287 2.8144631 2.2840533 1.0788937 -0.17551661 -1.450217 -2.3101463 -2.9053068 -3.1071615 -2.789227][-4.4525709 -3.61977 -2.210042 -0.55360126 1.0522346 2.1876254 2.5210233 2.1474695 1.0964556 -0.26732302 -1.854754 -3.0678248 -3.8798616 -4.0648637 -3.5336967][-4.9407167 -4.3433762 -3.3029974 -1.992038 -0.5926013 0.60095358 1.1652861 1.0308285 0.22374773 -0.9396503 -2.3370583 -3.6186233 -4.4932547 -4.7698536 -4.283823][-5.9080286 -5.4913464 -4.6318135 -3.5498595 -2.4379668 -1.3920374 -0.6141398 -0.52860928 -1.1739681 -2.2863367 -3.4530606 -4.4153905 -4.94888 -4.89223 -4.200139][-6.9222059 -6.8270054 -6.0506067 -5.0336523 -3.9574342 -3.1057954 -2.4984584 -2.2897751 -2.6800952 -3.5840178 -4.5889831 -5.1507745 -5.2385173 -5.0500922 -4.2858286][-7.60882 -7.5633507 -6.9199753 -5.903131 -4.8894043 -4.2284064 -3.8098543 -3.6296635 -3.9125242 -4.4922066 -5.0707688 -5.3792772 -5.3739371 -5.0723324 -4.3596773][-8.5613689 -8.3355846 -7.369668 -6.3136368 -5.5194774 -4.9620829 -4.6288848 -4.5302811 -4.7612243 -5.1156931 -5.2933536 -5.2078776 -5.0423164 -4.7305593 -4.1658549][-8.7703428 -8.4791346 -7.4878254 -6.2524624 -5.377924 -4.9553385 -4.7997093 -4.8381882 -5.0445266 -5.4164686 -5.5072064 -5.2766705 -5.0200915 -4.7328472 -4.2539988]]...]
INFO - root - 2017-12-16 08:19:04.094047: step 58510, loss = 0.25, batch loss = 0.19 (28.8 examples/sec; 0.278 sec/batch; 21h:10m:07s remains)
INFO - root - 2017-12-16 08:19:06.970759: step 58520, loss = 0.36, batch loss = 0.30 (27.3 examples/sec; 0.294 sec/batch; 22h:20m:33s remains)
INFO - root - 2017-12-16 08:19:09.832964: step 58530, loss = 0.32, batch loss = 0.27 (29.9 examples/sec; 0.267 sec/batch; 20h:20m:56s remains)
INFO - root - 2017-12-16 08:19:12.717003: step 58540, loss = 0.35, batch loss = 0.29 (26.6 examples/sec; 0.301 sec/batch; 22h:54m:39s remains)
INFO - root - 2017-12-16 08:19:15.553449: step 58550, loss = 0.24, batch loss = 0.18 (27.9 examples/sec; 0.287 sec/batch; 21h:50m:17s remains)
INFO - root - 2017-12-16 08:19:18.381076: step 58560, loss = 0.29, batch loss = 0.23 (28.7 examples/sec; 0.278 sec/batch; 21h:10m:29s remains)
INFO - root - 2017-12-16 08:19:21.222415: step 58570, loss = 0.22, batch loss = 0.16 (26.6 examples/sec; 0.301 sec/batch; 22h:55m:21s remains)
INFO - root - 2017-12-16 08:19:24.093026: step 58580, loss = 0.32, batch loss = 0.27 (27.8 examples/sec; 0.287 sec/batch; 21h:52m:16s remains)
INFO - root - 2017-12-16 08:19:26.975342: step 58590, loss = 0.27, batch loss = 0.21 (27.0 examples/sec; 0.297 sec/batch; 22h:33m:54s remains)
INFO - root - 2017-12-16 08:19:29.850461: step 58600, loss = 0.29, batch loss = 0.23 (27.8 examples/sec; 0.288 sec/batch; 21h:53m:10s remains)
2017-12-16 08:19:30.385431: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8591583 -3.643218 -3.320159 -2.8915024 -2.6297221 -2.6204209 -2.6943107 -3.083544 -3.3732948 -3.5501285 -3.7186472 -3.8696089 -4.2692976 -4.687809 -5.1911635][-3.2559998 -3.1102228 -2.7320995 -2.3767264 -2.047652 -1.8876209 -1.9073822 -2.3002141 -2.9126406 -3.3313642 -3.7386529 -4.0002589 -4.0919075 -4.2766271 -4.6866541][-2.1450765 -1.9787879 -1.5949113 -1.2647297 -0.934052 -0.85680676 -0.95018792 -1.4916906 -2.0843911 -2.8623214 -3.4794283 -3.8921094 -4.2161303 -4.1484017 -4.2686496][-1.3509641 -1.3628607 -1.2375276 -0.96625853 -0.57833314 -0.32731676 -0.17685175 -0.69326782 -1.4834728 -2.5999215 -3.2478156 -3.7536867 -3.9108984 -3.8683047 -4.1227994][-0.97953057 -0.94386005 -0.79295373 -0.47822928 0.00684309 0.41594696 0.76964045 0.4606905 -0.25128412 -1.2413893 -2.1034377 -2.7959633 -3.0227909 -3.0978222 -3.5460081][-0.84557986 -0.69264483 -0.45657825 -0.031377792 0.70522976 1.391036 1.7627549 1.6944709 1.0925117 0.12107801 -0.6291647 -1.2053709 -1.5691078 -2.1111276 -2.898958][-0.72155142 -0.42861605 -0.24680471 0.23820972 0.87879992 1.5299087 2.0450234 2.219214 2.0632548 1.3259735 0.67300081 0.15498114 -0.24083376 -1.0492086 -2.0066032][-0.67583323 -0.51995826 -0.46388602 -0.14639568 0.45054483 1.044044 1.6314597 2.1276608 2.4897056 2.3535242 2.1006904 1.4654498 0.92871189 -0.058144093 -1.4347441][-1.6245208 -1.4561472 -1.4460409 -1.394923 -1.0145833 -0.31961727 0.44945812 1.3077817 2.0717278 2.3868117 2.4992452 2.2447419 1.495749 0.44295645 -1.0593636][-2.5536652 -2.7520614 -2.9143829 -2.994771 -2.7067649 -2.2535355 -1.6447644 -0.49836683 0.55168819 1.3764262 1.8660326 1.8082256 1.3995438 0.47949314 -0.92034674][-3.2173755 -3.4477367 -3.8285241 -4.2807236 -4.5007334 -4.037539 -3.38733 -2.259696 -1.170049 0.065299511 0.93013239 0.86343336 0.41933537 -0.41588497 -1.7265739][-4.4640059 -4.8001847 -5.3406539 -5.7855215 -6.1184092 -6.10082 -5.7720518 -4.5880108 -2.990273 -1.639344 -0.81556416 -0.716177 -0.88614583 -1.8524611 -3.3384409][-5.7149463 -6.1450462 -6.9255528 -7.7106323 -8.4924212 -8.7930527 -8.6058388 -7.5959797 -5.90471 -4.1729541 -2.9899375 -2.6181102 -3.0523572 -3.8781743 -4.9449296][-6.3783007 -6.9297051 -7.9593019 -9.3268852 -10.680215 -11.262211 -10.906891 -9.8344078 -8.3727989 -6.7069359 -5.5101027 -5.1481762 -5.55988 -6.2537537 -7.09579][-6.5107508 -7.1688781 -8.5322828 -10.029408 -11.376238 -12.054607 -11.905539 -10.98093 -9.6386681 -8.1230354 -7.2024794 -7.1641474 -7.7069039 -8.1729832 -8.6610432]]...]
INFO - root - 2017-12-16 08:19:33.230395: step 58610, loss = 0.24, batch loss = 0.18 (27.7 examples/sec; 0.288 sec/batch; 21h:56m:55s remains)
INFO - root - 2017-12-16 08:19:36.050429: step 58620, loss = 0.24, batch loss = 0.18 (27.1 examples/sec; 0.295 sec/batch; 22h:26m:25s remains)
INFO - root - 2017-12-16 08:19:38.907423: step 58630, loss = 0.27, batch loss = 0.21 (28.8 examples/sec; 0.278 sec/batch; 21h:09m:36s remains)
INFO - root - 2017-12-16 08:19:41.763045: step 58640, loss = 0.33, batch loss = 0.27 (29.4 examples/sec; 0.272 sec/batch; 20h:41m:27s remains)
INFO - root - 2017-12-16 08:19:44.581433: step 58650, loss = 0.21, batch loss = 0.15 (28.5 examples/sec; 0.281 sec/batch; 21h:21m:08s remains)
INFO - root - 2017-12-16 08:19:47.480358: step 58660, loss = 0.25, batch loss = 0.19 (27.5 examples/sec; 0.291 sec/batch; 22h:07m:49s remains)
INFO - root - 2017-12-16 08:19:50.345097: step 58670, loss = 0.36, batch loss = 0.30 (28.6 examples/sec; 0.279 sec/batch; 21h:15m:27s remains)
INFO - root - 2017-12-16 08:19:53.149776: step 58680, loss = 0.24, batch loss = 0.18 (29.3 examples/sec; 0.273 sec/batch; 20h:44m:55s remains)
INFO - root - 2017-12-16 08:19:55.996122: step 58690, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.284 sec/batch; 21h:37m:21s remains)
INFO - root - 2017-12-16 08:19:58.904575: step 58700, loss = 0.26, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 21h:44m:37s remains)
2017-12-16 08:19:59.422182: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4040065 -4.6931415 -5.0856037 -5.8767939 -6.8165851 -7.399859 -8.0203838 -8.4787807 -8.7868614 -8.8429279 -8.5835819 -7.8716927 -6.7318506 -5.3659811 -4.1709914][-3.97986 -4.2893271 -4.8227844 -5.3472409 -6.197742 -7.3398924 -8.3573761 -8.7816744 -8.719347 -8.70837 -8.538187 -8.1535549 -7.1691771 -5.7734671 -4.5912037][-3.3481426 -3.7386668 -4.1707492 -4.8135014 -5.7077165 -6.3654795 -7.1149025 -7.8421755 -8.1557369 -8.0496368 -7.64594 -7.331953 -6.7223315 -5.6957474 -4.5982132][-2.5725658 -3.2588191 -3.9058645 -4.148469 -4.3939366 -4.8803349 -5.2927074 -5.6235433 -5.8687749 -6.2802992 -6.3809786 -6.4372749 -5.9475846 -5.1948051 -4.3249063][-2.4651613 -2.6833994 -3.1444592 -3.3474464 -3.3483334 -2.8307474 -2.2322707 -2.1025593 -2.4056995 -3.1664548 -3.9222991 -4.6630793 -4.7723575 -4.5308127 -3.8090003][-2.1919606 -2.2340598 -2.3361158 -2.1333144 -1.4658139 -0.44149613 0.92639589 1.7245965 1.5682325 0.36661005 -1.1263275 -2.5948081 -3.4204931 -3.5465689 -3.0449758][-1.9640181 -1.6466029 -1.3379006 -0.76957822 0.24576092 1.716784 3.4266238 4.2063627 3.9358244 2.4971495 0.87068462 -0.80030179 -1.9678602 -2.573684 -2.4161873][-2.3404276 -1.9417839 -1.2937908 -0.42085457 0.93823719 2.7529864 4.49866 5.1987572 4.6433849 3.0680342 1.3474455 -0.17135477 -1.1558542 -1.901397 -2.1854613][-2.796217 -2.5798068 -1.9838221 -1.22014 0.043065548 1.8156042 3.6515369 4.5597734 4.1985312 2.6065531 0.86920404 -0.5436554 -1.4247234 -2.0337098 -2.30043][-2.9948614 -3.3290472 -3.6748877 -3.0763235 -1.9028366 -0.66980886 0.69393253 1.7116818 1.9360676 1.1094666 -0.17802954 -1.2705598 -1.8971441 -2.5598569 -2.9786165][-4.8704686 -4.9859934 -5.0587082 -4.8735547 -4.3328924 -3.2892914 -2.1978824 -1.5410407 -1.0607276 -1.2653995 -1.9805892 -2.6558847 -3.0368142 -3.4144676 -3.7478249][-5.7112694 -6.2952948 -6.7679157 -6.3003798 -5.5196671 -4.9157887 -4.5024676 -4.1065974 -3.4898026 -3.2603161 -3.4554448 -4.0503926 -4.6033945 -4.8949637 -4.8436408][-5.6657052 -6.1464663 -6.6581883 -6.6988859 -6.3459744 -5.5697393 -4.9607606 -4.7426181 -4.378829 -4.2278671 -4.2914462 -4.5425916 -4.9074316 -5.2696209 -5.4964709][-5.114913 -5.2586908 -5.5022779 -5.6706667 -5.67834 -5.2448826 -4.7821326 -4.5418181 -4.2685986 -4.0702248 -4.0620356 -4.49191 -4.9311032 -4.9239082 -4.8589554][-4.408164 -4.3028851 -4.3103819 -4.4278874 -4.4085512 -4.185781 -4.0406613 -3.890162 -3.6787715 -3.7172952 -3.8367751 -3.9568045 -4.1797204 -4.3706303 -4.4475145]]...]
INFO - root - 2017-12-16 08:20:02.281909: step 58710, loss = 0.22, batch loss = 0.17 (29.1 examples/sec; 0.275 sec/batch; 20h:53m:04s remains)
INFO - root - 2017-12-16 08:20:05.138344: step 58720, loss = 0.26, batch loss = 0.20 (27.5 examples/sec; 0.291 sec/batch; 22h:07m:40s remains)
INFO - root - 2017-12-16 08:20:08.026837: step 58730, loss = 0.26, batch loss = 0.20 (27.2 examples/sec; 0.294 sec/batch; 22h:22m:17s remains)
INFO - root - 2017-12-16 08:20:10.910137: step 58740, loss = 0.25, batch loss = 0.19 (27.4 examples/sec; 0.292 sec/batch; 22h:13m:21s remains)
INFO - root - 2017-12-16 08:20:13.792740: step 58750, loss = 0.35, batch loss = 0.29 (27.0 examples/sec; 0.296 sec/batch; 22h:32m:21s remains)
INFO - root - 2017-12-16 08:20:16.654958: step 58760, loss = 0.24, batch loss = 0.18 (27.7 examples/sec; 0.289 sec/batch; 21h:57m:51s remains)
INFO - root - 2017-12-16 08:20:19.485416: step 58770, loss = 0.24, batch loss = 0.18 (28.9 examples/sec; 0.277 sec/batch; 21h:01m:44s remains)
INFO - root - 2017-12-16 08:20:22.338482: step 58780, loss = 0.23, batch loss = 0.17 (27.6 examples/sec; 0.290 sec/batch; 22h:03m:48s remains)
INFO - root - 2017-12-16 08:20:25.194334: step 58790, loss = 0.27, batch loss = 0.21 (27.6 examples/sec; 0.290 sec/batch; 22h:04m:39s remains)
INFO - root - 2017-12-16 08:20:28.020141: step 58800, loss = 0.21, batch loss = 0.15 (28.6 examples/sec; 0.280 sec/batch; 21h:15m:32s remains)
2017-12-16 08:20:28.546555: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9023652 -5.18069 -4.6942353 -4.6490188 -4.8297157 -5.1543255 -5.5138125 -5.5776253 -5.6807613 -5.5689359 -5.4249744 -4.8553071 -4.3113804 -4.0914383 -4.2766047][-6.3608856 -5.4369617 -4.6791568 -4.3560205 -4.0766459 -4.6153188 -4.9252338 -5.3205547 -5.6942034 -5.5694513 -5.4012995 -4.7982945 -4.3496943 -3.8517997 -3.6291449][-6.2245884 -5.123487 -4.4370618 -3.9312003 -3.6245508 -3.8817115 -3.867985 -4.4674621 -4.9813547 -5.3475304 -5.3203583 -4.8920121 -4.4382443 -3.8289196 -3.4147635][-5.3291016 -3.9517462 -2.8550327 -2.2372751 -2.1462791 -2.2915452 -2.7133336 -3.5584638 -4.2258139 -4.764575 -4.8519506 -4.6138 -4.3428025 -3.8176486 -3.3057427][-4.1585774 -2.2678912 -0.57212615 0.21258593 0.53523111 0.17568731 -0.68038154 -1.5989482 -2.8572648 -3.8247788 -4.3294153 -4.3353004 -4.14444 -3.8907413 -3.5437274][-3.254365 -1.1885951 0.68374968 2.0371161 2.8876109 2.6168485 2.035933 0.8092742 -0.7300725 -1.9398866 -3.02564 -3.4173317 -3.7315469 -3.7241049 -3.5720832][-3.0260034 -0.77941394 1.2349987 2.6478643 3.5004587 3.7488956 3.7877674 2.68612 1.0580058 -0.496696 -1.8289857 -2.7509925 -3.534621 -3.6160777 -3.719434][-2.9029262 -0.90063381 0.96963453 2.4480457 3.4654264 3.8012972 3.8248539 3.0372167 1.6929054 0.050780773 -1.1911297 -2.0098853 -2.7385702 -3.1458273 -3.6206536][-2.8423827 -1.4453986 -0.19519663 1.1142311 1.8194828 2.6499281 3.0021949 2.4663281 1.3097014 -0.25196981 -1.4314592 -2.2628236 -2.8627026 -3.316751 -3.7367816][-3.4880972 -3.0282297 -2.2758858 -1.2775807 -0.43981719 0.54098606 1.0967727 1.1013584 0.20878601 -1.0978439 -2.2091193 -3.0487444 -3.5298698 -3.8078189 -4.0256648][-4.240509 -4.0189695 -3.8151455 -3.5078678 -2.7370486 -1.6895535 -0.83956671 -0.56276989 -1.2631721 -2.3901494 -3.3385212 -3.8930869 -4.1902914 -4.3833466 -4.4242492][-4.5808368 -4.608202 -4.8731513 -5.018877 -4.5392771 -3.7765975 -2.8594828 -2.430675 -2.6234317 -3.2484694 -3.9336288 -4.3012047 -4.39402 -4.3747416 -4.3441997][-5.14175 -4.8396115 -5.198699 -5.4498591 -5.4259996 -4.9312658 -4.1654162 -3.6669919 -3.5298691 -3.9442315 -4.2354312 -4.2882514 -4.3424425 -4.3028154 -4.0237346][-5.4029069 -4.9503827 -5.1555424 -5.3525419 -5.5433521 -5.3096251 -4.93017 -4.491838 -4.2807941 -4.3229055 -4.3881192 -4.3642931 -4.3371687 -4.2088461 -3.8641531][-5.6674018 -4.9463553 -4.7145219 -4.8148479 -5.0561857 -5.0618172 -4.9393506 -4.6841159 -4.5369644 -4.456727 -4.2914124 -4.0894904 -3.9004192 -3.7353668 -3.5086474]]...]
INFO - root - 2017-12-16 08:20:31.411743: step 58810, loss = 0.26, batch loss = 0.20 (28.4 examples/sec; 0.282 sec/batch; 21h:26m:40s remains)
INFO - root - 2017-12-16 08:20:34.258584: step 58820, loss = 0.27, batch loss = 0.21 (28.3 examples/sec; 0.282 sec/batch; 21h:28m:09s remains)
INFO - root - 2017-12-16 08:20:37.193204: step 58830, loss = 0.25, batch loss = 0.19 (27.8 examples/sec; 0.288 sec/batch; 21h:53m:51s remains)
INFO - root - 2017-12-16 08:20:40.061720: step 58840, loss = 0.42, batch loss = 0.36 (27.9 examples/sec; 0.287 sec/batch; 21h:47m:46s remains)
INFO - root - 2017-12-16 08:20:42.911077: step 58850, loss = 0.30, batch loss = 0.24 (28.5 examples/sec; 0.281 sec/batch; 21h:19m:25s remains)
INFO - root - 2017-12-16 08:20:45.776464: step 58860, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 21h:16m:19s remains)
INFO - root - 2017-12-16 08:20:48.585624: step 58870, loss = 0.20, batch loss = 0.14 (28.1 examples/sec; 0.284 sec/batch; 21h:37m:21s remains)
INFO - root - 2017-12-16 08:20:51.468778: step 58880, loss = 0.33, batch loss = 0.27 (28.0 examples/sec; 0.285 sec/batch; 21h:41m:03s remains)
INFO - root - 2017-12-16 08:20:54.292974: step 58890, loss = 0.27, batch loss = 0.21 (29.3 examples/sec; 0.273 sec/batch; 20h:43m:58s remains)
INFO - root - 2017-12-16 08:20:57.171631: step 58900, loss = 0.29, batch loss = 0.23 (27.5 examples/sec; 0.291 sec/batch; 22h:05m:42s remains)
2017-12-16 08:20:57.691972: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7372642 -3.7234325 -3.7076244 -3.9828875 -4.1979241 -4.3661509 -4.409215 -4.2522044 -4.0512691 -3.6446433 -3.2467074 -2.8983016 -2.6420927 -2.4146872 -2.1814294][-3.8968496 -3.9106288 -3.9262714 -4.1478863 -4.3377304 -4.63821 -4.790988 -4.6752038 -4.4932337 -4.178545 -3.8418579 -3.4579103 -3.0921993 -2.7234616 -2.3577836][-4.1251717 -3.8720026 -3.75762 -3.8242683 -3.9506962 -4.1974545 -4.4589744 -4.6063027 -4.5853982 -4.5139585 -4.3490615 -4.1359897 -3.8371706 -3.4121103 -2.8508463][-4.3766279 -3.9286718 -3.4784086 -3.1491592 -3.0245748 -3.0584106 -3.2863383 -3.5149879 -3.8059454 -4.1223536 -4.3418074 -4.5183263 -4.5224509 -4.1719365 -3.5415096][-4.3805437 -3.6790562 -2.8072312 -1.8823102 -1.2359533 -0.924819 -0.98482442 -1.3472676 -1.8481853 -2.7152908 -3.5315576 -4.2021422 -4.5391178 -4.4936581 -4.0691986][-3.9969804 -2.9699264 -1.7930262 -0.33761406 0.90671635 1.6974277 1.905674 1.4828644 0.76537514 -0.5273869 -1.9215691 -3.2175374 -4.1517968 -4.5105624 -4.1928992][-3.4448347 -2.3167653 -1.0392425 0.724031 2.3993974 3.6917305 4.4765825 4.2778034 3.523921 1.9177756 0.10156012 -1.7518303 -3.1689727 -3.9383411 -4.0444307][-2.8252401 -1.8648009 -0.71099114 1.064208 2.9004364 4.4929991 5.596941 5.6469097 5.0870781 3.4186749 1.419703 -0.75911641 -2.5279028 -3.6163936 -3.9948804][-2.5834756 -1.8231971 -0.93156409 0.60056639 2.1837149 3.7146339 4.7626019 4.858449 4.348711 2.839416 0.96613693 -1.1760032 -2.9546733 -3.9838402 -4.3797269][-2.5565724 -2.0692518 -1.6559474 -0.65994954 0.52521181 1.4600501 2.1776619 2.2778907 2.0045748 0.78839159 -0.7952683 -2.6132791 -4.146471 -4.8884845 -4.9918413][-2.8517051 -2.5397346 -2.6204212 -2.2937925 -1.6601813 -1.1236217 -0.65175509 -0.81925607 -1.2725396 -2.0799534 -3.103693 -4.4721446 -5.5132484 -5.9551816 -5.8720994][-3.6056266 -3.2453084 -3.3751841 -3.4663403 -3.5327945 -3.5326967 -3.5812609 -3.855474 -4.1353488 -4.6328912 -5.1869869 -6.0234385 -6.7048526 -6.9098463 -6.7202311][-3.9028511 -3.403966 -3.5357373 -4.0457764 -4.6787558 -5.1343551 -5.543417 -5.9759684 -6.1709619 -6.2509027 -6.215332 -6.463274 -6.8111286 -6.9671607 -6.9115238][-4.3814812 -3.5053179 -3.3367877 -3.7918901 -4.5941811 -5.3721933 -6.052011 -6.53687 -6.5584631 -6.291204 -5.9833937 -5.9364715 -6.1121902 -6.2436323 -6.3160839][-4.5581255 -3.4020307 -2.7499661 -2.9098787 -3.6694233 -4.4231391 -5.1102123 -5.6642714 -5.9427876 -5.6153951 -5.0831728 -4.8223071 -4.7816072 -5.0464282 -5.5139642]]...]
INFO - root - 2017-12-16 08:21:00.534953: step 58910, loss = 0.20, batch loss = 0.14 (27.9 examples/sec; 0.287 sec/batch; 21h:46m:52s remains)
INFO - root - 2017-12-16 08:21:03.357450: step 58920, loss = 0.29, batch loss = 0.23 (29.0 examples/sec; 0.276 sec/batch; 20h:56m:52s remains)
INFO - root - 2017-12-16 08:21:06.211882: step 58930, loss = 0.22, batch loss = 0.16 (28.3 examples/sec; 0.283 sec/batch; 21h:28m:41s remains)
INFO - root - 2017-12-16 08:21:09.056327: step 58940, loss = 0.21, batch loss = 0.15 (28.1 examples/sec; 0.285 sec/batch; 21h:37m:30s remains)
INFO - root - 2017-12-16 08:21:11.931420: step 58950, loss = 0.24, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 21h:29m:33s remains)
INFO - root - 2017-12-16 08:21:14.772951: step 58960, loss = 0.32, batch loss = 0.26 (27.9 examples/sec; 0.286 sec/batch; 21h:45m:47s remains)
INFO - root - 2017-12-16 08:21:17.661082: step 58970, loss = 0.24, batch loss = 0.18 (27.3 examples/sec; 0.294 sec/batch; 22h:18m:10s remains)
INFO - root - 2017-12-16 08:21:20.488754: step 58980, loss = 0.22, batch loss = 0.17 (27.9 examples/sec; 0.286 sec/batch; 21h:45m:35s remains)
INFO - root - 2017-12-16 08:21:23.362271: step 58990, loss = 0.26, batch loss = 0.20 (27.4 examples/sec; 0.292 sec/batch; 22h:09m:58s remains)
INFO - root - 2017-12-16 08:21:26.215324: step 59000, loss = 0.31, batch loss = 0.25 (27.8 examples/sec; 0.288 sec/batch; 21h:52m:32s remains)
2017-12-16 08:21:26.736102: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3923981 -3.1477194 -2.9833574 -2.8445311 -2.6134784 -2.4509225 -2.4296875 -2.9544096 -3.4139342 -3.8184929 -4.1626878 -4.4961128 -4.5242376 -4.3079066 -4.3815088][-3.0341589 -2.924057 -2.9050741 -2.7539353 -2.4560785 -2.3738687 -2.3459995 -2.7091842 -2.9929059 -3.5170317 -4.05047 -4.3365617 -4.3111229 -4.268631 -4.4752388][-2.7208846 -2.651324 -2.7323496 -2.6613002 -2.3757572 -2.2344913 -2.1304255 -2.328429 -2.5922046 -2.8679333 -3.3273883 -3.8350914 -4.1894822 -4.2515535 -4.3906078][-1.837281 -1.9931672 -2.2931614 -2.3999269 -2.1875775 -1.9473777 -1.6952219 -1.6838958 -1.8024962 -1.9516351 -2.3165936 -2.8740335 -3.3447814 -3.7631247 -4.088707][-1.1755579 -1.5138621 -1.9410286 -2.1196926 -1.9725411 -1.7881618 -1.4338005 -0.9744153 -0.80512524 -1.0935555 -1.4978852 -1.9111497 -2.3105369 -2.7961419 -3.190403][-0.60469317 -1.191632 -1.5998249 -1.758656 -1.6292 -1.3917196 -0.91710067 -0.33294153 -0.013761044 -0.09700346 -0.4027462 -0.97025657 -1.5306752 -1.9970858 -2.2609568][-0.15221977 -0.63501334 -0.98994708 -1.1009817 -0.88424969 -0.56095767 -0.12226343 0.34248209 0.72923565 0.63029766 0.27601576 -0.11863184 -0.45329404 -0.98868728 -1.3360794][0.15300035 -0.28125381 -0.611048 -0.51206231 -0.26219559 0.1063571 0.52775669 0.98662329 1.3068562 1.2007751 0.80784178 0.33148718 -0.12546349 -0.69344211 -1.0055676][-0.085366249 -0.20218754 -0.29099941 -0.15286827 0.16597795 0.65882492 1.0335565 1.3300495 1.4968939 1.2914405 0.91484642 0.30474043 -0.27009058 -0.78759241 -1.1304483][-0.40193176 -0.21648741 -0.12741232 0.093910217 0.40416002 0.85368967 1.1917391 1.4748154 1.5769835 1.2057443 0.63940382 -0.038703442 -0.69908237 -1.3791549 -1.7904639][-0.87778163 -0.41688776 -0.31409359 -0.088252068 0.32387114 0.798203 1.1100497 1.0777626 0.81313372 0.41874838 -0.24518538 -0.95744991 -1.6149912 -2.2291751 -2.6078963][-2.0646012 -1.3273826 -1.0120413 -0.80396557 -0.42583609 0.047791958 0.226789 0.11620045 -0.17445898 -0.74130106 -1.53057 -2.1343646 -2.6486783 -3.1560144 -3.4775434][-3.052084 -2.2722723 -1.7367601 -1.4150445 -1.1246181 -0.680181 -0.44977427 -0.83441591 -1.3464482 -1.7247858 -2.0312173 -2.551065 -3.1696227 -3.5026088 -3.6076307][-3.9585652 -2.9820261 -2.2718081 -1.9833393 -1.8995543 -1.6554782 -1.568501 -1.8472614 -2.2818065 -2.7218916 -2.8661809 -2.9378691 -3.0632498 -3.2543073 -3.3529675][-4.4284444 -3.5344491 -2.7766163 -2.432466 -2.3343875 -2.2307141 -2.3647211 -2.7593775 -3.2467096 -3.4254563 -3.316977 -3.0385323 -2.9810934 -3.0015433 -2.9282479]]...]
INFO - root - 2017-12-16 08:21:29.617709: step 59010, loss = 0.29, batch loss = 0.23 (27.7 examples/sec; 0.288 sec/batch; 21h:54m:13s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 08:21:32.475271: step 59020, loss = 0.29, batch loss = 0.23 (27.7 examples/sec; 0.289 sec/batch; 21h:55m:18s remains)
INFO - root - 2017-12-16 08:21:35.299084: step 59030, loss = 0.24, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 20h:50m:50s remains)
INFO - root - 2017-12-16 08:21:38.134262: step 59040, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.284 sec/batch; 21h:34m:13s remains)
INFO - root - 2017-12-16 08:21:41.046702: step 59050, loss = 0.25, batch loss = 0.19 (27.7 examples/sec; 0.288 sec/batch; 21h:54m:32s remains)
INFO - root - 2017-12-16 08:21:43.876821: step 59060, loss = 0.29, batch loss = 0.23 (28.0 examples/sec; 0.286 sec/batch; 21h:41m:27s remains)
INFO - root - 2017-12-16 08:21:46.752495: step 59070, loss = 0.54, batch loss = 0.48 (28.3 examples/sec; 0.283 sec/batch; 21h:29m:26s remains)
INFO - root - 2017-12-16 08:21:49.576026: step 59080, loss = 0.28, batch loss = 0.23 (29.5 examples/sec; 0.271 sec/batch; 20h:36m:20s remains)
INFO - root - 2017-12-16 08:21:52.360829: step 59090, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.279 sec/batch; 21h:13m:33s remains)
INFO - root - 2017-12-16 08:21:55.216988: step 59100, loss = 0.25, batch loss = 0.19 (28.9 examples/sec; 0.277 sec/batch; 21h:02m:42s remains)
2017-12-16 08:21:55.720358: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8776207 -3.2957845 -3.8440697 -4.1965704 -4.1584921 -4.0771179 -4.143187 -4.1791992 -4.2160358 -4.1028438 -4.1025138 -4.0195012 -3.7984109 -3.6410387 -3.47182][-2.18029 -2.4432023 -2.9213171 -3.2851272 -3.5696023 -3.7235823 -3.9238827 -3.9191329 -3.9134355 -3.8492033 -3.7863851 -3.5895941 -3.2634132 -2.8925438 -2.6515405][-1.9398258 -2.2058582 -2.5869751 -2.9283261 -3.1798975 -3.3970742 -3.5529046 -3.5480509 -3.4715493 -3.1759639 -3.0025783 -2.8227143 -2.584661 -2.2828431 -1.8711863][-1.3741927 -1.8670731 -2.4036152 -2.652272 -2.7276344 -2.8313155 -2.8920979 -2.5490336 -2.2566497 -2.0094225 -1.7080193 -1.6667435 -1.6989057 -1.4925675 -1.1198001][-1.641367 -2.00477 -2.3249426 -2.5103984 -2.3787365 -1.9973302 -1.6001995 -1.434823 -1.3579929 -1.1505649 -1.1408701 -1.2288918 -1.1049337 -0.9724381 -0.80984926][-2.1396403 -2.562726 -2.7877605 -2.6804523 -2.1988294 -1.5623231 -0.91374159 -0.16919661 0.052184582 -0.24957323 -0.555737 -0.83269787 -1.1243267 -1.0143018 -0.57247734][-3.0221298 -3.3633068 -3.415226 -2.9284167 -1.9744096 -1.0053394 -0.026816368 0.62674189 0.77572441 0.4950614 -0.077025414 -0.75467014 -1.2827671 -1.4076648 -1.2927399][-3.6926363 -3.9940546 -3.9238057 -3.2770531 -2.0971375 -0.7317667 0.48732662 1.2203565 1.3011813 0.75281143 0.020576954 -0.92325163 -1.8188231 -2.0788479 -1.9650714][-4.0583892 -4.3785477 -4.2186375 -3.5862937 -2.5868125 -1.1327438 0.32036543 1.1030331 1.145155 0.61239052 -0.24574614 -1.3029985 -2.2040346 -2.7235506 -2.8923349][-4.3752642 -4.9406347 -5.0210533 -4.3233285 -3.1319735 -1.7836897 -0.528245 0.41622162 0.6382575 0.049225807 -0.91229463 -1.9926364 -2.9974542 -3.5189929 -3.5487375][-4.4880548 -5.1135983 -5.2792134 -4.843194 -3.7995787 -2.2154977 -0.80561304 -0.18835783 -0.22926474 -0.68720174 -1.4985428 -2.5382681 -3.4747834 -3.9468296 -4.0005336][-4.455369 -5.0568113 -5.185864 -4.6780524 -3.7709546 -2.5524752 -1.3323739 -0.70564127 -0.73563457 -1.2543705 -2.0476234 -2.9735262 -3.7708731 -4.1836071 -4.2442946][-4.4885983 -4.9409828 -4.9806919 -4.403914 -3.5862055 -2.4367361 -1.4066925 -0.91081977 -0.99060965 -1.5140159 -2.2482581 -3.0713997 -3.7769477 -4.2220945 -4.3029494][-4.4872575 -4.8548832 -4.671 -3.9205394 -3.0253968 -1.9672611 -1.1772418 -0.76859975 -0.9136579 -1.3365579 -1.902158 -2.6384172 -3.3711631 -3.8568134 -4.0411839][-4.3492765 -4.55293 -4.2762213 -3.4262192 -2.4402983 -1.4343739 -0.73023415 -0.51045108 -0.73164678 -1.1540782 -1.6796486 -2.3358359 -3.0926461 -3.6889298 -3.8911953]]...]
INFO - root - 2017-12-16 08:21:58.534926: step 59110, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.281 sec/batch; 21h:20m:16s remains)
INFO - root - 2017-12-16 08:22:01.390466: step 59120, loss = 0.25, batch loss = 0.19 (28.3 examples/sec; 0.283 sec/batch; 21h:29m:27s remains)
INFO - root - 2017-12-16 08:22:04.199483: step 59130, loss = 0.42, batch loss = 0.36 (27.8 examples/sec; 0.288 sec/batch; 21h:50m:56s remains)
INFO - root - 2017-12-16 08:22:07.046523: step 59140, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.279 sec/batch; 21h:08m:52s remains)
INFO - root - 2017-12-16 08:22:09.869716: step 59150, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.280 sec/batch; 21h:13m:27s remains)
INFO - root - 2017-12-16 08:22:12.705034: step 59160, loss = 0.26, batch loss = 0.20 (27.8 examples/sec; 0.287 sec/batch; 21h:49m:25s remains)
INFO - root - 2017-12-16 08:22:15.530637: step 59170, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.284 sec/batch; 21h:32m:58s remains)
INFO - root - 2017-12-16 08:22:18.343548: step 59180, loss = 0.23, batch loss = 0.17 (27.7 examples/sec; 0.289 sec/batch; 21h:54m:17s remains)
INFO - root - 2017-12-16 08:22:21.151459: step 59190, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.282 sec/batch; 21h:23m:50s remains)
INFO - root - 2017-12-16 08:22:24.013699: step 59200, loss = 0.26, batch loss = 0.21 (27.6 examples/sec; 0.289 sec/batch; 21h:58m:16s remains)
2017-12-16 08:22:24.519533: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7784665 -2.1230667 -2.4253268 -2.5534616 -2.5923026 -2.6966891 -2.7406356 -2.9016056 -3.1504078 -3.4858544 -3.7195079 -3.7176192 -3.6144185 -3.2644365 -2.9521294][-1.7599342 -1.8974164 -1.9496276 -1.8983815 -1.8012493 -1.9461741 -1.9383259 -2.0358429 -2.2592533 -2.777597 -3.2143602 -3.3346982 -3.2660637 -2.8589656 -2.3847513][-1.5871022 -1.5162427 -1.3567252 -0.95438075 -0.61898565 -0.58820581 -0.61783957 -0.88299346 -1.2224183 -1.8165374 -2.3514585 -2.8311124 -2.9725981 -2.5431223 -1.8909318][-1.0822616 -0.7026062 -0.36901045 0.28086424 0.87380362 1.0862322 0.91427422 0.38417387 -0.37038755 -1.1596053 -1.8108048 -2.304285 -2.6085639 -2.4117403 -1.7901795][-1.078372 -0.12664461 0.77464533 1.4166675 2.0008292 2.2741919 2.1361494 1.5461788 0.55331707 -0.30510569 -1.0123229 -1.5850658 -1.9294674 -1.9204128 -1.6187596][-1.2691638 -0.14352751 1.0900612 2.124958 2.862545 3.0412211 2.8656349 2.3129754 1.5026016 0.59537888 -0.21462107 -0.9154954 -1.5750873 -1.7513371 -1.6064041][-1.4599085 -0.12757778 1.231163 2.4894471 3.3426151 3.6301832 3.5715704 2.8182497 1.7884793 0.9228735 0.20565033 -0.65722609 -1.620681 -2.2251985 -2.6765685][-2.2454226 -0.97816825 0.47771072 1.8216462 2.76195 3.0748191 3.0572042 2.5699353 1.6349783 0.66332483 -0.22532225 -1.1168451 -2.1393132 -3.0879135 -3.7867472][-3.505126 -2.5160279 -1.1080763 0.25128603 1.3248715 1.8602757 1.8369627 1.4959483 0.78532028 -0.04064846 -0.92572474 -1.9296544 -2.9605713 -3.9030471 -4.5065351][-4.6456666 -3.8432312 -2.848428 -1.706049 -0.56102157 0.15306568 0.38538122 0.064695358 -0.52366042 -1.226619 -1.9549899 -2.7608175 -3.6169121 -4.384819 -4.8502064][-5.6060081 -4.8930454 -3.9678235 -3.1222796 -2.3477068 -1.8211329 -1.3400011 -1.4952536 -1.9748635 -2.5715809 -3.2067761 -3.9519031 -4.638607 -4.9136095 -4.9703975][-6.1495771 -5.7422881 -5.0827074 -4.360744 -3.709816 -3.2710524 -2.9306064 -3.0383174 -3.3276949 -3.746599 -4.2999287 -4.9004273 -5.3993521 -5.407948 -5.0102458][-5.8596287 -5.6128516 -5.2994 -4.8484941 -4.41988 -4.0166039 -3.8056333 -4.0027647 -4.2979641 -4.5533919 -4.84416 -5.213912 -5.475287 -5.4034624 -4.8702431][-5.1733866 -4.7970395 -4.5504656 -4.4088511 -4.2590141 -4.0640421 -3.9400573 -4.1097288 -4.3348217 -4.5187469 -4.5623064 -4.615438 -4.6175585 -4.4694271 -4.0311942][-4.4401236 -4.1735907 -3.9554098 -3.8959115 -3.8397844 -3.7658808 -3.7389526 -3.8340933 -3.9967136 -4.1468763 -4.1791191 -4.1422386 -4.0468535 -3.9554493 -3.6611009]]...]
INFO - root - 2017-12-16 08:22:27.345297: step 59210, loss = 0.37, batch loss = 0.31 (26.2 examples/sec; 0.305 sec/batch; 23h:08m:14s remains)
INFO - root - 2017-12-16 08:22:30.120035: step 59220, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 21h:43m:37s remains)
INFO - root - 2017-12-16 08:22:33.040105: step 59230, loss = 0.27, batch loss = 0.22 (27.6 examples/sec; 0.290 sec/batch; 22h:02m:13s remains)
INFO - root - 2017-12-16 08:22:35.900646: step 59240, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.280 sec/batch; 21h:16m:48s remains)
INFO - root - 2017-12-16 08:22:38.703525: step 59250, loss = 0.29, batch loss = 0.23 (28.4 examples/sec; 0.282 sec/batch; 21h:23m:51s remains)
INFO - root - 2017-12-16 08:22:41.513067: step 59260, loss = 0.28, batch loss = 0.22 (29.0 examples/sec; 0.276 sec/batch; 20h:57m:06s remains)
INFO - root - 2017-12-16 08:22:44.358263: step 59270, loss = 0.30, batch loss = 0.24 (28.3 examples/sec; 0.282 sec/batch; 21h:25m:56s remains)
INFO - root - 2017-12-16 08:22:47.210330: step 59280, loss = 0.29, batch loss = 0.23 (28.6 examples/sec; 0.280 sec/batch; 21h:12m:57s remains)
INFO - root - 2017-12-16 08:22:50.090388: step 59290, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 21h:41m:22s remains)
INFO - root - 2017-12-16 08:22:52.892635: step 59300, loss = 0.41, batch loss = 0.35 (28.9 examples/sec; 0.277 sec/batch; 21h:00m:58s remains)
2017-12-16 08:22:53.409956: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5842705 -5.7480364 -5.651669 -5.5556092 -5.50832 -5.4568973 -5.4325328 -5.5574164 -5.7090411 -5.8012891 -5.86552 -5.790422 -5.5388064 -5.15418 -4.800199][-5.6712961 -5.6054997 -5.4958992 -5.5392122 -5.5316811 -5.7421241 -5.9233365 -5.8881865 -5.9069338 -6.1928391 -6.3378506 -6.1504641 -5.8943028 -5.6038718 -5.3395233][-4.9673328 -4.9338822 -4.9125872 -4.8685536 -4.9430275 -5.1753964 -5.187326 -5.4064097 -5.6991529 -5.8488417 -6.0070081 -6.1837139 -6.1381092 -5.8136883 -5.5883808][-4.0635352 -3.6331053 -3.4174585 -3.5043125 -3.6144798 -3.7939677 -3.8080411 -3.9177547 -4.0833564 -4.5491171 -4.9417572 -5.2261529 -5.4166789 -5.400198 -5.3685088][-2.5061469 -1.7553537 -1.3924844 -1.5580499 -1.7996995 -1.831881 -1.5932755 -1.645611 -1.8692937 -2.4224319 -3.0537853 -3.6531792 -4.1431441 -4.3137808 -4.3153152][-1.4229319 -0.56253433 -0.082166195 0.081448078 0.14957285 0.24255514 0.717875 0.99393606 0.83795452 0.20135021 -0.59871268 -1.6618121 -2.5629015 -3.0107031 -3.1864352][-0.63687634 0.15479612 0.52659464 0.66370821 0.989799 1.6902833 2.7097239 3.2120519 3.030798 2.3892303 1.5036035 0.33802462 -0.79599404 -1.6125524 -2.0872698][-0.19040012 0.45689917 0.79767275 1.0401402 1.3464189 2.0154262 3.1026363 3.9288578 4.011096 3.3625154 2.4197435 1.1708293 0.020536423 -0.6587882 -1.2761345][-0.31712675 0.028745174 0.16799021 0.38767242 0.8457222 1.6023688 2.5844073 3.1825137 3.2461572 2.8239021 2.0571251 0.95002937 -0.11061049 -0.72092843 -1.128505][-0.62963533 -0.4802072 -0.56000853 -0.44386911 -0.11924124 0.46167517 1.2443929 1.7925644 1.8422651 1.38941 0.7641983 -0.061440468 -0.81176972 -1.1704063 -1.4445915][-1.2792675 -1.0799632 -1.0340853 -1.0303211 -0.83143353 -0.49449134 -0.039149761 0.12648249 0.058577061 -0.31022072 -0.81384254 -1.4644055 -1.9179406 -1.9791894 -2.0840602][-1.5912101 -1.3119338 -1.2844527 -1.2923028 -1.1634431 -0.96911192 -0.80795431 -0.93071485 -1.1975873 -1.7217269 -2.1959312 -2.4922566 -2.771883 -2.5780318 -2.4066296][-1.752754 -1.4014418 -1.2345114 -1.268616 -1.2841434 -1.1818523 -1.0831089 -1.4728415 -1.9424543 -2.4883051 -2.9306049 -3.1032743 -3.1892076 -2.8500037 -2.4608998][-1.8695698 -1.3877234 -1.1402924 -1.0320203 -0.96094942 -0.97854996 -0.90848184 -1.3648045 -2.0679083 -2.8867984 -3.3661454 -3.3447342 -3.1921325 -2.7938905 -2.413167][-1.8934524 -1.3923042 -1.0647161 -0.98950458 -0.90290308 -0.7594943 -0.63995481 -1.151572 -1.8566244 -2.703851 -3.3327286 -3.4585824 -3.3800101 -2.8540974 -2.4030516]]...]
INFO - root - 2017-12-16 08:22:56.233697: step 59310, loss = 0.18, batch loss = 0.12 (28.5 examples/sec; 0.281 sec/batch; 21h:18m:01s remains)
INFO - root - 2017-12-16 08:22:59.054857: step 59320, loss = 0.29, batch loss = 0.23 (28.7 examples/sec; 0.279 sec/batch; 21h:10m:36s remains)
INFO - root - 2017-12-16 08:23:01.912586: step 59330, loss = 0.21, batch loss = 0.15 (29.0 examples/sec; 0.276 sec/batch; 20h:57m:33s remains)
INFO - root - 2017-12-16 08:23:04.742227: step 59340, loss = 0.24, batch loss = 0.19 (28.1 examples/sec; 0.284 sec/batch; 21h:34m:32s remains)
INFO - root - 2017-12-16 08:23:07.547191: step 59350, loss = 0.34, batch loss = 0.28 (27.9 examples/sec; 0.287 sec/batch; 21h:46m:53s remains)
INFO - root - 2017-12-16 08:23:10.373515: step 59360, loss = 0.28, batch loss = 0.22 (29.2 examples/sec; 0.274 sec/batch; 20h:48m:43s remains)
INFO - root - 2017-12-16 08:23:13.196045: step 59370, loss = 0.27, batch loss = 0.22 (27.9 examples/sec; 0.287 sec/batch; 21h:44m:42s remains)
INFO - root - 2017-12-16 08:23:15.966953: step 59380, loss = 0.43, batch loss = 0.37 (28.9 examples/sec; 0.277 sec/batch; 20h:59m:49s remains)
INFO - root - 2017-12-16 08:23:18.776580: step 59390, loss = 0.24, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 21h:36m:52s remains)
INFO - root - 2017-12-16 08:23:21.664029: step 59400, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.286 sec/batch; 21h:41m:35s remains)
2017-12-16 08:23:22.181802: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2811818 -2.9629936 -2.7843962 -2.5089388 -2.4848769 -2.57855 -2.7480469 -2.7662187 -2.7440653 -2.8658547 -2.9378409 -3.1906571 -3.3774171 -3.6437936 -3.8504539][-3.5665059 -3.1133823 -2.7684598 -2.5215974 -2.5342445 -2.7054906 -2.9468112 -3.0048921 -3.0484281 -3.3023882 -3.509593 -3.7556381 -3.8961215 -4.1797295 -4.3301578][-3.8105123 -3.2773929 -2.8303542 -2.6151206 -2.5771332 -2.6977434 -2.8176672 -2.886095 -3.0564225 -3.2504878 -3.4951963 -3.9224885 -4.2280073 -4.5074353 -4.5283785][-3.8582356 -3.3209226 -2.810039 -2.5023282 -2.2412572 -2.2444866 -2.3305931 -2.2676597 -2.3376839 -2.6027131 -3.0483136 -3.5545628 -3.8850973 -4.1763186 -4.2767038][-4.0333037 -3.5829291 -2.992739 -2.4124663 -1.8412988 -1.5296791 -1.2932498 -1.168267 -1.3577213 -1.7001934 -2.1884313 -2.766788 -3.1843629 -3.4352953 -3.4111991][-3.8171315 -3.3722081 -2.6921804 -1.929723 -0.92157531 -0.23046303 0.23063469 0.3905549 0.1117053 -0.38993454 -1.1402123 -1.8287473 -2.1716144 -2.3044012 -2.2788157][-3.462399 -3.1215212 -2.4533253 -1.4711068 -0.22805452 0.67415428 1.368609 1.5341306 1.1835213 0.60221577 -0.13289881 -0.7155664 -0.95445371 -0.86163092 -0.57287908][-3.5046611 -3.1768928 -2.4461045 -1.388427 -0.095585346 1.0296831 1.8642049 1.9537554 1.5957265 0.99341106 0.35167933 0.036194324 0.086632252 0.51567554 0.97962427][-3.6596165 -3.3826041 -2.7501717 -1.7994461 -0.71007109 0.33078003 1.0223837 1.1410966 0.95119333 0.54673338 0.31973028 0.39666939 0.68932438 1.3446002 1.8732634][-3.7102294 -3.4959185 -3.0800743 -2.4549754 -1.6774943 -0.85145807 -0.37932873 -0.2811718 -0.306046 -0.42451239 -0.26068878 0.094773293 0.73084831 1.4965935 1.9538279][-3.8204944 -3.7258964 -3.557446 -3.2007997 -2.7804179 -2.189347 -1.8687475 -1.9089413 -1.8587372 -1.7400913 -1.2978449 -0.76371074 -0.005856514 0.75711107 1.1555886][-3.9268334 -3.7659593 -3.7710984 -3.65149 -3.6099877 -3.3197694 -3.2875633 -3.4348853 -3.3081508 -3.0052366 -2.4202507 -1.8285189 -1.08693 -0.46147871 -0.16510868][-3.5582018 -3.3652935 -3.4530337 -3.4214602 -3.6448536 -3.6629207 -3.9478896 -4.4537807 -4.5557256 -4.35173 -3.7229249 -3.1283598 -2.5197825 -2.0202503 -1.6841798][-3.244576 -3.0020771 -3.0969548 -3.1563656 -3.6028495 -3.9064324 -4.507957 -5.3150187 -5.6657219 -5.6183586 -5.0111508 -4.4995232 -3.9672017 -3.5826638 -3.3705463][-2.8636079 -2.4592676 -2.4885566 -2.6353457 -3.2717059 -3.9201903 -4.8583241 -5.8770804 -6.5316343 -6.7328339 -6.2477131 -5.6224303 -5.0096326 -4.6473856 -4.3688922]]...]
INFO - root - 2017-12-16 08:23:25.048168: step 59410, loss = 0.25, batch loss = 0.19 (28.7 examples/sec; 0.279 sec/batch; 21h:07m:44s remains)
INFO - root - 2017-12-16 08:23:27.860516: step 59420, loss = 0.27, batch loss = 0.21 (28.4 examples/sec; 0.282 sec/batch; 21h:23m:08s remains)
INFO - root - 2017-12-16 08:23:30.691806: step 59430, loss = 0.26, batch loss = 0.20 (27.6 examples/sec; 0.290 sec/batch; 21h:58m:23s remains)
INFO - root - 2017-12-16 08:23:33.531226: step 59440, loss = 0.27, batch loss = 0.21 (27.2 examples/sec; 0.294 sec/batch; 22h:18m:32s remains)
INFO - root - 2017-12-16 08:23:36.336915: step 59450, loss = 0.28, batch loss = 0.23 (28.1 examples/sec; 0.285 sec/batch; 21h:35m:03s remains)
INFO - root - 2017-12-16 08:23:39.169900: step 59460, loss = 0.24, batch loss = 0.19 (26.5 examples/sec; 0.301 sec/batch; 22h:51m:22s remains)
INFO - root - 2017-12-16 08:23:42.033778: step 59470, loss = 0.31, batch loss = 0.26 (27.7 examples/sec; 0.289 sec/batch; 21h:55m:55s remains)
INFO - root - 2017-12-16 08:23:44.845038: step 59480, loss = 0.26, batch loss = 0.20 (29.3 examples/sec; 0.273 sec/batch; 20h:44m:20s remains)
INFO - root - 2017-12-16 08:23:47.682391: step 59490, loss = 0.24, batch loss = 0.18 (26.8 examples/sec; 0.299 sec/batch; 22h:39m:55s remains)
INFO - root - 2017-12-16 08:23:50.521384: step 59500, loss = 0.28, batch loss = 0.23 (27.9 examples/sec; 0.287 sec/batch; 21h:46m:34s remains)
2017-12-16 08:23:51.122057: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1533227 -5.3948493 -5.4270487 -5.5751276 -5.5857282 -5.595428 -5.5992842 -5.4059162 -5.181015 -5.0234289 -4.8635588 -4.8021288 -4.6562905 -4.2354178 -4.0430937][-4.2587109 -4.6484222 -4.749332 -4.7383466 -4.672534 -4.9794612 -5.3194847 -5.171648 -5.0558295 -5.0426497 -4.9908543 -4.831531 -4.7260194 -4.2840519 -3.7972622][-3.8541865 -3.9370234 -3.8302128 -3.8189373 -3.9452076 -4.1101651 -4.3883343 -4.6608734 -4.65331 -4.6141939 -4.6785908 -4.7081151 -4.6866717 -4.1984382 -3.6491678][-2.8005431 -2.977612 -2.7145987 -2.328191 -2.2419462 -2.5070548 -3.1117306 -3.3810406 -3.4648273 -3.8664005 -3.9844584 -4.0600815 -4.2577677 -3.9763541 -3.292665][-2.1514256 -1.5832651 -0.71008325 -0.30563211 -0.27403545 -0.39478779 -0.874866 -1.1438713 -1.4645884 -1.9651141 -2.5149693 -3.0152228 -3.1974158 -3.4951782 -3.4475489][-2.2617245 -1.3856077 -0.049156189 1.1857109 1.7856054 1.6840076 1.1923561 1.0753155 0.9309597 0.37760973 -0.52174258 -1.7301438 -2.720715 -3.1247592 -3.1037273][-3.1870239 -1.6744196 0.04545784 1.4632311 2.4705586 2.9326634 3.0464082 3.045795 2.9521241 2.2191248 0.95663452 -0.60354161 -1.9826448 -3.1441073 -3.61467][-3.9170828 -2.1656573 -0.74107242 0.97262764 2.4624658 3.4182491 3.8747196 4.0802813 4.0161743 3.1309867 1.7794542 -0.082627773 -1.6988564 -2.9421237 -3.6405678][-5.4213095 -3.8432789 -2.1502705 -0.618526 0.7650938 2.3826895 3.5956793 3.9154549 3.6972885 2.6991668 1.087306 -0.801806 -2.3139603 -3.2944059 -3.6050684][-6.712245 -5.63471 -4.6444063 -3.0607533 -1.1796443 0.41658831 1.3948483 1.8788972 1.8363557 0.59850883 -1.0332141 -2.6487679 -3.9714923 -4.5949287 -4.5431085][-8.2811012 -7.4056559 -6.37107 -5.5297771 -4.3952785 -2.8539276 -1.4798605 -1.1688147 -1.4572687 -2.3233988 -3.5719664 -4.8570051 -5.6367674 -5.8285208 -5.5525155][-8.2744465 -8.239131 -7.9491558 -7.0617981 -6.013134 -5.0536957 -4.1922922 -3.7459764 -3.7692392 -4.4786334 -5.2802148 -5.99764 -6.3936954 -6.3554273 -5.9145565][-7.1940451 -7.5463076 -7.5850391 -7.2236671 -6.7734728 -6.0830255 -5.3766065 -5.129776 -5.0940156 -5.4408178 -6.04093 -6.5278015 -6.5855241 -6.3527021 -5.9520288][-5.832736 -6.3555374 -6.706852 -6.6875486 -6.5216241 -6.0181904 -5.5656466 -5.5561638 -5.5881824 -5.5946913 -5.5841689 -5.7653942 -5.9832859 -5.8153634 -5.5011144][-4.3004351 -4.6751475 -5.0285463 -5.3357553 -5.3943605 -5.1238613 -4.8543277 -4.6570745 -4.5570531 -4.556725 -4.5518384 -4.613224 -4.663228 -4.8232064 -4.8771291]]...]
INFO - root - 2017-12-16 08:23:53.913822: step 59510, loss = 0.21, batch loss = 0.16 (28.2 examples/sec; 0.283 sec/batch; 21h:29m:29s remains)
INFO - root - 2017-12-16 08:23:56.794861: step 59520, loss = 0.20, batch loss = 0.14 (28.3 examples/sec; 0.282 sec/batch; 21h:23m:59s remains)
INFO - root - 2017-12-16 08:23:59.619603: step 59530, loss = 0.26, batch loss = 0.20 (26.2 examples/sec; 0.305 sec/batch; 23h:09m:41s remains)
INFO - root - 2017-12-16 08:24:02.492886: step 59540, loss = 0.27, batch loss = 0.21 (28.7 examples/sec; 0.279 sec/batch; 21h:07m:43s remains)
INFO - root - 2017-12-16 08:24:05.376827: step 59550, loss = 0.23, batch loss = 0.17 (27.6 examples/sec; 0.290 sec/batch; 21h:57m:21s remains)
INFO - root - 2017-12-16 08:24:08.240783: step 59560, loss = 0.26, batch loss = 0.21 (27.6 examples/sec; 0.290 sec/batch; 21h:59m:39s remains)
INFO - root - 2017-12-16 08:24:11.074257: step 59570, loss = 0.26, batch loss = 0.20 (29.2 examples/sec; 0.274 sec/batch; 20h:48m:05s remains)
INFO - root - 2017-12-16 08:24:13.896394: step 59580, loss = 0.27, batch loss = 0.21 (28.8 examples/sec; 0.278 sec/batch; 21h:04m:45s remains)
INFO - root - 2017-12-16 08:24:16.708558: step 59590, loss = 0.46, batch loss = 0.40 (29.1 examples/sec; 0.275 sec/batch; 20h:51m:42s remains)
INFO - root - 2017-12-16 08:24:19.550453: step 59600, loss = 0.29, batch loss = 0.23 (29.0 examples/sec; 0.276 sec/batch; 20h:53m:07s remains)
2017-12-16 08:24:20.056821: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8611484 -3.1656671 -3.4496322 -3.8470695 -4.2543678 -4.6154871 -4.9719977 -5.501204 -5.9784245 -6.1985879 -6.1406765 -5.9392476 -5.646594 -5.1056151 -4.8330712][-2.5725181 -2.8189797 -3.0963206 -3.53012 -3.928818 -4.5455709 -5.1041832 -5.6441154 -6.1389179 -6.5308952 -6.7577238 -6.5353022 -6.1747112 -5.5508947 -4.9216127][-2.2078469 -2.2693298 -2.5158777 -2.9660606 -3.3637071 -3.9535518 -4.424747 -5.0105977 -5.5477562 -6.0171328 -6.4441633 -6.5227475 -6.3101931 -5.4697866 -4.7278342][-1.7311497 -1.7041018 -1.8332486 -2.2017443 -2.6516252 -3.0849657 -3.3501034 -3.6397309 -3.9716091 -4.5453486 -5.1736345 -5.6794691 -5.779438 -5.1139355 -4.2567391][-1.2934005 -1.0819178 -1.1500523 -1.4885581 -1.7815843 -1.7950768 -1.5732603 -1.5052578 -1.8148901 -2.3809793 -3.2267044 -4.1896148 -4.73771 -4.5238719 -3.8909628][-1.1572742 -0.82854462 -0.73636222 -0.77183008 -0.60924315 -0.28497887 0.29286623 0.93782425 0.92769527 0.043365 -1.4735832 -2.8189669 -3.6331594 -3.7822127 -3.4987752][-1.147948 -0.82771087 -0.75004077 -0.59048843 -0.13644981 0.72273827 1.9704037 2.827405 2.8024821 1.9690704 0.49731874 -1.256012 -2.6879325 -3.1084402 -2.9042544][-1.0910535 -0.9021759 -0.97960138 -0.69389915 -0.0021400452 1.2292705 2.7070909 3.7707844 4.1320467 3.3254986 1.7350383 0.078631878 -1.3990211 -2.3712053 -2.6262641][-1.6519756 -1.283839 -1.1204419 -0.97887349 -0.465348 0.78830719 2.2530251 3.5161672 4.0126343 3.3258128 1.9225245 0.26049805 -1.2261858 -2.223736 -2.3972132][-2.0804553 -2.1397195 -2.2506752 -2.0291772 -1.4205699 -0.58628345 0.35545778 1.5116167 2.1445818 1.8888693 0.9309454 -0.44747376 -1.6735635 -2.54451 -2.7271466][-3.1389775 -2.9369664 -2.9804871 -3.1754346 -3.020103 -2.4392247 -1.6279531 -0.82748818 -0.35979605 -0.30378008 -0.67442989 -1.4433167 -2.3002095 -2.9256806 -2.7842689][-4.2145181 -4.0828309 -4.1690087 -4.235384 -4.1980906 -4.1708307 -3.86838 -3.2242451 -2.6201921 -2.3370237 -2.4800322 -2.9023666 -3.2576704 -3.4870918 -3.1366582][-5.1695776 -5.2095332 -5.37021 -5.6520433 -5.8232841 -5.8389015 -5.6573248 -5.373404 -4.9854212 -4.5361342 -4.2047381 -4.2793193 -4.386569 -4.4173961 -3.9952707][-6.1691909 -6.2069159 -6.245141 -6.3877954 -6.6351528 -6.7871504 -6.8309579 -6.7006021 -6.4016471 -5.9581761 -5.5392766 -5.4046078 -5.2534738 -5.0886765 -4.5678525][-6.3294353 -6.3313479 -6.3038597 -6.4915557 -6.6730413 -6.6970491 -6.74702 -6.7839403 -6.7240505 -6.362679 -5.9700017 -5.70715 -5.495512 -5.249918 -4.8098354]]...]
INFO - root - 2017-12-16 08:24:22.879172: step 59610, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 21h:21m:26s remains)
INFO - root - 2017-12-16 08:24:25.708193: step 59620, loss = 0.28, batch loss = 0.22 (27.9 examples/sec; 0.286 sec/batch; 21h:42m:08s remains)
INFO - root - 2017-12-16 08:24:28.613623: step 59630, loss = 0.19, batch loss = 0.13 (27.3 examples/sec; 0.293 sec/batch; 22h:13m:51s remains)
INFO - root - 2017-12-16 08:24:31.419763: step 59640, loss = 0.23, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 21h:34m:27s remains)
INFO - root - 2017-12-16 08:24:34.299318: step 59650, loss = 0.31, batch loss = 0.25 (27.9 examples/sec; 0.286 sec/batch; 21h:42m:11s remains)
INFO - root - 2017-12-16 08:24:37.164613: step 59660, loss = 0.25, batch loss = 0.19 (26.5 examples/sec; 0.302 sec/batch; 22h:52m:16s remains)
INFO - root - 2017-12-16 08:24:39.999409: step 59670, loss = 0.27, batch loss = 0.21 (27.6 examples/sec; 0.289 sec/batch; 21h:56m:07s remains)
INFO - root - 2017-12-16 08:24:42.833763: step 59680, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 21h:39m:23s remains)
INFO - root - 2017-12-16 08:24:45.692241: step 59690, loss = 0.25, batch loss = 0.19 (27.5 examples/sec; 0.291 sec/batch; 22h:04m:06s remains)
INFO - root - 2017-12-16 08:24:48.545954: step 59700, loss = 0.25, batch loss = 0.19 (27.6 examples/sec; 0.290 sec/batch; 21h:56m:33s remains)
2017-12-16 08:24:49.073753: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.829289 -5.059166 -3.9450214 -2.9478669 -2.1096761 -1.802721 -1.695034 -2.0131836 -2.9171579 -3.74242 -4.2769628 -4.6479597 -4.6320858 -4.1850443 -3.554327][-5.1143432 -4.3749013 -3.4045312 -2.4619827 -1.6858914 -1.3260198 -1.273288 -1.6826808 -2.5126524 -3.3530293 -4.0885963 -4.3968086 -4.4028492 -4.1581006 -3.5768387][-4.1911573 -3.516573 -2.7193804 -1.8937478 -1.2930942 -1.032588 -0.98746014 -1.3737302 -2.2827318 -3.3364391 -4.1443405 -4.7434025 -4.8495531 -4.5377226 -3.9068816][-3.0115478 -2.5476778 -1.8465285 -1.3156707 -0.85143471 -0.60684204 -0.56571364 -0.94754028 -1.8920653 -3.0022471 -4.0143189 -4.6474767 -4.8534918 -4.4889636 -3.88706][-1.9680147 -1.6494513 -1.0884998 -0.74273229 -0.10793924 0.46479559 0.74367476 0.53144217 -0.50594115 -1.8757594 -3.2644746 -4.274087 -4.6841669 -4.5740342 -3.9513667][-1.6168606 -1.4791629 -1.0679889 -0.63846278 0.28899002 1.2916865 2.017837 1.9961619 1.1130137 -0.54986787 -2.3261256 -3.7857571 -4.6867905 -4.8954043 -4.444479][-1.6275749 -1.6491683 -1.3195128 -0.78711414 0.32472372 1.6533885 2.6416111 2.7529745 1.8397264 0.076556683 -2.0095003 -3.9119847 -5.0626769 -5.4538755 -5.1420736][-1.9924994 -1.9352591 -1.6137271 -0.942286 0.43756342 1.7693958 2.6862607 2.9588976 2.1420183 0.24827147 -2.054718 -3.9951324 -5.3191166 -5.8561516 -5.5065074][-2.3222263 -2.3424284 -1.9949989 -1.4459236 -0.087962151 1.3245301 2.4081554 2.7209082 1.8873281 0.20326328 -2.0206692 -4.1396556 -5.6613441 -6.2654266 -5.9413319][-2.467279 -2.5918243 -2.3467941 -1.8748302 -0.88036585 0.3177762 1.3729014 1.8004112 1.1706414 -0.40894175 -2.5272751 -4.4977384 -5.9684834 -6.6576881 -6.356905][-2.2045619 -2.5790386 -2.8109734 -2.5927427 -1.7343564 -0.59876919 0.32453203 0.72179556 0.26192188 -1.2110226 -3.1129553 -4.9666805 -6.2738333 -6.7980223 -6.5112119][-2.2868466 -2.7994769 -3.0686028 -2.9689469 -2.3106937 -1.327039 -0.55274081 -0.24611521 -0.7343502 -2.0041547 -3.7157655 -5.3862748 -6.4560614 -6.7030678 -6.2387981][-2.2511618 -2.928395 -3.2785902 -3.1900017 -2.4822111 -1.6434491 -1.0852559 -0.78032374 -1.3621304 -2.5381646 -4.0401258 -5.4242635 -6.1366606 -6.2737913 -5.8273697][-2.2346306 -2.9194942 -3.3537843 -3.2305813 -2.643611 -1.8259728 -1.2656891 -1.2421007 -1.8097928 -2.8512471 -4.2003188 -5.2691536 -5.8333611 -5.8223538 -5.2366061][-2.7848613 -3.377635 -3.6019959 -3.3193953 -2.667978 -1.8388782 -1.3508093 -1.3722761 -1.9115257 -3.0256927 -4.2028461 -5.1334562 -5.5738416 -5.3707671 -4.6728611]]...]
INFO - root - 2017-12-16 08:24:51.893946: step 59710, loss = 0.27, batch loss = 0.21 (29.4 examples/sec; 0.272 sec/batch; 20h:35m:42s remains)
INFO - root - 2017-12-16 08:24:54.774534: step 59720, loss = 0.25, batch loss = 0.19 (28.9 examples/sec; 0.277 sec/batch; 21h:00m:08s remains)
INFO - root - 2017-12-16 08:24:57.627289: step 59730, loss = 0.22, batch loss = 0.16 (27.7 examples/sec; 0.289 sec/batch; 21h:53m:52s remains)
INFO - root - 2017-12-16 08:25:00.487499: step 59740, loss = 0.32, batch loss = 0.26 (27.3 examples/sec; 0.293 sec/batch; 22h:14m:01s remains)
INFO - root - 2017-12-16 08:25:03.325392: step 59750, loss = 0.27, batch loss = 0.21 (27.6 examples/sec; 0.290 sec/batch; 21h:57m:10s remains)
INFO - root - 2017-12-16 08:25:06.179295: step 59760, loss = 0.24, batch loss = 0.19 (27.8 examples/sec; 0.288 sec/batch; 21h:48m:06s remains)
INFO - root - 2017-12-16 08:25:09.056638: step 59770, loss = 0.24, batch loss = 0.18 (28.5 examples/sec; 0.281 sec/batch; 21h:17m:46s remains)
INFO - root - 2017-12-16 08:25:11.903103: step 59780, loss = 0.28, batch loss = 0.22 (29.4 examples/sec; 0.273 sec/batch; 20h:38m:45s remains)
INFO - root - 2017-12-16 08:25:14.769766: step 59790, loss = 0.26, batch loss = 0.20 (29.2 examples/sec; 0.274 sec/batch; 20h:43m:50s remains)
INFO - root - 2017-12-16 08:25:17.594656: step 59800, loss = 0.26, batch loss = 0.20 (28.2 examples/sec; 0.284 sec/batch; 21h:29m:46s remains)
2017-12-16 08:25:18.097888: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9352946 -4.0916753 -4.2807703 -4.3989739 -4.5191751 -4.3447094 -4.0833173 -3.844995 -3.5949945 -3.5035267 -3.4862304 -3.3628755 -3.1562018 -3.0707586 -3.2114158][-3.0251784 -3.3514924 -3.794724 -4.1375117 -4.2926955 -4.3571916 -4.2181888 -4.0983744 -4.0504813 -4.0653291 -4.05337 -3.9239271 -3.6671352 -3.472394 -3.4917953][-2.197777 -2.603653 -3.1048179 -3.5741234 -3.8379085 -4.08732 -4.0782394 -4.1795449 -4.2395668 -4.3795156 -4.46669 -4.2768111 -3.9832182 -3.7331085 -3.5723872][-1.4831331 -2.1928825 -2.8486848 -3.1474526 -3.2315121 -3.312434 -3.3142514 -3.3164334 -3.29597 -3.6899421 -4.1110077 -4.0894537 -3.8906012 -3.5913177 -3.4844766][-1.2053068 -1.9057732 -2.4626913 -2.7531464 -2.6782055 -2.4879732 -2.1838636 -1.9865282 -1.9138703 -2.3876438 -2.8462608 -3.162251 -3.3273129 -3.4080474 -3.4797227][-1.3380268 -1.8325207 -2.2585371 -2.3511887 -1.8848579 -1.321306 -0.71843672 -0.20361376 0.05755949 -0.38762426 -1.1453621 -1.9115117 -2.5203104 -2.9002981 -3.0852814][-1.8586853 -2.3381627 -2.5153735 -1.874408 -0.78698564 0.29009056 1.3206282 2.0615206 2.4311833 1.8223152 0.66930628 -0.40514517 -1.4586904 -2.2005515 -2.602596][-2.6060174 -2.9718461 -2.9006534 -2.1452918 -0.815341 0.82525492 2.4478211 3.2718706 3.4839282 2.8393908 1.6644845 0.35079288 -1.0456223 -1.9293454 -2.444849][-3.7994852 -3.9924531 -3.7891808 -2.967114 -1.7104776 -0.033964157 1.7794061 2.7190137 3.0065007 2.3289289 1.1221609 -0.33252096 -1.6482365 -2.4615707 -2.8041308][-4.57584 -4.993701 -4.98959 -4.1547403 -2.8016541 -1.3061163 0.26645756 1.1821127 1.53056 1.0605774 -0.064484596 -1.3811276 -2.4474645 -3.0187111 -3.1252224][-5.2253785 -5.5299635 -5.5573878 -5.0624094 -3.9379425 -2.3983085 -0.94314718 -0.42072153 -0.43298936 -0.93427253 -1.755476 -2.6969533 -3.5260715 -3.7828763 -3.5454602][-5.4243197 -5.7344756 -5.9769869 -5.6294565 -4.8112836 -3.8327155 -2.724016 -2.1385529 -2.038049 -2.4436855 -3.1683948 -3.9271936 -4.3984666 -4.2472763 -3.6556323][-4.9368982 -5.2851634 -5.638906 -5.3230677 -4.5050964 -3.8111522 -3.0788839 -2.6815252 -2.5932331 -2.9177926 -3.4768665 -3.9411807 -4.1958656 -3.9613719 -3.3377564][-4.4598951 -4.4886422 -4.5982647 -4.258925 -3.5801079 -2.6587567 -1.8733089 -1.8058543 -2.0043993 -2.3737488 -2.9352226 -3.5183158 -3.8067851 -3.5923018 -2.9372213][-3.9929669 -3.8911092 -3.780407 -3.3267896 -2.5663154 -1.8069947 -1.3195603 -1.2125387 -1.2841892 -1.7718916 -2.5121925 -2.9448895 -3.1263227 -3.0359049 -2.5961466]]...]
INFO - root - 2017-12-16 08:25:20.904403: step 59810, loss = 0.27, batch loss = 0.21 (28.4 examples/sec; 0.281 sec/batch; 21h:18m:54s remains)
INFO - root - 2017-12-16 08:25:23.750904: step 59820, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.283 sec/batch; 21h:28m:13s remains)
INFO - root - 2017-12-16 08:25:26.590689: step 59830, loss = 0.27, batch loss = 0.22 (28.5 examples/sec; 0.281 sec/batch; 21h:15m:18s remains)
INFO - root - 2017-12-16 08:25:29.418621: step 59840, loss = 0.45, batch loss = 0.39 (28.3 examples/sec; 0.283 sec/batch; 21h:26m:27s remains)
INFO - root - 2017-12-16 08:25:32.258178: step 59850, loss = 0.22, batch loss = 0.16 (28.2 examples/sec; 0.284 sec/batch; 21h:31m:20s remains)
INFO - root - 2017-12-16 08:25:35.121609: step 59860, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.284 sec/batch; 21h:32m:08s remains)
INFO - root - 2017-12-16 08:25:37.967834: step 59870, loss = 0.32, batch loss = 0.26 (29.0 examples/sec; 0.276 sec/batch; 20h:52m:18s remains)
INFO - root - 2017-12-16 08:25:40.838846: step 59880, loss = 0.32, batch loss = 0.26 (29.2 examples/sec; 0.274 sec/batch; 20h:45m:42s remains)
INFO - root - 2017-12-16 08:25:43.702758: step 59890, loss = 0.28, batch loss = 0.22 (27.3 examples/sec; 0.293 sec/batch; 22h:12m:17s remains)
INFO - root - 2017-12-16 08:25:46.565229: step 59900, loss = 0.26, batch loss = 0.20 (28.2 examples/sec; 0.284 sec/batch; 21h:30m:34s remains)
2017-12-16 08:25:47.071666: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6556702 -4.52453 -5.4000835 -6.2390332 -6.8472996 -7.1946182 -7.4652176 -8.1749392 -8.7869549 -9.0738754 -9.36809 -9.5606852 -9.2424145 -8.5039339 -7.813611][-4.5160956 -5.6191807 -6.566474 -7.478013 -8.2379074 -8.8227348 -8.9753418 -9.1191816 -9.373188 -9.9669828 -10.440398 -10.358377 -10.138557 -9.4973087 -8.5123043][-5.1630859 -6.4199286 -7.4408178 -8.20393 -8.47152 -8.6044884 -8.50597 -8.5177374 -8.7109814 -8.8909645 -9.2100735 -9.5528526 -9.676919 -9.0480051 -8.2217112][-5.4132075 -6.7668052 -7.9014463 -8.5116959 -8.1564865 -7.4290771 -6.5226064 -5.7730803 -5.4370193 -5.9864793 -6.749052 -7.3203173 -7.7526603 -7.5585823 -6.8763456][-5.4069924 -6.3985791 -7.1276531 -7.1960263 -6.5446162 -5.1707721 -3.3798184 -1.9887347 -1.638068 -1.9635863 -2.9064896 -3.9512029 -4.7479625 -5.0213675 -4.9566464][-4.8226094 -5.5994034 -5.8300033 -5.4193063 -3.7622738 -1.5890253 0.3750391 2.0744519 2.8056264 2.0181985 0.31566095 -0.81457472 -1.2628224 -1.8728793 -2.3213396][-4.1336751 -4.5177445 -4.4671288 -3.6101403 -1.7884078 0.64165115 3.2515578 5.1371918 5.3724174 4.5212049 3.1980672 1.7294111 0.67366934 0.029152393 -0.40473461][-3.8357408 -3.956001 -3.6471329 -2.5561376 -0.50239635 2.1990337 4.4545727 5.8774519 6.0830803 5.2337046 3.9083338 2.8261395 2.0673771 1.0271215 0.28302765][-3.515244 -3.5430832 -3.1185472 -1.9999266 -0.17140913 1.854835 3.7736168 4.8646536 4.8037653 4.1884861 3.2859855 2.1884127 1.2289543 0.47561026 -0.017098904][-3.3325508 -3.4887009 -3.2851067 -2.3979197 -0.96064186 0.58775473 1.7101741 2.1952057 2.276051 1.9718604 1.3536372 0.58169365 -0.048095703 -0.66526604 -1.0577531][-3.7803726 -3.9075983 -3.8769999 -3.4359217 -2.4706764 -1.5734901 -0.779515 -0.4749701 -0.56539989 -0.99881816 -1.5776029 -1.9204409 -2.3854637 -2.8244882 -3.2096827][-4.219348 -4.4288206 -4.3858032 -4.3393354 -4.2088246 -3.98323 -3.690846 -3.5908282 -3.6862614 -4.1534123 -4.5689244 -4.89965 -5.0940456 -5.1258469 -5.2343535][-4.7671151 -5.0266776 -5.1803427 -5.347167 -5.388958 -5.5747128 -5.7026539 -5.8626127 -5.9224949 -5.988122 -6.1752868 -6.1967931 -6.2846661 -6.2464967 -5.9965563][-4.7016144 -5.0642676 -5.3780265 -5.6386929 -5.823298 -6.0689363 -6.2854981 -6.446496 -6.5723343 -6.6923962 -6.750968 -6.9619513 -7.0573993 -6.7917614 -6.6187906][-4.6516213 -4.7937679 -4.9607277 -5.3228493 -5.6307125 -5.9651356 -6.2858377 -6.4874358 -6.4946632 -6.4616289 -6.5670738 -6.55567 -6.4262719 -6.40296 -6.2720928]]...]
INFO - root - 2017-12-16 08:25:49.887798: step 59910, loss = 0.30, batch loss = 0.24 (28.2 examples/sec; 0.284 sec/batch; 21h:29m:54s remains)
INFO - root - 2017-12-16 08:25:52.735702: step 59920, loss = 0.31, batch loss = 0.25 (27.9 examples/sec; 0.286 sec/batch; 21h:40m:41s remains)
INFO - root - 2017-12-16 08:25:55.568460: step 59930, loss = 0.20, batch loss = 0.14 (28.0 examples/sec; 0.285 sec/batch; 21h:35m:56s remains)
INFO - root - 2017-12-16 08:25:58.421505: step 59940, loss = 0.29, batch loss = 0.24 (28.7 examples/sec; 0.279 sec/batch; 21h:07m:54s remains)
INFO - root - 2017-12-16 08:26:01.311735: step 59950, loss = 0.24, batch loss = 0.19 (27.2 examples/sec; 0.294 sec/batch; 22h:14m:44s remains)
INFO - root - 2017-12-16 08:26:04.183020: step 59960, loss = 0.33, batch loss = 0.27 (28.9 examples/sec; 0.276 sec/batch; 20h:55m:25s remains)
INFO - root - 2017-12-16 08:26:07.011310: step 59970, loss = 0.29, batch loss = 0.23 (26.9 examples/sec; 0.297 sec/batch; 22h:30m:59s remains)
INFO - root - 2017-12-16 08:26:09.931987: step 59980, loss = 0.41, batch loss = 0.35 (27.8 examples/sec; 0.288 sec/batch; 21h:47m:27s remains)
INFO - root - 2017-12-16 08:26:12.794089: step 59990, loss = 0.26, batch loss = 0.20 (27.6 examples/sec; 0.290 sec/batch; 21h:56m:42s remains)
INFO - root - 2017-12-16 08:26:15.605831: step 60000, loss = 0.25, batch loss = 0.20 (27.3 examples/sec; 0.293 sec/batch; 22h:10m:27s remains)
2017-12-16 08:26:16.162874: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4239106 -5.468153 -5.7183371 -6.1294184 -6.4776144 -6.6092963 -6.6041827 -6.4580441 -6.272995 -5.9258451 -5.8060904 -5.9523592 -6.3288951 -6.2499027 -6.1480675][-4.5243325 -4.7162457 -4.7356763 -5.159667 -5.6051769 -5.8879023 -5.78627 -5.7636919 -5.8142061 -5.6606326 -5.5589409 -5.6093969 -5.6705551 -5.4673357 -5.3841357][-3.2486486 -3.4843152 -3.6700227 -4.2741184 -4.4044175 -4.7743611 -4.763576 -4.583117 -4.5993781 -4.5230942 -4.6763062 -4.772788 -4.7529726 -4.3654771 -4.1203809][-1.9807882 -2.3849063 -2.7056236 -3.2004354 -3.2550688 -3.3027844 -3.0687675 -3.0804284 -3.1874223 -3.2408454 -3.5041766 -3.6061492 -3.4822576 -3.106925 -3.0994344][-1.3037658 -1.6480381 -1.8631287 -2.2859521 -2.2340262 -1.7716057 -1.2000599 -0.86667085 -1.0267134 -1.4157619 -1.966469 -2.3496571 -2.3443997 -2.0395107 -1.9067433][-1.5393744 -1.5950227 -1.548739 -1.2613561 -0.46905351 0.11933279 0.94775867 1.4655399 1.3580952 0.705708 -0.1693058 -0.81351256 -1.1277978 -0.97621226 -0.96163011][-2.2342663 -1.9276383 -1.590075 -0.87145376 0.53637886 1.7568526 2.8395529 3.294786 3.0638204 2.1734972 1.0036445 0.048012733 -0.43721819 -0.42327309 -0.41196346][-2.9179444 -2.4790573 -1.7575345 -0.74645066 0.86533785 2.5031729 3.9252481 4.4880934 4.200778 3.1576338 1.8057156 0.56344032 -0.34786034 -0.47264719 -0.53896356][-3.9430346 -3.2242618 -2.2758446 -0.97179127 0.52722692 2.1848574 3.7423038 4.1546917 3.976615 2.97964 1.605989 0.4543829 -0.4486053 -0.72997618 -0.83825827][-4.5035925 -3.8863895 -3.1693215 -1.9821601 -0.54024315 0.76009369 1.7482939 2.1102824 2.0151896 1.1959662 0.23242474 -0.75966668 -1.413986 -1.5986042 -1.5405715][-4.8747129 -4.2667823 -3.8201649 -3.2348449 -2.458704 -1.5318909 -0.85484338 -0.67007732 -0.59228587 -0.88042617 -1.4822285 -2.0388265 -2.444072 -2.6231551 -2.5046773][-5.2625833 -4.7920141 -4.6192527 -4.2868013 -3.8466692 -3.4711123 -3.3209746 -3.2621188 -3.1167011 -2.9392583 -2.9816623 -3.1167231 -3.2387779 -3.2864645 -3.3106675][-5.2830071 -5.0747013 -5.1569262 -5.2598033 -5.1220121 -4.9714355 -4.969347 -5.0352426 -4.8503351 -4.4813251 -4.071084 -3.8070967 -3.6432424 -3.7295213 -3.6346655][-5.1342111 -4.979157 -4.9912877 -5.2394443 -5.4448953 -5.5648608 -5.6650538 -5.5732951 -5.3690147 -5.02002 -4.5384693 -4.1082973 -3.8797908 -3.8786068 -3.9175248][-4.71155 -4.5430942 -4.4751568 -4.6267009 -4.7456546 -4.9882474 -5.24047 -5.2544093 -5.1361837 -4.8529696 -4.5608058 -4.1603122 -3.9216096 -3.9720132 -4.0743747]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 08:26:19.682393: step 60010, loss = 0.23, batch loss = 0.17 (27.6 examples/sec; 0.290 sec/batch; 21h:57m:22s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 08:26:22.553810: step 60020, loss = 0.31, batch loss = 0.25 (28.7 examples/sec; 0.279 sec/batch; 21h:06m:23s remains)
INFO - root - 2017-12-16 08:26:25.377395: step 60030, loss = 0.24, batch loss = 0.18 (28.7 examples/sec; 0.279 sec/batch; 21h:05m:53s remains)
INFO - root - 2017-12-16 08:26:28.243825: step 60040, loss = 0.34, batch loss = 0.29 (28.4 examples/sec; 0.282 sec/batch; 21h:19m:46s remains)
INFO - root - 2017-12-16 08:26:31.130898: step 60050, loss = 0.35, batch loss = 0.29 (28.7 examples/sec; 0.279 sec/batch; 21h:04m:49s remains)
INFO - root - 2017-12-16 08:26:33.997677: step 60060, loss = 0.28, batch loss = 0.22 (28.7 examples/sec; 0.279 sec/batch; 21h:04m:47s remains)
INFO - root - 2017-12-16 08:26:36.830535: step 60070, loss = 0.22, batch loss = 0.16 (27.7 examples/sec; 0.289 sec/batch; 21h:53m:22s remains)
INFO - root - 2017-12-16 08:26:39.689535: step 60080, loss = 0.29, batch loss = 0.23 (27.8 examples/sec; 0.287 sec/batch; 21h:44m:42s remains)
INFO - root - 2017-12-16 08:26:42.545038: step 60090, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 21h:34m:19s remains)
INFO - root - 2017-12-16 08:26:45.342688: step 60100, loss = 0.32, batch loss = 0.26 (28.7 examples/sec; 0.279 sec/batch; 21h:06m:21s remains)
2017-12-16 08:26:45.871854: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6516714 -4.7684956 -4.9292426 -4.8988786 -4.7661242 -4.85552 -4.8192754 -4.8017192 -4.8918581 -4.9299922 -5.0730119 -4.7091203 -4.2213979 -3.8786552 -3.4713926][-4.2010736 -4.1617942 -4.2029662 -4.1784139 -3.9066851 -4.0905271 -4.2925105 -4.5094576 -4.57562 -4.6041856 -4.7239957 -4.5766335 -4.1350179 -3.2737672 -2.4103792][-3.3003874 -3.0919409 -3.0935698 -3.0396297 -2.8526435 -2.9552855 -3.1824634 -3.50002 -3.6682851 -4.2530894 -4.6342845 -4.4637208 -3.924372 -3.2492666 -2.6153798][-1.7439697 -1.3342927 -1.3593745 -1.3798406 -1.3898754 -1.5809002 -1.978713 -2.6778119 -3.1852496 -3.812984 -4.2917919 -4.5720563 -4.2013431 -3.5706093 -2.8108439][-0.34152508 0.18313789 0.301301 0.43212175 0.30177832 -0.0168643 -0.45670676 -1.0905588 -1.7135856 -2.8663049 -3.8688028 -4.3781857 -4.1527185 -3.5508714 -2.8386078][0.53125143 1.1254206 1.2462363 1.7491522 1.9661131 1.8132405 1.4737363 0.77572489 0.050003529 -1.2051787 -2.4141338 -3.4866085 -3.9800665 -3.8933294 -3.4351172][0.6872735 1.3935232 1.6459603 2.1919713 2.4948974 2.6846566 2.7475281 2.1925554 1.5357141 0.1464262 -1.2834322 -2.7434983 -3.731225 -4.0014625 -3.8246169][0.37106752 0.90644217 1.0907016 1.7148833 2.2837005 2.4167886 2.4660316 2.3043351 1.943356 0.52793646 -1.1933811 -2.731585 -3.8246598 -4.4275208 -4.699223][-0.5977726 -0.22776365 0.011466026 0.46051168 0.96644974 1.5283308 1.9122334 1.5863338 1.1452703 -0.011883736 -1.6121228 -3.2441368 -4.5346665 -4.9047585 -4.8786755][-1.7017288 -1.7160237 -1.8167839 -1.2806902 -0.77598357 -0.23184776 0.33009052 0.27711058 -0.020859718 -1.0411465 -2.4820096 -4.18749 -5.5864086 -5.779686 -5.5313964][-2.6275573 -2.7090218 -2.9035523 -2.820693 -2.6138449 -2.1444538 -1.5874758 -1.5856061 -1.945399 -2.9816966 -4.3211594 -5.6210146 -6.5886021 -6.7844915 -6.2880912][-2.8358169 -3.2778697 -3.7783604 -4.009511 -3.984926 -3.8039942 -3.492954 -3.58484 -3.8450775 -4.5505004 -5.63387 -6.7991381 -7.4545174 -7.1095304 -6.2936697][-3.1364789 -3.4512181 -4.0731921 -4.5884957 -4.7721915 -4.6596956 -4.6524982 -4.6921058 -4.7469425 -5.2574582 -6.0713196 -6.6052017 -6.8059044 -6.3929596 -5.6992354][-3.5116873 -3.5247617 -4.1402025 -4.6127725 -5.0426574 -4.8675966 -4.8657541 -5.0375671 -5.2293091 -5.3719835 -5.707262 -6.0189304 -6.2149096 -5.5999737 -4.8774071][-4.3122268 -3.9374833 -4.1699967 -4.6208739 -5.0824738 -5.0461364 -5.08027 -5.0768495 -5.0893574 -5.0849094 -5.2281718 -5.295248 -5.2323074 -4.8472285 -4.3837376]]...]
INFO - root - 2017-12-16 08:26:48.738418: step 60110, loss = 0.33, batch loss = 0.27 (27.8 examples/sec; 0.287 sec/batch; 21h:44m:53s remains)
INFO - root - 2017-12-16 08:26:51.560276: step 60120, loss = 0.23, batch loss = 0.17 (27.9 examples/sec; 0.287 sec/batch; 21h:42m:33s remains)
INFO - root - 2017-12-16 08:26:54.388446: step 60130, loss = 0.23, batch loss = 0.18 (28.3 examples/sec; 0.282 sec/batch; 21h:21m:24s remains)
INFO - root - 2017-12-16 08:26:57.275460: step 60140, loss = 0.23, batch loss = 0.17 (27.3 examples/sec; 0.293 sec/batch; 22h:08m:54s remains)
INFO - root - 2017-12-16 08:27:00.176011: step 60150, loss = 0.28, batch loss = 0.22 (27.5 examples/sec; 0.290 sec/batch; 21h:58m:24s remains)
INFO - root - 2017-12-16 08:27:02.989202: step 60160, loss = 0.39, batch loss = 0.33 (28.4 examples/sec; 0.282 sec/batch; 21h:18m:13s remains)
INFO - root - 2017-12-16 08:27:05.882444: step 60170, loss = 0.23, batch loss = 0.18 (27.2 examples/sec; 0.294 sec/batch; 22h:13m:58s remains)
INFO - root - 2017-12-16 08:27:08.736976: step 60180, loss = 0.25, batch loss = 0.20 (28.2 examples/sec; 0.284 sec/batch; 21h:29m:08s remains)
INFO - root - 2017-12-16 08:27:11.569492: step 60190, loss = 0.23, batch loss = 0.17 (29.2 examples/sec; 0.274 sec/batch; 20h:41m:46s remains)
INFO - root - 2017-12-16 08:27:14.407958: step 60200, loss = 0.26, batch loss = 0.21 (27.3 examples/sec; 0.293 sec/batch; 22h:11m:45s remains)
2017-12-16 08:27:14.957431: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0942516 -3.7946358 -3.4691541 -3.2534409 -3.1555839 -3.2190897 -3.3858232 -3.4627101 -3.5510368 -3.50551 -3.4493544 -3.4427605 -3.3378057 -3.4354365 -3.576592][-3.0178115 -2.8438313 -2.7267864 -2.7852383 -3.0019436 -3.2603457 -3.5577884 -3.7771509 -3.84507 -3.7065179 -3.6586993 -3.6470542 -3.6437006 -3.7939124 -3.9269915][-1.8170853 -1.7744331 -1.8235934 -2.1488051 -2.6222649 -3.1659007 -3.6777287 -3.9609208 -4.02701 -3.9893072 -4.0503607 -4.1275125 -4.2838216 -4.42366 -4.5040789][-0.91873026 -1.2166765 -1.5185215 -1.8794487 -2.3206966 -2.8616419 -3.3425202 -3.5380764 -3.6025252 -3.7189627 -3.7914898 -4.041986 -4.4109793 -4.7665381 -5.0111217][-0.91729522 -1.2517054 -1.4927564 -1.7221053 -1.9059026 -2.1878221 -2.3967605 -2.4026256 -2.4687788 -2.8099031 -3.1181326 -3.5922253 -3.9539526 -4.3308482 -4.5985708][-1.1917913 -1.3648062 -1.4212773 -1.3388512 -1.1178894 -1.1402967 -1.1741731 -1.0167985 -1.0507424 -1.4138205 -1.9735456 -2.4911635 -2.9598131 -3.6383023 -3.9653113][-1.5255561 -1.5185685 -1.3509107 -0.9105773 -0.2217803 0.18437862 0.55644035 0.8344512 0.90321493 0.36971092 -0.47791338 -1.1488235 -1.8161867 -2.4474385 -2.8672528][-2.1348441 -2.0013633 -1.6792684 -1.0164142 -0.045436859 0.76462412 1.5279689 2.0572195 2.3955321 1.8695583 0.96638584 0.12425041 -0.68342733 -1.5233853 -2.101063][-2.6021147 -2.3824193 -1.9484274 -1.2499762 -0.28375912 0.68941355 1.4634366 1.9391718 2.1526141 1.6344109 0.81553364 0.033251286 -0.65381956 -1.2799127 -1.7097864][-2.8846726 -2.5949569 -2.1244056 -1.4603856 -0.63366818 0.012083054 0.40522242 0.64545488 0.6902895 0.16162491 -0.58275604 -1.2585447 -1.6899998 -1.9420638 -2.0718896][-2.9779477 -2.5699551 -2.1551867 -1.6731708 -1.0972509 -0.75097775 -0.70183516 -0.97510862 -1.4494078 -2.1288671 -2.7484818 -3.0938272 -3.2188275 -3.1404114 -2.8526006][-3.1257212 -2.6654768 -2.3374209 -1.9337614 -1.5974052 -1.7634451 -2.1470251 -2.7634065 -3.4137068 -4.1332045 -4.7182956 -4.9262524 -4.7639294 -4.2907386 -3.6813049][-3.106859 -2.6852658 -2.4130552 -2.1720169 -2.1981633 -2.6269851 -3.3142915 -4.1296492 -4.8891878 -5.590405 -6.0229778 -5.8775606 -5.4451365 -4.9286466 -4.3748856][-2.9045091 -2.3917117 -2.0642552 -1.9162359 -2.1206286 -2.6167188 -3.4177055 -4.3654203 -5.1073642 -5.7388954 -6.076993 -6.0098081 -5.6808662 -5.0367718 -4.4494724][-2.616529 -2.0781455 -1.6969888 -1.5078259 -1.665468 -2.1676683 -2.9009323 -3.7085414 -4.4254427 -5.1109433 -5.443491 -5.2729464 -4.9918704 -4.6617026 -4.3076105]]...]
INFO - root - 2017-12-16 08:27:17.765657: step 60210, loss = 0.30, batch loss = 0.24 (28.1 examples/sec; 0.285 sec/batch; 21h:32m:00s remains)
INFO - root - 2017-12-16 08:27:20.597890: step 60220, loss = 0.28, batch loss = 0.22 (29.2 examples/sec; 0.274 sec/batch; 20h:44m:23s remains)
INFO - root - 2017-12-16 08:27:23.457531: step 60230, loss = 0.30, batch loss = 0.24 (27.5 examples/sec; 0.291 sec/batch; 22h:00m:02s remains)
INFO - root - 2017-12-16 08:27:26.315263: step 60240, loss = 0.21, batch loss = 0.15 (27.6 examples/sec; 0.290 sec/batch; 21h:55m:00s remains)
INFO - root - 2017-12-16 08:27:29.194080: step 60250, loss = 0.23, batch loss = 0.17 (28.8 examples/sec; 0.278 sec/batch; 21h:00m:49s remains)
INFO - root - 2017-12-16 08:27:32.024817: step 60260, loss = 0.28, batch loss = 0.22 (28.7 examples/sec; 0.279 sec/batch; 21h:05m:19s remains)
INFO - root - 2017-12-16 08:27:34.852546: step 60270, loss = 0.38, batch loss = 0.32 (26.8 examples/sec; 0.298 sec/batch; 22h:33m:09s remains)
INFO - root - 2017-12-16 08:27:37.709293: step 60280, loss = 0.29, batch loss = 0.24 (27.8 examples/sec; 0.288 sec/batch; 21h:45m:08s remains)
INFO - root - 2017-12-16 08:27:40.559842: step 60290, loss = 0.22, batch loss = 0.16 (28.8 examples/sec; 0.278 sec/batch; 20h:59m:42s remains)
INFO - root - 2017-12-16 08:27:43.389290: step 60300, loss = 0.22, batch loss = 0.16 (28.9 examples/sec; 0.277 sec/batch; 20h:57m:35s remains)
2017-12-16 08:27:43.900342: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3246937 -4.4355273 -4.7993722 -5.1940136 -5.5854778 -5.6314745 -5.6820712 -5.6587238 -5.556345 -5.2482395 -4.9678044 -4.8557963 -4.9964752 -5.3467045 -5.4994316][-3.3883088 -3.4380927 -3.9058321 -4.3669214 -4.9143109 -5.3635378 -5.6980138 -5.533433 -5.2051454 -5.1151619 -4.9118195 -4.9216566 -5.0388255 -5.4330859 -5.6560874][-2.0770802 -2.0779908 -2.5851169 -3.2945831 -3.9542084 -4.1379991 -4.4212079 -4.6361656 -4.5828891 -4.4348621 -4.4970136 -4.8374977 -5.1276574 -5.4999027 -5.600749][-0.50779295 -0.45154786 -0.99514771 -1.6710734 -2.4638462 -2.9302502 -3.1488476 -3.0568538 -2.9495401 -3.0830817 -3.3944838 -4.0433531 -4.6262817 -5.1411576 -5.3557143][0.40271997 0.71898746 0.28048325 -0.37119532 -0.96541142 -1.0416434 -0.98910022 -0.97279453 -0.87061405 -1.1358728 -1.8070512 -2.956542 -3.8785837 -4.4588289 -4.7838087][0.64465332 1.1724072 1.1324296 0.77416849 0.3855052 0.51673126 0.85099411 1.0351558 1.0961289 0.67696047 -0.33560276 -1.7543421 -2.9933653 -3.7286623 -4.2299805][0.54223728 1.4620104 1.7691593 1.7316179 1.9715586 2.3731112 2.8202853 2.9354119 2.8470006 2.3347635 1.1676955 -0.24124146 -1.6506901 -2.8015094 -3.7553768][-0.48568439 0.29250956 0.6952343 1.0844398 1.7793937 2.7212276 3.7838478 4.1673145 4.0938129 3.4520993 2.0691233 0.44877768 -1.0686216 -2.2525837 -3.3725443][-2.8592334 -2.2695692 -1.6972785 -1.0898898 0.040820122 1.5268106 2.9281044 3.5785103 3.7553511 3.2844434 2.107831 0.56892681 -1.0445261 -2.5339627 -3.7754333][-4.9773073 -4.99004 -4.8263469 -3.84823 -2.2207956 -0.65372181 0.82866859 1.9304461 2.3651505 2.0485477 0.9036417 -0.26368046 -1.7306776 -3.2305522 -4.3787036][-7.1543493 -7.2199345 -7.1138382 -6.5388155 -5.1417518 -3.3797593 -1.7925427 -0.674422 -0.05974865 -0.1405654 -1.0813816 -2.4077258 -3.6491957 -4.8290081 -5.6296206][-8.9495945 -9.5052643 -9.6136732 -9.0030842 -7.710825 -6.1626048 -4.8090467 -3.461446 -2.8031263 -2.9208636 -3.7378454 -4.6403379 -5.5935259 -6.3581772 -6.64028][-10.26507 -10.81072 -11.028435 -10.539345 -9.5696888 -8.350647 -7.094964 -5.9218607 -5.5345926 -5.6175971 -6.1071281 -6.7645206 -7.2290721 -7.5259066 -7.2807193][-10.507887 -10.929694 -11.061508 -10.840681 -10.315234 -9.5902119 -8.9009342 -8.0967236 -7.7731619 -7.9273634 -8.315424 -8.5103951 -8.45258 -8.3033037 -7.639935][-9.8978376 -10.209358 -10.374771 -10.217529 -9.8988619 -9.4971924 -9.3007755 -9.1155567 -9.26812 -9.4651318 -9.5700378 -9.4484882 -9.06244 -8.5282545 -7.5272083]]...]
INFO - root - 2017-12-16 08:27:46.741880: step 60310, loss = 0.27, batch loss = 0.21 (27.8 examples/sec; 0.288 sec/batch; 21h:44m:47s remains)
INFO - root - 2017-12-16 08:27:49.671077: step 60320, loss = 0.25, batch loss = 0.19 (25.8 examples/sec; 0.310 sec/batch; 23h:28m:05s remains)
INFO - root - 2017-12-16 08:27:52.525768: step 60330, loss = 0.29, batch loss = 0.24 (29.1 examples/sec; 0.275 sec/batch; 20h:46m:52s remains)
INFO - root - 2017-12-16 08:27:55.403660: step 60340, loss = 0.23, batch loss = 0.17 (27.8 examples/sec; 0.288 sec/batch; 21h:44m:15s remains)
INFO - root - 2017-12-16 08:27:58.303462: step 60350, loss = 0.23, batch loss = 0.17 (27.7 examples/sec; 0.289 sec/batch; 21h:51m:18s remains)
INFO - root - 2017-12-16 08:28:01.165889: step 60360, loss = 0.30, batch loss = 0.24 (27.6 examples/sec; 0.290 sec/batch; 21h:55m:55s remains)
INFO - root - 2017-12-16 08:28:04.002997: step 60370, loss = 0.39, batch loss = 0.33 (29.0 examples/sec; 0.276 sec/batch; 20h:50m:20s remains)
INFO - root - 2017-12-16 08:28:06.905245: step 60380, loss = 0.29, batch loss = 0.23 (28.2 examples/sec; 0.284 sec/batch; 21h:28m:23s remains)
INFO - root - 2017-12-16 08:28:09.861796: step 60390, loss = 0.21, batch loss = 0.16 (27.7 examples/sec; 0.288 sec/batch; 21h:47m:26s remains)
INFO - root - 2017-12-16 08:28:12.714250: step 60400, loss = 0.36, batch loss = 0.31 (27.7 examples/sec; 0.289 sec/batch; 21h:49m:14s remains)
2017-12-16 08:28:13.223015: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0114632 -5.2715406 -5.5934405 -5.8669634 -5.9802589 -5.4126306 -4.6570382 -4.2760553 -3.8430247 -3.1979265 -2.615633 -2.3272195 -2.175148 -2.0856764 -2.1399064][-4.34747 -4.8296452 -5.2301564 -5.2529092 -5.234458 -4.91045 -4.4381833 -3.5977318 -2.9152589 -2.5401645 -2.2716484 -2.1144378 -2.0579276 -2.3028953 -2.5360315][-3.2356424 -3.885195 -4.4190989 -4.498445 -4.3009405 -3.7564216 -3.0960484 -2.550149 -2.1492944 -1.9130793 -1.8307371 -2.0207403 -2.3392313 -2.6350827 -2.77561][-2.3111594 -2.9543023 -3.5780189 -3.6108725 -3.1295066 -2.2721996 -1.3377306 -0.78520823 -0.56179309 -1.1066599 -1.7310948 -2.2974789 -2.7530184 -2.9931583 -3.2026572][-1.1651232 -1.9283352 -2.5459118 -2.4904914 -1.8818595 -0.68370104 0.65151072 1.325171 1.2192607 0.25302935 -1.0028653 -2.15861 -2.9150081 -3.2855568 -3.3562975][-0.83945131 -1.2117796 -1.3186135 -0.86499619 0.25969458 1.6477795 2.9697342 3.6242752 3.0990214 1.7767277 0.10169983 -1.7124441 -2.7960963 -3.3662839 -3.4088304][-0.98401856 -0.9601624 -0.80593514 0.17919731 2.0124578 4.1339512 5.6769924 5.7865019 4.8396358 2.862752 0.63353586 -1.3995266 -2.5326617 -3.1627383 -3.2772899][-1.7923572 -1.4851348 -0.98796749 0.79412603 2.9586945 5.324379 7.0894594 7.1294966 5.8663654 3.5161037 0.89291573 -1.2572443 -2.6199069 -3.0955625 -3.0744228][-3.7031085 -3.2005651 -2.0563626 -0.069537163 2.1853914 4.6256781 6.1087818 6.1653442 4.9521561 2.7187948 0.33756351 -1.6200163 -2.710623 -2.9980805 -2.8433328][-4.5584488 -4.6825709 -4.1427245 -2.4113777 -0.30349159 1.671175 2.6777406 2.9198775 2.062067 0.24964142 -1.6474109 -3.1285858 -3.6350837 -3.5556324 -3.2979181][-6.2565851 -6.2651486 -5.6251988 -4.7067542 -3.5906873 -2.095401 -1.0184081 -0.98257256 -1.7872708 -2.7819502 -3.9716756 -4.718812 -4.7898884 -4.3849664 -3.8820205][-7.0427451 -7.6844072 -7.8918486 -6.9737024 -5.5602636 -4.876863 -4.7283688 -4.724906 -4.839139 -5.2251749 -5.5891485 -5.9367313 -5.7697086 -5.28427 -4.8771191][-6.654973 -7.3559456 -7.774148 -7.794425 -7.5209007 -6.6745772 -6.00167 -6.2497249 -6.6524796 -6.904397 -6.6601381 -6.4567327 -6.230526 -6.1482 -5.9187951][-5.9484215 -6.2815332 -6.4871407 -6.5271988 -6.5811043 -6.5572243 -6.6488619 -6.9089003 -7.1175928 -7.0217543 -6.7968378 -6.5321879 -6.2638884 -5.9963188 -5.7875676][-5.1456032 -5.12262 -5.1717591 -5.0131011 -4.9503632 -5.08995 -5.4056864 -5.6553321 -5.8254833 -6.0749645 -6.1001458 -5.6547189 -5.1583676 -5.1441131 -5.2020292]]...]
INFO - root - 2017-12-16 08:28:16.075197: step 60410, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 21h:22m:10s remains)
INFO - root - 2017-12-16 08:28:18.942462: step 60420, loss = 0.24, batch loss = 0.18 (27.6 examples/sec; 0.290 sec/batch; 21h:53m:54s remains)
INFO - root - 2017-12-16 08:28:21.823198: step 60430, loss = 0.37, batch loss = 0.32 (28.7 examples/sec; 0.279 sec/batch; 21h:05m:24s remains)
INFO - root - 2017-12-16 08:28:24.726571: step 60440, loss = 0.32, batch loss = 0.26 (27.8 examples/sec; 0.288 sec/batch; 21h:45m:32s remains)
INFO - root - 2017-12-16 08:28:27.554793: step 60450, loss = 0.25, batch loss = 0.19 (27.7 examples/sec; 0.289 sec/batch; 21h:50m:05s remains)
INFO - root - 2017-12-16 08:28:30.482041: step 60460, loss = 0.27, batch loss = 0.21 (27.5 examples/sec; 0.291 sec/batch; 22h:00m:00s remains)
INFO - root - 2017-12-16 08:28:33.362399: step 60470, loss = 0.27, batch loss = 0.21 (27.7 examples/sec; 0.289 sec/batch; 21h:48m:57s remains)
INFO - root - 2017-12-16 08:28:36.197772: step 60480, loss = 0.52, batch loss = 0.46 (28.9 examples/sec; 0.276 sec/batch; 20h:52m:51s remains)
INFO - root - 2017-12-16 08:28:39.022726: step 60490, loss = 0.26, batch loss = 0.20 (27.8 examples/sec; 0.288 sec/batch; 21h:45m:35s remains)
INFO - root - 2017-12-16 08:28:41.865051: step 60500, loss = 0.28, batch loss = 0.23 (27.6 examples/sec; 0.290 sec/batch; 21h:52m:38s remains)
2017-12-16 08:28:42.373483: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9317045 -2.8092222 -2.654027 -2.5300791 -2.5031626 -2.6211557 -2.9501247 -3.3387744 -3.7944009 -4.0494032 -4.1814108 -4.138957 -3.9644895 -3.65693 -3.3019958][-3.0264802 -2.7878008 -2.5825567 -2.4573383 -2.4497824 -2.6381118 -2.9650855 -3.1960359 -3.5005686 -3.7541845 -3.8755937 -3.7668502 -3.4582744 -3.009836 -2.5936198][-2.8924434 -2.6594114 -2.5149395 -2.4351459 -2.4588985 -2.6064768 -2.8376343 -3.0171084 -3.1903114 -3.3076901 -3.328588 -3.1187358 -2.7724102 -2.2626307 -1.8816915][-2.6968403 -2.4259882 -2.3059051 -2.328995 -2.4245384 -2.5146375 -2.5456309 -2.51342 -2.4209368 -2.286845 -2.1327167 -1.915653 -1.6784885 -1.3009701 -1.1456048][-2.5380063 -2.1698451 -1.9927561 -1.9303958 -1.9292119 -1.9354455 -1.759408 -1.5636468 -1.2258947 -0.850296 -0.55183864 -0.37681818 -0.42336798 -0.47909713 -0.78269839][-2.3255658 -1.9081378 -1.6321785 -1.4215615 -1.274291 -1.2379549 -0.93710613 -0.44771552 0.11152029 0.57023335 0.89901447 0.99203062 0.65330315 0.1351819 -0.69023204][-2.2182806 -1.7860613 -1.471864 -1.1568162 -0.86125612 -0.48829794 0.13086891 0.72062492 1.405797 1.9930243 2.2517529 2.0500703 1.2975473 0.31761837 -0.89180374][-2.2408154 -1.8228571 -1.4135902 -0.96323156 -0.51041222 0.10682011 1.0604424 1.912281 2.7205372 3.2323732 3.3082519 2.8139629 1.7058225 0.26169538 -1.2487459][-2.0820127 -1.7792635 -1.4519296 -0.96223545 -0.47153592 0.19678211 1.1404734 2.1093984 2.9832697 3.4368992 3.4560757 2.8298359 1.5777626 -0.090858459 -1.6976533][-2.0493495 -2.0391688 -2.0014813 -1.6912546 -1.3629141 -0.79125047 0.073507309 0.96704483 1.7366023 2.0368962 2.0402284 1.4824266 0.39619255 -1.0971618 -2.5164928][-2.4765563 -2.6505375 -2.8708444 -2.87328 -2.8112967 -2.3561516 -1.6096926 -0.92801857 -0.36461115 -0.17983627 -0.10080767 -0.61410308 -1.5172329 -2.6463199 -3.6136742][-2.8131018 -3.1114306 -3.4738348 -3.6681013 -3.7879174 -3.5501518 -3.0130877 -2.5177121 -2.11773 -2.1065145 -2.1196852 -2.5611846 -3.2088559 -3.9758227 -4.5466547][-2.9503264 -3.3844619 -3.7858953 -3.9678004 -4.1327271 -4.089222 -3.8886123 -3.582001 -3.3681779 -3.5287778 -3.6392186 -4.0084047 -4.3928204 -4.9046206 -5.181685][-3.0756254 -3.4210517 -3.7482204 -3.9000793 -4.0410609 -3.9941092 -3.9567475 -3.8843639 -3.8768253 -4.1918459 -4.4694738 -4.9043159 -5.1690636 -5.4325047 -5.559742][-3.0308776 -3.1716213 -3.3445115 -3.4511232 -3.5048637 -3.4262028 -3.430263 -3.4873118 -3.6480398 -3.9863214 -4.3397059 -4.8479457 -5.1536613 -5.363893 -5.5284863]]...]
INFO - root - 2017-12-16 08:28:45.202141: step 60510, loss = 0.36, batch loss = 0.30 (28.3 examples/sec; 0.283 sec/batch; 21h:22m:04s remains)
INFO - root - 2017-12-16 08:28:48.027883: step 60520, loss = 0.30, batch loss = 0.25 (26.9 examples/sec; 0.298 sec/batch; 22h:29m:51s remains)
INFO - root - 2017-12-16 08:28:50.855747: step 60530, loss = 0.29, batch loss = 0.23 (28.8 examples/sec; 0.278 sec/batch; 21h:00m:25s remains)
INFO - root - 2017-12-16 08:28:53.718962: step 60540, loss = 0.28, batch loss = 0.22 (27.9 examples/sec; 0.286 sec/batch; 21h:38m:31s remains)
INFO - root - 2017-12-16 08:28:56.522831: step 60550, loss = 0.23, batch loss = 0.17 (29.3 examples/sec; 0.273 sec/batch; 20h:36m:02s remains)
INFO - root - 2017-12-16 08:28:59.345553: step 60560, loss = 0.19, batch loss = 0.13 (28.0 examples/sec; 0.285 sec/batch; 21h:33m:42s remains)
INFO - root - 2017-12-16 08:29:02.183774: step 60570, loss = 0.28, batch loss = 0.22 (26.4 examples/sec; 0.303 sec/batch; 22h:53m:37s remains)
INFO - root - 2017-12-16 08:29:05.016972: step 60580, loss = 0.24, batch loss = 0.18 (28.6 examples/sec; 0.280 sec/batch; 21h:09m:47s remains)
INFO - root - 2017-12-16 08:29:07.870431: step 60590, loss = 0.28, batch loss = 0.22 (28.0 examples/sec; 0.286 sec/batch; 21h:36m:40s remains)
INFO - root - 2017-12-16 08:29:10.738432: step 60600, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.286 sec/batch; 21h:33m:59s remains)
2017-12-16 08:29:11.240145: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.138051 -4.5610485 -5.0597219 -5.3217058 -5.5755482 -5.8836627 -5.9547973 -6.0025663 -6.2832026 -6.1276021 -5.619832 -4.9102015 -4.1959372 -3.7109342 -3.2728357][-3.3027923 -3.4371603 -3.8050828 -4.3168888 -5.053896 -5.7714591 -6.26952 -6.5383949 -6.7065978 -6.56848 -5.9463835 -4.9275851 -4.0717745 -3.6373928 -3.2911825][-2.4192593 -2.386833 -2.539376 -3.0079627 -3.9008622 -4.9682755 -5.8651814 -6.2353172 -6.4666662 -6.4092793 -5.9120617 -5.0936852 -4.3166256 -3.8573735 -3.5057142][-1.9134836 -1.7514865 -1.7086043 -1.8595703 -2.4824405 -3.5654721 -4.4440756 -4.9061379 -5.2415719 -5.3612733 -5.2908893 -4.84186 -4.4938049 -4.1877236 -3.7988014][-1.264137 -0.96952081 -0.85245395 -0.74123788 -1.0619395 -1.7995555 -2.5539689 -3.0277023 -3.4481518 -3.7255182 -3.9035029 -4.0390768 -4.1580958 -4.1448436 -3.90922][-0.95455313 -0.48918629 -0.065365791 0.3115263 0.36740637 0.10959911 -0.35545778 -0.81332874 -1.3595974 -2.1326382 -2.803606 -3.3448491 -3.8000441 -4.0056424 -3.8373477][-1.0674026 -0.48930788 0.17536116 0.8456111 1.2848482 1.497838 1.4948635 1.0710163 0.46093464 -0.46514177 -1.4147713 -2.3669424 -3.2300267 -3.6538227 -3.6389589][-1.3853998 -0.80009651 -0.14451265 1.0243669 1.8503709 2.2973485 2.5411658 2.3189688 1.8140144 0.86894321 -0.13265944 -1.2332785 -2.1947653 -2.9334333 -3.2518787][-2.0709906 -1.4106736 -0.87605643 0.01427412 0.91378832 1.896265 2.4941688 2.4069176 2.1138463 1.4245806 0.49550581 -0.81211495 -2.0141222 -2.8211517 -3.2294812][-2.8948967 -2.5916412 -2.2491536 -1.4628956 -0.69912171 0.18516016 0.91388035 1.4054732 1.3964052 0.78286886 -0.026171207 -1.0430694 -2.1504714 -3.037456 -3.4822698][-3.8657515 -3.86174 -3.5083182 -3.0141857 -2.588773 -1.7488177 -1.054142 -0.74114323 -0.67425513 -0.76319551 -1.2120671 -2.0279226 -2.7435918 -3.370641 -3.7163393][-4.7560468 -4.8846836 -4.8438997 -4.3426933 -3.8806314 -3.6305885 -3.3767748 -3.0183167 -2.8478081 -2.9269328 -3.1127172 -3.3225138 -3.5326343 -3.7414312 -3.8780193][-5.2107034 -5.3678827 -5.3903131 -5.2034278 -5.0306959 -4.9668155 -4.9381952 -5.0035377 -4.9684296 -4.7599306 -4.5892744 -4.4355969 -4.2409139 -4.0982027 -3.9314122][-4.968996 -4.9742284 -4.9761338 -5.1336012 -5.3710504 -5.6901422 -6.1463146 -6.3588057 -6.3354187 -6.1231484 -5.8281765 -5.3657732 -4.8208714 -4.3976908 -4.0199943][-4.60826 -4.4149075 -4.427278 -4.58537 -5.0206218 -5.7161417 -6.4921694 -6.9587135 -7.1100931 -6.8054848 -6.2679977 -5.655098 -5.0187569 -4.4743919 -3.9461367]]...]
INFO - root - 2017-12-16 08:29:14.097629: step 60610, loss = 0.27, batch loss = 0.21 (27.3 examples/sec; 0.293 sec/batch; 22h:07m:01s remains)
INFO - root - 2017-12-16 08:29:16.946197: step 60620, loss = 0.30, batch loss = 0.24 (27.2 examples/sec; 0.294 sec/batch; 22h:11m:56s remains)
INFO - root - 2017-12-16 08:29:19.768395: step 60630, loss = 0.23, batch loss = 0.17 (28.6 examples/sec; 0.280 sec/batch; 21h:08m:52s remains)
INFO - root - 2017-12-16 08:29:22.623108: step 60640, loss = 0.27, batch loss = 0.21 (29.1 examples/sec; 0.275 sec/batch; 20h:44m:42s remains)
INFO - root - 2017-12-16 08:29:25.476142: step 60650, loss = 0.21, batch loss = 0.16 (29.0 examples/sec; 0.276 sec/batch; 20h:51m:43s remains)
INFO - root - 2017-12-16 08:29:28.344797: step 60660, loss = 0.25, batch loss = 0.19 (27.9 examples/sec; 0.286 sec/batch; 21h:37m:10s remains)
INFO - root - 2017-12-16 08:29:31.293052: step 60670, loss = 0.30, batch loss = 0.25 (27.6 examples/sec; 0.290 sec/batch; 21h:53m:25s remains)
INFO - root - 2017-12-16 08:29:34.147370: step 60680, loss = 0.23, batch loss = 0.18 (26.5 examples/sec; 0.301 sec/batch; 22h:45m:42s remains)
INFO - root - 2017-12-16 08:29:37.007072: step 60690, loss = 0.36, batch loss = 0.30 (26.8 examples/sec; 0.298 sec/batch; 22h:30m:46s remains)
INFO - root - 2017-12-16 08:29:39.907009: step 60700, loss = 0.23, batch loss = 0.18 (28.5 examples/sec; 0.281 sec/batch; 21h:11m:17s remains)
2017-12-16 08:29:40.438575: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7912824 -4.004971 -4.0831742 -3.9255104 -3.7909646 -3.7446754 -3.6731446 -3.7176597 -3.7403054 -3.8464735 -3.8474262 -3.6010208 -3.3452573 -3.0826945 -2.9680018][-3.7612939 -3.9404573 -4.0716534 -4.0352125 -3.9680259 -4.0385046 -4.0032897 -3.8779118 -3.8048372 -3.8052669 -3.6887863 -3.591614 -3.4442627 -3.3167543 -3.2165914][-3.4056087 -3.5192852 -3.6036901 -3.6693459 -3.6697259 -3.773721 -3.7687309 -3.6367741 -3.5346909 -3.4031665 -3.2820914 -3.3018212 -3.2660279 -3.2332873 -3.2049861][-2.6972642 -2.8117924 -2.9812403 -3.1985903 -3.3528297 -3.4609675 -3.4202704 -3.1888542 -2.9966316 -2.8560786 -2.745388 -2.8930116 -3.1060882 -3.4128757 -3.5739114][-1.8463645 -1.8371665 -2.0266459 -2.358741 -2.5447617 -2.668252 -2.6037917 -2.3066149 -2.0149653 -1.8840535 -1.8918655 -2.0822272 -2.3960838 -2.8451366 -3.0969892][-0.679337 -0.67810655 -0.981791 -1.4621794 -1.6575506 -1.7365608 -1.5222366 -1.122642 -0.80854249 -0.71411014 -0.84675026 -1.2524357 -1.7084053 -2.1383228 -2.4023731][-0.31020689 -0.25573969 -0.5846796 -1.0231583 -1.1806662 -1.0297775 -0.59051037 -0.072736263 0.298738 0.22966766 -0.12126112 -0.61433554 -1.0926514 -1.565696 -1.7546082][-0.50124931 -0.34695387 -0.59146714 -0.76618958 -0.69106913 -0.29983425 0.32584667 0.92716217 1.2822647 1.0243368 0.34950781 -0.4056797 -1.0536671 -1.582181 -1.8425307][-1.3286979 -1.0770578 -1.0785327 -0.95030236 -0.61720657 -0.08665657 0.65609837 1.2574658 1.4935479 1.0999207 0.37475061 -0.51074576 -1.3053598 -1.7898734 -1.9533157][-2.3295791 -2.0845227 -1.8276143 -1.5827138 -1.1207893 -0.36181593 0.31284952 0.81169271 0.93177652 0.43388844 -0.4288826 -1.2596481 -1.9085603 -2.3065858 -2.4159431][-3.2762218 -3.0951385 -2.8478608 -2.4661026 -1.9325111 -1.2330577 -0.64724183 -0.35218191 -0.37974548 -0.92717266 -1.6805134 -2.329124 -2.7634816 -2.9271502 -2.8811493][-4.6736078 -4.620698 -4.2293606 -3.6893671 -3.0581064 -2.4492588 -2.0381882 -1.7957938 -1.88398 -2.4348459 -3.114141 -3.5036774 -3.6492691 -3.6239603 -3.4058816][-5.750268 -5.8431058 -5.5179338 -4.8506303 -4.2171397 -3.6248267 -3.2796359 -3.1639876 -3.3962379 -3.671402 -4.020133 -4.117382 -3.9981976 -3.7998071 -3.540297][-6.7760658 -6.993566 -6.6257 -5.9893007 -5.3549852 -4.7750731 -4.5906477 -4.4472051 -4.6238432 -4.7966266 -4.7948909 -4.532865 -4.1663914 -3.8008695 -3.3979232][-7.2955132 -7.6805463 -7.4792337 -6.8587103 -6.0727892 -5.3670092 -5.1040535 -4.9678025 -5.1081152 -5.2264061 -5.2728033 -4.8690481 -4.3496079 -3.9001079 -3.5420218]]...]
INFO - root - 2017-12-16 08:29:43.300830: step 60710, loss = 0.31, batch loss = 0.25 (28.8 examples/sec; 0.278 sec/batch; 20h:57m:58s remains)
INFO - root - 2017-12-16 08:29:46.117894: step 60720, loss = 0.32, batch loss = 0.26 (29.3 examples/sec; 0.273 sec/batch; 20h:38m:40s remains)
INFO - root - 2017-12-16 08:29:48.983813: step 60730, loss = 0.25, batch loss = 0.19 (26.5 examples/sec; 0.302 sec/batch; 22h:47m:04s remains)
INFO - root - 2017-12-16 08:29:51.835315: step 60740, loss = 0.28, batch loss = 0.22 (28.0 examples/sec; 0.285 sec/batch; 21h:31m:51s remains)
INFO - root - 2017-12-16 08:29:54.695589: step 60750, loss = 0.26, batch loss = 0.20 (27.5 examples/sec; 0.291 sec/batch; 21h:55m:55s remains)
INFO - root - 2017-12-16 08:29:57.555251: step 60760, loss = 0.29, batch loss = 0.23 (28.4 examples/sec; 0.282 sec/batch; 21h:15m:45s remains)
INFO - root - 2017-12-16 08:30:00.395402: step 60770, loss = 0.29, batch loss = 0.23 (28.6 examples/sec; 0.280 sec/batch; 21h:07m:28s remains)
INFO - root - 2017-12-16 08:30:03.281454: step 60780, loss = 0.24, batch loss = 0.18 (28.5 examples/sec; 0.281 sec/batch; 21h:12m:51s remains)
INFO - root - 2017-12-16 08:30:06.159827: step 60790, loss = 0.27, batch loss = 0.21 (28.7 examples/sec; 0.279 sec/batch; 21h:02m:22s remains)
INFO - root - 2017-12-16 08:30:09.025556: step 60800, loss = 0.30, batch loss = 0.25 (28.3 examples/sec; 0.283 sec/batch; 21h:21m:01s remains)
2017-12-16 08:30:09.588465: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7177262 -2.666414 -2.6006527 -2.5474019 -2.5824823 -2.8312941 -3.1802056 -3.4374802 -3.7899191 -3.998709 -4.0313654 -3.9758463 -3.8125205 -3.6112881 -3.3295267][-2.3852406 -2.3394065 -2.3461034 -2.5277653 -2.6743913 -2.8962216 -3.3586969 -3.7729423 -4.0365953 -4.263936 -4.4691396 -4.2535734 -3.9457376 -3.7445765 -3.51256][-1.2742362 -1.6758647 -2.0315237 -2.3115971 -2.5414617 -2.9766998 -3.4737511 -3.9697323 -4.4420924 -4.6805677 -4.7664518 -4.7013535 -4.4738264 -4.0617228 -3.7112434][-0.32324219 -0.87601542 -1.4785888 -1.8564095 -2.1030457 -2.3755484 -2.7878337 -3.1128929 -3.4978132 -4.0810094 -4.5000262 -4.5112782 -4.41145 -4.2500982 -3.8546042][0.55677223 -0.26856375 -0.97758317 -1.5339124 -1.8640752 -1.9072359 -1.990988 -2.236794 -2.7376771 -3.2048655 -3.6545238 -4.0320644 -4.0901284 -4.0158572 -3.7125885][1.3737841 0.39856005 -0.61166573 -1.2082727 -1.2080984 -1.1633728 -1.2788565 -1.3550847 -1.8030357 -2.5108519 -3.2319102 -3.6756361 -3.809092 -3.8135974 -3.6704731][1.0282168 0.16674328 -0.29642582 -0.45967007 -0.3693881 -0.14810276 0.060334682 -0.03382206 -0.4875803 -1.4331248 -2.4972148 -3.1165552 -3.4097576 -3.5939994 -3.5507712][0.32096434 -0.5760119 -0.92146087 -0.52441812 0.22615719 0.77584505 1.0595264 1.0273771 0.54200029 -0.42846203 -1.5427723 -2.3897164 -2.926542 -3.1362243 -3.2604611][-1.793071 -2.2100248 -2.0185349 -1.4540143 -0.40756512 0.51618147 0.98568678 1.0564394 0.63323212 -0.19578552 -1.112078 -1.8484378 -2.3673425 -2.5564003 -2.569768][-3.3569341 -4.0131612 -3.7117 -2.6505189 -1.3772037 -0.37625313 0.33284235 0.37927914 0.12740088 -0.44578838 -1.282654 -1.8123105 -1.8966482 -1.9046381 -1.8212347][-4.7573619 -5.1821575 -4.9670467 -4.1062469 -2.9335406 -1.8609998 -0.92109346 -0.47457194 -0.6357317 -1.1292069 -1.6575017 -1.8890085 -1.8124614 -1.5899227 -1.3887141][-5.3384047 -6.0531759 -6.0900059 -5.7124891 -5.1841021 -4.1066971 -2.9276605 -2.0078444 -1.5614202 -1.6337717 -1.8685663 -1.884347 -1.5102835 -1.2220562 -1.1089597][-5.3781972 -6.28111 -6.931694 -7.099165 -6.828125 -5.9648986 -4.7593837 -3.299397 -2.0784793 -1.6277819 -1.6920745 -1.5522883 -1.1640928 -1.0356667 -1.0090146][-4.6553531 -5.6273451 -6.5028582 -7.2005167 -7.5427504 -6.8138313 -5.3763795 -3.8101587 -2.4212964 -1.5919554 -1.3373499 -1.0766459 -0.61690068 -0.62247562 -0.8346107][-3.6206546 -4.657661 -6.0308747 -6.908679 -7.2397156 -7.1024127 -6.1115813 -4.4420023 -2.800487 -1.9035099 -1.1034665 -0.61842179 -0.36513472 -0.26154661 -0.47691941]]...]
INFO - root - 2017-12-16 08:30:12.427729: step 60810, loss = 0.26, batch loss = 0.21 (28.4 examples/sec; 0.282 sec/batch; 21h:15m:52s remains)
INFO - root - 2017-12-16 08:30:15.295396: step 60820, loss = 0.32, batch loss = 0.26 (26.9 examples/sec; 0.297 sec/batch; 22h:25m:39s remains)
INFO - root - 2017-12-16 08:30:18.153899: step 60830, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.285 sec/batch; 21h:32m:20s remains)
INFO - root - 2017-12-16 08:30:20.962658: step 60840, loss = 0.41, batch loss = 0.35 (28.8 examples/sec; 0.277 sec/batch; 20h:55m:40s remains)
INFO - root - 2017-12-16 08:30:23.808458: step 60850, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 21h:35m:35s remains)
INFO - root - 2017-12-16 08:30:26.666305: step 60860, loss = 0.31, batch loss = 0.25 (27.5 examples/sec; 0.291 sec/batch; 21h:56m:58s remains)
INFO - root - 2017-12-16 08:30:29.539580: step 60870, loss = 0.23, batch loss = 0.18 (27.1 examples/sec; 0.295 sec/batch; 22h:14m:37s remains)
INFO - root - 2017-12-16 08:30:32.437055: step 60880, loss = 0.36, batch loss = 0.30 (28.2 examples/sec; 0.284 sec/batch; 21h:23m:33s remains)
INFO - root - 2017-12-16 08:30:35.303897: step 60890, loss = 0.30, batch loss = 0.25 (27.7 examples/sec; 0.289 sec/batch; 21h:48m:51s remains)
INFO - root - 2017-12-16 08:30:38.170850: step 60900, loss = 0.30, batch loss = 0.24 (26.2 examples/sec; 0.306 sec/batch; 23h:03m:50s remains)
2017-12-16 08:30:38.723078: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1583381 -2.4780087 -2.9451504 -3.0918283 -3.1091497 -3.3045082 -3.5277281 -3.5234041 -3.3840714 -3.329241 -3.106791 -2.8461571 -2.6166739 -2.2593634 -2.0191832][-1.7425735 -2.0683229 -2.5839005 -2.9823608 -3.1773555 -3.2234821 -3.3455606 -3.5508504 -3.7208216 -3.5863802 -3.2738888 -2.9804287 -2.8234339 -2.5088897 -2.270016][-2.0595155 -2.3623548 -2.705389 -3.0096774 -3.1851766 -3.1734567 -3.1554644 -3.2904997 -3.511215 -3.6745491 -3.6662664 -3.3811007 -3.1941276 -3.0616319 -2.9769814][-2.6502523 -2.6083255 -2.5606654 -2.5976248 -2.6094718 -2.5877602 -2.7418256 -2.7758989 -2.9959395 -3.367794 -3.5906758 -3.7570584 -3.7960658 -3.7053175 -3.6513879][-2.8599677 -2.6365395 -2.4270873 -1.9654543 -1.4734876 -1.2839189 -1.4479234 -1.724905 -2.1336806 -2.7683277 -3.4325137 -3.8280463 -4.0048933 -4.2134666 -4.2307172][-3.0955334 -2.4466352 -1.9361324 -1.2592413 -0.53748369 0.023143291 0.29826832 0.077582836 -0.54184771 -1.3309793 -2.2377424 -3.2264354 -3.9654758 -4.3142176 -4.288682][-2.88475 -1.7735798 -0.752867 0.43683577 1.3025155 1.8735766 2.1185799 1.821321 1.2413211 0.12295198 -1.2120848 -2.4416206 -3.3689322 -3.9011493 -4.0477591][-2.5096352 -1.4704978 -0.25498152 1.2953396 2.558867 3.5295315 3.8619547 3.4454494 2.5920095 1.189321 -0.29707432 -1.940557 -3.2416749 -3.8202295 -4.0194311][-2.7176974 -1.7850006 -0.81068683 0.45876169 1.5827394 2.7523346 3.39179 3.29003 2.5100513 1.1400075 -0.34943819 -2.0633764 -3.4072232 -4.1763391 -4.5261021][-3.5398955 -3.009697 -2.3890371 -1.3612702 -0.48530841 0.52734089 1.1551261 1.2233338 0.75137663 -0.23005486 -1.4622231 -3.0598249 -4.3293409 -5.0392823 -5.3283677][-4.7042813 -4.1295748 -3.6779563 -3.2290692 -2.7298326 -2.0378075 -1.5874543 -1.4447496 -1.4681463 -2.0474596 -2.9048016 -4.0741076 -5.144402 -5.9404573 -6.3255887][-5.2355042 -5.0678782 -5.0182261 -4.6859965 -4.4809237 -4.2526665 -3.957159 -3.7168412 -3.5049636 -3.7082605 -4.0595984 -4.9567676 -5.7840166 -6.4059935 -6.9307261][-5.3790684 -5.4174166 -5.3622832 -5.4177341 -5.5701218 -5.3982763 -5.2136145 -5.0370073 -4.7344503 -4.6354556 -4.6770244 -5.1921282 -5.7567563 -6.4581003 -7.0490108][-4.2779279 -4.4924 -4.9337754 -5.1480427 -5.3772459 -5.4034944 -5.3085914 -5.1983237 -4.8808436 -4.4939394 -4.1986833 -4.4626603 -5.0284863 -5.621418 -6.2604089][-3.7094588 -3.7405748 -3.9149351 -4.3223348 -4.6259236 -4.7720561 -4.7483268 -4.466527 -4.0683041 -3.7989178 -3.6094613 -3.5425673 -3.9698098 -4.6317277 -5.4192619]]...]
INFO - root - 2017-12-16 08:30:41.589814: step 60910, loss = 0.21, batch loss = 0.15 (27.6 examples/sec; 0.290 sec/batch; 21h:52m:47s remains)
INFO - root - 2017-12-16 08:30:44.436751: step 60920, loss = 0.43, batch loss = 0.37 (28.0 examples/sec; 0.286 sec/batch; 21h:32m:59s remains)
INFO - root - 2017-12-16 08:30:47.278405: step 60930, loss = 0.23, batch loss = 0.17 (27.8 examples/sec; 0.287 sec/batch; 21h:40m:16s remains)
INFO - root - 2017-12-16 08:30:50.097768: step 60940, loss = 0.34, batch loss = 0.28 (28.1 examples/sec; 0.285 sec/batch; 21h:29m:06s remains)
INFO - root - 2017-12-16 08:30:52.952640: step 60950, loss = 0.27, batch loss = 0.21 (28.6 examples/sec; 0.280 sec/batch; 21h:06m:21s remains)
INFO - root - 2017-12-16 08:30:55.799673: step 60960, loss = 0.27, batch loss = 0.21 (28.1 examples/sec; 0.285 sec/batch; 21h:27m:38s remains)
INFO - root - 2017-12-16 08:30:58.659857: step 60970, loss = 0.21, batch loss = 0.15 (28.1 examples/sec; 0.285 sec/batch; 21h:27m:35s remains)
INFO - root - 2017-12-16 08:31:01.473420: step 60980, loss = 0.28, batch loss = 0.22 (29.4 examples/sec; 0.272 sec/batch; 20h:30m:57s remains)
INFO - root - 2017-12-16 08:31:04.319948: step 60990, loss = 0.25, batch loss = 0.19 (28.6 examples/sec; 0.279 sec/batch; 21h:03m:44s remains)
INFO - root - 2017-12-16 08:31:07.199959: step 61000, loss = 0.32, batch loss = 0.26 (27.1 examples/sec; 0.295 sec/batch; 22h:14m:00s remains)
2017-12-16 08:31:07.787515: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.769804 -5.1986752 -4.7549033 -4.7095942 -4.8658915 -4.9589047 -4.8235946 -4.8529191 -5.0614972 -5.0703726 -5.369257 -5.9031229 -6.3553395 -6.49596 -6.7245731][-5.9669695 -5.2789869 -4.7360044 -4.6205511 -4.7276163 -4.8242674 -4.8209133 -4.8600812 -4.9904108 -4.9973259 -5.3714867 -5.8465824 -6.0709362 -6.1457849 -6.2620354][-5.8061452 -4.962225 -4.3342566 -4.1784749 -4.1716166 -4.2498846 -4.0824165 -4.2181263 -4.3894796 -4.4564476 -4.9671731 -5.4790425 -5.76182 -5.4914789 -5.3546643][-5.2601271 -4.2659593 -3.4239588 -3.0902798 -3.0022869 -2.9417093 -2.8944769 -3.0759504 -3.2882211 -3.739023 -4.3065429 -4.7496119 -4.8227882 -4.6231251 -4.4700556][-4.471384 -3.182142 -1.9404378 -1.2848997 -0.909863 -0.85089278 -0.85822392 -1.093293 -1.5944221 -2.3333182 -3.2189136 -3.9006569 -4.06619 -3.8062863 -3.7863853][-3.6566949 -2.1924119 -0.92794585 0.27627373 1.2584457 1.4861593 1.5315547 1.1245713 0.37026358 -0.83316183 -2.0598471 -2.8407974 -3.2266982 -3.159235 -3.0571556][-2.94535 -1.6941648 -0.4868772 0.96570206 2.1522965 2.5995102 2.8426075 2.4780674 1.5782099 0.2217989 -1.1210907 -2.2172074 -2.8964734 -3.155025 -3.4111838][-3.1029859 -1.7649093 -0.58131003 0.8227911 1.9280176 2.5942559 3.0088329 2.7789607 2.165453 0.87637281 -0.60062337 -1.7926106 -2.7956164 -3.4375763 -4.0082879][-3.9974904 -2.8952279 -1.8459568 -0.52263737 0.60970545 1.3660197 1.8888373 1.9970379 1.6259093 0.58571243 -0.68111205 -1.88996 -2.9645686 -3.7868555 -4.5742245][-5.4491677 -4.4687104 -3.5209177 -2.4505992 -1.5691116 -0.70342064 -0.2307229 0.10843945 0.086820126 -0.62882042 -1.5713747 -2.6243138 -3.6708393 -4.5985594 -5.3861036][-7.0339231 -6.1529131 -5.5346117 -4.5681844 -3.7188354 -2.9321523 -2.2391129 -1.6883607 -1.6237266 -1.9042943 -2.4015505 -3.3544724 -4.377934 -5.2895403 -6.2008772][-8.1051025 -7.199079 -6.5817657 -5.9868031 -5.574687 -4.8543329 -4.3063836 -3.7021353 -3.2095289 -3.0868559 -3.3046989 -4.00497 -4.7914586 -5.7955112 -6.7941842][-8.3965063 -7.5572777 -7.0703106 -6.6554193 -6.45564 -6.0622511 -5.5756183 -5.1109352 -4.68526 -4.2298412 -4.1034212 -4.5893645 -5.183414 -5.996953 -6.8614044][-8.2380571 -7.2130642 -6.6674042 -6.4568195 -6.5563931 -6.421566 -6.3051062 -5.9632115 -5.4881968 -5.1190238 -4.9068575 -5.0946488 -5.42065 -6.0308852 -6.7890062][-7.3307405 -6.289784 -5.7449417 -5.7318707 -6.0303612 -6.1808124 -6.3054729 -6.1883841 -5.989646 -5.6411314 -5.3565116 -5.4609089 -5.573576 -5.8404741 -6.1907539]]...]
INFO - root - 2017-12-16 08:31:10.675245: step 61010, loss = 0.22, batch loss = 0.16 (27.6 examples/sec; 0.290 sec/batch; 21h:50m:30s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 08:31:13.529300: step 61020, loss = 0.28, batch loss = 0.22 (27.1 examples/sec; 0.295 sec/batch; 22h:16m:29s remains)
INFO - root - 2017-12-16 08:31:16.385727: step 61030, loss = 0.31, batch loss = 0.25 (28.7 examples/sec; 0.278 sec/batch; 20h:59m:47s remains)
INFO - root - 2017-12-16 08:31:19.239403: step 61040, loss = 0.22, batch loss = 0.16 (27.2 examples/sec; 0.294 sec/batch; 22h:08m:22s remains)
INFO - root - 2017-12-16 08:31:22.106030: step 61050, loss = 0.32, batch loss = 0.26 (27.5 examples/sec; 0.291 sec/batch; 21h:55m:07s remains)
INFO - root - 2017-12-16 08:31:24.972276: step 61060, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.284 sec/batch; 21h:22m:40s remains)
INFO - root - 2017-12-16 08:31:27.804782: step 61070, loss = 0.26, batch loss = 0.21 (28.8 examples/sec; 0.278 sec/batch; 20h:56m:54s remains)
INFO - root - 2017-12-16 08:31:30.658090: step 61080, loss = 0.21, batch loss = 0.15 (29.1 examples/sec; 0.275 sec/batch; 20h:43m:13s remains)
INFO - root - 2017-12-16 08:31:33.479627: step 61090, loss = 0.23, batch loss = 0.17 (27.7 examples/sec; 0.289 sec/batch; 21h:48m:23s remains)
INFO - root - 2017-12-16 08:31:36.317049: step 61100, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 20h:55m:36s remains)
2017-12-16 08:31:36.841186: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8654857 -3.1247685 -3.313426 -3.2886059 -3.1858196 -3.0223417 -2.9301455 -2.8189502 -2.681253 -2.5865242 -2.4370422 -2.2446082 -2.1451237 -2.1294847 -2.1097543][-2.5924048 -2.9450722 -3.361913 -3.4064155 -3.3047533 -3.0932856 -2.9244764 -2.756485 -2.6535425 -2.6517963 -2.5569692 -2.4522576 -2.3124723 -2.2550056 -2.2402937][-2.1194141 -2.6789832 -3.3278413 -3.489224 -3.2139835 -2.8461943 -2.6177564 -2.5113134 -2.3775935 -2.593102 -2.7877886 -2.7035832 -2.6145422 -2.6158397 -2.6192985][-1.6699619 -2.3773007 -3.1059186 -3.3591857 -2.9624581 -2.4439862 -2.009429 -1.8352034 -1.9151173 -2.2911091 -2.6323066 -2.8408992 -2.9144254 -2.9996586 -3.0763183][-1.7657404 -2.4072874 -2.8781209 -2.9302273 -2.3148708 -1.6129951 -1.0970323 -0.9158206 -1.135273 -1.766968 -2.3076272 -2.6064668 -2.7998934 -3.0088573 -3.2097101][-1.9711943 -2.4591846 -2.5682557 -2.1758113 -1.1946883 -0.30955172 0.28610611 0.49787426 0.17640305 -0.645365 -1.4756727 -1.952909 -2.2942054 -2.6850669 -3.0894432][-2.1036165 -2.2087438 -2.0575438 -1.3580065 -0.098063946 0.98692751 1.7634978 1.9593506 1.5812488 0.65671062 -0.44005919 -1.1536174 -1.6742816 -1.9817379 -2.2392995][-2.0546172 -2.0540836 -1.6847098 -0.88054729 0.27113485 1.4459429 2.3277159 2.599041 2.1593056 1.1513672 0.055856228 -0.58141685 -0.81155729 -0.91940355 -1.0243294][-2.0044801 -2.0492327 -1.7117209 -0.86745572 0.0676074 0.99134445 1.8116188 2.0900769 1.7684112 0.96962214 0.17883253 -0.10929012 0.015526772 0.38419628 0.66735172][-1.7185807 -1.8791251 -1.7810421 -1.1338587 -0.45565343 0.044686317 0.44978 0.56185341 0.17520475 -0.14698362 -0.27207136 0.018081188 0.59089136 1.0089765 1.363471][-1.6587524 -1.7053676 -1.8640568 -1.4713449 -1.187052 -1.1596165 -1.1677184 -1.2402799 -1.5198195 -1.5648594 -1.2112136 -0.35295582 0.62177992 1.1805177 1.4941502][-1.8789515 -1.9370143 -2.1578667 -2.1202493 -2.2771738 -2.8679008 -3.4954152 -3.708596 -3.8012869 -3.4392416 -2.6397285 -1.3693087 -0.12438631 0.65013504 1.2132654][-2.3202744 -2.3279524 -2.735703 -3.0913029 -3.7286348 -4.6814532 -5.7656994 -6.2028685 -6.2443762 -5.5218415 -4.1899734 -2.7208507 -1.2967155 -0.40441751 0.14756536][-2.8645229 -2.6852102 -3.0027514 -3.6291533 -4.5571251 -5.8241415 -7.1385055 -7.5771947 -7.4862461 -6.7720127 -5.6651545 -4.0934134 -2.5910096 -1.9116883 -1.5546472][-3.5692616 -3.2830729 -3.5571079 -4.2347512 -5.2680907 -6.6203184 -7.9583478 -8.39052 -8.1992788 -7.4004688 -6.4475322 -5.2153873 -3.9675539 -3.4153767 -3.128525]]...]
INFO - root - 2017-12-16 08:31:39.687331: step 61110, loss = 0.22, batch loss = 0.16 (28.5 examples/sec; 0.281 sec/batch; 21h:09m:31s remains)
INFO - root - 2017-12-16 08:31:42.504603: step 61120, loss = 0.22, batch loss = 0.16 (28.8 examples/sec; 0.277 sec/batch; 20h:54m:19s remains)
INFO - root - 2017-12-16 08:31:45.292803: step 61130, loss = 0.44, batch loss = 0.38 (28.9 examples/sec; 0.276 sec/batch; 20h:50m:21s remains)
INFO - root - 2017-12-16 08:31:48.194465: step 61140, loss = 0.21, batch loss = 0.15 (27.8 examples/sec; 0.288 sec/batch; 21h:42m:27s remains)
INFO - root - 2017-12-16 08:31:51.010728: step 61150, loss = 0.32, batch loss = 0.26 (29.4 examples/sec; 0.272 sec/batch; 20h:30m:43s remains)
INFO - root - 2017-12-16 08:31:53.852139: step 61160, loss = 0.21, batch loss = 0.16 (29.8 examples/sec; 0.269 sec/batch; 20h:15m:19s remains)
INFO - root - 2017-12-16 08:31:56.667199: step 61170, loss = 0.58, batch loss = 0.52 (28.2 examples/sec; 0.284 sec/batch; 21h:22m:02s remains)
INFO - root - 2017-12-16 08:31:59.509443: step 61180, loss = 0.29, batch loss = 0.23 (29.1 examples/sec; 0.275 sec/batch; 20h:44m:29s remains)
INFO - root - 2017-12-16 08:32:02.348549: step 61190, loss = 0.23, batch loss = 0.17 (28.1 examples/sec; 0.285 sec/batch; 21h:27m:17s remains)
INFO - root - 2017-12-16 08:32:05.147501: step 61200, loss = 0.30, batch loss = 0.24 (28.0 examples/sec; 0.286 sec/batch; 21h:32m:49s remains)
2017-12-16 08:32:05.703556: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0371809 -5.2926888 -5.384398 -5.6635866 -5.6259985 -5.7956004 -6.1965942 -6.5091949 -6.7064762 -6.6543164 -6.596489 -6.2167072 -5.8306522 -5.5786304 -5.4711361][-4.5486441 -5.01235 -5.1124754 -5.1534996 -4.887506 -4.9032841 -5.00724 -5.2125187 -5.4621763 -5.43626 -5.4653177 -5.3415637 -5.2562914 -5.1353126 -5.0401959][-4.4501443 -4.9842453 -5.0445256 -4.6679635 -3.9211111 -3.407511 -3.0778937 -3.1421061 -3.5220351 -3.941052 -4.3408456 -4.4579663 -4.5316906 -4.48338 -4.379838][-4.2385073 -4.7164712 -4.6042323 -3.87164 -2.5329571 -1.367835 -0.54456854 -0.4318819 -1.0292022 -1.8117359 -2.8609409 -3.464654 -3.77246 -3.7332873 -3.5623331][-3.9395745 -4.4659286 -3.8460736 -2.4838805 -0.63352132 1.0850849 2.3236403 2.3510656 1.6823549 0.27442455 -1.3225095 -2.3686714 -3.0194516 -3.0291147 -2.9568262][-3.7759831 -3.9347956 -3.181829 -1.2661066 1.3999782 3.5827312 5.064805 4.9593287 3.8315134 1.9309411 -0.083532333 -1.4895358 -2.235652 -2.3557391 -2.2287848][-3.32902 -3.5244 -2.68506 -0.50836945 2.3137212 4.96894 6.8163052 6.7972755 5.3807344 3.0914726 0.91993904 -0.6517415 -1.5265882 -1.6041961 -1.6403618][-3.2068706 -3.330327 -2.5029624 -0.6018517 2.1123242 4.8342171 6.64058 6.7991352 5.6506681 3.4051328 1.3004069 -0.17564869 -0.91721606 -1.1208305 -1.3545437][-3.3286667 -3.752666 -3.0520945 -1.2055018 0.8166585 3.0711317 4.9665117 5.284853 4.4852114 2.7837992 1.1258936 -0.081438065 -0.7197628 -1.041486 -1.4083822][-4.0028238 -4.6294971 -4.5596018 -3.1347151 -1.236675 0.57186127 1.9016628 2.6302481 2.4265509 1.2730069 0.22740698 -0.645947 -0.99087191 -1.2835116 -1.8497593][-4.758348 -5.539454 -5.6275606 -4.617239 -3.4668355 -1.8975782 -0.72492743 -0.22633314 -0.13823748 -0.65003181 -1.1740706 -1.6668751 -1.7694151 -1.830534 -2.266573][-5.6517673 -6.3481693 -6.7117643 -6.0922351 -5.0709167 -3.9345534 -3.292846 -2.7885485 -2.4584081 -2.6537247 -2.8284388 -3.1073649 -2.8816264 -2.6367126 -2.7970765][-6.147789 -6.6726003 -7.0721693 -6.7549663 -6.1784124 -5.2925158 -4.7531443 -4.6395292 -4.5391827 -4.3457332 -4.3326221 -4.5059104 -4.2625132 -3.7943633 -3.5495296][-6.4376574 -6.6748323 -6.8494835 -6.5594287 -6.205925 -5.6464105 -5.3525844 -5.3935437 -5.5704551 -5.6586204 -5.6563859 -5.7257204 -5.3334737 -4.66764 -4.216466][-6.0623765 -6.2624116 -6.4042816 -6.1642723 -5.8206692 -5.5065727 -5.4441676 -5.7305088 -6.1737347 -6.3846283 -6.6610489 -6.5838976 -6.2635431 -5.6434975 -4.8206329]]...]
INFO - root - 2017-12-16 08:32:08.505868: step 61210, loss = 0.23, batch loss = 0.17 (29.1 examples/sec; 0.275 sec/batch; 20h:43m:16s remains)
INFO - root - 2017-12-16 08:32:11.306748: step 61220, loss = 0.24, batch loss = 0.18 (28.6 examples/sec; 0.280 sec/batch; 21h:05m:24s remains)
INFO - root - 2017-12-16 08:32:14.149762: step 61230, loss = 0.27, batch loss = 0.21 (29.1 examples/sec; 0.275 sec/batch; 20h:43m:37s remains)
INFO - root - 2017-12-16 08:32:16.970527: step 61240, loss = 0.23, batch loss = 0.18 (28.2 examples/sec; 0.284 sec/batch; 21h:23m:36s remains)
INFO - root - 2017-12-16 08:32:19.797686: step 61250, loss = 0.25, batch loss = 0.19 (29.0 examples/sec; 0.276 sec/batch; 20h:46m:48s remains)
INFO - root - 2017-12-16 08:32:22.646451: step 61260, loss = 0.22, batch loss = 0.16 (27.1 examples/sec; 0.295 sec/batch; 22h:13m:32s remains)
INFO - root - 2017-12-16 08:32:25.496850: step 61270, loss = 0.23, batch loss = 0.17 (26.7 examples/sec; 0.299 sec/batch; 22h:33m:08s remains)
INFO - root - 2017-12-16 08:32:28.301086: step 61280, loss = 0.25, batch loss = 0.19 (29.3 examples/sec; 0.273 sec/batch; 20h:33m:00s remains)
INFO - root - 2017-12-16 08:32:31.074163: step 61290, loss = 0.24, batch loss = 0.18 (29.0 examples/sec; 0.276 sec/batch; 20h:45m:39s remains)
INFO - root - 2017-12-16 08:32:33.924105: step 61300, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 21h:13m:46s remains)
2017-12-16 08:32:34.434334: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2815018 -5.0634284 -4.7449222 -4.352169 -4.0568161 -3.978591 -4.0950437 -4.2355461 -4.3627853 -4.3279104 -4.2346497 -4.1120744 -4.0171447 -3.8328 -3.3770213][-6.4611239 -6.1740065 -5.7283716 -5.3922658 -5.2363396 -5.2301264 -5.2614031 -5.1117215 -5.0212407 -4.946538 -4.9758234 -4.9025793 -4.8472023 -4.7118115 -4.1919889][-7.4163656 -7.4062805 -7.1730318 -6.8534579 -6.4654174 -6.1692419 -5.9540982 -5.7153726 -5.4123793 -5.1839566 -5.1233883 -5.1079073 -5.3170328 -5.2806759 -4.8258891][-8.4051476 -8.45779 -8.2273273 -8.0417213 -7.6509848 -7.0841484 -6.2913418 -5.2975454 -4.5118995 -4.1156392 -4.0905113 -4.4250684 -5.0324922 -5.152246 -5.0044265][-8.295373 -8.5807991 -8.5154772 -8.2449913 -7.3609643 -6.1628847 -4.8250642 -3.5279074 -2.6748538 -2.3328431 -2.4816651 -3.1733758 -4.1300688 -4.5820637 -4.4628134][-7.1941915 -7.6183038 -7.6281986 -7.2983828 -6.12951 -4.4121518 -2.4321418 -0.65985036 0.43799591 0.77597046 0.053349495 -1.1351914 -2.3008089 -3.2482953 -3.5805638][-5.8134527 -6.118691 -6.2629356 -5.9354057 -4.7593374 -2.7634876 -0.36641312 1.5854216 2.7027469 2.9361172 2.1303573 0.6831584 -0.90916181 -1.8992844 -2.0644617][-4.4334908 -4.7004442 -4.7388105 -4.2738557 -2.9839926 -0.83010411 1.5108161 3.2793112 4.1967421 3.8923454 2.661046 1.3032579 0.083467484 -0.76364517 -1.1197138][-2.8789144 -3.2205625 -3.5466354 -3.1896152 -2.0232613 -0.098044872 1.8544364 3.3837562 4.0816717 3.6141787 2.24219 0.71280336 -0.1319313 -0.19568491 0.00078201294][-1.8049793 -2.117146 -2.5976462 -2.6648219 -1.9362068 -0.4842968 1.0439353 2.2847562 2.5633507 1.9944668 0.77467585 -0.37760878 -0.761827 -0.34435129 0.30808449][-0.8520124 -1.2540426 -1.9572458 -2.5009248 -2.3815286 -1.4444942 -0.27543068 0.567832 0.55983496 0.16386032 -0.7069881 -1.3001175 -1.1106949 -0.47782135 0.3184886][-0.51898217 -0.83019567 -1.5426152 -2.338779 -2.4905882 -2.3045616 -1.9511604 -1.4484076 -1.4125984 -1.6941962 -2.3443136 -2.6508703 -2.2312233 -1.2613742 -0.49547577][-0.0018415451 -0.16406822 -0.99272418 -2.151968 -2.963537 -2.9872086 -2.4657385 -2.3787873 -2.7090034 -2.9942565 -3.52264 -3.7703233 -3.3863425 -2.6990714 -2.1126084][0.40969849 0.37381506 -0.48094654 -1.7581842 -2.7965021 -3.3015552 -3.3996677 -3.2405987 -3.2875226 -3.5167167 -3.8916123 -4.092371 -4.0853586 -3.8916037 -3.5338144][0.90251732 0.89218187 0.01573801 -1.3644109 -2.5993323 -3.2991011 -3.6239936 -3.6346104 -3.7163436 -3.8725648 -4.2646785 -4.3632126 -4.0847626 -3.95417 -3.8417592]]...]
INFO - root - 2017-12-16 08:32:37.293535: step 61310, loss = 0.53, batch loss = 0.48 (28.0 examples/sec; 0.286 sec/batch; 21h:30m:49s remains)
INFO - root - 2017-12-16 08:32:40.130925: step 61320, loss = 0.20, batch loss = 0.15 (27.0 examples/sec; 0.296 sec/batch; 22h:19m:06s remains)
INFO - root - 2017-12-16 08:32:42.915821: step 61330, loss = 0.28, batch loss = 0.22 (28.8 examples/sec; 0.278 sec/batch; 20h:54m:41s remains)
INFO - root - 2017-12-16 08:32:45.766304: step 61340, loss = 0.23, batch loss = 0.17 (26.2 examples/sec; 0.306 sec/batch; 23h:01m:19s remains)
INFO - root - 2017-12-16 08:32:48.618580: step 61350, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.285 sec/batch; 21h:29m:59s remains)
INFO - root - 2017-12-16 08:32:51.394271: step 61360, loss = 0.38, batch loss = 0.33 (28.5 examples/sec; 0.281 sec/batch; 21h:08m:55s remains)
INFO - root - 2017-12-16 08:32:54.263869: step 61370, loss = 0.25, batch loss = 0.19 (26.7 examples/sec; 0.299 sec/batch; 22h:32m:39s remains)
INFO - root - 2017-12-16 08:32:57.131900: step 61380, loss = 0.26, batch loss = 0.20 (27.2 examples/sec; 0.294 sec/batch; 22h:07m:36s remains)
INFO - root - 2017-12-16 08:33:00.004535: step 61390, loss = 0.29, batch loss = 0.24 (28.2 examples/sec; 0.284 sec/batch; 21h:22m:09s remains)
INFO - root - 2017-12-16 08:33:02.852923: step 61400, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.285 sec/batch; 21h:28m:14s remains)
2017-12-16 08:33:03.362596: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.759131 -6.0713425 -6.5542226 -6.6729469 -6.7117748 -6.2610173 -6.0724239 -6.1055689 -5.8749819 -5.6123991 -5.0867987 -4.3279 -3.5598273 -3.1582642 -3.046824][-6.7694511 -6.5940266 -6.6625137 -6.9593272 -7.2103171 -7.3069987 -7.0972495 -6.9766464 -6.7567949 -6.4326792 -6.0959067 -5.5867643 -5.0727844 -4.5731711 -4.4089084][-7.2447042 -7.3108416 -7.2859497 -7.2248344 -7.2512913 -6.7971535 -6.1812482 -6.2956324 -6.5467834 -6.6368141 -6.4730215 -6.1803331 -5.94978 -5.6439061 -5.3071775][-7.2648296 -6.9750733 -6.940361 -6.625186 -6.2412291 -5.5501242 -4.670187 -4.2849855 -4.5318842 -5.4692717 -6.3083181 -6.6029596 -6.49567 -6.2153878 -5.843791][-6.8525295 -6.4855881 -6.2681508 -5.6031094 -4.665586 -3.4160492 -2.1220539 -1.4228971 -1.5321081 -2.7973731 -4.361608 -5.4831767 -5.9538746 -5.7823143 -5.2906027][-5.7542095 -5.3396025 -4.6951909 -3.5182543 -1.55201 0.33478022 1.7638988 2.4178286 2.0715761 0.37631178 -1.9104314 -3.7029688 -4.6670146 -4.9715347 -4.5448523][-4.9894476 -4.3307242 -3.4018383 -1.5464084 1.1087823 3.6669474 5.709095 6.2600136 5.5555458 3.3926549 0.79226255 -1.6262264 -3.0914898 -3.4413505 -3.3952563][-4.8037148 -4.0114932 -3.1596012 -1.1307218 1.6060939 4.3779917 6.537323 7.0973988 6.2338934 3.8252802 1.2127275 -0.80667734 -2.0085711 -2.5423026 -2.5865598][-4.9012284 -4.477972 -3.5955467 -1.9441717 0.24726963 2.7302275 4.6065979 5.3667679 4.8691187 2.9422746 0.62161016 -1.3923724 -2.3717713 -2.4926238 -2.0910451][-4.9421072 -4.8847909 -4.7517848 -3.7091451 -2.0326047 -0.13793564 1.3356962 1.9875431 1.6225605 0.11058521 -1.5565012 -3.0531278 -3.5721183 -3.2425628 -2.67978][-5.7790823 -6.0178251 -6.1179128 -5.6496406 -4.9400973 -3.8514969 -2.5672686 -2.0019391 -2.1970794 -3.1728745 -4.0987048 -4.8452644 -5.0917048 -4.5674415 -3.5622132][-6.1051435 -6.7256136 -7.3956456 -7.3257589 -6.9424839 -6.3749871 -5.8683977 -5.4819403 -5.3054433 -5.5653539 -5.9749374 -6.4385023 -6.2958722 -5.4981151 -4.4737172][-5.1874528 -6.0722542 -7.0189123 -7.6193972 -7.9548588 -7.4706059 -6.8275547 -6.405735 -6.2955623 -6.303977 -6.2856522 -6.4827366 -6.4875484 -6.0641031 -5.197763][-4.8079267 -5.2907915 -5.9209857 -6.5962315 -7.0895019 -7.0010653 -6.638309 -6.2307978 -6.0414858 -5.9066181 -5.7974854 -5.8315244 -5.898488 -5.728126 -5.1611924][-4.5814066 -4.7932467 -5.107842 -5.3548074 -5.5692463 -5.5835152 -5.4320483 -5.1277781 -4.8600025 -4.8885274 -5.0547523 -5.020484 -4.861526 -4.8686209 -4.8025641]]...]
INFO - root - 2017-12-16 08:33:06.136117: step 61410, loss = 0.25, batch loss = 0.19 (29.3 examples/sec; 0.273 sec/batch; 20h:31m:44s remains)
INFO - root - 2017-12-16 08:33:08.997223: step 61420, loss = 0.29, batch loss = 0.23 (28.1 examples/sec; 0.285 sec/batch; 21h:27m:16s remains)
INFO - root - 2017-12-16 08:33:11.844506: step 61430, loss = 0.22, batch loss = 0.16 (29.3 examples/sec; 0.273 sec/batch; 20h:32m:12s remains)
INFO - root - 2017-12-16 08:33:14.731719: step 61440, loss = 0.22, batch loss = 0.16 (28.8 examples/sec; 0.278 sec/batch; 20h:54m:58s remains)
INFO - root - 2017-12-16 08:33:17.539085: step 61450, loss = 0.35, batch loss = 0.29 (27.7 examples/sec; 0.289 sec/batch; 21h:47m:00s remains)
INFO - root - 2017-12-16 08:33:20.402349: step 61460, loss = 0.32, batch loss = 0.26 (28.0 examples/sec; 0.286 sec/batch; 21h:31m:12s remains)
INFO - root - 2017-12-16 08:33:23.252071: step 61470, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.280 sec/batch; 21h:05m:23s remains)
INFO - root - 2017-12-16 08:33:26.119836: step 61480, loss = 0.22, batch loss = 0.16 (28.4 examples/sec; 0.282 sec/batch; 21h:14m:23s remains)
INFO - root - 2017-12-16 08:33:28.979327: step 61490, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.280 sec/batch; 21h:06m:12s remains)
INFO - root - 2017-12-16 08:33:31.814910: step 61500, loss = 0.21, batch loss = 0.15 (27.9 examples/sec; 0.287 sec/batch; 21h:36m:11s remains)
2017-12-16 08:33:32.333211: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2787337 -4.5300961 -4.7276959 -4.7236776 -4.5581689 -3.961699 -3.3149414 -2.9979174 -2.9077697 -2.8809829 -3.1321337 -3.660444 -3.9900079 -4.0750256 -3.9234896][-5.0885673 -5.4166465 -5.6169748 -5.61145 -5.4825268 -4.986762 -4.3699689 -3.7273684 -3.3174372 -3.121927 -3.1353126 -3.3158433 -3.5821786 -3.5725453 -3.3525376][-5.5003395 -6.0564117 -6.4256315 -6.2629743 -6.0147705 -5.5221219 -4.8739858 -4.0934262 -3.4985869 -3.2847967 -3.2061458 -3.2840314 -3.4453244 -3.582552 -3.5859802][-5.7704859 -6.0932131 -6.148581 -6.0062623 -5.7333956 -5.1211562 -4.4149489 -3.7287502 -3.3189068 -3.1090591 -2.8397698 -3.225018 -3.7612848 -3.8099976 -3.6312947][-5.4372644 -5.5568762 -5.2980146 -4.6818833 -4.0706296 -3.5436518 -3.1209111 -2.7411938 -2.6106086 -2.6519313 -2.90454 -3.2743452 -3.7050703 -4.2907786 -4.5581646][-4.4402905 -4.2978129 -3.7454958 -2.7838755 -1.9874408 -1.5872495 -1.3476002 -1.0145557 -1.0903146 -1.6025505 -2.25998 -2.946835 -3.7796535 -4.2429295 -4.384408][-3.1716938 -2.6819553 -1.9252572 -0.68970084 0.26819611 0.903152 1.2074003 1.101613 0.70913839 0.10584927 -0.82514906 -2.0293446 -3.1605189 -3.9189115 -4.2577529][-1.9080594 -1.3528616 -0.6072433 0.574327 1.6808858 2.4505382 2.762825 2.6811934 2.2623434 1.2846923 0.028699875 -1.316335 -2.5157747 -3.3644845 -3.8789144][-1.2824149 -0.71808243 -0.033492565 0.72974443 1.4154515 2.1179471 2.5915236 2.6171365 2.3114376 1.6134434 0.47899294 -0.91299391 -2.1761849 -3.1607666 -3.8797994][-0.98894024 -0.519943 -0.49312568 -0.11967945 0.49026918 0.716166 0.96375179 1.3636608 1.3626118 0.94103289 0.17764473 -0.89382386 -1.9399021 -2.708075 -3.3570452][-1.3489339 -1.0331364 -1.2012732 -1.4649508 -1.6166053 -1.3654294 -0.89395332 -0.66854 -0.5266242 -0.37147903 -0.58551168 -1.1745207 -2.0275974 -2.6804321 -3.2158546][-2.2542007 -2.0628543 -2.4577694 -2.968204 -3.2876828 -3.7470992 -3.7184019 -2.9855514 -2.263207 -1.9668739 -1.8986647 -2.0387893 -2.5383961 -2.93782 -3.2929926][-2.8952904 -2.99931 -3.4994102 -4.3833113 -5.1137395 -5.3980665 -5.2202892 -4.7475953 -4.0947032 -3.2555084 -2.7263131 -2.5838466 -2.8422904 -3.1797018 -3.4406369][-3.636147 -3.6290238 -4.162416 -4.9777679 -5.5263662 -5.9307318 -5.9799867 -5.3743467 -4.6470671 -3.7735913 -3.1982322 -2.7997713 -2.7358251 -2.9578223 -3.1755085][-4.1601324 -4.180378 -4.4104428 -4.9221754 -5.4777603 -5.854393 -5.91437 -5.4404192 -4.8257527 -4.0507164 -3.4862781 -2.9256577 -2.7311947 -2.7124596 -2.7440882]]...]
INFO - root - 2017-12-16 08:33:35.124577: step 61510, loss = 0.28, batch loss = 0.23 (29.0 examples/sec; 0.276 sec/batch; 20h:45m:50s remains)
INFO - root - 2017-12-16 08:33:37.930539: step 61520, loss = 0.24, batch loss = 0.18 (27.4 examples/sec; 0.292 sec/batch; 22h:00m:30s remains)
INFO - root - 2017-12-16 08:33:40.793763: step 61530, loss = 0.24, batch loss = 0.18 (27.6 examples/sec; 0.289 sec/batch; 21h:47m:00s remains)
INFO - root - 2017-12-16 08:33:43.627698: step 61540, loss = 0.27, batch loss = 0.21 (29.3 examples/sec; 0.273 sec/batch; 20h:33m:04s remains)
INFO - root - 2017-12-16 08:33:46.484220: step 61550, loss = 0.23, batch loss = 0.18 (27.0 examples/sec; 0.296 sec/batch; 22h:18m:47s remains)
INFO - root - 2017-12-16 08:33:49.281806: step 61560, loss = 0.25, batch loss = 0.19 (28.9 examples/sec; 0.277 sec/batch; 20h:50m:25s remains)
INFO - root - 2017-12-16 08:33:52.117684: step 61570, loss = 0.23, batch loss = 0.18 (27.9 examples/sec; 0.286 sec/batch; 21h:32m:30s remains)
INFO - root - 2017-12-16 08:33:54.978177: step 61580, loss = 0.30, batch loss = 0.25 (28.3 examples/sec; 0.282 sec/batch; 21h:14m:45s remains)
INFO - root - 2017-12-16 08:33:57.819521: step 61590, loss = 0.23, batch loss = 0.17 (28.8 examples/sec; 0.277 sec/batch; 20h:52m:55s remains)
INFO - root - 2017-12-16 08:34:00.653724: step 61600, loss = 0.27, batch loss = 0.21 (27.8 examples/sec; 0.288 sec/batch; 21h:40m:41s remains)
2017-12-16 08:34:01.205550: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0501881 -4.704267 -5.309968 -5.6566038 -5.8333631 -5.6364388 -5.4104977 -5.289392 -5.1587577 -4.9217181 -4.6019497 -4.5640125 -4.4939604 -4.1580219 -3.6096554][-5.0543242 -5.8595963 -6.4015493 -6.6703072 -6.796288 -6.7579517 -6.5079441 -5.8294506 -5.5242772 -5.8053918 -6.0323606 -5.92846 -5.6825867 -5.4567547 -4.967484][-6.0518556 -7.0130992 -7.4598236 -7.2222939 -6.7887487 -6.1185355 -5.6243858 -5.5263824 -5.64289 -5.68995 -5.9315929 -6.5748706 -7.0315504 -6.92867 -6.3897834][-6.5816851 -7.3964076 -7.497551 -6.8838806 -5.8630185 -4.6317072 -3.7387767 -3.193922 -3.406116 -4.3771868 -5.3539839 -6.1135769 -6.8031845 -7.2945623 -7.2663822][-6.4743071 -6.9007835 -6.4618039 -5.1310487 -3.4161103 -1.6185179 -0.42826176 -0.17140627 -0.6919508 -1.8711057 -3.4111228 -5.1700559 -6.4758749 -7.15159 -7.3751559][-5.5797429 -5.6685486 -4.8624597 -3.0442865 -0.62926841 1.6318803 3.0728693 3.4570217 2.6925488 0.86893034 -1.3270192 -3.6092753 -5.5168614 -6.6666894 -6.9426427][-4.327981 -4.0047517 -3.0424449 -1.3705671 0.93781328 3.3865342 5.0938988 5.4420948 4.6892767 2.935811 0.52933216 -2.134079 -4.4105663 -5.9038625 -6.4804029][-3.6459434 -3.0198734 -1.9752791 -0.38176584 1.4737902 3.6164455 5.314744 5.7723694 5.1691427 3.3128819 0.94521475 -1.5491028 -3.7424164 -5.3392544 -6.0926528][-2.7523279 -2.1681526 -1.2510653 -0.094660282 1.1914325 2.8542976 4.2268124 4.7593203 4.2637224 2.6881437 0.51073837 -1.9794557 -4.1390729 -5.5718107 -6.2119756][-2.9350438 -2.2129607 -1.4113014 -0.42014217 0.43762016 1.3382745 2.4336228 3.0176029 2.7360797 1.3304424 -0.61614728 -2.6730895 -4.4919024 -5.9452133 -6.68939][-3.2574422 -2.6128178 -2.0555074 -1.2972813 -0.54825878 0.18863869 0.67054033 1.1162338 1.0617795 0.16892767 -1.5918758 -3.5928545 -5.170496 -6.1387453 -6.6237669][-3.9297831 -2.8295197 -2.1120617 -1.6347435 -1.2572157 -0.84565163 -0.75104 -0.57215405 -0.65456176 -1.3641062 -2.7085376 -4.3340926 -5.6288157 -6.4555078 -6.7501249][-4.1865253 -3.0073056 -2.0937288 -1.6499026 -1.4437466 -1.168891 -1.0523572 -1.3258808 -1.6074996 -2.1074114 -3.2308927 -4.72643 -5.8547049 -6.3334351 -6.4547281][-4.1997461 -2.9029183 -1.729785 -1.2435086 -1.0346715 -1.0630867 -1.2537277 -1.4859095 -1.7504373 -2.2270238 -3.2378268 -4.4353905 -5.3139143 -5.9282455 -6.1615591][-3.7751884 -2.5784364 -1.3057976 -0.825722 -0.81862187 -0.7752645 -0.86908245 -1.2595165 -1.7396169 -2.0536337 -2.6774254 -3.8506794 -4.6933088 -5.0272231 -5.112607]]...]
INFO - root - 2017-12-16 08:34:04.032936: step 61610, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.282 sec/batch; 21h:11m:33s remains)
INFO - root - 2017-12-16 08:34:06.863549: step 61620, loss = 0.27, batch loss = 0.21 (27.6 examples/sec; 0.290 sec/batch; 21h:47m:16s remains)
INFO - root - 2017-12-16 08:34:09.681268: step 61630, loss = 0.23, batch loss = 0.17 (27.0 examples/sec; 0.296 sec/batch; 22h:16m:58s remains)
INFO - root - 2017-12-16 08:34:12.523430: step 61640, loss = 0.26, batch loss = 0.21 (27.3 examples/sec; 0.293 sec/batch; 22h:04m:40s remains)
INFO - root - 2017-12-16 08:34:15.388200: step 61650, loss = 0.35, batch loss = 0.29 (27.4 examples/sec; 0.292 sec/batch; 22h:00m:17s remains)
INFO - root - 2017-12-16 08:34:18.251419: step 61660, loss = 0.21, batch loss = 0.15 (28.6 examples/sec; 0.280 sec/batch; 21h:04m:17s remains)
INFO - root - 2017-12-16 08:34:21.040043: step 61670, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.281 sec/batch; 21h:09m:17s remains)
INFO - root - 2017-12-16 08:34:23.846942: step 61680, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.286 sec/batch; 21h:30m:39s remains)
INFO - root - 2017-12-16 08:34:26.632619: step 61690, loss = 0.22, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 21h:00m:05s remains)
INFO - root - 2017-12-16 08:34:29.466301: step 61700, loss = 0.34, batch loss = 0.28 (29.0 examples/sec; 0.276 sec/batch; 20h:46m:16s remains)
2017-12-16 08:34:29.952338: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6930213 -5.1249781 -5.1572838 -5.6986437 -6.5384197 -7.131228 -7.5049276 -7.8818789 -8.2794361 -8.2238426 -7.8057556 -7.2294436 -6.3097882 -5.2573633 -4.4449062][-4.8253865 -4.3740044 -4.7629328 -5.3091269 -5.95482 -6.9119039 -7.6344318 -7.9373264 -8.0424871 -8.1788273 -8.3097839 -8.026783 -7.2282181 -6.0758967 -5.2626982][-3.6273587 -3.0177252 -3.361407 -4.0523319 -4.6420903 -5.3299365 -5.9081492 -6.5854807 -6.8117914 -7.2979794 -7.7371984 -7.9075117 -7.5273228 -6.8438554 -6.06416][-2.3441226 -1.536624 -1.7321377 -2.4279659 -3.0914211 -3.5688195 -3.6829827 -4.1539583 -4.724442 -5.4462552 -6.2503676 -7.0200467 -7.309494 -6.7417879 -5.9210839][-1.289129 -0.39451313 -0.32509089 -0.85355926 -1.1933265 -1.2678063 -0.98354053 -1.1591194 -1.73191 -2.8106585 -4.1636744 -5.2587538 -5.89881 -6.0195818 -5.6368122][-0.9826746 0.13505173 0.62397718 0.88757038 1.1560025 1.3882093 1.7666478 1.9400978 1.5828142 0.24431705 -1.4685516 -3.3116369 -4.64672 -4.9860291 -4.4133797][-1.495168 0.019675255 0.93906355 1.7334328 2.514792 3.3029613 4.09665 4.1484394 3.608984 2.2116838 0.42594051 -1.6175144 -3.1621811 -4.0026069 -4.0387535][-2.5324697 -1.0924776 0.099352837 1.4138293 2.4675202 3.331285 4.1562967 4.5526409 4.4269371 3.0847759 1.2209306 -0.88475013 -2.5007985 -3.3633542 -3.5926187][-4.5736418 -3.2524762 -1.7051971 -0.12701797 0.88050747 2.1537271 3.2413297 3.7041397 3.6256933 2.7213221 1.1795826 -0.783324 -2.2669506 -3.3267174 -3.8056056][-5.5976763 -4.526875 -3.602026 -2.1802814 -0.865149 0.2405858 0.95970678 1.6166582 1.7709169 1.0898023 -0.12119532 -1.6428607 -2.8337126 -3.7227356 -4.3079309][-6.6407661 -5.92684 -5.1763854 -4.183588 -3.3913713 -2.5235243 -1.7063494 -1.1410425 -1.0245168 -1.2197566 -1.9252346 -2.9400132 -3.9001141 -4.5488081 -4.9803381][-6.7344856 -6.5868673 -6.4850559 -6.0569038 -5.5065107 -4.8862219 -4.4267378 -3.9313707 -3.4980173 -3.4257622 -3.7917848 -4.32028 -4.9462457 -5.313921 -5.3968306][-6.0025816 -6.103282 -6.3196545 -6.4097242 -6.4428415 -6.2445545 -5.980957 -5.5915432 -5.3328671 -5.1171427 -4.9182954 -5.0587959 -5.4729176 -5.7849045 -5.7818227][-4.7327948 -5.0215712 -5.4619703 -5.7735667 -5.9775758 -6.1238308 -6.2848682 -6.0752897 -5.7938628 -5.5934987 -5.4872713 -5.5054846 -5.66623 -5.6862087 -5.5223513][-4.27878 -4.3495774 -4.5821037 -4.9418859 -5.2996163 -5.5444951 -5.768918 -5.6751804 -5.5328155 -5.4405689 -5.3639441 -5.2267075 -5.1735706 -5.0643911 -4.7439485]]...]
INFO - root - 2017-12-16 08:34:32.817144: step 61710, loss = 0.30, batch loss = 0.24 (27.0 examples/sec; 0.297 sec/batch; 22h:18m:52s remains)
INFO - root - 2017-12-16 08:34:35.689705: step 61720, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 21h:31m:32s remains)
INFO - root - 2017-12-16 08:34:38.591482: step 61730, loss = 0.23, batch loss = 0.17 (27.3 examples/sec; 0.293 sec/batch; 22h:01m:23s remains)
INFO - root - 2017-12-16 08:34:41.449828: step 61740, loss = 0.30, batch loss = 0.24 (27.0 examples/sec; 0.297 sec/batch; 22h:18m:57s remains)
INFO - root - 2017-12-16 08:34:44.274600: step 61750, loss = 0.29, batch loss = 0.23 (28.9 examples/sec; 0.277 sec/batch; 20h:48m:05s remains)
INFO - root - 2017-12-16 08:34:47.074254: step 61760, loss = 0.21, batch loss = 0.15 (27.8 examples/sec; 0.288 sec/batch; 21h:37m:59s remains)
INFO - root - 2017-12-16 08:34:49.924878: step 61770, loss = 0.30, batch loss = 0.24 (27.4 examples/sec; 0.292 sec/batch; 21h:57m:32s remains)
INFO - root - 2017-12-16 08:34:52.822625: step 61780, loss = 0.28, batch loss = 0.23 (27.6 examples/sec; 0.290 sec/batch; 21h:49m:50s remains)
INFO - root - 2017-12-16 08:34:55.654544: step 61790, loss = 0.27, batch loss = 0.21 (27.6 examples/sec; 0.290 sec/batch; 21h:49m:26s remains)
INFO - root - 2017-12-16 08:34:58.534193: step 61800, loss = 0.24, batch loss = 0.18 (29.1 examples/sec; 0.275 sec/batch; 20h:40m:50s remains)
2017-12-16 08:34:59.059211: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1774373 -4.1997809 -4.6363983 -5.3345585 -5.3215914 -5.1112204 -4.738657 -4.6667051 -4.64041 -4.6226454 -4.8212967 -4.7751031 -4.716445 -4.3808494 -4.2872267][-4.4440265 -4.3847275 -4.5991826 -4.9086161 -4.9725561 -5.18125 -4.9162989 -4.6712346 -4.5191727 -4.4650788 -4.4963078 -4.4138079 -4.4473906 -3.9323273 -3.3984919][-4.1401734 -3.9482296 -4.1327877 -4.3298726 -4.1961465 -4.0343428 -3.7642281 -4.1257319 -4.368844 -4.5777936 -4.6076875 -4.4964738 -4.3530908 -3.9947612 -3.5411139][-3.1603742 -2.9560471 -2.9250152 -2.6832252 -2.4088821 -2.2627673 -1.8786516 -2.2529778 -2.9735928 -3.6754138 -3.9273648 -4.2809324 -4.5709276 -4.2124796 -3.602694][-1.9981511 -1.3877466 -0.80276918 -0.59366322 -0.34539604 0.052939415 0.32295418 -0.0505538 -0.85729432 -2.1164956 -3.1985376 -3.9183769 -4.1796827 -4.2712464 -4.094851][-1.7655506 -0.92370391 -0.06604147 1.1132197 2.2413888 2.5962715 2.7191329 2.256433 1.1227503 -0.30575418 -1.6357839 -3.2485948 -4.4886618 -4.6741652 -4.3101749][-2.0280683 -0.82360315 0.199821 1.5148649 3.0551672 4.3390856 5.1070566 4.3727894 2.9818478 1.1587725 -0.67891 -2.3801744 -3.7611103 -4.3877096 -4.5360541][-2.789609 -1.9472313 -0.72296166 1.1945672 3.0448403 4.3839636 5.2734318 4.8665953 3.6084423 1.599618 -0.22612429 -2.1527872 -3.7009544 -4.3904338 -4.7678151][-3.9245224 -3.6586313 -2.6019816 -0.72018838 1.1228604 3.0487494 4.4250879 4.1961851 3.0210176 1.2197957 -0.56274772 -2.39169 -3.8789554 -4.5894732 -4.9198594][-4.9665852 -5.4438562 -5.125205 -3.2515531 -1.1243448 0.63198423 2.0555725 2.2923036 1.5332136 -0.1179595 -1.8780975 -3.371829 -4.5912266 -4.9342718 -4.8803954][-6.3549056 -6.3459845 -5.8469744 -4.8607216 -3.4919505 -1.8640373 -0.57075644 -0.37012577 -0.99047542 -2.3114412 -3.6604714 -4.6549106 -5.2334852 -5.1530008 -4.7689562][-6.1999311 -6.5678005 -6.7633829 -5.81032 -4.6116891 -3.8708541 -3.2283077 -2.9199796 -2.9599214 -3.7517097 -4.6230741 -5.2184362 -5.5431423 -5.0916576 -4.3006039][-6.0400457 -5.7291985 -5.8893404 -5.7926235 -5.3931937 -4.6602345 -4.1211653 -4.2461085 -4.4107304 -4.8629942 -5.2793007 -5.2754841 -5.16541 -4.4415412 -3.575819][-5.8855963 -5.2707348 -5.114656 -4.958703 -4.8809114 -4.7101569 -4.5649242 -4.6000032 -4.7809486 -5.05995 -5.2526569 -5.0173893 -4.6816306 -3.863811 -2.7346802][-5.8540931 -4.9329 -4.5041914 -4.3351107 -4.4305673 -4.4980621 -4.5137014 -4.5560851 -4.5723839 -4.76557 -4.8481097 -4.4612236 -4.141562 -3.5288727 -2.6866884]]...]
INFO - root - 2017-12-16 08:35:01.916280: step 61810, loss = 0.32, batch loss = 0.26 (28.6 examples/sec; 0.280 sec/batch; 21h:03m:06s remains)
INFO - root - 2017-12-16 08:35:04.722688: step 61820, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.280 sec/batch; 21h:04m:26s remains)
INFO - root - 2017-12-16 08:35:07.595698: step 61830, loss = 0.23, batch loss = 0.17 (27.5 examples/sec; 0.291 sec/batch; 21h:51m:59s remains)
INFO - root - 2017-12-16 08:35:10.391022: step 61840, loss = 0.19, batch loss = 0.13 (28.3 examples/sec; 0.282 sec/batch; 21h:13m:16s remains)
INFO - root - 2017-12-16 08:35:13.288520: step 61850, loss = 0.25, batch loss = 0.19 (26.8 examples/sec; 0.298 sec/batch; 22h:25m:15s remains)
INFO - root - 2017-12-16 08:35:16.109589: step 61860, loss = 0.20, batch loss = 0.14 (29.1 examples/sec; 0.275 sec/batch; 20h:41m:53s remains)
INFO - root - 2017-12-16 08:35:18.917618: step 61870, loss = 0.22, batch loss = 0.16 (27.9 examples/sec; 0.287 sec/batch; 21h:33m:17s remains)
INFO - root - 2017-12-16 08:35:21.774118: step 61880, loss = 0.23, batch loss = 0.17 (26.8 examples/sec; 0.298 sec/batch; 22h:24m:24s remains)
INFO - root - 2017-12-16 08:35:24.620360: step 61890, loss = 0.27, batch loss = 0.21 (28.1 examples/sec; 0.285 sec/batch; 21h:24m:56s remains)
INFO - root - 2017-12-16 08:35:27.413349: step 61900, loss = 0.33, batch loss = 0.27 (27.2 examples/sec; 0.294 sec/batch; 22h:07m:17s remains)
2017-12-16 08:35:27.932795: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8164082 -3.9276295 -4.2056646 -4.3859129 -4.4359131 -4.38465 -4.2586308 -4.064158 -3.8760774 -3.7417226 -3.661525 -3.6324415 -3.4910166 -3.2818804 -3.1688609][-3.6046493 -3.9345903 -4.3155231 -4.5426769 -4.5724936 -4.4204679 -4.2164989 -3.9783623 -3.7567971 -3.67347 -3.689961 -3.5880787 -3.4125772 -3.4182382 -3.3932657][-3.4034348 -3.8218684 -4.1934795 -4.2262416 -4.0558691 -3.8216746 -3.5473356 -3.4050646 -3.3389947 -3.3424315 -3.4401736 -3.472187 -3.4879098 -3.51507 -3.5220351][-3.0460265 -3.3954592 -3.6223547 -3.3972447 -2.8678718 -2.3727307 -2.1220608 -1.9865651 -1.9028761 -2.2635505 -2.7473459 -3.0033462 -3.1723442 -3.4111395 -3.6312666][-2.717144 -2.7678294 -2.5891252 -1.9955583 -1.1098514 -0.39301729 0.069221973 0.13240433 -0.020408154 -0.45177841 -0.94130039 -1.6667097 -2.2911265 -2.7448735 -3.1373367][-2.154489 -1.816098 -1.3536363 -0.41224766 0.68945408 1.4917254 2.0289364 2.1763439 1.8704901 1.1822772 0.47560883 -0.3212266 -1.2006595 -1.9941442 -2.5914164][-1.9168012 -1.1565549 -0.29901075 0.88559103 2.0555925 2.9232841 3.502326 3.5819492 3.2546477 2.47188 1.5026579 0.53615713 -0.35118771 -1.2045181 -1.9478688][-1.8465493 -1.0842261 -0.27614927 0.92564631 2.0090637 2.8600059 3.5487642 3.7003984 3.3315682 2.4859195 1.6016788 0.58851957 -0.35504484 -0.979851 -1.5945623][-2.1550636 -1.5942585 -0.96399832 -0.11333466 0.581655 1.5354266 2.3673272 2.5960779 2.3835382 1.6802449 0.93679905 -0.025967121 -0.94244313 -1.4678864 -1.7535965][-2.7007823 -2.4188488 -2.2342281 -1.7022142 -1.0634708 -0.31394577 0.30835867 0.79950428 0.82130957 0.29947233 -0.35608673 -1.2283154 -1.9148123 -2.4116523 -2.7311091][-3.724731 -3.4707606 -3.3299494 -3.2050939 -2.9581728 -2.2632771 -1.5611949 -1.1568413 -1.1873484 -1.4873087 -2.0047824 -2.6481402 -3.1492691 -3.4729543 -3.5964847][-4.5982232 -4.7309642 -4.8753481 -4.5745277 -4.2306895 -3.8111863 -3.3534148 -2.9269083 -2.6785753 -2.7880163 -3.1358662 -3.6961927 -4.0454535 -4.1948996 -4.0537248][-5.1411638 -5.1516027 -5.2857118 -5.2464914 -5.0989485 -4.6303778 -4.1107 -3.7060537 -3.4723458 -3.5117571 -3.5844932 -3.8896635 -4.14384 -4.4023628 -4.3537974][-5.1836128 -5.0914946 -5.0303211 -4.9473124 -4.7738409 -4.4698114 -4.1234803 -3.780745 -3.495661 -3.3616519 -3.3539081 -3.6260104 -3.9573917 -4.1237788 -4.0426083][-5.1349978 -4.8048277 -4.6450009 -4.5393233 -4.2852087 -4.1110582 -3.938328 -3.6572218 -3.3373704 -3.1818266 -3.0796752 -3.1650152 -3.3670664 -3.5351272 -3.5568135]]...]
INFO - root - 2017-12-16 08:35:30.765954: step 61910, loss = 0.33, batch loss = 0.27 (28.5 examples/sec; 0.280 sec/batch; 21h:04m:00s remains)
INFO - root - 2017-12-16 08:35:33.624418: step 61920, loss = 0.48, batch loss = 0.42 (29.2 examples/sec; 0.274 sec/batch; 20h:33m:46s remains)
INFO - root - 2017-12-16 08:35:36.474434: step 61930, loss = 0.29, batch loss = 0.23 (27.2 examples/sec; 0.294 sec/batch; 22h:07m:29s remains)
INFO - root - 2017-12-16 08:35:39.306232: step 61940, loss = 0.30, batch loss = 0.24 (28.2 examples/sec; 0.284 sec/batch; 21h:20m:12s remains)
INFO - root - 2017-12-16 08:35:42.163545: step 61950, loss = 0.27, batch loss = 0.21 (27.6 examples/sec; 0.290 sec/batch; 21h:46m:14s remains)
INFO - root - 2017-12-16 08:35:45.021561: step 61960, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.282 sec/batch; 21h:10m:08s remains)
INFO - root - 2017-12-16 08:35:47.856962: step 61970, loss = 0.23, batch loss = 0.17 (29.2 examples/sec; 0.274 sec/batch; 20h:35m:30s remains)
INFO - root - 2017-12-16 08:35:50.690318: step 61980, loss = 0.21, batch loss = 0.15 (28.3 examples/sec; 0.283 sec/batch; 21h:16m:25s remains)
INFO - root - 2017-12-16 08:35:53.577037: step 61990, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.285 sec/batch; 21h:25m:10s remains)
INFO - root - 2017-12-16 08:35:56.438274: step 62000, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.286 sec/batch; 21h:27m:56s remains)
2017-12-16 08:35:56.992109: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4429326 -4.8737612 -5.1919274 -5.33607 -5.4001632 -5.3514633 -5.3154116 -5.35991 -5.6279759 -6.1016617 -6.4967279 -6.9266133 -7.0130634 -6.4789753 -5.6205082][-5.2412181 -5.6975241 -5.91787 -5.9339738 -5.9262733 -6.0141559 -6.1684189 -6.3582335 -6.6533079 -7.2902656 -8.10915 -8.8260288 -9.0391254 -8.5589561 -7.4939861][-5.6509476 -6.1201587 -6.2208633 -5.9783773 -5.6572843 -5.5070591 -5.5059085 -6.0845175 -6.9268732 -7.7965784 -8.7898149 -9.7982979 -10.512578 -10.334614 -9.3145876][-5.8170834 -6.092474 -5.9197044 -5.3181996 -4.6764426 -4.1482911 -3.9423866 -4.2967505 -5.0430636 -6.5938916 -8.2677364 -9.6434813 -10.667122 -11.097781 -10.610258][-5.27418 -5.2680984 -4.7669621 -3.8037777 -2.6203964 -1.4285486 -0.79120922 -0.95797515 -1.8019512 -3.6251755 -5.7766681 -8.114995 -9.9933453 -10.973792 -10.881485][-4.6002145 -4.2958031 -3.3635652 -1.7393548 0.19793224 1.6744804 2.6102333 2.8158636 2.1410017 0.14361954 -2.6093369 -5.6967096 -8.3539267 -9.94735 -10.294902][-4.3281379 -3.7369337 -2.5043693 -0.75303245 1.7513523 4.1865044 5.8915987 6.0649385 5.1150045 3.2012196 0.29174709 -3.2696295 -6.3885679 -8.3135395 -8.8204165][-4.5821991 -3.8946886 -2.6619592 -0.56946015 2.0502682 4.5901489 6.8428421 7.6546373 6.8860674 4.6671629 1.5466566 -1.9954765 -5.0956073 -6.9763832 -7.41766][-4.9675283 -4.6840134 -3.4967904 -1.5441489 0.68477058 3.2847824 5.579215 6.4103203 5.9924955 4.2650728 1.4637284 -1.772177 -4.5846562 -6.2781439 -6.5692511][-5.0425305 -5.3906279 -4.9753437 -3.4091263 -1.4909077 0.58251715 2.4764042 3.6718044 3.6128645 2.0667982 -0.042462349 -2.4544544 -4.7915492 -6.2725387 -6.6611748][-5.8687181 -6.2242479 -6.0504837 -5.1324358 -3.8052495 -2.3367224 -1.0342739 -0.19719839 -0.066299915 -0.83814383 -2.3285534 -4.0688529 -5.5338321 -6.4570742 -6.7476821][-6.2888613 -6.6273818 -6.8196611 -6.4174442 -5.4830556 -4.4290924 -3.6215894 -3.1597085 -3.1269889 -3.6473441 -4.5455689 -5.6360536 -6.5224571 -6.9413977 -6.9263945][-5.8719692 -6.2588372 -6.5496531 -6.4952168 -6.3449526 -5.7599621 -4.9803605 -4.6365314 -4.6039758 -4.769372 -5.3258257 -6.1418619 -6.6203375 -6.9070511 -6.8336172][-5.209713 -5.3252583 -5.4383206 -5.5538359 -5.7151375 -5.5532341 -5.3516574 -5.2442174 -5.182025 -5.2715683 -5.2926807 -5.4866543 -5.8807535 -6.0596418 -5.9609041][-4.4689856 -4.4055023 -4.2647176 -4.2721782 -4.4361591 -4.5112691 -4.568471 -4.574028 -4.5990658 -4.736454 -4.8669834 -4.7518563 -4.5352511 -4.5432744 -4.6105275]]...]
INFO - root - 2017-12-16 08:35:59.881399: step 62010, loss = 0.31, batch loss = 0.25 (27.6 examples/sec; 0.290 sec/batch; 21h:45m:54s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 08:36:02.721924: step 62020, loss = 0.28, batch loss = 0.23 (28.2 examples/sec; 0.284 sec/batch; 21h:19m:14s remains)
INFO - root - 2017-12-16 08:36:05.620669: step 62030, loss = 0.32, batch loss = 0.26 (28.3 examples/sec; 0.283 sec/batch; 21h:15m:37s remains)
INFO - root - 2017-12-16 08:36:08.502957: step 62040, loss = 0.26, batch loss = 0.20 (28.0 examples/sec; 0.286 sec/batch; 21h:28m:18s remains)
INFO - root - 2017-12-16 08:36:11.320095: step 62050, loss = 0.20, batch loss = 0.14 (29.1 examples/sec; 0.275 sec/batch; 20h:40m:34s remains)
INFO - root - 2017-12-16 08:36:14.130460: step 62060, loss = 0.27, batch loss = 0.22 (29.5 examples/sec; 0.271 sec/batch; 20h:22m:44s remains)
INFO - root - 2017-12-16 08:36:16.960795: step 62070, loss = 0.23, batch loss = 0.17 (29.7 examples/sec; 0.270 sec/batch; 20h:15m:00s remains)
INFO - root - 2017-12-16 08:36:19.843146: step 62080, loss = 0.24, batch loss = 0.18 (27.0 examples/sec; 0.297 sec/batch; 22h:16m:46s remains)
INFO - root - 2017-12-16 08:36:22.681462: step 62090, loss = 0.34, batch loss = 0.28 (27.7 examples/sec; 0.289 sec/batch; 21h:43m:53s remains)
INFO - root - 2017-12-16 08:36:25.571222: step 62100, loss = 0.34, batch loss = 0.28 (28.3 examples/sec; 0.282 sec/batch; 21h:12m:13s remains)
2017-12-16 08:36:26.116515: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3982444 -4.102787 -3.8667064 -3.4584212 -3.3348937 -3.0584755 -2.993669 -2.9976766 -3.0584569 -3.3253136 -3.7219944 -4.1014657 -4.1897168 -4.2541237 -4.0344934][-5.321476 -4.8163166 -4.4545012 -4.0399647 -3.9063146 -3.6167183 -3.5855691 -3.6128731 -3.5990667 -3.8688748 -4.0897274 -4.3098688 -4.2882233 -4.1894169 -3.9404154][-5.1552315 -4.7423377 -4.437377 -4.0010624 -3.9259443 -3.7605858 -3.7416713 -3.7608752 -3.9119835 -4.132823 -4.3592324 -4.7230105 -4.8207383 -4.7184267 -4.3684559][-4.3538523 -3.8149266 -3.5891864 -3.460844 -3.5055573 -3.4105284 -3.4589891 -3.5013463 -3.7064819 -3.9850843 -4.266809 -4.703124 -4.8968873 -4.956542 -4.7394676][-2.7741177 -2.2533667 -2.1949396 -2.3156769 -2.4405274 -2.3465598 -2.3144655 -2.4854002 -2.720993 -3.1526904 -3.7197795 -4.4337096 -4.8235259 -5.1351929 -4.8815713][-0.5303669 -0.16721582 -0.51554418 -0.8850534 -1.119648 -1.1145515 -0.9730618 -0.99624681 -1.1069245 -1.607281 -2.393321 -3.5416825 -4.470726 -4.9774675 -4.916234][1.1671591 1.3911495 0.80710316 0.074807644 -0.29766083 -0.066358566 0.38129854 0.63854361 0.67937994 0.098786831 -0.90515542 -2.2774661 -3.552058 -4.4037137 -4.4866381][1.3667231 1.4645138 0.98957968 0.4299221 0.3081274 0.8053751 1.3711853 1.8580489 2.0229206 1.4144568 0.23360825 -1.4430931 -3.0204802 -4.0082054 -4.2100763][0.12177038 0.46269083 0.054046631 -0.33877182 -0.071661472 0.72905159 1.5838008 2.3758221 2.6710243 2.1911726 0.9938221 -0.79803228 -2.5427709 -3.696033 -4.0544381][-1.6802938 -1.5377054 -1.7157538 -1.763907 -1.269532 -0.197927 0.86976957 1.8825703 2.2903042 1.87392 0.73472595 -0.87732625 -2.5559711 -3.8038146 -4.3639188][-3.3552623 -3.4042931 -3.5399549 -3.468076 -3.0171371 -1.5906947 -0.30784178 0.69533539 1.143991 0.91578722 -0.08278656 -1.5816481 -3.1370406 -4.3186326 -4.9674363][-4.9595208 -5.1462345 -5.2683115 -5.0955696 -4.5411658 -3.2164125 -1.9643576 -0.812587 -0.32834387 -0.47418356 -1.2658031 -2.4915085 -3.7591186 -4.788867 -5.2510171][-5.9251537 -6.1678886 -6.4597435 -6.5412803 -6.2859225 -5.2983236 -4.2254076 -3.0827792 -2.501626 -2.5118585 -2.944694 -3.8409185 -4.7489114 -5.4017596 -5.6036191][-5.89225 -6.2613978 -6.7203088 -7.0519981 -7.0537834 -6.4170055 -5.4882312 -4.4950285 -3.9126439 -3.7439344 -4.0252013 -4.642673 -5.2427926 -5.7234297 -5.7400966][-5.7554789 -6.2472148 -6.7728863 -7.110116 -7.2027073 -6.7897944 -5.9603491 -4.811914 -4.0724092 -3.9741988 -4.1449633 -4.5480022 -4.9242525 -5.4398184 -5.4655952]]...]
INFO - root - 2017-12-16 08:36:28.968497: step 62110, loss = 0.25, batch loss = 0.19 (27.0 examples/sec; 0.296 sec/batch; 22h:14m:39s remains)
INFO - root - 2017-12-16 08:36:31.803082: step 62120, loss = 0.27, batch loss = 0.22 (29.0 examples/sec; 0.276 sec/batch; 20h:41m:53s remains)
INFO - root - 2017-12-16 08:36:34.673278: step 62130, loss = 0.30, batch loss = 0.24 (27.8 examples/sec; 0.288 sec/batch; 21h:36m:16s remains)
INFO - root - 2017-12-16 08:36:37.546710: step 62140, loss = 0.33, batch loss = 0.27 (28.5 examples/sec; 0.281 sec/batch; 21h:06m:05s remains)
INFO - root - 2017-12-16 08:36:40.370042: step 62150, loss = 0.27, batch loss = 0.21 (28.3 examples/sec; 0.283 sec/batch; 21h:13m:29s remains)
INFO - root - 2017-12-16 08:36:43.194487: step 62160, loss = 0.47, batch loss = 0.41 (28.0 examples/sec; 0.286 sec/batch; 21h:26m:59s remains)
INFO - root - 2017-12-16 08:36:46.020093: step 62170, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.286 sec/batch; 21h:29m:29s remains)
INFO - root - 2017-12-16 08:36:48.895137: step 62180, loss = 0.26, batch loss = 0.20 (28.2 examples/sec; 0.284 sec/batch; 21h:19m:27s remains)
INFO - root - 2017-12-16 08:36:51.780169: step 62190, loss = 0.20, batch loss = 0.14 (27.6 examples/sec; 0.289 sec/batch; 21h:43m:35s remains)
INFO - root - 2017-12-16 08:36:54.544316: step 62200, loss = 0.23, batch loss = 0.17 (28.6 examples/sec; 0.280 sec/batch; 21h:01m:26s remains)
2017-12-16 08:36:55.106070: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7970881 -5.1907644 -5.2134213 -5.0465488 -4.8044043 -4.681046 -4.5920591 -4.7031708 -4.8397064 -5.0118737 -5.1738591 -5.3446312 -5.5346246 -5.4793463 -5.2295651][-5.3344517 -5.7697906 -5.7505484 -5.5482464 -5.0967846 -4.9012957 -4.8462105 -4.9593544 -5.2311287 -5.5334196 -5.8696604 -6.142364 -6.3729935 -6.3086257 -5.9799762][-5.6359053 -6.036478 -5.9951534 -5.5312138 -4.8618989 -4.4747109 -4.0839777 -4.2646012 -4.680398 -5.2073312 -5.8445725 -6.42266 -6.9374981 -7.047246 -6.853694][-5.1591129 -5.6834779 -5.5413222 -4.6398597 -3.4788904 -2.6994929 -2.2365348 -2.3575652 -2.8268685 -3.7857175 -4.8579922 -5.8695517 -6.6854753 -7.2092724 -7.3759851][-4.4034166 -4.5149522 -3.9709792 -2.9372418 -1.4149015 -0.0075612068 0.84624958 0.78111744 0.079620838 -1.196192 -2.611033 -4.0616097 -5.4170508 -6.3948669 -7.0446644][-3.770153 -3.7428873 -3.0876045 -1.4587188 0.73173809 2.6454921 3.838645 4.05132 3.4415593 1.9133921 0.14622116 -1.6837025 -3.4709802 -4.9849272 -6.110076][-3.538533 -3.2603648 -2.3758476 -0.550159 1.9116936 4.2545338 6.0772362 6.533246 5.8915005 4.2923613 2.3765454 0.2938242 -1.9369891 -3.7192585 -4.9762716][-3.608964 -3.3083868 -2.5526867 -0.6955626 2.0137091 4.6746092 6.6500015 7.2031565 6.7907715 5.1968746 3.2136455 1.1449609 -0.97873497 -2.9028606 -4.5066519][-4.3061595 -4.0903425 -3.2702084 -1.7274988 0.38417482 2.855216 5.0697479 5.9072886 5.6375542 4.3908434 2.909698 1.1227155 -0.94317985 -2.7282195 -4.2453437][-5.4234648 -5.5397935 -4.9735394 -3.6229336 -1.8503239 0.27262402 2.031354 2.884378 2.9491773 2.1771336 1.0876255 -0.22775698 -1.8046877 -3.2229266 -4.5375614][-6.7952514 -6.9790444 -6.5766683 -5.6721482 -4.5286469 -3.0091119 -1.3642049 -0.50539613 -0.3732152 -0.82747388 -1.427927 -2.1485662 -3.204679 -4.1889939 -5.2798042][-7.6862612 -8.0020666 -7.8849154 -7.294117 -6.4970078 -5.5103388 -4.5001788 -3.7845476 -3.3583002 -3.3558168 -3.5188007 -4.0496955 -4.71481 -5.3496103 -6.1260891][-8.21982 -8.4881191 -8.39007 -7.9244032 -7.3841629 -6.8349633 -6.1868639 -5.58914 -5.1619673 -4.9037514 -4.8486714 -5.0764828 -5.3207383 -5.7704492 -6.4479575][-8.1745777 -8.3168736 -8.1601458 -7.8025508 -7.4848089 -7.0222931 -6.4832678 -6.2364836 -5.9947805 -5.5440702 -5.1447968 -5.276403 -5.6539841 -5.850913 -6.0181837][-7.1907063 -7.2523384 -6.9545856 -6.6570096 -6.480072 -6.2335572 -5.9968576 -5.799017 -5.6536326 -5.4565215 -5.1610637 -5.0740376 -5.1143088 -5.234777 -5.3976979]]...]
INFO - root - 2017-12-16 08:36:57.928960: step 62210, loss = 0.27, batch loss = 0.21 (28.7 examples/sec; 0.279 sec/batch; 20h:57m:47s remains)
INFO - root - 2017-12-16 08:37:00.788128: step 62220, loss = 0.20, batch loss = 0.14 (27.8 examples/sec; 0.288 sec/batch; 21h:37m:24s remains)
INFO - root - 2017-12-16 08:37:03.678906: step 62230, loss = 0.33, batch loss = 0.27 (28.3 examples/sec; 0.283 sec/batch; 21h:15m:28s remains)
INFO - root - 2017-12-16 08:37:06.496153: step 62240, loss = 0.21, batch loss = 0.15 (27.9 examples/sec; 0.287 sec/batch; 21h:31m:22s remains)
INFO - root - 2017-12-16 08:37:09.334498: step 62250, loss = 0.20, batch loss = 0.14 (28.5 examples/sec; 0.281 sec/batch; 21h:03m:36s remains)
INFO - root - 2017-12-16 08:37:12.195665: step 62260, loss = 0.21, batch loss = 0.15 (27.1 examples/sec; 0.296 sec/batch; 22h:11m:32s remains)
INFO - root - 2017-12-16 08:37:15.071309: step 62270, loss = 0.30, batch loss = 0.24 (27.8 examples/sec; 0.288 sec/batch; 21h:37m:20s remains)
INFO - root - 2017-12-16 08:37:17.949652: step 62280, loss = 0.36, batch loss = 0.30 (27.1 examples/sec; 0.295 sec/batch; 22h:07m:45s remains)
INFO - root - 2017-12-16 08:37:20.832946: step 62290, loss = 0.23, batch loss = 0.17 (29.0 examples/sec; 0.276 sec/batch; 20h:43m:07s remains)
INFO - root - 2017-12-16 08:37:23.718899: step 62300, loss = 0.30, batch loss = 0.24 (27.9 examples/sec; 0.287 sec/batch; 21h:31m:57s remains)
2017-12-16 08:37:24.264712: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4513726 -2.5091655 -2.5730295 -2.7200136 -2.9133041 -2.7960579 -2.6235149 -2.3765051 -2.289031 -1.9857199 -1.7288225 -1.7212462 -1.5704191 -1.5818181 -1.5958645][-1.605443 -1.7475445 -1.9766259 -2.2101867 -2.428997 -2.3462889 -2.1046734 -1.8815725 -1.782428 -1.688971 -1.5811639 -1.7501554 -1.7108467 -1.6425076 -1.4611254][-1.1884637 -1.5541794 -1.8830981 -2.1920741 -2.3630614 -2.2078142 -1.7667482 -1.3354661 -1.1909909 -1.2602603 -1.5435119 -2.0789137 -2.3149216 -2.2855554 -1.950907][-1.2315524 -1.7350042 -2.2333114 -2.4968512 -2.5014572 -2.0548856 -1.2913249 -0.63588548 -0.42937279 -0.72610831 -1.3756738 -2.2162004 -2.9337912 -3.0363722 -2.5285597][-1.5523562 -2.0780194 -2.4454327 -2.4789455 -2.2497785 -1.4393187 -0.29481411 0.602489 0.72879267 0.21307087 -0.82414031 -2.0765553 -2.966033 -3.4482024 -3.2832277][-2.1154299 -2.55892 -2.703043 -2.3705273 -1.4824286 -0.19351578 1.3114295 2.4662428 2.5775547 1.5497909 -0.068778992 -1.8751667 -3.3165693 -4.0792928 -4.0666366][-2.8571219 -3.12017 -2.98274 -2.1204016 -0.75686169 1.0312805 2.8138614 3.9142132 3.9726343 2.799953 0.79838181 -1.4864283 -3.3572884 -4.4815621 -4.78159][-3.5431201 -3.3609347 -3.0475986 -1.9778273 -0.28093863 1.727242 3.5334053 4.5610352 4.4152231 2.9664855 0.88218546 -1.5067244 -3.5312138 -4.9391842 -5.39255][-4.1993 -3.8009162 -3.0134671 -1.8323724 -0.34090281 1.5523787 3.1350136 3.9243631 3.6132326 2.0784259 -0.066234589 -2.35713 -4.2829142 -5.6736298 -6.2143307][-4.5511684 -4.2062287 -3.6075397 -2.4325218 -1.0720754 0.23565197 1.3356013 2.0098991 1.6778183 0.2039814 -1.6911898 -3.73707 -5.3664141 -6.4650769 -6.8373528][-4.5236187 -4.3544531 -4.0184159 -3.3148785 -2.306222 -1.2438455 -0.70797324 -0.45792246 -0.71558571 -1.637325 -3.0987015 -4.6839523 -5.8997364 -6.7006578 -6.8777704][-3.90605 -3.8951049 -3.9935513 -3.7500134 -3.2703052 -2.5313144 -2.0503891 -1.9078054 -2.2236774 -2.7702255 -3.5645993 -4.6639032 -5.5624585 -6.1391754 -6.28977][-3.238929 -3.4940052 -3.8420177 -4.0024519 -3.8963737 -3.4489853 -3.0046527 -2.6374807 -2.66645 -2.9445581 -3.3809004 -3.8937447 -4.4085178 -4.980865 -5.2751327][-2.8326185 -3.3065329 -3.80836 -4.1660953 -4.0835991 -3.6230857 -3.1343699 -2.5686107 -2.2893507 -2.189532 -2.4052944 -2.8092861 -3.045651 -3.2886553 -3.5530894][-2.4137971 -2.9351993 -3.5579009 -4.0329852 -3.9024978 -3.3141532 -2.5891728 -1.8717542 -1.2856889 -0.85122824 -0.87616181 -1.167284 -1.4618998 -1.7976725 -2.0953672]]...]
INFO - root - 2017-12-16 08:37:27.127745: step 62310, loss = 0.25, batch loss = 0.20 (28.9 examples/sec; 0.277 sec/batch; 20h:47m:26s remains)
INFO - root - 2017-12-16 08:37:30.005473: step 62320, loss = 0.20, batch loss = 0.14 (28.3 examples/sec; 0.283 sec/batch; 21h:15m:02s remains)
INFO - root - 2017-12-16 08:37:32.844481: step 62330, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.282 sec/batch; 21h:11m:35s remains)
INFO - root - 2017-12-16 08:37:35.667211: step 62340, loss = 0.33, batch loss = 0.27 (28.3 examples/sec; 0.282 sec/batch; 21h:10m:55s remains)
INFO - root - 2017-12-16 08:37:38.501870: step 62350, loss = 0.31, batch loss = 0.26 (28.5 examples/sec; 0.281 sec/batch; 21h:03m:24s remains)
INFO - root - 2017-12-16 08:37:41.305630: step 62360, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.280 sec/batch; 21h:02m:30s remains)
INFO - root - 2017-12-16 08:37:44.150973: step 62370, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.279 sec/batch; 20h:58m:02s remains)
INFO - root - 2017-12-16 08:37:46.984762: step 62380, loss = 0.25, batch loss = 0.19 (28.9 examples/sec; 0.277 sec/batch; 20h:48m:21s remains)
INFO - root - 2017-12-16 08:37:49.810587: step 62390, loss = 0.26, batch loss = 0.20 (29.2 examples/sec; 0.274 sec/batch; 20h:34m:17s remains)
INFO - root - 2017-12-16 08:37:52.631942: step 62400, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.283 sec/batch; 21h:15m:00s remains)
2017-12-16 08:37:53.184288: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7586408 -2.950748 -3.2242603 -3.2200732 -3.0974522 -2.9526591 -2.821027 -3.0926063 -3.3592968 -3.4044895 -3.2439623 -3.2489495 -3.3320618 -3.4420147 -3.4587331][-3.00984 -2.8955817 -3.0674183 -3.2149196 -2.9376249 -2.8427298 -2.9271798 -2.930037 -3.1703138 -3.503988 -3.7247267 -3.762542 -3.7841582 -4.0622592 -4.2388625][-3.0302424 -3.112318 -3.2627192 -3.3099751 -3.1199455 -2.8884168 -2.8013637 -2.8760056 -3.1208763 -3.2957802 -3.5390077 -3.8761592 -4.1329422 -4.5465174 -4.6824136][-3.1950088 -3.2311196 -3.4104278 -3.285912 -2.8572989 -2.5965741 -2.2494173 -1.9595325 -2.0583634 -2.4944026 -2.963661 -3.3772185 -3.8412135 -4.4532146 -4.5796814][-3.132617 -3.2654936 -3.2644598 -2.9861438 -2.4240429 -1.7878146 -1.2481933 -0.811038 -0.6561842 -0.96691966 -1.6115921 -2.3268485 -3.08359 -3.7251222 -4.1016531][-3.3720627 -3.3424976 -3.2236719 -2.6288891 -1.7246158 -0.89364815 -0.16352272 0.56831884 0.84049177 0.56826067 -0.059033394 -0.89834309 -1.7640309 -2.7073226 -3.4247355][-3.43198 -3.4256787 -3.1089063 -2.2698488 -1.0715809 -0.12994432 0.84703493 1.7012391 2.1819396 2.0266876 1.5145459 0.67927504 -0.32364464 -1.3923006 -2.359025][-3.07298 -3.1984358 -2.9269767 -2.1451349 -0.90863013 0.14814711 1.1668963 2.1229506 2.7960734 2.8134093 2.4748778 1.7737675 0.90928173 -0.35505438 -1.5703576][-2.9435127 -2.9529705 -2.8893621 -2.2522781 -1.228415 -0.26866293 0.60545397 1.5517426 2.3592958 2.6305027 2.6815119 2.1952891 1.4206266 0.20750046 -1.1032877][-2.8953657 -2.8865414 -2.9891164 -2.7083697 -2.0739157 -1.2941732 -0.44982004 0.37029266 1.1131797 1.3828778 1.6127949 1.3871856 0.74893665 -0.43416524 -1.5849671][-2.87425 -3.0320885 -3.2298706 -3.1617291 -2.9385948 -2.4373164 -1.711659 -1.0276179 -0.33757114 0.053275108 0.37361002 0.23288345 -0.21751451 -1.2339356 -2.2061064][-3.0624347 -3.1280186 -3.3845925 -3.6432109 -3.6287324 -3.2032347 -2.7122495 -2.2686403 -1.7593422 -1.556623 -1.238637 -1.0539303 -1.4465406 -2.158891 -2.8773983][-3.0235267 -3.052845 -3.0864017 -3.0643649 -3.0980396 -3.0403051 -2.7112319 -2.486042 -2.3157594 -2.2369246 -2.1139953 -1.9149446 -2.0654573 -2.5754824 -3.0439363][-3.0697503 -2.7716188 -2.3980255 -2.3122711 -2.0674138 -1.9506485 -1.9828773 -2.1586463 -2.3095932 -2.4819214 -2.5284276 -2.2650404 -2.2380486 -2.3854496 -2.8163722][-2.9361649 -2.2809112 -1.5253205 -1.0691483 -0.71463943 -0.65329647 -0.78213096 -1.4038982 -2.0536757 -2.3696635 -2.592963 -2.3344328 -2.3006513 -2.1966252 -2.385144]]...]
INFO - root - 2017-12-16 08:37:56.001797: step 62410, loss = 0.20, batch loss = 0.14 (28.5 examples/sec; 0.281 sec/batch; 21h:03m:13s remains)
INFO - root - 2017-12-16 08:37:58.891465: step 62420, loss = 0.30, batch loss = 0.24 (27.8 examples/sec; 0.287 sec/batch; 21h:33m:20s remains)
INFO - root - 2017-12-16 08:38:01.709527: step 62430, loss = 0.26, batch loss = 0.20 (29.8 examples/sec; 0.268 sec/batch; 20h:06m:39s remains)
INFO - root - 2017-12-16 08:38:04.518063: step 62440, loss = 0.23, batch loss = 0.18 (27.3 examples/sec; 0.293 sec/batch; 21h:56m:41s remains)
INFO - root - 2017-12-16 08:38:07.379170: step 62450, loss = 0.21, batch loss = 0.15 (27.8 examples/sec; 0.288 sec/batch; 21h:34m:41s remains)
INFO - root - 2017-12-16 08:38:10.229773: step 62460, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.284 sec/batch; 21h:18m:40s remains)
INFO - root - 2017-12-16 08:38:13.034633: step 62470, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 21h:17m:53s remains)
INFO - root - 2017-12-16 08:38:15.880234: step 62480, loss = 0.32, batch loss = 0.26 (29.3 examples/sec; 0.273 sec/batch; 20h:29m:19s remains)
INFO - root - 2017-12-16 08:38:18.731367: step 62490, loss = 0.46, batch loss = 0.40 (28.5 examples/sec; 0.281 sec/batch; 21h:03m:03s remains)
INFO - root - 2017-12-16 08:38:21.518973: step 62500, loss = 0.23, batch loss = 0.17 (29.5 examples/sec; 0.272 sec/batch; 20h:21m:53s remains)
2017-12-16 08:38:22.059319: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.75369596 -0.96470284 -1.3730223 -1.6690416 -2.0683618 -2.4631505 -2.7699509 -2.5846541 -2.3244748 -2.0691526 -1.7756913 -1.3885925 -1.482805 -2.0352488 -2.4549174][-1.8774128 -2.0908937 -2.3900061 -2.6153398 -2.8680348 -2.97128 -2.8795152 -2.8539147 -2.8298779 -2.5242434 -2.3536453 -2.1698797 -2.2784343 -2.7239776 -3.1659498][-3.4374161 -3.6703482 -3.8460708 -3.8645945 -3.5615757 -3.3265486 -3.1467652 -2.9553657 -3.0303509 -2.9797935 -3.0957394 -3.2355187 -3.5797906 -3.851136 -3.8828585][-4.8110113 -4.9142003 -4.707715 -4.1845236 -3.3260255 -2.5677071 -2.0755861 -1.8127916 -1.9984517 -2.3922455 -3.0088825 -3.687794 -4.1976337 -4.4275513 -4.30737][-5.7515364 -5.62424 -5.1539645 -4.0047588 -2.6148415 -1.3209562 -0.29190779 -0.16258097 -0.53888893 -1.3113098 -2.45061 -3.4682264 -4.3843136 -4.8001876 -4.5296707][-5.90044 -5.56876 -4.845232 -3.5168965 -1.6361477 0.025228977 1.1825228 1.5442791 1.0192413 -0.062217712 -1.4910769 -2.9573684 -4.07113 -4.584126 -4.7451177][-5.0689111 -4.6762686 -3.7602115 -2.2144716 -0.17269945 1.6481433 2.8805184 3.0953941 2.2707529 0.79283333 -0.96592832 -2.5485616 -3.9207473 -4.7458878 -4.643343][-4.3476758 -3.9142764 -2.9999876 -1.2873914 0.80303049 2.7991619 4.1363945 4.2859526 3.4323282 1.6745496 -0.2789464 -2.1258419 -3.5442646 -4.5027881 -4.9368434][-3.6925564 -3.3194289 -2.682622 -1.3450158 0.477952 2.3267508 3.5850821 3.781909 3.1643991 1.6706748 -0.10851669 -1.8536584 -3.2809508 -4.0947094 -4.5039816][-2.8750896 -2.8231061 -2.6504846 -1.971772 -0.70400238 0.70054293 1.7110319 2.0015502 1.5973177 0.52220249 -0.8543694 -2.2682528 -3.4415658 -4.1451945 -4.5094981][-2.1019785 -2.3274221 -2.5683017 -2.3646877 -1.620213 -0.81274152 -0.10823298 0.24099016 0.027200222 -0.69391274 -1.7047458 -2.6749365 -3.3928211 -4.0588412 -4.5082045][-1.7376146 -2.0373309 -2.5537565 -2.7195163 -2.4311776 -1.9630656 -1.5551593 -1.2220633 -1.1033533 -1.4143324 -2.1438689 -2.8598943 -3.2943544 -3.7537479 -4.1976752][-1.4909074 -1.9003589 -2.3899493 -2.8567033 -3.0959158 -2.9064069 -2.5246024 -2.1457703 -1.8262947 -1.8246658 -2.1551645 -2.6713727 -3.1024332 -3.556478 -3.9660401][-0.85125256 -1.233084 -1.9016037 -2.536726 -2.8409834 -3.0433488 -3.0580933 -2.661665 -2.214009 -2.0570946 -2.1751308 -2.4397101 -2.7961278 -3.1137645 -3.4159353][-0.35532856 -0.5161891 -1.1275823 -1.8699934 -2.3239877 -2.5451865 -2.6639547 -2.5974779 -2.3550858 -2.0309129 -2.0275736 -2.2876184 -2.4777155 -2.7986453 -3.0587273]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-62500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-62500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 08:38:25.684353: step 62510, loss = 0.25, batch loss = 0.19 (27.8 examples/sec; 0.288 sec/batch; 21h:34m:07s remains)
INFO - root - 2017-12-16 08:38:28.509377: step 62520, loss = 0.22, batch loss = 0.16 (28.2 examples/sec; 0.284 sec/batch; 21h:17m:44s remains)
INFO - root - 2017-12-16 08:38:31.415111: step 62530, loss = 0.44, batch loss = 0.38 (27.2 examples/sec; 0.294 sec/batch; 22h:04m:28s remains)
INFO - root - 2017-12-16 08:38:34.280409: step 62540, loss = 0.25, batch loss = 0.20 (29.0 examples/sec; 0.276 sec/batch; 20h:39m:42s remains)
INFO - root - 2017-12-16 08:38:37.132874: step 62550, loss = 0.28, batch loss = 0.22 (26.2 examples/sec; 0.305 sec/batch; 22h:53m:01s remains)
INFO - root - 2017-12-16 08:38:39.959641: step 62560, loss = 0.28, batch loss = 0.22 (28.6 examples/sec; 0.280 sec/batch; 20h:57m:48s remains)
INFO - root - 2017-12-16 08:38:42.756996: step 62570, loss = 0.21, batch loss = 0.15 (28.6 examples/sec; 0.280 sec/batch; 20h:59m:14s remains)
INFO - root - 2017-12-16 08:38:45.583041: step 62580, loss = 0.36, batch loss = 0.30 (28.4 examples/sec; 0.281 sec/batch; 21h:06m:05s remains)
INFO - root - 2017-12-16 08:38:48.436167: step 62590, loss = 0.28, batch loss = 0.23 (29.6 examples/sec; 0.271 sec/batch; 20h:17m:04s remains)
INFO - root - 2017-12-16 08:38:51.286264: step 62600, loss = 0.25, batch loss = 0.19 (25.3 examples/sec; 0.316 sec/batch; 23h:39m:45s remains)
2017-12-16 08:38:51.806057: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3223498 -2.8348036 -3.4106593 -3.6420894 -3.7025993 -3.8004303 -3.8841097 -3.8620763 -4.0499563 -4.13373 -4.2972775 -4.3796439 -4.3662243 -4.0313463 -3.3733938][-1.7499676 -2.2944982 -3.0412004 -3.5478868 -3.897665 -3.8533452 -3.6506467 -3.4119043 -3.5594931 -3.7521935 -3.863991 -3.8749692 -3.8646126 -3.4820719 -3.0277648][-1.8194506 -2.548094 -3.1804588 -3.6381371 -4.021359 -3.7310679 -3.3563504 -3.0194926 -2.8737602 -3.1182299 -3.2988164 -3.5202854 -3.6203773 -3.1568408 -2.577003][-1.6098328 -2.5561221 -3.37099 -3.618077 -3.5618837 -3.0934925 -2.4619541 -1.8531384 -1.6591463 -1.9629099 -2.1864245 -2.5451589 -2.8876767 -2.8901358 -2.4789171][-2.1555121 -2.584024 -2.8175087 -2.8208 -2.6605918 -1.7147818 -0.3970418 0.41674185 0.4886775 0.17606163 -0.2304678 -1.1725862 -1.9110041 -2.3013129 -2.2972498][-2.7414699 -3.1673381 -3.2200727 -2.6449895 -1.5153711 -0.15201187 1.3486328 2.6088037 2.9639 2.2845297 1.4383368 0.3006444 -0.7381115 -1.4426289 -1.8846478][-3.1140978 -3.1738181 -2.9792371 -2.1888902 -0.86708617 1.0020528 3.1700168 4.3764496 4.6997004 4.148777 3.1958861 1.7234344 0.1619873 -0.7810061 -1.2188566][-4.4886594 -4.2283764 -3.7018049 -2.4123325 -0.64105296 1.5009451 3.6956711 4.9831152 5.4093065 4.662344 3.5061555 2.1395459 0.72422457 -0.47436 -1.3685098][-6.0153728 -5.9054432 -5.2439909 -3.7711303 -1.8997996 0.48250723 2.5786543 3.692379 4.0990038 3.4728985 2.3863449 0.91887379 -0.35245228 -1.0494499 -1.6383216][-6.9809494 -7.1511745 -6.9670296 -5.8952909 -4.1508412 -1.9483318 -0.136662 0.99601841 1.3204966 0.91901255 0.095541954 -1.2320542 -2.3357038 -2.6192164 -2.7861295][-7.5842924 -7.749114 -7.6251206 -7.0877571 -6.1467819 -4.6660104 -3.0013523 -2.1090486 -1.8938229 -2.2079232 -2.8491857 -3.6914055 -4.3680725 -4.4308481 -4.252449][-7.309185 -7.5525713 -7.6793461 -7.3348722 -6.6456876 -5.6912174 -4.8131766 -4.3963256 -4.06935 -4.1912208 -4.7011895 -5.4398823 -5.7976694 -5.5421672 -5.2058811][-5.9937677 -6.34452 -6.7333765 -6.75796 -6.5977097 -6.0282493 -5.4714508 -5.2767816 -5.0695677 -5.1751246 -5.4137745 -5.8117223 -6.0099969 -5.6787033 -5.2503543][-4.9077482 -5.0441661 -5.2763386 -5.6194792 -5.8504238 -5.5882263 -5.26021 -5.204443 -5.1241651 -5.1196823 -5.1356893 -5.3573523 -5.5641632 -5.3445263 -4.9306717][-3.9641867 -4.0354705 -4.1883588 -4.4078617 -4.5625286 -4.5883565 -4.5151305 -4.4406247 -4.3343287 -4.4116654 -4.5957241 -4.6087651 -4.6018519 -4.6050029 -4.4678092]]...]
INFO - root - 2017-12-16 08:38:54.675906: step 62610, loss = 0.27, batch loss = 0.21 (27.6 examples/sec; 0.290 sec/batch; 21h:45m:55s remains)
INFO - root - 2017-12-16 08:38:57.484584: step 62620, loss = 0.27, batch loss = 0.21 (28.8 examples/sec; 0.278 sec/batch; 20h:49m:21s remains)
INFO - root - 2017-12-16 08:39:00.342572: step 62630, loss = 0.28, batch loss = 0.22 (27.3 examples/sec; 0.293 sec/batch; 21h:56m:41s remains)
INFO - root - 2017-12-16 08:39:03.172679: step 62640, loss = 0.20, batch loss = 0.14 (29.2 examples/sec; 0.274 sec/batch; 20h:32m:38s remains)
INFO - root - 2017-12-16 08:39:06.055492: step 62650, loss = 0.20, batch loss = 0.14 (27.7 examples/sec; 0.289 sec/batch; 21h:39m:18s remains)
INFO - root - 2017-12-16 08:39:08.935938: step 62660, loss = 0.28, batch loss = 0.22 (28.8 examples/sec; 0.278 sec/batch; 20h:48m:53s remains)
INFO - root - 2017-12-16 08:39:11.780423: step 62670, loss = 0.26, batch loss = 0.20 (29.3 examples/sec; 0.273 sec/batch; 20h:26m:40s remains)
INFO - root - 2017-12-16 08:39:14.653821: step 62680, loss = 0.27, batch loss = 0.21 (28.3 examples/sec; 0.283 sec/batch; 21h:12m:20s remains)
INFO - root - 2017-12-16 08:39:17.498848: step 62690, loss = 0.35, batch loss = 0.29 (28.7 examples/sec; 0.279 sec/batch; 20h:53m:39s remains)
INFO - root - 2017-12-16 08:39:20.348836: step 62700, loss = 0.34, batch loss = 0.28 (27.7 examples/sec; 0.289 sec/batch; 21h:38m:34s remains)
2017-12-16 08:39:20.936721: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4819694 -2.6608324 -2.8734698 -3.1431437 -3.37658 -3.6981463 -4.1021166 -4.6758609 -4.8969851 -4.7582235 -4.3232465 -3.7804704 -2.990356 -2.153949 -1.769877][-2.2622361 -2.5472684 -2.7927828 -3.0855856 -3.22413 -3.3360338 -3.5741334 -3.9514902 -4.0340009 -3.9068594 -3.4618807 -2.7987452 -2.0550203 -1.2495468 -0.90197086][-2.3313944 -2.7172604 -2.9983637 -3.2273188 -3.16606 -2.9505098 -2.9764383 -3.2455468 -3.308409 -3.1691895 -2.9593511 -2.427804 -1.6361299 -0.81865835 -0.55153632][-2.3933704 -2.8208997 -3.0863047 -3.0170276 -2.5699258 -1.9888947 -1.6859102 -1.9075832 -2.0554895 -2.1985743 -2.1525249 -1.9043055 -1.5909016 -1.1028347 -0.94485712][-2.9344258 -3.1612418 -2.9498265 -2.3458691 -1.456233 -0.40600014 0.29195213 0.18207979 -0.30092859 -0.96563292 -1.366152 -1.4268568 -1.3890214 -1.1859255 -1.2667029][-3.2308536 -3.2029266 -2.676343 -1.5004213 0.025264263 1.547123 2.4495301 2.4134016 1.780623 0.7537961 -0.18420506 -0.73878407 -1.1545293 -1.2370696 -1.2926829][-3.37079 -3.2098777 -2.3107502 -0.63937283 1.2027731 3.1219745 4.4491053 4.4681511 3.4324937 1.8652153 0.48114061 -0.43675208 -0.99578238 -1.194504 -1.3529458][-3.5430682 -3.2952948 -2.1994352 -0.22703934 1.9279094 3.8947105 5.169693 5.3288126 4.3181782 2.3499179 0.55792475 -0.65245533 -1.3012097 -1.3297985 -1.3645182][-3.5387654 -3.4204133 -2.4250195 -0.40514708 1.7216687 3.5410547 4.7028217 4.7365427 3.6421633 1.671247 -0.12483788 -1.3066492 -1.8305275 -1.7751739 -1.5374844][-3.4415624 -3.4608672 -2.7498124 -1.2071588 0.58987093 2.3266864 3.2813282 3.11134 2.0170212 0.13447952 -1.5045886 -2.8267696 -3.2764015 -2.9058549 -2.5735135][-3.629812 -3.6396966 -3.0583014 -1.7425795 -0.43019557 0.77916336 1.3427773 0.94028139 -0.1403079 -1.7404099 -3.2810626 -4.4291015 -4.8407412 -4.3823218 -3.7843668][-4.13295 -4.1789303 -3.8566322 -2.8544354 -1.6730793 -0.72425127 -0.60777617 -1.2721295 -2.2412689 -3.5539484 -4.8061895 -5.8789339 -6.2208786 -5.4825726 -4.541554][-4.4963775 -4.3382049 -4.1419005 -3.6468492 -3.0824342 -2.380383 -2.1772826 -2.7996142 -3.666497 -4.6668696 -5.5711374 -6.5336041 -6.7273989 -6.0935493 -4.9077449][-4.7633009 -4.2852163 -3.9225628 -3.6648057 -3.3773232 -2.8602729 -2.6382008 -3.3032217 -4.2903471 -5.1157308 -5.7758512 -6.4728384 -6.5040603 -5.5664372 -4.3345246][-4.693881 -4.1619964 -3.7618341 -3.654969 -3.5556042 -3.145447 -2.8878112 -3.1399574 -3.8560534 -4.6792407 -5.55498 -6.2215466 -6.3157883 -5.3847136 -3.9100342]]...]
INFO - root - 2017-12-16 08:39:23.808988: step 62710, loss = 0.26, batch loss = 0.20 (27.6 examples/sec; 0.290 sec/batch; 21h:44m:17s remains)
INFO - root - 2017-12-16 08:39:26.668292: step 62720, loss = 0.22, batch loss = 0.16 (28.9 examples/sec; 0.277 sec/batch; 20h:44m:02s remains)
INFO - root - 2017-12-16 08:39:29.554270: step 62730, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 21h:05m:49s remains)
INFO - root - 2017-12-16 08:39:32.362408: step 62740, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.284 sec/batch; 21h:17m:59s remains)
INFO - root - 2017-12-16 08:39:35.203557: step 62750, loss = 0.25, batch loss = 0.19 (28.3 examples/sec; 0.283 sec/batch; 21h:11m:10s remains)
INFO - root - 2017-12-16 08:39:38.033582: step 62760, loss = 0.34, batch loss = 0.28 (26.8 examples/sec; 0.298 sec/batch; 22h:21m:44s remains)
INFO - root - 2017-12-16 08:39:40.872669: step 62770, loss = 0.30, batch loss = 0.24 (29.2 examples/sec; 0.274 sec/batch; 20h:30m:35s remains)
INFO - root - 2017-12-16 08:39:43.761448: step 62780, loss = 0.23, batch loss = 0.17 (27.3 examples/sec; 0.293 sec/batch; 21h:56m:06s remains)
INFO - root - 2017-12-16 08:39:46.622142: step 62790, loss = 0.22, batch loss = 0.17 (28.2 examples/sec; 0.283 sec/batch; 21h:13m:36s remains)
INFO - root - 2017-12-16 08:39:49.436652: step 62800, loss = 0.30, batch loss = 0.24 (29.0 examples/sec; 0.276 sec/batch; 20h:38m:34s remains)
2017-12-16 08:39:49.947669: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3388829 -3.9998355 -3.8365006 -3.7595811 -3.5384624 -3.5247834 -3.7801878 -3.868331 -3.6155119 -3.5033579 -3.3399506 -3.1394658 -2.8658175 -2.6249208 -2.865844][-2.9945607 -2.6437421 -2.6698432 -2.8165088 -2.6619809 -2.5069084 -2.5967517 -2.8010361 -2.6543984 -2.7260299 -2.6479077 -2.5758636 -2.3857269 -1.9732022 -1.9501143][-1.4310021 -1.1617789 -1.252805 -1.4944413 -1.4889054 -1.5299034 -1.7308092 -2.031599 -1.9346237 -2.2991626 -2.7734876 -2.730346 -2.5042379 -2.224077 -1.9997067][0.10346699 0.33266783 0.0025258064 -0.26451015 -0.2626009 -0.5235908 -0.74966955 -1.0493727 -1.3985953 -2.0253072 -2.4830832 -2.8276734 -3.1121068 -2.7719705 -2.3297877][0.74242496 0.99015188 0.76245975 0.55821371 0.5619669 0.359488 0.26856327 -0.042737484 -0.33342218 -1.1089621 -2.0221615 -2.6927257 -3.2690394 -3.1687949 -2.7762411][0.60864735 1.0291171 1.0020642 1.1420469 1.4672098 1.4339285 1.5847902 1.4154735 1.1191936 0.0033097267 -1.3155575 -2.2086604 -3.2406006 -3.6769166 -3.5685635][0.099250317 0.68304634 0.92879772 1.3405576 1.8625302 2.2585163 2.6657009 2.5598321 2.2961369 1.2048898 -0.12427568 -1.4961276 -2.8627248 -3.6896241 -4.1141567][-0.78544736 -0.081588268 0.23120356 0.89904976 1.8105221 2.4740319 3.0347042 3.0827703 2.821053 1.7425103 0.3119297 -1.0696697 -2.4645863 -3.5121379 -4.2251506][-1.8241682 -1.0995564 -0.59526944 0.084301472 0.99688721 1.7591844 2.2375994 2.2292552 1.9374433 0.95153713 -0.28040648 -1.6744051 -3.0185332 -3.9118245 -4.4565268][-2.8011243 -2.0177913 -1.3096831 -0.730829 -0.052469254 0.49660921 0.6035037 0.31936884 -0.052733421 -0.73651385 -1.7291751 -3.099241 -4.2361531 -4.8708277 -5.15573][-3.2657409 -2.5653875 -2.0648162 -1.6807775 -1.2679806 -0.90855408 -0.88322234 -1.4185691 -2.0150521 -2.7335949 -3.5723858 -4.6659813 -5.5418196 -5.9568667 -6.0534725][-3.4653995 -2.8766265 -2.5402646 -2.4644544 -2.5576406 -2.6117382 -2.9122806 -3.566072 -4.1040039 -4.5393314 -5.073185 -5.7814236 -6.3181133 -6.541193 -6.506906][-3.395153 -2.8163106 -2.5373263 -2.7285972 -3.2600691 -3.6620343 -4.1488996 -4.7467232 -5.2269354 -5.48244 -5.6305256 -5.9442925 -6.353075 -6.5001564 -6.4653645][-3.1358993 -2.643554 -2.3644328 -2.6565909 -3.153656 -3.6538224 -4.3109241 -4.8393321 -5.0883908 -5.144495 -4.9393272 -5.0315642 -5.51084 -5.8120208 -6.0407181][-2.8684165 -2.4054036 -2.2360003 -2.328166 -2.9402738 -3.6427763 -4.2179389 -4.6809297 -4.8675532 -4.6127024 -4.1279244 -3.9367397 -4.1158805 -4.4462047 -4.9704189]]...]
INFO - root - 2017-12-16 08:39:52.794804: step 62810, loss = 0.24, batch loss = 0.18 (27.6 examples/sec; 0.290 sec/batch; 21h:44m:21s remains)
INFO - root - 2017-12-16 08:39:55.643803: step 62820, loss = 0.22, batch loss = 0.16 (25.9 examples/sec; 0.309 sec/batch; 23h:07m:08s remains)
INFO - root - 2017-12-16 08:39:58.523530: step 62830, loss = 0.33, batch loss = 0.27 (25.6 examples/sec; 0.313 sec/batch; 23h:24m:49s remains)
INFO - root - 2017-12-16 08:40:01.413156: step 62840, loss = 0.32, batch loss = 0.26 (27.8 examples/sec; 0.287 sec/batch; 21h:31m:19s remains)
INFO - root - 2017-12-16 08:40:04.209375: step 62850, loss = 0.22, batch loss = 0.16 (28.7 examples/sec; 0.279 sec/batch; 20h:53m:38s remains)
INFO - root - 2017-12-16 08:40:07.085588: step 62860, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.285 sec/batch; 21h:19m:31s remains)
INFO - root - 2017-12-16 08:40:09.956043: step 62870, loss = 0.23, batch loss = 0.17 (27.1 examples/sec; 0.296 sec/batch; 22h:08m:12s remains)
INFO - root - 2017-12-16 08:40:12.803272: step 62880, loss = 0.23, batch loss = 0.17 (28.8 examples/sec; 0.278 sec/batch; 20h:48m:20s remains)
INFO - root - 2017-12-16 08:40:15.651595: step 62890, loss = 0.31, batch loss = 0.25 (27.0 examples/sec; 0.296 sec/batch; 22h:09m:59s remains)
INFO - root - 2017-12-16 08:40:18.505127: step 62900, loss = 0.28, batch loss = 0.22 (27.7 examples/sec; 0.289 sec/batch; 21h:38m:47s remains)
2017-12-16 08:40:19.011428: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1122189 -5.1292734 -4.8612661 -4.9847846 -5.13947 -5.3434567 -5.7486324 -6.7000585 -7.5482244 -7.5776453 -6.9825816 -5.9628153 -4.8977637 -3.9844136 -3.60814][-5.7616549 -5.37496 -4.690115 -4.3782644 -4.3169613 -4.5506468 -5.0904322 -6.1072097 -7.367322 -8.033596 -7.8773146 -7.0916786 -6.0547953 -5.0396595 -4.582478][-6.3496222 -5.8369355 -4.8687348 -3.9731431 -3.3637795 -3.1956897 -3.536253 -4.6432109 -6.0456681 -7.2177849 -7.9886742 -7.8533268 -7.1075535 -5.9357939 -5.1297646][-6.5803003 -5.9030666 -4.7213092 -3.5266545 -2.3446658 -1.5498888 -1.4233067 -2.1706176 -3.6578026 -5.3127694 -6.710156 -7.5904818 -7.48974 -6.5349774 -5.6091223][-5.5420012 -4.6990676 -3.3794541 -1.9626262 -0.48246717 0.715827 1.4083529 1.0054312 -0.44762182 -2.3372147 -4.4594913 -5.9679289 -6.4727364 -6.3078108 -5.3719993][-4.2908468 -3.2655611 -1.778029 -0.11383915 1.566834 3.1303039 4.1328964 4.2287207 3.1999588 0.93830824 -1.5776081 -3.7824337 -5.0805292 -5.5251927 -5.1310511][-3.6300778 -2.3909485 -0.91257691 0.9807601 2.9230957 4.7607594 6.1590919 6.6835546 6.0651875 3.8956013 0.91086292 -2.0066972 -3.9403629 -4.87449 -4.7004461][-3.851819 -2.7578998 -1.358222 0.68554068 2.7340851 4.7755003 6.4403658 7.2320623 6.9975529 4.953723 2.0307283 -1.0144827 -3.3610744 -4.5779057 -4.6220784][-4.5597014 -3.6567521 -2.3906751 -0.8793757 0.94124794 3.0241513 4.7723455 5.7361956 5.6337366 4.005722 1.3734264 -1.4488206 -3.5361052 -4.7306585 -4.7925496][-5.8813348 -5.3029833 -4.5969739 -3.3850427 -1.728888 0.0012331009 1.4840684 2.5236073 2.5531535 1.4100714 -0.46829724 -2.7401986 -4.35831 -5.1303163 -5.16928][-7.3700185 -7.1857805 -6.6280661 -5.7465544 -4.9692321 -3.593358 -2.2202423 -1.6608801 -1.6635101 -2.2016668 -3.2707539 -4.52102 -5.3421116 -5.7070394 -5.5834885][-8.3786993 -8.5160494 -8.3326826 -7.8389425 -7.2537165 -6.3316317 -5.6513391 -5.1840739 -5.024899 -5.2381825 -5.4667482 -5.8638635 -6.0561152 -6.0382848 -5.8398829][-8.0822477 -8.6021957 -8.7637663 -8.8284941 -8.8564425 -8.5346718 -8.129281 -7.8168831 -7.5648952 -7.1960678 -6.7540269 -6.4469624 -6.0841441 -5.9097581 -5.7250309][-6.918149 -7.3617311 -7.698801 -8.1395311 -8.5779533 -8.743 -8.8768063 -8.7930269 -8.43396 -7.7962222 -6.9597979 -6.3440838 -5.8573403 -5.39174 -5.1428061][-5.5821071 -5.8259788 -6.2169294 -6.6607676 -7.1544361 -7.6228552 -7.8913431 -7.9162865 -7.6393118 -7.0707593 -6.330966 -5.5384493 -4.8798237 -4.60472 -4.4250097]]...]
INFO - root - 2017-12-16 08:40:21.851739: step 62910, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 21h:26m:00s remains)
INFO - root - 2017-12-16 08:40:24.726906: step 62920, loss = 0.22, batch loss = 0.16 (28.2 examples/sec; 0.283 sec/batch; 21h:12m:28s remains)
INFO - root - 2017-12-16 08:40:27.607928: step 62930, loss = 0.32, batch loss = 0.26 (27.1 examples/sec; 0.295 sec/batch; 22h:05m:16s remains)
INFO - root - 2017-12-16 08:40:30.458244: step 62940, loss = 0.30, batch loss = 0.24 (27.7 examples/sec; 0.289 sec/batch; 21h:37m:25s remains)
INFO - root - 2017-12-16 08:40:33.266898: step 62950, loss = 0.31, batch loss = 0.25 (27.6 examples/sec; 0.290 sec/batch; 21h:41m:41s remains)
INFO - root - 2017-12-16 08:40:36.142106: step 62960, loss = 0.22, batch loss = 0.17 (27.5 examples/sec; 0.291 sec/batch; 21h:49m:12s remains)
INFO - root - 2017-12-16 08:40:39.046552: step 62970, loss = 0.22, batch loss = 0.16 (28.9 examples/sec; 0.277 sec/batch; 20h:44m:29s remains)
INFO - root - 2017-12-16 08:40:41.862088: step 62980, loss = 0.24, batch loss = 0.18 (29.7 examples/sec; 0.269 sec/batch; 20h:09m:55s remains)
INFO - root - 2017-12-16 08:40:44.693669: step 62990, loss = 0.22, batch loss = 0.16 (28.2 examples/sec; 0.283 sec/batch; 21h:12m:21s remains)
INFO - root - 2017-12-16 08:40:47.508057: step 63000, loss = 0.27, batch loss = 0.21 (27.5 examples/sec; 0.291 sec/batch; 21h:47m:36s remains)
2017-12-16 08:40:48.017584: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.9099712 -7.073791 -7.1212254 -6.6855803 -6.3000178 -5.7094073 -5.1644917 -4.8058777 -4.6695809 -5.074894 -5.5069513 -6.303977 -7.209147 -7.8054266 -7.7913132][-7.732523 -8.1942654 -8.3006725 -7.8392248 -7.2410245 -6.3740635 -5.4275961 -4.8161845 -4.4957738 -5.071867 -5.8047562 -6.6018491 -7.34318 -8.0372868 -8.0329323][-8.2729158 -8.689784 -8.5944862 -7.9805574 -7.2259188 -6.1158543 -4.8502407 -4.1542253 -3.8671744 -4.2321959 -5.0295272 -6.019567 -7.1281109 -7.7697773 -7.7521954][-7.3468976 -7.7443666 -7.4437647 -6.5422854 -5.4628735 -4.3963532 -3.2182326 -2.7606454 -2.6215377 -2.9779606 -3.5899253 -4.4562087 -5.3816957 -6.2872505 -6.5589504][-5.8816566 -6.0090876 -5.5523562 -4.6441751 -3.5790083 -2.315824 -1.1201053 -0.55439281 -0.39173269 -0.88107419 -1.7205081 -2.4247832 -3.3028171 -4.394845 -4.95767][-4.4576869 -3.9482017 -2.8680212 -1.6011994 -0.26090622 0.66898632 1.3419414 1.5849628 1.5204039 0.91417837 -0.014665127 -0.84110641 -1.8862891 -2.9796712 -3.6106286][-3.1316946 -2.3713048 -1.2276087 0.35646391 2.1105056 3.211925 3.7750607 3.5247431 2.9837666 2.1237097 1.1060128 -0.070914268 -1.2061851 -2.5582585 -3.4643681][-2.8954077 -1.9576812 -0.63285995 0.90621376 2.4152517 3.5313568 4.2250881 4.1221056 3.4246778 2.3021441 0.99029684 -0.35827971 -1.6575589 -2.9930205 -3.8574538][-3.5766246 -2.7224891 -1.4349065 -0.13785744 1.2620811 2.4017544 3.1228395 3.0744786 2.429872 1.2860241 0.010296822 -1.4183524 -2.7227201 -3.86664 -4.4907408][-4.2125292 -3.9049659 -3.28611 -2.3166316 -1.0974987 -0.061512947 0.69193935 0.84065437 0.35633469 -0.59807754 -1.6646428 -3.0481887 -4.2641931 -5.0516343 -5.2815294][-5.8883796 -5.5792689 -4.9216027 -4.4814124 -3.9505744 -2.9660912 -2.0611808 -1.7646332 -1.8882551 -2.4470925 -3.1369824 -4.1831112 -5.1170573 -5.5193839 -5.349175][-7.0544949 -6.9903708 -6.7995558 -6.4374285 -5.8918343 -5.2200389 -4.7358384 -4.309711 -3.9874384 -4.1132126 -4.3461657 -4.8011708 -5.2075682 -5.1969986 -4.92261][-7.6961823 -7.7508192 -7.9063854 -7.8499775 -7.6843495 -7.2198582 -6.7425127 -6.3592815 -6.1012793 -5.8015938 -5.5429125 -5.491858 -5.5236349 -5.1672583 -4.6409822][-7.7020445 -7.5661016 -7.660152 -7.6957922 -7.639945 -7.4246836 -7.1525869 -6.940485 -6.6730423 -6.3121109 -6.0139685 -5.7361155 -5.5332327 -5.1174364 -4.6032748][-7.1942825 -6.7549238 -6.5761714 -6.6605568 -6.7736578 -6.8117452 -6.7576694 -6.6283193 -6.3820553 -6.0511427 -5.8052344 -5.4896278 -5.3016367 -5.0121794 -4.5998626]]...]
INFO - root - 2017-12-16 08:40:50.885725: step 63010, loss = 0.24, batch loss = 0.18 (27.7 examples/sec; 0.289 sec/batch; 21h:39m:02s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 08:40:53.754681: step 63020, loss = 0.33, batch loss = 0.27 (27.9 examples/sec; 0.287 sec/batch; 21h:28m:04s remains)
INFO - root - 2017-12-16 08:40:56.586015: step 63030, loss = 0.24, batch loss = 0.18 (27.4 examples/sec; 0.292 sec/batch; 21h:50m:41s remains)
INFO - root - 2017-12-16 08:40:59.411524: step 63040, loss = 0.22, batch loss = 0.16 (28.2 examples/sec; 0.283 sec/batch; 21h:13m:09s remains)
INFO - root - 2017-12-16 08:41:02.204777: step 63050, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.281 sec/batch; 21h:03m:26s remains)
INFO - root - 2017-12-16 08:41:04.990086: step 63060, loss = 0.26, batch loss = 0.20 (27.0 examples/sec; 0.296 sec/batch; 22h:09m:55s remains)
INFO - root - 2017-12-16 08:41:07.862423: step 63070, loss = 0.27, batch loss = 0.22 (29.1 examples/sec; 0.275 sec/batch; 20h:35m:41s remains)
INFO - root - 2017-12-16 08:41:10.713718: step 63080, loss = 0.34, batch loss = 0.29 (28.5 examples/sec; 0.281 sec/batch; 21h:02m:00s remains)
INFO - root - 2017-12-16 08:41:13.495486: step 63090, loss = 0.26, batch loss = 0.20 (27.8 examples/sec; 0.288 sec/batch; 21h:31m:41s remains)
INFO - root - 2017-12-16 08:41:16.350070: step 63100, loss = 0.30, batch loss = 0.24 (28.6 examples/sec; 0.279 sec/batch; 20h:54m:30s remains)
2017-12-16 08:41:16.860646: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0570011 -3.8442569 -3.7458711 -4.0695596 -4.3599858 -4.2980742 -4.0981245 -4.0678506 -4.1255145 -3.9229174 -3.7354376 -3.6385691 -3.7124457 -3.9368446 -4.0101509][-3.7693582 -3.7094562 -3.735574 -3.9015636 -4.0569415 -4.1702247 -4.1378 -3.9985545 -3.8176203 -3.8244796 -3.8297491 -3.5397031 -3.358999 -3.5360305 -3.6223209][-3.3859243 -3.312727 -3.3548598 -3.5182445 -3.5852337 -3.5062394 -3.3865142 -3.3125789 -3.2639375 -3.382256 -3.4824827 -3.2286906 -3.0519338 -2.9802804 -2.85049][-2.590553 -2.6133103 -2.7677202 -2.8659911 -2.7442565 -2.5430007 -2.4910553 -2.6224992 -2.6891322 -2.86811 -2.9981189 -2.7857013 -2.6736357 -2.4317288 -2.1967344][-1.9938054 -2.0468645 -2.0101209 -1.9845817 -1.7783904 -1.4551888 -1.3833232 -1.5229158 -1.8635855 -2.3134608 -2.5902388 -2.4673567 -2.0462945 -1.6969266 -1.5292175][-1.5027235 -1.5641468 -1.4837141 -1.1652532 -0.5774796 -0.16566038 -0.13054895 -0.40262747 -0.82898426 -1.4898326 -1.9168622 -1.9194193 -1.6016319 -1.1923678 -0.92831945][-1.1568899 -1.1159644 -0.9386394 -0.52216339 0.14678764 0.74168682 1.0651336 0.62718821 -0.039084435 -0.58645129 -0.98539472 -1.1856334 -1.1141598 -0.66795158 -0.28545046][-0.98757482 -0.86533022 -0.53549981 -0.0511775 0.42722559 0.950459 1.2013674 0.83094168 0.26006222 -0.37648582 -0.78883481 -0.77382374 -0.4853828 -0.012630939 0.1273036][-0.815531 -0.80399394 -0.49278426 -0.046766758 0.23286295 0.53440428 0.71220779 0.41604233 -0.072387218 -0.58371496 -0.76523113 -0.88540983 -0.78036833 -0.20166636 0.19742346][-0.80225325 -0.76007175 -0.79299092 -0.59999537 -0.44954944 -0.35520029 -0.38176298 -0.54614973 -0.86994123 -1.1286945 -1.0241857 -0.99174595 -0.80729532 -0.35929441 -0.052083015][-0.86257434 -0.81278634 -0.90267158 -1.0965841 -1.3753464 -1.5015118 -1.3521528 -1.4486637 -1.7859504 -1.8180993 -1.5606439 -1.3672464 -1.0862985 -0.70661521 -0.52918029][-0.91810036 -0.98400259 -1.2478919 -1.4470034 -1.7496173 -2.1636038 -2.3757367 -2.4844174 -2.469578 -2.4789824 -2.3605497 -2.1277502 -1.8274271 -1.3788013 -0.96952534][-0.94281316 -0.80052233 -1.1644826 -1.5445535 -2.0436709 -2.3991451 -2.5245872 -2.6938941 -2.8563886 -2.9173741 -2.8350549 -2.7879992 -2.5318518 -2.0591493 -1.6027365][-1.1688311 -0.78968406 -0.85419774 -1.1784205 -1.7357459 -2.072854 -2.3697681 -2.6335697 -2.7041979 -2.6824164 -2.6186166 -2.761179 -2.7035096 -2.2699645 -1.6736917][-1.0750148 -0.71130443 -0.65031362 -0.88419008 -1.2046275 -1.4935749 -1.7859607 -2.1060104 -2.3153429 -2.4462328 -2.5322578 -2.5294619 -2.3286047 -1.8412852 -1.4670212]]...]
INFO - root - 2017-12-16 08:41:19.649837: step 63110, loss = 0.35, batch loss = 0.29 (27.9 examples/sec; 0.287 sec/batch; 21h:28m:32s remains)
INFO - root - 2017-12-16 08:41:22.471748: step 63120, loss = 0.28, batch loss = 0.22 (27.7 examples/sec; 0.289 sec/batch; 21h:38m:35s remains)
INFO - root - 2017-12-16 08:41:25.289511: step 63130, loss = 0.25, batch loss = 0.19 (29.1 examples/sec; 0.275 sec/batch; 20h:36m:07s remains)
INFO - root - 2017-12-16 08:41:28.118312: step 63140, loss = 0.33, batch loss = 0.27 (28.1 examples/sec; 0.284 sec/batch; 21h:15m:53s remains)
INFO - root - 2017-12-16 08:41:30.958968: step 63150, loss = 0.30, batch loss = 0.24 (28.2 examples/sec; 0.284 sec/batch; 21h:15m:37s remains)
INFO - root - 2017-12-16 08:41:33.784501: step 63160, loss = 0.33, batch loss = 0.27 (29.5 examples/sec; 0.271 sec/batch; 20h:16m:45s remains)
INFO - root - 2017-12-16 08:41:36.617146: step 63170, loss = 0.35, batch loss = 0.29 (28.6 examples/sec; 0.280 sec/batch; 20h:56m:56s remains)
INFO - root - 2017-12-16 08:41:39.454158: step 63180, loss = 0.21, batch loss = 0.15 (29.6 examples/sec; 0.270 sec/batch; 20h:11m:41s remains)
INFO - root - 2017-12-16 08:41:42.311959: step 63190, loss = 0.27, batch loss = 0.21 (29.3 examples/sec; 0.273 sec/batch; 20h:24m:38s remains)
INFO - root - 2017-12-16 08:41:45.185544: step 63200, loss = 0.29, batch loss = 0.23 (27.8 examples/sec; 0.288 sec/batch; 21h:30m:56s remains)
2017-12-16 08:41:45.708745: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4028378 -3.83685 -4.14446 -4.2326083 -4.1137843 -3.8924458 -3.7582223 -3.840621 -3.9319956 -4.1762676 -4.4604855 -4.5599322 -4.5089712 -4.3706031 -4.0993433][-4.005568 -4.5510297 -5.0334711 -5.068459 -4.8976569 -4.6311264 -4.4678774 -4.426106 -4.3395944 -4.3183532 -4.6539092 -4.8865328 -4.9610963 -4.8127127 -4.5142407][-5.0118146 -5.833415 -6.3374882 -6.2651205 -5.7907591 -5.2229981 -4.6687584 -4.3125286 -4.1404171 -4.3574371 -4.9719229 -5.390193 -5.5986462 -5.4115472 -5.1641784][-5.922112 -6.6669788 -6.997509 -6.7130394 -5.8876805 -4.8843083 -3.8923371 -3.4002275 -3.2274837 -3.4876802 -4.2634678 -5.2617269 -5.9142656 -5.9389496 -5.5361748][-6.8479586 -7.3295 -7.2014589 -6.344007 -5.0632663 -3.6429963 -2.2598903 -1.4434595 -1.2218668 -1.7352867 -2.7663224 -3.9063365 -4.9768724 -5.4514437 -5.3359337][-6.2670164 -6.5228424 -6.0616827 -4.7865744 -2.9950089 -1.4225273 -0.035831451 0.70483303 0.94095087 0.4624877 -0.731302 -2.1578333 -3.6045036 -4.4429436 -4.7725835][-4.6857753 -4.7377124 -4.0486708 -2.7504139 -0.90927887 0.88660145 2.5295463 3.2070789 3.2126212 2.423563 1.057384 -0.53522706 -2.1704924 -3.1297562 -3.6813822][-3.3081985 -3.0891387 -2.3816478 -1.2986648 0.29311275 2.0087714 3.6808071 4.4148006 4.399663 3.550621 2.0935864 0.26175261 -1.7052987 -2.7561946 -3.2428317][-1.9726696 -1.7046833 -1.1246955 -0.37949753 0.60352135 1.9745464 3.397275 4.1912718 4.30443 3.4626484 2.0137081 0.1281085 -1.845876 -3.0630584 -3.54273][-1.3000815 -0.98554707 -0.92555928 -0.600698 0.17078495 1.1496491 2.0916319 2.8801389 3.0771637 2.4295964 1.1112423 -0.58654666 -2.3590672 -3.5995111 -4.0461011][-1.631968 -1.3034952 -1.22476 -1.2847767 -1.0398726 -0.27037621 0.59231997 1.2840929 1.4624677 0.96876955 -0.1365037 -1.5781646 -3.1293645 -4.028522 -4.2959919][-2.4995124 -2.3131654 -2.3614554 -2.1799507 -1.9538698 -1.5579314 -1.0717454 -0.47324228 -0.16070795 -0.50444913 -1.2857478 -2.3623362 -3.4372368 -4.0893564 -4.196095][-2.9270573 -2.8144627 -2.99894 -2.9414296 -2.7422807 -2.371583 -2.0619745 -1.6580136 -1.3512619 -1.5523014 -2.1614494 -2.7052884 -3.1598277 -3.5375137 -3.5977116][-3.6262672 -3.455965 -3.5051985 -3.3431501 -3.091939 -2.8695695 -2.7091432 -2.4228003 -2.3039567 -2.4271042 -2.5610974 -2.6885867 -2.7141743 -2.680716 -2.6215591][-3.9250836 -3.7757459 -3.7540593 -3.7386551 -3.6421056 -3.3016229 -3.1588469 -3.024662 -2.9211183 -2.9399133 -2.87326 -2.6316061 -2.3463776 -2.0526552 -1.7188983]]...]
INFO - root - 2017-12-16 08:41:48.535483: step 63210, loss = 0.39, batch loss = 0.33 (28.7 examples/sec; 0.279 sec/batch; 20h:52m:48s remains)
INFO - root - 2017-12-16 08:41:51.361342: step 63220, loss = 0.31, batch loss = 0.25 (28.8 examples/sec; 0.278 sec/batch; 20h:48m:21s remains)
INFO - root - 2017-12-16 08:41:54.248261: step 63230, loss = 0.27, batch loss = 0.22 (27.4 examples/sec; 0.292 sec/batch; 21h:51m:01s remains)
INFO - root - 2017-12-16 08:41:57.108452: step 63240, loss = 0.25, batch loss = 0.19 (28.8 examples/sec; 0.278 sec/batch; 20h:47m:48s remains)
INFO - root - 2017-12-16 08:41:59.940376: step 63250, loss = 0.21, batch loss = 0.15 (28.1 examples/sec; 0.284 sec/batch; 21h:16m:06s remains)
INFO - root - 2017-12-16 08:42:02.780598: step 63260, loss = 0.21, batch loss = 0.16 (27.4 examples/sec; 0.292 sec/batch; 21h:51m:38s remains)
INFO - root - 2017-12-16 08:42:05.593856: step 63270, loss = 0.34, batch loss = 0.28 (27.9 examples/sec; 0.287 sec/batch; 21h:28m:17s remains)
INFO - root - 2017-12-16 08:42:08.483551: step 63280, loss = 0.25, batch loss = 0.20 (26.8 examples/sec; 0.298 sec/batch; 22h:17m:32s remains)
INFO - root - 2017-12-16 08:42:11.372969: step 63290, loss = 0.43, batch loss = 0.37 (27.1 examples/sec; 0.295 sec/batch; 22h:05m:31s remains)
INFO - root - 2017-12-16 08:42:14.237164: step 63300, loss = 0.22, batch loss = 0.16 (26.7 examples/sec; 0.300 sec/batch; 22h:25m:57s remains)
2017-12-16 08:42:14.816641: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8736644 -4.9861536 -5.1494932 -5.4098377 -5.6992378 -5.8387375 -5.866456 -5.8313942 -5.6383414 -5.4045472 -5.1662211 -5.1582794 -5.3913 -5.7483845 -5.9777727][-3.9479017 -3.9191356 -4.0276523 -4.347311 -4.8278689 -5.4685531 -5.8458433 -5.7213 -5.5139971 -5.4217649 -5.3419576 -5.332397 -5.5273781 -5.9558682 -6.1851282][-2.5931919 -2.5824323 -2.7859676 -3.1525633 -3.4998052 -3.9798946 -4.352376 -4.5530581 -4.5685673 -4.5232716 -4.7591009 -5.1664076 -5.5328145 -6.0000134 -6.1898365][-1.4066188 -1.2677069 -1.2874498 -1.5164685 -2.0098679 -2.441247 -2.8839846 -2.900136 -2.8706727 -3.1660876 -3.5629 -4.3065715 -5.0347977 -5.5544868 -5.8428221][-0.66096449 -0.28832912 -0.18514585 -0.17080498 -0.3123455 -0.4663415 -0.57035685 -0.67125463 -0.68521953 -1.1550045 -1.9246068 -3.1114902 -4.1286483 -4.6175432 -4.8241324][-0.39023495 0.085739136 0.3970623 0.73748016 0.87030649 1.0689044 1.4627376 1.5166574 1.4890676 0.91830444 -0.071388721 -1.4091425 -2.7485762 -3.5165305 -3.9704938][-0.73687029 0.141963 0.72217655 1.4203234 2.1675596 2.7939119 3.4472842 3.7020102 3.6126585 2.9557519 1.6784973 0.096292019 -1.3241494 -2.3915403 -3.3686776][-1.6041481 -0.77639461 -0.12044668 0.77062464 1.9410114 3.308352 4.5397406 4.9679251 4.9195919 4.2382164 2.7739158 1.0292549 -0.56765819 -1.6868801 -2.7941141][-3.4240727 -2.7902374 -2.1613717 -1.2593632 0.10119629 1.7964859 3.3624444 4.0594511 4.395566 3.9136696 2.7643127 1.3441777 -0.33888435 -1.7609165 -3.0554969][-4.9751472 -5.004436 -4.9467068 -3.9981589 -2.4694669 -0.84465027 0.7426815 1.9729476 2.628417 2.3504925 1.4324784 0.42699194 -1.0023215 -2.4739792 -3.76696][-6.6122313 -6.8771553 -7.1246023 -6.7509518 -5.4629726 -3.76627 -2.2021778 -0.98328853 -0.14160252 -0.030379295 -0.73269391 -1.7181821 -2.9226823 -4.1797256 -5.1811795][-8.6403351 -9.182354 -9.5648108 -9.1624413 -8.0075893 -6.6628828 -5.3437815 -3.9391019 -3.1707323 -3.1783197 -3.6139166 -4.1555882 -4.9795384 -5.8284836 -6.3064413][-10.077649 -10.770988 -11.105616 -10.722124 -10.012745 -9.0844584 -7.8794537 -6.7706671 -6.17448 -6.0277319 -6.2548161 -6.5682373 -6.8976879 -7.2191648 -7.12582][-10.646465 -11.346581 -11.615616 -11.353349 -10.948404 -10.416855 -9.8576317 -9.0674477 -8.5992947 -8.5846272 -8.668829 -8.5855074 -8.4254341 -8.1813192 -7.5292749][-10.36816 -10.868419 -11.053799 -10.85396 -10.54318 -10.30871 -10.180786 -10.069538 -10.156969 -10.359302 -10.304028 -9.8915329 -9.3247852 -8.6409893 -7.5841074]]...]
INFO - root - 2017-12-16 08:42:17.609630: step 63310, loss = 0.23, batch loss = 0.17 (27.7 examples/sec; 0.289 sec/batch; 21h:36m:14s remains)
INFO - root - 2017-12-16 08:42:20.482131: step 63320, loss = 0.22, batch loss = 0.16 (27.8 examples/sec; 0.288 sec/batch; 21h:31m:22s remains)
INFO - root - 2017-12-16 08:42:23.314055: step 63330, loss = 0.28, batch loss = 0.22 (28.8 examples/sec; 0.278 sec/batch; 20h:48m:14s remains)
INFO - root - 2017-12-16 08:42:26.133480: step 63340, loss = 0.21, batch loss = 0.15 (28.9 examples/sec; 0.277 sec/batch; 20h:42m:29s remains)
INFO - root - 2017-12-16 08:42:28.972954: step 63350, loss = 0.29, batch loss = 0.23 (28.4 examples/sec; 0.282 sec/batch; 21h:04m:45s remains)
INFO - root - 2017-12-16 08:42:31.820969: step 63360, loss = 0.26, batch loss = 0.20 (29.0 examples/sec; 0.275 sec/batch; 20h:35m:44s remains)
INFO - root - 2017-12-16 08:42:34.689787: step 63370, loss = 0.24, batch loss = 0.18 (27.9 examples/sec; 0.286 sec/batch; 21h:24m:26s remains)
INFO - root - 2017-12-16 08:42:37.534076: step 63380, loss = 0.35, batch loss = 0.29 (28.5 examples/sec; 0.281 sec/batch; 21h:00m:57s remains)
INFO - root - 2017-12-16 08:42:40.402925: step 63390, loss = 0.19, batch loss = 0.13 (27.7 examples/sec; 0.289 sec/batch; 21h:35m:36s remains)
INFO - root - 2017-12-16 08:42:43.233906: step 63400, loss = 0.28, batch loss = 0.22 (27.7 examples/sec; 0.289 sec/batch; 21h:36m:35s remains)
2017-12-16 08:42:43.719569: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7017775 -3.881413 -4.1557341 -4.4805741 -4.7169933 -4.7862477 -4.7097654 -4.4046359 -4.0967307 -3.7949605 -3.6317613 -3.1987696 -2.6462183 -2.0672984 -1.5429688][-3.1593747 -3.3349752 -3.5456364 -3.8057823 -4.0309782 -3.8665023 -3.5233538 -3.1348939 -2.7805524 -2.5212092 -2.3335106 -1.97368 -1.5248723 -0.98918128 -0.6383791][-2.4575932 -2.7141378 -2.8714569 -2.9382143 -2.8497405 -2.4190259 -1.8090794 -1.3052657 -0.96471047 -0.97468972 -1.004283 -0.75363541 -0.39128447 0.053898811 0.2619977][-1.6357391 -1.9092534 -2.0287619 -1.8096371 -1.3089886 -0.67956209 -0.032143593 0.41058064 0.44577026 0.22209024 0.017244339 -0.061192513 0.13304281 0.43733549 0.49818659][-0.91536808 -1.134959 -1.1141489 -0.6596 0.049647808 0.73760319 1.4142675 1.6086993 1.402154 0.75249815 0.063142776 -0.29610348 -0.13868093 0.079309464 0.093052864][-0.38433981 -0.60970521 -0.48841715 0.38728905 1.4276156 2.2562327 2.8607345 2.8002429 2.2155361 1.0919609 0.092999458 -0.58049059 -0.73583651 -0.58421612 -0.53784108][0.21117592 0.17733002 0.56038427 1.6562686 2.9215574 4.0613403 4.5943604 4.1014919 3.0570965 1.4826412 0.079026222 -0.98417592 -1.3758943 -1.4425619 -1.5124948][0.30055618 0.45543528 1.0282459 2.2910333 3.6580582 4.692152 4.8336411 4.2136812 2.906146 1.0803809 -0.55199289 -1.7224162 -2.2042418 -2.5021491 -2.6408079][-0.91289258 -0.55773544 0.3154726 1.3849821 2.5376444 3.4830966 3.58529 2.8838525 1.5640645 -0.037645817 -1.5010424 -2.6967347 -3.254704 -3.4636345 -3.4257536][-2.4400935 -2.163018 -1.660826 -0.54723859 0.76364279 1.4971395 1.4132128 0.863997 -0.16275883 -1.4346449 -2.6097071 -3.5730391 -4.0635505 -4.1506071 -3.9433508][-3.6245086 -3.5100315 -3.145967 -2.4091244 -1.7246549 -0.89004517 -0.37307358 -0.73732758 -1.5439312 -2.4389334 -3.3160174 -4.073626 -4.5321355 -4.5736241 -4.3299842][-4.3801923 -4.474514 -4.5584407 -4.19251 -3.6109645 -2.79776 -2.346283 -2.3366358 -2.4320493 -2.9139361 -3.5201657 -4.099484 -4.4754267 -4.5034509 -4.3170156][-4.6940694 -4.9255381 -5.1547661 -5.1098228 -4.9278836 -4.3132062 -3.7142611 -3.290072 -3.0399044 -3.1799369 -3.4153757 -3.7377896 -3.9823079 -4.063683 -4.0768752][-4.3078551 -4.3564258 -4.3402791 -4.2332234 -4.1109982 -3.7634254 -3.4424486 -3.2151418 -3.0211315 -2.9061055 -2.950181 -3.2082179 -3.4154949 -3.3416266 -3.3720174][-3.3458309 -3.2736206 -3.2432196 -3.0732379 -3.0251653 -2.910717 -2.7523346 -2.6266174 -2.467818 -2.3476961 -2.3078866 -2.2323792 -2.1097789 -2.0611165 -2.3187487]]...]
INFO - root - 2017-12-16 08:42:46.545005: step 63410, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 20h:56m:15s remains)
INFO - root - 2017-12-16 08:42:49.348239: step 63420, loss = 0.23, batch loss = 0.18 (28.6 examples/sec; 0.280 sec/batch; 20h:53m:59s remains)
INFO - root - 2017-12-16 08:42:52.207535: step 63430, loss = 0.26, batch loss = 0.21 (28.7 examples/sec; 0.278 sec/batch; 20h:47m:51s remains)
INFO - root - 2017-12-16 08:42:55.044634: step 63440, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.281 sec/batch; 21h:02m:09s remains)
INFO - root - 2017-12-16 08:42:57.922370: step 63450, loss = 0.24, batch loss = 0.18 (27.7 examples/sec; 0.289 sec/batch; 21h:33m:56s remains)
INFO - root - 2017-12-16 08:43:00.800923: step 63460, loss = 0.25, batch loss = 0.19 (28.6 examples/sec; 0.279 sec/batch; 20h:53m:11s remains)
INFO - root - 2017-12-16 08:43:03.685552: step 63470, loss = 0.26, batch loss = 0.20 (27.9 examples/sec; 0.287 sec/batch; 21h:25m:59s remains)
INFO - root - 2017-12-16 08:43:06.561580: step 63480, loss = 0.46, batch loss = 0.40 (28.0 examples/sec; 0.285 sec/batch; 21h:18m:56s remains)
INFO - root - 2017-12-16 08:43:09.387080: step 63490, loss = 0.27, batch loss = 0.21 (26.7 examples/sec; 0.299 sec/batch; 22h:22m:23s remains)
INFO - root - 2017-12-16 08:43:12.246542: step 63500, loss = 0.29, batch loss = 0.24 (27.2 examples/sec; 0.294 sec/batch; 21h:58m:53s remains)
2017-12-16 08:43:12.758483: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2267914 -2.8978515 -2.8652391 -2.8285437 -3.0650086 -3.4404314 -3.8762369 -4.3972116 -4.7952595 -4.8500628 -4.9368243 -5.4031816 -5.652544 -5.4536338 -5.3401046][-2.6311269 -2.2109525 -1.912549 -2.0656767 -2.5007813 -3.1014154 -3.8279357 -4.5048847 -5.0898228 -5.6454926 -6.001955 -6.0006065 -6.1599994 -6.4943781 -6.4284563][-1.3130972 -0.89546466 -0.53726578 -0.82315993 -1.4556751 -2.4348066 -3.3859487 -4.2099094 -4.9647508 -5.3676367 -5.7860742 -6.3177991 -6.51165 -6.6056376 -6.6004372][-0.010679722 0.20498991 0.31297731 -0.036079407 -0.78038526 -1.654326 -2.4877515 -3.0650542 -3.8029227 -4.7719212 -5.5122595 -5.7605457 -6.1366396 -6.7353334 -6.5595098][0.14318275 0.46638155 0.61614752 0.21882105 -0.40654898 -1.0577116 -1.7405944 -2.4230635 -3.1022682 -3.6686544 -4.4391994 -5.4385424 -6.1086578 -6.1744852 -6.0077219][-0.18001986 0.22015381 0.61802149 0.47997952 0.21043253 -0.14155579 -0.46906996 -1.1288681 -1.9315078 -2.6617417 -3.4258189 -4.2078037 -4.8008928 -4.8529911 -4.3853779][-0.67891479 -0.16765499 0.25673246 0.49605942 0.58015394 0.45434284 0.22489309 -0.26192474 -0.95342016 -1.7374651 -2.5638902 -3.2428575 -3.3349621 -3.017025 -2.3775032][-1.3513675 -0.75777769 -0.24072886 0.1205554 0.19537783 0.068156719 -0.068984509 -0.47911286 -1.1588566 -1.8007922 -2.3199792 -2.5699952 -2.2920046 -1.6252396 -0.64346576][-2.5965691 -1.8876822 -1.2126253 -0.65274382 -0.36152554 -0.22829437 -0.2561059 -0.6784091 -1.3198233 -1.9509606 -2.4709506 -2.4217489 -1.9877934 -1.1094079 -0.0066933632][-4.3694067 -3.6337931 -2.8863273 -2.1700242 -1.5186751 -1.2008128 -1.0505276 -1.375572 -1.8292906 -2.2088346 -2.5309191 -2.5390882 -2.0090609 -0.98793197 -0.023779392][-4.7635517 -4.2753863 -3.7332764 -3.0570221 -2.4199226 -2.1933866 -2.1815412 -2.3706305 -2.5621862 -3.0453751 -3.2285652 -2.8853297 -2.2551346 -1.2613006 -0.31624031][-4.8391733 -4.5774536 -4.2680073 -3.8477621 -3.4828897 -3.2660851 -3.0800481 -3.1678538 -3.2836432 -3.4778037 -3.4895244 -3.4240556 -3.0121639 -2.2029679 -1.5533068][-4.38959 -4.2551007 -4.1659279 -4.0396862 -3.8831618 -3.8545918 -3.6970172 -3.6660812 -3.7717159 -3.9382443 -3.8821611 -3.6987884 -3.5196722 -3.081583 -2.6687875][-4.0514121 -3.7076654 -3.5627522 -3.4040008 -3.156311 -2.9974189 -2.8659878 -2.9406104 -3.0152206 -3.0103703 -3.1774366 -3.3455646 -3.3808987 -3.311543 -3.2884016][-3.673969 -3.2687325 -3.033654 -2.8263783 -2.6603568 -2.4957681 -2.372822 -2.3413913 -2.2214117 -2.1982751 -2.2815144 -2.3435268 -2.5698025 -2.8129935 -3.0487592]]...]
INFO - root - 2017-12-16 08:43:15.544496: step 63510, loss = 0.24, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 20h:29m:23s remains)
INFO - root - 2017-12-16 08:43:18.328312: step 63520, loss = 0.27, batch loss = 0.21 (28.8 examples/sec; 0.277 sec/batch; 20h:43m:17s remains)
INFO - root - 2017-12-16 08:43:21.155402: step 63530, loss = 0.22, batch loss = 0.16 (28.9 examples/sec; 0.277 sec/batch; 20h:41m:10s remains)
INFO - root - 2017-12-16 08:43:24.021315: step 63540, loss = 0.23, batch loss = 0.18 (27.9 examples/sec; 0.287 sec/batch; 21h:27m:37s remains)
INFO - root - 2017-12-16 08:43:26.843455: step 63550, loss = 0.34, batch loss = 0.29 (27.7 examples/sec; 0.289 sec/batch; 21h:36m:47s remains)
INFO - root - 2017-12-16 08:43:29.664641: step 63560, loss = 0.35, batch loss = 0.29 (29.0 examples/sec; 0.276 sec/batch; 20h:37m:28s remains)
INFO - root - 2017-12-16 08:43:32.520357: step 63570, loss = 0.25, batch loss = 0.19 (28.5 examples/sec; 0.281 sec/batch; 20h:59m:40s remains)
INFO - root - 2017-12-16 08:43:35.376200: step 63580, loss = 0.33, batch loss = 0.27 (27.4 examples/sec; 0.292 sec/batch; 21h:49m:03s remains)
INFO - root - 2017-12-16 08:43:38.256113: step 63590, loss = 0.29, batch loss = 0.23 (28.3 examples/sec; 0.283 sec/batch; 21h:08m:08s remains)
INFO - root - 2017-12-16 08:43:41.106865: step 63600, loss = 0.36, batch loss = 0.30 (28.7 examples/sec; 0.279 sec/batch; 20h:50m:03s remains)
2017-12-16 08:43:41.641103: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.222291 -5.5077443 -5.8394017 -6.7359838 -7.6119628 -8.4096622 -8.8421068 -9.1106567 -8.9118319 -8.100666 -7.0349674 -6.0597081 -5.3023882 -4.751018 -4.491437][-4.2083616 -4.3471255 -4.6277847 -5.5267043 -6.5516081 -7.7498755 -8.6386137 -9.1048822 -9.0471239 -8.48579 -7.8321486 -6.9960604 -6.2565222 -5.6826897 -5.2682877][-3.1354079 -2.9174519 -2.9255791 -3.6525714 -4.5224595 -5.5508709 -6.6469269 -7.5528326 -8.0395317 -7.9058323 -7.633985 -7.2556181 -6.7461963 -6.0836258 -5.5177107][-2.1674898 -1.486563 -1.3403909 -1.8506987 -2.5116792 -3.2755857 -4.1715193 -5.1793919 -6.0871124 -6.4049339 -6.5561924 -6.6781712 -6.4797697 -5.8727188 -5.2722807][-1.4432168 -0.33563328 0.35407591 0.12869215 -0.31184769 -0.73374176 -1.1946721 -2.0808611 -3.1260118 -3.9242885 -4.6975489 -5.2967353 -5.5173507 -5.1562586 -4.5585871][-1.4446759 -0.27407503 0.61020756 0.82647181 1.1501327 1.4428468 1.4601774 0.85660028 -0.28172016 -1.5395715 -2.7694693 -3.8587689 -4.467916 -4.1854296 -3.5503328][-1.9330096 -0.77878046 0.15976429 0.9609437 1.7649179 2.3382058 2.6957245 2.4123497 1.3047647 -0.073722839 -1.5004041 -2.7371805 -3.4059348 -3.184952 -2.4771917][-2.4873481 -1.3667042 -0.29977274 0.71370888 1.6226797 2.4470787 2.9898758 2.8451486 1.9472985 0.46267986 -0.90323687 -1.9454324 -2.5047038 -2.6126962 -2.1539304][-3.4509945 -2.399699 -1.4609733 -0.63689256 0.36312294 1.4144382 1.9719362 2.10037 1.5432539 0.24635839 -1.0538361 -2.1755245 -2.7036235 -2.6534278 -2.0747836][-4.167655 -3.56321 -2.8879185 -1.8144834 -0.92308784 -0.34565687 0.010590553 0.28192472 -0.08220911 -1.0482731 -2.002002 -2.8305082 -3.2234735 -3.1782055 -2.7103238][-5.3312106 -4.3620534 -3.4070792 -2.9489107 -2.7365284 -2.1430342 -1.9043446 -1.9750891 -2.2000046 -2.6467533 -3.2188275 -3.6857836 -3.7848365 -3.7635779 -3.43278][-6.6730671 -5.8194175 -5.1985264 -4.6264577 -4.2312822 -4.1667485 -4.3375287 -4.0688119 -3.9479816 -4.1313572 -4.3992524 -4.7137947 -4.7639823 -4.6551313 -4.3532538][-8.126833 -7.2032585 -6.4133778 -6.0649061 -5.9614592 -5.8427348 -5.8058081 -5.5280991 -5.4184504 -5.23379 -5.0983896 -5.2554021 -5.346674 -5.2266059 -4.9559927][-8.3357468 -7.5568466 -7.0110426 -6.7010264 -6.4449425 -6.4558296 -6.6055202 -6.3652182 -5.9838586 -5.4687481 -5.2332368 -5.3276982 -5.3895822 -5.2259083 -5.0149345][-7.80336 -7.2509713 -6.8745284 -6.7942724 -6.7318926 -6.7291946 -6.7276821 -6.5603371 -6.1902232 -5.6849909 -5.4035258 -5.2917142 -5.3250895 -5.2914162 -5.1106744]]...]
INFO - root - 2017-12-16 08:43:44.467771: step 63610, loss = 0.25, batch loss = 0.20 (28.9 examples/sec; 0.277 sec/batch; 20h:39m:55s remains)
INFO - root - 2017-12-16 08:43:47.278553: step 63620, loss = 0.27, batch loss = 0.21 (27.9 examples/sec; 0.287 sec/batch; 21h:26m:58s remains)
INFO - root - 2017-12-16 08:43:50.159827: step 63630, loss = 0.24, batch loss = 0.18 (27.5 examples/sec; 0.291 sec/batch; 21h:42m:08s remains)
INFO - root - 2017-12-16 08:43:52.984858: step 63640, loss = 0.31, batch loss = 0.25 (27.4 examples/sec; 0.292 sec/batch; 21h:47m:18s remains)
INFO - root - 2017-12-16 08:43:55.877387: step 63650, loss = 0.27, batch loss = 0.21 (28.9 examples/sec; 0.276 sec/batch; 20h:38m:21s remains)
INFO - root - 2017-12-16 08:43:58.667389: step 63660, loss = 0.22, batch loss = 0.16 (28.7 examples/sec; 0.279 sec/batch; 20h:48m:26s remains)
INFO - root - 2017-12-16 08:44:01.487222: step 63670, loss = 0.24, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 20h:27m:59s remains)
INFO - root - 2017-12-16 08:44:04.367415: step 63680, loss = 0.22, batch loss = 0.16 (26.2 examples/sec; 0.306 sec/batch; 22h:49m:22s remains)
INFO - root - 2017-12-16 08:44:07.235194: step 63690, loss = 0.28, batch loss = 0.22 (28.7 examples/sec; 0.279 sec/batch; 20h:48m:31s remains)
INFO - root - 2017-12-16 08:44:10.101195: step 63700, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 21h:21m:12s remains)
2017-12-16 08:44:10.637080: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9496713 -2.3451912 -2.9027634 -3.3565488 -3.6106658 -3.8745461 -3.9817719 -3.7759731 -3.4869018 -3.2119918 -3.1097522 -3.1302288 -3.0292935 -2.9988728 -2.8300145][-2.120153 -2.3674812 -2.9165211 -3.4589486 -3.9454281 -4.275557 -4.3922572 -4.2082081 -3.9437907 -3.6881275 -3.495162 -3.2549691 -2.9528589 -2.8731542 -2.7165437][-2.2449319 -2.4079943 -2.8780549 -3.3670716 -3.7610857 -4.03076 -4.11512 -4.0127625 -3.9393556 -3.6695046 -3.3684552 -3.154325 -2.9832997 -2.8286488 -2.5637617][-2.3703303 -2.3529038 -2.5574856 -2.8863187 -3.2501931 -3.5129633 -3.5559006 -3.4303598 -3.2741854 -3.1666458 -2.9729152 -2.6026974 -2.2927485 -2.1940513 -2.1977503][-2.0040009 -1.8173242 -1.9390094 -2.1669924 -2.4495173 -2.6165667 -2.727613 -2.9458485 -3.061101 -2.8943553 -2.4890122 -2.202168 -1.8325682 -1.5619705 -1.6311374][-1.476656 -1.1005766 -1.0311854 -0.92436051 -0.89007354 -1.1286194 -1.465807 -1.8795419 -2.2153239 -2.196239 -1.9578459 -1.5191193 -1.0013385 -0.75832677 -0.92773938][-1.0292418 -0.58310342 -0.36272335 -0.16067553 0.037466526 0.080228329 -0.16312361 -0.77046967 -1.371362 -1.4985435 -1.3441043 -0.85757542 -0.31415319 0.0085816383 -0.14509916][-0.8120687 -0.45581746 -0.36045885 -0.10599422 0.24763775 0.51391315 0.37234735 -0.12809992 -0.55963111 -0.67205191 -0.50256968 -0.10072327 0.34280634 0.61735725 0.280663][-1.2313995 -0.82634711 -0.65155196 -0.56464267 -0.35990238 0.084449768 0.18094015 -0.011728764 -0.1439662 -0.001824379 0.29679489 0.62157822 0.92070389 0.94045067 0.27553129][-1.7991626 -1.7642007 -1.8945682 -1.6766391 -1.2367966 -0.74515843 -0.48379898 -0.37130642 -0.30780506 -0.010962009 0.32074881 0.6709156 0.74356031 0.41114044 -0.41519356][-2.6093733 -2.6780975 -2.8857293 -3.0626254 -2.6493063 -2.047894 -1.6013668 -1.2263167 -0.951005 -0.5099113 -0.19416428 0.041089058 -0.1435833 -0.712425 -1.5016217][-3.1891019 -3.6513212 -4.1848826 -4.5788727 -4.4856534 -3.9381804 -3.244771 -2.5907311 -2.0033085 -1.4203596 -1.0872562 -0.93246555 -1.1752517 -1.9075184 -2.7244966][-3.98432 -4.5171685 -5.1592431 -5.66846 -5.803627 -5.4280872 -4.6926937 -3.8997567 -3.0966692 -2.2983434 -1.7808766 -1.815088 -2.2110002 -2.8467617 -3.3927097][-4.4169259 -5.0782361 -5.7773161 -6.4227848 -6.6649985 -6.3667784 -5.8242888 -5.0108843 -4.1627693 -3.3869872 -2.8747554 -2.6622133 -2.858973 -3.457639 -3.9212482][-4.7401156 -5.3718462 -6.1766262 -6.6755371 -6.7944384 -6.7071629 -6.2676578 -5.5898523 -5.0038133 -4.1929111 -3.6165843 -3.5043089 -3.7897255 -4.1274543 -4.2163382]]...]
INFO - root - 2017-12-16 08:44:13.461718: step 63710, loss = 0.19, batch loss = 0.13 (28.5 examples/sec; 0.281 sec/batch; 20h:59m:27s remains)
INFO - root - 2017-12-16 08:44:16.305750: step 63720, loss = 0.24, batch loss = 0.18 (28.5 examples/sec; 0.281 sec/batch; 20h:57m:11s remains)
INFO - root - 2017-12-16 08:44:19.162981: step 63730, loss = 0.19, batch loss = 0.14 (28.6 examples/sec; 0.280 sec/batch; 20h:53m:12s remains)
INFO - root - 2017-12-16 08:44:22.013250: step 63740, loss = 0.26, batch loss = 0.20 (27.6 examples/sec; 0.289 sec/batch; 21h:36m:27s remains)
INFO - root - 2017-12-16 08:44:24.878322: step 63750, loss = 0.31, batch loss = 0.25 (27.6 examples/sec; 0.290 sec/batch; 21h:37m:53s remains)
INFO - root - 2017-12-16 08:44:27.746248: step 63760, loss = 0.29, batch loss = 0.23 (27.4 examples/sec; 0.291 sec/batch; 21h:45m:25s remains)
INFO - root - 2017-12-16 08:44:30.599372: step 63770, loss = 0.21, batch loss = 0.15 (28.1 examples/sec; 0.285 sec/batch; 21h:16m:08s remains)
INFO - root - 2017-12-16 08:44:33.461690: step 63780, loss = 0.38, batch loss = 0.33 (28.3 examples/sec; 0.282 sec/batch; 21h:04m:55s remains)
INFO - root - 2017-12-16 08:44:36.263921: step 63790, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.286 sec/batch; 21h:21m:43s remains)
INFO - root - 2017-12-16 08:44:39.087947: step 63800, loss = 0.28, batch loss = 0.22 (28.6 examples/sec; 0.280 sec/batch; 20h:54m:22s remains)
2017-12-16 08:44:39.587931: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5756049 -2.5855792 -3.0055485 -3.2901733 -3.4162562 -3.7054007 -3.9409008 -4.1004243 -3.9471338 -3.5769982 -3.2158353 -3.1832259 -3.0081167 -2.5752232 -2.2411921][-2.9212608 -2.7622528 -2.8250012 -3.3880253 -3.7778301 -4.0298381 -4.3308883 -4.3797503 -4.1418076 -4.144134 -4.1306348 -3.8200734 -3.4038019 -3.0903687 -2.6341078][-3.2774472 -3.2525158 -3.6235709 -3.8227434 -3.8052063 -4.004539 -4.0918522 -4.4595785 -4.6878095 -4.6820931 -4.5889015 -4.5571055 -4.3187914 -3.7724626 -3.2011056][-4.3987403 -4.165585 -4.0054946 -3.8831873 -3.47749 -3.343174 -3.56977 -3.7602839 -3.9883177 -4.7224584 -5.3117771 -5.3529882 -5.0242805 -4.6194353 -4.1381693][-5.0484 -4.9023886 -4.5229521 -3.6092303 -2.4690142 -1.7478664 -1.4760563 -2.0182512 -3.1789951 -4.2498302 -5.0216193 -5.511364 -5.6699433 -5.1633129 -4.2824955][-5.505497 -4.9981747 -4.0573444 -2.4088295 -0.53047276 0.61120653 0.81230021 0.095702171 -1.2404654 -2.7744722 -4.2740231 -5.1961045 -5.3103652 -5.2066269 -4.6883073][-5.9118242 -5.0905404 -3.6393433 -1.606173 0.82537508 2.656817 3.2879038 2.5070953 0.62909174 -1.3483403 -2.9668469 -4.18833 -4.8576093 -4.4830074 -3.2670388][-5.53733 -5.094111 -3.9872367 -1.4033313 1.3548985 3.5022717 4.0966215 3.1917892 1.6169724 -0.35583067 -2.3121691 -3.3400712 -3.5962865 -3.5892224 -2.8763161][-5.5993695 -4.9899778 -3.8136885 -1.9672863 0.30769873 2.6547236 3.8120356 3.2537036 1.4024987 -0.51951241 -2.1458366 -3.4327784 -3.6570029 -2.892637 -1.836386][-5.1903391 -5.3577967 -5.0743551 -3.442615 -1.3765941 0.48101854 1.1770577 1.0491414 0.21985245 -1.4908941 -3.0095038 -3.8622689 -3.8260405 -3.1825626 -2.1427767][-5.5393596 -5.759234 -5.7470841 -5.244534 -4.3510571 -2.9634264 -1.7971859 -1.5750875 -2.2107565 -3.2111568 -4.2489409 -4.7778959 -4.4055004 -3.8354614 -2.8774319][-5.2877359 -5.6762705 -6.2957115 -6.2852087 -5.9464822 -5.4683986 -4.9733906 -4.4551148 -4.2747536 -4.6934562 -5.1569624 -5.6661406 -5.552721 -4.747489 -3.755398][-5.010932 -5.5052671 -5.9998517 -6.4392481 -6.8585997 -6.5393462 -6.0699406 -5.6832738 -5.2399759 -5.2409449 -5.6393957 -5.8766613 -5.814187 -5.6739488 -5.0379529][-4.5228124 -4.74293 -5.1175947 -5.5130253 -6.02137 -6.2319 -6.1228204 -5.8049517 -5.4101877 -4.941236 -4.7427411 -5.3334436 -5.91802 -5.8468981 -5.4115152][-4.0705657 -4.0864563 -4.1501913 -4.3795857 -4.6423316 -4.7858872 -4.899807 -4.6976347 -4.3948231 -4.2976909 -4.3763013 -4.7037153 -5.3046756 -5.6988769 -5.679893]]...]
INFO - root - 2017-12-16 08:44:42.431292: step 63810, loss = 0.28, batch loss = 0.22 (27.7 examples/sec; 0.289 sec/batch; 21h:35m:29s remains)
INFO - root - 2017-12-16 08:44:45.267951: step 63820, loss = 0.30, batch loss = 0.24 (28.1 examples/sec; 0.284 sec/batch; 21h:13m:26s remains)
INFO - root - 2017-12-16 08:44:48.094865: step 63830, loss = 0.28, batch loss = 0.22 (28.5 examples/sec; 0.281 sec/batch; 20h:56m:24s remains)
INFO - root - 2017-12-16 08:44:50.929466: step 63840, loss = 0.43, batch loss = 0.37 (27.6 examples/sec; 0.290 sec/batch; 21h:38m:05s remains)
INFO - root - 2017-12-16 08:44:53.761776: step 63850, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.282 sec/batch; 21h:02m:56s remains)
INFO - root - 2017-12-16 08:44:56.617237: step 63860, loss = 0.25, batch loss = 0.19 (28.3 examples/sec; 0.283 sec/batch; 21h:07m:53s remains)
INFO - root - 2017-12-16 08:44:59.462774: step 63870, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 20h:53m:32s remains)
INFO - root - 2017-12-16 08:45:02.314455: step 63880, loss = 0.40, batch loss = 0.34 (28.6 examples/sec; 0.280 sec/batch; 20h:52m:19s remains)
INFO - root - 2017-12-16 08:45:05.106534: step 63890, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 20h:42m:58s remains)
INFO - root - 2017-12-16 08:45:07.974525: step 63900, loss = 0.24, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 20h:26m:12s remains)
2017-12-16 08:45:08.541317: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7358932 -5.0019832 -4.4526191 -4.23392 -4.2999864 -4.4241171 -4.6238608 -5.0785241 -5.4536495 -5.6020212 -5.7447777 -5.7047148 -5.5719533 -5.4813571 -5.4133267][-5.9408917 -5.1245794 -4.45717 -4.1205158 -3.9481542 -4.2171679 -4.4847708 -4.924777 -5.294435 -5.3684587 -5.4096484 -5.2472181 -5.2728682 -5.0952797 -4.8244677][-5.6647716 -4.5469027 -3.7715716 -3.3520336 -3.1289251 -3.4669576 -3.9832268 -4.54605 -4.7262626 -4.9252572 -5.0904322 -4.9874954 -4.8721633 -4.6817904 -4.5929575][-5.3106112 -3.7940071 -2.7465556 -2.3520808 -2.4364822 -2.7447419 -3.1076455 -3.7355816 -4.070271 -4.1645751 -4.087214 -4.3745952 -4.6876612 -4.5009193 -4.0130448][-4.6282511 -3.0841937 -1.8345897 -1.2769303 -1.2883723 -1.7365651 -2.2013938 -2.6381531 -2.8918357 -3.072228 -3.3991208 -3.6415887 -3.8173575 -4.0867877 -3.9173121][-3.661509 -2.1838381 -0.84762883 -0.1011076 0.01255846 -0.32559776 -0.62239718 -1.0639045 -1.3976123 -1.6852326 -2.3229592 -3.2764993 -4.058372 -4.3483782 -4.1666379][-3.161788 -1.7198541 -0.42758894 0.36873055 0.62103748 0.59009171 0.61798096 0.35673141 0.1520977 -0.1498251 -0.80996275 -2.122359 -3.4401631 -4.259037 -4.4480824][-3.293292 -1.7217019 -0.42687607 0.35213041 0.77679396 0.97478104 1.1523972 1.1963863 1.1049862 0.78255939 0.037798882 -1.2696531 -2.4813414 -3.6552134 -4.3301358][-3.4135146 -2.1133876 -0.76061487 -0.061844349 0.17836809 0.55171108 0.97426987 1.1671414 1.2848639 1.1407104 0.55808496 -0.76881027 -2.1094644 -3.0547795 -3.5695035][-3.3900855 -2.1837053 -1.1645315 -0.61803937 -0.2616024 -0.052684784 0.1984725 0.59461308 0.87040997 0.79241514 0.22851086 -0.71339679 -1.7557111 -2.6726246 -3.20828][-3.4019508 -2.245244 -1.3116467 -1.1103616 -1.1859217 -1.0819187 -0.78270054 -0.41663074 -0.30684471 -0.2700696 -0.64501095 -1.255856 -1.8636751 -2.4953766 -2.8783684][-3.3041878 -2.3071983 -1.713609 -1.6285431 -1.9237399 -2.239475 -2.3279045 -1.9899008 -1.6215208 -1.5394983 -1.7818844 -1.9937596 -2.2468293 -2.5538995 -2.7808208][-3.281219 -2.4117134 -2.0663564 -2.310174 -2.8486118 -3.3422437 -3.4428077 -3.2499514 -3.1031294 -2.8441699 -2.6974361 -2.7510285 -3.0738292 -3.0931849 -3.0562823][-3.5868816 -2.867769 -2.6608188 -2.9285283 -3.5802093 -4.3715782 -4.7966 -4.5189118 -4.1189361 -3.7697043 -3.6990256 -3.6809638 -3.6680679 -3.7869344 -3.8410492][-3.8469548 -3.1074514 -2.9830811 -3.3855071 -4.1419411 -5.0205536 -5.6251225 -5.5248537 -5.2963791 -4.9008007 -4.8288307 -4.9562459 -4.9913292 -4.911983 -4.747941]]...]
INFO - root - 2017-12-16 08:45:11.347002: step 63910, loss = 0.37, batch loss = 0.32 (29.2 examples/sec; 0.274 sec/batch; 20h:25m:45s remains)
INFO - root - 2017-12-16 08:45:14.164844: step 63920, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.284 sec/batch; 21h:10m:16s remains)
INFO - root - 2017-12-16 08:45:17.003503: step 63930, loss = 0.26, batch loss = 0.21 (28.0 examples/sec; 0.285 sec/batch; 21h:17m:51s remains)
INFO - root - 2017-12-16 08:45:19.838348: step 63940, loss = 0.27, batch loss = 0.21 (29.0 examples/sec; 0.276 sec/batch; 20h:34m:05s remains)
INFO - root - 2017-12-16 08:45:22.653790: step 63950, loss = 0.25, batch loss = 0.19 (29.3 examples/sec; 0.273 sec/batch; 20h:23m:30s remains)
INFO - root - 2017-12-16 08:45:25.535247: step 63960, loss = 0.28, batch loss = 0.22 (28.5 examples/sec; 0.281 sec/batch; 20h:56m:05s remains)
INFO - root - 2017-12-16 08:45:28.407335: step 63970, loss = 0.24, batch loss = 0.18 (27.8 examples/sec; 0.288 sec/batch; 21h:26m:58s remains)
INFO - root - 2017-12-16 08:45:31.249620: step 63980, loss = 0.27, batch loss = 0.22 (28.3 examples/sec; 0.283 sec/batch; 21h:05m:16s remains)
INFO - root - 2017-12-16 08:45:34.080872: step 63990, loss = 0.34, batch loss = 0.28 (29.4 examples/sec; 0.272 sec/batch; 20h:16m:11s remains)
INFO - root - 2017-12-16 08:45:36.930152: step 64000, loss = 0.30, batch loss = 0.25 (28.8 examples/sec; 0.278 sec/batch; 20h:42m:45s remains)
2017-12-16 08:45:37.439645: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8493705 -3.8854961 -4.0687838 -4.5266948 -5.0247135 -5.3947845 -5.59521 -5.8183918 -5.866786 -5.5720778 -5.2394924 -5.1297941 -5.0700369 -4.9224362 -4.704154][-2.9783401 -3.3597772 -3.8020451 -4.2827163 -4.776823 -5.2759676 -5.5016818 -5.4020967 -5.2297468 -4.9995561 -4.8572764 -4.7447953 -4.7662439 -4.6846361 -4.5002341][-2.5256271 -2.9459171 -3.5671105 -4.1605048 -4.5634885 -4.7219315 -4.6362004 -4.3488369 -4.17785 -4.1398878 -4.3795357 -4.5517983 -4.5925035 -4.4583139 -4.2797909][-2.06197 -2.6675982 -3.5099201 -4.1113911 -4.2838535 -3.8558323 -3.2672138 -2.9695096 -2.7564275 -2.8766708 -3.3128014 -3.9228024 -4.2198195 -3.9562168 -3.4422381][-1.7167425 -2.5961876 -3.53164 -3.9321496 -3.7308939 -2.9504585 -1.8198006 -1.1898634 -1.1709034 -1.7267835 -2.6642401 -3.5165331 -3.8501511 -3.6733348 -3.142848][-2.0040607 -2.9290719 -3.7644074 -3.9464314 -3.1468892 -1.7315643 -0.24539852 0.57093 0.32281494 -0.75850511 -2.2509768 -3.5289726 -4.1898432 -3.9503777 -3.298316][-2.2292078 -3.3434763 -4.1535978 -3.9787631 -2.7950706 -0.88555 0.97549486 1.8593369 1.4795647 -0.086414337 -1.9095945 -3.3532081 -4.2064719 -4.08498 -3.4484625][-2.1365893 -3.4287949 -4.3973532 -4.2487893 -2.7823291 -0.70156956 1.202713 2.1196775 1.7693682 0.19307852 -1.8705947 -3.3910985 -4.1112633 -4.1764112 -3.6882482][-2.2894571 -3.4362631 -4.2409506 -4.2829165 -2.9963994 -0.9110167 0.91819954 1.8928523 1.6089697 0.044677258 -2.136132 -3.8159404 -4.6953063 -4.756165 -4.3278956][-2.1074226 -3.4477944 -4.6235304 -4.7145281 -3.6319871 -1.7396352 -0.031032085 0.98309374 0.77313089 -0.71910667 -2.7479978 -4.5211396 -5.5648966 -5.6360765 -5.0454092][-1.8930018 -3.218 -4.5251226 -4.9891462 -4.3342204 -2.8764043 -1.2578089 -0.47762752 -0.84731317 -2.0574222 -3.8689916 -5.3867464 -6.1995897 -6.2597713 -5.7368822][-2.4329615 -3.4477055 -4.5880203 -5.1677861 -4.8275952 -3.8136485 -2.6798909 -2.1461809 -2.5798705 -3.5630095 -4.8877234 -5.8938217 -6.2609367 -6.0844212 -5.6347437][-3.1468377 -3.92075 -4.8409071 -5.4749508 -5.7125282 -5.1360693 -4.2801948 -3.9103594 -4.2576084 -4.9949117 -5.6773524 -6.1780262 -6.2411675 -5.8377314 -5.36427][-3.4391863 -4.0143132 -4.7591772 -5.44882 -5.9054408 -5.8934431 -5.7033582 -5.5801735 -5.7405276 -6.1347122 -6.4698086 -6.4600554 -6.0964346 -5.6076531 -5.2197609][-3.674994 -3.9764519 -4.6331387 -5.4256992 -6.0910239 -6.4182138 -6.7272511 -6.8263493 -6.8936272 -6.878583 -6.6685495 -6.395442 -5.9953966 -5.4140239 -5.06697]]...]
INFO - root - 2017-12-16 08:45:40.300935: step 64010, loss = 0.23, batch loss = 0.17 (27.7 examples/sec; 0.288 sec/batch; 21h:30m:10s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 08:45:43.109777: step 64020, loss = 0.35, batch loss = 0.29 (29.0 examples/sec; 0.276 sec/batch; 20h:36m:31s remains)
INFO - root - 2017-12-16 08:45:46.019579: step 64030, loss = 0.22, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 20h:48m:05s remains)
INFO - root - 2017-12-16 08:45:48.872983: step 64040, loss = 0.29, batch loss = 0.23 (27.3 examples/sec; 0.293 sec/batch; 21h:49m:00s remains)
INFO - root - 2017-12-16 08:45:51.724909: step 64050, loss = 0.33, batch loss = 0.27 (28.6 examples/sec; 0.280 sec/batch; 20h:51m:35s remains)
INFO - root - 2017-12-16 08:45:54.604157: step 64060, loss = 0.24, batch loss = 0.18 (26.7 examples/sec; 0.299 sec/batch; 22h:19m:23s remains)
INFO - root - 2017-12-16 08:45:57.441307: step 64070, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 21h:19m:03s remains)
INFO - root - 2017-12-16 08:46:00.285063: step 64080, loss = 0.31, batch loss = 0.25 (28.6 examples/sec; 0.280 sec/batch; 20h:53m:20s remains)
INFO - root - 2017-12-16 08:46:03.105711: step 64090, loss = 0.30, batch loss = 0.25 (29.2 examples/sec; 0.274 sec/batch; 20h:25m:34s remains)
INFO - root - 2017-12-16 08:46:05.983685: step 64100, loss = 0.26, batch loss = 0.20 (27.3 examples/sec; 0.293 sec/batch; 21h:50m:28s remains)
2017-12-16 08:46:06.535148: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3086348 -3.3514328 -3.2712731 -3.5926549 -3.9727402 -4.4599733 -5.0028877 -5.21938 -5.3050909 -5.2941303 -5.2298856 -4.8379865 -4.5945921 -4.4621134 -4.2922163][-2.3410685 -2.1269264 -2.0901575 -2.4170384 -2.8449345 -3.825695 -4.6317539 -4.9894118 -5.1672916 -5.0984669 -4.9971247 -4.849143 -4.7259374 -4.4437423 -4.2974148][-1.0794094 -0.77911019 -0.81059551 -1.2842219 -1.844939 -2.6050081 -3.3556523 -4.1223631 -4.5160065 -4.7283587 -4.8053508 -4.6751132 -4.4731922 -4.0093312 -3.7069726][0.07814312 0.18252563 -0.33005333 -0.80275464 -1.0982599 -1.5280962 -2.01662 -2.6343527 -3.2461846 -3.7748332 -3.8787663 -3.9396932 -3.6627567 -3.3080094 -3.1432486][1.1600475 0.96508789 0.433393 -0.032935619 -0.33192539 -0.31807041 -0.43095779 -1.0164287 -1.6589017 -2.6084847 -3.4677453 -3.7954397 -3.5023887 -3.0029707 -2.5390823][0.96178722 0.84906197 0.63221693 0.80596209 1.3343625 1.544632 1.460042 0.90706635 -0.15329504 -1.5128655 -2.7431972 -3.5429242 -3.8974175 -3.5991592 -3.0801826][-0.037806988 0.1316514 0.15690231 0.75458765 1.9341173 3.0404119 3.5016437 2.6346059 1.2305059 -0.3206377 -1.8656628 -3.1754231 -3.9189174 -3.9869125 -3.8463643][-1.3097522 -1.0628574 -0.74150205 0.36595392 1.8077049 3.2922392 4.1452951 3.681159 2.2857041 0.26245546 -1.6136334 -2.8189425 -3.6929698 -4.143734 -4.1841841][-2.5330219 -2.014643 -1.300071 -0.079516411 1.2915907 2.683599 3.5169678 2.9822369 1.6608877 -0.1819973 -2.1549578 -3.6528082 -4.7408962 -5.1206851 -4.9206634][-2.9339824 -2.5657525 -2.084862 -0.95761275 0.27222157 1.2724895 1.7618327 1.5440326 0.57799721 -1.1879451 -3.02304 -4.6156087 -5.7368946 -6.1272154 -5.7569146][-3.4256546 -3.1135182 -2.7209876 -2.2289324 -1.5933342 -0.73068476 -0.03612566 -0.1842308 -0.98700714 -2.0115664 -3.3448329 -4.6131039 -5.6643434 -5.8782058 -5.3690948][-3.2939391 -2.9832582 -2.972527 -2.8549767 -2.3968022 -1.9799659 -1.5477977 -1.6090462 -2.0398378 -2.6355562 -3.639389 -4.5869 -5.1509829 -5.2136841 -4.8840957][-2.6315498 -2.3840196 -2.4310482 -2.648818 -2.5891676 -2.4038274 -2.0552881 -2.052139 -2.2779322 -2.8138976 -3.5460448 -4.4175863 -4.9300919 -4.8756094 -4.3681498][-2.0839179 -2.20395 -2.1513162 -2.120404 -2.0293906 -2.1547217 -2.1262186 -2.1793821 -2.2610097 -2.6243839 -3.3934131 -4.1044145 -4.4940014 -4.499619 -4.0227661][-1.792592 -1.720897 -1.4727452 -1.3416717 -1.3389342 -1.3277736 -1.2892525 -1.3190703 -1.6791482 -2.0762746 -2.8474505 -3.6216104 -4.2032847 -4.2212934 -3.8279006]]...]
INFO - root - 2017-12-16 08:46:09.399285: step 64110, loss = 0.24, batch loss = 0.18 (25.5 examples/sec; 0.313 sec/batch; 23h:20m:40s remains)
INFO - root - 2017-12-16 08:46:12.209464: step 64120, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 20h:44m:06s remains)
INFO - root - 2017-12-16 08:46:15.061459: step 64130, loss = 0.32, batch loss = 0.26 (29.2 examples/sec; 0.274 sec/batch; 20h:25m:58s remains)
INFO - root - 2017-12-16 08:46:17.908214: step 64140, loss = 0.23, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 21h:13m:58s remains)
INFO - root - 2017-12-16 08:46:20.782871: step 64150, loss = 0.23, batch loss = 0.18 (26.7 examples/sec; 0.299 sec/batch; 22h:18m:35s remains)
INFO - root - 2017-12-16 08:46:23.670030: step 64160, loss = 0.29, batch loss = 0.23 (27.5 examples/sec; 0.291 sec/batch; 21h:40m:46s remains)
INFO - root - 2017-12-16 08:46:26.523107: step 64170, loss = 0.20, batch loss = 0.14 (28.1 examples/sec; 0.285 sec/batch; 21h:15m:05s remains)
INFO - root - 2017-12-16 08:46:29.335584: step 64180, loss = 0.23, batch loss = 0.17 (29.3 examples/sec; 0.273 sec/batch; 20h:20m:35s remains)
INFO - root - 2017-12-16 08:46:32.213477: step 64190, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 21h:19m:36s remains)
INFO - root - 2017-12-16 08:46:35.074241: step 64200, loss = 0.28, batch loss = 0.23 (28.3 examples/sec; 0.282 sec/batch; 21h:01m:53s remains)
2017-12-16 08:46:35.640759: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7467337 -6.7268734 -6.5783296 -6.3337789 -6.132688 -6.0874958 -6.1579981 -6.2503729 -6.1734385 -5.824851 -5.4340467 -5.1905303 -5.062747 -5.029901 -5.2447863][-6.1327839 -6.3191347 -6.2356315 -5.9959841 -5.7204723 -5.4056616 -5.2301345 -5.2377729 -5.1022787 -4.8681936 -4.765564 -4.7142777 -4.705224 -4.7125387 -4.9279246][-5.3494878 -5.7409654 -5.6631289 -5.30821 -4.8610144 -4.4265175 -4.00248 -3.7286618 -3.690362 -3.793798 -3.9122357 -4.122448 -4.4413223 -4.5869832 -4.7538381][-4.6754026 -5.2437105 -5.4244995 -4.9818549 -3.959475 -2.9521382 -2.2569966 -1.9422355 -1.9389079 -2.3613987 -2.9125395 -3.3391087 -3.7757173 -4.0396471 -4.2793126][-4.1313572 -4.5747352 -4.533174 -4.0078511 -2.8793457 -1.4428313 -0.18951273 0.2756381 0.10028887 -0.61080575 -1.5521858 -2.3914497 -3.1616249 -3.60088 -3.9213061][-3.4865513 -3.786891 -3.5581017 -2.7038534 -1.1941974 0.42054176 1.8586326 2.4189091 2.0316162 0.90939808 -0.40974522 -1.5906267 -2.5379891 -3.1678076 -3.6060305][-2.8440967 -3.0304441 -2.8500931 -1.9670076 -0.37201691 1.440485 3.0169268 3.5207162 3.0624266 1.9068098 0.52230024 -0.84148693 -1.9259934 -2.6540909 -3.2594981][-2.3308835 -2.3510149 -2.0339687 -1.2844558 0.019525051 1.6436052 3.0891232 3.5835 3.2365413 2.1546044 0.84564209 -0.40929556 -1.4303973 -2.2941329 -3.0895777][-1.811841 -1.9044595 -1.6792245 -1.063092 -0.16431284 0.85295296 1.9772763 2.5430865 2.3524432 1.6312723 0.7637167 -0.27102232 -1.2236514 -2.1010957 -3.0372405][-1.4758787 -1.5735283 -1.5909829 -1.5075181 -1.1723659 -0.56409836 0.039081097 0.56189632 0.65507126 0.39061642 -0.054048061 -0.77731109 -1.540045 -2.4432459 -3.3770728][-1.8997245 -1.9777205 -2.0133865 -2.3353519 -2.6112776 -2.48452 -2.0783606 -1.6539643 -1.4876394 -1.3522081 -1.2799158 -1.6642001 -2.2834768 -3.1237538 -4.0036893][-3.2003264 -3.3689675 -3.5216069 -3.9313304 -4.307744 -4.550909 -4.6164961 -4.1472211 -3.6833506 -3.3840237 -3.2254422 -3.275795 -3.467453 -4.079442 -4.7756634][-4.914896 -5.0671887 -5.2145524 -5.577867 -6.0243559 -6.2365742 -6.2808437 -6.0120497 -5.7386303 -5.1978426 -4.6659179 -4.7110248 -4.9416871 -5.2539339 -5.555006][-6.5047321 -6.5825911 -6.5348511 -6.640357 -6.9377055 -7.1669378 -7.4679956 -7.4856491 -7.3717842 -6.979661 -6.5510406 -6.29202 -6.1452775 -6.3395591 -6.5047884][-7.6719408 -7.7275591 -7.6222982 -7.4653368 -7.5173874 -7.7276878 -8.0221872 -8.2265425 -8.4097385 -8.3214283 -8.0477734 -7.7756114 -7.533494 -7.4206934 -7.2192316]]...]
INFO - root - 2017-12-16 08:46:38.486922: step 64210, loss = 0.32, batch loss = 0.26 (29.5 examples/sec; 0.271 sec/batch; 20h:13m:59s remains)
INFO - root - 2017-12-16 08:46:41.352361: step 64220, loss = 0.30, batch loss = 0.24 (28.4 examples/sec; 0.282 sec/batch; 21h:00m:45s remains)
INFO - root - 2017-12-16 08:46:44.153063: step 64230, loss = 0.30, batch loss = 0.24 (28.3 examples/sec; 0.282 sec/batch; 21h:02m:41s remains)
INFO - root - 2017-12-16 08:46:46.993781: step 64240, loss = 0.21, batch loss = 0.15 (28.1 examples/sec; 0.285 sec/batch; 21h:15m:03s remains)
INFO - root - 2017-12-16 08:46:49.814522: step 64250, loss = 0.22, batch loss = 0.16 (28.4 examples/sec; 0.281 sec/batch; 20h:57m:33s remains)
INFO - root - 2017-12-16 08:46:52.663195: step 64260, loss = 0.29, batch loss = 0.23 (28.5 examples/sec; 0.281 sec/batch; 20h:55m:48s remains)
INFO - root - 2017-12-16 08:46:55.528413: step 64270, loss = 0.20, batch loss = 0.14 (28.4 examples/sec; 0.282 sec/batch; 20h:58m:29s remains)
INFO - root - 2017-12-16 08:46:58.394369: step 64280, loss = 0.30, batch loss = 0.24 (28.8 examples/sec; 0.278 sec/batch; 20h:43m:38s remains)
INFO - root - 2017-12-16 08:47:01.224987: step 64290, loss = 0.21, batch loss = 0.15 (29.1 examples/sec; 0.275 sec/batch; 20h:29m:02s remains)
INFO - root - 2017-12-16 08:47:04.120289: step 64300, loss = 0.30, batch loss = 0.24 (28.7 examples/sec; 0.279 sec/batch; 20h:46m:27s remains)
2017-12-16 08:47:04.674074: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0352607 -3.3268313 -3.6293292 -4.0061727 -4.3536696 -4.6549168 -4.9689136 -5.45446 -5.8898997 -6.1281939 -6.0997252 -5.95398 -5.7324524 -5.2850704 -5.0723939][-2.7514071 -2.9733815 -3.2638578 -3.6975858 -4.057085 -4.6342297 -5.1004505 -5.5939474 -6.0639772 -6.4861164 -6.7635965 -6.6219759 -6.3498359 -5.7528267 -5.1594815][-2.3706291 -2.4447329 -2.7455034 -3.2229662 -3.562494 -4.111443 -4.5248756 -5.0621767 -5.5709572 -6.0818558 -6.5899687 -6.7319679 -6.5764022 -5.8049011 -5.0979581][-1.951298 -1.9425294 -2.0965941 -2.4796393 -2.8548756 -3.2025213 -3.4342639 -3.6775553 -4.0140443 -4.6781626 -5.4181657 -6.0311441 -6.1685057 -5.488575 -4.5866446][-1.4868846 -1.3081045 -1.4123361 -1.7514172 -1.9552538 -1.8264773 -1.5186963 -1.4210441 -1.8202145 -2.5599356 -3.5578153 -4.6191626 -5.1551728 -4.9034748 -4.2072186][-1.2974477 -1.0172503 -0.96413708 -0.96771073 -0.68193293 -0.21544743 0.4490819 1.0869222 1.0023894 -0.055846691 -1.7434256 -3.1844859 -3.9713693 -4.0464616 -3.6680427][-1.2177649 -0.98189616 -0.92612767 -0.71933413 -0.13026571 0.91406727 2.29571 3.1038938 2.8898482 1.864387 0.26105833 -1.5566671 -2.9499302 -3.2980537 -3.0100241][-1.1196082 -1.0022175 -1.0706627 -0.72040725 0.089542866 1.5123825 3.11978 4.1515141 4.3161545 3.2645917 1.511364 -0.24932528 -1.6857865 -2.5495942 -2.7261465][-1.6033158 -1.3026693 -1.1406355 -0.93444276 -0.29942989 1.1255894 2.7019405 3.9688835 4.3325806 3.4630065 1.9021797 0.12054539 -1.3671949 -2.3135967 -2.4712367][-2.0729256 -2.1819656 -2.2995729 -2.06939 -1.3595903 -0.40413952 0.62749481 1.8581219 2.4555898 2.1304941 1.011795 -0.45967603 -1.7048111 -2.6159801 -2.7864492][-3.2545514 -3.0702195 -3.1507943 -3.3935642 -3.1828723 -2.4808192 -1.5434201 -0.63703656 -0.14240074 -0.12445641 -0.567014 -1.3616989 -2.2624788 -2.9857001 -2.8605256][-4.4350095 -4.3213005 -4.4187474 -4.4612994 -4.3989992 -4.3088255 -3.9033108 -3.1535816 -2.3917973 -2.1223681 -2.3651555 -2.8021579 -3.1805124 -3.4191656 -3.1101661][-5.39104 -5.4623766 -5.6765113 -5.9824977 -6.1281033 -6.0308108 -5.7716084 -5.4094896 -4.9134731 -4.3804121 -4.0350838 -4.1094913 -4.2884736 -4.3346367 -3.9514132][-6.4443169 -6.5250273 -6.5900521 -6.7712436 -7.0395737 -7.1431751 -7.103004 -6.8458347 -6.4410944 -5.9377246 -5.5059929 -5.3266497 -5.1702819 -4.9081335 -4.4226971][-6.6525517 -6.6978 -6.7202344 -6.9470797 -7.12897 -7.1041346 -7.0865464 -7.0727749 -6.9388547 -6.4833708 -6.0284867 -5.7367682 -5.5379667 -5.2177124 -4.736928]]...]
INFO - root - 2017-12-16 08:47:07.504662: step 64310, loss = 0.35, batch loss = 0.29 (27.9 examples/sec; 0.287 sec/batch; 21h:22m:27s remains)
INFO - root - 2017-12-16 08:47:10.343760: step 64320, loss = 0.31, batch loss = 0.25 (27.6 examples/sec; 0.289 sec/batch; 21h:33m:50s remains)
INFO - root - 2017-12-16 08:47:13.244180: step 64330, loss = 0.30, batch loss = 0.24 (28.9 examples/sec; 0.276 sec/batch; 20h:35m:27s remains)
INFO - root - 2017-12-16 08:47:16.110848: step 64340, loss = 0.31, batch loss = 0.25 (27.6 examples/sec; 0.290 sec/batch; 21h:37m:38s remains)
INFO - root - 2017-12-16 08:47:18.984759: step 64350, loss = 0.23, batch loss = 0.18 (27.0 examples/sec; 0.296 sec/batch; 22h:02m:37s remains)
INFO - root - 2017-12-16 08:47:21.816311: step 64360, loss = 0.22, batch loss = 0.16 (28.2 examples/sec; 0.283 sec/batch; 21h:06m:38s remains)
INFO - root - 2017-12-16 08:47:24.687294: step 64370, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.285 sec/batch; 21h:13m:51s remains)
INFO - root - 2017-12-16 08:47:27.576490: step 64380, loss = 0.39, batch loss = 0.33 (29.5 examples/sec; 0.271 sec/batch; 20h:12m:58s remains)
INFO - root - 2017-12-16 08:47:30.415940: step 64390, loss = 0.23, batch loss = 0.17 (27.2 examples/sec; 0.294 sec/batch; 21h:53m:35s remains)
INFO - root - 2017-12-16 08:47:33.236548: step 64400, loss = 0.26, batch loss = 0.20 (28.2 examples/sec; 0.283 sec/batch; 21h:05m:27s remains)
2017-12-16 08:47:33.770448: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8407433 -4.1745234 -4.5031595 -4.8237805 -5.1069942 -5.2170877 -5.2783132 -5.5064492 -5.678575 -5.6903505 -5.4485178 -5.1787939 -4.8138013 -4.2837362 -3.8657222][-3.6132083 -3.9461396 -4.2656889 -4.6154437 -4.9302187 -5.3240943 -5.6436272 -5.6773157 -5.6361246 -5.7629185 -5.8299065 -5.6546679 -5.25217 -4.6824546 -4.1361752][-3.3932848 -3.7334957 -4.0206285 -4.2703447 -4.4701881 -4.6984553 -4.836091 -5.07916 -5.2899652 -5.3791218 -5.5455842 -5.6455517 -5.5486717 -5.0444455 -4.4659705][-3.0385227 -3.2690341 -3.397151 -3.3617258 -3.2989526 -3.2442942 -3.2336822 -3.3140819 -3.4841695 -4.0004325 -4.593822 -5.0026059 -5.227138 -5.0434122 -4.6818371][-2.6609201 -2.6066966 -2.448966 -2.0965636 -1.6202219 -1.1598401 -0.84946895 -0.75556159 -1.0162632 -1.7011778 -2.7175074 -3.7262423 -4.4020567 -4.5617008 -4.4366713][-2.624671 -2.4159586 -1.8453898 -0.931659 0.15566587 1.0419822 1.6920357 1.9573889 1.5378428 0.6677351 -0.59608173 -2.0220156 -3.2796721 -3.940115 -4.1046748][-2.7534924 -2.3894629 -1.7537136 -0.59268427 1.132246 2.7182283 3.8132935 4.222887 3.8798647 2.8513117 1.2456942 -0.49437594 -1.9848421 -2.9704237 -3.5143752][-3.2193294 -2.6962113 -1.9416115 -0.66281915 1.0545583 2.8427091 4.3017893 4.939 4.6971817 3.839344 2.3649883 0.5152564 -1.2192862 -2.3731933 -3.058744][-3.9130034 -3.5897641 -2.7226167 -1.5090926 -0.010796547 1.6778688 3.1981659 4.0120707 4.0579863 3.358089 2.1209464 0.59096432 -0.98876834 -2.263978 -3.0660262][-4.3815441 -4.4751015 -4.2264056 -3.4203315 -2.0308568 -0.45844412 0.85619736 1.8302441 2.2829094 1.9324617 1.03199 -0.34193325 -1.7060194 -2.7762909 -3.4686265][-5.221735 -5.1432576 -4.8703117 -4.6576643 -4.0490818 -2.9068608 -1.7170284 -0.72195435 -0.18281841 -0.0721674 -0.42075109 -1.3614576 -2.4829979 -3.4509726 -4.0386324][-5.5285053 -5.6632786 -5.5409126 -5.3567238 -4.9053135 -4.3264146 -3.6496508 -2.9205399 -2.3944695 -2.1505151 -2.2060585 -2.7310905 -3.376492 -4.028173 -4.4951882][-5.4104137 -5.6129608 -5.7371564 -5.7016287 -5.5819197 -5.2248545 -4.7133923 -4.1726537 -3.7018933 -3.3866556 -3.3128433 -3.602941 -3.9124269 -4.3567138 -4.7078729][-4.9883132 -5.1251478 -5.1902137 -5.20134 -5.1573343 -4.9261193 -4.7403169 -4.4149728 -4.1061559 -3.7826512 -3.505754 -3.6037204 -3.830899 -4.1205249 -4.3274159][-4.2806764 -4.2453136 -4.2032518 -4.3505726 -4.4400911 -4.3005271 -4.1693935 -3.979672 -3.8308415 -3.6581841 -3.4939141 -3.4237626 -3.3837066 -3.5164635 -3.6874084]]...]
INFO - root - 2017-12-16 08:47:36.672331: step 64410, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.284 sec/batch; 21h:08m:35s remains)
INFO - root - 2017-12-16 08:47:39.548115: step 64420, loss = 0.23, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 21h:04m:26s remains)
INFO - root - 2017-12-16 08:47:42.361816: step 64430, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.284 sec/batch; 21h:08m:03s remains)
INFO - root - 2017-12-16 08:47:45.150304: step 64440, loss = 0.48, batch loss = 0.42 (27.7 examples/sec; 0.289 sec/batch; 21h:28m:59s remains)
INFO - root - 2017-12-16 08:47:47.997543: step 64450, loss = 0.34, batch loss = 0.28 (28.5 examples/sec; 0.280 sec/batch; 20h:52m:50s remains)
INFO - root - 2017-12-16 08:47:50.879044: step 64460, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.284 sec/batch; 21h:08m:26s remains)
INFO - root - 2017-12-16 08:47:53.744851: step 64470, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.285 sec/batch; 21h:15m:12s remains)
INFO - root - 2017-12-16 08:47:56.598516: step 64480, loss = 0.28, batch loss = 0.22 (29.2 examples/sec; 0.274 sec/batch; 20h:25m:34s remains)
INFO - root - 2017-12-16 08:47:59.430750: step 64490, loss = 0.23, batch loss = 0.17 (27.7 examples/sec; 0.288 sec/batch; 21h:28m:25s remains)
INFO - root - 2017-12-16 08:48:02.293504: step 64500, loss = 0.26, batch loss = 0.20 (27.8 examples/sec; 0.288 sec/batch; 21h:26m:14s remains)
2017-12-16 08:48:02.837454: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5523794 -2.7512283 -3.1664195 -3.5584941 -3.8927765 -4.1967821 -4.4399571 -4.4253869 -4.6400466 -5.0066514 -5.2893128 -5.2997675 -5.138721 -4.9782543 -4.8041391][-1.351295 -1.6058509 -2.2869241 -3.0462947 -3.7137694 -4.2536592 -4.5359378 -4.7894564 -4.9406533 -5.1429062 -5.4141607 -5.3403163 -5.0807786 -4.9032016 -4.7961078][-0.71616983 -1.1136286 -1.9099123 -2.5856571 -3.0930574 -3.73461 -4.2431803 -4.50975 -4.7777224 -5.0710778 -5.1961832 -5.0471945 -4.9323025 -4.6841626 -4.8710909][-0.7837522 -0.93673658 -1.5504694 -2.062773 -2.6689973 -3.2129753 -3.417712 -3.404459 -3.4296162 -3.7960305 -4.3032641 -4.4541826 -4.5577965 -4.7689118 -5.1014261][-1.2805128 -1.3673079 -1.5938942 -1.7724867 -1.8474681 -1.8111751 -1.7864985 -1.7259822 -1.844718 -2.1273887 -2.4678359 -3.0387158 -3.7191379 -4.4445214 -5.0297384][-1.7489412 -1.7647917 -1.6823034 -1.4203079 -0.87344837 -0.46526718 -0.052107334 0.33548212 0.43520498 0.011562347 -0.90792084 -1.7839172 -2.6833334 -3.7377844 -4.5000196][-2.3431714 -2.2310386 -1.8189547 -1.0961964 -0.20296192 0.63735485 1.6640105 2.2945366 2.4216132 1.8338442 0.833498 -0.31541443 -1.6076479 -2.7500157 -3.5690312][-2.8164656 -2.7385879 -2.29168 -1.3600259 -0.38713217 0.83578634 2.2289705 3.0132256 3.250443 2.6754913 1.7023487 0.41584635 -1.0163512 -2.1809344 -3.0053802][-3.4554052 -3.7491646 -3.4790514 -2.6263828 -1.6536188 -0.068346024 1.4862852 2.423583 2.7770414 2.3744617 1.489337 0.12420511 -1.1165862 -1.9033654 -2.54006][-4.0458026 -4.7303643 -4.9643793 -4.4745164 -3.5098567 -2.0481071 -0.5105319 0.541121 1.0426617 0.83921289 0.21766281 -0.89368439 -1.9522362 -2.5579443 -2.8804946][-4.5111804 -5.5279365 -6.08323 -6.1030884 -5.5966825 -4.37901 -2.796463 -1.7328825 -1.3343213 -1.3678563 -1.7410622 -2.4880717 -3.0783646 -3.339191 -3.3505793][-4.3903995 -5.4875569 -6.556798 -7.0895958 -7.1010408 -6.427074 -5.1485128 -4.0649881 -3.3672531 -3.1682496 -3.4492028 -3.9322052 -4.2362738 -4.2245321 -3.968118][-3.7982616 -4.8251634 -5.9937038 -6.707417 -7.1226778 -6.93337 -6.1538043 -5.3291187 -4.5773597 -4.1072516 -4.0316677 -4.3901186 -4.6357627 -4.517446 -4.2757668][-2.9781387 -3.8803804 -4.9150457 -5.6485562 -6.1934972 -6.1972046 -5.8018713 -5.2853217 -4.578053 -3.937119 -3.5849421 -3.7642274 -4.1230812 -4.1625977 -3.9193439][-2.1949298 -2.7696326 -3.6234007 -4.3393712 -4.9013181 -5.0624638 -4.7710066 -4.1936808 -3.5309963 -3.0931072 -2.9120812 -2.9472475 -3.1632342 -3.3681078 -3.477128]]...]
INFO - root - 2017-12-16 08:48:05.668614: step 64510, loss = 0.21, batch loss = 0.15 (29.4 examples/sec; 0.272 sec/batch; 20h:13m:37s remains)
INFO - root - 2017-12-16 08:48:08.554547: step 64520, loss = 0.37, batch loss = 0.31 (28.2 examples/sec; 0.284 sec/batch; 21h:06m:53s remains)
INFO - root - 2017-12-16 08:48:11.485959: step 64530, loss = 0.25, batch loss = 0.19 (28.7 examples/sec; 0.279 sec/batch; 20h:45m:17s remains)
INFO - root - 2017-12-16 08:48:14.307021: step 64540, loss = 0.32, batch loss = 0.26 (27.8 examples/sec; 0.287 sec/batch; 21h:22m:57s remains)
INFO - root - 2017-12-16 08:48:17.154000: step 64550, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.281 sec/batch; 20h:55m:46s remains)
INFO - root - 2017-12-16 08:48:19.971779: step 64560, loss = 0.32, batch loss = 0.26 (27.8 examples/sec; 0.288 sec/batch; 21h:26m:54s remains)
INFO - root - 2017-12-16 08:48:22.842298: step 64570, loss = 0.38, batch loss = 0.33 (28.8 examples/sec; 0.278 sec/batch; 20h:41m:26s remains)
INFO - root - 2017-12-16 08:48:25.690997: step 64580, loss = 0.33, batch loss = 0.27 (28.9 examples/sec; 0.277 sec/batch; 20h:36m:45s remains)
INFO - root - 2017-12-16 08:48:28.567818: step 64590, loss = 0.22, batch loss = 0.16 (26.4 examples/sec; 0.303 sec/batch; 22h:33m:00s remains)
INFO - root - 2017-12-16 08:48:31.491918: step 64600, loss = 0.26, batch loss = 0.20 (25.4 examples/sec; 0.316 sec/batch; 23h:28m:56s remains)
2017-12-16 08:48:32.012618: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6814246 -4.6183791 -4.6243734 -4.7410374 -4.8613505 -5.0688581 -5.1883235 -5.3559828 -5.7253351 -6.0379505 -6.1349382 -6.1133294 -5.9868712 -5.8712821 -5.6283636][-4.8606572 -4.7680793 -4.9957924 -5.3032851 -5.5367823 -5.6488743 -5.7812166 -5.7194214 -5.6840396 -6.0567064 -6.5349092 -6.60674 -6.4257617 -6.2727718 -6.0675607][-4.1070433 -4.539525 -4.9902577 -5.6078796 -6.1144142 -5.9270964 -5.5223694 -5.37484 -5.3841634 -5.4209137 -5.6282206 -6.162889 -6.6485028 -6.653852 -6.5237942][-3.5206447 -4.2362494 -4.8827572 -5.5331697 -5.9698124 -5.7465854 -5.0369606 -4.182137 -3.6140468 -3.7134714 -3.8581185 -4.4367046 -5.2348156 -5.9058361 -6.5280428][-3.1282964 -3.771518 -4.5693569 -5.1990523 -5.4544487 -4.9450741 -3.7334743 -2.2540216 -1.1618772 -1.1115656 -1.4249277 -2.2395208 -3.2495108 -4.722486 -6.1141181][-2.3695672 -2.9683928 -3.665025 -4.3296885 -4.2858367 -3.4079974 -1.9416864 -0.35530519 0.90904665 1.4775238 1.1779275 -0.077343941 -1.5587366 -3.2886987 -4.9197454][-1.1378987 -2.0662844 -2.9885693 -3.31563 -2.925159 -1.7378838 0.017303467 1.4373307 2.6079764 3.0354896 2.7450242 1.5144901 -0.17743492 -2.1024673 -4.0429072][-0.758301 -1.6845217 -2.7879832 -3.0279365 -2.5865057 -1.1520214 0.7271328 2.2759962 3.5939903 3.6294689 3.1148767 2.1544542 0.62695932 -1.4252338 -3.635433][-1.3166897 -2.1043112 -2.7819793 -2.9862359 -2.8513341 -1.4983339 0.065281868 1.4846544 2.7035966 2.8303819 2.5098896 1.5660338 0.10514164 -1.6591055 -3.6763704][-1.1503961 -2.4262285 -3.6013212 -3.6905437 -3.0956492 -2.0349085 -1.2746282 -0.33586884 0.50631857 0.85888815 1.0355458 0.23298597 -1.0767293 -2.6874776 -4.4514461][-1.9633162 -2.9737294 -3.8534331 -4.2459025 -3.9739497 -3.2137685 -2.4722292 -2.1073465 -1.8605103 -1.6311626 -1.384445 -1.7848682 -2.4951105 -3.6307468 -5.0988169][-3.0099692 -4.03455 -5.1028776 -5.4445925 -5.4039326 -4.8743925 -4.299046 -4.0366325 -3.8380494 -3.8929105 -4.0490108 -4.2678881 -4.6144872 -5.3944468 -6.2169685][-4.1347189 -5.1442475 -6.1138363 -6.536829 -6.8849163 -6.5883827 -6.2110181 -5.8079553 -5.4596868 -5.3533993 -5.3227153 -5.6527596 -6.220118 -6.6851234 -7.01198][-4.7987857 -5.4959249 -6.3473749 -6.8506031 -7.3297253 -7.2643657 -7.0846539 -6.873117 -6.5337038 -6.3471537 -6.1880307 -6.3385792 -6.6820183 -6.8324432 -6.891964][-5.2333632 -5.5522232 -6.1920824 -6.7236042 -7.2164221 -7.4620256 -7.4822516 -7.2736359 -6.986021 -6.8057165 -6.5839882 -6.6398878 -6.6821671 -6.6810408 -6.3734913]]...]
INFO - root - 2017-12-16 08:48:34.832451: step 64610, loss = 0.24, batch loss = 0.19 (28.0 examples/sec; 0.285 sec/batch; 21h:13m:43s remains)
INFO - root - 2017-12-16 08:48:37.718859: step 64620, loss = 0.24, batch loss = 0.18 (27.7 examples/sec; 0.288 sec/batch; 21h:27m:23s remains)
INFO - root - 2017-12-16 08:48:40.616375: step 64630, loss = 0.19, batch loss = 0.13 (28.3 examples/sec; 0.282 sec/batch; 21h:01m:08s remains)
INFO - root - 2017-12-16 08:48:43.468751: step 64640, loss = 0.32, batch loss = 0.26 (28.1 examples/sec; 0.285 sec/batch; 21h:12m:47s remains)
INFO - root - 2017-12-16 08:48:46.325787: step 64650, loss = 0.24, batch loss = 0.18 (28.7 examples/sec; 0.279 sec/batch; 20h:43m:26s remains)
INFO - root - 2017-12-16 08:48:49.155346: step 64660, loss = 0.28, batch loss = 0.22 (27.7 examples/sec; 0.289 sec/batch; 21h:30m:47s remains)
INFO - root - 2017-12-16 08:48:51.989990: step 64670, loss = 0.26, batch loss = 0.20 (28.2 examples/sec; 0.283 sec/batch; 21h:04m:26s remains)
INFO - root - 2017-12-16 08:48:54.889168: step 64680, loss = 0.28, batch loss = 0.22 (28.7 examples/sec; 0.279 sec/batch; 20h:46m:18s remains)
INFO - root - 2017-12-16 08:48:57.799375: step 64690, loss = 0.30, batch loss = 0.25 (27.9 examples/sec; 0.286 sec/batch; 21h:17m:53s remains)
INFO - root - 2017-12-16 08:49:00.668915: step 64700, loss = 0.29, batch loss = 0.23 (28.1 examples/sec; 0.285 sec/batch; 21h:10m:43s remains)
2017-12-16 08:49:01.183661: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.3530078 -7.1169472 -6.9259167 -6.8856907 -6.7277689 -6.7670383 -6.7592597 -7.05383 -7.3970871 -7.6121421 -7.64496 -7.2533894 -6.5064287 -5.5497522 -4.5837817][-7.2477322 -7.1118565 -6.7843466 -6.7731924 -6.7679625 -7.0488739 -7.3487368 -7.7959366 -8.0545063 -8.2237816 -8.339098 -8.031497 -7.3510108 -6.401691 -5.4207315][-6.2152009 -5.9295778 -5.6878872 -5.5922642 -5.5245981 -5.8716469 -6.3739576 -7.1335773 -7.8002443 -8.1625252 -8.2415352 -8.1375914 -7.9092464 -7.1691437 -6.260891][-4.5063629 -3.7407393 -3.2202578 -3.092314 -3.1662247 -3.5710104 -3.9929354 -4.8284879 -5.6360693 -6.4875154 -7.1920843 -7.5549417 -7.7570124 -7.4508781 -6.7008667][-2.7367373 -1.4104254 -0.24361753 0.010363102 -0.085587025 -0.54916811 -1.2293892 -1.9689887 -2.6178288 -3.5547369 -4.8755741 -5.9812851 -6.6738062 -7.003994 -6.7114811][-1.874383 -0.14182663 1.3022752 2.1555557 2.6852198 2.3737831 1.9069843 1.2808661 0.565742 -0.52226377 -2.243789 -4.2234306 -5.7736378 -6.5110197 -6.4738503][-2.5935795 -0.5571034 1.0982099 2.3601327 3.4604454 3.8786516 4.0381775 3.6800461 3.1213317 1.8736424 -0.15741634 -2.5577614 -4.6341305 -5.7274833 -5.8178654][-3.8139052 -1.7455974 0.0078687668 1.6773472 3.2023492 4.0758142 4.5493155 4.3609247 3.8320675 2.390202 0.11581039 -2.25675 -4.0552626 -5.0913906 -5.2220526][-5.3759594 -3.7224574 -1.9901328 -0.29103327 1.436132 2.818294 3.7047796 3.9401188 3.5052404 2.2176113 0.24331665 -2.1607087 -3.9242442 -4.8862529 -4.9915833][-6.7125196 -5.5359759 -4.2943969 -2.7650135 -0.96032 0.46611977 1.4067421 1.8680964 1.6630049 0.355309 -1.2204595 -3.1570826 -4.6893239 -5.2986217 -5.2025695][-7.2963724 -6.467484 -5.5345721 -4.3603039 -2.9745107 -1.6882224 -0.66639423 -0.32635355 -0.64179754 -1.6128025 -2.8181095 -4.0298023 -5.1125417 -5.5444055 -5.3812251][-6.7279367 -6.2904129 -5.7696028 -4.9673433 -4.0029521 -3.1050074 -2.5665271 -2.3383904 -2.5453286 -3.3000526 -4.0636916 -4.92626 -5.420723 -5.4501204 -5.1319366][-5.4749913 -5.2355938 -5.009686 -4.6576028 -4.2729158 -3.753767 -3.4867396 -3.4479113 -3.5492477 -4.0581264 -4.4579759 -4.9251804 -5.3345776 -5.3847351 -5.0585375][-4.0240846 -3.9644375 -3.9057484 -3.7893276 -3.712867 -3.5347953 -3.4733214 -3.6254787 -3.8491578 -4.1947212 -4.3280859 -4.4988933 -4.6488895 -4.5184603 -4.2590671][-3.0211382 -2.8236051 -2.7244296 -2.746213 -2.7832727 -2.8422804 -2.8995349 -3.0045853 -3.1649976 -3.4222279 -3.6049933 -3.5299931 -3.4179854 -3.3512726 -3.2190788]]...]
INFO - root - 2017-12-16 08:49:04.068839: step 64710, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.284 sec/batch; 21h:07m:07s remains)
INFO - root - 2017-12-16 08:49:06.917261: step 64720, loss = 0.31, batch loss = 0.25 (28.5 examples/sec; 0.281 sec/batch; 20h:53m:13s remains)
INFO - root - 2017-12-16 08:49:09.777108: step 64730, loss = 0.25, batch loss = 0.20 (27.7 examples/sec; 0.289 sec/batch; 21h:28m:03s remains)
INFO - root - 2017-12-16 08:49:12.593613: step 64740, loss = 0.25, batch loss = 0.19 (27.4 examples/sec; 0.292 sec/batch; 21h:41m:24s remains)
INFO - root - 2017-12-16 08:49:15.485983: step 64750, loss = 0.31, batch loss = 0.25 (27.6 examples/sec; 0.290 sec/batch; 21h:33m:51s remains)
INFO - root - 2017-12-16 08:49:18.368086: step 64760, loss = 0.27, batch loss = 0.21 (27.7 examples/sec; 0.289 sec/batch; 21h:29m:16s remains)
INFO - root - 2017-12-16 08:49:21.197698: step 64770, loss = 0.36, batch loss = 0.30 (27.9 examples/sec; 0.287 sec/batch; 21h:21m:18s remains)
INFO - root - 2017-12-16 08:49:24.092735: step 64780, loss = 0.28, batch loss = 0.23 (28.7 examples/sec; 0.278 sec/batch; 20h:42m:03s remains)
INFO - root - 2017-12-16 08:49:26.974842: step 64790, loss = 0.32, batch loss = 0.26 (26.9 examples/sec; 0.297 sec/batch; 22h:07m:22s remains)
INFO - root - 2017-12-16 08:49:29.842237: step 64800, loss = 0.45, batch loss = 0.40 (27.4 examples/sec; 0.292 sec/batch; 21h:43m:18s remains)
2017-12-16 08:49:30.422143: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4810176 -2.6400485 -2.9474397 -3.3234351 -3.3292251 -3.1843877 -3.2072363 -3.4790015 -3.8331389 -4.32384 -5.1378069 -6.0864615 -6.9858704 -7.155931 -7.0149922][-1.4563236 -1.7676909 -2.158144 -2.6143808 -2.6996775 -2.6853838 -2.7654238 -3.0231771 -3.3927267 -4.0150909 -4.8578959 -5.8107343 -6.8474507 -7.3281012 -7.5053463][-1.1954141 -1.5735085 -2.0123954 -2.3035047 -2.3113225 -2.1524138 -2.1079075 -2.3952913 -2.8097076 -3.3396811 -4.1241369 -5.223155 -6.3096056 -6.8860111 -7.2494655][-0.5661819 -0.92367983 -1.2361345 -1.3587503 -1.2097199 -0.9827404 -0.862484 -1.1224477 -1.5902038 -2.1386793 -2.8727269 -3.939271 -5.1460361 -5.9782023 -6.470624][-0.913321 -0.98100019 -0.96199584 -0.88133478 -0.58719277 -0.23925686 -0.09295702 -0.21056366 -0.43562222 -0.92217183 -1.5282595 -2.3848314 -3.4122372 -4.179678 -4.6988783][-1.7698343 -1.5201848 -1.1215 -0.87673354 -0.46642303 0.032972336 0.40241289 0.42945433 0.28035021 0.13257504 -0.17622185 -0.86068034 -1.7684619 -2.5643339 -3.073216][-2.7412357 -2.1727455 -1.5211961 -0.91511488 -0.17834091 0.39580631 0.81473017 0.92200947 0.89535427 0.8600955 0.54575634 0.07781744 -0.43487477 -1.1573398 -1.6461954][-3.6727574 -2.9358463 -2.1448269 -1.3958361 -0.56619787 0.15169239 0.74579287 0.91636753 1.0337663 1.0930037 0.849061 0.51755857 0.22216368 -0.10541153 -0.24221134][-4.6570182 -3.8498898 -3.011507 -2.1829381 -1.3225245 -0.63099 -0.064558983 0.19913769 0.4792881 0.55237246 0.326324 0.12188721 0.056599617 0.089715958 0.16616344][-5.1298127 -4.3408895 -3.6061773 -2.8295293 -2.067821 -1.542114 -1.0963542 -0.85673261 -0.63549328 -0.51190209 -0.60496855 -0.83156848 -0.8214407 -0.68757677 -0.47617555][-5.2375336 -4.7325358 -4.2183857 -3.5748796 -2.9537864 -2.4769285 -2.096386 -1.9387565 -1.8339138 -1.9250598 -2.1807277 -2.3990426 -2.3336079 -2.1447275 -1.791671][-5.22859 -5.0749712 -4.9023008 -4.559556 -4.124259 -3.7690229 -3.4745445 -3.2787285 -3.2240715 -3.3435688 -3.5370617 -3.7447414 -3.8015385 -3.6305332 -3.183754][-4.7243495 -4.8516264 -5.0212207 -5.0361896 -4.9725704 -4.8500242 -4.6393485 -4.3506584 -4.2400765 -4.4205666 -4.7136464 -4.9203253 -5.0093246 -5.0764494 -4.8796744][-3.7080264 -4.0394149 -4.4514666 -4.6967044 -4.8922906 -4.9113083 -4.8739958 -4.7425795 -4.7731242 -5.0163779 -5.3631134 -5.7301412 -5.9513373 -6.0906105 -5.9730425][-2.7784495 -2.9656863 -3.3241832 -3.6953549 -4.0448232 -4.31505 -4.4927411 -4.5248423 -4.6785154 -4.9406157 -5.3108759 -5.7050309 -6.0945158 -6.3275023 -6.2022672]]...]
INFO - root - 2017-12-16 08:49:33.247294: step 64810, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.280 sec/batch; 20h:47m:11s remains)
INFO - root - 2017-12-16 08:49:36.138151: step 64820, loss = 0.39, batch loss = 0.34 (27.6 examples/sec; 0.290 sec/batch; 21h:32m:45s remains)
INFO - root - 2017-12-16 08:49:38.956208: step 64830, loss = 0.25, batch loss = 0.19 (28.5 examples/sec; 0.281 sec/batch; 20h:53m:33s remains)
INFO - root - 2017-12-16 08:49:41.803129: step 64840, loss = 0.29, batch loss = 0.23 (26.3 examples/sec; 0.304 sec/batch; 22h:36m:41s remains)
INFO - root - 2017-12-16 08:49:44.677199: step 64850, loss = 0.19, batch loss = 0.13 (28.6 examples/sec; 0.280 sec/batch; 20h:46m:53s remains)
INFO - root - 2017-12-16 08:49:47.535440: step 64860, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.285 sec/batch; 21h:13m:24s remains)
INFO - root - 2017-12-16 08:49:50.366152: step 64870, loss = 0.34, batch loss = 0.29 (28.2 examples/sec; 0.283 sec/batch; 21h:03m:42s remains)
INFO - root - 2017-12-16 08:49:53.183977: step 64880, loss = 0.26, batch loss = 0.20 (27.4 examples/sec; 0.292 sec/batch; 21h:40m:25s remains)
INFO - root - 2017-12-16 08:49:56.054231: step 64890, loss = 0.27, batch loss = 0.22 (27.4 examples/sec; 0.292 sec/batch; 21h:40m:51s remains)
INFO - root - 2017-12-16 08:49:58.941325: step 64900, loss = 0.26, batch loss = 0.20 (27.4 examples/sec; 0.292 sec/batch; 21h:44m:11s remains)
2017-12-16 08:49:59.466963: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.075531 -3.0852523 -3.1676345 -3.2800293 -3.3456802 -3.3397245 -3.3127198 -3.247551 -3.1827281 -3.0594406 -2.9178679 -2.8338704 -2.7115126 -2.6151102 -2.504426][-3.2455204 -3.3524981 -3.4469771 -3.5738852 -3.6880505 -3.7609391 -3.6442611 -3.4079065 -3.1947289 -3.0698915 -3.0367498 -2.9142141 -2.8255076 -2.7338696 -2.5769658][-3.2498655 -3.5790019 -3.8594174 -3.8238654 -3.6500878 -3.5355248 -3.2568965 -3.0006862 -2.8720479 -2.8553455 -3.0940146 -3.1591985 -3.1659582 -3.0966153 -2.8665998][-3.1455998 -3.4919658 -3.7649655 -3.7318013 -3.3957484 -2.9244308 -2.2965996 -2.0278988 -2.047173 -2.2890081 -2.7296584 -3.0392308 -3.3734493 -3.452915 -3.2528219][-2.8700964 -3.0454445 -3.1490254 -3.0334017 -2.5293565 -1.7617748 -1.0001204 -0.59442568 -0.54625058 -1.0734305 -1.9071438 -2.6271205 -3.2266779 -3.3953736 -3.3280649][-2.5860751 -2.723134 -2.5934143 -1.9744279 -0.9269259 0.15378189 1.1913991 1.6848435 1.4348388 0.53976679 -0.59208918 -1.6438737 -2.6164351 -3.1140761 -3.1796217][-2.0389719 -1.8972683 -1.4478049 -0.59543681 0.732543 2.0823884 3.2206836 3.6669817 3.3352475 2.1520972 0.44168091 -1.1739504 -2.3325458 -2.8107789 -2.8715425][-1.7702699 -1.5634553 -1.0267203 0.023301601 1.5241013 3.0167089 4.0999451 4.231287 3.4086447 1.832242 0.15912151 -1.4033096 -2.6346879 -3.0904105 -3.0440435][-1.9291623 -1.6720827 -1.2083371 -0.39590454 0.78459787 2.0650997 2.9671059 2.8139415 2.0211797 0.59255791 -0.96471119 -2.5640588 -3.6972787 -3.782515 -3.4770706][-1.8952236 -1.7410765 -1.3410845 -0.67015648 0.091905594 0.73630476 1.3276362 1.1584492 0.28780746 -1.2689128 -2.838824 -4.0141158 -4.7415 -4.707859 -4.1956811][-1.9700313 -1.9718554 -1.8213353 -1.3216887 -0.69468594 -0.29029179 -0.60028696 -1.4046853 -2.4076781 -3.6055987 -4.687119 -5.6104803 -5.9579172 -5.5980835 -4.8715606][-2.1657684 -1.9236565 -1.9599743 -2.0087779 -2.0732317 -2.1794031 -2.5524049 -3.3446102 -4.4308639 -5.5709734 -6.3448863 -6.9753456 -6.902534 -6.3012753 -5.4669776][-2.0408146 -1.775537 -1.7183104 -1.8350272 -2.2433445 -2.7384031 -3.1955433 -4.1581264 -5.0598736 -5.7892065 -6.3511734 -6.7953854 -6.7744331 -6.4232831 -5.8771024][-1.8438251 -1.4452894 -1.1709049 -1.2835426 -1.5896516 -2.0847585 -2.8053071 -3.5117774 -4.219151 -4.708168 -5.2106109 -5.6936531 -5.779283 -5.6458044 -5.4223909][-1.5712106 -1.0283103 -0.60945678 -0.51055884 -0.73197269 -1.0862908 -1.8465433 -2.5674758 -3.1650083 -3.6362009 -4.1511765 -4.5390735 -4.560946 -4.68243 -4.6526957]]...]
INFO - root - 2017-12-16 08:50:02.308111: step 64910, loss = 0.23, batch loss = 0.17 (27.6 examples/sec; 0.289 sec/batch; 21h:30m:38s remains)
INFO - root - 2017-12-16 08:50:05.131605: step 64920, loss = 0.28, batch loss = 0.22 (28.6 examples/sec; 0.280 sec/batch; 20h:47m:25s remains)
INFO - root - 2017-12-16 08:50:07.979711: step 64930, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.278 sec/batch; 20h:41m:16s remains)
INFO - root - 2017-12-16 08:50:10.859513: step 64940, loss = 0.24, batch loss = 0.18 (29.0 examples/sec; 0.276 sec/batch; 20h:29m:11s remains)
INFO - root - 2017-12-16 08:50:13.684576: step 64950, loss = 0.27, batch loss = 0.21 (28.3 examples/sec; 0.282 sec/batch; 20h:59m:33s remains)
INFO - root - 2017-12-16 08:50:16.584390: step 64960, loss = 0.24, batch loss = 0.19 (27.9 examples/sec; 0.286 sec/batch; 21h:17m:06s remains)
INFO - root - 2017-12-16 08:50:19.439732: step 64970, loss = 0.24, batch loss = 0.19 (27.6 examples/sec; 0.290 sec/batch; 21h:33m:03s remains)
INFO - root - 2017-12-16 08:50:22.293471: step 64980, loss = 0.24, batch loss = 0.18 (28.5 examples/sec; 0.281 sec/batch; 20h:53m:06s remains)
INFO - root - 2017-12-16 08:50:25.143735: step 64990, loss = 0.20, batch loss = 0.15 (28.7 examples/sec; 0.279 sec/batch; 20h:43m:57s remains)
INFO - root - 2017-12-16 08:50:28.009043: step 65000, loss = 0.22, batch loss = 0.16 (27.7 examples/sec; 0.288 sec/batch; 21h:25m:51s remains)
2017-12-16 08:50:28.537517: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5455909 -5.5946975 -5.7595062 -6.1910729 -6.3254457 -6.0313478 -5.5568695 -5.4487438 -5.4864593 -5.9385934 -6.2528443 -6.5918803 -6.9705 -6.9188128 -6.7077904][-5.2249737 -5.5056429 -5.7120781 -5.9078588 -5.9706821 -6.1461287 -5.9806061 -5.7956781 -5.7396293 -5.9469743 -6.4019361 -7.1360016 -7.6104436 -7.6434946 -7.3604331][-4.7358379 -4.946156 -5.0241337 -5.2260389 -5.1926289 -5.1404886 -4.9320726 -4.9649191 -5.067975 -5.4509916 -6.3047962 -6.8765187 -7.55883 -7.5792522 -7.6280785][-3.8248429 -3.9653857 -3.7518735 -3.6336675 -3.3966634 -3.2506218 -2.94915 -3.0355654 -3.0039856 -3.3242497 -4.0418105 -5.3967266 -7.0117512 -7.7011375 -7.9837451][-3.1154776 -2.6288395 -2.0445361 -1.6814554 -1.1992745 -0.57190895 -0.12427568 -0.076280594 -0.2703414 -1.0092187 -2.1506522 -3.5394883 -5.2651019 -6.9046659 -8.0638447][-2.8685362 -2.128783 -0.87777734 0.051926613 0.9809761 1.642334 2.2084503 2.6841273 2.8468437 2.0869417 0.63789797 -1.747961 -4.4695497 -6.4949384 -7.7436714][-3.3986018 -2.0938146 -0.64752388 0.63333035 2.064033 3.1577024 4.2795153 4.8173685 4.886775 4.1061411 2.4593835 -0.080684662 -2.9297142 -5.53624 -7.3735523][-3.7151871 -2.5131714 -1.1794641 0.34097004 1.6770625 2.8744464 4.2440176 5.0479555 5.3770018 4.5752296 2.9347205 0.50645828 -2.4399743 -4.7654963 -6.3191471][-4.5860209 -3.4620986 -2.3535764 -1.2214231 -0.12778425 1.1472602 2.393621 3.2504578 3.8184395 3.332058 2.1644073 0.13421535 -2.0888708 -4.1075549 -5.526484][-5.0368633 -4.5502305 -4.1903214 -3.3609869 -2.55435 -1.6848817 -0.76010108 0.13588572 0.75152159 0.76481533 0.15808105 -1.0826547 -2.6050453 -4.0653167 -4.9514923][-5.6646523 -5.3078222 -5.0180845 -4.9613724 -4.945477 -4.4077082 -3.5917244 -2.8623271 -2.4110947 -2.3369534 -2.3158503 -2.8255091 -3.5793345 -4.2520494 -4.6731949][-5.464397 -5.5655994 -5.9137626 -5.9023709 -5.9687457 -6.0586205 -5.7279334 -5.1044745 -4.490674 -4.0284991 -3.6928458 -3.8675983 -4.1117592 -4.5251703 -4.8127174][-4.8238473 -4.9715395 -5.3567028 -5.6291533 -6.0762687 -6.2082324 -6.0203991 -5.8365226 -5.5037084 -5.0980287 -4.7288694 -4.4766703 -4.2818508 -4.4235511 -4.5973258][-4.2334032 -4.2420564 -4.47235 -4.8027105 -5.181057 -5.3069081 -5.3385367 -5.3104982 -5.1234241 -4.8662305 -4.5341334 -4.4319735 -4.3250809 -4.2190814 -4.1202435][-3.7301171 -3.6933124 -3.7702229 -3.9959579 -4.2433014 -4.3305459 -4.3269429 -4.16169 -3.909359 -3.709729 -3.5059824 -3.3050969 -3.1771412 -3.1933308 -3.2357478]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-65000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-65000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 08:50:32.022048: step 65010, loss = 0.41, batch loss = 0.35 (28.1 examples/sec; 0.284 sec/batch; 21h:07m:39s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 08:50:34.856260: step 65020, loss = 0.30, batch loss = 0.24 (28.7 examples/sec; 0.279 sec/batch; 20h:43m:46s remains)
INFO - root - 2017-12-16 08:50:37.749551: step 65030, loss = 0.25, batch loss = 0.19 (27.4 examples/sec; 0.292 sec/batch; 21h:43m:39s remains)
INFO - root - 2017-12-16 08:50:40.573857: step 65040, loss = 0.28, batch loss = 0.22 (28.2 examples/sec; 0.284 sec/batch; 21h:06m:10s remains)
INFO - root - 2017-12-16 08:50:43.379395: step 65050, loss = 0.28, batch loss = 0.23 (29.6 examples/sec; 0.270 sec/batch; 20h:03m:28s remains)
INFO - root - 2017-12-16 08:50:46.228177: step 65060, loss = 0.24, batch loss = 0.18 (27.8 examples/sec; 0.288 sec/batch; 21h:23m:07s remains)
INFO - root - 2017-12-16 08:50:49.061944: step 65070, loss = 0.22, batch loss = 0.16 (28.0 examples/sec; 0.286 sec/batch; 21h:13m:00s remains)
INFO - root - 2017-12-16 08:50:51.904056: step 65080, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 20h:42m:33s remains)
INFO - root - 2017-12-16 08:50:54.766682: step 65090, loss = 0.22, batch loss = 0.16 (27.6 examples/sec; 0.290 sec/batch; 21h:30m:41s remains)
INFO - root - 2017-12-16 08:50:57.560597: step 65100, loss = 0.33, batch loss = 0.27 (29.8 examples/sec; 0.268 sec/batch; 19h:54m:47s remains)
2017-12-16 08:50:58.080710: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9371173 -4.430532 -4.8902493 -5.1800141 -5.3304205 -5.2821226 -5.0343857 -4.9488711 -4.7948384 -4.5473962 -4.2009277 -3.7342706 -3.2459223 -2.6494575 -2.1755185][-3.4187016 -3.8287828 -4.2997093 -4.660718 -4.7589955 -4.8328371 -4.69586 -4.4594588 -4.0155292 -3.7195866 -3.4519715 -3.1609826 -2.8232327 -2.1737981 -1.5307143][-2.6375966 -3.3063068 -3.8622441 -4.2123675 -4.2709236 -4.0922914 -3.7782688 -3.4311073 -3.1116333 -2.9133213 -2.7991209 -2.4147332 -2.0863664 -1.8141661 -1.6956487][-2.2232437 -2.750351 -3.2806473 -3.4673114 -3.2863684 -3.1541059 -2.7909622 -2.175822 -1.6192088 -1.5668719 -1.8057594 -2.00586 -2.187 -1.989553 -1.6646669][-1.9163084 -2.3775678 -2.7255063 -2.7155914 -2.3905704 -1.8235006 -1.1399572 -0.39310837 0.046419621 -0.27432728 -0.92159343 -1.2919593 -1.5004606 -1.9986546 -2.1802223][-1.4887545 -1.742552 -1.787214 -1.6259289 -0.90426135 -0.18296194 0.57280731 1.2621918 1.6251864 1.132432 0.25280905 -0.72512007 -1.6338849 -2.09054 -2.1202967][-1.7789133 -1.6624777 -1.2669163 -0.4920361 0.61555767 1.6165905 2.4366531 2.9925323 2.9909592 2.1327949 0.92775726 -0.24826384 -1.2900274 -2.3034461 -2.8375268][-2.3317523 -2.0112088 -1.2745948 -0.18027115 1.2612276 2.4456692 3.2315769 3.7298441 3.6306772 2.494781 1.0929532 -0.2578516 -1.4763474 -2.594202 -3.2213333][-2.6621552 -2.249409 -1.4152067 -0.23668671 1.1173196 2.276227 2.9964132 3.2605743 2.8077855 1.7847142 0.55687571 -0.81083965 -2.051311 -3.0806613 -3.7400749][-3.1968946 -2.9093196 -2.1426384 -0.99706316 0.20427942 1.0183144 1.506424 1.7026668 1.4197497 0.49374247 -0.69709063 -1.807699 -2.8132327 -3.6614437 -4.2819128][-3.2155101 -3.0169137 -2.5798984 -1.9012365 -1.2002394 -0.48757243 -0.033933163 0.031345367 -0.30624533 -0.84734488 -1.5486987 -2.3737135 -3.1937165 -3.9035175 -4.3880234][-2.6843016 -2.8292336 -2.7415514 -2.3453093 -2.0994997 -1.9756956 -1.7948742 -1.5705667 -1.5693464 -1.9186366 -2.3451097 -2.7229097 -3.1716893 -3.7060752 -4.1466618][-2.2449207 -2.4469295 -2.7373636 -2.8700309 -3.0679724 -3.0186381 -2.8266401 -2.7038577 -2.6666579 -2.7131734 -2.8957069 -3.1613474 -3.4378972 -3.7491534 -3.9582613][-1.5563362 -1.8028519 -2.1525905 -2.5358343 -3.0326533 -3.2598116 -3.2599926 -3.1803832 -3.0949154 -2.9634573 -2.9197874 -3.0130835 -3.2590289 -3.4911096 -3.6482339][-0.66707754 -1.0045583 -1.6521864 -2.2729332 -3.0301385 -3.4266243 -3.45901 -3.2178283 -2.9726737 -2.81861 -2.7531974 -2.713062 -2.7934275 -2.9697847 -3.0739322]]...]
INFO - root - 2017-12-16 08:51:00.919526: step 65110, loss = 0.25, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 20h:47m:45s remains)
INFO - root - 2017-12-16 08:51:03.764030: step 65120, loss = 0.28, batch loss = 0.22 (27.8 examples/sec; 0.288 sec/batch; 21h:23m:47s remains)
INFO - root - 2017-12-16 08:51:06.601508: step 65130, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 21h:00m:45s remains)
INFO - root - 2017-12-16 08:51:09.560700: step 65140, loss = 0.27, batch loss = 0.21 (28.4 examples/sec; 0.282 sec/batch; 20h:56m:14s remains)
INFO - root - 2017-12-16 08:51:12.397076: step 65150, loss = 0.23, batch loss = 0.18 (26.8 examples/sec; 0.299 sec/batch; 22h:11m:27s remains)
INFO - root - 2017-12-16 08:51:15.223952: step 65160, loss = 0.26, batch loss = 0.20 (28.8 examples/sec; 0.277 sec/batch; 20h:35m:33s remains)
INFO - root - 2017-12-16 08:51:18.065044: step 65170, loss = 0.36, batch loss = 0.30 (26.4 examples/sec; 0.304 sec/batch; 22h:32m:35s remains)
INFO - root - 2017-12-16 08:51:20.952735: step 65180, loss = 0.22, batch loss = 0.16 (26.6 examples/sec; 0.301 sec/batch; 22h:21m:14s remains)
INFO - root - 2017-12-16 08:51:23.800666: step 65190, loss = 0.31, batch loss = 0.25 (27.0 examples/sec; 0.296 sec/batch; 21h:58m:19s remains)
INFO - root - 2017-12-16 08:51:26.644931: step 65200, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.280 sec/batch; 20h:48m:40s remains)
2017-12-16 08:51:27.148816: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.119657 -5.6003995 -5.1885743 -5.1079221 -5.0928745 -5.0655179 -5.0673857 -4.9325418 -4.7824607 -4.4889221 -3.8529654 -3.0828972 -2.3148434 -1.9092882 -1.9625444][-5.5689893 -5.1499004 -4.7583637 -4.4921904 -4.2196279 -4.1083612 -4.199501 -4.0650883 -3.8992941 -3.4137461 -2.7953875 -2.1431603 -1.4921803 -0.8669107 -0.83143878][-4.8109565 -4.0842581 -3.5102732 -3.1731119 -2.6394958 -2.6004734 -2.863564 -2.9861405 -2.8487349 -2.7350459 -2.3280976 -1.5687194 -0.9077673 -0.35119343 -0.26158142][-2.9649663 -2.1875439 -1.782475 -1.5248787 -1.2675381 -1.0900061 -1.3050153 -2.0720882 -2.5818624 -2.4817488 -1.9640512 -1.6775672 -1.3531582 -0.74999166 -0.3375864][-1.3754058 -0.22007608 0.61682415 0.66637373 0.64538288 0.5730567 0.0073623657 -0.73174882 -1.4062204 -1.967927 -2.3035266 -2.1281283 -1.7295539 -1.5492983 -1.4882393][-0.34123468 1.096663 1.9925232 2.4001651 2.7296648 2.340416 1.5678935 0.59309053 -0.31482458 -1.149286 -1.8860247 -2.2172692 -2.5268269 -2.238843 -1.6805904][0.13457155 1.5409522 2.3907609 2.8020234 3.0647182 3.056632 2.5488448 1.4465036 0.33422422 -0.59929276 -1.5165339 -2.1569929 -2.7211013 -2.9088891 -2.6968055][-0.32473135 0.79421282 1.5955362 2.1358032 2.46457 2.5925164 2.2989306 1.5847821 0.59690809 -0.57236481 -1.645906 -2.5612702 -3.1261115 -3.2985435 -3.1221752][-2.1081927 -1.0294769 -0.26209736 0.32868528 0.90495777 1.3540492 1.4305258 0.84502029 0.038143158 -1.1080501 -2.121073 -3.1058939 -3.7642848 -3.8826594 -3.5481517][-3.3500857 -3.1277976 -3.0267203 -2.2193563 -1.2985556 -0.643697 -0.41552353 -0.74549961 -1.2914772 -2.0987434 -3.001066 -3.9848933 -4.6040869 -4.4844174 -4.0015831][-5.1173048 -5.0040388 -4.750762 -4.4507213 -3.8828125 -2.9660373 -2.311322 -2.308784 -2.7559929 -3.4963188 -4.2080479 -4.8173509 -5.2283349 -5.0306993 -4.6635709][-5.9375629 -6.1329503 -6.2145219 -5.8475151 -5.1301122 -4.4556289 -3.9923027 -3.9698622 -4.0551529 -4.4165387 -4.7403269 -5.275372 -5.51418 -5.0483255 -4.5993576][-5.6699 -5.9638805 -6.2169566 -6.0218987 -5.5764413 -5.002305 -4.6267424 -4.5447288 -4.5271997 -4.6697459 -4.8526621 -5.0967031 -5.1654878 -4.6960549 -4.2599325][-4.8397546 -5.1829877 -5.4766355 -5.5455508 -5.4386835 -4.91078 -4.3891044 -4.495327 -4.5660157 -4.3879361 -4.3017039 -4.3905764 -4.3524203 -3.9196897 -3.5947349][-4.2363157 -4.4275532 -4.609283 -4.7267642 -4.6889286 -4.4229932 -4.2227411 -4.1009812 -3.9446285 -3.88556 -3.7834005 -3.6314549 -3.3494215 -2.8853059 -2.5327551]]...]
INFO - root - 2017-12-16 08:51:30.014118: step 65210, loss = 0.38, batch loss = 0.32 (28.0 examples/sec; 0.286 sec/batch; 21h:14m:32s remains)
INFO - root - 2017-12-16 08:51:32.876442: step 65220, loss = 0.25, batch loss = 0.19 (27.4 examples/sec; 0.292 sec/batch; 21h:41m:34s remains)
INFO - root - 2017-12-16 08:51:35.756660: step 65230, loss = 0.31, batch loss = 0.26 (27.7 examples/sec; 0.288 sec/batch; 21h:24m:54s remains)
INFO - root - 2017-12-16 08:51:38.657184: step 65240, loss = 0.26, batch loss = 0.20 (28.4 examples/sec; 0.282 sec/batch; 20h:56m:10s remains)
INFO - root - 2017-12-16 08:51:41.483210: step 65250, loss = 0.63, batch loss = 0.57 (28.8 examples/sec; 0.278 sec/batch; 20h:37m:32s remains)
INFO - root - 2017-12-16 08:51:44.307560: step 65260, loss = 0.27, batch loss = 0.21 (29.4 examples/sec; 0.272 sec/batch; 20h:10m:06s remains)
INFO - root - 2017-12-16 08:51:47.127671: step 65270, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.285 sec/batch; 21h:10m:21s remains)
INFO - root - 2017-12-16 08:51:49.974391: step 65280, loss = 0.21, batch loss = 0.15 (28.2 examples/sec; 0.284 sec/batch; 21h:05m:36s remains)
INFO - root - 2017-12-16 08:51:52.833979: step 65290, loss = 0.22, batch loss = 0.16 (28.4 examples/sec; 0.282 sec/batch; 20h:55m:39s remains)
INFO - root - 2017-12-16 08:51:55.646706: step 65300, loss = 0.22, batch loss = 0.17 (27.6 examples/sec; 0.290 sec/batch; 21h:30m:47s remains)
2017-12-16 08:51:56.173309: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6885085 -2.6413944 -2.7227592 -2.8867657 -2.8939714 -2.8508358 -2.8010623 -2.6825824 -2.555335 -2.42134 -2.2769601 -2.0820343 -1.8497534 -1.7452288 -1.8822405][-2.000561 -2.096992 -2.288722 -2.5020428 -2.534966 -2.4462762 -2.3913331 -2.3670835 -2.3197095 -2.1808536 -2.072886 -2.0239418 -1.9688864 -1.647294 -1.3248603][-1.7583528 -2.0017476 -2.2540889 -2.3722768 -2.220803 -1.95755 -1.8004122 -1.8198326 -1.920737 -2.1774867 -2.4815762 -2.5842714 -2.5277762 -2.1356943 -1.5418434][-1.7676399 -2.2463753 -2.6759837 -2.6808877 -2.3615422 -1.7164266 -1.0810983 -0.95330596 -1.1915779 -1.844584 -2.4949152 -3.0751915 -3.2595344 -2.76935 -1.9106312][-2.0915115 -2.6093814 -2.8498845 -2.7501409 -2.0799665 -1.0153182 -0.2215848 -0.089211941 -0.51183033 -1.4896243 -2.6353407 -3.4006181 -3.6167507 -3.3730059 -2.6566248][-2.4715934 -2.890676 -2.9273005 -2.2284088 -0.97896028 0.39503193 1.4167805 1.5305581 0.7314558 -0.74737358 -2.3791752 -3.6511247 -4.2973852 -4.1412873 -3.380506][-3.0556307 -3.0224037 -2.6537149 -1.5888753 0.0020928383 1.8240185 3.090426 3.1064124 2.1053896 0.35702515 -1.5317013 -3.2157145 -4.3294196 -4.7488179 -4.3429375][-3.3454037 -3.0708146 -2.3873537 -0.95715117 0.75827646 2.5600653 3.82306 3.8226709 2.7572279 0.84967089 -1.2688634 -3.1253381 -4.3532462 -4.9843731 -4.8880062][-3.6356225 -3.1399994 -2.1884758 -0.78143 0.89399147 2.4687819 3.5207891 3.5501466 2.5230565 0.6399889 -1.5497425 -3.514039 -4.893456 -5.6897173 -5.6258864][-3.8422871 -3.5320773 -2.7792811 -1.5689952 -0.11548376 1.1041007 1.9434824 2.0133824 1.2215652 -0.36232424 -2.3308084 -4.2300878 -5.6444297 -6.4403868 -6.352314][-4.0910306 -3.9136376 -3.4156508 -2.6407876 -1.5967345 -0.69338894 -0.083301067 -0.14128065 -0.82825255 -2.0354226 -3.4853466 -4.8599539 -5.9213405 -6.5810843 -6.48186][-3.7921686 -3.8952525 -3.9077616 -3.5101311 -2.7262514 -2.0350423 -1.6034257 -1.6474288 -2.1511643 -3.0275426 -4.0841403 -5.0298662 -5.6831064 -6.2320557 -6.3180041][-3.4674664 -3.7600412 -3.8758154 -3.7576127 -3.320312 -2.8205838 -2.3476834 -2.1470222 -2.6024113 -3.2537355 -3.9006965 -4.5514503 -5.0485082 -5.4923987 -5.5980244][-3.2823739 -3.6959848 -3.8202367 -3.697969 -3.261375 -2.8381205 -2.4609804 -2.1787934 -2.2307086 -2.5195622 -2.9681225 -3.4293582 -3.7500548 -4.0994768 -4.3178253][-2.9536502 -3.3249681 -3.4994349 -3.3185289 -2.7424631 -2.0931404 -1.6666038 -1.3843675 -1.3331046 -1.3907657 -1.6934216 -2.0310218 -2.3789055 -2.8255162 -3.0554705]]...]
INFO - root - 2017-12-16 08:51:58.947196: step 65310, loss = 0.24, batch loss = 0.18 (29.5 examples/sec; 0.271 sec/batch; 20h:08m:46s remains)
INFO - root - 2017-12-16 08:52:01.791267: step 65320, loss = 0.31, batch loss = 0.25 (28.7 examples/sec; 0.279 sec/batch; 20h:41m:51s remains)
INFO - root - 2017-12-16 08:52:04.699611: step 65330, loss = 0.37, batch loss = 0.31 (27.0 examples/sec; 0.296 sec/batch; 21h:57m:20s remains)
INFO - root - 2017-12-16 08:52:07.505493: step 65340, loss = 0.27, batch loss = 0.22 (29.4 examples/sec; 0.272 sec/batch; 20h:11m:06s remains)
INFO - root - 2017-12-16 08:52:10.415730: step 65350, loss = 0.25, batch loss = 0.19 (26.6 examples/sec; 0.300 sec/batch; 22h:17m:42s remains)
INFO - root - 2017-12-16 08:52:13.305807: step 65360, loss = 0.22, batch loss = 0.17 (28.5 examples/sec; 0.280 sec/batch; 20h:48m:21s remains)
INFO - root - 2017-12-16 08:52:16.234098: step 65370, loss = 0.24, batch loss = 0.19 (26.7 examples/sec; 0.300 sec/batch; 22h:15m:01s remains)
INFO - root - 2017-12-16 08:52:19.100080: step 65380, loss = 0.23, batch loss = 0.17 (27.7 examples/sec; 0.289 sec/batch; 21h:25m:10s remains)
INFO - root - 2017-12-16 08:52:21.979886: step 65390, loss = 0.30, batch loss = 0.24 (27.6 examples/sec; 0.290 sec/batch; 21h:30m:27s remains)
INFO - root - 2017-12-16 08:52:24.836347: step 65400, loss = 0.30, batch loss = 0.24 (29.0 examples/sec; 0.275 sec/batch; 20h:26m:13s remains)
2017-12-16 08:52:25.381106: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6639793 -1.7518492 -1.8748221 -1.9130199 -2.0531325 -1.9881814 -1.979429 -1.8591592 -1.9183276 -2.2146385 -2.2852407 -2.3989666 -2.4708667 -2.7302399 -2.9259105][-1.2753911 -1.4922223 -1.8537126 -2.063117 -2.1736298 -2.0566022 -2.0778797 -1.9164047 -1.85689 -2.0744994 -2.2647076 -2.3852053 -2.36941 -2.5814729 -2.88408][-1.4519217 -1.9200082 -2.4359918 -2.6880581 -2.7162194 -2.4562924 -2.2172406 -2.07718 -1.9476421 -1.9713447 -2.1723042 -2.469152 -2.7231107 -2.9110005 -3.1072226][-1.8274119 -2.4657261 -3.1084661 -3.3213665 -3.1424198 -2.7264128 -2.1431706 -1.5485618 -1.2093661 -1.3320231 -1.6369183 -2.0753369 -2.4635425 -2.7226119 -2.9889259][-2.144109 -2.9150553 -3.6890049 -3.7638755 -3.2981222 -2.597651 -1.8474002 -1.05809 -0.50168061 -0.66417837 -1.0948794 -1.5740316 -1.9542863 -2.1420014 -2.3386366][-2.6263955 -3.1303172 -3.821476 -3.8906028 -3.252614 -2.1794209 -1.0464103 -0.05787468 0.57351923 0.46022081 -0.17166233 -0.77958131 -1.1451373 -1.4037383 -1.6609933][-2.3745427 -2.9496973 -3.4219425 -3.3589375 -2.7142835 -1.4117649 0.0959115 1.0892391 1.7180943 1.5823817 0.7822547 -0.056637764 -0.60622048 -0.77584052 -0.93235874][-1.30095 -1.8674188 -2.6322036 -2.4682343 -1.7406418 -0.6342957 0.79768944 1.8831778 2.4032388 2.1284285 1.3847909 0.60913706 0.032821655 -0.17138195 -0.34556389][-0.55977011 -1.0666516 -1.9028296 -1.9555433 -1.4007199 -0.29076385 0.92543364 1.9029531 2.3941689 2.0607128 1.3896966 0.74485731 0.28639841 0.16244507 0.01982832][-0.11994314 -0.5536623 -1.5288448 -1.8374753 -1.5798693 -0.61877894 0.36000586 1.0608845 1.4885197 1.2929926 0.76969862 0.30208302 0.069897175 -0.034686089 -0.15873289][0.010373116 -0.51169944 -1.4833324 -1.9253187 -1.9726014 -1.2696726 -0.41677666 0.15677595 0.581583 0.4060359 -0.11796999 -0.57321024 -0.70462918 -0.58470345 -0.57740927][-0.20388269 -0.83604169 -1.8710291 -2.2456527 -2.2300653 -1.791451 -1.2034745 -0.85055709 -0.58536935 -0.61968136 -0.94404793 -1.3959954 -1.504813 -1.2246437 -1.0559604][-0.50069785 -0.891783 -1.6769249 -2.1284926 -2.2271345 -1.9562316 -1.7106345 -1.5870638 -1.4749012 -1.5410709 -1.8481464 -2.1554596 -2.123944 -1.6974351 -1.2918096][-0.32107878 -0.57007933 -1.0689392 -1.4281423 -1.6164391 -1.4744859 -1.4259505 -1.6979246 -1.8789575 -1.9712684 -2.2597051 -2.706193 -2.6194286 -1.9300706 -1.2418187][-0.30122805 -0.16156721 -0.38144541 -0.48181987 -0.465194 -0.42296743 -0.72308612 -1.2413495 -1.7167771 -2.1632268 -2.697927 -3.1348228 -3.0033731 -2.1398504 -1.047657]]...]
INFO - root - 2017-12-16 08:52:28.241374: step 65410, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.283 sec/batch; 21h:01m:08s remains)
INFO - root - 2017-12-16 08:52:31.093565: step 65420, loss = 0.28, batch loss = 0.22 (27.9 examples/sec; 0.287 sec/batch; 21h:17m:39s remains)
INFO - root - 2017-12-16 08:52:33.924065: step 65430, loss = 0.22, batch loss = 0.16 (28.9 examples/sec; 0.277 sec/batch; 20h:32m:18s remains)
INFO - root - 2017-12-16 08:52:36.785050: step 65440, loss = 0.35, batch loss = 0.29 (29.0 examples/sec; 0.276 sec/batch; 20h:28m:39s remains)
INFO - root - 2017-12-16 08:52:39.611717: step 65450, loss = 0.28, batch loss = 0.22 (28.3 examples/sec; 0.283 sec/batch; 20h:59m:30s remains)
INFO - root - 2017-12-16 08:52:42.432143: step 65460, loss = 0.36, batch loss = 0.30 (28.8 examples/sec; 0.277 sec/batch; 20h:34m:35s remains)
INFO - root - 2017-12-16 08:52:45.278134: step 65470, loss = 0.23, batch loss = 0.17 (29.1 examples/sec; 0.275 sec/batch; 20h:24m:46s remains)
INFO - root - 2017-12-16 08:52:48.099649: step 65480, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 20h:45m:06s remains)
INFO - root - 2017-12-16 08:52:50.866156: step 65490, loss = 0.28, batch loss = 0.22 (29.1 examples/sec; 0.275 sec/batch; 20h:23m:57s remains)
INFO - root - 2017-12-16 08:52:53.685516: step 65500, loss = 0.23, batch loss = 0.18 (27.8 examples/sec; 0.288 sec/batch; 21h:22m:03s remains)
2017-12-16 08:52:54.225311: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0376742 -3.192512 -3.0875123 -2.9048066 -2.8290863 -2.676456 -2.534893 -2.4619429 -2.4745235 -2.2357512 -1.8431067 -1.4062166 -1.0328369 -0.72782612 -0.62059784][-2.9347105 -3.2069478 -3.3203096 -3.2597871 -3.1659365 -2.838815 -2.5020049 -2.1859133 -2.081533 -1.9081063 -1.6669881 -1.2630808 -1.0240114 -0.87264419 -0.96303368][-3.3565478 -3.9027481 -4.1915021 -4.1729951 -3.8721411 -3.2733717 -2.4652796 -1.7192526 -1.3092804 -1.0052762 -0.99788833 -1.0620377 -1.1954615 -1.2757492 -1.5432942][-4.2387834 -4.8532434 -5.0495663 -4.9933834 -4.5566754 -3.569881 -2.2193851 -1.0485735 -0.32658672 -0.031525135 -0.14722013 -0.35569859 -1.0189841 -1.5409153 -2.0450871][-4.8654017 -5.5471768 -5.6571541 -5.2985544 -4.3950944 -3.0176501 -1.2357268 0.4247098 1.3957481 1.6309018 1.1158247 0.46118736 -0.49976254 -1.4752316 -2.4248183][-4.8694825 -5.6672912 -5.7542429 -5.1616669 -3.7911253 -2.0599611 -0.0032143593 2.0435057 3.4736829 3.6961174 2.9244819 1.8291574 0.412611 -1.0882423 -2.387234][-4.5794759 -5.3765936 -5.500587 -4.9373684 -3.4420624 -1.158056 1.4425611 3.520093 4.9657869 5.5139275 4.7388649 3.0938802 1.1938095 -0.54699445 -2.0951457][-4.0502658 -4.8595304 -5.0449805 -4.4887242 -3.1493225 -1.0205154 1.6253643 4.0971918 5.659481 5.9631958 5.0954609 3.5218496 1.5298944 -0.55546474 -2.3390121][-3.3241215 -4.3748651 -4.773561 -4.2477078 -3.1156373 -1.29807 0.79942846 2.8921552 4.45669 4.837389 4.0564432 2.4998875 0.63062572 -1.2360029 -2.8072767][-2.9326475 -4.1354833 -5.0964589 -4.9363155 -4.0875535 -2.6843739 -1.0956731 0.67070389 2.0559206 2.5188332 1.9940791 0.69495392 -0.84361053 -2.32382 -3.5716522][-3.0904207 -3.8650346 -4.6267242 -5.0034962 -4.8556232 -3.6855679 -2.5394325 -1.5869062 -0.61479688 -0.15362406 -0.41258192 -1.3248122 -2.3443995 -3.3742614 -4.2297187][-3.600966 -4.0955863 -4.5071912 -4.6083941 -4.5618234 -4.1350174 -3.5371423 -2.9173572 -2.3971636 -2.2312543 -2.3483706 -2.8492837 -3.3679338 -3.8769033 -4.4117084][-3.3982296 -3.6691878 -3.7376263 -3.6145897 -3.7331071 -3.7409303 -3.6215048 -3.5605454 -3.3371682 -3.2537937 -3.25669 -3.3883066 -3.5466218 -3.6496091 -3.8558028][-3.4891577 -3.12435 -2.8243361 -2.4249446 -2.2443573 -2.3354402 -2.5710614 -3.0641789 -3.3647227 -3.2951138 -3.0936859 -3.2068317 -3.2788646 -3.0810826 -3.0393016][-3.5886462 -2.9019454 -2.1259422 -1.5486372 -1.3352103 -1.2479036 -1.5013659 -2.1253812 -2.4857171 -2.7024434 -2.7668991 -2.6986408 -2.6228366 -2.4674156 -2.2772131]]...]
INFO - root - 2017-12-16 08:52:57.041269: step 65510, loss = 0.33, batch loss = 0.27 (28.7 examples/sec; 0.278 sec/batch; 20h:39m:08s remains)
INFO - root - 2017-12-16 08:52:59.920924: step 65520, loss = 0.36, batch loss = 0.31 (28.0 examples/sec; 0.286 sec/batch; 21h:12m:14s remains)
INFO - root - 2017-12-16 08:53:02.676892: step 65530, loss = 0.20, batch loss = 0.14 (28.1 examples/sec; 0.285 sec/batch; 21h:08m:40s remains)
INFO - root - 2017-12-16 08:53:05.521137: step 65540, loss = 0.25, batch loss = 0.19 (28.3 examples/sec; 0.283 sec/batch; 20h:58m:46s remains)
INFO - root - 2017-12-16 08:53:08.373888: step 65550, loss = 0.27, batch loss = 0.21 (27.6 examples/sec; 0.289 sec/batch; 21h:28m:02s remains)
INFO - root - 2017-12-16 08:53:11.227019: step 65560, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 20h:39m:33s remains)
INFO - root - 2017-12-16 08:53:14.067579: step 65570, loss = 0.22, batch loss = 0.16 (28.3 examples/sec; 0.283 sec/batch; 20h:57m:33s remains)
INFO - root - 2017-12-16 08:53:16.911043: step 65580, loss = 0.27, batch loss = 0.21 (28.6 examples/sec; 0.279 sec/batch; 20h:42m:15s remains)
INFO - root - 2017-12-16 08:53:19.729742: step 65590, loss = 0.28, batch loss = 0.22 (29.0 examples/sec; 0.276 sec/batch; 20h:25m:42s remains)
INFO - root - 2017-12-16 08:53:22.561086: step 65600, loss = 0.30, batch loss = 0.24 (28.2 examples/sec; 0.284 sec/batch; 21h:02m:58s remains)
2017-12-16 08:53:23.121322: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3551791 -2.8717794 -3.3343673 -3.5382569 -3.6595054 -3.717622 -3.6022549 -3.2890987 -3.1070848 -2.9598494 -2.7906449 -2.7857523 -2.8728902 -2.9913919 -3.133028][-2.1369803 -2.5125527 -2.986063 -3.1217861 -3.1870701 -3.2182586 -3.1015944 -2.9618979 -2.9162774 -2.7887328 -2.8613422 -2.8114748 -2.6485791 -2.4391537 -2.4824431][-2.3729126 -2.2812419 -2.3626764 -2.2650378 -2.2816234 -2.3374217 -2.4926562 -2.587923 -2.5805893 -2.6120706 -2.655158 -2.5882647 -2.5770442 -2.3436439 -2.2111866][-2.0756111 -1.891763 -1.8101308 -1.2730298 -0.90552115 -0.892668 -1.1982601 -1.6415524 -1.9138126 -2.2269349 -2.395891 -2.5616417 -2.7978384 -2.7496767 -2.6955724][-1.8792434 -1.3698218 -0.7237761 -0.23317671 0.038750648 0.227005 0.18485832 -0.30601597 -0.84965348 -1.2393746 -1.8620193 -2.5246747 -3.0568171 -3.3927989 -3.5970407][-2.0557141 -1.2516067 -0.40504026 0.45059538 1.2890487 1.5727816 1.6516075 1.5069523 1.0948963 0.48466396 -0.47716093 -1.7758107 -3.055583 -3.746747 -4.0803919][-2.5362277 -1.3364756 -0.24701548 0.92346716 2.0544071 2.7644277 3.3681498 3.1383495 2.6735196 2.0802011 0.83131313 -0.84476829 -2.4877639 -3.742177 -4.4239063][-3.5401912 -2.2675476 -1.0999212 0.2624402 1.4733324 2.5187283 3.5962629 3.7034712 3.2538896 2.3850031 1.1053534 -0.63157344 -2.7208886 -3.8763866 -4.4488573][-5.2284913 -4.1719141 -2.849956 -1.4085922 -0.12720537 1.1220713 2.2676067 2.7773728 2.8049955 1.9412613 0.47830915 -1.3685524 -3.3288913 -4.5101547 -4.9620228][-6.2762346 -5.840076 -5.1348042 -3.9758885 -2.5277753 -1.0925715 0.053414345 0.48851919 0.68731833 -0.044655323 -1.4449599 -3.1351752 -4.7621732 -5.5434866 -5.6884251][-6.7684755 -6.5124321 -6.23592 -5.8330259 -5.0204697 -3.9035885 -2.5038466 -1.8980634 -2.0861819 -2.8151379 -3.8050566 -4.9795995 -6.0203977 -6.2531271 -5.9541974][-6.7475834 -6.5984869 -6.3702011 -6.1088634 -5.771524 -5.1819305 -4.3278422 -3.9399815 -3.9155855 -4.3278208 -5.118885 -6.0273991 -6.594739 -6.4929552 -5.9110155][-5.8578305 -6.0080566 -6.1502013 -5.884038 -5.5408545 -5.28289 -5.0398045 -4.967308 -4.9951463 -5.3642311 -5.9215708 -6.0869493 -6.0342016 -5.5877934 -5.0862527][-4.9308195 -4.8619208 -4.9588385 -5.0604916 -5.0512056 -4.7714667 -4.5439806 -4.7565479 -5.0970373 -5.1322808 -5.1992517 -5.2919235 -5.2143722 -4.5940323 -3.79166][-4.1525903 -4.0857105 -4.0758538 -4.1236038 -4.2083817 -4.2215867 -4.2605114 -4.3169317 -4.4431133 -4.5270476 -4.5480614 -4.3935337 -4.1759019 -3.7309344 -3.1435199]]...]
INFO - root - 2017-12-16 08:53:25.991667: step 65610, loss = 0.44, batch loss = 0.38 (27.4 examples/sec; 0.292 sec/batch; 21h:37m:32s remains)
INFO - root - 2017-12-16 08:53:28.822728: step 65620, loss = 0.22, batch loss = 0.16 (28.1 examples/sec; 0.285 sec/batch; 21h:08m:12s remains)
INFO - root - 2017-12-16 08:53:31.641481: step 65630, loss = 0.29, batch loss = 0.24 (28.6 examples/sec; 0.280 sec/batch; 20h:46m:10s remains)
INFO - root - 2017-12-16 08:53:34.516103: step 65640, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.283 sec/batch; 20h:59m:45s remains)
INFO - root - 2017-12-16 08:53:37.341122: step 65650, loss = 0.33, batch loss = 0.27 (28.9 examples/sec; 0.277 sec/batch; 20h:32m:44s remains)
INFO - root - 2017-12-16 08:53:40.201421: step 65660, loss = 0.33, batch loss = 0.27 (28.8 examples/sec; 0.277 sec/batch; 20h:33m:56s remains)
INFO - root - 2017-12-16 08:53:43.006061: step 65670, loss = 0.28, batch loss = 0.22 (27.9 examples/sec; 0.287 sec/batch; 21h:17m:19s remains)
INFO - root - 2017-12-16 08:53:45.793799: step 65680, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.282 sec/batch; 20h:54m:32s remains)
INFO - root - 2017-12-16 08:53:48.618874: step 65690, loss = 0.22, batch loss = 0.16 (28.7 examples/sec; 0.278 sec/batch; 20h:37m:31s remains)
INFO - root - 2017-12-16 08:53:51.457454: step 65700, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.283 sec/batch; 21h:00m:28s remains)
2017-12-16 08:53:52.037992: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7146358 -2.8079665 -3.1083856 -3.5281179 -3.8666196 -4.1433811 -4.2309461 -4.3770332 -4.4203548 -4.1626291 -3.6157961 -3.1047912 -2.5934272 -2.1690872 -1.7163169][-2.3064644 -2.5522292 -2.9993424 -3.5122201 -3.837605 -4.0271153 -4.1237154 -4.113287 -3.88519 -3.3742805 -2.78589 -2.144428 -1.5758016 -1.1769042 -0.87206745][-2.3379471 -2.6910343 -3.1253462 -3.6083381 -3.8073902 -3.8067608 -3.5341728 -3.2424707 -3.0183043 -2.5320239 -1.951611 -1.4266706 -1.0562785 -0.79806828 -0.56446838][-2.6162987 -3.0603275 -3.4522653 -3.6674237 -3.5792782 -3.1708932 -2.47149 -1.6888225 -1.3183186 -0.9678371 -0.6789217 -0.69176149 -0.91932034 -0.98661971 -0.7993753][-3.1464777 -3.6634874 -3.9974022 -3.937382 -3.3977442 -2.5225756 -1.3279777 -0.19417572 0.33468246 0.56677485 0.41393948 0.013305664 -0.73021913 -1.2411156 -1.4172196][-3.4562309 -3.9626212 -4.1302509 -3.7754252 -2.8385434 -1.3922186 0.3088994 1.8528156 2.5648155 2.5278821 1.7597494 0.7093873 -0.48077607 -1.302165 -1.7272825][-3.4515667 -3.8495147 -3.7750168 -2.9850292 -1.6287727 0.34026623 2.5194526 4.2318277 4.7784939 4.3764572 3.1060066 1.3810282 -0.3001895 -1.3570662 -1.8051345][-3.5142121 -3.8785563 -3.604301 -2.5178859 -0.81084538 1.4904151 3.7792521 5.5405178 6.1458817 5.2137976 3.3297362 1.3411951 -0.4731741 -1.6059167 -2.0987635][-3.531565 -3.9704442 -3.8706229 -2.5352941 -0.57091951 1.6719995 3.7716351 5.2525473 5.6394062 4.5038824 2.5070028 0.45985222 -1.0901184 -2.0501428 -2.5064473][-3.6388323 -4.1421924 -4.224268 -3.2261076 -1.4810727 0.71987486 2.5544019 3.7342939 3.9280195 2.866333 0.96685743 -0.9473083 -2.28142 -2.9573107 -3.1818709][-4.1794958 -4.6470451 -4.57154 -3.6059868 -2.1447518 -0.3515048 1.1138086 1.9702802 1.8921227 0.98164225 -0.60001278 -2.2346032 -3.2958884 -3.7643845 -3.9594355][-4.6098309 -5.1228313 -5.1334672 -4.249711 -2.8317909 -1.3960741 -0.5008657 -0.075135708 -0.18771458 -1.06797 -2.3035476 -3.5012085 -4.3420043 -4.7719 -4.8617754][-4.7204032 -5.1143684 -5.2034945 -4.7591176 -4.0034671 -2.9392219 -2.0245657 -1.6873112 -1.9454439 -2.4807937 -3.3493702 -4.3714457 -5.0767817 -5.5791221 -5.7039347][-4.5602841 -4.687273 -4.577404 -4.3158836 -3.6821485 -2.997684 -2.3934727 -2.0943484 -2.3743339 -2.9620476 -3.7510853 -4.4761286 -5.0706091 -5.4840841 -5.4617367][-3.9018264 -3.9532423 -3.8166173 -3.5867834 -3.2242894 -2.6331229 -2.0376546 -1.8099806 -2.126128 -2.6561761 -3.3354893 -4.0768471 -4.6111789 -4.9076838 -4.8228021]]...]
INFO - root - 2017-12-16 08:53:54.834277: step 65710, loss = 0.31, batch loss = 0.26 (28.2 examples/sec; 0.284 sec/batch; 21h:03m:23s remains)
INFO - root - 2017-12-16 08:53:57.640742: step 65720, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.276 sec/batch; 20h:29m:16s remains)
INFO - root - 2017-12-16 08:54:00.421106: step 65730, loss = 0.28, batch loss = 0.23 (29.0 examples/sec; 0.276 sec/batch; 20h:25m:09s remains)
INFO - root - 2017-12-16 08:54:03.210837: step 65740, loss = 0.24, batch loss = 0.18 (29.3 examples/sec; 0.273 sec/batch; 20h:14m:46s remains)
INFO - root - 2017-12-16 08:54:06.081701: step 65750, loss = 0.26, batch loss = 0.21 (28.6 examples/sec; 0.280 sec/batch; 20h:43m:13s remains)
INFO - root - 2017-12-16 08:54:08.870173: step 65760, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.277 sec/batch; 20h:32m:20s remains)
INFO - root - 2017-12-16 08:54:11.709716: step 65770, loss = 0.32, batch loss = 0.26 (27.2 examples/sec; 0.294 sec/batch; 21h:45m:39s remains)
INFO - root - 2017-12-16 08:54:14.555695: step 65780, loss = 0.22, batch loss = 0.16 (28.2 examples/sec; 0.283 sec/batch; 20h:58m:56s remains)
INFO - root - 2017-12-16 08:54:17.377756: step 65790, loss = 0.33, batch loss = 0.27 (27.6 examples/sec; 0.290 sec/batch; 21h:26m:53s remains)
INFO - root - 2017-12-16 08:54:20.194717: step 65800, loss = 0.36, batch loss = 0.30 (29.2 examples/sec; 0.274 sec/batch; 20h:19m:09s remains)
2017-12-16 08:54:20.716307: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9885314 -4.0917692 -4.0484076 -3.9657176 -3.9330957 -3.8749108 -3.8544383 -3.8483934 -3.9620128 -4.0528941 -4.2013612 -4.0713129 -3.7629471 -3.3131852 -2.8494954][-3.9628091 -3.9128146 -3.8749855 -3.7437549 -3.6057024 -3.5789015 -3.5235929 -3.4863982 -3.6496525 -3.730376 -3.8246639 -3.7864883 -3.5905676 -3.0406761 -2.5305696][-3.5818107 -3.6589813 -3.6771383 -3.5060625 -3.2896829 -3.1076262 -2.9436693 -2.8713281 -2.9483175 -3.1560335 -3.4792533 -3.4059234 -3.1219144 -2.786716 -2.5411787][-2.9829516 -3.2516031 -3.4596589 -3.1479702 -2.69044 -2.2868245 -2.0913031 -2.0091336 -2.1186066 -2.3332224 -2.6585984 -3.0122709 -3.1798921 -2.8895197 -2.5230761][-2.3807039 -2.7581172 -2.9236507 -2.5318379 -1.9242451 -1.1753423 -0.65356636 -0.57982755 -0.83159351 -1.4103932 -2.1946278 -2.699645 -2.9131222 -3.1263704 -3.1077623][-1.8515022 -2.218219 -2.2877519 -1.7059743 -0.67354083 0.33512878 1.0747547 1.2825851 0.79844141 -0.16817379 -1.3498733 -2.4540613 -3.2048664 -3.4467826 -3.3958557][-1.684078 -1.9719927 -1.8913236 -0.98834538 0.37482214 1.8761063 2.9656348 3.1829567 2.5395751 1.1169853 -0.66100073 -2.256912 -3.2559855 -3.6126189 -3.7095819][-1.6818342 -1.9217615 -1.9937365 -0.967268 0.54751062 2.2506752 3.5407829 3.9324646 3.316762 1.6686316 -0.49398708 -2.4831712 -3.7393827 -4.2924256 -4.3936567][-1.9436765 -2.093369 -2.1942351 -1.5807576 -0.35237074 1.5210233 3.0423679 3.3865519 2.7193303 1.061821 -1.1233733 -3.2811439 -4.6293168 -5.1674109 -5.0457258][-2.1390691 -2.3718736 -2.6501038 -2.4368162 -1.5553265 -0.24566412 0.91675377 1.56003 1.1608162 -0.433964 -2.5350204 -4.4123011 -5.5736408 -5.8888879 -5.5510607][-2.2083874 -2.5834327 -3.2046294 -3.6245823 -3.4269512 -2.5459363 -1.5604091 -1.1903811 -1.5861087 -2.6140227 -4.2910838 -5.8398151 -6.7038522 -6.8286371 -6.3071079][-2.3082125 -2.7147532 -3.6791911 -4.4464884 -4.7646208 -4.659677 -4.2527637 -3.9201365 -3.9403467 -4.5983977 -5.7777882 -6.9765224 -7.5337534 -7.4223695 -6.8135862][-2.3370512 -2.8718174 -3.8168044 -4.8547096 -5.659049 -5.986093 -5.9736342 -5.8266325 -5.7723784 -6.1341763 -6.7185717 -7.3028088 -7.5737066 -7.5201173 -6.9959621][-2.2119269 -2.5741084 -3.3769455 -4.2611356 -4.969039 -5.5142107 -5.9067192 -6.1675186 -6.2483363 -6.3581271 -6.6098866 -7.00432 -7.2445364 -7.0166392 -6.5148163][-2.2395678 -2.2655106 -2.5939269 -3.1414981 -3.7836862 -4.3542323 -4.7784019 -4.9973836 -5.2075648 -5.3973904 -5.6282687 -5.8062425 -5.9148569 -5.9486508 -5.7328491]]...]
INFO - root - 2017-12-16 08:54:23.539143: step 65810, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.281 sec/batch; 20h:50m:02s remains)
INFO - root - 2017-12-16 08:54:26.441234: step 65820, loss = 0.34, batch loss = 0.28 (27.4 examples/sec; 0.292 sec/batch; 21h:39m:19s remains)
INFO - root - 2017-12-16 08:54:29.290460: step 65830, loss = 0.26, batch loss = 0.21 (27.7 examples/sec; 0.289 sec/batch; 21h:23m:04s remains)
INFO - root - 2017-12-16 08:54:32.117578: step 65840, loss = 0.25, batch loss = 0.19 (27.6 examples/sec; 0.290 sec/batch; 21h:28m:06s remains)
INFO - root - 2017-12-16 08:54:34.949858: step 65850, loss = 0.27, batch loss = 0.22 (29.1 examples/sec; 0.275 sec/batch; 20h:23m:50s remains)
INFO - root - 2017-12-16 08:54:37.820455: step 65860, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 20h:37m:46s remains)
INFO - root - 2017-12-16 08:54:40.624265: step 65870, loss = 0.34, batch loss = 0.28 (28.5 examples/sec; 0.281 sec/batch; 20h:47m:14s remains)
INFO - root - 2017-12-16 08:54:43.452227: step 65880, loss = 0.30, batch loss = 0.24 (28.2 examples/sec; 0.283 sec/batch; 20h:59m:37s remains)
INFO - root - 2017-12-16 08:54:46.372769: step 65890, loss = 0.45, batch loss = 0.39 (27.7 examples/sec; 0.289 sec/batch; 21h:22m:39s remains)
INFO - root - 2017-12-16 08:54:49.229003: step 65900, loss = 0.32, batch loss = 0.26 (28.6 examples/sec; 0.280 sec/batch; 20h:44m:25s remains)
2017-12-16 08:54:49.763311: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8811655 -6.4494476 -6.6793013 -6.8086128 -6.9112015 -6.8975458 -6.5791616 -6.5913897 -6.6000376 -6.6299763 -6.6976347 -6.9000039 -7.1764584 -7.4392972 -7.744976][-6.1044059 -6.7638407 -7.028532 -7.1257439 -7.0347843 -7.0501323 -7.0393543 -6.99625 -7.1091065 -7.386651 -7.5620751 -7.6643109 -7.9974728 -8.1285419 -8.24916][-5.9469547 -6.4159851 -6.4137259 -6.4996548 -6.3088126 -6.1152191 -5.9607954 -6.0732679 -6.4653997 -7.0184913 -7.5812745 -7.86413 -8.0342712 -7.9939137 -7.8161159][-5.4649105 -5.7107916 -5.6121945 -5.3662639 -4.5834103 -4.3198447 -4.0816374 -4.0123081 -4.7034349 -5.7123108 -6.4678631 -6.8985529 -6.7419844 -6.6746569 -6.5253158][-4.8144641 -4.7375269 -4.203444 -3.4873834 -2.4708643 -1.6788588 -0.97086406 -0.99514031 -1.7671208 -3.1202645 -4.2048435 -4.8960876 -4.6781979 -4.4767056 -4.3626533][-4.4604177 -3.9834154 -3.0027609 -1.7938147 0.13109541 1.5154243 2.2661805 2.4890366 1.3987722 -0.45467734 -2.029974 -2.712265 -2.402117 -2.1262438 -1.9022942][-4.61059 -4.0495682 -2.797358 -0.85986876 1.6758614 3.7773247 5.1199808 5.1670332 3.7835217 1.7623067 -0.079179764 -1.2956679 -1.11322 -0.55025482 -0.24533033][-5.1881223 -4.82011 -3.6717069 -1.5024321 1.382092 4.0073423 5.6187811 6.0629826 4.9280882 2.8271737 0.94135809 -0.35791254 -0.28373432 -0.052058697 0.087082386][-6.1919756 -5.9547324 -4.9634619 -2.9936984 -0.28833342 2.3683743 4.3119822 4.7373734 3.8489857 1.9124317 0.2677002 -0.8313024 -0.99628639 -0.77057743 -0.49751258][-6.8971634 -6.7878871 -6.2737694 -4.633718 -2.3734851 -0.20532227 1.3595819 1.7920113 1.2507138 -0.16373587 -1.2253344 -2.0562387 -2.1113367 -1.8622186 -1.6930363][-7.8404284 -7.9598427 -7.6376934 -6.4715862 -5.0067616 -3.3409233 -2.1971822 -1.7322164 -1.830261 -2.6707292 -3.4456756 -3.7586105 -3.8392522 -3.7081954 -3.5475721][-8.5154781 -8.7078876 -8.5338154 -7.7667994 -6.7489347 -5.7771049 -5.2351246 -4.8450794 -4.6218948 -4.8031921 -4.9362645 -5.2981811 -5.5063834 -5.5998373 -5.5905557][-8.5191708 -8.6215878 -8.6617165 -8.4718418 -8.07168 -7.5629959 -7.1175528 -6.662611 -6.0913906 -5.8724318 -5.6512365 -5.7359667 -6.0380592 -6.2417088 -6.3339157][-7.6882195 -7.655158 -7.6617808 -7.8073597 -7.8063293 -7.6659565 -7.4652648 -7.0940957 -6.5480261 -6.0691805 -5.6363864 -5.8189187 -6.1724849 -6.5264258 -6.77946][-7.0130491 -6.9129753 -6.7883711 -6.8135018 -6.778904 -6.8966885 -6.8403888 -6.5160027 -6.0536909 -5.6254482 -5.4073963 -5.4691887 -5.6668873 -6.1496191 -6.2710576]]...]
INFO - root - 2017-12-16 08:54:52.532873: step 65910, loss = 0.28, batch loss = 0.22 (28.9 examples/sec; 0.277 sec/batch; 20h:30m:30s remains)
INFO - root - 2017-12-16 08:54:55.407674: step 65920, loss = 0.25, batch loss = 0.20 (28.0 examples/sec; 0.286 sec/batch; 21h:09m:11s remains)
INFO - root - 2017-12-16 08:54:58.251115: step 65930, loss = 0.31, batch loss = 0.25 (26.7 examples/sec; 0.300 sec/batch; 22h:13m:26s remains)
INFO - root - 2017-12-16 08:55:01.050645: step 65940, loss = 0.29, batch loss = 0.24 (29.3 examples/sec; 0.273 sec/batch; 20h:14m:44s remains)
INFO - root - 2017-12-16 08:55:03.807430: step 65950, loss = 0.35, batch loss = 0.30 (29.3 examples/sec; 0.273 sec/batch; 20h:14m:29s remains)
INFO - root - 2017-12-16 08:55:06.660134: step 65960, loss = 0.26, batch loss = 0.21 (28.9 examples/sec; 0.277 sec/batch; 20h:31m:21s remains)
INFO - root - 2017-12-16 08:55:09.500350: step 65970, loss = 0.29, batch loss = 0.23 (29.4 examples/sec; 0.272 sec/batch; 20h:10m:10s remains)
INFO - root - 2017-12-16 08:55:12.362938: step 65980, loss = 0.32, batch loss = 0.26 (27.5 examples/sec; 0.291 sec/batch; 21h:32m:55s remains)
INFO - root - 2017-12-16 08:55:15.209182: step 65990, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.280 sec/batch; 20h:44m:13s remains)
INFO - root - 2017-12-16 08:55:18.054720: step 66000, loss = 0.22, batch loss = 0.17 (28.8 examples/sec; 0.278 sec/batch; 20h:34m:08s remains)
2017-12-16 08:55:18.633252: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8001447 -4.6931276 -4.87494 -5.493691 -6.3042321 -7.0058918 -7.4451652 -7.4968805 -7.1642132 -6.44757 -6.0607953 -5.8057327 -5.7896204 -5.8059158 -6.0217314][-4.1774812 -4.0675697 -4.0830922 -4.6819019 -5.4497662 -6.5360184 -7.052845 -7.06206 -6.655755 -5.9756465 -5.4934635 -5.247417 -5.417769 -5.5981274 -6.03969][-3.2634711 -2.8109245 -2.9162421 -3.5017259 -4.2865081 -5.2591858 -5.6483622 -5.8486261 -5.5754285 -5.0097475 -4.455687 -4.3495522 -4.5753522 -4.9441018 -5.4921007][-2.0006604 -1.5658116 -1.5762098 -1.9845033 -2.81262 -3.7324462 -3.8455579 -3.6942677 -3.4818029 -3.301338 -3.0042324 -2.9644008 -3.4783878 -4.1175761 -4.844676][-1.3033381 -0.80391431 -0.56356859 -0.77973413 -1.3974383 -2.2320421 -2.2567561 -2.006489 -1.6174333 -1.6222134 -1.4826341 -1.6879358 -2.378577 -3.3696878 -4.2660675][-1.4638836 -0.67998576 -0.41817665 -0.24988842 -0.25143337 -0.65942931 -0.34625244 0.24920702 0.56056547 0.41343069 0.37040377 -0.045455456 -0.91204357 -2.1565056 -3.0870051][-2.5243733 -1.6276965 -0.84466147 -0.34038591 0.0080423355 0.31135368 1.0423827 1.8692055 2.3645225 2.1161294 1.8486695 1.3639493 0.51040411 -0.746088 -2.0682061][-3.7655337 -2.8997703 -2.1505241 -1.3459592 -0.581491 0.10678387 1.1503415 2.1861982 2.8001523 2.7183061 2.456501 1.8089471 1.0670285 -0.11796236 -1.3010237][-4.9399142 -4.0777884 -3.3411515 -2.5737724 -1.8758132 -0.91066575 0.17295456 1.2599182 1.8872585 2.1652431 2.2574782 1.8861537 1.3142195 0.49617052 -0.520905][-5.9257765 -5.3976574 -4.7401943 -4.0324583 -3.3161216 -2.4479744 -1.45608 -0.43725204 0.30029154 0.74723959 0.9563942 0.77377272 0.57281017 0.15868616 -0.51406264][-6.0549479 -5.5391808 -5.3052826 -4.9049048 -4.4658427 -3.7340109 -3.0071497 -2.1488001 -1.4031644 -0.95728731 -0.73434353 -0.769336 -0.73813581 -0.89398122 -1.3712969][-5.94982 -5.5739236 -5.3990817 -5.2937489 -5.1811218 -4.8152618 -4.3559756 -3.7851744 -3.1827817 -2.6710882 -2.3241506 -2.3349109 -2.2884002 -2.1531198 -2.3741961][-5.1663203 -4.9202085 -4.9312177 -4.9876776 -5.0626945 -5.0654826 -5.029851 -4.6970496 -4.0864286 -3.6305795 -3.324863 -3.1873746 -3.080101 -2.9815021 -3.0324557][-4.4007573 -4.0897503 -4.0938597 -4.2784204 -4.5144572 -4.5850143 -4.6578856 -4.4482822 -4.1461911 -3.7965786 -3.5719409 -3.5220287 -3.5023453 -3.3851163 -3.3696349][-3.761539 -3.4758546 -3.4659863 -3.5882895 -3.7209196 -3.8334212 -3.9187808 -3.8023438 -3.6296957 -3.4660254 -3.3700905 -3.3505578 -3.416393 -3.3813977 -3.2242241]]...]
INFO - root - 2017-12-16 08:55:21.522390: step 66010, loss = 0.21, batch loss = 0.15 (27.2 examples/sec; 0.295 sec/batch; 21h:48m:08s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 08:55:24.364173: step 66020, loss = 0.43, batch loss = 0.37 (28.7 examples/sec; 0.279 sec/batch; 20h:38m:43s remains)
INFO - root - 2017-12-16 08:55:27.199114: step 66030, loss = 0.24, batch loss = 0.18 (26.2 examples/sec; 0.306 sec/batch; 22h:37m:52s remains)
INFO - root - 2017-12-16 08:55:30.007157: step 66040, loss = 0.27, batch loss = 0.21 (29.2 examples/sec; 0.274 sec/batch; 20h:17m:43s remains)
INFO - root - 2017-12-16 08:55:32.863964: step 66050, loss = 0.42, batch loss = 0.36 (28.5 examples/sec; 0.281 sec/batch; 20h:48m:38s remains)
INFO - root - 2017-12-16 08:55:35.761490: step 66060, loss = 0.19, batch loss = 0.14 (27.4 examples/sec; 0.292 sec/batch; 21h:36m:11s remains)
INFO - root - 2017-12-16 08:55:38.596513: step 66070, loss = 0.29, batch loss = 0.24 (26.8 examples/sec; 0.299 sec/batch; 22h:06m:40s remains)
INFO - root - 2017-12-16 08:55:41.434448: step 66080, loss = 0.30, batch loss = 0.24 (28.8 examples/sec; 0.278 sec/batch; 20h:34m:43s remains)
INFO - root - 2017-12-16 08:55:44.290883: step 66090, loss = 0.31, batch loss = 0.25 (28.0 examples/sec; 0.286 sec/batch; 21h:07m:41s remains)
INFO - root - 2017-12-16 08:55:47.093275: step 66100, loss = 0.23, batch loss = 0.17 (29.2 examples/sec; 0.274 sec/batch; 20h:16m:38s remains)
2017-12-16 08:55:47.621527: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6157305 -3.6718054 -3.8301394 -4.1769495 -4.4012489 -4.4143772 -4.315856 -4.1034093 -3.9991384 -3.70048 -3.358958 -3.037991 -2.9832206 -3.207387 -3.1855907][-2.9325914 -2.969449 -3.0950856 -3.391273 -3.5817263 -3.6081395 -3.5328126 -3.4177237 -3.3486745 -3.0922112 -2.7964239 -2.4004443 -2.0765924 -2.1143508 -2.1104476][-2.3891442 -2.4485638 -2.5933259 -2.7566657 -2.8466949 -2.8943973 -2.8876066 -2.8167157 -2.7690735 -2.5975509 -2.3366344 -1.8707345 -1.4101834 -1.1427367 -0.97124243][-1.7944064 -1.8748956 -2.0095723 -2.0896873 -2.121213 -2.1700397 -2.1921375 -2.2596462 -2.3163469 -2.2155788 -2.0816813 -1.4817829 -0.84503365 -0.442976 -0.11541843][-1.1155212 -1.2722464 -1.4396966 -1.5292742 -1.4290555 -1.397728 -1.3522227 -1.4418681 -1.5937777 -1.5742884 -1.5818763 -1.1575758 -0.55436397 0.021811008 0.3354764][-0.61731005 -0.84664488 -0.93817687 -0.90260363 -0.65395021 -0.46589422 -0.29986143 -0.33053637 -0.52510977 -0.77031255 -0.94821739 -0.6476953 -0.15320444 0.27634621 0.428061][-0.49129176 -0.63922453 -0.63331366 -0.46688485 -0.11991501 0.1017518 0.36149931 0.36937952 0.15254068 -0.18597174 -0.58947039 -0.40491629 -0.049856663 0.28948832 0.49007034][-0.30809546 -0.44489121 -0.50461841 -0.33804512 -0.030470371 0.097830296 0.25727034 0.20648813 -0.056849003 -0.38184309 -0.76481509 -0.65218449 -0.39002419 -0.032278538 0.22552729][-0.24850512 -0.32876873 -0.43921113 -0.38115311 -0.32897949 -0.32665348 -0.24117708 -0.26366806 -0.49846268 -0.81565642 -1.1556327 -1.1128857 -0.99970603 -0.77233696 -0.55500364][-0.41382408 -0.48242831 -0.81432152 -0.84554362 -0.90135169 -1.0789361 -1.1049311 -1.115802 -1.3292258 -1.4400251 -1.535346 -1.602993 -1.6998832 -1.5080228 -1.2480998][-0.73576689 -0.68849826 -1.0559072 -1.2938519 -1.5935366 -1.8291111 -1.8854537 -1.972995 -2.1909232 -2.3288913 -2.4461377 -2.4962757 -2.5611644 -2.4034767 -2.1023107][-1.1886911 -1.1008904 -1.4268372 -1.7456877 -2.1575077 -2.3882496 -2.4840617 -2.588285 -2.8754957 -2.9960785 -3.1979647 -3.3910213 -3.5421519 -3.34347 -2.887413][-1.091804 -1.0034299 -1.3145461 -1.623744 -2.0394893 -2.3483078 -2.5120425 -2.5484648 -2.8368764 -3.0661178 -3.4108829 -3.7277257 -3.9142885 -3.7936773 -3.359025][-1.2160289 -1.0072305 -1.1202133 -1.3209698 -1.6977029 -1.8922527 -2.0853784 -2.3063664 -2.5863967 -2.7183475 -2.9868779 -3.4324346 -3.7575703 -3.5881126 -3.0386944][-1.4456189 -1.1726935 -1.1276145 -1.082437 -1.1972499 -1.217593 -1.3234544 -1.6060374 -1.9592669 -2.2188523 -2.6315181 -3.0530543 -3.3620586 -3.2519145 -2.7043891]]...]
INFO - root - 2017-12-16 08:55:50.422877: step 66110, loss = 0.33, batch loss = 0.27 (28.8 examples/sec; 0.278 sec/batch; 20h:32m:48s remains)
INFO - root - 2017-12-16 08:55:53.344287: step 66120, loss = 0.23, batch loss = 0.17 (28.1 examples/sec; 0.285 sec/batch; 21h:04m:06s remains)
INFO - root - 2017-12-16 08:55:56.166631: step 66130, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 21h:03m:44s remains)
INFO - root - 2017-12-16 08:55:59.059330: step 66140, loss = 0.26, batch loss = 0.21 (28.6 examples/sec; 0.280 sec/batch; 20h:42m:25s remains)
INFO - root - 2017-12-16 08:56:01.881760: step 66150, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.282 sec/batch; 20h:52m:22s remains)
INFO - root - 2017-12-16 08:56:04.716764: step 66160, loss = 0.24, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 21h:07m:36s remains)
INFO - root - 2017-12-16 08:56:07.580981: step 66170, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 21h:03m:07s remains)
INFO - root - 2017-12-16 08:56:10.447485: step 66180, loss = 0.48, batch loss = 0.42 (26.6 examples/sec; 0.301 sec/batch; 22h:15m:29s remains)
INFO - root - 2017-12-16 08:56:13.322888: step 66190, loss = 0.48, batch loss = 0.42 (28.1 examples/sec; 0.285 sec/batch; 21h:03m:55s remains)
INFO - root - 2017-12-16 08:56:16.222168: step 66200, loss = 0.25, batch loss = 0.19 (28.6 examples/sec; 0.279 sec/batch; 20h:40m:06s remains)
2017-12-16 08:56:16.741528: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7920361 -3.9149075 -4.0069141 -4.0045061 -4.116838 -4.0705547 -4.1487465 -4.3474894 -4.8831344 -5.1368694 -5.1987848 -5.4216509 -5.4420228 -5.1876993 -4.5671053][-4.6436853 -4.768404 -4.6618204 -4.3915329 -4.4187737 -4.3986182 -4.5521765 -4.8845553 -5.4230156 -5.8763933 -6.2989521 -6.2294245 -5.7988272 -5.52192 -4.8313932][-5.8612986 -5.655139 -5.2505732 -4.7393365 -4.6176229 -4.422853 -4.4312482 -4.8011012 -5.3577771 -5.903791 -6.5341911 -6.8016396 -6.6767864 -6.189414 -5.2955346][-6.9079275 -6.5375824 -5.7839127 -4.8222094 -3.9781175 -3.3908639 -3.2776349 -3.4593565 -4.0062943 -4.8489294 -5.7465072 -6.2146206 -6.3251486 -6.1910567 -5.6200824][-7.4265471 -6.8723593 -5.6425905 -4.135572 -2.7249429 -1.545383 -0.89032841 -0.82117248 -1.4157891 -2.5730495 -3.8829668 -5.0254655 -5.6740351 -5.8266077 -5.5028963][-6.878561 -6.0991755 -4.7916822 -2.9072268 -0.8202107 0.92919445 1.9766684 2.1937342 1.4997501 0.0078663826 -1.8386376 -3.6149952 -4.8517218 -5.6156244 -5.7096834][-5.7364388 -5.090445 -3.7396591 -1.5749905 0.78838158 2.8369503 4.37671 4.7180815 3.9741468 2.3066297 0.0760231 -2.1301186 -3.8422532 -5.06813 -5.4722619][-4.78272 -4.4063087 -3.1072421 -1.1756907 1.1360807 3.3380175 4.8474379 5.2651033 4.7611427 3.1929474 0.9322772 -1.2978942 -3.0943003 -4.5427685 -5.1012087][-3.7965658 -4.0314307 -3.2335029 -1.6698406 0.081951141 2.1773729 3.7677097 4.2344084 3.8666954 2.5364008 0.56702137 -1.5173407 -3.1661892 -4.4379854 -4.9490485][-3.3835628 -3.8372838 -3.4904237 -2.5479414 -1.3578904 0.27432394 1.4510083 1.9898496 2.0081291 0.9494462 -0.55446935 -2.3891752 -3.81616 -4.7062192 -5.1310334][-3.5051496 -4.030858 -4.0316253 -3.5859828 -2.9649956 -1.9072635 -0.84923816 -0.21247578 -0.11238861 -0.86302495 -1.8120153 -3.0558455 -4.2377925 -4.8802381 -5.0370126][-3.7829733 -4.3546486 -4.5279503 -4.4310641 -4.1284156 -3.5145538 -2.8481874 -2.2747664 -1.9532347 -2.133285 -2.618341 -3.5154629 -4.2762709 -4.7316151 -4.7888594][-3.9173868 -4.4189129 -4.8420291 -4.8417664 -4.7056618 -4.2739334 -3.4921396 -2.8880162 -2.5359378 -2.5578356 -2.8584888 -3.4250214 -3.9147716 -4.4101195 -4.6523471][-4.0895615 -4.3464804 -4.7548523 -4.7601876 -4.6377721 -4.2534156 -3.3635244 -2.5614285 -1.9882917 -1.6887822 -1.9506083 -2.4909029 -3.0873809 -3.63983 -4.1209445][-4.4143262 -4.3449712 -4.6405849 -4.7320571 -4.6152349 -4.0700178 -3.1370964 -2.0365212 -0.96707153 -0.39952946 -0.7468226 -1.2513478 -1.9527185 -2.8397238 -3.4569268]]...]
INFO - root - 2017-12-16 08:56:19.600035: step 66210, loss = 0.20, batch loss = 0.14 (26.3 examples/sec; 0.304 sec/batch; 22h:27m:53s remains)
INFO - root - 2017-12-16 08:56:22.462248: step 66220, loss = 0.29, batch loss = 0.23 (29.4 examples/sec; 0.272 sec/batch; 20h:08m:19s remains)
INFO - root - 2017-12-16 08:56:25.315372: step 66230, loss = 0.24, batch loss = 0.18 (27.4 examples/sec; 0.292 sec/batch; 21h:34m:45s remains)
INFO - root - 2017-12-16 08:56:28.151571: step 66240, loss = 0.42, batch loss = 0.36 (28.4 examples/sec; 0.282 sec/batch; 20h:51m:07s remains)
INFO - root - 2017-12-16 08:56:31.026611: step 66250, loss = 0.22, batch loss = 0.17 (26.0 examples/sec; 0.307 sec/batch; 22h:43m:52s remains)
INFO - root - 2017-12-16 08:56:33.872779: step 66260, loss = 0.27, batch loss = 0.21 (27.8 examples/sec; 0.288 sec/batch; 21h:19m:04s remains)
INFO - root - 2017-12-16 08:56:36.738294: step 66270, loss = 0.23, batch loss = 0.17 (26.6 examples/sec; 0.301 sec/batch; 22h:15m:22s remains)
INFO - root - 2017-12-16 08:56:39.576154: step 66280, loss = 0.26, batch loss = 0.20 (28.2 examples/sec; 0.284 sec/batch; 20h:59m:55s remains)
INFO - root - 2017-12-16 08:56:42.371190: step 66290, loss = 0.21, batch loss = 0.16 (28.1 examples/sec; 0.284 sec/batch; 21h:01m:35s remains)
INFO - root - 2017-12-16 08:56:45.169941: step 66300, loss = 0.29, batch loss = 0.23 (28.9 examples/sec; 0.277 sec/batch; 20h:29m:57s remains)
2017-12-16 08:56:45.675448: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2174759 -3.1620617 -3.1483994 -2.9130332 -2.8334904 -2.7562037 -2.9054317 -3.1126847 -3.2570176 -3.4419198 -3.700546 -3.799011 -4.1397591 -4.5131221 -5.055366][-2.8020868 -2.8928866 -2.7899003 -2.6486483 -2.4609709 -2.185807 -2.0420341 -2.1371918 -2.536859 -2.9798462 -3.4637761 -3.8344288 -4.034245 -4.2851791 -4.7010212][-1.9680443 -2.0507853 -1.7931855 -1.5569966 -1.3021924 -1.082294 -0.97627091 -1.1527503 -1.7056999 -2.4541011 -3.0979943 -3.4941316 -3.8461528 -3.9790533 -4.3254461][-1.161844 -1.3476827 -1.3741353 -1.1269321 -0.87753439 -0.66235828 -0.39826632 -0.69065738 -1.2472887 -2.2887342 -2.9038959 -3.333622 -3.4838853 -3.4832726 -3.9208739][-0.65286541 -0.71611929 -0.66844869 -0.39112473 -0.024707317 0.32691574 0.62717056 0.49406624 -0.1226759 -1.0631814 -1.7712302 -2.3862252 -2.5606904 -2.65514 -3.1826205][-0.52481556 -0.37895346 -0.10651922 0.37234259 1.0856681 1.6410966 1.9673061 1.829566 1.3471589 0.2942524 -0.51420093 -0.97971845 -1.2471492 -1.7041199 -2.4660463][-0.21778154 -0.0087862015 0.16442776 0.62008095 1.2969551 1.8386436 2.2876139 2.4571705 2.3674369 1.4550986 0.73780155 0.18219566 -0.17717695 -0.776387 -1.6272161][-0.29737949 -0.14407444 -0.078748226 0.20400763 0.74385023 1.2295709 1.8530741 2.4026608 2.7525959 2.5099287 2.1207023 1.575068 1.0136299 0.17978764 -1.1871483][-1.2235641 -1.1599371 -1.2111123 -1.0609081 -0.59941816 -0.063010693 0.63718748 1.6366591 2.4345164 2.6081905 2.640687 2.361659 1.7285175 0.84351683 -0.78963351][-2.0634186 -2.2406633 -2.4737465 -2.56065 -2.2367258 -1.8208568 -1.2059166 -0.096166134 0.89505291 1.6607351 2.1695204 2.1886678 1.8395429 0.9592042 -0.55847621][-2.8234782 -2.9960136 -3.3221145 -3.76506 -3.9662919 -3.5901628 -2.8966389 -1.7688904 -0.71685648 0.40876722 1.3270254 1.3295054 1.0124068 0.14099693 -1.3454926][-4.0979 -4.3211842 -4.8416243 -5.42925 -5.7111096 -5.5421624 -5.2339406 -4.1486344 -2.6019568 -1.2304678 -0.4440465 -0.23924732 -0.35952568 -1.2341785 -2.864099][-5.4194117 -5.8435268 -6.5402288 -7.3838634 -8.1728334 -8.4017925 -8.185956 -7.0410252 -5.3723931 -3.7863836 -2.6134324 -2.2639837 -2.5587554 -3.3312163 -4.453125][-6.1322727 -6.7485642 -7.7205186 -9.0494461 -10.386755 -10.884907 -10.505251 -9.3674994 -7.9091744 -6.1745081 -4.9760771 -4.7792053 -5.2052021 -5.7892523 -6.657588][-6.3398495 -7.0401964 -8.2985363 -9.6995907 -10.907181 -11.591742 -11.376869 -10.431197 -9.1185913 -7.6834903 -6.7438707 -6.863555 -7.3219194 -7.7143812 -8.2995949]]...]
INFO - root - 2017-12-16 08:56:48.484180: step 66310, loss = 0.29, batch loss = 0.23 (28.8 examples/sec; 0.278 sec/batch; 20h:31m:30s remains)
INFO - root - 2017-12-16 08:56:51.253629: step 66320, loss = 0.33, batch loss = 0.28 (29.0 examples/sec; 0.276 sec/batch; 20h:25m:33s remains)
INFO - root - 2017-12-16 08:56:54.122499: step 66330, loss = 0.23, batch loss = 0.17 (26.9 examples/sec; 0.297 sec/batch; 21h:59m:38s remains)
INFO - root - 2017-12-16 08:56:57.009881: step 66340, loss = 0.28, batch loss = 0.22 (28.7 examples/sec; 0.279 sec/batch; 20h:37m:11s remains)
INFO - root - 2017-12-16 08:56:59.833166: step 66350, loss = 0.27, batch loss = 0.21 (28.9 examples/sec; 0.277 sec/batch; 20h:27m:52s remains)
INFO - root - 2017-12-16 08:57:02.678527: step 66360, loss = 0.34, batch loss = 0.28 (28.7 examples/sec; 0.279 sec/batch; 20h:35m:55s remains)
INFO - root - 2017-12-16 08:57:05.567926: step 66370, loss = 0.22, batch loss = 0.17 (29.0 examples/sec; 0.275 sec/batch; 20h:21m:57s remains)
INFO - root - 2017-12-16 08:57:08.434107: step 66380, loss = 0.39, batch loss = 0.34 (26.1 examples/sec; 0.306 sec/batch; 22h:38m:02s remains)
INFO - root - 2017-12-16 08:57:11.251888: step 66390, loss = 0.23, batch loss = 0.17 (26.8 examples/sec; 0.299 sec/batch; 22h:05m:21s remains)
INFO - root - 2017-12-16 08:57:14.117150: step 66400, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 20h:48m:47s remains)
2017-12-16 08:57:14.659886: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7391481 -4.5353727 -3.8366361 -4.071198 -4.5408182 -4.7849064 -4.890965 -4.9585104 -5.1392765 -5.2813387 -5.4374027 -5.2776203 -4.9771233 -4.6237946 -4.4063139][-5.2866764 -4.1708064 -3.4869893 -3.5334096 -3.8980095 -4.4401174 -4.7355523 -4.748023 -4.8074255 -4.9938149 -5.1860127 -5.1280541 -4.9747434 -4.6258783 -4.2527909][-4.5221348 -3.5586643 -2.989275 -2.7561915 -2.8695757 -3.1447315 -3.2331715 -3.5091569 -3.9062057 -4.6093144 -5.0502663 -5.0656071 -5.0772667 -4.8396416 -4.3781948][-3.5336273 -2.4609923 -1.8695595 -1.4447925 -1.2572453 -1.1201837 -1.0437362 -1.4034841 -2.14155 -3.2461934 -4.2927394 -4.868248 -5.0268841 -4.7810721 -4.289475][-3.2310948 -1.7564452 -0.57808232 0.16932297 0.63102579 0.78479052 0.83495 0.62750578 -0.26111984 -1.8031626 -3.2320337 -4.1488161 -4.5782509 -4.6278057 -4.125422][-3.8996243 -2.1618316 -0.57446361 1.1405802 2.6072102 3.1474204 3.0432463 2.3710709 1.2057009 -0.50625157 -2.0087874 -3.1271939 -3.8897033 -3.8545718 -3.4223046][-5.321732 -3.6205921 -1.6340091 0.84664059 3.0238876 4.4572821 5.0448866 4.2684765 2.759923 0.55190372 -1.3396342 -2.4250429 -3.1565156 -3.3305492 -3.0626097][-6.7325253 -5.4097795 -3.2807403 -0.56779552 1.9102159 3.6894026 4.7465687 4.4151821 3.2185321 1.0762963 -0.67080784 -1.8427033 -2.5686388 -2.9110732 -2.7687228][-8.084116 -7.3717117 -5.6088181 -2.9728205 -0.49244165 1.5517383 2.9592333 3.0831542 2.1169844 0.38544989 -1.0988441 -2.1483364 -2.7455106 -2.9853439 -2.8452408][-9.0939827 -8.790246 -7.6281519 -5.4185629 -2.9035923 -0.98186779 0.61906385 1.1949992 0.70059538 -0.74130535 -2.0485857 -2.8743556 -3.2901683 -3.3604081 -3.1590557][-10.094688 -9.4629173 -8.4598122 -7.012929 -5.3977847 -3.6210835 -2.0079405 -1.3176982 -1.3631642 -2.2942946 -3.3079939 -3.89 -4.0628576 -3.9525332 -3.663631][-9.8518867 -9.0490112 -8.2406855 -7.301053 -6.3028407 -5.3855495 -4.4063067 -3.7133374 -3.4549093 -3.7356534 -4.3407936 -4.8233943 -4.9127417 -4.5438781 -4.0269432][-9.2532358 -7.9170408 -6.8301573 -6.1673784 -6.0021729 -5.899796 -5.6079874 -5.2830057 -5.0450296 -5.0816288 -5.1661882 -5.1518774 -4.9804139 -4.5781407 -3.948493][-8.6154127 -7.0321589 -5.8082819 -5.0079193 -4.9021945 -5.2214909 -5.4005651 -5.5119181 -5.4767437 -5.35838 -5.2346973 -5.1194782 -4.8516459 -4.3344874 -3.7348135][-8.0103931 -6.1911407 -4.9082565 -4.2418122 -4.226367 -4.6428809 -5.1307788 -5.3400378 -5.4145727 -5.4028769 -5.366416 -5.3999386 -5.1613827 -4.7135816 -4.0661135]]...]
INFO - root - 2017-12-16 08:57:17.481960: step 66410, loss = 0.32, batch loss = 0.26 (28.5 examples/sec; 0.281 sec/batch; 20h:45m:13s remains)
INFO - root - 2017-12-16 08:57:20.333821: step 66420, loss = 0.32, batch loss = 0.26 (29.3 examples/sec; 0.273 sec/batch; 20h:11m:26s remains)
INFO - root - 2017-12-16 08:57:23.163878: step 66430, loss = 0.25, batch loss = 0.20 (28.5 examples/sec; 0.280 sec/batch; 20h:43m:15s remains)
INFO - root - 2017-12-16 08:57:26.010714: step 66440, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 20h:58m:00s remains)
INFO - root - 2017-12-16 08:57:28.875214: step 66450, loss = 0.23, batch loss = 0.17 (27.9 examples/sec; 0.287 sec/batch; 21h:11m:50s remains)
INFO - root - 2017-12-16 08:57:31.713453: step 66460, loss = 0.29, batch loss = 0.23 (28.6 examples/sec; 0.279 sec/batch; 20h:38m:12s remains)
INFO - root - 2017-12-16 08:57:34.492099: step 66470, loss = 0.21, batch loss = 0.16 (28.7 examples/sec; 0.278 sec/batch; 20h:34m:35s remains)
INFO - root - 2017-12-16 08:57:37.340799: step 66480, loss = 0.28, batch loss = 0.23 (28.1 examples/sec; 0.285 sec/batch; 21h:04m:17s remains)
INFO - root - 2017-12-16 08:57:40.250909: step 66490, loss = 0.28, batch loss = 0.22 (26.2 examples/sec; 0.305 sec/batch; 22h:33m:42s remains)
INFO - root - 2017-12-16 08:57:43.185053: step 66500, loss = 0.21, batch loss = 0.15 (26.4 examples/sec; 0.303 sec/batch; 22h:21m:23s remains)
2017-12-16 08:57:43.709207: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0751591 -3.8801255 -3.7308207 -3.5957108 -3.5347283 -3.5369754 -3.6472831 -3.8137007 -3.9507957 -3.9834752 -3.9166894 -3.8151529 -3.7577577 -3.5052397 -3.2807367][-4.3088818 -4.1286831 -3.9276562 -3.6452363 -3.4651144 -3.3633132 -3.4337707 -3.5803463 -3.68391 -3.6573133 -3.5347371 -3.4312682 -3.4145873 -3.182457 -2.967824][-4.4558434 -4.2701163 -4.1033149 -3.7315559 -3.3878388 -3.1623898 -3.2044361 -3.2398596 -3.2013512 -3.0928314 -2.9464095 -2.95287 -3.0543022 -2.9321733 -2.7751346][-4.3641496 -4.1766028 -3.9123154 -3.5126295 -3.261097 -2.981492 -2.9581423 -3.0066504 -2.8423429 -2.6005569 -2.3897235 -2.4833865 -2.7471032 -2.6940532 -2.415719][-3.9173789 -3.6733832 -3.3526754 -2.9557605 -2.7796535 -2.6280637 -2.6170142 -2.6170726 -2.3689134 -2.0758929 -1.8786604 -2.0881243 -2.4606297 -2.6055319 -2.377491][-3.4212313 -3.0224352 -2.5772843 -2.335638 -2.2618327 -2.18006 -2.1655469 -2.1318538 -1.8566854 -1.540674 -1.416646 -1.7574737 -2.2488484 -2.4762657 -2.3862116][-2.6965184 -2.239764 -1.7824214 -1.4996276 -1.5067189 -1.4937487 -1.5082736 -1.5586038 -1.3536856 -1.1075866 -1.118026 -1.5475953 -2.2050185 -2.5355618 -2.4630785][-2.2471311 -1.5997491 -1.0391312 -0.81057191 -0.95144057 -1.1540728 -1.3139422 -1.3008513 -0.95303249 -0.64011145 -0.71280479 -1.1840186 -1.8858173 -2.45182 -2.580205][-1.8261125 -1.2720158 -0.88783908 -0.83848381 -1.1142399 -1.308424 -1.3990076 -1.2627194 -0.82462597 -0.26880407 -0.17525005 -0.84838629 -1.7710993 -2.3458836 -2.4560194][-1.6375754 -1.138612 -0.87765551 -1.002317 -1.365051 -1.475831 -1.5497477 -1.2695158 -0.76296639 -0.31933355 -0.34245968 -0.8538456 -1.6197927 -2.3638742 -2.6573148][-1.8298545 -1.400481 -1.3007495 -1.5764735 -2.1281493 -2.4311907 -2.6181855 -2.2362058 -1.6640751 -1.1450918 -1.1116092 -1.5158367 -2.1605637 -2.7363224 -2.6990204][-2.3560634 -2.053901 -2.067013 -2.4749365 -3.2164073 -3.8263025 -4.2499161 -3.8606768 -3.3653126 -2.8376138 -2.6006351 -2.774802 -3.1157346 -3.3129621 -2.9571404][-2.9865272 -2.7687154 -2.9243302 -3.4982185 -4.3079691 -5.0003142 -5.5178742 -5.3151588 -4.974493 -4.5174265 -4.2506142 -4.3072081 -4.4285493 -4.3340688 -3.6739285][-3.7017996 -3.4924324 -3.5541945 -4.0592175 -4.9493027 -5.6586237 -6.2009697 -6.3412681 -6.4088449 -6.178658 -5.9190598 -5.8132763 -5.7160554 -5.4115729 -4.5930767][-4.6093841 -4.4629445 -4.6068282 -4.950048 -5.6504254 -6.4968481 -7.1840668 -7.43857 -7.5766468 -7.5909214 -7.4993758 -7.220583 -6.8605046 -6.3214355 -5.3369274]]...]
INFO - root - 2017-12-16 08:57:46.542578: step 66510, loss = 0.26, batch loss = 0.21 (27.8 examples/sec; 0.288 sec/batch; 21h:16m:14s remains)
INFO - root - 2017-12-16 08:57:49.345487: step 66520, loss = 0.32, batch loss = 0.26 (30.1 examples/sec; 0.266 sec/batch; 19h:39m:56s remains)
INFO - root - 2017-12-16 08:57:52.215485: step 66530, loss = 0.26, batch loss = 0.20 (28.2 examples/sec; 0.284 sec/batch; 20h:57m:00s remains)
INFO - root - 2017-12-16 08:57:55.061759: step 66540, loss = 0.32, batch loss = 0.26 (28.9 examples/sec; 0.277 sec/batch; 20h:28m:23s remains)
INFO - root - 2017-12-16 08:57:57.875700: step 66550, loss = 0.25, batch loss = 0.19 (27.9 examples/sec; 0.286 sec/batch; 21h:08m:42s remains)
INFO - root - 2017-12-16 08:58:00.741054: step 66560, loss = 0.22, batch loss = 0.16 (27.7 examples/sec; 0.288 sec/batch; 21h:18m:05s remains)
INFO - root - 2017-12-16 08:58:03.602469: step 66570, loss = 0.24, batch loss = 0.18 (27.8 examples/sec; 0.288 sec/batch; 21h:15m:47s remains)
INFO - root - 2017-12-16 08:58:06.432095: step 66580, loss = 0.24, batch loss = 0.18 (27.8 examples/sec; 0.287 sec/batch; 21h:13m:44s remains)
INFO - root - 2017-12-16 08:58:09.323268: step 66590, loss = 0.31, batch loss = 0.25 (27.7 examples/sec; 0.289 sec/batch; 21h:21m:20s remains)
INFO - root - 2017-12-16 08:58:12.152992: step 66600, loss = 0.36, batch loss = 0.30 (27.4 examples/sec; 0.292 sec/batch; 21h:35m:20s remains)
2017-12-16 08:58:12.704730: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6397514 -3.5056877 -3.6153681 -3.8725128 -4.0511317 -4.1736641 -4.1647463 -3.961477 -3.7419369 -3.4625525 -3.2296355 -2.9843755 -2.6713181 -2.4892521 -2.2454925][-3.7593889 -3.6005523 -3.5367398 -3.7851546 -4.0750637 -4.4308691 -4.5180087 -4.3931255 -4.2508922 -4.0236244 -3.7581749 -3.5479217 -3.2237473 -2.7973952 -2.3634565][-3.6723344 -3.401967 -3.2946718 -3.3026237 -3.3460317 -3.7563975 -4.1515193 -4.4071441 -4.4884281 -4.4547987 -4.502666 -4.3006883 -3.8903029 -3.478754 -2.8257532][-3.7850966 -3.241605 -2.7864909 -2.4870868 -2.2514894 -2.3763905 -2.7457874 -3.2434118 -3.7968211 -4.1922426 -4.531301 -4.7592492 -4.6758022 -4.1891303 -3.5149133][-3.5766494 -2.7128968 -1.8100584 -1.152842 -0.57808089 -0.40864038 -0.69305921 -1.1329217 -1.9255869 -2.940469 -3.8787286 -4.402863 -4.6127982 -4.5175505 -3.9898334][-3.0207524 -1.8943474 -0.73812723 0.45336533 1.4594173 1.9069724 1.7982578 1.2440839 0.27412128 -0.99168324 -2.385308 -3.5955927 -4.29747 -4.3653855 -4.0700455][-2.4963651 -1.336235 -0.041257858 1.37253 2.7235918 3.6284142 3.9665365 3.516593 2.5257497 0.93469715 -0.71102405 -2.2374864 -3.4675672 -3.9916203 -3.8680613][-2.2780237 -1.242033 -0.061928749 1.4490509 3.0338073 4.1685438 4.7049179 4.4562836 3.5038514 1.8410544 0.058713913 -1.6769655 -2.9248879 -3.7501416 -3.9972947][-2.4300683 -1.6656539 -0.71873641 0.48258877 1.8527517 3.1340294 3.8221607 3.5877056 2.7104344 1.2050581 -0.509887 -2.3022811 -3.6353159 -4.196857 -4.310204][-2.756773 -2.3055596 -1.8726442 -1.0812297 -0.064256191 0.68107986 1.3304296 1.3523359 0.7430625 -0.60683894 -2.0932102 -3.4926586 -4.6095176 -5.02159 -4.8732352][-3.3715243 -2.9791143 -2.8644166 -2.6476943 -2.195137 -1.6929209 -1.364928 -1.6491473 -2.0949745 -2.8788953 -3.9808517 -5.0016122 -5.7063451 -5.8650656 -5.6727276][-4.0702515 -3.6042316 -3.6249933 -3.5685561 -3.6992724 -4.046772 -4.2167416 -4.3127561 -4.608077 -5.0206356 -5.4281559 -6.1120949 -6.5550284 -6.4708958 -6.1707487][-4.0144529 -3.6399019 -3.7127957 -4.1022139 -4.7555385 -5.2785978 -5.8110418 -6.2843227 -6.2906613 -6.2532964 -6.2353563 -6.2955208 -6.3599043 -6.3738317 -6.271656][-4.0551972 -3.5170879 -3.5710523 -3.7624936 -4.4873295 -5.393158 -6.1153789 -6.5646095 -6.638175 -6.3225532 -5.9013863 -5.7816644 -5.7546306 -5.6483555 -5.6835127][-4.1225743 -3.0653884 -2.6667335 -2.9947193 -3.685993 -4.3670835 -5.1600833 -5.6722808 -5.7658896 -5.6497631 -5.2192597 -4.7644615 -4.5348969 -4.687572 -4.9422407]]...]
INFO - root - 2017-12-16 08:58:15.539841: step 66610, loss = 0.36, batch loss = 0.30 (28.4 examples/sec; 0.282 sec/batch; 20h:48m:22s remains)
INFO - root - 2017-12-16 08:58:18.388149: step 66620, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.282 sec/batch; 20h:48m:43s remains)
INFO - root - 2017-12-16 08:58:21.210568: step 66630, loss = 0.39, batch loss = 0.33 (28.6 examples/sec; 0.280 sec/batch; 20h:39m:17s remains)
INFO - root - 2017-12-16 08:58:24.030902: step 66640, loss = 0.36, batch loss = 0.30 (27.9 examples/sec; 0.287 sec/batch; 21h:10m:51s remains)
INFO - root - 2017-12-16 08:58:26.877791: step 66650, loss = 0.28, batch loss = 0.22 (29.0 examples/sec; 0.276 sec/batch; 20h:23m:15s remains)
INFO - root - 2017-12-16 08:58:29.718950: step 66660, loss = 0.31, batch loss = 0.25 (29.1 examples/sec; 0.275 sec/batch; 20h:17m:33s remains)
INFO - root - 2017-12-16 08:58:32.544185: step 66670, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.281 sec/batch; 20h:43m:22s remains)
INFO - root - 2017-12-16 08:58:35.364974: step 66680, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 21h:06m:40s remains)
INFO - root - 2017-12-16 08:58:38.221736: step 66690, loss = 0.22, batch loss = 0.16 (28.2 examples/sec; 0.284 sec/batch; 20h:56m:31s remains)
INFO - root - 2017-12-16 08:58:41.038547: step 66700, loss = 0.24, batch loss = 0.18 (29.0 examples/sec; 0.276 sec/batch; 20h:20m:39s remains)
2017-12-16 08:58:41.576675: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7948561 -3.4421511 -3.1957746 -3.0617251 -3.1124558 -3.1763616 -3.350832 -3.3524032 -3.2343793 -2.8735676 -2.3355134 -1.8839428 -1.6541348 -1.7924643 -1.9912069][-3.8898473 -3.7303281 -3.6015425 -3.3744061 -3.2958694 -3.4362 -3.7006888 -3.4976258 -3.1257188 -2.9016323 -2.605752 -2.3265328 -2.2361257 -2.4087963 -2.6006002][-3.6767642 -3.4400434 -3.4299839 -3.423912 -3.4583757 -3.535356 -3.6698239 -3.510113 -3.2231393 -2.9677243 -2.6982274 -2.6518593 -2.6973133 -2.8230696 -3.0749416][-3.2062168 -3.249773 -3.4323978 -3.5733383 -3.6072106 -3.5911076 -3.5126977 -3.2617862 -3.0333772 -2.9714785 -2.9249339 -2.9615166 -2.9817772 -3.1605635 -3.4037127][-2.3404841 -2.4531143 -2.7835786 -3.0606933 -3.1986272 -3.1530755 -2.9428389 -2.6218996 -2.2992661 -2.231472 -2.2840667 -2.4318366 -2.6779747 -2.8510451 -2.9661047][-1.5396316 -1.8778079 -2.3491702 -2.5812378 -2.4632323 -2.1982653 -1.8288016 -1.5126541 -1.3218138 -1.2868359 -1.4001243 -1.5527098 -1.7653923 -1.995733 -2.2296035][-0.89970732 -1.3393493 -1.7418435 -2.0103531 -1.8566849 -1.379976 -0.76783991 -0.38161945 -0.21434546 -0.2456007 -0.44571781 -0.72372532 -0.90964317 -1.0383637 -1.0707259][-0.26843071 -0.60274291 -0.97160316 -1.086045 -0.841033 -0.34702969 0.093352318 0.42513466 0.55170679 0.35882807 0.0080690384 -0.39639616 -0.62298012 -0.72190213 -0.71236753][-0.15545082 -0.28337765 -0.40792847 -0.30316687 0.068378925 0.57063866 0.94694376 1.0249953 0.9946394 0.76971149 0.2444768 -0.39219332 -0.77106571 -0.90745044 -0.91050863][-0.33231544 -0.21672678 -0.097419262 0.16541004 0.60800552 0.91540813 1.0996594 1.1194596 0.7510252 0.12030649 -0.60430288 -1.1956811 -1.5972707 -1.7272723 -1.610682][-0.97076082 -0.58157444 -0.19925833 0.17799425 0.37637377 0.69539642 0.73072004 0.38817263 -0.25797892 -0.79938245 -1.6109762 -2.2845526 -2.5130138 -2.5333254 -2.3700876][-2.1698661 -1.5528257 -0.97113895 -0.32540941 0.11877632 0.031152248 -0.481987 -0.74168825 -1.174423 -1.995183 -2.807827 -3.2222142 -3.2369943 -3.1565032 -2.8887808][-3.1026044 -2.2768207 -1.6402926 -1.2128801 -0.96414185 -0.89642978 -1.1730494 -1.8432429 -2.5921328 -2.9444735 -3.3918033 -3.6686921 -3.5452414 -3.1925709 -2.8997207][-4.231287 -3.2591705 -2.4359174 -2.0120583 -1.8879421 -2.1936579 -2.7641931 -3.0916376 -3.3481984 -3.6180682 -3.8695745 -3.700069 -3.2626531 -2.7875047 -2.4610395][-4.8406992 -4.0267878 -3.4212921 -3.0540171 -2.9641044 -3.2144055 -3.6008635 -4.0114665 -4.3058062 -4.2820897 -4.0244565 -3.7379839 -3.1847315 -2.5748217 -2.1093054]]...]
INFO - root - 2017-12-16 08:58:44.437927: step 66710, loss = 0.30, batch loss = 0.25 (26.0 examples/sec; 0.308 sec/batch; 22h:43m:35s remains)
INFO - root - 2017-12-16 08:58:47.298735: step 66720, loss = 0.25, batch loss = 0.19 (26.9 examples/sec; 0.297 sec/batch; 21h:56m:52s remains)
INFO - root - 2017-12-16 08:58:50.134244: step 66730, loss = 0.26, batch loss = 0.21 (27.3 examples/sec; 0.293 sec/batch; 21h:37m:02s remains)
INFO - root - 2017-12-16 08:58:53.023256: step 66740, loss = 0.22, batch loss = 0.16 (27.0 examples/sec; 0.297 sec/batch; 21h:53m:43s remains)
INFO - root - 2017-12-16 08:58:55.863750: step 66750, loss = 0.30, batch loss = 0.25 (28.5 examples/sec; 0.280 sec/batch; 20h:41m:08s remains)
INFO - root - 2017-12-16 08:58:58.745301: step 66760, loss = 0.26, batch loss = 0.20 (27.6 examples/sec; 0.290 sec/batch; 21h:26m:00s remains)
INFO - root - 2017-12-16 08:59:01.618508: step 66770, loss = 0.31, batch loss = 0.25 (28.6 examples/sec; 0.280 sec/batch; 20h:39m:30s remains)
INFO - root - 2017-12-16 08:59:04.475916: step 66780, loss = 0.22, batch loss = 0.16 (27.3 examples/sec; 0.294 sec/batch; 21h:40m:05s remains)
INFO - root - 2017-12-16 08:59:07.329285: step 66790, loss = 0.24, batch loss = 0.18 (27.9 examples/sec; 0.287 sec/batch; 21h:09m:52s remains)
INFO - root - 2017-12-16 08:59:10.283633: step 66800, loss = 0.32, batch loss = 0.26 (26.4 examples/sec; 0.303 sec/batch; 22h:19m:41s remains)
2017-12-16 08:59:10.804676: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5035038 -4.3490987 -4.2488756 -4.6857576 -5.3798795 -6.142108 -6.8006072 -6.9027672 -6.8683228 -6.3703394 -5.8329144 -5.4773417 -4.8286142 -4.0906072 -3.3928466][-5.1300178 -5.5801768 -6.0250254 -6.2564993 -7.0801611 -7.3001881 -7.2842913 -7.1350803 -6.6255932 -6.2229733 -5.9966555 -5.5210786 -4.744308 -4.1431465 -3.5749793][-5.7416277 -6.29831 -6.7675123 -6.9805956 -6.9705677 -6.6217775 -6.5435553 -6.1497631 -5.7388616 -5.4288311 -5.0809174 -5.0952787 -5.0271988 -4.5915732 -3.983566][-6.159173 -7.0678844 -7.3944917 -7.0753746 -6.4058943 -5.2900066 -4.367415 -3.7021973 -3.4273107 -3.9401071 -4.6371975 -4.8795533 -4.8861361 -4.929884 -4.6553121][-6.0837765 -7.0441103 -7.1088715 -6.2463064 -4.6374803 -2.7619534 -1.4173794 -0.48598576 -0.60833859 -1.7863059 -3.0501022 -4.4013782 -5.307282 -5.2919459 -4.7334909][-4.6665587 -5.4498467 -5.550045 -4.4081035 -1.9103744 0.70945215 2.6534376 3.4847956 2.6819878 0.7816 -1.7301121 -3.8398628 -5.0375781 -5.5079727 -5.1473389][-3.3724351 -4.064846 -3.7253547 -2.0992298 0.32888174 3.0456572 5.2325544 5.9640541 4.8574409 2.2981672 -0.66679168 -3.3018141 -5.0251193 -5.42937 -4.8826714][-2.7511125 -3.5077457 -3.3265166 -1.54564 1.225265 4.0899897 6.0493412 6.2049179 4.9119787 2.2419162 -1.0066867 -3.6266742 -5.0186963 -5.298768 -4.7879872][-2.6824322 -3.2384658 -2.7482133 -1.5502312 0.18529081 2.6869392 4.656414 4.8935461 3.4593496 0.67863512 -2.2761216 -4.5414219 -5.7528925 -5.7373147 -4.9939508][-2.7129972 -3.4407187 -3.1365161 -2.1083236 -0.52006841 0.96629047 1.7728124 2.0126133 0.90212059 -1.5073798 -4.025044 -5.9119482 -6.5600996 -6.2818122 -5.46765][-3.4658058 -3.8236732 -3.6118104 -3.3947926 -2.9161046 -1.6629984 -0.492069 -0.63525009 -2.005528 -3.6114974 -5.284256 -6.7258186 -7.1746235 -6.6113558 -5.6643028][-4.509553 -4.7441897 -4.8211131 -4.374712 -3.7594247 -3.4815989 -3.2388601 -3.2389655 -3.8332841 -5.0652752 -6.3879118 -7.0777555 -7.04585 -6.4385781 -5.5963321][-5.4051223 -5.5104795 -5.5915422 -5.4940705 -5.3616753 -4.7433071 -4.2579837 -4.643682 -5.3149943 -5.9315557 -6.6935706 -7.0877352 -6.9143009 -6.2484131 -5.4964786][-6.1443996 -6.090312 -6.0510812 -5.9448967 -5.7851343 -5.361064 -5.2110672 -5.102725 -5.3103356 -5.9180527 -6.3675952 -6.372015 -6.1157169 -5.5307879 -4.9585576][-6.7671804 -6.6634517 -6.517374 -6.2486496 -5.9275737 -5.5143676 -5.25514 -4.996655 -5.0301886 -5.3298507 -5.5711412 -5.4709921 -5.1051993 -4.5403161 -4.0524411]]...]
INFO - root - 2017-12-16 08:59:13.634843: step 66810, loss = 0.26, batch loss = 0.20 (26.2 examples/sec; 0.305 sec/batch; 22h:32m:19s remains)
INFO - root - 2017-12-16 08:59:16.468385: step 66820, loss = 0.23, batch loss = 0.17 (29.5 examples/sec; 0.271 sec/batch; 19h:58m:49s remains)
INFO - root - 2017-12-16 08:59:19.309838: step 66830, loss = 0.33, batch loss = 0.28 (27.8 examples/sec; 0.288 sec/batch; 21h:15m:05s remains)
INFO - root - 2017-12-16 08:59:22.140458: step 66840, loss = 0.23, batch loss = 0.17 (27.8 examples/sec; 0.288 sec/batch; 21h:15m:29s remains)
INFO - root - 2017-12-16 08:59:25.030381: step 66850, loss = 0.25, batch loss = 0.19 (27.0 examples/sec; 0.296 sec/batch; 21h:49m:47s remains)
INFO - root - 2017-12-16 08:59:27.919836: step 66860, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.286 sec/batch; 21h:06m:03s remains)
INFO - root - 2017-12-16 08:59:30.810102: step 66870, loss = 0.66, batch loss = 0.60 (27.0 examples/sec; 0.296 sec/batch; 21h:51m:20s remains)
INFO - root - 2017-12-16 08:59:33.702467: step 66880, loss = 0.26, batch loss = 0.21 (27.4 examples/sec; 0.292 sec/batch; 21h:33m:27s remains)
INFO - root - 2017-12-16 08:59:36.542279: step 66890, loss = 0.36, batch loss = 0.30 (28.6 examples/sec; 0.280 sec/batch; 20h:37m:23s remains)
INFO - root - 2017-12-16 08:59:39.372448: step 66900, loss = 0.25, batch loss = 0.19 (29.6 examples/sec; 0.270 sec/batch; 19h:54m:48s remains)
2017-12-16 08:59:39.923695: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2351165 -2.3058538 -2.4050467 -2.5107822 -2.6022549 -2.6618104 -2.7037797 -2.7170572 -2.6814604 -2.6240864 -2.5487795 -2.5529895 -2.5552382 -2.5361867 -2.5200109][-2.3344004 -2.51367 -2.716228 -2.9311762 -3.1189961 -3.2472818 -3.3259165 -3.3122642 -3.231931 -3.189424 -3.1205206 -2.9880233 -2.8805199 -2.9044251 -2.9042425][-2.5671678 -2.9178457 -3.2925537 -3.5205851 -3.6700892 -3.8362937 -3.9205604 -3.9698691 -3.9044144 -3.7987747 -3.6681409 -3.6068668 -3.5826828 -3.5112188 -3.3902278][-2.870337 -3.1916242 -3.6098788 -3.95221 -4.0830307 -4.0605865 -3.9671569 -3.8588967 -3.7527623 -3.7275558 -3.670258 -3.6898584 -3.817122 -4.0367179 -4.1004095][-3.0593338 -3.5137103 -3.8827546 -4.0841651 -3.9709649 -3.6883662 -3.4600155 -3.1131754 -2.9209123 -2.8278086 -2.7874713 -3.0013993 -3.320879 -3.8583846 -4.2730093][-3.0328894 -3.494617 -3.8491411 -3.8541985 -3.4848585 -2.9472458 -2.3991175 -1.9503675 -1.6554587 -1.4403474 -1.4083123 -1.7517598 -2.4126289 -3.187346 -3.7761645][-2.8778315 -3.2091308 -3.3786464 -3.223115 -2.6175213 -1.7093475 -0.82717061 -0.19231033 0.11183167 0.16715813 -0.0052952766 -0.42721033 -1.161072 -2.3058543 -3.3406291][-2.3173661 -2.6136527 -2.6318605 -2.3519065 -1.5835252 -0.50735712 0.48360729 1.3552685 1.7648935 1.7848077 1.540329 0.82331133 -0.254138 -1.6113422 -2.9306769][-1.831284 -1.8868935 -1.9711461 -1.6315758 -0.86413908 0.14818668 1.0856018 1.9252772 2.4375172 2.5633397 2.3310575 1.7225566 0.53857994 -1.2396448 -2.8804066][-1.6515882 -1.6137962 -1.6609352 -1.4657757 -0.87563038 -0.10745335 0.54078293 1.4020438 2.0571346 2.4266706 2.4749608 1.8737001 0.73997831 -1.0371146 -2.5985794][-1.50839 -1.3660021 -1.4068713 -1.4072144 -1.1379788 -0.54861665 0.0051608086 0.6431241 1.2805886 1.7278242 1.9168367 1.4351749 0.35421991 -1.2581728 -2.7616649][-1.7089109 -1.2360659 -1.2856331 -1.2725296 -1.2125964 -1.0957811 -0.89486551 -0.39998007 0.35943556 0.8800292 0.92595577 0.57457685 -0.36356115 -1.7174144 -3.1396198][-1.7881753 -1.39252 -1.4157591 -1.6325159 -1.7544625 -1.7242756 -1.4935334 -1.0661345 -0.56101632 0.023560524 0.35751629 0.13263178 -0.53960109 -1.5687203 -2.5628846][-1.9028141 -1.3568907 -1.2514524 -1.5477834 -1.822392 -1.8555641 -1.7111411 -1.290421 -0.76456714 -0.28295231 0.073185444 0.056507587 -0.29513884 -0.99810815 -1.7655268][-1.8714001 -1.2427902 -1.0812616 -1.1923144 -1.4827366 -1.5712736 -1.3831959 -1.2129421 -0.88579869 -0.46759081 -0.1162796 0.023314476 -0.25563526 -0.72281909 -1.4093523]]...]
INFO - root - 2017-12-16 08:59:42.772076: step 66910, loss = 0.28, batch loss = 0.22 (27.6 examples/sec; 0.290 sec/batch; 21h:24m:02s remains)
INFO - root - 2017-12-16 08:59:45.647084: step 66920, loss = 0.30, batch loss = 0.24 (27.0 examples/sec; 0.296 sec/batch; 21h:51m:52s remains)
INFO - root - 2017-12-16 08:59:48.504041: step 66930, loss = 0.20, batch loss = 0.14 (28.1 examples/sec; 0.285 sec/batch; 21h:00m:55s remains)
INFO - root - 2017-12-16 08:59:51.323251: step 66940, loss = 0.22, batch loss = 0.16 (29.1 examples/sec; 0.275 sec/batch; 20h:17m:18s remains)
INFO - root - 2017-12-16 08:59:54.181797: step 66950, loss = 0.32, batch loss = 0.26 (29.5 examples/sec; 0.271 sec/batch; 19h:59m:15s remains)
INFO - root - 2017-12-16 08:59:57.043056: step 66960, loss = 0.29, batch loss = 0.23 (27.2 examples/sec; 0.294 sec/batch; 21h:41m:41s remains)
INFO - root - 2017-12-16 08:59:59.890907: step 66970, loss = 0.24, batch loss = 0.18 (28.5 examples/sec; 0.281 sec/batch; 20h:44m:16s remains)
INFO - root - 2017-12-16 09:00:02.754397: step 66980, loss = 0.22, batch loss = 0.17 (27.3 examples/sec; 0.293 sec/batch; 21h:38m:24s remains)
INFO - root - 2017-12-16 09:00:05.609155: step 66990, loss = 0.21, batch loss = 0.15 (28.8 examples/sec; 0.278 sec/batch; 20h:30m:12s remains)
INFO - root - 2017-12-16 09:00:08.434382: step 67000, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.277 sec/batch; 20h:27m:07s remains)
2017-12-16 09:00:08.959185: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5369263 -4.4372215 -4.4020524 -4.4115009 -4.5571971 -4.6881661 -4.7958746 -5.1211619 -5.2628574 -4.91424 -4.5902748 -4.4774356 -4.4096184 -4.18235 -3.966459][-3.6997025 -3.62084 -3.7937751 -4.0151305 -4.1509953 -4.362215 -4.340786 -4.3482537 -4.3624439 -4.3223672 -4.3186269 -4.0654263 -4.0441995 -4.0081186 -3.930253][-3.4812062 -3.6177158 -3.7149768 -3.9976068 -4.0700455 -3.79622 -3.4002795 -3.2227144 -3.2136388 -3.4294674 -3.6555252 -3.6823423 -3.6853054 -3.6957846 -3.7949183][-2.901866 -3.4162323 -3.9711316 -4.174346 -3.8416209 -3.1714644 -2.4314048 -2.148865 -2.3695927 -2.7289722 -3.2049694 -3.5459983 -3.5834694 -3.2258887 -3.0009785][-3.0378916 -3.6195364 -4.0112195 -4.1119347 -3.4761772 -2.3623137 -1.3482218 -0.89411473 -1.105474 -1.968739 -3.0885704 -3.5642738 -3.6498239 -3.5623157 -3.4173179][-3.091692 -4.0885396 -4.4429345 -4.1857185 -3.0875812 -1.4684641 0.0539279 0.56375837 0.057835579 -1.0367377 -2.3823128 -3.5972285 -4.3391471 -4.144506 -3.8941102][-3.2556939 -4.426506 -4.9189706 -4.3931451 -2.8116112 -0.98005629 0.71487093 1.4597492 1.0289912 -0.34851074 -2.0306096 -3.3337045 -4.1448307 -4.4697876 -4.4677906][-3.1132545 -4.3928103 -4.9863577 -4.4637356 -2.7700579 -0.69263649 1.1053348 1.7929406 1.263299 -0.32185602 -2.2021203 -3.4486933 -4.0909247 -4.4027939 -4.4567885][-2.6925216 -3.9137096 -4.6542578 -4.4013114 -2.8955231 -0.75182724 0.990119 1.766932 1.3954573 -0.015771866 -2.1068757 -3.8396273 -4.6625652 -4.5966835 -4.3924465][-2.0193949 -3.563478 -4.5227304 -4.5042067 -3.3080571 -1.3581576 0.50909662 1.3108954 0.7935605 -0.50597978 -2.366375 -3.9841185 -5.0657315 -5.2465734 -4.8954825][-1.3388972 -2.9067183 -4.1851368 -4.333858 -3.5268011 -1.8666799 -0.20493555 0.63480854 0.22491407 -1.1245685 -2.795938 -4.1405544 -4.9960551 -5.236412 -5.0593243][-1.4762096 -2.7634325 -3.7161996 -3.9926653 -3.4168582 -2.1176741 -1.1583555 -0.49313545 -0.67543006 -1.83564 -3.2612724 -4.2553205 -4.6867852 -4.5888996 -4.3735628][-1.7133532 -2.8686481 -3.910723 -4.1190667 -3.7840092 -2.7521324 -1.8465724 -1.7105949 -2.2537253 -2.8708787 -3.5817721 -4.1874838 -4.3648844 -4.1061587 -3.7791977][-1.9238591 -2.8002162 -3.4975 -3.9280243 -3.8873758 -3.3293221 -3.0531628 -2.7877855 -2.964232 -3.5393393 -3.9798388 -3.8737545 -3.4511969 -3.0956755 -2.8712823][-1.9686232 -2.6405444 -3.4498515 -3.7980895 -3.8228996 -3.6483753 -3.6192453 -3.7357574 -4.0332294 -4.1132588 -4.0355949 -3.5752904 -2.9675312 -2.3507285 -1.9484398]]...]
INFO - root - 2017-12-16 09:00:11.788939: step 67010, loss = 0.26, batch loss = 0.21 (29.4 examples/sec; 0.272 sec/batch; 20h:02m:36s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 09:00:14.617338: step 67020, loss = 0.39, batch loss = 0.33 (27.9 examples/sec; 0.286 sec/batch; 21h:06m:38s remains)
INFO - root - 2017-12-16 09:00:17.429326: step 67030, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 20h:28m:57s remains)
INFO - root - 2017-12-16 09:00:20.222125: step 67040, loss = 0.28, batch loss = 0.22 (29.7 examples/sec; 0.270 sec/batch; 19h:52m:44s remains)
INFO - root - 2017-12-16 09:00:23.085077: step 67050, loss = 0.26, batch loss = 0.20 (26.2 examples/sec; 0.305 sec/batch; 22h:29m:02s remains)
INFO - root - 2017-12-16 09:00:25.905337: step 67060, loss = 0.46, batch loss = 0.40 (28.9 examples/sec; 0.276 sec/batch; 20h:23m:13s remains)
INFO - root - 2017-12-16 09:00:28.700763: step 67070, loss = 0.27, batch loss = 0.21 (29.5 examples/sec; 0.271 sec/batch; 19h:58m:20s remains)
INFO - root - 2017-12-16 09:00:31.522628: step 67080, loss = 0.30, batch loss = 0.24 (28.2 examples/sec; 0.284 sec/batch; 20h:55m:16s remains)
INFO - root - 2017-12-16 09:00:34.333680: step 67090, loss = 0.22, batch loss = 0.16 (28.3 examples/sec; 0.283 sec/batch; 20h:50m:21s remains)
INFO - root - 2017-12-16 09:00:37.165296: step 67100, loss = 0.27, batch loss = 0.21 (28.1 examples/sec; 0.285 sec/batch; 21h:00m:52s remains)
2017-12-16 09:00:37.700828: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6687379 -3.5361953 -3.664 -4.1375279 -4.5587082 -4.6457133 -4.601016 -4.4766693 -4.2589741 -4.1114984 -4.0597296 -4.0897708 -4.2598686 -4.3596683 -4.290678][-2.4397783 -2.3270586 -2.5282323 -2.9741879 -3.2425838 -3.5166221 -3.7207081 -3.7227809 -3.8205009 -3.907618 -3.9878085 -3.9997818 -4.0377545 -4.031302 -4.052002][-0.9321084 -0.69743061 -0.88215709 -1.3849702 -1.9088123 -2.3138869 -2.6038194 -2.8618059 -3.1200805 -3.528708 -3.870605 -3.9590755 -3.8709188 -3.5604641 -3.2597437][0.68699837 1.0169687 0.5495224 -0.044403553 -0.52795243 -1.0894899 -1.5190215 -1.9112384 -2.4757302 -3.1189961 -3.5879397 -3.7357001 -3.5915556 -3.1875441 -2.9277778][1.5111279 1.7436256 1.265739 0.68017817 0.25101757 -0.10702658 -0.28322077 -0.72345543 -1.3574181 -2.3648217 -3.2787397 -3.6048367 -3.3838391 -2.9056468 -2.4203269][1.4252586 1.7432652 1.5412426 1.2913632 1.3092146 1.2117262 1.0954552 0.6956315 -0.13043737 -1.3491845 -2.2994027 -2.9479904 -3.1142073 -2.8183403 -2.3689148][0.88243723 1.3135967 1.3058844 1.3842402 1.6508689 1.9914403 2.2741318 2.0517168 1.2803679 -0.080555439 -1.4701633 -2.4847906 -2.8473167 -2.7963529 -2.3884556][-0.17301607 0.2347827 0.22280931 0.63495922 1.2721639 2.0593386 2.8022161 2.9263263 2.2518868 0.77504778 -0.93738246 -2.229717 -2.9810722 -3.0831182 -2.7795906][-1.5235236 -1.0660162 -0.79189205 -0.34735537 0.45136833 1.4863787 2.4499912 2.6897521 2.0428467 0.63776016 -1.0821147 -2.6048813 -3.7180498 -3.9805012 -3.6587944][-2.1656272 -1.922786 -1.6980996 -1.1413529 -0.35688734 0.46266174 1.2975397 1.5555587 1.0939059 -0.098217964 -1.8006413 -3.3703847 -4.4165082 -4.765831 -4.5065627][-2.4756722 -2.3772998 -2.3938036 -2.1868162 -1.7817297 -1.0582216 -0.351501 -0.15674448 -0.56046462 -1.3829706 -2.5799959 -3.8084259 -4.7193646 -5.0444555 -4.8005552][-2.693727 -2.8239508 -3.0205765 -3.0772 -2.8327827 -2.4385605 -1.9432843 -1.7114754 -1.899653 -2.4758015 -3.4393778 -4.27612 -4.9088249 -5.1674914 -4.8790932][-2.3062525 -2.6371512 -3.1181569 -3.4666152 -3.4186633 -3.2154179 -2.8755317 -2.7478466 -2.9368041 -3.1929927 -3.6000066 -4.2633033 -4.7062035 -4.7264738 -4.3890233][-2.1386943 -2.6910605 -3.2989817 -3.7775643 -3.9145126 -3.9476166 -3.82916 -3.5026388 -3.3258517 -3.33812 -3.514394 -3.6396794 -3.7104371 -3.8407817 -3.639997][-2.4122858 -2.8730016 -3.4281545 -3.891171 -4.057229 -4.0370989 -3.977896 -3.7839813 -3.5867739 -3.3847897 -3.3466775 -3.2607913 -3.1667676 -3.0611348 -2.7858217]]...]
INFO - root - 2017-12-16 09:00:40.531533: step 67110, loss = 0.30, batch loss = 0.24 (28.2 examples/sec; 0.283 sec/batch; 20h:52m:44s remains)
INFO - root - 2017-12-16 09:00:43.324674: step 67120, loss = 0.35, batch loss = 0.29 (27.1 examples/sec; 0.295 sec/batch; 21h:43m:34s remains)
INFO - root - 2017-12-16 09:00:46.194963: step 67130, loss = 0.36, batch loss = 0.30 (26.5 examples/sec; 0.302 sec/batch; 22h:14m:30s remains)
INFO - root - 2017-12-16 09:00:48.948431: step 67140, loss = 0.21, batch loss = 0.15 (29.7 examples/sec; 0.270 sec/batch; 19h:52m:24s remains)
INFO - root - 2017-12-16 09:00:51.756579: step 67150, loss = 0.21, batch loss = 0.15 (28.7 examples/sec; 0.278 sec/batch; 20h:31m:02s remains)
INFO - root - 2017-12-16 09:00:54.579631: step 67160, loss = 0.35, batch loss = 0.29 (27.3 examples/sec; 0.293 sec/batch; 21h:35m:19s remains)
INFO - root - 2017-12-16 09:00:57.421532: step 67170, loss = 0.31, batch loss = 0.25 (27.3 examples/sec; 0.293 sec/batch; 21h:37m:49s remains)
INFO - root - 2017-12-16 09:01:00.302309: step 67180, loss = 0.45, batch loss = 0.40 (28.0 examples/sec; 0.286 sec/batch; 21h:05m:26s remains)
INFO - root - 2017-12-16 09:01:03.103088: step 67190, loss = 0.30, batch loss = 0.24 (27.9 examples/sec; 0.287 sec/batch; 21h:08m:32s remains)
INFO - root - 2017-12-16 09:01:05.946445: step 67200, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.282 sec/batch; 20h:47m:23s remains)
2017-12-16 09:01:06.496815: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7151515 -3.5995896 -3.6856103 -3.5640087 -3.4438767 -3.292624 -2.9676886 -2.5860262 -2.4671879 -2.431035 -2.5107751 -2.3298686 -2.3800023 -2.4608033 -2.7106504][-3.6099625 -3.3152971 -3.337208 -3.3931065 -3.1789365 -2.9395306 -2.5506697 -2.068604 -1.7908072 -1.6336014 -1.6012888 -1.395427 -1.4427991 -1.6078458 -1.9768097][-2.6605172 -2.700048 -2.9838881 -2.9831071 -2.7613192 -2.4482667 -1.9273849 -1.2968421 -0.89720917 -0.8925755 -1.120888 -1.0389385 -1.109858 -1.3497653 -1.6740589][-1.3903112 -1.5085306 -2.0153847 -2.3768237 -2.1504998 -1.758363 -1.2471585 -0.521014 -0.12213182 -0.21486807 -0.583843 -1.131603 -1.5961211 -1.8363762 -2.0498548][-0.40933132 -0.59570861 -1.0789075 -1.6223338 -1.60901 -1.0726542 -0.34885597 0.34849882 0.64490128 0.19336987 -0.74946117 -1.4432025 -1.9017606 -2.2935305 -2.4437447][-0.76247287 -0.61870623 -0.80028868 -1.1532502 -0.75909567 -0.0036196709 0.96559525 1.6516075 1.6921034 1.2064161 0.11289549 -1.3537567 -2.353302 -2.7281983 -2.7203734][-1.2522233 -0.99239016 -0.90117931 -0.9183073 -0.19818783 1.0124774 2.2250562 2.9022841 2.7953234 1.7632527 0.010565758 -1.6560597 -2.6829562 -3.1372182 -3.0266118][-1.7088578 -1.565346 -1.5167937 -1.1245897 -0.086967945 1.3246717 2.7801776 3.4800262 3.0369639 1.6500983 -0.37324619 -2.4805112 -3.7453525 -4.0920973 -3.9124234][-3.3933139 -2.9086258 -2.4714708 -1.9896057 -0.80834341 1.0018721 2.559031 3.2098875 2.7382216 1.246542 -0.87184978 -2.9337735 -4.2514153 -4.6412525 -4.2868385][-4.4458447 -4.2104878 -3.9424052 -3.1428 -1.8641801 -0.3587575 1.0076795 1.7198496 1.3647165 0.001727581 -1.9627533 -3.7801445 -4.907093 -5.197053 -4.8365216][-5.0341907 -5.10778 -4.9820423 -4.5480456 -3.5744412 -2.1758177 -0.94909978 -0.47125888 -0.6855104 -1.5940115 -3.1132336 -4.5473433 -5.4526157 -5.7085366 -5.3384428][-5.2474356 -5.4594216 -5.5901136 -5.3678703 -4.7702417 -3.8835747 -2.9244239 -2.4485226 -2.4403381 -2.8738084 -3.7655575 -4.8537016 -5.6806788 -5.907805 -5.6719017][-5.1802979 -5.4910989 -5.6534462 -5.5780911 -5.4449806 -4.8675909 -4.1640553 -3.7948854 -3.6020942 -3.5420578 -3.7600596 -4.3290191 -4.8813639 -5.0889783 -5.0340257][-5.1518621 -5.3637495 -5.4412289 -5.4004989 -5.3044095 -5.1330996 -4.806664 -4.4105811 -4.0431585 -3.730041 -3.4286082 -3.3928683 -3.6089606 -3.7845459 -3.9378419][-4.8718576 -4.9382935 -4.9209356 -4.8534932 -4.8173714 -4.7829452 -4.6283245 -4.4037023 -4.058939 -3.5302033 -2.9598632 -2.6723197 -2.6853676 -2.8669493 -3.1568444]]...]
INFO - root - 2017-12-16 09:01:09.319393: step 67210, loss = 0.24, batch loss = 0.18 (27.3 examples/sec; 0.293 sec/batch; 21h:37m:33s remains)
INFO - root - 2017-12-16 09:01:12.126233: step 67220, loss = 0.20, batch loss = 0.14 (29.3 examples/sec; 0.273 sec/batch; 20h:07m:20s remains)
INFO - root - 2017-12-16 09:01:14.962952: step 67230, loss = 0.37, batch loss = 0.31 (28.9 examples/sec; 0.276 sec/batch; 20h:21m:56s remains)
INFO - root - 2017-12-16 09:01:17.772427: step 67240, loss = 0.26, batch loss = 0.20 (29.0 examples/sec; 0.276 sec/batch; 20h:19m:54s remains)
INFO - root - 2017-12-16 09:01:20.624926: step 67250, loss = 0.47, batch loss = 0.42 (26.8 examples/sec; 0.298 sec/batch; 21h:58m:46s remains)
INFO - root - 2017-12-16 09:01:23.503724: step 67260, loss = 0.22, batch loss = 0.16 (26.2 examples/sec; 0.306 sec/batch; 22h:30m:55s remains)
INFO - root - 2017-12-16 09:01:26.317426: step 67270, loss = 0.35, batch loss = 0.29 (28.3 examples/sec; 0.282 sec/batch; 20h:47m:42s remains)
INFO - root - 2017-12-16 09:01:29.117661: step 67280, loss = 0.24, batch loss = 0.18 (27.7 examples/sec; 0.289 sec/batch; 21h:16m:52s remains)
INFO - root - 2017-12-16 09:01:31.968897: step 67290, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 20h:48m:54s remains)
INFO - root - 2017-12-16 09:01:34.765378: step 67300, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.281 sec/batch; 20h:43m:44s remains)
2017-12-16 09:01:35.273076: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5775833 -2.7839961 -3.117393 -3.6270394 -4.1647129 -4.4775052 -4.6474714 -4.63899 -4.6637645 -4.6880236 -4.541409 -4.4288211 -4.3012309 -4.2242842 -4.2690215][-2.3369689 -2.6083786 -2.992847 -3.2677281 -3.5231102 -3.7048879 -3.9361234 -3.9965279 -3.9217863 -4.0927639 -4.2678995 -4.3242464 -4.3579636 -4.2247314 -4.23162][-2.2480738 -2.4182518 -2.6679397 -2.9910288 -3.163569 -3.0116096 -2.9973648 -3.2079229 -3.4317622 -3.6558344 -3.9353056 -4.1571546 -4.3229413 -4.2604327 -4.199317][-2.1519642 -2.4302731 -2.702405 -2.8566575 -2.9234147 -2.7741666 -2.5397141 -2.4617028 -2.67899 -3.0625885 -3.3604474 -3.6960125 -3.947947 -3.92265 -3.8633831][-2.1592894 -2.47678 -2.741724 -2.8477707 -2.7414911 -2.3939409 -1.9256923 -1.7002158 -1.8525898 -2.1889889 -2.5780883 -2.9597225 -3.3750629 -3.5149548 -3.4386382][-2.3377118 -2.60295 -2.7892745 -2.8346415 -2.5106115 -1.9888561 -1.4358783 -1.0815206 -1.1457791 -1.5110428 -1.9750741 -2.3491516 -2.6219783 -2.7765903 -2.7405305][-2.5696607 -2.8133116 -2.9101205 -2.747771 -2.2094719 -1.4637976 -0.65074635 -0.18400145 -0.19861269 -0.4910357 -0.91636395 -1.3725965 -1.7246664 -1.8514402 -1.7760687][-2.6843009 -2.8179164 -2.9325519 -2.6783018 -1.9707415 -1.0472288 -0.12923288 0.54142427 0.81103134 0.57365322 0.017586231 -0.40740395 -0.68475938 -0.83279109 -0.79803205][-2.7624378 -2.8400869 -2.7361968 -2.4588466 -2.0079997 -1.1158864 -0.21215057 0.4222827 0.84848928 0.87065458 0.58852243 0.10297728 -0.14591408 -0.16873074 -0.024721622][-2.4955609 -2.5934815 -3.0163026 -2.9175375 -2.5588188 -2.0216179 -1.2834411 -0.60023594 -0.13003349 0.020384789 -0.0029845238 -0.2867837 -0.37843132 -0.2066288 0.15214157][-2.7530077 -2.9113193 -3.1473513 -3.463017 -3.779851 -3.4625371 -3.0257812 -2.5625181 -2.0131135 -1.7862782 -1.5704825 -1.5284114 -1.3326242 -1.0462835 -0.44596267][-3.073864 -3.6794438 -4.5571613 -4.9986172 -5.286942 -5.364006 -5.1639662 -4.692523 -4.4065862 -4.0394959 -3.5933425 -3.3371892 -2.8412914 -2.4179423 -1.8846567][-3.4956937 -4.3409281 -5.4443107 -6.4254913 -7.1293931 -7.168819 -6.9480915 -6.5131903 -6.1262913 -5.853529 -5.5476379 -5.200707 -4.83652 -4.4492774 -3.803478][-4.0912833 -5.0181365 -6.1119757 -7.13505 -7.7832689 -8.03424 -7.8915339 -7.5112228 -7.1573668 -6.7869387 -6.636497 -6.464366 -5.9857969 -5.7061191 -5.5422888][-4.3081975 -5.2457938 -6.255228 -7.09043 -7.5530005 -7.7980671 -7.79058 -7.6586776 -7.4076643 -7.1839495 -7.0192547 -6.95272 -6.8681831 -6.6584387 -6.36847]]...]
INFO - root - 2017-12-16 09:01:38.120123: step 67310, loss = 0.38, batch loss = 0.32 (27.7 examples/sec; 0.289 sec/batch; 21h:18m:40s remains)
INFO - root - 2017-12-16 09:01:40.994149: step 67320, loss = 0.47, batch loss = 0.41 (27.1 examples/sec; 0.295 sec/batch; 21h:44m:31s remains)
INFO - root - 2017-12-16 09:01:43.794762: step 67330, loss = 0.22, batch loss = 0.16 (29.2 examples/sec; 0.274 sec/batch; 20h:09m:49s remains)
INFO - root - 2017-12-16 09:01:46.617422: step 67340, loss = 0.27, batch loss = 0.21 (28.6 examples/sec; 0.280 sec/batch; 20h:36m:30s remains)
INFO - root - 2017-12-16 09:01:49.441854: step 67350, loss = 0.34, batch loss = 0.28 (28.2 examples/sec; 0.284 sec/batch; 20h:54m:51s remains)
INFO - root - 2017-12-16 09:01:52.307416: step 67360, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.281 sec/batch; 20h:42m:11s remains)
INFO - root - 2017-12-16 09:01:55.173342: step 67370, loss = 0.23, batch loss = 0.17 (27.7 examples/sec; 0.289 sec/batch; 21h:16m:38s remains)
INFO - root - 2017-12-16 09:01:58.007093: step 67380, loss = 0.28, batch loss = 0.22 (28.0 examples/sec; 0.285 sec/batch; 21h:00m:22s remains)
INFO - root - 2017-12-16 09:02:00.848221: step 67390, loss = 0.30, batch loss = 0.24 (28.2 examples/sec; 0.284 sec/batch; 20h:54m:17s remains)
INFO - root - 2017-12-16 09:02:03.649882: step 67400, loss = 0.29, batch loss = 0.23 (28.0 examples/sec; 0.286 sec/batch; 21h:01m:36s remains)
2017-12-16 09:02:04.207728: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1554313 -2.7058103 -3.2202253 -3.8820143 -4.6774879 -5.6938882 -6.59624 -7.3986964 -7.708993 -7.2626657 -6.334074 -5.4510078 -4.6105623 -3.7402596 -3.3659804][-2.1277246 -2.4630861 -2.8590462 -3.2891269 -3.8143969 -4.6293888 -5.5649929 -6.1318226 -6.1251054 -5.8055615 -5.3592458 -4.4531078 -3.5357485 -2.8649278 -2.7170815][-2.3679538 -2.6402712 -2.9151907 -3.0103693 -2.9028673 -3.0077207 -3.4077067 -3.9750724 -4.3483238 -4.3076534 -4.0315652 -3.5605266 -2.7179816 -1.706146 -1.2397664][-3.0452523 -3.0301819 -2.9225743 -2.7065034 -2.0835516 -1.4618294 -1.5156732 -2.0463417 -2.5152583 -2.6491075 -2.5086403 -2.2274432 -1.8690352 -1.2369845 -0.88696408][-3.7377384 -3.5732961 -3.232049 -2.3834403 -1.3022957 -0.17754602 0.39470768 0.19525099 -0.49953723 -1.2060187 -1.7092512 -1.6658318 -1.5103128 -1.2872243 -1.458431][-4.770885 -4.4369683 -3.6641622 -2.3082733 -0.47166777 1.2078819 2.0909562 2.12597 1.2834864 0.15169811 -0.87601519 -1.3812957 -1.6855993 -1.7227421 -1.8710248][-4.7204657 -4.6147485 -3.7503622 -2.1359251 -0.21733236 1.6560788 2.782392 2.7527566 1.7283545 0.30565119 -0.89909434 -1.5950482 -1.9684854 -2.0660589 -2.1692083][-4.3361845 -4.1269264 -3.2113171 -1.6435723 0.035384178 1.6232553 2.3694763 2.1596708 1.1487451 -0.33896637 -1.5051403 -2.062762 -2.2708459 -2.3967149 -2.5268974][-3.5748389 -3.5715449 -2.8720193 -1.5074024 -0.025366306 1.0599966 1.5785246 1.3110075 0.37148333 -0.87339568 -1.8698738 -2.4011531 -2.473351 -2.381779 -2.3245549][-2.8309171 -3.126646 -2.7622323 -1.8161941 -0.7569406 0.14297247 0.53210068 0.21936369 -0.54242635 -1.4455307 -2.1153796 -2.4980502 -2.5177116 -2.3698473 -2.2051003][-2.6529899 -3.0882587 -3.0117607 -2.4861131 -1.8041213 -1.1487696 -0.7907815 -1.0342219 -1.5292394 -2.1089144 -2.5485854 -2.652328 -2.5575514 -2.4113894 -2.2761698][-3.0076647 -3.4516373 -3.6267653 -3.3851254 -2.877944 -2.3501408 -2.0042725 -1.9862533 -2.1028035 -2.4834805 -2.8396261 -2.8995183 -2.7525954 -2.6233935 -2.4289312][-3.3556323 -3.9077427 -4.2142596 -4.005527 -3.6120834 -3.1425104 -2.7724683 -2.6732707 -2.7620287 -2.8200645 -2.7419405 -2.7842689 -2.6932201 -2.5523686 -2.5489931][-3.6933699 -3.9967239 -4.0708761 -3.9595714 -3.8504434 -3.5309041 -3.1964116 -3.1268325 -3.0757093 -2.9768133 -2.7510219 -2.5404692 -2.3466613 -2.3679421 -2.5369897][-3.6811273 -3.8023715 -3.8365726 -3.6189704 -3.487184 -3.4194267 -3.3062446 -3.247757 -3.2794619 -3.0741849 -2.6668267 -2.247257 -1.9689221 -1.9444098 -2.1514661]]...]
INFO - root - 2017-12-16 09:02:07.042302: step 67410, loss = 0.27, batch loss = 0.22 (25.9 examples/sec; 0.309 sec/batch; 22h:43m:02s remains)
INFO - root - 2017-12-16 09:02:09.887261: step 67420, loss = 0.33, batch loss = 0.27 (27.7 examples/sec; 0.289 sec/batch; 21h:16m:15s remains)
INFO - root - 2017-12-16 09:02:12.694400: step 67430, loss = 0.50, batch loss = 0.44 (28.7 examples/sec; 0.278 sec/batch; 20h:29m:21s remains)
INFO - root - 2017-12-16 09:02:15.508684: step 67440, loss = 0.24, batch loss = 0.19 (27.8 examples/sec; 0.287 sec/batch; 21h:09m:33s remains)
INFO - root - 2017-12-16 09:02:18.390421: step 67450, loss = 0.22, batch loss = 0.16 (27.9 examples/sec; 0.287 sec/batch; 21h:08m:17s remains)
INFO - root - 2017-12-16 09:02:21.268343: step 67460, loss = 0.25, batch loss = 0.19 (26.0 examples/sec; 0.307 sec/batch; 22h:37m:07s remains)
INFO - root - 2017-12-16 09:02:24.096102: step 67470, loss = 0.22, batch loss = 0.16 (28.8 examples/sec; 0.277 sec/batch; 20h:25m:21s remains)
INFO - root - 2017-12-16 09:02:26.897151: step 67480, loss = 0.25, batch loss = 0.19 (29.1 examples/sec; 0.274 sec/batch; 20h:12m:24s remains)
INFO - root - 2017-12-16 09:02:29.763124: step 67490, loss = 0.25, batch loss = 0.19 (28.3 examples/sec; 0.283 sec/batch; 20h:50m:40s remains)
INFO - root - 2017-12-16 09:02:32.579847: step 67500, loss = 0.46, batch loss = 0.41 (27.5 examples/sec; 0.291 sec/batch; 21h:26m:11s remains)
2017-12-16 09:02:33.109065: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8611517 -2.6727605 -2.2670317 -1.9839377 -2.1944082 -2.164968 -2.1450589 -2.0834818 -1.9904015 -1.5401623 -0.87148309 -0.655148 -0.79778886 -1.1963685 -1.7279494][-1.8479168 -1.504822 -0.98273087 -0.5692668 -0.50531077 -0.47853446 -0.79790115 -1.0064392 -1.12557 -0.94466662 -0.54750323 -0.30890894 -0.25657225 -0.75449038 -1.3345532][-1.4344623 -0.71432972 0.098662376 0.48708868 0.534616 0.49646473 -0.084648609 -0.54459548 -0.90063787 -1.0557673 -0.8667419 -0.68219233 -0.4623003 -0.727257 -1.0754337][-1.0338287 -0.61171222 0.09868145 0.73161888 1.0372524 1.1332998 0.57158518 -0.18163538 -0.91787171 -1.355516 -1.6131766 -1.9069729 -1.8612523 -1.8331428 -1.6954553][-1.5149667 -1.0539274 -0.21012592 0.49715614 1.0842352 1.3179736 0.87278461 0.10904264 -0.770432 -1.5483484 -2.3484752 -2.9408228 -3.2392192 -3.5026999 -3.3302345][-2.1551325 -1.6314719 -0.71291184 0.29072237 1.2463541 1.7667451 1.6536841 0.95174932 -0.057427883 -1.257499 -2.4770703 -3.4351997 -3.9389424 -4.1582737 -4.0058727][-2.659972 -2.1017239 -1.1804276 0.093011379 1.2294345 2.0371518 2.2525778 1.8652182 1.0976801 -0.31606102 -1.9889095 -3.57635 -4.5491052 -4.8308229 -4.7195044][-2.8957791 -2.3530011 -1.5326927 -0.2075839 1.0046492 1.8962502 2.3179836 2.0075679 1.246408 -0.090451717 -1.8586149 -3.6442924 -4.9209766 -5.4190683 -5.356051][-3.2820082 -2.6141269 -1.7883449 -0.71514487 0.37707424 1.2245984 1.6183543 1.5220213 0.87100124 -0.40706968 -2.1540525 -4.0979104 -5.5836573 -6.2646995 -6.1129961][-3.431603 -2.9027114 -2.2122116 -1.3972869 -0.61750412 0.02046442 0.32234192 0.1540494 -0.51772189 -1.6279631 -3.1111054 -4.7765374 -6.1178446 -6.7393703 -6.5399957][-3.3365297 -2.8667853 -2.4770014 -2.1444514 -1.8569143 -1.5216844 -1.3429651 -1.6556756 -2.2389286 -3.1286931 -4.4016948 -5.8065777 -6.8815422 -7.23557 -6.9524031][-3.2671356 -2.8377719 -2.6859674 -2.7152209 -2.7836642 -2.8754911 -3.0804992 -3.4066696 -3.7421303 -4.3249846 -5.3107781 -6.5572557 -7.5324593 -7.8927569 -7.4961915][-2.9286549 -2.6649978 -2.6005442 -2.9474311 -3.4467149 -3.7376733 -4.0437417 -4.4410472 -4.8781934 -5.2196264 -5.7936869 -6.6913552 -7.4511571 -7.7875066 -7.3970909][-2.7265713 -2.497767 -2.4994912 -2.8917651 -3.3227348 -3.7085974 -4.0618448 -4.306088 -4.5814652 -4.9829345 -5.6140995 -6.2395935 -6.7220726 -6.7358713 -6.1383238][-2.4944117 -2.2798479 -2.3409407 -2.6184731 -2.8856604 -3.1991086 -3.4431076 -3.6194754 -3.7478504 -3.9595144 -4.46861 -5.0429754 -5.3936763 -5.3525505 -4.7036381]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-67500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-67500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 09:02:36.769496: step 67510, loss = 0.29, batch loss = 0.23 (27.6 examples/sec; 0.290 sec/batch; 21h:18m:41s remains)
INFO - root - 2017-12-16 09:02:39.596955: step 67520, loss = 0.31, batch loss = 0.25 (28.6 examples/sec; 0.279 sec/batch; 20h:33m:49s remains)
INFO - root - 2017-12-16 09:02:42.438595: step 67530, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 21h:02m:42s remains)
INFO - root - 2017-12-16 09:02:45.340424: step 67540, loss = 0.22, batch loss = 0.16 (26.8 examples/sec; 0.298 sec/batch; 21h:57m:08s remains)
INFO - root - 2017-12-16 09:02:48.228426: step 67550, loss = 0.31, batch loss = 0.25 (28.3 examples/sec; 0.282 sec/batch; 20h:47m:01s remains)
INFO - root - 2017-12-16 09:02:51.104826: step 67560, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.284 sec/batch; 20h:54m:42s remains)
INFO - root - 2017-12-16 09:02:53.901512: step 67570, loss = 0.30, batch loss = 0.24 (28.2 examples/sec; 0.284 sec/batch; 20h:51m:54s remains)
INFO - root - 2017-12-16 09:02:56.750391: step 67580, loss = 0.25, batch loss = 0.19 (27.6 examples/sec; 0.290 sec/batch; 21h:19m:47s remains)
INFO - root - 2017-12-16 09:02:59.642460: step 67590, loss = 0.27, batch loss = 0.21 (27.3 examples/sec; 0.293 sec/batch; 21h:31m:45s remains)
INFO - root - 2017-12-16 09:03:02.509175: step 67600, loss = 0.37, batch loss = 0.32 (29.1 examples/sec; 0.275 sec/batch; 20h:13m:03s remains)
2017-12-16 09:03:03.030104: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8560679 -3.3914819 -3.9546342 -4.4917164 -4.9596272 -5.4066181 -5.9496803 -6.7557988 -7.4212303 -7.5176883 -7.220253 -6.8020263 -5.9715643 -4.7948818 -3.8211155][-3.7866058 -4.4101553 -4.8784418 -5.312645 -5.6874862 -6.1571412 -6.6693029 -7.3114119 -7.8140745 -8.1768723 -8.2125263 -7.6707163 -6.9225712 -5.9503574 -4.873311][-4.6730928 -5.4322767 -5.9026794 -5.9578314 -5.8110189 -5.8012056 -5.9789939 -6.5999537 -7.1798782 -7.5320258 -7.7092476 -7.6213512 -7.3191185 -6.5907836 -5.6449866][-5.46124 -6.0466032 -6.2997017 -6.1257334 -5.5919757 -4.9643612 -4.5899954 -4.4902306 -4.6027441 -5.1739383 -6.0075097 -6.6281033 -6.9390631 -6.6111412 -5.770618][-5.7676125 -5.9552193 -5.7516313 -4.9868693 -3.8348227 -2.631424 -1.6074896 -1.0525017 -1.2188761 -2.1400671 -3.4038663 -4.72785 -5.8786945 -6.3038197 -5.863728][-5.2875853 -5.09636 -4.4830174 -3.3714952 -1.7178171 -0.041457176 1.6850786 2.7950048 2.7486587 1.4912581 -0.6012342 -2.8385479 -4.7760715 -5.6324568 -5.4912648][-4.4654336 -4.2272377 -3.2705607 -1.7227995 0.17266607 2.107213 4.1030769 5.4118109 5.4544287 4.1653442 1.7828817 -0.9440372 -3.3652802 -4.8290658 -5.1021757][-3.5410349 -3.1452303 -2.3487537 -0.68373656 1.5573201 3.7807684 5.6600761 6.5014944 6.3157072 4.9214611 2.3932834 -0.606195 -3.0589373 -4.4804859 -4.8986845][-2.969348 -2.4125943 -1.422204 -0.024907589 1.7696099 3.5939837 5.0084448 5.5855818 5.2071028 3.5731144 1.2215152 -1.3365481 -3.7172134 -5.0355654 -5.2684097][-3.2108669 -2.6399584 -1.7889552 -0.51638126 0.83972549 2.2908072 3.2123251 3.2765031 2.6802192 1.1690869 -0.9109962 -2.9587746 -4.5972323 -5.6066751 -5.6888056][-4.1890869 -3.4785752 -2.6899474 -1.7212448 -0.48624492 0.27587795 0.73307371 0.54986763 -0.36814594 -1.6674619 -3.1245947 -4.7085829 -5.7514019 -6.0453773 -5.687109][-4.7193208 -4.158155 -3.460444 -2.7873631 -1.8709104 -1.410497 -1.531389 -2.1195097 -3.0430918 -4.17248 -5.2901073 -6.13325 -6.4319158 -6.0673532 -5.447989][-5.6954212 -4.9831295 -4.1218772 -3.1429057 -2.6194291 -2.3617353 -2.4975529 -3.3634188 -4.5133905 -5.3997717 -6.0517883 -6.5529785 -6.5203061 -5.8252068 -4.839304][-6.2740889 -5.3682394 -4.3014703 -3.1969438 -2.4385359 -2.2718782 -2.4386275 -3.2664664 -4.3481288 -5.388433 -6.0527444 -6.1521897 -5.7680931 -5.0461044 -4.1265054][-5.7954388 -4.9632707 -3.7951448 -2.6234837 -1.6602194 -1.2425637 -1.4001062 -2.2468736 -3.3837259 -4.4471035 -4.998394 -5.2854185 -5.1557312 -4.2151642 -3.2697349]]...]
INFO - root - 2017-12-16 09:03:05.818909: step 67610, loss = 0.24, batch loss = 0.18 (29.1 examples/sec; 0.275 sec/batch; 20h:13m:43s remains)
INFO - root - 2017-12-16 09:03:08.672482: step 67620, loss = 0.33, batch loss = 0.27 (27.4 examples/sec; 0.292 sec/batch; 21h:30m:34s remains)
INFO - root - 2017-12-16 09:03:11.504589: step 67630, loss = 0.27, batch loss = 0.22 (27.5 examples/sec; 0.291 sec/batch; 21h:25m:19s remains)
INFO - root - 2017-12-16 09:03:14.323872: step 67640, loss = 0.21, batch loss = 0.15 (27.8 examples/sec; 0.288 sec/batch; 21h:10m:31s remains)
INFO - root - 2017-12-16 09:03:17.165912: step 67650, loss = 0.22, batch loss = 0.16 (28.7 examples/sec; 0.279 sec/batch; 20h:30m:29s remains)
INFO - root - 2017-12-16 09:03:19.994670: step 67660, loss = 0.24, batch loss = 0.19 (28.3 examples/sec; 0.282 sec/batch; 20h:45m:54s remains)
INFO - root - 2017-12-16 09:03:22.803980: step 67670, loss = 0.25, batch loss = 0.19 (27.3 examples/sec; 0.293 sec/batch; 21h:32m:04s remains)
INFO - root - 2017-12-16 09:03:25.682572: step 67680, loss = 0.31, batch loss = 0.25 (27.6 examples/sec; 0.289 sec/batch; 21h:17m:27s remains)
INFO - root - 2017-12-16 09:03:28.502612: step 67690, loss = 0.34, batch loss = 0.28 (28.4 examples/sec; 0.282 sec/batch; 20h:43m:38s remains)
INFO - root - 2017-12-16 09:03:31.304071: step 67700, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.285 sec/batch; 20h:59m:24s remains)
2017-12-16 09:03:31.834634: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.513782 -7.3967161 -7.150423 -6.9649324 -6.7465115 -6.8465881 -6.8919272 -7.2306585 -7.4311452 -7.5195031 -7.489006 -7.1057734 -6.39387 -5.3963881 -4.5161357][-7.5896025 -7.5344229 -7.2505445 -7.0553975 -7.0198994 -7.3421316 -7.6384525 -8.1838274 -8.5441113 -8.734375 -8.5554018 -8.1359625 -7.5366931 -6.4766965 -5.4791994][-6.7728286 -6.4462242 -6.0884967 -5.9174967 -5.9000382 -6.2902336 -6.8066397 -7.7429137 -8.4055138 -8.8925095 -8.8696136 -8.5869541 -8.2341919 -7.3360977 -6.4122005][-5.1485476 -4.3520637 -3.6775661 -3.3566422 -3.4295168 -3.937552 -4.4237103 -5.3557339 -6.2318993 -7.2428904 -7.9711866 -8.0603981 -8.1041584 -7.6760006 -6.8712969][-3.3120432 -1.9790359 -0.73501158 -0.24976397 -0.18613482 -0.6658287 -1.4497931 -2.3750575 -3.1620831 -4.1573749 -5.3687091 -6.4668818 -7.145535 -7.1709251 -6.8439121][-2.2650304 -0.61308455 0.9147954 2.1850209 2.7864571 2.4380393 1.9519305 1.0680175 0.12288284 -0.99477553 -2.5551121 -4.3541837 -5.9319687 -6.5923743 -6.4847188][-2.7370009 -0.67129183 1.1112595 2.5371671 3.723052 4.049983 4.0559931 3.5392857 2.93496 1.6818452 -0.20941257 -2.4892678 -4.5259871 -5.5300732 -5.70188][-3.640389 -1.7921121 0.11008787 1.847322 3.35072 4.0663042 4.5599413 4.2350407 3.7899141 2.55366 0.45492077 -1.8041985 -3.6757152 -4.793221 -5.0526819][-5.0431867 -3.4878607 -1.8893352 -0.38497114 1.3559589 2.6495967 3.5212178 3.633419 3.3216443 2.3625779 0.637331 -1.6878464 -3.601799 -4.59341 -4.792592][-6.251833 -5.3391714 -4.2073107 -2.9588361 -1.3950903 -0.0616107 1.0239882 1.531198 1.4500017 0.51874971 -0.88802457 -2.7050378 -4.2810221 -5.0383558 -5.0666933][-6.9151382 -6.2283182 -5.4539218 -4.62191 -3.4579921 -2.267489 -1.1732502 -0.74373174 -0.826113 -1.5012341 -2.5690522 -3.8907163 -4.9373732 -5.446907 -5.37193][-6.5285997 -6.2556381 -5.8705893 -5.2430191 -4.504499 -3.7785292 -3.0734329 -2.7834997 -2.8248415 -3.3974509 -3.9891319 -4.8036394 -5.4061246 -5.5183883 -5.2441955][-5.4364743 -5.2498035 -5.0831 -4.8053 -4.533195 -4.1769581 -3.8665051 -3.8519258 -3.8977747 -4.24028 -4.4889226 -4.968502 -5.3867059 -5.477603 -5.1392918][-4.01704 -3.9422214 -3.8891273 -3.7690556 -3.7704265 -3.6289158 -3.5919168 -3.7456975 -3.9243321 -4.2585788 -4.3628759 -4.4877958 -4.5598907 -4.4724331 -4.2850304][-2.936038 -2.7505579 -2.6716242 -2.7295513 -2.7758107 -2.8253071 -2.9368129 -3.035799 -3.1886716 -3.3930731 -3.5075514 -3.4719739 -3.37291 -3.2815776 -3.1485844]]...]
INFO - root - 2017-12-16 09:03:34.648474: step 67710, loss = 0.29, batch loss = 0.24 (28.7 examples/sec; 0.278 sec/batch; 20h:29m:01s remains)
INFO - root - 2017-12-16 09:03:37.519130: step 67720, loss = 0.20, batch loss = 0.14 (27.7 examples/sec; 0.289 sec/batch; 21h:13m:45s remains)
INFO - root - 2017-12-16 09:03:40.369086: step 67730, loss = 0.31, batch loss = 0.25 (29.6 examples/sec; 0.270 sec/batch; 19h:52m:53s remains)
INFO - root - 2017-12-16 09:03:43.204571: step 67740, loss = 0.21, batch loss = 0.15 (28.0 examples/sec; 0.285 sec/batch; 20h:58m:50s remains)
INFO - root - 2017-12-16 09:03:46.020430: step 67750, loss = 0.22, batch loss = 0.16 (28.3 examples/sec; 0.283 sec/batch; 20h:48m:05s remains)
INFO - root - 2017-12-16 09:03:48.900590: step 67760, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.283 sec/batch; 20h:50m:00s remains)
INFO - root - 2017-12-16 09:03:51.722491: step 67770, loss = 0.22, batch loss = 0.16 (29.1 examples/sec; 0.275 sec/batch; 20h:12m:43s remains)
INFO - root - 2017-12-16 09:03:54.572639: step 67780, loss = 0.23, batch loss = 0.17 (28.9 examples/sec; 0.277 sec/batch; 20h:20m:12s remains)
INFO - root - 2017-12-16 09:03:57.406714: step 67790, loss = 0.32, batch loss = 0.26 (28.7 examples/sec; 0.278 sec/batch; 20h:28m:30s remains)
INFO - root - 2017-12-16 09:04:00.235624: step 67800, loss = 0.21, batch loss = 0.15 (28.3 examples/sec; 0.282 sec/batch; 20h:45m:40s remains)
2017-12-16 09:04:00.740770: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1068568 -5.6465592 -5.1340394 -4.9236774 -4.664073 -4.6709433 -4.4763355 -4.4884152 -4.5604539 -4.5943913 -4.7470756 -4.7486463 -4.7275939 -4.6327357 -4.4911327][-6.154325 -5.7361169 -5.1973562 -4.8662381 -4.640192 -5.0929995 -5.3215494 -5.5497293 -5.5162916 -5.5418372 -5.7584891 -5.790041 -5.7323227 -5.421587 -5.2275467][-6.1909409 -5.3849359 -4.6784892 -4.1499 -3.7850056 -4.261662 -4.85846 -5.7746344 -6.1388474 -6.3467293 -6.4200497 -6.3402553 -6.3818412 -6.1965709 -6.0651445][-5.59232 -4.8558068 -3.8387113 -3.0193331 -2.5204983 -2.6666453 -2.963768 -3.9120421 -4.93089 -5.6754961 -5.9936271 -6.1084285 -6.1595654 -6.1176615 -6.3471394][-4.1881752 -3.2217202 -2.2529271 -1.3478205 -0.52502227 -0.38890648 -0.71163893 -1.3539841 -1.963299 -2.9079981 -4.1098351 -4.7920823 -5.3146229 -5.6997213 -6.0659075][-3.6783972 -2.5601616 -1.1841745 0.33077526 1.7630997 2.3604059 2.3393369 1.5823107 0.58387995 -0.357234 -1.4364824 -2.5167611 -3.6082752 -4.4607506 -5.3421249][-3.2138774 -2.2265682 -0.84409738 0.927505 2.7011814 4.1271391 4.846571 4.3360043 3.3978448 2.2854462 0.75963163 -0.71928334 -2.1847737 -3.3261194 -4.2476268][-3.1216946 -2.3980961 -1.3569207 0.67158079 2.7059922 4.4217281 5.4360533 5.437789 4.8947544 3.736867 2.1048589 0.48005342 -1.1418591 -2.6434464 -3.9918919][-4.1446662 -3.5406539 -2.601624 -0.98468232 1.0346537 3.0861244 4.3053255 4.630908 4.4317093 3.484663 1.9369035 0.18191385 -1.319941 -2.6586337 -3.7812674][-4.7611489 -4.63498 -4.5848703 -3.3752661 -1.7364323 0.10010529 1.4285226 2.2069774 2.5037384 2.0272198 0.973474 -0.61990047 -2.2446995 -3.6368055 -4.6567793][-6.4849577 -5.9399562 -5.519495 -4.9985113 -4.423502 -3.1788449 -2.0126827 -1.0270953 -0.41773796 -0.33926153 -0.82017159 -1.9383202 -3.1827044 -4.4957466 -5.5589437][-7.2660475 -7.003768 -6.8041587 -6.3274908 -5.8450437 -5.2952938 -4.8273354 -4.0486374 -3.1751621 -2.8052714 -3.0373285 -3.8085308 -4.5226421 -5.4203453 -6.1608334][-7.2466707 -6.9021845 -6.7497644 -6.5456986 -6.457159 -6.29862 -6.1790371 -5.8206215 -5.1908789 -4.7367158 -4.5456805 -5.0642495 -5.6882529 -6.3742123 -6.7352524][-7.4276037 -6.7257066 -6.362896 -6.2415648 -6.2600417 -6.1454306 -6.2366486 -6.3333941 -6.1764607 -5.7940855 -5.6571927 -6.0015736 -6.2225013 -6.533042 -6.7672744][-6.8174114 -6.1421266 -5.5695915 -5.5598059 -5.8645906 -5.8503613 -5.8458004 -5.8859382 -6.0094357 -5.9857607 -5.960638 -6.1316032 -6.3497448 -6.4765434 -6.4304361]]...]
INFO - root - 2017-12-16 09:04:03.595326: step 67810, loss = 0.26, batch loss = 0.20 (29.1 examples/sec; 0.275 sec/batch; 20h:14m:34s remains)
INFO - root - 2017-12-16 09:04:06.452476: step 67820, loss = 0.20, batch loss = 0.14 (27.6 examples/sec; 0.290 sec/batch; 21h:20m:52s remains)
INFO - root - 2017-12-16 09:04:09.299172: step 67830, loss = 0.34, batch loss = 0.29 (27.7 examples/sec; 0.289 sec/batch; 21h:13m:09s remains)
INFO - root - 2017-12-16 09:04:12.142088: step 67840, loss = 0.24, batch loss = 0.18 (27.9 examples/sec; 0.287 sec/batch; 21h:06m:21s remains)
INFO - root - 2017-12-16 09:04:14.997189: step 67850, loss = 0.24, batch loss = 0.18 (29.3 examples/sec; 0.273 sec/batch; 20h:04m:26s remains)
INFO - root - 2017-12-16 09:04:17.855573: step 67860, loss = 0.29, batch loss = 0.23 (27.6 examples/sec; 0.290 sec/batch; 21h:17m:18s remains)
INFO - root - 2017-12-16 09:04:20.650949: step 67870, loss = 0.20, batch loss = 0.14 (28.3 examples/sec; 0.283 sec/batch; 20h:46m:53s remains)
INFO - root - 2017-12-16 09:04:23.492147: step 67880, loss = 0.30, batch loss = 0.25 (28.1 examples/sec; 0.285 sec/batch; 20h:56m:40s remains)
INFO - root - 2017-12-16 09:04:26.377041: step 67890, loss = 0.22, batch loss = 0.16 (27.2 examples/sec; 0.294 sec/batch; 21h:35m:58s remains)
INFO - root - 2017-12-16 09:04:29.182765: step 67900, loss = 0.20, batch loss = 0.14 (28.8 examples/sec; 0.278 sec/batch; 20h:24m:14s remains)
2017-12-16 09:04:29.715508: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0780568 -3.1800976 -3.2698951 -3.1197109 -3.0028377 -2.8935709 -2.7827625 -2.6398098 -2.4103112 -2.1589038 -1.9112754 -1.7353606 -1.669724 -1.72331 -1.9320214][-2.9030235 -3.0962691 -3.1786389 -3.1891503 -3.1750753 -3.0793796 -3.0057759 -2.84732 -2.6624036 -2.4620051 -2.2325413 -2.0345633 -1.8797016 -1.809449 -1.7959902][-2.7573953 -3.1957755 -3.5649478 -3.6059897 -3.5805535 -3.4770439 -3.4311471 -3.2611909 -3.0684457 -2.9150579 -2.807126 -2.6486635 -2.4677844 -2.3459456 -2.2144811][-2.7433555 -3.5035591 -4.0810175 -4.2722554 -4.263586 -3.994957 -3.6486237 -3.3244414 -3.1092658 -3.0747206 -3.0305719 -3.0745282 -3.1053867 -2.929209 -2.6729591][-2.9781556 -3.7901216 -4.4420586 -4.7212024 -4.586328 -3.9674239 -3.375977 -2.9482861 -2.5925057 -2.5796833 -2.6440749 -2.8912792 -3.1287737 -3.2970767 -3.4159212][-3.1029968 -3.9413986 -4.5790629 -4.7479391 -4.396512 -3.5307059 -2.6503508 -1.8758633 -1.2563229 -1.0913615 -1.2800925 -1.9659724 -2.7161589 -3.1519141 -3.4514153][-3.293272 -3.9313908 -4.4945641 -4.5325427 -3.8974566 -2.6798277 -1.3711891 -0.35476017 0.32217598 0.43729496 0.22188854 -0.45893884 -1.51806 -2.6084516 -3.4661393][-3.2547824 -3.5344808 -3.7524402 -3.6311383 -2.976182 -1.6787274 -0.23503065 0.88009405 1.7651963 2.0255642 1.6352391 0.50804186 -0.95211482 -2.2711453 -3.34728][-2.7212543 -2.8908772 -2.9432333 -2.655724 -1.9311533 -0.90202713 0.32869148 1.3713732 2.1326795 2.3498507 1.9101253 0.84670782 -0.74000597 -2.2789872 -3.5647728][-2.5042646 -2.5022602 -2.4199696 -2.0921998 -1.3997884 -0.48644924 0.496027 1.2977276 1.8399782 1.8977742 1.3531342 0.30600071 -1.1276939 -2.5507505 -3.5636606][-2.337837 -2.2883077 -2.1433849 -1.9444768 -1.483968 -0.73352551 0.11623144 0.67138767 0.64441919 0.64910936 0.21383762 -0.66322637 -1.7642863 -2.8192253 -3.5186856][-2.1414754 -1.8737476 -1.7016253 -1.6509368 -1.2742951 -0.92837238 -0.71691728 -0.48500896 -0.32844067 -0.3953824 -0.87611365 -1.601043 -2.3684297 -2.9287107 -3.2497392][-1.6952047 -1.4096262 -1.4997888 -1.6296546 -1.4778433 -1.124738 -0.74710464 -0.86742878 -1.2260203 -1.3331311 -1.4831154 -1.7836561 -2.3442609 -2.6247683 -2.8257766][-1.5805812 -0.98276973 -0.84292293 -1.1233385 -1.2382321 -1.2886126 -1.282845 -1.3254318 -1.414818 -1.6350734 -1.8434124 -1.8615601 -1.9975331 -2.0319803 -2.0553637][-0.65651965 -0.23936367 -0.36762333 -0.80144215 -1.0984635 -1.1014016 -1.1633503 -1.4280143 -1.8535535 -1.9091289 -1.7242949 -1.4927371 -1.3678162 -1.2801158 -1.2299576]]...]
INFO - root - 2017-12-16 09:04:32.550250: step 67910, loss = 0.23, batch loss = 0.17 (25.2 examples/sec; 0.317 sec/batch; 23h:19m:40s remains)
INFO - root - 2017-12-16 09:04:35.441719: step 67920, loss = 0.37, batch loss = 0.32 (28.3 examples/sec; 0.283 sec/batch; 20h:47m:42s remains)
INFO - root - 2017-12-16 09:04:38.351417: step 67930, loss = 0.26, batch loss = 0.20 (28.5 examples/sec; 0.281 sec/batch; 20h:39m:09s remains)
INFO - root - 2017-12-16 09:04:41.241529: step 67940, loss = 0.23, batch loss = 0.17 (26.7 examples/sec; 0.299 sec/batch; 22h:00m:11s remains)
INFO - root - 2017-12-16 09:04:44.084221: step 67950, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.283 sec/batch; 20h:46m:29s remains)
INFO - root - 2017-12-16 09:04:46.905177: step 67960, loss = 0.57, batch loss = 0.51 (28.4 examples/sec; 0.282 sec/batch; 20h:42m:10s remains)
INFO - root - 2017-12-16 09:04:49.757379: step 67970, loss = 0.21, batch loss = 0.15 (29.0 examples/sec; 0.276 sec/batch; 20h:17m:29s remains)
INFO - root - 2017-12-16 09:04:52.610983: step 67980, loss = 0.27, batch loss = 0.21 (28.9 examples/sec; 0.277 sec/batch; 20h:20m:35s remains)
INFO - root - 2017-12-16 09:04:55.463186: step 67990, loss = 0.29, batch loss = 0.23 (28.6 examples/sec; 0.280 sec/batch; 20h:34m:22s remains)
INFO - root - 2017-12-16 09:04:58.330932: step 68000, loss = 0.22, batch loss = 0.16 (27.8 examples/sec; 0.288 sec/batch; 21h:07m:58s remains)
2017-12-16 09:04:58.855192: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.56603 -4.8578019 -5.2129064 -5.703681 -5.9696455 -6.0258994 -5.7446308 -5.4802551 -5.3573861 -5.3439283 -5.56321 -5.869153 -6.4502149 -6.7580342 -6.9843836][-4.4315605 -4.8882132 -5.1438346 -5.5752754 -5.77925 -6.007966 -5.6887455 -5.3220906 -5.1841846 -5.302371 -5.6500888 -5.9527903 -6.5492468 -6.9897108 -7.1690645][-3.9930859 -4.3156466 -4.446661 -4.6430416 -4.71979 -4.70394 -4.372797 -4.0906587 -3.9887214 -4.1290474 -4.7783413 -5.3399787 -6.2066455 -6.6794953 -7.24444][-3.1554241 -3.2637672 -3.2124186 -3.0701475 -2.670562 -2.3598788 -1.9326322 -1.7164955 -1.5249016 -1.9237533 -2.706089 -3.6834395 -5.25404 -6.2989407 -7.1347656][-2.7866709 -2.4504771 -1.8325932 -1.2336988 -0.50191832 0.17199183 0.92368269 1.0959229 1.0951815 0.45316696 -0.40633821 -1.8595648 -3.645663 -5.3293805 -6.8668537][-3.1308131 -2.5413945 -1.4909046 -0.29808378 1.0782142 2.171083 3.0254006 3.4394054 3.5024071 2.939424 1.7459364 -0.20456743 -2.5405104 -4.8455648 -6.5477591][-4.0442419 -3.1944466 -1.8180392 -0.17685366 1.5141025 2.9299421 4.1982479 4.7438812 4.97149 4.5282946 3.4040737 1.3471422 -1.332252 -3.9220719 -6.0146003][-4.8221154 -3.792613 -2.5848966 -0.93980432 0.80166483 2.311172 3.6881218 4.5361958 5.2234783 4.7479572 3.6544857 1.6725421 -0.88693762 -3.3702078 -5.3143978][-5.5935063 -4.7467408 -3.7235603 -2.2476692 -0.954082 0.39211464 1.8281059 2.7015634 3.3228827 3.2230549 2.5187716 0.81073713 -1.2877634 -3.3887396 -4.9799147][-6.2171268 -5.6084514 -5.0099845 -4.101253 -3.2263367 -2.1062503 -1.1623161 -0.39027452 0.29135466 0.30446196 -0.13437223 -1.2011483 -2.6294308 -3.9210992 -4.9061193][-6.1594486 -5.8655148 -5.6312389 -5.2065868 -4.8488345 -4.18773 -3.3958807 -2.8405545 -2.4973636 -2.4537444 -2.4809284 -2.991519 -3.7071373 -4.3410354 -4.8262825][-5.5171032 -5.4816461 -5.6135955 -5.5553112 -5.5612483 -5.2851362 -4.8178773 -4.4506764 -3.9503152 -3.6910605 -3.5875018 -3.8027267 -4.1252608 -4.4284172 -4.66273][-4.4456286 -4.5267015 -4.8375163 -4.9319444 -5.1522908 -5.2054167 -5.1423893 -5.0004168 -4.6816187 -4.5507026 -4.4344163 -4.4252191 -4.3788238 -4.3876128 -4.5071507][-3.7629142 -3.736552 -3.8984213 -4.100461 -4.3815231 -4.456552 -4.4895854 -4.5076118 -4.4342566 -4.4048853 -4.2500424 -4.2486444 -4.3406549 -4.22516 -4.0616636][-3.3180134 -3.2479162 -3.3089495 -3.4230108 -3.5929024 -3.6952863 -3.7131302 -3.66238 -3.55381 -3.429318 -3.3010507 -3.2875793 -3.3371568 -3.3271105 -3.3355269]]...]
INFO - root - 2017-12-16 09:05:01.679940: step 68010, loss = 0.30, batch loss = 0.24 (29.0 examples/sec; 0.276 sec/batch; 20h:15m:51s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 09:05:04.495908: step 68020, loss = 0.25, batch loss = 0.19 (27.6 examples/sec; 0.290 sec/batch; 21h:19m:05s remains)
INFO - root - 2017-12-16 09:05:07.333048: step 68030, loss = 0.32, batch loss = 0.26 (27.8 examples/sec; 0.287 sec/batch; 21h:06m:12s remains)
INFO - root - 2017-12-16 09:05:10.162418: step 68040, loss = 0.27, batch loss = 0.21 (29.3 examples/sec; 0.273 sec/batch; 20h:02m:57s remains)
INFO - root - 2017-12-16 09:05:13.010124: step 68050, loss = 0.29, batch loss = 0.24 (27.2 examples/sec; 0.294 sec/batch; 21h:34m:28s remains)
INFO - root - 2017-12-16 09:05:15.876166: step 68060, loss = 0.33, batch loss = 0.27 (28.2 examples/sec; 0.284 sec/batch; 20h:50m:26s remains)
INFO - root - 2017-12-16 09:05:18.728396: step 68070, loss = 0.30, batch loss = 0.24 (28.0 examples/sec; 0.286 sec/batch; 21h:01m:16s remains)
INFO - root - 2017-12-16 09:05:21.558112: step 68080, loss = 0.22, batch loss = 0.16 (28.9 examples/sec; 0.277 sec/batch; 20h:18m:32s remains)
INFO - root - 2017-12-16 09:05:24.434202: step 68090, loss = 0.24, batch loss = 0.18 (27.1 examples/sec; 0.296 sec/batch; 21h:42m:24s remains)
INFO - root - 2017-12-16 09:05:27.262864: step 68100, loss = 0.24, batch loss = 0.18 (27.3 examples/sec; 0.293 sec/batch; 21h:31m:23s remains)
2017-12-16 09:05:27.816144: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8454604 -4.3182979 -4.7735014 -5.1771212 -5.4530849 -5.3924336 -5.0138974 -4.8203969 -4.5077214 -4.2043633 -3.8685451 -3.3067231 -2.7519054 -2.1879535 -1.7185144][-3.3267598 -3.8117046 -4.341568 -4.6742177 -4.8553987 -4.8680863 -4.6153388 -4.3001204 -3.9064944 -3.6136026 -3.3414273 -3.0056431 -2.606849 -1.9949613 -1.4263756][-2.8570111 -3.3570662 -3.8665023 -4.188518 -4.1627755 -3.9544272 -3.5136857 -3.0534739 -2.6978824 -2.6966972 -2.6138456 -2.1976917 -1.8539128 -1.6633019 -1.6413262][-2.5589075 -2.9668894 -3.2647867 -3.2822013 -3.0255487 -2.6153212 -2.1006358 -1.6099708 -1.2295856 -1.3400149 -1.6117933 -1.8400233 -2.00661 -1.9636028 -1.7655642][-2.2853816 -2.5409775 -2.6786728 -2.5139148 -2.0144479 -1.3952086 -0.627856 0.062208652 0.31426144 -0.1569066 -0.92472053 -1.4774976 -1.8177514 -2.3476632 -2.442477][-2.0052764 -2.0461082 -1.8610885 -1.4151111 -0.55022335 0.32742167 1.1390376 1.7336407 1.9118915 1.104495 -0.001750946 -0.98412156 -1.9304643 -2.6406486 -2.6809242][-2.2366276 -1.9415116 -1.4026022 -0.49936891 0.71449995 1.8668976 2.7618947 3.2600856 3.0988622 2.1540008 0.88455963 -0.48480392 -1.6596928 -2.6424708 -3.0382171][-2.6117368 -2.1147048 -1.3603592 -0.25940466 1.1593885 2.4120216 3.1971436 3.5999737 3.3415666 2.1944475 0.71689844 -0.68219113 -1.7845345 -2.8957651 -3.3690653][-2.6689239 -2.290452 -1.5421417 -0.49932051 0.83177042 2.1738348 2.9154243 3.0952578 2.5282154 1.3608789 0.0096898079 -1.3364842 -2.4329648 -3.2639389 -3.7191482][-3.205066 -2.895206 -2.2378368 -1.2524519 -0.090248108 1.002665 1.6176949 1.7843099 1.3216715 0.30476618 -0.83607459 -1.9384418 -2.8937917 -3.5670309 -4.0325165][-3.2131214 -2.9715087 -2.5985789 -1.9895985 -1.3004248 -0.420784 0.13242054 0.19426298 -0.17927504 -0.84607434 -1.6208727 -2.3790162 -3.0470819 -3.5676229 -4.007781][-2.8634923 -2.8956587 -2.8791988 -2.6391954 -2.3112254 -1.8199582 -1.4595144 -1.3873904 -1.5082004 -1.8967199 -2.2798381 -2.6421313 -2.9911509 -3.3318527 -3.7036104][-2.5634661 -2.7614312 -3.0631363 -3.1464438 -3.2443607 -2.9871883 -2.6821649 -2.5672889 -2.6008682 -2.7774811 -2.9698231 -3.1473808 -3.2974234 -3.4459383 -3.6353936][-1.986881 -2.0732908 -2.3819358 -2.8310075 -3.2217484 -3.2657518 -3.2077928 -3.2507157 -3.2074289 -3.2119215 -3.2103081 -3.2434864 -3.2881751 -3.3183508 -3.4307568][-1.1500661 -1.3780878 -1.9248323 -2.5335174 -3.1488934 -3.4298735 -3.3979063 -3.3186529 -3.1548457 -3.0621459 -2.9626825 -2.9038253 -2.8459554 -2.8569803 -2.9237106]]...]
INFO - root - 2017-12-16 09:05:30.700296: step 68110, loss = 0.19, batch loss = 0.14 (27.5 examples/sec; 0.291 sec/batch; 21h:21m:15s remains)
INFO - root - 2017-12-16 09:05:33.596717: step 68120, loss = 0.24, batch loss = 0.18 (27.5 examples/sec; 0.291 sec/batch; 21h:21m:20s remains)
INFO - root - 2017-12-16 09:05:36.422207: step 68130, loss = 0.39, batch loss = 0.33 (26.9 examples/sec; 0.298 sec/batch; 21h:52m:14s remains)
INFO - root - 2017-12-16 09:05:39.284270: step 68140, loss = 0.24, batch loss = 0.18 (25.9 examples/sec; 0.309 sec/batch; 22h:40m:03s remains)
INFO - root - 2017-12-16 09:05:42.153891: step 68150, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 20h:51m:42s remains)
INFO - root - 2017-12-16 09:05:44.992267: step 68160, loss = 0.42, batch loss = 0.37 (27.3 examples/sec; 0.293 sec/batch; 21h:29m:34s remains)
INFO - root - 2017-12-16 09:05:47.839490: step 68170, loss = 0.31, batch loss = 0.25 (28.0 examples/sec; 0.285 sec/batch; 20h:57m:16s remains)
INFO - root - 2017-12-16 09:05:50.706101: step 68180, loss = 0.23, batch loss = 0.18 (27.8 examples/sec; 0.288 sec/batch; 21h:07m:13s remains)
INFO - root - 2017-12-16 09:05:53.616194: step 68190, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 20h:59m:51s remains)
INFO - root - 2017-12-16 09:05:56.442172: step 68200, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.281 sec/batch; 20h:38m:40s remains)
2017-12-16 09:05:56.997126: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1995459 -5.92297 -5.6298904 -5.5526695 -5.287601 -5.229228 -5.23732 -5.2928696 -5.2745161 -4.8995318 -4.5543971 -4.164041 -3.7268691 -3.3413382 -3.1893215][-6.2655034 -5.8509054 -5.4195313 -5.1718111 -4.9069643 -4.8878765 -4.832984 -4.8702869 -4.6453366 -4.2522855 -3.7515016 -3.1430092 -2.5544887 -1.9646859 -1.8283074][-5.85348 -5.2643771 -4.6562967 -4.0966883 -3.7038479 -3.545836 -3.5734098 -3.7314672 -3.8210957 -3.605551 -3.0117249 -2.4194469 -1.992058 -1.3138347 -0.86266541][-4.1830745 -3.4768147 -2.6897225 -2.1521165 -1.8027966 -1.7258158 -1.8494232 -2.2294216 -2.8023257 -2.8666582 -2.7110124 -2.4178903 -2.071517 -1.5556765 -1.0806999][-2.57835 -1.3929002 -0.28363276 0.1336298 0.35242558 0.18203878 -0.05918169 -0.58203721 -1.309221 -1.8610094 -2.3834257 -2.5269394 -2.5599053 -2.371439 -2.0035326][-1.5283253 -0.15546656 1.0803642 1.9406672 2.3976698 2.174449 1.7996287 1.1344867 0.28355455 -0.68585968 -1.6307585 -2.3575356 -2.9932091 -3.062562 -2.8685629][-1.071115 0.43155098 1.5404048 2.4976883 3.0685372 3.2186007 3.0620818 2.2294474 1.1818185 0.07424593 -0.99947906 -2.0563543 -2.9949703 -3.4669275 -3.4818006][-1.7050295 -0.20162821 1.1792817 2.0928516 2.491879 2.8374252 2.8855019 2.3376098 1.4402685 0.30211687 -0.84002137 -2.0640178 -3.0673008 -3.7184083 -3.8968058][-2.9011264 -1.8479917 -0.77282 0.16668844 1.0288887 1.7289815 1.9419222 1.6849508 1.0233746 -0.071625233 -1.1877501 -2.5058975 -3.65286 -4.316453 -4.4311619][-4.4176731 -4.00804 -3.3543422 -2.460371 -1.3894517 -0.39558792 0.27272463 0.18942356 -0.29238081 -1.1218853 -2.1271486 -3.4384651 -4.4596334 -4.9077272 -4.936821][-6.0575509 -5.9779592 -5.752058 -5.0892596 -4.1102738 -2.9262226 -1.9111729 -1.74213 -2.0528364 -2.7404761 -3.5854554 -4.62958 -5.3442259 -5.4300356 -5.212759][-6.4175596 -6.6757526 -6.85165 -6.4315205 -5.6001444 -4.5862417 -3.7562039 -3.5025382 -3.4087472 -3.7585113 -4.4008217 -5.1407671 -5.6107955 -5.4875569 -5.2355676][-5.813818 -6.3701181 -6.7992125 -6.6470013 -6.2279072 -5.5593014 -4.782619 -4.48775 -4.3457403 -4.4108195 -4.5390468 -5.0074158 -5.388422 -5.0999932 -4.889575][-5.1237383 -5.5565891 -5.9746966 -6.1288414 -6.0929704 -5.7096934 -5.2035594 -4.9413109 -4.64965 -4.4288111 -4.261168 -4.3866653 -4.480988 -4.2220902 -3.9806669][-4.4890723 -4.7176046 -4.9122591 -5.1287546 -5.2208447 -5.1597624 -4.9908533 -4.7107916 -4.2914705 -3.9122491 -3.6764841 -3.5505488 -3.3228397 -2.9774225 -2.6450634]]...]
INFO - root - 2017-12-16 09:05:59.847745: step 68210, loss = 0.26, batch loss = 0.20 (26.7 examples/sec; 0.299 sec/batch; 21h:58m:23s remains)
INFO - root - 2017-12-16 09:06:02.732795: step 68220, loss = 0.35, batch loss = 0.29 (28.9 examples/sec; 0.277 sec/batch; 20h:18m:06s remains)
INFO - root - 2017-12-16 09:06:05.632440: step 68230, loss = 0.22, batch loss = 0.17 (27.3 examples/sec; 0.293 sec/batch; 21h:28m:48s remains)
INFO - root - 2017-12-16 09:06:08.504173: step 68240, loss = 0.29, batch loss = 0.23 (26.6 examples/sec; 0.301 sec/batch; 22h:05m:23s remains)
INFO - root - 2017-12-16 09:06:11.358811: step 68250, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.283 sec/batch; 20h:48m:25s remains)
INFO - root - 2017-12-16 09:06:14.240690: step 68260, loss = 0.22, batch loss = 0.16 (28.2 examples/sec; 0.283 sec/batch; 20h:47m:20s remains)
INFO - root - 2017-12-16 09:06:17.163701: step 68270, loss = 0.27, batch loss = 0.22 (27.0 examples/sec; 0.296 sec/batch; 21h:43m:11s remains)
INFO - root - 2017-12-16 09:06:19.975453: step 68280, loss = 0.31, batch loss = 0.25 (28.8 examples/sec; 0.278 sec/batch; 20h:25m:12s remains)
INFO - root - 2017-12-16 09:06:22.832625: step 68290, loss = 0.24, batch loss = 0.18 (28.0 examples/sec; 0.286 sec/batch; 20h:59m:32s remains)
INFO - root - 2017-12-16 09:06:25.706063: step 68300, loss = 0.34, batch loss = 0.28 (28.1 examples/sec; 0.285 sec/batch; 20h:54m:50s remains)
2017-12-16 09:06:26.274167: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7492716 -1.8184979 -2.0870237 -2.5585511 -3.0102463 -3.1503484 -3.2641191 -3.5385339 -3.7969146 -3.9380317 -3.6293707 -3.3877909 -3.0112 -2.5472198 -2.1835828][-1.443011 -1.6753118 -2.0206449 -2.4895709 -2.9229178 -3.3159945 -3.4813566 -3.6645582 -3.9567053 -4.0768156 -3.8244472 -3.6165674 -3.2617583 -2.7352905 -2.2128038][-1.6097903 -1.9580109 -2.3758371 -2.7190294 -2.8792744 -3.1016607 -3.1741042 -3.3548558 -3.5776412 -3.8368187 -4.0463982 -4.0667362 -3.9393709 -3.4545927 -2.70827][-2.2782772 -2.6458163 -2.8921847 -2.9702086 -2.8589842 -2.6332622 -2.3418667 -2.29834 -2.5730491 -3.2449915 -3.7220302 -4.1169767 -4.3376255 -3.989135 -3.2357316][-2.7555161 -3.052448 -3.081224 -2.7627192 -2.1711812 -1.5360184 -0.94664025 -0.63512516 -0.87413645 -1.7766714 -2.7771561 -3.6684554 -4.14192 -4.199614 -3.788167][-2.9382064 -3.0976896 -2.7885747 -2.007951 -0.86156487 0.22680902 1.190865 1.6523223 1.2295418 0.032728195 -1.5153213 -3.1037366 -4.130414 -4.3755531 -4.1264424][-2.9222844 -2.8843572 -2.3732626 -1.3201752 0.17854548 1.7556424 3.1632838 3.639595 3.03519 1.56703 -0.4008913 -2.4024279 -3.8789334 -4.6742182 -4.6172752][-2.8517432 -2.7221699 -2.2264662 -0.96282768 0.72073793 2.5739737 4.1202364 4.6093245 3.9593668 2.1343193 -0.13030672 -2.2496564 -3.9169319 -4.9121876 -5.1048369][-2.8516726 -2.694911 -2.1443913 -1.0707424 0.33685398 2.1708598 3.6231642 4.1484842 3.607193 1.7994771 -0.43316817 -2.6940165 -4.3914948 -5.3198495 -5.5441551][-2.6070886 -2.7473912 -2.5334904 -1.6537127 -0.46373916 0.92606926 1.9785986 2.3742833 1.778295 0.25928307 -1.5266418 -3.3253698 -4.7327061 -5.4745216 -5.6316633][-2.8316059 -2.9900937 -3.0219927 -2.4944122 -1.5788202 -0.50666857 0.18303299 0.2960391 -0.28549719 -1.541594 -2.9270625 -4.1655297 -5.1251407 -5.5797081 -5.6553011][-2.9998579 -3.2329245 -3.4588735 -3.1533289 -2.5877135 -1.8567545 -1.4438436 -1.6035903 -2.0752864 -2.8029304 -3.7220221 -4.5966105 -5.1646676 -5.3895946 -5.5198507][-3.318233 -3.6126702 -3.6980796 -3.6011541 -3.3968289 -2.8740697 -2.5690804 -2.6071537 -2.9212506 -3.4356217 -3.9004431 -4.3400269 -4.6836019 -4.8827987 -5.0033803][-3.5867634 -3.7666302 -3.731066 -3.5979707 -3.3888929 -3.0520406 -2.8772948 -2.9138772 -2.995023 -3.1179457 -3.2954316 -3.5700548 -3.8021388 -3.9204588 -4.0961061][-3.2416568 -3.3366566 -3.2113736 -2.9414804 -2.5386391 -2.1196668 -1.9229178 -2.0042188 -2.0953367 -2.2538667 -2.4558318 -2.5111852 -2.6742413 -2.8918972 -3.1058636]]...]
INFO - root - 2017-12-16 09:06:29.122973: step 68310, loss = 0.28, batch loss = 0.23 (28.0 examples/sec; 0.285 sec/batch; 20h:56m:20s remains)
INFO - root - 2017-12-16 09:06:31.964383: step 68320, loss = 0.28, batch loss = 0.23 (28.6 examples/sec; 0.280 sec/batch; 20h:33m:29s remains)
INFO - root - 2017-12-16 09:06:34.766588: step 68330, loss = 0.25, batch loss = 0.19 (28.9 examples/sec; 0.276 sec/batch; 20h:17m:19s remains)
INFO - root - 2017-12-16 09:06:37.638580: step 68340, loss = 0.24, batch loss = 0.18 (29.3 examples/sec; 0.273 sec/batch; 20h:03m:44s remains)
INFO - root - 2017-12-16 09:06:40.506573: step 68350, loss = 0.30, batch loss = 0.24 (27.8 examples/sec; 0.288 sec/batch; 21h:05m:48s remains)
INFO - root - 2017-12-16 09:06:43.343268: step 68360, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.279 sec/batch; 20h:28m:06s remains)
INFO - root - 2017-12-16 09:06:46.204692: step 68370, loss = 0.27, batch loss = 0.22 (28.0 examples/sec; 0.286 sec/batch; 20h:59m:20s remains)
INFO - root - 2017-12-16 09:06:49.040017: step 68380, loss = 0.45, batch loss = 0.39 (27.5 examples/sec; 0.291 sec/batch; 21h:22m:36s remains)
INFO - root - 2017-12-16 09:06:51.882785: step 68390, loss = 0.28, batch loss = 0.23 (29.3 examples/sec; 0.273 sec/batch; 20h:01m:07s remains)
INFO - root - 2017-12-16 09:06:54.748582: step 68400, loss = 0.21, batch loss = 0.15 (28.8 examples/sec; 0.278 sec/batch; 20h:22m:45s remains)
2017-12-16 09:06:55.304417: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9560428 -2.8054209 -2.9422488 -3.6315477 -4.364213 -4.9156904 -5.4559565 -5.8292646 -6.0276809 -6.0977535 -6.0460873 -5.6870708 -5.0395555 -4.4218392 -4.2958345][-1.6781323 -1.7044601 -2.1407928 -2.8700535 -3.4945626 -4.3043013 -5.1950655 -5.7238636 -5.8599887 -5.9579992 -5.9303703 -5.7273512 -5.2687254 -4.4261451 -3.9055064][-0.30260563 -0.52712226 -1.1470034 -2.0338285 -2.9535656 -3.5825682 -4.0232296 -4.7084966 -5.1076159 -5.2011533 -5.2221913 -4.9660769 -4.6017866 -3.9338305 -3.4090235][1.1619301 0.6384778 -0.29603815 -1.2854421 -2.0090113 -2.3773174 -2.5295269 -2.8399117 -3.0590148 -3.5612068 -3.8003602 -3.879005 -3.8256829 -3.1518826 -2.4445786][1.306963 0.66747952 -0.20245838 -0.82096553 -1.2180121 -1.050432 -0.61276054 -0.5830853 -0.722708 -1.5147591 -2.4438148 -3.1298141 -3.128921 -2.7645383 -2.2526648][0.045376778 -0.34522867 -0.80521774 -0.7834568 -0.17537832 0.6368885 1.4076676 1.7553487 1.500113 0.31359863 -1.1751926 -2.5272272 -3.2101204 -3.0987954 -2.4645357][-1.5370598 -1.3175163 -1.1233318 -0.60316324 0.7044673 2.3549676 3.6869373 3.9662266 3.41615 1.9406319 -0.071737766 -1.8365555 -2.8233681 -3.1693568 -2.9728026][-2.5939064 -2.2425194 -1.7505229 -0.64066911 1.0202589 2.7492933 4.1135712 4.5718164 3.8453903 2.0744224 -0.038275242 -1.7913179 -2.8734407 -3.3467886 -3.3329439][-3.7229838 -3.5157125 -2.625689 -1.4505656 -0.12072945 1.6732998 3.1053853 3.5020027 2.7083483 0.98826361 -0.95680642 -2.5628481 -3.5874741 -4.0444345 -4.0036173][-4.2437978 -4.3484106 -4.0242891 -3.0316217 -1.41816 -0.061272621 0.84205866 1.37708 0.97615623 -0.46020818 -2.0447907 -3.448894 -4.1696239 -4.3817997 -4.2346926][-4.9387126 -5.2969689 -5.0467873 -4.5395741 -3.6047974 -2.480227 -1.6118913 -1.1466281 -1.4045589 -2.2660353 -3.0957575 -3.9513447 -4.39702 -4.19894 -3.7600014][-4.987535 -5.5568233 -5.8367753 -5.4824572 -4.7928686 -3.9958296 -3.532269 -3.2071462 -3.1818466 -3.5335665 -4.0310659 -4.4891486 -4.6108356 -4.2059922 -3.6240363][-4.7601337 -5.0846891 -5.2716422 -5.1631165 -4.9015732 -4.3597889 -3.883013 -3.863173 -3.9480836 -4.0992823 -4.3015447 -4.5262766 -4.5225639 -4.1734333 -3.71952][-4.5403686 -4.5640626 -4.6283226 -4.50702 -4.3344765 -3.9936709 -3.778405 -3.7564871 -3.6701241 -3.873714 -3.9768164 -4.0335932 -4.0889044 -3.9566751 -3.7370706][-3.7123458 -3.4458153 -3.328054 -3.1993041 -3.2558923 -3.160543 -2.9999042 -3.0317736 -3.1855152 -3.4973133 -3.6508348 -3.7391069 -3.8489075 -3.8743234 -3.8404961]]...]
INFO - root - 2017-12-16 09:06:58.196750: step 68410, loss = 0.29, batch loss = 0.23 (28.1 examples/sec; 0.285 sec/batch; 20h:53m:42s remains)
INFO - root - 2017-12-16 09:07:01.030050: step 68420, loss = 0.26, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 20h:48m:39s remains)
INFO - root - 2017-12-16 09:07:03.895544: step 68430, loss = 0.29, batch loss = 0.23 (28.6 examples/sec; 0.280 sec/batch; 20h:31m:02s remains)
INFO - root - 2017-12-16 09:07:06.776665: step 68440, loss = 0.30, batch loss = 0.24 (26.8 examples/sec; 0.298 sec/batch; 21h:53m:37s remains)
INFO - root - 2017-12-16 09:07:09.616021: step 68450, loss = 0.25, batch loss = 0.19 (28.8 examples/sec; 0.278 sec/batch; 20h:21m:59s remains)
INFO - root - 2017-12-16 09:07:12.509592: step 68460, loss = 0.20, batch loss = 0.14 (28.1 examples/sec; 0.285 sec/batch; 20h:52m:52s remains)
INFO - root - 2017-12-16 09:07:15.357714: step 68470, loss = 0.32, batch loss = 0.27 (26.7 examples/sec; 0.300 sec/batch; 21h:58m:11s remains)
INFO - root - 2017-12-16 09:07:18.201946: step 68480, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.282 sec/batch; 20h:39m:01s remains)
INFO - root - 2017-12-16 09:07:21.041089: step 68490, loss = 0.24, batch loss = 0.18 (28.6 examples/sec; 0.280 sec/batch; 20h:31m:22s remains)
INFO - root - 2017-12-16 09:07:23.870285: step 68500, loss = 0.30, batch loss = 0.24 (27.1 examples/sec; 0.295 sec/batch; 21h:36m:42s remains)
2017-12-16 09:07:24.440160: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7285831 -2.9797385 -3.2092271 -3.2565017 -3.2179034 -3.1707988 -3.3078203 -3.4694326 -3.7528362 -3.9212766 -3.884212 -4.0536265 -4.1791568 -4.1385627 -4.3596253][-2.8717391 -3.0196064 -3.1051812 -3.2257524 -3.2590828 -3.1940851 -3.24512 -3.4739044 -3.8857195 -4.128788 -4.122436 -4.2216687 -4.2594891 -4.1733322 -4.2231584][-3.1301284 -3.1951814 -3.1093483 -2.9903755 -2.8772836 -2.8060386 -2.9368825 -3.2690506 -3.6569016 -3.941174 -4.0123057 -4.2083449 -4.3574762 -4.3750477 -4.4192948][-3.5149651 -3.1059203 -2.6380887 -2.3962648 -2.1882582 -2.1179643 -2.3359857 -2.6043 -2.9746687 -3.3960614 -3.43268 -3.4948359 -3.5926623 -3.8067958 -4.1393342][-3.4015682 -2.5537705 -1.7222779 -1.2366505 -0.84821105 -0.80016661 -1.0825531 -1.5541701 -1.9415729 -2.1447508 -2.1095159 -2.1213105 -1.9495001 -2.0759656 -2.56958][-2.799695 -1.6116271 -0.55542135 0.11560822 0.60491037 0.59446859 0.32668543 -0.1098547 -0.64003348 -0.94028282 -0.84389663 -0.59762669 -0.24266624 -0.5042243 -1.1743][-1.8628273 -0.71955276 0.28738356 1.1473775 1.7714396 2.0087638 1.8089194 1.3602042 0.89638281 0.41657782 0.28188467 0.45673418 0.77138138 0.37208796 -0.535321][-0.94483471 -0.029066086 0.7329998 1.5920496 2.2486935 2.6448512 2.7273684 2.4876847 2.1517286 1.6436343 1.4809756 1.5693955 1.6967969 0.94962788 -0.52096939][-0.28964996 0.25370741 0.73885632 1.325747 1.842711 2.3249326 2.5455542 2.5406919 2.3282948 1.9240336 1.7658916 1.6904268 1.6727843 0.82554626 -0.65919447][-0.44546986 -0.41111708 -0.19892883 0.32633257 0.84963274 1.3353519 1.5078154 1.6199169 1.3782949 0.93681 0.72005844 0.66087294 0.64459181 -0.11396599 -1.4434235][-0.6404171 -0.8963933 -1.0120311 -0.73498654 -0.45618272 0.038972378 0.33786678 0.40036774 0.12051725 -0.41126585 -0.82568932 -1.0937269 -1.3352587 -1.9305182 -2.7789445][-0.83647466 -1.3814299 -1.9585001 -1.9606142 -1.8105104 -1.4862316 -1.2190099 -1.1293385 -1.2623982 -1.7764308 -2.2501736 -2.6208549 -2.9186363 -3.425724 -4.1249857][-1.2667441 -1.9791343 -2.5775745 -2.9397111 -3.1353445 -2.8930323 -2.7781191 -2.6968923 -2.8201661 -3.1798828 -3.4796188 -3.7026241 -3.9015822 -4.2422004 -4.63152][-1.3684273 -1.9551907 -2.5831952 -3.0818381 -3.5399644 -3.6698802 -3.8252 -3.9338038 -4.1071825 -4.3809218 -4.609478 -4.6960688 -4.6839538 -4.8026161 -4.8366995][-1.566793 -1.9038484 -2.4808645 -3.0076003 -3.4475658 -3.7104304 -4.0176759 -4.3286209 -4.5599194 -4.8262672 -4.8747191 -4.7547674 -4.5345206 -4.4550471 -4.4028168]]...]
INFO - root - 2017-12-16 09:07:27.286474: step 68510, loss = 0.23, batch loss = 0.17 (28.1 examples/sec; 0.285 sec/batch; 20h:51m:55s remains)
INFO - root - 2017-12-16 09:07:30.122978: step 68520, loss = 0.38, batch loss = 0.32 (27.9 examples/sec; 0.286 sec/batch; 21h:00m:04s remains)
INFO - root - 2017-12-16 09:07:32.924423: step 68530, loss = 0.29, batch loss = 0.23 (29.4 examples/sec; 0.272 sec/batch; 19h:55m:18s remains)
INFO - root - 2017-12-16 09:07:35.808245: step 68540, loss = 0.26, batch loss = 0.21 (27.1 examples/sec; 0.295 sec/batch; 21h:39m:37s remains)
INFO - root - 2017-12-16 09:07:38.682504: step 68550, loss = 0.35, batch loss = 0.29 (28.0 examples/sec; 0.286 sec/batch; 20h:58m:36s remains)
INFO - root - 2017-12-16 09:07:41.563897: step 68560, loss = 0.29, batch loss = 0.23 (27.1 examples/sec; 0.295 sec/batch; 21h:38m:18s remains)
INFO - root - 2017-12-16 09:07:44.433307: step 68570, loss = 0.42, batch loss = 0.36 (27.4 examples/sec; 0.292 sec/batch; 21h:22m:37s remains)
INFO - root - 2017-12-16 09:07:47.247234: step 68580, loss = 0.32, batch loss = 0.27 (29.0 examples/sec; 0.276 sec/batch; 20h:12m:21s remains)
INFO - root - 2017-12-16 09:07:50.097625: step 68590, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.286 sec/batch; 20h:56m:45s remains)
INFO - root - 2017-12-16 09:07:52.924241: step 68600, loss = 0.27, batch loss = 0.21 (27.4 examples/sec; 0.292 sec/batch; 21h:22m:20s remains)
2017-12-16 09:07:53.510138: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.58662772 -1.0604806 -1.6519296 -2.099396 -2.5172923 -2.774719 -3.0527358 -2.8962059 -2.6071053 -2.3267782 -2.1528039 -1.8183088 -1.2065158 -1.2625842 -1.6331174][-0.75323415 -1.0682278 -1.5178444 -2.0506351 -2.3681452 -2.4204502 -2.5081463 -2.5663853 -2.7135043 -2.4566231 -2.4113252 -2.2513816 -2.0204568 -1.9655123 -2.3340015][-1.530741 -1.7380621 -2.0312834 -2.2511704 -2.2143202 -2.1027937 -2.0122766 -2.1884511 -2.4617071 -2.7669659 -3.1641493 -3.280741 -3.237206 -3.3482831 -3.6663852][-2.3329148 -2.5177002 -2.7499003 -2.5632927 -2.0951085 -1.6042066 -1.3311119 -1.5272765 -2.0229337 -2.4958029 -3.1712036 -3.8519218 -4.316308 -4.5763631 -4.6812911][-3.0897033 -3.1478996 -3.10311 -2.5448322 -1.7706888 -0.995384 -0.42811537 -0.36278677 -0.89646077 -1.8761141 -2.9764628 -3.8276749 -4.5676751 -5.1291413 -5.2278934][-3.7064521 -3.6765261 -3.2679143 -2.3967035 -1.2550313 0.016181946 1.0754275 1.3577719 0.81786728 -0.25884676 -1.6607916 -3.1461878 -4.3726721 -5.1177421 -5.3316312][-3.5754125 -3.4444757 -2.989094 -1.9956 -0.67458081 0.85971689 2.0285788 2.502573 2.0931425 0.83984852 -0.92002249 -2.6788344 -4.1659012 -5.1157069 -5.5139933][-3.2274485 -3.1321416 -2.7332299 -1.6163085 -0.19727659 1.3444958 2.4590578 2.8742003 2.4365983 1.1523366 -0.57140255 -2.4722416 -4.1635427 -5.3725977 -5.9462004][-3.2675536 -3.1590242 -2.6529403 -1.7190931 -0.51256418 0.87293625 2.0171556 2.6507931 2.3815069 1.2572694 -0.39756632 -2.2561107 -3.9573047 -5.2856455 -5.887208][-4.2378607 -3.9940469 -3.2765379 -2.3383586 -1.3794696 -0.20932865 0.88769388 1.5891719 1.5181804 0.7745285 -0.54799485 -2.2954044 -3.92873 -5.0323539 -5.5243688][-5.1582952 -5.082643 -4.5981207 -3.6300702 -2.544426 -1.4270985 -0.42370176 0.44185972 0.70076275 0.28040648 -0.73821545 -2.1479442 -3.5480287 -4.3894792 -4.5991368][-5.9322329 -6.0416555 -5.7267742 -5.0115428 -3.9300625 -2.7079468 -1.6467569 -0.79364681 -0.46047616 -0.55912566 -1.292695 -2.3352773 -3.2247248 -3.8311062 -3.8914235][-6.7567797 -6.8787889 -6.5276189 -5.7608595 -4.6070623 -3.364774 -2.1971595 -1.2680578 -0.86935949 -0.88335252 -1.3382795 -2.0072429 -2.6189113 -2.8583398 -2.7357287][-6.9477525 -7.1919527 -6.76835 -5.8626022 -4.5074129 -3.0083106 -1.6376364 -0.76953197 -0.34853172 -0.30991697 -0.75867057 -1.2427223 -1.6664181 -1.6147296 -1.2287424][-6.12883 -6.6053658 -6.3477697 -5.3279781 -3.8650949 -2.3022771 -0.90007925 -0.0837121 0.14836168 0.16073513 -0.15725899 -0.77980137 -1.1992481 -1.2632692 -0.965657]]...]
INFO - root - 2017-12-16 09:07:56.383829: step 68610, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.285 sec/batch; 20h:53m:33s remains)
INFO - root - 2017-12-16 09:07:59.275078: step 68620, loss = 0.24, batch loss = 0.18 (26.6 examples/sec; 0.301 sec/batch; 22h:02m:32s remains)
INFO - root - 2017-12-16 09:08:02.100911: step 68630, loss = 0.25, batch loss = 0.19 (28.8 examples/sec; 0.278 sec/batch; 20h:23m:35s remains)
INFO - root - 2017-12-16 09:08:04.940072: step 68640, loss = 0.33, batch loss = 0.27 (28.0 examples/sec; 0.285 sec/batch; 20h:54m:33s remains)
INFO - root - 2017-12-16 09:08:07.751488: step 68650, loss = 0.20, batch loss = 0.14 (28.6 examples/sec; 0.279 sec/batch; 20h:28m:31s remains)
INFO - root - 2017-12-16 09:08:10.587384: step 68660, loss = 0.25, batch loss = 0.19 (28.6 examples/sec; 0.280 sec/batch; 20h:29m:05s remains)
INFO - root - 2017-12-16 09:08:13.426493: step 68670, loss = 0.37, batch loss = 0.31 (27.1 examples/sec; 0.295 sec/batch; 21h:36m:50s remains)
INFO - root - 2017-12-16 09:08:16.280093: step 68680, loss = 0.30, batch loss = 0.24 (28.3 examples/sec; 0.282 sec/batch; 20h:41m:55s remains)
INFO - root - 2017-12-16 09:08:19.091083: step 68690, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 20h:30m:58s remains)
INFO - root - 2017-12-16 09:08:21.926066: step 68700, loss = 0.27, batch loss = 0.22 (27.5 examples/sec; 0.291 sec/batch; 21h:20m:06s remains)
2017-12-16 09:08:22.474279: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4044683 -3.8814127 -4.4859805 -5.0209217 -5.4189186 -5.9292641 -6.4732404 -7.0208993 -7.2153225 -7.1134591 -6.8287649 -6.2383046 -5.447165 -4.6578641 -4.2743688][-3.8072832 -4.487586 -5.1288662 -5.6578412 -5.9796581 -6.3453393 -6.8205271 -7.3113708 -7.4914017 -7.4391284 -7.2651792 -6.7477865 -5.896699 -4.8822503 -4.2916441][-4.2705345 -5.0036659 -5.5758433 -5.8633618 -5.8014307 -5.793468 -5.9660029 -6.3208489 -6.6066518 -6.7523088 -6.7812257 -6.4241085 -5.6915631 -4.8359327 -4.1835704][-4.5618072 -5.3382211 -5.8765945 -5.9024181 -5.4468346 -4.7669859 -4.357532 -4.2694335 -4.5114813 -4.9344554 -5.3552251 -5.5086865 -5.1741037 -4.6109171 -4.17117][-4.4926343 -5.1148319 -5.2899985 -4.8242354 -3.827683 -2.6743956 -1.8942478 -1.605303 -1.969533 -2.723557 -3.473561 -3.9900942 -4.0869656 -4.0094495 -3.8656425][-4.3440366 -4.7384791 -4.6159077 -3.5844555 -1.9970238 -0.31079435 0.96599054 1.4655352 1.0540762 -0.16899395 -1.5021994 -2.4552355 -2.9150705 -3.1395111 -3.0320053][-4.2251945 -4.1990037 -3.8429337 -2.6239474 -0.75015664 1.2456121 2.847537 3.4416833 2.9506698 1.6948166 0.33463573 -0.87861371 -1.5772634 -1.9207888 -1.8800452][-3.8305206 -3.7666841 -3.2904606 -1.8564796 0.0045995712 1.9662237 3.6173725 4.12099 3.673727 2.3513622 1.096612 0.21556091 -0.23847008 -0.39492273 -0.15553474][-3.5159497 -3.3735113 -2.9395328 -1.8233685 -0.4392848 1.2641759 2.7290449 3.2421045 2.9958582 1.977201 1.1125584 0.56275463 0.47237015 0.59368277 1.1294918][-3.6148567 -3.7083583 -3.6594808 -2.9919715 -2.0418692 -0.76615357 0.31223249 0.91399384 1.0432858 0.669487 0.5001297 0.55981159 0.98897171 1.3483596 1.902812][-4.0572872 -4.454186 -4.8010521 -4.6750932 -4.2916036 -3.4727106 -2.6967471 -2.0186265 -1.5478537 -1.2552009 -0.62720251 0.0046424866 0.77049685 1.308898 1.9232883][-4.505167 -5.1835494 -5.9518905 -6.3876014 -6.5039153 -6.0646424 -5.6246381 -5.029285 -4.330565 -3.4308167 -2.28472 -1.2892957 -0.27153444 0.27035332 0.76416016][-4.9168186 -5.790904 -6.7363958 -7.4497614 -7.9508791 -8.031661 -7.9276438 -7.4409676 -6.7569494 -5.684762 -4.3239474 -3.2080641 -2.3176584 -1.8915393 -1.3874781][-5.3542786 -6.1980209 -7.0156245 -7.7995853 -8.4803276 -8.79829 -8.8634558 -8.6586637 -8.1730595 -7.2494593 -6.110569 -5.0790768 -4.2859573 -3.9682913 -3.5372968][-5.3745918 -6.13558 -6.8435469 -7.5286837 -8.127533 -8.6345539 -8.9950829 -9.0764484 -8.7917976 -8.1791506 -7.3682666 -6.5466328 -5.9285 -5.5991597 -5.1629205]]...]
INFO - root - 2017-12-16 09:08:25.301089: step 68710, loss = 0.26, batch loss = 0.21 (28.4 examples/sec; 0.282 sec/batch; 20h:40m:02s remains)
INFO - root - 2017-12-16 09:08:28.148103: step 68720, loss = 0.31, batch loss = 0.25 (27.8 examples/sec; 0.288 sec/batch; 21h:05m:32s remains)
INFO - root - 2017-12-16 09:08:30.994187: step 68730, loss = 0.31, batch loss = 0.25 (28.3 examples/sec; 0.283 sec/batch; 20h:44m:48s remains)
INFO - root - 2017-12-16 09:08:33.857766: step 68740, loss = 0.22, batch loss = 0.16 (25.4 examples/sec; 0.315 sec/batch; 23h:03m:15s remains)
INFO - root - 2017-12-16 09:08:36.665224: step 68750, loss = 0.20, batch loss = 0.15 (28.8 examples/sec; 0.278 sec/batch; 20h:22m:33s remains)
INFO - root - 2017-12-16 09:08:39.485622: step 68760, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.286 sec/batch; 20h:57m:27s remains)
INFO - root - 2017-12-16 09:08:42.297093: step 68770, loss = 0.23, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 20h:05m:09s remains)
INFO - root - 2017-12-16 09:08:45.100837: step 68780, loss = 0.24, batch loss = 0.18 (29.1 examples/sec; 0.275 sec/batch; 20h:08m:43s remains)
INFO - root - 2017-12-16 09:08:47.945898: step 68790, loss = 0.27, batch loss = 0.21 (25.8 examples/sec; 0.310 sec/batch; 22h:43m:38s remains)
INFO - root - 2017-12-16 09:08:50.777039: step 68800, loss = 0.30, batch loss = 0.24 (27.8 examples/sec; 0.287 sec/batch; 21h:03m:27s remains)
2017-12-16 09:08:51.293392: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6508529 -3.642724 -3.6956763 -3.9836328 -3.9399848 -3.8858321 -3.6196604 -3.3966651 -3.3654041 -3.3547187 -3.304193 -2.9255729 -2.6072402 -2.399683 -2.2209084][-3.4067054 -3.2673016 -3.4510109 -3.6915898 -3.6476164 -3.5840778 -3.3203442 -3.1395764 -3.1113241 -3.0078626 -2.9426229 -2.6894288 -2.5231709 -2.2760987 -2.1032145][-2.7694702 -2.7418823 -2.963043 -3.3258247 -3.3941078 -3.2907658 -3.014822 -2.7756672 -2.6865768 -2.8202305 -2.9447956 -2.7880263 -2.7371936 -2.5954256 -2.3683856][-1.8909404 -1.9555786 -2.156523 -2.5376868 -2.5834756 -2.528933 -2.2349026 -2.04958 -2.152138 -2.3913057 -2.6501403 -2.869272 -2.9808006 -2.9117565 -2.7957897][-1.5265453 -1.2215273 -1.2841756 -1.5575562 -1.5382464 -1.295845 -0.86502957 -0.69944787 -0.95239711 -1.6675475 -2.3016453 -2.6608944 -2.9702003 -2.9815257 -2.7664366][-1.1316347 -0.67115831 -0.55292559 -0.47468853 -0.11304808 0.30929947 0.79916096 0.89799118 0.51136208 -0.52860951 -1.5935481 -2.3343716 -2.6621242 -2.8803911 -2.8686998][-1.1494491 -0.49249864 -0.063043118 0.25914335 0.93348503 1.7713246 2.5401621 2.6454306 2.0643573 0.73880768 -0.63734937 -1.9319048 -2.6103163 -2.4349003 -2.0625267][-1.4579012 -0.84862161 -0.39552021 0.20141029 1.1037741 2.1778483 3.0633616 3.2638388 2.6318579 1.1164646 -0.42080212 -1.6657503 -2.3566222 -2.4557865 -2.0780826][-2.0196154 -1.4368815 -1.0069575 -0.47178674 0.48544216 1.7132864 2.6643224 2.9237666 2.338026 0.82038832 -0.79682636 -2.1576898 -2.7792761 -2.7945089 -2.2264349][-2.2971144 -2.0384645 -1.951699 -1.6344242 -0.849632 0.28577137 1.1402755 1.4498682 0.97356939 -0.37940645 -1.9035726 -3.2875409 -3.8332152 -3.6760194 -2.8459489][-2.7151895 -2.6524796 -2.7565966 -2.69214 -2.2234471 -1.5084903 -1.0365183 -0.78903842 -1.2038352 -2.4092703 -3.7224913 -4.9431224 -5.4255338 -5.1090426 -4.0270433][-2.9752924 -2.8766823 -3.1846066 -3.5014219 -3.4872429 -2.885036 -2.6101708 -2.678359 -3.0988133 -4.2113857 -5.3801508 -6.4781485 -6.9503522 -6.6381836 -5.3597016][-3.1715667 -3.0288053 -3.3492208 -3.7184842 -3.9607918 -3.7662706 -3.8112593 -4.0370569 -4.5140953 -5.5163732 -6.5824842 -7.6622267 -8.0561237 -7.672884 -6.4187922][-2.9911766 -2.6824927 -2.8558435 -3.2772031 -3.6782336 -3.693836 -3.88294 -4.226378 -4.9096689 -5.9743338 -7.046351 -8.1640377 -8.6150408 -8.35532 -7.2766385][-2.8983169 -2.5216203 -2.5639477 -2.8617136 -3.2184386 -3.3044009 -3.5910025 -4.0560861 -4.6838322 -5.7501221 -6.9691162 -7.9357281 -8.4599295 -8.3185253 -7.3746729]]...]
INFO - root - 2017-12-16 09:08:54.106014: step 68810, loss = 0.30, batch loss = 0.24 (29.4 examples/sec; 0.272 sec/batch; 19h:56m:58s remains)
INFO - root - 2017-12-16 09:08:56.959917: step 68820, loss = 0.27, batch loss = 0.21 (26.4 examples/sec; 0.303 sec/batch; 22h:13m:27s remains)
INFO - root - 2017-12-16 09:08:59.740118: step 68830, loss = 0.32, batch loss = 0.26 (29.5 examples/sec; 0.272 sec/batch; 19h:53m:40s remains)
INFO - root - 2017-12-16 09:09:02.523373: step 68840, loss = 0.30, batch loss = 0.24 (28.0 examples/sec; 0.286 sec/batch; 20h:55m:21s remains)
INFO - root - 2017-12-16 09:09:05.306086: step 68850, loss = 0.43, batch loss = 0.37 (27.6 examples/sec; 0.290 sec/batch; 21h:12m:36s remains)
INFO - root - 2017-12-16 09:09:08.197074: step 68860, loss = 0.23, batch loss = 0.17 (27.7 examples/sec; 0.288 sec/batch; 21h:07m:13s remains)
INFO - root - 2017-12-16 09:09:11.064095: step 68870, loss = 0.30, batch loss = 0.24 (27.3 examples/sec; 0.293 sec/batch; 21h:29m:29s remains)
INFO - root - 2017-12-16 09:09:13.945737: step 68880, loss = 0.26, batch loss = 0.20 (27.1 examples/sec; 0.296 sec/batch; 21h:38m:54s remains)
INFO - root - 2017-12-16 09:09:16.826691: step 68890, loss = 0.22, batch loss = 0.16 (26.9 examples/sec; 0.297 sec/batch; 21h:46m:43s remains)
INFO - root - 2017-12-16 09:09:19.706014: step 68900, loss = 0.35, batch loss = 0.29 (27.3 examples/sec; 0.293 sec/batch; 21h:25m:45s remains)
2017-12-16 09:09:20.192087: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.134944 -4.598577 -4.9924583 -5.462142 -5.9101152 -6.0120125 -6.1975632 -6.1202927 -6.1573071 -5.9423542 -5.7321653 -5.593946 -5.3132663 -5.0192595 -4.6407523][-3.2016587 -3.7660656 -4.1849589 -4.5742288 -5.001296 -5.182044 -5.2934189 -5.2136035 -5.2921472 -5.1765676 -5.0768113 -4.9379253 -4.6251082 -4.229938 -3.8157356][-2.9056325 -3.3683939 -3.7693462 -4.0359378 -4.1699095 -4.0865269 -4.131856 -4.1535287 -4.2738771 -4.3713932 -4.5427747 -4.4366145 -4.1290741 -3.8782706 -3.4350557][-3.5202777 -3.8536978 -3.9237452 -3.6986151 -3.2409844 -2.7130187 -2.3695281 -2.240608 -2.4106469 -2.875385 -3.3979764 -3.7060962 -3.7661448 -3.6246617 -3.3372891][-4.0685749 -4.3441777 -4.187407 -3.4140754 -2.183176 -0.86032724 0.16335535 0.44794989 0.21423388 -0.647012 -1.7333965 -2.7059748 -3.3433514 -3.4278097 -3.1924977][-4.7887669 -4.8342757 -4.2421093 -2.8399398 -0.84708166 1.2734122 2.8889575 3.4387078 3.0873475 1.5813937 -0.29539108 -1.9117279 -3.0249705 -3.6088958 -3.4855909][-5.2285109 -4.9024558 -4.019403 -2.1486442 0.3111577 2.967742 5.1312714 5.9216261 5.4842129 3.7866306 1.55333 -0.89086366 -2.7834277 -3.700582 -3.6869884][-4.8234754 -4.54016 -3.6024389 -1.5866072 1.0432591 3.9960213 6.3777952 7.2066879 6.604207 4.6752281 2.1407042 -0.56962967 -2.5082402 -3.5018725 -3.4897876][-4.5603662 -4.198256 -3.3238354 -1.870132 0.26080847 3.018939 5.3224926 6.3592463 5.872447 3.8994284 1.3242888 -1.3350048 -3.3264909 -4.0131679 -3.6240673][-4.220037 -4.2125797 -3.8638 -2.8079402 -1.2219131 0.59268427 2.3084464 3.4021974 3.0412579 1.6463895 -0.31291294 -2.6637311 -4.39682 -4.9845 -4.4525437][-4.1690207 -4.3441029 -4.3378453 -4.0128717 -3.2604249 -2.1029377 -0.91179943 -0.3768568 -0.73981857 -1.6796248 -3.1360865 -4.385035 -5.2668772 -5.4389162 -4.7693481][-3.9575 -4.3278708 -4.614985 -4.653583 -4.0466681 -3.6893122 -3.5067282 -3.198288 -3.4562907 -4.1261363 -5.098465 -5.8301058 -6.2347479 -6.1628332 -5.5331473][-3.2135074 -3.4771407 -3.927804 -4.4988441 -4.5004826 -4.3235083 -4.2511272 -4.3152781 -4.499927 -4.858727 -5.4070272 -5.7040815 -6.027133 -6.3375263 -6.1382866][-2.9688389 -2.9013219 -2.9674351 -3.5053384 -3.8351662 -4.1707764 -4.3894968 -4.470562 -4.5142055 -4.4855294 -4.5059428 -4.7578497 -5.2501349 -5.8039989 -6.1567669][-2.1276989 -1.9445219 -1.884788 -2.2223225 -2.578793 -3.0570092 -3.3945804 -3.5038633 -3.3413391 -3.245631 -3.2803621 -3.1464992 -3.4221106 -4.2534165 -5.0795164]]...]
INFO - root - 2017-12-16 09:09:23.050763: step 68910, loss = 0.29, batch loss = 0.23 (26.7 examples/sec; 0.300 sec/batch; 21h:56m:41s remains)
INFO - root - 2017-12-16 09:09:25.909746: step 68920, loss = 0.28, batch loss = 0.22 (27.3 examples/sec; 0.294 sec/batch; 21h:29m:32s remains)
INFO - root - 2017-12-16 09:09:28.744683: step 68930, loss = 0.30, batch loss = 0.24 (26.6 examples/sec; 0.301 sec/batch; 22h:02m:11s remains)
INFO - root - 2017-12-16 09:09:31.548430: step 68940, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 20h:20m:02s remains)
INFO - root - 2017-12-16 09:09:34.399843: step 68950, loss = 0.30, batch loss = 0.24 (28.6 examples/sec; 0.279 sec/batch; 20h:27m:03s remains)
INFO - root - 2017-12-16 09:09:37.309641: step 68960, loss = 0.27, batch loss = 0.22 (28.7 examples/sec; 0.279 sec/batch; 20h:23m:46s remains)
INFO - root - 2017-12-16 09:09:40.159400: step 68970, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 20h:19m:37s remains)
INFO - root - 2017-12-16 09:09:42.985519: step 68980, loss = 0.21, batch loss = 0.15 (28.0 examples/sec; 0.286 sec/batch; 20h:55m:01s remains)
INFO - root - 2017-12-16 09:09:45.850529: step 68990, loss = 0.28, batch loss = 0.22 (28.7 examples/sec; 0.278 sec/batch; 20h:22m:41s remains)
INFO - root - 2017-12-16 09:09:48.660896: step 69000, loss = 0.27, batch loss = 0.21 (27.4 examples/sec; 0.292 sec/batch; 21h:23m:21s remains)
2017-12-16 09:09:49.161998: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6472907 -4.5172944 -4.3802481 -4.211432 -4.2505794 -4.4601965 -4.7849274 -4.8821692 -4.8978415 -4.720397 -4.5460391 -4.2788768 -4.0865536 -3.8184607 -3.372333][-5.6169205 -5.4369707 -5.1973171 -5.3090792 -5.5848794 -5.7930069 -5.95007 -5.9555612 -5.8808718 -5.6575837 -5.5208216 -5.2397633 -5.0398283 -4.803391 -4.3079891][-6.5775361 -6.6032095 -6.5487032 -6.53885 -6.5722208 -6.650672 -6.6416111 -6.4784727 -6.2100472 -5.7937078 -5.5087304 -5.4023108 -5.5050116 -5.4571314 -5.0298882][-7.7380538 -7.7844257 -7.6442175 -7.533164 -7.3802338 -7.0536327 -6.5154161 -5.6846871 -5.0175238 -4.5935831 -4.6805725 -4.9452152 -5.3936729 -5.5726728 -5.4305115][-7.8448033 -7.9885206 -7.8642578 -7.4970331 -6.761282 -5.7803984 -4.6795292 -3.7374122 -3.0915048 -2.7787426 -2.9975967 -3.7864244 -4.8430271 -5.4064426 -5.2794175][-6.9211082 -7.1371183 -6.9152856 -6.2902188 -5.1622133 -3.5626786 -1.8260455 -0.3024621 0.43198395 0.343956 -0.7643652 -2.0709612 -3.1624491 -4.2049427 -4.5460467][-5.7286377 -5.8461356 -5.5702581 -4.7548575 -3.2166247 -1.3891184 0.65047836 2.2368045 3.0531301 2.7873082 1.4970102 -0.32734823 -2.0508509 -2.9133797 -2.9277287][-4.6985979 -4.7216082 -4.3480153 -3.383275 -1.7016463 0.45436096 2.5233493 3.9058771 4.3586159 3.46737 1.7682371 0.29690886 -0.7515285 -1.5284739 -1.6404793][-3.1618104 -3.4266243 -3.4252691 -2.6638362 -1.1412911 0.76860237 2.6212931 3.8879747 4.1681719 3.2023435 1.4939432 -0.085543156 -0.657367 -0.5510385 -0.25482798][-2.0677605 -2.5473037 -2.8445129 -2.5315437 -1.2899652 0.33435249 1.7134862 2.641696 2.6629062 1.7082129 0.35864258 -0.69357014 -0.74295092 -0.10823822 0.62409353][-1.3305106 -1.9761178 -2.5009315 -2.7585688 -2.1964245 -0.93767738 0.2678504 0.82893229 0.56233597 -0.12658215 -1.0081394 -1.3087008 -0.72460032 0.065825939 0.82105255][-0.98673534 -1.7498908 -2.4888945 -3.0854397 -2.7706285 -2.2431164 -1.8183231 -1.4222023 -1.5361881 -1.895257 -2.5425885 -2.5699015 -1.795553 -0.71075964 -0.030160427][-0.44675612 -1.158844 -2.2009182 -3.2293544 -3.582418 -3.1973987 -2.5804586 -2.525326 -2.960134 -3.4475012 -3.9232213 -3.8185315 -3.0893092 -2.2579014 -1.7995684][0.038626671 -0.54526234 -1.7198372 -3.0446782 -3.6606688 -3.7390089 -3.6600556 -3.4415896 -3.5836718 -3.9963143 -4.3943157 -4.3700094 -4.1853237 -3.8140378 -3.4829698][0.3411684 -0.26488972 -1.4502332 -2.8177283 -3.6617095 -4.0217676 -4.0082107 -3.9730904 -4.1134109 -4.4208236 -4.7875342 -4.7472134 -4.376965 -4.1528749 -4.1773305]]...]
INFO - root - 2017-12-16 09:09:52.049015: step 69010, loss = 0.28, batch loss = 0.22 (27.5 examples/sec; 0.291 sec/batch; 21h:19m:02s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 09:09:54.888365: step 69020, loss = 0.27, batch loss = 0.22 (29.2 examples/sec; 0.274 sec/batch; 20h:01m:33s remains)
INFO - root - 2017-12-16 09:09:57.727635: step 69030, loss = 0.25, batch loss = 0.19 (29.1 examples/sec; 0.275 sec/batch; 20h:09m:15s remains)
INFO - root - 2017-12-16 09:10:00.522666: step 69040, loss = 0.40, batch loss = 0.34 (28.6 examples/sec; 0.280 sec/batch; 20h:29m:13s remains)
INFO - root - 2017-12-16 09:10:03.358116: step 69050, loss = 0.25, batch loss = 0.19 (27.4 examples/sec; 0.292 sec/batch; 21h:22m:13s remains)
INFO - root - 2017-12-16 09:10:06.139791: step 69060, loss = 0.38, batch loss = 0.32 (29.2 examples/sec; 0.274 sec/batch; 20h:01m:00s remains)
INFO - root - 2017-12-16 09:10:08.957828: step 69070, loss = 0.22, batch loss = 0.16 (27.0 examples/sec; 0.296 sec/batch; 21h:39m:20s remains)
INFO - root - 2017-12-16 09:10:11.759844: step 69080, loss = 0.22, batch loss = 0.16 (27.8 examples/sec; 0.287 sec/batch; 21h:01m:34s remains)
INFO - root - 2017-12-16 09:10:14.637232: step 69090, loss = 0.20, batch loss = 0.14 (28.2 examples/sec; 0.284 sec/batch; 20h:46m:03s remains)
INFO - root - 2017-12-16 09:10:17.500354: step 69100, loss = 0.43, batch loss = 0.37 (28.9 examples/sec; 0.277 sec/batch; 20h:16m:49s remains)
2017-12-16 09:10:18.044993: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6158729 -4.9572597 -5.1204414 -5.1739469 -5.1471658 -4.9565353 -4.7921472 -4.8552661 -5.0425797 -5.2569532 -5.5391917 -5.7786355 -5.8371334 -5.5969658 -5.223402][-4.7772393 -5.1786637 -5.2765265 -5.2331305 -5.0961251 -5.0453978 -5.1331344 -5.2152009 -5.4069743 -5.7586751 -6.1439185 -6.4605761 -6.5841894 -6.3858304 -6.0544095][-4.8965092 -5.2049518 -5.1164813 -4.8815169 -4.5558558 -4.2926097 -4.2841244 -4.6441236 -5.1148429 -5.5526638 -5.980278 -6.472785 -6.9102983 -6.9924321 -6.9000053][-4.8639627 -4.926506 -4.5220561 -3.9622016 -3.3744574 -2.8892381 -2.6633439 -2.8828862 -3.3493857 -4.2158542 -5.0952029 -5.8526797 -6.5573874 -7.1303186 -7.4285874][-4.9147496 -4.5362649 -3.6278129 -2.6891024 -1.7271004 -0.91494989 -0.40158606 -0.37364626 -0.69848251 -1.5656829 -2.7081704 -4.26713 -5.7207446 -6.8745356 -7.6233983][-4.8834643 -4.2271557 -3.0302367 -1.6934235 -0.19217253 1.0208755 1.9468226 2.2882962 2.0342855 1.1561337 -0.12180281 -2.2159748 -4.4559269 -6.4239192 -7.5282116][-5.0282822 -4.2611876 -2.8566616 -1.1315417 0.73620844 2.3989549 3.7005901 4.242959 4.0384235 2.9920549 1.4484286 -0.90486836 -3.3924155 -5.736475 -7.1197443][-5.4986186 -4.3565803 -2.7671883 -1.1209133 0.79050159 2.7103415 4.3155937 4.8921309 4.7329445 3.4933305 1.7944441 -0.77927375 -3.2756562 -5.2908158 -6.4205437][-5.9852457 -4.9859891 -3.5164165 -1.8363433 0.0507226 1.8304453 3.3388462 3.9952307 3.8297911 2.6875467 1.0847158 -1.4318078 -3.9898291 -5.7973061 -6.5243864][-6.5841188 -5.8086772 -4.6167383 -3.1519198 -1.5388474 0.21009636 1.642807 2.1518769 1.9106846 0.87821436 -0.52710128 -2.7617908 -4.943295 -6.5062752 -7.1311226][-6.3490562 -5.8788786 -5.0750475 -3.9389086 -2.8982413 -1.5050213 -0.24810934 0.21936798 -0.15096664 -1.2352829 -2.4577198 -4.1552505 -5.6201925 -6.8501205 -7.4660196][-6.1067262 -5.460228 -4.8077931 -4.1628885 -3.5938525 -2.7659574 -2.077914 -1.8374021 -2.0320804 -2.7266779 -3.7738745 -5.2859139 -6.5087028 -7.1849718 -7.479187][-5.3232021 -4.932056 -4.4364552 -3.9121766 -3.5825315 -3.2164679 -2.9122047 -2.8513703 -2.9780903 -3.4392424 -4.1868038 -5.3038559 -6.2790761 -6.983429 -7.2640896][-4.0926175 -3.9141026 -3.733892 -3.4689724 -3.2819908 -2.9507375 -2.83613 -2.9874415 -3.3018122 -3.603333 -3.9187381 -4.6354513 -5.2895761 -5.7017574 -5.8737984][-3.2053554 -2.8624554 -2.6176019 -2.6702609 -2.8037875 -2.6658721 -2.7091355 -2.7799721 -2.9337916 -3.2144785 -3.5090964 -3.7961066 -3.9693882 -4.2398086 -4.4007626]]...]
INFO - root - 2017-12-16 09:10:20.888533: step 69110, loss = 0.20, batch loss = 0.14 (28.8 examples/sec; 0.277 sec/batch; 20h:17m:34s remains)
INFO - root - 2017-12-16 09:10:23.786249: step 69120, loss = 0.25, batch loss = 0.19 (27.7 examples/sec; 0.289 sec/batch; 21h:08m:11s remains)
INFO - root - 2017-12-16 09:10:26.600037: step 69130, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 20h:54m:02s remains)
INFO - root - 2017-12-16 09:10:29.407986: step 69140, loss = 0.32, batch loss = 0.26 (29.3 examples/sec; 0.273 sec/batch; 19h:57m:18s remains)
INFO - root - 2017-12-16 09:10:32.262150: step 69150, loss = 0.42, batch loss = 0.36 (29.3 examples/sec; 0.273 sec/batch; 19h:57m:18s remains)
INFO - root - 2017-12-16 09:10:35.091468: step 69160, loss = 0.23, batch loss = 0.17 (27.5 examples/sec; 0.291 sec/batch; 21h:16m:15s remains)
INFO - root - 2017-12-16 09:10:37.957277: step 69170, loss = 0.32, batch loss = 0.26 (26.8 examples/sec; 0.298 sec/batch; 21h:48m:28s remains)
INFO - root - 2017-12-16 09:10:40.765524: step 69180, loss = 0.27, batch loss = 0.21 (28.3 examples/sec; 0.282 sec/batch; 20h:38m:55s remains)
INFO - root - 2017-12-16 09:10:43.570722: step 69190, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.280 sec/batch; 20h:30m:30s remains)
INFO - root - 2017-12-16 09:10:46.421658: step 69200, loss = 0.26, batch loss = 0.20 (27.8 examples/sec; 0.287 sec/batch; 21h:00m:40s remains)
2017-12-16 09:10:46.935161: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9978659 -3.9419127 -3.8662145 -3.7073293 -3.5186353 -3.3274248 -3.1620202 -3.0635257 -3.0692034 -3.121098 -3.0896368 -2.9634423 -2.801554 -2.567874 -2.3643091][-3.7764282 -3.8546009 -3.9715593 -4.027267 -3.8825185 -3.6702852 -3.4624553 -3.2229431 -3.1165433 -3.1792445 -3.2752931 -3.1708093 -2.9702964 -2.7953238 -2.6451979][-3.629786 -3.8253071 -4.0664186 -4.1858439 -4.083549 -3.79893 -3.2660236 -2.8836243 -2.8440721 -2.9486997 -3.1133847 -3.2102349 -3.2195644 -3.1991591 -3.1165626][-3.0281169 -3.5121033 -4.0234604 -4.0006618 -3.6723359 -3.2668233 -2.6976433 -2.2117624 -1.9366374 -2.2220144 -2.7079482 -3.0814252 -3.3265417 -3.4645164 -3.4966989][-2.30674 -2.7968726 -3.2949748 -3.3235908 -2.8836253 -2.1523192 -1.3068287 -0.74760509 -0.6076715 -1.2849123 -2.1444163 -2.8188128 -3.2929363 -3.5771329 -3.625514][-1.9683268 -2.326695 -2.5923033 -2.3166816 -1.4980242 -0.52825069 0.45082378 1.0855641 1.1174335 0.27580452 -0.88798761 -1.9833548 -2.72788 -3.2304273 -3.4477863][-1.9841521 -2.0604258 -2.0793459 -1.4671583 -0.41947651 0.68554974 1.8741693 2.684485 2.650795 1.4890471 0.0678854 -1.1683819 -2.0581062 -2.6340575 -2.7624097][-2.165714 -2.0214086 -1.9212 -1.2647507 -0.16929579 1.0821357 2.3849816 3.0723338 3.0217218 1.8592811 0.36012793 -0.93026876 -1.8468087 -2.3638244 -2.4054937][-2.4463758 -2.2981303 -2.2930882 -1.8491716 -1.0972834 0.032543659 1.4486609 2.2023993 2.1902413 1.1918573 -0.11531162 -1.3642831 -2.0514021 -2.1928039 -1.9286156][-2.7411718 -2.8581629 -3.1027431 -2.9443891 -2.4797282 -1.6939919 -0.70685768 0.020577908 0.2179327 -0.56259584 -1.6447809 -2.6133957 -2.9770956 -2.7747736 -2.1558189][-3.1209583 -3.3975039 -3.8303494 -4.128242 -4.1853786 -3.7155027 -3.0207477 -2.5335326 -2.4976673 -3.183435 -3.9697268 -4.5441189 -4.6742873 -4.170577 -3.1863589][-3.2319133 -3.6817195 -4.4887562 -5.0631142 -5.4238563 -5.431797 -5.184123 -4.9935131 -5.0299129 -5.5315971 -6.0886703 -6.3572168 -6.1823459 -5.4472432 -4.2598844][-3.2561588 -3.7034779 -4.4761963 -5.1798539 -5.831182 -6.1807575 -6.3150997 -6.4709034 -6.7267933 -7.0590763 -7.2168231 -7.21019 -6.9003906 -6.1983013 -5.2107282][-3.1131434 -3.3646126 -3.961237 -4.7249165 -5.4183369 -5.9432449 -6.4328327 -6.9352145 -7.2818375 -7.591855 -7.7507868 -7.6136589 -7.2662163 -6.6535969 -5.8783965][-2.8119822 -3.0150647 -3.3069072 -3.9156976 -4.6996264 -5.4197111 -6.0246124 -6.5746446 -7.0827274 -7.3912849 -7.3923817 -7.0846682 -6.7189083 -6.3187451 -5.8772883]]...]
INFO - root - 2017-12-16 09:10:49.772842: step 69210, loss = 0.23, batch loss = 0.17 (27.6 examples/sec; 0.289 sec/batch; 21h:09m:50s remains)
INFO - root - 2017-12-16 09:10:52.599723: step 69220, loss = 0.21, batch loss = 0.16 (27.4 examples/sec; 0.292 sec/batch; 21h:23m:07s remains)
INFO - root - 2017-12-16 09:10:55.443172: step 69230, loss = 0.24, batch loss = 0.18 (28.7 examples/sec; 0.279 sec/batch; 20h:22m:12s remains)
INFO - root - 2017-12-16 09:10:58.284504: step 69240, loss = 0.28, batch loss = 0.22 (29.0 examples/sec; 0.276 sec/batch; 20h:11m:31s remains)
INFO - root - 2017-12-16 09:11:01.147394: step 69250, loss = 0.27, batch loss = 0.21 (27.0 examples/sec; 0.297 sec/batch; 21h:41m:55s remains)
INFO - root - 2017-12-16 09:11:03.974288: step 69260, loss = 0.21, batch loss = 0.16 (29.4 examples/sec; 0.272 sec/batch; 19h:54m:23s remains)
INFO - root - 2017-12-16 09:11:06.790982: step 69270, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.277 sec/batch; 20h:16m:43s remains)
INFO - root - 2017-12-16 09:11:09.686453: step 69280, loss = 0.20, batch loss = 0.14 (26.7 examples/sec; 0.300 sec/batch; 21h:55m:55s remains)
INFO - root - 2017-12-16 09:11:12.509399: step 69290, loss = 0.25, batch loss = 0.20 (28.7 examples/sec; 0.279 sec/batch; 20h:24m:27s remains)
INFO - root - 2017-12-16 09:11:15.338720: step 69300, loss = 0.28, batch loss = 0.23 (28.3 examples/sec; 0.282 sec/batch; 20h:37m:55s remains)
2017-12-16 09:11:15.853583: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6145568 -1.903677 -2.3987079 -3.0297046 -3.6472969 -4.0934744 -4.4698062 -4.9088869 -5.2688122 -5.2680144 -5.0320525 -4.7685089 -4.5101171 -4.1255689 -3.5925198][-1.9854138 -2.5466065 -3.1999416 -3.7901413 -4.2114029 -4.5655417 -4.8455362 -4.9872346 -5.0511408 -5.1714811 -5.225544 -4.98867 -4.655951 -4.4556584 -4.1379714][-2.6768868 -3.3838911 -4.1566734 -4.6518555 -4.7130632 -4.5144749 -4.2287607 -4.1870022 -4.2563386 -4.2560859 -4.2889814 -4.5074625 -4.7694211 -4.6791797 -4.2777247][-3.2886271 -3.9406478 -4.6868896 -5.07224 -4.869174 -4.0715733 -3.1440918 -2.504842 -2.2680917 -2.5084054 -3.0872922 -3.577204 -4.0819511 -4.4394188 -4.4579263][-3.9321287 -4.3870177 -4.697144 -4.6849146 -4.0265484 -2.7648115 -1.3062444 -0.22181225 0.16162157 -0.22360563 -1.2647321 -2.5854571 -3.6909857 -4.250309 -4.2992249][-4.0182109 -4.323905 -4.1723123 -3.578217 -2.4562902 -0.87810588 0.78746367 2.0216265 2.3205209 1.6968846 0.47669935 -1.1112096 -2.6309 -3.8034554 -4.2631717][-3.5086589 -3.498044 -3.1217539 -2.1038563 -0.69084406 0.95710135 2.5527596 3.6815605 3.8571444 3.0423269 1.576982 -0.17516661 -1.8238108 -3.0502734 -3.8871384][-2.91755 -2.6596136 -2.0795028 -0.9107492 0.70875454 2.3173485 3.6446047 4.2157812 4.0989923 3.1805005 1.6930156 -0.013510704 -1.6358078 -3.0206616 -3.8922083][-2.9543166 -2.5538058 -1.8649647 -0.648921 0.79749537 2.2657332 3.375174 3.7293425 3.2637563 2.3229017 1.1567965 -0.30476236 -1.8243976 -3.1807737 -4.1440725][-3.5571632 -2.9219944 -2.0961585 -0.91580963 0.36495972 1.4115639 2.0669823 2.2773204 2.006 1.0976171 0.14301538 -0.89758897 -2.1585472 -3.4140406 -4.4638915][-4.4140844 -3.7297344 -2.8220708 -1.6879909 -0.65413809 0.047717094 0.5250535 0.61787653 0.44508982 -0.094073296 -0.94413996 -2.0049174 -3.1694951 -4.2071681 -5.0265594][-5.3269997 -4.784327 -3.829154 -2.7575636 -1.7483361 -1.1873553 -0.78426623 -0.701231 -1.0032771 -1.4656816 -2.0178099 -2.9572887 -4.1605029 -5.1851454 -5.7448225][-6.284874 -5.7163329 -4.7432742 -3.5955923 -2.4213667 -1.8124011 -1.4236593 -1.2244368 -1.3817389 -1.9846461 -2.8356309 -3.5969696 -4.5004487 -5.4869776 -6.1125793][-6.5924759 -5.9750867 -4.9894142 -3.8235834 -2.5405164 -1.6555104 -1.187933 -1.1556039 -1.3517566 -1.8795648 -2.6732352 -3.7229004 -4.7038188 -5.5722208 -6.060379][-6.0462341 -5.4482551 -4.5135169 -3.4747293 -2.1730003 -1.1033099 -0.565053 -0.62031364 -0.93911743 -1.4590271 -2.2371554 -3.2518191 -4.2093453 -4.9118443 -5.268393]]...]
INFO - root - 2017-12-16 09:11:18.713569: step 69310, loss = 0.25, batch loss = 0.19 (27.7 examples/sec; 0.289 sec/batch; 21h:07m:38s remains)
INFO - root - 2017-12-16 09:11:21.547168: step 69320, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.280 sec/batch; 20h:29m:55s remains)
INFO - root - 2017-12-16 09:11:24.386851: step 69330, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.283 sec/batch; 20h:39m:31s remains)
INFO - root - 2017-12-16 09:11:27.203417: step 69340, loss = 0.19, batch loss = 0.13 (28.4 examples/sec; 0.282 sec/batch; 20h:37m:16s remains)
INFO - root - 2017-12-16 09:11:30.087740: step 69350, loss = 0.30, batch loss = 0.24 (28.4 examples/sec; 0.281 sec/batch; 20h:33m:55s remains)
INFO - root - 2017-12-16 09:11:32.904847: step 69360, loss = 0.25, batch loss = 0.19 (29.7 examples/sec; 0.269 sec/batch; 19h:40m:38s remains)
INFO - root - 2017-12-16 09:11:35.733046: step 69370, loss = 0.20, batch loss = 0.14 (27.1 examples/sec; 0.295 sec/batch; 21h:32m:14s remains)
INFO - root - 2017-12-16 09:11:38.609437: step 69380, loss = 0.36, batch loss = 0.31 (27.6 examples/sec; 0.290 sec/batch; 21h:13m:22s remains)
INFO - root - 2017-12-16 09:11:41.495728: step 69390, loss = 0.20, batch loss = 0.14 (28.6 examples/sec; 0.279 sec/batch; 20h:25m:15s remains)
INFO - root - 2017-12-16 09:11:44.381650: step 69400, loss = 0.20, batch loss = 0.14 (27.2 examples/sec; 0.294 sec/batch; 21h:31m:12s remains)
2017-12-16 09:11:44.908306: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3780522 -6.1224318 -6.6825066 -7.0747137 -7.2885933 -7.3519559 -7.2835951 -7.2766294 -7.283659 -7.2158332 -6.9237947 -6.5983582 -6.2198992 -5.5895343 -5.0099978][-6.4410229 -7.2435646 -7.6867294 -7.8663397 -7.8669968 -8.0875854 -8.4174957 -8.667717 -8.8860254 -8.9805813 -8.9727049 -8.657671 -8.0616817 -7.3523073 -6.6334162][-6.8626356 -7.5384769 -7.705698 -7.5215406 -7.22781 -7.1858892 -7.3000345 -7.9271545 -8.7655106 -9.3753986 -9.8094568 -9.8530483 -9.5721035 -8.9453926 -8.1196365][-6.5980411 -6.9744968 -6.751111 -6.2346053 -5.5983853 -5.1349387 -4.9205422 -5.29538 -6.0587454 -7.3279457 -8.64446 -9.3150806 -9.61075 -9.4153481 -8.8546076][-5.7959757 -5.5850191 -4.7901945 -3.6901846 -2.4489489 -1.658587 -1.3164718 -1.6747179 -2.5711844 -4.103766 -5.7570772 -7.3405342 -8.5776863 -8.9326725 -8.7988138][-4.4967403 -3.9426067 -2.7344086 -0.99442625 0.93316078 2.3385563 3.2397571 3.0097265 1.8363309 -0.19815254 -2.3522513 -4.5162358 -6.4485359 -7.6944752 -8.2306337][-3.9770396 -2.9733152 -1.4806993 0.49819517 2.8483524 4.8282461 6.2973337 6.3763809 5.5907249 3.7085714 1.3343067 -1.350287 -3.8813787 -5.7811575 -6.9642458][-4.3654728 -3.2809029 -1.8663864 0.448833 3.1844382 5.2866821 7.1132936 7.5485115 7.0144939 5.1664009 2.883709 0.40280342 -2.1765497 -4.406198 -5.7512403][-5.3824296 -4.6044912 -3.3198123 -1.2842336 1.0674419 3.2442393 5.430625 6.0637035 6.1207895 4.8897572 2.9029789 0.34670782 -2.2708962 -4.2514834 -5.3936453][-6.0985394 -5.9165535 -5.486517 -4.1362839 -2.3668592 -0.46457982 1.4281435 2.4633651 3.1412821 2.3755741 1.0463667 -0.92487121 -3.1116385 -5.0124683 -6.1250615][-7.0941153 -7.0111027 -6.68203 -6.0401292 -5.3644452 -4.070756 -2.5334084 -1.5244613 -0.92027044 -1.2083991 -1.7752292 -3.1824732 -4.6034923 -5.7227569 -6.4747696][-7.4950562 -7.5583415 -7.7517586 -7.5438619 -6.8693714 -6.2139039 -5.6090541 -4.8281369 -4.2296448 -4.1992021 -4.4337521 -5.2544541 -5.8883872 -6.4017191 -6.6901965][-6.9001665 -7.127306 -7.3465948 -7.30855 -7.3962393 -7.1137671 -6.5746064 -6.1283417 -5.8354053 -5.6604915 -5.6029949 -6.0706625 -6.3823495 -6.746911 -6.9420443][-5.3425527 -5.6869712 -6.0814686 -6.388082 -6.5794029 -6.4700193 -6.5232 -6.374217 -6.1223254 -5.9679947 -5.6449895 -5.6509285 -5.822156 -6.1094313 -6.1097555][-4.421556 -4.3667321 -4.3787184 -4.7620549 -5.2535124 -5.403183 -5.5825787 -5.4505749 -5.2728004 -5.1291361 -4.9369736 -4.7494631 -4.4835439 -4.5238504 -4.6443887]]...]
INFO - root - 2017-12-16 09:11:47.744020: step 69410, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.281 sec/batch; 20h:33m:20s remains)
INFO - root - 2017-12-16 09:11:50.569068: step 69420, loss = 0.22, batch loss = 0.16 (27.4 examples/sec; 0.292 sec/batch; 21h:18m:15s remains)
INFO - root - 2017-12-16 09:11:53.396982: step 69430, loss = 0.30, batch loss = 0.24 (27.5 examples/sec; 0.290 sec/batch; 21h:13m:22s remains)
INFO - root - 2017-12-16 09:11:56.203491: step 69440, loss = 0.51, batch loss = 0.46 (28.3 examples/sec; 0.283 sec/batch; 20h:39m:46s remains)
INFO - root - 2017-12-16 09:11:59.083112: step 69450, loss = 0.25, batch loss = 0.20 (27.8 examples/sec; 0.288 sec/batch; 21h:03m:09s remains)
INFO - root - 2017-12-16 09:12:01.913253: step 69460, loss = 0.24, batch loss = 0.19 (27.8 examples/sec; 0.288 sec/batch; 21h:02m:55s remains)
INFO - root - 2017-12-16 09:12:04.750920: step 69470, loss = 0.23, batch loss = 0.17 (28.8 examples/sec; 0.278 sec/batch; 20h:16m:45s remains)
INFO - root - 2017-12-16 09:12:07.569779: step 69480, loss = 0.23, batch loss = 0.17 (27.8 examples/sec; 0.288 sec/batch; 21h:03m:00s remains)
INFO - root - 2017-12-16 09:12:10.461057: step 69490, loss = 0.28, batch loss = 0.22 (25.4 examples/sec; 0.316 sec/batch; 23h:03m:17s remains)
INFO - root - 2017-12-16 09:12:13.298111: step 69500, loss = 0.24, batch loss = 0.18 (27.1 examples/sec; 0.296 sec/batch; 21h:35m:31s remains)
2017-12-16 09:12:13.854842: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0565414 -5.4319639 -4.9762449 -4.921947 -5.2098422 -5.2646232 -5.2480927 -5.3296423 -5.5001144 -5.5703111 -5.532814 -5.3803186 -5.0166121 -4.4891453 -3.8625419][-6.4001212 -5.7591295 -5.0060358 -4.6466284 -4.517262 -4.8451529 -5.0039277 -5.2611542 -5.6612511 -5.8583941 -6.0281858 -5.9396362 -5.65629 -4.7631955 -3.7763956][-6.5531182 -5.6419888 -4.7750444 -4.1217079 -3.747458 -3.8355119 -3.7179446 -4.2598 -4.9090047 -5.6046715 -6.2166691 -6.3259339 -6.2179785 -5.4207282 -4.4047179][-6.2030258 -5.0360703 -3.9082487 -2.9509482 -2.4943342 -2.2289 -2.2058854 -2.818733 -3.5321333 -4.5653086 -5.4306059 -6.250349 -6.4194379 -5.6746087 -4.6347203][-5.2660623 -3.8453157 -2.2999597 -1.1172817 -0.37901115 -0.18893862 -0.28605604 -0.78605747 -1.7911668 -3.0845666 -4.1800714 -5.1704431 -5.62267 -5.40926 -4.6018014][-4.4253235 -2.9013002 -1.1288404 0.60670233 2.0013289 2.5397568 2.6811366 1.9816642 0.66423321 -0.76238823 -2.3605578 -3.8445075 -4.9319315 -5.1469846 -4.5153332][-4.1976871 -2.5537481 -0.57618237 1.4398127 2.9280119 3.95049 4.6024685 4.1870651 3.1688108 1.4516492 -0.361413 -2.157047 -3.7914472 -4.4150934 -4.3113914][-3.7987411 -2.4010065 -0.726362 1.1929345 2.7151728 3.8477249 4.5716457 4.4508791 3.8593788 2.3670216 0.61243343 -1.2771187 -3.0081475 -3.9830015 -4.3093696][-3.5827394 -2.632411 -1.4171574 0.00020647049 1.2895341 2.4701228 3.4065824 3.4716487 3.0043607 1.8864255 0.31879282 -1.3664672 -2.9801862 -4.0183306 -4.3514562][-4.0801725 -3.6535444 -2.8468804 -1.8185296 -0.72589231 0.41016722 1.3750467 1.7206092 1.5866075 0.59954977 -0.78259444 -2.2882953 -3.9347403 -4.8280411 -5.0167356][-4.6070886 -4.061522 -3.5664363 -3.2742643 -2.7844944 -2.0420568 -1.109571 -0.69360995 -0.68605995 -1.4703553 -2.5192332 -3.7784562 -4.9153767 -5.5270233 -5.6571894][-5.1352196 -4.7389374 -4.5582137 -4.5209069 -4.6116781 -4.3055792 -3.6529756 -3.1863554 -3.0804768 -3.4735818 -4.2661753 -5.2890372 -5.906703 -5.936656 -5.5864534][-5.9019856 -5.3139691 -5.0112562 -5.1439438 -5.39789 -5.5001493 -5.1390772 -4.8325248 -4.7692523 -4.9857583 -5.4706097 -5.9840889 -6.1756172 -5.8292494 -5.2927456][-6.8483162 -5.94079 -5.428102 -5.1752005 -5.2320051 -5.43083 -5.292645 -5.1310148 -5.10057 -5.2140846 -5.4418936 -5.7023268 -5.7411852 -5.1686282 -4.4418769][-7.2621088 -6.0864463 -5.3387108 -5.0744104 -5.2346745 -5.4647355 -5.58367 -5.4550152 -5.2345939 -5.1241369 -5.0874462 -4.9979539 -4.7389889 -4.1519718 -3.5124037]]...]
INFO - root - 2017-12-16 09:12:16.721465: step 69510, loss = 0.29, batch loss = 0.24 (27.1 examples/sec; 0.296 sec/batch; 21h:36m:05s remains)
INFO - root - 2017-12-16 09:12:19.552304: step 69520, loss = 0.23, batch loss = 0.17 (27.9 examples/sec; 0.286 sec/batch; 20h:55m:18s remains)
INFO - root - 2017-12-16 09:12:22.401035: step 69530, loss = 0.49, batch loss = 0.44 (28.7 examples/sec; 0.279 sec/batch; 20h:23m:30s remains)
INFO - root - 2017-12-16 09:12:25.187693: step 69540, loss = 0.25, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 20h:26m:45s remains)
INFO - root - 2017-12-16 09:12:28.001990: step 69550, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.286 sec/batch; 20h:52m:49s remains)
INFO - root - 2017-12-16 09:12:30.854337: step 69560, loss = 0.27, batch loss = 0.21 (27.1 examples/sec; 0.296 sec/batch; 21h:35m:41s remains)
INFO - root - 2017-12-16 09:12:33.703991: step 69570, loss = 0.29, batch loss = 0.23 (27.3 examples/sec; 0.293 sec/batch; 21h:22m:19s remains)
INFO - root - 2017-12-16 09:12:36.523473: step 69580, loss = 0.33, batch loss = 0.27 (27.5 examples/sec; 0.291 sec/batch; 21h:15m:43s remains)
INFO - root - 2017-12-16 09:12:39.338540: step 69590, loss = 0.22, batch loss = 0.17 (28.7 examples/sec; 0.278 sec/batch; 20h:19m:32s remains)
INFO - root - 2017-12-16 09:12:42.193321: step 69600, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.281 sec/batch; 20h:29m:08s remains)
2017-12-16 09:12:42.729941: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7062936 -3.5002522 -3.4130459 -3.5028725 -3.5000424 -3.5935566 -3.73202 -3.89299 -3.9080884 -3.826611 -3.6169651 -3.3731329 -3.1778831 -3.0176077 -3.1763616][-2.6225634 -2.4738543 -2.5409038 -2.9239531 -3.2085121 -3.4817257 -3.6968336 -3.9350538 -4.1140785 -4.1655951 -4.0436058 -3.757926 -3.4944041 -3.3171473 -3.398684][-1.753011 -1.7142482 -1.881321 -2.3879478 -2.7908587 -3.1028733 -3.4501479 -3.8562751 -4.2245393 -4.4364939 -4.47482 -4.2369089 -3.8915253 -3.6914563 -3.7598815][-1.0293624 -1.1190319 -1.3489051 -1.7771018 -2.0731452 -2.3651514 -2.7516055 -3.2136931 -3.6535261 -4.0453892 -4.3247285 -4.33029 -4.0712771 -3.8797209 -3.912169][-1.2861283 -1.3131375 -1.3515496 -1.4617028 -1.4175584 -1.346756 -1.5415666 -1.9453032 -2.5246274 -3.0740261 -3.4812903 -3.6994073 -3.5958607 -3.4895635 -3.4931259][-1.8900719 -1.6712918 -1.3541842 -1.0016236 -0.53775692 -0.079164028 -0.10509348 -0.404387 -0.95365095 -1.6773367 -2.3882296 -2.7516444 -2.7514443 -2.7819738 -2.875282][-2.3982773 -1.9490564 -1.2698772 -0.31334257 0.59600687 1.4411473 1.646976 1.2552128 0.647316 -0.15073061 -1.0688372 -1.6579578 -1.7689152 -1.6489608 -1.6424944][-2.7077582 -1.93754 -1.0049217 0.19006443 1.3057423 2.289052 2.582912 2.2749429 1.8124013 1.0678692 0.25457096 -0.3449893 -0.423131 -0.23626757 -0.2660079][-3.0569706 -2.1730726 -1.1743789 0.039002419 1.2782693 2.3993187 2.6956916 2.4794683 2.1239552 1.5938244 1.1637774 0.81517315 0.97026062 1.1368361 1.0460634][-3.4619479 -2.6796441 -1.7535472 -0.550015 0.59660578 1.5851893 1.8098812 1.7526641 1.6201806 1.4036913 1.374125 1.2937813 1.5642495 1.675539 1.5094562][-4.1957231 -3.5598006 -2.7381177 -1.6896038 -0.78145671 0.13720179 0.39751816 0.35175085 0.27025747 0.41422176 0.80282927 1.0965986 1.5087557 1.67488 1.5289927][-5.4336166 -4.9237437 -4.2700152 -3.2823143 -2.3977654 -1.6482513 -1.3386667 -1.1159267 -1.0528305 -0.6335597 0.099630833 0.6398077 1.0736504 1.1056814 0.85311127][-6.848527 -6.4320555 -5.7317653 -4.9028997 -4.1571026 -3.3563247 -2.9499869 -2.6242375 -2.4332244 -1.8852842 -1.1191442 -0.72633028 -0.45786881 -0.46728492 -0.77913213][-8.012846 -7.5245476 -6.7967944 -6.0163879 -5.2931185 -4.5866456 -4.152442 -3.741375 -3.5478449 -3.0950658 -2.5612082 -2.4303145 -2.3817081 -2.4209969 -2.6463194][-8.1167116 -7.6808052 -7.0544286 -6.5062952 -6.0175209 -5.3168716 -4.7273917 -4.2351394 -4.0786691 -3.7800705 -3.5634894 -3.7157664 -3.9099197 -4.0119796 -4.0322833]]...]
INFO - root - 2017-12-16 09:12:45.518283: step 69610, loss = 0.36, batch loss = 0.30 (29.1 examples/sec; 0.275 sec/batch; 20h:03m:18s remains)
INFO - root - 2017-12-16 09:12:48.451503: step 69620, loss = 0.34, batch loss = 0.28 (27.7 examples/sec; 0.289 sec/batch; 21h:04m:09s remains)
INFO - root - 2017-12-16 09:12:51.307618: step 69630, loss = 0.20, batch loss = 0.14 (28.5 examples/sec; 0.280 sec/batch; 20h:28m:01s remains)
INFO - root - 2017-12-16 09:12:54.143759: step 69640, loss = 0.26, batch loss = 0.20 (26.7 examples/sec; 0.300 sec/batch; 21h:55m:06s remains)
INFO - root - 2017-12-16 09:12:57.012227: step 69650, loss = 0.35, batch loss = 0.29 (28.6 examples/sec; 0.280 sec/batch; 20h:27m:17s remains)
INFO - root - 2017-12-16 09:12:59.848293: step 69660, loss = 0.23, batch loss = 0.17 (28.6 examples/sec; 0.280 sec/batch; 20h:24m:55s remains)
INFO - root - 2017-12-16 09:13:02.678036: step 69670, loss = 0.25, batch loss = 0.20 (28.0 examples/sec; 0.286 sec/batch; 20h:51m:12s remains)
INFO - root - 2017-12-16 09:13:05.551759: step 69680, loss = 0.26, batch loss = 0.20 (26.6 examples/sec; 0.300 sec/batch; 21h:55m:53s remains)
INFO - root - 2017-12-16 09:13:08.379013: step 69690, loss = 0.29, batch loss = 0.24 (29.7 examples/sec; 0.270 sec/batch; 19h:40m:28s remains)
INFO - root - 2017-12-16 09:13:11.212649: step 69700, loss = 0.25, batch loss = 0.19 (26.5 examples/sec; 0.302 sec/batch; 22h:01m:56s remains)
2017-12-16 09:13:11.734679: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4334302 -3.2771387 -3.1101141 -2.8193145 -2.6615796 -2.45651 -2.2626898 -2.4589703 -2.8287091 -3.1961365 -3.4019804 -3.5177734 -3.5003467 -3.438983 -3.4233336][-3.3255754 -3.2718356 -3.2617798 -3.0196307 -2.7909825 -2.5617859 -2.2728074 -2.3035491 -2.5243254 -2.9059911 -3.2283471 -3.4960055 -3.574681 -3.5949135 -3.7628636][-3.1491003 -3.1083033 -3.1454732 -2.9826822 -2.7055213 -2.3311648 -1.9150329 -1.8518777 -2.0345895 -2.2734241 -2.5342402 -3.0580654 -3.5906858 -3.8511975 -4.1121621][-2.6003585 -2.7158823 -2.991992 -3.0242219 -2.8518243 -2.4250274 -1.9033797 -1.6059048 -1.4995384 -1.6455905 -1.9307444 -2.49324 -3.1242762 -3.715836 -4.2398834][-2.1435874 -2.3639679 -2.7011008 -2.7877045 -2.5345998 -2.0946703 -1.5200925 -0.98061895 -0.59507895 -0.60359836 -0.80783033 -1.3020678 -1.921406 -2.6515679 -3.3016987][-1.3161306 -1.8072584 -2.2568278 -2.4049759 -2.1464813 -1.6229377 -0.93611193 -0.30210972 0.18786573 0.21566868 0.031733036 -0.4438982 -1.0471017 -1.6685114 -2.2141619][-0.66785789 -1.189079 -1.6516454 -1.8084779 -1.5884953 -1.1368356 -0.52349734 0.027905464 0.51073837 0.56851721 0.36983585 0.12012339 -0.2728529 -0.842731 -1.2825785][-0.14151573 -0.64705729 -1.0713427 -1.2278395 -1.1483006 -0.79391575 -0.20777321 0.32489538 0.6304493 0.59668589 0.53392744 0.39537668 0.095512867 -0.32571316 -0.60313916][-0.038993359 -0.47239089 -0.68749571 -0.7441597 -0.7019732 -0.46926093 0.06951046 0.545393 0.77715635 0.79460621 0.8384676 0.71081543 0.38301659 0.02550745 -0.2387743][-0.44433713 -0.57695723 -0.64580154 -0.55557919 -0.43344069 -0.16604137 0.25333881 0.60798931 0.72334671 0.59503889 0.49626827 0.44608402 0.19415998 -0.19163656 -0.44851971][-1.1447725 -1.0221915 -0.99629712 -0.77673936 -0.540925 -0.28271103 0.11592484 0.3976264 0.26590014 0.019176483 -0.18737316 -0.26327181 -0.46287942 -0.8103528 -1.0325832][-2.0601082 -1.722224 -1.461081 -1.1186526 -0.90817404 -0.57388163 -0.13862991 0.047753334 -0.20427465 -0.50603843 -0.82685709 -0.9925909 -1.2615011 -1.6241958 -1.8869429][-2.8580394 -2.3441589 -1.844835 -1.3431287 -0.98682189 -0.70232105 -0.26364231 -0.077568531 -0.58140063 -1.0065339 -1.2700148 -1.6273434 -2.1297247 -2.455606 -2.5733147][-3.5824878 -2.8564847 -2.0662313 -1.4098561 -1.1216185 -0.95746613 -0.69872522 -0.720948 -1.2593327 -1.8790653 -2.35372 -2.5615592 -2.6865735 -2.8642116 -2.8783951][-3.9876733 -3.1709468 -2.3664966 -1.5692363 -1.209451 -1.2413001 -1.2477067 -1.4740593 -2.1853936 -2.8587456 -3.2617905 -3.227972 -3.0089815 -2.9732113 -2.7963877]]...]
INFO - root - 2017-12-16 09:13:14.577348: step 69710, loss = 0.40, batch loss = 0.35 (28.6 examples/sec; 0.279 sec/batch; 20h:23m:35s remains)
INFO - root - 2017-12-16 09:13:17.401676: step 69720, loss = 0.31, batch loss = 0.25 (28.0 examples/sec; 0.286 sec/batch; 20h:50m:45s remains)
INFO - root - 2017-12-16 09:13:20.292075: step 69730, loss = 0.27, batch loss = 0.22 (27.3 examples/sec; 0.293 sec/batch; 21h:25m:15s remains)
INFO - root - 2017-12-16 09:13:23.140006: step 69740, loss = 0.32, batch loss = 0.27 (28.3 examples/sec; 0.283 sec/batch; 20h:39m:03s remains)
INFO - root - 2017-12-16 09:13:25.969519: step 69750, loss = 0.19, batch loss = 0.14 (27.0 examples/sec; 0.297 sec/batch; 21h:38m:29s remains)
INFO - root - 2017-12-16 09:13:28.806390: step 69760, loss = 0.27, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 20h:33m:22s remains)
INFO - root - 2017-12-16 09:13:31.670574: step 69770, loss = 0.29, batch loss = 0.23 (28.6 examples/sec; 0.280 sec/batch; 20h:26m:02s remains)
INFO - root - 2017-12-16 09:13:34.527275: step 69780, loss = 0.23, batch loss = 0.17 (29.7 examples/sec; 0.270 sec/batch; 19h:41m:07s remains)
INFO - root - 2017-12-16 09:13:37.439263: step 69790, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.279 sec/batch; 20h:22m:50s remains)
INFO - root - 2017-12-16 09:13:40.289388: step 69800, loss = 0.28, batch loss = 0.22 (28.3 examples/sec; 0.283 sec/batch; 20h:37m:50s remains)
2017-12-16 09:13:40.812235: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8608823 -3.2143507 -3.6562822 -4.0420752 -4.3419576 -4.4529662 -4.5337615 -4.3664522 -4.0807748 -3.8097208 -3.5260704 -3.4255238 -3.2458186 -3.3048992 -3.4942346][-2.5190043 -3.0225668 -3.6011648 -4.2436838 -4.743217 -4.8395939 -4.78321 -4.4737978 -4.0336857 -3.7071357 -3.4686961 -3.2236533 -2.949466 -2.9762521 -3.0854762][-2.2768803 -2.87287 -3.6375215 -4.1377659 -4.4483519 -4.5252209 -4.2532558 -3.8096466 -3.3366694 -2.937644 -2.7584834 -2.7984457 -2.7334261 -2.6176996 -2.5306768][-2.7912745 -3.4585564 -4.2215624 -4.7536025 -4.8412809 -4.5129638 -3.9029479 -3.0957408 -2.4275224 -2.1055925 -2.0800114 -2.2949936 -2.3651834 -2.325063 -2.3028994][-3.3505216 -4.0725174 -4.5688815 -4.6950455 -4.3854275 -3.6033928 -2.4501626 -1.5914621 -1.1906638 -1.0525427 -1.3526187 -1.9054508 -2.147428 -2.0703592 -1.9736872][-3.5877519 -4.1167574 -4.3552289 -3.9331391 -2.8386841 -1.6842215 -0.47418189 0.4201622 0.70462751 0.36510134 -0.47983718 -1.1749821 -1.5935359 -1.6772022 -1.5781944][-3.9692886 -4.1605182 -3.8497703 -3.0048966 -1.5489733 0.093714714 1.77914 2.6668081 2.6062517 1.8961277 0.848001 -0.18834448 -0.99317122 -1.2463527 -1.3073053][-4.3375969 -4.58929 -4.2333536 -3.1060596 -1.4250925 0.44607115 2.0497947 2.8718076 2.9371376 2.1875887 1.0264955 -0.19099045 -1.0721307 -1.3246861 -1.4498193][-4.7378721 -4.85264 -4.4764681 -3.5349736 -2.0948923 -0.239007 1.454905 2.2931046 2.32271 1.6952381 0.76356173 -0.27226639 -1.0522087 -1.3940148 -1.547302][-5.0158143 -5.3133407 -5.1820793 -4.3966804 -3.0872808 -1.7464793 -0.49243832 0.606164 1.0156922 0.70410347 0.063108921 -0.59402943 -1.0860176 -1.4720716 -1.6984084][-5.2711706 -5.8526316 -6.0405107 -5.4652295 -4.5625677 -3.4706564 -2.3053851 -1.3582609 -0.83372211 -0.70498204 -0.79592037 -1.0539422 -1.3855798 -1.6317897 -1.7563851][-5.6257553 -6.2806726 -6.6668963 -6.47097 -5.8118215 -4.9528747 -4.2789106 -3.457546 -2.8697844 -2.5925939 -2.2387245 -1.8995266 -1.7984347 -2.0063627 -2.1247897][-5.7979007 -6.568747 -7.1906481 -7.3147769 -7.0867076 -6.7314148 -6.4182787 -5.7180347 -5.0656419 -4.566164 -3.9463747 -3.3552365 -2.8161941 -2.5400934 -2.329226][-5.6082211 -6.3461514 -6.9522543 -7.2437143 -7.3538165 -7.4441576 -7.5500431 -7.1850996 -6.5388227 -5.7031021 -4.8535967 -4.1916637 -3.6566432 -3.5258217 -3.3226929][-4.8946433 -5.3814383 -5.9245281 -6.1990843 -6.4937668 -6.9848919 -7.4813623 -7.4039364 -7.0861073 -6.5922017 -5.7525134 -4.9652052 -4.4630466 -4.4573226 -4.490746]]...]
INFO - root - 2017-12-16 09:13:43.621157: step 69810, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 20h:37m:18s remains)
INFO - root - 2017-12-16 09:13:46.472404: step 69820, loss = 0.25, batch loss = 0.19 (28.6 examples/sec; 0.280 sec/batch; 20h:24m:25s remains)
INFO - root - 2017-12-16 09:13:49.275320: step 69830, loss = 0.27, batch loss = 0.21 (28.8 examples/sec; 0.278 sec/batch; 20h:17m:02s remains)
INFO - root - 2017-12-16 09:13:52.118027: step 69840, loss = 0.30, batch loss = 0.25 (27.6 examples/sec; 0.290 sec/batch; 21h:09m:51s remains)
INFO - root - 2017-12-16 09:13:54.975394: step 69850, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.279 sec/batch; 20h:22m:39s remains)
INFO - root - 2017-12-16 09:13:57.858259: step 69860, loss = 0.24, batch loss = 0.18 (27.8 examples/sec; 0.287 sec/batch; 20h:57m:25s remains)
INFO - root - 2017-12-16 09:14:00.709014: step 69870, loss = 0.32, batch loss = 0.26 (28.6 examples/sec; 0.280 sec/batch; 20h:24m:45s remains)
INFO - root - 2017-12-16 09:14:03.534486: step 69880, loss = 0.22, batch loss = 0.16 (28.0 examples/sec; 0.285 sec/batch; 20h:49m:25s remains)
INFO - root - 2017-12-16 09:14:06.380725: step 69890, loss = 0.30, batch loss = 0.25 (28.7 examples/sec; 0.278 sec/batch; 20h:18m:01s remains)
INFO - root - 2017-12-16 09:14:09.234827: step 69900, loss = 0.30, batch loss = 0.24 (29.1 examples/sec; 0.275 sec/batch; 20h:01m:35s remains)
2017-12-16 09:14:09.784974: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0348153 -3.3403807 -3.7122283 -4.0602303 -4.2988157 -4.2681208 -3.9003069 -3.4958835 -3.2691641 -2.9657872 -2.7302217 -2.9250269 -3.2042 -3.173717 -3.0399218][-2.3132761 -2.5823703 -2.9958813 -3.4181447 -3.5403025 -3.4321465 -3.2491131 -3.0980844 -2.8715434 -2.5724711 -2.4748888 -2.5101876 -2.5686836 -2.5731294 -2.40896][-1.5969882 -1.9360271 -2.3543768 -2.7469893 -3.0067117 -2.978271 -2.6583915 -2.4653339 -2.3795419 -2.2845094 -2.1583142 -2.0298517 -2.0495327 -2.0200818 -1.8901751][-1.0034068 -1.3187904 -1.6244903 -1.887641 -1.9235075 -1.9166791 -1.7926428 -1.6742811 -1.5353842 -1.5443251 -1.5547781 -1.4605634 -1.5083122 -1.5031044 -1.524142][-0.21701384 -0.58981609 -0.85371494 -0.93170619 -0.72229671 -0.54371071 -0.529896 -0.51595974 -0.59415126 -0.76006365 -0.91989374 -0.9561727 -0.97554708 -1.0759556 -1.2576318][-0.42084217 -0.61299968 -0.60001493 -0.61851287 -0.35821152 0.16469669 0.49275875 0.45919991 0.25834179 0.03361845 -0.27744055 -0.40854597 -0.50924921 -0.67264676 -0.95041847][-0.96355176 -1.2086763 -1.2133896 -0.96119475 -0.41876841 -0.017085552 0.36601448 0.61582422 0.524611 0.30205154 -0.029542923 -0.22390366 -0.32594109 -0.46477461 -0.77340865][-1.8108265 -2.205 -2.2781341 -2.0613415 -1.3988891 -0.702446 -0.25346231 0.099297047 0.44561768 0.40715122 0.075430393 -0.038218021 -0.10397291 -0.13047266 -0.36293697][-2.4829388 -2.9648256 -3.1991434 -3.0723057 -2.6014218 -1.8884652 -1.1030614 -0.63548255 -0.58405972 -0.41458941 -0.29013968 -0.356174 -0.41092253 -0.26184845 -0.27789259][-3.4585259 -4.0404038 -4.2516313 -4.1466079 -3.7326391 -3.1458197 -2.546165 -1.7806113 -1.1668916 -1.0154147 -0.97845888 -0.96076989 -1.0473142 -0.88430738 -0.66860223][-3.8788722 -4.4150009 -4.82684 -4.8857346 -4.6014643 -4.1016397 -3.4354708 -2.8005252 -2.4227831 -2.1592109 -1.8666658 -1.6967068 -1.4681604 -1.2199621 -1.091387][-4.2405324 -4.536972 -4.7706094 -4.9972353 -5.0911613 -4.6990685 -4.1667595 -3.8341844 -3.4186401 -3.120225 -2.9632306 -2.6279941 -2.2177289 -1.6608055 -1.1754713][-4.8006015 -4.7009897 -4.601891 -4.6144686 -4.5682049 -4.404068 -4.1241136 -3.9270682 -3.7789164 -3.7836738 -3.6882987 -3.3367176 -2.5699847 -1.7592039 -1.190016][-4.9070697 -4.488071 -4.1141362 -3.8839002 -3.9513621 -3.791503 -3.65993 -3.7034135 -3.7965775 -3.9198298 -3.8380477 -3.5166047 -2.6907625 -1.6561282 -1.0565374][-4.7838182 -4.2349639 -3.6286113 -3.2987285 -3.159853 -3.0956626 -3.1425292 -3.3647361 -3.6784964 -4.0129752 -3.9111342 -3.3225131 -2.2591991 -1.1133323 -0.49717689]]...]
INFO - root - 2017-12-16 09:14:12.640813: step 69910, loss = 0.23, batch loss = 0.17 (26.5 examples/sec; 0.302 sec/batch; 22h:00m:25s remains)
INFO - root - 2017-12-16 09:14:15.482209: step 69920, loss = 0.28, batch loss = 0.22 (27.9 examples/sec; 0.287 sec/batch; 20h:53m:58s remains)
INFO - root - 2017-12-16 09:14:18.397560: step 69930, loss = 0.23, batch loss = 0.18 (27.8 examples/sec; 0.288 sec/batch; 20h:59m:37s remains)
INFO - root - 2017-12-16 09:14:21.273277: step 69940, loss = 0.21, batch loss = 0.15 (27.2 examples/sec; 0.294 sec/batch; 21h:26m:26s remains)
INFO - root - 2017-12-16 09:14:24.181103: step 69950, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.286 sec/batch; 20h:50m:05s remains)
INFO - root - 2017-12-16 09:14:27.098401: step 69960, loss = 0.27, batch loss = 0.21 (25.2 examples/sec; 0.317 sec/batch; 23h:08m:32s remains)
INFO - root - 2017-12-16 09:14:29.927891: step 69970, loss = 0.21, batch loss = 0.15 (28.5 examples/sec; 0.280 sec/batch; 20h:26m:29s remains)
INFO - root - 2017-12-16 09:14:32.830115: step 69980, loss = 0.31, batch loss = 0.26 (26.4 examples/sec; 0.303 sec/batch; 22h:03m:42s remains)
INFO - root - 2017-12-16 09:14:35.651262: step 69990, loss = 0.33, batch loss = 0.27 (28.0 examples/sec; 0.285 sec/batch; 20h:47m:54s remains)
INFO - root - 2017-12-16 09:14:38.504525: step 70000, loss = 0.27, batch loss = 0.21 (27.9 examples/sec; 0.286 sec/batch; 20h:52m:24s remains)
2017-12-16 09:14:39.058313: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0687404 -4.5042963 -4.6688366 -5.0391645 -5.5260472 -5.8769283 -6.0423288 -6.1941223 -6.3240218 -6.4560184 -6.4696684 -5.9952927 -5.474102 -4.7530742 -3.990119][-3.9309702 -4.4229341 -4.81184 -4.9596672 -4.8534122 -5.0550342 -5.265461 -5.4062271 -5.7436428 -5.9453573 -6.1268644 -5.8356266 -5.0069485 -4.157938 -3.566745][-4.2377996 -4.7626143 -4.9396982 -4.8315582 -4.4884577 -4.0120783 -3.7382572 -3.8375044 -4.10615 -4.5630641 -4.9561949 -4.8736415 -4.5887156 -3.9669268 -3.4169517][-4.5037475 -5.246726 -5.4616404 -5.020174 -4.1302047 -3.2509356 -2.4387627 -2.2483287 -2.6547484 -3.1349363 -3.6014047 -3.8851826 -3.8202477 -3.469265 -3.2083366][-4.5013442 -5.28693 -5.4345527 -4.7878256 -3.5472031 -2.1389647 -1.0050135 -0.61171031 -0.87171936 -1.6162767 -2.535594 -3.2426929 -3.5767128 -3.4888654 -3.3543639][-4.4729137 -4.9603558 -4.8688064 -4.0560589 -2.4776881 -0.65681553 0.82812977 1.4112453 1.0284505 -0.01654911 -1.2198136 -2.4616253 -3.475462 -3.8406947 -3.787086][-4.572258 -5.0241866 -4.5179472 -3.4868307 -1.8527708 0.047632217 1.7492042 2.6290464 2.3993092 1.3821945 -0.17528772 -1.9309225 -3.2847369 -4.0503221 -4.4372864][-4.2243781 -4.7462687 -4.5826449 -3.3771586 -1.5891657 0.30290127 1.9956326 2.8012133 2.5871172 1.4975233 -0.14349556 -1.8549778 -3.1680732 -3.9583645 -4.3624811][-3.9426918 -4.2747498 -4.1105185 -3.4741924 -1.7851281 0.14521408 1.5814734 2.1287274 1.8546357 0.86047173 -0.72626209 -2.3546953 -3.4904637 -4.00911 -4.1071777][-3.8438308 -4.2437034 -4.3352332 -3.8509066 -2.6142974 -1.2080519 0.015280247 0.35931349 -0.27109766 -1.2182112 -2.4151223 -3.49959 -4.2603626 -4.6601038 -4.6421957][-4.3477612 -4.8310294 -5.0055513 -4.8950272 -4.15564 -3.1512513 -2.2174206 -2.0975423 -3.0700493 -3.9219606 -4.6388917 -4.9786677 -5.0032573 -4.9964619 -4.9406047][-5.2192426 -5.8976378 -6.0392189 -5.923851 -5.3893929 -4.7619128 -4.601572 -4.5576 -4.9899487 -5.8202071 -6.5408716 -6.2654595 -5.5780993 -5.1018214 -4.7146511][-6.0359287 -6.8831973 -7.205204 -7.0906353 -6.6853027 -6.4926748 -6.5210171 -6.533474 -7.094821 -7.310545 -7.2008743 -6.7966013 -6.1361055 -5.4581375 -4.764009][-6.75743 -7.5823193 -7.8586974 -7.7822709 -7.4330268 -7.4564447 -7.9439511 -8.3122711 -8.6802273 -8.53018 -8.0673046 -7.3905654 -6.54809 -5.802197 -5.1007695][-6.555016 -7.3353691 -7.6332011 -7.5373735 -7.2847972 -7.3347025 -7.6728129 -8.2548332 -8.9699459 -9.0121288 -8.6403255 -7.7707734 -6.769702 -6.1048355 -5.4776711]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-70000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-70000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 09:14:42.510202: step 70010, loss = 0.25, batch loss = 0.19 (27.4 examples/sec; 0.292 sec/batch; 21h:18m:09s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 09:14:45.387034: step 70020, loss = 0.27, batch loss = 0.21 (27.9 examples/sec; 0.287 sec/batch; 20h:56m:25s remains)
INFO - root - 2017-12-16 09:14:48.278787: step 70030, loss = 0.27, batch loss = 0.22 (27.2 examples/sec; 0.294 sec/batch; 21h:24m:23s remains)
INFO - root - 2017-12-16 09:14:51.134686: step 70040, loss = 0.29, batch loss = 0.23 (27.9 examples/sec; 0.286 sec/batch; 20h:52m:33s remains)
INFO - root - 2017-12-16 09:14:53.970402: step 70050, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.282 sec/batch; 20h:34m:48s remains)
INFO - root - 2017-12-16 09:14:56.832779: step 70060, loss = 0.37, batch loss = 0.31 (27.7 examples/sec; 0.289 sec/batch; 21h:03m:17s remains)
INFO - root - 2017-12-16 09:14:59.672020: step 70070, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 20h:33m:03s remains)
INFO - root - 2017-12-16 09:15:02.493555: step 70080, loss = 0.23, batch loss = 0.17 (29.0 examples/sec; 0.276 sec/batch; 20h:05m:24s remains)
INFO - root - 2017-12-16 09:15:05.345673: step 70090, loss = 0.23, batch loss = 0.18 (28.6 examples/sec; 0.280 sec/batch; 20h:22m:31s remains)
INFO - root - 2017-12-16 09:15:08.216057: step 70100, loss = 0.27, batch loss = 0.21 (26.9 examples/sec; 0.297 sec/batch; 21h:40m:07s remains)
2017-12-16 09:15:08.794556: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.63618 -2.4180968 -2.2736287 -2.1589057 -2.2545941 -2.4082212 -2.6712792 -2.7829592 -2.7127776 -2.6745124 -2.6619956 -2.8626287 -3.0851688 -3.2306836 -3.4803796][-3.1786418 -2.724931 -2.3736987 -2.2174525 -2.3419945 -2.5804813 -2.9662373 -3.1071751 -3.1606565 -3.3536925 -3.40065 -3.3937018 -3.4781156 -3.6740823 -3.8221703][-3.6818347 -3.0474348 -2.4305198 -2.1504211 -2.1372559 -2.3075612 -2.6552734 -3.0007796 -3.2991221 -3.4446328 -3.5068653 -3.6729081 -3.7871909 -3.8058381 -3.7414806][-4.0815444 -3.3182693 -2.5822184 -1.9632881 -1.6842792 -1.834173 -2.1479361 -2.3566589 -2.5624814 -3.0287733 -3.4089763 -3.4874887 -3.412468 -3.5328939 -3.5148857][-4.6703644 -3.8995192 -2.9834695 -2.0791821 -1.36972 -1.0886502 -1.098598 -1.3287981 -1.7222092 -2.2409494 -2.4993 -2.7011232 -2.7828484 -2.6980023 -2.5064163][-4.57529 -3.897295 -2.8912463 -1.7039368 -0.56243873 0.090407372 0.35922289 0.19118261 -0.22291803 -0.93574572 -1.5439293 -1.7432759 -1.543215 -1.4038377 -1.2943909][-4.0928063 -3.4948406 -2.5204332 -1.189069 0.15212774 1.072041 1.5271277 1.4357371 0.84600973 0.15942955 -0.30857086 -0.48523808 -0.31533718 0.0403533 0.31140518][-3.98232 -3.4973006 -2.6439335 -1.3054192 0.089443207 1.237515 1.8769002 1.7823515 1.1595984 0.49700451 0.15336466 0.29552126 0.77870321 1.345212 1.650732][-4.0839257 -3.7944098 -3.0684624 -1.8812702 -0.79391861 0.24152803 0.92898512 0.90019464 0.59482288 0.17999887 0.19669008 0.59782696 1.2803955 1.9264655 2.1962957][-3.9015167 -3.6989083 -3.30488 -2.4711347 -1.6796927 -0.92308378 -0.51932955 -0.54144835 -0.613986 -0.76617956 -0.4823308 0.12183046 0.94899607 1.6013808 1.737071][-3.8023903 -3.7987611 -3.6110559 -3.1924927 -2.8326681 -2.2805121 -2.1430566 -2.2474048 -2.2539937 -2.1360447 -1.5530553 -0.87372041 0.008810997 0.64719915 0.718493][-3.7649851 -3.7139401 -3.716578 -3.6430128 -3.6171193 -3.4775128 -3.7099125 -3.9447236 -3.8978381 -3.6071236 -2.8929327 -2.0899963 -1.1122077 -0.58359742 -0.5551002][-3.4501934 -3.2833114 -3.3874397 -3.4811983 -3.7473459 -3.9606757 -4.5767126 -5.1172562 -5.2598438 -4.9587784 -4.1985893 -3.4557543 -2.6696696 -2.1771238 -1.9736364][-3.0607548 -2.9163237 -2.9381905 -3.1636229 -3.6841207 -4.1853566 -5.1766186 -6.0788145 -6.5263286 -6.2407441 -5.5746098 -4.890913 -4.1414671 -3.7850246 -3.5803785][-2.7062516 -2.3682144 -2.3225415 -2.6490049 -3.4136095 -4.2810769 -5.5655622 -6.6288805 -7.2638178 -7.262538 -6.6471796 -5.8685822 -5.1344461 -4.699821 -4.4193044]]...]
INFO - root - 2017-12-16 09:15:11.636714: step 70110, loss = 0.26, batch loss = 0.20 (27.5 examples/sec; 0.291 sec/batch; 21h:10m:58s remains)
INFO - root - 2017-12-16 09:15:14.474376: step 70120, loss = 0.43, batch loss = 0.38 (27.9 examples/sec; 0.287 sec/batch; 20h:54m:52s remains)
INFO - root - 2017-12-16 09:15:17.291586: step 70130, loss = 0.23, batch loss = 0.17 (26.3 examples/sec; 0.305 sec/batch; 22h:11m:56s remains)
INFO - root - 2017-12-16 09:15:20.133054: step 70140, loss = 0.20, batch loss = 0.14 (28.8 examples/sec; 0.278 sec/batch; 20h:15m:49s remains)
INFO - root - 2017-12-16 09:15:23.019545: step 70150, loss = 0.38, batch loss = 0.32 (28.4 examples/sec; 0.282 sec/batch; 20h:33m:08s remains)
INFO - root - 2017-12-16 09:15:25.854807: step 70160, loss = 0.28, batch loss = 0.22 (27.5 examples/sec; 0.291 sec/batch; 21h:12m:34s remains)
INFO - root - 2017-12-16 09:15:28.736080: step 70170, loss = 0.36, batch loss = 0.30 (27.6 examples/sec; 0.290 sec/batch; 21h:08m:50s remains)
INFO - root - 2017-12-16 09:15:31.620009: step 70180, loss = 0.23, batch loss = 0.17 (26.8 examples/sec; 0.298 sec/batch; 21h:43m:02s remains)
INFO - root - 2017-12-16 09:15:34.438238: step 70190, loss = 0.28, batch loss = 0.22 (28.6 examples/sec; 0.279 sec/batch; 20h:21m:07s remains)
INFO - root - 2017-12-16 09:15:37.284980: step 70200, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.285 sec/batch; 20h:47m:57s remains)
2017-12-16 09:15:37.813793: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3098335 -5.3946338 -5.272923 -5.2623539 -5.2464981 -5.3625259 -5.8773537 -6.8147354 -7.6316404 -7.7120056 -7.2691278 -6.3324695 -5.1697316 -4.0685782 -3.5633211][-5.9539008 -5.6346846 -5.024004 -4.6340733 -4.4975324 -4.6064 -5.0643487 -6.1086807 -7.3718128 -8.0795 -8.0616541 -7.4049749 -6.389473 -5.2390752 -4.4953809][-6.8040004 -6.3792715 -5.4240007 -4.4088068 -3.6531048 -3.2870231 -3.4732525 -4.4838252 -5.8928618 -7.3061056 -8.0795574 -7.97575 -7.3194375 -6.1618614 -5.2651372][-6.9228382 -6.3997326 -5.2828259 -3.9627466 -2.6238794 -1.7238381 -1.3992136 -1.9887621 -3.335453 -5.0435915 -6.6085248 -7.5483675 -7.6248493 -6.6331973 -5.6430945][-5.8307638 -5.0977049 -3.8566504 -2.3745506 -0.71930647 0.593657 1.3885684 1.1902742 0.041457653 -1.8059831 -4.0114512 -5.7898111 -6.5436869 -6.2587581 -5.3249269][-4.6265187 -3.5756555 -2.0610988 -0.29812431 1.5314412 3.1162906 4.3304529 4.52734 3.7610683 1.7131591 -0.95914435 -3.4250712 -5.042841 -5.5832777 -5.0692916][-4.0901423 -2.9334521 -1.2818439 0.78948641 2.9323115 4.9228048 6.4906034 7.1802578 6.9119244 4.870945 1.752667 -1.3776307 -3.7233415 -4.8092394 -4.8251934][-4.2153773 -3.1342926 -1.5980148 0.45214987 2.6045966 4.8138247 6.6851463 7.7334127 7.8366365 6.0952463 3.0892062 -0.35713816 -3.1730046 -4.57574 -4.6924758][-4.8901668 -3.9703612 -2.5978556 -1.0209563 0.77526951 2.8226361 4.8659391 6.231348 6.4628906 4.9647131 2.3213367 -0.80463052 -3.2921817 -4.7082496 -4.86602][-6.2691975 -5.6414251 -4.6126633 -3.4500678 -1.8540447 -0.17744446 1.3995857 2.6669159 3.0845108 1.9463916 -0.034252644 -2.4136577 -4.1278596 -5.0596008 -5.2185912][-7.830585 -7.5176516 -6.8404722 -5.9232593 -4.9806795 -3.6209121 -2.2011795 -1.4824572 -1.3467438 -1.8805954 -2.8674159 -4.2923174 -5.2051907 -5.5492153 -5.5209446][-8.8349686 -8.9672222 -8.7801619 -8.0859375 -7.4890552 -6.6934047 -5.8020515 -5.168467 -4.927083 -5.146678 -5.3614388 -5.7124987 -5.9294157 -5.9555011 -5.8198571][-8.650322 -9.06439 -9.188324 -9.3041344 -9.3919306 -9.0154409 -8.62122 -8.2441559 -7.8344207 -7.4149332 -6.888566 -6.44812 -6.0438128 -5.9218216 -5.8868713][-7.3519411 -7.8327894 -8.17363 -8.6269741 -9.0762682 -9.3005533 -9.479393 -9.3474646 -8.8989592 -8.168438 -7.2374139 -6.4986124 -5.9269471 -5.5065513 -5.3739967][-6.0150146 -6.2445374 -6.627738 -7.1315918 -7.6470747 -8.1246548 -8.4260731 -8.4165068 -8.0818577 -7.4461536 -6.5568247 -5.7097764 -5.0820036 -4.8357773 -4.7400885]]...]
INFO - root - 2017-12-16 09:15:40.662354: step 70210, loss = 0.25, batch loss = 0.19 (28.9 examples/sec; 0.277 sec/batch; 20h:09m:56s remains)
INFO - root - 2017-12-16 09:15:43.534912: step 70220, loss = 0.31, batch loss = 0.25 (28.8 examples/sec; 0.278 sec/batch; 20h:14m:30s remains)
INFO - root - 2017-12-16 09:15:46.331738: step 70230, loss = 0.23, batch loss = 0.17 (27.9 examples/sec; 0.287 sec/batch; 20h:53m:45s remains)
INFO - root - 2017-12-16 09:15:49.128704: step 70240, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 20h:22m:32s remains)
INFO - root - 2017-12-16 09:15:51.947722: step 70250, loss = 0.27, batch loss = 0.21 (26.9 examples/sec; 0.297 sec/batch; 21h:39m:50s remains)
INFO - root - 2017-12-16 09:15:54.802257: step 70260, loss = 0.23, batch loss = 0.17 (27.9 examples/sec; 0.286 sec/batch; 20h:51m:18s remains)
INFO - root - 2017-12-16 09:15:57.606929: step 70270, loss = 0.33, batch loss = 0.27 (28.1 examples/sec; 0.284 sec/batch; 20h:42m:54s remains)
INFO - root - 2017-12-16 09:16:00.430161: step 70280, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 20h:49m:44s remains)
INFO - root - 2017-12-16 09:16:03.233544: step 70290, loss = 0.26, batch loss = 0.20 (28.5 examples/sec; 0.281 sec/batch; 20h:27m:35s remains)
INFO - root - 2017-12-16 09:16:06.049669: step 70300, loss = 0.30, batch loss = 0.24 (28.2 examples/sec; 0.284 sec/batch; 20h:40m:11s remains)
2017-12-16 09:16:06.637731: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.491637 -2.6195936 -2.9183545 -3.1980438 -3.4566119 -3.6296039 -3.6135712 -3.541482 -3.3950934 -3.2816434 -3.2697098 -3.3578877 -3.3693593 -3.1105404 -2.9036541][-2.3618209 -2.4844561 -2.8225057 -3.1571195 -3.4314344 -3.4528484 -3.2672715 -3.0828848 -2.8674614 -2.71283 -2.6304998 -2.7730997 -2.9253323 -2.717 -2.7378054][-1.9406335 -2.1110501 -2.4792905 -2.8837676 -3.1523929 -3.1285207 -2.8845208 -2.5984497 -2.3017373 -2.0754056 -2.0564063 -2.0802941 -2.0398629 -1.9403212 -2.0160067][-1.7955976 -2.0552168 -2.4586868 -2.6848559 -2.7091565 -2.6959898 -2.4253201 -2.0926459 -1.7630143 -1.5703955 -1.5102949 -1.4795358 -1.5695052 -1.7385244 -2.0645177][-1.5854523 -1.8050685 -2.1276574 -2.3528361 -2.4127417 -2.2590706 -1.9423509 -1.7854979 -1.6119373 -1.3798358 -1.2972834 -1.2699616 -1.4544432 -1.7951112 -2.3644862][-1.4922018 -1.687284 -1.9652448 -2.0595818 -1.9567337 -1.7614431 -1.4988039 -1.2983181 -1.1310744 -1.0076575 -1.0389688 -1.074512 -1.2963123 -1.8141906 -2.5280261][-1.3374579 -1.3962207 -1.5152681 -1.3767366 -1.0779493 -0.8103447 -0.50216317 -0.46487641 -0.49680972 -0.49568367 -0.55874777 -0.72474527 -1.0480089 -1.4751649 -2.163579][-1.2698689 -1.2002146 -1.1011691 -0.91080952 -0.65204358 -0.37659502 -0.12298346 -0.10415173 -0.16343737 -0.13164043 -0.10811853 -0.20870113 -0.44291973 -0.97166896 -1.7690837][-1.4044538 -1.3103929 -1.1930656 -1.018255 -0.76308465 -0.44396806 -0.27347088 -0.24144554 -0.24650097 -0.097790241 0.074473381 0.10365629 -0.043952465 -0.59302449 -1.4067249][-1.7909296 -1.687382 -1.5708611 -1.494137 -1.2954035 -1.1533003 -1.05953 -0.8132205 -0.73714328 -0.43175697 -0.083144665 -0.037700176 -0.23309565 -0.78556275 -1.500025][-2.4509053 -2.3996737 -2.3574286 -2.4741013 -2.4156756 -2.2702229 -2.1636841 -1.9045627 -1.8355703 -1.3818939 -0.93646836 -0.8178103 -0.95909452 -1.5984807 -2.2006829][-3.4210358 -3.4434161 -3.4795368 -3.7529407 -3.8777137 -3.7765284 -3.6856387 -3.2598846 -2.9782758 -2.5136189 -2.1471765 -2.0163426 -2.1088324 -2.7481489 -3.1903293][-4.790884 -4.8965111 -5.0529623 -5.3961062 -5.6090159 -5.4856987 -5.3050203 -4.7370048 -4.36067 -3.9149907 -3.5592835 -3.6012692 -3.823493 -4.202579 -4.2937355][-6.1106443 -6.1106873 -6.1902328 -6.425683 -6.5026922 -6.3808455 -6.2181125 -5.7808223 -5.552784 -5.2195592 -4.9913096 -5.1220465 -5.2974625 -5.38492 -5.2081747][-6.84334 -6.8330355 -6.92939 -7.0987635 -7.0735846 -6.9410758 -6.8340654 -6.4898548 -6.3792472 -6.26996 -6.2284231 -6.3169646 -6.3284774 -6.0654421 -5.5477953]]...]
INFO - root - 2017-12-16 09:16:09.514675: step 70310, loss = 0.29, batch loss = 0.23 (27.5 examples/sec; 0.291 sec/batch; 21h:09m:36s remains)
INFO - root - 2017-12-16 09:16:12.358083: step 70320, loss = 0.37, batch loss = 0.31 (26.4 examples/sec; 0.303 sec/batch; 22h:05m:51s remains)
INFO - root - 2017-12-16 09:16:15.246903: step 70330, loss = 0.26, batch loss = 0.20 (26.9 examples/sec; 0.297 sec/batch; 21h:39m:45s remains)
INFO - root - 2017-12-16 09:16:18.102203: step 70340, loss = 0.31, batch loss = 0.25 (28.0 examples/sec; 0.285 sec/batch; 20h:46m:29s remains)
INFO - root - 2017-12-16 09:16:20.908652: step 70350, loss = 0.27, batch loss = 0.22 (29.1 examples/sec; 0.275 sec/batch; 20h:02m:01s remains)
INFO - root - 2017-12-16 09:16:23.735565: step 70360, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 20h:12m:24s remains)
INFO - root - 2017-12-16 09:16:26.527440: step 70370, loss = 0.24, batch loss = 0.18 (28.9 examples/sec; 0.277 sec/batch; 20h:08m:11s remains)
INFO - root - 2017-12-16 09:16:29.385445: step 70380, loss = 0.38, batch loss = 0.33 (28.2 examples/sec; 0.284 sec/batch; 20h:39m:17s remains)
INFO - root - 2017-12-16 09:16:32.216048: step 70390, loss = 0.29, batch loss = 0.24 (28.5 examples/sec; 0.281 sec/batch; 20h:26m:18s remains)
INFO - root - 2017-12-16 09:16:35.113733: step 70400, loss = 0.25, batch loss = 0.19 (27.9 examples/sec; 0.286 sec/batch; 20h:51m:20s remains)
2017-12-16 09:16:35.681103: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8846936 -5.0882611 -3.9462342 -2.9191828 -2.0728664 -1.7691872 -1.6716127 -1.9888699 -2.8840637 -3.7031975 -4.2067032 -4.5476241 -4.5115075 -4.0477967 -3.4197578][-5.1602745 -4.4022589 -3.4024978 -2.4343719 -1.6682973 -1.3280516 -1.2995183 -1.7135837 -2.5303738 -3.3492568 -4.0334091 -4.3007483 -4.272738 -4.0096931 -3.4181132][-4.2252903 -3.5140581 -2.6864867 -1.8464377 -1.2642877 -1.0332649 -1.0235653 -1.4340534 -2.3552718 -3.3708444 -4.1214156 -4.6681213 -4.7176428 -4.3699045 -3.7305732][-3.0284758 -2.5190623 -1.788121 -1.2515993 -0.82142067 -0.62234306 -0.62829685 -1.0414414 -1.9951973 -3.0718117 -4.01526 -4.5884514 -4.735363 -4.3264112 -3.7155387][-1.9528079 -1.5878639 -0.9995935 -0.65141869 -0.058642864 0.46265316 0.68040895 0.42947769 -0.60606766 -1.9350488 -3.2578163 -4.2044926 -4.5667477 -4.4168983 -3.779552][-1.6177497 -1.4122593 -0.96511388 -0.52489471 0.35818529 1.3112345 1.9647055 1.9017234 1.0111961 -0.60287237 -2.3139384 -3.7180376 -4.5631418 -4.7313795 -4.2682538][-1.6408339 -1.6063631 -1.2371135 -0.68858218 0.39477825 1.6804113 2.6100683 2.6884804 1.7657342 0.034668922 -1.9989021 -3.8522763 -4.9422784 -5.303421 -4.9814506][-1.9840095 -1.8831654 -1.5314431 -0.8564055 0.49984646 1.7957978 2.6810842 2.9295845 2.1040344 0.24466658 -2.0093246 -3.9104977 -5.200284 -5.7105064 -5.3596678][-2.3204186 -2.2971141 -1.9270148 -1.3838847 -0.059447289 1.3357306 2.4057665 2.7157078 1.8835521 0.22605658 -1.952323 -4.0410004 -5.5367613 -6.1313562 -5.8091035][-2.4396179 -2.5425582 -2.2905564 -1.8110116 -0.84827471 0.33315897 1.3749642 1.8059068 1.1882501 -0.36714172 -2.450052 -4.4045358 -5.8750691 -6.5630817 -6.2518787][-2.1360745 -2.4972491 -2.7283878 -2.5177603 -1.6847055 -0.56118107 0.3607316 0.76270819 0.28629923 -1.1632245 -3.040844 -4.9008427 -6.2113495 -6.7293859 -6.4286237][-2.1976104 -2.7111714 -2.9762087 -2.902266 -2.2851443 -1.3109841 -0.52856278 -0.21555471 -0.697757 -1.9598577 -3.6611018 -5.3272696 -6.3962517 -6.6401315 -6.1593351][-2.1559234 -2.8512912 -3.2190008 -3.1484923 -2.4802861 -1.6582611 -1.0853584 -0.76412058 -1.3387043 -2.5126414 -4.0032239 -5.37605 -6.0818362 -6.2088909 -5.7447662][-2.1423137 -2.8401372 -3.3011422 -3.2158377 -2.6678987 -1.8554368 -1.2798107 -1.2439399 -1.7971511 -2.8122566 -4.1443148 -5.2099266 -5.7680931 -5.7563424 -5.1495991][-2.6890402 -3.3057241 -3.566426 -3.314126 -2.7094889 -1.8994963 -1.4296207 -1.4445283 -1.9630008 -3.0422082 -4.180438 -5.0961065 -5.487123 -5.2809668 -4.5818791]]...]
INFO - root - 2017-12-16 09:16:38.500653: step 70410, loss = 0.31, batch loss = 0.25 (29.2 examples/sec; 0.274 sec/batch; 19h:57m:28s remains)
INFO - root - 2017-12-16 09:16:41.372714: step 70420, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.284 sec/batch; 20h:40m:23s remains)
INFO - root - 2017-12-16 09:16:44.210983: step 70430, loss = 0.28, batch loss = 0.22 (29.4 examples/sec; 0.272 sec/batch; 19h:50m:01s remains)
INFO - root - 2017-12-16 09:16:47.011577: step 70440, loss = 0.20, batch loss = 0.14 (28.5 examples/sec; 0.281 sec/batch; 20h:27m:31s remains)
INFO - root - 2017-12-16 09:16:49.831541: step 70450, loss = 0.34, batch loss = 0.28 (26.7 examples/sec; 0.300 sec/batch; 21h:48m:54s remains)
INFO - root - 2017-12-16 09:16:52.648941: step 70460, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.281 sec/batch; 20h:27m:08s remains)
INFO - root - 2017-12-16 09:16:55.482135: step 70470, loss = 0.29, batch loss = 0.23 (28.3 examples/sec; 0.282 sec/batch; 20h:32m:31s remains)
INFO - root - 2017-12-16 09:16:58.360177: step 70480, loss = 0.25, batch loss = 0.19 (27.7 examples/sec; 0.289 sec/batch; 21h:01m:02s remains)
INFO - root - 2017-12-16 09:17:01.249030: step 70490, loss = 0.38, batch loss = 0.32 (28.9 examples/sec; 0.277 sec/batch; 20h:07m:34s remains)
INFO - root - 2017-12-16 09:17:04.074934: step 70500, loss = 0.25, batch loss = 0.19 (29.0 examples/sec; 0.276 sec/batch; 20h:06m:05s remains)
2017-12-16 09:17:04.647455: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6818953 -4.3768873 -4.2238183 -4.0896091 -4.3363414 -4.4091487 -4.4606323 -4.4362063 -4.2040734 -3.8185415 -3.2615695 -2.8430524 -2.5229726 -2.4353824 -2.6284618][-3.4959202 -3.1748288 -3.1060395 -3.1994619 -3.5082049 -3.8407388 -4.0350533 -4.0664792 -3.8383074 -3.697036 -3.4069548 -3.1039965 -2.8679113 -2.8498187 -3.0100095][-2.1117775 -1.9915736 -2.1337261 -2.3201616 -2.6987379 -3.1031294 -3.4031196 -3.5651572 -3.6654096 -3.742008 -3.829402 -3.5936751 -3.3543687 -3.3717732 -3.5798264][-0.72680712 -0.9174509 -1.3694534 -1.6721301 -1.9690261 -2.295969 -2.423296 -2.585732 -2.79035 -3.1857738 -3.6304579 -3.8163762 -3.956264 -4.0369091 -4.08313][0.12605381 -0.25041628 -0.87595224 -1.3909876 -1.6441271 -1.6587107 -1.3410101 -1.1743259 -1.2746763 -1.9179404 -2.672848 -3.1927748 -3.6459339 -3.9632783 -4.1420412][0.089966774 -0.39540768 -0.99848557 -1.3130934 -1.0118668 -0.73935342 -0.27641916 0.11757755 0.23417282 -0.42092323 -1.2542248 -2.1660659 -2.8494196 -3.3375854 -3.5261023][-0.40306091 -0.6754353 -1.1511736 -1.1708674 -0.47200942 0.40141678 1.3381376 1.8068066 1.8356357 1.1735559 0.16439342 -0.95933223 -1.9160628 -2.5642538 -2.8233032][-1.2852283 -1.4936554 -1.7096884 -1.3631778 -0.50297284 0.76674747 2.11694 2.9125867 3.0579591 2.2669482 1.1319785 -0.23040771 -1.3736436 -2.07017 -2.3347015][-2.3194616 -2.5231481 -2.7624197 -2.3495331 -1.4537737 0.029541969 1.4565182 2.2801747 2.6314588 2.0073843 0.92152882 -0.50313735 -1.7111464 -2.2702 -2.4665117][-2.9312689 -3.2160017 -3.6422603 -3.3046732 -2.525564 -1.346422 -0.22090244 0.44759607 0.76460791 0.17369509 -0.78016281 -1.990309 -2.8924224 -3.0679278 -2.9173326][-3.6839018 -3.8112624 -3.8473878 -3.7022533 -3.270946 -2.3055966 -1.3590217 -1.1454227 -1.3390729 -1.7735775 -2.3862364 -3.2693849 -4.0299873 -4.1091866 -3.7619238][-3.8689601 -4.0298414 -4.394908 -4.351603 -3.9409361 -3.4207015 -2.8665924 -2.5976744 -2.4845433 -2.8789003 -3.4986048 -4.2707462 -4.7384405 -4.4716692 -4.0441008][-3.7176235 -3.7241344 -4.0282207 -4.2293358 -4.2164936 -3.9753177 -3.5328903 -3.491889 -3.5047667 -3.5789187 -3.8960109 -4.2127657 -4.5247107 -4.3912692 -4.2114668][-3.6838291 -3.4025784 -3.4385731 -3.5769114 -3.5945992 -3.4373803 -3.1609275 -3.2995515 -3.3440917 -3.1941466 -3.2068031 -3.4027948 -3.8136122 -3.8943281 -3.9174323][-3.555469 -3.036839 -2.8780317 -2.9212756 -2.9274185 -2.8220148 -2.7097464 -2.691884 -2.5028265 -2.3363659 -2.332638 -2.2326224 -2.3816168 -2.8542008 -3.4097757]]...]
INFO - root - 2017-12-16 09:17:07.454089: step 70510, loss = 0.30, batch loss = 0.24 (29.5 examples/sec; 0.271 sec/batch; 19h:42m:33s remains)
INFO - root - 2017-12-16 09:17:10.299259: step 70520, loss = 0.33, batch loss = 0.27 (28.1 examples/sec; 0.285 sec/batch; 20h:44m:19s remains)
INFO - root - 2017-12-16 09:17:13.132049: step 70530, loss = 0.24, batch loss = 0.18 (27.0 examples/sec; 0.296 sec/batch; 21h:34m:08s remains)
INFO - root - 2017-12-16 09:17:15.923051: step 70540, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.277 sec/batch; 20h:07m:40s remains)
INFO - root - 2017-12-16 09:17:18.755697: step 70550, loss = 0.24, batch loss = 0.18 (27.2 examples/sec; 0.294 sec/batch; 21h:22m:45s remains)
INFO - root - 2017-12-16 09:17:21.587432: step 70560, loss = 0.45, batch loss = 0.40 (29.1 examples/sec; 0.274 sec/batch; 19h:58m:17s remains)
INFO - root - 2017-12-16 09:17:24.425204: step 70570, loss = 0.25, batch loss = 0.19 (28.7 examples/sec; 0.279 sec/batch; 20h:18m:31s remains)
INFO - root - 2017-12-16 09:17:27.263727: step 70580, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.285 sec/batch; 20h:45m:44s remains)
INFO - root - 2017-12-16 09:17:30.080656: step 70590, loss = 0.25, batch loss = 0.19 (28.8 examples/sec; 0.278 sec/batch; 20h:12m:49s remains)
INFO - root - 2017-12-16 09:17:32.877382: step 70600, loss = 0.36, batch loss = 0.31 (28.8 examples/sec; 0.278 sec/batch; 20h:13m:41s remains)
2017-12-16 09:17:33.429753: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6939697 -4.470715 -5.2452545 -5.9925785 -6.4790215 -6.6577811 -6.8879929 -7.5920773 -8.1483707 -8.51119 -8.7367582 -8.8988419 -8.6208954 -8.0292425 -7.4141541][-4.5150838 -5.5869913 -6.4401116 -7.2267923 -7.8059468 -8.29257 -8.4946489 -8.7341957 -9.0184612 -9.6025972 -9.9929419 -9.8559093 -9.5499344 -8.8777466 -8.0601616][-5.1634865 -6.2733164 -7.2127838 -7.8301678 -7.9699612 -8.0258818 -7.8790951 -8.0529938 -8.472023 -8.8470078 -9.1505814 -9.4139566 -9.3689 -8.4969015 -7.66142][-5.4418926 -6.7442646 -7.6091433 -7.9720926 -7.5280671 -6.7487268 -5.929842 -5.4735551 -5.3478003 -6.0181694 -6.9840145 -7.5674453 -7.6354055 -7.1938734 -6.6372523][-5.46233 -6.3852382 -6.9414835 -6.8303404 -6.0026555 -4.6955285 -3.0387049 -1.9092317 -1.8740194 -2.5319824 -3.4700868 -4.3586125 -4.9537997 -4.9794683 -4.790379][-4.6771364 -5.4280543 -5.5964408 -5.1363029 -3.4179039 -1.3014147 0.53685045 1.8709288 2.3061948 1.1828685 -0.56609964 -1.6024499 -1.7487316 -2.0450683 -2.3919897][-4.0580382 -4.3541918 -4.1454525 -3.2271323 -1.4573572 0.8314085 3.2706385 4.9093084 4.7123871 3.5412264 2.127758 0.7919755 -0.022580147 -0.3936305 -0.56264639][-3.7667027 -3.8886576 -3.5308309 -2.3512487 -0.18844509 2.4039927 4.4211483 5.5470018 5.5265551 4.4577932 2.9980426 1.9449792 1.4582386 0.66267204 0.048966408][-3.4506972 -3.4742446 -2.9688931 -1.8572736 0.020709038 1.9354901 3.6299105 4.5442057 4.29383 3.5778379 2.5924177 1.5433712 0.79508495 0.17923307 -0.35257149][-3.2690592 -3.4069054 -3.2020729 -2.3138087 -0.93316221 0.58444834 1.7301211 2.0761309 1.960041 1.4536238 0.74207306 0.12704563 -0.26520729 -0.6952107 -1.0016298][-3.6493177 -3.8323812 -3.8045177 -3.3516183 -2.3426962 -1.4954939 -0.77993488 -0.57601452 -0.7534163 -1.1984146 -1.7006266 -2.0431452 -2.4038966 -2.7033114 -3.0157285][-4.0960383 -4.2709832 -4.2164702 -4.1048937 -3.9006326 -3.741101 -3.5225992 -3.5146093 -3.5657773 -3.9893417 -4.3921385 -4.7769713 -5.0556464 -5.0503244 -5.0425963][-4.7169862 -4.9937363 -5.1572027 -5.3119769 -5.2986512 -5.3998828 -5.484776 -5.6548438 -5.7809038 -5.8105087 -5.9848909 -6.0035224 -6.0247507 -6.0732384 -5.8752489][-4.5700283 -4.9625463 -5.2994552 -5.5389318 -5.7022767 -5.8933587 -6.0622563 -6.2522087 -6.3293247 -6.437921 -6.4135108 -6.6519308 -6.7852407 -6.636137 -6.3511972][-4.6151514 -4.7221704 -4.8798051 -5.2223177 -5.5196934 -5.8362803 -6.1533065 -6.2976947 -6.2766771 -6.24314 -6.3034892 -6.3141084 -6.2790456 -6.2647309 -6.1322908]]...]
INFO - root - 2017-12-16 09:17:36.226793: step 70610, loss = 0.26, batch loss = 0.20 (28.5 examples/sec; 0.281 sec/batch; 20h:24m:42s remains)
INFO - root - 2017-12-16 09:17:39.106322: step 70620, loss = 0.22, batch loss = 0.17 (28.1 examples/sec; 0.285 sec/batch; 20h:42m:13s remains)
INFO - root - 2017-12-16 09:17:41.961873: step 70630, loss = 0.30, batch loss = 0.24 (26.4 examples/sec; 0.303 sec/batch; 22h:03m:16s remains)
INFO - root - 2017-12-16 09:17:44.782653: step 70640, loss = 0.28, batch loss = 0.22 (28.5 examples/sec; 0.281 sec/batch; 20h:25m:48s remains)
INFO - root - 2017-12-16 09:17:47.620737: step 70650, loss = 0.26, batch loss = 0.21 (27.7 examples/sec; 0.288 sec/batch; 20h:58m:30s remains)
INFO - root - 2017-12-16 09:17:50.424704: step 70660, loss = 0.21, batch loss = 0.15 (29.0 examples/sec; 0.276 sec/batch; 20h:04m:36s remains)
INFO - root - 2017-12-16 09:17:53.223405: step 70670, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.284 sec/batch; 20h:38m:51s remains)
INFO - root - 2017-12-16 09:17:56.072226: step 70680, loss = 0.42, batch loss = 0.36 (26.1 examples/sec; 0.307 sec/batch; 22h:17m:48s remains)
INFO - root - 2017-12-16 09:17:58.911322: step 70690, loss = 0.45, batch loss = 0.39 (27.7 examples/sec; 0.289 sec/batch; 20h:59m:47s remains)
INFO - root - 2017-12-16 09:18:01.733408: step 70700, loss = 0.35, batch loss = 0.29 (28.3 examples/sec; 0.283 sec/batch; 20h:34m:35s remains)
2017-12-16 09:18:02.217558: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3852158 -4.5836372 -4.8565407 -5.715023 -6.4670095 -6.8283105 -7.0538111 -7.1282663 -7.0589728 -7.0888486 -7.2644863 -7.5832829 -7.5747747 -7.5663614 -7.6237926][-3.8405139 -4.014864 -4.326376 -4.8134413 -5.3419056 -5.8475218 -6.2967424 -6.2319417 -5.8941755 -5.8216591 -6.03222 -6.1622272 -6.1649771 -6.2694311 -6.5440569][-2.9655566 -3.1009717 -3.2636948 -3.500423 -3.9634361 -4.3216252 -4.7103591 -4.8345938 -4.6671367 -4.3361588 -4.3222451 -4.4137983 -4.5008659 -4.478426 -4.6208482][-1.9777932 -1.8043752 -1.8766389 -1.8918242 -2.0528333 -2.4661264 -2.915525 -3.0817084 -2.7855768 -2.6117122 -2.2703817 -2.3056788 -2.6174254 -2.7747865 -2.8512797][-1.2983496 -0.68594408 -0.38450861 -0.24813843 -0.51659822 -0.75871706 -1.0801165 -1.5848532 -1.3760324 -1.1456349 -0.96616292 -0.83836985 -0.89207411 -1.0111458 -1.0985904][-1.4101985 -0.36071062 0.41367674 0.90985155 0.92882919 0.40750551 -0.045540333 -0.23043013 -0.20420027 -0.066718578 0.28209448 0.55562973 0.58965397 0.68253994 0.75507545][-2.1286762 -0.65009475 0.46432972 1.2218142 1.5187197 1.3205438 1.053112 0.69498539 0.61686039 0.63903427 0.96529531 1.4543052 1.8882451 2.1810036 2.0225677][-3.0208142 -1.3727472 -0.39368343 0.44823408 0.88928318 0.88129473 0.903656 0.78188658 0.86845541 0.78098154 0.83257914 1.2964568 1.7927146 2.1959953 2.5080781][-4.4199882 -3.1545603 -1.9463847 -1.1518426 -0.87893248 -0.71300673 -0.39337158 -0.24385643 0.034332752 0.48679495 0.7589283 0.74900961 0.97726536 1.3564892 1.3801041][-4.4357052 -3.7941251 -3.3862371 -2.7565036 -2.3197463 -2.149663 -1.9598777 -1.5273168 -1.0071936 -0.6613481 -0.18125486 0.1760788 0.28115177 0.24799061 0.054625034][-4.5357537 -4.2086744 -4.0529661 -3.9472294 -3.8300278 -3.5071502 -3.1654613 -2.9477649 -2.4547772 -1.9723284 -1.6802328 -1.5540435 -1.4424825 -1.6500037 -2.0823791][-4.6283522 -4.42521 -4.5671411 -4.8641372 -5.0344176 -4.971612 -4.6933842 -4.3025208 -3.9807079 -3.6697965 -3.4396505 -3.4379544 -3.6158166 -3.8221714 -4.2236853][-4.7416716 -4.5559506 -4.7142758 -5.1272526 -5.6963091 -5.910203 -5.7809048 -5.4117975 -4.9475627 -4.6250639 -4.3152571 -4.326704 -4.7237062 -5.0019622 -5.1467814][-4.3438721 -4.3586912 -4.7344646 -5.0815954 -5.5477591 -5.9786425 -6.21594 -5.9262462 -5.4501405 -5.1201839 -4.8934226 -4.86395 -5.1167817 -5.5071735 -5.7645431][-3.8440681 -3.9136987 -4.2778444 -4.7170739 -5.3048859 -5.76275 -6.0227976 -5.7801852 -5.4272885 -5.2245755 -5.1309719 -5.1958246 -5.19539 -5.1640015 -5.1991644]]...]
INFO - root - 2017-12-16 09:18:05.053184: step 70710, loss = 0.38, batch loss = 0.32 (26.4 examples/sec; 0.303 sec/batch; 22h:03m:47s remains)
INFO - root - 2017-12-16 09:18:07.889565: step 70720, loss = 0.26, batch loss = 0.21 (29.0 examples/sec; 0.276 sec/batch; 20h:03m:12s remains)
INFO - root - 2017-12-16 09:18:10.678054: step 70730, loss = 0.20, batch loss = 0.15 (29.3 examples/sec; 0.273 sec/batch; 19h:51m:49s remains)
INFO - root - 2017-12-16 09:18:13.513478: step 70740, loss = 0.33, batch loss = 0.28 (27.9 examples/sec; 0.286 sec/batch; 20h:49m:44s remains)
INFO - root - 2017-12-16 09:18:16.332938: step 70750, loss = 0.21, batch loss = 0.15 (28.5 examples/sec; 0.281 sec/batch; 20h:23m:46s remains)
INFO - root - 2017-12-16 09:18:19.176147: step 70760, loss = 0.26, batch loss = 0.20 (27.0 examples/sec; 0.296 sec/batch; 21h:31m:23s remains)
INFO - root - 2017-12-16 09:18:21.995617: step 70770, loss = 0.27, batch loss = 0.21 (28.6 examples/sec; 0.280 sec/batch; 20h:21m:07s remains)
INFO - root - 2017-12-16 09:18:24.839268: step 70780, loss = 0.21, batch loss = 0.15 (28.7 examples/sec; 0.278 sec/batch; 20h:13m:52s remains)
INFO - root - 2017-12-16 09:18:27.688486: step 70790, loss = 0.42, batch loss = 0.36 (27.2 examples/sec; 0.294 sec/batch; 21h:24m:14s remains)
INFO - root - 2017-12-16 09:18:30.522703: step 70800, loss = 0.38, batch loss = 0.32 (28.7 examples/sec; 0.279 sec/batch; 20h:15m:54s remains)
2017-12-16 09:18:30.999060: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4028292 -4.8915267 -4.3889623 -4.2677917 -4.3928304 -4.5656986 -4.5445957 -4.6178412 -4.6730037 -4.7067323 -4.7298956 -4.7600069 -4.7998195 -4.8932247 -4.7775888][-5.5928726 -4.9316463 -4.4358215 -4.1906643 -4.1093698 -4.6178813 -4.73317 -4.7266498 -4.6116996 -4.7428389 -4.9005213 -4.7742763 -4.8072777 -5.0208845 -4.9023418][-5.6499209 -4.9606776 -4.1481996 -3.5193696 -3.5073724 -3.9231348 -4.123589 -4.3836 -4.3628559 -4.43078 -4.5437803 -4.72178 -5.0414443 -5.203577 -5.1156092][-5.0443048 -4.0544729 -3.1088471 -2.37983 -2.3058553 -2.4912953 -2.6965909 -2.968215 -3.114327 -3.5263965 -3.8766615 -4.3512549 -4.7918653 -5.1392555 -5.3273711][-4.6351128 -3.225769 -1.8551435 -0.813391 -0.29606104 -0.43988442 -0.69564462 -0.98121071 -1.2194495 -1.7773278 -2.4120872 -3.0136743 -3.7899542 -4.5830259 -4.9425845][-4.5073285 -2.8427203 -1.1116908 0.46647549 1.4824672 1.5431643 1.3616285 1.0843148 0.83489513 0.25552368 -0.68159652 -1.7701814 -2.7889853 -3.5820718 -4.2710142][-4.3750787 -2.6268568 -1.0187519 0.61151171 1.8953543 2.5639591 2.8224039 2.6228037 2.3175545 1.5083251 0.44918919 -0.85949636 -2.1184187 -3.1597636 -4.2033114][-4.70798 -3.1001344 -1.4117785 0.34818268 1.5314279 2.4495301 3.1817918 3.2587323 3.0732479 2.3171067 1.1530113 -0.23003483 -1.6088262 -2.9794731 -4.0928822][-5.4692545 -4.1340942 -2.6572826 -0.95219111 0.55377865 1.7119727 2.390048 2.6162109 2.5987959 2.050621 1.0449834 -0.18741846 -1.4776559 -2.6810155 -3.7848239][-6.4541411 -5.5155253 -4.5231791 -3.0552087 -1.4987528 -0.25593567 0.66104746 1.002059 0.92735434 0.30345345 -0.50888157 -1.4024713 -2.4469674 -3.3371241 -4.0821457][-7.8962932 -7.0830412 -6.1822824 -5.0831113 -3.9988277 -2.7253351 -1.5967438 -1.3651764 -1.3786762 -1.7713861 -2.3766255 -3.0618663 -3.8400424 -4.5374961 -4.9838848][-8.7588673 -7.95529 -7.3590784 -6.5032783 -5.5808783 -4.5886641 -3.8111477 -3.4075413 -3.1191225 -3.380352 -3.8318141 -4.3402033 -4.786108 -5.095005 -5.3114324][-8.6612644 -7.6493878 -7.0344315 -6.373045 -5.7414484 -5.1353631 -4.7960911 -4.6239781 -4.40018 -4.4796848 -4.5689387 -4.5804429 -4.9141431 -5.1192274 -4.9974022][-8.5672159 -7.1825886 -6.2309003 -5.5348821 -5.2534361 -5.1122713 -5.1483827 -5.2027826 -5.0221472 -4.8246841 -4.5910444 -4.4408059 -4.4573803 -4.4018564 -4.20101][-8.6050272 -6.9619107 -5.6671424 -4.89423 -4.6399951 -4.6421151 -4.9043884 -5.08941 -4.9959726 -4.8803034 -4.6220069 -4.1870723 -3.8080711 -3.6911776 -3.5876377]]...]
INFO - root - 2017-12-16 09:18:33.849145: step 70810, loss = 0.26, batch loss = 0.20 (28.8 examples/sec; 0.278 sec/batch; 20h:12m:26s remains)
INFO - root - 2017-12-16 09:18:36.764085: step 70820, loss = 0.19, batch loss = 0.13 (27.9 examples/sec; 0.287 sec/batch; 20h:52m:36s remains)
INFO - root - 2017-12-16 09:18:39.578323: step 70830, loss = 0.30, batch loss = 0.24 (28.3 examples/sec; 0.283 sec/batch; 20h:32m:07s remains)
INFO - root - 2017-12-16 09:18:42.443663: step 70840, loss = 0.31, batch loss = 0.25 (28.5 examples/sec; 0.280 sec/batch; 20h:22m:59s remains)
INFO - root - 2017-12-16 09:18:45.255426: step 70850, loss = 0.29, batch loss = 0.23 (28.2 examples/sec; 0.284 sec/batch; 20h:37m:19s remains)
INFO - root - 2017-12-16 09:18:48.086204: step 70860, loss = 0.27, batch loss = 0.21 (27.9 examples/sec; 0.287 sec/batch; 20h:51m:55s remains)
INFO - root - 2017-12-16 09:18:50.903890: step 70870, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 20h:15m:23s remains)
INFO - root - 2017-12-16 09:18:53.707798: step 70880, loss = 0.25, batch loss = 0.19 (29.2 examples/sec; 0.274 sec/batch; 19h:55m:14s remains)
INFO - root - 2017-12-16 09:18:56.530754: step 70890, loss = 0.35, batch loss = 0.30 (28.8 examples/sec; 0.278 sec/batch; 20h:11m:58s remains)
INFO - root - 2017-12-16 09:18:59.371356: step 70900, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.285 sec/batch; 20h:41m:41s remains)
2017-12-16 09:18:59.936108: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.247721 -3.3297958 -3.3944974 -3.35935 -3.2715447 -3.2209563 -3.1903982 -3.1173248 -3.1111422 -3.1365225 -3.1507747 -3.1195717 -2.9579725 -2.7938218 -2.633852][-3.1787465 -3.3392835 -3.4999573 -3.6152878 -3.5981588 -3.5215929 -3.3862739 -3.2860315 -3.2906122 -3.4139357 -3.5672619 -3.5529559 -3.4159446 -3.2449126 -2.9593134][-3.0561635 -3.3875983 -3.6658635 -3.7403383 -3.6249578 -3.4906373 -3.3407569 -3.2668376 -3.3586993 -3.5681438 -3.8343229 -4.080514 -4.0686736 -3.8664293 -3.5761404][-2.9586647 -3.3073239 -3.6221843 -3.650229 -3.4600861 -3.1354494 -2.8377733 -2.6434667 -2.7270408 -3.1617632 -3.6698947 -4.18066 -4.4427066 -4.4004707 -4.072155][-2.5932221 -2.926753 -3.1272993 -3.0372915 -2.683177 -2.0810685 -1.5845828 -1.3912294 -1.6418753 -2.216059 -2.9818206 -3.764802 -4.2964945 -4.5287232 -4.3363237][-2.2449231 -2.378756 -2.4338756 -2.1531155 -1.5147884 -0.70194936 -0.033753395 0.30189848 0.090859413 -0.65259385 -1.7782018 -3.0636849 -4.0312319 -4.4798307 -4.4402595][-2.0586898 -2.0296786 -1.8865256 -1.3927925 -0.53069568 0.56609535 1.5742116 2.0632682 1.772882 0.73529387 -0.65822864 -2.1833565 -3.424181 -4.1478333 -4.3089752][-2.1841624 -1.992866 -1.8841307 -1.32778 -0.30230188 0.97679377 2.0914345 2.6662836 2.5380931 1.5070124 -0.07312727 -1.8445306 -3.2726467 -4.1312304 -4.3820105][-2.4173298 -2.3061757 -2.2259345 -1.9204319 -1.1923034 0.17974377 1.5320792 2.2004185 2.0431027 1.0262251 -0.48093843 -2.343519 -3.8272333 -4.5672646 -4.7424078][-2.6739345 -2.764966 -2.9744985 -2.8251808 -2.2173584 -1.2891135 -0.34734249 0.43489361 0.46629095 -0.5296495 -1.9073458 -3.4864154 -4.7935648 -5.3886547 -5.3629351][-3.0068152 -3.1625404 -3.5034521 -3.8461182 -3.802824 -3.0670075 -2.1582992 -1.8194821 -1.9159822 -2.5204043 -3.5972567 -4.9780064 -5.9713397 -6.3483977 -6.1598692][-3.1147442 -3.4868648 -4.0683165 -4.5571003 -4.83009 -4.7796164 -4.4996662 -4.1644936 -4.1155348 -4.521 -5.3382807 -6.299139 -6.9341221 -7.076128 -6.7234082][-2.9521503 -3.3531342 -4.0860825 -4.9818611 -5.7930222 -6.0927835 -6.0283237 -5.8190966 -5.87997 -6.1235337 -6.4861889 -6.9172997 -7.2987585 -7.4645014 -7.0734863][-2.5340829 -2.8752613 -3.5294709 -4.4369144 -5.2735014 -5.8089237 -6.0889225 -6.3054733 -6.3011084 -6.2045054 -6.4426746 -6.9640455 -7.2973919 -7.1759806 -6.8026733][-2.1584666 -2.4668217 -3.0286288 -3.901202 -4.7762761 -5.3691359 -5.6170645 -5.56467 -5.5345736 -5.6390381 -5.7974038 -6.0745177 -6.2733393 -6.4169731 -6.2933307]]...]
INFO - root - 2017-12-16 09:19:02.770294: step 70910, loss = 0.41, batch loss = 0.35 (29.0 examples/sec; 0.276 sec/batch; 20h:02m:20s remains)
INFO - root - 2017-12-16 09:19:05.640913: step 70920, loss = 0.23, batch loss = 0.17 (26.6 examples/sec; 0.300 sec/batch; 21h:48m:48s remains)
INFO - root - 2017-12-16 09:19:08.478000: step 70930, loss = 0.22, batch loss = 0.16 (27.6 examples/sec; 0.289 sec/batch; 21h:02m:04s remains)
INFO - root - 2017-12-16 09:19:11.336225: step 70940, loss = 0.25, batch loss = 0.19 (28.8 examples/sec; 0.278 sec/batch; 20h:11m:09s remains)
INFO - root - 2017-12-16 09:19:14.163982: step 70950, loss = 0.21, batch loss = 0.15 (28.6 examples/sec; 0.280 sec/batch; 20h:19m:39s remains)
INFO - root - 2017-12-16 09:19:17.013944: step 70960, loss = 0.26, batch loss = 0.20 (26.9 examples/sec; 0.297 sec/batch; 21h:36m:18s remains)
INFO - root - 2017-12-16 09:19:19.903434: step 70970, loss = 0.33, batch loss = 0.27 (26.0 examples/sec; 0.308 sec/batch; 22h:22m:31s remains)
INFO - root - 2017-12-16 09:19:22.732387: step 70980, loss = 0.21, batch loss = 0.15 (28.6 examples/sec; 0.280 sec/batch; 20h:21m:19s remains)
INFO - root - 2017-12-16 09:19:25.617145: step 70990, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 20h:44m:53s remains)
INFO - root - 2017-12-16 09:19:28.439430: step 71000, loss = 0.24, batch loss = 0.18 (27.5 examples/sec; 0.291 sec/batch; 21h:06m:57s remains)
2017-12-16 09:19:28.977853: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8023362 -5.374012 -5.7269969 -6.121345 -6.3649759 -6.2962255 -6.2224808 -6.3335819 -6.4516287 -6.1509094 -5.68668 -5.2613449 -4.99531 -4.831028 -4.7276273][-3.693866 -4.29508 -4.7141695 -4.857358 -4.720211 -4.426713 -4.3216887 -4.3351159 -4.5341735 -4.6886768 -4.632535 -4.4108639 -4.3160214 -4.1995788 -4.0482688][-2.6425347 -3.1572962 -3.6427102 -3.8151274 -3.6185827 -2.9778619 -2.4913189 -2.5016994 -2.8011839 -3.3443162 -3.957674 -4.1283984 -3.9471586 -3.7014513 -3.4665759][-1.6539598 -2.075238 -2.7915893 -2.8586597 -2.1170018 -1.2043524 -0.49181557 -0.54290986 -1.2751765 -2.2372396 -3.1688361 -3.7476947 -3.9114647 -3.4672487 -2.9137926][-0.96325135 -1.4568863 -2.0140309 -2.0629244 -1.2579801 -0.014508724 0.91352081 0.92029095 0.058429718 -1.4103036 -2.9189692 -3.6961596 -3.6727695 -3.2395327 -2.8739941][-0.76237369 -1.3398774 -1.8121202 -1.4607522 -0.14837265 1.1666741 2.03095 1.9005389 0.82102966 -0.85217237 -2.3875299 -3.3347747 -3.5240564 -3.0945563 -2.6915374][-0.86715221 -1.3048599 -1.6207864 -1.1237869 0.33265591 1.8380938 2.7464042 2.3299599 1.1512742 -0.47133565 -2.0238016 -2.8914104 -3.0356631 -2.8079705 -2.69421][-0.88014817 -1.396678 -1.6368375 -1.1861415 0.14867592 1.5080404 2.2350416 1.9414463 0.85826778 -0.64234042 -2.0030897 -2.6472666 -2.7887239 -2.7268476 -2.7252359][-0.5621736 -1.1390083 -1.5079534 -1.3352299 -0.41956902 0.7568593 1.3515143 1.0919933 0.18379688 -1.1179557 -2.26734 -2.8736048 -3.0361121 -3.0445297 -3.0794353][-0.38869286 -1.0451212 -1.6714144 -1.7576978 -1.1882412 -0.46590638 -0.12378216 -0.21389771 -0.79455781 -1.837333 -2.7881582 -3.3177333 -3.4260859 -3.4405282 -3.3441792][-0.61334205 -1.3307974 -2.0284538 -2.3225713 -2.1372085 -1.7746692 -1.4673045 -1.5559418 -2.0411108 -2.8743749 -3.5937693 -3.9747655 -4.0834379 -3.984143 -3.7223945][-1.2403595 -1.8171821 -2.6491585 -3.1070571 -3.1354961 -3.037631 -2.9867077 -3.0373921 -3.3424 -3.9019039 -4.5314417 -4.807013 -4.7966361 -4.553237 -4.1062555][-2.0586367 -2.4150765 -3.125922 -3.6168902 -3.9776871 -4.0243835 -4.0650229 -4.2304077 -4.5549135 -5.0337563 -5.355382 -5.5178576 -5.4155254 -5.0381651 -4.5008936][-2.9153233 -2.9047179 -3.4628577 -3.8990238 -4.4021549 -4.6421547 -4.8078752 -4.9957886 -5.1549854 -5.515811 -5.7355175 -5.7578869 -5.6611881 -5.3312955 -4.8884811][-3.8252773 -3.6220291 -3.9655242 -4.3408689 -4.7606282 -4.9840322 -5.0869675 -5.3002448 -5.5859709 -5.8446136 -6.019413 -6.043189 -5.9504981 -5.5962906 -5.1743546]]...]
INFO - root - 2017-12-16 09:19:31.877419: step 71010, loss = 0.27, batch loss = 0.21 (27.9 examples/sec; 0.287 sec/batch; 20h:50m:28s remains)
INFO - root - 2017-12-16 09:19:34.779586: step 71020, loss = 0.24, batch loss = 0.19 (27.7 examples/sec; 0.289 sec/batch; 20h:58m:44s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 09:19:37.629689: step 71030, loss = 0.32, batch loss = 0.27 (28.4 examples/sec; 0.282 sec/batch; 20h:28m:47s remains)
INFO - root - 2017-12-16 09:19:40.432371: step 71040, loss = 0.34, batch loss = 0.28 (28.5 examples/sec; 0.281 sec/batch; 20h:24m:11s remains)
INFO - root - 2017-12-16 09:19:43.254148: step 71050, loss = 0.30, batch loss = 0.24 (28.4 examples/sec; 0.282 sec/batch; 20h:27m:12s remains)
INFO - root - 2017-12-16 09:19:46.119806: step 71060, loss = 0.24, batch loss = 0.19 (27.5 examples/sec; 0.291 sec/batch; 21h:06m:53s remains)
INFO - root - 2017-12-16 09:19:48.967999: step 71070, loss = 0.26, batch loss = 0.20 (28.2 examples/sec; 0.284 sec/batch; 20h:37m:44s remains)
INFO - root - 2017-12-16 09:19:51.837052: step 71080, loss = 0.29, batch loss = 0.23 (28.2 examples/sec; 0.284 sec/batch; 20h:35m:37s remains)
INFO - root - 2017-12-16 09:19:54.651625: step 71090, loss = 0.31, batch loss = 0.25 (29.4 examples/sec; 0.272 sec/batch; 19h:46m:01s remains)
INFO - root - 2017-12-16 09:19:57.563485: step 71100, loss = 0.23, batch loss = 0.17 (27.4 examples/sec; 0.292 sec/batch; 21h:13m:27s remains)
2017-12-16 09:19:58.046410: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9133053 -5.9338121 -5.9292045 -6.139091 -6.1838422 -5.8742208 -5.4193087 -5.150301 -4.8200579 -4.3986998 -3.8813443 -3.3617508 -2.7746513 -2.2801311 -1.9641619][-5.6780415 -5.8019876 -5.7685308 -5.6624184 -5.4115496 -5.2941642 -5.0154443 -4.45637 -3.87741 -3.6013281 -3.2332644 -2.7163954 -2.189038 -1.8397074 -1.5644321][-4.96634 -5.076467 -4.9883475 -4.6524377 -4.1196795 -3.6581669 -3.2709699 -3.148834 -2.9728913 -2.7866278 -2.4995596 -2.2168069 -1.9433112 -1.6959109 -1.6839986][-3.7708879 -3.794075 -3.6349249 -3.0137389 -2.2978704 -1.7271283 -1.1933761 -0.98050451 -0.95720768 -1.3883445 -1.7634366 -2.0277956 -2.1881073 -2.095217 -2.1094294][-2.2970219 -2.1348135 -1.6717832 -0.98533177 -0.0961647 0.93172359 1.6132021 1.5531964 1.0971146 0.24374294 -0.67618084 -1.4870281 -2.0954621 -2.5532284 -2.682817][-1.0981944 -0.86434269 -0.26588297 1.0865102 2.5963416 3.6801386 4.2555761 4.1508579 3.3311787 1.9230752 0.41184187 -1.0778327 -2.2337914 -2.9040575 -3.1699364][-1.1465256 -0.5053122 0.28772783 1.7580233 3.5101376 5.3551674 6.5149164 6.2951565 5.0788383 3.1705823 1.0461702 -0.7958591 -2.2082019 -3.1495237 -3.5622985][-1.7464049 -1.5877488 -0.63359833 1.5424891 3.6344872 5.6491346 6.9279976 7.0066366 6.0431337 3.9284248 1.5352979 -0.61702418 -2.2365446 -3.3185906 -3.8678834][-3.5441623 -3.2664762 -2.4397974 -0.55648804 1.7583909 4.2474203 5.8895388 6.026247 5.0081034 3.1468658 1.026372 -1.0655172 -2.5428305 -3.3156085 -3.6733689][-5.0376153 -5.3720188 -5.0356789 -3.3497219 -1.4014866 0.59819317 2.2561622 2.9613762 2.4130177 0.928844 -0.82013011 -2.4515481 -3.577621 -4.0904188 -4.2476873][-6.9556704 -7.2523947 -6.91002 -6.0083184 -4.8382025 -2.9829369 -1.3018246 -0.71257544 -0.84416032 -1.6725061 -2.8325834 -4.0244861 -4.8892407 -5.1886463 -5.21616][-7.4618292 -8.1531906 -8.6458864 -7.9115458 -6.6619706 -5.4468508 -4.3071432 -3.6561742 -3.3836858 -3.7261229 -4.4069381 -5.4354639 -6.1143265 -6.1630011 -6.1092052][-7.0429244 -7.6471977 -8.0134068 -7.9729624 -7.6706314 -6.7787561 -5.8237724 -5.3367085 -5.1885338 -5.3108006 -5.54923 -6.0600281 -6.4814649 -6.499404 -6.3302803][-6.0192862 -6.2253618 -6.3547277 -6.4845471 -6.3918967 -5.896904 -5.4642954 -5.4333744 -5.4911623 -5.376967 -5.4481678 -5.9315929 -6.2547722 -6.1072817 -5.9076538][-4.6512203 -4.4494157 -4.3020773 -4.2238593 -4.0878997 -3.9335601 -3.8157818 -3.9458277 -4.2184577 -4.5257335 -4.84234 -5.1747885 -5.3811817 -5.4252238 -5.3530431]]...]
INFO - root - 2017-12-16 09:20:00.904768: step 71110, loss = 0.34, batch loss = 0.29 (27.4 examples/sec; 0.291 sec/batch; 21h:09m:46s remains)
INFO - root - 2017-12-16 09:20:03.754312: step 71120, loss = 0.25, batch loss = 0.20 (27.7 examples/sec; 0.289 sec/batch; 20h:58m:48s remains)
INFO - root - 2017-12-16 09:20:06.592210: step 71130, loss = 0.27, batch loss = 0.21 (28.8 examples/sec; 0.278 sec/batch; 20h:11m:46s remains)
INFO - root - 2017-12-16 09:20:09.467066: step 71140, loss = 0.27, batch loss = 0.21 (27.5 examples/sec; 0.291 sec/batch; 21h:09m:22s remains)
INFO - root - 2017-12-16 09:20:12.327165: step 71150, loss = 0.49, batch loss = 0.44 (28.8 examples/sec; 0.278 sec/batch; 20h:10m:39s remains)
INFO - root - 2017-12-16 09:20:15.133235: step 71160, loss = 0.34, batch loss = 0.28 (28.9 examples/sec; 0.277 sec/batch; 20h:05m:42s remains)
INFO - root - 2017-12-16 09:20:17.972325: step 71170, loss = 0.23, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 20h:09m:56s remains)
INFO - root - 2017-12-16 09:20:20.868738: step 71180, loss = 0.27, batch loss = 0.21 (28.3 examples/sec; 0.282 sec/batch; 20h:29m:08s remains)
INFO - root - 2017-12-16 09:20:23.799562: step 71190, loss = 0.37, batch loss = 0.31 (27.1 examples/sec; 0.295 sec/batch; 21h:24m:33s remains)
INFO - root - 2017-12-16 09:20:26.689646: step 71200, loss = 0.22, batch loss = 0.16 (28.7 examples/sec; 0.279 sec/batch; 20h:16m:01s remains)
2017-12-16 09:20:27.215774: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5861483 -1.9320328 -2.2576702 -2.3411448 -2.3331289 -2.3879077 -2.4935591 -2.4727271 -2.547719 -2.8395641 -3.1662626 -3.6728749 -4.1276026 -4.5852318 -4.6742821][-1.9090807 -2.2385736 -2.7866395 -3.1989174 -3.3888326 -3.4746923 -3.6148152 -3.7627211 -4.0317326 -4.2259026 -4.5131917 -4.8411331 -5.0623393 -5.1252422 -4.954535][-2.9443569 -3.4288173 -4.0427871 -4.4950976 -4.7319078 -4.8732977 -4.999465 -5.06126 -5.2365651 -5.4561296 -5.7738581 -5.91203 -5.9834867 -5.7746315 -5.3767028][-4.3649735 -4.7216353 -5.0961494 -5.4101844 -5.4081349 -5.2376428 -5.0320787 -4.9637575 -5.0669727 -5.2948837 -5.6888642 -5.8630705 -5.9119363 -5.7297273 -5.2908826][-5.2946949 -5.6096668 -5.9212651 -5.8479705 -5.4131722 -4.7922568 -4.0096645 -3.5117958 -3.3019049 -3.4571166 -3.8893893 -4.4946089 -5.064373 -5.1319857 -4.9255896][-5.5867276 -5.75045 -5.8437824 -5.4402008 -4.6624551 -3.5799477 -2.3076026 -1.3668256 -0.77349043 -0.74299526 -1.2586603 -2.1028638 -3.155066 -3.9079731 -4.2837129][-4.951283 -4.9515753 -4.8585305 -4.2250996 -3.1292224 -1.6162412 0.18214035 1.630331 2.4683304 2.443985 1.7390003 0.49134302 -1.039032 -2.4117937 -3.3860853][-3.5619597 -3.4919331 -3.322104 -2.7512968 -1.6535528 -0.07464838 1.9169259 3.6356325 4.8173 4.9446688 4.0516806 2.4486876 0.51073837 -1.3897345 -2.8063502][-1.7942376 -1.8962333 -2.008426 -1.6522932 -0.83713293 0.37396049 1.9399114 3.495213 4.7929287 5.014555 4.1415424 2.4977984 0.39781523 -1.6533141 -3.2264466][-1.3939509 -1.2361817 -1.3056424 -1.2176681 -0.80773973 0.12366199 1.3216991 2.3989029 3.1746793 3.209867 2.583075 1.188612 -0.759259 -2.6107528 -4.0648136][-1.3710124 -1.1302745 -1.0785811 -1.060992 -0.99305248 -0.39367867 0.44754267 1.001533 1.3253908 1.2033358 0.59291506 -0.51724243 -2.0309989 -3.4224706 -4.5927668][-1.9514236 -1.4374189 -1.2393222 -1.2118306 -1.3545074 -1.237011 -0.89745378 -0.56108761 -0.31883049 -0.65428138 -1.136152 -1.9804075 -3.0947065 -4.0114608 -4.8793254][-2.6678815 -1.9505837 -1.6365056 -1.3863595 -1.5205638 -1.6545832 -1.6663859 -1.5680022 -1.4874859 -1.7494328 -2.0859451 -2.6610615 -3.3669081 -3.9846466 -4.6194773][-3.2877188 -2.121558 -1.2737734 -0.80134583 -0.95619774 -1.0675416 -1.1530216 -1.3052239 -1.3656206 -1.4498332 -1.6634767 -2.1874833 -2.8050032 -3.3004069 -3.9471688][-3.0482774 -1.8651667 -0.98688412 -0.39339685 -0.23781872 -0.2849493 -0.3919673 -0.32005739 -0.2837224 -0.43523717 -0.80906343 -1.366405 -2.04736 -2.7479289 -3.4755721]]...]
INFO - root - 2017-12-16 09:20:30.099837: step 71210, loss = 0.28, batch loss = 0.22 (28.7 examples/sec; 0.278 sec/batch; 20h:12m:21s remains)
INFO - root - 2017-12-16 09:20:32.933280: step 71220, loss = 0.24, batch loss = 0.18 (29.4 examples/sec; 0.272 sec/batch; 19h:45m:21s remains)
INFO - root - 2017-12-16 09:20:35.777109: step 71230, loss = 0.27, batch loss = 0.21 (27.2 examples/sec; 0.294 sec/batch; 21h:21m:49s remains)
INFO - root - 2017-12-16 09:20:38.568790: step 71240, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.284 sec/batch; 20h:35m:23s remains)
INFO - root - 2017-12-16 09:20:41.402885: step 71250, loss = 0.22, batch loss = 0.16 (29.0 examples/sec; 0.275 sec/batch; 19h:59m:25s remains)
INFO - root - 2017-12-16 09:20:44.224121: step 71260, loss = 0.39, batch loss = 0.33 (28.3 examples/sec; 0.283 sec/batch; 20h:30m:03s remains)
INFO - root - 2017-12-16 09:20:47.040658: step 71270, loss = 0.24, batch loss = 0.18 (28.7 examples/sec; 0.279 sec/batch; 20h:14m:09s remains)
INFO - root - 2017-12-16 09:20:49.856064: step 71280, loss = 0.24, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 20h:31m:27s remains)
INFO - root - 2017-12-16 09:20:52.714447: step 71290, loss = 0.39, batch loss = 0.33 (28.4 examples/sec; 0.281 sec/batch; 20h:24m:38s remains)
INFO - root - 2017-12-16 09:20:55.588392: step 71300, loss = 0.37, batch loss = 0.31 (28.3 examples/sec; 0.283 sec/batch; 20h:32m:02s remains)
2017-12-16 09:20:56.127853: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7482429 -4.113152 -4.5845466 -5.3077369 -6.0466051 -6.4995995 -6.9009304 -6.9628892 -6.5944381 -5.8195806 -4.99784 -4.370894 -3.7062888 -3.2360473 -3.0239422][-3.2284384 -3.5578909 -4.1368871 -4.9609585 -5.7179446 -6.3391409 -6.959579 -7.0108967 -6.6680956 -6.0340271 -5.3218279 -4.7298717 -4.1134539 -3.5547843 -3.1475325][-2.5618682 -2.9025552 -3.3117862 -3.8518283 -4.4501348 -5.0619678 -5.6581593 -5.8784342 -5.8475552 -5.57688 -5.3383789 -5.1194224 -4.6359873 -4.0650973 -3.5727868][-1.9159753 -2.1080124 -2.5735397 -2.9917231 -3.2129171 -3.3753791 -3.6107397 -3.8361707 -4.0030808 -4.2329445 -4.5240273 -4.8694644 -4.9590397 -4.5549889 -3.9015677][-1.4904306 -1.6527963 -1.9256926 -2.0388498 -1.9604394 -1.485661 -1.092463 -1.2626324 -1.6961625 -2.3577435 -3.2080398 -4.1894875 -4.7662854 -4.6879153 -4.2495894][-1.3563502 -1.3655715 -1.2825243 -0.9037497 -0.10990047 0.94367123 1.7704382 1.9870176 1.4005194 -0.0056242943 -1.7351224 -3.4624181 -4.593339 -4.9008417 -4.59029][-1.2452936 -1.1151648 -0.76070738 -0.0041050911 1.3457723 2.8618994 4.0976295 4.4334316 3.6943789 1.9785953 -0.20193672 -2.4128315 -4.0395017 -4.7395139 -4.66372][-1.8347647 -1.6321702 -1.2236562 -0.22342968 1.3153095 3.0922732 4.8361874 5.3304453 4.7086458 2.9421992 0.4317193 -1.9283147 -3.6595674 -4.5194764 -4.5986309][-2.8003387 -2.5817218 -2.1602087 -1.4015095 -0.067139626 1.7521749 3.3973589 3.9494343 3.5787411 2.0078077 -0.23783207 -2.4440541 -4.1452765 -4.9060445 -4.830512][-3.1025014 -3.2763858 -3.3565054 -2.8322508 -1.7773459 -0.65169168 0.46632719 1.198091 0.89957428 -0.46280932 -2.1541638 -3.82301 -4.9092255 -5.3375521 -5.2001929][-3.5426693 -3.6625891 -3.8769228 -4.1692171 -3.9071555 -2.9277434 -2.1167636 -2.0604618 -2.4238603 -3.06399 -4.0996237 -5.2505183 -5.8236547 -5.6915936 -5.131031][-4.1298871 -4.5437346 -5.1119609 -5.399251 -5.2090273 -5.1229048 -4.9884505 -4.6012321 -4.7326651 -5.341897 -5.7990465 -6.1309757 -6.1723304 -5.8006721 -5.0941052][-4.2951455 -4.8120837 -5.5542946 -6.077929 -6.3748522 -6.2982445 -6.1169233 -6.233026 -6.435029 -6.4829473 -6.5078125 -6.5166349 -6.1999078 -5.7143097 -5.0583739][-4.0364652 -4.4260011 -5.0050359 -5.5394115 -5.971231 -6.1486616 -6.3577061 -6.4557409 -6.5197039 -6.6084051 -6.4763441 -6.2647758 -5.8886156 -5.3585038 -4.7821407][-3.835813 -4.0776796 -4.4333491 -4.7166615 -4.9092636 -5.1356878 -5.3783889 -5.4900723 -5.6348209 -5.7640758 -5.6967058 -5.369988 -4.8771853 -4.5403595 -4.1255765]]...]
INFO - root - 2017-12-16 09:20:58.932561: step 71310, loss = 0.24, batch loss = 0.19 (27.6 examples/sec; 0.290 sec/batch; 21h:02m:04s remains)
INFO - root - 2017-12-16 09:21:01.818702: step 71320, loss = 0.30, batch loss = 0.25 (26.8 examples/sec; 0.299 sec/batch; 21h:41m:23s remains)
INFO - root - 2017-12-16 09:21:04.672988: step 71330, loss = 0.27, batch loss = 0.21 (27.4 examples/sec; 0.292 sec/batch; 21h:11m:44s remains)
INFO - root - 2017-12-16 09:21:07.484427: step 71340, loss = 0.31, batch loss = 0.25 (28.9 examples/sec; 0.277 sec/batch; 20h:03m:51s remains)
INFO - root - 2017-12-16 09:21:10.298580: step 71350, loss = 0.23, batch loss = 0.17 (27.9 examples/sec; 0.287 sec/batch; 20h:49m:10s remains)
INFO - root - 2017-12-16 09:21:13.158662: step 71360, loss = 0.34, batch loss = 0.28 (28.8 examples/sec; 0.278 sec/batch; 20h:08m:29s remains)
INFO - root - 2017-12-16 09:21:15.971790: step 71370, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.285 sec/batch; 20h:40m:33s remains)
INFO - root - 2017-12-16 09:21:18.897101: step 71380, loss = 0.28, batch loss = 0.22 (28.3 examples/sec; 0.283 sec/batch; 20h:32m:25s remains)
INFO - root - 2017-12-16 09:21:21.712649: step 71390, loss = 0.25, batch loss = 0.20 (29.0 examples/sec; 0.276 sec/batch; 19h:59m:12s remains)
INFO - root - 2017-12-16 09:21:24.582705: step 71400, loss = 0.20, batch loss = 0.14 (28.8 examples/sec; 0.278 sec/batch; 20h:10m:40s remains)
2017-12-16 09:21:25.062978: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7952075 -3.8822224 -3.8470378 -3.9270449 -3.7636008 -3.6907923 -3.7149253 -4.0454917 -4.3198638 -4.42955 -4.6235137 -4.7092309 -4.6986842 -4.3141985 -4.1196022][-3.3617194 -3.3946424 -3.3565125 -3.4439366 -3.3247957 -3.4454191 -3.642401 -3.8001842 -3.8809798 -4.1549244 -4.38366 -4.5423484 -4.6162553 -4.4285312 -4.3269749][-2.8381627 -2.6954966 -2.6355553 -2.7629004 -2.8590665 -2.929318 -3.088017 -3.3797207 -3.502861 -3.5707905 -3.7641444 -4.1238813 -4.3541565 -4.2998562 -4.34254][-1.9728587 -1.8556917 -1.9485192 -2.1492972 -2.3859336 -2.5199113 -2.6007571 -2.7509284 -2.660872 -2.6646752 -2.8467464 -3.2586555 -3.6635418 -3.9706802 -4.2347827][-1.097527 -0.88343763 -0.9161427 -1.288651 -1.6289473 -1.8019164 -1.7987957 -1.6460261 -1.2427523 -1.0887415 -1.3572884 -1.8697274 -2.391861 -2.9975166 -3.4146919][-0.2385273 -0.12379313 -0.304008 -0.75924826 -1.0677128 -1.1242371 -0.93068767 -0.52616572 0.033641338 0.31341219 0.14335728 -0.63613081 -1.4525447 -2.1926537 -2.6570122][-0.20160723 -0.11381245 -0.2704916 -0.73630929 -0.90404677 -0.65838265 -0.09347868 0.53945208 1.2089615 1.4233308 0.94405222 -0.035331249 -0.9575963 -1.7966003 -2.2167437][-0.89564896 -0.69170475 -0.74930239 -0.92585421 -0.76533794 -0.23245907 0.58700275 1.445334 2.0831914 2.0869737 1.4164348 0.18876743 -0.9535594 -1.8898153 -2.3448272][-1.9511976 -1.6828113 -1.5100725 -1.4145238 -0.99003696 -0.31094885 0.5604949 1.5335932 2.0563917 1.9731011 1.2302027 -0.048007965 -1.2253938 -2.1236742 -2.4508955][-2.8427296 -2.7248855 -2.3682218 -2.0864739 -1.6410732 -0.83159447 0.071671486 0.93632174 1.2604814 1.01372 0.19140625 -0.97506022 -1.9451585 -2.6073508 -2.8786783][-3.5392718 -3.5610244 -3.3152504 -2.9190426 -2.3903534 -1.7103631 -1.0027003 -0.46108866 -0.40750551 -0.70663953 -1.4644034 -2.3304861 -2.9884071 -3.3795438 -3.4590595][-4.4842334 -4.6873507 -4.4872475 -4.1974468 -3.6016107 -3.0480385 -2.6440682 -2.220304 -2.1925249 -2.5195057 -3.13893 -3.6772487 -3.9387345 -4.0251746 -3.9406316][-5.1745276 -5.7290554 -5.8347526 -5.5901413 -5.1712837 -4.6603217 -4.1312513 -3.9243238 -3.9796462 -4.0642142 -4.2917418 -4.4622664 -4.3425789 -4.0691276 -3.7722614][-5.8536959 -6.6108913 -6.9013586 -6.86059 -6.4954233 -6.0228982 -5.6544423 -5.3958406 -5.3061986 -5.3117304 -5.2616072 -5.0392723 -4.5970526 -4.0922527 -3.6574006][-6.4401941 -7.2733169 -7.7562132 -7.7807875 -7.3534813 -6.7130728 -6.1352139 -5.8740506 -5.7309303 -5.6990709 -5.6661434 -5.3531747 -4.77664 -4.2285423 -3.7735198]]...]
INFO - root - 2017-12-16 09:21:27.901113: step 71410, loss = 0.31, batch loss = 0.26 (28.3 examples/sec; 0.283 sec/batch; 20h:31m:49s remains)
INFO - root - 2017-12-16 09:21:30.761145: step 71420, loss = 0.26, batch loss = 0.20 (28.4 examples/sec; 0.282 sec/batch; 20h:27m:28s remains)
INFO - root - 2017-12-16 09:21:33.640485: step 71430, loss = 0.21, batch loss = 0.15 (26.5 examples/sec; 0.302 sec/batch; 21h:54m:23s remains)
INFO - root - 2017-12-16 09:21:36.486581: step 71440, loss = 0.19, batch loss = 0.13 (28.6 examples/sec; 0.280 sec/batch; 20h:17m:57s remains)
INFO - root - 2017-12-16 09:21:39.337522: step 71450, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 20h:38m:23s remains)
INFO - root - 2017-12-16 09:21:42.161311: step 71460, loss = 0.22, batch loss = 0.16 (28.9 examples/sec; 0.276 sec/batch; 20h:02m:45s remains)
INFO - root - 2017-12-16 09:21:45.024148: step 71470, loss = 0.23, batch loss = 0.17 (27.6 examples/sec; 0.290 sec/batch; 21h:00m:20s remains)
INFO - root - 2017-12-16 09:21:47.909263: step 71480, loss = 0.21, batch loss = 0.15 (27.9 examples/sec; 0.287 sec/batch; 20h:48m:52s remains)
INFO - root - 2017-12-16 09:21:50.797383: step 71490, loss = 0.24, batch loss = 0.18 (27.5 examples/sec; 0.291 sec/batch; 21h:06m:43s remains)
INFO - root - 2017-12-16 09:21:53.656822: step 71500, loss = 0.32, batch loss = 0.26 (27.2 examples/sec; 0.294 sec/batch; 21h:20m:04s remains)
2017-12-16 09:21:54.162592: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7119546 -2.9098172 -3.1933124 -3.3924866 -3.4243538 -3.4290781 -3.477066 -3.5067837 -3.5131245 -3.4819422 -3.4431109 -3.5990083 -3.8763 -4.1073947 -4.5255747][-2.5478215 -2.8659887 -3.0860813 -3.3094776 -3.4590328 -3.6001318 -3.6521814 -3.6074331 -3.5814118 -3.6013081 -3.6825688 -3.7881184 -3.9920363 -4.2633047 -4.6513424][-2.7574842 -3.0014105 -3.1799772 -3.32369 -3.3550358 -3.3847017 -3.4460189 -3.5547378 -3.5908861 -3.5988464 -3.6356144 -3.9590478 -4.3732052 -4.6093383 -4.8652053][-3.3065069 -3.26584 -3.190599 -3.0962687 -3.0558195 -3.0648959 -3.0247827 -2.9503932 -2.9349368 -3.1734176 -3.380805 -3.5364847 -3.6684794 -3.9167161 -4.3363514][-3.1618874 -2.7609723 -2.434999 -2.2509992 -2.0229874 -1.8777175 -1.7861857 -1.8091385 -1.9203835 -2.1196878 -2.23253 -2.4646077 -2.4882445 -2.6230395 -3.100781][-2.9222889 -2.2347312 -1.6709101 -1.1561067 -0.64751816 -0.46228576 -0.35025549 -0.38467312 -0.67107296 -1.1761947 -1.3753166 -1.4386592 -1.3018 -1.4782681 -1.9005022][-2.2181675 -1.4718826 -0.85871911 -0.38848352 0.27388477 0.87293768 1.2058148 1.0701981 0.71748686 0.12650108 -0.36129284 -0.66278887 -0.58057284 -0.79668331 -1.4752684][-1.2059221 -0.74463892 -0.48773885 0.033379555 0.83635139 1.6559978 2.2022834 2.3106484 2.0196271 1.3703279 0.78563166 0.32733965 0.072668552 -0.49790525 -1.4999087][-0.7382288 -0.64899707 -0.45937276 -0.15155888 0.50080156 1.5613837 2.2913122 2.4374881 2.1566482 1.5214539 1.0263457 0.51619244 0.13646984 -0.49380398 -1.6042371][-0.4056282 -0.76142907 -1.0500352 -0.7630198 -0.069964409 0.8529706 1.4493074 1.6312537 1.4003806 0.74665308 0.1823163 -0.27106762 -0.56143975 -1.1604965 -2.2137735][-0.49378848 -1.0130179 -1.4522176 -1.5279381 -1.1000724 -0.27096319 0.39881563 0.53226948 0.18548346 -0.40380383 -1.0753672 -1.652148 -2.062196 -2.5466843 -3.3474522][-0.88697338 -1.6902766 -2.3320193 -2.4844322 -2.1310632 -1.3912663 -0.86275029 -0.7999599 -1.077076 -1.6789236 -2.3356273 -2.9563541 -3.4014962 -3.7420115 -4.2085567][-1.1868386 -1.9034822 -2.6324179 -2.8939376 -2.896811 -2.4264798 -2.0103767 -1.9039528 -2.0976868 -2.6285772 -3.183167 -3.6918044 -4.0327907 -4.2958388 -4.52261][-1.2814095 -1.9127791 -2.4719696 -2.7969215 -2.9057178 -2.7543435 -2.6095595 -2.7089512 -2.9175644 -3.3535118 -3.7617788 -4.1236868 -4.3280134 -4.4237108 -4.4124951][-1.938719 -2.2586296 -2.5395761 -2.703362 -2.6392157 -2.5266285 -2.533963 -2.8132887 -3.1610498 -3.6626406 -4.0010314 -4.1229558 -4.05749 -4.0106425 -3.8930924]]...]
INFO - root - 2017-12-16 09:21:56.996606: step 71510, loss = 0.26, batch loss = 0.20 (29.0 examples/sec; 0.276 sec/batch; 20h:00m:08s remains)
INFO - root - 2017-12-16 09:21:59.852658: step 71520, loss = 0.24, batch loss = 0.19 (27.6 examples/sec; 0.290 sec/batch; 21h:02m:14s remains)
INFO - root - 2017-12-16 09:22:02.709634: step 71530, loss = 0.33, batch loss = 0.27 (28.2 examples/sec; 0.284 sec/batch; 20h:35m:52s remains)
INFO - root - 2017-12-16 09:22:05.524469: step 71540, loss = 0.22, batch loss = 0.16 (29.2 examples/sec; 0.274 sec/batch; 19h:50m:44s remains)
INFO - root - 2017-12-16 09:22:08.416226: step 71550, loss = 0.22, batch loss = 0.16 (25.7 examples/sec; 0.311 sec/batch; 22h:33m:47s remains)
INFO - root - 2017-12-16 09:22:11.279128: step 71560, loss = 0.30, batch loss = 0.25 (27.3 examples/sec; 0.293 sec/batch; 21h:12m:19s remains)
INFO - root - 2017-12-16 09:22:14.113530: step 71570, loss = 0.34, batch loss = 0.28 (28.2 examples/sec; 0.284 sec/batch; 20h:33m:46s remains)
INFO - root - 2017-12-16 09:22:16.965719: step 71580, loss = 0.22, batch loss = 0.16 (28.1 examples/sec; 0.285 sec/batch; 20h:39m:07s remains)
INFO - root - 2017-12-16 09:22:19.836916: step 71590, loss = 0.27, batch loss = 0.21 (28.9 examples/sec; 0.277 sec/batch; 20h:04m:36s remains)
INFO - root - 2017-12-16 09:22:22.652907: step 71600, loss = 0.32, batch loss = 0.27 (27.1 examples/sec; 0.295 sec/batch; 21h:23m:06s remains)
2017-12-16 09:22:23.184543: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0002203 -4.9234409 -4.8668408 -4.7528949 -4.5971441 -4.3832612 -4.2903543 -4.0736413 -3.9405582 -3.8952205 -3.8247433 -3.9568884 -3.9620128 -3.8430104 -3.8070641][-4.7632861 -4.7309947 -4.6501861 -4.5579271 -4.475965 -4.3352709 -4.2593307 -3.9233758 -3.5385783 -3.3113289 -3.1498013 -3.2633922 -3.3800402 -3.3897395 -3.5145164][-4.3086686 -4.1466756 -3.9876485 -3.8912838 -3.8970172 -3.8236308 -3.7102423 -3.4140837 -2.9824181 -2.6138365 -2.3345015 -2.4666381 -2.7134075 -2.8435597 -3.0568428][-3.3496413 -3.0916231 -2.8677807 -2.810904 -2.890626 -2.8094535 -2.6026845 -2.1976733 -1.58428 -1.1033671 -0.80616522 -1.0269954 -1.4306784 -1.9042554 -2.44987][-2.3155541 -2.000845 -1.6819475 -1.6415052 -1.7334273 -1.7401824 -1.5570197 -1.0496871 -0.41085529 -0.032897472 0.18853092 -0.12939358 -0.71411729 -1.3536761 -1.9196038][-1.6061397 -1.2961686 -0.90261626 -0.80142593 -0.914109 -0.93215847 -0.73404813 -0.35948372 0.087233067 0.34788275 0.42835331 0.074149609 -0.49110794 -1.0533063 -1.5133846][-1.3225245 -1.0266078 -0.71055579 -0.51599884 -0.56247377 -0.58976007 -0.48633409 -0.22459316 0.12363482 0.32797527 0.37905073 0.15777111 -0.25127792 -0.8098855 -1.2346961][-1.2569697 -0.79114819 -0.40279961 -0.25418711 -0.26698256 -0.33439159 -0.368021 -0.28290653 -0.081310272 -0.0518713 -0.16594362 -0.33216047 -0.58018732 -1.1185415 -1.6065404][-1.3086486 -0.78191733 -0.37572813 -0.19891834 -0.14608908 -0.16594267 -0.24624538 -0.32101965 -0.30451012 -0.42490292 -0.67051721 -0.94807553 -1.1978388 -1.5874245 -1.9385715][-1.4062502 -0.7846458 -0.41111851 -0.20099688 -0.12329531 -0.22185469 -0.46513414 -0.63880372 -0.7215333 -0.90851355 -1.1946464 -1.4227176 -1.5977967 -1.9610705 -2.3567665][-1.7274139 -0.88619852 -0.49033976 -0.41129065 -0.4605968 -0.60195923 -0.95570135 -1.271637 -1.4301891 -1.5935481 -1.8078537 -2.0397718 -2.2664475 -2.5695839 -2.9132662][-2.5569596 -1.5549123 -1.1068039 -1.1031711 -1.2693446 -1.5358615 -1.9585469 -2.2713163 -2.4145288 -2.4322448 -2.5176172 -2.6483014 -2.8753603 -3.0992565 -3.4007638][-3.7546794 -2.7054217 -2.2521663 -2.2306185 -2.3889382 -2.5829058 -2.8886967 -3.0516756 -3.0936713 -2.8626504 -2.7302766 -2.6551814 -2.7842922 -2.9739504 -3.2589495][-4.947372 -3.9915268 -3.5398464 -3.4012728 -3.4694803 -3.5620549 -3.729506 -3.7616456 -3.6488652 -3.2211604 -2.9156189 -2.597683 -2.4611254 -2.4707274 -2.6824937][-5.7904344 -4.9557781 -4.5701666 -4.510561 -4.607132 -4.6652913 -4.74962 -4.6674194 -4.4260902 -3.8532944 -3.395792 -2.9322371 -2.6468561 -2.4789119 -2.5414138]]...]
INFO - root - 2017-12-16 09:22:26.016340: step 71610, loss = 0.25, batch loss = 0.19 (27.9 examples/sec; 0.286 sec/batch; 20h:44m:59s remains)
INFO - root - 2017-12-16 09:22:28.864440: step 71620, loss = 0.45, batch loss = 0.39 (28.1 examples/sec; 0.285 sec/batch; 20h:37m:22s remains)
INFO - root - 2017-12-16 09:22:31.750713: step 71630, loss = 0.36, batch loss = 0.30 (27.6 examples/sec; 0.290 sec/batch; 21h:00m:57s remains)
INFO - root - 2017-12-16 09:22:34.631270: step 71640, loss = 0.32, batch loss = 0.26 (28.1 examples/sec; 0.284 sec/batch; 20h:36m:30s remains)
INFO - root - 2017-12-16 09:22:37.457590: step 71650, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.282 sec/batch; 20h:27m:26s remains)
INFO - root - 2017-12-16 09:22:40.283324: step 71660, loss = 0.23, batch loss = 0.17 (26.7 examples/sec; 0.299 sec/batch; 21h:40m:13s remains)
INFO - root - 2017-12-16 09:22:43.148102: step 71670, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.279 sec/batch; 20h:14m:27s remains)
INFO - root - 2017-12-16 09:22:45.973901: step 71680, loss = 0.24, batch loss = 0.18 (28.6 examples/sec; 0.280 sec/batch; 20h:15m:33s remains)
INFO - root - 2017-12-16 09:22:48.803853: step 71690, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.285 sec/batch; 20h:39m:39s remains)
INFO - root - 2017-12-16 09:22:51.612053: step 71700, loss = 0.27, batch loss = 0.22 (29.1 examples/sec; 0.275 sec/batch; 19h:55m:02s remains)
2017-12-16 09:22:52.150794: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3484106 -6.018044 -5.5518522 -5.3536072 -5.1096935 -4.988379 -4.9301677 -4.9142771 -5.0768824 -5.1113315 -5.379612 -5.7307391 -6.0494604 -6.304472 -6.4107323][-6.7697077 -6.3174953 -5.7771978 -5.5199594 -5.2324419 -5.0660787 -4.8126268 -4.7499928 -4.9754057 -5.1215873 -5.5055909 -5.7642317 -5.9080319 -5.9633055 -6.1120157][-7.0021524 -6.3062487 -5.6383934 -5.1104169 -4.633235 -4.4462996 -4.1766524 -4.2580638 -4.551836 -4.7671742 -5.1526394 -5.4369063 -5.6204324 -5.3597007 -5.3304567][-6.6714621 -5.8164735 -4.8410678 -4.037642 -3.3524344 -2.8526182 -2.6243756 -2.9410143 -3.5234668 -4.0202675 -4.38568 -4.5665817 -4.5958047 -4.4082589 -4.4490743][-5.8971543 -4.786747 -3.4158249 -2.2409856 -1.2008884 -0.56912804 -0.38409376 -0.84146857 -1.774008 -2.808881 -3.5120826 -3.7404354 -3.713238 -3.5680246 -3.680666][-5.2524395 -3.9449553 -2.3532777 -0.549649 1.1336942 1.9884157 2.2481976 1.5128732 0.1700387 -1.3263206 -2.3885427 -2.8189516 -2.9781923 -2.8400671 -2.9750278][-4.4447708 -3.2124605 -1.5656261 0.50206137 2.3132529 3.4706264 3.990428 3.2089448 1.7083411 -0.0048437119 -1.3416657 -2.1270146 -2.6823273 -2.8296905 -3.1552811][-4.1964588 -2.9531956 -1.3429782 0.67380476 2.498611 3.7579842 4.3656693 3.7652464 2.4954696 0.77367973 -0.67518377 -1.6735504 -2.4969287 -3.0271049 -3.7148883][-4.7437677 -3.8063169 -2.4792209 -0.576396 1.1681523 2.4878535 3.2351489 2.9576678 1.9830952 0.48538542 -0.91231465 -1.9391992 -2.7528963 -3.3960676 -4.1329751][-6.056674 -5.460794 -4.4367018 -2.8003087 -1.2590935 0.033268929 0.81442738 0.91899157 0.4166708 -0.58932185 -1.7500534 -2.79288 -3.657697 -4.3161316 -5.0668569][-7.6816311 -7.2997704 -6.6358604 -5.3411894 -4.0122762 -2.6115656 -1.6286922 -1.2320471 -1.3117948 -1.7972023 -2.5172775 -3.5045438 -4.3892593 -5.138196 -5.9557047][-8.8986645 -8.4644089 -8.1267967 -7.3071594 -6.3949118 -5.1997056 -4.2058954 -3.5222032 -3.123116 -3.0891829 -3.3777845 -4.0972152 -4.8258944 -5.6656775 -6.4883738][-9.3294907 -8.7437038 -8.4127474 -7.90014 -7.4655046 -6.8003454 -6.1597528 -5.4739866 -4.8117409 -4.3492479 -4.2395844 -4.7003393 -5.2139654 -5.9678354 -6.7746048][-8.8757763 -8.0808611 -7.6581726 -7.3908648 -7.3485136 -7.0259895 -6.794569 -6.3692436 -5.8728065 -5.3801165 -5.0140433 -5.1959572 -5.4732037 -6.0476789 -6.5786929][-7.8051114 -6.8424778 -6.2439127 -6.1798053 -6.3921161 -6.4747219 -6.61001 -6.44348 -6.1901646 -5.7708836 -5.4007154 -5.4308481 -5.50604 -5.8142548 -6.0888014]]...]
INFO - root - 2017-12-16 09:22:54.987919: step 71710, loss = 0.28, batch loss = 0.22 (29.5 examples/sec; 0.271 sec/batch; 19h:36m:58s remains)
INFO - root - 2017-12-16 09:22:57.835929: step 71720, loss = 0.23, batch loss = 0.17 (27.2 examples/sec; 0.294 sec/batch; 21h:16m:39s remains)
INFO - root - 2017-12-16 09:23:00.666340: step 71730, loss = 0.27, batch loss = 0.21 (28.8 examples/sec; 0.278 sec/batch; 20h:09m:20s remains)
INFO - root - 2017-12-16 09:23:03.537578: step 71740, loss = 0.25, batch loss = 0.19 (27.3 examples/sec; 0.293 sec/batch; 21h:12m:02s remains)
INFO - root - 2017-12-16 09:23:06.347079: step 71750, loss = 0.21, batch loss = 0.15 (27.6 examples/sec; 0.289 sec/batch; 20h:58m:01s remains)
INFO - root - 2017-12-16 09:23:09.203793: step 71760, loss = 0.33, batch loss = 0.27 (28.4 examples/sec; 0.282 sec/batch; 20h:25m:22s remains)
INFO - root - 2017-12-16 09:23:12.049418: step 71770, loss = 0.24, batch loss = 0.18 (28.9 examples/sec; 0.276 sec/batch; 20h:00m:54s remains)
INFO - root - 2017-12-16 09:23:14.912835: step 71780, loss = 0.24, batch loss = 0.18 (26.7 examples/sec; 0.299 sec/batch; 21h:41m:10s remains)
INFO - root - 2017-12-16 09:23:17.781568: step 71790, loss = 0.24, batch loss = 0.19 (27.8 examples/sec; 0.288 sec/batch; 20h:50m:42s remains)
INFO - root - 2017-12-16 09:23:20.568326: step 71800, loss = 0.24, batch loss = 0.18 (29.5 examples/sec; 0.271 sec/batch; 19h:36m:41s remains)
2017-12-16 09:23:21.079740: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2186537 -3.27943 -3.3918777 -3.6763549 -4.2201781 -4.8322186 -5.3438692 -5.7290812 -5.8656549 -5.6221428 -5.0362234 -4.3846107 -3.8774252 -3.4495516 -3.1734095][-3.1817861 -3.2808414 -3.4831183 -3.8219495 -4.2251496 -4.8554788 -5.582047 -5.9651527 -5.9542952 -5.7579179 -5.37748 -4.6799254 -4.0214319 -3.508158 -3.2325523][-3.3043332 -3.4582944 -3.677578 -3.926414 -4.2477312 -4.6015353 -5.1463108 -5.5300374 -5.6803184 -5.5077038 -5.17476 -4.805028 -4.442574 -3.7805152 -3.3826728][-3.3252625 -3.4078593 -3.6142998 -3.7674417 -3.9527311 -3.8950384 -3.9054244 -3.9426258 -3.8564572 -4.0101018 -4.2360005 -4.4432783 -4.3433089 -3.8854616 -3.5327106][-3.1759629 -3.1314421 -3.2018788 -3.0324888 -2.6270308 -2.1022336 -1.571378 -1.3073215 -1.1633761 -1.4359674 -2.14273 -3.1729093 -3.6553106 -3.6546803 -3.273756][-2.7312055 -2.5236721 -2.3418152 -1.8910608 -0.96871614 0.20207548 1.3902516 2.0599093 2.0830798 1.2909083 -0.028024673 -1.6190882 -2.6258254 -2.9389019 -2.7815371][-2.4738922 -2.0360847 -1.7771106 -1.0540798 0.17181444 2.0514441 3.9469614 4.8561077 4.903945 3.7220125 1.882905 -0.15751362 -1.5895369 -2.1807082 -2.1656444][-2.3755038 -2.0307736 -1.7364616 -0.75372243 0.63487434 2.7246771 4.8264866 6.0634785 6.2599134 4.9218225 2.9672318 0.81216526 -0.76252317 -1.6005754 -1.8566258][-2.4706488 -2.3992407 -2.2404122 -1.5263152 -0.29556942 1.8815174 3.8099222 5.1270466 5.2487736 4.1632786 2.5898786 0.68917847 -0.79948449 -1.6371734 -1.9270113][-3.0259693 -3.0393238 -3.3172562 -3.05027 -2.1715827 -0.51610518 1.0209122 2.4223351 2.655942 1.9468818 0.91168451 -0.44228196 -1.5799642 -2.239742 -2.4530308][-3.837079 -3.9961433 -4.2739997 -4.2961836 -4.0497031 -3.0453455 -1.7819417 -0.64765882 -0.24979782 -0.3581543 -0.90977669 -1.7548685 -2.4359007 -2.8729901 -2.9261661][-4.0171671 -4.3142986 -4.886714 -5.0737238 -4.9680328 -4.5758109 -3.9728458 -3.1062486 -2.4496179 -2.1814988 -2.2655847 -2.6491442 -3.0766456 -3.2747655 -3.1886966][-3.8346994 -3.9879224 -4.3904662 -4.8240194 -5.2696128 -5.1442442 -4.7059774 -4.125504 -3.659447 -3.2663789 -2.9915133 -3.0370564 -3.1655488 -3.2817082 -3.17517][-3.4316418 -3.4190993 -3.6172152 -3.9024322 -4.2322664 -4.3409176 -4.2818666 -3.9058566 -3.50007 -3.1399627 -2.9543757 -2.942739 -2.9481421 -2.98286 -2.8523309][-3.0507021 -2.8257926 -2.825418 -2.9139102 -3.1568513 -3.291266 -3.2827208 -3.1456518 -2.9107113 -2.6756263 -2.5358255 -2.4635334 -2.4502959 -2.5350931 -2.5233679]]...]
INFO - root - 2017-12-16 09:23:23.919989: step 71810, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 20h:36m:38s remains)
INFO - root - 2017-12-16 09:23:26.800107: step 71820, loss = 0.29, batch loss = 0.23 (27.3 examples/sec; 0.294 sec/batch; 21h:15m:13s remains)
INFO - root - 2017-12-16 09:23:29.681036: step 71830, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 20h:40m:54s remains)
INFO - root - 2017-12-16 09:23:32.549092: step 71840, loss = 0.25, batch loss = 0.19 (28.8 examples/sec; 0.278 sec/batch; 20h:06m:26s remains)
INFO - root - 2017-12-16 09:23:35.378963: step 71850, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 20h:25m:17s remains)
INFO - root - 2017-12-16 09:23:38.321458: step 71860, loss = 0.26, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 20h:14m:37s remains)
INFO - root - 2017-12-16 09:23:41.233266: step 71870, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 20h:41m:10s remains)
INFO - root - 2017-12-16 09:23:44.125163: step 71880, loss = 0.27, batch loss = 0.21 (27.8 examples/sec; 0.287 sec/batch; 20h:48m:08s remains)
INFO - root - 2017-12-16 09:23:47.030094: step 71890, loss = 0.22, batch loss = 0.16 (27.7 examples/sec; 0.289 sec/batch; 20h:53m:51s remains)
INFO - root - 2017-12-16 09:23:49.841988: step 71900, loss = 0.30, batch loss = 0.24 (28.7 examples/sec; 0.278 sec/batch; 20h:08m:56s remains)
2017-12-16 09:23:50.396549: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0311303 -1.9567373 -2.2529275 -2.6776695 -3.3017206 -3.8422027 -4.1612048 -3.9878626 -3.807399 -3.6283569 -3.5011129 -3.4966879 -3.6472836 -3.6366777 -3.8136044][-1.7952464 -1.812618 -2.0513623 -2.4186425 -2.993947 -3.4897075 -3.7194023 -3.5409582 -3.3270662 -3.4120226 -3.6108522 -3.6643353 -3.8466213 -3.9947276 -4.1165843][-1.8299603 -1.8061152 -2.0095634 -2.309628 -2.7041857 -3.00562 -3.1682444 -3.0574179 -2.9479094 -3.0910728 -3.3575048 -3.7502818 -4.1143312 -4.0778437 -4.0993495][-2.0216343 -2.1835647 -2.4548483 -2.637785 -2.7914917 -2.7385554 -2.6422038 -2.2918758 -1.8845315 -2.4199598 -3.1059327 -3.6420362 -4.0898628 -4.0276818 -3.5623422][-2.4097178 -2.4933877 -2.6665928 -2.5853238 -2.3889205 -2.0439582 -1.5096102 -1.2055764 -1.1365812 -1.6140249 -2.3856616 -3.3333392 -3.898155 -3.712997 -3.2013884][-2.5932755 -2.6455591 -2.5379176 -2.1548026 -1.4166603 -0.6044786 0.13619423 0.45404387 0.29139328 -0.79584694 -2.139065 -3.2033305 -3.8282437 -3.6730502 -3.0457788][-3.2179055 -3.17419 -2.6137159 -1.748585 -0.58587885 0.69076824 1.8113308 2.0629025 1.6427655 0.37180853 -1.1311255 -2.6882226 -3.6743879 -3.5908871 -3.0995641][-3.8386853 -3.9478986 -3.5902565 -2.3197284 -0.57286477 0.91283274 2.1282306 2.3499823 1.8722868 0.74748993 -0.74517226 -2.1348441 -3.1236744 -3.2627306 -2.9236214][-4.8576951 -4.5623198 -3.7066715 -2.6408994 -1.1243172 0.43806314 1.653966 1.764883 1.1614695 0.17664814 -1.1815197 -2.7949495 -3.8493028 -3.9244952 -3.6249378][-4.7376528 -4.496841 -3.9606135 -2.9371228 -1.4270678 -0.16775703 0.56438732 0.81178379 0.48486233 -0.50749731 -1.7556233 -3.0894237 -4.0830069 -4.3482051 -4.3080497][-4.7054863 -4.5204663 -4.0186071 -3.3652797 -2.5420094 -1.6504653 -0.83490705 -0.57813 -0.70409894 -1.1433606 -2.0206082 -3.0623739 -3.9351492 -4.239552 -4.2592359][-4.3197718 -3.9328253 -3.7665079 -3.7712417 -3.4187789 -2.8696342 -2.3156781 -2.0275247 -2.00789 -2.2449634 -2.7552581 -3.3003249 -3.9291172 -4.3676109 -4.5060577][-3.3044589 -3.170728 -3.2987723 -3.6157751 -3.8594577 -3.9044187 -3.7460639 -3.5656729 -3.4016576 -3.2446985 -3.22614 -3.5083582 -3.9288466 -4.1364694 -4.2573457][-2.2193727 -2.2053006 -2.6071081 -3.2334204 -3.9250224 -4.3265266 -4.391398 -4.2383404 -3.7936378 -3.2990718 -3.1341689 -3.1069479 -3.2765002 -3.59549 -3.9370704][-1.3175507 -1.2293382 -1.6387606 -2.460928 -3.2823052 -3.9704506 -4.2265749 -4.0310936 -3.6382015 -3.1268578 -2.8930397 -2.9076977 -3.2626004 -3.59916 -3.7823145]]...]
INFO - root - 2017-12-16 09:23:53.242937: step 71910, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.285 sec/batch; 20h:35m:52s remains)
INFO - root - 2017-12-16 09:23:56.100958: step 71920, loss = 0.25, batch loss = 0.19 (28.3 examples/sec; 0.282 sec/batch; 20h:26m:49s remains)
INFO - root - 2017-12-16 09:23:58.921711: step 71930, loss = 0.20, batch loss = 0.14 (28.1 examples/sec; 0.285 sec/batch; 20h:36m:51s remains)
INFO - root - 2017-12-16 09:24:01.783656: step 71940, loss = 0.26, batch loss = 0.20 (28.8 examples/sec; 0.278 sec/batch; 20h:06m:40s remains)
INFO - root - 2017-12-16 09:24:04.622903: step 71950, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 20h:06m:01s remains)
INFO - root - 2017-12-16 09:24:07.451684: step 71960, loss = 0.39, batch loss = 0.34 (28.9 examples/sec; 0.277 sec/batch; 20h:03m:58s remains)
INFO - root - 2017-12-16 09:24:10.305284: step 71970, loss = 0.30, batch loss = 0.24 (27.7 examples/sec; 0.288 sec/batch; 20h:52m:05s remains)
INFO - root - 2017-12-16 09:24:13.126497: step 71980, loss = 0.33, batch loss = 0.27 (28.7 examples/sec; 0.279 sec/batch; 20h:12m:13s remains)
INFO - root - 2017-12-16 09:24:15.996617: step 71990, loss = 0.31, batch loss = 0.25 (27.4 examples/sec; 0.292 sec/batch; 21h:05m:44s remains)
INFO - root - 2017-12-16 09:24:18.900608: step 72000, loss = 0.26, batch loss = 0.20 (27.9 examples/sec; 0.286 sec/batch; 20h:43m:30s remains)
2017-12-16 09:24:19.426121: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7691073 -3.0962491 -3.4145293 -3.8608756 -4.3110876 -4.7020817 -5.0737391 -5.6204958 -6.11164 -6.330832 -6.2743363 -6.0820131 -5.7848139 -5.2147584 -4.9262342][-2.4909387 -2.7516832 -3.0699167 -3.545187 -3.9879911 -4.6509686 -5.220871 -5.7758131 -6.2899737 -6.7035689 -6.9683008 -6.7677574 -6.3998356 -5.7127886 -5.0479827][-2.0994325 -2.1723406 -2.4642777 -2.9755952 -3.4264212 -4.0290484 -4.4998693 -5.1159868 -5.6707678 -6.1832561 -6.6473169 -6.7730036 -6.5570602 -5.6987638 -4.9236979][-1.5845613 -1.5662491 -1.72895 -2.1466496 -2.6372683 -3.1004057 -3.3676372 -3.6599936 -4.0100927 -4.6223907 -5.3021417 -5.8921657 -6.03416 -5.349103 -4.4502249][-1.1370564 -0.93257737 -1.0276279 -1.4017036 -1.7176096 -1.7215071 -1.4712486 -1.3925495 -1.7288151 -2.3476083 -3.2570381 -4.2967181 -4.8995032 -4.7173114 -4.0922885][-1.0144033 -0.71735525 -0.63286161 -0.66612697 -0.50978637 -0.15778446 0.4673996 1.1472487 1.1446419 0.22035122 -1.376159 -2.8462949 -3.7626767 -3.9547286 -3.6524324][-0.98573542 -0.70994329 -0.65886545 -0.48716187 -0.011533737 0.89810753 2.2140894 3.1275887 3.1239047 2.2598372 0.71405458 -1.1580124 -2.72606 -3.2252662 -3.0181828][-0.94732356 -0.8054173 -0.90672994 -0.60576916 0.12471962 1.3956418 2.9597254 4.1256647 4.5311718 3.6908016 2.0148077 0.26249981 -1.313215 -2.36005 -2.6306143][-1.5901971 -1.2414515 -1.0676677 -0.95129132 -0.42222452 0.89947653 2.467535 3.8126316 4.3589706 3.6754179 2.2103205 0.45722342 -1.1133575 -2.1732669 -2.3979855][-2.0622566 -2.1762383 -2.2865682 -2.0703733 -1.4492896 -0.56529713 0.451499 1.7194786 2.4189057 2.15687 1.1662579 -0.25754213 -1.5534666 -2.5032167 -2.7361484][-3.242959 -3.050982 -3.0810511 -3.2830634 -3.1237392 -2.5263238 -1.6561902 -0.750607 -0.21217775 -0.13197231 -0.49239802 -1.2928021 -2.1878831 -2.8704767 -2.7858131][-4.3853459 -4.2765603 -4.3173676 -4.3916421 -4.3854151 -4.3466954 -3.9865308 -3.2741272 -2.6340504 -2.3024187 -2.3932066 -2.8105879 -3.1804795 -3.4452875 -3.1456943][-5.3902268 -5.4057312 -5.5624943 -5.861928 -6.0476928 -6.0907269 -5.922533 -5.5712996 -5.1094651 -4.6070719 -4.2436032 -4.2710552 -4.36889 -4.3906784 -4.0046787][-6.4399538 -6.4711418 -6.4810333 -6.6116972 -6.8822408 -7.0687876 -7.10974 -6.945641 -6.6290889 -6.1298084 -5.6436262 -5.4703641 -5.3126888 -4.9928122 -4.44986][-6.5358124 -6.5308795 -6.5052938 -6.6992912 -6.8959174 -6.9261045 -6.9879045 -7.0178642 -6.9482818 -6.5644364 -6.1279111 -5.809021 -5.5649834 -5.2628946 -4.7625375]]...]
INFO - root - 2017-12-16 09:24:22.312260: step 72010, loss = 0.22, batch loss = 0.16 (28.0 examples/sec; 0.286 sec/batch; 20h:40m:39s remains)
INFO - root - 2017-12-16 09:24:25.162054: step 72020, loss = 0.26, batch loss = 0.21 (28.2 examples/sec; 0.283 sec/batch; 20h:30m:37s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 09:24:27.948215: step 72030, loss = 0.23, batch loss = 0.17 (29.3 examples/sec; 0.273 sec/batch; 19h:43m:34s remains)
INFO - root - 2017-12-16 09:24:30.811847: step 72040, loss = 0.29, batch loss = 0.23 (28.6 examples/sec; 0.279 sec/batch; 20h:12m:56s remains)
INFO - root - 2017-12-16 09:24:33.667035: step 72050, loss = 0.34, batch loss = 0.28 (27.4 examples/sec; 0.292 sec/batch; 21h:06m:25s remains)
INFO - root - 2017-12-16 09:24:36.466967: step 72060, loss = 0.31, batch loss = 0.25 (27.3 examples/sec; 0.293 sec/batch; 21h:13m:33s remains)
INFO - root - 2017-12-16 09:24:39.336057: step 72070, loss = 0.27, batch loss = 0.21 (28.9 examples/sec; 0.277 sec/batch; 20h:01m:48s remains)
INFO - root - 2017-12-16 09:24:42.203563: step 72080, loss = 0.28, batch loss = 0.22 (28.7 examples/sec; 0.278 sec/batch; 20h:08m:22s remains)
INFO - root - 2017-12-16 09:24:45.024950: step 72090, loss = 0.24, batch loss = 0.18 (29.1 examples/sec; 0.275 sec/batch; 19h:52m:14s remains)
INFO - root - 2017-12-16 09:24:47.880491: step 72100, loss = 0.30, batch loss = 0.25 (27.9 examples/sec; 0.287 sec/batch; 20h:45m:48s remains)
2017-12-16 09:24:48.388171: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.281765 -2.7493179 -3.1612113 -3.8164196 -4.7020745 -5.7365403 -6.4453983 -7.2293034 -7.8423829 -7.5454168 -6.7128153 -5.9364433 -5.0503292 -3.9320314 -3.3572526][-2.2822375 -2.4315915 -2.7875075 -3.2967265 -3.737339 -4.5741663 -5.563283 -6.100883 -6.2379503 -6.1077404 -5.8097854 -5.0801358 -4.1082463 -3.1503382 -2.6763473][-2.6490498 -2.9707851 -3.0465937 -2.9975019 -2.9027119 -2.9580798 -3.3074803 -3.8749692 -4.2602549 -4.3173666 -4.2738657 -3.9086602 -3.0296283 -1.7984636 -1.269629][-3.3918381 -3.3350127 -3.2480016 -3.0092838 -2.3053138 -1.6313479 -1.5493984 -2.0184319 -2.4435053 -2.5149586 -2.3961487 -2.2933798 -2.1139059 -1.3813043 -0.83401346][-3.9547455 -3.8802609 -3.5878954 -2.7552438 -1.6230171 -0.32610226 0.33306217 0.097689152 -0.511076 -1.1664138 -1.7066178 -1.7577901 -1.6491501 -1.3317168 -1.4310341][-4.9996138 -4.6204844 -3.9038796 -2.6175761 -0.74890137 1.0692654 2.0231414 2.067924 1.2073011 0.037734985 -1.0142846 -1.5203431 -1.7927933 -1.8408506 -2.0122488][-4.8184347 -4.7334433 -3.9383202 -2.3371565 -0.44801474 1.5173197 2.7650523 2.7092662 1.6653438 0.2365942 -1.0168328 -1.6898568 -2.0538294 -2.1238945 -2.1970141][-4.3891172 -4.1381354 -3.2852042 -1.7934484 -0.11708927 1.4368334 2.1876707 2.1083517 1.0716748 -0.46563125 -1.6657689 -2.1802437 -2.3245714 -2.3671591 -2.5328732][-3.6400352 -3.6184807 -2.9068937 -1.6025984 -0.24803066 0.84440231 1.4140077 1.1355567 0.20031023 -1.0363259 -2.0783479 -2.5760872 -2.6217852 -2.4864264 -2.4312072][-2.938468 -3.2347684 -2.8938031 -1.9945514 -0.96248412 -0.10261917 0.21902418 -0.0079908371 -0.6334579 -1.5725505 -2.3325589 -2.6894195 -2.7200108 -2.524838 -2.2875164][-2.7861104 -3.2202597 -3.1067557 -2.569612 -1.891017 -1.2622588 -0.92186689 -1.1544971 -1.7045712 -2.2281325 -2.6158009 -2.7877619 -2.7119937 -2.5291471 -2.3784559][-3.1554379 -3.6203663 -3.7873092 -3.5471938 -2.9470787 -2.348511 -2.0328972 -1.999902 -2.1141851 -2.5321724 -2.9415817 -2.9601769 -2.8517802 -2.7483163 -2.5011625][-3.5044258 -3.9856002 -4.341877 -4.1395407 -3.6513104 -3.1847436 -2.74483 -2.6374836 -2.8294506 -2.8708861 -2.826498 -2.8564818 -2.7585838 -2.5896492 -2.5670288][-3.8599687 -4.0599709 -4.1873846 -4.1669097 -3.9535077 -3.5212426 -3.179225 -3.1230652 -3.0406227 -3.0097575 -2.8672543 -2.6424332 -2.4532213 -2.4319139 -2.50869][-3.8467693 -3.9042764 -3.8627393 -3.7154708 -3.5882492 -3.4906888 -3.3214893 -3.2742288 -3.3332415 -3.1640847 -2.7723873 -2.3827555 -2.0769007 -1.9780762 -2.1243289]]...]
INFO - root - 2017-12-16 09:24:51.232746: step 72110, loss = 0.30, batch loss = 0.24 (28.4 examples/sec; 0.282 sec/batch; 20h:24m:03s remains)
INFO - root - 2017-12-16 09:24:54.042346: step 72120, loss = 0.21, batch loss = 0.15 (29.0 examples/sec; 0.276 sec/batch; 19h:58m:46s remains)
INFO - root - 2017-12-16 09:24:56.834488: step 72130, loss = 0.25, batch loss = 0.19 (28.8 examples/sec; 0.277 sec/batch; 20h:04m:08s remains)
INFO - root - 2017-12-16 09:24:59.686708: step 72140, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 20h:04m:10s remains)
INFO - root - 2017-12-16 09:25:02.477022: step 72150, loss = 0.21, batch loss = 0.15 (28.0 examples/sec; 0.286 sec/batch; 20h:40m:13s remains)
INFO - root - 2017-12-16 09:25:05.299474: step 72160, loss = 0.39, batch loss = 0.34 (28.7 examples/sec; 0.279 sec/batch; 20h:10m:54s remains)
INFO - root - 2017-12-16 09:25:08.188135: step 72170, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 20h:21m:24s remains)
INFO - root - 2017-12-16 09:25:11.022862: step 72180, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.284 sec/batch; 20h:32m:20s remains)
INFO - root - 2017-12-16 09:25:13.818942: step 72190, loss = 0.23, batch loss = 0.18 (28.6 examples/sec; 0.279 sec/batch; 20h:11m:28s remains)
INFO - root - 2017-12-16 09:25:16.650519: step 72200, loss = 0.22, batch loss = 0.16 (29.3 examples/sec; 0.273 sec/batch; 19h:44m:55s remains)
2017-12-16 09:25:17.156646: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1329019 -3.0889125 -3.0336912 -3.2063336 -3.4382536 -3.5686617 -3.6294405 -3.5024259 -3.4653456 -3.3794165 -3.1468816 -2.8816328 -2.9527426 -3.1600642 -3.3400545][-2.8383849 -2.598556 -2.4902873 -2.7016864 -2.9477239 -3.0430431 -3.0979233 -3.084924 -3.1911836 -3.292047 -3.2619309 -3.0255175 -3.1133981 -3.3072691 -3.5519593][-2.7161102 -2.4953818 -2.4184644 -2.3382523 -2.3936489 -2.5623674 -2.6377437 -2.619256 -2.5980263 -2.823184 -2.9820669 -3.0016146 -3.1996098 -3.4263303 -3.7833676][-2.6052055 -2.1458192 -1.9385302 -1.7622032 -1.5930033 -1.5957088 -1.7368553 -1.9335852 -2.15136 -2.4808445 -2.7692199 -2.9118545 -3.2399976 -3.5832262 -3.8525195][-2.7691958 -2.0863547 -1.4262245 -1.0621922 -0.69653225 -0.28110743 -0.24980688 -0.52306652 -1.0600107 -1.5705719 -2.1271942 -2.5873737 -3.018199 -3.3874495 -3.6630177][-3.3778136 -2.3766916 -1.399889 -0.45535064 0.48566103 1.0755634 1.3534112 1.0556464 0.30173302 -0.56776142 -1.4675906 -2.0302944 -2.3655646 -2.8065739 -3.2991972][-3.9926543 -2.9409361 -1.8122394 -0.47051239 0.96143484 1.9295549 2.6330156 2.2496581 1.2065396 0.060143471 -1.0343051 -1.8887308 -2.4399586 -2.6303105 -2.85567][-4.5241919 -3.6501338 -2.4459596 -0.89647484 0.64622116 1.9222655 2.824708 2.6539803 1.6658468 0.29352951 -0.88990545 -1.6960032 -2.2482729 -2.6745048 -3.0541115][-5.1205988 -4.2964959 -3.1153746 -1.6946414 -0.19380665 1.1863036 2.0891542 1.8966727 0.94271278 -0.3709259 -1.3922517 -2.2348185 -2.5685837 -3.0067255 -3.447937][-5.6746016 -5.4460087 -4.6216755 -3.1730046 -1.6682529 -0.54928136 0.13264227 0.23952246 -0.38746405 -1.5074875 -2.3769259 -3.0571027 -3.4407442 -3.6632848 -3.9471059][-6.2545462 -5.9545908 -5.4665766 -4.7711282 -3.5491509 -2.4947789 -1.7589071 -1.6025057 -1.9936786 -2.7184811 -3.2115541 -3.6479759 -3.8911469 -4.3022866 -4.6199436][-6.59764 -6.2407732 -5.928906 -5.4468236 -4.8616896 -4.2884789 -3.7689033 -3.3980322 -3.441462 -3.8627172 -4.1397476 -4.3645096 -4.483427 -4.6243777 -4.9119077][-6.3056955 -5.824039 -5.5991368 -5.4149766 -5.1221633 -4.8398671 -4.6416707 -4.4427433 -4.1586833 -4.3216205 -4.4095979 -4.5448885 -4.6828041 -4.7247849 -4.7739506][-5.9080048 -5.2909832 -5.0228777 -4.8919783 -4.9575157 -5.0079641 -5.0608807 -5.0562954 -4.8572216 -4.7543454 -4.57416 -4.4976549 -4.5434718 -4.6396947 -4.6023364][-5.8921976 -4.9109 -4.4421268 -4.4118195 -4.6575603 -4.7464557 -4.9775448 -5.06956 -4.9746394 -4.9219432 -4.7707534 -4.5758238 -4.5393209 -4.665349 -4.6988659]]...]
INFO - root - 2017-12-16 09:25:19.969394: step 72210, loss = 0.31, batch loss = 0.25 (28.4 examples/sec; 0.281 sec/batch; 20h:20m:27s remains)
INFO - root - 2017-12-16 09:25:22.801909: step 72220, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.281 sec/batch; 20h:19m:57s remains)
INFO - root - 2017-12-16 09:25:25.652748: step 72230, loss = 0.32, batch loss = 0.26 (29.1 examples/sec; 0.275 sec/batch; 19h:54m:12s remains)
INFO - root - 2017-12-16 09:25:28.443291: step 72240, loss = 0.33, batch loss = 0.27 (29.6 examples/sec; 0.270 sec/batch; 19h:30m:41s remains)
INFO - root - 2017-12-16 09:25:31.246021: step 72250, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.285 sec/batch; 20h:36m:20s remains)
INFO - root - 2017-12-16 09:25:34.049623: step 72260, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 20h:41m:14s remains)
INFO - root - 2017-12-16 09:25:36.856475: step 72270, loss = 0.36, batch loss = 0.30 (28.0 examples/sec; 0.286 sec/batch; 20h:38m:23s remains)
INFO - root - 2017-12-16 09:25:39.735074: step 72280, loss = 0.21, batch loss = 0.15 (27.1 examples/sec; 0.295 sec/batch; 21h:19m:26s remains)
INFO - root - 2017-12-16 09:25:42.536892: step 72290, loss = 0.26, batch loss = 0.20 (29.4 examples/sec; 0.272 sec/batch; 19h:39m:24s remains)
INFO - root - 2017-12-16 09:25:45.367742: step 72300, loss = 0.33, batch loss = 0.27 (26.3 examples/sec; 0.304 sec/batch; 21h:58m:43s remains)
2017-12-16 09:25:45.892615: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6176872 -2.9008429 -3.1479678 -3.5839043 -3.9862647 -4.345933 -4.7148657 -4.9881859 -5.1806645 -5.12337 -4.90468 -4.7258716 -4.4851589 -4.1089149 -3.7519307][-3.0530167 -3.4211612 -3.7944942 -4.1659989 -4.4345918 -4.7988849 -5.0914435 -5.1595554 -5.3396473 -5.5601654 -5.7223706 -5.6382012 -5.3769422 -4.8313255 -4.3348312][-3.8125577 -4.456008 -4.6611729 -4.6656852 -4.6934729 -4.6945434 -4.6353459 -4.8208776 -5.0909753 -5.3616982 -5.7659531 -6.2114248 -6.1544051 -5.4528832 -4.5647111][-3.9650218 -4.98985 -5.5684743 -5.3843927 -4.5634003 -3.7543538 -3.2407579 -3.0837193 -3.5329366 -4.4292312 -5.4764495 -6.3606744 -6.6173468 -6.0026493 -4.7571311][-4.5075946 -5.2233047 -5.4243956 -5.0237169 -3.7953379 -2.0907414 -0.73317719 -0.2963295 -0.96423125 -2.4808273 -4.3857288 -5.846591 -6.2732859 -5.8671541 -4.9186268][-4.747602 -5.435873 -5.4694157 -4.3970008 -2.3010654 0.20077276 2.2747421 3.0087771 2.0739045 -0.022978306 -2.5955203 -4.7466278 -5.697917 -5.4960756 -4.5137577][-4.3006387 -4.8379865 -4.7998648 -3.524893 -1.2109478 1.5941596 4.1534338 5.3638716 4.4965839 1.9224558 -0.93557453 -3.243299 -4.4756966 -4.6699362 -4.0279083][-3.8536971 -4.1922717 -4.0743923 -2.6178198 -0.28005886 2.7665873 5.279603 6.0003729 4.9774809 2.424789 -0.47292304 -2.6173153 -3.4455788 -3.4618318 -3.0237355][-3.52778 -3.7262917 -3.2012317 -2.0862262 -0.028813839 2.7649708 4.7643023 5.3545856 4.1546774 1.8587341 -0.49401259 -2.5101752 -3.3484964 -3.0221424 -2.4542727][-3.4317183 -3.8813038 -3.8887043 -2.7122393 -0.79586291 1.2352948 2.6377277 2.8375106 1.6422071 -0.13410187 -1.508004 -2.4495225 -2.746043 -2.6293368 -2.0141151][-4.2638745 -4.5250726 -4.6980076 -4.1180277 -2.7354112 -1.2600455 -0.342124 -0.49540186 -1.6644676 -2.9092894 -3.6077638 -3.4045019 -2.6395488 -2.1204195 -1.4662764][-4.7947083 -5.5355124 -5.927844 -5.3788204 -4.3192787 -3.4910152 -3.1688552 -3.5510695 -4.3701258 -5.098022 -5.3016829 -4.52177 -3.2803628 -2.4089367 -1.8820863][-5.4143562 -6.3717403 -6.9010863 -6.7182722 -6.2416754 -5.6500421 -5.1386008 -5.2911615 -6.0118046 -6.380126 -6.015275 -4.9589133 -3.9505446 -3.2582889 -2.535181][-6.3138347 -7.0987415 -7.4973326 -7.6300206 -7.3372493 -7.0118518 -6.7688675 -6.5657139 -6.498807 -6.5435414 -6.341115 -5.4748259 -4.5399671 -3.9735279 -3.4316895][-6.185976 -7.0156198 -7.4593 -7.6440248 -7.4851189 -7.3835273 -7.1299944 -6.8776941 -6.7561083 -6.4296932 -6.0645075 -5.5853934 -5.1693954 -4.88342 -4.4038177]]...]
INFO - root - 2017-12-16 09:25:48.718127: step 72310, loss = 0.34, batch loss = 0.28 (28.9 examples/sec; 0.277 sec/batch; 20h:01m:44s remains)
INFO - root - 2017-12-16 09:25:51.551735: step 72320, loss = 0.32, batch loss = 0.26 (27.5 examples/sec; 0.291 sec/batch; 21h:03m:29s remains)
INFO - root - 2017-12-16 09:25:54.374303: step 72330, loss = 0.28, batch loss = 0.22 (27.1 examples/sec; 0.295 sec/batch; 21h:19m:03s remains)
INFO - root - 2017-12-16 09:25:57.186938: step 72340, loss = 0.28, batch loss = 0.23 (29.3 examples/sec; 0.273 sec/batch; 19h:42m:20s remains)
INFO - root - 2017-12-16 09:26:00.010881: step 72350, loss = 0.22, batch loss = 0.16 (28.6 examples/sec; 0.280 sec/batch; 20h:13m:50s remains)
INFO - root - 2017-12-16 09:26:02.825527: step 72360, loss = 0.26, batch loss = 0.20 (29.5 examples/sec; 0.271 sec/batch; 19h:37m:04s remains)
INFO - root - 2017-12-16 09:26:05.673774: step 72370, loss = 0.32, batch loss = 0.26 (27.0 examples/sec; 0.296 sec/batch; 21h:23m:38s remains)
INFO - root - 2017-12-16 09:26:08.496412: step 72380, loss = 0.36, batch loss = 0.30 (27.7 examples/sec; 0.289 sec/batch; 20h:52m:02s remains)
INFO - root - 2017-12-16 09:26:11.332717: step 72390, loss = 0.33, batch loss = 0.27 (28.0 examples/sec; 0.285 sec/batch; 20h:36m:33s remains)
INFO - root - 2017-12-16 09:26:14.145353: step 72400, loss = 0.27, batch loss = 0.21 (25.9 examples/sec; 0.309 sec/batch; 22h:18m:44s remains)
2017-12-16 09:26:14.691061: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6427541 -5.5808268 -6.0648985 -7.1454182 -8.3999586 -9.03949 -9.2889891 -9.4102907 -9.1282444 -8.41647 -7.375277 -6.200511 -5.1427336 -4.358449 -3.8815491][-5.4563503 -5.411181 -5.74092 -6.6461353 -7.4532123 -8.6213455 -9.326313 -9.5099449 -9.4653149 -9.1258726 -8.4330616 -7.4187126 -6.3292961 -5.3212361 -4.5369253][-4.6593628 -4.5756421 -4.6436744 -5.058507 -5.5652266 -6.4924231 -7.3684006 -8.1294241 -8.7301044 -8.9284248 -8.81731 -8.0637627 -7.1623535 -6.1809096 -5.2238021][-3.4003968 -3.0178385 -2.8038404 -2.8393364 -2.992754 -3.4743814 -4.0579796 -5.1576886 -6.5097704 -7.613863 -8.2138758 -7.9938369 -7.4486332 -6.3774319 -5.3967204][-2.6750128 -1.6410944 -0.61005449 -0.026888371 0.31344604 0.13304424 -0.43758798 -1.5302575 -3.1012621 -5.2439384 -6.9421997 -7.3775988 -6.9668837 -6.00618 -5.021759][-2.155462 -0.86758327 0.73450756 2.3879466 3.7863064 3.9807014 3.3439355 2.0009847 -0.1261692 -2.6823165 -4.7908077 -6.0484347 -6.2940679 -5.2908087 -4.1287155][-2.6181188 -0.94592404 1.1471539 3.156827 5.0886631 6.1652393 6.2417679 4.7039452 2.2524786 -0.63103056 -3.1855292 -4.6365094 -4.9706244 -4.3438458 -3.5596104][-3.5986657 -2.03871 -0.074337959 2.4082351 4.7319212 6.1025381 6.5251036 5.3166761 3.1005945 0.18691015 -2.3270025 -3.8125696 -4.4103036 -4.0265412 -3.4411054][-5.804276 -4.3993096 -2.3895309 0.15857124 2.4059725 4.24868 4.9926748 4.1920776 2.291337 -0.34012079 -2.5641918 -3.8608768 -4.3332109 -4.1125875 -3.8275588][-6.7505512 -6.1979237 -5.1340857 -2.85338 -0.7168901 1.0117245 1.6480379 1.392117 0.20114326 -1.7598832 -3.584486 -4.7438641 -5.1449752 -5.036902 -4.7876267][-8.2924194 -7.6829996 -6.7416306 -5.5387969 -4.33244 -2.9412663 -2.1216908 -2.0510721 -2.6658328 -3.7952385 -4.9379807 -5.7840633 -6.136364 -5.8354015 -5.5398993][-8.2836714 -8.3656111 -8.2752628 -7.4700484 -6.5712996 -5.7821193 -5.3701611 -5.1809759 -5.1941252 -5.5689049 -6.1265659 -6.6523566 -6.8964596 -6.5590763 -5.938324][-7.4594021 -7.8269496 -8.17099 -7.959548 -7.7101107 -7.2465849 -6.9369917 -6.765821 -6.6589737 -6.7223139 -6.7606745 -6.8573151 -6.8080549 -6.6022086 -5.9677382][-5.829361 -6.1239028 -6.4662352 -6.745008 -6.9924173 -6.8999081 -6.8756537 -6.8516717 -6.7483273 -6.6056542 -6.5131378 -6.481904 -6.3698454 -6.0468745 -5.4501076][-4.8922772 -4.9634533 -5.1533418 -5.4643283 -5.7935739 -6.0623474 -6.2407031 -6.1394572 -6.0029378 -5.94916 -5.9398813 -5.7855582 -5.5496626 -5.2796321 -4.8600569]]...]
INFO - root - 2017-12-16 09:26:17.467034: step 72410, loss = 0.25, batch loss = 0.19 (28.8 examples/sec; 0.278 sec/batch; 20h:06m:04s remains)
INFO - root - 2017-12-16 09:26:20.284638: step 72420, loss = 0.21, batch loss = 0.15 (28.5 examples/sec; 0.281 sec/batch; 20h:17m:50s remains)
INFO - root - 2017-12-16 09:26:23.145579: step 72430, loss = 0.28, batch loss = 0.22 (27.5 examples/sec; 0.291 sec/batch; 21h:00m:36s remains)
INFO - root - 2017-12-16 09:26:25.976331: step 72440, loss = 0.28, batch loss = 0.23 (28.2 examples/sec; 0.283 sec/batch; 20h:27m:31s remains)
INFO - root - 2017-12-16 09:26:28.806054: step 72450, loss = 0.24, batch loss = 0.18 (29.0 examples/sec; 0.276 sec/batch; 19h:55m:10s remains)
INFO - root - 2017-12-16 09:26:31.677802: step 72460, loss = 0.26, batch loss = 0.20 (29.2 examples/sec; 0.274 sec/batch; 19h:45m:46s remains)
INFO - root - 2017-12-16 09:26:34.522300: step 72470, loss = 0.26, batch loss = 0.20 (26.6 examples/sec; 0.301 sec/batch; 21h:42m:22s remains)
INFO - root - 2017-12-16 09:26:37.340124: step 72480, loss = 0.24, batch loss = 0.19 (28.5 examples/sec; 0.281 sec/batch; 20h:17m:35s remains)
INFO - root - 2017-12-16 09:26:40.178889: step 72490, loss = 0.29, batch loss = 0.24 (27.5 examples/sec; 0.290 sec/batch; 20h:58m:27s remains)
INFO - root - 2017-12-16 09:26:42.980260: step 72500, loss = 0.37, batch loss = 0.31 (28.0 examples/sec; 0.286 sec/batch; 20h:37m:11s remains)
2017-12-16 09:26:43.510666: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1509619 -5.1770215 -4.4619551 -4.3032126 -4.5903196 -5.2684207 -5.5917931 -5.503643 -5.3402209 -5.0713491 -4.9045529 -4.3887033 -4.2098804 -4.2238746 -4.4771833][-6.1489592 -4.9818249 -4.1613746 -3.9438183 -3.9480128 -4.6081777 -4.97964 -5.1594505 -4.9958963 -4.6123185 -4.5168471 -4.1233082 -3.9802263 -3.7405853 -3.8111579][-5.5317259 -4.3281488 -3.5381169 -3.281718 -3.392374 -3.995743 -4.3362594 -4.65361 -4.6207914 -4.4699688 -4.4622154 -4.2541008 -4.0354271 -3.5716214 -3.4373491][-4.6487026 -3.0850134 -2.0304875 -1.9484217 -2.395999 -2.8549562 -3.315218 -3.7256255 -3.9812157 -4.0432124 -3.9253163 -3.9700332 -3.9562814 -3.5360639 -3.3325558][-3.6665385 -1.8521237 -0.3946538 -0.051274776 -0.23191643 -0.74375749 -1.4368684 -1.9933338 -2.5199203 -2.8946316 -3.3514805 -3.6253881 -3.6987953 -3.3754084 -3.1151352][-3.0183187 -1.1072986 0.29832554 0.99091721 1.2407598 0.99974346 0.74909353 0.16676855 -0.55045676 -1.1281681 -2.1133583 -2.7371132 -3.1192849 -3.0563409 -2.9327145][-2.9720168 -1.0599289 0.26186371 1.1189885 1.3681827 1.6580138 1.8915124 1.5273018 0.89399004 0.1078229 -1.035574 -2.1770828 -2.9982276 -3.0733769 -3.1903079][-3.5209193 -1.7433615 -0.26725292 0.73491383 1.1100426 1.5903587 2.0418611 2.0423417 1.7092042 0.90458345 -0.16390133 -1.2761216 -2.3236701 -2.8512061 -3.4514208][-3.8823531 -2.7482331 -1.5040102 -0.43263888 0.068449974 0.82633209 1.4271827 1.638114 1.3026896 0.513021 -0.54380488 -1.6057541 -2.5229433 -3.2170572 -3.9528568][-4.6587658 -4.0192342 -3.2599165 -2.4102051 -1.728426 -0.80301523 -0.16193581 0.27453709 0.1269269 -0.59948778 -1.5752361 -2.5144906 -3.2654929 -3.8074062 -4.4610157][-5.2364416 -4.7592783 -4.4295015 -4.0300093 -3.4863698 -2.6357222 -1.7244623 -1.1601322 -1.2516017 -1.7315013 -2.5482469 -3.342891 -4.0505991 -4.5746574 -5.035809][-5.6592526 -5.2913847 -5.1649041 -5.0258269 -4.8060923 -4.2148781 -3.43988 -2.825743 -2.6398554 -2.8392029 -3.3261032 -3.8399415 -4.353694 -4.776063 -5.1326885][-6.099597 -5.5868464 -5.4774132 -5.5046315 -5.6153674 -5.3612165 -4.7806931 -4.2593589 -3.9645829 -4.01653 -4.1920815 -4.46371 -4.7412324 -4.9231977 -4.9815331][-6.4586754 -5.7480383 -5.5471807 -5.5570889 -5.6458778 -5.6500425 -5.4267683 -4.959012 -4.5966353 -4.4741182 -4.4247255 -4.4219904 -4.5363445 -4.5486288 -4.5032034][-6.2952614 -5.4013004 -4.8891749 -4.7397261 -4.881989 -5.03303 -4.9978561 -4.7188749 -4.4491625 -4.2285652 -4.0604358 -3.9062264 -3.8978078 -3.83914 -3.7391934]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-72500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-72500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 09:26:46.922912: step 72510, loss = 0.36, batch loss = 0.30 (26.7 examples/sec; 0.300 sec/batch; 21h:38m:43s remains)
INFO - root - 2017-12-16 09:26:49.823154: step 72520, loss = 0.23, batch loss = 0.17 (27.9 examples/sec; 0.287 sec/batch; 20h:41m:51s remains)
INFO - root - 2017-12-16 09:26:52.626206: step 72530, loss = 0.34, batch loss = 0.28 (28.2 examples/sec; 0.284 sec/batch; 20h:30m:50s remains)
INFO - root - 2017-12-16 09:26:55.469417: step 72540, loss = 0.33, batch loss = 0.27 (27.5 examples/sec; 0.291 sec/batch; 20h:59m:37s remains)
INFO - root - 2017-12-16 09:26:58.302710: step 72550, loss = 0.22, batch loss = 0.17 (29.0 examples/sec; 0.276 sec/batch; 19h:55m:20s remains)
INFO - root - 2017-12-16 09:27:01.122683: step 72560, loss = 0.26, batch loss = 0.20 (29.0 examples/sec; 0.276 sec/batch; 19h:54m:14s remains)
INFO - root - 2017-12-16 09:27:03.911167: step 72570, loss = 0.35, batch loss = 0.29 (28.8 examples/sec; 0.278 sec/batch; 20h:03m:49s remains)
INFO - root - 2017-12-16 09:27:06.710478: step 72580, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 20h:34m:20s remains)
INFO - root - 2017-12-16 09:27:09.564381: step 72590, loss = 0.25, batch loss = 0.20 (28.4 examples/sec; 0.281 sec/batch; 20h:18m:53s remains)
INFO - root - 2017-12-16 09:27:12.387361: step 72600, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 20h:34m:33s remains)
2017-12-16 09:27:12.887111: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5037866 -3.6655381 -3.8601389 -4.0324874 -4.1431894 -4.1544547 -4.1085048 -4.0926709 -4.1199741 -3.9435513 -3.7678876 -3.6912844 -3.5328183 -3.5146875 -3.5026977][-3.7084265 -3.9819291 -4.198555 -4.3888144 -4.4609942 -4.382257 -4.2007642 -4.0634775 -4.0398664 -3.9082215 -3.8214784 -3.6686311 -3.4857161 -3.5698135 -3.5978844][-4.1576109 -4.4518571 -4.6569514 -4.7559786 -4.604919 -4.3019838 -3.9609711 -3.8318331 -3.778862 -3.6969562 -3.6499207 -3.5983469 -3.5651393 -3.57128 -3.5492525][-4.3389831 -4.5656824 -4.7012038 -4.6368017 -4.1159973 -3.5501611 -2.958003 -2.6407747 -2.5619555 -2.7851386 -3.0404844 -3.1296535 -3.221698 -3.3088558 -3.4027793][-4.3196435 -4.4640379 -4.337276 -3.9260943 -3.0067425 -2.1020069 -1.1921589 -0.75312161 -0.81146526 -1.2350287 -1.662385 -2.0554626 -2.379447 -2.6262326 -2.8408074][-3.9780266 -4.0392814 -3.8001213 -2.9055324 -1.5813241 -0.33579636 0.88318014 1.2840209 1.0514951 0.29376936 -0.4725647 -1.0755544 -1.5348113 -1.9164519 -2.1405017][-3.4211805 -3.3117766 -3.0481162 -1.9799147 -0.43716383 1.0302444 2.4586844 2.8564734 2.5678821 1.5428262 0.44295979 -0.34521818 -0.97738743 -1.3574567 -1.5481839][-2.9565215 -3.0956717 -2.8486557 -1.7641919 -0.33476353 1.1882501 2.6644864 3.125628 2.8983617 1.8364043 0.79191303 -0.028673172 -0.71652937 -1.1359425 -1.3505535][-2.885982 -3.0978947 -2.8290641 -2.0170624 -0.9269383 0.52548313 1.8535705 2.3133345 2.1711111 1.3522325 0.54184484 -0.45408964 -1.1904771 -1.5609567 -1.7298582][-3.3024011 -3.7210808 -3.8106415 -3.1785965 -2.1645989 -1.0230842 0.094614029 0.67385054 0.649035 -0.01076889 -0.6180048 -1.5303411 -2.3183529 -2.7846034 -2.9987507][-4.1342268 -4.6095438 -4.7556705 -4.4182029 -3.7512009 -2.7719846 -1.7039189 -1.3076897 -1.316397 -1.686882 -2.2339075 -2.9818771 -3.6141737 -4.09317 -4.2857714][-4.7130437 -5.1390147 -5.2966924 -5.1046939 -4.7596884 -4.1511288 -3.4381037 -3.0117636 -2.7119327 -2.8777907 -3.2397306 -3.888706 -4.4824414 -4.9187875 -5.0391111][-4.8696861 -5.0797963 -5.2546635 -5.1749997 -4.9840198 -4.5598736 -4.143199 -3.7470524 -3.3892131 -3.4795592 -3.6667049 -4.1791248 -4.674623 -5.1519456 -5.3056378][-4.8514824 -4.6793966 -4.6028428 -4.5011129 -4.3502345 -4.1159229 -3.887867 -3.6412146 -3.4575548 -3.4486506 -3.6036117 -4.0887265 -4.5524735 -4.7732449 -4.8371277][-4.7068586 -4.3452682 -4.1398759 -3.952373 -3.7309971 -3.6154287 -3.5219123 -3.3375244 -3.1967258 -3.1797664 -3.3316259 -3.6051154 -3.8363245 -3.939348 -3.9992368]]...]
INFO - root - 2017-12-16 09:27:15.741812: step 72610, loss = 0.30, batch loss = 0.24 (25.9 examples/sec; 0.309 sec/batch; 22h:18m:06s remains)
INFO - root - 2017-12-16 09:27:18.552152: step 72620, loss = 0.35, batch loss = 0.29 (28.5 examples/sec; 0.280 sec/batch; 20h:14m:24s remains)
INFO - root - 2017-12-16 09:27:21.447180: step 72630, loss = 0.28, batch loss = 0.22 (27.4 examples/sec; 0.292 sec/batch; 21h:02m:36s remains)
INFO - root - 2017-12-16 09:27:24.271368: step 72640, loss = 0.25, batch loss = 0.19 (27.7 examples/sec; 0.289 sec/batch; 20h:51m:16s remains)
INFO - root - 2017-12-16 09:27:27.146997: step 72650, loss = 0.22, batch loss = 0.16 (26.0 examples/sec; 0.308 sec/batch; 22h:14m:31s remains)
INFO - root - 2017-12-16 09:27:29.947145: step 72660, loss = 0.27, batch loss = 0.21 (29.7 examples/sec; 0.270 sec/batch; 19h:27m:46s remains)
INFO - root - 2017-12-16 09:27:32.818104: step 72670, loss = 0.32, batch loss = 0.26 (28.8 examples/sec; 0.278 sec/batch; 20h:02m:51s remains)
INFO - root - 2017-12-16 09:27:35.631151: step 72680, loss = 0.28, batch loss = 0.22 (29.3 examples/sec; 0.273 sec/batch; 19h:43m:45s remains)
INFO - root - 2017-12-16 09:27:38.452806: step 72690, loss = 0.22, batch loss = 0.16 (28.2 examples/sec; 0.284 sec/batch; 20h:30m:30s remains)
INFO - root - 2017-12-16 09:27:41.313624: step 72700, loss = 0.28, batch loss = 0.22 (28.9 examples/sec; 0.277 sec/batch; 19h:58m:39s remains)
2017-12-16 09:27:41.852964: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5544457 -4.98041 -5.2905784 -5.4459462 -5.5429654 -5.5075312 -5.4956937 -5.5880146 -5.9178042 -6.4826908 -6.9023685 -7.3662043 -7.4307909 -6.824297 -5.9265685][-5.3696108 -5.814836 -6.0168738 -6.0252285 -6.0411391 -6.1766748 -6.3717723 -6.6174774 -7.027298 -7.753808 -8.6344614 -9.38151 -9.5228 -8.9282112 -7.8007946][-5.7774353 -6.2297387 -6.3015161 -6.0579762 -5.7617164 -5.6593218 -5.7243733 -6.3433981 -7.2556605 -8.2141914 -9.3419228 -10.374361 -11.005995 -10.742357 -9.6184711][-5.9590096 -6.1852083 -5.9878111 -5.3556819 -4.7536116 -4.2808447 -4.1186585 -4.5112162 -5.3388805 -6.9589529 -8.7221079 -10.187718 -11.201056 -11.521872 -10.861204][-5.3922744 -5.3278747 -4.7648363 -3.8198843 -2.672833 -1.5480618 -0.98897076 -1.1688087 -2.0499094 -3.9795992 -6.2360287 -8.5878849 -10.442927 -11.402374 -11.140631][-4.5776572 -4.3003097 -3.358326 -1.7437429 0.18922043 1.6364765 2.5257888 2.717072 1.9984794 -0.14693069 -3.0641246 -6.230125 -8.8335476 -10.345442 -10.538343][-4.3538752 -3.7155271 -2.4913812 -0.78598404 1.7407589 4.2011032 5.9489021 6.1372614 5.1882582 3.0495892 -0.098308086 -3.8162556 -6.9775276 -8.7603865 -9.0291634][-4.6913514 -3.9429936 -2.7401919 -0.65607238 2.0123487 4.5879631 6.8873014 7.6893854 6.8597689 4.4858866 1.2184372 -2.487673 -5.5828476 -7.3277235 -7.5921106][-5.1309824 -4.8356433 -3.670742 -1.700542 0.57673311 3.284554 5.6154718 6.409215 5.9513216 4.0613403 1.1300907 -2.1624296 -4.9127393 -6.53149 -6.6826906][-5.285975 -5.5880017 -5.1977057 -3.5879216 -1.6268868 0.60036564 2.510592 3.6368408 3.4717088 1.8343554 -0.29965353 -2.7139411 -5.0231795 -6.4259915 -6.7656269][-6.1703558 -6.4908209 -6.3012266 -5.3117709 -3.9628723 -2.4027665 -1.1025581 -0.32137537 -0.30687523 -1.2163494 -2.7128329 -4.361083 -5.7568045 -6.6152 -6.9070368][-6.594615 -6.9064932 -7.0673661 -6.5855432 -5.5967402 -4.4875841 -3.663331 -3.1778374 -3.2489643 -3.8807447 -4.8116484 -5.9206171 -6.775816 -7.1356997 -7.1325121][-6.060504 -6.4192123 -6.6881809 -6.5986567 -6.4022522 -5.7773952 -5.0004282 -4.620141 -4.5957947 -4.7886276 -5.4038363 -6.2544723 -6.7732978 -7.0887895 -7.0433207][-5.3272734 -5.4488277 -5.5431781 -5.6404 -5.7383003 -5.5675449 -5.406208 -5.3160625 -5.2836895 -5.3273096 -5.33817 -5.5347338 -5.9388947 -6.146524 -6.0530081][-4.5962877 -4.5120559 -4.3866611 -4.3753815 -4.4687481 -4.5189037 -4.5428472 -4.5537658 -4.615582 -4.77686 -4.909677 -4.8061032 -4.597569 -4.63531 -4.6845641]]...]
INFO - root - 2017-12-16 09:27:44.676823: step 72710, loss = 0.20, batch loss = 0.14 (28.0 examples/sec; 0.285 sec/batch; 20h:35m:11s remains)
INFO - root - 2017-12-16 09:27:47.555287: step 72720, loss = 0.27, batch loss = 0.21 (28.8 examples/sec; 0.278 sec/batch; 20h:01m:37s remains)
INFO - root - 2017-12-16 09:27:50.411315: step 72730, loss = 0.27, batch loss = 0.21 (28.7 examples/sec; 0.279 sec/batch; 20h:06m:41s remains)
INFO - root - 2017-12-16 09:27:53.255980: step 72740, loss = 0.21, batch loss = 0.15 (28.0 examples/sec; 0.286 sec/batch; 20h:37m:31s remains)
INFO - root - 2017-12-16 09:27:56.131369: step 72750, loss = 0.26, batch loss = 0.20 (27.7 examples/sec; 0.289 sec/batch; 20h:49m:47s remains)
INFO - root - 2017-12-16 09:27:58.994697: step 72760, loss = 0.32, batch loss = 0.27 (28.0 examples/sec; 0.286 sec/batch; 20h:36m:18s remains)
INFO - root - 2017-12-16 09:28:01.862214: step 72770, loss = 0.26, batch loss = 0.20 (29.0 examples/sec; 0.276 sec/batch; 19h:52m:41s remains)
INFO - root - 2017-12-16 09:28:04.724157: step 72780, loss = 0.26, batch loss = 0.20 (29.1 examples/sec; 0.275 sec/batch; 19h:48m:59s remains)
INFO - root - 2017-12-16 09:28:07.538828: step 72790, loss = 0.25, batch loss = 0.19 (29.6 examples/sec; 0.270 sec/batch; 19h:29m:12s remains)
INFO - root - 2017-12-16 09:28:10.418330: step 72800, loss = 0.20, batch loss = 0.14 (28.9 examples/sec; 0.277 sec/batch; 19h:57m:15s remains)
2017-12-16 09:28:10.925382: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0211997 -4.5626149 -3.8903933 -3.5207052 -3.4826665 -3.4861677 -3.6891363 -3.6678836 -3.4534886 -3.3134758 -3.4084895 -3.6544147 -3.9440775 -4.3455873 -4.8144126][-4.5833082 -3.897465 -3.1054087 -2.6766491 -2.6173868 -2.706064 -2.9164495 -2.8992918 -2.70262 -2.5785031 -2.7815828 -3.0390272 -3.2044797 -3.4660511 -3.9292493][-4.088428 -3.3549726 -2.609611 -2.0630782 -1.9463611 -2.1208949 -2.4177253 -2.3853307 -2.1188612 -2.0037317 -2.097693 -2.4062879 -2.75027 -2.916019 -3.2130055][-3.2066109 -2.3592093 -1.7116418 -1.5085483 -1.7527487 -1.9272673 -2.0538063 -1.9417763 -1.5917337 -1.5099659 -1.5934415 -1.8575604 -2.2834346 -2.4886603 -2.8007319][-2.2521255 -1.6288543 -1.205399 -1.013386 -1.0370152 -0.86563349 -0.79586935 -0.73005486 -0.64134169 -0.60593963 -0.86330247 -1.3781524 -1.8026004 -2.213798 -2.6662586][-1.9953146 -1.4694533 -1.0527527 -0.80086112 -0.50688648 -0.092820644 0.2219758 0.60699654 0.72593212 0.49409056 -0.041819572 -0.93374419 -1.784487 -2.2824292 -2.7347636][-2.0713553 -2.0663664 -1.9023223 -1.4541538 -0.73630905 0.070976734 0.72893333 1.1533031 1.4487658 1.2592959 0.48077631 -0.48286557 -1.4280784 -2.3502955 -3.2147932][-2.5462043 -2.939847 -2.9207757 -2.465982 -1.3820894 -0.2784152 0.6118021 1.2106757 1.5136404 1.5390911 0.87540531 -0.24134636 -1.2566648 -2.2904608 -3.2304058][-3.0890341 -3.4810598 -3.5747349 -3.2771 -2.1360466 -0.8246212 0.20033693 0.91580486 1.2756081 1.2515512 0.51686335 -0.54222274 -1.6005499 -2.636085 -3.5935266][-3.3895214 -4.1965184 -4.3735342 -3.8520319 -3.1361661 -1.9412861 -0.98013139 -0.1453824 0.42545462 0.4643383 -0.062588692 -1.0145159 -2.0096319 -2.9394567 -3.7625141][-3.4191976 -4.3491635 -4.5858955 -4.3340359 -3.54478 -2.6626749 -1.8971965 -1.1876199 -0.66907597 -0.61380529 -0.98565936 -1.6077726 -2.3575764 -3.0405977 -3.5629883][-3.2732358 -4.0175505 -4.317452 -4.3317356 -3.7556615 -3.0281658 -2.5655866 -2.2638185 -2.0099685 -1.7441838 -1.977097 -2.4054444 -2.8831227 -3.3728023 -3.6770656][-3.3278322 -3.6533749 -3.8490961 -3.7322576 -3.5997448 -3.2079 -2.9572153 -2.8119111 -2.7440324 -2.7660284 -2.6795483 -2.7674918 -2.9569259 -3.2302384 -3.4036634][-3.523967 -3.6288605 -3.7129958 -3.3407047 -3.0609865 -2.9652383 -3.0333622 -3.1273541 -3.0188644 -3.1638117 -3.087045 -2.8664854 -2.8596711 -2.9557803 -3.0909944][-3.7512736 -3.6128988 -3.4169521 -3.111181 -2.9958756 -2.9377851 -2.9522016 -3.0133572 -2.9379845 -2.8241076 -2.5718832 -2.4484293 -2.5852857 -2.8131175 -2.9212027]]...]
INFO - root - 2017-12-16 09:28:13.721459: step 72810, loss = 0.48, batch loss = 0.43 (28.2 examples/sec; 0.284 sec/batch; 20h:27m:40s remains)
INFO - root - 2017-12-16 09:28:16.555652: step 72820, loss = 0.38, batch loss = 0.32 (27.4 examples/sec; 0.292 sec/batch; 21h:04m:41s remains)
INFO - root - 2017-12-16 09:28:19.371703: step 72830, loss = 0.27, batch loss = 0.21 (27.6 examples/sec; 0.290 sec/batch; 20h:55m:27s remains)
INFO - root - 2017-12-16 09:28:22.225483: step 72840, loss = 0.21, batch loss = 0.15 (28.8 examples/sec; 0.278 sec/batch; 20h:03m:08s remains)
INFO - root - 2017-12-16 09:28:25.101051: step 72850, loss = 0.22, batch loss = 0.16 (28.1 examples/sec; 0.285 sec/batch; 20h:32m:26s remains)
INFO - root - 2017-12-16 09:28:27.990879: step 72860, loss = 0.21, batch loss = 0.15 (28.8 examples/sec; 0.277 sec/batch; 20h:00m:30s remains)
INFO - root - 2017-12-16 09:28:30.802423: step 72870, loss = 0.25, batch loss = 0.19 (28.7 examples/sec; 0.279 sec/batch; 20h:06m:23s remains)
INFO - root - 2017-12-16 09:28:33.654000: step 72880, loss = 0.24, batch loss = 0.18 (28.6 examples/sec; 0.280 sec/batch; 20h:10m:46s remains)
INFO - root - 2017-12-16 09:28:36.479545: step 72890, loss = 0.30, batch loss = 0.24 (28.6 examples/sec; 0.280 sec/batch; 20h:10m:37s remains)
INFO - root - 2017-12-16 09:28:39.284875: step 72900, loss = 0.30, batch loss = 0.25 (29.4 examples/sec; 0.272 sec/batch; 19h:36m:44s remains)
2017-12-16 09:28:39.793539: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0737581 -5.5075774 -5.6327906 -5.5871196 -5.4924579 -5.3478312 -5.2226777 -5.2724595 -5.4726839 -5.6817064 -5.762413 -5.7619505 -5.5787783 -5.172327 -4.6633835][-5.8877325 -6.2674885 -6.2599607 -6.0692644 -5.7520986 -5.6097441 -5.585392 -5.6776319 -5.8627424 -6.231832 -6.6473694 -6.7004137 -6.5364661 -6.1005468 -5.5069861][-6.4499006 -6.7414217 -6.5599136 -6.0128088 -5.4088645 -5.0379829 -4.7636461 -5.1143012 -5.70371 -6.3954091 -6.911561 -7.1711693 -7.3280115 -6.9568219 -6.4002066][-6.4135551 -6.6037874 -6.1479745 -5.2013254 -4.2190685 -3.5503337 -3.1825125 -3.5018458 -4.2276468 -5.4542189 -6.6181707 -7.2371693 -7.5006971 -7.4691162 -7.1170487][-6.023695 -5.7693586 -4.7639756 -3.3810425 -1.9341052 -0.96344709 -0.62983489 -1.0824471 -2.1248529 -3.55998 -4.9584093 -6.154191 -7.0864773 -7.4335766 -7.2926025][-5.3510809 -4.8554745 -3.4761844 -1.5310886 0.54757881 1.9339004 2.4279752 1.8752928 0.60436869 -1.255769 -3.1166139 -4.5943894 -5.8795142 -6.7805319 -7.0916805][-4.8259768 -4.2002506 -2.7346051 -0.53099632 1.7346535 3.2601976 4.083498 3.7641621 2.5063367 0.63074112 -1.3196847 -3.149473 -4.8593135 -6.0936618 -6.4706097][-4.6457024 -3.9212115 -2.6191425 -0.3817358 2.047668 3.6049318 4.4557495 4.106432 2.9498434 1.2247386 -0.60175157 -2.3608265 -4.1698422 -5.538177 -6.0298018][-4.8467512 -4.177763 -2.9181058 -1.0756202 0.91767979 2.6476288 3.7871227 3.6655006 2.8310556 1.1717405 -0.58583784 -2.4828019 -4.2704191 -5.4770732 -5.8857393][-5.3146396 -5.0922027 -4.2315702 -2.6456182 -0.91339135 0.57693338 1.4384613 1.7315245 1.3045664 0.1109519 -1.3167126 -3.0302105 -4.6853085 -5.9375134 -6.2796941][-6.4739647 -6.4280663 -5.663322 -4.4224482 -3.1856577 -2.0490842 -0.87819052 -0.4276762 -0.79316592 -1.5954778 -2.63556 -3.9589384 -5.2297549 -6.1800423 -6.5441055][-7.1502137 -7.0854855 -6.610673 -5.8817263 -4.8140063 -3.9374413 -3.127058 -2.6132004 -2.4994559 -3.2204447 -4.2606764 -5.2794051 -6.1215191 -6.7203546 -6.7431498][-7.4847569 -7.543 -7.2656355 -6.6295929 -5.9332261 -5.265327 -4.4665914 -3.9854438 -3.8048973 -4.1837063 -4.8610463 -5.6750064 -6.3393097 -6.8010736 -6.7686415][-7.1722555 -7.2135749 -7.0383239 -6.6277237 -6.1112986 -5.5269055 -5.057848 -4.7007351 -4.4822879 -4.6092463 -4.9399352 -5.5165319 -5.9275742 -6.1195211 -6.0376282][-6.1131654 -5.9438081 -5.6595321 -5.5285921 -5.4026132 -5.0757003 -4.72702 -4.4873486 -4.4382682 -4.6381207 -4.8799195 -5.01351 -5.0833631 -5.1772132 -5.1407623]]...]
INFO - root - 2017-12-16 09:28:42.621213: step 72910, loss = 0.25, batch loss = 0.19 (27.7 examples/sec; 0.289 sec/batch; 20h:50m:09s remains)
INFO - root - 2017-12-16 09:28:45.464591: step 72920, loss = 0.37, batch loss = 0.31 (28.1 examples/sec; 0.285 sec/batch; 20h:32m:13s remains)
INFO - root - 2017-12-16 09:28:48.294843: step 72930, loss = 0.37, batch loss = 0.32 (29.0 examples/sec; 0.276 sec/batch; 19h:53m:50s remains)
INFO - root - 2017-12-16 09:28:51.131989: step 72940, loss = 0.21, batch loss = 0.16 (27.8 examples/sec; 0.288 sec/batch; 20h:45m:16s remains)
INFO - root - 2017-12-16 09:28:54.010252: step 72950, loss = 0.23, batch loss = 0.18 (27.5 examples/sec; 0.291 sec/batch; 20h:58m:12s remains)
INFO - root - 2017-12-16 09:28:56.858507: step 72960, loss = 0.26, batch loss = 0.20 (26.7 examples/sec; 0.299 sec/batch; 21h:33m:59s remains)
INFO - root - 2017-12-16 09:28:59.714408: step 72970, loss = 0.29, batch loss = 0.23 (27.9 examples/sec; 0.287 sec/batch; 20h:39m:35s remains)
INFO - root - 2017-12-16 09:29:02.505187: step 72980, loss = 0.34, batch loss = 0.29 (28.4 examples/sec; 0.282 sec/batch; 20h:19m:31s remains)
INFO - root - 2017-12-16 09:29:05.383550: step 72990, loss = 0.21, batch loss = 0.15 (28.3 examples/sec; 0.283 sec/batch; 20h:23m:50s remains)
INFO - root - 2017-12-16 09:29:08.186197: step 73000, loss = 0.22, batch loss = 0.16 (29.2 examples/sec; 0.274 sec/batch; 19h:46m:01s remains)
2017-12-16 09:29:08.734180: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.64942 -5.7964854 -5.670269 -5.59109 -5.5876169 -5.5694084 -5.5366964 -5.6723385 -5.826087 -5.8714294 -5.9537249 -5.8732562 -5.6024141 -5.2370887 -4.9192815][-5.714345 -5.636858 -5.6082053 -5.6477094 -5.6555219 -5.8770871 -6.0581 -6.0459976 -6.0682344 -6.3056822 -6.4308968 -6.24058 -6.0077291 -5.7125292 -5.4426832][-5.0633698 -5.0225143 -5.019949 -5.0302153 -5.1119366 -5.3374248 -5.4024076 -5.6079412 -5.8411212 -5.9807053 -6.1811018 -6.3710651 -6.3075204 -5.9233913 -5.7009][-4.146647 -3.6917226 -3.5427155 -3.6704628 -3.7585082 -3.9329379 -3.9350705 -4.0675187 -4.27811 -4.7138166 -5.0390072 -5.3469667 -5.5459962 -5.5031748 -5.5145874][-2.5218558 -1.7378397 -1.4474323 -1.6974154 -1.9618955 -1.9973755 -1.828398 -1.8876188 -2.0115721 -2.5303519 -3.1819582 -3.750284 -4.1829586 -4.3632264 -4.3838868][-1.431334 -0.60409045 -0.16226053 -0.080947876 -0.050670147 0.078062534 0.56362057 0.78731585 0.60928106 0.053812981 -0.6814332 -1.6800311 -2.5562148 -3.0380526 -3.2483857][-0.64989614 0.10472393 0.39493465 0.50250006 0.85682583 1.5705714 2.5350375 2.9970207 2.8377552 2.1824064 1.3894677 0.36369419 -0.68099141 -1.5181873 -2.1097567][-0.21304417 0.40230465 0.77565336 1.0222545 1.2480154 1.9229279 3.0382438 3.7843981 3.8359804 3.2097235 2.3223319 1.1867375 0.14421272 -0.54120469 -1.1913459][-0.32101536 0.020474911 0.14907885 0.35380983 0.827055 1.5534348 2.5052714 3.0556326 3.1521912 2.7554717 2.0554547 1.0759149 0.11456919 -0.55577016 -1.0667193][-0.65104723 -0.49546385 -0.60314846 -0.45594454 -0.12140894 0.43642569 1.1840777 1.7034364 1.7930822 1.4025226 0.89852333 0.24717569 -0.4763751 -0.89677691 -1.2453086][-1.294378 -1.1243827 -1.1036251 -1.0967014 -0.87314129 -0.57225132 -0.13707685 0.058423042 0.073924541 -0.23779202 -0.65270019 -1.1653433 -1.5103633 -1.6478219 -1.8749664][-1.5712335 -1.3419232 -1.3596365 -1.399112 -1.2633414 -1.080869 -0.921643 -1.0775564 -1.3082721 -1.7329426 -2.0884118 -2.312717 -2.4176817 -2.1865468 -2.1323216][-1.7527173 -1.4262228 -1.3017023 -1.3927555 -1.4070752 -1.2938592 -1.2442665 -1.7094209 -2.1146126 -2.5933185 -2.9215283 -2.9956508 -2.9884462 -2.6514616 -2.376174][-1.8775401 -1.4112363 -1.1727188 -1.1359055 -1.0758107 -1.1100433 -1.0452254 -1.5363874 -2.3010969 -3.1221805 -3.3945513 -3.3725991 -3.152379 -2.719739 -2.3919146][-1.9076297 -1.4390237 -1.1342101 -1.0847862 -0.98657441 -0.86009145 -0.76926231 -1.3184896 -2.155741 -3.027097 -3.5431123 -3.6369689 -3.4906912 -3.0283415 -2.6512394]]...]
INFO - root - 2017-12-16 09:29:11.619425: step 73010, loss = 0.26, batch loss = 0.20 (26.3 examples/sec; 0.305 sec/batch; 21h:56m:59s remains)
INFO - root - 2017-12-16 09:29:14.480866: step 73020, loss = 0.26, batch loss = 0.21 (27.0 examples/sec; 0.296 sec/batch; 21h:21m:22s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 09:29:17.333291: step 73030, loss = 0.24, batch loss = 0.18 (28.5 examples/sec; 0.281 sec/batch; 20h:13m:11s remains)
INFO - root - 2017-12-16 09:29:20.135950: step 73040, loss = 0.27, batch loss = 0.21 (27.3 examples/sec; 0.293 sec/batch; 21h:07m:51s remains)
INFO - root - 2017-12-16 09:29:22.951750: step 73050, loss = 0.24, batch loss = 0.19 (28.2 examples/sec; 0.283 sec/batch; 20h:24m:36s remains)
INFO - root - 2017-12-16 09:29:25.776984: step 73060, loss = 0.22, batch loss = 0.17 (29.4 examples/sec; 0.272 sec/batch; 19h:36m:16s remains)
INFO - root - 2017-12-16 09:29:28.627633: step 73070, loss = 0.24, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 20h:17m:54s remains)
INFO - root - 2017-12-16 09:29:31.447828: step 73080, loss = 0.27, batch loss = 0.21 (28.8 examples/sec; 0.278 sec/batch; 20h:02m:44s remains)
INFO - root - 2017-12-16 09:29:34.241793: step 73090, loss = 0.21, batch loss = 0.15 (28.7 examples/sec; 0.279 sec/batch; 20h:06m:22s remains)
INFO - root - 2017-12-16 09:29:37.071702: step 73100, loss = 0.24, batch loss = 0.18 (27.7 examples/sec; 0.289 sec/batch; 20h:47m:29s remains)
2017-12-16 09:29:37.605358: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5704596 -3.9384758 -4.3469105 -4.64688 -4.7563925 -4.6556225 -4.4761748 -4.2766614 -4.0701079 -3.9354224 -3.8220592 -3.8815134 -3.8749754 -3.8189988 -3.67879][-3.6257148 -4.1777549 -4.6888175 -5.0283947 -5.2029262 -5.3308516 -5.2100062 -4.8417277 -4.4811769 -4.1946478 -4.081171 -4.1180458 -4.134244 -4.1546831 -4.0013084][-3.8882194 -4.4312434 -5.0114326 -5.3439264 -5.3672519 -5.3133292 -4.982481 -4.72004 -4.3508396 -3.9669654 -3.9237242 -4.1178064 -4.3909497 -4.4703107 -4.3209605][-3.9704435 -4.5677242 -5.034122 -5.2078214 -5.0552845 -4.5516286 -3.7666714 -3.257576 -2.8492212 -2.8304143 -3.1497862 -3.5448422 -4.1018496 -4.4488368 -4.4607811][-3.7276013 -4.0299664 -4.1531324 -4.0612564 -3.5112545 -2.5954609 -1.5425544 -0.70551181 -0.29328251 -0.55802321 -1.3581023 -2.4504781 -3.4143949 -3.9894903 -4.09723][-3.3509324 -3.3843164 -3.1649911 -2.4532368 -1.1389055 0.15815163 1.4181828 2.4115224 2.6095085 2.0301843 0.77200937 -0.87083864 -2.2319045 -3.0907319 -3.4553649][-3.1409082 -2.8943849 -2.4152398 -1.4373205 0.19774866 2.1213927 4.0417147 5.1095362 5.1764574 4.3776712 2.6837277 0.6810565 -1.0046129 -2.0669339 -2.6280119][-3.0070922 -2.537766 -1.8952858 -0.77439809 0.8836422 2.8846173 4.9279079 6.0850449 6.211092 5.3000145 3.4897132 1.5234642 -0.15778732 -1.4529686 -2.2060175][-2.7706137 -2.4624631 -1.9317 -1.0141337 0.25789833 1.9453044 3.6665792 4.8406878 5.0301151 4.2273645 2.741653 0.8799243 -0.59274912 -1.625788 -2.2884433][-2.8014483 -3.0088172 -3.0514474 -2.4190776 -1.4159255 -0.19067478 0.89554691 1.9290533 2.3802462 1.8662605 0.80779886 -0.57185411 -1.7082014 -2.5197182 -2.888648][-3.5206935 -3.567204 -3.6974525 -3.7354987 -3.2914028 -2.2299998 -1.3883975 -0.78870487 -0.46470976 -0.58474159 -1.1532862 -2.1057732 -2.8475034 -3.2974482 -3.4300342][-3.7218206 -3.9616094 -4.320787 -4.3940706 -4.2230711 -3.8872054 -3.4087548 -2.8113532 -2.6074672 -2.6794057 -2.8803387 -3.3485258 -3.7114463 -3.9035759 -3.7951627][-3.7270598 -3.8865368 -4.1165614 -4.2493229 -4.3670106 -4.2630467 -4.022018 -3.7874026 -3.6195328 -3.5770409 -3.5658381 -3.6923945 -3.7469635 -3.7905011 -3.6816983][-3.5567842 -3.6055615 -3.673126 -3.6713967 -3.7596619 -3.6807022 -3.6425283 -3.6323233 -3.4881077 -3.3741336 -3.2603338 -3.3170729 -3.3267882 -3.3199496 -3.2086241][-3.1961975 -2.9833393 -2.8472824 -2.8104353 -2.8621244 -2.835835 -2.8690944 -2.8997252 -2.880168 -2.8336575 -2.6842608 -2.5727427 -2.5575457 -2.6702201 -2.6911724]]...]
INFO - root - 2017-12-16 09:29:40.478080: step 73110, loss = 0.31, batch loss = 0.25 (27.0 examples/sec; 0.297 sec/batch; 21h:22m:59s remains)
INFO - root - 2017-12-16 09:29:43.344827: step 73120, loss = 0.21, batch loss = 0.15 (28.2 examples/sec; 0.284 sec/batch; 20h:28m:16s remains)
INFO - root - 2017-12-16 09:29:46.159575: step 73130, loss = 0.32, batch loss = 0.26 (29.3 examples/sec; 0.273 sec/batch; 19h:41m:56s remains)
INFO - root - 2017-12-16 09:29:49.052409: step 73140, loss = 0.27, batch loss = 0.21 (27.3 examples/sec; 0.293 sec/batch; 21h:07m:08s remains)
INFO - root - 2017-12-16 09:29:51.897172: step 73150, loss = 0.37, batch loss = 0.32 (27.3 examples/sec; 0.293 sec/batch; 21h:07m:59s remains)
INFO - root - 2017-12-16 09:29:54.775062: step 73160, loss = 0.26, batch loss = 0.20 (27.7 examples/sec; 0.289 sec/batch; 20h:49m:08s remains)
INFO - root - 2017-12-16 09:29:57.645557: step 73170, loss = 0.28, batch loss = 0.23 (27.6 examples/sec; 0.290 sec/batch; 20h:52m:18s remains)
INFO - root - 2017-12-16 09:30:00.493082: step 73180, loss = 0.33, batch loss = 0.27 (27.7 examples/sec; 0.289 sec/batch; 20h:50m:17s remains)
INFO - root - 2017-12-16 09:30:03.329484: step 73190, loss = 0.30, batch loss = 0.24 (26.9 examples/sec; 0.297 sec/batch; 21h:23m:45s remains)
INFO - root - 2017-12-16 09:30:06.214452: step 73200, loss = 0.23, batch loss = 0.17 (27.9 examples/sec; 0.287 sec/batch; 20h:38m:54s remains)
2017-12-16 09:30:06.762013: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7837992 -1.984179 -2.2745259 -2.4848268 -2.7184615 -2.8564377 -2.93606 -2.8009887 -2.6180294 -2.2394168 -1.9319453 -1.8915823 -1.9635856 -2.3555019 -2.6591656][-2.7079153 -2.6873999 -2.6964669 -2.886452 -3.0985742 -3.1849618 -3.2104504 -3.1210926 -3.1063519 -2.8253403 -2.5227764 -2.3645277 -2.4091902 -2.9413652 -3.3269992][-3.7151952 -3.5927444 -3.3777804 -3.2067106 -3.1568761 -3.1708524 -3.1583781 -3.2278523 -3.3428216 -3.3827403 -3.3604417 -3.2900472 -3.2130017 -3.5327997 -3.8514628][-4.4420629 -4.0015297 -3.4830749 -2.8377938 -2.2969315 -2.1384416 -2.0937757 -2.3524139 -2.8232546 -3.3988633 -3.7760744 -3.8490338 -3.8465917 -3.9885688 -4.14723][-5.0077109 -4.3572917 -3.3794737 -2.390008 -1.4677241 -0.93180585 -0.68829417 -1.0392511 -1.9181507 -2.8578644 -3.6889918 -4.0860252 -4.3331647 -4.3730664 -4.2049112][-4.6906409 -4.0838394 -2.9757924 -1.5625501 -0.2723155 0.41868353 0.63127327 0.17260027 -0.83648849 -1.9643939 -3.0903754 -3.8305249 -4.2207317 -4.2450929 -4.1440077][-3.5894406 -2.8769269 -1.7214055 -0.22386742 1.1622238 1.8887711 2.0741172 1.413341 0.14830303 -1.379735 -2.6188858 -3.4108558 -4.0074692 -4.046658 -3.640173][-2.9854453 -2.301307 -0.999773 0.63757515 2.0293164 2.7689409 2.960103 2.3257608 1.0325685 -0.57922626 -2.0885642 -3.0426662 -3.71037 -3.9856973 -3.6985807][-2.6111088 -2.038347 -0.87640333 0.62914467 1.9569325 2.6738558 2.7871962 2.2725692 1.0895996 -0.33451176 -1.7261262 -2.7795687 -3.6306727 -3.9915166 -3.8515801][-2.4711709 -2.1844046 -1.3450189 -0.17109823 1.0706816 1.6990128 1.7504139 1.2642169 0.29116106 -0.91989827 -2.1407511 -3.2226067 -4.1594181 -4.6593332 -4.5424962][-3.0033908 -2.8060579 -2.09041 -1.1298809 -0.20234013 0.35118151 0.61980867 0.29517269 -0.49447036 -1.4626579 -2.5281816 -3.4985232 -4.35118 -4.9810791 -5.1211205][-3.3024635 -3.3591719 -3.14709 -2.1630676 -1.217972 -0.74535584 -0.52278638 -0.56001306 -1.0003722 -1.79495 -2.7265182 -3.524066 -4.3579588 -5.1868849 -5.4863529][-4.0305328 -4.0480471 -3.8208282 -3.3406496 -2.5584149 -1.8021228 -1.3426111 -1.2801359 -1.4591956 -1.8813174 -2.5986862 -3.3959723 -4.2042661 -5.1350141 -5.8927021][-4.14027 -4.5376291 -4.3298426 -3.9111927 -3.2716107 -2.6522787 -2.1216769 -1.7246413 -1.7146544 -1.9475169 -2.4132144 -3.110342 -3.8211658 -4.5683522 -5.4095054][-4.12616 -4.6934037 -4.979558 -4.5976782 -3.8404603 -3.2096357 -2.5825825 -2.0505021 -1.8313475 -1.8847117 -2.1818357 -2.7205491 -3.3206952 -4.2217236 -5.0347929]]...]
INFO - root - 2017-12-16 09:30:09.595922: step 73210, loss = 0.24, batch loss = 0.19 (27.2 examples/sec; 0.294 sec/batch; 21h:12m:04s remains)
INFO - root - 2017-12-16 09:30:12.408393: step 73220, loss = 0.27, batch loss = 0.21 (28.9 examples/sec; 0.277 sec/batch; 19h:57m:47s remains)
INFO - root - 2017-12-16 09:30:15.262832: step 73230, loss = 0.24, batch loss = 0.18 (28.5 examples/sec; 0.280 sec/batch; 20h:11m:38s remains)
INFO - root - 2017-12-16 09:30:18.128699: step 73240, loss = 0.25, batch loss = 0.19 (27.2 examples/sec; 0.294 sec/batch; 21h:10m:43s remains)
INFO - root - 2017-12-16 09:30:21.010688: step 73250, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.284 sec/batch; 20h:25m:10s remains)
INFO - root - 2017-12-16 09:30:23.822337: step 73260, loss = 0.22, batch loss = 0.16 (28.7 examples/sec; 0.279 sec/batch; 20h:04m:47s remains)
INFO - root - 2017-12-16 09:30:26.645211: step 73270, loss = 0.27, batch loss = 0.21 (26.9 examples/sec; 0.297 sec/batch; 21h:23m:13s remains)
INFO - root - 2017-12-16 09:30:29.560319: step 73280, loss = 0.24, batch loss = 0.18 (25.6 examples/sec; 0.313 sec/batch; 22h:32m:12s remains)
INFO - root - 2017-12-16 09:30:32.432653: step 73290, loss = 0.21, batch loss = 0.15 (28.9 examples/sec; 0.277 sec/batch; 19h:55m:31s remains)
INFO - root - 2017-12-16 09:30:35.253068: step 73300, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.285 sec/batch; 20h:32m:53s remains)
2017-12-16 09:30:35.754173: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6004963 -3.76838 -4.0910296 -4.3601918 -4.3554983 -4.1978874 -4.0163732 -3.8509305 -3.8040752 -3.710741 -3.6994765 -3.6578822 -3.7471888 -3.8084373 -3.58573][-3.4941306 -3.6183274 -3.7791343 -3.9103904 -3.9889774 -4.06314 -3.9954817 -3.6967139 -3.3832073 -3.1892858 -3.206135 -3.1705213 -3.0379367 -2.9712493 -2.6650381][-3.2325335 -3.2115152 -3.1383114 -3.0811481 -2.9369278 -2.9157677 -3.0942125 -3.2368879 -3.1509414 -3.0015152 -2.8318977 -2.6570704 -2.574965 -2.3986449 -2.125536][-2.6769309 -2.6485081 -2.4051816 -2.0156209 -1.5159531 -1.2877307 -1.3237748 -1.6069524 -2.0773938 -2.3955536 -2.5950761 -2.5256863 -2.390748 -2.4050381 -2.2783399][-2.2716799 -1.7361512 -1.033694 -0.41836405 0.24042606 0.54481411 0.49727345 0.21244049 -0.44458222 -1.2346213 -1.9487014 -2.3692203 -2.4471314 -2.6561713 -2.6387002][-2.6743562 -1.7530448 -0.47443318 0.88147688 2.1915636 2.8386364 2.8706512 2.3144755 1.400003 0.38410997 -0.5362711 -1.4586775 -2.3308072 -2.712316 -2.8257089][-3.44194 -2.0227273 -0.57036591 1.1705065 2.8669686 3.9276276 4.4476519 4.2178593 3.4158883 2.1033602 0.47441053 -0.84115458 -1.9315469 -2.7747812 -3.0837226][-4.799489 -3.3130817 -1.700913 0.21538067 2.00912 3.3838835 4.23818 4.2185 3.6726751 2.4946165 0.97708273 -0.68156672 -2.0231888 -2.7188597 -3.1034312][-6.4561949 -5.1446943 -3.5739233 -1.7908151 -0.063847065 1.6157222 2.7490859 3.0084834 2.6438837 1.6422696 0.23183918 -1.3742943 -2.7101638 -3.2261295 -3.3137779][-7.9621511 -7.0834417 -6.00177 -4.4635367 -2.8377104 -1.1216383 0.20617676 0.60411835 0.49166679 -0.28893566 -1.4568594 -2.8557343 -4.0211558 -4.5158648 -4.5783091][-8.4988241 -7.9993992 -7.2606616 -6.3636112 -5.5487804 -4.2946506 -2.8463302 -2.2536023 -2.390136 -3.1189351 -3.885679 -4.8210835 -5.5485606 -5.425334 -5.084249][-8.3072548 -8.201314 -7.7638659 -6.9616709 -6.2607489 -5.4542308 -4.7126064 -4.3210592 -4.0886631 -4.378396 -4.942451 -5.6931372 -6.0314236 -5.8233461 -5.3278532][-7.0988092 -7.1838636 -7.0648346 -6.5535793 -6.0035834 -5.378521 -4.8900924 -4.64606 -4.532289 -4.7261147 -4.9391341 -5.3398008 -5.5513773 -5.3156977 -4.8292789][-5.6318841 -5.3755074 -5.248754 -5.1130161 -4.9194112 -4.4705367 -4.1097 -4.1259322 -4.2330985 -4.19599 -4.1259818 -4.3867369 -4.5947704 -4.4362721 -4.0165753][-4.3867297 -4.0930853 -3.9621372 -3.8567712 -3.8242066 -3.6959467 -3.4770813 -3.3561296 -3.3367774 -3.5026953 -3.5888424 -3.5404696 -3.4659457 -3.3067613 -3.2016258]]...]
INFO - root - 2017-12-16 09:30:38.670156: step 73310, loss = 0.27, batch loss = 0.21 (28.1 examples/sec; 0.285 sec/batch; 20h:31m:31s remains)
INFO - root - 2017-12-16 09:30:41.525783: step 73320, loss = 0.25, batch loss = 0.20 (28.0 examples/sec; 0.286 sec/batch; 20h:33m:44s remains)
INFO - root - 2017-12-16 09:30:44.410299: step 73330, loss = 0.39, batch loss = 0.33 (28.8 examples/sec; 0.278 sec/batch; 19h:58m:41s remains)
INFO - root - 2017-12-16 09:30:47.244472: step 73340, loss = 0.37, batch loss = 0.31 (29.2 examples/sec; 0.274 sec/batch; 19h:44m:00s remains)
INFO - root - 2017-12-16 09:30:50.083872: step 73350, loss = 0.24, batch loss = 0.18 (29.1 examples/sec; 0.275 sec/batch; 19h:45m:45s remains)
INFO - root - 2017-12-16 09:30:52.878034: step 73360, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.284 sec/batch; 20h:26m:08s remains)
INFO - root - 2017-12-16 09:30:55.746443: step 73370, loss = 0.23, batch loss = 0.17 (27.5 examples/sec; 0.291 sec/batch; 20h:55m:56s remains)
INFO - root - 2017-12-16 09:30:58.611695: step 73380, loss = 0.24, batch loss = 0.18 (26.7 examples/sec; 0.300 sec/batch; 21h:33m:35s remains)
INFO - root - 2017-12-16 09:31:01.495435: step 73390, loss = 0.34, batch loss = 0.28 (28.0 examples/sec; 0.286 sec/batch; 20h:32m:56s remains)
INFO - root - 2017-12-16 09:31:04.388472: step 73400, loss = 0.21, batch loss = 0.15 (27.6 examples/sec; 0.290 sec/batch; 20h:53m:25s remains)
2017-12-16 09:31:04.918295: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0113983 -4.3355813 -4.6818633 -4.9485664 -5.0172868 -4.8934112 -4.8770142 -5.0537939 -5.2572575 -5.3970184 -5.4363179 -5.4092937 -5.1979351 -4.9442725 -4.6263542][-3.7977748 -4.0163255 -4.3683147 -4.5605268 -4.585741 -4.5993142 -4.7234545 -5.0906324 -5.6334085 -6.0002985 -6.208333 -6.3379784 -6.2158265 -5.8941574 -5.4800529][-3.6588116 -3.7138283 -3.823961 -4.0041356 -4.0219259 -3.8934247 -3.9886084 -4.4515209 -5.0935717 -5.8090143 -6.3805513 -6.7032547 -6.8041172 -6.6709547 -6.1826968][-3.3122981 -3.2522123 -3.217273 -3.1615376 -2.9935446 -2.7570577 -2.6443067 -3.1205578 -3.869431 -4.7075162 -5.5101595 -6.1871581 -6.5895834 -6.7015276 -6.3405995][-2.8179364 -2.5308862 -2.3425989 -2.1442533 -1.7198985 -1.3105195 -1.0083611 -1.2036102 -1.7105794 -2.6262445 -3.6912203 -4.7363534 -5.5733166 -5.91415 -5.7138844][-2.6248221 -2.2122788 -1.8386374 -1.4926946 -0.81436825 -0.069775581 0.58106852 0.70277166 0.48775816 -0.17463493 -1.3100471 -2.659302 -3.8993833 -4.7391977 -4.9898424][-2.5670185 -2.0358539 -1.4808362 -0.87589812 0.0051064491 0.99770546 2.032 2.3954372 2.3795056 1.8210731 0.80118132 -0.68571639 -2.2405479 -3.3413205 -3.9095278][-2.6941757 -2.0846274 -1.4602547 -0.86354852 0.081708431 1.1559048 2.1692257 2.7002778 2.9183993 2.5815148 1.7675462 0.34094429 -1.2359571 -2.5128319 -3.2231517][-2.9143081 -2.3754475 -1.8008859 -1.3053322 -0.61747789 0.30309868 1.2976542 1.7034974 1.823194 1.7022548 1.1426454 -0.10319853 -1.510514 -2.5697405 -3.1241179][-3.3032622 -2.9099028 -2.4635463 -2.0524876 -1.5880175 -0.99808717 -0.32745504 -0.011665344 -0.02809 -0.23842382 -0.6247263 -1.5841444 -2.6049154 -3.2694554 -3.6081648][-3.5356045 -3.1929533 -2.8566551 -2.6929727 -2.6570063 -2.3501396 -1.9462287 -1.9322131 -1.9296424 -2.1253846 -2.4140563 -2.9573193 -3.5546536 -3.9816196 -4.11938][-3.6159084 -3.4255438 -3.2158637 -3.1060989 -3.3198152 -3.3639975 -3.2122478 -3.3090148 -3.4548612 -3.6244535 -3.702353 -4.0316424 -4.3681636 -4.5090718 -4.5115132][-3.8556373 -3.7642844 -3.6451273 -3.5433192 -3.6227589 -3.6892433 -3.8108916 -3.9064775 -3.9808095 -4.2594261 -4.3679528 -4.4420128 -4.4857917 -4.5079546 -4.4738827][-3.9903114 -3.9930289 -3.7845011 -3.4753644 -3.3507266 -3.3064852 -3.3995981 -3.6576529 -3.8278925 -3.9507902 -4.0557795 -4.0315652 -3.9132781 -3.7841005 -3.7552607][-4.1196818 -4.1069407 -3.8850527 -3.3186288 -2.7930617 -2.6412845 -2.6484823 -2.8021114 -2.9809735 -3.254293 -3.442409 -3.3544927 -3.1151662 -2.9255733 -2.8954523]]...]
INFO - root - 2017-12-16 09:31:07.778676: step 73410, loss = 0.25, batch loss = 0.19 (28.3 examples/sec; 0.283 sec/batch; 20h:22m:09s remains)
INFO - root - 2017-12-16 09:31:10.673660: step 73420, loss = 0.28, batch loss = 0.22 (27.6 examples/sec; 0.290 sec/batch; 20h:51m:56s remains)
INFO - root - 2017-12-16 09:31:13.535732: step 73430, loss = 0.28, batch loss = 0.22 (28.3 examples/sec; 0.283 sec/batch; 20h:22m:20s remains)
INFO - root - 2017-12-16 09:31:16.411818: step 73440, loss = 0.21, batch loss = 0.15 (28.4 examples/sec; 0.282 sec/batch; 20h:16m:59s remains)
INFO - root - 2017-12-16 09:31:19.269468: step 73450, loss = 0.39, batch loss = 0.34 (28.3 examples/sec; 0.283 sec/batch; 20h:21m:21s remains)
INFO - root - 2017-12-16 09:31:22.168210: step 73460, loss = 0.22, batch loss = 0.16 (28.0 examples/sec; 0.286 sec/batch; 20h:35m:03s remains)
INFO - root - 2017-12-16 09:31:25.014459: step 73470, loss = 0.20, batch loss = 0.14 (27.3 examples/sec; 0.293 sec/batch; 21h:05m:08s remains)
INFO - root - 2017-12-16 09:31:27.872052: step 73480, loss = 0.22, batch loss = 0.16 (27.5 examples/sec; 0.290 sec/batch; 20h:53m:52s remains)
INFO - root - 2017-12-16 09:31:30.728975: step 73490, loss = 0.30, batch loss = 0.24 (27.7 examples/sec; 0.288 sec/batch; 20h:45m:15s remains)
INFO - root - 2017-12-16 09:31:33.579558: step 73500, loss = 0.25, batch loss = 0.20 (28.1 examples/sec; 0.284 sec/batch; 20h:27m:32s remains)
2017-12-16 09:31:34.164717: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4371018 -2.4588015 -2.7178245 -2.4274805 -2.3090212 -2.3239617 -2.69119 -3.0890868 -3.2418675 -3.3321891 -3.1976972 -3.0463767 -2.6948009 -2.5013254 -2.3525167][-1.3615799 -1.3375685 -1.6045 -1.5660949 -1.5868309 -1.5055032 -1.7960565 -2.174356 -2.4281793 -2.5731454 -2.6832852 -2.6013498 -2.1578581 -2.0574059 -1.9639299][-0.29578447 -0.45265198 -0.66737413 -0.72241616 -0.78380704 -0.92356634 -1.2407355 -1.3463109 -1.5165985 -1.9213586 -2.3894982 -2.434449 -2.2214358 -2.1968455 -2.1029668][0.62618971 0.35395193 -0.18455219 -0.44427204 -0.39527369 -0.49830341 -0.59306717 -0.46815491 -0.64216709 -1.3245118 -2.1150753 -2.5151157 -2.7959938 -2.9488614 -2.8370838][0.712697 0.48367167 0.094808578 -0.054759979 0.088615894 0.13634443 0.33811712 0.50830984 0.46922588 -0.17438745 -1.208698 -1.975642 -2.6664028 -3.0495167 -3.1988511][0.80461693 0.55727243 0.18992519 0.24248552 0.76879835 1.0946689 1.600677 2.1106453 2.0759196 1.1908965 -0.059511185 -1.2097347 -2.2403185 -2.992738 -3.4116404][0.32367086 0.2933569 0.27804565 0.60532618 1.4680176 2.3478937 3.181006 3.677947 3.5365715 2.5371966 1.1316247 -0.32227468 -1.8096862 -2.9067025 -3.6841512][-0.25735807 -0.24544668 -0.13912821 0.47773933 1.6452479 2.8288946 3.916234 4.387969 4.2573166 3.1932878 1.7239237 0.029977798 -1.5114236 -2.6973715 -3.6138177][-1.2199988 -1.06583 -0.72699475 -0.097703457 1.0387578 2.3495255 3.3424082 3.4602509 3.0862207 2.1689272 0.88233757 -0.6705718 -2.0001142 -2.9914522 -3.7537487][-2.1428616 -1.8334312 -1.3783641 -0.74210691 0.21130562 1.0446272 1.5155563 1.4104929 0.87311029 -0.040507793 -1.1163235 -2.3342698 -3.3373907 -3.9469123 -4.4090495][-2.6519332 -2.19393 -1.7934425 -1.2546611 -0.69509387 -0.18890333 -0.050601959 -0.67528248 -1.4885938 -2.3586881 -3.3070803 -4.2689061 -4.9601421 -5.2586412 -5.4637065][-3.0155387 -2.5224819 -2.1422584 -1.7132874 -1.6317539 -1.697758 -1.9534471 -2.8008244 -3.7329762 -4.4926643 -5.1237535 -5.6785159 -6.0335212 -6.1083565 -6.2454104][-2.9801097 -2.5451422 -2.2628224 -2.2042956 -2.4112751 -2.8511057 -3.4737287 -4.2722855 -4.9405155 -5.5047145 -5.8214149 -5.8076224 -5.8878961 -6.0235238 -6.2212172][-2.9032016 -2.4005833 -2.0878804 -1.890425 -2.1226232 -2.6263347 -3.2775617 -4.1403203 -4.7588096 -5.0345979 -5.1162724 -5.1396112 -5.2154627 -5.2984037 -5.6639285][-2.5938425 -1.9909217 -1.64025 -1.4102044 -1.6727304 -2.1419218 -2.7898638 -3.5259788 -4.1227326 -4.4049349 -4.2775259 -3.9538934 -3.870465 -4.1165104 -4.7062068]]...]
INFO - root - 2017-12-16 09:31:37.020493: step 73510, loss = 0.29, batch loss = 0.23 (29.4 examples/sec; 0.273 sec/batch; 19h:36m:21s remains)
INFO - root - 2017-12-16 09:31:39.867185: step 73520, loss = 0.27, batch loss = 0.21 (28.7 examples/sec; 0.279 sec/batch; 20h:03m:31s remains)
INFO - root - 2017-12-16 09:31:42.671055: step 73530, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.285 sec/batch; 20h:30m:41s remains)
INFO - root - 2017-12-16 09:31:45.506031: step 73540, loss = 0.30, batch loss = 0.24 (28.2 examples/sec; 0.283 sec/batch; 20h:23m:00s remains)
INFO - root - 2017-12-16 09:31:48.332134: step 73550, loss = 0.37, batch loss = 0.31 (28.9 examples/sec; 0.277 sec/batch; 19h:56m:24s remains)
INFO - root - 2017-12-16 09:31:51.199542: step 73560, loss = 0.23, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 20h:28m:23s remains)
INFO - root - 2017-12-16 09:31:54.034238: step 73570, loss = 0.27, batch loss = 0.21 (29.1 examples/sec; 0.274 sec/batch; 19h:44m:33s remains)
INFO - root - 2017-12-16 09:31:56.918775: step 73580, loss = 0.23, batch loss = 0.18 (28.6 examples/sec; 0.280 sec/batch; 20h:06m:23s remains)
INFO - root - 2017-12-16 09:31:59.720659: step 73590, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.283 sec/batch; 20h:22m:56s remains)
INFO - root - 2017-12-16 09:32:02.588725: step 73600, loss = 0.25, batch loss = 0.19 (26.7 examples/sec; 0.299 sec/batch; 21h:31m:59s remains)
2017-12-16 09:32:03.122579: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5061593 -4.6439028 -4.609695 -4.7633781 -5.136024 -5.6218724 -6.4610629 -7.5319839 -8.2524376 -8.2208519 -7.5728574 -6.4423723 -5.0267587 -3.6073732 -2.6493943][-5.1249294 -4.8186274 -4.3850541 -4.2379432 -4.3684797 -4.7980151 -5.6243024 -6.6851053 -7.764657 -8.3550062 -8.062499 -6.9557028 -5.4543939 -4.070868 -3.1309958][-6.1332746 -5.651619 -4.844677 -4.0200329 -3.5612161 -3.6799912 -4.3744278 -5.4446716 -6.5305223 -7.4301825 -7.8231158 -7.2267947 -5.8873215 -4.4891405 -3.4945617][-6.58594 -5.7864447 -4.70021 -3.5468843 -2.643733 -2.1865828 -2.3825808 -3.198946 -4.3147626 -5.4907331 -6.3486724 -6.6813736 -5.9982343 -4.7450919 -3.7223239][-5.5182514 -4.56339 -3.343533 -2.03182 -0.829818 0.044993877 0.37258673 -0.15863895 -1.2688789 -2.8249903 -4.3837609 -5.2799482 -5.2762518 -4.6495013 -3.503068][-3.9421806 -2.7485309 -1.320349 0.099846363 1.5234375 2.6696153 3.3783789 3.1892214 2.0395937 0.08245945 -1.9372761 -3.5632436 -4.2453375 -4.0005436 -3.151021][-2.9366074 -1.6474147 -0.19141245 1.492867 3.1092997 4.6693439 5.8140106 5.8282738 4.7891674 2.4636307 -0.21331358 -2.3845479 -3.5311151 -3.5992141 -2.9652488][-2.7280045 -1.4528105 -0.14034605 1.645606 3.3568721 4.9585009 6.281168 6.6616211 5.8807383 3.591856 0.87355137 -1.6929452 -3.2372072 -3.4856749 -3.0397429][-2.9032748 -1.7867155 -0.55755234 0.73695469 2.0963135 3.7624292 5.0951405 5.5438786 4.7928381 2.6961455 0.21920013 -2.1027091 -3.6132011 -3.9673972 -3.5353031][-3.9602969 -2.9477081 -2.0970418 -1.2139564 -0.10953188 1.0753593 2.126132 2.6355419 2.0595069 0.50892878 -1.458322 -3.1497626 -4.2653394 -4.4746771 -4.2347164][-5.8275704 -4.9900904 -4.3207426 -3.7981639 -3.2664213 -2.3244929 -1.3370914 -1.0023196 -1.2870965 -2.1489632 -3.2860436 -4.2070327 -4.8839908 -4.9210491 -4.6661229][-7.1655064 -6.8745341 -6.9595528 -6.6214709 -6.0011153 -5.4170856 -4.9904141 -4.5958114 -4.3272605 -4.4101973 -4.7427931 -5.0537949 -5.2776895 -5.2156935 -4.9880691][-7.6995697 -7.7368288 -7.8221035 -8.306222 -8.5959921 -8.2483788 -7.7203293 -7.2873917 -6.9848289 -6.46049 -5.840097 -5.4154463 -5.3540397 -5.289578 -5.0130482][-6.8482742 -7.2182021 -7.6354818 -8.215601 -8.7252254 -9.013195 -9.04657 -8.5474968 -7.7315507 -6.8903265 -6.1761842 -5.5454516 -5.1006675 -4.7626524 -4.6198897][-5.8218918 -6.089025 -6.6145158 -7.2614412 -7.832387 -8.1538172 -8.1594925 -7.7726631 -7.1071424 -6.3063993 -5.5546756 -4.9256172 -4.5500317 -4.2470207 -4.029562]]...]
INFO - root - 2017-12-16 09:32:05.986355: step 73610, loss = 0.23, batch loss = 0.17 (27.6 examples/sec; 0.289 sec/batch; 20h:48m:30s remains)
INFO - root - 2017-12-16 09:32:08.809838: step 73620, loss = 0.23, batch loss = 0.17 (27.5 examples/sec; 0.291 sec/batch; 20h:56m:58s remains)
INFO - root - 2017-12-16 09:32:11.607829: step 73630, loss = 0.23, batch loss = 0.17 (28.1 examples/sec; 0.285 sec/batch; 20h:28m:25s remains)
INFO - root - 2017-12-16 09:32:14.410301: step 73640, loss = 0.27, batch loss = 0.21 (28.8 examples/sec; 0.278 sec/batch; 19h:57m:40s remains)
INFO - root - 2017-12-16 09:32:17.290964: step 73650, loss = 0.26, batch loss = 0.21 (25.4 examples/sec; 0.315 sec/batch; 22h:40m:46s remains)
INFO - root - 2017-12-16 09:32:20.148381: step 73660, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 20h:19m:18s remains)
INFO - root - 2017-12-16 09:32:23.018245: step 73670, loss = 0.23, batch loss = 0.17 (25.4 examples/sec; 0.314 sec/batch; 22h:36m:18s remains)
INFO - root - 2017-12-16 09:32:25.896911: step 73680, loss = 0.27, batch loss = 0.21 (27.4 examples/sec; 0.292 sec/batch; 21h:00m:41s remains)
INFO - root - 2017-12-16 09:32:28.714732: step 73690, loss = 0.28, batch loss = 0.23 (28.2 examples/sec; 0.283 sec/batch; 20h:22m:40s remains)
INFO - root - 2017-12-16 09:32:31.572559: step 73700, loss = 0.26, batch loss = 0.20 (27.5 examples/sec; 0.291 sec/batch; 20h:53m:26s remains)
2017-12-16 09:32:32.106563: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8707628 -5.1840868 -5.4008412 -5.5429072 -5.4753704 -5.415668 -5.2546797 -5.1740804 -4.9481087 -4.720809 -4.6099029 -4.5259442 -4.5801105 -4.3441005 -4.342751][-5.2423635 -5.382442 -5.383709 -5.434309 -5.4003334 -5.5771418 -5.6430078 -5.6939521 -5.6224551 -5.5565457 -5.5720954 -5.3739109 -5.24792 -4.8930659 -4.8366489][-5.3449793 -5.269321 -5.1039314 -4.80506 -4.4222612 -4.6131454 -4.8923268 -5.3189812 -5.6766424 -5.8217907 -5.9039063 -5.7261419 -5.52756 -5.1059346 -5.0280166][-5.2677073 -4.8146119 -4.1397657 -3.484273 -2.9180672 -2.7909584 -2.9746728 -3.7930796 -4.6396475 -5.2122221 -5.5303907 -5.3089752 -4.9220867 -4.5792403 -4.7053366][-4.4909558 -3.7124958 -2.6640348 -1.5419872 -0.59910178 -0.2278595 -0.526778 -1.4960496 -2.4656665 -3.4574416 -4.0132442 -3.9664221 -3.7075171 -3.4379942 -3.6858928][-3.8823011 -2.8780751 -1.5205681 0.1640358 1.7786036 2.4298711 2.1339855 1.1723537 -0.017456055 -1.3413029 -1.9814484 -2.0772119 -1.8332009 -1.8191228 -2.2523108][-3.2286382 -2.3529494 -0.97959852 1.0035849 3.0036345 4.2979355 4.5607243 3.5223751 1.9523048 0.56337976 -0.08783102 -0.40641212 -0.24005508 -0.13554335 -0.54221654][-3.0272207 -2.1968246 -0.95035148 0.97697639 3.0732384 4.5436792 4.9997387 4.3454056 3.1178026 1.8261132 1.1983719 1.0918937 1.3239388 1.2932577 0.77623224][-3.2023091 -2.6604762 -1.7359314 -0.20619249 1.5779934 3.0390697 3.8418207 3.6231003 2.753727 1.9097705 1.6587429 1.5666361 1.5929532 1.5784712 1.1901793][-3.7582297 -3.6072726 -3.2137556 -2.1247473 -0.72085905 0.6579423 1.6439252 1.8230023 1.388845 0.97877407 1.004571 1.0733085 1.0946965 1.0259504 0.59281254][-5.3843746 -5.1850886 -4.8347187 -4.238174 -3.4873388 -2.3384256 -1.2961848 -0.82332826 -0.78536797 -0.71741867 -0.33842611 -0.071900845 0.0549345 -0.012269497 -0.24774981][-6.78849 -6.7411394 -6.4773769 -5.9761209 -5.3584318 -4.6890111 -4.0202241 -3.3491459 -2.9191365 -2.4863334 -1.7479601 -1.1852667 -0.93567967 -1.1423693 -1.3654807][-7.863874 -7.7896214 -7.6514578 -7.13141 -6.44201 -5.85009 -5.2600241 -4.5923777 -4.0793715 -3.5941687 -2.9167688 -2.2897892 -1.9587049 -2.1608922 -2.4339476][-8.8245335 -8.6016178 -8.1914225 -7.55221 -6.8181314 -6.1788068 -5.5694876 -5.0148864 -4.6366324 -4.2609482 -3.8483155 -3.4155703 -3.1912141 -3.4768858 -3.6730895][-9.3769932 -9.0692043 -8.528059 -7.8236952 -6.9815245 -6.1745372 -5.5455532 -5.1043091 -4.8527408 -4.8378305 -4.7134404 -4.3913822 -4.2434082 -4.5814142 -4.7232504]]...]
INFO - root - 2017-12-16 09:32:34.944920: step 73710, loss = 0.31, batch loss = 0.26 (27.9 examples/sec; 0.287 sec/batch; 20h:37m:04s remains)
INFO - root - 2017-12-16 09:32:37.782719: step 73720, loss = 0.22, batch loss = 0.17 (27.6 examples/sec; 0.290 sec/batch; 20h:52m:07s remains)
INFO - root - 2017-12-16 09:32:40.649013: step 73730, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 20h:24m:47s remains)
INFO - root - 2017-12-16 09:32:43.546402: step 73740, loss = 0.23, batch loss = 0.17 (27.5 examples/sec; 0.291 sec/batch; 20h:56m:35s remains)
INFO - root - 2017-12-16 09:32:46.391309: step 73750, loss = 0.27, batch loss = 0.22 (28.2 examples/sec; 0.284 sec/batch; 20h:22m:54s remains)
INFO - root - 2017-12-16 09:32:49.214596: step 73760, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 20h:01m:59s remains)
INFO - root - 2017-12-16 09:32:52.068665: step 73770, loss = 0.27, batch loss = 0.21 (28.7 examples/sec; 0.278 sec/batch; 20h:00m:50s remains)
INFO - root - 2017-12-16 09:32:54.908379: step 73780, loss = 0.39, batch loss = 0.33 (27.8 examples/sec; 0.288 sec/batch; 20h:40m:19s remains)
INFO - root - 2017-12-16 09:32:57.793373: step 73790, loss = 0.34, batch loss = 0.28 (26.8 examples/sec; 0.298 sec/batch; 21h:24m:53s remains)
INFO - root - 2017-12-16 09:33:00.667515: step 73800, loss = 0.23, batch loss = 0.17 (26.0 examples/sec; 0.308 sec/batch; 22h:09m:05s remains)
2017-12-16 09:33:01.184434: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3543715 -5.2278824 -5.4976244 -5.9103751 -6.4174843 -6.3181162 -6.476037 -6.8989058 -6.847249 -6.3787079 -5.7096186 -4.7415471 -3.5349417 -2.8268251 -2.4152071][-6.4701247 -5.9328394 -5.7567182 -6.1875439 -6.7243266 -7.6004133 -7.742362 -7.7087293 -7.4897385 -7.2610579 -6.7948065 -6.0384192 -5.18539 -4.4694743 -4.0149217][-6.86681 -6.6690922 -6.4951916 -6.6737423 -7.0493097 -6.9866753 -6.8001404 -6.9408383 -7.1644068 -7.3662777 -7.1597123 -6.8715529 -6.3616667 -5.9429388 -5.4420195][-7.1791124 -6.8722134 -6.7376356 -6.5864353 -6.371501 -6.2553358 -5.6177258 -4.9988875 -5.039835 -5.9552965 -7.0795512 -7.3864794 -7.0605907 -6.4488821 -5.96548][-6.9737778 -6.6942282 -6.725461 -6.0335789 -5.1261697 -3.9712305 -2.6639278 -1.6681838 -1.5094492 -3.0549071 -4.9255123 -6.4615602 -7.0937796 -6.5572662 -5.77695][-5.9238524 -5.7128906 -5.3913589 -4.3540573 -2.3772182 -0.098098755 1.7466812 2.7291875 2.4754572 0.67170334 -2.0443745 -4.7280431 -5.9690256 -6.0443306 -5.15546][-5.5479026 -4.8687472 -4.2114224 -2.4676654 0.3579402 3.3724122 6.060463 6.903389 6.2219028 3.7261372 0.65378904 -2.3909464 -4.56104 -4.4438987 -3.8417044][-5.1175847 -4.8662252 -4.2571497 -2.3029127 0.81861925 4.2263441 6.7188005 7.5608788 6.7672968 3.9370184 0.64897013 -1.7092586 -2.731132 -3.0214558 -2.8718436][-5.2901511 -5.1388364 -4.642858 -3.0985718 -0.89114237 2.3859735 4.8641253 5.7266645 5.1378336 2.7384462 -0.32404137 -2.6977539 -3.5345039 -3.0135264 -2.2852445][-5.3092213 -5.525259 -5.6122804 -4.5003042 -2.6568584 -0.7547977 0.95775557 1.8281531 1.7486882 -0.23557234 -2.4796765 -4.3744273 -4.7603493 -3.8514762 -2.9504251][-6.4883089 -6.6817112 -6.5999093 -6.1789217 -5.6106811 -4.4090095 -3.0749404 -2.6861858 -2.9866395 -4.012671 -5.1123881 -5.955677 -5.906599 -4.8969955 -3.7308238][-6.7622876 -7.6126776 -8.19958 -8.05677 -7.6168137 -7.2200007 -6.8849592 -6.5857363 -6.42616 -6.5603771 -6.7813416 -7.2418871 -6.9314375 -5.6336217 -4.4443035][-6.0805883 -7.16566 -8.2303658 -8.7420826 -9.0847378 -8.5952492 -7.9167528 -7.7285709 -7.5837321 -7.4748163 -7.1593237 -7.0816069 -6.7850051 -6.1441116 -5.2237797][-5.8244147 -6.4187355 -7.0041561 -7.6262045 -8.0902147 -8.0235758 -7.7371974 -7.4533739 -7.2137127 -6.9106979 -6.5495262 -6.3458004 -6.2401743 -5.8782406 -5.210731][-5.5278807 -5.7433004 -5.9615412 -6.2491751 -6.3569436 -6.4003496 -6.3363829 -6.0880733 -5.8048081 -5.7523117 -5.7412567 -5.4015813 -5.1386242 -5.2194262 -5.10477]]...]
INFO - root - 2017-12-16 09:33:04.038507: step 73810, loss = 0.22, batch loss = 0.16 (29.0 examples/sec; 0.276 sec/batch; 19h:50m:49s remains)
INFO - root - 2017-12-16 09:33:06.901465: step 73820, loss = 0.29, batch loss = 0.23 (27.6 examples/sec; 0.290 sec/batch; 20h:48m:43s remains)
INFO - root - 2017-12-16 09:33:09.755975: step 73830, loss = 0.23, batch loss = 0.17 (27.1 examples/sec; 0.295 sec/batch; 21h:11m:33s remains)
INFO - root - 2017-12-16 09:33:12.596631: step 73840, loss = 0.23, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 20h:02m:08s remains)
INFO - root - 2017-12-16 09:33:15.410120: step 73850, loss = 0.47, batch loss = 0.41 (29.1 examples/sec; 0.275 sec/batch; 19h:46m:20s remains)
INFO - root - 2017-12-16 09:33:18.267213: step 73860, loss = 0.20, batch loss = 0.14 (28.4 examples/sec; 0.282 sec/batch; 20h:14m:21s remains)
INFO - root - 2017-12-16 09:33:21.120794: step 73870, loss = 0.41, batch loss = 0.35 (29.0 examples/sec; 0.276 sec/batch; 19h:50m:27s remains)
INFO - root - 2017-12-16 09:33:23.988598: step 73880, loss = 0.28, batch loss = 0.22 (28.2 examples/sec; 0.283 sec/batch; 20h:21m:44s remains)
INFO - root - 2017-12-16 09:33:26.874951: step 73890, loss = 0.29, batch loss = 0.23 (28.0 examples/sec; 0.285 sec/batch; 20h:30m:19s remains)
INFO - root - 2017-12-16 09:33:29.737072: step 73900, loss = 0.23, batch loss = 0.17 (26.5 examples/sec; 0.302 sec/batch; 21h:40m:30s remains)
2017-12-16 09:33:30.284187: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8346834 -3.6279211 -3.38829 -3.2518907 -3.1822419 -3.3068056 -3.779283 -4.2045178 -4.56812 -4.7589927 -4.7805705 -4.5539236 -4.2471957 -4.0516186 -3.7158375][-3.0309937 -2.6549516 -2.2762177 -2.0199196 -1.9346783 -2.0819094 -2.5234694 -3.0867448 -3.4586689 -3.8250065 -3.9710841 -3.7605267 -3.4660022 -3.0539773 -2.374481][-2.094944 -1.6808167 -1.3816965 -0.87210274 -0.59647012 -0.71329927 -1.2312844 -1.879555 -2.4565392 -2.9289517 -3.0736058 -2.8919604 -2.3313382 -1.6388764 -0.82338905][-1.4670584 -1.1041112 -0.71723223 -0.080569267 0.38516569 0.46670485 0.047755718 -0.62660861 -1.3619912 -2.0438981 -2.4167092 -2.400887 -1.8236499 -1.0558505 -0.13323355][-1.4856496 -1.1091795 -0.55971456 0.099675655 0.57497454 0.82005739 0.58445406 0.035920143 -0.63991427 -1.4397068 -1.9477737 -2.032403 -1.7790501 -1.2269831 -0.40955782][-2.1342828 -1.67788 -1.0081823 -0.01457262 0.790833 1.1608458 1.0837889 0.58484507 -0.2068429 -1.1931238 -1.9423089 -2.2564206 -2.2187397 -1.8103468 -1.2222266][-2.4659338 -1.9929557 -1.3137782 -0.24886227 0.70597172 1.24436 1.414515 1.0312557 0.26699781 -0.887655 -1.8438308 -2.637373 -3.0921049 -3.0281212 -2.6564841][-2.6086543 -2.0056512 -1.2498705 -0.22054625 0.71025848 1.367156 1.5494213 1.2661738 0.52771091 -0.64929843 -1.7988899 -2.8093262 -3.4529362 -3.8072939 -3.7074068][-3.0009949 -2.4227488 -1.7179525 -0.72321105 0.23235464 1.0712128 1.4135742 1.1834316 0.38530064 -0.77015734 -2.0960495 -3.3402088 -4.1698418 -4.52658 -4.4472647][-3.3983102 -2.8149786 -2.1198947 -1.2113538 -0.27257013 0.50963354 0.797802 0.52183771 -0.20244646 -1.2892857 -2.5307584 -3.6956105 -4.4680886 -4.6994553 -4.6005135][-3.3158987 -2.8618088 -2.3395848 -1.6975794 -1.096158 -0.6129775 -0.53742218 -1.0519059 -1.729362 -2.6348264 -3.6913426 -4.5606461 -5.0416675 -5.0832515 -4.9257941][-3.3115234 -2.9077148 -2.587414 -2.2735848 -2.1866102 -2.1197083 -2.340157 -2.9668484 -3.5766461 -4.1728516 -4.7029271 -5.2642207 -5.581862 -5.6056547 -5.44263][-3.3402414 -3.0211325 -2.8912296 -2.9294562 -3.2279744 -3.46635 -3.8782513 -4.5189061 -5.1040068 -5.4350586 -5.5140934 -5.6398535 -5.7322264 -5.698432 -5.568162][-3.0810213 -2.7051239 -2.5539124 -2.81784 -3.4460654 -3.9758639 -4.5244374 -5.1656046 -5.5889478 -5.7771926 -5.6386766 -5.5146189 -5.4711037 -5.4559212 -5.4605875][-2.7185149 -2.2494323 -2.0871787 -2.3508284 -3.0594082 -3.7884421 -4.4859624 -5.0491648 -5.251339 -5.3531404 -5.1908603 -4.939105 -4.820025 -5.04022 -5.286602]]...]
INFO - root - 2017-12-16 09:33:33.121840: step 73910, loss = 0.26, batch loss = 0.20 (27.7 examples/sec; 0.289 sec/batch; 20h:46m:24s remains)
INFO - root - 2017-12-16 09:33:36.005740: step 73920, loss = 0.30, batch loss = 0.24 (26.7 examples/sec; 0.299 sec/batch; 21h:30m:24s remains)
INFO - root - 2017-12-16 09:33:38.926251: step 73930, loss = 0.27, batch loss = 0.21 (27.7 examples/sec; 0.289 sec/batch; 20h:43m:35s remains)
INFO - root - 2017-12-16 09:33:41.769890: step 73940, loss = 0.25, batch loss = 0.19 (29.0 examples/sec; 0.276 sec/batch; 19h:50m:01s remains)
INFO - root - 2017-12-16 09:33:44.599627: step 73950, loss = 0.23, batch loss = 0.17 (27.8 examples/sec; 0.288 sec/batch; 20h:40m:49s remains)
INFO - root - 2017-12-16 09:33:47.415443: step 73960, loss = 0.24, batch loss = 0.18 (28.8 examples/sec; 0.278 sec/batch; 19h:58m:56s remains)
INFO - root - 2017-12-16 09:33:50.218012: step 73970, loss = 0.28, batch loss = 0.22 (28.4 examples/sec; 0.281 sec/batch; 20h:12m:10s remains)
INFO - root - 2017-12-16 09:33:53.047410: step 73980, loss = 0.31, batch loss = 0.25 (27.8 examples/sec; 0.288 sec/batch; 20h:39m:06s remains)
INFO - root - 2017-12-16 09:33:55.876297: step 73990, loss = 0.27, batch loss = 0.21 (27.9 examples/sec; 0.287 sec/batch; 20h:36m:22s remains)
INFO - root - 2017-12-16 09:33:58.720436: step 74000, loss = 0.23, batch loss = 0.18 (29.1 examples/sec; 0.275 sec/batch; 19h:44m:38s remains)
2017-12-16 09:33:59.252355: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2360592 -5.6248088 -6.0815873 -6.73492 -7.4404044 -7.8832045 -8.3038578 -9.0532646 -9.7520714 -9.6788492 -9.0283527 -8.2128658 -7.1651783 -5.8776722 -4.7891784][-4.7252951 -5.2192421 -5.7592254 -6.4053912 -6.9917746 -7.6316791 -8.241519 -8.7892942 -9.0638227 -9.0185642 -8.5547333 -7.6602416 -6.6270685 -5.5810213 -4.6550822][-4.0149288 -4.4448838 -4.9020023 -5.3944383 -5.7584481 -6.0070434 -6.1758776 -6.5874038 -6.9781671 -7.1112795 -6.9840794 -6.5405579 -5.9973869 -5.3802757 -4.8303561][-3.012512 -3.3989167 -3.7396488 -3.9349258 -4.024744 -3.8836734 -3.611671 -3.5827005 -3.7480326 -3.9349208 -4.217876 -4.6785545 -5.0915465 -5.0996213 -4.886126][-2.1283967 -2.3922217 -2.3914442 -2.3319428 -2.0528376 -1.3499699 -0.56072664 -0.063349247 -0.020945549 -0.5929749 -1.6009657 -2.7556195 -3.7302127 -4.519628 -5.0184603][-1.1152208 -1.1685486 -0.88929749 -0.38279438 0.31019926 1.1740999 2.4000254 3.2813287 3.3477521 2.5188808 0.98339367 -1.1539402 -3.1855443 -4.43971 -5.0951471][-0.83148074 -0.59427166 -0.22842932 0.48628092 1.7316875 3.1283274 4.5303459 5.1672621 5.1157503 4.0547867 2.033577 -0.60216236 -2.9760666 -4.6171184 -5.4731097][-0.93511629 -0.77927041 -0.60910368 0.18668509 1.3008971 2.7243514 4.3698044 5.2153244 5.055563 3.8461285 1.8654375 -0.791904 -3.3153772 -4.9216619 -5.6744261][-1.6833246 -1.8397143 -1.7466609 -1.2403033 -0.31122303 1.1539841 2.5065126 3.2881866 3.5078564 2.5752687 0.872128 -1.5318866 -3.8767309 -5.46428 -6.2424879][-2.5327337 -2.9323184 -3.5128632 -3.3190873 -2.3194203 -1.2223532 -0.23058891 0.56440926 0.76006508 0.049643993 -1.0529144 -2.9622636 -5.0464339 -6.3318276 -6.8336997][-4.056016 -4.5058346 -4.9681444 -5.2094889 -4.9933949 -4.1141191 -3.1205173 -2.4767764 -2.2259312 -2.3848953 -3.0757837 -4.4021893 -5.7982712 -6.7871532 -7.0013871][-5.0382147 -5.8143325 -6.8139248 -7.2443132 -7.0094137 -6.5501404 -6.2150211 -5.3560371 -4.568152 -4.4349346 -4.8675065 -5.6223445 -6.37228 -7.0203171 -7.0026693][-6.2894926 -7.2772284 -8.1881809 -8.728756 -9.0116005 -8.5088062 -7.5913877 -6.768837 -6.1879859 -5.6790295 -5.5538645 -5.987668 -6.5802832 -6.9959674 -6.6896458][-7.3810091 -8.288269 -9.262888 -10.069802 -10.378834 -10.034899 -9.4333572 -8.2919645 -7.1804934 -6.6263218 -6.6123447 -6.6141872 -6.5213747 -6.4507437 -6.2146497][-7.8964891 -8.8243237 -9.7172737 -10.403451 -10.68074 -10.414549 -9.7847214 -8.85089 -8.248333 -7.6001797 -7.1007075 -6.9965472 -6.8117876 -6.4514084 -5.7946157]]...]
INFO - root - 2017-12-16 09:34:02.103213: step 74010, loss = 0.33, batch loss = 0.28 (27.9 examples/sec; 0.286 sec/batch; 20h:33m:12s remains)
INFO - root - 2017-12-16 09:34:04.911833: step 74020, loss = 0.30, batch loss = 0.25 (28.4 examples/sec; 0.281 sec/batch; 20h:11m:54s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 09:34:07.757529: step 74030, loss = 0.23, batch loss = 0.17 (26.6 examples/sec; 0.301 sec/batch; 21h:37m:53s remains)
INFO - root - 2017-12-16 09:34:10.589391: step 74040, loss = 0.25, batch loss = 0.19 (29.4 examples/sec; 0.272 sec/batch; 19h:31m:35s remains)
INFO - root - 2017-12-16 09:34:13.448450: step 74050, loss = 0.27, batch loss = 0.21 (27.9 examples/sec; 0.287 sec/batch; 20h:36m:15s remains)
INFO - root - 2017-12-16 09:34:16.268708: step 74060, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.277 sec/batch; 19h:51m:24s remains)
INFO - root - 2017-12-16 09:34:19.088521: step 74070, loss = 0.25, batch loss = 0.19 (26.6 examples/sec; 0.301 sec/batch; 21h:36m:51s remains)
INFO - root - 2017-12-16 09:34:21.879005: step 74080, loss = 0.24, batch loss = 0.18 (28.6 examples/sec; 0.280 sec/batch; 20h:04m:40s remains)
INFO - root - 2017-12-16 09:34:24.740104: step 74090, loss = 0.26, batch loss = 0.20 (29.1 examples/sec; 0.275 sec/batch; 19h:44m:43s remains)
INFO - root - 2017-12-16 09:34:27.587096: step 74100, loss = 0.33, batch loss = 0.27 (26.3 examples/sec; 0.304 sec/batch; 21h:48m:15s remains)
2017-12-16 09:34:28.148395: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0087228 -5.59284 -4.8944206 -4.0265336 -3.3562551 -2.9309814 -2.7364645 -2.6042302 -2.5661485 -2.4390857 -2.2499392 -2.0000348 -1.9857955 -2.0451248 -2.2205875][-5.8390884 -5.4929976 -4.7106285 -3.8040905 -3.1165853 -2.6795313 -2.4821553 -2.2464406 -2.0848651 -2.0193157 -1.940593 -1.6918652 -1.5604041 -1.4830008 -1.6163347][-4.7580624 -4.4942751 -3.895082 -3.1771438 -2.5356159 -2.1111233 -1.8965161 -1.71403 -1.6412394 -1.5973084 -1.6002419 -1.4732516 -1.3680735 -1.1835823 -1.1527548][-2.9662971 -2.8024776 -2.4727993 -1.9574735 -1.4472096 -1.1215813 -0.94872141 -1.0152283 -1.3178644 -1.4359143 -1.4040511 -1.2981057 -1.2477496 -0.93410516 -0.68232179][-1.2649877 -1.1532347 -1.000128 -0.767689 -0.33452892 -0.17215729 -0.067868233 -0.187356 -0.52279449 -1.0309315 -1.3788614 -1.1985147 -0.81105113 -0.48462296 -0.19938517][-0.22391129 0.0887022 0.37381077 0.72443676 1.1513991 1.1599832 1.0090284 0.67452669 0.17092752 -0.59109139 -1.0657282 -1.0185342 -0.75009489 -0.30479336 0.24481058][0.44063187 0.86148405 1.1793251 1.6829762 2.189189 2.4162626 2.4347444 1.8326483 1.0497012 0.26988792 -0.31595612 -0.54745865 -0.40233994 0.037721634 0.57257986][0.75560427 1.2533007 1.4930696 1.9109941 2.3358006 2.6178927 2.6651697 2.3526316 1.7001328 0.66665411 -0.13350773 -0.42073059 -0.23555422 -0.0066862106 0.47007275][0.24856806 0.56110811 0.81231022 1.3434601 1.7672191 2.0534053 2.0763674 1.7926259 1.043664 0.054823875 -0.60034442 -0.85702395 -0.7142725 -0.43009591 0.040913105][-0.43206954 -0.38115692 -0.45472932 -0.058541298 0.46125937 0.89334249 1.0279884 0.902863 0.40039206 -0.48394108 -1.1770563 -1.5077786 -1.4952939 -1.2194998 -0.77034259][-1.56511 -1.7709959 -1.9252653 -1.8512435 -1.6421072 -1.0259328 -0.44925308 -0.29284763 -0.52462029 -1.0899711 -1.6087887 -1.9433644 -2.0305486 -1.8539045 -1.5571837][-2.4651761 -2.7840214 -3.1804812 -3.1613708 -2.8536873 -2.3732343 -1.9498417 -1.4318137 -1.1254528 -1.5530658 -2.0668166 -2.3280764 -2.3800712 -2.2412021 -2.0956182][-3.4395151 -3.720171 -4.0290642 -3.931509 -3.6776245 -3.2144964 -2.6511736 -2.1449842 -1.8660131 -2.0267997 -2.2244236 -2.3751111 -2.4188101 -2.3229461 -2.2147448][-4.4784756 -4.4914131 -4.3947368 -4.1460729 -3.7007983 -3.1977158 -2.7053103 -2.3434055 -2.1198788 -2.1508913 -2.2690277 -2.3462479 -2.3998742 -2.3422801 -2.1593943][-4.6849022 -4.7522397 -4.7309828 -4.1807652 -3.4563122 -2.895421 -2.4169052 -2.01009 -1.7396135 -1.8627779 -2.1111138 -2.1907177 -2.1164851 -2.0758224 -2.0012426]]...]
INFO - root - 2017-12-16 09:34:31.022361: step 74110, loss = 0.38, batch loss = 0.32 (25.6 examples/sec; 0.313 sec/batch; 22h:28m:14s remains)
INFO - root - 2017-12-16 09:34:33.810822: step 74120, loss = 0.31, batch loss = 0.25 (29.8 examples/sec; 0.269 sec/batch; 19h:17m:58s remains)
INFO - root - 2017-12-16 09:34:36.631868: step 74130, loss = 0.26, batch loss = 0.20 (27.9 examples/sec; 0.286 sec/batch; 20h:33m:00s remains)
INFO - root - 2017-12-16 09:34:39.487658: step 74140, loss = 0.34, batch loss = 0.28 (27.9 examples/sec; 0.287 sec/batch; 20h:33m:50s remains)
INFO - root - 2017-12-16 09:34:42.316028: step 74150, loss = 0.26, batch loss = 0.20 (29.6 examples/sec; 0.270 sec/batch; 19h:23m:06s remains)
INFO - root - 2017-12-16 09:34:45.164353: step 74160, loss = 0.24, batch loss = 0.19 (29.1 examples/sec; 0.275 sec/batch; 19h:42m:04s remains)
INFO - root - 2017-12-16 09:34:48.010124: step 74170, loss = 0.41, batch loss = 0.35 (29.4 examples/sec; 0.272 sec/batch; 19h:32m:32s remains)
INFO - root - 2017-12-16 09:34:50.823529: step 74180, loss = 0.21, batch loss = 0.15 (28.2 examples/sec; 0.283 sec/batch; 20h:20m:32s remains)
INFO - root - 2017-12-16 09:34:53.618037: step 74190, loss = 0.28, batch loss = 0.22 (29.5 examples/sec; 0.271 sec/batch; 19h:26m:50s remains)
INFO - root - 2017-12-16 09:34:56.443993: step 74200, loss = 0.29, batch loss = 0.24 (28.3 examples/sec; 0.283 sec/batch; 20h:16m:51s remains)
2017-12-16 09:34:56.987222: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9227719 -4.98575 -5.0747128 -5.2310119 -5.2950711 -5.1750979 -5.1243567 -5.1893034 -5.4643211 -5.801405 -5.8225889 -5.8832169 -5.6889806 -5.1256766 -4.2102661][-5.1877484 -5.3030834 -5.3411036 -5.4562421 -5.5724716 -5.3380728 -5.1141467 -4.8310251 -4.905015 -5.05431 -5.2031875 -5.3505263 -5.176672 -5.061214 -4.5518246][-5.2762356 -5.5299149 -5.4622917 -5.2985644 -5.1001878 -4.6758742 -4.3082328 -3.9772403 -4.1182365 -4.2453747 -4.5378017 -4.7481751 -4.7064171 -4.789331 -4.56163][-4.741457 -5.006341 -5.0691361 -4.8320231 -4.4665332 -3.8068819 -3.0805526 -2.3738394 -2.2253737 -2.5715463 -3.2328074 -3.9217896 -4.5748253 -4.9955554 -4.8367753][-3.380219 -3.6407838 -3.727819 -3.5231264 -3.1908114 -2.3342793 -1.4292188 -0.58332491 -0.28973246 -0.86618209 -1.9041688 -3.113687 -4.3429537 -5.0934296 -5.1436062][-2.0622702 -2.102797 -1.9057455 -1.6262128 -1.1576614 -0.35604095 0.45893335 1.1791387 1.2817411 0.53337288 -0.71184587 -2.353224 -3.8240047 -4.9671974 -5.3577442][-0.88972235 -0.80310488 -0.79481554 -0.42865753 0.18923998 1.0492387 1.9505382 2.478744 2.2191768 1.16928 -0.38615704 -2.2666395 -3.9809229 -5.055264 -5.34439][0.11143446 -0.0098333359 -0.24076271 -0.21980715 0.020493031 0.94568825 1.8762355 2.1313367 1.7119665 0.54612207 -1.024132 -2.8358605 -4.3733392 -5.285192 -5.3951993][-0.032088757 -0.064255238 -0.31461096 -0.78566074 -0.88822818 -0.17663479 0.36350965 0.65503979 0.51654243 -0.3543148 -1.6436844 -3.308722 -4.6714606 -5.5103846 -5.6672297][-0.9312036 -1.0359938 -1.4885666 -2.0935838 -2.2826159 -1.9148498 -1.5249255 -1.1427522 -1.2006972 -1.6974857 -2.6312704 -3.9954395 -5.1182642 -5.6564455 -5.6925735][-2.1632116 -2.5382509 -3.2119751 -3.9308851 -4.306469 -4.0827875 -3.5644817 -3.2552228 -3.2045202 -3.5095325 -4.0375714 -4.9546485 -5.7620773 -6.0127869 -5.8764825][-3.6016686 -4.1296291 -4.8733997 -5.6782751 -6.2222033 -6.019671 -5.648365 -5.2716627 -4.965857 -5.1153655 -5.5208316 -6.1435103 -6.5723467 -6.4622459 -6.1356039][-4.3611894 -5.0248861 -5.818181 -6.5739679 -6.9589357 -6.8870153 -6.6427269 -6.2198658 -5.9898424 -5.9322281 -6.1080823 -6.5661068 -6.7593541 -6.4429464 -6.0461354][-4.908319 -5.4150743 -6.0347371 -6.5544281 -6.768981 -6.6714683 -6.38303 -6.1759772 -6.050333 -5.9183421 -5.9299994 -6.1471548 -6.1444912 -5.8144622 -5.4680214][-5.3247809 -5.63114 -6.022172 -6.2375908 -6.2642241 -6.0612855 -5.79307 -5.694849 -5.5617151 -5.5969505 -5.67419 -5.7657037 -5.7545657 -5.4560795 -5.1392317]]...]
INFO - root - 2017-12-16 09:34:59.787281: step 74210, loss = 0.26, batch loss = 0.21 (28.3 examples/sec; 0.283 sec/batch; 20h:17m:40s remains)
INFO - root - 2017-12-16 09:35:02.639903: step 74220, loss = 0.27, batch loss = 0.22 (28.2 examples/sec; 0.284 sec/batch; 20h:22m:10s remains)
INFO - root - 2017-12-16 09:35:05.455952: step 74230, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.286 sec/batch; 20h:29m:48s remains)
INFO - root - 2017-12-16 09:35:08.342610: step 74240, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 20h:26m:53s remains)
INFO - root - 2017-12-16 09:35:11.257739: step 74250, loss = 0.32, batch loss = 0.26 (27.8 examples/sec; 0.288 sec/batch; 20h:39m:07s remains)
INFO - root - 2017-12-16 09:35:14.085231: step 74260, loss = 0.26, batch loss = 0.21 (29.3 examples/sec; 0.273 sec/batch; 19h:33m:51s remains)
INFO - root - 2017-12-16 09:35:16.937988: step 74270, loss = 0.34, batch loss = 0.28 (28.2 examples/sec; 0.284 sec/batch; 20h:20m:32s remains)
INFO - root - 2017-12-16 09:35:19.760824: step 74280, loss = 0.31, batch loss = 0.25 (29.3 examples/sec; 0.273 sec/batch; 19h:35m:23s remains)
INFO - root - 2017-12-16 09:35:22.570040: step 74290, loss = 0.25, batch loss = 0.19 (29.2 examples/sec; 0.274 sec/batch; 19h:38m:30s remains)
INFO - root - 2017-12-16 09:35:25.408080: step 74300, loss = 0.24, batch loss = 0.19 (27.7 examples/sec; 0.289 sec/batch; 20h:43m:18s remains)
2017-12-16 09:35:25.948830: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.884881 -4.9731 -4.9726386 -5.0922256 -5.1805882 -5.14519 -4.861268 -4.887486 -5.3638582 -5.9747438 -6.5913682 -7.3865242 -8.3165932 -8.9686928 -9.3796873][-4.6847262 -4.9368458 -4.9726329 -5.0306621 -5.0811744 -5.2828212 -5.1123481 -4.7119741 -4.6151414 -5.0061913 -5.9267588 -6.791338 -7.6610694 -8.2072477 -8.6954613][-4.2846727 -4.5060468 -4.3953357 -4.2535949 -4.1113706 -4.2518024 -4.4855833 -4.5825124 -4.6806703 -4.7241592 -5.164022 -5.7816539 -6.7700586 -7.3221416 -7.6688995][-3.3635492 -3.711009 -3.7919004 -3.277323 -2.7079506 -2.5693555 -2.512569 -2.8933775 -3.5750439 -3.9219794 -4.0822244 -4.4848542 -5.4298744 -6.1658149 -7.024539][-2.4535022 -2.4358218 -2.1444583 -1.793417 -1.3448601 -0.94071984 -0.639817 -0.71765757 -1.0695851 -1.8569925 -2.7299728 -2.9276209 -3.4108949 -4.2066565 -5.4380746][-2.3112967 -1.5857582 -0.60707974 0.46065712 1.3237586 1.5306025 1.5798526 1.4230342 1.0230293 0.56661892 0.22978115 -0.39370012 -1.3762219 -1.9308674 -3.0415511][-2.5909638 -1.7811022 -0.61298895 0.80480576 2.2959652 3.2939463 3.6514425 3.1938753 2.7145329 2.120122 1.6640449 1.2844892 0.58368349 -0.423481 -2.0656321][-3.2770314 -2.3821495 -1.1042843 0.65385962 2.3178573 3.4400811 4.3139315 4.4150019 3.9160614 2.8519659 2.1457009 1.5559187 0.69253063 -0.35934162 -1.6706088][-4.2337565 -3.7360456 -2.56772 -1.0959568 0.58992958 1.757494 2.5326109 2.6377149 2.7313533 2.2301102 1.7000718 0.95138168 0.19603205 -0.58230662 -1.4419348][-4.5921869 -4.3248816 -3.9148667 -3.0116081 -1.8185499 -0.94413304 -0.22522783 -0.056332111 -0.10784388 -0.35892582 -0.30949783 -0.63028932 -1.2222562 -1.6979568 -2.3527591][-5.0600266 -4.7666712 -4.5228887 -4.2350092 -4.1604571 -3.7544396 -3.0392342 -2.9922478 -3.1223674 -3.2506819 -3.2068405 -3.4165468 -3.8358459 -3.8417058 -3.9641824][-5.4869576 -5.2523713 -5.2735157 -5.2941394 -5.375432 -5.506156 -5.6921778 -5.7537794 -5.68746 -5.72721 -5.5581417 -5.6047158 -5.6421309 -5.4312105 -5.3734188][-5.3260536 -5.3597569 -5.6463389 -5.8660846 -6.2051687 -6.3048882 -6.3403463 -6.360117 -6.3123326 -6.2903428 -6.2317672 -6.23078 -6.2787628 -6.13997 -5.94044][-4.3024631 -4.4218397 -4.6653452 -5.089613 -5.532907 -5.7653418 -5.9522491 -6.0214725 -6.1111059 -6.0834842 -5.9097528 -6.1574655 -6.4089165 -6.238379 -6.044188][-4.324439 -4.1729984 -4.1430645 -4.3336062 -4.5529819 -4.8353086 -5.1130323 -5.1923585 -5.1091266 -5.2808795 -5.436677 -5.3682532 -5.2390308 -5.2718191 -5.3380423]]...]
INFO - root - 2017-12-16 09:35:28.780493: step 74310, loss = 0.28, batch loss = 0.22 (28.8 examples/sec; 0.278 sec/batch; 19h:55m:35s remains)
INFO - root - 2017-12-16 09:35:31.626528: step 74320, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.283 sec/batch; 20h:18m:06s remains)
INFO - root - 2017-12-16 09:35:34.430191: step 74330, loss = 0.30, batch loss = 0.24 (28.2 examples/sec; 0.284 sec/batch; 20h:21m:25s remains)
INFO - root - 2017-12-16 09:35:37.279837: step 74340, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.284 sec/batch; 20h:22m:07s remains)
INFO - root - 2017-12-16 09:35:40.177134: step 74350, loss = 0.21, batch loss = 0.15 (28.4 examples/sec; 0.282 sec/batch; 20h:11m:40s remains)
INFO - root - 2017-12-16 09:35:43.037168: step 74360, loss = 0.22, batch loss = 0.17 (28.7 examples/sec; 0.279 sec/batch; 20h:01m:11s remains)
INFO - root - 2017-12-16 09:35:45.889667: step 74370, loss = 0.29, batch loss = 0.23 (25.7 examples/sec; 0.311 sec/batch; 22h:18m:56s remains)
INFO - root - 2017-12-16 09:35:48.708713: step 74380, loss = 0.26, batch loss = 0.20 (28.4 examples/sec; 0.282 sec/batch; 20h:11m:06s remains)
INFO - root - 2017-12-16 09:35:51.577353: step 74390, loss = 0.30, batch loss = 0.24 (28.5 examples/sec; 0.281 sec/batch; 20h:08m:41s remains)
INFO - root - 2017-12-16 09:35:54.441655: step 74400, loss = 0.32, batch loss = 0.27 (28.2 examples/sec; 0.284 sec/batch; 20h:20m:43s remains)
2017-12-16 09:35:54.966693: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6064572 -3.734416 -3.8230968 -3.7725322 -3.7066288 -3.6937048 -3.6489389 -3.8795493 -4.09301 -4.2788835 -4.4288411 -4.3859057 -4.1201391 -3.598907 -3.444592][-3.26908 -3.3525059 -3.4192977 -3.4339511 -3.4639444 -3.6818037 -3.8081064 -3.8775282 -3.8981354 -4.0837646 -4.277215 -4.2203865 -4.0785336 -3.816045 -3.7336326][-2.6814494 -2.605092 -2.6431079 -2.7507672 -2.949424 -3.1273551 -3.2602153 -3.5484757 -3.5677013 -3.5719137 -3.6742153 -3.8089185 -3.8318388 -3.7304254 -3.7491252][-1.7076805 -1.6807797 -1.8703701 -2.0971169 -2.4678013 -2.7535315 -2.86329 -2.9788256 -2.884553 -2.9208038 -2.8995271 -3.1503224 -3.4832473 -3.7645965 -4.0843806][-0.87885618 -0.68390846 -0.80401444 -1.1925728 -1.549571 -1.8845804 -1.9845994 -1.9083014 -1.5654578 -1.5388198 -1.6182165 -1.9291854 -2.2552924 -2.8254757 -3.4450982][-0.023691654 0.22501707 0.042827129 -0.43655872 -0.87262726 -1.1546774 -1.0542679 -0.76509714 -0.30947208 -0.10654116 -0.19800901 -0.76918912 -1.3891683 -2.0329077 -2.5983868][0.10009241 0.30625963 0.14603376 -0.30746222 -0.60209703 -0.66725755 -0.34212637 0.17483711 0.77207327 0.8886528 0.564558 -0.11822033 -0.98075485 -1.7067389 -2.1912606][-0.611161 -0.25454998 -0.25387287 -0.4854176 -0.51479912 -0.2720871 0.32268429 1.0545654 1.6520963 1.6389022 1.0114312 0.011668205 -1.1182733 -1.9922788 -2.560534][-1.6078351 -1.2339947 -0.98513508 -1.042264 -0.74809957 -0.28691244 0.34729767 1.2002587 1.6787276 1.6515574 1.013433 -0.12081385 -1.3460462 -2.2075517 -2.6692374][-2.5751586 -2.2205296 -1.8159678 -1.7378953 -1.3378077 -0.768806 -0.070946693 0.5816493 0.922709 0.81448126 0.029438972 -1.0366123 -2.0901029 -2.7783186 -3.0647271][-3.2529831 -3.1494656 -2.921689 -2.7396259 -2.3575559 -1.7329767 -1.0877247 -0.669636 -0.52034545 -0.66029859 -1.3585501 -2.1920197 -2.93295 -3.4014931 -3.4940991][-4.3662004 -4.443975 -4.2985744 -4.0666862 -3.6347723 -3.0848181 -2.60604 -2.2148304 -2.0677097 -2.2944164 -2.9626689 -3.5341341 -3.8608167 -4.0398874 -3.9350865][-5.1606512 -5.5410266 -5.6430578 -5.4175797 -4.9474816 -4.2916179 -3.8643832 -3.6834292 -3.6667826 -3.7055452 -4.0103064 -4.2416434 -4.21314 -4.0237503 -3.7441263][-5.8806343 -6.4936514 -6.6663866 -6.6115332 -6.1889796 -5.7118611 -5.2717743 -4.943068 -4.8819332 -4.9439268 -5.012219 -4.843555 -4.5363145 -4.1321359 -3.6851544][-6.5798664 -7.2657628 -7.6394577 -7.5609832 -7.0756483 -6.4308262 -5.8185863 -5.4766536 -5.3777637 -5.3654685 -5.4671488 -5.2358913 -4.7865524 -4.2843618 -3.8230217]]...]
INFO - root - 2017-12-16 09:35:57.879059: step 74410, loss = 0.40, batch loss = 0.35 (28.0 examples/sec; 0.286 sec/batch; 20h:30m:35s remains)
INFO - root - 2017-12-16 09:36:00.759157: step 74420, loss = 0.23, batch loss = 0.18 (27.6 examples/sec; 0.290 sec/batch; 20h:46m:58s remains)
INFO - root - 2017-12-16 09:36:03.604073: step 74430, loss = 0.25, batch loss = 0.19 (29.0 examples/sec; 0.275 sec/batch; 19h:44m:51s remains)
INFO - root - 2017-12-16 09:36:06.458622: step 74440, loss = 0.30, batch loss = 0.24 (28.7 examples/sec; 0.279 sec/batch; 19h:57m:59s remains)
INFO - root - 2017-12-16 09:36:09.300097: step 74450, loss = 0.27, batch loss = 0.21 (29.2 examples/sec; 0.274 sec/batch; 19h:37m:20s remains)
INFO - root - 2017-12-16 09:36:12.192558: step 74460, loss = 0.22, batch loss = 0.16 (27.6 examples/sec; 0.290 sec/batch; 20h:45m:28s remains)
INFO - root - 2017-12-16 09:36:14.988079: step 74470, loss = 0.37, batch loss = 0.31 (28.0 examples/sec; 0.286 sec/batch; 20h:27m:56s remains)
INFO - root - 2017-12-16 09:36:17.870974: step 74480, loss = 0.27, batch loss = 0.21 (27.2 examples/sec; 0.294 sec/batch; 21h:05m:01s remains)
INFO - root - 2017-12-16 09:36:20.748260: step 74490, loss = 0.26, batch loss = 0.20 (27.6 examples/sec; 0.289 sec/batch; 20h:44m:48s remains)
INFO - root - 2017-12-16 09:36:23.621155: step 74500, loss = 0.29, batch loss = 0.23 (26.7 examples/sec; 0.299 sec/batch; 21h:26m:19s remains)
2017-12-16 09:36:24.162586: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8211722 -2.157845 -2.5727527 -2.9587805 -3.1524067 -3.2327204 -3.3927097 -3.3446541 -3.4031959 -3.4317174 -3.4495864 -3.5006967 -3.5021462 -3.595964 -3.9053216][-1.9702654 -2.1744883 -2.4029634 -2.8202028 -3.1269641 -3.2949388 -3.4645195 -3.5557561 -3.7076635 -3.6169295 -3.4614594 -3.4007907 -3.3564429 -3.4845424 -3.7343361][-2.5952139 -2.6504059 -2.6748757 -2.775857 -2.8676946 -2.9941783 -3.2314658 -3.4556131 -3.7373466 -3.7471232 -3.636385 -3.6007214 -3.5400772 -3.5062158 -3.6341789][-3.4332988 -3.10799 -2.729425 -2.5042484 -2.3465085 -2.413862 -2.6333351 -2.8593647 -3.1439319 -3.266897 -3.2801704 -3.1758418 -3.0554819 -3.2077503 -3.4863954][-3.5404985 -2.690558 -1.8441234 -1.3883221 -1.157053 -1.1418831 -1.3341999 -1.7402816 -2.0685921 -2.0652039 -1.9043953 -1.8948743 -1.8123879 -1.9437878 -2.3672948][-3.2195115 -1.9389749 -0.7857492 0.036885738 0.51181221 0.49099636 0.20510435 -0.23955727 -0.69756818 -0.91016412 -0.80741429 -0.64667511 -0.44039583 -0.692384 -1.1786025][-2.2535791 -0.85548162 0.33185482 1.2155075 1.7648787 1.9837227 1.8745556 1.3602891 0.81682205 0.43408918 0.2189765 0.17013502 0.38485575 0.11070108 -0.64608765][-1.0210657 0.12540293 1.039156 1.9521856 2.5701065 2.8992419 2.942924 2.6064396 2.0785422 1.605835 1.3123636 1.0763955 1.0193181 0.40838003 -0.66601825][-0.34217453 0.34786081 1.0132723 1.6695943 2.1810532 2.7912107 2.9593263 2.749928 2.316381 1.8383446 1.5319014 1.1558428 0.939291 0.1920495 -1.0362597][-0.21918201 -0.15101862 -0.089652538 0.57145786 1.0971932 1.6386523 1.8394179 1.841259 1.4897919 0.95883846 0.51715803 0.18642282 0.0032448769 -0.73434448 -1.8714836][-0.4980557 -0.65273952 -0.78956127 -0.56462121 -0.26267385 0.25211525 0.44463444 0.41849422 0.022444248 -0.45537019 -0.90680838 -1.357976 -1.693012 -2.2915754 -3.1771398][-0.84738851 -1.325206 -1.7527912 -1.6507077 -1.5049038 -1.1339281 -1.0338278 -1.0207472 -1.2489543 -1.7559109 -2.227344 -2.6912265 -3.0838008 -3.6230731 -4.2870088][-1.5183313 -1.939172 -2.4511948 -2.6179171 -2.7135129 -2.5287085 -2.4285839 -2.3731947 -2.5565176 -2.8337326 -3.1293783 -3.5483837 -3.9022164 -4.27549 -4.69838][-1.8430057 -2.1734362 -2.5498674 -2.878746 -3.1739154 -3.2746706 -3.4307451 -3.5106449 -3.6471014 -3.8149848 -3.9634707 -4.1585016 -4.3169918 -4.4374852 -4.5710721][-2.6374717 -2.6509223 -2.8209302 -3.0485194 -3.2908707 -3.4403853 -3.586525 -3.815115 -3.9797506 -4.1375489 -4.2220373 -4.24246 -4.173265 -4.1469655 -4.1087866]]...]
INFO - root - 2017-12-16 09:36:27.004557: step 74510, loss = 0.23, batch loss = 0.17 (26.8 examples/sec; 0.299 sec/batch; 21h:25m:45s remains)
INFO - root - 2017-12-16 09:36:29.842854: step 74520, loss = 0.26, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 20h:29m:21s remains)
INFO - root - 2017-12-16 09:36:32.626418: step 74530, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.281 sec/batch; 20h:09m:36s remains)
INFO - root - 2017-12-16 09:36:35.481327: step 74540, loss = 0.22, batch loss = 0.16 (28.8 examples/sec; 0.278 sec/batch; 19h:53m:22s remains)
INFO - root - 2017-12-16 09:36:38.334287: step 74550, loss = 0.24, batch loss = 0.19 (28.7 examples/sec; 0.279 sec/batch; 19h:59m:19s remains)
INFO - root - 2017-12-16 09:36:41.226385: step 74560, loss = 0.34, batch loss = 0.29 (28.6 examples/sec; 0.280 sec/batch; 20h:03m:11s remains)
INFO - root - 2017-12-16 09:36:44.083196: step 74570, loss = 0.28, batch loss = 0.22 (26.6 examples/sec; 0.301 sec/batch; 21h:34m:02s remains)
INFO - root - 2017-12-16 09:36:46.906914: step 74580, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 20h:24m:04s remains)
INFO - root - 2017-12-16 09:36:49.816925: step 74590, loss = 0.39, batch loss = 0.33 (27.5 examples/sec; 0.291 sec/batch; 20h:52m:06s remains)
INFO - root - 2017-12-16 09:36:52.721371: step 74600, loss = 0.25, batch loss = 0.19 (27.2 examples/sec; 0.294 sec/batch; 21h:04m:20s remains)
2017-12-16 09:36:53.281415: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2502778 -2.3947458 -2.523561 -2.5934005 -2.6498723 -2.6289976 -2.5979357 -2.5548382 -2.5306053 -2.473491 -2.3775275 -2.3482997 -2.2892106 -2.2618837 -2.1811523][-2.5669312 -2.7666996 -2.9753227 -3.1352088 -3.2392423 -3.2562895 -3.2133389 -3.0840545 -3.0072031 -2.9628873 -2.873486 -2.7885838 -2.6861072 -2.6163812 -2.4734416][-3.0985742 -3.4169972 -3.6525748 -3.7743926 -3.8623614 -3.816474 -3.7330954 -3.7063041 -3.5616379 -3.3879337 -3.3411634 -3.3290315 -3.2710764 -3.1552272 -3.0104709][-3.624959 -3.8845041 -4.0235186 -4.0514607 -4.0380316 -3.8738642 -3.6973231 -3.5767808 -3.4364688 -3.4319539 -3.3838234 -3.4356985 -3.5345469 -3.5552511 -3.4345832][-4.0528684 -4.1610689 -4.136445 -3.86805 -3.5646281 -3.3357117 -3.1106892 -2.833581 -2.7010522 -2.7853346 -2.8211679 -3.022882 -3.2363372 -3.5671721 -3.672555][-4.0143261 -4.05359 -3.7910137 -3.2749443 -2.6939766 -2.1992896 -1.8794391 -1.7167208 -1.6775982 -1.6355295 -1.7413301 -2.2107315 -2.7452621 -3.2317238 -3.5055835][-3.6612036 -3.5168812 -3.0182679 -2.2478733 -1.3321004 -0.573503 -0.14999533 0.00074386597 -0.10012484 -0.40340471 -0.72915411 -1.2044201 -2.0006566 -2.975853 -3.6146708][-3.1702614 -2.8394904 -2.1489861 -1.2181292 -0.19505596 0.67020178 1.1751356 1.4614711 1.4062448 0.96757078 0.4296937 -0.54835176 -1.6555154 -2.6523829 -3.5313425][-2.2550344 -2.0596085 -1.4216437 -0.56444645 0.23045111 1.0957308 1.8921952 2.2433133 2.0299277 1.6947236 1.148952 -0.0435791 -1.4639671 -2.8011582 -3.6604717][-1.7050047 -1.565484 -1.1241817 -0.48076797 0.123909 0.94572592 1.757164 2.26124 2.186378 1.8152561 1.0749979 -0.20820522 -1.6583211 -2.953824 -3.8460038][-1.1569734 -0.95333982 -0.83169246 -0.65982842 -0.34412193 0.36886168 1.1990867 1.8155813 1.6749215 1.3012567 0.6177001 -0.7491138 -2.2222946 -3.2931514 -3.9586463][-1.0162554 -0.8296566 -0.780262 -0.80818081 -0.7947185 -0.42680168 0.12509203 0.76244879 0.87159872 0.49402237 -0.27261496 -1.3409557 -2.4685264 -3.3625035 -3.849525][-1.03638 -0.83321 -0.9599719 -1.234277 -1.3484502 -0.94403028 -0.31444359 0.041059494 0.1483779 0.099365234 -0.3528018 -1.1901052 -2.0917165 -2.7065539 -2.9512763][-1.0889206 -0.77635908 -0.83973503 -1.1616974 -1.2117774 -0.975759 -0.51253605 -0.048212528 0.085652351 0.020701885 -0.43651152 -0.962549 -1.5766673 -1.9327543 -2.0976846][-0.85633659 -0.48196745 -0.4944222 -0.79455924 -0.8477087 -0.60521865 -0.24496412 0.00093364716 -0.0801487 -0.055015564 -0.32765436 -0.84214568 -1.3993797 -1.4940617 -1.4199355]]...]
INFO - root - 2017-12-16 09:36:56.123141: step 74610, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.283 sec/batch; 20h:18m:16s remains)
INFO - root - 2017-12-16 09:36:58.944397: step 74620, loss = 0.25, batch loss = 0.20 (28.2 examples/sec; 0.284 sec/batch; 20h:20m:30s remains)
INFO - root - 2017-12-16 09:37:01.782375: step 74630, loss = 0.26, batch loss = 0.21 (29.3 examples/sec; 0.273 sec/batch; 19h:31m:48s remains)
INFO - root - 2017-12-16 09:37:04.615002: step 74640, loss = 0.21, batch loss = 0.16 (27.8 examples/sec; 0.288 sec/batch; 20h:38m:08s remains)
INFO - root - 2017-12-16 09:37:07.414494: step 74650, loss = 0.28, batch loss = 0.22 (28.7 examples/sec; 0.279 sec/batch; 19h:57m:06s remains)
INFO - root - 2017-12-16 09:37:10.254876: step 74660, loss = 0.22, batch loss = 0.16 (27.1 examples/sec; 0.295 sec/batch; 21h:09m:43s remains)
INFO - root - 2017-12-16 09:37:13.079843: step 74670, loss = 0.49, batch loss = 0.43 (28.3 examples/sec; 0.282 sec/batch; 20h:13m:18s remains)
INFO - root - 2017-12-16 09:37:15.941709: step 74680, loss = 0.32, batch loss = 0.26 (29.2 examples/sec; 0.274 sec/batch; 19h:36m:01s remains)
INFO - root - 2017-12-16 09:37:18.808146: step 74690, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.281 sec/batch; 20h:06m:33s remains)
INFO - root - 2017-12-16 09:37:21.685337: step 74700, loss = 0.34, batch loss = 0.28 (28.5 examples/sec; 0.280 sec/batch; 20h:04m:23s remains)
2017-12-16 09:37:22.204132: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1279964 -3.7008297 -4.3499451 -5.5153346 -6.3565235 -6.3666182 -6.0644279 -6.1525621 -5.9912767 -5.6679463 -5.3859758 -4.9964089 -4.5051103 -3.9152136 -3.4979081][-3.6684165 -4.773025 -5.7367 -6.1134405 -6.2456641 -6.4912128 -6.2885041 -5.783061 -5.2895403 -5.2154384 -5.138597 -4.9216232 -4.7496781 -4.3448191 -3.883517][-4.2072196 -5.2650509 -6.0962815 -6.5011854 -6.3285174 -5.5318751 -4.5688968 -4.1349792 -3.8622715 -3.7420263 -4.097899 -4.5473175 -4.7777305 -4.6225514 -4.3770151][-4.693428 -5.7912707 -6.3985963 -6.0687485 -5.0822144 -3.9062643 -2.6435003 -1.6638348 -1.1893959 -1.5421927 -2.2853279 -3.1014752 -4.0939074 -4.6418366 -4.5965643][-5.1074839 -5.7760735 -5.7018881 -4.9927568 -3.6729553 -1.7555377 0.29247236 1.4150381 1.6024818 0.98346424 -0.17635489 -1.6837366 -3.0790195 -3.9580131 -4.4731688][-5.4872952 -5.79122 -5.2610645 -3.9746192 -1.87288 0.4620471 2.6823721 4.19954 4.7355671 3.8743391 2.0747747 -0.080909252 -2.1853371 -3.7250717 -4.4662395][-5.0755897 -5.5990639 -5.0848446 -3.5491946 -1.3111143 1.4419999 4.2077017 6.0267324 6.4813595 5.788908 4.0920429 1.4734898 -1.2395978 -3.2364831 -4.3043065][-4.7506609 -5.2215166 -4.9610372 -3.8353903 -1.6455085 0.96911573 3.6012154 5.6671495 6.5199614 5.7584763 3.9834261 1.836072 -0.52017069 -2.7500348 -4.1665463][-4.72712 -5.5976276 -5.8417444 -4.9879489 -3.53619 -1.2367859 1.4271436 3.2971573 4.2755785 3.9274902 2.4876785 0.42549086 -1.7387376 -3.3388553 -4.3642044][-5.5329008 -6.4416618 -7.0595455 -6.9140477 -5.7861981 -4.3505292 -2.7189415 -0.7642169 0.53009796 0.25605154 -0.685359 -1.8778567 -3.3134058 -4.4831104 -5.1417084][-5.5455613 -6.7426476 -7.8418303 -8.209734 -7.8997269 -6.7776489 -5.2384596 -4.2202663 -3.5937786 -3.2555575 -3.55936 -4.261498 -4.8271675 -5.2039094 -5.4035673][-5.6298733 -6.6964836 -7.675005 -8.30259 -8.3768578 -8.050107 -7.408102 -6.3272395 -5.5563731 -5.3821788 -5.300766 -5.2817621 -5.4528761 -5.50625 -5.4208369][-4.5938907 -5.5440931 -6.6690054 -7.3939843 -7.8232713 -7.8033772 -7.50021 -7.2379265 -6.81364 -6.2687554 -6.1864638 -5.9385638 -5.55287 -5.4725676 -5.3967505][-3.7188463 -4.0016141 -4.6781917 -5.600399 -6.2192111 -6.4097366 -6.5351238 -6.5205345 -6.4068804 -6.0285807 -5.4735122 -5.215693 -5.1753235 -4.9722762 -4.7404728][-2.601203 -2.4057889 -2.579273 -3.2913742 -4.0901756 -4.6302257 -4.8503866 -4.7155995 -4.5103297 -4.4061565 -4.3308563 -3.9916935 -3.5683892 -3.6476102 -3.7409012]]...]
INFO - root - 2017-12-16 09:37:25.086251: step 74710, loss = 0.21, batch loss = 0.15 (27.9 examples/sec; 0.287 sec/batch; 20h:31m:49s remains)
INFO - root - 2017-12-16 09:37:27.903135: step 74720, loss = 0.24, batch loss = 0.18 (28.1 examples/sec; 0.285 sec/batch; 20h:24m:20s remains)
INFO - root - 2017-12-16 09:37:30.719259: step 74730, loss = 0.21, batch loss = 0.15 (29.6 examples/sec; 0.270 sec/batch; 19h:21m:31s remains)
INFO - root - 2017-12-16 09:37:33.600488: step 74740, loss = 0.23, batch loss = 0.17 (27.6 examples/sec; 0.290 sec/batch; 20h:47m:23s remains)
INFO - root - 2017-12-16 09:37:36.418939: step 74750, loss = 0.34, batch loss = 0.29 (28.3 examples/sec; 0.282 sec/batch; 20h:13m:12s remains)
INFO - root - 2017-12-16 09:37:39.306433: step 74760, loss = 0.25, batch loss = 0.20 (28.2 examples/sec; 0.283 sec/batch; 20h:16m:29s remains)
INFO - root - 2017-12-16 09:37:42.109740: step 74770, loss = 0.22, batch loss = 0.16 (28.2 examples/sec; 0.284 sec/batch; 20h:20m:07s remains)
INFO - root - 2017-12-16 09:37:44.974838: step 74780, loss = 0.22, batch loss = 0.16 (27.1 examples/sec; 0.295 sec/batch; 21h:08m:47s remains)
INFO - root - 2017-12-16 09:37:47.789936: step 74790, loss = 0.32, batch loss = 0.26 (28.0 examples/sec; 0.285 sec/batch; 20h:25m:16s remains)
INFO - root - 2017-12-16 09:37:50.617220: step 74800, loss = 0.29, batch loss = 0.23 (28.2 examples/sec; 0.284 sec/batch; 20h:18m:50s remains)
2017-12-16 09:37:51.126136: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2620521 -3.4888496 -3.7644722 -4.2193341 -5.0101285 -5.6624289 -6.171381 -6.6994724 -6.8353872 -6.4480715 -5.4639521 -4.5527554 -3.5210342 -2.8182483 -2.5580804][-2.8635151 -3.1977358 -3.7486393 -4.3740058 -4.9988842 -5.9674244 -6.7821627 -7.1545887 -7.0405073 -6.6376314 -5.7634497 -4.5312843 -3.401947 -2.645715 -2.2164178][-2.5392015 -2.9392648 -3.5422361 -4.2737141 -4.9683218 -5.5978079 -5.9606714 -6.334125 -6.3811975 -5.9799142 -5.2200928 -4.2793183 -3.5486779 -2.6427069 -2.1020973][-1.6682625 -2.1852841 -2.8969584 -3.5650344 -3.9480331 -4.0363245 -4.0461192 -4.1100097 -4.0002728 -4.1057568 -3.9234085 -3.6183422 -3.188252 -2.5541797 -2.0072298][-0.92196941 -1.208328 -1.7385578 -1.8907318 -1.7639472 -1.4197283 -0.81190062 -0.71793962 -1.0004678 -1.7680206 -2.46384 -2.8979702 -2.8680449 -2.4565949 -2.0753343][-1.0216324 -1.1597824 -1.1729002 -0.73310614 0.38340139 1.6727605 2.6455722 2.7675991 2.0666447 0.62124586 -0.82411504 -2.0832818 -2.7255657 -2.627285 -2.1504679][-1.3956134 -1.351754 -1.0149579 -0.062519073 1.5365214 3.4330044 5.154788 5.4194126 4.3793383 2.428865 0.35919 -1.3977726 -2.3293478 -2.5636187 -2.2691796][-1.6593902 -1.4606438 -0.92879105 0.31548119 2.2283816 4.0827579 5.3188992 5.5955706 4.6686926 2.6783757 0.47760677 -1.2947619 -2.3027802 -2.6607606 -2.6596236][-2.0407736 -1.9058089 -1.3822165 -0.18895102 1.2331438 2.8748507 4.23431 4.2405853 3.2569246 1.4983907 -0.37591028 -1.945987 -2.9718266 -3.2409887 -3.2363636][-2.5717242 -2.4031403 -2.3272822 -1.6107311 -0.46746159 0.69548512 1.4375439 1.5407453 1.1506844 -0.37860727 -1.7324729 -2.88208 -3.7169611 -3.9760871 -3.9871447][-2.6530423 -2.6597698 -2.7272918 -2.7012 -2.4115312 -1.8022411 -1.197017 -1.116714 -1.6174438 -2.2286365 -2.9861224 -3.9078939 -4.2515044 -4.284441 -4.2274156][-2.2876506 -2.32899 -2.7218218 -3.0266747 -3.326231 -3.4058702 -3.4118257 -3.3362007 -3.3488302 -3.6183538 -4.0219927 -4.1828485 -4.2190065 -4.2521038 -4.0819869][-1.7866213 -1.8301966 -2.3451376 -2.9135571 -3.636276 -4.010509 -4.1822076 -4.3686328 -4.4095783 -4.4118395 -4.2780352 -4.2448907 -4.0059538 -3.7073205 -3.4499989][-1.3227749 -1.4246638 -1.8122785 -2.6419785 -3.4492564 -3.9729829 -4.258606 -4.5243921 -4.5593023 -4.3839664 -4.1506472 -3.8183334 -3.3984213 -3.0625453 -2.6915307][-1.0029943 -0.88418293 -1.2614334 -2.0487683 -2.9153483 -3.5757644 -4.0036635 -4.2076464 -4.2578344 -4.1190734 -3.788909 -3.3227987 -2.7227945 -2.2617333 -2.0382924]]...]
INFO - root - 2017-12-16 09:37:53.964968: step 74810, loss = 0.20, batch loss = 0.14 (27.7 examples/sec; 0.289 sec/batch; 20h:41m:34s remains)
INFO - root - 2017-12-16 09:37:56.805531: step 74820, loss = 0.22, batch loss = 0.16 (29.6 examples/sec; 0.270 sec/batch; 19h:21m:08s remains)
INFO - root - 2017-12-16 09:37:59.632278: step 74830, loss = 0.24, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 20h:15m:05s remains)
INFO - root - 2017-12-16 09:38:02.518809: step 74840, loss = 0.28, batch loss = 0.22 (26.5 examples/sec; 0.301 sec/batch; 21h:34m:31s remains)
INFO - root - 2017-12-16 09:38:05.349275: step 74850, loss = 0.32, batch loss = 0.26 (27.8 examples/sec; 0.287 sec/batch; 20h:34m:12s remains)
INFO - root - 2017-12-16 09:38:08.212883: step 74860, loss = 0.26, batch loss = 0.20 (27.1 examples/sec; 0.295 sec/batch; 21h:06m:59s remains)
INFO - root - 2017-12-16 09:38:11.036840: step 74870, loss = 0.26, batch loss = 0.20 (29.2 examples/sec; 0.274 sec/batch; 19h:34m:29s remains)
INFO - root - 2017-12-16 09:38:13.892078: step 74880, loss = 0.22, batch loss = 0.16 (26.8 examples/sec; 0.299 sec/batch; 21h:22m:12s remains)
INFO - root - 2017-12-16 09:38:16.706776: step 74890, loss = 0.25, batch loss = 0.19 (29.2 examples/sec; 0.274 sec/batch; 19h:34m:21s remains)
INFO - root - 2017-12-16 09:38:19.539274: step 74900, loss = 0.28, batch loss = 0.22 (28.0 examples/sec; 0.286 sec/batch; 20h:28m:22s remains)
2017-12-16 09:38:20.054315: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4922869 -3.1889338 -2.7846255 -2.7128315 -3.047 -3.7664316 -4.6802831 -5.5485888 -6.2693472 -6.397891 -6.3087611 -6.1351886 -5.9021544 -5.7399316 -5.3027344][-3.5711679 -3.1087296 -2.6199365 -2.3134224 -2.4010997 -3.2712717 -4.5038357 -5.5026627 -6.3499165 -6.7895241 -6.76687 -6.3697643 -6.0343828 -5.8874121 -5.3563166][-3.748981 -3.0481391 -2.3775856 -2.0308192 -2.1966062 -2.8356943 -3.9594808 -5.0538855 -5.9771442 -6.5912714 -6.7050419 -6.5786357 -6.2635546 -5.9110928 -5.2483726][-3.7170081 -2.8959212 -2.0263195 -1.6520827 -1.7442298 -2.1123114 -3.0111144 -3.9188557 -4.8094244 -5.68227 -6.2235723 -6.3410726 -6.0938807 -5.7977848 -5.0538445][-3.2143216 -2.19348 -1.054749 -0.27699709 -0.0086903572 -0.0430274 -0.66602731 -1.416522 -2.7182522 -4.0408754 -5.2788358 -5.8764005 -5.8157792 -5.381165 -4.4749341][-2.4448943 -1.3459756 -0.015511036 1.0371432 1.8479414 2.4810138 2.4028358 1.5210948 -0.12509537 -2.0313225 -3.8603735 -5.0889997 -5.4091744 -4.997735 -4.263422][-2.0418196 -0.914237 0.47855663 1.584794 2.6387844 3.790863 4.16422 3.6863432 2.1964698 -0.11366987 -2.7019935 -4.4328132 -4.8602581 -4.305212 -3.6182413][-1.6829367 -0.68003297 0.26376057 1.3936687 2.7536469 4.2361307 5.0624475 4.6639748 3.0240417 0.68926477 -1.7679641 -3.7206788 -4.2922368 -3.956542 -3.4011741][-1.9450762 -0.95277119 0.13939857 0.96938181 2.0131984 3.4300303 4.6023436 4.4007149 3.0247335 0.690011 -1.8628743 -3.8307297 -4.4974203 -4.2466264 -3.6043527][-2.6666284 -1.8746791 -1.2656603 -0.52097225 0.40089607 1.4473076 2.1286073 2.2258544 1.3421764 -0.703264 -2.8523083 -4.4967861 -5.0315571 -4.766109 -4.0685492][-3.5863447 -3.1484954 -2.6046956 -1.9835236 -1.4911144 -0.88244939 -0.22335386 -0.19890642 -1.146101 -2.7709248 -4.4969325 -5.8072248 -6.2242584 -5.8166718 -4.9640121][-4.089807 -3.6739147 -3.1891298 -2.5808778 -2.0826046 -1.819427 -1.8379035 -2.0706882 -2.7553973 -4.0592232 -5.4620438 -6.5873837 -7.0855131 -6.9018993 -6.0643735][-4.0687461 -3.7436004 -3.2752848 -2.7391262 -2.5734866 -2.4560285 -2.5046504 -2.8915391 -3.4569857 -4.2588272 -5.1127629 -5.8899593 -6.2126727 -6.1122541 -5.5692863][-4.1820512 -3.82179 -3.37261 -2.8643417 -2.6570129 -2.4407697 -2.4798737 -2.8393879 -3.3885508 -3.9735382 -4.3199143 -4.7695603 -5.1191635 -5.1213393 -4.7654309][-4.0624475 -3.5599084 -3.0854936 -2.5943313 -2.1950009 -1.877955 -1.7488704 -1.9424455 -2.2885251 -2.8376305 -3.3720388 -3.7943661 -3.9709713 -4.1198645 -4.0163]]...]
INFO - root - 2017-12-16 09:38:22.879370: step 74910, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.282 sec/batch; 20h:09m:44s remains)
INFO - root - 2017-12-16 09:38:25.681880: step 74920, loss = 0.38, batch loss = 0.32 (28.7 examples/sec; 0.279 sec/batch; 19h:58m:09s remains)
INFO - root - 2017-12-16 09:38:28.571136: step 74930, loss = 0.26, batch loss = 0.21 (27.3 examples/sec; 0.293 sec/batch; 20h:57m:29s remains)
INFO - root - 2017-12-16 09:38:31.374177: step 74940, loss = 0.33, batch loss = 0.27 (28.7 examples/sec; 0.279 sec/batch; 19h:57m:47s remains)
INFO - root - 2017-12-16 09:38:34.221425: step 74950, loss = 0.30, batch loss = 0.24 (27.9 examples/sec; 0.287 sec/batch; 20h:31m:23s remains)
INFO - root - 2017-12-16 09:38:37.041890: step 74960, loss = 0.23, batch loss = 0.17 (26.7 examples/sec; 0.299 sec/batch; 21h:24m:30s remains)
INFO - root - 2017-12-16 09:38:39.887199: step 74970, loss = 0.32, batch loss = 0.26 (28.7 examples/sec; 0.278 sec/batch; 19h:54m:58s remains)
INFO - root - 2017-12-16 09:38:42.694070: step 74980, loss = 0.26, batch loss = 0.20 (29.8 examples/sec; 0.268 sec/batch; 19h:10m:37s remains)
INFO - root - 2017-12-16 09:38:45.557534: step 74990, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.277 sec/batch; 19h:47m:18s remains)
INFO - root - 2017-12-16 09:38:48.389564: step 75000, loss = 0.35, batch loss = 0.29 (28.0 examples/sec; 0.286 sec/batch; 20h:25m:36s remains)
2017-12-16 09:38:48.949307: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4760447 -6.0435815 -6.6092196 -7.0163426 -7.20917 -7.2519855 -7.2427568 -7.2916832 -7.4116912 -7.5237556 -7.2709727 -6.9879885 -6.5041327 -5.7978983 -5.0232534][-6.7503977 -7.2183733 -7.4783459 -7.5687284 -7.6588821 -7.95599 -8.2911406 -8.5592632 -8.7693491 -9.0579634 -9.0419521 -8.7822189 -8.0607586 -7.2356935 -6.355175][-7.19816 -7.43205 -7.5628424 -7.2304144 -6.9699578 -7.0312691 -7.3106527 -8.2562227 -9.1456861 -9.7552471 -9.9485064 -9.8399076 -9.4494371 -8.7759571 -7.9656763][-7.2367964 -7.1175218 -6.6018314 -5.9670973 -5.3015895 -4.9373808 -4.9400992 -5.8006835 -7.0137129 -8.7028122 -9.6869059 -9.7689934 -9.5920372 -9.4232807 -9.1142521][-6.5844364 -5.6946044 -4.2329526 -2.778481 -1.4598005 -0.84216285 -1.0569162 -1.8474071 -3.0127008 -4.9512758 -6.8107834 -8.4459419 -9.3493481 -9.6286087 -9.7393532][-5.3943911 -4.0529985 -2.145309 0.27070284 2.4599833 3.736311 4.0088053 2.9331384 1.2382169 -0.94471788 -3.1686382 -5.4650555 -7.571846 -9.0550785 -9.6380816][-5.1417542 -3.762289 -1.3354075 1.5940557 4.3860092 6.0634947 6.7330112 5.9660044 4.3931093 2.167028 -0.35046434 -3.1790321 -5.7569261 -7.4953752 -8.3915215][-5.7349024 -4.2027388 -2.0305598 0.83446455 4.19909 6.593914 7.5711985 6.9276676 5.4848614 3.1619139 0.62610722 -2.0745614 -4.5419869 -6.3849769 -7.21727][-6.6928339 -5.4457378 -3.5063543 -0.64028 2.1923895 4.4832926 6.2609348 6.3226271 5.3150816 3.1198955 0.63426065 -2.1309075 -4.6307821 -6.0840282 -6.2785234][-7.6550455 -6.923707 -5.6151056 -3.6288218 -1.3676608 0.94443464 2.6664481 2.903666 2.5409532 0.87059355 -1.0673175 -3.3703685 -5.328618 -6.2980852 -6.5074263][-8.2812414 -8.0485611 -7.0824509 -5.8064771 -4.6216969 -3.1404412 -1.3282132 -0.43352747 -0.58145905 -2.0731468 -3.3719587 -4.7994266 -6.1281524 -6.86913 -7.0642085][-8.3726854 -8.1692047 -7.8749375 -6.9938068 -5.8751497 -5.0216713 -4.00582 -3.5363688 -3.2050605 -3.7645864 -4.9151735 -6.4390845 -7.3476143 -7.2942319 -7.0993714][-7.2355576 -7.120389 -7.2059731 -6.719173 -6.171741 -5.338037 -4.4856329 -4.5411205 -4.4656396 -4.8404994 -5.36776 -6.2752032 -6.9791994 -7.092761 -6.986124][-5.7024508 -5.6741619 -5.6591425 -5.5251508 -5.3943729 -4.7821927 -4.1894736 -4.3890419 -4.6252389 -4.785 -4.8307724 -5.3471208 -5.9256124 -5.921381 -5.6553931][-4.3486152 -4.0975223 -3.8996537 -3.8540466 -3.8471873 -3.694088 -3.598038 -3.700824 -3.7628784 -3.97019 -4.0611362 -4.0590525 -4.1227713 -4.2893686 -4.4092083]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-75000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-75000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 09:38:52.527052: step 75010, loss = 0.24, batch loss = 0.18 (27.8 examples/sec; 0.288 sec/batch; 20h:34m:33s remains)
INFO - root - 2017-12-16 09:38:55.377952: step 75020, loss = 0.34, batch loss = 0.28 (26.8 examples/sec; 0.298 sec/batch; 21h:20m:29s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 09:38:58.219826: step 75030, loss = 0.27, batch loss = 0.21 (29.1 examples/sec; 0.275 sec/batch; 19h:39m:25s remains)
INFO - root - 2017-12-16 09:39:01.056168: step 75040, loss = 0.34, batch loss = 0.28 (29.2 examples/sec; 0.274 sec/batch; 19h:35m:07s remains)
INFO - root - 2017-12-16 09:39:03.925601: step 75050, loss = 0.23, batch loss = 0.17 (27.8 examples/sec; 0.288 sec/batch; 20h:36m:35s remains)
INFO - root - 2017-12-16 09:39:06.735120: step 75060, loss = 0.30, batch loss = 0.24 (28.6 examples/sec; 0.280 sec/batch; 20h:00m:54s remains)
INFO - root - 2017-12-16 09:39:09.616031: step 75070, loss = 0.38, batch loss = 0.32 (27.9 examples/sec; 0.287 sec/batch; 20h:30m:44s remains)
INFO - root - 2017-12-16 09:39:12.490598: step 75080, loss = 0.20, batch loss = 0.14 (26.1 examples/sec; 0.306 sec/batch; 21h:54m:08s remains)
INFO - root - 2017-12-16 09:39:15.381428: step 75090, loss = 0.22, batch loss = 0.16 (28.8 examples/sec; 0.278 sec/batch; 19h:52m:25s remains)
INFO - root - 2017-12-16 09:39:18.307248: step 75100, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.284 sec/batch; 20h:19m:25s remains)
2017-12-16 09:39:18.804248: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.866713 -5.0752649 -3.9258211 -2.889755 -2.0171061 -1.7537379 -1.6531906 -1.9677336 -2.8545585 -3.687274 -4.2525821 -4.5803332 -4.5578332 -4.1073122 -3.4478986][-5.16205 -4.3971028 -3.3874228 -2.3973024 -1.59128 -1.2154255 -1.1625702 -1.5787284 -2.3631594 -3.2576628 -3.9851425 -4.3033133 -4.3210664 -4.0453758 -3.4808252][-4.28492 -3.5784137 -2.7288191 -1.8378327 -1.1859777 -0.90317941 -0.85173225 -1.2454724 -2.1025822 -3.2171302 -4.0827231 -4.6844091 -4.7393594 -4.4059 -3.8042245][-3.0798182 -2.5840204 -1.8432541 -1.2551315 -0.74033475 -0.47969413 -0.43520021 -0.83282638 -1.8237829 -2.9917698 -4.0366492 -4.6679258 -4.8514948 -4.4396029 -3.7766585][-1.9699364 -1.6199253 -1.0249684 -0.63054776 0.029671669 0.62332678 0.8885479 0.64875984 -0.43205571 -1.8623054 -3.2771196 -4.2838697 -4.6651859 -4.516768 -3.8430543][-1.5786192 -1.4040456 -0.96642494 -0.50256014 0.44568348 1.4550982 2.1637707 2.1087494 1.1871138 -0.5227077 -2.3313296 -3.7958441 -4.6758161 -4.8529544 -4.34655][-1.5549772 -1.5615199 -1.1970584 -0.62821221 0.498044 1.8205819 2.777946 2.8623943 1.8894324 0.073759556 -2.0475218 -3.9541295 -5.0803823 -5.42961 -5.0652094][-1.8879495 -1.8205881 -1.4880555 -0.79686046 0.58767796 1.9096699 2.7982316 3.0457363 2.180757 0.23462296 -2.1028781 -4.0383244 -5.332119 -5.8267317 -5.4201641][-2.2413108 -2.2439051 -1.8910661 -1.3412235 0.013164043 1.42452 2.4949503 2.7757688 1.9034228 0.16795349 -2.0805619 -4.1941967 -5.6833925 -6.2391872 -5.851943][-2.4074633 -2.5308967 -2.2875073 -1.8080714 -0.82944059 0.36525297 1.4055109 1.815424 1.1533747 -0.46293092 -2.6122029 -4.5783463 -6.0149994 -6.6526709 -6.2884493][-2.1342142 -2.524972 -2.7738271 -2.5546489 -1.7031939 -0.55974865 0.33614063 0.71272993 0.21252728 -1.2972677 -3.2117379 -5.0612249 -6.3271055 -6.7983675 -6.4480333][-2.2448165 -2.7582932 -3.0299251 -2.9431896 -2.2833211 -1.2948461 -0.52996111 -0.23335314 -0.77005291 -2.0689554 -3.7983658 -5.466176 -6.4976377 -6.6907744 -6.1566935][-2.2325411 -2.9265983 -3.2725425 -3.1707118 -2.4613228 -1.6052771 -1.0512655 -0.75103188 -1.3706365 -2.5826554 -4.110744 -5.4887686 -6.169847 -6.2528882 -5.7481928][-2.2092116 -2.8994184 -3.3408961 -3.2036085 -2.6161156 -1.7787964 -1.2058008 -1.1987214 -1.7981117 -2.8769526 -4.2439981 -5.3178391 -5.8573861 -5.7979784 -5.1642022][-2.7478871 -3.3460259 -3.5648713 -3.2862005 -2.6196465 -1.7703199 -1.2808063 -1.3178582 -1.8783875 -3.0203419 -4.22091 -5.166142 -5.574934 -5.324913 -4.5965185]]...]
INFO - root - 2017-12-16 09:39:21.642615: step 75110, loss = 0.21, batch loss = 0.16 (26.4 examples/sec; 0.303 sec/batch; 21h:40m:10s remains)
INFO - root - 2017-12-16 09:39:24.498970: step 75120, loss = 0.26, batch loss = 0.20 (27.8 examples/sec; 0.288 sec/batch; 20h:35m:02s remains)
INFO - root - 2017-12-16 09:39:27.379059: step 75130, loss = 0.38, batch loss = 0.32 (27.4 examples/sec; 0.292 sec/batch; 20h:53m:36s remains)
INFO - root - 2017-12-16 09:39:30.197150: step 75140, loss = 0.29, batch loss = 0.24 (28.2 examples/sec; 0.284 sec/batch; 20h:18m:25s remains)
INFO - root - 2017-12-16 09:39:33.049774: step 75150, loss = 0.31, batch loss = 0.25 (27.8 examples/sec; 0.287 sec/batch; 20h:32m:18s remains)
INFO - root - 2017-12-16 09:39:35.902088: step 75160, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.277 sec/batch; 19h:46m:52s remains)
INFO - root - 2017-12-16 09:39:38.732122: step 75170, loss = 0.36, batch loss = 0.31 (28.2 examples/sec; 0.284 sec/batch; 20h:17m:46s remains)
INFO - root - 2017-12-16 09:39:41.564555: step 75180, loss = 0.21, batch loss = 0.15 (27.6 examples/sec; 0.289 sec/batch; 20h:41m:05s remains)
INFO - root - 2017-12-16 09:39:44.382943: step 75190, loss = 0.36, batch loss = 0.30 (28.1 examples/sec; 0.284 sec/batch; 20h:19m:14s remains)
INFO - root - 2017-12-16 09:39:47.240968: step 75200, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 20h:16m:29s remains)
2017-12-16 09:39:47.798687: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.9375174 -0.96965528 -1.3805375 -1.831172 -2.1251228 -2.3674715 -2.639611 -2.6080976 -2.7211204 -2.7109513 -2.7715657 -2.85155 -2.8921165 -3.002214 -3.0012133][-0.20775557 -0.21772337 -0.53725147 -0.93973184 -1.2850201 -1.5080504 -1.6178117 -1.6544299 -1.8742547 -2.0714185 -2.2201741 -2.3353708 -2.5424623 -2.7187891 -2.8199565][0.025563717 0.059585571 -0.10881901 -0.28240108 -0.41296005 -0.80631566 -1.1156027 -1.2155437 -1.4935384 -1.7831497 -2.1998713 -2.3828058 -2.4614332 -2.6995504 -2.8751583][0.47112274 0.52509642 0.55289364 0.51277781 0.54949 0.16138744 -0.21926689 -0.55932593 -1.1283848 -1.6168983 -2.1492968 -2.39203 -2.6142421 -2.7756655 -2.7309842][0.46370506 0.72609568 0.94906425 1.2257314 1.4258628 0.95160913 0.55012321 -0.034232616 -0.80371213 -1.5588825 -2.3605273 -2.7291036 -2.967947 -3.0342624 -2.9219232][-0.20271301 0.26737213 0.75187969 1.3650613 1.9141521 1.7067003 1.3300061 0.7081461 -0.14088869 -1.125006 -2.0347164 -2.5817242 -3.0433683 -3.2498217 -3.1851401][-1.0550799 -0.37965918 0.34009409 1.1807871 2.0262637 2.1518626 1.9756861 1.2839746 0.27864981 -0.88506651 -2.0615804 -2.7777374 -3.1560831 -3.2540793 -3.178925][-1.6497777 -1.0671148 -0.44986367 0.5477314 1.6407957 2.0347042 1.983139 1.4074225 0.54249716 -0.59998393 -1.9135442 -2.8346453 -3.2355933 -3.3309293 -3.2659597][-2.6514785 -1.997613 -1.3146098 -0.4343729 0.53668642 1.2246981 1.577445 1.1886253 0.43670654 -0.5431726 -1.6434121 -2.5764177 -3.2010908 -3.3810616 -3.3258352][-3.5932732 -3.058799 -2.3947499 -1.6106601 -0.65378428 0.09905529 0.58136129 0.37490797 -0.18379831 -0.91404057 -1.8096826 -2.5527244 -3.0090542 -3.1384802 -3.1468258][-3.9949341 -3.694675 -3.2847044 -2.6613822 -1.7583079 -0.98544359 -0.48857617 -0.49722242 -0.79757 -1.2818916 -2.0820403 -2.7606597 -3.0435433 -3.0101514 -2.8539906][-4.0949469 -3.9979973 -3.7780416 -3.3114002 -2.6861954 -1.949578 -1.366807 -1.2994709 -1.4760189 -1.7127662 -2.1728439 -2.6612957 -2.8792429 -2.8228204 -2.6897426][-3.7850113 -3.6826696 -3.4231186 -3.1142564 -2.6537185 -2.2175653 -1.9099882 -1.7778955 -1.739707 -1.7610486 -2.0297568 -2.2208116 -2.198626 -2.0864069 -1.8864505][-3.3082356 -3.136467 -2.8034124 -2.4713712 -2.0098381 -1.7421374 -1.6233621 -1.6274416 -1.6720822 -1.5706267 -1.529676 -1.4492977 -1.3129935 -1.0853405 -0.88393569][-2.9063962 -2.5577607 -2.0863469 -1.6973796 -1.3523591 -1.2305152 -1.1873815 -1.0764749 -1.0072408 -0.86387038 -0.73933554 -0.48361588 -0.35761786 -0.25349092 -0.045711994]]...]
INFO - root - 2017-12-16 09:39:50.591624: step 75210, loss = 0.21, batch loss = 0.16 (27.0 examples/sec; 0.296 sec/batch; 21h:08m:15s remains)
INFO - root - 2017-12-16 09:39:53.440672: step 75220, loss = 0.25, batch loss = 0.19 (27.8 examples/sec; 0.288 sec/batch; 20h:33m:32s remains)
INFO - root - 2017-12-16 09:39:56.302758: step 75230, loss = 0.26, batch loss = 0.21 (26.9 examples/sec; 0.297 sec/batch; 21h:14m:25s remains)
INFO - root - 2017-12-16 09:39:59.128755: step 75240, loss = 0.23, batch loss = 0.17 (28.0 examples/sec; 0.285 sec/batch; 20h:23m:15s remains)
INFO - root - 2017-12-16 09:40:01.971163: step 75250, loss = 0.28, batch loss = 0.22 (28.0 examples/sec; 0.286 sec/batch; 20h:27m:08s remains)
INFO - root - 2017-12-16 09:40:04.801665: step 75260, loss = 0.26, batch loss = 0.20 (27.5 examples/sec; 0.291 sec/batch; 20h:47m:36s remains)
INFO - root - 2017-12-16 09:40:07.678034: step 75270, loss = 0.29, batch loss = 0.23 (27.0 examples/sec; 0.296 sec/batch; 21h:10m:50s remains)
INFO - root - 2017-12-16 09:40:10.583199: step 75280, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.285 sec/batch; 20h:23m:35s remains)
INFO - root - 2017-12-16 09:40:13.425632: step 75290, loss = 0.32, batch loss = 0.26 (28.6 examples/sec; 0.280 sec/batch; 20h:00m:12s remains)
INFO - root - 2017-12-16 09:40:16.224223: step 75300, loss = 0.33, batch loss = 0.27 (26.6 examples/sec; 0.301 sec/batch; 21h:28m:28s remains)
2017-12-16 09:40:16.818375: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3676844 -2.5267327 -2.777621 -3.1106069 -3.5843523 -3.9870107 -4.3074059 -4.7782636 -5.2476654 -5.4760985 -5.2929578 -4.9394031 -4.455277 -3.8298554 -3.2693014][-2.9617927 -3.262444 -3.4956229 -3.5997093 -3.7908657 -4.2022424 -4.5138712 -4.7924814 -4.9352293 -5.2749696 -5.6644769 -5.5611196 -5.0282764 -4.2559438 -3.5809264][-3.8733046 -4.2468171 -4.3841267 -4.2905869 -4.24082 -4.0990143 -3.8360569 -4.1228971 -4.4402871 -4.66091 -4.8183618 -5.1252713 -5.2910442 -4.6258273 -3.7054858][-4.3510532 -4.8541541 -4.932373 -4.5538087 -4.0053358 -3.2960191 -2.7226038 -2.3514519 -2.367758 -3.0704918 -3.883414 -4.3676667 -4.5649762 -4.3932762 -3.8746][-4.6808538 -4.8898139 -4.5706129 -3.8144951 -2.74933 -1.3278811 -0.08865881 0.36421061 0.11833668 -0.57651782 -1.7180798 -3.1386585 -4.0772047 -4.0788274 -3.7425079][-4.627202 -4.7286615 -4.0987587 -2.9427958 -1.3042524 0.58758926 2.1739607 2.9295068 2.504714 1.1247149 -0.53843379 -2.1908426 -3.4762259 -3.9163587 -3.6891263][-3.7985344 -3.651999 -2.8875942 -1.5694134 0.18105459 2.0043974 3.6254892 4.2819624 3.7719774 2.2928085 0.14823818 -1.949244 -3.3221765 -3.8720198 -3.7803111][-3.1226492 -2.5842965 -1.6857727 -0.33628654 1.4514365 3.2424669 4.4148006 4.4504328 3.576767 1.9956198 -0.0437994 -2.2029335 -3.7393594 -4.3227763 -4.2221794][-2.8728633 -2.341677 -1.1212204 0.23920012 1.4476585 2.6550274 3.4152727 3.0979609 1.9334307 0.32323122 -1.4969163 -3.297811 -4.7428517 -5.2575293 -4.9733424][-2.8705518 -2.3884077 -1.4411564 -0.11408091 0.94719267 1.3397632 1.3060536 0.826128 -0.20889282 -1.8142564 -3.3744435 -4.56262 -5.3562431 -5.7252436 -5.49883][-3.5695429 -2.8902655 -2.0275695 -1.0070338 -0.07800293 0.011528015 -0.58545327 -1.4103251 -2.3835013 -3.432287 -4.5253887 -5.3963423 -5.8333616 -5.6653056 -5.1839757][-4.1757879 -3.4902203 -2.6175175 -1.8577282 -1.3322568 -1.3611922 -1.9252782 -2.9680874 -4.1425123 -5.0167289 -5.6826625 -5.8998556 -5.7982769 -5.3286881 -4.7133789][-4.9294162 -4.0327435 -3.0808148 -2.136395 -1.7746828 -2.1024134 -2.79066 -3.6327441 -4.7581887 -5.7518916 -6.1431446 -6.067955 -5.6895485 -4.8563385 -4.0586591][-5.1734223 -4.2513 -3.2076106 -2.1768312 -1.5274501 -1.6166224 -2.3836088 -3.408987 -4.4210982 -5.3623872 -5.99672 -5.758503 -5.0176458 -4.2242217 -3.532053][-4.6218481 -3.7578003 -2.7073891 -1.6046081 -0.8290925 -0.68352413 -1.2496009 -2.415283 -3.7714508 -4.7151284 -5.1985989 -5.2422 -4.7056441 -3.6932766 -2.860539]]...]
INFO - root - 2017-12-16 09:40:19.621270: step 75310, loss = 0.27, batch loss = 0.21 (28.5 examples/sec; 0.281 sec/batch; 20h:03m:54s remains)
INFO - root - 2017-12-16 09:40:22.514846: step 75320, loss = 0.25, batch loss = 0.19 (28.6 examples/sec; 0.279 sec/batch; 19h:57m:43s remains)
INFO - root - 2017-12-16 09:40:25.332142: step 75330, loss = 0.30, batch loss = 0.25 (28.1 examples/sec; 0.285 sec/batch; 20h:19m:35s remains)
INFO - root - 2017-12-16 09:40:28.140974: step 75340, loss = 0.25, batch loss = 0.19 (28.2 examples/sec; 0.284 sec/batch; 20h:17m:00s remains)
INFO - root - 2017-12-16 09:40:30.953896: step 75350, loss = 0.28, batch loss = 0.22 (27.7 examples/sec; 0.289 sec/batch; 20h:38m:41s remains)
INFO - root - 2017-12-16 09:40:33.750019: step 75360, loss = 0.33, batch loss = 0.27 (29.1 examples/sec; 0.275 sec/batch; 19h:37m:12s remains)
INFO - root - 2017-12-16 09:40:36.585667: step 75370, loss = 0.40, batch loss = 0.34 (28.0 examples/sec; 0.285 sec/batch; 20h:23m:13s remains)
INFO - root - 2017-12-16 09:40:39.428845: step 75380, loss = 0.22, batch loss = 0.17 (28.5 examples/sec; 0.281 sec/batch; 20h:03m:34s remains)
INFO - root - 2017-12-16 09:40:42.275304: step 75390, loss = 0.29, batch loss = 0.23 (29.5 examples/sec; 0.271 sec/batch; 19h:20m:59s remains)
INFO - root - 2017-12-16 09:40:45.136660: step 75400, loss = 0.25, batch loss = 0.19 (27.5 examples/sec; 0.291 sec/batch; 20h:47m:25s remains)
2017-12-16 09:40:45.647910: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0087762 -5.7401752 -6.1389322 -6.2312317 -6.100666 -5.9072151 -5.8512583 -5.8958344 -5.9865961 -6.2110257 -6.2366838 -6.2072372 -6.0491252 -5.5837278 -5.1139536][-5.9366379 -6.7174749 -6.9648428 -6.937428 -6.5852423 -6.4180164 -6.4436684 -6.6409369 -7.0807962 -7.5887117 -7.9232774 -8.0123262 -7.781455 -7.21448 -6.6727986][-6.9008455 -7.6381912 -7.6511602 -7.0882607 -6.285254 -5.8799286 -5.6296735 -6.1421313 -7.0447159 -7.8915591 -8.7281666 -9.20676 -9.3266373 -8.9318266 -8.25562][-7.1197262 -7.62746 -7.244688 -6.1494923 -4.9127064 -4.001111 -3.6298537 -4.1943817 -5.0247083 -6.528039 -8.2053165 -9.25749 -9.8729706 -9.8564968 -9.4623032][-6.6310844 -6.5153723 -5.446538 -3.8162694 -2.0059283 -0.52165627 0.11583328 -0.3463583 -1.5648243 -3.552269 -5.6579533 -7.53494 -9.1448517 -9.7756729 -9.6939087][-5.8446341 -5.2397771 -3.7151175 -1.3410518 1.3262229 3.2407608 4.4000587 4.3457975 2.9911041 0.49523926 -2.4804459 -5.1973629 -7.1720095 -8.3829575 -9.0244045][-5.1139374 -4.3276029 -2.586329 0.067723274 2.9874892 5.4651489 7.255764 7.3951349 6.2320805 3.9489546 0.78186035 -2.4794281 -5.0270281 -6.7294483 -7.6421924][-4.6467404 -3.7796404 -2.0591791 0.44802475 3.3548965 5.9821463 7.8775177 8.2451038 7.2222347 4.9182816 1.866116 -1.0089333 -3.3753562 -5.2958636 -6.6656389][-4.7490721 -4.1119885 -2.7591612 -0.5805192 2.0545731 4.3665304 6.2512178 6.7918196 6.1310911 4.2706795 1.629076 -0.90310764 -3.0416651 -4.6608195 -5.8231478][-5.3366232 -5.1401653 -4.46603 -2.9530711 -1.1171789 0.89871454 2.5958972 3.1415076 2.9573522 1.7584491 -0.0031037331 -2.1033089 -3.8007033 -5.0266724 -6.0497646][-6.3089261 -6.370337 -6.0668383 -5.2135496 -4.1839557 -2.8618453 -1.7630193 -1.1449556 -1.0458961 -1.89378 -3.0282297 -4.3319187 -5.38825 -6.1570024 -6.6114035][-6.8675165 -6.9992914 -6.9333673 -6.7778955 -6.369204 -5.631918 -4.9238367 -4.5320587 -4.459393 -4.7116227 -5.2624221 -6.2304096 -6.9552431 -7.2484765 -7.3016944][-6.3638144 -6.6444678 -6.9720707 -7.00692 -6.9328451 -6.6346436 -6.0304856 -5.6922817 -5.5087571 -5.6819191 -6.0945973 -6.6277571 -6.9908304 -7.3072767 -7.3349476][-5.3411412 -5.4822078 -5.7697291 -5.9546723 -6.312264 -6.1901455 -5.9129725 -5.8413353 -5.6011696 -5.6276603 -5.7244282 -6.1608253 -6.538456 -6.6131167 -6.4388514][-4.4053226 -4.3676853 -4.3694987 -4.5757651 -4.8768687 -4.9225221 -4.9755282 -4.9026318 -4.6960258 -4.79924 -4.901381 -5.0350852 -5.1414642 -5.2836661 -5.1707382]]...]
INFO - root - 2017-12-16 09:40:48.491729: step 75410, loss = 0.30, batch loss = 0.25 (28.9 examples/sec; 0.277 sec/batch; 19h:47m:09s remains)
INFO - root - 2017-12-16 09:40:51.314687: step 75420, loss = 0.24, batch loss = 0.19 (29.5 examples/sec; 0.272 sec/batch; 19h:23m:36s remains)
INFO - root - 2017-12-16 09:40:54.130286: step 75430, loss = 0.44, batch loss = 0.38 (28.7 examples/sec; 0.279 sec/batch; 19h:53m:22s remains)
INFO - root - 2017-12-16 09:40:56.937905: step 75440, loss = 0.34, batch loss = 0.28 (28.7 examples/sec; 0.279 sec/batch; 19h:55m:35s remains)
INFO - root - 2017-12-16 09:40:59.821247: step 75450, loss = 0.26, batch loss = 0.20 (29.2 examples/sec; 0.274 sec/batch; 19h:33m:07s remains)
INFO - root - 2017-12-16 09:41:02.659912: step 75460, loss = 0.39, batch loss = 0.33 (26.5 examples/sec; 0.302 sec/batch; 21h:34m:38s remains)
INFO - root - 2017-12-16 09:41:05.445195: step 75470, loss = 0.22, batch loss = 0.16 (27.9 examples/sec; 0.286 sec/batch; 20h:27m:07s remains)
INFO - root - 2017-12-16 09:41:08.264878: step 75480, loss = 0.23, batch loss = 0.17 (27.4 examples/sec; 0.292 sec/batch; 20h:52m:28s remains)
INFO - root - 2017-12-16 09:41:11.117221: step 75490, loss = 0.26, batch loss = 0.20 (28.3 examples/sec; 0.283 sec/batch; 20h:11m:25s remains)
INFO - root - 2017-12-16 09:41:13.980810: step 75500, loss = 0.30, batch loss = 0.24 (29.2 examples/sec; 0.274 sec/batch; 19h:33m:18s remains)
2017-12-16 09:41:14.542151: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.83183 -5.3396111 -5.5361786 -5.418117 -5.240469 -5.104291 -4.8963265 -5.0348845 -5.2393789 -5.2919545 -5.2567673 -5.098753 -5.0625248 -4.9646959 -4.6951008][-4.4761772 -4.9912233 -5.2756596 -5.0569625 -4.566628 -4.5275955 -4.6600475 -4.6591473 -4.5115228 -4.7355552 -5.0984793 -5.0716743 -4.9478221 -4.7057796 -4.319211][-3.625423 -4.018456 -4.2577386 -4.0967288 -3.7962663 -3.5853028 -3.197051 -3.2421575 -3.5902474 -3.9471934 -4.2438283 -4.497478 -4.75855 -4.6494808 -4.4565492][-2.2666833 -2.4344766 -2.5719128 -2.2775018 -1.7962399 -1.6725066 -1.5195174 -1.6092727 -1.6324587 -2.1852069 -2.9112773 -3.7393198 -4.50327 -4.6909628 -4.6253004][-0.90140247 -0.79364967 -0.81318474 -0.40837955 -0.04551363 0.13001728 0.51086378 0.57749224 0.38197231 -0.4465344 -1.3539059 -2.385504 -3.3030338 -4.0896091 -4.6081476][-0.25257635 0.00412941 0.19563484 0.77325535 1.4979305 1.9207687 2.3224063 2.4135985 2.3679767 1.6673155 0.60414028 -0.73402405 -2.1255898 -3.1455541 -3.783277][-0.15941525 0.17458296 0.46156311 1.2713189 2.2281036 2.9835172 3.8048496 4.0616951 4.0173521 3.1860147 2.054903 0.55736971 -0.98852324 -2.2016366 -3.1242261][-0.36852646 -0.20700455 0.063286781 0.94183826 2.1988859 3.1248145 3.8780622 4.1067171 4.0901604 3.333775 2.19555 0.76076174 -0.60299921 -1.8772981 -2.8942602][-1.1403654 -1.2700086 -1.0928702 -0.25977421 0.79368162 1.7028713 2.7105751 2.9461951 2.7797537 1.986506 0.89637089 -0.31253529 -1.4577267 -2.2745 -2.8156605][-2.2079685 -2.5951715 -2.7849698 -2.1529415 -1.0343759 -0.14619255 0.512877 0.69662476 0.62903786 -0.13924694 -1.145927 -2.0479655 -2.7954025 -3.3296802 -3.5656927][-2.6340675 -3.1772051 -3.5933552 -3.2386351 -2.6905546 -2.0491178 -1.3769114 -1.3650894 -1.6959529 -2.4592614 -3.2771301 -3.9103067 -4.42751 -4.5337296 -4.3626947][-2.9135709 -3.39917 -3.7963381 -3.8828387 -3.5987353 -3.3498619 -3.117188 -3.238096 -3.6248159 -4.42276 -5.0945063 -5.6509762 -5.9222727 -5.6155024 -5.1242666][-2.8774228 -3.2501578 -3.8218932 -4.0709376 -4.1430049 -4.1859493 -4.2571173 -4.4979978 -4.8697681 -5.4820313 -6.025815 -6.3474932 -6.3604531 -5.9364686 -5.4739561][-3.3345497 -3.6911147 -4.068882 -4.2531886 -4.4106927 -4.5135183 -4.5379577 -4.7974396 -5.1515517 -5.426353 -5.5168681 -5.8377419 -6.0315433 -5.7009478 -5.1387119][-3.7437091 -3.7148747 -3.9381766 -4.2103148 -4.5041809 -4.5665312 -4.6666713 -4.7895308 -4.854918 -4.9291854 -5.0702133 -5.0887618 -4.9965239 -4.8665318 -4.6945825]]...]
INFO - root - 2017-12-16 09:41:17.342734: step 75510, loss = 0.29, batch loss = 0.24 (27.5 examples/sec; 0.291 sec/batch; 20h:47m:02s remains)
INFO - root - 2017-12-16 09:41:20.153591: step 75520, loss = 0.28, batch loss = 0.22 (27.5 examples/sec; 0.290 sec/batch; 20h:44m:10s remains)
INFO - root - 2017-12-16 09:41:22.952010: step 75530, loss = 0.50, batch loss = 0.44 (29.7 examples/sec; 0.270 sec/batch; 19h:15m:16s remains)
INFO - root - 2017-12-16 09:41:25.741454: step 75540, loss = 0.43, batch loss = 0.37 (28.4 examples/sec; 0.282 sec/batch; 20h:06m:27s remains)
INFO - root - 2017-12-16 09:41:28.568502: step 75550, loss = 0.34, batch loss = 0.28 (28.4 examples/sec; 0.282 sec/batch; 20h:07m:53s remains)
INFO - root - 2017-12-16 09:41:31.422207: step 75560, loss = 0.24, batch loss = 0.18 (29.0 examples/sec; 0.276 sec/batch; 19h:39m:59s remains)
INFO - root - 2017-12-16 09:41:34.220543: step 75570, loss = 0.21, batch loss = 0.15 (29.3 examples/sec; 0.273 sec/batch; 19h:29m:25s remains)
INFO - root - 2017-12-16 09:41:37.066644: step 75580, loss = 0.21, batch loss = 0.16 (27.0 examples/sec; 0.296 sec/batch; 21h:07m:30s remains)
INFO - root - 2017-12-16 09:41:39.891011: step 75590, loss = 0.25, batch loss = 0.19 (27.9 examples/sec; 0.287 sec/batch; 20h:28m:51s remains)
INFO - root - 2017-12-16 09:41:42.686950: step 75600, loss = 0.22, batch loss = 0.17 (28.0 examples/sec; 0.286 sec/batch; 20h:24m:53s remains)
2017-12-16 09:41:43.235542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3832366 -3.8155532 -4.0747004 -3.8622618 -3.6257441 -3.4713001 -3.4517531 -3.7210479 -3.7918544 -3.8423193 -3.9862854 -4.1598816 -4.4331203 -4.646049 -4.6946306][-3.1786113 -3.6904049 -4.1190124 -4.2668972 -4.2546744 -4.2504082 -4.1769848 -4.0693355 -3.9432607 -3.9781322 -3.8987789 -3.8682847 -4.1913562 -4.2974358 -4.309629][-3.1614614 -3.9152329 -4.7260704 -5.11322 -5.1735792 -5.0335269 -4.8560061 -4.6532326 -4.2225738 -4.1201444 -4.2267179 -4.0616827 -3.9924476 -4.1368289 -4.4677777][-3.2727833 -4.1240258 -5.2653475 -5.7768259 -5.5529628 -5.1001019 -4.5795207 -4.1302748 -3.5315142 -3.2855682 -3.3246419 -3.8176432 -4.492929 -4.6199 -4.6187091][-3.7885518 -4.5790367 -5.0576692 -5.4197159 -5.1326847 -4.0494437 -2.7708359 -2.0847771 -1.6899474 -1.9311335 -2.3391058 -2.8219438 -3.5611677 -4.3993449 -5.0181818][-3.8123763 -4.8169479 -5.3716035 -4.9786267 -3.81747 -2.3708744 -0.79022765 0.28161621 0.84730673 0.51152945 -0.41764832 -1.7105777 -2.9361353 -3.5946085 -4.1201644][-4.4602671 -4.879456 -5.0240183 -4.257153 -2.6075597 -0.46903443 1.6102653 2.9536176 3.531045 2.9922833 1.6448884 0.027993202 -1.4419396 -2.5648513 -3.3409777][-4.6211576 -4.9381151 -4.8850889 -3.7421117 -1.7522292 0.82131577 3.3149543 4.7657595 5.1607304 4.4709673 3.1420088 1.2549162 -0.67563367 -1.9603527 -2.7847404][-4.990025 -5.3786588 -5.0329089 -3.8557439 -2.0820823 0.45201826 3.15372 4.6713085 4.9898434 4.1869822 2.8440504 1.145195 -0.62993455 -1.9757416 -2.8534517][-5.1383185 -5.9138708 -6.2467437 -5.1354008 -3.290957 -1.12779 0.9922471 2.4993591 3.2403388 2.8120728 1.5550094 -0.1791234 -1.7639251 -2.8265977 -3.3549981][-5.4089255 -6.2075796 -6.6519084 -6.051682 -4.8458529 -2.9682343 -1.0427864 0.21850157 0.53603888 0.12087011 -0.66402817 -1.8789437 -3.0428441 -3.8953943 -4.2567077][-5.3672452 -6.1293764 -6.7252221 -6.4643173 -5.7107334 -4.5791192 -3.2427778 -2.2638435 -1.8190506 -2.1453359 -2.7104893 -3.5823998 -4.3825746 -4.8171396 -4.7996159][-5.0433435 -5.7377543 -6.2836218 -6.2122746 -5.7721515 -5.0303297 -4.2550077 -3.6758463 -3.2902784 -3.4513183 -3.713347 -4.1924725 -4.6115947 -4.9257722 -4.7676783][-4.4257712 -4.7882023 -5.0588236 -5.0370007 -4.8597608 -4.4739203 -4.0813289 -3.7531698 -3.661999 -3.6772721 -3.7562666 -3.9800022 -4.0979295 -4.2363276 -4.1085882][-3.7546406 -3.7033668 -3.7035892 -3.6073568 -3.5969076 -3.5525329 -3.4981761 -3.3791363 -3.3482115 -3.4561255 -3.5786042 -3.4901524 -3.3144088 -3.2792749 -3.2278774]]...]
INFO - root - 2017-12-16 09:41:46.032859: step 75610, loss = 0.23, batch loss = 0.17 (29.6 examples/sec; 0.271 sec/batch; 19h:18m:49s remains)
INFO - root - 2017-12-16 09:41:48.894127: step 75620, loss = 0.23, batch loss = 0.18 (28.4 examples/sec; 0.282 sec/batch; 20h:05m:31s remains)
INFO - root - 2017-12-16 09:41:51.675847: step 75630, loss = 0.27, batch loss = 0.21 (29.0 examples/sec; 0.276 sec/batch; 19h:39m:47s remains)
INFO - root - 2017-12-16 09:41:54.534009: step 75640, loss = 0.29, batch loss = 0.23 (27.9 examples/sec; 0.286 sec/batch; 20h:26m:16s remains)
INFO - root - 2017-12-16 09:41:57.373791: step 75650, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.285 sec/batch; 20h:19m:58s remains)
INFO - root - 2017-12-16 09:42:00.187001: step 75660, loss = 0.33, batch loss = 0.27 (28.1 examples/sec; 0.285 sec/batch; 20h:20m:40s remains)
INFO - root - 2017-12-16 09:42:03.020840: step 75670, loss = 0.23, batch loss = 0.17 (29.4 examples/sec; 0.272 sec/batch; 19h:24m:36s remains)
INFO - root - 2017-12-16 09:42:05.886133: step 75680, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.284 sec/batch; 20h:16m:23s remains)
INFO - root - 2017-12-16 09:42:08.765417: step 75690, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.286 sec/batch; 20h:23m:51s remains)
INFO - root - 2017-12-16 09:42:11.595299: step 75700, loss = 0.25, batch loss = 0.19 (29.1 examples/sec; 0.275 sec/batch; 19h:36m:03s remains)
2017-12-16 09:42:12.127261: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0674644 -4.4270773 -4.7862768 -5.0989866 -5.18906 -5.198524 -5.1532459 -5.4388795 -5.7722955 -6.0841551 -6.2011371 -6.1326556 -5.820682 -5.2025127 -4.5397391][-4.3257265 -4.7511177 -5.1922579 -5.5965075 -5.8106632 -6.1513453 -6.4204025 -6.6499643 -6.9285564 -7.3857441 -7.6621909 -7.6544552 -7.3136396 -6.5743508 -5.71491][-4.1331954 -4.710566 -5.3051448 -5.6602116 -5.796011 -6.0996227 -6.4033823 -6.9449425 -7.5318689 -7.9851818 -8.4333687 -8.5899715 -8.4486046 -7.743228 -6.9100337][-3.4913094 -3.95078 -4.2918944 -4.697 -4.8606524 -5.0203352 -5.093832 -5.4096127 -5.8605371 -6.7044559 -7.457696 -7.95844 -8.4450388 -8.2975388 -7.7769513][-2.7712455 -2.9535215 -2.9521339 -2.9147639 -2.5023177 -2.1804411 -1.8141518 -1.787251 -2.2011676 -3.4189537 -4.8916478 -6.170917 -7.2285271 -7.8480177 -7.8581147][-2.285809 -1.9389257 -1.3264832 -0.76246786 0.14137268 1.0515933 1.9728856 2.5406232 2.2335076 0.85734749 -1.0955677 -3.3981974 -5.4803715 -6.8579073 -7.3491106][-1.9196603 -1.3609385 -0.43184972 0.83931684 2.4388394 3.7716293 5.1607914 5.9135532 5.8669567 4.4229488 2.1171246 -0.52588248 -3.0904098 -5.1719732 -6.3561511][-2.0929222 -1.269804 -0.13072777 1.3490019 3.1290526 4.9243011 6.8061266 7.7091961 7.8286877 6.39802 3.9529152 1.0273471 -1.8764415 -4.2409272 -5.4868364][-2.7865009 -1.9783919 -0.98959613 0.31360483 1.8859472 3.4798465 5.2064161 6.2698755 6.6400032 5.6020727 3.7050457 0.99016619 -1.7575932 -3.9438412 -5.0845108][-3.3922482 -3.1281607 -2.8817544 -1.9923806 -0.98864937 0.18444967 1.3693328 2.2057977 2.9421124 2.4547567 1.2229872 -1.0476382 -3.1883807 -4.8265004 -5.6531386][-4.3318958 -4.31807 -4.3624048 -4.3833728 -4.3103909 -3.4427321 -2.3680024 -1.8336065 -1.516093 -1.7224059 -2.1500685 -3.4940259 -5.024416 -6.0909762 -6.434957][-4.7493839 -5.0547895 -5.6927691 -5.9387531 -6.1012268 -6.1115141 -5.6299324 -5.099369 -4.5828757 -4.6545339 -4.9222374 -5.679285 -6.3026352 -6.700428 -6.7938747][-4.7010336 -5.0753613 -5.759202 -6.1468182 -6.6648593 -6.7098546 -6.5651293 -6.5850887 -6.3524504 -6.3091588 -6.1440549 -6.4426851 -6.7719612 -6.8581967 -6.705678][-4.189147 -4.4860406 -4.8925428 -5.2282009 -5.7023726 -5.7389994 -5.8022718 -5.9688654 -5.968226 -5.8646297 -5.6377773 -5.8265958 -5.8966551 -5.86074 -5.7192755][-3.702616 -3.6818357 -3.7240169 -3.9766905 -4.2692785 -4.3749757 -4.4999976 -4.5111556 -4.4328008 -4.3878717 -4.25796 -4.0862436 -3.9910777 -4.0714459 -4.1108351]]...]
INFO - root - 2017-12-16 09:42:14.961314: step 75710, loss = 0.30, batch loss = 0.24 (27.7 examples/sec; 0.289 sec/batch; 20h:34m:59s remains)
INFO - root - 2017-12-16 09:42:17.789360: step 75720, loss = 0.31, batch loss = 0.25 (28.4 examples/sec; 0.282 sec/batch; 20h:05m:18s remains)
INFO - root - 2017-12-16 09:42:20.621968: step 75730, loss = 0.24, batch loss = 0.18 (29.0 examples/sec; 0.276 sec/batch; 19h:40m:49s remains)
INFO - root - 2017-12-16 09:42:23.466257: step 75740, loss = 0.26, batch loss = 0.21 (27.8 examples/sec; 0.288 sec/batch; 20h:30m:32s remains)
INFO - root - 2017-12-16 09:42:26.285257: step 75750, loss = 0.21, batch loss = 0.15 (27.7 examples/sec; 0.289 sec/batch; 20h:37m:58s remains)
INFO - root - 2017-12-16 09:42:29.099042: step 75760, loss = 0.33, batch loss = 0.27 (29.4 examples/sec; 0.272 sec/batch; 19h:23m:43s remains)
INFO - root - 2017-12-16 09:42:31.933635: step 75770, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 20h:15m:43s remains)
INFO - root - 2017-12-16 09:42:34.735178: step 75780, loss = 0.24, batch loss = 0.18 (27.7 examples/sec; 0.288 sec/batch; 20h:34m:18s remains)
INFO - root - 2017-12-16 09:42:37.582040: step 75790, loss = 0.21, batch loss = 0.15 (28.3 examples/sec; 0.282 sec/batch; 20h:07m:21s remains)
INFO - root - 2017-12-16 09:42:40.448363: step 75800, loss = 0.23, batch loss = 0.18 (28.1 examples/sec; 0.284 sec/batch; 20h:16m:30s remains)
2017-12-16 09:42:40.952647: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8731608 -4.344008 -3.6398883 -3.3732753 -3.369452 -3.5578713 -3.8580942 -3.88755 -3.778368 -3.6446528 -3.613986 -3.8226991 -4.2543139 -4.8989515 -5.3949895][-4.5767989 -3.8689833 -3.0232434 -2.550921 -2.4933004 -2.7801983 -3.0465527 -3.1341274 -2.8970244 -2.6971357 -2.8002152 -3.0442929 -3.3205533 -3.7986107 -4.3224483][-4.2042756 -3.2422409 -2.3509998 -1.8209529 -1.7829719 -2.1512082 -2.4569471 -2.4225628 -2.158365 -2.05357 -2.0814445 -2.3916426 -2.8573668 -3.1242485 -3.4314351][-3.2644997 -2.1863928 -1.3213732 -1.0885139 -1.4497366 -1.8836043 -2.1364439 -2.0279226 -1.6380744 -1.5048034 -1.6301908 -1.8727944 -2.2071996 -2.5184772 -2.7615492][-2.1605318 -1.0555885 -0.33835363 -0.28695107 -0.55540276 -0.779737 -0.85043883 -0.76401019 -0.633667 -0.626446 -0.86588287 -1.4330306 -1.8188214 -2.0904894 -2.4468822][-1.6255105 -0.64160848 0.050018787 0.035866261 -0.06256628 0.092169285 0.42432213 0.84509277 1.01192 0.5232234 -0.2296505 -1.0755575 -1.8737314 -2.4057455 -2.6841097][-1.6597586 -1.2474651 -0.86125517 -0.67075682 -0.27475977 0.39142179 1.2235889 1.7366905 1.9173517 1.4151912 0.44088268 -0.54723096 -1.4269295 -2.3788509 -3.1966171][-2.3676307 -2.1760888 -1.9773684 -1.7705979 -0.824883 0.24452829 1.5065861 2.3717613 2.5559669 2.0857391 1.0618086 -0.12315321 -1.1357174 -2.1701794 -3.1085434][-3.0787659 -3.2581608 -3.11547 -2.651473 -1.6240504 0.0077786446 1.43502 2.2611456 2.5746417 2.1219897 1.0031824 -0.2442317 -1.3764353 -2.5198402 -3.6048739][-3.6821337 -4.3157339 -4.3068466 -3.6839685 -2.6112175 -1.2167919 0.18054771 1.2797294 1.6894112 1.3289618 0.49842215 -0.54762769 -1.6470194 -2.735826 -3.7597952][-3.644068 -4.4424706 -4.7618976 -4.5352125 -3.4208145 -2.1761127 -1.1408737 -0.09845829 0.35630846 0.1980381 -0.50493264 -1.3003607 -2.1007109 -2.889931 -3.5843587][-3.4354086 -4.24814 -4.5443583 -4.4989414 -3.9331563 -2.919317 -2.0974641 -1.5929089 -1.210887 -1.0769536 -1.453274 -2.117476 -2.732399 -3.3395493 -3.7509341][-3.318903 -3.8392878 -4.0914936 -3.8638887 -3.5323157 -3.045532 -2.6282167 -2.3606679 -2.2355497 -2.2958071 -2.3310153 -2.457809 -2.7581003 -3.0961063 -3.3786585][-3.353888 -3.604377 -3.6520805 -3.2990112 -2.919312 -2.7027001 -2.7439668 -2.8152909 -2.7594795 -2.8440132 -2.7619648 -2.6679888 -2.7016826 -2.7759314 -2.8566985][-3.6468441 -3.631609 -3.30796 -2.8763266 -2.6410003 -2.5486789 -2.5204344 -2.6078291 -2.6452842 -2.6243393 -2.3584507 -2.2117777 -2.3461721 -2.5873098 -2.6561217]]...]
INFO - root - 2017-12-16 09:42:43.825425: step 75810, loss = 0.27, batch loss = 0.21 (28.1 examples/sec; 0.285 sec/batch; 20h:20m:07s remains)
INFO - root - 2017-12-16 09:42:46.652155: step 75820, loss = 0.23, batch loss = 0.17 (29.1 examples/sec; 0.275 sec/batch; 19h:34m:44s remains)
INFO - root - 2017-12-16 09:42:49.461804: step 75830, loss = 0.23, batch loss = 0.17 (28.3 examples/sec; 0.283 sec/batch; 20h:09m:55s remains)
INFO - root - 2017-12-16 09:42:52.312843: step 75840, loss = 0.24, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 20h:10m:19s remains)
INFO - root - 2017-12-16 09:42:55.167397: step 75850, loss = 0.28, batch loss = 0.22 (27.3 examples/sec; 0.293 sec/batch; 20h:51m:33s remains)
INFO - root - 2017-12-16 09:42:57.972114: step 75860, loss = 0.24, batch loss = 0.18 (29.4 examples/sec; 0.272 sec/batch; 19h:23m:53s remains)
INFO - root - 2017-12-16 09:43:00.761082: step 75870, loss = 0.20, batch loss = 0.14 (29.6 examples/sec; 0.271 sec/batch; 19h:17m:27s remains)
INFO - root - 2017-12-16 09:43:03.596843: step 75880, loss = 0.32, batch loss = 0.26 (28.0 examples/sec; 0.286 sec/batch; 20h:21m:53s remains)
INFO - root - 2017-12-16 09:43:06.411554: step 75890, loss = 0.20, batch loss = 0.14 (29.0 examples/sec; 0.276 sec/batch; 19h:38m:52s remains)
INFO - root - 2017-12-16 09:43:09.291707: step 75900, loss = 0.23, batch loss = 0.17 (25.6 examples/sec; 0.313 sec/batch; 22h:18m:32s remains)
2017-12-16 09:43:09.817578: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1391954 -5.6360588 -5.1911325 -5.1133904 -5.0276308 -5.0285611 -5.0950708 -5.0517869 -4.8690147 -4.4619679 -3.9125133 -3.149509 -2.4710684 -2.158731 -2.1787443][-5.7494984 -5.1722717 -4.7174153 -4.4477425 -4.1303353 -4.0885983 -4.15065 -4.169591 -3.9287536 -3.5115573 -2.8783545 -2.1169333 -1.4421568 -0.92566681 -0.90665531][-4.9282031 -4.2149811 -3.6762333 -3.2259355 -2.7987385 -2.7455435 -3.001368 -3.1774392 -3.099966 -2.9497414 -2.4160247 -1.6536334 -0.978843 -0.35730457 -0.18257046][-3.067888 -2.3131063 -1.8709183 -1.6240711 -1.3866174 -1.29638 -1.6351061 -2.2661965 -2.7420743 -2.6914575 -2.3200579 -1.8829963 -1.3553822 -0.81144786 -0.44459057][-1.5463667 -0.51689672 0.22399282 0.38694477 0.47801542 0.39170408 -0.13587236 -0.91961789 -1.6193852 -2.1534085 -2.5170121 -2.4286046 -2.0764511 -1.7703943 -1.452013][-0.49209094 0.80310297 1.564311 1.9938731 2.3397202 2.0869174 1.3896546 0.43083382 -0.47388411 -1.3407347 -2.1101737 -2.4657221 -2.7055631 -2.4588933 -1.9265139][-0.11408997 1.1925859 1.967751 2.413415 2.7046242 2.6513786 2.2131042 1.2273359 0.14117765 -0.81133938 -1.7341733 -2.4316788 -2.9439044 -3.0944076 -2.8684163][-0.84769297 0.39639378 1.2467732 1.8361292 2.2069044 2.3478909 2.0339699 1.2985644 0.35595322 -0.74816513 -1.764904 -2.6572385 -3.1984239 -3.3804288 -3.2206588][-2.4589581 -1.4552727 -0.67597866 0.065000534 0.67772245 1.1887732 1.1985188 0.679193 -0.11563396 -1.1526036 -2.1568816 -3.0984006 -3.746213 -3.8716137 -3.583684][-3.7327313 -3.4143152 -3.0794892 -2.2943761 -1.4375956 -0.7072084 -0.4058795 -0.70499134 -1.2942703 -2.1267092 -3.0275145 -3.9357266 -4.5242891 -4.4546952 -4.0795903][-5.3884592 -5.3539209 -5.1088347 -4.6436515 -3.9182816 -2.9759467 -2.3335569 -2.3753867 -2.7744708 -3.4405882 -4.1684227 -4.8305416 -5.1241789 -4.9257727 -4.5936613][-5.9292464 -6.19212 -6.2776213 -5.9102464 -5.2240586 -4.4438677 -3.9297609 -3.829453 -3.9809356 -4.3207307 -4.6982207 -5.20222 -5.3895712 -4.9852147 -4.5705385][-5.5788317 -6.0456963 -6.363349 -6.2246914 -5.8242006 -5.2429333 -4.7260246 -4.5056195 -4.4769115 -4.605639 -4.76689 -4.9994779 -5.0515718 -4.6156855 -4.2209787][-4.8132863 -5.1736321 -5.5076532 -5.6318536 -5.54241 -5.0966272 -4.62047 -4.5456958 -4.5623341 -4.4594822 -4.4198589 -4.5103135 -4.4314613 -4.0354838 -3.7287474][-4.3157239 -4.4909544 -4.6443253 -4.7231688 -4.6722469 -4.4995608 -4.32146 -4.2131643 -4.1069522 -4.023931 -3.9068584 -3.7632012 -3.4635336 -3.0095329 -2.6220844]]...]
INFO - root - 2017-12-16 09:43:12.639211: step 75910, loss = 0.32, batch loss = 0.26 (27.1 examples/sec; 0.296 sec/batch; 21h:03m:59s remains)
INFO - root - 2017-12-16 09:43:15.502033: step 75920, loss = 0.28, batch loss = 0.22 (29.4 examples/sec; 0.273 sec/batch; 19h:25m:27s remains)
INFO - root - 2017-12-16 09:43:18.303075: step 75930, loss = 0.35, batch loss = 0.29 (29.0 examples/sec; 0.276 sec/batch; 19h:38m:15s remains)
INFO - root - 2017-12-16 09:43:21.164046: step 75940, loss = 0.26, batch loss = 0.20 (27.5 examples/sec; 0.291 sec/batch; 20h:45m:28s remains)
INFO - root - 2017-12-16 09:43:23.982240: step 75950, loss = 0.39, batch loss = 0.33 (28.0 examples/sec; 0.286 sec/batch; 20h:20m:51s remains)
INFO - root - 2017-12-16 09:43:26.813340: step 75960, loss = 0.56, batch loss = 0.50 (27.9 examples/sec; 0.287 sec/batch; 20h:27m:19s remains)
INFO - root - 2017-12-16 09:43:29.668888: step 75970, loss = 0.25, batch loss = 0.19 (27.6 examples/sec; 0.290 sec/batch; 20h:39m:25s remains)
INFO - root - 2017-12-16 09:43:32.505543: step 75980, loss = 0.26, batch loss = 0.20 (28.7 examples/sec; 0.278 sec/batch; 19h:50m:36s remains)
INFO - root - 2017-12-16 09:43:35.330713: step 75990, loss = 0.32, batch loss = 0.27 (28.7 examples/sec; 0.278 sec/batch; 19h:49m:57s remains)
INFO - root - 2017-12-16 09:43:38.134330: step 76000, loss = 0.20, batch loss = 0.14 (28.5 examples/sec; 0.280 sec/batch; 19h:58m:57s remains)
2017-12-16 09:43:38.743967: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7402492 -1.5441132 -1.4370458 -1.7183521 -2.1892591 -2.3676634 -2.5419433 -2.6804223 -2.9506209 -3.2603822 -3.6671333 -3.8334591 -3.8364635 -3.6832683 -3.367137][-1.0109468 -1.0197477 -1.1540833 -1.4448407 -1.8968008 -2.1285403 -2.3465381 -2.3650804 -2.5899107 -3.0307417 -3.4319031 -3.6549678 -3.7955933 -3.6342421 -3.2781725][-0.76583576 -0.96766162 -1.2436683 -1.7439573 -2.0399745 -1.9399903 -1.8849959 -1.9447005 -2.1418757 -2.2246768 -2.5503082 -3.1128039 -3.449981 -3.4279854 -3.3162541][-0.86183453 -1.2410192 -1.7737582 -2.2234378 -2.3589284 -2.158354 -1.7712526 -1.5300817 -1.4057872 -1.4914241 -1.8000128 -2.2173724 -2.7325778 -3.02004 -3.0984526][-1.3447649 -1.94772 -2.4556437 -2.7841527 -2.5654736 -1.8563817 -1.1099226 -0.59723711 -0.21055079 -0.13335609 -0.39018869 -0.93521714 -1.6668417 -2.4073462 -2.9933004][-1.9208684 -2.4580083 -2.8496768 -2.9675782 -2.5143161 -1.4978642 -0.43290997 0.52159452 1.2498846 1.5130224 1.2604179 0.44049406 -0.59768486 -1.5564797 -2.5619326][-2.4293442 -2.6752133 -2.7966714 -2.6925876 -2.0671787 -1.0545893 0.12323141 1.1487656 1.955596 2.4175858 2.3323455 1.459455 0.27733803 -1.0714254 -2.4030793][-2.7582872 -2.814105 -2.76193 -2.4791574 -1.8185921 -0.76649332 0.45508146 1.3937283 2.193851 2.4840136 2.1662693 1.3062835 0.041192055 -1.3787248 -2.6981683][-2.8732092 -2.7025478 -2.5951021 -2.2328894 -1.5334997 -0.78629279 0.0068955421 0.80146837 1.4373016 1.5987821 1.2660017 0.41925287 -0.82173014 -2.1938715 -3.4768651][-2.9101655 -2.772048 -2.6219752 -2.2983005 -1.8927939 -1.2923169 -0.676527 -0.37967491 -0.16902924 -0.15059376 -0.38166094 -0.99855351 -1.9221561 -2.8431931 -3.7760489][-2.8505216 -2.6672029 -2.5804141 -2.5320132 -2.4387617 -2.0772994 -1.7012558 -1.6421745 -1.7936935 -2.0195744 -2.2073431 -2.6057019 -3.1263766 -3.6804571 -4.3055162][-2.6806197 -2.6360536 -2.622647 -2.6272862 -2.5593886 -2.3481684 -2.2395594 -2.3882768 -2.7040775 -3.0768185 -3.3944159 -3.6275656 -3.8662674 -4.172513 -4.62432][-2.6194468 -2.611238 -2.6687555 -2.6856656 -2.5878177 -2.4746246 -2.3432653 -2.4281297 -2.7593863 -3.1256206 -3.4670548 -3.7337935 -3.8358212 -3.8748178 -4.1914682][-2.7703786 -2.8860502 -2.8404121 -2.9104605 -2.8761046 -2.5261908 -2.1655304 -2.1597595 -2.3237679 -2.5189843 -2.68923 -2.9437017 -3.0964327 -3.1534419 -3.3689866][-2.66176 -2.8092041 -2.8200612 -2.8513315 -2.6867948 -2.2002523 -1.7632575 -1.4305501 -1.3878214 -1.4269078 -1.6001611 -1.8405154 -1.9658194 -2.1097982 -2.4481215]]...]
INFO - root - 2017-12-16 09:43:41.599570: step 76010, loss = 0.25, batch loss = 0.19 (28.0 examples/sec; 0.286 sec/batch; 20h:23m:28s remains)
INFO - root - 2017-12-16 09:43:44.475801: step 76020, loss = 0.28, batch loss = 0.23 (28.3 examples/sec; 0.283 sec/batch; 20h:09m:07s remains)
INFO - root - 2017-12-16 09:43:47.331018: step 76030, loss = 0.21, batch loss = 0.15 (29.0 examples/sec; 0.275 sec/batch; 19h:37m:16s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 09:43:50.150013: step 76040, loss = 0.26, batch loss = 0.20 (27.6 examples/sec; 0.290 sec/batch; 20h:38m:42s remains)
INFO - root - 2017-12-16 09:43:53.037498: step 76050, loss = 0.22, batch loss = 0.17 (27.4 examples/sec; 0.292 sec/batch; 20h:46m:08s remains)
INFO - root - 2017-12-16 09:43:55.898474: step 76060, loss = 0.30, batch loss = 0.24 (26.8 examples/sec; 0.299 sec/batch; 21h:17m:27s remains)
INFO - root - 2017-12-16 09:43:58.750839: step 76070, loss = 0.28, batch loss = 0.22 (28.6 examples/sec; 0.280 sec/batch; 19h:56m:27s remains)
INFO - root - 2017-12-16 09:44:01.610514: step 76080, loss = 0.29, batch loss = 0.23 (28.6 examples/sec; 0.280 sec/batch; 19h:57m:19s remains)
INFO - root - 2017-12-16 09:44:04.434882: step 76090, loss = 0.29, batch loss = 0.23 (28.3 examples/sec; 0.283 sec/batch; 20h:09m:47s remains)
INFO - root - 2017-12-16 09:44:07.287236: step 76100, loss = 0.33, batch loss = 0.27 (27.5 examples/sec; 0.290 sec/batch; 20h:41m:12s remains)
2017-12-16 09:44:07.896355: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2972665 -2.1414428 -2.0623839 -2.0738151 -2.1715276 -2.2772386 -2.4768376 -2.4098976 -2.4672298 -2.6087284 -2.5348639 -2.5961413 -2.6634285 -2.8924005 -3.0824649][-1.5939567 -1.6101999 -1.7641466 -1.8862371 -2.0800364 -2.3373449 -2.5234261 -2.5429974 -2.5199618 -2.6497612 -2.6887782 -2.7117457 -2.7877164 -2.9790912 -3.2029586][-1.2634211 -1.4359882 -1.8157773 -2.2068479 -2.6062107 -2.9645238 -3.1332998 -3.0843384 -3.0240507 -3.1174486 -3.1717107 -3.1726298 -3.1799879 -3.1713419 -3.3707533][-1.2134607 -1.5528226 -2.019907 -2.5793755 -3.0593925 -3.4112163 -3.4766061 -3.3418269 -3.1539688 -3.1708865 -3.2188478 -3.2921507 -3.3737197 -3.3616087 -3.4117069][-1.3297565 -1.9627762 -2.6006427 -2.9177837 -3.1950366 -3.3005433 -3.3226049 -3.1919222 -2.9048367 -2.8409729 -2.8600063 -2.8933816 -2.9693265 -2.9454584 -3.0341008][-1.613548 -2.1548092 -2.811769 -2.9255476 -2.7879527 -2.5807519 -2.3425555 -2.1677384 -1.9838338 -2.055841 -2.1626792 -2.2311993 -2.3305926 -2.3053453 -2.4186592][-1.8677275 -2.1133664 -2.6535187 -2.7894864 -2.4331207 -1.661097 -0.87524509 -0.4709475 -0.32141638 -0.584846 -0.99779582 -1.2751639 -1.5345774 -1.6249743 -1.7838514][-1.8324172 -1.98311 -2.3832836 -2.5067463 -2.2545824 -1.3180382 -0.11543941 0.83519268 1.2955146 1.0116267 0.50454569 -0.13476276 -0.69979882 -0.96374369 -1.2899446][-1.2270355 -1.4232674 -1.9223232 -2.1049564 -1.7320583 -0.76311684 0.45077991 1.6407847 2.4077315 2.2232966 1.5609293 0.84963751 0.24508476 -0.22090387 -0.68609047][-0.64367056 -0.89093375 -1.6170437 -1.8505545 -1.6866121 -0.88728094 0.18382692 1.1484313 1.8350682 1.858849 1.4706879 0.93349886 0.51912451 0.15165901 -0.24601078][0.16143703 -0.30441666 -1.3414471 -1.8814597 -1.8909774 -1.2026532 -0.19516516 0.56538868 1.0529108 1.0568748 0.73185015 0.46063328 0.20098543 2.2411346e-05 -0.27721882][-0.0099720955 -0.60337329 -1.8086119 -2.3999591 -2.4325314 -1.8421643 -1.0165114 -0.37548208 0.10913324 0.10110331 -0.22468853 -0.41167402 -0.56471109 -0.78139424 -0.92100191][-0.15920639 -0.857826 -2.0369809 -2.6937046 -2.8516245 -2.5592856 -1.9829481 -1.5187726 -1.0984993 -1.0171154 -1.2734847 -1.4502106 -1.5060484 -1.4653656 -1.36411][-0.2228713 -0.71891832 -1.6126909 -2.3443193 -2.7356887 -2.637208 -2.5023477 -2.2749388 -2.1161289 -2.1016209 -2.2610834 -2.3172534 -2.0468652 -1.6748824 -1.3837783][-0.19135332 -0.45391726 -1.0447562 -1.6710021 -2.0164661 -2.1906517 -2.3133264 -2.3392267 -2.4196272 -2.6104193 -2.9538984 -2.9720619 -2.620491 -2.0749066 -1.32071]]...]
INFO - root - 2017-12-16 09:44:10.704853: step 76110, loss = 0.35, batch loss = 0.29 (28.8 examples/sec; 0.278 sec/batch; 19h:48m:33s remains)
INFO - root - 2017-12-16 09:44:13.552509: step 76120, loss = 0.27, batch loss = 0.22 (28.4 examples/sec; 0.282 sec/batch; 20h:03m:00s remains)
INFO - root - 2017-12-16 09:44:16.342584: step 76130, loss = 0.29, batch loss = 0.23 (30.1 examples/sec; 0.266 sec/batch; 18h:57m:02s remains)
INFO - root - 2017-12-16 09:44:19.199194: step 76140, loss = 0.24, batch loss = 0.19 (27.4 examples/sec; 0.292 sec/batch; 20h:45m:56s remains)
INFO - root - 2017-12-16 09:44:22.031879: step 76150, loss = 0.23, batch loss = 0.17 (27.8 examples/sec; 0.288 sec/batch; 20h:28m:51s remains)
INFO - root - 2017-12-16 09:44:24.920910: step 76160, loss = 0.43, batch loss = 0.37 (27.0 examples/sec; 0.297 sec/batch; 21h:07m:52s remains)
INFO - root - 2017-12-16 09:44:27.714825: step 76170, loss = 0.22, batch loss = 0.16 (28.5 examples/sec; 0.280 sec/batch; 19h:57m:45s remains)
INFO - root - 2017-12-16 09:44:30.546687: step 76180, loss = 0.26, batch loss = 0.20 (26.9 examples/sec; 0.298 sec/batch; 21h:11m:38s remains)
INFO - root - 2017-12-16 09:44:33.339066: step 76190, loss = 0.20, batch loss = 0.14 (28.9 examples/sec; 0.276 sec/batch; 19h:41m:06s remains)
INFO - root - 2017-12-16 09:44:36.195675: step 76200, loss = 0.30, batch loss = 0.24 (27.7 examples/sec; 0.289 sec/batch; 20h:35m:16s remains)
2017-12-16 09:44:36.763285: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.90641 -1.1820285 -1.2932742 -1.4759212 -1.5989945 -1.8785057 -2.0561793 -2.1164155 -2.0816989 -2.0059171 -1.9143832 -1.8183377 -1.8767855 -2.0612924 -2.3134222][-0.54392266 -0.76390409 -1.0766609 -1.3769832 -1.5392749 -1.7818434 -1.9219935 -1.9737551 -1.925195 -1.8356256 -1.8139861 -1.7722514 -1.8417716 -2.0464745 -2.3500984][-1.2279999 -1.4210396 -1.7588527 -2.0518005 -2.2237554 -2.2545233 -2.130367 -1.9598014 -1.739799 -1.5847995 -1.59059 -1.6662984 -1.900578 -2.1258142 -2.4416928][-2.6215925 -2.695919 -2.6799164 -2.7978506 -2.8470042 -2.6352558 -2.1165271 -1.768796 -1.5296812 -1.5027556 -1.5744994 -1.7241623 -2.0373776 -2.3245964 -2.5729017][-3.8001204 -3.9644737 -3.8572426 -3.6563463 -3.2925439 -2.716629 -1.9784284 -1.265636 -0.98501253 -1.0514686 -1.3340645 -1.6683874 -2.003655 -2.288569 -2.3913579][-4.2303967 -4.5699196 -4.6712217 -4.3693972 -3.6331234 -2.7750223 -1.8554845 -0.88695621 -0.37483883 -0.4526701 -0.94297576 -1.3929896 -1.6453738 -1.847368 -1.7331033][-4.1824946 -4.6764212 -4.8943396 -4.6107316 -3.8491302 -2.6590981 -1.3764911 -0.37796021 0.023277283 -0.034224987 -0.55859256 -1.2083025 -1.5845075 -1.4570646 -0.84090304][-4.1128426 -4.7724457 -4.7734394 -4.3894577 -3.4567432 -2.1827385 -0.77169108 0.30628967 0.65356541 0.43383551 -0.21858692 -0.86739659 -1.1251605 -1.0042202 -0.27010345][-2.8132305 -3.5393577 -3.9285705 -3.7851384 -2.9317765 -1.7775118 -0.60054708 0.38531446 0.67687178 0.377429 -0.31437397 -0.89865065 -1.0560122 -0.90307355 -0.11386919][-1.5462291 -2.2555842 -2.970118 -3.1331553 -2.6679175 -1.7928269 -0.88767791 -0.2064209 -0.01426363 -0.19704628 -0.62290454 -1.1118236 -1.1958067 -0.91390753 -0.0535841][-1.322026 -1.6163669 -2.0444846 -2.3796494 -2.3379662 -1.9139032 -1.3677237 -1.0729876 -1.062072 -0.98384047 -1.0566428 -1.2532732 -1.1617317 -0.83161569 -0.15726376][-1.7012823 -1.6027775 -1.555922 -1.8455155 -2.0214846 -2.2240262 -2.2947743 -2.3260491 -2.4052274 -2.1558192 -1.8640127 -1.6255882 -1.2750149 -0.95177794 -0.44100714][-2.0072663 -1.394377 -1.1892688 -1.444525 -1.8516531 -2.4981008 -2.9809744 -3.3741255 -3.4779084 -3.1841042 -2.679244 -2.1463671 -1.7156007 -1.5680366 -1.3551571][-1.8534591 -0.79645967 -0.58008671 -0.838995 -1.4713705 -2.3531208 -3.2539439 -3.8491156 -3.8312075 -3.3846779 -2.7529006 -2.2237477 -1.7647479 -1.6642165 -1.7034461][-1.8263426 -0.51822329 0.032263279 -0.30620527 -1.1804781 -2.265461 -3.2737298 -3.8000197 -3.8748827 -3.415098 -2.6289802 -1.9218557 -1.2760098 -1.0934577 -1.3438201]]...]
INFO - root - 2017-12-16 09:44:39.661201: step 76210, loss = 0.32, batch loss = 0.26 (25.9 examples/sec; 0.309 sec/batch; 21h:58m:06s remains)
INFO - root - 2017-12-16 09:44:42.513752: step 76220, loss = 0.23, batch loss = 0.17 (27.1 examples/sec; 0.295 sec/batch; 21h:00m:06s remains)
INFO - root - 2017-12-16 09:44:45.396119: step 76230, loss = 0.33, batch loss = 0.27 (27.3 examples/sec; 0.293 sec/batch; 20h:53m:14s remains)
INFO - root - 2017-12-16 09:44:48.271685: step 76240, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.283 sec/batch; 20h:10m:33s remains)
INFO - root - 2017-12-16 09:44:51.126474: step 76250, loss = 0.33, batch loss = 0.28 (26.9 examples/sec; 0.297 sec/batch; 21h:09m:42s remains)
INFO - root - 2017-12-16 09:44:53.981563: step 76260, loss = 0.22, batch loss = 0.16 (28.4 examples/sec; 0.282 sec/batch; 20h:04m:44s remains)
INFO - root - 2017-12-16 09:44:56.845577: step 76270, loss = 0.22, batch loss = 0.16 (27.9 examples/sec; 0.287 sec/batch; 20h:26m:14s remains)
INFO - root - 2017-12-16 09:44:59.695814: step 76280, loss = 0.28, batch loss = 0.22 (27.7 examples/sec; 0.289 sec/batch; 20h:35m:27s remains)
INFO - root - 2017-12-16 09:45:02.529315: step 76290, loss = 0.24, batch loss = 0.18 (27.4 examples/sec; 0.292 sec/batch; 20h:45m:26s remains)
INFO - root - 2017-12-16 09:45:05.392108: step 76300, loss = 0.36, batch loss = 0.30 (27.7 examples/sec; 0.288 sec/batch; 20h:31m:03s remains)
2017-12-16 09:45:05.954690: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8132396 -3.3670759 -3.4732389 -3.502914 -3.3008466 -3.3451738 -3.5988991 -3.6325586 -3.7478836 -3.8439064 -3.7599521 -3.6821761 -3.6154323 -3.583921 -3.335772][-2.1338391 -2.5287631 -2.5989408 -2.8396158 -2.8130836 -2.9025536 -3.1803954 -3.3193944 -3.5806403 -3.6541994 -3.5653708 -3.5389686 -3.3872359 -3.1609459 -2.7414093][-1.2494392 -1.7218211 -1.9168987 -2.1843534 -2.2851536 -2.4037116 -2.6467071 -2.8618548 -3.3082213 -3.6492324 -3.6939189 -3.6047196 -3.3854136 -2.8214459 -2.2217045][-0.47591019 -0.83736634 -1.119082 -1.503242 -1.6485183 -1.688019 -1.9123418 -2.2940409 -2.8861122 -3.3913944 -3.6627324 -3.639071 -3.2805839 -2.5670445 -1.8639171][-0.26676178 -0.32313395 -0.46697307 -0.81655383 -0.95391726 -0.99302006 -1.2575872 -1.741652 -2.4947562 -3.0370724 -3.2530174 -3.2978909 -2.9125023 -2.1825535 -1.5416908][-0.32127094 -0.30356169 -0.51830435 -0.74469638 -0.71123743 -0.7158134 -0.96056271 -1.5290306 -2.333926 -2.8986454 -3.0625429 -2.8731058 -2.3851242 -1.6364846 -0.92272139][-0.89549613 -0.78665042 -0.92346025 -1.0431619 -0.968837 -0.87566924 -0.96151924 -1.4172425 -2.1225746 -2.5061326 -2.512918 -2.2900257 -1.8754296 -1.2278686 -0.59790492][-1.7653186 -1.6941266 -1.8250902 -1.8512995 -1.6656227 -1.3493428 -1.210824 -1.4187396 -1.8046505 -1.9424636 -1.7285826 -1.5709786 -1.2473817 -0.79546428 -0.39817905][-2.7953446 -2.6499009 -2.686409 -2.6501262 -2.4284906 -1.9713216 -1.5687547 -1.365443 -1.3888478 -1.2848845 -0.83823252 -0.66762424 -0.49936056 -0.32789755 -0.083960056][-3.6851783 -3.8099017 -3.800688 -3.5449173 -3.20085 -2.7454731 -2.1258874 -1.5938845 -1.3144493 -1.0583858 -0.62221503 -0.3318615 -0.15017605 -0.30080271 -0.28035307][-4.8710489 -5.1834092 -5.1170273 -4.8427238 -4.3444538 -3.6729317 -2.9125195 -2.1905959 -1.7056212 -1.2958066 -0.94971061 -0.75964117 -0.62905478 -0.85696149 -0.91999197][-6.0162153 -6.5331154 -6.4669123 -5.9252377 -5.2102594 -4.5578823 -3.8199964 -2.962914 -2.4350071 -2.2140963 -2.0563612 -1.9840498 -1.990639 -2.2665884 -2.3014135][-7.022769 -7.6930714 -7.7010126 -7.166544 -6.3225918 -5.5510712 -4.8760648 -4.2094016 -3.7353468 -3.4590287 -3.3660238 -3.5322976 -3.6559923 -3.8890409 -3.818557][-7.8216672 -8.4736919 -8.5207415 -8.0679169 -7.2762213 -6.6018829 -6.0526 -5.4489422 -5.1232185 -5.0284581 -5.0441813 -5.216157 -5.3357725 -5.5474386 -5.3764253][-8.2646179 -8.86491 -8.9908676 -8.5780373 -7.8935542 -7.35874 -6.9805088 -6.6187992 -6.4735985 -6.45411 -6.5491524 -6.8028336 -6.9240093 -6.9216375 -6.5757189]]...]
INFO - root - 2017-12-16 09:45:08.816061: step 76310, loss = 0.24, batch loss = 0.18 (26.5 examples/sec; 0.302 sec/batch; 21h:27m:53s remains)
INFO - root - 2017-12-16 09:45:11.621607: step 76320, loss = 0.35, batch loss = 0.29 (29.2 examples/sec; 0.274 sec/batch; 19h:31m:25s remains)
INFO - root - 2017-12-16 09:45:14.474015: step 76330, loss = 0.27, batch loss = 0.21 (27.6 examples/sec; 0.290 sec/batch; 20h:39m:21s remains)
INFO - root - 2017-12-16 09:45:17.310335: step 76340, loss = 0.25, batch loss = 0.19 (28.4 examples/sec; 0.281 sec/batch; 20h:00m:56s remains)
INFO - root - 2017-12-16 09:45:20.155292: step 76350, loss = 0.29, batch loss = 0.23 (27.6 examples/sec; 0.290 sec/batch; 20h:38m:45s remains)
INFO - root - 2017-12-16 09:45:22.944880: step 76360, loss = 0.22, batch loss = 0.16 (28.7 examples/sec; 0.279 sec/batch; 19h:50m:11s remains)
INFO - root - 2017-12-16 09:45:25.808945: step 76370, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.285 sec/batch; 20h:15m:22s remains)
INFO - root - 2017-12-16 09:45:28.593028: step 76380, loss = 0.24, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 20h:06m:47s remains)
INFO - root - 2017-12-16 09:45:31.449817: step 76390, loss = 0.33, batch loss = 0.27 (29.0 examples/sec; 0.276 sec/batch; 19h:39m:11s remains)
INFO - root - 2017-12-16 09:45:34.273894: step 76400, loss = 0.34, batch loss = 0.28 (28.4 examples/sec; 0.282 sec/batch; 20h:03m:50s remains)
2017-12-16 09:45:34.819951: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7699156 -5.6491475 -5.6507287 -5.960361 -6.1449819 -6.0060139 -5.7286377 -5.4339981 -5.1339822 -4.9015603 -4.7198968 -4.6003809 -4.4544821 -4.2318592 -4.0885463][-5.1226621 -4.9791069 -4.971559 -5.2420025 -5.4714942 -5.5108452 -5.4359894 -5.1252317 -4.8985186 -4.7944193 -4.6715875 -4.5985518 -4.4481859 -4.1849561 -3.9244337][-3.5131614 -3.4147167 -3.3839178 -3.6637611 -4.0754819 -4.2160916 -4.260829 -4.18895 -4.1617918 -4.2289724 -4.396142 -4.3701129 -4.1638556 -3.8413544 -3.5261354][-1.5581701 -1.2671335 -1.2993591 -1.7168021 -2.2678952 -2.6221457 -2.8847747 -2.8488424 -2.8607683 -3.0651789 -3.3567553 -3.6301417 -3.6586027 -3.4077542 -3.0764194][0.30848312 0.70931053 0.57029581 0.008450985 -0.65624881 -1.0905321 -1.307853 -1.2413929 -1.2393713 -1.4911978 -1.9123192 -2.32526 -2.5482547 -2.731564 -2.6521931][1.0945868 1.6461706 1.5212274 0.89262772 0.32302332 0.016993523 -0.0059905052 0.35636711 0.58439159 0.36893749 -0.14189243 -0.91328526 -1.5784369 -2.0183206 -2.1355493][1.3283415 1.7935362 1.521769 1.0440536 0.70690346 0.58089542 1.002419 1.6819935 2.1290879 2.0445137 1.4992929 0.57071352 -0.51534224 -1.5336187 -2.0219662][0.94839621 1.091495 0.74978113 0.33120012 0.17588949 0.4342041 1.3843265 2.476151 3.1808834 3.1716552 2.5430751 1.2881823 -0.090672016 -1.4746156 -2.3158534][-0.26134396 -0.22775888 -0.62251592 -1.034729 -0.9251287 -0.23961353 0.96802807 2.1425142 3.0585833 3.1321454 2.4468036 0.99048805 -0.64887261 -2.2095718 -3.2055378][-1.4968903 -1.8697615 -2.2851467 -2.5570021 -2.409698 -1.4190838 -0.033247471 1.2837296 2.1368065 2.0866022 1.3790445 -0.10316372 -1.8846235 -3.5487826 -4.4984446][-2.5196118 -2.9976544 -3.3651471 -3.6562047 -3.6703563 -2.7360077 -1.3516464 -0.2099719 0.54379034 0.48075485 -0.31820393 -1.7678826 -3.3365941 -4.8834743 -5.816463][-3.4650209 -3.8826172 -4.3435068 -4.4853449 -4.3862243 -3.559752 -2.6145887 -1.6802733 -1.110317 -1.2774861 -1.8546085 -3.2627556 -4.6151724 -5.7630749 -6.5084639][-3.8480265 -4.1177812 -4.669096 -4.8769884 -4.8557305 -4.2727962 -3.6751478 -2.9459915 -2.4456589 -2.610136 -3.2338486 -4.2740712 -5.2805338 -6.2004094 -6.5833912][-3.8515325 -3.7045255 -3.9703698 -4.2517948 -4.3568497 -4.0148568 -3.7727675 -3.5503783 -3.4667377 -3.4776473 -3.7115819 -4.6610775 -5.3660593 -5.8623395 -6.0186963][-3.7854805 -3.3410349 -3.3292847 -3.5286732 -3.6565092 -3.5627766 -3.5718474 -3.506387 -3.4912944 -3.6009135 -3.8292494 -4.3727341 -4.8090177 -5.1406088 -4.9007306]]...]
INFO - root - 2017-12-16 09:45:37.581741: step 76410, loss = 0.24, batch loss = 0.19 (29.3 examples/sec; 0.273 sec/batch; 19h:25m:15s remains)
INFO - root - 2017-12-16 09:45:40.420888: step 76420, loss = 0.23, batch loss = 0.17 (28.2 examples/sec; 0.284 sec/batch; 20h:10m:36s remains)
INFO - root - 2017-12-16 09:45:43.294589: step 76430, loss = 0.26, batch loss = 0.20 (29.0 examples/sec; 0.276 sec/batch; 19h:37m:46s remains)
INFO - root - 2017-12-16 09:45:46.175484: step 76440, loss = 0.24, batch loss = 0.18 (29.0 examples/sec; 0.276 sec/batch; 19h:37m:15s remains)
INFO - root - 2017-12-16 09:45:49.018907: step 76450, loss = 0.23, batch loss = 0.18 (28.3 examples/sec; 0.283 sec/batch; 20h:05m:35s remains)
INFO - root - 2017-12-16 09:45:51.824562: step 76460, loss = 0.25, batch loss = 0.19 (28.9 examples/sec; 0.277 sec/batch; 19h:42m:18s remains)
INFO - root - 2017-12-16 09:45:54.702117: step 76470, loss = 0.22, batch loss = 0.16 (27.9 examples/sec; 0.287 sec/batch; 20h:23m:20s remains)
INFO - root - 2017-12-16 09:45:57.536766: step 76480, loss = 0.36, batch loss = 0.30 (27.8 examples/sec; 0.288 sec/batch; 20h:30m:04s remains)
INFO - root - 2017-12-16 09:46:00.353254: step 76490, loss = 0.23, batch loss = 0.17 (29.1 examples/sec; 0.275 sec/batch; 19h:31m:54s remains)
INFO - root - 2017-12-16 09:46:03.213808: step 76500, loss = 0.23, batch loss = 0.18 (27.6 examples/sec; 0.290 sec/batch; 20h:36m:45s remains)
2017-12-16 09:46:03.811127: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3082023 -3.4110656 -3.3836033 -3.2218881 -3.1042044 -2.7502234 -2.8256433 -2.6833839 -2.6690106 -2.6631465 -2.73219 -2.8944139 -2.7885122 -2.779501 -2.4470468][-1.7336447 -1.9069774 -1.8843169 -1.616864 -1.1761081 -0.71740007 -0.79835176 -1.0842249 -1.4415507 -1.7149153 -2.2094738 -2.4693513 -2.3719041 -2.1281548 -1.5587072][-0.699728 -0.596153 -0.52262163 -0.21633816 0.23283625 0.42458487 0.32107115 0.24106216 -0.16372776 -1.012728 -1.8894348 -2.4006176 -2.411994 -2.1498768 -1.4643753][-0.0048470497 -0.222332 -0.3448925 -0.053343773 0.22111034 0.843863 1.0874863 0.93275023 0.36355305 -0.58159542 -1.4432857 -2.1639349 -2.484705 -2.377315 -1.7944324][-0.41886854 -0.68047333 -0.83285618 -0.61758614 0.023065567 0.69645691 1.2579236 1.3996 0.71105909 -0.47236538 -1.6853855 -2.4710555 -2.8251572 -2.951334 -2.5696902][-1.2639036 -1.8109906 -2.0560119 -1.44699 -0.30174446 0.71146107 1.5127425 1.7882891 1.5113192 0.37201643 -1.1263478 -2.2558765 -2.8971586 -3.2216239 -3.076345][-2.2250659 -2.605201 -2.7015727 -1.9912574 -0.772176 0.49039936 1.5842614 2.2279949 2.13688 1.1505303 -0.19743395 -1.440604 -2.3182676 -2.8113647 -2.777864][-2.9451048 -3.269062 -3.12718 -2.2713943 -0.944819 0.46614838 1.7502146 2.3279061 2.2892351 1.5749822 0.34806108 -0.93375468 -1.8778417 -2.3782523 -2.3779573][-3.4639797 -3.7270858 -3.4474518 -2.6109843 -1.4139564 -0.10846615 1.0795488 1.5760612 1.5816026 1.0313091 0.10427427 -0.99367595 -1.6376512 -1.7588615 -1.3546836][-3.7432194 -3.9360394 -3.644351 -2.8834927 -1.9250531 -0.94508982 -0.050795078 0.33330965 0.16719151 -0.40548086 -1.0784245 -1.7186794 -1.8083768 -1.5628781 -0.82910419][-3.7517624 -3.8689 -3.6311107 -3.011976 -2.3650241 -1.6910427 -1.1077833 -1.0082843 -1.4629865 -2.0731044 -2.7010508 -3.2507644 -3.2034285 -2.5335851 -1.379051][-3.6447887 -3.5454578 -3.3519287 -3.1094384 -2.8034706 -2.62311 -2.4878798 -2.5693707 -3.0788407 -3.7693527 -4.2488217 -4.5077434 -4.3772416 -3.6433215 -2.3825989][-3.3712955 -3.1906004 -2.9947672 -2.8010523 -2.7486744 -2.8682623 -3.0212247 -3.4294088 -4.1552935 -4.8975596 -5.261858 -5.4160752 -5.2769279 -4.6976647 -3.5650845][-3.3149142 -3.0634265 -2.833307 -2.6323385 -2.6573071 -2.8338995 -3.123327 -3.612453 -4.4671931 -5.4594011 -6.1540174 -6.4665775 -6.4197125 -6.0914745 -5.1859536][-3.3173175 -2.969487 -2.6738079 -2.4955173 -2.4102638 -2.4729638 -2.8493941 -3.4156623 -4.3893824 -5.7031326 -6.792202 -7.50439 -7.818243 -7.7068233 -6.9406362]]...]
INFO - root - 2017-12-16 09:46:06.650522: step 76510, loss = 0.23, batch loss = 0.17 (28.6 examples/sec; 0.280 sec/batch; 19h:54m:39s remains)
INFO - root - 2017-12-16 09:46:09.521333: step 76520, loss = 0.27, batch loss = 0.21 (29.0 examples/sec; 0.276 sec/batch; 19h:38m:06s remains)
INFO - root - 2017-12-16 09:46:12.336195: step 76530, loss = 0.27, batch loss = 0.21 (28.2 examples/sec; 0.284 sec/batch; 20h:12m:10s remains)
INFO - root - 2017-12-16 09:46:15.204481: step 76540, loss = 0.23, batch loss = 0.17 (27.5 examples/sec; 0.291 sec/batch; 20h:42m:48s remains)
INFO - root - 2017-12-16 09:46:18.005087: step 76550, loss = 0.23, batch loss = 0.17 (30.3 examples/sec; 0.264 sec/batch; 18h:47m:33s remains)
INFO - root - 2017-12-16 09:46:20.876323: step 76560, loss = 0.25, batch loss = 0.20 (26.8 examples/sec; 0.298 sec/batch; 21h:12m:19s remains)
INFO - root - 2017-12-16 09:46:23.718777: step 76570, loss = 0.25, batch loss = 0.19 (27.4 examples/sec; 0.292 sec/batch; 20h:45m:53s remains)
INFO - root - 2017-12-16 09:46:26.597002: step 76580, loss = 0.27, batch loss = 0.21 (27.7 examples/sec; 0.289 sec/batch; 20h:31m:05s remains)
INFO - root - 2017-12-16 09:46:29.436721: step 76590, loss = 0.23, batch loss = 0.18 (29.6 examples/sec; 0.270 sec/batch; 19h:11m:46s remains)
INFO - root - 2017-12-16 09:46:32.294622: step 76600, loss = 0.24, batch loss = 0.18 (28.7 examples/sec; 0.278 sec/batch; 19h:47m:22s remains)
2017-12-16 09:46:32.820830: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5451651 -5.0526152 -5.1919842 -5.376318 -5.4322977 -5.5420775 -5.4964108 -5.5081134 -5.6164908 -5.6227808 -5.5849042 -5.55942 -5.644835 -5.5444202 -5.5224719][-4.6731133 -5.1953988 -5.2889242 -5.4754305 -5.4724751 -5.6466308 -5.6454062 -5.8130836 -6.01777 -6.1815791 -6.3274608 -6.210392 -6.1308718 -6.0055246 -5.7111487][-4.7478809 -5.1115971 -5.0835404 -5.24111 -5.13835 -5.188426 -5.1348782 -5.168149 -5.3310208 -5.6205177 -5.8925705 -5.9417434 -5.8153667 -5.4153647 -5.0205154][-4.3069153 -4.5502467 -4.4320564 -4.33389 -4.01781 -3.9354858 -3.774756 -3.6858931 -4.0134096 -4.5290084 -4.8435774 -5.015336 -4.814671 -4.44033 -3.9860778][-3.669755 -3.624157 -3.3230839 -2.9856491 -2.3790646 -1.8223181 -1.5175178 -1.5749147 -2.0143909 -2.7462451 -3.4744925 -3.8056426 -3.550575 -2.9787884 -2.2878647][-3.1636198 -2.9088526 -2.3328061 -1.6032248 -0.39426517 0.60765028 1.2869167 1.3133855 0.32958889 -0.85910964 -1.9738328 -2.6241488 -2.2507026 -1.4434979 -0.75060606][-2.9652023 -2.4469094 -1.680275 -0.56191564 1.1254573 2.78764 3.7707777 3.821434 2.6442509 0.85160732 -0.79425049 -1.7675662 -1.6047785 -0.8163228 0.053793907][-2.7193925 -2.2798994 -1.511204 -0.082496166 1.6709909 3.5387683 4.7385855 4.8294029 3.5436387 1.5145655 -0.39071846 -1.5910685 -1.4020979 -0.5188396 0.33539629][-2.9716158 -2.37056 -1.4427147 -0.40021086 0.98566866 2.5746365 3.5199122 3.5390615 2.5302734 0.73794031 -1.131587 -2.2792325 -2.0802777 -1.1936131 -0.18534994][-3.5997698 -3.4554348 -3.049746 -2.1527386 -0.74180007 0.45672655 0.99068451 1.1203637 0.37424421 -0.88202262 -2.2998543 -3.3907576 -3.2681379 -2.5238576 -1.5566933][-4.8521619 -4.9300566 -4.5958548 -4.1284838 -3.379267 -2.3972557 -1.9140937 -1.8340728 -2.1047218 -2.7859917 -3.7108023 -4.4627309 -4.4738617 -4.049468 -3.2587447][-5.9480624 -6.1671333 -6.1930685 -6.1120944 -5.6049757 -5.0547647 -4.7998548 -4.6417956 -4.548667 -4.6382937 -5.0106287 -5.3579469 -5.2992458 -5.0777035 -4.4434872][-7.1277761 -7.4345722 -7.5105543 -7.7032027 -7.5846872 -7.2865925 -6.9415188 -6.540823 -6.09187 -5.784718 -5.6945944 -5.7005415 -5.5009689 -5.4321284 -5.0216665][-8.29676 -8.4113979 -8.2482176 -8.2198524 -8.0686769 -7.8823147 -7.7416096 -7.3400512 -6.8104625 -6.2505622 -5.8660183 -5.7659945 -5.6082325 -5.6938725 -5.444983][-8.5141764 -8.537796 -8.2728548 -7.9276896 -7.6354685 -7.4270086 -7.17282 -6.8062038 -6.3359652 -5.8446083 -5.547987 -5.3218012 -5.2200418 -5.3462324 -5.2371693]]...]
INFO - root - 2017-12-16 09:46:35.687615: step 76610, loss = 0.21, batch loss = 0.15 (29.2 examples/sec; 0.274 sec/batch; 19h:29m:37s remains)
INFO - root - 2017-12-16 09:46:38.495878: step 76620, loss = 0.23, batch loss = 0.18 (29.5 examples/sec; 0.271 sec/batch; 19h:16m:47s remains)
INFO - root - 2017-12-16 09:46:41.353057: step 76630, loss = 0.26, batch loss = 0.20 (28.8 examples/sec; 0.278 sec/batch; 19h:44m:53s remains)
INFO - root - 2017-12-16 09:46:44.274219: step 76640, loss = 0.24, batch loss = 0.18 (27.1 examples/sec; 0.296 sec/batch; 21h:00m:24s remains)
INFO - root - 2017-12-16 09:46:47.144890: step 76650, loss = 0.27, batch loss = 0.22 (27.6 examples/sec; 0.290 sec/batch; 20h:37m:21s remains)
INFO - root - 2017-12-16 09:46:49.978207: step 76660, loss = 0.34, batch loss = 0.28 (29.0 examples/sec; 0.276 sec/batch; 19h:35m:03s remains)
INFO - root - 2017-12-16 09:46:52.813634: step 76670, loss = 0.23, batch loss = 0.17 (28.6 examples/sec; 0.280 sec/batch; 19h:53m:14s remains)
INFO - root - 2017-12-16 09:46:55.724059: step 76680, loss = 0.24, batch loss = 0.18 (26.6 examples/sec; 0.301 sec/batch; 21h:24m:19s remains)
INFO - root - 2017-12-16 09:46:58.521754: step 76690, loss = 0.24, batch loss = 0.18 (29.5 examples/sec; 0.271 sec/batch; 19h:14m:40s remains)
INFO - root - 2017-12-16 09:47:01.348165: step 76700, loss = 0.29, batch loss = 0.23 (27.5 examples/sec; 0.291 sec/batch; 20h:38m:38s remains)
2017-12-16 09:47:01.870367: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7168589 -6.9843316 -7.0937958 -6.7017269 -6.3577762 -5.9462261 -5.5383224 -5.1045785 -4.8375158 -5.0849576 -5.4683552 -6.1138458 -6.9373484 -7.296772 -7.3144236][-7.4157205 -7.9801931 -8.1852455 -7.7900896 -7.2699432 -6.6628485 -5.957963 -5.3599062 -4.9673572 -5.1998296 -5.7391338 -6.5056372 -7.2981005 -7.7712374 -7.8952861][-7.7197542 -8.21838 -8.2721767 -7.8183193 -7.2877069 -6.4929276 -5.5528073 -4.956214 -4.6338115 -4.6520967 -5.1469774 -6.0430112 -7.0761676 -7.6314383 -7.7224569][-6.81017 -7.29418 -7.2322717 -6.5581994 -5.6495852 -4.7668142 -3.9251609 -3.6076155 -3.388283 -3.4491234 -3.9577961 -4.7037277 -5.5869021 -6.1232934 -6.3485556][-5.3237233 -5.5401287 -5.3400936 -4.701407 -3.7836635 -2.5995176 -1.6556296 -1.2181664 -0.91474152 -1.2247815 -1.8021941 -2.3598361 -3.0050678 -3.781019 -4.50175][-4.1812963 -3.7599249 -2.8743906 -1.7698097 -0.58811736 0.31286 0.88206673 1.1369562 1.1348276 0.64726734 -0.051674843 -0.61951947 -1.3029015 -2.0748956 -2.8426723][-2.9817114 -2.3359883 -1.3072214 0.12174416 1.769033 2.8290448 3.3832545 3.2046428 2.8130779 2.1711493 1.400589 0.5024991 -0.39365244 -1.4605441 -2.4796987][-2.6498914 -1.9377542 -0.74886703 0.78469324 2.3622031 3.6193724 4.3734531 4.2468414 3.6875 2.7125096 1.7419667 0.61761618 -0.51722765 -1.7370477 -2.878089][-3.1350541 -2.4514403 -1.2683899 0.046080112 1.3727889 2.5759444 3.4204569 3.4976573 3.0076818 1.9612589 0.96077108 -0.27436209 -1.4382527 -2.4991729 -3.5216875][-3.4384184 -3.3764608 -2.8408017 -1.8504198 -0.76014614 0.23307705 0.98655605 1.249577 0.9215126 0.085888863 -0.66680431 -1.8226418 -2.9516511 -3.8388772 -4.4468369][-4.8299408 -4.6540618 -4.1161246 -3.7573946 -3.34128 -2.4989321 -1.6751657 -1.313097 -1.4054027 -1.8520164 -2.2575476 -3.14679 -4.0629406 -4.5710125 -4.70268][-6.0033894 -6.0537252 -5.9391541 -5.6016769 -5.2558575 -4.8285079 -4.4531126 -4.035665 -3.7491775 -3.7808547 -3.7923913 -4.133925 -4.5284829 -4.61599 -4.5067296][-7.0007358 -7.1140919 -7.1990523 -7.1163406 -7.1040773 -6.8582988 -6.5178738 -6.2412677 -6.0232196 -5.74539 -5.3537073 -5.2219787 -5.1633282 -4.8474474 -4.4587226][-7.4843073 -7.3484592 -7.3839889 -7.4219122 -7.4569168 -7.3265429 -7.1553249 -7.0442839 -6.8399057 -6.5178843 -6.1311054 -5.8145027 -5.5659642 -5.1473064 -4.6591539][-7.3875227 -6.9792709 -6.7780995 -6.8463793 -6.9490771 -6.9856062 -7.0173016 -6.9345303 -6.7289057 -6.4525919 -6.1836343 -5.8659921 -5.5503783 -5.1985 -4.8057466]]...]
INFO - root - 2017-12-16 09:47:04.747722: step 76710, loss = 0.27, batch loss = 0.21 (27.2 examples/sec; 0.295 sec/batch; 20h:55m:43s remains)
INFO - root - 2017-12-16 09:47:07.609437: step 76720, loss = 0.24, batch loss = 0.18 (28.2 examples/sec; 0.284 sec/batch; 20h:10m:54s remains)
INFO - root - 2017-12-16 09:47:10.505263: step 76730, loss = 0.21, batch loss = 0.16 (27.7 examples/sec; 0.288 sec/batch; 20h:29m:06s remains)
INFO - root - 2017-12-16 09:47:13.396877: step 76740, loss = 0.30, batch loss = 0.24 (28.2 examples/sec; 0.284 sec/batch; 20h:10m:18s remains)
INFO - root - 2017-12-16 09:47:16.206979: step 76750, loss = 0.24, batch loss = 0.18 (29.2 examples/sec; 0.274 sec/batch; 19h:27m:51s remains)
INFO - root - 2017-12-16 09:47:19.084584: step 76760, loss = 0.28, batch loss = 0.22 (26.8 examples/sec; 0.298 sec/batch; 21h:11m:46s remains)
INFO - root - 2017-12-16 09:47:21.951617: step 76770, loss = 0.26, batch loss = 0.20 (25.9 examples/sec; 0.309 sec/batch; 21h:58m:19s remains)
INFO - root - 2017-12-16 09:47:24.830891: step 76780, loss = 0.28, batch loss = 0.22 (27.0 examples/sec; 0.297 sec/batch; 21h:04m:04s remains)
INFO - root - 2017-12-16 09:47:27.686866: step 76790, loss = 0.23, batch loss = 0.17 (28.4 examples/sec; 0.281 sec/batch; 19h:59m:13s remains)
INFO - root - 2017-12-16 09:47:30.521028: step 76800, loss = 0.25, batch loss = 0.20 (27.7 examples/sec; 0.289 sec/batch; 20h:29m:38s remains)
2017-12-16 09:47:31.076535: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3710947 -4.760047 -4.9765291 -5.0414762 -5.0434461 -4.8744345 -4.73287 -4.7868466 -5.0114584 -5.1828194 -5.3471456 -5.4961123 -5.4639258 -5.2174273 -4.8536577][-4.4817324 -4.9114389 -5.0801954 -5.0933805 -5.0298533 -5.0118117 -5.1109624 -5.2121191 -5.3677363 -5.7047048 -6.148777 -6.3875089 -6.3655152 -6.0762668 -5.6551356][-4.5658584 -4.8723435 -4.8095551 -4.6349435 -4.4187851 -4.2406387 -4.2767611 -4.6429367 -5.06144 -5.5148273 -5.9521947 -6.4570556 -6.8943014 -6.8305697 -6.5423803][-4.5290313 -4.6113033 -4.2349539 -3.6619051 -3.1362476 -2.7916591 -2.6745729 -2.8640037 -3.2470703 -4.1707058 -5.1049662 -5.9653044 -6.667922 -7.1720114 -7.3869243][-4.37496 -3.9867232 -3.1583457 -2.233947 -1.3082688 -0.60069895 -0.18902349 -0.22237253 -0.590863 -1.511132 -2.7240644 -4.4237933 -5.8703704 -6.94203 -7.5365014][-4.3765316 -3.6089211 -2.3030887 -0.99152374 0.46846962 1.5581632 2.3227997 2.6147842 2.3751664 1.3709812 -0.10965824 -2.3411262 -4.5136847 -6.3138647 -7.2406197][-4.6613669 -3.6897078 -2.1418331 -0.47517323 1.3792658 3.0011125 4.2131662 4.6995993 4.46393 3.3381476 1.608664 -0.91285658 -3.4199095 -5.6417375 -6.7918706][-5.1825809 -4.0284 -2.3988624 -0.52917075 1.4877038 3.3850522 4.9842567 5.5057726 5.328125 3.9739094 2.1273661 -0.53194261 -3.159883 -5.2789006 -6.2616968][-5.8628068 -4.8954635 -3.3613975 -1.4309704 0.44151163 2.2830563 3.948514 4.56777 4.4256344 3.1550074 1.372942 -1.2281427 -3.7627225 -5.5170522 -6.2592878][-6.5268793 -5.7033463 -4.4958019 -2.9840331 -1.4728787 0.314034 1.8108549 2.4311342 2.3450437 1.2826462 -0.26654291 -2.5977201 -4.7460833 -6.3187661 -6.9983244][-6.4327936 -6.0571642 -5.2766466 -3.9556565 -2.7550211 -1.3854489 -0.20292377 0.3582468 0.038917065 -0.95183516 -2.1129274 -3.94916 -5.5624676 -6.7692137 -7.2402649][-6.1771164 -5.6497679 -5.0550394 -4.3284259 -3.6574597 -2.6430726 -1.8403933 -1.6010573 -1.8293602 -2.4745808 -3.5365431 -5.1725335 -6.3824215 -7.0303779 -7.3221235][-5.2370954 -4.9219337 -4.6183119 -4.1590905 -3.8230345 -3.2860539 -2.8752503 -2.6955304 -2.7763898 -3.307375 -4.0997677 -5.1674805 -6.1075363 -6.8803806 -7.1931229][-3.9364939 -3.8348513 -3.7918754 -3.6604674 -3.5186129 -3.1213675 -2.896944 -2.8955326 -3.1173587 -3.3927364 -3.7676029 -4.5652542 -5.3112955 -5.72256 -5.798542][-3.1882768 -2.8215592 -2.5667727 -2.6467264 -2.8310435 -2.7372251 -2.7301416 -2.7404108 -2.8100023 -3.0363703 -3.3897443 -3.7207546 -3.8575435 -4.0779066 -4.2297425]]...]
INFO - root - 2017-12-16 09:47:33.949808: step 76810, loss = 0.20, batch loss = 0.15 (26.7 examples/sec; 0.299 sec/batch; 21h:14m:40s remains)
INFO - root - 2017-12-16 09:47:36.883133: step 76820, loss = 0.23, batch loss = 0.17 (27.3 examples/sec; 0.293 sec/batch; 20h:50m:33s remains)
