INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "178"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0005-clip20
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-10 03:52:37.670937: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 03:52:37.670976: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 03:52:37.670983: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 03:52:37.670987: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 03:52:37.670991: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 03:52:38.010562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-10 03:52:38.010600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-10 03:52:38.010607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-10 03:52:38.010614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-10 03:52:41.000274: step 0, loss = 2.28, batch loss = 2.23 (3.6 examples/sec; 2.244 sec/batch; 207h:15m:51s remains)
2017-12-10 03:52:41.364615: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290175 -4.4290109 -4.4290047 -4.4289894 -4.4289651 -4.4289336 -4.4288964 -4.42886 -4.4288483 -4.4288645 -4.4289012 -4.4289408 -4.4289789 -4.4290075 -4.4290261][-4.4290133 -4.428997 -4.4289765 -4.4289441 -4.4289026 -4.4288526 -4.4287992 -4.4287539 -4.42875 -4.4287834 -4.4288392 -4.4288955 -4.4289455 -4.4289856 -4.4290161][-4.4290042 -4.4289789 -4.4289436 -4.4288864 -4.4288168 -4.4287367 -4.4286551 -4.42859 -4.4285913 -4.42865 -4.4287362 -4.4288235 -4.428895 -4.4289532 -4.4289994][-4.428988 -4.4289618 -4.428916 -4.42883 -4.4287243 -4.4286027 -4.4284725 -4.4283671 -4.4283738 -4.4284725 -4.4286065 -4.4287395 -4.428843 -4.4289212 -4.4289818][-4.4289627 -4.4289446 -4.4288893 -4.4287715 -4.4286208 -4.4284434 -4.4282417 -4.428081 -4.4281077 -4.4282727 -4.4284759 -4.4286671 -4.4288063 -4.4289031 -4.4289718][-4.4289308 -4.4289155 -4.4288454 -4.4286947 -4.4284964 -4.42825 -4.4279532 -4.4277377 -4.4278288 -4.4280934 -4.4283738 -4.4286165 -4.4287844 -4.4288969 -4.4289708][-4.4288888 -4.4288678 -4.4287839 -4.4286151 -4.4283857 -4.4280844 -4.4277182 -4.4275064 -4.4277129 -4.4280634 -4.4283781 -4.428628 -4.4287963 -4.4289093 -4.4289789][-4.4288416 -4.4288054 -4.4287157 -4.4285564 -4.4283423 -4.4280586 -4.42774 -4.4276361 -4.4279051 -4.4282355 -4.4285011 -4.4287033 -4.4288392 -4.4289346 -4.4289918][-4.4287963 -4.42875 -4.4286709 -4.4285502 -4.4283972 -4.4282007 -4.4280076 -4.4280062 -4.4282341 -4.4284735 -4.4286513 -4.4287868 -4.428885 -4.4289579 -4.4290032][-4.428762 -4.4287157 -4.4286537 -4.4285741 -4.4284897 -4.4283891 -4.428298 -4.4283366 -4.4284921 -4.4286447 -4.4287481 -4.428834 -4.4289083 -4.4289684 -4.4290075][-4.4287462 -4.4287043 -4.4286594 -4.4286127 -4.4285803 -4.428545 -4.4285121 -4.4285536 -4.428647 -4.4287376 -4.4287977 -4.4288592 -4.4289227 -4.4289761 -4.4290094][-4.4287572 -4.4287276 -4.4287004 -4.428679 -4.4286766 -4.4286752 -4.4286675 -4.4286952 -4.4287395 -4.4287915 -4.4288306 -4.4288855 -4.4289455 -4.4289894 -4.4290147][-4.4287925 -4.4287791 -4.4287686 -4.4287653 -4.4287748 -4.4287796 -4.4287705 -4.4287739 -4.4287753 -4.4288015 -4.4288368 -4.4288964 -4.4289584 -4.4289942 -4.4290142][-4.4288282 -4.4288287 -4.4288306 -4.4288335 -4.4288354 -4.4288263 -4.4288006 -4.4287753 -4.42875 -4.4287667 -4.4288077 -4.4288807 -4.42895 -4.4289856 -4.4290075][-4.4288392 -4.4288478 -4.42886 -4.4288669 -4.4288554 -4.428823 -4.4287729 -4.4287267 -4.4286942 -4.4287148 -4.4287667 -4.4288535 -4.4289293 -4.4289718 -4.4290004]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0005-clip20/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0005-clip20/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-10 03:52:43.853582: step 10, loss = 2.28, batch loss = 2.23 (39.8 examples/sec; 0.201 sec/batch; 18h:33m:24s remains)
INFO - root - 2017-12-10 03:52:45.826017: step 20, loss = 2.28, batch loss = 2.23 (40.6 examples/sec; 0.197 sec/batch; 18h:10m:33s remains)
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>]
INFO - root - 2017-12-10 03:52:47.826719: step 30, loss = 2.28, batch loss = 2.23 (39.6 examples/sec; 0.202 sec/batch; 18h:40m:48s remains)
INFO - root - 2017-12-10 03:52:49.816165: step 40, loss = 2.28, batch loss = 2.23 (40.9 examples/sec; 0.196 sec/batch; 18h:04m:31s remains)
INFO - root - 2017-12-10 03:52:51.816732: step 50, loss = 2.28, batch loss = 2.23 (40.4 examples/sec; 0.198 sec/batch; 18h:16m:20s remains)
INFO - root - 2017-12-10 03:52:53.825502: step 60, loss = 2.28, batch loss = 2.23 (39.6 examples/sec; 0.202 sec/batch; 18h:39m:44s remains)
INFO - root - 2017-12-10 03:52:55.825635: step 70, loss = 2.28, batch loss = 2.23 (39.9 examples/sec; 0.201 sec/batch; 18h:31m:24s remains)
INFO - root - 2017-12-10 03:52:57.864806: step 80, loss = 2.28, batch loss = 2.23 (41.0 examples/sec; 0.195 sec/batch; 18h:00m:54s remains)
INFO - root - 2017-12-10 03:52:59.861091: step 90, loss = 2.28, batch loss = 2.23 (38.9 examples/sec; 0.206 sec/batch; 18h:59m:28s remains)
INFO - root - 2017-12-10 03:53:01.858709: step 100, loss = 2.28, batch loss = 2.23 (40.2 examples/sec; 0.199 sec/batch; 18h:21m:49s remains)
2017-12-10 03:53:02.152612: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288373 -4.4287753 -4.4287086 -4.4286723 -4.4286838 -4.4287205 -4.4287567 -4.4288187 -4.4288754 -4.4288869 -4.4288759 -4.42886 -4.428823 -4.4287739 -4.4287529][-4.428844 -4.4287682 -4.4286771 -4.4286165 -4.428618 -4.4286585 -4.428699 -4.4287624 -4.4288163 -4.4288325 -4.4288316 -4.4288292 -4.4288044 -4.4287634 -4.428751][-4.4288425 -4.4287581 -4.4286585 -4.4285884 -4.4285822 -4.4286175 -4.4286575 -4.4287148 -4.4287686 -4.42879 -4.4287977 -4.428803 -4.4287882 -4.4287548 -4.4287453][-4.4288354 -4.4287472 -4.4286513 -4.4285831 -4.4285688 -4.4285922 -4.4286256 -4.4286771 -4.4287376 -4.4287705 -4.4287839 -4.428791 -4.4287806 -4.42875 -4.4287372][-4.4288297 -4.4287419 -4.4286513 -4.4285855 -4.4285612 -4.42857 -4.4285903 -4.4286337 -4.428699 -4.4287453 -4.4287634 -4.42877 -4.4287648 -4.4287424 -4.4287305][-4.4288239 -4.4287391 -4.4286551 -4.428587 -4.4285488 -4.4285417 -4.4285431 -4.4285669 -4.4286289 -4.4286871 -4.4287138 -4.4287252 -4.428731 -4.4287262 -4.4287224][-4.4288211 -4.4287372 -4.4286537 -4.4285779 -4.4285245 -4.4285049 -4.4284897 -4.4284906 -4.4285417 -4.4286118 -4.42865 -4.4286685 -4.4286852 -4.4286966 -4.4287033][-4.4288211 -4.4287386 -4.4286561 -4.4285789 -4.4285231 -4.4284997 -4.4284782 -4.4284649 -4.428503 -4.4285755 -4.42862 -4.4286437 -4.4286675 -4.42869 -4.4287019][-4.4288254 -4.4287477 -4.4286695 -4.4285989 -4.42855 -4.42853 -4.4285131 -4.4285026 -4.4285374 -4.4286 -4.4286323 -4.4286461 -4.4286661 -4.4286885 -4.4286981][-4.4288287 -4.4287524 -4.4286733 -4.4286065 -4.4285641 -4.4285488 -4.4285417 -4.428544 -4.4285817 -4.4286337 -4.4286528 -4.4286523 -4.4286613 -4.4286771 -4.4286809][-4.4288383 -4.4287629 -4.4286842 -4.4286194 -4.4285817 -4.4285684 -4.4285669 -4.4285765 -4.4286122 -4.4286518 -4.4286633 -4.4286556 -4.4286566 -4.4286675 -4.4286728][-4.4288564 -4.4287848 -4.4287114 -4.4286532 -4.4286222 -4.4286118 -4.4286184 -4.4286332 -4.4286637 -4.42869 -4.4286919 -4.4286804 -4.428678 -4.4286847 -4.4286895][-4.4288845 -4.428822 -4.4287581 -4.4287071 -4.4286809 -4.4286776 -4.4286947 -4.42872 -4.4287448 -4.4287567 -4.428751 -4.4287381 -4.4287314 -4.4287333 -4.4287343][-4.428925 -4.4288783 -4.4288306 -4.4287882 -4.4287658 -4.4287677 -4.428791 -4.42882 -4.42884 -4.4288445 -4.4288359 -4.4288225 -4.428813 -4.4288116 -4.4288106][-4.4289641 -4.4289355 -4.4289083 -4.4288821 -4.4288654 -4.4288669 -4.4288855 -4.4289079 -4.4289203 -4.4289203 -4.4289136 -4.4289026 -4.428895 -4.4288931 -4.428895]]...]
INFO - root - 2017-12-10 03:53:04.178201: step 110, loss = 2.28, batch loss = 2.23 (40.2 examples/sec; 0.199 sec/batch; 18h:21m:09s remains)
INFO - root - 2017-12-10 03:53:06.186946: step 120, loss = 2.28, batch loss = 2.23 (40.3 examples/sec; 0.198 sec/batch; 18h:19m:05s remains)
INFO - root - 2017-12-10 03:53:08.209726: step 130, loss = 2.28, batch loss = 2.23 (39.1 examples/sec; 0.205 sec/batch; 18h:53m:50s remains)
INFO - root - 2017-12-10 03:53:10.227154: step 140, loss = 2.28, batch loss = 2.23 (39.0 examples/sec; 0.205 sec/batch; 18h:56m:38s remains)
INFO - root - 2017-12-10 03:53:12.233113: step 150, loss = 2.28, batch loss = 2.23 (39.1 examples/sec; 0.205 sec/batch; 18h:52m:59s remains)
INFO - root - 2017-12-10 03:53:14.252612: step 160, loss = 2.28, batch loss = 2.23 (39.5 examples/sec; 0.202 sec/batch; 18h:41m:17s remains)
INFO - root - 2017-12-10 03:53:16.268262: step 170, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.202 sec/batch; 18h:36m:59s remains)
INFO - root - 2017-12-10 03:53:18.268915: step 180, loss = 2.28, batch loss = 2.23 (40.7 examples/sec; 0.196 sec/batch; 18h:08m:09s remains)
INFO - root - 2017-12-10 03:53:20.281430: step 190, loss = 2.28, batch loss = 2.23 (40.2 examples/sec; 0.199 sec/batch; 18h:22m:55s remains)
INFO - root - 2017-12-10 03:53:22.315366: step 200, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:48m:55s remains)
2017-12-10 03:53:22.664561: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288073 -4.4287853 -4.4287839 -4.4287863 -4.428762 -4.4287524 -4.42876 -4.4287624 -4.428762 -4.4287667 -4.4287763 -4.428791 -4.4288011 -4.4288177 -4.4288182][-4.428803 -4.428771 -4.4287615 -4.4287596 -4.4287395 -4.4287291 -4.4287357 -4.4287286 -4.4287205 -4.428731 -4.4287505 -4.4287696 -4.428782 -4.428792 -4.4287934][-4.4287891 -4.42877 -4.4287624 -4.4287577 -4.4287477 -4.4287386 -4.4287376 -4.4287295 -4.4287238 -4.4287395 -4.4287567 -4.4287744 -4.4287853 -4.42878 -4.4287758][-4.42876 -4.4287553 -4.4287581 -4.4287548 -4.4287534 -4.4287367 -4.4287257 -4.4287343 -4.4287462 -4.4287677 -4.4287763 -4.4287825 -4.4287772 -4.4287558 -4.4287405][-4.4287672 -4.4287572 -4.42876 -4.4287491 -4.428741 -4.4287105 -4.4286828 -4.4287057 -4.4287481 -4.4287882 -4.4287977 -4.428791 -4.4287615 -4.4287314 -4.4287171][-4.4287505 -4.4287295 -4.4287434 -4.428731 -4.4287128 -4.4286551 -4.4285893 -4.4286213 -4.4287143 -4.4287863 -4.4288054 -4.428792 -4.4287515 -4.4287186 -4.4287152][-4.4287128 -4.4286866 -4.428709 -4.4286976 -4.4286571 -4.428544 -4.4284072 -4.4284482 -4.4286194 -4.4287438 -4.4287806 -4.42876 -4.4287114 -4.4286866 -4.4287028][-4.4286623 -4.4286284 -4.4286518 -4.428647 -4.4285779 -4.4283948 -4.4281645 -4.4282079 -4.4284754 -4.4286714 -4.4287333 -4.4286933 -4.4286342 -4.42863 -4.4286766][-4.4286227 -4.4285951 -4.4286289 -4.4286361 -4.4285688 -4.4283853 -4.4281554 -4.4281793 -4.4284439 -4.4286556 -4.4287343 -4.4287057 -4.4286461 -4.4286351 -4.4286728][-4.4286542 -4.4286284 -4.428659 -4.428679 -4.4286356 -4.4285269 -4.4283886 -4.4283891 -4.4285469 -4.4287004 -4.4287739 -4.4287691 -4.4287295 -4.4287105 -4.4287171][-4.4287152 -4.4286804 -4.4286947 -4.4287124 -4.4286928 -4.4286413 -4.4285731 -4.428565 -4.4286456 -4.4287419 -4.4287972 -4.4288049 -4.4287739 -4.42875 -4.4287438][-4.4287367 -4.4287076 -4.4287262 -4.4287462 -4.4287424 -4.4287167 -4.4286766 -4.4286628 -4.4287024 -4.4287605 -4.4287987 -4.4288063 -4.4287858 -4.42876 -4.4287539][-4.4287415 -4.4287271 -4.4287519 -4.4287667 -4.4287715 -4.4287548 -4.428731 -4.4287205 -4.4287391 -4.4287758 -4.4288025 -4.4288082 -4.4287996 -4.4287868 -4.4287877][-4.4287567 -4.4287548 -4.4287815 -4.4287872 -4.4287949 -4.4287872 -4.4287748 -4.4287724 -4.4287772 -4.4287939 -4.4288125 -4.4288225 -4.4288287 -4.4288278 -4.428834][-4.4287858 -4.4287934 -4.42882 -4.4288259 -4.4288321 -4.4288335 -4.4288254 -4.4288182 -4.4288144 -4.4288182 -4.4288349 -4.4288516 -4.4288611 -4.428863 -4.4288659]]...]
INFO - root - 2017-12-10 03:53:24.662915: step 210, loss = 2.28, batch loss = 2.23 (40.2 examples/sec; 0.199 sec/batch; 18h:21m:09s remains)
INFO - root - 2017-12-10 03:53:26.663957: step 220, loss = 2.28, batch loss = 2.23 (40.3 examples/sec; 0.198 sec/batch; 18h:18m:33s remains)
INFO - root - 2017-12-10 03:53:28.685457: step 230, loss = 2.28, batch loss = 2.23 (40.1 examples/sec; 0.200 sec/batch; 18h:26m:04s remains)
INFO - root - 2017-12-10 03:53:30.727070: step 240, loss = 2.28, batch loss = 2.23 (40.5 examples/sec; 0.197 sec/batch; 18h:12m:33s remains)
INFO - root - 2017-12-10 03:53:32.752015: step 250, loss = 2.28, batch loss = 2.23 (40.4 examples/sec; 0.198 sec/batch; 18h:17m:40s remains)
INFO - root - 2017-12-10 03:53:34.781661: step 260, loss = 2.28, batch loss = 2.23 (40.5 examples/sec; 0.197 sec/batch; 18h:13m:37s remains)
INFO - root - 2017-12-10 03:53:36.808616: step 270, loss = 2.28, batch loss = 2.23 (38.5 examples/sec; 0.208 sec/batch; 19h:10m:51s remains)
INFO - root - 2017-12-10 03:53:38.849434: step 280, loss = 2.28, batch loss = 2.23 (38.2 examples/sec; 0.209 sec/batch; 19h:19m:01s remains)
INFO - root - 2017-12-10 03:53:40.858668: step 290, loss = 2.28, batch loss = 2.23 (39.8 examples/sec; 0.201 sec/batch; 18h:33m:34s remains)
INFO - root - 2017-12-10 03:53:42.873212: step 300, loss = 2.28, batch loss = 2.23 (39.5 examples/sec; 0.202 sec/batch; 18h:40m:35s remains)
2017-12-10 03:53:43.181471: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288254 -4.4287615 -4.4286966 -4.4286356 -4.4285865 -4.4285555 -4.42855 -4.42857 -4.4286132 -4.4286652 -4.4287167 -4.4287539 -4.4287577 -4.4287319 -4.4287095][-4.4288273 -4.4287744 -4.4287224 -4.4286757 -4.4286413 -4.4286289 -4.4286423 -4.4286714 -4.4287119 -4.4287548 -4.4287896 -4.4288049 -4.4287939 -4.4287591 -4.4287319][-4.428802 -4.4287682 -4.4287281 -4.4286909 -4.4286704 -4.4286776 -4.4287086 -4.4287438 -4.4287806 -4.4288173 -4.4288425 -4.428843 -4.4288216 -4.4287858 -4.4287596][-4.4287224 -4.4287014 -4.4286728 -4.4286456 -4.4286342 -4.4286542 -4.4286947 -4.4287319 -4.4287658 -4.4287915 -4.4288058 -4.4287992 -4.4287825 -4.4287591 -4.4287443][-4.4286194 -4.4285955 -4.4285688 -4.4285383 -4.4285164 -4.4285288 -4.4285712 -4.428618 -4.4286561 -4.4286723 -4.4286785 -4.4286776 -4.4286685 -4.4286609 -4.4286675][-4.4284825 -4.428442 -4.428411 -4.4283671 -4.4283156 -4.4283061 -4.4283447 -4.4284096 -4.4284678 -4.4284992 -4.4285159 -4.4285326 -4.4285412 -4.4285483 -4.4285736][-4.4283605 -4.4282928 -4.428246 -4.4281783 -4.4280906 -4.4280505 -4.42807 -4.4281397 -4.4282174 -4.4282684 -4.4283042 -4.4283423 -4.4283752 -4.4284029 -4.428452][-4.4283724 -4.4283037 -4.4282589 -4.4282031 -4.42812 -4.4280577 -4.4280348 -4.4280686 -4.4281278 -4.4281635 -4.4281874 -4.4282136 -4.4282384 -4.4282713 -4.4283271][-4.428442 -4.4284058 -4.4283929 -4.4283795 -4.4283442 -4.4282994 -4.428268 -4.4282737 -4.4283023 -4.4283085 -4.4283037 -4.4283018 -4.4283023 -4.428318 -4.4283543][-4.4285097 -4.4285135 -4.4285388 -4.4285612 -4.4285545 -4.4285274 -4.4284968 -4.4284859 -4.428493 -4.4284921 -4.4284859 -4.4284821 -4.4284787 -4.4284844 -4.4284997][-4.4286237 -4.428638 -4.4286771 -4.4287133 -4.4287229 -4.4287076 -4.4286842 -4.4286714 -4.4286742 -4.4286785 -4.4286795 -4.4286785 -4.4286718 -4.4286642 -4.4286561][-4.4287481 -4.4287558 -4.4287806 -4.4288054 -4.4288125 -4.4288039 -4.4287896 -4.4287815 -4.4287896 -4.4288015 -4.4288111 -4.4288125 -4.4288015 -4.4287844 -4.42876][-4.428803 -4.4288058 -4.4288197 -4.4288344 -4.4288406 -4.4288363 -4.4288292 -4.4288287 -4.4288416 -4.4288568 -4.4288683 -4.4288688 -4.4288573 -4.4288363 -4.4288092][-4.42882 -4.4288197 -4.4288249 -4.4288292 -4.4288297 -4.4288278 -4.4288268 -4.4288335 -4.4288492 -4.4288654 -4.4288764 -4.428875 -4.4288645 -4.4288483 -4.4288273][-4.4288416 -4.4288392 -4.4288392 -4.4288378 -4.4288349 -4.4288325 -4.428833 -4.4288397 -4.4288516 -4.428864 -4.4288721 -4.4288716 -4.4288645 -4.428853 -4.4288387]]...]
INFO - root - 2017-12-10 03:53:45.204066: step 310, loss = 2.28, batch loss = 2.23 (39.6 examples/sec; 0.202 sec/batch; 18h:39m:41s remains)
INFO - root - 2017-12-10 03:53:47.247249: step 320, loss = 2.28, batch loss = 2.23 (39.0 examples/sec; 0.205 sec/batch; 18h:54m:50s remains)
INFO - root - 2017-12-10 03:53:49.282099: step 330, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:48m:42s remains)
INFO - root - 2017-12-10 03:53:51.316882: step 340, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.201 sec/batch; 18h:35m:23s remains)
INFO - root - 2017-12-10 03:53:53.342161: step 350, loss = 2.28, batch loss = 2.23 (40.8 examples/sec; 0.196 sec/batch; 18h:04m:48s remains)
INFO - root - 2017-12-10 03:53:55.392048: step 360, loss = 2.28, batch loss = 2.23 (40.1 examples/sec; 0.200 sec/batch; 18h:25m:10s remains)
INFO - root - 2017-12-10 03:53:57.452997: step 370, loss = 2.28, batch loss = 2.23 (39.3 examples/sec; 0.204 sec/batch; 18h:46m:42s remains)
INFO - root - 2017-12-10 03:53:59.481113: step 380, loss = 2.28, batch loss = 2.23 (40.2 examples/sec; 0.199 sec/batch; 18h:22m:26s remains)
INFO - root - 2017-12-10 03:54:01.510026: step 390, loss = 2.28, batch loss = 2.23 (40.6 examples/sec; 0.197 sec/batch; 18h:10m:55s remains)
INFO - root - 2017-12-10 03:54:03.542156: step 400, loss = 2.28, batch loss = 2.23 (40.8 examples/sec; 0.196 sec/batch; 18h:05m:50s remains)
2017-12-10 03:54:03.820948: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288511 -4.4288363 -4.4288378 -4.4288611 -4.4289026 -4.4289188 -4.4289 -4.42887 -4.4288425 -4.4288039 -4.4287744 -4.4287558 -4.4287667 -4.42879 -4.4288158][-4.4288421 -4.42882 -4.4288163 -4.4288363 -4.4288831 -4.4289 -4.4288774 -4.4288464 -4.4288154 -4.4287705 -4.4287305 -4.4286976 -4.4286981 -4.4287167 -4.4287539][-4.4288077 -4.4287672 -4.4287505 -4.4287648 -4.4288144 -4.4288325 -4.4288125 -4.4287858 -4.4287615 -4.4287281 -4.4286938 -4.4286547 -4.4286528 -4.4286675 -4.4287138][-4.4287605 -4.42869 -4.4286461 -4.4286447 -4.4286904 -4.4287124 -4.4287052 -4.4286919 -4.4286828 -4.4286723 -4.4286547 -4.428628 -4.4286366 -4.4286547 -4.4287043][-4.4287677 -4.428678 -4.4286013 -4.4285622 -4.4285808 -4.4285865 -4.4285846 -4.4285908 -4.4286075 -4.4286227 -4.4286237 -4.4286184 -4.4286413 -4.4286652 -4.4287128][-4.4288154 -4.4287438 -4.4286652 -4.4286056 -4.4285936 -4.428566 -4.4285312 -4.4285169 -4.4285393 -4.4285693 -4.428586 -4.4286041 -4.4286447 -4.4286761 -4.4287252][-4.4288259 -4.4287839 -4.4287372 -4.4287047 -4.4287057 -4.4286747 -4.4286051 -4.4285426 -4.4285278 -4.4285374 -4.4285445 -4.4285722 -4.4286304 -4.4286776 -4.4287348][-4.4287519 -4.4287167 -4.4287004 -4.4287186 -4.4287748 -4.4287853 -4.4287262 -4.4286475 -4.4285975 -4.4285636 -4.4285421 -4.4285631 -4.4286251 -4.4286804 -4.4287443][-4.4286389 -4.4285917 -4.4285932 -4.4286489 -4.4287543 -4.4288149 -4.4287944 -4.42874 -4.4286895 -4.4286342 -4.4285913 -4.4285979 -4.42865 -4.4287033 -4.4287624][-4.42858 -4.4285164 -4.4285164 -4.4285851 -4.4287148 -4.4288039 -4.4288163 -4.428791 -4.4287562 -4.4287095 -4.428669 -4.4286695 -4.4287066 -4.4287496 -4.4287949][-4.42861 -4.4285316 -4.428515 -4.4285707 -4.4286976 -4.428792 -4.42882 -4.4288154 -4.4287963 -4.4287663 -4.4287405 -4.4287462 -4.4287744 -4.4288034 -4.4288321][-4.4287024 -4.4286261 -4.4285941 -4.4286208 -4.4287109 -4.4287853 -4.4288125 -4.4288197 -4.4288163 -4.4288044 -4.4287944 -4.4288054 -4.4288311 -4.4288511 -4.42887][-4.4288006 -4.4287457 -4.4287162 -4.4287186 -4.4287677 -4.4288111 -4.4288306 -4.4288425 -4.4288487 -4.4288473 -4.4288435 -4.4288526 -4.4288716 -4.4288878 -4.4289036][-4.428874 -4.4288406 -4.4288216 -4.4288182 -4.428844 -4.4288645 -4.428875 -4.4288845 -4.4288926 -4.428895 -4.4288912 -4.4288926 -4.4289026 -4.4289145 -4.4289284][-4.4289203 -4.4289021 -4.4288917 -4.4288898 -4.4289036 -4.4289122 -4.4289174 -4.4289241 -4.4289303 -4.4289341 -4.4289327 -4.4289308 -4.4289331 -4.4289393 -4.42895]]...]
INFO - root - 2017-12-10 03:54:05.856107: step 410, loss = 2.28, batch loss = 2.23 (39.0 examples/sec; 0.205 sec/batch; 18h:54m:43s remains)
INFO - root - 2017-12-10 03:54:07.894151: step 420, loss = 2.28, batch loss = 2.23 (39.4 examples/sec; 0.203 sec/batch; 18h:45m:11s remains)
INFO - root - 2017-12-10 03:54:09.928484: step 430, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.201 sec/batch; 18h:33m:53s remains)
INFO - root - 2017-12-10 03:54:11.952853: step 440, loss = 2.28, batch loss = 2.23 (40.1 examples/sec; 0.200 sec/batch; 18h:24m:43s remains)
INFO - root - 2017-12-10 03:54:13.975350: step 450, loss = 2.28, batch loss = 2.23 (39.6 examples/sec; 0.202 sec/batch; 18h:36m:40s remains)
INFO - root - 2017-12-10 03:54:16.001418: step 460, loss = 2.28, batch loss = 2.23 (40.0 examples/sec; 0.200 sec/batch; 18h:27m:02s remains)
INFO - root - 2017-12-10 03:54:18.044170: step 470, loss = 2.28, batch loss = 2.23 (40.5 examples/sec; 0.197 sec/batch; 18h:11m:50s remains)
INFO - root - 2017-12-10 03:54:20.086271: step 480, loss = 2.28, batch loss = 2.23 (40.4 examples/sec; 0.198 sec/batch; 18h:16m:48s remains)
INFO - root - 2017-12-10 03:54:22.134479: step 490, loss = 2.28, batch loss = 2.23 (39.5 examples/sec; 0.203 sec/batch; 18h:41m:42s remains)
INFO - root - 2017-12-10 03:54:24.182430: step 500, loss = 2.28, batch loss = 2.23 (39.3 examples/sec; 0.204 sec/batch; 18h:47m:33s remains)
2017-12-10 03:54:24.459414: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287353 -4.4287548 -4.4288054 -4.4288464 -4.4288907 -4.4289227 -4.4289222 -4.4289026 -4.428875 -4.4288459 -4.4288383 -4.428853 -4.4288712 -4.4288836 -4.4288712][-4.4286594 -4.4286962 -4.4287753 -4.4288411 -4.4289069 -4.4289494 -4.4289412 -4.4289026 -4.4288535 -4.4288111 -4.4288034 -4.4288259 -4.4288492 -4.4288659 -4.4288535][-4.4286122 -4.4286675 -4.4287724 -4.4288669 -4.4289427 -4.4289808 -4.4289622 -4.4289012 -4.42883 -4.4287782 -4.4287734 -4.4288025 -4.42883 -4.42885 -4.4288487][-4.4285941 -4.42867 -4.4287944 -4.4289031 -4.4289751 -4.4289985 -4.4289656 -4.4288812 -4.4287906 -4.4287319 -4.4287338 -4.4287806 -4.4288225 -4.4288535 -4.4288692][-4.4286284 -4.4287138 -4.4288282 -4.4289231 -4.42898 -4.428988 -4.4289327 -4.4288216 -4.4287143 -4.4286566 -4.4286795 -4.4287586 -4.4288225 -4.42887 -4.428905][-4.4287109 -4.428781 -4.4288611 -4.4289241 -4.4289556 -4.4289412 -4.4288492 -4.4287057 -4.4285884 -4.4285626 -4.4286284 -4.4287376 -4.428812 -4.4288688 -4.428915][-4.4288158 -4.4288521 -4.4288945 -4.4289203 -4.4289155 -4.4288597 -4.4287157 -4.4285192 -4.4284062 -4.4284492 -4.4285755 -4.4287124 -4.428791 -4.4288459 -4.4288874][-4.4288793 -4.4288874 -4.4289017 -4.4288969 -4.4288526 -4.4287491 -4.4285436 -4.4282794 -4.4281988 -4.4283414 -4.4285283 -4.4286871 -4.4287724 -4.4288235 -4.4288511][-4.4288692 -4.428864 -4.4288673 -4.4288473 -4.4287796 -4.4286432 -4.4284077 -4.4281325 -4.4281211 -4.4283266 -4.4285312 -4.4286842 -4.4287705 -4.4288177 -4.4288325][-4.4288244 -4.4288087 -4.4288058 -4.4287882 -4.4287205 -4.4285865 -4.4283895 -4.4282026 -4.4282479 -4.4284329 -4.4286022 -4.4287224 -4.4287882 -4.4288235 -4.4288292][-4.4287753 -4.4287534 -4.4287419 -4.42873 -4.4286718 -4.4285645 -4.4284434 -4.42836 -4.4284282 -4.4285641 -4.428688 -4.4287753 -4.4288192 -4.4288416 -4.4288421][-4.4287181 -4.4286923 -4.4286747 -4.4286757 -4.4286394 -4.4285779 -4.4285264 -4.428503 -4.4285688 -4.4286613 -4.4287491 -4.4288111 -4.4288445 -4.4288635 -4.4288664][-4.4286747 -4.4286556 -4.4286432 -4.4286566 -4.4286485 -4.4286289 -4.4286194 -4.4286218 -4.4286704 -4.42873 -4.4287891 -4.428833 -4.4288607 -4.4288793 -4.4288864][-4.4287019 -4.4286928 -4.4286895 -4.42871 -4.42872 -4.4287233 -4.4287314 -4.4287381 -4.4287663 -4.428803 -4.4288425 -4.4288712 -4.4288907 -4.4289064 -4.428915][-4.4288015 -4.4287958 -4.4287949 -4.4288125 -4.4288211 -4.4288278 -4.4288368 -4.4288406 -4.4288578 -4.4288826 -4.4289074 -4.4289236 -4.4289331 -4.4289417 -4.4289484]]...]
INFO - root - 2017-12-10 03:54:26.489619: step 510, loss = 2.28, batch loss = 2.23 (39.5 examples/sec; 0.202 sec/batch; 18h:40m:25s remains)
INFO - root - 2017-12-10 03:54:28.535368: step 520, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.211 sec/batch; 19h:25m:26s remains)
INFO - root - 2017-12-10 03:54:30.573695: step 530, loss = 2.28, batch loss = 2.23 (38.4 examples/sec; 0.208 sec/batch; 19h:11m:32s remains)
INFO - root - 2017-12-10 03:54:32.601035: step 540, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.201 sec/batch; 18h:34m:36s remains)
INFO - root - 2017-12-10 03:54:34.627961: step 550, loss = 2.28, batch loss = 2.23 (39.3 examples/sec; 0.203 sec/batch; 18h:45m:16s remains)
INFO - root - 2017-12-10 03:54:36.681895: step 560, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:48m:34s remains)
INFO - root - 2017-12-10 03:54:38.739851: step 570, loss = 2.28, batch loss = 2.23 (38.6 examples/sec; 0.207 sec/batch; 19h:07m:17s remains)
INFO - root - 2017-12-10 03:54:40.792038: step 580, loss = 2.28, batch loss = 2.23 (39.3 examples/sec; 0.204 sec/batch; 18h:47m:04s remains)
INFO - root - 2017-12-10 03:54:42.839034: step 590, loss = 2.28, batch loss = 2.23 (40.1 examples/sec; 0.199 sec/batch; 18h:23m:29s remains)
INFO - root - 2017-12-10 03:54:44.873891: step 600, loss = 2.28, batch loss = 2.23 (38.8 examples/sec; 0.206 sec/batch; 19h:00m:03s remains)
2017-12-10 03:54:45.174140: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288421 -4.4288139 -4.4287925 -4.4287767 -4.4287696 -4.4287639 -4.4287949 -4.4288597 -4.4289179 -4.4289422 -4.4289508 -4.42893 -4.428875 -4.4288354 -4.4288454][-4.4288187 -4.4288015 -4.4287906 -4.4287791 -4.4287663 -4.4287505 -4.4287696 -4.4288268 -4.4288874 -4.4289131 -4.4289141 -4.4288945 -4.4288611 -4.4288478 -4.4288797][-4.4287829 -4.4287682 -4.4287705 -4.4287758 -4.4287686 -4.4287424 -4.4287477 -4.428791 -4.42884 -4.42886 -4.4288545 -4.4288387 -4.4288197 -4.4288206 -4.4288673][-4.42874 -4.4287276 -4.4287376 -4.42875 -4.42874 -4.428699 -4.4286876 -4.4287152 -4.4287562 -4.4287815 -4.4287848 -4.4287782 -4.4287629 -4.4287653 -4.42881][-4.4286947 -4.4286928 -4.4287095 -4.4287128 -4.42868 -4.4286208 -4.428597 -4.4286208 -4.4286704 -4.4287114 -4.4287329 -4.4287286 -4.4287009 -4.4286909 -4.4287243][-4.4286618 -4.4286737 -4.4286861 -4.42866 -4.4285884 -4.4285088 -4.4284773 -4.4285135 -4.4285908 -4.4286518 -4.4286795 -4.4286642 -4.4286151 -4.4285779 -4.4285913][-4.42866 -4.4286656 -4.4286566 -4.4285917 -4.4284816 -4.4283791 -4.4283466 -4.4283981 -4.428493 -4.42856 -4.4285893 -4.4285631 -4.428494 -4.4284363 -4.428443][-4.4286718 -4.42866 -4.4286313 -4.4285445 -4.4284177 -4.428308 -4.4282746 -4.4283261 -4.428411 -4.4284596 -4.4284725 -4.4284334 -4.4283576 -4.4283118 -4.4283481][-4.4287043 -4.4286804 -4.4286523 -4.4285765 -4.4284673 -4.428371 -4.4283333 -4.4283552 -4.4283957 -4.4284067 -4.4283981 -4.4283495 -4.4282832 -4.4282718 -4.4283466][-4.4287443 -4.4287281 -4.4287138 -4.4286728 -4.4286041 -4.4285383 -4.4285026 -4.4284997 -4.4285049 -4.4284863 -4.4284592 -4.4284005 -4.4283433 -4.4283557 -4.428442][-4.4287887 -4.4287825 -4.42878 -4.428772 -4.4287415 -4.4287081 -4.4286909 -4.4286776 -4.428669 -4.4286461 -4.4286141 -4.4285574 -4.4285088 -4.428525 -4.4285941][-4.4288092 -4.4288058 -4.4288087 -4.4288182 -4.4288163 -4.428812 -4.4288163 -4.4288096 -4.4288011 -4.4287863 -4.4287639 -4.4287219 -4.4286857 -4.4286952 -4.4287429][-4.4288154 -4.4288116 -4.42881 -4.4288211 -4.4288325 -4.4288497 -4.4288654 -4.4288673 -4.4288626 -4.4288559 -4.428843 -4.4288139 -4.4287872 -4.4287939 -4.4288259][-4.4288177 -4.4288144 -4.4288073 -4.4288149 -4.4288359 -4.428864 -4.4288836 -4.4288855 -4.4288759 -4.4288635 -4.4288511 -4.4288268 -4.428802 -4.4288096 -4.4288368][-4.4288287 -4.4288292 -4.4288249 -4.4288321 -4.428853 -4.4288745 -4.4288855 -4.4288807 -4.4288626 -4.4288392 -4.4288192 -4.428793 -4.4287691 -4.4287825 -4.428813]]...]
INFO - root - 2017-12-10 03:54:47.201350: step 610, loss = 2.28, batch loss = 2.23 (38.8 examples/sec; 0.206 sec/batch; 19h:00m:32s remains)
INFO - root - 2017-12-10 03:54:49.223858: step 620, loss = 2.28, batch loss = 2.23 (40.3 examples/sec; 0.199 sec/batch; 18h:19m:16s remains)
INFO - root - 2017-12-10 03:54:51.259030: step 630, loss = 2.28, batch loss = 2.23 (38.8 examples/sec; 0.206 sec/batch; 19h:01m:43s remains)
INFO - root - 2017-12-10 03:54:53.296416: step 640, loss = 2.28, batch loss = 2.23 (39.1 examples/sec; 0.204 sec/batch; 18h:50m:46s remains)
INFO - root - 2017-12-10 03:54:55.340306: step 650, loss = 2.28, batch loss = 2.23 (39.9 examples/sec; 0.200 sec/batch; 18h:27m:44s remains)
INFO - root - 2017-12-10 03:54:57.381402: step 660, loss = 2.28, batch loss = 2.23 (40.4 examples/sec; 0.198 sec/batch; 18h:16m:10s remains)
INFO - root - 2017-12-10 03:54:59.416693: step 670, loss = 2.28, batch loss = 2.23 (38.9 examples/sec; 0.205 sec/batch; 18h:56m:20s remains)
INFO - root - 2017-12-10 03:55:01.457321: step 680, loss = 2.28, batch loss = 2.23 (38.3 examples/sec; 0.209 sec/batch; 19h:14m:00s remains)
INFO - root - 2017-12-10 03:55:03.496680: step 690, loss = 2.28, batch loss = 2.23 (40.2 examples/sec; 0.199 sec/batch; 18h:21m:10s remains)
INFO - root - 2017-12-10 03:55:05.536919: step 700, loss = 2.28, batch loss = 2.23 (40.3 examples/sec; 0.199 sec/batch; 18h:18m:53s remains)
2017-12-10 03:55:05.816993: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286828 -4.4286752 -4.4286442 -4.4285879 -4.4285417 -4.4285431 -4.4285665 -4.4285755 -4.4285622 -4.42854 -4.428524 -4.4285245 -4.4285326 -4.4285464 -4.4285879][-4.4286733 -4.4286685 -4.4286489 -4.4285951 -4.4285474 -4.4285488 -4.4285765 -4.4285941 -4.4285765 -4.4285364 -4.4284935 -4.4284739 -4.4284611 -4.4284596 -4.4284992][-4.4286518 -4.4286442 -4.4286189 -4.4285564 -4.4284992 -4.4284945 -4.4285445 -4.4285955 -4.4285951 -4.428545 -4.4284863 -4.4284568 -4.4284248 -4.4284067 -4.4284382][-4.4286284 -4.4286189 -4.4285893 -4.4285159 -4.4284329 -4.4283981 -4.4284453 -4.4285197 -4.4285517 -4.4285135 -4.4284534 -4.42842 -4.4283795 -4.4283657 -4.4283957][-4.4286242 -4.4286194 -4.4286008 -4.4285369 -4.4284434 -4.4283748 -4.42839 -4.4284668 -4.4285288 -4.4285212 -4.4284725 -4.4284434 -4.428422 -4.4284348 -4.4284596][-4.4286246 -4.4286208 -4.4286122 -4.4285665 -4.4284763 -4.4283781 -4.4283457 -4.4283986 -4.428483 -4.4285188 -4.4285026 -4.4284925 -4.4285092 -4.4285564 -4.4285817][-4.42862 -4.4286151 -4.42862 -4.4285975 -4.42852 -4.4284077 -4.4283328 -4.428339 -4.4284124 -4.4284744 -4.42849 -4.428503 -4.4285455 -4.4286113 -4.4286408][-4.4285979 -4.4285889 -4.4286108 -4.4286156 -4.428556 -4.4284415 -4.4283433 -4.4283094 -4.4283581 -4.4284263 -4.4284635 -4.4284992 -4.4285607 -4.428638 -4.4286757][-4.4285865 -4.4285707 -4.4286027 -4.4286318 -4.4286079 -4.4285169 -4.4284096 -4.4283381 -4.4283347 -4.4283662 -4.4283977 -4.4284468 -4.4285316 -4.4286313 -4.4286909][-4.4285941 -4.4285817 -4.4286218 -4.4286718 -4.4286885 -4.4286451 -4.4285583 -4.4284682 -4.4284029 -4.4283595 -4.4283452 -4.4283824 -4.4284835 -4.4286084 -4.4287004][-4.4286036 -4.4285927 -4.4286356 -4.4286976 -4.4287386 -4.42873 -4.4286752 -4.4285879 -4.4284849 -4.4283729 -4.4283028 -4.4283171 -4.4284172 -4.4285736 -4.4287086][-4.42862 -4.4286041 -4.4286466 -4.42871 -4.4287553 -4.4287648 -4.4287338 -4.4286666 -4.4285626 -4.4284182 -4.4283004 -4.4282751 -4.4283609 -4.4285259 -4.4286871][-4.4286447 -4.4286256 -4.4286618 -4.4287171 -4.4287548 -4.4287705 -4.4287624 -4.4287314 -4.4286547 -4.4285264 -4.4284 -4.4283481 -4.428412 -4.4285536 -4.4286938][-4.4286752 -4.4286618 -4.4286857 -4.428719 -4.4287419 -4.428761 -4.4287696 -4.4287663 -4.4287229 -4.4286337 -4.4285345 -4.4284821 -4.4285178 -4.4286175 -4.4287229][-4.4286928 -4.4286866 -4.4286995 -4.4287133 -4.4287276 -4.4287491 -4.4287691 -4.4287839 -4.4287691 -4.4287205 -4.4286585 -4.4286189 -4.4286256 -4.4286766 -4.4287467]]...]
INFO - root - 2017-12-10 03:55:07.850990: step 710, loss = 2.28, batch loss = 2.23 (38.5 examples/sec; 0.208 sec/batch; 19h:10m:24s remains)
INFO - root - 2017-12-10 03:55:09.882762: step 720, loss = 2.28, batch loss = 2.23 (38.6 examples/sec; 0.207 sec/batch; 19h:04m:50s remains)
INFO - root - 2017-12-10 03:55:11.926220: step 730, loss = 2.28, batch loss = 2.23 (38.3 examples/sec; 0.209 sec/batch; 19h:16m:10s remains)
INFO - root - 2017-12-10 03:55:14.026321: step 740, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:55m:48s remains)
INFO - root - 2017-12-10 03:55:16.087324: step 750, loss = 2.28, batch loss = 2.23 (38.6 examples/sec; 0.207 sec/batch; 19h:05m:34s remains)
INFO - root - 2017-12-10 03:55:18.123880: step 760, loss = 2.28, batch loss = 2.23 (40.4 examples/sec; 0.198 sec/batch; 18h:14m:58s remains)
INFO - root - 2017-12-10 03:55:20.163538: step 770, loss = 2.28, batch loss = 2.23 (40.0 examples/sec; 0.200 sec/batch; 18h:25m:51s remains)
INFO - root - 2017-12-10 03:55:22.228679: step 780, loss = 2.28, batch loss = 2.23 (39.4 examples/sec; 0.203 sec/batch; 18h:43m:37s remains)
INFO - root - 2017-12-10 03:55:24.278205: step 790, loss = 2.28, batch loss = 2.23 (39.4 examples/sec; 0.203 sec/batch; 18h:41m:28s remains)
INFO - root - 2017-12-10 03:55:26.318632: step 800, loss = 2.28, batch loss = 2.23 (40.2 examples/sec; 0.199 sec/batch; 18h:20m:32s remains)
2017-12-10 03:55:26.577383: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289036 -4.4288859 -4.4288788 -4.4288864 -4.428885 -4.4288588 -4.4288287 -4.4288025 -4.4287848 -4.4287691 -4.42876 -4.42876 -4.4287572 -4.4287477 -4.4287338][-4.4289122 -4.4288979 -4.4288893 -4.4288917 -4.4288821 -4.42885 -4.4288149 -4.4287796 -4.4287548 -4.4287343 -4.4287248 -4.4287257 -4.428721 -4.4287105 -4.4286976][-4.428905 -4.4288945 -4.4288836 -4.4288769 -4.4288549 -4.4288125 -4.4287705 -4.4287281 -4.4286976 -4.4286785 -4.428679 -4.4286952 -4.4287024 -4.4287009 -4.4286966][-4.4288759 -4.4288659 -4.4288521 -4.4288363 -4.4288058 -4.4287586 -4.4287143 -4.4286671 -4.4286332 -4.4286222 -4.4286385 -4.4286709 -4.4286942 -4.4287057 -4.4287086][-4.4288344 -4.4288197 -4.4287992 -4.4287724 -4.4287324 -4.4286857 -4.4286394 -4.4285889 -4.4285603 -4.4285731 -4.4286118 -4.4286575 -4.4286919 -4.4287124 -4.4287219][-4.4287944 -4.42877 -4.4287333 -4.4286852 -4.4286294 -4.428576 -4.4285226 -4.428463 -4.4284453 -4.4284983 -4.428575 -4.4286427 -4.4286866 -4.4287086 -4.4287176][-4.42877 -4.4287448 -4.428699 -4.4286337 -4.42856 -4.4284844 -4.428401 -4.4283113 -4.4282947 -4.4283834 -4.4285021 -4.4285951 -4.4286447 -4.4286561 -4.4286485][-4.4287686 -4.4287519 -4.4287071 -4.4286385 -4.4285607 -4.4284792 -4.4283872 -4.4282904 -4.4282751 -4.4283652 -4.4284821 -4.42857 -4.4286113 -4.4286065 -4.4285812][-4.4287868 -4.428781 -4.4287448 -4.4286828 -4.4286194 -4.4285588 -4.4284897 -4.4284148 -4.4283986 -4.4284606 -4.428545 -4.4286103 -4.428637 -4.4286227 -4.4285951][-4.4288211 -4.42882 -4.428793 -4.4287419 -4.4286952 -4.4286547 -4.4286032 -4.4285407 -4.4285192 -4.4285555 -4.4286151 -4.4286652 -4.4286809 -4.4286628 -4.4286408][-4.4288673 -4.4288669 -4.4288492 -4.4288135 -4.4287829 -4.4287548 -4.4287128 -4.4286575 -4.4286294 -4.4286466 -4.4286857 -4.4287214 -4.4287357 -4.428721 -4.4287052][-4.428905 -4.428905 -4.428895 -4.4288745 -4.4288568 -4.4288373 -4.4288054 -4.4287634 -4.4287386 -4.4287438 -4.4287667 -4.428792 -4.4288063 -4.4287992 -4.4287882][-4.4289131 -4.4289145 -4.4289117 -4.4289007 -4.42889 -4.4288769 -4.4288554 -4.4288287 -4.4288111 -4.42881 -4.4288197 -4.4288335 -4.4288464 -4.4288483 -4.4288478][-4.428916 -4.4289174 -4.4289184 -4.4289122 -4.4289045 -4.428894 -4.4288812 -4.4288659 -4.428854 -4.4288487 -4.4288464 -4.4288516 -4.4288607 -4.4288678 -4.428874][-4.4289279 -4.4289312 -4.4289346 -4.4289317 -4.4289269 -4.4289193 -4.4289117 -4.4289041 -4.4288969 -4.4288926 -4.4288874 -4.4288845 -4.4288859 -4.4288898 -4.4288921]]...]
INFO - root - 2017-12-10 03:55:28.632214: step 810, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:36m:06s remains)
INFO - root - 2017-12-10 03:55:30.671134: step 820, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:26m:16s remains)
INFO - root - 2017-12-10 03:55:32.702274: step 830, loss = 2.28, batch loss = 2.23 (39.0 examples/sec; 0.205 sec/batch; 18h:54m:16s remains)
INFO - root - 2017-12-10 03:55:34.745458: step 840, loss = 2.28, batch loss = 2.23 (38.2 examples/sec; 0.209 sec/batch; 19h:16m:08s remains)
INFO - root - 2017-12-10 03:55:36.790606: step 850, loss = 2.28, batch loss = 2.23 (38.3 examples/sec; 0.209 sec/batch; 19h:14m:27s remains)
INFO - root - 2017-12-10 03:55:38.823702: step 860, loss = 2.28, batch loss = 2.23 (39.5 examples/sec; 0.202 sec/batch; 18h:38m:45s remains)
INFO - root - 2017-12-10 03:55:40.861160: step 870, loss = 2.28, batch loss = 2.23 (39.1 examples/sec; 0.204 sec/batch; 18h:50m:17s remains)
INFO - root - 2017-12-10 03:55:42.912728: step 880, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.210 sec/batch; 19h:22m:14s remains)
INFO - root - 2017-12-10 03:55:44.946671: step 890, loss = 2.28, batch loss = 2.23 (38.8 examples/sec; 0.206 sec/batch; 18h:59m:21s remains)
INFO - root - 2017-12-10 03:55:47.009955: step 900, loss = 2.28, batch loss = 2.23 (39.3 examples/sec; 0.204 sec/batch; 18h:45m:57s remains)
2017-12-10 03:55:47.327150: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286489 -4.4286342 -4.4286017 -4.4285607 -4.4285398 -4.4285526 -4.4286013 -4.4286337 -4.4286547 -4.4286842 -4.4286761 -4.4286714 -4.4286575 -4.428616 -4.4286265][-4.4285922 -4.4286342 -4.4286366 -4.4285975 -4.4285593 -4.4285455 -4.4285836 -4.4286275 -4.4286575 -4.428689 -4.42867 -4.4286456 -4.4286084 -4.428566 -4.4285855][-4.4284916 -4.42861 -4.42867 -4.428648 -4.4286003 -4.42856 -4.4285827 -4.4286251 -4.4286532 -4.4286852 -4.4286737 -4.4286313 -4.4285812 -4.4285383 -4.4285555][-4.4284067 -4.4285731 -4.4286814 -4.4286923 -4.4286456 -4.4285769 -4.4285588 -4.4285774 -4.4286075 -4.428647 -4.4286509 -4.4286208 -4.4285841 -4.4285445 -4.428555][-4.4285007 -4.4286289 -4.4287248 -4.4287329 -4.4286737 -4.4285755 -4.4285078 -4.4284883 -4.4285259 -4.4285889 -4.4286146 -4.4286075 -4.4285908 -4.4285531 -4.4285541][-4.4286585 -4.4287219 -4.4287643 -4.4287386 -4.4286561 -4.4285326 -4.4284258 -4.4283695 -4.4284205 -4.4285169 -4.4285736 -4.4285941 -4.4285865 -4.4285474 -4.4285407][-4.4287672 -4.4287829 -4.428772 -4.4286985 -4.4285679 -4.4284043 -4.4282627 -4.4281898 -4.4282856 -4.4284368 -4.42853 -4.4285703 -4.428566 -4.4285173 -4.4284987][-4.4288182 -4.4287972 -4.4287453 -4.4286356 -4.4284582 -4.4282494 -4.4280767 -4.428009 -4.4281735 -4.4283929 -4.4285183 -4.4285736 -4.4285612 -4.4284873 -4.4284554][-4.4288168 -4.4287691 -4.4287004 -4.4285975 -4.4284472 -4.4282923 -4.4281831 -4.4281592 -4.4283214 -4.4285097 -4.4286118 -4.4286642 -4.428648 -4.4285588 -4.428504][-4.4288106 -4.4287777 -4.4287343 -4.4286642 -4.42857 -4.4284849 -4.4284339 -4.4284406 -4.4285541 -4.4286571 -4.428699 -4.428731 -4.4287252 -4.4286494 -4.4285922][-4.4288282 -4.4288311 -4.4288239 -4.4287891 -4.4287295 -4.4286771 -4.4286427 -4.428647 -4.4287033 -4.4287457 -4.4287457 -4.4287591 -4.4287548 -4.4286985 -4.4286437][-4.4288406 -4.4288573 -4.4288669 -4.4288554 -4.428823 -4.4287934 -4.428772 -4.4287705 -4.4287925 -4.428803 -4.4287844 -4.4287829 -4.4287739 -4.4287186 -4.42865][-4.4288383 -4.4288378 -4.4288464 -4.4288468 -4.4288278 -4.4288092 -4.4288034 -4.4288096 -4.42882 -4.4288273 -4.4288039 -4.428792 -4.4287734 -4.4287229 -4.428658][-4.428833 -4.4288216 -4.428823 -4.4288263 -4.4288192 -4.428812 -4.4288197 -4.4288311 -4.4288354 -4.4288397 -4.428823 -4.428812 -4.4287953 -4.4287577 -4.4287019][-4.4288425 -4.4288287 -4.4288249 -4.4288278 -4.4288316 -4.4288344 -4.4288473 -4.4288635 -4.4288669 -4.4288678 -4.4288549 -4.4288492 -4.4288378 -4.4288073 -4.4287558]]...]
INFO - root - 2017-12-10 03:55:49.395127: step 910, loss = 2.28, batch loss = 2.23 (39.9 examples/sec; 0.201 sec/batch; 18h:29m:15s remains)
INFO - root - 2017-12-10 03:55:51.437908: step 920, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.211 sec/batch; 19h:24m:10s remains)
INFO - root - 2017-12-10 03:55:53.503995: step 930, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:25m:03s remains)
INFO - root - 2017-12-10 03:55:55.547175: step 940, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:26m:14s remains)
INFO - root - 2017-12-10 03:55:57.586568: step 950, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:36m:27s remains)
INFO - root - 2017-12-10 03:55:59.627700: step 960, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.202 sec/batch; 18h:34m:34s remains)
INFO - root - 2017-12-10 03:56:01.660372: step 970, loss = 2.28, batch loss = 2.23 (39.0 examples/sec; 0.205 sec/batch; 18h:52m:51s remains)
INFO - root - 2017-12-10 03:56:03.726920: step 980, loss = 2.28, batch loss = 2.23 (40.0 examples/sec; 0.200 sec/batch; 18h:25m:35s remains)
INFO - root - 2017-12-10 03:56:05.768599: step 990, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 19h:30m:38s remains)
INFO - root - 2017-12-10 03:56:07.860049: step 1000, loss = 2.28, batch loss = 2.23 (38.2 examples/sec; 0.209 sec/batch; 19h:16m:00s remains)
2017-12-10 03:56:08.123896: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286256 -4.4286385 -4.4286542 -4.4286857 -4.4287076 -4.4287109 -4.4286947 -4.4286871 -4.4286895 -4.4286857 -4.428709 -4.4287329 -4.4287734 -4.4288263 -4.4288721][-4.4286013 -4.4286189 -4.4286366 -4.4286718 -4.4286981 -4.4286976 -4.4286709 -4.428658 -4.4286733 -4.4286847 -4.4287181 -4.4287481 -4.4287896 -4.4288387 -4.4288778][-4.4286003 -4.4286284 -4.4286509 -4.4286823 -4.4286985 -4.4286852 -4.4286504 -4.4286323 -4.4286556 -4.4286819 -4.428721 -4.4287529 -4.4287939 -4.4288411 -4.42888][-4.4286208 -4.4286594 -4.4286885 -4.4287128 -4.4287076 -4.4286723 -4.4286222 -4.4286008 -4.4286313 -4.4286675 -4.4287119 -4.4287467 -4.4287915 -4.4288421 -4.4288864][-4.4286637 -4.4286942 -4.4287171 -4.4287252 -4.4286971 -4.42864 -4.4285736 -4.4285526 -4.4285936 -4.4286423 -4.4286914 -4.4287324 -4.4287868 -4.4288449 -4.4288988][-4.42873 -4.4287386 -4.4287405 -4.4287205 -4.4286642 -4.4285817 -4.4284959 -4.4284787 -4.428545 -4.4286208 -4.4286833 -4.4287324 -4.428793 -4.4288564 -4.4289145][-4.4287624 -4.4287481 -4.428731 -4.4286847 -4.4286056 -4.4284997 -4.4283934 -4.4283886 -4.4284925 -4.428606 -4.4286861 -4.4287424 -4.4288039 -4.4288678 -4.4289255][-4.4287481 -4.4287214 -4.4286976 -4.4286404 -4.4285512 -4.4284329 -4.4283123 -4.4283228 -4.42846 -4.4286008 -4.4286947 -4.4287567 -4.4288163 -4.4288774 -4.4289331][-4.4287329 -4.4287033 -4.428679 -4.4286246 -4.4285507 -4.428452 -4.4283476 -4.4283652 -4.4284925 -4.4286218 -4.4287086 -4.4287658 -4.4288197 -4.42888 -4.4289384][-4.4287062 -4.4286771 -4.4286575 -4.4286208 -4.4285793 -4.4285254 -4.4284587 -4.4284692 -4.4285517 -4.4286385 -4.4287024 -4.42875 -4.4288039 -4.4288707 -4.4289355][-4.4286804 -4.4286489 -4.4286346 -4.4286184 -4.428606 -4.4285903 -4.428555 -4.4285583 -4.4286008 -4.4286513 -4.4286947 -4.4287372 -4.4287934 -4.4288654 -4.4289351][-4.4286695 -4.4286385 -4.4286289 -4.4286265 -4.4286313 -4.428638 -4.4286222 -4.4286194 -4.4286413 -4.4286742 -4.428709 -4.428751 -4.4288068 -4.4288745 -4.42894][-4.4286709 -4.4286437 -4.428638 -4.4286466 -4.4286604 -4.4286776 -4.428669 -4.4286633 -4.428679 -4.4287076 -4.4287429 -4.4287868 -4.4288397 -4.4288979 -4.4289522][-4.428709 -4.4286895 -4.4286885 -4.4286995 -4.4287119 -4.4287257 -4.4287138 -4.4287043 -4.4287224 -4.4287529 -4.428792 -4.4288359 -4.4288821 -4.4289279 -4.4289689][-4.4287825 -4.428771 -4.4287715 -4.4287786 -4.4287829 -4.4287868 -4.428772 -4.4287658 -4.4287872 -4.4288187 -4.4288545 -4.4288883 -4.4289212 -4.4289532 -4.4289823]]...]
INFO - root - 2017-12-10 03:56:10.218065: step 1010, loss = 2.28, batch loss = 2.23 (39.4 examples/sec; 0.203 sec/batch; 18h:41m:33s remains)
INFO - root - 2017-12-10 03:56:12.286759: step 1020, loss = 2.28, batch loss = 2.23 (39.1 examples/sec; 0.204 sec/batch; 18h:49m:46s remains)
INFO - root - 2017-12-10 03:56:14.324681: step 1030, loss = 2.28, batch loss = 2.23 (40.3 examples/sec; 0.199 sec/batch; 18h:16m:54s remains)
INFO - root - 2017-12-10 03:56:16.385293: step 1040, loss = 2.28, batch loss = 2.23 (38.3 examples/sec; 0.209 sec/batch; 19h:12m:46s remains)
INFO - root - 2017-12-10 03:56:18.419973: step 1050, loss = 2.28, batch loss = 2.23 (39.3 examples/sec; 0.203 sec/batch; 18h:44m:01s remains)
INFO - root - 2017-12-10 03:56:20.459040: step 1060, loss = 2.28, batch loss = 2.23 (39.9 examples/sec; 0.200 sec/batch; 18h:26m:32s remains)
INFO - root - 2017-12-10 03:56:22.529731: step 1070, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:30m:48s remains)
INFO - root - 2017-12-10 03:56:24.584505: step 1080, loss = 2.28, batch loss = 2.23 (38.2 examples/sec; 0.209 sec/batch; 19h:15m:18s remains)
INFO - root - 2017-12-10 03:56:26.642934: step 1090, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 19h:20m:40s remains)
INFO - root - 2017-12-10 03:56:28.706507: step 1100, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 19h:19m:06s remains)
2017-12-10 03:56:28.988801: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287357 -4.4286547 -4.4285626 -4.428545 -4.4286008 -4.4286509 -4.4286933 -4.4287367 -4.4287543 -4.4287672 -4.4287896 -4.428822 -4.4288626 -4.4288993 -4.4289174][-4.4287744 -4.4287109 -4.4286361 -4.4286 -4.4286022 -4.4285989 -4.428617 -4.4286728 -4.4287157 -4.4287443 -4.4287767 -4.4288092 -4.4288507 -4.42889 -4.4289155][-4.428792 -4.4287333 -4.4286618 -4.4285913 -4.4285297 -4.4284725 -4.4284868 -4.4285755 -4.42866 -4.4287195 -4.4287686 -4.4288058 -4.4288445 -4.4288807 -4.4289074][-4.428793 -4.42874 -4.4286714 -4.4285822 -4.4284792 -4.4283891 -4.4284034 -4.4285159 -4.4286294 -4.4287124 -4.4287739 -4.4288111 -4.428843 -4.4288731 -4.4288983][-4.4287906 -4.4287481 -4.4286938 -4.4286151 -4.4285169 -4.4284258 -4.4284306 -4.4285235 -4.4286318 -4.4287138 -4.4287724 -4.4288087 -4.4288373 -4.428865 -4.4288878][-4.4287744 -4.42875 -4.4287071 -4.4286447 -4.428565 -4.4284892 -4.4284835 -4.428535 -4.4286137 -4.4286847 -4.42874 -4.4287844 -4.4288187 -4.4288545 -4.4288831][-4.4287462 -4.4287319 -4.4286919 -4.4286361 -4.4285693 -4.4284997 -4.4284739 -4.4284821 -4.4285288 -4.4285946 -4.4286633 -4.4287295 -4.4287848 -4.4288378 -4.4288793][-4.4287353 -4.4287267 -4.42869 -4.4286346 -4.4285727 -4.4285 -4.4284449 -4.4284062 -4.4284196 -4.4284878 -4.4285736 -4.4286671 -4.4287486 -4.4288211 -4.4288778][-4.4287796 -4.4287729 -4.428741 -4.428688 -4.428627 -4.4285493 -4.4284792 -4.4284182 -4.4284067 -4.4284668 -4.428555 -4.4286523 -4.4287424 -4.4288268 -4.4288907][-4.42883 -4.4288292 -4.4288058 -4.42876 -4.4286957 -4.4286137 -4.4285469 -4.428493 -4.4284782 -4.4285231 -4.4286022 -4.4286923 -4.4287782 -4.4288578 -4.4289174][-4.4288397 -4.4288468 -4.4288359 -4.428802 -4.4287372 -4.4286556 -4.4285955 -4.42856 -4.4285469 -4.42857 -4.4286332 -4.4287233 -4.4288092 -4.4288816 -4.4289336][-4.4288158 -4.42883 -4.4288373 -4.4288187 -4.4287682 -4.4286995 -4.4286423 -4.4286108 -4.4285917 -4.4285913 -4.4286351 -4.4287248 -4.428813 -4.4288831 -4.42893][-4.42881 -4.4288268 -4.4288445 -4.4288416 -4.428812 -4.4287634 -4.4287128 -4.4286795 -4.4286513 -4.42863 -4.4286537 -4.4287271 -4.4288068 -4.428875 -4.4289188][-4.4288406 -4.4288516 -4.428865 -4.428865 -4.4288507 -4.4288244 -4.4287877 -4.4287586 -4.4287262 -4.4286909 -4.4286971 -4.4287477 -4.428812 -4.4288716 -4.4289126][-4.42887 -4.4288673 -4.4288659 -4.4288611 -4.428854 -4.4288464 -4.4288273 -4.4288073 -4.4287758 -4.4287333 -4.4287267 -4.4287643 -4.4288197 -4.4288688 -4.4289041]]...]
INFO - root - 2017-12-10 03:56:31.026266: step 1110, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.202 sec/batch; 18h:33m:39s remains)
INFO - root - 2017-12-10 03:56:33.068542: step 1120, loss = 2.28, batch loss = 2.23 (38.4 examples/sec; 0.208 sec/batch; 19h:11m:31s remains)
INFO - root - 2017-12-10 03:56:35.106398: step 1130, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.201 sec/batch; 18h:31m:57s remains)
INFO - root - 2017-12-10 03:56:37.158434: step 1140, loss = 2.28, batch loss = 2.23 (39.5 examples/sec; 0.203 sec/batch; 18h:39m:09s remains)
INFO - root - 2017-12-10 03:56:39.230960: step 1150, loss = 2.28, batch loss = 2.23 (38.7 examples/sec; 0.207 sec/batch; 19h:01m:03s remains)
INFO - root - 2017-12-10 03:56:41.270943: step 1160, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:46m:56s remains)
INFO - root - 2017-12-10 03:56:43.303128: step 1170, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:48m:16s remains)
INFO - root - 2017-12-10 03:56:45.355941: step 1180, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:46m:32s remains)
INFO - root - 2017-12-10 03:56:47.415577: step 1190, loss = 2.28, batch loss = 2.23 (38.4 examples/sec; 0.208 sec/batch; 19h:10m:32s remains)
INFO - root - 2017-12-10 03:56:49.465861: step 1200, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:26m:30s remains)
2017-12-10 03:56:49.755669: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288483 -4.4288568 -4.4288783 -4.4288869 -4.4288816 -4.42888 -4.4288893 -4.4289165 -4.4289346 -4.4289465 -4.4289546 -4.4289589 -4.4289494 -4.42893 -4.4289112][-4.4287753 -4.4287825 -4.4288273 -4.428864 -4.4288721 -4.4288778 -4.4288921 -4.4289312 -4.4289508 -4.4289536 -4.4289513 -4.428947 -4.4289322 -4.4289107 -4.4288955][-4.4286952 -4.4287024 -4.4287672 -4.428834 -4.428854 -4.4288578 -4.4288673 -4.4289088 -4.4289308 -4.4289303 -4.4289279 -4.4289289 -4.4289136 -4.4288878 -4.4288707][-4.4286518 -4.4286661 -4.4287362 -4.4288073 -4.4288177 -4.4288034 -4.4288068 -4.4288497 -4.4288869 -4.4289055 -4.4289174 -4.4289289 -4.4289155 -4.4288831 -4.4288578][-4.4286346 -4.4286509 -4.4287143 -4.4287658 -4.4287505 -4.4287033 -4.4286857 -4.4287386 -4.4288068 -4.428865 -4.4289012 -4.4289279 -4.428925 -4.42889 -4.4288573][-4.4286313 -4.4286466 -4.428688 -4.4287028 -4.4286437 -4.4285398 -4.4284697 -4.428535 -4.4286656 -4.4287853 -4.4288583 -4.4289079 -4.4289241 -4.4288979 -4.4288573][-4.4286604 -4.4286728 -4.4286804 -4.4286413 -4.4285254 -4.4283466 -4.428184 -4.4282513 -4.4284725 -4.4286776 -4.428803 -4.4288783 -4.42891 -4.4288983 -4.4288535][-4.4286923 -4.4286904 -4.4286747 -4.4285975 -4.4284525 -4.4282417 -4.4280157 -4.4280753 -4.4283676 -4.428627 -4.4287848 -4.428874 -4.428906 -4.4289021 -4.4288607][-4.4287214 -4.4287133 -4.4286895 -4.4286184 -4.4285049 -4.4283614 -4.42821 -4.4282441 -4.4284668 -4.428679 -4.4288173 -4.428894 -4.42892 -4.4289155 -4.4288788][-4.428761 -4.4287567 -4.4287457 -4.4287 -4.4286327 -4.4285722 -4.4285111 -4.4285278 -4.4286475 -4.4287748 -4.4288683 -4.4289184 -4.4289303 -4.428925 -4.4288969][-4.4288182 -4.4288197 -4.4288187 -4.428792 -4.4287539 -4.4287443 -4.4287419 -4.4287586 -4.4288135 -4.4288759 -4.428926 -4.4289465 -4.4289422 -4.4289365 -4.428916][-4.4288754 -4.4288716 -4.4288716 -4.4288616 -4.4288435 -4.4288592 -4.4288793 -4.4288955 -4.4289107 -4.4289303 -4.4289503 -4.4289494 -4.4289355 -4.4289279 -4.4289141][-4.4289083 -4.4288983 -4.4289026 -4.428896 -4.4288816 -4.4288921 -4.4289093 -4.4289126 -4.4289017 -4.4289 -4.4289074 -4.4289079 -4.4289017 -4.4288931 -4.4288769][-4.4289165 -4.428905 -4.4289126 -4.4289079 -4.4288893 -4.42889 -4.4288974 -4.4288926 -4.4288688 -4.4288564 -4.4288583 -4.4288592 -4.4288692 -4.4288721 -4.4288564][-4.4289212 -4.4289079 -4.4289107 -4.428906 -4.428894 -4.4288907 -4.4288907 -4.4288816 -4.4288568 -4.4288392 -4.4288344 -4.4288321 -4.4288526 -4.4288716 -4.4288669]]...]
INFO - root - 2017-12-10 03:56:51.803341: step 1210, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 19h:19m:17s remains)
INFO - root - 2017-12-10 03:56:53.857801: step 1220, loss = 2.28, batch loss = 2.23 (40.0 examples/sec; 0.200 sec/batch; 18h:25m:01s remains)
INFO - root - 2017-12-10 03:56:55.971778: step 1230, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.202 sec/batch; 18h:32m:35s remains)
INFO - root - 2017-12-10 03:56:58.018931: step 1240, loss = 2.28, batch loss = 2.23 (39.6 examples/sec; 0.202 sec/batch; 18h:34m:24s remains)
INFO - root - 2017-12-10 03:57:00.059970: step 1250, loss = 2.28, batch loss = 2.23 (39.3 examples/sec; 0.204 sec/batch; 18h:45m:00s remains)
INFO - root - 2017-12-10 03:57:02.097007: step 1260, loss = 2.28, batch loss = 2.23 (40.0 examples/sec; 0.200 sec/batch; 18h:24m:40s remains)
INFO - root - 2017-12-10 03:57:04.144534: step 1270, loss = 2.28, batch loss = 2.23 (38.6 examples/sec; 0.207 sec/batch; 19h:04m:47s remains)
INFO - root - 2017-12-10 03:57:06.189522: step 1280, loss = 2.28, batch loss = 2.23 (38.2 examples/sec; 0.209 sec/batch; 19h:14m:36s remains)
INFO - root - 2017-12-10 03:57:08.224995: step 1290, loss = 2.28, batch loss = 2.23 (39.4 examples/sec; 0.203 sec/batch; 18h:40m:08s remains)
INFO - root - 2017-12-10 03:57:10.270507: step 1300, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:47m:48s remains)
2017-12-10 03:57:10.539158: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287004 -4.4286928 -4.4286871 -4.4286737 -4.428669 -4.4286461 -4.4286013 -4.4286308 -4.4286933 -4.4287329 -4.4287558 -4.4287567 -4.4287739 -4.4288011 -4.4288092][-4.4286661 -4.428668 -4.4286747 -4.4286714 -4.4286714 -4.4286585 -4.428616 -4.4286275 -4.4286914 -4.4287381 -4.4287529 -4.4287562 -4.4287834 -4.428812 -4.4288206][-4.4286556 -4.4286618 -4.4286723 -4.4286661 -4.4286609 -4.428648 -4.4286127 -4.4286046 -4.4286556 -4.4287019 -4.4287214 -4.4287391 -4.4287796 -4.4288034 -4.4288087][-4.4286466 -4.4286456 -4.4286494 -4.4286389 -4.4286251 -4.4286084 -4.4285808 -4.428565 -4.4285893 -4.4286337 -4.4286647 -4.4286928 -4.4287429 -4.4287577 -4.4287519][-4.4285946 -4.4285717 -4.4285564 -4.4285421 -4.4285231 -4.4285073 -4.4284873 -4.4284654 -4.4284611 -4.4284992 -4.428546 -4.4285817 -4.42864 -4.4286594 -4.4286509][-4.4285226 -4.428462 -4.4284215 -4.4283962 -4.4283705 -4.4283557 -4.42834 -4.4282975 -4.4282503 -4.4282861 -4.4283638 -4.4284172 -4.42849 -4.4285288 -4.4285293][-4.4284692 -4.4283776 -4.4283156 -4.4282832 -4.4282589 -4.4282427 -4.4282165 -4.4281459 -4.4280543 -4.4280825 -4.4281926 -4.4282684 -4.4283447 -4.4283938 -4.4284034][-4.4284678 -4.4283714 -4.4283133 -4.4282818 -4.4282584 -4.4282317 -4.428194 -4.4281225 -4.4280357 -4.4280486 -4.428154 -4.428226 -4.4282804 -4.4283218 -4.4283328][-4.4284992 -4.4284163 -4.4283676 -4.4283419 -4.4283137 -4.4282808 -4.4282479 -4.4282064 -4.4281559 -4.4281535 -4.4282207 -4.42827 -4.4282923 -4.4283218 -4.428339][-4.4285593 -4.4284964 -4.4284616 -4.4284487 -4.4284282 -4.4284053 -4.4283886 -4.4283738 -4.4283485 -4.4283242 -4.4283381 -4.4283519 -4.4283419 -4.4283476 -4.4283628][-4.4286375 -4.428596 -4.4285769 -4.4285808 -4.4285774 -4.428575 -4.4285784 -4.428576 -4.4285531 -4.4285107 -4.4284825 -4.4284649 -4.42843 -4.4284124 -4.4284153][-4.4286957 -4.428668 -4.4286571 -4.428668 -4.4286795 -4.4286952 -4.4287105 -4.4287114 -4.4286923 -4.4286518 -4.4286094 -4.4285793 -4.4285421 -4.428514 -4.42851][-4.428721 -4.4286957 -4.4286833 -4.4286866 -4.4286981 -4.4287219 -4.4287453 -4.4287534 -4.4287457 -4.4287243 -4.4286947 -4.4286752 -4.4286509 -4.4286304 -4.4286289][-4.4287348 -4.4287076 -4.4286852 -4.4286714 -4.4286709 -4.4286938 -4.4287219 -4.4287362 -4.4287372 -4.4287295 -4.4287176 -4.4287162 -4.4287114 -4.4287009 -4.4287024][-4.4287643 -4.428741 -4.4287133 -4.428689 -4.428679 -4.4286942 -4.4287195 -4.4287338 -4.4287362 -4.4287333 -4.4287276 -4.4287262 -4.428721 -4.4287128 -4.4287138]]...]
INFO - root - 2017-12-10 03:57:12.582844: step 1310, loss = 2.28, batch loss = 2.23 (38.7 examples/sec; 0.207 sec/batch; 19h:02m:03s remains)
INFO - root - 2017-12-10 03:57:14.643059: step 1320, loss = 2.28, batch loss = 2.23 (38.5 examples/sec; 0.208 sec/batch; 19h:05m:41s remains)
INFO - root - 2017-12-10 03:57:16.686131: step 1330, loss = 2.28, batch loss = 2.23 (39.0 examples/sec; 0.205 sec/batch; 18h:51m:49s remains)
INFO - root - 2017-12-10 03:57:18.755394: step 1340, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.210 sec/batch; 19h:21m:05s remains)
INFO - root - 2017-12-10 03:57:20.804190: step 1350, loss = 2.28, batch loss = 2.23 (38.2 examples/sec; 0.210 sec/batch; 19h:16m:31s remains)
INFO - root - 2017-12-10 03:57:22.868694: step 1360, loss = 2.28, batch loss = 2.23 (38.6 examples/sec; 0.207 sec/batch; 19h:05m:08s remains)
INFO - root - 2017-12-10 03:57:24.923201: step 1370, loss = 2.28, batch loss = 2.23 (39.4 examples/sec; 0.203 sec/batch; 18h:39m:54s remains)
INFO - root - 2017-12-10 03:57:26.965427: step 1380, loss = 2.28, batch loss = 2.23 (40.0 examples/sec; 0.200 sec/batch; 18h:23m:11s remains)
INFO - root - 2017-12-10 03:57:29.002171: step 1390, loss = 2.28, batch loss = 2.23 (40.3 examples/sec; 0.199 sec/batch; 18h:15m:52s remains)
INFO - root - 2017-12-10 03:57:31.036142: step 1400, loss = 2.28, batch loss = 2.23 (40.3 examples/sec; 0.198 sec/batch; 18h:14m:53s remains)
2017-12-10 03:57:31.317048: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288836 -4.4288759 -4.4288678 -4.4288535 -4.4288311 -4.4288149 -4.4288192 -4.4288411 -4.428865 -4.4288859 -4.4289 -4.4289174 -4.4289331 -4.4289503 -4.4289703][-4.4288349 -4.4288206 -4.4288111 -4.4288 -4.4287748 -4.4287572 -4.4287629 -4.4287868 -4.4288149 -4.4288387 -4.4288559 -4.4288859 -4.4289103 -4.4289341 -4.4289584][-4.4287715 -4.4287443 -4.4287305 -4.4287229 -4.4287057 -4.4286957 -4.4287052 -4.4287252 -4.4287562 -4.4287858 -4.4288096 -4.428854 -4.4288907 -4.4289217 -4.4289513][-4.4287047 -4.4286623 -4.4286413 -4.4286404 -4.4286366 -4.4286356 -4.4286489 -4.42866 -4.4286976 -4.42874 -4.4287724 -4.4288244 -4.4288697 -4.4289103 -4.4289474][-4.4286575 -4.4286008 -4.4285784 -4.4285951 -4.4286036 -4.4286017 -4.4286017 -4.4285989 -4.428647 -4.4287086 -4.4287462 -4.4287953 -4.4288464 -4.4288945 -4.4289403][-4.4286466 -4.4285812 -4.4285679 -4.4286046 -4.4286275 -4.428617 -4.4285841 -4.4285593 -4.4286127 -4.4286885 -4.4287267 -4.4287696 -4.4288225 -4.4288759 -4.4289317][-4.4286656 -4.4286003 -4.4285922 -4.428638 -4.4286652 -4.4286427 -4.4285812 -4.4285412 -4.4285979 -4.4286757 -4.4287133 -4.4287572 -4.4288125 -4.4288673 -4.4289289][-4.428699 -4.4286447 -4.4286427 -4.4286857 -4.4287066 -4.4286704 -4.4286008 -4.4285612 -4.4286251 -4.4286981 -4.4287343 -4.4287744 -4.4288263 -4.4288774 -4.4289355][-4.4287486 -4.4287124 -4.4287162 -4.4287419 -4.4287534 -4.4287128 -4.4286494 -4.4286213 -4.4286928 -4.4287653 -4.4287939 -4.4288173 -4.4288535 -4.4288955 -4.428946][-4.4287934 -4.4287591 -4.4287615 -4.4287729 -4.4287748 -4.4287381 -4.4286866 -4.4286761 -4.4287519 -4.4288239 -4.4288454 -4.4288535 -4.428874 -4.428906 -4.4289508][-4.4288206 -4.42878 -4.4287744 -4.4287729 -4.4287696 -4.4287348 -4.4286938 -4.4286971 -4.4287739 -4.4288459 -4.4288659 -4.42887 -4.4288816 -4.4289103 -4.4289527][-4.4288487 -4.4288025 -4.4287763 -4.4287562 -4.4287467 -4.42871 -4.428669 -4.42868 -4.4287548 -4.4288235 -4.4288449 -4.4288573 -4.428874 -4.4289069 -4.42895][-4.428884 -4.4288383 -4.4287963 -4.4287581 -4.4287357 -4.4286861 -4.428628 -4.4286332 -4.4287014 -4.4287715 -4.4287982 -4.42882 -4.4288507 -4.428896 -4.4289441][-4.4289179 -4.4288793 -4.42883 -4.4287829 -4.4287486 -4.4286885 -4.4286046 -4.4285874 -4.4286447 -4.4287219 -4.4287581 -4.42879 -4.4288349 -4.4288912 -4.4289436][-4.4289269 -4.4288988 -4.4288568 -4.42881 -4.4287643 -4.4286957 -4.4285965 -4.4285674 -4.4286242 -4.4287066 -4.4287519 -4.4287915 -4.4288383 -4.4288964 -4.428947]]...]
INFO - root - 2017-12-10 03:57:33.347039: step 1410, loss = 2.28, batch loss = 2.23 (39.5 examples/sec; 0.202 sec/batch; 18h:36m:55s remains)
INFO - root - 2017-12-10 03:57:35.425335: step 1420, loss = 2.28, batch loss = 2.23 (38.6 examples/sec; 0.207 sec/batch; 19h:04m:31s remains)
INFO - root - 2017-12-10 03:57:37.486238: step 1430, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:47m:01s remains)
INFO - root - 2017-12-10 03:57:39.552503: step 1440, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:29m:39s remains)
INFO - root - 2017-12-10 03:57:41.588440: step 1450, loss = 2.28, batch loss = 2.23 (38.3 examples/sec; 0.209 sec/batch; 19h:12m:54s remains)
INFO - root - 2017-12-10 03:57:43.639097: step 1460, loss = 2.28, batch loss = 2.23 (38.5 examples/sec; 0.208 sec/batch; 19h:06m:57s remains)
INFO - root - 2017-12-10 03:57:45.726838: step 1470, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 19h:29m:04s remains)
INFO - root - 2017-12-10 03:57:47.784296: step 1480, loss = 2.28, batch loss = 2.23 (38.9 examples/sec; 0.206 sec/batch; 18h:55m:48s remains)
INFO - root - 2017-12-10 03:57:49.824163: step 1490, loss = 2.28, batch loss = 2.23 (39.5 examples/sec; 0.202 sec/batch; 18h:36m:34s remains)
INFO - root - 2017-12-10 03:57:51.868636: step 1500, loss = 2.28, batch loss = 2.23 (38.7 examples/sec; 0.207 sec/batch; 19h:00m:17s remains)
2017-12-10 03:57:52.165391: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288349 -4.4288106 -4.4288116 -4.428813 -4.42881 -4.4288292 -4.4288497 -4.4288635 -4.4288635 -4.4288397 -4.4288187 -4.4288464 -4.4288921 -4.4289131 -4.4289169][-4.4287357 -4.4287024 -4.4287281 -4.4287663 -4.428771 -4.4287829 -4.4287982 -4.4288049 -4.4287977 -4.4287658 -4.4287462 -4.4287906 -4.4288535 -4.42888 -4.428885][-4.4286194 -4.4285827 -4.4286366 -4.42871 -4.4287181 -4.4287124 -4.4287171 -4.4287243 -4.4287138 -4.4286766 -4.4286652 -4.4287214 -4.4287982 -4.4288368 -4.4288497][-4.4285488 -4.4285121 -4.4285841 -4.4286742 -4.4286752 -4.4286404 -4.4286251 -4.4286265 -4.4286265 -4.4286056 -4.4286141 -4.4286857 -4.4287653 -4.4288111 -4.4288311][-4.4285588 -4.4285192 -4.4285893 -4.4286695 -4.4286504 -4.4285955 -4.4285655 -4.4285593 -4.4285674 -4.4285731 -4.4286056 -4.42869 -4.428771 -4.4288177 -4.42884][-4.4286294 -4.4285831 -4.4286375 -4.4286942 -4.4286528 -4.4285817 -4.4285431 -4.4285359 -4.4285502 -4.4285703 -4.4286122 -4.4287043 -4.42879 -4.4288368 -4.4288573][-4.4286852 -4.4286423 -4.4286809 -4.4287152 -4.428659 -4.42857 -4.4285197 -4.4285169 -4.4285388 -4.4285569 -4.4285941 -4.4286819 -4.428771 -4.4288282 -4.4288568][-4.4286852 -4.4286547 -4.4286866 -4.4287009 -4.4286332 -4.4285274 -4.4284658 -4.4284678 -4.4284954 -4.4285045 -4.4285269 -4.4286075 -4.4287014 -4.4287791 -4.4288254][-4.4286594 -4.4286394 -4.4286714 -4.4286728 -4.4286013 -4.4284844 -4.4284105 -4.42841 -4.4284406 -4.4284391 -4.4284353 -4.4285073 -4.42861 -4.428709 -4.4287772][-4.4286547 -4.4286342 -4.4286575 -4.4286542 -4.4285913 -4.4284844 -4.4284105 -4.4284058 -4.4284329 -4.428422 -4.4283876 -4.4284415 -4.4285421 -4.4286523 -4.4287343][-4.4286404 -4.4286132 -4.42863 -4.4286361 -4.42861 -4.42854 -4.4284811 -4.4284678 -4.4284835 -4.4284668 -4.4284215 -4.4284496 -4.4285288 -4.4286289 -4.4287057][-4.4286265 -4.4286003 -4.4286222 -4.4286437 -4.4286532 -4.4286232 -4.4285817 -4.4285622 -4.4285655 -4.4285531 -4.4285145 -4.4285226 -4.4285717 -4.4286518 -4.4287133][-4.4286609 -4.4286447 -4.4286742 -4.4287047 -4.4287271 -4.4287181 -4.42869 -4.4286714 -4.4286709 -4.4286704 -4.4286447 -4.4286375 -4.428658 -4.4287167 -4.4287696][-4.4287386 -4.4287338 -4.4287658 -4.4287977 -4.4288249 -4.4288292 -4.4288135 -4.4288 -4.4288006 -4.4288082 -4.4287944 -4.4287782 -4.4287772 -4.4288135 -4.4288535][-4.4288368 -4.4288387 -4.4288635 -4.42889 -4.4289165 -4.4289284 -4.4289236 -4.4289184 -4.42892 -4.4289274 -4.4289193 -4.4289045 -4.4289002 -4.4289203 -4.4289412]]...]
INFO - root - 2017-12-10 03:57:54.207018: step 1510, loss = 2.28, batch loss = 2.23 (40.5 examples/sec; 0.198 sec/batch; 18h:09m:31s remains)
INFO - root - 2017-12-10 03:57:56.249171: step 1520, loss = 2.28, batch loss = 2.23 (40.1 examples/sec; 0.200 sec/batch; 18h:21m:12s remains)
INFO - root - 2017-12-10 03:57:58.328587: step 1530, loss = 2.28, batch loss = 2.23 (38.9 examples/sec; 0.206 sec/batch; 18h:54m:55s remains)
INFO - root - 2017-12-10 03:58:00.385047: step 1540, loss = 2.28, batch loss = 2.23 (39.4 examples/sec; 0.203 sec/batch; 18h:39m:36s remains)
INFO - root - 2017-12-10 03:58:02.456093: step 1550, loss = 2.28, batch loss = 2.23 (39.1 examples/sec; 0.205 sec/batch; 18h:49m:41s remains)
INFO - root - 2017-12-10 03:58:04.516179: step 1560, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 19h:26m:53s remains)
INFO - root - 2017-12-10 03:58:06.587639: step 1570, loss = 2.28, batch loss = 2.23 (38.2 examples/sec; 0.209 sec/batch; 19h:13m:55s remains)
INFO - root - 2017-12-10 03:58:08.693219: step 1580, loss = 2.28, batch loss = 2.23 (39.3 examples/sec; 0.204 sec/batch; 18h:43m:26s remains)
INFO - root - 2017-12-10 03:58:10.731192: step 1590, loss = 2.28, batch loss = 2.23 (38.9 examples/sec; 0.205 sec/batch; 18h:52m:53s remains)
INFO - root - 2017-12-10 03:58:12.775526: step 1600, loss = 2.28, batch loss = 2.23 (39.6 examples/sec; 0.202 sec/batch; 18h:32m:55s remains)
2017-12-10 03:58:13.056531: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290004 -4.42901 -4.4290218 -4.4290304 -4.4290276 -4.4290109 -4.4289875 -4.4289751 -4.4289808 -4.4289837 -4.4289813 -4.4289761 -4.4289689 -4.4289622 -4.4289613][-4.4289589 -4.4289651 -4.4289837 -4.4290018 -4.4290042 -4.4289918 -4.428967 -4.4289522 -4.4289608 -4.428966 -4.4289646 -4.4289556 -4.4289403 -4.4289274 -4.4289207][-4.428926 -4.4289236 -4.4289436 -4.4289646 -4.42897 -4.4289613 -4.42894 -4.4289293 -4.4289474 -4.4289618 -4.4289656 -4.428957 -4.4289341 -4.4289126 -4.4288979][-4.4289026 -4.4288836 -4.4288912 -4.4289012 -4.4288912 -4.4288707 -4.4288445 -4.4288411 -4.4288793 -4.4289203 -4.4289422 -4.4289427 -4.4289246 -4.4289045 -4.4288864][-4.4288855 -4.4288521 -4.4288373 -4.4288249 -4.4287953 -4.4287553 -4.4287157 -4.4287176 -4.428791 -4.4288707 -4.4289117 -4.4289193 -4.4289079 -4.42889 -4.4288716][-4.4288764 -4.4288287 -4.4287915 -4.4287496 -4.4286819 -4.4286017 -4.4285235 -4.4285107 -4.4286304 -4.4287653 -4.4288411 -4.4288707 -4.4288716 -4.4288607 -4.4288449][-4.4288793 -4.42882 -4.428761 -4.4286823 -4.4285607 -4.4284177 -4.4282613 -4.42821 -4.428371 -4.4285703 -4.4287014 -4.4287844 -4.4288197 -4.4288268 -4.4288249][-4.4289074 -4.4288607 -4.4288149 -4.4287462 -4.4286265 -4.4284682 -4.4282751 -4.4281878 -4.4283257 -4.4285092 -4.4286432 -4.4287453 -4.428793 -4.4288096 -4.4288225][-4.4289331 -4.4289088 -4.4288964 -4.4288731 -4.4287963 -4.4286747 -4.4285212 -4.42844 -4.4285111 -4.4286156 -4.4287 -4.42877 -4.4287958 -4.4288054 -4.4288244][-4.4289179 -4.4289 -4.4289036 -4.428906 -4.4288578 -4.4287663 -4.42866 -4.4286046 -4.4286461 -4.4287033 -4.4287467 -4.4287796 -4.4287834 -4.4287877 -4.4288073][-4.4288874 -4.428865 -4.4288735 -4.4288869 -4.4288559 -4.4287925 -4.4287291 -4.4287028 -4.4287424 -4.4287825 -4.4288011 -4.4288044 -4.4287934 -4.428793 -4.428803][-4.4288611 -4.4288268 -4.4288244 -4.4288321 -4.4288092 -4.428771 -4.4287424 -4.4287405 -4.4287906 -4.4288306 -4.4288383 -4.4288292 -4.4288206 -4.4288192 -4.4288225][-4.4288406 -4.428792 -4.4287753 -4.4287715 -4.4287496 -4.4287276 -4.4287148 -4.4287238 -4.4287872 -4.4288344 -4.4288421 -4.4288306 -4.4288239 -4.4288216 -4.4288216][-4.4288373 -4.4287772 -4.4287443 -4.428731 -4.4287128 -4.4287004 -4.4286933 -4.4287043 -4.4287682 -4.4288197 -4.4288292 -4.4288259 -4.4288259 -4.4288239 -4.4288225][-4.4288607 -4.4288 -4.4287586 -4.4287362 -4.4287157 -4.4287033 -4.4286962 -4.4287047 -4.4287553 -4.4287982 -4.4288125 -4.4288235 -4.4288373 -4.428843 -4.4288459]]...]
INFO - root - 2017-12-10 03:58:15.118970: step 1610, loss = 2.28, batch loss = 2.23 (38.2 examples/sec; 0.210 sec/batch; 19h:16m:24s remains)
INFO - root - 2017-12-10 03:58:17.166134: step 1620, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:46m:13s remains)
INFO - root - 2017-12-10 03:58:19.217269: step 1630, loss = 2.28, batch loss = 2.23 (38.4 examples/sec; 0.208 sec/batch; 19h:08m:35s remains)
INFO - root - 2017-12-10 03:58:21.251852: step 1640, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.202 sec/batch; 18h:32m:04s remains)
INFO - root - 2017-12-10 03:58:23.314138: step 1650, loss = 2.28, batch loss = 2.23 (38.9 examples/sec; 0.206 sec/batch; 18h:54m:50s remains)
INFO - root - 2017-12-10 03:58:25.347477: step 1660, loss = 2.28, batch loss = 2.23 (40.4 examples/sec; 0.198 sec/batch; 18h:12m:43s remains)
INFO - root - 2017-12-10 03:58:27.430725: step 1670, loss = 2.28, batch loss = 2.23 (38.2 examples/sec; 0.209 sec/batch; 19h:14m:24s remains)
INFO - root - 2017-12-10 03:58:29.474542: step 1680, loss = 2.28, batch loss = 2.23 (39.5 examples/sec; 0.202 sec/batch; 18h:36m:16s remains)
INFO - root - 2017-12-10 03:58:31.505616: step 1690, loss = 2.28, batch loss = 2.23 (39.8 examples/sec; 0.201 sec/batch; 18h:27m:31s remains)
INFO - root - 2017-12-10 03:58:33.547905: step 1700, loss = 2.28, batch loss = 2.23 (39.8 examples/sec; 0.201 sec/batch; 18h:28m:40s remains)
2017-12-10 03:58:33.810691: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286108 -4.4285617 -4.428616 -4.4287252 -4.4288316 -4.4289012 -4.4289269 -4.4289231 -4.4289117 -4.4288917 -4.4288874 -4.4288931 -4.4288969 -4.4288721 -4.4288397][-4.4285 -4.4284415 -4.4285212 -4.4286647 -4.4287987 -4.4288778 -4.4289036 -4.4289017 -4.4288907 -4.4288745 -4.4288745 -4.4288845 -4.42889 -4.4288559 -4.42881][-4.4284081 -4.4283471 -4.4284496 -4.4286242 -4.42877 -4.4288487 -4.428863 -4.428853 -4.4288468 -4.4288521 -4.4288664 -4.4288883 -4.4288945 -4.428853 -4.4287953][-4.4284267 -4.42838 -4.4284854 -4.4286528 -4.4287796 -4.4288282 -4.4288025 -4.4287715 -4.4287763 -4.4288149 -4.4288564 -4.4288926 -4.4288979 -4.428853 -4.4287906][-4.4285522 -4.428544 -4.4286294 -4.4287539 -4.4288316 -4.4288192 -4.4287205 -4.4286528 -4.4286752 -4.4287529 -4.4288235 -4.4288764 -4.4288855 -4.42884 -4.4287815][-4.4286966 -4.428721 -4.4287686 -4.4288216 -4.4288278 -4.4287372 -4.4285564 -4.4284568 -4.4285183 -4.4286537 -4.42876 -4.428833 -4.4288507 -4.4288092 -4.4287581][-4.42879 -4.428813 -4.4288177 -4.4288011 -4.4287281 -4.4285412 -4.4282684 -4.428153 -4.4282904 -4.4285088 -4.4286637 -4.4287572 -4.4287872 -4.4287572 -4.4287248][-4.428843 -4.4288507 -4.4288216 -4.4287548 -4.4286113 -4.4283342 -4.4279919 -4.4279 -4.4281273 -4.4284043 -4.4285879 -4.4286928 -4.4287343 -4.428719 -4.4287062][-4.4288855 -4.42887 -4.4288206 -4.42874 -4.4285946 -4.4283409 -4.4280624 -4.4280252 -4.4282284 -4.4284492 -4.4286003 -4.4286847 -4.4287162 -4.4287 -4.4286976][-4.4289002 -4.4288735 -4.4288235 -4.4287581 -4.4286561 -4.4284954 -4.4283414 -4.4283433 -4.4284654 -4.4285946 -4.428688 -4.4287286 -4.4287271 -4.4287004 -4.4287052][-4.4289055 -4.4288793 -4.42884 -4.4287977 -4.42874 -4.4286537 -4.4285817 -4.4285965 -4.4286661 -4.4287343 -4.4287829 -4.4287877 -4.4287658 -4.4287357 -4.428741][-4.4289093 -4.42889 -4.4288621 -4.4288397 -4.4288154 -4.4287767 -4.4287438 -4.42876 -4.4288015 -4.4288354 -4.4288526 -4.428843 -4.4288216 -4.4287939 -4.4287896][-4.4289126 -4.4288979 -4.42888 -4.4288745 -4.428875 -4.4288664 -4.4288573 -4.4288712 -4.4288907 -4.4289017 -4.4289 -4.428884 -4.4288635 -4.4288378 -4.4288259][-4.4289184 -4.4289079 -4.428905 -4.4289112 -4.4289203 -4.4289241 -4.4289246 -4.4289303 -4.4289351 -4.4289317 -4.4289212 -4.4289055 -4.4288907 -4.4288716 -4.4288597][-4.4289279 -4.4289169 -4.4289212 -4.4289341 -4.4289465 -4.4289541 -4.4289546 -4.4289517 -4.4289451 -4.4289346 -4.4289227 -4.4289145 -4.4289107 -4.4289026 -4.4288955]]...]
INFO - root - 2017-12-10 03:58:35.856674: step 1710, loss = 2.28, batch loss = 2.23 (39.6 examples/sec; 0.202 sec/batch; 18h:32m:48s remains)
INFO - root - 2017-12-10 03:58:37.914212: step 1720, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.201 sec/batch; 18h:30m:04s remains)
INFO - root - 2017-12-10 03:58:39.981912: step 1730, loss = 2.28, batch loss = 2.23 (39.9 examples/sec; 0.201 sec/batch; 18h:25m:37s remains)
INFO - root - 2017-12-10 03:58:42.026365: step 1740, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:24m:37s remains)
INFO - root - 2017-12-10 03:58:44.072153: step 1750, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 19h:36m:34s remains)
INFO - root - 2017-12-10 03:58:46.116162: step 1760, loss = 2.28, batch loss = 2.23 (38.7 examples/sec; 0.207 sec/batch; 18h:59m:19s remains)
INFO - root - 2017-12-10 03:58:48.162770: step 1770, loss = 2.28, batch loss = 2.23 (39.6 examples/sec; 0.202 sec/batch; 18h:33m:54s remains)
INFO - root - 2017-12-10 03:58:50.208025: step 1780, loss = 2.28, batch loss = 2.23 (39.1 examples/sec; 0.204 sec/batch; 18h:47m:08s remains)
INFO - root - 2017-12-10 03:58:52.255389: step 1790, loss = 2.28, batch loss = 2.23 (38.7 examples/sec; 0.207 sec/batch; 18h:59m:33s remains)
INFO - root - 2017-12-10 03:58:54.318011: step 1800, loss = 2.28, batch loss = 2.23 (39.9 examples/sec; 0.200 sec/batch; 18h:23m:52s remains)
2017-12-10 03:58:54.593899: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286189 -4.4286723 -4.4287109 -4.4287357 -4.4287062 -4.4286332 -4.4285917 -4.4285674 -4.4285207 -4.4285107 -4.428575 -4.428627 -4.4286785 -4.428721 -4.4287252][-4.4286623 -4.4287248 -4.4287715 -4.4287982 -4.42877 -4.4286919 -4.4286289 -4.4285755 -4.4284997 -4.4284868 -4.4285741 -4.4286466 -4.4287124 -4.4287629 -4.4287667][-4.4287143 -4.4287724 -4.428813 -4.428834 -4.4288082 -4.4287386 -4.4286666 -4.4285927 -4.4284949 -4.4284821 -4.4285927 -4.4286814 -4.4287505 -4.4288006 -4.4288087][-4.4287543 -4.4287949 -4.4288158 -4.4288278 -4.4288 -4.4287357 -4.4286642 -4.4285836 -4.4284773 -4.42847 -4.4286022 -4.4286995 -4.4287672 -4.4288139 -4.4288216][-4.4287591 -4.428782 -4.428791 -4.4287934 -4.4287591 -4.4286995 -4.4286284 -4.42854 -4.4284258 -4.4284282 -4.4285812 -4.4286904 -4.4287577 -4.4288011 -4.4288011][-4.4287472 -4.4287648 -4.4287672 -4.4287577 -4.4287181 -4.42866 -4.42858 -4.4284658 -4.4283185 -4.4283328 -4.4285226 -4.4286613 -4.4287348 -4.4287753 -4.42877][-4.4287071 -4.4287276 -4.42873 -4.4287229 -4.4286952 -4.4286337 -4.4285336 -4.4283695 -4.4281616 -4.4281826 -4.4284334 -4.4286165 -4.4286976 -4.4287348 -4.4287276][-4.4286404 -4.4286709 -4.4286904 -4.4287052 -4.4286947 -4.4286251 -4.4284978 -4.4282837 -4.4280095 -4.42804 -4.4283581 -4.4285913 -4.4286847 -4.4287162 -4.4287043][-4.4286017 -4.4286342 -4.4286733 -4.4287114 -4.4287105 -4.4286327 -4.4284959 -4.4282842 -4.4280248 -4.4280829 -4.4284043 -4.4286318 -4.42872 -4.4287419 -4.428719][-4.4286289 -4.4286466 -4.428689 -4.4287338 -4.4287395 -4.4286757 -4.4285717 -4.4284177 -4.42825 -4.4283128 -4.4285455 -4.4287024 -4.4287624 -4.4287696 -4.4287338][-4.4286933 -4.4287009 -4.4287367 -4.4287753 -4.4287848 -4.4287467 -4.4286819 -4.4285865 -4.4284935 -4.4285307 -4.4286623 -4.4287467 -4.42878 -4.4287815 -4.4287415][-4.4287486 -4.4287596 -4.4287786 -4.4288034 -4.4288235 -4.4288087 -4.428772 -4.4287157 -4.4286556 -4.428659 -4.4287181 -4.4287558 -4.4287744 -4.4287763 -4.4287381][-4.4287777 -4.4287839 -4.4287806 -4.4287858 -4.428812 -4.4288173 -4.4287977 -4.4287648 -4.4287171 -4.4287057 -4.428741 -4.4287596 -4.4287686 -4.428761 -4.4287128][-4.4287887 -4.4287858 -4.4287562 -4.4287419 -4.4287643 -4.4287825 -4.4287691 -4.4287453 -4.4287181 -4.4287157 -4.4287543 -4.4287729 -4.4287729 -4.4287477 -4.4286833][-4.4287896 -4.42878 -4.4287367 -4.4287047 -4.4287171 -4.4287348 -4.42872 -4.4286995 -4.4286962 -4.4287105 -4.4287534 -4.4287767 -4.4287724 -4.4287314 -4.4286656]]...]
INFO - root - 2017-12-10 03:58:56.659939: step 1810, loss = 2.28, batch loss = 2.23 (39.6 examples/sec; 0.202 sec/batch; 18h:32m:19s remains)
INFO - root - 2017-12-10 03:58:58.726330: step 1820, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.202 sec/batch; 18h:31m:04s remains)
INFO - root - 2017-12-10 03:59:00.777890: step 1830, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:46m:07s remains)
INFO - root - 2017-12-10 03:59:02.819520: step 1840, loss = 2.28, batch loss = 2.23 (39.4 examples/sec; 0.203 sec/batch; 18h:39m:53s remains)
INFO - root - 2017-12-10 03:59:04.893018: step 1850, loss = 2.28, batch loss = 2.23 (38.4 examples/sec; 0.209 sec/batch; 19h:09m:07s remains)
INFO - root - 2017-12-10 03:59:06.959528: step 1860, loss = 2.28, batch loss = 2.23 (39.1 examples/sec; 0.205 sec/batch; 18h:48m:16s remains)
INFO - root - 2017-12-10 03:59:09.021726: step 1870, loss = 2.28, batch loss = 2.23 (38.5 examples/sec; 0.208 sec/batch; 19h:05m:45s remains)
INFO - root - 2017-12-10 03:59:11.081215: step 1880, loss = 2.28, batch loss = 2.23 (38.6 examples/sec; 0.207 sec/batch; 19h:02m:13s remains)
INFO - root - 2017-12-10 03:59:13.125716: step 1890, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.211 sec/batch; 19h:20m:38s remains)
INFO - root - 2017-12-10 03:59:15.170674: step 1900, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.211 sec/batch; 19h:25m:10s remains)
2017-12-10 03:59:15.451205: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287333 -4.4286633 -4.4286447 -4.4287033 -4.4287577 -4.4287887 -4.4287944 -4.4288034 -4.4288239 -4.4288239 -4.4288239 -4.4288173 -4.4287777 -4.4287581 -4.4287772][-4.4287271 -4.4286265 -4.4285717 -4.4286175 -4.4286928 -4.4287376 -4.42873 -4.4287386 -4.4287748 -4.4287891 -4.4287953 -4.4287953 -4.4287629 -4.428741 -4.4287477][-4.42871 -4.4285946 -4.4285207 -4.4285483 -4.4286346 -4.4286876 -4.4286695 -4.4286704 -4.4287081 -4.4287338 -4.4287581 -4.4287815 -4.4287663 -4.4287372 -4.4287276][-4.4286842 -4.4285769 -4.4285069 -4.428515 -4.4285913 -4.4286537 -4.4286413 -4.4286346 -4.4286618 -4.4286885 -4.4287243 -4.4287667 -4.42877 -4.4287395 -4.4287205][-4.4286518 -4.4285684 -4.428515 -4.4285131 -4.4285722 -4.4286289 -4.4286251 -4.4286242 -4.4286547 -4.4286833 -4.4287224 -4.4287686 -4.4287763 -4.4287419 -4.4287133][-4.4286447 -4.4285884 -4.4285445 -4.4285264 -4.4285536 -4.4285855 -4.4285822 -4.4285955 -4.4286456 -4.4286871 -4.4287143 -4.4287515 -4.4287519 -4.4287186 -4.4286904][-4.4286437 -4.4286046 -4.4285679 -4.4285336 -4.4285212 -4.4285183 -4.4285049 -4.4285259 -4.4285946 -4.4286408 -4.4286537 -4.428678 -4.4286804 -4.4286661 -4.4286485][-4.4286327 -4.4286046 -4.4285727 -4.4285345 -4.4285011 -4.4284678 -4.428442 -4.428452 -4.4285173 -4.4285665 -4.4285793 -4.428606 -4.4286194 -4.4286256 -4.4286213][-4.4286122 -4.4285855 -4.4285579 -4.4285297 -4.428504 -4.4284682 -4.4284368 -4.4284353 -4.4284773 -4.4285216 -4.4285474 -4.4285846 -4.428606 -4.428617 -4.4286127][-4.4286017 -4.4285707 -4.428546 -4.428525 -4.4285069 -4.4284816 -4.4284539 -4.4284468 -4.4284716 -4.428514 -4.4285545 -4.4286041 -4.4286318 -4.4286385 -4.4286261][-4.4286485 -4.4286194 -4.4286 -4.4285808 -4.4285622 -4.4285407 -4.4285197 -4.4285126 -4.4285254 -4.4285517 -4.4285889 -4.4286456 -4.4286828 -4.4286914 -4.4286737][-4.4287357 -4.428719 -4.4287057 -4.4286871 -4.4286685 -4.42865 -4.4286346 -4.428627 -4.4286289 -4.4286342 -4.428648 -4.4286885 -4.4287219 -4.4287343 -4.4287238][-4.4288478 -4.4288383 -4.4288306 -4.4288149 -4.4287977 -4.4287839 -4.4287748 -4.4287682 -4.4287629 -4.4287548 -4.42875 -4.4287691 -4.4287882 -4.4287996 -4.4287968][-4.4289417 -4.4289341 -4.4289284 -4.4289169 -4.428905 -4.428894 -4.4288855 -4.4288788 -4.428874 -4.4288645 -4.4288545 -4.4288592 -4.4288664 -4.4288721 -4.428874][-4.4289913 -4.4289837 -4.4289823 -4.428977 -4.4289703 -4.4289637 -4.428957 -4.4289522 -4.4289474 -4.4289403 -4.4289308 -4.4289293 -4.4289308 -4.4289341 -4.4289379]]...]
INFO - root - 2017-12-10 03:59:17.487636: step 1910, loss = 2.28, batch loss = 2.23 (38.7 examples/sec; 0.206 sec/batch; 18h:57m:34s remains)
INFO - root - 2017-12-10 03:59:19.530655: step 1920, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 19h:18m:01s remains)
INFO - root - 2017-12-10 03:59:21.599986: step 1930, loss = 2.28, batch loss = 2.23 (39.6 examples/sec; 0.202 sec/batch; 18h:32m:36s remains)
INFO - root - 2017-12-10 03:59:23.639805: step 1940, loss = 2.28, batch loss = 2.23 (38.9 examples/sec; 0.206 sec/batch; 18h:53m:13s remains)
INFO - root - 2017-12-10 03:59:25.685032: step 1950, loss = 2.28, batch loss = 2.23 (39.4 examples/sec; 0.203 sec/batch; 18h:37m:32s remains)
INFO - root - 2017-12-10 03:59:27.738171: step 1960, loss = 2.28, batch loss = 2.23 (39.4 examples/sec; 0.203 sec/batch; 18h:39m:55s remains)
INFO - root - 2017-12-10 03:59:29.768605: step 1970, loss = 2.28, batch loss = 2.23 (40.6 examples/sec; 0.197 sec/batch; 18h:05m:52s remains)
INFO - root - 2017-12-10 03:59:31.813471: step 1980, loss = 2.28, batch loss = 2.23 (38.9 examples/sec; 0.206 sec/batch; 18h:53m:59s remains)
INFO - root - 2017-12-10 03:59:33.858495: step 1990, loss = 2.28, batch loss = 2.23 (38.8 examples/sec; 0.206 sec/batch; 18h:54m:20s remains)
INFO - root - 2017-12-10 03:59:35.892354: step 2000, loss = 2.28, batch loss = 2.23 (38.6 examples/sec; 0.207 sec/batch; 19h:01m:46s remains)
2017-12-10 03:59:36.200395: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288073 -4.4288015 -4.4288163 -4.4288597 -4.4288926 -4.4289021 -4.4289002 -4.428895 -4.428894 -4.4289036 -4.4289112 -4.42891 -4.4289074 -4.4289069 -4.4289041][-4.4288192 -4.4288244 -4.4288416 -4.4288688 -4.4288826 -4.4288845 -4.428884 -4.4288797 -4.428875 -4.4288812 -4.4288945 -4.4289093 -4.4289141 -4.4289179 -4.4289179][-4.4288321 -4.4288464 -4.428865 -4.4288769 -4.4288683 -4.4288583 -4.4288507 -4.428844 -4.42884 -4.4288416 -4.428854 -4.4288745 -4.4288831 -4.4288945 -4.4289026][-4.4288373 -4.4288578 -4.4288774 -4.4288793 -4.4288564 -4.4288344 -4.4288192 -4.4288092 -4.4288096 -4.4288106 -4.4288125 -4.4288278 -4.4288392 -4.4288635 -4.4288831][-4.4288321 -4.4288535 -4.4288726 -4.4288735 -4.428844 -4.4288116 -4.42879 -4.4287839 -4.428793 -4.4287968 -4.4287872 -4.4287896 -4.4288063 -4.4288445 -4.4288735][-4.4288273 -4.4288445 -4.4288616 -4.4288621 -4.4288268 -4.428772 -4.4287324 -4.428731 -4.4287577 -4.4287796 -4.4287643 -4.4287562 -4.4287834 -4.428844 -4.4288816][-4.4288249 -4.4288344 -4.4288464 -4.42884 -4.4287939 -4.4287038 -4.42861 -4.42861 -4.4286919 -4.4287529 -4.4287505 -4.4287381 -4.4287744 -4.4288487 -4.4288917][-4.4288163 -4.4288144 -4.4288216 -4.4288158 -4.428762 -4.4286413 -4.428483 -4.4284739 -4.4286184 -4.428721 -4.428741 -4.4287319 -4.428762 -4.4288349 -4.4288759][-4.4287958 -4.4287868 -4.4287996 -4.4288063 -4.4287639 -4.4286547 -4.4284968 -4.4284759 -4.4286175 -4.428719 -4.4287443 -4.4287362 -4.4287572 -4.4288135 -4.4288411][-4.4287777 -4.42877 -4.4287877 -4.4288049 -4.4287868 -4.4287205 -4.4286289 -4.4286165 -4.4286933 -4.4287515 -4.4287586 -4.4287448 -4.4287596 -4.4288015 -4.4288216][-4.4287729 -4.4287729 -4.4287944 -4.4288182 -4.4288154 -4.4287858 -4.4287434 -4.4287386 -4.42877 -4.42879 -4.4287777 -4.4287562 -4.4287658 -4.4288087 -4.4288282][-4.428771 -4.42878 -4.4288068 -4.4288373 -4.4288406 -4.4288268 -4.4287996 -4.4287915 -4.4288039 -4.4288063 -4.4287844 -4.42876 -4.4287715 -4.4288182 -4.4288349][-4.4287767 -4.4287882 -4.4288249 -4.4288564 -4.4288578 -4.4288459 -4.4288154 -4.4287963 -4.4288039 -4.42881 -4.428792 -4.4287663 -4.4287729 -4.428813 -4.4288259][-4.4288044 -4.4288135 -4.4288454 -4.4288669 -4.4288645 -4.4288526 -4.4288206 -4.4287953 -4.4288054 -4.4288197 -4.42881 -4.4287858 -4.42878 -4.428802 -4.4288044][-4.4288368 -4.4288487 -4.4288692 -4.4288778 -4.4288726 -4.4288621 -4.428833 -4.42881 -4.4288206 -4.4288383 -4.4288335 -4.4288068 -4.4287906 -4.4287963 -4.4287858]]...]
INFO - root - 2017-12-10 03:59:38.240959: step 2010, loss = 2.28, batch loss = 2.23 (40.1 examples/sec; 0.199 sec/batch; 18h:17m:48s remains)
INFO - root - 2017-12-10 03:59:40.279552: step 2020, loss = 2.28, batch loss = 2.23 (38.8 examples/sec; 0.206 sec/batch; 18h:56m:47s remains)
INFO - root - 2017-12-10 03:59:42.318040: step 2030, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.201 sec/batch; 18h:28m:53s remains)
INFO - root - 2017-12-10 03:59:44.375731: step 2040, loss = 2.28, batch loss = 2.23 (38.2 examples/sec; 0.209 sec/batch; 19h:12m:43s remains)
INFO - root - 2017-12-10 03:59:46.416549: step 2050, loss = 2.28, batch loss = 2.23 (39.4 examples/sec; 0.203 sec/batch; 18h:39m:21s remains)
INFO - root - 2017-12-10 03:59:48.455452: step 2060, loss = 2.28, batch loss = 2.23 (38.8 examples/sec; 0.206 sec/batch; 18h:54m:29s remains)
INFO - root - 2017-12-10 03:59:50.499586: step 2070, loss = 2.28, batch loss = 2.23 (39.9 examples/sec; 0.201 sec/batch; 18h:24m:18s remains)
INFO - root - 2017-12-10 03:59:52.535237: step 2080, loss = 2.28, batch loss = 2.23 (39.1 examples/sec; 0.204 sec/batch; 18h:45m:51s remains)
INFO - root - 2017-12-10 03:59:54.596988: step 2090, loss = 2.28, batch loss = 2.23 (38.4 examples/sec; 0.208 sec/batch; 19h:06m:44s remains)
INFO - root - 2017-12-10 03:59:56.632647: step 2100, loss = 2.28, batch loss = 2.23 (38.4 examples/sec; 0.208 sec/batch; 19h:06m:53s remains)
2017-12-10 03:59:56.929286: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289927 -4.4289651 -4.4289193 -4.4288459 -4.4287643 -4.4287148 -4.428709 -4.4287319 -4.42878 -4.4288483 -4.4289064 -4.4289436 -4.4289732 -4.4289994 -4.4290204][-4.4289837 -4.428947 -4.4288888 -4.4287963 -4.4286942 -4.4286337 -4.42863 -4.4286661 -4.4287271 -4.4288087 -4.428884 -4.4289322 -4.428966 -4.4289942 -4.4290166][-4.4289784 -4.4289374 -4.4288621 -4.4287457 -4.428627 -4.4285579 -4.4285512 -4.4285989 -4.4286733 -4.4287648 -4.4288573 -4.4289184 -4.4289575 -4.428988 -4.4290113][-4.4289722 -4.4289293 -4.4288373 -4.4286962 -4.428565 -4.4285 -4.4284911 -4.4285383 -4.42862 -4.428721 -4.4288292 -4.4289041 -4.4289484 -4.4289808 -4.4290061][-4.4289637 -4.4289184 -4.428813 -4.4286437 -4.4284897 -4.4284382 -4.4284458 -4.4284916 -4.4285712 -4.4286814 -4.4287996 -4.4288874 -4.4289389 -4.4289722 -4.4290009][-4.428956 -4.4289083 -4.4287987 -4.4286065 -4.4284225 -4.42838 -4.4284244 -4.4284849 -4.4285588 -4.4286633 -4.4287815 -4.4288692 -4.428925 -4.4289608 -4.4289923][-4.4289551 -4.42891 -4.428812 -4.4286294 -4.4284306 -4.4283676 -4.4284368 -4.4285169 -4.4285884 -4.4286795 -4.42878 -4.4288564 -4.4289083 -4.4289474 -4.4289846][-4.4289584 -4.428925 -4.4288554 -4.4287148 -4.428545 -4.428452 -4.4284897 -4.4285622 -4.4286232 -4.4286966 -4.428781 -4.4288487 -4.428896 -4.428936 -4.428978][-4.4289665 -4.4289451 -4.4289017 -4.42881 -4.4286876 -4.4285865 -4.428566 -4.4286022 -4.4286408 -4.4287014 -4.4287806 -4.4288497 -4.428896 -4.4289312 -4.4289713][-4.42897 -4.4289608 -4.428937 -4.4288788 -4.4287915 -4.4286976 -4.428638 -4.4286318 -4.4286437 -4.4286952 -4.4287791 -4.4288545 -4.4289036 -4.4289336 -4.4289675][-4.4289637 -4.4289646 -4.4289565 -4.4289212 -4.4288597 -4.428782 -4.4287105 -4.4286695 -4.4286489 -4.428679 -4.4287558 -4.4288397 -4.4288983 -4.4289317 -4.4289637][-4.4289594 -4.4289646 -4.4289641 -4.4289451 -4.4289045 -4.4288483 -4.4287848 -4.4287257 -4.4286747 -4.4286757 -4.4287362 -4.42882 -4.4288888 -4.4289289 -4.4289622][-4.4289589 -4.4289637 -4.4289637 -4.4289513 -4.428925 -4.4288874 -4.42884 -4.4287848 -4.4287276 -4.4287081 -4.4287481 -4.4288216 -4.4288878 -4.42893 -4.4289632][-4.4289627 -4.4289622 -4.42896 -4.4289503 -4.4289336 -4.4289112 -4.4288793 -4.42884 -4.4287996 -4.4287763 -4.428793 -4.4288435 -4.4288979 -4.4289346 -4.428966][-4.4289603 -4.428956 -4.4289503 -4.4289436 -4.4289327 -4.4289217 -4.4289055 -4.4288855 -4.4288635 -4.4288445 -4.4288464 -4.428875 -4.4289112 -4.42894 -4.4289641]]...]
INFO - root - 2017-12-10 03:59:58.975128: step 2110, loss = 2.28, batch loss = 2.23 (38.8 examples/sec; 0.206 sec/batch; 18h:56m:38s remains)
INFO - root - 2017-12-10 04:00:01.036706: step 2120, loss = 2.28, batch loss = 2.23 (40.3 examples/sec; 0.198 sec/batch; 18h:12m:49s remains)
INFO - root - 2017-12-10 04:00:03.076320: step 2130, loss = 2.28, batch loss = 2.23 (39.8 examples/sec; 0.201 sec/batch; 18h:26m:57s remains)
INFO - root - 2017-12-10 04:00:05.128671: step 2140, loss = 2.28, batch loss = 2.23 (38.8 examples/sec; 0.206 sec/batch; 18h:56m:00s remains)
INFO - root - 2017-12-10 04:00:07.180732: step 2150, loss = 2.28, batch loss = 2.23 (39.0 examples/sec; 0.205 sec/batch; 18h:49m:38s remains)
INFO - root - 2017-12-10 04:00:09.242143: step 2160, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 19h:17m:15s remains)
INFO - root - 2017-12-10 04:00:11.298866: step 2170, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.202 sec/batch; 18h:30m:31s remains)
INFO - root - 2017-12-10 04:00:13.337673: step 2180, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:43m:10s remains)
INFO - root - 2017-12-10 04:00:15.377015: step 2190, loss = 2.28, batch loss = 2.23 (39.1 examples/sec; 0.204 sec/batch; 18h:45m:32s remains)
INFO - root - 2017-12-10 04:00:17.452587: step 2200, loss = 2.28, batch loss = 2.23 (38.6 examples/sec; 0.208 sec/batch; 19h:02m:20s remains)
2017-12-10 04:00:17.751154: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288921 -4.4289012 -4.4289341 -4.428936 -4.4288855 -4.428791 -4.4286833 -4.4286075 -4.4286079 -4.428668 -4.4287167 -4.4287477 -4.4287696 -4.4288015 -4.4288244][-4.4288888 -4.4289002 -4.4289341 -4.4289327 -4.4288721 -4.4287691 -4.4286604 -4.42859 -4.4286032 -4.428669 -4.4287267 -4.428772 -4.4288049 -4.428843 -4.4288611][-4.4288898 -4.4289117 -4.4289474 -4.4289374 -4.4288578 -4.42874 -4.428617 -4.4285522 -4.4285779 -4.4286594 -4.4287386 -4.4287963 -4.4288416 -4.4288912 -4.4289112][-4.4288931 -4.4289241 -4.4289575 -4.4289322 -4.4288363 -4.4286933 -4.4285426 -4.4284678 -4.4285064 -4.4286194 -4.4287314 -4.4288087 -4.4288664 -4.4289184 -4.4289351][-4.4288979 -4.4289336 -4.4289594 -4.4289169 -4.4288049 -4.4286323 -4.4284511 -4.4283662 -4.4284291 -4.4285827 -4.4287271 -4.4288249 -4.4288883 -4.4289293 -4.4289374][-4.4288979 -4.4289427 -4.4289584 -4.4289 -4.42877 -4.4285684 -4.4283547 -4.4282565 -4.4283481 -4.4285512 -4.4287276 -4.428843 -4.4289055 -4.4289341 -4.4289293][-4.4288921 -4.428946 -4.4289584 -4.4288955 -4.4287596 -4.428535 -4.4282837 -4.4281497 -4.4282451 -4.4284897 -4.4287004 -4.428843 -4.4289136 -4.4289374 -4.4289207][-4.4289026 -4.428956 -4.4289651 -4.428906 -4.42878 -4.42856 -4.4282928 -4.428112 -4.4281807 -4.4284339 -4.4286723 -4.4288392 -4.4289203 -4.4289455 -4.4289222][-4.4289346 -4.4289804 -4.4289837 -4.4289308 -4.4288254 -4.4286351 -4.4283829 -4.4281754 -4.4282064 -4.4284325 -4.4286728 -4.4288468 -4.4289322 -4.428956 -4.4289322][-4.4289622 -4.4290028 -4.4290071 -4.4289665 -4.428885 -4.428731 -4.4285078 -4.4282975 -4.4282875 -4.4284687 -4.4286928 -4.4288549 -4.4289393 -4.4289665 -4.4289484][-4.4289718 -4.4290075 -4.4290161 -4.4289894 -4.4289322 -4.4288187 -4.428638 -4.4284458 -4.4284039 -4.4285326 -4.4287219 -4.4288583 -4.4289355 -4.4289718 -4.4289565][-4.4289689 -4.4289923 -4.4290032 -4.4289889 -4.42895 -4.4288726 -4.4287415 -4.4285955 -4.4285502 -4.4286304 -4.4287653 -4.4288659 -4.4289279 -4.428967 -4.4289575][-4.42897 -4.4289751 -4.42898 -4.4289718 -4.4289432 -4.4288859 -4.4287977 -4.4287057 -4.4286823 -4.4287372 -4.4288239 -4.4288874 -4.4289308 -4.4289622 -4.4289546][-4.4289703 -4.42896 -4.4289556 -4.4289484 -4.4289236 -4.428875 -4.4288135 -4.4287643 -4.42876 -4.4288068 -4.4288654 -4.428906 -4.4289341 -4.4289565 -4.42895][-4.4289675 -4.4289474 -4.4289346 -4.428925 -4.4289026 -4.428865 -4.4288249 -4.4287987 -4.4288025 -4.4288387 -4.4288821 -4.4289117 -4.4289322 -4.4289484 -4.4289427]]...]
INFO - root - 2017-12-10 04:00:19.794437: step 2210, loss = 2.28, batch loss = 2.23 (39.0 examples/sec; 0.205 sec/batch; 18h:49m:27s remains)
INFO - root - 2017-12-10 04:00:21.861891: step 2220, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 19h:26m:32s remains)
INFO - root - 2017-12-10 04:00:23.918715: step 2230, loss = 2.28, batch loss = 2.23 (39.5 examples/sec; 0.203 sec/batch; 18h:34m:44s remains)
INFO - root - 2017-12-10 04:00:25.961657: step 2240, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:44m:34s remains)
INFO - root - 2017-12-10 04:00:28.034206: step 2250, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:23m:05s remains)
INFO - root - 2017-12-10 04:00:30.102419: step 2260, loss = 2.28, batch loss = 2.23 (39.4 examples/sec; 0.203 sec/batch; 18h:37m:27s remains)
INFO - root - 2017-12-10 04:00:32.155860: step 2270, loss = 2.28, batch loss = 2.23 (39.4 examples/sec; 0.203 sec/batch; 18h:37m:19s remains)
INFO - root - 2017-12-10 04:00:34.216390: step 2280, loss = 2.28, batch loss = 2.23 (38.7 examples/sec; 0.206 sec/batch; 18h:56m:24s remains)
INFO - root - 2017-12-10 04:00:36.256787: step 2290, loss = 2.28, batch loss = 2.23 (39.6 examples/sec; 0.202 sec/batch; 18h:32m:28s remains)
INFO - root - 2017-12-10 04:00:38.355860: step 2300, loss = 2.28, batch loss = 2.23 (39.1 examples/sec; 0.205 sec/batch; 18h:47m:25s remains)
2017-12-10 04:00:38.656066: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288321 -4.4288135 -4.4287882 -4.428751 -4.4287171 -4.4286757 -4.4286327 -4.4286108 -4.4286051 -4.4285975 -4.4285588 -4.4285178 -4.428534 -4.4286051 -4.428679][-4.4288263 -4.428792 -4.4287581 -4.428719 -4.4286757 -4.4286194 -4.4285679 -4.4285259 -4.4284911 -4.4284911 -4.4285007 -4.4285264 -4.4285855 -4.428679 -4.4287567][-4.4287925 -4.4287343 -4.4286914 -4.4286604 -4.4286189 -4.4285622 -4.4285083 -4.42845 -4.4283957 -4.4284163 -4.4284811 -4.4285555 -4.42864 -4.4287434 -4.4288197][-4.4287682 -4.4286919 -4.4286466 -4.42862 -4.4285779 -4.4285111 -4.4284534 -4.4284077 -4.4283729 -4.4284225 -4.4285183 -4.4286122 -4.4286952 -4.4287806 -4.4288373][-4.428751 -4.4286761 -4.42863 -4.4285994 -4.4285531 -4.4284739 -4.4284072 -4.4283895 -4.4284148 -4.4284925 -4.4285908 -4.4286833 -4.4287581 -4.4288173 -4.4288511][-4.4287286 -4.4286704 -4.4286189 -4.4285765 -4.4285154 -4.4284058 -4.428288 -4.4282932 -4.4284143 -4.4285398 -4.4286413 -4.4287348 -4.4288092 -4.4288507 -4.428864][-4.4287138 -4.4286766 -4.4286208 -4.4285455 -4.4284377 -4.4282594 -4.4280796 -4.4281368 -4.4283733 -4.428555 -4.4286728 -4.4287734 -4.4288464 -4.4288745 -4.4288754][-4.4286814 -4.4286494 -4.4285893 -4.4284873 -4.4283538 -4.4281716 -4.4280353 -4.428153 -4.428422 -4.4286075 -4.42871 -4.4287934 -4.4288559 -4.4288764 -4.4288778][-4.428616 -4.4285932 -4.4285564 -4.428494 -4.4284186 -4.4283366 -4.4283113 -4.4284191 -4.4285927 -4.4287024 -4.4287639 -4.4288116 -4.4288511 -4.4288659 -4.4288774][-4.4285655 -4.4285793 -4.428587 -4.4285784 -4.4285622 -4.4285502 -4.4285736 -4.4286413 -4.4287252 -4.4287791 -4.4288154 -4.4288368 -4.4288535 -4.4288654 -4.4288864][-4.4285851 -4.4286261 -4.4286594 -4.4286723 -4.4286728 -4.4286723 -4.4286976 -4.4287424 -4.4287825 -4.428813 -4.4288445 -4.4288645 -4.4288716 -4.4288859 -4.4289079][-4.4286432 -4.4286861 -4.4287214 -4.4287362 -4.428731 -4.4287162 -4.4287295 -4.428762 -4.4287829 -4.4288087 -4.4288478 -4.4288731 -4.4288878 -4.4289079 -4.4289293][-4.4287052 -4.4287281 -4.4287477 -4.42875 -4.42874 -4.4287243 -4.4287362 -4.4287653 -4.4287863 -4.4288116 -4.4288483 -4.4288793 -4.4289055 -4.42893 -4.428947][-4.42872 -4.42872 -4.4287262 -4.4287248 -4.4287267 -4.4287343 -4.4287581 -4.4287887 -4.4288082 -4.4288273 -4.4288592 -4.4288917 -4.4289217 -4.4289432 -4.4289556][-4.4287076 -4.4286971 -4.4286971 -4.4287052 -4.428731 -4.4287667 -4.4287949 -4.4288182 -4.4288349 -4.4288564 -4.428884 -4.4289088 -4.4289327 -4.4289503 -4.428957]]...]
INFO - root - 2017-12-10 04:00:40.717651: step 2310, loss = 2.28, batch loss = 2.23 (38.8 examples/sec; 0.206 sec/batch; 18h:54m:36s remains)
INFO - root - 2017-12-10 04:00:42.754556: step 2320, loss = 2.28, batch loss = 2.23 (40.2 examples/sec; 0.199 sec/batch; 18h:15m:23s remains)
INFO - root - 2017-12-10 04:00:44.800691: step 2330, loss = 2.28, batch loss = 2.23 (38.7 examples/sec; 0.207 sec/batch; 18h:56m:29s remains)
INFO - root - 2017-12-10 04:00:46.858294: step 2340, loss = 2.28, batch loss = 2.23 (39.5 examples/sec; 0.203 sec/batch; 18h:35m:25s remains)
INFO - root - 2017-12-10 04:00:48.938685: step 2350, loss = 2.28, batch loss = 2.23 (39.8 examples/sec; 0.201 sec/batch; 18h:25m:15s remains)
INFO - root - 2017-12-10 04:00:50.987614: step 2360, loss = 2.28, batch loss = 2.23 (38.5 examples/sec; 0.208 sec/batch; 19h:03m:29s remains)
INFO - root - 2017-12-10 04:00:53.018616: step 2370, loss = 2.28, batch loss = 2.23 (40.4 examples/sec; 0.198 sec/batch; 18h:10m:49s remains)
INFO - root - 2017-12-10 04:00:55.061848: step 2380, loss = 2.28, batch loss = 2.23 (39.5 examples/sec; 0.203 sec/batch; 18h:34m:44s remains)
INFO - root - 2017-12-10 04:00:57.102381: step 2390, loss = 2.28, batch loss = 2.23 (39.1 examples/sec; 0.205 sec/batch; 18h:45m:32s remains)
INFO - root - 2017-12-10 04:00:59.140923: step 2400, loss = 2.28, batch loss = 2.23 (39.0 examples/sec; 0.205 sec/batch; 18h:48m:45s remains)
2017-12-10 04:00:59.445848: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428659 -4.4286618 -4.428709 -4.4287543 -4.42878 -4.4287944 -4.4287877 -4.4287796 -4.4287786 -4.4287977 -4.428823 -4.4288368 -4.4288292 -4.4288015 -4.4287844][-4.4286475 -4.4286437 -4.4286876 -4.4287338 -4.4287658 -4.42879 -4.428803 -4.42881 -4.4288087 -4.4288177 -4.428833 -4.4288368 -4.4288225 -4.4287949 -4.4287839][-4.4287004 -4.4286909 -4.4287181 -4.4287519 -4.428782 -4.4288096 -4.4288335 -4.4288468 -4.4288392 -4.4288373 -4.428843 -4.4288378 -4.4288225 -4.4287949 -4.4287815][-4.4287534 -4.4287491 -4.4287696 -4.428793 -4.4288149 -4.4288306 -4.4288425 -4.4288468 -4.4288273 -4.4288239 -4.4288325 -4.4288278 -4.4288173 -4.428791 -4.4287686][-4.428781 -4.4287872 -4.42881 -4.4288239 -4.4288297 -4.428823 -4.4288073 -4.4287863 -4.4287515 -4.4287553 -4.4287872 -4.4288096 -4.4288211 -4.4288025 -4.4287729][-4.4287653 -4.428772 -4.4287953 -4.4288096 -4.4288044 -4.4287715 -4.4287162 -4.4286537 -4.4286036 -4.4286294 -4.4287043 -4.4287724 -4.4288182 -4.4288173 -4.4287906][-4.4287148 -4.4287148 -4.4287395 -4.4287553 -4.4287357 -4.4286675 -4.4285622 -4.4284587 -4.4284091 -4.4284844 -4.4286203 -4.428741 -4.4288173 -4.4288406 -4.4288268][-4.4286795 -4.4286752 -4.4287033 -4.4287195 -4.4286842 -4.4285874 -4.42845 -4.4283228 -4.428287 -4.4284043 -4.4285703 -4.42871 -4.428802 -4.4288468 -4.4288549][-4.4286962 -4.4286923 -4.4287229 -4.4287405 -4.4287052 -4.428617 -4.4285026 -4.42841 -4.4284005 -4.4284978 -4.4286218 -4.4287252 -4.428802 -4.4288583 -4.42888][-4.4287372 -4.4287291 -4.4287515 -4.428772 -4.4287562 -4.4287052 -4.4286466 -4.4286103 -4.428617 -4.4286671 -4.4287257 -4.4287786 -4.4288306 -4.4288883 -4.4289179][-4.4287996 -4.4287815 -4.4287848 -4.4287958 -4.4287992 -4.4287829 -4.4287682 -4.4287691 -4.4287777 -4.42879 -4.4288063 -4.4288235 -4.4288568 -4.4289093 -4.4289384][-4.4288464 -4.4288249 -4.4288177 -4.4288197 -4.428833 -4.4288387 -4.4288516 -4.4288659 -4.4288645 -4.4288564 -4.4288487 -4.4288421 -4.4288573 -4.428895 -4.4289179][-4.4288664 -4.4288521 -4.4288445 -4.4288445 -4.4288626 -4.4288812 -4.4289031 -4.4289131 -4.4288974 -4.4288735 -4.4288473 -4.42882 -4.4288144 -4.4288321 -4.4288492][-4.4288635 -4.42886 -4.4288578 -4.4288597 -4.4288807 -4.4289031 -4.4289231 -4.4289222 -4.4288931 -4.4288564 -4.4288125 -4.4287643 -4.4287391 -4.428741 -4.4287596][-4.4288359 -4.4288383 -4.4288449 -4.4288507 -4.4288678 -4.4288855 -4.428905 -4.4288974 -4.4288611 -4.4288187 -4.428771 -4.428719 -4.42869 -4.4286895 -4.4287152]]...]
INFO - root - 2017-12-10 04:01:01.514468: step 2410, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:22m:13s remains)
INFO - root - 2017-12-10 04:01:03.572378: step 2420, loss = 2.28, batch loss = 2.23 (39.4 examples/sec; 0.203 sec/batch; 18h:36m:26s remains)
INFO - root - 2017-12-10 04:01:05.645316: step 2430, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 19h:23m:47s remains)
INFO - root - 2017-12-10 04:01:07.702246: step 2440, loss = 2.28, batch loss = 2.23 (39.4 examples/sec; 0.203 sec/batch; 18h:36m:06s remains)
INFO - root - 2017-12-10 04:01:09.738556: step 2450, loss = 2.28, batch loss = 2.23 (38.9 examples/sec; 0.206 sec/batch; 18h:51m:17s remains)
INFO - root - 2017-12-10 04:01:11.778785: step 2460, loss = 2.28, batch loss = 2.23 (38.2 examples/sec; 0.210 sec/batch; 19h:13m:21s remains)
INFO - root - 2017-12-10 04:01:13.837131: step 2470, loss = 2.28, batch loss = 2.23 (39.9 examples/sec; 0.200 sec/batch; 18h:22m:25s remains)
INFO - root - 2017-12-10 04:01:15.891608: step 2480, loss = 2.28, batch loss = 2.23 (39.4 examples/sec; 0.203 sec/batch; 18h:36m:04s remains)
INFO - root - 2017-12-10 04:01:17.930156: step 2490, loss = 2.28, batch loss = 2.23 (40.8 examples/sec; 0.196 sec/batch; 17h:59m:30s remains)
INFO - root - 2017-12-10 04:01:19.995728: step 2500, loss = 2.28, batch loss = 2.23 (39.8 examples/sec; 0.201 sec/batch; 18h:24m:55s remains)
2017-12-10 04:01:20.261434: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289312 -4.4289203 -4.4289112 -4.4289041 -4.4289045 -4.4289083 -4.4289083 -4.428906 -4.4289064 -4.4289055 -4.4289045 -4.4289036 -4.4289055 -4.4289117 -4.4289203][-4.4289279 -4.4289093 -4.4288931 -4.4288826 -4.4288797 -4.4288783 -4.428875 -4.4288731 -4.4288826 -4.4288912 -4.428894 -4.4288869 -4.4288812 -4.4288831 -4.4288921][-4.4289179 -4.4288797 -4.4288545 -4.4288354 -4.4288306 -4.4288259 -4.4288139 -4.428813 -4.4288363 -4.4288621 -4.4288721 -4.4288626 -4.42885 -4.4288459 -4.4288526][-4.428884 -4.4288235 -4.428781 -4.4287519 -4.4287429 -4.4287324 -4.4287081 -4.4287086 -4.4287519 -4.4287968 -4.4288158 -4.4288111 -4.4287977 -4.4287877 -4.4287939][-4.4288411 -4.4287667 -4.4287066 -4.42867 -4.4286566 -4.4286284 -4.4285841 -4.4285793 -4.428647 -4.4287157 -4.4287443 -4.4287448 -4.4287343 -4.4287238 -4.4287319][-4.4287982 -4.4287286 -4.4286575 -4.4286051 -4.4285707 -4.4285178 -4.4284477 -4.42842 -4.4285011 -4.4286003 -4.4286542 -4.4286766 -4.4286814 -4.4286847 -4.4287004][-4.4287686 -4.4287124 -4.4286404 -4.4285665 -4.4284964 -4.428412 -4.4282994 -4.4282222 -4.4282975 -4.4284482 -4.4285479 -4.4286075 -4.4286385 -4.4286656 -4.4286947][-4.4287577 -4.4287152 -4.4286604 -4.4285769 -4.4284749 -4.4283686 -4.428225 -4.4280968 -4.4281383 -4.4283257 -4.4284692 -4.42855 -4.4285965 -4.4286447 -4.4286914][-4.4287791 -4.4287419 -4.4287066 -4.4286332 -4.4285355 -4.4284377 -4.4283128 -4.4281907 -4.4282088 -4.428349 -4.4284663 -4.4285269 -4.428565 -4.428618 -4.4286819][-4.42881 -4.4287777 -4.4287572 -4.4287024 -4.4286146 -4.42853 -4.4284391 -4.4283581 -4.4283609 -4.4284258 -4.4284806 -4.4285145 -4.4285407 -4.4285808 -4.4286528][-4.4288082 -4.4287739 -4.4287567 -4.4287271 -4.4286671 -4.4285994 -4.4285374 -4.4284868 -4.428473 -4.4284754 -4.428473 -4.4284892 -4.4285135 -4.4285479 -4.4286184][-4.428802 -4.4287643 -4.4287429 -4.428731 -4.4287 -4.4286528 -4.4286113 -4.4285884 -4.4285765 -4.4285493 -4.4285135 -4.4285116 -4.4285431 -4.4285784 -4.4286342][-4.4288368 -4.4288054 -4.4287791 -4.4287658 -4.4287453 -4.4287167 -4.4286923 -4.4286876 -4.4286833 -4.4286575 -4.428618 -4.4286084 -4.4286418 -4.4286771 -4.4287152][-4.4288807 -4.4288611 -4.4288378 -4.4288239 -4.428802 -4.4287882 -4.4287806 -4.4287863 -4.428793 -4.4287834 -4.42875 -4.4287386 -4.4287653 -4.428792 -4.4288154][-4.4289145 -4.4289093 -4.4288993 -4.4288874 -4.4288726 -4.4288707 -4.42887 -4.4288821 -4.4288936 -4.4288969 -4.4288745 -4.4288635 -4.4288764 -4.4288893 -4.4289007]]...]
INFO - root - 2017-12-10 04:01:22.302944: step 2510, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.201 sec/batch; 18h:27m:45s remains)
INFO - root - 2017-12-10 04:01:24.380551: step 2520, loss = 2.28, batch loss = 2.23 (39.8 examples/sec; 0.201 sec/batch; 18h:24m:34s remains)
INFO - root - 2017-12-10 04:01:26.422470: step 2530, loss = 2.28, batch loss = 2.23 (40.5 examples/sec; 0.198 sec/batch; 18h:07m:25s remains)
INFO - root - 2017-12-10 04:01:28.467159: step 2540, loss = 2.28, batch loss = 2.23 (39.3 examples/sec; 0.204 sec/batch; 18h:40m:41s remains)
INFO - root - 2017-12-10 04:01:30.506861: step 2550, loss = 2.28, batch loss = 2.23 (38.8 examples/sec; 0.206 sec/batch; 18h:54m:23s remains)
INFO - root - 2017-12-10 04:01:32.544413: step 2560, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.202 sec/batch; 18h:28m:22s remains)
INFO - root - 2017-12-10 04:01:34.619523: step 2570, loss = 2.28, batch loss = 2.23 (39.0 examples/sec; 0.205 sec/batch; 18h:49m:24s remains)
INFO - root - 2017-12-10 04:01:36.691104: step 2580, loss = 2.28, batch loss = 2.23 (38.4 examples/sec; 0.208 sec/batch; 19h:05m:21s remains)
INFO - root - 2017-12-10 04:01:38.750299: step 2590, loss = 2.28, batch loss = 2.23 (39.1 examples/sec; 0.205 sec/batch; 18h:44m:27s remains)
INFO - root - 2017-12-10 04:01:40.786243: step 2600, loss = 2.28, batch loss = 2.23 (39.1 examples/sec; 0.205 sec/batch; 18h:45m:33s remains)
2017-12-10 04:01:41.071564: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289565 -4.4289656 -4.4289789 -4.428987 -4.4289913 -4.4289923 -4.4289918 -4.4289918 -4.4289913 -4.4289913 -4.4289908 -4.4289889 -4.4289875 -4.428988 -4.4289894][-4.4288621 -4.4288807 -4.4289088 -4.4289327 -4.4289479 -4.4289532 -4.4289541 -4.4289565 -4.4289589 -4.42896 -4.4289589 -4.4289536 -4.4289441 -4.4289389 -4.4289374][-4.4287271 -4.428751 -4.4287987 -4.4288487 -4.4288859 -4.4289031 -4.4289093 -4.4289155 -4.4289227 -4.4289293 -4.4289312 -4.4289227 -4.4289083 -4.428896 -4.4288869][-4.4285717 -4.4285865 -4.428648 -4.4287224 -4.4287825 -4.4288111 -4.4288211 -4.428833 -4.4288516 -4.42887 -4.4288821 -4.4288788 -4.4288673 -4.4288559 -4.4288478][-4.4284544 -4.4284449 -4.4285007 -4.4285827 -4.4286513 -4.42868 -4.4286852 -4.4287019 -4.4287376 -4.4287767 -4.4288049 -4.4288116 -4.4288092 -4.4288063 -4.4288096][-4.4284306 -4.4283895 -4.4284177 -4.42848 -4.42853 -4.4285345 -4.4285159 -4.4285293 -4.4285836 -4.4286518 -4.4287081 -4.4287319 -4.428741 -4.4287515 -4.428771][-4.4285326 -4.4284835 -4.4284873 -4.4285126 -4.4285192 -4.4284773 -4.4284139 -4.4283991 -4.4284534 -4.428535 -4.4286132 -4.4286561 -4.4286776 -4.4286971 -4.4287295][-4.4286656 -4.428638 -4.4286461 -4.4286628 -4.4286566 -4.4286065 -4.4285316 -4.428493 -4.4285188 -4.4285746 -4.4286361 -4.4286685 -4.4286795 -4.4286885 -4.4287152][-4.42875 -4.4287505 -4.4287772 -4.428803 -4.4288068 -4.4287744 -4.4287167 -4.42868 -4.4286857 -4.4287162 -4.4287548 -4.428772 -4.4287686 -4.428762 -4.4287734][-4.4287658 -4.4287853 -4.4288321 -4.4288754 -4.4289002 -4.428896 -4.4288673 -4.4288454 -4.4288478 -4.4288645 -4.428885 -4.4288864 -4.4288692 -4.4288468 -4.4288397][-4.4287195 -4.4287448 -4.4288044 -4.4288669 -4.4289131 -4.4289351 -4.4289327 -4.4289269 -4.4289317 -4.4289436 -4.4289546 -4.4289432 -4.4289088 -4.4288707 -4.4288492][-4.4286218 -4.428637 -4.4286976 -4.428772 -4.4288335 -4.4288726 -4.42889 -4.4289002 -4.4289131 -4.4289227 -4.4289265 -4.4289031 -4.4288478 -4.428791 -4.4287624][-4.4284973 -4.4284897 -4.4285383 -4.4286103 -4.4286733 -4.4287162 -4.4287391 -4.4287572 -4.4287767 -4.42879 -4.4287934 -4.4287624 -4.4286909 -4.42862 -4.4285917][-4.4284582 -4.4284344 -4.4284687 -4.4285288 -4.4285822 -4.428618 -4.4286356 -4.4286489 -4.4286656 -4.4286771 -4.4286795 -4.4286466 -4.4285707 -4.4284978 -4.4284739][-4.4285684 -4.4285469 -4.4285717 -4.428616 -4.4286537 -4.4286776 -4.4286871 -4.4286947 -4.4287062 -4.4287157 -4.428719 -4.4286962 -4.4286394 -4.4285827 -4.4285665]]...]
INFO - root - 2017-12-10 04:01:43.117208: step 2610, loss = 2.28, batch loss = 2.23 (40.0 examples/sec; 0.200 sec/batch; 18h:19m:57s remains)
INFO - root - 2017-12-10 04:01:45.171782: step 2620, loss = 2.28, batch loss = 2.23 (39.8 examples/sec; 0.201 sec/batch; 18h:25m:30s remains)
INFO - root - 2017-12-10 04:01:47.247754: step 2630, loss = 2.28, batch loss = 2.23 (39.6 examples/sec; 0.202 sec/batch; 18h:30m:41s remains)
INFO - root - 2017-12-10 04:01:49.279885: step 2640, loss = 2.28, batch loss = 2.23 (39.8 examples/sec; 0.201 sec/batch; 18h:26m:19s remains)
INFO - root - 2017-12-10 04:01:51.326969: step 2650, loss = 2.28, batch loss = 2.23 (39.1 examples/sec; 0.205 sec/batch; 18h:45m:39s remains)
INFO - root - 2017-12-10 04:01:53.404036: step 2660, loss = 2.28, batch loss = 2.23 (38.9 examples/sec; 0.205 sec/batch; 18h:49m:10s remains)
INFO - root - 2017-12-10 04:01:55.473293: step 2670, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:42m:57s remains)
INFO - root - 2017-12-10 04:01:57.551796: step 2680, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:42m:24s remains)
INFO - root - 2017-12-10 04:01:59.627553: step 2690, loss = 2.28, batch loss = 2.23 (39.8 examples/sec; 0.201 sec/batch; 18h:24m:18s remains)
INFO - root - 2017-12-10 04:02:01.668778: step 2700, loss = 2.28, batch loss = 2.23 (40.5 examples/sec; 0.198 sec/batch; 18h:05m:57s remains)
2017-12-10 04:02:01.950679: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289551 -4.4289474 -4.4289508 -4.4289536 -4.4289474 -4.4289327 -4.4289126 -4.4288993 -4.4288974 -4.4289074 -4.428925 -4.4289355 -4.4289317 -4.4289126 -4.4288859][-4.4289746 -4.428967 -4.4289675 -4.4289646 -4.4289532 -4.4289308 -4.4289045 -4.4288874 -4.4288855 -4.4288979 -4.4289217 -4.4289436 -4.4289494 -4.4289417 -4.4289269][-4.4289865 -4.4289751 -4.4289689 -4.4289551 -4.4289284 -4.4288888 -4.4288526 -4.428834 -4.4288349 -4.4288516 -4.4288826 -4.4289179 -4.4289408 -4.4289484 -4.428947][-4.4289885 -4.428968 -4.4289455 -4.4289088 -4.428853 -4.4287825 -4.4287257 -4.4287024 -4.4287133 -4.4287457 -4.4287982 -4.4288588 -4.4289045 -4.4289279 -4.4289403][-4.4289641 -4.4289289 -4.428884 -4.4288177 -4.4287257 -4.4286094 -4.4285064 -4.4284663 -4.4285011 -4.4285769 -4.42867 -4.4287658 -4.4288383 -4.4288793 -4.4289045][-4.4289007 -4.4288521 -4.4287887 -4.4286971 -4.4285574 -4.4283738 -4.428196 -4.42813 -4.4282188 -4.4283719 -4.428524 -4.4286528 -4.4287453 -4.4288049 -4.4288435][-4.4288015 -4.428741 -4.4286613 -4.4285407 -4.4283562 -4.428113 -4.4278712 -4.4277945 -4.42797 -4.428215 -4.4284115 -4.4285555 -4.4286561 -4.4287257 -4.4287772][-4.4286985 -4.4286337 -4.4285436 -4.4284067 -4.4282112 -4.4279757 -4.4277587 -4.4277129 -4.4279132 -4.4281678 -4.4283586 -4.4284925 -4.4285851 -4.4286556 -4.4287195][-4.4286566 -4.4285975 -4.428515 -4.4283972 -4.4282508 -4.4280987 -4.4279723 -4.4279485 -4.4280739 -4.4282494 -4.4283929 -4.4284992 -4.428576 -4.4286389 -4.4287033][-4.428688 -4.4286494 -4.4286008 -4.4285321 -4.4284496 -4.4283719 -4.4283066 -4.4282813 -4.4283338 -4.4284315 -4.4285254 -4.4286046 -4.4286633 -4.4287105 -4.4287596][-4.4287667 -4.4287481 -4.4287333 -4.42871 -4.4286742 -4.4286375 -4.4286051 -4.4285841 -4.4286022 -4.4286542 -4.4287162 -4.4287739 -4.428813 -4.4288378 -4.428864][-4.4288197 -4.4288068 -4.428813 -4.428823 -4.4288263 -4.4288239 -4.4288163 -4.4288092 -4.4288182 -4.4288473 -4.4288878 -4.4289246 -4.428946 -4.4289532 -4.428957][-4.4288163 -4.4288011 -4.4288197 -4.428853 -4.4288855 -4.4289103 -4.4289203 -4.4289212 -4.428926 -4.4289408 -4.4289646 -4.4289894 -4.4290032 -4.429008 -4.4290061][-4.4287672 -4.428741 -4.4287591 -4.4288058 -4.4288607 -4.4289069 -4.4289317 -4.4289341 -4.4289312 -4.4289374 -4.4289532 -4.4289756 -4.4289956 -4.4290104 -4.4290142][-4.4287138 -4.4286685 -4.4286733 -4.428721 -4.4287882 -4.4288478 -4.42888 -4.4288807 -4.428865 -4.4288564 -4.428864 -4.4288912 -4.4289289 -4.4289632 -4.4289823]]...]
INFO - root - 2017-12-10 04:02:03.997402: step 2710, loss = 2.28, batch loss = 2.23 (38.2 examples/sec; 0.210 sec/batch; 19h:12m:19s remains)
INFO - root - 2017-12-10 04:02:06.036389: step 2720, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:43m:04s remains)
INFO - root - 2017-12-10 04:02:08.096241: step 2730, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:42m:21s remains)
INFO - root - 2017-12-10 04:02:10.153145: step 2740, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 19h:39m:49s remains)
INFO - root - 2017-12-10 04:02:12.209943: step 2750, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.202 sec/batch; 18h:27m:40s remains)
INFO - root - 2017-12-10 04:02:14.257879: step 2760, loss = 2.28, batch loss = 2.23 (38.6 examples/sec; 0.207 sec/batch; 18h:59m:04s remains)
INFO - root - 2017-12-10 04:02:16.298655: step 2770, loss = 2.28, batch loss = 2.23 (39.4 examples/sec; 0.203 sec/batch; 18h:35m:54s remains)
INFO - root - 2017-12-10 04:02:18.360656: step 2780, loss = 2.28, batch loss = 2.23 (39.9 examples/sec; 0.201 sec/batch; 18h:21m:50s remains)
INFO - root - 2017-12-10 04:02:20.398235: step 2790, loss = 2.28, batch loss = 2.23 (39.8 examples/sec; 0.201 sec/batch; 18h:23m:24s remains)
INFO - root - 2017-12-10 04:02:22.469782: step 2800, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:20m:58s remains)
2017-12-10 04:02:22.742352: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42891 -4.42889 -4.4288874 -4.4288869 -4.4288845 -4.4288845 -4.4288826 -4.428854 -4.4288197 -4.4288044 -4.4288025 -4.4288077 -4.4288225 -4.4288363 -4.4288454][-4.4288912 -4.42887 -4.4288683 -4.4288697 -4.4288673 -4.4288607 -4.428853 -4.4288192 -4.4287853 -4.4287744 -4.4287767 -4.4287791 -4.4287891 -4.4288139 -4.4288378][-4.4288416 -4.4288273 -4.4288354 -4.4288507 -4.4288592 -4.4288597 -4.4288487 -4.4288163 -4.4287915 -4.4287906 -4.428802 -4.4288058 -4.428812 -4.4288335 -4.42886][-4.4288011 -4.4287987 -4.4288168 -4.428843 -4.4288592 -4.428863 -4.428853 -4.4288225 -4.4288068 -4.428813 -4.4288287 -4.4288383 -4.428844 -4.4288545 -4.4288688][-4.4287868 -4.4287968 -4.4288168 -4.4288449 -4.4288564 -4.4288507 -4.4288349 -4.4288034 -4.4287877 -4.4288011 -4.4288235 -4.4288378 -4.4288468 -4.4288597 -4.4288678][-4.4287505 -4.4287543 -4.4287663 -4.4287939 -4.4288025 -4.4287834 -4.4287496 -4.428699 -4.4286737 -4.4286985 -4.4287343 -4.4287543 -4.4287763 -4.4288054 -4.4288197][-4.4286695 -4.4286652 -4.42868 -4.428709 -4.4287128 -4.4286823 -4.4286246 -4.4285393 -4.428494 -4.42853 -4.4285836 -4.428618 -4.4286609 -4.4287176 -4.4287553][-4.428606 -4.428606 -4.4286323 -4.42867 -4.4286857 -4.4286556 -4.4285831 -4.4284706 -4.4284048 -4.428441 -4.4285021 -4.4285545 -4.4286175 -4.428689 -4.4287419][-4.4286261 -4.4286389 -4.4286737 -4.4287124 -4.4287362 -4.4287248 -4.4286809 -4.4286036 -4.4285493 -4.4285607 -4.4286013 -4.4286442 -4.4286971 -4.4287553 -4.428803][-4.4287076 -4.4287248 -4.4287634 -4.4288039 -4.4288206 -4.42881 -4.4287834 -4.4287353 -4.4286966 -4.4286952 -4.4287238 -4.42876 -4.428802 -4.4288425 -4.4288721][-4.4287806 -4.4287877 -4.4288158 -4.4288435 -4.4288454 -4.4288287 -4.4288034 -4.4287686 -4.4287457 -4.4287496 -4.4287844 -4.428822 -4.4288559 -4.4288836 -4.4289021][-4.4288211 -4.4288106 -4.4288239 -4.4288354 -4.4288244 -4.4287939 -4.4287653 -4.4287419 -4.4287324 -4.4287462 -4.4287839 -4.4288216 -4.428853 -4.4288778 -4.428894][-4.4288235 -4.4287963 -4.4287958 -4.4288087 -4.4288054 -4.4287772 -4.4287477 -4.4287291 -4.4287205 -4.4287281 -4.4287534 -4.4287806 -4.4288049 -4.4288249 -4.4288411][-4.4288125 -4.4287848 -4.4287863 -4.4288125 -4.4288206 -4.4287934 -4.428762 -4.4287448 -4.4287305 -4.4287243 -4.4287343 -4.4287491 -4.4287596 -4.4287696 -4.4287844][-4.4288063 -4.428792 -4.428802 -4.4288349 -4.4288478 -4.428823 -4.4287915 -4.4287772 -4.4287591 -4.4287405 -4.4287381 -4.4287429 -4.4287419 -4.4287415 -4.4287543]]...]
INFO - root - 2017-12-10 04:02:24.784494: step 2810, loss = 2.28, batch loss = 2.23 (38.5 examples/sec; 0.208 sec/batch; 19h:01m:53s remains)
INFO - root - 2017-12-10 04:02:26.858220: step 2820, loss = 2.28, batch loss = 2.23 (39.5 examples/sec; 0.203 sec/batch; 18h:32m:59s remains)
INFO - root - 2017-12-10 04:02:28.917103: step 2830, loss = 2.28, batch loss = 2.23 (39.4 examples/sec; 0.203 sec/batch; 18h:35m:31s remains)
INFO - root - 2017-12-10 04:02:30.965312: step 2840, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.201 sec/batch; 18h:26m:04s remains)
INFO - root - 2017-12-10 04:02:33.000981: step 2850, loss = 2.28, batch loss = 2.23 (39.8 examples/sec; 0.201 sec/batch; 18h:23m:48s remains)
INFO - root - 2017-12-10 04:02:35.046106: step 2860, loss = 2.28, batch loss = 2.23 (39.9 examples/sec; 0.201 sec/batch; 18h:21m:45s remains)
INFO - root - 2017-12-10 04:02:37.110368: step 2870, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:29m:38s remains)
INFO - root - 2017-12-10 04:02:39.204988: step 2880, loss = 2.28, batch loss = 2.23 (39.3 examples/sec; 0.204 sec/batch; 18h:38m:47s remains)
INFO - root - 2017-12-10 04:02:41.264556: step 2890, loss = 2.28, batch loss = 2.23 (38.3 examples/sec; 0.209 sec/batch; 19h:06m:49s remains)
INFO - root - 2017-12-10 04:02:43.310340: step 2900, loss = 2.28, batch loss = 2.23 (38.5 examples/sec; 0.208 sec/batch; 19h:02m:03s remains)
2017-12-10 04:02:43.622671: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288683 -4.4288869 -4.4289 -4.42888 -4.4288568 -4.4288483 -4.4288449 -4.4288321 -4.4288311 -4.4288478 -4.4288568 -4.4288597 -4.4288731 -4.4288788 -4.4288588][-4.428803 -4.4288292 -4.4288497 -4.4288254 -4.4287868 -4.4287772 -4.4287853 -4.4287839 -4.428792 -4.428813 -4.428822 -4.4288154 -4.4288125 -4.4287977 -4.4287682][-4.4287095 -4.428741 -4.4287658 -4.4287338 -4.4286842 -4.4286771 -4.4287 -4.4287195 -4.4287491 -4.4287796 -4.4287825 -4.4287553 -4.4287362 -4.4287047 -4.4286652][-4.4286132 -4.4286518 -4.4286728 -4.4286342 -4.4285812 -4.4285765 -4.428606 -4.4286442 -4.4287014 -4.4287496 -4.4287438 -4.4286838 -4.4286437 -4.4286137 -4.4285741][-4.4285555 -4.4285932 -4.428606 -4.4285674 -4.4285259 -4.428515 -4.42854 -4.4285779 -4.428648 -4.428699 -4.4286809 -4.4286051 -4.428555 -4.4285297 -4.428493][-4.42857 -4.4286036 -4.4286137 -4.4285789 -4.4285483 -4.4285288 -4.4285359 -4.428555 -4.428607 -4.4286394 -4.4286008 -4.4285297 -4.4284916 -4.4284878 -4.4284663][-4.4286642 -4.4286785 -4.4286804 -4.4286571 -4.4286342 -4.4286079 -4.4285989 -4.4286008 -4.42863 -4.4286342 -4.4285789 -4.4285183 -4.4284949 -4.4285135 -4.4285088][-4.4287786 -4.4287715 -4.4287677 -4.4287434 -4.4287181 -4.4286861 -4.4286757 -4.4286723 -4.428678 -4.4286537 -4.428587 -4.4285364 -4.4285359 -4.42858 -4.428586][-4.4288578 -4.4288406 -4.4288344 -4.4288006 -4.4287643 -4.4287295 -4.4287148 -4.4287114 -4.4286985 -4.4286427 -4.4285622 -4.4285212 -4.428556 -4.4286237 -4.4286342][-4.4288683 -4.42884 -4.4288349 -4.4288011 -4.4287624 -4.428731 -4.4287057 -4.4287019 -4.4286919 -4.4286146 -4.4285192 -4.4284883 -4.4285483 -4.4286227 -4.4286304][-4.42882 -4.428781 -4.4287748 -4.4287481 -4.4287214 -4.4286962 -4.4286528 -4.4286413 -4.4286437 -4.4285712 -4.42849 -4.4284854 -4.4285583 -4.4286208 -4.4286256][-4.4287658 -4.4287186 -4.4287076 -4.4286819 -4.4286656 -4.4286475 -4.4285913 -4.4285679 -4.4285779 -4.4285197 -4.4284749 -4.42851 -4.4285932 -4.428647 -4.4286513][-4.4287109 -4.4286532 -4.4286404 -4.4286237 -4.42863 -4.4286294 -4.428576 -4.428546 -4.42856 -4.4285212 -4.4285016 -4.4285564 -4.4286356 -4.4286742 -4.4286809][-4.4286575 -4.4285908 -4.4285774 -4.4285827 -4.428617 -4.4286375 -4.4286079 -4.4285917 -4.4286132 -4.4285908 -4.4285731 -4.4286208 -4.42868 -4.428688 -4.42869][-4.4286232 -4.428555 -4.428546 -4.4285746 -4.4286289 -4.4286613 -4.4286551 -4.4286609 -4.4286895 -4.4286723 -4.428648 -4.4286757 -4.4287057 -4.4286861 -4.4286847]]...]
INFO - root - 2017-12-10 04:02:45.647137: step 2910, loss = 2.28, batch loss = 2.23 (39.5 examples/sec; 0.203 sec/batch; 18h:33m:19s remains)
INFO - root - 2017-12-10 04:02:47.691227: step 2920, loss = 2.28, batch loss = 2.23 (38.9 examples/sec; 0.206 sec/batch; 18h:49m:53s remains)
INFO - root - 2017-12-10 04:02:49.741879: step 2930, loss = 2.28, batch loss = 2.23 (39.9 examples/sec; 0.201 sec/batch; 18h:21m:33s remains)
INFO - root - 2017-12-10 04:02:51.788169: step 2940, loss = 2.28, batch loss = 2.23 (39.3 examples/sec; 0.203 sec/batch; 18h:37m:24s remains)
INFO - root - 2017-12-10 04:02:53.848910: step 2950, loss = 2.28, batch loss = 2.23 (39.1 examples/sec; 0.205 sec/batch; 18h:44m:20s remains)
INFO - root - 2017-12-10 04:02:55.892124: step 2960, loss = 2.28, batch loss = 2.23 (39.6 examples/sec; 0.202 sec/batch; 18h:30m:18s remains)
INFO - root - 2017-12-10 04:02:57.934521: step 2970, loss = 2.28, batch loss = 2.23 (38.8 examples/sec; 0.206 sec/batch; 18h:52m:06s remains)
INFO - root - 2017-12-10 04:02:59.989125: step 2980, loss = 2.28, batch loss = 2.23 (39.8 examples/sec; 0.201 sec/batch; 18h:23m:59s remains)
INFO - root - 2017-12-10 04:03:02.050008: step 2990, loss = 2.28, batch loss = 2.23 (38.4 examples/sec; 0.208 sec/batch; 19h:04m:48s remains)
INFO - root - 2017-12-10 04:03:04.122689: step 3000, loss = 2.28, batch loss = 2.23 (38.7 examples/sec; 0.207 sec/batch; 18h:56m:35s remains)
2017-12-10 04:03:04.408362: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287581 -4.4287262 -4.4287124 -4.4287271 -4.4287605 -4.4287829 -4.4288058 -4.4288135 -4.4287963 -4.4287729 -4.4287643 -4.4287653 -4.4287753 -4.428792 -4.4288144][-4.4287157 -4.4286795 -4.4286652 -4.4286828 -4.42872 -4.4287477 -4.4287734 -4.42878 -4.4287624 -4.4287367 -4.4287181 -4.4287062 -4.4287114 -4.4287262 -4.4287438][-4.4286885 -4.4286537 -4.4286461 -4.4286695 -4.4287128 -4.4287462 -4.4287672 -4.4287629 -4.42874 -4.4287071 -4.4286733 -4.4286523 -4.4286542 -4.4286675 -4.4286861][-4.4286637 -4.4286237 -4.4286132 -4.4286346 -4.4286752 -4.4287062 -4.4287214 -4.4287133 -4.4286933 -4.4286551 -4.4286036 -4.4285808 -4.4285922 -4.4286175 -4.4286432][-4.4286332 -4.4285808 -4.4285603 -4.4285684 -4.4285917 -4.4286108 -4.42862 -4.4286184 -4.4286132 -4.4285736 -4.4285126 -4.4284954 -4.428524 -4.428565 -4.4286065][-4.4285889 -4.4285192 -4.4284787 -4.4284663 -4.4284635 -4.4284554 -4.4284496 -4.428463 -4.4284835 -4.4284625 -4.4284153 -4.4284134 -4.4284625 -4.4285221 -4.4285769][-4.4285369 -4.4284472 -4.4283834 -4.4283481 -4.4283204 -4.428288 -4.4282751 -4.4283066 -4.4283557 -4.428369 -4.4283433 -4.4283442 -4.4283895 -4.4284549 -4.4285188][-4.4284887 -4.4283829 -4.4283028 -4.42825 -4.4282093 -4.4281697 -4.4281592 -4.4282017 -4.4282665 -4.4283042 -4.4283009 -4.4282966 -4.4283228 -4.428381 -4.4284453][-4.428472 -4.4283667 -4.4282856 -4.4282293 -4.428185 -4.4281473 -4.4281459 -4.4281878 -4.4282408 -4.428277 -4.4282875 -4.4282908 -4.4283109 -4.4283643 -4.4284282][-4.4285469 -4.4284625 -4.4283967 -4.4283409 -4.428298 -4.4282703 -4.4282751 -4.4283056 -4.4283371 -4.4283595 -4.4283705 -4.4283838 -4.4284115 -4.4284625 -4.4285231][-4.4286828 -4.4286294 -4.4285841 -4.4285359 -4.4284964 -4.4284735 -4.4284797 -4.4285016 -4.4285212 -4.4285331 -4.4285383 -4.4285502 -4.4285793 -4.4286218 -4.4286723][-4.4288158 -4.42879 -4.4287648 -4.428731 -4.4287009 -4.4286852 -4.4286876 -4.4287019 -4.4287152 -4.4287224 -4.4287257 -4.4287324 -4.4287534 -4.4287844 -4.42882][-4.4289231 -4.4289165 -4.4289064 -4.4288898 -4.4288735 -4.4288645 -4.428863 -4.4288688 -4.4288745 -4.4288778 -4.4288788 -4.4288793 -4.4288878 -4.428905 -4.4289265][-4.428987 -4.4289885 -4.4289865 -4.42898 -4.4289727 -4.4289694 -4.428967 -4.428967 -4.4289675 -4.4289665 -4.4289646 -4.4289627 -4.4289646 -4.4289718 -4.4289813][-4.4290113 -4.4290137 -4.4290109 -4.4290071 -4.4290042 -4.4290032 -4.4290028 -4.4290028 -4.4290023 -4.4290009 -4.4289985 -4.4289975 -4.4289975 -4.4290004 -4.4290032]]...]
INFO - root - 2017-12-10 04:03:06.474973: step 3010, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.211 sec/batch; 19h:17m:26s remains)
INFO - root - 2017-12-10 04:03:08.536398: step 3020, loss = 2.28, batch loss = 2.23 (38.5 examples/sec; 0.208 sec/batch; 19h:01m:47s remains)
INFO - root - 2017-12-10 04:03:10.595938: step 3030, loss = 2.28, batch loss = 2.23 (40.0 examples/sec; 0.200 sec/batch; 18h:18m:46s remains)
INFO - root - 2017-12-10 04:03:12.677884: step 3040, loss = 2.28, batch loss = 2.23 (38.5 examples/sec; 0.208 sec/batch; 19h:02m:11s remains)
INFO - root - 2017-12-10 04:03:14.746844: step 3050, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.202 sec/batch; 18h:27m:07s remains)
INFO - root - 2017-12-10 04:03:16.790656: step 3060, loss = 2.28, batch loss = 2.23 (39.6 examples/sec; 0.202 sec/batch; 18h:28m:32s remains)
INFO - root - 2017-12-10 04:03:18.829050: step 3070, loss = 2.28, batch loss = 2.23 (40.2 examples/sec; 0.199 sec/batch; 18h:11m:27s remains)
INFO - root - 2017-12-10 04:03:20.869568: step 3080, loss = 2.28, batch loss = 2.23 (38.4 examples/sec; 0.208 sec/batch; 19h:03m:41s remains)
INFO - root - 2017-12-10 04:03:22.929488: step 3090, loss = 2.28, batch loss = 2.23 (39.4 examples/sec; 0.203 sec/batch; 18h:34m:47s remains)
INFO - root - 2017-12-10 04:03:24.971941: step 3100, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:27m:55s remains)
2017-12-10 04:03:25.259263: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428885 -4.428884 -4.4288464 -4.4288063 -4.4288092 -4.4288359 -4.4288397 -4.4288054 -4.4287548 -4.4287453 -4.4287839 -4.4288306 -4.4288297 -4.4287758 -4.428719][-4.4288535 -4.4288492 -4.4288106 -4.42877 -4.4287739 -4.4288011 -4.4288006 -4.4287677 -4.4287214 -4.428721 -4.4287696 -4.4288211 -4.4288273 -4.4287834 -4.4287405][-4.4287996 -4.428812 -4.4287968 -4.4287786 -4.4287934 -4.4288197 -4.4288192 -4.4287987 -4.4287763 -4.428791 -4.4288378 -4.4288812 -4.4288883 -4.4288554 -4.4288177][-4.4288011 -4.4288239 -4.4288368 -4.4288464 -4.428865 -4.4288778 -4.4288726 -4.4288626 -4.4288626 -4.4288812 -4.4289145 -4.4289412 -4.4289427 -4.428915 -4.42888][-4.4288383 -4.428854 -4.4288735 -4.4288917 -4.4289036 -4.428896 -4.4288731 -4.4288554 -4.4288616 -4.4288816 -4.4289079 -4.4289274 -4.4289303 -4.4289136 -4.4288874][-4.4288611 -4.4288492 -4.4288526 -4.4288664 -4.4288721 -4.4288468 -4.4287987 -4.4287591 -4.4287591 -4.4287844 -4.4288182 -4.4288449 -4.4288573 -4.4288568 -4.4288449][-4.4288363 -4.4287891 -4.4287686 -4.4287734 -4.4287696 -4.4287257 -4.428648 -4.4285874 -4.4285936 -4.4286466 -4.4287062 -4.4287496 -4.4287777 -4.4287992 -4.4288058][-4.4287925 -4.4287219 -4.4286776 -4.4286675 -4.4286547 -4.428597 -4.4285021 -4.4284363 -4.4284596 -4.4285483 -4.4286413 -4.4287086 -4.4287543 -4.4287958 -4.4288187][-4.4287887 -4.4287286 -4.4286866 -4.4286733 -4.4286604 -4.4286137 -4.4285378 -4.4284892 -4.428514 -4.4286032 -4.4286981 -4.4287648 -4.4288073 -4.4288464 -4.4288669][-4.4288225 -4.4287891 -4.42877 -4.42877 -4.4287682 -4.4287457 -4.4287062 -4.4286771 -4.4286933 -4.4287534 -4.4288187 -4.4288611 -4.4288821 -4.4288969 -4.4289002][-4.428863 -4.4288507 -4.4288511 -4.4288645 -4.428874 -4.4288731 -4.4288597 -4.4288468 -4.4288535 -4.428885 -4.4289188 -4.4289374 -4.4289351 -4.428925 -4.42891][-4.428896 -4.4288926 -4.4289017 -4.4289184 -4.428936 -4.428947 -4.428947 -4.4289408 -4.4289422 -4.4289584 -4.4289727 -4.4289751 -4.4289608 -4.4289432 -4.4289236][-4.4289188 -4.428916 -4.4289207 -4.4289274 -4.4289408 -4.4289546 -4.4289551 -4.4289465 -4.4289432 -4.4289517 -4.4289613 -4.4289637 -4.4289517 -4.4289389 -4.4289308][-4.4289212 -4.4289126 -4.4289055 -4.428894 -4.4288926 -4.4289012 -4.4288974 -4.4288855 -4.4288826 -4.4288945 -4.4289122 -4.4289222 -4.428916 -4.42891 -4.4289126][-4.4289055 -4.4288898 -4.4288712 -4.4288421 -4.4288268 -4.4288321 -4.4288306 -4.4288197 -4.4288177 -4.4288325 -4.4288616 -4.4288831 -4.4288831 -4.4288774 -4.428884]]...]
INFO - root - 2017-12-10 04:03:27.309591: step 3110, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.202 sec/batch; 18h:27m:15s remains)
INFO - root - 2017-12-10 04:03:29.346331: step 3120, loss = 2.28, batch loss = 2.23 (39.4 examples/sec; 0.203 sec/batch; 18h:33m:30s remains)
INFO - root - 2017-12-10 04:03:31.385261: step 3130, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.202 sec/batch; 18h:27m:02s remains)
INFO - root - 2017-12-10 04:03:33.425103: step 3140, loss = 2.28, batch loss = 2.23 (39.9 examples/sec; 0.200 sec/batch; 18h:19m:59s remains)
INFO - root - 2017-12-10 04:03:35.463032: step 3150, loss = 2.28, batch loss = 2.23 (39.3 examples/sec; 0.203 sec/batch; 18h:36m:56s remains)
INFO - root - 2017-12-10 04:03:37.548435: step 3160, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:41m:23s remains)
INFO - root - 2017-12-10 04:03:39.590048: step 3170, loss = 2.28, batch loss = 2.23 (38.4 examples/sec; 0.208 sec/batch; 19h:03m:25s remains)
INFO - root - 2017-12-10 04:03:41.628139: step 3180, loss = 2.28, batch loss = 2.23 (39.0 examples/sec; 0.205 sec/batch; 18h:46m:56s remains)
INFO - root - 2017-12-10 04:03:43.671164: step 3190, loss = 2.28, batch loss = 2.23 (38.5 examples/sec; 0.208 sec/batch; 18h:59m:49s remains)
INFO - root - 2017-12-10 04:03:45.708583: step 3200, loss = 2.28, batch loss = 2.23 (38.9 examples/sec; 0.206 sec/batch; 18h:49m:07s remains)
2017-12-10 04:03:46.002391: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428947 -4.4289279 -4.4289179 -4.4289103 -4.4289174 -4.428936 -4.4289355 -4.4289246 -4.4289203 -4.4289207 -4.4289169 -4.4289112 -4.4289117 -4.428926 -4.4289432][-4.4289222 -4.4288931 -4.4288731 -4.4288573 -4.4288607 -4.4288807 -4.4288793 -4.428865 -4.4288607 -4.4288654 -4.4288692 -4.4288707 -4.4288769 -4.4288988 -4.4289231][-4.42889 -4.4288511 -4.4288235 -4.4287982 -4.4287949 -4.4288082 -4.4287982 -4.4287772 -4.4287782 -4.4287853 -4.4287963 -4.4288058 -4.4288177 -4.4288535 -4.4288917][-4.4288712 -4.428822 -4.4287806 -4.4287434 -4.4287286 -4.4287281 -4.4287081 -4.4286761 -4.4286795 -4.4286871 -4.4287047 -4.4287295 -4.4287562 -4.4288106 -4.428865][-4.4288473 -4.4287834 -4.4287176 -4.4286571 -4.4286232 -4.428616 -4.4285936 -4.4285583 -4.4285617 -4.4285569 -4.4285679 -4.4286075 -4.4286723 -4.4287667 -4.428844][-4.42881 -4.4287267 -4.4286361 -4.4285522 -4.4284959 -4.4284754 -4.4284511 -4.4284072 -4.4283986 -4.4283624 -4.4283538 -4.4284096 -4.4285235 -4.4286785 -4.4287925][-4.4287591 -4.4286556 -4.4285426 -4.4284449 -4.4283752 -4.4283428 -4.4283042 -4.4282413 -4.4282017 -4.4281292 -4.4281011 -4.428174 -4.4283457 -4.4285703 -4.4287286][-4.4286814 -4.4285703 -4.4284573 -4.4283619 -4.4282961 -4.4282613 -4.4282041 -4.4281278 -4.4280739 -4.4279966 -4.4279709 -4.4280639 -4.4282727 -4.4285254 -4.4286966][-4.4286542 -4.4285583 -4.4284678 -4.428391 -4.4283309 -4.4282975 -4.4282365 -4.4281707 -4.4281335 -4.4280849 -4.4280863 -4.4281878 -4.4283733 -4.428587 -4.4287276][-4.4286947 -4.4286213 -4.4285588 -4.4285064 -4.428462 -4.4284382 -4.4283957 -4.4283619 -4.4283462 -4.4283209 -4.4283314 -4.4284148 -4.4285445 -4.4286947 -4.4287944][-4.4287796 -4.4287276 -4.4286861 -4.4286561 -4.4286318 -4.4286275 -4.428618 -4.4286146 -4.4286132 -4.4286027 -4.4286113 -4.4286647 -4.4287419 -4.4288316 -4.4288864][-4.4288626 -4.4288297 -4.4288015 -4.4287844 -4.4287777 -4.4287877 -4.4287982 -4.4288125 -4.428823 -4.4288216 -4.4288273 -4.4288583 -4.4289012 -4.4289479 -4.4289708][-4.42892 -4.4289002 -4.4288821 -4.42887 -4.4288707 -4.4288845 -4.4288988 -4.4289145 -4.4289279 -4.42893 -4.4289346 -4.4289546 -4.4289813 -4.4290056 -4.4290147][-4.4289546 -4.4289436 -4.4289351 -4.42893 -4.4289317 -4.4289412 -4.4289503 -4.4289618 -4.4289708 -4.4289722 -4.4289761 -4.4289889 -4.4290037 -4.4290156 -4.4290204][-4.4289813 -4.4289722 -4.4289689 -4.428968 -4.4289703 -4.4289751 -4.4289804 -4.4289865 -4.4289908 -4.4289923 -4.4289951 -4.4290032 -4.4290104 -4.4290161 -4.4290195]]...]
INFO - root - 2017-12-10 04:03:48.051149: step 3210, loss = 2.28, batch loss = 2.23 (39.0 examples/sec; 0.205 sec/batch; 18h:45m:18s remains)
INFO - root - 2017-12-10 04:03:50.090685: step 3220, loss = 2.28, batch loss = 2.23 (40.3 examples/sec; 0.198 sec/batch; 18h:08m:25s remains)
INFO - root - 2017-12-10 04:03:52.172027: step 3230, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 19h:21m:26s remains)
INFO - root - 2017-12-10 04:03:54.233313: step 3240, loss = 2.28, batch loss = 2.23 (39.0 examples/sec; 0.205 sec/batch; 18h:46m:58s remains)
INFO - root - 2017-12-10 04:03:56.297194: step 3250, loss = 2.28, batch loss = 2.23 (38.9 examples/sec; 0.205 sec/batch; 18h:47m:23s remains)
INFO - root - 2017-12-10 04:03:58.375411: step 3260, loss = 2.28, batch loss = 2.23 (38.6 examples/sec; 0.207 sec/batch; 18h:57m:47s remains)
INFO - root - 2017-12-10 04:04:00.415319: step 3270, loss = 2.28, batch loss = 2.23 (38.5 examples/sec; 0.208 sec/batch; 18h:59m:50s remains)
INFO - root - 2017-12-10 04:04:02.455484: step 3280, loss = 2.28, batch loss = 2.23 (39.0 examples/sec; 0.205 sec/batch; 18h:46m:11s remains)
INFO - root - 2017-12-10 04:04:04.522838: step 3290, loss = 2.28, batch loss = 2.23 (38.4 examples/sec; 0.208 sec/batch; 19h:03m:23s remains)
INFO - root - 2017-12-10 04:04:06.563037: step 3300, loss = 2.28, batch loss = 2.23 (39.0 examples/sec; 0.205 sec/batch; 18h:44m:22s remains)
2017-12-10 04:04:06.850848: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288526 -4.4288983 -4.4289474 -4.4289842 -4.42901 -4.4290147 -4.4289942 -4.42895 -4.4288945 -4.4288478 -4.4288092 -4.4287786 -4.4287858 -4.42884 -4.4288855][-4.4288483 -4.4289069 -4.4289646 -4.4290056 -4.4290318 -4.4290447 -4.42904 -4.4290152 -4.4289794 -4.42895 -4.4289222 -4.4288926 -4.4288812 -4.4288969 -4.428906][-4.4288096 -4.4288783 -4.4289441 -4.4289894 -4.4290156 -4.42903 -4.4290323 -4.4290195 -4.4290013 -4.4289908 -4.4289794 -4.4289594 -4.4289436 -4.4289379 -4.428916][-4.428802 -4.4288626 -4.428915 -4.428947 -4.4289622 -4.4289689 -4.428967 -4.4289579 -4.4289565 -4.4289675 -4.4289751 -4.4289708 -4.428966 -4.4289622 -4.4289351][-4.4288387 -4.4288726 -4.4288936 -4.4288969 -4.428884 -4.4288611 -4.4288344 -4.4288163 -4.42883 -4.4288716 -4.4289131 -4.4289436 -4.4289665 -4.4289808 -4.428967][-4.4288926 -4.4289012 -4.4288888 -4.428854 -4.4287977 -4.4287229 -4.428648 -4.4286075 -4.4286394 -4.4287224 -4.4288092 -4.428885 -4.4289422 -4.4289761 -4.4289765][-4.4289379 -4.4289331 -4.4288955 -4.428823 -4.4287176 -4.4285793 -4.4284449 -4.4283776 -4.4284339 -4.4285674 -4.4287038 -4.42882 -4.4289002 -4.4289412 -4.4289451][-4.4289842 -4.4289746 -4.4289188 -4.4288144 -4.4286671 -4.428473 -4.4282804 -4.4281917 -4.428277 -4.4284568 -4.4286323 -4.4287729 -4.4288592 -4.4289 -4.4289069][-4.4290309 -4.429018 -4.4289522 -4.4288368 -4.4286766 -4.4284668 -4.4282565 -4.4281688 -4.4282684 -4.4284587 -4.4286408 -4.4287829 -4.4288626 -4.4288964 -4.4289069][-4.4290595 -4.429039 -4.4289732 -4.4288692 -4.4287353 -4.4285693 -4.4284129 -4.4283643 -4.428453 -4.4285989 -4.4287381 -4.4288492 -4.4289112 -4.428936 -4.4289465][-4.4290605 -4.4290323 -4.4289722 -4.4288926 -4.4288034 -4.4287066 -4.4286275 -4.4286203 -4.4286847 -4.4287691 -4.4288497 -4.4289174 -4.4289613 -4.4289827 -4.4289947][-4.4290371 -4.4290042 -4.4289565 -4.4289074 -4.4288664 -4.4288316 -4.4288125 -4.4288268 -4.4288616 -4.4288926 -4.4289207 -4.4289474 -4.4289737 -4.428998 -4.4290166][-4.4290023 -4.428966 -4.4289265 -4.4289036 -4.4289002 -4.4289088 -4.4289274 -4.4289508 -4.4289613 -4.4289541 -4.4289432 -4.4289389 -4.4289517 -4.4289832 -4.4290128][-4.4289632 -4.4289222 -4.428884 -4.4288754 -4.4288964 -4.42893 -4.4289684 -4.4289975 -4.4289975 -4.4289718 -4.4289346 -4.4289069 -4.4289093 -4.428947 -4.42899][-4.4289403 -4.4289064 -4.4288754 -4.428875 -4.4289036 -4.4289417 -4.42898 -4.4290061 -4.4290037 -4.4289737 -4.428925 -4.428885 -4.4288807 -4.4289184 -4.428967]]...]
INFO - root - 2017-12-10 04:04:08.914563: step 3310, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:33m:13s remains)
INFO - root - 2017-12-10 04:04:10.996374: step 3320, loss = 2.28, batch loss = 2.23 (38.7 examples/sec; 0.207 sec/batch; 18h:54m:31s remains)
INFO - root - 2017-12-10 04:04:13.049033: step 3330, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 19h:10m:52s remains)
INFO - root - 2017-12-10 04:04:15.119696: step 3340, loss = 2.28, batch loss = 2.23 (40.2 examples/sec; 0.199 sec/batch; 18h:12m:44s remains)
INFO - root - 2017-12-10 04:04:17.168246: step 3350, loss = 2.28, batch loss = 2.23 (39.4 examples/sec; 0.203 sec/batch; 18h:34m:09s remains)
INFO - root - 2017-12-10 04:04:19.208644: step 3360, loss = 2.28, batch loss = 2.23 (39.4 examples/sec; 0.203 sec/batch; 18h:34m:55s remains)
INFO - root - 2017-12-10 04:04:21.260103: step 3370, loss = 2.28, batch loss = 2.23 (38.4 examples/sec; 0.208 sec/batch; 19h:01m:31s remains)
INFO - root - 2017-12-10 04:04:23.324683: step 3380, loss = 2.28, batch loss = 2.23 (39.3 examples/sec; 0.203 sec/batch; 18h:36m:10s remains)
INFO - root - 2017-12-10 04:04:25.387092: step 3390, loss = 2.28, batch loss = 2.23 (39.6 examples/sec; 0.202 sec/batch; 18h:27m:02s remains)
INFO - root - 2017-12-10 04:04:27.442833: step 3400, loss = 2.28, batch loss = 2.23 (39.6 examples/sec; 0.202 sec/batch; 18h:26m:53s remains)
2017-12-10 04:04:27.742297: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285035 -4.4285569 -4.4286404 -4.4287229 -4.4287534 -4.4287415 -4.4287176 -4.4287043 -4.4287138 -4.4287372 -4.4287591 -4.4287634 -4.4287524 -4.4287362 -4.4287262][-4.4284773 -4.4285522 -4.4286523 -4.4287419 -4.4287767 -4.4287734 -4.4287529 -4.4287324 -4.4287262 -4.4287338 -4.4287448 -4.4287491 -4.42874 -4.42871 -4.4286771][-4.4285531 -4.4286413 -4.4287405 -4.4288158 -4.4288406 -4.4288354 -4.4288125 -4.428781 -4.4287572 -4.4287496 -4.4287558 -4.4287663 -4.428762 -4.4287233 -4.4286757][-4.42867 -4.4287438 -4.4288177 -4.4288707 -4.4288845 -4.4288697 -4.428833 -4.4287848 -4.4287424 -4.42873 -4.4287443 -4.4287663 -4.4287739 -4.4287415 -4.4286933][-4.4287467 -4.4287939 -4.4288363 -4.4288664 -4.4288678 -4.4288335 -4.4287667 -4.4286852 -4.4286275 -4.4286313 -4.4286723 -4.4287252 -4.4287639 -4.4287553 -4.4287252][-4.42874 -4.4287639 -4.4287863 -4.4288006 -4.4287877 -4.4287257 -4.4286165 -4.4284911 -4.4284244 -4.42847 -4.4285626 -4.428658 -4.4287319 -4.4287534 -4.42875][-4.4287019 -4.4286952 -4.4286962 -4.4286947 -4.4286652 -4.4285789 -4.4284291 -4.4282584 -4.428195 -4.4283195 -4.4284863 -4.4286261 -4.4287295 -4.4287767 -4.428791][-4.4286795 -4.42865 -4.4286361 -4.4286237 -4.4285831 -4.4284892 -4.4283338 -4.4281716 -4.4281607 -4.4283438 -4.4285417 -4.4286847 -4.4287848 -4.4288268 -4.4288368][-4.4286804 -4.4286528 -4.4286461 -4.4286323 -4.4285965 -4.4285269 -4.4284339 -4.4283595 -4.4283915 -4.4285374 -4.4286814 -4.4287696 -4.4288292 -4.4288516 -4.428853][-4.42871 -4.4286833 -4.4286838 -4.428679 -4.4286585 -4.4286222 -4.428597 -4.4285903 -4.4286242 -4.4287133 -4.4287877 -4.4288092 -4.428813 -4.428813 -4.4288058][-4.4287295 -4.4286962 -4.4286962 -4.4287024 -4.4286976 -4.4286876 -4.4287014 -4.4287238 -4.4287496 -4.4287925 -4.4288154 -4.4287949 -4.428761 -4.4287324 -4.4287109][-4.4287319 -4.4286966 -4.4286909 -4.4287024 -4.4287109 -4.4287109 -4.428731 -4.4287539 -4.4287677 -4.4287906 -4.4288 -4.4287648 -4.4287086 -4.4286566 -4.428627][-4.4287186 -4.4286795 -4.4286714 -4.4286885 -4.4287019 -4.4286942 -4.428699 -4.4287171 -4.4287357 -4.42876 -4.4287763 -4.4287448 -4.4286776 -4.4286151 -4.4285908][-4.4287066 -4.428679 -4.4286757 -4.4286909 -4.4286933 -4.4286728 -4.4286566 -4.4286618 -4.428688 -4.4287238 -4.4287524 -4.4287329 -4.4286747 -4.4286208 -4.4286103][-4.4287219 -4.4287086 -4.4287143 -4.4287257 -4.4287148 -4.4286809 -4.4286442 -4.4286332 -4.428658 -4.4287004 -4.4287362 -4.4287319 -4.4286942 -4.4286609 -4.428658]]...]
INFO - root - 2017-12-10 04:04:29.769123: step 3410, loss = 2.28, batch loss = 2.23 (40.0 examples/sec; 0.200 sec/batch; 18h:18m:14s remains)
INFO - root - 2017-12-10 04:04:31.864018: step 3420, loss = 2.28, batch loss = 2.23 (38.8 examples/sec; 0.206 sec/batch; 18h:50m:55s remains)
INFO - root - 2017-12-10 04:04:33.936352: step 3430, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:16m:39s remains)
INFO - root - 2017-12-10 04:04:35.973282: step 3440, loss = 2.28, batch loss = 2.23 (38.4 examples/sec; 0.208 sec/batch; 19h:01m:29s remains)
INFO - root - 2017-12-10 04:04:38.030681: step 3450, loss = 2.28, batch loss = 2.23 (39.6 examples/sec; 0.202 sec/batch; 18h:28m:09s remains)
INFO - root - 2017-12-10 04:04:40.071220: step 3460, loss = 2.28, batch loss = 2.23 (40.0 examples/sec; 0.200 sec/batch; 18h:17m:18s remains)
INFO - root - 2017-12-10 04:04:42.115760: step 3470, loss = 2.28, batch loss = 2.23 (38.4 examples/sec; 0.208 sec/batch; 19h:01m:35s remains)
INFO - root - 2017-12-10 04:04:44.156740: step 3480, loss = 2.28, batch loss = 2.23 (38.5 examples/sec; 0.208 sec/batch; 18h:59m:11s remains)
INFO - root - 2017-12-10 04:04:46.221787: step 3490, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:18m:09s remains)
INFO - root - 2017-12-10 04:04:48.285802: step 3500, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:37m:44s remains)
2017-12-10 04:04:48.557214: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290133 -4.4290056 -4.428998 -4.4289923 -4.4289904 -4.4289927 -4.429 -4.4290061 -4.4290113 -4.4290147 -4.4290142 -4.4290142 -4.4290152 -4.4290185 -4.4290218][-4.4289927 -4.4289775 -4.4289637 -4.4289575 -4.428957 -4.42896 -4.4289641 -4.4289694 -4.4289756 -4.4289804 -4.4289813 -4.4289823 -4.4289875 -4.4289923 -4.4289951][-4.4289718 -4.4289484 -4.4289279 -4.4289184 -4.4289193 -4.4289217 -4.4289188 -4.4289169 -4.4289179 -4.4289207 -4.4289236 -4.4289317 -4.4289446 -4.4289527 -4.4289589][-4.4289522 -4.4289279 -4.4289045 -4.4288874 -4.4288788 -4.4288716 -4.4288588 -4.4288421 -4.4288335 -4.4288321 -4.4288397 -4.4288611 -4.4288831 -4.4289021 -4.4289188][-4.4289331 -4.4289107 -4.4288859 -4.4288564 -4.4288306 -4.4288092 -4.4287882 -4.428762 -4.4287467 -4.4287438 -4.4287567 -4.4287868 -4.4288192 -4.428853 -4.4288831][-4.4289212 -4.4289036 -4.4288769 -4.4288349 -4.4287944 -4.4287567 -4.4287186 -4.428679 -4.4286675 -4.4286766 -4.4286966 -4.4287271 -4.4287663 -4.4288082 -4.4288511][-4.428916 -4.4289021 -4.428875 -4.4288292 -4.4287796 -4.4287229 -4.4286604 -4.4286127 -4.4286108 -4.4286366 -4.4286695 -4.4287033 -4.4287472 -4.4287915 -4.428834][-4.4289041 -4.4288964 -4.4288726 -4.4288249 -4.4287686 -4.4286938 -4.4286156 -4.4285712 -4.4285803 -4.4286237 -4.4286795 -4.4287248 -4.4287663 -4.4288006 -4.4288316][-4.4288907 -4.4288917 -4.428874 -4.4288273 -4.42876 -4.4286704 -4.428586 -4.428556 -4.4285803 -4.4286366 -4.4287133 -4.4287634 -4.4287953 -4.4288206 -4.4288359][-4.4288993 -4.4289122 -4.4289074 -4.4288678 -4.4287996 -4.428709 -4.4286313 -4.4286137 -4.4286437 -4.4286938 -4.4287558 -4.4287972 -4.4288239 -4.4288454 -4.4288545][-4.4289184 -4.4289436 -4.4289613 -4.4289446 -4.4288917 -4.4288135 -4.4287472 -4.4287338 -4.4287577 -4.42879 -4.4288235 -4.4288435 -4.4288659 -4.4288821 -4.4288821][-4.4289212 -4.4289546 -4.4289842 -4.4289856 -4.4289541 -4.4289 -4.4288535 -4.4288454 -4.4288573 -4.4288726 -4.4288831 -4.4288878 -4.4289002 -4.4289055 -4.4288955][-4.4289041 -4.4289408 -4.4289737 -4.4289818 -4.4289632 -4.4289255 -4.4288931 -4.4288874 -4.4288921 -4.4288983 -4.4289031 -4.4289036 -4.42891 -4.4289055 -4.4288888][-4.4288754 -4.4289088 -4.4289422 -4.4289536 -4.4289393 -4.4289083 -4.4288816 -4.4288731 -4.4288764 -4.4288831 -4.4288888 -4.42889 -4.4288955 -4.4288864 -4.4288659][-4.4288545 -4.4288774 -4.4289007 -4.4289069 -4.4288926 -4.428865 -4.428843 -4.4288349 -4.4288435 -4.4288578 -4.4288673 -4.4288692 -4.4288678 -4.4288545 -4.4288354]]...]
INFO - root - 2017-12-10 04:04:50.608964: step 3510, loss = 2.28, batch loss = 2.23 (39.1 examples/sec; 0.205 sec/batch; 18h:42m:49s remains)
INFO - root - 2017-12-10 04:04:52.681935: step 3520, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:50m:07s remains)
INFO - root - 2017-12-10 04:04:54.753097: step 3530, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 20h:02m:25s remains)
INFO - root - 2017-12-10 04:04:56.799609: step 3540, loss = 2.28, batch loss = 2.23 (40.7 examples/sec; 0.196 sec/batch; 17h:56m:50s remains)
INFO - root - 2017-12-10 04:04:58.864985: step 3550, loss = 2.28, batch loss = 2.23 (39.8 examples/sec; 0.201 sec/batch; 18h:22m:20s remains)
INFO - root - 2017-12-10 04:05:00.949549: step 3560, loss = 2.28, batch loss = 2.23 (38.3 examples/sec; 0.209 sec/batch; 19h:03m:44s remains)
INFO - root - 2017-12-10 04:05:03.005286: step 3570, loss = 2.28, batch loss = 2.23 (38.7 examples/sec; 0.207 sec/batch; 18h:52m:51s remains)
INFO - root - 2017-12-10 04:05:05.053020: step 3580, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:17m:31s remains)
INFO - root - 2017-12-10 04:05:07.117612: step 3590, loss = 2.28, batch loss = 2.23 (39.3 examples/sec; 0.203 sec/batch; 18h:34m:56s remains)
INFO - root - 2017-12-10 04:05:09.158649: step 3600, loss = 2.28, batch loss = 2.23 (39.0 examples/sec; 0.205 sec/batch; 18h:44m:10s remains)
2017-12-10 04:05:09.438461: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428894 -4.4289141 -4.4289074 -4.42888 -4.428844 -4.428793 -4.4287224 -4.4286604 -4.4286242 -4.4286051 -4.428618 -4.4286809 -4.4287705 -4.4288416 -4.4288888][-4.4289265 -4.4289336 -4.42891 -4.4288731 -4.4288411 -4.4287977 -4.4287443 -4.4287009 -4.4286675 -4.4286475 -4.428658 -4.4287062 -4.4287782 -4.428833 -4.4288697][-4.4289203 -4.4289227 -4.42889 -4.4288507 -4.4288211 -4.4287782 -4.4287353 -4.4287095 -4.4286909 -4.4286876 -4.4287066 -4.4287438 -4.4287839 -4.4288049 -4.4288268][-4.4288778 -4.4288855 -4.4288487 -4.4288063 -4.4287734 -4.4287248 -4.428679 -4.4286594 -4.428658 -4.4286685 -4.4286942 -4.4287233 -4.4287448 -4.4287405 -4.4287558][-4.4288387 -4.4288359 -4.428782 -4.4287276 -4.428689 -4.4286323 -4.4285746 -4.4285617 -4.4285855 -4.4286103 -4.4286385 -4.4286613 -4.4286718 -4.4286532 -4.4286704][-4.42882 -4.428793 -4.4287176 -4.4286466 -4.4285927 -4.4285164 -4.4284358 -4.4284267 -4.4284739 -4.4285178 -4.4285636 -4.4285908 -4.4285965 -4.4285784 -4.4286032][-4.4288068 -4.4287539 -4.428659 -4.4285722 -4.4285059 -4.4284172 -4.4283156 -4.4282985 -4.4283495 -4.428412 -4.4284925 -4.4285445 -4.4285445 -4.4285188 -4.428556][-4.4287839 -4.4287167 -4.4286189 -4.4285407 -4.4284906 -4.4284196 -4.4283233 -4.4283004 -4.4283419 -4.4284062 -4.4285016 -4.4285564 -4.4285364 -4.4284892 -4.4285226][-4.4287562 -4.4286942 -4.4286222 -4.4285793 -4.4285684 -4.4285412 -4.4284868 -4.4284787 -4.4285126 -4.4285526 -4.4286256 -4.4286551 -4.4286089 -4.4285522 -4.4285707][-4.4287405 -4.4287028 -4.4286671 -4.4286523 -4.4286675 -4.4286709 -4.4286418 -4.428648 -4.42868 -4.4287052 -4.4287543 -4.4287658 -4.4287148 -4.4286709 -4.4286909][-4.4287548 -4.428741 -4.4287314 -4.4287295 -4.4287486 -4.4287634 -4.4287457 -4.4287524 -4.4287796 -4.4287963 -4.4288244 -4.4288278 -4.4287848 -4.4287653 -4.4287987][-4.4287972 -4.4287853 -4.4287815 -4.4287777 -4.4287925 -4.4288058 -4.4287972 -4.4288011 -4.4288154 -4.4288254 -4.4288392 -4.4288373 -4.4288144 -4.4288211 -4.4288611][-4.428833 -4.4288106 -4.4287982 -4.4287896 -4.4287963 -4.4288077 -4.4288111 -4.4288182 -4.4288249 -4.42883 -4.428834 -4.4288349 -4.428833 -4.4288554 -4.4288921][-4.4288507 -4.4288225 -4.4288054 -4.4287896 -4.4287853 -4.4287906 -4.4287944 -4.4287963 -4.4287953 -4.4288068 -4.428822 -4.4288397 -4.4288549 -4.4288778 -4.4289107][-4.4288707 -4.4288363 -4.4288149 -4.4287953 -4.42878 -4.4287748 -4.428772 -4.4287663 -4.4287663 -4.4287887 -4.4288168 -4.4288425 -4.4288607 -4.4288859 -4.4289236]]...]
INFO - root - 2017-12-10 04:05:11.518647: step 3610, loss = 2.28, batch loss = 2.23 (38.9 examples/sec; 0.205 sec/batch; 18h:46m:14s remains)
INFO - root - 2017-12-10 04:05:13.569789: step 3620, loss = 2.28, batch loss = 2.23 (38.8 examples/sec; 0.206 sec/batch; 18h:51m:17s remains)
INFO - root - 2017-12-10 04:05:15.665483: step 3630, loss = 2.28, batch loss = 2.23 (38.7 examples/sec; 0.207 sec/batch; 18h:52m:31s remains)
INFO - root - 2017-12-10 04:05:17.723637: step 3640, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 20h:01m:03s remains)
INFO - root - 2017-12-10 04:05:19.778677: step 3650, loss = 2.28, batch loss = 2.23 (39.8 examples/sec; 0.201 sec/batch; 18h:21m:18s remains)
INFO - root - 2017-12-10 04:05:21.869854: step 3660, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:05m:05s remains)
INFO - root - 2017-12-10 04:05:23.918775: step 3670, loss = 2.28, batch loss = 2.23 (40.0 examples/sec; 0.200 sec/batch; 18h:15m:31s remains)
INFO - root - 2017-12-10 04:05:25.960236: step 3680, loss = 2.28, batch loss = 2.23 (39.0 examples/sec; 0.205 sec/batch; 18h:43m:16s remains)
INFO - root - 2017-12-10 04:05:28.041797: step 3690, loss = 2.28, batch loss = 2.23 (39.0 examples/sec; 0.205 sec/batch; 18h:43m:02s remains)
INFO - root - 2017-12-10 04:05:30.101820: step 3700, loss = 2.28, batch loss = 2.23 (38.6 examples/sec; 0.207 sec/batch; 18h:55m:31s remains)
2017-12-10 04:05:30.393944: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286666 -4.4286304 -4.4286304 -4.4286714 -4.4287143 -4.4287558 -4.4287739 -4.4287581 -4.4287057 -4.4286666 -4.4286747 -4.4287505 -4.4288554 -4.4289422 -4.4289904][-4.4285364 -4.4285192 -4.4285464 -4.4286141 -4.4286814 -4.4287324 -4.4287367 -4.4286981 -4.4286313 -4.4285851 -4.4285975 -4.4286833 -4.4288015 -4.4289064 -4.4289737][-4.4285026 -4.4285135 -4.4285645 -4.4286427 -4.4287219 -4.4287686 -4.4287491 -4.4286861 -4.4286108 -4.4285731 -4.4285946 -4.4286742 -4.428781 -4.428885 -4.4289594][-4.428546 -4.4285793 -4.4286447 -4.4287176 -4.4287739 -4.4287853 -4.4287224 -4.4286251 -4.4285517 -4.4285536 -4.4286084 -4.4286942 -4.4287887 -4.428885 -4.4289546][-4.4285879 -4.4286323 -4.4287043 -4.4287643 -4.42878 -4.4287362 -4.4286151 -4.42848 -4.4284296 -4.4284983 -4.4286079 -4.4287248 -4.4288239 -4.4289079 -4.4289613][-4.428669 -4.4286842 -4.4287324 -4.4287648 -4.4287295 -4.4286213 -4.4284296 -4.4282441 -4.42824 -4.4284043 -4.4285846 -4.4287395 -4.4288607 -4.4289412 -4.4289789][-4.4287443 -4.4287319 -4.4287448 -4.4287391 -4.4286485 -4.4284496 -4.4281316 -4.4278569 -4.4279251 -4.4282331 -4.4285126 -4.428719 -4.4288664 -4.4289551 -4.4289842][-4.4288139 -4.4288025 -4.4287834 -4.42873 -4.4285841 -4.4282966 -4.4278398 -4.4274507 -4.4276185 -4.4280796 -4.4284391 -4.4286838 -4.4288492 -4.4289451 -4.4289784][-4.4288774 -4.4288683 -4.4288192 -4.4287329 -4.4285765 -4.4282923 -4.4278688 -4.4275489 -4.4277234 -4.4281335 -4.428441 -4.4286613 -4.4288168 -4.42892 -4.4289618][-4.4288797 -4.4288759 -4.4288239 -4.4287415 -4.4286165 -4.428411 -4.4281278 -4.4279556 -4.4280758 -4.4283342 -4.4285293 -4.428688 -4.428812 -4.4289007 -4.428946][-4.4288745 -4.4288735 -4.4288239 -4.4287491 -4.4286618 -4.4285393 -4.4283876 -4.4283233 -4.4283977 -4.4285345 -4.4286437 -4.4287491 -4.4288392 -4.4289074 -4.4289446][-4.4288759 -4.428875 -4.4288335 -4.4287724 -4.4287148 -4.4286408 -4.4285712 -4.4285774 -4.4286332 -4.4286957 -4.4287462 -4.4288135 -4.4288797 -4.4289308 -4.4289541][-4.428863 -4.4288635 -4.4288316 -4.4287934 -4.4287605 -4.4287195 -4.4286904 -4.4287233 -4.4287682 -4.4287906 -4.4288096 -4.428853 -4.4289069 -4.4289474 -4.4289622][-4.4288464 -4.4288468 -4.4288206 -4.4287972 -4.428781 -4.4287591 -4.4287391 -4.4287686 -4.4288082 -4.4288278 -4.4288387 -4.4288764 -4.4289222 -4.4289565 -4.4289689][-4.4288373 -4.4288383 -4.4288192 -4.4288025 -4.4287896 -4.4287615 -4.4287257 -4.4287348 -4.4287724 -4.4288015 -4.4288278 -4.428875 -4.428925 -4.42896 -4.4289727]]...]
INFO - root - 2017-12-10 04:05:32.426523: step 3710, loss = 2.28, batch loss = 2.23 (39.8 examples/sec; 0.201 sec/batch; 18h:21m:41s remains)
INFO - root - 2017-12-10 04:05:34.463828: step 3720, loss = 2.28, batch loss = 2.23 (39.3 examples/sec; 0.204 sec/batch; 18h:36m:43s remains)
INFO - root - 2017-12-10 04:05:36.501427: step 3730, loss = 2.28, batch loss = 2.23 (39.5 examples/sec; 0.203 sec/batch; 18h:30m:14s remains)
INFO - root - 2017-12-10 04:05:38.555332: step 3740, loss = 2.28, batch loss = 2.23 (39.6 examples/sec; 0.202 sec/batch; 18h:27m:53s remains)
INFO - root - 2017-12-10 04:05:40.606270: step 3750, loss = 2.28, batch loss = 2.23 (38.7 examples/sec; 0.207 sec/batch; 18h:53m:02s remains)
INFO - root - 2017-12-10 04:05:42.647730: step 3760, loss = 2.28, batch loss = 2.23 (39.1 examples/sec; 0.205 sec/batch; 18h:40m:33s remains)
INFO - root - 2017-12-10 04:05:44.687828: step 3770, loss = 2.28, batch loss = 2.23 (38.7 examples/sec; 0.206 sec/batch; 18h:51m:15s remains)
INFO - root - 2017-12-10 04:05:46.726314: step 3780, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:38m:52s remains)
INFO - root - 2017-12-10 04:05:48.769744: step 3790, loss = 2.28, batch loss = 2.23 (39.5 examples/sec; 0.202 sec/batch; 18h:28m:56s remains)
INFO - root - 2017-12-10 04:05:50.805638: step 3800, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.202 sec/batch; 18h:24m:33s remains)
2017-12-10 04:05:51.092525: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289541 -4.4289417 -4.4289308 -4.4289203 -4.4289122 -4.4289103 -4.428915 -4.4289231 -4.42892 -4.4289141 -4.4289141 -4.4289231 -4.4289284 -4.4289289 -4.4289265][-4.4289546 -4.42894 -4.4289241 -4.4289103 -4.4288955 -4.4288845 -4.428884 -4.4288931 -4.4288907 -4.4288855 -4.4288874 -4.4289012 -4.4289141 -4.4289184 -4.4289179][-4.4289422 -4.428915 -4.4288855 -4.4288664 -4.4288516 -4.4288416 -4.42884 -4.4288507 -4.4288535 -4.4288535 -4.4288588 -4.4288735 -4.4288907 -4.4289007 -4.4289021][-4.4289165 -4.4288688 -4.4288177 -4.428782 -4.4287624 -4.4287653 -4.4287815 -4.4288054 -4.4288216 -4.4288321 -4.428843 -4.4288521 -4.4288707 -4.4288855 -4.428885][-4.428885 -4.4288125 -4.4287305 -4.42866 -4.428618 -4.428616 -4.4286466 -4.42869 -4.4287376 -4.4287777 -4.4288039 -4.4288197 -4.4288497 -4.42887 -4.4288678][-4.4288769 -4.4287872 -4.4286833 -4.4285741 -4.4284887 -4.4284449 -4.4284487 -4.4285007 -4.4285994 -4.4286904 -4.4287376 -4.4287686 -4.4288177 -4.4288483 -4.4288521][-4.4288912 -4.4287977 -4.4286833 -4.428546 -4.4284086 -4.4282837 -4.428194 -4.428206 -4.4283605 -4.4285207 -4.4286084 -4.4286742 -4.428751 -4.4287987 -4.4288182][-4.4289122 -4.42883 -4.4287243 -4.4285893 -4.4284306 -4.4282427 -4.4280357 -4.4279461 -4.4281082 -4.4283128 -4.42845 -4.4285669 -4.4286737 -4.4287405 -4.4287791][-4.4289346 -4.4288754 -4.4287992 -4.4287047 -4.4285831 -4.4284225 -4.42823 -4.4281058 -4.428165 -4.4283028 -4.428431 -4.4285622 -4.4286723 -4.4287395 -4.4287834][-4.4289474 -4.4289107 -4.4288645 -4.4288106 -4.4287343 -4.42863 -4.428514 -4.4284344 -4.4284296 -4.4284806 -4.42856 -4.4286561 -4.428731 -4.4287715 -4.4288058][-4.4289484 -4.4289293 -4.4289017 -4.4288745 -4.428833 -4.428772 -4.4287167 -4.4286819 -4.4286633 -4.4286747 -4.428719 -4.4287767 -4.4288154 -4.4288297 -4.4288464][-4.4289446 -4.4289308 -4.4289107 -4.4288931 -4.428875 -4.4288492 -4.4288349 -4.4288368 -4.4288244 -4.4288244 -4.4288492 -4.42888 -4.4288955 -4.4288945 -4.4288964][-4.4289441 -4.4289389 -4.428925 -4.4289112 -4.428905 -4.4289041 -4.4289126 -4.4289317 -4.4289241 -4.4289203 -4.4289365 -4.4289513 -4.4289517 -4.4289374 -4.4289255][-4.4289489 -4.4289441 -4.4289341 -4.4289231 -4.4289241 -4.428936 -4.4289517 -4.4289684 -4.4289627 -4.4289584 -4.4289675 -4.4289737 -4.428967 -4.4289489 -4.428937][-4.4289727 -4.428968 -4.4289603 -4.4289522 -4.4289517 -4.42896 -4.4289694 -4.4289756 -4.4289718 -4.4289694 -4.4289751 -4.428978 -4.4289708 -4.4289589 -4.4289527]]...]
INFO - root - 2017-12-10 04:05:53.143716: step 3810, loss = 2.28, batch loss = 2.23 (39.4 examples/sec; 0.203 sec/batch; 18h:31m:24s remains)
INFO - root - 2017-12-10 04:05:55.203940: step 3820, loss = 2.28, batch loss = 2.23 (38.9 examples/sec; 0.206 sec/batch; 18h:46m:15s remains)
INFO - root - 2017-12-10 04:05:57.249729: step 3830, loss = 2.28, batch loss = 2.23 (40.1 examples/sec; 0.200 sec/batch; 18h:12m:52s remains)
INFO - root - 2017-12-10 04:05:59.289618: step 3840, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.201 sec/batch; 18h:23m:37s remains)
INFO - root - 2017-12-10 04:06:01.331226: step 3850, loss = 2.28, batch loss = 2.23 (39.5 examples/sec; 0.203 sec/batch; 18h:30m:26s remains)
INFO - root - 2017-12-10 04:06:03.410082: step 3860, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:38m:48s remains)
INFO - root - 2017-12-10 04:06:05.490576: step 3870, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:38m:41s remains)
INFO - root - 2017-12-10 04:06:07.569038: step 3880, loss = 2.28, batch loss = 2.23 (38.5 examples/sec; 0.208 sec/batch; 18h:57m:09s remains)
INFO - root - 2017-12-10 04:06:09.663665: step 3890, loss = 2.28, batch loss = 2.23 (38.8 examples/sec; 0.206 sec/batch; 18h:48m:28s remains)
INFO - root - 2017-12-10 04:06:11.704640: step 3900, loss = 2.28, batch loss = 2.23 (39.4 examples/sec; 0.203 sec/batch; 18h:31m:51s remains)
2017-12-10 04:06:11.996793: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286923 -4.42853 -4.4284167 -4.4283934 -4.4284825 -4.4286041 -4.4286909 -4.4287329 -4.4287257 -4.4286852 -4.4286571 -4.4286451 -4.4286394 -4.4286385 -4.4286494][-4.4286704 -4.4284816 -4.4283476 -4.4283328 -4.4284544 -4.4285941 -4.428689 -4.4287395 -4.4287286 -4.42868 -4.4286489 -4.4286418 -4.4286389 -4.4286366 -4.4286451][-4.4286604 -4.4284606 -4.42832 -4.4283137 -4.4284577 -4.4286108 -4.4287043 -4.4287539 -4.4287372 -4.4286809 -4.4286456 -4.4286432 -4.4286423 -4.4286385 -4.4286494][-4.4286618 -4.4284539 -4.4283051 -4.4283066 -4.4284611 -4.4286232 -4.4287176 -4.4287634 -4.4287477 -4.4286909 -4.428647 -4.4286394 -4.42864 -4.4286408 -4.4286647][-4.4286757 -4.4284639 -4.4283056 -4.428309 -4.4284649 -4.4286346 -4.4287286 -4.4287729 -4.4287639 -4.4287176 -4.4286704 -4.4286566 -4.4286571 -4.4286661 -4.4287024][-4.4286995 -4.4284935 -4.4283381 -4.4283352 -4.428483 -4.428648 -4.4287324 -4.4287715 -4.4287686 -4.4287338 -4.4286942 -4.428679 -4.4286804 -4.4286976 -4.4287381][-4.4287205 -4.4285312 -4.4283915 -4.4283843 -4.4285169 -4.4286714 -4.4287457 -4.4287782 -4.4287825 -4.428751 -4.4287133 -4.4286966 -4.4287024 -4.4287257 -4.4287648][-4.4287367 -4.428566 -4.4284492 -4.4284487 -4.4285612 -4.4287028 -4.4287724 -4.4287977 -4.4288106 -4.4287848 -4.4287395 -4.4287105 -4.4287167 -4.4287477 -4.4287844][-4.4287562 -4.4285994 -4.4284987 -4.4285054 -4.4286036 -4.4287338 -4.4288015 -4.4288254 -4.4288468 -4.4288311 -4.4287848 -4.4287453 -4.4287419 -4.4287629 -4.4287887][-4.4287772 -4.4286304 -4.4285369 -4.428544 -4.4286308 -4.4287519 -4.4288173 -4.4288421 -4.4288669 -4.4288607 -4.428823 -4.4287858 -4.4287791 -4.4287934 -4.4288125][-4.4287896 -4.4286556 -4.42857 -4.4285731 -4.4286504 -4.4287634 -4.42883 -4.4288564 -4.4288783 -4.4288731 -4.4288421 -4.42881 -4.4288063 -4.4288249 -4.4288416][-4.4288044 -4.4286885 -4.4286113 -4.4286127 -4.4286785 -4.4287758 -4.4288397 -4.428864 -4.428885 -4.428885 -4.4288588 -4.42883 -4.4288282 -4.4288521 -4.428875][-4.4288273 -4.4287329 -4.4286695 -4.428668 -4.4287162 -4.4287915 -4.4288459 -4.4288678 -4.4288845 -4.428885 -4.4288621 -4.4288378 -4.42884 -4.4288673 -4.4288964][-4.428854 -4.4287825 -4.4287324 -4.4287291 -4.4287643 -4.4288197 -4.4288607 -4.4288745 -4.42888 -4.4288726 -4.4288487 -4.4288316 -4.42884 -4.4288654 -4.4288974][-4.4288907 -4.4288387 -4.4288015 -4.4287982 -4.4288239 -4.428864 -4.4288921 -4.4288964 -4.4288898 -4.4288721 -4.4288478 -4.4288373 -4.42885 -4.4288745 -4.4289041]]...]
INFO - root - 2017-12-10 04:06:14.070120: step 3910, loss = 2.28, batch loss = 2.23 (39.6 examples/sec; 0.202 sec/batch; 18h:26m:50s remains)
INFO - root - 2017-12-10 04:06:16.124090: step 3920, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:17m:25s remains)
INFO - root - 2017-12-10 04:06:18.193602: step 3930, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:21m:26s remains)
INFO - root - 2017-12-10 04:06:20.249370: step 3940, loss = 2.28, batch loss = 2.23 (39.1 examples/sec; 0.204 sec/batch; 18h:39m:03s remains)
INFO - root - 2017-12-10 04:06:22.308901: step 3950, loss = 2.28, batch loss = 2.23 (38.2 examples/sec; 0.209 sec/batch; 19h:06m:27s remains)
INFO - root - 2017-12-10 04:06:24.365869: step 3960, loss = 2.28, batch loss = 2.23 (39.0 examples/sec; 0.205 sec/batch; 18h:43m:41s remains)
INFO - root - 2017-12-10 04:06:26.448272: step 3970, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:38m:04s remains)
INFO - root - 2017-12-10 04:06:28.489829: step 3980, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:38m:48s remains)
INFO - root - 2017-12-10 04:06:30.552188: step 3990, loss = 2.28, batch loss = 2.23 (39.3 examples/sec; 0.204 sec/batch; 18h:35m:37s remains)
INFO - root - 2017-12-10 04:06:32.612821: step 4000, loss = 2.28, batch loss = 2.23 (39.1 examples/sec; 0.204 sec/batch; 18h:39m:32s remains)
2017-12-10 04:06:32.874342: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288111 -4.4288254 -4.4288607 -4.4288797 -4.4288945 -4.4289131 -4.4289269 -4.4289432 -4.4289556 -4.4289622 -4.4289646 -4.4289536 -4.4289417 -4.4289293 -4.42892][-4.42879 -4.4288077 -4.4288507 -4.4288716 -4.4288788 -4.4288912 -4.4289036 -4.4289193 -4.4289341 -4.4289455 -4.42895 -4.428937 -4.4289241 -4.4289126 -4.4289026][-4.4287753 -4.4287825 -4.4288144 -4.4288297 -4.4288292 -4.4288335 -4.4288445 -4.4288678 -4.4288979 -4.4289265 -4.4289412 -4.4289346 -4.4289203 -4.4289055 -4.428885][-4.4287586 -4.4287472 -4.4287615 -4.4287677 -4.4287639 -4.4287658 -4.4287777 -4.42881 -4.4288592 -4.4289088 -4.4289379 -4.4289384 -4.4289217 -4.4288988 -4.4288659][-4.4287405 -4.4287119 -4.4287128 -4.4287167 -4.4287143 -4.4287167 -4.4287305 -4.4287696 -4.4288292 -4.4288869 -4.4289241 -4.4289312 -4.4289165 -4.4288883 -4.4288464][-4.4287462 -4.4287076 -4.4286966 -4.4286966 -4.4286933 -4.4286909 -4.4286942 -4.4287271 -4.4287858 -4.4288435 -4.4288869 -4.428906 -4.4289041 -4.4288821 -4.4288378][-4.4287829 -4.4287415 -4.428721 -4.4287128 -4.4287 -4.428679 -4.4286523 -4.4286556 -4.4287052 -4.428772 -4.4288321 -4.428874 -4.4288969 -4.428895 -4.4288621][-4.4288254 -4.4287972 -4.4287739 -4.4287543 -4.4287176 -4.4286652 -4.428597 -4.42855 -4.4285851 -4.4286771 -4.4287696 -4.4288425 -4.4288921 -4.4289117 -4.4289][-4.4288511 -4.4288435 -4.42883 -4.4288058 -4.42875 -4.42867 -4.4285641 -4.4284682 -4.4284859 -4.4285984 -4.42872 -4.4288163 -4.4288826 -4.4289174 -4.428925][-4.4288683 -4.42888 -4.4288759 -4.428853 -4.4287963 -4.4287124 -4.4285994 -4.4284892 -4.4284844 -4.4285812 -4.4286985 -4.4287968 -4.4288673 -4.4289117 -4.4289355][-4.4288764 -4.428896 -4.4288979 -4.4288793 -4.4288354 -4.4287677 -4.4286704 -4.4285741 -4.4285564 -4.4286227 -4.4287143 -4.4287958 -4.4288616 -4.4289088 -4.4289432][-4.4288917 -4.4289122 -4.4289155 -4.4288988 -4.4288654 -4.4288144 -4.4287367 -4.4286633 -4.428648 -4.4286952 -4.4287648 -4.4288249 -4.4288797 -4.4289222 -4.4289546][-4.4289274 -4.4289455 -4.4289446 -4.4289236 -4.4288955 -4.4288573 -4.4288025 -4.4287543 -4.4287453 -4.4287806 -4.4288311 -4.42887 -4.4289103 -4.4289441 -4.4289689][-4.4289546 -4.4289694 -4.4289627 -4.4289346 -4.4289045 -4.4288726 -4.4288392 -4.4288173 -4.4288168 -4.4288435 -4.4288826 -4.42891 -4.4289341 -4.428957 -4.4289746][-4.4289579 -4.4289718 -4.4289627 -4.428925 -4.4288807 -4.428844 -4.42882 -4.4288168 -4.4288282 -4.4288545 -4.428896 -4.4289241 -4.4289422 -4.4289589 -4.4289751]]...]
INFO - root - 2017-12-10 04:06:34.914569: step 4010, loss = 2.28, batch loss = 2.23 (39.6 examples/sec; 0.202 sec/batch; 18h:26m:52s remains)
INFO - root - 2017-12-10 04:06:36.956210: step 4020, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.202 sec/batch; 18h:23m:54s remains)
INFO - root - 2017-12-10 04:06:38.998884: step 4030, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:37m:43s remains)
INFO - root - 2017-12-10 04:06:41.035200: step 4040, loss = 2.28, batch loss = 2.23 (38.8 examples/sec; 0.206 sec/batch; 18h:49m:48s remains)
INFO - root - 2017-12-10 04:06:43.074061: step 4050, loss = 2.28, batch loss = 2.23 (39.0 examples/sec; 0.205 sec/batch; 18h:41m:35s remains)
INFO - root - 2017-12-10 04:06:45.124645: step 4060, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.201 sec/batch; 18h:22m:23s remains)
INFO - root - 2017-12-10 04:06:47.164523: step 4070, loss = 2.28, batch loss = 2.23 (38.4 examples/sec; 0.208 sec/batch; 18h:59m:24s remains)
INFO - root - 2017-12-10 04:06:49.203089: step 4080, loss = 2.28, batch loss = 2.23 (39.4 examples/sec; 0.203 sec/batch; 18h:32m:03s remains)
INFO - root - 2017-12-10 04:06:51.237445: step 4090, loss = 2.28, batch loss = 2.23 (39.5 examples/sec; 0.203 sec/batch; 18h:29m:27s remains)
INFO - root - 2017-12-10 04:06:53.324298: step 4100, loss = 2.28, batch loss = 2.23 (39.5 examples/sec; 0.203 sec/batch; 18h:28m:45s remains)
2017-12-10 04:06:53.596894: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289775 -4.4289737 -4.4289742 -4.4289708 -4.4289627 -4.4289579 -4.4289594 -4.4289637 -4.4289689 -4.4289732 -4.4289756 -4.428978 -4.4289789 -4.4289804 -4.4289837][-4.4289241 -4.4289165 -4.4289155 -4.4289103 -4.4288983 -4.4288926 -4.4288979 -4.4289107 -4.4289231 -4.4289303 -4.4289351 -4.4289393 -4.4289422 -4.4289451 -4.4289489][-4.4288654 -4.4288535 -4.42885 -4.4288421 -4.4288216 -4.42881 -4.4288182 -4.4288464 -4.428875 -4.4288912 -4.4289002 -4.4289083 -4.4289141 -4.428916 -4.428915][-4.4288354 -4.4288192 -4.4288092 -4.4287877 -4.4287486 -4.4287195 -4.4287205 -4.4287567 -4.4288082 -4.4288511 -4.4288793 -4.4288979 -4.4289074 -4.4289069 -4.4288974][-4.4288378 -4.4288177 -4.4287987 -4.4287591 -4.4286909 -4.4286246 -4.4285884 -4.4286079 -4.4286823 -4.428771 -4.4288373 -4.4288754 -4.4288945 -4.4288974 -4.428885][-4.4288421 -4.428822 -4.4287968 -4.4287443 -4.4286489 -4.4285326 -4.428421 -4.4283729 -4.4284439 -4.4285917 -4.4287152 -4.4287868 -4.4288249 -4.4288449 -4.4288497][-4.4288487 -4.4288311 -4.4288039 -4.4287424 -4.4286323 -4.4284763 -4.4282827 -4.4281263 -4.4281497 -4.4283457 -4.428524 -4.4286265 -4.4286923 -4.4287462 -4.4287868][-4.4288654 -4.4288573 -4.4288387 -4.4287848 -4.42869 -4.4285407 -4.4283352 -4.4281387 -4.4281063 -4.4282641 -4.4284244 -4.428515 -4.4285827 -4.4286594 -4.4287291][-4.428885 -4.4288907 -4.4288836 -4.4288449 -4.428782 -4.4286819 -4.4285417 -4.4284086 -4.428381 -4.4284658 -4.4285464 -4.4285789 -4.4286079 -4.4286594 -4.4287138][-4.4289021 -4.4289207 -4.428925 -4.4288983 -4.428854 -4.4287958 -4.4287262 -4.4286695 -4.4286661 -4.4287124 -4.4287362 -4.428721 -4.4287047 -4.4287114 -4.4287295][-4.4289126 -4.4289436 -4.4289641 -4.4289556 -4.4289293 -4.4288993 -4.4288726 -4.4288573 -4.4288597 -4.4288774 -4.4288745 -4.4288373 -4.4287949 -4.4287715 -4.4287629][-4.4289002 -4.4289336 -4.428966 -4.4289751 -4.4289665 -4.4289565 -4.4289503 -4.4289494 -4.428946 -4.4289436 -4.4289289 -4.4288878 -4.4288445 -4.4288135 -4.4287987][-4.4288726 -4.4289021 -4.4289389 -4.4289603 -4.428966 -4.4289665 -4.4289684 -4.4289665 -4.428956 -4.4289436 -4.4289231 -4.4288883 -4.4288573 -4.4288397 -4.4288273][-4.4288454 -4.4288673 -4.4289021 -4.4289293 -4.4289417 -4.4289441 -4.4289422 -4.4289317 -4.4289155 -4.4289002 -4.4288831 -4.4288616 -4.4288459 -4.4288387 -4.4288263][-4.4288216 -4.4288378 -4.4288712 -4.4289002 -4.4289141 -4.4289117 -4.4288955 -4.4288735 -4.4288564 -4.428844 -4.4288292 -4.42881 -4.4288 -4.4287944 -4.4287868]]...]
INFO - root - 2017-12-10 04:06:55.653883: step 4110, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:16m:04s remains)
INFO - root - 2017-12-10 04:06:57.725251: step 4120, loss = 2.28, batch loss = 2.23 (40.1 examples/sec; 0.199 sec/batch; 18h:10m:46s remains)
INFO - root - 2017-12-10 04:06:59.772943: step 4130, loss = 2.28, batch loss = 2.23 (40.0 examples/sec; 0.200 sec/batch; 18h:14m:02s remains)
INFO - root - 2017-12-10 04:07:01.804618: step 4140, loss = 2.28, batch loss = 2.23 (39.8 examples/sec; 0.201 sec/batch; 18h:21m:23s remains)
INFO - root - 2017-12-10 04:07:03.851976: step 4150, loss = 2.28, batch loss = 2.23 (40.0 examples/sec; 0.200 sec/batch; 18h:15m:28s remains)
INFO - root - 2017-12-10 04:07:05.905743: step 4160, loss = 2.28, batch loss = 2.23 (38.8 examples/sec; 0.206 sec/batch; 18h:48m:55s remains)
INFO - root - 2017-12-10 04:07:07.947335: step 4170, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.201 sec/batch; 18h:21m:26s remains)
INFO - root - 2017-12-10 04:07:10.009928: step 4180, loss = 2.28, batch loss = 2.23 (39.0 examples/sec; 0.205 sec/batch; 18h:42m:52s remains)
INFO - root - 2017-12-10 04:07:12.055844: step 4190, loss = 2.28, batch loss = 2.23 (38.6 examples/sec; 0.207 sec/batch; 18h:53m:02s remains)
INFO - root - 2017-12-10 04:07:14.102496: step 4200, loss = 2.28, batch loss = 2.23 (38.5 examples/sec; 0.208 sec/batch; 18h:57m:08s remains)
2017-12-10 04:07:14.485322: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287519 -4.4287643 -4.4287834 -4.4287934 -4.4287863 -4.4287481 -4.428668 -4.4286051 -4.4286046 -4.4286427 -4.4286551 -4.4286404 -4.4286666 -4.428721 -4.42878][-4.4287729 -4.4287944 -4.4288139 -4.4288154 -4.4288044 -4.4287658 -4.4286904 -4.428628 -4.4286203 -4.4286423 -4.4286394 -4.4286103 -4.4286194 -4.4286604 -4.42872][-4.4288135 -4.4288344 -4.4288511 -4.4288464 -4.4288263 -4.4287763 -4.4286981 -4.42864 -4.428637 -4.4286556 -4.4286489 -4.4286146 -4.4286141 -4.4286408 -4.4286928][-4.4288568 -4.4288659 -4.4288716 -4.4288507 -4.4288106 -4.4287357 -4.4286413 -4.428586 -4.4286017 -4.4286418 -4.4286561 -4.4286323 -4.4286337 -4.4286561 -4.428699][-4.4288859 -4.4288836 -4.4288745 -4.428844 -4.4287829 -4.4286771 -4.4285541 -4.4284959 -4.4285321 -4.428607 -4.4286547 -4.4286571 -4.4286737 -4.4286976 -4.4287305][-4.4289079 -4.4289045 -4.4288888 -4.4288564 -4.4287767 -4.4286451 -4.4285 -4.4284291 -4.4284697 -4.4285693 -4.4286518 -4.4286871 -4.4287186 -4.4287477 -4.4287734][-4.4289236 -4.4289265 -4.4289141 -4.42888 -4.428793 -4.42866 -4.4285164 -4.4284282 -4.4284368 -4.4285178 -4.4286175 -4.4286847 -4.4287324 -4.4287653 -4.4287963][-4.4289169 -4.4289236 -4.4289145 -4.4288821 -4.4288073 -4.4286952 -4.4285769 -4.4284825 -4.4284534 -4.4284906 -4.4285841 -4.4286766 -4.4287477 -4.4287896 -4.4288239][-4.4289136 -4.4289169 -4.4289136 -4.4288898 -4.4288325 -4.4287472 -4.4286518 -4.4285617 -4.4285078 -4.4285088 -4.4285784 -4.4286785 -4.428762 -4.4288111 -4.4288483][-4.4289255 -4.4289246 -4.428925 -4.428915 -4.4288793 -4.4288154 -4.4287348 -4.4286451 -4.428575 -4.428555 -4.4286013 -4.4286971 -4.4287848 -4.4288397 -4.4288774][-4.4289517 -4.4289479 -4.4289484 -4.4289446 -4.428926 -4.4288845 -4.4288168 -4.4287329 -4.4286628 -4.428637 -4.4286685 -4.42875 -4.42883 -4.4288888 -4.4289289][-4.4289689 -4.4289632 -4.4289608 -4.4289651 -4.4289627 -4.42894 -4.4288917 -4.4288244 -4.4287667 -4.4287386 -4.4287524 -4.4288073 -4.4288721 -4.428925 -4.4289608][-4.428966 -4.4289589 -4.428957 -4.4289618 -4.428966 -4.4289603 -4.4289355 -4.428895 -4.4288564 -4.4288321 -4.428834 -4.4288616 -4.4288988 -4.4289341 -4.4289627][-4.4289589 -4.4289517 -4.4289508 -4.428956 -4.4289622 -4.4289656 -4.4289594 -4.42894 -4.4289207 -4.4289055 -4.428905 -4.4289169 -4.4289284 -4.4289422 -4.4289589][-4.4289503 -4.4289451 -4.4289427 -4.4289479 -4.4289556 -4.4289608 -4.4289622 -4.4289541 -4.428946 -4.4289403 -4.4289412 -4.4289465 -4.4289441 -4.4289446 -4.4289503]]...]
INFO - root - 2017-12-10 04:07:16.526031: step 4210, loss = 2.28, batch loss = 2.23 (39.1 examples/sec; 0.205 sec/batch; 18h:40m:15s remains)
INFO - root - 2017-12-10 04:07:18.581576: step 4220, loss = 2.28, batch loss = 2.23 (38.6 examples/sec; 0.207 sec/batch; 18h:53m:50s remains)
INFO - root - 2017-12-10 04:07:20.655315: step 4230, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.210 sec/batch; 19h:10m:21s remains)
INFO - root - 2017-12-10 04:07:22.741566: step 4240, loss = 2.28, batch loss = 2.23 (38.7 examples/sec; 0.207 sec/batch; 18h:50m:18s remains)
INFO - root - 2017-12-10 04:07:24.813653: step 4250, loss = 2.28, batch loss = 2.23 (39.5 examples/sec; 0.202 sec/batch; 18h:26m:58s remains)
INFO - root - 2017-12-10 04:07:26.854418: step 4260, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.202 sec/batch; 18h:22m:50s remains)
INFO - root - 2017-12-10 04:07:28.896514: step 4270, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:37m:11s remains)
INFO - root - 2017-12-10 04:07:30.970025: step 4280, loss = 2.28, batch loss = 2.23 (39.8 examples/sec; 0.201 sec/batch; 18h:20m:31s remains)
INFO - root - 2017-12-10 04:07:33.015268: step 4290, loss = 2.28, batch loss = 2.23 (38.6 examples/sec; 0.207 sec/batch; 18h:53m:02s remains)
INFO - root - 2017-12-10 04:07:35.053165: step 4300, loss = 2.28, batch loss = 2.23 (39.1 examples/sec; 0.205 sec/batch; 18h:40m:16s remains)
2017-12-10 04:07:35.379397: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290895 -4.42908 -4.4290709 -4.4290643 -4.42906 -4.4290581 -4.429059 -4.4290643 -4.4290709 -4.4290729 -4.42907 -4.4290667 -4.4290671 -4.4290705 -4.4290724][-4.4290853 -4.4290657 -4.4290466 -4.4290328 -4.4290242 -4.4290209 -4.4290252 -4.4290361 -4.4290466 -4.4290533 -4.4290533 -4.4290514 -4.4290533 -4.4290624 -4.4290714][-4.4290581 -4.4290242 -4.4289932 -4.4289713 -4.4289556 -4.4289451 -4.4289441 -4.4289541 -4.4289694 -4.4289875 -4.4289985 -4.4290061 -4.429018 -4.4290366 -4.4290566][-4.4290118 -4.4289546 -4.4289055 -4.4288692 -4.4288383 -4.4288087 -4.4287906 -4.42879 -4.4288087 -4.4288521 -4.4288936 -4.4289222 -4.4289489 -4.4289823 -4.4290218][-4.4289713 -4.4288869 -4.4288073 -4.4287381 -4.4286718 -4.42861 -4.4285717 -4.4285574 -4.4285688 -4.4286318 -4.4287124 -4.4287806 -4.4288416 -4.428905 -4.4289732][-4.4289417 -4.4288316 -4.4287133 -4.4286013 -4.4284897 -4.4283791 -4.4283085 -4.4282718 -4.4282789 -4.4283652 -4.4284873 -4.4286013 -4.4287086 -4.4288168 -4.4289212][-4.4289188 -4.4287925 -4.4286461 -4.4284949 -4.4283447 -4.4282022 -4.4280858 -4.4280024 -4.4279947 -4.4281025 -4.4282603 -4.4284172 -4.4285765 -4.4287395 -4.4288807][-4.4289174 -4.4287925 -4.4286451 -4.4284883 -4.4283333 -4.428175 -4.428019 -4.4278827 -4.4278421 -4.42795 -4.4281311 -4.4283233 -4.4285221 -4.4287181 -4.4288735][-4.4289432 -4.4288368 -4.4287119 -4.4285789 -4.4284487 -4.4283037 -4.4281483 -4.4280052 -4.4279604 -4.428051 -4.42822 -4.4284129 -4.4286036 -4.4287815 -4.4289126][-4.4289923 -4.4289107 -4.4288168 -4.4287214 -4.4286308 -4.4285235 -4.4284034 -4.4282956 -4.4282746 -4.42835 -4.4284778 -4.4286232 -4.4287672 -4.4288979 -4.4289856][-4.4290376 -4.4289856 -4.4289246 -4.428864 -4.4288063 -4.4287415 -4.4286661 -4.4286036 -4.4286027 -4.4286585 -4.4287395 -4.4288316 -4.428925 -4.4290051 -4.4290514][-4.4290657 -4.4290385 -4.4290056 -4.4289708 -4.4289393 -4.4289103 -4.428875 -4.4288459 -4.4288473 -4.4288793 -4.4289269 -4.42898 -4.42903 -4.4290662 -4.42908][-4.429081 -4.4290662 -4.4290495 -4.4290295 -4.4290118 -4.4289989 -4.428988 -4.4289823 -4.4289851 -4.4290004 -4.4290252 -4.4290519 -4.4290719 -4.4290795 -4.4290767][-4.42909 -4.4290814 -4.4290709 -4.4290576 -4.4290452 -4.4290366 -4.4290328 -4.4290328 -4.4290366 -4.4290452 -4.4290571 -4.4290681 -4.4290724 -4.4290705 -4.4290638][-4.4290953 -4.4290891 -4.4290829 -4.4290738 -4.4290633 -4.4290538 -4.4290476 -4.4290462 -4.4290504 -4.4290566 -4.4290619 -4.4290652 -4.4290657 -4.4290619 -4.4290557]]...]
INFO - root - 2017-12-10 04:07:37.476782: step 4310, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 19h:09m:42s remains)
INFO - root - 2017-12-10 04:07:39.600621: step 4320, loss = 2.28, batch loss = 2.23 (39.1 examples/sec; 0.204 sec/batch; 18h:38m:13s remains)
INFO - root - 2017-12-10 04:07:41.690820: step 4330, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:15m:10s remains)
INFO - root - 2017-12-10 04:07:43.808686: step 4340, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:39m:25s remains)
INFO - root - 2017-12-10 04:07:45.954762: step 4350, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:25m:00s remains)
INFO - root - 2017-12-10 04:07:48.080991: step 4360, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:29m:23s remains)
INFO - root - 2017-12-10 04:07:50.199814: step 4370, loss = 2.28, batch loss = 2.23 (39.1 examples/sec; 0.205 sec/batch; 18h:39m:35s remains)
INFO - root - 2017-12-10 04:07:52.343521: step 4380, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:34m:52s remains)
INFO - root - 2017-12-10 04:07:54.440205: step 4390, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:35m:57s remains)
INFO - root - 2017-12-10 04:07:56.558527: step 4400, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:28m:22s remains)
2017-12-10 04:07:56.873468: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287248 -4.4287405 -4.4287467 -4.4287324 -4.4287243 -4.42874 -4.4287605 -4.4287558 -4.4287248 -4.4286947 -4.4286947 -4.4287214 -4.428762 -4.42879 -4.4287724][-4.4286938 -4.4287133 -4.4287195 -4.4287124 -4.4287262 -4.4287629 -4.4287815 -4.4287596 -4.42872 -4.4286828 -4.4286785 -4.4287076 -4.428741 -4.4287562 -4.4287338][-4.4286613 -4.4286828 -4.4286866 -4.4286776 -4.428689 -4.4287181 -4.4287343 -4.4287248 -4.4287014 -4.4286671 -4.4286523 -4.4286709 -4.428689 -4.4286952 -4.4286895][-4.4285822 -4.428606 -4.4286237 -4.428617 -4.4286165 -4.4286289 -4.4286404 -4.4286461 -4.4286456 -4.4286237 -4.4286079 -4.4286132 -4.42861 -4.4286022 -4.4286041][-4.4284949 -4.4285269 -4.42856 -4.4285583 -4.4285483 -4.42855 -4.4285583 -4.4285574 -4.4285536 -4.4285474 -4.4285393 -4.42854 -4.4285288 -4.428515 -4.4285192][-4.4284711 -4.4285092 -4.4285407 -4.428534 -4.4285169 -4.4285069 -4.4284983 -4.4284673 -4.4284534 -4.4284706 -4.4284749 -4.4284663 -4.4284391 -4.4284163 -4.4284225][-4.42848 -4.4285078 -4.4285131 -4.4284816 -4.428441 -4.4283977 -4.42834 -4.4282618 -4.4282579 -4.4283266 -4.4283547 -4.4283586 -4.4283481 -4.4283338 -4.4283533][-4.4285331 -4.4285245 -4.4284921 -4.4284315 -4.4283643 -4.4282751 -4.4281559 -4.4280376 -4.4280691 -4.4281969 -4.4282517 -4.4282827 -4.4283185 -4.4283452 -4.4284019][-4.4286423 -4.428606 -4.428556 -4.4284987 -4.4284348 -4.4283361 -4.428215 -4.428123 -4.4281726 -4.428287 -4.428328 -4.4283676 -4.4284315 -4.4284859 -4.4285564][-4.4287472 -4.4287071 -4.4286637 -4.4286256 -4.4285822 -4.4285126 -4.4284439 -4.4284067 -4.4284492 -4.4285135 -4.4285254 -4.4285502 -4.4286032 -4.4286456 -4.4286981][-4.4288225 -4.4287963 -4.4287672 -4.4287429 -4.4287114 -4.4286723 -4.428647 -4.4286413 -4.4286671 -4.4286971 -4.4286957 -4.4287081 -4.4287348 -4.4287539 -4.42878][-4.4288797 -4.4288635 -4.428843 -4.4288249 -4.4288054 -4.4287891 -4.4287834 -4.4287825 -4.4287891 -4.4288006 -4.4288 -4.4288092 -4.4288187 -4.4288182 -4.4288278][-4.4289269 -4.4289112 -4.4288926 -4.4288769 -4.4288673 -4.428865 -4.4288645 -4.4288597 -4.4288554 -4.4288578 -4.4288554 -4.4288545 -4.4288478 -4.428834 -4.4288306][-4.4289584 -4.4289455 -4.4289312 -4.4289212 -4.4289165 -4.4289188 -4.4289188 -4.4289141 -4.4289079 -4.4289055 -4.4288993 -4.4288855 -4.4288616 -4.428833 -4.4288158][-4.4289804 -4.4289694 -4.4289603 -4.4289527 -4.4289489 -4.4289489 -4.42895 -4.4289513 -4.4289513 -4.4289503 -4.428946 -4.4289327 -4.4289069 -4.4288735 -4.4288425]]...]
INFO - root - 2017-12-10 04:07:58.991375: step 4410, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.211 sec/batch; 19h:11m:26s remains)
INFO - root - 2017-12-10 04:08:01.126099: step 4420, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 19h:07m:16s remains)
INFO - root - 2017-12-10 04:08:03.234273: step 4430, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 19h:33m:03s remains)
INFO - root - 2017-12-10 04:08:05.333012: step 4440, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:36m:45s remains)
INFO - root - 2017-12-10 04:08:07.427062: step 4450, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.214 sec/batch; 19h:27m:43s remains)
INFO - root - 2017-12-10 04:08:09.536457: step 4460, loss = 2.28, batch loss = 2.23 (38.7 examples/sec; 0.207 sec/batch; 18h:49m:39s remains)
INFO - root - 2017-12-10 04:08:11.637554: step 4470, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:45m:04s remains)
INFO - root - 2017-12-10 04:08:13.729046: step 4480, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 19h:17m:21s remains)
INFO - root - 2017-12-10 04:08:15.830554: step 4490, loss = 2.28, batch loss = 2.23 (39.1 examples/sec; 0.205 sec/batch; 18h:39m:57s remains)
INFO - root - 2017-12-10 04:08:17.897431: step 4500, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.210 sec/batch; 19h:10m:31s remains)
2017-12-10 04:08:18.242574: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42876 -4.4287529 -4.4287629 -4.4287581 -4.4287634 -4.4287748 -4.4287572 -4.4287133 -4.428668 -4.4286113 -4.4285769 -4.4285889 -4.4286385 -4.4287219 -4.4288015][-4.4287524 -4.4287505 -4.4287615 -4.42875 -4.4287467 -4.4287472 -4.4287252 -4.428678 -4.4286232 -4.4285545 -4.4285197 -4.428544 -4.4286056 -4.4287019 -4.428792][-4.4287448 -4.4287448 -4.4287467 -4.4287243 -4.4287181 -4.4287176 -4.4286976 -4.4286551 -4.4285984 -4.4285135 -4.4284739 -4.4285111 -4.4285884 -4.4286957 -4.4287939][-4.4287353 -4.4287276 -4.4287119 -4.4286695 -4.428659 -4.4286671 -4.4286609 -4.428638 -4.4285831 -4.4284868 -4.428441 -4.4284849 -4.4285712 -4.4286842 -4.4287896][-4.4287348 -4.4287152 -4.428678 -4.428618 -4.4286 -4.4286151 -4.4286222 -4.4286218 -4.4285765 -4.4284692 -4.4284177 -4.4284596 -4.4285474 -4.4286661 -4.4287796][-4.4287415 -4.4287181 -4.4286671 -4.4285932 -4.4285507 -4.4285426 -4.4285445 -4.4285622 -4.4285321 -4.4284377 -4.4283929 -4.4284339 -4.4285216 -4.4286447 -4.4287663][-4.4287386 -4.4287324 -4.4286914 -4.428617 -4.428544 -4.4284954 -4.4284716 -4.4284873 -4.4284639 -4.4283824 -4.4283495 -4.4283953 -4.4284849 -4.4286141 -4.4287477][-4.4287133 -4.42873 -4.4287119 -4.4286518 -4.4285703 -4.4285064 -4.4284668 -4.4284668 -4.4284234 -4.428339 -4.428308 -4.4283562 -4.4284458 -4.4285841 -4.4287314][-4.4286752 -4.4287109 -4.4287186 -4.4286852 -4.428616 -4.4285526 -4.4285069 -4.4284887 -4.4284334 -4.4283347 -4.4282937 -4.4283342 -4.4284225 -4.4285636 -4.428719][-4.428658 -4.4286981 -4.4287281 -4.4287233 -4.4286819 -4.4286313 -4.4285865 -4.4285512 -4.4284844 -4.4283733 -4.4283118 -4.4283404 -4.4284244 -4.4285626 -4.4287186][-4.4286847 -4.42871 -4.4287429 -4.4287519 -4.4287314 -4.4287057 -4.4286709 -4.428628 -4.42856 -4.4284511 -4.4283834 -4.4284015 -4.428472 -4.4285951 -4.4287391][-4.4287224 -4.4287295 -4.4287481 -4.4287543 -4.4287453 -4.4287448 -4.4287338 -4.4286985 -4.428638 -4.4285426 -4.4284821 -4.4284978 -4.4285569 -4.428658 -4.4287777][-4.4287581 -4.4287505 -4.4287553 -4.4287562 -4.4287562 -4.4287724 -4.428781 -4.4287629 -4.4287176 -4.42864 -4.4285927 -4.4286113 -4.4286618 -4.4287391 -4.4288325][-4.4287996 -4.4287844 -4.4287796 -4.4287748 -4.4287782 -4.4287972 -4.4288163 -4.4288149 -4.4287844 -4.4287271 -4.4286957 -4.4287171 -4.42876 -4.4288197 -4.4288874][-4.4288468 -4.428834 -4.4288335 -4.428834 -4.4288373 -4.4288492 -4.428865 -4.4288712 -4.4288535 -4.4288149 -4.4287958 -4.4288154 -4.4288487 -4.4288907 -4.4289317]]...]
INFO - root - 2017-12-10 04:08:20.330360: step 4510, loss = 2.28, batch loss = 2.23 (39.4 examples/sec; 0.203 sec/batch; 18h:29m:58s remains)
INFO - root - 2017-12-10 04:08:22.511867: step 4520, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:53m:59s remains)
INFO - root - 2017-12-10 04:08:24.643210: step 4530, loss = 2.28, batch loss = 2.23 (39.0 examples/sec; 0.205 sec/batch; 18h:41m:49s remains)
INFO - root - 2017-12-10 04:08:26.764304: step 4540, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:36m:50s remains)
INFO - root - 2017-12-10 04:08:28.872741: step 4550, loss = 2.28, batch loss = 2.23 (38.2 examples/sec; 0.209 sec/batch; 19h:03m:13s remains)
INFO - root - 2017-12-10 04:08:31.010648: step 4560, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 19h:17m:39s remains)
INFO - root - 2017-12-10 04:08:33.083344: step 4570, loss = 2.28, batch loss = 2.23 (39.6 examples/sec; 0.202 sec/batch; 18h:25m:01s remains)
INFO - root - 2017-12-10 04:08:35.189289: step 4580, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:36m:26s remains)
INFO - root - 2017-12-10 04:08:37.302337: step 4590, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.210 sec/batch; 19h:09m:20s remains)
INFO - root - 2017-12-10 04:08:39.417289: step 4600, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 19h:08m:55s remains)
2017-12-10 04:08:39.734335: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289188 -4.4289217 -4.4289227 -4.4289265 -4.4289293 -4.4289279 -4.4289227 -4.4289079 -4.4288831 -4.4288454 -4.4287972 -4.4287529 -4.4287338 -4.4287515 -4.4287925][-4.42892 -4.4289293 -4.4289341 -4.4289403 -4.4289432 -4.4289384 -4.4289284 -4.4289079 -4.4288793 -4.4288378 -4.4287858 -4.4287362 -4.4287133 -4.4287314 -4.4287739][-4.4289269 -4.4289389 -4.4289474 -4.428956 -4.4289608 -4.428957 -4.4289489 -4.4289322 -4.4289103 -4.42888 -4.4288397 -4.4287987 -4.4287763 -4.4287863 -4.4288139][-4.4289322 -4.4289417 -4.4289489 -4.4289541 -4.4289575 -4.4289513 -4.4289408 -4.4289255 -4.4289126 -4.4289021 -4.4288845 -4.4288616 -4.4288497 -4.4288578 -4.428874][-4.4289203 -4.4289241 -4.4289255 -4.4289222 -4.4289174 -4.4289 -4.4288764 -4.4288511 -4.4288392 -4.4288464 -4.4288526 -4.428853 -4.4288583 -4.4288769 -4.428895][-4.4288731 -4.4288697 -4.4288616 -4.4288454 -4.4288282 -4.428793 -4.4287486 -4.4287047 -4.4286947 -4.4287243 -4.4287543 -4.4287739 -4.428791 -4.428823 -4.4288511][-4.4288011 -4.4287882 -4.4287658 -4.42873 -4.428689 -4.42863 -4.4285583 -4.4284925 -4.4284906 -4.428555 -4.4286075 -4.4286418 -4.4286661 -4.4287086 -4.4287505][-4.4287276 -4.4287028 -4.4286685 -4.4286141 -4.4285474 -4.4284649 -4.4283667 -4.4282756 -4.4282742 -4.4283638 -4.42843 -4.4284739 -4.4285107 -4.4285674 -4.4286313][-4.4286938 -4.4286671 -4.4286361 -4.4285879 -4.4285259 -4.4284544 -4.4283648 -4.428277 -4.4282613 -4.4283214 -4.4283533 -4.4283695 -4.4283915 -4.4284458 -4.4285254][-4.4287281 -4.4287114 -4.4286933 -4.4286661 -4.4286313 -4.4285955 -4.4285445 -4.4284906 -4.428473 -4.4284925 -4.4284797 -4.4284525 -4.4284382 -4.4284644 -4.4285312][-4.4287853 -4.4287829 -4.4287729 -4.4287596 -4.4287472 -4.4287376 -4.4287195 -4.428699 -4.4286952 -4.4287033 -4.4286823 -4.4286475 -4.4286237 -4.42863 -4.4286671][-4.4287977 -4.4288087 -4.4288054 -4.4288025 -4.4288025 -4.4288068 -4.4288058 -4.4288044 -4.428813 -4.4288239 -4.428813 -4.4287887 -4.4287724 -4.4287782 -4.4287982][-4.4287834 -4.4288058 -4.4288092 -4.4288144 -4.428822 -4.4288297 -4.428833 -4.4288387 -4.4288507 -4.42886 -4.4288554 -4.4288387 -4.4288297 -4.4288373 -4.4288511][-4.4287758 -4.4288025 -4.428812 -4.4288244 -4.4288354 -4.4288406 -4.4288421 -4.4288468 -4.4288549 -4.4288578 -4.4288497 -4.428833 -4.4288249 -4.4288344 -4.4288511][-4.4287815 -4.4287996 -4.4288073 -4.4288197 -4.4288297 -4.4288349 -4.4288383 -4.4288416 -4.4288445 -4.4288397 -4.4288254 -4.4288049 -4.4287939 -4.4288054 -4.4288268]]...]
INFO - root - 2017-12-10 04:08:41.868306: step 4610, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.210 sec/batch; 19h:09m:37s remains)
INFO - root - 2017-12-10 04:08:43.979338: step 4620, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:52m:11s remains)
INFO - root - 2017-12-10 04:08:46.117889: step 4630, loss = 2.28, batch loss = 2.23 (38.4 examples/sec; 0.208 sec/batch; 18h:59m:10s remains)
INFO - root - 2017-12-10 04:08:48.239836: step 4640, loss = 2.28, batch loss = 2.23 (38.5 examples/sec; 0.208 sec/batch; 18h:55m:27s remains)
INFO - root - 2017-12-10 04:08:50.373650: step 4650, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 19h:17m:09s remains)
INFO - root - 2017-12-10 04:08:52.516705: step 4660, loss = 2.28, batch loss = 2.23 (38.8 examples/sec; 0.206 sec/batch; 18h:45m:58s remains)
INFO - root - 2017-12-10 04:08:54.664383: step 4670, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:33m:47s remains)
INFO - root - 2017-12-10 04:08:56.750306: step 4680, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 19h:07m:59s remains)
INFO - root - 2017-12-10 04:08:58.889150: step 4690, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:43m:01s remains)
INFO - root - 2017-12-10 04:09:00.982107: step 4700, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:14m:10s remains)
2017-12-10 04:09:01.328040: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288096 -4.428791 -4.4287977 -4.4288206 -4.428833 -4.4288287 -4.4287963 -4.4287729 -4.4287796 -4.4287806 -4.4287462 -4.4287047 -4.4287019 -4.4287558 -4.4288073][-4.4287906 -4.428771 -4.428791 -4.4288268 -4.4288449 -4.4288349 -4.4287982 -4.4287677 -4.4287658 -4.4287753 -4.428762 -4.4287348 -4.4287381 -4.4287944 -4.4288507][-4.428751 -4.4287467 -4.4287815 -4.4288249 -4.4288416 -4.4288154 -4.42877 -4.4287405 -4.428741 -4.4287605 -4.4287567 -4.4287391 -4.4287405 -4.4287992 -4.4288621][-4.4287162 -4.4287243 -4.428772 -4.4288192 -4.4288292 -4.42878 -4.4287214 -4.4287057 -4.4287233 -4.428751 -4.4287472 -4.4287286 -4.42873 -4.428792 -4.42886][-4.4287181 -4.4287176 -4.4287658 -4.4288054 -4.4287934 -4.4287019 -4.4286156 -4.4286222 -4.4286838 -4.4287324 -4.4287276 -4.4287081 -4.4287162 -4.4287906 -4.428863][-4.4287286 -4.428721 -4.4287615 -4.4287791 -4.428721 -4.4285688 -4.4284472 -4.428483 -4.4286056 -4.4286885 -4.4286819 -4.428659 -4.4286823 -4.4287753 -4.4288659][-4.4287386 -4.4287267 -4.4287558 -4.4287462 -4.4286432 -4.4284391 -4.4282894 -4.4283528 -4.4285269 -4.4286332 -4.4286237 -4.4285975 -4.4286423 -4.4287519 -4.4288626][-4.4287686 -4.4287596 -4.4287815 -4.4287558 -4.4286308 -4.4284229 -4.428278 -4.4283495 -4.4285283 -4.4286284 -4.4286089 -4.4285808 -4.4286404 -4.4287596 -4.4288783][-4.428812 -4.428812 -4.4288297 -4.4287992 -4.4286909 -4.428524 -4.4284043 -4.4284544 -4.428596 -4.4286771 -4.4286518 -4.4286222 -4.4286842 -4.4288 -4.4289136][-4.428844 -4.4288487 -4.4288654 -4.4288411 -4.4287658 -4.42865 -4.4285545 -4.4285741 -4.4286718 -4.4287367 -4.428721 -4.4286909 -4.4287457 -4.4288521 -4.4289451][-4.4288669 -4.428874 -4.4288874 -4.4288731 -4.4288239 -4.4287486 -4.4286742 -4.4286737 -4.4287424 -4.4288 -4.4287987 -4.4287682 -4.4288025 -4.4288888 -4.4289584][-4.4288721 -4.4288821 -4.4288983 -4.4288945 -4.428865 -4.4288211 -4.4287682 -4.428761 -4.4288063 -4.4288568 -4.4288664 -4.42884 -4.4288568 -4.4289169 -4.4289627][-4.4288735 -4.4288907 -4.4289131 -4.4289174 -4.4289007 -4.4288754 -4.4288445 -4.4288425 -4.4288759 -4.4289122 -4.4289203 -4.4288979 -4.4289069 -4.4289446 -4.428968][-4.4288845 -4.4289055 -4.428926 -4.4289331 -4.4289265 -4.4289107 -4.4288964 -4.4289041 -4.4289346 -4.4289565 -4.428956 -4.4289365 -4.4289408 -4.4289584 -4.4289632][-4.4288912 -4.4289107 -4.428925 -4.4289322 -4.4289327 -4.4289231 -4.4289155 -4.4289269 -4.4289565 -4.4289713 -4.4289665 -4.4289517 -4.42895 -4.4289508 -4.4289432]]...]
INFO - root - 2017-12-10 04:09:03.449137: step 4710, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:35m:51s remains)
INFO - root - 2017-12-10 04:09:05.587487: step 4720, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:37m:51s remains)
INFO - root - 2017-12-10 04:09:07.740297: step 4730, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 21h:16m:24s remains)
INFO - root - 2017-12-10 04:09:09.905463: step 4740, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:49m:38s remains)
INFO - root - 2017-12-10 04:09:12.017075: step 4750, loss = 2.28, batch loss = 2.23 (38.7 examples/sec; 0.207 sec/batch; 18h:50m:19s remains)
INFO - root - 2017-12-10 04:09:14.128670: step 4760, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:50m:46s remains)
INFO - root - 2017-12-10 04:09:16.219812: step 4770, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 19h:06m:41s remains)
INFO - root - 2017-12-10 04:09:18.344927: step 4780, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:26m:48s remains)
INFO - root - 2017-12-10 04:09:20.464964: step 4790, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 19h:17m:17s remains)
INFO - root - 2017-12-10 04:09:22.602101: step 4800, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:42m:41s remains)
2017-12-10 04:09:22.933579: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286823 -4.4286714 -4.4286957 -4.42874 -4.4287553 -4.4287724 -4.4287834 -4.4287825 -4.4287934 -4.4287939 -4.4287815 -4.4287596 -4.4287505 -4.4287357 -4.4287109][-4.4286737 -4.4286594 -4.4286776 -4.428709 -4.4287138 -4.42873 -4.4287329 -4.4287291 -4.4287419 -4.4287481 -4.4287434 -4.4287248 -4.4287133 -4.4286971 -4.4286728][-4.4286723 -4.4286523 -4.4286666 -4.4286947 -4.4286942 -4.4287062 -4.4287014 -4.4286914 -4.4287071 -4.4287176 -4.4287214 -4.4287105 -4.4287019 -4.4286861 -4.428659][-4.4286642 -4.4286351 -4.4286456 -4.4286752 -4.4286723 -4.4286733 -4.428658 -4.4286442 -4.4286647 -4.4286814 -4.42869 -4.4286876 -4.4286785 -4.4286585 -4.4286294][-4.4286494 -4.4286008 -4.4285951 -4.4286079 -4.4286013 -4.428597 -4.4285741 -4.428556 -4.4285817 -4.4286046 -4.4286222 -4.4286275 -4.4286113 -4.4285831 -4.4285531][-4.4286666 -4.4286036 -4.4285784 -4.4285712 -4.4285526 -4.4285326 -4.4284964 -4.428473 -4.4285073 -4.42855 -4.4285817 -4.428596 -4.4285779 -4.4285488 -4.428515][-4.4287114 -4.4286489 -4.4286108 -4.42859 -4.4285545 -4.4285126 -4.4284554 -4.4284239 -4.4284658 -4.4285231 -4.4285688 -4.4285955 -4.4285913 -4.42857 -4.4285345][-4.4287333 -4.4286785 -4.4286418 -4.428617 -4.4285793 -4.4285321 -4.4284768 -4.4284468 -4.42848 -4.4285331 -4.4285779 -4.4286017 -4.4285979 -4.4285717 -4.4285388][-4.4287386 -4.4286952 -4.428668 -4.4286613 -4.42865 -4.4286232 -4.4285913 -4.4285736 -4.4285932 -4.4286251 -4.4286523 -4.4286604 -4.4286432 -4.4286041 -4.4285669][-4.4287448 -4.4287162 -4.4287052 -4.4287233 -4.4287381 -4.4287314 -4.428719 -4.4287124 -4.4287252 -4.4287415 -4.4287539 -4.4287462 -4.4287205 -4.4286819 -4.428647][-4.4287415 -4.4287229 -4.4287314 -4.4287682 -4.4287934 -4.4288011 -4.4288054 -4.4288058 -4.4288154 -4.4288273 -4.4288354 -4.4288254 -4.4287996 -4.4287663 -4.4287415][-4.428741 -4.4287281 -4.4287457 -4.4287944 -4.4288292 -4.4288521 -4.4288692 -4.4288764 -4.4288859 -4.4288964 -4.4289031 -4.4288969 -4.4288759 -4.4288449 -4.4288216][-4.4287672 -4.4287524 -4.4287653 -4.4288182 -4.42886 -4.4288878 -4.4289083 -4.4289174 -4.4289222 -4.428926 -4.4289303 -4.4289293 -4.4289179 -4.4288974 -4.4288774][-4.4287972 -4.4287777 -4.428781 -4.4288249 -4.428863 -4.4288883 -4.4289083 -4.428916 -4.428916 -4.4289122 -4.4289126 -4.4289122 -4.4289041 -4.4288888 -4.4288731][-4.4288287 -4.4288015 -4.4287963 -4.4288321 -4.4288664 -4.4288864 -4.4289036 -4.4289145 -4.4289126 -4.4289036 -4.4288983 -4.428895 -4.4288931 -4.4288812 -4.42887]]...]
INFO - root - 2017-12-10 04:09:25.048350: step 4810, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 19h:08m:14s remains)
INFO - root - 2017-12-10 04:09:27.231470: step 4820, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:35m:28s remains)
INFO - root - 2017-12-10 04:09:29.411570: step 4830, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 19h:31m:05s remains)
INFO - root - 2017-12-10 04:09:31.551490: step 4840, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:39m:47s remains)
INFO - root - 2017-12-10 04:09:33.691641: step 4850, loss = 2.28, batch loss = 2.23 (38.5 examples/sec; 0.208 sec/batch; 18h:54m:12s remains)
INFO - root - 2017-12-10 04:09:35.832560: step 4860, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 19h:05m:47s remains)
INFO - root - 2017-12-10 04:09:37.966130: step 4870, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:17m:59s remains)
INFO - root - 2017-12-10 04:09:40.105479: step 4880, loss = 2.28, batch loss = 2.23 (38.4 examples/sec; 0.209 sec/batch; 18h:58m:45s remains)
INFO - root - 2017-12-10 04:09:42.237939: step 4890, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:16m:39s remains)
INFO - root - 2017-12-10 04:09:44.373377: step 4900, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:27m:16s remains)
2017-12-10 04:09:44.734567: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428688 -4.4287629 -4.4288206 -4.4288244 -4.4287586 -4.4287176 -4.4287233 -4.4287262 -4.4287543 -4.4288087 -4.42887 -4.4289103 -4.428905 -4.4288611 -4.428843][-4.4287119 -4.4287977 -4.4288573 -4.4288597 -4.4287977 -4.4287567 -4.4287548 -4.4287543 -4.4287839 -4.428843 -4.4289036 -4.4289427 -4.4289441 -4.4289026 -4.4288726][-4.428709 -4.4288082 -4.4288712 -4.4288745 -4.4288173 -4.4287705 -4.4287562 -4.4287577 -4.4288015 -4.4288731 -4.4289327 -4.428967 -4.4289718 -4.4289312 -4.4288888][-4.4286861 -4.4287982 -4.4288688 -4.4288721 -4.4288111 -4.4287519 -4.4287233 -4.4287324 -4.4288006 -4.428885 -4.4289432 -4.4289718 -4.4289761 -4.4289351 -4.4288807][-4.4286532 -4.4287682 -4.4288468 -4.4288559 -4.4287939 -4.4287157 -4.4286633 -4.4286752 -4.4287653 -4.4288621 -4.4289231 -4.4289536 -4.4289632 -4.4289303 -4.4288735][-4.4286351 -4.4287395 -4.428822 -4.42884 -4.4287772 -4.4286747 -4.4285855 -4.4285779 -4.4286895 -4.4288096 -4.428884 -4.4289255 -4.4289389 -4.4289055 -4.4288464][-4.4286513 -4.4287362 -4.4288144 -4.4288378 -4.4287753 -4.4286532 -4.4285207 -4.4284754 -4.4286036 -4.4287562 -4.4288545 -4.42891 -4.4289217 -4.4288754 -4.42881][-4.4287071 -4.4287696 -4.4288292 -4.4288454 -4.4287872 -4.4286652 -4.4285188 -4.4284554 -4.4285855 -4.4287529 -4.428865 -4.4289289 -4.4289389 -4.4288845 -4.4288116][-4.4287677 -4.428802 -4.4288311 -4.4288273 -4.428772 -4.4286742 -4.4285569 -4.42852 -4.4286389 -4.428791 -4.4288974 -4.4289613 -4.4289756 -4.4289274 -4.4288554][-4.428793 -4.4287963 -4.4287982 -4.4287753 -4.4287167 -4.428647 -4.428587 -4.4285979 -4.4287052 -4.4288287 -4.4289169 -4.4289742 -4.4289927 -4.4289603 -4.4289002][-4.4287829 -4.4287682 -4.4287581 -4.4287257 -4.4286675 -4.4286294 -4.4286284 -4.4286718 -4.4287653 -4.4288621 -4.4289331 -4.4289789 -4.4289942 -4.4289718 -4.4289284][-4.4287615 -4.4287405 -4.4287291 -4.4286995 -4.4286509 -4.4286423 -4.4286795 -4.4287353 -4.4288135 -4.4288898 -4.428947 -4.4289789 -4.4289832 -4.4289651 -4.4289374][-4.4287786 -4.4287553 -4.4287429 -4.4287167 -4.428679 -4.4286847 -4.4287367 -4.4287953 -4.428863 -4.4289217 -4.4289551 -4.428968 -4.4289603 -4.4289417 -4.4289269][-4.4288268 -4.4288063 -4.428793 -4.4287658 -4.4287314 -4.4287415 -4.4287949 -4.4288487 -4.4289045 -4.428946 -4.4289613 -4.4289618 -4.428947 -4.4289289 -4.4289246][-4.4288678 -4.4288597 -4.4288554 -4.4288378 -4.4288073 -4.4288116 -4.42885 -4.4288878 -4.4289227 -4.428946 -4.4289527 -4.4289489 -4.4289374 -4.4289269 -4.4289303]]...]
INFO - root - 2017-12-10 04:09:46.871292: step 4910, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:33m:54s remains)
INFO - root - 2017-12-10 04:09:49.017906: step 4920, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:20m:42s remains)
INFO - root - 2017-12-10 04:09:51.192126: step 4930, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:21m:49s remains)
INFO - root - 2017-12-10 04:09:53.324049: step 4940, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 20h:08m:23s remains)
INFO - root - 2017-12-10 04:09:55.537308: step 4950, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:57m:03s remains)
INFO - root - 2017-12-10 04:09:57.666223: step 4960, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 19h:06m:13s remains)
INFO - root - 2017-12-10 04:09:59.801969: step 4970, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.211 sec/batch; 19h:10m:30s remains)
INFO - root - 2017-12-10 04:10:01.931272: step 4980, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:20m:52s remains)
INFO - root - 2017-12-10 04:10:04.079570: step 4990, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:33m:34s remains)
INFO - root - 2017-12-10 04:10:06.198575: step 5000, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:20m:10s remains)
2017-12-10 04:10:06.549962: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287381 -4.4287686 -4.4287982 -4.4287953 -4.4287362 -4.4286752 -4.4286318 -4.4286108 -4.4286251 -4.4286609 -4.42872 -4.4287882 -4.4288521 -4.4289074 -4.4289279][-4.4287086 -4.4287548 -4.4287953 -4.428793 -4.4287372 -4.4286852 -4.4286427 -4.4286118 -4.4286051 -4.4286337 -4.4286876 -4.4287577 -4.4288244 -4.4288764 -4.4288974][-4.4286871 -4.42874 -4.4287815 -4.428782 -4.4287362 -4.428699 -4.4286656 -4.428628 -4.4286065 -4.4286275 -4.4286723 -4.4287353 -4.4287968 -4.4288425 -4.4288635][-4.4286394 -4.4286942 -4.4287415 -4.4287548 -4.4287286 -4.4287105 -4.4286795 -4.4286222 -4.4285879 -4.428616 -4.4286609 -4.428719 -4.428772 -4.4288125 -4.4288397][-4.4285855 -4.4286547 -4.4287095 -4.4287348 -4.4287348 -4.4287362 -4.4286985 -4.4286008 -4.4285545 -4.4286046 -4.4286537 -4.4287009 -4.428741 -4.4287724 -4.4288068][-4.4285817 -4.4286733 -4.4287376 -4.4287624 -4.42876 -4.4287496 -4.4286785 -4.4285264 -4.4284759 -4.4285564 -4.4286189 -4.428658 -4.42869 -4.4287243 -4.4287705][-4.4286137 -4.4287038 -4.4287586 -4.4287686 -4.4287481 -4.4287004 -4.428566 -4.4283471 -4.42831 -4.42846 -4.4285622 -4.4286108 -4.4286447 -4.4286866 -4.4287472][-4.4286432 -4.4287148 -4.4287586 -4.4287567 -4.4287095 -4.4286137 -4.4283996 -4.428092 -4.4280982 -4.4283581 -4.4285159 -4.42857 -4.4285946 -4.4286375 -4.4287062][-4.428688 -4.4287286 -4.4287548 -4.4287419 -4.4286809 -4.4285784 -4.42836 -4.4280424 -4.428093 -4.4283905 -4.4285383 -4.4285583 -4.4285483 -4.4285731 -4.4286427][-4.4287553 -4.428771 -4.4287839 -4.4287648 -4.4287071 -4.4286408 -4.4285 -4.4282856 -4.4283323 -4.4285388 -4.4286175 -4.4285946 -4.42856 -4.4285665 -4.4286246][-4.4288316 -4.4288278 -4.4288249 -4.4288082 -4.4287629 -4.4287248 -4.42865 -4.4285207 -4.4285488 -4.4286647 -4.4286895 -4.4286394 -4.4285884 -4.4285836 -4.428627][-4.428874 -4.4288588 -4.4288516 -4.4288406 -4.42881 -4.4287829 -4.4287415 -4.428669 -4.4286842 -4.4287395 -4.4287391 -4.4286785 -4.4286218 -4.4286189 -4.428659][-4.4288855 -4.4288726 -4.4288712 -4.4288735 -4.42886 -4.4288373 -4.4288077 -4.4287663 -4.4287729 -4.4287953 -4.42879 -4.4287395 -4.428689 -4.428679 -4.4286966][-4.4289126 -4.4289017 -4.4288983 -4.4289012 -4.4288945 -4.4288793 -4.4288568 -4.4288316 -4.42883 -4.42884 -4.42884 -4.428812 -4.4287786 -4.4287572 -4.4287472][-4.4289331 -4.4289236 -4.4289193 -4.428915 -4.428905 -4.4288979 -4.4288869 -4.4288707 -4.4288626 -4.4288683 -4.4288707 -4.4288573 -4.428843 -4.4288278 -4.4288111]]...]
INFO - root - 2017-12-10 04:10:08.676489: step 5010, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:48m:14s remains)
INFO - root - 2017-12-10 04:10:10.776588: step 5020, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.211 sec/batch; 19h:13m:51s remains)
INFO - root - 2017-12-10 04:10:12.913573: step 5030, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:33m:31s remains)
INFO - root - 2017-12-10 04:10:15.052001: step 5040, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 19h:16m:21s remains)
INFO - root - 2017-12-10 04:10:17.177191: step 5050, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:37m:35s remains)
INFO - root - 2017-12-10 04:10:19.317387: step 5060, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:45m:41s remains)
INFO - root - 2017-12-10 04:10:21.474240: step 5070, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:43m:54s remains)
INFO - root - 2017-12-10 04:10:23.653695: step 5080, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:43m:17s remains)
INFO - root - 2017-12-10 04:10:25.785756: step 5090, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.211 sec/batch; 19h:13m:57s remains)
INFO - root - 2017-12-10 04:10:27.959296: step 5100, loss = 2.28, batch loss = 2.23 (33.8 examples/sec; 0.236 sec/batch; 21h:30m:00s remains)
2017-12-10 04:10:28.308127: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286957 -4.4287372 -4.4287639 -4.4287572 -4.42877 -4.42879 -4.4287858 -4.4287677 -4.4287596 -4.4287815 -4.4288321 -4.4288387 -4.42879 -4.428731 -4.4287][-4.4286265 -4.4286647 -4.4287057 -4.4287119 -4.4287248 -4.42874 -4.4287362 -4.4287238 -4.4287138 -4.4287424 -4.4288049 -4.4288158 -4.4287567 -4.4286895 -4.4286537][-4.42867 -4.4286914 -4.4287248 -4.4287329 -4.4287314 -4.4287286 -4.4287081 -4.4286914 -4.4286919 -4.4287367 -4.4288111 -4.4288306 -4.4287767 -4.4287181 -4.4286904][-4.4287658 -4.4287686 -4.4287834 -4.4287825 -4.4287639 -4.428731 -4.428688 -4.4286685 -4.4286876 -4.4287448 -4.4288192 -4.4288416 -4.4288054 -4.4287682 -4.4287596][-4.4288306 -4.4288244 -4.4288135 -4.4287896 -4.4287496 -4.4286919 -4.4286232 -4.4286027 -4.4286504 -4.4287171 -4.4287891 -4.4288168 -4.4288087 -4.428802 -4.4288144][-4.4288163 -4.4288096 -4.4287839 -4.4287305 -4.4286551 -4.4285655 -4.4284654 -4.4284406 -4.428524 -4.428616 -4.4286962 -4.4287462 -4.4287691 -4.4287953 -4.4288292][-4.4287534 -4.4287453 -4.4287076 -4.4286165 -4.4284921 -4.4283485 -4.4281993 -4.4281673 -4.4283066 -4.4284515 -4.4285617 -4.4286475 -4.4287 -4.4287524 -4.4288073][-4.4286971 -4.4286895 -4.4286509 -4.4285378 -4.4283872 -4.4282184 -4.4280462 -4.4280205 -4.4282112 -4.4284005 -4.4285297 -4.4286251 -4.428678 -4.4287276 -4.4287834][-4.4287252 -4.4287119 -4.4286737 -4.4285731 -4.4284515 -4.4283319 -4.4282212 -4.4282289 -4.4283953 -4.4285479 -4.4286394 -4.4287009 -4.4287162 -4.4287319 -4.4287682][-4.4287872 -4.4287648 -4.4287319 -4.42866 -4.428576 -4.4285045 -4.4284477 -4.4284668 -4.4285855 -4.4286833 -4.4287362 -4.4287581 -4.4287386 -4.4287281 -4.4287457][-4.4288173 -4.4288082 -4.428793 -4.4287481 -4.4286966 -4.428658 -4.4286394 -4.4286633 -4.4287338 -4.4287829 -4.4287996 -4.4287896 -4.4287529 -4.4287281 -4.4287343][-4.428833 -4.4288363 -4.4288363 -4.4288154 -4.4287891 -4.4287753 -4.4287786 -4.4288011 -4.4288349 -4.4288483 -4.42884 -4.428813 -4.428771 -4.4287391 -4.4287362][-4.4288692 -4.4288735 -4.4288788 -4.428875 -4.4288645 -4.42886 -4.4288678 -4.4288793 -4.4288864 -4.4288812 -4.428863 -4.4288325 -4.4287934 -4.4287596 -4.4287524][-4.4289341 -4.4289408 -4.4289455 -4.4289441 -4.4289379 -4.4289336 -4.4289351 -4.4289351 -4.4289289 -4.4289169 -4.4288988 -4.428875 -4.4288449 -4.4288182 -4.4288154][-4.42897 -4.4289804 -4.4289856 -4.4289861 -4.4289842 -4.4289823 -4.4289813 -4.428978 -4.4289708 -4.4289603 -4.4289479 -4.4289341 -4.4289184 -4.4289026 -4.4289002]]...]
INFO - root - 2017-12-10 04:10:30.438682: step 5110, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:18m:21s remains)
INFO - root - 2017-12-10 04:10:32.549413: step 5120, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:27m:29s remains)
INFO - root - 2017-12-10 04:10:34.681605: step 5130, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.210 sec/batch; 19h:08m:19s remains)
INFO - root - 2017-12-10 04:10:36.800996: step 5140, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:21m:38s remains)
INFO - root - 2017-12-10 04:10:38.893114: step 5150, loss = 2.28, batch loss = 2.23 (38.5 examples/sec; 0.208 sec/batch; 18h:54m:36s remains)
INFO - root - 2017-12-10 04:10:41.016222: step 5160, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.212 sec/batch; 19h:19m:14s remains)
INFO - root - 2017-12-10 04:10:43.159442: step 5170, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:27m:17s remains)
INFO - root - 2017-12-10 04:10:45.258781: step 5180, loss = 2.28, batch loss = 2.23 (38.9 examples/sec; 0.206 sec/batch; 18h:41m:31s remains)
INFO - root - 2017-12-10 04:10:47.415738: step 5190, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 19h:14m:29s remains)
INFO - root - 2017-12-10 04:10:49.567445: step 5200, loss = 2.28, batch loss = 2.23 (38.8 examples/sec; 0.206 sec/batch; 18h:43m:33s remains)
2017-12-10 04:10:49.909420: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289508 -4.4289393 -4.42893 -4.4289274 -4.4289308 -4.4289355 -4.4289408 -4.4289455 -4.4289494 -4.4289536 -4.4289584 -4.4289613 -4.4289584 -4.4289517 -4.4289451][-4.428956 -4.4289427 -4.4289241 -4.4289107 -4.4289088 -4.428915 -4.4289255 -4.4289374 -4.4289465 -4.4289517 -4.4289546 -4.428956 -4.4289541 -4.4289484 -4.4289436][-4.4289508 -4.4289393 -4.4289126 -4.428884 -4.4288712 -4.4288797 -4.4289012 -4.428925 -4.4289446 -4.4289556 -4.4289579 -4.4289556 -4.4289532 -4.42895 -4.4289474][-4.4289241 -4.4289145 -4.4288769 -4.4288278 -4.428802 -4.4288149 -4.4288545 -4.4288993 -4.428937 -4.4289594 -4.4289665 -4.4289622 -4.428957 -4.4289532 -4.4289517][-4.4288869 -4.428884 -4.4288254 -4.4287448 -4.4286952 -4.4287071 -4.4287682 -4.4288454 -4.4289117 -4.4289527 -4.42897 -4.4289694 -4.4289641 -4.4289594 -4.4289575][-4.4288473 -4.4288626 -4.4287858 -4.42866 -4.4285641 -4.4285593 -4.4286385 -4.4287534 -4.4288597 -4.4289312 -4.4289613 -4.4289665 -4.4289646 -4.4289608 -4.4289603][-4.4288054 -4.4288507 -4.4287696 -4.4285975 -4.4284363 -4.428391 -4.428483 -4.4286332 -4.42878 -4.4288893 -4.4289422 -4.4289551 -4.428956 -4.4289536 -4.428956][-4.4287715 -4.4288416 -4.4287734 -4.428586 -4.4283681 -4.42825 -4.4283242 -4.4285016 -4.4286838 -4.4288287 -4.4289103 -4.4289365 -4.4289384 -4.428936 -4.4289417][-4.4287763 -4.4288392 -4.4287896 -4.4286327 -4.4284344 -4.4282846 -4.4282913 -4.4284353 -4.4286127 -4.4287686 -4.4288692 -4.4289145 -4.4289212 -4.4289165 -4.4289231][-4.4288335 -4.428853 -4.4288082 -4.4287019 -4.4285784 -4.4284859 -4.4284635 -4.4285254 -4.4286385 -4.428761 -4.4288535 -4.428906 -4.4289212 -4.4289145 -4.428915][-4.4288888 -4.428863 -4.4288154 -4.4287534 -4.4287033 -4.4286742 -4.4286656 -4.4286885 -4.4287386 -4.4288073 -4.4288726 -4.4289188 -4.4289384 -4.428936 -4.4289269][-4.4289007 -4.4288507 -4.428803 -4.4287696 -4.4287677 -4.4287772 -4.4287858 -4.4287906 -4.4288 -4.4288321 -4.4288821 -4.4289284 -4.4289551 -4.4289589 -4.4289484][-4.4288735 -4.4288135 -4.4287715 -4.4287605 -4.4287829 -4.4288025 -4.4288082 -4.4287906 -4.4287672 -4.4287839 -4.42884 -4.4289045 -4.4289494 -4.4289641 -4.4289579][-4.42883 -4.428772 -4.4287419 -4.4287505 -4.4287825 -4.4287887 -4.428761 -4.4287004 -4.4286447 -4.428658 -4.4287343 -4.4288306 -4.4289112 -4.4289513 -4.4289536][-4.4287896 -4.4287429 -4.4287295 -4.4287586 -4.4288 -4.4287934 -4.428719 -4.4285893 -4.4284806 -4.4284811 -4.428586 -4.4287195 -4.4288378 -4.428915 -4.4289384]]...]
INFO - root - 2017-12-10 04:10:52.054389: step 5210, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:36m:29s remains)
INFO - root - 2017-12-10 04:10:54.218349: step 5220, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.210 sec/batch; 19h:07m:15s remains)
INFO - root - 2017-12-10 04:10:56.340559: step 5230, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.223 sec/batch; 20h:13m:42s remains)
INFO - root - 2017-12-10 04:10:58.501293: step 5240, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:34m:22s remains)
INFO - root - 2017-12-10 04:11:00.623122: step 5250, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:48m:16s remains)
INFO - root - 2017-12-10 04:11:02.749376: step 5260, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.210 sec/batch; 19h:07m:32s remains)
INFO - root - 2017-12-10 04:11:04.880490: step 5270, loss = 2.28, batch loss = 2.23 (38.3 examples/sec; 0.209 sec/batch; 18h:57m:42s remains)
INFO - root - 2017-12-10 04:11:06.995013: step 5280, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:56m:41s remains)
INFO - root - 2017-12-10 04:11:09.120453: step 5290, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:20m:22s remains)
INFO - root - 2017-12-10 04:11:11.232732: step 5300, loss = 2.28, batch loss = 2.23 (38.3 examples/sec; 0.209 sec/batch; 19h:00m:14s remains)
2017-12-10 04:11:11.586708: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288912 -4.4288774 -4.4288754 -4.4288759 -4.4288797 -4.4288807 -4.4288826 -4.4288826 -4.4288845 -4.4288979 -4.4289083 -4.4289169 -4.4289222 -4.4289203 -4.4289207][-4.4288616 -4.4288392 -4.4288325 -4.4288363 -4.4288421 -4.42885 -4.4288573 -4.4288588 -4.4288549 -4.428863 -4.4288678 -4.4288707 -4.4288726 -4.4288707 -4.4288769][-4.4288425 -4.428813 -4.4287963 -4.4288 -4.4288063 -4.4288211 -4.4288359 -4.4288392 -4.4288349 -4.4288421 -4.4288397 -4.428834 -4.4288321 -4.428833 -4.4288449][-4.4288111 -4.4287767 -4.4287491 -4.4287472 -4.4287529 -4.4287724 -4.4287953 -4.4288073 -4.4288087 -4.4288173 -4.4288111 -4.4288039 -4.4287977 -4.4288058 -4.4288235][-4.42876 -4.4287252 -4.4286895 -4.428679 -4.4286838 -4.4287081 -4.428731 -4.4287486 -4.4287624 -4.4287786 -4.42878 -4.4287796 -4.4287734 -4.4287858 -4.4288073][-4.4286928 -4.4286556 -4.4286051 -4.4285736 -4.4285684 -4.42859 -4.428606 -4.4286337 -4.4286742 -4.4287152 -4.42874 -4.4287591 -4.4287605 -4.4287739 -4.4287953][-4.428616 -4.4285731 -4.4285073 -4.4284339 -4.4283977 -4.4284034 -4.4284081 -4.4284539 -4.4285283 -4.428606 -4.4286675 -4.4287143 -4.4287367 -4.4287553 -4.4287791][-4.4285741 -4.4285288 -4.4284587 -4.4283571 -4.4282894 -4.4282632 -4.4282389 -4.4282827 -4.42838 -4.4284916 -4.4285812 -4.4286494 -4.4286895 -4.4287233 -4.4287605][-4.4286056 -4.4285755 -4.4285264 -4.4284463 -4.4283895 -4.4283462 -4.4282951 -4.4283071 -4.4283762 -4.4284778 -4.428565 -4.4286308 -4.4286714 -4.4287119 -4.428761][-4.4286857 -4.4286747 -4.4286532 -4.4286084 -4.4285808 -4.4285512 -4.4285069 -4.4284983 -4.4285307 -4.4286003 -4.4286613 -4.4287047 -4.428731 -4.4287581 -4.4287982][-4.4287848 -4.4287882 -4.4287848 -4.4287615 -4.4287496 -4.4287348 -4.42871 -4.4287009 -4.4287124 -4.4287558 -4.4287953 -4.4288244 -4.4288368 -4.4288445 -4.4288621][-4.4288712 -4.4288836 -4.4288878 -4.428875 -4.42887 -4.4288669 -4.4288597 -4.4288549 -4.4288583 -4.4288831 -4.4289079 -4.4289289 -4.4289331 -4.4289255 -4.4289222][-4.4289217 -4.4289236 -4.4289279 -4.4289284 -4.4289312 -4.4289341 -4.4289351 -4.4289346 -4.4289365 -4.4289513 -4.428967 -4.42898 -4.4289794 -4.4289656 -4.428956][-4.4289412 -4.4289336 -4.4289355 -4.4289412 -4.428947 -4.4289489 -4.4289503 -4.4289513 -4.4289546 -4.4289665 -4.4289794 -4.428988 -4.4289856 -4.4289742 -4.428966][-4.4289455 -4.4289331 -4.4289322 -4.428937 -4.42894 -4.4289403 -4.4289412 -4.4289422 -4.4289451 -4.4289527 -4.4289632 -4.4289713 -4.4289718 -4.428968 -4.4289665]]...]
INFO - root - 2017-12-10 04:11:13.693356: step 5310, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:35m:37s remains)
INFO - root - 2017-12-10 04:11:15.838332: step 5320, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.210 sec/batch; 19h:06m:34s remains)
INFO - root - 2017-12-10 04:11:17.954125: step 5330, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 19h:15m:24s remains)
INFO - root - 2017-12-10 04:11:20.088851: step 5340, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:24m:44s remains)
INFO - root - 2017-12-10 04:11:22.256425: step 5350, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:22m:21s remains)
INFO - root - 2017-12-10 04:11:24.404034: step 5360, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 19h:04m:26s remains)
INFO - root - 2017-12-10 04:11:26.522482: step 5370, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 19h:04m:40s remains)
INFO - root - 2017-12-10 04:11:28.662150: step 5380, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 19h:22m:01s remains)
INFO - root - 2017-12-10 04:11:30.771711: step 5390, loss = 2.28, batch loss = 2.23 (38.9 examples/sec; 0.206 sec/batch; 18h:41m:30s remains)
INFO - root - 2017-12-10 04:11:32.908616: step 5400, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 19h:05m:20s remains)
2017-12-10 04:11:33.229285: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289694 -4.428968 -4.4289689 -4.4289632 -4.4289575 -4.4289484 -4.4289432 -4.4289365 -4.4289241 -4.4289088 -4.4288855 -4.428854 -4.4288316 -4.42883 -4.4288421][-4.4289732 -4.428977 -4.4289804 -4.4289746 -4.428966 -4.4289536 -4.4289474 -4.4289408 -4.4289322 -4.4289184 -4.42889 -4.4288459 -4.428813 -4.4288011 -4.4288044][-4.4289565 -4.4289589 -4.4289618 -4.4289546 -4.428936 -4.4289165 -4.4289069 -4.4289069 -4.4289112 -4.4289079 -4.4288864 -4.4288406 -4.4287996 -4.4287744 -4.4287672][-4.4289379 -4.4289341 -4.4289308 -4.4289184 -4.4288874 -4.4288621 -4.428844 -4.4288383 -4.4288521 -4.4288654 -4.42886 -4.4288254 -4.4287806 -4.4287457 -4.4287343][-4.428916 -4.428905 -4.428894 -4.4288697 -4.42882 -4.4287734 -4.42873 -4.4287004 -4.4287195 -4.4287643 -4.4287925 -4.4287877 -4.4287586 -4.4287314 -4.4287281][-4.4289007 -4.4288874 -4.4288697 -4.4288306 -4.4287553 -4.4286685 -4.4285831 -4.4285073 -4.4285183 -4.428596 -4.4286728 -4.4287186 -4.428731 -4.4287338 -4.4287496][-4.428885 -4.4288793 -4.4288568 -4.4287972 -4.4286938 -4.4285607 -4.4284196 -4.4282846 -4.4282637 -4.4283705 -4.4285016 -4.4286084 -4.4286838 -4.4287338 -4.428781][-4.4288845 -4.4288855 -4.4288678 -4.4288015 -4.4286847 -4.4285245 -4.4283471 -4.4281764 -4.4281173 -4.4282165 -4.4283757 -4.4285188 -4.4286375 -4.4287262 -4.4287925][-4.4289036 -4.4289074 -4.4289045 -4.4288588 -4.4287753 -4.4286542 -4.4285159 -4.4283795 -4.4283 -4.4283342 -4.4284325 -4.4285321 -4.4286261 -4.4287105 -4.4287672][-4.4289227 -4.4289351 -4.4289474 -4.4289265 -4.428885 -4.42882 -4.4287391 -4.4286466 -4.4285626 -4.4285321 -4.42854 -4.4285626 -4.4286003 -4.4286509 -4.4286928][-4.4289179 -4.4289346 -4.4289527 -4.42894 -4.4289179 -4.42889 -4.4288559 -4.4287968 -4.4287262 -4.428668 -4.4286184 -4.4285851 -4.4285803 -4.428596 -4.4286203][-4.4289045 -4.4289145 -4.4289255 -4.428915 -4.4288979 -4.4288874 -4.4288831 -4.4288583 -4.4288163 -4.4287715 -4.4287109 -4.4286528 -4.42862 -4.4286141 -4.4286232][-4.4288945 -4.4288945 -4.4288936 -4.4288759 -4.4288592 -4.4288654 -4.428884 -4.428884 -4.428864 -4.4288387 -4.428791 -4.4287295 -4.4286842 -4.4286666 -4.4286695][-4.4289017 -4.428894 -4.4288864 -4.4288669 -4.4288464 -4.428854 -4.4288826 -4.428894 -4.4288893 -4.4288807 -4.4288521 -4.4288077 -4.4287667 -4.4287405 -4.4287343][-4.4288974 -4.4288836 -4.4288731 -4.4288588 -4.4288392 -4.4288421 -4.42887 -4.4288826 -4.4288807 -4.4288797 -4.42887 -4.4288483 -4.4288239 -4.4287996 -4.4287906]]...]
INFO - root - 2017-12-10 04:11:35.333419: step 5410, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:42m:33s remains)
INFO - root - 2017-12-10 04:11:37.496720: step 5420, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:25m:55s remains)
INFO - root - 2017-12-10 04:11:39.662638: step 5430, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:41m:04s remains)
INFO - root - 2017-12-10 04:11:41.773616: step 5440, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:16m:27s remains)
INFO - root - 2017-12-10 04:11:43.914057: step 5450, loss = 2.28, batch loss = 2.23 (38.6 examples/sec; 0.207 sec/batch; 18h:49m:20s remains)
INFO - root - 2017-12-10 04:11:46.046904: step 5460, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:59m:08s remains)
INFO - root - 2017-12-10 04:11:48.175066: step 5470, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 19h:14m:10s remains)
INFO - root - 2017-12-10 04:11:50.285019: step 5480, loss = 2.28, batch loss = 2.23 (38.5 examples/sec; 0.208 sec/batch; 18h:53m:12s remains)
INFO - root - 2017-12-10 04:11:52.407893: step 5490, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:15m:04s remains)
INFO - root - 2017-12-10 04:11:54.543995: step 5500, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:32m:09s remains)
2017-12-10 04:11:54.873098: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289732 -4.4289327 -4.4288936 -4.42887 -4.4288669 -4.4288607 -4.4288654 -4.4288869 -4.4288993 -4.4289107 -4.4289179 -4.4289217 -4.4289289 -4.4289389 -4.4289508][-4.428926 -4.428865 -4.4288158 -4.4287915 -4.4287882 -4.4287739 -4.4287782 -4.4288125 -4.4288325 -4.4288435 -4.4288545 -4.4288659 -4.428885 -4.4289064 -4.4289279][-4.4288883 -4.4288049 -4.4287415 -4.428709 -4.4286938 -4.4286547 -4.4286461 -4.4286962 -4.4287338 -4.4287457 -4.4287615 -4.4287853 -4.4288211 -4.4288621 -4.4288993][-4.4288692 -4.4287634 -4.4286814 -4.4286308 -4.4285946 -4.4285278 -4.4284968 -4.4285526 -4.4286056 -4.4286151 -4.4286375 -4.428679 -4.4287405 -4.42881 -4.4288697][-4.4288573 -4.4287343 -4.428638 -4.4285631 -4.4284997 -4.4283867 -4.4283061 -4.4283514 -4.42843 -4.4284549 -4.4284935 -4.4285626 -4.4286566 -4.4287591 -4.428843][-4.4288316 -4.4286909 -4.4285712 -4.4284577 -4.4283619 -4.4282022 -4.4280562 -4.4280915 -4.4282217 -4.4282928 -4.4283686 -4.42847 -4.4285917 -4.4287152 -4.4288177][-4.4288154 -4.4286575 -4.4285135 -4.428369 -4.42823 -4.4279947 -4.4277325 -4.4277234 -4.427918 -4.4280853 -4.4282293 -4.4283729 -4.4285188 -4.4286675 -4.428792][-4.4288096 -4.4286518 -4.4285 -4.4283385 -4.428154 -4.4278316 -4.4274416 -4.4273763 -4.4276428 -4.4279265 -4.4281406 -4.4283147 -4.4284849 -4.4286494 -4.4287853][-4.4288187 -4.4286823 -4.4285526 -4.428411 -4.428236 -4.4279108 -4.4275203 -4.4274592 -4.4277115 -4.4279861 -4.428194 -4.4283552 -4.4285192 -4.428678 -4.4288082][-4.4288344 -4.4287257 -4.4286275 -4.428525 -4.4284005 -4.4281459 -4.4278588 -4.4278426 -4.4280248 -4.4282069 -4.4283514 -4.4284716 -4.4286027 -4.4287338 -4.4288449][-4.4288406 -4.4287567 -4.4286966 -4.4286375 -4.4285636 -4.4283943 -4.4282169 -4.4282355 -4.4283619 -4.4284749 -4.42856 -4.4286289 -4.4287176 -4.428812 -4.4288926][-4.4288559 -4.4287996 -4.4287663 -4.4287267 -4.4286776 -4.4285784 -4.428493 -4.4285374 -4.42863 -4.4287024 -4.4287515 -4.4287896 -4.4288368 -4.4288893 -4.4289374][-4.4288716 -4.4288311 -4.4288082 -4.4287772 -4.4287462 -4.4287014 -4.4286776 -4.4287314 -4.4287996 -4.4288478 -4.4288745 -4.4288931 -4.4289126 -4.4289331 -4.428956][-4.4288988 -4.4288664 -4.42885 -4.4288239 -4.4287992 -4.4287834 -4.4287972 -4.4288478 -4.4288936 -4.428926 -4.4289408 -4.4289489 -4.4289551 -4.4289565 -4.4289551][-4.4289432 -4.4289203 -4.4289107 -4.4288907 -4.4288774 -4.4288807 -4.428905 -4.4289384 -4.4289627 -4.428977 -4.4289808 -4.4289827 -4.42898 -4.4289651 -4.4289389]]...]
INFO - root - 2017-12-10 04:11:56.972401: step 5510, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 19h:03m:32s remains)
INFO - root - 2017-12-10 04:11:59.123690: step 5520, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:15m:57s remains)
INFO - root - 2017-12-10 04:12:01.264069: step 5530, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 19h:30m:15s remains)
INFO - root - 2017-12-10 04:12:03.418198: step 5540, loss = 2.28, batch loss = 2.23 (38.7 examples/sec; 0.207 sec/batch; 18h:46m:16s remains)
INFO - root - 2017-12-10 04:12:05.521449: step 5550, loss = 2.28, batch loss = 2.23 (38.9 examples/sec; 0.206 sec/batch; 18h:40m:11s remains)
INFO - root - 2017-12-10 04:12:07.655739: step 5560, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:19m:27s remains)
INFO - root - 2017-12-10 04:12:09.737857: step 5570, loss = 2.28, batch loss = 2.23 (39.2 examples/sec; 0.204 sec/batch; 18h:31m:23s remains)
INFO - root - 2017-12-10 04:12:11.882354: step 5580, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 20h:08m:42s remains)
INFO - root - 2017-12-10 04:12:14.035317: step 5590, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.211 sec/batch; 19h:12m:08s remains)
INFO - root - 2017-12-10 04:12:16.215504: step 5600, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 19h:02m:34s remains)
2017-12-10 04:12:16.530328: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287572 -4.42871 -4.4286957 -4.4287066 -4.4287333 -4.428761 -4.4287748 -4.4287705 -4.4287271 -4.4286771 -4.4286451 -4.4286489 -4.4286718 -4.4286809 -4.4286866][-4.4287138 -4.4286652 -4.4286518 -4.428668 -4.4287 -4.428731 -4.4287562 -4.42876 -4.428721 -4.4286766 -4.4286427 -4.4286337 -4.4286542 -4.4286771 -4.4286985][-4.4287095 -4.4286671 -4.4286566 -4.4286737 -4.4287024 -4.4287329 -4.4287634 -4.42877 -4.4287381 -4.4287043 -4.428668 -4.428637 -4.4286423 -4.428669 -4.4286947][-4.4286985 -4.4286532 -4.4286356 -4.4286475 -4.4286785 -4.4287143 -4.4287462 -4.4287553 -4.4287424 -4.4287276 -4.4286981 -4.4286585 -4.4286451 -4.4286623 -4.4286795][-4.4286618 -4.4286013 -4.4285755 -4.4285884 -4.4286222 -4.4286571 -4.4286804 -4.4286823 -4.4286737 -4.4286656 -4.428637 -4.428597 -4.4285774 -4.4285913 -4.4286079][-4.4286232 -4.4285436 -4.4285107 -4.4285145 -4.4285378 -4.4285583 -4.4285612 -4.4285455 -4.4285326 -4.4285369 -4.4285245 -4.4284968 -4.4284787 -4.4284868 -4.4285035][-4.4285965 -4.428514 -4.4284754 -4.428462 -4.4284606 -4.4284487 -4.4284163 -4.4283838 -4.4283719 -4.42839 -4.42841 -4.4284191 -4.4284186 -4.4284239 -4.4284439][-4.4286232 -4.4285569 -4.4285164 -4.42848 -4.4284449 -4.4283867 -4.4283147 -4.4282689 -4.4282703 -4.4283237 -4.4283795 -4.4284186 -4.428432 -4.4284358 -4.4284625][-4.428699 -4.4286451 -4.428597 -4.4285426 -4.4284916 -4.4284234 -4.42835 -4.4283118 -4.428329 -4.428391 -4.4284482 -4.4284878 -4.4285073 -4.4285216 -4.4285593][-4.428782 -4.4287434 -4.4287024 -4.4286571 -4.4286232 -4.4285803 -4.4285321 -4.4285045 -4.4285111 -4.4285455 -4.4285817 -4.4286094 -4.428628 -4.4286494 -4.42869][-4.428833 -4.428812 -4.4287896 -4.4287624 -4.4287405 -4.4287167 -4.4286895 -4.4286728 -4.4286742 -4.4286866 -4.4287043 -4.4287257 -4.4287505 -4.4287772 -4.4288092][-4.4288659 -4.4288578 -4.4288526 -4.4288416 -4.4288316 -4.4288187 -4.428802 -4.42879 -4.4287858 -4.4287853 -4.4287939 -4.4288111 -4.4288368 -4.428864 -4.4288855][-4.4289055 -4.4289036 -4.4289041 -4.4289002 -4.4288988 -4.4289 -4.4288969 -4.4288912 -4.428884 -4.4288759 -4.4288745 -4.4288816 -4.428896 -4.4289112 -4.4289217][-4.4289432 -4.4289436 -4.4289436 -4.4289427 -4.4289446 -4.4289513 -4.4289579 -4.42896 -4.4289541 -4.4289446 -4.4289384 -4.4289365 -4.4289379 -4.4289412 -4.4289451][-4.4289765 -4.4289761 -4.4289756 -4.4289746 -4.4289765 -4.4289813 -4.428987 -4.42899 -4.4289894 -4.4289851 -4.4289804 -4.4289761 -4.4289737 -4.4289737 -4.4289742]]...]
INFO - root - 2017-12-10 04:12:18.641559: step 5610, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:18m:33s remains)
INFO - root - 2017-12-10 04:12:20.750491: step 5620, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 19h:13m:04s remains)
INFO - root - 2017-12-10 04:12:22.861816: step 5630, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 19h:22m:26s remains)
INFO - root - 2017-12-10 04:12:24.992007: step 5640, loss = 2.28, batch loss = 2.23 (38.2 examples/sec; 0.210 sec/batch; 19h:01m:39s remains)
INFO - root - 2017-12-10 04:12:27.126460: step 5650, loss = 2.28, batch loss = 2.23 (38.7 examples/sec; 0.207 sec/batch; 18h:45m:04s remains)
INFO - root - 2017-12-10 04:12:29.234212: step 5660, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 19h:05m:17s remains)
INFO - root - 2017-12-10 04:12:31.388321: step 5670, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 19h:21m:40s remains)
INFO - root - 2017-12-10 04:12:33.542305: step 5680, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:18m:12s remains)
INFO - root - 2017-12-10 04:12:35.740340: step 5690, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 21h:03m:51s remains)
INFO - root - 2017-12-10 04:12:37.895610: step 5700, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 19h:28m:16s remains)
2017-12-10 04:12:38.282942: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288635 -4.4288468 -4.4288311 -4.4288249 -4.428834 -4.428853 -4.4288721 -4.4288831 -4.4288859 -4.4288816 -4.4288783 -4.428885 -4.4289074 -4.4289327 -4.428946][-4.4288211 -4.4287958 -4.4287739 -4.428761 -4.4287591 -4.4287763 -4.4288034 -4.4288225 -4.4288292 -4.428822 -4.4288144 -4.428822 -4.4288483 -4.4288774 -4.4288888][-4.4287872 -4.4287639 -4.4287415 -4.4287229 -4.4287114 -4.4287267 -4.4287643 -4.4287934 -4.4288025 -4.4287906 -4.4287772 -4.428781 -4.4288049 -4.4288287 -4.4288316][-4.4287515 -4.4287367 -4.4287238 -4.4287062 -4.4286876 -4.4286914 -4.428731 -4.4287724 -4.4287906 -4.4287782 -4.4287562 -4.4287591 -4.4287815 -4.4287944 -4.4287806][-4.4287372 -4.4287362 -4.4287329 -4.4287148 -4.4286795 -4.4286494 -4.428669 -4.4287157 -4.42875 -4.4287562 -4.4287453 -4.4287586 -4.4287877 -4.4287896 -4.4287524][-4.4287615 -4.4287615 -4.4287505 -4.4287152 -4.4286466 -4.4285679 -4.4285431 -4.4285741 -4.4286342 -4.4286919 -4.4287233 -4.4287648 -4.4288158 -4.4288149 -4.4287505][-4.4288244 -4.428812 -4.4287796 -4.428709 -4.4285913 -4.4284558 -4.4283633 -4.4283552 -4.4284511 -4.4285855 -4.4286909 -4.4287753 -4.4288473 -4.4288464 -4.4287596][-4.4288836 -4.4288526 -4.4288006 -4.4286985 -4.4285374 -4.42835 -4.4281864 -4.4281454 -4.4282866 -4.4285011 -4.4286752 -4.428792 -4.4288683 -4.428863 -4.4287543][-4.4289188 -4.4288735 -4.4288087 -4.428689 -4.4285097 -4.4282961 -4.4281025 -4.428062 -4.4282465 -4.4285011 -4.4286909 -4.4288039 -4.4288673 -4.428854 -4.4287319][-4.4289441 -4.4288921 -4.4288192 -4.4286981 -4.4285336 -4.4283423 -4.4281888 -4.428195 -4.4283891 -4.428618 -4.4287624 -4.4288287 -4.4288611 -4.4288392 -4.4287229][-4.4289732 -4.4289231 -4.4288516 -4.4287505 -4.4286251 -4.428494 -4.4284205 -4.4284654 -4.4286189 -4.4287753 -4.4288568 -4.4288669 -4.4288554 -4.4288244 -4.4287305][-4.428987 -4.4289603 -4.4289117 -4.4288435 -4.4287686 -4.4287028 -4.4286909 -4.4287376 -4.428822 -4.4288969 -4.4289236 -4.4288974 -4.4288545 -4.4288158 -4.4287529][-4.4289784 -4.4289856 -4.428967 -4.4289279 -4.4288936 -4.42887 -4.4288812 -4.42891 -4.42894 -4.4289594 -4.4289517 -4.4289117 -4.4288621 -4.4288273 -4.4287958][-4.4289422 -4.4289813 -4.4289861 -4.4289675 -4.4289541 -4.4289508 -4.4289637 -4.4289732 -4.4289713 -4.428966 -4.4289474 -4.4289117 -4.4288774 -4.4288607 -4.4288554][-4.4288898 -4.4289479 -4.4289708 -4.4289646 -4.4289608 -4.428966 -4.4289765 -4.4289727 -4.4289589 -4.42895 -4.4289331 -4.4289088 -4.4288945 -4.4288974 -4.4289088]]...]
INFO - root - 2017-12-10 04:12:40.448251: step 5710, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:16m:28s remains)
INFO - root - 2017-12-10 04:12:42.594801: step 5720, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:19m:45s remains)
INFO - root - 2017-12-10 04:12:44.732237: step 5730, loss = 2.28, batch loss = 2.23 (38.5 examples/sec; 0.208 sec/batch; 18h:51m:59s remains)
INFO - root - 2017-12-10 04:12:46.838324: step 5740, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:24m:27s remains)
INFO - root - 2017-12-10 04:12:48.975553: step 5750, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:30m:38s remains)
INFO - root - 2017-12-10 04:12:51.079373: step 5760, loss = 2.28, batch loss = 2.23 (38.6 examples/sec; 0.207 sec/batch; 18h:47m:45s remains)
INFO - root - 2017-12-10 04:12:53.184867: step 5770, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:43m:54s remains)
INFO - root - 2017-12-10 04:12:55.311907: step 5780, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:20m:05s remains)
INFO - root - 2017-12-10 04:12:57.435930: step 5790, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:27m:43s remains)
INFO - root - 2017-12-10 04:12:59.552231: step 5800, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.211 sec/batch; 19h:07m:44s remains)
2017-12-10 04:12:59.883071: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288011 -4.4288125 -4.4288216 -4.4288015 -4.4288063 -4.4288149 -4.42881 -4.4288158 -4.4288211 -4.4288192 -4.4288106 -4.4288182 -4.4288387 -4.428844 -4.4288235][-4.4288144 -4.428803 -4.4287992 -4.4287877 -4.4288058 -4.4288225 -4.4288249 -4.4288406 -4.4288468 -4.4288368 -4.4288197 -4.4288273 -4.4288397 -4.4288383 -4.4288268][-4.4287972 -4.4287667 -4.4287581 -4.4287653 -4.4287963 -4.4288187 -4.4288278 -4.42885 -4.4288554 -4.4288316 -4.4287963 -4.4287896 -4.42879 -4.4287815 -4.4287887][-4.42876 -4.428721 -4.4287157 -4.4287438 -4.4287863 -4.4288096 -4.4288192 -4.4288311 -4.4288397 -4.4288163 -4.4287715 -4.4287515 -4.4287405 -4.4287319 -4.4287539][-4.4287348 -4.4286962 -4.4286957 -4.4287319 -4.4287729 -4.4287839 -4.4287753 -4.4287648 -4.4287829 -4.4287863 -4.4287658 -4.4287548 -4.4287462 -4.4287362 -4.4287558][-4.4287362 -4.4287024 -4.4287 -4.4287229 -4.4287376 -4.4287114 -4.4286556 -4.4286036 -4.4286389 -4.4287028 -4.4287381 -4.4287572 -4.4287634 -4.4287586 -4.42877][-4.4287467 -4.4287057 -4.4286895 -4.4286928 -4.4286752 -4.428597 -4.4284616 -4.4283438 -4.4284096 -4.4285607 -4.4286618 -4.4287114 -4.4287415 -4.42876 -4.4287777][-4.4287534 -4.4287028 -4.4286733 -4.4286642 -4.4286308 -4.4285235 -4.428328 -4.428144 -4.4282269 -4.4284463 -4.4285874 -4.4286513 -4.4287 -4.4287438 -4.4287672][-4.4287548 -4.4287028 -4.4286761 -4.428678 -4.4286642 -4.42859 -4.4284353 -4.4282851 -4.4283361 -4.4285021 -4.4286032 -4.4286418 -4.4286871 -4.4287424 -4.4287744][-4.4287581 -4.42871 -4.4286962 -4.4287167 -4.4287333 -4.4287167 -4.4286437 -4.4285588 -4.4285784 -4.4286585 -4.4286942 -4.4286971 -4.4287305 -4.4287815 -4.4288111][-4.42878 -4.4287362 -4.42873 -4.4287548 -4.42878 -4.4287963 -4.4287796 -4.4287453 -4.4287529 -4.428782 -4.4287839 -4.4287772 -4.4288106 -4.4288568 -4.4288797][-4.4288306 -4.4287987 -4.4287944 -4.4288082 -4.4288225 -4.4288397 -4.4288421 -4.4288359 -4.4288459 -4.4288573 -4.4288526 -4.4288554 -4.4288912 -4.428926 -4.4289389][-4.4289041 -4.4288869 -4.4288831 -4.428884 -4.4288836 -4.4288888 -4.4288893 -4.4288931 -4.4289041 -4.4289093 -4.4289079 -4.4289179 -4.4289484 -4.428968 -4.4289689][-4.4289684 -4.4289565 -4.4289513 -4.4289489 -4.4289432 -4.4289417 -4.4289441 -4.4289551 -4.4289656 -4.4289675 -4.4289651 -4.42897 -4.4289875 -4.4289966 -4.4289927][-4.4289989 -4.4289913 -4.4289856 -4.4289846 -4.4289832 -4.4289842 -4.4289894 -4.4290018 -4.4290104 -4.4290071 -4.4290023 -4.4290042 -4.4290137 -4.4290137 -4.4290075]]...]
INFO - root - 2017-12-10 04:13:02.036076: step 5810, loss = 2.28, batch loss = 2.23 (38.4 examples/sec; 0.208 sec/batch; 18h:53m:28s remains)
INFO - root - 2017-12-10 04:13:04.156802: step 5820, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:15m:11s remains)
INFO - root - 2017-12-10 04:13:06.279373: step 5830, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:14m:27s remains)
INFO - root - 2017-12-10 04:13:08.437705: step 5840, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:49m:37s remains)
INFO - root - 2017-12-10 04:13:10.625404: step 5850, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:41m:05s remains)
INFO - root - 2017-12-10 04:13:12.766394: step 5860, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:55m:59s remains)
INFO - root - 2017-12-10 04:13:14.909670: step 5870, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:10m:04s remains)
INFO - root - 2017-12-10 04:13:17.046988: step 5880, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 19h:26m:56s remains)
INFO - root - 2017-12-10 04:13:19.170856: step 5890, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 19h:20m:56s remains)
INFO - root - 2017-12-10 04:13:21.310554: step 5900, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.214 sec/batch; 19h:22m:37s remains)
2017-12-10 04:13:21.679998: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287329 -4.4287086 -4.4287086 -4.4287095 -4.4287219 -4.4287372 -4.4287505 -4.4287629 -4.4287763 -4.4287863 -4.4287934 -4.4288049 -4.4288187 -4.4288311 -4.4288397][-4.4287124 -4.4286823 -4.4286776 -4.4286723 -4.4286819 -4.4286914 -4.4287019 -4.4287186 -4.4287438 -4.4287663 -4.4287815 -4.4287996 -4.4288163 -4.4288292 -4.428834][-4.4287162 -4.4286757 -4.4286637 -4.4286561 -4.4286633 -4.4286652 -4.42867 -4.4286871 -4.4287219 -4.4287558 -4.428782 -4.4288073 -4.4288263 -4.4288335 -4.4288282][-4.4287319 -4.4286819 -4.4286671 -4.4286637 -4.4286704 -4.4286613 -4.4286513 -4.428659 -4.4286938 -4.4287362 -4.4287744 -4.4288135 -4.4288406 -4.4288468 -4.4288325][-4.4287262 -4.428669 -4.4286628 -4.4286771 -4.4286895 -4.4286714 -4.4286437 -4.4286366 -4.4286647 -4.4287114 -4.428761 -4.4288135 -4.4288497 -4.4288568 -4.4288335][-4.4286962 -4.42863 -4.42864 -4.4286795 -4.4287024 -4.4286804 -4.4286423 -4.4286251 -4.428647 -4.4286976 -4.4287543 -4.4288139 -4.428853 -4.4288568 -4.428823][-4.4286642 -4.4285936 -4.4286232 -4.4286866 -4.428721 -4.4286962 -4.4286523 -4.4286323 -4.4286532 -4.4287043 -4.4287653 -4.4288273 -4.4288688 -4.4288707 -4.4288306][-4.4286437 -4.4285774 -4.4286242 -4.4287004 -4.4287381 -4.4287109 -4.4286633 -4.4286408 -4.4286594 -4.4287114 -4.4287782 -4.4288454 -4.4288898 -4.4288921 -4.428853][-4.4286327 -4.4285736 -4.4286327 -4.4287119 -4.4287453 -4.4287143 -4.4286661 -4.4286456 -4.4286652 -4.4287233 -4.4287982 -4.4288707 -4.4289165 -4.4289207 -4.428884][-4.4286408 -4.42859 -4.4286518 -4.4287276 -4.4287577 -4.4287276 -4.4286895 -4.4286823 -4.4287119 -4.4287734 -4.4288445 -4.4289088 -4.4289474 -4.4289494 -4.4289165][-4.4286871 -4.4286442 -4.428699 -4.4287643 -4.42879 -4.4287615 -4.4287324 -4.4287381 -4.4287772 -4.4288392 -4.4288993 -4.4289484 -4.4289746 -4.4289718 -4.4289412][-4.4287386 -4.4287024 -4.4287438 -4.4287963 -4.4288173 -4.428792 -4.4287696 -4.4287777 -4.4288139 -4.4288688 -4.4289174 -4.4289536 -4.4289722 -4.4289675 -4.4289432][-4.4287829 -4.4287505 -4.4287767 -4.4288163 -4.4288349 -4.4288192 -4.42881 -4.4288206 -4.4288468 -4.4288878 -4.4289222 -4.4289451 -4.4289556 -4.4289522 -4.4289365][-4.42883 -4.428802 -4.4288158 -4.428844 -4.4288597 -4.4288545 -4.4288564 -4.4288683 -4.4288869 -4.4289145 -4.4289331 -4.4289441 -4.428947 -4.4289446 -4.4289365][-4.4288788 -4.4288564 -4.4288607 -4.4288778 -4.4288912 -4.4288917 -4.4288974 -4.4289064 -4.4289169 -4.4289331 -4.4289441 -4.4289494 -4.4289484 -4.4289455 -4.4289417]]...]
INFO - root - 2017-12-10 04:13:23.757972: step 5910, loss = 2.28, batch loss = 2.23 (38.4 examples/sec; 0.208 sec/batch; 18h:53m:03s remains)
INFO - root - 2017-12-10 04:13:25.871362: step 5920, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:39m:40s remains)
INFO - root - 2017-12-10 04:13:28.003035: step 5930, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 19h:12m:42s remains)
INFO - root - 2017-12-10 04:13:30.136052: step 5940, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:18m:45s remains)
INFO - root - 2017-12-10 04:13:32.267725: step 5950, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 19h:03m:19s remains)
INFO - root - 2017-12-10 04:13:34.395606: step 5960, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:10m:01s remains)
INFO - root - 2017-12-10 04:13:36.506164: step 5970, loss = 2.28, batch loss = 2.23 (38.2 examples/sec; 0.210 sec/batch; 19h:01m:08s remains)
INFO - root - 2017-12-10 04:13:38.655994: step 5980, loss = 2.28, batch loss = 2.23 (38.3 examples/sec; 0.209 sec/batch; 18h:57m:34s remains)
INFO - root - 2017-12-10 04:13:40.775269: step 5990, loss = 2.28, batch loss = 2.23 (38.7 examples/sec; 0.207 sec/batch; 18h:45m:13s remains)
INFO - root - 2017-12-10 04:13:42.890446: step 6000, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 19h:28m:34s remains)
2017-12-10 04:13:43.263102: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290333 -4.429029 -4.4290223 -4.4290161 -4.4290137 -4.4290109 -4.4290018 -4.428987 -4.4289775 -4.4289775 -4.4289856 -4.4289966 -4.429008 -4.429018 -4.4290237][-4.4290314 -4.4290266 -4.4290195 -4.4290137 -4.4290113 -4.4290028 -4.4289813 -4.4289556 -4.4289432 -4.428946 -4.4289589 -4.4289746 -4.4289918 -4.4290085 -4.42902][-4.4290323 -4.4290276 -4.4290204 -4.4290113 -4.4290023 -4.4289813 -4.4289412 -4.4289002 -4.42888 -4.4288845 -4.4289064 -4.428937 -4.42897 -4.428997 -4.4290161][-4.42903 -4.4290233 -4.4290118 -4.4289937 -4.4289722 -4.4289341 -4.4288697 -4.4288068 -4.4287772 -4.4287858 -4.4288273 -4.4288855 -4.4289455 -4.428988 -4.4290137][-4.429028 -4.42902 -4.4290013 -4.4289694 -4.4289327 -4.4288707 -4.4287715 -4.4286823 -4.4286504 -4.428678 -4.428751 -4.4288449 -4.4289322 -4.4289875 -4.4290156][-4.4290295 -4.4290171 -4.4289918 -4.4289474 -4.4288836 -4.4287782 -4.4286332 -4.4285226 -4.4285073 -4.4285793 -4.4287004 -4.4288297 -4.4289355 -4.4289961 -4.42902][-4.429019 -4.4290056 -4.4289751 -4.4289107 -4.4288135 -4.4286618 -4.4284716 -4.4283519 -4.4283805 -4.4285169 -4.4286914 -4.4288445 -4.4289551 -4.42901 -4.4290257][-4.4290009 -4.4289842 -4.4289403 -4.4288497 -4.428721 -4.4285259 -4.4282932 -4.4281912 -4.4282904 -4.4284921 -4.4287057 -4.4288645 -4.4289727 -4.42902 -4.4290285][-4.428968 -4.4289446 -4.4288826 -4.4287705 -4.4286208 -4.4283924 -4.4281383 -4.4280858 -4.4282627 -4.4285126 -4.4287357 -4.428884 -4.4289827 -4.4290233 -4.429028][-4.4289169 -4.4289 -4.4288306 -4.4287086 -4.42855 -4.4283214 -4.4280906 -4.4280949 -4.4283204 -4.4285803 -4.4287829 -4.4289083 -4.4289889 -4.4290214 -4.4290242][-4.4288635 -4.4288712 -4.428813 -4.4286952 -4.4285445 -4.4283667 -4.4282084 -4.4282494 -4.4284649 -4.4286914 -4.4288588 -4.4289546 -4.4290075 -4.4290252 -4.4290214][-4.4288235 -4.428863 -4.428833 -4.4287443 -4.4286327 -4.4285278 -4.4284539 -4.4285097 -4.4286675 -4.4288239 -4.4289374 -4.429 -4.4290276 -4.42903 -4.42902][-4.4288096 -4.4288769 -4.4288807 -4.4288392 -4.4287777 -4.4287248 -4.4286962 -4.42874 -4.4288349 -4.4289231 -4.4289856 -4.4290223 -4.4290371 -4.4290342 -4.4290228][-4.4288225 -4.4289083 -4.4289374 -4.4289312 -4.4288945 -4.4288607 -4.4288445 -4.4288688 -4.4289184 -4.4289641 -4.4289966 -4.429019 -4.4290328 -4.4290338 -4.4290261][-4.4288497 -4.4289451 -4.4289856 -4.4289889 -4.4289551 -4.4289188 -4.4288988 -4.4289069 -4.4289289 -4.4289532 -4.4289746 -4.4289966 -4.4290161 -4.4290252 -4.4290266]]...]
INFO - root - 2017-12-10 04:13:45.376982: step 6010, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:07m:07s remains)
INFO - root - 2017-12-10 04:13:47.523922: step 6020, loss = 2.28, batch loss = 2.23 (38.2 examples/sec; 0.209 sec/batch; 18h:59m:36s remains)
INFO - root - 2017-12-10 04:13:49.676699: step 6030, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:34m:04s remains)
INFO - root - 2017-12-10 04:13:51.835664: step 6040, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:11m:11s remains)
INFO - root - 2017-12-10 04:13:53.956832: step 6050, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 19h:11m:07s remains)
INFO - root - 2017-12-10 04:13:56.146183: step 6060, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 19h:19m:57s remains)
INFO - root - 2017-12-10 04:13:58.272716: step 6070, loss = 2.28, batch loss = 2.23 (38.2 examples/sec; 0.210 sec/batch; 18h:59m:51s remains)
INFO - root - 2017-12-10 04:14:00.405279: step 6080, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:34m:43s remains)
INFO - root - 2017-12-10 04:14:02.498084: step 6090, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 19h:25m:37s remains)
INFO - root - 2017-12-10 04:14:04.663135: step 6100, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:30m:00s remains)
2017-12-10 04:14:05.022849: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288797 -4.428875 -4.4288754 -4.4288774 -4.4288797 -4.4288831 -4.4288926 -4.428905 -4.4289179 -4.42893 -4.4289384 -4.4289384 -4.4289317 -4.4289236 -4.42893][-4.4289031 -4.4288969 -4.4288983 -4.4289031 -4.4289103 -4.4289184 -4.4289274 -4.4289384 -4.4289474 -4.4289541 -4.4289603 -4.4289622 -4.4289594 -4.4289508 -4.4289503][-4.4289346 -4.4289255 -4.4289231 -4.4289246 -4.4289279 -4.4289322 -4.428937 -4.4289436 -4.42895 -4.4289546 -4.4289613 -4.4289665 -4.4289641 -4.428957 -4.4289565][-4.4289303 -4.4289184 -4.4289079 -4.4289002 -4.428895 -4.4288926 -4.4288926 -4.4288964 -4.4289031 -4.4289103 -4.4289217 -4.4289317 -4.4289355 -4.4289351 -4.4289403][-4.4288769 -4.4288588 -4.4288344 -4.4288135 -4.4287972 -4.428791 -4.42879 -4.4287925 -4.4288 -4.4288111 -4.4288278 -4.4288468 -4.4288597 -4.4288707 -4.4288869][-4.4288044 -4.4287863 -4.4287524 -4.4287152 -4.4286842 -4.428669 -4.4286656 -4.4286675 -4.4286819 -4.4287009 -4.4287276 -4.4287539 -4.4287705 -4.4287872 -4.4288135][-4.4287524 -4.4287415 -4.4287071 -4.428659 -4.4286089 -4.4285746 -4.4285531 -4.4285479 -4.4285703 -4.428607 -4.4286513 -4.4286861 -4.4287 -4.4287119 -4.4287376][-4.4287443 -4.4287472 -4.4287238 -4.4286785 -4.4286194 -4.4285669 -4.4285259 -4.4285126 -4.428535 -4.4285784 -4.4286275 -4.4286609 -4.4286661 -4.4286637 -4.4286776][-4.4287858 -4.4287992 -4.4287896 -4.428761 -4.4287119 -4.4286623 -4.4286232 -4.4286122 -4.4286308 -4.4286633 -4.4286976 -4.4287138 -4.4287019 -4.4286804 -4.4286752][-4.4288378 -4.4288545 -4.4288578 -4.4288497 -4.4288235 -4.4287944 -4.4287724 -4.4287705 -4.4287877 -4.4288092 -4.4288268 -4.4288259 -4.4287992 -4.4287629 -4.4287419][-4.4288831 -4.4288921 -4.4288979 -4.4289017 -4.4288955 -4.4288869 -4.4288821 -4.4288845 -4.4288969 -4.4289122 -4.4289212 -4.4289169 -4.4288931 -4.4288592 -4.4288344][-4.4289217 -4.4289246 -4.4289265 -4.4289308 -4.4289284 -4.4289255 -4.4289265 -4.428926 -4.4289312 -4.4289393 -4.4289417 -4.4289379 -4.4289279 -4.4289093 -4.4288926][-4.428946 -4.428947 -4.4289441 -4.4289432 -4.4289341 -4.4289227 -4.4289145 -4.4289055 -4.4289045 -4.4289103 -4.4289107 -4.4289079 -4.4289069 -4.4289041 -4.4289012][-4.4289403 -4.4289427 -4.4289412 -4.4289374 -4.4289246 -4.428906 -4.4288898 -4.428874 -4.4288664 -4.4288659 -4.4288635 -4.4288607 -4.428864 -4.4288726 -4.4288859][-4.4289246 -4.4289279 -4.4289303 -4.4289284 -4.42892 -4.4289036 -4.4288888 -4.428874 -4.428864 -4.4288568 -4.42885 -4.4288449 -4.4288483 -4.4288616 -4.4288836]]...]
INFO - root - 2017-12-10 04:14:07.187048: step 6110, loss = 2.28, batch loss = 2.23 (38.3 examples/sec; 0.209 sec/batch; 18h:56m:45s remains)
INFO - root - 2017-12-10 04:14:09.307641: step 6120, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:39m:12s remains)
INFO - root - 2017-12-10 04:14:11.454330: step 6130, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:29m:14s remains)
INFO - root - 2017-12-10 04:14:13.590672: step 6140, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 19h:27m:40s remains)
INFO - root - 2017-12-10 04:14:15.733152: step 6150, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 19h:43m:39s remains)
INFO - root - 2017-12-10 04:14:17.849251: step 6160, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:08m:19s remains)
INFO - root - 2017-12-10 04:14:20.012602: step 6170, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:55m:48s remains)
INFO - root - 2017-12-10 04:14:22.151693: step 6180, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:29m:41s remains)
INFO - root - 2017-12-10 04:14:24.279264: step 6190, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:47m:31s remains)
INFO - root - 2017-12-10 04:14:26.412282: step 6200, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.211 sec/batch; 19h:09m:41s remains)
2017-12-10 04:14:26.811203: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288182 -4.428793 -4.4288111 -4.4288425 -4.4288521 -4.4288363 -4.4287767 -4.4286766 -4.428587 -4.4285865 -4.4286666 -4.4287419 -4.4287829 -4.4288173 -4.4288783][-4.428834 -4.4288254 -4.4288492 -4.428874 -4.4288769 -4.4288516 -4.4287848 -4.4286838 -4.4286051 -4.4286103 -4.4286942 -4.4287753 -4.4288239 -4.4288664 -4.4289236][-4.4288712 -4.4288793 -4.4289017 -4.4289117 -4.4288983 -4.4288611 -4.4287834 -4.4286804 -4.4286132 -4.42862 -4.4287057 -4.4287958 -4.4288516 -4.4289031 -4.4289522][-4.428915 -4.4289365 -4.4289556 -4.4289494 -4.4289131 -4.4288516 -4.4287519 -4.428637 -4.4285817 -4.428607 -4.4287095 -4.4288154 -4.4288836 -4.42894 -4.4289718][-4.4289455 -4.4289732 -4.42899 -4.4289656 -4.4289012 -4.4288082 -4.4286809 -4.4285479 -4.4285097 -4.4285703 -4.428699 -4.4288292 -4.4289165 -4.4289694 -4.4289756][-4.4289641 -4.4289894 -4.4289956 -4.4289536 -4.4288597 -4.4287248 -4.4285645 -4.4284263 -4.4284296 -4.428544 -4.4287043 -4.4288511 -4.4289508 -4.4289908 -4.4289613][-4.4289622 -4.4289775 -4.4289651 -4.4289002 -4.4287715 -4.4285979 -4.4284215 -4.4283185 -4.4283972 -4.4285645 -4.4287429 -4.4288931 -4.4289851 -4.4290023 -4.4289403][-4.4289265 -4.4289284 -4.4289036 -4.4288235 -4.4286671 -4.4284811 -4.4283395 -4.4283214 -4.4284568 -4.4286308 -4.4288025 -4.4289408 -4.429009 -4.4289961 -4.4289055][-4.4288759 -4.428875 -4.4288464 -4.4287519 -4.4285855 -4.4284282 -4.4283681 -4.4284286 -4.4285707 -4.4287167 -4.4288607 -4.4289732 -4.4290085 -4.428966 -4.4288621][-4.4288368 -4.4288459 -4.4288125 -4.4287043 -4.4285455 -4.4284382 -4.4284563 -4.4285464 -4.4286695 -4.4287877 -4.4289055 -4.4289865 -4.42899 -4.4289284 -4.4288254][-4.428812 -4.4288263 -4.42878 -4.4286604 -4.42852 -4.4284663 -4.4285278 -4.4286246 -4.4287319 -4.4288344 -4.4289289 -4.4289823 -4.428967 -4.4288955 -4.4287934][-4.4287872 -4.4287925 -4.4287281 -4.4286108 -4.4285092 -4.4285054 -4.4285932 -4.4286909 -4.4287863 -4.4288769 -4.4289513 -4.4289823 -4.4289489 -4.428863 -4.4287605][-4.4287782 -4.4287705 -4.4286976 -4.4285994 -4.4285412 -4.4285622 -4.4286509 -4.4287472 -4.4288344 -4.4289165 -4.4289718 -4.4289804 -4.428926 -4.4288225 -4.4287143][-4.4287977 -4.4287753 -4.4286966 -4.4286242 -4.4286065 -4.4286442 -4.4287205 -4.4288006 -4.4288683 -4.4289289 -4.4289646 -4.4289608 -4.4288974 -4.4287882 -4.4286785][-4.4288254 -4.4287958 -4.4287286 -4.4286819 -4.4286838 -4.4287224 -4.4287815 -4.4288383 -4.4288816 -4.4289222 -4.4289451 -4.4289336 -4.4288669 -4.4287577 -4.4286518]]...]
INFO - root - 2017-12-10 04:14:28.953739: step 6210, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:14m:20s remains)
INFO - root - 2017-12-10 04:14:31.110579: step 6220, loss = 2.28, batch loss = 2.23 (38.3 examples/sec; 0.209 sec/batch; 18h:54m:34s remains)
INFO - root - 2017-12-10 04:14:33.232796: step 6230, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 20h:14m:27s remains)
INFO - root - 2017-12-10 04:14:35.364800: step 6240, loss = 2.28, batch loss = 2.23 (38.5 examples/sec; 0.208 sec/batch; 18h:50m:54s remains)
INFO - root - 2017-12-10 04:14:37.471729: step 6250, loss = 2.28, batch loss = 2.23 (38.8 examples/sec; 0.206 sec/batch; 18h:41m:54s remains)
INFO - root - 2017-12-10 04:14:39.598579: step 6260, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 19h:02m:44s remains)
INFO - root - 2017-12-10 04:14:41.746748: step 6270, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:33m:49s remains)
INFO - root - 2017-12-10 04:14:43.884895: step 6280, loss = 2.28, batch loss = 2.23 (38.2 examples/sec; 0.209 sec/batch; 18h:57m:22s remains)
INFO - root - 2017-12-10 04:14:46.036041: step 6290, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:52m:30s remains)
INFO - root - 2017-12-10 04:14:48.173769: step 6300, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 19h:27m:24s remains)
2017-12-10 04:14:48.573053: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42869 -4.4287124 -4.4287219 -4.4287271 -4.4287491 -4.4287782 -4.4288044 -4.4288211 -4.4288197 -4.4288068 -4.4288068 -4.4288039 -4.4288054 -4.4288011 -4.4287891][-4.4287372 -4.4287786 -4.4287987 -4.4287977 -4.428803 -4.4288216 -4.428843 -4.4288578 -4.4288521 -4.4288273 -4.4288158 -4.4288068 -4.4288011 -4.4287992 -4.4288015][-4.4287963 -4.4288397 -4.428865 -4.428863 -4.4288616 -4.4288712 -4.4288797 -4.4288855 -4.4288764 -4.4288483 -4.4288244 -4.4288015 -4.4287853 -4.4287906 -4.4288106][-4.4288387 -4.4288726 -4.4288993 -4.4289026 -4.4289 -4.4288983 -4.4288898 -4.4288826 -4.4288726 -4.4288449 -4.428812 -4.4287763 -4.4287572 -4.4287753 -4.4288063][-4.4288321 -4.4288526 -4.4288788 -4.4288845 -4.4288678 -4.4288378 -4.4287968 -4.4287763 -4.4287791 -4.4287682 -4.4287405 -4.4287033 -4.4286909 -4.4287267 -4.42877][-4.4287939 -4.428802 -4.4288239 -4.4288211 -4.4287782 -4.4286952 -4.4285984 -4.4285674 -4.42861 -4.4286423 -4.4286394 -4.4286141 -4.4286184 -4.4286752 -4.4287314][-4.4287467 -4.4287372 -4.4287457 -4.4287286 -4.4286494 -4.4284983 -4.4283204 -4.4282832 -4.4284072 -4.4285169 -4.428555 -4.4285483 -4.4285722 -4.4286442 -4.428709][-4.4287028 -4.4286671 -4.4286561 -4.4286251 -4.4285278 -4.428328 -4.4280806 -4.4280543 -4.428278 -4.428463 -4.4285278 -4.4285278 -4.4285541 -4.4286251 -4.4286914][-4.4286804 -4.4286332 -4.4286184 -4.42859 -4.4285097 -4.4283319 -4.4281125 -4.4281125 -4.4283404 -4.4285226 -4.4285808 -4.42857 -4.4285822 -4.42864 -4.4286942][-4.4287071 -4.4286656 -4.4286566 -4.4286466 -4.4286032 -4.4284821 -4.4283433 -4.4283519 -4.4285035 -4.4286275 -4.4286656 -4.4286523 -4.4286566 -4.4286981 -4.4287376][-4.428771 -4.4287429 -4.4287391 -4.428741 -4.4287229 -4.4286542 -4.4285755 -4.4285746 -4.4286504 -4.4287176 -4.4287434 -4.4287405 -4.4287448 -4.4287729 -4.4288015][-4.4288349 -4.4288192 -4.4288154 -4.4288177 -4.4288096 -4.4287753 -4.4287357 -4.4287248 -4.4287524 -4.42878 -4.4287996 -4.4288058 -4.4288125 -4.4288316 -4.4288487][-4.4288754 -4.4288678 -4.4288673 -4.4288654 -4.4288611 -4.4288507 -4.4288368 -4.4288206 -4.4288197 -4.4288254 -4.4288416 -4.4288521 -4.4288545 -4.4288616 -4.428863][-4.4289045 -4.4289055 -4.42891 -4.428906 -4.4289026 -4.428906 -4.4289036 -4.4288878 -4.4288735 -4.4288692 -4.4288807 -4.428885 -4.4288764 -4.42887 -4.4288516][-4.4289279 -4.4289355 -4.4289417 -4.428937 -4.4289317 -4.428937 -4.4289389 -4.4289279 -4.4289107 -4.4289002 -4.4289021 -4.428894 -4.4288769 -4.4288611 -4.4288235]]...]
INFO - root - 2017-12-10 04:14:50.690033: step 6310, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 19h:10m:21s remains)
INFO - root - 2017-12-10 04:14:52.844281: step 6320, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:36m:27s remains)
INFO - root - 2017-12-10 04:14:54.960212: step 6330, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 19h:18m:17s remains)
INFO - root - 2017-12-10 04:14:57.155695: step 6340, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 19h:25m:08s remains)
INFO - root - 2017-12-10 04:14:59.295126: step 6350, loss = 2.28, batch loss = 2.23 (38.8 examples/sec; 0.206 sec/batch; 18h:41m:57s remains)
INFO - root - 2017-12-10 04:15:01.466273: step 6360, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:31m:43s remains)
INFO - root - 2017-12-10 04:15:03.635477: step 6370, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:15m:37s remains)
INFO - root - 2017-12-10 04:15:05.811428: step 6380, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:37m:16s remains)
INFO - root - 2017-12-10 04:15:07.955723: step 6390, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:13m:45s remains)
INFO - root - 2017-12-10 04:15:10.095495: step 6400, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:15m:55s remains)
2017-12-10 04:15:10.444527: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289141 -4.4288945 -4.4288797 -4.4288669 -4.4288707 -4.4288888 -4.4289145 -4.4289312 -4.4289269 -4.4289074 -4.4288807 -4.4288597 -4.4288616 -4.4288721 -4.428884][-4.4288936 -4.4288616 -4.42884 -4.4288282 -4.4288383 -4.4288626 -4.4288931 -4.4289126 -4.4289074 -4.4288855 -4.4288611 -4.4288449 -4.4288545 -4.4288745 -4.4288993][-4.4288745 -4.4288316 -4.4288015 -4.4287934 -4.4288111 -4.4288383 -4.4288664 -4.4288836 -4.4288778 -4.4288592 -4.4288445 -4.4288435 -4.428864 -4.4288864 -4.4289103][-4.428844 -4.4287987 -4.4287686 -4.428762 -4.4287748 -4.42879 -4.4288077 -4.4288216 -4.4288216 -4.4288154 -4.4288144 -4.4288335 -4.4288678 -4.4288936 -4.4289093][-4.4288 -4.4287581 -4.4287319 -4.4287257 -4.4287248 -4.4287152 -4.4287152 -4.4287324 -4.4287519 -4.4287686 -4.4287848 -4.4288163 -4.4288578 -4.4288845 -4.428896][-4.4287643 -4.4287333 -4.4287128 -4.4286909 -4.428658 -4.4286146 -4.4285994 -4.428627 -4.4286723 -4.4287152 -4.4287457 -4.4287844 -4.4288254 -4.428853 -4.428865][-4.4287477 -4.42873 -4.4287167 -4.42868 -4.4286141 -4.4285398 -4.4285159 -4.4285474 -4.4286065 -4.428669 -4.4287162 -4.4287624 -4.4287992 -4.428822 -4.4288378][-4.4287453 -4.4287362 -4.4287286 -4.4286876 -4.4286127 -4.4285383 -4.4285254 -4.4285631 -4.4286194 -4.4286733 -4.4287186 -4.4287643 -4.4287934 -4.4288087 -4.428822][-4.4287481 -4.4287395 -4.4287343 -4.4287033 -4.4286442 -4.4285913 -4.4285922 -4.4286342 -4.4286809 -4.4287205 -4.4287524 -4.4287858 -4.4288073 -4.4288154 -4.42882][-4.42877 -4.428761 -4.4287591 -4.4287443 -4.4287124 -4.4286814 -4.42869 -4.4287248 -4.4287605 -4.4287896 -4.4288106 -4.4288335 -4.4288454 -4.428843 -4.4288335][-4.4287968 -4.428791 -4.4287968 -4.4287972 -4.4287877 -4.4287758 -4.428782 -4.4288025 -4.4288216 -4.4288392 -4.4288516 -4.428863 -4.4288621 -4.4288468 -4.4288268][-4.4288373 -4.428833 -4.4288406 -4.4288449 -4.4288435 -4.428843 -4.4288478 -4.4288549 -4.4288578 -4.4288626 -4.4288673 -4.4288683 -4.4288568 -4.4288335 -4.4288082][-4.4288859 -4.4288816 -4.4288888 -4.4288898 -4.4288878 -4.4288888 -4.4288893 -4.4288898 -4.4288869 -4.4288859 -4.4288845 -4.4288793 -4.428864 -4.42884 -4.4288163][-4.4289155 -4.428915 -4.4289222 -4.4289231 -4.4289179 -4.4289126 -4.42891 -4.4289083 -4.4289055 -4.4289031 -4.4289002 -4.428895 -4.428885 -4.4288716 -4.42886][-4.4289317 -4.4289336 -4.42894 -4.4289393 -4.4289327 -4.4289255 -4.4289212 -4.4289193 -4.4289165 -4.4289155 -4.4289141 -4.4289107 -4.4289074 -4.4289045 -4.4289036]]...]
INFO - root - 2017-12-10 04:15:12.599037: step 6410, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:23m:09s remains)
INFO - root - 2017-12-10 04:15:14.726290: step 6420, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:22m:52s remains)
INFO - root - 2017-12-10 04:15:16.883975: step 6430, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:31m:38s remains)
INFO - root - 2017-12-10 04:15:18.998422: step 6440, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:40m:00s remains)
INFO - root - 2017-12-10 04:15:21.164734: step 6450, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:15m:59s remains)
INFO - root - 2017-12-10 04:15:23.276708: step 6460, loss = 2.28, batch loss = 2.23 (38.6 examples/sec; 0.207 sec/batch; 18h:46m:44s remains)
INFO - root - 2017-12-10 04:15:25.439165: step 6470, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:57m:28s remains)
INFO - root - 2017-12-10 04:15:27.594443: step 6480, loss = 2.28, batch loss = 2.23 (38.2 examples/sec; 0.209 sec/batch; 18h:57m:15s remains)
INFO - root - 2017-12-10 04:15:29.733531: step 6490, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:36m:29s remains)
INFO - root - 2017-12-10 04:15:31.880602: step 6500, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:11m:30s remains)
2017-12-10 04:15:32.270310: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289536 -4.4289079 -4.4288378 -4.4287658 -4.4287038 -4.4286494 -4.4286404 -4.4286776 -4.4287233 -4.428772 -4.4287996 -4.4287925 -4.4287667 -4.42874 -4.4287114][-4.4289351 -4.4288754 -4.4287891 -4.4287076 -4.4286394 -4.4285855 -4.4285865 -4.4286366 -4.4286938 -4.4287472 -4.4287624 -4.4287372 -4.4287019 -4.4286728 -4.4286427][-4.428916 -4.4288373 -4.4287295 -4.4286323 -4.4285522 -4.4285016 -4.428525 -4.4285927 -4.4286547 -4.4287071 -4.4287114 -4.428678 -4.4286389 -4.4286084 -4.4285769][-4.4288955 -4.4288 -4.428678 -4.4285722 -4.4284835 -4.4284377 -4.428483 -4.4285617 -4.4286261 -4.4286728 -4.4286695 -4.4286346 -4.4285932 -4.4285588 -4.4285264][-4.4288745 -4.4287705 -4.4286494 -4.4285483 -4.4284611 -4.4284272 -4.4284816 -4.4285536 -4.4286122 -4.4286509 -4.428648 -4.4286137 -4.42857 -4.4285293 -4.4285007][-4.4288573 -4.4287472 -4.4286261 -4.4285226 -4.4284406 -4.42841 -4.4284458 -4.428493 -4.4285436 -4.4285817 -4.4285955 -4.4285765 -4.4285407 -4.4285021 -4.4284763][-4.4288468 -4.4287333 -4.4286075 -4.4284883 -4.4283957 -4.4283605 -4.4283667 -4.4283891 -4.428441 -4.4284892 -4.4285307 -4.4285421 -4.4285297 -4.4284983 -4.4284639][-4.4288335 -4.4287138 -4.4285827 -4.4284515 -4.4283462 -4.428298 -4.4282856 -4.4283085 -4.4283805 -4.4284353 -4.4284878 -4.4285188 -4.4285264 -4.4285 -4.4284515][-4.4288106 -4.4286785 -4.4285388 -4.4284086 -4.4283056 -4.4282513 -4.4282312 -4.42827 -4.4283624 -4.4284177 -4.4284673 -4.4285073 -4.4285269 -4.4285049 -4.4284482][-4.4287925 -4.428658 -4.428524 -4.4284139 -4.4283271 -4.4282708 -4.4282484 -4.4283028 -4.4284039 -4.4284577 -4.4285049 -4.4285436 -4.4285617 -4.4285359 -4.4284806][-4.4287939 -4.4286737 -4.4285631 -4.42848 -4.4284134 -4.4283633 -4.4283509 -4.42841 -4.4285026 -4.4285574 -4.4286041 -4.4286351 -4.4286389 -4.4286036 -4.4285531][-4.4288125 -4.4287186 -4.4286366 -4.428576 -4.4285226 -4.4284868 -4.4284921 -4.4285479 -4.428618 -4.4286656 -4.42871 -4.4287338 -4.4287257 -4.4286895 -4.4286504][-4.4288392 -4.428772 -4.4287186 -4.4286804 -4.4286394 -4.4286122 -4.4286261 -4.4286709 -4.4287143 -4.4287472 -4.4287796 -4.4287977 -4.4287906 -4.428761 -4.4287324][-4.4288797 -4.4288349 -4.4288044 -4.4287848 -4.4287591 -4.4287386 -4.4287491 -4.4287758 -4.4287996 -4.4288163 -4.428834 -4.4288435 -4.4288368 -4.4288177 -4.4287987][-4.4289231 -4.428895 -4.4288759 -4.4288621 -4.428843 -4.42883 -4.4288359 -4.4288492 -4.4288607 -4.4288673 -4.4288735 -4.4288759 -4.4288716 -4.4288616 -4.42885]]...]
INFO - root - 2017-12-10 04:15:34.392707: step 6510, loss = 2.28, batch loss = 2.23 (38.4 examples/sec; 0.208 sec/batch; 18h:51m:31s remains)
INFO - root - 2017-12-10 04:15:36.514157: step 6520, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:23m:29s remains)
INFO - root - 2017-12-10 04:15:38.678046: step 6530, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:30m:58s remains)
INFO - root - 2017-12-10 04:15:40.795111: step 6540, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:07m:49s remains)
INFO - root - 2017-12-10 04:15:42.922806: step 6550, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:15m:58s remains)
INFO - root - 2017-12-10 04:15:45.046469: step 6560, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 19h:19m:32s remains)
INFO - root - 2017-12-10 04:15:47.174297: step 6570, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:06m:23s remains)
INFO - root - 2017-12-10 04:15:49.361883: step 6580, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:48m:12s remains)
INFO - root - 2017-12-10 04:15:51.472172: step 6590, loss = 2.28, batch loss = 2.23 (39.7 examples/sec; 0.201 sec/batch; 18h:14m:09s remains)
INFO - root - 2017-12-10 04:15:53.624810: step 6600, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 19h:24m:00s remains)
2017-12-10 04:15:53.945894: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289474 -4.428906 -4.4288492 -4.4287877 -4.4287467 -4.4287143 -4.4286785 -4.428647 -4.4286203 -4.4286051 -4.4285922 -4.4285789 -4.4285707 -4.4285684 -4.4285645][-4.4289532 -4.4289131 -4.4288406 -4.4287663 -4.4287214 -4.4286938 -4.4286675 -4.4286442 -4.4286108 -4.4285841 -4.4285736 -4.4285707 -4.4285746 -4.4285774 -4.4285693][-4.4289656 -4.4289289 -4.4288497 -4.4287691 -4.4287205 -4.4286909 -4.4286714 -4.4286542 -4.4286304 -4.428606 -4.4285979 -4.4286046 -4.4286108 -4.428597 -4.4285693][-4.4289742 -4.4289393 -4.4288597 -4.4287772 -4.4287238 -4.4286885 -4.4286609 -4.4286437 -4.4286323 -4.4286308 -4.4286361 -4.4286413 -4.428638 -4.428618 -4.4285903][-4.4289718 -4.4289355 -4.4288583 -4.428782 -4.4287314 -4.4286947 -4.4286556 -4.428617 -4.428606 -4.4286246 -4.4286485 -4.4286504 -4.4286385 -4.4286246 -4.42861][-4.4289637 -4.4289222 -4.4288478 -4.4287786 -4.4287281 -4.4286871 -4.428627 -4.428556 -4.4285436 -4.4285793 -4.4286184 -4.4286218 -4.4286108 -4.4286137 -4.4286227][-4.4289556 -4.428905 -4.4288182 -4.428741 -4.4286819 -4.4286227 -4.4285345 -4.4284363 -4.4284253 -4.42848 -4.4285345 -4.4285522 -4.428545 -4.4285545 -4.4285774][-4.4289546 -4.4288974 -4.4287996 -4.4287109 -4.428638 -4.4285607 -4.428452 -4.4283395 -4.4283247 -4.4283834 -4.4284425 -4.4284782 -4.4284806 -4.428484 -4.428503][-4.4289603 -4.428906 -4.4288206 -4.4287424 -4.4286718 -4.4285917 -4.4284844 -4.4283681 -4.4283352 -4.4283776 -4.428431 -4.4284706 -4.4284692 -4.4284496 -4.42845][-4.4289618 -4.4289155 -4.4288487 -4.4287858 -4.4287233 -4.4286509 -4.4285526 -4.4284449 -4.4284015 -4.428432 -4.4284849 -4.4285183 -4.4285035 -4.4284596 -4.4284363][-4.4289579 -4.4289126 -4.428853 -4.428792 -4.428731 -4.42866 -4.4285779 -4.4284921 -4.4284515 -4.4284749 -4.4285221 -4.4285512 -4.4285393 -4.4284968 -4.4284678][-4.4289551 -4.4289107 -4.4288521 -4.4287891 -4.4287281 -4.4286647 -4.4286032 -4.4285469 -4.4285197 -4.4285345 -4.4285641 -4.4285793 -4.4285707 -4.4285278 -4.4284945][-4.4289579 -4.4289179 -4.4288611 -4.4287958 -4.4287372 -4.4286895 -4.42865 -4.4286146 -4.4285979 -4.4285989 -4.4286118 -4.4286132 -4.4286027 -4.4285631 -4.42853][-4.4289708 -4.4289346 -4.4288807 -4.42882 -4.4287705 -4.4287343 -4.4287128 -4.4286947 -4.4286809 -4.4286766 -4.4286804 -4.4286795 -4.4286728 -4.4286385 -4.4286027][-4.4289913 -4.4289622 -4.4289179 -4.42887 -4.4288335 -4.4288135 -4.4288073 -4.4288011 -4.428793 -4.4287858 -4.4287815 -4.4287844 -4.4287834 -4.4287543 -4.4287152]]...]
INFO - root - 2017-12-10 04:15:56.082124: step 6610, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 19h:19m:10s remains)
INFO - root - 2017-12-10 04:15:58.214635: step 6620, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.211 sec/batch; 19h:04m:03s remains)
INFO - root - 2017-12-10 04:16:00.344737: step 6630, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.214 sec/batch; 19h:19m:39s remains)
INFO - root - 2017-12-10 04:16:02.491484: step 6640, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:13m:31s remains)
INFO - root - 2017-12-10 04:16:04.627799: step 6650, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:32m:24s remains)
INFO - root - 2017-12-10 04:16:06.745055: step 6660, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:47m:19s remains)
INFO - root - 2017-12-10 04:16:08.891138: step 6670, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:28m:58s remains)
INFO - root - 2017-12-10 04:16:11.047830: step 6680, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:20m:56s remains)
INFO - root - 2017-12-10 04:16:13.155413: step 6690, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 19h:18m:54s remains)
INFO - root - 2017-12-10 04:16:15.306544: step 6700, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:21m:55s remains)
2017-12-10 04:16:15.688435: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287219 -4.4287152 -4.4287305 -4.4287395 -4.428751 -4.4287558 -4.4287434 -4.4287014 -4.4286566 -4.4286661 -4.4286618 -4.42866 -4.4286966 -4.4287643 -4.4288449][-4.4286571 -4.4286461 -4.4286695 -4.4286761 -4.4286757 -4.428679 -4.4286771 -4.4286432 -4.4286 -4.4286132 -4.4286089 -4.4286008 -4.4286408 -4.42872 -4.4288154][-4.4286695 -4.4286523 -4.42868 -4.4286823 -4.4286671 -4.4286695 -4.42867 -4.4286361 -4.4285922 -4.4286065 -4.4286103 -4.4286022 -4.4286366 -4.4287095 -4.428803][-4.4287219 -4.4287114 -4.4287381 -4.4287343 -4.4287128 -4.428709 -4.4286971 -4.4286413 -4.4285922 -4.4286084 -4.4286218 -4.428617 -4.4286437 -4.4287057 -4.4287906][-4.4287667 -4.428771 -4.428792 -4.4287786 -4.4287448 -4.4287152 -4.4286647 -4.4285707 -4.4285264 -4.4285603 -4.4285865 -4.4286022 -4.4286304 -4.4286895 -4.4287758][-4.4288087 -4.4288244 -4.4288373 -4.4288011 -4.4287357 -4.4286575 -4.4285493 -4.4283943 -4.4283442 -4.4284077 -4.4284668 -4.4285331 -4.4285846 -4.4286556 -4.4287581][-4.42881 -4.42882 -4.4288182 -4.4287472 -4.4286385 -4.4285159 -4.4283328 -4.4280868 -4.4280376 -4.4281778 -4.4283004 -4.4284229 -4.4285197 -4.4286213 -4.4287462][-4.4287829 -4.4287705 -4.4287486 -4.4286652 -4.428544 -4.4283996 -4.4281621 -4.4278378 -4.42782 -4.42806 -4.4282231 -4.4283586 -4.4284782 -4.4286036 -4.4287391][-4.4287858 -4.4287457 -4.4287143 -4.4286461 -4.4285612 -4.4284682 -4.4282947 -4.4280529 -4.4280405 -4.4282312 -4.4283471 -4.4284306 -4.4285231 -4.4286342 -4.4287515][-4.4288263 -4.428781 -4.4287519 -4.4287148 -4.428679 -4.4286451 -4.4285421 -4.4283834 -4.428359 -4.4284744 -4.4285421 -4.4285731 -4.4286256 -4.4287014 -4.4287806][-4.4288607 -4.4288225 -4.4288092 -4.4287944 -4.4287839 -4.42878 -4.4287214 -4.428618 -4.4285827 -4.4286475 -4.4286914 -4.4286947 -4.4287128 -4.4287553 -4.4288058][-4.42889 -4.4288559 -4.4288459 -4.4288425 -4.4288511 -4.4288669 -4.4288411 -4.4287825 -4.4287515 -4.4287772 -4.428803 -4.4287963 -4.4287882 -4.4288044 -4.4288306][-4.4289083 -4.4288745 -4.4288616 -4.4288635 -4.4288778 -4.4288926 -4.4288836 -4.4288473 -4.4288211 -4.428833 -4.428853 -4.4288511 -4.4288363 -4.4288406 -4.4288526][-4.4289107 -4.4288783 -4.4288683 -4.4288721 -4.4288797 -4.4288845 -4.4288716 -4.4288445 -4.4288249 -4.4288354 -4.4288568 -4.4288659 -4.4288616 -4.4288673 -4.4288793][-4.428916 -4.4288936 -4.4288869 -4.4288888 -4.4288893 -4.428884 -4.4288692 -4.42885 -4.4288383 -4.4288454 -4.4288635 -4.4288793 -4.4288874 -4.4289002 -4.428915]]...]
INFO - root - 2017-12-10 04:16:17.835996: step 6710, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:35m:07s remains)
INFO - root - 2017-12-10 04:16:19.952402: step 6720, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:11m:23s remains)
INFO - root - 2017-12-10 04:16:22.144823: step 6730, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 19h:17m:00s remains)
INFO - root - 2017-12-10 04:16:24.259642: step 6740, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:36m:42s remains)
INFO - root - 2017-12-10 04:16:26.358168: step 6750, loss = 2.28, batch loss = 2.23 (38.3 examples/sec; 0.209 sec/batch; 18h:55m:17s remains)
INFO - root - 2017-12-10 04:16:28.515720: step 6760, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:05m:02s remains)
INFO - root - 2017-12-10 04:16:30.651970: step 6770, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:58m:52s remains)
INFO - root - 2017-12-10 04:16:32.864622: step 6780, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:38m:19s remains)
INFO - root - 2017-12-10 04:16:35.016743: step 6790, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:20m:11s remains)
INFO - root - 2017-12-10 04:16:37.141876: step 6800, loss = 2.28, batch loss = 2.23 (38.2 examples/sec; 0.209 sec/batch; 18h:56m:45s remains)
2017-12-10 04:16:37.509472: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287791 -4.4287148 -4.4286685 -4.4286528 -4.42869 -4.4287214 -4.4287248 -4.4287233 -4.4287548 -4.4287753 -4.4287691 -4.4287648 -4.4287696 -4.4287629 -4.4287181][-4.4287782 -4.4287109 -4.4286671 -4.4286466 -4.4286695 -4.4286871 -4.4286752 -4.428659 -4.4286952 -4.4287381 -4.4287424 -4.4287295 -4.4287286 -4.4287243 -4.4286785][-4.4287786 -4.4287138 -4.4286714 -4.4286485 -4.4286528 -4.4286561 -4.42863 -4.4285927 -4.4286194 -4.4286871 -4.4287062 -4.4286804 -4.4286733 -4.42867 -4.4286203][-4.4287844 -4.4287243 -4.4286823 -4.4286485 -4.4286304 -4.4286156 -4.4285831 -4.428534 -4.4285426 -4.4286275 -4.4286623 -4.428628 -4.4286165 -4.4286103 -4.4285612][-4.4287872 -4.4287291 -4.4286828 -4.4286351 -4.4286041 -4.4285831 -4.42856 -4.428514 -4.428515 -4.4286003 -4.4286437 -4.4286227 -4.4286165 -4.4286113 -4.428565][-4.4287906 -4.4287276 -4.4286718 -4.4286146 -4.4285827 -4.4285641 -4.4285569 -4.4285331 -4.4285445 -4.4286208 -4.4286618 -4.4286642 -4.4286737 -4.4286733 -4.4286361][-4.4287863 -4.4287076 -4.42864 -4.4285779 -4.4285502 -4.4285307 -4.42853 -4.42853 -4.4285622 -4.4286294 -4.428669 -4.4286909 -4.4287162 -4.4287229 -4.4286976][-4.4287586 -4.428658 -4.4285755 -4.4285173 -4.4284916 -4.4284611 -4.4284482 -4.4284549 -4.4285007 -4.4285665 -4.4286132 -4.4286494 -4.4286809 -4.4286923 -4.4286828][-4.4287219 -4.4285965 -4.4284925 -4.4284339 -4.4284091 -4.4283657 -4.4283295 -4.4283233 -4.4283667 -4.4284353 -4.4284921 -4.428545 -4.4285889 -4.4286137 -4.4286294][-4.42869 -4.4285483 -4.4284248 -4.428359 -4.4283309 -4.4282866 -4.4282455 -4.4282241 -4.4282537 -4.428318 -4.4283805 -4.4284496 -4.4285092 -4.4285579 -4.4286032][-4.4286923 -4.4285569 -4.4284348 -4.4283638 -4.428339 -4.4283166 -4.4282951 -4.4282804 -4.4283009 -4.4283495 -4.428401 -4.4284663 -4.4285326 -4.428596 -4.4286537][-4.4287548 -4.42865 -4.4285536 -4.4284916 -4.4284739 -4.4284697 -4.4284687 -4.4284692 -4.4284859 -4.42852 -4.4285564 -4.4286051 -4.4286633 -4.428721 -4.4287682][-4.4288478 -4.4287791 -4.4287176 -4.4286766 -4.4286628 -4.4286666 -4.4286776 -4.4286852 -4.4286962 -4.4287186 -4.4287405 -4.4287672 -4.4288015 -4.4288387 -4.4288669][-4.4289293 -4.4288864 -4.4288492 -4.4288239 -4.4288111 -4.4288149 -4.4288278 -4.4288359 -4.4288406 -4.428853 -4.4288645 -4.4288764 -4.4288917 -4.4289103 -4.4289246][-4.42898 -4.4289536 -4.4289327 -4.4289184 -4.4289088 -4.4289093 -4.4289169 -4.4289222 -4.428925 -4.4289308 -4.4289346 -4.4289374 -4.4289427 -4.4289503 -4.4289579]]...]
INFO - root - 2017-12-10 04:16:39.670921: step 6810, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.210 sec/batch; 19h:02m:32s remains)
INFO - root - 2017-12-10 04:16:41.869210: step 6820, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:18m:46s remains)
INFO - root - 2017-12-10 04:16:44.010064: step 6830, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:21m:40s remains)
INFO - root - 2017-12-10 04:16:46.186311: step 6840, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:27m:18s remains)
INFO - root - 2017-12-10 04:16:48.301132: step 6850, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.210 sec/batch; 19h:01m:14s remains)
INFO - root - 2017-12-10 04:16:50.456090: step 6860, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:31m:27s remains)
INFO - root - 2017-12-10 04:16:52.641275: step 6870, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 20h:13m:45s remains)
INFO - root - 2017-12-10 04:16:54.807170: step 6880, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 19h:23m:19s remains)
INFO - root - 2017-12-10 04:16:57.012244: step 6890, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.211 sec/batch; 19h:02m:49s remains)
INFO - root - 2017-12-10 04:16:59.166957: step 6900, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 19h:16m:43s remains)
2017-12-10 04:16:59.524983: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288573 -4.4287925 -4.4287486 -4.4287362 -4.4287043 -4.4286704 -4.4286485 -4.4286456 -4.4286909 -4.4287343 -4.4287624 -4.4287863 -4.4288025 -4.42878 -4.4287319][-4.4288378 -4.4287643 -4.4287176 -4.4287162 -4.428688 -4.4286466 -4.4286127 -4.4286051 -4.4286671 -4.4287262 -4.4287596 -4.42878 -4.4287863 -4.4287467 -4.4286833][-4.4288268 -4.4287481 -4.4286947 -4.4286923 -4.4286671 -4.428628 -4.4285855 -4.4285712 -4.4286413 -4.4287124 -4.4287539 -4.4287782 -4.4287772 -4.4287176 -4.4286304][-4.4288216 -4.4287348 -4.4286709 -4.4286604 -4.42864 -4.4286017 -4.4285421 -4.4285026 -4.4285674 -4.4286532 -4.4287181 -4.4287586 -4.428761 -4.4286942 -4.4286003][-4.4288249 -4.4287353 -4.4286628 -4.4286366 -4.428607 -4.4285603 -4.42846 -4.4283757 -4.4284477 -4.428576 -4.4286828 -4.4287477 -4.4287634 -4.4287047 -4.42861][-4.428843 -4.42876 -4.4286752 -4.4286137 -4.4285426 -4.4284496 -4.4282646 -4.4281073 -4.4282351 -4.4284782 -4.4286585 -4.4287543 -4.4287906 -4.42875 -4.4286642][-4.4288616 -4.4287896 -4.4287033 -4.4286113 -4.4284892 -4.428318 -4.4280076 -4.4277744 -4.4280219 -4.4283934 -4.428638 -4.428772 -4.4288306 -4.4288177 -4.4287624][-4.4288731 -4.4288025 -4.4287248 -4.4286418 -4.4285212 -4.4283323 -4.4279971 -4.42779 -4.4280629 -4.4284129 -4.4286389 -4.4287753 -4.4288392 -4.4288492 -4.4288254][-4.4288716 -4.4287953 -4.4287271 -4.4286704 -4.428587 -4.428453 -4.4282327 -4.4281378 -4.4283147 -4.428524 -4.4286633 -4.428761 -4.4288106 -4.4288325 -4.4288254][-4.428884 -4.4288034 -4.4287381 -4.4286928 -4.4286323 -4.4285464 -4.4284182 -4.4283805 -4.4284821 -4.4286013 -4.4286876 -4.4287534 -4.4287839 -4.4287949 -4.4287882][-4.4289141 -4.4288416 -4.4287834 -4.4287443 -4.4286857 -4.4286084 -4.428514 -4.4284959 -4.4285622 -4.4286332 -4.4286952 -4.4287467 -4.42876 -4.4287591 -4.4287534][-4.4289436 -4.4288807 -4.4288254 -4.4287858 -4.4287248 -4.4286451 -4.428555 -4.4285417 -4.4286017 -4.4286518 -4.4286962 -4.42874 -4.42875 -4.4287486 -4.4287424][-4.4289565 -4.4289088 -4.4288626 -4.4288239 -4.4287605 -4.4286771 -4.4285855 -4.4285722 -4.4286275 -4.4286618 -4.4287014 -4.4287424 -4.428761 -4.428762 -4.4287534][-4.42896 -4.4289293 -4.4288979 -4.4288611 -4.4288025 -4.4287262 -4.4286442 -4.4286356 -4.4286742 -4.4287004 -4.4287372 -4.4287786 -4.4288034 -4.4288049 -4.4287906][-4.428957 -4.4289379 -4.4289188 -4.4288878 -4.4288387 -4.4287829 -4.4287291 -4.4287181 -4.4287391 -4.4287596 -4.4287963 -4.4288373 -4.4288635 -4.4288683 -4.4288588]]...]
INFO - root - 2017-12-10 04:17:01.706027: step 6910, loss = 2.28, batch loss = 2.23 (38.3 examples/sec; 0.209 sec/batch; 18h:52m:49s remains)
INFO - root - 2017-12-10 04:17:03.852316: step 6920, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 20h:06m:21s remains)
INFO - root - 2017-12-10 04:17:06.045474: step 6930, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:53m:54s remains)
INFO - root - 2017-12-10 04:17:08.259532: step 6940, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:27m:35s remains)
INFO - root - 2017-12-10 04:17:10.403738: step 6950, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 18h:57m:55s remains)
INFO - root - 2017-12-10 04:17:12.542348: step 6960, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:49m:49s remains)
INFO - root - 2017-12-10 04:17:14.696393: step 6970, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.250 sec/batch; 22h:38m:36s remains)
INFO - root - 2017-12-10 04:17:16.846812: step 6980, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:35m:37s remains)
INFO - root - 2017-12-10 04:17:18.970619: step 6990, loss = 2.28, batch loss = 2.23 (38.4 examples/sec; 0.208 sec/batch; 18h:50m:31s remains)
INFO - root - 2017-12-10 04:17:21.145852: step 7000, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:31m:55s remains)
2017-12-10 04:17:21.544775: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286289 -4.4286475 -4.4286389 -4.4286103 -4.4285946 -4.4285865 -4.4285774 -4.4285607 -4.4285293 -4.4285 -4.428504 -4.4285398 -4.4285488 -4.42856 -4.4285908][-4.4285822 -4.4285831 -4.4285769 -4.4285645 -4.4285593 -4.4285545 -4.4285507 -4.4285464 -4.428524 -4.4285164 -4.4285231 -4.4285345 -4.428524 -4.4285188 -4.4285378][-4.4285555 -4.4285421 -4.4285436 -4.4285383 -4.4285259 -4.42851 -4.4285021 -4.428503 -4.428514 -4.4285417 -4.4285583 -4.4285445 -4.4285135 -4.4284968 -4.4285069][-4.4285517 -4.4285407 -4.4285522 -4.4285522 -4.428525 -4.4284825 -4.4284477 -4.4284391 -4.428483 -4.4285512 -4.4285865 -4.4285545 -4.4285178 -4.4285054 -4.4285197][-4.4285431 -4.4285355 -4.428556 -4.4285583 -4.428525 -4.4284534 -4.4283791 -4.4283619 -4.4284506 -4.428566 -4.4286308 -4.4286056 -4.4285741 -4.428566 -4.4285722][-4.428503 -4.4285059 -4.4285398 -4.4285545 -4.4285145 -4.4284058 -4.42827 -4.4282417 -4.4283957 -4.4285636 -4.4286571 -4.428659 -4.4286432 -4.4286342 -4.4286265][-4.4284124 -4.4284382 -4.4284959 -4.4285188 -4.4284687 -4.4283214 -4.428123 -4.4280972 -4.4283328 -4.4285412 -4.4286418 -4.4286575 -4.4286528 -4.4286432 -4.4286385][-4.4283381 -4.4283795 -4.4284487 -4.4284744 -4.4284143 -4.4282703 -4.4280963 -4.42811 -4.4283371 -4.4285183 -4.4285927 -4.4286084 -4.4286075 -4.4286132 -4.4286318][-4.4283419 -4.4284024 -4.4284668 -4.4284883 -4.4284468 -4.4283547 -4.4282603 -4.4282885 -4.4284215 -4.4285178 -4.428546 -4.428545 -4.4285445 -4.4285617 -4.4285932][-4.4284048 -4.4284759 -4.4285278 -4.4285445 -4.4285259 -4.4284711 -4.4284134 -4.4284248 -4.4284892 -4.4285226 -4.4285164 -4.4285049 -4.4284964 -4.4285054 -4.4285336][-4.4285007 -4.4285746 -4.4286156 -4.428618 -4.4285955 -4.4285469 -4.4284978 -4.4284883 -4.428515 -4.4285212 -4.4285092 -4.4285016 -4.42848 -4.4284759 -4.4285021][-4.4285879 -4.4286528 -4.4286785 -4.4286666 -4.4286337 -4.428576 -4.4285192 -4.4285069 -4.42852 -4.4285264 -4.4285254 -4.42853 -4.4285026 -4.4284835 -4.4285173][-4.42863 -4.428669 -4.4286766 -4.428668 -4.4286337 -4.4285727 -4.4285178 -4.42852 -4.428546 -4.4285545 -4.42856 -4.428566 -4.4285426 -4.4285221 -4.428556][-4.4286251 -4.4286418 -4.4286432 -4.4286408 -4.4286165 -4.4285717 -4.4285221 -4.428525 -4.4285588 -4.4285684 -4.4285727 -4.4285789 -4.42856 -4.4285483 -4.4285789][-4.4286017 -4.4286127 -4.4286208 -4.4286256 -4.4286122 -4.4285803 -4.428524 -4.4285083 -4.428546 -4.4285679 -4.428587 -4.4286008 -4.4285793 -4.4285665 -4.4285913]]...]
INFO - root - 2017-12-10 04:17:23.712391: step 7010, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:59m:23s remains)
INFO - root - 2017-12-10 04:17:25.852354: step 7020, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 19h:22m:00s remains)
INFO - root - 2017-12-10 04:17:27.994086: step 7030, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 19h:23m:30s remains)
INFO - root - 2017-12-10 04:17:30.155711: step 7040, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:33m:14s remains)
INFO - root - 2017-12-10 04:17:32.314167: step 7050, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:14m:45s remains)
INFO - root - 2017-12-10 04:17:34.476512: step 7060, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:44m:33s remains)
INFO - root - 2017-12-10 04:17:36.643839: step 7070, loss = 2.28, batch loss = 2.23 (38.7 examples/sec; 0.207 sec/batch; 18h:42m:21s remains)
INFO - root - 2017-12-10 04:17:38.788261: step 7080, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 19h:24m:12s remains)
INFO - root - 2017-12-10 04:17:40.942645: step 7090, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:34m:44s remains)
INFO - root - 2017-12-10 04:17:43.067826: step 7100, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.211 sec/batch; 19h:06m:55s remains)
2017-12-10 04:17:43.424712: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428781 -4.4287596 -4.4287248 -4.4286809 -4.4286566 -4.4286509 -4.4286709 -4.4286823 -4.4286647 -4.4286823 -4.4287357 -4.428771 -4.4287534 -4.4287167 -4.4287295][-4.4287467 -4.4287391 -4.4287167 -4.4286833 -4.428659 -4.4286637 -4.4287033 -4.4287333 -4.4287229 -4.4287186 -4.4287448 -4.4287653 -4.4287405 -4.4287033 -4.4287176][-4.4287066 -4.4287176 -4.4287062 -4.4286876 -4.4286737 -4.428688 -4.4287434 -4.428791 -4.4287963 -4.4287858 -4.428782 -4.4287763 -4.4287477 -4.4287133 -4.4287229][-4.4286604 -4.4286871 -4.428689 -4.4286752 -4.4286675 -4.4286823 -4.4287343 -4.4287877 -4.4288173 -4.4288254 -4.4288163 -4.4287982 -4.4287682 -4.4287381 -4.4287376][-4.4286427 -4.4286776 -4.4286857 -4.4286671 -4.4286489 -4.4286385 -4.4286566 -4.4286895 -4.4287424 -4.4287982 -4.4288163 -4.4288058 -4.4287825 -4.4287562 -4.4287477][-4.4286809 -4.4287066 -4.4287162 -4.4286909 -4.4286432 -4.4285808 -4.4285197 -4.4284887 -4.4285569 -4.4286833 -4.42876 -4.4287791 -4.4287715 -4.4287596 -4.4287577][-4.4287663 -4.4287806 -4.4287834 -4.4287467 -4.4286633 -4.4285426 -4.4283819 -4.4282537 -4.4283075 -4.4285069 -4.4286571 -4.4287162 -4.4287348 -4.4287548 -4.4287815][-4.4288545 -4.4288669 -4.4288635 -4.4288197 -4.4287243 -4.4285717 -4.4283586 -4.4281659 -4.4281735 -4.4283757 -4.42856 -4.4286466 -4.42869 -4.4287467 -4.4288092][-4.4288821 -4.4289 -4.4289 -4.4288683 -4.4287887 -4.4286566 -4.4284768 -4.4283195 -4.4282966 -4.4284062 -4.4285221 -4.4285855 -4.4286342 -4.428709 -4.4287963][-4.42884 -4.4288626 -4.4288692 -4.428863 -4.428822 -4.4287386 -4.4286275 -4.4285326 -4.4285088 -4.4285359 -4.4285536 -4.4285526 -4.4285703 -4.42863 -4.4287181][-4.4287496 -4.4287782 -4.4287982 -4.4288096 -4.4288015 -4.4287691 -4.4287252 -4.4286833 -4.4286709 -4.4286685 -4.428638 -4.4285879 -4.4285579 -4.4285803 -4.4286494][-4.428659 -4.4286923 -4.4287271 -4.4287472 -4.4287415 -4.4287353 -4.4287343 -4.4287362 -4.4287486 -4.4287596 -4.4287286 -4.428669 -4.428618 -4.4286118 -4.428648][-4.4286194 -4.4286532 -4.4287033 -4.4287286 -4.428709 -4.4286909 -4.4286957 -4.4287128 -4.4287491 -4.428782 -4.4287729 -4.4287338 -4.4286923 -4.4286819 -4.4286938][-4.4286604 -4.4286885 -4.4287438 -4.4287677 -4.4287372 -4.4286981 -4.4286771 -4.42868 -4.4287119 -4.428751 -4.4287572 -4.4287424 -4.4287252 -4.42873 -4.4287405][-4.4287333 -4.4287543 -4.4287958 -4.4288139 -4.428792 -4.4287567 -4.4287157 -4.42869 -4.428689 -4.4286995 -4.4287047 -4.42871 -4.4287171 -4.4287343 -4.4287505]]...]
INFO - root - 2017-12-10 04:17:45.581813: step 7110, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:37m:38s remains)
INFO - root - 2017-12-10 04:17:47.764805: step 7120, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 20h:38m:44s remains)
INFO - root - 2017-12-10 04:17:49.958223: step 7130, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:34m:53s remains)
INFO - root - 2017-12-10 04:17:52.097884: step 7140, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:15m:06s remains)
INFO - root - 2017-12-10 04:17:54.264203: step 7150, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:42m:20s remains)
INFO - root - 2017-12-10 04:17:56.432391: step 7160, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.211 sec/batch; 19h:06m:11s remains)
INFO - root - 2017-12-10 04:17:58.635634: step 7170, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 20h:05m:05s remains)
INFO - root - 2017-12-10 04:18:00.755240: step 7180, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:10m:44s remains)
INFO - root - 2017-12-10 04:18:02.912626: step 7190, loss = 2.28, batch loss = 2.23 (38.5 examples/sec; 0.208 sec/batch; 18h:47m:52s remains)
INFO - root - 2017-12-10 04:18:05.071486: step 7200, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:29m:12s remains)
2017-12-10 04:18:05.438600: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288859 -4.4288745 -4.42886 -4.4288535 -4.4288611 -4.4288735 -4.4288616 -4.42885 -4.4288507 -4.4288406 -4.4288154 -4.4288044 -4.4288197 -4.4288464 -4.4288621][-4.4289126 -4.4288864 -4.4288611 -4.4288492 -4.4288578 -4.4288735 -4.4288607 -4.4288406 -4.4288325 -4.4288225 -4.4288006 -4.4287891 -4.4288006 -4.4288163 -4.4288187][-4.4289303 -4.4289 -4.4288688 -4.428853 -4.42886 -4.4288678 -4.4288497 -4.4288263 -4.428822 -4.428833 -4.4288321 -4.4288244 -4.428823 -4.4288206 -4.4288058][-4.4289246 -4.4289007 -4.428865 -4.4288363 -4.4288273 -4.428812 -4.4287806 -4.4287553 -4.4287724 -4.4288287 -4.4288716 -4.4288845 -4.4288797 -4.4288621 -4.4288344][-4.4289093 -4.4288926 -4.4288526 -4.4287939 -4.428741 -4.4286785 -4.428606 -4.428565 -4.4286151 -4.4287357 -4.4288378 -4.4288983 -4.4289284 -4.42892 -4.4288921][-4.4289184 -4.4289088 -4.4288573 -4.4287539 -4.4286385 -4.4285178 -4.4283738 -4.4282827 -4.4283614 -4.4285626 -4.4287405 -4.428865 -4.4289479 -4.4289646 -4.4289408][-4.4289403 -4.4289427 -4.4288812 -4.4287434 -4.42858 -4.4283919 -4.4281578 -4.4279804 -4.4280658 -4.4283481 -4.4286003 -4.4287877 -4.4289241 -4.4289732 -4.4289536][-4.4289427 -4.4289584 -4.4289021 -4.4287667 -4.4285936 -4.4283853 -4.4281144 -4.427887 -4.4279432 -4.428237 -4.4285107 -4.4287214 -4.4288859 -4.4289541 -4.4289384][-4.4289317 -4.4289594 -4.428925 -4.4288216 -4.4286776 -4.4285073 -4.4282937 -4.4281111 -4.4281263 -4.4283247 -4.4285388 -4.4287195 -4.428863 -4.4289222 -4.4289069][-4.4289393 -4.4289765 -4.4289646 -4.4288931 -4.4287853 -4.4286675 -4.4285331 -4.4284191 -4.4284091 -4.4285126 -4.4286513 -4.4287763 -4.4288659 -4.4288878 -4.4288564][-4.428957 -4.4289889 -4.4289861 -4.4289317 -4.4288449 -4.4287705 -4.4286995 -4.428637 -4.4286294 -4.428689 -4.4287791 -4.428854 -4.4288883 -4.4288745 -4.4288273][-4.4289675 -4.4289865 -4.4289775 -4.4289303 -4.4288659 -4.428822 -4.4287953 -4.4287739 -4.4287767 -4.42882 -4.4288826 -4.4289169 -4.428916 -4.4288859 -4.4288325][-4.4289541 -4.4289622 -4.4289508 -4.4289222 -4.4288907 -4.4288726 -4.4288712 -4.4288731 -4.4288797 -4.4289079 -4.428947 -4.4289551 -4.4289327 -4.4288955 -4.4288459][-4.4289389 -4.4289451 -4.4289379 -4.4289265 -4.4289169 -4.4289107 -4.4289103 -4.4289174 -4.4289293 -4.4289484 -4.4289703 -4.4289727 -4.42895 -4.4289117 -4.4288669][-4.4289374 -4.4289403 -4.428937 -4.4289331 -4.4289317 -4.4289279 -4.4289255 -4.4289322 -4.4289451 -4.4289603 -4.4289737 -4.4289761 -4.4289584 -4.4289279 -4.428895]]...]
INFO - root - 2017-12-10 04:18:07.586077: step 7210, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:12m:10s remains)
INFO - root - 2017-12-10 04:18:09.728164: step 7220, loss = 2.28, batch loss = 2.23 (38.2 examples/sec; 0.209 sec/batch; 18h:54m:40s remains)
INFO - root - 2017-12-10 04:18:11.839031: step 7230, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:24m:34s remains)
INFO - root - 2017-12-10 04:18:13.980630: step 7240, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:43m:09s remains)
INFO - root - 2017-12-10 04:18:16.128163: step 7250, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:46m:42s remains)
INFO - root - 2017-12-10 04:18:18.253257: step 7260, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:18m:44s remains)
INFO - root - 2017-12-10 04:18:20.385870: step 7270, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:18m:28s remains)
INFO - root - 2017-12-10 04:18:22.517329: step 7280, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:43m:51s remains)
INFO - root - 2017-12-10 04:18:24.646097: step 7290, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 19h:20m:58s remains)
INFO - root - 2017-12-10 04:18:26.769536: step 7300, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.211 sec/batch; 19h:06m:06s remains)
2017-12-10 04:18:27.100196: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288459 -4.4288363 -4.4288406 -4.428853 -4.428864 -4.4288654 -4.428864 -4.428874 -4.428885 -4.4288826 -4.4288664 -4.4288478 -4.4288416 -4.4288487 -4.4288778][-4.4287753 -4.4287534 -4.4287548 -4.4287734 -4.4287963 -4.4288073 -4.4288068 -4.4288197 -4.428833 -4.4288249 -4.4287963 -4.4287715 -4.4287648 -4.4287767 -4.4288206][-4.4286971 -4.4286475 -4.4286356 -4.4286604 -4.4286966 -4.4287205 -4.4287295 -4.4287491 -4.4287658 -4.4287553 -4.4287267 -4.4287028 -4.4287009 -4.4287171 -4.4287686][-4.4286594 -4.4285884 -4.4285631 -4.4285884 -4.4286366 -4.4286718 -4.4286923 -4.4287248 -4.4287462 -4.4287343 -4.4287133 -4.4287043 -4.4287109 -4.4287262 -4.4287739][-4.4286575 -4.42858 -4.42855 -4.4285707 -4.4286137 -4.4286418 -4.4286604 -4.4286938 -4.4287305 -4.4287405 -4.4287386 -4.4287453 -4.42875 -4.4287581 -4.4287982][-4.4286828 -4.4286146 -4.4285884 -4.4285975 -4.4286108 -4.4286041 -4.428587 -4.4286008 -4.4286618 -4.4287267 -4.4287672 -4.4287906 -4.4287834 -4.4287767 -4.4288073][-4.4287205 -4.4286771 -4.42866 -4.4286523 -4.428627 -4.4285693 -4.428472 -4.4284191 -4.4285011 -4.4286518 -4.4287577 -4.4288135 -4.4288092 -4.4287906 -4.42881][-4.4287515 -4.42873 -4.4287195 -4.4287052 -4.4286594 -4.4285612 -4.4283838 -4.4282317 -4.4283085 -4.4285488 -4.4287133 -4.4287944 -4.4288144 -4.4288082 -4.4288282][-4.4287577 -4.4287529 -4.4287581 -4.4287581 -4.4287243 -4.42863 -4.4284549 -4.4282851 -4.4283137 -4.4285293 -4.4286942 -4.4287744 -4.428812 -4.4288235 -4.4288487][-4.4287505 -4.4287624 -4.4288 -4.4288268 -4.4288206 -4.4287667 -4.4286523 -4.428544 -4.4285383 -4.4286404 -4.4287353 -4.4287791 -4.4288096 -4.4288182 -4.4288425][-4.4287357 -4.4287457 -4.4287987 -4.4288483 -4.4288626 -4.4288497 -4.428802 -4.4287462 -4.4287357 -4.4287705 -4.4287996 -4.428803 -4.4288125 -4.4288116 -4.4288263][-4.4287286 -4.4287271 -4.4287748 -4.4288244 -4.4288464 -4.4288559 -4.42885 -4.4288325 -4.4288349 -4.4288511 -4.4288616 -4.4288526 -4.4288487 -4.4288368 -4.4288354][-4.4287486 -4.42873 -4.4287629 -4.4287982 -4.4288154 -4.4288287 -4.4288411 -4.4288459 -4.4288635 -4.4288864 -4.4288979 -4.4288926 -4.4288874 -4.4288735 -4.4288616][-4.4287872 -4.4287672 -4.42879 -4.4288077 -4.4288106 -4.4288197 -4.4288325 -4.4288468 -4.4288669 -4.428896 -4.4289088 -4.428906 -4.4289031 -4.4288955 -4.4288812][-4.4288263 -4.4288135 -4.4288249 -4.42883 -4.428822 -4.4288211 -4.4288273 -4.4288435 -4.4288645 -4.42889 -4.4289031 -4.428905 -4.4289064 -4.4289031 -4.4288907]]...]
INFO - root - 2017-12-10 04:18:29.262391: step 7310, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:32m:26s remains)
INFO - root - 2017-12-10 04:18:31.407034: step 7320, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 19h:23m:24s remains)
INFO - root - 2017-12-10 04:18:33.578816: step 7330, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:18m:11s remains)
INFO - root - 2017-12-10 04:18:35.732672: step 7340, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:21m:20s remains)
INFO - root - 2017-12-10 04:18:37.869036: step 7350, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:35m:01s remains)
INFO - root - 2017-12-10 04:18:40.026578: step 7360, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.211 sec/batch; 19h:01m:41s remains)
INFO - root - 2017-12-10 04:18:42.169161: step 7370, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:30m:05s remains)
INFO - root - 2017-12-10 04:18:44.326629: step 7380, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:20m:19s remains)
INFO - root - 2017-12-10 04:18:46.506892: step 7390, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:48m:22s remains)
INFO - root - 2017-12-10 04:18:48.655802: step 7400, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:10m:31s remains)
2017-12-10 04:18:49.024390: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288611 -4.4288883 -4.4288745 -4.4288335 -4.4287825 -4.4287443 -4.428688 -4.4286022 -4.4285369 -4.4285426 -4.4285979 -4.4286733 -4.4287519 -4.4288044 -4.4288397][-4.4288955 -4.4289012 -4.4288731 -4.4288349 -4.4287925 -4.4287615 -4.4287133 -4.4286227 -4.4285622 -4.4285841 -4.4286361 -4.4287019 -4.42877 -4.4288197 -4.4288578][-4.4289169 -4.4289145 -4.428885 -4.4288478 -4.4288154 -4.4287891 -4.4287472 -4.4286594 -4.4286122 -4.4286466 -4.4287038 -4.4287629 -4.4288177 -4.4288621 -4.4288969][-4.4289341 -4.4289269 -4.4288979 -4.4288664 -4.4288373 -4.4288082 -4.4287605 -4.4286675 -4.4286242 -4.4286675 -4.42874 -4.4288054 -4.4288507 -4.4288969 -4.4289351][-4.4289651 -4.4289575 -4.428925 -4.4288869 -4.4288406 -4.4287853 -4.4287028 -4.4285846 -4.4285417 -4.4286113 -4.4287109 -4.4287915 -4.4288421 -4.4288993 -4.4289489][-4.4289937 -4.4289842 -4.4289503 -4.4289002 -4.42882 -4.42871 -4.4285564 -4.4283786 -4.42834 -4.4284739 -4.4286313 -4.4287419 -4.4288144 -4.4288859 -4.4289455][-4.4290075 -4.4289975 -4.428967 -4.428906 -4.4287906 -4.4286113 -4.428359 -4.4280849 -4.4280624 -4.4282951 -4.4285264 -4.4286833 -4.4287882 -4.4288778 -4.4289494][-4.4290085 -4.4289947 -4.4289637 -4.428895 -4.4287605 -4.4285488 -4.4282475 -4.4279227 -4.4279008 -4.4281783 -4.4284534 -4.428647 -4.4287777 -4.4288783 -4.428956][-4.4290071 -4.4289942 -4.4289613 -4.4288898 -4.4287663 -4.4285951 -4.4283581 -4.4280996 -4.42804 -4.4282246 -4.4284525 -4.4286437 -4.4287825 -4.4288864 -4.4289656][-4.4290104 -4.428998 -4.4289665 -4.4289002 -4.4288044 -4.4286871 -4.4285374 -4.4283671 -4.4282875 -4.4283657 -4.4285145 -4.4286747 -4.4288077 -4.4289074 -4.4289832][-4.4290104 -4.428998 -4.4289708 -4.428916 -4.4288449 -4.4287634 -4.428668 -4.4285607 -4.4284954 -4.4285288 -4.4286251 -4.4287467 -4.4288578 -4.4289393 -4.4290023][-4.4290152 -4.4290028 -4.4289775 -4.4289284 -4.4288721 -4.4288116 -4.4287434 -4.4286723 -4.4286342 -4.4286604 -4.4287348 -4.4288259 -4.4289093 -4.4289703 -4.4290171][-4.4290147 -4.4290013 -4.4289813 -4.4289417 -4.4288964 -4.4288487 -4.4287958 -4.4287467 -4.4287276 -4.4287505 -4.4288111 -4.42888 -4.4289403 -4.4289842 -4.4290175][-4.4290023 -4.4289951 -4.4289846 -4.4289575 -4.4289212 -4.4288816 -4.42884 -4.4288025 -4.428791 -4.428812 -4.4288592 -4.4289107 -4.4289532 -4.4289842 -4.429008][-4.4289832 -4.42899 -4.4289889 -4.4289722 -4.4289455 -4.4289155 -4.4288859 -4.4288592 -4.42885 -4.4288664 -4.4289002 -4.4289346 -4.4289618 -4.4289818 -4.4289961]]...]
INFO - root - 2017-12-10 04:18:51.195393: step 7410, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:18m:02s remains)
INFO - root - 2017-12-10 04:18:53.362069: step 7420, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:19m:33s remains)
INFO - root - 2017-12-10 04:18:55.507486: step 7430, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:35m:26s remains)
INFO - root - 2017-12-10 04:18:57.628469: step 7440, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.211 sec/batch; 19h:05m:31s remains)
INFO - root - 2017-12-10 04:18:59.741378: step 7450, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:04m:24s remains)
INFO - root - 2017-12-10 04:19:01.912968: step 7460, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:19m:13s remains)
INFO - root - 2017-12-10 04:19:04.128621: step 7470, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:29m:12s remains)
INFO - root - 2017-12-10 04:19:06.317490: step 7480, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:33m:50s remains)
INFO - root - 2017-12-10 04:19:08.465029: step 7490, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 19h:06m:51s remains)
INFO - root - 2017-12-10 04:19:10.599388: step 7500, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 18h:56m:33s remains)
2017-12-10 04:19:10.924135: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286838 -4.4286733 -4.4286795 -4.4286594 -4.4286308 -4.4286232 -4.4286613 -4.4287443 -4.4288144 -4.4288478 -4.4288621 -4.4288664 -4.4288588 -4.4288292 -4.42876][-4.4286013 -4.4286017 -4.4286208 -4.4285965 -4.4285564 -4.428544 -4.4285946 -4.4286919 -4.4287667 -4.4288058 -4.4288249 -4.4288321 -4.4288368 -4.42882 -4.42876][-4.4285493 -4.428566 -4.428607 -4.4285874 -4.4285398 -4.4285154 -4.428545 -4.428638 -4.4287205 -4.4287796 -4.4288054 -4.4288154 -4.4288397 -4.4288449 -4.428791][-4.4285741 -4.428596 -4.4286423 -4.4286261 -4.4285727 -4.4285212 -4.4284987 -4.4285583 -4.4286456 -4.4287276 -4.4287696 -4.4287963 -4.4288411 -4.4288692 -4.4288273][-4.4286323 -4.4286456 -4.4286733 -4.4286489 -4.4285874 -4.4285092 -4.4284348 -4.4284458 -4.4285345 -4.4286461 -4.4287229 -4.428771 -4.4288249 -4.4288678 -4.4288421][-4.4286513 -4.4286437 -4.4286566 -4.4286304 -4.428556 -4.42845 -4.4283285 -4.4283051 -4.4284139 -4.4285727 -4.428679 -4.4287386 -4.4287982 -4.4288435 -4.4288297][-4.4286332 -4.4286108 -4.4286141 -4.4285827 -4.4284992 -4.4283657 -4.4281979 -4.4281578 -4.4283051 -4.4285088 -4.4286461 -4.4287176 -4.4287782 -4.4288144 -4.4287977][-4.4285927 -4.4285803 -4.4285946 -4.4285707 -4.4284873 -4.4283347 -4.428153 -4.4281344 -4.4283156 -4.4285264 -4.4286528 -4.428721 -4.4287744 -4.4287925 -4.4287605][-4.4285927 -4.4285979 -4.4286404 -4.428628 -4.4285536 -4.4284163 -4.428266 -4.4282703 -4.4284315 -4.4286041 -4.4286919 -4.428731 -4.4287658 -4.42877 -4.4287329][-4.4286771 -4.4286895 -4.4287291 -4.4287186 -4.4286628 -4.4285774 -4.4284797 -4.4284792 -4.4285693 -4.428669 -4.4287243 -4.4287462 -4.4287791 -4.4287858 -4.4287596][-4.4287715 -4.428772 -4.4287872 -4.4287739 -4.42875 -4.4287238 -4.4286742 -4.428658 -4.428679 -4.4287062 -4.4287276 -4.4287333 -4.4287705 -4.4287982 -4.428792][-4.4288487 -4.4288368 -4.4288449 -4.4288316 -4.4288225 -4.4288254 -4.428813 -4.4287877 -4.4287724 -4.4287567 -4.4287438 -4.4287252 -4.4287553 -4.4288149 -4.4288363][-4.4289122 -4.4289031 -4.428915 -4.4289026 -4.4288874 -4.4288864 -4.4288821 -4.42885 -4.4288349 -4.4288063 -4.4287772 -4.4287586 -4.4287872 -4.4288578 -4.4288945][-4.42894 -4.4289346 -4.4289451 -4.4289308 -4.4289126 -4.4289064 -4.4288945 -4.4288573 -4.4288497 -4.42884 -4.4288206 -4.4288139 -4.4288487 -4.4289112 -4.4289427][-4.4289312 -4.4289217 -4.4289289 -4.4289207 -4.4289074 -4.4289012 -4.4288859 -4.4288511 -4.428853 -4.4288616 -4.4288478 -4.4288416 -4.4288731 -4.4289207 -4.4289422]]...]
INFO - root - 2017-12-10 04:19:13.085617: step 7510, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.211 sec/batch; 19h:01m:04s remains)
INFO - root - 2017-12-10 04:19:15.216800: step 7520, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.210 sec/batch; 18h:58m:51s remains)
INFO - root - 2017-12-10 04:19:17.378451: step 7530, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:36m:44s remains)
INFO - root - 2017-12-10 04:19:19.495876: step 7540, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 19h:07m:33s remains)
INFO - root - 2017-12-10 04:19:21.596628: step 7550, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:39m:37s remains)
INFO - root - 2017-12-10 04:19:23.775736: step 7560, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:29m:52s remains)
INFO - root - 2017-12-10 04:19:25.866201: step 7570, loss = 2.28, batch loss = 2.23 (38.2 examples/sec; 0.210 sec/batch; 18h:55m:25s remains)
INFO - root - 2017-12-10 04:19:27.994462: step 7580, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 19h:05m:56s remains)
INFO - root - 2017-12-10 04:19:30.151232: step 7590, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 19h:16m:05s remains)
INFO - root - 2017-12-10 04:19:32.275751: step 7600, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.210 sec/batch; 18h:59m:05s remains)
2017-12-10 04:19:32.600237: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289827 -4.4289613 -4.4289422 -4.4289293 -4.4289212 -4.4289155 -4.4289107 -4.4289136 -4.4289255 -4.428936 -4.4289412 -4.4289474 -4.4289589 -4.428968 -4.4289689][-4.4289374 -4.4288988 -4.428864 -4.4288397 -4.4288235 -4.4288087 -4.4287992 -4.4288054 -4.4288306 -4.4288568 -4.4288721 -4.4288831 -4.4289002 -4.428916 -4.4289174][-4.4288893 -4.4288354 -4.4287829 -4.4287453 -4.4287195 -4.4286909 -4.4286709 -4.4286776 -4.4287124 -4.4287558 -4.4287877 -4.428812 -4.4288383 -4.42886 -4.4288616][-4.4288473 -4.4287839 -4.4287195 -4.4286723 -4.4286385 -4.4285979 -4.4285626 -4.4285717 -4.4286079 -4.4286475 -4.4286895 -4.4287324 -4.4287767 -4.42881 -4.4288139][-4.4288173 -4.4287491 -4.428679 -4.4286232 -4.4285784 -4.4285254 -4.4284782 -4.4284911 -4.4285226 -4.4285479 -4.4285822 -4.4286318 -4.428688 -4.4287362 -4.4287515][-4.4288 -4.4287262 -4.4286494 -4.4285784 -4.42852 -4.4284558 -4.4284 -4.4284186 -4.4284563 -4.42847 -4.4284911 -4.4285283 -4.4285851 -4.4286427 -4.4286728][-4.4287968 -4.428719 -4.4286361 -4.4285531 -4.4284749 -4.4283776 -4.428298 -4.4283404 -4.4284167 -4.42845 -4.4284658 -4.428483 -4.428524 -4.4285879 -4.4286327][-4.4287968 -4.4287176 -4.4286375 -4.4285574 -4.4284663 -4.4283228 -4.4281926 -4.4282594 -4.4283924 -4.4284668 -4.42849 -4.4284964 -4.4285259 -4.4285889 -4.4286408][-4.4288087 -4.4287372 -4.4286714 -4.4286113 -4.4285378 -4.4283919 -4.4282341 -4.4282756 -4.4284134 -4.42851 -4.4285431 -4.4285522 -4.4285755 -4.4286218 -4.428658][-4.4288278 -4.4287691 -4.4287176 -4.4286823 -4.4286466 -4.4285526 -4.4284306 -4.4284344 -4.4285173 -4.4285851 -4.4286184 -4.4286404 -4.428669 -4.4286919 -4.4286942][-4.4288445 -4.4287953 -4.4287534 -4.4287343 -4.4287262 -4.4286747 -4.428597 -4.4285932 -4.4286375 -4.4286742 -4.4287014 -4.4287319 -4.4287591 -4.4287667 -4.4287424][-4.4288516 -4.4288073 -4.4287715 -4.4287615 -4.4287629 -4.4287324 -4.4286895 -4.4286985 -4.428731 -4.4287491 -4.4287677 -4.4287968 -4.4288158 -4.4288139 -4.42879][-4.4288535 -4.4288068 -4.4287715 -4.4287643 -4.4287724 -4.4287572 -4.4287457 -4.4287705 -4.4288 -4.4288077 -4.4288168 -4.4288368 -4.4288492 -4.428844 -4.4288278][-4.4288578 -4.428803 -4.4287558 -4.4287415 -4.4287519 -4.4287515 -4.4287629 -4.4288034 -4.4288368 -4.4288421 -4.4288392 -4.4288473 -4.4288568 -4.4288521 -4.4288416][-4.4288692 -4.4288096 -4.42875 -4.4287205 -4.4287214 -4.4287262 -4.4287462 -4.4287939 -4.428833 -4.4288445 -4.4288373 -4.4288359 -4.4288454 -4.4288459 -4.4288435]]...]
INFO - root - 2017-12-10 04:19:34.741766: step 7610, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 20h:04m:29s remains)
INFO - root - 2017-12-10 04:19:36.908506: step 7620, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:19m:29s remains)
INFO - root - 2017-12-10 04:19:39.059541: step 7630, loss = 2.28, batch loss = 2.23 (38.5 examples/sec; 0.208 sec/batch; 18h:44m:20s remains)
INFO - root - 2017-12-10 04:19:41.176099: step 7640, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 18h:56m:15s remains)
INFO - root - 2017-12-10 04:19:43.330089: step 7650, loss = 2.28, batch loss = 2.23 (38.6 examples/sec; 0.207 sec/batch; 18h:41m:38s remains)
INFO - root - 2017-12-10 04:19:45.483356: step 7660, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:43m:02s remains)
INFO - root - 2017-12-10 04:19:47.643485: step 7670, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:56m:26s remains)
INFO - root - 2017-12-10 04:19:49.763636: step 7680, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 18h:56m:51s remains)
INFO - root - 2017-12-10 04:19:51.906872: step 7690, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:45m:36s remains)
INFO - root - 2017-12-10 04:19:54.029894: step 7700, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 18h:55m:31s remains)
2017-12-10 04:19:54.406721: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286132 -4.4285736 -4.4286089 -4.4286819 -4.4287786 -4.4288678 -4.4289279 -4.4289484 -4.4289408 -4.4289136 -4.4288726 -4.4288368 -4.4288244 -4.428834 -4.4288459][-4.428731 -4.4287195 -4.4287486 -4.42879 -4.4288464 -4.4288974 -4.428925 -4.4289269 -4.4289141 -4.4288859 -4.4288516 -4.42883 -4.4288325 -4.4288516 -4.428864][-4.4288449 -4.4288483 -4.4288588 -4.4288597 -4.4288688 -4.4288859 -4.4288898 -4.42888 -4.428865 -4.4288397 -4.4288082 -4.4288011 -4.428823 -4.428854 -4.4288688][-4.4289007 -4.4289069 -4.4289007 -4.4288774 -4.4288526 -4.4288454 -4.4288249 -4.4288006 -4.4287825 -4.4287624 -4.4287415 -4.4287548 -4.4287972 -4.4288421 -4.4288621][-4.4289207 -4.4289246 -4.4289036 -4.4288549 -4.4288039 -4.428772 -4.4287257 -4.4286876 -4.4286733 -4.4286685 -4.4286637 -4.4286938 -4.4287553 -4.4288168 -4.4288459][-4.428936 -4.4289336 -4.4288921 -4.4288125 -4.4287319 -4.4286733 -4.4286122 -4.4285684 -4.4285603 -4.4285784 -4.4285979 -4.4286509 -4.4287348 -4.4288082 -4.4288416][-4.4289494 -4.428936 -4.4288673 -4.4287539 -4.4286432 -4.4285645 -4.4285011 -4.4284592 -4.4284687 -4.4285169 -4.4285731 -4.4286642 -4.4287605 -4.4288292 -4.4288549][-4.4289651 -4.4289336 -4.4288363 -4.4286971 -4.4285712 -4.428483 -4.4284163 -4.42839 -4.4284286 -4.4285064 -4.4286008 -4.4287133 -4.4288025 -4.4288549 -4.4288692][-4.4289837 -4.4289346 -4.4288211 -4.4286718 -4.4285393 -4.4284449 -4.4283881 -4.428391 -4.4284577 -4.4285569 -4.4286675 -4.4287806 -4.42885 -4.4288826 -4.4288836][-4.428997 -4.4289365 -4.42882 -4.4286704 -4.4285355 -4.4284511 -4.4284244 -4.4284639 -4.4285531 -4.428659 -4.4287639 -4.428854 -4.4288964 -4.4289103 -4.4288955][-4.4289918 -4.4289327 -4.4288254 -4.4286861 -4.4285688 -4.4285135 -4.4285207 -4.4285855 -4.4286814 -4.4287796 -4.4288626 -4.4289165 -4.4289346 -4.4289308 -4.428905][-4.4289727 -4.4289274 -4.4288425 -4.4287362 -4.4286633 -4.4286489 -4.4286804 -4.4287448 -4.4288182 -4.4288898 -4.4289384 -4.4289551 -4.4289536 -4.4289389 -4.4289088][-4.4289536 -4.4289303 -4.4288759 -4.4288096 -4.4287786 -4.4287896 -4.4288216 -4.4288645 -4.4289079 -4.4289455 -4.4289646 -4.4289584 -4.428946 -4.42893 -4.4289045][-4.428936 -4.4289284 -4.4289031 -4.4288683 -4.42886 -4.4288726 -4.4288955 -4.4289212 -4.4289417 -4.4289536 -4.4289532 -4.4289393 -4.4289279 -4.4289188 -4.4289045][-4.4289274 -4.4289293 -4.4289217 -4.4289069 -4.4289026 -4.4289074 -4.4289188 -4.42893 -4.4289336 -4.4289317 -4.4289246 -4.428915 -4.4289141 -4.4289174 -4.4289141]]...]
INFO - root - 2017-12-10 04:19:56.613541: step 7710, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:29m:19s remains)
INFO - root - 2017-12-10 04:19:58.780103: step 7720, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:23m:30s remains)
INFO - root - 2017-12-10 04:20:00.921898: step 7730, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:28m:43s remains)
INFO - root - 2017-12-10 04:20:03.067701: step 7740, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.212 sec/batch; 19h:10m:10s remains)
INFO - root - 2017-12-10 04:20:05.201670: step 7750, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.211 sec/batch; 19h:04m:00s remains)
INFO - root - 2017-12-10 04:20:07.341327: step 7760, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:09m:48s remains)
INFO - root - 2017-12-10 04:20:09.507957: step 7770, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:06m:33s remains)
INFO - root - 2017-12-10 04:20:11.666633: step 7780, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.219 sec/batch; 19h:47m:49s remains)
INFO - root - 2017-12-10 04:20:13.822379: step 7790, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:17m:52s remains)
INFO - root - 2017-12-10 04:20:15.977500: step 7800, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:09m:36s remains)
2017-12-10 04:20:16.488237: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287782 -4.4287758 -4.4287739 -4.4287643 -4.4287515 -4.4287491 -4.4287534 -4.4287586 -4.4287639 -4.4287691 -4.4287815 -4.4288116 -4.428843 -4.4288645 -4.4288712][-4.4287024 -4.4287043 -4.4287119 -4.4287038 -4.4286871 -4.4286728 -4.4286671 -4.4286747 -4.4286919 -4.4287086 -4.4287276 -4.4287515 -4.4287782 -4.4288054 -4.4288192][-4.428678 -4.4286728 -4.4286728 -4.42866 -4.4286308 -4.428597 -4.4285727 -4.4285822 -4.4286184 -4.4286547 -4.4286904 -4.4287162 -4.4287329 -4.4287562 -4.4287777][-4.4287076 -4.4286871 -4.42868 -4.4286652 -4.4286275 -4.4285674 -4.4285135 -4.4285297 -4.4285922 -4.4286423 -4.4286842 -4.4287114 -4.4287238 -4.4287472 -4.4287653][-4.4287148 -4.4286823 -4.4286675 -4.4286456 -4.4285989 -4.4285059 -4.4284029 -4.4284134 -4.4285188 -4.4286041 -4.4286695 -4.4287119 -4.42873 -4.4287553 -4.4287724][-4.4286828 -4.4286437 -4.4286141 -4.4285755 -4.4285011 -4.4283605 -4.4281678 -4.4281535 -4.4283442 -4.4284997 -4.4286084 -4.4286728 -4.4287043 -4.4287376 -4.4287562][-4.4286852 -4.4286418 -4.4285917 -4.4285254 -4.4284191 -4.4282284 -4.4279456 -4.4278984 -4.4281816 -4.4284129 -4.4285541 -4.4286304 -4.4286761 -4.4287148 -4.4287333][-4.4287486 -4.42872 -4.4286723 -4.4286141 -4.4285216 -4.4283762 -4.4281626 -4.4281135 -4.428328 -4.428525 -4.4286404 -4.4287019 -4.4287376 -4.4287596 -4.4287677][-4.4288158 -4.4288 -4.4287682 -4.4287343 -4.4286633 -4.4285674 -4.4284434 -4.4284005 -4.42853 -4.4286704 -4.4287553 -4.4287939 -4.42881 -4.4288182 -4.428822][-4.4288616 -4.4288483 -4.4288259 -4.4288044 -4.4287395 -4.4286537 -4.4285712 -4.4285359 -4.4286141 -4.4287224 -4.4287992 -4.4288344 -4.4288459 -4.428853 -4.4288568][-4.4288774 -4.428874 -4.4288588 -4.4288349 -4.4287724 -4.4286962 -4.4286351 -4.42861 -4.4286594 -4.4287429 -4.4288044 -4.4288335 -4.428843 -4.4288483 -4.4288516][-4.4288669 -4.4288783 -4.4288783 -4.4288588 -4.4288082 -4.4287529 -4.428719 -4.4287138 -4.4287462 -4.4287953 -4.4288316 -4.4288473 -4.4288511 -4.42885 -4.4288492][-4.4288478 -4.4288716 -4.4288912 -4.4288836 -4.4288507 -4.4288168 -4.4288058 -4.4288163 -4.4288363 -4.4288588 -4.4288759 -4.4288826 -4.4288831 -4.4288807 -4.4288774][-4.4288406 -4.4288692 -4.4289007 -4.4289083 -4.4288907 -4.428875 -4.428875 -4.428885 -4.428896 -4.4289064 -4.4289136 -4.4289155 -4.4289131 -4.4289112 -4.4289093][-4.4288535 -4.4288807 -4.4289141 -4.4289308 -4.428925 -4.4289184 -4.4289207 -4.4289279 -4.4289303 -4.4289322 -4.4289351 -4.4289355 -4.4289341 -4.4289336 -4.4289336]]...]
INFO - root - 2017-12-10 04:20:18.653954: step 7810, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:49m:55s remains)
INFO - root - 2017-12-10 04:20:20.788210: step 7820, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:32m:16s remains)
INFO - root - 2017-12-10 04:20:22.949704: step 7830, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:34m:55s remains)
INFO - root - 2017-12-10 04:20:25.143971: step 7840, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.214 sec/batch; 19h:15m:52s remains)
INFO - root - 2017-12-10 04:20:27.316107: step 7850, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:34m:10s remains)
INFO - root - 2017-12-10 04:20:29.466320: step 7860, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:10m:36s remains)
INFO - root - 2017-12-10 04:20:31.619719: step 7870, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:48m:50s remains)
INFO - root - 2017-12-10 04:20:33.783717: step 7880, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 19h:05m:34s remains)
INFO - root - 2017-12-10 04:20:35.937756: step 7890, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:40m:23s remains)
INFO - root - 2017-12-10 04:20:38.121212: step 7900, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:49m:18s remains)
2017-12-10 04:20:38.483462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288058 -4.428791 -4.4287949 -4.42881 -4.4288197 -4.4288187 -4.4288106 -4.4288011 -4.4288011 -4.4288177 -4.4288459 -4.4288731 -4.4288931 -4.4289017 -4.4289041][-4.4287262 -4.4287066 -4.4287186 -4.4287443 -4.4287586 -4.4287515 -4.4287353 -4.428721 -4.4287205 -4.4287434 -4.428781 -4.4288173 -4.428843 -4.4288545 -4.4288568][-4.4286838 -4.4286661 -4.4286847 -4.4287143 -4.4287271 -4.4287114 -4.4286909 -4.4286733 -4.4286728 -4.428699 -4.4287419 -4.4287839 -4.4288135 -4.4288273 -4.4288306][-4.4286833 -4.4286671 -4.4286871 -4.4287105 -4.4287176 -4.4286942 -4.4286709 -4.4286509 -4.4286461 -4.4286714 -4.4287171 -4.4287643 -4.4287953 -4.4288096 -4.4288163][-4.4286962 -4.4286871 -4.4287086 -4.4287248 -4.428721 -4.4286866 -4.4286542 -4.4286313 -4.4286265 -4.4286518 -4.428699 -4.428751 -4.4287834 -4.4288011 -4.428813][-4.4287124 -4.4287133 -4.4287338 -4.4287386 -4.4287195 -4.4286776 -4.4286413 -4.428618 -4.4286156 -4.4286456 -4.4286952 -4.4287477 -4.4287777 -4.4287958 -4.428813][-4.4287238 -4.4287233 -4.4287395 -4.428731 -4.4287028 -4.4286661 -4.4286346 -4.428617 -4.4286227 -4.4286585 -4.42871 -4.4287591 -4.4287853 -4.4287982 -4.428813][-4.4287157 -4.4287047 -4.4287186 -4.428709 -4.4286823 -4.4286604 -4.428647 -4.4286485 -4.4286613 -4.428689 -4.4287291 -4.4287643 -4.4287753 -4.4287767 -4.4287887][-4.4287009 -4.4286809 -4.4286981 -4.4287004 -4.42869 -4.4286923 -4.4287028 -4.4287171 -4.4287257 -4.4287329 -4.4287481 -4.4287596 -4.4287434 -4.4287257 -4.4287338][-4.4287004 -4.4286761 -4.4286981 -4.4287162 -4.4287224 -4.42874 -4.428762 -4.4287791 -4.4287791 -4.4287658 -4.4287548 -4.4287434 -4.4287047 -4.4286718 -4.4286819][-4.4287014 -4.4286766 -4.4287047 -4.4287376 -4.4287572 -4.4287839 -4.4288106 -4.4288244 -4.4288144 -4.4287829 -4.4287534 -4.42873 -4.4286852 -4.4286485 -4.4286647][-4.4287043 -4.4286857 -4.4287195 -4.4287643 -4.4287958 -4.4288273 -4.4288497 -4.4288497 -4.4288235 -4.4287806 -4.4287457 -4.4287305 -4.4287014 -4.4286733 -4.4286942][-4.4287152 -4.4287081 -4.4287472 -4.4287982 -4.4288383 -4.4288692 -4.42888 -4.4288611 -4.4288239 -4.428781 -4.4287615 -4.4287648 -4.4287572 -4.4287462 -4.4287653][-4.4287395 -4.4287477 -4.4287896 -4.4288359 -4.4288726 -4.4288993 -4.4289041 -4.4288812 -4.428843 -4.4288087 -4.4288039 -4.4288177 -4.428823 -4.4288206 -4.4288378][-4.428793 -4.428812 -4.4288507 -4.4288836 -4.4289055 -4.4289203 -4.4289193 -4.4288979 -4.4288635 -4.4288411 -4.4288468 -4.4288654 -4.428874 -4.4288788 -4.4288964]]...]
INFO - root - 2017-12-10 04:20:40.622308: step 7910, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:31m:30s remains)
INFO - root - 2017-12-10 04:20:42.744292: step 7920, loss = 2.28, batch loss = 2.23 (38.5 examples/sec; 0.208 sec/batch; 18h:43m:02s remains)
INFO - root - 2017-12-10 04:20:44.872428: step 7930, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:29m:04s remains)
INFO - root - 2017-12-10 04:20:47.031924: step 7940, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:37m:16s remains)
INFO - root - 2017-12-10 04:20:49.253573: step 7950, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:27m:10s remains)
INFO - root - 2017-12-10 04:20:51.376517: step 7960, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:38m:41s remains)
INFO - root - 2017-12-10 04:20:53.517475: step 7970, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:23m:12s remains)
INFO - root - 2017-12-10 04:20:55.605350: step 7980, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 19h:05m:21s remains)
INFO - root - 2017-12-10 04:20:57.788436: step 7990, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:24m:34s remains)
INFO - root - 2017-12-10 04:20:59.931776: step 8000, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:32m:20s remains)
2017-12-10 04:21:00.294087: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287543 -4.4287357 -4.4287481 -4.428782 -4.4288058 -4.4288082 -4.4288034 -4.4288049 -4.428812 -4.4288244 -4.428834 -4.4288344 -4.428834 -4.4288306 -4.4288254][-4.4287448 -4.4287205 -4.4287329 -4.42876 -4.428771 -4.428762 -4.4287543 -4.4287586 -4.4287763 -4.4287987 -4.4288216 -4.428833 -4.4288354 -4.4288306 -4.428822][-4.4287357 -4.4287095 -4.4287119 -4.428719 -4.4287028 -4.428668 -4.4286451 -4.4286509 -4.4286876 -4.4287348 -4.428781 -4.428813 -4.42883 -4.4288363 -4.4288354][-4.428731 -4.4287105 -4.4287024 -4.4286866 -4.4286456 -4.4285817 -4.428535 -4.4285378 -4.4285975 -4.4286742 -4.4287381 -4.4287825 -4.42881 -4.4288259 -4.428834][-4.4287353 -4.4287195 -4.4287047 -4.4286752 -4.428616 -4.4285207 -4.4284363 -4.4284286 -4.4285178 -4.4286275 -4.4287043 -4.4287529 -4.4287882 -4.4288082 -4.4288163][-4.4287448 -4.4287276 -4.4287004 -4.4286494 -4.4285636 -4.4284229 -4.4282789 -4.4282541 -4.4283876 -4.4285436 -4.4286427 -4.4287095 -4.4287667 -4.4287982 -4.42881][-4.4287591 -4.4287362 -4.4286909 -4.4286146 -4.4284992 -4.4283257 -4.4281373 -4.4281011 -4.4282708 -4.4284558 -4.42857 -4.42866 -4.4287453 -4.4287953 -4.4288182][-4.4287724 -4.4287443 -4.4286904 -4.4286089 -4.4285026 -4.4283619 -4.4282146 -4.428185 -4.428309 -4.4284492 -4.4285359 -4.42862 -4.4287119 -4.4287686 -4.4288034][-4.4287786 -4.4287496 -4.4286947 -4.4286232 -4.4285479 -4.4284687 -4.4283886 -4.4283667 -4.4284229 -4.4284949 -4.42854 -4.4285965 -4.4286695 -4.4287195 -4.4287581][-4.4287643 -4.4287305 -4.4286685 -4.428596 -4.4285412 -4.4285131 -4.4284921 -4.4284897 -4.428515 -4.4285455 -4.428565 -4.428597 -4.4286466 -4.4286919 -4.4287333][-4.4287491 -4.4287062 -4.4286332 -4.4285555 -4.4285188 -4.4285388 -4.4285674 -4.4285831 -4.4285913 -4.4285975 -4.4285979 -4.4286046 -4.4286366 -4.4286795 -4.4287205][-4.428762 -4.4287138 -4.4286389 -4.4285684 -4.4285512 -4.4286022 -4.428648 -4.4286556 -4.4286432 -4.4286289 -4.42862 -4.4286184 -4.42865 -4.4286885 -4.4287195][-4.428802 -4.4287558 -4.4286847 -4.4286232 -4.4286242 -4.4286861 -4.4287229 -4.4287133 -4.4286857 -4.4286604 -4.4286494 -4.4286571 -4.4287004 -4.4287357 -4.4287496][-4.4288435 -4.4288082 -4.4287505 -4.4287038 -4.4287195 -4.4287815 -4.4288 -4.4287715 -4.4287286 -4.4286947 -4.4286923 -4.4287157 -4.4287686 -4.4287934 -4.4287882][-4.4288721 -4.428853 -4.4288177 -4.4287906 -4.4288139 -4.428863 -4.4288611 -4.42882 -4.4287682 -4.4287333 -4.4287448 -4.4287858 -4.4288349 -4.42884 -4.4288192]]...]
INFO - root - 2017-12-10 04:21:02.462858: step 8010, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:47m:25s remains)
INFO - root - 2017-12-10 04:21:04.617954: step 8020, loss = 2.28, batch loss = 2.23 (38.5 examples/sec; 0.208 sec/batch; 18h:45m:09s remains)
INFO - root - 2017-12-10 04:21:06.744654: step 8030, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:40m:53s remains)
INFO - root - 2017-12-10 04:21:08.911163: step 8040, loss = 2.28, batch loss = 2.23 (38.4 examples/sec; 0.208 sec/batch; 18h:47m:20s remains)
INFO - root - 2017-12-10 04:21:11.021725: step 8050, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:31m:36s remains)
INFO - root - 2017-12-10 04:21:13.173985: step 8060, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:33m:59s remains)
INFO - root - 2017-12-10 04:21:15.369812: step 8070, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:41m:18s remains)
INFO - root - 2017-12-10 04:21:17.471899: step 8080, loss = 2.28, batch loss = 2.23 (39.5 examples/sec; 0.203 sec/batch; 18h:15m:33s remains)
INFO - root - 2017-12-10 04:21:19.595378: step 8090, loss = 2.28, batch loss = 2.23 (38.2 examples/sec; 0.210 sec/batch; 18h:53m:42s remains)
INFO - root - 2017-12-10 04:21:21.750126: step 8100, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:00m:07s remains)
2017-12-10 04:21:22.066481: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288192 -4.4287653 -4.4287391 -4.4287486 -4.4287953 -4.4288325 -4.4288387 -4.4288173 -4.4288077 -4.4287968 -4.4287877 -4.4288039 -4.4288354 -4.4288716 -4.428895][-4.4288063 -4.4287472 -4.4287148 -4.4287162 -4.4287572 -4.4287891 -4.4287891 -4.4287715 -4.4287782 -4.4287868 -4.4287887 -4.428802 -4.4288225 -4.4288516 -4.4288735][-4.4288034 -4.4287472 -4.42871 -4.4287004 -4.4287291 -4.4287543 -4.4287534 -4.4287472 -4.4287748 -4.4288015 -4.4288139 -4.4288235 -4.4288273 -4.4288416 -4.4288597][-4.4288 -4.4287577 -4.4287233 -4.428699 -4.428709 -4.4287219 -4.428721 -4.4287319 -4.4287825 -4.4288249 -4.4288526 -4.428863 -4.4288516 -4.4288478 -4.4288573][-4.4287953 -4.4287705 -4.4287429 -4.4287052 -4.42869 -4.4286771 -4.4286633 -4.4286871 -4.4287629 -4.4288259 -4.4288707 -4.4288878 -4.4288692 -4.4288535 -4.4288583][-4.4287724 -4.4287663 -4.4287491 -4.4287076 -4.4286728 -4.4286289 -4.4285841 -4.4286027 -4.4287024 -4.4287953 -4.4288583 -4.4288836 -4.4288664 -4.4288487 -4.4288554][-4.4287567 -4.4287543 -4.4287395 -4.4287043 -4.4286618 -4.4285913 -4.4284997 -4.4284883 -4.4286075 -4.4287395 -4.4288254 -4.42886 -4.42885 -4.4288411 -4.4288559][-4.428772 -4.4287553 -4.4287291 -4.4286938 -4.4286509 -4.4285688 -4.4284453 -4.4283981 -4.4285245 -4.4286852 -4.4287868 -4.4288297 -4.4288316 -4.4288397 -4.4288659][-4.4288125 -4.4287715 -4.4287252 -4.4286866 -4.4286566 -4.4285955 -4.4284935 -4.4284415 -4.4285426 -4.4286861 -4.4287734 -4.428812 -4.4288249 -4.4288511 -4.4288864][-4.4288645 -4.4288077 -4.4287472 -4.4287052 -4.4286933 -4.428668 -4.4286141 -4.4285855 -4.4286485 -4.4287405 -4.4287944 -4.428822 -4.4288411 -4.4288745 -4.4289103][-4.4289021 -4.4288425 -4.42878 -4.4287386 -4.4287405 -4.4287448 -4.4287305 -4.4287281 -4.4287663 -4.4288111 -4.4288321 -4.4288483 -4.4288664 -4.4289 -4.4289289][-4.4289155 -4.4288611 -4.4288054 -4.4287672 -4.4287777 -4.4288049 -4.4288192 -4.428833 -4.42885 -4.4288611 -4.4288607 -4.4288678 -4.4288831 -4.4289117 -4.4289341][-4.4289117 -4.4288726 -4.4288287 -4.4287987 -4.4288144 -4.4288535 -4.4288812 -4.4288969 -4.4289002 -4.4288907 -4.42888 -4.4288812 -4.4288931 -4.4289179 -4.4289331][-4.4289131 -4.4288907 -4.4288621 -4.4288373 -4.4288454 -4.4288821 -4.4289107 -4.4289241 -4.4289222 -4.4289074 -4.4288936 -4.4288936 -4.4289079 -4.4289312 -4.4289379][-4.4289274 -4.42892 -4.4289079 -4.4288864 -4.4288826 -4.4289079 -4.4289308 -4.4289389 -4.4289346 -4.4289207 -4.4289055 -4.4289045 -4.4289174 -4.4289346 -4.4289346]]...]
INFO - root - 2017-12-10 04:21:24.252067: step 8110, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.211 sec/batch; 19h:02m:44s remains)
INFO - root - 2017-12-10 04:21:26.377517: step 8120, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:17m:54s remains)
INFO - root - 2017-12-10 04:21:28.550841: step 8130, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 18h:59m:50s remains)
INFO - root - 2017-12-10 04:21:30.692474: step 8140, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:39m:25s remains)
INFO - root - 2017-12-10 04:21:32.841338: step 8150, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:26m:40s remains)
INFO - root - 2017-12-10 04:21:35.018918: step 8160, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:49m:18s remains)
INFO - root - 2017-12-10 04:21:37.128910: step 8170, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:07m:36s remains)
INFO - root - 2017-12-10 04:21:39.247978: step 8180, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:14m:41s remains)
INFO - root - 2017-12-10 04:21:41.477086: step 8190, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:48m:26s remains)
INFO - root - 2017-12-10 04:21:43.611357: step 8200, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:11m:52s remains)
2017-12-10 04:21:43.944992: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288406 -4.4288073 -4.428803 -4.4288116 -4.428812 -4.428803 -4.4288044 -4.4288139 -4.4288411 -4.4288559 -4.4288416 -4.4288149 -4.4288054 -4.4288063 -4.4288177][-4.4288216 -4.4287715 -4.4287577 -4.4287615 -4.4287586 -4.4287491 -4.428741 -4.4287419 -4.4287815 -4.4288149 -4.428802 -4.4287677 -4.4287486 -4.4287481 -4.4287624][-4.4288282 -4.4287729 -4.42875 -4.4287438 -4.4287276 -4.4287076 -4.428688 -4.4286728 -4.4287238 -4.4287748 -4.42877 -4.4287353 -4.4287066 -4.4287066 -4.4287276][-4.4288354 -4.428782 -4.4287567 -4.4287395 -4.4287019 -4.428669 -4.4286385 -4.4286103 -4.4286633 -4.4287276 -4.428741 -4.4287171 -4.4286814 -4.4286714 -4.4286928][-4.4288278 -4.4287772 -4.4287548 -4.4287286 -4.4286795 -4.4286342 -4.4285922 -4.4285641 -4.4286141 -4.4286852 -4.4287195 -4.4287186 -4.4286861 -4.4286637 -4.4286819][-4.4288011 -4.4287462 -4.4287248 -4.4286947 -4.4286404 -4.4285765 -4.4284987 -4.4284658 -4.4285264 -4.4286203 -4.428688 -4.4287276 -4.4287086 -4.4286842 -4.4286981][-4.428751 -4.4286771 -4.4286561 -4.428638 -4.4285884 -4.4285016 -4.4283867 -4.42833 -4.4284019 -4.4285283 -4.4286437 -4.4287233 -4.428721 -4.4286966 -4.4287062][-4.4286923 -4.4286075 -4.4286051 -4.4286251 -4.4286056 -4.4285259 -4.4283977 -4.4283156 -4.4283657 -4.4285011 -4.4286413 -4.4287353 -4.4287391 -4.4287109 -4.4287148][-4.428668 -4.4285951 -4.4286184 -4.4286695 -4.428678 -4.4286184 -4.4285069 -4.4284229 -4.4284492 -4.4285684 -4.4286995 -4.4287825 -4.4287877 -4.4287572 -4.4287481][-4.4287138 -4.428659 -4.4286942 -4.4287515 -4.428772 -4.4287362 -4.4286437 -4.428565 -4.4285722 -4.4286609 -4.4287715 -4.4288354 -4.4288487 -4.4288282 -4.4288096][-4.4288011 -4.428761 -4.4287877 -4.4288259 -4.4288487 -4.4288306 -4.4287543 -4.4286852 -4.4286823 -4.4287405 -4.4288235 -4.4288683 -4.4288831 -4.4288764 -4.4288645][-4.4288726 -4.4288421 -4.4288592 -4.4288826 -4.4288993 -4.428884 -4.4288273 -4.4287758 -4.4287715 -4.428802 -4.4288588 -4.428894 -4.4289093 -4.42891 -4.428905][-4.4289107 -4.428894 -4.4289079 -4.4289184 -4.4289265 -4.4289145 -4.4288716 -4.4288363 -4.4288359 -4.4288511 -4.4288912 -4.4289236 -4.4289393 -4.4289412 -4.4289379][-4.4289494 -4.42894 -4.428947 -4.428947 -4.4289479 -4.4289365 -4.428906 -4.4288893 -4.4289007 -4.4289174 -4.4289455 -4.428966 -4.4289746 -4.4289727 -4.4289656][-4.4289708 -4.4289637 -4.428968 -4.4289651 -4.4289622 -4.4289522 -4.4289384 -4.4289351 -4.4289441 -4.4289565 -4.4289756 -4.428988 -4.4289923 -4.4289885 -4.4289804]]...]
INFO - root - 2017-12-10 04:21:46.091972: step 8210, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 19h:12m:35s remains)
INFO - root - 2017-12-10 04:21:48.206937: step 8220, loss = 2.28, batch loss = 2.23 (39.3 examples/sec; 0.204 sec/batch; 18h:21m:12s remains)
INFO - root - 2017-12-10 04:21:50.331751: step 8230, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.210 sec/batch; 18h:57m:22s remains)
INFO - root - 2017-12-10 04:21:52.488626: step 8240, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 20h:18m:43s remains)
INFO - root - 2017-12-10 04:21:54.629245: step 8250, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:48m:13s remains)
INFO - root - 2017-12-10 04:21:56.799781: step 8260, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:00m:29s remains)
INFO - root - 2017-12-10 04:21:58.972126: step 8270, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:20m:55s remains)
INFO - root - 2017-12-10 04:22:01.070791: step 8280, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.210 sec/batch; 18h:57m:20s remains)
INFO - root - 2017-12-10 04:22:03.274530: step 8290, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:09m:03s remains)
INFO - root - 2017-12-10 04:22:05.420178: step 8300, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:21m:05s remains)
2017-12-10 04:22:05.813423: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.429038 -4.429029 -4.4290195 -4.4290133 -4.4290142 -4.429019 -4.4290223 -4.4290214 -4.42902 -4.4290185 -4.4290166 -4.4290156 -4.4290161 -4.4290161 -4.4290185][-4.4290357 -4.4290261 -4.4290133 -4.4290028 -4.4290013 -4.4290047 -4.4290075 -4.4290066 -4.4290056 -4.4290066 -4.4290042 -4.4289994 -4.4289947 -4.4289885 -4.4289865][-4.4290323 -4.429019 -4.429 -4.4289832 -4.4289756 -4.4289722 -4.4289694 -4.4289675 -4.4289665 -4.4289722 -4.4289722 -4.4289637 -4.4289527 -4.4289389 -4.4289331][-4.4290137 -4.4289918 -4.4289651 -4.4289451 -4.4289293 -4.4289107 -4.4288936 -4.4288826 -4.4288797 -4.4288921 -4.4289 -4.4288907 -4.4288754 -4.4288559 -4.42885][-4.4289727 -4.4289379 -4.4289041 -4.4288764 -4.4288478 -4.4288125 -4.4287734 -4.4287443 -4.4287386 -4.4287624 -4.4287858 -4.4287858 -4.4287724 -4.4287572 -4.4287591][-4.4289112 -4.4288573 -4.428812 -4.4287763 -4.4287376 -4.4286904 -4.428628 -4.428576 -4.4285722 -4.4286237 -4.4286704 -4.4286838 -4.4286804 -4.4286723 -4.4286852][-4.4288583 -4.4287872 -4.4287367 -4.428699 -4.4286523 -4.4285879 -4.428493 -4.4284043 -4.42841 -4.4285107 -4.4285908 -4.4286184 -4.4286251 -4.4286237 -4.428647][-4.4288363 -4.4287567 -4.4287 -4.4286571 -4.42859 -4.4284949 -4.4283609 -4.4282331 -4.4282608 -4.4284239 -4.428546 -4.4285984 -4.428616 -4.4286146 -4.4286389][-4.4288478 -4.42877 -4.428709 -4.4286518 -4.4285631 -4.4284439 -4.4283013 -4.4281793 -4.4282346 -4.4284263 -4.4285731 -4.4286437 -4.4286637 -4.4286556 -4.4286637][-4.4288807 -4.4288125 -4.4287462 -4.4286771 -4.4285846 -4.4284787 -4.4283729 -4.428299 -4.4283576 -4.42852 -4.4286537 -4.4287267 -4.4287496 -4.428741 -4.4287305][-4.4289036 -4.4288473 -4.4287825 -4.4287138 -4.4286375 -4.4285622 -4.4284964 -4.4284573 -4.4284945 -4.4286 -4.4287043 -4.4287767 -4.42881 -4.4288068 -4.4287844][-4.4289231 -4.4288788 -4.4288287 -4.4287777 -4.4287276 -4.4286833 -4.4286475 -4.4286256 -4.4286289 -4.4286795 -4.428751 -4.4288187 -4.4288573 -4.4288626 -4.4288378][-4.4289508 -4.4289236 -4.4288955 -4.4288688 -4.4288464 -4.4288249 -4.4288058 -4.428793 -4.4287796 -4.4287925 -4.4288311 -4.4288807 -4.4289169 -4.428926 -4.4289045][-4.4289775 -4.4289646 -4.4289532 -4.428946 -4.4289427 -4.428937 -4.4289303 -4.4289255 -4.42891 -4.4289021 -4.4289145 -4.4289417 -4.4289665 -4.4289751 -4.4289613][-4.429 -4.4289908 -4.4289861 -4.4289885 -4.4289937 -4.4289951 -4.4289942 -4.4289942 -4.4289846 -4.4289737 -4.4289732 -4.4289842 -4.4289985 -4.4290066 -4.4290018]]...]
INFO - root - 2017-12-10 04:22:07.988995: step 8310, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 19h:12m:41s remains)
INFO - root - 2017-12-10 04:22:10.184968: step 8320, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 19h:18m:16s remains)
INFO - root - 2017-12-10 04:22:12.282357: step 8330, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:15m:12s remains)
INFO - root - 2017-12-10 04:22:14.430480: step 8340, loss = 2.28, batch loss = 2.23 (38.4 examples/sec; 0.208 sec/batch; 18h:46m:19s remains)
INFO - root - 2017-12-10 04:22:16.575796: step 8350, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:25m:09s remains)
INFO - root - 2017-12-10 04:22:18.742127: step 8360, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 18h:59m:56s remains)
INFO - root - 2017-12-10 04:22:20.931846: step 8370, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:57m:42s remains)
INFO - root - 2017-12-10 04:22:23.057174: step 8380, loss = 2.28, batch loss = 2.23 (38.8 examples/sec; 0.206 sec/batch; 18h:34m:27s remains)
INFO - root - 2017-12-10 04:22:25.164232: step 8390, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:09m:53s remains)
INFO - root - 2017-12-10 04:22:27.308226: step 8400, loss = 2.28, batch loss = 2.23 (38.7 examples/sec; 0.207 sec/batch; 18h:37m:45s remains)
2017-12-10 04:22:27.657799: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289651 -4.4289556 -4.4289346 -4.428906 -4.4288754 -4.428844 -4.4288034 -4.4287658 -4.428761 -4.4287915 -4.42883 -4.4288597 -4.4288836 -4.4288979 -4.4288883][-4.4289637 -4.4289517 -4.4289236 -4.4288797 -4.42883 -4.4287767 -4.4287076 -4.4286566 -4.4286685 -4.4287224 -4.428781 -4.4288187 -4.4288492 -4.4288726 -4.4288673][-4.4289665 -4.4289527 -4.428916 -4.4288554 -4.4287834 -4.4287057 -4.4286103 -4.4285564 -4.42859 -4.4286661 -4.4287477 -4.4287944 -4.4288316 -4.4288611 -4.42886][-4.4289718 -4.4289513 -4.4288979 -4.4288158 -4.4287167 -4.4286146 -4.4285078 -4.4284673 -4.4285235 -4.4286184 -4.4287162 -4.4287691 -4.428813 -4.428854 -4.4288697][-4.428978 -4.4289513 -4.4288812 -4.4287753 -4.4286489 -4.4285316 -4.4284244 -4.4284024 -4.4284716 -4.4285669 -4.4286642 -4.4287205 -4.4287682 -4.4288168 -4.428854][-4.4289823 -4.4289532 -4.4288716 -4.4287486 -4.4286051 -4.4284811 -4.4283795 -4.4283729 -4.4284573 -4.4285469 -4.4286289 -4.4286747 -4.4287162 -4.4287567 -4.428802][-4.4289827 -4.4289522 -4.4288607 -4.4287262 -4.4285769 -4.4284587 -4.4283671 -4.4283838 -4.4284778 -4.4285545 -4.4286151 -4.4286404 -4.4286709 -4.4286985 -4.4287367][-4.4289804 -4.4289436 -4.4288416 -4.4286985 -4.4285574 -4.4284635 -4.4283886 -4.4284172 -4.4285126 -4.4285712 -4.4286137 -4.4286208 -4.4286375 -4.4286571 -4.4286838][-4.4289808 -4.4289408 -4.4288387 -4.4287052 -4.4285884 -4.428514 -4.4284444 -4.4284673 -4.4285517 -4.4285951 -4.4286304 -4.4286361 -4.4286504 -4.4286642 -4.4286814][-4.4289865 -4.4289503 -4.4288554 -4.42874 -4.4286442 -4.4285717 -4.4284911 -4.4284997 -4.4285779 -4.428617 -4.4286461 -4.4286571 -4.4286714 -4.428679 -4.4286914][-4.42899 -4.4289579 -4.4288692 -4.4287615 -4.4286761 -4.4285994 -4.4285097 -4.4285064 -4.4285855 -4.428627 -4.4286528 -4.428659 -4.4286675 -4.428679 -4.4286928][-4.4289956 -4.428968 -4.4288855 -4.4287829 -4.4287004 -4.4286251 -4.4285359 -4.4285169 -4.4285979 -4.4286413 -4.4286642 -4.4286704 -4.4286776 -4.4286976 -4.4287205][-4.4290004 -4.428977 -4.4289031 -4.4288135 -4.4287357 -4.4286661 -4.4285865 -4.4285555 -4.4286337 -4.4286809 -4.4286971 -4.4287043 -4.4286981 -4.4287138 -4.4287457][-4.4290023 -4.4289827 -4.4289231 -4.4288487 -4.4287753 -4.4287114 -4.4286523 -4.4286246 -4.42869 -4.4287367 -4.428751 -4.4287453 -4.4287257 -4.42874 -4.4287782][-4.4290009 -4.4289842 -4.4289384 -4.4288831 -4.4288239 -4.42877 -4.4287176 -4.4286919 -4.4287472 -4.428792 -4.4288111 -4.4287963 -4.4287791 -4.4288044 -4.4288497]]...]
INFO - root - 2017-12-10 04:22:29.785938: step 8410, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:34m:49s remains)
INFO - root - 2017-12-10 04:22:31.925171: step 8420, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:16m:32s remains)
INFO - root - 2017-12-10 04:22:34.088040: step 8430, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:24m:17s remains)
INFO - root - 2017-12-10 04:22:36.423807: step 8440, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:20m:37s remains)
INFO - root - 2017-12-10 04:22:40.075602: step 8450, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:26m:13s remains)
INFO - root - 2017-12-10 04:22:43.227189: step 8460, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.428 sec/batch; 38h:33m:43s remains)
INFO - root - 2017-12-10 04:22:47.530196: step 8470, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.430 sec/batch; 38h:43m:59s remains)
INFO - root - 2017-12-10 04:22:51.776011: step 8480, loss = 2.28, batch loss = 2.23 (18.9 examples/sec; 0.422 sec/batch; 38h:01m:03s remains)
INFO - root - 2017-12-10 04:22:56.006033: step 8490, loss = 2.28, batch loss = 2.23 (18.9 examples/sec; 0.422 sec/batch; 38h:00m:42s remains)
INFO - root - 2017-12-10 04:23:00.345392: step 8500, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.438 sec/batch; 39h:23m:22s remains)
2017-12-10 04:23:00.829971: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289494 -4.4289484 -4.4289455 -4.4289441 -4.4289455 -4.4289503 -4.4289541 -4.4289546 -4.4289517 -4.4289508 -4.4289513 -4.4289517 -4.4289532 -4.4289546 -4.4289532][-4.4289427 -4.4289455 -4.4289436 -4.4289422 -4.4289441 -4.428947 -4.4289465 -4.4289432 -4.4289384 -4.4289346 -4.4289336 -4.4289341 -4.428936 -4.4289384 -4.4289379][-4.42894 -4.4289417 -4.4289384 -4.4289355 -4.4289374 -4.42894 -4.428937 -4.4289322 -4.4289269 -4.4289246 -4.428926 -4.42893 -4.4289346 -4.4289389 -4.428937][-4.4289303 -4.4289269 -4.4289165 -4.4289079 -4.4289074 -4.4289093 -4.428906 -4.4289017 -4.4289 -4.4289036 -4.4289117 -4.4289217 -4.4289312 -4.4289351 -4.4289312][-4.4289193 -4.42891 -4.4288883 -4.4288673 -4.4288568 -4.4288511 -4.4288387 -4.4288287 -4.4288297 -4.4288445 -4.4288669 -4.4288893 -4.4289055 -4.4289083 -4.4289][-4.4289002 -4.4288845 -4.4288487 -4.4288087 -4.42878 -4.428761 -4.4287348 -4.4287143 -4.4287148 -4.4287424 -4.4287853 -4.4288254 -4.42885 -4.4288516 -4.4288392][-4.4288845 -4.4288678 -4.4288158 -4.42875 -4.4286933 -4.4286475 -4.4285955 -4.4285574 -4.4285531 -4.4285941 -4.4286613 -4.4287214 -4.4287558 -4.4287586 -4.4287462][-4.428874 -4.4288683 -4.4288177 -4.4287429 -4.4286704 -4.4286079 -4.4285364 -4.4284816 -4.4284654 -4.4285016 -4.4285712 -4.4286318 -4.4286647 -4.4286675 -4.4286585][-4.4288344 -4.4288416 -4.4288025 -4.42874 -4.4286823 -4.4286404 -4.4285932 -4.4285607 -4.4285541 -4.4285822 -4.4286332 -4.4286766 -4.4286985 -4.4287009 -4.4286957][-4.4288225 -4.4288359 -4.42881 -4.4287667 -4.4287305 -4.4287119 -4.4286952 -4.4286909 -4.4286985 -4.4287229 -4.4287567 -4.4287848 -4.4287992 -4.4288025 -4.4288011][-4.428863 -4.4288764 -4.4288654 -4.4288397 -4.4288187 -4.4288116 -4.4288092 -4.4288154 -4.4288249 -4.4288435 -4.4288645 -4.4288821 -4.4288931 -4.4288974 -4.4288979][-4.4289117 -4.4289203 -4.4289174 -4.4289031 -4.4288912 -4.4288888 -4.4288917 -4.4288988 -4.4289074 -4.4289203 -4.4289317 -4.4289412 -4.4289489 -4.4289536 -4.4289551][-4.4289465 -4.4289489 -4.428946 -4.4289374 -4.4289284 -4.4289236 -4.4289236 -4.4289274 -4.4289327 -4.4289393 -4.4289432 -4.4289474 -4.4289522 -4.4289565 -4.4289589][-4.42896 -4.4289565 -4.4289503 -4.4289427 -4.4289346 -4.4289284 -4.428926 -4.4289269 -4.4289293 -4.4289312 -4.4289303 -4.42893 -4.4289303 -4.4289322 -4.4289346][-4.428946 -4.4289412 -4.4289351 -4.4289317 -4.42893 -4.4289284 -4.4289284 -4.4289303 -4.4289322 -4.4289327 -4.4289284 -4.4289231 -4.42892 -4.4289188 -4.4289203]]...]
INFO - root - 2017-12-10 04:23:05.063028: step 8510, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.441 sec/batch; 39h:39m:37s remains)
INFO - root - 2017-12-10 04:23:09.331165: step 8520, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.437 sec/batch; 39h:21m:22s remains)
INFO - root - 2017-12-10 04:23:13.681639: step 8530, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 40h:03m:28s remains)
INFO - root - 2017-12-10 04:23:17.960012: step 8540, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.430 sec/batch; 38h:42m:39s remains)
INFO - root - 2017-12-10 04:23:22.285569: step 8550, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.425 sec/batch; 38h:12m:25s remains)
INFO - root - 2017-12-10 04:23:26.495248: step 8560, loss = 2.28, batch loss = 2.23 (18.9 examples/sec; 0.424 sec/batch; 38h:11m:20s remains)
INFO - root - 2017-12-10 04:23:30.855726: step 8570, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.428 sec/batch; 38h:30m:51s remains)
INFO - root - 2017-12-10 04:23:34.780846: step 8580, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.425 sec/batch; 38h:14m:43s remains)
INFO - root - 2017-12-10 04:23:39.065224: step 8590, loss = 2.28, batch loss = 2.23 (19.0 examples/sec; 0.420 sec/batch; 37h:49m:44s remains)
INFO - root - 2017-12-10 04:23:43.314545: step 8600, loss = 2.28, batch loss = 2.23 (18.9 examples/sec; 0.423 sec/batch; 38h:01m:53s remains)
2017-12-10 04:23:43.832080: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288845 -4.4288793 -4.4288754 -4.428875 -4.428874 -4.4288712 -4.4288669 -4.428864 -4.428865 -4.4288683 -4.4288716 -4.4288735 -4.4288735 -4.4288683 -4.4288568][-4.4288812 -4.4288735 -4.4288731 -4.4288788 -4.4288826 -4.42888 -4.4288712 -4.4288654 -4.4288664 -4.4288731 -4.4288793 -4.4288826 -4.4288812 -4.428875 -4.4288654][-4.4288797 -4.4288716 -4.4288735 -4.4288836 -4.4288878 -4.4288826 -4.4288678 -4.4288588 -4.4288673 -4.4288831 -4.4288921 -4.428894 -4.4288883 -4.42888 -4.4288754][-4.4288511 -4.428843 -4.4288478 -4.4288554 -4.4288511 -4.4288311 -4.4288015 -4.428792 -4.42882 -4.4288559 -4.4288759 -4.4288788 -4.4288721 -4.4288659 -4.42887][-4.428782 -4.4287734 -4.4287844 -4.4287925 -4.4287734 -4.4287214 -4.428647 -4.428616 -4.4286747 -4.42875 -4.4287958 -4.4288135 -4.4288163 -4.4288177 -4.4288335][-4.4286962 -4.4286938 -4.4287176 -4.4287381 -4.4287114 -4.4286165 -4.4284682 -4.4283843 -4.4284692 -4.4285965 -4.4286804 -4.42872 -4.4287329 -4.428741 -4.428762][-4.42866 -4.4286556 -4.4286819 -4.4287114 -4.4286847 -4.4285679 -4.42837 -4.4282393 -4.428329 -4.4284868 -4.4286017 -4.4286637 -4.4286857 -4.4286962 -4.4287148][-4.42866 -4.4286551 -4.4286737 -4.4287081 -4.4286957 -4.4286122 -4.428473 -4.4283781 -4.4284382 -4.4285569 -4.4286528 -4.4287114 -4.4287286 -4.4287291 -4.4287372][-4.4286809 -4.428678 -4.4286919 -4.4287214 -4.4287176 -4.4286795 -4.4286208 -4.4285803 -4.4286137 -4.4286809 -4.4287415 -4.4287815 -4.4287891 -4.4287782 -4.4287715][-4.4286942 -4.4286947 -4.4287071 -4.4287333 -4.4287338 -4.4287148 -4.4287033 -4.4287014 -4.4287195 -4.4287558 -4.4287963 -4.4288254 -4.428834 -4.4288187 -4.4287992][-4.4287128 -4.4287133 -4.4287314 -4.4287586 -4.428761 -4.4287472 -4.4287505 -4.4287634 -4.4287777 -4.4288063 -4.4288416 -4.4288659 -4.4288735 -4.428863 -4.4288435][-4.4287615 -4.42876 -4.4287758 -4.428791 -4.42879 -4.4287796 -4.4287853 -4.4287992 -4.4288106 -4.4288406 -4.4288716 -4.4288888 -4.4288955 -4.428894 -4.4288883][-4.4288373 -4.4288373 -4.4288421 -4.42884 -4.428833 -4.4288206 -4.428822 -4.4288268 -4.42883 -4.4288492 -4.4288683 -4.4288731 -4.4288745 -4.42888 -4.4288898][-4.4289107 -4.4289107 -4.4289036 -4.42889 -4.4288764 -4.4288583 -4.4288492 -4.4288435 -4.4288373 -4.4288454 -4.428853 -4.4288492 -4.4288454 -4.4288521 -4.4288678][-4.4289384 -4.4289436 -4.428937 -4.4289231 -4.428906 -4.4288826 -4.4288616 -4.4288449 -4.4288292 -4.4288259 -4.4288268 -4.4288244 -4.4288235 -4.428834 -4.4288492]]...]
INFO - root - 2017-12-10 04:23:48.153374: step 8610, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.437 sec/batch; 39h:21m:34s remains)
INFO - root - 2017-12-10 04:23:52.376294: step 8620, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.436 sec/batch; 39h:15m:30s remains)
INFO - root - 2017-12-10 04:23:56.643968: step 8630, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.433 sec/batch; 38h:57m:06s remains)
INFO - root - 2017-12-10 04:24:00.855092: step 8640, loss = 2.28, batch loss = 2.23 (19.8 examples/sec; 0.404 sec/batch; 36h:18m:58s remains)
INFO - root - 2017-12-10 04:24:05.051501: step 8650, loss = 2.28, batch loss = 2.23 (19.2 examples/sec; 0.417 sec/batch; 37h:29m:59s remains)
INFO - root - 2017-12-10 04:24:09.362520: step 8660, loss = 2.28, batch loss = 2.23 (19.0 examples/sec; 0.422 sec/batch; 37h:57m:05s remains)
INFO - root - 2017-12-10 04:24:13.643399: step 8670, loss = 2.28, batch loss = 2.23 (19.3 examples/sec; 0.415 sec/batch; 37h:19m:04s remains)
INFO - root - 2017-12-10 04:24:17.945682: step 8680, loss = 2.28, batch loss = 2.23 (19.0 examples/sec; 0.421 sec/batch; 37h:50m:33s remains)
INFO - root - 2017-12-10 04:24:22.201564: step 8690, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.431 sec/batch; 38h:43m:28s remains)
INFO - root - 2017-12-10 04:24:26.116539: step 8700, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.425 sec/batch; 38h:15m:52s remains)
2017-12-10 04:24:26.628119: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289122 -4.4288797 -4.4288497 -4.4288406 -4.4288578 -4.4288831 -4.4288955 -4.4288912 -4.4288707 -4.4288445 -4.4288168 -4.428802 -4.4288054 -4.4288273 -4.4288549][-4.4288988 -4.4288659 -4.4288263 -4.4288087 -4.4288197 -4.4288516 -4.4288721 -4.428863 -4.4288211 -4.4287839 -4.4287562 -4.4287524 -4.4287663 -4.4287949 -4.4288268][-4.4288807 -4.4288526 -4.4288011 -4.4287677 -4.42877 -4.4288034 -4.4288239 -4.4287982 -4.4287324 -4.4286904 -4.4286752 -4.4286914 -4.4287238 -4.4287624 -4.4288049][-4.4288554 -4.4288282 -4.4287767 -4.42874 -4.4287386 -4.4287524 -4.4287438 -4.4286923 -4.4286194 -4.4285989 -4.4286118 -4.4286528 -4.4287043 -4.4287534 -4.4288068][-4.4288368 -4.4288044 -4.4287539 -4.4287167 -4.4287028 -4.4286728 -4.4286108 -4.4285274 -4.428472 -4.4284916 -4.4285536 -4.4286308 -4.4287009 -4.428762 -4.4288197][-4.4288349 -4.4287887 -4.4287205 -4.4286695 -4.4286256 -4.4285393 -4.428412 -4.428299 -4.4282837 -4.4283605 -4.4284763 -4.428597 -4.4287 -4.4287734 -4.4288278][-4.4288311 -4.42878 -4.4286985 -4.4286222 -4.4285231 -4.4283547 -4.4281454 -4.4280047 -4.4280648 -4.4282289 -4.4283924 -4.4285502 -4.4286871 -4.4287786 -4.4288335][-4.4288254 -4.4287848 -4.4287 -4.4286027 -4.4284611 -4.4282417 -4.4279819 -4.4278526 -4.427999 -4.4282236 -4.4283934 -4.4285464 -4.4286866 -4.4287858 -4.428844][-4.4288206 -4.428782 -4.4287009 -4.4286008 -4.4284568 -4.4282589 -4.4280396 -4.4279637 -4.4281349 -4.4283495 -4.42849 -4.4285975 -4.4287043 -4.4287896 -4.42885][-4.4288335 -4.428791 -4.4287167 -4.4286251 -4.4285121 -4.4283738 -4.42824 -4.428216 -4.4283457 -4.4284983 -4.4285951 -4.4286575 -4.4287219 -4.428791 -4.4288516][-4.4288697 -4.4288249 -4.4287615 -4.4286814 -4.4286056 -4.4285316 -4.4284735 -4.4284759 -4.428555 -4.4286461 -4.4287057 -4.4287305 -4.4287658 -4.4288197 -4.4288731][-4.4289174 -4.42888 -4.4288316 -4.4287624 -4.4287138 -4.4286871 -4.4286809 -4.4286971 -4.4287415 -4.4287887 -4.4288163 -4.4288225 -4.428844 -4.4288807 -4.4289184][-4.4289484 -4.4289136 -4.428874 -4.4288244 -4.4288073 -4.4288135 -4.4288311 -4.4288487 -4.4288678 -4.4288821 -4.4288878 -4.4288921 -4.4289064 -4.4289346 -4.4289579][-4.4289484 -4.4289088 -4.4288688 -4.4288449 -4.4288568 -4.4288831 -4.4289036 -4.4289093 -4.4289088 -4.4289103 -4.4289136 -4.4289241 -4.4289346 -4.4289532 -4.4289651][-4.4289231 -4.4288874 -4.4288545 -4.4288421 -4.428865 -4.4288974 -4.4289145 -4.4289088 -4.4288988 -4.4288969 -4.4289031 -4.428916 -4.4289265 -4.4289393 -4.4289474]]...]
INFO - root - 2017-12-10 04:24:30.967084: step 8710, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.431 sec/batch; 38h:46m:43s remains)
INFO - root - 2017-12-10 04:24:35.201172: step 8720, loss = 2.28, batch loss = 2.23 (19.4 examples/sec; 0.412 sec/batch; 37h:05m:15s remains)
INFO - root - 2017-12-10 04:24:39.438983: step 8730, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.427 sec/batch; 38h:25m:16s remains)
INFO - root - 2017-12-10 04:24:43.713974: step 8740, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.428 sec/batch; 38h:31m:32s remains)
INFO - root - 2017-12-10 04:24:48.016773: step 8750, loss = 2.28, batch loss = 2.23 (19.5 examples/sec; 0.411 sec/batch; 36h:56m:48s remains)
INFO - root - 2017-12-10 04:24:52.266729: step 8760, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.428 sec/batch; 38h:28m:04s remains)
INFO - root - 2017-12-10 04:24:56.529402: step 8770, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.426 sec/batch; 38h:20m:07s remains)
INFO - root - 2017-12-10 04:25:00.762420: step 8780, loss = 2.28, batch loss = 2.23 (19.1 examples/sec; 0.419 sec/batch; 37h:39m:51s remains)
INFO - root - 2017-12-10 04:25:05.060873: step 8790, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.426 sec/batch; 38h:17m:08s remains)
INFO - root - 2017-12-10 04:25:09.372494: step 8800, loss = 2.28, batch loss = 2.23 (19.3 examples/sec; 0.415 sec/batch; 37h:18m:09s remains)
2017-12-10 04:25:09.864815: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289727 -4.4289818 -4.4289885 -4.4289889 -4.4289889 -4.4289861 -4.4289775 -4.4289656 -4.4289556 -4.4289508 -4.4289522 -4.42896 -4.4289737 -4.42899 -4.4290075][-4.4289007 -4.4289179 -4.4289308 -4.4289308 -4.428925 -4.4289193 -4.4289107 -4.4289012 -4.4288964 -4.4288936 -4.4288926 -4.4289 -4.4289179 -4.42894 -4.4289651][-4.4287953 -4.4288163 -4.428833 -4.4288406 -4.4288311 -4.4288163 -4.4288116 -4.4288163 -4.4288235 -4.4288254 -4.4288197 -4.4288211 -4.4288387 -4.4288659 -4.4289026][-4.4286618 -4.428678 -4.428699 -4.4287157 -4.4287081 -4.4286819 -4.4286776 -4.4286914 -4.428709 -4.4287181 -4.4287105 -4.4287095 -4.4287229 -4.4287477 -4.4288015][-4.428545 -4.4285417 -4.4285488 -4.428555 -4.4285355 -4.4284959 -4.4284906 -4.42852 -4.4285588 -4.4285855 -4.42859 -4.4285922 -4.4286046 -4.4286246 -4.4286842][-4.4285007 -4.4284735 -4.428452 -4.4284253 -4.4283743 -4.4283171 -4.4283075 -4.4283485 -4.4284134 -4.4284763 -4.428515 -4.428535 -4.4285474 -4.4285645 -4.4286246][-4.4285407 -4.4284964 -4.42844 -4.4283729 -4.4283004 -4.4282441 -4.4282355 -4.4282737 -4.4283361 -4.4284291 -4.4285126 -4.4285541 -4.428565 -4.428575 -4.4286284][-4.4286375 -4.4285979 -4.4285274 -4.4284425 -4.4283624 -4.4283137 -4.4283018 -4.4283228 -4.4283605 -4.4284506 -4.4285583 -4.428618 -4.4286289 -4.4286304 -4.4286618][-4.4287519 -4.4287372 -4.4286776 -4.4285884 -4.4284992 -4.4284539 -4.4284496 -4.4284682 -4.42848 -4.428535 -4.4286323 -4.4287 -4.4287109 -4.4287033 -4.42871][-4.4288249 -4.4288387 -4.428802 -4.4287257 -4.4286442 -4.4286032 -4.4286108 -4.4286404 -4.428647 -4.4286633 -4.4287076 -4.42875 -4.4287534 -4.4287295 -4.4287219][-4.4288487 -4.428885 -4.4288788 -4.4288321 -4.42877 -4.428731 -4.4287395 -4.4287748 -4.428793 -4.4288011 -4.4288073 -4.4288116 -4.4287949 -4.4287558 -4.4287362][-4.4288225 -4.4288754 -4.4289012 -4.4288864 -4.4288478 -4.4288149 -4.4288116 -4.428833 -4.4288545 -4.4288683 -4.42887 -4.4288645 -4.4288368 -4.4287891 -4.4287667][-4.4287615 -4.4288197 -4.4288659 -4.4288793 -4.4288664 -4.4288478 -4.4288421 -4.4288454 -4.4288559 -4.4288678 -4.4288745 -4.4288807 -4.4288588 -4.4288111 -4.4287896][-4.428689 -4.4287429 -4.4287996 -4.4288368 -4.4288521 -4.428853 -4.4288449 -4.4288387 -4.4288387 -4.4288397 -4.4288397 -4.4288487 -4.4288383 -4.4287996 -4.4287834][-4.4286647 -4.4287033 -4.4287591 -4.4288063 -4.4288392 -4.4288559 -4.4288521 -4.4288397 -4.4288344 -4.4288273 -4.4288158 -4.4288154 -4.4288039 -4.4287772 -4.4287677]]...]
INFO - root - 2017-12-10 04:25:14.104667: step 8810, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.425 sec/batch; 38h:13m:10s remains)
INFO - root - 2017-12-10 04:25:18.094240: step 8820, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.434 sec/batch; 39h:03m:10s remains)
INFO - root - 2017-12-10 04:25:22.315690: step 8830, loss = 2.28, batch loss = 2.23 (19.4 examples/sec; 0.412 sec/batch; 37h:02m:08s remains)
INFO - root - 2017-12-10 04:25:26.542071: step 8840, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.443 sec/batch; 39h:51m:59s remains)
INFO - root - 2017-12-10 04:25:30.756268: step 8850, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.431 sec/batch; 38h:43m:30s remains)
INFO - root - 2017-12-10 04:25:35.100964: step 8860, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.426 sec/batch; 38h:17m:41s remains)
INFO - root - 2017-12-10 04:25:39.387826: step 8870, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.433 sec/batch; 38h:53m:57s remains)
INFO - root - 2017-12-10 04:25:43.611811: step 8880, loss = 2.28, batch loss = 2.23 (19.2 examples/sec; 0.418 sec/batch; 37h:32m:28s remains)
INFO - root - 2017-12-10 04:25:47.958398: step 8890, loss = 2.28, batch loss = 2.23 (19.2 examples/sec; 0.418 sec/batch; 37h:32m:03s remains)
INFO - root - 2017-12-10 04:25:52.192878: step 8900, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.430 sec/batch; 38h:39m:01s remains)
2017-12-10 04:25:52.705701: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.429009 -4.4289966 -4.4289746 -4.428946 -4.4289227 -4.4289083 -4.4289079 -4.4289217 -4.4289556 -4.4289904 -4.4290156 -4.429029 -4.4290333 -4.4290338 -4.4290323][-4.4290056 -4.4289794 -4.4289417 -4.4288983 -4.428865 -4.42885 -4.4288535 -4.4288726 -4.4289112 -4.4289551 -4.4289913 -4.4290113 -4.4290166 -4.42902 -4.4290228][-4.4289985 -4.4289603 -4.428906 -4.4288416 -4.4287939 -4.4287777 -4.4287887 -4.4288177 -4.4288654 -4.4289174 -4.4289637 -4.428987 -4.4289918 -4.428997 -4.4290037][-4.4289865 -4.4289432 -4.4288735 -4.4287858 -4.4287167 -4.4286919 -4.4287047 -4.4287419 -4.4288034 -4.428874 -4.4289351 -4.4289632 -4.4289684 -4.4289732 -4.4289818][-4.4289804 -4.4289341 -4.4288526 -4.4287415 -4.4286423 -4.4285917 -4.4285855 -4.4286218 -4.4287086 -4.4288068 -4.4288874 -4.4289269 -4.4289427 -4.428957 -4.4289708][-4.4289784 -4.4289336 -4.4288497 -4.4287205 -4.4285841 -4.42848 -4.4284182 -4.4284372 -4.4285631 -4.4287095 -4.42882 -4.4288807 -4.4289165 -4.4289479 -4.4289684][-4.4289842 -4.4289484 -4.4288683 -4.4287276 -4.4285536 -4.4283781 -4.4282184 -4.428185 -4.4283633 -4.4285946 -4.4287548 -4.4288425 -4.4289 -4.4289436 -4.4289703][-4.4289994 -4.4289737 -4.4289041 -4.4287653 -4.4285722 -4.4283338 -4.4280553 -4.4279194 -4.42814 -4.428462 -4.4286747 -4.4287944 -4.4288778 -4.4289393 -4.4289746][-4.4290061 -4.4289942 -4.4289474 -4.4288306 -4.4286494 -4.4284029 -4.4280825 -4.4278665 -4.4280429 -4.42837 -4.4285994 -4.4287348 -4.4288411 -4.4289246 -4.4289742][-4.4289927 -4.4289985 -4.42898 -4.4289021 -4.4287639 -4.428566 -4.42831 -4.4281068 -4.4281583 -4.4283752 -4.428556 -4.4286766 -4.428792 -4.4288979 -4.428967][-4.4289637 -4.4289808 -4.4289875 -4.4289565 -4.4288759 -4.4287438 -4.42857 -4.4284067 -4.4283705 -4.4284663 -4.4285765 -4.4286661 -4.42877 -4.4288826 -4.4289603][-4.4289336 -4.4289641 -4.4289904 -4.428997 -4.428968 -4.4288945 -4.4287863 -4.4286714 -4.4286036 -4.4286184 -4.4286718 -4.4287329 -4.4288168 -4.4289126 -4.4289775][-4.428895 -4.4289474 -4.428988 -4.4290166 -4.4290223 -4.428987 -4.4289246 -4.42886 -4.4288082 -4.428792 -4.4288135 -4.4288511 -4.4289088 -4.4289718 -4.4290109][-4.4288497 -4.4289279 -4.4289813 -4.4290214 -4.4290404 -4.4290247 -4.4289851 -4.4289536 -4.4289322 -4.4289169 -4.4289246 -4.428946 -4.4289804 -4.4290156 -4.4290347][-4.4288116 -4.4289055 -4.4289694 -4.4290166 -4.4290423 -4.4290366 -4.4290128 -4.4289975 -4.4289956 -4.4289918 -4.4289947 -4.4290051 -4.4290223 -4.4290385 -4.4290462]]...]
INFO - root - 2017-12-10 04:25:56.993358: step 8910, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.432 sec/batch; 38h:51m:02s remains)
INFO - root - 2017-12-10 04:26:01.444848: step 8920, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.454 sec/batch; 40h:48m:17s remains)
INFO - root - 2017-12-10 04:26:05.730567: step 8930, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.428 sec/batch; 38h:27m:23s remains)
INFO - root - 2017-12-10 04:26:09.706661: step 8940, loss = 2.28, batch loss = 2.23 (28.9 examples/sec; 0.277 sec/batch; 24h:52m:06s remains)
INFO - root - 2017-12-10 04:26:13.905313: step 8950, loss = 2.28, batch loss = 2.23 (19.0 examples/sec; 0.421 sec/batch; 37h:48m:47s remains)
INFO - root - 2017-12-10 04:26:18.261560: step 8960, loss = 2.28, batch loss = 2.23 (19.3 examples/sec; 0.415 sec/batch; 37h:16m:48s remains)
INFO - root - 2017-12-10 04:26:22.565498: step 8970, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.426 sec/batch; 38h:18m:03s remains)
INFO - root - 2017-12-10 04:26:26.882402: step 8980, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.430 sec/batch; 38h:41m:07s remains)
INFO - root - 2017-12-10 04:26:31.167680: step 8990, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 39h:31m:03s remains)
INFO - root - 2017-12-10 04:26:35.419425: step 9000, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.435 sec/batch; 39h:06m:45s remains)
2017-12-10 04:26:35.926766: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287453 -4.4287491 -4.4287448 -4.4287434 -4.4287415 -4.4287372 -4.4287324 -4.4287405 -4.4287462 -4.4287496 -4.42876 -4.4287696 -4.42877 -4.4287515 -4.4287124][-4.4287329 -4.4287257 -4.4287176 -4.428719 -4.4287162 -4.4287076 -4.4286942 -4.428689 -4.428689 -4.4286962 -4.4287238 -4.4287524 -4.428772 -4.4287691 -4.4287367][-4.4287224 -4.4287047 -4.4286933 -4.4286971 -4.4286919 -4.4286766 -4.4286561 -4.42864 -4.4286385 -4.4286528 -4.4286971 -4.4287419 -4.428771 -4.4287729 -4.4287424][-4.4286957 -4.4286723 -4.42866 -4.4286647 -4.4286604 -4.4286451 -4.4286227 -4.4285984 -4.4285989 -4.4286265 -4.4286909 -4.42875 -4.428781 -4.4287834 -4.4287548][-4.4286394 -4.4286218 -4.4286184 -4.4286261 -4.4286184 -4.4285951 -4.4285636 -4.4285288 -4.4285331 -4.4285851 -4.4286785 -4.42876 -4.4287996 -4.4288077 -4.4287863][-4.4285607 -4.4285507 -4.42856 -4.4285655 -4.4285431 -4.4284959 -4.428441 -4.4283857 -4.4283886 -4.4284739 -4.428606 -4.4287257 -4.4287906 -4.4288197 -4.4288154][-4.428503 -4.4284916 -4.4285045 -4.428504 -4.4284635 -4.4283905 -4.4283047 -4.4282169 -4.4281993 -4.4283071 -4.4284759 -4.4286342 -4.4287329 -4.4287896 -4.4288077][-4.4285541 -4.4285383 -4.4285479 -4.4285421 -4.4284973 -4.42842 -4.4283252 -4.4282174 -4.4281611 -4.4282403 -4.4283943 -4.4285507 -4.4286542 -4.4287167 -4.428741][-4.4286561 -4.4286461 -4.4286604 -4.4286604 -4.4286327 -4.4285855 -4.4285293 -4.4284563 -4.4283977 -4.4284229 -4.4285064 -4.4286008 -4.4286613 -4.4286919 -4.4286966][-4.4287395 -4.4287376 -4.4287548 -4.4287581 -4.4287462 -4.4287291 -4.42871 -4.4286785 -4.4286466 -4.4286489 -4.4286823 -4.428721 -4.4287415 -4.4287448 -4.4287362][-4.4287748 -4.4287767 -4.4287972 -4.4288077 -4.4288135 -4.4288163 -4.4288144 -4.4288087 -4.4288015 -4.4288025 -4.4288163 -4.428833 -4.4288383 -4.4288325 -4.4288168][-4.4287977 -4.4287987 -4.4288163 -4.42883 -4.42884 -4.4288454 -4.42885 -4.4288573 -4.4288635 -4.4288669 -4.4288726 -4.4288855 -4.42889 -4.4288888 -4.4288788][-4.4288316 -4.4288273 -4.4288359 -4.428844 -4.4288411 -4.4288325 -4.4288359 -4.4288454 -4.4288521 -4.4288545 -4.4288578 -4.4288678 -4.4288721 -4.4288716 -4.428864][-4.4288564 -4.4288468 -4.4288483 -4.42885 -4.4288411 -4.4288282 -4.4288292 -4.4288335 -4.4288344 -4.4288316 -4.4288297 -4.428833 -4.4288344 -4.4288335 -4.4288273][-4.4288597 -4.4288511 -4.4288526 -4.4288559 -4.4288511 -4.4288421 -4.4288416 -4.4288406 -4.4288359 -4.4288292 -4.4288254 -4.4288263 -4.4288297 -4.4288335 -4.4288344]]...]
INFO - root - 2017-12-10 04:26:40.133467: step 9010, loss = 2.28, batch loss = 2.23 (19.0 examples/sec; 0.422 sec/batch; 37h:54m:49s remains)
INFO - root - 2017-12-10 04:26:44.343753: step 9020, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.430 sec/batch; 38h:39m:11s remains)
INFO - root - 2017-12-10 04:26:48.633506: step 9030, loss = 2.28, batch loss = 2.23 (19.0 examples/sec; 0.421 sec/batch; 37h:51m:35s remains)
INFO - root - 2017-12-10 04:26:52.940945: step 9040, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.449 sec/batch; 40h:22m:16s remains)
INFO - root - 2017-12-10 04:26:57.241132: step 9050, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.429 sec/batch; 38h:33m:05s remains)
INFO - root - 2017-12-10 04:27:01.447167: step 9060, loss = 2.28, batch loss = 2.23 (27.5 examples/sec; 0.291 sec/batch; 26h:09m:17s remains)
INFO - root - 2017-12-10 04:27:05.505216: step 9070, loss = 2.28, batch loss = 2.23 (19.5 examples/sec; 0.411 sec/batch; 36h:55m:25s remains)
INFO - root - 2017-12-10 04:27:09.772563: step 9080, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.425 sec/batch; 38h:08m:33s remains)
INFO - root - 2017-12-10 04:27:14.039871: step 9090, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.425 sec/batch; 38h:12m:17s remains)
INFO - root - 2017-12-10 04:27:18.246444: step 9100, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.434 sec/batch; 38h:57m:52s remains)
2017-12-10 04:27:18.730359: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287868 -4.428792 -4.428813 -4.428823 -4.4288421 -4.4288683 -4.4288816 -4.4288869 -4.4288917 -4.4288878 -4.4288754 -4.4288659 -4.4288573 -4.4288454 -4.4288177][-4.4287329 -4.4287481 -4.4287758 -4.4287887 -4.4288077 -4.4288316 -4.4288411 -4.4288411 -4.4288397 -4.4288321 -4.4288182 -4.4288087 -4.4288077 -4.428803 -4.4287767][-4.4286957 -4.4287243 -4.4287581 -4.4287653 -4.4287786 -4.428802 -4.4288082 -4.4287996 -4.4287934 -4.4287763 -4.4287457 -4.4287314 -4.4287372 -4.4287477 -4.4287391][-4.428669 -4.428699 -4.4287267 -4.4287343 -4.42874 -4.4287558 -4.4287519 -4.4287386 -4.428741 -4.4287238 -4.4286709 -4.4286442 -4.4286556 -4.4286861 -4.428709][-4.4286637 -4.4286914 -4.42871 -4.4287152 -4.4287062 -4.4286966 -4.4286489 -4.4286222 -4.4286609 -4.4286766 -4.4286289 -4.4286027 -4.4286213 -4.42867 -4.4287214][-4.4286456 -4.4286675 -4.428688 -4.428689 -4.4286609 -4.4286146 -4.4284949 -4.4284282 -4.4285235 -4.428616 -4.4286251 -4.4286256 -4.4286528 -4.4287143 -4.4287777][-4.428606 -4.4286137 -4.4286337 -4.4286394 -4.4286027 -4.4285126 -4.4283071 -4.428184 -4.4283457 -4.4285297 -4.4286175 -4.4286661 -4.4287138 -4.4287748 -4.428834][-4.4285731 -4.4285641 -4.4285889 -4.4286141 -4.4285793 -4.42845 -4.4281764 -4.4279895 -4.4281907 -4.4284363 -4.4285836 -4.428679 -4.4287591 -4.4288211 -4.4288678][-4.4285769 -4.4285617 -4.4285955 -4.428638 -4.4286222 -4.4285011 -4.4282384 -4.4280562 -4.4282231 -4.4284439 -4.42859 -4.428699 -4.4287839 -4.42883 -4.4288511][-4.4286165 -4.4286089 -4.4286408 -4.4286838 -4.4286928 -4.4286089 -4.4284191 -4.4283018 -4.4284067 -4.4285493 -4.428647 -4.4287348 -4.4288006 -4.4288216 -4.4288168][-4.4286857 -4.4286819 -4.4287066 -4.4287443 -4.4287705 -4.4287238 -4.4286003 -4.4285359 -4.4285984 -4.4286823 -4.4287329 -4.4287844 -4.4288197 -4.4288173 -4.4287968][-4.4287457 -4.4287443 -4.4287791 -4.4288177 -4.4288492 -4.428823 -4.4287539 -4.4287257 -4.4287672 -4.4288144 -4.4288306 -4.4288473 -4.4288507 -4.4288311 -4.4287944][-4.4287786 -4.4287777 -4.428823 -4.4288697 -4.4289007 -4.428895 -4.4288635 -4.4288545 -4.4288807 -4.4289031 -4.4289021 -4.4289 -4.4288893 -4.42887 -4.4288335][-4.42879 -4.4287887 -4.4288454 -4.4289069 -4.4289379 -4.4289389 -4.4289231 -4.4289212 -4.4289331 -4.4289389 -4.4289312 -4.4289188 -4.428906 -4.4288983 -4.4288831][-4.4287953 -4.4287982 -4.4288564 -4.4289207 -4.4289451 -4.4289451 -4.4289374 -4.42894 -4.4289446 -4.4289441 -4.4289336 -4.42892 -4.4289122 -4.428916 -4.4289145]]...]
INFO - root - 2017-12-10 04:27:23.063400: step 9110, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.444 sec/batch; 39h:55m:38s remains)
INFO - root - 2017-12-10 04:27:27.302667: step 9120, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.426 sec/batch; 38h:14m:32s remains)
INFO - root - 2017-12-10 04:27:31.544041: step 9130, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.452 sec/batch; 40h:38m:03s remains)
INFO - root - 2017-12-10 04:27:35.795814: step 9140, loss = 2.28, batch loss = 2.23 (19.3 examples/sec; 0.415 sec/batch; 37h:15m:23s remains)
INFO - root - 2017-12-10 04:27:40.104826: step 9150, loss = 2.28, batch loss = 2.23 (18.9 examples/sec; 0.423 sec/batch; 37h:57m:15s remains)
INFO - root - 2017-12-10 04:27:44.368632: step 9160, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.425 sec/batch; 38h:08m:47s remains)
INFO - root - 2017-12-10 04:27:48.651719: step 9170, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.426 sec/batch; 38h:17m:49s remains)
INFO - root - 2017-12-10 04:27:52.909190: step 9180, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.428 sec/batch; 38h:25m:01s remains)
INFO - root - 2017-12-10 04:27:56.964102: step 9190, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.448 sec/batch; 40h:12m:52s remains)
INFO - root - 2017-12-10 04:28:01.296078: step 9200, loss = 2.28, batch loss = 2.23 (18.9 examples/sec; 0.423 sec/batch; 37h:57m:51s remains)
2017-12-10 04:28:01.885115: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286494 -4.4286838 -4.4287319 -4.4287572 -4.4287481 -4.4287314 -4.4287109 -4.4287 -4.4286942 -4.4287086 -4.4287033 -4.4286852 -4.428699 -4.42872 -4.4287519][-4.4286447 -4.4286566 -4.4286866 -4.4286966 -4.4286842 -4.4286761 -4.4286647 -4.4286571 -4.4286547 -4.4286833 -4.4286928 -4.4286838 -4.4287086 -4.4287415 -4.428782][-4.4286814 -4.42867 -4.4286866 -4.4286923 -4.4286838 -4.4286838 -4.4286733 -4.4286623 -4.428669 -4.4287052 -4.4287205 -4.4287138 -4.4287438 -4.4287868 -4.4288249][-4.4287395 -4.4287109 -4.4287148 -4.4287176 -4.428709 -4.4287062 -4.4286809 -4.4286509 -4.4286532 -4.4286933 -4.4287133 -4.4287157 -4.4287553 -4.4288082 -4.4288487][-4.4287734 -4.4287391 -4.4287291 -4.428709 -4.4286804 -4.4286537 -4.4285946 -4.4285436 -4.4285617 -4.4286313 -4.4286761 -4.4286966 -4.4287534 -4.4288192 -4.4288673][-4.428762 -4.4287262 -4.4287028 -4.4286637 -4.4286094 -4.42852 -4.4283986 -4.4283276 -4.4283848 -4.4285111 -4.42861 -4.428668 -4.4287457 -4.4288239 -4.4288735][-4.4287157 -4.4286804 -4.4286504 -4.4285946 -4.4285016 -4.4283338 -4.4281311 -4.4280457 -4.4281821 -4.4283948 -4.4285607 -4.4286623 -4.4287567 -4.4288306 -4.4288721][-4.428679 -4.4286437 -4.428607 -4.4285355 -4.4284172 -4.4282241 -4.4280057 -4.4279523 -4.4281449 -4.4283929 -4.4285855 -4.4286966 -4.4287825 -4.4288397 -4.4288621][-4.4286933 -4.4286661 -4.4286327 -4.4285655 -4.4284682 -4.4283366 -4.4282165 -4.4282336 -4.4283924 -4.4285717 -4.4287043 -4.4287786 -4.4288311 -4.4288592 -4.428865][-4.4287143 -4.4286976 -4.4286838 -4.4286404 -4.4285774 -4.4285173 -4.4284825 -4.4285383 -4.4286456 -4.4287381 -4.4287949 -4.428823 -4.4288416 -4.4288487 -4.4288521][-4.4287338 -4.4287281 -4.4287367 -4.4287152 -4.4286785 -4.4286566 -4.42866 -4.4287157 -4.4287796 -4.428823 -4.4288335 -4.4288235 -4.4288173 -4.4288158 -4.4288235][-4.4287424 -4.4287505 -4.4287782 -4.4287767 -4.4287543 -4.4287477 -4.4287558 -4.4287925 -4.4288244 -4.42884 -4.4288263 -4.4287953 -4.4287834 -4.4287844 -4.4287982][-4.4287133 -4.4287429 -4.4287882 -4.4288025 -4.4287848 -4.4287767 -4.4287739 -4.4287868 -4.4287963 -4.4287996 -4.428781 -4.4287515 -4.4287529 -4.4287715 -4.4287992][-4.4286966 -4.4287381 -4.4287868 -4.42881 -4.4288 -4.4287944 -4.4287882 -4.4287863 -4.4287796 -4.428772 -4.4287472 -4.4287133 -4.4287248 -4.4287605 -4.4288068][-4.4287233 -4.4287558 -4.4287934 -4.4288154 -4.428813 -4.4288096 -4.4288044 -4.4287992 -4.4287839 -4.42877 -4.4287486 -4.4287262 -4.4287519 -4.4287972 -4.4288421]]...]
INFO - root - 2017-12-10 04:28:06.136345: step 9210, loss = 2.28, batch loss = 2.23 (18.9 examples/sec; 0.424 sec/batch; 38h:05m:06s remains)
INFO - root - 2017-12-10 04:28:10.406056: step 9220, loss = 2.28, batch loss = 2.23 (18.9 examples/sec; 0.424 sec/batch; 38h:02m:29s remains)
INFO - root - 2017-12-10 04:28:14.696316: step 9230, loss = 2.28, batch loss = 2.23 (19.0 examples/sec; 0.422 sec/batch; 37h:51m:35s remains)
INFO - root - 2017-12-10 04:28:18.969763: step 9240, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.426 sec/batch; 38h:15m:18s remains)
INFO - root - 2017-12-10 04:28:23.231091: step 9250, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.433 sec/batch; 38h:51m:45s remains)
INFO - root - 2017-12-10 04:28:27.382865: step 9260, loss = 2.28, batch loss = 2.23 (19.2 examples/sec; 0.417 sec/batch; 37h:26m:37s remains)
INFO - root - 2017-12-10 04:28:31.587905: step 9270, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.443 sec/batch; 39h:48m:10s remains)
INFO - root - 2017-12-10 04:28:35.871930: step 9280, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.448 sec/batch; 40h:15m:59s remains)
INFO - root - 2017-12-10 04:28:40.175115: step 9290, loss = 2.28, batch loss = 2.23 (18.9 examples/sec; 0.423 sec/batch; 37h:59m:28s remains)
INFO - root - 2017-12-10 04:28:44.520938: step 9300, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.427 sec/batch; 38h:21m:05s remains)
2017-12-10 04:28:45.044419: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287286 -4.4287014 -4.4286752 -4.428658 -4.4286613 -4.4286675 -4.4286847 -4.4287338 -4.42879 -4.4288154 -4.4288073 -4.4287772 -4.4287305 -4.4287004 -4.4287276][-4.4286957 -4.4286594 -4.42862 -4.4285908 -4.4285979 -4.4286208 -4.4286504 -4.4287009 -4.4287577 -4.4287815 -4.4287748 -4.4287534 -4.4287195 -4.4286976 -4.4287295][-4.4286613 -4.4286165 -4.4285674 -4.4285345 -4.4285545 -4.4286027 -4.4286442 -4.4286914 -4.4287438 -4.4287667 -4.428761 -4.4287419 -4.4287119 -4.4286947 -4.42873][-4.4286313 -4.4285808 -4.4285297 -4.4285073 -4.4285407 -4.4285975 -4.4286366 -4.4286757 -4.4287238 -4.428751 -4.4287529 -4.4287386 -4.4287114 -4.4286962 -4.42873][-4.4286275 -4.42858 -4.4285388 -4.42853 -4.4285622 -4.428606 -4.4286304 -4.4286551 -4.4286938 -4.4287243 -4.4287362 -4.4287319 -4.4287109 -4.4286966 -4.4287286][-4.4286146 -4.4285674 -4.4285359 -4.4285331 -4.4285593 -4.4285889 -4.4285975 -4.4286127 -4.4286485 -4.4286842 -4.4287047 -4.4287086 -4.4286928 -4.4286823 -4.4287171][-4.4285789 -4.4285254 -4.4284911 -4.4284849 -4.4285092 -4.4285388 -4.4285507 -4.4285731 -4.4286118 -4.4286513 -4.4286771 -4.4286847 -4.4286723 -4.4286661 -4.4287066][-4.4285407 -4.4284797 -4.4284372 -4.4284215 -4.4284444 -4.4284849 -4.4285192 -4.4285645 -4.4286075 -4.4286447 -4.4286675 -4.4286723 -4.4286594 -4.4286575 -4.4287047][-4.4285426 -4.4284906 -4.428453 -4.4284329 -4.4284492 -4.4284921 -4.4285421 -4.4286017 -4.4286466 -4.4286757 -4.4286904 -4.4286847 -4.4286652 -4.4286642 -4.4287124][-4.4285688 -4.4285269 -4.4284949 -4.4284744 -4.4284863 -4.4285269 -4.4285836 -4.4286489 -4.4286971 -4.4287243 -4.4287357 -4.4287233 -4.4286952 -4.428689 -4.4287295][-4.4285903 -4.4285583 -4.4285331 -4.4285107 -4.4285173 -4.4285531 -4.428606 -4.42867 -4.4287205 -4.4287529 -4.4287677 -4.4287524 -4.428721 -4.4287105 -4.4287453][-4.4286 -4.4285808 -4.4285655 -4.4285502 -4.428556 -4.4285817 -4.4286189 -4.428669 -4.4287181 -4.4287543 -4.4287729 -4.4287581 -4.4287286 -4.4287186 -4.4287524][-4.4285965 -4.4285808 -4.4285679 -4.4285593 -4.4285645 -4.42858 -4.4286003 -4.4286356 -4.4286785 -4.4287143 -4.4287395 -4.4287362 -4.4287162 -4.4287109 -4.4287477][-4.4285989 -4.4285774 -4.4285588 -4.4285512 -4.4285555 -4.4285631 -4.4285712 -4.4285946 -4.4286275 -4.4286585 -4.4286876 -4.4286971 -4.4286885 -4.4286895 -4.42873][-4.4286265 -4.4286056 -4.4285831 -4.4285707 -4.4285731 -4.4285827 -4.4285893 -4.4286089 -4.4286327 -4.4286556 -4.42868 -4.4286895 -4.4286785 -4.4286747 -4.4287119]]...]
INFO - root - 2017-12-10 04:28:49.071081: step 9310, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.426 sec/batch; 38h:14m:44s remains)
INFO - root - 2017-12-10 04:28:53.407093: step 9320, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.430 sec/batch; 38h:36m:11s remains)
INFO - root - 2017-12-10 04:28:57.750813: step 9330, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.433 sec/batch; 38h:54m:28s remains)
INFO - root - 2017-12-10 04:29:02.069615: step 9340, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.434 sec/batch; 38h:57m:47s remains)
INFO - root - 2017-12-10 04:29:06.388196: step 9350, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.436 sec/batch; 39h:05m:38s remains)
INFO - root - 2017-12-10 04:29:10.670405: step 9360, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.424 sec/batch; 38h:06m:10s remains)
INFO - root - 2017-12-10 04:29:14.922349: step 9370, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.430 sec/batch; 38h:34m:42s remains)
INFO - root - 2017-12-10 04:29:19.230040: step 9380, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.430 sec/batch; 38h:36m:45s remains)
INFO - root - 2017-12-10 04:29:23.429478: step 9390, loss = 2.28, batch loss = 2.23 (19.0 examples/sec; 0.422 sec/batch; 37h:52m:39s remains)
INFO - root - 2017-12-10 04:29:27.690754: step 9400, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.439 sec/batch; 39h:21m:45s remains)
2017-12-10 04:29:28.165514: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428853 -4.4288397 -4.4288421 -4.4288425 -4.4288459 -4.4288592 -4.428854 -4.4288445 -4.4288235 -4.4287796 -4.42871 -4.428617 -4.4285355 -4.428484 -4.4284329][-4.4288507 -4.4288464 -4.4288568 -4.4288659 -4.4288774 -4.4288964 -4.4289017 -4.4289045 -4.4288983 -4.4288788 -4.4288349 -4.4287615 -4.4286795 -4.4286041 -4.4285169][-4.4288793 -4.4288774 -4.4288869 -4.4289 -4.428915 -4.4289284 -4.4289293 -4.4289327 -4.4289379 -4.4289374 -4.4289126 -4.428863 -4.4287982 -4.4287257 -4.4286242][-4.4288974 -4.4288926 -4.4288955 -4.4289021 -4.4289103 -4.428916 -4.4289145 -4.4289169 -4.4289184 -4.4289241 -4.4289145 -4.4288821 -4.42884 -4.4287858 -4.4286904][-4.42889 -4.4288778 -4.4288707 -4.4288616 -4.428854 -4.4288516 -4.4288487 -4.4288478 -4.4288468 -4.4288535 -4.4288578 -4.4288459 -4.4288316 -4.4288006 -4.4287353][-4.4288731 -4.4288464 -4.4288282 -4.428812 -4.4287877 -4.4287634 -4.4287395 -4.4287281 -4.4287305 -4.4287395 -4.4287481 -4.4287519 -4.42877 -4.42878 -4.4287658][-4.4288573 -4.4288125 -4.4287825 -4.4287548 -4.4287028 -4.428637 -4.4285741 -4.4285474 -4.428544 -4.4285426 -4.4285507 -4.4285755 -4.4286418 -4.4287019 -4.4287405][-4.4288597 -4.4287987 -4.4287481 -4.4286876 -4.4285903 -4.4284635 -4.4283543 -4.42831 -4.4282937 -4.4282784 -4.4282966 -4.4283638 -4.4284863 -4.4286041 -4.42869][-4.4288797 -4.4288211 -4.4287567 -4.4286518 -4.4284959 -4.4283204 -4.4281893 -4.42814 -4.4281216 -4.4281077 -4.4281492 -4.4282541 -4.4284105 -4.4285631 -4.428679][-4.4288669 -4.4288154 -4.4287529 -4.4286432 -4.4284844 -4.4283285 -4.4282303 -4.4281955 -4.428194 -4.4282165 -4.4282928 -4.4284081 -4.428544 -4.4286675 -4.4287572][-4.4287767 -4.4287434 -4.4287066 -4.4286447 -4.4285545 -4.4284825 -4.4284649 -4.428472 -4.4284921 -4.4285359 -4.4286089 -4.428688 -4.4287658 -4.4288273 -4.4288664][-4.4286089 -4.4286137 -4.4286208 -4.4286213 -4.4286237 -4.4286447 -4.428699 -4.4287491 -4.4287863 -4.4288263 -4.428864 -4.4288936 -4.4289207 -4.4289403 -4.4289527][-4.4283881 -4.4284582 -4.4285336 -4.4286084 -4.42868 -4.4287543 -4.4288397 -4.4289036 -4.4289412 -4.4289637 -4.4289737 -4.4289789 -4.4289846 -4.428988 -4.4289923][-4.4282546 -4.4283919 -4.4285278 -4.4286513 -4.42875 -4.42883 -4.4289045 -4.4289565 -4.4289832 -4.4289904 -4.4289851 -4.4289827 -4.4289861 -4.4289913 -4.4289961][-4.4283619 -4.428515 -4.4286537 -4.4287648 -4.428844 -4.4288988 -4.4289379 -4.4289637 -4.4289804 -4.4289804 -4.4289718 -4.4289708 -4.42898 -4.4289908 -4.429]]...]
INFO - root - 2017-12-10 04:29:32.516076: step 9410, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.443 sec/batch; 39h:46m:38s remains)
INFO - root - 2017-12-10 04:29:36.799980: step 9420, loss = 2.28, batch loss = 2.23 (19.5 examples/sec; 0.410 sec/batch; 36h:50m:04s remains)
INFO - root - 2017-12-10 04:29:40.766104: step 9430, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.428 sec/batch; 38h:26m:44s remains)
INFO - root - 2017-12-10 04:29:45.126191: step 9440, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.439 sec/batch; 39h:25m:25s remains)
INFO - root - 2017-12-10 04:29:49.408607: step 9450, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.431 sec/batch; 38h:42m:39s remains)
INFO - root - 2017-12-10 04:29:53.673068: step 9460, loss = 2.28, batch loss = 2.23 (19.0 examples/sec; 0.420 sec/batch; 37h:43m:02s remains)
INFO - root - 2017-12-10 04:29:57.957244: step 9470, loss = 2.28, batch loss = 2.23 (19.2 examples/sec; 0.417 sec/batch; 37h:26m:05s remains)
INFO - root - 2017-12-10 04:30:02.206132: step 9480, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.439 sec/batch; 39h:24m:00s remains)
INFO - root - 2017-12-10 04:30:06.487630: step 9490, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.439 sec/batch; 39h:21m:14s remains)
INFO - root - 2017-12-10 04:30:10.841782: step 9500, loss = 2.28, batch loss = 2.23 (14.8 examples/sec; 0.540 sec/batch; 48h:24m:55s remains)
2017-12-10 04:30:11.314342: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428793 -4.4288259 -4.4288478 -4.4288197 -4.4287357 -4.4286208 -4.4285479 -4.42857 -4.428658 -4.4287658 -4.4288321 -4.4288712 -4.4288983 -4.428925 -4.4289217][-4.4288168 -4.4288387 -4.4288592 -4.4288359 -4.4287567 -4.4286361 -4.4285522 -4.4285507 -4.4286275 -4.4287419 -4.428823 -4.428875 -4.4289145 -4.4289351 -4.4289284][-4.4288173 -4.4288254 -4.4288383 -4.4288192 -4.4287558 -4.4286461 -4.4285507 -4.42851 -4.4285579 -4.428678 -4.4287744 -4.4288392 -4.4288855 -4.428905 -4.4289007][-4.4288092 -4.4288006 -4.4288006 -4.4287815 -4.4287319 -4.4286494 -4.4285522 -4.4284859 -4.4285054 -4.428617 -4.4287181 -4.4287839 -4.4288311 -4.428854 -4.4288526][-4.4287782 -4.428761 -4.4287443 -4.4287071 -4.428647 -4.4285793 -4.4285021 -4.428452 -4.4284668 -4.4285679 -4.4286647 -4.4287295 -4.4287763 -4.428802 -4.4288073][-4.4287424 -4.4287224 -4.4286795 -4.428606 -4.4285254 -4.4284687 -4.428421 -4.4283929 -4.4284053 -4.4284987 -4.4285975 -4.4286704 -4.4287281 -4.4287558 -4.4287667][-4.4287167 -4.4286995 -4.4286385 -4.4285378 -4.4284487 -4.4284019 -4.428381 -4.428369 -4.428371 -4.4284477 -4.4285421 -4.4286289 -4.4286971 -4.4287238 -4.4287372][-4.428719 -4.4287157 -4.4286704 -4.428576 -4.4284739 -4.4284182 -4.4284058 -4.428412 -4.42842 -4.4284811 -4.4285626 -4.42865 -4.4287148 -4.4287314 -4.4287434][-4.4287543 -4.4287629 -4.4287381 -4.42867 -4.4285693 -4.4285011 -4.4284835 -4.4284954 -4.428514 -4.4285665 -4.4286313 -4.4287057 -4.428772 -4.4287896 -4.4288][-4.4288006 -4.4288192 -4.4288092 -4.42877 -4.4286971 -4.428647 -4.4286389 -4.4286461 -4.42865 -4.4286785 -4.4287176 -4.4287763 -4.4288344 -4.4288564 -4.4288654][-4.428834 -4.428865 -4.4288759 -4.4288568 -4.42881 -4.4287877 -4.42879 -4.4287868 -4.4287648 -4.4287696 -4.428792 -4.428834 -4.428874 -4.4288917 -4.4289007][-4.4288297 -4.4288669 -4.4288945 -4.4288812 -4.428844 -4.4288411 -4.4288545 -4.4288568 -4.4288354 -4.4288244 -4.4288306 -4.4288497 -4.4288726 -4.4288874 -4.4288983][-4.4288335 -4.4288611 -4.4288874 -4.4288683 -4.428833 -4.4288311 -4.4288435 -4.4288511 -4.4288392 -4.4288311 -4.4288354 -4.4288421 -4.4288568 -4.4288726 -4.428885][-4.4288712 -4.4288836 -4.428896 -4.4288816 -4.4288526 -4.4288454 -4.428854 -4.428864 -4.4288573 -4.428853 -4.4288535 -4.4288564 -4.4288673 -4.4288821 -4.428895][-4.4289103 -4.4289126 -4.4289155 -4.4289083 -4.4288917 -4.4288836 -4.4288859 -4.428894 -4.4288931 -4.4288912 -4.4288917 -4.428894 -4.4289026 -4.4289141 -4.428925]]...]
INFO - root - 2017-12-10 04:30:15.607070: step 9510, loss = 2.28, batch loss = 2.23 (18.9 examples/sec; 0.424 sec/batch; 38h:03m:41s remains)
INFO - root - 2017-12-10 04:30:19.886382: step 9520, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.435 sec/batch; 38h:59m:15s remains)
INFO - root - 2017-12-10 04:30:24.176996: step 9530, loss = 2.28, batch loss = 2.23 (19.1 examples/sec; 0.418 sec/batch; 37h:30m:59s remains)
INFO - root - 2017-12-10 04:30:28.430455: step 9540, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.427 sec/batch; 38h:16m:53s remains)
INFO - root - 2017-12-10 04:30:32.466232: step 9550, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.438 sec/batch; 39h:18m:36s remains)
INFO - root - 2017-12-10 04:30:36.780749: step 9560, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.439 sec/batch; 39h:22m:41s remains)
INFO - root - 2017-12-10 04:30:41.089377: step 9570, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.448 sec/batch; 40h:09m:45s remains)
INFO - root - 2017-12-10 04:30:45.371357: step 9580, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.441 sec/batch; 39h:33m:48s remains)
INFO - root - 2017-12-10 04:30:49.647864: step 9590, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.431 sec/batch; 38h:42m:09s remains)
INFO - root - 2017-12-10 04:30:53.957866: step 9600, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.437 sec/batch; 39h:12m:56s remains)
2017-12-10 04:30:54.521251: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287996 -4.4287825 -4.4287815 -4.4287877 -4.4287691 -4.4287105 -4.4286671 -4.4287 -4.4287405 -4.4287462 -4.4287443 -4.4287324 -4.4287128 -4.4286957 -4.4286909][-4.428782 -4.4287605 -4.4287591 -4.4287629 -4.4287376 -4.4286752 -4.428637 -4.4286847 -4.4287453 -4.428762 -4.4287696 -4.4287677 -4.42876 -4.4287486 -4.4287419][-4.4287872 -4.4287615 -4.4287567 -4.4287562 -4.4287238 -4.428659 -4.4286213 -4.4286723 -4.4287395 -4.428761 -4.4287786 -4.428792 -4.4287996 -4.4287958 -4.428792][-4.4287982 -4.4287677 -4.4287591 -4.4287553 -4.428721 -4.4286542 -4.4286122 -4.428659 -4.42872 -4.4287353 -4.42875 -4.4287667 -4.4287815 -4.4287858 -4.4287887][-4.4287977 -4.4287591 -4.4287405 -4.4287276 -4.4286876 -4.428617 -4.4285712 -4.4286227 -4.4286838 -4.4286985 -4.4287028 -4.4287114 -4.4287243 -4.4287372 -4.4287505][-4.4287906 -4.4287376 -4.428699 -4.4286642 -4.4286027 -4.4285169 -4.4284678 -4.4285345 -4.4286118 -4.4286366 -4.42864 -4.4286442 -4.428658 -4.4286809 -4.4287033][-4.4287858 -4.4287229 -4.4286666 -4.4286017 -4.4285016 -4.4283848 -4.4283252 -4.4284105 -4.4285116 -4.428555 -4.4285727 -4.4285874 -4.4286103 -4.4286427 -4.4286718][-4.4287887 -4.4287281 -4.4286718 -4.4286013 -4.4284854 -4.42835 -4.4282751 -4.4283547 -4.4284616 -4.4285183 -4.4285526 -4.4285812 -4.4286094 -4.4286432 -4.4286714][-4.4287996 -4.4287496 -4.4287071 -4.428658 -4.4285717 -4.4284658 -4.4283981 -4.4284449 -4.4285235 -4.42857 -4.4286041 -4.4286327 -4.4286551 -4.4286833 -4.4287095][-4.4288177 -4.4287815 -4.4287548 -4.42873 -4.4286823 -4.4286222 -4.4285812 -4.428606 -4.4286485 -4.4286695 -4.4286876 -4.4287052 -4.4287167 -4.428731 -4.4287519][-4.42884 -4.4288125 -4.4287906 -4.4287715 -4.4287467 -4.4287214 -4.4287004 -4.4287119 -4.4287291 -4.428731 -4.4287333 -4.4287415 -4.4287491 -4.4287543 -4.4287653][-4.4288635 -4.4288349 -4.4288087 -4.428782 -4.42876 -4.4287496 -4.4287405 -4.4287462 -4.4287462 -4.4287367 -4.4287324 -4.4287391 -4.4287481 -4.4287496 -4.42875][-4.4288883 -4.4288568 -4.428823 -4.4287863 -4.4287605 -4.4287562 -4.4287586 -4.4287643 -4.4287543 -4.4287372 -4.4287281 -4.4287281 -4.4287329 -4.4287357 -4.42874][-4.42892 -4.4288926 -4.4288607 -4.4288216 -4.428792 -4.4287877 -4.4287958 -4.4288039 -4.4287896 -4.4287667 -4.4287481 -4.4287338 -4.4287295 -4.42874 -4.428761][-4.4289517 -4.4289331 -4.4289112 -4.4288783 -4.4288478 -4.4288349 -4.4288373 -4.4288464 -4.4288354 -4.4288125 -4.4287848 -4.4287567 -4.42874 -4.4287524 -4.4287872]]...]
INFO - root - 2017-12-10 04:30:58.762032: step 9610, loss = 2.28, batch loss = 2.23 (19.2 examples/sec; 0.416 sec/batch; 37h:17m:20s remains)
INFO - root - 2017-12-10 04:31:03.028372: step 9620, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.430 sec/batch; 38h:35m:09s remains)
INFO - root - 2017-12-10 04:31:07.308996: step 9630, loss = 2.28, batch loss = 2.23 (19.0 examples/sec; 0.421 sec/batch; 37h:46m:45s remains)
INFO - root - 2017-12-10 04:31:11.629952: step 9640, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.429 sec/batch; 38h:29m:09s remains)
INFO - root - 2017-12-10 04:31:15.862332: step 9650, loss = 2.28, batch loss = 2.23 (19.9 examples/sec; 0.403 sec/batch; 36h:08m:04s remains)
INFO - root - 2017-12-10 04:31:20.125176: step 9660, loss = 2.28, batch loss = 2.23 (19.2 examples/sec; 0.417 sec/batch; 37h:21m:47s remains)
INFO - root - 2017-12-10 04:31:24.242349: step 9670, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.446 sec/batch; 40h:02m:09s remains)
INFO - root - 2017-12-10 04:31:28.493340: step 9680, loss = 2.28, batch loss = 2.23 (18.9 examples/sec; 0.423 sec/batch; 37h:58m:23s remains)
INFO - root - 2017-12-10 04:31:32.810427: step 9690, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.432 sec/batch; 38h:46m:24s remains)
INFO - root - 2017-12-10 04:31:37.110063: step 9700, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.435 sec/batch; 39h:02m:24s remains)
2017-12-10 04:31:37.658310: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288983 -4.4288907 -4.4288912 -4.4288855 -4.4288917 -4.4288988 -4.4289007 -4.4289083 -4.4289117 -4.4289165 -4.4289246 -4.4289355 -4.4289446 -4.4289527 -4.4289608][-4.42887 -4.4288626 -4.4288616 -4.4288468 -4.4288454 -4.4288511 -4.4288445 -4.4288425 -4.4288468 -4.42886 -4.428885 -4.4289122 -4.4289293 -4.42894 -4.4289484][-4.4288449 -4.4288344 -4.4288206 -4.4287891 -4.4287739 -4.428771 -4.4287496 -4.4287419 -4.4287577 -4.428791 -4.4288354 -4.4288797 -4.4289079 -4.4289246 -4.4289355][-4.4288239 -4.4288015 -4.4287658 -4.4287219 -4.4286981 -4.4286871 -4.4286432 -4.4286222 -4.4286494 -4.4287047 -4.4287696 -4.4288344 -4.428875 -4.4288979 -4.4289131][-4.4288011 -4.4287682 -4.4287143 -4.4286661 -4.4286366 -4.4286041 -4.4285192 -4.4284687 -4.428514 -4.4285994 -4.428689 -4.4287753 -4.4288297 -4.4288654 -4.4288898][-4.42878 -4.4287395 -4.4286766 -4.4286232 -4.4285855 -4.4285245 -4.4283876 -4.4283009 -4.4283743 -4.4284973 -4.4286203 -4.4287248 -4.428791 -4.4288387 -4.4288726][-4.4287839 -4.4287438 -4.4286776 -4.4286165 -4.428556 -4.428452 -4.4282441 -4.4281235 -4.4282465 -4.4284296 -4.4285917 -4.4287066 -4.4287839 -4.4288378 -4.4288712][-4.42881 -4.4287834 -4.4287152 -4.4286323 -4.4285254 -4.4283471 -4.4280338 -4.4278913 -4.428113 -4.4283781 -4.4285755 -4.4286985 -4.4287887 -4.4288445 -4.4288745][-4.4288154 -4.4288044 -4.4287419 -4.4286413 -4.4284921 -4.4282508 -4.4278436 -4.4276972 -4.4280219 -4.4283595 -4.4285831 -4.4287081 -4.4288049 -4.4288588 -4.4288826][-4.4288015 -4.42881 -4.428772 -4.4286771 -4.4285226 -4.4282947 -4.4279432 -4.4278331 -4.4281173 -4.4284282 -4.4286337 -4.4287372 -4.4288263 -4.4288764 -4.428894][-4.4287872 -4.428812 -4.428803 -4.4287305 -4.4285936 -4.4284124 -4.4281483 -4.4280648 -4.4282784 -4.4285259 -4.4286852 -4.4287643 -4.4288411 -4.4288874 -4.4289021][-4.4287581 -4.4287992 -4.4288158 -4.4287634 -4.4286509 -4.4285126 -4.4283214 -4.4282637 -4.4284248 -4.4286122 -4.4287252 -4.4287825 -4.4288449 -4.4288907 -4.4289074][-4.4287257 -4.428782 -4.4288163 -4.42879 -4.4287128 -4.4286189 -4.4284911 -4.4284477 -4.4285564 -4.4286909 -4.428771 -4.4288096 -4.4288564 -4.4288988 -4.428916][-4.4287357 -4.4287972 -4.4288411 -4.4288411 -4.4287982 -4.428741 -4.428658 -4.4286189 -4.4286814 -4.4287724 -4.4288263 -4.4288459 -4.4288716 -4.428905 -4.4289212][-4.4287663 -4.4288278 -4.4288754 -4.4288888 -4.4288669 -4.4288292 -4.428771 -4.4287376 -4.4287758 -4.4288392 -4.4288745 -4.4288745 -4.4288783 -4.4288993 -4.4289174]]...]
INFO - root - 2017-12-10 04:31:41.980814: step 9710, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.450 sec/batch; 40h:19m:39s remains)
INFO - root - 2017-12-10 04:31:46.213465: step 9720, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.438 sec/batch; 39h:14m:00s remains)
INFO - root - 2017-12-10 04:31:50.482233: step 9730, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.429 sec/batch; 38h:28m:14s remains)
INFO - root - 2017-12-10 04:31:54.793984: step 9740, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.426 sec/batch; 38h:12m:40s remains)
INFO - root - 2017-12-10 04:31:59.144577: step 9750, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.430 sec/batch; 38h:31m:55s remains)
INFO - root - 2017-12-10 04:32:03.499759: step 9760, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.436 sec/batch; 39h:04m:45s remains)
INFO - root - 2017-12-10 04:32:07.818184: step 9770, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.425 sec/batch; 38h:07m:13s remains)
INFO - root - 2017-12-10 04:32:12.061612: step 9780, loss = 2.28, batch loss = 2.23 (19.2 examples/sec; 0.417 sec/batch; 37h:22m:12s remains)
INFO - root - 2017-12-10 04:32:16.051951: step 9790, loss = 2.28, batch loss = 2.23 (18.9 examples/sec; 0.423 sec/batch; 37h:56m:12s remains)
INFO - root - 2017-12-10 04:32:20.348139: step 9800, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.426 sec/batch; 38h:09m:39s remains)
2017-12-10 04:32:20.855794: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428956 -4.4289513 -4.4289479 -4.428946 -4.428946 -4.428947 -4.4289489 -4.42895 -4.4289494 -4.4289451 -4.4289412 -4.4289322 -4.4289246 -4.4289122 -4.4288812][-4.4289055 -4.4289 -4.4288926 -4.4288869 -4.4288864 -4.4288907 -4.4288988 -4.4289064 -4.4289107 -4.4289107 -4.4289074 -4.4288964 -4.4288836 -4.4288688 -4.4288378][-4.4288659 -4.4288592 -4.4288468 -4.4288368 -4.4288349 -4.4288445 -4.42886 -4.4288774 -4.42889 -4.4288988 -4.4289007 -4.4288893 -4.4288731 -4.4288521 -4.4288158][-4.4288349 -4.4288249 -4.4288034 -4.4287825 -4.4287744 -4.4287858 -4.4288111 -4.428843 -4.4288712 -4.4288936 -4.4289069 -4.4288974 -4.4288754 -4.4288454 -4.4287968][-4.4287915 -4.4287758 -4.4287415 -4.4287019 -4.4286757 -4.428678 -4.4287062 -4.4287481 -4.4287896 -4.4288282 -4.428864 -4.42887 -4.4288545 -4.4288254 -4.4287729][-4.4286995 -4.4286819 -4.4286346 -4.4285746 -4.4285235 -4.4285064 -4.4285231 -4.4285617 -4.4286084 -4.4286642 -4.4287305 -4.4287677 -4.4287791 -4.4287663 -4.4287286][-4.4286571 -4.4286437 -4.4285932 -4.4285197 -4.4284487 -4.4284058 -4.4283938 -4.4284062 -4.4284368 -4.4284997 -4.4285917 -4.4286637 -4.4287095 -4.428721 -4.4287076][-4.4287581 -4.42875 -4.4287057 -4.4286394 -4.428576 -4.4285259 -4.4284897 -4.4284711 -4.428473 -4.4285192 -4.428607 -4.4286828 -4.4287362 -4.4287524 -4.4287481][-4.4288697 -4.4288664 -4.4288378 -4.4287939 -4.4287558 -4.4287252 -4.4286938 -4.4286671 -4.4286475 -4.4286594 -4.4287095 -4.428762 -4.4288 -4.4288058 -4.4287996][-4.4289064 -4.4289103 -4.4289 -4.4288783 -4.4288621 -4.4288483 -4.4288263 -4.4287992 -4.4287691 -4.4287529 -4.4287686 -4.4287996 -4.4288273 -4.4288273 -4.4288168][-4.4289021 -4.4289107 -4.4289117 -4.4289045 -4.4289007 -4.4288983 -4.4288859 -4.428863 -4.428834 -4.4288063 -4.4288039 -4.4288206 -4.4288387 -4.428834 -4.4288197][-4.42891 -4.4289117 -4.4289122 -4.428906 -4.4288993 -4.4288974 -4.428894 -4.4288816 -4.428865 -4.4288464 -4.428844 -4.4288507 -4.4288578 -4.4288425 -4.4288225][-4.42894 -4.4289327 -4.4289241 -4.4289079 -4.4288855 -4.4288692 -4.4288616 -4.4288554 -4.4288526 -4.4288483 -4.4288526 -4.4288526 -4.4288487 -4.4288273 -4.4288082][-4.4289703 -4.4289603 -4.4289479 -4.428925 -4.4288878 -4.4288516 -4.4288268 -4.4288149 -4.4288163 -4.4288177 -4.4288263 -4.4288211 -4.4288077 -4.4287848 -4.4287729][-4.4289947 -4.4289889 -4.4289794 -4.4289589 -4.42892 -4.428874 -4.4288335 -4.4288092 -4.4288025 -4.4287996 -4.4288068 -4.4288025 -4.4287896 -4.4287696 -4.4287634]]...]
INFO - root - 2017-12-10 04:32:25.155371: step 9810, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.441 sec/batch; 39h:31m:42s remains)
INFO - root - 2017-12-10 04:32:29.381594: step 9820, loss = 2.28, batch loss = 2.23 (19.2 examples/sec; 0.417 sec/batch; 37h:23m:19s remains)
INFO - root - 2017-12-10 04:32:33.627351: step 9830, loss = 2.28, batch loss = 2.23 (18.9 examples/sec; 0.424 sec/batch; 38h:00m:24s remains)
INFO - root - 2017-12-10 04:32:37.932080: step 9840, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.458 sec/batch; 41h:04m:27s remains)
INFO - root - 2017-12-10 04:32:42.306351: step 9850, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.454 sec/batch; 40h:42m:32s remains)
INFO - root - 2017-12-10 04:32:46.692388: step 9860, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.437 sec/batch; 39h:08m:23s remains)
INFO - root - 2017-12-10 04:32:51.032810: step 9870, loss = 2.28, batch loss = 2.23 (19.0 examples/sec; 0.421 sec/batch; 37h:46m:27s remains)
INFO - root - 2017-12-10 04:32:55.282913: step 9880, loss = 2.28, batch loss = 2.23 (19.5 examples/sec; 0.411 sec/batch; 36h:50m:23s remains)
INFO - root - 2017-12-10 04:32:59.550838: step 9890, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.446 sec/batch; 39h:58m:10s remains)
INFO - root - 2017-12-10 04:33:03.831213: step 9900, loss = 2.28, batch loss = 2.23 (19.3 examples/sec; 0.414 sec/batch; 37h:05m:03s remains)
2017-12-10 04:33:04.334576: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287524 -4.4287972 -4.4288387 -4.4288464 -4.428843 -4.4288373 -4.4288378 -4.4288335 -4.4288249 -4.4288135 -4.428812 -4.4288111 -4.4287972 -4.4287672 -4.4287357][-4.4287076 -4.4287581 -4.4288054 -4.428813 -4.4288039 -4.4287915 -4.4287853 -4.428781 -4.4287739 -4.4287648 -4.4287581 -4.4287524 -4.4287314 -4.4286909 -4.42865][-4.4286871 -4.4287357 -4.4287753 -4.4287753 -4.4287596 -4.42874 -4.4287224 -4.4287105 -4.4287086 -4.428709 -4.4287076 -4.4287095 -4.4286919 -4.4286489 -4.4285979][-4.4287133 -4.4287539 -4.4287796 -4.4287667 -4.4287405 -4.4287066 -4.4286695 -4.42864 -4.4286375 -4.4286518 -4.4286671 -4.4286904 -4.428688 -4.4286566 -4.4286013][-4.4287481 -4.4287758 -4.4287844 -4.4287596 -4.4287252 -4.4286771 -4.4286242 -4.4285717 -4.4285674 -4.4285979 -4.428638 -4.42869 -4.4287162 -4.4287009 -4.4286408][-4.4287477 -4.4287677 -4.4287624 -4.4287271 -4.4286809 -4.4286275 -4.4285703 -4.4285169 -4.4285259 -4.4285769 -4.4286389 -4.4287086 -4.4287558 -4.4287534 -4.428688][-4.4287162 -4.4287357 -4.4287295 -4.42869 -4.4286275 -4.4285626 -4.42851 -4.4284782 -4.4285078 -4.4285722 -4.428647 -4.4287224 -4.4287786 -4.4287829 -4.428721][-4.4286604 -4.4286752 -4.428668 -4.4286361 -4.4285679 -4.4284954 -4.4284539 -4.4284539 -4.4284997 -4.4285717 -4.4286537 -4.4287243 -4.428772 -4.4287744 -4.4287271][-4.4286132 -4.4286079 -4.4285936 -4.4285688 -4.42851 -4.4284449 -4.4284191 -4.4284425 -4.4284992 -4.4285827 -4.4286628 -4.4287148 -4.4287415 -4.4287415 -4.42871][-4.4285908 -4.4285665 -4.4285417 -4.4285135 -4.4284725 -4.4284353 -4.4284315 -4.4284635 -4.4285231 -4.4286032 -4.4286709 -4.42871 -4.4287276 -4.4287248 -4.4286981][-4.4285893 -4.4285526 -4.428515 -4.4284816 -4.4284673 -4.4284668 -4.4284816 -4.4285126 -4.4285569 -4.428616 -4.4286742 -4.4287171 -4.4287281 -4.4287171 -4.428688][-4.4286175 -4.4285765 -4.4285297 -4.4284954 -4.4284949 -4.4285197 -4.4285469 -4.4285741 -4.4286027 -4.4286408 -4.4286861 -4.4287262 -4.4287267 -4.4287109 -4.428679][-4.4286413 -4.4286032 -4.4285617 -4.4285374 -4.42855 -4.428587 -4.4286232 -4.4286451 -4.4286628 -4.42869 -4.4287238 -4.4287515 -4.4287434 -4.4287257 -4.42869][-4.4286842 -4.4286556 -4.4286251 -4.428616 -4.4286356 -4.4286733 -4.4287109 -4.4287333 -4.4287491 -4.4287672 -4.4287877 -4.4288011 -4.428792 -4.4287753 -4.4287395][-4.4287653 -4.4287496 -4.4287395 -4.4287462 -4.4287653 -4.42879 -4.428812 -4.4288239 -4.4288325 -4.4288445 -4.4288583 -4.4288645 -4.4288564 -4.428843 -4.4288163]]...]
INFO - root - 2017-12-10 04:33:08.353965: step 9910, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.431 sec/batch; 38h:36m:30s remains)
INFO - root - 2017-12-10 04:33:12.625828: step 9920, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.437 sec/batch; 39h:08m:32s remains)
INFO - root - 2017-12-10 04:33:16.847160: step 9930, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.425 sec/batch; 38h:02m:27s remains)
INFO - root - 2017-12-10 04:33:21.094599: step 9940, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 39h:27m:01s remains)
INFO - root - 2017-12-10 04:33:25.388387: step 9950, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.435 sec/batch; 38h:57m:08s remains)
INFO - root - 2017-12-10 04:33:29.695941: step 9960, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.432 sec/batch; 38h:41m:39s remains)
INFO - root - 2017-12-10 04:33:34.009112: step 9970, loss = 2.28, batch loss = 2.23 (19.1 examples/sec; 0.419 sec/batch; 37h:33m:34s remains)
INFO - root - 2017-12-10 04:33:38.238750: step 9980, loss = 2.28, batch loss = 2.23 (19.0 examples/sec; 0.420 sec/batch; 37h:39m:15s remains)
INFO - root - 2017-12-10 04:33:42.588614: step 9990, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.427 sec/batch; 38h:17m:45s remains)
INFO - root - 2017-12-10 04:33:46.905254: step 10000, loss = 2.28, batch loss = 2.23 (19.4 examples/sec; 0.413 sec/batch; 37h:00m:50s remains)
2017-12-10 04:33:47.439922: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289703 -4.4289408 -4.4289117 -4.4288888 -4.4288683 -4.4288473 -4.428833 -4.4288349 -4.4288526 -4.428865 -4.4288726 -4.4288874 -4.4289045 -4.4289217 -4.4289374][-4.4289432 -4.4288993 -4.4288573 -4.4288273 -4.4287977 -4.4287648 -4.4287367 -4.4287343 -4.428761 -4.4287844 -4.4287996 -4.4288216 -4.4288483 -4.4288783 -4.4289036][-4.4289227 -4.4288616 -4.4288039 -4.4287605 -4.4287171 -4.4286695 -4.4286218 -4.4286094 -4.4286385 -4.428668 -4.4286933 -4.4287295 -4.4287705 -4.42882 -4.4288621][-4.4289036 -4.4288249 -4.4287496 -4.4286909 -4.4286222 -4.4285522 -4.4284763 -4.4284449 -4.4284849 -4.428534 -4.4285784 -4.4286337 -4.4286966 -4.4287686 -4.4288306][-4.4288836 -4.428793 -4.4286957 -4.4286189 -4.4285307 -4.428421 -4.42829 -4.4282265 -4.4283037 -4.4284053 -4.4284935 -4.4285727 -4.4286542 -4.4287376 -4.42881][-4.4288754 -4.4287877 -4.4286766 -4.4285779 -4.4284544 -4.4282875 -4.4280686 -4.4279366 -4.4280677 -4.4282522 -4.4284029 -4.4285188 -4.4286213 -4.428709 -4.4287829][-4.4288764 -4.4287987 -4.4286947 -4.4285793 -4.42842 -4.4281921 -4.4278755 -4.4276538 -4.4278436 -4.428122 -4.4283419 -4.4284949 -4.4286118 -4.4287028 -4.4287806][-4.4288797 -4.428812 -4.4287291 -4.4286308 -4.4284849 -4.4282789 -4.4280133 -4.4278369 -4.4279904 -4.4282336 -4.4284244 -4.428555 -4.42865 -4.4287271 -4.4287968][-4.4288783 -4.4288197 -4.4287591 -4.4286895 -4.4285855 -4.4284439 -4.4282808 -4.4281926 -4.4282966 -4.4284544 -4.4285831 -4.4286618 -4.4287186 -4.4287734 -4.4288259][-4.4288697 -4.4288125 -4.4287624 -4.4287066 -4.4286337 -4.4285436 -4.428462 -4.4284434 -4.4285364 -4.4286489 -4.4287324 -4.4287739 -4.4288 -4.4288349 -4.4288664][-4.4288921 -4.4288511 -4.428813 -4.4287677 -4.4287162 -4.4286656 -4.4286375 -4.4286523 -4.4287267 -4.4287996 -4.4288454 -4.4288678 -4.4288774 -4.4288921 -4.4289074][-4.4289403 -4.4289246 -4.4289088 -4.4288769 -4.4288416 -4.4288116 -4.4288015 -4.4288092 -4.4288497 -4.4288874 -4.4289117 -4.4289293 -4.4289312 -4.4289331 -4.4289393][-4.4289794 -4.4289775 -4.4289737 -4.42895 -4.4289246 -4.42891 -4.4289064 -4.4289045 -4.42892 -4.4289389 -4.4289503 -4.4289613 -4.4289622 -4.4289589 -4.42896][-4.4289942 -4.4289937 -4.4289904 -4.4289703 -4.4289536 -4.4289451 -4.4289436 -4.4289384 -4.4289412 -4.4289532 -4.4289618 -4.42897 -4.4289737 -4.4289708 -4.4289718][-4.4289994 -4.4289951 -4.4289889 -4.4289737 -4.4289613 -4.4289546 -4.4289532 -4.4289489 -4.4289494 -4.4289589 -4.428966 -4.4289708 -4.4289751 -4.4289756 -4.4289794]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0005-clip20/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0005-clip20/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-10 04:33:52.224719: step 10010, loss = 2.28, batch loss = 2.23 (19.3 examples/sec; 0.415 sec/batch; 37h:10m:44s remains)
INFO - root - 2017-12-10 04:33:56.507112: step 10020, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.429 sec/batch; 38h:28m:00s remains)
INFO - root - 2017-12-10 04:34:00.495580: step 10030, loss = 2.28, batch loss = 2.23 (19.0 examples/sec; 0.421 sec/batch; 37h:42m:26s remains)
INFO - root - 2017-12-10 04:34:04.825084: step 10040, loss = 2.28, batch loss = 2.23 (19.3 examples/sec; 0.414 sec/batch; 37h:02m:45s remains)
INFO - root - 2017-12-10 04:34:09.175987: step 10050, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.427 sec/batch; 38h:17m:27s remains)
INFO - root - 2017-12-10 04:34:13.482045: step 10060, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.444 sec/batch; 39h:47m:22s remains)
INFO - root - 2017-12-10 04:34:17.737759: step 10070, loss = 2.28, batch loss = 2.23 (18.9 examples/sec; 0.424 sec/batch; 37h:56m:50s remains)
INFO - root - 2017-12-10 04:34:22.111278: step 10080, loss = 2.28, batch loss = 2.23 (19.1 examples/sec; 0.419 sec/batch; 37h:33m:09s remains)
INFO - root - 2017-12-10 04:34:26.451114: step 10090, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.436 sec/batch; 39h:01m:43s remains)
INFO - root - 2017-12-10 04:34:30.737112: step 10100, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.427 sec/batch; 38h:15m:50s remains)
2017-12-10 04:34:31.266823: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288731 -4.4288769 -4.4289188 -4.42898 -4.4290223 -4.4290442 -4.4290462 -4.4290276 -4.4290061 -4.4289885 -4.4289865 -4.429 -4.429018 -4.4290261 -4.429028][-4.4288526 -4.4288459 -4.4288759 -4.4289289 -4.4289737 -4.4290028 -4.4290175 -4.4290071 -4.4289856 -4.4289613 -4.4289513 -4.4289594 -4.4289756 -4.4289842 -4.428988][-4.428843 -4.4288211 -4.4288316 -4.4288673 -4.428906 -4.428936 -4.4289584 -4.428956 -4.4289393 -4.4289188 -4.4289117 -4.4289184 -4.42893 -4.4289379 -4.4289451][-4.428834 -4.4287925 -4.4287791 -4.4287968 -4.4288216 -4.42884 -4.4288626 -4.4288726 -4.4288664 -4.4288545 -4.4288526 -4.4288592 -4.4288678 -4.4288816 -4.4288993][-4.4288168 -4.4287524 -4.4287152 -4.4287095 -4.4287081 -4.4287033 -4.428719 -4.4287457 -4.4287658 -4.4287796 -4.4287915 -4.4288015 -4.4288149 -4.4288387 -4.4288664][-4.4288049 -4.4287171 -4.4286523 -4.4286165 -4.4285784 -4.428535 -4.4285226 -4.4285617 -4.4286242 -4.4286866 -4.4287329 -4.4287667 -4.4288 -4.4288349 -4.428863][-4.4288096 -4.4287062 -4.428618 -4.4285517 -4.4284654 -4.4283566 -4.4282913 -4.428339 -4.4284635 -4.4286022 -4.4287066 -4.4287748 -4.4288259 -4.4288616 -4.42888][-4.4288244 -4.4287248 -4.4286318 -4.4285483 -4.4284225 -4.4282513 -4.42813 -4.4281788 -4.4283495 -4.4285464 -4.4286985 -4.4287939 -4.4288583 -4.428894 -4.4289069][-4.4288282 -4.4287505 -4.428679 -4.4286146 -4.4285097 -4.4283576 -4.4282379 -4.4282479 -4.4283752 -4.4285469 -4.4286957 -4.4287958 -4.4288645 -4.4289021 -4.4289188][-4.4288015 -4.4287539 -4.4287224 -4.4287071 -4.4286613 -4.4285793 -4.4284992 -4.4284739 -4.4285259 -4.4286256 -4.4287229 -4.4287925 -4.4288421 -4.4288678 -4.4288812][-4.4287505 -4.4287243 -4.4287286 -4.4287648 -4.4287777 -4.4287596 -4.4287243 -4.4286957 -4.428709 -4.4287515 -4.4287906 -4.4288096 -4.4288211 -4.4288182 -4.4288092][-4.4287133 -4.4286957 -4.4287262 -4.4287968 -4.428844 -4.4288621 -4.4288621 -4.42885 -4.4288549 -4.428863 -4.42886 -4.4288387 -4.4288144 -4.428781 -4.4287462][-4.4287319 -4.4287186 -4.4287586 -4.4288387 -4.4288912 -4.4289141 -4.4289203 -4.4289145 -4.4289126 -4.4289036 -4.4288831 -4.4288468 -4.4288111 -4.4287724 -4.4287295][-4.4288058 -4.4287968 -4.4288325 -4.4289007 -4.4289365 -4.4289422 -4.4289284 -4.4289017 -4.4288754 -4.4288487 -4.4288244 -4.4288 -4.428781 -4.4287682 -4.4287491][-4.4288926 -4.4288864 -4.42891 -4.4289575 -4.4289722 -4.4289589 -4.4289269 -4.4288816 -4.4288282 -4.4287777 -4.4287467 -4.4287324 -4.4287329 -4.4287496 -4.4287682]]...]
INFO - root - 2017-12-10 04:34:35.532733: step 10110, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.432 sec/batch; 38h:38m:56s remains)
INFO - root - 2017-12-10 04:34:39.843667: step 10120, loss = 2.28, batch loss = 2.23 (19.2 examples/sec; 0.417 sec/batch; 37h:23m:01s remains)
INFO - root - 2017-12-10 04:34:44.076817: step 10130, loss = 2.28, batch loss = 2.23 (19.9 examples/sec; 0.402 sec/batch; 35h:57m:15s remains)
INFO - root - 2017-12-10 04:34:48.422017: step 10140, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.427 sec/batch; 38h:12m:00s remains)
INFO - root - 2017-12-10 04:34:52.420640: step 10150, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.457 sec/batch; 40h:52m:45s remains)
INFO - root - 2017-12-10 04:34:56.671934: step 10160, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.425 sec/batch; 38h:01m:47s remains)
