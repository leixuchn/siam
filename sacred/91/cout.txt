INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "91"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fix
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-06 06:51:30.096593: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 06:51:30.096631: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 06:51:30.096638: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 06:51:30.096642: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 06:51:30.096646: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-06 06:51:34.803731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 6.39GiB
2017-12-06 06:51:34.803782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-06 06:51:34.803790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-06 06:51:34.803808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-06 06:51:59.741536: step 0, loss = 2.03, batch loss = 1.97 (0.5 examples/sec; 16.742 sec/batch; 1546h:18m:31s remains)
2017-12-06 06:52:00.717931: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3281651 -4.324882 -4.3271704 -4.3293996 -4.3270731 -4.3180833 -4.3042178 -4.2883644 -4.2776432 -4.2735672 -4.2772717 -4.2903118 -4.3097625 -4.3277316 -4.33855][-4.3273273 -4.3243828 -4.3281474 -4.3293037 -4.320322 -4.3004055 -4.2746882 -4.2510252 -4.2399058 -4.2398973 -4.2499776 -4.2686057 -4.2944088 -4.3168893 -4.3316283][-4.3241334 -4.32218 -4.3259335 -4.3217039 -4.3004045 -4.26644 -4.2252674 -4.1917629 -4.1803966 -4.188972 -4.211575 -4.2402358 -4.2743349 -4.30304 -4.3235135][-4.317327 -4.3177776 -4.3196115 -4.3029885 -4.264535 -4.211544 -4.1494021 -4.1052685 -4.0978642 -4.1220217 -4.1624942 -4.20754 -4.2531333 -4.2891712 -4.315855][-4.3046966 -4.3095431 -4.3080034 -4.2781968 -4.2239218 -4.1508675 -4.0627203 -4.0036988 -4.0043292 -4.0488133 -4.1085954 -4.1727924 -4.2328949 -4.2781067 -4.3109841][-4.2886481 -4.2976236 -4.2908807 -4.2486305 -4.177074 -4.0812354 -3.9655774 -3.8910475 -3.9106121 -3.9833627 -4.0658412 -4.1482906 -4.219821 -4.2724051 -4.308845][-4.2720375 -4.2814517 -4.2658324 -4.2120838 -4.1246138 -4.0047879 -3.8589041 -3.7746346 -3.8296676 -3.9353948 -4.0377307 -4.1345425 -4.2123284 -4.2688274 -4.3074794][-4.2570705 -4.2602954 -4.2351584 -4.1731544 -4.072217 -3.9298029 -3.7556486 -3.6770723 -3.7840037 -3.9232838 -4.0398707 -4.1383905 -4.2150488 -4.2711406 -4.3085489][-4.2398028 -4.2322912 -4.2017069 -4.1410213 -4.0446815 -3.90865 -3.7515037 -3.7167935 -3.8502464 -3.9858029 -4.0858192 -4.1656904 -4.2311354 -4.2801776 -4.3125563][-4.2204318 -4.2055855 -4.1774616 -4.1345458 -4.0658593 -3.9701176 -3.8725302 -3.880604 -3.986762 -4.0822792 -4.146275 -4.1986117 -4.24944 -4.2910156 -4.3182817][-4.2051005 -4.1869264 -4.1645103 -4.140008 -4.0999713 -4.0442276 -3.9948368 -4.0156503 -4.0853362 -4.1446028 -4.1797485 -4.2124944 -4.2566037 -4.2973924 -4.3229651][-4.1932974 -4.1755476 -4.1596251 -4.145504 -4.1253729 -4.0968719 -4.0759997 -4.0972977 -4.1407638 -4.1757317 -4.1942606 -4.2201653 -4.2628012 -4.3029628 -4.3271461][-4.1925888 -4.1817245 -4.1750774 -4.171412 -4.1647463 -4.1537075 -4.1478734 -4.1634655 -4.18389 -4.1976991 -4.2061863 -4.2297978 -4.2707367 -4.3087387 -4.33058][-4.211256 -4.2096457 -4.2122593 -4.2151442 -4.2145681 -4.2096272 -4.2048922 -4.209846 -4.2103033 -4.2098255 -4.21238 -4.2343292 -4.274807 -4.3113365 -4.3317981][-4.2296786 -4.2366805 -4.245039 -4.2487917 -4.247098 -4.2397513 -4.2274132 -4.2207642 -4.2107816 -4.204556 -4.2056503 -4.2284408 -4.270997 -4.3082485 -4.329648]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fix/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fix/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 06:52:11.249834: step 10, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 83h:41m:00s remains)
INFO - root - 2017-12-06 06:52:20.550319: step 20, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 84h:52m:25s remains)
INFO - root - 2017-12-06 06:52:29.624626: step 30, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.936 sec/batch; 86h:26m:39s remains)
INFO - root - 2017-12-06 06:52:38.810135: step 40, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 85h:35m:01s remains)
INFO - root - 2017-12-06 06:52:47.787574: step 50, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.904 sec/batch; 83h:26m:50s remains)
INFO - root - 2017-12-06 06:52:57.094300: step 60, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 80h:43m:05s remains)
INFO - root - 2017-12-06 06:53:06.242474: step 70, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 84h:55m:45s remains)
INFO - root - 2017-12-06 06:53:15.522934: step 80, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 85h:35m:32s remains)
INFO - root - 2017-12-06 06:53:24.656255: step 90, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.978 sec/batch; 90h:18m:03s remains)
INFO - root - 2017-12-06 06:53:33.860997: step 100, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 84h:54m:55s remains)
2017-12-06 06:53:34.623863: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2507062 -4.22276 -4.1936889 -4.1672287 -4.153398 -4.1616688 -4.1811223 -4.1992688 -4.2159548 -4.2246866 -4.2210746 -4.2242622 -4.2251019 -4.2091694 -4.1905322][-4.261117 -4.226151 -4.1844306 -4.1427217 -4.1151142 -4.1158032 -4.1361089 -4.1538968 -4.1678095 -4.1781626 -4.1822252 -4.1930761 -4.1994796 -4.1875377 -4.1733618][-4.2661791 -4.2258406 -4.1771622 -4.1298614 -4.0973058 -4.0927391 -4.1112871 -4.1282978 -4.1399341 -4.1544247 -4.1651597 -4.1750307 -4.1789284 -4.1672883 -4.1532903][-4.26627 -4.2233281 -4.1744962 -4.1282806 -4.0974097 -4.0898871 -4.1056609 -4.1222839 -4.13439 -4.1539702 -4.1703887 -4.1762729 -4.1750045 -4.1619849 -4.1452641][-4.2642918 -4.2208405 -4.1744046 -4.1314111 -4.1019683 -4.0893641 -4.0993466 -4.1123447 -4.1220441 -4.1419759 -4.1617293 -4.1680493 -4.1670537 -4.1579928 -4.1433024][-4.2612724 -4.2201881 -4.17873 -4.1403322 -4.1098022 -4.0882874 -4.0870333 -4.0860405 -4.0828981 -4.0986814 -4.126174 -4.1399789 -4.1447015 -4.1412106 -4.1308169][-4.2543979 -4.2140779 -4.1728954 -4.131784 -4.0960855 -4.06796 -4.0585375 -4.0451589 -4.0243616 -4.0349288 -4.0726681 -4.0952816 -4.1055622 -4.1091003 -4.1064076][-4.2490816 -4.2079763 -4.1641436 -4.1181059 -4.0787992 -4.0511017 -4.0401907 -4.0213327 -3.9904447 -3.9988735 -4.0404296 -4.0640755 -4.073401 -4.0827932 -4.0875][-4.25058 -4.2116103 -4.1683421 -4.121716 -4.0858874 -4.0657797 -4.0577188 -4.0404015 -4.012249 -4.0208712 -4.0552049 -4.0702138 -4.0704918 -4.0763946 -4.0816765][-4.2556577 -4.21838 -4.1764846 -4.1303306 -4.0976167 -4.08365 -4.0779724 -4.0635061 -4.0462971 -4.0605845 -4.0865 -4.0889449 -4.0782204 -4.0775952 -4.0812697][-4.266942 -4.2308407 -4.190382 -4.1467786 -4.1180267 -4.109901 -4.1044321 -4.0891566 -4.0787115 -4.0974741 -4.1192875 -4.1143951 -4.0975981 -4.0925541 -4.0944529][-4.2818975 -4.2475948 -4.2094688 -4.1708603 -4.149303 -4.1485162 -4.1472697 -4.1364489 -4.1307645 -4.1455669 -4.1596355 -4.1525912 -4.1364059 -4.1315289 -4.133039][-4.2952 -4.2638059 -4.23015 -4.1979508 -4.1828766 -4.186687 -4.1927314 -4.1923447 -4.1931825 -4.20278 -4.2099051 -4.203371 -4.19005 -4.1840792 -4.1831994][-4.311697 -4.2874746 -4.2618251 -4.2376456 -4.2253685 -4.2283354 -4.2380605 -4.2474871 -4.2551823 -4.2622395 -4.2638397 -4.25665 -4.2457728 -4.2399487 -4.2383475][-4.3297963 -4.3148246 -4.2992535 -4.2840514 -4.2745204 -4.2746167 -4.2823691 -4.2926836 -4.300849 -4.3052239 -4.3047328 -4.2993636 -4.2926707 -4.289732 -4.2903304]]...]
INFO - root - 2017-12-06 06:53:43.719907: step 110, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 83h:50m:45s remains)
INFO - root - 2017-12-06 06:53:52.707129: step 120, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 85h:21m:18s remains)
INFO - root - 2017-12-06 06:54:01.849798: step 130, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 85h:57m:57s remains)
INFO - root - 2017-12-06 06:54:11.001337: step 140, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 83h:48m:40s remains)
INFO - root - 2017-12-06 06:54:20.281687: step 150, loss = 2.10, batch loss = 2.05 (8.7 examples/sec; 0.919 sec/batch; 84h:50m:23s remains)
INFO - root - 2017-12-06 06:54:29.235768: step 160, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 83h:50m:10s remains)
INFO - root - 2017-12-06 06:54:38.407317: step 170, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.891 sec/batch; 82h:17m:13s remains)
INFO - root - 2017-12-06 06:54:47.673957: step 180, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 86h:37m:29s remains)
INFO - root - 2017-12-06 06:54:56.749319: step 190, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 83h:33m:07s remains)
INFO - root - 2017-12-06 06:55:05.939814: step 200, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 82h:37m:49s remains)
2017-12-06 06:55:06.681693: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3126144 -4.2782435 -4.2457929 -4.2285066 -4.2392707 -4.2546134 -4.2565937 -4.2533178 -4.2429757 -4.2291021 -4.2178121 -4.2273 -4.2423067 -4.2516236 -4.2556872][-4.30594 -4.2760415 -4.2547507 -4.24722 -4.2652426 -4.2875209 -4.2887077 -4.2780685 -4.2587304 -4.2387438 -4.222384 -4.2219143 -4.2248263 -4.224617 -4.2223482][-4.2879839 -4.2625237 -4.2538676 -4.257658 -4.284452 -4.3123794 -4.3156943 -4.2980371 -4.2703719 -4.2468781 -4.2277079 -4.2200623 -4.2146497 -4.206399 -4.1961379][-4.2658415 -4.2457228 -4.248702 -4.2606521 -4.2889776 -4.319231 -4.3240395 -4.2987285 -4.2635813 -4.2404146 -4.2252297 -4.2216711 -4.2210736 -4.2151003 -4.202775][-4.2522078 -4.2355814 -4.2415237 -4.255362 -4.2795773 -4.3028283 -4.3012834 -4.2690792 -4.2354679 -4.2229657 -4.2183437 -4.2257853 -4.2361856 -4.238718 -4.2303252][-4.2503176 -4.2319012 -4.2276917 -4.2337332 -4.2452693 -4.2489233 -4.2277622 -4.18339 -4.1681032 -4.191608 -4.2138643 -4.2371044 -4.2537961 -4.2636395 -4.2609787][-4.2645183 -4.2436767 -4.2253876 -4.2143259 -4.1966429 -4.1600761 -4.0934625 -4.0288854 -4.0503259 -4.1355586 -4.2010851 -4.2465038 -4.2715735 -4.2852798 -4.2847061][-4.2867165 -4.2656708 -4.2391186 -4.2065511 -4.1485028 -4.0502844 -3.91003 -3.8034091 -3.8791423 -4.0455532 -4.1634293 -4.2396407 -4.2833328 -4.3049631 -4.297843][-4.3042159 -4.2848835 -4.2515211 -4.202539 -4.1162696 -3.9792533 -3.80126 -3.6806564 -3.7917542 -3.9951108 -4.1429009 -4.2394452 -4.2948766 -4.3202105 -4.301106][-4.3103747 -4.2921023 -4.2535067 -4.196579 -4.1123304 -3.9977703 -3.86607 -3.7895131 -3.8724332 -4.0358105 -4.1641092 -4.2501941 -4.2994351 -4.3174038 -4.2865124][-4.3024516 -4.28501 -4.24996 -4.1983027 -4.1315179 -4.058311 -3.9894681 -3.9566743 -4.0108037 -4.1211596 -4.217308 -4.2790937 -4.307693 -4.3058591 -4.2622705][-4.2923388 -4.2746229 -4.2501941 -4.2108283 -4.1643829 -4.1266012 -4.1006379 -4.097178 -4.13736 -4.2097783 -4.2783985 -4.3155937 -4.3194561 -4.2955122 -4.2381105][-4.2908716 -4.2722692 -4.2567945 -4.2361507 -4.2141066 -4.2008038 -4.19811 -4.2105575 -4.2411456 -4.28829 -4.3291969 -4.3402109 -4.3239169 -4.2855287 -4.217617][-4.2930517 -4.274384 -4.2623744 -4.2607851 -4.264925 -4.2708378 -4.2802687 -4.295465 -4.3139544 -4.3361096 -4.3521528 -4.3449521 -4.32013 -4.2788181 -4.211884][-4.29762 -4.2809505 -4.2708039 -4.27513 -4.2915015 -4.308701 -4.3239546 -4.3377681 -4.3448396 -4.3495588 -4.3509707 -4.3363867 -4.3121319 -4.2770667 -4.222003]]...]
INFO - root - 2017-12-06 06:55:15.770762: step 210, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.948 sec/batch; 87h:27m:25s remains)
INFO - root - 2017-12-06 06:55:24.933802: step 220, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.877 sec/batch; 80h:54m:23s remains)
INFO - root - 2017-12-06 06:55:33.967867: step 230, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 85h:04m:35s remains)
INFO - root - 2017-12-06 06:55:43.173360: step 240, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.896 sec/batch; 82h:41m:38s remains)
INFO - root - 2017-12-06 06:55:52.340727: step 250, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 83h:35m:38s remains)
INFO - root - 2017-12-06 06:56:01.300138: step 260, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 81h:51m:42s remains)
INFO - root - 2017-12-06 06:56:10.521762: step 270, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.897 sec/batch; 82h:45m:53s remains)
INFO - root - 2017-12-06 06:56:19.824822: step 280, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 85h:03m:26s remains)
INFO - root - 2017-12-06 06:56:28.803809: step 290, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 85h:25m:14s remains)
INFO - root - 2017-12-06 06:56:37.989356: step 300, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 85h:12m:46s remains)
2017-12-06 06:56:38.747057: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1937923 -4.1870074 -4.1636758 -4.1467319 -4.145062 -4.1568174 -4.1682019 -4.1708345 -4.1627221 -4.1615829 -4.1766062 -4.1950502 -4.2000551 -4.1813865 -4.1573586][-4.179306 -4.178196 -4.1719847 -4.1684761 -4.16446 -4.1648569 -4.1645188 -4.1591716 -4.1559067 -4.1636677 -4.1801038 -4.1947589 -4.1947432 -4.1770325 -4.1622128][-4.1579204 -4.1643367 -4.1768074 -4.1896653 -4.1884985 -4.1785355 -4.165638 -4.1535492 -4.1516442 -4.1632996 -4.1777787 -4.1872807 -4.1830907 -4.168292 -4.1634436][-4.1334572 -4.1450529 -4.1733294 -4.1999378 -4.2033558 -4.18711 -4.1650329 -4.1449838 -4.1359425 -4.1390848 -4.1455894 -4.1525922 -4.1546316 -4.1555972 -4.165844][-4.1271472 -4.1379471 -4.171144 -4.2082448 -4.2165365 -4.1948934 -4.1620283 -4.1284714 -4.1005788 -4.0846472 -4.0823879 -4.0956726 -4.114264 -4.1365848 -4.1628389][-4.1500292 -4.1537762 -4.1799707 -4.21532 -4.223608 -4.1978297 -4.1527276 -4.0984912 -4.0475097 -4.0098596 -3.9971471 -4.0179839 -4.0570016 -4.102838 -4.1468863][-4.1832929 -4.1828785 -4.1994214 -4.224493 -4.227232 -4.1980205 -4.1432581 -4.0762291 -4.0124702 -3.9591191 -3.9321361 -3.945478 -3.9912949 -4.0594087 -4.1278853][-4.205081 -4.2037377 -4.2155032 -4.2332578 -4.237237 -4.2145648 -4.1640368 -4.0985923 -4.0375652 -3.9800477 -3.9402723 -3.931392 -3.9611645 -4.0319028 -4.112936][-4.2315416 -4.2280121 -4.2353253 -4.2465725 -4.25103 -4.238584 -4.2026105 -4.1521053 -4.105021 -4.0551524 -4.0096564 -3.9807296 -3.9827027 -4.0350041 -4.1113324][-4.2672529 -4.2641354 -4.2664037 -4.2705045 -4.2726436 -4.2677431 -4.2483888 -4.2164855 -4.1833758 -4.1403732 -4.0922213 -4.0537682 -4.0375166 -4.0650845 -4.1186495][-4.293622 -4.2963662 -4.2998085 -4.3030624 -4.3047996 -4.3042321 -4.2958074 -4.2773061 -4.2519522 -4.2136393 -4.1667972 -4.1259456 -4.0981536 -4.1015444 -4.1261973][-4.2975 -4.3068962 -4.3148179 -4.3198485 -4.3221693 -4.3245592 -4.3220825 -4.3099895 -4.2903271 -4.2586842 -4.2200675 -4.1846676 -4.1542788 -4.141459 -4.1399693][-4.2802987 -4.2965913 -4.3089924 -4.3171172 -4.3236523 -4.331531 -4.3326669 -4.3246574 -4.3111339 -4.2877097 -4.2593637 -4.2308693 -4.202188 -4.1803913 -4.1597323][-4.2741971 -4.2934961 -4.3068681 -4.3145881 -4.3209515 -4.3309073 -4.3354487 -4.3312988 -4.3231387 -4.30774 -4.2896976 -4.2716794 -4.2466192 -4.2184949 -4.1847582][-4.2811847 -4.3008256 -4.3123322 -4.315784 -4.3179874 -4.3234982 -4.3298774 -4.3286476 -4.322289 -4.313695 -4.3030405 -4.294291 -4.2775869 -4.2521305 -4.2135429]]...]
INFO - root - 2017-12-06 06:56:47.795821: step 310, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 85h:57m:08s remains)
INFO - root - 2017-12-06 06:56:57.095639: step 320, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.950 sec/batch; 87h:37m:31s remains)
INFO - root - 2017-12-06 06:57:06.100376: step 330, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 83h:46m:40s remains)
INFO - root - 2017-12-06 06:57:15.321174: step 340, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 84h:12m:40s remains)
INFO - root - 2017-12-06 06:57:24.387464: step 350, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.894 sec/batch; 82h:29m:58s remains)
INFO - root - 2017-12-06 06:57:33.397409: step 360, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.925 sec/batch; 85h:20m:56s remains)
INFO - root - 2017-12-06 06:57:42.524805: step 370, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 85h:50m:13s remains)
INFO - root - 2017-12-06 06:57:51.709993: step 380, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.948 sec/batch; 87h:25m:59s remains)
INFO - root - 2017-12-06 06:58:00.920353: step 390, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 79h:18m:50s remains)
INFO - root - 2017-12-06 06:58:10.042960: step 400, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 78h:03m:00s remains)
2017-12-06 06:58:10.612659: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1992617 -4.1884122 -4.1830583 -4.186893 -4.1836734 -4.1894808 -4.2088985 -4.2190304 -4.2240324 -4.2483754 -4.2779794 -4.2911334 -4.2952361 -4.2976952 -4.296526][-4.1889043 -4.1953526 -4.1925325 -4.1926885 -4.1870823 -4.1865907 -4.1968474 -4.203474 -4.2105365 -4.2396421 -4.2733712 -4.2881274 -4.2916188 -4.2915483 -4.2871189][-4.15719 -4.1819549 -4.1900625 -4.1927791 -4.1856327 -4.1789184 -4.1742096 -4.1757283 -4.1891766 -4.2240095 -4.2597308 -4.2773142 -4.2816167 -4.2808647 -4.2747903][-4.1462927 -4.1743116 -4.1842132 -4.1855526 -4.1770034 -4.1643162 -4.1416917 -4.1344957 -4.1601944 -4.2045655 -4.2419415 -4.2634106 -4.2676563 -4.2640095 -4.2613263][-4.1822066 -4.196599 -4.1898942 -4.1762872 -4.162343 -4.1371775 -4.0934205 -4.0783143 -4.1284828 -4.1853409 -4.2210436 -4.24354 -4.2435861 -4.2326083 -4.23205][-4.2136121 -4.2044435 -4.1754084 -4.1456962 -4.1227589 -4.0748591 -3.9989378 -3.9807625 -4.0736952 -4.15057 -4.18392 -4.2076073 -4.2032657 -4.1830373 -4.1819472][-4.1996217 -4.1770234 -4.1338854 -4.0937872 -4.0593691 -3.9797997 -3.8602991 -3.8351166 -3.9724586 -4.0745487 -4.1101527 -4.1394019 -4.137845 -4.1147861 -4.1118765][-4.1775932 -4.1559429 -4.1140308 -4.0719175 -4.0319557 -3.9409976 -3.8012371 -3.7579575 -3.8912573 -3.9942567 -4.0355487 -4.0693541 -4.0705342 -4.0472336 -4.0416284][-4.1880879 -4.1735573 -4.1393375 -4.1028671 -4.0708427 -4.004221 -3.9016569 -3.86008 -3.9377205 -4.0047388 -4.0343142 -4.0645971 -4.0682554 -4.0444131 -4.03204][-4.2293897 -4.2230444 -4.1997671 -4.1681638 -4.1392994 -4.1012907 -4.0465631 -4.0214686 -4.0570421 -4.08854 -4.1058564 -4.1316895 -4.1391492 -4.1217794 -4.1097713][-4.280406 -4.2772822 -4.2639966 -4.2345524 -4.2006707 -4.174881 -4.1514564 -4.1404819 -4.155642 -4.174758 -4.1906052 -4.2146554 -4.2247543 -4.2173781 -4.21148][-4.3103137 -4.307055 -4.2994394 -4.2735782 -4.2354279 -4.2129283 -4.2045293 -4.200942 -4.2067523 -4.2237148 -4.2407236 -4.2611685 -4.2718387 -4.2712183 -4.2691426][-4.2991595 -4.2972226 -4.2955928 -4.2781267 -4.2412195 -4.219646 -4.2157845 -4.2098265 -4.2069035 -4.2206669 -4.23627 -4.2540183 -4.2647834 -4.26662 -4.2663784][-4.2599683 -4.2647028 -4.271904 -4.2653484 -4.236958 -4.2176976 -4.2128429 -4.2021327 -4.1892586 -4.1915889 -4.2005548 -4.2159438 -4.2263274 -4.2253418 -4.2241735][-4.2096477 -4.2225947 -4.2389054 -4.2452044 -4.2302756 -4.2175817 -4.2152061 -4.2039785 -4.1842208 -4.1718488 -4.1681504 -4.1762342 -4.1821356 -4.1755767 -4.1705322]]...]
INFO - root - 2017-12-06 06:58:19.872459: step 410, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 86h:02m:29s remains)
INFO - root - 2017-12-06 06:58:28.903120: step 420, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 77h:10m:01s remains)
INFO - root - 2017-12-06 06:58:38.021345: step 430, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 86h:49m:32s remains)
INFO - root - 2017-12-06 06:58:47.185603: step 440, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 81h:13m:39s remains)
INFO - root - 2017-12-06 06:58:56.389890: step 450, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 82h:23m:07s remains)
INFO - root - 2017-12-06 06:59:05.392233: step 460, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.878 sec/batch; 81h:00m:03s remains)
INFO - root - 2017-12-06 06:59:14.574518: step 470, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 83h:00m:53s remains)
INFO - root - 2017-12-06 06:59:23.734012: step 480, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.936 sec/batch; 86h:21m:31s remains)
INFO - root - 2017-12-06 06:59:32.752198: step 490, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 86h:52m:04s remains)
INFO - root - 2017-12-06 06:59:41.910464: step 500, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.904 sec/batch; 83h:22m:43s remains)
2017-12-06 06:59:42.515969: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2219172 -4.2141342 -4.2063909 -4.1892362 -4.1793394 -4.1818886 -4.1866326 -4.1947908 -4.2039127 -4.2021408 -4.1870718 -4.1584516 -4.1249294 -4.0963287 -4.0741653][-4.1886954 -4.1798482 -4.1636319 -4.1414714 -4.1303835 -4.1348734 -4.1490893 -4.1747651 -4.1962981 -4.1927395 -4.1663413 -4.1282091 -4.0936327 -4.0693884 -4.0506282][-4.1684022 -4.1517353 -4.1215034 -4.0902662 -4.07862 -4.0916686 -4.1201134 -4.1613994 -4.1902242 -4.1786537 -4.1413889 -4.1010952 -4.0681496 -4.04585 -4.0344653][-4.1550884 -4.1266122 -4.082046 -4.0436287 -4.033771 -4.0519991 -4.0886974 -4.1400404 -4.1728497 -4.1556458 -4.1151714 -4.08108 -4.0514803 -4.0324335 -4.0352497][-4.1403255 -4.10186 -4.0505853 -4.0110574 -3.9968121 -4.0080013 -4.0403891 -4.0895104 -4.1222968 -4.1029348 -4.0680418 -4.0480075 -4.0324869 -4.0308185 -4.0511074][-4.1348815 -4.0984268 -4.0467153 -3.9995768 -3.9613976 -3.9431846 -3.9510262 -3.9871907 -4.0232329 -4.0139875 -3.9947062 -4.0017157 -4.010447 -4.0279064 -4.0573983][-4.1286659 -4.107305 -4.0644507 -4.0091367 -3.9391816 -3.8707247 -3.827086 -3.8464112 -3.9078245 -3.9346273 -3.9456234 -3.9798424 -4.002933 -4.0254641 -4.0577493][-4.1080337 -4.0964465 -4.0669894 -4.0151329 -3.9377644 -3.8441312 -3.7593746 -3.7621903 -3.8490686 -3.909579 -3.9496939 -3.9997859 -4.0271649 -4.0417132 -4.0649781][-4.0856419 -4.0690923 -4.0479059 -4.0137935 -3.9620659 -3.8945115 -3.8256793 -3.8262293 -3.8953815 -3.9468248 -3.9918928 -4.0476484 -4.0739841 -4.0782113 -4.0870709][-4.0695539 -4.0567884 -4.0494905 -4.0396061 -4.0167785 -3.9830465 -3.9452369 -3.9466155 -3.9811218 -4.0031185 -4.0342445 -4.0833797 -4.1067472 -4.1039357 -4.099721][-4.0737681 -4.0806513 -4.0886655 -4.0900712 -4.0788822 -4.0636673 -4.0437632 -4.0425372 -4.0534673 -4.046988 -4.0534172 -4.0881324 -4.1107178 -4.1065507 -4.0976639][-4.1084094 -4.1254244 -4.1355295 -4.1309032 -4.1171908 -4.1052871 -4.0886974 -4.0805421 -4.0804944 -4.0642524 -4.058032 -4.0818071 -4.1053996 -4.1110783 -4.1125193][-4.1415029 -4.1560283 -4.1611505 -4.1488223 -4.1331506 -4.1218247 -4.1046596 -4.0930448 -4.0911927 -4.0816441 -4.0785913 -4.0958138 -4.1157293 -4.1300397 -4.1408834][-4.1627512 -4.171958 -4.1739798 -4.1612124 -4.1478057 -4.1382828 -4.1241183 -4.1183968 -4.1218929 -4.1213202 -4.1205387 -4.124011 -4.133625 -4.1484556 -4.1610007][-4.1708183 -4.1807518 -4.184761 -4.1764903 -4.1682005 -4.1588926 -4.149859 -4.15315 -4.161725 -4.1660991 -4.1627431 -4.1534743 -4.1531215 -4.1615586 -4.1693239]]...]
INFO - root - 2017-12-06 06:59:51.580933: step 510, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.939 sec/batch; 86h:34m:03s remains)
INFO - root - 2017-12-06 07:00:00.978989: step 520, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 88h:26m:14s remains)
INFO - root - 2017-12-06 07:00:10.362564: step 530, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.907 sec/batch; 83h:40m:01s remains)
INFO - root - 2017-12-06 07:00:19.437336: step 540, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.958 sec/batch; 88h:20m:18s remains)
INFO - root - 2017-12-06 07:00:28.419591: step 550, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 84h:59m:13s remains)
INFO - root - 2017-12-06 07:00:37.457233: step 560, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.949 sec/batch; 87h:28m:54s remains)
INFO - root - 2017-12-06 07:00:46.490095: step 570, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.904 sec/batch; 83h:21m:28s remains)
INFO - root - 2017-12-06 07:00:55.581218: step 580, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 82h:57m:04s remains)
INFO - root - 2017-12-06 07:01:04.699575: step 590, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 83h:35m:24s remains)
INFO - root - 2017-12-06 07:01:13.823515: step 600, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.914 sec/batch; 84h:18m:34s remains)
2017-12-06 07:01:14.464237: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1954226 -4.1836782 -4.1758084 -4.167522 -4.1607051 -4.1570692 -4.1722088 -4.2104278 -4.246366 -4.2660246 -4.2740784 -4.2626781 -4.2341533 -4.2178779 -4.2243061][-4.1898313 -4.1810632 -4.1773562 -4.1690874 -4.1605897 -4.1582007 -4.1764779 -4.2162104 -4.248889 -4.2630119 -4.2675328 -4.2598605 -4.2420378 -4.2339969 -4.2444806][-4.1757474 -4.1663628 -4.1694341 -4.1722379 -4.171844 -4.1755772 -4.1967082 -4.2327776 -4.2556443 -4.2596116 -4.2557716 -4.2459879 -4.2320738 -4.229095 -4.2460284][-4.1543179 -4.143621 -4.1534724 -4.1666412 -4.1693683 -4.1691785 -4.1843095 -4.21129 -4.2234426 -4.2228551 -4.2201934 -4.2156334 -4.2042127 -4.2049131 -4.2281637][-4.137907 -4.1304502 -4.1390109 -4.1528649 -4.1487656 -4.1343884 -4.1359735 -4.1548123 -4.1679678 -4.1772633 -4.1871748 -4.1881723 -4.1767378 -4.1740236 -4.1944675][-4.1158519 -4.1144524 -4.1178627 -4.1228919 -4.1057572 -4.0758972 -4.0682154 -4.092823 -4.1233959 -4.1512456 -4.1709671 -4.1708946 -4.1528168 -4.1358852 -4.1433182][-4.1076727 -4.1103296 -4.1015821 -4.0803642 -4.039248 -3.993062 -3.9815142 -4.0208478 -4.0736475 -4.114285 -4.1346631 -4.1287913 -4.098362 -4.0714393 -4.0753188][-4.1144214 -4.1197896 -4.1015973 -4.0571108 -3.9963157 -3.9407082 -3.9301722 -3.9752243 -4.0332828 -4.0642109 -4.072782 -4.0613551 -4.0262446 -4.001555 -4.0194893][-4.1273217 -4.1315851 -4.1158609 -4.0713363 -4.0134215 -3.9663022 -3.95749 -3.9892898 -4.0267076 -4.0328484 -4.02289 -4.0065756 -3.9756775 -3.96553 -4.0047474][-4.1567616 -4.1606908 -4.1513119 -4.1177754 -4.0748639 -4.0436716 -4.0357022 -4.0472932 -4.0596414 -4.0478668 -4.0269871 -4.0076447 -3.9799507 -3.9798353 -4.0286436][-4.1934795 -4.1957078 -4.1891341 -4.1674676 -4.1416869 -4.1278071 -4.1252546 -4.1271677 -4.1276369 -4.1112919 -4.0881186 -4.0666404 -4.0404358 -4.0424409 -4.0826578][-4.2215514 -4.2149982 -4.2066588 -4.1934066 -4.1830649 -4.1847739 -4.1930923 -4.1972771 -4.1956878 -4.1816268 -4.1616964 -4.1418991 -4.1212811 -4.1221948 -4.146749][-4.2264442 -4.2138286 -4.203927 -4.1993475 -4.2020206 -4.2185245 -4.2373676 -4.2466216 -4.2466612 -4.2372718 -4.2232018 -4.2082586 -4.1947489 -4.194643 -4.2075219][-4.2150755 -4.2022052 -4.1911707 -4.1925583 -4.2057824 -4.2313032 -4.2557592 -4.2688947 -4.2718129 -4.2660632 -4.2562733 -4.2460184 -4.2363796 -4.2350783 -4.2430811][-4.2111912 -4.19903 -4.1844745 -4.18755 -4.2065039 -4.2335491 -4.2563138 -4.2686119 -4.272253 -4.269259 -4.2626729 -4.2558064 -4.2488508 -4.2467942 -4.252027]]...]
INFO - root - 2017-12-06 07:01:23.735287: step 610, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 87h:31m:07s remains)
INFO - root - 2017-12-06 07:01:32.705413: step 620, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.747 sec/batch; 68h:51m:31s remains)
INFO - root - 2017-12-06 07:01:41.900152: step 630, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 83h:30m:39s remains)
INFO - root - 2017-12-06 07:01:51.103546: step 640, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 81h:20m:36s remains)
INFO - root - 2017-12-06 07:02:00.386379: step 650, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.986 sec/batch; 90h:54m:35s remains)
INFO - root - 2017-12-06 07:02:09.754228: step 660, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.914 sec/batch; 84h:17m:45s remains)
INFO - root - 2017-12-06 07:02:18.857349: step 670, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.903 sec/batch; 83h:11m:55s remains)
INFO - root - 2017-12-06 07:02:28.185914: step 680, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.916 sec/batch; 84h:26m:47s remains)
INFO - root - 2017-12-06 07:02:37.146495: step 690, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 88h:31m:07s remains)
INFO - root - 2017-12-06 07:02:46.401566: step 700, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 83h:35m:46s remains)
2017-12-06 07:02:47.055109: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.203136 -4.2003188 -4.1985264 -4.1973529 -4.1976242 -4.1980762 -4.1972604 -4.1963134 -4.1974816 -4.2009134 -4.2036691 -4.1990352 -4.1853781 -4.1653924 -4.1518831][-4.201479 -4.1951885 -4.1890116 -4.18267 -4.1777158 -4.1730094 -4.1671562 -4.1618118 -4.16084 -4.1650872 -4.1692567 -4.16485 -4.1513104 -4.1307888 -4.1182494][-4.2232876 -4.2147951 -4.2061853 -4.1964941 -4.18822 -4.1799703 -4.1699162 -4.1601863 -4.1559873 -4.1581426 -4.1605954 -4.1553617 -4.1448135 -4.1270566 -4.1163177][-4.244483 -4.2351241 -4.2260137 -4.2164006 -4.2092471 -4.2018113 -4.19066 -4.1796374 -4.1749039 -4.176846 -4.1782007 -4.173121 -4.1654463 -4.1506763 -4.1410136][-4.2336245 -4.223002 -4.2127461 -4.205905 -4.2040758 -4.2017574 -4.1938109 -4.1865544 -4.1868467 -4.1947594 -4.2005849 -4.198545 -4.1929412 -4.1779609 -4.1657534][-4.197515 -4.1809249 -4.1665759 -4.163372 -4.16864 -4.1710114 -4.1653576 -4.1622462 -4.1715293 -4.1909328 -4.207665 -4.2139955 -4.2127247 -4.198236 -4.1837969][-4.1598763 -4.13466 -4.1167707 -4.1173415 -4.1267405 -4.1282115 -4.1184087 -4.1149311 -4.1321592 -4.1647162 -4.1956697 -4.2146959 -4.2214689 -4.2102065 -4.1961842][-4.1409721 -4.1139522 -4.0964403 -4.0991287 -4.1075172 -4.1006193 -4.0796485 -4.0694146 -4.08816 -4.1289124 -4.1713691 -4.2022867 -4.2180281 -4.2123704 -4.2009153][-4.1417642 -4.1228013 -4.1111851 -4.1164551 -4.121985 -4.1097012 -4.0818944 -4.0637789 -4.0759783 -4.1138563 -4.1558189 -4.1882591 -4.2065949 -4.2050166 -4.19628][-4.1567426 -4.1478462 -4.1422925 -4.1493182 -4.1546788 -4.1447892 -4.1235862 -4.1103382 -4.1178846 -4.1448345 -4.1764216 -4.19916 -4.2097278 -4.2033858 -4.1927032][-4.1829915 -4.1781082 -4.1724277 -4.1752548 -4.1763358 -4.1677389 -4.1548576 -4.1513824 -4.1620893 -4.1839046 -4.2084165 -4.2230463 -4.2248387 -4.21095 -4.1950397][-4.213995 -4.2065587 -4.196188 -4.1921797 -4.1870561 -4.1760054 -4.1656303 -4.1678138 -4.1815434 -4.2018008 -4.2229042 -4.2343364 -4.2325773 -4.215044 -4.1965861][-4.2337832 -4.2252979 -4.2141767 -4.2086763 -4.2039018 -4.1942143 -4.1846118 -4.1857324 -4.1954284 -4.2097473 -4.224236 -4.231245 -4.2277026 -4.2114 -4.1964545][-4.2353649 -4.2269239 -4.2184796 -4.2166104 -4.2170014 -4.2135425 -4.2068548 -4.2053328 -4.2083979 -4.2141743 -4.22114 -4.2232375 -4.2168493 -4.2020478 -4.1922984][-4.230267 -4.2221088 -4.2145929 -4.2139874 -4.2160168 -4.2169404 -4.2155328 -4.2146082 -4.2146788 -4.2151542 -4.217082 -4.2173138 -4.2105603 -4.1980977 -4.192256]]...]
INFO - root - 2017-12-06 07:02:56.145724: step 710, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 82h:18m:20s remains)
INFO - root - 2017-12-06 07:03:05.292124: step 720, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.972 sec/batch; 89h:33m:57s remains)
INFO - root - 2017-12-06 07:03:14.532732: step 730, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 87h:45m:15s remains)
INFO - root - 2017-12-06 07:03:23.592739: step 740, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.901 sec/batch; 83h:02m:18s remains)
INFO - root - 2017-12-06 07:03:32.977712: step 750, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 81h:09m:43s remains)
INFO - root - 2017-12-06 07:03:41.844496: step 760, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.907 sec/batch; 83h:33m:59s remains)
INFO - root - 2017-12-06 07:03:51.043960: step 770, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 82h:28m:45s remains)
INFO - root - 2017-12-06 07:04:00.296014: step 780, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.956 sec/batch; 88h:04m:11s remains)
INFO - root - 2017-12-06 07:04:09.457940: step 790, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.954 sec/batch; 87h:55m:27s remains)
INFO - root - 2017-12-06 07:04:18.509120: step 800, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 87h:50m:59s remains)
2017-12-06 07:04:19.090413: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1385384 -4.1792603 -4.2010479 -4.1967258 -4.1761394 -4.1529655 -4.1370063 -4.137063 -4.1594248 -4.1759195 -4.1788783 -4.169826 -4.1710219 -4.1811185 -4.1893916][-4.1789155 -4.2228613 -4.2395554 -4.230711 -4.2129536 -4.1880803 -4.158669 -4.1491222 -4.1689653 -4.1826434 -4.1821918 -4.1734276 -4.17295 -4.1800909 -4.1921835][-4.2097516 -4.2526374 -4.2684288 -4.2623205 -4.2537956 -4.2354388 -4.2071333 -4.1906567 -4.2040353 -4.2138138 -4.2063127 -4.1924939 -4.1838388 -4.1848445 -4.200572][-4.2424474 -4.2766166 -4.289289 -4.2854466 -4.2760296 -4.2600031 -4.2347469 -4.2108331 -4.2153053 -4.2192283 -4.2038212 -4.1878839 -4.1764221 -4.1778789 -4.1997228][-4.2621522 -4.2773671 -4.2793522 -4.2740526 -4.2611346 -4.2451015 -4.2193203 -4.1879611 -4.1868887 -4.1903386 -4.1737509 -4.1610217 -4.1545038 -4.1600585 -4.1846085][-4.2566772 -4.2475624 -4.2297993 -4.2170753 -4.2009444 -4.1850252 -4.1575189 -4.1153913 -4.1157894 -4.1304765 -4.123096 -4.1172848 -4.1193881 -4.1305246 -4.1575074][-4.2208672 -4.1942682 -4.1611462 -4.1391897 -4.1215873 -4.1051631 -4.067708 -4.0059404 -4.0130725 -4.0516586 -4.0651255 -4.0729065 -4.0855165 -4.1045508 -4.1358757][-4.1796989 -4.1509562 -4.1182318 -4.1026931 -4.0923767 -4.0777473 -4.0302567 -3.9534802 -3.9659827 -4.0192575 -4.0495291 -4.0721955 -4.0933642 -4.1151786 -4.1454663][-4.1592026 -4.1461468 -4.135756 -4.1424747 -4.1433415 -4.1347418 -4.0971742 -4.0399323 -4.0468431 -4.0781307 -4.097693 -4.1171718 -4.1351962 -4.1479044 -4.1712427][-4.1751156 -4.1829486 -4.1955056 -4.2135849 -4.2197785 -4.2174196 -4.192544 -4.157722 -4.1566048 -4.1640997 -4.1672058 -4.1712723 -4.1759715 -4.1763525 -4.1942544][-4.2065792 -4.229073 -4.25183 -4.2713771 -4.2799006 -4.2816076 -4.2630615 -4.236434 -4.2313132 -4.228653 -4.2190275 -4.2069392 -4.1954608 -4.1892991 -4.2071128][-4.2267318 -4.25612 -4.2781334 -4.2927613 -4.3025556 -4.306572 -4.2892542 -4.2643008 -4.2581778 -4.2541046 -4.2407818 -4.2175517 -4.1973467 -4.19465 -4.2143707][-4.2253385 -4.254024 -4.2665434 -4.273807 -4.2856407 -4.2923841 -4.275929 -4.2532105 -4.2480083 -4.2442908 -4.2348132 -4.2150431 -4.1987219 -4.2021313 -4.22569][-4.2170444 -4.2365541 -4.2370605 -4.2385378 -4.2530637 -4.2625341 -4.2496452 -4.2331324 -4.2302089 -4.22849 -4.2241211 -4.2137308 -4.2083488 -4.2145462 -4.2363253][-4.2303753 -4.2405124 -4.23376 -4.23004 -4.2416968 -4.2492085 -4.2380152 -4.2234006 -4.220037 -4.2213635 -4.2204323 -4.2166047 -4.2184715 -4.2262645 -4.2423191]]...]
INFO - root - 2017-12-06 07:04:28.136837: step 810, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 84h:17m:29s remains)
INFO - root - 2017-12-06 07:04:37.067110: step 820, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.872 sec/batch; 80h:22m:00s remains)
INFO - root - 2017-12-06 07:04:46.048895: step 830, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 82h:17m:44s remains)
INFO - root - 2017-12-06 07:04:55.293473: step 840, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 85h:04m:53s remains)
INFO - root - 2017-12-06 07:05:04.392347: step 850, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 82h:24m:45s remains)
INFO - root - 2017-12-06 07:05:13.612872: step 860, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.957 sec/batch; 88h:10m:22s remains)
INFO - root - 2017-12-06 07:05:22.777151: step 870, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 83h:59m:32s remains)
INFO - root - 2017-12-06 07:05:32.118128: step 880, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.925 sec/batch; 85h:11m:21s remains)
INFO - root - 2017-12-06 07:05:41.015901: step 890, loss = 2.09, batch loss = 2.04 (9.0 examples/sec; 0.885 sec/batch; 81h:30m:36s remains)
INFO - root - 2017-12-06 07:05:50.151581: step 900, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 83h:48m:20s remains)
2017-12-06 07:05:50.823803: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2576809 -4.2619958 -4.2704115 -4.2748194 -4.2774243 -4.2783422 -4.2766786 -4.2787323 -4.2915878 -4.304338 -4.3108563 -4.3059897 -4.2890625 -4.2645922 -4.2398195][-4.2615819 -4.2663517 -4.2678127 -4.2630439 -4.2609944 -4.2625437 -4.2636352 -4.2702818 -4.2869177 -4.3024254 -4.3132644 -4.3149085 -4.3063612 -4.295125 -4.2806644][-4.2581205 -4.2688055 -4.2662215 -4.2503524 -4.2381315 -4.2350836 -4.2391114 -4.2518592 -4.272265 -4.2891417 -4.30095 -4.3066692 -4.3050122 -4.3035812 -4.3003721][-4.2563744 -4.2718811 -4.265532 -4.237452 -4.2118034 -4.2029357 -4.2130365 -4.2370191 -4.2629757 -4.2813411 -4.2912822 -4.2962375 -4.2955871 -4.298305 -4.30208][-4.2433648 -4.2612677 -4.2555094 -4.2199984 -4.1810007 -4.1588922 -4.17021 -4.2074528 -4.2445416 -4.2710252 -4.2835546 -4.2884088 -4.2848973 -4.2877841 -4.2949638][-4.2149181 -4.2372618 -4.2352324 -4.1993427 -4.1471114 -4.1024361 -4.1007271 -4.1461763 -4.2015285 -4.2446976 -4.267396 -4.2801151 -4.2797122 -4.280776 -4.2857556][-4.1759105 -4.2049632 -4.2085056 -4.17566 -4.1133504 -4.0398421 -4.008626 -4.05608 -4.1333971 -4.1973372 -4.2354207 -4.2601724 -4.2689114 -4.2721462 -4.2749367][-4.1430893 -4.1757174 -4.1852126 -4.1580954 -4.0949192 -3.9993083 -3.9263732 -3.9576023 -4.0527129 -4.136888 -4.1908455 -4.2274895 -4.2458735 -4.251832 -4.2517915][-4.1300211 -4.157074 -4.1679468 -4.1484952 -4.0955877 -3.9972863 -3.9011061 -3.9020791 -3.9868839 -4.0732379 -4.132762 -4.1717567 -4.1953683 -4.2072496 -4.2080755][-4.1458769 -4.1642628 -4.1780119 -4.1703672 -4.1365576 -4.0607471 -3.9765499 -3.9525664 -3.9913666 -4.0485754 -4.0942922 -4.1223383 -4.1415715 -4.1540723 -4.1580167][-4.1737289 -4.1836329 -4.1973796 -4.2005048 -4.1865883 -4.1429715 -4.0876303 -4.0546994 -4.054862 -4.0755839 -4.0951152 -4.105864 -4.1175146 -4.1286287 -4.1340814][-4.1942482 -4.1947541 -4.2061772 -4.2151523 -4.2137308 -4.1941957 -4.1648006 -4.1349859 -4.1165109 -4.1149631 -4.1171093 -4.116313 -4.1217732 -4.1322513 -4.138958][-4.2137356 -4.2114291 -4.2221222 -4.2302923 -4.2311296 -4.2229481 -4.2097158 -4.1909728 -4.1707635 -4.1603904 -4.1546111 -4.14892 -4.1500082 -4.1586537 -4.1651311][-4.2322083 -4.2317195 -4.2421207 -4.2486029 -4.2483535 -4.2445474 -4.2388144 -4.2284088 -4.216 -4.2070212 -4.199254 -4.19265 -4.1919494 -4.1986189 -4.2050557][-4.2340922 -4.2379045 -4.2493954 -4.2548618 -4.2525363 -4.24746 -4.2403345 -4.23275 -4.2244129 -4.2180095 -4.2114325 -4.2075009 -4.2084379 -4.21533 -4.2233453]]...]
INFO - root - 2017-12-06 07:05:59.992625: step 910, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.906 sec/batch; 83h:24m:37s remains)
INFO - root - 2017-12-06 07:06:09.250795: step 920, loss = 2.09, batch loss = 2.04 (8.9 examples/sec; 0.895 sec/batch; 82h:25m:03s remains)
INFO - root - 2017-12-06 07:06:18.414394: step 930, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.894 sec/batch; 82h:20m:53s remains)
INFO - root - 2017-12-06 07:06:27.557594: step 940, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.961 sec/batch; 88h:31m:38s remains)
INFO - root - 2017-12-06 07:06:36.668390: step 950, loss = 2.09, batch loss = 2.04 (8.8 examples/sec; 0.911 sec/batch; 83h:56m:35s remains)
INFO - root - 2017-12-06 07:06:45.632397: step 960, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 84h:15m:54s remains)
INFO - root - 2017-12-06 07:06:54.801176: step 970, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 86h:00m:53s remains)
INFO - root - 2017-12-06 07:07:03.967435: step 980, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 84h:33m:41s remains)
INFO - root - 2017-12-06 07:07:13.190771: step 990, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.939 sec/batch; 86h:29m:18s remains)
INFO - root - 2017-12-06 07:07:22.268366: step 1000, loss = 2.03, batch loss = 1.98 (8.6 examples/sec; 0.928 sec/batch; 85h:29m:01s remains)
2017-12-06 07:07:22.893200: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1052794 -4.1119637 -4.1336026 -4.1612039 -4.1830235 -4.1870365 -4.1713214 -4.1468368 -4.1399503 -4.1437821 -4.1591153 -4.1767273 -4.198113 -4.2270579 -4.2537041][-4.1112309 -4.1226649 -4.1433692 -4.1705418 -4.1913042 -4.1918206 -4.1717873 -4.1454897 -4.1433105 -4.1520596 -4.170557 -4.1910024 -4.2116246 -4.2382159 -4.2617989][-4.1014376 -4.1173048 -4.136138 -4.1612811 -4.1804023 -4.1774635 -4.15257 -4.1239128 -4.1234846 -4.1376839 -4.1623635 -4.1851592 -4.2061505 -4.2325325 -4.2582545][-4.1064782 -4.1181221 -4.1276579 -4.1422038 -4.1516905 -4.14279 -4.1149392 -4.0884809 -4.094398 -4.1149721 -4.1423793 -4.1685615 -4.1935229 -4.2235365 -4.2535186][-4.1328855 -4.13335 -4.1285391 -4.1260023 -4.11895 -4.0966873 -4.059732 -4.0343781 -4.0514922 -4.0851064 -4.1197619 -4.1532674 -4.1852808 -4.2183037 -4.2519774][-4.1664405 -4.1537871 -4.1337438 -4.1137009 -4.0872846 -4.0452743 -3.9920974 -3.9649186 -3.9972932 -4.0526886 -4.101789 -4.1430535 -4.1799846 -4.2164145 -4.2521253][-4.1842594 -4.1621456 -4.134624 -4.1045914 -4.0664539 -4.0112362 -3.9453511 -3.9177356 -3.9672217 -4.0431132 -4.1054225 -4.1509876 -4.187047 -4.221488 -4.2570829][-4.189733 -4.1666894 -4.1408858 -4.1121125 -4.0774784 -4.0284877 -3.9682269 -3.9445748 -3.9955285 -4.0685391 -4.1278453 -4.1692047 -4.200171 -4.2295747 -4.2634239][-4.186738 -4.1650338 -4.1464009 -4.1296878 -4.1122322 -4.083972 -4.0450444 -4.0271945 -4.058969 -4.1071978 -4.148458 -4.1789355 -4.2043929 -4.2309752 -4.2646656][-4.1809025 -4.1620917 -4.1502724 -4.1454759 -4.14375 -4.1344786 -4.1128387 -4.094964 -4.1051707 -4.1289549 -4.1525526 -4.1733303 -4.1967583 -4.2251024 -4.2612624][-4.1826124 -4.1656828 -4.156929 -4.1581607 -4.1645522 -4.1679788 -4.1591644 -4.1422672 -4.1408772 -4.1505394 -4.16264 -4.1771464 -4.2001667 -4.2293668 -4.2640948][-4.1921072 -4.1755624 -4.167501 -4.1718135 -4.1832423 -4.1950917 -4.1944494 -4.1795425 -4.1732411 -4.1764841 -4.183373 -4.1959066 -4.2190681 -4.24648 -4.2765365][-4.2124853 -4.1969085 -4.190073 -4.195931 -4.2083821 -4.22064 -4.2204561 -4.2053881 -4.1982055 -4.201705 -4.2100868 -4.2247133 -4.2491565 -4.2737412 -4.2977281][-4.2449727 -4.2326255 -4.2274885 -4.2322516 -4.2405238 -4.2483506 -4.2455959 -4.231338 -4.2266126 -4.2337446 -4.2463732 -4.2620611 -4.2831392 -4.3018475 -4.3185515][-4.2746863 -4.265945 -4.2621889 -4.2645888 -4.2697048 -4.2741261 -4.2707806 -4.2601485 -4.25842 -4.2665515 -4.2784629 -4.2920403 -4.3073907 -4.3196507 -4.3301558]]...]
INFO - root - 2017-12-06 07:07:31.935058: step 1010, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 85h:04m:56s remains)
INFO - root - 2017-12-06 07:07:40.912669: step 1020, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.975 sec/batch; 89h:45m:00s remains)
INFO - root - 2017-12-06 07:07:50.013010: step 1030, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 80h:04m:37s remains)
INFO - root - 2017-12-06 07:07:59.109445: step 1040, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 0.772 sec/batch; 71h:05m:46s remains)
INFO - root - 2017-12-06 07:08:08.292642: step 1050, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 85h:02m:58s remains)
INFO - root - 2017-12-06 07:08:17.501918: step 1060, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 84h:20m:59s remains)
INFO - root - 2017-12-06 07:08:26.744494: step 1070, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 82h:53m:49s remains)
INFO - root - 2017-12-06 07:08:35.940667: step 1080, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 83h:40m:14s remains)
INFO - root - 2017-12-06 07:08:45.040724: step 1090, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 85h:42m:35s remains)
INFO - root - 2017-12-06 07:08:54.140999: step 1100, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 82h:50m:34s remains)
2017-12-06 07:08:54.876408: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2011089 -4.159317 -4.1198759 -4.1001511 -4.0936685 -4.1175 -4.1680675 -4.2075157 -4.2303448 -4.24587 -4.2564416 -4.26269 -4.2734036 -4.282536 -4.289259][-4.1963072 -4.1529241 -4.0970936 -4.0468268 -4.0075812 -4.0261579 -4.1028609 -4.1684918 -4.2072988 -4.2317924 -4.2491164 -4.2579818 -4.2687044 -4.2819242 -4.2928605][-4.2099409 -4.1694565 -4.1101203 -4.04625 -3.9858832 -3.9903703 -4.0666075 -4.1409459 -4.1856217 -4.2141147 -4.2350841 -4.24829 -4.2622523 -4.2793031 -4.2928524][-4.2287993 -4.197648 -4.1535888 -4.1006408 -4.0398169 -4.0273371 -4.0749483 -4.1333055 -4.1752443 -4.2024 -4.2224765 -4.2396011 -4.2583084 -4.2792845 -4.2935581][-4.2256036 -4.1989236 -4.1660895 -4.1260247 -4.0761828 -4.0575624 -4.0749507 -4.1100578 -4.1497416 -4.1810794 -4.2088661 -4.231359 -4.2535725 -4.27651 -4.2931585][-4.2000704 -4.1759911 -4.1443281 -4.1097312 -4.0692215 -4.0422192 -4.0316486 -4.0461054 -4.0899835 -4.1362495 -4.1779418 -4.20756 -4.2366786 -4.2651858 -4.2876005][-4.174758 -4.1540451 -4.1294785 -4.1021428 -4.06367 -4.0264492 -3.9949582 -3.9865024 -4.0283461 -4.0890417 -4.1423521 -4.1801329 -4.2145786 -4.2498875 -4.279387][-4.1812534 -4.1600432 -4.1386323 -4.1183071 -4.0839806 -4.0450783 -4.0093126 -3.987196 -4.015378 -4.0732679 -4.1264844 -4.1672378 -4.2039037 -4.243464 -4.276649][-4.2092943 -4.1886873 -4.1664677 -4.1447697 -4.1142883 -4.0843663 -4.0604277 -4.0404449 -4.0506868 -4.0892024 -4.1317186 -4.1686454 -4.2069473 -4.2486396 -4.2835894][-4.224721 -4.2117114 -4.1925082 -4.1674304 -4.1351657 -4.1085472 -4.0913682 -4.0752573 -4.0753341 -4.0984154 -4.1349254 -4.1714506 -4.2104168 -4.2533426 -4.289381][-4.2174182 -4.2153034 -4.2058225 -4.1861043 -4.1554837 -4.1246276 -4.1036882 -4.0866919 -4.080163 -4.0929356 -4.1261549 -4.1651449 -4.207058 -4.2520232 -4.2883167][-4.2104192 -4.2167106 -4.2189608 -4.2105489 -4.1874876 -4.1556277 -4.13003 -4.1080489 -4.0917439 -4.0928092 -4.1200018 -4.1577535 -4.200541 -4.2446642 -4.2797537][-4.2261639 -4.2337193 -4.2392259 -4.2383776 -4.2256885 -4.20124 -4.1758418 -4.1494541 -4.1238155 -4.1147051 -4.1327667 -4.1628847 -4.2000527 -4.240242 -4.2725315][-4.2436976 -4.2461538 -4.2490096 -4.25173 -4.2490187 -4.237956 -4.2213874 -4.1948237 -4.164114 -4.1476593 -4.1565084 -4.1778688 -4.2069283 -4.2399893 -4.2682567][-4.2425704 -4.2402363 -4.2396259 -4.241931 -4.2443132 -4.2426567 -4.2333689 -4.2093091 -4.1795731 -4.1630144 -4.1688867 -4.1861768 -4.2107477 -4.237802 -4.262517]]...]
INFO - root - 2017-12-06 07:09:04.065803: step 1110, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 84h:20m:28s remains)
INFO - root - 2017-12-06 07:09:13.052420: step 1120, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.925 sec/batch; 85h:06m:13s remains)
INFO - root - 2017-12-06 07:09:22.247019: step 1130, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 83h:09m:06s remains)
INFO - root - 2017-12-06 07:09:31.404619: step 1140, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 85h:01m:47s remains)
INFO - root - 2017-12-06 07:09:40.466666: step 1150, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 85h:17m:22s remains)
INFO - root - 2017-12-06 07:09:49.523797: step 1160, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 83h:08m:41s remains)
INFO - root - 2017-12-06 07:09:58.689464: step 1170, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 83h:15m:43s remains)
INFO - root - 2017-12-06 07:10:08.091269: step 1180, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 88h:05m:27s remains)
INFO - root - 2017-12-06 07:10:17.096474: step 1190, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.948 sec/batch; 87h:13m:14s remains)
INFO - root - 2017-12-06 07:10:26.586986: step 1200, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 84h:42m:06s remains)
2017-12-06 07:10:27.242751: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2450819 -4.2547169 -4.2750778 -4.2872677 -4.287508 -4.2863941 -4.2955313 -4.3100352 -4.3202624 -4.3197103 -4.3044538 -4.2855334 -4.2710786 -4.263669 -4.2650757][-4.1970191 -4.2137651 -4.2491913 -4.27475 -4.2821107 -4.2862654 -4.297873 -4.318882 -4.3328438 -4.3360524 -4.3182435 -4.2907133 -4.2653947 -4.2500758 -4.24475][-4.156682 -4.1795359 -4.2271504 -4.2583084 -4.2650161 -4.264205 -4.2742844 -4.300806 -4.3206682 -4.3313637 -4.3223562 -4.2994542 -4.2701 -4.2472658 -4.2333384][-4.1648865 -4.1881409 -4.2319179 -4.2540646 -4.2452679 -4.228271 -4.2288737 -4.2564116 -4.2820187 -4.2999978 -4.305511 -4.29954 -4.2791028 -4.2550206 -4.2357159][-4.1917171 -4.2138915 -4.2411366 -4.2433748 -4.209517 -4.162497 -4.143033 -4.1758347 -4.2193346 -4.2501578 -4.2721591 -4.2861147 -4.282855 -4.2676897 -4.2467165][-4.2047038 -4.2208991 -4.2290239 -4.2072258 -4.1449208 -4.058754 -3.9982922 -4.04018 -4.1249962 -4.1872787 -4.23225 -4.265481 -4.280982 -4.2787366 -4.2623377][-4.2203822 -4.2267594 -4.2154527 -4.1708651 -4.0830803 -3.9569993 -3.8481555 -3.8964741 -4.0288877 -4.1292853 -4.199616 -4.2482309 -4.276114 -4.2823548 -4.2714105][-4.2202039 -4.2214813 -4.2008295 -4.1497936 -4.0618057 -3.94333 -3.8408816 -3.8877926 -4.0202727 -4.1233644 -4.1972523 -4.2466364 -4.2752008 -4.2870836 -4.2823863][-4.2110782 -4.2114029 -4.192296 -4.1560841 -4.1021743 -4.03231 -3.9795239 -4.0079122 -4.0909572 -4.1608815 -4.215353 -4.25325 -4.2770753 -4.2911563 -4.2954082][-4.215889 -4.2182531 -4.2104807 -4.1936703 -4.1699529 -4.144721 -4.1296515 -4.1442509 -4.1830053 -4.2164936 -4.2445831 -4.26256 -4.2744393 -4.286253 -4.2992778][-4.2394848 -4.2416868 -4.23894 -4.2312832 -4.2256141 -4.2254944 -4.2302122 -4.2413955 -4.2565556 -4.2642164 -4.2699718 -4.2711873 -4.2709532 -4.27705 -4.2958522][-4.2741418 -4.2704678 -4.2647429 -4.2586169 -4.2606764 -4.2745018 -4.2865815 -4.2944565 -4.2927628 -4.2841578 -4.2759037 -4.2704163 -4.2650061 -4.2677908 -4.2846122][-4.3051615 -4.2973242 -4.2889023 -4.2794862 -4.2801218 -4.2958856 -4.3054795 -4.3044395 -4.2903523 -4.2742596 -4.2613192 -4.2552509 -4.2521143 -4.2561097 -4.26801][-4.3074393 -4.3000989 -4.2931538 -4.281426 -4.2773757 -4.2882104 -4.2927361 -4.28712 -4.2676468 -4.2511683 -4.23958 -4.2329469 -4.2309465 -4.2400084 -4.2550759][-4.2942529 -4.2894464 -4.2835894 -4.2758336 -4.2732296 -4.2789497 -4.2790661 -4.2739143 -4.25668 -4.2425256 -4.2333546 -4.227509 -4.2264318 -4.2408247 -4.26139]]...]
INFO - root - 2017-12-06 07:10:36.437509: step 1210, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.894 sec/batch; 82h:15m:21s remains)
INFO - root - 2017-12-06 07:10:45.435628: step 1220, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 83h:57m:23s remains)
INFO - root - 2017-12-06 07:10:54.825432: step 1230, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 81h:41m:41s remains)
INFO - root - 2017-12-06 07:11:03.955167: step 1240, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 84h:45m:03s remains)
INFO - root - 2017-12-06 07:11:13.019227: step 1250, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.948 sec/batch; 87h:11m:05s remains)
INFO - root - 2017-12-06 07:11:22.093433: step 1260, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 85h:41m:44s remains)
INFO - root - 2017-12-06 07:11:31.501488: step 1270, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.997 sec/batch; 91h:42m:36s remains)
INFO - root - 2017-12-06 07:11:40.704933: step 1280, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 81h:13m:41s remains)
INFO - root - 2017-12-06 07:11:49.857104: step 1290, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 82h:26m:18s remains)
INFO - root - 2017-12-06 07:11:59.005220: step 1300, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 83h:18m:48s remains)
2017-12-06 07:11:59.765307: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2000055 -4.2154684 -4.2259121 -4.2296381 -4.2241712 -4.2119956 -4.1858187 -4.1296854 -4.0678139 -4.0828333 -4.123426 -4.1544333 -4.1950111 -4.2377944 -4.27058][-4.1654162 -4.1796184 -4.1916347 -4.1968694 -4.1936083 -4.1898208 -4.1722054 -4.1202025 -4.0561323 -4.0677018 -4.1064038 -4.1357636 -4.1775932 -4.2251287 -4.2636023][-4.1284871 -4.1398225 -4.153923 -4.1621628 -4.1635113 -4.1709065 -4.161787 -4.1120539 -4.0456877 -4.0543127 -4.091773 -4.120677 -4.1653996 -4.2166348 -4.2579432][-4.0957961 -4.1017795 -4.1173677 -4.1294231 -4.1368194 -4.1558414 -4.1529202 -4.1037745 -4.0365725 -4.0473223 -4.0881028 -4.1183343 -4.1658735 -4.2177896 -4.258162][-4.0651627 -4.0660715 -4.0835037 -4.0984821 -4.1098433 -4.1362753 -4.1348372 -4.082056 -4.0142689 -4.0343275 -4.0845695 -4.1211843 -4.1731043 -4.2253251 -4.2632756][-4.0424519 -4.0353312 -4.0508814 -4.0632987 -4.0712833 -4.098249 -4.0949688 -4.0361881 -3.9709792 -4.0078974 -4.0737109 -4.1231689 -4.1820807 -4.2354145 -4.2706733][-4.0513639 -4.0349312 -4.0401711 -4.0398273 -4.0326056 -4.0512543 -4.0393467 -3.9707136 -3.910886 -3.9712796 -4.0545483 -4.1174126 -4.1848917 -4.2411804 -4.2756605][-4.0875096 -4.0667305 -4.0618353 -4.0463538 -4.0222173 -4.026495 -3.9981489 -3.9178395 -3.8615198 -3.9406548 -4.0371885 -4.107482 -4.1807675 -4.2410021 -4.2768555][-4.1319385 -4.1123161 -4.102344 -4.0786576 -4.0495706 -4.0432081 -4.00291 -3.9203739 -3.8717215 -3.9555926 -4.0493889 -4.1127968 -4.1812592 -4.2405424 -4.2761693][-4.1760221 -4.160255 -4.1483736 -4.12359 -4.0983987 -4.0877752 -4.0452352 -3.9737191 -3.9370012 -4.0153475 -4.0933948 -4.1401925 -4.1940317 -4.2437692 -4.2753][-4.2086368 -4.1950421 -4.1826382 -4.1626081 -4.1463313 -4.1359525 -4.0996852 -4.0448437 -4.017405 -4.0843058 -4.1459641 -4.178153 -4.2155733 -4.2523346 -4.2770987][-4.2297707 -4.21592 -4.2031479 -4.1899743 -4.1819234 -4.1745439 -4.1486697 -4.1097412 -4.0878587 -4.1406469 -4.1895308 -4.2123346 -4.2379804 -4.2640367 -4.2814522][-4.2466083 -4.2346659 -4.2235951 -4.2155237 -4.21207 -4.2065444 -4.1901078 -4.1631842 -4.1455178 -4.1858616 -4.2244782 -4.24081 -4.2577767 -4.2763119 -4.2879062][-4.2661495 -4.2581568 -4.2499218 -4.2453971 -4.2437177 -4.2401567 -4.2309604 -4.2121968 -4.1992054 -4.2294788 -4.257978 -4.2689409 -4.28037 -4.2923489 -4.2981696][-4.2854762 -4.2799416 -4.2745886 -4.2722645 -4.2713108 -4.2703953 -4.2668381 -4.2546144 -4.2462626 -4.2689543 -4.2884722 -4.2955828 -4.3041625 -4.3107605 -4.3116007]]...]
INFO - root - 2017-12-06 07:12:08.975078: step 1310, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 83h:04m:13s remains)
INFO - root - 2017-12-06 07:12:18.196723: step 1320, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 86h:26m:17s remains)
INFO - root - 2017-12-06 07:12:27.114250: step 1330, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.900 sec/batch; 82h:46m:21s remains)
INFO - root - 2017-12-06 07:12:36.304785: step 1340, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 88h:14m:50s remains)
INFO - root - 2017-12-06 07:12:45.392860: step 1350, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 84h:33m:10s remains)
INFO - root - 2017-12-06 07:12:54.521021: step 1360, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 86h:49m:36s remains)
INFO - root - 2017-12-06 07:13:03.693815: step 1370, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 82h:39m:48s remains)
INFO - root - 2017-12-06 07:13:12.762567: step 1380, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.891 sec/batch; 81h:57m:33s remains)
INFO - root - 2017-12-06 07:13:21.846431: step 1390, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.938 sec/batch; 86h:18m:29s remains)
INFO - root - 2017-12-06 07:13:30.758828: step 1400, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.958 sec/batch; 88h:08m:21s remains)
2017-12-06 07:13:31.441669: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1961336 -4.20619 -4.2108593 -4.2098141 -4.2043157 -4.2009692 -4.2015486 -4.2035103 -4.1940775 -4.1801171 -4.1722503 -4.1690211 -4.1700153 -4.1732011 -4.1832261][-4.1860518 -4.1969771 -4.2051497 -4.2088137 -4.2050738 -4.2017508 -4.2018719 -4.2033806 -4.1944981 -4.1849995 -4.1822867 -4.1819596 -4.181951 -4.1821361 -4.1880322][-4.1859856 -4.1955 -4.2026176 -4.2069697 -4.2043314 -4.2004642 -4.2006283 -4.2012992 -4.1917834 -4.18403 -4.1850448 -4.1894593 -4.1919966 -4.1945524 -4.1987405][-4.1762185 -4.1790915 -4.1810064 -4.1850743 -4.1851149 -4.1816559 -4.1809168 -4.1825743 -4.173945 -4.1680679 -4.1733122 -4.1849637 -4.193716 -4.1991363 -4.2029161][-4.1569643 -4.1494818 -4.145246 -4.1480174 -4.15037 -4.1460104 -4.1397834 -4.137085 -4.1286826 -4.1278543 -4.136126 -4.15351 -4.170156 -4.1801276 -4.1851277][-4.1455874 -4.1269093 -4.1139679 -4.1091847 -4.1050591 -4.0941124 -4.0775962 -4.0696 -4.0652876 -4.0762067 -4.0942154 -4.1174469 -4.1410537 -4.1564951 -4.1626625][-4.1724696 -4.1443653 -4.1178217 -4.095747 -4.0755506 -4.0473795 -4.0099592 -3.9930303 -3.9944668 -4.0236821 -4.0621061 -4.0980611 -4.1308045 -4.1536961 -4.1623511][-4.2162809 -4.1832962 -4.1477857 -4.1151009 -4.0816875 -4.03931 -3.9831285 -3.9573042 -3.9611049 -4.0019526 -4.059835 -4.1103892 -4.1544504 -4.1851082 -4.1979818][-4.2510486 -4.2205095 -4.18683 -4.158628 -4.1295042 -4.0902352 -4.03637 -4.0116315 -4.0147424 -4.050734 -4.1024795 -4.1470594 -4.1845183 -4.2134161 -4.2293086][-4.2713666 -4.2461987 -4.2204819 -4.2009912 -4.1790113 -4.1522918 -4.1155696 -4.1020141 -4.1069565 -4.1306386 -4.1613665 -4.1857066 -4.2017283 -4.2146454 -4.2243237][-4.28051 -4.2634592 -4.2426405 -4.2262988 -4.2100215 -4.1919932 -4.1693668 -4.15982 -4.1607771 -4.17222 -4.185482 -4.1934633 -4.1946321 -4.1937504 -4.1974373][-4.27522 -4.27003 -4.2555771 -4.2417731 -4.2290559 -4.213378 -4.19301 -4.1772704 -4.1683016 -4.1677213 -4.1700811 -4.17005 -4.1692944 -4.169136 -4.1774182][-4.2534952 -4.2633967 -4.2613544 -4.253963 -4.2460694 -4.2327809 -4.2114158 -4.1871424 -4.1679783 -4.15953 -4.1559687 -4.1549559 -4.1636591 -4.1743116 -4.1921244][-4.219604 -4.24197 -4.253788 -4.2565656 -4.2549682 -4.2472634 -4.2296934 -4.2070565 -4.1864295 -4.1743221 -4.16632 -4.1649013 -4.177649 -4.1955528 -4.221015][-4.1798916 -4.20347 -4.2203941 -4.2310448 -4.2386222 -4.2423439 -4.2379365 -4.2270136 -4.2129979 -4.2009139 -4.1910529 -4.1880751 -4.1985216 -4.2147231 -4.237556]]...]
INFO - root - 2017-12-06 07:13:40.795934: step 1410, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 88h:31m:30s remains)
INFO - root - 2017-12-06 07:13:49.818862: step 1420, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 83h:22m:09s remains)
INFO - root - 2017-12-06 07:13:59.152775: step 1430, loss = 2.10, batch loss = 2.05 (8.8 examples/sec; 0.908 sec/batch; 83h:28m:25s remains)
INFO - root - 2017-12-06 07:14:08.386932: step 1440, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 89h:15m:21s remains)
INFO - root - 2017-12-06 07:14:17.547212: step 1450, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.908 sec/batch; 83h:28m:29s remains)
INFO - root - 2017-12-06 07:14:26.714076: step 1460, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.900 sec/batch; 82h:43m:12s remains)
INFO - root - 2017-12-06 07:14:35.818738: step 1470, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.970 sec/batch; 89h:09m:29s remains)
INFO - root - 2017-12-06 07:14:44.952284: step 1480, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 84h:28m:30s remains)
INFO - root - 2017-12-06 07:14:53.788380: step 1490, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.910 sec/batch; 83h:37m:57s remains)
INFO - root - 2017-12-06 07:15:02.914144: step 1500, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.931 sec/batch; 85h:36m:21s remains)
2017-12-06 07:15:03.696511: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2518377 -4.2493715 -4.2461987 -4.2388077 -4.2232103 -4.223938 -4.2257161 -4.2322774 -4.2409616 -4.2302804 -4.2142682 -4.2207475 -4.2433352 -4.2559481 -4.25664][-4.1938562 -4.1956105 -4.2078829 -4.2139325 -4.1988759 -4.1971292 -4.2029672 -4.2154016 -4.2217078 -4.2068334 -4.1885509 -4.1980968 -4.2267742 -4.2453322 -4.2473054][-4.1397314 -4.1466193 -4.1737709 -4.19009 -4.1744838 -4.1666789 -4.173852 -4.1904888 -4.19371 -4.1785679 -4.1627841 -4.1746373 -4.2068377 -4.2305665 -4.23668][-4.1138 -4.1212997 -4.15463 -4.1741743 -4.1598625 -4.1462917 -4.1504149 -4.1653628 -4.1695056 -4.1609745 -4.15021 -4.1627307 -4.1952562 -4.2194881 -4.2294321][-4.1120505 -4.1127043 -4.1451049 -4.1647358 -4.1473083 -4.1299763 -4.1358137 -4.1504927 -4.15899 -4.1574469 -4.1505914 -4.163403 -4.1942863 -4.2178783 -4.2302723][-4.120954 -4.1164994 -4.1432147 -4.1561527 -4.1297364 -4.106142 -4.1157379 -4.1362667 -4.1521058 -4.1535816 -4.1474338 -4.16225 -4.1940503 -4.218739 -4.2333956][-4.1356874 -4.1318121 -4.1483526 -4.1490183 -4.1110706 -4.0761266 -4.08193 -4.1090741 -4.1338081 -4.1363568 -4.1264939 -4.1386766 -4.1728439 -4.2042708 -4.2277827][-4.1365018 -4.13414 -4.1347651 -4.1214027 -4.0758343 -4.031013 -4.0231676 -4.0507131 -4.0868106 -4.095222 -4.0807223 -4.0903568 -4.1314917 -4.1773 -4.2137485][-4.1155519 -4.117907 -4.1118121 -4.0919094 -4.0452609 -3.9909365 -3.9624736 -3.9815578 -4.0252547 -4.0366545 -4.0134654 -4.0142632 -4.0623455 -4.1254568 -4.1767254][-4.0967665 -4.1008821 -4.0957403 -4.0792341 -4.0397525 -3.9857564 -3.9466038 -3.9529049 -3.9925151 -4.0018682 -3.9742296 -3.9667206 -4.0145125 -4.0832915 -4.140841][-4.0916443 -4.0968485 -4.0947332 -4.0877042 -4.0635338 -4.0253272 -3.9907992 -3.9867523 -4.0126634 -4.0202379 -3.99563 -3.985791 -4.0244956 -4.0805149 -4.13058][-4.1047573 -4.1127563 -4.1146641 -4.1163058 -4.110034 -4.0918665 -4.0653839 -4.0520959 -4.0614967 -4.0655704 -4.0480375 -4.039721 -4.0674963 -4.110014 -4.1491423][-4.1464729 -4.159543 -4.1650534 -4.1688933 -4.16947 -4.1640015 -4.1466117 -4.1306167 -4.1261072 -4.1257291 -4.1162024 -4.1113048 -4.1289892 -4.1582794 -4.1905274][-4.20156 -4.2164841 -4.2243214 -4.2270679 -4.2279816 -4.2266846 -4.2190189 -4.2081213 -4.198812 -4.1949544 -4.1901641 -4.186964 -4.1946244 -4.211524 -4.2368803][-4.2549272 -4.264575 -4.2717805 -4.2750287 -4.2751961 -4.2743611 -4.2728086 -4.269176 -4.2640452 -4.2603493 -4.2564697 -4.2550082 -4.2576442 -4.2668295 -4.2829652]]...]
INFO - root - 2017-12-06 07:15:12.875573: step 1510, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 84h:42m:31s remains)
INFO - root - 2017-12-06 07:15:22.114749: step 1520, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.928 sec/batch; 85h:19m:25s remains)
INFO - root - 2017-12-06 07:15:31.284588: step 1530, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 83h:18m:30s remains)
INFO - root - 2017-12-06 07:15:40.224757: step 1540, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 83h:33m:05s remains)
INFO - root - 2017-12-06 07:15:49.255106: step 1550, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 81h:26m:58s remains)
INFO - root - 2017-12-06 07:15:58.407657: step 1560, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 82h:31m:30s remains)
INFO - root - 2017-12-06 07:16:07.675262: step 1570, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 84h:38m:16s remains)
INFO - root - 2017-12-06 07:16:16.776966: step 1580, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 83h:50m:42s remains)
INFO - root - 2017-12-06 07:16:25.970636: step 1590, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 84h:05m:07s remains)
INFO - root - 2017-12-06 07:16:35.185610: step 1600, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.940 sec/batch; 86h:26m:06s remains)
2017-12-06 07:16:35.929033: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3281026 -4.3300982 -4.3318768 -4.3283186 -4.3217535 -4.315969 -4.3101912 -4.3094854 -4.3130345 -4.3184786 -4.326189 -4.3369007 -4.3461342 -4.3502913 -4.350286][-4.3122287 -4.3155642 -4.319663 -4.3156362 -4.3071384 -4.3007569 -4.2954068 -4.2957125 -4.3008718 -4.3086333 -4.3201981 -4.330894 -4.3356142 -4.3327193 -4.3269844][-4.2901483 -4.2948542 -4.29969 -4.2950196 -4.2855792 -4.2804146 -4.2773108 -4.2784619 -4.2846603 -4.2936673 -4.307549 -4.3180141 -4.3177261 -4.3069739 -4.2953176][-4.2692547 -4.2733107 -4.2743468 -4.2644691 -4.2505937 -4.2435656 -4.2419844 -4.2483578 -4.261528 -4.2780809 -4.2988997 -4.3118062 -4.3089957 -4.2943454 -4.2791119][-4.2516685 -4.2486792 -4.2389569 -4.2202187 -4.197969 -4.183208 -4.1767836 -4.1832614 -4.2076783 -4.2406011 -4.2756209 -4.2952652 -4.2963724 -4.2864246 -4.2728734][-4.233789 -4.2170486 -4.1938515 -4.1661596 -4.1336055 -4.10472 -4.0817547 -4.0826745 -4.1255436 -4.1822042 -4.2358809 -4.2668133 -4.276926 -4.2772098 -4.2690268][-4.2336326 -4.2062345 -4.1696682 -4.1258225 -4.07085 -4.0104136 -3.9504459 -3.9363811 -4.0035334 -4.0936933 -4.1745696 -4.2273116 -4.2541847 -4.2658925 -4.2638674][-4.2430191 -4.2151146 -4.1785665 -4.1292624 -4.0596561 -3.9711609 -3.8709385 -3.8348026 -3.9107976 -4.0168128 -4.1171789 -4.1917276 -4.2345805 -4.2565084 -4.2610335][-4.2616086 -4.2488389 -4.2325816 -4.2029519 -4.1511354 -4.0780668 -3.9899096 -3.9486361 -3.9938045 -4.0649052 -4.1440005 -4.2094264 -4.2474661 -4.266892 -4.2698193][-4.2727294 -4.2737408 -4.273541 -4.2589846 -4.2195115 -4.1614451 -4.0959692 -4.0622892 -4.0818133 -4.1166782 -4.1698227 -4.2198143 -4.2505422 -4.2665367 -4.2714243][-4.25278 -4.258657 -4.2666674 -4.259191 -4.2270579 -4.1832857 -4.1397443 -4.1189814 -4.1292315 -4.1452632 -4.180964 -4.2154655 -4.239851 -4.2542877 -4.2621083][-4.2269678 -4.2314076 -4.2422342 -4.240983 -4.21747 -4.1900067 -4.1671572 -4.1593323 -4.1724195 -4.1841 -4.2079396 -4.23027 -4.247921 -4.2586889 -4.2640963][-4.2049108 -4.20462 -4.2103381 -4.208066 -4.1903696 -4.1748071 -4.1629491 -4.1620722 -4.1778855 -4.1899352 -4.2114916 -4.2326965 -4.250761 -4.2635117 -4.26894][-4.1856275 -4.1777439 -4.1753483 -4.170845 -4.1596594 -4.1522369 -4.145709 -4.1491313 -4.1693773 -4.1865406 -4.2101049 -4.2333283 -4.2536912 -4.267715 -4.2712274][-4.1867442 -4.1732354 -4.1676083 -4.1637745 -4.157846 -4.1547923 -4.1513519 -4.1569443 -4.1754112 -4.1929064 -4.2152681 -4.2384744 -4.2599988 -4.2741194 -4.2764959]]...]
INFO - root - 2017-12-06 07:16:44.869374: step 1610, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.923 sec/batch; 84h:47m:56s remains)
INFO - root - 2017-12-06 07:16:53.955857: step 1620, loss = 2.03, batch loss = 1.97 (7.9 examples/sec; 1.014 sec/batch; 93h:12m:40s remains)
INFO - root - 2017-12-06 07:17:03.206334: step 1630, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 84h:46m:35s remains)
INFO - root - 2017-12-06 07:17:12.419985: step 1640, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 84h:56m:50s remains)
INFO - root - 2017-12-06 07:17:21.488734: step 1650, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.972 sec/batch; 89h:18m:08s remains)
INFO - root - 2017-12-06 07:17:30.875145: step 1660, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.925 sec/batch; 84h:58m:37s remains)
INFO - root - 2017-12-06 07:17:40.085074: step 1670, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 82h:20m:52s remains)
INFO - root - 2017-12-06 07:17:49.206376: step 1680, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 83h:38m:31s remains)
INFO - root - 2017-12-06 07:17:58.122072: step 1690, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.880 sec/batch; 80h:53m:35s remains)
INFO - root - 2017-12-06 07:18:07.191696: step 1700, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 82h:35m:03s remains)
2017-12-06 07:18:07.889277: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2187324 -4.1726551 -4.1320648 -4.134088 -4.1874084 -4.2511272 -4.3023314 -4.326241 -4.32662 -4.3172593 -4.3048444 -4.3027496 -4.3130088 -4.3178258 -4.3023591][-4.1880169 -4.1208353 -4.0606337 -4.0700083 -4.1516609 -4.23593 -4.2909617 -4.3085885 -4.3026314 -4.2966962 -4.291832 -4.2976241 -4.3130965 -4.3184743 -4.2987275][-4.1628361 -4.0804691 -4.0062671 -4.0269818 -4.1316485 -4.2252417 -4.272254 -4.2769866 -4.2645383 -4.263988 -4.2720265 -4.2919445 -4.3169279 -4.3207045 -4.2941413][-4.16728 -4.0906463 -4.0257387 -4.0568538 -4.1589031 -4.2434182 -4.270999 -4.2515593 -4.2240486 -4.2247295 -4.2465425 -4.2825103 -4.3206973 -4.3212771 -4.2869773][-4.1836128 -4.1358695 -4.1098943 -4.1496134 -4.22128 -4.2741838 -4.2712793 -4.2179251 -4.1685443 -4.16916 -4.2089982 -4.263526 -4.3105316 -4.3083949 -4.2731948][-4.1935396 -4.1789861 -4.1906672 -4.2284045 -4.2624068 -4.2687984 -4.2216597 -4.1254287 -4.0520287 -4.0693054 -4.1480527 -4.2303581 -4.2832885 -4.2805419 -4.2510419][-4.1953845 -4.2083473 -4.2378616 -4.261797 -4.2604861 -4.2235069 -4.120244 -3.9639418 -3.8676484 -3.9267387 -4.0604172 -4.1748977 -4.23746 -4.2412596 -4.2200384][-4.2106495 -4.2377687 -4.26759 -4.2737961 -4.2457385 -4.1752954 -4.033442 -3.84092 -3.7454147 -3.8507171 -4.0185761 -4.1445088 -4.209094 -4.2160668 -4.1965494][-4.2387705 -4.2639637 -4.2823296 -4.2695661 -4.2305112 -4.1611967 -4.0408859 -3.8961751 -3.8501391 -3.9473877 -4.0799608 -4.1765141 -4.2207422 -4.2162204 -4.1919737][-4.262495 -4.2799463 -4.28568 -4.2666698 -4.2314363 -4.1813459 -4.1053076 -4.0314989 -4.0260267 -4.0969663 -4.1846633 -4.244679 -4.2598543 -4.2374096 -4.2084489][-4.2727494 -4.2895284 -4.2926717 -4.2756815 -4.2492533 -4.2180905 -4.17674 -4.1433449 -4.1518507 -4.2058673 -4.2641335 -4.2968845 -4.291985 -4.2630062 -4.2356272][-4.2813206 -4.2963982 -4.30037 -4.2897015 -4.2749825 -4.2594476 -4.2359657 -4.2164049 -4.22443 -4.2614675 -4.2991371 -4.3159051 -4.3066845 -4.284236 -4.2633171][-4.2974315 -4.3075652 -4.3071442 -4.298213 -4.2894316 -4.2847376 -4.2747869 -4.2645993 -4.2682471 -4.2877927 -4.3058009 -4.3113551 -4.3035245 -4.2906837 -4.2777834][-4.3162928 -4.32057 -4.3170462 -4.3094959 -4.3037033 -4.304029 -4.3040276 -4.30225 -4.304512 -4.3122606 -4.316246 -4.313189 -4.3053145 -4.2974553 -4.2911057][-4.326334 -4.3277416 -4.3231106 -4.3163967 -4.3114018 -4.3122058 -4.3160038 -4.3202658 -4.3237982 -4.3253293 -4.3225851 -4.3172531 -4.3127942 -4.31034 -4.3080912]]...]
INFO - root - 2017-12-06 07:18:17.096896: step 1710, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.904 sec/batch; 83h:05m:05s remains)
INFO - root - 2017-12-06 07:18:26.221458: step 1720, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 82h:30m:45s remains)
INFO - root - 2017-12-06 07:18:35.432444: step 1730, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 84h:26m:05s remains)
INFO - root - 2017-12-06 07:18:44.688221: step 1740, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 83h:10m:10s remains)
INFO - root - 2017-12-06 07:18:53.618009: step 1750, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 0.755 sec/batch; 69h:19m:46s remains)
INFO - root - 2017-12-06 07:19:02.894528: step 1760, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.881 sec/batch; 80h:54m:42s remains)
INFO - root - 2017-12-06 07:19:12.090298: step 1770, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 85h:19m:20s remains)
INFO - root - 2017-12-06 07:19:21.218099: step 1780, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.897 sec/batch; 82h:25m:40s remains)
INFO - root - 2017-12-06 07:19:30.500764: step 1790, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.964 sec/batch; 88h:32m:30s remains)
INFO - root - 2017-12-06 07:19:39.773285: step 1800, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.904 sec/batch; 83h:03m:03s remains)
2017-12-06 07:19:40.439578: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.235147 -4.2246909 -4.2155862 -4.2029858 -4.18781 -4.1911874 -4.2088933 -4.2247849 -4.2388458 -4.2500019 -4.2565217 -4.2563462 -4.2548938 -4.2490954 -4.2413292][-4.1990442 -4.1809626 -4.1687636 -4.1572342 -4.1511626 -4.1651998 -4.1882348 -4.2047749 -4.2199917 -4.2302313 -4.2347388 -4.2299318 -4.2241931 -4.2160854 -4.2097197][-4.160274 -4.1300893 -4.1106281 -4.0970078 -4.1000519 -4.1252093 -4.1560388 -4.1778913 -4.1987123 -4.2110395 -4.2138066 -4.2060227 -4.2004457 -4.1947637 -4.1919351][-4.13205 -4.0944963 -4.067966 -4.049037 -4.0509529 -4.08008 -4.1172256 -4.1487083 -4.1781483 -4.1918392 -4.1916523 -4.182476 -4.1779757 -4.1771393 -4.1812277][-4.1443038 -4.1124625 -4.0878892 -4.0612345 -4.0479054 -4.0648003 -4.1002588 -4.1397581 -4.1775417 -4.1927295 -4.1858115 -4.1717134 -4.1630225 -4.1618476 -4.1693578][-4.1748419 -4.1592808 -4.1437106 -4.111814 -4.0760484 -4.0606308 -4.078763 -4.12108 -4.1690731 -4.1952467 -4.1863971 -4.1676745 -4.1488314 -4.1336303 -4.1307454][-4.1773896 -4.1842465 -4.1828661 -4.154304 -4.0996113 -4.0489435 -4.0381422 -4.0716491 -4.1311669 -4.1820989 -4.1844282 -4.1637683 -4.1319518 -4.0926056 -4.0673566][-4.1576505 -4.1826725 -4.1953816 -4.1765418 -4.1196651 -4.0398927 -3.9869947 -3.9987795 -4.0680766 -4.1475644 -4.1688294 -4.1442595 -4.100493 -4.0324335 -3.9784486][-4.1241856 -4.160295 -4.1810484 -4.1757989 -4.1259494 -4.025074 -3.9195113 -3.9014626 -3.9826765 -4.0910492 -4.134995 -4.1189308 -4.0747032 -3.9844372 -3.908915][-4.0918393 -4.1327105 -4.1594863 -4.162745 -4.1230245 -4.0243816 -3.8864036 -3.83553 -3.9196286 -4.046174 -4.1115294 -4.1131496 -4.0795641 -4.0020814 -3.9347825][-4.0751548 -4.1097093 -4.13936 -4.1513996 -4.1292744 -4.0680771 -3.961514 -3.897388 -3.9492044 -4.0537944 -4.1190577 -4.1335163 -4.1237831 -4.0794973 -4.0346818][-4.098886 -4.1175027 -4.1441188 -4.1629181 -4.1626706 -4.1457005 -4.1019454 -4.0590005 -4.0705881 -4.1259713 -4.1665311 -4.1809182 -4.1879115 -4.1679444 -4.1367812][-4.1733127 -4.1747127 -4.1888742 -4.2034054 -4.2091889 -4.2160754 -4.2179856 -4.2045875 -4.201725 -4.2187362 -4.2360973 -4.2390933 -4.2432718 -4.2320318 -4.2092762][-4.2479339 -4.2423406 -4.2433586 -4.2500105 -4.2515259 -4.2609911 -4.2765365 -4.2768807 -4.2751145 -4.2801666 -4.2890215 -4.2857494 -4.2822642 -4.2762117 -4.2605472][-4.2980685 -4.2953076 -4.2929215 -4.2921724 -4.2898679 -4.2972918 -4.3090281 -4.313755 -4.3176351 -4.3208604 -4.32712 -4.3243446 -4.3198113 -4.31631 -4.3067317]]...]
INFO - root - 2017-12-06 07:19:49.693768: step 1810, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 1.000 sec/batch; 91h:49m:52s remains)
INFO - root - 2017-12-06 07:19:58.633781: step 1820, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 0.774 sec/batch; 71h:05m:04s remains)
INFO - root - 2017-12-06 07:20:07.802093: step 1830, loss = 2.03, batch loss = 1.98 (8.6 examples/sec; 0.932 sec/batch; 85h:35m:04s remains)
INFO - root - 2017-12-06 07:20:16.994655: step 1840, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 85h:00m:30s remains)
INFO - root - 2017-12-06 07:20:26.184461: step 1850, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 87h:00m:39s remains)
INFO - root - 2017-12-06 07:20:35.293912: step 1860, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.881 sec/batch; 80h:56m:51s remains)
INFO - root - 2017-12-06 07:20:44.450876: step 1870, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 84h:43m:52s remains)
INFO - root - 2017-12-06 07:20:53.608533: step 1880, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 79h:23m:17s remains)
INFO - root - 2017-12-06 07:21:02.654231: step 1890, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.902 sec/batch; 82h:49m:42s remains)
INFO - root - 2017-12-06 07:21:11.707611: step 1900, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 84h:13m:02s remains)
2017-12-06 07:21:12.431237: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1923656 -4.2056885 -4.2175956 -4.2297034 -4.2402163 -4.2484708 -4.251 -4.2476845 -4.2415309 -4.2357035 -4.231082 -4.2285395 -4.2296224 -4.2327614 -4.2347136][-4.2276115 -4.2409115 -4.2540956 -4.2663693 -4.2744784 -4.2776423 -4.2742438 -4.2646236 -4.2522635 -4.2411947 -4.232801 -4.2278304 -4.2270751 -4.2284293 -4.2286487][-4.2612824 -4.2724314 -4.28412 -4.2935805 -4.2971058 -4.29438 -4.2843637 -4.2683907 -4.2511678 -4.2370696 -4.2268991 -4.2208371 -4.2190156 -4.2185745 -4.2171226][-4.2802038 -4.2871408 -4.2933993 -4.296711 -4.2935681 -4.2842851 -4.2696466 -4.2518287 -4.2359767 -4.225184 -4.2185264 -4.2146578 -4.2126188 -4.2105727 -4.2078009][-4.2762923 -4.2759809 -4.2732944 -4.2675877 -4.2557797 -4.2395549 -4.2221308 -4.20687 -4.1993551 -4.1994061 -4.2022986 -4.2042608 -4.2044435 -4.2027116 -4.1998825][-4.2540445 -4.2467532 -4.2348514 -4.2196307 -4.1995449 -4.1763248 -4.1559067 -4.1450243 -4.1488309 -4.1627321 -4.1780634 -4.1889019 -4.1944485 -4.1959558 -4.1948066][-4.230545 -4.2185149 -4.1995769 -4.1769962 -4.1514091 -4.1246581 -4.1036377 -4.09755 -4.1107616 -4.134769 -4.158793 -4.1761508 -4.186645 -4.1922741 -4.1939583][-4.216229 -4.203063 -4.1838346 -4.1628842 -4.1425877 -4.1241345 -4.1118855 -4.1114216 -4.124898 -4.1458912 -4.1665373 -4.1822886 -4.1926885 -4.1991539 -4.2019334][-4.2159953 -4.2057948 -4.1927848 -4.1813297 -4.1733665 -4.1685686 -4.1674418 -4.1708708 -4.1791286 -4.1901722 -4.200994 -4.2093549 -4.2149382 -4.2183628 -4.2195873][-4.2271729 -4.2210903 -4.2155242 -4.2134347 -4.2151361 -4.2189174 -4.223228 -4.2273717 -4.2316828 -4.2362313 -4.2400737 -4.2422123 -4.2428145 -4.2423887 -4.2410469][-4.2457361 -4.2431793 -4.2425976 -4.2455125 -4.2508612 -4.2566919 -4.2614784 -4.264596 -4.2665744 -4.2679482 -4.2687621 -4.2682886 -4.2667241 -4.2643 -4.2617493][-4.2673554 -4.2668362 -4.2679853 -4.2713552 -4.2755265 -4.2793479 -4.28222 -4.2838855 -4.2849069 -4.2856674 -4.2862644 -4.2859683 -4.2846479 -4.2825313 -4.280477][-4.285532 -4.28639 -4.2878542 -4.2904062 -4.2929659 -4.2949696 -4.2963061 -4.297101 -4.2978163 -4.2986188 -4.2992969 -4.2991333 -4.2982993 -4.2971134 -4.2960734][-4.2971363 -4.2988524 -4.30035 -4.302371 -4.3041778 -4.30573 -4.3071113 -4.3081689 -4.3088512 -4.3093185 -4.3095632 -4.309238 -4.3086472 -4.3079805 -4.3073626][-4.3036165 -4.3058372 -4.307343 -4.3094511 -4.3114991 -4.3133821 -4.3152089 -4.3167238 -4.3173923 -4.3174334 -4.3173184 -4.3169389 -4.3166447 -4.3163185 -4.3157463]]...]
INFO - root - 2017-12-06 07:21:21.720856: step 1910, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 85h:03m:51s remains)
INFO - root - 2017-12-06 07:21:30.981789: step 1920, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 86h:18m:14s remains)
INFO - root - 2017-12-06 07:21:40.130706: step 1930, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 81h:52m:12s remains)
INFO - root - 2017-12-06 07:21:49.118801: step 1940, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.904 sec/batch; 82h:58m:50s remains)
INFO - root - 2017-12-06 07:21:57.993635: step 1950, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.874 sec/batch; 80h:14m:23s remains)
INFO - root - 2017-12-06 07:22:07.107537: step 1960, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.894 sec/batch; 82h:05m:06s remains)
INFO - root - 2017-12-06 07:22:16.103992: step 1970, loss = 2.09, batch loss = 2.04 (8.5 examples/sec; 0.937 sec/batch; 86h:00m:14s remains)
INFO - root - 2017-12-06 07:22:25.215338: step 1980, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.979 sec/batch; 89h:52m:33s remains)
INFO - root - 2017-12-06 07:22:34.359853: step 1990, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.902 sec/batch; 82h:49m:00s remains)
INFO - root - 2017-12-06 07:22:43.681043: step 2000, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 86h:46m:54s remains)
2017-12-06 07:22:44.319632: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3631606 -4.3600159 -4.3553605 -4.3536444 -4.3526425 -4.3515425 -4.3533831 -4.3536944 -4.349556 -4.3417754 -4.33524 -4.3345222 -4.3377666 -4.344101 -4.351964][-4.3568597 -4.34842 -4.3394217 -4.3342 -4.3286753 -4.32433 -4.3254528 -4.3217731 -4.3086276 -4.29166 -4.2785749 -4.274478 -4.2818408 -4.2998528 -4.32181][-4.3479705 -4.3331418 -4.3187332 -4.3077669 -4.2954636 -4.2863388 -4.2841711 -4.2735353 -4.2475438 -4.2156887 -4.1885471 -4.179544 -4.1946812 -4.2298169 -4.2713103][-4.3340135 -4.3121805 -4.2904854 -4.271286 -4.2526822 -4.2404628 -4.2357783 -4.2162609 -4.1738505 -4.1234255 -4.0792055 -4.0653896 -4.0920935 -4.1473966 -4.2109494][-4.3094149 -4.27636 -4.2466521 -4.2210855 -4.2011495 -4.1895394 -4.1852303 -4.15651 -4.1003809 -4.0428629 -3.9961793 -3.9866641 -4.0238533 -4.0935793 -4.1712422][-4.2687535 -4.2228069 -4.1843119 -4.1504526 -4.1283197 -4.1192741 -4.1207438 -4.0933113 -4.0319071 -3.9862072 -3.9629524 -3.9708581 -4.0169005 -4.0921617 -4.1715832][-4.2234187 -4.1648092 -4.1070614 -4.049727 -4.0199718 -4.019835 -4.0378971 -4.0223117 -3.97228 -3.9607482 -3.9736862 -4.0003772 -4.0504041 -4.123569 -4.1986403][-4.1804986 -4.1068788 -4.0250654 -3.9345121 -3.9004669 -3.9204371 -3.9685419 -3.9788888 -3.9613831 -3.9804964 -4.0169044 -4.0568061 -4.1081982 -4.1737828 -4.2381849][-4.1613312 -4.0804667 -3.9881256 -3.8836882 -3.8604348 -3.9047594 -3.9780147 -4.0174451 -4.0274487 -4.0568943 -4.0946522 -4.1339321 -4.1787429 -4.2301631 -4.2765565][-4.1838741 -4.11493 -4.0472093 -3.9774117 -3.9683921 -4.0102949 -4.0786023 -4.1227579 -4.1393051 -4.16213 -4.1880317 -4.2160239 -4.2475753 -4.2825112 -4.3107557][-4.2374516 -4.1896882 -4.1540847 -4.1258268 -4.1293077 -4.1592083 -4.2079706 -4.2381639 -4.2455049 -4.2532344 -4.2639742 -4.2788181 -4.2991261 -4.3215284 -4.3362908][-4.2920189 -4.2644596 -4.2508535 -4.2474861 -4.2594743 -4.2807922 -4.3078623 -4.3197556 -4.31691 -4.3143473 -4.3156919 -4.3220053 -4.334435 -4.3467665 -4.3521891][-4.3277397 -4.3141141 -4.3101935 -4.315423 -4.3266439 -4.33944 -4.3530765 -4.3567581 -4.3523731 -4.3482203 -4.3470497 -4.3489337 -4.354444 -4.3595848 -4.3608208][-4.3471613 -4.3429093 -4.3418241 -4.3465018 -4.3534846 -4.3606558 -4.367002 -4.3679771 -4.365509 -4.3626132 -4.3613176 -4.3617558 -4.3634338 -4.3647 -4.3644266][-4.3544788 -4.3550811 -4.3556018 -4.3575144 -4.3603315 -4.3641157 -4.3673086 -4.3678441 -4.3666396 -4.3654509 -4.3646936 -4.3649826 -4.3655338 -4.3652935 -4.3643756]]...]
INFO - root - 2017-12-06 07:22:53.530507: step 2010, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 83h:37m:32s remains)
INFO - root - 2017-12-06 07:23:02.563229: step 2020, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.985 sec/batch; 90h:26m:27s remains)
INFO - root - 2017-12-06 07:23:11.728822: step 2030, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 85h:14m:05s remains)
INFO - root - 2017-12-06 07:23:20.834629: step 2040, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 82h:38m:40s remains)
INFO - root - 2017-12-06 07:23:30.078585: step 2050, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 83h:37m:46s remains)
INFO - root - 2017-12-06 07:23:39.315235: step 2060, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.908 sec/batch; 83h:19m:03s remains)
INFO - root - 2017-12-06 07:23:48.602078: step 2070, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 85h:14m:28s remains)
INFO - root - 2017-12-06 07:23:57.767709: step 2080, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 78h:27m:07s remains)
INFO - root - 2017-12-06 07:24:06.947067: step 2090, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 85h:39m:00s remains)
INFO - root - 2017-12-06 07:24:16.080730: step 2100, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 88h:10m:20s remains)
2017-12-06 07:24:16.729970: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.174829 -4.1799073 -4.186799 -4.2100124 -4.2291942 -4.2301631 -4.2337279 -4.24137 -4.2511435 -4.2685094 -4.3018966 -4.32443 -4.3376093 -4.3424435 -4.3349786][-4.1872416 -4.1911573 -4.2009125 -4.2201509 -4.2347965 -4.2340064 -4.2353468 -4.2421246 -4.2583489 -4.2881207 -4.3264079 -4.345932 -4.3545189 -4.3577023 -4.34697][-4.2053676 -4.1982083 -4.2076807 -4.2244673 -4.2337008 -4.2261291 -4.2180109 -4.2185917 -4.2371211 -4.2783046 -4.3223438 -4.3433642 -4.3548865 -4.3592644 -4.3472061][-4.2167697 -4.2026863 -4.2111354 -4.2278218 -4.2297349 -4.2042513 -4.1756215 -4.1672325 -4.1879725 -4.2436562 -4.2977796 -4.3251452 -4.3420539 -4.3493109 -4.3374548][-4.2281561 -4.216475 -4.2224503 -4.230896 -4.21659 -4.1607971 -4.0994616 -4.0787554 -4.110549 -4.1864944 -4.2556424 -4.2969322 -4.32427 -4.3390818 -4.3331332][-4.2414136 -4.2383895 -4.2437334 -4.2424908 -4.2124038 -4.13255 -4.0376883 -3.9937704 -4.0340433 -4.1292839 -4.2115669 -4.2662349 -4.3046989 -4.3274536 -4.3316445][-4.2508879 -4.2592487 -4.267252 -4.263937 -4.2288947 -4.1457663 -4.04001 -3.9770353 -4.0114508 -4.1048083 -4.1865582 -4.2473226 -4.2953162 -4.321342 -4.3292465][-4.2556758 -4.272182 -4.2812924 -4.2758594 -4.24068 -4.1684146 -4.0702009 -4.0018296 -4.0258284 -4.1074939 -4.1834764 -4.2430992 -4.2935476 -4.3155241 -4.3187728][-4.247086 -4.2683115 -4.2766576 -4.2678485 -4.2357116 -4.1791029 -4.0943556 -4.0233941 -4.0346632 -4.1022091 -4.1784539 -4.2408214 -4.2928944 -4.31293 -4.3103828][-4.2346549 -4.2565427 -4.2628293 -4.2541962 -4.2278962 -4.1858854 -4.1141686 -4.0481744 -4.0552554 -4.111311 -4.1802082 -4.2395821 -4.2888923 -4.3073339 -4.301671][-4.2412524 -4.2570825 -4.2549686 -4.2438488 -4.2260108 -4.1977162 -4.1410384 -4.0855455 -4.0921721 -4.14365 -4.2018981 -4.2504034 -4.2838063 -4.2944059 -4.2894273][-4.2664661 -4.2779188 -4.2666345 -4.2511153 -4.23364 -4.212965 -4.1737051 -4.1365595 -4.1442356 -4.18829 -4.2311049 -4.263164 -4.2764053 -4.2695484 -4.2631264][-4.2959929 -4.3063445 -4.2946072 -4.2764277 -4.255312 -4.2359495 -4.2099762 -4.1900578 -4.199645 -4.2349548 -4.2645369 -4.2781291 -4.2681022 -4.23342 -4.213521][-4.3058944 -4.3182726 -4.313283 -4.3007107 -4.284945 -4.27099 -4.2539864 -4.2456546 -4.2568116 -4.2798171 -4.2940636 -4.2930241 -4.2630844 -4.2048073 -4.1675897][-4.29171 -4.304935 -4.3092179 -4.3126507 -4.3104725 -4.3041239 -4.2942543 -4.292213 -4.3026137 -4.3159847 -4.3165178 -4.3023677 -4.260818 -4.1965718 -4.1524181]]...]
INFO - root - 2017-12-06 07:24:25.562777: step 2110, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 82h:55m:01s remains)
INFO - root - 2017-12-06 07:24:34.778353: step 2120, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 84h:20m:47s remains)
INFO - root - 2017-12-06 07:24:43.839140: step 2130, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.934 sec/batch; 85h:43m:06s remains)
INFO - root - 2017-12-06 07:24:53.147106: step 2140, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 84h:48m:08s remains)
INFO - root - 2017-12-06 07:25:02.238852: step 2150, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 85h:48m:46s remains)
INFO - root - 2017-12-06 07:25:11.440506: step 2160, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 85h:23m:36s remains)
INFO - root - 2017-12-06 07:25:20.650576: step 2170, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 81h:40m:37s remains)
INFO - root - 2017-12-06 07:25:29.639015: step 2180, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 86h:36m:03s remains)
INFO - root - 2017-12-06 07:25:38.758049: step 2190, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 86h:12m:22s remains)
INFO - root - 2017-12-06 07:25:48.008433: step 2200, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.983 sec/batch; 90h:10m:41s remains)
2017-12-06 07:25:48.703637: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.26877 -4.2795014 -4.2835035 -4.267374 -4.2401304 -4.2002354 -4.1545148 -4.1294093 -4.1420503 -4.1686292 -4.1770515 -4.1915827 -4.2184548 -4.2393708 -4.24663][-4.2555008 -4.2698426 -4.2805328 -4.2704458 -4.2425628 -4.1988349 -4.1479774 -4.1199512 -4.1349792 -4.1704531 -4.1883521 -4.2097077 -4.2448039 -4.2680168 -4.2743111][-4.2419386 -4.2635875 -4.2789497 -4.2695432 -4.2351213 -4.1859727 -4.1316524 -4.1072555 -4.1299858 -4.1702571 -4.1985703 -4.2268667 -4.2654619 -4.29258 -4.2972231][-4.2328897 -4.2657614 -4.2847958 -4.2700191 -4.2214708 -4.1583395 -4.092823 -4.0720773 -4.1101203 -4.165905 -4.2101607 -4.2481985 -4.2906637 -4.3202796 -4.3192725][-4.232594 -4.2722111 -4.2923651 -4.2696133 -4.20673 -4.1260314 -4.04302 -4.0226126 -4.0768323 -4.1557417 -4.2206593 -4.271297 -4.3187981 -4.3464823 -4.33706][-4.2385235 -4.2794614 -4.2964845 -4.2648859 -4.1869159 -4.0835776 -3.9810603 -3.9599841 -4.0332661 -4.1394725 -4.2278795 -4.292964 -4.3422089 -4.3613014 -4.3383017][-4.2500467 -4.2884398 -4.2988095 -4.2586927 -4.165041 -4.0331182 -3.905314 -3.8785563 -3.970808 -4.1057057 -4.21777 -4.2976313 -4.3476372 -4.3589382 -4.3266692][-4.2710195 -4.2995481 -4.3009372 -4.2542105 -4.1487026 -3.992974 -3.8384993 -3.79464 -3.8946834 -4.0521727 -4.1853385 -4.2812772 -4.3365364 -4.3434691 -4.3047667][-4.3002462 -4.315536 -4.3057694 -4.2543817 -4.1469216 -3.9854393 -3.8159733 -3.7496488 -3.8381274 -3.9999156 -4.144989 -4.2559471 -4.3191876 -4.3271189 -4.2864232][-4.3320413 -4.33709 -4.3187065 -4.2643533 -4.1624804 -4.0126224 -3.8495383 -3.7715883 -3.8381956 -3.9847775 -4.1247334 -4.2353263 -4.2989683 -4.3094363 -4.2749157][-4.356123 -4.356957 -4.3355975 -4.2847953 -4.1980987 -4.0747356 -3.9325049 -3.851361 -3.8905413 -4.0069823 -4.1285219 -4.2274566 -4.2832413 -4.2938948 -4.2693295][-4.3642297 -4.3628826 -4.3434687 -4.3011217 -4.2337246 -4.1434817 -4.0328412 -3.959168 -3.9744856 -4.0555377 -4.1507921 -4.230381 -4.2749305 -4.283473 -4.2641644][-4.3560019 -4.3521619 -4.3358984 -4.3043761 -4.2564492 -4.1931248 -4.1160011 -4.0615063 -4.0657654 -4.1196494 -4.1906219 -4.2503414 -4.282021 -4.2896442 -4.2728305][-4.3426781 -4.3350687 -4.319324 -4.2958646 -4.2618337 -4.2164907 -4.16609 -4.13283 -4.1394467 -4.1809697 -4.2339983 -4.2770643 -4.2981348 -4.3017435 -4.2854776][-4.3295593 -4.317728 -4.302845 -4.2856836 -4.2613869 -4.2290492 -4.1961732 -4.1757107 -4.1818447 -4.21333 -4.2527218 -4.283082 -4.298233 -4.3027573 -4.2914481]]...]
INFO - root - 2017-12-06 07:25:57.927272: step 2210, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 83h:50m:00s remains)
INFO - root - 2017-12-06 07:26:06.918087: step 2220, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.958 sec/batch; 87h:52m:50s remains)
INFO - root - 2017-12-06 07:26:16.060488: step 2230, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 84h:17m:30s remains)
INFO - root - 2017-12-06 07:26:25.176040: step 2240, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 82h:36m:18s remains)
INFO - root - 2017-12-06 07:26:34.217626: step 2250, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.943 sec/batch; 86h:31m:28s remains)
INFO - root - 2017-12-06 07:26:43.527668: step 2260, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 83h:55m:45s remains)
INFO - root - 2017-12-06 07:26:52.705078: step 2270, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 84h:11m:09s remains)
INFO - root - 2017-12-06 07:27:01.881032: step 2280, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 76h:46m:32s remains)
INFO - root - 2017-12-06 07:27:11.133923: step 2290, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 84h:25m:04s remains)
INFO - root - 2017-12-06 07:27:20.358033: step 2300, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 84h:20m:14s remains)
2017-12-06 07:27:21.080024: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2595606 -4.2558789 -4.2426858 -4.2166352 -4.1831012 -4.1576614 -4.1358023 -4.1121588 -4.0971775 -4.0833321 -4.0739617 -4.0731063 -4.0789762 -4.1028152 -4.1471162][-4.2630258 -4.2462158 -4.2152042 -4.175878 -4.140842 -4.1150856 -4.0934939 -4.0684543 -4.0417809 -4.0150442 -4.0213919 -4.0553904 -4.0969539 -4.1401935 -4.193716][-4.251606 -4.2240086 -4.1813936 -4.1382618 -4.1119881 -4.0930958 -4.073143 -4.0480175 -4.0182881 -3.9905074 -4.009428 -4.0573339 -4.1105375 -4.1622539 -4.2197356][-4.2441349 -4.2146783 -4.171782 -4.1273112 -4.1000991 -4.0829282 -4.06374 -4.0334134 -4.0099916 -4.0108347 -4.0423703 -4.0882425 -4.1399469 -4.1882496 -4.23982][-4.2419448 -4.2164712 -4.1824703 -4.1368647 -4.09616 -4.0661092 -4.0290794 -3.9810269 -3.9702919 -4.0174665 -4.0751705 -4.1285062 -4.1831651 -4.2266469 -4.2643886][-4.2232194 -4.2093711 -4.1898046 -4.1462531 -4.087944 -4.0280356 -3.9504256 -3.8717124 -3.8808167 -3.9880183 -4.08218 -4.1510816 -4.2145214 -4.258779 -4.284934][-4.1966391 -4.1847682 -4.1710997 -4.1304297 -4.060812 -3.9815407 -3.8794661 -3.7969711 -3.839833 -3.9857254 -4.09671 -4.1700482 -4.232873 -4.2748437 -4.2949119][-4.1500177 -4.1374583 -4.1315908 -4.1097279 -4.0640526 -4.0130358 -3.9488597 -3.9135897 -3.9630272 -4.0713949 -4.1483088 -4.2024546 -4.2490535 -4.2810526 -4.2939215][-4.0836873 -4.0885344 -4.1077948 -4.115036 -4.1032195 -4.08704 -4.0659385 -4.0663023 -4.1082678 -4.1723747 -4.2172022 -4.2470441 -4.2722583 -4.2874322 -4.2933278][-4.0653515 -4.09416 -4.1276851 -4.1439629 -4.142643 -4.1391768 -4.1332965 -4.1449804 -4.18351 -4.230217 -4.2605948 -4.2818413 -4.2954416 -4.2965422 -4.2984648][-4.0918159 -4.1257377 -4.1574736 -4.1670189 -4.1680779 -4.1713958 -4.1701164 -4.1838441 -4.2165146 -4.2512755 -4.2740393 -4.2956605 -4.3077126 -4.30746 -4.3104038][-4.13989 -4.1642122 -4.1851177 -4.1920853 -4.194623 -4.1928897 -4.1873689 -4.200098 -4.2280903 -4.2555671 -4.276495 -4.2989483 -4.3129497 -4.3156686 -4.3204184][-4.1676512 -4.1855512 -4.2001476 -4.2046018 -4.2081881 -4.2091856 -4.2026348 -4.209589 -4.2337923 -4.2602906 -4.282568 -4.30642 -4.3191667 -4.3230615 -4.3261638][-4.1731915 -4.1910009 -4.2024803 -4.2020812 -4.20539 -4.2160134 -4.22503 -4.2346158 -4.25506 -4.2793708 -4.2998762 -4.3192277 -4.3266678 -4.3276973 -4.3283443][-4.189146 -4.208447 -4.2196383 -4.2171507 -4.2197275 -4.2380719 -4.2574043 -4.2703757 -4.288271 -4.3057146 -4.3181009 -4.3280349 -4.3305087 -4.3295889 -4.3285151]]...]
INFO - root - 2017-12-06 07:27:30.301046: step 2310, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 84h:26m:28s remains)
INFO - root - 2017-12-06 07:27:39.280240: step 2320, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 81h:27m:08s remains)
INFO - root - 2017-12-06 07:27:48.458471: step 2330, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.905 sec/batch; 83h:01m:08s remains)
INFO - root - 2017-12-06 07:27:57.638135: step 2340, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 84h:04m:23s remains)
INFO - root - 2017-12-06 07:28:06.803084: step 2350, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.019 sec/batch; 93h:27m:35s remains)
INFO - root - 2017-12-06 07:28:15.979258: step 2360, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 88h:39m:07s remains)
INFO - root - 2017-12-06 07:28:25.232945: step 2370, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 85h:03m:30s remains)
INFO - root - 2017-12-06 07:28:34.426018: step 2380, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.913 sec/batch; 83h:41m:54s remains)
INFO - root - 2017-12-06 07:28:43.436062: step 2390, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 81h:20m:53s remains)
INFO - root - 2017-12-06 07:28:52.661422: step 2400, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 84h:36m:37s remains)
2017-12-06 07:28:53.390698: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1596775 -4.1762624 -4.2056761 -4.2274117 -4.2444797 -4.256691 -4.2442918 -4.2138419 -4.195435 -4.1992326 -4.2112136 -4.2282066 -4.2456036 -4.2661114 -4.2876744][-4.1492929 -4.1731849 -4.20843 -4.2362051 -4.2559886 -4.2626295 -4.2412753 -4.204927 -4.18089 -4.183342 -4.1964674 -4.2194185 -4.2416515 -4.2630477 -4.2859306][-4.1649895 -4.1868892 -4.2194805 -4.2445407 -4.2583027 -4.2540278 -4.2228875 -4.1809492 -4.1538281 -4.1562591 -4.1739392 -4.2025008 -4.228508 -4.2539244 -4.2807741][-4.1884184 -4.2072811 -4.2351356 -4.2469707 -4.2429643 -4.2222304 -4.18297 -4.1390619 -4.1162944 -4.1297545 -4.1596937 -4.1939416 -4.2218595 -4.2513037 -4.2807503][-4.2116013 -4.2250123 -4.2362828 -4.2261367 -4.201683 -4.1631751 -4.1104717 -4.0598941 -4.0480471 -4.0853271 -4.1366777 -4.179379 -4.2134728 -4.2487378 -4.2807846][-4.2049313 -4.2060118 -4.1999311 -4.1746922 -4.1387 -4.0880303 -4.016613 -3.9453044 -3.9390447 -4.0042186 -4.0801964 -4.1406422 -4.1894803 -4.235414 -4.2734008][-4.1727319 -4.1683836 -4.1572042 -4.128469 -4.0955009 -4.0463605 -3.9693725 -3.8839669 -3.8734341 -3.9416654 -4.0249829 -4.0990696 -4.1611295 -4.21832 -4.2641978][-4.1563573 -4.1556053 -4.151238 -4.131649 -4.1157036 -4.0903521 -4.0372815 -3.9670734 -3.9460847 -3.9817677 -4.0411472 -4.1049237 -4.1622252 -4.2172079 -4.2627559][-4.1690011 -4.1712742 -4.17894 -4.1751242 -4.1734548 -4.1663418 -4.1364412 -4.0867281 -4.0631766 -4.0726948 -4.1046376 -4.1451764 -4.1866817 -4.2296586 -4.2672143][-4.2087336 -4.2099619 -4.2219362 -4.227139 -4.2306385 -4.2287388 -4.2139254 -4.1827817 -4.1617389 -4.161006 -4.1763244 -4.1944518 -4.2166891 -4.2453537 -4.2749052][-4.2595997 -4.2592311 -4.2685843 -4.2719269 -4.2720685 -4.2713814 -4.2669063 -4.2501497 -4.2354717 -4.2320442 -4.2373004 -4.2415648 -4.2479935 -4.2624025 -4.2829828][-4.2946153 -4.2962618 -4.3017979 -4.3027678 -4.3026085 -4.30262 -4.3026814 -4.2956538 -4.2881331 -4.2841377 -4.2842741 -4.2825828 -4.2810965 -4.2862525 -4.2971282][-4.3168807 -4.317842 -4.32137 -4.3221536 -4.324029 -4.3276019 -4.3315625 -4.329247 -4.3237944 -4.3190131 -4.3184195 -4.3172212 -4.3137259 -4.3136377 -4.3169608][-4.3263569 -4.3262634 -4.3281741 -4.3288059 -4.3313565 -4.3352089 -4.3384738 -4.3378658 -4.3353658 -4.3335586 -4.3340726 -4.3342552 -4.3323665 -4.3319626 -4.3328533][-4.3320732 -4.3306289 -4.33019 -4.3301272 -4.3310633 -4.3319397 -4.3328485 -4.3336773 -4.3338752 -4.3346095 -4.3367696 -4.3386145 -4.3388495 -4.339736 -4.3406491]]...]
INFO - root - 2017-12-06 07:29:02.594488: step 2410, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 84h:45m:20s remains)
INFO - root - 2017-12-06 07:29:11.592941: step 2420, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 86h:03m:47s remains)
INFO - root - 2017-12-06 07:29:20.560543: step 2430, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.910 sec/batch; 83h:25m:10s remains)
INFO - root - 2017-12-06 07:29:29.657089: step 2440, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.918 sec/batch; 84h:10m:34s remains)
INFO - root - 2017-12-06 07:29:38.696472: step 2450, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.923 sec/batch; 84h:37m:42s remains)
INFO - root - 2017-12-06 07:29:47.694671: step 2460, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.812 sec/batch; 74h:28m:54s remains)
INFO - root - 2017-12-06 07:29:56.927661: step 2470, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 83h:50m:24s remains)
INFO - root - 2017-12-06 07:30:06.004555: step 2480, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.979 sec/batch; 89h:46m:53s remains)
INFO - root - 2017-12-06 07:30:15.170266: step 2490, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 89h:07m:54s remains)
INFO - root - 2017-12-06 07:30:24.488785: step 2500, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 84h:32m:21s remains)
2017-12-06 07:30:25.156771: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3060536 -4.3024797 -4.29568 -4.2899766 -4.2833843 -4.2802782 -4.2823806 -4.2911949 -4.3029289 -4.3079491 -4.3042216 -4.2938304 -4.285543 -4.2857256 -4.2910576][-4.2975941 -4.2902412 -4.278121 -4.2661142 -4.2561145 -4.2497487 -4.2495089 -4.262826 -4.2821155 -4.2926526 -4.293745 -4.2847443 -4.2727776 -4.2659335 -4.2676868][-4.2750611 -4.2616334 -4.2398653 -4.2212782 -4.2091413 -4.1996927 -4.1974297 -4.2165833 -4.2442884 -4.2625022 -4.2702436 -4.2669411 -4.2580209 -4.2480292 -4.246316][-4.2327471 -4.207027 -4.1763215 -4.1567478 -4.1478925 -4.13757 -4.1322794 -4.1552062 -4.1914077 -4.2174463 -4.2310033 -4.2341752 -4.2298813 -4.2181087 -4.2152047][-4.2133427 -4.1779919 -4.141253 -4.1200242 -4.1090717 -4.0915022 -4.0757093 -4.0929461 -4.1372175 -4.1704321 -4.1875472 -4.1928787 -4.1915088 -4.186224 -4.1855273][-4.1997685 -4.1625519 -4.1165347 -4.084619 -4.0609236 -4.0317874 -4.0011044 -4.0015974 -4.0567608 -4.1122994 -4.1417336 -4.1534872 -4.1588535 -4.1668959 -4.1742549][-4.1864548 -4.1554704 -4.1089735 -4.0607071 -4.0146813 -3.9600179 -3.9010644 -3.8739319 -3.9375341 -4.0310631 -4.0872979 -4.1130495 -4.1286945 -4.1460671 -4.16195][-4.1936321 -4.1711607 -4.1300335 -4.072906 -4.0134368 -3.9435928 -3.8669884 -3.8312626 -3.8898282 -3.9928024 -4.0600152 -4.092484 -4.1105633 -4.1308055 -4.15183][-4.2098 -4.1891165 -4.1525211 -4.0948663 -4.0391779 -3.9857512 -3.9285123 -3.9160295 -3.9684629 -4.0371809 -4.0787277 -4.0965581 -4.1056943 -4.1215158 -4.14329][-4.2226911 -4.2083826 -4.1790061 -4.1290755 -4.0824318 -4.0368938 -3.9936879 -3.9926844 -4.0341125 -4.0689816 -4.0880032 -4.0993323 -4.1018949 -4.1077423 -4.1258521][-4.2020173 -4.1932211 -4.1781111 -4.1509361 -4.1219654 -4.0846491 -4.0489054 -4.0458269 -4.0700717 -4.0811906 -4.08145 -4.0888734 -4.0941286 -4.0977154 -4.1157331][-4.1870213 -4.1806717 -4.180078 -4.1709442 -4.1566124 -4.1296792 -4.1048741 -4.1023154 -4.1142864 -4.1085606 -4.0935459 -4.0957823 -4.1088591 -4.1181622 -4.1357255][-4.1961265 -4.1950722 -4.2000427 -4.1963277 -4.1838346 -4.1641183 -4.1469746 -4.1427431 -4.1496863 -4.1388483 -4.1202521 -4.1206608 -4.139668 -4.1554694 -4.17337][-4.2132063 -4.2165847 -4.2264371 -4.2261915 -4.2173171 -4.2080016 -4.199883 -4.196898 -4.2020383 -4.1930761 -4.1748767 -4.1743388 -4.188272 -4.2008767 -4.2153234][-4.2472458 -4.2518888 -4.2642393 -4.2672195 -4.2640119 -4.2601233 -4.255486 -4.2575364 -4.2648807 -4.2624474 -4.2499208 -4.2481666 -4.2551241 -4.2608027 -4.2675366]]...]
INFO - root - 2017-12-06 07:30:34.358429: step 2510, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.891 sec/batch; 81h:42m:04s remains)
INFO - root - 2017-12-06 07:30:43.585656: step 2520, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 84h:25m:07s remains)
INFO - root - 2017-12-06 07:30:52.878102: step 2530, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 87h:17m:02s remains)
INFO - root - 2017-12-06 07:31:02.022750: step 2540, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 83h:27m:15s remains)
INFO - root - 2017-12-06 07:31:11.192044: step 2550, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.904 sec/batch; 82h:53m:37s remains)
INFO - root - 2017-12-06 07:31:20.363283: step 2560, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.894 sec/batch; 81h:58m:30s remains)
INFO - root - 2017-12-06 07:31:29.469898: step 2570, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 82h:13m:27s remains)
INFO - root - 2017-12-06 07:31:38.682720: step 2580, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 85h:38m:00s remains)
INFO - root - 2017-12-06 07:31:47.811345: step 2590, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 86h:32m:51s remains)
INFO - root - 2017-12-06 07:31:56.921785: step 2600, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 84h:03m:06s remains)
2017-12-06 07:31:57.644097: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3174963 -4.3233323 -4.3325291 -4.3393879 -4.3431835 -4.3436236 -4.3425808 -4.340239 -4.3375983 -4.3351412 -4.3332253 -4.3321409 -4.3295107 -4.3257771 -4.323688][-4.25559 -4.2641792 -4.283483 -4.3008633 -4.3126111 -4.317452 -4.3181729 -4.3165174 -4.3145909 -4.3135252 -4.3133755 -4.3138857 -4.3097816 -4.300693 -4.2917132][-4.1748624 -4.1835346 -4.2099733 -4.2371578 -4.25851 -4.269454 -4.2722087 -4.2718396 -4.2725348 -4.2752953 -4.2807083 -4.2865882 -4.2837753 -4.2716427 -4.2592173][-4.0881996 -4.0923071 -4.1224947 -4.1591663 -4.1918144 -4.2094212 -4.2134109 -4.2151189 -4.2224588 -4.2321362 -4.2435737 -4.2535086 -4.2539167 -4.2433329 -4.231349][-4.0250149 -4.013907 -4.0358915 -4.0748305 -4.1134248 -4.1326847 -4.1351504 -4.138412 -4.1536245 -4.1746721 -4.1956296 -4.2100987 -4.2147331 -4.2097783 -4.2020445][-4.0142655 -3.983885 -3.9864759 -4.0105419 -4.0387859 -4.0469036 -4.0375738 -4.0364475 -4.0576668 -4.0927949 -4.1269603 -4.1487784 -4.1601682 -4.1648135 -4.1679864][-4.07157 -4.0361419 -4.0213823 -4.0234313 -4.0304942 -4.0189781 -3.9899213 -3.9732616 -3.9914944 -4.034534 -4.0783906 -4.1051836 -4.1206164 -4.1315951 -4.1423354][-4.1519971 -4.1317677 -4.12264 -4.1210356 -4.1200771 -4.0999994 -4.0615382 -4.0297642 -4.0307474 -4.0587993 -4.0911622 -4.1097393 -4.1195822 -4.1286697 -4.139308][-4.212275 -4.2085595 -4.2110362 -4.2163754 -4.222074 -4.2110171 -4.18439 -4.1591253 -4.1548367 -4.170114 -4.1875291 -4.1929822 -4.1892648 -4.1845412 -4.182735][-4.2361636 -4.242136 -4.2537394 -4.2670455 -4.2816391 -4.2826614 -4.271431 -4.2595682 -4.2584167 -4.2679977 -4.2770538 -4.2767854 -4.265439 -4.2499633 -4.2380085][-4.2250862 -4.2338047 -4.2504311 -4.2697287 -4.2905097 -4.300487 -4.3002663 -4.2969623 -4.2976 -4.3027668 -4.3069715 -4.3054495 -4.2920341 -4.2714553 -4.2543688][-4.1681728 -4.1751828 -4.1964183 -4.223999 -4.2525992 -4.2693262 -4.2757649 -4.2787957 -4.281971 -4.2855935 -4.2877421 -4.2848587 -4.2660751 -4.2367978 -4.214395][-4.0857806 -4.0803695 -4.0972414 -4.1260705 -4.1579905 -4.177856 -4.1863351 -4.19437 -4.2032075 -4.2118106 -4.2179928 -4.2169819 -4.1926646 -4.153872 -4.1271348][-4.0375919 -4.0206141 -4.03016 -4.0540705 -4.0834112 -4.1016049 -4.1064587 -4.1125073 -4.1205449 -4.1302242 -4.1389341 -4.1397734 -4.1150055 -4.0742273 -4.0474424][-4.0782866 -4.0613565 -4.0682864 -4.0881991 -4.11219 -4.125649 -4.126245 -4.1281657 -4.1318607 -4.1377664 -4.144546 -4.14592 -4.12705 -4.0955348 -4.074019]]...]
INFO - root - 2017-12-06 07:32:06.686475: step 2610, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 86h:33m:37s remains)
INFO - root - 2017-12-06 07:32:15.751892: step 2620, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.894 sec/batch; 81h:53m:24s remains)
INFO - root - 2017-12-06 07:32:24.907565: step 2630, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 84h:03m:23s remains)
INFO - root - 2017-12-06 07:32:34.052364: step 2640, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 83h:46m:06s remains)
INFO - root - 2017-12-06 07:32:43.238831: step 2650, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 82h:13m:07s remains)
INFO - root - 2017-12-06 07:32:52.402470: step 2660, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 82h:15m:02s remains)
INFO - root - 2017-12-06 07:33:01.562472: step 2670, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.949 sec/batch; 86h:54m:34s remains)
INFO - root - 2017-12-06 07:33:10.428012: step 2680, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 88h:27m:40s remains)
INFO - root - 2017-12-06 07:33:19.714444: step 2690, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 86h:36m:21s remains)
INFO - root - 2017-12-06 07:33:28.898819: step 2700, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.951 sec/batch; 87h:08m:06s remains)
2017-12-06 07:33:29.570406: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3393021 -4.3372545 -4.3377085 -4.3377285 -4.3368826 -4.3324323 -4.326839 -4.3207803 -4.3167462 -4.3142452 -4.3149 -4.3160415 -4.3188162 -4.3182306 -4.3128533][-4.3473825 -4.3465929 -4.3478441 -4.3479576 -4.3455691 -4.3392792 -4.3301096 -4.3208885 -4.3151264 -4.3131852 -4.3160195 -4.3208804 -4.3281302 -4.3310676 -4.3287897][-4.3437219 -4.3434792 -4.3431082 -4.3407726 -4.3326449 -4.3168521 -4.2974029 -4.28066 -4.2724028 -4.272419 -4.2810254 -4.2934322 -4.3085361 -4.3191748 -4.3237667][-4.3323746 -4.3294215 -4.324369 -4.3143358 -4.2925138 -4.2584 -4.2200451 -4.1879969 -4.1769991 -4.1857357 -4.2091603 -4.2380195 -4.2668481 -4.286325 -4.3005109][-4.3091884 -4.3011661 -4.2881851 -4.2663183 -4.2278018 -4.1699929 -4.1005659 -4.0426097 -4.030447 -4.0611863 -4.1109657 -4.1611676 -4.2046108 -4.2344942 -4.2573152][-4.279129 -4.2670789 -4.2436261 -4.2054152 -4.1457248 -4.0605068 -3.9544408 -3.8631248 -3.8532734 -3.9249797 -4.0163565 -4.0910182 -4.1438961 -4.1803751 -4.2088227][-4.2341318 -4.2153296 -4.1779203 -4.122458 -4.0476227 -3.9480157 -3.8255849 -3.7176003 -3.7239079 -3.8423166 -3.9672108 -4.0532594 -4.10492 -4.1389194 -4.1687117][-4.1848888 -4.1622686 -4.1181579 -4.0562477 -3.9814193 -3.8949027 -3.8039522 -3.7338555 -3.7550726 -3.8623796 -3.9719205 -4.0460367 -4.0908413 -4.1173849 -4.1409531][-4.1635237 -4.1439581 -4.1071062 -4.0553937 -4.0009713 -3.95307 -3.916187 -3.8889608 -3.8996816 -3.9548371 -4.0178685 -4.0686212 -4.1055465 -4.1251197 -4.140605][-4.1834736 -4.1704073 -4.1469836 -4.1143303 -4.0844975 -4.0660157 -4.0569043 -4.0469794 -4.04335 -4.0598145 -4.08874 -4.1222143 -4.15108 -4.1674008 -4.1754379][-4.2329087 -4.2205205 -4.2059813 -4.1883636 -4.1750484 -4.1683192 -4.1658783 -4.1609039 -4.1547084 -4.1603069 -4.1759372 -4.1956782 -4.213346 -4.2260022 -4.2294989][-4.262063 -4.2489953 -4.2420344 -4.2382488 -4.2375894 -4.2388558 -4.2425766 -4.2433023 -4.2412667 -4.2440786 -4.2523332 -4.2630515 -4.2738957 -4.2835288 -4.28483][-4.2649846 -4.2548156 -4.2529559 -4.2594581 -4.2699394 -4.2799697 -4.290041 -4.2947459 -4.2932744 -4.2913952 -4.2935748 -4.3017573 -4.3132124 -4.3234363 -4.3247437][-4.2433214 -4.2346277 -4.2374611 -4.2527833 -4.2737322 -4.293128 -4.3091373 -4.315526 -4.3109622 -4.3022385 -4.2987132 -4.3067493 -4.322433 -4.3370776 -4.3400111][-4.2196422 -4.2029037 -4.2015076 -4.2175589 -4.2446609 -4.2732592 -4.2965417 -4.3059473 -4.2980089 -4.2831707 -4.2763515 -4.2850275 -4.3057485 -4.3262959 -4.3345175]]...]
INFO - root - 2017-12-06 07:33:38.793505: step 2710, loss = 2.10, batch loss = 2.05 (8.8 examples/sec; 0.913 sec/batch; 83h:40m:21s remains)
INFO - root - 2017-12-06 07:33:48.078959: step 2720, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 90h:08m:40s remains)
INFO - root - 2017-12-06 07:33:57.223668: step 2730, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.927 sec/batch; 84h:53m:40s remains)
INFO - root - 2017-12-06 07:34:06.393713: step 2740, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.928 sec/batch; 85h:00m:39s remains)
INFO - root - 2017-12-06 07:34:15.247889: step 2750, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 82h:05m:21s remains)
INFO - root - 2017-12-06 07:34:24.559874: step 2760, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.981 sec/batch; 89h:50m:52s remains)
INFO - root - 2017-12-06 07:34:33.858922: step 2770, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 85h:48m:11s remains)
INFO - root - 2017-12-06 07:34:42.934494: step 2780, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.908 sec/batch; 83h:08m:12s remains)
INFO - root - 2017-12-06 07:34:52.033378: step 2790, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 80h:10m:54s remains)
INFO - root - 2017-12-06 07:35:01.130043: step 2800, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 82h:12m:04s remains)
2017-12-06 07:35:01.838527: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3419652 -4.342824 -4.3418427 -4.3425765 -4.3435955 -4.3434343 -4.3432508 -4.340899 -4.3375263 -4.3356447 -4.3360047 -4.3371091 -4.3391495 -4.34066 -4.3420944][-4.335113 -4.3335562 -4.3314185 -4.3327794 -4.33549 -4.3379774 -4.3404551 -4.3381891 -4.3335404 -4.3323469 -4.3348656 -4.3365545 -4.3384986 -4.3392491 -4.339169][-4.3102179 -4.3053341 -4.3025117 -4.3069835 -4.3158088 -4.3236041 -4.3307223 -4.3297677 -4.323597 -4.3233018 -4.3291607 -4.3332314 -4.3360758 -4.33485 -4.3303566][-4.2722654 -4.2606087 -4.2563081 -4.2652893 -4.2793236 -4.2908049 -4.3029861 -4.30404 -4.2974157 -4.3014154 -4.3139448 -4.3221927 -4.3270574 -4.3247418 -4.3160348][-4.22937 -4.2093644 -4.20088 -4.2108526 -4.2266665 -4.2386994 -4.2505026 -4.2489243 -4.2391071 -4.2491174 -4.2705216 -4.2862062 -4.2959919 -4.2957 -4.2854834][-4.1942868 -4.1672916 -4.1542439 -4.1589007 -4.1666436 -4.1716266 -4.1740961 -4.156961 -4.1379128 -4.1555252 -4.1894584 -4.2165956 -4.2363405 -4.2428265 -4.2369661][-4.1602879 -4.1262221 -4.1075554 -4.1031103 -4.097261 -4.090157 -4.0759568 -4.0315413 -3.9980538 -4.0269766 -4.0785127 -4.1212244 -4.15597 -4.1750488 -4.1830411][-4.1436272 -4.1031604 -4.079133 -4.0659575 -4.0484471 -4.0310826 -4.0000668 -3.9288497 -3.8797269 -3.9167409 -3.9797621 -4.0350513 -4.0852914 -4.1173797 -4.13745][-4.1737738 -4.1443644 -4.1294041 -4.1163077 -4.1001782 -4.0933657 -4.0710011 -4.0097404 -3.9680021 -3.9892874 -4.0314646 -4.0686016 -4.1045508 -4.128098 -4.1453428][-4.219357 -4.2027664 -4.1978278 -4.1894908 -4.1797795 -4.1860585 -4.1786594 -4.1418772 -4.1172948 -4.1286469 -4.1526747 -4.1701989 -4.18528 -4.1932106 -4.1962638][-4.2631254 -4.2548113 -4.2576747 -4.2561765 -4.2514453 -4.2604475 -4.2570477 -4.2347641 -4.2217026 -4.2281632 -4.2426081 -4.2534709 -4.2619414 -4.2620416 -4.2539816][-4.2847137 -4.2812114 -4.2885838 -4.2925787 -4.2938123 -4.3027234 -4.3014569 -4.2871046 -4.2787614 -4.2831192 -4.2941017 -4.3026695 -4.3088784 -4.3076611 -4.2959728][-4.2909713 -4.2905054 -4.2965226 -4.30103 -4.305316 -4.31308 -4.314527 -4.3066998 -4.2997189 -4.3023467 -4.310123 -4.3161554 -4.3203321 -4.3203688 -4.3130212][-4.3033557 -4.3027573 -4.3049932 -4.3069844 -4.3098016 -4.3138795 -4.3139744 -4.3098545 -4.305397 -4.3080106 -4.3142643 -4.3186417 -4.3205147 -4.3208017 -4.3165746][-4.3164363 -4.3163738 -4.316967 -4.3176193 -4.3182373 -4.3187737 -4.3174191 -4.3151455 -4.3131728 -4.3148608 -4.3182564 -4.3207784 -4.3214936 -4.322474 -4.321342]]...]
INFO - root - 2017-12-06 07:35:10.909884: step 2810, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 86h:00m:47s remains)
INFO - root - 2017-12-06 07:35:19.960690: step 2820, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.943 sec/batch; 86h:19m:44s remains)
INFO - root - 2017-12-06 07:35:29.122368: step 2830, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 81h:56m:30s remains)
INFO - root - 2017-12-06 07:35:38.277462: step 2840, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.906 sec/batch; 83h:00m:36s remains)
INFO - root - 2017-12-06 07:35:47.443894: step 2850, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 78h:50m:43s remains)
INFO - root - 2017-12-06 07:35:56.734059: step 2860, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 82h:03m:09s remains)
INFO - root - 2017-12-06 07:36:06.042883: step 2870, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.960 sec/batch; 87h:55m:38s remains)
INFO - root - 2017-12-06 07:36:14.920916: step 2880, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 82h:12m:38s remains)
INFO - root - 2017-12-06 07:36:23.969318: step 2890, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 85h:28m:30s remains)
INFO - root - 2017-12-06 07:36:33.176946: step 2900, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 84h:21m:59s remains)
2017-12-06 07:36:33.881454: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2961311 -4.3076906 -4.3135953 -4.3079515 -4.2918067 -4.2640214 -4.2288013 -4.2065263 -4.2039351 -4.2247 -4.2564116 -4.2762823 -4.2771163 -4.2645144 -4.24976][-4.2998056 -4.3037157 -4.2988663 -4.2882705 -4.2755985 -4.2537694 -4.2233634 -4.2045755 -4.2052965 -4.2328677 -4.2718296 -4.2994075 -4.3043542 -4.2920218 -4.273098][-4.2958703 -4.2895575 -4.2726984 -4.2610955 -4.2607756 -4.2547822 -4.2343516 -4.2168984 -4.2134657 -4.239037 -4.2788897 -4.3084526 -4.3155675 -4.3056626 -4.2861037][-4.2937284 -4.2784419 -4.2560048 -4.2479796 -4.2603903 -4.2704406 -4.2621207 -4.2446823 -4.2327714 -4.2472491 -4.2790132 -4.30213 -4.3070865 -4.2983437 -4.2797885][-4.2980433 -4.2794724 -4.25674 -4.2520542 -4.2682858 -4.2839017 -4.283412 -4.2687716 -4.2509804 -4.2525125 -4.2721696 -4.2859054 -4.28383 -4.2723942 -4.2559223][-4.3006434 -4.2874351 -4.2699971 -4.2669911 -4.2790494 -4.2921424 -4.2942486 -4.2833757 -4.2644739 -4.2564573 -4.2638354 -4.2676668 -4.2571149 -4.2409134 -4.2252121][-4.3026366 -4.2976189 -4.2877893 -4.2866988 -4.2936006 -4.3000622 -4.3022108 -4.2962966 -4.2817121 -4.2714329 -4.2733173 -4.2708068 -4.2527127 -4.2293453 -4.2100267][-4.2994304 -4.3029485 -4.29908 -4.2965603 -4.2954712 -4.2960958 -4.2989197 -4.2971582 -4.291646 -4.28919 -4.294508 -4.2930675 -4.2729506 -4.2464871 -4.2230587][-4.2887793 -4.3020625 -4.3012362 -4.2917175 -4.278687 -4.27329 -4.2780571 -4.2800179 -4.2804766 -4.2842956 -4.2940717 -4.296804 -4.2814865 -4.26122 -4.2417474][-4.2695723 -4.2950397 -4.2958794 -4.2772985 -4.2529526 -4.2436657 -4.2513666 -4.2549963 -4.2570848 -4.2619557 -4.2691526 -4.2732987 -4.266109 -4.2579947 -4.2509651][-4.2482839 -4.2839408 -4.2862639 -4.2601624 -4.228497 -4.2185254 -4.2305684 -4.234303 -4.2318368 -4.2299113 -4.2300482 -4.2327266 -4.23148 -4.23594 -4.2443838][-4.2487488 -4.2867069 -4.2873082 -4.258389 -4.22452 -4.2143555 -4.2272043 -4.2297263 -4.2190924 -4.2061787 -4.1946945 -4.1883764 -4.1875443 -4.20205 -4.2245803][-4.2716069 -4.299861 -4.2941103 -4.2653236 -4.2348781 -4.2272005 -4.239491 -4.241044 -4.2259893 -4.2052946 -4.1811438 -4.1609416 -4.1554141 -4.1748681 -4.2016649][-4.291225 -4.3029885 -4.2901831 -4.265449 -4.2461967 -4.2452474 -4.2566605 -4.2589984 -4.2467647 -4.2261896 -4.1957526 -4.1668973 -4.15832 -4.1756034 -4.1922579][-4.2880554 -4.2867427 -4.2739391 -4.2599683 -4.2549839 -4.2594576 -4.2684822 -4.2724447 -4.2643261 -4.2472305 -4.2177644 -4.189414 -4.1828518 -4.1944823 -4.194325]]...]
INFO - root - 2017-12-06 07:36:43.064125: step 2910, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.957 sec/batch; 87h:36m:04s remains)
INFO - root - 2017-12-06 07:36:52.176925: step 2920, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.894 sec/batch; 81h:53m:20s remains)
INFO - root - 2017-12-06 07:37:01.425361: step 2930, loss = 2.10, batch loss = 2.04 (9.0 examples/sec; 0.891 sec/batch; 81h:34m:59s remains)
INFO - root - 2017-12-06 07:37:10.460437: step 2940, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 83h:23m:25s remains)
INFO - root - 2017-12-06 07:37:19.523018: step 2950, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.924 sec/batch; 84h:34m:45s remains)
INFO - root - 2017-12-06 07:37:28.564182: step 2960, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.932 sec/batch; 85h:18m:43s remains)
INFO - root - 2017-12-06 07:37:37.776131: step 2970, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 81h:44m:40s remains)
INFO - root - 2017-12-06 07:37:46.886508: step 2980, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 82h:14m:03s remains)
INFO - root - 2017-12-06 07:37:56.070022: step 2990, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 84h:01m:45s remains)
INFO - root - 2017-12-06 07:38:05.214789: step 3000, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 84h:13m:06s remains)
2017-12-06 07:38:05.833392: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2189884 -4.2132769 -4.209322 -4.2180405 -4.2251177 -4.226819 -4.2172194 -4.2046437 -4.1862617 -4.1729555 -4.1688318 -4.1776333 -4.1950784 -4.215054 -4.2343822][-4.1878133 -4.1824579 -4.1818228 -4.1949687 -4.2053037 -4.2113996 -4.204627 -4.1931911 -4.176825 -4.1598163 -4.1505661 -4.1563177 -4.1715236 -4.187716 -4.2054391][-4.1699004 -4.1667142 -4.1702523 -4.1887264 -4.2006855 -4.203403 -4.1926813 -4.1802282 -4.1641297 -4.1411376 -4.1199532 -4.1205325 -4.1345916 -4.1537347 -4.1737823][-4.1562343 -4.1562128 -4.1618519 -4.1804309 -4.185905 -4.1774187 -4.1596 -4.1476331 -4.1344018 -4.1086931 -4.0782776 -4.0759583 -4.0943213 -4.1225619 -4.1494][-4.1291866 -4.1274853 -4.1326709 -4.1459351 -4.1405063 -4.1208153 -4.1016917 -4.096508 -4.0913377 -4.0628705 -4.0249319 -4.0241604 -4.051414 -4.0920877 -4.1324263][-4.0882597 -4.0777135 -4.0723176 -4.0715361 -4.0544591 -4.0314803 -4.0195584 -4.0302839 -4.0417275 -4.01896 -3.9800754 -3.9839342 -4.0161319 -4.0639262 -4.11272][-4.0393972 -4.0179663 -4.0016155 -3.9842191 -3.9564021 -3.9336803 -3.9347529 -3.9616737 -3.9856451 -3.9784713 -3.951745 -3.9568052 -3.9865444 -4.0329976 -4.0795555][-3.9914908 -3.9614532 -3.9366109 -3.9121244 -3.8791194 -3.8571596 -3.8656914 -3.90239 -3.9342527 -3.9424429 -3.9280279 -3.9254515 -3.9447412 -3.9822164 -4.0251656][-3.9537215 -3.9181113 -3.890481 -3.868403 -3.8396115 -3.8247871 -3.837513 -3.8728061 -3.9050522 -3.9219031 -3.9147224 -3.90848 -3.9204123 -3.9485936 -3.9865263][-3.9593091 -3.9255068 -3.9000401 -3.8832891 -3.8643129 -3.8605664 -3.8746407 -3.9014339 -3.9238217 -3.9366558 -3.9335413 -3.9293518 -3.9400139 -3.9663196 -4.0013542][-4.0256157 -3.999963 -3.9803991 -3.9669182 -3.9557264 -3.956389 -3.965919 -3.9815381 -3.9912341 -3.996012 -3.9946675 -3.993515 -4.0044165 -4.0285349 -4.0591955][-4.1135216 -4.0982914 -4.0863447 -4.0757957 -4.0656395 -4.0635161 -4.06673 -4.0743828 -4.0804758 -4.0846839 -4.085207 -4.0855112 -4.0952659 -4.1158047 -4.1411529][-4.1959953 -4.1889892 -4.1822515 -4.1751633 -4.1685882 -4.1651521 -4.1657896 -4.1708589 -4.1765547 -4.1801381 -4.1803641 -4.1807346 -4.186573 -4.19973 -4.2180896][-4.2630472 -4.2620707 -4.2598147 -4.2567496 -4.2535839 -4.250916 -4.2494321 -4.2500496 -4.2517734 -4.2528248 -4.2535143 -4.2541623 -4.258256 -4.26619 -4.2778606][-4.3031788 -4.3042545 -4.304009 -4.3030372 -4.3022676 -4.3016682 -4.3001976 -4.2989054 -4.2984757 -4.2983003 -4.2989097 -4.29995 -4.3029485 -4.3083344 -4.3150845]]...]
INFO - root - 2017-12-06 07:38:14.877126: step 3010, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 78h:51m:58s remains)
INFO - root - 2017-12-06 07:38:23.978249: step 3020, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 82h:47m:39s remains)
INFO - root - 2017-12-06 07:38:33.009272: step 3030, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 85h:09m:49s remains)
INFO - root - 2017-12-06 07:38:42.084485: step 3040, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 78h:31m:31s remains)
INFO - root - 2017-12-06 07:38:51.173199: step 3050, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 83h:08m:02s remains)
INFO - root - 2017-12-06 07:39:00.234378: step 3060, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.932 sec/batch; 85h:16m:01s remains)
INFO - root - 2017-12-06 07:39:09.462363: step 3070, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 86h:42m:57s remains)
INFO - root - 2017-12-06 07:39:18.477131: step 3080, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 80h:49m:59s remains)
INFO - root - 2017-12-06 07:39:27.583423: step 3090, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 85h:08m:02s remains)
INFO - root - 2017-12-06 07:39:36.657816: step 3100, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 0.772 sec/batch; 70h:40m:36s remains)
2017-12-06 07:39:37.327177: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2766089 -4.2595153 -4.2403259 -4.2217689 -4.2082505 -4.2028933 -4.2137637 -4.2357931 -4.2548957 -4.2650108 -4.2591386 -4.24286 -4.2261004 -4.2127242 -4.2027][-4.2803106 -4.2613015 -4.2418013 -4.2251215 -4.2169776 -4.2148108 -4.2248406 -4.2482324 -4.2697706 -4.2791991 -4.2731771 -4.2608447 -4.2493515 -4.2415004 -4.2389889][-4.2895417 -4.2693715 -4.2520185 -4.2387495 -4.23305 -4.2308288 -4.2354774 -4.2510571 -4.2681928 -4.2752333 -4.2739553 -4.2711225 -4.2689981 -4.2696838 -4.2759743][-4.2954559 -4.2756224 -4.2592196 -4.2459188 -4.2355137 -4.2231841 -4.2123475 -4.2143369 -4.228981 -4.2420287 -4.2524023 -4.2624922 -4.2724586 -4.2830229 -4.29645][-4.2893372 -4.2667575 -4.246376 -4.2290039 -4.2111869 -4.1853776 -4.15788 -4.14885 -4.1665 -4.1937075 -4.2209949 -4.2426839 -4.2601609 -4.2755637 -4.2919831][-4.2554159 -4.2243938 -4.198225 -4.1748238 -4.1469069 -4.108654 -4.0723991 -4.06339 -4.092236 -4.1406474 -4.1864243 -4.2178016 -4.2407637 -4.2599535 -4.2780652][-4.1873193 -4.1481457 -4.1189547 -4.0950451 -4.0610418 -4.0135312 -3.9775302 -3.9818735 -4.0283237 -4.095366 -4.156642 -4.1964607 -4.2234735 -4.2444468 -4.2641635][-4.1152577 -4.0717163 -4.0413995 -4.0161972 -3.9758229 -3.9243007 -3.8963761 -3.916523 -3.9779642 -4.0577354 -4.1318784 -4.1803207 -4.2107382 -4.2291622 -4.2470293][-4.0882587 -4.04587 -4.0163789 -3.9908607 -3.9507146 -3.9053891 -3.8909678 -3.9242368 -3.9912615 -4.0705013 -4.144002 -4.1901784 -4.215919 -4.2242866 -4.2345858][-4.1113396 -4.0788078 -4.0538416 -4.0309377 -4.0002279 -3.9753425 -3.979681 -4.0194831 -4.0795121 -4.1439428 -4.1993618 -4.2288928 -4.2375531 -4.2312818 -4.23047][-4.1644506 -4.1428647 -4.12473 -4.1066713 -4.0847869 -4.0746455 -4.0887394 -4.1268611 -4.1736231 -4.2187243 -4.2530336 -4.2667952 -4.2608571 -4.2438073 -4.2342334][-4.2202883 -4.2097011 -4.2011991 -4.1915388 -4.1776786 -4.1724796 -4.1853948 -4.2132392 -4.2421355 -4.2678638 -4.2873993 -4.2945447 -4.2842979 -4.2640381 -4.2486339][-4.2605777 -4.2602553 -4.260745 -4.259531 -4.2531438 -4.2490554 -4.2549419 -4.2682157 -4.2809482 -4.2927933 -4.302567 -4.3091297 -4.3027363 -4.2853775 -4.2684531][-4.2705612 -4.2743626 -4.2805176 -4.2881107 -4.2894535 -4.287724 -4.2863736 -4.2861805 -4.2867937 -4.28876 -4.2933855 -4.3005176 -4.2995939 -4.2862372 -4.2690668][-4.256175 -4.2598181 -4.26887 -4.2812309 -4.2877593 -4.286695 -4.2777972 -4.2658267 -4.2570333 -4.2546382 -4.2574477 -4.2645688 -4.2660489 -4.2570634 -4.244513]]...]
INFO - root - 2017-12-06 07:39:46.678470: step 3110, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 84h:07m:43s remains)
INFO - root - 2017-12-06 07:39:55.761496: step 3120, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 84h:14m:30s remains)
INFO - root - 2017-12-06 07:40:04.921712: step 3130, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.973 sec/batch; 89h:03m:49s remains)
INFO - root - 2017-12-06 07:40:14.231241: step 3140, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 86h:54m:37s remains)
INFO - root - 2017-12-06 07:40:23.201864: step 3150, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.895 sec/batch; 81h:54m:44s remains)
INFO - root - 2017-12-06 07:40:32.268230: step 3160, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 82h:36m:04s remains)
INFO - root - 2017-12-06 07:40:41.411403: step 3170, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 82h:38m:21s remains)
INFO - root - 2017-12-06 07:40:50.391523: step 3180, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.929 sec/batch; 84h:59m:18s remains)
INFO - root - 2017-12-06 07:40:59.391088: step 3190, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 85h:09m:30s remains)
INFO - root - 2017-12-06 07:41:08.491023: step 3200, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 82h:00m:37s remains)
2017-12-06 07:41:09.249770: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2985582 -4.2860475 -4.2737923 -4.2672429 -4.2704391 -4.2795806 -4.2810926 -4.2752433 -4.2671776 -4.2623563 -4.262116 -4.2601748 -4.2620282 -4.269866 -4.2791786][-4.2841353 -4.2647243 -4.2467275 -4.2352057 -4.2384353 -4.2523308 -4.2536573 -4.2453427 -4.2357049 -4.2328796 -4.2357087 -4.238955 -4.2450671 -4.2571387 -4.2689166][-4.2678657 -4.2411489 -4.2167158 -4.2017365 -4.2042708 -4.2222023 -4.22021 -4.2064662 -4.1974597 -4.19665 -4.2037888 -4.212697 -4.2238345 -4.2411623 -4.2584281][-4.2524438 -4.2194338 -4.1876431 -4.1696515 -4.1707053 -4.1866088 -4.1770434 -4.155664 -4.1465473 -4.1492114 -4.1607924 -4.1773844 -4.1958165 -4.2217832 -4.2452621][-4.2354922 -4.1954608 -4.1545386 -4.1289043 -4.1219482 -4.129601 -4.1127419 -4.0884781 -4.082438 -4.0889893 -4.1018982 -4.1227612 -4.1505375 -4.1901474 -4.224618][-4.2111921 -4.1691904 -4.123384 -4.0891857 -4.0719686 -4.0672688 -4.04144 -4.0115943 -4.000689 -4.0022936 -4.0120745 -4.0345979 -4.075387 -4.1375937 -4.1927581][-4.171515 -4.124897 -4.0722528 -4.0291505 -4.0024266 -3.9906766 -3.9610453 -3.9224057 -3.8986795 -3.8900969 -3.8964334 -3.922792 -3.9791367 -4.06974 -4.1520896][-4.1221542 -4.0722671 -4.014823 -3.9677382 -3.9379253 -3.9228239 -3.8899798 -3.8385992 -3.803092 -3.7891862 -3.7990732 -3.8341432 -3.9089804 -4.0245614 -4.1262455][-4.0999432 -4.0569587 -4.0082273 -3.9647937 -3.9339585 -3.9156556 -3.8828185 -3.8298483 -3.8010321 -3.7992444 -3.8232262 -3.8677983 -3.9450593 -4.0527592 -4.1460962][-4.1285534 -4.0991774 -4.0623364 -4.0293622 -4.0036488 -3.9871783 -3.9616585 -3.9206581 -3.9023695 -3.910377 -3.9381728 -3.9825995 -4.0456729 -4.1260839 -4.1967058][-4.1814871 -4.1627293 -4.1379189 -4.12145 -4.1085105 -4.0987754 -4.0859089 -4.066062 -4.0601921 -4.0682364 -4.0889273 -4.1228318 -4.1657329 -4.2166185 -4.2629204][-4.2400918 -4.2315469 -4.2197504 -4.2167354 -4.2148938 -4.2106771 -4.2053671 -4.2012095 -4.2022018 -4.2082157 -4.2216983 -4.2426534 -4.2663937 -4.29351 -4.3173246][-4.2825484 -4.2806773 -4.2781115 -4.2791023 -4.2807536 -4.28136 -4.2800751 -4.279593 -4.2828603 -4.2871318 -4.2950807 -4.30752 -4.3208079 -4.33292 -4.3430214][-4.3084588 -4.3075061 -4.3082619 -4.3112574 -4.314662 -4.3172665 -4.3173237 -4.3167305 -4.3178353 -4.3206997 -4.3256645 -4.3329954 -4.3400335 -4.3456049 -4.3499055][-4.3296 -4.3287177 -4.3293805 -4.3310862 -4.3323345 -4.33321 -4.3320327 -4.3314228 -4.3321838 -4.3338461 -4.3366065 -4.3405714 -4.3449988 -4.3489313 -4.3522873]]...]
INFO - root - 2017-12-06 07:41:18.193137: step 3210, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 0.784 sec/batch; 71h:43m:28s remains)
INFO - root - 2017-12-06 07:41:27.352730: step 3220, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 84h:47m:17s remains)
INFO - root - 2017-12-06 07:41:36.552325: step 3230, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 81h:22m:16s remains)
INFO - root - 2017-12-06 07:41:45.708025: step 3240, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 82h:21m:21s remains)
INFO - root - 2017-12-06 07:41:54.755295: step 3250, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 83h:47m:31s remains)
INFO - root - 2017-12-06 07:42:04.018802: step 3260, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 83h:13m:19s remains)
INFO - root - 2017-12-06 07:42:13.110653: step 3270, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.885 sec/batch; 80h:57m:50s remains)
INFO - root - 2017-12-06 07:42:22.200414: step 3280, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.913 sec/batch; 83h:26m:58s remains)
INFO - root - 2017-12-06 07:42:31.384997: step 3290, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.947 sec/batch; 86h:37m:24s remains)
INFO - root - 2017-12-06 07:42:40.699908: step 3300, loss = 2.05, batch loss = 1.99 (7.9 examples/sec; 1.012 sec/batch; 92h:31m:01s remains)
2017-12-06 07:42:41.399314: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1935992 -4.2054243 -4.2258615 -4.2381454 -4.2473121 -4.2460871 -4.2322321 -4.2288747 -4.2340279 -4.2359962 -4.2363 -4.2402945 -4.2330527 -4.2048378 -4.1670032][-4.15474 -4.1676235 -4.1955137 -4.2165089 -4.2308021 -4.23316 -4.2188773 -4.2156596 -4.2204289 -4.2201357 -4.21533 -4.2200379 -4.2231021 -4.2072968 -4.1814542][-4.1127286 -4.1241803 -4.1535225 -4.1745543 -4.1864638 -4.1911373 -4.1858325 -4.1919823 -4.2036824 -4.2019534 -4.1889977 -4.1945677 -4.2087359 -4.2031608 -4.1898603][-4.0865021 -4.0944986 -4.1186438 -4.1315589 -4.1376796 -4.145607 -4.1573105 -4.1770492 -4.1947265 -4.1873059 -4.1664305 -4.1761308 -4.1962204 -4.2024317 -4.207489][-4.0847435 -4.0921192 -4.107923 -4.1078472 -4.1034937 -4.1069441 -4.1276388 -4.156497 -4.1799498 -4.1729484 -4.15785 -4.1747465 -4.1969466 -4.2111583 -4.2263985][-4.1095705 -4.1185188 -4.1248064 -4.1063075 -4.084199 -4.0734677 -4.0835948 -4.1085367 -4.13707 -4.1497078 -4.157413 -4.1843295 -4.2057123 -4.2198219 -4.2355814][-4.1416473 -4.15558 -4.1529737 -4.1208014 -4.0833516 -4.0515089 -4.0373349 -4.0419359 -4.0650086 -4.100112 -4.135488 -4.1752872 -4.2005057 -4.2164292 -4.2319961][-4.161799 -4.1764283 -4.1704454 -4.1338725 -4.0878153 -4.0356307 -3.991606 -3.9673667 -3.9740915 -4.0247388 -4.089829 -4.1460385 -4.1794958 -4.1984138 -4.2171593][-4.1578827 -4.17606 -4.1744761 -4.1402135 -4.0897689 -4.0286679 -3.9674337 -3.9172883 -3.9058521 -3.9649258 -4.050643 -4.1179223 -4.1587877 -4.1795754 -4.1968513][-4.1543636 -4.1794305 -4.1812925 -4.157465 -4.11728 -4.0679231 -4.0159917 -3.9663572 -3.9423375 -3.9799027 -4.0478697 -4.1054344 -4.1456962 -4.1621671 -4.1713972][-4.1684189 -4.1932378 -4.1964588 -4.1865735 -4.1621628 -4.132926 -4.1011767 -4.0671358 -4.0379567 -4.0494595 -4.0848308 -4.1147637 -4.1421552 -4.1491561 -4.1482368][-4.1904974 -4.2155857 -4.2208805 -4.2145224 -4.1893854 -4.1668773 -4.1463113 -4.1275516 -4.1039658 -4.10164 -4.1116896 -4.1209574 -4.1376781 -4.14117 -4.1357026][-4.2072892 -4.2295642 -4.2368736 -4.227881 -4.1974154 -4.1750665 -4.1608529 -4.1564069 -4.1421976 -4.1300468 -4.120399 -4.1142097 -4.1306591 -4.1410775 -4.1390114][-4.2274585 -4.24718 -4.2579551 -4.2500968 -4.2187076 -4.1910248 -4.1757927 -4.1807075 -4.1741323 -4.157198 -4.1370077 -4.1259594 -4.1446466 -4.1601167 -4.1646452][-4.245369 -4.2592506 -4.2729812 -4.2674985 -4.2369485 -4.2091284 -4.1965976 -4.2139368 -4.2193022 -4.2069025 -4.1868415 -4.1770353 -4.19316 -4.2051125 -4.2080965]]...]
INFO - root - 2017-12-06 07:42:50.487871: step 3310, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 77h:48m:51s remains)
INFO - root - 2017-12-06 07:42:59.449095: step 3320, loss = 2.10, batch loss = 2.05 (8.7 examples/sec; 0.915 sec/batch; 83h:38m:47s remains)
INFO - root - 2017-12-06 07:43:08.677035: step 3330, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 81h:02m:49s remains)
INFO - root - 2017-12-06 07:43:17.860752: step 3340, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.902 sec/batch; 82h:30m:10s remains)
INFO - root - 2017-12-06 07:43:26.976543: step 3350, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.945 sec/batch; 86h:21m:41s remains)
INFO - root - 2017-12-06 07:43:36.307811: step 3360, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 82h:32m:07s remains)
INFO - root - 2017-12-06 07:43:45.407285: step 3370, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.890 sec/batch; 81h:20m:43s remains)
INFO - root - 2017-12-06 07:43:54.616397: step 3380, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.946 sec/batch; 86h:27m:16s remains)
INFO - root - 2017-12-06 07:44:03.682535: step 3390, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 84h:36m:36s remains)
INFO - root - 2017-12-06 07:44:13.076039: step 3400, loss = 2.04, batch loss = 1.99 (8.2 examples/sec; 0.980 sec/batch; 89h:37m:07s remains)
2017-12-06 07:44:13.754074: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2568088 -4.2642536 -4.2926497 -4.3108416 -4.3184161 -4.3211408 -4.3119721 -4.2941623 -4.2818995 -4.2765183 -4.2781291 -4.2851849 -4.2857184 -4.2673607 -4.2299376][-4.2446718 -4.2487574 -4.2759428 -4.29179 -4.2982435 -4.2977505 -4.2840314 -4.2611227 -4.246027 -4.2380795 -4.2398648 -4.2503567 -4.2525263 -4.2277713 -4.1763544][-4.212039 -4.2066689 -4.2345114 -4.2544918 -4.263206 -4.2591047 -4.2378168 -4.2082729 -4.1897993 -4.1827292 -4.1922121 -4.2129765 -4.2223682 -4.1978464 -4.1426153][-4.162632 -4.1403656 -4.1721587 -4.2034831 -4.2159843 -4.2067947 -4.1745429 -4.1369753 -4.116323 -4.11893 -4.1461811 -4.1835542 -4.2053485 -4.188519 -4.1394157][-4.1062675 -4.0671778 -4.1077995 -4.1565757 -4.1742568 -4.1548061 -4.1037164 -4.0523338 -4.0343575 -4.0590515 -4.1129823 -4.1650915 -4.1940908 -4.1876616 -4.1548643][-4.0741057 -4.0308027 -4.0785661 -4.1295967 -4.1385889 -4.1003056 -4.0232024 -3.9486296 -3.9412224 -4.0016384 -4.0850534 -4.1476712 -4.1800213 -4.1859965 -4.174293][-4.0886836 -4.0608134 -4.1066308 -4.1393709 -4.1208172 -4.0548043 -3.9472795 -3.8472087 -3.8575451 -3.9570341 -4.0627365 -4.1290455 -4.1630821 -4.1771226 -4.1818347][-4.1315007 -4.1232882 -4.1588564 -4.1632738 -4.1179543 -4.0300112 -3.9008195 -3.7942977 -3.8293967 -3.9562154 -4.06557 -4.1291819 -4.16353 -4.1798429 -4.1869397][-4.1733503 -4.17418 -4.1979847 -4.1896753 -4.136878 -4.0466413 -3.9278204 -3.8544784 -3.9019356 -4.0165133 -4.1046548 -4.1571026 -4.1874404 -4.2003078 -4.1978416][-4.2074156 -4.2071137 -4.2214022 -4.2132916 -4.1700621 -4.0973492 -4.0197539 -3.9930737 -4.0369244 -4.1147485 -4.1711583 -4.205214 -4.2271 -4.2336006 -4.2192144][-4.2547426 -4.2495542 -4.2549787 -4.2465911 -4.2181859 -4.1701179 -4.1287255 -4.1281314 -4.1586757 -4.2042913 -4.236639 -4.25618 -4.2663713 -4.2620435 -4.2434273][-4.2976823 -4.2896376 -4.2835836 -4.2684083 -4.249826 -4.2215738 -4.2036161 -4.2109237 -4.2312541 -4.2616062 -4.287631 -4.3002877 -4.2980261 -4.2837639 -4.2655272][-4.3233213 -4.3153543 -4.3022766 -4.2840614 -4.2700615 -4.2547097 -4.2486777 -4.257165 -4.2729421 -4.2966533 -4.3198237 -4.3283434 -4.3191233 -4.3013258 -4.2849274][-4.3195992 -4.3145337 -4.3045621 -4.2907615 -4.2805176 -4.2714834 -4.2712026 -4.282918 -4.29912 -4.3185687 -4.334579 -4.3363309 -4.3237314 -4.306644 -4.2916627][-4.302948 -4.3002625 -4.2972579 -4.2936382 -4.2896609 -4.282743 -4.2823253 -4.2942662 -4.308 -4.3206539 -4.3271036 -4.3243361 -4.3131709 -4.3004355 -4.2894506]]...]
INFO - root - 2017-12-06 07:44:22.772398: step 3410, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.896 sec/batch; 81h:53m:58s remains)
INFO - root - 2017-12-06 07:44:32.037190: step 3420, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 82h:02m:23s remains)
INFO - root - 2017-12-06 07:44:41.299854: step 3430, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.965 sec/batch; 88h:13m:09s remains)
INFO - root - 2017-12-06 07:44:52.414951: step 3440, loss = 2.06, batch loss = 2.00 (6.7 examples/sec; 1.195 sec/batch; 109h:11m:11s remains)
INFO - root - 2017-12-06 07:45:04.380262: step 3450, loss = 2.08, batch loss = 2.02 (6.7 examples/sec; 1.202 sec/batch; 109h:51m:56s remains)
INFO - root - 2017-12-06 07:45:16.201630: step 3460, loss = 2.07, batch loss = 2.01 (6.8 examples/sec; 1.181 sec/batch; 107h:56m:48s remains)
INFO - root - 2017-12-06 07:45:28.236492: step 3470, loss = 2.07, batch loss = 2.01 (7.0 examples/sec; 1.144 sec/batch; 104h:31m:00s remains)
INFO - root - 2017-12-06 07:45:40.199394: step 3480, loss = 2.07, batch loss = 2.01 (6.4 examples/sec; 1.245 sec/batch; 113h:48m:32s remains)
INFO - root - 2017-12-06 07:45:51.998602: step 3490, loss = 2.06, batch loss = 2.00 (6.7 examples/sec; 1.186 sec/batch; 108h:23m:51s remains)
INFO - root - 2017-12-06 07:46:03.849064: step 3500, loss = 2.03, batch loss = 1.97 (6.9 examples/sec; 1.165 sec/batch; 106h:28m:26s remains)
2017-12-06 07:46:04.658088: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3411779 -4.3441319 -4.3455095 -4.3448682 -4.3431225 -4.3426604 -4.3436313 -4.3445415 -4.3467159 -4.3498092 -4.3538866 -4.357728 -4.358077 -4.3551025 -4.3521547][-4.3370223 -4.3360982 -4.3351097 -4.3337812 -4.3309364 -4.329248 -4.3294387 -4.3311644 -4.3352494 -4.3392987 -4.3438187 -4.3482924 -4.3485842 -4.3447948 -4.3424053][-4.3274961 -4.3231025 -4.3200579 -4.3185396 -4.3146276 -4.3100662 -4.3070712 -4.30772 -4.3125043 -4.3149576 -4.3172784 -4.320405 -4.3228478 -4.3222537 -4.3254447][-4.3195868 -4.31178 -4.304718 -4.299984 -4.29237 -4.2830386 -4.2754421 -4.2718034 -4.2737226 -4.274159 -4.2748241 -4.2778015 -4.2837648 -4.2890406 -4.2999115][-4.31127 -4.2987738 -4.2853665 -4.2718 -4.2572775 -4.243855 -4.2336545 -4.2271123 -4.2251496 -4.2251439 -4.227294 -4.2348747 -4.2457957 -4.2549639 -4.2713985][-4.300137 -4.2845712 -4.2646909 -4.2440939 -4.2241116 -4.2072878 -4.19786 -4.1941342 -4.1885262 -4.1848927 -4.1895609 -4.2054715 -4.2215133 -4.2331471 -4.2508979][-4.2891884 -4.2716994 -4.2488084 -4.2257857 -4.1985555 -4.1749339 -4.166163 -4.1682105 -4.1648841 -4.15998 -4.1662192 -4.1888471 -4.2076793 -4.219245 -4.2372289][-4.2836027 -4.2667007 -4.2447581 -4.2183886 -4.1794596 -4.14176 -4.1312904 -4.1395597 -4.149859 -4.1557059 -4.1698256 -4.1974449 -4.21529 -4.2224188 -4.234674][-4.2791948 -4.2679443 -4.2462482 -4.2106905 -4.1574979 -4.112061 -4.1060829 -4.1221256 -4.1481376 -4.1719041 -4.1970749 -4.2213883 -4.2330227 -4.232739 -4.2372437][-4.2810435 -4.2780318 -4.2544541 -4.2069235 -4.1452174 -4.1025229 -4.107872 -4.1346107 -4.1707859 -4.2004004 -4.2259369 -4.2456069 -4.2516532 -4.24468 -4.2440557][-4.2915692 -4.2977986 -4.2789092 -4.233603 -4.1772032 -4.1420074 -4.1503048 -4.1767707 -4.2066908 -4.2309127 -4.2494855 -4.2634535 -4.267756 -4.2605457 -4.2565904][-4.3081656 -4.3214173 -4.3129396 -4.2836108 -4.2422256 -4.21627 -4.2205648 -4.2372675 -4.255187 -4.2689672 -4.2764792 -4.2813492 -4.2810426 -4.2714481 -4.2634654][-4.3126731 -4.3287845 -4.3306532 -4.3188162 -4.2941828 -4.2746162 -4.2736144 -4.2815037 -4.29112 -4.2971568 -4.29756 -4.2952404 -4.2890329 -4.2751307 -4.2617993][-4.3049188 -4.3183203 -4.323204 -4.3204322 -4.3067837 -4.2914581 -4.2866077 -4.2916288 -4.3010554 -4.304338 -4.3023505 -4.2958775 -4.2857275 -4.2683067 -4.2536297][-4.2834287 -4.2901182 -4.29167 -4.2909303 -4.2830997 -4.2715273 -4.2645802 -4.2671051 -4.2783957 -4.2826657 -4.2796826 -4.2733421 -4.26534 -4.2517691 -4.2412348]]...]
INFO - root - 2017-12-06 07:46:16.608954: step 3510, loss = 2.08, batch loss = 2.02 (6.6 examples/sec; 1.217 sec/batch; 111h:14m:35s remains)
INFO - root - 2017-12-06 07:46:28.256509: step 3520, loss = 2.07, batch loss = 2.01 (7.1 examples/sec; 1.132 sec/batch; 103h:27m:09s remains)
INFO - root - 2017-12-06 07:46:39.990422: step 3530, loss = 2.06, batch loss = 2.00 (6.9 examples/sec; 1.165 sec/batch; 106h:27m:34s remains)
INFO - root - 2017-12-06 07:46:51.712483: step 3540, loss = 2.08, batch loss = 2.02 (6.6 examples/sec; 1.214 sec/batch; 110h:57m:15s remains)
INFO - root - 2017-12-06 07:47:03.424020: step 3550, loss = 2.05, batch loss = 2.00 (6.9 examples/sec; 1.164 sec/batch; 106h:24m:16s remains)
INFO - root - 2017-12-06 07:47:15.223627: step 3560, loss = 2.07, batch loss = 2.01 (6.8 examples/sec; 1.169 sec/batch; 106h:48m:18s remains)
INFO - root - 2017-12-06 07:47:27.061953: step 3570, loss = 2.09, batch loss = 2.03 (6.8 examples/sec; 1.174 sec/batch; 107h:16m:05s remains)
INFO - root - 2017-12-06 07:47:39.057624: step 3580, loss = 2.06, batch loss = 2.00 (6.7 examples/sec; 1.202 sec/batch; 109h:47m:14s remains)
INFO - root - 2017-12-06 07:47:50.845526: step 3590, loss = 2.07, batch loss = 2.01 (7.1 examples/sec; 1.132 sec/batch; 103h:22m:45s remains)
INFO - root - 2017-12-06 07:48:02.602567: step 3600, loss = 2.07, batch loss = 2.01 (6.9 examples/sec; 1.156 sec/batch; 105h:34m:49s remains)
2017-12-06 07:48:03.434258: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2763696 -4.2650003 -4.2503872 -4.2273521 -4.1963596 -4.1723814 -4.1639733 -4.1653152 -4.1668792 -4.1787624 -4.1999006 -4.2097759 -4.2019262 -4.1782637 -4.1535659][-4.26167 -4.247098 -4.2390571 -4.2271037 -4.1980391 -4.1629882 -4.1445093 -4.1489892 -4.1635084 -4.1809578 -4.2047992 -4.2150683 -4.2021103 -4.1690025 -4.1363063][-4.2407484 -4.2308736 -4.2330031 -4.235673 -4.2151008 -4.1740417 -4.14374 -4.1447144 -4.1682167 -4.1916809 -4.2175655 -4.2299776 -4.2190542 -4.1876006 -4.1556916][-4.2166748 -4.2190762 -4.2314978 -4.2460432 -4.2377496 -4.19745 -4.1585221 -4.1544776 -4.184401 -4.2152896 -4.2437291 -4.26039 -4.2559304 -4.2309842 -4.2019358][-4.1910062 -4.2046895 -4.222352 -4.2405152 -4.24015 -4.2045021 -4.1618214 -4.1565623 -4.1905193 -4.2280235 -4.2608619 -4.283618 -4.2872281 -4.2699776 -4.2413468][-4.1662931 -4.183022 -4.2008471 -4.217947 -4.22083 -4.1927094 -4.1534452 -4.1496062 -4.1873302 -4.2266397 -4.2597179 -4.286521 -4.2986021 -4.28737 -4.2582092][-4.1522884 -4.1585455 -4.1710739 -4.1871872 -4.1916404 -4.1705027 -4.1384125 -4.1399193 -4.1789956 -4.21501 -4.2414107 -4.2641292 -4.2800388 -4.2730274 -4.2482257][-4.1555533 -4.1427732 -4.1452417 -4.1633692 -4.1739011 -4.1591158 -4.1348782 -4.1385331 -4.171402 -4.1991897 -4.2137728 -4.2281041 -4.242815 -4.239964 -4.2260232][-4.1798739 -4.1527815 -4.1439319 -4.1606236 -4.1785841 -4.1739655 -4.1601419 -4.1640921 -4.1861067 -4.1975803 -4.1978173 -4.2010088 -4.2107897 -4.2123418 -4.2117395][-4.2077265 -4.1799736 -4.1652417 -4.1756573 -4.1942749 -4.1966796 -4.1895795 -4.1938205 -4.206265 -4.2033491 -4.1937213 -4.1909022 -4.1957741 -4.1982841 -4.2053747][-4.2258458 -4.2058535 -4.1908755 -4.1947579 -4.2092004 -4.214695 -4.2113266 -4.2151756 -4.2205448 -4.2109141 -4.1979032 -4.1969814 -4.2006636 -4.2009587 -4.2073884][-4.2366052 -4.2238297 -4.2102542 -4.2088561 -4.217804 -4.2228613 -4.2218065 -4.224504 -4.2237639 -4.2129745 -4.2008233 -4.2033448 -4.2081895 -4.20813 -4.2122893][-4.2453504 -4.2374735 -4.2256737 -4.2209311 -4.226583 -4.2325535 -4.2334642 -4.2324123 -4.2274184 -4.2167578 -4.2073636 -4.2106924 -4.2158041 -4.2168388 -4.2210813][-4.2566638 -4.2531576 -4.245697 -4.241251 -4.245008 -4.2508397 -4.2518473 -4.2473159 -4.2394814 -4.2288165 -4.2212925 -4.2231007 -4.2287521 -4.2320938 -4.237072][-4.2680798 -4.2683043 -4.2645807 -4.2627368 -4.2671843 -4.27319 -4.2754717 -4.2711654 -4.2633452 -4.2541013 -4.2465167 -4.2456594 -4.2494349 -4.2522297 -4.2548313]]...]
INFO - root - 2017-12-06 07:48:15.154998: step 3610, loss = 2.08, batch loss = 2.02 (6.6 examples/sec; 1.215 sec/batch; 110h:59m:50s remains)
INFO - root - 2017-12-06 07:48:26.827386: step 3620, loss = 2.05, batch loss = 1.99 (6.7 examples/sec; 1.186 sec/batch; 108h:19m:47s remains)
INFO - root - 2017-12-06 07:48:38.560797: step 3630, loss = 2.05, batch loss = 1.99 (6.6 examples/sec; 1.220 sec/batch; 111h:28m:34s remains)
INFO - root - 2017-12-06 07:48:50.356204: step 3640, loss = 2.05, batch loss = 2.00 (7.0 examples/sec; 1.150 sec/batch; 105h:03m:53s remains)
INFO - root - 2017-12-06 07:49:02.218730: step 3650, loss = 2.08, batch loss = 2.02 (7.0 examples/sec; 1.141 sec/batch; 104h:15m:11s remains)
INFO - root - 2017-12-06 07:49:14.093818: step 3660, loss = 2.10, batch loss = 2.04 (6.5 examples/sec; 1.231 sec/batch; 112h:27m:05s remains)
INFO - root - 2017-12-06 07:49:25.837155: step 3670, loss = 2.06, batch loss = 2.00 (6.8 examples/sec; 1.181 sec/batch; 107h:50m:21s remains)
INFO - root - 2017-12-06 07:49:37.576591: step 3680, loss = 2.05, batch loss = 2.00 (6.6 examples/sec; 1.209 sec/batch; 110h:27m:23s remains)
INFO - root - 2017-12-06 07:49:49.318646: step 3690, loss = 2.07, batch loss = 2.01 (7.0 examples/sec; 1.144 sec/batch; 104h:30m:35s remains)
INFO - root - 2017-12-06 07:50:01.264475: step 3700, loss = 2.06, batch loss = 2.00 (6.3 examples/sec; 1.263 sec/batch; 115h:22m:32s remains)
2017-12-06 07:50:02.071526: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2020845 -4.1968036 -4.2123456 -4.232842 -4.2519145 -4.2681856 -4.2617316 -4.2396064 -4.1987343 -4.1795592 -4.19445 -4.2389169 -4.2813883 -4.3111467 -4.3250532][-4.1630759 -4.1672277 -4.1907606 -4.21714 -4.2353125 -4.2486567 -4.2330456 -4.2004061 -4.1489539 -4.1282411 -4.1516118 -4.2052212 -4.2557464 -4.2954884 -4.3153877][-4.1662593 -4.1758876 -4.2010226 -4.2275004 -4.2415638 -4.2429953 -4.2138443 -4.17453 -4.1245904 -4.1134076 -4.1416554 -4.1954689 -4.2446713 -4.2838864 -4.304646][-4.182694 -4.1959829 -4.2206807 -4.243084 -4.2471066 -4.226522 -4.1759281 -4.131021 -4.1002636 -4.115541 -4.1535821 -4.208971 -4.252779 -4.2853184 -4.3009644][-4.1882029 -4.2060127 -4.2309322 -4.2449207 -4.22875 -4.175364 -4.0958848 -4.0576634 -4.0682244 -4.1215134 -4.176342 -4.2361364 -4.2732315 -4.2947407 -4.3044124][-4.1975679 -4.2125154 -4.2260518 -4.2207665 -4.1686516 -4.0696363 -3.9523447 -3.9304049 -4.0041614 -4.1066289 -4.181798 -4.247015 -4.2826419 -4.2988257 -4.3083377][-4.2092347 -4.2127047 -4.2058253 -4.173996 -4.0796609 -3.9176869 -3.7355998 -3.73568 -3.8955927 -4.0603533 -4.1686845 -4.246263 -4.2862229 -4.3017869 -4.3124466][-4.2312546 -4.2217216 -4.1956549 -4.1361341 -4.0080881 -3.7874839 -3.5391541 -3.5766263 -3.8187802 -4.0261793 -4.1522031 -4.2359247 -4.2796874 -4.2980614 -4.3100648][-4.2469263 -4.2267156 -4.1927137 -4.1265845 -4.0077505 -3.8050849 -3.5994506 -3.6553831 -3.8722861 -4.0533848 -4.162374 -4.2286716 -4.261333 -4.2820239 -4.2992654][-4.2564039 -4.23304 -4.2001467 -4.1421733 -4.0571027 -3.9261975 -3.8154991 -3.8738575 -4.0168371 -4.1347728 -4.2041488 -4.2400465 -4.2548528 -4.27128 -4.290514][-4.2651143 -4.2425933 -4.213079 -4.1682568 -4.1152935 -4.04419 -3.9969788 -4.0503006 -4.1385059 -4.2080927 -4.249135 -4.2668118 -4.2684174 -4.2749653 -4.2898149][-4.2762809 -4.2592835 -4.2366414 -4.2060618 -4.1770763 -4.1422534 -4.1253586 -4.1665139 -4.2181277 -4.2557077 -4.2780004 -4.2880955 -4.285183 -4.2869077 -4.2970428][-4.2842107 -4.2769651 -4.2656212 -4.2523394 -4.2405834 -4.2211962 -4.2091947 -4.2354369 -4.2651463 -4.2870755 -4.2981663 -4.3032274 -4.2988062 -4.2994347 -4.3071518][-4.28698 -4.28535 -4.2824397 -4.2781715 -4.2714167 -4.2540479 -4.2421427 -4.2599978 -4.2821407 -4.2982597 -4.3079762 -4.3151307 -4.3123493 -4.3132524 -4.3176851][-4.2835984 -4.284514 -4.2851758 -4.2807341 -4.26872 -4.2478843 -4.236156 -4.2509532 -4.2729769 -4.290905 -4.3040209 -4.3157229 -4.3172526 -4.3198314 -4.3232007]]...]
INFO - root - 2017-12-06 07:50:13.765871: step 3710, loss = 2.06, batch loss = 2.00 (6.9 examples/sec; 1.154 sec/batch; 105h:24m:32s remains)
INFO - root - 2017-12-06 07:50:25.587865: step 3720, loss = 2.07, batch loss = 2.01 (6.8 examples/sec; 1.184 sec/batch; 108h:07m:04s remains)
INFO - root - 2017-12-06 07:50:37.567009: step 3730, loss = 2.05, batch loss = 2.00 (6.8 examples/sec; 1.170 sec/batch; 106h:50m:54s remains)
INFO - root - 2017-12-06 07:50:49.520465: step 3740, loss = 2.08, batch loss = 2.02 (6.8 examples/sec; 1.174 sec/batch; 107h:15m:06s remains)
INFO - root - 2017-12-06 07:51:01.281735: step 3750, loss = 2.09, batch loss = 2.03 (6.9 examples/sec; 1.167 sec/batch; 106h:32m:25s remains)
INFO - root - 2017-12-06 07:51:12.980851: step 3760, loss = 2.07, batch loss = 2.01 (7.0 examples/sec; 1.142 sec/batch; 104h:18m:51s remains)
INFO - root - 2017-12-06 07:51:25.057614: step 3770, loss = 2.05, batch loss = 1.99 (6.4 examples/sec; 1.246 sec/batch; 113h:47m:39s remains)
INFO - root - 2017-12-06 07:51:36.916472: step 3780, loss = 2.05, batch loss = 1.99 (6.8 examples/sec; 1.177 sec/batch; 107h:28m:58s remains)
INFO - root - 2017-12-06 07:51:48.801274: step 3790, loss = 2.06, batch loss = 2.00 (6.6 examples/sec; 1.205 sec/batch; 109h:59m:14s remains)
INFO - root - 2017-12-06 07:52:00.680873: step 3800, loss = 2.08, batch loss = 2.02 (6.5 examples/sec; 1.231 sec/batch; 112h:21m:51s remains)
2017-12-06 07:52:01.466984: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1958189 -4.120894 -4.073204 -4.0697284 -4.1314282 -4.2039475 -4.233551 -4.2318573 -4.2135019 -4.1929469 -4.1935196 -4.2214756 -4.2370033 -4.2408128 -4.2488651][-4.2110591 -4.1550922 -4.1272197 -4.1390758 -4.1936765 -4.2405281 -4.2488556 -4.2383447 -4.2133632 -4.1880064 -4.1958532 -4.2337623 -4.2600145 -4.2759004 -4.2814384][-4.2502174 -4.2139478 -4.2002335 -4.2142005 -4.2507505 -4.2730093 -4.2629466 -4.2338963 -4.1943755 -4.1704054 -4.193306 -4.2382355 -4.2687912 -4.2897792 -4.2943892][-4.2952728 -4.2790847 -4.272634 -4.2783422 -4.2912955 -4.2875376 -4.2509513 -4.1884689 -4.1293783 -4.1225519 -4.1763639 -4.2346044 -4.2689 -4.2888789 -4.2912707][-4.31747 -4.3197365 -4.32132 -4.3189349 -4.3084707 -4.2763548 -4.2009425 -4.0887008 -4.0125704 -4.0464811 -4.1394682 -4.2091866 -4.250844 -4.277389 -4.2876434][-4.3267822 -4.3391767 -4.3404541 -4.324182 -4.2903647 -4.2275286 -4.1061225 -3.9344606 -3.8575017 -3.9590611 -4.0938997 -4.1762981 -4.2265644 -4.2621613 -4.2832565][-4.33377 -4.3492894 -4.3461637 -4.3161325 -4.2574964 -4.1561675 -3.983438 -3.7601116 -3.7184136 -3.8977821 -4.0622606 -4.1549768 -4.2138877 -4.2571621 -4.2819448][-4.3356376 -4.3520088 -4.3429322 -4.300272 -4.2179565 -4.0919681 -3.901711 -3.6954198 -3.72332 -3.9188213 -4.0716076 -4.1547036 -4.2138114 -4.2607365 -4.284183][-4.3302054 -4.3407164 -4.3236494 -4.2689042 -4.1749163 -4.0574474 -3.9054432 -3.7889256 -3.8599389 -4.0105286 -4.1203604 -4.1832094 -4.2337675 -4.2761116 -4.2928696][-4.3228769 -4.3278661 -4.3065495 -4.24857 -4.1620331 -4.0720339 -3.9788218 -3.9394283 -4.0116677 -4.1105781 -4.1831222 -4.2315168 -4.27152 -4.2999439 -4.3059916][-4.3206568 -4.324296 -4.3022428 -4.2508883 -4.1884618 -4.1319265 -4.0880723 -4.0861883 -4.1385555 -4.1983252 -4.2449131 -4.2818766 -4.3098912 -4.3227315 -4.3205442][-4.3223009 -4.32463 -4.3057623 -4.2713466 -4.23897 -4.2139063 -4.2038445 -4.2147326 -4.2430921 -4.2746639 -4.3008537 -4.3233948 -4.3367991 -4.3362222 -4.3288031][-4.3253994 -4.32692 -4.3148704 -4.2990594 -4.2911215 -4.2877774 -4.2916627 -4.2993717 -4.3088007 -4.3217783 -4.3328838 -4.3424244 -4.3447347 -4.3363261 -4.3282743][-4.3262382 -4.3283134 -4.3251567 -4.3247643 -4.3281937 -4.3320179 -4.3360782 -4.3336873 -4.3313594 -4.3345442 -4.3375187 -4.339395 -4.3359852 -4.3276339 -4.3228912][-4.32022 -4.3220692 -4.32369 -4.3270907 -4.3319173 -4.336791 -4.33967 -4.3351965 -4.3288817 -4.3252568 -4.323863 -4.3236837 -4.3206124 -4.3160157 -4.3144197]]...]
INFO - root - 2017-12-06 07:52:12.931759: step 3810, loss = 2.06, batch loss = 2.00 (7.1 examples/sec; 1.133 sec/batch; 103h:25m:20s remains)
INFO - root - 2017-12-06 07:52:24.570874: step 3820, loss = 2.06, batch loss = 2.00 (6.7 examples/sec; 1.189 sec/batch; 108h:35m:30s remains)
INFO - root - 2017-12-06 07:52:36.564818: step 3830, loss = 2.06, batch loss = 2.00 (6.2 examples/sec; 1.297 sec/batch; 118h:23m:23s remains)
INFO - root - 2017-12-06 07:52:48.434919: step 3840, loss = 2.05, batch loss = 1.99 (6.9 examples/sec; 1.151 sec/batch; 105h:06m:21s remains)
INFO - root - 2017-12-06 07:53:00.460527: step 3850, loss = 2.09, batch loss = 2.03 (6.2 examples/sec; 1.285 sec/batch; 117h:18m:14s remains)
INFO - root - 2017-12-06 07:53:12.362070: step 3860, loss = 2.06, batch loss = 2.00 (6.7 examples/sec; 1.191 sec/batch; 108h:44m:45s remains)
INFO - root - 2017-12-06 07:53:24.145543: step 3870, loss = 2.07, batch loss = 2.01 (7.1 examples/sec; 1.133 sec/batch; 103h:25m:51s remains)
INFO - root - 2017-12-06 07:53:35.813514: step 3880, loss = 2.08, batch loss = 2.02 (6.4 examples/sec; 1.244 sec/batch; 113h:31m:52s remains)
INFO - root - 2017-12-06 07:53:47.749772: step 3890, loss = 2.06, batch loss = 2.00 (6.8 examples/sec; 1.184 sec/batch; 108h:02m:28s remains)
INFO - root - 2017-12-06 07:53:59.664369: step 3900, loss = 2.06, batch loss = 2.00 (6.5 examples/sec; 1.231 sec/batch; 112h:21m:37s remains)
2017-12-06 07:54:00.451231: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2166014 -4.1981177 -4.2049894 -4.2308297 -4.2546873 -4.2614 -4.2507029 -4.2236738 -4.1972189 -4.1806417 -4.1698761 -4.162004 -4.1504426 -4.1132269 -4.0732851][-4.1850543 -4.1573973 -4.1624937 -4.1966457 -4.2210903 -4.2186227 -4.1960921 -4.1650705 -4.1415954 -4.128293 -4.1221814 -4.124866 -4.124948 -4.0939369 -4.052681][-4.170013 -4.134676 -4.1366091 -4.1728997 -4.1923361 -4.1775641 -4.1450634 -4.1128511 -4.0940056 -4.0914941 -4.0956564 -4.1132956 -4.1268492 -4.1083584 -4.0752392][-4.1793795 -4.1384482 -4.135973 -4.1681447 -4.1794724 -4.1530347 -4.111486 -4.0820279 -4.075973 -4.0904245 -4.1073565 -4.1358032 -4.1542344 -4.1441169 -4.124054][-4.2007842 -4.157053 -4.1473684 -4.1655064 -4.1624603 -4.11796 -4.0639973 -4.0486312 -4.0665336 -4.1059346 -4.1402025 -4.1694083 -4.1843615 -4.1786594 -4.1741915][-4.2148571 -4.1682205 -4.1464963 -4.1435418 -4.1207423 -4.0497169 -3.9670017 -3.9603078 -4.0102696 -4.0860152 -4.1417952 -4.1712976 -4.182323 -4.1812305 -4.188561][-4.2170906 -4.1669297 -4.133297 -4.1126671 -4.0651884 -3.9526908 -3.820627 -3.8115158 -3.9097757 -4.0289626 -4.1041217 -4.1327934 -4.137938 -4.1458154 -4.164093][-4.1980686 -4.1460996 -4.1081758 -4.0775237 -4.010078 -3.8608398 -3.6782315 -3.67226 -3.829706 -3.983593 -4.0679288 -4.0882173 -4.0811362 -4.0920315 -4.1197724][-4.1589289 -4.11352 -4.0834117 -4.0590434 -3.9947209 -3.8531294 -3.6830626 -3.6896558 -3.8500619 -3.9888191 -4.0522218 -4.0521755 -4.0331607 -4.0442328 -4.0748334][-4.1080761 -4.0758481 -4.06855 -4.0676508 -4.0317574 -3.9420307 -3.8419855 -3.848022 -3.9489753 -4.0337415 -4.057992 -4.0406766 -4.0232458 -4.0373654 -4.065516][-4.0735555 -4.0519719 -4.0670023 -4.0926733 -4.0894322 -4.0546331 -4.0150089 -4.0171933 -4.0614028 -4.0957246 -4.0927262 -4.07409 -4.0698738 -4.0890179 -4.1103287][-4.0884361 -4.0721908 -4.097877 -4.1380053 -4.1560621 -4.1549473 -4.1468859 -4.148983 -4.1646395 -4.1766109 -4.1670766 -4.1570864 -4.1630731 -4.1782141 -4.1878934][-4.1572161 -4.14296 -4.1636152 -4.2001905 -4.2234035 -4.2346249 -4.2375383 -4.24225 -4.2509732 -4.2573733 -4.2524414 -4.2473359 -4.251143 -4.2573929 -4.2582107][-4.2414207 -4.2286606 -4.2371569 -4.2585521 -4.2755437 -4.284399 -4.2895532 -4.2941356 -4.3012633 -4.3070073 -4.3059297 -4.3024707 -4.3011508 -4.3003278 -4.2962365][-4.307282 -4.2974887 -4.2969303 -4.30257 -4.3088307 -4.3124466 -4.31587 -4.3192105 -4.3242593 -4.3291297 -4.3307195 -4.3289876 -4.3251739 -4.321198 -4.3153553]]...]
INFO - root - 2017-12-06 07:54:12.300506: step 3910, loss = 2.08, batch loss = 2.03 (7.3 examples/sec; 1.102 sec/batch; 100h:33m:16s remains)
INFO - root - 2017-12-06 07:54:24.063012: step 3920, loss = 2.07, batch loss = 2.01 (6.7 examples/sec; 1.187 sec/batch; 108h:19m:01s remains)
INFO - root - 2017-12-06 07:54:35.774416: step 3930, loss = 2.09, batch loss = 2.04 (6.7 examples/sec; 1.192 sec/batch; 108h:49m:34s remains)
INFO - root - 2017-12-06 07:54:47.298492: step 3940, loss = 2.07, batch loss = 2.01 (6.8 examples/sec; 1.176 sec/batch; 107h:17m:28s remains)
INFO - root - 2017-12-06 07:54:58.912540: step 3950, loss = 2.09, batch loss = 2.03 (6.8 examples/sec; 1.172 sec/batch; 106h:57m:07s remains)
INFO - root - 2017-12-06 07:55:10.538792: step 3960, loss = 2.05, batch loss = 2.00 (7.5 examples/sec; 1.066 sec/batch; 97h:15m:23s remains)
INFO - root - 2017-12-06 07:55:22.325103: step 3970, loss = 2.07, batch loss = 2.01 (6.6 examples/sec; 1.216 sec/batch; 110h:56m:44s remains)
INFO - root - 2017-12-06 07:55:34.086797: step 3980, loss = 2.11, batch loss = 2.05 (6.9 examples/sec; 1.161 sec/batch; 105h:54m:59s remains)
INFO - root - 2017-12-06 07:55:45.963990: step 3990, loss = 2.06, batch loss = 2.00 (6.8 examples/sec; 1.173 sec/batch; 107h:03m:50s remains)
INFO - root - 2017-12-06 07:55:58.061289: step 4000, loss = 2.07, batch loss = 2.01 (6.7 examples/sec; 1.189 sec/batch; 108h:28m:16s remains)
2017-12-06 07:55:58.897502: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.21351 -4.2139025 -4.2416024 -4.2714338 -4.2832608 -4.2839003 -4.2849193 -4.28956 -4.2968082 -4.3068705 -4.3217425 -4.326057 -4.3137755 -4.2994566 -4.2847919][-4.20787 -4.2074351 -4.2349563 -4.2654819 -4.2776995 -4.2748971 -4.2750063 -4.2781653 -4.2857428 -4.2982264 -4.3135605 -4.3175788 -4.3085918 -4.29755 -4.2809253][-4.2077074 -4.2002668 -4.2187753 -4.2420926 -4.251327 -4.2448754 -4.2424879 -4.2460656 -4.2615142 -4.2856231 -4.3046837 -4.3097153 -4.3059449 -4.2949195 -4.2730312][-4.2067213 -4.1876535 -4.1924582 -4.2041345 -4.2095466 -4.2049131 -4.2027941 -4.209383 -4.2324977 -4.267508 -4.2948232 -4.3041921 -4.3037586 -4.2919779 -4.266119][-4.202579 -4.1721005 -4.1659651 -4.1704903 -4.174458 -4.1758404 -4.1777859 -4.1860213 -4.2109504 -4.2485757 -4.2792606 -4.2932582 -4.2963147 -4.2859192 -4.2587309][-4.2074046 -4.1724224 -4.1565604 -4.1534081 -4.1548252 -4.1568818 -4.1580782 -4.163 -4.1835756 -4.2207851 -4.253406 -4.2742543 -4.2843828 -4.2787428 -4.2541785][-4.2161436 -4.1868815 -4.168632 -4.1599112 -4.1563063 -4.1477017 -4.1332879 -4.1196036 -4.1310515 -4.1735377 -4.2179818 -4.2521081 -4.2726932 -4.2740974 -4.2574458][-4.2201729 -4.2037678 -4.192071 -4.1827025 -4.1721649 -4.1485562 -4.1100197 -4.0671411 -4.0591846 -4.1099863 -4.1777482 -4.2335963 -4.2660327 -4.2754431 -4.2681026][-4.2243509 -4.2173729 -4.2138257 -4.2065229 -4.1929564 -4.1641345 -4.1120796 -4.0462804 -4.0160203 -4.070632 -4.1558185 -4.2261934 -4.2660289 -4.2806926 -4.2797441][-4.2346673 -4.2320023 -4.2317824 -4.2245083 -4.2113733 -4.1881161 -4.1416283 -4.0776854 -4.0407166 -4.0807505 -4.157692 -4.2254667 -4.2661777 -4.2842593 -4.2853279][-4.2492352 -4.2481375 -4.2489905 -4.243082 -4.2333593 -4.2170353 -4.1819992 -4.1324191 -4.0989065 -4.1186848 -4.1748128 -4.2305694 -4.2664032 -4.2846346 -4.2866564][-4.2646713 -4.2619491 -4.2630143 -4.2610526 -4.2586689 -4.251121 -4.2261405 -4.1898689 -4.161128 -4.1677642 -4.2043586 -4.2447739 -4.2716041 -4.2872806 -4.2885666][-4.2800221 -4.2767377 -4.2765927 -4.2745647 -4.2732711 -4.2689548 -4.2538829 -4.2336903 -4.2144308 -4.2160697 -4.2393475 -4.2644525 -4.2806907 -4.2923303 -4.2907572][-4.2827163 -4.2792406 -4.2782388 -4.2734771 -4.265625 -4.2564864 -4.2478619 -4.2450328 -4.2390518 -4.243947 -4.262466 -4.2783918 -4.2877965 -4.2957125 -4.2922726][-4.2745056 -4.2687349 -4.2652287 -4.2538481 -4.2359385 -4.2199359 -4.2149434 -4.2244172 -4.2314157 -4.2428737 -4.2611861 -4.2755313 -4.2858596 -4.2936926 -4.2905693]]...]
INFO - root - 2017-12-06 07:56:10.603588: step 4010, loss = 2.05, batch loss = 1.99 (6.9 examples/sec; 1.158 sec/batch; 105h:39m:06s remains)
INFO - root - 2017-12-06 07:56:22.577900: step 4020, loss = 2.05, batch loss = 1.99 (6.5 examples/sec; 1.231 sec/batch; 112h:20m:38s remains)
INFO - root - 2017-12-06 07:56:34.678234: step 4030, loss = 2.06, batch loss = 2.00 (5.9 examples/sec; 1.354 sec/batch; 123h:32m:51s remains)
INFO - root - 2017-12-06 07:56:46.666876: step 4040, loss = 2.04, batch loss = 1.98 (6.6 examples/sec; 1.211 sec/batch; 110h:28m:47s remains)
INFO - root - 2017-12-06 07:56:58.346972: step 4050, loss = 2.05, batch loss = 1.99 (6.8 examples/sec; 1.172 sec/batch; 106h:53m:17s remains)
INFO - root - 2017-12-06 07:57:10.177199: step 4060, loss = 2.04, batch loss = 1.98 (6.3 examples/sec; 1.262 sec/batch; 115h:08m:39s remains)
INFO - root - 2017-12-06 07:57:21.941707: step 4070, loss = 2.08, batch loss = 2.02 (6.6 examples/sec; 1.204 sec/batch; 109h:49m:34s remains)
INFO - root - 2017-12-06 07:57:33.761606: step 4080, loss = 2.06, batch loss = 2.00 (6.8 examples/sec; 1.183 sec/batch; 107h:54m:38s remains)
INFO - root - 2017-12-06 07:57:45.503962: step 4090, loss = 2.04, batch loss = 1.99 (6.9 examples/sec; 1.167 sec/batch; 106h:27m:35s remains)
INFO - root - 2017-12-06 07:57:57.538480: step 4100, loss = 2.08, batch loss = 2.02 (6.7 examples/sec; 1.197 sec/batch; 109h:08m:59s remains)
2017-12-06 07:57:58.261361: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3073764 -4.3079772 -4.3089228 -4.3073344 -4.3052464 -4.304934 -4.3075485 -4.3114409 -4.3143077 -4.315134 -4.3157449 -4.3163114 -4.3160448 -4.31597 -4.3157883][-4.27322 -4.2747869 -4.2762656 -4.2728367 -4.2677922 -4.2675977 -4.2733412 -4.2822127 -4.2895684 -4.292551 -4.2941275 -4.2953 -4.295073 -4.2948275 -4.2944493][-4.2343678 -4.2353716 -4.2364125 -4.2287807 -4.2188983 -4.2182941 -4.2296772 -4.2472954 -4.2639918 -4.2736659 -4.2778964 -4.2799549 -4.2793188 -4.2773962 -4.273828][-4.211061 -4.2077813 -4.2017703 -4.1831 -4.1620564 -4.1552691 -4.1676874 -4.1941042 -4.2280507 -4.2563324 -4.273026 -4.2815261 -4.2830925 -4.27915 -4.2694941][-4.202826 -4.1923327 -4.17657 -4.1467814 -4.1107116 -4.0854311 -4.0818162 -4.1052747 -4.1579313 -4.2147279 -4.2535024 -4.2750168 -4.2842512 -4.2815342 -4.2700305][-4.1944118 -4.1780758 -4.1547914 -4.1171656 -4.0652051 -4.0090117 -3.9636166 -3.9605219 -4.0281072 -4.1154847 -4.1790161 -4.2187347 -4.2429786 -4.2511315 -4.2493839][-4.1928749 -4.1734591 -4.1463742 -4.1046753 -4.0412135 -3.9572499 -3.8634467 -3.8136396 -3.8819594 -3.9912229 -4.0697718 -4.1215529 -4.1604371 -4.189394 -4.2085381][-4.209796 -4.1930361 -4.1689458 -4.1341968 -4.0774212 -3.9947276 -3.8935714 -3.8285966 -3.8685622 -3.9539795 -4.0157833 -4.0574851 -4.0980816 -4.1398506 -4.1709][-4.2390132 -4.2288194 -4.2117562 -4.1889806 -4.153183 -4.0966892 -4.0265489 -3.983434 -3.9992959 -4.037148 -4.0636926 -4.0815287 -4.1042366 -4.1332264 -4.15513][-4.2721066 -4.2707057 -4.2624369 -4.2489176 -4.2305026 -4.2008538 -4.1625447 -4.1405187 -4.1453962 -4.1551814 -4.1541066 -4.1467276 -4.1477246 -4.1559591 -4.1639795][-4.2960291 -4.3038893 -4.3074017 -4.30325 -4.2944174 -4.2800913 -4.2617712 -4.2486558 -4.2430673 -4.2388959 -4.2252765 -4.2049937 -4.1919031 -4.1877961 -4.1878953][-4.3013754 -4.3156815 -4.3294883 -4.3358593 -4.3365622 -4.3306909 -4.3204165 -4.30991 -4.301116 -4.2917809 -4.2723093 -4.2459154 -4.2281857 -4.2181091 -4.2119207][-4.2907052 -4.3070612 -4.3273082 -4.3413157 -4.3474975 -4.3444915 -4.3360329 -4.3263416 -4.3181148 -4.3068762 -4.2864904 -4.2620373 -4.2457504 -4.2329173 -4.2231035][-4.2766566 -4.2927394 -4.3137817 -4.3285017 -4.3337231 -4.3300514 -4.31982 -4.3085952 -4.299305 -4.2873011 -4.2681756 -4.24962 -4.2367234 -4.2258883 -4.2154412][-4.2598672 -4.2755589 -4.2955203 -4.3075929 -4.3084106 -4.2989111 -4.2821822 -4.2689152 -4.2581153 -4.2444706 -4.2286825 -4.2166715 -4.2078395 -4.1979432 -4.1886573]]...]
INFO - root - 2017-12-06 07:58:10.224082: step 4110, loss = 2.05, batch loss = 2.00 (6.4 examples/sec; 1.254 sec/batch; 114h:25m:00s remains)
INFO - root - 2017-12-06 07:58:22.278552: step 4120, loss = 2.08, batch loss = 2.02 (6.9 examples/sec; 1.152 sec/batch; 105h:03m:03s remains)
INFO - root - 2017-12-06 07:58:34.465744: step 4130, loss = 2.05, batch loss = 1.99 (6.5 examples/sec; 1.240 sec/batch; 113h:05m:31s remains)
INFO - root - 2017-12-06 07:58:46.374847: step 4140, loss = 2.03, batch loss = 1.98 (6.8 examples/sec; 1.179 sec/batch; 107h:29m:38s remains)
INFO - root - 2017-12-06 07:58:58.384712: step 4150, loss = 2.05, batch loss = 1.99 (6.6 examples/sec; 1.215 sec/batch; 110h:49m:31s remains)
INFO - root - 2017-12-06 07:59:10.338086: step 4160, loss = 2.07, batch loss = 2.01 (6.7 examples/sec; 1.202 sec/batch; 109h:35m:14s remains)
INFO - root - 2017-12-06 07:59:22.237923: step 4170, loss = 2.09, batch loss = 2.03 (6.7 examples/sec; 1.199 sec/batch; 109h:21m:13s remains)
INFO - root - 2017-12-06 07:59:34.334509: step 4180, loss = 2.07, batch loss = 2.02 (6.6 examples/sec; 1.219 sec/batch; 111h:11m:19s remains)
INFO - root - 2017-12-06 07:59:46.301227: step 4190, loss = 2.08, batch loss = 2.02 (6.5 examples/sec; 1.224 sec/batch; 111h:39m:40s remains)
INFO - root - 2017-12-06 07:59:58.161713: step 4200, loss = 2.07, batch loss = 2.01 (6.6 examples/sec; 1.204 sec/batch; 109h:46m:12s remains)
2017-12-06 07:59:58.957119: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1780491 -4.1925454 -4.204072 -4.2114344 -4.2097516 -4.186151 -4.1506095 -4.11722 -4.1206646 -4.1311421 -4.1177058 -4.1146259 -4.1440454 -4.1817575 -4.210669][-4.176178 -4.1959267 -4.2110758 -4.2176738 -4.2143397 -4.1901069 -4.1575351 -4.1260872 -4.1290779 -4.1336713 -4.1044583 -4.081974 -4.1038723 -4.1515932 -4.1973863][-4.1861811 -4.2129307 -4.2319775 -4.233911 -4.2232118 -4.1930785 -4.1627741 -4.1346855 -4.1309738 -4.1274266 -4.0919724 -4.0587058 -4.0686636 -4.1101995 -4.1588593][-4.2052031 -4.2319751 -4.2506504 -4.2477722 -4.2277322 -4.1875005 -4.151176 -4.1254487 -4.1246967 -4.120441 -4.0878005 -4.0541239 -4.0590906 -4.0915895 -4.1334352][-4.2340164 -4.2535496 -4.2669578 -4.2569628 -4.221138 -4.1604013 -4.1037273 -4.0738606 -4.0858212 -4.0990758 -4.08407 -4.0616755 -4.06997 -4.0972753 -4.1327329][-4.2637959 -4.2758584 -4.2814264 -4.260591 -4.205337 -4.1172457 -4.0322862 -3.9908853 -4.014708 -4.054862 -4.070899 -4.0745797 -4.0910606 -4.1158872 -4.145699][-4.2837462 -4.290453 -4.2881465 -4.258585 -4.19149 -4.088316 -3.9822938 -3.9218054 -3.9386945 -3.9947369 -4.0458393 -4.0845513 -4.1142578 -4.1427641 -4.1694][-4.2964525 -4.300786 -4.2924542 -4.2581906 -4.1923246 -4.0946007 -3.988632 -3.9115334 -3.9028482 -3.9535613 -4.0223289 -4.0862832 -4.1309481 -4.1681476 -4.1971841][-4.3024287 -4.3077559 -4.3003507 -4.2672429 -4.2118635 -4.1363411 -4.048913 -3.9680271 -3.9308221 -3.9562767 -4.0227613 -4.0983429 -4.1556826 -4.2012277 -4.2312021][-4.2995915 -4.3058038 -4.3037367 -4.2790279 -4.237988 -4.184525 -4.1143141 -4.0393448 -3.9914155 -3.9926875 -4.0429721 -4.1170835 -4.1791234 -4.2258854 -4.2538495][-4.2939587 -4.2992373 -4.3025289 -4.2900391 -4.2669225 -4.2330174 -4.177269 -4.1128469 -4.0641332 -4.048223 -4.0790186 -4.1418557 -4.2023273 -4.2485514 -4.2738485][-4.2935991 -4.296792 -4.3058281 -4.3068013 -4.2985668 -4.2791905 -4.2398853 -4.1902475 -4.1481295 -4.1247315 -4.1365924 -4.1816626 -4.232049 -4.2730803 -4.2951436][-4.3046293 -4.3069687 -4.3168249 -4.3240004 -4.3247352 -4.3153629 -4.2920313 -4.2571588 -4.2227855 -4.199194 -4.1984825 -4.2272229 -4.266634 -4.3009934 -4.3194313][-4.314559 -4.3165159 -4.3246088 -4.3317223 -4.334775 -4.3327675 -4.321424 -4.2997932 -4.2739816 -4.2524929 -4.2461758 -4.2622766 -4.2893019 -4.3148694 -4.3301969][-4.3235841 -4.3245273 -4.3299031 -4.3363438 -4.3396835 -4.3408155 -4.3364582 -4.3235693 -4.3055582 -4.2887807 -4.2821226 -4.290688 -4.3056703 -4.3218646 -4.3329778]]...]
INFO - root - 2017-12-06 08:00:11.032057: step 4210, loss = 2.07, batch loss = 2.01 (7.1 examples/sec; 1.123 sec/batch; 102h:25m:18s remains)
INFO - root - 2017-12-06 08:00:22.953737: step 4220, loss = 2.09, batch loss = 2.03 (6.8 examples/sec; 1.169 sec/batch; 106h:36m:29s remains)
INFO - root - 2017-12-06 08:00:34.797969: step 4230, loss = 2.05, batch loss = 1.99 (6.6 examples/sec; 1.205 sec/batch; 109h:50m:05s remains)
INFO - root - 2017-12-06 08:00:46.799430: step 4240, loss = 2.07, batch loss = 2.01 (6.8 examples/sec; 1.179 sec/batch; 107h:32m:32s remains)
INFO - root - 2017-12-06 08:00:58.813134: step 4250, loss = 2.04, batch loss = 1.98 (6.4 examples/sec; 1.246 sec/batch; 113h:36m:12s remains)
INFO - root - 2017-12-06 08:01:10.759170: step 4260, loss = 2.09, batch loss = 2.03 (7.1 examples/sec; 1.128 sec/batch; 102h:48m:46s remains)
INFO - root - 2017-12-06 08:01:22.656689: step 4270, loss = 2.05, batch loss = 1.99 (6.7 examples/sec; 1.196 sec/batch; 109h:01m:34s remains)
INFO - root - 2017-12-06 08:01:34.693281: step 4280, loss = 2.04, batch loss = 1.98 (6.6 examples/sec; 1.209 sec/batch; 110h:14m:29s remains)
INFO - root - 2017-12-06 08:01:46.645558: step 4290, loss = 2.08, batch loss = 2.02 (6.7 examples/sec; 1.199 sec/batch; 109h:20m:50s remains)
INFO - root - 2017-12-06 08:01:58.395361: step 4300, loss = 2.05, batch loss = 1.99 (6.3 examples/sec; 1.264 sec/batch; 115h:14m:46s remains)
2017-12-06 08:01:59.178072: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3614316 -4.3561096 -4.3510637 -4.3457923 -4.3422275 -4.3411546 -4.3413835 -4.34255 -4.3454571 -4.3478608 -4.3473358 -4.3448753 -4.3435512 -4.3446727 -4.3459268][-4.3634739 -4.3544135 -4.3434887 -4.3321142 -4.3236156 -4.3201036 -4.3208842 -4.324563 -4.3301687 -4.3365908 -4.33974 -4.3394623 -4.3393416 -4.3418779 -4.3441944][-4.3573771 -4.3426762 -4.3246927 -4.3064647 -4.2924628 -4.2833424 -4.2776976 -4.2784333 -4.2861133 -4.298996 -4.3115358 -4.3203931 -4.3272691 -4.3344774 -4.3405256][-4.3407097 -4.3179116 -4.2930822 -4.2702403 -4.2493129 -4.2310066 -4.2164793 -4.2121358 -4.2191529 -4.23609 -4.258739 -4.2788048 -4.2964478 -4.3126554 -4.3271236][-4.3182936 -4.2835312 -4.2504549 -4.22247 -4.192915 -4.1610622 -4.1330152 -4.1213722 -4.1247497 -4.1438589 -4.1762381 -4.21139 -4.24427 -4.2767906 -4.3052506][-4.3026128 -4.2575593 -4.2123632 -4.1702018 -4.125062 -4.074842 -4.0271168 -4.0003915 -3.9968121 -4.0209637 -4.0656691 -4.1173205 -4.1691952 -4.2244921 -4.2747707][-4.2919693 -4.2391844 -4.1830697 -4.1283679 -4.0704966 -4.008781 -3.9453905 -3.8961482 -3.8741176 -3.8962049 -3.950104 -4.0161867 -4.0892715 -4.1712303 -4.2481184][-4.2937346 -4.2396841 -4.1827674 -4.1280284 -4.0703316 -4.0061727 -3.9320726 -3.8605824 -3.8179796 -3.8318138 -3.8902855 -3.9693871 -4.0596905 -4.1576147 -4.2463088][-4.3090358 -4.2624655 -4.2152224 -4.1698689 -4.1221819 -4.065392 -3.9944263 -3.9207582 -3.8761332 -3.8874497 -3.9440265 -4.0237455 -4.1124253 -4.2029586 -4.2798409][-4.331615 -4.2984881 -4.2655954 -4.2327127 -4.1973844 -4.1554961 -4.1021376 -4.045414 -4.0153131 -4.0312324 -4.0762429 -4.1382565 -4.2072525 -4.2732258 -4.324224][-4.352037 -4.3325357 -4.3129787 -4.2915921 -4.2683434 -4.2436943 -4.2124367 -4.1766181 -4.1597171 -4.1741972 -4.2069564 -4.2471662 -4.2926865 -4.3331904 -4.3600392][-4.3661251 -4.3557553 -4.3432131 -4.327642 -4.3142023 -4.3038683 -4.2884383 -4.268497 -4.2609029 -4.270503 -4.2912688 -4.3168821 -4.3433943 -4.3634019 -4.3734574][-4.3726792 -4.3683038 -4.3605571 -4.3488908 -4.3392272 -4.3335066 -4.3268147 -4.3191991 -4.3184166 -4.3252106 -4.3370957 -4.3505545 -4.3613029 -4.3673844 -4.3682084][-4.3719282 -4.3709183 -4.3683052 -4.3625593 -4.3560867 -4.3495283 -4.3431134 -4.3386927 -4.3399372 -4.3447304 -4.35063 -4.3562713 -4.3596206 -4.3607111 -4.3594165][-4.3692665 -4.3687778 -4.3686519 -4.3672752 -4.3648367 -4.3608809 -4.3553877 -4.3505993 -4.3496723 -4.352066 -4.3546686 -4.357007 -4.3582153 -4.3578606 -4.35624]]...]
INFO - root - 2017-12-06 08:02:11.099160: step 4310, loss = 2.10, batch loss = 2.04 (6.7 examples/sec; 1.195 sec/batch; 108h:54m:46s remains)
INFO - root - 2017-12-06 08:02:23.008215: step 4320, loss = 2.07, batch loss = 2.01 (6.9 examples/sec; 1.156 sec/batch; 105h:23m:19s remains)
INFO - root - 2017-12-06 08:02:34.852430: step 4330, loss = 2.06, batch loss = 2.00 (6.8 examples/sec; 1.179 sec/batch; 107h:28m:10s remains)
INFO - root - 2017-12-06 08:02:46.719078: step 4340, loss = 2.06, batch loss = 2.01 (6.6 examples/sec; 1.203 sec/batch; 109h:41m:42s remains)
INFO - root - 2017-12-06 08:02:58.821697: step 4350, loss = 2.07, batch loss = 2.01 (6.6 examples/sec; 1.205 sec/batch; 109h:52m:46s remains)
INFO - root - 2017-12-06 08:03:10.655311: step 4360, loss = 2.08, batch loss = 2.02 (6.8 examples/sec; 1.170 sec/batch; 106h:40m:50s remains)
INFO - root - 2017-12-06 08:03:22.586766: step 4370, loss = 2.07, batch loss = 2.01 (6.6 examples/sec; 1.215 sec/batch; 110h:42m:41s remains)
INFO - root - 2017-12-06 08:03:34.780155: step 4380, loss = 2.07, batch loss = 2.01 (6.9 examples/sec; 1.162 sec/batch; 105h:54m:50s remains)
INFO - root - 2017-12-06 08:03:46.900731: step 4390, loss = 2.04, batch loss = 1.98 (6.2 examples/sec; 1.288 sec/batch; 117h:23m:18s remains)
INFO - root - 2017-12-06 08:03:58.397165: step 4400, loss = 2.08, batch loss = 2.02 (7.5 examples/sec; 1.070 sec/batch; 97h:32m:28s remains)
2017-12-06 08:03:59.183853: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1450572 -4.1559138 -4.1649914 -4.1574616 -4.1463075 -4.1523943 -4.1656508 -4.1632934 -4.1368709 -4.1179838 -4.1247687 -4.1475582 -4.1790137 -4.1853371 -4.1652288][-4.1464348 -4.1545405 -4.1605625 -4.1473746 -4.1391492 -4.1525631 -4.1688609 -4.1690273 -4.1457782 -4.1310844 -4.1372027 -4.1521292 -4.1621847 -4.1532755 -4.1358185][-4.1179252 -4.1197515 -4.1198974 -4.1041141 -4.0919623 -4.1005912 -4.1202278 -4.1347971 -4.1329265 -4.126905 -4.1288209 -4.1291022 -4.123138 -4.1099596 -4.1033306][-4.0363326 -4.0364575 -4.0449948 -4.039 -4.0235791 -4.0254745 -4.0470648 -4.0723181 -4.0872545 -4.0892324 -4.0887823 -4.0800772 -4.064599 -4.051477 -4.0472221][-3.9500928 -3.9580166 -3.9788864 -3.9831226 -3.9691589 -3.9688396 -3.983705 -4.0028348 -4.0193071 -4.0278225 -4.0307684 -4.0212846 -4.0058632 -3.9940357 -3.9846961][-3.9223785 -3.9364235 -3.9546175 -3.9509032 -3.928802 -3.9193246 -3.9104602 -3.9065895 -3.9321055 -3.9585607 -3.9668164 -3.958662 -3.947705 -3.9450984 -3.942116][-3.9278972 -3.9373415 -3.9421439 -3.9224658 -3.8867474 -3.8478632 -3.791759 -3.7595668 -3.8115907 -3.8685098 -3.8865688 -3.8939679 -3.9030495 -3.9213772 -3.9398708][-3.970526 -3.9625351 -3.948627 -3.9162333 -3.8700094 -3.8058729 -3.7169862 -3.6729348 -3.7511671 -3.8304417 -3.8610196 -3.8925791 -3.9280403 -3.9674187 -4.0083966][-4.0509663 -4.0311489 -4.0074368 -3.9758286 -3.9371264 -3.884603 -3.8155096 -3.7886868 -3.8558145 -3.9157591 -3.9389765 -3.9747164 -4.0176725 -4.058722 -4.1006413][-4.1219306 -4.1010628 -4.0790148 -4.0551028 -4.0332503 -4.0058489 -3.970016 -3.9600329 -4.0025258 -4.0347505 -4.0455174 -4.0723553 -4.1047726 -4.136 -4.1703639][-4.1719637 -4.158289 -4.1457629 -4.1321387 -4.1210775 -4.1089678 -4.0943627 -4.0916843 -4.1125197 -4.1279521 -4.1313434 -4.1465392 -4.1646962 -4.1799407 -4.1996703][-4.21356 -4.2025232 -4.1943169 -4.1882925 -4.1856427 -4.183392 -4.1787162 -4.1748166 -4.1802168 -4.1863074 -4.1872506 -4.1921844 -4.1918306 -4.1868796 -4.1892147][-4.253716 -4.2427678 -4.2348633 -4.2313528 -4.2324839 -4.2355475 -4.2347569 -4.2289219 -4.2253613 -4.224122 -4.2169738 -4.2078133 -4.1895094 -4.1681275 -4.1569963][-4.2867851 -4.2792578 -4.2726412 -4.2695932 -4.2705092 -4.27175 -4.2704124 -4.2653766 -4.2611938 -4.2577376 -4.2464623 -4.229496 -4.19939 -4.1644349 -4.1385417][-4.3112364 -4.3079677 -4.30289 -4.2980251 -4.2958918 -4.2944164 -4.2925515 -4.2906437 -4.2890968 -4.2872667 -4.2781863 -4.26124 -4.2268529 -4.1825266 -4.1443291]]...]
INFO - root - 2017-12-06 08:04:11.305535: step 4410, loss = 2.09, batch loss = 2.03 (6.7 examples/sec; 1.189 sec/batch; 108h:20m:18s remains)
INFO - root - 2017-12-06 08:04:23.145197: step 4420, loss = 2.07, batch loss = 2.01 (6.8 examples/sec; 1.180 sec/batch; 107h:31m:21s remains)
INFO - root - 2017-12-06 08:04:35.081177: step 4430, loss = 2.07, batch loss = 2.01 (6.5 examples/sec; 1.229 sec/batch; 112h:00m:20s remains)
INFO - root - 2017-12-06 08:04:46.808833: step 4440, loss = 2.07, batch loss = 2.01 (6.7 examples/sec; 1.200 sec/batch; 109h:22m:03s remains)
INFO - root - 2017-12-06 08:04:58.692309: step 4450, loss = 2.07, batch loss = 2.01 (6.7 examples/sec; 1.191 sec/batch; 108h:29m:05s remains)
INFO - root - 2017-12-06 08:05:10.666576: step 4460, loss = 2.07, batch loss = 2.01 (6.7 examples/sec; 1.189 sec/batch; 108h:22m:35s remains)
INFO - root - 2017-12-06 08:05:22.559872: step 4470, loss = 2.07, batch loss = 2.01 (6.7 examples/sec; 1.195 sec/batch; 108h:53m:19s remains)
INFO - root - 2017-12-06 08:05:34.629239: step 4480, loss = 2.06, batch loss = 2.01 (6.5 examples/sec; 1.223 sec/batch; 111h:23m:47s remains)
INFO - root - 2017-12-06 08:05:46.574100: step 4490, loss = 2.07, batch loss = 2.02 (6.6 examples/sec; 1.211 sec/batch; 110h:21m:01s remains)
INFO - root - 2017-12-06 08:05:58.531459: step 4500, loss = 2.04, batch loss = 1.98 (7.3 examples/sec; 1.099 sec/batch; 100h:06m:23s remains)
2017-12-06 08:05:59.336676: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2367139 -4.2373891 -4.2470732 -4.2494893 -4.2401886 -4.2130647 -4.184165 -4.1823277 -4.2089195 -4.2333169 -4.2414565 -4.2391572 -4.2384071 -4.2367539 -4.2427387][-4.2140694 -4.2134337 -4.2297764 -4.24084 -4.2339373 -4.2053757 -4.1741772 -4.1708512 -4.1996665 -4.2257576 -4.2321224 -4.2230744 -4.2098384 -4.19456 -4.1954727][-4.1906834 -4.1802325 -4.1916304 -4.1989193 -4.1898041 -4.161798 -4.1390529 -4.1414628 -4.17538 -4.2035651 -4.2056251 -4.1884766 -4.1609387 -4.1310186 -4.1311445][-4.1657772 -4.1386619 -4.1342206 -4.1304917 -4.1198359 -4.0969706 -4.0816283 -4.0912771 -4.1355457 -4.1706223 -4.1726522 -4.1496186 -4.11389 -4.0809436 -4.0861449][-4.1455216 -4.0992942 -4.0720243 -4.0536847 -4.0451193 -4.0290556 -4.0116224 -4.0230913 -4.0833945 -4.1348696 -4.1454573 -4.1264153 -4.0979714 -4.07945 -4.0894036][-4.1203351 -4.0623469 -4.0194049 -3.9936724 -3.9878101 -3.9694686 -3.9361391 -3.9438925 -4.0264006 -4.1029377 -4.1324644 -4.1308293 -4.1213083 -4.1196914 -4.1250582][-4.1103354 -4.0510616 -3.9956169 -3.9598441 -3.946301 -3.914583 -3.8560119 -3.8515644 -3.9542956 -4.0599637 -4.1165242 -4.1381211 -4.1460977 -4.1530356 -4.15196][-4.1311584 -4.0818014 -4.0234475 -3.9795029 -3.957119 -3.922996 -3.8576274 -3.8444924 -3.9413452 -4.0494232 -4.1140704 -4.1458197 -4.1641545 -4.1752129 -4.1730704][-4.1771216 -4.1435304 -4.0952554 -4.0569448 -4.0366373 -4.0078006 -3.9495056 -3.9367211 -4.0118 -4.0977812 -4.1434393 -4.1627336 -4.1776438 -4.1890054 -4.1932049][-4.222393 -4.201848 -4.1685128 -4.1458187 -4.1357617 -4.1132522 -4.0651441 -4.0497375 -4.0939727 -4.1472087 -4.16821 -4.1731462 -4.1787605 -4.1888757 -4.2008247][-4.2508068 -4.2408609 -4.221137 -4.2091088 -4.2014637 -4.180469 -4.1441545 -4.1310534 -4.1526375 -4.1788774 -4.1871896 -4.1869941 -4.1848578 -4.1873078 -4.2004251][-4.2626638 -4.2640867 -4.25642 -4.2530656 -4.246645 -4.2277918 -4.2016158 -4.1924124 -4.2015667 -4.2135229 -4.216795 -4.2117915 -4.1994057 -4.192503 -4.19965][-4.2616138 -4.2711926 -4.2723742 -4.2774663 -4.2781796 -4.2679553 -4.2496004 -4.2411575 -4.2437129 -4.2483768 -4.2487855 -4.2399135 -4.2206845 -4.2064261 -4.2073679][-4.2604008 -4.2709332 -4.2747016 -4.2837791 -4.2921324 -4.2917209 -4.2831917 -4.2768531 -4.2759228 -4.2751575 -4.2732086 -4.2617974 -4.240819 -4.2234607 -4.2188029][-4.2607837 -4.2713652 -4.2746 -4.2813334 -4.2917976 -4.2984271 -4.2984953 -4.2957292 -4.2922754 -4.287694 -4.2837434 -4.27239 -4.2524366 -4.2336316 -4.224277]]...]
INFO - root - 2017-12-06 08:06:11.378305: step 4510, loss = 2.04, batch loss = 1.99 (6.6 examples/sec; 1.204 sec/batch; 109h:39m:53s remains)
INFO - root - 2017-12-06 08:06:23.235222: step 4520, loss = 2.07, batch loss = 2.01 (6.6 examples/sec; 1.204 sec/batch; 109h:42m:56s remains)
INFO - root - 2017-12-06 08:06:35.226388: step 4530, loss = 2.06, batch loss = 2.00 (6.6 examples/sec; 1.207 sec/batch; 109h:58m:12s remains)
INFO - root - 2017-12-06 08:06:46.909645: step 4540, loss = 2.04, batch loss = 1.99 (6.7 examples/sec; 1.192 sec/batch; 108h:34m:50s remains)
INFO - root - 2017-12-06 08:06:58.840808: step 4550, loss = 2.06, batch loss = 2.00 (6.7 examples/sec; 1.190 sec/batch; 108h:24m:17s remains)
INFO - root - 2017-12-06 08:07:10.476629: step 4560, loss = 2.08, batch loss = 2.02 (6.8 examples/sec; 1.184 sec/batch; 107h:50m:06s remains)
INFO - root - 2017-12-06 08:07:22.536394: step 4570, loss = 2.05, batch loss = 2.00 (6.7 examples/sec; 1.199 sec/batch; 109h:13m:02s remains)
INFO - root - 2017-12-06 08:07:34.457156: step 4580, loss = 2.08, batch loss = 2.02 (6.0 examples/sec; 1.324 sec/batch; 120h:33m:39s remains)
INFO - root - 2017-12-06 08:07:46.508307: step 4590, loss = 2.07, batch loss = 2.02 (6.6 examples/sec; 1.203 sec/batch; 109h:34m:50s remains)
INFO - root - 2017-12-06 08:07:58.480003: step 4600, loss = 2.08, batch loss = 2.02 (6.6 examples/sec; 1.215 sec/batch; 110h:42m:01s remains)
2017-12-06 08:07:59.255805: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.31514 -4.315064 -4.3126354 -4.3082185 -4.3039479 -4.3029814 -4.3061638 -4.3078575 -4.3057823 -4.3019695 -4.3007832 -4.3048978 -4.3107815 -4.3149476 -4.3142076][-4.2959366 -4.296402 -4.2928762 -4.284688 -4.2758207 -4.2723718 -4.275919 -4.2781138 -4.2752047 -4.2696815 -4.2662096 -4.2707419 -4.2794533 -4.2865281 -4.2866988][-4.27963 -4.2797112 -4.2743545 -4.2619123 -4.2487388 -4.242291 -4.24468 -4.2463245 -4.2430973 -4.234221 -4.2270074 -4.2326369 -4.2429042 -4.251647 -4.2513466][-4.2486854 -4.2466588 -4.238575 -4.2229075 -4.2086244 -4.2009811 -4.198679 -4.1945705 -4.1926088 -4.1864066 -4.1809473 -4.1909661 -4.2052226 -4.2144036 -4.2133622][-4.2074161 -4.2052188 -4.1970673 -4.1789107 -4.1643977 -4.1551886 -4.1400833 -4.1215024 -4.1221337 -4.13187 -4.1397877 -4.1605463 -4.178957 -4.1847959 -4.1780143][-4.1557508 -4.1537528 -4.1444244 -4.1229644 -4.1085548 -4.0942712 -4.0585041 -4.0128112 -4.0132508 -4.0507793 -4.0844426 -4.1181817 -4.1433363 -4.1501107 -4.1425962][-4.1049471 -4.1030388 -4.0900807 -4.0624061 -4.0431604 -4.0198164 -3.9652307 -3.8902681 -3.8922608 -3.9713171 -4.0349226 -4.0751991 -4.099916 -4.1074557 -4.1044407][-4.0914125 -4.0882554 -4.0702772 -4.0385585 -4.0151472 -3.9890332 -3.9323676 -3.8562999 -3.8637059 -3.9607158 -4.0370889 -4.0713897 -4.0850987 -4.0841703 -4.0759668][-4.1282988 -4.1263337 -4.1091576 -4.0803642 -4.0589185 -4.038908 -4.0001516 -3.9543529 -3.968843 -4.0446739 -4.1039305 -4.1218977 -4.1151142 -4.0978427 -4.0805845][-4.1643953 -4.168242 -4.1583619 -4.1364512 -4.1199369 -4.1052265 -4.0794468 -4.0535173 -4.0646381 -4.1137619 -4.1509967 -4.1588464 -4.1429234 -4.1177011 -4.0950842][-4.1630039 -4.1744218 -4.1764984 -4.1660185 -4.1536722 -4.1397362 -4.1181812 -4.1009459 -4.1077557 -4.1394444 -4.1644926 -4.1713185 -4.1618633 -4.1412625 -4.1200614][-4.143343 -4.161118 -4.1742845 -4.175457 -4.1678133 -4.1528506 -4.1286936 -4.1140385 -4.1195226 -4.1435475 -4.1637497 -4.1727014 -4.1739216 -4.1636224 -4.14572][-4.1395 -4.1555128 -4.1725307 -4.1817212 -4.1747069 -4.1546569 -4.1284046 -4.1164436 -4.1196179 -4.1380777 -4.16054 -4.176702 -4.1833806 -4.1791511 -4.1680756][-4.1501622 -4.1598244 -4.1731715 -4.1839161 -4.17375 -4.1455746 -4.1160688 -4.10743 -4.1140471 -4.1345568 -4.1626434 -4.1836758 -4.1904774 -4.187808 -4.1837072][-4.152319 -4.157012 -4.1617846 -4.1686163 -4.157423 -4.1227546 -4.0932021 -4.0899096 -4.1031694 -4.1289058 -4.161025 -4.1830111 -4.1845732 -4.1793437 -4.1802711]]...]
INFO - root - 2017-12-06 08:08:11.392374: step 4610, loss = 2.04, batch loss = 1.98 (6.5 examples/sec; 1.240 sec/batch; 112h:55m:19s remains)
INFO - root - 2017-12-06 08:08:23.328239: step 4620, loss = 2.08, batch loss = 2.02 (6.6 examples/sec; 1.219 sec/batch; 110h:59m:35s remains)
INFO - root - 2017-12-06 08:08:35.276801: step 4630, loss = 2.06, batch loss = 2.00 (6.8 examples/sec; 1.178 sec/batch; 107h:17m:20s remains)
INFO - root - 2017-12-06 08:08:47.362213: step 4640, loss = 2.06, batch loss = 2.00 (6.2 examples/sec; 1.289 sec/batch; 117h:24m:27s remains)
INFO - root - 2017-12-06 08:08:59.247586: step 4650, loss = 2.05, batch loss = 1.99 (6.7 examples/sec; 1.195 sec/batch; 108h:52m:18s remains)
INFO - root - 2017-12-06 08:09:11.131863: step 4660, loss = 2.08, batch loss = 2.02 (6.3 examples/sec; 1.274 sec/batch; 115h:59m:00s remains)
INFO - root - 2017-12-06 08:09:23.074598: step 4670, loss = 2.03, batch loss = 1.98 (6.6 examples/sec; 1.219 sec/batch; 110h:59m:56s remains)
INFO - root - 2017-12-06 08:09:35.153872: step 4680, loss = 2.06, batch loss = 2.00 (6.8 examples/sec; 1.183 sec/batch; 107h:42m:01s remains)
INFO - root - 2017-12-06 08:09:46.996311: step 4690, loss = 2.05, batch loss = 1.99 (7.0 examples/sec; 1.137 sec/batch; 103h:31m:02s remains)
INFO - root - 2017-12-06 08:09:58.856064: step 4700, loss = 2.06, batch loss = 2.00 (6.8 examples/sec; 1.170 sec/batch; 106h:33m:30s remains)
2017-12-06 08:09:59.670564: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2713337 -4.2799692 -4.2875676 -4.2922435 -4.2942886 -4.2937503 -4.2907119 -4.288414 -4.2840996 -4.2783504 -4.2731366 -4.2667265 -4.2545609 -4.2340555 -4.2157679][-4.28626 -4.2970047 -4.3076677 -4.314868 -4.3159375 -4.3109264 -4.3032393 -4.2971153 -4.2894716 -4.2780128 -4.265336 -4.2496815 -4.2311745 -4.2096076 -4.1982312][-4.2875514 -4.3012018 -4.31558 -4.3235168 -4.3213468 -4.3117843 -4.2995648 -4.2887654 -4.2781034 -4.2621279 -4.241478 -4.2167706 -4.1924329 -4.1709924 -4.1672311][-4.2736096 -4.2894588 -4.3054576 -4.3113971 -4.3036366 -4.2864251 -4.2696095 -4.2577281 -4.2475262 -4.2297254 -4.2057943 -4.1770186 -4.1520009 -4.1330953 -4.1353159][-4.2458797 -4.2570457 -4.2684813 -4.266953 -4.2489719 -4.2217259 -4.2037535 -4.1992464 -4.19735 -4.1866641 -4.1703196 -4.1490736 -4.1322179 -4.1217394 -4.1277843][-4.2028108 -4.2007184 -4.2017078 -4.1897054 -4.1595 -4.1213894 -4.1059914 -4.1203375 -4.1424723 -4.1495719 -4.1452918 -4.1365514 -4.1323295 -4.1385155 -4.15345][-4.1789522 -4.1619434 -4.1472325 -4.1214604 -4.074739 -4.0170288 -3.9956102 -4.0333157 -4.0864906 -4.1174994 -4.1266403 -4.1263785 -4.1326523 -4.1550546 -4.1847734][-4.1866493 -4.1646905 -4.1438451 -4.1159029 -4.0655909 -3.9975994 -3.9678786 -4.01029 -4.0766096 -4.1194816 -4.1326656 -4.128777 -4.1309304 -4.1556568 -4.1937137][-4.2035513 -4.1873746 -4.1731753 -4.1573038 -4.1238494 -4.0711803 -4.0447307 -4.0723991 -4.1213021 -4.1558142 -4.1644697 -4.1543627 -4.1446328 -4.1528568 -4.1813383][-4.221396 -4.2139149 -4.2078114 -4.2035394 -4.18682 -4.1534128 -4.1342292 -4.1478362 -4.1748724 -4.1949973 -4.1946411 -4.1783719 -4.1604767 -4.1538882 -4.1655941][-4.2238927 -4.2222533 -4.2245741 -4.2309809 -4.2298374 -4.2129812 -4.2000842 -4.2060351 -4.2203159 -4.2308288 -4.2235904 -4.2028675 -4.17887 -4.1608338 -4.1570148][-4.2265987 -4.2255464 -4.235219 -4.2503633 -4.2557898 -4.2433696 -4.2286959 -4.2282939 -4.2379994 -4.2470212 -4.2410016 -4.22098 -4.196475 -4.1728168 -4.15712][-4.250082 -4.2485514 -4.2617092 -4.2760005 -4.275013 -4.25287 -4.2294073 -4.2231975 -4.2326412 -4.2465777 -4.2468486 -4.2309804 -4.2101731 -4.189321 -4.1698232][-4.2804275 -4.2804923 -4.2898717 -4.2934923 -4.2802143 -4.2468047 -4.2151403 -4.2041149 -4.2134748 -4.2317524 -4.2398448 -4.23015 -4.2158012 -4.2003703 -4.1847143][-4.2968497 -4.296277 -4.2986937 -4.29116 -4.26929 -4.2331805 -4.2024593 -4.1913061 -4.1981282 -4.2169552 -4.2308292 -4.227334 -4.2171669 -4.206099 -4.1962562]]...]
INFO - root - 2017-12-06 08:10:11.545234: step 4710, loss = 2.07, batch loss = 2.01 (6.7 examples/sec; 1.191 sec/batch; 108h:24m:25s remains)
INFO - root - 2017-12-06 08:10:23.293762: step 4720, loss = 2.04, batch loss = 1.98 (6.8 examples/sec; 1.174 sec/batch; 106h:52m:09s remains)
INFO - root - 2017-12-06 08:10:35.085418: step 4730, loss = 2.06, batch loss = 2.00 (6.8 examples/sec; 1.181 sec/batch; 107h:33m:54s remains)
INFO - root - 2017-12-06 08:10:46.812415: step 4740, loss = 2.06, batch loss = 2.00 (6.8 examples/sec; 1.181 sec/batch; 107h:33m:46s remains)
INFO - root - 2017-12-06 08:10:58.790142: step 4750, loss = 2.03, batch loss = 1.97 (6.7 examples/sec; 1.197 sec/batch; 108h:56m:45s remains)
INFO - root - 2017-12-06 08:11:10.882826: step 4760, loss = 2.06, batch loss = 2.00 (6.4 examples/sec; 1.258 sec/batch; 114h:31m:11s remains)
INFO - root - 2017-12-06 08:11:22.758140: step 4770, loss = 2.05, batch loss = 1.99 (6.8 examples/sec; 1.172 sec/batch; 106h:40m:44s remains)
INFO - root - 2017-12-06 08:11:34.773452: step 4780, loss = 2.09, batch loss = 2.03 (6.7 examples/sec; 1.202 sec/batch; 109h:25m:59s remains)
INFO - root - 2017-12-06 08:11:46.578034: step 4790, loss = 2.07, batch loss = 2.01 (6.5 examples/sec; 1.231 sec/batch; 112h:03m:07s remains)
INFO - root - 2017-12-06 08:11:58.397587: step 4800, loss = 2.04, batch loss = 1.98 (7.1 examples/sec; 1.120 sec/batch; 101h:59m:22s remains)
2017-12-06 08:11:59.134185: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1895213 -4.1943264 -4.2050347 -4.2130127 -4.2127967 -4.2087612 -4.205709 -4.2108064 -4.2198753 -4.2254677 -4.223083 -4.2237039 -4.2354784 -4.2551923 -4.2702074][-4.2011414 -4.1989822 -4.2051582 -4.21344 -4.2201571 -4.2248898 -4.2280612 -4.2336173 -4.2402897 -4.2428689 -4.2399473 -4.24417 -4.2585697 -4.2767782 -4.2879686][-4.2188139 -4.2133965 -4.2164755 -4.2242594 -4.2338085 -4.2413292 -4.2451305 -4.2477117 -4.2495408 -4.2466226 -4.2407351 -4.2449436 -4.2597208 -4.2759533 -4.2835155][-4.2353139 -4.2307773 -4.233284 -4.2398362 -4.2482438 -4.2517905 -4.2486038 -4.2440214 -4.2405186 -4.2331018 -4.2233396 -4.2229724 -4.2348433 -4.2484875 -4.2547617][-4.23801 -4.233901 -4.2362056 -4.2431421 -4.2473516 -4.2376075 -4.2185583 -4.2046161 -4.1997485 -4.1937041 -4.1835036 -4.1795821 -4.1884494 -4.2011862 -4.2100983][-4.216712 -4.2090464 -4.2097125 -4.2177758 -4.2146254 -4.1843948 -4.1416345 -4.1232 -4.1292872 -4.1354556 -4.1322412 -4.1302204 -4.1416378 -4.156301 -4.1696391][-4.1781254 -4.1677656 -4.1657114 -4.1732407 -4.1594415 -4.1025009 -4.0314264 -4.0156493 -4.048573 -4.0792809 -4.0886855 -4.0963368 -4.1156435 -4.1337185 -4.1481004][-4.1340494 -4.1252661 -4.1225858 -4.1287441 -4.1052351 -4.0262218 -3.9313347 -3.9244266 -3.9897149 -4.0467515 -4.0705628 -4.0876927 -4.112493 -4.1330328 -4.1467843][-4.0969372 -4.09131 -4.0885277 -4.1006918 -4.0893664 -4.02168 -3.9391866 -3.9458485 -4.0221467 -4.0838819 -4.1092329 -4.1237383 -4.1408176 -4.1563244 -4.1664915][-4.0676408 -4.0615406 -4.0637007 -4.0919514 -4.1085429 -4.0770254 -4.0333023 -4.0478668 -4.1074171 -4.1558962 -4.175477 -4.181807 -4.18623 -4.1929364 -4.1973238][-4.0528016 -4.0420094 -4.0532107 -4.0994058 -4.1375365 -4.1363916 -4.12262 -4.1355891 -4.1742034 -4.2111988 -4.23022 -4.2360411 -4.2359829 -4.2372112 -4.2377591][-4.0566239 -4.0446396 -4.067184 -4.1213918 -4.1665673 -4.1788573 -4.1791534 -4.1892252 -4.2145338 -4.2433848 -4.2627158 -4.2720318 -4.2745605 -4.27553 -4.2740221][-4.0831275 -4.0763855 -4.1052542 -4.15319 -4.1908126 -4.2031231 -4.2078276 -4.2182388 -4.2381935 -4.2602015 -4.2792358 -4.2918172 -4.2967834 -4.2964954 -4.2923632][-4.13336 -4.1292319 -4.1507096 -4.1817241 -4.2064819 -4.2149639 -4.2215676 -4.2335753 -4.2495666 -4.2665949 -4.2851405 -4.299108 -4.3033805 -4.2988782 -4.29109][-4.1708064 -4.1681166 -4.1822281 -4.201293 -4.2185788 -4.2252679 -4.232008 -4.2414341 -4.2487445 -4.2568812 -4.2721944 -4.2854147 -4.2859392 -4.2753196 -4.2630038]]...]
INFO - root - 2017-12-06 08:12:11.004079: step 4810, loss = 2.06, batch loss = 2.00 (7.0 examples/sec; 1.146 sec/batch; 104h:18m:34s remains)
INFO - root - 2017-12-06 08:12:22.810462: step 4820, loss = 2.07, batch loss = 2.01 (7.0 examples/sec; 1.139 sec/batch; 103h:37m:53s remains)
INFO - root - 2017-12-06 08:12:34.744335: step 4830, loss = 2.05, batch loss = 1.99 (6.7 examples/sec; 1.195 sec/batch; 108h:46m:46s remains)
INFO - root - 2017-12-06 08:12:46.556866: step 4840, loss = 2.08, batch loss = 2.02 (6.7 examples/sec; 1.198 sec/batch; 109h:02m:35s remains)
INFO - root - 2017-12-06 08:12:58.358920: step 4850, loss = 2.07, batch loss = 2.01 (6.6 examples/sec; 1.218 sec/batch; 110h:53m:06s remains)
INFO - root - 2017-12-06 08:13:10.381373: step 4860, loss = 2.07, batch loss = 2.02 (6.8 examples/sec; 1.180 sec/batch; 107h:24m:13s remains)
INFO - root - 2017-12-06 08:13:22.203699: step 4870, loss = 2.06, batch loss = 2.00 (6.9 examples/sec; 1.160 sec/batch; 105h:33m:17s remains)
INFO - root - 2017-12-06 08:13:34.424937: step 4880, loss = 2.05, batch loss = 1.99 (6.5 examples/sec; 1.224 sec/batch; 111h:25m:34s remains)
INFO - root - 2017-12-06 08:13:46.401841: step 4890, loss = 2.09, batch loss = 2.03 (6.6 examples/sec; 1.211 sec/batch; 110h:13m:25s remains)
INFO - root - 2017-12-06 08:13:58.374166: step 4900, loss = 2.05, batch loss = 1.99 (6.6 examples/sec; 1.205 sec/batch; 109h:37m:16s remains)
2017-12-06 08:13:59.128776: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3048744 -4.2964678 -4.2864208 -4.2749052 -4.2585025 -4.2392583 -4.2245831 -4.218472 -4.2207069 -4.2290406 -4.238183 -4.2431312 -4.24561 -4.2460866 -4.2466984][-4.2983685 -4.2910361 -4.2839103 -4.2734823 -4.2597528 -4.2458143 -4.2362533 -4.2338548 -4.2394557 -4.2503967 -4.2606897 -4.2639179 -4.2633061 -4.2634425 -4.2641587][-4.2634749 -4.2585936 -4.2565937 -4.2505894 -4.2428064 -4.2360539 -4.232573 -4.2330923 -4.2392378 -4.2500105 -4.2593255 -4.2593665 -4.2546554 -4.2544837 -4.258194][-4.2046857 -4.2087574 -4.2179174 -4.2220912 -4.2242107 -4.2263465 -4.2268572 -4.2267022 -4.2282729 -4.232317 -4.2358875 -4.2323542 -4.2257009 -4.2268085 -4.2352409][-4.1460657 -4.1623626 -4.1806149 -4.1910372 -4.1982474 -4.2027249 -4.2008567 -4.1930513 -4.1859345 -4.1834369 -4.1840978 -4.1790609 -4.173882 -4.179172 -4.1938915][-4.1172085 -4.1348853 -4.1497736 -4.1575561 -4.1609225 -4.1572132 -4.1447787 -4.1264009 -4.1158247 -4.1169863 -4.1222239 -4.1207204 -4.119844 -4.1288972 -4.1484575][-4.13077 -4.1325455 -4.125154 -4.1159611 -4.1051559 -4.0848403 -4.0568914 -4.0339303 -4.0369616 -4.0600414 -4.0825214 -4.0930743 -4.0962052 -4.10341 -4.1183476][-4.1476412 -4.1271105 -4.0941753 -4.0648785 -4.0388117 -4.0059481 -3.9710765 -3.955081 -3.9852281 -4.0385447 -4.0806623 -4.1043134 -4.112638 -4.1173425 -4.1217871][-4.1537623 -4.120923 -4.0751181 -4.0389924 -4.0121031 -3.9842572 -3.9612687 -3.9629681 -4.0089474 -4.0684857 -4.1107774 -4.1339607 -4.1413646 -4.1422434 -4.1385422][-4.1408973 -4.1109443 -4.0793471 -4.0629678 -4.0539184 -4.0412116 -4.0330625 -4.0424743 -4.0791569 -4.1220293 -4.1484962 -4.1609097 -4.1643763 -4.1644292 -4.1592846][-4.1161904 -4.0936975 -4.0881729 -4.1018329 -4.1186476 -4.1237454 -4.1252112 -4.1330762 -4.1519938 -4.1702175 -4.1740746 -4.1701059 -4.1688948 -4.1720672 -4.1712685][-4.08889 -4.0758858 -4.0929437 -4.130558 -4.166997 -4.1858153 -4.1927123 -4.1974592 -4.202261 -4.2010756 -4.1892591 -4.1722317 -4.1626921 -4.1605535 -4.1556787][-4.0851989 -4.0736518 -4.0952148 -4.1355171 -4.175137 -4.19988 -4.2112083 -4.2188053 -4.2200766 -4.2110481 -4.1941328 -4.1740856 -4.1566515 -4.1410651 -4.1245184][-4.1264982 -4.1105566 -4.1219983 -4.1491485 -4.1767259 -4.195384 -4.2061591 -4.2155118 -4.2190828 -4.2134504 -4.2052231 -4.1926627 -4.1703744 -4.1386633 -4.1068888][-4.1690249 -4.1545482 -4.1594954 -4.1748071 -4.1878233 -4.1952467 -4.1986418 -4.2030988 -4.2060738 -4.2075272 -4.2109637 -4.2083364 -4.186389 -4.14829 -4.1112823]]...]
INFO - root - 2017-12-06 08:14:11.141807: step 4910, loss = 2.07, batch loss = 2.01 (7.1 examples/sec; 1.133 sec/batch; 103h:06m:31s remains)
INFO - root - 2017-12-06 08:14:23.199135: step 4920, loss = 2.06, batch loss = 2.00 (6.4 examples/sec; 1.243 sec/batch; 113h:04m:06s remains)
INFO - root - 2017-12-06 08:14:35.164180: step 4930, loss = 2.06, batch loss = 2.00 (6.6 examples/sec; 1.221 sec/batch; 111h:08m:02s remains)
INFO - root - 2017-12-06 08:14:46.876039: step 4940, loss = 2.08, batch loss = 2.03 (6.8 examples/sec; 1.173 sec/batch; 106h:44m:39s remains)
INFO - root - 2017-12-06 08:14:58.999643: step 4950, loss = 2.05, batch loss = 1.99 (6.9 examples/sec; 1.157 sec/batch; 105h:16m:52s remains)
INFO - root - 2017-12-06 08:15:11.128309: step 4960, loss = 2.08, batch loss = 2.02 (6.8 examples/sec; 1.178 sec/batch; 107h:12m:31s remains)
INFO - root - 2017-12-06 08:15:22.910843: step 4970, loss = 2.07, batch loss = 2.01 (6.7 examples/sec; 1.202 sec/batch; 109h:20m:34s remains)
INFO - root - 2017-12-06 08:15:34.822295: step 4980, loss = 2.10, batch loss = 2.04 (6.5 examples/sec; 1.231 sec/batch; 112h:01m:40s remains)
INFO - root - 2017-12-06 08:15:46.888466: step 4990, loss = 2.06, batch loss = 2.00 (6.7 examples/sec; 1.190 sec/batch; 108h:16m:02s remains)
INFO - root - 2017-12-06 08:15:58.699672: step 5000, loss = 2.06, batch loss = 2.00 (6.7 examples/sec; 1.201 sec/batch; 109h:13m:15s remains)
2017-12-06 08:15:59.451678: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3368969 -4.3036742 -4.2568016 -4.2201843 -4.1941128 -4.1785026 -4.1657209 -4.1543851 -4.1558514 -4.1737466 -4.2003493 -4.2117891 -4.1981483 -4.1824121 -4.1708097][-4.345634 -4.3147292 -4.269218 -4.2349215 -4.209331 -4.1894946 -4.1644115 -4.1343622 -4.11764 -4.1357026 -4.174376 -4.195425 -4.1812315 -4.1603427 -4.141911][-4.3488665 -4.3172731 -4.2739534 -4.24099 -4.2166905 -4.1980381 -4.1665897 -4.1237717 -4.0925703 -4.1089549 -4.1513844 -4.1747203 -4.1574559 -4.1318736 -4.11505][-4.3458252 -4.3108592 -4.262785 -4.2244244 -4.2011561 -4.1876669 -4.1622181 -4.1275072 -4.0987077 -4.113369 -4.1543684 -4.1729918 -4.1524134 -4.1223793 -4.1088419][-4.3392854 -4.2973614 -4.2374072 -4.18768 -4.1605916 -4.1540022 -4.1468782 -4.1394167 -4.1284108 -4.1448364 -4.1813622 -4.1934028 -4.1707773 -4.1401229 -4.1305857][-4.3312364 -4.2803802 -4.2065892 -4.1415658 -4.1026359 -4.093884 -4.1044493 -4.1298933 -4.1467772 -4.1758013 -4.2134371 -4.2246103 -4.2017236 -4.1751413 -4.1701188][-4.3253555 -4.2717214 -4.1907964 -4.1108484 -4.0506315 -4.0246205 -4.0409656 -4.0937414 -4.1338487 -4.1769676 -4.22061 -4.2397051 -4.2209578 -4.1952276 -4.19131][-4.3220997 -4.2750216 -4.2045188 -4.1296582 -4.0620704 -4.0189991 -4.0259109 -4.0789852 -4.1226726 -4.1682868 -4.210083 -4.2326732 -4.2227182 -4.2027817 -4.1968241][-4.3215117 -4.2825346 -4.2302938 -4.1765175 -4.124723 -4.0862637 -4.0861182 -4.123929 -4.155611 -4.1793232 -4.2039604 -4.2189531 -4.2104306 -4.1948028 -4.1896625][-4.324224 -4.2929683 -4.2562695 -4.2265382 -4.1944523 -4.1675158 -4.1674485 -4.1953468 -4.2170734 -4.2184553 -4.2222276 -4.2253489 -4.2148113 -4.2022152 -4.2000289][-4.3277287 -4.3015804 -4.2751627 -4.2611995 -4.2436576 -4.2287183 -4.2313647 -4.2496519 -4.26507 -4.259325 -4.2524109 -4.2476692 -4.2380834 -4.2294869 -4.2308178][-4.3284044 -4.3031507 -4.2783556 -4.266881 -4.2548308 -4.2451558 -4.2472248 -4.2589459 -4.2712917 -4.2686982 -4.2627964 -4.2555809 -4.2442741 -4.2400112 -4.2468061][-4.3233032 -4.2940249 -4.2635179 -4.2448149 -4.228538 -4.21913 -4.2221584 -4.230504 -4.2392397 -4.2378497 -4.2340727 -4.2260938 -4.2181592 -4.22066 -4.23356][-4.31233 -4.2765427 -4.2366104 -4.205904 -4.1798592 -4.1681619 -4.17404 -4.1838264 -4.1893225 -4.1877666 -4.1878943 -4.1829042 -4.181829 -4.1927915 -4.20668][-4.3062496 -4.268507 -4.2210946 -4.1804118 -4.14312 -4.1261196 -4.1279845 -4.1338139 -4.1368914 -4.1373205 -4.1422834 -4.1410484 -4.1470103 -4.1642642 -4.1799831]]...]
INFO - root - 2017-12-06 08:16:11.625687: step 5010, loss = 2.06, batch loss = 2.00 (6.8 examples/sec; 1.176 sec/batch; 106h:56m:31s remains)
INFO - root - 2017-12-06 08:16:23.507646: step 5020, loss = 2.07, batch loss = 2.01 (6.4 examples/sec; 1.246 sec/batch; 113h:19m:53s remains)
INFO - root - 2017-12-06 08:16:35.747608: step 5030, loss = 2.08, batch loss = 2.02 (6.4 examples/sec; 1.250 sec/batch; 113h:40m:22s remains)
INFO - root - 2017-12-06 08:16:47.633811: step 5040, loss = 2.08, batch loss = 2.02 (6.6 examples/sec; 1.203 sec/batch; 109h:27m:35s remains)
INFO - root - 2017-12-06 08:16:59.665396: step 5050, loss = 2.04, batch loss = 1.98 (6.2 examples/sec; 1.282 sec/batch; 116h:36m:28s remains)
INFO - root - 2017-12-06 08:17:11.759672: step 5060, loss = 2.04, batch loss = 1.98 (6.3 examples/sec; 1.274 sec/batch; 115h:51m:48s remains)
INFO - root - 2017-12-06 08:17:23.496685: step 5070, loss = 2.09, batch loss = 2.03 (6.8 examples/sec; 1.184 sec/batch; 107h:39m:24s remains)
INFO - root - 2017-12-06 08:17:35.647037: step 5080, loss = 2.07, batch loss = 2.01 (6.5 examples/sec; 1.225 sec/batch; 111h:24m:13s remains)
INFO - root - 2017-12-06 08:17:47.553830: step 5090, loss = 2.08, batch loss = 2.02 (6.9 examples/sec; 1.162 sec/batch; 105h:41m:50s remains)
INFO - root - 2017-12-06 08:17:59.716116: step 5100, loss = 2.08, batch loss = 2.02 (6.6 examples/sec; 1.204 sec/batch; 109h:28m:07s remains)
2017-12-06 08:18:00.441520: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2537451 -4.25779 -4.2653723 -4.2660785 -4.25289 -4.2328286 -4.2164493 -4.2097821 -4.1985445 -4.1831794 -4.1683626 -4.1572843 -4.1519375 -4.151391 -4.1639013][-4.2538457 -4.2576571 -4.2665095 -4.2663426 -4.2582026 -4.249002 -4.2426772 -4.2329535 -4.2097759 -4.17856 -4.1504722 -4.1313429 -4.1254206 -4.1371269 -4.1629157][-4.2474937 -4.2529869 -4.2589235 -4.2565484 -4.2556047 -4.2556572 -4.2569437 -4.2436986 -4.2112794 -4.1691747 -4.1341758 -4.1127791 -4.1114264 -4.134675 -4.171979][-4.2314863 -4.2400317 -4.2498488 -4.2515311 -4.25716 -4.2601256 -4.2582421 -4.2393851 -4.2029495 -4.1607652 -4.1250534 -4.1052556 -4.1100712 -4.1426287 -4.1862693][-4.2241912 -4.2335677 -4.2450004 -4.2510395 -4.258553 -4.2598886 -4.2501078 -4.2235618 -4.1901636 -4.1531396 -4.1238346 -4.1066051 -4.1148763 -4.1482649 -4.1914768][-4.2236977 -4.2324867 -4.243217 -4.2539034 -4.2615161 -4.2542009 -4.232059 -4.1933608 -4.1589537 -4.13477 -4.1197333 -4.1101856 -4.1199913 -4.1483574 -4.1854863][-4.2106261 -4.2272348 -4.2380996 -4.2474208 -4.2424188 -4.2171903 -4.1778688 -4.1260085 -4.0940914 -4.0905814 -4.0962338 -4.1019306 -4.1175165 -4.1429043 -4.1736][-4.2005539 -4.2280388 -4.2352223 -4.230763 -4.2031937 -4.1551232 -4.0931954 -4.0214758 -3.9913728 -4.0149469 -4.0510411 -4.0821447 -4.1110435 -4.1348529 -4.1578259][-4.2024465 -4.2348642 -4.2327142 -4.2102094 -4.1606226 -4.0874906 -3.9963617 -3.9064498 -3.8880465 -3.9448521 -4.0124021 -4.0665903 -4.1055903 -4.1266971 -4.1397033][-4.2139292 -4.2422118 -4.2286448 -4.1920433 -4.1287231 -4.0363293 -3.9327066 -3.8563352 -3.868403 -3.9464445 -4.0201669 -4.0745277 -4.1113052 -4.128427 -4.1332431][-4.2258415 -4.2423 -4.2193809 -4.1760321 -4.111867 -4.0249767 -3.9466281 -3.9178605 -3.9563692 -4.02292 -4.0759878 -4.1136379 -4.1389637 -4.1499152 -4.1491828][-4.2395029 -4.2432714 -4.2103863 -4.1635294 -4.1080871 -4.0433145 -4.00428 -4.0183396 -4.06869 -4.1191788 -4.1501107 -4.1717706 -4.1863322 -4.1919212 -4.1900978][-4.2628374 -4.2513347 -4.2080636 -4.1562891 -4.1027675 -4.0591669 -4.0516224 -4.0906086 -4.1456447 -4.1861992 -4.2074852 -4.2215257 -4.2308712 -4.233933 -4.2348871][-4.2824097 -4.255537 -4.2020326 -4.1402006 -4.08022 -4.0513678 -4.0683279 -4.1217046 -4.1762357 -4.2096434 -4.2280841 -4.24002 -4.2482877 -4.2537413 -4.2595582][-4.2829933 -4.2480245 -4.1814623 -4.1039281 -4.040554 -4.026216 -4.0667982 -4.1320357 -4.1840892 -4.2120719 -4.2278805 -4.2390022 -4.2488689 -4.2574711 -4.2671661]]...]
INFO - root - 2017-12-06 08:18:12.517738: step 5110, loss = 2.07, batch loss = 2.01 (6.5 examples/sec; 1.229 sec/batch; 111h:45m:46s remains)
INFO - root - 2017-12-06 08:18:24.525743: step 5120, loss = 2.06, batch loss = 2.00 (6.5 examples/sec; 1.221 sec/batch; 111h:04m:27s remains)
INFO - root - 2017-12-06 08:18:36.372212: step 5130, loss = 2.08, batch loss = 2.02 (6.5 examples/sec; 1.222 sec/batch; 111h:06m:21s remains)
INFO - root - 2017-12-06 08:18:48.406440: step 5140, loss = 2.09, batch loss = 2.03 (6.5 examples/sec; 1.225 sec/batch; 111h:25m:00s remains)
INFO - root - 2017-12-06 08:19:00.428903: step 5150, loss = 2.05, batch loss = 1.99 (6.7 examples/sec; 1.197 sec/batch; 108h:52m:58s remains)
INFO - root - 2017-12-06 08:19:12.154595: step 5160, loss = 2.09, batch loss = 2.03 (7.0 examples/sec; 1.137 sec/batch; 103h:23m:58s remains)
INFO - root - 2017-12-06 08:19:24.144793: step 5170, loss = 2.07, batch loss = 2.01 (6.9 examples/sec; 1.164 sec/batch; 105h:49m:05s remains)
INFO - root - 2017-12-06 08:19:35.991858: step 5180, loss = 2.07, batch loss = 2.01 (7.2 examples/sec; 1.110 sec/batch; 100h:55m:11s remains)
INFO - root - 2017-12-06 08:19:48.026993: step 5190, loss = 2.06, batch loss = 2.00 (6.5 examples/sec; 1.223 sec/batch; 111h:09m:06s remains)
INFO - root - 2017-12-06 08:19:59.940609: step 5200, loss = 2.07, batch loss = 2.01 (6.7 examples/sec; 1.187 sec/batch; 107h:55m:13s remains)
2017-12-06 08:20:00.655712: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2896056 -4.2813706 -4.2743359 -4.2688937 -4.2675095 -4.269558 -4.2734246 -4.27567 -4.2760243 -4.2747154 -4.2743258 -4.2745528 -4.27352 -4.2707076 -4.2683735][-4.2862611 -4.2820272 -4.27247 -4.2590303 -4.2501097 -4.2501354 -4.2585344 -4.2689743 -4.27583 -4.2772613 -4.275794 -4.2739291 -4.2717438 -4.2690744 -4.2678165][-4.2660551 -4.269702 -4.2563725 -4.2324538 -4.2138858 -4.2109461 -4.22544 -4.248724 -4.269352 -4.2806125 -4.2823329 -4.2793465 -4.2752976 -4.2713604 -4.2699714][-4.2289867 -4.2452259 -4.2269039 -4.1867104 -4.1517806 -4.1433496 -4.1659484 -4.2065158 -4.2478456 -4.2758503 -4.2880831 -4.2878785 -4.2832723 -4.2771358 -4.2738762][-4.1830578 -4.2154303 -4.1935887 -4.1311493 -4.068028 -4.0432115 -4.0734987 -4.1374245 -4.2055225 -4.2574673 -4.2858224 -4.292901 -4.2895331 -4.2821312 -4.2771878][-4.1487937 -4.1992693 -4.1823225 -4.1038122 -4.0027933 -3.9406404 -3.9604785 -4.0426989 -4.1395769 -4.2190504 -4.2705708 -4.2912784 -4.2914615 -4.2831774 -4.27718][-4.1395297 -4.2018771 -4.1970158 -4.1220746 -4.0012631 -3.893023 -3.873601 -3.9539645 -4.0677266 -4.1688781 -4.2422056 -4.2805338 -4.288353 -4.280437 -4.2741084][-4.1722918 -4.222651 -4.2249417 -4.1673279 -4.0613751 -3.9484663 -3.889895 -3.9278753 -4.0253806 -4.1271162 -4.2094455 -4.2618265 -4.2805467 -4.2758431 -4.2689481][-4.2309208 -4.2538943 -4.2508311 -4.2083254 -4.1352119 -4.0567207 -4.006494 -4.0099463 -4.0617085 -4.1335144 -4.2029562 -4.2558541 -4.2816529 -4.2797365 -4.269618][-4.2780571 -4.2791963 -4.2631397 -4.2294755 -4.187644 -4.1470761 -4.1209178 -4.1202645 -4.1412096 -4.1762834 -4.2207031 -4.2640152 -4.2919393 -4.2930851 -4.2798362][-4.2961984 -4.2830148 -4.256218 -4.2268181 -4.2055678 -4.1898036 -4.1797576 -4.1798792 -4.1865468 -4.2017918 -4.2313204 -4.2678423 -4.297091 -4.3034468 -4.2918744][-4.2904687 -4.264194 -4.2308774 -4.2057595 -4.1977019 -4.1949034 -4.1891022 -4.1821771 -4.1735344 -4.1787448 -4.2090707 -4.2523904 -4.2897258 -4.3050833 -4.2996869][-4.2699528 -4.2333093 -4.197597 -4.1788793 -4.1800075 -4.1817174 -4.1699634 -4.1433821 -4.1086287 -4.1006489 -4.1399865 -4.2021513 -4.2573738 -4.2900257 -4.2973962][-4.2412758 -4.2039313 -4.1753969 -4.1690612 -4.1810207 -4.1851587 -4.1616087 -4.1061859 -4.0319915 -3.9965584 -4.0377269 -4.1196532 -4.1974525 -4.2538905 -4.2824035][-4.2145643 -4.1843576 -4.1715126 -4.1807933 -4.2031441 -4.2102509 -4.180728 -4.1091847 -4.00929 -3.9328434 -3.9486244 -4.0344396 -4.1269555 -4.2035756 -4.2542744]]...]
INFO - root - 2017-12-06 08:20:12.487544: step 5210, loss = 2.06, batch loss = 2.00 (7.0 examples/sec; 1.135 sec/batch; 103h:13m:33s remains)
INFO - root - 2017-12-06 08:20:24.519555: step 5220, loss = 2.07, batch loss = 2.01 (6.5 examples/sec; 1.231 sec/batch; 111h:56m:16s remains)
INFO - root - 2017-12-06 08:20:36.506491: step 5230, loss = 2.07, batch loss = 2.01 (6.4 examples/sec; 1.243 sec/batch; 112h:57m:48s remains)
INFO - root - 2017-12-06 08:20:48.497957: step 5240, loss = 2.08, batch loss = 2.02 (6.9 examples/sec; 1.156 sec/batch; 105h:02m:39s remains)
INFO - root - 2017-12-06 08:21:00.563292: step 5250, loss = 2.06, batch loss = 2.00 (6.5 examples/sec; 1.231 sec/batch; 111h:55m:25s remains)
INFO - root - 2017-12-06 08:21:12.506596: step 5260, loss = 2.10, batch loss = 2.04 (6.7 examples/sec; 1.187 sec/batch; 107h:52m:46s remains)
INFO - root - 2017-12-06 08:21:24.675037: step 5270, loss = 2.07, batch loss = 2.01 (6.9 examples/sec; 1.165 sec/batch; 105h:54m:33s remains)
INFO - root - 2017-12-06 08:21:36.642292: step 5280, loss = 2.06, batch loss = 2.01 (6.6 examples/sec; 1.207 sec/batch; 109h:44m:16s remains)
INFO - root - 2017-12-06 08:21:48.623034: step 5290, loss = 2.07, batch loss = 2.02 (6.5 examples/sec; 1.226 sec/batch; 111h:24m:15s remains)
INFO - root - 2017-12-06 08:22:00.641331: step 5300, loss = 2.06, batch loss = 2.00 (6.8 examples/sec; 1.173 sec/batch; 106h:35m:35s remains)
2017-12-06 08:22:01.427136: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3177629 -4.3080487 -4.3007851 -4.2885046 -4.2774391 -4.2718916 -4.2797828 -4.2850008 -4.2906146 -4.2996955 -4.3027263 -4.2944961 -4.2873459 -4.2863431 -4.286448][-4.3069115 -4.2909994 -4.2796845 -4.2665367 -4.2549505 -4.2527947 -4.267179 -4.2754974 -4.28468 -4.2952251 -4.297564 -4.2866049 -4.276794 -4.2727723 -4.2704][-4.2850084 -4.2624025 -4.2478051 -4.2353592 -4.22455 -4.2264466 -4.2446508 -4.2551975 -4.2689295 -4.2832289 -4.2872162 -4.28019 -4.2737637 -4.2685061 -4.2623096][-4.2494445 -4.2229362 -4.2053204 -4.1953778 -4.184782 -4.1918435 -4.2135115 -4.2280951 -4.2444396 -4.2609262 -4.2662029 -4.2642159 -4.2634611 -4.2628617 -4.2572427][-4.2016664 -4.1750269 -4.1528959 -4.1387115 -4.1308956 -4.1379366 -4.1525903 -4.1686378 -4.1926618 -4.217772 -4.2320113 -4.2352142 -4.2411242 -4.2460413 -4.2445488][-4.136456 -4.1078057 -4.074903 -4.0489645 -4.0390973 -4.0447259 -4.0505786 -4.0728741 -4.1142144 -4.1549206 -4.1842918 -4.1973314 -4.2099705 -4.2180886 -4.22197][-4.074882 -4.0405903 -3.9904604 -3.9421451 -3.9217103 -3.9209459 -3.9162886 -3.9471009 -4.0066919 -4.0622096 -4.1093636 -4.1422963 -4.1671267 -4.1825471 -4.1940064][-4.0371089 -4.0043907 -3.9473581 -3.8881471 -3.8571603 -3.837184 -3.8118694 -3.8362234 -3.9004805 -3.9676116 -4.0324316 -4.083425 -4.1230316 -4.1510177 -4.1724138][-4.0549955 -4.0327234 -3.9891448 -3.9453416 -3.9224727 -3.8965216 -3.8597765 -3.8612664 -3.902329 -3.9558804 -4.0167394 -4.0686164 -4.1114159 -4.1440759 -4.1722407][-4.123425 -4.1125917 -4.0874925 -4.0627022 -4.0467954 -4.0257912 -3.9948835 -3.9825768 -4.0014768 -4.0377312 -4.0818214 -4.1198473 -4.1523318 -4.1781731 -4.20327][-4.1968489 -4.1943116 -4.1835394 -4.1707983 -4.1610327 -4.1473413 -4.1280122 -4.1160007 -4.1217322 -4.143888 -4.1744232 -4.2011671 -4.2233362 -4.2382841 -4.2515316][-4.2593966 -4.2604241 -4.2566538 -4.2514353 -4.2449393 -4.2374206 -4.2272272 -4.2185874 -4.2187166 -4.2305393 -4.2509031 -4.2694411 -4.2851977 -4.2916164 -4.293726][-4.2910714 -4.2887912 -4.2839932 -4.2818012 -4.2785177 -4.2769508 -4.27395 -4.2697425 -4.2694445 -4.2757745 -4.2898121 -4.3039532 -4.31364 -4.3165064 -4.31555][-4.2946796 -4.2897782 -4.2849407 -4.2843165 -4.2840285 -4.2844539 -4.2833195 -4.2823944 -4.2829437 -4.2878356 -4.2986407 -4.3109417 -4.3190947 -4.3218493 -4.3220849][-4.2865272 -4.2818437 -4.27898 -4.2797647 -4.2805576 -4.2807713 -4.2799926 -4.2798386 -4.2800484 -4.2833214 -4.2915592 -4.3015127 -4.31008 -4.315906 -4.3197608]]...]
INFO - root - 2017-12-06 08:22:13.543245: step 5310, loss = 2.12, batch loss = 2.06 (7.1 examples/sec; 1.127 sec/batch; 102h:27m:37s remains)
INFO - root - 2017-12-06 08:22:25.545219: step 5320, loss = 2.05, batch loss = 1.99 (6.5 examples/sec; 1.225 sec/batch; 111h:22m:29s remains)
INFO - root - 2017-12-06 08:22:37.425831: step 5330, loss = 2.07, batch loss = 2.01 (6.5 examples/sec; 1.236 sec/batch; 112h:18m:08s remains)
INFO - root - 2017-12-06 08:22:49.323052: step 5340, loss = 2.04, batch loss = 1.98 (6.7 examples/sec; 1.195 sec/batch; 108h:37m:15s remains)
INFO - root - 2017-12-06 08:23:01.208975: step 5350, loss = 2.07, batch loss = 2.01 (6.8 examples/sec; 1.183 sec/batch; 107h:27m:45s remains)
INFO - root - 2017-12-06 08:23:13.139103: step 5360, loss = 2.07, batch loss = 2.02 (6.9 examples/sec; 1.153 sec/batch; 104h:47m:10s remains)
INFO - root - 2017-12-06 08:23:25.294032: step 5370, loss = 2.06, batch loss = 2.01 (6.5 examples/sec; 1.224 sec/batch; 111h:11m:10s remains)
INFO - root - 2017-12-06 08:23:37.256413: step 5380, loss = 2.07, batch loss = 2.02 (6.6 examples/sec; 1.206 sec/batch; 109h:36m:53s remains)
INFO - root - 2017-12-06 08:23:49.220491: step 5390, loss = 2.07, batch loss = 2.01 (6.8 examples/sec; 1.177 sec/batch; 106h:58m:31s remains)
INFO - root - 2017-12-06 08:24:00.989982: step 5400, loss = 2.05, batch loss = 1.99 (6.7 examples/sec; 1.196 sec/batch; 108h:40m:19s remains)
2017-12-06 08:24:01.772126: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.310266 -4.3094125 -4.3070993 -4.3013234 -4.2990093 -4.3001294 -4.3008552 -4.3012457 -4.2997341 -4.2937016 -4.2853303 -4.2800031 -4.2790594 -4.2827578 -4.2897949][-4.3149061 -4.320044 -4.3225646 -4.3174934 -4.3148775 -4.3175473 -4.3201046 -4.3214707 -4.3205509 -4.3114004 -4.2974367 -4.2839756 -4.2752657 -4.2739873 -4.2765222][-4.3003125 -4.3081651 -4.3125548 -4.3062315 -4.3019314 -4.3043547 -4.3094921 -4.314445 -4.3179965 -4.3096857 -4.2925634 -4.2723269 -4.2594748 -4.256918 -4.2571392][-4.274703 -4.2818775 -4.2856264 -4.2749133 -4.2650204 -4.263443 -4.2689028 -4.2819176 -4.2985892 -4.29894 -4.2822847 -4.2596145 -4.2458248 -4.2426386 -4.2427578][-4.2522087 -4.25426 -4.2519655 -4.2346206 -4.21558 -4.2011528 -4.2012477 -4.223875 -4.2597523 -4.2770586 -4.2690568 -4.2511215 -4.241559 -4.2408051 -4.2417784][-4.2349858 -4.22576 -4.2118945 -4.1851315 -4.1529169 -4.1162963 -4.0981445 -4.1318722 -4.191401 -4.2354789 -4.247714 -4.2440615 -4.2438588 -4.2482643 -4.2502027][-4.2371569 -4.2200656 -4.1944776 -4.1527438 -4.0977159 -4.0292072 -3.9813747 -4.0154815 -4.0970006 -4.1688795 -4.208879 -4.2289538 -4.2442374 -4.2578807 -4.2648292][-4.2408266 -4.2226863 -4.1883531 -4.1258492 -4.0417614 -3.9418135 -3.8622189 -3.8812945 -3.978337 -4.0749164 -4.1437845 -4.1944413 -4.227921 -4.2494893 -4.2618947][-4.2414408 -4.2278275 -4.1965446 -4.1343584 -4.0485573 -3.9503174 -3.8670566 -3.8642287 -3.9415557 -4.0283504 -4.099216 -4.15938 -4.198719 -4.2218795 -4.2351403][-4.2532258 -4.2530155 -4.239387 -4.2004967 -4.147747 -4.0840526 -4.01956 -3.9950666 -4.023212 -4.0640664 -4.1034193 -4.1446924 -4.1724539 -4.1887608 -4.2007384][-4.2528472 -4.2593589 -4.253984 -4.2325845 -4.2099538 -4.1803422 -4.1395493 -4.111083 -4.1027331 -4.1050854 -4.1146607 -4.1323972 -4.1451583 -4.1573133 -4.1729722][-4.2289457 -4.2352314 -4.2361546 -4.2257991 -4.2187967 -4.2109861 -4.192009 -4.1733475 -4.1555395 -4.1418524 -4.1340666 -4.1366696 -4.1381159 -4.1496973 -4.1694918][-4.2128549 -4.2173514 -4.2216039 -4.2153268 -4.2166643 -4.2232995 -4.2235146 -4.22001 -4.2074466 -4.1900477 -4.1745563 -4.1710372 -4.1700683 -4.1803317 -4.1992764][-4.2187471 -4.2185531 -4.2198615 -4.2160783 -4.2247787 -4.241456 -4.2522316 -4.2590656 -4.2563391 -4.2440429 -4.229125 -4.2211933 -4.2165122 -4.222404 -4.2349463][-4.2415161 -4.2383976 -4.2392116 -4.2378993 -4.2482367 -4.2649407 -4.273665 -4.2801971 -4.2832813 -4.2806287 -4.2711854 -4.2611322 -4.2547174 -4.2560253 -4.2624345]]...]
INFO - root - 2017-12-06 08:24:13.766759: step 5410, loss = 2.07, batch loss = 2.01 (6.6 examples/sec; 1.210 sec/batch; 109h:53m:40s remains)
INFO - root - 2017-12-06 08:24:25.667545: step 5420, loss = 2.05, batch loss = 2.00 (6.4 examples/sec; 1.252 sec/batch; 113h:43m:29s remains)
INFO - root - 2017-12-06 08:24:37.821183: step 5430, loss = 2.06, batch loss = 2.00 (6.4 examples/sec; 1.254 sec/batch; 113h:55m:25s remains)
INFO - root - 2017-12-06 08:24:49.878228: step 5440, loss = 2.06, batch loss = 2.00 (6.8 examples/sec; 1.180 sec/batch; 107h:14m:47s remains)
INFO - root - 2017-12-06 08:25:01.828865: step 5450, loss = 2.07, batch loss = 2.01 (6.2 examples/sec; 1.294 sec/batch; 117h:33m:19s remains)
INFO - root - 2017-12-06 08:25:13.847866: step 5460, loss = 2.08, batch loss = 2.02 (6.3 examples/sec; 1.267 sec/batch; 115h:08m:02s remains)
INFO - root - 2017-12-06 08:25:26.348621: step 5470, loss = 2.08, batch loss = 2.02 (6.5 examples/sec; 1.235 sec/batch; 112h:09m:54s remains)
INFO - root - 2017-12-06 08:25:38.423767: step 5480, loss = 2.09, batch loss = 2.04 (6.6 examples/sec; 1.209 sec/batch; 109h:47m:51s remains)
INFO - root - 2017-12-06 08:25:50.300947: step 5490, loss = 2.08, batch loss = 2.02 (6.8 examples/sec; 1.173 sec/batch; 106h:34m:02s remains)
INFO - root - 2017-12-06 08:26:02.644155: step 5500, loss = 2.07, batch loss = 2.01 (6.8 examples/sec; 1.182 sec/batch; 107h:23m:50s remains)
2017-12-06 08:26:03.411740: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1493239 -4.0872784 -4.0874195 -4.1364069 -4.1920466 -4.225101 -4.2160587 -4.1865983 -4.1783953 -4.19764 -4.2048259 -4.1692619 -4.1077943 -4.0599356 -4.0571384][-4.1399345 -4.072928 -4.0786195 -4.1344433 -4.1889305 -4.2120671 -4.1972709 -4.1675272 -4.156239 -4.1743655 -4.1932092 -4.1791162 -4.1365719 -4.0920305 -4.0791912][-4.13696 -4.0631413 -4.0663533 -4.1207166 -4.1716628 -4.1895227 -4.1778278 -4.1559682 -4.1541204 -4.176929 -4.2009993 -4.1960182 -4.159102 -4.1087117 -4.08562][-4.1262279 -4.0490456 -4.0487986 -4.10116 -4.1522532 -4.1697631 -4.1597652 -4.1440892 -4.1541791 -4.1822853 -4.2084312 -4.2085938 -4.1752138 -4.1252351 -4.0987906][-4.1115451 -4.0365138 -4.0309787 -4.0765872 -4.1271868 -4.1429152 -4.1262569 -4.1139474 -4.1291585 -4.1648569 -4.1989484 -4.20945 -4.1866126 -4.1436567 -4.1172795][-4.1100407 -4.0404053 -4.0202522 -4.0457931 -4.0895815 -4.1002522 -4.0714769 -4.0502653 -4.0631824 -4.1099758 -4.1669602 -4.1977162 -4.1911435 -4.1593971 -4.13567][-4.1227856 -4.0624318 -4.0228086 -4.0152178 -4.0437651 -4.0532851 -4.0143604 -3.9794455 -3.9843659 -4.0421052 -4.1203074 -4.1699576 -4.17971 -4.1626453 -4.1460266][-4.1446381 -4.0929003 -4.0414863 -4.004096 -4.0128355 -4.0197277 -3.9805863 -3.9407895 -3.9418435 -3.9989715 -4.0763636 -4.128891 -4.1450033 -4.142036 -4.1378465][-4.1772332 -4.1334777 -4.0838265 -4.0363727 -4.0242214 -4.0171809 -3.9833632 -3.9480381 -3.9454517 -3.9907184 -4.055841 -4.1014953 -4.1144471 -4.1156344 -4.1189637][-4.2104106 -4.176538 -4.135756 -4.0938621 -4.0720959 -4.0566568 -4.0345073 -4.0030851 -3.9862471 -4.0096292 -4.0594025 -4.0991092 -4.1068811 -4.0999708 -4.09874][-4.2449889 -4.2202559 -4.1880655 -4.1560407 -4.136117 -4.1200733 -4.108067 -4.0849528 -4.06154 -4.06625 -4.101058 -4.1338382 -4.1336331 -4.1133 -4.1014833][-4.2763748 -4.2605653 -4.2402835 -4.2216973 -4.2092137 -4.194705 -4.1855969 -4.171875 -4.1515241 -4.1491809 -4.1696324 -4.1889153 -4.17918 -4.1477456 -4.1269188][-4.2887373 -4.2826018 -4.278729 -4.272676 -4.2659407 -4.253973 -4.2452717 -4.2382855 -4.2258644 -4.2186608 -4.2262015 -4.2315516 -4.2146912 -4.1815705 -4.1585789][-4.2777982 -4.2781215 -4.2830925 -4.2868538 -4.2863955 -4.2801847 -4.2721891 -4.26755 -4.2616806 -4.2552929 -4.2547264 -4.253201 -4.23765 -4.2124782 -4.192112][-4.2519407 -4.2519851 -4.2578812 -4.2650294 -4.2703576 -4.2701035 -4.2659955 -4.2646937 -4.2654395 -4.2637014 -4.2622771 -4.2611771 -4.2517276 -4.23691 -4.2223692]]...]
INFO - root - 2017-12-06 08:26:15.484636: step 5510, loss = 2.09, batch loss = 2.03 (6.5 examples/sec; 1.232 sec/batch; 111h:53m:10s remains)
INFO - root - 2017-12-06 08:26:27.281298: step 5520, loss = 2.08, batch loss = 2.02 (6.7 examples/sec; 1.191 sec/batch; 108h:09m:50s remains)
INFO - root - 2017-12-06 08:26:39.225462: step 5530, loss = 2.06, batch loss = 2.00 (6.7 examples/sec; 1.202 sec/batch; 109h:12m:30s remains)
INFO - root - 2017-12-06 08:26:51.184004: step 5540, loss = 2.07, batch loss = 2.02 (6.4 examples/sec; 1.244 sec/batch; 113h:00m:16s remains)
INFO - root - 2017-12-06 08:27:03.010060: step 5550, loss = 2.08, batch loss = 2.03 (6.4 examples/sec; 1.246 sec/batch; 113h:10m:57s remains)
INFO - root - 2017-12-06 08:27:14.828331: step 5560, loss = 2.07, batch loss = 2.01 (6.7 examples/sec; 1.196 sec/batch; 108h:36m:29s remains)
INFO - root - 2017-12-06 08:27:26.665630: step 5570, loss = 2.05, batch loss = 1.99 (7.0 examples/sec; 1.140 sec/batch; 103h:31m:00s remains)
INFO - root - 2017-12-06 08:27:38.555637: step 5580, loss = 2.08, batch loss = 2.02 (6.7 examples/sec; 1.191 sec/batch; 108h:09m:38s remains)
INFO - root - 2017-12-06 08:27:50.684681: step 5590, loss = 2.07, batch loss = 2.01 (6.7 examples/sec; 1.200 sec/batch; 108h:57m:57s remains)
INFO - root - 2017-12-06 08:28:02.727464: step 5600, loss = 2.08, batch loss = 2.02 (6.5 examples/sec; 1.228 sec/batch; 111h:32m:54s remains)
2017-12-06 08:28:03.494205: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2255688 -4.2033777 -4.1931143 -4.1878562 -4.1885591 -4.20008 -4.2079525 -4.2051916 -4.1874166 -4.1615162 -4.1398811 -4.1266403 -4.1360254 -4.1402721 -4.1409497][-4.19169 -4.1648612 -4.1515517 -4.1443453 -4.1481481 -4.1647477 -4.1759238 -4.1820226 -4.1744871 -4.1584134 -4.1437159 -4.1325316 -4.1405535 -4.1432533 -4.1446743][-4.1742163 -4.1486397 -4.13533 -4.1305122 -4.1387081 -4.1533513 -4.1610146 -4.1680036 -4.1661429 -4.1619759 -4.153636 -4.1397653 -4.1377296 -4.1366911 -4.13692][-4.1654654 -4.1381893 -4.1204391 -4.1111917 -4.1191964 -4.1326103 -4.1380239 -4.14507 -4.14816 -4.1498127 -4.1398578 -4.1223097 -4.108695 -4.1064467 -4.110198][-4.1499734 -4.1144581 -4.0886269 -4.074914 -4.0833526 -4.1001568 -4.1063576 -4.1094003 -4.110137 -4.1091185 -4.0943313 -4.0761423 -4.0600276 -4.0588722 -4.062912][-4.137538 -4.09189 -4.0572472 -4.039453 -4.0451736 -4.0595188 -4.0549645 -4.037065 -4.0221677 -4.01909 -4.0119696 -4.0076094 -4.0023432 -4.0068564 -4.0114818][-4.1273613 -4.0836182 -4.051352 -4.0340872 -4.0285835 -4.0204725 -3.9873493 -3.9412608 -3.9142063 -3.9223514 -3.9394698 -3.9608393 -3.9711411 -3.9782212 -3.9844792][-4.13992 -4.1103764 -4.0912838 -4.0732 -4.0475216 -4.0122471 -3.9551821 -3.893925 -3.8645768 -3.8860743 -3.9289441 -3.9700859 -3.9908516 -3.9974895 -4.0036025][-4.1750975 -4.1551657 -4.141438 -4.1205053 -4.0864058 -4.0432272 -3.9893801 -3.9419239 -3.924181 -3.949043 -3.9906461 -4.0277143 -4.0475049 -4.0550771 -4.0649614][-4.2083411 -4.1946044 -4.1864352 -4.1753864 -4.1556683 -4.1284761 -4.0965519 -4.0703206 -4.0590944 -4.0690041 -4.0914 -4.1149564 -4.1316042 -4.14266 -4.1569123][-4.2310257 -4.2226682 -4.2247891 -4.2278171 -4.2255445 -4.216248 -4.1994481 -4.1825166 -4.1726947 -4.1736374 -4.1829548 -4.1979203 -4.2113495 -4.2230077 -4.2346606][-4.244338 -4.2392325 -4.2462716 -4.2577252 -4.265851 -4.2696791 -4.2641416 -4.2548332 -4.2468252 -4.2446756 -4.2469282 -4.25279 -4.2586794 -4.2656641 -4.2724066][-4.2643733 -4.25929 -4.2646689 -4.2746277 -4.285388 -4.2952237 -4.2975378 -4.2936516 -4.2892575 -4.2882414 -4.2886338 -4.2879872 -4.2857413 -4.2852874 -4.2873588][-4.2924953 -4.2852616 -4.2843118 -4.2880964 -4.2939806 -4.3012428 -4.30583 -4.307385 -4.3083496 -4.3101068 -4.3113127 -4.3088408 -4.3036242 -4.2997918 -4.2997708][-4.3200121 -4.3145037 -4.3107347 -4.3096147 -4.3104577 -4.3137193 -4.3173261 -4.3208714 -4.3241882 -4.3267193 -4.3270946 -4.3244996 -4.3206477 -4.3179684 -4.317822]]...]
INFO - root - 2017-12-06 08:28:15.324067: step 5610, loss = 2.08, batch loss = 2.03 (6.9 examples/sec; 1.167 sec/batch; 105h:59m:50s remains)
INFO - root - 2017-12-06 08:28:27.318779: step 5620, loss = 2.06, batch loss = 2.01 (6.7 examples/sec; 1.201 sec/batch; 109h:05m:26s remains)
INFO - root - 2017-12-06 08:28:39.169908: step 5630, loss = 2.10, batch loss = 2.04 (6.5 examples/sec; 1.237 sec/batch; 112h:20m:34s remains)
INFO - root - 2017-12-06 08:28:51.136797: step 5640, loss = 2.07, batch loss = 2.01 (6.4 examples/sec; 1.241 sec/batch; 112h:41m:36s remains)
INFO - root - 2017-12-06 08:29:02.263646: step 5650, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 84h:37m:35s remains)
INFO - root - 2017-12-06 08:29:11.369680: step 5660, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 85h:44m:15s remains)
INFO - root - 2017-12-06 08:29:20.770149: step 5670, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.988 sec/batch; 89h:43m:14s remains)
INFO - root - 2017-12-06 08:29:30.124056: step 5680, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 82h:46m:23s remains)
INFO - root - 2017-12-06 08:29:39.266448: step 5690, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 82h:13m:48s remains)
INFO - root - 2017-12-06 08:29:48.222229: step 5700, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.929 sec/batch; 84h:22m:29s remains)
2017-12-06 08:29:48.906582: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2475119 -4.2138443 -4.1787157 -4.150115 -4.1335764 -4.1317668 -4.1197276 -4.1091437 -4.1012506 -4.0966005 -4.0941057 -4.0967178 -4.0896063 -4.0706391 -4.0608935][-4.2482696 -4.2083049 -4.1614723 -4.1188965 -4.0900693 -4.0822725 -4.0716281 -4.0597329 -4.0446448 -4.0343885 -4.036005 -4.0437646 -4.0362735 -4.0164747 -4.0098324][-4.25217 -4.2104654 -4.1596866 -4.1116467 -4.0798869 -4.0700808 -4.0625706 -4.0512762 -4.0354118 -4.0239592 -4.0257139 -4.0291991 -4.0178227 -3.9987669 -3.9931085][-4.2533669 -4.216867 -4.1727772 -4.13257 -4.1077743 -4.1004667 -4.0983224 -4.0907593 -4.0798225 -4.0728374 -4.0726523 -4.0661058 -4.0483928 -4.0270753 -4.017663][-4.2547097 -4.2285347 -4.1979203 -4.173697 -4.1607342 -4.1586928 -4.1623034 -4.160543 -4.1556535 -4.1513925 -4.1472435 -4.1340985 -4.1130733 -4.0911927 -4.080081][-4.2541857 -4.2372971 -4.2169929 -4.2024965 -4.19608 -4.1966066 -4.2041388 -4.2093964 -4.2122464 -4.21074 -4.2044611 -4.1932554 -4.17697 -4.1597538 -4.1524649][-4.2518129 -4.2391815 -4.2215872 -4.2085218 -4.2010555 -4.2004862 -4.2096572 -4.2195063 -4.22638 -4.227056 -4.2239003 -4.2221546 -4.2163992 -4.2092509 -4.2084179][-4.24742 -4.2322416 -4.211062 -4.1943111 -4.18401 -4.1832476 -4.1929717 -4.2020221 -4.2084341 -4.2093072 -4.2112012 -4.2206197 -4.2269845 -4.2309723 -4.236958][-4.2413154 -4.2197709 -4.1937575 -4.1735821 -4.1607633 -4.1602888 -4.1683631 -4.1724315 -4.175312 -4.176065 -4.1830311 -4.2007504 -4.2179518 -4.2315598 -4.242938][-4.2348213 -4.2093172 -4.1824627 -4.1613903 -4.14761 -4.1473184 -4.1527386 -4.1506057 -4.1500373 -4.152782 -4.1629629 -4.1847363 -4.2073526 -4.2265816 -4.2409215][-4.2309327 -4.2052727 -4.1812844 -4.1607513 -4.1457958 -4.145081 -4.1507978 -4.1473155 -4.1467957 -4.1517048 -4.1629453 -4.1845884 -4.2079329 -4.2282677 -4.2427144][-4.2322035 -4.2101932 -4.1901717 -4.171349 -4.1565161 -4.1561875 -4.1649728 -4.1649837 -4.1662683 -4.170825 -4.1798668 -4.198782 -4.2195015 -4.2365627 -4.2481194][-4.2403703 -4.2222352 -4.2048726 -4.1882906 -4.1757989 -4.1767483 -4.1893334 -4.1942182 -4.1970181 -4.2007227 -4.206995 -4.2209129 -4.2361078 -4.24777 -4.2546749][-4.2527266 -4.2383356 -4.22268 -4.2084684 -4.1986756 -4.2015057 -4.2170482 -4.2256989 -4.2303557 -4.2334208 -4.2363977 -4.2448006 -4.2540922 -4.2601023 -4.2624512][-4.2647209 -4.2533107 -4.2388344 -4.2281747 -4.2212796 -4.2256994 -4.2411623 -4.2509642 -4.2562423 -4.2577791 -4.2588143 -4.2635636 -4.2689486 -4.2704864 -4.2697277]]...]
INFO - root - 2017-12-06 08:29:58.215287: step 5710, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 84h:44m:45s remains)
INFO - root - 2017-12-06 08:30:07.394305: step 5720, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 82h:22m:11s remains)
INFO - root - 2017-12-06 08:30:16.691363: step 5730, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.914 sec/batch; 83h:00m:19s remains)
INFO - root - 2017-12-06 08:30:25.915805: step 5740, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.909 sec/batch; 82h:31m:22s remains)
INFO - root - 2017-12-06 08:30:35.058801: step 5750, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.902 sec/batch; 81h:53m:10s remains)
INFO - root - 2017-12-06 08:30:44.289583: step 5760, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.913 sec/batch; 82h:51m:20s remains)
INFO - root - 2017-12-06 08:30:53.557233: step 5770, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 83h:39m:25s remains)
INFO - root - 2017-12-06 08:31:02.754126: step 5780, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.931 sec/batch; 84h:31m:52s remains)
INFO - root - 2017-12-06 08:31:11.909881: step 5790, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.942 sec/batch; 85h:26m:45s remains)
INFO - root - 2017-12-06 08:31:21.122256: step 5800, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 86h:40m:46s remains)
2017-12-06 08:31:21.756783: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2744989 -4.2668624 -4.2433233 -4.2315779 -4.237833 -4.24639 -4.255949 -4.2583814 -4.2495861 -4.2326484 -4.2207522 -4.21465 -4.2154121 -4.2148695 -4.2062626][-4.2740936 -4.2587123 -4.2401538 -4.2351766 -4.2416472 -4.244185 -4.2485123 -4.2509341 -4.2461295 -4.22288 -4.2024746 -4.1896281 -4.1875157 -4.1962905 -4.2069473][-4.2523875 -4.2339497 -4.2235761 -4.227386 -4.2341094 -4.2312317 -4.2269831 -4.2282915 -4.2317071 -4.2086072 -4.1815534 -4.1592712 -4.1521621 -4.170682 -4.198133][-4.2237234 -4.2028 -4.19915 -4.2104874 -4.2189155 -4.2086449 -4.1889753 -4.1846213 -4.2015209 -4.1952729 -4.1718593 -4.1486263 -4.1404982 -4.1607618 -4.1902909][-4.2094216 -4.184947 -4.1831818 -4.1965218 -4.1994615 -4.1719937 -4.1240444 -4.1081333 -4.1450019 -4.1665621 -4.1631808 -4.1506114 -4.1475744 -4.1627398 -4.1825418][-4.2105851 -4.1825371 -4.1758313 -4.1775937 -4.1618195 -4.1013246 -4.0089436 -3.9740391 -4.04258 -4.1036711 -4.1300182 -4.1366072 -4.1414828 -4.15401 -4.1685743][-4.2150173 -4.1833591 -4.1677942 -4.151979 -4.1154065 -4.0259967 -3.8891876 -3.8334727 -3.9393346 -4.0422225 -4.0899572 -4.1098218 -4.1243238 -4.1391912 -4.1508665][-4.2163668 -4.1857557 -4.1705108 -4.1529007 -4.1176329 -4.0372305 -3.9144602 -3.8612058 -3.9516697 -4.0447617 -4.0863867 -4.1042266 -4.1212626 -4.1390138 -4.1532879][-4.2150722 -4.1875992 -4.1811218 -4.1798773 -4.1679735 -4.1254711 -4.0501809 -4.01497 -4.0640478 -4.1144056 -4.1270146 -4.1297617 -4.1434684 -4.1639023 -4.1786518][-4.2138605 -4.19094 -4.1938357 -4.2073703 -4.2120209 -4.1982393 -4.1609559 -4.1429777 -4.1641827 -4.1797786 -4.1733637 -4.1719408 -4.1868358 -4.2074614 -4.2178955][-4.2199755 -4.2051649 -4.2115531 -4.2269573 -4.2348323 -4.234931 -4.218646 -4.2116485 -4.2228255 -4.2227697 -4.2127323 -4.2165413 -4.2360983 -4.257174 -4.2648449][-4.2431931 -4.2346349 -4.2381687 -4.244926 -4.2485051 -4.2512441 -4.2455721 -4.2445531 -4.2534976 -4.24999 -4.2419291 -4.2499557 -4.2700229 -4.2874761 -4.2929215][-4.272963 -4.2653432 -4.2656751 -4.2663441 -4.2663217 -4.26973 -4.2710352 -4.2737355 -4.2794209 -4.2739015 -4.2676682 -4.27781 -4.2933035 -4.3049293 -4.3096323][-4.29729 -4.2918835 -4.2928567 -4.2927313 -4.2935195 -4.2986522 -4.30304 -4.306777 -4.3093033 -4.3041019 -4.298738 -4.3050451 -4.3131065 -4.3174043 -4.31835][-4.3137064 -4.3112612 -4.3140769 -4.31704 -4.3199396 -4.325994 -4.3314161 -4.3335257 -4.332829 -4.3278389 -4.3231955 -4.3253722 -4.3265467 -4.3254433 -4.3243856]]...]
INFO - root - 2017-12-06 08:31:31.052672: step 5810, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.943 sec/batch; 85h:35m:53s remains)
INFO - root - 2017-12-06 08:31:40.344850: step 5820, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.916 sec/batch; 83h:06m:13s remains)
INFO - root - 2017-12-06 08:31:49.595209: step 5830, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 88h:08m:17s remains)
INFO - root - 2017-12-06 08:31:58.615708: step 5840, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 78h:43m:24s remains)
INFO - root - 2017-12-06 08:32:07.833038: step 5850, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 83h:04m:03s remains)
INFO - root - 2017-12-06 08:32:17.164824: step 5860, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 85h:18m:40s remains)
INFO - root - 2017-12-06 08:32:26.456655: step 5870, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 80h:16m:47s remains)
INFO - root - 2017-12-06 08:32:35.802623: step 5880, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 82h:14m:09s remains)
INFO - root - 2017-12-06 08:32:44.971790: step 5890, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 82h:40m:30s remains)
INFO - root - 2017-12-06 08:32:54.154555: step 5900, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.909 sec/batch; 82h:30m:28s remains)
2017-12-06 08:32:54.871136: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2448363 -4.255022 -4.2761254 -4.29064 -4.294302 -4.2882123 -4.2750893 -4.2653074 -4.2673311 -4.2759562 -4.282588 -4.2851863 -4.2772689 -4.2638717 -4.2465305][-4.2152948 -4.2281504 -4.2549834 -4.2724953 -4.2804909 -4.2749591 -4.25698 -4.2437353 -4.2465925 -4.2559171 -4.2649755 -4.2685375 -4.2576261 -4.2366214 -4.2071505][-4.1895709 -4.2049079 -4.2372661 -4.2552724 -4.2617569 -4.2511396 -4.2243218 -4.2083764 -4.2112522 -4.2214661 -4.2341118 -4.2441492 -4.2365732 -4.2078028 -4.1653762][-4.1757045 -4.189702 -4.21887 -4.2331128 -4.2350817 -4.21676 -4.179749 -4.1636872 -4.1688633 -4.1836343 -4.1990051 -4.2162948 -4.2206411 -4.1913319 -4.1396317][-4.1630616 -4.1772566 -4.2004995 -4.2041955 -4.1916833 -4.1615815 -4.1144304 -4.0974741 -4.1136575 -4.13979 -4.1612005 -4.183197 -4.1995149 -4.1763687 -4.11907][-4.1519465 -4.1690097 -4.18505 -4.1763554 -4.142643 -4.091022 -4.0264072 -4.0059237 -4.0426717 -4.0908985 -4.1278615 -4.1591 -4.1849818 -4.1684165 -4.10843][-4.1559896 -4.1753592 -4.1804867 -4.1539121 -4.0921268 -4.0101147 -3.9226813 -3.902842 -3.9600339 -4.0361905 -4.10124 -4.1497684 -4.1830792 -4.1741104 -4.1164455][-4.1830668 -4.1983581 -4.191071 -4.1478071 -4.0619931 -3.9522209 -3.8526905 -3.8359928 -3.9034741 -4.0001984 -4.0868144 -4.1512709 -4.19126 -4.1888413 -4.1405821][-4.2104487 -4.2266741 -4.216712 -4.1744566 -4.0946012 -3.9882746 -3.898663 -3.8840685 -3.9406593 -4.0236878 -4.10261 -4.1693816 -4.2074265 -4.200686 -4.1579618][-4.2116294 -4.229003 -4.2243748 -4.2001896 -4.1495419 -4.0750237 -4.007081 -3.9906991 -4.0291114 -4.0789814 -4.1326108 -4.1894689 -4.2184191 -4.2002378 -4.1550913][-4.2068491 -4.2233572 -4.2261777 -4.221065 -4.2018995 -4.1626973 -4.1142206 -4.0915022 -4.1072021 -4.1339917 -4.168201 -4.2136078 -4.2360158 -4.2095914 -4.157619][-4.21101 -4.2260523 -4.2356453 -4.2455468 -4.2497506 -4.2382317 -4.2047544 -4.1789718 -4.1798892 -4.1934352 -4.21286 -4.2453084 -4.2594829 -4.230392 -4.17399][-4.2297559 -4.2458606 -4.261385 -4.2769561 -4.2887816 -4.2891312 -4.2661576 -4.241797 -4.2402792 -4.2524 -4.2621274 -4.2831421 -4.2882996 -4.2596965 -4.2057486][-4.2651672 -4.2875175 -4.3063812 -4.3175626 -4.3232188 -4.318336 -4.2995348 -4.2819071 -4.2882814 -4.3044858 -4.311707 -4.3214521 -4.3167911 -4.2869978 -4.2425079][-4.3025889 -4.3277645 -4.3454828 -4.3506641 -4.346302 -4.33287 -4.3128972 -4.3005791 -4.3109055 -4.3303857 -4.3396707 -4.343215 -4.332891 -4.3079419 -4.2756777]]...]
INFO - root - 2017-12-06 08:33:03.985452: step 5910, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.904 sec/batch; 81h:59m:24s remains)
INFO - root - 2017-12-06 08:33:13.144899: step 5920, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.889 sec/batch; 80h:37m:57s remains)
INFO - root - 2017-12-06 08:33:22.332896: step 5930, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 78h:26m:38s remains)
INFO - root - 2017-12-06 08:33:31.568389: step 5940, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 83h:59m:45s remains)
INFO - root - 2017-12-06 08:33:40.746420: step 5950, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 86h:47m:50s remains)
INFO - root - 2017-12-06 08:33:49.931462: step 5960, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 84h:28m:30s remains)
INFO - root - 2017-12-06 08:33:59.150785: step 5970, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 84h:11m:45s remains)
INFO - root - 2017-12-06 08:34:08.347343: step 5980, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 84h:05m:51s remains)
INFO - root - 2017-12-06 08:34:17.390406: step 5990, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 83h:26m:45s remains)
INFO - root - 2017-12-06 08:34:26.735206: step 6000, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 83h:19m:59s remains)
2017-12-06 08:34:27.413126: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.361567 -4.3631363 -4.3645759 -4.3644347 -4.3612432 -4.3501782 -4.3296366 -4.3071818 -4.2919831 -4.2896781 -4.2982111 -4.31867 -4.3378415 -4.35088 -4.3557086][-4.3700323 -4.3735561 -4.3756156 -4.3725376 -4.3620257 -4.3380475 -4.301497 -4.2638988 -4.2415566 -4.2402978 -4.2556367 -4.2901659 -4.3230224 -4.3440824 -4.3536172][-4.3744941 -4.3805203 -4.3823657 -4.3726745 -4.3512297 -4.3113914 -4.2563434 -4.2025676 -4.1737266 -4.1778941 -4.2073965 -4.2594156 -4.3045268 -4.3330159 -4.3475876][-4.3768578 -4.3846684 -4.3837767 -4.3663168 -4.3340187 -4.2786436 -4.20622 -4.1380482 -4.1071668 -4.1234612 -4.1712303 -4.237597 -4.2921071 -4.3256845 -4.3429532][-4.3801064 -4.3872519 -4.3822103 -4.3558149 -4.3090835 -4.2340169 -4.1428776 -4.0599647 -4.0348854 -4.0737081 -4.1468883 -4.2291784 -4.2907972 -4.3251953 -4.3420687][-4.376555 -4.3821754 -4.3723869 -4.3331671 -4.2694912 -4.1746387 -4.0595875 -3.9628718 -3.9536688 -4.0269604 -4.1326408 -4.2302017 -4.2950978 -4.329061 -4.3450642][-4.3581324 -4.3641806 -4.3505263 -4.3011856 -4.2224345 -4.1074152 -3.9667063 -3.86573 -3.8883669 -3.9966443 -4.1268725 -4.233418 -4.300456 -4.3349404 -4.3500738][-4.326839 -4.3362017 -4.3203411 -4.2633 -4.1738033 -4.0456142 -3.8928702 -3.8112624 -3.8758073 -4.0093246 -4.1456327 -4.2477078 -4.3095345 -4.341404 -4.3536472][-4.2926188 -4.3087773 -4.2942977 -4.2370944 -4.1462808 -4.0209417 -3.8874125 -3.8395622 -3.929625 -4.066525 -4.1897893 -4.2744718 -4.324194 -4.3475661 -4.3544469][-4.2661219 -4.2929187 -4.2862244 -4.2361603 -4.1542811 -4.0529952 -3.9649191 -3.952002 -4.0382128 -4.15134 -4.24716 -4.3106904 -4.3435879 -4.3552518 -4.3552356][-4.247983 -4.2887244 -4.2946148 -4.2622004 -4.2026014 -4.1361408 -4.0910249 -4.0982327 -4.1607075 -4.237103 -4.3017831 -4.34448 -4.362318 -4.3640056 -4.3581347][-4.2391124 -4.2946644 -4.3132076 -4.3034463 -4.2701597 -4.229928 -4.2038612 -4.2100768 -4.24757 -4.2935314 -4.3357816 -4.3638749 -4.373702 -4.370892 -4.3625069][-4.2382078 -4.304585 -4.3349457 -4.3413305 -4.3211646 -4.2909493 -4.2667527 -4.2624717 -4.2797432 -4.3057375 -4.3382883 -4.3623114 -4.3730164 -4.3718629 -4.3648539][-4.2522469 -4.3223028 -4.3576927 -4.3636475 -4.3412094 -4.3073173 -4.2752957 -4.2611437 -4.2672625 -4.2843423 -4.3163962 -4.3450918 -4.3616447 -4.3654265 -4.3628855][-4.2822466 -4.3408427 -4.36597 -4.3596754 -4.3260517 -4.2836304 -4.2424073 -4.2203526 -4.2251725 -4.2462859 -4.2870469 -4.3234043 -4.3461895 -4.35658 -4.3586974]]...]
INFO - root - 2017-12-06 08:34:36.624262: step 6010, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 85h:12m:32s remains)
INFO - root - 2017-12-06 08:34:45.791871: step 6020, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.947 sec/batch; 85h:52m:27s remains)
INFO - root - 2017-12-06 08:34:55.045952: step 6030, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.942 sec/batch; 85h:28m:15s remains)
INFO - root - 2017-12-06 08:35:04.277248: step 6040, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 82h:38m:58s remains)
INFO - root - 2017-12-06 08:35:13.468797: step 6050, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 81h:36m:49s remains)
INFO - root - 2017-12-06 08:35:22.533091: step 6060, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.964 sec/batch; 87h:23m:20s remains)
INFO - root - 2017-12-06 08:35:31.752792: step 6070, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 81h:11m:24s remains)
INFO - root - 2017-12-06 08:35:40.921445: step 6080, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.909 sec/batch; 82h:23m:26s remains)
INFO - root - 2017-12-06 08:35:50.191078: step 6090, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.943 sec/batch; 85h:30m:32s remains)
INFO - root - 2017-12-06 08:35:59.496540: step 6100, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.970 sec/batch; 87h:56m:19s remains)
2017-12-06 08:36:00.170118: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3184271 -4.3242741 -4.33255 -4.3355985 -4.3292861 -4.317091 -4.3049731 -4.2999773 -4.3009248 -4.305161 -4.3169661 -4.3281689 -4.3317847 -4.3252282 -4.3158464][-4.3091211 -4.3116832 -4.3175182 -4.3180532 -4.3086329 -4.2914691 -4.2741776 -4.2691126 -4.2706327 -4.2737479 -4.2882295 -4.3068767 -4.3151479 -4.307272 -4.2964058][-4.3057303 -4.3043346 -4.3066478 -4.3054757 -4.29269 -4.269474 -4.2472105 -4.2402759 -4.2442479 -4.2459259 -4.2603712 -4.2855344 -4.3002815 -4.2919436 -4.2792392][-4.3030682 -4.2974496 -4.2943821 -4.2911663 -4.2770634 -4.2496595 -4.2210264 -4.2097297 -4.2180367 -4.2206693 -4.2334576 -4.2602 -4.2788367 -4.2709603 -4.257906][-4.2957835 -4.2849817 -4.2769547 -4.2719522 -4.2616529 -4.2358994 -4.2022085 -4.1877155 -4.20276 -4.2102804 -4.2201781 -4.2378426 -4.2516661 -4.2418017 -4.226819][-4.2925839 -4.2782216 -4.2672157 -4.259459 -4.2507887 -4.2274575 -4.1926436 -4.1776958 -4.1987658 -4.2141585 -4.2259836 -4.2362905 -4.2398462 -4.2207203 -4.1975765][-4.2909174 -4.2730088 -4.2591152 -4.2475419 -4.2346873 -4.2111664 -4.1767693 -4.16221 -4.18323 -4.20442 -4.2250752 -4.239089 -4.2379475 -4.2134151 -4.1843953][-4.2897162 -4.2699718 -4.2526789 -4.2367554 -4.2198672 -4.19508 -4.1599045 -4.1425471 -4.1540051 -4.1738997 -4.2055326 -4.2310357 -4.2336869 -4.2133665 -4.1882062][-4.2920103 -4.2753143 -4.2587371 -4.2410645 -4.2198205 -4.19216 -4.1577506 -4.1358685 -4.1346846 -4.1522717 -4.191587 -4.2252131 -4.23218 -4.2221923 -4.2090316][-4.296392 -4.2858181 -4.273097 -4.2557282 -4.2311611 -4.198544 -4.1619935 -4.1389718 -4.1353168 -4.1520224 -4.1891737 -4.2213526 -4.230907 -4.2301049 -4.2236938][-4.2958436 -4.2882318 -4.2791753 -4.2627368 -4.2375283 -4.2053604 -4.1668196 -4.14235 -4.142096 -4.161293 -4.1954479 -4.223237 -4.2340775 -4.2363086 -4.2329922][-4.2913194 -4.2829556 -4.2751365 -4.2592545 -4.2369761 -4.2107587 -4.1782675 -4.1580563 -4.1622758 -4.1831427 -4.2122078 -4.232306 -4.2400613 -4.2430253 -4.2426338][-4.2889919 -4.2793469 -4.2737455 -4.2611642 -4.2469716 -4.2345128 -4.2146192 -4.2024093 -4.2072968 -4.2232642 -4.239749 -4.2489376 -4.2509675 -4.25131 -4.2521839][-4.2877522 -4.2784247 -4.2756772 -4.269187 -4.2648988 -4.2641487 -4.256835 -4.2518415 -4.2557564 -4.2641392 -4.2703753 -4.2720757 -4.2701926 -4.2691197 -4.2701745][-4.29005 -4.2802787 -4.277041 -4.2734346 -4.2738252 -4.2780027 -4.2786069 -4.2796249 -4.2832856 -4.2863755 -4.2868481 -4.2850337 -4.2818427 -4.2802057 -4.2806258]]...]
INFO - root - 2017-12-06 08:36:09.407270: step 6110, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 83h:13m:23s remains)
INFO - root - 2017-12-06 08:36:18.589797: step 6120, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.929 sec/batch; 84h:16m:02s remains)
INFO - root - 2017-12-06 08:36:27.827183: step 6130, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.946 sec/batch; 85h:43m:43s remains)
INFO - root - 2017-12-06 08:36:37.040864: step 6140, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 85h:47m:49s remains)
INFO - root - 2017-12-06 08:36:46.230589: step 6150, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 83h:40m:38s remains)
INFO - root - 2017-12-06 08:36:55.554526: step 6160, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 81h:13m:11s remains)
INFO - root - 2017-12-06 08:37:04.798176: step 6170, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 82h:19m:12s remains)
INFO - root - 2017-12-06 08:37:13.980011: step 6180, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 0.803 sec/batch; 72h:44m:53s remains)
INFO - root - 2017-12-06 08:37:23.137837: step 6190, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.898 sec/batch; 81h:24m:15s remains)
INFO - root - 2017-12-06 08:37:32.207083: step 6200, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.925 sec/batch; 83h:48m:10s remains)
2017-12-06 08:37:32.798493: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2273054 -4.216414 -4.2147889 -4.2256179 -4.234005 -4.2236681 -4.1961246 -4.157876 -4.1111484 -4.0903115 -4.1122994 -4.1562762 -4.1929073 -4.2284718 -4.2720461][-4.2321992 -4.2304745 -4.236115 -4.2498374 -4.2529845 -4.2334766 -4.1950946 -4.1531615 -4.1094451 -4.0918608 -4.1151013 -4.1662426 -4.2154121 -4.2616963 -4.3055367][-4.2439022 -4.2521939 -4.2638016 -4.2777824 -4.2752895 -4.2434878 -4.1911864 -4.1402278 -4.0987086 -4.088275 -4.119688 -4.1806626 -4.2389903 -4.2877388 -4.3252845][-4.2633152 -4.2776985 -4.2917762 -4.301013 -4.2857351 -4.2381835 -4.170155 -4.1034875 -4.0634956 -4.0683875 -4.1189938 -4.1939588 -4.2622023 -4.3097692 -4.334126][-4.2833705 -4.2998528 -4.3121142 -4.31238 -4.2816572 -4.2163324 -4.1271977 -4.04207 -4.0116949 -4.0479832 -4.1240158 -4.2136393 -4.2875695 -4.3283229 -4.3339324][-4.2960606 -4.3123069 -4.3184457 -4.3051648 -4.2562652 -4.1673975 -4.0539002 -3.9640045 -3.9665728 -4.0462136 -4.1468573 -4.24057 -4.3070683 -4.3338871 -4.3196239][-4.2862353 -4.2989607 -4.2980738 -4.2724233 -4.204586 -4.0952754 -3.9722161 -3.9016271 -3.952327 -4.0654387 -4.175241 -4.2613153 -4.3115239 -4.3227406 -4.2914057][-4.2554464 -4.2643766 -4.2623482 -4.2295275 -4.1519113 -4.0417671 -3.9412267 -3.9191465 -3.9985235 -4.110323 -4.2063622 -4.2767925 -4.3091021 -4.3031473 -4.2584696][-4.2191739 -4.2318749 -4.2338128 -4.1981072 -4.1207476 -4.0248594 -3.9630313 -3.9837236 -4.0675297 -4.1614094 -4.2391253 -4.2915521 -4.3049793 -4.2841 -4.2291584][-4.1839685 -4.2049069 -4.2098689 -4.1703911 -4.0984211 -4.025476 -3.9991977 -4.0440741 -4.1252685 -4.2027931 -4.2661886 -4.3035374 -4.3026853 -4.2697134 -4.2086415][-4.1613388 -4.18876 -4.1903343 -4.1481686 -4.0902243 -4.0463209 -4.0466537 -4.1015987 -4.174686 -4.2334547 -4.2819118 -4.3100295 -4.2969203 -4.2526636 -4.187252][-4.167418 -4.1924725 -4.1857719 -4.1448874 -4.1046095 -4.0841336 -4.0974226 -4.1486959 -4.2096863 -4.2565527 -4.2961311 -4.313302 -4.287868 -4.23308 -4.1636004][-4.1842618 -4.1994805 -4.1865063 -4.1510873 -4.1276689 -4.1228995 -4.139564 -4.1823311 -4.233048 -4.2740831 -4.30461 -4.3093138 -4.2751122 -4.21311 -4.1402125][-4.2001138 -4.2111812 -4.2026792 -4.1754789 -4.1612778 -4.1593876 -4.1724558 -4.2056837 -4.2451735 -4.276659 -4.29561 -4.2898512 -4.25137 -4.1879625 -4.1201739][-4.2234058 -4.2392392 -4.2381129 -4.2168255 -4.2000957 -4.1910429 -4.1964092 -4.2190886 -4.2438583 -4.2613883 -4.2687407 -4.2575336 -4.2187457 -4.1612029 -4.1103163]]...]
INFO - root - 2017-12-06 08:37:42.065112: step 6210, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 82h:06m:13s remains)
INFO - root - 2017-12-06 08:37:51.391864: step 6220, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 82h:03m:32s remains)
INFO - root - 2017-12-06 08:38:00.611275: step 6230, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 83h:59m:22s remains)
INFO - root - 2017-12-06 08:38:09.865956: step 6240, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 87h:59m:50s remains)
INFO - root - 2017-12-06 08:38:19.035570: step 6250, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.935 sec/batch; 84h:42m:52s remains)
INFO - root - 2017-12-06 08:38:28.265166: step 6260, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.908 sec/batch; 82h:15m:24s remains)
INFO - root - 2017-12-06 08:38:37.371377: step 6270, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.908 sec/batch; 82h:18m:48s remains)
INFO - root - 2017-12-06 08:38:46.602257: step 6280, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 83h:30m:36s remains)
INFO - root - 2017-12-06 08:38:55.889593: step 6290, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 86h:44m:52s remains)
INFO - root - 2017-12-06 08:39:04.972059: step 6300, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 83h:36m:17s remains)
2017-12-06 08:39:05.627954: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3229866 -4.2888875 -4.2457666 -4.1816888 -4.1089234 -4.0739684 -4.1008444 -4.1383824 -4.1527128 -4.1586885 -4.1636529 -4.1700616 -4.1773891 -4.1934471 -4.2109265][-4.32585 -4.2908487 -4.2437444 -4.1709752 -4.0824819 -4.0294175 -4.0595889 -4.1184964 -4.1468835 -4.1537957 -4.1560378 -4.1616087 -4.1727018 -4.1961246 -4.2204976][-4.3300195 -4.2974219 -4.2488174 -4.1673608 -4.0627546 -3.9858825 -4.0115342 -4.091424 -4.1360512 -4.146842 -4.1472616 -4.1522985 -4.1675844 -4.1986217 -4.22887][-4.3335447 -4.306107 -4.2604489 -4.1753092 -4.0605621 -3.9601531 -3.9705353 -4.0638008 -4.1240234 -4.1399417 -4.1369796 -4.1390061 -4.15866 -4.1966658 -4.2314229][-4.3342247 -4.312089 -4.2715225 -4.1898417 -4.074645 -3.9571798 -3.9434655 -4.039084 -4.1134038 -4.1379805 -4.1346841 -4.1329761 -4.1525397 -4.1908255 -4.2246127][-4.3321447 -4.3153887 -4.2831597 -4.2128458 -4.1051116 -3.9805045 -3.9435134 -4.0260696 -4.1061893 -4.1393366 -4.140852 -4.1368203 -4.15187 -4.1838212 -4.2089076][-4.3299313 -4.3172207 -4.2937508 -4.2349906 -4.137641 -4.0143943 -3.9586537 -4.0171175 -4.0939655 -4.1357474 -4.1462541 -4.143085 -4.1508417 -4.170557 -4.1816506][-4.3282514 -4.3181119 -4.301826 -4.2555575 -4.1733856 -4.0609331 -3.994756 -4.0260973 -4.0904055 -4.1412134 -4.1636863 -4.1660838 -4.166018 -4.168797 -4.1627793][-4.3287687 -4.3207788 -4.3105841 -4.2763195 -4.2130656 -4.1183796 -4.0515785 -4.0593958 -4.1038785 -4.1568503 -4.1901155 -4.2028418 -4.2000089 -4.1892486 -4.1684666][-4.3307266 -4.32422 -4.3174076 -4.2918444 -4.248148 -4.1735482 -4.1142745 -4.1083694 -4.13437 -4.1806293 -4.2192903 -4.2423177 -4.2413883 -4.2228994 -4.1935849][-4.3324018 -4.3266 -4.3214784 -4.3017192 -4.2750683 -4.2230086 -4.1741257 -4.1605515 -4.1700559 -4.2045541 -4.2462974 -4.2810521 -4.2871714 -4.2679086 -4.2346573][-4.3343883 -4.3285832 -4.3228717 -4.3070569 -4.2927732 -4.2642059 -4.2268791 -4.2095847 -4.205862 -4.2283344 -4.2710781 -4.3142304 -4.3271627 -4.3103366 -4.2787213][-4.3373837 -4.3315306 -4.326458 -4.3136292 -4.3059778 -4.2922182 -4.2656488 -4.2481303 -4.2390604 -4.2541208 -4.2932768 -4.339066 -4.3567014 -4.3436337 -4.3170981][-4.3399787 -4.3336806 -4.3275523 -4.3161726 -4.3120093 -4.307744 -4.2916279 -4.2772508 -4.267128 -4.2765985 -4.3067961 -4.3464613 -4.364584 -4.3566241 -4.3381324][-4.3393912 -4.3318157 -4.3247776 -4.3167982 -4.3159852 -4.3171558 -4.3105488 -4.29894 -4.2873492 -4.2880387 -4.3061152 -4.3368855 -4.3555236 -4.3546867 -4.3443146]]...]
INFO - root - 2017-12-06 08:39:14.760594: step 6310, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 86h:57m:00s remains)
INFO - root - 2017-12-06 08:39:23.816792: step 6320, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 83h:25m:59s remains)
INFO - root - 2017-12-06 08:39:33.063911: step 6330, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.925 sec/batch; 83h:45m:55s remains)
INFO - root - 2017-12-06 08:39:42.115865: step 6340, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.951 sec/batch; 86h:09m:05s remains)
INFO - root - 2017-12-06 08:39:51.420167: step 6350, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.948 sec/batch; 85h:51m:32s remains)
INFO - root - 2017-12-06 08:40:00.660880: step 6360, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 85h:06m:14s remains)
INFO - root - 2017-12-06 08:40:09.791642: step 6370, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 84h:31m:39s remains)
INFO - root - 2017-12-06 08:40:18.846441: step 6380, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 77h:32m:18s remains)
INFO - root - 2017-12-06 08:40:28.160589: step 6390, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 85h:47m:00s remains)
INFO - root - 2017-12-06 08:40:37.386408: step 6400, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.912 sec/batch; 82h:34m:29s remains)
2017-12-06 08:40:38.009358: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2186618 -4.2246289 -4.2272463 -4.2220159 -4.2068782 -4.1939578 -4.1869464 -4.1836171 -4.1814504 -4.1848621 -4.1943436 -4.207582 -4.2135019 -4.2042351 -4.1853862][-4.2334967 -4.2292552 -4.222558 -4.2111926 -4.1920705 -4.1779985 -4.1746631 -4.1792459 -4.1837525 -4.1895456 -4.1991572 -4.21417 -4.2247581 -4.2202229 -4.2028937][-4.230227 -4.2208986 -4.2130628 -4.2006388 -4.1796045 -4.161447 -4.1549997 -4.1635885 -4.1746054 -4.1858907 -4.1970649 -4.21199 -4.2217526 -4.2169318 -4.1996965][-4.2186079 -4.2086143 -4.2030187 -4.1923456 -4.1698751 -4.1424708 -4.1285682 -4.1388526 -4.1607413 -4.180346 -4.1922064 -4.1994638 -4.2007728 -4.1935887 -4.1789379][-4.2054033 -4.1946712 -4.1909904 -4.1824441 -4.1593237 -4.1222944 -4.0992641 -4.1083961 -4.1394396 -4.166225 -4.1757207 -4.1729436 -4.1648211 -4.1595316 -4.152071][-4.1830206 -4.1752415 -4.1767359 -4.1715879 -4.1482878 -4.1047368 -4.0697684 -4.0730224 -4.1094294 -4.1437378 -4.154953 -4.1462812 -4.1336641 -4.1321192 -4.1385183][-4.1588988 -4.1525636 -4.155086 -4.1535807 -4.1333194 -4.08822 -4.0444236 -4.0390744 -4.0766797 -4.1166253 -4.1353822 -4.1371679 -4.1341915 -4.1416245 -4.1621103][-4.137321 -4.129158 -4.1277347 -4.1289263 -4.1137848 -4.0734282 -4.0286026 -4.0165915 -4.0512495 -4.0955377 -4.1220236 -4.1383491 -4.15489 -4.176518 -4.2032433][-4.141129 -4.1280837 -4.1201553 -4.1182766 -4.1086497 -4.0777278 -4.0406089 -4.0254369 -4.0512366 -4.0876336 -4.1127491 -4.1374073 -4.1687994 -4.2016912 -4.230031][-4.1623664 -4.1452956 -4.1286097 -4.1223769 -4.1211052 -4.1067672 -4.0828619 -4.0664163 -4.0776057 -4.1012769 -4.1219273 -4.15017 -4.1842895 -4.2156091 -4.2363281][-4.1852317 -4.1650105 -4.1433244 -4.1339684 -4.1360612 -4.1362748 -4.1276875 -4.1152134 -4.116643 -4.1274128 -4.1404877 -4.1633577 -4.1862106 -4.2041707 -4.2107663][-4.2060766 -4.1868992 -4.1642723 -4.1523724 -4.1536555 -4.1602993 -4.1623878 -4.1581736 -4.1593318 -4.1635551 -4.1669488 -4.1760612 -4.180408 -4.1831021 -4.179781][-4.22395 -4.207139 -4.1882138 -4.1752806 -4.1738892 -4.1817293 -4.1901007 -4.1905928 -4.1936345 -4.1970963 -4.1938152 -4.1875911 -4.1760926 -4.1661153 -4.1590381][-4.2453394 -4.2295971 -4.2130442 -4.2001686 -4.194253 -4.199667 -4.208703 -4.2134075 -4.2204738 -4.2246361 -4.2175379 -4.2026229 -4.1845031 -4.1719842 -4.1667523][-4.2670302 -4.2545424 -4.2395787 -4.2272377 -4.2205067 -4.2230182 -4.2302732 -4.2343025 -4.2409353 -4.245254 -4.237587 -4.2223692 -4.207922 -4.2025428 -4.2018366]]...]
INFO - root - 2017-12-06 08:40:47.284215: step 6410, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.996 sec/batch; 90h:12m:09s remains)
INFO - root - 2017-12-06 08:40:56.507555: step 6420, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 86h:56m:32s remains)
INFO - root - 2017-12-06 08:41:05.673044: step 6430, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 81h:45m:19s remains)
INFO - root - 2017-12-06 08:41:14.849461: step 6440, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 84h:29m:08s remains)
INFO - root - 2017-12-06 08:41:24.065341: step 6450, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 81h:09m:37s remains)
INFO - root - 2017-12-06 08:41:33.299192: step 6460, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 85h:02m:35s remains)
INFO - root - 2017-12-06 08:41:42.521270: step 6470, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.971 sec/batch; 87h:54m:06s remains)
INFO - root - 2017-12-06 08:41:51.699533: step 6480, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.956 sec/batch; 86h:32m:12s remains)
INFO - root - 2017-12-06 08:42:00.896874: step 6490, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 82h:06m:54s remains)
INFO - root - 2017-12-06 08:42:10.068994: step 6500, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 83h:00m:52s remains)
2017-12-06 08:42:10.768880: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3531265 -4.3760228 -4.3852272 -4.3816552 -4.3536558 -4.3066659 -4.2577829 -4.2149038 -4.1772566 -4.131753 -4.101119 -4.1036034 -4.1432486 -4.2034311 -4.2555981][-4.358613 -4.3831844 -4.38858 -4.3795943 -4.3411331 -4.2853718 -4.2275262 -4.1803145 -4.1417217 -4.10748 -4.087575 -4.093246 -4.1279225 -4.19205 -4.2495489][-4.361886 -4.38476 -4.3829861 -4.3639555 -4.3146195 -4.2541814 -4.1871929 -4.1336493 -4.0953293 -4.068768 -4.0591779 -4.0689006 -4.1073036 -4.1821532 -4.2449079][-4.363174 -4.3855457 -4.3792324 -4.3510947 -4.2916484 -4.2247605 -4.15437 -4.1021032 -4.0566177 -4.030107 -4.0255475 -4.0395393 -4.0923233 -4.17927 -4.2467546][-4.3695745 -4.3927131 -4.3798819 -4.3358459 -4.2601972 -4.1841841 -4.1156116 -4.0630674 -4.0124831 -3.9896395 -3.9954059 -4.0251651 -4.0988069 -4.1937542 -4.2598786][-4.3821235 -4.4008927 -4.3781009 -4.3170977 -4.2211175 -4.131825 -4.0594139 -3.9991696 -3.9588661 -3.955817 -3.9796674 -4.0340776 -4.1247358 -4.2194557 -4.2801776][-4.3877449 -4.3985572 -4.3657455 -4.2911453 -4.1781745 -4.0645666 -3.9708648 -3.9089937 -3.902678 -3.9325678 -3.9799685 -4.0585365 -4.1602073 -4.2472591 -4.2976818][-4.3850055 -4.3831038 -4.3401451 -4.2554226 -4.1260667 -3.9771771 -3.8535082 -3.805872 -3.8483102 -3.9144859 -3.9859374 -4.0881357 -4.193265 -4.2658958 -4.2998343][-4.3751669 -4.3580923 -4.3039856 -4.210464 -4.0744963 -3.9043136 -3.7610216 -3.7304246 -3.8044872 -3.8927433 -3.9835019 -4.1036553 -4.2064996 -4.2671423 -4.2865448][-4.3575058 -4.3265371 -4.2626781 -4.1661057 -4.041255 -3.886703 -3.7578325 -3.7421088 -3.8170114 -3.9026589 -3.9956994 -4.1162548 -4.2099161 -4.2600312 -4.2717295][-4.3352747 -4.2956028 -4.2246633 -4.1374145 -4.0425978 -3.9355125 -3.8524482 -3.8508682 -3.9053664 -3.9686677 -4.04573 -4.1441207 -4.2187457 -4.254631 -4.2613282][-4.31618 -4.2751412 -4.209291 -4.1407242 -4.0780292 -4.01289 -3.9706368 -3.9779665 -4.0137315 -4.0527239 -4.1055059 -4.1763492 -4.2304649 -4.253264 -4.2561407][-4.3058848 -4.2696619 -4.2149425 -4.1613173 -4.1190972 -4.0775723 -4.0580573 -4.0734429 -4.0996461 -4.1252561 -4.16261 -4.2133865 -4.248692 -4.2625508 -4.2626848][-4.3102355 -4.2860327 -4.246542 -4.2044821 -4.1748285 -4.1479659 -4.1401153 -4.1536732 -4.1714759 -4.1894145 -4.217823 -4.2531195 -4.2739134 -4.28123 -4.2810497][-4.3255277 -4.3135443 -4.2905288 -4.2626462 -4.2426581 -4.2268558 -4.2227354 -4.2294326 -4.2410064 -4.2534838 -4.2719965 -4.2927036 -4.302597 -4.3049121 -4.30418]]...]
INFO - root - 2017-12-06 08:42:20.075687: step 6510, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.942 sec/batch; 85h:18m:17s remains)
INFO - root - 2017-12-06 08:42:29.442723: step 6520, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.906 sec/batch; 82h:04m:54s remains)
INFO - root - 2017-12-06 08:42:38.736283: step 6530, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 82h:12m:00s remains)
INFO - root - 2017-12-06 08:42:47.816761: step 6540, loss = 2.04, batch loss = 1.99 (8.4 examples/sec; 0.949 sec/batch; 85h:58m:08s remains)
INFO - root - 2017-12-06 08:42:56.862943: step 6550, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.897 sec/batch; 81h:13m:05s remains)
INFO - root - 2017-12-06 08:43:06.168058: step 6560, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.925 sec/batch; 83h:44m:06s remains)
INFO - root - 2017-12-06 08:43:15.439745: step 6570, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.949 sec/batch; 85h:55m:21s remains)
INFO - root - 2017-12-06 08:43:24.442506: step 6580, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 83h:05m:38s remains)
INFO - root - 2017-12-06 08:43:33.599729: step 6590, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 84h:40m:42s remains)
INFO - root - 2017-12-06 08:43:42.868765: step 6600, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 85h:07m:24s remains)
2017-12-06 08:43:43.564178: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2653446 -4.2921343 -4.3086014 -4.3141475 -4.3067441 -4.2803426 -4.250443 -4.2158446 -4.2029185 -4.2373385 -4.2728109 -4.2692728 -4.2352509 -4.2235727 -4.2242665][-4.2711787 -4.2875767 -4.2987227 -4.2999539 -4.2915258 -4.2645388 -4.2324133 -4.1975088 -4.1898279 -4.2341475 -4.2765751 -4.2804976 -4.2560987 -4.2429638 -4.2363133][-4.2741604 -4.2781949 -4.2842026 -4.2859364 -4.2767859 -4.2546587 -4.2285681 -4.1981468 -4.1926403 -4.2382245 -4.2763 -4.2839222 -4.2696114 -4.2615523 -4.2519379][-4.2751179 -4.2653341 -4.2648983 -4.2649889 -4.249053 -4.2247877 -4.1978178 -4.1666889 -4.1625061 -4.2141361 -4.2545104 -4.2609673 -4.2510495 -4.2474542 -4.2378545][-4.2669873 -4.2396154 -4.2351346 -4.2268982 -4.1987505 -4.1642785 -4.1304579 -4.0831523 -4.0743923 -4.1480913 -4.2107382 -4.2220907 -4.209095 -4.1975722 -4.1843414][-4.2402558 -4.2006888 -4.1946621 -4.1816144 -4.1454883 -4.0970592 -4.0279379 -3.9435165 -3.94496 -4.0700021 -4.1651535 -4.1848516 -4.1682887 -4.1447873 -4.1214676][-4.1879959 -4.15226 -4.1501226 -4.1334429 -4.0944018 -4.0338793 -3.9201455 -3.7782712 -3.7993197 -3.994864 -4.12065 -4.1487947 -4.1297565 -4.0953288 -4.0645413][-4.1379576 -4.1161528 -4.1195879 -4.1040053 -4.0707469 -4.0144763 -3.8939281 -3.7476163 -3.7802067 -3.9803061 -4.1029167 -4.1342316 -4.11943 -4.0867267 -4.0597296][-4.1046872 -4.09512 -4.1037121 -4.0972047 -4.0847549 -4.0528307 -3.9781804 -3.8947349 -3.9286494 -4.0540109 -4.1303539 -4.1584711 -4.1533732 -4.1328335 -4.117332][-4.0947561 -4.0967813 -4.1113129 -4.1147122 -4.119607 -4.1107974 -4.0719438 -4.0319414 -4.0581651 -4.1223249 -4.166101 -4.1906595 -4.1937108 -4.1850524 -4.1772332][-4.1048241 -4.1150336 -4.1321583 -4.136632 -4.1484179 -4.1497593 -4.1344995 -4.119308 -4.1376185 -4.1687655 -4.189177 -4.2065344 -4.2189727 -4.2203026 -4.213665][-4.1425323 -4.1548057 -4.1659141 -4.1632442 -4.1699576 -4.1752386 -4.1755915 -4.1724849 -4.1861372 -4.204968 -4.2109857 -4.2191429 -4.23437 -4.2404542 -4.2341185][-4.2071257 -4.2150989 -4.2201805 -4.2136388 -4.2127156 -4.2147837 -4.220645 -4.2215533 -4.2298613 -4.2394109 -4.2373648 -4.2395554 -4.2514911 -4.2600574 -4.2556238][-4.2758956 -4.2783489 -4.2761931 -4.2684 -4.264184 -4.2651796 -4.2725163 -4.2756548 -4.2822657 -4.2853661 -4.2796865 -4.2756286 -4.2842822 -4.2920823 -4.2896385][-4.3229246 -4.3248534 -4.3236213 -4.3168974 -4.3112111 -4.31114 -4.3176274 -4.3213363 -4.32682 -4.326457 -4.3193808 -4.3134212 -4.3198514 -4.3270116 -4.3259888]]...]
INFO - root - 2017-12-06 08:43:52.755038: step 6610, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.900 sec/batch; 81h:27m:07s remains)
INFO - root - 2017-12-06 08:44:01.808936: step 6620, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 81h:43m:03s remains)
INFO - root - 2017-12-06 08:44:11.074374: step 6630, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.904 sec/batch; 81h:47m:44s remains)
INFO - root - 2017-12-06 08:44:20.271105: step 6640, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 84h:03m:45s remains)
INFO - root - 2017-12-06 08:44:29.321817: step 6650, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 83h:23m:37s remains)
INFO - root - 2017-12-06 08:44:38.585761: step 6660, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 87h:25m:33s remains)
INFO - root - 2017-12-06 08:44:47.686755: step 6670, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.926 sec/batch; 83h:46m:58s remains)
INFO - root - 2017-12-06 08:44:56.880537: step 6680, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 83h:22m:22s remains)
INFO - root - 2017-12-06 08:45:06.005309: step 6690, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.976 sec/batch; 88h:17m:35s remains)
INFO - root - 2017-12-06 08:45:15.268296: step 6700, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.890 sec/batch; 80h:31m:18s remains)
2017-12-06 08:45:15.960521: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1416292 -4.1509571 -4.1692948 -4.181107 -4.166697 -4.1285715 -4.0763583 -4.0246453 -4.008215 -4.0393977 -4.0852027 -4.1316714 -4.1747484 -4.2052579 -4.2201247][-4.1396914 -4.1509056 -4.1688662 -4.1890497 -4.1866608 -4.1550484 -4.1004591 -4.0412254 -4.008841 -4.0189581 -4.04792 -4.0822473 -4.1228681 -4.1571622 -4.1841078][-4.1342058 -4.1560836 -4.1834235 -4.2132921 -4.2189126 -4.1916003 -4.1438832 -4.0871072 -4.0393958 -4.02546 -4.0322552 -4.0473647 -4.0765486 -4.1120367 -4.1494608][-4.1106091 -4.1420903 -4.1855626 -4.2244091 -4.2339892 -4.2087011 -4.1683869 -4.1189308 -4.0747266 -4.0622 -4.0635505 -4.0639267 -4.0820155 -4.107779 -4.1384921][-4.0783634 -4.1026134 -4.1567869 -4.2053952 -4.216886 -4.1891661 -4.1483684 -4.09534 -4.0595117 -4.0687652 -4.0902181 -4.0997262 -4.1165581 -4.1313419 -4.1460214][-4.0464516 -4.0567169 -4.1119256 -4.16667 -4.1831956 -4.1534891 -4.1003437 -4.029985 -3.9905519 -4.0235167 -4.0795259 -4.1162143 -4.1473088 -4.1597538 -4.166852][-4.0322943 -4.0346289 -4.0812864 -4.1323171 -4.1497173 -4.1201167 -4.0575218 -3.97711 -3.93407 -3.9758484 -4.050993 -4.1116848 -4.157876 -4.1760335 -4.1848412][-4.0190988 -4.0205312 -4.0548267 -4.0937834 -4.1104684 -4.0861788 -4.0285158 -3.9569395 -3.9234629 -3.9577923 -4.0240388 -4.08658 -4.13607 -4.1576395 -4.1688766][-4.0135341 -4.0164776 -4.0411577 -4.0693693 -4.0849833 -4.0692825 -4.0275469 -3.9745374 -3.9508743 -3.9765637 -4.0248928 -4.0783386 -4.1214185 -4.1383758 -4.1436954][-4.0171733 -4.0335059 -4.0618591 -4.0875597 -4.0973277 -4.0826221 -4.0533714 -4.0202584 -4.00506 -4.0246792 -4.0616288 -4.1046672 -4.138484 -4.1487503 -4.1455317][-4.0290322 -4.0569048 -4.0913 -4.1179509 -4.1235204 -4.1090651 -4.0897918 -4.0708752 -4.0643544 -4.0838962 -4.116384 -4.1552086 -4.18357 -4.1891789 -4.1803451][-4.0800066 -4.1071844 -4.1400509 -4.1654282 -4.1706972 -4.1578736 -4.1447282 -4.1313677 -4.1302147 -4.1500182 -4.1787486 -4.21384 -4.2380371 -4.2414875 -4.2298789][-4.1629872 -4.180994 -4.204556 -4.2261243 -4.2310748 -4.2229195 -4.2148938 -4.2050595 -4.2036424 -4.2181649 -4.2408118 -4.2693486 -4.2875557 -4.2908115 -4.2813921][-4.2349987 -4.2438087 -4.2590752 -4.2760806 -4.2819624 -4.278945 -4.2730293 -4.2643766 -4.2624211 -4.2725315 -4.2897644 -4.31183 -4.3245435 -4.3275604 -4.3207521][-4.2824368 -4.2868843 -4.297945 -4.3107538 -4.3160882 -4.3143797 -4.3080964 -4.3009815 -4.2987504 -4.3040042 -4.3161445 -4.3322411 -4.341651 -4.3447223 -4.3408046]]...]
INFO - root - 2017-12-06 08:45:25.126461: step 6710, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 81h:09m:13s remains)
INFO - root - 2017-12-06 08:45:34.358888: step 6720, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.966 sec/batch; 87h:26m:01s remains)
INFO - root - 2017-12-06 08:45:43.505689: step 6730, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 85h:11m:21s remains)
INFO - root - 2017-12-06 08:45:52.686933: step 6740, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 82h:25m:06s remains)
INFO - root - 2017-12-06 08:46:02.012955: step 6750, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.909 sec/batch; 82h:14m:12s remains)
INFO - root - 2017-12-06 08:46:10.694557: step 6760, loss = 2.09, batch loss = 2.03 (13.9 examples/sec; 0.576 sec/batch; 52h:05m:47s remains)
INFO - root - 2017-12-06 08:46:19.917971: step 6770, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 83h:56m:11s remains)
INFO - root - 2017-12-06 08:46:29.084096: step 6780, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.965 sec/batch; 87h:18m:36s remains)
INFO - root - 2017-12-06 08:46:38.374436: step 6790, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 82h:54m:37s remains)
INFO - root - 2017-12-06 08:46:47.531727: step 6800, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 81h:13m:02s remains)
2017-12-06 08:46:48.170884: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2315354 -4.2320552 -4.2353015 -4.2383556 -4.2522945 -4.2428651 -4.2036428 -4.1817884 -4.2103314 -4.2651567 -4.2825012 -4.2725263 -4.2640319 -4.2512479 -4.2545166][-4.2230949 -4.2183881 -4.2129545 -4.2170792 -4.2399549 -4.2348132 -4.1952896 -4.1723657 -4.202498 -4.2552819 -4.269773 -4.2518783 -4.2396531 -4.2292233 -4.2394838][-4.2246757 -4.2137265 -4.1984959 -4.1972156 -4.2204456 -4.2166638 -4.18002 -4.1522608 -4.1813645 -4.2334266 -4.2512584 -4.233232 -4.2196951 -4.2134619 -4.2287493][-4.2458458 -4.23408 -4.2101564 -4.2018371 -4.2129345 -4.2004738 -4.1593246 -4.1217618 -4.1530724 -4.2113895 -4.2396083 -4.2307534 -4.2213912 -4.2192206 -4.2345586][-4.264513 -4.2542624 -4.2290459 -4.2181716 -4.2121534 -4.1797457 -4.1176696 -4.0646834 -4.0999937 -4.1728616 -4.2170072 -4.2214332 -4.2242613 -4.2256742 -4.240521][-4.2727933 -4.2622032 -4.2413111 -4.2306385 -4.207016 -4.1521106 -4.0600729 -3.9794934 -4.0157862 -4.1097021 -4.1813416 -4.2092872 -4.2258139 -4.2313037 -4.2430663][-4.2669015 -4.2588334 -4.245707 -4.2400737 -4.2065234 -4.1341438 -4.0132961 -3.9018238 -3.9336698 -4.049284 -4.1477761 -4.1936555 -4.2230563 -4.2367654 -4.247406][-4.2623248 -4.2594585 -4.2574091 -4.2576528 -4.2233696 -4.146204 -4.0225525 -3.9015412 -3.9213698 -4.0393786 -4.1412125 -4.1924248 -4.2319956 -4.2534671 -4.2627473][-4.2620044 -4.2659597 -4.2707 -4.2721572 -4.2424583 -4.1723466 -4.0695586 -3.9694831 -3.9793639 -4.0771818 -4.1630597 -4.2095466 -4.2498026 -4.2721338 -4.27753][-4.2584634 -4.2685418 -4.2791238 -4.2840352 -4.2642093 -4.2074285 -4.1275115 -4.0533128 -4.0571561 -4.1294451 -4.1923194 -4.2286592 -4.2608438 -4.2789783 -4.2831078][-4.259264 -4.2734342 -4.2882118 -4.2956963 -4.2839384 -4.2391911 -4.17556 -4.123805 -4.1270809 -4.1790395 -4.2223277 -4.2467909 -4.2676587 -4.2820563 -4.287612][-4.2696218 -4.2850161 -4.29993 -4.3057456 -4.2955055 -4.2581434 -4.2044015 -4.1690226 -4.1775951 -4.2196589 -4.2514267 -4.2642131 -4.2713943 -4.2807479 -4.2863612][-4.2720952 -4.2864637 -4.2999129 -4.3018818 -4.2882814 -4.2545705 -4.2133608 -4.190824 -4.2030258 -4.2367296 -4.2597113 -4.2666159 -4.2701254 -4.2743421 -4.2757721][-4.2592678 -4.2723403 -4.2849755 -4.2884254 -4.2758269 -4.2485862 -4.2220144 -4.2091756 -4.2188196 -4.2361932 -4.248044 -4.2555289 -4.2613168 -4.2654328 -4.2641225][-4.2320242 -4.2439718 -4.2602372 -4.2715168 -4.2670932 -4.2475185 -4.2326517 -4.2244639 -4.2250161 -4.2231708 -4.2256026 -4.2386589 -4.2525048 -4.2620916 -4.2609968]]...]
INFO - root - 2017-12-06 08:46:57.536450: step 6810, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.904 sec/batch; 81h:49m:15s remains)
INFO - root - 2017-12-06 08:47:06.741223: step 6820, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 84h:10m:44s remains)
INFO - root - 2017-12-06 08:47:15.919138: step 6830, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.823 sec/batch; 74h:25m:28s remains)
INFO - root - 2017-12-06 08:47:25.020556: step 6840, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 80h:17m:07s remains)
INFO - root - 2017-12-06 08:47:34.225422: step 6850, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 82h:03m:16s remains)
INFO - root - 2017-12-06 08:47:43.513023: step 6860, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 85h:10m:39s remains)
INFO - root - 2017-12-06 08:47:52.691893: step 6870, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 81h:49m:24s remains)
INFO - root - 2017-12-06 08:48:01.885251: step 6880, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 82h:59m:15s remains)
INFO - root - 2017-12-06 08:48:11.094785: step 6890, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 85h:55m:11s remains)
INFO - root - 2017-12-06 08:48:20.269624: step 6900, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 82h:11m:48s remains)
2017-12-06 08:48:20.911260: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2348504 -4.2154903 -4.2009592 -4.204596 -4.2193336 -4.2246089 -4.2225461 -4.2202969 -4.2148361 -4.2096758 -4.2122927 -4.2268052 -4.2523427 -4.2651629 -4.2648873][-4.24249 -4.2357006 -4.225225 -4.2243576 -4.2373214 -4.2481475 -4.2508907 -4.2488608 -4.2429004 -4.2352176 -4.2316113 -4.2423658 -4.2663236 -4.2798009 -4.2763357][-4.2270374 -4.2294555 -4.22321 -4.2244749 -4.2430844 -4.2595272 -4.2616205 -4.2526355 -4.24358 -4.238091 -4.2350621 -4.2461247 -4.2686672 -4.2818704 -4.2786708][-4.2050939 -4.2129025 -4.2128491 -4.2222533 -4.2479987 -4.2632961 -4.2522879 -4.2221589 -4.2017107 -4.2004704 -4.208529 -4.2301974 -4.2607408 -4.2765422 -4.2733979][-4.1914654 -4.2043929 -4.2113323 -4.22913 -4.25438 -4.2567244 -4.2194848 -4.1582437 -4.1169157 -4.1266646 -4.1598382 -4.2016964 -4.2448497 -4.2670107 -4.264575][-4.18224 -4.2010827 -4.2108159 -4.22891 -4.2434859 -4.2247472 -4.1587787 -4.055644 -3.9865444 -4.0193963 -4.0961437 -4.1692324 -4.2275028 -4.25663 -4.2561097][-4.1802435 -4.2010589 -4.2032447 -4.2066245 -4.202414 -4.157896 -4.0552893 -3.8996761 -3.8109851 -3.8996661 -4.039741 -4.1454692 -4.215899 -4.2472496 -4.2448158][-4.1928186 -4.2053442 -4.1939673 -4.1821957 -4.1563263 -4.085247 -3.946552 -3.7537818 -3.6761637 -3.8327782 -4.0145917 -4.1420097 -4.2144771 -4.2449284 -4.2445707][-4.2145371 -4.2136178 -4.192872 -4.1669931 -4.1217837 -4.0417361 -3.9177084 -3.775224 -3.7560954 -3.8997984 -4.0559878 -4.1678867 -4.2297287 -4.2562366 -4.2569108][-4.2269382 -4.2188363 -4.1955814 -4.158504 -4.1080546 -4.04551 -3.9779167 -3.9238262 -3.9374995 -4.0288796 -4.1275139 -4.2023873 -4.2437711 -4.2611227 -4.260952][-4.2235918 -4.2179489 -4.1905341 -4.1461535 -4.1048203 -4.0726213 -4.0531263 -4.0438538 -4.0606327 -4.1067162 -4.1599703 -4.207355 -4.2373118 -4.24628 -4.2495108][-4.2098885 -4.2025709 -4.1700253 -4.128284 -4.1082087 -4.1106372 -4.1147804 -4.1170712 -4.128355 -4.1430397 -4.16463 -4.1937823 -4.2183132 -4.2283978 -4.2405415][-4.190454 -4.1810966 -4.1508632 -4.1170588 -4.1157436 -4.1455932 -4.1671104 -4.1739206 -4.177475 -4.1713676 -4.1707721 -4.184535 -4.2033978 -4.218039 -4.235682][-4.1664314 -4.1651306 -4.1414423 -4.1128058 -4.1194553 -4.165318 -4.2033305 -4.2153478 -4.2126145 -4.1971836 -4.1810279 -4.1763473 -4.1857486 -4.2057266 -4.2312946][-4.1657529 -4.1721835 -4.1472821 -4.11409 -4.1134343 -4.162662 -4.2129297 -4.2307434 -4.2272735 -4.2086473 -4.1864924 -4.1738091 -4.1743965 -4.1966314 -4.222517]]...]
INFO - root - 2017-12-06 08:48:30.013705: step 6910, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.999 sec/batch; 90h:23m:36s remains)
INFO - root - 2017-12-06 08:48:39.241065: step 6920, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 82h:55m:03s remains)
INFO - root - 2017-12-06 08:48:48.463474: step 6930, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.977 sec/batch; 88h:20m:00s remains)
INFO - root - 2017-12-06 08:48:57.743885: step 6940, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 81h:19m:53s remains)
INFO - root - 2017-12-06 08:49:06.984607: step 6950, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.889 sec/batch; 80h:23m:07s remains)
INFO - root - 2017-12-06 08:49:16.263066: step 6960, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.909 sec/batch; 82h:10m:18s remains)
INFO - root - 2017-12-06 08:49:25.455389: step 6970, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.957 sec/batch; 86h:33m:41s remains)
INFO - root - 2017-12-06 08:49:34.473542: step 6980, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 84h:31m:25s remains)
INFO - root - 2017-12-06 08:49:43.742469: step 6990, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 82h:01m:20s remains)
INFO - root - 2017-12-06 08:49:52.843802: step 7000, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 83h:02m:01s remains)
2017-12-06 08:49:53.470397: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2695279 -4.264183 -4.2615128 -4.25658 -4.253757 -4.2557893 -4.2573724 -4.2553296 -4.2508469 -4.2449374 -4.2458973 -4.2613435 -4.2762833 -4.29063 -4.3071322][-4.2306466 -4.2206888 -4.2174754 -4.2119932 -4.2077427 -4.2096424 -4.2117114 -4.2103653 -4.2067256 -4.2011023 -4.2044768 -4.2252588 -4.2432027 -4.2603717 -4.2827563][-4.1886992 -4.1748438 -4.1722145 -4.1675029 -4.1620092 -4.1627984 -4.1658263 -4.167716 -4.1704831 -4.1731029 -4.1809068 -4.2019758 -4.2189379 -4.2355924 -4.2608047][-4.1448894 -4.1312971 -4.1314197 -4.1281552 -4.1216497 -4.12054 -4.1220374 -4.1266036 -4.1379733 -4.15039 -4.1640534 -4.1825676 -4.1978889 -4.2130795 -4.241468][-4.1108875 -4.0976496 -4.0970774 -4.0886717 -4.0750804 -4.0654874 -4.0575628 -4.0609145 -4.0866342 -4.1176214 -4.1449208 -4.1630917 -4.1773963 -4.1962662 -4.2309117][-4.0909295 -4.072948 -4.0685239 -4.0502443 -4.0224695 -3.9953673 -3.9664917 -3.9614985 -4.0032887 -4.0577159 -4.10433 -4.1291614 -4.1465688 -4.1744819 -4.2172861][-4.0617428 -4.0366755 -4.025465 -3.9985065 -3.9572179 -3.9092121 -3.8520679 -3.8364315 -3.9011571 -3.9792538 -4.0424957 -4.0800557 -4.1092706 -4.1494007 -4.2002125][-4.0439019 -4.015461 -3.9965916 -3.9614859 -3.9131067 -3.8534737 -3.77837 -3.758322 -3.8400314 -3.9285669 -3.9958148 -4.0442023 -4.0871449 -4.1370578 -4.1919947][-4.0721707 -4.0520482 -4.0338945 -4.0036731 -3.9623389 -3.9119189 -3.8481536 -3.8330414 -3.8995297 -3.96705 -4.0179448 -4.0618525 -4.1061792 -4.15296 -4.19922][-4.1230607 -4.1178784 -4.1103964 -4.092525 -4.0666881 -4.0326 -3.9878874 -3.9765511 -4.0175266 -4.056149 -4.0857854 -4.1182523 -4.1526222 -4.1877279 -4.22116][-4.1820164 -4.1881261 -4.1913476 -4.1860719 -4.1760263 -4.1563983 -4.1282015 -4.120975 -4.1412349 -4.1579561 -4.1719232 -4.1922369 -4.216258 -4.2377996 -4.2570386][-4.2362747 -4.2455182 -4.2520614 -4.2544422 -4.253089 -4.2429452 -4.22608 -4.22352 -4.2332759 -4.2392349 -4.2441225 -4.2565584 -4.2722273 -4.2839203 -4.2932472][-4.27001 -4.2756286 -4.2793388 -4.2811856 -4.2818913 -4.278194 -4.2692223 -4.2688136 -4.2744541 -4.277463 -4.279737 -4.2893367 -4.3021626 -4.3118677 -4.3170562][-4.2774005 -4.2787337 -4.2790809 -4.28037 -4.2831225 -4.2827134 -4.2776189 -4.2778258 -4.2818174 -4.2838507 -4.2858024 -4.294651 -4.3076653 -4.3190088 -4.3244781][-4.2828341 -4.2833762 -4.2828455 -4.2845874 -4.2866311 -4.2858658 -4.282414 -4.2818937 -4.2838755 -4.2857428 -4.287631 -4.2949381 -4.3070674 -4.3187747 -4.3260984]]...]
INFO - root - 2017-12-06 08:50:02.755347: step 7010, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 84h:59m:26s remains)
INFO - root - 2017-12-06 08:50:11.852813: step 7020, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 84h:41m:58s remains)
INFO - root - 2017-12-06 08:50:21.078495: step 7030, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.933 sec/batch; 84h:20m:13s remains)
INFO - root - 2017-12-06 08:50:30.247528: step 7040, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 85h:03m:35s remains)
INFO - root - 2017-12-06 08:50:39.156321: step 7050, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 83h:12m:58s remains)
INFO - root - 2017-12-06 08:50:48.435992: step 7060, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 82h:32m:04s remains)
INFO - root - 2017-12-06 08:50:57.605384: step 7070, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 79h:33m:45s remains)
INFO - root - 2017-12-06 08:51:06.743138: step 7080, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 79h:21m:21s remains)
INFO - root - 2017-12-06 08:51:15.897832: step 7090, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 81h:36m:55s remains)
INFO - root - 2017-12-06 08:51:25.058620: step 7100, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.900 sec/batch; 81h:18m:30s remains)
2017-12-06 08:51:25.655099: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2877631 -4.2977743 -4.303905 -4.299036 -4.2893491 -4.2838244 -4.2856622 -4.2889977 -4.2892203 -4.2822046 -4.2728672 -4.2705741 -4.2816453 -4.2993464 -4.3063321][-4.2960372 -4.2950506 -4.2879047 -4.2748413 -4.2591209 -4.2515807 -4.2634349 -4.282795 -4.2974739 -4.3011503 -4.2950268 -4.287324 -4.2882853 -4.2952824 -4.2968416][-4.2991939 -4.2875819 -4.2658176 -4.2436881 -4.2199373 -4.2067471 -4.2272854 -4.2628179 -4.2898765 -4.3021688 -4.2984366 -4.2848873 -4.2733989 -4.2667189 -4.2619686][-4.2923269 -4.27273 -4.2382374 -4.2031074 -4.1659951 -4.1428308 -4.1705613 -4.2232146 -4.2639227 -4.2838945 -4.2866817 -4.2734447 -4.2567415 -4.2398977 -4.2273912][-4.2867026 -4.2651739 -4.2217917 -4.1711569 -4.114924 -4.0758643 -4.1055307 -4.1763568 -4.2344952 -4.2636967 -4.2783523 -4.2751827 -4.26266 -4.2436733 -4.2269759][-4.2879996 -4.2687683 -4.2190042 -4.1494064 -4.0667591 -4.00453 -4.0300975 -4.1234422 -4.2022748 -4.243782 -4.2679224 -4.2793889 -4.2781706 -4.2671914 -4.2536769][-4.2834368 -4.270103 -4.2218852 -4.141295 -4.0376859 -3.9550369 -3.9718029 -4.0762858 -4.1693134 -4.2198124 -4.246944 -4.2656794 -4.2764645 -4.2809572 -4.2798314][-4.2652397 -4.2573872 -4.2196465 -4.1450577 -4.0424519 -3.9553785 -3.9621389 -4.0602546 -4.1532741 -4.2017841 -4.2240691 -4.2412305 -4.2586718 -4.2762952 -4.2859435][-4.2482405 -4.2435718 -4.2210121 -4.1664319 -4.0828047 -4.0030971 -4.0018992 -4.0835586 -4.164093 -4.2037811 -4.2175379 -4.2286997 -4.2479935 -4.2732143 -4.2858992][-4.2532511 -4.2454166 -4.2328115 -4.1965427 -4.1356268 -4.0712528 -4.0635653 -4.1292892 -4.1964397 -4.2252178 -4.2314372 -4.2358418 -4.2528 -4.2777553 -4.2891235][-4.2732077 -4.2622437 -4.2537808 -4.23056 -4.1894021 -4.1447387 -4.1350775 -4.1856108 -4.2397842 -4.259203 -4.2578154 -4.2548494 -4.2643857 -4.2820997 -4.2908688][-4.2829804 -4.2715788 -4.2679052 -4.2571378 -4.2335367 -4.2067757 -4.198524 -4.2333078 -4.2736397 -4.2870851 -4.2839913 -4.2789664 -4.2834105 -4.2927566 -4.2972317][-4.2773695 -4.2698393 -4.2734179 -4.2752657 -4.2646561 -4.2482696 -4.2389393 -4.2561955 -4.2793884 -4.2890196 -4.2922568 -4.2938709 -4.2969308 -4.3006248 -4.3004169][-4.2624922 -4.2597079 -4.2681508 -4.279716 -4.2783661 -4.26725 -4.2524147 -4.2519226 -4.2575083 -4.2644658 -4.2766962 -4.2888918 -4.2955961 -4.2972474 -4.2953825][-4.2494593 -4.250104 -4.2601471 -4.2758517 -4.28094 -4.2715392 -4.2500849 -4.2323022 -4.2208223 -4.22376 -4.2430787 -4.2643862 -4.2772012 -4.2827868 -4.2843189]]...]
INFO - root - 2017-12-06 08:51:34.838403: step 7110, loss = 2.11, batch loss = 2.05 (8.8 examples/sec; 0.904 sec/batch; 81h:44m:53s remains)
INFO - root - 2017-12-06 08:51:43.748119: step 7120, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.939 sec/batch; 84h:50m:50s remains)
INFO - root - 2017-12-06 08:51:52.945883: step 7130, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.904 sec/batch; 81h:40m:39s remains)
INFO - root - 2017-12-06 08:52:02.196312: step 7140, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 83h:48m:16s remains)
INFO - root - 2017-12-06 08:52:11.405250: step 7150, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 86h:29m:44s remains)
INFO - root - 2017-12-06 08:52:20.514642: step 7160, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 83h:36m:50s remains)
INFO - root - 2017-12-06 08:52:29.700004: step 7170, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.951 sec/batch; 85h:57m:13s remains)
INFO - root - 2017-12-06 08:52:38.753540: step 7180, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 84h:14m:05s remains)
INFO - root - 2017-12-06 08:52:47.842081: step 7190, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 86h:43m:49s remains)
INFO - root - 2017-12-06 08:52:57.071482: step 7200, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 83h:10m:58s remains)
2017-12-06 08:52:57.651201: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.238524 -4.2465115 -4.2561531 -4.2599769 -4.264781 -4.2749224 -4.2733979 -4.263135 -4.2623153 -4.2585378 -4.2510514 -4.2472935 -4.2492795 -4.2591119 -4.2673635][-4.2816215 -4.2837324 -4.2852955 -4.282403 -4.2829571 -4.2880712 -4.2827888 -4.266715 -4.2625475 -4.260076 -4.2528658 -4.2480702 -4.2478905 -4.2570171 -4.264708][-4.3026185 -4.3031507 -4.2983565 -4.2882586 -4.28058 -4.2780371 -4.2672949 -4.2445726 -4.23652 -4.2403507 -4.2436442 -4.2429109 -4.23751 -4.238606 -4.2384639][-4.3008208 -4.3082457 -4.3052621 -4.2884769 -4.2671356 -4.24847 -4.2236271 -4.192225 -4.1892295 -4.2139277 -4.2415562 -4.2563148 -4.2530823 -4.2470713 -4.2391639][-4.2786717 -4.2949667 -4.2974191 -4.2732444 -4.2300162 -4.182415 -4.1292663 -4.0863976 -4.1001592 -4.161129 -4.226964 -4.2698383 -4.282105 -4.2783213 -4.2661219][-4.2612462 -4.2814922 -4.283464 -4.2442093 -4.1706977 -4.082962 -3.9858103 -3.9152579 -3.9413254 -4.0508723 -4.1661091 -4.2450914 -4.2866826 -4.301805 -4.2970982][-4.254705 -4.2771173 -4.276556 -4.2210426 -4.1194448 -3.9939065 -3.8472352 -3.7309074 -3.760427 -3.9235373 -4.0891433 -4.1999545 -4.2704406 -4.3049679 -4.3049974][-4.2468348 -4.2821374 -4.2915425 -4.236402 -4.1306105 -3.9974756 -3.837441 -3.6921895 -3.7006695 -3.8712826 -4.050024 -4.1713591 -4.2507086 -4.2918487 -4.289484][-4.2261496 -4.27482 -4.3011203 -4.2667012 -4.1839113 -4.0784659 -3.9547617 -3.8373814 -3.8249829 -3.9391177 -4.076355 -4.175735 -4.2413955 -4.2735767 -4.2668052][-4.2214274 -4.276711 -4.3130617 -4.2974825 -4.242548 -4.1706061 -4.0926719 -4.0207171 -4.0016036 -4.0607514 -4.1466732 -4.2091317 -4.2456322 -4.2561526 -4.239428][-4.2564144 -4.2986031 -4.3244524 -4.3126335 -4.2750673 -4.2286711 -4.1872063 -4.1526589 -4.1403456 -4.170536 -4.2204962 -4.2552891 -4.2714944 -4.26515 -4.2372551][-4.2924876 -4.3158913 -4.3254271 -4.3112044 -4.2778869 -4.2452497 -4.224544 -4.2153821 -4.2195745 -4.2427068 -4.2736025 -4.2919068 -4.2933736 -4.27627 -4.2467971][-4.3042369 -4.3187361 -4.318676 -4.3059149 -4.2848415 -4.2630167 -4.2529492 -4.2553482 -4.26615 -4.2847733 -4.30431 -4.3144846 -4.3084173 -4.28501 -4.2573633][-4.3080635 -4.3189631 -4.3149414 -4.3044019 -4.2923946 -4.2785387 -4.2712212 -4.2724957 -4.2802 -4.2953796 -4.31039 -4.3197732 -4.3150644 -4.2945004 -4.2715073][-4.3101192 -4.3170309 -4.3116164 -4.3039145 -4.297195 -4.2884622 -4.2829227 -4.2824407 -4.2873516 -4.2978435 -4.3089328 -4.3160911 -4.3154826 -4.3043652 -4.2908297]]...]
INFO - root - 2017-12-06 08:53:06.925543: step 7210, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 83h:53m:08s remains)
INFO - root - 2017-12-06 08:53:16.067075: step 7220, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.907 sec/batch; 81h:54m:29s remains)
INFO - root - 2017-12-06 08:53:25.226409: step 7230, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 79h:19m:37s remains)
INFO - root - 2017-12-06 08:53:34.453650: step 7240, loss = 2.02, batch loss = 1.96 (8.6 examples/sec; 0.930 sec/batch; 84h:02m:22s remains)
INFO - root - 2017-12-06 08:53:43.589767: step 7250, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.945 sec/batch; 85h:25m:11s remains)
INFO - root - 2017-12-06 08:53:52.652325: step 7260, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 85h:16m:33s remains)
INFO - root - 2017-12-06 08:54:01.867963: step 7270, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 88h:16m:18s remains)
INFO - root - 2017-12-06 08:54:10.958995: step 7280, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 82h:26m:43s remains)
INFO - root - 2017-12-06 08:54:20.200881: step 7290, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 82h:49m:12s remains)
INFO - root - 2017-12-06 08:54:29.408809: step 7300, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.871 sec/batch; 78h:40m:24s remains)
2017-12-06 08:54:30.006134: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1895 -4.1997194 -4.212944 -4.2192788 -4.2262831 -4.2293987 -4.2226081 -4.2017093 -4.1860266 -4.1856728 -4.1959424 -4.2171488 -4.217453 -4.1967134 -4.1795874][-4.1843796 -4.196167 -4.2103729 -4.2171078 -4.2247829 -4.2262292 -4.212111 -4.1839881 -4.1709251 -4.1765532 -4.193994 -4.2193103 -4.2215128 -4.1985054 -4.1692414][-4.1689816 -4.1812129 -4.1941724 -4.2053838 -4.2178669 -4.222353 -4.203227 -4.1704664 -4.1602821 -4.1751022 -4.1974697 -4.2191792 -4.2187309 -4.1921453 -4.15492][-4.1387868 -4.1564064 -4.1752458 -4.19053 -4.208849 -4.2116394 -4.184298 -4.1549191 -4.1511893 -4.1750121 -4.200675 -4.2150831 -4.2059226 -4.1737976 -4.1375866][-4.11125 -4.1343288 -4.162858 -4.1843348 -4.2023206 -4.1975455 -4.1663322 -4.1394448 -4.1443062 -4.1836114 -4.2166147 -4.2209148 -4.1926847 -4.1537719 -4.1265969][-4.1116571 -4.13918 -4.1773353 -4.1997733 -4.2054081 -4.1906309 -4.154428 -4.1231294 -4.1315832 -4.1790552 -4.2211051 -4.2213249 -4.1817393 -4.1417332 -4.1244283][-4.12645 -4.1578164 -4.1960192 -4.2135291 -4.2067518 -4.1844854 -4.149641 -4.1144657 -4.1127992 -4.1559362 -4.2058167 -4.2113929 -4.1771297 -4.1465459 -4.1401439][-4.1361351 -4.1679115 -4.2004852 -4.2141533 -4.2032776 -4.1810741 -4.1493731 -4.1155243 -4.1006188 -4.1354971 -4.1887679 -4.1962762 -4.1710191 -4.1611524 -4.1701431][-4.1526361 -4.1720743 -4.1935039 -4.2029724 -4.1921263 -4.1734557 -4.1471934 -4.1198673 -4.1038303 -4.1296654 -4.1753292 -4.1808147 -4.1638255 -4.1660013 -4.180047][-4.1851587 -4.1965389 -4.2027211 -4.2042575 -4.194386 -4.1807718 -4.1587377 -4.143259 -4.1318474 -4.1432247 -4.1738391 -4.177527 -4.16017 -4.1607418 -4.1700573][-4.2130227 -4.22505 -4.2250261 -4.2178884 -4.2086678 -4.1960936 -4.1771541 -4.1671333 -4.1585608 -4.1672072 -4.190259 -4.1895185 -4.1661825 -4.1585178 -4.162281][-4.2178221 -4.2406306 -4.2438712 -4.2292247 -4.2169876 -4.19928 -4.1791964 -4.174026 -4.1740031 -4.1909065 -4.2124748 -4.2098727 -4.1848993 -4.1724215 -4.1687164][-4.1919703 -4.2301335 -4.2417073 -4.2317748 -4.2241921 -4.202467 -4.1781883 -4.1782293 -4.1895323 -4.214323 -4.231607 -4.22538 -4.2045097 -4.1877866 -4.171617][-4.1474795 -4.1959414 -4.2232809 -4.2282128 -4.2285771 -4.2065954 -4.1836081 -4.1819081 -4.1965766 -4.2214661 -4.2338147 -4.22764 -4.2102647 -4.1917138 -4.1719851][-4.1153913 -4.1713119 -4.2125058 -4.2287741 -4.2316866 -4.210259 -4.1863956 -4.1738687 -4.177629 -4.1989532 -4.2112937 -4.2098308 -4.1986966 -4.1821251 -4.1688652]]...]
INFO - root - 2017-12-06 08:54:38.975847: step 7310, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.902 sec/batch; 81h:26m:38s remains)
INFO - root - 2017-12-06 08:54:48.235343: step 7320, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 76h:51m:25s remains)
INFO - root - 2017-12-06 08:54:57.502683: step 7330, loss = 2.05, batch loss = 2.00 (8.0 examples/sec; 0.995 sec/batch; 89h:51m:56s remains)
INFO - root - 2017-12-06 08:55:06.728228: step 7340, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.930 sec/batch; 83h:58m:02s remains)
INFO - root - 2017-12-06 08:55:15.942991: step 7350, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 85h:21m:05s remains)
INFO - root - 2017-12-06 08:55:25.046682: step 7360, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 83h:48m:25s remains)
INFO - root - 2017-12-06 08:55:34.300566: step 7370, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 83h:15m:06s remains)
INFO - root - 2017-12-06 08:55:43.389387: step 7380, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.980 sec/batch; 88h:28m:55s remains)
INFO - root - 2017-12-06 08:55:52.682796: step 7390, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 84h:13m:49s remains)
INFO - root - 2017-12-06 08:56:01.813040: step 7400, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 83h:20m:58s remains)
2017-12-06 08:56:02.442524: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1379457 -4.1482453 -4.1535521 -4.156683 -4.1535921 -4.1395617 -4.131403 -4.1464949 -4.1758609 -4.1997185 -4.215374 -4.2224574 -4.2205763 -4.2210317 -4.222312][-4.1166549 -4.1362429 -4.1494079 -4.1550016 -4.1473951 -4.130475 -4.1183352 -4.133038 -4.1662688 -4.1910477 -4.2072959 -4.215096 -4.2178454 -4.2232852 -4.2229147][-4.0951171 -4.1214323 -4.1413751 -4.1486473 -4.1381264 -4.1180768 -4.1020646 -4.1090779 -4.1365113 -4.16126 -4.1793828 -4.1920767 -4.2027292 -4.2101955 -4.2031946][-4.0963459 -4.1284547 -4.156703 -4.1667767 -4.1549182 -4.1288686 -4.104671 -4.0963354 -4.112175 -4.1386552 -4.1625185 -4.1850595 -4.2027245 -4.2036643 -4.1816053][-4.1227608 -4.1569023 -4.1863046 -4.1943583 -4.1770773 -4.1431518 -4.102325 -4.0735373 -4.0783815 -4.1127558 -4.1525049 -4.1860886 -4.2076721 -4.2006726 -4.1667571][-4.1520748 -4.1798563 -4.1995559 -4.2021461 -4.1796031 -4.1353083 -4.0742211 -4.0253973 -4.028173 -4.07881 -4.1410069 -4.1906748 -4.212883 -4.201654 -4.1659379][-4.1704197 -4.189415 -4.201189 -4.1993842 -4.1725459 -4.1168289 -4.0326195 -3.9652269 -3.9755898 -4.0480089 -4.1335068 -4.194447 -4.2149372 -4.2018094 -4.1658192][-4.1735368 -4.1888471 -4.2002444 -4.2010813 -4.1712189 -4.1022439 -3.9987998 -3.9204843 -3.9462626 -4.0344739 -4.1324382 -4.2013288 -4.2224255 -4.2101135 -4.173574][-4.186686 -4.2048268 -4.2217727 -4.2264524 -4.1949406 -4.1217179 -4.0191216 -3.9521744 -3.991797 -4.0749779 -4.1611013 -4.223599 -4.2408743 -4.22714 -4.1952105][-4.2114897 -4.2332592 -4.2555923 -4.262064 -4.2333059 -4.1682634 -4.0878797 -4.0456142 -4.085659 -4.1471424 -4.2065535 -4.2506852 -4.2576032 -4.2413974 -4.2156382][-4.2394223 -4.2615061 -4.2825489 -4.2858529 -4.2593913 -4.2083778 -4.1543503 -4.1354613 -4.1739264 -4.2198582 -4.2563577 -4.2807727 -4.2756824 -4.2529221 -4.2260494][-4.2635207 -4.28464 -4.3009911 -4.3002281 -4.2781506 -4.2421441 -4.2075324 -4.2018948 -4.2356324 -4.267735 -4.2895384 -4.3005705 -4.28665 -4.258265 -4.2314491][-4.27989 -4.2967334 -4.3073959 -4.302053 -4.2836084 -4.2589121 -4.2362313 -4.2359023 -4.2612615 -4.2824187 -4.2934871 -4.2931919 -4.27443 -4.2516823 -4.23445][-4.2939816 -4.3038511 -4.3073258 -4.2986526 -4.2841997 -4.2673035 -4.2531805 -4.2547441 -4.2707253 -4.2816567 -4.2849665 -4.2787547 -4.2635832 -4.2513185 -4.2453184][-4.313026 -4.3147206 -4.31243 -4.3049264 -4.2965608 -4.2875066 -4.2804308 -4.2820168 -4.2901783 -4.2956786 -4.296618 -4.2915182 -4.2826591 -4.2777257 -4.2779756]]...]
INFO - root - 2017-12-06 08:56:11.658005: step 7410, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.925 sec/batch; 83h:29m:31s remains)
INFO - root - 2017-12-06 08:56:21.033767: step 7420, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.987 sec/batch; 89h:07m:35s remains)
INFO - root - 2017-12-06 08:56:30.373093: step 7430, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.964 sec/batch; 87h:03m:09s remains)
INFO - root - 2017-12-06 08:56:39.585238: step 7440, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.824 sec/batch; 74h:25m:58s remains)
INFO - root - 2017-12-06 08:56:48.914844: step 7450, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.892 sec/batch; 80h:33m:17s remains)
INFO - root - 2017-12-06 08:56:58.120950: step 7460, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 83h:52m:49s remains)
INFO - root - 2017-12-06 08:57:07.279982: step 7470, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 79h:37m:00s remains)
INFO - root - 2017-12-06 08:57:16.464473: step 7480, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 86h:14m:09s remains)
INFO - root - 2017-12-06 08:57:25.592609: step 7490, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 84h:01m:58s remains)
INFO - root - 2017-12-06 08:57:34.908102: step 7500, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.939 sec/batch; 84h:47m:24s remains)
2017-12-06 08:57:35.503169: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2369604 -4.2150283 -4.2048469 -4.1926012 -4.1609955 -4.1294217 -4.1180525 -4.1516576 -4.2237406 -4.2783542 -4.3023539 -4.3094444 -4.2924786 -4.2612944 -4.2138915][-4.1949182 -4.1752243 -4.1717567 -4.1612086 -4.1189294 -4.074245 -4.05745 -4.1035161 -4.1964369 -4.2649841 -4.2962675 -4.3038821 -4.2878852 -4.2602096 -4.2139778][-4.1457129 -4.1360612 -4.147141 -4.1477785 -4.1016188 -4.0496583 -4.0276923 -4.071887 -4.1730576 -4.2507486 -4.281281 -4.2832336 -4.2698393 -4.25389 -4.2184024][-4.1320605 -4.1295443 -4.1497445 -4.1599841 -4.1236806 -4.0734625 -4.0493593 -4.0777874 -4.1628184 -4.2324119 -4.2578883 -4.25418 -4.2424283 -4.2402048 -4.2183037][-4.1556373 -4.1553645 -4.1762872 -4.1873584 -4.1606483 -4.114316 -4.0819788 -4.0839186 -4.1409683 -4.1980157 -4.2165065 -4.2145486 -4.2106771 -4.2199378 -4.2137079][-4.1817088 -4.1727333 -4.1869035 -4.19892 -4.1746387 -4.1240215 -4.0697012 -4.0492425 -4.0879397 -4.1436543 -4.1723213 -4.1814127 -4.1878524 -4.2002416 -4.200316][-4.1802826 -4.1563139 -4.1628804 -4.1777096 -4.1502738 -4.0860496 -4.005712 -3.9669795 -4.0112319 -4.08595 -4.1373305 -4.1589918 -4.1749024 -4.1888995 -4.190805][-4.1695752 -4.1320062 -4.1318588 -4.15292 -4.1293664 -4.0648217 -3.970922 -3.9177856 -3.9734118 -4.0741591 -4.1443343 -4.1743455 -4.1913886 -4.1963921 -4.1890469][-4.1783266 -4.139339 -4.1375556 -4.1675043 -4.1555872 -4.096127 -4.010705 -3.9631507 -4.0162864 -4.1108794 -4.1760845 -4.2042074 -4.2145567 -4.2114305 -4.1961765][-4.1962795 -4.1651778 -4.1667376 -4.2012596 -4.1980805 -4.1471176 -4.0836248 -4.0523849 -4.092104 -4.1607165 -4.2068477 -4.2188873 -4.2192278 -4.2135239 -4.2050552][-4.2310352 -4.2050171 -4.2068915 -4.2342491 -4.229908 -4.186677 -4.1452608 -4.1340923 -4.16535 -4.2130089 -4.2352967 -4.2322822 -4.2250342 -4.2205029 -4.2185369][-4.2754893 -4.2553806 -4.2564745 -4.2698503 -4.2580037 -4.2226448 -4.1954112 -4.1945577 -4.2199254 -4.2536454 -4.2613349 -4.2517848 -4.243742 -4.2380409 -4.24032][-4.3014936 -4.2915878 -4.29382 -4.29826 -4.2831297 -4.2550721 -4.2341013 -4.2355914 -4.2513976 -4.2732735 -4.2816491 -4.2757745 -4.2713761 -4.2655807 -4.2664161][-4.3067989 -4.3066306 -4.3095431 -4.31095 -4.2954683 -4.2745209 -4.2618175 -4.2600207 -4.2687011 -4.2862272 -4.2971063 -4.2938871 -4.2874465 -4.2848687 -4.2881269][-4.3137612 -4.3154907 -4.3160586 -4.3148637 -4.3031387 -4.2907243 -4.282167 -4.2787256 -4.2822261 -4.2962666 -4.3061247 -4.3017273 -4.2971907 -4.2982888 -4.3027663]]...]
INFO - root - 2017-12-06 08:57:44.615485: step 7510, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.914 sec/batch; 82h:33m:02s remains)
INFO - root - 2017-12-06 08:57:53.824996: step 7520, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.952 sec/batch; 85h:57m:49s remains)
INFO - root - 2017-12-06 08:58:03.205306: step 7530, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 84h:18m:08s remains)
INFO - root - 2017-12-06 08:58:12.369836: step 7540, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 77h:31m:03s remains)
INFO - root - 2017-12-06 08:58:21.463031: step 7550, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 82h:58m:48s remains)
INFO - root - 2017-12-06 08:58:30.826951: step 7560, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 86h:47m:20s remains)
INFO - root - 2017-12-06 08:58:40.013795: step 7570, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.939 sec/batch; 84h:45m:32s remains)
INFO - root - 2017-12-06 08:58:49.170171: step 7580, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 83h:57m:26s remains)
INFO - root - 2017-12-06 08:58:58.442246: step 7590, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.986 sec/batch; 88h:56m:45s remains)
INFO - root - 2017-12-06 08:59:07.637460: step 7600, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.920 sec/batch; 83h:04m:16s remains)
2017-12-06 08:59:08.329127: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3347726 -4.29412 -4.24351 -4.2037392 -4.1668124 -4.1513295 -4.1826944 -4.2225761 -4.2446151 -4.260592 -4.2821455 -4.3001256 -4.2941427 -4.2728562 -4.2377682][-4.3443608 -4.3011613 -4.2461438 -4.1982713 -4.15362 -4.1316385 -4.1598926 -4.1946254 -4.210484 -4.22599 -4.2580686 -4.2901773 -4.2926264 -4.27225 -4.2333813][-4.3555679 -4.3153191 -4.26227 -4.2106113 -4.1572828 -4.120944 -4.134407 -4.1587348 -4.1638737 -4.1761508 -4.2189083 -4.268281 -4.2862525 -4.275198 -4.2377515][-4.3641806 -4.33127 -4.2868958 -4.2408733 -4.1821895 -4.1308956 -4.1289163 -4.1468821 -4.1421165 -4.1494584 -4.1976194 -4.253686 -4.2826438 -4.2796817 -4.2493248][-4.3632574 -4.3345418 -4.2978506 -4.2623682 -4.2038345 -4.1467423 -4.1414795 -4.1622114 -4.1544714 -4.1493087 -4.1857438 -4.2369642 -4.2705173 -4.2810516 -4.2663617][-4.3559828 -4.3273063 -4.2964511 -4.2644043 -4.1979179 -4.1353779 -4.131361 -4.158402 -4.1508608 -4.129673 -4.1440153 -4.1888437 -4.2359309 -4.2697563 -4.27752][-4.3441186 -4.3137546 -4.28313 -4.2452865 -4.1622386 -4.0879602 -4.0841503 -4.1157985 -4.1029482 -4.0581646 -4.0532994 -4.1045036 -4.176096 -4.2379 -4.2689118][-4.3350539 -4.3041434 -4.2718806 -4.2257385 -4.1301289 -4.0425034 -4.0368118 -4.0722318 -4.057333 -3.9950237 -3.9780912 -4.0405426 -4.1321387 -4.2121515 -4.2588987][-4.3371844 -4.3087215 -4.276628 -4.2312484 -4.1413822 -4.0551162 -4.0503039 -4.090445 -4.08037 -4.0188379 -3.9939528 -4.0424604 -4.1257315 -4.2022486 -4.2498879][-4.3443675 -4.3235812 -4.2949777 -4.2566657 -4.1873517 -4.118432 -4.1190243 -4.1607943 -4.1584244 -4.1123309 -4.0829496 -4.099308 -4.1523972 -4.2107649 -4.2475028][-4.3542948 -4.3386292 -4.3142905 -4.2870355 -4.2435703 -4.2008829 -4.2086272 -4.2453761 -4.2464747 -4.214921 -4.1875529 -4.1837964 -4.2090983 -4.2433181 -4.2629275][-4.360095 -4.3483343 -4.3262138 -4.3026428 -4.2718592 -4.2473269 -4.2614484 -4.295918 -4.3022151 -4.2863588 -4.269042 -4.261704 -4.271378 -4.2902231 -4.299818][-4.3553677 -4.3475509 -4.3282051 -4.3042893 -4.2746854 -4.2578111 -4.2770271 -4.311482 -4.3206239 -4.3133049 -4.3033013 -4.2997475 -4.3075562 -4.3211894 -4.3297739][-4.3439426 -4.3364162 -4.3188291 -4.293808 -4.2625046 -4.2470884 -4.2685237 -4.3025451 -4.3126121 -4.3066225 -4.2988091 -4.29828 -4.3076625 -4.3217549 -4.33276][-4.3414955 -4.3335729 -4.3146954 -4.2874513 -4.2573509 -4.2461371 -4.2680907 -4.2984529 -4.3076506 -4.301753 -4.2954059 -4.2962861 -4.3056469 -4.318346 -4.3277373]]...]
INFO - root - 2017-12-06 08:59:17.497282: step 7610, loss = 2.03, batch loss = 1.97 (8.9 examples/sec; 0.900 sec/batch; 81h:14m:49s remains)
INFO - root - 2017-12-06 08:59:26.501272: step 7620, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 81h:40m:53s remains)
INFO - root - 2017-12-06 08:59:35.750344: step 7630, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 82h:20m:14s remains)
INFO - root - 2017-12-06 08:59:44.772893: step 7640, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 83h:38m:07s remains)
INFO - root - 2017-12-06 08:59:53.887625: step 7650, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 80h:56m:03s remains)
INFO - root - 2017-12-06 09:00:03.089012: step 7660, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 82h:36m:12s remains)
INFO - root - 2017-12-06 09:00:12.188298: step 7670, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.939 sec/batch; 84h:43m:48s remains)
INFO - root - 2017-12-06 09:00:21.311891: step 7680, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 82h:55m:38s remains)
INFO - root - 2017-12-06 09:00:30.432905: step 7690, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.914 sec/batch; 82h:29m:31s remains)
INFO - root - 2017-12-06 09:00:39.747254: step 7700, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.910 sec/batch; 82h:07m:37s remains)
2017-12-06 09:00:40.383041: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.16605 -4.1698637 -4.1809897 -4.187026 -4.1936393 -4.1996741 -4.1940222 -4.1851854 -4.1769509 -4.1790738 -4.1908293 -4.2145877 -4.2320433 -4.2330728 -4.2272596][-4.1888504 -4.1805406 -4.1796246 -4.1781278 -4.1827569 -4.1888566 -4.1875777 -4.1903362 -4.192318 -4.1938329 -4.20058 -4.219306 -4.2375193 -4.2406616 -4.2355251][-4.2212334 -4.2056084 -4.1948047 -4.1861582 -4.1822028 -4.1757388 -4.1692362 -4.1801767 -4.1937575 -4.2004833 -4.2059703 -4.2184191 -4.2341213 -4.2413893 -4.2434812][-4.2472825 -4.2312751 -4.2223077 -4.2104206 -4.1939812 -4.1681108 -4.1499162 -4.1621833 -4.1841445 -4.1997013 -4.208179 -4.2149615 -4.2234735 -4.2312145 -4.2415872][-4.2481256 -4.2420011 -4.2445645 -4.23342 -4.2021213 -4.1550651 -4.121016 -4.1287346 -4.1598492 -4.1927953 -4.2118731 -4.217689 -4.21699 -4.216536 -4.2248287][-4.2272229 -4.23066 -4.2415071 -4.2280068 -4.1811943 -4.1121664 -4.0575962 -4.0580392 -4.1087246 -4.1693192 -4.2088747 -4.2205 -4.2146916 -4.2027311 -4.2017217][-4.2071581 -4.205368 -4.2129984 -4.1961904 -4.1412125 -4.0586514 -3.9872575 -3.9792995 -4.054985 -4.1453238 -4.2046928 -4.2262936 -4.22011 -4.2025971 -4.1939631][-4.2087975 -4.1925669 -4.1873379 -4.1712918 -4.1222911 -4.0425463 -3.9607058 -3.9394717 -4.0242081 -4.1298676 -4.2032471 -4.2364168 -4.2368526 -4.2207446 -4.2068214][-4.22893 -4.2017303 -4.1839094 -4.1717434 -4.1407194 -4.0832272 -4.0178714 -3.99155 -4.0517197 -4.1404505 -4.2116017 -4.25163 -4.2616749 -4.2505774 -4.2348428][-4.2433143 -4.2160921 -4.1941428 -4.1876769 -4.1825047 -4.1588869 -4.1293273 -4.113215 -4.1387138 -4.1835485 -4.2292738 -4.2665052 -4.2828922 -4.2723007 -4.2525053][-4.2384582 -4.2211223 -4.2036366 -4.20276 -4.2180886 -4.2225661 -4.2203479 -4.2153983 -4.2206197 -4.2263293 -4.2397628 -4.2653317 -4.2837224 -4.2773161 -4.2575035][-4.2285638 -4.2261653 -4.2179937 -4.2174215 -4.2361822 -4.2515459 -4.2628665 -4.2650633 -4.2620716 -4.2482438 -4.2380147 -4.2461348 -4.2597728 -4.2566895 -4.2390685][-4.22614 -4.2332578 -4.2342606 -4.2331572 -4.2470031 -4.2604332 -4.2717581 -4.2713327 -4.2591968 -4.2387023 -4.2231503 -4.2232132 -4.2298579 -4.2271776 -4.2146091][-4.2175069 -4.2271442 -4.2369614 -4.2404122 -4.2511034 -4.2605982 -4.2663121 -4.25928 -4.2369261 -4.21132 -4.1978211 -4.2024055 -4.2120953 -4.2146864 -4.2124085][-4.228168 -4.2328205 -4.2391205 -4.2402172 -4.2461185 -4.251689 -4.2515759 -4.2436829 -4.2214365 -4.1978774 -4.1915779 -4.205781 -4.2241077 -4.233129 -4.2354021]]...]
INFO - root - 2017-12-06 09:00:49.744451: step 7710, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.980 sec/batch; 88h:23m:29s remains)
INFO - root - 2017-12-06 09:00:59.011269: step 7720, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 82h:04m:08s remains)
INFO - root - 2017-12-06 09:01:08.361006: step 7730, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 85h:44m:02s remains)
INFO - root - 2017-12-06 09:01:17.642354: step 7740, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.922 sec/batch; 83h:11m:00s remains)
INFO - root - 2017-12-06 09:01:26.847202: step 7750, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 84h:13m:17s remains)
INFO - root - 2017-12-06 09:01:35.912870: step 7760, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.885 sec/batch; 79h:49m:42s remains)
INFO - root - 2017-12-06 09:01:45.108884: step 7770, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 82h:37m:32s remains)
INFO - root - 2017-12-06 09:01:54.389199: step 7780, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 83h:59m:50s remains)
INFO - root - 2017-12-06 09:02:03.545600: step 7790, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.904 sec/batch; 81h:33m:02s remains)
INFO - root - 2017-12-06 09:02:12.717054: step 7800, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 85h:03m:25s remains)
2017-12-06 09:02:13.361174: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.180315 -4.1740432 -4.1850462 -4.2062445 -4.2253551 -4.2406034 -4.25924 -4.2799034 -4.2892132 -4.2887959 -4.2844977 -4.280302 -4.2788048 -4.2772536 -4.2708597][-4.1892757 -4.1858277 -4.1977549 -4.2196732 -4.2373981 -4.2483735 -4.2642713 -4.2845192 -4.2966976 -4.3005548 -4.3005214 -4.2984433 -4.2980051 -4.2977519 -4.2910585][-4.2007756 -4.1945496 -4.2004409 -4.2148824 -4.2292848 -4.2350664 -4.2470865 -4.2720366 -4.2921166 -4.3039737 -4.3109574 -4.3117294 -4.311924 -4.3126183 -4.3069577][-4.2025981 -4.1850805 -4.172667 -4.1666389 -4.168438 -4.1629834 -4.1686025 -4.2031054 -4.2359338 -4.2580481 -4.2762752 -4.2854924 -4.2908869 -4.2979035 -4.2992334][-4.204196 -4.1699181 -4.131978 -4.0998473 -4.0819592 -4.0548921 -4.0428705 -4.0920863 -4.147377 -4.1819954 -4.2106662 -4.2303519 -4.2447143 -4.2598658 -4.2716393][-4.2039289 -4.1495762 -4.0846949 -4.025661 -3.9797206 -3.9161437 -3.8723106 -3.9384787 -4.0265832 -4.0790305 -4.1146479 -4.14073 -4.1667204 -4.1990433 -4.2265935][-4.194469 -4.1245475 -4.0407186 -3.9660723 -3.8971181 -3.7850859 -3.6886778 -3.7724738 -3.8989322 -3.967823 -4.0115671 -4.0460181 -4.08508 -4.1347456 -4.1741824][-4.1959944 -4.1342654 -4.0613403 -3.9992318 -3.9336994 -3.81076 -3.6902258 -3.7521491 -3.8728776 -3.9361792 -3.9764297 -4.0068436 -4.0433626 -4.0940504 -4.134819][-4.2158222 -4.1815987 -4.1375432 -4.1073179 -4.0721216 -3.991709 -3.9081433 -3.9328828 -3.99647 -4.0214868 -4.0374389 -4.0471826 -4.0669088 -4.1061034 -4.1380224][-4.2367177 -4.2264414 -4.2096295 -4.2000651 -4.18879 -4.1498685 -4.1040826 -4.1111712 -4.1385851 -4.1359944 -4.1302123 -4.1220622 -4.1251168 -4.15093 -4.1722507][-4.2591729 -4.2639775 -4.2635326 -4.2631721 -4.2615027 -4.2468157 -4.2270236 -4.2251306 -4.2317247 -4.2183819 -4.20424 -4.1917939 -4.1899986 -4.2055497 -4.2159643][-4.2855921 -4.2948718 -4.3012133 -4.3038936 -4.3050079 -4.2987304 -4.2902622 -4.290524 -4.291328 -4.2787914 -4.26335 -4.2528253 -4.2493272 -4.257082 -4.2605739][-4.305954 -4.316195 -4.3235612 -4.3272858 -4.3304129 -4.3284411 -4.3247085 -4.3266468 -4.3289571 -4.3237662 -4.3126493 -4.3042521 -4.2976403 -4.2966185 -4.2953582][-4.3174253 -4.3244596 -4.3317685 -4.3366313 -4.3410716 -4.3422565 -4.3409238 -4.342463 -4.3446913 -4.3438888 -4.3392091 -4.3334656 -4.3252029 -4.3188252 -4.3136277][-4.3221154 -4.3247495 -4.3289428 -4.3318987 -4.3342018 -4.3347158 -4.3334074 -4.3340373 -4.3362474 -4.3367243 -4.3363748 -4.3342276 -4.3282475 -4.3217273 -4.3163767]]...]
INFO - root - 2017-12-06 09:02:22.559508: step 7810, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 81h:02m:31s remains)
INFO - root - 2017-12-06 09:02:31.669327: step 7820, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 81h:11m:59s remains)
INFO - root - 2017-12-06 09:02:40.648809: step 7830, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 80h:00m:25s remains)
INFO - root - 2017-12-06 09:02:49.702244: step 7840, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.893 sec/batch; 80h:34m:30s remains)
INFO - root - 2017-12-06 09:02:58.945661: step 7850, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 84h:57m:48s remains)
INFO - root - 2017-12-06 09:03:08.118998: step 7860, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.915 sec/batch; 82h:32m:39s remains)
INFO - root - 2017-12-06 09:03:17.369753: step 7870, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 87h:35m:24s remains)
INFO - root - 2017-12-06 09:03:26.633718: step 7880, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 83h:26m:38s remains)
INFO - root - 2017-12-06 09:03:35.853029: step 7890, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.891 sec/batch; 80h:22m:40s remains)
INFO - root - 2017-12-06 09:03:45.020745: step 7900, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 81h:46m:38s remains)
2017-12-06 09:03:45.706410: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3035369 -4.297267 -4.2969027 -4.2966881 -4.29598 -4.2942653 -4.2939615 -4.2918205 -4.2884607 -4.2870746 -4.2923689 -4.2966604 -4.2972589 -4.2965946 -4.3005157][-4.2865434 -4.2787285 -4.278389 -4.2754407 -4.2716489 -4.2677917 -4.2625527 -4.2528796 -4.2418084 -4.2387271 -4.2488122 -4.2561855 -4.257628 -4.2573261 -4.2648969][-4.2595081 -4.2441893 -4.240418 -4.2350802 -4.2282314 -4.2200708 -4.2058792 -4.1875148 -4.1707454 -4.1691647 -4.1889482 -4.2078056 -4.2175331 -4.2199244 -4.2318373][-4.2084541 -4.1802516 -4.1730266 -4.16339 -4.1512313 -4.1374364 -4.1099911 -4.0802927 -4.0655293 -4.0739861 -4.1095171 -4.1462278 -4.1713347 -4.1816654 -4.199944][-4.1474748 -4.1039658 -4.086246 -4.07003 -4.0488248 -4.0251722 -3.9808061 -3.9459548 -3.9550309 -3.9897954 -4.0439353 -4.0926313 -4.1280694 -4.1488996 -4.1722431][-4.1023927 -4.0391059 -4.0006809 -3.9708941 -3.9297073 -3.888231 -3.8119044 -3.7733374 -3.8306751 -3.9103808 -3.9920347 -4.04635 -4.0807548 -4.1113863 -4.1423178][-4.0942688 -4.0170078 -3.9577124 -3.9114947 -3.8523669 -3.7878044 -3.6714842 -3.6254807 -3.7251656 -3.843575 -3.9472532 -4.0000229 -4.026556 -4.061831 -4.1057491][-4.1230025 -4.0578957 -4.0021534 -3.958075 -3.9022954 -3.8467498 -3.7494111 -3.7105606 -3.7861032 -3.8837185 -3.9694107 -3.9985878 -4.0047593 -4.0291367 -4.075861][-4.1478605 -4.1057982 -4.0692167 -4.0371165 -4.0000248 -3.9654093 -3.9113443 -3.890095 -3.9301198 -3.9897711 -4.0418396 -4.0499258 -4.0411348 -4.0509958 -4.0860114][-4.1686707 -4.1395788 -4.11605 -4.0963349 -4.0770493 -4.0578518 -4.02885 -4.019196 -4.0399551 -4.0727186 -4.0992641 -4.0998688 -4.0886889 -4.0914884 -4.1171584][-4.1980877 -4.1777844 -4.162941 -4.1510916 -4.1424789 -4.1332884 -4.1167703 -4.1108551 -4.1156373 -4.1286097 -4.1392145 -4.1401548 -4.13617 -4.13985 -4.1607442][-4.2233849 -4.2087154 -4.2001433 -4.1951523 -4.1925006 -4.1892867 -4.1819491 -4.1813378 -4.1812229 -4.1841283 -4.1862307 -4.1880374 -4.1904445 -4.1953578 -4.2115269][-4.2487183 -4.2407427 -4.236896 -4.2364058 -4.2373328 -4.235683 -4.2328968 -4.2345757 -4.2349429 -4.2342787 -4.2341909 -4.236012 -4.2388487 -4.2430673 -4.254746][-4.2742867 -4.2683372 -4.2662725 -4.2692513 -4.2725773 -4.2722197 -4.2709155 -4.272119 -4.2727141 -4.2731404 -4.2748737 -4.2772431 -4.2803483 -4.2833662 -4.2894225][-4.2979851 -4.294302 -4.293714 -4.2962222 -4.2980089 -4.2976966 -4.2971969 -4.2976108 -4.2979064 -4.29912 -4.3011513 -4.3032517 -4.3065639 -4.3100905 -4.3146367]]...]
INFO - root - 2017-12-06 09:03:54.930597: step 7910, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 83h:58m:37s remains)
INFO - root - 2017-12-06 09:04:04.115506: step 7920, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 84h:06m:53s remains)
INFO - root - 2017-12-06 09:04:13.357747: step 7930, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 84h:02m:27s remains)
INFO - root - 2017-12-06 09:04:22.472874: step 7940, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 83h:34m:03s remains)
INFO - root - 2017-12-06 09:04:31.573969: step 7950, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 78h:56m:05s remains)
INFO - root - 2017-12-06 09:04:40.863982: step 7960, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.954 sec/batch; 86h:01m:50s remains)
INFO - root - 2017-12-06 09:04:49.734133: step 7970, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 75h:34m:16s remains)
INFO - root - 2017-12-06 09:04:58.895762: step 7980, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.948 sec/batch; 85h:29m:46s remains)
INFO - root - 2017-12-06 09:05:08.049074: step 7990, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 83h:54m:52s remains)
INFO - root - 2017-12-06 09:05:17.362797: step 8000, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.936 sec/batch; 84h:21m:36s remains)
2017-12-06 09:05:18.007796: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1747394 -4.1743913 -4.1883464 -4.195682 -4.1983719 -4.20551 -4.2047873 -4.195837 -4.1874819 -4.1894603 -4.195951 -4.1936884 -4.1868429 -4.1806793 -4.1723218][-4.1852684 -4.1863585 -4.1996922 -4.2056403 -4.2052441 -4.2063522 -4.1981258 -4.1849012 -4.1721225 -4.170455 -4.1777434 -4.1761909 -4.1640577 -4.146008 -4.1257176][-4.210876 -4.2116346 -4.2221379 -4.2236724 -4.2154484 -4.2052379 -4.1868491 -4.1691933 -4.1535826 -4.1466856 -4.15007 -4.1458745 -4.1247292 -4.0906892 -4.056489][-4.2227988 -4.2161765 -4.2183304 -4.2099881 -4.1893473 -4.1631055 -4.1318445 -4.108253 -4.0932727 -4.08712 -4.0913954 -4.0891371 -4.0632753 -4.0199986 -3.9808836][-4.1995158 -4.1797957 -4.1691689 -4.1500921 -4.1134233 -4.0685015 -4.0271945 -4.002471 -3.9963856 -4.0083904 -4.0314779 -4.0414543 -4.0227876 -3.9908085 -3.9648809][-4.1391978 -4.1084595 -4.0881085 -4.0639272 -4.0180359 -3.9664838 -3.92961 -3.9163659 -3.9289837 -3.9658322 -4.0074725 -4.0271869 -4.02167 -4.0089364 -4.0013852][-4.0766039 -4.0481462 -4.031342 -4.0136595 -3.9748797 -3.9337795 -3.9150591 -3.9186888 -3.937768 -3.9738276 -4.0094266 -4.0223327 -4.0210247 -4.0193477 -4.0227809][-4.0660725 -4.0535913 -4.0501356 -4.0437646 -4.0181546 -3.9883757 -3.9797838 -3.9842966 -3.989392 -3.9995682 -4.0135107 -4.0157785 -4.0161238 -4.0215521 -4.0328269][-4.0956597 -4.096921 -4.105288 -4.1073632 -4.0909047 -4.0666852 -4.0573397 -4.0555172 -4.046279 -4.0340047 -4.0269823 -4.0202351 -4.0216684 -4.0356317 -4.0540137][-4.132237 -4.1400604 -4.1569777 -4.1673508 -4.1604013 -4.1413527 -4.1271162 -4.1179247 -4.0996122 -4.0794907 -4.0627241 -4.0480027 -4.0470729 -4.0640168 -4.0834045][-4.1687078 -4.1784124 -4.1981921 -4.2137232 -4.21494 -4.1995735 -4.1805296 -4.1615343 -4.1349134 -4.11354 -4.0944686 -4.074719 -4.0722919 -4.0851011 -4.0990787][-4.2046685 -4.2126141 -4.228847 -4.2416124 -4.2436833 -4.2286129 -4.2063513 -4.1799293 -4.150918 -4.1328826 -4.1158113 -4.0942612 -4.0882406 -4.0919657 -4.0985575][-4.231339 -4.2337565 -4.2416549 -4.248023 -4.2468309 -4.2320786 -4.2084851 -4.1813545 -4.1560392 -4.1440635 -4.1335874 -4.1156073 -4.105638 -4.10075 -4.1041379][-4.2417111 -4.2358155 -4.2356505 -4.2374244 -4.2362461 -4.2271304 -4.2090106 -4.1888185 -4.1705055 -4.161798 -4.1557326 -4.1419582 -4.1272497 -4.1146746 -4.1141386][-4.248951 -4.2393179 -4.2341185 -4.2312202 -4.2303543 -4.2293687 -4.2230029 -4.2135606 -4.2019176 -4.193244 -4.1846995 -4.1687994 -4.150034 -4.1318369 -4.1238232]]...]
INFO - root - 2017-12-06 09:05:27.181571: step 8010, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 81h:05m:45s remains)
INFO - root - 2017-12-06 09:05:36.408765: step 8020, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.914 sec/batch; 82h:20m:19s remains)
INFO - root - 2017-12-06 09:05:45.702332: step 8030, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.927 sec/batch; 83h:32m:16s remains)
INFO - root - 2017-12-06 09:05:54.590989: step 8040, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 80h:46m:59s remains)
INFO - root - 2017-12-06 09:06:03.748034: step 8050, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.940 sec/batch; 84h:40m:44s remains)
INFO - root - 2017-12-06 09:06:12.915283: step 8060, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.942 sec/batch; 84h:51m:20s remains)
INFO - root - 2017-12-06 09:06:22.029127: step 8070, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 84h:09m:18s remains)
INFO - root - 2017-12-06 09:06:31.198866: step 8080, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 83h:13m:47s remains)
INFO - root - 2017-12-06 09:06:40.489297: step 8090, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 83h:39m:18s remains)
INFO - root - 2017-12-06 09:06:49.575330: step 8100, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.899 sec/batch; 81h:02m:18s remains)
2017-12-06 09:06:50.178622: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3283095 -4.322175 -4.3136492 -4.2999616 -4.276927 -4.2589259 -4.2546773 -4.2522359 -4.2443542 -4.2493448 -4.2572269 -4.264039 -4.2779489 -4.2994232 -4.3174567][-4.3357053 -4.3265529 -4.3091679 -4.2838507 -4.2473845 -4.2185 -4.2079463 -4.2056255 -4.1940241 -4.2002916 -4.2122631 -4.2239518 -4.2423277 -4.2715244 -4.2992659][-4.3385286 -4.3234682 -4.2949934 -4.2569218 -4.2077537 -4.1668253 -4.150054 -4.1486759 -4.1352525 -4.1438885 -4.1604643 -4.1777558 -4.2003541 -4.23373 -4.2701259][-4.3333178 -4.3139119 -4.2753057 -4.2251983 -4.1613007 -4.1061931 -4.0871305 -4.0881963 -4.0763679 -4.0876937 -4.1070218 -4.1254678 -4.1501465 -4.1884604 -4.2343674][-4.3229871 -4.2976236 -4.2504869 -4.1877146 -4.1120038 -4.0434246 -4.02454 -4.0305476 -4.0195632 -4.0294919 -4.0417719 -4.0597796 -4.0929275 -4.1424952 -4.1994629][-4.3067217 -4.2738194 -4.2183404 -4.1501675 -4.0751157 -4.0110722 -3.9968295 -3.9995263 -3.9792848 -3.9699981 -3.9615779 -3.9786 -4.0308704 -4.1033549 -4.1745429][-4.2958088 -4.2611322 -4.2044525 -4.1370478 -4.0688343 -4.0151429 -4.0036316 -4.0017991 -3.9745238 -3.9440172 -3.9151871 -3.9296293 -3.9979582 -4.0857253 -4.1650038][-4.2914534 -4.2622094 -4.2098622 -4.1469574 -4.0866032 -4.0381455 -4.02737 -4.022 -3.9952438 -3.9561241 -3.9143062 -3.9272048 -4.0004773 -4.0935593 -4.1735549][-4.2956405 -4.2779174 -4.2375016 -4.1864061 -4.1376843 -4.0931177 -4.0783563 -4.0630264 -4.03974 -4.0028749 -3.9618464 -3.9773707 -4.0445805 -4.1316042 -4.2011805][-4.3065896 -4.3002729 -4.276947 -4.2430372 -4.2086649 -4.16914 -4.1471214 -4.1211772 -4.0995812 -4.0730977 -4.0392094 -4.0567527 -4.1125531 -4.1879892 -4.2426524][-4.3196211 -4.3216434 -4.312335 -4.294445 -4.2690272 -4.2341518 -4.2093263 -4.183 -4.1674228 -4.1536369 -4.1297417 -4.1436725 -4.188621 -4.2470779 -4.2828145][-4.3277035 -4.3357024 -4.3365803 -4.3256431 -4.3049049 -4.2775655 -4.2567368 -4.2366562 -4.2242913 -4.2182875 -4.2031221 -4.2146463 -4.2514057 -4.2933826 -4.3128147][-4.327672 -4.3386035 -4.3476825 -4.3439956 -4.3293872 -4.3106117 -4.2933187 -4.2790418 -4.2691078 -4.2659774 -4.2565064 -4.2647214 -4.2900963 -4.31831 -4.3296676][-4.3193679 -4.3286719 -4.3396039 -4.3410892 -4.3354917 -4.3266115 -4.3165536 -4.3076739 -4.3021483 -4.3019524 -4.2998705 -4.303494 -4.3159337 -4.3323345 -4.3393283][-4.315259 -4.3205757 -4.3279133 -4.331593 -4.332725 -4.33136 -4.3285708 -4.3255863 -4.3237033 -4.3248816 -4.3260236 -4.3267269 -4.3315215 -4.3398128 -4.3441396]]...]
INFO - root - 2017-12-06 09:06:59.158247: step 8110, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.876 sec/batch; 78h:53m:36s remains)
INFO - root - 2017-12-06 09:07:08.309442: step 8120, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.890 sec/batch; 80h:11m:26s remains)
INFO - root - 2017-12-06 09:07:17.463868: step 8130, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 79h:56m:04s remains)
INFO - root - 2017-12-06 09:07:26.644115: step 8140, loss = 2.03, batch loss = 1.97 (8.9 examples/sec; 0.903 sec/batch; 81h:21m:24s remains)
INFO - root - 2017-12-06 09:07:35.852202: step 8150, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 83h:42m:25s remains)
INFO - root - 2017-12-06 09:07:45.070400: step 8160, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 82h:12m:18s remains)
INFO - root - 2017-12-06 09:07:54.230909: step 8170, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.989 sec/batch; 89h:05m:53s remains)
INFO - root - 2017-12-06 09:08:03.291656: step 8180, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.819 sec/batch; 73h:47m:18s remains)
INFO - root - 2017-12-06 09:08:12.559077: step 8190, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 83h:42m:39s remains)
INFO - root - 2017-12-06 09:08:21.650234: step 8200, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 84h:26m:53s remains)
2017-12-06 09:08:22.306024: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3354278 -4.3331828 -4.3098559 -4.2706351 -4.2346206 -4.20286 -4.1782012 -4.2020621 -4.2291436 -4.2309918 -4.2251887 -4.2320743 -4.2514076 -4.2794375 -4.3062978][-4.3280106 -4.3183694 -4.2865806 -4.2389607 -4.1989565 -4.1667542 -4.1492095 -4.18573 -4.2248259 -4.23854 -4.2388272 -4.24538 -4.2589703 -4.2785645 -4.29701][-4.3179383 -4.3014965 -4.2631383 -4.210413 -4.164793 -4.1294236 -4.1195574 -4.1717634 -4.2233562 -4.2486696 -4.2573023 -4.2643833 -4.2717547 -4.2815251 -4.2906036][-4.3130612 -4.2946696 -4.2548337 -4.1993265 -4.1464591 -4.1045213 -4.0936713 -4.152338 -4.2134438 -4.2488775 -4.2666192 -4.2768154 -4.2828341 -4.28852 -4.2922606][-4.312335 -4.2959552 -4.2582583 -4.2006874 -4.1391158 -4.0838294 -4.0562305 -4.10341 -4.1655979 -4.213799 -4.2477918 -4.2702594 -4.2823753 -4.2894273 -4.2925663][-4.3141737 -4.3014574 -4.2655158 -4.2053123 -4.133347 -4.0532928 -3.987083 -4.0059042 -4.064744 -4.1286354 -4.1870241 -4.2323966 -4.2605276 -4.2769885 -4.2843885][-4.3169341 -4.3071327 -4.2708769 -4.2033968 -4.1149082 -4.0022569 -3.8882313 -3.8785517 -3.9380879 -4.0178862 -4.1015177 -4.1718779 -4.2194023 -4.2493787 -4.2647657][-4.3223934 -4.315763 -4.2781057 -4.2029548 -4.1002841 -3.9595523 -3.8116131 -3.7917833 -3.8577926 -3.942987 -4.0354633 -4.1151752 -4.1710739 -4.2069778 -4.2287531][-4.3196106 -4.3135633 -4.2792711 -4.2089672 -4.1150074 -3.9818687 -3.8425269 -3.8371542 -3.9087219 -3.978605 -4.04873 -4.1103816 -4.1539655 -4.1797309 -4.1993995][-4.3118711 -4.3075976 -4.2813549 -4.228857 -4.1621246 -4.0603 -3.9573505 -3.9666119 -4.0321455 -4.0815029 -4.1225796 -4.1564021 -4.1768975 -4.1832585 -4.1936417][-4.3059459 -4.3022361 -4.287364 -4.2577267 -4.2214766 -4.1542349 -4.08581 -4.0966334 -4.147017 -4.1766396 -4.1948829 -4.2084 -4.2118368 -4.2040234 -4.2055769][-4.304193 -4.2992043 -4.2917595 -4.2781229 -4.2631121 -4.2229185 -4.1793036 -4.1877747 -4.2212429 -4.2367258 -4.2414007 -4.2453322 -4.2419634 -4.2272487 -4.2226582][-4.3092322 -4.3019614 -4.2947316 -4.288053 -4.2840815 -4.2649055 -4.2389402 -4.246954 -4.269805 -4.2744036 -4.2687922 -4.2662878 -4.2594266 -4.2436471 -4.2380781][-4.3136048 -4.30375 -4.2905831 -4.2785511 -4.2759371 -4.2698083 -4.2565808 -4.2664838 -4.2849407 -4.2862816 -4.2776279 -4.2709308 -4.2628736 -4.2521448 -4.2499433][-4.3164406 -4.3050027 -4.2859907 -4.2632694 -4.2544937 -4.2478724 -4.2382808 -4.2536459 -4.2764354 -4.2851658 -4.2840343 -4.2792668 -4.2725177 -4.265605 -4.2638893]]...]
INFO - root - 2017-12-06 09:08:31.610585: step 8210, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 85h:54m:42s remains)
INFO - root - 2017-12-06 09:08:40.768776: step 8220, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 81h:20m:33s remains)
INFO - root - 2017-12-06 09:08:50.101308: step 8230, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 80h:46m:00s remains)
INFO - root - 2017-12-06 09:08:59.305892: step 8240, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 85h:10m:36s remains)
INFO - root - 2017-12-06 09:09:08.663909: step 8250, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 87h:29m:19s remains)
INFO - root - 2017-12-06 09:09:17.630016: step 8260, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.996 sec/batch; 89h:42m:40s remains)
INFO - root - 2017-12-06 09:09:26.723197: step 8270, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 85h:10m:24s remains)
INFO - root - 2017-12-06 09:09:35.902293: step 8280, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 84h:25m:36s remains)
INFO - root - 2017-12-06 09:09:45.144923: step 8290, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 82h:33m:59s remains)
INFO - root - 2017-12-06 09:09:54.341380: step 8300, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 84h:31m:03s remains)
2017-12-06 09:09:54.920301: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3597426 -4.3571725 -4.3535585 -4.350563 -4.3492336 -4.3490992 -4.3479266 -4.3445988 -4.3426819 -4.343863 -4.3463616 -4.34825 -4.3503561 -4.3532887 -4.3579979][-4.3538127 -4.3506851 -4.3460016 -4.3413811 -4.3382955 -4.3372316 -4.3355103 -4.3307095 -4.3291087 -4.331882 -4.3352261 -4.3368006 -4.3374853 -4.3388886 -4.3435497][-4.3462019 -4.341825 -4.3355908 -4.3291287 -4.3245435 -4.3221083 -4.3172073 -4.3088579 -4.3065829 -4.3116832 -4.3161244 -4.3171234 -4.3159318 -4.3149972 -4.3186693][-4.3353744 -4.328022 -4.3180728 -4.30761 -4.2999477 -4.2934608 -4.2797446 -4.2647796 -4.26069 -4.2683053 -4.2763414 -4.2780938 -4.2767391 -4.2754359 -4.2798276][-4.3141947 -4.2992344 -4.2826519 -4.2660742 -4.2539039 -4.2415819 -4.2149682 -4.1882644 -4.1811361 -4.1959815 -4.21186 -4.2175045 -4.2190027 -4.2213783 -4.2312665][-4.2787576 -4.2533793 -4.2294512 -4.20621 -4.1872282 -4.1679382 -4.127336 -4.0848908 -4.075469 -4.1057267 -4.1391563 -4.1534233 -4.1609249 -4.1688004 -4.1857319][-4.2359152 -4.2016158 -4.1724515 -4.1464081 -4.1222172 -4.0954094 -4.0392375 -3.9716167 -3.9589388 -4.0185261 -4.080471 -4.1090784 -4.1235504 -4.1354394 -4.157145][-4.2054338 -4.1671872 -4.1371541 -4.1077137 -4.0727153 -4.0346179 -3.9627607 -3.8694038 -3.8627477 -3.9629822 -4.056654 -4.0968418 -4.1113939 -4.1204042 -4.1444058][-4.2047968 -4.1714988 -4.1433287 -4.1086993 -4.0606413 -4.0111036 -3.9366424 -3.8448558 -3.8545909 -3.9723358 -4.0750794 -4.1155658 -4.1242943 -4.1282573 -4.1483822][-4.2276435 -4.2005854 -4.1711373 -4.1304855 -4.0794864 -4.0336995 -3.9792652 -3.9211278 -3.9407916 -4.0387211 -4.1236558 -4.1570373 -4.16201 -4.1616211 -4.1735191][-4.245235 -4.222507 -4.1929531 -4.154624 -4.1147285 -4.0814271 -4.0499439 -4.0210209 -4.0395055 -4.1052341 -4.1657352 -4.1945491 -4.2025971 -4.2023892 -4.2064633][-4.2620726 -4.2439337 -4.2204804 -4.1922178 -4.1674514 -4.1484222 -4.1329703 -4.1185679 -4.1248326 -4.1579247 -4.1958852 -4.2227097 -4.2370863 -4.2417521 -4.2409949][-4.29171 -4.27978 -4.2653351 -4.2493095 -4.2364841 -4.2275867 -4.2219381 -4.2137432 -4.2101741 -4.2192464 -4.2372293 -4.2564869 -4.2736397 -4.2829614 -4.2806683][-4.3210244 -4.3171978 -4.3133311 -4.3087893 -4.3052826 -4.3028278 -4.3027339 -4.2980194 -4.2888727 -4.2830091 -4.2845936 -4.2939062 -4.3087091 -4.3194242 -4.3187513][-4.3432493 -4.3428197 -4.3432693 -4.343349 -4.34461 -4.3459435 -4.3486247 -4.3459325 -4.3361659 -4.324944 -4.317481 -4.3201318 -4.3319521 -4.3438864 -4.3467669]]...]
INFO - root - 2017-12-06 09:10:04.241880: step 8310, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.959 sec/batch; 86h:20m:23s remains)
INFO - root - 2017-12-06 09:10:13.456372: step 8320, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 82h:06m:18s remains)
INFO - root - 2017-12-06 09:10:22.561422: step 8330, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 84h:53m:43s remains)
INFO - root - 2017-12-06 09:10:31.801103: step 8340, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 84h:33m:56s remains)
INFO - root - 2017-12-06 09:10:40.988494: step 8350, loss = 2.11, batch loss = 2.05 (8.6 examples/sec; 0.930 sec/batch; 83h:46m:07s remains)
INFO - root - 2017-12-06 09:10:50.329592: step 8360, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 82h:56m:27s remains)
INFO - root - 2017-12-06 09:10:59.558554: step 8370, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 82h:52m:37s remains)
INFO - root - 2017-12-06 09:11:08.753913: step 8380, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.955 sec/batch; 86h:00m:46s remains)
INFO - root - 2017-12-06 09:11:17.885399: step 8390, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 82h:27m:00s remains)
INFO - root - 2017-12-06 09:11:26.878296: step 8400, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.904 sec/batch; 81h:22m:21s remains)
2017-12-06 09:11:27.515670: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3638773 -4.3746161 -4.3747272 -4.3486462 -4.2908483 -4.2160883 -4.153214 -4.1241822 -4.140543 -4.1855679 -4.2388182 -4.2760997 -4.2947278 -4.2946868 -4.2871413][-4.37101 -4.3841076 -4.3911219 -4.3759503 -4.3321366 -4.2661047 -4.2013721 -4.1571369 -4.148653 -4.1706634 -4.2117028 -4.2518768 -4.2838507 -4.2971377 -4.29853][-4.3790846 -4.3948174 -4.4046021 -4.39441 -4.3581634 -4.2984858 -4.2314749 -4.1733866 -4.1409435 -4.139771 -4.1724734 -4.2196236 -4.2663617 -4.2967439 -4.3111572][-4.3848896 -4.4019136 -4.4124002 -4.4037046 -4.369163 -4.3109469 -4.2393813 -4.1709838 -4.1214619 -4.10105 -4.1282549 -4.1816983 -4.2422485 -4.2916322 -4.3224077][-4.38907 -4.4059105 -4.4138317 -4.40419 -4.3684378 -4.3095117 -4.2363205 -4.1608219 -4.0950193 -4.0548868 -4.0780454 -4.1373129 -4.20927 -4.274478 -4.3209066][-4.3909979 -4.4049063 -4.4067965 -4.3913226 -4.3505321 -4.2886138 -4.2127795 -4.1266322 -4.0422673 -3.9851079 -4.0105786 -4.081655 -4.1676755 -4.2476826 -4.3073359][-4.3883924 -4.3982158 -4.3921914 -4.365797 -4.3144279 -4.2449274 -4.1621242 -4.0615311 -3.9555106 -3.8861372 -3.9253483 -4.0164237 -4.1189814 -4.2133164 -4.2864037][-4.3819652 -4.3865285 -4.3715 -4.3316336 -4.2676744 -4.1906338 -4.1030469 -3.9951589 -3.877593 -3.804903 -3.860086 -3.9668314 -4.0784793 -4.1808443 -4.2631769][-4.3739147 -4.3709726 -4.3464031 -4.295907 -4.2250404 -4.1469932 -4.0643535 -3.965893 -3.8600471 -3.8032544 -3.8619857 -3.9624467 -4.067935 -4.1688943 -4.25316][-4.3651209 -4.3551264 -4.324676 -4.270597 -4.199944 -4.1269522 -4.0599527 -3.9881282 -3.9152019 -3.8887231 -3.9386294 -4.0163474 -4.103128 -4.1924205 -4.2679625][-4.3590631 -4.3460684 -4.3163476 -4.2666125 -4.2044482 -4.14264 -4.0934649 -4.0501904 -4.0112224 -4.0088243 -4.0455337 -4.100143 -4.167418 -4.2395711 -4.2985406][-4.3568263 -4.3459787 -4.3236475 -4.287147 -4.2404342 -4.1931176 -4.159122 -4.1374383 -4.1238813 -4.1327577 -4.1560616 -4.190794 -4.237236 -4.2886391 -4.32781][-4.3556213 -4.3490763 -4.3360834 -4.3149123 -4.2852378 -4.2537103 -4.2333708 -4.2261009 -4.2285051 -4.2405715 -4.2550559 -4.2754974 -4.302021 -4.3309393 -4.3474832][-4.3542376 -4.3514838 -4.3463135 -4.3375463 -4.3233891 -4.3072853 -4.2982392 -4.2992291 -4.3072772 -4.31725 -4.3265328 -4.3376489 -4.3488913 -4.3566027 -4.3532629][-4.3518229 -4.3514585 -4.3501997 -4.3483181 -4.34472 -4.3407431 -4.3400273 -4.3441381 -4.350884 -4.3566375 -4.3623743 -4.3667212 -4.3643932 -4.3563929 -4.3422651]]...]
INFO - root - 2017-12-06 09:11:36.683703: step 8410, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 82h:42m:03s remains)
INFO - root - 2017-12-06 09:11:45.797014: step 8420, loss = 2.03, batch loss = 1.98 (8.7 examples/sec; 0.922 sec/batch; 83h:01m:13s remains)
INFO - root - 2017-12-06 09:11:54.906038: step 8430, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 84h:26m:14s remains)
INFO - root - 2017-12-06 09:12:03.983386: step 8440, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 83h:18m:09s remains)
INFO - root - 2017-12-06 09:12:13.150421: step 8450, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.943 sec/batch; 84h:52m:50s remains)
INFO - root - 2017-12-06 09:12:22.243730: step 8460, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.854 sec/batch; 76h:49m:36s remains)
INFO - root - 2017-12-06 09:12:31.296779: step 8470, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.952 sec/batch; 85h:39m:04s remains)
INFO - root - 2017-12-06 09:12:40.486577: step 8480, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 83h:34m:12s remains)
INFO - root - 2017-12-06 09:12:49.676427: step 8490, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 82h:32m:31s remains)
INFO - root - 2017-12-06 09:12:58.839450: step 8500, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 83h:40m:33s remains)
2017-12-06 09:12:59.485568: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2420068 -4.2304225 -4.219944 -4.2185574 -4.2146759 -4.1987681 -4.190999 -4.1943636 -4.2140808 -4.2484202 -4.275929 -4.2888865 -4.2935338 -4.286099 -4.260087][-4.2269559 -4.2155447 -4.2078729 -4.2117314 -4.2104859 -4.2014751 -4.2011333 -4.2043962 -4.2180343 -4.2449389 -4.2648082 -4.2687426 -4.2654109 -4.2568955 -4.2347422][-4.2174273 -4.2061677 -4.2024679 -4.208612 -4.2084842 -4.2033291 -4.2055573 -4.20908 -4.219007 -4.2426929 -4.2548594 -4.248466 -4.2338591 -4.2195559 -4.2041335][-4.2244954 -4.2120614 -4.2063875 -4.2075453 -4.2027 -4.1942344 -4.1908183 -4.1913614 -4.2031808 -4.2289538 -4.2396331 -4.2290754 -4.2030277 -4.1820893 -4.1716752][-4.2284241 -4.21786 -4.2072463 -4.1962934 -4.1773591 -4.1539822 -4.1368561 -4.1336336 -4.1511011 -4.1845026 -4.2028561 -4.1986518 -4.1725421 -4.1501522 -4.1425729][-4.2205849 -4.2133694 -4.1945624 -4.1685076 -4.1327677 -4.090075 -4.0549974 -4.0456662 -4.0670218 -4.1109176 -4.1450186 -4.1561909 -4.1434741 -4.1250076 -4.1171732][-4.2031908 -4.1991115 -4.1716757 -4.130641 -4.0815659 -4.025342 -3.9789596 -3.9606836 -3.9768605 -4.0269046 -4.0791221 -4.1089892 -4.115016 -4.10903 -4.1064944][-4.1980071 -4.1883917 -4.1495504 -4.09716 -4.044416 -3.9909763 -3.9514368 -3.934293 -3.9452021 -3.9947536 -4.0514679 -4.0896764 -4.1052766 -4.1084852 -4.1126285][-4.2079897 -4.1880994 -4.1400633 -4.0846395 -4.03614 -3.9984107 -3.9808235 -3.9792132 -3.9917085 -4.035439 -4.0847392 -4.116189 -4.1277723 -4.1286554 -4.1315155][-4.230545 -4.2038283 -4.1542912 -4.1064997 -4.0711255 -4.0540504 -4.0603743 -4.0732131 -4.0864987 -4.1197486 -4.1513009 -4.1627836 -4.1541872 -4.14445 -4.1416774][-4.2485938 -4.2201748 -4.1782207 -4.1449051 -4.1228271 -4.1213045 -4.1398916 -4.1550417 -4.1645269 -4.185596 -4.2014866 -4.1875019 -4.151752 -4.1238842 -4.1164546][-4.2564545 -4.2317162 -4.2015944 -4.1810117 -4.1665826 -4.1706147 -4.1927915 -4.2067308 -4.2099242 -4.2176323 -4.2198219 -4.1857634 -4.1339855 -4.0965247 -4.0850081][-4.2599015 -4.2398081 -4.2167773 -4.2012219 -4.1875339 -4.1895847 -4.20629 -4.2155375 -4.2159357 -4.212811 -4.2021942 -4.1583233 -4.1072941 -4.0754218 -4.0720038][-4.2647405 -4.2498546 -4.2311807 -4.2128015 -4.1932082 -4.1863728 -4.18927 -4.190558 -4.1887412 -4.1829796 -4.1698689 -4.1287 -4.0889473 -4.0730557 -4.0817723][-4.2671051 -4.257894 -4.2441139 -4.2236681 -4.1960168 -4.1763315 -4.162282 -4.152873 -4.1521959 -4.1554108 -4.1533365 -4.1244278 -4.0976639 -4.0995049 -4.1202536]]...]
INFO - root - 2017-12-06 09:13:08.612826: step 8510, loss = 2.09, batch loss = 2.04 (8.8 examples/sec; 0.904 sec/batch; 81h:22m:48s remains)
INFO - root - 2017-12-06 09:13:17.867651: step 8520, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 81h:40m:16s remains)
INFO - root - 2017-12-06 09:13:27.062775: step 8530, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 84h:44m:25s remains)
INFO - root - 2017-12-06 09:13:36.089097: step 8540, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 81h:25m:14s remains)
INFO - root - 2017-12-06 09:13:45.371017: step 8550, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 83h:59m:52s remains)
INFO - root - 2017-12-06 09:13:54.601453: step 8560, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 84h:21m:52s remains)
INFO - root - 2017-12-06 09:14:03.801098: step 8570, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 83h:13m:57s remains)
INFO - root - 2017-12-06 09:14:12.993757: step 8580, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.909 sec/batch; 81h:47m:34s remains)
INFO - root - 2017-12-06 09:14:22.176765: step 8590, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.884 sec/batch; 79h:32m:33s remains)
INFO - root - 2017-12-06 09:14:31.478281: step 8600, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.930 sec/batch; 83h:41m:33s remains)
2017-12-06 09:14:32.137214: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2774625 -4.2750435 -4.2744327 -4.2745166 -4.2737708 -4.2710433 -4.2675591 -4.2655458 -4.2654724 -4.2664638 -4.2676525 -4.267704 -4.265748 -4.2600088 -4.2489195][-4.285831 -4.2843928 -4.2848773 -4.2865658 -4.2866654 -4.2821627 -4.2755075 -4.271214 -4.2710071 -4.2734733 -4.2759943 -4.2771897 -4.2751508 -4.2677631 -4.2546353][-4.2921681 -4.2902212 -4.2921944 -4.2966814 -4.2989106 -4.292995 -4.2820616 -4.2748947 -4.2760444 -4.2813506 -4.2863212 -4.2889452 -4.286067 -4.2780356 -4.2643089][-4.2810059 -4.277319 -4.2805376 -4.287303 -4.2909532 -4.2844453 -4.269618 -4.2605362 -4.2669783 -4.27888 -4.2872076 -4.2906132 -4.2868433 -4.27892 -4.2679448][-4.2559204 -4.2484431 -4.2518358 -4.2590108 -4.2596436 -4.2461462 -4.2242689 -4.2125726 -4.2277222 -4.2532434 -4.2689819 -4.2763686 -4.2744961 -4.2688479 -4.2624063][-4.20659 -4.19991 -4.2080932 -4.2196069 -4.2158928 -4.1878996 -4.1450496 -4.1168971 -4.137116 -4.183919 -4.2161074 -4.2328057 -4.2377429 -4.2367249 -4.2368369][-4.1526537 -4.1451263 -4.1570396 -4.1760941 -4.1713691 -4.1278772 -4.0545783 -3.9964824 -4.0153155 -4.0852046 -4.1365309 -4.16636 -4.1818781 -4.1869764 -4.1939969][-4.1398611 -4.1273236 -4.1341887 -4.1507773 -4.1436443 -4.0952682 -4.0146337 -3.9491827 -3.9655833 -4.0391712 -4.0967903 -4.1311622 -4.1485147 -4.1544852 -4.1627407][-4.1500678 -4.1358581 -4.1388812 -4.1517334 -4.1457968 -4.1127682 -4.0611229 -4.0244389 -4.0396457 -4.0899749 -4.1304464 -4.1532893 -4.1608377 -4.1571665 -4.1573467][-4.1629038 -4.1498175 -4.1501346 -4.1591539 -4.1539826 -4.1342216 -4.1099977 -4.0987854 -4.1125684 -4.14276 -4.167089 -4.1812458 -4.1825733 -4.1701293 -4.1587553][-4.1588531 -4.1541591 -4.1562943 -4.1637783 -4.16042 -4.1508965 -4.1423435 -4.14132 -4.1477675 -4.1653032 -4.1836348 -4.1977143 -4.2013888 -4.1883979 -4.1714749][-4.1579533 -4.1582379 -4.1633177 -4.1707754 -4.1698022 -4.1666942 -4.1656713 -4.1666431 -4.1675024 -4.1768456 -4.193336 -4.2084441 -4.2134261 -4.2025146 -4.18762][-4.1777787 -4.1782026 -4.18457 -4.1904035 -4.1918006 -4.1930156 -4.1921024 -4.1924 -4.1909871 -4.1956716 -4.2086062 -4.2207947 -4.2237353 -4.2167559 -4.2091818][-4.205122 -4.2043662 -4.2092018 -4.2122512 -4.2136455 -4.21536 -4.2140927 -4.213295 -4.2128062 -4.2153473 -4.2231603 -4.2308259 -4.2320228 -4.2274942 -4.2244177][-4.2347236 -4.2320876 -4.23335 -4.2334619 -4.2328582 -4.2321277 -4.2296257 -4.2268438 -4.2243838 -4.2225237 -4.225389 -4.22863 -4.2279663 -4.2246766 -4.2227721]]...]
INFO - root - 2017-12-06 09:14:41.178353: step 8610, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 82h:27m:03s remains)
INFO - root - 2017-12-06 09:14:50.377397: step 8620, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.953 sec/batch; 85h:42m:33s remains)
INFO - root - 2017-12-06 09:14:59.527266: step 8630, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 82h:34m:10s remains)
INFO - root - 2017-12-06 09:15:08.611698: step 8640, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.932 sec/batch; 83h:52m:03s remains)
INFO - root - 2017-12-06 09:15:17.918652: step 8650, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.927 sec/batch; 83h:25m:24s remains)
INFO - root - 2017-12-06 09:15:27.178750: step 8660, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 82h:28m:16s remains)
INFO - root - 2017-12-06 09:15:36.377745: step 8670, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.949 sec/batch; 85h:20m:29s remains)
INFO - root - 2017-12-06 09:15:45.587003: step 8680, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.892 sec/batch; 80h:14m:44s remains)
INFO - root - 2017-12-06 09:15:54.835158: step 8690, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 86h:19m:29s remains)
INFO - root - 2017-12-06 09:16:04.074304: step 8700, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.953 sec/batch; 85h:44m:43s remains)
2017-12-06 09:16:04.655164: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2523193 -4.2514319 -4.2514019 -4.2516413 -4.248785 -4.2459025 -4.2463126 -4.2482572 -4.2506413 -4.2526546 -4.2542586 -4.25549 -4.2560554 -4.2568803 -4.2582755][-4.2390761 -4.2370443 -4.2362342 -4.2349758 -4.2305593 -4.2277455 -4.2288814 -4.2316108 -4.2347512 -4.2377439 -4.2395558 -4.242198 -4.2453747 -4.2492795 -4.2534618][-4.2211952 -4.2177863 -4.2151203 -4.2110977 -4.2041569 -4.2015095 -4.2033172 -4.2073851 -4.2132487 -4.2182183 -4.2208953 -4.2252479 -4.2313385 -4.2390194 -4.2463984][-4.2019811 -4.1961317 -4.1886349 -4.1782279 -4.1683884 -4.1641822 -4.1636539 -4.167635 -4.1780863 -4.188036 -4.1938014 -4.2029 -4.216145 -4.2288179 -4.23834][-4.18647 -4.1762338 -4.161983 -4.1407185 -4.1211815 -4.1080813 -4.0987663 -4.1004386 -4.1155562 -4.1362805 -4.1537528 -4.1756711 -4.2015986 -4.22328 -4.2361984][-4.178103 -4.158124 -4.1337132 -4.1004381 -4.0671859 -4.0356793 -4.0127521 -4.0143065 -4.03479 -4.0650849 -4.0950518 -4.1305394 -4.1702466 -4.20269 -4.2244983][-4.1743445 -4.1419296 -4.103301 -4.0579371 -4.0101953 -3.9527693 -3.90952 -3.9136562 -3.9487805 -3.9917521 -4.0324788 -4.0782728 -4.1254935 -4.1644449 -4.1974721][-4.1709552 -4.1279521 -4.0790777 -4.0275941 -3.969986 -3.8914084 -3.8270392 -3.8345892 -3.8874917 -3.942471 -3.9880321 -4.0325146 -4.0759521 -4.1181784 -4.1597395][-4.171216 -4.1248679 -4.0806341 -4.0379367 -3.9923382 -3.928345 -3.8750153 -3.8763685 -3.9115462 -3.949316 -3.9804263 -4.0100412 -4.042697 -4.0832524 -4.1304922][-4.1742411 -4.1378608 -4.1120868 -4.08628 -4.05909 -4.0190663 -3.9910316 -3.9908504 -3.9934628 -3.9975131 -4.0084715 -4.0220337 -4.0391145 -4.0693431 -4.1139164][-4.1892333 -4.1689496 -4.1572952 -4.1411953 -4.12313 -4.0983739 -4.092104 -4.0949326 -4.0794296 -4.0554438 -4.0450177 -4.0403128 -4.0426531 -4.0624771 -4.1031232][-4.2135839 -4.2016377 -4.1949625 -4.1797829 -4.1624794 -4.1447382 -4.1461473 -4.1512856 -4.1332812 -4.0947528 -4.0636935 -4.0446115 -4.0452967 -4.06356 -4.099525][-4.2374463 -4.2290711 -4.2194662 -4.2018456 -4.182549 -4.1677871 -4.1681175 -4.1716752 -4.1594977 -4.1254721 -4.0912933 -4.0706835 -4.0716667 -4.0872335 -4.1132989][-4.2487731 -4.2398038 -4.2264204 -4.2054372 -4.1858935 -4.1716981 -4.1670589 -4.1664648 -4.1631045 -4.1475081 -4.1238179 -4.1067948 -4.1025596 -4.1073127 -4.1189241][-4.2354379 -4.2226315 -4.2090888 -4.1914248 -4.175426 -4.1606884 -4.1488395 -4.1409197 -4.1362867 -4.129499 -4.1189623 -4.1075888 -4.10686 -4.1083026 -4.1080527]]...]
INFO - root - 2017-12-06 09:16:13.896708: step 8710, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 83h:59m:15s remains)
INFO - root - 2017-12-06 09:16:22.981152: step 8720, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 80h:17m:17s remains)
INFO - root - 2017-12-06 09:16:32.136679: step 8730, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 83h:30m:48s remains)
INFO - root - 2017-12-06 09:16:41.365364: step 8740, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 81h:58m:15s remains)
INFO - root - 2017-12-06 09:16:50.591800: step 8750, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 84h:04m:06s remains)
INFO - root - 2017-12-06 09:16:59.864068: step 8760, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 81h:40m:58s remains)
INFO - root - 2017-12-06 09:17:08.901107: step 8770, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.894 sec/batch; 80h:24m:08s remains)
INFO - root - 2017-12-06 09:17:18.081164: step 8780, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.923 sec/batch; 82h:59m:54s remains)
INFO - root - 2017-12-06 09:17:27.331932: step 8790, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.926 sec/batch; 83h:14m:27s remains)
INFO - root - 2017-12-06 09:17:36.540028: step 8800, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 82h:21m:02s remains)
2017-12-06 09:17:37.192776: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2105279 -4.2121062 -4.2070532 -4.2041039 -4.1901956 -4.1766977 -4.1788731 -4.2013664 -4.2257066 -4.2476449 -4.2713246 -4.2904224 -4.3072453 -4.3133812 -4.3000059][-4.2091203 -4.2187929 -4.22188 -4.2221661 -4.2102423 -4.1950464 -4.1926327 -4.2072315 -4.2266259 -4.2468657 -4.2655821 -4.2820592 -4.299253 -4.3055429 -4.2912936][-4.2182484 -4.2338758 -4.2400365 -4.23666 -4.2227221 -4.2051854 -4.1944547 -4.1964612 -4.2085853 -4.2279439 -4.2450147 -4.2619538 -4.2817812 -4.2911439 -4.2786822][-4.2367425 -4.2544661 -4.2563405 -4.2416935 -4.2163458 -4.187 -4.1633472 -4.1578193 -4.1724005 -4.1963458 -4.2178073 -4.2420516 -4.2669325 -4.2779164 -4.2654185][-4.2692528 -4.27972 -4.2679319 -4.2357674 -4.1886954 -4.1340494 -4.0890021 -4.083322 -4.1170716 -4.1633158 -4.1975465 -4.2305741 -4.2568588 -4.2670255 -4.2517824][-4.298996 -4.300456 -4.2744627 -4.2264118 -4.1537018 -4.0608869 -3.979934 -3.9762466 -4.0485973 -4.13409 -4.1888523 -4.2260094 -4.2490015 -4.2548409 -4.2362714][-4.3097568 -4.30615 -4.2732754 -4.2152681 -4.1199841 -3.9858525 -3.8616621 -3.8692808 -3.991648 -4.1167741 -4.1894231 -4.2282333 -4.2452493 -4.2455044 -4.2249427][-4.3040566 -4.3008161 -4.2705016 -4.2126408 -4.1071835 -3.948885 -3.8041368 -3.8278794 -3.9845862 -4.1250968 -4.2018533 -4.2366767 -4.2465982 -4.2400961 -4.2206063][-4.2896066 -4.2937369 -4.2753897 -4.2276912 -4.1303134 -3.9829712 -3.8614776 -3.8904035 -4.0291557 -4.1487694 -4.2133913 -4.2405438 -4.2432084 -4.234724 -4.2221174][-4.2688484 -4.28197 -4.2818184 -4.255372 -4.1808209 -4.0688133 -3.9863844 -4.00479 -4.0954423 -4.1765432 -4.2234583 -4.2414861 -4.238204 -4.2336373 -4.23275][-4.2432294 -4.2656522 -4.2880511 -4.2883005 -4.2433605 -4.1684623 -4.1138792 -4.1117282 -4.1530437 -4.2023487 -4.2355337 -4.2453785 -4.2395635 -4.238677 -4.2490945][-4.2247453 -4.2548585 -4.2954412 -4.3140831 -4.2952375 -4.2480264 -4.2044611 -4.1845951 -4.1955719 -4.2283378 -4.2530355 -4.2567334 -4.2496924 -4.2496667 -4.2626495][-4.2274189 -4.25869 -4.3045216 -4.3296638 -4.324265 -4.2935138 -4.2549648 -4.2258449 -4.2238111 -4.2472878 -4.2668395 -4.268332 -4.2635126 -4.2623887 -4.2700186][-4.24039 -4.2672591 -4.3082438 -4.3332276 -4.3329124 -4.3121428 -4.2807484 -4.2488475 -4.2397451 -4.2555871 -4.26949 -4.2689314 -4.2620869 -4.2599277 -4.2648306][-4.25982 -4.2802577 -4.3104811 -4.3301096 -4.3308792 -4.3137178 -4.2849655 -4.2503295 -4.23351 -4.2436447 -4.25683 -4.2576213 -4.2476816 -4.2410889 -4.2434096]]...]
INFO - root - 2017-12-06 09:17:46.453276: step 8810, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 81h:47m:50s remains)
INFO - root - 2017-12-06 09:17:55.492660: step 8820, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 83h:43m:57s remains)
INFO - root - 2017-12-06 09:18:04.652992: step 8830, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 83h:05m:25s remains)
INFO - root - 2017-12-06 09:18:13.879160: step 8840, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.977 sec/batch; 87h:49m:25s remains)
INFO - root - 2017-12-06 09:18:23.197185: step 8850, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.945 sec/batch; 84h:58m:38s remains)
INFO - root - 2017-12-06 09:18:32.396054: step 8860, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 84h:21m:01s remains)
INFO - root - 2017-12-06 09:18:41.568855: step 8870, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 81h:46m:19s remains)
INFO - root - 2017-12-06 09:18:50.588836: step 8880, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 85h:29m:28s remains)
INFO - root - 2017-12-06 09:18:59.729947: step 8890, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 0.764 sec/batch; 68h:41m:06s remains)
INFO - root - 2017-12-06 09:19:08.778172: step 8900, loss = 2.05, batch loss = 1.99 (10.1 examples/sec; 0.790 sec/batch; 71h:02m:47s remains)
2017-12-06 09:19:09.485380: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1825681 -4.1713729 -4.1760941 -4.191226 -4.1962905 -4.1913419 -4.1954837 -4.1993704 -4.209065 -4.2188716 -4.2104874 -4.1927929 -4.1746697 -4.1540685 -4.1405449][-4.1268048 -4.127018 -4.1442919 -4.1777596 -4.2023339 -4.2094955 -4.2182245 -4.2195215 -4.2209263 -4.2300019 -4.2210383 -4.1861134 -4.145978 -4.1125307 -4.0971227][-4.0532665 -4.0777407 -4.1208391 -4.174459 -4.2110939 -4.221786 -4.2257442 -4.2167883 -4.2096 -4.2140021 -4.2000608 -4.1539021 -4.096406 -4.052536 -4.0350137][-3.9703617 -4.0295973 -4.1047149 -4.1774092 -4.2148032 -4.2170076 -4.2014389 -4.1791453 -4.1667233 -4.1647296 -4.1515369 -4.1096044 -4.0564914 -4.0201077 -4.011879][-3.9716361 -4.0570736 -4.1429181 -4.2034278 -4.2203913 -4.1969938 -4.149085 -4.1073461 -4.0932913 -4.0962758 -4.0990906 -4.0845151 -4.0615773 -4.0496116 -4.0537663][-4.0689993 -4.145155 -4.2039371 -4.225225 -4.2073016 -4.1508536 -4.0695815 -4.0106211 -4.0125656 -4.0417657 -4.0724039 -4.09123 -4.0988684 -4.1082597 -4.1148062][-4.1517658 -4.2000318 -4.2191544 -4.1964779 -4.141283 -4.0546584 -3.9414673 -3.8721907 -3.9209688 -4.0002861 -4.0660238 -4.1108165 -4.1354222 -4.1477857 -4.14349][-4.1906419 -4.2113743 -4.1965218 -4.1321511 -4.0416284 -3.930768 -3.7969356 -3.7379246 -3.8524144 -3.9768116 -4.0628152 -4.1207347 -4.1517944 -4.1542273 -4.1328344][-4.2058344 -4.20363 -4.1635494 -4.0790772 -3.9792643 -3.8781395 -3.7758336 -3.7529998 -3.8853121 -4.0085649 -4.0822344 -4.1330128 -4.1642222 -4.1563687 -4.124002][-4.1975374 -4.1841865 -4.1352577 -4.0587454 -3.9789147 -3.9159486 -3.871151 -3.8782785 -3.9802907 -4.0702639 -4.1167927 -4.1519132 -4.1782374 -4.1671982 -4.139431][-4.1909933 -4.1717024 -4.1193023 -4.0553317 -3.9990194 -3.969727 -3.9635456 -3.9833457 -4.0549769 -4.1157827 -4.1435766 -4.1661043 -4.1863194 -4.17662 -4.1572094][-4.2030869 -4.182106 -4.136817 -4.0921273 -4.0595379 -4.0526505 -4.0642753 -4.0850115 -4.1273856 -4.1637716 -4.1774621 -4.1884675 -4.2021127 -4.1963439 -4.1875072][-4.2409415 -4.22614 -4.1983318 -4.1755261 -4.1617079 -4.1645713 -4.1776366 -4.18999 -4.2073021 -4.2231293 -4.2281294 -4.2330661 -4.2435622 -4.2430496 -4.2436676][-4.2860942 -4.2783618 -4.2654967 -4.2559671 -4.25063 -4.2557874 -4.2665854 -4.2745538 -4.2803421 -4.2859616 -4.2871523 -4.2903833 -4.2974825 -4.297823 -4.2993989][-4.3134861 -4.3112478 -4.305819 -4.3034382 -4.3016524 -4.3063469 -4.313096 -4.3179779 -4.3205528 -4.3221011 -4.3208447 -4.3215256 -4.32383 -4.32232 -4.3221817]]...]
INFO - root - 2017-12-06 09:19:18.758165: step 8910, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 84h:39m:41s remains)
INFO - root - 2017-12-06 09:19:27.997069: step 8920, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.909 sec/batch; 81h:39m:41s remains)
INFO - root - 2017-12-06 09:19:37.278442: step 8930, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 81h:51m:51s remains)
INFO - root - 2017-12-06 09:19:46.623973: step 8940, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.925 sec/batch; 83h:06m:13s remains)
INFO - root - 2017-12-06 09:19:55.836207: step 8950, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 83h:34m:11s remains)
INFO - root - 2017-12-06 09:20:05.063493: step 8960, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 81h:30m:16s remains)
INFO - root - 2017-12-06 09:20:14.213560: step 8970, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 81h:41m:01s remains)
INFO - root - 2017-12-06 09:20:23.507635: step 8980, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 79h:10m:39s remains)
INFO - root - 2017-12-06 09:20:32.671014: step 8990, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 82h:01m:39s remains)
INFO - root - 2017-12-06 09:20:41.997572: step 9000, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 84h:27m:26s remains)
2017-12-06 09:20:42.683840: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3024249 -4.2926426 -4.2848468 -4.286334 -4.2923937 -4.2887945 -4.2801352 -4.2613916 -4.2428665 -4.2328696 -4.2429628 -4.2707024 -4.298708 -4.3172889 -4.3216949][-4.2769914 -4.2662034 -4.25446 -4.25506 -4.2605543 -4.2517333 -4.2355523 -4.2103767 -4.1951733 -4.1887121 -4.1996202 -4.2298369 -4.2666817 -4.2943573 -4.3027773][-4.2420683 -4.2276845 -4.2125964 -4.2088203 -4.2120094 -4.1977673 -4.1735764 -4.1394229 -4.1327581 -4.1348066 -4.1459889 -4.1780691 -4.2206621 -4.2563896 -4.27103][-4.1997442 -4.1808329 -4.1613951 -4.1427836 -4.1339283 -4.105247 -4.067914 -4.0307527 -4.0483856 -4.0715418 -4.0920424 -4.1249113 -4.1690445 -4.2118187 -4.2353935][-4.1621451 -4.1392684 -4.1150646 -4.0733433 -4.030014 -3.9636283 -3.8953886 -3.8612514 -3.9278553 -3.997122 -4.0407982 -4.0754046 -4.1202846 -4.1719656 -4.2076416][-4.1390285 -4.1127205 -4.0817409 -4.0220776 -3.9416423 -3.8194067 -3.6926649 -3.6532848 -3.782371 -3.9135427 -3.9918711 -4.0375896 -4.0890474 -4.1511 -4.1967769][-4.1208282 -4.099894 -4.0725355 -4.0127778 -3.916594 -3.7540779 -3.5624928 -3.4975972 -3.6723869 -3.8459249 -3.9533331 -4.0139079 -4.0797434 -4.1525273 -4.2017188][-4.0944624 -4.0889096 -4.0781493 -4.0351009 -3.9602921 -3.8258233 -3.6439953 -3.5617702 -3.7049403 -3.8609684 -3.960989 -4.0204663 -4.0930805 -4.1691413 -4.2164631][-4.0858374 -4.0979228 -4.1142759 -4.0971661 -4.0504065 -3.9574821 -3.8152981 -3.7404413 -3.830827 -3.9410844 -4.0137153 -4.061029 -4.1291332 -4.1955509 -4.2352209][-4.0889831 -4.1152329 -4.1554685 -4.1615672 -4.1345587 -4.0671115 -3.956701 -3.893749 -3.9505894 -4.0289974 -4.07772 -4.1125565 -4.1699634 -4.2241864 -4.25446][-4.1241889 -4.1556053 -4.1983871 -4.2077861 -4.1904345 -4.1490345 -4.0670114 -4.0207515 -4.0582027 -4.1168518 -4.1506486 -4.1742253 -4.2183995 -4.2576237 -4.2755709][-4.1779118 -4.2070446 -4.2381353 -4.2434034 -4.2368298 -4.2171435 -4.1573572 -4.1270247 -4.155015 -4.1972394 -4.2170753 -4.2319169 -4.2630763 -4.2863655 -4.2941651][-4.2263827 -4.2510352 -4.2705116 -4.2667928 -4.2591405 -4.2521577 -4.2147365 -4.1995039 -4.2191443 -4.2477775 -4.2607908 -4.2695546 -4.2900734 -4.3017955 -4.3061271][-4.252275 -4.2708197 -4.2829108 -4.2769451 -4.2709823 -4.2689676 -4.2497649 -4.2421656 -4.2542648 -4.2722058 -4.2803645 -4.2831964 -4.2959275 -4.3050752 -4.3109927][-4.2672062 -4.2765589 -4.2815351 -4.2778616 -4.27754 -4.2795215 -4.271934 -4.2701311 -4.2769051 -4.2878666 -4.2939506 -4.2960558 -4.3037767 -4.3113337 -4.3165317]]...]
INFO - root - 2017-12-06 09:20:51.846050: step 9010, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 80h:22m:57s remains)
INFO - root - 2017-12-06 09:21:01.107556: step 9020, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.884 sec/batch; 79h:27m:18s remains)
INFO - root - 2017-12-06 09:21:10.268994: step 9030, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 84h:00m:48s remains)
INFO - root - 2017-12-06 09:21:19.018649: step 9040, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.882 sec/batch; 79h:16m:33s remains)
INFO - root - 2017-12-06 09:21:28.266445: step 9050, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 83h:26m:51s remains)
INFO - root - 2017-12-06 09:21:37.373305: step 9060, loss = 2.09, batch loss = 2.04 (9.1 examples/sec; 0.882 sec/batch; 79h:13m:48s remains)
INFO - root - 2017-12-06 09:21:46.562446: step 9070, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 83h:53m:43s remains)
INFO - root - 2017-12-06 09:21:55.942108: step 9080, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.937 sec/batch; 84h:13m:15s remains)
INFO - root - 2017-12-06 09:22:05.182122: step 9090, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 78h:04m:45s remains)
INFO - root - 2017-12-06 09:22:14.409647: step 9100, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 82h:05m:30s remains)
2017-12-06 09:22:15.074763: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3394594 -4.3296576 -4.3020792 -4.2559857 -4.205955 -4.1880169 -4.1894808 -4.1916652 -4.2007 -4.2167511 -4.2233396 -4.2305393 -4.2394571 -4.246871 -4.2360964][-4.3420839 -4.3302574 -4.3015618 -4.257339 -4.2121243 -4.1975732 -4.201726 -4.2057052 -4.2138515 -4.2266755 -4.2288523 -4.2344151 -4.245522 -4.254663 -4.243134][-4.3444753 -4.3295746 -4.2991962 -4.2553644 -4.2152586 -4.2017975 -4.2045388 -4.2096782 -4.2229843 -4.23614 -4.2366939 -4.2367473 -4.2454934 -4.2552471 -4.2446618][-4.3503351 -4.3353777 -4.3039937 -4.2582178 -4.2149916 -4.1972046 -4.1962366 -4.2026377 -4.2218351 -4.2366681 -4.2402406 -4.2353034 -4.2350087 -4.2401824 -4.2345638][-4.3574572 -4.3442769 -4.3116217 -4.2615261 -4.2085471 -4.1793513 -4.1699653 -4.1736817 -4.1977253 -4.2223148 -4.2329087 -4.2288718 -4.2229381 -4.2210822 -4.2199926][-4.3635397 -4.3518043 -4.3184009 -4.2627525 -4.1943235 -4.142592 -4.1127214 -4.1022048 -4.1327019 -4.1846232 -4.21825 -4.2253132 -4.2198873 -4.2143621 -4.2142367][-4.3658581 -4.3563242 -4.324358 -4.2677379 -4.1873441 -4.1074686 -4.0387359 -3.9978118 -4.037488 -4.1295447 -4.1970477 -4.2275949 -4.2319903 -4.228869 -4.2249274][-4.3666596 -4.3596077 -4.3329406 -4.2811289 -4.1993294 -4.1007872 -3.994669 -3.9090178 -3.9455609 -4.0688548 -4.1664457 -4.2226257 -4.245306 -4.2505369 -4.2438378][-4.3675365 -4.3623104 -4.3413615 -4.2988005 -4.2262607 -4.1342926 -4.0291262 -3.9381459 -3.9565461 -4.0587583 -4.1480627 -4.2120237 -4.2492218 -4.2617083 -4.2546158][-4.3680511 -4.3630328 -4.3464351 -4.3132339 -4.255321 -4.18398 -4.1108866 -4.0520854 -4.0555053 -4.1029377 -4.1521964 -4.2062812 -4.2486286 -4.2659636 -4.2621779][-4.3678951 -4.3621273 -4.3469 -4.3154621 -4.2633834 -4.2069659 -4.1634369 -4.1350379 -4.139132 -4.1573358 -4.1804461 -4.2182851 -4.2550941 -4.2702317 -4.2640224][-4.365211 -4.3582296 -4.3406944 -4.3052735 -4.2535033 -4.2060103 -4.1781611 -4.1631713 -4.1710982 -4.1878133 -4.2095776 -4.2347722 -4.2581434 -4.2634692 -4.2503886][-4.362196 -4.3546391 -4.3345966 -4.2950172 -4.2419348 -4.1989722 -4.1736288 -4.1600614 -4.1705117 -4.1970849 -4.2258019 -4.2441988 -4.256319 -4.2513771 -4.2316079][-4.3613324 -4.3529315 -4.3305926 -4.2905 -4.2412643 -4.2019181 -4.1782665 -4.1676397 -4.1841159 -4.2150478 -4.23918 -4.2492204 -4.2526646 -4.2408237 -4.2164726][-4.3606853 -4.3516808 -4.32903 -4.2940097 -4.25596 -4.2249889 -4.2046494 -4.1960011 -4.2133017 -4.2408891 -4.2556734 -4.2585225 -4.2570477 -4.2461176 -4.2245197]]...]
INFO - root - 2017-12-06 09:22:24.175109: step 9110, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.985 sec/batch; 88h:27m:00s remains)
INFO - root - 2017-12-06 09:22:33.277054: step 9120, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 80h:43m:33s remains)
INFO - root - 2017-12-06 09:22:42.508336: step 9130, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 84h:42m:48s remains)
INFO - root - 2017-12-06 09:22:51.672744: step 9140, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 84h:45m:10s remains)
INFO - root - 2017-12-06 09:23:00.851671: step 9150, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 82h:26m:58s remains)
INFO - root - 2017-12-06 09:23:10.036468: step 9160, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.951 sec/batch; 85h:23m:34s remains)
INFO - root - 2017-12-06 09:23:19.130141: step 9170, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 81h:47m:54s remains)
INFO - root - 2017-12-06 09:23:28.251781: step 9180, loss = 2.11, batch loss = 2.05 (9.0 examples/sec; 0.890 sec/batch; 79h:53m:49s remains)
INFO - root - 2017-12-06 09:23:37.600524: step 9190, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 80h:37m:00s remains)
INFO - root - 2017-12-06 09:23:46.731392: step 9200, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 79h:55m:36s remains)
2017-12-06 09:23:47.430798: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.32777 -4.31528 -4.2945065 -4.2668486 -4.2422938 -4.2294116 -4.2291689 -4.2411981 -4.2513227 -4.2603679 -4.2747288 -4.2844276 -4.2848825 -4.2841215 -4.2801991][-4.3288059 -4.3107553 -4.2719531 -4.2232766 -4.1826606 -4.1615624 -4.1594777 -4.183187 -4.2055473 -4.22532 -4.2432613 -4.2537308 -4.2553983 -4.2532296 -4.2480292][-4.3292003 -4.3049135 -4.2474546 -4.1755843 -4.1207967 -4.0863757 -4.0780611 -4.1173396 -4.1576042 -4.1888781 -4.2110972 -4.2227416 -4.2267737 -4.2246556 -4.2187209][-4.3327165 -4.3031311 -4.2350507 -4.1517625 -4.0824547 -4.0186019 -3.9904594 -4.0453806 -4.1084204 -4.1536207 -4.1788917 -4.187562 -4.190001 -4.1880035 -4.18588][-4.3382845 -4.3077331 -4.2306986 -4.141109 -4.0534449 -3.9468374 -3.8841953 -3.9555023 -4.0499549 -4.11128 -4.1409583 -4.1443586 -4.1436911 -4.1437883 -4.1519814][-4.339304 -4.3063912 -4.2255993 -4.1278853 -4.0175948 -3.8674812 -3.7534449 -3.8263121 -3.964726 -4.0537143 -4.0961714 -4.104115 -4.1054192 -4.1106529 -4.1300507][-4.3324165 -4.2955408 -4.2111206 -4.0978284 -3.9606638 -3.7736573 -3.6058626 -3.6693671 -3.8652294 -3.9965217 -4.0586829 -4.0747404 -4.079967 -4.094451 -4.1202826][-4.3262033 -4.2870045 -4.1980166 -4.0770288 -3.9274244 -3.7413259 -3.5892091 -3.657625 -3.8614767 -3.9920628 -4.0517178 -4.0673089 -4.073668 -4.0942073 -4.1199131][-4.3194265 -4.28226 -4.1975713 -4.0878978 -3.9558558 -3.815485 -3.7321634 -3.8011742 -3.947226 -4.0357351 -4.071826 -4.0800138 -4.0842586 -4.1042967 -4.1288371][-4.3152213 -4.284059 -4.2127423 -4.1268849 -4.0269265 -3.91961 -3.8771794 -3.9330747 -4.0252194 -4.0760932 -4.0964904 -4.1053462 -4.1102571 -4.1258368 -4.1481872][-4.3114953 -4.2884035 -4.2369595 -4.1728945 -4.0946603 -4.0130281 -3.993217 -4.0380111 -4.0881667 -4.1111608 -4.123672 -4.1393218 -4.150394 -4.1605592 -4.1783733][-4.3145185 -4.29944 -4.2608185 -4.207129 -4.1452065 -4.091805 -4.088697 -4.1226597 -4.1445832 -4.1478271 -4.1523786 -4.172998 -4.1907597 -4.1977172 -4.2132907][-4.3247466 -4.3136668 -4.2835264 -4.2428827 -4.2026448 -4.1746049 -4.1789207 -4.1990805 -4.205512 -4.1980453 -4.195075 -4.2149282 -4.2360682 -4.2391987 -4.2505651][-4.3377795 -4.3295164 -4.3109317 -4.2843862 -4.260097 -4.2488728 -4.2570024 -4.2688155 -4.2690282 -4.2611094 -4.2549877 -4.2675304 -4.2824478 -4.2794328 -4.2823458][-4.3443775 -4.3396559 -4.3274136 -4.308567 -4.2936964 -4.2932768 -4.305151 -4.3162613 -4.3152385 -4.3089256 -4.3038945 -4.3086433 -4.3152652 -4.3091588 -4.3087745]]...]
INFO - root - 2017-12-06 09:23:56.644666: step 9210, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.917 sec/batch; 82h:20m:33s remains)
INFO - root - 2017-12-06 09:24:05.960094: step 9220, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.928 sec/batch; 83h:21m:47s remains)
INFO - root - 2017-12-06 09:24:15.046658: step 9230, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.840 sec/batch; 75h:25m:07s remains)
INFO - root - 2017-12-06 09:24:24.169580: step 9240, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.932 sec/batch; 83h:40m:43s remains)
INFO - root - 2017-12-06 09:24:33.344635: step 9250, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 82h:34m:23s remains)
INFO - root - 2017-12-06 09:24:42.525969: step 9260, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.969 sec/batch; 87h:00m:26s remains)
INFO - root - 2017-12-06 09:24:51.763225: step 9270, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 83h:56m:51s remains)
INFO - root - 2017-12-06 09:25:01.153053: step 9280, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.960 sec/batch; 86h:11m:46s remains)
INFO - root - 2017-12-06 09:25:10.354881: step 9290, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 84h:01m:45s remains)
INFO - root - 2017-12-06 09:25:19.390283: step 9300, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.965 sec/batch; 86h:35m:45s remains)
2017-12-06 09:25:20.002691: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1924915 -4.1694913 -4.1511664 -4.1365027 -4.1353927 -4.141891 -4.1565719 -4.1845145 -4.2057414 -4.2083406 -4.1948409 -4.1676288 -4.1414237 -4.1341448 -4.1589375][-4.1633658 -4.1358476 -4.1134334 -4.0943251 -4.0971804 -4.1124363 -4.1321383 -4.1625094 -4.1848874 -4.186049 -4.1757531 -4.1572423 -4.1393728 -4.1388664 -4.1667461][-4.1187644 -4.0884161 -4.0642762 -4.0459557 -4.0569286 -4.0807509 -4.1033869 -4.1306586 -4.1517744 -4.1555767 -4.1520114 -4.1417952 -4.131773 -4.1381454 -4.1695085][-4.08893 -4.0582538 -4.0353174 -4.0234184 -4.0416532 -4.0684657 -4.087966 -4.1097045 -4.1317358 -4.1410141 -4.1431503 -4.1372595 -4.1316047 -4.1390409 -4.168983][-4.0845733 -4.0576921 -4.0395169 -4.0336232 -4.050869 -4.0728106 -4.0852652 -4.1014557 -4.1238627 -4.1365652 -4.1420517 -4.1414833 -4.1380959 -4.1426606 -4.1692138][-4.0809307 -4.0556941 -4.0381584 -4.0315161 -4.0409794 -4.05491 -4.0599208 -4.0738225 -4.0995212 -4.1176376 -4.1285057 -4.1348248 -4.1355906 -4.1403661 -4.1674347][-4.0536261 -4.0252538 -4.0056219 -3.9958744 -4.0005808 -4.0146685 -4.0244255 -4.0442543 -4.0759697 -4.0997353 -4.1144123 -4.1231608 -4.12629 -4.1340485 -4.1651349][-4.0250535 -3.9962504 -3.9729269 -3.9586172 -3.9630117 -3.9798989 -4.0004206 -4.0318842 -4.0689039 -4.0960579 -4.1103373 -4.1165485 -4.1203771 -4.1317716 -4.1669145][-4.014658 -3.9926507 -3.97279 -3.955842 -3.9565864 -3.9734492 -4.003047 -4.0428143 -4.0828757 -4.1115775 -4.1246424 -4.1264338 -4.1277962 -4.1394191 -4.1736178][-4.0305805 -4.0137215 -3.9976966 -3.9811196 -3.9807258 -3.9966261 -4.0278692 -4.0698843 -4.1091132 -4.1398954 -4.1541018 -4.1521435 -4.1482139 -4.1551542 -4.18392][-4.0523996 -4.0384417 -4.0214763 -4.00338 -4.0013475 -4.0160851 -4.0459766 -4.0869837 -4.1254315 -4.1595926 -4.1785688 -4.1758957 -4.1676054 -4.1705513 -4.1937809][-4.0675335 -4.0581355 -4.0453758 -4.0296192 -4.0282941 -4.0411091 -4.066041 -4.1011095 -4.1371446 -4.1707897 -4.1903644 -4.1869154 -4.1760764 -4.1774287 -4.198338][-4.0664744 -4.0586867 -4.0493031 -4.0390811 -4.03875 -4.0451903 -4.05837 -4.084393 -4.1168022 -4.1490512 -4.1723828 -4.173171 -4.1656651 -4.1693006 -4.1918945][-4.0634718 -4.0525694 -4.0412335 -4.0323682 -4.0296311 -4.0287352 -4.0302787 -4.0479283 -4.0764 -4.1074891 -4.1364594 -4.1449723 -4.1439724 -4.151535 -4.177248][-4.0738006 -4.0608873 -4.0473752 -4.0378509 -4.0353489 -4.0346994 -4.0338421 -4.04683 -4.0707521 -4.0976667 -4.1245089 -4.1328335 -4.1313171 -4.1367087 -4.1629162]]...]
INFO - root - 2017-12-06 09:25:29.174094: step 9310, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 79h:59m:46s remains)
INFO - root - 2017-12-06 09:25:38.401044: step 9320, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 83h:58m:19s remains)
INFO - root - 2017-12-06 09:25:47.726922: step 9330, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.884 sec/batch; 79h:20m:39s remains)
INFO - root - 2017-12-06 09:25:56.985351: step 9340, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.939 sec/batch; 84h:18m:51s remains)
INFO - root - 2017-12-06 09:26:06.161440: step 9350, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.894 sec/batch; 80h:12m:16s remains)
INFO - root - 2017-12-06 09:26:15.405859: step 9360, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 82h:47m:01s remains)
INFO - root - 2017-12-06 09:26:24.554496: step 9370, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.943 sec/batch; 84h:39m:08s remains)
INFO - root - 2017-12-06 09:26:33.773146: step 9380, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.891 sec/batch; 79h:57m:31s remains)
INFO - root - 2017-12-06 09:26:42.811036: step 9390, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 80h:26m:35s remains)
INFO - root - 2017-12-06 09:26:51.994265: step 9400, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.963 sec/batch; 86h:23m:15s remains)
2017-12-06 09:26:52.629438: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1612587 -4.1666265 -4.1903877 -4.2112241 -4.2155356 -4.2178564 -4.2323523 -4.2467728 -4.2239342 -4.16041 -4.1287837 -4.1410823 -4.1695857 -4.1910315 -4.2098103][-4.1670384 -4.1853929 -4.2183428 -4.232502 -4.2242436 -4.2209563 -4.2382865 -4.2581577 -4.233943 -4.1638165 -4.1250491 -4.1338797 -4.1667337 -4.1878204 -4.2043891][-4.1920829 -4.2116537 -4.234746 -4.2362819 -4.2229519 -4.2166252 -4.2306767 -4.247694 -4.2254033 -4.1553059 -4.1176877 -4.1298537 -4.1641388 -4.1851778 -4.2016788][-4.2140908 -4.2258124 -4.2291794 -4.2190971 -4.2091513 -4.21138 -4.2201304 -4.2282319 -4.2080908 -4.146153 -4.117363 -4.136754 -4.1699295 -4.1901712 -4.2052736][-4.235652 -4.2368326 -4.2119117 -4.1863408 -4.1786733 -4.1879411 -4.1961 -4.1979103 -4.1789184 -4.1266246 -4.108438 -4.1352444 -4.1638279 -4.1839895 -4.2010779][-4.2262526 -4.210288 -4.1548352 -4.1052246 -4.0912223 -4.1047058 -4.1118832 -4.104413 -4.0871387 -4.0515914 -4.0489883 -4.089726 -4.1222911 -4.147851 -4.17457][-4.1865611 -4.1516237 -4.0664339 -3.9854472 -3.9610536 -3.9799428 -3.9888496 -3.9671159 -3.9525576 -3.9386933 -3.9563258 -4.0137625 -4.0594249 -4.0941172 -4.1394544][-4.1364193 -4.0968189 -4.0052433 -3.91705 -3.8954911 -3.9239786 -3.9402232 -3.9115174 -3.9018061 -3.9113042 -3.9379401 -3.9906638 -4.0361075 -4.0730743 -4.1267557][-4.1041274 -4.0721273 -3.9912279 -3.9130754 -3.8963771 -3.9283845 -3.9478667 -3.9160252 -3.9160097 -3.9528792 -3.9904604 -4.0367937 -4.0753527 -4.1060319 -4.1536288][-4.10808 -4.0852876 -4.0303493 -3.9740634 -3.9594617 -3.983361 -3.995472 -3.9651966 -3.9712148 -4.0215693 -4.0666981 -4.1142888 -4.1534247 -4.1827364 -4.2181516][-4.1440763 -4.1324544 -4.1073761 -4.0786119 -4.0644011 -4.0730419 -4.0772543 -4.0561037 -4.0635285 -4.106566 -4.1497421 -4.1947165 -4.2288351 -4.2503843 -4.2724605][-4.1915503 -4.1899066 -4.1860938 -4.1777387 -4.1705413 -4.1717257 -4.171545 -4.1565289 -4.1554437 -4.1768503 -4.2076168 -4.2437968 -4.2703986 -4.2852111 -4.2968][-4.2409039 -4.2412286 -4.2443953 -4.2460823 -4.2434072 -4.2442842 -4.2457047 -4.2375708 -4.2297287 -4.2265773 -4.2390733 -4.2621613 -4.2784433 -4.2886758 -4.2962594][-4.2788925 -4.27609 -4.2776189 -4.2786574 -4.2765193 -4.2778893 -4.2794962 -4.2729473 -4.2587304 -4.237299 -4.2366123 -4.253624 -4.2658834 -4.2769732 -4.2857857][-4.2914639 -4.286305 -4.2855749 -4.285893 -4.2862916 -4.2882032 -4.2886066 -4.281301 -4.2611537 -4.2256446 -4.2121854 -4.2259336 -4.2403059 -4.2571115 -4.2719975]]...]
INFO - root - 2017-12-06 09:27:01.858317: step 9410, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 82h:21m:33s remains)
INFO - root - 2017-12-06 09:27:11.026746: step 9420, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 84h:33m:59s remains)
INFO - root - 2017-12-06 09:27:20.360982: step 9430, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 86h:43m:27s remains)
INFO - root - 2017-12-06 09:27:29.547982: step 9440, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 84h:13m:04s remains)
INFO - root - 2017-12-06 09:27:38.815088: step 9450, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 84h:19m:29s remains)
INFO - root - 2017-12-06 09:27:47.857504: step 9460, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 82h:54m:18s remains)
INFO - root - 2017-12-06 09:27:57.037144: step 9470, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.904 sec/batch; 81h:08m:05s remains)
INFO - root - 2017-12-06 09:28:06.254621: step 9480, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 81h:43m:30s remains)
INFO - root - 2017-12-06 09:28:15.494446: step 9490, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 81h:03m:34s remains)
INFO - root - 2017-12-06 09:28:24.608361: step 9500, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 83h:12m:57s remains)
2017-12-06 09:28:25.250073: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2179141 -4.2003217 -4.1974711 -4.2099171 -4.2199025 -4.2228069 -4.2143607 -4.187386 -4.1610618 -4.1427441 -4.1324191 -4.1255016 -4.1253748 -4.1427021 -4.1732531][-4.227787 -4.2130494 -4.2122936 -4.2287993 -4.2436571 -4.2512317 -4.2456155 -4.2182422 -4.1902223 -4.168643 -4.1556168 -4.1462522 -4.1435161 -4.1567097 -4.183156][-4.2347836 -4.2259197 -4.2268219 -4.2426896 -4.2570405 -4.2647166 -4.260128 -4.2339005 -4.2085853 -4.1916895 -4.1821456 -4.1725397 -4.16704 -4.1730456 -4.1926975][-4.2332864 -4.2343664 -4.2384262 -4.2526212 -4.2617493 -4.2597909 -4.248085 -4.2197356 -4.1978912 -4.191988 -4.1925869 -4.1864662 -4.1780014 -4.1740265 -4.1856236][-4.2222543 -4.2323937 -4.2411218 -4.2530632 -4.2536349 -4.2409315 -4.2158427 -4.1803021 -4.1628971 -4.1716461 -4.1848211 -4.1849365 -4.1759558 -4.1639643 -4.1679974][-4.2062259 -4.2194972 -4.2307487 -4.2405624 -4.2336025 -4.2108383 -4.1704764 -4.1251059 -4.1112251 -4.1350718 -4.1614885 -4.1696215 -4.161962 -4.1450419 -4.1428533][-4.1920509 -4.206059 -4.2183065 -4.2288418 -4.2200933 -4.1922245 -4.1424656 -4.0885954 -4.0741653 -4.104641 -4.1390362 -4.1541138 -4.147975 -4.1300445 -4.123373][-4.1798258 -4.1946654 -4.2081456 -4.2205815 -4.2133293 -4.187305 -4.1370935 -4.0846972 -4.0698905 -4.0965848 -4.1287022 -4.1449709 -4.1407237 -4.1266742 -4.1201754][-4.1652985 -4.1789145 -4.1936245 -4.2083931 -4.2033319 -4.1816206 -4.13538 -4.0918479 -4.0813737 -4.1014557 -4.1240811 -4.1378441 -4.1379566 -4.1320391 -4.130044][-4.1509285 -4.1634769 -4.1787906 -4.1947155 -4.1927633 -4.176259 -4.1378675 -4.1074119 -4.1031075 -4.1178107 -4.1292729 -4.1379242 -4.1446829 -4.149579 -4.1529441][-4.1461043 -4.156446 -4.1740561 -4.192275 -4.1935306 -4.1815357 -4.1486177 -4.1265917 -4.1249886 -4.1351542 -4.1415625 -4.1475987 -4.1600838 -4.1736965 -4.1790323][-4.1439161 -4.1550732 -4.1759191 -4.1963978 -4.2021484 -4.1924577 -4.1636648 -4.1473908 -4.1493654 -4.1575351 -4.16062 -4.1645837 -4.1768646 -4.1910205 -4.1961164][-4.1459012 -4.1550722 -4.1752253 -4.1966815 -4.2070746 -4.2025456 -4.1815758 -4.1722469 -4.1744404 -4.1785741 -4.1775742 -4.1771622 -4.1827092 -4.191144 -4.1934681][-4.1503277 -4.1547146 -4.172966 -4.19516 -4.2123308 -4.2142138 -4.2003565 -4.1954689 -4.19352 -4.1901369 -4.1845779 -4.1786442 -4.1754174 -4.1740818 -4.171566][-4.1509371 -4.15257 -4.1709714 -4.1944184 -4.2144775 -4.2189212 -4.2102509 -4.2077861 -4.2014213 -4.1906514 -4.1824827 -4.1767607 -4.1677117 -4.156971 -4.1477375]]...]
INFO - root - 2017-12-06 09:28:34.454643: step 9510, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.929 sec/batch; 83h:19m:38s remains)
INFO - root - 2017-12-06 09:28:43.782689: step 9520, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.928 sec/batch; 83h:17m:15s remains)
INFO - root - 2017-12-06 09:28:52.987294: step 9530, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 81h:53m:59s remains)
INFO - root - 2017-12-06 09:29:02.112420: step 9540, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 82h:18m:41s remains)
INFO - root - 2017-12-06 09:29:11.279596: step 9550, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 80h:53m:01s remains)
INFO - root - 2017-12-06 09:29:20.674590: step 9560, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.963 sec/batch; 86h:20m:54s remains)
INFO - root - 2017-12-06 09:29:29.711729: step 9570, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.926 sec/batch; 83h:02m:42s remains)
INFO - root - 2017-12-06 09:29:38.934872: step 9580, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 81h:56m:43s remains)
INFO - root - 2017-12-06 09:29:48.198416: step 9590, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 81h:27m:54s remains)
INFO - root - 2017-12-06 09:29:57.225564: step 9600, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 79h:13m:59s remains)
2017-12-06 09:29:57.875976: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3204451 -4.3219128 -4.3216605 -4.3137112 -4.3001041 -4.2778897 -4.25416 -4.2369523 -4.2261863 -4.2169032 -4.2011933 -4.1842256 -4.1615644 -4.12764 -4.0998545][-4.3073153 -4.3105383 -4.3157468 -4.3102927 -4.2930331 -4.2636385 -4.2316861 -4.2110972 -4.2042456 -4.1989012 -4.1870036 -4.1730518 -4.1496534 -4.1010771 -4.0512815][-4.2860589 -4.2929063 -4.3046875 -4.3008151 -4.2817063 -4.251183 -4.2185335 -4.1998873 -4.1987038 -4.1995282 -4.1908455 -4.1802025 -4.15927 -4.1045456 -4.0430989][-4.2564907 -4.2673182 -4.2867351 -4.2876048 -4.2697387 -4.2418222 -4.2151875 -4.2021966 -4.2073793 -4.2127638 -4.2077026 -4.2063327 -4.1979518 -4.1585393 -4.1078563][-4.2161012 -4.2306247 -4.2571926 -4.2656083 -4.2532792 -4.2315803 -4.2153492 -4.2092986 -4.2204332 -4.2288795 -4.2268682 -4.2327037 -4.2366147 -4.2161136 -4.1868911][-4.1701851 -4.18511 -4.2199669 -4.2383347 -4.2318935 -4.216846 -4.2091928 -4.2088637 -4.2222242 -4.2328172 -4.2346268 -4.2448959 -4.2557788 -4.2509 -4.243866][-4.1186166 -4.1372523 -4.1793079 -4.2054296 -4.2040787 -4.1911855 -4.1835389 -4.1825733 -4.1977186 -4.2139211 -4.2262263 -4.2429223 -4.2602186 -4.26861 -4.2750845][-4.0930686 -4.1140914 -4.1505532 -4.1722574 -4.17221 -4.1581917 -4.1472034 -4.1441393 -4.1567411 -4.179338 -4.2047133 -4.2301121 -4.2536678 -4.2700849 -4.2834454][-4.110599 -4.1251655 -4.1388164 -4.1386685 -4.128623 -4.1065764 -4.0875425 -4.0781307 -4.0897312 -4.121572 -4.1571302 -4.1944022 -4.2298889 -4.2553086 -4.2757478][-4.1397343 -4.1402831 -4.1259007 -4.0962543 -4.0613475 -4.0206585 -3.9881921 -3.9756932 -3.9933953 -4.0406418 -4.0919118 -4.1425705 -4.1926322 -4.22861 -4.2590442][-4.1621323 -4.1469512 -4.1085906 -4.0590096 -4.0067496 -3.9559705 -3.9271953 -3.9228754 -3.9495549 -4.0055985 -4.0633707 -4.1152573 -4.166276 -4.2071905 -4.2452116][-4.1769075 -4.1526532 -4.1045961 -4.053329 -4.011281 -3.9761252 -3.965487 -3.9718261 -3.9966421 -4.045094 -4.0950637 -4.1341982 -4.1703134 -4.2053227 -4.2405157][-4.1875243 -4.1619158 -4.1210032 -4.0839033 -4.0648966 -4.0538225 -4.0546021 -4.0635238 -4.0818591 -4.116518 -4.1556935 -4.1837006 -4.2047687 -4.2276411 -4.2526841][-4.2052593 -4.1815367 -4.154469 -4.1352515 -4.1357121 -4.1452632 -4.1573234 -4.1694875 -4.1821165 -4.2030158 -4.2279792 -4.244648 -4.2544112 -4.2664156 -4.2790256][-4.2439146 -4.2247825 -4.21338 -4.2112484 -4.2206788 -4.2369704 -4.253778 -4.2635179 -4.2711167 -4.2818847 -4.2928658 -4.30042 -4.3036752 -4.3054295 -4.3031774]]...]
INFO - root - 2017-12-06 09:30:07.216282: step 9610, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 84h:15m:52s remains)
INFO - root - 2017-12-06 09:30:16.347615: step 9620, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 81h:47m:56s remains)
INFO - root - 2017-12-06 09:30:25.381617: step 9630, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 81h:55m:02s remains)
INFO - root - 2017-12-06 09:30:34.686923: step 9640, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.904 sec/batch; 81h:05m:08s remains)
INFO - root - 2017-12-06 09:30:43.923305: step 9650, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 81h:24m:51s remains)
INFO - root - 2017-12-06 09:30:53.099667: step 9660, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 82h:07m:32s remains)
INFO - root - 2017-12-06 09:31:02.214385: step 9670, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 78h:46m:12s remains)
INFO - root - 2017-12-06 09:31:11.385652: step 9680, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 83h:30m:54s remains)
INFO - root - 2017-12-06 09:31:20.562055: step 9690, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.965 sec/batch; 86h:34m:05s remains)
INFO - root - 2017-12-06 09:31:29.767735: step 9700, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 84h:34m:19s remains)
2017-12-06 09:31:30.400771: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2843328 -4.2961559 -4.2946897 -4.2901607 -4.28582 -4.2917352 -4.3053074 -4.3034616 -4.2967424 -4.2954984 -4.2991042 -4.3082361 -4.3201947 -4.3250494 -4.3215175][-4.2685962 -4.2809114 -4.2731 -4.2637992 -4.2574697 -4.2632689 -4.2771335 -4.2673688 -4.2550616 -4.2607689 -4.273591 -4.2919292 -4.3133039 -4.3235888 -4.3198447][-4.2520242 -4.2628613 -4.248014 -4.2298331 -4.2215142 -4.2261529 -4.2384071 -4.2156377 -4.1981039 -4.2138157 -4.2394361 -4.2686915 -4.2982755 -4.3117256 -4.309094][-4.2406139 -4.2503386 -4.2271681 -4.1976833 -4.1852679 -4.1853557 -4.1872754 -4.1440687 -4.1202736 -4.1512012 -4.1944242 -4.2378464 -4.27675 -4.2955794 -4.2955709][-4.228621 -4.2351184 -4.2061415 -4.1678591 -4.1497731 -4.145041 -4.1284819 -4.0554585 -4.0212 -4.07378 -4.136631 -4.1985564 -4.2496214 -4.2766509 -4.2825851][-4.2247891 -4.2266603 -4.1958165 -4.1547027 -4.1330776 -4.1209345 -4.0783167 -3.9650562 -3.9202824 -4.0025539 -4.0890179 -4.1654587 -4.2268515 -4.2621121 -4.2739143][-4.2395468 -4.2374496 -4.2115011 -4.1716385 -4.1401467 -4.105968 -4.0200391 -3.8583968 -3.8131528 -3.9464703 -4.0647697 -4.1499214 -4.2180414 -4.2563219 -4.2703719][-4.2384334 -4.2394452 -4.22642 -4.1954331 -4.1504431 -4.0814939 -3.937834 -3.7166011 -3.6795971 -3.8782072 -4.0378056 -4.1361547 -4.2122378 -4.256083 -4.2733126][-4.2197981 -4.2278934 -4.2331638 -4.2161622 -4.1685147 -4.0810361 -3.919106 -3.7033563 -3.6837821 -3.8940773 -4.0548573 -4.1484585 -4.2182026 -4.2614422 -4.2797527][-4.2072291 -4.2170324 -4.22982 -4.2245455 -4.1801434 -4.0964236 -3.961138 -3.8046596 -3.8014982 -3.9706352 -4.0992641 -4.1730623 -4.2315869 -4.2709556 -4.2872963][-4.1975784 -4.2060804 -4.2248235 -4.2263823 -4.1900992 -4.1162043 -4.00685 -3.892122 -3.9015093 -4.039526 -4.1425014 -4.2017918 -4.2501688 -4.2839475 -4.293931][-4.1853471 -4.1930261 -4.2182045 -4.2273111 -4.2004104 -4.1440787 -4.0658107 -3.9818375 -3.9940519 -4.1003604 -4.1794615 -4.2258444 -4.2645922 -4.2939286 -4.2987165][-4.1815219 -4.1881366 -4.218441 -4.2375193 -4.2262745 -4.196178 -4.1466622 -4.0860763 -4.0952463 -4.1690917 -4.2236905 -4.2559938 -4.2857504 -4.3059592 -4.3051481][-4.1901817 -4.1955929 -4.2303367 -4.2551451 -4.256238 -4.2458777 -4.2193575 -4.1745024 -4.1788554 -4.2266846 -4.2651672 -4.2874851 -4.3055534 -4.3161058 -4.3105555][-4.2008977 -4.2090139 -4.2425671 -4.2677355 -4.2776127 -4.2794895 -4.2645035 -4.231585 -4.2311392 -4.2617469 -4.2901082 -4.3056355 -4.3153286 -4.3192768 -4.3112426]]...]
INFO - root - 2017-12-06 09:31:39.709623: step 9710, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 80h:46m:36s remains)
INFO - root - 2017-12-06 09:31:49.109215: step 9720, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 85h:39m:01s remains)
INFO - root - 2017-12-06 09:31:58.459040: step 9730, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 81h:38m:24s remains)
INFO - root - 2017-12-06 09:32:07.515335: step 9740, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.824 sec/batch; 73h:49m:58s remains)
INFO - root - 2017-12-06 09:32:16.779228: step 9750, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 83h:37m:32s remains)
INFO - root - 2017-12-06 09:32:25.809071: step 9760, loss = 2.04, batch loss = 1.98 (9.9 examples/sec; 0.805 sec/batch; 72h:09m:15s remains)
INFO - root - 2017-12-06 09:32:35.221966: step 9770, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.928 sec/batch; 83h:11m:41s remains)
INFO - root - 2017-12-06 09:32:44.605697: step 9780, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.988 sec/batch; 88h:36m:08s remains)
INFO - root - 2017-12-06 09:32:53.902346: step 9790, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.963 sec/batch; 86h:17m:24s remains)
INFO - root - 2017-12-06 09:33:03.244071: step 9800, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 80h:42m:31s remains)
2017-12-06 09:33:03.927501: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3466372 -4.3474464 -4.3451252 -4.3410544 -4.3368468 -4.3342962 -4.3343296 -4.3364582 -4.3392258 -4.3416591 -4.3432379 -4.3443642 -4.3453283 -4.3457041 -4.3459945][-4.3604212 -4.3591595 -4.35301 -4.3431115 -4.3319111 -4.3238053 -4.3214173 -4.3243079 -4.3306112 -4.3383884 -4.3454146 -4.3511271 -4.3553514 -4.3583312 -4.3605361][-4.3658605 -4.3605781 -4.3473268 -4.3274608 -4.3047085 -4.2858772 -4.2756977 -4.2745385 -4.2826924 -4.2982607 -4.3162746 -4.332705 -4.3460712 -4.3560305 -4.3633432][-4.3626237 -4.3502736 -4.3250189 -4.2880406 -4.2454047 -4.2074413 -4.1829529 -4.1740737 -4.184813 -4.2124858 -4.2484455 -4.2838173 -4.3133287 -4.3353758 -4.3515124][-4.3577604 -4.3376055 -4.2984033 -4.240315 -4.1729088 -4.1124125 -4.0703068 -4.0508947 -4.063139 -4.1041822 -4.1594977 -4.2154684 -4.2639947 -4.3009605 -4.3287797][-4.3575144 -4.3339238 -4.2863059 -4.2129211 -4.1277204 -4.0531363 -4.0000973 -3.9715078 -3.9789014 -4.0205393 -4.0816293 -4.1461282 -4.2058363 -4.2545376 -4.2952843][-4.362844 -4.3405361 -4.2919383 -4.2150545 -4.1284318 -4.0564165 -4.0081248 -3.9807839 -3.9809237 -4.0087953 -4.0570064 -4.1113343 -4.1661115 -4.2172976 -4.265903][-4.3685756 -4.3505864 -4.3077331 -4.2388582 -4.1643057 -4.1060791 -4.0688453 -4.0464568 -4.0403929 -4.0522418 -4.08262 -4.1210818 -4.1639342 -4.2092361 -4.2572813][-4.3732028 -4.3611574 -4.3305063 -4.2801423 -4.2260971 -4.18395 -4.1558409 -4.1353688 -4.1221037 -4.12006 -4.1348104 -4.1608143 -4.1935368 -4.2306638 -4.2716856][-4.3747125 -4.3679466 -4.3508778 -4.3225193 -4.2919259 -4.2670565 -4.2486119 -4.2319646 -4.2164221 -4.2060213 -4.2082605 -4.2221155 -4.2438173 -4.2702103 -4.2998538][-4.3739743 -4.3701472 -4.3625684 -4.3500924 -4.3367977 -4.3262167 -4.3183675 -4.310101 -4.2993093 -4.2875214 -4.2821865 -4.2850747 -4.2942548 -4.309032 -4.3272562][-4.3723516 -4.369967 -4.3672352 -4.3632808 -4.3592334 -4.3565979 -4.3556342 -4.3542056 -4.349462 -4.3408003 -4.3334627 -4.33 -4.330328 -4.3362727 -4.3456068][-4.3708773 -4.368154 -4.3659492 -4.3642592 -4.3635306 -4.3644204 -4.3668251 -4.3686562 -4.3681865 -4.3643088 -4.3594661 -4.3552551 -4.3525429 -4.3522344 -4.3530664][-4.3702464 -4.366437 -4.362597 -4.3594208 -4.3578262 -4.3579698 -4.3595724 -4.3615074 -4.3630743 -4.3634391 -4.3631244 -4.3623652 -4.3607926 -4.3580909 -4.3541803][-4.3713894 -4.3676438 -4.3630681 -4.35846 -4.3549476 -4.3530626 -4.3529277 -4.353807 -4.3554845 -4.3576412 -4.3598876 -4.3617234 -4.3623204 -4.3601804 -4.3548741]]...]
INFO - root - 2017-12-06 09:33:13.092482: step 9810, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 75h:33m:03s remains)
INFO - root - 2017-12-06 09:33:22.430883: step 9820, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.955 sec/batch; 85h:36m:33s remains)
INFO - root - 2017-12-06 09:33:31.569369: step 9830, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.925 sec/batch; 82h:56m:54s remains)
INFO - root - 2017-12-06 09:33:40.810458: step 9840, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 82h:00m:02s remains)
INFO - root - 2017-12-06 09:33:49.968843: step 9850, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.932 sec/batch; 83h:30m:42s remains)
INFO - root - 2017-12-06 09:33:59.083613: step 9860, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 80h:28m:21s remains)
INFO - root - 2017-12-06 09:34:08.257300: step 9870, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.918 sec/batch; 82h:15m:56s remains)
INFO - root - 2017-12-06 09:34:17.384179: step 9880, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.908 sec/batch; 81h:24m:13s remains)
INFO - root - 2017-12-06 09:34:26.485891: step 9890, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.921 sec/batch; 82h:29m:33s remains)
INFO - root - 2017-12-06 09:34:35.516606: step 9900, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 83h:56m:12s remains)
2017-12-06 09:34:36.204161: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2654462 -4.2758684 -4.2814379 -4.2823439 -4.2819719 -4.2780638 -4.2740788 -4.2714887 -4.2682157 -4.2628093 -4.2525959 -4.2373214 -4.223371 -4.2085443 -4.1992][-4.2583632 -4.2659941 -4.2674923 -4.2621417 -4.2544088 -4.2475648 -4.2422409 -4.23927 -4.2369289 -4.2315903 -4.2213178 -4.20543 -4.1865454 -4.1658988 -4.1559515][-4.2505012 -4.2491198 -4.244215 -4.234107 -4.2223482 -4.2132607 -4.20336 -4.1987629 -4.2023826 -4.2072048 -4.2049789 -4.1919274 -4.1671491 -4.1410279 -4.12763][-4.2484903 -4.2362819 -4.2239923 -4.2083249 -4.19361 -4.1813145 -4.1651883 -4.1594496 -4.1708169 -4.1885338 -4.1963291 -4.1908827 -4.169095 -4.1428714 -4.1204853][-4.2489486 -4.2306547 -4.2108207 -4.1897545 -4.1704359 -4.1527143 -4.1311755 -4.1225338 -4.1432905 -4.1726437 -4.190073 -4.1950846 -4.180923 -4.1580009 -4.1241493][-4.24607 -4.2237988 -4.19692 -4.1692586 -4.1411109 -4.1180911 -4.0968337 -4.0922651 -4.120573 -4.1553774 -4.1778779 -4.1936989 -4.1883931 -4.1683049 -4.1297879][-4.2235265 -4.1994834 -4.1711435 -4.1380367 -4.1032658 -4.0777097 -4.06674 -4.0755596 -4.1058893 -4.1393051 -4.1649408 -4.1865234 -4.1841516 -4.1678209 -4.1369457][-4.1898136 -4.1700077 -4.144176 -4.1088791 -4.0695238 -4.043529 -4.0486832 -4.0715594 -4.1030388 -4.1359177 -4.1628847 -4.1792908 -4.1713114 -4.1584792 -4.1416011][-4.1607842 -4.1356564 -4.1080251 -4.0761433 -4.0426378 -4.022038 -4.0376034 -4.0624986 -4.095243 -4.1305079 -4.1551127 -4.1664314 -4.1569881 -4.1465116 -4.1346478][-4.1446319 -4.1081018 -4.0771236 -4.0544033 -4.0355926 -4.0280671 -4.0469851 -4.0672107 -4.0984941 -4.1307116 -4.1508126 -4.1614628 -4.1568923 -4.1453495 -4.1279106][-4.1429381 -4.09558 -4.0618148 -4.04866 -4.0470858 -4.0552983 -4.076601 -4.0930214 -4.1125245 -4.1310716 -4.1506391 -4.168108 -4.1688156 -4.1557817 -4.1311188][-4.1472163 -4.1032162 -4.0746808 -4.0672951 -4.0748587 -4.0947156 -4.1179986 -4.1275272 -4.1330314 -4.1420503 -4.1655741 -4.1887436 -4.1929455 -4.1774793 -4.1463971][-4.1526823 -4.1179738 -4.09871 -4.0984011 -4.1134009 -4.13811 -4.1607718 -4.1679225 -4.1714263 -4.1800485 -4.2011023 -4.2185154 -4.2203937 -4.2049775 -4.1722007][-4.1769361 -4.1502223 -4.13836 -4.1439037 -4.1624527 -4.1859069 -4.204987 -4.2141113 -4.2252245 -4.2364583 -4.2467165 -4.2533312 -4.2530112 -4.2407751 -4.2137771][-4.2322993 -4.216619 -4.2124395 -4.2192645 -4.2349844 -4.2501369 -4.2593737 -4.2659063 -4.2765965 -4.2859612 -4.2866898 -4.2865639 -4.2862887 -4.2788076 -4.2616205]]...]
INFO - root - 2017-12-06 09:34:45.349629: step 9910, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 83h:32m:52s remains)
INFO - root - 2017-12-06 09:34:54.466986: step 9920, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 82h:59m:22s remains)
INFO - root - 2017-12-06 09:35:03.630963: step 9930, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 83h:30m:11s remains)
INFO - root - 2017-12-06 09:35:12.796778: step 9940, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 82h:58m:41s remains)
INFO - root - 2017-12-06 09:35:21.922200: step 9950, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 83h:40m:09s remains)
INFO - root - 2017-12-06 09:35:30.707320: step 9960, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.884 sec/batch; 79h:11m:29s remains)
INFO - root - 2017-12-06 09:35:40.002626: step 9970, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 85h:59m:51s remains)
INFO - root - 2017-12-06 09:35:49.254236: step 9980, loss = 2.05, batch loss = 2.00 (8.1 examples/sec; 0.993 sec/batch; 88h:55m:11s remains)
INFO - root - 2017-12-06 09:35:58.484522: step 9990, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 79h:40m:40s remains)
INFO - root - 2017-12-06 09:36:07.717051: step 10000, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 82h:36m:47s remains)
2017-12-06 09:36:08.484762: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3129148 -4.3130975 -4.3065305 -4.2989931 -4.293426 -4.2861385 -4.2789025 -4.28014 -4.2870889 -4.2935724 -4.2963419 -4.2999897 -4.3024273 -4.3048563 -4.3061647][-4.310214 -4.3122907 -4.304718 -4.2923684 -4.2816806 -4.26899 -4.2552185 -4.2531877 -4.2597818 -4.2667327 -4.2715673 -4.2790108 -4.2863455 -4.2929192 -4.29642][-4.3006568 -4.301167 -4.2880459 -4.2693782 -4.2520514 -4.2329197 -4.2154732 -4.2110744 -4.218946 -4.2302113 -4.2369046 -4.247 -4.257957 -4.2688251 -4.2764859][-4.2938414 -4.2918515 -4.2735848 -4.2478275 -4.2206297 -4.1935244 -4.1707311 -4.1621566 -4.1751966 -4.1937337 -4.2046618 -4.2144971 -4.2251134 -4.2370105 -4.2503257][-4.2899022 -4.2858667 -4.262486 -4.2278104 -4.1866441 -4.1444244 -4.1081276 -4.0917511 -4.1148872 -4.1504641 -4.1781878 -4.193027 -4.2016506 -4.2133923 -4.2303591][-4.27633 -4.2705045 -4.2434974 -4.1983981 -4.1414733 -4.0768089 -4.0111594 -3.9772944 -4.0170636 -4.0866084 -4.1437407 -4.1783438 -4.1926274 -4.20351 -4.220264][-4.2570224 -4.2512321 -4.2229166 -4.1728444 -4.1033821 -4.0130653 -3.9028194 -3.8265243 -3.8766513 -3.9919899 -4.0881796 -4.1478672 -4.1753187 -4.1921058 -4.2119093][-4.2433791 -4.2397509 -4.2142253 -4.167275 -4.08942 -3.9743726 -3.8246758 -3.7001419 -3.7498102 -3.90866 -4.0382314 -4.1208825 -4.1665974 -4.1936336 -4.2137871][-4.2394061 -4.2377958 -4.2203603 -4.1906786 -4.1272931 -4.0280128 -3.8991296 -3.7900777 -3.8219006 -3.950645 -4.0649505 -4.1447196 -4.1901298 -4.2132282 -4.2260661][-4.2364469 -4.2369356 -4.2262969 -4.2104859 -4.1752272 -4.1137457 -4.0345168 -3.9667058 -3.9871795 -4.0667562 -4.1421776 -4.198576 -4.2317591 -4.2446713 -4.2494216][-4.238924 -4.2410073 -4.2358713 -4.2265906 -4.211237 -4.1863909 -4.1455064 -4.1103044 -4.1308017 -4.182395 -4.2273293 -4.2608414 -4.28459 -4.2904758 -4.2867832][-4.2581482 -4.2680736 -4.2724462 -4.2704949 -4.2672529 -4.2596679 -4.2412305 -4.2223449 -4.23948 -4.2720451 -4.2954187 -4.313448 -4.3273139 -4.32618 -4.316155][-4.2766128 -4.2923975 -4.3046827 -4.31121 -4.3155117 -4.3127561 -4.303916 -4.2944269 -4.3024611 -4.3182745 -4.3286386 -4.3378658 -4.3436909 -4.338583 -4.3268476][-4.2893286 -4.3034549 -4.3160534 -4.3266873 -4.3338528 -4.3330078 -4.3273354 -4.3235126 -4.3259192 -4.3333616 -4.3384824 -4.3435903 -4.344903 -4.3395481 -4.3296018][-4.2979431 -4.305964 -4.3132596 -4.3214164 -4.327456 -4.3269577 -4.3231921 -4.3216424 -4.3229542 -4.3269095 -4.3304057 -4.3342829 -4.3355646 -4.3321934 -4.3255954]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fix/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fix/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 09:36:18.355794: step 10010, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.948 sec/batch; 84h:52m:58s remains)
INFO - root - 2017-12-06 09:36:27.589200: step 10020, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 81h:44m:23s remains)
INFO - root - 2017-12-06 09:36:36.530913: step 10030, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 79h:19m:20s remains)
INFO - root - 2017-12-06 09:36:45.886348: step 10040, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 80h:51m:21s remains)
INFO - root - 2017-12-06 09:36:55.080067: step 10050, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 81h:05m:43s remains)
INFO - root - 2017-12-06 09:37:04.227930: step 10060, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.958 sec/batch; 85h:48m:26s remains)
INFO - root - 2017-12-06 09:37:13.361414: step 10070, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.924 sec/batch; 82h:43m:41s remains)
INFO - root - 2017-12-06 09:37:22.489619: step 10080, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 82h:53m:33s remains)
INFO - root - 2017-12-06 09:37:31.674057: step 10090, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 85h:41m:22s remains)
INFO - root - 2017-12-06 09:37:40.589660: step 10100, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 84h:10m:13s remains)
2017-12-06 09:37:41.275588: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1854033 -4.1964316 -4.2049012 -4.2085576 -4.1967368 -4.1822619 -4.1748581 -4.1660247 -4.1592546 -4.1523676 -4.1476707 -4.1555357 -4.1814513 -4.2062068 -4.2134995][-4.1891975 -4.1965237 -4.1987686 -4.1969171 -4.183567 -4.1716423 -4.169137 -4.1630349 -4.1549606 -4.1445551 -4.1382065 -4.1473556 -4.1734266 -4.19791 -4.2086425][-4.1974998 -4.1991463 -4.19605 -4.1866632 -4.1692019 -4.1587806 -4.1612706 -4.1616721 -4.1562486 -4.1450238 -4.1388888 -4.1479068 -4.1687851 -4.1890993 -4.2030535][-4.2076454 -4.2091289 -4.2007046 -4.1823645 -4.1592369 -4.1481743 -4.1559849 -4.1670136 -4.1703815 -4.1634445 -4.1590538 -4.1651 -4.1754994 -4.1884151 -4.20418][-4.2159581 -4.2174821 -4.2075052 -4.1840887 -4.159421 -4.1484127 -4.157403 -4.1735497 -4.1835265 -4.1854072 -4.1886992 -4.1925755 -4.1933308 -4.1982574 -4.2129869][-4.2214031 -4.2190318 -4.2042289 -4.1772656 -4.1523676 -4.1410151 -4.1513329 -4.1715293 -4.1873779 -4.1989822 -4.2118721 -4.2167759 -4.2103834 -4.2088647 -4.2200456][-4.2037511 -4.1990461 -4.1839576 -4.1563072 -4.1290178 -4.1118212 -4.1156297 -4.1359248 -4.1562738 -4.1732087 -4.1924858 -4.1998291 -4.1889358 -4.1819506 -4.1885176][-4.1487894 -4.1444354 -4.1289539 -4.10253 -4.0723953 -4.044178 -4.031992 -4.0445986 -4.0723095 -4.1004953 -4.1306057 -4.1434717 -4.1311507 -4.120399 -4.1234784][-4.09583 -4.0952711 -4.0807714 -4.0545173 -4.01815 -3.9727309 -3.9385145 -3.9388537 -3.9721467 -4.0178337 -4.0662732 -4.0905948 -4.0826635 -4.0711203 -4.0704422][-4.1119623 -4.11973 -4.1120563 -4.0946565 -4.0610595 -4.0140052 -3.9755101 -3.9639871 -3.9818165 -4.0213866 -4.0711317 -4.101851 -4.1037178 -4.1002989 -4.1004162][-4.1799006 -4.1923361 -4.1924248 -4.1812353 -4.1509929 -4.1133428 -4.0845761 -4.0700431 -4.0686536 -4.0843892 -4.1190658 -4.1461997 -4.1524062 -4.157764 -4.1605115][-4.2332716 -4.2447309 -4.2465086 -4.2356358 -4.2074475 -4.1780782 -4.1561928 -4.1394634 -4.12575 -4.1250205 -4.1463976 -4.167129 -4.1722822 -4.1810865 -4.1887541][-4.2457352 -4.255096 -4.256557 -4.2460461 -4.2233057 -4.2042341 -4.1915221 -4.1792426 -4.1631293 -4.1567316 -4.1670103 -4.1758623 -4.176537 -4.1871753 -4.2004905][-4.2501311 -4.2548513 -4.2547588 -4.2460179 -4.2303014 -4.21851 -4.2126837 -4.2083731 -4.19967 -4.1954651 -4.1976123 -4.1972685 -4.1937923 -4.2010756 -4.2135873][-4.2463956 -4.2476358 -4.247066 -4.2393484 -4.2278352 -4.220664 -4.2176123 -4.2157645 -4.2141404 -4.2185984 -4.2216206 -4.2216883 -4.2175865 -4.2171216 -4.2212796]]...]
INFO - root - 2017-12-06 09:37:50.437036: step 10110, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 81h:05m:26s remains)
INFO - root - 2017-12-06 09:37:59.760933: step 10120, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.904 sec/batch; 80h:59m:04s remains)
INFO - root - 2017-12-06 09:38:09.021877: step 10130, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 84h:26m:58s remains)
INFO - root - 2017-12-06 09:38:18.149600: step 10140, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 82h:20m:27s remains)
INFO - root - 2017-12-06 09:38:27.373616: step 10150, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 81h:05m:36s remains)
INFO - root - 2017-12-06 09:38:36.369750: step 10160, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.943 sec/batch; 84h:24m:29s remains)
INFO - root - 2017-12-06 09:38:45.331723: step 10170, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.899 sec/batch; 80h:29m:12s remains)
INFO - root - 2017-12-06 09:38:54.607734: step 10180, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.957 sec/batch; 85h:42m:20s remains)
INFO - root - 2017-12-06 09:39:03.898630: step 10190, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.898 sec/batch; 80h:26m:17s remains)
INFO - root - 2017-12-06 09:39:12.986439: step 10200, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 87h:13m:14s remains)
2017-12-06 09:39:13.618791: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2755723 -4.2617779 -4.2501512 -4.2393937 -4.2285609 -4.2221003 -4.208714 -4.195384 -4.1806912 -4.1721754 -4.1713896 -4.1699357 -4.166801 -4.1626759 -4.1532559][-4.2611332 -4.2423186 -4.2276936 -4.2120504 -4.1968632 -4.191184 -4.1798034 -4.1665826 -4.1498756 -4.1376634 -4.1326542 -4.1290441 -4.1276789 -4.1249113 -4.1172304][-4.2367134 -4.2163134 -4.2025204 -4.185463 -4.1667595 -4.1623921 -4.1519384 -4.1369224 -4.1226387 -4.115449 -4.1117015 -4.1112061 -4.1147175 -4.1153874 -4.1122942][-4.2088385 -4.1928649 -4.1847396 -4.1745834 -4.1600676 -4.1570463 -4.1431489 -4.1180739 -4.1008921 -4.1006441 -4.1036406 -4.1035762 -4.1091309 -4.1135659 -4.1162686][-4.1934319 -4.1804781 -4.176609 -4.1735682 -4.1652451 -4.1653547 -4.1445723 -4.098299 -4.072772 -4.0769176 -4.0786052 -4.0740609 -4.0790815 -4.0901766 -4.0993886][-4.1803827 -4.1675611 -4.1667275 -4.1672316 -4.1574578 -4.1504741 -4.1112518 -4.040462 -4.0084057 -4.026464 -4.0359764 -4.032382 -4.0422945 -4.0622616 -4.072309][-4.1637354 -4.151897 -4.149827 -4.1449957 -4.122673 -4.0983109 -4.032187 -3.9382985 -3.9156783 -3.9648125 -3.9943798 -4.0031314 -4.0276027 -4.0585732 -4.0716324][-4.1192441 -4.0982804 -4.0842886 -4.0664268 -4.0308323 -3.9966869 -3.9267218 -3.8414552 -3.8507111 -3.9302256 -3.9777064 -4.0025096 -4.0380707 -4.0720205 -4.0853672][-4.0585356 -4.0194645 -3.9898405 -3.9653976 -3.9375494 -3.9186714 -3.8775244 -3.8367851 -3.8706093 -3.9454479 -3.9908943 -4.0179462 -4.0516891 -4.0794272 -4.0938416][-4.02648 -3.977777 -3.9399076 -3.9199915 -3.9106829 -3.9113212 -3.8987317 -3.8932858 -3.9317067 -3.9845109 -4.0132794 -4.0315356 -4.0554667 -4.0752573 -4.0901527][-4.0536346 -4.0073829 -3.9684417 -3.9497526 -3.9490047 -3.9583421 -3.9599221 -3.9657228 -3.9941423 -4.0253758 -4.0405836 -4.0534883 -4.0730877 -4.0908394 -4.1055155][-4.1208014 -4.0823469 -4.0457988 -4.0254493 -4.0222325 -4.031333 -4.0365171 -4.04441 -4.064497 -4.0832138 -4.0910044 -4.0999737 -4.1162639 -4.132782 -4.1459823][-4.1910124 -4.1629777 -4.133244 -4.1156688 -4.1102986 -4.1139188 -4.1177287 -4.1263885 -4.1418357 -4.1546063 -4.1605506 -4.1667981 -4.1790829 -4.192028 -4.2021012][-4.2526169 -4.2366219 -4.2163424 -4.2036543 -4.1996326 -4.2036595 -4.2100954 -4.2186246 -4.22786 -4.234365 -4.237587 -4.2408795 -4.2471404 -4.2538838 -4.2593517][-4.2971869 -4.29025 -4.2788463 -4.2724552 -4.2724128 -4.2779984 -4.2842989 -4.290638 -4.2952614 -4.2969384 -4.2972875 -4.2974505 -4.2987895 -4.3011718 -4.3042006]]...]
INFO - root - 2017-12-06 09:39:22.952860: step 10210, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 83h:38m:23s remains)
INFO - root - 2017-12-06 09:39:32.175304: step 10220, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 80h:50m:55s remains)
INFO - root - 2017-12-06 09:39:41.286881: step 10230, loss = 2.11, batch loss = 2.05 (8.9 examples/sec; 0.900 sec/batch; 80h:32m:31s remains)
INFO - root - 2017-12-06 09:39:50.317882: step 10240, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.902 sec/batch; 80h:44m:39s remains)
INFO - root - 2017-12-06 09:39:59.522385: step 10250, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 83h:32m:15s remains)
INFO - root - 2017-12-06 09:40:09.029668: step 10260, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 83h:16m:54s remains)
INFO - root - 2017-12-06 09:40:18.192185: step 10270, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 81h:08m:23s remains)
INFO - root - 2017-12-06 09:40:27.538506: step 10280, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.945 sec/batch; 84h:32m:46s remains)
INFO - root - 2017-12-06 09:40:36.677158: step 10290, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.829 sec/batch; 74h:12m:03s remains)
INFO - root - 2017-12-06 09:40:45.857992: step 10300, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.947 sec/batch; 84h:47m:45s remains)
2017-12-06 09:40:46.522000: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3510904 -4.3555193 -4.3550205 -4.3487225 -4.3375916 -4.3263822 -4.31981 -4.3210449 -4.3278437 -4.3348317 -4.3396225 -4.3413191 -4.3417645 -4.3421226 -4.3432918][-4.3566494 -4.3610945 -4.358973 -4.3485518 -4.33288 -4.3157473 -4.30411 -4.3045821 -4.3152151 -4.3268685 -4.3360791 -4.3418341 -4.3452997 -4.3463655 -4.3472333][-4.3610692 -4.365375 -4.3615804 -4.3467245 -4.3239956 -4.2966604 -4.274436 -4.2700419 -4.2849097 -4.3067069 -4.3243666 -4.3362751 -4.3445759 -4.3488407 -4.3503218][-4.3625045 -4.3663373 -4.3616447 -4.3429585 -4.312058 -4.2706404 -4.2308016 -4.214231 -4.2303214 -4.2639489 -4.2951517 -4.3191652 -4.3358865 -4.344636 -4.3479452][-4.3613896 -4.3664045 -4.362639 -4.3402839 -4.2992363 -4.2377319 -4.17296 -4.1369538 -4.1488438 -4.1925631 -4.2418561 -4.2848859 -4.3148379 -4.331687 -4.3398986][-4.3614149 -4.3679562 -4.3651567 -4.3378243 -4.282845 -4.1954808 -4.098609 -4.0382085 -4.0427513 -4.0967731 -4.1684752 -4.2373314 -4.286871 -4.3152795 -4.3313713][-4.3533759 -4.36284 -4.3615265 -4.3294187 -4.2580032 -4.1417265 -4.0109339 -3.9252307 -3.9216957 -3.9898283 -4.089283 -4.1854081 -4.2542205 -4.2958374 -4.322113][-4.328454 -4.3412647 -4.3423133 -4.3087687 -4.2278838 -4.0889373 -3.9274292 -3.8156228 -3.8037138 -3.8926151 -4.0240111 -4.14813 -4.2346277 -4.2856693 -4.3176751][-4.2841196 -4.299819 -4.3057938 -4.277133 -4.2028813 -4.0706048 -3.912498 -3.7964287 -3.7746191 -3.8639078 -4.0079637 -4.1447396 -4.2388358 -4.2919431 -4.3231606][-4.2123966 -4.2367611 -4.2560334 -4.2471695 -4.20137 -4.1063643 -3.990098 -3.8975611 -3.8690786 -3.9343851 -4.0570469 -4.1793637 -4.2631769 -4.3086472 -4.3329062][-4.1461444 -4.1864343 -4.2240205 -4.2390428 -4.2218938 -4.1642933 -4.0884933 -4.0240316 -3.9993453 -4.042264 -4.1325011 -4.2281322 -4.2931747 -4.3258038 -4.3415995][-4.1359854 -4.1893425 -4.2369537 -4.2630606 -4.2589927 -4.2228851 -4.174943 -4.1338987 -4.1191363 -4.1480141 -4.2095079 -4.2759118 -4.3207645 -4.3391833 -4.346261][-4.18848 -4.2401381 -4.2833581 -4.3067579 -4.3063788 -4.2830706 -4.2530923 -4.2293191 -4.221468 -4.236907 -4.2737794 -4.3144841 -4.3404031 -4.3484325 -4.3496704][-4.2633924 -4.300323 -4.3291893 -4.3432174 -4.3411965 -4.3279195 -4.3114014 -4.2972541 -4.2915616 -4.2977223 -4.3171768 -4.3387127 -4.3500967 -4.352088 -4.3514824][-4.3197632 -4.3396921 -4.352953 -4.3579865 -4.3550081 -4.3481307 -4.3397636 -4.3316121 -4.3275018 -4.3297663 -4.339324 -4.3485613 -4.3514023 -4.350904 -4.3503542]]...]
INFO - root - 2017-12-06 09:40:55.678764: step 10310, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.998 sec/batch; 89h:21m:35s remains)
INFO - root - 2017-12-06 09:41:04.998451: step 10320, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.949 sec/batch; 84h:57m:09s remains)
INFO - root - 2017-12-06 09:41:14.158725: step 10330, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 83h:19m:24s remains)
INFO - root - 2017-12-06 09:41:23.249588: step 10340, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 82h:54m:46s remains)
INFO - root - 2017-12-06 09:41:32.483769: step 10350, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.001 sec/batch; 89h:34m:56s remains)
INFO - root - 2017-12-06 09:41:41.511747: step 10360, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 87h:24m:48s remains)
INFO - root - 2017-12-06 09:41:50.542735: step 10370, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 82h:01m:46s remains)
INFO - root - 2017-12-06 09:41:59.585097: step 10380, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 83h:53m:37s remains)
INFO - root - 2017-12-06 09:42:08.883026: step 10390, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 82h:35m:56s remains)
INFO - root - 2017-12-06 09:42:18.174467: step 10400, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 85h:37m:05s remains)
2017-12-06 09:42:18.895379: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3340006 -4.3332143 -4.3235331 -4.3024135 -4.2739239 -4.2480631 -4.238822 -4.2536979 -4.2822065 -4.3073297 -4.322629 -4.330864 -4.3325448 -4.3310637 -4.3298988][-4.3355279 -4.3321433 -4.3182836 -4.2918167 -4.2580504 -4.2298975 -4.2237582 -4.2443647 -4.2773438 -4.304688 -4.3212314 -4.3307629 -4.3325973 -4.3311391 -4.3304119][-4.3337722 -4.3277183 -4.3093038 -4.276679 -4.2366309 -4.2083635 -4.2073169 -4.2336378 -4.2708454 -4.3005848 -4.31848 -4.3295217 -4.3323059 -4.3312693 -4.3309431][-4.3211155 -4.3108749 -4.283761 -4.240684 -4.194108 -4.1694193 -4.176198 -4.2092533 -4.2524729 -4.2880735 -4.3114405 -4.3266668 -4.3322392 -4.3323345 -4.3323631][-4.2896843 -4.2715983 -4.2299132 -4.1731052 -4.1228042 -4.1050763 -4.1219163 -4.16036 -4.2084417 -4.2536955 -4.2887063 -4.3128843 -4.3251996 -4.3293319 -4.3319097][-4.2385616 -4.2091174 -4.1521568 -4.0854139 -4.0389833 -4.033144 -4.0585303 -4.096559 -4.1440649 -4.1985497 -4.2497845 -4.2877288 -4.3098946 -4.3207393 -4.3271432][-4.1868548 -4.1459312 -4.0785613 -4.0112181 -3.9754066 -3.98147 -4.0114584 -4.0431294 -4.0820374 -4.1404524 -4.205204 -4.2561116 -4.2905884 -4.31024 -4.321218][-4.1561489 -4.1113763 -4.0454054 -3.9876344 -3.9630578 -3.9734728 -3.9986413 -4.0182228 -4.0443711 -4.1009722 -4.1719446 -4.2307968 -4.2751441 -4.3026323 -4.3185153][-4.1439018 -4.1024384 -4.0485888 -4.006216 -3.9908578 -4.0005274 -4.0170445 -4.0240355 -4.0398712 -4.09262 -4.1622739 -4.222106 -4.2707119 -4.30189 -4.3199115][-4.148653 -4.1143017 -4.0753374 -4.0462923 -4.0371771 -4.043273 -4.0508485 -4.0484004 -4.0580187 -4.1044645 -4.1663346 -4.2216249 -4.2699432 -4.3025227 -4.3207188][-4.17168 -4.1454568 -4.1199603 -4.1033936 -4.1000772 -4.1058393 -4.1095443 -4.1030488 -4.1075444 -4.14166 -4.1874909 -4.23247 -4.2737694 -4.3030124 -4.3194184][-4.2049036 -4.1902118 -4.1783466 -4.1735473 -4.1763453 -4.1829343 -4.1848454 -4.1776562 -4.1772003 -4.1967087 -4.2262096 -4.2586884 -4.2890148 -4.3097639 -4.3198605][-4.2344279 -4.227108 -4.2243061 -4.2279916 -4.23592 -4.2438345 -4.2467661 -4.2425466 -4.239543 -4.2479377 -4.26536 -4.2876911 -4.3071742 -4.3188467 -4.3214855][-4.2645459 -4.2600341 -4.2600121 -4.2650042 -4.2722492 -4.2787747 -4.2818022 -4.2794232 -4.2759347 -4.2781944 -4.2877121 -4.3027306 -4.3153386 -4.3210359 -4.3192477][-4.292727 -4.2896605 -4.288929 -4.2909012 -4.2940135 -4.296824 -4.2980771 -4.2963467 -4.2937937 -4.2941642 -4.2988763 -4.3075628 -4.3143077 -4.3162093 -4.3139796]]...]
INFO - root - 2017-12-06 09:42:28.057549: step 10410, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.925 sec/batch; 82h:45m:19s remains)
INFO - root - 2017-12-06 09:42:37.307804: step 10420, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.938 sec/batch; 83h:57m:02s remains)
INFO - root - 2017-12-06 09:42:46.456272: step 10430, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.947 sec/batch; 84h:41m:27s remains)
INFO - root - 2017-12-06 09:42:55.622556: step 10440, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 81h:46m:30s remains)
INFO - root - 2017-12-06 09:43:04.624698: step 10450, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.878 sec/batch; 78h:30m:53s remains)
INFO - root - 2017-12-06 09:43:13.758745: step 10460, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.890 sec/batch; 79h:34m:19s remains)
INFO - root - 2017-12-06 09:43:22.992615: step 10470, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 83h:45m:00s remains)
INFO - root - 2017-12-06 09:43:32.250240: step 10480, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 82h:44m:11s remains)
INFO - root - 2017-12-06 09:43:41.290205: step 10490, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 83h:46m:30s remains)
INFO - root - 2017-12-06 09:43:50.456583: step 10500, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 84h:16m:29s remains)
2017-12-06 09:43:51.165388: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0220685 -4.0053992 -4.03996 -4.0932183 -4.1304359 -4.1372128 -4.1287665 -4.1320133 -4.152823 -4.180603 -4.2042713 -4.2215552 -4.2308149 -4.2357488 -4.2468033][-4.0297322 -4.0041728 -4.039814 -4.1002474 -4.1441197 -4.15968 -4.1533246 -4.1513572 -4.1680856 -4.195334 -4.2164922 -4.2276831 -4.2314234 -4.2370272 -4.2506495][-4.0563645 -4.0298662 -4.059659 -4.1125927 -4.1450987 -4.1558161 -4.1492729 -4.1472282 -4.1638622 -4.1908464 -4.2135482 -4.2247076 -4.2269917 -4.2351933 -4.2529793][-4.0969787 -4.0700431 -4.0853057 -4.1175776 -4.1282864 -4.1236625 -4.1137462 -4.1162052 -4.1373043 -4.1671948 -4.1967463 -4.2137771 -4.2205319 -4.2324171 -4.2533054][-4.133985 -4.1179285 -4.1237435 -4.1324115 -4.118578 -4.0980182 -4.0865555 -4.0952454 -4.1166291 -4.1448569 -4.1764073 -4.2007461 -4.2132506 -4.2281456 -4.2515283][-4.1543617 -4.1541924 -4.15651 -4.1438732 -4.1137276 -4.0872955 -4.0818753 -4.0968432 -4.1131325 -4.1317182 -4.1572518 -4.1848016 -4.2021956 -4.2207294 -4.2476954][-4.1713948 -4.1772003 -4.1717577 -4.1450734 -4.1102285 -4.0922384 -4.0964375 -4.1137328 -4.12223 -4.1283441 -4.1463175 -4.1720877 -4.1905518 -4.2113438 -4.2425427][-4.1978555 -4.2039223 -4.1887164 -4.1504579 -4.1174035 -4.1142011 -4.1269708 -4.1407423 -4.1423292 -4.1388612 -4.1471968 -4.16566 -4.1808209 -4.2026281 -4.2376442][-4.2105403 -4.2114406 -4.1953506 -4.1599426 -4.1344023 -4.1392641 -4.1562161 -4.1682286 -4.1648278 -4.1580696 -4.1585197 -4.163229 -4.1699471 -4.1905823 -4.2293315][-4.1969867 -4.1904893 -4.1788588 -4.1578913 -4.146307 -4.15864 -4.1807451 -4.1899376 -4.1791868 -4.1651726 -4.1585541 -4.15616 -4.1582651 -4.1775637 -4.2174354][-4.1608152 -4.1480083 -4.1427174 -4.1436081 -4.1527591 -4.1753325 -4.2010713 -4.2086563 -4.1868439 -4.1619439 -4.1459594 -4.1380849 -4.1425118 -4.1660538 -4.2068663][-4.1327939 -4.1073966 -4.1011739 -4.1229992 -4.1517658 -4.18133 -4.2093859 -4.2167354 -4.1919188 -4.1570921 -4.1323643 -4.1217079 -4.130702 -4.158968 -4.2007289][-4.1174107 -4.0829296 -4.0717635 -4.1047964 -4.1425023 -4.1745543 -4.2011271 -4.2114992 -4.1934586 -4.1586404 -4.1320629 -4.1213045 -4.1310458 -4.1584368 -4.1977191][-4.1188159 -4.0903406 -4.08203 -4.1103325 -4.1375346 -4.1569843 -4.176578 -4.1998019 -4.2006822 -4.1743979 -4.1528745 -4.1420555 -4.1454115 -4.1651039 -4.1979256][-4.1299171 -4.1109986 -4.1080031 -4.1274252 -4.140379 -4.1443858 -4.1618185 -4.1947379 -4.2074685 -4.1900172 -4.1754456 -4.16572 -4.1633973 -4.1745973 -4.2001052]]...]
INFO - root - 2017-12-06 09:44:00.407766: step 10510, loss = 2.11, batch loss = 2.05 (8.6 examples/sec; 0.932 sec/batch; 83h:19m:53s remains)
INFO - root - 2017-12-06 09:44:09.571471: step 10520, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.726 sec/batch; 64h:54m:38s remains)
INFO - root - 2017-12-06 09:44:18.756655: step 10530, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.891 sec/batch; 79h:42m:05s remains)
INFO - root - 2017-12-06 09:44:27.950473: step 10540, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 83h:06m:31s remains)
INFO - root - 2017-12-06 09:44:37.041523: step 10550, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 80h:42m:27s remains)
INFO - root - 2017-12-06 09:44:46.079658: step 10560, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 82h:24m:03s remains)
INFO - root - 2017-12-06 09:44:55.322242: step 10570, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 84h:10m:56s remains)
INFO - root - 2017-12-06 09:45:04.466314: step 10580, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 82h:22m:07s remains)
INFO - root - 2017-12-06 09:45:13.833934: step 10590, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 81h:53m:25s remains)
INFO - root - 2017-12-06 09:45:22.832565: step 10600, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.898 sec/batch; 80h:15m:50s remains)
2017-12-06 09:45:23.550930: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2181339 -4.1403408 -4.0577946 -4.0007439 -4.0307012 -4.1088681 -4.1592436 -4.1733546 -4.1708441 -4.1669436 -4.1757288 -4.1803694 -4.1811895 -4.1801224 -4.1676006][-4.2090697 -4.1377816 -4.0627594 -4.0129352 -4.0391059 -4.0986586 -4.1285949 -4.13064 -4.1213555 -4.1177444 -4.1383491 -4.1575818 -4.1719704 -4.1846666 -4.1880264][-4.2040553 -4.1420112 -4.0832286 -4.0473042 -4.0659103 -4.099143 -4.1074829 -4.1003175 -4.0910964 -4.0942173 -4.130619 -4.1659369 -4.1877551 -4.2009597 -4.2072477][-4.2161837 -4.1697927 -4.1328 -4.1133523 -4.123477 -4.1350126 -4.1266308 -4.1140132 -4.1052804 -4.111618 -4.1512647 -4.1900563 -4.210742 -4.2197042 -4.2257967][-4.2421594 -4.2081981 -4.1859846 -4.1780958 -4.1809726 -4.1819115 -4.175066 -4.1739841 -4.1722493 -4.17412 -4.1951809 -4.2160859 -4.2251639 -4.2301526 -4.2367487][-4.2630157 -4.2328773 -4.2150774 -4.2066565 -4.2029195 -4.1988626 -4.1986513 -4.2146068 -4.2302189 -4.2340426 -4.2306161 -4.2217631 -4.2145863 -4.21444 -4.2264261][-4.2724371 -4.2356195 -4.2049861 -4.1856441 -4.1724658 -4.159812 -4.1561575 -4.1780519 -4.2089968 -4.2257848 -4.2183247 -4.1979027 -4.1871786 -4.1931524 -4.2179813][-4.2609882 -4.2119384 -4.1671257 -4.1311936 -4.1018696 -4.077795 -4.0636039 -4.0762835 -4.1146555 -4.1419144 -4.1423793 -4.1362586 -4.1467175 -4.1762381 -4.2210565][-4.2297568 -4.1668005 -4.11316 -4.0716538 -4.0352631 -4.0046759 -3.9809206 -3.9762809 -4.0126677 -4.0475993 -4.0522976 -4.068068 -4.1131077 -4.1728373 -4.2314811][-4.2074089 -4.1346073 -4.0767283 -4.036037 -4.002192 -3.9763739 -3.9481282 -3.9277437 -3.9533978 -3.9804969 -3.9839594 -4.0258751 -4.1082692 -4.1867213 -4.2444205][-4.2049356 -4.1343026 -4.0799184 -4.0403624 -4.014936 -3.9991994 -3.9827924 -3.9662118 -3.9773011 -3.9849229 -3.9841514 -4.0416985 -4.1437359 -4.2234235 -4.2629757][-4.2160473 -4.1569591 -4.1166558 -4.0900245 -4.0767274 -4.0752411 -4.0777307 -4.0715966 -4.0754685 -4.0752616 -4.0742879 -4.1218781 -4.20542 -4.260498 -4.2687387][-4.2273955 -4.1799049 -4.1547632 -4.1459608 -4.1469827 -4.1538558 -4.1624432 -4.1577163 -4.161109 -4.1692419 -4.1794486 -4.210463 -4.2584171 -4.2800417 -4.2639508][-4.2395806 -4.2002797 -4.1834521 -4.185966 -4.1965284 -4.2080398 -4.2176838 -4.2110896 -4.2142625 -4.2277193 -4.2462697 -4.2696333 -4.2906642 -4.2878747 -4.2576022][-4.2476244 -4.212996 -4.2009006 -4.2095556 -4.2255707 -4.2386746 -4.2464666 -4.2434692 -4.2488942 -4.2626357 -4.2767057 -4.2928433 -4.2989631 -4.2860126 -4.2544646]]...]
INFO - root - 2017-12-06 09:45:32.722754: step 10610, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 81h:58m:34s remains)
INFO - root - 2017-12-06 09:45:41.936377: step 10620, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.925 sec/batch; 82h:40m:15s remains)
INFO - root - 2017-12-06 09:45:50.952576: step 10630, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 86h:50m:35s remains)
INFO - root - 2017-12-06 09:46:00.149489: step 10640, loss = 2.03, batch loss = 1.97 (8.4 examples/sec; 0.949 sec/batch; 84h:52m:11s remains)
INFO - root - 2017-12-06 09:46:09.340516: step 10650, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 76h:46m:07s remains)
INFO - root - 2017-12-06 09:46:18.719966: step 10660, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 82h:51m:54s remains)
INFO - root - 2017-12-06 09:46:27.825420: step 10670, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.940 sec/batch; 84h:04m:33s remains)
INFO - root - 2017-12-06 09:46:37.145319: step 10680, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 83h:13m:35s remains)
INFO - root - 2017-12-06 09:46:46.232570: step 10690, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 82h:42m:20s remains)
INFO - root - 2017-12-06 09:46:55.436850: step 10700, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 81h:31m:08s remains)
2017-12-06 09:46:56.162209: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1639128 -4.1776953 -4.2007647 -4.2256989 -4.2474217 -4.2627172 -4.2711086 -4.2758508 -4.275043 -4.2696614 -4.2626915 -4.2578359 -4.2551112 -4.2524877 -4.2492819][-4.1750126 -4.1883807 -4.2111683 -4.2357569 -4.2573185 -4.2714324 -4.2785783 -4.2819972 -4.2800808 -4.2740026 -4.265852 -4.2587504 -4.2542171 -4.25056 -4.2477732][-4.2010107 -4.2146 -4.2344742 -4.2537618 -4.2693224 -4.2775021 -4.2802978 -4.2813554 -4.2783451 -4.2721047 -4.2638 -4.2565732 -4.2522521 -4.2496276 -4.2485719][-4.2288532 -4.2403703 -4.2529297 -4.2629533 -4.2689581 -4.2698131 -4.2689409 -4.268435 -4.2662516 -4.2621264 -4.25576 -4.2504525 -4.2478476 -4.2470856 -4.2480712][-4.2541542 -4.2576804 -4.2586117 -4.2567458 -4.2521706 -4.2467608 -4.243577 -4.2436666 -4.2455096 -4.2473583 -4.2467232 -4.245954 -4.2467222 -4.2483048 -4.2507906][-4.2726049 -4.2671919 -4.2577758 -4.2453613 -4.2316747 -4.221334 -4.2170539 -4.2193208 -4.226573 -4.2356648 -4.24261 -4.2476573 -4.2525682 -4.2569146 -4.2605433][-4.2829237 -4.272902 -4.2588511 -4.2417617 -4.2245741 -4.2129145 -4.2092524 -4.2137618 -4.2239971 -4.2361169 -4.2462573 -4.2534976 -4.2594528 -4.2652192 -4.2704172][-4.2855453 -4.2764215 -4.26375 -4.248208 -4.2327394 -4.2225466 -4.2197533 -4.2243471 -4.2331095 -4.2423797 -4.249464 -4.2542887 -4.2589107 -4.2644649 -4.2710466][-4.283905 -4.2776022 -4.2681675 -4.2562389 -4.2440281 -4.2362208 -4.2346268 -4.2383528 -4.2435207 -4.2477012 -4.2500377 -4.2514024 -4.2538 -4.2579088 -4.2640877][-4.2820854 -4.2779946 -4.2715435 -4.2631865 -4.2544212 -4.24911 -4.2485137 -4.2508612 -4.2528658 -4.2536125 -4.253201 -4.2523842 -4.2528176 -4.2550144 -4.2593107][-4.2828417 -4.2801156 -4.27596 -4.2705722 -4.2649865 -4.2618318 -4.261847 -4.2631707 -4.2632709 -4.262321 -4.2608795 -4.259378 -4.25891 -4.2599511 -4.2626591][-4.2845993 -4.2825856 -4.2799377 -4.2768207 -4.2735906 -4.2718492 -4.2721372 -4.2730112 -4.2728024 -4.271831 -4.2706404 -4.2693014 -4.2684503 -4.268816 -4.2703362][-4.2856951 -4.2838969 -4.2819672 -4.2802176 -4.2784963 -4.2776771 -4.2781672 -4.2789831 -4.2791162 -4.2787023 -4.2782221 -4.2775097 -4.2766871 -4.2763243 -4.2764425][-4.2861395 -4.2841692 -4.2824621 -4.2814293 -4.2804976 -4.2801867 -4.2807312 -4.2813654 -4.28143 -4.2810469 -4.2808752 -4.2805548 -4.2797661 -4.2788506 -4.2779908][-4.2857 -4.2842817 -4.2834139 -4.2833157 -4.2831969 -4.2833591 -4.2838392 -4.2840385 -4.2835288 -4.2825003 -4.281569 -4.2804561 -4.2788854 -4.2770162 -4.2754617]]...]
INFO - root - 2017-12-06 09:47:05.332026: step 10710, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 82h:22m:31s remains)
INFO - root - 2017-12-06 09:47:14.623224: step 10720, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 82h:54m:16s remains)
INFO - root - 2017-12-06 09:47:23.885548: step 10730, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 81h:28m:34s remains)
INFO - root - 2017-12-06 09:47:33.077741: step 10740, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 79h:44m:58s remains)
INFO - root - 2017-12-06 09:47:42.271228: step 10750, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.007 sec/batch; 90h:00m:21s remains)
INFO - root - 2017-12-06 09:47:51.332441: step 10760, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 84h:13m:50s remains)
INFO - root - 2017-12-06 09:48:00.719421: step 10770, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.941 sec/batch; 84h:06m:26s remains)
INFO - root - 2017-12-06 09:48:09.917240: step 10780, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.884 sec/batch; 79h:00m:05s remains)
INFO - root - 2017-12-06 09:48:19.133045: step 10790, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 78h:34m:29s remains)
INFO - root - 2017-12-06 09:48:28.349431: step 10800, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 80h:40m:02s remains)
2017-12-06 09:48:29.014070: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3125978 -4.3080478 -4.3047695 -4.3010397 -4.2931619 -4.2825828 -4.2730322 -4.2651978 -4.2636471 -4.2656474 -4.2660017 -4.2587729 -4.2444086 -4.2168903 -4.1783066][-4.3052735 -4.2985754 -4.292377 -4.283205 -4.2698078 -4.257513 -4.2488713 -4.2404361 -4.2415953 -4.2497239 -4.2556305 -4.2485814 -4.2238989 -4.1827369 -4.13748][-4.2984638 -4.289865 -4.2813787 -4.2698421 -4.256566 -4.2457857 -4.2379766 -4.2290268 -4.2300849 -4.2414112 -4.2475796 -4.2388625 -4.2120953 -4.1730204 -4.1369538][-4.2885141 -4.2818422 -4.27461 -4.2635036 -4.2480574 -4.2268782 -4.2041287 -4.1866093 -4.1857662 -4.1976643 -4.2102289 -4.214108 -4.2038746 -4.1826477 -4.1627884][-4.2886209 -4.2833509 -4.2719965 -4.2529206 -4.2239618 -4.1751432 -4.1194792 -4.0905581 -4.1003742 -4.1248555 -4.1503625 -4.1710572 -4.1836023 -4.1838222 -4.1806979][-4.2975216 -4.2937222 -4.2714276 -4.2345581 -4.1813531 -4.0966907 -3.9960794 -3.9561992 -3.9955869 -4.0448513 -4.0829859 -4.1209669 -4.1562963 -4.1781864 -4.1897902][-4.3094382 -4.3024616 -4.2661138 -4.2048488 -4.1254873 -4.0147614 -3.8868513 -3.8442669 -3.9211237 -3.9952037 -4.0406322 -4.0838051 -4.1349797 -4.1803961 -4.2135286][-4.2977509 -4.2885509 -4.249444 -4.1803718 -4.0992494 -4.0080032 -3.9201872 -3.9040105 -3.9767988 -4.0337758 -4.0615859 -4.0961719 -4.1500025 -4.2017994 -4.2428532][-4.2640505 -4.2597394 -4.2365985 -4.1907468 -4.1356258 -4.0889006 -4.0581665 -4.0563159 -4.0929179 -4.1140571 -4.119782 -4.1436706 -4.1886415 -4.2301445 -4.260551][-4.2135811 -4.2239728 -4.2305431 -4.2163687 -4.1894321 -4.1752234 -4.1768122 -4.184402 -4.1957564 -4.18813 -4.176167 -4.1858482 -4.2150989 -4.2407532 -4.2593555][-4.1565614 -4.1877122 -4.2168407 -4.2218194 -4.2131019 -4.2139759 -4.2280602 -4.2371154 -4.2335958 -4.212657 -4.1930828 -4.1933823 -4.2143264 -4.2353625 -4.2491341][-4.1350908 -4.175292 -4.2007527 -4.2082257 -4.2045608 -4.2061591 -4.2229986 -4.2355285 -4.2309256 -4.2113137 -4.1962438 -4.1964941 -4.2165513 -4.2389884 -4.2544689][-4.1445255 -4.1701317 -4.1841273 -4.1864471 -4.1817813 -4.1811137 -4.1992321 -4.2181025 -4.2218957 -4.2161283 -4.2129912 -4.2178793 -4.2410803 -4.2649541 -4.2780561][-4.1520586 -4.1589284 -4.1614466 -4.1633177 -4.1608343 -4.1609774 -4.1864762 -4.2170744 -4.2346287 -4.2419424 -4.2472577 -4.2572675 -4.2794652 -4.2998586 -4.30703][-4.146574 -4.1447034 -4.1469579 -4.1542215 -4.1661863 -4.1806135 -4.2114248 -4.245429 -4.2687106 -4.2811942 -4.2859321 -4.2912836 -4.3048716 -4.3171849 -4.3208241]]...]
INFO - root - 2017-12-06 09:48:38.004527: step 10810, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 80h:07m:22s remains)
INFO - root - 2017-12-06 09:48:47.030020: step 10820, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 78h:52m:11s remains)
INFO - root - 2017-12-06 09:48:56.377925: step 10830, loss = 2.03, batch loss = 1.97 (8.4 examples/sec; 0.951 sec/batch; 85h:00m:13s remains)
INFO - root - 2017-12-06 09:49:05.608888: step 10840, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 76h:47m:02s remains)
INFO - root - 2017-12-06 09:49:14.932420: step 10850, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.899 sec/batch; 80h:18m:25s remains)
INFO - root - 2017-12-06 09:49:24.217653: step 10860, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.961 sec/batch; 85h:52m:00s remains)
INFO - root - 2017-12-06 09:49:33.499934: step 10870, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 83h:03m:55s remains)
INFO - root - 2017-12-06 09:49:42.485981: step 10880, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 87h:20m:10s remains)
INFO - root - 2017-12-06 09:49:51.404219: step 10890, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.899 sec/batch; 80h:16m:14s remains)
INFO - root - 2017-12-06 09:50:00.678070: step 10900, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 82h:46m:35s remains)
2017-12-06 09:50:01.325231: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3333158 -4.3207984 -4.3075132 -4.2970171 -4.287817 -4.2791262 -4.2777605 -4.2829413 -4.294867 -4.3027959 -4.3120074 -4.3214655 -4.3260646 -4.330328 -4.3343511][-4.3192029 -4.3012691 -4.2866635 -4.2747135 -4.2616391 -4.2444839 -4.2380619 -4.2455516 -4.2682276 -4.2861629 -4.3001056 -4.3117247 -4.3174238 -4.3215384 -4.328084][-4.3055496 -4.28795 -4.2742772 -4.2598557 -4.2413273 -4.2126007 -4.1965585 -4.2007332 -4.2327218 -4.2599611 -4.2809629 -4.2970934 -4.3080993 -4.314064 -4.3206863][-4.2963095 -4.280992 -4.2698956 -4.2536197 -4.2270937 -4.1814094 -4.1481137 -4.1430373 -4.1866121 -4.2247448 -4.2555547 -4.27668 -4.2910347 -4.29849 -4.3077245][-4.2902851 -4.27723 -4.2669716 -4.248776 -4.2127542 -4.1460838 -4.0853529 -4.06908 -4.1326408 -4.1878324 -4.2285447 -4.2538443 -4.2679267 -4.2748795 -4.2882042][-4.2834187 -4.2628465 -4.2435141 -4.2141914 -4.1635351 -4.0750322 -3.9838755 -3.9605038 -4.0548368 -4.1380615 -4.19493 -4.228158 -4.2460294 -4.2549586 -4.2696004][-4.2745991 -4.2410316 -4.205801 -4.1554389 -4.0804482 -3.9615498 -3.8335695 -3.7983875 -3.9274759 -4.0514808 -4.1372166 -4.1885595 -4.2220135 -4.2402735 -4.2566843][-4.2614627 -4.2165995 -4.1634097 -4.0907922 -3.9929435 -3.8545175 -3.7048924 -3.6572964 -3.8061023 -3.9671969 -4.083756 -4.1567674 -4.207005 -4.2358866 -4.2542052][-4.2544332 -4.2049918 -4.1408982 -4.0591559 -3.9599104 -3.8399403 -3.7232671 -3.6947527 -3.8264086 -3.9770057 -4.0924149 -4.1668487 -4.2163267 -4.2444034 -4.2623854][-4.2538576 -4.2052097 -4.1352539 -4.05206 -3.9636049 -3.8733284 -3.8003411 -3.7999883 -3.9041576 -4.0271945 -4.1277657 -4.1950383 -4.2372603 -4.259099 -4.2758865][-4.2577944 -4.2097287 -4.1397066 -4.0604334 -3.9810786 -3.9076886 -3.8612385 -3.8810048 -3.9698136 -4.0699925 -4.158287 -4.2220893 -4.2590828 -4.2769151 -4.2918305][-4.2697716 -4.22434 -4.1641116 -4.1000338 -4.0357032 -3.9742219 -3.9466469 -3.9766748 -4.0491676 -4.1262412 -4.198576 -4.2539954 -4.2858171 -4.2990174 -4.30919][-4.2932777 -4.2610946 -4.2215862 -4.1805239 -4.1391425 -4.1002245 -4.0876622 -4.1135468 -4.1616158 -4.2083344 -4.2533441 -4.2906933 -4.3136911 -4.3219504 -4.3270259][-4.3159151 -4.2984824 -4.2790923 -4.2613573 -4.2447309 -4.2270107 -4.222559 -4.2358751 -4.2604518 -4.2832665 -4.3055539 -4.3228955 -4.3351631 -4.3390546 -4.340848][-4.329267 -4.3195982 -4.3107595 -4.3039179 -4.2978239 -4.2927451 -4.2930412 -4.3005528 -4.3127117 -4.3225927 -4.3314333 -4.3373218 -4.3427482 -4.3454213 -4.3472595]]...]
INFO - root - 2017-12-06 09:50:10.515236: step 10910, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 83h:41m:56s remains)
INFO - root - 2017-12-06 09:50:19.771123: step 10920, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 79h:12m:42s remains)
INFO - root - 2017-12-06 09:50:29.063276: step 10930, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 82h:16m:54s remains)
INFO - root - 2017-12-06 09:50:38.457591: step 10940, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.982 sec/batch; 87h:42m:58s remains)
INFO - root - 2017-12-06 09:50:47.643636: step 10950, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.952 sec/batch; 85h:02m:52s remains)
INFO - root - 2017-12-06 09:50:56.882137: step 10960, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 78h:14m:56s remains)
INFO - root - 2017-12-06 09:51:06.115853: step 10970, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 77h:45m:18s remains)
INFO - root - 2017-12-06 09:51:15.288188: step 10980, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 80h:30m:45s remains)
INFO - root - 2017-12-06 09:51:24.528606: step 10990, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 82h:25m:39s remains)
INFO - root - 2017-12-06 09:51:33.827707: step 11000, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.949 sec/batch; 84h:42m:42s remains)
2017-12-06 09:51:34.493910: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3493037 -4.3452306 -4.3386326 -4.3322024 -4.3310537 -4.3362408 -4.3424187 -4.3454762 -4.3450556 -4.3434668 -4.3400149 -4.3353271 -4.3327603 -4.3340306 -4.33858][-4.3406062 -4.3298645 -4.3207779 -4.315702 -4.3174224 -4.3269448 -4.3366523 -4.3375235 -4.33064 -4.3241348 -4.3197284 -4.3168988 -4.318346 -4.323555 -4.33213][-4.3206682 -4.300457 -4.2892628 -4.2876506 -4.2924166 -4.3026857 -4.3137832 -4.3104448 -4.2973371 -4.2898092 -4.2867136 -4.285028 -4.289814 -4.3011661 -4.3184919][-4.2859735 -4.2513237 -4.232461 -4.2307968 -4.2363667 -4.2421188 -4.2514281 -4.2440109 -4.2292981 -4.2300959 -4.2354379 -4.2330451 -4.2357793 -4.2527137 -4.2809787][-4.2470255 -4.1985388 -4.1679654 -4.1612673 -4.1654024 -4.1624246 -4.1593075 -4.1369953 -4.1265488 -4.1519203 -4.1794848 -4.1799607 -4.1797371 -4.1993847 -4.2341094][-4.2166371 -4.1610208 -4.1211576 -4.107513 -4.1039677 -4.0802569 -4.0445776 -3.9912808 -3.9871809 -4.0561471 -4.1209416 -4.1370625 -4.13844 -4.1584878 -4.1942339][-4.20361 -4.1483173 -4.1037307 -4.0792837 -4.0578732 -4.0062938 -3.9302111 -3.8420923 -3.8514769 -3.9728322 -4.0788617 -4.1163731 -4.1218133 -4.1358933 -4.1646895][-4.2076325 -4.1592836 -4.1215425 -4.0976648 -4.066896 -4.0013151 -3.9101915 -3.8195651 -3.8397968 -3.967015 -4.0749884 -4.1196427 -4.130434 -4.1410618 -4.1625891][-4.2221122 -4.1850166 -4.1611743 -4.1459131 -4.1177025 -4.0614786 -3.9941196 -3.9328108 -3.9515324 -4.0397868 -4.1177621 -4.1509767 -4.1568623 -4.166811 -4.1843138][-4.2361259 -4.2115917 -4.2018967 -4.198143 -4.17916 -4.1403346 -4.0995078 -4.0649066 -4.076365 -4.1245761 -4.1689444 -4.184104 -4.1841578 -4.1915622 -4.2067537][-4.2476249 -4.2301021 -4.2289109 -4.234652 -4.2286167 -4.2112918 -4.1939735 -4.1786833 -4.1859674 -4.2107792 -4.2286587 -4.2263274 -4.2214961 -4.2260318 -4.236074][-4.2624412 -4.24811 -4.25012 -4.2606869 -4.2632079 -4.260272 -4.2578874 -4.2567987 -4.2656546 -4.2789435 -4.2824082 -4.2707558 -4.2602172 -4.2594967 -4.2640605][-4.2822471 -4.2703018 -4.2720404 -4.2835121 -4.2907991 -4.29265 -4.2977195 -4.3050938 -4.317028 -4.3260746 -4.3233829 -4.3079052 -4.2925668 -4.2862487 -4.2863336][-4.2983761 -4.2883191 -4.2900381 -4.3004303 -4.3089228 -4.31248 -4.3175836 -4.3248777 -4.3335948 -4.3390632 -4.3363366 -4.3238597 -4.3084044 -4.3009071 -4.2997441][-4.3087525 -4.29974 -4.299161 -4.3059821 -4.31268 -4.3178253 -4.3231349 -4.3293128 -4.3360825 -4.3403807 -4.33955 -4.3325672 -4.3214974 -4.3154578 -4.3140593]]...]
INFO - root - 2017-12-06 09:51:43.691035: step 11010, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 83h:16m:06s remains)
INFO - root - 2017-12-06 09:51:52.204928: step 11020, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.951 sec/batch; 84h:57m:39s remains)
INFO - root - 2017-12-06 09:52:01.320767: step 11030, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 87h:12m:10s remains)
INFO - root - 2017-12-06 09:52:10.425783: step 11040, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 83h:05m:29s remains)
INFO - root - 2017-12-06 09:52:19.569264: step 11050, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 80h:36m:59s remains)
INFO - root - 2017-12-06 09:52:28.801421: step 11060, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 85h:17m:03s remains)
INFO - root - 2017-12-06 09:52:37.950679: step 11070, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 82h:12m:40s remains)
INFO - root - 2017-12-06 09:52:47.235888: step 11080, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 80h:56m:19s remains)
INFO - root - 2017-12-06 09:52:56.283632: step 11090, loss = 2.10, batch loss = 2.05 (9.0 examples/sec; 0.888 sec/batch; 79h:15m:07s remains)
INFO - root - 2017-12-06 09:53:05.452713: step 11100, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 84h:46m:36s remains)
2017-12-06 09:53:06.191391: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2186017 -4.2021823 -4.171699 -4.1610017 -4.1819844 -4.2035284 -4.2246127 -4.2405629 -4.2466321 -4.2414231 -4.2297926 -4.2331305 -4.2430224 -4.2503886 -4.2493033][-4.2170553 -4.19553 -4.1549149 -4.1406994 -4.1598372 -4.180934 -4.2039933 -4.2220955 -4.2322454 -4.2389789 -4.2396245 -4.2425561 -4.2457056 -4.2431016 -4.2310147][-4.2158031 -4.1894403 -4.1448369 -4.1235623 -4.135695 -4.1506248 -4.1744547 -4.1949482 -4.2137794 -4.2321296 -4.2372484 -4.232667 -4.2248216 -4.2166729 -4.207026][-4.2104526 -4.1789336 -4.1299381 -4.1010094 -4.1037827 -4.1145453 -4.1394711 -4.1608939 -4.1864662 -4.2167144 -4.2229543 -4.2117229 -4.20348 -4.2018752 -4.206574][-4.2037516 -4.1668735 -4.1132612 -4.0740595 -4.0687289 -4.0731692 -4.0936294 -4.1137085 -4.1478024 -4.1940584 -4.2113624 -4.2055531 -4.2019529 -4.2032247 -4.2063146][-4.1883745 -4.1550097 -4.0983887 -4.0555806 -4.0413876 -4.0238056 -4.0252943 -4.0372944 -4.0842891 -4.1498508 -4.1865597 -4.1962 -4.1890669 -4.1769314 -4.1680975][-4.1613755 -4.1356783 -4.0852613 -4.04254 -4.0082412 -3.9587412 -3.928916 -3.9284363 -3.9996977 -4.096426 -4.1524854 -4.1648507 -4.1519752 -4.1371708 -4.1287847][-4.1373506 -4.1179123 -4.0755978 -4.0287743 -3.9719687 -3.9014521 -3.8541088 -3.8509431 -3.943311 -4.0564041 -4.1154366 -4.1253095 -4.1168308 -4.106442 -4.10478][-4.1311331 -4.1175971 -4.0853453 -4.0436606 -3.9845278 -3.9211845 -3.8848379 -3.8911076 -3.9719739 -4.0589905 -4.1036963 -4.1132579 -4.1118908 -4.1046762 -4.1070762][-4.1288047 -4.1285324 -4.1128497 -4.0851321 -4.0426059 -4.0014844 -3.9755049 -3.9791243 -4.0282855 -4.0780063 -4.1037531 -4.1116719 -4.1096559 -4.1019354 -4.10936][-4.1486206 -4.1603336 -4.1594963 -4.1440444 -4.1194959 -4.09556 -4.0713725 -4.0675712 -4.0900006 -4.1100326 -4.1212487 -4.1242237 -4.1165938 -4.1104712 -4.1267495][-4.1892223 -4.2028303 -4.2079849 -4.2000632 -4.18782 -4.1722345 -4.1518955 -4.1460395 -4.1525745 -4.1589203 -4.165688 -4.1682673 -4.1599145 -4.1587434 -4.1783][-4.229218 -4.243269 -4.2495074 -4.2464385 -4.2406583 -4.2298417 -4.2155123 -4.2112479 -4.2121224 -4.2164702 -4.22223 -4.2235475 -4.2158875 -4.2151675 -4.2286549][-4.2731652 -4.2855062 -4.2898107 -4.2870936 -4.2826624 -4.274087 -4.2647581 -4.2630038 -4.2634792 -4.2670984 -4.2710714 -4.2711582 -4.2641215 -4.262414 -4.2714725][-4.30714 -4.3167372 -4.3185539 -4.3141627 -4.3081188 -4.3002043 -4.2945437 -4.2951884 -4.2963047 -4.2990952 -4.3024836 -4.3028316 -4.2985878 -4.2973619 -4.3041253]]...]
INFO - root - 2017-12-06 09:53:15.346841: step 11110, loss = 2.09, batch loss = 2.04 (8.6 examples/sec; 0.931 sec/batch; 83h:09m:13s remains)
INFO - root - 2017-12-06 09:53:24.529638: step 11120, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 84h:16m:46s remains)
INFO - root - 2017-12-06 09:53:33.645652: step 11130, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 82h:20m:35s remains)
INFO - root - 2017-12-06 09:53:42.760064: step 11140, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 80h:46m:20s remains)
INFO - root - 2017-12-06 09:53:52.002741: step 11150, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 80h:16m:09s remains)
INFO - root - 2017-12-06 09:54:00.960803: step 11160, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 82h:05m:51s remains)
INFO - root - 2017-12-06 09:54:10.251157: step 11170, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.970 sec/batch; 86h:37m:11s remains)
INFO - root - 2017-12-06 09:54:19.387481: step 11180, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 81h:16m:37s remains)
INFO - root - 2017-12-06 09:54:28.598202: step 11190, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 78h:46m:05s remains)
INFO - root - 2017-12-06 09:54:37.784397: step 11200, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.913 sec/batch; 81h:31m:35s remains)
2017-12-06 09:54:38.555586: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2433424 -4.2554522 -4.2696037 -4.274025 -4.2735147 -4.2756081 -4.2842612 -4.2956862 -4.3038449 -4.3026147 -4.2909713 -4.270576 -4.2494745 -4.2381186 -4.2392821][-4.2282758 -4.2383204 -4.2541623 -4.259057 -4.2558408 -4.25486 -4.2649345 -4.282465 -4.2978449 -4.3015618 -4.2927828 -4.2750697 -4.2555146 -4.2447262 -4.2466769][-4.222908 -4.2300873 -4.2430458 -4.2414546 -4.2311106 -4.2241168 -4.2336636 -4.2569218 -4.2800183 -4.2889581 -4.28535 -4.2752528 -4.2651453 -4.2620034 -4.2683868][-4.2359838 -4.2416759 -4.2463393 -4.2302074 -4.2065115 -4.1901731 -4.1944842 -4.2189574 -4.2506027 -4.2702584 -4.277328 -4.2780094 -4.2792668 -4.28627 -4.2989254][-4.2583861 -4.2665181 -4.266716 -4.2357788 -4.1937132 -4.160336 -4.1427679 -4.1507392 -4.1866975 -4.2259688 -4.2536445 -4.2697625 -4.2831087 -4.2997937 -4.3174891][-4.2732186 -4.2836843 -4.2826838 -4.2464724 -4.1888127 -4.1332054 -4.0773783 -4.04504 -4.0779009 -4.1485143 -4.20796 -4.2440495 -4.2699618 -4.2942476 -4.3164244][-4.2793875 -4.2923403 -4.2938833 -4.2592745 -4.193861 -4.1183372 -4.0211477 -3.9311728 -3.9452322 -4.0516038 -4.1506891 -4.2110868 -4.2454782 -4.2722554 -4.294807][-4.2803326 -4.2961531 -4.3055811 -4.2819548 -4.21933 -4.1367645 -4.0233264 -3.89796 -3.8829787 -3.998302 -4.1162372 -4.1895204 -4.22552 -4.2453985 -4.2645741][-4.2822471 -4.3034029 -4.3240328 -4.3151383 -4.2646904 -4.1938863 -4.1020527 -4.0006657 -3.9754426 -4.0503421 -4.139802 -4.1918192 -4.2086987 -4.2053738 -4.213099][-4.2830043 -4.3094869 -4.3391614 -4.3424377 -4.3060484 -4.2550364 -4.1957426 -4.1343641 -4.1138048 -4.1468673 -4.1867762 -4.1995373 -4.1775327 -4.1298509 -4.1187587][-4.2812581 -4.3098903 -4.34363 -4.3535166 -4.3265448 -4.2894578 -4.2532859 -4.220068 -4.2090864 -4.2183833 -4.2218771 -4.1986871 -4.1284361 -4.019166 -3.9743857][-4.2802973 -4.3081908 -4.3406444 -4.3527007 -4.3341351 -4.3065348 -4.2835536 -4.26728 -4.2662606 -4.2676105 -4.2536483 -4.2087626 -4.1093707 -3.9569364 -3.8677981][-4.2809515 -4.3073835 -4.3372159 -4.3495216 -4.340188 -4.3225055 -4.3089337 -4.3023262 -4.3065591 -4.3088036 -4.2909956 -4.2421565 -4.1526713 -4.021688 -3.9345782][-4.2808218 -4.3046079 -4.3321877 -4.3445425 -4.3414655 -4.3319554 -4.3266106 -4.325264 -4.3319435 -4.3371797 -4.321382 -4.2795057 -4.2192039 -4.1395059 -4.0845685][-4.2793179 -4.3009496 -4.3267484 -4.3391485 -4.3376985 -4.3314347 -4.3303623 -4.3328791 -4.3419228 -4.35 -4.33604 -4.302 -4.2643924 -4.2244892 -4.1991115]]...]
INFO - root - 2017-12-06 09:54:47.746690: step 11210, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 82h:18m:47s remains)
INFO - root - 2017-12-06 09:54:56.832918: step 11220, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 83h:21m:21s remains)
INFO - root - 2017-12-06 09:55:05.961665: step 11230, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 74h:46m:24s remains)
INFO - root - 2017-12-06 09:55:15.100073: step 11240, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 82h:57m:34s remains)
INFO - root - 2017-12-06 09:55:24.200504: step 11250, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.901 sec/batch; 80h:26m:19s remains)
INFO - root - 2017-12-06 09:55:33.425536: step 11260, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.909 sec/batch; 81h:08m:37s remains)
INFO - root - 2017-12-06 09:55:42.566408: step 11270, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 81h:57m:19s remains)
INFO - root - 2017-12-06 09:55:51.746918: step 11280, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 81h:46m:10s remains)
INFO - root - 2017-12-06 09:56:00.868343: step 11290, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 85h:04m:54s remains)
INFO - root - 2017-12-06 09:56:10.220695: step 11300, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 81h:09m:12s remains)
2017-12-06 09:56:10.956243: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.185451 -4.1910448 -4.1830616 -4.175456 -4.1696935 -4.1655989 -4.1690965 -4.179687 -4.1936069 -4.1883516 -4.1572647 -4.1222568 -4.1001053 -4.1173348 -4.1503296][-4.1907926 -4.1919093 -4.18293 -4.1773214 -4.1712875 -4.1617537 -4.1599388 -4.1693592 -4.1809168 -4.1752472 -4.1463685 -4.1077747 -4.08113 -4.1016212 -4.1365519][-4.1640038 -4.1654887 -4.1618662 -4.1608548 -4.1537714 -4.1418571 -4.1418085 -4.153481 -4.1672759 -4.1649036 -4.1399746 -4.0968466 -4.0710478 -4.0998931 -4.1372633][-4.1256142 -4.1293941 -4.1367388 -4.1412449 -4.1341443 -4.1263814 -4.1292868 -4.1410074 -4.1553049 -4.1580124 -4.1395879 -4.0978875 -4.0794334 -4.1161065 -4.1541343][-4.0960488 -4.1032691 -4.1207256 -4.1290188 -4.1214747 -4.116775 -4.1197147 -4.1253333 -4.1363192 -4.1421533 -4.131165 -4.1017203 -4.09867 -4.1390343 -4.1726675][-4.0806494 -4.0911865 -4.1157866 -4.12716 -4.1174459 -4.1078825 -4.1018124 -4.0985003 -4.1057296 -4.1157017 -4.1166034 -4.1043286 -4.117166 -4.1607556 -4.1893787][-4.0785031 -4.08971 -4.1179028 -4.1301212 -4.1160645 -4.094161 -4.0723772 -4.0587416 -4.063581 -4.0775638 -4.08475 -4.0788221 -4.095861 -4.1478014 -4.1809459][-4.0904751 -4.1037664 -4.1298966 -4.1345172 -4.1103454 -4.0725689 -4.0348105 -4.0102482 -4.0146589 -4.0310278 -4.0336504 -4.0119081 -4.0190606 -4.0798125 -4.125751][-4.1147923 -4.1307878 -4.1491346 -4.1396494 -4.1001863 -4.0487247 -4.0015583 -3.9752927 -3.9811041 -3.9977102 -3.9934173 -3.951937 -3.9392202 -4.0056319 -4.064744][-4.1320353 -4.1456261 -4.156136 -4.1369715 -4.0930076 -4.0431285 -3.9980826 -3.9794302 -3.9946909 -4.019453 -4.0194778 -3.9787316 -3.9597754 -4.0138693 -4.0677547][-4.1385155 -4.1465011 -4.1513171 -4.1339107 -4.0978127 -4.058363 -4.0232267 -4.0112982 -4.0327086 -4.0624852 -4.0692186 -4.0384727 -4.0242748 -4.0655179 -4.10753][-4.1294875 -4.1349163 -4.1410046 -4.1333957 -4.1114144 -4.0842257 -4.0598989 -4.0478268 -4.0643592 -4.0881677 -4.0953755 -4.0773692 -4.0748959 -4.1093688 -4.1436024][-4.11584 -4.1222372 -4.1313653 -4.133841 -4.1263852 -4.1118922 -4.0988455 -4.0858145 -4.0878787 -4.0960159 -4.0967226 -4.0931826 -4.1023245 -4.1349559 -4.1647859][-4.1125154 -4.1202016 -4.1285968 -4.1344833 -4.137023 -4.1350741 -4.1339793 -4.1205435 -4.1131735 -4.1119065 -4.106246 -4.1031733 -4.1142993 -4.1468954 -4.1757388][-4.107986 -4.113986 -4.1253366 -4.1356106 -4.1484528 -4.158309 -4.1627173 -4.1500688 -4.1413741 -4.1417165 -4.1371737 -4.131608 -4.1398029 -4.1632581 -4.1846304]]...]
INFO - root - 2017-12-06 09:56:19.921728: step 11310, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 81h:45m:00s remains)
INFO - root - 2017-12-06 09:56:29.192594: step 11320, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.936 sec/batch; 83h:30m:47s remains)
INFO - root - 2017-12-06 09:56:38.367767: step 11330, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.891 sec/batch; 79h:30m:59s remains)
INFO - root - 2017-12-06 09:56:47.581929: step 11340, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 82h:22m:14s remains)
INFO - root - 2017-12-06 09:56:56.870411: step 11350, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 82h:50m:18s remains)
INFO - root - 2017-12-06 09:57:05.915588: step 11360, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 80h:49m:46s remains)
INFO - root - 2017-12-06 09:57:15.059643: step 11370, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 82h:56m:46s remains)
INFO - root - 2017-12-06 09:57:24.194903: step 11380, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.894 sec/batch; 79h:43m:59s remains)
INFO - root - 2017-12-06 09:57:33.442324: step 11390, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 80h:11m:07s remains)
INFO - root - 2017-12-06 09:57:42.659885: step 11400, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 80h:00m:11s remains)
2017-12-06 09:57:43.446680: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3141141 -4.3084197 -4.3049731 -4.3025675 -4.3027568 -4.3079786 -4.3060522 -4.2973256 -4.2868514 -4.2808409 -4.2824802 -4.2952719 -4.3136492 -4.3247437 -4.3287311][-4.2936392 -4.2839122 -4.27842 -4.2725844 -4.2716351 -4.2759385 -4.2704177 -4.2597427 -4.2515364 -4.2511034 -4.2583475 -4.27807 -4.3046937 -4.3196106 -4.3240204][-4.2678151 -4.2571173 -4.2500205 -4.2386518 -4.2339725 -4.2375045 -4.2259111 -4.2079883 -4.1984854 -4.2087088 -4.2255654 -4.2511883 -4.2845054 -4.3055735 -4.3112736][-4.2375455 -4.2257466 -4.2158022 -4.1961904 -4.181426 -4.1796589 -4.1626744 -4.1383123 -4.127326 -4.1521187 -4.1838865 -4.2150292 -4.2530508 -4.277184 -4.283534][-4.2055821 -4.19235 -4.1800613 -4.1518807 -4.1190867 -4.0947638 -4.06067 -4.0253606 -4.021337 -4.0729427 -4.1292844 -4.1680379 -4.20454 -4.22812 -4.2381468][-4.181993 -4.1641693 -4.1484365 -4.1159196 -4.0640659 -4.0011935 -3.925611 -3.860297 -3.8685226 -3.9642808 -4.0555687 -4.1091537 -4.1482229 -4.1777878 -4.196734][-4.1749864 -4.1476684 -4.1260138 -4.0939412 -4.0370851 -3.9435141 -3.8110967 -3.6829314 -3.6885281 -3.8384857 -3.9787195 -4.0605936 -4.1127548 -4.1524944 -4.1795835][-4.1689081 -4.1288447 -4.1055088 -4.0881524 -4.0488691 -3.9605885 -3.8098218 -3.6444592 -3.6343751 -3.7959318 -3.9556432 -4.0544467 -4.1165462 -4.1617832 -4.1917257][-4.1751165 -4.1284842 -4.1099577 -4.1101651 -4.0977392 -4.0439792 -3.9292998 -3.7978759 -3.7778244 -3.8859403 -4.0074906 -4.0920014 -4.1480732 -4.1907105 -4.2176704][-4.1821818 -4.1374521 -4.12645 -4.1378889 -4.1487041 -4.1308851 -4.0609593 -3.9738157 -3.9551213 -4.013485 -4.0907626 -4.1495504 -4.1889844 -4.2222319 -4.242847][-4.1991105 -4.1584392 -4.1501074 -4.1644778 -4.18814 -4.1948295 -4.1605287 -4.1082253 -4.0958123 -4.126193 -4.1712089 -4.2051606 -4.2289791 -4.2520747 -4.2664676][-4.2300439 -4.1996922 -4.1917171 -4.1975379 -4.2189279 -4.2371025 -4.2278666 -4.2007656 -4.1941414 -4.2100992 -4.2358475 -4.25139 -4.2645631 -4.280715 -4.2887669][-4.2621555 -4.244678 -4.2394719 -4.2396135 -4.2532578 -4.2692757 -4.2700891 -4.2596989 -4.2575207 -4.268364 -4.281096 -4.2839484 -4.2886877 -4.296207 -4.3001628][-4.2832079 -4.2713203 -4.2646832 -4.2631454 -4.2702541 -4.2797561 -4.2835178 -4.28203 -4.2820454 -4.2884326 -4.2948818 -4.2947564 -4.2967873 -4.3018332 -4.3067026][-4.2986221 -4.2912822 -4.2844577 -4.283145 -4.2863832 -4.2893004 -4.291934 -4.2942219 -4.2945662 -4.297152 -4.3012452 -4.3023119 -4.3050327 -4.3102694 -4.3153524]]...]
INFO - root - 2017-12-06 09:57:52.738925: step 11410, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.884 sec/batch; 78h:49m:23s remains)
INFO - root - 2017-12-06 09:58:01.910021: step 11420, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 81h:30m:16s remains)
INFO - root - 2017-12-06 09:58:11.005224: step 11430, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 78h:13m:20s remains)
INFO - root - 2017-12-06 09:58:20.241246: step 11440, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 80h:08m:26s remains)
INFO - root - 2017-12-06 09:58:29.420183: step 11450, loss = 2.02, batch loss = 1.97 (8.5 examples/sec; 0.940 sec/batch; 83h:50m:42s remains)
INFO - root - 2017-12-06 09:58:38.588260: step 11460, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 79h:46m:55s remains)
INFO - root - 2017-12-06 09:58:47.811867: step 11470, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 82h:11m:32s remains)
INFO - root - 2017-12-06 09:58:56.903606: step 11480, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 80h:06m:01s remains)
INFO - root - 2017-12-06 09:59:05.997491: step 11490, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.909 sec/batch; 81h:00m:56s remains)
INFO - root - 2017-12-06 09:59:15.122099: step 11500, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 81h:18m:56s remains)
2017-12-06 09:59:15.824248: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2593532 -4.2610774 -4.2596059 -4.2440305 -4.2157149 -4.1774578 -4.156518 -4.1624603 -4.1732712 -4.1919088 -4.216465 -4.2346969 -4.2533031 -4.2771416 -4.297225][-4.2434454 -4.2474966 -4.2489562 -4.2360473 -4.208385 -4.1627593 -4.129848 -4.127718 -4.1390791 -4.1637969 -4.1997738 -4.226243 -4.24824 -4.2739844 -4.2974524][-4.225112 -4.2287612 -4.2314816 -4.2274752 -4.2071166 -4.1588178 -4.1136856 -4.1015654 -4.1110611 -4.1358714 -4.1801095 -4.2189012 -4.2461071 -4.273365 -4.2996879][-4.2070107 -4.2085156 -4.2103496 -4.2143459 -4.2008839 -4.1537104 -4.0972748 -4.0754824 -4.0854244 -4.1052732 -4.1508927 -4.2022214 -4.2406683 -4.2707505 -4.2998366][-4.1929507 -4.1974292 -4.1974626 -4.2059069 -4.1953821 -4.1475697 -4.0783124 -4.0471177 -4.0689259 -4.0945277 -4.1389585 -4.1924477 -4.2372284 -4.2679167 -4.2972407][-4.1642408 -4.1765423 -4.1796231 -4.1920652 -4.1849165 -4.1323862 -4.045712 -4.0008307 -4.0401263 -4.0878034 -4.1351771 -4.1892123 -4.2340846 -4.264576 -4.2920971][-4.1026468 -4.128963 -4.1453085 -4.1676755 -4.1626573 -4.1043282 -3.9907055 -3.9169736 -3.9702387 -4.0533247 -4.114222 -4.1775575 -4.2298608 -4.2643323 -4.2894273][-4.0123029 -4.0512524 -4.0950532 -4.1356716 -4.1405029 -4.0773549 -3.9323785 -3.8118207 -3.865324 -3.992115 -4.0821981 -4.1651583 -4.2314162 -4.2704387 -4.2913108][-3.9386103 -3.9715555 -4.0388622 -4.1020207 -4.126955 -4.0740871 -3.9154067 -3.7386031 -3.7698712 -3.9306424 -4.0587821 -4.161685 -4.2356715 -4.2760825 -4.2934871][-3.9312184 -3.9475455 -4.0134244 -4.0856214 -4.1284475 -4.1029377 -3.9704511 -3.7844729 -3.7722611 -3.918211 -4.0538859 -4.1588411 -4.2336583 -4.27595 -4.294733][-3.9962163 -3.9965258 -4.0469451 -4.1071987 -4.1495171 -4.1464396 -4.06449 -3.9196367 -3.8779681 -3.9700623 -4.0802679 -4.1687565 -4.23297 -4.2732906 -4.2952318][-4.0909615 -4.0801749 -4.1115632 -4.1541519 -4.1841149 -4.1933417 -4.1550288 -4.0609374 -4.0118632 -4.0555158 -4.1276727 -4.1943789 -4.2449055 -4.2780929 -4.2992272][-4.1823568 -4.1634741 -4.1770968 -4.2068114 -4.2248921 -4.2379208 -4.2258034 -4.1709566 -4.1284952 -4.1424007 -4.1860552 -4.2319803 -4.2692442 -4.2922673 -4.3064327][-4.2454844 -4.226347 -4.22846 -4.2487407 -4.2623596 -4.2754145 -4.2743783 -4.2438931 -4.2106113 -4.2069478 -4.233418 -4.2630749 -4.2886915 -4.3047929 -4.3129597][-4.2771287 -4.2628379 -4.2601786 -4.2737813 -4.2856374 -4.2978325 -4.300015 -4.2851825 -4.2589388 -4.2480106 -4.2638521 -4.2827711 -4.3000746 -4.3119192 -4.3172431]]...]
INFO - root - 2017-12-06 09:59:25.078025: step 11510, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 83h:55m:31s remains)
INFO - root - 2017-12-06 09:59:34.181236: step 11520, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 77h:22m:28s remains)
INFO - root - 2017-12-06 09:59:43.482039: step 11530, loss = 2.03, batch loss = 1.97 (8.2 examples/sec; 0.978 sec/batch; 87h:12m:42s remains)
INFO - root - 2017-12-06 09:59:52.805223: step 11540, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.976 sec/batch; 87h:00m:50s remains)
INFO - root - 2017-12-06 10:00:01.895794: step 11550, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 78h:18m:27s remains)
INFO - root - 2017-12-06 10:00:11.050230: step 11560, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 78h:12m:50s remains)
INFO - root - 2017-12-06 10:00:20.278485: step 11570, loss = 2.02, batch loss = 1.96 (8.9 examples/sec; 0.899 sec/batch; 80h:09m:53s remains)
INFO - root - 2017-12-06 10:00:29.683661: step 11580, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.907 sec/batch; 80h:52m:51s remains)
INFO - root - 2017-12-06 10:00:38.728495: step 11590, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 80h:54m:33s remains)
INFO - root - 2017-12-06 10:00:47.922572: step 11600, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 78h:14m:02s remains)
2017-12-06 10:00:48.558492: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.309998 -4.3079929 -4.3083692 -4.3123279 -4.314816 -4.3172154 -4.3207273 -4.3231711 -4.32708 -4.3320022 -4.3355117 -4.33331 -4.3320222 -4.3327718 -4.335062][-4.2731476 -4.268024 -4.2660947 -4.2675471 -4.2681956 -4.2707496 -4.2750688 -4.2787981 -4.2845969 -4.2917976 -4.3001113 -4.3032622 -4.3072104 -4.3112006 -4.3166523][-4.2244329 -4.2127938 -4.2102423 -4.2146659 -4.2168317 -4.2227669 -4.2268538 -4.2313762 -4.2391324 -4.2480831 -4.2602773 -4.2697811 -4.2803464 -4.2907286 -4.3017735][-4.1692972 -4.1482253 -4.1459093 -4.1551085 -4.1638904 -4.1745348 -4.1790972 -4.1835613 -4.1932635 -4.2050905 -4.2187772 -4.23311 -4.2495718 -4.2633829 -4.2785859][-4.1244512 -4.0966296 -4.091229 -4.0999775 -4.1105671 -4.1207566 -4.1234393 -4.1266723 -4.1371279 -4.1555314 -4.1733837 -4.1942143 -4.2152905 -4.2315693 -4.252326][-4.0898018 -4.0579929 -4.0480061 -4.0485277 -4.0521231 -4.05233 -4.0440593 -4.0411315 -4.0507064 -4.0807252 -4.1135159 -4.1452136 -4.176815 -4.2039642 -4.2338204][-4.0745163 -4.0364051 -4.0170755 -4.00647 -3.9952512 -3.9774361 -3.9454083 -3.918478 -3.9229167 -3.9712553 -4.0298367 -4.0842423 -4.1349463 -4.1790075 -4.219317][-4.0885897 -4.0530977 -4.0327229 -4.0155215 -3.9903319 -3.953599 -3.8952608 -3.8357179 -3.8257332 -3.8854544 -3.9642639 -4.0358233 -4.1012483 -4.1596451 -4.2084575][-4.1139688 -4.0877671 -4.075726 -4.0627542 -4.0360222 -3.9931512 -3.9252479 -3.8536544 -3.835418 -3.8860033 -3.9593136 -4.028192 -4.0926242 -4.1547575 -4.2046537][-4.1410804 -4.1208072 -4.1131582 -4.107605 -4.0861249 -4.0470405 -3.9857316 -3.9243233 -3.9057369 -3.9403944 -3.9956496 -4.0510826 -4.1080194 -4.16466 -4.2111812][-4.1747894 -4.1568704 -4.1484795 -4.1462688 -4.131146 -4.0997519 -4.0522361 -4.0067143 -3.9908454 -4.0123763 -4.0519037 -4.095161 -4.1391454 -4.1848993 -4.2250571][-4.213923 -4.1994624 -4.1922159 -4.1913781 -4.1813164 -4.1587453 -4.1242485 -4.0919681 -4.0792747 -4.0923533 -4.1200066 -4.15024 -4.1815534 -4.2163796 -4.2506814][-4.2568722 -4.2486014 -4.2467165 -4.2468781 -4.24143 -4.2280426 -4.2046509 -4.1799078 -4.1683588 -4.1748981 -4.1927228 -4.2107096 -4.2304292 -4.256124 -4.2843771][-4.2988749 -4.2966442 -4.2978611 -4.2981586 -4.2940741 -4.285737 -4.2723732 -4.256144 -4.2477045 -4.252439 -4.2636638 -4.2728567 -4.2826996 -4.298624 -4.3177896][-4.3275461 -4.3291917 -4.3313723 -4.3309603 -4.3271523 -4.3219128 -4.3151507 -4.3075619 -4.3050938 -4.3089066 -4.3153219 -4.3193636 -4.3234315 -4.3309979 -4.3416243]]...]
INFO - root - 2017-12-06 10:00:57.852218: step 11610, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.994 sec/batch; 88h:38m:26s remains)
INFO - root - 2017-12-06 10:01:06.997556: step 11620, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.908 sec/batch; 80h:56m:52s remains)
INFO - root - 2017-12-06 10:01:16.228454: step 11630, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.972 sec/batch; 86h:40m:09s remains)
INFO - root - 2017-12-06 10:01:25.480082: step 11640, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.904 sec/batch; 80h:33m:31s remains)
INFO - root - 2017-12-06 10:01:34.683369: step 11650, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.892 sec/batch; 79h:28m:14s remains)
INFO - root - 2017-12-06 10:01:43.721779: step 11660, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 78h:38m:23s remains)
INFO - root - 2017-12-06 10:01:52.824605: step 11670, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.857 sec/batch; 76h:22m:24s remains)
INFO - root - 2017-12-06 10:02:01.851505: step 11680, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 76h:13m:29s remains)
INFO - root - 2017-12-06 10:02:10.938063: step 11690, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 80h:10m:57s remains)
INFO - root - 2017-12-06 10:02:20.171422: step 11700, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 82h:07m:48s remains)
2017-12-06 10:02:20.846479: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1980305 -4.1881657 -4.208446 -4.2365842 -4.2570567 -4.26768 -4.2741141 -4.282702 -4.2937088 -4.2978625 -4.2853389 -4.260725 -4.2387066 -4.2273927 -4.2262855][-4.2526526 -4.239646 -4.2473545 -4.2633309 -4.2763577 -4.2840371 -4.2908988 -4.3018651 -4.313808 -4.32202 -4.3188648 -4.2998724 -4.2764521 -4.2595 -4.2513852][-4.2881656 -4.2792196 -4.2769003 -4.2797794 -4.2809935 -4.2822189 -4.2861042 -4.2965503 -4.3103671 -4.3227639 -4.3294406 -4.3219514 -4.3045425 -4.2891154 -4.2784877][-4.2926121 -4.2852988 -4.2804623 -4.2783575 -4.2689376 -4.2613297 -4.2595634 -4.2689633 -4.2860613 -4.3025603 -4.3159642 -4.3222141 -4.3198714 -4.3105483 -4.2981553][-4.2555075 -4.243031 -4.24186 -4.2435179 -4.2221618 -4.19844 -4.1896391 -4.2058358 -4.23582 -4.2624807 -4.2802896 -4.2935977 -4.304832 -4.3060842 -4.2958341][-4.1810465 -4.1666451 -4.1724067 -4.1804976 -4.1458263 -4.0935559 -4.064455 -4.0879889 -4.1425605 -4.1881018 -4.2155943 -4.2382212 -4.261075 -4.2753563 -4.2719626][-4.1061592 -4.0947647 -4.1011429 -4.1084232 -4.0642371 -3.9799967 -3.9098461 -3.9274907 -4.0136538 -4.0919881 -4.1398215 -4.1778345 -4.212656 -4.2382412 -4.2443061][-4.1224103 -4.1087213 -4.1013122 -4.0945425 -4.0509453 -3.9602363 -3.8602943 -3.8498826 -3.9395597 -4.0369086 -4.101871 -4.1492052 -4.1857686 -4.2117662 -4.221014][-4.1954455 -4.1827269 -4.1740842 -4.1663566 -4.1388946 -4.0778565 -3.9981544 -3.9651077 -4.0058165 -4.0723352 -4.1275463 -4.1691604 -4.1992097 -4.2150464 -4.2154951][-4.2509236 -4.2432761 -4.241971 -4.2393756 -4.2261696 -4.1937881 -4.1457963 -4.1144824 -4.1201339 -4.1476593 -4.1744032 -4.2009654 -4.2225246 -4.2317028 -4.2277117][-4.2754045 -4.2716765 -4.2746453 -4.2740355 -4.2618709 -4.2387171 -4.2130346 -4.1992593 -4.1998625 -4.2086859 -4.2146082 -4.2235637 -4.2322769 -4.237103 -4.2344003][-4.2692275 -4.2673559 -4.2726612 -4.272912 -4.2577176 -4.2345591 -4.2197495 -4.223207 -4.2371655 -4.2458224 -4.2442851 -4.2381616 -4.2330675 -4.233439 -4.2343111][-4.2495294 -4.2457552 -4.2515631 -4.2526469 -4.2376328 -4.2170196 -4.2040253 -4.2136369 -4.2430859 -4.2643805 -4.26481 -4.2487669 -4.2315712 -4.2250342 -4.2277641][-4.24134 -4.2363167 -4.2441478 -4.2475219 -4.2366438 -4.21725 -4.1998372 -4.2046227 -4.2372131 -4.2694979 -4.2764964 -4.2588077 -4.2313643 -4.2142267 -4.2123384][-4.2461295 -4.239048 -4.2495852 -4.2596645 -4.2565336 -4.2399359 -4.2201295 -4.2155213 -4.2391872 -4.26904 -4.2779818 -4.2601123 -4.2251997 -4.1949487 -4.1859031]]...]
INFO - root - 2017-12-06 10:02:30.008475: step 11710, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 80h:59m:29s remains)
INFO - root - 2017-12-06 10:02:39.116791: step 11720, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.971 sec/batch; 86h:29m:18s remains)
INFO - root - 2017-12-06 10:02:47.966058: step 11730, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.885 sec/batch; 78h:51m:08s remains)
INFO - root - 2017-12-06 10:02:57.153728: step 11740, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 81h:00m:03s remains)
INFO - root - 2017-12-06 10:03:06.064119: step 11750, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 79h:14m:31s remains)
INFO - root - 2017-12-06 10:03:15.192041: step 11760, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 81h:03m:10s remains)
INFO - root - 2017-12-06 10:03:24.416684: step 11770, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 80h:37m:29s remains)
INFO - root - 2017-12-06 10:03:33.616124: step 11780, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 82h:02m:21s remains)
INFO - root - 2017-12-06 10:03:42.848999: step 11790, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 83h:30m:23s remains)
INFO - root - 2017-12-06 10:03:51.779487: step 11800, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 76h:20m:30s remains)
2017-12-06 10:03:52.450545: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1927738 -4.183908 -4.204906 -4.2152753 -4.2146034 -4.2139773 -4.2151079 -4.21748 -4.2254009 -4.2382393 -4.2521796 -4.258141 -4.259347 -4.2675924 -4.2729673][-4.1775 -4.1739769 -4.2000828 -4.213449 -4.2158861 -4.2194014 -4.2239876 -4.2259078 -4.2318592 -4.2380118 -4.2447248 -4.246963 -4.2459455 -4.2517257 -4.2575912][-4.1563725 -4.1563063 -4.1864595 -4.1993189 -4.2016459 -4.2085004 -4.2216759 -4.2316337 -4.2424569 -4.2489109 -4.2555609 -4.2576356 -4.2523446 -4.2508721 -4.2533536][-4.1254897 -4.1142564 -4.1367226 -4.1454892 -4.1470275 -4.1572828 -4.1820712 -4.2079978 -4.2302027 -4.2437935 -4.2568107 -4.2639303 -4.2571254 -4.2527509 -4.2543569][-4.0960069 -4.0707026 -4.079926 -4.08164 -4.0825367 -4.0948529 -4.1276402 -4.1675024 -4.2020311 -4.2235193 -4.2417336 -4.2539835 -4.25146 -4.2512751 -4.2577748][-4.0762916 -4.0480537 -4.0481148 -4.0432372 -4.0371509 -4.0405855 -4.0649438 -4.1085949 -4.1544623 -4.1835127 -4.2070146 -4.2247443 -4.2314706 -4.2415323 -4.2564063][-4.0648017 -4.0416384 -4.042346 -4.0409632 -4.0350838 -4.0301542 -4.0387363 -4.0718184 -4.1164293 -4.1426716 -4.164762 -4.1855659 -4.2023993 -4.2230406 -4.246933][-4.0378304 -4.0204954 -4.0260696 -4.0351963 -4.0333214 -4.02797 -4.0325265 -4.0604906 -4.0977592 -4.1138396 -4.1268368 -4.1441069 -4.1653485 -4.1944432 -4.2278867][-4.0111709 -3.9972665 -4.0069766 -4.0218816 -4.0207944 -4.0206389 -4.0321774 -4.0600824 -4.0939445 -4.1040182 -4.11077 -4.120111 -4.1383305 -4.1688418 -4.2060671][-4.0096354 -4.0024762 -4.0198345 -4.035872 -4.0355754 -4.0385666 -4.0528383 -4.075141 -4.1020403 -4.110218 -4.1171007 -4.1251078 -4.140626 -4.1690331 -4.2019095][-4.0294728 -4.0282907 -4.0505857 -4.0676351 -4.0703492 -4.0760188 -4.0893168 -4.1024141 -4.1189613 -4.1262956 -4.1345053 -4.142334 -4.1554637 -4.1815906 -4.2104454][-4.0622673 -4.0609627 -4.0794291 -4.0951376 -4.1011534 -4.110672 -4.1267185 -4.1366138 -4.1438465 -4.1484971 -4.1561289 -4.1624832 -4.1721697 -4.195467 -4.222003][-4.1151538 -4.1137962 -4.1253862 -4.134778 -4.139349 -4.1498022 -4.1677036 -4.1770253 -4.1812706 -4.1852107 -4.1880355 -4.1887116 -4.1944866 -4.2123671 -4.2354774][-4.177043 -4.1772571 -4.1842194 -4.189013 -4.19203 -4.1995444 -4.2119412 -4.2191548 -4.2234368 -4.2279387 -4.2283134 -4.2246809 -4.2260609 -4.2375665 -4.2545443][-4.2310734 -4.2316303 -4.2349634 -4.2374325 -4.238184 -4.2413406 -4.2471447 -4.2524538 -4.2582355 -4.2639861 -4.2660117 -4.2633586 -4.2618551 -4.2681375 -4.2794366]]...]
INFO - root - 2017-12-06 10:04:01.598387: step 11810, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 83h:36m:07s remains)
INFO - root - 2017-12-06 10:04:10.467912: step 11820, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.968 sec/batch; 86h:16m:16s remains)
INFO - root - 2017-12-06 10:04:19.585605: step 11830, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.946 sec/batch; 84h:17m:47s remains)
INFO - root - 2017-12-06 10:04:28.666705: step 11840, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.938 sec/batch; 83h:33m:01s remains)
INFO - root - 2017-12-06 10:04:37.948969: step 11850, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 79h:30m:51s remains)
INFO - root - 2017-12-06 10:04:47.163500: step 11860, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.921 sec/batch; 82h:03m:31s remains)
INFO - root - 2017-12-06 10:04:56.260518: step 11870, loss = 2.03, batch loss = 1.97 (9.7 examples/sec; 0.821 sec/batch; 73h:05m:59s remains)
INFO - root - 2017-12-06 10:05:05.374803: step 11880, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 77h:11m:54s remains)
INFO - root - 2017-12-06 10:05:14.403814: step 11890, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 82h:35m:24s remains)
INFO - root - 2017-12-06 10:05:23.531209: step 11900, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 86h:02m:04s remains)
2017-12-06 10:05:24.243924: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3146596 -4.3080239 -4.2952456 -4.2863941 -4.2848248 -4.2817664 -4.2710309 -4.2601662 -4.2410655 -4.2054644 -4.1701922 -4.1547804 -4.1669531 -4.2114053 -4.2594972][-4.3227959 -4.3208923 -4.3079662 -4.2932191 -4.2832484 -4.2741618 -4.2586756 -4.2434492 -4.2263384 -4.2006617 -4.1762362 -4.1688795 -4.1796002 -4.2222838 -4.2720952][-4.3180718 -4.320405 -4.3079896 -4.2928305 -4.281817 -4.2676353 -4.2418246 -4.2125306 -4.190352 -4.1695089 -4.1584888 -4.1616464 -4.1770658 -4.2218285 -4.2731075][-4.3025374 -4.3047757 -4.2900581 -4.2753649 -4.269011 -4.2548037 -4.2239251 -4.1841426 -4.1522155 -4.124239 -4.1252747 -4.1474862 -4.1711922 -4.2177582 -4.2692404][-4.2834988 -4.2796965 -4.2597103 -4.2430797 -4.2369189 -4.2224865 -4.189662 -4.1415529 -4.1000171 -4.0719018 -4.0876451 -4.1293454 -4.1601233 -4.2071214 -4.2590785][-4.2685504 -4.25636 -4.2296915 -4.2082314 -4.1932354 -4.1710882 -4.13031 -4.0673866 -4.0113506 -3.98997 -4.032969 -4.0981011 -4.1420279 -4.195466 -4.2500944][-4.2590365 -4.2430282 -4.2170277 -4.1950979 -4.1672182 -4.1302371 -4.0706649 -3.9787767 -3.8918178 -3.8722041 -3.9548426 -4.0523505 -4.117321 -4.1788406 -4.2380023][-4.2489853 -4.2323437 -4.2143035 -4.1941948 -4.1556754 -4.1085305 -4.03735 -3.92932 -3.8201194 -3.8013513 -3.9107518 -4.0245185 -4.1009569 -4.1666241 -4.2273521][-4.252264 -4.2376294 -4.2299485 -4.2157736 -4.1744809 -4.1245461 -4.0570312 -3.9575157 -3.8553209 -3.8404384 -3.9433727 -4.0393538 -4.1087303 -4.1696153 -4.2263284][-4.2635231 -4.257225 -4.2630725 -4.253572 -4.2148371 -4.1676989 -4.1104918 -4.0313134 -3.9460096 -3.9316945 -4.0088925 -4.0775104 -4.132936 -4.1856537 -4.2355762][-4.2797089 -4.2801914 -4.2910271 -4.2858992 -4.2534509 -4.21624 -4.1717849 -4.1124907 -4.0470695 -4.0391154 -4.0943422 -4.1388192 -4.1795292 -4.2207613 -4.2596569][-4.3042769 -4.306004 -4.3127146 -4.3063769 -4.2803621 -4.2550783 -4.2240286 -4.18128 -4.1373115 -4.1388769 -4.1782341 -4.2072668 -4.2342577 -4.2625861 -4.2875304][-4.3328619 -4.3326817 -4.3316936 -4.3219891 -4.3034377 -4.2874637 -4.2675896 -4.2379951 -4.2106991 -4.2152982 -4.2386642 -4.2548122 -4.2691779 -4.2880087 -4.3040519][-4.343277 -4.3367157 -4.3289971 -4.3225036 -4.3156343 -4.3099957 -4.3019953 -4.2869735 -4.2718377 -4.2743893 -4.2795329 -4.2791214 -4.2823949 -4.2968054 -4.30965][-4.3318853 -4.3259945 -4.318974 -4.317337 -4.3173242 -4.3192592 -4.3194222 -4.3118467 -4.3029089 -4.3006639 -4.290329 -4.2744551 -4.2705 -4.2857571 -4.3022943]]...]
INFO - root - 2017-12-06 10:05:33.395679: step 11910, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 79h:44m:51s remains)
INFO - root - 2017-12-06 10:05:42.604478: step 11920, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 82h:50m:07s remains)
INFO - root - 2017-12-06 10:05:51.798411: step 11930, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 82h:01m:24s remains)
INFO - root - 2017-12-06 10:06:01.048801: step 11940, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 81h:37m:51s remains)
INFO - root - 2017-12-06 10:06:09.903209: step 11950, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 80h:09m:27s remains)
INFO - root - 2017-12-06 10:06:19.059721: step 11960, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.958 sec/batch; 85h:18m:23s remains)
INFO - root - 2017-12-06 10:06:28.210671: step 11970, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 81h:36m:53s remains)
INFO - root - 2017-12-06 10:06:37.406768: step 11980, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 81h:25m:44s remains)
INFO - root - 2017-12-06 10:06:46.631315: step 11990, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 79h:45m:57s remains)
INFO - root - 2017-12-06 10:06:55.798454: step 12000, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 80h:59m:28s remains)
2017-12-06 10:06:56.532744: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2347817 -4.2294793 -4.2356143 -4.2449403 -4.2394967 -4.2266974 -4.2132759 -4.2088919 -4.2131262 -4.220212 -4.2324858 -4.2391963 -4.233253 -4.230464 -4.2314696][-4.2090116 -4.2007575 -4.2089558 -4.2191596 -4.2070665 -4.1820159 -4.1595626 -4.1556554 -4.16807 -4.1841006 -4.2008572 -4.205955 -4.1934261 -4.1863379 -4.1897206][-4.1901407 -4.1793246 -4.1884589 -4.1989174 -4.1781034 -4.1402521 -4.1104026 -4.1101656 -4.1313863 -4.1511917 -4.1686 -4.1740732 -4.15768 -4.1485844 -4.15518][-4.17663 -4.1600471 -4.1701632 -4.1814318 -4.159668 -4.1175523 -4.0862622 -4.0879264 -4.10629 -4.1164827 -4.1321669 -4.1459694 -4.1366167 -4.1298709 -4.1363921][-4.165854 -4.1363711 -4.1409569 -4.1527886 -4.1365829 -4.09808 -4.0681081 -4.0651526 -4.07322 -4.0669985 -4.0796413 -4.1112976 -4.1215949 -4.1206551 -4.1248145][-4.162406 -4.1193538 -4.1146641 -4.1236515 -4.112278 -4.0810804 -4.0544128 -4.040369 -4.0283122 -4.0028048 -4.0130019 -4.0652838 -4.1047435 -4.1198325 -4.1230869][-4.1763153 -4.1284332 -4.11711 -4.1226912 -4.1116667 -4.0821853 -4.0552483 -4.0293493 -3.9962144 -3.9565992 -3.9682653 -4.0351167 -4.0986576 -4.1294923 -4.1343837][-4.194664 -4.1561427 -4.1438713 -4.1448021 -4.1274252 -4.090312 -4.0628109 -4.0355649 -3.9991035 -3.9655135 -3.9841766 -4.0432143 -4.1042728 -4.1398435 -4.1538944][-4.2084179 -4.1786647 -4.163888 -4.1594272 -4.1368055 -4.0990734 -4.08057 -4.063498 -4.0367575 -4.0231071 -4.0473714 -4.0793657 -4.1137657 -4.1405821 -4.1650367][-4.213676 -4.1802063 -4.1596947 -4.1542258 -4.1401019 -4.117 -4.1128645 -4.1074219 -4.0930614 -4.090765 -4.1061687 -4.1119623 -4.1186771 -4.1349015 -4.1633024][-4.212152 -4.1692648 -4.1448908 -4.1443677 -4.1481442 -4.1426153 -4.1477427 -4.1482873 -4.1433005 -4.1436391 -4.14686 -4.1319122 -4.1203837 -4.1273103 -4.1496186][-4.2179971 -4.1680112 -4.1420388 -4.148838 -4.1637955 -4.1664639 -4.1760821 -4.1741681 -4.1688967 -4.1676059 -4.1626806 -4.1375995 -4.1189404 -4.1249857 -4.1419415][-4.2403817 -4.1905589 -4.1680465 -4.1741405 -4.1918244 -4.1926279 -4.1993947 -4.1922879 -4.1823592 -4.179431 -4.1729779 -4.1454854 -4.1301665 -4.1411529 -4.1546545][-4.2615013 -4.2161956 -4.1954522 -4.2011547 -4.2183928 -4.2168832 -4.2164645 -4.2059956 -4.1945443 -4.1953707 -4.1956673 -4.1785712 -4.1719685 -4.1841369 -4.1890116][-4.288321 -4.2490978 -4.2281346 -4.2295871 -4.23791 -4.2317057 -4.2275887 -4.2178626 -4.2132545 -4.22682 -4.2372289 -4.2330613 -4.2304621 -4.2379084 -4.230957]]...]
INFO - root - 2017-12-06 10:07:05.853222: step 12010, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.913 sec/batch; 81h:18m:38s remains)
INFO - root - 2017-12-06 10:07:14.721532: step 12020, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 81h:21m:44s remains)
INFO - root - 2017-12-06 10:07:23.827631: step 12030, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.898 sec/batch; 79h:55m:41s remains)
INFO - root - 2017-12-06 10:07:33.001431: step 12040, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.865 sec/batch; 77h:00m:27s remains)
INFO - root - 2017-12-06 10:07:42.176680: step 12050, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 79h:48m:41s remains)
INFO - root - 2017-12-06 10:07:51.301408: step 12060, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 81h:08m:17s remains)
INFO - root - 2017-12-06 10:08:00.539168: step 12070, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 83h:14m:34s remains)
INFO - root - 2017-12-06 10:08:09.831016: step 12080, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.977 sec/batch; 86h:59m:54s remains)
INFO - root - 2017-12-06 10:08:18.790117: step 12090, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.952 sec/batch; 84h:45m:09s remains)
INFO - root - 2017-12-06 10:08:27.955589: step 12100, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 83h:23m:37s remains)
2017-12-06 10:08:28.606642: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2201719 -4.2155552 -4.2153625 -4.2083187 -4.1936235 -4.1761041 -4.1730351 -4.1923337 -4.2181869 -4.2353864 -4.2474065 -4.2503562 -4.2519379 -4.2568817 -4.2669611][-4.20661 -4.2073555 -4.2112508 -4.2051859 -4.1916647 -4.1770911 -4.1784439 -4.1993856 -4.2272916 -4.2433572 -4.2485905 -4.2428904 -4.2410207 -4.2461476 -4.2539806][-4.19072 -4.1979508 -4.20232 -4.1970754 -4.1864653 -4.17775 -4.1846757 -4.2124829 -4.24586 -4.2650461 -4.2671332 -4.2548184 -4.2446685 -4.2419634 -4.2422652][-4.1865244 -4.1936193 -4.198266 -4.1947985 -4.1846495 -4.1718674 -4.1744022 -4.2057977 -4.2470565 -4.2745867 -4.2830234 -4.2711415 -4.2566929 -4.24545 -4.2377338][-4.2097182 -4.2122707 -4.2131648 -4.2075672 -4.1927533 -4.1655903 -4.1460085 -4.163331 -4.2092814 -4.2565923 -4.2818089 -4.2793055 -4.2675686 -4.2550335 -4.2442932][-4.2442918 -4.2433982 -4.2415671 -4.2321162 -4.2109957 -4.166759 -4.11527 -4.0956659 -4.1327152 -4.2008386 -4.2509069 -4.2671709 -4.2677956 -4.2619066 -4.2560987][-4.2707539 -4.2719088 -4.2701559 -4.2590027 -4.2306633 -4.1703472 -4.0858994 -4.0188007 -4.0294995 -4.1118145 -4.1937613 -4.2338433 -4.2529206 -4.2620587 -4.2653][-4.2880769 -4.2978363 -4.3003445 -4.2898769 -4.2606206 -4.1995072 -4.1014276 -3.9933376 -3.952775 -4.0192833 -4.1200447 -4.187079 -4.2277961 -4.2515111 -4.2634974][-4.2890668 -4.3087473 -4.3199978 -4.3138518 -4.2886572 -4.2429614 -4.1668921 -4.065248 -3.9903193 -3.9993973 -4.0692172 -4.1369524 -4.190074 -4.2250075 -4.2475281][-4.2812362 -4.3038349 -4.3215113 -4.3247871 -4.3116236 -4.2835679 -4.2350678 -4.1640992 -4.0967298 -4.0720291 -4.0934434 -4.1295414 -4.16652 -4.1953511 -4.2194862][-4.2626085 -4.286202 -4.3083196 -4.3219719 -4.3217854 -4.3069096 -4.2778573 -4.2322984 -4.1852403 -4.1595435 -4.1611967 -4.1743255 -4.1880846 -4.1964254 -4.2080007][-4.2500052 -4.2734494 -4.2941818 -4.3110175 -4.3190975 -4.3106074 -4.2880688 -4.2541189 -4.2223282 -4.2050843 -4.2086859 -4.2191577 -4.2283792 -4.2260947 -4.2227654][-4.25285 -4.2731619 -4.2867937 -4.2985272 -4.3047385 -4.2971096 -4.2727842 -4.2392759 -4.2127252 -4.2026744 -4.2136369 -4.2344537 -4.2508683 -4.2516232 -4.2452183][-4.2671576 -4.2784519 -4.2827244 -4.2873092 -4.2882104 -4.2777061 -4.2498355 -4.2145581 -4.1838026 -4.1745949 -4.1920848 -4.2242932 -4.2484112 -4.2549281 -4.2531252][-4.277977 -4.2776675 -4.2764964 -4.2781644 -4.2763 -4.2654181 -4.2384109 -4.2019548 -4.1681037 -4.1594877 -4.1811662 -4.2141972 -4.2370191 -4.242013 -4.2421064]]...]
INFO - root - 2017-12-06 10:08:37.862906: step 12110, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 81h:25m:50s remains)
INFO - root - 2017-12-06 10:08:46.936945: step 12120, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 79h:01m:10s remains)
INFO - root - 2017-12-06 10:08:56.174758: step 12130, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 82h:14m:25s remains)
INFO - root - 2017-12-06 10:09:05.309295: step 12140, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.952 sec/batch; 84h:42m:27s remains)
INFO - root - 2017-12-06 10:09:14.513624: step 12150, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 81h:12m:00s remains)
INFO - root - 2017-12-06 10:09:23.473120: step 12160, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 80h:53m:36s remains)
INFO - root - 2017-12-06 10:09:32.679774: step 12170, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 82h:50m:10s remains)
INFO - root - 2017-12-06 10:09:42.003721: step 12180, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 83h:04m:30s remains)
INFO - root - 2017-12-06 10:09:51.180089: step 12190, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 82h:12m:14s remains)
INFO - root - 2017-12-06 10:10:00.395202: step 12200, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 81h:17m:49s remains)
2017-12-06 10:10:01.189030: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2925887 -4.2979183 -4.301394 -4.302948 -4.3058629 -4.3092036 -4.3097687 -4.307972 -4.3053527 -4.3018231 -4.2990651 -4.2997708 -4.3035126 -4.3085771 -4.3108082][-4.2854528 -4.2939582 -4.2990303 -4.2977509 -4.2972069 -4.2983088 -4.2970843 -4.2950106 -4.2948456 -4.2931194 -4.2926965 -4.2963138 -4.3033156 -4.3110414 -4.3139][-4.2651958 -4.277143 -4.2848191 -4.2811894 -4.2747293 -4.2692723 -4.2645154 -4.2650552 -4.2724509 -4.2780943 -4.2845254 -4.2929807 -4.3024569 -4.3095574 -4.3096933][-4.2312064 -4.2435255 -4.2531943 -4.2474132 -4.2325954 -4.2179914 -4.208837 -4.2155213 -4.2354045 -4.2527189 -4.2682695 -4.2843218 -4.2987723 -4.3060236 -4.3029294][-4.1892333 -4.1980882 -4.2069616 -4.1993856 -4.1760726 -4.1513095 -4.1359372 -4.1478634 -4.1784 -4.2069507 -4.2323747 -4.2581892 -4.2791624 -4.288475 -4.2846003][-4.1480327 -4.1503081 -4.156383 -4.146976 -4.1152644 -4.0784178 -4.0496068 -4.0582347 -4.0997453 -4.1434135 -4.18408 -4.22088 -4.2478075 -4.2563629 -4.2490387][-4.1069293 -4.10102 -4.1008759 -4.0856986 -4.0416641 -3.9834611 -3.9305089 -3.9307184 -3.9899204 -4.0585575 -4.1191645 -4.1706047 -4.2049556 -4.2151108 -4.2053494][-4.0646925 -4.0469036 -4.0342116 -4.0082564 -3.9503431 -3.8675323 -3.7837358 -3.7762752 -3.8632989 -3.9661331 -4.0505476 -4.1116986 -4.151546 -4.1647668 -4.1579084][-4.0307279 -3.9974556 -3.9713593 -3.9374416 -3.8804841 -3.7990561 -3.7113454 -3.701901 -3.8014178 -3.9203129 -4.0119405 -4.0685472 -4.1036239 -4.1189466 -4.1220741][-4.0179682 -3.9756718 -3.9466643 -3.9176025 -3.8833375 -3.840836 -3.793819 -3.7913959 -3.8589597 -3.9503691 -4.019999 -4.0569177 -4.0779052 -4.0899343 -4.1040664][-4.0323858 -3.9925199 -3.9694657 -3.9534752 -3.9430189 -3.9341624 -3.9211853 -3.921356 -3.9541361 -4.0054808 -4.0461526 -4.0677333 -4.0797248 -4.0879817 -4.1038418][-4.0637422 -4.0322995 -4.0189023 -4.0145845 -4.0159521 -4.0217056 -4.0212064 -4.0199389 -4.032372 -4.0590582 -4.0827103 -4.0991483 -4.1113405 -4.1197276 -4.1313171][-4.0999579 -4.0768557 -4.0718713 -4.0747094 -4.0819111 -4.0925083 -4.095767 -4.0935612 -4.0994287 -4.1145039 -4.1304607 -4.145577 -4.1591997 -4.1681647 -4.1754532][-4.136734 -4.1201954 -4.1210952 -4.1277318 -4.1366391 -4.1466784 -4.1491952 -4.1474781 -4.1509919 -4.1601243 -4.1732907 -4.1886 -4.2024636 -4.2120309 -4.2176504][-4.180553 -4.1700296 -4.1730509 -4.1804647 -4.1883063 -4.1942897 -4.194633 -4.1925712 -4.1941524 -4.1999092 -4.2094383 -4.221447 -4.2326169 -4.2412529 -4.2460713]]...]
INFO - root - 2017-12-06 10:10:10.441564: step 12210, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 81h:38m:12s remains)
INFO - root - 2017-12-06 10:10:19.318097: step 12220, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.892 sec/batch; 79h:23m:54s remains)
INFO - root - 2017-12-06 10:10:28.290143: step 12230, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.885 sec/batch; 78h:44m:38s remains)
INFO - root - 2017-12-06 10:10:37.451795: step 12240, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.891 sec/batch; 79h:17m:40s remains)
INFO - root - 2017-12-06 10:10:46.584474: step 12250, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 82h:49m:17s remains)
INFO - root - 2017-12-06 10:10:55.771537: step 12260, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 80h:18m:54s remains)
INFO - root - 2017-12-06 10:11:04.924993: step 12270, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 84h:21m:39s remains)
INFO - root - 2017-12-06 10:11:13.828735: step 12280, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 63h:05m:12s remains)
INFO - root - 2017-12-06 10:11:23.082534: step 12290, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 77h:49m:32s remains)
INFO - root - 2017-12-06 10:11:32.054352: step 12300, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.951 sec/batch; 84h:35m:44s remains)
2017-12-06 10:11:32.722165: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3631163 -4.3269191 -4.2770915 -4.2304082 -4.2063227 -4.2188821 -4.2587762 -4.2912159 -4.304276 -4.2951169 -4.2755928 -4.2624741 -4.2588692 -4.271781 -4.3010964][-4.3686547 -4.3238688 -4.2592912 -4.1940784 -4.1652226 -4.1957345 -4.2454176 -4.2766838 -4.29173 -4.2857409 -4.2662778 -4.2569089 -4.2636185 -4.2871041 -4.3167949][-4.3630152 -4.3143711 -4.2421536 -4.1603594 -4.1277914 -4.171411 -4.2275214 -4.2571177 -4.2715521 -4.265399 -4.2468028 -4.2464285 -4.2651982 -4.294806 -4.324079][-4.3460817 -4.2965722 -4.2304945 -4.152144 -4.1195979 -4.162179 -4.2109628 -4.2305837 -4.2364407 -4.2317429 -4.2198806 -4.2308049 -4.2617774 -4.2987761 -4.3284879][-4.3297577 -4.2791982 -4.2196813 -4.153286 -4.1216316 -4.1467814 -4.1776266 -4.1867723 -4.1834826 -4.1830692 -4.1828632 -4.2080288 -4.247952 -4.2898035 -4.3193893][-4.3222389 -4.2730818 -4.2218132 -4.164361 -4.1241016 -4.1161838 -4.1213689 -4.127634 -4.1271214 -4.137001 -4.1549735 -4.1973715 -4.2387161 -4.2784233 -4.3056445][-4.3341784 -4.2944851 -4.2468233 -4.1863127 -4.1272 -4.0735769 -4.0394664 -4.0486865 -4.070117 -4.1066842 -4.1506519 -4.206563 -4.2480369 -4.2819481 -4.3047147][-4.3575244 -4.3322697 -4.2864809 -4.2114282 -4.1196208 -4.0112972 -3.931093 -3.9558129 -4.0264521 -4.1001368 -4.1683064 -4.2272711 -4.2645006 -4.2937512 -4.312223][-4.3753862 -4.3593378 -4.3104672 -4.2208891 -4.0997066 -3.9561095 -3.8519855 -3.9051 -4.0242095 -4.1219563 -4.1964931 -4.2493582 -4.2824435 -4.3069468 -4.3226066][-4.3911285 -4.3786821 -4.3252025 -4.2322607 -4.1089425 -3.9631853 -3.8685632 -3.9333735 -4.0647655 -4.1651931 -4.2307119 -4.2755747 -4.3021255 -4.319572 -4.3296337][-4.4069595 -4.3943644 -4.3392434 -4.2488494 -4.1371717 -4.0177011 -3.9553039 -4.0165911 -4.1286917 -4.2144761 -4.2657237 -4.2966294 -4.3140459 -4.3257966 -4.3332953][-4.4145145 -4.405797 -4.3548622 -4.2722549 -4.1773648 -4.0911407 -4.0603337 -4.1145644 -4.1993265 -4.2629223 -4.2974019 -4.3118677 -4.3183675 -4.3274384 -4.3374214][-4.4076309 -4.402564 -4.3617225 -4.2922812 -4.2173047 -4.1604147 -4.15034 -4.193748 -4.2533407 -4.2970443 -4.3153157 -4.31395 -4.3107147 -4.3197913 -4.3341475][-4.3902802 -4.3860674 -4.3577156 -4.30771 -4.25517 -4.219245 -4.2186909 -4.2497115 -4.2927885 -4.3218613 -4.3247428 -4.3091569 -4.2981024 -4.3082085 -4.3272858][-4.374052 -4.370707 -4.3515983 -4.319478 -4.2871656 -4.2640114 -4.2688327 -4.2915511 -4.3198905 -4.33214 -4.3187351 -4.2927551 -4.2799683 -4.2944531 -4.3189454]]...]
INFO - root - 2017-12-06 10:11:41.907708: step 12310, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 78h:25m:28s remains)
INFO - root - 2017-12-06 10:11:51.058393: step 12320, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.940 sec/batch; 83h:35m:25s remains)
INFO - root - 2017-12-06 10:12:00.284586: step 12330, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.925 sec/batch; 82h:14m:02s remains)
INFO - root - 2017-12-06 10:12:09.501095: step 12340, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 84h:55m:38s remains)
INFO - root - 2017-12-06 10:12:18.679864: step 12350, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 82h:52m:45s remains)
INFO - root - 2017-12-06 10:12:27.863348: step 12360, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 78h:41m:47s remains)
INFO - root - 2017-12-06 10:12:36.789882: step 12370, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.880 sec/batch; 78h:13m:20s remains)
INFO - root - 2017-12-06 10:12:45.972440: step 12380, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.943 sec/batch; 83h:50m:25s remains)
INFO - root - 2017-12-06 10:12:55.162197: step 12390, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 81h:26m:17s remains)
INFO - root - 2017-12-06 10:13:04.258226: step 12400, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 80h:57m:30s remains)
2017-12-06 10:13:04.981576: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1657624 -4.1560764 -4.162653 -4.1773839 -4.1894951 -4.20244 -4.2132411 -4.2122207 -4.1986818 -4.1832376 -4.1723385 -4.1707621 -4.1747842 -4.1800361 -4.1970358][-4.17649 -4.1714258 -4.1772332 -4.1849079 -4.1876707 -4.1923752 -4.1959248 -4.1909127 -4.1763468 -4.1646242 -4.1601863 -4.1628017 -4.166976 -4.1708837 -4.1896429][-4.21957 -4.2166691 -4.2175393 -4.2128539 -4.2035751 -4.1986284 -4.1942968 -4.1867437 -4.1765103 -4.1752405 -4.1830139 -4.19396 -4.2003956 -4.2031903 -4.2168422][-4.2549992 -4.24972 -4.248076 -4.2375751 -4.222352 -4.2112217 -4.1998658 -4.1876979 -4.177958 -4.1848226 -4.2041545 -4.224999 -4.2366104 -4.2405686 -4.2493749][-4.2440519 -4.2326427 -4.2359424 -4.2359452 -4.2283068 -4.2145538 -4.1921329 -4.1631103 -4.1450849 -4.1579351 -4.18801 -4.2180543 -4.2365355 -4.2442594 -4.24981][-4.1974821 -4.1778626 -4.1849403 -4.1965094 -4.1975102 -4.1817522 -4.1444154 -4.0876479 -4.0525823 -4.0693831 -4.1115413 -4.1546631 -4.1847868 -4.1998491 -4.2053204][-4.1669679 -4.1454372 -4.1472292 -4.156745 -4.156538 -4.1333885 -4.0783224 -3.9957106 -3.9440799 -3.9591634 -4.00422 -4.0502796 -4.08801 -4.11276 -4.1230283][-4.1886477 -4.1753025 -4.1730342 -4.1725693 -4.1623087 -4.1331139 -4.0760818 -3.9930606 -3.940073 -3.9499054 -3.9828477 -4.0111961 -4.0313873 -4.0439653 -4.0497465][-4.2262473 -4.2207074 -4.2151976 -4.2099667 -4.1985884 -4.1806278 -4.1482778 -4.0940833 -4.0559673 -4.0567541 -4.0699782 -4.0785723 -4.0813313 -4.0718923 -4.0545621][-4.2284555 -4.22356 -4.2144632 -4.2079225 -4.20167 -4.20107 -4.1974735 -4.1745176 -4.153203 -4.1491637 -4.1498508 -4.1496334 -4.1534395 -4.1465006 -4.1226344][-4.207305 -4.1983175 -4.1860495 -4.1794419 -4.1779337 -4.1886768 -4.2008543 -4.1971407 -4.1906977 -4.1913033 -4.1870823 -4.1788859 -4.1800966 -4.18075 -4.1657224][-4.2113662 -4.1978602 -4.1823626 -4.1756773 -4.1741934 -4.1841578 -4.1948304 -4.1956487 -4.1972094 -4.20218 -4.196229 -4.1824327 -4.1792326 -4.1804748 -4.1734653][-4.2242446 -4.2113013 -4.2004175 -4.193871 -4.1888485 -4.1919079 -4.1948967 -4.1949282 -4.1989789 -4.2013474 -4.1926727 -4.1793542 -4.1757245 -4.1735649 -4.1659565][-4.2238865 -4.2217484 -4.2221756 -4.2197208 -4.21211 -4.2103539 -4.2043166 -4.1995997 -4.2003975 -4.197207 -4.1852837 -4.1759586 -4.1765933 -4.1732783 -4.1613441][-4.1974163 -4.2123427 -4.2272077 -4.2312622 -4.223629 -4.2204623 -4.2104287 -4.2045517 -4.2000036 -4.185504 -4.1701179 -4.1666555 -4.1736436 -4.1738234 -4.160162]]...]
INFO - root - 2017-12-06 10:13:14.172255: step 12410, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.923 sec/batch; 82h:03m:32s remains)
INFO - root - 2017-12-06 10:13:23.269366: step 12420, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 77h:32m:01s remains)
INFO - root - 2017-12-06 10:13:32.412537: step 12430, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 82h:17m:00s remains)
INFO - root - 2017-12-06 10:13:41.386199: step 12440, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 81h:31m:27s remains)
INFO - root - 2017-12-06 10:13:50.520625: step 12450, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 80h:29m:50s remains)
INFO - root - 2017-12-06 10:13:59.659471: step 12460, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 79h:08m:57s remains)
INFO - root - 2017-12-06 10:14:08.856586: step 12470, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 78h:19m:09s remains)
INFO - root - 2017-12-06 10:14:18.040988: step 12480, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 80h:40m:26s remains)
INFO - root - 2017-12-06 10:14:27.200893: step 12490, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 76h:20m:26s remains)
INFO - root - 2017-12-06 10:14:36.480118: step 12500, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 83h:03m:57s remains)
2017-12-06 10:14:37.119866: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2394342 -4.2385883 -4.2290888 -4.2079725 -4.1780128 -4.1538525 -4.1473913 -4.1633034 -4.1953549 -4.22243 -4.2365851 -4.2499547 -4.2611222 -4.2645226 -4.2586956][-4.2018528 -4.2014523 -4.1937113 -4.1764927 -4.1546736 -4.132473 -4.1224003 -4.13864 -4.1761341 -4.2096357 -4.2295966 -4.2477303 -4.262908 -4.2672963 -4.2594151][-4.1644325 -4.1691117 -4.166358 -4.1572223 -4.1476688 -4.1347756 -4.1234465 -4.1311893 -4.1621 -4.1947093 -4.220921 -4.24899 -4.2736526 -4.2858186 -4.2812467][-4.153903 -4.165338 -4.1651807 -4.1608515 -4.1587815 -4.150454 -4.1357741 -4.13037 -4.1496716 -4.1806726 -4.2127323 -4.2491732 -4.2793207 -4.2973223 -4.2986622][-4.1758828 -4.1878839 -4.1851006 -4.1801338 -4.1753788 -4.1636724 -4.1441298 -4.12833 -4.1357942 -4.161335 -4.1921296 -4.2318397 -4.2667403 -4.2904119 -4.2966771][-4.2072196 -4.2149792 -4.2104869 -4.20413 -4.1915889 -4.1707067 -4.1455183 -4.1207085 -4.114522 -4.1318007 -4.1605506 -4.20268 -4.2452192 -4.2721534 -4.2792091][-4.24237 -4.2456889 -4.2372408 -4.2239113 -4.1975813 -4.163765 -4.1291809 -4.0959206 -4.08046 -4.0936871 -4.128458 -4.1782651 -4.2304316 -4.2607956 -4.2655568][-4.2632322 -4.2613654 -4.2494941 -4.2283006 -4.1918125 -4.1508579 -4.108017 -4.068573 -4.0476842 -4.0622554 -4.1083555 -4.1672292 -4.224607 -4.2582459 -4.2649837][-4.2574573 -4.2498817 -4.23601 -4.21282 -4.1762133 -4.1360908 -4.0916886 -4.0508809 -4.0332212 -4.0568614 -4.1151986 -4.1772108 -4.2307119 -4.2648387 -4.275609][-4.2323613 -4.2173047 -4.2020688 -4.1798778 -4.1480479 -4.114892 -4.0790534 -4.053194 -4.0553308 -4.0965815 -4.1580706 -4.210577 -4.2486677 -4.2737074 -4.2812314][-4.1819391 -4.1618733 -4.1512389 -4.13645 -4.1151328 -4.0924439 -4.0729327 -4.0742373 -4.1077075 -4.1664691 -4.2246919 -4.2596121 -4.2758265 -4.2814255 -4.2747049][-4.1211624 -4.1058688 -4.1097636 -4.1114182 -4.1066847 -4.09932 -4.1028757 -4.1310353 -4.1846089 -4.2445397 -4.289454 -4.3061872 -4.3006353 -4.2839718 -4.2613144][-4.1072893 -4.1032557 -4.1231861 -4.142643 -4.1538434 -4.16229 -4.1800933 -4.2151818 -4.2649617 -4.311307 -4.336112 -4.3330274 -4.3091464 -4.2788219 -4.24546][-4.1404018 -4.1494565 -4.1778364 -4.20465 -4.2197194 -4.2282414 -4.2439365 -4.27108 -4.3084621 -4.339983 -4.3469677 -4.3300452 -4.3005738 -4.2681942 -4.2328753][-4.1952066 -4.20934 -4.2379708 -4.2635689 -4.2727041 -4.2692056 -4.2725811 -4.2865844 -4.308558 -4.3269286 -4.3255806 -4.307611 -4.2841177 -4.2608728 -4.2364135]]...]
INFO - root - 2017-12-06 10:14:45.957627: step 12510, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 80h:53m:51s remains)
INFO - root - 2017-12-06 10:14:55.074267: step 12520, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 77h:20m:36s remains)
INFO - root - 2017-12-06 10:15:04.379285: step 12530, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 82h:21m:54s remains)
INFO - root - 2017-12-06 10:15:13.514399: step 12540, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 81h:37m:12s remains)
INFO - root - 2017-12-06 10:15:22.566030: step 12550, loss = 2.09, batch loss = 2.04 (8.6 examples/sec; 0.926 sec/batch; 82h:17m:59s remains)
INFO - root - 2017-12-06 10:15:31.744761: step 12560, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 84h:03m:04s remains)
INFO - root - 2017-12-06 10:15:40.802166: step 12570, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.896 sec/batch; 79h:37m:36s remains)
INFO - root - 2017-12-06 10:15:49.909753: step 12580, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.924 sec/batch; 82h:07m:13s remains)
INFO - root - 2017-12-06 10:15:58.793797: step 12590, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.904 sec/batch; 80h:22m:37s remains)
INFO - root - 2017-12-06 10:16:07.896051: step 12600, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 81h:13m:12s remains)
2017-12-06 10:16:08.823009: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1404042 -4.1379471 -4.1259518 -4.100162 -4.0897064 -4.1043868 -4.1302381 -4.1517262 -4.15886 -4.1478529 -4.125246 -4.10988 -4.1292834 -4.161149 -4.188138][-4.1250715 -4.1267915 -4.1059341 -4.0696497 -4.0530777 -4.0624409 -4.0840592 -4.1055503 -4.1164951 -4.1103158 -4.0852175 -4.0652781 -4.0864072 -4.1302109 -4.1723919][-4.12429 -4.1237187 -4.0968461 -4.0552421 -4.0351548 -4.0377746 -4.0532541 -4.073874 -4.0947824 -4.1016555 -4.0805664 -4.0554771 -4.0671921 -4.1080222 -4.1564741][-4.1113963 -4.1096997 -4.0834723 -4.04342 -4.022788 -4.0168428 -4.0245275 -4.0502191 -4.0852642 -4.1058116 -4.0935674 -4.0755625 -4.082489 -4.1075835 -4.1492648][-4.1092334 -4.1036253 -4.072032 -4.0328069 -4.0085955 -3.990468 -3.9838309 -4.0076056 -4.053369 -4.0942311 -4.10553 -4.1062155 -4.1137981 -4.12399 -4.1523523][-4.1148386 -4.1070824 -4.0750127 -4.0391636 -4.0077329 -3.9716372 -3.9462643 -3.9559977 -4.0077391 -4.0656204 -4.1045847 -4.1297846 -4.1442518 -4.149179 -4.1663003][-4.1158991 -4.1153655 -4.0926337 -4.0632343 -4.0265121 -3.9724052 -3.9254322 -3.9177165 -3.9654508 -4.0271811 -4.0813012 -4.1302919 -4.1619635 -4.1741195 -4.1884661][-4.1144924 -4.1215224 -4.1101403 -4.08776 -4.0489507 -3.990169 -3.9422555 -3.9287782 -3.9639845 -4.0096049 -4.0551109 -4.1081047 -4.1543746 -4.1838722 -4.2029066][-4.127326 -4.1367579 -4.1317949 -4.1152534 -4.0807319 -4.0345578 -3.9998496 -3.9859028 -4.00155 -4.022747 -4.0516644 -4.0959535 -4.1475515 -4.1910639 -4.2139792][-4.1602964 -4.1686368 -4.1645064 -4.1514845 -4.1229815 -4.0889945 -4.0652404 -4.0538239 -4.0582886 -4.0647831 -4.0830541 -4.1154828 -4.1617665 -4.2047939 -4.2276058][-4.1930723 -4.2015905 -4.198667 -4.1869493 -4.1628623 -4.1351352 -4.1179948 -4.1112866 -4.1127715 -4.1138253 -4.12461 -4.1466417 -4.1887817 -4.2279434 -4.248837][-4.2165275 -4.220881 -4.2204652 -4.2131205 -4.1941972 -4.1731715 -4.1607471 -4.1580215 -4.1604156 -4.1615934 -4.1649275 -4.1782937 -4.2165108 -4.2529206 -4.2717419][-4.2398324 -4.2389607 -4.2349687 -4.2307572 -4.2202473 -4.2078733 -4.2023172 -4.2012825 -4.202354 -4.2027969 -4.2044764 -4.2156296 -4.2464457 -4.2771754 -4.2921557][-4.265779 -4.2637162 -4.2574935 -4.2537136 -4.2491021 -4.2445951 -4.242835 -4.2409139 -4.2412353 -4.2439127 -4.2461824 -4.2553163 -4.2773752 -4.2989726 -4.3079443][-4.2828922 -4.2818503 -4.2765784 -4.2728667 -4.27067 -4.2695227 -4.2687373 -4.2673192 -4.2672358 -4.2708259 -4.2745948 -4.281467 -4.295785 -4.3090825 -4.3156881]]...]
INFO - root - 2017-12-06 10:16:18.039485: step 12610, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.888 sec/batch; 78h:54m:00s remains)
INFO - root - 2017-12-06 10:16:27.054835: step 12620, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 82h:41m:33s remains)
INFO - root - 2017-12-06 10:16:36.341749: step 12630, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 80h:51m:16s remains)
INFO - root - 2017-12-06 10:16:45.465068: step 12640, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.941 sec/batch; 83h:37m:11s remains)
INFO - root - 2017-12-06 10:16:54.748505: step 12650, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 84h:25m:23s remains)
INFO - root - 2017-12-06 10:17:03.853400: step 12660, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.949 sec/batch; 84h:20m:54s remains)
INFO - root - 2017-12-06 10:17:13.026989: step 12670, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 79h:29m:19s remains)
INFO - root - 2017-12-06 10:17:22.184844: step 12680, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.908 sec/batch; 80h:39m:13s remains)
INFO - root - 2017-12-06 10:17:31.487243: step 12690, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 84h:29m:18s remains)
INFO - root - 2017-12-06 10:17:40.709271: step 12700, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 85h:52m:10s remains)
2017-12-06 10:17:41.393112: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1307492 -4.1500635 -4.1797462 -4.1957912 -4.2151017 -4.2566385 -4.2984786 -4.3163261 -4.3062882 -4.2600484 -4.2273378 -4.2092218 -4.1861238 -4.1506867 -4.1185555][-4.1348166 -4.1485858 -4.1781569 -4.1953869 -4.2183704 -4.2583818 -4.2910666 -4.2954092 -4.2800741 -4.2271528 -4.187696 -4.1675506 -4.1567235 -4.1396909 -4.1152186][-4.1277246 -4.1329308 -4.1598763 -4.1750045 -4.2010193 -4.2400947 -4.2629218 -4.2561007 -4.2343569 -4.1813154 -4.14224 -4.1223965 -4.1221018 -4.1198845 -4.1032238][-4.12771 -4.1190963 -4.1353345 -4.1429687 -4.1623359 -4.2022314 -4.2212582 -4.2083683 -4.1852646 -4.1390247 -4.1050234 -4.087769 -4.0906463 -4.0971937 -4.0902944][-4.1476679 -4.1242356 -4.121778 -4.1119385 -4.11681 -4.155127 -4.1743808 -4.1613984 -4.1438761 -4.1100059 -4.0835052 -4.0734987 -4.08076 -4.0939221 -4.0967603][-4.1629605 -4.1272593 -4.1134248 -4.0953789 -4.0887356 -4.1145267 -4.1320195 -4.1264424 -4.1137376 -4.0835662 -4.062624 -4.0740747 -4.0953794 -4.1101 -4.1167684][-4.1425118 -4.095119 -4.0762568 -4.0593233 -4.051846 -4.0744767 -4.1029634 -4.1112518 -4.1061416 -4.0767474 -4.0587831 -4.0903344 -4.1225295 -4.1281781 -4.1339874][-4.0949974 -4.0480685 -4.0328832 -4.017983 -4.0183349 -4.0496154 -4.0951047 -4.1086578 -4.10754 -4.0801415 -4.068315 -4.1114092 -4.1488838 -4.1528687 -4.1640439][-4.0704107 -4.0419745 -4.0362582 -4.0201159 -4.0273156 -4.0649991 -4.10871 -4.1162577 -4.1065841 -4.0791659 -4.06993 -4.116436 -4.1593375 -4.1653228 -4.1829047][-4.0721855 -4.059588 -4.0676818 -4.0655317 -4.084868 -4.11951 -4.1478171 -4.1450291 -4.1318951 -4.1150155 -4.1087627 -4.1462965 -4.1904726 -4.1981359 -4.2108831][-4.0887938 -4.0841937 -4.1005836 -4.1127062 -4.143465 -4.1763477 -4.193398 -4.1899319 -4.1838264 -4.1833229 -4.1840305 -4.2139812 -4.25231 -4.2613444 -4.2663331][-4.1244278 -4.1207256 -4.1390467 -4.1548214 -4.180233 -4.2097521 -4.2284083 -4.2341957 -4.2351122 -4.2418437 -4.2479439 -4.2719817 -4.3035665 -4.313221 -4.3128843][-4.1784949 -4.180367 -4.1959114 -4.2028766 -4.2177143 -4.2458663 -4.2649884 -4.2746272 -4.2780986 -4.2858591 -4.2952523 -4.3121929 -4.3329215 -4.338306 -4.3345675][-4.2320414 -4.2359095 -4.2446308 -4.2402034 -4.2470655 -4.2756095 -4.2944064 -4.3004408 -4.3012953 -4.3054428 -4.312727 -4.322648 -4.3341284 -4.33591 -4.3294673][-4.2517848 -4.2539911 -4.2602377 -4.2506733 -4.2533631 -4.2807364 -4.296701 -4.2969594 -4.2942715 -4.2940397 -4.2963414 -4.3003368 -4.3075337 -4.3090482 -4.3045983]]...]
INFO - root - 2017-12-06 10:17:50.543173: step 12710, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.881 sec/batch; 78h:17m:59s remains)
INFO - root - 2017-12-06 10:17:59.663797: step 12720, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 74h:29m:18s remains)
INFO - root - 2017-12-06 10:18:08.831123: step 12730, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 80h:29m:49s remains)
INFO - root - 2017-12-06 10:18:17.936881: step 12740, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 77h:50m:25s remains)
INFO - root - 2017-12-06 10:18:26.918602: step 12750, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 78h:57m:20s remains)
INFO - root - 2017-12-06 10:18:36.175084: step 12760, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.918 sec/batch; 81h:30m:26s remains)
INFO - root - 2017-12-06 10:18:45.319437: step 12770, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 78h:03m:15s remains)
INFO - root - 2017-12-06 10:18:54.536486: step 12780, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.971 sec/batch; 86h:15m:01s remains)
INFO - root - 2017-12-06 10:19:03.730570: step 12790, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.953 sec/batch; 84h:38m:50s remains)
INFO - root - 2017-12-06 10:19:12.769460: step 12800, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.956 sec/batch; 84h:52m:59s remains)
2017-12-06 10:19:13.469579: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2310023 -4.2042494 -4.1671648 -4.1188097 -4.0775752 -4.0581789 -4.0704546 -4.1154585 -4.1816773 -4.2488623 -4.2974081 -4.3191738 -4.3240075 -4.324492 -4.3275914][-4.238163 -4.2404447 -4.2344151 -4.2176652 -4.1955414 -4.1782627 -4.1748452 -4.1905761 -4.2278967 -4.2741961 -4.3113403 -4.3251791 -4.3253503 -4.3229303 -4.324028][-4.2478538 -4.2578197 -4.261518 -4.256042 -4.2393241 -4.2208962 -4.21919 -4.2331748 -4.2631383 -4.2990217 -4.3277111 -4.3354545 -4.3310909 -4.3244705 -4.321691][-4.2598228 -4.2564907 -4.2499475 -4.2379289 -4.2157793 -4.1921153 -4.1930943 -4.2168417 -4.2557716 -4.2963481 -4.330503 -4.3421912 -4.3388114 -4.3300076 -4.3228035][-4.2795725 -4.253221 -4.2167106 -4.1750636 -4.1347365 -4.1081219 -4.1190238 -4.1592903 -4.210475 -4.2642913 -4.3100524 -4.3343782 -4.3397846 -4.3340092 -4.326364][-4.3000059 -4.2568846 -4.1907806 -4.1095004 -4.0338988 -3.9912794 -4.0046711 -4.0581965 -4.1251507 -4.1957812 -4.2593474 -4.3042078 -4.3269367 -4.3315239 -4.3275189][-4.3057294 -4.2683454 -4.1977415 -4.097929 -3.992687 -3.9126935 -3.8956006 -3.9433951 -4.0249338 -4.1131086 -4.1939569 -4.2601719 -4.3035588 -4.3239851 -4.3265386][-4.2837 -4.2672 -4.2234831 -4.1453443 -4.0492096 -3.9529307 -3.8859706 -3.8875723 -3.955318 -4.0472655 -4.1380019 -4.2188349 -4.2789931 -4.3133254 -4.3229823][-4.2327538 -4.2411447 -4.239449 -4.2117529 -4.15901 -4.0884209 -4.0156078 -3.9750526 -3.9857609 -4.0380545 -4.1127038 -4.1949439 -4.2625241 -4.3046761 -4.3182769][-4.1824093 -4.2144709 -4.2471981 -4.2620559 -4.2515211 -4.220232 -4.1745629 -4.1339526 -4.11668 -4.1253695 -4.1596293 -4.2146344 -4.2689724 -4.3068914 -4.3194189][-4.16994 -4.2139783 -4.2553725 -4.2826443 -4.2918448 -4.2863855 -4.2649531 -4.2398381 -4.2258544 -4.2252855 -4.239027 -4.2686577 -4.3008294 -4.3232236 -4.3272719][-4.2014937 -4.2438316 -4.2763772 -4.2933512 -4.2957525 -4.2925458 -4.2781129 -4.262845 -4.2602315 -4.2687268 -4.2842155 -4.3085027 -4.3319192 -4.343442 -4.3387818][-4.2606788 -4.2882781 -4.2987657 -4.2911553 -4.2710714 -4.2523165 -4.2304645 -4.2196126 -4.2293425 -4.2524304 -4.28053 -4.3144956 -4.3448844 -4.3585572 -4.3524475][-4.3066463 -4.3135138 -4.3024931 -4.2707067 -4.2214465 -4.1753082 -4.1402693 -4.1317034 -4.1551604 -4.1945753 -4.2401028 -4.2892408 -4.3318868 -4.3555651 -4.3567953][-4.3140016 -4.3076477 -4.2909088 -4.2519054 -4.1870937 -4.1122956 -4.0473952 -4.0252466 -4.0509553 -4.1000261 -4.1611123 -4.2284532 -4.2886925 -4.3298831 -4.3453918]]...]
INFO - root - 2017-12-06 10:19:22.506352: step 12810, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.749 sec/batch; 66h:28m:25s remains)
INFO - root - 2017-12-06 10:19:31.524517: step 12820, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.907 sec/batch; 80h:29m:53s remains)
INFO - root - 2017-12-06 10:19:40.607783: step 12830, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 80h:59m:45s remains)
INFO - root - 2017-12-06 10:19:49.771091: step 12840, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 80h:57m:04s remains)
INFO - root - 2017-12-06 10:19:58.893160: step 12850, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 81h:26m:49s remains)
INFO - root - 2017-12-06 10:20:08.039714: step 12860, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.958 sec/batch; 85h:04m:02s remains)
INFO - root - 2017-12-06 10:20:16.960695: step 12870, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.940 sec/batch; 83h:27m:08s remains)
INFO - root - 2017-12-06 10:20:25.900124: step 12880, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.942 sec/batch; 83h:38m:23s remains)
INFO - root - 2017-12-06 10:20:35.062756: step 12890, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 78h:37m:49s remains)
INFO - root - 2017-12-06 10:20:44.216757: step 12900, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 82h:02m:14s remains)
2017-12-06 10:20:44.893019: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2976947 -4.3082595 -4.3178191 -4.3209124 -4.3187222 -4.3186846 -4.3220744 -4.3260055 -4.3278227 -4.3277917 -4.3249846 -4.3124309 -4.2906337 -4.2715816 -4.2653422][-4.27305 -4.2927313 -4.307672 -4.3103285 -4.3028646 -4.2979503 -4.2984624 -4.3001227 -4.3040614 -4.3094482 -4.3085856 -4.2914 -4.2591376 -4.2282448 -4.2187686][-4.2396207 -4.2675672 -4.2886238 -4.2934256 -4.2794824 -4.2662106 -4.2574267 -4.2496133 -4.2552814 -4.2710609 -4.2787733 -4.2626247 -4.2244172 -4.1888618 -4.1805182][-4.2044158 -4.2350879 -4.2602372 -4.2661819 -4.2456703 -4.2212772 -4.1973066 -4.1758122 -4.1883607 -4.2222824 -4.2442193 -4.2365022 -4.200902 -4.1682124 -4.1607075][-4.1756997 -4.2054224 -4.2289252 -4.2289824 -4.196156 -4.152801 -4.1025648 -4.0661774 -4.0988336 -4.1617579 -4.2067437 -4.2129812 -4.184834 -4.1588321 -4.1513362][-4.1647019 -4.19269 -4.2114654 -4.2023816 -4.1560783 -4.0899658 -4.0052843 -3.9501834 -4.0077195 -4.1040092 -4.1712623 -4.1921268 -4.1738052 -4.1559825 -4.1475358][-4.1690645 -4.196938 -4.2159514 -4.2028618 -4.147934 -4.0638771 -3.9477494 -3.8736761 -3.9543333 -4.0721307 -4.1505809 -4.1839809 -4.1769161 -4.164278 -4.1518555][-4.1694989 -4.20213 -4.2300887 -4.2243252 -4.1735816 -4.0869508 -3.9661248 -3.8886049 -3.9666505 -4.0782642 -4.1547575 -4.1944885 -4.1949978 -4.1852503 -4.1701179][-4.1660538 -4.2044435 -4.2412672 -4.2481256 -4.2113037 -4.1405272 -4.0431485 -3.9833696 -4.0378489 -4.1214342 -4.1838937 -4.2190623 -4.222084 -4.2163529 -4.20169][-4.1666741 -4.2024693 -4.2372875 -4.2530909 -4.2357154 -4.1905279 -4.1272569 -4.0894113 -4.123435 -4.1750793 -4.2141833 -4.2373385 -4.2406783 -4.2393312 -4.2268591][-4.1738367 -4.1993179 -4.2207494 -4.2306037 -4.2256479 -4.2073541 -4.1766019 -4.1562452 -4.175292 -4.2035074 -4.22356 -4.23598 -4.2387996 -4.2396731 -4.2307472][-4.1813164 -4.1952982 -4.1993895 -4.1990747 -4.2010908 -4.2019444 -4.196332 -4.1880484 -4.1950665 -4.2063413 -4.2115283 -4.2141652 -4.2138681 -4.2144761 -4.2110767][-4.1844211 -4.1893387 -4.1921372 -4.1927371 -4.1987596 -4.2077494 -4.2151618 -4.2135358 -4.2096395 -4.2023253 -4.1949944 -4.1921539 -4.1924539 -4.1964617 -4.1994052][-4.1756926 -4.1737533 -4.1887841 -4.20752 -4.2222672 -4.2317657 -4.2393241 -4.238071 -4.2273641 -4.2071381 -4.1914539 -4.1857166 -4.18914 -4.1969585 -4.2033691][-4.1569314 -4.1451583 -4.169826 -4.2072611 -4.2319827 -4.241868 -4.2464581 -4.2470851 -4.2369585 -4.215097 -4.1980896 -4.1931314 -4.1986985 -4.2075725 -4.2145462]]...]
INFO - root - 2017-12-06 10:20:54.059836: step 12910, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.894 sec/batch; 79h:22m:42s remains)
INFO - root - 2017-12-06 10:21:03.243431: step 12920, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 80h:01m:33s remains)
INFO - root - 2017-12-06 10:21:12.434399: step 12930, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 79h:40m:41s remains)
INFO - root - 2017-12-06 10:21:21.395710: step 12940, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.892 sec/batch; 79h:09m:30s remains)
INFO - root - 2017-12-06 10:21:30.552855: step 12950, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.976 sec/batch; 86h:36m:15s remains)
INFO - root - 2017-12-06 10:21:39.818866: step 12960, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.930 sec/batch; 82h:33m:43s remains)
INFO - root - 2017-12-06 10:21:49.048817: step 12970, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 82h:19m:53s remains)
INFO - root - 2017-12-06 10:21:58.235544: step 12980, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 79h:29m:43s remains)
INFO - root - 2017-12-06 10:22:07.282932: step 12990, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 77h:50m:46s remains)
INFO - root - 2017-12-06 10:22:16.521160: step 13000, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 80h:27m:13s remains)
2017-12-06 10:22:17.211776: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.308722 -4.3128982 -4.3102984 -4.3041477 -4.2981014 -4.2940044 -4.290926 -4.288177 -4.2866631 -4.2870636 -4.2890038 -4.2919579 -4.2956257 -4.29983 -4.3034792][-4.30744 -4.3092709 -4.305665 -4.2986064 -4.2905068 -4.2830195 -4.2756567 -4.2686782 -4.2633896 -4.2621412 -4.2653713 -4.2721663 -4.28119 -4.2913771 -4.3001342][-4.3090782 -4.3079257 -4.3022838 -4.2917109 -4.2770381 -4.2593222 -4.2398477 -4.2221718 -4.2098904 -4.2081127 -4.2172847 -4.2339516 -4.2539954 -4.2743559 -4.2919035][-4.3060122 -4.3029356 -4.2934914 -4.2764115 -4.2503037 -4.2147136 -4.1748738 -4.1397085 -4.117897 -4.1171193 -4.13718 -4.1712651 -4.2095633 -4.2459116 -4.276041][-4.2895112 -4.2871647 -4.27517 -4.2510962 -4.2108092 -4.1524711 -4.0869536 -4.0327768 -4.0052533 -4.0109177 -4.0486012 -4.1063132 -4.1653585 -4.218152 -4.2601671][-4.2548738 -4.2550683 -4.2464228 -4.2237568 -4.1797743 -4.11006 -4.0267067 -3.9591508 -3.9316964 -3.9477534 -4.0036936 -4.0814567 -4.1546254 -4.2141767 -4.2580261][-4.2061605 -4.2116694 -4.2140779 -4.2058339 -4.177916 -4.1246591 -4.0530143 -3.9928303 -3.9708056 -3.9903073 -4.0469861 -4.1210561 -4.1867857 -4.23618 -4.268991][-4.1447873 -4.1556687 -4.1755838 -4.1926928 -4.1959138 -4.1768661 -4.1349459 -4.0941296 -4.0787735 -4.0940132 -4.1367316 -4.1906433 -4.2357683 -4.2661376 -4.2833138][-4.09633 -4.1087241 -4.1425853 -4.1817722 -4.2137537 -4.2273655 -4.2160516 -4.1958747 -4.1853275 -4.1933174 -4.2187476 -4.2510834 -4.2771153 -4.2915254 -4.295857][-4.0762987 -4.0851669 -4.1230197 -4.1702929 -4.21965 -4.2570395 -4.2696638 -4.266984 -4.2622037 -4.2640772 -4.275034 -4.29101 -4.3036895 -4.307013 -4.3022771][-4.0806255 -4.082346 -4.1107612 -4.1523819 -4.2074814 -4.2598014 -4.2910581 -4.3031116 -4.3031263 -4.298615 -4.2969317 -4.3003888 -4.3041887 -4.3019896 -4.2936335][-4.0889196 -4.0811949 -4.0975394 -4.1307912 -4.1847286 -4.2434306 -4.2843056 -4.3032889 -4.301919 -4.2884421 -4.2757244 -4.2719655 -4.2736554 -4.27357 -4.2693925][-4.1021328 -4.0889297 -4.09793 -4.1226244 -4.1711254 -4.2289686 -4.2694187 -4.2815375 -4.2666955 -4.2365241 -4.2113938 -4.2044272 -4.21077 -4.2203531 -4.2256813][-4.1278872 -4.117003 -4.1242628 -4.1428804 -4.1816688 -4.2292242 -4.2565923 -4.2499957 -4.2117772 -4.160501 -4.12461 -4.1180434 -4.1327128 -4.1554074 -4.1714177][-4.1687303 -4.1617155 -4.1708288 -4.1858096 -4.2138691 -4.2471251 -4.2577295 -4.2302613 -4.1705694 -4.1019812 -4.0607309 -4.0592456 -4.0847836 -4.1193075 -4.1421218]]...]
INFO - root - 2017-12-06 10:22:26.056706: step 13010, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 75h:52m:15s remains)
INFO - root - 2017-12-06 10:22:35.207519: step 13020, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 82h:19m:56s remains)
INFO - root - 2017-12-06 10:22:44.383714: step 13030, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.957 sec/batch; 84h:56m:19s remains)
INFO - root - 2017-12-06 10:22:53.539029: step 13040, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 79h:51m:49s remains)
INFO - root - 2017-12-06 10:23:02.622992: step 13050, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 86h:42m:54s remains)
INFO - root - 2017-12-06 10:23:11.842167: step 13060, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.003 sec/batch; 88h:58m:19s remains)
INFO - root - 2017-12-06 10:23:21.060540: step 13070, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 79h:54m:28s remains)
INFO - root - 2017-12-06 10:23:29.839198: step 13080, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.936 sec/batch; 83h:01m:03s remains)
INFO - root - 2017-12-06 10:23:38.949266: step 13090, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 81h:46m:08s remains)
INFO - root - 2017-12-06 10:23:48.109904: step 13100, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 81h:08m:33s remains)
2017-12-06 10:23:48.775997: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3225794 -4.3093662 -4.2952495 -4.285501 -4.2813573 -4.2775245 -4.272851 -4.270875 -4.2693248 -4.26329 -4.2600303 -4.2610035 -4.266305 -4.2707615 -4.2719116][-4.302999 -4.2836742 -4.2636867 -4.2467203 -4.2365317 -4.2277031 -4.217495 -4.2149291 -4.2207284 -4.2197022 -4.2169704 -4.2208934 -4.2305183 -4.23726 -4.2330923][-4.2739792 -4.2449117 -4.2155018 -4.1857648 -4.1624117 -4.1425743 -4.129035 -4.1330957 -4.1489716 -4.1554155 -4.1545067 -4.1625395 -4.1765785 -4.1849208 -4.1786776][-4.2413206 -4.2022605 -4.1609092 -4.1197658 -4.0827007 -4.05037 -4.0344357 -4.0465021 -4.0672169 -4.0761871 -4.0767326 -4.0872974 -4.1083436 -4.1230574 -4.1234937][-4.2094193 -4.1605277 -4.105134 -4.0501266 -3.9984639 -3.9526038 -3.9298935 -3.9471474 -3.9735866 -3.9857967 -3.9849482 -4.000689 -4.03163 -4.0576921 -4.0697603][-4.1838875 -4.1263943 -4.0578423 -3.9895494 -3.9222994 -3.8607783 -3.8260517 -3.8411448 -3.8769014 -3.8968952 -3.9070809 -3.938242 -3.9834616 -4.01977 -4.0394535][-4.1684742 -4.0995708 -4.0190148 -3.9374065 -3.8533397 -3.7701061 -3.7133577 -3.7321396 -3.7943859 -3.8304935 -3.8639107 -3.9192472 -3.976464 -4.0165896 -4.0314889][-4.1583681 -4.0773849 -3.985749 -3.8989158 -3.8124204 -3.7275047 -3.6751077 -3.7170377 -3.8108613 -3.8677566 -3.9119084 -3.9671044 -4.0189877 -4.05399 -4.060894][-4.1501231 -4.0705171 -3.9864178 -3.9161685 -3.8585963 -3.8070703 -3.7823408 -3.8345919 -3.9189193 -3.9646835 -3.9986048 -4.0398269 -4.0880823 -4.1212831 -4.1293163][-4.1592293 -4.0975776 -4.0390515 -3.9926951 -3.9616859 -3.9365449 -3.9242678 -3.9608121 -4.0143962 -4.0400586 -4.0619535 -4.0997405 -4.1496763 -4.1891546 -4.2091594][-4.188601 -4.1507878 -4.1163292 -4.0875492 -4.0686359 -4.0527096 -4.0424485 -4.0629044 -4.0940676 -4.1077862 -4.1201653 -4.1548738 -4.2028627 -4.2410235 -4.2639341][-4.2293806 -4.209259 -4.1912742 -4.1716242 -4.1566257 -4.149281 -4.1467466 -4.1617017 -4.1872892 -4.202035 -4.2132063 -4.2384162 -4.2703967 -4.29059 -4.30258][-4.2604389 -4.2458892 -4.2345986 -4.2241631 -4.2184877 -4.2218156 -4.2290893 -4.2471085 -4.2686028 -4.2787433 -4.283514 -4.2962089 -4.3091092 -4.3149176 -4.3202014][-4.2773118 -4.2646322 -4.2588577 -4.2570252 -4.2577558 -4.2654424 -4.2790008 -4.2971311 -4.3125315 -4.316556 -4.31526 -4.3177528 -4.3205404 -4.3242941 -4.3296328][-4.2968059 -4.2842808 -4.2781134 -4.27611 -4.2780628 -4.2842846 -4.29518 -4.3094316 -4.3219023 -4.325655 -4.32515 -4.3248162 -4.3259463 -4.3296247 -4.3351469]]...]
INFO - root - 2017-12-06 10:23:57.846913: step 13110, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 81h:45m:07s remains)
INFO - root - 2017-12-06 10:24:06.978920: step 13120, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.921 sec/batch; 81h:44m:45s remains)
INFO - root - 2017-12-06 10:24:16.115976: step 13130, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 80h:28m:48s remains)
INFO - root - 2017-12-06 10:24:25.352748: step 13140, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.933 sec/batch; 82h:46m:51s remains)
INFO - root - 2017-12-06 10:24:34.153347: step 13150, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.887 sec/batch; 78h:41m:54s remains)
INFO - root - 2017-12-06 10:24:43.254689: step 13160, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 78h:21m:44s remains)
INFO - root - 2017-12-06 10:24:52.373133: step 13170, loss = 2.10, batch loss = 2.04 (9.7 examples/sec; 0.824 sec/batch; 73h:03m:56s remains)
INFO - root - 2017-12-06 10:25:01.578428: step 13180, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 79h:13m:51s remains)
INFO - root - 2017-12-06 10:25:10.597331: step 13190, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.884 sec/batch; 78h:22m:43s remains)
INFO - root - 2017-12-06 10:25:19.975706: step 13200, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.941 sec/batch; 83h:26m:54s remains)
2017-12-06 10:25:20.661472: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.28874 -4.2822943 -4.2753119 -4.269784 -4.2665849 -4.270803 -4.2758994 -4.2812228 -4.2788987 -4.267725 -4.2627735 -4.2607703 -4.2642708 -4.2671213 -4.2715425][-4.2756176 -4.2651424 -4.2526588 -4.2439971 -4.2431254 -4.2519197 -4.2569175 -4.2584453 -4.25293 -4.2450938 -4.2462521 -4.2451019 -4.2450213 -4.2456889 -4.2513528][-4.254252 -4.2379537 -4.2197447 -4.2082882 -4.2098408 -4.2201366 -4.2265143 -4.2250743 -4.2136903 -4.2144723 -4.2268953 -4.2264886 -4.2169657 -4.2126064 -4.2212811][-4.23003 -4.2065439 -4.1836572 -4.1692019 -4.1671233 -4.1747956 -4.1873622 -4.1859269 -4.1710629 -4.1830697 -4.2098022 -4.2134237 -4.1943836 -4.1817837 -4.1891026][-4.204874 -4.1786847 -4.1542673 -4.135376 -4.1260028 -4.1296992 -4.143271 -4.1424303 -4.1286936 -4.1479907 -4.1871834 -4.1989832 -4.1788549 -4.15701 -4.1625085][-4.1931548 -4.1680412 -4.140234 -4.1105652 -4.0871873 -4.0801029 -4.0949674 -4.1090178 -4.1001077 -4.1164389 -4.158637 -4.1780763 -4.1534009 -4.1273303 -4.1425719][-4.1927795 -4.1704488 -4.1397166 -4.1009912 -4.0681462 -4.0470839 -4.0600233 -4.08674 -4.0834708 -4.092381 -4.1282392 -4.1462932 -4.1104264 -4.0813217 -4.1155295][-4.189971 -4.1737843 -4.1517673 -4.117053 -4.0835142 -4.0550203 -4.0549622 -4.081985 -4.0886679 -4.0887675 -4.1059518 -4.1072803 -4.0552835 -4.0247884 -4.080411][-4.186306 -4.17881 -4.1697888 -4.1447887 -4.1248031 -4.1010485 -4.0888886 -4.1086192 -4.1195974 -4.1085396 -4.1049848 -4.0924997 -4.0293226 -4.0033193 -4.0769081][-4.1856213 -4.1838813 -4.1895003 -4.1785326 -4.1723428 -4.1522031 -4.130651 -4.1351948 -4.1448874 -4.1322513 -4.1216307 -4.1084461 -4.056993 -4.0450244 -4.1104069][-4.1892624 -4.1886997 -4.1958103 -4.1922517 -4.1987262 -4.1805716 -4.153266 -4.1477671 -4.1525049 -4.1505294 -4.1438479 -4.1350327 -4.0982132 -4.0912285 -4.1393557][-4.1988959 -4.1986041 -4.2011032 -4.1940432 -4.2029243 -4.1867523 -4.155736 -4.1449208 -4.1525488 -4.1638684 -4.1645226 -4.1598177 -4.1329689 -4.1277409 -4.1592827][-4.2145252 -4.217432 -4.2153053 -4.2029805 -4.2071815 -4.1898708 -4.1641216 -4.1552148 -4.1643968 -4.180521 -4.18807 -4.1869726 -4.1690645 -4.1625795 -4.1746445][-4.2237253 -4.2309737 -4.2327456 -4.2243767 -4.2234917 -4.2053056 -4.187078 -4.1874514 -4.2005267 -4.214602 -4.2259989 -4.2272162 -4.2135682 -4.2026067 -4.2016134][-4.2361636 -4.2463322 -4.253418 -4.2523961 -4.2519269 -4.237062 -4.2252827 -4.2339272 -4.2479477 -4.2577434 -4.2670255 -4.2681684 -4.2572155 -4.2456765 -4.2402372]]...]
INFO - root - 2017-12-06 10:25:29.728377: step 13210, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.835 sec/batch; 74h:01m:36s remains)
INFO - root - 2017-12-06 10:25:38.676844: step 13220, loss = 2.09, batch loss = 2.04 (11.9 examples/sec; 0.670 sec/batch; 59h:24m:52s remains)
INFO - root - 2017-12-06 10:25:47.816511: step 13230, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.863 sec/batch; 76h:33m:57s remains)
INFO - root - 2017-12-06 10:25:56.937644: step 13240, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 79h:07m:37s remains)
INFO - root - 2017-12-06 10:26:06.098365: step 13250, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.901 sec/batch; 79h:54m:48s remains)
INFO - root - 2017-12-06 10:26:15.355684: step 13260, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.872 sec/batch; 77h:20m:39s remains)
INFO - root - 2017-12-06 10:26:24.622015: step 13270, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.971 sec/batch; 86h:07m:44s remains)
INFO - root - 2017-12-06 10:26:33.687427: step 13280, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.951 sec/batch; 84h:17m:59s remains)
INFO - root - 2017-12-06 10:26:42.899453: step 13290, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 81h:54m:42s remains)
INFO - root - 2017-12-06 10:26:51.944456: step 13300, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 82h:21m:45s remains)
2017-12-06 10:26:52.683121: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1979418 -4.1956949 -4.2017827 -4.20408 -4.1930346 -4.1747251 -4.1426039 -4.0815167 -4.0052967 -3.956979 -3.9607861 -4.0036359 -4.0507569 -4.0744925 -4.0573382][-4.2339435 -4.2343373 -4.2384372 -4.2323828 -4.2137876 -4.1902714 -4.1620741 -4.1150556 -4.053688 -4.0166674 -4.0221138 -4.0514464 -4.0821905 -4.0943356 -4.0737176][-4.244761 -4.2452011 -4.2455006 -4.2351213 -4.2144413 -4.191381 -4.1725769 -4.1456532 -4.1045694 -4.0765724 -4.0724068 -4.0756526 -4.0876927 -4.0943661 -4.0827184][-4.2384129 -4.2410297 -4.2393856 -4.2243466 -4.1960821 -4.1706662 -4.1579022 -4.1423726 -4.111505 -4.0935445 -4.0872059 -4.0743437 -4.0755963 -4.084249 -4.0831513][-4.2276559 -4.2320867 -4.2317944 -4.2130594 -4.1759386 -4.1453667 -4.1280918 -4.1111164 -4.0831242 -4.071877 -4.0687246 -4.0576954 -4.0655189 -4.0825653 -4.089395][-4.2134075 -4.2189283 -4.2199059 -4.1977286 -4.1486664 -4.1108918 -4.0909281 -4.0673714 -4.039381 -4.0269752 -4.0210352 -4.0271883 -4.0546818 -4.0840979 -4.0969415][-4.1879454 -4.1883097 -4.1873946 -4.1657882 -4.112781 -4.0781994 -4.0617766 -4.038507 -4.0133328 -3.9975309 -3.9832182 -4.0048447 -4.0455928 -4.0827045 -4.1044221][-4.1591744 -4.1553788 -4.1484671 -4.1207304 -4.0666494 -4.0394387 -4.03165 -4.02194 -4.0040283 -3.9861042 -3.9722397 -3.9978766 -4.0355396 -4.0665121 -4.0887289][-4.1350145 -4.1256723 -4.1171412 -4.0883088 -4.0436754 -4.0289917 -4.0306926 -4.0295367 -4.0201921 -4.0079565 -4.0002542 -4.0137844 -4.0290976 -4.0450292 -4.0638313][-4.1363478 -4.1286063 -4.1216369 -4.0907965 -4.0523047 -4.0483847 -4.06029 -4.0676832 -4.0690675 -4.066864 -4.0650182 -4.059411 -4.0453525 -4.0400934 -4.0486956][-4.158411 -4.1489491 -4.1375751 -4.1003284 -4.0665846 -4.071701 -4.0873518 -4.0950947 -4.0999837 -4.1076651 -4.1142378 -4.099411 -4.0693731 -4.0463858 -4.0434246][-4.1660223 -4.1536841 -4.1391549 -4.1042414 -4.0768013 -4.0801978 -4.0871267 -4.0850558 -4.0936303 -4.1162481 -4.1305447 -4.11648 -4.0854268 -4.0591969 -4.0496941][-4.1856852 -4.1742053 -4.1530457 -4.1175642 -4.0853577 -4.0723076 -4.0663114 -4.0607305 -4.0802326 -4.1164756 -4.1365967 -4.1249156 -4.10031 -4.0825267 -4.0729938][-4.2105012 -4.1951008 -4.1697745 -4.1352382 -4.0987682 -4.0689383 -4.0513849 -4.0466933 -4.0728621 -4.1157885 -4.1370354 -4.1284285 -4.1125445 -4.1015968 -4.0926642][-4.2044935 -4.1874628 -4.1668406 -4.1421423 -4.1108642 -4.0782371 -4.0550776 -4.0506725 -4.0754147 -4.1133862 -4.1317005 -4.1304555 -4.1225367 -4.1148119 -4.1099281]]...]
INFO - root - 2017-12-06 10:27:01.899042: step 13310, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 78h:48m:12s remains)
INFO - root - 2017-12-06 10:27:11.064601: step 13320, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 78h:19m:41s remains)
INFO - root - 2017-12-06 10:27:20.242131: step 13330, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 78h:13m:52s remains)
INFO - root - 2017-12-06 10:27:29.325545: step 13340, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 86h:25m:42s remains)
INFO - root - 2017-12-06 10:27:38.533007: step 13350, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 85h:23m:41s remains)
INFO - root - 2017-12-06 10:27:47.829001: step 13360, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.946 sec/batch; 83h:53m:22s remains)
INFO - root - 2017-12-06 10:27:56.892273: step 13370, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 80h:13m:45s remains)
INFO - root - 2017-12-06 10:28:06.224378: step 13380, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 80h:43m:28s remains)
INFO - root - 2017-12-06 10:28:15.389843: step 13390, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.891 sec/batch; 78h:56m:11s remains)
INFO - root - 2017-12-06 10:28:24.596840: step 13400, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.917 sec/batch; 81h:16m:02s remains)
2017-12-06 10:28:25.267156: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1126008 -4.11144 -4.122056 -4.1319489 -4.1411538 -4.1478 -4.1511326 -4.1441941 -4.1395092 -4.1570997 -4.1920176 -4.2287455 -4.260057 -4.2804227 -4.2794271][-4.1286283 -4.1294451 -4.1462784 -4.1664267 -4.1810212 -4.1849952 -4.1843972 -4.1804652 -4.1838145 -4.202826 -4.2276826 -4.2509809 -4.2684808 -4.2760272 -4.2616358][-4.1532173 -4.1571774 -4.1800556 -4.2041507 -4.2130036 -4.2059283 -4.1969228 -4.18916 -4.1874442 -4.1980224 -4.2112017 -4.226769 -4.2407036 -4.2446856 -4.2263727][-4.1832004 -4.1879382 -4.2130046 -4.2295485 -4.2252531 -4.2055774 -4.1850348 -4.1667147 -4.153779 -4.1550779 -4.15952 -4.172771 -4.1918273 -4.2020335 -4.188889][-4.1982384 -4.2047553 -4.2275195 -4.2352352 -4.2208834 -4.1918564 -4.161025 -4.1350422 -4.1166668 -4.110002 -4.1066976 -4.120213 -4.1468759 -4.1643715 -4.1579289][-4.1894608 -4.1981835 -4.2168694 -4.2187872 -4.1989245 -4.16327 -4.1285486 -4.1044917 -4.08499 -4.0712652 -4.0688763 -4.0924544 -4.1302094 -4.152421 -4.1509113][-4.1628013 -4.1716118 -4.187161 -4.1835933 -4.161994 -4.130877 -4.1075249 -4.0915723 -4.0677671 -4.0489521 -4.0548124 -4.0899792 -4.1337266 -4.1605144 -4.1662822][-4.1381636 -4.1421447 -4.1505389 -4.1441393 -4.1317477 -4.1163163 -4.1090674 -4.0954642 -4.0682826 -4.0504332 -4.065104 -4.1052127 -4.147305 -4.176218 -4.1871605][-4.1148043 -4.1072454 -4.1122718 -4.1150093 -4.1209655 -4.1248384 -4.12589 -4.1108155 -4.0835476 -4.0744905 -4.0983434 -4.1388245 -4.1751943 -4.19995 -4.2125726][-4.1043139 -4.0864811 -4.0872078 -4.1008835 -4.1258969 -4.1412849 -4.1435957 -4.1297131 -4.1117115 -4.1175809 -4.1504769 -4.1911926 -4.2178044 -4.2320094 -4.2415681][-4.1094108 -4.0870352 -4.089561 -4.113163 -4.1483555 -4.1681581 -4.1719427 -4.16424 -4.1611195 -4.1793966 -4.2125058 -4.2448087 -4.2598267 -4.2648067 -4.2704339][-4.1199512 -4.1041722 -4.1183586 -4.1532583 -4.1909156 -4.2115316 -4.218914 -4.2194638 -4.2243834 -4.2420092 -4.2652426 -4.283287 -4.2890959 -4.288372 -4.288928][-4.1494493 -4.1478252 -4.1739531 -4.2124696 -4.2445917 -4.2609763 -4.266932 -4.26998 -4.2734418 -4.2806854 -4.2896333 -4.296607 -4.297225 -4.2936134 -4.291564][-4.2031431 -4.211771 -4.2384729 -4.267488 -4.2864752 -4.2929258 -4.2941542 -4.2948408 -4.2949734 -4.2958908 -4.2980819 -4.2999897 -4.2985663 -4.2944713 -4.2918434][-4.2546091 -4.2645378 -4.2822757 -4.2978859 -4.3056707 -4.3052382 -4.301188 -4.2980695 -4.2967219 -4.2971787 -4.2985935 -4.2993402 -4.2976375 -4.293159 -4.2889137]]...]
INFO - root - 2017-12-06 10:28:34.496719: step 13410, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.909 sec/batch; 80h:35m:19s remains)
INFO - root - 2017-12-06 10:28:43.729593: step 13420, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 86h:55m:50s remains)
INFO - root - 2017-12-06 10:28:52.840994: step 13430, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 81h:05m:10s remains)
INFO - root - 2017-12-06 10:29:01.836375: step 13440, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 79h:49m:57s remains)
INFO - root - 2017-12-06 10:29:10.932674: step 13450, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 80h:38m:02s remains)
INFO - root - 2017-12-06 10:29:20.244365: step 13460, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.927 sec/batch; 82h:08m:04s remains)
INFO - root - 2017-12-06 10:29:29.479500: step 13470, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 82h:56m:33s remains)
INFO - root - 2017-12-06 10:29:38.519611: step 13480, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 83h:02m:26s remains)
INFO - root - 2017-12-06 10:29:47.712173: step 13490, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 80h:44m:00s remains)
INFO - root - 2017-12-06 10:29:56.790369: step 13500, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.912 sec/batch; 80h:48m:22s remains)
2017-12-06 10:29:57.524387: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2984467 -4.2817082 -4.2680774 -4.2606859 -4.2610855 -4.2662544 -4.2622824 -4.2462912 -4.2372632 -4.2491584 -4.2674756 -4.2905111 -4.3040447 -4.3048315 -4.3018594][-4.2945108 -4.2827587 -4.2739758 -4.2692146 -4.2664561 -4.2694058 -4.2621984 -4.242137 -4.2348957 -4.2520876 -4.2687912 -4.285676 -4.2926817 -4.2857671 -4.2761016][-4.2770605 -4.2783551 -4.2819476 -4.2799578 -4.2728667 -4.2693696 -4.2544184 -4.2275476 -4.2206054 -4.2435846 -4.2621155 -4.2716508 -4.2687616 -4.2537007 -4.2410793][-4.2592306 -4.2743659 -4.2882471 -4.2857928 -4.2732763 -4.2585673 -4.2309871 -4.1927538 -4.1815529 -4.2116733 -4.2396264 -4.2504563 -4.2444963 -4.2280593 -4.2146583][-4.2447343 -4.2649803 -4.2804184 -4.2715173 -4.2495632 -4.2228217 -4.1830091 -4.1341572 -4.1196294 -4.1548495 -4.1939278 -4.2169061 -4.2222905 -4.214519 -4.2017722][-4.2302318 -4.2508993 -4.2646627 -4.2486725 -4.2121625 -4.1720581 -4.1219893 -4.0677962 -4.0495162 -4.0804729 -4.1259246 -4.1710873 -4.2021751 -4.2170138 -4.2142067][-4.2200847 -4.2421489 -4.255343 -4.22974 -4.1733942 -4.1144581 -4.0534229 -3.9901557 -3.9634628 -3.9865663 -4.0403094 -4.1134315 -4.1746049 -4.2163382 -4.2281795][-4.2358551 -4.2553959 -4.259016 -4.2170691 -4.1399765 -4.0599384 -3.9829361 -3.9125469 -3.880101 -3.8999493 -3.9662819 -4.0647683 -4.1504908 -4.2127194 -4.2393112][-4.2646194 -4.2798553 -4.2674241 -4.2076445 -4.1141548 -4.0210934 -3.9392209 -3.8782206 -3.8566766 -3.8830371 -3.9577537 -4.0629277 -4.1559668 -4.2242646 -4.2576466][-4.2908916 -4.3041754 -4.28057 -4.209115 -4.110002 -4.0146255 -3.9401093 -3.8989289 -3.9016519 -3.9441373 -4.0198073 -4.1110725 -4.1907105 -4.2478433 -4.2710986][-4.3071022 -4.3141165 -4.2858105 -4.2183061 -4.133275 -4.0574121 -4.0034466 -3.9818738 -4.0008922 -4.0497184 -4.1134534 -4.1771069 -4.2265806 -4.255868 -4.2570858][-4.3059835 -4.3036766 -4.2744417 -4.219275 -4.1607909 -4.11641 -4.0929146 -4.0888433 -4.1105423 -4.152638 -4.1994753 -4.2365818 -4.2533674 -4.2460217 -4.2182784][-4.2794561 -4.2742057 -4.2494783 -4.2097178 -4.1772118 -4.16187 -4.1612663 -4.1709151 -4.1937914 -4.226819 -4.2566872 -4.2738895 -4.2653761 -4.2253933 -4.1671324][-4.2343497 -4.2347536 -4.2154856 -4.1841936 -4.1681576 -4.1699963 -4.1809683 -4.2018762 -4.2323709 -4.2646112 -4.2855492 -4.288496 -4.258996 -4.1943851 -4.1123347][-4.1902428 -4.1954131 -4.1770549 -4.14566 -4.1311312 -4.1386285 -4.1580319 -4.190115 -4.2322106 -4.2700043 -4.2880378 -4.2814164 -4.2351685 -4.154357 -4.0621405]]...]
INFO - root - 2017-12-06 10:30:06.588356: step 13510, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 79h:06m:48s remains)
INFO - root - 2017-12-06 10:30:15.717378: step 13520, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.888 sec/batch; 78h:40m:46s remains)
INFO - root - 2017-12-06 10:30:24.983455: step 13530, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 81h:12m:02s remains)
INFO - root - 2017-12-06 10:30:34.269381: step 13540, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 85h:32m:55s remains)
INFO - root - 2017-12-06 10:30:43.276835: step 13550, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.947 sec/batch; 83h:54m:31s remains)
INFO - root - 2017-12-06 10:30:52.410220: step 13560, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.967 sec/batch; 85h:39m:26s remains)
INFO - root - 2017-12-06 10:31:01.543711: step 13570, loss = 2.03, batch loss = 1.98 (9.1 examples/sec; 0.883 sec/batch; 78h:15m:53s remains)
INFO - root - 2017-12-06 10:31:10.588419: step 13580, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.962 sec/batch; 85h:15m:21s remains)
INFO - root - 2017-12-06 10:31:19.761906: step 13590, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 77h:44m:24s remains)
INFO - root - 2017-12-06 10:31:28.916048: step 13600, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 82h:45m:41s remains)
2017-12-06 10:31:29.576869: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2433457 -4.235148 -4.2161884 -4.1850462 -4.1527724 -4.1186819 -4.1158051 -4.1050973 -4.0503297 -4.0251093 -4.0486469 -4.083571 -4.1145473 -4.1305 -4.1253991][-4.2452278 -4.2367344 -4.2108092 -4.1721864 -4.1371417 -4.1040478 -4.1003489 -4.0986319 -4.0606036 -4.0429621 -4.0557661 -4.0710869 -4.0873 -4.100781 -4.1028891][-4.2489548 -4.2385092 -4.2067227 -4.1597486 -4.1170526 -4.0794387 -4.0673251 -4.073174 -4.0640082 -4.0658889 -4.0723844 -4.0621071 -4.0469675 -4.0513721 -4.0612221][-4.2442265 -4.2332296 -4.198864 -4.1429758 -4.0869765 -4.0422854 -4.0250716 -4.0451937 -4.0697017 -4.0918322 -4.0962887 -4.0648818 -4.0230131 -4.0216136 -4.0389156][-4.2343931 -4.2235112 -4.1864815 -4.1257343 -4.0607166 -4.003664 -3.9811635 -4.0202837 -4.0728602 -4.1082683 -4.1143079 -4.07286 -4.0180163 -4.0171337 -4.037981][-4.2339883 -4.2223711 -4.1827946 -4.1184545 -4.0446849 -3.9706087 -3.9396992 -3.9970465 -4.0717216 -4.1170983 -4.1255765 -4.0851784 -4.0274625 -4.0207567 -4.0439892][-4.2293754 -4.216444 -4.1767988 -4.107378 -4.0152597 -3.9098852 -3.8607738 -3.9448667 -4.0532641 -4.1179814 -4.1362743 -4.1078572 -4.0564461 -4.0457463 -4.0704055][-4.2212105 -4.2088151 -4.1680231 -4.0882297 -3.97364 -3.8243728 -3.7429628 -3.8641427 -4.0189633 -4.106884 -4.1394382 -4.1321979 -4.0989552 -4.093792 -4.1101923][-4.2164493 -4.2021365 -4.158493 -4.0769515 -3.9574506 -3.7986479 -3.7084534 -3.8404882 -4.0015316 -4.0945516 -4.1349463 -4.150393 -4.1426253 -4.1405916 -4.1425705][-4.2074342 -4.186655 -4.1425443 -4.0719976 -3.9808738 -3.8743989 -3.829052 -3.9158921 -4.0222163 -4.0919724 -4.1332989 -4.1673961 -4.1789365 -4.1800246 -4.168076][-4.192533 -4.1636233 -4.1245584 -4.0740995 -4.0198517 -3.9705732 -3.958966 -4.0027709 -4.0554409 -4.1001263 -4.137929 -4.1782069 -4.1997151 -4.2016287 -4.1792989][-4.18223 -4.1510205 -4.1193752 -4.0844612 -4.0568161 -4.0381627 -4.0374656 -4.0584788 -4.0836153 -4.1155391 -4.1467695 -4.1817555 -4.2038684 -4.2036324 -4.1743922][-4.1714473 -4.1456447 -4.1198111 -4.0962439 -4.08481 -4.0827718 -4.0873632 -4.0920486 -4.1041784 -4.1304936 -4.1535182 -4.1723166 -4.1897736 -4.1881843 -4.15348][-4.1487818 -4.133399 -4.1160355 -4.1024475 -4.10073 -4.1060495 -4.1143484 -4.1161389 -4.1234665 -4.1478319 -4.1592493 -4.1608443 -4.1681089 -4.1670804 -4.1308613][-4.1277332 -4.1213374 -4.1105504 -4.1013846 -4.1047492 -4.1110034 -4.1214938 -4.1249495 -4.1366482 -4.158699 -4.1639194 -4.1551714 -4.1543474 -4.153904 -4.1212692]]...]
INFO - root - 2017-12-06 10:31:38.619768: step 13610, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.925 sec/batch; 81h:54m:39s remains)
INFO - root - 2017-12-06 10:31:47.814404: step 13620, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 80h:53m:50s remains)
INFO - root - 2017-12-06 10:31:56.943727: step 13630, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 84h:43m:36s remains)
INFO - root - 2017-12-06 10:32:06.097371: step 13640, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 83h:07m:38s remains)
INFO - root - 2017-12-06 10:32:15.271585: step 13650, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 85h:03m:36s remains)
INFO - root - 2017-12-06 10:32:24.544747: step 13660, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 81h:15m:43s remains)
INFO - root - 2017-12-06 10:32:33.695417: step 13670, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 77h:53m:11s remains)
INFO - root - 2017-12-06 10:32:42.710385: step 13680, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.928 sec/batch; 82h:11m:52s remains)
INFO - root - 2017-12-06 10:32:51.968155: step 13690, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.991 sec/batch; 87h:47m:51s remains)
INFO - root - 2017-12-06 10:33:01.217199: step 13700, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.913 sec/batch; 80h:48m:31s remains)
2017-12-06 10:33:01.896841: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1407981 -4.1471329 -4.1680942 -4.1898074 -4.2025537 -4.208745 -4.2137241 -4.2145591 -4.207201 -4.2079549 -4.21768 -4.2203732 -4.2135768 -4.2014809 -4.2006588][-4.1470976 -4.1560297 -4.182024 -4.2045236 -4.2119036 -4.212409 -4.2110887 -4.2067103 -4.1923075 -4.1811161 -4.1807556 -4.1854692 -4.1891084 -4.18637 -4.1907415][-4.1586304 -4.17576 -4.2006121 -4.22073 -4.2234859 -4.2173619 -4.2085638 -4.1989479 -4.1831846 -4.1650805 -4.1504259 -4.1502819 -4.1568294 -4.1608987 -4.172585][-4.16881 -4.1929388 -4.2145681 -4.2282553 -4.2269111 -4.214921 -4.1968985 -4.1814094 -4.1681228 -4.1504908 -4.13139 -4.1275368 -4.1331325 -4.145092 -4.1631141][-4.174963 -4.1966925 -4.2129478 -4.2212982 -4.2163038 -4.1940503 -4.1647639 -4.144465 -4.1390185 -4.1255431 -4.1081152 -4.1077495 -4.1174884 -4.1369781 -4.1586018][-4.1842442 -4.201138 -4.2128911 -4.2128615 -4.1966295 -4.1538157 -4.1025195 -4.080461 -4.0925226 -4.0930934 -4.0888624 -4.0985413 -4.1124954 -4.1368175 -4.1618376][-4.1901288 -4.2012749 -4.2078438 -4.197454 -4.1586204 -4.0802178 -3.9920387 -3.9757438 -4.02851 -4.0658979 -4.0838208 -4.0997682 -4.109561 -4.1280165 -4.1485553][-4.1811829 -4.190608 -4.1965275 -4.17732 -4.1198974 -4.013658 -3.8962622 -3.8928566 -3.9891298 -4.0608006 -4.0948062 -4.1061192 -4.1036983 -4.1113029 -4.1264014][-4.1785564 -4.1849842 -4.186595 -4.1622877 -4.1102338 -4.028616 -3.9488993 -3.9600925 -4.0484033 -4.1117549 -4.1383066 -4.1348696 -4.1189842 -4.1183677 -4.1264586][-4.1890907 -4.1925735 -4.1946726 -4.1746907 -4.1412706 -4.1002231 -4.0650606 -4.0802336 -4.1347027 -4.1755543 -4.1887555 -4.1758676 -4.15584 -4.1483841 -4.1535611][-4.2018728 -4.2087383 -4.215322 -4.2053242 -4.1856785 -4.1646342 -4.1486979 -4.1584053 -4.1856675 -4.2068567 -4.2133222 -4.1992755 -4.1760573 -4.1662211 -4.1723838][-4.2163858 -4.2211766 -4.2246923 -4.2190728 -4.2058067 -4.1913013 -4.18097 -4.1838279 -4.1955376 -4.2065449 -4.2134495 -4.2035022 -4.1802988 -4.1688318 -4.1707206][-4.2279325 -4.2224321 -4.2142797 -4.2074118 -4.193542 -4.1788712 -4.1718216 -4.1708713 -4.1786537 -4.1912689 -4.2090631 -4.2084465 -4.1911983 -4.1812525 -4.1785088][-4.2283883 -4.212152 -4.1944766 -4.1830397 -4.1687236 -4.1585703 -4.1564503 -4.1563687 -4.1695418 -4.1896496 -4.21699 -4.2272611 -4.2209244 -4.2153134 -4.208806][-4.2162786 -4.196002 -4.1780949 -4.1713181 -4.16729 -4.1656442 -4.1657658 -4.1672096 -4.1820168 -4.2020855 -4.2280259 -4.2432232 -4.2469654 -4.2462034 -4.2396588]]...]
INFO - root - 2017-12-06 10:33:11.131828: step 13710, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 79h:01m:17s remains)
