INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "249"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-deformonlyinstance
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
111111111 Tensor("siamese_fc/conv5/concat:0", shape=(8, 6, 6, 256), dtype=float32) Tensor("siamese_fc/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/db1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 6, 6), dtype=float32)
111111111 Tensor("siamese_fc_1/conv5/concat:0", shape=(8, 20, 20, 256), dtype=float32) Tensor("siamese_fc_1/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/db1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 20, 20), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-13 03:26:18.007675: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 03:26:18.007714: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 03:26:18.007720: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 03:26:18.007724: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 03:26:18.007728: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 03:26:18.342626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-13 03:26:18.342662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-13 03:26:18.342668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-13 03:26:18.342675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-13 03:26:21.265482: step 0, loss = 0.27, batch loss = 0.19 (3.6 examples/sec; 2.220 sec/batch; 205h:03m:00s remains)
2017-12-13 03:26:21.670792: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4592133 -4.7217779 -5.21734 -5.8356385 -6.2756548 -6.4515486 -6.4426203 -6.3961983 -6.4668388 -6.6908007 -6.9615903 -7.05068 -6.7592077 -6.0727215 -5.2293696][-4.8277755 -5.2425179 -5.96533 -6.7339315 -7.0837283 -7.031642 -6.84556 -6.8065281 -7.01497 -7.4128113 -7.8559456 -8.068964 -7.8077164 -7.0014238 -5.913013][-5.150774 -5.6813107 -6.5414515 -7.2596788 -7.2480249 -6.7272644 -6.1880293 -6.0379763 -6.3045492 -6.9008627 -7.694541 -8.2946186 -8.3526516 -7.66076 -6.4513855][-5.280777 -5.8497672 -6.7370119 -7.2672672 -6.7653961 -5.6805964 -4.67152 -4.2189975 -4.370379 -5.149776 -6.4531412 -7.67892 -8.303319 -7.9526443 -6.7780409][-5.0137219 -5.5798593 -6.4815874 -6.8420119 -5.9095087 -4.3069396 -2.7084055 -1.6760218 -1.5356164 -2.5223279 -4.4757872 -6.4460239 -7.7612371 -7.8994708 -6.9021683][-4.3487377 -4.9278951 -5.8760104 -6.1102066 -4.8396974 -2.7520084 -0.41238737 1.4666505 2.0179529 0.71797323 -2.0138164 -4.7820969 -6.7863154 -7.450253 -6.7429428][-3.5496335 -4.1382527 -5.0896893 -5.2068386 -3.7540684 -1.3802161 1.537219 4.12599 4.9343395 3.2284265 -0.17550659 -3.5181456 -5.9162779 -6.8966961 -6.4301367][-2.9529893 -3.5479336 -4.4782825 -4.6081414 -3.3079832 -1.0962539 1.8234138 4.5133667 5.2181749 3.2387533 -0.36363029 -3.7159157 -5.9328957 -6.7723684 -6.2947936][-2.8370848 -3.3815637 -4.2728195 -4.57648 -3.7049692 -1.9805756 0.4616909 2.7267108 3.1747117 1.3266611 -1.893868 -4.7230234 -6.3448253 -6.7960076 -6.2256279][-3.1923933 -3.5971045 -4.3970919 -4.9231133 -4.5254803 -3.3198943 -1.4582286 0.25437832 0.54118776 -0.90624022 -3.4661686 -5.5936046 -6.5948277 -6.6846604 -6.0620475][-3.8678739 -4.1219811 -4.7914567 -5.4464951 -5.3666763 -4.504283 -3.1320591 -1.9322767 -1.7692616 -2.8048463 -4.6914759 -6.2084556 -6.7470078 -6.5666957 -5.8972092][-4.4446616 -4.6140285 -5.1772656 -5.8557291 -5.8649569 -5.12992 -4.0539613 -3.212517 -3.1439757 -3.8635793 -5.2203574 -6.3478255 -6.6896725 -6.4324994 -5.780458][-4.5698471 -4.7494822 -5.3039751 -5.9767337 -5.9281917 -5.1469984 -4.1464419 -3.4426146 -3.3839846 -3.8312292 -4.8260427 -5.832067 -6.2794261 -6.1763163 -5.6379104][-4.3633666 -4.6010828 -5.2246656 -5.9005094 -5.7379694 -4.8170385 -3.7392809 -3.0264823 -2.9440081 -3.2205179 -4.06063 -5.1328278 -5.8130407 -5.9329243 -5.5075407][-3.9170585 -4.1440587 -4.8262978 -5.5229278 -5.2669644 -4.1579676 -2.9503343 -2.2744222 -2.3206966 -2.6542125 -3.5717254 -4.8117962 -5.6885366 -5.9273148 -5.5087056]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-deformonlyinstance/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-deformonlyinstance/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-13 03:26:24.165442: step 10, loss = 0.95, batch loss = 0.87 (36.9 examples/sec; 0.217 sec/batch; 20h:01m:06s remains)
INFO - root - 2017-12-13 03:26:26.350110: step 20, loss = 0.36, batch loss = 0.27 (34.1 examples/sec; 0.235 sec/batch; 21h:40m:29s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-deformonlyinstance
INFO - root - 2017-12-13 03:26:28.530156: step 30, loss = 0.50, batch loss = 0.42 (35.9 examples/sec; 0.223 sec/batch; 20h:35m:25s remains)
INFO - root - 2017-12-13 03:26:30.709041: step 40, loss = 0.57, batch loss = 0.48 (36.9 examples/sec; 0.217 sec/batch; 20h:02m:20s remains)
INFO - root - 2017-12-13 03:26:32.877650: step 50, loss = 0.42, batch loss = 0.34 (36.7 examples/sec; 0.218 sec/batch; 20h:06m:52s remains)
INFO - root - 2017-12-13 03:26:35.067978: step 60, loss = 1.05, batch loss = 0.96 (37.2 examples/sec; 0.215 sec/batch; 19h:52m:26s remains)
INFO - root - 2017-12-13 03:26:37.272294: step 70, loss = 0.51, batch loss = 0.43 (36.8 examples/sec; 0.218 sec/batch; 20h:05m:06s remains)
INFO - root - 2017-12-13 03:26:39.442229: step 80, loss = 0.43, batch loss = 0.35 (36.8 examples/sec; 0.217 sec/batch; 20h:04m:04s remains)
INFO - root - 2017-12-13 03:26:41.645835: step 90, loss = 0.58, batch loss = 0.50 (35.2 examples/sec; 0.227 sec/batch; 20h:58m:49s remains)
INFO - root - 2017-12-13 03:26:43.828400: step 100, loss = 0.51, batch loss = 0.42 (36.3 examples/sec; 0.221 sec/batch; 20h:21m:40s remains)
2017-12-13 03:26:44.118916: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.37117362 0.12342644 0.80382013 1.4218621 1.6393175 1.3304038 0.66380739 -0.094037056 -0.70427155 -1.0228152 -0.97113752 -0.54021597 0.23224163 1.141324 1.8018656][-0.18213463 0.447289 1.2595577 1.9979544 2.3297076 2.1389642 1.5976663 0.95102978 0.3855052 0.014955044 -0.065355778 0.21205711 0.82615519 1.5572233 2.035646][0.06237936 0.79443264 1.688539 2.4941854 2.9107633 2.842907 2.4422545 1.9111667 1.3937049 0.98784733 0.79476118 0.91744089 1.3524165 1.9005055 2.2342672][0.21703148 0.96700287 1.8560791 2.6651282 3.1429405 3.2017527 2.9499316 2.5497718 2.1198268 1.7441812 1.5110383 1.5317173 1.8238707 2.2346539 2.487318][0.22323132 0.9090395 1.7191501 2.4775882 2.980485 3.1409478 3.0398474 2.8127985 2.54385 2.2748685 2.0696263 2.0388017 2.2320418 2.5495467 2.7556424][0.15677452 0.77616739 1.5214496 2.244143 2.7684765 3.0118237 3.0526695 3.0061817 2.8951497 2.7080221 2.5110388 2.4493413 2.6060252 2.9040308 3.0998158][0.086326122 0.68118954 1.4149508 2.1462545 2.7063599 3.0260029 3.1917877 3.2822447 3.2487326 3.0422926 2.7758389 2.6721225 2.8453608 3.1999736 3.4240093][0.010509491 0.60406971 1.3576293 2.1273241 2.7450376 3.1509008 3.4192429 3.5689578 3.5033102 3.1817784 2.7909155 2.6350336 2.8414564 3.2607412 3.5075393][-0.060907364 0.534708 1.3108945 2.1137795 2.7777495 3.2439175 3.5481067 3.6446738 3.4483075 2.9868569 2.5145402 2.3587165 2.6074595 3.0416718 3.2456632][-0.11329508 0.46653891 1.2289653 2.0143852 2.6732874 3.1436038 3.4005938 3.3525357 2.9770589 2.403244 1.9309778 1.845428 2.1499162 2.5555768 2.6634436][-0.13951874 0.39778233 1.0853071 1.7762709 2.3623953 2.7795725 2.941565 2.7402153 2.2235708 1.5994711 1.1773314 1.1754313 1.5173402 1.8644085 1.8630385][-0.18656158 0.27036619 0.81656313 1.3358698 1.7794085 2.0949712 2.1603942 1.8824096 1.349977 0.78316259 0.45054674 0.50333643 0.82545614 1.0747113 0.96699905][-0.27680826 0.0807004 0.47370672 0.81599474 1.1073637 1.3097057 1.2952037 1.0101647 0.5623374 0.13538027 -0.080730438 -3.1471252e-05 0.25938129 0.39481783 0.20778465][-0.39851236 -0.13457394 0.14212656 0.35981035 0.52929974 0.62051678 0.53304434 0.26863337 -0.052321434 -0.30435991 -0.39165092 -0.29030776 -0.10508442 -0.069041729 -0.2968092][-0.5454576 -0.38111043 -0.20980573 -0.089629173 -0.018731594 -0.016654015 -0.14020967 -0.34951782 -0.5345304 -0.63253546 -0.62382388 -0.52806854 -0.42135811 -0.44912362 -0.65657926]]...]
INFO - root - 2017-12-13 03:26:46.300586: step 110, loss = 0.63, batch loss = 0.55 (36.8 examples/sec; 0.218 sec/batch; 20h:05m:38s remains)
INFO - root - 2017-12-13 03:26:48.506758: step 120, loss = 0.69, batch loss = 0.61 (34.6 examples/sec; 0.231 sec/batch; 21h:21m:21s remains)
INFO - root - 2017-12-13 03:26:50.697169: step 130, loss = 0.46, batch loss = 0.38 (36.4 examples/sec; 0.220 sec/batch; 20h:17m:44s remains)
INFO - root - 2017-12-13 03:26:52.907373: step 140, loss = 0.40, batch loss = 0.32 (34.9 examples/sec; 0.229 sec/batch; 21h:09m:51s remains)
INFO - root - 2017-12-13 03:26:55.091473: step 150, loss = 0.77, batch loss = 0.69 (36.6 examples/sec; 0.219 sec/batch; 20h:11m:13s remains)
INFO - root - 2017-12-13 03:26:57.279048: step 160, loss = 0.52, batch loss = 0.44 (37.3 examples/sec; 0.215 sec/batch; 19h:48m:49s remains)
INFO - root - 2017-12-13 03:26:59.470850: step 170, loss = 0.45, batch loss = 0.36 (36.4 examples/sec; 0.220 sec/batch; 20h:15m:51s remains)
INFO - root - 2017-12-13 03:27:01.652156: step 180, loss = 0.37, batch loss = 0.29 (36.3 examples/sec; 0.220 sec/batch; 20h:20m:50s remains)
INFO - root - 2017-12-13 03:27:03.843965: step 190, loss = 0.53, batch loss = 0.45 (36.9 examples/sec; 0.217 sec/batch; 20h:01m:40s remains)
INFO - root - 2017-12-13 03:27:06.042536: step 200, loss = 0.38, batch loss = 0.30 (35.5 examples/sec; 0.225 sec/batch; 20h:47m:44s remains)
2017-12-13 03:27:06.316326: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.0177994 -7.11444 -7.2189064 -7.2889223 -7.1867366 -6.834342 -6.3715005 -6.0573568 -6.0170965 -6.1646767 -6.243784 -6.1809988 -6.1573744 -6.1697793 -6.1713085][-7.2052326 -7.3952293 -7.7178006 -8.0523129 -8.2638474 -8.1996555 -7.8987365 -7.5956659 -7.4889317 -7.5292177 -7.5111227 -7.4039235 -7.333065 -7.2879596 -7.2505322][-6.4132428 -6.6721544 -7.1590462 -7.6590896 -8.0477486 -8.1636276 -7.9831724 -7.69613 -7.5445156 -7.507618 -7.4518452 -7.3754182 -7.3193626 -7.2622061 -7.1985493][-5.3143244 -5.5398321 -6.0100756 -6.4563227 -6.7709646 -6.8399644 -6.6811771 -6.4668159 -6.417151 -6.4657059 -6.4860535 -6.4805059 -6.4353428 -6.334558 -6.1936493][-4.0400124 -4.2679935 -4.6972814 -4.9630833 -4.9670105 -4.71066 -4.3957887 -4.3037734 -4.5754004 -4.9497695 -5.1864476 -5.2671833 -5.220911 -5.090795 -4.9201665][-2.6987553 -2.8969464 -3.2318754 -3.1759334 -2.5713258 -1.5935407 -0.80739784 -0.80290484 -1.6230621 -2.6315444 -3.3088269 -3.5740914 -3.6048045 -3.5651217 -3.5114939][-1.3628595 -1.4543056 -1.6740696 -1.3025191 -0.017467737 1.8620534 3.2976093 3.2612672 1.8144107 0.016638756 -1.2624996 -1.8054781 -1.9428492 -2.0335441 -2.1566362][-0.38222218 -0.45989871 -0.77361131 -0.42393494 1.1379747 3.5516591 5.412488 5.3498573 3.4762206 1.0993023 -0.66669965 -1.4391418 -1.5832202 -1.6314011 -1.7040935][-0.18631029 -0.43365717 -1.0646932 -1.0778022 0.21882296 2.5019927 4.2773361 4.1675978 2.2816005 -0.14502454 -2.0031505 -2.8197465 -2.8817487 -2.7220645 -2.4806919][-0.921695 -1.2838295 -2.0579476 -2.350106 -1.5040524 0.24129963 1.6175971 1.5208764 0.0028600693 -2.0092039 -3.6167486 -4.364471 -4.41177 -4.110528 -3.5806761][-1.8941925 -2.1781292 -2.7487662 -2.9940107 -2.4439311 -1.2866552 -0.3553896 -0.34328532 -1.2550733 -2.597019 -3.8079417 -4.485908 -4.620831 -4.3183718 -3.6818874][-2.5028691 -2.6534457 -2.9127705 -2.9309063 -2.5147493 -1.8363125 -1.2941709 -1.1884987 -1.541537 -2.2502449 -3.090847 -3.7348986 -3.9845035 -3.7312806 -3.0814521][-3.1393466 -3.3232265 -3.4605188 -3.3367584 -3.0183227 -2.673769 -2.4111447 -2.2673569 -2.277616 -2.5675695 -3.1417971 -3.7308841 -4.003974 -3.7724507 -3.1603348][-3.7813706 -4.1340261 -4.3412523 -4.2895432 -4.13781 -4.0515695 -3.984674 -3.8247972 -3.6283579 -3.6234624 -3.9324174 -4.33492 -4.4693837 -4.1976123 -3.6459689][-3.6121371 -4.1211886 -4.4433713 -4.5393767 -4.5523739 -4.6335211 -4.6870823 -4.5287561 -4.2225614 -4.0209141 -4.094131 -4.2762146 -4.2651238 -4.0007439 -3.5843186]]...]
INFO - root - 2017-12-13 03:27:08.515161: step 210, loss = 0.28, batch loss = 0.20 (36.3 examples/sec; 0.220 sec/batch; 20h:21m:03s remains)
INFO - root - 2017-12-13 03:27:10.714303: step 220, loss = 0.51, batch loss = 0.43 (36.3 examples/sec; 0.220 sec/batch; 20h:20m:25s remains)
INFO - root - 2017-12-13 03:27:12.936045: step 230, loss = 0.44, batch loss = 0.35 (36.0 examples/sec; 0.222 sec/batch; 20h:29m:12s remains)
INFO - root - 2017-12-13 03:27:15.159416: step 240, loss = 0.35, batch loss = 0.27 (36.3 examples/sec; 0.220 sec/batch; 20h:18m:53s remains)
INFO - root - 2017-12-13 03:27:17.370928: step 250, loss = 0.47, batch loss = 0.39 (35.9 examples/sec; 0.223 sec/batch; 20h:35m:33s remains)
INFO - root - 2017-12-13 03:27:19.552008: step 260, loss = 0.45, batch loss = 0.37 (36.7 examples/sec; 0.218 sec/batch; 20h:06m:35s remains)
INFO - root - 2017-12-13 03:27:21.780064: step 270, loss = 0.61, batch loss = 0.53 (37.1 examples/sec; 0.216 sec/batch; 19h:55m:08s remains)
INFO - root - 2017-12-13 03:27:23.971719: step 280, loss = 0.40, batch loss = 0.32 (36.6 examples/sec; 0.219 sec/batch; 20h:11m:43s remains)
INFO - root - 2017-12-13 03:27:26.180235: step 290, loss = 0.45, batch loss = 0.37 (35.5 examples/sec; 0.225 sec/batch; 20h:47m:53s remains)
INFO - root - 2017-12-13 03:27:28.396125: step 300, loss = 0.45, batch loss = 0.37 (36.7 examples/sec; 0.218 sec/batch; 20h:08m:20s remains)
2017-12-13 03:27:28.675401: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1434031 -0.96354985 -1.182518 -1.822027 -2.6020658 -3.1668272 -3.329581 -3.0848389 -2.548605 -1.9537786 -1.5312409 -1.3828819 -1.4593301 -1.6769185 -1.9527005][-2.2424693 -1.974671 -1.9572145 -2.34482 -2.9968 -3.5933719 -3.8727195 -3.7278178 -3.2638161 -2.7511883 -2.4112623 -2.2793109 -2.2239513 -2.1996899 -2.248847][-3.0666533 -2.7133746 -2.4306488 -2.4878707 -2.9135647 -3.4579077 -3.8087368 -3.7641842 -3.3967903 -2.9980774 -2.7721019 -2.6870408 -2.5459526 -2.3341479 -2.2084637][-3.2136683 -2.7212677 -2.1835623 -1.9388853 -2.1366169 -2.5835657 -2.9273927 -2.9249163 -2.6349802 -2.3798819 -2.336848 -2.3846931 -2.2842076 -2.0398197 -1.909235][-2.6268265 -1.8815751 -1.0980327 -0.6571877 -0.74176574 -1.144877 -1.4594126 -1.4502578 -1.1966436 -1.0618947 -1.2091241 -1.433377 -1.4799187 -1.3679559 -1.4171832][-1.7363539 -0.67291212 0.35433149 0.92623472 0.891397 0.50962496 0.25076771 0.30069256 0.54425144 0.59338617 0.29518604 -0.1038506 -0.36421871 -0.50000834 -0.83334279][-1.1064911 0.21850061 1.4574471 2.1678624 2.2157021 1.9081745 1.7323012 1.7876019 1.9896979 1.9888544 1.6567111 1.2077761 0.79674196 0.42555523 -0.16887474][-1.0799477 0.30941343 1.6551661 2.5075316 2.7342339 2.6199188 2.5838337 2.6449289 2.7866168 2.7703528 2.5019646 2.1217327 1.6594353 1.1151967 0.32935953][-1.6020062 -0.4423213 0.77401304 1.6501737 2.0668931 2.2343869 2.4141459 2.55681 2.6977987 2.7120857 2.5293941 2.2316527 1.7742243 1.1435766 0.274518][-2.14152 -1.475296 -0.67620325 -0.013172865 0.41683865 0.77160692 1.1625767 1.4787564 1.7227478 1.8228865 1.7157631 1.4554887 1.0205836 0.41208744 -0.38921118][-2.2066865 -2.0961857 -1.8262053 -1.5407164 -1.3050699 -0.97644234 -0.50727105 -0.031332016 0.35538721 0.55110884 0.48593235 0.20312357 -0.2257309 -0.74724317 -1.3599846][-1.6753006 -1.956328 -2.1406727 -2.2739062 -2.3767741 -2.2670994 -1.8755507 -1.3333502 -0.82682896 -0.53844118 -0.56026125 -0.8634305 -1.2924476 -1.7191796 -2.1159148][-0.97528315 -1.3447177 -1.7577155 -2.2051907 -2.6439004 -2.8182569 -2.6100225 -2.1108527 -1.5428369 -1.1654794 -1.1153507 -1.4049327 -1.8363311 -2.2066031 -2.4373438][-0.89678717 -1.1328554 -1.5424478 -2.1008408 -2.7118297 -3.0525107 -2.9847028 -2.5583582 -1.9564649 -1.4871614 -1.3358614 -1.5704093 -1.9947481 -2.3587308 -2.5245001][-1.5218525 -1.6030786 -1.9283645 -2.4627609 -3.0955989 -3.4925208 -3.5110543 -3.1372571 -2.5085371 -1.9461781 -1.6759708 -1.7991431 -2.1511126 -2.479682 -2.602797]]...]
INFO - root - 2017-12-13 03:27:30.896766: step 310, loss = 0.35, batch loss = 0.26 (35.5 examples/sec; 0.225 sec/batch; 20h:46m:28s remains)
INFO - root - 2017-12-13 03:27:33.115207: step 320, loss = 0.32, batch loss = 0.24 (35.8 examples/sec; 0.224 sec/batch; 20h:38m:31s remains)
INFO - root - 2017-12-13 03:27:35.315858: step 330, loss = 0.36, batch loss = 0.27 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:58s remains)
INFO - root - 2017-12-13 03:27:37.531588: step 340, loss = 0.41, batch loss = 0.32 (36.8 examples/sec; 0.218 sec/batch; 20h:04m:06s remains)
INFO - root - 2017-12-13 03:27:39.765100: step 350, loss = 0.35, batch loss = 0.27 (36.9 examples/sec; 0.217 sec/batch; 19h:59m:38s remains)
INFO - root - 2017-12-13 03:27:41.977262: step 360, loss = 0.46, batch loss = 0.38 (36.7 examples/sec; 0.218 sec/batch; 20h:07m:34s remains)
INFO - root - 2017-12-13 03:27:44.220666: step 370, loss = 0.46, batch loss = 0.37 (36.7 examples/sec; 0.218 sec/batch; 20h:05m:05s remains)
INFO - root - 2017-12-13 03:27:46.482697: step 380, loss = 0.47, batch loss = 0.38 (35.3 examples/sec; 0.226 sec/batch; 20h:53m:20s remains)
INFO - root - 2017-12-13 03:27:48.734068: step 390, loss = 0.40, batch loss = 0.32 (34.9 examples/sec; 0.229 sec/batch; 21h:07m:52s remains)
INFO - root - 2017-12-13 03:27:50.941383: step 400, loss = 0.32, batch loss = 0.24 (35.3 examples/sec; 0.226 sec/batch; 20h:52m:56s remains)
2017-12-13 03:27:51.224466: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8763413 -5.1265411 -5.0068007 -5.0337996 -5.4827204 -5.9557891 -6.0747747 -6.0028419 -5.8485861 -5.5531454 -5.3646126 -5.4201765 -5.6809449 -6.1737518 -6.5956278][-4.2950206 -4.6087265 -4.6098709 -4.7491775 -5.318449 -5.7830405 -5.73355 -5.4088511 -5.004982 -4.4113 -3.9545343 -3.8194146 -3.9983256 -4.6075239 -5.2808037][-3.7337415 -3.9011898 -3.8225942 -3.9048519 -4.4527521 -4.826612 -4.6068807 -4.0663438 -3.4221215 -2.5438352 -1.8438494 -1.6191924 -1.8350186 -2.6698697 -3.6891792][-3.281039 -3.1602063 -2.83915 -2.7144995 -3.1279356 -3.3929505 -3.0748756 -2.4072723 -1.6030879 -0.5502708 0.25920177 0.46543717 0.11186624 -1.0232844 -2.46031][-2.8273621 -2.2605975 -1.5422857 -1.0584979 -1.2349889 -1.3974237 -1.0810714 -0.41203952 0.44042039 1.5271103 2.3031714 2.4094026 1.8868554 0.4253552 -1.5015862][-2.706296 -1.7531178 -0.59321451 0.38646436 0.6119225 0.66990304 1.0235636 1.6606691 2.462137 3.4000828 3.9282372 3.76418 2.9985888 1.2483194 -1.0812149][-2.7588296 -1.7232904 -0.36774683 0.91401982 1.4734294 1.7752612 2.2652977 2.9872305 3.8014619 4.5617905 4.7499037 4.2095404 3.1223319 1.1403487 -1.3781848][-2.2709713 -1.4909489 -0.3693862 0.748451 1.2374022 1.5160124 2.0904219 3.0042617 3.9860666 4.7303743 4.7149811 3.8821352 2.504379 0.35324454 -2.1788292][-0.95795131 -0.57663417 0.037976742 0.59013963 0.5223248 0.29448724 0.62504077 1.6017625 2.7893212 3.7020128 3.7589152 2.9036028 1.4241707 -0.7610817 -3.1401749][0.73701739 0.80690789 0.92383695 0.79873633 -0.10899019 -1.2465785 -1.5667498 -0.85322809 0.3619678 1.4417489 1.7325189 1.0967305 -0.22892642 -2.2035091 -4.2256451][1.5708826 1.5223482 1.3992927 0.82950568 -0.75821352 -2.7337625 -3.7736483 -3.5137653 -2.5106113 -1.4600976 -1.0253294 -1.4062743 -2.421025 -3.9650228 -5.4586244][0.9099052 0.9099462 0.85769534 0.22591424 -1.5803254 -3.9057755 -5.3541994 -5.5003371 -4.8641996 -4.0587897 -3.6553898 -3.8510149 -4.5195527 -5.5545797 -6.4780531][-1.0198774 -0.89461255 -0.65204072 -1.0110154 -2.5376854 -4.6323485 -6.0497694 -6.4143825 -6.1625576 -5.7154932 -5.4730883 -5.5827389 -5.9666414 -6.5606003 -7.0304074][-3.2567685 -2.9594843 -2.3837554 -2.3355794 -3.3276553 -4.8803463 -6.0107918 -6.4352312 -6.475563 -6.3646326 -6.3172531 -6.4218788 -6.6365843 -6.9366431 -7.1309195][-4.5551386 -4.185925 -3.4677544 -3.209204 -3.7866743 -4.7887979 -5.5766392 -5.9686618 -6.1718507 -6.2875433 -6.3997555 -6.5458522 -6.7013049 -6.8678322 -6.9652548]]...]
INFO - root - 2017-12-13 03:27:53.444154: step 410, loss = 0.49, batch loss = 0.40 (34.8 examples/sec; 0.230 sec/batch; 21h:10m:46s remains)
INFO - root - 2017-12-13 03:27:55.670358: step 420, loss = 0.53, batch loss = 0.45 (35.8 examples/sec; 0.223 sec/batch; 20h:36m:57s remains)
INFO - root - 2017-12-13 03:27:57.916323: step 430, loss = 0.37, batch loss = 0.29 (35.5 examples/sec; 0.226 sec/batch; 20h:48m:03s remains)
INFO - root - 2017-12-13 03:28:00.140483: step 440, loss = 0.32, batch loss = 0.24 (35.8 examples/sec; 0.223 sec/batch; 20h:36m:05s remains)
INFO - root - 2017-12-13 03:28:02.372876: step 450, loss = 0.53, batch loss = 0.45 (36.5 examples/sec; 0.219 sec/batch; 20h:14m:20s remains)
INFO - root - 2017-12-13 03:28:04.591611: step 460, loss = 0.30, batch loss = 0.21 (33.6 examples/sec; 0.238 sec/batch; 21h:56m:21s remains)
INFO - root - 2017-12-13 03:28:06.821866: step 470, loss = 0.32, batch loss = 0.23 (36.5 examples/sec; 0.219 sec/batch; 20h:14m:09s remains)
INFO - root - 2017-12-13 03:28:09.084940: step 480, loss = 0.43, batch loss = 0.34 (35.5 examples/sec; 0.226 sec/batch; 20h:48m:44s remains)
INFO - root - 2017-12-13 03:28:11.325344: step 490, loss = 0.37, batch loss = 0.28 (35.9 examples/sec; 0.223 sec/batch; 20h:32m:32s remains)
INFO - root - 2017-12-13 03:28:13.537353: step 500, loss = 0.53, batch loss = 0.44 (36.0 examples/sec; 0.222 sec/batch; 20h:29m:19s remains)
2017-12-13 03:28:13.832429: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6373873 -3.7709386 -3.7008574 -3.5696654 -3.4066825 -3.2464507 -3.1264551 -3.2023873 -3.3378632 -3.278379 -2.9879966 -2.5692649 -2.1077938 -1.6346352 -1.2141926][-3.2532511 -3.4120443 -3.3252394 -3.1782358 -3.0069249 -2.8748558 -2.843694 -3.0087512 -3.1124601 -2.8752835 -2.3794179 -1.857311 -1.3726857 -0.8781476 -0.4023726][-2.8841932 -3.0177636 -2.9364097 -2.8505759 -2.7466953 -2.6910214 -2.7391119 -2.92584 -2.9027283 -2.3861737 -1.5806348 -0.84609675 -0.25449371 0.23219943 0.5697062][-2.4662867 -2.5117009 -2.4397769 -2.4089868 -2.344914 -2.3228769 -2.4137304 -2.5792894 -2.3915138 -1.542527 -0.36783385 0.620836 1.2870152 1.5885918 1.5005972][-1.9194058 -1.8496988 -1.7597032 -1.6920943 -1.5382109 -1.4258437 -1.4812858 -1.5911839 -1.2552969 -0.12654591 1.3175581 2.4071014 2.9086497 2.7723691 2.0930483][-1.3903205 -1.2277565 -1.1049762 -0.89737916 -0.50442505 -0.12185884 0.031466246 0.087747335 0.5498507 1.7435791 3.1242802 3.9562833 4.0058985 3.3414586 2.1933115][-0.96684289 -0.73798442 -0.57914996 -0.23578286 0.38656878 1.069627 1.5189073 1.7789176 2.2389386 3.1725147 4.1099186 4.4319572 4.0213766 3.043658 1.7688253][-0.64676976 -0.312505 -0.11893106 0.2410543 0.90109754 1.7242153 2.388665 2.7650077 3.0581944 3.5067761 3.8188555 3.59808 2.8861864 1.847141 0.70404363][-0.35032797 0.15421987 0.38081717 0.58174586 1.0047352 1.6686394 2.2926824 2.5773175 2.5815489 2.545028 2.3262665 1.7226202 0.89576364 -0.057371378 -0.93371296][-0.022291183 0.56572843 0.73280311 0.61798739 0.58719516 0.84491611 1.1782405 1.2203901 0.98310733 0.69524693 0.25194669 -0.46702623 -1.2367346 -1.9968375 -2.517828][0.21178508 0.58935189 0.50368667 0.022186518 -0.43374085 -0.53875947 -0.456681 -0.52665687 -0.75832582 -0.98891187 -1.341522 -1.9224013 -2.5064814 -3.044754 -3.2850933][-0.045409441 -0.10099864 -0.49356651 -1.1707807 -1.7813258 -1.9906049 -1.9548032 -1.9286945 -1.9386668 -1.9265646 -2.0318239 -2.3507097 -2.70364 -3.0692954 -3.2079623][-0.97673154 -1.3980706 -1.9463416 -2.5559094 -3.0419064 -3.1456478 -3.0282989 -2.8256366 -2.5728331 -2.3253844 -2.215812 -2.3153224 -2.4951718 -2.7847443 -2.984983][-2.2774229 -2.8248115 -3.3387003 -3.7262621 -3.9504166 -3.9143586 -3.7525861 -3.4603152 -3.0642879 -2.7058744 -2.514327 -2.5258522 -2.6227641 -2.8753488 -3.1568174][-3.4529819 -3.9422598 -4.3054438 -4.4485388 -4.4681444 -4.3980732 -4.3086872 -4.0582976 -3.6716874 -3.3392897 -3.1729424 -3.16585 -3.1864018 -3.3595908 -3.6408799]]...]
INFO - root - 2017-12-13 03:28:16.062964: step 510, loss = 0.39, batch loss = 0.31 (35.7 examples/sec; 0.224 sec/batch; 20h:39m:12s remains)
INFO - root - 2017-12-13 03:28:18.294394: step 520, loss = 0.35, batch loss = 0.27 (34.8 examples/sec; 0.230 sec/batch; 21h:10m:44s remains)
INFO - root - 2017-12-13 03:28:20.535289: step 530, loss = 0.35, batch loss = 0.27 (35.5 examples/sec; 0.226 sec/batch; 20h:48m:26s remains)
INFO - root - 2017-12-13 03:28:22.800494: step 540, loss = 0.31, batch loss = 0.23 (37.0 examples/sec; 0.216 sec/batch; 19h:56m:18s remains)
INFO - root - 2017-12-13 03:28:25.022015: step 550, loss = 0.32, batch loss = 0.24 (36.1 examples/sec; 0.222 sec/batch; 20h:27m:17s remains)
INFO - root - 2017-12-13 03:28:27.251678: step 560, loss = 0.33, batch loss = 0.25 (36.0 examples/sec; 0.222 sec/batch; 20h:29m:08s remains)
INFO - root - 2017-12-13 03:28:29.476588: step 570, loss = 0.30, batch loss = 0.22 (36.5 examples/sec; 0.219 sec/batch; 20h:12m:31s remains)
INFO - root - 2017-12-13 03:28:31.714501: step 580, loss = 0.37, batch loss = 0.29 (36.3 examples/sec; 0.220 sec/batch; 20h:18m:11s remains)
INFO - root - 2017-12-13 03:28:33.957289: step 590, loss = 0.44, batch loss = 0.35 (36.7 examples/sec; 0.218 sec/batch; 20h:06m:10s remains)
INFO - root - 2017-12-13 03:28:36.213472: step 600, loss = 0.32, batch loss = 0.24 (36.0 examples/sec; 0.222 sec/batch; 20h:30m:18s remains)
2017-12-13 03:28:36.488193: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0245981 -4.1043448 -4.4583573 -4.6391778 -4.5127616 -4.2777896 -4.2803421 -4.8685579 -5.7321014 -6.13594 -5.606709 -4.1983447 -2.6642096 -1.8174918 -1.770775][-3.9780154 -4.1381264 -4.5327849 -4.6437206 -4.2884259 -3.8321383 -3.8401668 -4.583602 -5.4785357 -5.7548485 -5.1534705 -3.9234362 -2.8058648 -2.3244591 -2.3899813][-3.5171394 -3.6777697 -4.0744996 -4.1452622 -3.6438107 -3.0649314 -3.1648381 -4.0648394 -4.9168563 -5.0061646 -4.3504629 -3.3495643 -2.63447 -2.5095491 -2.8100357][-2.8786616 -2.9989674 -3.3237736 -3.2498908 -2.4943357 -1.6892188 -1.8343441 -2.9238393 -3.8391171 -3.8951864 -3.296921 -2.4882269 -1.9615436 -1.9859966 -2.5289874][-2.3205462 -2.4535933 -2.6560009 -2.2791409 -1.0739458 0.10830379 -0.0007379055 -1.3025415 -2.4292021 -2.6623478 -2.2046332 -1.4442236 -0.83116174 -0.7764976 -1.4988494][-2.027411 -2.1797256 -2.2135973 -1.4666085 0.22189522 1.7942936 1.7575066 0.25761724 -1.131361 -1.5918205 -1.2583892 -0.46336102 0.34695363 0.59763122 -0.18890524][-2.1582561 -2.2141902 -2.053838 -1.0336983 0.94426703 2.742605 2.7779825 1.2144258 -0.28609776 -0.84086514 -0.59080768 0.15307856 0.98730445 1.2820365 0.46084905][-2.6613786 -2.5253139 -2.2191682 -1.1620312 0.7252562 2.4131892 2.4545825 1.0236175 -0.30698657 -0.702549 -0.38383842 0.28053164 0.9219811 1.0490983 0.14365554][-3.3243005 -3.1163526 -2.8665657 -2.0678558 -0.59376717 0.73962522 0.78571439 -0.30093551 -1.2043917 -1.2647071 -0.78513932 -0.17409658 0.2254951 0.14845943 -0.77597547][-3.7317638 -3.6330218 -3.6357822 -3.3288212 -2.4938755 -1.6731951 -1.6583693 -2.3456895 -2.7660537 -2.4928212 -1.861973 -1.2915769 -1.1151164 -1.3333879 -2.1205175][-3.883383 -3.9427104 -4.1879373 -4.3296909 -4.1150861 -3.8409774 -3.9848452 -4.477221 -4.6653938 -4.2788873 -3.615639 -3.0648944 -2.9559195 -3.1338987 -3.6288841][-4.0019417 -4.1739388 -4.5074573 -4.8362837 -4.9628148 -5.0638976 -5.3948717 -5.8848209 -6.0986958 -5.8358068 -5.2959461 -4.7898374 -4.6495185 -4.7151365 -4.8918605][-4.0701571 -4.2910609 -4.5778918 -4.8539791 -5.0421543 -5.2815266 -5.6962829 -6.19541 -6.4633989 -6.3534918 -5.9749823 -5.5676622 -5.43659 -5.4720535 -5.4904437][-4.1327281 -4.3789024 -4.5829058 -4.7090096 -4.800703 -4.9804683 -5.3044276 -5.7038054 -5.9491673 -5.9223909 -5.6680365 -5.3569837 -5.21766 -5.2531729 -5.2732043][-4.4209547 -4.6008415 -4.6058192 -4.4794564 -4.3928928 -4.4153762 -4.5339813 -4.7374644 -4.886724 -4.8854671 -4.7405782 -4.5457506 -4.4324431 -4.4810982 -4.526866]]...]
INFO - root - 2017-12-13 03:28:38.738299: step 610, loss = 0.36, batch loss = 0.27 (35.4 examples/sec; 0.226 sec/batch; 20h:50m:08s remains)
INFO - root - 2017-12-13 03:28:40.995252: step 620, loss = 0.26, batch loss = 0.18 (35.7 examples/sec; 0.224 sec/batch; 20h:40m:30s remains)
INFO - root - 2017-12-13 03:28:43.234418: step 630, loss = 0.31, batch loss = 0.23 (34.6 examples/sec; 0.231 sec/batch; 21h:17m:13s remains)
INFO - root - 2017-12-13 03:28:45.470290: step 640, loss = 0.37, batch loss = 0.29 (36.8 examples/sec; 0.218 sec/batch; 20h:03m:35s remains)
INFO - root - 2017-12-13 03:28:47.714669: step 650, loss = 0.37, batch loss = 0.29 (35.8 examples/sec; 0.224 sec/batch; 20h:36m:51s remains)
INFO - root - 2017-12-13 03:28:49.969761: step 660, loss = 0.33, batch loss = 0.25 (36.1 examples/sec; 0.221 sec/batch; 20h:24m:09s remains)
INFO - root - 2017-12-13 03:28:52.238033: step 670, loss = 0.50, batch loss = 0.41 (34.9 examples/sec; 0.229 sec/batch; 21h:09m:06s remains)
INFO - root - 2017-12-13 03:28:54.474331: step 680, loss = 0.44, batch loss = 0.36 (34.8 examples/sec; 0.230 sec/batch; 21h:10m:14s remains)
INFO - root - 2017-12-13 03:28:56.714444: step 690, loss = 0.35, batch loss = 0.27 (36.0 examples/sec; 0.222 sec/batch; 20h:29m:46s remains)
INFO - root - 2017-12-13 03:28:58.982615: step 700, loss = 0.40, batch loss = 0.32 (34.7 examples/sec; 0.230 sec/batch; 21h:13m:36s remains)
2017-12-13 03:28:59.283615: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7476385 -3.3712051 -3.8957396 -4.2111263 -4.3071556 -4.2942858 -4.2890038 -4.3845906 -4.6484852 -4.9624777 -5.1103444 -4.9473929 -4.4079175 -3.7732108 -3.3616936][-2.7470775 -3.3980258 -3.9227538 -4.1446791 -4.1155381 -4.0369191 -4.0540781 -4.2794204 -4.6915054 -5.0728531 -5.1195674 -4.6678143 -3.7817435 -2.9081824 -2.45061][-3.642338 -4.1292534 -4.4109755 -4.3174233 -3.9837856 -3.727998 -3.7696033 -4.2357435 -4.9375639 -5.4669476 -5.3824053 -4.5517015 -3.2657585 -2.2025595 -1.8175936][-5.1878653 -5.4218521 -5.3100276 -4.7544971 -3.9705367 -3.398262 -3.3598866 -4.0214767 -5.0433259 -5.8014374 -5.7055979 -4.6114659 -3.0431101 -1.8681511 -1.5590458][-6.4631519 -6.4378958 -5.8879447 -4.7965345 -3.4466417 -2.3664978 -2.0107405 -2.6789348 -3.9845476 -5.1159306 -5.3235016 -4.33925 -2.7529905 -1.4939275 -1.070276][-6.9799995 -6.694396 -5.70607 -4.0338569 -2.0306945 -0.28764224 0.5949595 0.092967749 -1.5138428 -3.2494533 -4.1329041 -3.677556 -2.3858967 -1.0887368 -0.31577706][-6.6138897 -6.1644115 -4.8662043 -2.7216828 -0.11609221 2.3050082 3.7968791 3.5715668 1.6967866 -0.74216652 -2.4975452 -2.8343661 -2.1095943 -0.95723343 0.14124155][-5.9598932 -5.5478878 -4.2230577 -1.9228399 1.0168555 3.9213331 5.9071169 5.968421 4.005868 1.12732 -1.2560625 -2.2896922 -2.2426484 -1.4989483 -0.35507703][-5.518291 -5.3231716 -4.2350779 -2.1500287 0.6891892 3.6572316 5.8274603 6.1384382 4.3933821 1.5552127 -0.974107 -2.3421841 -2.7803388 -2.4984257 -1.5655913][-5.13433 -5.2030034 -4.493187 -2.8865302 -0.57403064 1.9383876 3.8606837 4.2902613 3.0027592 0.68318486 -1.4714353 -2.7323203 -3.3169794 -3.3428433 -2.7278931][-4.8441963 -5.0526085 -4.6459732 -3.5319083 -1.9295882 -0.2361877 1.0308683 1.3275473 0.51559663 -1.0120556 -2.3994737 -3.1827993 -3.5798814 -3.6897566 -3.3903887][-4.8112288 -5.0054851 -4.7224569 -3.9350896 -2.9504313 -2.110497 -1.6413891 -1.6550241 -2.11171 -2.8142345 -3.3081791 -3.461719 -3.5177042 -3.6027617 -3.5906439][-5.0979795 -5.1593981 -4.8300762 -4.1603169 -3.5657291 -3.3725688 -3.5686975 -3.8845272 -4.0944972 -4.1303654 -3.8973408 -3.5215058 -3.274827 -3.3475039 -3.6162529][-5.554287 -5.5075126 -5.1054478 -4.4628139 -4.0751419 -4.2616754 -4.8470497 -5.3170323 -5.3255715 -4.9129133 -4.2350454 -3.5475526 -3.1258175 -3.230226 -3.7490203][-5.8896227 -5.8307686 -5.43001 -4.8541 -4.5871186 -4.9035544 -5.5551558 -5.9720907 -5.7872982 -5.1501803 -4.3345776 -3.5880053 -3.1595392 -3.3356941 -4.012445]]...]
INFO - root - 2017-12-13 03:29:01.531075: step 710, loss = 0.37, batch loss = 0.29 (35.4 examples/sec; 0.226 sec/batch; 20h:50m:47s remains)
INFO - root - 2017-12-13 03:29:03.783431: step 720, loss = 0.39, batch loss = 0.31 (34.4 examples/sec; 0.232 sec/batch; 21h:24m:20s remains)
INFO - root - 2017-12-13 03:29:06.018526: step 730, loss = 0.37, batch loss = 0.29 (35.8 examples/sec; 0.224 sec/batch; 20h:36m:20s remains)
INFO - root - 2017-12-13 03:29:08.252353: step 740, loss = 0.51, batch loss = 0.43 (36.4 examples/sec; 0.220 sec/batch; 20h:14m:54s remains)
INFO - root - 2017-12-13 03:29:10.502507: step 750, loss = 0.39, batch loss = 0.31 (36.0 examples/sec; 0.222 sec/batch; 20h:27m:38s remains)
INFO - root - 2017-12-13 03:29:12.752488: step 760, loss = 0.35, batch loss = 0.27 (33.2 examples/sec; 0.241 sec/batch; 22h:11m:34s remains)
INFO - root - 2017-12-13 03:29:15.011908: step 770, loss = 0.26, batch loss = 0.18 (36.0 examples/sec; 0.222 sec/batch; 20h:28m:32s remains)
INFO - root - 2017-12-13 03:29:17.280192: step 780, loss = 0.64, batch loss = 0.55 (35.9 examples/sec; 0.223 sec/batch; 20h:31m:08s remains)
INFO - root - 2017-12-13 03:29:19.542361: step 790, loss = 0.35, batch loss = 0.27 (35.4 examples/sec; 0.226 sec/batch; 20h:48m:12s remains)
INFO - root - 2017-12-13 03:29:21.814027: step 800, loss = 0.50, batch loss = 0.42 (35.7 examples/sec; 0.224 sec/batch; 20h:40m:04s remains)
2017-12-13 03:29:22.090765: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5481021 -3.3063593 -2.5975981 -1.9028676 -2.0350318 -2.7285085 -3.186043 -2.979048 -1.9604777 -0.75543928 -0.4847672 -1.1559582 -1.7778834 -2.1212125 -2.3711796][-3.8105671 -3.6655169 -3.1682358 -2.560246 -2.4748456 -2.6489291 -2.5535512 -1.9643315 -0.93315649 -0.075396776 -0.22322726 -1.0348501 -1.4549775 -1.55334 -1.7148776][-3.4352405 -3.3908608 -3.1155117 -2.7209892 -2.6046295 -2.4408441 -1.927457 -1.0623467 -0.061133146 0.46734786 -0.05116868 -0.98005962 -1.2403681 -1.1074951 -1.1634588][-3.0630996 -3.1755683 -3.11929 -2.9488144 -2.8067489 -2.3687062 -1.4767394 -0.31290007 0.66856885 0.88329482 0.076678991 -0.86977458 -0.991133 -0.73605704 -0.73255634][-2.8004594 -3.1798644 -3.4130776 -3.3528166 -2.9813194 -2.0682566 -0.66305733 0.75528407 1.5201476 1.2290156 0.14852357 -0.72284675 -0.65966845 -0.31004047 -0.32073736][-2.3768492 -3.131304 -3.7394433 -3.6964428 -2.8544023 -1.1072111 1.0649049 2.6449897 2.8342621 1.7649767 0.33793998 -0.42955661 -0.16747069 0.251009 0.17427135][-1.8537488 -2.9465017 -3.8655565 -3.7405496 -2.3070674 0.42631745 3.4062674 4.9379797 4.2379904 2.2419727 0.51372457 -0.036953211 0.44087338 0.88955426 0.71150517][-1.5978622 -2.7887945 -3.7892728 -3.5295537 -1.6710865 1.6623809 4.9650383 6.0903845 4.5278244 1.9194863 0.24370527 0.053690434 0.686733 1.1081741 0.83419538][-1.6121075 -2.6801686 -3.5894041 -3.2825975 -1.4236627 1.7717822 4.6274672 5.0445957 3.0244248 0.49369025 -0.68587422 -0.5215373 0.08865571 0.42716479 0.17441654][-1.6040421 -2.5397708 -3.3495998 -3.1457067 -1.618758 0.92684579 2.9459093 2.7793338 0.84429526 -1.099194 -1.7521247 -1.5714405 -1.2635021 -1.0498443 -1.1571097][-1.5024929 -2.3910089 -3.1598575 -3.1203408 -2.0047791 -0.20597768 1.0630167 0.68817973 -0.798527 -2.0801308 -2.5351579 -2.7391696 -2.9098814 -2.8341465 -2.7041278][-1.2589014 -2.235271 -3.0449011 -3.1870928 -2.4795768 -1.3607116 -0.65428925 -1.0298879 -2.0382977 -2.8360167 -3.2917662 -3.8795271 -4.3923764 -4.352911 -3.9121525][-0.97904038 -2.1375358 -3.0624979 -3.3744144 -3.0137763 -2.4135776 -2.137738 -2.5227742 -3.2126498 -3.7267728 -4.1608133 -4.81762 -5.3422527 -5.2431779 -4.55604][-0.62366128 -1.9915469 -3.1333256 -3.6532991 -3.541435 -3.2516606 -3.2667449 -3.6993618 -4.243732 -4.5776548 -4.8616924 -5.2792239 -5.5856228 -5.4252052 -4.67168][-0.53673339 -2.0318213 -3.3482218 -3.9943714 -3.9527469 -3.732971 -3.8351662 -4.2559566 -4.7075367 -4.9015064 -4.9792328 -5.1344194 -5.2823372 -5.1512561 -4.481349]]...]
INFO - root - 2017-12-13 03:29:24.315920: step 810, loss = 0.44, batch loss = 0.36 (35.6 examples/sec; 0.225 sec/batch; 20h:41m:54s remains)
INFO - root - 2017-12-13 03:29:26.562346: step 820, loss = 0.30, batch loss = 0.22 (35.5 examples/sec; 0.225 sec/batch; 20h:45m:59s remains)
INFO - root - 2017-12-13 03:29:28.813908: step 830, loss = 0.45, batch loss = 0.36 (35.6 examples/sec; 0.225 sec/batch; 20h:43m:34s remains)
INFO - root - 2017-12-13 03:29:31.065842: step 840, loss = 0.29, batch loss = 0.21 (37.3 examples/sec; 0.215 sec/batch; 19h:46m:56s remains)
INFO - root - 2017-12-13 03:29:33.292352: step 850, loss = 0.33, batch loss = 0.25 (36.4 examples/sec; 0.220 sec/batch; 20h:15m:16s remains)
INFO - root - 2017-12-13 03:29:35.538537: step 860, loss = 0.41, batch loss = 0.33 (35.7 examples/sec; 0.224 sec/batch; 20h:40m:03s remains)
INFO - root - 2017-12-13 03:29:37.786631: step 870, loss = 0.36, batch loss = 0.28 (34.9 examples/sec; 0.230 sec/batch; 21h:08m:35s remains)
INFO - root - 2017-12-13 03:29:40.024794: step 880, loss = 0.53, batch loss = 0.45 (35.6 examples/sec; 0.225 sec/batch; 20h:42m:51s remains)
INFO - root - 2017-12-13 03:29:42.277578: step 890, loss = 0.41, batch loss = 0.33 (34.9 examples/sec; 0.229 sec/batch; 21h:05m:41s remains)
INFO - root - 2017-12-13 03:29:44.515673: step 900, loss = 0.28, batch loss = 0.20 (35.9 examples/sec; 0.223 sec/batch; 20h:30m:31s remains)
2017-12-13 03:29:44.783705: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.75913262 -1.2439046 -1.674319 -1.8727731 -1.9071996 -1.9782563 -2.1248937 -2.2568421 -2.2332511 -2.1872838 -2.1606343 -2.2059202 -2.3735075 -2.5182967 -2.6777837][0.55982924 -0.40605426 -1.336313 -1.8691823 -1.9547977 -1.8916341 -1.8849285 -2.0084047 -2.0852604 -2.1123233 -2.0312624 -1.9966283 -2.1571465 -2.3802128 -2.6652572][1.2060776 -0.16102076 -1.4828737 -2.2212973 -2.2262483 -1.9219911 -1.6609455 -1.6601136 -1.8148909 -1.9598596 -1.9161364 -1.8870057 -2.0564098 -2.3424006 -2.7267995][0.69792223 -0.64700317 -1.9173166 -2.4477408 -2.1160843 -1.4071054 -0.75373793 -0.54741311 -0.81676245 -1.2245495 -1.476779 -1.6848956 -1.9465513 -2.2741089 -2.6766725][-0.5386765 -1.3676832 -2.1031194 -2.0318651 -1.1450188 0.079591036 1.1395054 1.4281597 0.82940674 -0.10351729 -0.94926834 -1.5658386 -1.9473633 -2.1898942 -2.4124894][-1.6207218 -1.6803012 -1.5774901 -0.70748258 0.77458334 2.4391184 3.6983666 3.778563 2.6009588 0.9378705 -0.57005382 -1.4884653 -1.8093029 -1.7746834 -1.6934938][-1.9501532 -1.3793976 -0.564363 0.90642118 2.7972407 4.6381855 5.8066349 5.5146561 3.7775798 1.485395 -0.48896885 -1.4563317 -1.5130997 -1.0983901 -0.7130332][-1.9166648 -1.1941302 -0.18856263 1.3695989 3.3069448 5.0845013 6.0497227 5.5162888 3.5709543 1.12744 -0.86883473 -1.6450202 -1.4113665 -0.74188447 -0.1886487][-2.533896 -2.1293337 -1.4247751 -0.20448995 1.4999719 3.1047115 3.9829993 3.5920086 2.0077434 -0.016897202 -1.6109065 -2.0636604 -1.6551274 -0.96508956 -0.43192554][-3.864552 -3.8305645 -3.4883795 -2.6690795 -1.2967696 0.097765446 0.99295759 1.0073562 0.11410284 -1.1819673 -2.1993346 -2.3796639 -1.9536842 -1.4360921 -1.1099026][-5.085494 -5.2370028 -5.0895796 -4.5718303 -3.6001194 -2.5345268 -1.7129934 -1.4017887 -1.7163835 -2.3185143 -2.7144966 -2.6291025 -2.2004883 -1.8584148 -1.7848173][-5.4114718 -5.5262809 -5.3876419 -5.0412493 -4.4483833 -3.7877593 -3.2443831 -2.9677353 -3.0936508 -3.2913203 -3.1593823 -2.7387748 -2.1829202 -1.8983691 -2.0126011][-5.0633392 -5.05163 -4.8388753 -4.5010514 -4.0918212 -3.7225862 -3.5410569 -3.5843434 -3.8869531 -4.0259862 -3.6031592 -2.8763957 -2.1001253 -1.7063025 -1.828117][-4.7314396 -4.6560745 -4.4149609 -4.0613146 -3.7205355 -3.4964869 -3.5325003 -3.8202548 -4.3244929 -4.59353 -4.1472287 -3.281533 -2.3302684 -1.7411642 -1.7050625][-4.5446062 -4.4807096 -4.2802505 -3.9673934 -3.6804523 -3.5210836 -3.5794206 -3.8691471 -4.3835015 -4.7052393 -4.3335295 -3.5062304 -2.5713394 -1.9093393 -1.730299]]...]
INFO - root - 2017-12-13 03:29:47.029130: step 910, loss = 0.37, batch loss = 0.29 (35.6 examples/sec; 0.225 sec/batch; 20h:43m:25s remains)
INFO - root - 2017-12-13 03:29:49.269006: step 920, loss = 0.57, batch loss = 0.49 (35.2 examples/sec; 0.227 sec/batch; 20h:54m:22s remains)
INFO - root - 2017-12-13 03:29:51.536113: step 930, loss = 0.38, batch loss = 0.30 (34.9 examples/sec; 0.229 sec/batch; 21h:05m:09s remains)
INFO - root - 2017-12-13 03:29:53.759474: step 940, loss = 0.29, batch loss = 0.20 (36.4 examples/sec; 0.220 sec/batch; 20h:15m:44s remains)
INFO - root - 2017-12-13 03:29:56.010821: step 950, loss = 0.38, batch loss = 0.29 (35.7 examples/sec; 0.224 sec/batch; 20h:37m:44s remains)
INFO - root - 2017-12-13 03:29:58.244989: step 960, loss = 0.40, batch loss = 0.32 (35.2 examples/sec; 0.227 sec/batch; 20h:55m:25s remains)
INFO - root - 2017-12-13 03:30:00.481464: step 970, loss = 0.40, batch loss = 0.31 (35.1 examples/sec; 0.228 sec/batch; 20h:58m:35s remains)
INFO - root - 2017-12-13 03:30:02.703127: step 980, loss = 0.33, batch loss = 0.25 (35.1 examples/sec; 0.228 sec/batch; 20h:59m:17s remains)
INFO - root - 2017-12-13 03:30:04.943773: step 990, loss = 0.32, batch loss = 0.24 (35.1 examples/sec; 0.228 sec/batch; 20h:57m:52s remains)
INFO - root - 2017-12-13 03:30:07.204333: step 1000, loss = 0.37, batch loss = 0.29 (36.5 examples/sec; 0.219 sec/batch; 20h:11m:01s remains)
2017-12-13 03:30:07.477692: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4984913 -3.6753302 -3.7458088 -3.585566 -3.4349177 -3.2877417 -3.1469984 -3.4794757 -4.0375643 -4.4326124 -4.7063217 -5.1544709 -5.40882 -5.4188309 -5.1341367][-2.5040755 -2.8636065 -3.139483 -3.1068902 -2.988873 -2.7323275 -2.4114928 -2.70982 -3.4546852 -4.1425838 -4.6365252 -5.2932172 -5.756629 -5.8143549 -5.4451704][-1.5100272 -2.0482025 -2.5227568 -2.645828 -2.5583858 -2.1747856 -1.6692406 -1.9072798 -2.8157585 -3.765959 -4.4500217 -5.2349005 -5.8942595 -6.0365839 -5.6437373][-0.92888856 -1.6258776 -2.2310662 -2.3852656 -2.1710997 -1.5241709 -0.72863221 -0.80568385 -1.8016323 -3.016561 -3.9517484 -4.8967242 -5.7778172 -6.0806837 -5.7896004][-0.93198419 -1.6521171 -2.2115414 -2.2015033 -1.6984982 -0.66083813 0.50543571 0.62416816 -0.4488256 -1.9521798 -3.2123632 -4.3869181 -5.5351057 -6.0333772 -5.8793888][-1.4392369 -2.0211949 -2.3439853 -2.0196579 -1.1312113 0.36539125 1.9371254 2.2704456 1.1420972 -0.63248873 -2.2587891 -3.7325611 -5.1750164 -5.8967171 -5.9039955][-2.0604477 -2.4161692 -2.4177675 -1.7407759 -0.467412 1.4415238 3.3492014 3.8628762 2.6933315 0.69590354 -1.2771719 -3.0680332 -4.7649074 -5.6990519 -5.8597045][-2.5111563 -2.5938525 -2.3152218 -1.3903756 0.083872795 2.1849639 4.26488 4.8890629 3.7192786 1.6143734 -0.56081533 -2.577553 -4.4185138 -5.4705534 -5.7317753][-2.6184087 -2.4176831 -1.9898424 -1.0629482 0.26973271 2.2062318 4.2002621 4.8366985 3.7581508 1.7591631 -0.34331083 -2.369554 -4.1903496 -5.2170382 -5.5125432][-2.4855812 -2.0782831 -1.6513129 -0.93528366 0.0025520325 1.5133536 3.2074611 3.7649357 2.8438227 1.1362307 -0.63955808 -2.4626231 -4.1492114 -5.0776677 -5.3411951][-2.3948009 -1.9469346 -1.6067152 -1.1515136 -0.61923814 0.43743181 1.7613499 2.1865852 1.4294164 0.062383652 -1.3282762 -2.8916612 -4.4119759 -5.1946397 -5.3445492][-2.5498252 -2.224999 -1.9908257 -1.7200202 -1.4437134 -0.7262249 0.25661945 0.53069329 -0.11309671 -1.2245197 -2.3189254 -3.651763 -4.9943304 -5.5793056 -5.52855][-3.0710015 -2.9349086 -2.7833903 -2.6038024 -2.4450092 -1.9458635 -1.240196 -1.1010869 -1.6806031 -2.6343441 -3.5433097 -4.6697111 -5.783164 -6.1245723 -5.8178115][-3.8955996 -3.9054904 -3.8068278 -3.6699262 -3.5500526 -3.1783321 -2.6703699 -2.619715 -3.093313 -3.861901 -4.6267414 -5.5785141 -6.4274311 -6.5316544 -6.0370884][-4.4067459 -4.5102959 -4.4362893 -4.3188615 -4.2048087 -3.9188578 -3.5612514 -3.5673981 -3.9830048 -4.6333938 -5.2778573 -6.0082045 -6.573802 -6.5053267 -5.900979]]...]
INFO - root - 2017-12-13 03:30:09.743198: step 1010, loss = 0.33, batch loss = 0.25 (35.8 examples/sec; 0.223 sec/batch; 20h:34m:35s remains)
INFO - root - 2017-12-13 03:30:11.981303: step 1020, loss = 0.34, batch loss = 0.26 (35.1 examples/sec; 0.228 sec/batch; 21h:00m:21s remains)
INFO - root - 2017-12-13 03:30:14.233509: step 1030, loss = 0.53, batch loss = 0.45 (35.7 examples/sec; 0.224 sec/batch; 20h:37m:33s remains)
INFO - root - 2017-12-13 03:30:16.480072: step 1040, loss = 0.30, batch loss = 0.22 (35.0 examples/sec; 0.229 sec/batch; 21h:03m:12s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-deformonlyinstance
INFO - root - 2017-12-13 03:30:18.718828: step 1050, loss = 0.53, batch loss = 0.45 (36.2 examples/sec; 0.221 sec/batch; 20h:21m:25s remains)
INFO - root - 2017-12-13 03:30:20.997934: step 1060, loss = 0.36, batch loss = 0.28 (36.3 examples/sec; 0.220 sec/batch; 20h:18m:00s remains)
INFO - root - 2017-12-13 03:30:23.263849: step 1070, loss = 0.33, batch loss = 0.25 (35.3 examples/sec; 0.227 sec/batch; 20h:52m:43s remains)
INFO - root - 2017-12-13 03:30:25.483770: step 1080, loss = 0.29, batch loss = 0.21 (36.1 examples/sec; 0.221 sec/batch; 20h:23m:05s remains)
