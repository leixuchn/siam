INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "210"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-11 09:14:44.993246: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 09:14:44.993283: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 09:14:44.993395: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 09:14:44.993400: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 09:14:44.993404: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 09:14:45.398002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 8.74GiB
2017-12-11 09:14:45.398038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-11 09:14:45.398045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-11 09:14:45.398053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
111111111 Tensor("siamese_fc/conv5/concat:0", shape=(8, 6, 6, 256), dtype=float32) Tensor("siamese_fc/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/db1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 6, 6), dtype=float32)
sdiufhasudf Tensor("siamese_fc/conv5/b1/BiasAdd:0", shape=(8, 6, 6, 128), dtype=float32)
111111111 Tensor("siamese_fc_1/conv5/concat:0", shape=(8, 20, 20, 256), dtype=float32) Tensor("siamese_fc_1/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/db1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 20, 20), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/b1/BiasAdd:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
aiaiaiaiaiaiaiiaaiia [<tf.Variable 'siamese_fc/conv5/def/offset1/weights:0' shape=(3, 3, 256, 18) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/db1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref>]
INFO - root - 2017-12-11 09:14:51.932126: step 0, loss = 0.25, batch loss = 0.17 (1.7 examples/sec; 4.577 sec/batch; 422h:44m:34s remains)
2017-12-11 09:14:52.758451: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5809941 -5.029706 -5.7303 -6.4165573 -6.7891865 -6.78719 -6.525794 -6.2195559 -6.1850157 -6.4404936 -6.7983541 -7.0963955 -6.9925632 -6.4680767 -5.7330194][-4.9415164 -5.5868697 -6.484602 -7.18419 -7.3424072 -7.0430527 -6.5827484 -6.2598052 -6.3673639 -6.844305 -7.3852797 -7.8085003 -7.7412233 -7.1412172 -6.2490687][-5.1423764 -5.9059286 -6.8483353 -7.3017731 -6.9394064 -6.0272665 -5.1279678 -4.6344013 -4.844697 -5.659029 -6.615871 -7.4793625 -7.79238 -7.3932877 -6.507865][-5.02158 -5.8182983 -6.697772 -6.7820292 -5.8025174 -4.174109 -2.7073255 -1.8570349 -2.0281384 -3.2312088 -4.8148861 -6.3909984 -7.3244076 -7.3023024 -6.5359325][-4.5481124 -5.3633685 -6.1903105 -5.9621806 -4.482058 -2.2227201 -0.1159482 1.3045421 1.3461308 -0.27652121 -2.6043105 -4.9838543 -6.6172996 -7.0493803 -6.4818258][-3.8253074 -4.6749306 -5.4503055 -4.9512892 -3.0283165 -0.14853239 2.7146912 4.884613 5.2163162 3.05754 -0.15874958 -3.4067633 -5.7380815 -6.6078892 -6.3012171][-3.0975113 -3.928858 -4.5865741 -3.8869524 -1.7499461 1.4181452 4.7064886 7.3456411 7.7530432 5.0676365 1.1981001 -2.5670753 -5.2052555 -6.2220984 -6.0625105][-2.6853514 -3.4820304 -4.066608 -3.4306831 -1.5761461 1.2246456 4.2192888 6.68972 6.8928633 4.1056118 0.30337906 -3.2192426 -5.4928679 -6.2206783 -5.9818592][-2.8094611 -3.5297432 -4.12279 -3.7976723 -2.5459828 -0.45165873 1.8837743 3.8802347 3.9353485 1.5412612 -1.6423681 -4.4488997 -6.011436 -6.3018303 -5.9558654][-3.296917 -3.8688908 -4.4754648 -4.5146518 -3.8494668 -2.3685715 -0.63108039 0.89796352 0.91989756 -0.91605926 -3.3574328 -5.4097891 -6.3061123 -6.2547765 -5.8725553][-3.8476684 -4.2978425 -4.89178 -5.192481 -4.9007611 -3.814477 -2.5733614 -1.5005081 -1.5250916 -2.86559 -4.637435 -6.0753403 -6.4724255 -6.1778846 -5.7696347][-4.2969041 -4.6985 -5.3015294 -5.7685561 -5.665626 -4.8166456 -3.9313662 -3.1925447 -3.2170568 -4.1284056 -5.3398194 -6.35222 -6.4892616 -6.1088157 -5.71384][-4.3232203 -4.7439609 -5.4166217 -5.9985466 -5.9180989 -5.1288748 -4.3699036 -3.7432661 -3.652586 -4.2042036 -5.0735712 -5.961072 -6.1594305 -5.91638 -5.6331983][-4.13572 -4.5667262 -5.3001466 -5.9185796 -5.6999464 -4.7807031 -3.9209538 -3.212872 -2.9323068 -3.210691 -3.9772882 -5.003479 -5.5018158 -5.5582018 -5.4595013][-3.6739802 -4.107821 -4.9191165 -5.5929742 -5.2099295 -4.0650163 -3.0100441 -2.2230256 -1.8838229 -2.0790231 -2.9572268 -4.2256107 -4.9958072 -5.272141 -5.2864261]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-shortcut/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-shortcut/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-shortcut
INFO - root - 2017-12-11 09:14:59.080624: step 10, loss = 0.28, batch loss = 0.19 (15.0 examples/sec; 0.534 sec/batch; 49h:17m:02s remains)
INFO - root - 2017-12-11 09:15:04.402148: step 20, loss = 0.27, batch loss = 0.19 (14.6 examples/sec; 0.548 sec/batch; 50h:38m:26s remains)
INFO - root - 2017-12-11 09:15:09.734991: step 30, loss = 0.26, batch loss = 0.18 (15.0 examples/sec; 0.534 sec/batch; 49h:19m:39s remains)
INFO - root - 2017-12-11 09:15:15.001556: step 40, loss = 0.40, batch loss = 0.32 (15.7 examples/sec; 0.510 sec/batch; 47h:08m:31s remains)
INFO - root - 2017-12-11 09:15:20.366310: step 50, loss = 0.35, batch loss = 0.26 (14.9 examples/sec; 0.538 sec/batch; 49h:40m:36s remains)
INFO - root - 2017-12-11 09:15:25.476013: step 60, loss = 0.50, batch loss = 0.41 (15.1 examples/sec; 0.528 sec/batch; 48h:46m:36s remains)
INFO - root - 2017-12-11 09:15:30.910232: step 70, loss = 0.25, batch loss = 0.16 (14.8 examples/sec; 0.540 sec/batch; 49h:52m:46s remains)
INFO - root - 2017-12-11 09:15:36.330440: step 80, loss = 0.25, batch loss = 0.16 (14.9 examples/sec; 0.536 sec/batch; 49h:29m:03s remains)
INFO - root - 2017-12-11 09:15:41.644542: step 90, loss = 0.29, batch loss = 0.21 (15.0 examples/sec; 0.533 sec/batch; 49h:13m:53s remains)
INFO - root - 2017-12-11 09:15:47.033727: step 100, loss = 0.26, batch loss = 0.18 (15.3 examples/sec; 0.524 sec/batch; 48h:22m:15s remains)
2017-12-11 09:15:47.667024: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5174561 -4.8758535 -4.2457032 -4.1990337 -4.6348987 -5.1869555 -5.9067082 -6.5133057 -6.4956121 -5.8179903 -4.4246769 -2.2834611 0.076202869 2.1433067 2.977994][-6.2921343 -5.71874 -5.1438279 -4.9866114 -5.1822028 -5.569304 -6.0585885 -6.3754749 -6.0244069 -5.0573087 -3.5916493 -1.5751321 0.63254023 2.6856189 3.6034594][-7.0421457 -6.5787659 -6.067359 -5.6911979 -5.4328208 -5.3287263 -5.282279 -5.2047696 -4.7433882 -3.9562788 -2.9340613 -1.5196643 0.20389938 2.0311375 2.95755][-7.6867571 -7.2508516 -6.7309742 -6.0434952 -5.1580052 -4.272965 -3.4924502 -2.9981177 -2.5882735 -2.2972414 -2.097707 -1.6432052 -0.73996162 0.4728632 1.1523542][-8.2096348 -7.7457943 -7.1619129 -6.1184697 -4.4972787 -2.6746421 -1.0924373 -0.17896032 0.14709568 -0.25063944 -1.1643066 -1.8415861 -1.8104575 -1.33798 -1.0801041][-8.3008614 -7.7323475 -7.0639896 -5.7487879 -3.53967 -0.91366029 1.3742204 2.7106056 3.0256095 1.9644852 -0.08671999 -1.8582945 -2.655515 -2.9941471 -3.299159][-7.57446 -7.007195 -6.4235234 -5.1366606 -2.7981968 0.18511724 2.9184217 4.6217155 4.9829664 3.4500937 0.57850361 -1.9373708 -3.3453488 -4.3945036 -5.2131162][-6.05921 -5.6556964 -5.3738933 -4.4616089 -2.5331559 0.23127413 3.0493526 4.9926004 5.450141 3.7383127 0.53506613 -2.2389612 -3.9418986 -5.398458 -6.4449477][-4.073298 -3.991323 -4.1945734 -3.8479023 -2.6806002 -0.69864321 1.6311717 3.3934059 3.8970184 2.4114962 -0.417058 -2.7350836 -4.1826606 -5.5696759 -6.5570397][-1.1937361 -1.6179574 -2.529767 -2.9371758 -2.7991104 -1.9910746 -0.60389042 0.55253744 0.90895319 -0.17945719 -2.1482317 -3.4805579 -4.2151237 -5.1519704 -5.8990116][1.662333 0.70533371 -0.95635247 -2.1053097 -2.9171839 -3.1931632 -2.747889 -2.2900639 -2.2043417 -2.9362206 -4.0319762 -4.3684673 -4.3121858 -4.6545873 -5.0371523][3.6249132 2.298068 0.064593792 -1.6672645 -3.2068996 -4.2922273 -4.594944 -4.657218 -4.706953 -5.0587888 -5.4134493 -5.0010767 -4.3409581 -4.1817331 -4.2230911][4.2272196 2.8702474 0.45321941 -1.5661731 -3.4481564 -4.8582325 -5.4769859 -5.6918592 -5.6601839 -5.6829891 -5.6101418 -4.8905029 -4.0309873 -3.6468587 -3.509387][3.1282372 1.9706197 -0.16589069 -2.0055864 -3.7139111 -4.9073181 -5.3634062 -5.3871703 -5.1418314 -4.9748755 -4.80584 -4.1663175 -3.4410965 -3.0837154 -2.9924245][1.1247187 0.29129362 -1.2834361 -2.6589561 -3.9328125 -4.6392436 -4.6817627 -4.3660965 -3.8871465 -3.6583457 -3.6160891 -3.2799277 -2.899816 -2.749697 -2.8386335]]...]
INFO - root - 2017-12-11 09:15:52.994235: step 110, loss = 0.33, batch loss = 0.25 (14.3 examples/sec; 0.558 sec/batch; 51h:33m:34s remains)
INFO - root - 2017-12-11 09:15:58.277417: step 120, loss = 0.31, batch loss = 0.23 (15.0 examples/sec; 0.535 sec/batch; 49h:23m:47s remains)
INFO - root - 2017-12-11 09:16:03.674692: step 130, loss = 0.31, batch loss = 0.23 (14.5 examples/sec; 0.552 sec/batch; 50h:59m:57s remains)
INFO - root - 2017-12-11 09:16:09.055261: step 140, loss = 0.34, batch loss = 0.25 (14.9 examples/sec; 0.539 sec/batch; 49h:43m:07s remains)
INFO - root - 2017-12-11 09:16:14.460120: step 150, loss = 0.35, batch loss = 0.27 (14.9 examples/sec; 0.539 sec/batch; 49h:43m:53s remains)
INFO - root - 2017-12-11 09:16:19.458429: step 160, loss = 0.29, batch loss = 0.20 (15.4 examples/sec; 0.519 sec/batch; 47h:56m:11s remains)
INFO - root - 2017-12-11 09:16:24.894180: step 170, loss = 0.35, batch loss = 0.27 (14.9 examples/sec; 0.537 sec/batch; 49h:36m:34s remains)
INFO - root - 2017-12-11 09:16:30.266214: step 180, loss = 0.25, batch loss = 0.17 (15.2 examples/sec; 0.525 sec/batch; 48h:30m:18s remains)
INFO - root - 2017-12-11 09:16:35.713663: step 190, loss = 0.31, batch loss = 0.23 (14.0 examples/sec; 0.571 sec/batch; 52h:40m:27s remains)
INFO - root - 2017-12-11 09:16:41.074827: step 200, loss = 0.28, batch loss = 0.19 (14.8 examples/sec; 0.541 sec/batch; 49h:58m:45s remains)
2017-12-11 09:16:41.691338: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.0687637 -6.76263 -6.4911671 -6.0813956 -5.4923491 -5.0437584 -4.7785358 -4.4504213 -4.2844133 -4.3793335 -4.5083995 -4.4769983 -4.4540148 -4.6001773 -4.624506][-7.1370564 -6.7740078 -6.4914212 -6.0458221 -5.2698822 -4.5929403 -4.233336 -3.8527513 -3.7127037 -3.8542812 -4.0191808 -4.0734043 -4.1528845 -4.3724041 -4.5627069][-5.9212928 -5.6716366 -5.4751759 -4.9726911 -3.9683802 -3.180984 -2.9693956 -2.8907635 -3.0769939 -3.325716 -3.4034953 -3.3728206 -3.3705339 -3.4882486 -3.6876068][-4.3907223 -4.3559079 -4.3798952 -3.8441892 -2.5522261 -1.5980406 -1.555017 -2.0039997 -2.7522223 -3.2022028 -3.1493323 -2.8620896 -2.46804 -2.2385118 -2.3501668][-3.4536769 -3.5159619 -3.6932254 -3.0849142 -1.4335725 -0.049695015 0.17590523 -0.64534259 -1.9521058 -2.6797547 -2.6100254 -2.1582532 -1.4244721 -0.93092608 -1.0983121][-2.6218309 -2.5270405 -2.6582942 -1.9429617 -0.011516094 1.8907323 2.5497437 1.4918971 -0.35188055 -1.4256887 -1.4399357 -0.92296553 -0.095277309 0.39141893 0.092519283][-1.7987399 -1.3293092 -1.2186637 -0.296659 1.858459 4.1582642 5.1742239 3.8068867 1.3541279 -0.13831043 -0.27844954 0.29474211 1.0059404 1.2183251 0.7352457][-1.3582556 -0.51646852 -0.1416235 1.0465961 3.4299688 5.9691839 7.1938295 5.5506639 2.5752907 0.61567068 0.15761185 0.65638971 1.2255507 1.2194986 0.686882][-1.8327746 -0.88177013 -0.42599583 0.78439951 3.0310011 5.3597527 6.5442133 5.13768 2.3690352 0.39461708 -0.29610682 0.0094232559 0.54389238 0.65074825 0.3929472][-2.6863203 -1.769186 -1.3265882 -0.3641758 1.3052812 2.9097314 3.7051287 2.8076181 0.8251338 -0.62844753 -1.2506404 -1.1449449 -0.5926795 -0.21157742 -0.076145649][-3.7770386 -2.8985605 -2.4291451 -1.6934953 -0.60713506 0.30862522 0.69475603 0.18483686 -1.0917776 -1.982434 -2.3636985 -2.3033197 -1.6794081 -1.0675392 -0.67957449][-5.0323377 -4.2314973 -3.7828526 -3.2562294 -2.6305757 -2.197289 -2.0208392 -2.2083967 -2.9321847 -3.3521595 -3.462213 -3.3682046 -2.7387185 -2.0520809 -1.6123464][-5.7247987 -5.0141363 -4.6620116 -4.39486 -4.1697621 -4.0946569 -4.0546813 -4.0719814 -4.4771338 -4.6487083 -4.573627 -4.4358168 -3.907037 -3.308722 -2.9047523][-6.49529 -5.9445591 -5.6648078 -5.5357175 -5.481617 -5.5382209 -5.5476713 -5.5080509 -5.7930555 -5.90166 -5.7463374 -5.51735 -5.0659423 -4.5885892 -4.2707314][-6.9080276 -6.5866776 -6.3617887 -6.2594643 -6.2106185 -6.2516422 -6.2430611 -6.159411 -6.3317814 -6.4469385 -6.3515315 -6.1591811 -5.8160462 -5.4413452 -5.1851492]]...]
INFO - root - 2017-12-11 09:16:47.075487: step 210, loss = 0.25, batch loss = 0.16 (14.7 examples/sec; 0.546 sec/batch; 50h:22m:58s remains)
INFO - root - 2017-12-11 09:16:52.495983: step 220, loss = 0.29, batch loss = 0.21 (14.3 examples/sec; 0.558 sec/batch; 51h:31m:39s remains)
INFO - root - 2017-12-11 09:16:57.826110: step 230, loss = 0.26, batch loss = 0.18 (15.3 examples/sec; 0.523 sec/batch; 48h:14m:56s remains)
INFO - root - 2017-12-11 09:17:03.201541: step 240, loss = 0.28, batch loss = 0.20 (14.6 examples/sec; 0.547 sec/batch; 50h:28m:34s remains)
INFO - root - 2017-12-11 09:17:08.546413: step 250, loss = 0.35, batch loss = 0.27 (15.0 examples/sec; 0.532 sec/batch; 49h:04m:32s remains)
INFO - root - 2017-12-11 09:17:13.683735: step 260, loss = 0.31, batch loss = 0.23 (14.2 examples/sec; 0.562 sec/batch; 51h:53m:29s remains)
INFO - root - 2017-12-11 09:17:19.112253: step 270, loss = 0.32, batch loss = 0.24 (14.4 examples/sec; 0.555 sec/batch; 51h:13m:57s remains)
INFO - root - 2017-12-11 09:17:24.511674: step 280, loss = 0.24, batch loss = 0.15 (15.1 examples/sec; 0.530 sec/batch; 48h:56m:38s remains)
INFO - root - 2017-12-11 09:17:29.903099: step 290, loss = 0.27, batch loss = 0.18 (15.0 examples/sec; 0.533 sec/batch; 49h:10m:42s remains)
INFO - root - 2017-12-11 09:17:35.361080: step 300, loss = 0.36, batch loss = 0.27 (15.0 examples/sec; 0.532 sec/batch; 49h:05m:57s remains)
2017-12-11 09:17:35.932913: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.0265069 0.36480522 -0.3714447 -0.5029273 -0.8806262 -1.3119328 -1.1047707 -0.77921939 -0.43629646 0.027412415 0.38766861 0.29688025 -0.627892 -1.3600631 -1.6911223][-0.38212585 -0.93705869 -1.3857615 -1.2013006 -1.3253863 -1.7017801 -1.6144676 -1.3709157 -1.0675023 -0.69470525 -0.58350444 -0.88832736 -1.6705844 -2.0394146 -2.0006766][-2.2084138 -2.4682531 -2.4173822 -1.7520874 -1.4764855 -1.7354975 -1.8490863 -1.8470995 -1.7789385 -1.6554012 -1.8243186 -2.201997 -2.6211741 -2.5086746 -2.0811849][-3.7840135 -3.7153151 -3.0840855 -1.9110944 -1.2407663 -1.3290627 -1.5291927 -1.6845298 -1.9236515 -2.1592789 -2.6397035 -3.0624266 -3.2014756 -2.7656665 -2.0523543][-4.3268614 -3.92995 -2.7599425 -1.1713517 -0.21777534 -0.16245222 -0.32217979 -0.48644352 -0.98967528 -1.5471961 -2.2797356 -2.8205605 -2.9353075 -2.4690485 -1.6752872][-3.7439947 -2.9791276 -1.425247 0.33525133 1.3333721 1.4490604 1.449914 1.461421 0.82358027 0.099266529 -0.72532487 -1.455493 -1.7778561 -1.4885926 -0.82654476][-2.8322515 -1.6454508 0.14251757 1.8359256 2.6773334 2.7713175 2.9799519 3.2633376 2.6421514 1.9900303 1.3493776 0.55493832 0.051448822 0.11769629 0.46434164][-2.0066085 -0.53530383 1.2733574 2.747716 3.3548436 3.4022403 3.7525167 4.263938 3.7879887 3.3811202 3.0754557 2.3501787 1.7644539 1.6169639 1.5656219][-1.5206101 -0.11618853 1.4705353 2.6604013 3.0414715 3.0505366 3.5006771 4.1788807 3.9534883 3.8389664 3.83465 3.25395 2.6426053 2.278697 1.8200831][-1.6570165 -0.557255 0.6545639 1.5096927 1.681838 1.6714916 2.21001 3.0218644 3.0903378 3.2035303 3.3599277 2.9254732 2.3396749 1.8153429 1.0893378][-1.8639984 -1.272728 -0.59990954 -0.2258091 -0.36864519 -0.44521642 0.11489964 0.99178982 1.2959909 1.5673952 1.758162 1.3642454 0.79964638 0.23323441 -0.55496645][-1.7537723 -1.7776082 -1.7558372 -2.009702 -2.7000141 -3.0666394 -2.6796544 -1.8602111 -1.3365905 -0.79507089 -0.41922045 -0.6645124 -1.0831454 -1.5189638 -2.1800141][-1.4520514 -1.826721 -2.1217546 -2.7177265 -3.7998173 -4.4251447 -4.3458414 -3.8122547 -3.2691784 -2.5426736 -1.9319329 -1.9837067 -2.2555327 -2.5756779 -3.0998414][-1.2319677 -1.7729392 -2.1306081 -2.7896566 -3.928823 -4.5675421 -4.6816068 -4.4474669 -3.965018 -3.1599898 -2.4063721 -2.3325706 -2.5462904 -2.8181434 -3.2518353][-1.4499369 -1.927448 -2.2150989 -2.812907 -3.8200769 -4.3651595 -4.5830326 -4.5202694 -4.0314045 -3.1750543 -2.3662019 -2.2227933 -2.381336 -2.5521915 -2.8153269]]...]
INFO - root - 2017-12-11 09:17:41.362655: step 310, loss = 0.30, batch loss = 0.22 (14.6 examples/sec; 0.549 sec/batch; 50h:39m:39s remains)
INFO - root - 2017-12-11 09:17:46.757442: step 320, loss = 0.24, batch loss = 0.16 (14.6 examples/sec; 0.547 sec/batch; 50h:29m:02s remains)
INFO - root - 2017-12-11 09:17:52.147110: step 330, loss = 0.29, batch loss = 0.21 (15.2 examples/sec; 0.528 sec/batch; 48h:43m:12s remains)
INFO - root - 2017-12-11 09:17:57.547277: step 340, loss = 0.25, batch loss = 0.16 (14.9 examples/sec; 0.538 sec/batch; 49h:36m:26s remains)
INFO - root - 2017-12-11 09:18:02.953842: step 350, loss = 0.27, batch loss = 0.19 (14.7 examples/sec; 0.544 sec/batch; 50h:08m:57s remains)
INFO - root - 2017-12-11 09:18:08.044973: step 360, loss = 0.29, batch loss = 0.21 (14.9 examples/sec; 0.537 sec/batch; 49h:33m:31s remains)
INFO - root - 2017-12-11 09:18:13.511190: step 370, loss = 0.32, batch loss = 0.24 (15.4 examples/sec; 0.518 sec/batch; 47h:46m:37s remains)
INFO - root - 2017-12-11 09:18:18.848029: step 380, loss = 0.26, batch loss = 0.18 (14.9 examples/sec; 0.537 sec/batch; 49h:32m:10s remains)
INFO - root - 2017-12-11 09:18:24.187244: step 390, loss = 0.24, batch loss = 0.16 (15.1 examples/sec; 0.529 sec/batch; 48h:47m:55s remains)
INFO - root - 2017-12-11 09:18:29.597330: step 400, loss = 0.24, batch loss = 0.15 (14.4 examples/sec; 0.557 sec/batch; 51h:24m:21s remains)
2017-12-11 09:18:30.175470: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1848073 -5.5689487 -5.6862226 -5.7792444 -6.1479626 -6.4151435 -6.1133914 -5.5928535 -5.3941965 -5.8241186 -6.4004841 -6.503211 -6.4436188 -6.8700171 -7.6499367][-5.577446 -5.9640789 -5.96475 -5.9305806 -6.2073455 -6.3336635 -5.8183336 -5.1051364 -4.901691 -5.5090408 -6.1829967 -6.2613773 -6.0501986 -6.3052616 -7.0426207][-5.5601196 -5.8889146 -5.6717634 -5.3920126 -5.4338741 -5.2589173 -4.4210882 -3.4578857 -3.2755551 -4.1356516 -5.0241809 -5.2792048 -5.1327887 -5.3507719 -6.0713573][-4.7978911 -4.9534373 -4.4687619 -3.9567561 -3.8316486 -3.5125656 -2.5649517 -1.4785564 -1.2883599 -2.2566025 -3.2250671 -3.5764363 -3.516216 -3.8075607 -4.6610765][-3.9586809 -3.8419666 -3.0316141 -2.2140462 -1.8515179 -1.4099281 -0.4995358 0.55207682 0.753592 -0.23081923 -1.2136223 -1.582237 -1.5706267 -1.9988794 -3.0986116][-3.4747875 -3.2788959 -2.4439695 -1.4963043 -0.88664246 -0.23195219 0.77535152 1.9254346 2.3036795 1.4137988 0.3622303 -0.1380024 -0.2567296 -0.84069371 -2.1327009][-2.7136974 -2.5395718 -1.9465086 -1.2744203 -0.89389372 -0.4582727 0.46645737 1.7858715 2.5831814 2.042439 1.0554514 0.4022069 0.047147274 -0.71247911 -2.089009][-1.8145521 -1.4703462 -1.0202181 -0.73437381 -0.92201567 -1.1771793 -0.72610092 0.52150917 1.6998763 1.6539216 0.91876221 0.18846607 -0.44429064 -1.41203 -2.8209708][-0.95029855 -0.25854874 0.25725126 0.25828791 -0.576736 -1.713841 -1.9969692 -1.1281946 0.17581129 0.54617167 0.1351738 -0.56294608 -1.3866105 -2.5055928 -3.8968682][-0.40254068 0.59634304 1.2694883 1.1547432 -0.17124414 -2.0750637 -3.0943887 -2.7295561 -1.513278 -0.89827037 -1.0567319 -1.7045496 -2.623836 -3.7845812 -5.0547223][-0.76176238 0.50062418 1.4795985 1.534019 0.091849327 -2.2009084 -3.760354 -3.8947351 -2.9569712 -2.3483016 -2.4482021 -3.1233253 -4.0987387 -5.2053795 -6.2489033][-1.8383327 -0.64417076 0.45296192 0.769114 -0.39096165 -2.5108891 -4.1774917 -4.6569271 -4.1016903 -3.6957862 -3.8533263 -4.5306363 -5.4619007 -6.4267421 -7.2035627][-3.3833771 -2.4832351 -1.489388 -1.0161507 -1.7468417 -3.3079283 -4.6304307 -5.1209292 -4.8334546 -4.6400433 -4.8570948 -5.4806352 -6.293025 -7.0704479 -7.6067591][-4.880549 -4.1858582 -3.2945011 -2.7317159 -3.074832 -4.0531125 -4.9309931 -5.2848334 -5.1555 -5.1161642 -5.3422642 -5.83883 -6.4562855 -7.0121517 -7.3398447][-5.6794338 -5.2357025 -4.5522337 -4.048996 -4.1560316 -4.6874361 -5.1767097 -5.3539009 -5.2804761 -5.2866044 -5.4418616 -5.7570896 -6.1414027 -6.4652719 -6.61131]]...]
INFO - root - 2017-12-11 09:18:35.473157: step 410, loss = 0.30, batch loss = 0.21 (15.1 examples/sec; 0.530 sec/batch; 48h:52m:38s remains)
INFO - root - 2017-12-11 09:18:40.937083: step 420, loss = 0.28, batch loss = 0.20 (14.6 examples/sec; 0.547 sec/batch; 50h:25m:51s remains)
INFO - root - 2017-12-11 09:18:46.423754: step 430, loss = 0.25, batch loss = 0.17 (14.2 examples/sec; 0.563 sec/batch; 51h:55m:04s remains)
INFO - root - 2017-12-11 09:18:51.809632: step 440, loss = 0.29, batch loss = 0.20 (14.4 examples/sec; 0.556 sec/batch; 51h:18m:58s remains)
INFO - root - 2017-12-11 09:18:57.172696: step 450, loss = 0.31, batch loss = 0.22 (14.4 examples/sec; 0.554 sec/batch; 51h:05m:57s remains)
INFO - root - 2017-12-11 09:19:02.283787: step 460, loss = 0.30, batch loss = 0.22 (14.9 examples/sec; 0.537 sec/batch; 49h:29m:15s remains)
INFO - root - 2017-12-11 09:19:07.729256: step 470, loss = 0.23, batch loss = 0.15 (14.6 examples/sec; 0.548 sec/batch; 50h:32m:05s remains)
INFO - root - 2017-12-11 09:19:13.159850: step 480, loss = 0.41, batch loss = 0.33 (14.9 examples/sec; 0.536 sec/batch; 49h:24m:59s remains)
INFO - root - 2017-12-11 09:19:18.572172: step 490, loss = 0.26, batch loss = 0.18 (15.0 examples/sec; 0.535 sec/batch; 49h:19m:40s remains)
INFO - root - 2017-12-11 09:19:23.942960: step 500, loss = 0.35, batch loss = 0.27 (15.1 examples/sec; 0.529 sec/batch; 48h:48m:26s remains)
2017-12-11 09:19:24.543906: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9860592 -3.2926235 -4.5845857 -6.3198662 -7.5880966 -7.9730606 -7.4781342 -6.3702888 -5.1918449 -4.5041637 -4.2602558 -3.9978716 -3.2602179 -2.4379621 -2.0704842][-2.9157915 -3.6013324 -5.1415863 -6.8535657 -7.6647873 -7.437398 -6.5229692 -5.3180175 -4.3637104 -4.1647487 -4.6093311 -4.7514114 -4.0863423 -3.2240264 -2.8129311][-3.2156365 -4.1144576 -5.6126418 -6.947238 -7.0354013 -6.072597 -4.6644244 -3.3253846 -2.783983 -3.3972702 -4.789782 -5.589139 -5.13887 -4.1804743 -3.54351][-3.3443408 -4.1933026 -5.4391532 -6.2472849 -5.5672588 -3.8887167 -1.945261 -0.43225455 -0.42321634 -2.0132723 -4.3575687 -5.8712387 -5.8560848 -4.9229755 -3.9606533][-3.4568448 -4.3407421 -5.3724027 -5.6033382 -4.0703216 -1.5312922 1.2562976 3.3680024 3.1873055 0.794919 -2.4360495 -4.8233652 -5.6422129 -5.1263123 -4.0555124][-3.6869156 -4.642828 -5.4698191 -5.1084089 -2.716373 0.767128 4.5558968 7.5719242 7.5701742 4.6082582 0.54789066 -2.8244252 -4.6823397 -4.8547816 -3.9134614][-4.0788994 -4.978528 -5.5813169 -4.8409767 -2.0066259 2.0288577 6.4096975 10.02094 10.325219 7.2133064 2.767272 -1.2130439 -3.8033414 -4.5403848 -3.7900133][-4.3876505 -5.1395187 -5.6192865 -4.9522128 -2.4430494 1.320756 5.4420805 8.8739748 9.4521666 6.8846817 2.8959475 -0.89104629 -3.5515709 -4.4595551 -3.812115][-4.4738579 -5.0391779 -5.5543342 -5.3681669 -3.824667 -1.0691679 2.1007853 4.7693148 5.4976959 3.8851309 0.94607449 -2.0586922 -4.2300811 -4.8897181 -4.2339454][-4.1460967 -4.5907869 -5.2641706 -5.7100077 -5.3160462 -3.7970026 -1.6794999 0.24460554 1.0981126 0.41990137 -1.4121163 -3.505934 -4.9862208 -5.225709 -4.5464821][-3.6087294 -4.0326056 -4.83812 -5.6780157 -6.0179257 -5.3548951 -3.9367638 -2.5058491 -1.6667488 -1.8280754 -2.9487624 -4.3641429 -5.2574234 -5.1492763 -4.485486][-3.1023424 -3.6686349 -4.603178 -5.5810184 -6.1825981 -5.9067354 -4.82765 -3.6708307 -2.9551907 -2.9990628 -3.8683369 -4.9289565 -5.4627295 -5.1866064 -4.5664692][-2.291971 -2.9834013 -4.0147014 -5.0239515 -5.7121954 -5.7016239 -4.9673085 -4.0922513 -3.5522728 -3.6429327 -4.4353156 -5.2732987 -5.5731125 -5.2501864 -4.685246][-1.591274 -2.21513 -3.1043432 -3.9204195 -4.5050769 -4.6683135 -4.319767 -3.8273606 -3.6149974 -3.9689329 -4.8383031 -5.563694 -5.7287288 -5.4002309 -4.8828039][-1.2064939 -1.6185553 -2.1958127 -2.687763 -3.0443158 -3.239769 -3.1799524 -3.0583229 -3.2501259 -3.9729741 -4.995038 -5.72716 -5.9490852 -5.7442131 -5.3281651]]...]
INFO - root - 2017-12-11 09:19:29.952517: step 510, loss = 0.32, batch loss = 0.24 (15.0 examples/sec; 0.534 sec/batch; 49h:16m:38s remains)
INFO - root - 2017-12-11 09:19:35.320109: step 520, loss = 0.29, batch loss = 0.20 (14.8 examples/sec; 0.539 sec/batch; 49h:43m:39s remains)
INFO - root - 2017-12-11 09:19:40.705355: step 530, loss = 0.35, batch loss = 0.27 (15.0 examples/sec; 0.534 sec/batch; 49h:15m:51s remains)
INFO - root - 2017-12-11 09:19:46.086506: step 540, loss = 0.31, batch loss = 0.22 (14.8 examples/sec; 0.539 sec/batch; 49h:43m:41s remains)
INFO - root - 2017-12-11 09:19:51.499292: step 550, loss = 0.28, batch loss = 0.20 (14.4 examples/sec; 0.555 sec/batch; 51h:08m:00s remains)
INFO - root - 2017-12-11 09:19:56.526491: step 560, loss = 0.27, batch loss = 0.18 (15.3 examples/sec; 0.523 sec/batch; 48h:15m:54s remains)
INFO - root - 2017-12-11 09:20:01.903591: step 570, loss = 0.29, batch loss = 0.21 (14.9 examples/sec; 0.536 sec/batch; 49h:25m:00s remains)
INFO - root - 2017-12-11 09:20:07.307344: step 580, loss = 0.27, batch loss = 0.19 (14.6 examples/sec; 0.547 sec/batch; 50h:23m:52s remains)
INFO - root - 2017-12-11 09:20:12.669844: step 590, loss = 0.33, batch loss = 0.25 (15.1 examples/sec; 0.530 sec/batch; 48h:52m:27s remains)
INFO - root - 2017-12-11 09:20:17.948895: step 600, loss = 0.23, batch loss = 0.15 (15.1 examples/sec; 0.530 sec/batch; 48h:50m:58s remains)
2017-12-11 09:20:18.568706: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8422494 -3.392262 -2.8578606 -2.5225506 -2.73179 -2.8563445 -3.0945392 -3.9000952 -4.8877988 -5.1596408 -4.7997155 -3.8437746 -2.4698215 -1.3430004 -0.84838843][-4.8424554 -4.68938 -4.4192829 -4.2491212 -4.4270391 -4.2764506 -4.27582 -4.8761644 -5.6319389 -5.7296214 -5.1791406 -4.3476238 -3.312664 -2.4003804 -1.9640126][-5.1057787 -5.1661625 -5.0838313 -5.0900049 -5.2567692 -4.7857137 -4.5541639 -5.0624204 -5.6813197 -5.7104349 -5.1223259 -4.4078035 -3.5786955 -2.7674885 -2.3859754][-4.4985266 -4.6049566 -4.5717716 -4.621026 -4.5795813 -3.5708368 -2.9945064 -3.4745674 -4.0900526 -4.2644811 -3.9626682 -3.5714903 -3.0587392 -2.39407 -2.0776355][-3.5974019 -3.7753906 -3.8579557 -3.8771877 -3.4515653 -1.7409279 -0.6455729 -0.99320722 -1.6095362 -2.0440722 -2.1936042 -2.1854377 -2.040302 -1.6050813 -1.3809676][-2.3236859 -2.6734242 -2.916687 -2.7806587 -1.9226747 0.28594303 1.6623378 1.2708735 0.49825382 -0.24985552 -0.80224848 -0.93699455 -0.88755655 -0.548712 -0.39675903][-1.0534232 -1.5983891 -1.9202604 -1.3916938 -0.003631115 2.492909 3.8753052 3.2027788 2.084125 0.961411 0.029589176 -0.070183277 0.14058113 0.53489447 0.61607885][-0.64738655 -1.4045081 -1.8745902 -1.0501447 0.71914577 3.3228016 4.6665239 3.8265295 2.5562158 1.360836 0.35456657 0.46738482 0.92590809 1.2736597 1.1262469][-1.0103631 -1.7772408 -2.3174975 -1.3699329 0.45921183 2.8248367 3.9648066 3.0475483 1.8628769 0.91488886 0.14652014 0.57136869 1.2759695 1.5305452 1.0580425][-1.6783013 -2.2740781 -2.7796059 -1.9488342 -0.40571213 1.441258 2.2279205 1.2831397 0.23382235 -0.45485282 -0.95465612 -0.26870441 0.7028904 1.0655522 0.51215887][-2.169416 -2.6400371 -3.2040458 -2.8222079 -1.916898 -0.76095176 -0.33239651 -1.1290658 -1.9072111 -2.2915697 -2.500618 -1.6740468 -0.60526204 -0.20525503 -0.72588992][-2.4276965 -2.8385968 -3.4604297 -3.52355 -3.2528849 -2.7678061 -2.7056313 -3.3749797 -3.926719 -4.0938911 -4.0726962 -3.2125878 -2.1229472 -1.7222013 -2.1396635][-2.5996995 -2.9174724 -3.4378629 -3.6852498 -3.8161643 -3.887557 -4.2162366 -4.8878155 -5.3709636 -5.5299406 -5.4970837 -4.7440405 -3.6580179 -3.1968677 -3.3491349][-2.8800683 -3.1245487 -3.4338708 -3.6401968 -3.9351244 -4.3494411 -4.91047 -5.5056353 -5.8458633 -5.9595127 -5.9466782 -5.3408194 -4.3848381 -4.05101 -4.1103973][-3.4088976 -3.6173756 -3.6354289 -3.6155744 -3.837378 -4.2884426 -4.8138394 -5.1993809 -5.3361354 -5.3558903 -5.321907 -4.8044329 -3.9596395 -3.8072834 -3.9160523]]...]
INFO - root - 2017-12-11 09:20:24.015944: step 610, loss = 0.28, batch loss = 0.20 (13.8 examples/sec; 0.579 sec/batch; 53h:24m:22s remains)
INFO - root - 2017-12-11 09:20:29.459422: step 620, loss = 0.25, batch loss = 0.16 (14.6 examples/sec; 0.548 sec/batch; 50h:29m:33s remains)
INFO - root - 2017-12-11 09:20:34.815815: step 630, loss = 0.25, batch loss = 0.17 (15.5 examples/sec; 0.517 sec/batch; 47h:38m:43s remains)
INFO - root - 2017-12-11 09:20:40.135292: step 640, loss = 0.33, batch loss = 0.25 (15.6 examples/sec; 0.514 sec/batch; 47h:23m:37s remains)
INFO - root - 2017-12-11 09:20:45.479130: step 650, loss = 0.30, batch loss = 0.22 (14.7 examples/sec; 0.546 sec/batch; 50h:19m:13s remains)
INFO - root - 2017-12-11 09:20:50.596085: step 660, loss = 0.26, batch loss = 0.17 (14.9 examples/sec; 0.536 sec/batch; 49h:26m:07s remains)
INFO - root - 2017-12-11 09:20:56.009519: step 670, loss = 0.31, batch loss = 0.23 (15.0 examples/sec; 0.534 sec/batch; 49h:11m:30s remains)
INFO - root - 2017-12-11 09:21:01.375103: step 680, loss = 0.36, batch loss = 0.28 (14.8 examples/sec; 0.542 sec/batch; 49h:59m:20s remains)
INFO - root - 2017-12-11 09:21:06.729860: step 690, loss = 0.32, batch loss = 0.24 (15.1 examples/sec; 0.529 sec/batch; 48h:43m:14s remains)
INFO - root - 2017-12-11 09:21:12.115954: step 700, loss = 0.27, batch loss = 0.18 (14.9 examples/sec; 0.536 sec/batch; 49h:24m:34s remains)
2017-12-11 09:21:12.702697: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5661509 -2.6032493 -2.6732225 -2.820529 -3.2434902 -3.7934718 -4.206737 -4.3262644 -4.0371428 -3.4759398 -2.8268673 -2.3273058 -2.1512766 -2.1536474 -2.1560569][-2.8911726 -2.9288921 -2.9848433 -3.0636878 -3.3828244 -3.8986726 -4.4278607 -4.7885752 -4.7896633 -4.4388819 -3.8266263 -3.182796 -2.7501836 -2.5074606 -2.3531222][-3.0924292 -3.0783038 -3.1304445 -3.2043362 -3.4210761 -3.7438526 -4.1372004 -4.5285783 -4.8261962 -4.9033122 -4.6220303 -4.1100349 -3.5665684 -3.0972061 -2.738235][-3.0852921 -2.8758759 -2.8698311 -3.0366502 -3.2677712 -3.412169 -3.5600071 -3.7849846 -4.2737861 -4.8214703 -5.0099225 -4.8465633 -4.3869991 -3.8004136 -3.2535565][-2.9554374 -2.4782956 -2.311079 -2.4339032 -2.5017085 -2.3036678 -2.0807195 -2.0271659 -2.6790376 -3.7795818 -4.6119471 -5.0510459 -4.9452057 -4.4371896 -3.7633905][-2.7364428 -1.9633791 -1.5148332 -1.391849 -0.99953437 -0.31033897 0.27266932 0.55247641 -0.29021549 -1.95561 -3.5014286 -4.6518688 -5.0884933 -4.84893 -4.14038][-2.7355814 -1.7524021 -0.95846772 -0.44988728 0.57002354 1.7929697 2.6550407 3.0689363 2.0479746 -0.039147854 -2.1549854 -3.8830318 -4.8530235 -4.9857159 -4.3225379][-3.104512 -2.1546307 -1.1694231 -0.36020851 1.2364368 3.0178523 4.2050667 4.799777 3.7539816 1.4875946 -0.94864726 -3.0231667 -4.3941541 -4.8924246 -4.3096704][-3.5427141 -2.9253078 -2.1215758 -1.3514042 0.49429989 2.7120848 4.3501482 5.32191 4.5676155 2.476665 -0.0050091743 -2.1938851 -3.7788424 -4.5569153 -4.0822215][-3.7721572 -3.6946692 -3.4082975 -2.9953775 -1.2573235 1.1464462 3.2137232 4.6395321 4.4058218 2.7752638 0.4881115 -1.6375856 -3.2677355 -4.1693978 -3.7782583][-3.6452961 -3.9835508 -4.1804852 -4.1798134 -2.8387604 -0.68021917 1.4715495 3.1447587 3.395051 2.34133 0.42370844 -1.5013645 -3.0388346 -3.9131043 -3.5785069][-3.3090982 -3.874197 -4.4076114 -4.7529154 -3.95354 -2.3354745 -0.42376375 1.2864051 1.9059267 1.4239783 -0.0085802078 -1.6465209 -3.0272598 -3.8113952 -3.5370331][-2.8806849 -3.496397 -4.2183471 -4.8405209 -4.6387239 -3.7166464 -2.2586896 -0.66349983 0.23346424 0.28170443 -0.58933616 -1.8501515 -3.023406 -3.7097166 -3.5340357][-2.3946462 -2.8843365 -3.5868115 -4.3095646 -4.59979 -4.3899426 -3.5399117 -2.2422743 -1.2259884 -0.77116895 -1.103075 -1.9464827 -2.8979161 -3.5157824 -3.466043][-2.0357602 -2.3129332 -2.8172288 -3.4298153 -3.9652958 -4.3171577 -4.1057982 -3.2610583 -2.3234832 -1.6283722 -1.5147135 -1.9398093 -2.6614912 -3.2296939 -3.2962294]]...]
INFO - root - 2017-12-11 09:21:17.981255: step 710, loss = 0.31, batch loss = 0.23 (14.9 examples/sec; 0.536 sec/batch; 49h:24m:59s remains)
INFO - root - 2017-12-11 09:21:23.340843: step 720, loss = 0.26, batch loss = 0.17 (15.5 examples/sec; 0.516 sec/batch; 47h:30m:49s remains)
INFO - root - 2017-12-11 09:21:28.721552: step 730, loss = 0.30, batch loss = 0.22 (14.5 examples/sec; 0.553 sec/batch; 50h:58m:23s remains)
INFO - root - 2017-12-11 09:21:34.145621: step 740, loss = 0.37, batch loss = 0.29 (14.7 examples/sec; 0.545 sec/batch; 50h:12m:11s remains)
INFO - root - 2017-12-11 09:21:39.555727: step 750, loss = 0.30, batch loss = 0.22 (14.9 examples/sec; 0.536 sec/batch; 49h:22m:40s remains)
INFO - root - 2017-12-11 09:21:44.656217: step 760, loss = 0.28, batch loss = 0.19 (15.2 examples/sec; 0.527 sec/batch; 48h:31m:34s remains)
INFO - root - 2017-12-11 09:21:50.032102: step 770, loss = 0.27, batch loss = 0.18 (14.6 examples/sec; 0.548 sec/batch; 50h:29m:59s remains)
INFO - root - 2017-12-11 09:21:55.436736: step 780, loss = 0.34, batch loss = 0.25 (15.0 examples/sec; 0.535 sec/batch; 49h:18m:18s remains)
INFO - root - 2017-12-11 09:22:00.854009: step 790, loss = 0.35, batch loss = 0.26 (15.4 examples/sec; 0.520 sec/batch; 47h:54m:04s remains)
INFO - root - 2017-12-11 09:22:06.239859: step 800, loss = 0.25, batch loss = 0.17 (14.6 examples/sec; 0.550 sec/batch; 50h:39m:04s remains)
2017-12-11 09:22:06.907651: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.597847 -3.2399492 -2.9075565 -2.420712 -1.5273724 -0.4381032 0.23029089 0.14586687 -0.3872242 -0.97815943 -1.6155684 -2.41295 -3.3457675 -3.9221637 -3.7225635][-4.2929335 -3.9638968 -3.6117249 -3.1223674 -2.2271116 -1.0968649 -0.31689358 -0.20428228 -0.55066776 -0.99262547 -1.5239823 -2.1615977 -2.8772454 -3.2806766 -2.9284191][-4.6193213 -4.244101 -3.8513489 -3.3878646 -2.5836034 -1.5408695 -0.7618072 -0.54135919 -0.78611994 -1.1785879 -1.6200042 -2.0515039 -2.5083141 -2.7219267 -2.2265975][-4.0836773 -3.5267849 -2.9744763 -2.4790287 -1.8464105 -1.0942581 -0.57751679 -0.56179953 -1.048012 -1.724256 -2.2519951 -2.5456171 -2.8195176 -2.9003961 -2.2919965][-3.0517638 -2.3457589 -1.5897319 -0.92500782 -0.256732 0.42972755 0.85167265 0.63578033 -0.27225733 -1.4628005 -2.2510378 -2.5364883 -2.7390406 -2.7362137 -2.080678][-2.0726955 -1.1729181 -0.15672874 0.77718449 1.6549129 2.4944763 3.0104423 2.5893421 1.1486993 -0.70579195 -1.9538543 -2.4708645 -2.8251984 -2.878161 -2.2219636][-1.3155837 -0.31992388 0.87410545 2.0396914 3.1395283 4.2029514 4.9598227 4.5121346 2.6908693 0.2686162 -1.4175999 -2.2099495 -2.7478971 -2.8072529 -2.006284][-1.301158 -0.39148712 0.8350625 2.1698174 3.5359964 4.9285355 6.0907736 5.9133492 4.1005135 1.5007677 -0.32661963 -1.1847005 -1.7386491 -1.6905646 -0.679198][-1.8173318 -1.0053067 0.14397478 1.4204278 2.7109356 4.004734 5.13457 5.0805941 3.5208807 1.2016902 -0.38361597 -1.065289 -1.4890826 -1.2522814 0.027280807][-2.7461138 -2.1414154 -1.2227249 -0.18804407 0.84404469 1.854342 2.7663507 2.8339748 1.7717752 0.084308624 -1.0342209 -1.4327834 -1.6608427 -1.2514343 0.18386745][-4.250246 -3.8565626 -3.1956897 -2.3962491 -1.5564656 -0.72649384 0.033284187 0.26326942 -0.28633785 -1.3666701 -2.1091864 -2.3038845 -2.3339775 -1.7881863 -0.30062628][-5.8510652 -5.6180177 -5.235219 -4.7332158 -4.1604691 -3.5778332 -3.0441103 -2.7848003 -3.0077381 -3.6668487 -4.1923633 -4.3078637 -4.2057834 -3.5723922 -2.1044052][-6.7439957 -6.6254082 -6.4648685 -6.2243042 -5.8937221 -5.5187039 -5.1773357 -4.9634686 -5.0262027 -5.4269753 -5.8287039 -5.953928 -5.8331652 -5.2444563 -3.9737287][-6.6454763 -6.5870543 -6.548018 -6.4525223 -6.2663522 -6.0277243 -5.835681 -5.7258091 -5.772428 -6.0544653 -6.3887844 -6.5612292 -6.5261869 -6.1409931 -5.2698278][-5.8220425 -5.8081064 -5.8387542 -5.8366857 -5.7783022 -5.6945591 -5.6548166 -5.6593385 -5.7252517 -5.9217615 -6.172904 -6.363986 -6.4460392 -6.3141146 -5.87409]]...]
INFO - root - 2017-12-11 09:22:12.263035: step 810, loss = 0.28, batch loss = 0.20 (15.1 examples/sec; 0.530 sec/batch; 48h:51m:02s remains)
INFO - root - 2017-12-11 09:22:17.598610: step 820, loss = 0.26, batch loss = 0.18 (15.1 examples/sec; 0.528 sec/batch; 48h:40m:39s remains)
INFO - root - 2017-12-11 09:22:22.921196: step 830, loss = 0.38, batch loss = 0.29 (15.2 examples/sec; 0.527 sec/batch; 48h:34m:40s remains)
INFO - root - 2017-12-11 09:22:28.272444: step 840, loss = 0.26, batch loss = 0.18 (14.6 examples/sec; 0.549 sec/batch; 50h:34m:45s remains)
INFO - root - 2017-12-11 09:22:33.597115: step 850, loss = 0.29, batch loss = 0.20 (14.6 examples/sec; 0.548 sec/batch; 50h:26m:45s remains)
INFO - root - 2017-12-11 09:22:38.697239: step 860, loss = 0.38, batch loss = 0.30 (14.5 examples/sec; 0.551 sec/batch; 50h:44m:24s remains)
INFO - root - 2017-12-11 09:22:44.141347: step 870, loss = 0.28, batch loss = 0.20 (15.0 examples/sec; 0.532 sec/batch; 49h:01m:16s remains)
INFO - root - 2017-12-11 09:22:49.501674: step 880, loss = 0.39, batch loss = 0.30 (14.2 examples/sec; 0.563 sec/batch; 51h:49m:13s remains)
INFO - root - 2017-12-11 09:22:54.880500: step 890, loss = 0.37, batch loss = 0.29 (15.0 examples/sec; 0.533 sec/batch; 49h:05m:37s remains)
INFO - root - 2017-12-11 09:23:00.230835: step 900, loss = 0.24, batch loss = 0.16 (15.5 examples/sec; 0.517 sec/batch; 47h:35m:16s remains)
2017-12-11 09:23:00.796294: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2075272 -1.5082192 -1.805208 -1.9472551 -2.5160303 -3.6431861 -4.7195692 -5.4271126 -5.7335572 -5.9824333 -5.6465988 -4.6489506 -3.3535061 -2.772439 -2.8809419][-0.51570725 -2.1191971 -3.0807908 -3.4504421 -3.7308712 -4.3442411 -4.9545431 -5.3015919 -5.58031 -5.7524939 -5.2263708 -4.0051756 -2.4802439 -1.9338918 -2.1611969][-0.075489521 -2.6957502 -4.16579 -4.5604277 -4.3257303 -4.2468548 -4.3713837 -4.3806634 -4.7053919 -5.1654029 -4.9420214 -4.0360622 -2.6825778 -2.2634776 -2.6132181][-0.66064811 -3.6139786 -5.1322279 -5.0854335 -3.9178915 -2.6918809 -1.977572 -1.5837615 -2.038403 -3.064388 -3.510704 -3.3008232 -2.4687331 -2.1554794 -2.5480623][-2.1774397 -4.5782905 -5.42903 -4.4491315 -2.1652517 0.34998512 1.976994 2.525352 1.6456571 -0.18959761 -1.5161586 -2.1546466 -2.0015719 -1.6933436 -1.9601977][-3.9833527 -5.2573714 -5.0147038 -2.9172692 0.3230114 3.8924961 6.219676 6.527051 4.9687834 2.3590965 0.34861755 -0.8995676 -1.2157521 -0.74976659 -0.80741382][-5.331975 -5.3969684 -4.0971284 -1.1776085 2.5865369 6.7467327 9.5132332 9.3949633 7.0703564 3.8301229 1.4519444 -0.0376153 -0.47501588 0.3060441 0.61428881][-6.0967069 -5.4141107 -3.6116405 -0.60616946 2.8992558 6.875432 9.6181879 9.1393814 6.2687206 2.6959939 0.22438288 -1.2445683 -1.5473034 -0.38213587 0.44764853][-6.7426367 -6.1318932 -4.5509472 -2.0740795 0.738431 4.0728731 6.5602188 6.1148682 3.3051944 -0.033262253 -2.1726477 -3.4160337 -3.600405 -2.323997 -1.1243887][-7.0901361 -6.96027 -5.8938231 -4.057106 -1.8791721 0.75589228 2.8345108 2.5258875 0.072796345 -2.7141998 -4.2149773 -5.0156789 -5.0635233 -3.8921371 -2.5715022][-7.2450933 -7.5226674 -6.9179096 -5.6735773 -4.1300507 -2.294946 -0.84541631 -1.0716972 -3.0024481 -5.0478125 -5.7822113 -6.0071349 -5.7889485 -4.6537032 -3.3766594][-7.0715494 -7.3258 -6.8714314 -6.0354233 -5.0992465 -4.1538563 -3.5261352 -3.8718784 -5.2626352 -6.5086823 -6.5635624 -6.3166418 -5.848403 -4.686058 -3.4943528][-6.7879143 -6.6822066 -6.1634731 -5.5474615 -5.0532851 -4.7611179 -4.7854791 -5.3181725 -6.3714495 -7.0946054 -6.8184156 -6.3422661 -5.6799541 -4.4477139 -3.2381575][-6.9349494 -6.5308304 -5.9703851 -5.4701819 -5.1513934 -5.0713673 -5.2722836 -5.7633319 -6.5765762 -7.0769386 -6.8340831 -6.3831487 -5.610765 -4.3008676 -2.9840565][-7.14522 -6.7081375 -6.2224979 -5.8376389 -5.549418 -5.3994274 -5.4300919 -5.6177869 -6.1495275 -6.5010853 -6.3530269 -6.0094266 -5.2130928 -3.939585 -2.6149158]]...]
INFO - root - 2017-12-11 09:23:06.096510: step 910, loss = 0.33, batch loss = 0.25 (14.8 examples/sec; 0.541 sec/batch; 49h:47m:33s remains)
INFO - root - 2017-12-11 09:23:11.468182: step 920, loss = 0.41, batch loss = 0.32 (15.8 examples/sec; 0.507 sec/batch; 46h:40m:27s remains)
INFO - root - 2017-12-11 09:23:16.828230: step 930, loss = 0.26, batch loss = 0.17 (14.8 examples/sec; 0.542 sec/batch; 49h:55m:24s remains)
INFO - root - 2017-12-11 09:23:22.172488: step 940, loss = 0.26, batch loss = 0.17 (14.9 examples/sec; 0.538 sec/batch; 49h:35m:28s remains)
INFO - root - 2017-12-11 09:23:27.543507: step 950, loss = 0.27, batch loss = 0.19 (15.0 examples/sec; 0.535 sec/batch; 49h:16m:17s remains)
INFO - root - 2017-12-11 09:23:32.640689: step 960, loss = 0.34, batch loss = 0.25 (22.3 examples/sec; 0.359 sec/batch; 33h:03m:46s remains)
INFO - root - 2017-12-11 09:23:38.089472: step 970, loss = 0.26, batch loss = 0.18 (14.6 examples/sec; 0.549 sec/batch; 50h:35m:36s remains)
INFO - root - 2017-12-11 09:23:43.445067: step 980, loss = 0.27, batch loss = 0.18 (14.9 examples/sec; 0.536 sec/batch; 49h:18m:51s remains)
INFO - root - 2017-12-11 09:23:48.934508: step 990, loss = 0.26, batch loss = 0.18 (14.1 examples/sec; 0.567 sec/batch; 52h:12m:06s remains)
INFO - root - 2017-12-11 09:23:54.403086: step 1000, loss = 0.34, batch loss = 0.25 (14.9 examples/sec; 0.538 sec/batch; 49h:32m:23s remains)
2017-12-11 09:23:54.967232: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.751246 -3.5102544 -4.17587 -4.9343576 -4.99601 -3.8817806 -3.2896543 -3.690239 -4.6894126 -5.8328285 -7.245317 -8.2342529 -7.7840338 -6.4813428 -5.1548567][-1.3443117 -2.3648221 -3.2751579 -4.2632737 -4.4282084 -3.1431355 -2.3363814 -2.7026587 -3.8405483 -5.1429038 -6.6958704 -7.8508015 -7.6169386 -6.5667868 -5.3873167][-0.59502816 -1.9225481 -3.081089 -4.1598382 -4.232265 -2.6289065 -1.3759899 -1.4500797 -2.5316148 -3.959996 -5.7082119 -7.093442 -7.1616545 -6.4471459 -5.4588809][-0.41179466 -1.8628571 -2.9939163 -3.7693279 -3.3301 -1.1371281 0.7048769 0.90155458 -0.32350588 -2.1664734 -4.3186584 -6.0355873 -6.46438 -6.119195 -5.3637691][-1.1276197 -2.3574448 -3.1012802 -3.2064347 -1.926461 1.0477705 3.5530586 3.9979763 2.4332542 -0.12059021 -2.8878078 -5.0384068 -5.8892069 -5.9016809 -5.3478436][-2.3616219 -3.0843029 -3.1396432 -2.330941 -0.12293673 3.5641737 6.6303186 7.2480307 5.2595205 1.9015751 -1.5529084 -4.1985688 -5.507679 -5.8560629 -5.4681969][-3.727488 -3.9132187 -3.3037047 -1.7032049 1.1413651 5.1384869 8.412487 9.0973768 6.8667088 3.0699816 -0.75130939 -3.7037075 -5.3300219 -5.8907728 -5.6134858][-5.0248365 -4.8371773 -3.9342625 -2.1347513 0.62369585 4.2459822 7.2495022 7.9691954 5.9715796 2.5005622 -1.0132022 -3.8070791 -5.4105711 -5.9642258 -5.6994915][-5.087245 -4.7148895 -3.9002967 -2.5538201 -0.65026736 1.9087772 4.1728411 4.8174782 3.3201289 0.66366482 -2.0722909 -4.3301945 -5.6162844 -5.9859762 -5.6700115][-4.2119317 -3.8335228 -3.3129835 -2.6960049 -1.917321 -0.63817644 0.71528387 1.1979713 0.20375967 -1.5696013 -3.4479616 -5.0631 -5.9113841 -6.0145292 -5.6051607][-3.5195498 -3.2332461 -3.0043659 -3.003284 -3.132344 -2.8912024 -2.3048408 -2.0059378 -2.6300097 -3.7171636 -4.92203 -6.0074196 -6.4652863 -6.3016672 -5.745924][-2.9084535 -2.7412989 -2.7063463 -3.0731943 -3.728471 -4.1288867 -4.0810995 -4.0315742 -4.5871162 -5.3806467 -6.2443428 -7.0073113 -7.1950378 -6.8078384 -6.0642691][-3.1043224 -3.1164637 -3.2350008 -3.7320786 -4.4941773 -5.064539 -5.2306557 -5.37902 -6.028595 -6.7950525 -7.5361571 -8.0794926 -8.0091953 -7.3780308 -6.4187255][-4.1960754 -4.3550229 -4.53287 -4.9579129 -5.5347805 -5.9468365 -6.0844488 -6.3182225 -7.0307951 -7.8134742 -8.48521 -8.8359451 -8.5210991 -7.6703272 -6.5514994][-5.1512566 -5.361867 -5.5359311 -5.8159318 -6.1294651 -6.2856684 -6.2826939 -6.4591575 -7.0837669 -7.794693 -8.3916349 -8.6317921 -8.2291 -7.3459921 -6.2650685]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-shortcut
INFO - root - 2017-12-11 09:24:00.284017: step 1010, loss = 0.31, batch loss = 0.23 (14.6 examples/sec; 0.547 sec/batch; 50h:22m:02s remains)
INFO - root - 2017-12-11 09:24:05.648570: step 1020, loss = 0.24, batch loss = 0.16 (15.1 examples/sec; 0.529 sec/batch; 48h:43m:55s remains)
INFO - root - 2017-12-11 09:24:11.046648: step 1030, loss = 0.33, batch loss = 0.25 (15.0 examples/sec; 0.533 sec/batch; 49h:03m:35s remains)
INFO - root - 2017-12-11 09:24:16.441596: step 1040, loss = 0.28, batch loss = 0.20 (14.8 examples/sec; 0.540 sec/batch; 49h:42m:11s remains)
INFO - root - 2017-12-11 09:24:21.819202: step 1050, loss = 0.31, batch loss = 0.23 (15.1 examples/sec; 0.531 sec/batch; 48h:55m:12s remains)
INFO - root - 2017-12-11 09:24:27.044444: step 1060, loss = 0.25, batch loss = 0.17 (20.3 examples/sec; 0.394 sec/batch; 36h:18m:42s remains)
INFO - root - 2017-12-11 09:24:32.343805: step 1070, loss = 0.28, batch loss = 0.19 (15.1 examples/sec; 0.529 sec/batch; 48h:43m:23s remains)
INFO - root - 2017-12-11 09:24:37.766938: step 1080, loss = 0.29, batch loss = 0.21 (14.6 examples/sec; 0.547 sec/batch; 50h:21m:48s remains)
INFO - root - 2017-12-11 09:24:43.211738: step 1090, loss = 0.35, batch loss = 0.26 (14.5 examples/sec; 0.550 sec/batch; 50h:38m:21s remains)
INFO - root - 2017-12-11 09:24:48.644294: step 1100, loss = 0.25, batch loss = 0.16 (14.4 examples/sec; 0.557 sec/batch; 51h:18m:58s remains)
2017-12-11 09:24:49.161688: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1685679 -1.5694704 -1.8605003 -1.8814461 -1.9343314 -1.9932268 -2.0207946 -2.04714 -2.0842521 -2.1514597 -2.2762845 -2.4355497 -2.5843439 -2.7199256 -2.7307949][-1.7257535 -2.1808081 -2.5056357 -2.5652053 -2.6396184 -2.7403789 -2.8192828 -2.8745804 -2.9122579 -2.9557428 -3.0696177 -3.2368984 -3.4096012 -3.5521164 -3.5504184][-2.5077224 -3.00433 -3.359808 -3.4551549 -3.5387111 -3.6670434 -3.7847762 -3.8474545 -3.8341532 -3.7555866 -3.739938 -3.8311918 -3.9906213 -4.133069 -4.1135139][-3.3161278 -3.7047009 -3.9577217 -3.9829154 -4.0011435 -4.1060023 -4.234376 -4.2964692 -4.2237854 -4.0040979 -3.8198607 -3.7967279 -3.9190972 -4.0798392 -4.1199803][-3.8442657 -4.0216336 -4.0975938 -4.0071464 -3.9075313 -3.8654697 -3.8327568 -3.7828243 -3.6816931 -3.5034871 -3.3841114 -3.4145718 -3.5590425 -3.73066 -3.8088078][-3.9492915 -3.9202895 -3.8722012 -3.7537153 -3.5808282 -3.3099885 -2.9048386 -2.5065267 -2.2612708 -2.1957071 -2.3191857 -2.5497971 -2.788702 -3.0174122 -3.1422088][-3.484688 -3.1946826 -3.0160189 -2.9247069 -2.7755992 -2.355886 -1.6109393 -0.80401587 -0.31620026 -0.32975483 -0.71625972 -1.1608353 -1.5382326 -1.9369676 -2.2362666][-2.4750633 -1.9727318 -1.700206 -1.6442156 -1.5375032 -1.0203683 -0.030688763 1.1358771 1.8817744 1.8104863 1.1813164 0.52138567 -0.069777012 -0.78391314 -1.3996711][-1.2562537 -0.66720772 -0.38550282 -0.32875443 -0.21324444 0.30123806 1.2264781 2.3911581 3.1771827 3.0347304 2.3179865 1.6340981 0.96168852 0.021687508 -0.83009124][-0.2393527 0.21808672 0.33177614 0.34819031 0.49785805 0.90806818 1.4675016 2.2407236 2.8304982 2.6383224 2.0147977 1.5363927 1.0543933 0.21564913 -0.57364178][0.18124199 0.35286665 0.1607995 0.032258511 0.15473557 0.41223145 0.59678888 0.974875 1.4099159 1.3201652 0.95651388 0.770658 0.56861544 0.01546669 -0.52692914][0.10490656 0.0049242973 -0.47010159 -0.7982204 -0.801311 -0.75018692 -0.88374543 -0.78983331 -0.44388008 -0.34634924 -0.37963915 -0.26785278 -0.19365978 -0.39627266 -0.62521839][-0.35599709 -0.63464952 -1.2610526 -1.7023635 -1.8210642 -1.9560475 -2.28997 -2.3427539 -2.0574167 -1.8353837 -1.6366951 -1.3250833 -1.0352921 -0.90662026 -0.83879662][-0.81527972 -1.0959415 -1.6906693 -2.1386833 -2.3444438 -2.626729 -3.0710738 -3.2159886 -3.0026531 -2.7519703 -2.4463649 -2.0329669 -1.6095145 -1.2539859 -0.97808075][-1.1445875 -1.366152 -1.8480058 -2.2517579 -2.4947672 -2.7944095 -3.1918225 -3.3508871 -3.2125437 -2.9975405 -2.6990886 -2.2988944 -1.8655913 -1.450202 -1.1116495]]...]
INFO - root - 2017-12-11 09:24:54.478786: step 1110, loss = 0.27, batch loss = 0.18 (15.1 examples/sec; 0.529 sec/batch; 48h:41m:59s remains)
INFO - root - 2017-12-11 09:24:59.879396: step 1120, loss = 0.44, batch loss = 0.36 (15.0 examples/sec; 0.533 sec/batch; 49h:04m:32s remains)
INFO - root - 2017-12-11 09:25:05.286271: step 1130, loss = 0.31, batch loss = 0.22 (15.0 examples/sec; 0.532 sec/batch; 48h:56m:44s remains)
INFO - root - 2017-12-11 09:25:10.671143: step 1140, loss = 0.32, batch loss = 0.24 (15.1 examples/sec; 0.530 sec/batch; 48h:47m:35s remains)
INFO - root - 2017-12-11 09:25:16.129061: step 1150, loss = 0.31, batch loss = 0.23 (15.1 examples/sec; 0.530 sec/batch; 48h:48m:38s remains)
INFO - root - 2017-12-11 09:25:21.515783: step 1160, loss = 0.27, batch loss = 0.19 (15.0 examples/sec; 0.533 sec/batch; 49h:05m:14s remains)
INFO - root - 2017-12-11 09:25:26.526328: step 1170, loss = 0.36, batch loss = 0.27 (14.8 examples/sec; 0.542 sec/batch; 49h:53m:18s remains)
INFO - root - 2017-12-11 09:25:31.872579: step 1180, loss = 0.25, batch loss = 0.16 (14.9 examples/sec; 0.536 sec/batch; 49h:21m:38s remains)
INFO - root - 2017-12-11 09:25:37.239279: step 1190, loss = 0.21, batch loss = 0.13 (14.8 examples/sec; 0.540 sec/batch; 49h:42m:27s remains)
INFO - root - 2017-12-11 09:25:42.629800: step 1200, loss = 0.27, batch loss = 0.18 (14.9 examples/sec; 0.535 sec/batch; 49h:16m:04s remains)
2017-12-11 09:25:43.180327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1541438 -4.9635382 -5.6308641 -5.7734876 -5.6871138 -5.5176058 -5.0458317 -4.6446428 -4.5001488 -4.6712852 -5.0440593 -5.1666451 -5.2861547 -5.2476745 -4.80232][-3.5195625 -4.509161 -5.4510131 -5.7527757 -5.6768246 -5.4687967 -4.937583 -4.5927029 -4.6187105 -4.9637322 -5.4294147 -5.5106425 -5.5286083 -5.4829922 -5.1342683][-2.5196657 -3.7131958 -4.9181113 -5.3688612 -5.2344332 -4.8788447 -4.3254576 -4.1810064 -4.5031619 -5.1073408 -5.7031307 -5.7373772 -5.5268307 -5.2398467 -4.8220496][-1.5434055 -2.872283 -4.2439389 -4.7570333 -4.3782611 -3.5687087 -2.7705688 -2.7251046 -3.3202066 -4.2159448 -5.0527754 -5.2435994 -5.0067482 -4.4983349 -3.9059396][-1.0972676 -2.25199 -3.4420352 -3.7230885 -2.8150311 -1.279366 -0.059696674 -0.15576839 -1.2313426 -2.6358221 -3.9197621 -4.5004148 -4.5234995 -4.0117369 -3.312192][-0.65395188 -1.6013327 -2.4912672 -2.3681443 -0.80989766 1.5398321 3.2950253 3.0545039 1.3523107 -0.67243481 -2.4356437 -3.4495974 -3.887825 -3.6370363 -3.0107954][-0.61705589 -1.3942802 -1.8922606 -1.230428 0.95628738 3.9865789 6.3011923 6.0892563 3.9044113 1.4169998 -0.6483078 -1.9524915 -2.8080335 -3.0274405 -2.683516][-1.1650965 -1.6944253 -1.6928289 -0.45786119 2.1705608 5.4876642 8.0405674 7.8454866 5.3049746 2.471487 0.21229219 -1.24224 -2.3195829 -2.9304776 -2.92836][-1.5217409 -1.9517736 -1.7946765 -0.476084 2.0199695 4.9540529 7.1756105 7.0517206 4.7482624 2.1330419 0.0092301369 -1.37715 -2.4355512 -3.2267027 -3.5391927][-2.3774831 -2.8493507 -2.8317318 -1.8172519 0.1757412 2.3987484 4.0698957 4.1039839 2.5220709 0.61764431 -1.0148666 -2.092217 -2.934885 -3.6773872 -4.1879535][-3.7317162 -4.1987147 -4.2079797 -3.4091816 -1.9235978 -0.4742291 0.49894 0.51452541 -0.38151693 -1.5000591 -2.4822731 -3.0675011 -3.5796275 -4.1365905 -4.6813841][-5.0032973 -5.3541989 -5.2479248 -4.5677691 -3.5939085 -2.9436355 -2.7290983 -2.891762 -3.2239017 -3.5387795 -3.7550898 -3.7471545 -3.9262464 -4.3384376 -4.9125867][-5.6888394 -5.8890834 -5.7063074 -5.1756186 -4.6238375 -4.5222983 -4.7936597 -5.0507007 -4.9323015 -4.6195927 -4.2801929 -3.8888297 -3.9251735 -4.3329735 -4.9463553][-5.556138 -5.6030836 -5.4388132 -5.139091 -4.8624935 -4.9474244 -5.3262682 -5.5402489 -5.1766963 -4.6108756 -4.1148791 -3.6742051 -3.7333283 -4.1701546 -4.769155][-5.0482135 -4.9912887 -4.8565378 -4.7054129 -4.5573506 -4.6806083 -5.1147256 -5.388082 -5.0441914 -4.4987941 -4.0563436 -3.69734 -3.7903543 -4.1925693 -4.6776175]]...]
INFO - root - 2017-12-11 09:25:48.497481: step 1210, loss = 0.33, batch loss = 0.24 (15.0 examples/sec; 0.535 sec/batch; 49h:14m:35s remains)
INFO - root - 2017-12-11 09:25:53.847873: step 1220, loss = 0.25, batch loss = 0.17 (14.7 examples/sec; 0.543 sec/batch; 49h:57m:27s remains)
INFO - root - 2017-12-11 09:25:59.255506: step 1230, loss = 0.34, batch loss = 0.25 (15.2 examples/sec; 0.526 sec/batch; 48h:24m:56s remains)
INFO - root - 2017-12-11 09:26:04.743057: step 1240, loss = 0.29, batch loss = 0.21 (14.8 examples/sec; 0.540 sec/batch; 49h:40m:21s remains)
INFO - root - 2017-12-11 09:26:10.110579: step 1250, loss = 0.41, batch loss = 0.32 (15.1 examples/sec; 0.531 sec/batch; 48h:52m:43s remains)
INFO - root - 2017-12-11 09:26:15.503197: step 1260, loss = 0.28, batch loss = 0.20 (13.7 examples/sec; 0.583 sec/batch; 53h:38m:39s remains)
INFO - root - 2017-12-11 09:26:20.589598: step 1270, loss = 0.40, batch loss = 0.31 (15.1 examples/sec; 0.530 sec/batch; 48h:45m:33s remains)
INFO - root - 2017-12-11 09:26:26.032976: step 1280, loss = 0.38, batch loss = 0.30 (15.1 examples/sec; 0.531 sec/batch; 48h:51m:18s remains)
INFO - root - 2017-12-11 09:26:31.380461: step 1290, loss = 0.32, batch loss = 0.24 (15.0 examples/sec; 0.534 sec/batch; 49h:08m:39s remains)
INFO - root - 2017-12-11 09:26:36.727724: step 1300, loss = 0.29, batch loss = 0.21 (15.0 examples/sec; 0.532 sec/batch; 48h:58m:27s remains)
2017-12-11 09:26:37.348695: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4637251 -4.2448792 -3.9582865 -3.4389787 -2.4993639 -1.2622926 0.063579082 0.94318533 0.60131645 -0.81252408 -2.5183072 -4.1054792 -5.179327 -5.4146318 -5.091804][-4.5918479 -4.3609233 -4.0792661 -3.5877209 -2.6440253 -1.4457166 -0.20892 0.57257271 0.2466135 -1.0465555 -2.6747916 -4.2143431 -5.2812276 -5.51854 -5.1613884][-4.620688 -4.3742528 -4.1190476 -3.7437506 -2.9295411 -1.8681147 -0.736418 -0.023346424 -0.33104038 -1.539675 -3.0862932 -4.52692 -5.5218086 -5.7080464 -5.2725487][-4.2649031 -4.071619 -3.8966513 -3.7150772 -3.1466837 -2.2848508 -1.1953211 -0.42981553 -0.70647621 -1.9346955 -3.461597 -4.8434854 -5.7938228 -5.945713 -5.4318371][-3.005096 -2.8540442 -2.7986698 -2.8453083 -2.5822186 -1.9366574 -0.80510044 0.12423325 -0.16789675 -1.5905521 -3.2687306 -4.7625656 -5.8106079 -6.0373683 -5.5395322][-1.899425 -1.6072743 -1.4198835 -1.3422639 -1.0695319 -0.4055562 0.92269373 2.0465002 1.5860047 -0.2719655 -2.3547015 -4.2123613 -5.5381722 -5.9457359 -5.5587449][-1.6263936 -1.2242112 -0.76855707 -0.29795551 0.26609516 1.1868415 2.8684125 4.2139568 3.5363379 1.2405968 -1.2824938 -3.5607381 -5.1951485 -5.7793012 -5.5191059][-1.7518857 -1.3551064 -0.7003541 0.06022644 0.8182106 1.9771581 3.9871149 5.4930353 4.67877 2.2179174 -0.49676442 -3.0125976 -4.8466187 -5.5605707 -5.4150324][-2.2434769 -1.8999772 -1.1300523 -0.28370428 0.46594381 1.701961 3.7533875 5.0957794 4.1153669 1.7430372 -0.82639861 -3.2029991 -4.9041681 -5.5305495 -5.3491488][-2.6149626 -2.2688184 -1.4461155 -0.62710881 0.068923473 1.280664 3.0616817 3.8873758 2.6219363 0.34225225 -2.0076907 -4.0529537 -5.3985195 -5.7796807 -5.4150147][-3.324245 -2.8475266 -1.9623876 -1.1172216 -0.32775021 0.85655451 2.233984 2.4880195 0.99529076 -1.2099214 -3.3965702 -5.1179166 -6.070889 -6.1828661 -5.6002464][-4.3187909 -3.8084159 -3.0047116 -2.187556 -1.2955666 -0.22431469 0.73121977 0.60406351 -0.93428493 -2.9936557 -4.9460392 -6.2690353 -6.76911 -6.5743833 -5.7910709][-5.0887523 -4.670032 -4.0931106 -3.4204149 -2.5372829 -1.6762664 -1.1059761 -1.4223809 -2.8830767 -4.7286196 -6.3495183 -7.2519422 -7.3241634 -6.823976 -5.8895469][-5.7233572 -5.4472942 -5.1095829 -4.6395316 -3.8823881 -3.27313 -3.0236044 -3.4141326 -4.7205887 -6.2766776 -7.4851351 -7.9810066 -7.7029934 -6.9300909 -5.8833952][-5.8193884 -5.65608 -5.4918303 -5.2103949 -4.6596794 -4.2943239 -4.271461 -4.6828237 -5.7883129 -7.0273805 -7.8519239 -8.0524292 -7.5990992 -6.7184486 -5.6919675]]...]
INFO - root - 2017-12-11 09:26:42.714927: step 1310, loss = 0.31, batch loss = 0.23 (15.2 examples/sec; 0.528 sec/batch; 48h:34m:19s remains)
INFO - root - 2017-12-11 09:26:48.118172: step 1320, loss = 0.23, batch loss = 0.14 (14.2 examples/sec; 0.565 sec/batch; 51h:59m:30s remains)
INFO - root - 2017-12-11 09:26:53.495233: step 1330, loss = 0.35, batch loss = 0.27 (15.0 examples/sec; 0.532 sec/batch; 48h:55m:10s remains)
INFO - root - 2017-12-11 09:26:58.971381: step 1340, loss = 0.30, batch loss = 0.22 (14.6 examples/sec; 0.549 sec/batch; 50h:28m:00s remains)
INFO - root - 2017-12-11 09:27:04.341117: step 1350, loss = 0.28, batch loss = 0.20 (14.2 examples/sec; 0.565 sec/batch; 51h:58m:40s remains)
INFO - root - 2017-12-11 09:27:09.730998: step 1360, loss = 0.27, batch loss = 0.19 (14.9 examples/sec; 0.537 sec/batch; 49h:22m:32s remains)
INFO - root - 2017-12-11 09:27:14.787217: step 1370, loss = 0.21, batch loss = 0.13 (15.1 examples/sec; 0.529 sec/batch; 48h:37m:11s remains)
INFO - root - 2017-12-11 09:27:20.247961: step 1380, loss = 0.24, batch loss = 0.15 (15.0 examples/sec; 0.533 sec/batch; 49h:03m:25s remains)
INFO - root - 2017-12-11 09:27:25.712817: step 1390, loss = 0.30, batch loss = 0.22 (15.3 examples/sec; 0.524 sec/batch; 48h:11m:52s remains)
INFO - root - 2017-12-11 09:27:31.046137: step 1400, loss = 0.27, batch loss = 0.19 (15.0 examples/sec; 0.533 sec/batch; 48h:58m:59s remains)
2017-12-11 09:27:31.625326: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7489109 -6.7535591 -7.18451 -7.5499353 -7.5466156 -7.5097761 -7.7933645 -8.3460054 -8.8498058 -9.3157959 -9.4568577 -9.0005283 -8.0491467 -6.7482886 -5.33561][-6.1845474 -5.7414155 -5.969717 -6.436553 -6.684979 -6.9411945 -7.41024 -8.0586214 -8.6992989 -9.3719864 -9.7549934 -9.4617729 -8.60791 -7.2407851 -5.6321745][-4.2443304 -3.2078295 -3.1412492 -3.7980666 -4.5118837 -5.2029858 -5.840631 -6.5097733 -7.3119812 -8.2812967 -9.0287094 -9.0719938 -8.5023384 -7.2814584 -5.67735][-2.2571976 -0.8600719 -0.65246224 -1.5376585 -2.5975015 -3.4196408 -3.8686821 -4.305985 -5.19995 -6.4913273 -7.6471381 -8.1095648 -7.9287744 -6.9980545 -5.5624142][-0.4744606 1.1959977 1.5283914 0.51209784 -0.653558 -1.2755902 -1.2136111 -1.2391419 -2.2190068 -3.924789 -5.5844965 -6.5601411 -6.8863964 -6.3922744 -5.3036089][0.64401531 2.4618697 2.9925427 2.0727353 1.0665908 0.81237841 1.4632497 1.8930206 0.85589981 -1.2278035 -3.3127956 -4.7488332 -5.5860872 -5.5966763 -4.9720364][0.42564678 2.1483221 2.8751936 2.2340088 1.4853678 1.4499874 2.4108124 3.1411653 2.1119814 -0.086206436 -2.1960804 -3.6907017 -4.7361865 -5.0929775 -4.8330674][-0.5762918 0.88863659 1.7244391 1.3791909 0.84846354 0.84372139 1.7484379 2.4822326 1.3940563 -0.71771264 -2.4685378 -3.6042166 -4.5066595 -4.9979987 -4.9319506][-2.2157955 -1.0399926 -0.10381842 -0.044672966 -0.15843725 -0.0074305534 0.74333382 1.2782598 0.026948929 -2.0147161 -3.4176059 -4.1245737 -4.7691803 -5.2288427 -5.1640229][-3.8044724 -2.653929 -1.4409914 -0.80069995 -0.22755146 0.29655838 0.94502211 1.1244307 -0.44123554 -2.5997877 -3.9499154 -4.4787159 -4.9992409 -5.4151697 -5.2913947][-4.8843122 -3.6863575 -2.2399447 -1.034786 0.32906723 1.3925452 2.0720053 1.8976531 -0.019882202 -2.4001467 -3.9246936 -4.50851 -5.0247293 -5.3892803 -5.1858945][-5.9101863 -4.7273574 -3.2276287 -1.6810396 0.325902 1.9681983 2.9106684 2.659615 0.58346462 -1.9485369 -3.6960073 -4.4341617 -4.9609962 -5.25361 -4.96006][-6.5856085 -5.569324 -4.2314043 -2.6494517 -0.36473656 1.6517339 2.9039259 2.7756543 0.7881465 -1.7487059 -3.7073696 -4.6446538 -5.14899 -5.3154626 -4.8791018][-6.8025637 -6.2262611 -5.343895 -4.0456009 -1.8323243 0.28952217 1.7128887 1.7435617 0.0067744255 -2.3771932 -4.470962 -5.4814963 -5.7792273 -5.6989369 -5.0528045][-6.5971384 -6.568161 -6.2160788 -5.2767768 -3.3227878 -1.3107731 0.11098003 0.25934029 -1.1761262 -3.3192828 -5.4153 -6.3524 -6.3681469 -6.06248 -5.2638874]]...]
INFO - root - 2017-12-11 09:27:36.944010: step 1410, loss = 0.36, batch loss = 0.28 (14.8 examples/sec; 0.540 sec/batch; 49h:40m:57s remains)
INFO - root - 2017-12-11 09:27:42.371410: step 1420, loss = 0.24, batch loss = 0.15 (14.3 examples/sec; 0.559 sec/batch; 51h:22m:21s remains)
INFO - root - 2017-12-11 09:27:47.927937: step 1430, loss = 0.27, batch loss = 0.19 (14.3 examples/sec; 0.561 sec/batch; 51h:33m:12s remains)
INFO - root - 2017-12-11 09:27:53.294794: step 1440, loss = 0.29, batch loss = 0.20 (14.9 examples/sec; 0.535 sec/batch; 49h:12m:50s remains)
INFO - root - 2017-12-11 09:27:58.691645: step 1450, loss = 0.32, batch loss = 0.23 (13.8 examples/sec; 0.580 sec/batch; 53h:19m:22s remains)
INFO - root - 2017-12-11 09:28:04.042332: step 1460, loss = 0.31, batch loss = 0.23 (14.8 examples/sec; 0.540 sec/batch; 49h:38m:26s remains)
INFO - root - 2017-12-11 09:28:09.189983: step 1470, loss = 0.35, batch loss = 0.27 (14.5 examples/sec; 0.551 sec/batch; 50h:40m:42s remains)
INFO - root - 2017-12-11 09:28:14.567931: step 1480, loss = 0.33, batch loss = 0.24 (14.9 examples/sec; 0.537 sec/batch; 49h:22m:28s remains)
INFO - root - 2017-12-11 09:28:19.950498: step 1490, loss = 0.44, batch loss = 0.36 (14.5 examples/sec; 0.550 sec/batch; 50h:35m:22s remains)
INFO - root - 2017-12-11 09:28:25.286356: step 1500, loss = 0.29, batch loss = 0.21 (15.5 examples/sec; 0.517 sec/batch; 47h:33m:43s remains)
2017-12-11 09:28:25.863572: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1360493 -5.513648 -5.877954 -5.7305245 -5.0731483 -4.77681 -5.1146927 -5.2857995 -5.393393 -5.4308405 -5.1406026 -5.2976136 -5.7057486 -5.6836352 -5.5217228][-5.4769812 -5.8532434 -6.210043 -6.074049 -5.3846536 -5.0547743 -5.287344 -5.3316612 -5.552599 -5.6899233 -5.2595854 -5.4495893 -6.1367865 -6.3552709 -6.2758284][-5.342782 -5.6864939 -6.0386977 -5.9458795 -5.2454839 -4.9187927 -5.0349641 -4.9118214 -5.2862883 -5.5796127 -5.004498 -5.15637 -6.0965385 -6.6393418 -6.7233062][-4.755887 -5.0293884 -5.3636837 -5.2899861 -4.587244 -4.3553009 -4.4083867 -4.1168442 -4.6674838 -5.1876192 -4.5612464 -4.6753373 -5.8664885 -6.8047495 -7.1045103][-4.01064 -4.1627674 -4.3698797 -4.1717887 -3.381217 -3.2707725 -3.2986808 -2.8237019 -3.4825478 -4.2209368 -3.6760602 -3.7904952 -5.2102461 -6.585103 -7.167819][-3.1263785 -2.9772673 -2.8005586 -2.215632 -1.1738834 -1.0842195 -1.1305726 -0.63169646 -1.4745419 -2.5477293 -2.3282099 -2.5535374 -4.1366277 -5.8795452 -6.7709112][-2.4700274 -1.8458903 -1.0586071 0.12282801 1.507308 1.6719375 1.571229 1.9387078 0.8639164 -0.58991194 -0.8358078 -1.2872741 -2.9938655 -4.9944673 -6.1917477][-2.5581753 -1.5490537 -0.20430279 1.51968 3.1367888 3.3888526 3.3161144 3.5475736 2.3428421 0.68268347 0.11430836 -0.47945 -2.2027631 -4.3011379 -5.7138877][-3.3052785 -2.2260821 -0.68700695 1.2119088 2.7407756 3.0572314 3.2282152 3.5637598 2.5689764 1.0499811 0.46844864 -0.1445694 -1.8752956 -3.9849236 -5.5044045][-4.5529203 -3.6779416 -2.2767246 -0.58816266 0.56920528 0.93344021 1.5591221 2.2862682 1.7853808 0.71136808 0.38231897 -0.18521261 -1.9803109 -4.088418 -5.6219411][-6.0077629 -5.5399132 -4.4758706 -3.1396255 -2.3644316 -1.9469938 -0.85428429 0.30004692 0.28865242 -0.29154015 -0.26402664 -0.70102954 -2.4505219 -4.4399638 -5.8875618][-7.0940747 -6.9681082 -6.2399769 -5.2942624 -4.8729134 -4.5194016 -3.2961221 -2.0949051 -1.9482629 -2.1700697 -1.8199866 -2.0709887 -3.5857 -5.2530165 -6.4122581][-7.3930707 -7.452291 -7.0141335 -6.47674 -6.3985577 -6.2172914 -5.2106543 -4.2984509 -4.2165074 -4.2269073 -3.7038574 -3.7814248 -4.8952293 -6.0735412 -6.8206964][-6.9240818 -7.0805111 -6.8597918 -6.6447506 -6.7944093 -6.7911205 -6.1721344 -5.6773348 -5.7455497 -5.7071891 -5.2128506 -5.1708779 -5.8120341 -6.4454985 -6.7666531][-6.0655694 -6.286026 -6.2423143 -6.2287512 -6.4589524 -6.5753822 -6.3228879 -6.1401272 -6.2520871 -6.2038541 -5.828 -5.7164927 -5.9720078 -6.1774073 -6.1920071]]...]
INFO - root - 2017-12-11 09:28:31.207384: step 1510, loss = 0.31, batch loss = 0.23 (15.0 examples/sec; 0.533 sec/batch; 49h:02m:24s remains)
INFO - root - 2017-12-11 09:28:36.606512: step 1520, loss = 0.27, batch loss = 0.19 (15.1 examples/sec; 0.529 sec/batch; 48h:38m:54s remains)
INFO - root - 2017-12-11 09:28:42.019128: step 1530, loss = 0.26, batch loss = 0.18 (14.7 examples/sec; 0.545 sec/batch; 50h:06m:07s remains)
INFO - root - 2017-12-11 09:28:47.383433: step 1540, loss = 0.31, batch loss = 0.22 (15.0 examples/sec; 0.535 sec/batch; 49h:10m:42s remains)
INFO - root - 2017-12-11 09:28:52.731621: step 1550, loss = 0.35, batch loss = 0.27 (14.7 examples/sec; 0.543 sec/batch; 49h:56m:34s remains)
INFO - root - 2017-12-11 09:28:58.090017: step 1560, loss = 0.23, batch loss = 0.15 (15.2 examples/sec; 0.526 sec/batch; 48h:22m:32s remains)
INFO - root - 2017-12-11 09:29:03.228420: step 1570, loss = 0.29, batch loss = 0.21 (14.3 examples/sec; 0.560 sec/batch; 51h:30m:56s remains)
INFO - root - 2017-12-11 09:29:08.703972: step 1580, loss = 0.30, batch loss = 0.21 (15.3 examples/sec; 0.524 sec/batch; 48h:08m:22s remains)
INFO - root - 2017-12-11 09:29:14.130768: step 1590, loss = 0.29, batch loss = 0.20 (15.0 examples/sec; 0.532 sec/batch; 48h:52m:45s remains)
INFO - root - 2017-12-11 09:29:19.435931: step 1600, loss = 0.32, batch loss = 0.24 (15.0 examples/sec; 0.535 sec/batch; 49h:09m:55s remains)
2017-12-11 09:29:20.073234: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8960061 -5.5653787 -5.9270253 -5.7627058 -5.2319589 -4.3766341 -3.542892 -3.0946293 -3.3646722 -4.0703688 -4.7008214 -5.3767653 -5.8872628 -5.9612236 -5.5276732][-5.8655376 -6.9227285 -7.6030235 -7.5238853 -6.8782015 -5.7770519 -4.6211605 -3.8869584 -4.2445879 -5.2927861 -6.1502943 -6.9458203 -7.4784293 -7.4100552 -6.6307697][-6.5236659 -7.7763848 -8.6191607 -8.5801754 -7.8579035 -6.5567045 -5.0883617 -4.0860095 -4.5368314 -5.9031954 -6.9567928 -7.7943974 -8.2977638 -8.1537724 -7.1712961][-6.5187783 -7.589057 -8.2076416 -7.976151 -7.1232886 -5.6245403 -3.8845572 -2.7797124 -3.4546008 -5.1687131 -6.440774 -7.3801107 -7.9816961 -7.9975605 -7.125587][-6.2623968 -6.8250847 -6.7969813 -6.0063663 -4.8020954 -2.9357815 -0.85842133 0.14923811 -1.0128219 -3.2877731 -5.0208821 -6.3402281 -7.2787971 -7.6646595 -7.1279893][-5.9770336 -5.9933057 -5.2014861 -3.7049594 -2.0066841 0.4248848 3.0013447 3.8711786 2.1439161 -0.69647956 -2.9392364 -4.7115831 -5.9840484 -6.6932883 -6.5407][-5.7274504 -5.3071308 -3.8923573 -1.7853537 0.47233295 3.6455593 6.8465853 7.7003508 5.5951519 2.3603988 -0.37836361 -2.6769979 -4.317399 -5.3299475 -5.550436][-5.9316506 -5.3184643 -3.5902009 -1.117712 1.6481323 5.5148592 9.3063049 10.332852 8.1778355 4.7794037 1.628026 -1.2202685 -3.2762129 -4.5806508 -5.0506892][-6.8160038 -6.534709 -5.13584 -2.9093871 -0.12731123 3.864893 7.7295132 9.0193672 7.3592291 4.4024582 1.485878 -1.3180935 -3.4147215 -4.8257957 -5.3475418][-7.9903984 -8.3261023 -7.5256047 -5.6961985 -2.9960666 0.86762667 4.4144411 5.7801104 4.5310087 1.9959712 -0.46729946 -2.8406272 -4.6550913 -5.9604626 -6.3064895][-8.456255 -9.3429279 -9.13183 -7.7426252 -5.260097 -1.7333107 1.2870712 2.5263915 1.4452958 -0.86186147 -2.8935647 -4.721602 -6.1174307 -7.1542864 -7.1302395][-8.1225672 -9.3310862 -9.6214437 -8.7855606 -6.8445539 -3.9732707 -1.6050661 -0.58708858 -1.5773034 -3.6614916 -5.2711473 -6.6079559 -7.6259561 -8.3388195 -7.9023676][-7.5989647 -8.734251 -9.0881767 -8.5305758 -7.1370363 -5.0218925 -3.2706566 -2.4994254 -3.4183927 -5.2546706 -6.5619307 -7.6774549 -8.6016788 -9.18417 -8.5754118][-6.7239189 -7.6024685 -7.850513 -7.491293 -6.6592269 -5.3448515 -4.2490773 -3.8045223 -4.5968747 -6.0271053 -6.9953938 -7.879282 -8.639967 -9.0328894 -8.4026966][-5.522212 -6.1326933 -6.2969604 -6.1234479 -5.7017908 -4.9669642 -4.38334 -4.2596426 -4.9289355 -5.9657221 -6.6899438 -7.3689003 -7.9118614 -8.100585 -7.5439162]]...]
INFO - root - 2017-12-11 09:29:25.497886: step 1610, loss = 0.26, batch loss = 0.18 (14.7 examples/sec; 0.546 sec/batch; 50h:11m:14s remains)
INFO - root - 2017-12-11 09:29:30.850440: step 1620, loss = 0.27, batch loss = 0.18 (14.8 examples/sec; 0.542 sec/batch; 49h:50m:49s remains)
INFO - root - 2017-12-11 09:29:36.200833: step 1630, loss = 0.28, batch loss = 0.20 (14.9 examples/sec; 0.537 sec/batch; 49h:21m:39s remains)
INFO - root - 2017-12-11 09:29:41.517919: step 1640, loss = 0.25, batch loss = 0.17 (15.1 examples/sec; 0.529 sec/batch; 48h:36m:58s remains)
INFO - root - 2017-12-11 09:29:46.921879: step 1650, loss = 0.25, batch loss = 0.16 (15.1 examples/sec; 0.528 sec/batch; 48h:31m:56s remains)
INFO - root - 2017-12-11 09:29:52.436853: step 1660, loss = 0.26, batch loss = 0.18 (13.4 examples/sec; 0.598 sec/batch; 54h:59m:53s remains)
INFO - root - 2017-12-11 09:29:57.546589: step 1670, loss = 0.25, batch loss = 0.16 (14.8 examples/sec; 0.540 sec/batch; 49h:36m:11s remains)
INFO - root - 2017-12-11 09:30:03.016728: step 1680, loss = 0.30, batch loss = 0.21 (14.5 examples/sec; 0.553 sec/batch; 50h:47m:58s remains)
INFO - root - 2017-12-11 09:30:08.372734: step 1690, loss = 0.29, batch loss = 0.21 (15.0 examples/sec; 0.532 sec/batch; 48h:51m:02s remains)
INFO - root - 2017-12-11 09:30:13.703180: step 1700, loss = 0.24, batch loss = 0.15 (14.9 examples/sec; 0.538 sec/batch; 49h:25m:17s remains)
2017-12-11 09:30:14.274794: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.35244608 1.4958749 1.0835595 -0.74805641 -3.1700242 -5.3759775 -6.9236655 -7.1612806 -6.5462551 -5.850553 -5.530046 -5.7473078 -5.7565775 -5.4000912 -4.8863082][1.3556066 2.5482912 2.1268539 0.28644896 -2.1554058 -4.3856297 -5.9825191 -6.3226919 -6.0278511 -5.9698744 -6.2939959 -6.8988934 -6.9975619 -6.56048 -5.8742919][1.6608663 2.5860233 2.0404682 0.32291174 -1.8031511 -3.58205 -4.7044668 -4.7687593 -4.7021809 -5.3383 -6.4064512 -7.503871 -7.8094664 -7.4005022 -6.6375542][1.0572996 1.4929094 0.724421 -0.81049132 -2.3851957 -3.3216825 -3.4701133 -2.8455167 -2.7726846 -3.97587 -5.7332354 -7.3546276 -8.0029488 -7.7679968 -7.04957][0.26098061 0.17076874 -0.80082774 -2.0160921 -2.78403 -2.5967746 -1.5688713 -0.20935392 -0.15141535 -1.927696 -4.34198 -6.5199404 -7.626235 -7.6630411 -7.03711][-0.52901244 -1.0392795 -2.0249727 -2.6380248 -2.2285559 -0.64722824 1.5660114 3.438333 3.2713122 0.83834028 -2.2132347 -4.8898768 -6.418222 -6.6621423 -6.0525208][-1.6156285 -2.2503016 -2.9463091 -2.7206759 -1.0573404 1.7063789 4.7276859 6.7809849 6.2519665 3.260746 -0.25779963 -3.2337706 -4.9795661 -5.2677097 -4.6043262][-2.7482724 -3.1585445 -3.3657403 -2.3648639 0.080818176 3.2126575 6.2240362 7.9829922 7.0956306 3.9285345 0.4000535 -2.3974364 -3.9110429 -3.9591606 -3.1852884][-3.6210341 -3.6878848 -3.4748108 -2.0273798 0.571239 3.3437672 5.6927595 6.7459593 5.5456114 2.6344218 -0.46434498 -2.732513 -3.679394 -3.2808027 -2.3663685][-3.7452726 -3.638418 -3.3527522 -1.9947009 0.16662312 2.1965666 3.6934919 3.9835138 2.5790954 0.10631514 -2.3578157 -3.9442449 -4.2048144 -3.3198724 -2.2531483][-3.7251759 -3.6382382 -3.5183744 -2.5857298 -1.1428878 0.11949348 0.94314146 0.73419571 -0.67838383 -2.6414831 -4.4323611 -5.3624716 -5.0295596 -3.7752993 -2.5527487][-4.4316335 -4.3831854 -4.4089265 -3.8875585 -3.1016374 -2.4426033 -2.0579045 -2.4082408 -3.4954729 -4.8143921 -5.9213829 -6.339169 -5.704464 -4.4002614 -3.2362132][-5.390851 -5.3857088 -5.5179429 -5.321537 -5.0231886 -4.765367 -4.6258049 -4.8799181 -5.4955788 -6.1626573 -6.6177478 -6.613965 -5.837595 -4.6513529 -3.656297][-5.7266073 -5.6750269 -5.8385472 -5.8676329 -5.8514996 -5.7836776 -5.7381606 -5.8366089 -6.055953 -6.2584724 -6.312644 -6.1288271 -5.4283185 -4.4956989 -3.7470541][-5.2758493 -5.1549597 -5.3103881 -5.4801817 -5.6137671 -5.6297216 -5.6150107 -5.6190615 -5.6165667 -5.563395 -5.4111338 -5.16578 -4.6188731 -3.9634569 -3.464879]]...]
INFO - root - 2017-12-11 09:30:19.594117: step 1710, loss = 0.43, batch loss = 0.34 (14.6 examples/sec; 0.549 sec/batch; 50h:24m:06s remains)
INFO - root - 2017-12-11 09:30:24.967991: step 1720, loss = 0.43, batch loss = 0.34 (14.9 examples/sec; 0.535 sec/batch; 49h:11m:07s remains)
INFO - root - 2017-12-11 09:30:30.388086: step 1730, loss = 0.24, batch loss = 0.15 (15.1 examples/sec; 0.530 sec/batch; 48h:42m:52s remains)
INFO - root - 2017-12-11 09:30:35.822668: step 1740, loss = 0.33, batch loss = 0.25 (15.1 examples/sec; 0.531 sec/batch; 48h:48m:23s remains)
INFO - root - 2017-12-11 09:30:41.279343: step 1750, loss = 0.36, batch loss = 0.27 (14.9 examples/sec; 0.535 sec/batch; 49h:11m:16s remains)
INFO - root - 2017-12-11 09:30:46.667237: step 1760, loss = 0.26, batch loss = 0.18 (15.2 examples/sec; 0.527 sec/batch; 48h:25m:27s remains)
INFO - root - 2017-12-11 09:30:51.841025: step 1770, loss = 0.30, batch loss = 0.22 (14.9 examples/sec; 0.536 sec/batch; 49h:14m:50s remains)
INFO - root - 2017-12-11 09:30:57.260986: step 1780, loss = 0.33, batch loss = 0.25 (14.8 examples/sec; 0.541 sec/batch; 49h:44m:27s remains)
INFO - root - 2017-12-11 09:31:02.645976: step 1790, loss = 0.24, batch loss = 0.16 (14.9 examples/sec; 0.535 sec/batch; 49h:10m:13s remains)
INFO - root - 2017-12-11 09:31:08.016512: step 1800, loss = 0.23, batch loss = 0.15 (14.7 examples/sec; 0.544 sec/batch; 49h:58m:37s remains)
2017-12-11 09:31:08.598871: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5170574 -2.9394898 -3.1086092 -3.1172118 -3.2723017 -3.6851058 -4.1922426 -4.5573421 -4.6861119 -4.5267329 -4.1407928 -3.9208431 -3.7780428 -3.6153016 -3.4701219][-1.4417586 -1.5481033 -1.5980268 -1.6693597 -2.055799 -2.8446155 -3.6523969 -4.0396008 -4.0348864 -3.8276877 -3.5007281 -3.3981948 -3.3699613 -3.3252907 -3.33044][-1.2058835 -1.1450045 -1.1095779 -1.1934743 -1.6384218 -2.561377 -3.5114589 -3.9452658 -3.9079325 -3.6863496 -3.4145141 -3.3848062 -3.405216 -3.438364 -3.612844][-1.8644073 -1.838496 -1.7354054 -1.6696873 -1.806226 -2.4019234 -3.1686549 -3.6303043 -3.7651374 -3.6932116 -3.4819291 -3.3933358 -3.2751818 -3.2041531 -3.4974155][-2.7425218 -2.9345481 -2.8306499 -2.5111108 -2.0738723 -1.9141605 -2.1551161 -2.5460215 -2.9595246 -3.19388 -3.0679545 -2.7613974 -2.2277079 -1.7751796 -2.1004031][-2.9573302 -3.401978 -3.3789215 -2.8602 -1.7927933 -0.70325541 -0.26972532 -0.5324347 -1.266717 -1.9456916 -2.0920496 -1.6919806 -0.6723268 0.4043622 0.31814718][-2.239306 -2.9136753 -3.0575838 -2.48722 -1.0031862 0.8750205 2.0119863 1.9635086 0.97143745 -0.15851879 -0.68719316 -0.44219351 0.85346365 2.5373583 2.9218192][-0.83092022 -1.8158221 -2.2624414 -1.7586789 -0.038543224 2.4656992 4.3314762 4.5768185 3.2951651 1.6051784 0.51760912 0.38890362 1.6595025 3.6845074 4.5313826][0.18177795 -1.0637352 -1.7232347 -1.2313888 0.55814743 3.308 5.6957426 6.3470173 4.9752264 2.8012271 1.1260967 0.46594238 1.3391957 3.2717409 4.357316][0.75384951 -0.6430378 -1.4602039 -1.116709 0.33492661 2.636282 4.9266663 5.8827705 4.84591 2.7401123 0.93942451 0.0157938 0.4194169 1.9459295 2.9798427][0.62570381 -0.60985637 -1.3994703 -1.2334285 -0.29461145 1.1700616 2.7510877 3.5410471 2.8679485 1.2055345 -0.29709768 -1.1810515 -1.145139 -0.12940121 0.69559908][-0.74653673 -1.5523269 -2.1481984 -2.1203094 -1.6351223 -0.93972921 -0.18235207 0.17852736 -0.24521112 -1.2901068 -2.2742333 -2.9593744 -3.1576498 -2.6217752 -2.0677781][-2.9167681 -3.2994308 -3.6437025 -3.717546 -3.6052704 -3.4267731 -3.2401657 -3.2176027 -3.4459212 -3.9032569 -4.3410769 -4.7124324 -4.8681993 -4.5249519 -4.1135492][-5.0319118 -5.1672344 -5.2326179 -5.252358 -5.2989883 -5.3720036 -5.457674 -5.5958881 -5.6988659 -5.7903271 -5.9260035 -6.1228275 -6.1979904 -5.9048362 -5.49937][-6.4907684 -6.6148944 -6.588582 -6.5953255 -6.7352934 -6.919404 -7.0756311 -7.2288942 -7.2550263 -7.188632 -7.1971936 -7.2878976 -7.2944527 -7.047637 -6.67662]]...]
INFO - root - 2017-12-11 09:31:13.971786: step 1810, loss = 0.25, batch loss = 0.17 (15.2 examples/sec; 0.526 sec/batch; 48h:21m:22s remains)
INFO - root - 2017-12-11 09:31:19.284469: step 1820, loss = 0.31, batch loss = 0.23 (14.9 examples/sec; 0.537 sec/batch; 49h:20m:31s remains)
INFO - root - 2017-12-11 09:31:24.754500: step 1830, loss = 0.24, batch loss = 0.16 (14.3 examples/sec; 0.560 sec/batch; 51h:24m:16s remains)
INFO - root - 2017-12-11 09:31:30.135474: step 1840, loss = 0.27, batch loss = 0.19 (14.6 examples/sec; 0.546 sec/batch; 50h:11m:02s remains)
INFO - root - 2017-12-11 09:31:35.521747: step 1850, loss = 0.26, batch loss = 0.18 (14.1 examples/sec; 0.566 sec/batch; 51h:59m:35s remains)
INFO - root - 2017-12-11 09:31:40.905001: step 1860, loss = 0.27, batch loss = 0.18 (14.4 examples/sec; 0.556 sec/batch; 51h:04m:19s remains)
INFO - root - 2017-12-11 09:31:45.995313: step 1870, loss = 0.29, batch loss = 0.21 (15.2 examples/sec; 0.528 sec/batch; 48h:27m:34s remains)
INFO - root - 2017-12-11 09:31:51.413005: step 1880, loss = 0.29, batch loss = 0.21 (14.8 examples/sec; 0.539 sec/batch; 49h:30m:37s remains)
INFO - root - 2017-12-11 09:31:56.828412: step 1890, loss = 0.34, batch loss = 0.25 (15.1 examples/sec; 0.531 sec/batch; 48h:43m:26s remains)
INFO - root - 2017-12-11 09:32:02.271172: step 1900, loss = 0.26, batch loss = 0.18 (14.7 examples/sec; 0.543 sec/batch; 49h:52m:35s remains)
2017-12-11 09:32:02.854128: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.282135 -2.2945828 -2.4777691 -2.7372603 -3.1070137 -3.542834 -4.0698524 -4.4784 -4.6633654 -4.4642849 -3.811944 -3.6434002 -3.8328478 -3.8657537 -3.7389991][-3.2197232 -3.2867925 -3.4277754 -3.638788 -3.9346223 -4.2659631 -4.6816812 -5.0554624 -5.3150692 -5.2503462 -4.9015617 -5.0744314 -5.4649372 -5.5764923 -5.3271227][-4.2826447 -4.3255086 -4.3621883 -4.5319018 -4.7753243 -4.9925833 -5.1965175 -5.3716793 -5.5466518 -5.5789418 -5.5829749 -6.0483584 -6.5405021 -6.6768842 -6.37913][-4.9613967 -4.7919192 -4.5260186 -4.4742031 -4.4933157 -4.4263792 -4.2277012 -4.0857553 -4.2175336 -4.4833183 -4.9298038 -5.6285152 -6.0809631 -6.1007481 -5.7533894][-5.0584512 -4.7083735 -4.1046691 -3.6228971 -3.1284277 -2.4322538 -1.4887223 -0.89659429 -1.174144 -2.0078952 -3.2050934 -4.3521814 -4.8825159 -4.7838721 -4.3706274][-4.5950112 -4.2943859 -3.488723 -2.5036736 -1.3418424 0.14626074 2.04499 3.2119746 2.8049188 1.4232345 -0.52417326 -2.20675 -2.9671483 -2.8808076 -2.5445728][-3.9616361 -3.7620435 -2.8055959 -1.3694546 0.35772896 2.4842367 5.1760359 6.7940474 6.2685385 4.4877386 1.9571776 -0.21836948 -1.2531476 -1.21872 -0.9849894][-3.5432751 -3.4789739 -2.5884843 -1.1460202 0.61304188 2.8538098 5.8404751 7.6553631 7.1277294 5.27308 2.5357285 0.10532379 -1.0716274 -0.99098682 -0.68635821][-3.336134 -3.5395463 -3.0359378 -2.1492195 -0.97055221 0.8251276 3.5735493 5.4095564 5.2095518 3.8566408 1.5548244 -0.66369367 -1.7858961 -1.6572707 -1.2516258][-3.2842317 -3.7099724 -3.6043971 -3.3475566 -2.8300428 -1.6139207 0.58310413 2.2239704 2.3507991 1.6063972 0.011921883 -1.7255251 -2.6877794 -2.5927289 -2.1990995][-3.6300516 -4.1819963 -4.3549886 -4.5655742 -4.5506659 -3.8065305 -2.2435973 -0.9660008 -0.72575068 -1.1008408 -2.0594451 -3.2022514 -3.8332245 -3.6318634 -3.1429973][-3.9459229 -4.5917873 -5.0008941 -5.5260797 -5.8103518 -5.4409332 -4.5153627 -3.6735392 -3.4288492 -3.5359247 -3.9440598 -4.4744482 -4.6904478 -4.3340268 -3.7374949][-3.7976646 -4.3553185 -4.82722 -5.4553361 -5.8850188 -5.88083 -5.5689588 -5.1963243 -5.0149531 -4.9131517 -4.8684568 -4.86525 -4.7338982 -4.3256292 -3.7884696][-3.3988805 -3.7919455 -4.1612983 -4.6318493 -4.9486947 -5.0709543 -5.0890636 -5.0549884 -5.0128517 -4.8858428 -4.6770043 -4.4451609 -4.1964726 -3.8871851 -3.5266342][-2.8421068 -3.0451403 -3.2119012 -3.3769622 -3.4511476 -3.519047 -3.6452839 -3.8545971 -4.011395 -3.9860387 -3.8236623 -3.6497655 -3.5753858 -3.5611362 -3.4635611]]...]
INFO - root - 2017-12-11 09:32:08.242923: step 1910, loss = 0.26, batch loss = 0.18 (14.4 examples/sec; 0.554 sec/batch; 50h:53m:23s remains)
INFO - root - 2017-12-11 09:32:13.696840: step 1920, loss = 0.24, batch loss = 0.16 (14.9 examples/sec; 0.538 sec/batch; 49h:26m:19s remains)
INFO - root - 2017-12-11 09:32:19.080235: step 1930, loss = 0.33, batch loss = 0.25 (15.0 examples/sec; 0.532 sec/batch; 48h:53m:41s remains)
INFO - root - 2017-12-11 09:32:24.441806: step 1940, loss = 0.27, batch loss = 0.19 (14.4 examples/sec; 0.554 sec/batch; 50h:54m:10s remains)
INFO - root - 2017-12-11 09:32:29.824306: step 1950, loss = 0.31, batch loss = 0.22 (15.0 examples/sec; 0.532 sec/batch; 48h:53m:36s remains)
INFO - root - 2017-12-11 09:32:35.194217: step 1960, loss = 0.26, batch loss = 0.18 (14.9 examples/sec; 0.537 sec/batch; 49h:18m:54s remains)
INFO - root - 2017-12-11 09:32:40.399395: step 1970, loss = 0.34, batch loss = 0.26 (13.6 examples/sec; 0.589 sec/batch; 54h:02m:03s remains)
INFO - root - 2017-12-11 09:32:45.827164: step 1980, loss = 0.24, batch loss = 0.16 (14.9 examples/sec; 0.535 sec/batch; 49h:08m:56s remains)
INFO - root - 2017-12-11 09:32:51.217195: step 1990, loss = 0.36, batch loss = 0.28 (14.4 examples/sec; 0.557 sec/batch; 51h:09m:56s remains)
INFO - root - 2017-12-11 09:32:56.592388: step 2000, loss = 0.28, batch loss = 0.20 (14.6 examples/sec; 0.546 sec/batch; 50h:09m:34s remains)
2017-12-11 09:32:57.164293: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2240109 -3.3457098 -3.4762764 -3.6039789 -3.7058816 -3.8803082 -4.0486393 -4.1187415 -4.18308 -4.3666415 -4.612061 -4.8391075 -5.001667 -5.0161729 -4.87179][-3.3264933 -3.4957142 -3.681808 -3.7953095 -3.8175747 -3.9305773 -4.0455618 -4.0094643 -3.9574416 -4.1480088 -4.484652 -4.8241239 -5.1474652 -5.2938423 -5.2018466][-3.3990984 -3.5552084 -3.717895 -3.6931717 -3.53054 -3.5106189 -3.540771 -3.3597841 -3.13493 -3.3297987 -3.7817068 -4.2538862 -4.7517347 -5.0740595 -5.130537][-3.3760986 -3.4834657 -3.5804327 -3.3434772 -2.9387507 -2.7330041 -2.6136861 -2.1970453 -1.7059116 -1.8691561 -2.4182484 -3.0191722 -3.6684971 -4.1680174 -4.4731212][-3.2783461 -3.3294466 -3.3615394 -2.8954802 -2.2493508 -1.7787576 -1.4263453 -0.70199728 0.089731216 -0.064620495 -0.74564433 -1.6142328 -2.5515263 -3.26299 -3.8036346][-3.1836004 -3.19118 -3.1134636 -2.3822024 -1.4857419 -0.71547389 -0.12451267 0.86738968 1.8348923 1.5601535 0.68297529 -0.59669161 -1.9740381 -2.9240394 -3.5980177][-2.9263222 -2.8446393 -2.5413802 -1.4838586 -0.34671974 0.66477442 1.4246826 2.5537949 3.4752617 2.8813291 1.7286367 0.036345482 -1.7805409 -2.976954 -3.7635679][-2.4261379 -2.2764084 -1.7716713 -0.4759624 0.75876236 1.8669052 2.6839375 3.7749891 4.4303885 3.4036551 1.9779129 0.037391663 -2.0186458 -3.3385084 -4.1691055][-1.9116209 -1.8040285 -1.2917404 -0.02999115 1.0501552 2.0579162 2.7984738 3.7123451 4.0796223 2.7704973 1.2048221 -0.78935146 -2.8381848 -4.0735254 -4.762887][-1.8171442 -1.8630445 -1.5624192 -0.6093235 0.16181946 0.95299578 1.5220923 2.1628637 2.2900987 0.90109921 -0.6480701 -2.4746637 -4.2073355 -5.0945625 -5.43867][-2.4344845 -2.7246404 -2.7801282 -2.2672763 -1.7750995 -1.1324372 -0.65535331 -0.21087313 -0.20945835 -1.4854987 -2.8753355 -4.3529959 -5.59792 -6.0536137 -6.0145369][-3.3165319 -3.7492783 -4.0744529 -3.9627204 -3.7186751 -3.2546685 -2.8571925 -2.5010304 -2.440475 -3.3636298 -4.4293265 -5.4769053 -6.2717762 -6.437747 -6.1594219][-3.8495109 -4.2798028 -4.7140217 -4.8727369 -4.8475876 -4.6219563 -4.3762717 -4.09883 -3.9545083 -4.4690328 -5.1305056 -5.7413921 -6.1551008 -6.1494117 -5.7694926][-4.1284738 -4.4822445 -4.8341961 -5.006032 -5.0324054 -4.9826736 -4.923676 -4.7852135 -4.6753249 -4.9237375 -5.2450128 -5.4824147 -5.5898747 -5.4797411 -5.1089067][-4.2707596 -4.5852685 -4.8441062 -4.953558 -4.9473925 -4.9290185 -4.9086561 -4.7870173 -4.6653895 -4.7127986 -4.7902832 -4.8094969 -4.7749805 -4.6678624 -4.4087949]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-shortcut
INFO - root - 2017-12-11 09:33:02.454682: step 2010, loss = 0.37, batch loss = 0.28 (14.5 examples/sec; 0.551 sec/batch; 50h:33m:56s remains)
INFO - root - 2017-12-11 09:33:07.899819: step 2020, loss = 0.29, batch loss = 0.21 (13.8 examples/sec; 0.579 sec/batch; 53h:09m:52s remains)
INFO - root - 2017-12-11 09:33:13.291500: step 2030, loss = 0.29, batch loss = 0.21 (15.1 examples/sec; 0.530 sec/batch; 48h:39m:03s remains)
INFO - root - 2017-12-11 09:33:18.676454: step 2040, loss = 0.26, batch loss = 0.18 (14.6 examples/sec; 0.547 sec/batch; 50h:11m:20s remains)
INFO - root - 2017-12-11 09:33:24.044806: step 2050, loss = 0.39, batch loss = 0.31 (15.0 examples/sec; 0.535 sec/batch; 49h:04m:51s remains)
INFO - root - 2017-12-11 09:33:29.473951: step 2060, loss = 0.41, batch loss = 0.33 (15.1 examples/sec; 0.529 sec/batch; 48h:34m:17s remains)
INFO - root - 2017-12-11 09:33:34.544211: step 2070, loss = 0.33, batch loss = 0.24 (14.3 examples/sec; 0.559 sec/batch; 51h:19m:18s remains)
INFO - root - 2017-12-11 09:33:39.954538: step 2080, loss = 0.30, batch loss = 0.21 (14.6 examples/sec; 0.549 sec/batch; 50h:23m:19s remains)
INFO - root - 2017-12-11 09:33:45.325257: step 2090, loss = 0.25, batch loss = 0.17 (14.3 examples/sec; 0.558 sec/batch; 51h:12m:38s remains)
INFO - root - 2017-12-11 09:33:50.664214: step 2100, loss = 0.35, batch loss = 0.27 (15.3 examples/sec; 0.523 sec/batch; 47h:57m:46s remains)
2017-12-11 09:33:51.224823: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3829041 -2.3116682 -2.504271 -3.0829878 -3.3659363 -3.0413871 -2.4222593 -2.2855666 -2.9535742 -3.9302809 -4.9343033 -5.7093806 -6.1892266 -6.2312407 -5.7646184][-2.9107494 -2.9086332 -3.361449 -4.2118196 -4.5225544 -4.0402045 -3.4013443 -3.3036587 -4.0061107 -5.1532521 -6.344449 -7.09577 -7.3447318 -7.1861715 -6.5427208][-3.7571492 -3.6973383 -4.0798445 -4.8904629 -5.0733361 -4.3748302 -3.6440833 -3.464931 -4.1348295 -5.4832797 -6.9100704 -7.7423906 -7.9522095 -7.7587471 -6.9816179][-4.4313841 -4.29865 -4.4018755 -4.86025 -4.7085662 -3.7302618 -2.7869248 -2.4100051 -2.9820681 -4.5506639 -6.316782 -7.4397807 -7.9114542 -7.944468 -7.1605883][-4.8133678 -4.7654223 -4.630549 -4.62488 -4.0137486 -2.6373734 -1.2451556 -0.4400177 -0.83367515 -2.6226268 -4.8031945 -6.3299546 -7.2301412 -7.6793866 -7.094305][-4.7796488 -4.9256454 -4.7365856 -4.3554778 -3.2906666 -1.4979959 0.48625422 1.9450383 1.8938804 -0.03660059 -2.6662183 -4.6623907 -6.0664549 -7.0515356 -6.8849859][-4.4506645 -4.964345 -4.984365 -4.4607439 -3.0970731 -0.98824191 1.5230451 3.6734514 4.1075068 2.1811571 -0.80339074 -3.1820855 -4.9834285 -6.4471626 -6.7542524][-3.8526118 -4.7137179 -5.0683064 -4.619668 -3.2100315 -1.0576313 1.6579838 4.2491722 5.13443 3.4103007 0.41979265 -1.9970257 -3.8937252 -5.645916 -6.3863382][-3.1759362 -4.1413856 -4.6876364 -4.3850622 -3.1198115 -1.1821632 1.3484559 3.9373302 4.9577541 3.4570227 0.72648716 -1.4344707 -3.125133 -4.8642063 -5.7981863][-3.0583627 -3.9419706 -4.4573908 -4.1799965 -3.049931 -1.3705125 0.79232693 3.0603762 3.8945684 2.5135345 0.16372967 -1.4987552 -2.6193602 -3.9408324 -4.7956967][-3.5402164 -4.3338723 -4.7072883 -4.3385906 -3.2762656 -1.7733145 0.055999756 1.9160686 2.431427 1.061296 -0.93477225 -2.0325606 -2.3662848 -2.9206083 -3.4215336][-4.3344841 -5.1001806 -5.3325238 -4.8836951 -3.9194946 -2.59016 -1.0850525 0.28582191 0.39653635 -0.99499536 -2.6242986 -3.1430814 -2.6509805 -2.244566 -2.1384614][-4.7535233 -5.6291361 -5.8686457 -5.5406384 -4.8458934 -3.7932758 -2.6506367 -1.7807484 -1.9643903 -3.2184162 -4.3781738 -4.2984133 -3.0770144 -1.7465205 -0.98548937][-4.3497634 -5.4153194 -5.8220677 -5.8318653 -5.6002331 -4.9568572 -4.2454967 -3.8570814 -4.2486587 -5.29558 -6.070437 -5.5844622 -3.8916526 -1.8895731 -0.58585882][-3.5040119 -4.7799873 -5.3261247 -5.6023426 -5.8168135 -5.6739874 -5.436192 -5.4601111 -5.9774 -6.844408 -7.415081 -6.8063288 -4.9843526 -2.6868379 -1.0582538]]...]
INFO - root - 2017-12-11 09:33:56.563338: step 2110, loss = 0.31, batch loss = 0.22 (13.8 examples/sec; 0.578 sec/batch; 53h:05m:12s remains)
INFO - root - 2017-12-11 09:34:02.044072: step 2120, loss = 0.27, batch loss = 0.19 (13.7 examples/sec; 0.584 sec/batch; 53h:37m:05s remains)
INFO - root - 2017-12-11 09:34:07.483906: step 2130, loss = 0.25, batch loss = 0.16 (15.3 examples/sec; 0.524 sec/batch; 48h:02m:47s remains)
INFO - root - 2017-12-11 09:34:12.875365: step 2140, loss = 0.25, batch loss = 0.17 (14.6 examples/sec; 0.549 sec/batch; 50h:24m:35s remains)
INFO - root - 2017-12-11 09:34:18.327538: step 2150, loss = 0.30, batch loss = 0.22 (14.9 examples/sec; 0.537 sec/batch; 49h:19m:18s remains)
INFO - root - 2017-12-11 09:34:23.679897: step 2160, loss = 0.28, batch loss = 0.20 (15.0 examples/sec; 0.533 sec/batch; 48h:55m:40s remains)
INFO - root - 2017-12-11 09:34:28.777369: step 2170, loss = 0.34, batch loss = 0.25 (15.7 examples/sec; 0.510 sec/batch; 46h:46m:00s remains)
INFO - root - 2017-12-11 09:34:34.143462: step 2180, loss = 0.27, batch loss = 0.19 (14.9 examples/sec; 0.537 sec/batch; 49h:17m:32s remains)
INFO - root - 2017-12-11 09:34:39.568488: step 2190, loss = 0.31, batch loss = 0.23 (14.8 examples/sec; 0.540 sec/batch; 49h:35m:17s remains)
INFO - root - 2017-12-11 09:34:45.069894: step 2200, loss = 0.27, batch loss = 0.19 (14.3 examples/sec; 0.559 sec/batch; 51h:15m:27s remains)
2017-12-11 09:34:45.653698: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2890825 -6.4808474 -6.7383518 -6.3747644 -5.6766796 -4.75517 -3.6272879 -3.2576718 -4.2264495 -5.2575636 -5.8200507 -6.3667154 -6.6101837 -6.33917 -6.2520118][-6.5554113 -6.6658254 -6.839325 -6.4094009 -5.5427794 -4.4832 -3.2851458 -2.8617551 -3.8961649 -5.1075516 -5.9446611 -6.9051771 -7.5934277 -7.6477156 -7.6850352][-6.7215362 -6.84809 -6.9860182 -6.4969349 -5.3702111 -4.0188818 -2.5356839 -1.7760444 -2.6697645 -4.0066695 -5.2033615 -6.7361135 -8.0688305 -8.6201029 -8.8290462][-6.8165779 -7.0386496 -7.1035271 -6.391139 -4.8259511 -2.9916079 -1.0544586 0.19598341 -0.48474479 -2.0211921 -3.782011 -6.0776844 -8.1515646 -9.1944885 -9.398984][-6.945096 -7.32592 -7.2891154 -6.2181983 -4.0861897 -1.5939491 0.97838449 2.8426905 2.4663515 0.67488766 -1.8456955 -5.0501838 -7.8667307 -9.2509594 -9.1937809][-6.9252863 -7.5264063 -7.4576335 -6.1269298 -3.5999131 -0.52368045 2.7611823 5.3754711 5.5583534 3.7416649 0.61302614 -3.4294124 -6.982234 -8.7473183 -8.5284729][-7.0863171 -7.8253622 -7.6892457 -6.220809 -3.592876 -0.2877512 3.3440938 6.3621197 7.0314541 5.4192209 2.1154461 -2.3180447 -6.2932768 -8.3166084 -8.0478153][-7.3191547 -8.1464672 -7.9754777 -6.5353718 -4.0417442 -0.80517936 2.7614512 5.6837759 6.5062933 5.1592169 2.0745935 -2.2277141 -6.162487 -8.2056541 -8.0118351][-7.5346828 -8.3650532 -8.2476254 -7.0630012 -4.958806 -2.0501468 1.1857305 3.7435989 4.5227966 3.4988108 0.94379282 -2.7875404 -6.2835336 -8.1464281 -8.1315155][-7.5598969 -8.2341137 -8.207057 -7.4560919 -5.9813251 -3.6877067 -1.0695817 0.92007208 1.5416512 0.8900342 -0.93111134 -3.8088562 -6.5959439 -8.0897293 -8.2035656][-7.1613684 -7.6333766 -7.7217932 -7.4489908 -6.655941 -5.0842862 -3.1870809 -1.8090789 -1.3861911 -1.7294347 -2.8356984 -4.8285613 -6.8789234 -8.0258579 -8.3180332][-6.4974456 -6.7725363 -6.9508839 -7.077507 -6.8796949 -5.9901156 -4.7769222 -3.9665313 -3.8007345 -4.0330338 -4.7003775 -6.0227728 -7.41797 -8.2111044 -8.5715981][-5.5514402 -5.650598 -5.861773 -6.2222786 -6.4346075 -6.0789771 -5.4104486 -5.0459809 -5.136261 -5.4090157 -5.90697 -6.8288031 -7.7406378 -8.2179976 -8.5307283][-4.5883055 -4.58333 -4.780582 -5.189043 -5.5577674 -5.5071516 -5.1943746 -5.0989709 -5.3149648 -5.586812 -5.9619255 -6.6167755 -7.2452025 -7.582118 -7.8806973][-3.8979557 -3.8668032 -4.0399423 -4.3942132 -4.74939 -4.8147058 -4.685442 -4.7057486 -4.9270773 -5.1384249 -5.3986759 -5.8634148 -6.3469787 -6.6780148 -6.9893684]]...]
INFO - root - 2017-12-11 09:34:51.009199: step 2210, loss = 0.29, batch loss = 0.21 (14.7 examples/sec; 0.545 sec/batch; 49h:58m:16s remains)
INFO - root - 2017-12-11 09:34:56.360241: step 2220, loss = 0.26, batch loss = 0.18 (15.1 examples/sec; 0.530 sec/batch; 48h:39m:16s remains)
INFO - root - 2017-12-11 09:35:01.800984: step 2230, loss = 0.30, batch loss = 0.21 (15.0 examples/sec; 0.533 sec/batch; 48h:52m:21s remains)
INFO - root - 2017-12-11 09:35:07.184959: step 2240, loss = 0.26, batch loss = 0.18 (14.7 examples/sec; 0.544 sec/batch; 49h:52m:57s remains)
INFO - root - 2017-12-11 09:35:12.580105: step 2250, loss = 0.34, batch loss = 0.25 (14.5 examples/sec; 0.551 sec/batch; 50h:33m:13s remains)
INFO - root - 2017-12-11 09:35:17.971399: step 2260, loss = 0.42, batch loss = 0.33 (15.1 examples/sec; 0.530 sec/batch; 48h:39m:10s remains)
INFO - root - 2017-12-11 09:35:23.088074: step 2270, loss = 0.35, batch loss = 0.26 (14.9 examples/sec; 0.537 sec/batch; 49h:17m:04s remains)
INFO - root - 2017-12-11 09:35:28.532780: step 2280, loss = 0.37, batch loss = 0.29 (14.9 examples/sec; 0.535 sec/batch; 49h:06m:26s remains)
INFO - root - 2017-12-11 09:35:33.885750: step 2290, loss = 0.29, batch loss = 0.20 (14.7 examples/sec; 0.546 sec/batch; 50h:03m:25s remains)
INFO - root - 2017-12-11 09:35:39.306096: step 2300, loss = 0.25, batch loss = 0.17 (15.0 examples/sec; 0.534 sec/batch; 49h:00m:26s remains)
2017-12-11 09:35:39.882513: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0024104 -3.1956413 -2.4857631 -2.0277455 -1.9420886 -2.1892779 -2.499531 -2.3452384 -1.8275349 -1.575505 -1.3149178 -0.73108506 -0.12491465 -0.020144463 -0.36064911][-4.0197663 -3.4131951 -2.6737964 -2.0831122 -1.9434583 -2.3131218 -2.9061344 -2.8428068 -2.4078565 -2.4037659 -2.1248188 -1.1654155 -0.23868847 0.0049052238 -0.34557104][-4.2773471 -3.9003985 -3.0825057 -2.2225559 -1.9015653 -2.2370605 -3.0384777 -3.2601609 -3.144556 -3.4534404 -3.1511593 -1.8505533 -0.66441774 -0.34437943 -0.73978591][-4.6245651 -4.4546666 -3.6152349 -2.5322351 -1.9788709 -2.0809639 -2.8158731 -3.2043924 -3.3335638 -3.8093455 -3.5501175 -2.2647984 -1.1210895 -0.86169267 -1.3114035][-4.8151612 -4.71041 -3.8434827 -2.5849752 -1.787833 -1.6093776 -2.164773 -2.6565225 -2.9690411 -3.5334976 -3.4948378 -2.6110454 -1.7209079 -1.4750597 -1.7857401][-4.9119573 -4.7510471 -3.8755946 -2.545321 -1.5625334 -1.1693103 -1.489327 -1.9052014 -2.220264 -2.7835765 -3.0512505 -2.7566497 -2.2598791 -2.028549 -2.0767198][-4.9955516 -4.72464 -3.8146992 -2.4233398 -1.2291365 -0.53814673 -0.41667604 -0.47601676 -0.63195634 -1.2192047 -1.9383733 -2.3769481 -2.4485936 -2.4096453 -2.3681118][-5.0006862 -4.6078944 -3.6465688 -2.2151985 -0.88299775 0.03555584 0.54390335 0.86157131 0.83046484 0.12384224 -1.0430083 -2.1021764 -2.6413534 -2.7758307 -2.7181482][-5.0262103 -4.605216 -3.667006 -2.3174222 -1.0609515 -0.11531067 0.64652157 1.3211837 1.4654012 0.7583642 -0.64332771 -2.0751979 -2.8613873 -3.0957205 -3.0170612][-5.0621023 -4.7238007 -3.859823 -2.5903995 -1.4778728 -0.59491038 0.28644609 1.179358 1.4076672 0.73202562 -0.68997836 -2.2066917 -2.9673715 -3.1595707 -3.0313463][-4.9472494 -4.7281437 -3.9874036 -2.8606606 -1.9903526 -1.2625737 -0.36420298 0.60783482 0.85022545 0.23633671 -0.99425793 -2.3025022 -2.791759 -2.8510427 -2.7064333][-4.6446161 -4.5270023 -3.9230103 -2.9785614 -2.3552744 -1.7578263 -0.85587406 0.10851908 0.30885839 -0.26957417 -1.2313571 -2.2143409 -2.4388337 -2.3888583 -2.2109256][-4.2245717 -4.1732807 -3.6680114 -2.8619883 -2.4276431 -1.980145 -1.2484546 -0.53135848 -0.49506307 -1.0183284 -1.5746019 -2.0509312 -2.002156 -1.8336599 -1.5484455][-3.9909277 -4.0163 -3.5853751 -2.8600168 -2.575736 -2.2984126 -1.7855759 -1.3094144 -1.3380153 -1.6826282 -1.7791114 -1.7564924 -1.577527 -1.3685753 -0.99415517][-3.8903093 -4.00872 -3.6726637 -3.0065088 -2.8119776 -2.6420259 -2.2560437 -1.866473 -1.8873801 -2.1396632 -1.9941208 -1.7597778 -1.6816487 -1.5073352 -1.0451703]]...]
INFO - root - 2017-12-11 09:35:45.276449: step 2310, loss = 0.25, batch loss = 0.17 (14.9 examples/sec; 0.538 sec/batch; 49h:17m:58s remains)
INFO - root - 2017-12-11 09:35:50.706102: step 2320, loss = 0.30, batch loss = 0.22 (14.5 examples/sec; 0.550 sec/batch; 50h:28m:35s remains)
INFO - root - 2017-12-11 09:35:56.126030: step 2330, loss = 0.30, batch loss = 0.21 (15.2 examples/sec; 0.526 sec/batch; 48h:16m:54s remains)
INFO - root - 2017-12-11 09:36:01.524827: step 2340, loss = 0.22, batch loss = 0.13 (14.7 examples/sec; 0.544 sec/batch; 49h:51m:51s remains)
INFO - root - 2017-12-11 09:36:06.989310: step 2350, loss = 0.31, batch loss = 0.22 (14.8 examples/sec; 0.539 sec/batch; 49h:28m:26s remains)
INFO - root - 2017-12-11 09:36:12.357356: step 2360, loss = 0.38, batch loss = 0.30 (14.9 examples/sec; 0.538 sec/batch; 49h:22m:41s remains)
INFO - root - 2017-12-11 09:36:17.521283: step 2370, loss = 0.28, batch loss = 0.19 (14.5 examples/sec; 0.552 sec/batch; 50h:36m:16s remains)
INFO - root - 2017-12-11 09:36:22.922870: step 2380, loss = 0.29, batch loss = 0.20 (14.7 examples/sec; 0.546 sec/batch; 50h:02m:35s remains)
INFO - root - 2017-12-11 09:36:28.362182: step 2390, loss = 0.28, batch loss = 0.19 (14.9 examples/sec; 0.536 sec/batch; 49h:08m:33s remains)
INFO - root - 2017-12-11 09:36:33.745558: step 2400, loss = 0.25, batch loss = 0.17 (15.0 examples/sec; 0.532 sec/batch; 48h:47m:36s remains)
2017-12-11 09:36:34.346276: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1088943 -0.85106206 -1.0841868 -1.4953806 -2.5455375 -4.0636754 -4.4487433 -3.9109251 -3.4938886 -3.4343154 -3.4455645 -3.9042583 -5.1242733 -6.1644263 -6.3892465][-0.59988928 -1.1254764 -1.8952374 -2.5153046 -3.4059124 -4.539403 -4.564364 -3.8514237 -3.5049291 -3.5586016 -3.5897872 -4.06858 -5.3566318 -6.4132538 -6.5812469][-0.20834208 -1.4035327 -2.7263684 -3.688319 -4.3860831 -4.7811127 -4.0388684 -2.979857 -2.7295942 -3.0187273 -3.240685 -3.8967206 -5.3353486 -6.4562626 -6.6239948][-0.24527168 -1.8081431 -3.3760285 -4.395957 -4.5882831 -3.8844683 -2.1070182 -0.65832114 -0.68380308 -1.5607431 -2.3283579 -3.3279138 -4.9183669 -6.1346555 -6.4176884][-0.84369946 -2.420634 -3.834904 -4.477149 -3.8496144 -1.9813602 0.810719 2.7167625 2.3726687 0.54581976 -1.2564285 -2.930372 -4.795589 -6.1271019 -6.5067658][-1.3357615 -2.5835438 -3.5601614 -3.6366377 -2.2551708 0.42838669 4.0060148 6.5135422 6.1082458 3.3588643 0.30183697 -2.3735392 -4.8028049 -6.3893156 -6.877039][-1.2962563 -2.057116 -2.5904837 -2.3245919 -0.6745646 2.2057447 6.1092024 9.1089811 8.8733339 5.6200018 1.6676707 -1.8855112 -4.908637 -6.764842 -7.3167706][-1.094027 -1.594259 -2.0345857 -1.9275677 -0.66437626 1.7142859 5.3130569 8.4108381 8.4713478 5.3714 1.2680693 -2.5850053 -5.7865424 -7.5733027 -7.9176779][-1.3360105 -1.8085732 -2.3915341 -2.673902 -2.0760632 -0.50207829 2.3830605 5.212492 5.570488 3.0404038 -0.65132976 -4.2900472 -7.2519665 -8.6723747 -8.603076][-2.5606546 -2.9515359 -3.5160375 -3.9514651 -3.8084981 -2.9570646 -0.97219229 1.2967596 1.8469276 0.057323456 -2.899586 -6.0019569 -8.5003452 -9.4856625 -9.0395765][-4.1850448 -4.424737 -4.8714805 -5.3304377 -5.4581885 -5.1386333 -4.0401955 -2.5255053 -1.9757965 -3.029304 -5.0476437 -7.3265338 -9.18087 -9.73481 -9.0301418][-5.1466427 -5.2446589 -5.6023779 -6.1175818 -6.4772415 -6.5616713 -6.2327414 -5.5400505 -5.1991138 -5.6666241 -6.69864 -7.9603271 -8.9908266 -9.0976667 -8.2570877][-5.1365352 -5.1038628 -5.3236089 -5.7930412 -6.232316 -6.5069542 -6.6145916 -6.488039 -6.378931 -6.542449 -6.9332438 -7.4511433 -7.8511529 -7.6983137 -6.9284124][-4.6809025 -4.6020508 -4.6782389 -4.9655361 -5.2918625 -5.5512137 -5.7700195 -5.8590813 -5.865294 -5.9208422 -6.0446138 -6.2169065 -6.3279243 -6.148015 -5.6114535][-4.2060847 -4.1426888 -4.1367383 -4.2409844 -4.3977551 -4.5579667 -4.7255321 -4.8284988 -4.8687196 -4.9033289 -4.9518294 -5.0056658 -5.0208507 -4.8939028 -4.5886555]]...]
INFO - root - 2017-12-11 09:36:39.716897: step 2410, loss = 0.26, batch loss = 0.17 (14.5 examples/sec; 0.553 sec/batch; 50h:43m:25s remains)
INFO - root - 2017-12-11 09:36:45.074939: step 2420, loss = 0.32, batch loss = 0.23 (15.0 examples/sec; 0.535 sec/batch; 49h:03m:29s remains)
INFO - root - 2017-12-11 09:36:50.501367: step 2430, loss = 0.27, batch loss = 0.19 (14.9 examples/sec; 0.538 sec/batch; 49h:20m:31s remains)
INFO - root - 2017-12-11 09:36:55.865794: step 2440, loss = 0.31, batch loss = 0.22 (14.8 examples/sec; 0.539 sec/batch; 49h:25m:10s remains)
INFO - root - 2017-12-11 09:37:01.222795: step 2450, loss = 0.24, batch loss = 0.16 (15.2 examples/sec; 0.527 sec/batch; 48h:21m:06s remains)
INFO - root - 2017-12-11 09:37:06.606810: step 2460, loss = 0.22, batch loss = 0.13 (14.3 examples/sec; 0.559 sec/batch; 51h:14m:52s remains)
INFO - root - 2017-12-11 09:37:11.680453: step 2470, loss = 0.29, batch loss = 0.21 (14.7 examples/sec; 0.545 sec/batch; 49h:57m:35s remains)
INFO - root - 2017-12-11 09:37:17.128205: step 2480, loss = 0.30, batch loss = 0.22 (15.1 examples/sec; 0.530 sec/batch; 48h:34m:03s remains)
INFO - root - 2017-12-11 09:37:22.582823: step 2490, loss = 0.23, batch loss = 0.15 (14.6 examples/sec; 0.548 sec/batch; 50h:16m:15s remains)
INFO - root - 2017-12-11 09:37:27.946273: step 2500, loss = 0.23, batch loss = 0.15 (15.0 examples/sec; 0.532 sec/batch; 48h:48m:06s remains)
2017-12-11 09:37:28.481056: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7230978 -5.0859966 -5.173636 -5.1447082 -5.1075168 -5.0671549 -4.9528255 -5.0451541 -5.4196162 -5.6127744 -5.4814715 -5.1538658 -4.7765474 -4.5174022 -4.2425327][-5.9500093 -6.441227 -6.4910717 -6.3934865 -6.2660546 -6.127131 -5.8266978 -5.8355355 -6.3819885 -6.7090168 -6.6639585 -6.4493475 -6.1491203 -5.9108715 -5.498601][-6.9335632 -7.5051603 -7.429852 -7.1147842 -6.7052946 -6.2304697 -5.4871407 -5.2689633 -6.0104361 -6.5968084 -6.8123903 -6.8941736 -6.9098616 -6.933136 -6.5485234][-7.3325281 -8.0028915 -7.7939253 -7.131587 -6.2904558 -5.3648109 -4.0499444 -3.497988 -4.44168 -5.4456038 -6.0974569 -6.5611734 -6.9547944 -7.35036 -7.1196985][-7.4523125 -8.1719675 -7.8400993 -6.80562 -5.5034437 -4.1638117 -2.3252635 -1.3149958 -2.2776008 -3.681211 -4.8229203 -5.6607666 -6.4290328 -7.2302055 -7.2938843][-7.0175595 -7.5368438 -7.001729 -5.5303259 -3.6574693 -1.8208895 0.48947954 1.9405866 0.95568705 -1.0678952 -2.9385796 -4.196733 -5.2567048 -6.4174647 -6.9202695][-5.5799322 -5.7911611 -5.14933 -3.3494353 -0.87136793 1.5854073 4.3849688 6.268775 5.3583984 2.7150307 0.079391479 -1.6060359 -2.8911848 -4.3633065 -5.368607][-4.5134997 -4.6417379 -4.0715523 -2.1921608 0.50814056 3.1222615 5.8954592 7.8076172 7.07226 4.4936619 1.8265519 0.12297535 -1.1389112 -2.7085872 -3.9975019][-4.2667966 -4.376967 -3.9245794 -2.2242868 0.25708294 2.5244184 4.7347612 6.167264 5.5908165 3.7419662 1.7086558 0.25889826 -0.8592658 -2.361594 -3.7351289][-4.080935 -4.1276913 -3.8086898 -2.4752548 -0.42816687 1.4261146 3.1534982 4.2256603 3.9797621 3.084877 1.6962576 0.24618053 -0.92369604 -2.3465426 -3.7844894][-4.2319531 -4.3245521 -4.1956019 -3.2592764 -1.6421242 -0.15683937 1.1946397 2.0209346 2.1325665 2.1387615 1.3763604 -0.12164164 -1.4134347 -2.6989346 -4.0852203][-5.069828 -5.2788439 -5.2146049 -4.4346423 -3.0796411 -1.9219933 -0.99103975 -0.517086 -0.34489202 0.20300388 0.059376717 -1.2431037 -2.5527561 -3.7216613 -4.9929047][-5.9068894 -6.1234717 -5.9623632 -5.1907067 -4.0258718 -3.1552081 -2.5442798 -2.3146219 -2.1994989 -1.4478805 -1.1721759 -2.2327471 -3.6056387 -4.8354683 -6.08482][-6.4978919 -6.6531153 -6.4128647 -5.7243609 -4.8467631 -4.3316145 -4.0711508 -4.0792618 -4.0734 -3.3613987 -2.8416514 -3.5261698 -4.6940989 -5.7768011 -6.8000689][-7.0662632 -7.2750072 -7.0509634 -6.4657221 -5.8226743 -5.5872169 -5.6510658 -5.9177237 -6.1050191 -5.66692 -5.145402 -5.3914146 -6.01737 -6.5982304 -7.1111965]]...]
INFO - root - 2017-12-11 09:37:33.795692: step 2510, loss = 0.24, batch loss = 0.15 (14.9 examples/sec; 0.537 sec/batch; 49h:15m:05s remains)
INFO - root - 2017-12-11 09:37:39.170080: step 2520, loss = 0.30, batch loss = 0.21 (15.1 examples/sec; 0.531 sec/batch; 48h:39m:54s remains)
INFO - root - 2017-12-11 09:37:44.503978: step 2530, loss = 0.27, batch loss = 0.19 (14.8 examples/sec; 0.542 sec/batch; 49h:41m:12s remains)
INFO - root - 2017-12-11 09:37:49.822953: step 2540, loss = 0.27, batch loss = 0.19 (14.8 examples/sec; 0.541 sec/batch; 49h:34m:42s remains)
INFO - root - 2017-12-11 09:37:55.192416: step 2550, loss = 0.37, batch loss = 0.29 (14.9 examples/sec; 0.535 sec/batch; 49h:04m:24s remains)
INFO - root - 2017-12-11 09:38:00.644921: step 2560, loss = 0.33, batch loss = 0.25 (14.9 examples/sec; 0.537 sec/batch; 49h:10m:52s remains)
INFO - root - 2017-12-11 09:38:05.709888: step 2570, loss = 0.28, batch loss = 0.19 (24.9 examples/sec; 0.322 sec/batch; 29h:28m:07s remains)
INFO - root - 2017-12-11 09:38:11.121219: step 2580, loss = 0.23, batch loss = 0.15 (14.8 examples/sec; 0.539 sec/batch; 49h:23m:40s remains)
INFO - root - 2017-12-11 09:38:16.485289: step 2590, loss = 0.30, batch loss = 0.22 (15.4 examples/sec; 0.519 sec/batch; 47h:32m:11s remains)
INFO - root - 2017-12-11 09:38:21.893376: step 2600, loss = 0.27, batch loss = 0.19 (15.3 examples/sec; 0.524 sec/batch; 48h:02m:27s remains)
2017-12-11 09:38:22.469407: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4537444 -5.4783859 -5.3996792 -5.2382383 -5.0557346 -4.9088392 -4.8001804 -4.7346663 -4.7154379 -4.7437811 -4.7763672 -4.7731161 -4.6703172 -4.5103912 -4.5141211][-5.245821 -5.2619758 -5.273756 -5.2514868 -5.2199693 -5.19476 -5.1510539 -5.1543894 -5.2250104 -5.3609705 -5.4924555 -5.5183296 -5.3024259 -4.9480891 -4.8473825][-4.1949935 -4.0386033 -4.0720186 -4.2324634 -4.4623904 -4.6521792 -4.7229695 -4.8435707 -5.0831714 -5.4037256 -5.7044106 -5.8206711 -5.5521636 -5.071804 -4.9193144][-2.541595 -1.9892082 -1.8856895 -2.166985 -2.6445968 -3.0341892 -3.22966 -3.5656722 -4.1429405 -4.7853079 -5.301434 -5.4777956 -5.0882826 -4.428803 -4.2182975][-0.83621073 0.32413006 0.76855659 0.53647661 0.0093874931 -0.34706879 -0.48628974 -1.0110211 -2.0758169 -3.253854 -4.1600585 -4.5004997 -4.0223026 -3.1983087 -2.943615][0.17771578 1.9536104 2.8604126 2.8740325 2.5791488 2.5854249 2.7677526 2.189775 0.62782192 -1.1635797 -2.5667074 -3.1645832 -2.6347306 -1.6776648 -1.4152453][0.42865515 2.6342287 3.9591684 4.3332834 4.4553919 4.9985666 5.6238508 5.093709 3.1107912 0.7588253 -1.0901175 -1.8953133 -1.2771163 -0.19506788 0.024243832][-0.37852764 1.9412003 3.5021272 4.235383 4.8458748 5.986887 7.1722822 6.9730377 4.959651 2.3604431 0.22174644 -0.800925 -0.23844051 0.821599 0.91410637][-1.9071889 0.091248989 1.5401974 2.4045382 3.2822785 4.7072773 6.2277126 6.4819155 4.9034367 2.5868049 0.56757545 -0.47293305 0.015154839 0.99430847 0.98180342][-2.9782557 -1.6133075 -0.61271071 0.044219971 0.78330278 1.9573264 3.2860169 3.7169313 2.6390982 0.8802042 -0.69276834 -1.4646263 -0.86511683 0.17187595 0.22565269][-3.1222854 -2.2932272 -1.789279 -1.5458739 -1.2541263 -0.6973536 0.10171652 0.45231342 -0.26158381 -1.4953418 -2.6026297 -3.0354691 -2.1815042 -0.9562819 -0.73738813][-2.0770795 -1.4089987 -1.1791217 -1.3323011 -1.6100514 -1.8310969 -1.7140768 -1.6044962 -2.1320858 -2.9813361 -3.7152932 -3.8096561 -2.5571642 -1.0436482 -0.706686][-0.629246 0.22429562 0.47275591 0.10134745 -0.61176157 -1.4615343 -1.9067032 -2.0270305 -2.4817562 -3.0911083 -3.5825899 -3.4003944 -1.7202337 0.093608379 0.44190073][0.31899977 1.6032615 2.1530294 1.8332167 0.96600723 -0.19255877 -0.93487382 -1.1949298 -1.6788373 -2.2598395 -2.7184346 -2.419055 -0.43821716 1.6184874 1.9519854][0.31270218 2.0092726 2.9717336 2.9269071 2.1949773 1.0959587 0.38115263 0.162889 -0.277802 -0.84393764 -1.3251402 -1.0542946 0.93157721 3.0005226 3.2636914]]...]
INFO - root - 2017-12-11 09:38:27.771199: step 2610, loss = 0.26, batch loss = 0.17 (14.8 examples/sec; 0.539 sec/batch; 49h:23m:59s remains)
INFO - root - 2017-12-11 09:38:33.177504: step 2620, loss = 0.25, batch loss = 0.17 (14.6 examples/sec; 0.549 sec/batch; 50h:19m:53s remains)
INFO - root - 2017-12-11 09:38:38.546977: step 2630, loss = 0.28, batch loss = 0.20 (14.7 examples/sec; 0.544 sec/batch; 49h:50m:46s remains)
INFO - root - 2017-12-11 09:38:44.010363: step 2640, loss = 0.24, batch loss = 0.16 (14.3 examples/sec; 0.559 sec/batch; 51h:12m:51s remains)
INFO - root - 2017-12-11 09:38:49.400167: step 2650, loss = 0.31, batch loss = 0.23 (15.0 examples/sec; 0.533 sec/batch; 48h:51m:20s remains)
INFO - root - 2017-12-11 09:38:54.759172: step 2660, loss = 0.29, batch loss = 0.20 (15.1 examples/sec; 0.530 sec/batch; 48h:34m:18s remains)
INFO - root - 2017-12-11 09:39:00.029199: step 2670, loss = 0.31, batch loss = 0.23 (16.9 examples/sec; 0.473 sec/batch; 43h:17m:29s remains)
INFO - root - 2017-12-11 09:39:05.224530: step 2680, loss = 0.35, batch loss = 0.27 (14.6 examples/sec; 0.548 sec/batch; 50h:11m:30s remains)
INFO - root - 2017-12-11 09:39:10.595842: step 2690, loss = 0.32, batch loss = 0.24 (14.9 examples/sec; 0.539 sec/batch; 49h:20m:03s remains)
INFO - root - 2017-12-11 09:39:15.980931: step 2700, loss = 0.26, batch loss = 0.18 (14.9 examples/sec; 0.537 sec/batch; 49h:10m:32s remains)
2017-12-11 09:39:16.555621: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6933873 -2.3153079 -2.8469744 -3.232867 -3.5687177 -3.8177884 -4.09321 -4.2904296 -4.3674693 -4.3507562 -4.3034062 -4.2782941 -4.2248769 -4.1342845 -4.0432649][-2.4953992 -3.1884017 -3.6426892 -4.0186362 -4.3767452 -4.6012931 -4.8188272 -4.9757428 -5.020905 -4.9261889 -4.7693939 -4.6349816 -4.4814868 -4.3085828 -4.1446395][-3.6663105 -4.3105025 -4.5062718 -4.6387472 -4.7838745 -4.8353009 -4.929594 -5.0773144 -5.2033415 -5.2034059 -5.1626468 -5.1314478 -5.0389113 -4.8982668 -4.7212529][-4.3416963 -4.8379931 -4.7551703 -4.6124005 -4.4885621 -4.3686929 -4.3968239 -4.5967588 -4.8891797 -5.0951271 -5.329679 -5.549468 -5.5833473 -5.4659891 -5.237874][-4.6553011 -4.9215803 -4.5795045 -4.1991916 -3.8035235 -3.4697359 -3.3440642 -3.5227916 -3.915586 -4.2487111 -4.69061 -5.1429067 -5.3233113 -5.2174315 -4.9571753][-4.82847 -4.793447 -4.1721992 -3.5145094 -2.7800756 -2.1690438 -1.8987083 -2.1414111 -2.7274692 -3.2938986 -4.0023942 -4.7105985 -5.0313239 -4.8587303 -4.4186869][-4.2076473 -3.9049134 -3.1799126 -2.4127665 -1.3932252 -0.42182159 0.047165871 -0.24106073 -1.0153418 -1.8924127 -2.9349065 -3.909761 -4.3637686 -4.1536427 -3.5566263][-1.9380724 -1.4656858 -0.81525111 -0.075707912 1.2133222 2.6395502 3.3516994 3.0373507 2.0303378 0.69281149 -0.84932041 -2.1577384 -2.8624392 -2.8873906 -2.4689031][0.23312235 0.66025972 1.0126023 1.5382414 2.9308076 4.7208071 5.6882133 5.4066076 4.2063656 2.4919815 0.63969517 -0.72408819 -1.4486976 -1.5971129 -1.2343307][0.57184935 0.76341963 0.65259647 0.79260731 2.0271697 3.9419765 5.1671171 5.0807505 3.9746447 2.2680302 0.51175451 -0.6315825 -1.2399008 -1.4413147 -1.0569489][-1.478725 -1.3928025 -1.7320454 -1.8091009 -0.84230065 0.91177225 2.2291956 2.3098478 1.4067774 -0.058575153 -1.4610343 -2.2806387 -2.7765954 -3.04781 -2.663002][-4.284235 -4.1602635 -4.4986615 -4.616396 -3.92159 -2.5037704 -1.2426867 -1.0420642 -1.6571257 -2.66527 -3.4716389 -3.7853258 -4.0275955 -4.1749668 -3.6090226][-5.2132225 -4.9865928 -5.2266049 -5.363378 -4.9779396 -4.007133 -2.9785485 -2.7688355 -3.142801 -3.7090573 -3.9546437 -3.8166466 -3.7792292 -3.7156796 -2.9530778][-4.4560533 -4.0657787 -4.1388421 -4.2680535 -4.1168804 -3.4918175 -2.66119 -2.4458146 -2.6419954 -2.9205458 -2.9059191 -2.6740012 -2.6495247 -2.6454632 -2.0395954][-3.4450204 -2.8639419 -2.712554 -2.7638824 -2.7489533 -2.3620002 -1.6752117 -1.4677775 -1.5725629 -1.7507932 -1.7526619 -1.7199807 -1.9942629 -2.3312497 -2.1573677]]...]
INFO - root - 2017-12-11 09:39:21.857168: step 2710, loss = 0.38, batch loss = 0.30 (14.6 examples/sec; 0.549 sec/batch; 50h:17m:17s remains)
INFO - root - 2017-12-11 09:39:27.270535: step 2720, loss = 0.35, batch loss = 0.27 (15.0 examples/sec; 0.533 sec/batch; 48h:47m:06s remains)
INFO - root - 2017-12-11 09:39:32.702772: step 2730, loss = 0.22, batch loss = 0.14 (15.0 examples/sec; 0.534 sec/batch; 48h:55m:01s remains)
INFO - root - 2017-12-11 09:39:38.288631: step 2740, loss = 0.33, batch loss = 0.25 (14.4 examples/sec; 0.554 sec/batch; 50h:44m:02s remains)
INFO - root - 2017-12-11 09:39:43.745042: step 2750, loss = 0.23, batch loss = 0.15 (14.3 examples/sec; 0.560 sec/batch; 51h:16m:43s remains)
INFO - root - 2017-12-11 09:39:49.133697: step 2760, loss = 0.29, batch loss = 0.21 (14.7 examples/sec; 0.544 sec/batch; 49h:47m:46s remains)
INFO - root - 2017-12-11 09:39:54.503274: step 2770, loss = 0.27, batch loss = 0.19 (14.9 examples/sec; 0.537 sec/batch; 49h:12m:12s remains)
INFO - root - 2017-12-11 09:39:59.534266: step 2780, loss = 0.25, batch loss = 0.16 (15.0 examples/sec; 0.534 sec/batch; 48h:54m:53s remains)
INFO - root - 2017-12-11 09:40:04.962311: step 2790, loss = 0.25, batch loss = 0.17 (14.6 examples/sec; 0.548 sec/batch; 50h:08m:58s remains)
INFO - root - 2017-12-11 09:40:10.369819: step 2800, loss = 0.38, batch loss = 0.30 (14.8 examples/sec; 0.541 sec/batch; 49h:35m:00s remains)
2017-12-11 09:40:10.955694: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5768883 -3.9126151 -5.2206192 -5.8051672 -5.553792 -4.863749 -4.1644297 -3.5503163 -3.3871264 -4.2581072 -5.6286464 -6.3866086 -6.4669609 -6.3952613 -5.9930124][-1.4771228 -2.6410785 -3.9893339 -4.8109212 -4.787323 -4.2175436 -3.4834538 -2.7852077 -2.6252015 -3.5509291 -5.0309086 -5.9719172 -6.2506447 -6.3123174 -6.0196075][-0.8899374 -1.6584294 -2.6644955 -3.4395752 -3.6058574 -3.2840166 -2.6829081 -1.9485266 -1.7563586 -2.7113729 -4.2422104 -5.3508091 -5.8934712 -6.1635113 -6.0237169][-1.0079069 -1.4267478 -1.8802266 -2.265301 -2.3716488 -2.1905696 -1.7697389 -1.1734486 -1.1914446 -2.4351296 -4.0946116 -5.244091 -5.8473849 -6.2113724 -6.1600304][-1.5739119 -1.8835065 -1.9330354 -1.8212729 -1.543443 -1.1503718 -0.71092057 -0.25965214 -0.62155724 -2.3067951 -4.2155676 -5.4228721 -6.0307274 -6.4451609 -6.4165964][-2.0264342 -2.3826153 -2.2515213 -1.7022109 -0.86067796 0.0159235 0.69475746 1.1349726 0.58797264 -1.4069715 -3.6755147 -5.2165012 -6.0855179 -6.6827497 -6.6792088][-2.0350246 -2.4057946 -2.1120377 -1.1465149 0.313344 1.729713 2.6698208 3.1945758 2.6213613 0.41838264 -2.3234448 -4.4872556 -5.8900647 -6.777813 -6.7958803][-2.1743829 -2.3823619 -1.7389953 -0.33837891 1.5898643 3.2637539 4.1950397 4.751545 4.2243071 1.9598866 -1.1059122 -3.7891741 -5.6724234 -6.7936869 -6.8138375][-2.4813094 -2.3986843 -1.4248285 0.0829978 1.9542284 3.4451022 4.1749897 4.7442369 4.4113321 2.4031687 -0.51087523 -3.2699585 -5.381187 -6.6740313 -6.7381935][-2.8461914 -2.4539585 -1.3317552 -0.09601593 1.1733637 2.0831132 2.5267715 3.0784597 2.9840636 1.5095286 -0.75557303 -3.0498462 -5.0181565 -6.357131 -6.5228405][-3.0161042 -2.2813478 -1.0902936 -0.2236681 0.35481882 0.68747234 0.88976431 1.3005195 1.192709 0.066176414 -1.5353529 -3.1360407 -4.6835103 -5.9092932 -6.1978436][-2.9736929 -1.8390529 -0.47837329 0.20623112 0.34504747 0.28414106 0.30033541 0.50600719 0.11606121 -1.0865309 -2.4217806 -3.5249939 -4.6083641 -5.5986819 -5.9413548][-3.2817132 -1.9228451 -0.44272828 0.21236944 0.23634434 0.056380749 -0.030498028 -0.0021810532 -0.67388439 -2.0718713 -3.3855405 -4.2366743 -4.9270711 -5.6041059 -5.8744926][-4.1454043 -2.6424906 -1.0764315 -0.45085025 -0.49520683 -0.731195 -0.93696547 -1.0482535 -1.7528527 -3.0331061 -4.1579924 -4.826951 -5.2766032 -5.698082 -5.8582754][-5.2022295 -3.6766207 -2.0716825 -1.4293602 -1.4792778 -1.7453456 -2.0942762 -2.3853383 -3.0449886 -4.0258484 -4.7887149 -5.2238436 -5.4715762 -5.6709366 -5.7056603]]...]
INFO - root - 2017-12-11 09:40:16.266574: step 2810, loss = 0.29, batch loss = 0.21 (15.2 examples/sec; 0.528 sec/batch; 48h:20m:14s remains)
INFO - root - 2017-12-11 09:40:21.673423: step 2820, loss = 0.23, batch loss = 0.15 (14.4 examples/sec; 0.555 sec/batch; 50h:47m:48s remains)
INFO - root - 2017-12-11 09:40:27.029390: step 2830, loss = 0.25, batch loss = 0.17 (14.9 examples/sec; 0.536 sec/batch; 49h:04m:51s remains)
INFO - root - 2017-12-11 09:40:32.402085: step 2840, loss = 0.27, batch loss = 0.18 (15.0 examples/sec; 0.535 sec/batch; 48h:58m:19s remains)
INFO - root - 2017-12-11 09:40:37.808682: step 2850, loss = 0.27, batch loss = 0.18 (14.5 examples/sec; 0.552 sec/batch; 50h:33m:52s remains)
INFO - root - 2017-12-11 09:40:43.262397: step 2860, loss = 0.27, batch loss = 0.19 (14.8 examples/sec; 0.541 sec/batch; 49h:33m:38s remains)
INFO - root - 2017-12-11 09:40:48.740499: step 2870, loss = 0.25, batch loss = 0.17 (14.9 examples/sec; 0.537 sec/batch; 49h:08m:11s remains)
INFO - root - 2017-12-11 09:40:53.859933: step 2880, loss = 0.29, batch loss = 0.21 (14.2 examples/sec; 0.562 sec/batch; 51h:27m:59s remains)
INFO - root - 2017-12-11 09:40:59.312021: step 2890, loss = 0.31, batch loss = 0.23 (14.4 examples/sec; 0.556 sec/batch; 50h:52m:29s remains)
INFO - root - 2017-12-11 09:41:04.737110: step 2900, loss = 0.25, batch loss = 0.17 (14.4 examples/sec; 0.556 sec/batch; 50h:52m:07s remains)
2017-12-11 09:41:05.303457: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2151945 -2.7567167 -2.4682202 -2.5218744 -2.57442 -2.1726604 -1.6219885 -1.4137502 -1.4561546 -1.5915833 -1.868968 -2.4477122 -3.058084 -3.2091563 -2.7855802][-3.0478134 -2.4970522 -2.0566478 -2.0683632 -2.2534881 -2.0788324 -1.6996901 -1.5099411 -1.4711432 -1.5381107 -1.8550222 -2.5639105 -3.3579142 -3.7148008 -3.3626761][-3.0675373 -2.5453663 -2.0192673 -1.963599 -2.1698494 -2.0542808 -1.670619 -1.3699739 -1.1930866 -1.1900353 -1.5826766 -2.4472961 -3.4211426 -4.002182 -3.8251734][-3.2358756 -2.803251 -2.2579226 -2.1285992 -2.2544703 -2.0703592 -1.5812323 -1.1269827 -0.81835055 -0.77385426 -1.2701142 -2.2757478 -3.3602154 -4.0660191 -4.0067372][-3.5901897 -3.2818122 -2.7791381 -2.573885 -2.5556788 -2.2727673 -1.7227743 -1.2048702 -0.82097435 -0.71935105 -1.2352476 -2.22026 -3.2174687 -3.8453579 -3.7624507][-4.0058575 -3.763788 -3.2866845 -2.9758997 -2.7950549 -2.4687779 -2.0234253 -1.6661119 -1.4124587 -1.3732336 -1.9021521 -2.7593851 -3.4906697 -3.8201394 -3.5545423][-4.1921477 -3.9774365 -3.5512516 -3.1795034 -2.8915181 -2.5975852 -2.3799417 -2.3121278 -2.2931938 -2.3840427 -2.9361987 -3.6449983 -4.0804758 -4.076623 -3.611979][-4.1550961 -4.0831671 -3.8103445 -3.4517145 -3.1033049 -2.8691106 -2.8771307 -3.0330474 -3.1454597 -3.263433 -3.7590752 -4.3273358 -4.583457 -4.4216642 -3.912462][-4.0738835 -4.2815685 -4.1932988 -3.8057635 -3.3300958 -3.0929346 -3.2302902 -3.4550979 -3.5237656 -3.5032697 -3.8095891 -4.1907272 -4.352253 -4.2334213 -3.9127998][-3.8146393 -4.311358 -4.375597 -3.9059758 -3.2458129 -2.9409163 -3.1125574 -3.3170996 -3.2855425 -3.1118889 -3.2156291 -3.4206157 -3.5524051 -3.6122074 -3.6408734][-3.3759594 -4.1538091 -4.3785644 -3.8827024 -3.0738096 -2.6324036 -2.7009697 -2.8056664 -2.66353 -2.3617909 -2.258369 -2.2658339 -2.3655584 -2.6395752 -3.0707302][-3.0700421 -4.0059223 -4.3181596 -3.8397355 -2.9477568 -2.3507562 -2.2441726 -2.2192543 -1.9780276 -1.5605006 -1.2280576 -0.98309135 -0.99360228 -1.4269011 -2.1836402][-2.9841242 -3.8181558 -4.0406547 -3.5685828 -2.6926446 -2.0278709 -1.7880402 -1.6796877 -1.3947713 -0.93212986 -0.43186831 0.05502224 0.15619183 -0.37024784 -1.311868][-2.9103553 -3.4113574 -3.4409142 -3.0294976 -2.3593822 -1.8368826 -1.5987844 -1.4638102 -1.1720309 -0.74216127 -0.19578505 0.45924807 0.67238855 0.15257072 -0.77632284][-2.6733613 -2.8852763 -2.8159478 -2.5822191 -2.2499602 -1.9921865 -1.8375466 -1.7245469 -1.4932506 -1.2045052 -0.74969363 -0.014095783 0.33058691 -0.033232212 -0.71326184]]...]
INFO - root - 2017-12-11 09:41:10.758914: step 2910, loss = 0.25, batch loss = 0.16 (14.6 examples/sec; 0.548 sec/batch; 50h:12m:36s remains)
INFO - root - 2017-12-11 09:41:16.148159: step 2920, loss = 0.27, batch loss = 0.19 (14.9 examples/sec; 0.536 sec/batch; 49h:03m:28s remains)
INFO - root - 2017-12-11 09:41:21.505708: step 2930, loss = 0.39, batch loss = 0.31 (14.9 examples/sec; 0.536 sec/batch; 49h:02m:11s remains)
INFO - root - 2017-12-11 09:41:26.929150: step 2940, loss = 0.25, batch loss = 0.16 (15.2 examples/sec; 0.526 sec/batch; 48h:11m:02s remains)
INFO - root - 2017-12-11 09:41:32.423971: step 2950, loss = 0.36, batch loss = 0.27 (14.6 examples/sec; 0.550 sec/batch; 50h:18m:40s remains)
INFO - root - 2017-12-11 09:41:37.840749: step 2960, loss = 0.38, batch loss = 0.30 (15.0 examples/sec; 0.534 sec/batch; 48h:54m:24s remains)
INFO - root - 2017-12-11 09:41:43.238674: step 2970, loss = 0.32, batch loss = 0.23 (14.8 examples/sec; 0.539 sec/batch; 49h:18m:55s remains)
INFO - root - 2017-12-11 09:41:48.330895: step 2980, loss = 0.29, batch loss = 0.20 (14.7 examples/sec; 0.543 sec/batch; 49h:42m:00s remains)
INFO - root - 2017-12-11 09:41:53.720637: step 2990, loss = 0.32, batch loss = 0.23 (14.9 examples/sec; 0.538 sec/batch; 49h:12m:10s remains)
INFO - root - 2017-12-11 09:41:59.057719: step 3000, loss = 0.29, batch loss = 0.21 (14.3 examples/sec; 0.559 sec/batch; 51h:12m:21s remains)
2017-12-11 09:41:59.606092: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4696856 -6.4676294 -6.2467456 -6.6095324 -6.4354968 -5.8093362 -5.2781 -5.340951 -5.3773785 -4.7170925 -3.859318 -3.6148865 -3.9095073 -4.10304 -4.2292857][-6.8700151 -6.6811609 -6.2374768 -6.4875646 -6.1996574 -5.3550177 -4.6640272 -4.7911606 -4.9453325 -4.0717378 -2.8426208 -2.5583432 -3.0318391 -3.374238 -3.5024602][-6.5492439 -6.0618095 -5.2857885 -5.2760782 -4.8577728 -3.81627 -2.9347939 -3.0970812 -3.4123192 -2.5116365 -0.99748826 -0.68615365 -1.4398632 -2.1535401 -2.4519331][-5.4690547 -4.6684375 -3.5527692 -3.1493335 -2.5423248 -1.4456694 -0.5620141 -0.8257525 -1.2667863 -0.44563365 1.198051 1.5400815 0.49105644 -0.72225666 -1.4235177][-4.0497022 -3.0453162 -1.6308494 -0.66419744 0.28369141 1.364501 2.1278086 1.6229863 0.91292095 1.3236628 2.6839833 2.9372644 1.6907754 0.090628147 -1.0769727][-2.844954 -1.7821901 -0.16048622 1.4135718 2.8143044 3.9034958 4.5836954 3.8491697 2.7667394 2.5012231 3.24613 3.382349 2.1736474 0.48653889 -0.96259737][-2.437376 -1.5186808 0.14361429 2.1295743 3.8478966 4.8868752 5.5013618 4.6425648 3.3259768 2.5259533 2.7494445 2.8728561 1.8739562 0.348032 -1.1369326][-2.8503721 -2.1185889 -0.51638603 1.6191082 3.4142189 4.2674217 4.7411833 3.8872128 2.6101074 1.5843921 1.4390531 1.5224161 0.68851137 -0.56881022 -1.8383181][-3.9275725 -3.3159213 -1.7981882 0.212533 1.8378906 2.433465 2.719698 1.9665732 0.98742867 0.08640337 -0.25582933 -0.25174046 -0.92695475 -1.8753581 -2.8421462][-5.2647734 -4.7952223 -3.4180145 -1.7188621 -0.47852039 -0.18201113 -0.035256863 -0.54357791 -1.050802 -1.5897713 -1.9664147 -2.086117 -2.696774 -3.4677222 -4.1994877][-6.0961075 -5.7900534 -4.6628437 -3.3993824 -2.6135683 -2.5677214 -2.5563622 -2.875155 -3.0782619 -3.3472669 -3.65594 -3.7995903 -4.2932868 -4.8841677 -5.3410778][-6.2029629 -6.1043587 -5.352006 -4.5440021 -4.1267943 -4.2135773 -4.2907853 -4.509768 -4.6319427 -4.8059664 -4.96384 -4.949512 -5.1947131 -5.5309858 -5.6882262][-5.5558028 -5.609148 -5.2569203 -4.8532391 -4.6565924 -4.7190137 -4.7497616 -4.8372107 -4.911931 -5.0508127 -5.1260571 -5.0616922 -5.194212 -5.4035258 -5.4470339][-4.5867381 -4.7245564 -4.688921 -4.60522 -4.5689445 -4.6119819 -4.6247897 -4.6497974 -4.6956468 -4.7922406 -4.8280997 -4.7760124 -4.8404579 -4.9593563 -4.9772558][-3.9531345 -4.1221886 -4.2330818 -4.2905283 -4.3294282 -4.3748231 -4.3939953 -4.4054365 -4.4261484 -4.4585757 -4.4467473 -4.386363 -4.3635187 -4.363842 -4.3133078]]...]
INFO - root - 2017-12-11 09:42:04.960248: step 3010, loss = 0.26, batch loss = 0.17 (14.3 examples/sec; 0.558 sec/batch; 51h:04m:21s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-shortcut
INFO - root - 2017-12-11 09:42:10.478642: step 3020, loss = 0.23, batch loss = 0.14 (13.8 examples/sec; 0.580 sec/batch; 53h:05m:15s remains)
INFO - root - 2017-12-11 09:42:15.948758: step 3030, loss = 0.32, batch loss = 0.24 (15.0 examples/sec; 0.534 sec/batch; 48h:50m:04s remains)
INFO - root - 2017-12-11 09:42:21.326189: step 3040, loss = 0.27, batch loss = 0.19 (15.1 examples/sec; 0.531 sec/batch; 48h:33m:37s remains)
INFO - root - 2017-12-11 09:42:26.690363: step 3050, loss = 0.26, batch loss = 0.17 (15.2 examples/sec; 0.527 sec/batch; 48h:16m:22s remains)
INFO - root - 2017-12-11 09:42:32.093415: step 3060, loss = 0.40, batch loss = 0.32 (14.9 examples/sec; 0.537 sec/batch; 49h:07m:29s remains)
INFO - root - 2017-12-11 09:42:37.496301: step 3070, loss = 0.31, batch loss = 0.23 (14.9 examples/sec; 0.537 sec/batch; 49h:09m:27s remains)
INFO - root - 2017-12-11 09:42:42.534466: step 3080, loss = 0.40, batch loss = 0.32 (14.8 examples/sec; 0.542 sec/batch; 49h:33m:04s remains)
INFO - root - 2017-12-11 09:42:47.951230: step 3090, loss = 0.37, batch loss = 0.28 (15.0 examples/sec; 0.535 sec/batch; 48h:56m:56s remains)
INFO - root - 2017-12-11 09:42:53.400711: step 3100, loss = 0.31, batch loss = 0.23 (14.9 examples/sec; 0.536 sec/batch; 49h:00m:04s remains)
2017-12-11 09:42:53.979171: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5372205 -6.8555384 -7.1372924 -7.3406224 -7.4612885 -7.4410362 -7.4487762 -7.6512218 -7.8250585 -8.0273676 -8.0773067 -7.8151155 -7.1829033 -6.284133 -5.6006002][-6.7326632 -7.2086172 -7.4745569 -7.59961 -7.5850043 -7.2906618 -7.0481453 -7.263134 -7.6559534 -8.1787062 -8.405983 -8.12398 -7.3689089 -6.2463703 -5.3641562][-6.4230824 -7.0599537 -7.20984 -7.122128 -6.7911191 -6.0058737 -5.3443751 -5.5316138 -6.2775187 -7.3027458 -7.8593464 -7.6919355 -6.9837255 -5.8391957 -4.9185023][-5.8003621 -6.5126657 -6.4537754 -6.0805769 -5.33117 -3.953974 -2.778121 -2.8048792 -3.8182068 -5.2713952 -6.1251211 -6.1352429 -5.6652222 -4.8232689 -4.2294168][-4.7057476 -5.377378 -5.0919542 -4.4563146 -3.2787747 -1.3504648 0.28093624 0.43930769 -0.8193028 -2.6556139 -3.7897649 -3.9899452 -3.789763 -3.4126177 -3.3743711][-3.8789151 -4.3355784 -3.7985506 -2.9074035 -1.2897904 1.1648855 3.2524877 3.622674 2.0865011 -0.18264246 -1.676507 -2.1787956 -2.2898977 -2.4111805 -2.9331923][-3.6879611 -3.8333671 -3.0326574 -1.9003818 0.055342674 2.8863583 5.3361197 5.8990335 4.1432819 1.5634108 -0.17770815 -0.93379569 -1.2826962 -1.8164568 -2.8110757][-4.0308795 -4.0305004 -3.1160052 -1.8529749 0.21322775 3.1105332 5.6631861 6.3286467 4.5244436 1.873579 0.038522243 -0.88909173 -1.3794274 -2.1174927 -3.3475678][-4.6672826 -4.7161818 -3.9283659 -2.7472596 -0.84942746 1.7491827 4.0598364 4.752636 3.2513642 0.914866 -0.81486726 -1.8436067 -2.4310141 -3.1800938 -4.3324614][-5.1004295 -5.2490249 -4.7524614 -3.9006772 -2.460403 -0.48661542 1.2725401 1.83817 0.7443819 -1.049963 -2.4416964 -3.3351355 -3.7864356 -4.2923532 -5.1108179][-5.4957414 -5.65294 -5.4111409 -4.952352 -4.0908456 -2.850708 -1.7556696 -1.428261 -2.1799226 -3.4000242 -4.3379774 -4.9301381 -5.1132607 -5.321022 -5.7752848][-5.404635 -5.6152248 -5.6137266 -5.502008 -5.1419773 -4.5001349 -3.951932 -3.8596087 -4.4081283 -5.1991553 -5.7788076 -6.1048508 -6.072341 -6.0318956 -6.165575][-4.6780796 -4.9836135 -5.1903186 -5.3247952 -5.3207283 -5.09439 -4.8932948 -4.9456105 -5.3705163 -5.8959885 -6.261312 -6.4512491 -6.3620362 -6.2130041 -6.1229181][-3.9857209 -4.3057389 -4.6326671 -4.9456935 -5.2222762 -5.3115468 -5.3269906 -5.4178724 -5.6635303 -5.9080358 -6.0440702 -6.1136951 -6.0527973 -5.916882 -5.72869][-3.4323621 -3.728204 -4.0910769 -4.4975634 -4.96573 -5.2939596 -5.4773903 -5.59007 -5.6761765 -5.6694164 -5.5543251 -5.438302 -5.3444381 -5.2226958 -4.9982681]]...]
INFO - root - 2017-12-11 09:42:59.277035: step 3110, loss = 0.28, batch loss = 0.20 (14.9 examples/sec; 0.536 sec/batch; 49h:01m:44s remains)
INFO - root - 2017-12-11 09:43:04.654154: step 3120, loss = 0.32, batch loss = 0.23 (14.8 examples/sec; 0.540 sec/batch; 49h:22m:26s remains)
INFO - root - 2017-12-11 09:43:10.063133: step 3130, loss = 0.37, batch loss = 0.28 (14.3 examples/sec; 0.559 sec/batch; 51h:10m:44s remains)
INFO - root - 2017-12-11 09:43:15.529709: step 3140, loss = 0.34, batch loss = 0.26 (13.0 examples/sec; 0.615 sec/batch; 56h:14m:43s remains)
INFO - root - 2017-12-11 09:43:20.850714: step 3150, loss = 0.31, batch loss = 0.22 (15.1 examples/sec; 0.528 sec/batch; 48h:20m:05s remains)
INFO - root - 2017-12-11 09:43:26.256878: step 3160, loss = 0.27, batch loss = 0.18 (13.5 examples/sec; 0.592 sec/batch; 54h:07m:01s remains)
INFO - root - 2017-12-11 09:43:31.700627: step 3170, loss = 0.28, batch loss = 0.20 (14.4 examples/sec; 0.554 sec/batch; 50h:41m:27s remains)
INFO - root - 2017-12-11 09:43:36.780966: step 3180, loss = 0.44, batch loss = 0.36 (15.0 examples/sec; 0.534 sec/batch; 48h:53m:30s remains)
INFO - root - 2017-12-11 09:43:42.229776: step 3190, loss = 0.42, batch loss = 0.34 (14.6 examples/sec; 0.546 sec/batch; 49h:59m:07s remains)
INFO - root - 2017-12-11 09:43:47.581135: step 3200, loss = 0.24, batch loss = 0.16 (15.1 examples/sec; 0.530 sec/batch; 48h:31m:28s remains)
2017-12-11 09:43:48.139960: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.674088 -4.2774706 -5.0745058 -5.762989 -6.1879168 -6.4119062 -6.5280437 -6.5018239 -6.2080908 -5.7160525 -5.2334247 -4.9637537 -4.849576 -4.7402496 -4.6308165][-4.016356 -4.8272672 -5.8095808 -6.5439339 -6.8865366 -6.9312491 -6.7978048 -6.5992584 -6.1924362 -5.5244622 -4.8587255 -4.5590687 -4.524261 -4.5043573 -4.4916921][-4.3602037 -5.3624058 -6.4403419 -7.028842 -7.07441 -6.7254882 -6.105494 -5.6016955 -5.0591717 -4.2670183 -3.5745826 -3.4597383 -3.6981113 -3.9553142 -4.2649193][-4.5828233 -5.7729492 -6.8977728 -7.2113781 -6.8609314 -6.0618849 -4.9374375 -4.1190848 -3.4435244 -2.5620689 -1.9477639 -2.1432636 -2.7216759 -3.3436289 -4.08942][-4.5980639 -5.8383164 -6.8047981 -6.6620064 -5.7922912 -4.4534678 -2.8644409 -1.886672 -1.3037908 -0.55667281 -0.21220684 -0.81159043 -1.7189193 -2.7374415 -3.9199471][-4.6568341 -5.8930836 -6.5582819 -5.891469 -4.4155807 -2.3631506 -0.2018795 0.94379282 1.2816591 1.6521053 1.4576044 0.29512835 -0.916595 -2.2446957 -3.7463362][-4.8604441 -6.06261 -6.3911972 -5.2556119 -3.1743197 -0.33643818 2.4806571 3.9007397 4.0317497 3.9295053 3.0464859 1.3227587 -0.1013546 -1.5668919 -3.1913633][-4.8931661 -6.0028844 -6.0919123 -4.7978468 -2.4908042 0.72540617 3.8659124 5.4245348 5.3888397 4.8680611 3.4718618 1.5860777 0.32978725 -0.86199093 -2.2348387][-4.853591 -5.9020615 -5.9976878 -4.9424505 -2.9176528 0.084675312 2.9907184 4.4129744 4.2675896 3.5081906 1.9390945 0.32711697 -0.4295013 -1.0557663 -1.8959384][-4.7389612 -5.7254305 -5.9602 -5.3187022 -3.7901108 -1.2632716 1.2065849 2.422998 2.2353592 1.3781967 -0.15922165 -1.414818 -1.7422328 -1.9275935 -2.2788751][-4.5516286 -5.5229311 -5.9824171 -5.7884526 -4.7469983 -2.690491 -0.63572431 0.40052509 0.18718624 -0.66978765 -2.0646927 -3.0511084 -3.1438766 -3.1191216 -3.2053971][-4.3843951 -5.3373194 -5.99424 -6.1314783 -5.4783854 -3.8831668 -2.2412722 -1.3904223 -1.6441445 -2.4364724 -3.6220975 -4.4800887 -4.5141182 -4.4424863 -4.4739003][-4.118825 -4.9077926 -5.5939631 -5.8732939 -5.5033755 -4.416841 -3.2948847 -2.7548351 -3.0907941 -3.7540193 -4.6876097 -5.4756274 -5.595283 -5.5935335 -5.7052064][-3.91693 -4.5002842 -5.0809312 -5.3592434 -5.1882892 -4.5772886 -3.9779119 -3.8007414 -4.2574482 -4.8300314 -5.5808945 -6.3441763 -6.5702877 -6.6138067 -6.773016][-3.7351506 -4.0862131 -4.4389572 -4.6202178 -4.5437417 -4.2554183 -4.0549769 -4.1859746 -4.7303519 -5.2600017 -5.890028 -6.6008406 -6.9042959 -7.0028439 -7.2126822]]...]
INFO - root - 2017-12-11 09:43:53.423224: step 3210, loss = 0.28, batch loss = 0.20 (15.4 examples/sec; 0.519 sec/batch; 47h:27m:14s remains)
INFO - root - 2017-12-11 09:43:58.812255: step 3220, loss = 0.25, batch loss = 0.16 (15.0 examples/sec; 0.532 sec/batch; 48h:38m:23s remains)
INFO - root - 2017-12-11 09:44:04.314253: step 3230, loss = 0.33, batch loss = 0.25 (14.3 examples/sec; 0.560 sec/batch; 51h:10m:49s remains)
INFO - root - 2017-12-11 09:44:09.727145: step 3240, loss = 0.29, batch loss = 0.20 (14.4 examples/sec; 0.556 sec/batch; 50h:52m:51s remains)
INFO - root - 2017-12-11 09:44:15.193602: step 3250, loss = 0.26, batch loss = 0.17 (14.8 examples/sec; 0.541 sec/batch; 49h:27m:12s remains)
INFO - root - 2017-12-11 09:44:20.733059: step 3260, loss = 0.34, batch loss = 0.25 (14.8 examples/sec; 0.542 sec/batch; 49h:31m:55s remains)
INFO - root - 2017-12-11 09:44:26.080960: step 3270, loss = 0.34, batch loss = 0.26 (15.3 examples/sec; 0.523 sec/batch; 47h:51m:29s remains)
INFO - root - 2017-12-11 09:44:31.189209: step 3280, loss = 0.20, batch loss = 0.12 (14.5 examples/sec; 0.550 sec/batch; 50h:18m:47s remains)
INFO - root - 2017-12-11 09:44:36.615896: step 3290, loss = 0.27, batch loss = 0.19 (15.0 examples/sec; 0.534 sec/batch; 48h:47m:47s remains)
INFO - root - 2017-12-11 09:44:41.940891: step 3300, loss = 0.23, batch loss = 0.15 (14.6 examples/sec; 0.548 sec/batch; 50h:08m:26s remains)
2017-12-11 09:44:42.507599: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4554172 -2.631187 -3.5714736 -4.7711411 -5.774406 -6.5248022 -6.4842148 -5.733119 -4.594861 -2.9991903 -1.5284927 -0.82393718 -0.92625713 -1.7266178 -2.9308147][-2.999227 -3.2421184 -4.01793 -4.9185309 -5.6563425 -6.2410765 -6.2831173 -5.821178 -5.0324678 -3.8216681 -2.5977952 -1.8951559 -1.8220062 -2.3430581 -3.2585855][-2.8551919 -3.1803539 -3.9127412 -4.7011447 -5.3229818 -5.8054008 -5.9114265 -5.7375612 -5.40702 -4.832325 -4.1648207 -3.6903954 -3.4862316 -3.5618136 -3.8382092][-2.735604 -3.0340347 -3.67581 -4.352078 -4.801434 -5.0342579 -5.0046377 -4.9568267 -5.06373 -5.2759681 -5.4374185 -5.4718204 -5.3296003 -5.0469522 -4.6627693][-3.0911062 -3.1887689 -3.4861834 -3.7876267 -3.7829862 -3.4316182 -2.9134617 -2.6787963 -3.0488706 -4.0332985 -5.1680455 -6.0304766 -6.3876414 -6.2136755 -5.5990376][-3.827183 -3.6576409 -3.4716463 -3.200171 -2.5031903 -1.3240352 -0.071668148 0.61299515 0.1903162 -1.4078467 -3.4205832 -5.1555414 -6.2373309 -6.5216827 -6.094656][-4.380352 -4.0563445 -3.4647746 -2.614028 -1.1745582 0.84179783 2.8375087 4.0609922 3.7212887 1.7023888 -0.94333386 -3.3372273 -5.0324593 -5.7871027 -5.7231131][-4.8254685 -4.4814143 -3.6390443 -2.3266656 -0.25652647 2.4159355 5.0077381 6.7288513 6.5406857 4.289422 1.2511806 -1.5547805 -3.6545539 -4.7647147 -5.0881376][-5.4407158 -5.1648612 -4.26436 -2.7305093 -0.33273554 2.6463928 5.5460119 7.5720625 7.5396404 5.2990942 2.2087994 -0.67545581 -2.8771782 -4.1237583 -4.7015185][-6.050426 -5.7941217 -4.9424486 -3.4485402 -1.1203547 1.7201986 4.4811144 6.4078026 6.4077244 4.4267778 1.6971841 -0.83538079 -2.7565403 -3.8737702 -4.5122752][-6.4564886 -6.1046729 -5.3366904 -4.1072273 -2.245507 0.0099258423 2.1820087 3.6182957 3.5301251 1.9663281 -0.12020445 -1.987303 -3.3281324 -4.0929232 -4.6215286][-6.5531049 -6.1142049 -5.4902782 -4.6751909 -3.5259726 -2.1149735 -0.75508761 0.064488888 -0.08694458 -1.1287456 -2.4601784 -3.5617704 -4.2288918 -4.5685692 -4.9216089][-6.30545 -5.7898283 -5.2725558 -4.8299866 -4.3705945 -3.7842326 -3.203747 -2.9336169 -3.0911412 -3.59683 -4.1844807 -4.5571151 -4.6239176 -4.6277876 -4.8666377][-5.7216878 -5.2160149 -4.7920361 -4.6103482 -4.6490512 -4.6662412 -4.6449313 -4.7368016 -4.86259 -4.9220252 -4.884995 -4.6696649 -4.3247437 -4.173955 -4.420579][-4.9926109 -4.6057529 -4.2854929 -4.245707 -4.532608 -4.8686609 -5.1308312 -5.3741336 -5.4325395 -5.1725416 -4.7184587 -4.1563439 -3.642024 -3.5136805 -3.8618751]]...]
INFO - root - 2017-12-11 09:44:47.878870: step 3310, loss = 0.31, batch loss = 0.23 (14.5 examples/sec; 0.550 sec/batch; 50h:19m:50s remains)
INFO - root - 2017-12-11 09:44:53.319275: step 3320, loss = 0.45, batch loss = 0.36 (14.7 examples/sec; 0.544 sec/batch; 49h:45m:28s remains)
INFO - root - 2017-12-11 09:44:58.751023: step 3330, loss = 0.34, batch loss = 0.26 (14.9 examples/sec; 0.536 sec/batch; 49h:01m:07s remains)
INFO - root - 2017-12-11 09:45:04.100167: step 3340, loss = 0.24, batch loss = 0.15 (15.1 examples/sec; 0.531 sec/batch; 48h:35m:10s remains)
INFO - root - 2017-12-11 09:45:09.496753: step 3350, loss = 0.36, batch loss = 0.27 (14.5 examples/sec; 0.552 sec/batch; 50h:26m:46s remains)
INFO - root - 2017-12-11 09:45:14.915050: step 3360, loss = 0.21, batch loss = 0.12 (14.5 examples/sec; 0.552 sec/batch; 50h:29m:41s remains)
INFO - root - 2017-12-11 09:45:20.346387: step 3370, loss = 0.34, batch loss = 0.26 (14.9 examples/sec; 0.539 sec/batch; 49h:14m:34s remains)
INFO - root - 2017-12-11 09:45:25.537704: step 3380, loss = 0.26, batch loss = 0.18 (15.1 examples/sec; 0.529 sec/batch; 48h:20m:04s remains)
INFO - root - 2017-12-11 09:45:31.010629: step 3390, loss = 0.34, batch loss = 0.26 (15.0 examples/sec; 0.534 sec/batch; 48h:48m:26s remains)
INFO - root - 2017-12-11 09:45:36.444343: step 3400, loss = 0.24, batch loss = 0.16 (14.8 examples/sec; 0.542 sec/batch; 49h:30m:19s remains)
2017-12-11 09:45:36.958262: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2063017 -5.4399509 -5.8165407 -6.3648233 -6.8623543 -7.1763983 -7.1977282 -7.0369244 -6.8535042 -6.6439314 -6.5951242 -6.496727 -6.0098186 -5.2988052 -4.8464122][-6.31549 -6.5393262 -6.9804144 -7.6923685 -8.2972994 -8.5434036 -8.3486595 -7.9321642 -7.5448055 -7.20661 -7.1545238 -6.917099 -6.0932312 -4.9589696 -4.2271309][-6.29527 -6.3196268 -6.7678456 -7.7258105 -8.54417 -8.8035145 -8.47176 -7.9135275 -7.4919367 -7.2303219 -7.3155966 -6.912528 -5.6721859 -4.0754275 -2.9999266][-5.2793169 -4.990375 -5.4917908 -6.7639337 -7.745595 -7.9513969 -7.4562855 -6.84291 -6.5946822 -6.64859 -7.0435295 -6.5461035 -4.9572272 -3.0245633 -1.6937776][-3.67822 -3.0863285 -3.8185511 -5.4788532 -6.4935765 -6.4288044 -5.5744772 -4.82292 -4.7753544 -5.2439489 -6.057653 -5.7209225 -4.1363506 -2.1975799 -0.75007391][-2.1834354 -1.3109961 -2.1787858 -3.8779027 -4.4982443 -3.7871633 -2.2654216 -1.257921 -1.5042772 -2.5708013 -4.0062165 -4.2713442 -3.2754791 -1.8232493 -0.485883][-1.2152483 -0.21658373 -1.0272665 -2.335855 -2.2961643 -0.88513684 1.3524661 2.6783094 2.1175675 0.44902039 -1.5393569 -2.4816148 -2.3339493 -1.6713023 -0.68582439][-1.4840715 -0.52606487 -1.0563707 -1.674726 -0.96009445 0.98059845 3.7457294 5.2837639 4.4060564 2.2591934 -0.06129694 -1.5316286 -2.1901686 -2.2835763 -1.6497757][-2.5231233 -1.6554611 -1.7545769 -1.6209817 -0.44894218 1.6216941 4.4204569 5.8389454 4.6983032 2.3669786 0.01844883 -1.7267835 -2.9535484 -3.5031943 -2.9902751][-3.6469462 -2.8655763 -2.6245265 -2.079036 -0.9619081 0.62979794 2.7760801 3.7195683 2.5744829 0.584527 -1.3504734 -2.9359641 -4.264359 -4.8739381 -4.2836328][-4.87142 -4.2578535 -3.9132607 -3.3685398 -2.7518206 -1.9942706 -0.78404641 -0.30367374 -1.1252389 -2.4384613 -3.6863379 -4.7584925 -5.7812777 -6.2097044 -5.599885][-6.0723305 -5.7442808 -5.4572792 -5.0721064 -4.9109015 -4.7213283 -4.1538849 -3.8971257 -4.3351092 -5.0661316 -5.7638378 -6.339859 -6.9177489 -7.0790205 -6.5202694][-6.8620625 -6.86382 -6.7162933 -6.4822922 -6.533988 -6.5647869 -6.3472357 -6.2377248 -6.4858503 -6.9400139 -7.3489208 -7.5548711 -7.6656451 -7.5474072 -7.0645857][-6.7618036 -7.0221782 -7.0442419 -6.9710021 -7.1583962 -7.3444 -7.3870015 -7.4419479 -7.6870041 -8.0757084 -8.344265 -8.245369 -7.9135647 -7.5354347 -7.1178226][-5.5972967 -6.0000372 -6.2041759 -6.3313642 -6.6671596 -7.0125628 -7.2256851 -7.3668318 -7.6096334 -7.9289541 -8.0408955 -7.6802487 -7.0393891 -6.5119114 -6.1649561]]...]
INFO - root - 2017-12-11 09:45:42.289794: step 3410, loss = 0.22, batch loss = 0.14 (15.1 examples/sec; 0.531 sec/batch; 48h:31m:49s remains)
INFO - root - 2017-12-11 09:45:47.685830: step 3420, loss = 0.28, batch loss = 0.19 (14.6 examples/sec; 0.546 sec/batch; 49h:57m:20s remains)
INFO - root - 2017-12-11 09:45:53.132987: step 3430, loss = 0.28, batch loss = 0.20 (14.2 examples/sec; 0.565 sec/batch; 51h:38m:06s remains)
INFO - root - 2017-12-11 09:45:58.539015: step 3440, loss = 0.40, batch loss = 0.32 (14.7 examples/sec; 0.543 sec/batch; 49h:36m:57s remains)
INFO - root - 2017-12-11 09:46:03.950469: step 3450, loss = 0.44, batch loss = 0.36 (14.2 examples/sec; 0.563 sec/batch; 51h:26m:17s remains)
INFO - root - 2017-12-11 09:46:09.261591: step 3460, loss = 0.31, batch loss = 0.22 (15.3 examples/sec; 0.524 sec/batch; 47h:53m:31s remains)
INFO - root - 2017-12-11 09:46:14.635487: step 3470, loss = 0.25, batch loss = 0.17 (14.9 examples/sec; 0.537 sec/batch; 49h:02m:12s remains)
INFO - root - 2017-12-11 09:46:19.797544: step 3480, loss = 0.34, batch loss = 0.26 (14.2 examples/sec; 0.563 sec/batch; 51h:26m:07s remains)
INFO - root - 2017-12-11 09:46:25.107817: step 3490, loss = 0.26, batch loss = 0.17 (14.9 examples/sec; 0.537 sec/batch; 49h:06m:13s remains)
INFO - root - 2017-12-11 09:46:30.468785: step 3500, loss = 0.24, batch loss = 0.16 (15.2 examples/sec; 0.528 sec/batch; 48h:14m:58s remains)
2017-12-11 09:46:31.060910: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6820083 -5.1831775 -5.0348334 -4.9949489 -5.2756648 -5.5930023 -5.4530411 -4.9513078 -4.6234369 -4.8579979 -5.1105561 -4.9354949 -4.7353711 -4.3483105 -3.6141415][-5.5049992 -4.920115 -4.8662939 -4.9901986 -5.1783314 -5.3228092 -5.0181317 -4.37791 -4.0472717 -4.47919 -4.9811993 -4.7827539 -4.3486085 -3.7430384 -2.7933292][-4.1222496 -3.6387539 -3.8679576 -4.3947988 -4.7382689 -4.8063612 -4.2991133 -3.4929931 -3.1797035 -3.8086526 -4.6154819 -4.5517831 -4.0439715 -3.3625226 -2.3371873][-2.4281197 -2.1795356 -2.7210402 -3.6458902 -4.1901255 -4.1872315 -3.46495 -2.5113766 -2.2500665 -3.0043855 -4.09533 -4.2479076 -3.6958408 -2.8967304 -1.7038944][-0.93427849 -0.86858726 -1.522438 -2.6259675 -3.292676 -3.2264745 -2.3533609 -1.3176594 -1.1061294 -1.8733447 -3.1612983 -3.5835702 -3.0544662 -2.1865153 -0.80155325][-0.77061892 -0.56373405 -0.84780383 -1.6465364 -2.1580236 -1.8964219 -0.89435959 0.15012312 0.25912142 -0.49629521 -1.8910446 -2.5933402 -2.2760446 -1.6283469 -0.27956533][-1.7213166 -1.1959221 -0.85401821 -1.0337546 -1.1885118 -0.5618546 0.64712954 1.6545167 1.5068889 0.60810614 -0.87792253 -1.8380725 -1.8439355 -1.5005579 -0.22734404][-2.2935057 -1.6652768 -0.87270808 -0.52447486 -0.39206696 0.61808062 2.1116161 3.1307211 2.7023993 1.5851846 0.041774273 -1.1346359 -1.4984872 -1.3790207 -0.10871887][-2.4112833 -1.900599 -0.94709682 -0.39090443 -0.26169157 0.94425869 2.677155 3.7564812 3.2248173 2.0724249 0.7002964 -0.43918061 -0.94244981 -0.87289715 0.32576323][-2.5887415 -2.3442385 -1.5330226 -1.1487567 -1.3819313 -0.35963583 1.3213177 2.353241 1.8807459 0.93248129 -0.027088165 -0.84597874 -1.1933641 -0.9885602 0.10001993][-2.9827569 -2.9744406 -2.4601212 -2.4229746 -3.0916653 -2.4465778 -1.0866144 -0.25194979 -0.60777855 -1.2619431 -1.8174016 -2.3150992 -2.4538622 -2.090167 -1.1219645][-2.9554608 -3.0369987 -2.8072524 -3.0750837 -4.0229158 -3.7661517 -2.86575 -2.3570955 -2.6627665 -3.1326432 -3.500977 -3.8793035 -3.9351947 -3.5066068 -2.7205584][-2.3949783 -2.4798908 -2.4807532 -2.9880397 -4.0550981 -4.131959 -3.7047186 -3.5665751 -3.9590454 -4.4427252 -4.848773 -5.2646728 -5.32051 -4.8493881 -4.1692605][-2.604296 -2.5082331 -2.5144219 -3.0555615 -4.0073895 -4.2396326 -4.1372404 -4.2398076 -4.7114463 -5.2610097 -5.7354832 -6.2161388 -6.3734303 -5.9930844 -5.4397988][-3.0163414 -2.6716506 -2.5791864 -3.077621 -3.8533888 -4.1356807 -4.2286935 -4.4111238 -4.8695712 -5.4294844 -5.916532 -6.4190426 -6.6687346 -6.4092159 -5.9983058]]...]
INFO - root - 2017-12-11 09:46:36.448380: step 3510, loss = 0.37, batch loss = 0.28 (14.3 examples/sec; 0.558 sec/batch; 50h:59m:56s remains)
INFO - root - 2017-12-11 09:46:41.846931: step 3520, loss = 0.26, batch loss = 0.18 (14.9 examples/sec; 0.537 sec/batch; 49h:01m:47s remains)
INFO - root - 2017-12-11 09:46:47.271969: step 3530, loss = 0.29, batch loss = 0.21 (14.9 examples/sec; 0.538 sec/batch; 49h:10m:02s remains)
