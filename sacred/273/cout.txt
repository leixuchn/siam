INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "273"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
inputs Tensor("batch:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
net Tensor("siamese_fc/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
inputs Tensor("batch:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_1/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
net Tensor("siamese_fc_1/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-15 06:02:21.810096: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-15 06:02:21.810133: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-15 06:02:21.810139: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-15 06:02:21.810144: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-15 06:02:21.810150: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-15 06:02:22.157710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-15 06:02:22.157749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-15 06:02:22.157756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-15 06:02:22.157764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01/model.ckpt-140000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01/model.ckpt-140000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-15 06:02:25.195902: step 0, loss = 0.19, batch loss = 0.15 (3.5 examples/sec; 2.308 sec/batch; 213h:10m:28s remains)
2017-12-15 06:02:25.540062: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1442966 -6.3151956 -7.0496902 -7.8639407 -8.2394733 -8.2790775 -7.98606 -7.192452 -6.522543 -6.5626473 -6.8909755 -7.2509685 -7.6574392 -7.8630495 -7.2196512][-5.2786031 -6.9010773 -7.7391639 -8.5907011 -8.7527275 -8.3973389 -7.671989 -6.6158967 -5.9928408 -6.3228965 -7.1253715 -7.8567553 -8.5567255 -8.9544926 -8.2184143][-6.025867 -6.7298522 -7.4640193 -8.0294895 -7.69863 -6.839407 -5.6520524 -4.2119274 -3.6121206 -4.3850269 -5.737155 -6.99707 -8.2224188 -9.0280323 -8.4015455][-6.1363821 -6.0313797 -6.6275425 -6.7585793 -5.9888806 -4.7242308 -3.0644631 -1.1299222 -0.57014287 -1.8040632 -3.7035794 -5.5380831 -7.4230714 -8.7743483 -8.3423882][-5.560286 -5.0549092 -5.5611658 -5.3294826 -4.2904506 -2.631197 -0.34086561 2.1731906 2.67416 0.98588169 -1.530417 -3.9490576 -6.5432253 -8.448082 -8.2844067][-4.5443449 -4.1286774 -4.51347 -4.0096011 -2.79056 -0.68032908 2.3679566 5.4387736 5.9346685 3.7805877 0.47704041 -2.6553662 -5.8600287 -8.1107712 -8.1729984][-3.589303 -3.5511773 -3.8919086 -3.3905005 -2.1846137 0.18323457 3.5883508 6.799746 7.2644811 4.8993144 1.0837468 -2.4655216 -5.787406 -7.9152789 -7.9766459][-3.5651023 -3.5839133 -3.9801655 -3.7446408 -2.6993997 -0.40633869 2.768364 5.6039944 6.0152316 3.7744532 -0.011804104 -3.4268761 -6.3869271 -7.9952545 -7.8753238][-4.3803468 -4.2140136 -4.6251268 -4.6649346 -3.7046232 -1.6001701 1.0840431 3.29187 3.5270634 1.546198 -1.7751631 -4.7172756 -7.0534315 -8.1198759 -7.8167377][-5.6810126 -5.2588487 -5.6342263 -5.8854513 -4.9921427 -3.1666696 -1.1373502 0.399361 0.40086544 -1.3092227 -3.9290786 -6.0379424 -7.5285869 -8.1228542 -7.6847548][-6.9096971 -6.3714137 -6.8176227 -7.2423158 -6.5076761 -5.1847544 -3.871356 -2.7889185 -2.7588191 -4.0969086 -5.8749824 -6.9757118 -7.7254176 -8.0132933 -7.5394287][-7.328639 -6.755372 -7.3233271 -7.7223015 -7.06069 -6.2424097 -5.5023308 -4.6330209 -4.4426913 -5.4427819 -6.5354018 -6.9642282 -7.449944 -7.7873259 -7.4315829][-6.9464593 -6.3628626 -6.9224358 -7.0868506 -6.3547964 -5.7861142 -5.2773528 -4.4387174 -4.1124167 -4.8777575 -5.5663872 -5.7818489 -6.5071197 -7.2458854 -7.1965137][-6.3050303 -5.7178521 -6.1969023 -6.2482467 -5.5349293 -5.0283146 -4.39865 -3.5182097 -3.1336656 -3.6790524 -4.1129336 -4.3339853 -5.391572 -6.5490251 -6.862999][-5.6501856 -4.9700584 -5.3236275 -5.49788 -4.9726586 -4.3910742 -3.5394924 -2.7120457 -2.4441328 -2.8531241 -3.2064118 -3.5992067 -4.8570089 -6.1932249 -6.6695766]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 06:02:28.084515: step 10, loss = 0.25, batch loss = 0.21 (37.0 examples/sec; 0.216 sec/batch; 19h:57m:56s remains)
INFO - root - 2017-12-15 06:02:30.245224: step 20, loss = 0.21, batch loss = 0.17 (38.0 examples/sec; 0.211 sec/batch; 19h:27m:47s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 06:02:32.470817: step 30, loss = 0.24, batch loss = 0.20 (36.6 examples/sec; 0.218 sec/batch; 20h:10m:07s remains)
INFO - root - 2017-12-15 06:02:34.659151: step 40, loss = 0.31, batch loss = 0.27 (35.2 examples/sec; 0.228 sec/batch; 21h:00m:35s remains)
INFO - root - 2017-12-15 06:02:36.856551: step 50, loss = 0.36, batch loss = 0.32 (36.9 examples/sec; 0.217 sec/batch; 20h:00m:31s remains)
INFO - root - 2017-12-15 06:02:39.070890: step 60, loss = 0.32, batch loss = 0.28 (35.3 examples/sec; 0.227 sec/batch; 20h:57m:20s remains)
INFO - root - 2017-12-15 06:02:41.232540: step 70, loss = 0.28, batch loss = 0.24 (37.8 examples/sec; 0.212 sec/batch; 19h:32m:03s remains)
INFO - root - 2017-12-15 06:02:43.401264: step 80, loss = 0.27, batch loss = 0.24 (36.1 examples/sec; 0.222 sec/batch; 20h:28m:54s remains)
INFO - root - 2017-12-15 06:02:45.608097: step 90, loss = 0.27, batch loss = 0.23 (35.9 examples/sec; 0.223 sec/batch; 20h:35m:53s remains)
INFO - root - 2017-12-15 06:02:47.827018: step 100, loss = 0.27, batch loss = 0.23 (36.3 examples/sec; 0.220 sec/batch; 20h:19m:34s remains)
2017-12-15 06:02:48.110133: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.779999 -4.3996096 -4.2640285 -2.9677846 -1.3526065 -0.532212 -0.32261443 -0.37509286 -1.0767682 -2.0656917 -2.2500064 -1.8079234 -1.902845 -1.9668981 -1.6851087][-2.8553731 -4.0193033 -3.8430526 -2.5832577 -1.0792007 -0.30532789 -0.072834134 -0.20148373 -0.97530669 -1.8346403 -1.9510723 -1.6087396 -1.7262257 -1.8279154 -1.7518133][-3.0294037 -3.7332096 -3.6107745 -2.477879 -1.0829804 -0.35807168 -0.26388633 -0.5563668 -1.2201732 -1.8133007 -1.8699175 -1.6367835 -1.7533747 -1.811066 -1.7200247][-3.497673 -3.8527088 -3.7591338 -2.7562339 -1.489305 -0.79031825 -0.71230209 -0.96939021 -1.4435325 -1.8398782 -1.9251559 -1.7431545 -1.7798772 -1.7856061 -1.7059619][-4.051671 -4.0582066 -3.87846 -2.9233859 -1.7000332 -0.9115268 -0.72772467 -0.85316646 -1.1339881 -1.4545348 -1.7162352 -1.6604173 -1.6521232 -1.6048411 -1.5229666][-4.3978443 -4.2232494 -3.8541849 -2.850709 -1.5931234 -0.63013077 -0.24124706 -0.18835568 -0.35728753 -0.810742 -1.4025259 -1.5407786 -1.5321616 -1.4764659 -1.4028385][-4.6464882 -4.3465557 -3.7781627 -2.7193844 -1.4232159 -0.30800343 0.26142645 0.46677876 0.33958769 -0.33075035 -1.27993 -1.6237822 -1.6580046 -1.5951712 -1.463053][-4.688097 -4.3497181 -3.6724708 -2.6332221 -1.3761303 -0.24311018 0.39784265 0.69576192 0.60475564 -0.22415376 -1.3409396 -1.7786833 -1.8722978 -1.9041605 -1.7540404][-4.5995502 -4.2519555 -3.6236262 -2.6987243 -1.6213843 -0.70491445 -0.20398259 0.066251993 -0.0010498762 -0.83702815 -1.7802075 -2.0659649 -2.1391506 -2.2888985 -2.1719937][-4.4341965 -4.1611781 -3.7150037 -3.0022531 -2.2252631 -1.589905 -1.2343683 -0.98663747 -1.0590498 -1.7842818 -2.4152024 -2.5177689 -2.6223702 -2.9305987 -2.8603086][-4.3398747 -4.2084012 -4.0078931 -3.5772414 -3.1304295 -2.6971424 -2.3745754 -2.1301694 -2.2527637 -2.82927 -3.1428375 -3.1503654 -3.3364711 -3.70022 -3.677567][-4.2926722 -4.310524 -4.3317642 -4.1543732 -3.9819696 -3.7244315 -3.4783473 -3.3348322 -3.516932 -3.9224119 -3.9630084 -3.8578277 -4.0224948 -4.3031225 -4.2468905][-4.1904979 -4.2359419 -4.3709211 -4.3817277 -4.3860283 -4.2614965 -4.1086526 -4.046627 -4.2372894 -4.5087481 -4.4898243 -4.4491668 -4.5809746 -4.6917324 -4.5805717][-4.1021967 -4.1036768 -4.2737408 -4.4064116 -4.4946079 -4.4208536 -4.3467917 -4.3834124 -4.5469084 -4.6836367 -4.6507854 -4.6659307 -4.7577276 -4.7454295 -4.6207361][-3.9530118 -3.8676982 -3.998327 -4.1443486 -4.2346926 -4.1885662 -4.1722345 -4.2774611 -4.4086514 -4.4575329 -4.44563 -4.4740334 -4.4742823 -4.3706803 -4.2701912]]...]
INFO - root - 2017-12-15 06:02:50.287940: step 110, loss = 0.32, batch loss = 0.28 (35.8 examples/sec; 0.223 sec/batch; 20h:36m:32s remains)
INFO - root - 2017-12-15 06:02:52.478764: step 120, loss = 0.38, batch loss = 0.34 (36.4 examples/sec; 0.220 sec/batch; 20h:19m:03s remains)
INFO - root - 2017-12-15 06:02:54.686488: step 130, loss = 0.37, batch loss = 0.33 (36.4 examples/sec; 0.220 sec/batch; 20h:15m:59s remains)
INFO - root - 2017-12-15 06:02:56.897989: step 140, loss = 0.25, batch loss = 0.21 (36.5 examples/sec; 0.219 sec/batch; 20h:12m:55s remains)
INFO - root - 2017-12-15 06:02:59.142474: step 150, loss = 0.40, batch loss = 0.36 (35.7 examples/sec; 0.224 sec/batch; 20h:39m:53s remains)
INFO - root - 2017-12-15 06:03:01.364280: step 160, loss = 0.34, batch loss = 0.30 (36.2 examples/sec; 0.221 sec/batch; 20h:24m:43s remains)
INFO - root - 2017-12-15 06:03:03.580837: step 170, loss = 0.34, batch loss = 0.30 (35.6 examples/sec; 0.225 sec/batch; 20h:43m:37s remains)
INFO - root - 2017-12-15 06:03:05.809952: step 180, loss = 0.23, batch loss = 0.19 (35.0 examples/sec; 0.229 sec/batch; 21h:06m:56s remains)
INFO - root - 2017-12-15 06:03:08.038056: step 190, loss = 0.38, batch loss = 0.34 (36.0 examples/sec; 0.222 sec/batch; 20h:30m:17s remains)
INFO - root - 2017-12-15 06:03:10.263866: step 200, loss = 0.31, batch loss = 0.27 (37.2 examples/sec; 0.215 sec/batch; 19h:50m:16s remains)
2017-12-15 06:03:10.549109: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8816276 -5.5201907 -5.36035 -5.2048965 -5.0811892 -4.9902887 -4.8557343 -4.53789 -4.2566538 -4.2733483 -4.6241326 -5.0106688 -5.1993403 -5.2348723 -5.1097178][-6.0249696 -5.9944811 -5.7258558 -5.6244392 -5.6017103 -5.5249796 -5.2899537 -4.7464085 -4.2998314 -4.39268 -4.9898477 -5.5439086 -5.7536483 -5.7619095 -5.5811081][-6.8363576 -5.9863172 -5.6588621 -5.6309686 -5.6193128 -5.4036579 -4.9192004 -4.0794711 -3.49503 -3.7540641 -4.6499548 -5.4015508 -5.6844363 -5.7057228 -5.5053377][-7.1581969 -5.9291005 -5.5114012 -5.3965063 -5.11344 -4.5142622 -3.6266069 -2.4955816 -1.9273041 -2.5198369 -3.8234992 -4.9331818 -5.4650035 -5.629653 -5.4813666][-7.4795446 -5.7816081 -5.1979485 -4.8289423 -4.1630626 -3.1521239 -1.8207715 -0.42721117 -0.037415147 -1.0631611 -2.7864819 -4.2837663 -5.1193328 -5.4991674 -5.4397321][-7.0318189 -5.1315117 -4.2467318 -3.5283942 -2.5068388 -1.1746486 0.6238271 2.2339053 2.2415915 0.66217124 -1.4555129 -3.2911386 -4.3995471 -5.0501866 -5.1631012][-5.8802233 -4.1494422 -2.9806547 -1.9419994 -0.64672792 0.99220526 3.273057 4.9971905 4.5132284 2.3826618 -0.13485289 -2.3263144 -3.6028643 -4.4552631 -4.7927041][-5.0273647 -3.0596018 -1.8079468 -0.63459814 0.71236551 2.486702 4.9624577 6.5825086 5.6549892 3.1785517 0.37175429 -2.0443542 -3.2925754 -4.1766992 -4.6333623][-4.0515776 -2.208159 -1.1655962 -0.10992992 1.1087402 2.8174524 5.1579823 6.4900222 5.3649764 2.9013853 0.13307321 -2.2103229 -3.2558551 -4.0241227 -4.5267816][-3.659394 -2.1810708 -1.6431893 -0.99679267 -0.0016692877 1.6366669 3.7282658 4.7947516 3.7903261 1.6140915 -0.80595505 -2.7606268 -3.510781 -4.1099095 -4.6031828][-3.6508763 -2.598331 -2.6322031 -2.6146009 -2.0257862 -0.57911944 1.0560743 1.8318843 1.0579115 -0.73631811 -2.5616887 -3.7965817 -4.1619034 -4.5114441 -4.8493066][-3.8530066 -3.1484456 -3.5559337 -4.0289454 -3.9069662 -2.9507995 -1.9318348 -1.5114406 -2.1163218 -3.4849825 -4.5578923 -4.9605117 -4.960104 -5.0418978 -5.1310759][-4.5785036 -4.2275543 -4.8316159 -5.4872866 -5.6694708 -5.1565804 -4.5592179 -4.3409052 -4.7399964 -5.6187706 -6.0524817 -5.8674417 -5.6158214 -5.493978 -5.4284163][-4.9988518 -4.8087163 -5.3582883 -5.9145122 -6.2374563 -6.0956616 -5.8066783 -5.7134714 -5.96622 -6.408186 -6.406702 -6.0044103 -5.730927 -5.5830469 -5.4855413][-5.0581856 -4.8506784 -5.2221408 -5.608829 -5.9478345 -6.004456 -5.9147372 -5.9417257 -6.1029649 -6.1596413 -5.9147286 -5.5641985 -5.3912463 -5.2895145 -5.1627932]]...]
INFO - root - 2017-12-15 06:03:12.799418: step 210, loss = 0.26, batch loss = 0.22 (33.6 examples/sec; 0.238 sec/batch; 21h:59m:53s remains)
INFO - root - 2017-12-15 06:03:15.041362: step 220, loss = 0.29, batch loss = 0.25 (36.8 examples/sec; 0.217 sec/batch; 20h:04m:16s remains)
INFO - root - 2017-12-15 06:03:17.272454: step 230, loss = 0.29, batch loss = 0.25 (36.3 examples/sec; 0.220 sec/batch; 20h:19m:24s remains)
INFO - root - 2017-12-15 06:03:19.495813: step 240, loss = 0.31, batch loss = 0.27 (36.9 examples/sec; 0.217 sec/batch; 20h:01m:35s remains)
INFO - root - 2017-12-15 06:03:21.732695: step 250, loss = 0.33, batch loss = 0.29 (35.5 examples/sec; 0.225 sec/batch; 20h:47m:20s remains)
INFO - root - 2017-12-15 06:03:23.978142: step 260, loss = 0.37, batch loss = 0.33 (33.0 examples/sec; 0.243 sec/batch; 22h:23m:33s remains)
INFO - root - 2017-12-15 06:03:26.225718: step 270, loss = 0.35, batch loss = 0.31 (33.5 examples/sec; 0.239 sec/batch; 22h:03m:08s remains)
INFO - root - 2017-12-15 06:03:28.453183: step 280, loss = 0.23, batch loss = 0.19 (34.8 examples/sec; 0.230 sec/batch; 21h:12m:13s remains)
INFO - root - 2017-12-15 06:03:30.673175: step 290, loss = 0.22, batch loss = 0.18 (36.2 examples/sec; 0.221 sec/batch; 20h:23m:08s remains)
INFO - root - 2017-12-15 06:03:32.912095: step 300, loss = 0.30, batch loss = 0.26 (36.5 examples/sec; 0.219 sec/batch; 20h:13m:42s remains)
2017-12-15 06:03:33.175354: I tensorflow/core/kernels/logging_ops.cc:79] [[[2.5656805 1.3637757 -0.28272367 -1.6125429 -2.6225641 -2.7512789 -2.1582673 -1.0428083 0.3447752 1.4255733 1.7759821 0.581424 -0.92382395 -1.3989174 -1.7992331][0.21612215 -0.020902157 -1.1791756 -2.2643826 -3.2093735 -3.3777769 -2.9545612 -2.0737591 -0.95391011 -0.2980864 -0.2886492 -1.2547495 -2.3596377 -2.570003 -2.7733622][-2.3653054 -1.6790073 -2.1268601 -2.6736124 -3.4973593 -3.7591698 -3.5691578 -3.0891528 -2.3572567 -2.0904138 -2.2410128 -2.8169096 -3.3640714 -3.199183 -3.2155242][-4.5130196 -3.1151977 -2.7497318 -2.5702524 -3.1536789 -3.5416532 -3.5649066 -3.5265083 -3.3106794 -3.2614784 -3.3377466 -3.4652519 -3.5531554 -3.191999 -3.2037778][-5.8137178 -3.7456169 -2.5031898 -1.4421272 -1.6566734 -2.178086 -2.4567411 -3.0156655 -3.4236169 -3.4091835 -3.0783174 -2.7152026 -2.546927 -2.2149386 -2.4694903][-6.2308311 -3.6181386 -1.842662 -0.10246396 0.14295268 -0.308447 -0.68731678 -1.771363 -2.6807528 -2.5284173 -1.7613312 -1.0687485 -1.001003 -0.99897373 -1.5981141][-5.6066208 -2.8711984 -1.0323378 0.87749124 1.495594 1.3160667 1.1218357 -0.080387712 -1.1437409 -0.74067807 0.26377749 0.95122457 0.56272674 -0.039664507 -1.0762362][-4.5593305 -1.6192188 0.0733099 1.7946682 2.5061307 2.3998327 2.1976023 0.97186995 0.016940355 0.73241568 1.893321 2.5524168 1.8191531 0.70589662 -0.60541856][-3.762645 -0.87068152 0.57224154 1.9464319 2.7052546 2.6759152 2.3137155 1.0258093 0.22620988 1.1838059 2.5294547 3.34063 2.5190549 1.0949214 -0.31357229][-3.7844071 -1.3711767 -0.30170643 0.61761 1.3202906 1.4418323 1.1089206 0.034532547 -0.46727085 0.7037673 2.0936284 2.9704676 2.1916981 0.57380056 -0.86035931][-4.0743122 -2.4663877 -1.8431159 -1.5493189 -1.2052984 -1.0482073 -1.2757523 -1.9163705 -1.9550059 -0.68758821 0.52581596 1.2916257 0.64070964 -1.0363531 -2.455687][-4.391407 -3.7087541 -3.5150819 -3.7687926 -3.7767 -3.5377207 -3.541677 -3.68988 -3.306859 -2.1332753 -1.3418957 -0.92783284 -1.4208672 -2.9193044 -4.1856432][-4.3283944 -4.5537415 -4.6708431 -5.3116837 -5.5069056 -5.0741005 -4.7573996 -4.4758897 -3.9116106 -2.976424 -2.679065 -2.7339861 -3.14548 -4.4004364 -5.4101367][-4.1356478 -4.8971124 -5.1680393 -6.0102382 -6.4758444 -6.1072116 -5.6446629 -5.0268726 -4.2200127 -3.3638539 -3.3897865 -3.7898915 -4.1308861 -5.125864 -5.8610611][-4.3408494 -5.0335627 -5.2217031 -6.0334449 -6.6916628 -6.5263462 -6.1004839 -5.2637482 -4.1824479 -3.332849 -3.4733062 -3.9510756 -4.1671448 -4.9222212 -5.4217138]]...]
INFO - root - 2017-12-15 06:03:35.406727: step 310, loss = 0.35, batch loss = 0.31 (35.7 examples/sec; 0.224 sec/batch; 20h:40m:49s remains)
INFO - root - 2017-12-15 06:03:37.626693: step 320, loss = 0.24, batch loss = 0.20 (35.7 examples/sec; 0.224 sec/batch; 20h:39m:52s remains)
INFO - root - 2017-12-15 06:03:39.889269: step 330, loss = 0.27, batch loss = 0.23 (36.2 examples/sec; 0.221 sec/batch; 20h:23m:16s remains)
INFO - root - 2017-12-15 06:03:42.120305: step 340, loss = 0.26, batch loss = 0.22 (36.3 examples/sec; 0.220 sec/batch; 20h:19m:10s remains)
INFO - root - 2017-12-15 06:03:44.335812: step 350, loss = 0.31, batch loss = 0.28 (36.4 examples/sec; 0.220 sec/batch; 20h:17m:43s remains)
INFO - root - 2017-12-15 06:03:46.601048: step 360, loss = 0.49, batch loss = 0.45 (35.9 examples/sec; 0.223 sec/batch; 20h:32m:30s remains)
INFO - root - 2017-12-15 06:03:48.868931: step 370, loss = 0.31, batch loss = 0.27 (35.2 examples/sec; 0.227 sec/batch; 20h:58m:12s remains)
INFO - root - 2017-12-15 06:03:51.096286: step 380, loss = 0.33, batch loss = 0.30 (37.3 examples/sec; 0.215 sec/batch; 19h:48m:06s remains)
INFO - root - 2017-12-15 06:03:53.342074: step 390, loss = 0.28, batch loss = 0.24 (35.3 examples/sec; 0.227 sec/batch; 20h:54m:26s remains)
INFO - root - 2017-12-15 06:03:55.582045: step 400, loss = 0.20, batch loss = 0.16 (35.6 examples/sec; 0.225 sec/batch; 20h:43m:50s remains)
2017-12-15 06:03:55.869981: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3822951 -5.9554887 -5.9295821 -5.9453216 -6.0851336 -6.1738348 -6.4749207 -6.683341 -7.0335546 -7.4980478 -7.913969 -7.9566326 -7.9176631 -7.9382205 -8.2241755][-6.0939107 -6.5063424 -6.3729634 -6.3962927 -6.6106534 -6.6392283 -6.6324072 -6.787498 -7.3420334 -8.0976772 -8.5912952 -8.5506725 -8.1146 -7.9444804 -8.424159][-6.7102742 -5.9720025 -5.5823879 -5.40743 -5.4988737 -5.1960573 -4.7389641 -4.6605783 -5.4585433 -6.6580787 -7.3356133 -7.1398234 -6.4035721 -6.1878238 -7.0201674][-6.2192688 -4.7018991 -4.0663843 -3.8181643 -3.8931203 -3.4258804 -2.6338944 -2.3364782 -3.307482 -4.9138465 -5.751833 -5.4521351 -4.5434794 -4.3683228 -5.5564456][-4.846653 -2.8565629 -1.9646502 -1.6929787 -1.9542729 -1.6458802 -0.79756975 -0.32586825 -1.311753 -3.1495285 -4.1058884 -3.7329464 -2.7916818 -2.7782164 -4.3327475][-4.3944488 -1.8260422 -0.61538363 -0.075232267 -0.11017394 0.26400077 1.1605834 1.7959219 0.88398778 -1.1137652 -2.1774607 -1.873729 -1.2035899 -1.5522974 -3.5277812][-3.3547585 -1.0139142 0.12812054 0.66342604 0.743925 1.0922581 1.9400164 2.74785 2.150063 0.3056072 -0.68876207 -0.51952994 -0.2934792 -1.0221741 -3.2589059][-2.9561949 -0.54250395 0.35030758 0.63597238 0.58959043 0.73967731 1.4204978 2.3699079 2.2708063 0.86577094 0.024153709 -0.15819383 -0.60181379 -1.7003115 -3.8890367][-2.7685773 -0.071298 0.7611326 0.89562166 0.53326142 0.16568077 0.34952223 1.201522 1.4672269 0.5526408 -0.065433979 -0.60137951 -1.6748427 -3.073097 -5.0968609][-2.368325 0.44665468 1.100341 0.99769247 0.12830198 -1.0610602 -1.6499661 -1.0912344 -0.523131 -0.87982929 -1.1229753 -1.6661626 -2.9772923 -4.4490995 -6.2142439][-2.2312074 0.60827625 1.1714412 1.0812048 0.037212968 -1.7712487 -3.1098008 -2.9196336 -2.276612 -2.368129 -2.5216289 -3.1208282 -4.4235282 -5.685071 -7.0304475][-2.3831131 0.393597 0.93810856 0.94963682 -0.058879733 -2.1660774 -3.9884334 -4.1005716 -3.458853 -3.4598157 -3.7458549 -4.4721656 -5.6967907 -6.7305346 -7.5788374][-3.1427031 -0.7869128 -0.49921882 -0.53821266 -1.433959 -3.4502738 -5.2312751 -5.2694855 -4.4885969 -4.2886391 -4.5866265 -5.3386664 -6.4409237 -7.2454896 -7.7166438][-4.6645846 -2.608649 -2.1705213 -1.9716787 -2.5743639 -4.2818294 -5.804781 -5.83323 -5.162456 -4.8938441 -5.0945306 -5.7244759 -6.5855222 -7.1663733 -7.3848138][-5.4461823 -3.57036 -2.9524312 -2.5683823 -3.0159624 -4.3445697 -5.4601631 -5.468739 -5.0470738 -4.8562212 -4.9791822 -5.4143147 -6.0207911 -6.4384136 -6.5699863]]...]
INFO - root - 2017-12-15 06:03:58.116944: step 410, loss = 0.31, batch loss = 0.27 (36.8 examples/sec; 0.218 sec/batch; 20h:04m:41s remains)
INFO - root - 2017-12-15 06:04:00.354473: step 420, loss = 0.33, batch loss = 0.29 (36.3 examples/sec; 0.220 sec/batch; 20h:18m:38s remains)
INFO - root - 2017-12-15 06:04:02.592086: step 430, loss = 0.32, batch loss = 0.28 (35.3 examples/sec; 0.226 sec/batch; 20h:53m:14s remains)
INFO - root - 2017-12-15 06:04:04.818799: step 440, loss = 0.30, batch loss = 0.26 (36.0 examples/sec; 0.222 sec/batch; 20h:30m:53s remains)
INFO - root - 2017-12-15 06:04:07.089124: step 450, loss = 0.29, batch loss = 0.25 (34.4 examples/sec; 0.233 sec/batch; 21h:26m:41s remains)
INFO - root - 2017-12-15 06:04:09.345244: step 460, loss = 0.30, batch loss = 0.26 (36.0 examples/sec; 0.222 sec/batch; 20h:28m:57s remains)
INFO - root - 2017-12-15 06:04:11.593511: step 470, loss = 0.22, batch loss = 0.18 (35.2 examples/sec; 0.227 sec/batch; 20h:57m:06s remains)
INFO - root - 2017-12-15 06:04:13.837266: step 480, loss = 0.24, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 20h:28m:51s remains)
INFO - root - 2017-12-15 06:04:16.079984: step 490, loss = 0.25, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 20h:40m:20s remains)
INFO - root - 2017-12-15 06:04:18.298674: step 500, loss = 0.30, batch loss = 0.27 (35.5 examples/sec; 0.225 sec/batch; 20h:47m:08s remains)
2017-12-15 06:04:18.584553: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7713945 -4.3127627 -4.2805195 -3.7063329 -3.2399559 -3.3210545 -3.5543523 -3.7593696 -3.9798632 -4.0651464 -3.8089037 -3.3420918 -2.856329 -2.7467675 -2.8860042][-4.0002317 -3.9509134 -3.7328367 -2.9589872 -2.4713323 -2.6231594 -3.0239887 -3.4214096 -3.741061 -3.737534 -3.4082274 -3.0121479 -2.5005858 -2.2204568 -2.2112544][-4.1433282 -3.6418993 -3.3068869 -2.5147262 -2.1967692 -2.5382605 -3.118536 -3.681756 -3.9032536 -3.4728041 -2.8020346 -2.2955775 -1.6611502 -1.2059535 -1.1637919][-3.774071 -2.8503923 -2.4506853 -1.8016487 -1.8027744 -2.3796008 -3.1647453 -3.9473898 -4.0104122 -3.0555217 -1.8881099 -1.1658245 -0.50419104 -0.04145968 -0.22847438][-3.4066763 -2.1644261 -1.7751163 -1.2529566 -1.409229 -1.9871348 -2.7876518 -3.6617484 -3.5719619 -2.1182775 -0.43026435 0.43226039 0.86665213 1.0062469 0.38042223][-3.0820322 -1.7321485 -1.3053374 -0.67158628 -0.59299469 -0.74739027 -1.1742449 -1.8645517 -1.5847944 0.060878634 1.8554281 2.5405984 2.4291139 1.9558018 0.6960057][-2.699053 -1.5027719 -1.0721862 -0.16037214 0.41370142 0.92144692 1.1818694 0.8197006 1.0539609 2.3086047 3.60786 3.776597 3.1213622 2.1700454 0.51574767][-2.4234908 -1.1769054 -0.80992937 0.19620359 1.1889457 2.3191261 3.2350821 3.1100426 2.9540043 3.3940697 3.8537049 3.4205523 2.4309077 1.3478202 -0.26581633][-2.251204 -0.91482234 -0.64411366 0.17403948 1.2052635 2.4765697 3.5691881 3.3987441 2.7558923 2.5195336 2.2805033 1.438538 0.50060022 -0.34795988 -1.4587855][-2.159183 -0.747761 -0.68448293 -0.32385409 0.32253635 1.1987509 1.95335 1.621806 0.84381759 0.41509068 -0.1214627 -0.97768205 -1.4999679 -1.8500391 -2.2630019][-1.6376053 -0.35328352 -0.59619272 -0.76804447 -0.66856921 -0.39791906 -0.12720966 -0.5056684 -1.0558441 -1.2316692 -1.6781496 -2.3463144 -2.3975937 -2.2898765 -2.1531479][-1.369319 -0.40328574 -0.8503902 -1.3033357 -1.520442 -1.595394 -1.54595 -1.6908048 -1.8125168 -1.6345249 -1.8841946 -2.3703175 -2.2081814 -1.9441106 -1.6200922][-1.5057437 -0.88489175 -1.391197 -1.7820009 -2.0024419 -2.1854451 -2.1733735 -2.0625658 -1.8582928 -1.4961053 -1.5868959 -1.8998319 -1.6688625 -1.4313529 -1.186038][-1.8201371 -1.4457364 -1.8304361 -1.9278039 -1.9683285 -2.1544495 -2.1700633 -1.944834 -1.6695389 -1.3590069 -1.4009445 -1.543196 -1.2693696 -1.1317655 -1.1147125][-2.3538654 -2.1371961 -2.3539538 -2.2321968 -2.1975989 -2.4512916 -2.5077214 -2.2882078 -2.1042101 -1.9276443 -1.9345691 -1.8777704 -1.4836818 -1.3734763 -1.5510135]]...]
INFO - root - 2017-12-15 06:04:20.856399: step 510, loss = 0.28, batch loss = 0.24 (34.8 examples/sec; 0.230 sec/batch; 21h:12m:31s remains)
INFO - root - 2017-12-15 06:04:23.112704: step 520, loss = 0.25, batch loss = 0.21 (35.1 examples/sec; 0.228 sec/batch; 20h:59m:31s remains)
INFO - root - 2017-12-15 06:04:25.359943: step 530, loss = 0.29, batch loss = 0.25 (36.0 examples/sec; 0.222 sec/batch; 20h:29m:44s remains)
INFO - root - 2017-12-15 06:04:27.598242: step 540, loss = 0.24, batch loss = 0.20 (36.2 examples/sec; 0.221 sec/batch; 20h:21m:00s remains)
INFO - root - 2017-12-15 06:04:29.826477: step 550, loss = 0.30, batch loss = 0.26 (35.8 examples/sec; 0.224 sec/batch; 20h:37m:43s remains)
INFO - root - 2017-12-15 06:04:32.060581: step 560, loss = 0.28, batch loss = 0.24 (36.0 examples/sec; 0.222 sec/batch; 20h:30m:24s remains)
INFO - root - 2017-12-15 06:04:34.286370: step 570, loss = 0.26, batch loss = 0.22 (35.9 examples/sec; 0.223 sec/batch; 20h:33m:14s remains)
INFO - root - 2017-12-15 06:04:36.514683: step 580, loss = 0.26, batch loss = 0.22 (36.1 examples/sec; 0.222 sec/batch; 20h:25m:21s remains)
INFO - root - 2017-12-15 06:04:38.758651: step 590, loss = 0.31, batch loss = 0.27 (35.9 examples/sec; 0.223 sec/batch; 20h:31m:58s remains)
INFO - root - 2017-12-15 06:04:41.042277: step 600, loss = 0.22, batch loss = 0.18 (35.0 examples/sec; 0.228 sec/batch; 21h:03m:34s remains)
2017-12-15 06:04:41.326767: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.9022132 2.5198097 1.9348582 0.75377214 0.29415405 0.7555989 0.98705447 0.32966912 -0.24851143 -1.0126809 -1.6445012 -2.0732284 -2.6388254 -3.1784019 -3.04747][1.9734021 3.1461782 2.4787478 1.0443274 0.397488 0.91791308 1.3107375 0.75708234 0.16374242 -0.697242 -1.536386 -2.1352072 -2.8637309 -3.5687213 -3.4560328][1.1140057 2.6234097 1.9257513 0.29603493 -0.51907766 0.018928409 0.66636527 0.35382235 -0.23685181 -1.1619314 -2.101841 -2.6786673 -3.1873305 -3.6444182 -3.3357654][-0.96735597 0.49844658 -0.031416893 -1.3219125 -1.8369291 -1.1813422 -0.29237115 -0.34944785 -0.9465704 -1.8867022 -2.7997782 -3.2152557 -3.3605719 -3.3986931 -2.8506551][-3.510036 -2.2318027 -2.273396 -2.6082845 -2.3710966 -1.5018146 -0.53088534 -0.50448394 -1.254845 -2.2978685 -3.1751137 -3.4695282 -3.3037639 -2.9769416 -2.3054][-4.6779714 -3.5765507 -3.0499523 -2.4603631 -1.6152707 -0.73265529 0.031873584 0.023314 -0.87547708 -2.036278 -2.8455577 -3.0816014 -2.747509 -2.2529674 -1.6891897][-4.4742994 -3.3833361 -2.2897 -0.98596704 0.1803242 0.91030848 1.2945515 1.0313295 -0.046399117 -1.3542581 -2.1112156 -2.3638089 -2.1165431 -1.716107 -1.4312186][-4.2143946 -2.7463906 -1.2510526 0.48068821 1.7743372 2.4088206 2.4993215 1.9426237 0.66414726 -0.79826713 -1.5873778 -1.9909543 -1.9521279 -1.7716503 -1.7426056][-3.6665716 -2.2133074 -0.77034581 0.80755794 1.8342239 2.2151918 2.1066322 1.4495429 0.18024385 -1.2846463 -2.0665994 -2.5226011 -2.5483692 -2.3946722 -2.3876162][-3.373661 -2.2520437 -1.2266061 -0.11639988 0.43928587 0.44579136 0.12374842 -0.48926246 -1.4559119 -2.6324697 -3.2105927 -3.5173872 -3.4036582 -3.1061223 -2.9664974][-3.177937 -2.41123 -1.8992982 -1.3266008 -1.213197 -1.4875548 -1.8996437 -2.3148971 -2.882448 -3.7230108 -4.0514927 -4.1405706 -3.8674343 -3.4078813 -3.1029696][-2.8071754 -2.1674633 -2.0037677 -1.8371249 -2.0564432 -2.5119586 -2.9159803 -3.0656123 -3.3007555 -3.8922906 -4.1237864 -4.1696124 -3.9035487 -3.3315248 -2.8526986][-2.5543511 -1.8651557 -1.8288187 -1.8514658 -2.2313633 -2.7547326 -3.0993588 -3.0284524 -3.0771141 -3.4782255 -3.6445429 -3.7534437 -3.6218719 -3.0482452 -2.4753902][-2.403456 -1.6709704 -1.7041609 -1.8199087 -2.2046187 -2.6230145 -2.7762244 -2.4886019 -2.4020541 -2.6548505 -2.7814419 -2.9857769 -3.0027905 -2.5103166 -1.9832983][-2.2164364 -1.491529 -1.5168656 -1.6491576 -1.9829952 -2.2585583 -2.2694049 -1.9243082 -1.850636 -2.0569518 -2.187438 -2.4772296 -2.5929236 -2.1830964 -1.7190363]]...]
INFO - root - 2017-12-15 06:04:43.566842: step 610, loss = 0.29, batch loss = 0.25 (33.5 examples/sec; 0.239 sec/batch; 22h:02m:09s remains)
INFO - root - 2017-12-15 06:04:45.821587: step 620, loss = 0.23, batch loss = 0.20 (34.9 examples/sec; 0.229 sec/batch; 21h:09m:01s remains)
INFO - root - 2017-12-15 06:04:48.046638: step 630, loss = 0.22, batch loss = 0.19 (35.2 examples/sec; 0.227 sec/batch; 20h:55m:42s remains)
INFO - root - 2017-12-15 06:04:50.284348: step 640, loss = 0.39, batch loss = 0.35 (34.8 examples/sec; 0.230 sec/batch; 21h:09m:52s remains)
INFO - root - 2017-12-15 06:04:52.507495: step 650, loss = 0.30, batch loss = 0.26 (35.6 examples/sec; 0.225 sec/batch; 20h:42m:49s remains)
INFO - root - 2017-12-15 06:04:54.772878: step 660, loss = 0.24, batch loss = 0.20 (36.4 examples/sec; 0.220 sec/batch; 20h:16m:23s remains)
INFO - root - 2017-12-15 06:04:56.993848: step 670, loss = 0.33, batch loss = 0.29 (35.8 examples/sec; 0.223 sec/batch; 20h:35m:22s remains)
INFO - root - 2017-12-15 06:04:59.239696: step 680, loss = 0.43, batch loss = 0.39 (36.4 examples/sec; 0.220 sec/batch; 20h:16m:59s remains)
INFO - root - 2017-12-15 06:05:01.521012: step 690, loss = 0.29, batch loss = 0.25 (35.4 examples/sec; 0.226 sec/batch; 20h:50m:25s remains)
INFO - root - 2017-12-15 06:05:03.777757: step 700, loss = 0.28, batch loss = 0.24 (34.1 examples/sec; 0.235 sec/batch; 21h:37m:50s remains)
2017-12-15 06:05:04.067490: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5890784 -3.9595757 -5.2621617 -5.8926105 -5.7940989 -5.9301128 -5.9347878 -5.7461972 -5.8738594 -6.241179 -6.4453254 -6.6487012 -6.8617873 -6.6121259 -5.9206352][-3.87665 -5.0651588 -6.3375473 -6.7660947 -6.5599637 -6.4624481 -6.1072297 -5.6584668 -5.7673311 -6.2283993 -6.5730557 -6.8398919 -6.8441148 -6.3252764 -5.4958363][-5.283535 -6.0925183 -7.0903769 -7.2106662 -6.9042258 -6.5553856 -5.834403 -5.1370125 -5.2137766 -5.669838 -6.0673676 -6.3491621 -6.1684442 -5.5499697 -4.8396959][-6.3703694 -6.7764907 -7.5280013 -7.3958898 -6.8023877 -5.9490266 -4.7225823 -3.8714397 -4.1544256 -4.859931 -5.4804525 -5.8803964 -5.5725932 -4.9390426 -4.4961958][-7.4846826 -7.2505264 -7.5583506 -7.010251 -5.8820448 -4.2178774 -2.2341158 -1.203185 -1.9266944 -3.3226953 -4.4467196 -4.9193444 -4.3502922 -3.4947948 -3.1111665][-7.4921865 -6.8564954 -6.7970567 -5.995605 -4.3684487 -1.8800896 0.87184644 2.0915227 0.97089195 -1.2583697 -3.1331244 -3.8453927 -3.0826545 -2.0142784 -1.5546733][-6.5686793 -5.7094584 -5.392365 -4.3222084 -2.1378584 1.0405076 4.2600436 5.5282474 4.1920905 1.1748796 -1.5472416 -2.6676714 -1.9963164 -0.95247877 -0.35780013][-6.0140743 -4.9789772 -4.4019423 -2.9017613 -0.12132502 3.5707836 6.8761668 7.931231 6.3725791 2.839406 -0.28382325 -1.6295187 -1.1805798 -0.14992094 0.75854754][-5.6771116 -4.8449821 -4.2992706 -2.8720138 -0.10340226 3.599628 6.7870488 7.7343535 6.1770263 2.6903214 -0.29851615 -1.6318088 -1.5248787 -0.78023195 0.21474147][-5.6535039 -5.0092754 -4.7996039 -3.9164655 -1.7073876 1.4800155 4.2601366 5.1157146 3.8965578 1.0806215 -1.2555771 -2.4604521 -2.7668612 -2.4711833 -1.7169471][-5.906992 -5.3606997 -5.3895125 -4.9472985 -3.4217868 -1.0659759 0.94700241 1.5118775 0.63060856 -1.2454975 -2.6206875 -3.3038583 -3.5330071 -3.3161745 -2.7877572][-6.1944256 -5.6940093 -5.8402157 -5.6624069 -4.6935635 -3.1631291 -2.0083165 -1.9263945 -2.6400533 -3.6382413 -4.0775909 -4.1685038 -4.1071091 -3.8460081 -3.6508718][-6.6050048 -6.2338881 -6.4911666 -6.4350986 -5.7589436 -4.7539058 -4.1278496 -4.23866 -4.7236958 -5.1217403 -5.0007949 -4.7904949 -4.6507883 -4.5810232 -4.6988621][-6.6731415 -6.4166951 -6.6717963 -6.5820923 -6.0239019 -5.3758068 -5.0386744 -5.2626767 -5.6811571 -5.8520718 -5.3942113 -4.9814591 -4.8158336 -4.776022 -4.9706697][-6.4913535 -6.2312946 -6.3918123 -6.2437906 -5.6686783 -5.226182 -5.0734978 -5.4325686 -5.9544544 -6.1043558 -5.5760565 -5.1105223 -4.9124112 -4.8307085 -5.0829029]]...]
INFO - root - 2017-12-15 06:05:06.325984: step 710, loss = 0.36, batch loss = 0.33 (36.8 examples/sec; 0.217 sec/batch; 20h:02m:38s remains)
INFO - root - 2017-12-15 06:05:08.583935: step 720, loss = 0.30, batch loss = 0.26 (35.7 examples/sec; 0.224 sec/batch; 20h:40m:17s remains)
INFO - root - 2017-12-15 06:05:10.836917: step 730, loss = 0.30, batch loss = 0.26 (36.2 examples/sec; 0.221 sec/batch; 20h:23m:26s remains)
INFO - root - 2017-12-15 06:05:13.106365: step 740, loss = 0.41, batch loss = 0.37 (34.7 examples/sec; 0.231 sec/batch; 21h:15m:01s remains)
INFO - root - 2017-12-15 06:05:15.363107: step 750, loss = 0.30, batch loss = 0.26 (34.1 examples/sec; 0.235 sec/batch; 21h:37m:08s remains)
INFO - root - 2017-12-15 06:05:17.627505: step 760, loss = 0.27, batch loss = 0.23 (34.7 examples/sec; 0.230 sec/batch; 21h:13m:14s remains)
INFO - root - 2017-12-15 06:05:19.878373: step 770, loss = 0.25, batch loss = 0.21 (35.1 examples/sec; 0.228 sec/batch; 21h:00m:08s remains)
INFO - root - 2017-12-15 06:05:22.141823: step 780, loss = 0.49, batch loss = 0.45 (36.1 examples/sec; 0.221 sec/batch; 20h:24m:20s remains)
INFO - root - 2017-12-15 06:05:24.417408: step 790, loss = 0.29, batch loss = 0.25 (34.8 examples/sec; 0.230 sec/batch; 21h:12m:10s remains)
INFO - root - 2017-12-15 06:05:26.663391: step 800, loss = 0.24, batch loss = 0.20 (36.5 examples/sec; 0.219 sec/batch; 20h:13m:13s remains)
2017-12-15 06:05:26.933500: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7497883 -4.3006697 -3.5196629 -3.3813052 -4.0457134 -5.0843935 -5.6277552 -4.9364843 -3.5319476 -2.1824687 -1.1738381 -1.3682183 -2.047869 -2.3469455 -2.3119421][-5.5256643 -5.9847369 -5.095696 -4.5893135 -4.7764864 -5.28382 -5.3353148 -4.4299107 -3.2399282 -2.3104718 -1.4862173 -1.3954096 -1.5622318 -1.7011684 -1.6684576][-6.4291177 -6.3941531 -5.5306678 -4.916348 -4.7887764 -4.7612805 -4.241631 -3.0634556 -2.1739392 -1.7883751 -1.2664936 -0.98721004 -0.726998 -0.78940392 -0.86064219][-6.7291064 -6.2311349 -5.3937979 -4.7250948 -4.2996092 -3.771841 -2.7573967 -1.4357586 -1.0108073 -1.3115878 -1.2427175 -0.867254 -0.3056258 -0.28670073 -0.37348878][-6.4717813 -5.8297081 -5.1851535 -4.4784155 -3.596427 -2.3701227 -0.78208065 0.51629186 0.20129466 -0.97760141 -1.5250769 -1.2276757 -0.5821712 -0.50902832 -0.51762843][-5.7271953 -5.2538152 -5.02734 -4.3526497 -2.8714561 -0.70493019 1.5796151 2.7488656 1.5648208 -0.60019362 -1.8427154 -1.759123 -1.17339 -1.0112413 -0.86088252][-4.5535507 -4.4164176 -4.5248814 -3.7852135 -1.5926441 1.5297537 4.353992 5.2457643 3.251399 0.26895213 -1.442086 -1.5375752 -1.1523275 -1.02937 -0.82491362][-3.9059658 -3.9240971 -4.1707697 -3.2494764 -0.34166682 3.637991 6.9141207 7.6289892 5.1356235 1.7043078 -0.27233183 -0.64134836 -0.64031732 -0.8540796 -0.83237624][-4.1010876 -4.0945482 -4.4676843 -3.6502891 -0.59447467 3.5230689 6.6699572 7.0386086 4.3610258 0.926198 -0.93758035 -1.2809443 -1.3275454 -1.6116111 -1.6197842][-4.495307 -4.3693705 -4.9119797 -4.5093608 -1.94994 1.5045073 3.8042889 3.5406723 0.82501268 -2.3078349 -3.7255015 -3.6418695 -3.2866988 -3.2207007 -3.0644898][-4.096015 -3.7242494 -4.3692851 -4.4782066 -2.7679241 -0.32056284 0.99576616 0.22437906 -2.2616451 -4.8612828 -5.8994131 -5.5155821 -4.9240789 -4.6961365 -4.5920944][-2.9851749 -2.440594 -3.1062074 -3.6110494 -2.7983711 -1.4654903 -1.1327116 -2.3012972 -4.3491406 -6.2949018 -7.0627646 -6.5523367 -5.9390292 -5.728282 -5.6894794][-2.5166171 -1.9960599 -2.5942392 -3.2258363 -3.0920477 -2.6837103 -3.1344759 -4.6182394 -6.3592849 -7.8079157 -8.2982674 -7.52838 -6.6136603 -6.0295911 -5.7030382][-1.9026526 -1.634352 -2.3708827 -3.1593516 -3.5088944 -3.6123819 -4.3142128 -5.7212877 -7.0451746 -8.108078 -8.4638042 -7.5441422 -6.3877621 -5.503243 -4.9506874][-1.2641864 -1.3972251 -2.470005 -3.4341791 -3.9550195 -4.002027 -4.3622246 -5.3026195 -5.9924212 -6.5648551 -6.829864 -6.0447407 -5.0346355 -4.3662882 -4.1050715]]...]
INFO - root - 2017-12-15 06:05:29.190797: step 810, loss = 0.34, batch loss = 0.30 (35.1 examples/sec; 0.228 sec/batch; 20h:58m:11s remains)
INFO - root - 2017-12-15 06:05:31.476470: step 820, loss = 0.24, batch loss = 0.20 (34.9 examples/sec; 0.229 sec/batch; 21h:05m:46s remains)
INFO - root - 2017-12-15 06:05:33.739197: step 830, loss = 0.37, batch loss = 0.33 (35.5 examples/sec; 0.225 sec/batch; 20h:46m:13s remains)
INFO - root - 2017-12-15 06:05:35.991617: step 840, loss = 0.27, batch loss = 0.23 (36.1 examples/sec; 0.221 sec/batch; 20h:23m:35s remains)
INFO - root - 2017-12-15 06:05:38.229244: step 850, loss = 0.28, batch loss = 0.24 (36.4 examples/sec; 0.220 sec/batch; 20h:13m:50s remains)
INFO - root - 2017-12-15 06:05:40.491275: step 860, loss = 0.41, batch loss = 0.37 (36.0 examples/sec; 0.222 sec/batch; 20h:28m:11s remains)
INFO - root - 2017-12-15 06:05:42.745030: step 870, loss = 0.29, batch loss = 0.25 (36.2 examples/sec; 0.221 sec/batch; 20h:22m:19s remains)
INFO - root - 2017-12-15 06:05:44.984404: step 880, loss = 0.48, batch loss = 0.45 (35.6 examples/sec; 0.225 sec/batch; 20h:42m:12s remains)
INFO - root - 2017-12-15 06:05:47.229762: step 890, loss = 0.31, batch loss = 0.27 (36.2 examples/sec; 0.221 sec/batch; 20h:21m:19s remains)
INFO - root - 2017-12-15 06:05:49.470327: step 900, loss = 0.24, batch loss = 0.20 (36.6 examples/sec; 0.219 sec/batch; 20h:09m:20s remains)
2017-12-15 06:05:49.796762: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0570426 -3.3265672 -2.8090169 -1.9573914 -1.2095735 -0.58732092 -0.14815998 0.00086534023 -0.094820261 -0.66093624 -1.4309746 -2.1987345 -2.7088137 -2.8650329 -2.9102502][-3.3742008 -3.2986467 -2.749068 -1.9291407 -1.2691236 -0.77045846 -0.45518839 -0.46973431 -0.77325964 -1.277946 -2.1040325 -3.1284938 -3.971715 -4.2712584 -4.1325469][-3.2466767 -2.8384514 -2.3173478 -1.6744901 -1.1779655 -0.85078371 -0.69050181 -0.8736186 -1.2720901 -1.7912869 -2.7024517 -3.86967 -4.8208723 -5.049212 -4.5332026][-3.0708733 -2.4833069 -2.0892355 -1.6587847 -1.3580568 -1.1638416 -1.1051307 -1.2962434 -1.6664873 -2.1906912 -3.0803175 -4.1548414 -4.9227118 -4.8783827 -3.98589][-3.1119108 -2.3359275 -2.0836411 -1.8113788 -1.6929992 -1.5535246 -1.3916739 -1.4070992 -1.5967098 -2.0656717 -2.8750865 -3.7449348 -4.2764263 -3.9988408 -2.9539311][-2.7604976 -1.9290583 -1.8219488 -1.7037386 -1.6968582 -1.5815415 -1.2480929 -1.0480702 -1.0446219 -1.4567488 -2.2474029 -3.0118847 -3.42305 -2.9616027 -1.8495699][-1.7303383 -0.89806783 -0.77206624 -0.700426 -0.71702623 -0.6082375 -0.18756139 0.27002227 0.55505574 0.3162123 -0.38563418 -1.1051563 -1.420944 -0.867764 0.12068355][-1.5191061 -0.3777262 -0.13060319 -0.031254649 0.012410045 0.21087468 0.75252616 1.4501389 2.0348139 2.0547013 1.451665 0.61156547 0.091650844 0.33816779 0.95926416][-1.7943933 -0.48657 -0.1073786 0.024302006 0.11487067 0.26319683 0.64945614 1.3102826 2.0663328 2.3900466 1.9923397 1.139201 0.42858088 0.30789554 0.54468977][-2.5280149 -1.2106621 -0.712646 -0.42570579 -0.15630448 0.021771312 0.16219866 0.51040328 1.1495625 1.7004801 1.6077801 0.89614809 0.10639465 -0.33058167 -0.40625179][-3.5645325 -2.453985 -1.9643776 -1.5507255 -1.0282173 -0.61961877 -0.55589223 -0.58892107 -0.40247655 0.0067869425 0.075579762 -0.43412006 -1.1223865 -1.6614242 -1.8890626][-4.6060939 -3.7798235 -3.4111352 -3.0482249 -2.455694 -1.850186 -1.643718 -1.8479881 -2.1459465 -2.1310232 -2.0984175 -2.3362255 -2.7296052 -3.1444926 -3.3714058][-4.9375243 -4.4030852 -4.2479467 -4.0678363 -3.6087651 -2.9682922 -2.5787487 -2.693794 -3.1976953 -3.4962838 -3.5955455 -3.6896729 -3.8285742 -4.0510597 -4.1531353][-4.6611814 -4.2988563 -4.3193951 -4.36392 -4.2325115 -3.8501012 -3.4684367 -3.4118049 -3.7913437 -4.1269255 -4.2674017 -4.2711926 -4.2011437 -4.2267718 -4.2546306][-4.0532346 -3.6698809 -3.7597489 -3.9536076 -4.1118822 -4.0488696 -3.8128426 -3.6772118 -3.8354127 -4.0368023 -4.1477246 -4.1116409 -3.9848633 -3.9212556 -3.8805561]]...]
INFO - root - 2017-12-15 06:05:52.054637: step 910, loss = 0.29, batch loss = 0.25 (35.2 examples/sec; 0.227 sec/batch; 20h:55m:21s remains)
INFO - root - 2017-12-15 06:05:54.336318: step 920, loss = 0.41, batch loss = 0.37 (34.2 examples/sec; 0.234 sec/batch; 21h:31m:52s remains)
INFO - root - 2017-12-15 06:05:56.630297: step 930, loss = 0.28, batch loss = 0.24 (34.7 examples/sec; 0.231 sec/batch; 21h:14m:53s remains)
INFO - root - 2017-12-15 06:05:58.892483: step 940, loss = 0.21, batch loss = 0.17 (34.3 examples/sec; 0.233 sec/batch; 21h:28m:39s remains)
INFO - root - 2017-12-15 06:06:01.129285: step 950, loss = 0.35, batch loss = 0.31 (36.0 examples/sec; 0.222 sec/batch; 20h:28m:47s remains)
INFO - root - 2017-12-15 06:06:03.395556: step 960, loss = 0.22, batch loss = 0.19 (35.3 examples/sec; 0.227 sec/batch; 20h:52m:14s remains)
INFO - root - 2017-12-15 06:06:05.687736: step 970, loss = 0.33, batch loss = 0.29 (35.6 examples/sec; 0.225 sec/batch; 20h:42m:40s remains)
INFO - root - 2017-12-15 06:06:07.944862: step 980, loss = 0.30, batch loss = 0.26 (34.9 examples/sec; 0.229 sec/batch; 21h:06m:14s remains)
INFO - root - 2017-12-15 06:06:10.219127: step 990, loss = 0.28, batch loss = 0.24 (34.7 examples/sec; 0.231 sec/batch; 21h:14m:15s remains)
INFO - root - 2017-12-15 06:06:12.505845: step 1000, loss = 0.33, batch loss = 0.29 (34.6 examples/sec; 0.231 sec/batch; 21h:18m:39s remains)
2017-12-15 06:06:12.775699: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6599164 -3.733922 -3.4854641 -3.7659025 -4.4659338 -5.0354862 -5.34721 -5.4285536 -4.9190445 -4.1548791 -3.5360188 -3.0131955 -2.8514979 -3.2167261 -4.0297413][-5.2539797 -4.3692675 -4.0166221 -4.3210139 -5.1111164 -5.68777 -5.9301267 -5.9189272 -5.3569632 -4.7243571 -4.3693047 -4.0341368 -3.9123278 -4.3033676 -5.1572142][-6.2835422 -4.5798407 -4.0446296 -4.247952 -4.9019356 -5.2749586 -5.2899103 -5.1399374 -4.72268 -4.5560436 -4.6919413 -4.6765442 -4.6183381 -4.9741521 -5.7883167][-6.7821503 -4.4979715 -3.7325339 -3.6955295 -3.9668567 -3.8339186 -3.3582058 -2.9539695 -2.8325312 -3.3952713 -4.2194548 -4.6653523 -4.8253531 -5.2539663 -6.0515718][-6.6126914 -3.9475627 -3.0230756 -2.7609921 -2.5591033 -1.7005095 -0.49574745 0.319175 0.16493154 -1.1852819 -2.7675364 -3.7681375 -4.3873215 -5.1111317 -6.0094643][-5.9006205 -3.2688537 -2.2809722 -1.8331122 -1.2107847 0.3005476 2.1457782 3.2870669 2.8757043 0.96159816 -1.0696484 -2.4326656 -3.5281129 -4.5965157 -5.6968412][-5.2325988 -2.7265825 -1.6127415 -0.91220939 0.019726276 1.8544998 3.8928223 5.0040994 4.3595262 2.3917704 0.52117085 -0.85412848 -2.321223 -3.7826519 -5.2643962][-4.8276467 -2.209044 -0.8610189 0.1155529 1.0728302 2.6341681 4.1909719 4.8322988 4.048192 2.5937982 1.4897528 0.48380637 -1.0904425 -2.7747972 -4.6567049][-5.2947788 -2.5337992 -0.9186734 0.31715202 1.1917164 2.2011089 2.9296222 2.8951855 2.0610504 1.3892167 1.2742703 0.77794051 -0.68598866 -2.3178787 -4.3988304][-6.2819695 -3.5490904 -1.8238949 -0.38541961 0.51637554 1.0961301 1.0989733 0.47833204 -0.28085923 -0.22026014 0.45034647 0.27738833 -1.0533465 -2.5166736 -4.5819569][-7.1562982 -4.7109823 -3.0705492 -1.5724161 -0.64515078 -0.35292268 -0.84206688 -1.7808278 -2.3456933 -1.7867272 -0.70367 -0.72687948 -1.8711716 -3.0615163 -4.8721347][-8.0292892 -6.090374 -4.7530394 -3.4052043 -2.52544 -2.375926 -3.0541334 -4.0271978 -4.3797607 -3.6289997 -2.4955618 -2.4344125 -3.2694407 -4.055171 -5.3549023][-8.169075 -6.7372055 -5.801126 -4.7588959 -4.0424786 -3.9614711 -4.5922928 -5.3928866 -5.569962 -4.8659973 -3.9116073 -3.82037 -4.3558121 -4.8119464 -5.59519][-7.3777118 -6.297493 -5.7344112 -5.0719366 -4.6089845 -4.586009 -5.0626345 -5.6093473 -5.6767168 -5.1610813 -4.4961457 -4.38987 -4.6490326 -4.8488851 -5.22121][-6.3629746 -5.5194359 -5.2443428 -4.9010029 -4.664413 -4.6643667 -4.940064 -5.2256923 -5.2293625 -4.91947 -4.5225034 -4.4095011 -4.47784 -4.5183325 -4.623992]]...]
INFO - root - 2017-12-15 06:06:15.051740: step 1010, loss = 0.29, batch loss = 0.25 (33.9 examples/sec; 0.236 sec/batch; 21h:42m:34s remains)
INFO - root - 2017-12-15 06:06:17.299545: step 1020, loss = 0.27, batch loss = 0.23 (35.8 examples/sec; 0.224 sec/batch; 20h:35m:04s remains)
INFO - root - 2017-12-15 06:06:19.546188: step 1030, loss = 0.29, batch loss = 0.25 (34.0 examples/sec; 0.235 sec/batch; 21h:39m:38s remains)
INFO - root - 2017-12-15 06:06:21.781761: step 1040, loss = 0.30, batch loss = 0.26 (35.5 examples/sec; 0.225 sec/batch; 20h:44m:23s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 06:06:24.023832: step 1050, loss = 0.33, batch loss = 0.29 (35.9 examples/sec; 0.223 sec/batch; 20h:32m:29s remains)
INFO - root - 2017-12-15 06:06:26.246671: step 1060, loss = 0.23, batch loss = 0.19 (35.2 examples/sec; 0.228 sec/batch; 20h:56m:47s remains)
INFO - root - 2017-12-15 06:06:28.518570: step 1070, loss = 0.27, batch loss = 0.23 (34.9 examples/sec; 0.229 sec/batch; 21h:04m:50s remains)
INFO - root - 2017-12-15 06:06:30.815012: step 1080, loss = 0.26, batch loss = 0.22 (36.1 examples/sec; 0.222 sec/batch; 20h:23m:56s remains)
INFO - root - 2017-12-15 06:06:33.043128: step 1090, loss = 0.31, batch loss = 0.27 (36.2 examples/sec; 0.221 sec/batch; 20h:19m:46s remains)
INFO - root - 2017-12-15 06:06:35.294430: step 1100, loss = 0.24, batch loss = 0.20 (34.5 examples/sec; 0.232 sec/batch; 21h:22m:11s remains)
2017-12-15 06:06:35.589653: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0215025 -0.047495484 1.185159 1.7706983 1.4755225 0.85473657 -0.20506084 -1.7445029 -3.0797927 -3.6708522 -4.1613345 -4.42494 -4.0415516 -3.4422951 -2.8292096][-1.9688865 -0.91541719 0.64368606 1.8221416 2.0238922 1.8281658 0.848825 -0.98032826 -2.7347867 -3.4826012 -4.0991898 -4.4201136 -4.0532341 -3.4803512 -2.9383845][-3.14883 -2.0246119 -0.19100189 1.7090859 2.5989654 2.8283327 1.9881144 -0.0069247484 -2.0505571 -3.1007454 -3.9127097 -4.204071 -3.774106 -3.2502208 -2.8074224][-4.1284285 -3.0160384 -1.1459548 1.2579987 2.7263143 3.3177145 2.6372774 0.69855809 -1.4337804 -2.8810565 -4.0104985 -4.2666392 -3.7050622 -3.1827803 -2.8122232][-4.8545723 -3.7664289 -2.1745186 0.22100782 2.0779293 3.1208403 2.8433998 1.3373921 -0.55641019 -2.2795846 -3.7731762 -4.1066213 -3.5437651 -3.2046075 -3.0119021][-5.1693 -4.2350969 -2.9811764 -1.050164 0.77067041 2.2361205 2.6121581 1.8114836 0.35605717 -1.4192231 -3.2237191 -3.7603655 -3.4047565 -3.3497529 -3.3443494][-4.9810758 -4.2077169 -3.2779722 -1.9320694 -0.36818004 1.4236579 2.4676087 2.430721 1.3944619 -0.49678636 -2.5934811 -3.3887672 -3.4168482 -3.611038 -3.6491427][-5.0634384 -4.29329 -3.5229673 -2.5107491 -1.092093 0.94755816 2.5201466 3.1177871 2.3809698 0.37925839 -1.9001974 -3.0088429 -3.4572554 -3.8530815 -3.8580673][-5.1592927 -4.5540171 -3.9360995 -3.0279055 -1.7247411 0.23814654 1.9886887 2.9517128 2.2979414 0.30198479 -1.8279264 -3.0229211 -3.7449989 -4.2209005 -4.0866213][-5.1430955 -4.6976204 -4.3135529 -3.4970798 -2.3226459 -0.644469 0.94260168 1.9599702 1.3927641 -0.43754423 -2.2328296 -3.2413421 -3.9828806 -4.4227877 -4.1837091][-4.9204884 -4.6590023 -4.6144795 -4.0263977 -3.0568366 -1.715515 -0.38188112 0.6549015 0.30642533 -1.1438746 -2.4251242 -3.1251614 -3.8092041 -4.1846437 -3.9916606][-4.3809996 -4.2590666 -4.5513248 -4.3583884 -3.7648454 -2.8176923 -1.7383492 -0.64547861 -0.62272322 -1.5352509 -2.258693 -2.6780879 -3.3070028 -3.6243715 -3.5529575][-4.3101625 -4.1764908 -4.5835752 -4.5812335 -4.2660007 -3.6843207 -2.8230469 -1.6396165 -1.205847 -1.598433 -1.9241118 -2.2533102 -2.8417203 -3.0418615 -3.0848763][-4.7154989 -4.4748235 -4.8489747 -4.8691416 -4.7124453 -4.413949 -3.7425685 -2.4678741 -1.6576028 -1.6146773 -1.6729095 -1.9560878 -2.4476292 -2.5721531 -2.7418222][-5.0126543 -4.5937738 -4.8251085 -4.7551041 -4.6959696 -4.6455288 -4.1221943 -2.7772019 -1.7584057 -1.4688885 -1.3536154 -1.5615077 -1.9548638 -2.122937 -2.4288571]]...]
INFO - root - 2017-12-15 06:06:37.853475: step 1110, loss = 0.30, batch loss = 0.26 (34.3 examples/sec; 0.233 sec/batch; 21h:26m:35s remains)
INFO - root - 2017-12-15 06:06:40.113615: step 1120, loss = 0.42, batch loss = 0.38 (35.5 examples/sec; 0.226 sec/batch; 20h:46m:01s remains)
INFO - root - 2017-12-15 06:06:42.382965: step 1130, loss = 0.33, batch loss = 0.29 (34.4 examples/sec; 0.233 sec/batch; 21h:25m:08s remains)
INFO - root - 2017-12-15 06:06:44.651895: step 1140, loss = 0.29, batch loss = 0.26 (34.3 examples/sec; 0.233 sec/batch; 21h:27m:53s remains)
INFO - root - 2017-12-15 06:06:46.936167: step 1150, loss = 0.34, batch loss = 0.30 (34.8 examples/sec; 0.230 sec/batch; 21h:10m:36s remains)
INFO - root - 2017-12-15 06:06:49.185100: step 1160, loss = 0.29, batch loss = 0.25 (36.3 examples/sec; 0.220 sec/batch; 20h:15m:24s remains)
INFO - root - 2017-12-15 06:06:51.453892: step 1170, loss = 0.29, batch loss = 0.25 (35.1 examples/sec; 0.228 sec/batch; 20h:57m:54s remains)
INFO - root - 2017-12-15 06:06:53.684630: step 1180, loss = 0.22, batch loss = 0.18 (35.9 examples/sec; 0.223 sec/batch; 20h:30m:55s remains)
INFO - root - 2017-12-15 06:06:55.955745: step 1190, loss = 0.20, batch loss = 0.16 (35.6 examples/sec; 0.225 sec/batch; 20h:41m:04s remains)
INFO - root - 2017-12-15 06:06:58.190745: step 1200, loss = 0.28, batch loss = 0.24 (35.5 examples/sec; 0.225 sec/batch; 20h:43m:18s remains)
2017-12-15 06:06:58.481071: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0192394 -3.2448475 -3.1987486 -3.1622005 -2.9902225 -3.3869944 -3.9140744 -3.5741835 -3.4485226 -3.8502018 -4.7974606 -5.599719 -5.9878354 -5.9749684 -5.4480309][-2.62115 -2.4551032 -2.4836583 -2.5808768 -2.5508759 -3.3235879 -4.4232178 -4.2152157 -4.00298 -4.4218087 -5.3568945 -6.0477271 -6.224123 -6.0068521 -5.3318176][-1.7047865 -1.1354395 -1.3365238 -1.7233491 -1.9117609 -2.8795643 -4.243144 -4.1298275 -3.8664143 -4.2964425 -5.1170726 -5.6320395 -5.6242704 -5.1903939 -4.3131313][-1.2823193 -0.38302863 -0.7035588 -1.1978217 -1.3530529 -1.9958309 -2.9984419 -2.800899 -2.591639 -3.1480398 -3.9496722 -4.4561114 -4.5184741 -4.0469856 -3.0837517][-1.3384471 -0.12256145 -0.39283431 -0.64555621 -0.4488703 -0.48881698 -0.84049451 -0.54548585 -0.57591093 -1.4991509 -2.5109138 -3.2409279 -3.6423378 -3.3674357 -2.4777355][-0.72645295 0.4477123 0.037790418 -0.019469023 0.54656422 1.0684015 1.3988575 1.8769823 1.5908726 0.20045412 -1.1028392 -2.1334457 -2.915765 -2.9091718 -2.1973121][-0.090066552 0.62834084 -0.16770649 -0.087623 0.855785 2.0375566 3.2395291 4.1270614 3.7137728 1.9323214 0.31026542 -0.98725957 -2.0768054 -2.3033013 -1.8511039][0.0010687113 0.31889403 -0.75919461 -0.57045829 0.68604791 2.3539929 4.154356 5.2748523 4.7157106 2.6892204 0.92308342 -0.54759657 -1.8704882 -2.3603282 -2.2329705][-0.1664778 -0.058795452 -1.2202269 -1.1523921 -0.13421631 1.3116285 2.9599457 3.8582206 3.2716098 1.5205561 0.12542307 -1.1445299 -2.4466591 -3.0888047 -3.2722039][-1.0819418 -0.94536769 -1.9443115 -2.122884 -1.6973602 -0.922428 0.12630618 0.59744179 0.11072743 -0.97564006 -1.6482927 -2.4026577 -3.3954117 -4.0472288 -4.442728][-2.9852347 -2.6337655 -3.1592789 -3.3671722 -3.4137995 -3.298151 -2.9237037 -2.9562678 -3.4130876 -3.8881025 -3.8860188 -4.0409446 -4.4887977 -4.912034 -5.3032293][-5.2604651 -4.7359672 -4.7433681 -4.8134627 -5.0608759 -5.2964659 -5.3899817 -5.8327551 -6.256773 -6.1727877 -5.5892358 -5.1871681 -5.013536 -5.0172138 -5.252666][-7.1106133 -6.4914145 -6.0941362 -5.9068255 -6.03502 -6.1861234 -6.3385539 -6.8017621 -7.02534 -6.492063 -5.6258135 -5.0302486 -4.6271849 -4.4324293 -4.5188189][-7.5442166 -6.8288631 -6.1552563 -5.7734361 -5.7501659 -5.70654 -5.7141805 -6.0584073 -6.2033043 -5.6268668 -4.9092565 -4.5025969 -4.2027693 -4.0528979 -4.019742][-6.9514694 -6.2602911 -5.5563617 -5.1761065 -5.1822195 -5.0697246 -4.9656334 -5.2375045 -5.426857 -5.0189762 -4.5218263 -4.2769189 -4.1045713 -4.0664144 -4.0815043]]...]
INFO - root - 2017-12-15 06:07:00.726290: step 1210, loss = 0.38, batch loss = 0.34 (35.8 examples/sec; 0.223 sec/batch; 20h:33m:57s remains)
INFO - root - 2017-12-15 06:07:03.002133: step 1220, loss = 0.26, batch loss = 0.22 (34.4 examples/sec; 0.233 sec/batch; 21h:24m:12s remains)
INFO - root - 2017-12-15 06:07:05.248753: step 1230, loss = 0.21, batch loss = 0.17 (35.3 examples/sec; 0.227 sec/batch; 20h:52m:19s remains)
INFO - root - 2017-12-15 06:07:07.540131: step 1240, loss = 0.34, batch loss = 0.30 (34.5 examples/sec; 0.232 sec/batch; 21h:20m:59s remains)
INFO - root - 2017-12-15 06:07:09.806268: step 1250, loss = 0.40, batch loss = 0.36 (34.5 examples/sec; 0.232 sec/batch; 21h:21m:12s remains)
INFO - root - 2017-12-15 06:07:12.105520: step 1260, loss = 0.24, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 20h:26m:13s remains)
INFO - root - 2017-12-15 06:07:14.375741: step 1270, loss = 0.36, batch loss = 0.32 (34.6 examples/sec; 0.231 sec/batch; 21h:14m:48s remains)
INFO - root - 2017-12-15 06:07:16.618310: step 1280, loss = 0.40, batch loss = 0.36 (36.2 examples/sec; 0.221 sec/batch; 20h:20m:28s remains)
INFO - root - 2017-12-15 06:07:18.885354: step 1290, loss = 0.37, batch loss = 0.33 (35.5 examples/sec; 0.225 sec/batch; 20h:42m:19s remains)
INFO - root - 2017-12-15 06:07:21.120937: step 1300, loss = 0.34, batch loss = 0.31 (35.1 examples/sec; 0.228 sec/batch; 20h:56m:28s remains)
2017-12-15 06:07:21.394952: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.87675607 -2.8355598 -3.5144117 -4.159924 -4.4210019 -3.92049 -3.4033434 -3.1534071 -2.7149966 -2.9125316 -4.1534104 -3.9787092 -3.1295996 -2.6652765 -2.3761258][-3.0401039 -4.0630994 -4.4186878 -4.6645508 -4.5139256 -3.4502249 -2.6283853 -2.5791619 -2.4629829 -2.8250806 -4.3979788 -4.69505 -4.10874 -3.6091757 -3.2423539][-4.9480152 -5.2138453 -5.3376989 -5.2515597 -4.7921405 -3.3573298 -2.2402444 -2.38762 -2.7647657 -3.257843 -4.7702203 -5.3634529 -4.9910774 -4.3378406 -3.5576611][-6.0822444 -5.4700136 -5.2424288 -4.7257209 -3.9717708 -2.3469796 -1.0455248 -1.2542872 -1.9790596 -2.636343 -4.0860968 -5.0386248 -5.1398196 -4.552392 -3.4042087][-6.2545066 -5.0436449 -4.6593328 -3.9759049 -3.1347742 -1.4949473 -0.011068106 0.11744153 -0.6742028 -1.7218838 -3.3862419 -4.6997061 -5.2622108 -4.7435193 -3.3225188][-5.4457531 -3.8441422 -3.1996636 -2.3394659 -1.4173198 0.15200198 1.6156598 2.026 0.97902954 -0.82625878 -2.9154453 -4.4762206 -5.2058873 -4.6487446 -3.0206039][-4.5191488 -2.7355723 -1.8662193 -0.936247 0.18910897 1.96213 3.5719934 4.31437 3.006669 0.36358988 -2.1315563 -3.7375314 -4.4227223 -3.7418985 -2.0116093][-4.601254 -2.7446058 -1.9640636 -1.1593974 0.23473299 2.3965945 4.1613135 5.0663133 3.6621222 0.64335883 -1.7986211 -3.1052666 -3.5841751 -2.7983937 -1.1827589][-4.6930223 -2.8213139 -2.1003036 -1.3353646 0.25548661 2.4799132 4.0120592 4.6449571 3.2166982 0.40479314 -1.4307297 -2.1112814 -2.3395643 -1.6483665 -0.4222393][-4.5916624 -2.7947745 -2.221782 -1.5741966 -0.10468614 1.7228988 2.6268187 2.741169 1.5078682 -0.57355309 -1.6147153 -1.7087749 -1.7539483 -1.1575422 -0.16864204][-4.5208893 -2.8589246 -2.463963 -2.049468 -1.0499489 0.025785327 0.22570622 -0.015133142 -0.76167333 -1.9095737 -2.3216343 -2.1348729 -2.1510408 -1.63097 -0.66646779][-4.5265374 -2.9745452 -2.615025 -2.2943208 -1.6727846 -1.1993947 -1.4430945 -1.8423134 -2.2235761 -2.7809331 -2.967442 -2.8160429 -2.8790278 -2.4691644 -1.5686203][-4.2605453 -2.8016815 -2.433964 -2.1741204 -1.8947848 -1.9097711 -2.3692298 -2.7463644 -2.9316921 -3.2785749 -3.5577898 -3.6189775 -3.7242761 -3.3784897 -2.6062939][-3.6863427 -2.1774936 -1.6946478 -1.4151372 -1.3442085 -1.622539 -2.0873604 -2.3536465 -2.5102079 -2.9431658 -3.5139983 -3.9501021 -4.2573385 -4.0978289 -3.5788689][-3.7813554 -2.3389933 -1.7701488 -1.3858826 -1.2725731 -1.4975562 -1.7806175 -1.911514 -2.0728962 -2.4894648 -3.021328 -3.4620919 -3.7914302 -3.8388453 -3.6906]]...]
INFO - root - 2017-12-15 06:07:23.682730: step 1310, loss = 0.51, batch loss = 0.47 (33.6 examples/sec; 0.238 sec/batch; 21h:54m:56s remains)
INFO - root - 2017-12-15 06:07:25.938080: step 1320, loss = 0.29, batch loss = 0.25 (36.8 examples/sec; 0.218 sec/batch; 20h:00m:34s remains)
INFO - root - 2017-12-15 06:07:28.216102: step 1330, loss = 0.46, batch loss = 0.42 (35.1 examples/sec; 0.228 sec/batch; 20h:58m:29s remains)
INFO - root - 2017-12-15 06:07:30.465005: step 1340, loss = 0.25, batch loss = 0.21 (35.3 examples/sec; 0.227 sec/batch; 20h:50m:42s remains)
INFO - root - 2017-12-15 06:07:32.746515: step 1350, loss = 0.28, batch loss = 0.24 (35.2 examples/sec; 0.227 sec/batch; 20h:52m:52s remains)
INFO - root - 2017-12-15 06:07:35.011232: step 1360, loss = 0.23, batch loss = 0.19 (34.5 examples/sec; 0.232 sec/batch; 21h:19m:34s remains)
INFO - root - 2017-12-15 06:07:37.281915: step 1370, loss = 0.19, batch loss = 0.15 (35.2 examples/sec; 0.227 sec/batch; 20h:53m:31s remains)
INFO - root - 2017-12-15 06:07:39.561344: step 1380, loss = 0.32, batch loss = 0.28 (36.5 examples/sec; 0.219 sec/batch; 20h:11m:10s remains)
INFO - root - 2017-12-15 06:07:41.850448: step 1390, loss = 0.41, batch loss = 0.37 (35.1 examples/sec; 0.228 sec/batch; 20h:56m:23s remains)
INFO - root - 2017-12-15 06:07:44.099311: step 1400, loss = 0.23, batch loss = 0.19 (35.2 examples/sec; 0.228 sec/batch; 20h:55m:41s remains)
2017-12-15 06:07:44.357227: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3257637 -4.7433186 -4.972846 -5.3753681 -5.4224348 -5.2684431 -5.0491128 -4.7130866 -4.5996675 -4.408875 -4.165936 -4.0087366 -4.1885195 -4.5010338 -5.0040288][-5.086555 -4.9255047 -5.2513328 -5.7324862 -5.7607708 -5.5217156 -5.2300434 -4.8692241 -4.80658 -4.6197186 -4.3771744 -4.1297359 -4.1026316 -4.1834641 -4.5823245][-5.7461014 -5.2401519 -5.4674354 -5.8198586 -5.6499233 -5.140029 -4.7131639 -4.4574566 -4.6471915 -4.7214422 -4.6190815 -4.3309188 -4.0745945 -3.7925825 -3.9401424][-6.2312517 -5.5182328 -5.6241779 -5.7519956 -5.2145324 -4.3365517 -3.7618632 -3.784277 -4.4494872 -5.0686584 -5.3557544 -5.1934562 -4.8473315 -4.3069973 -4.1308002][-6.2906308 -5.3387012 -5.24929 -5.0323768 -3.939537 -2.4831681 -1.616501 -1.8657421 -2.9865642 -4.2181659 -4.9674129 -4.9910908 -4.7299423 -4.1993508 -3.9250946][-5.6816359 -4.7292757 -4.340044 -3.64898 -1.8635854 0.26700509 1.4006072 0.89703548 -0.82272267 -2.7822776 -4.0035205 -4.1798449 -3.9914012 -3.4738154 -3.1248479][-5.2616577 -4.2709141 -3.4809771 -2.0916705 0.50082839 3.1342506 4.3114409 3.3429217 0.88793552 -1.7800131 -3.4728923 -3.7571692 -3.63175 -3.2011912 -2.7869391][-5.9199924 -4.6232853 -3.3619914 -1.2391713 1.9975895 4.8472209 5.9140859 4.5742536 1.5249964 -1.7110596 -3.798172 -4.2203956 -4.2343583 -3.9544644 -3.5000615][-7.0482497 -5.4243369 -3.8046761 -1.2150936 2.151454 4.7593875 5.5786686 4.1410966 0.89940131 -2.5493579 -4.8046064 -5.36751 -5.5075541 -5.3001881 -4.7545152][-8.1063061 -6.3425198 -4.5297112 -1.7222908 1.4297119 3.5859752 4.1435146 2.7611041 -0.27905679 -3.4669688 -5.440465 -5.8928118 -5.9745612 -5.7772827 -5.2945256][-9.0203209 -7.4461694 -5.827208 -3.2284989 -0.47846985 1.3294913 1.8672248 0.86006176 -1.520659 -4.0131192 -5.4108615 -5.6460595 -5.6115532 -5.4641075 -5.1945682][-9.83979 -8.6764927 -7.5393109 -5.554647 -3.425611 -1.7611175 -0.90219784 -1.1509447 -2.517921 -3.9250793 -4.5168543 -4.5877714 -4.6233335 -4.6964855 -4.8130527][-10.229938 -9.53006 -8.8681707 -7.5639353 -6.1485438 -4.672843 -3.4498122 -2.9309111 -3.202884 -3.4307384 -3.2511606 -3.1928885 -3.4563422 -3.9538546 -4.6003113][-10.060717 -9.6465149 -9.2955236 -8.5094843 -7.6663895 -6.448772 -5.1106195 -4.0891333 -3.5119724 -2.8829467 -2.3089595 -2.3459239 -3.0006509 -4.0621905 -5.1975136][-9.2402658 -9.0905027 -9.03266 -8.6030407 -8.1511259 -7.22141 -6.0013037 -4.8255563 -3.851687 -2.9336319 -2.4024315 -2.6413028 -3.6049778 -4.9801245 -6.2015595]]...]
INFO - root - 2017-12-15 06:07:46.625129: step 1410, loss = 0.35, batch loss = 0.31 (35.7 examples/sec; 0.224 sec/batch; 20h:37m:09s remains)
INFO - root - 2017-12-15 06:07:48.844922: step 1420, loss = 0.27, batch loss = 0.23 (36.7 examples/sec; 0.218 sec/batch; 20h:02m:49s remains)
INFO - root - 2017-12-15 06:07:51.080183: step 1430, loss = 0.26, batch loss = 0.23 (37.3 examples/sec; 0.215 sec/batch; 19h:45m:02s remains)
INFO - root - 2017-12-15 06:07:53.366359: step 1440, loss = 0.30, batch loss = 0.26 (34.3 examples/sec; 0.233 sec/batch; 21h:26m:17s remains)
INFO - root - 2017-12-15 06:07:55.622225: step 1450, loss = 0.26, batch loss = 0.22 (35.2 examples/sec; 0.227 sec/batch; 20h:53m:19s remains)
INFO - root - 2017-12-15 06:07:57.876134: step 1460, loss = 0.28, batch loss = 0.24 (34.9 examples/sec; 0.229 sec/batch; 21h:05m:45s remains)
INFO - root - 2017-12-15 06:08:00.152346: step 1470, loss = 0.45, batch loss = 0.41 (35.1 examples/sec; 0.228 sec/batch; 20h:57m:43s remains)
INFO - root - 2017-12-15 06:08:02.422185: step 1480, loss = 0.25, batch loss = 0.21 (34.7 examples/sec; 0.230 sec/batch; 21h:10m:37s remains)
INFO - root - 2017-12-15 06:08:04.698927: step 1490, loss = 0.41, batch loss = 0.37 (35.9 examples/sec; 0.223 sec/batch; 20h:28m:06s remains)
INFO - root - 2017-12-15 06:08:07.013366: step 1500, loss = 0.29, batch loss = 0.25 (34.2 examples/sec; 0.234 sec/batch; 21h:30m:36s remains)
2017-12-15 06:08:07.289079: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.680479 -3.703054 -4.5682335 -5.0412183 -5.1860223 -5.2269487 -5.1580086 -4.8939924 -4.777225 -4.5821195 -4.5995345 -5.0375915 -5.4197664 -5.5553913 -5.2895203][-1.7407639 -2.5025234 -3.9494395 -4.7686839 -4.8407149 -4.4071741 -4.0082049 -3.7192881 -3.7334383 -3.7714376 -4.035274 -4.6697021 -5.1369762 -5.2563329 -4.8747635][-0.86388361 -1.4450058 -3.3045154 -4.2500935 -4.0676541 -3.0819292 -2.4603562 -2.2928958 -2.5717418 -2.9208279 -3.444205 -4.2714539 -4.8737531 -4.9659986 -4.3851194][-0.91246474 -1.1428455 -3.0036736 -3.7741175 -3.18316 -1.6880738 -0.88869607 -0.76928318 -1.3232751 -2.0868919 -3.0169845 -4.1263175 -4.8741069 -4.9788995 -4.3264265][-2.2075303 -2.144465 -3.5895941 -3.8428516 -2.6507938 -0.65601671 0.39997256 0.48840368 -0.26485169 -1.4088141 -2.7641973 -4.20182 -5.1531816 -5.3179135 -4.7709985][-3.9363222 -3.8994136 -4.7036939 -4.2193432 -2.3088121 0.17042625 1.4867009 1.4414297 0.5015527 -0.79160178 -2.3477459 -4.0900159 -5.3882213 -5.742269 -5.4600954][-4.9821038 -4.8533835 -5.1392031 -4.0940619 -1.7652727 0.9045974 2.354948 2.2727981 1.3297979 0.16814411 -1.2409104 -2.9448695 -4.4308028 -5.0771914 -5.2114849][-4.9311576 -4.4686861 -4.6430492 -3.493186 -1.1471276 1.4330879 2.8364277 2.7360244 1.9353906 1.0308853 -0.091120362 -1.5983629 -3.2070837 -4.1094084 -4.5505414][-3.5970414 -3.0589137 -3.4577775 -2.6429186 -0.64709938 1.6372701 2.8382721 2.665143 2.0561504 1.5803961 0.85036075 -0.55216515 -2.3213987 -3.3618908 -3.837326][-2.3459058 -1.7513968 -2.5269141 -2.3166156 -0.88466072 0.96180403 1.9037389 1.6027185 1.1180547 1.0204982 0.73438036 -0.43077481 -2.0533094 -3.0502968 -3.449][-2.6146631 -1.8049214 -2.5937681 -2.6491306 -1.7105591 -0.42099321 0.15481985 -0.33964229 -0.82397604 -0.69197488 -0.68071949 -1.5935259 -2.9316626 -3.7932868 -4.05467][-3.4862344 -2.4160972 -2.8680706 -2.9331298 -2.4740787 -1.8077054 -1.5603108 -2.1555591 -2.592448 -2.2912798 -2.2011418 -3.041239 -4.1392622 -4.8234997 -4.9456229][-4.0371633 -2.8000469 -2.9289668 -3.0194433 -3.0122805 -2.909291 -2.951391 -3.5455813 -3.8995814 -3.5786047 -3.5652995 -4.3325562 -5.148025 -5.638001 -5.6851997][-4.3807983 -3.1198878 -3.0970759 -3.2733803 -3.6112187 -3.9132657 -4.1988258 -4.7488475 -5.0126572 -4.7450333 -4.7905021 -5.3741536 -5.9065533 -6.1886773 -6.1326771][-4.38292 -3.2509058 -3.2681794 -3.5812113 -4.076488 -4.5100489 -4.8700395 -5.3152127 -5.4903374 -5.2611642 -5.2539492 -5.5975327 -5.892271 -5.9771519 -5.8465152]]...]
INFO - root - 2017-12-15 06:08:09.572227: step 1510, loss = 0.38, batch loss = 0.34 (35.6 examples/sec; 0.225 sec/batch; 20h:38m:37s remains)
INFO - root - 2017-12-15 06:08:11.869918: step 1520, loss = 0.26, batch loss = 0.22 (35.4 examples/sec; 0.226 sec/batch; 20h:45m:27s remains)
INFO - root - 2017-12-15 06:08:14.087200: step 1530, loss = 0.28, batch loss = 0.24 (36.0 examples/sec; 0.222 sec/batch; 20h:25m:09s remains)
INFO - root - 2017-12-15 06:08:16.330575: step 1540, loss = 0.39, batch loss = 0.35 (34.9 examples/sec; 0.229 sec/batch; 21h:04m:10s remains)
INFO - root - 2017-12-15 06:08:18.578433: step 1550, loss = 0.29, batch loss = 0.25 (35.5 examples/sec; 0.225 sec/batch; 20h:41m:20s remains)
INFO - root - 2017-12-15 06:08:20.853932: step 1560, loss = 0.23, batch loss = 0.19 (36.1 examples/sec; 0.222 sec/batch; 20h:23m:03s remains)
INFO - root - 2017-12-15 06:08:23.151274: step 1570, loss = 0.33, batch loss = 0.29 (34.0 examples/sec; 0.235 sec/batch; 21h:38m:18s remains)
INFO - root - 2017-12-15 06:08:25.429831: step 1580, loss = 0.28, batch loss = 0.24 (34.7 examples/sec; 0.231 sec/batch; 21h:12m:06s remains)
INFO - root - 2017-12-15 06:08:27.710416: step 1590, loss = 0.30, batch loss = 0.27 (34.9 examples/sec; 0.229 sec/batch; 21h:05m:21s remains)
INFO - root - 2017-12-15 06:08:30.030112: step 1600, loss = 0.38, batch loss = 0.34 (34.5 examples/sec; 0.232 sec/batch; 21h:18m:43s remains)
2017-12-15 06:08:30.304309: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7520479 -1.7413746 -2.699357 -4.0406933 -4.7882466 -5.3797011 -5.9916687 -6.1632514 -5.6039028 -4.87732 -4.0780196 -3.0066805 -2.0979371 -1.0488561 -0.54129755][-2.0703099 -2.0407593 -3.1182971 -4.5886254 -5.2158566 -5.6075745 -6.0077171 -6.0080137 -5.3293133 -4.4679122 -3.5307572 -2.3342049 -1.3208182 -0.43553245 -0.24744332][-2.5181966 -2.2326095 -3.2450142 -4.523016 -4.9638095 -5.0529108 -4.9986429 -4.6837134 -4.0268283 -3.3097916 -2.4420331 -1.2063398 -0.15226066 0.468606 0.39634275][-2.7079778 -2.2174814 -3.0887017 -3.9187045 -3.8920534 -3.4007919 -2.6695628 -2.0283849 -1.5962348 -1.3034556 -0.7393198 0.30127192 1.1964746 1.5267024 1.2694917][-2.3748913 -1.6803524 -2.3428493 -2.6416144 -2.1399679 -1.1010094 0.13783455 0.7984302 0.73795176 0.36802912 0.40537214 1.097075 1.8617971 2.1772165 2.0482302][-1.2848804 -0.92051971 -1.4687746 -1.3702154 -0.52788639 0.89470577 2.4848981 3.0916133 2.4912887 1.3912678 0.76034212 0.95751238 1.5337164 2.0546718 2.3009958][-0.33219409 -0.25892293 -0.65806496 -0.32803106 0.63914394 2.1817718 3.9041123 4.4668274 3.5210395 1.9738166 0.86527371 0.61261392 0.94712234 1.6403394 2.2447734][-0.039655805 0.38368702 0.13258719 0.41477489 1.1109891 2.3458328 3.8488326 4.315793 3.3343058 1.8314283 0.68075585 0.20666718 0.37138605 1.2717013 2.2292953][-0.41705108 0.38341618 0.26874375 0.43693519 0.81343818 1.5859504 2.677875 3.0325832 2.2874503 1.1932623 0.35112548 -0.10778904 0.045046091 1.0774419 2.2031097][-1.3794417 -0.35415912 -0.30090845 -0.13182032 0.11954188 0.63268733 1.3440504 1.5812545 1.1405206 0.55780339 0.13884878 -0.14398336 0.039554358 0.96076369 1.8676505][-2.3012359 -1.2924626 -1.1633357 -1.0624387 -0.94634473 -0.61458683 -0.20630467 -0.10517478 -0.28941715 -0.45940745 -0.45937634 -0.48100197 -0.26721239 0.42546225 1.0086555][-3.1813588 -2.2631269 -2.0068758 -1.9529121 -2.0287764 -1.9717193 -1.8578247 -1.884831 -1.9457883 -1.9342191 -1.6483413 -1.3762313 -1.1190226 -0.64543581 -0.36884511][-3.3765564 -2.5249326 -2.0819385 -1.9973518 -2.2098842 -2.3419347 -2.3986959 -2.4729364 -2.5100164 -2.5298643 -2.2072859 -1.8110358 -1.5907493 -1.3087049 -1.2504711][-3.1263628 -2.282644 -1.7311982 -1.6413496 -1.9594642 -2.189496 -2.247638 -2.3009489 -2.3223336 -2.3475025 -2.0232637 -1.6547658 -1.5937376 -1.5006387 -1.5435765][-2.5811362 -1.7910995 -1.2693853 -1.2123374 -1.5878613 -1.8625966 -1.8673315 -1.9541658 -2.0600379 -2.0422773 -1.7082188 -1.3860397 -1.4145951 -1.4670368 -1.6009095]]...]
INFO - root - 2017-12-15 06:08:32.553642: step 1610, loss = 0.25, batch loss = 0.21 (34.9 examples/sec; 0.229 sec/batch; 21h:03m:19s remains)
INFO - root - 2017-12-15 06:08:34.818453: step 1620, loss = 0.25, batch loss = 0.21 (35.2 examples/sec; 0.227 sec/batch; 20h:52m:54s remains)
INFO - root - 2017-12-15 06:08:37.115043: step 1630, loss = 0.33, batch loss = 0.29 (36.0 examples/sec; 0.222 sec/batch; 20h:24m:56s remains)
INFO - root - 2017-12-15 06:08:39.358488: step 1640, loss = 0.27, batch loss = 0.23 (35.3 examples/sec; 0.227 sec/batch; 20h:50m:12s remains)
INFO - root - 2017-12-15 06:08:41.688573: step 1650, loss = 0.24, batch loss = 0.20 (35.9 examples/sec; 0.223 sec/batch; 20h:27m:55s remains)
INFO - root - 2017-12-15 06:08:43.958064: step 1660, loss = 0.27, batch loss = 0.23 (35.6 examples/sec; 0.225 sec/batch; 20h:38m:06s remains)
INFO - root - 2017-12-15 06:08:46.234161: step 1670, loss = 0.20, batch loss = 0.16 (35.8 examples/sec; 0.224 sec/batch; 20h:32m:31s remains)
INFO - root - 2017-12-15 06:08:48.530192: step 1680, loss = 0.25, batch loss = 0.21 (34.2 examples/sec; 0.234 sec/batch; 21h:29m:41s remains)
INFO - root - 2017-12-15 06:08:50.800255: step 1690, loss = 0.23, batch loss = 0.19 (35.3 examples/sec; 0.227 sec/batch; 20h:49m:02s remains)
INFO - root - 2017-12-15 06:08:53.065044: step 1700, loss = 0.25, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 20h:34m:21s remains)
2017-12-15 06:08:53.372767: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.23441529 0.93375385 1.5230595 0.25105536 -1.9091642 -4.202271 -5.4689751 -5.8502717 -6.1934218 -6.3085418 -6.2913613 -6.2711082 -5.8231115 -4.8060455 -4.0020247][0.051368117 1.7230321 2.0456548 0.37806928 -1.9807602 -4.2451806 -5.32435 -5.613934 -6.0368457 -6.629425 -7.2286153 -7.48538 -6.9487996 -5.769186 -4.8608327][-0.071431994 1.8803221 1.8802441 0.096557736 -2.0366828 -3.8614144 -4.4342847 -4.40301 -4.7932096 -5.8244791 -6.9950829 -7.6283622 -7.21924 -6.0764866 -5.0852551][-0.5619657 1.2858788 0.96537673 -0.6242007 -2.0989294 -3.0110579 -2.69974 -2.0852027 -2.3716862 -3.8158503 -5.4878831 -6.5792146 -6.5671473 -5.7325425 -4.812006][-0.93564773 0.33231771 -0.37315869 -1.5309725 -1.9637262 -1.6350207 -0.25007081 0.95201457 0.63494456 -1.2705271 -3.4020648 -4.9642057 -5.4338841 -4.9622335 -4.1552315][-1.6683478 -1.0679355 -2.0167553 -2.4935803 -1.6084874 0.10730565 2.5511041 4.20296 3.6467175 1.2121137 -1.4089308 -3.461843 -4.4259181 -4.281383 -3.5812926][-2.7563655 -2.60466 -3.5129571 -3.2141545 -1.1735929 1.521648 4.4961109 6.2067666 5.2453923 2.3909674 -0.547027 -2.8435695 -4.048749 -4.0251117 -3.3502913][-3.5766368 -3.485395 -4.1637869 -3.250999 -0.55627131 2.4341478 5.2679625 6.5762715 5.1221824 2.0354023 -0.98181176 -3.2393355 -4.37251 -4.1877589 -3.3501127][-3.8755736 -3.6170998 -4.0625448 -2.8965318 -0.17851996 2.443656 4.5862474 5.1896858 3.3578925 0.36457193 -2.4467788 -4.40299 -5.2389121 -4.7626033 -3.6835885][-3.5594268 -3.1857781 -3.5318818 -2.5312879 -0.35035789 1.4605221 2.6612153 2.5595164 0.59254944 -2.010011 -4.3111424 -5.7286921 -6.0828266 -5.2599387 -4.0150819][-3.3332319 -2.9027374 -3.1718669 -2.5369143 -1.1169523 -0.13507164 0.28779304 -0.30034769 -2.1716287 -4.239368 -5.901649 -6.7215185 -6.5572224 -5.4171357 -4.108233][-3.9860997 -3.478348 -3.6203871 -3.3272052 -2.615901 -2.2430139 -2.2761421 -3.0333972 -4.5964322 -6.1048875 -7.1633463 -7.4533634 -6.8357244 -5.4779496 -4.1724038][-5.0337596 -4.3871331 -4.4434509 -4.4558811 -4.3233528 -4.3437757 -4.5337029 -5.1376452 -6.1872592 -7.0912118 -7.5912809 -7.4645767 -6.5824971 -5.2029443 -4.0074558][-5.851965 -4.9398422 -4.9063191 -5.0913296 -5.324029 -5.5535245 -5.7667518 -6.1266317 -6.6472178 -7.0163326 -7.1233163 -6.841578 -6.0435066 -4.94435 -4.0155854][-6.2500477 -5.1251116 -5.0367589 -5.2627807 -5.6309624 -5.9439406 -6.160398 -6.3379879 -6.44653 -6.4227614 -6.2962537 -6.0060387 -5.4351549 -4.7275219 -4.1534882]]...]
INFO - root - 2017-12-15 06:08:55.656350: step 1710, loss = 0.42, batch loss = 0.38 (35.1 examples/sec; 0.228 sec/batch; 20h:55m:24s remains)
INFO - root - 2017-12-15 06:08:57.913225: step 1720, loss = 0.46, batch loss = 0.42 (35.8 examples/sec; 0.223 sec/batch; 20h:32m:06s remains)
INFO - root - 2017-12-15 06:09:00.178110: step 1730, loss = 0.24, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 20h:24m:38s remains)
INFO - root - 2017-12-15 06:09:02.442206: step 1740, loss = 0.28, batch loss = 0.24 (35.1 examples/sec; 0.228 sec/batch; 20h:56m:27s remains)
INFO - root - 2017-12-15 06:09:04.724004: step 1750, loss = 0.26, batch loss = 0.22 (32.6 examples/sec; 0.245 sec/batch; 22h:33m:01s remains)
INFO - root - 2017-12-15 06:09:06.971758: step 1760, loss = 0.35, batch loss = 0.31 (36.4 examples/sec; 0.220 sec/batch; 20h:10m:09s remains)
INFO - root - 2017-12-15 06:09:09.281156: step 1770, loss = 0.29, batch loss = 0.25 (35.4 examples/sec; 0.226 sec/batch; 20h:46m:01s remains)
INFO - root - 2017-12-15 06:09:11.525031: step 1780, loss = 0.38, batch loss = 0.34 (35.0 examples/sec; 0.229 sec/batch; 20h:59m:29s remains)
INFO - root - 2017-12-15 06:09:13.795830: step 1790, loss = 0.25, batch loss = 0.21 (36.2 examples/sec; 0.221 sec/batch; 20h:19m:29s remains)
INFO - root - 2017-12-15 06:09:16.041733: step 1800, loss = 0.23, batch loss = 0.19 (35.2 examples/sec; 0.227 sec/batch; 20h:51m:33s remains)
2017-12-15 06:09:16.318389: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5045385 -3.7971363 -3.621876 -3.1911626 -3.1699603 -3.5649161 -3.8524151 -3.834018 -3.9241748 -4.3012881 -4.5908818 -4.6799393 -4.5241795 -4.2721338 -3.9549613][-2.6449947 -3.4586921 -3.09205 -2.6595135 -2.810051 -3.3845522 -3.713757 -3.6604471 -3.7385831 -4.12516 -4.512301 -4.7205458 -4.6875944 -4.6211743 -4.4141226][-2.9087923 -3.1872294 -2.6613994 -2.2606068 -2.5679255 -3.2505143 -3.6166687 -3.6048741 -3.7506337 -4.1434946 -4.5531182 -4.8222761 -4.8326912 -4.8668475 -4.779428][-3.6488037 -3.366785 -2.6561191 -2.1275523 -2.2469482 -2.7224743 -3.0355077 -3.0373054 -3.1895518 -3.5034404 -3.8211355 -4.1151462 -4.2172709 -4.3750205 -4.544209][-4.92635 -4.29693 -3.440248 -2.558831 -2.1255093 -2.1014471 -2.2008808 -2.2482667 -2.4348991 -2.6543498 -2.8899517 -3.1485476 -3.1923971 -3.2525659 -3.5106421][-5.574863 -4.92789 -4.1578221 -3.0378919 -2.0039978 -1.3357582 -1.1538188 -1.1291316 -1.3036293 -1.5708807 -1.8724124 -2.1025407 -2.0149944 -1.900071 -2.129189][-4.6042824 -4.2732019 -3.8112333 -2.7172003 -1.2841462 -0.20210218 0.18318856 0.34313262 0.2535249 -0.08436656 -0.43447804 -0.582417 -0.31110859 0.042942882 -0.13415861][-2.4983871 -2.4629803 -2.3717439 -1.4471776 0.082423329 1.3435098 1.8752288 2.138267 2.0399408 1.5119678 0.96534526 0.76767647 1.1018633 1.6618112 1.5866216][-0.84063661 -1.0584685 -1.249594 -0.47442842 1.041797 2.39576 3.0648117 3.3466325 3.0469074 2.1688209 1.2902237 0.93364179 1.2614721 1.9864925 2.1420579][0.3562721 0.059965491 -0.25265169 0.30184066 1.5726477 2.8167982 3.4758935 3.645463 3.0558887 1.8844239 0.86271155 0.40599573 0.60511029 1.3771335 1.7622439][0.40131891 0.25630867 -0.0799644 0.18312418 0.91056406 1.7574757 2.3448749 2.4738021 1.8290612 0.72687232 -0.18900955 -0.8107537 -0.95049953 -0.37538135 0.071107507][-0.57498109 -0.45035446 -0.66934776 -0.6981982 -0.67875612 -0.4308145 -0.084657788 0.077812076 -0.22087216 -0.88934743 -1.611946 -2.3668957 -2.8538108 -2.6247268 -2.2582765][-2.0047228 -1.6383264 -1.6720483 -1.8664198 -2.3468623 -2.6602449 -2.6403682 -2.4826336 -2.4171176 -2.6256309 -3.0700865 -3.7698312 -4.3298655 -4.2485762 -3.9448485][-3.7088246 -3.2478662 -3.0620914 -3.1808262 -3.7711778 -4.3498292 -4.5556455 -4.4430437 -4.2021236 -4.1625495 -4.4272094 -4.9577446 -5.3664303 -5.2819357 -5.01679][-5.0546374 -4.640183 -4.3599997 -4.3267336 -4.7643723 -5.3206987 -5.57295 -5.5266414 -5.3094187 -5.205688 -5.3791337 -5.73584 -5.9901028 -5.9023557 -5.6980834]]...]
INFO - root - 2017-12-15 06:09:18.629358: step 1810, loss = 0.25, batch loss = 0.21 (35.3 examples/sec; 0.227 sec/batch; 20h:50m:30s remains)
INFO - root - 2017-12-15 06:09:20.912356: step 1820, loss = 0.30, batch loss = 0.26 (35.3 examples/sec; 0.227 sec/batch; 20h:50m:32s remains)
INFO - root - 2017-12-15 06:09:23.203174: step 1830, loss = 0.19, batch loss = 0.15 (36.1 examples/sec; 0.222 sec/batch; 20h:22m:02s remains)
INFO - root - 2017-12-15 06:09:25.463061: step 1840, loss = 0.26, batch loss = 0.22 (34.4 examples/sec; 0.232 sec/batch; 21h:21m:00s remains)
INFO - root - 2017-12-15 06:09:27.731986: step 1850, loss = 0.26, batch loss = 0.22 (34.9 examples/sec; 0.229 sec/batch; 21h:04m:33s remains)
INFO - root - 2017-12-15 06:09:29.987958: step 1860, loss = 0.23, batch loss = 0.19 (35.3 examples/sec; 0.227 sec/batch; 20h:49m:28s remains)
INFO - root - 2017-12-15 06:09:32.317181: step 1870, loss = 0.23, batch loss = 0.19 (33.0 examples/sec; 0.242 sec/batch; 22h:16m:02s remains)
INFO - root - 2017-12-15 06:09:34.561121: step 1880, loss = 0.27, batch loss = 0.23 (34.6 examples/sec; 0.232 sec/batch; 21h:15m:43s remains)
INFO - root - 2017-12-15 06:09:36.799643: step 1890, loss = 0.26, batch loss = 0.23 (35.9 examples/sec; 0.223 sec/batch; 20h:26m:17s remains)
INFO - root - 2017-12-15 06:09:39.041644: step 1900, loss = 0.29, batch loss = 0.25 (36.0 examples/sec; 0.222 sec/batch; 20h:24m:44s remains)
2017-12-15 06:09:39.323377: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7293048 -6.4266162 -6.6093369 -6.6508288 -6.643568 -6.5088592 -6.3027887 -6.1948557 -6.2302566 -6.4790826 -6.7269311 -6.6793876 -6.4304113 -6.3219919 -6.2344832][-5.8507838 -5.8936715 -6.1411147 -6.287291 -6.3021159 -6.0019765 -5.5514975 -5.2412391 -5.2154927 -5.533772 -5.9337029 -6.0427551 -5.8675776 -5.7741361 -5.6656218][-4.7100492 -4.5908256 -5.1765995 -5.4980273 -5.30798 -4.5906711 -3.8158269 -3.3662429 -3.4220085 -4.000195 -4.639966 -4.8594842 -4.5849571 -4.2882156 -3.9501698][-3.995717 -3.6579406 -4.5161505 -4.8686461 -4.20809 -2.942863 -1.8825471 -1.3733156 -1.5802441 -2.41861 -3.1867294 -3.2446451 -2.6188653 -1.9639502 -1.3504238][-3.2420106 -2.4335291 -3.5255313 -3.9869795 -2.9962168 -1.3765602 -0.16695452 0.33142197 0.019608855 -1.0314124 -1.8407216 -1.6486164 -0.68954682 0.20728695 0.93748033][-1.3419385 -0.34992027 -1.9218447 -2.7608385 -1.7135119 0.030630708 1.4229742 2.0179257 1.6435908 0.2865268 -0.81947851 -0.7337445 0.20583189 1.1198231 1.8398017][0.12479985 1.0085257 -0.9151783 -1.7971029 -0.61253452 1.1530699 2.55828 3.0885477 2.4788523 0.8356663 -0.57881629 -0.67131066 0.1078192 0.9934305 1.6430355][0.44221866 1.3127285 -0.60890734 -1.1552839 0.17403328 1.7336401 2.8266115 3.1576333 2.348949 0.70145547 -0.78413904 -1.0713584 -0.5678488 0.11287344 0.5563432][-0.078649759 0.70720589 -0.97045135 -1.0883749 0.18985355 1.3560513 2.1314402 2.3853045 1.6728154 0.36651933 -0.99875969 -1.5282383 -1.5247968 -1.2869952 -1.1938913][-1.3287523 -0.7420876 -2.0050213 -1.6793298 -0.48752761 0.50066817 1.175496 1.3974553 0.86467135 -0.047482848 -1.231035 -1.9378967 -2.4007728 -2.6043487 -2.8334451][-3.0308356 -2.6575432 -3.421979 -2.7232814 -1.548826 -0.54850459 0.00015556812 -0.057327509 -0.5447489 -1.0888751 -1.9218702 -2.5627217 -3.1984754 -3.6533961 -4.0754337][-4.303823 -3.9535794 -4.2481904 -3.3953609 -2.3426242 -1.422689 -1.0726469 -1.4460332 -1.8334111 -1.9448963 -2.2709229 -2.6428995 -3.284008 -3.8876104 -4.5119529][-4.5078244 -4.1966343 -4.4730535 -3.9177551 -3.2285576 -2.48834 -2.1334159 -2.4314177 -2.4909697 -2.2742326 -2.3361354 -2.6473608 -3.3645811 -4.0556197 -4.727232][-3.9450121 -3.8531055 -4.504209 -4.5268111 -4.2275167 -3.7035103 -3.381444 -3.4739366 -3.3288293 -3.0817709 -3.1360133 -3.4492865 -4.0466065 -4.4891391 -4.8089166][-2.9127014 -3.0284808 -4.0004749 -4.4425778 -4.5013089 -4.4170771 -4.4904389 -4.7622285 -4.7721825 -4.6367497 -4.5664067 -4.5274954 -4.687057 -4.6504216 -4.4980793]]...]
INFO - root - 2017-12-15 06:09:41.593786: step 1910, loss = 0.22, batch loss = 0.18 (36.6 examples/sec; 0.219 sec/batch; 20h:04m:59s remains)
INFO - root - 2017-12-15 06:09:43.870578: step 1920, loss = 0.23, batch loss = 0.19 (34.7 examples/sec; 0.230 sec/batch; 21h:09m:36s remains)
INFO - root - 2017-12-15 06:09:46.114095: step 1930, loss = 0.37, batch loss = 0.33 (36.0 examples/sec; 0.222 sec/batch; 20h:23m:18s remains)
INFO - root - 2017-12-15 06:09:48.393776: step 1940, loss = 0.26, batch loss = 0.22 (36.8 examples/sec; 0.217 sec/batch; 19h:57m:25s remains)
INFO - root - 2017-12-15 06:09:50.654333: step 1950, loss = 0.27, batch loss = 0.24 (34.8 examples/sec; 0.230 sec/batch; 21h:07m:12s remains)
INFO - root - 2017-12-15 06:09:52.910643: step 1960, loss = 0.34, batch loss = 0.30 (34.8 examples/sec; 0.230 sec/batch; 21h:08m:07s remains)
INFO - root - 2017-12-15 06:09:55.170599: step 1970, loss = 0.38, batch loss = 0.34 (36.0 examples/sec; 0.222 sec/batch; 20h:25m:30s remains)
INFO - root - 2017-12-15 06:09:57.438499: step 1980, loss = 0.20, batch loss = 0.16 (35.4 examples/sec; 0.226 sec/batch; 20h:44m:39s remains)
INFO - root - 2017-12-15 06:09:59.721125: step 1990, loss = 0.39, batch loss = 0.35 (36.3 examples/sec; 0.221 sec/batch; 20h:14m:52s remains)
INFO - root - 2017-12-15 06:10:01.988920: step 2000, loss = 0.26, batch loss = 0.22 (34.7 examples/sec; 0.231 sec/batch; 21h:10m:56s remains)
2017-12-15 06:10:02.295173: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0330052 -4.7780957 -4.8608961 -4.9096289 -4.9174671 -4.9397707 -5.0486259 -5.2384667 -5.5355268 -5.9118023 -6.1734705 -6.2916031 -6.2570877 -6.0654793 -5.8313169][-4.1892662 -5.5940342 -5.7724819 -5.8539333 -5.78899 -5.6758785 -5.6479769 -5.7174721 -5.908659 -6.2385082 -6.4583721 -6.4948578 -6.3769989 -6.14937 -5.9745464][-5.0908456 -6.0778723 -6.3469782 -6.4348593 -6.2258368 -5.8601985 -5.5762777 -5.4069262 -5.3463936 -5.5436 -5.7152667 -5.6737928 -5.494545 -5.3073397 -5.3158951][-5.9372196 -6.481575 -6.7422657 -6.7323942 -6.2336864 -5.4936094 -4.8420134 -4.3749037 -4.0514345 -4.088953 -4.227582 -4.1602573 -3.9901402 -3.9345646 -4.2060456][-6.6742973 -6.7553444 -6.927763 -6.7403393 -5.8718481 -4.6541796 -3.5258477 -2.7581203 -2.1848257 -2.0821292 -2.2641439 -2.2939157 -2.2643862 -2.4223185 -3.0203156][-6.703619 -6.4555798 -6.3961143 -5.96766 -4.7537432 -3.1103725 -1.6295978 -0.68239439 -0.030998111 0.030038834 -0.33082151 -0.66534972 -0.95618904 -1.3618321 -2.2299933][-6.1561213 -5.585166 -5.1205225 -4.4082708 -2.9090903 -0.98672467 0.66569734 1.6093545 2.0307748 1.7628217 1.0198789 0.13189793 -0.60737169 -1.2617404 -2.246222][-5.345892 -4.322547 -3.432133 -2.5165122 -0.95064712 1.0675151 2.717068 3.4388492 3.4480374 2.7579567 1.6175933 0.1558485 -1.063351 -1.9122245 -2.8501897][-4.6346073 -3.228905 -2.0839372 -1.1179503 0.272305 2.1889956 3.6620505 4.0063019 3.6098382 2.5878146 1.199338 -0.64840007 -2.1852958 -3.0663123 -3.793241][-3.9756956 -2.2549088 -0.98606056 -0.10925961 0.89797497 2.3671424 3.3476713 3.1786754 2.4575446 1.3006065 -0.10227704 -2.0181754 -3.5569315 -4.2764187 -4.6835675][-3.909677 -2.1578283 -1.0249218 -0.38568974 0.20226121 1.1573789 1.635525 1.1083679 0.32140517 -0.71913815 -1.9003654 -3.5280108 -4.7615128 -5.1845703 -5.2549973][-4.7227254 -3.2675171 -2.5242505 -2.1632195 -1.8480525 -1.2273793 -1.0112598 -1.5799444 -2.1795206 -2.9092128 -3.7111855 -4.7974787 -5.5257998 -5.6246424 -5.4455242][-5.7868443 -4.8439713 -4.6060038 -4.5260487 -4.3841596 -3.9833875 -3.8324978 -4.1540604 -4.3831949 -4.6650429 -4.9560261 -5.4187632 -5.659934 -5.5555081 -5.2861171][-6.56901 -6.060113 -6.1666727 -6.2266474 -6.1315374 -5.8400459 -5.6848016 -5.7259245 -5.6501436 -5.582799 -5.4736638 -5.4639006 -5.38783 -5.2162209 -4.9684095][-6.9135504 -6.6288805 -6.8506765 -6.9198475 -6.8248482 -6.5791836 -6.35781 -6.1899977 -5.95562 -5.7077694 -5.416604 -5.1992149 -5.0179577 -4.8706656 -4.6896296]]...]
INFO - root - 2017-12-15 06:10:04.561816: step 2010, loss = 0.33, batch loss = 0.30 (36.3 examples/sec; 0.220 sec/batch; 20h:13m:01s remains)
INFO - root - 2017-12-15 06:10:06.832055: step 2020, loss = 0.27, batch loss = 0.23 (34.8 examples/sec; 0.230 sec/batch; 21h:04m:54s remains)
INFO - root - 2017-12-15 06:10:09.124084: step 2030, loss = 0.28, batch loss = 0.24 (36.3 examples/sec; 0.221 sec/batch; 20h:15m:01s remains)
INFO - root - 2017-12-15 06:10:11.385233: step 2040, loss = 0.26, batch loss = 0.22 (34.7 examples/sec; 0.230 sec/batch; 21h:08m:08s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 06:10:13.659297: step 2050, loss = 0.50, batch loss = 0.46 (34.4 examples/sec; 0.233 sec/batch; 21h:22m:39s remains)
INFO - root - 2017-12-15 06:10:15.941237: step 2060, loss = 0.34, batch loss = 0.30 (35.8 examples/sec; 0.224 sec/batch; 20h:31m:00s remains)
INFO - root - 2017-12-15 06:10:18.173859: step 2070, loss = 0.28, batch loss = 0.24 (36.1 examples/sec; 0.221 sec/batch; 20h:19m:10s remains)
INFO - root - 2017-12-15 06:10:20.433697: step 2080, loss = 0.40, batch loss = 0.37 (35.2 examples/sec; 0.228 sec/batch; 20h:53m:04s remains)
INFO - root - 2017-12-15 06:10:22.716464: step 2090, loss = 0.21, batch loss = 0.17 (35.3 examples/sec; 0.227 sec/batch; 20h:48m:45s remains)
INFO - root - 2017-12-15 06:10:24.983292: step 2100, loss = 0.27, batch loss = 0.23 (35.0 examples/sec; 0.229 sec/batch; 20h:58m:27s remains)
2017-12-15 06:10:25.294879: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.72222507 -2.3621531 -3.0478969 -3.3486657 -3.5728073 -3.7741666 -4.0743904 -4.4090891 -4.5062327 -4.9366055 -5.9659257 -7.0034461 -7.4665508 -7.1433411 -6.0214548][-1.3360312 -2.465694 -3.2818296 -3.6679063 -3.8128893 -4.1422253 -4.6516771 -5.1924973 -5.6410966 -6.4879394 -7.7030668 -8.8111286 -9.1688976 -8.6323929 -7.3881159][-2.1366436 -2.537854 -3.129241 -3.2728996 -3.1184554 -3.2972684 -3.7336941 -4.3466959 -5.1265936 -6.3135657 -7.6818352 -8.8233051 -9.1391315 -8.5729961 -7.5385461][-3.0122924 -2.8160107 -2.9092946 -2.4671831 -1.663039 -1.2810502 -1.331671 -1.8791358 -2.9470048 -4.4947329 -6.1395736 -7.5296173 -8.0424271 -7.6473351 -6.9980865][-3.9859416 -3.3660035 -2.9335945 -1.8386738 -0.29432988 1.0205688 1.7742445 1.5881577 0.319623 -1.702353 -3.9097772 -5.8235254 -6.7382746 -6.6641841 -6.4691339][-4.759788 -4.0145006 -3.2550664 -1.7209699 0.4031477 2.686388 4.4546976 4.8887539 3.6148386 1.1839442 -1.6632389 -4.22958 -5.6746569 -6.0159082 -6.220314][-4.804162 -4.1087055 -3.2862358 -1.7859911 0.37980223 3.1574516 5.6645036 6.6959343 5.5758142 2.9659281 -0.21410263 -3.1966085 -5.0943604 -5.8490605 -6.2950773][-4.4863515 -3.7315092 -3.0691249 -1.9025288 -0.07033658 2.7099895 5.4865317 6.8619957 5.981607 3.4627752 0.29915905 -2.7370286 -4.801796 -5.8307748 -6.3784361][-4.0090942 -3.2396603 -2.788929 -2.0424871 -0.64638984 1.8433974 4.5255666 6.0009813 5.3907137 3.128849 0.1991961 -2.5877233 -4.5128369 -5.616446 -6.1845183][-4.1139588 -3.2711926 -2.9019816 -2.4190853 -1.360195 0.75064778 3.1607118 4.5985155 4.2770114 2.4267449 -0.059680343 -2.3167419 -3.7740979 -4.7408981 -5.3115315][-5.1185594 -4.0868926 -3.7055874 -3.3410692 -2.4900677 -0.7394079 1.3401687 2.7111039 2.7187638 1.3891001 -0.46238983 -1.9458718 -2.7067337 -3.3805637 -3.9828119][-6.6623669 -5.4702597 -5.0433331 -4.7489357 -4.0669074 -2.7215662 -1.1061416 0.12741542 0.51144028 -0.10498452 -1.0845964 -1.5216405 -1.329253 -1.518188 -2.1390996][-7.8569231 -6.6631746 -6.26739 -6.125421 -5.6880674 -4.8063784 -3.7924063 -2.8951752 -2.3248227 -2.2351127 -2.2099304 -1.4437194 -0.091466665 0.51141691 0.085123777][-8.14656 -7.2012405 -7.0159297 -7.144958 -6.9847817 -6.4729767 -6.0219951 -5.6285734 -5.1858463 -4.7186203 -3.9156747 -2.1278446 0.31997633 1.9364703 2.0770669][-7.3051758 -6.7978191 -6.9509754 -7.451086 -7.6077724 -7.3848181 -7.3392425 -7.4423189 -7.3920889 -7.0095925 -5.9758863 -3.7759733 -0.748065 1.6793823 2.5791354]]...]
INFO - root - 2017-12-15 06:10:27.584747: step 2110, loss = 0.42, batch loss = 0.38 (34.8 examples/sec; 0.230 sec/batch; 21h:04m:56s remains)
INFO - root - 2017-12-15 06:10:29.852578: step 2120, loss = 0.26, batch loss = 0.22 (36.1 examples/sec; 0.222 sec/batch; 20h:19m:41s remains)
INFO - root - 2017-12-15 06:10:32.097568: step 2130, loss = 0.23, batch loss = 0.20 (35.1 examples/sec; 0.228 sec/batch; 20h:56m:21s remains)
INFO - root - 2017-12-15 06:10:34.353119: step 2140, loss = 0.31, batch loss = 0.27 (35.3 examples/sec; 0.226 sec/batch; 20h:46m:44s remains)
INFO - root - 2017-12-15 06:10:36.632210: step 2150, loss = 0.33, batch loss = 0.29 (34.4 examples/sec; 0.233 sec/batch; 21h:20m:21s remains)
INFO - root - 2017-12-15 06:10:38.937325: step 2160, loss = 0.28, batch loss = 0.24 (35.9 examples/sec; 0.223 sec/batch; 20h:28m:23s remains)
INFO - root - 2017-12-15 06:10:41.180751: step 2170, loss = 0.33, batch loss = 0.29 (35.2 examples/sec; 0.227 sec/batch; 20h:49m:35s remains)
INFO - root - 2017-12-15 06:10:43.470360: step 2180, loss = 0.19, batch loss = 0.15 (34.9 examples/sec; 0.229 sec/batch; 21h:02m:26s remains)
INFO - root - 2017-12-15 06:10:45.722780: step 2190, loss = 0.40, batch loss = 0.36 (35.7 examples/sec; 0.224 sec/batch; 20h:32m:31s remains)
INFO - root - 2017-12-15 06:10:47.965886: step 2200, loss = 0.30, batch loss = 0.26 (36.2 examples/sec; 0.221 sec/batch; 20h:16m:05s remains)
2017-12-15 06:10:48.257606: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1182079 -6.2852359 -5.453764 -4.0205817 -2.8405428 -1.7315656 -0.73202753 -0.36300623 -0.87996221 -1.5166556 -1.9354016 -2.5356479 -3.0910606 -3.1749921 -2.9575338][-4.6569333 -5.3751645 -4.3734236 -2.9241076 -1.8326317 -0.82308292 0.14866316 0.47830498 -0.18591392 -1.1470044 -1.8252196 -2.5464363 -3.2907166 -3.633687 -3.6360857][-4.3909082 -4.5431285 -3.4334481 -2.0433953 -1.0005502 0.045102239 1.0678624 1.3464669 0.47073543 -0.88371909 -1.9741129 -3.0029712 -3.941668 -4.4168148 -4.4554944][-4.1697707 -3.933681 -2.7480681 -1.3499645 -0.18797159 1.0919577 2.2601151 2.4352093 1.2138644 -0.71411574 -2.4344409 -3.8780351 -4.8645048 -5.1753306 -4.9131932][-4.2536149 -3.7765584 -2.5331833 -0.9979707 0.49885666 2.2263889 3.6636524 3.8050618 2.2549186 -0.25034297 -2.667866 -4.5577426 -5.5103288 -5.5266471 -4.7961831][-4.2636547 -3.8074107 -2.532825 -0.82149661 1.1109406 3.4435024 5.311625 5.631597 3.9448414 0.99964225 -2.0918505 -4.5132966 -5.5994711 -5.4855227 -4.4255037][-4.4519515 -4.170692 -2.8722205 -1.0301871 1.2358521 4.0369239 6.2199388 6.7346005 5.0835366 1.9215485 -1.6292648 -4.413518 -5.5738664 -5.2957892 -4.004302][-5.3574739 -5.0499725 -3.7011504 -1.7957836 0.61286771 3.5641251 5.7975726 6.4341197 4.9459152 1.9138948 -1.6564829 -4.4569154 -5.62312 -5.2547369 -3.8862376][-6.6728806 -6.1081882 -4.7197189 -2.9062059 -0.58706677 2.260015 4.4028187 5.0958695 3.8938684 1.3041028 -1.9097912 -4.4829206 -5.6657372 -5.4187164 -4.33572][-7.9811583 -7.0470138 -5.705586 -4.1740518 -2.1770554 0.33179486 2.2319355 2.8722868 1.9751259 -0.0466969 -2.7017531 -4.8775196 -5.9103651 -5.7353506 -5.0913711][-8.70355 -7.5121889 -6.3684435 -5.2508621 -3.750129 -1.7925342 -0.30115128 0.15373623 -0.48942995 -1.9622984 -4.026154 -5.7397361 -6.4649086 -6.2904673 -6.0562506][-8.7016859 -7.4283791 -6.5170612 -5.7814145 -4.7869115 -3.4407465 -2.4075298 -2.1801884 -2.6454554 -3.7035446 -5.2824655 -6.6027122 -7.0480981 -6.9530745 -7.1558447][-8.0822811 -6.8823633 -6.2199731 -5.7751422 -5.1721516 -4.3442254 -3.7605958 -3.7976508 -4.2335148 -5.0688248 -6.2804184 -7.2267942 -7.4160275 -7.4228272 -7.9460692][-7.0660286 -6.0671268 -5.6130252 -5.3322086 -4.9901972 -4.5813127 -4.4175715 -4.6840925 -5.1192036 -5.8029952 -6.7138929 -7.3152347 -7.310667 -7.4138069 -8.05903][-6.2068038 -5.4208674 -5.0852551 -4.8506193 -4.627574 -4.4887619 -4.5975842 -4.9367347 -5.2885685 -5.795814 -6.4320745 -6.7977648 -6.7818503 -6.9911 -7.5914526]]...]
INFO - root - 2017-12-15 06:10:50.502293: step 2210, loss = 0.29, batch loss = 0.25 (35.9 examples/sec; 0.223 sec/batch; 20h:25m:54s remains)
INFO - root - 2017-12-15 06:10:52.764897: step 2220, loss = 0.27, batch loss = 0.23 (35.0 examples/sec; 0.229 sec/batch; 20h:58m:13s remains)
INFO - root - 2017-12-15 06:10:55.055064: step 2230, loss = 0.26, batch loss = 0.22 (35.9 examples/sec; 0.223 sec/batch; 20h:27m:46s remains)
INFO - root - 2017-12-15 06:10:57.336190: step 2240, loss = 0.25, batch loss = 0.21 (35.5 examples/sec; 0.225 sec/batch; 20h:41m:08s remains)
INFO - root - 2017-12-15 06:10:59.592723: step 2250, loss = 0.35, batch loss = 0.31 (34.8 examples/sec; 0.230 sec/batch; 21h:03m:56s remains)
INFO - root - 2017-12-15 06:11:01.886487: step 2260, loss = 0.39, batch loss = 0.35 (34.7 examples/sec; 0.230 sec/batch; 21h:08m:36s remains)
INFO - root - 2017-12-15 06:11:04.149810: step 2270, loss = 0.34, batch loss = 0.30 (35.3 examples/sec; 0.226 sec/batch; 20h:46m:05s remains)
INFO - root - 2017-12-15 06:11:06.447419: step 2280, loss = 0.36, batch loss = 0.32 (35.8 examples/sec; 0.224 sec/batch; 20h:30m:19s remains)
INFO - root - 2017-12-15 06:11:08.742279: step 2290, loss = 0.26, batch loss = 0.22 (35.4 examples/sec; 0.226 sec/batch; 20h:43m:15s remains)
INFO - root - 2017-12-15 06:11:11.045140: step 2300, loss = 0.24, batch loss = 0.20 (36.1 examples/sec; 0.222 sec/batch; 20h:19m:58s remains)
2017-12-15 06:11:11.305678: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2749465 -4.1578174 -3.9423549 -3.6209226 -3.1882038 -2.7095284 -2.2024312 -1.7486591 -1.3971131 -0.69972861 0.50084496 1.0470707 0.4258709 -0.50926566 -1.1293671][-3.247999 -3.7541122 -3.3180144 -2.8726292 -2.5050566 -2.0293961 -1.4722854 -0.813797 -0.20269632 0.28535151 0.79080725 0.83147192 0.18772793 -0.59117377 -1.1199633][-3.1052177 -3.2779129 -2.8544674 -2.4782948 -2.2671709 -1.774097 -1.0275962 -0.059105277 0.79192805 1.0061581 0.93311548 0.67306352 0.12754798 -0.60993433 -1.2031019][-2.6635861 -3.0196545 -3.060791 -2.9380453 -2.7690723 -1.970145 -0.66559088 0.73523331 1.6486433 1.5246098 0.9721787 0.46243238 -0.0231148 -0.7943058 -1.5155108][-3.0350385 -3.5134897 -3.9015429 -3.5340128 -2.8087244 -1.3236659 0.58736873 1.9920363 2.3282561 1.629864 0.64801526 -0.1030426 -0.67283654 -1.4799089 -2.2128327][-3.2606664 -3.6553879 -4.0307016 -3.081233 -1.5526873 0.58365631 2.6425471 3.3613648 2.6801839 1.4202271 0.34103608 -0.481215 -1.2093608 -2.0882914 -2.7249823][-2.9075727 -2.9552219 -3.0479794 -1.5532857 0.3628242 2.3884888 3.8386989 3.6506734 2.3027039 0.963289 0.0648489 -0.76529288 -1.6312621 -2.4845567 -2.9803417][-2.1266165 -1.8162543 -1.9506145 -0.53213394 1.1083975 2.4246187 2.9739938 2.2613912 0.98559904 0.076015949 -0.47117972 -1.1325552 -2.0317585 -2.7812397 -3.0592532][-1.3164871 -1.0507166 -1.7404954 -1.0496079 -0.086496592 0.62312984 0.67993259 0.066626549 -0.61353886 -0.97247839 -1.2043343 -1.6452125 -2.3727479 -2.8276589 -2.7608666][-1.1339407 -1.12413 -2.2345464 -2.0360396 -1.4711279 -1.0384679 -1.1340967 -1.4749551 -1.6407294 -1.725228 -1.8952674 -2.2134504 -2.5868552 -2.6119821 -2.3364356][-1.7854438 -1.8023673 -2.6868503 -2.5296969 -2.0204885 -1.59791 -1.5839769 -1.732178 -1.778335 -1.9575034 -2.2440431 -2.4706118 -2.5323894 -2.3343644 -2.1229327][-2.4227471 -2.2547166 -2.7047267 -2.540554 -2.1106081 -1.6550132 -1.5603099 -1.6981055 -1.853212 -2.1301651 -2.3626876 -2.4934468 -2.4574652 -2.2726345 -2.2391675][-2.5437057 -2.0686488 -2.2528334 -2.2319403 -2.0725293 -1.8201673 -1.8047049 -1.9929152 -2.2094986 -2.4387598 -2.5331297 -2.5127444 -2.4368594 -2.3401189 -2.4779236][-2.7893271 -2.0717607 -2.0122397 -2.1649418 -2.3408275 -2.3548176 -2.4094052 -2.4925184 -2.6027672 -2.6727378 -2.5503836 -2.3646817 -2.2936254 -2.3716023 -2.649781][-3.1714463 -2.2977045 -2.073565 -2.2750397 -2.5822859 -2.7274406 -2.7959681 -2.7465115 -2.7013612 -2.6394734 -2.3744414 -2.1643631 -2.265631 -2.568656 -2.9050918]]...]
INFO - root - 2017-12-15 06:11:13.557481: step 2310, loss = 0.25, batch loss = 0.21 (34.9 examples/sec; 0.229 sec/batch; 20h:59m:52s remains)
INFO - root - 2017-12-15 06:11:15.817146: step 2320, loss = 0.28, batch loss = 0.24 (36.5 examples/sec; 0.219 sec/batch; 20h:05m:04s remains)
INFO - root - 2017-12-15 06:11:18.112122: step 2330, loss = 0.23, batch loss = 0.19 (34.3 examples/sec; 0.233 sec/batch; 21h:22m:06s remains)
INFO - root - 2017-12-15 06:11:20.371653: step 2340, loss = 0.24, batch loss = 0.20 (35.2 examples/sec; 0.227 sec/batch; 20h:50m:05s remains)
INFO - root - 2017-12-15 06:11:22.642095: step 2350, loss = 0.28, batch loss = 0.24 (36.0 examples/sec; 0.222 sec/batch; 20h:23m:49s remains)
INFO - root - 2017-12-15 06:11:24.885759: step 2360, loss = 0.32, batch loss = 0.28 (36.0 examples/sec; 0.222 sec/batch; 20h:21m:48s remains)
INFO - root - 2017-12-15 06:11:27.213071: step 2370, loss = 0.26, batch loss = 0.22 (35.3 examples/sec; 0.226 sec/batch; 20h:46m:02s remains)
INFO - root - 2017-12-15 06:11:29.457209: step 2380, loss = 0.24, batch loss = 0.20 (35.7 examples/sec; 0.224 sec/batch; 20h:32m:45s remains)
INFO - root - 2017-12-15 06:11:31.715253: step 2390, loss = 0.24, batch loss = 0.20 (35.3 examples/sec; 0.227 sec/batch; 20h:47m:55s remains)
INFO - root - 2017-12-15 06:11:34.001534: step 2400, loss = 0.23, batch loss = 0.19 (35.6 examples/sec; 0.225 sec/batch; 20h:36m:24s remains)
2017-12-15 06:11:34.299964: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.65320885 0.30620778 -0.73560762 -2.2295637 -3.8908534 -4.8582 -3.6694939 -2.4843168 -2.3898876 -2.9851689 -3.8179312 -4.9882503 -6.2106242 -6.810884 -6.3927469][-0.45927465 -0.53416 -1.4827492 -2.3821561 -3.3682685 -4.063756 -3.03881 -2.0355797 -2.1919115 -2.8369682 -3.515 -4.6938987 -6.0782676 -6.8423958 -6.507247][-1.0825837 -1.3112721 -2.476229 -3.1762295 -3.6588674 -3.8487687 -2.6542153 -1.5900401 -1.8621174 -2.5256851 -3.1888013 -4.4884267 -6.0118985 -6.8425417 -6.5619397][-1.5862031 -2.0546455 -3.5079226 -4.1346865 -4.1466789 -3.6026652 -1.8725289 -0.52934015 -0.78598452 -1.6201394 -2.6014457 -4.2393546 -5.9635072 -6.9027486 -6.767014][-2.1382565 -2.6534634 -4.1559806 -4.6454206 -4.0991287 -2.6651225 -0.2083112 1.4690405 1.2191478 0.050464749 -1.4734554 -3.5971611 -5.5783525 -6.6947064 -6.7631264][-2.5189359 -2.8625872 -4.0395947 -4.176713 -3.0271115 -0.8215884 2.2987237 4.3954325 4.1917806 2.5289807 0.24183476 -2.5875685 -5.0201082 -6.384161 -6.6393971][-2.1892519 -2.3663187 -3.0531962 -2.7633195 -1.1525149 1.4445089 5.026505 7.6638589 7.6290593 5.4809136 2.4051776 -1.30128 -4.3865037 -6.0699687 -6.4993067][-1.6992352 -1.5106874 -1.7692357 -1.2299817 0.38939822 2.8131161 6.4676833 9.4716482 9.6438942 7.3154416 3.8132377 -0.542588 -4.1070409 -5.9274759 -6.4044409][-1.4033484 -0.945519 -1.0102153 -0.59150243 0.60948813 2.4203544 5.6716361 8.6547918 8.9953575 6.9014773 3.5003052 -0.9954288 -4.5957818 -6.2209721 -6.5132451][-1.5663209 -0.9286691 -1.0339575 -0.96358645 -0.34704316 0.6957289 3.181428 5.7410893 6.1829782 4.5887976 1.67249 -2.4119709 -5.5821109 -6.746491 -6.6762][-2.6459031 -1.8952377 -2.0361915 -2.2927299 -2.2529495 -1.9361064 -0.3609997 1.5223953 2.0173187 1.0428032 -1.1437414 -4.3494334 -6.7362852 -7.432148 -7.0043225][-4.8857703 -4.0430675 -4.2057552 -4.56519 -4.8215671 -4.9622116 -4.1416507 -2.9096661 -2.4398894 -2.9280014 -4.3133469 -6.3436289 -7.7472787 -7.9615707 -7.2667303][-6.5294356 -5.6101069 -5.7187009 -6.0146713 -6.3194242 -6.6234217 -6.3122611 -5.6197362 -5.2560449 -5.4365993 -6.1523666 -7.1175113 -7.6654496 -7.5079908 -6.6577954][-6.525249 -5.6039453 -5.6884332 -5.9040513 -6.1523938 -6.4253626 -6.3922787 -6.0802231 -5.8391786 -5.8460212 -6.0787735 -6.334806 -6.3748326 -6.0596337 -5.2487278][-5.223218 -4.3718843 -4.4491463 -4.5836382 -4.774169 -4.9985228 -5.0754175 -4.9529686 -4.8056107 -4.7521544 -4.7604523 -4.72927 -4.6043468 -4.328618 -3.714056]]...]
INFO - root - 2017-12-15 06:11:36.610616: step 2410, loss = 0.33, batch loss = 0.29 (34.5 examples/sec; 0.232 sec/batch; 21h:16m:16s remains)
INFO - root - 2017-12-15 06:11:38.889229: step 2420, loss = 0.39, batch loss = 0.35 (34.3 examples/sec; 0.233 sec/batch; 21h:23m:57s remains)
INFO - root - 2017-12-15 06:11:41.148176: step 2430, loss = 0.22, batch loss = 0.18 (36.4 examples/sec; 0.220 sec/batch; 20h:08m:45s remains)
INFO - root - 2017-12-15 06:11:43.436433: step 2440, loss = 0.26, batch loss = 0.22 (34.9 examples/sec; 0.229 sec/batch; 21h:01m:13s remains)
INFO - root - 2017-12-15 06:11:45.722722: step 2450, loss = 0.24, batch loss = 0.21 (34.7 examples/sec; 0.231 sec/batch; 21h:08m:29s remains)
INFO - root - 2017-12-15 06:11:47.984448: step 2460, loss = 0.22, batch loss = 0.18 (36.0 examples/sec; 0.223 sec/batch; 20h:23m:54s remains)
INFO - root - 2017-12-15 06:11:50.268459: step 2470, loss = 0.27, batch loss = 0.23 (34.3 examples/sec; 0.233 sec/batch; 21h:22m:41s remains)
INFO - root - 2017-12-15 06:11:52.518611: step 2480, loss = 0.26, batch loss = 0.22 (35.0 examples/sec; 0.229 sec/batch; 20h:58m:07s remains)
INFO - root - 2017-12-15 06:11:54.816617: step 2490, loss = 0.34, batch loss = 0.30 (35.5 examples/sec; 0.225 sec/batch; 20h:40m:12s remains)
INFO - root - 2017-12-15 06:11:57.082162: step 2500, loss = 0.22, batch loss = 0.18 (36.0 examples/sec; 0.223 sec/batch; 20h:23m:45s remains)
2017-12-15 06:11:57.343206: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1644473 -5.3757744 -5.5037646 -5.4891624 -5.3605056 -5.2356911 -5.2997475 -5.376092 -5.6128292 -5.7598944 -5.646863 -5.4630909 -5.2644286 -5.0952778 -4.82514][-5.7206988 -6.6037474 -6.8129091 -6.7897081 -6.56054 -6.2349782 -6.1089725 -6.1842604 -6.6017075 -6.8166337 -6.6773195 -6.5235128 -6.3662977 -6.2102628 -5.864131][-6.4581032 -6.899323 -7.1256986 -7.0478144 -6.676105 -6.0614243 -5.6082797 -5.5941133 -6.1430426 -6.5080733 -6.5510011 -6.6990285 -6.8898296 -6.9851246 -6.6351056][-6.3089933 -6.2384295 -6.3306766 -5.977077 -5.2290397 -4.1106534 -3.1966305 -3.1200285 -3.9863982 -4.8171139 -5.2855129 -5.78077 -6.3402267 -6.7898927 -6.5449529][-6.641748 -6.1710134 -6.0098014 -5.1724062 -3.8886003 -2.2529569 -0.89672434 -0.69679546 -1.7905197 -3.0765038 -3.9168327 -4.5910034 -5.4087176 -6.2562714 -6.2115021][-6.4781127 -5.757432 -5.294816 -4.0096354 -2.3655226 -0.44828498 1.205624 1.6093789 0.53269613 -1.0903482 -2.286844 -3.1354136 -4.212285 -5.4912434 -5.7233429][-5.2661576 -4.5322046 -3.9875007 -2.4923496 -0.60642004 1.6058642 3.6514349 4.4181528 3.5088024 1.5575002 -0.070584416 -1.1086532 -2.2899246 -3.7840166 -4.3314104][-4.21752 -3.6487379 -3.3718071 -2.0591006 -0.27744389 1.9553689 4.1664286 5.1536551 4.5008383 2.6207595 0.99831641 0.018701673 -1.0432971 -2.4979861 -3.3638654][-3.9800332 -3.5623717 -3.5533679 -2.5494552 -1.0875797 0.89780152 2.9795055 3.9409704 3.62052 2.2859077 1.057876 0.18106401 -0.81814325 -2.1736767 -3.3140478][-4.6183572 -4.1523366 -4.2152867 -3.475317 -2.2876091 -0.47927344 1.4890927 2.4466157 2.498086 1.8565997 1.071937 0.15208113 -0.97262549 -2.2882826 -3.6080966][-5.4921761 -4.864459 -4.9486709 -4.4865804 -3.5757294 -2.0353847 -0.35410345 0.44429052 0.76233709 0.87697995 0.70315278 -0.055086732 -1.2185725 -2.5450206 -3.9938102][-6.8143144 -6.1065812 -6.10663 -5.6762657 -4.8640785 -3.6180618 -2.4631958 -2.0556617 -1.6960763 -0.98350787 -0.51714993 -0.97198617 -2.1280689 -3.5385051 -5.0029607][-8.0658884 -7.2556386 -7.1041465 -6.5703397 -5.7764935 -4.8611965 -4.2217855 -4.0842338 -3.698859 -2.7047868 -1.9385265 -2.2512498 -3.4711192 -5.0209656 -6.4315586][-8.5254345 -7.6728477 -7.4822912 -7.0044146 -6.4408703 -5.9870391 -5.8206282 -5.8290405 -5.4457836 -4.4790726 -3.6543617 -3.7906506 -4.8357363 -6.2126765 -7.3524523][-8.7794027 -8.100481 -8.0111094 -7.6283388 -7.2383 -7.05915 -7.1322408 -7.20541 -6.9227791 -6.2118125 -5.5138063 -5.4175768 -6.0056715 -6.8994551 -7.6009746]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-2500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-2500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 06:12:00.100000: step 2510, loss = 0.26, batch loss = 0.22 (35.8 examples/sec; 0.224 sec/batch; 20h:30m:12s remains)
INFO - root - 2017-12-15 06:12:02.360893: step 2520, loss = 0.31, batch loss = 0.27 (35.6 examples/sec; 0.225 sec/batch; 20h:36m:56s remains)
INFO - root - 2017-12-15 06:12:04.624365: step 2530, loss = 0.32, batch loss = 0.28 (34.8 examples/sec; 0.230 sec/batch; 21h:02m:58s remains)
INFO - root - 2017-12-15 06:12:06.870086: step 2540, loss = 0.25, batch loss = 0.22 (34.8 examples/sec; 0.230 sec/batch; 21h:04m:59s remains)
INFO - root - 2017-12-15 06:12:09.151311: step 2550, loss = 0.33, batch loss = 0.29 (34.8 examples/sec; 0.230 sec/batch; 21h:02m:50s remains)
INFO - root - 2017-12-15 06:12:11.421607: step 2560, loss = 0.35, batch loss = 0.31 (34.3 examples/sec; 0.233 sec/batch; 21h:21m:19s remains)
INFO - root - 2017-12-15 06:12:13.741780: step 2570, loss = 0.27, batch loss = 0.23 (34.7 examples/sec; 0.231 sec/batch; 21h:08m:41s remains)
INFO - root - 2017-12-15 06:12:16.034236: step 2580, loss = 0.21, batch loss = 0.17 (35.4 examples/sec; 0.226 sec/batch; 20h:43m:29s remains)
INFO - root - 2017-12-15 06:12:18.290582: step 2590, loss = 0.23, batch loss = 0.20 (35.9 examples/sec; 0.223 sec/batch; 20h:24m:52s remains)
INFO - root - 2017-12-15 06:12:20.536316: step 2600, loss = 0.27, batch loss = 0.23 (34.9 examples/sec; 0.229 sec/batch; 20h:58m:46s remains)
2017-12-15 06:12:20.831882: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5436493 -1.9619581 -2.2186954 -2.571099 -2.9400008 -3.0523381 -3.21559 -3.8117692 -4.1327367 -3.9527857 -3.7625566 -3.8095946 -4.0808177 -4.5947309 -4.8974848][-1.9042406 -1.2173426 -1.291667 -1.4376695 -1.6381249 -1.6775001 -1.7940178 -2.500437 -3.0145617 -2.9651506 -2.8461697 -2.9742725 -3.2162142 -3.7390504 -3.9171569][-2.05099 -0.5834645 -0.66195846 -0.70110655 -0.70795822 -0.64803207 -0.641616 -1.3178211 -1.9929205 -2.1401331 -2.0713205 -2.2596133 -2.3768332 -2.72409 -2.6624818][-1.9451112 -0.084385037 -0.42001069 -0.53226972 -0.32638502 -0.0554924 0.17849708 -0.28982222 -1.0848441 -1.5437868 -1.5702802 -1.8271209 -1.843637 -1.9135871 -1.4861829][-1.7416419 0.34375238 -0.21243978 -0.35570002 0.15746951 0.78608036 1.309979 1.1107323 0.13953543 -0.84660482 -1.127264 -1.4999323 -1.4909768 -1.2474253 -0.42655241][-1.5786904 0.54508758 0.033940554 0.079972744 1.0298994 2.0978208 2.8840261 2.8849797 1.6460242 -0.024195552 -0.79743016 -1.5500917 -1.8492311 -1.5365955 -0.58439231][-1.8560336 0.25562382 -0.0020341873 0.41060519 1.9261484 3.5454607 4.619904 4.8260055 3.4839673 1.3581235 0.12620544 -1.0767379 -1.9237502 -1.9383566 -1.2286025][-2.8889263 -0.83361328 -0.78812575 0.043483019 2.0424042 4.0597281 5.2452574 5.5122085 4.1952329 1.9256649 0.47870445 -1.0100586 -2.3369894 -2.7616749 -2.3726308][-4.0844064 -2.3442175 -2.2781897 -1.334373 0.77538657 2.8652859 3.9743562 4.2471471 3.1429291 1.1682382 -0.067768574 -1.4326048 -2.8656456 -3.4142468 -3.0969305][-5.1360307 -3.6815925 -3.6674838 -2.7752664 -0.80362642 1.2099183 2.2020397 2.4266348 1.5702479 0.057683229 -0.76345825 -1.73169 -2.9462957 -3.3809597 -2.8919106][-6.1325731 -4.7644491 -4.7046633 -3.8702581 -2.1326718 -0.30076647 0.57982445 0.7707088 0.12017417 -0.99276614 -1.4550217 -2.0556645 -3.0768583 -3.4182096 -2.7921944][-6.4013162 -5.0404387 -4.9462166 -4.1696429 -2.6649795 -1.0432194 -0.18558931 0.046453238 -0.43215346 -1.1854396 -1.348253 -1.7019813 -2.6801684 -3.1177382 -2.6374409][-5.4677906 -4.0656862 -4.0152245 -3.4029698 -2.2479968 -0.96364 -0.18603528 -0.023530483 -0.5714016 -1.1914266 -1.2215896 -1.435339 -2.3527617 -2.9022031 -2.6902795][-3.5624595 -2.0455565 -2.1491241 -1.9511577 -1.4841979 -0.87528193 -0.41057253 -0.4513967 -1.1920112 -1.8158958 -1.7832005 -1.8673364 -2.6135833 -3.153882 -3.2179239][-1.6494521 0.14626741 0.034444571 -0.090040088 -0.32220745 -0.46047139 -0.3918854 -0.67149413 -1.6520381 -2.363621 -2.403091 -2.5159645 -3.1311226 -3.6729851 -4.0565996]]...]
INFO - root - 2017-12-15 06:12:23.070982: step 2610, loss = 0.25, batch loss = 0.21 (35.5 examples/sec; 0.225 sec/batch; 20h:38m:35s remains)
INFO - root - 2017-12-15 06:12:25.382554: step 2620, loss = 0.25, batch loss = 0.21 (33.8 examples/sec; 0.237 sec/batch; 21h:42m:36s remains)
INFO - root - 2017-12-15 06:12:27.655472: step 2630, loss = 0.22, batch loss = 0.18 (35.5 examples/sec; 0.225 sec/batch; 20h:38m:21s remains)
INFO - root - 2017-12-15 06:12:29.918992: step 2640, loss = 0.31, batch loss = 0.27 (35.6 examples/sec; 0.224 sec/batch; 20h:33m:51s remains)
INFO - root - 2017-12-15 06:12:32.218430: step 2650, loss = 0.31, batch loss = 0.27 (35.8 examples/sec; 0.223 sec/batch; 20h:27m:10s remains)
INFO - root - 2017-12-15 06:12:34.489411: step 2660, loss = 0.28, batch loss = 0.25 (34.4 examples/sec; 0.233 sec/batch; 21h:19m:41s remains)
INFO - root - 2017-12-15 06:12:36.752838: step 2670, loss = 0.38, batch loss = 0.34 (34.3 examples/sec; 0.233 sec/batch; 21h:21m:33s remains)
INFO - root - 2017-12-15 06:12:39.032050: step 2680, loss = 0.37, batch loss = 0.33 (36.2 examples/sec; 0.221 sec/batch; 20h:16m:00s remains)
INFO - root - 2017-12-15 06:12:41.294174: step 2690, loss = 0.24, batch loss = 0.20 (35.4 examples/sec; 0.226 sec/batch; 20h:41m:58s remains)
INFO - root - 2017-12-15 06:12:43.596451: step 2700, loss = 0.25, batch loss = 0.21 (35.2 examples/sec; 0.227 sec/batch; 20h:50m:09s remains)
2017-12-15 06:12:43.880047: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9642506 -7.0221672 -7.2557764 -7.56561 -7.7761455 -7.829802 -7.8160238 -7.7748408 -7.7816954 -7.9591064 -8.09687 -8.0463161 -7.9090576 -7.8160591 -7.7616677][-7.1894069 -7.6813955 -8.1636114 -8.6065445 -8.8093338 -8.7969685 -8.7161484 -8.7042732 -8.8195858 -9.1126575 -9.40756 -9.4536839 -9.3199005 -9.1392975 -8.8967438][-8.076849 -7.9945531 -8.4630947 -8.7764225 -8.8146667 -8.6136742 -8.4200039 -8.4431324 -8.6554995 -9.0099211 -9.3589649 -9.5164528 -9.5208378 -9.3876381 -9.0731106][-8.636919 -7.8988256 -8.1472683 -8.2090435 -8.0359545 -7.582386 -7.1558313 -7.121686 -7.43047 -7.8629975 -8.3015165 -8.6405859 -8.8758755 -8.8976421 -8.6772652][-8.5103569 -7.0106049 -6.850451 -6.553061 -6.0488663 -5.1658106 -4.28502 -4.0489631 -4.567081 -5.3389063 -6.0765438 -6.7330279 -7.3172235 -7.638557 -7.6622934][-7.2232909 -5.7977233 -5.2685413 -4.557394 -3.5346911 -1.9964195 -0.45081055 0.078345537 -0.76493752 -2.1521349 -3.4451733 -4.5452652 -5.4348841 -5.9181814 -6.0456209][-5.1384692 -4.148591 -3.2982335 -2.2264769 -0.78209722 1.2559023 3.296618 4.02016 2.8418913 0.84536052 -1.0528204 -2.6647515 -3.883611 -4.471694 -4.5708466][-3.3699644 -2.0509653 -1.0443351 0.092364311 1.5935893 3.7437758 5.9962134 6.8485065 5.5257525 3.1042919 0.75574708 -1.2542727 -2.7880421 -3.5458694 -3.7234159][-1.4638538 -0.46501803 0.13557005 0.86087632 2.1176133 4.1606092 6.5279 7.5660329 6.4348493 4.0075755 1.5524209 -0.59032 -2.2658417 -3.1310377 -3.3987169][-0.6783483 -0.1694963 -0.093713522 0.21636009 1.158483 2.9083657 5.1379638 6.2563939 5.4913268 3.4492249 1.2330925 -0.79194081 -2.4221194 -3.3575592 -3.7404532][-1.7272172 -1.6069185 -1.7886158 -1.6081901 -0.85867071 0.47752476 2.242559 3.2402458 2.8833809 1.4214275 -0.29622567 -1.9700837 -3.3604431 -4.256669 -4.6670847][-4.2730708 -4.3785644 -4.637228 -4.4022837 -3.714499 -2.6869392 -1.4075005 -0.64408517 -0.751367 -1.6048185 -2.6817827 -3.7329268 -4.5841079 -5.2529106 -5.5200968][-6.2049236 -6.23173 -6.4951396 -6.3596582 -5.994451 -5.3909235 -4.6336255 -4.2295709 -4.353745 -4.81228 -5.2538452 -5.5860376 -5.857645 -6.2027411 -6.2954354][-6.4237537 -6.1265292 -6.3733869 -6.5555172 -6.7750459 -6.818996 -6.6903377 -6.6934042 -6.9103723 -7.0354838 -6.878746 -6.6040111 -6.5548739 -6.7590981 -6.8003588][-5.86602 -5.1097593 -5.3306751 -5.8973026 -6.6788592 -7.2485952 -7.5052505 -7.6509495 -7.791285 -7.5491853 -6.9339094 -6.3360763 -6.2229366 -6.499289 -6.6672406]]...]
INFO - root - 2017-12-15 06:12:46.135454: step 2710, loss = 0.47, batch loss = 0.43 (33.7 examples/sec; 0.238 sec/batch; 21h:45m:55s remains)
INFO - root - 2017-12-15 06:12:48.405248: step 2720, loss = 0.38, batch loss = 0.34 (36.5 examples/sec; 0.219 sec/batch; 20h:03m:31s remains)
INFO - root - 2017-12-15 06:12:50.664710: step 2730, loss = 0.18, batch loss = 0.14 (36.5 examples/sec; 0.219 sec/batch; 20h:03m:55s remains)
INFO - root - 2017-12-15 06:12:52.957875: step 2740, loss = 0.32, batch loss = 0.28 (34.6 examples/sec; 0.231 sec/batch; 21h:11m:37s remains)
INFO - root - 2017-12-15 06:12:55.226282: step 2750, loss = 0.25, batch loss = 0.21 (35.2 examples/sec; 0.227 sec/batch; 20h:49m:36s remains)
INFO - root - 2017-12-15 06:12:57.489810: step 2760, loss = 0.28, batch loss = 0.24 (34.5 examples/sec; 0.232 sec/batch; 21h:13m:31s remains)
INFO - root - 2017-12-15 06:12:59.734967: step 2770, loss = 0.22, batch loss = 0.18 (33.9 examples/sec; 0.236 sec/batch; 21h:37m:15s remains)
INFO - root - 2017-12-15 06:13:01.994016: step 2780, loss = 0.24, batch loss = 0.20 (36.4 examples/sec; 0.220 sec/batch; 20h:08m:56s remains)
INFO - root - 2017-12-15 06:13:04.261625: step 2790, loss = 0.38, batch loss = 0.34 (35.5 examples/sec; 0.226 sec/batch; 20h:39m:36s remains)
INFO - root - 2017-12-15 06:13:06.546797: step 2800, loss = 0.23, batch loss = 0.19 (34.6 examples/sec; 0.231 sec/batch; 21h:11m:59s remains)
2017-12-15 06:13:06.850808: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.14063704 -1.5813377 -2.2857785 -2.2427697 -1.1892396 0.82196903 2.2314856 1.5987411 -0.59247649 -2.7338331 -4.3434815 -5.4674845 -5.8012428 -5.2512922 -4.1788654][-0.11856735 -0.77987754 -1.2124035 -1.1555054 -0.2557652 1.5553591 2.9379094 2.3577211 0.2752862 -1.8897432 -3.81523 -5.3695617 -5.9220276 -5.3624911 -4.2025013][-0.9906019 -0.83949172 -0.70410931 -0.35264075 0.51564264 1.9140399 3.043247 2.5403235 0.69125962 -1.4323874 -3.5804462 -5.4271355 -6.0803843 -5.4670448 -4.1782084][-2.2978525 -1.6891254 -0.9358058 0.041573048 1.1430295 2.1320188 2.8980281 2.4368842 0.92231584 -1.1243184 -3.4340186 -5.5068235 -6.2687941 -5.6485977 -4.2442813][-3.3151517 -2.6386747 -1.4949387 0.14823079 1.6751072 2.5683892 3.1702731 2.8155138 1.5743203 -0.59706354 -3.2146702 -5.6350155 -6.5673819 -5.9406977 -4.3831472][-3.4495811 -3.0611188 -1.9132125 0.13156223 2.1350992 3.39346 4.1364822 3.8074663 2.5484436 0.056953192 -3.0157743 -5.8290882 -6.9238195 -6.2673874 -4.5830555][-2.9804134 -3.0280552 -2.1653478 -0.15298975 2.1901948 3.9678972 4.9901171 4.6711159 3.2987602 0.49870539 -2.9613061 -6.0676556 -7.2609968 -6.5649977 -4.8286753][-2.8603148 -2.9434373 -2.4110708 -0.7875247 1.31653 3.1179125 4.2589846 4.2317705 3.2321665 0.6465466 -2.8333526 -6.064189 -7.3638458 -6.72176 -5.0147734][-3.1317105 -2.9383707 -2.5904875 -1.5546198 -0.13044047 1.2089076 2.3487499 2.8441088 2.5866868 0.60116982 -2.5474672 -5.6877055 -7.0891829 -6.5785713 -5.0112066][-3.3150697 -2.7212276 -2.353915 -1.7446239 -0.9307425 -0.060954332 1.1008754 1.933183 2.0531085 0.388633 -2.3713624 -5.2188883 -6.5780611 -6.1469383 -4.7876906][-3.3244956 -2.3351846 -1.7067921 -1.0592916 -0.51387453 -0.011216521 0.8978579 1.4628723 1.3366723 -0.36577284 -2.6780076 -5.0494308 -6.1621618 -5.6957283 -4.4478493][-3.0807221 -1.8555193 -1.0414195 -0.25406432 0.15873075 0.30887437 0.72421288 0.68194127 0.1069777 -1.570431 -3.3548002 -5.1723385 -5.9975166 -5.4344215 -4.1804557][-2.8236694 -1.4117842 -0.66296673 -0.055143237 0.13777328 0.014562607 -0.046222448 -0.66723716 -1.4685584 -2.8763108 -4.1411047 -5.5489206 -6.1454468 -5.4711552 -4.0760536][-2.6691279 -1.087604 -0.4979409 -0.16899002 -0.22716153 -0.61076629 -1.1233951 -2.1580644 -3.0177453 -4.111311 -4.9535623 -6.01003 -6.3985405 -5.6121836 -4.0401363][-2.8110726 -1.1877172 -0.68978524 -0.46789968 -0.56616414 -1.0463953 -1.8762089 -3.1279078 -4.0560718 -4.9601822 -5.5644188 -6.3554525 -6.5580158 -5.6893263 -3.9915686]]...]
INFO - root - 2017-12-15 06:13:09.114838: step 2810, loss = 0.39, batch loss = 0.36 (35.3 examples/sec; 0.227 sec/batch; 20h:46m:49s remains)
INFO - root - 2017-12-15 06:13:11.379163: step 2820, loss = 0.22, batch loss = 0.18 (35.1 examples/sec; 0.228 sec/batch; 20h:51m:58s remains)
INFO - root - 2017-12-15 06:13:13.650085: step 2830, loss = 0.21, batch loss = 0.17 (36.1 examples/sec; 0.222 sec/batch; 20h:19m:17s remains)
INFO - root - 2017-12-15 06:13:15.941710: step 2840, loss = 0.23, batch loss = 0.20 (33.9 examples/sec; 0.236 sec/batch; 21h:36m:50s remains)
INFO - root - 2017-12-15 06:13:18.203808: step 2850, loss = 0.21, batch loss = 0.17 (35.0 examples/sec; 0.229 sec/batch; 20h:55m:58s remains)
INFO - root - 2017-12-15 06:13:20.458591: step 2860, loss = 0.30, batch loss = 0.26 (36.4 examples/sec; 0.220 sec/batch; 20h:06m:40s remains)
INFO - root - 2017-12-15 06:13:22.709775: step 2870, loss = 0.25, batch loss = 0.21 (34.6 examples/sec; 0.231 sec/batch; 21h:09m:37s remains)
INFO - root - 2017-12-15 06:13:24.971924: step 2880, loss = 0.31, batch loss = 0.27 (36.0 examples/sec; 0.222 sec/batch; 20h:20m:07s remains)
INFO - root - 2017-12-15 06:13:27.221326: step 2890, loss = 0.29, batch loss = 0.25 (35.4 examples/sec; 0.226 sec/batch; 20h:40m:27s remains)
INFO - root - 2017-12-15 06:13:29.509453: step 2900, loss = 0.22, batch loss = 0.18 (34.9 examples/sec; 0.230 sec/batch; 21h:00m:56s remains)
2017-12-15 06:13:29.795045: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4308262 -3.4378896 -3.2164309 -3.0359731 -2.8508821 -2.6066904 -2.2725244 -2.1926651 -2.0733421 -1.8740087 -2.1635745 -2.832669 -3.4704356 -4.0148268 -3.849839][-2.8936923 -3.2442236 -2.6945651 -2.5208178 -2.5245323 -2.3036587 -2.132467 -2.3036172 -2.3952591 -2.3862836 -2.7141154 -3.2820435 -3.8806288 -4.5089922 -4.3651066][-3.4068708 -3.1013894 -2.3501468 -2.3082235 -2.4987183 -2.2239373 -2.0115333 -2.2955685 -2.5441489 -2.67851 -3.0176063 -3.4992306 -4.0236378 -4.6833496 -4.6421471][-3.7587383 -3.0395536 -2.2930872 -2.40709 -2.6920745 -2.3577602 -2.0082903 -2.2160237 -2.4642117 -2.558218 -2.7492969 -3.0828996 -3.5883231 -4.3061323 -4.3903618][-4.1142292 -3.3404281 -2.8477068 -3.0927303 -3.3754973 -2.9728203 -2.4368651 -2.435287 -2.5752716 -2.5514472 -2.6309893 -2.8616548 -3.2783861 -3.9444582 -4.0711689][-4.8315763 -4.0981636 -3.8155951 -3.9895823 -4.1158838 -3.6487174 -2.9567108 -2.7951224 -2.8433576 -2.8231039 -2.9472027 -3.1283557 -3.3986011 -3.8717635 -3.9787736][-5.3130703 -4.9137425 -4.7518215 -4.7121487 -4.6123338 -4.1424532 -3.4670014 -3.2566812 -3.2833295 -3.357002 -3.5922072 -3.7494426 -3.8650715 -4.1374931 -4.2062812][-5.7416072 -5.5826406 -5.5066953 -5.2156372 -4.8241081 -4.311379 -3.7522557 -3.6123495 -3.7561646 -4.0310588 -4.3977523 -4.5385532 -4.460906 -4.51018 -4.5888591][-5.611866 -5.71642 -5.7184434 -5.2587605 -4.6432924 -4.0618896 -3.6033919 -3.5577722 -3.8305421 -4.2657881 -4.7000003 -4.8265791 -4.6717644 -4.6493917 -4.8632226][-4.9619288 -5.2327242 -5.2810154 -4.7321854 -4.03616 -3.497175 -3.1823254 -3.2500238 -3.6656179 -4.1716862 -4.5400372 -4.5715446 -4.3501019 -4.3137479 -4.659194][-4.3343163 -4.72987 -4.8383656 -4.2778435 -3.5907125 -3.09542 -2.8404584 -2.8782504 -3.2826467 -3.6616359 -3.8239484 -3.6866853 -3.3765159 -3.340548 -3.8416157][-4.0669961 -4.4232411 -4.4730749 -3.9163384 -3.2561488 -2.6972966 -2.3399384 -2.2496648 -2.5363405 -2.7099371 -2.6294136 -2.2916791 -1.8482161 -1.8293872 -2.511909][-3.922945 -4.0488472 -3.9683578 -3.4381096 -2.7881441 -2.17108 -1.7008071 -1.4907835 -1.6734453 -1.735087 -1.4719464 -0.93312418 -0.34779584 -0.31001914 -1.0537043][-3.9227371 -3.7252545 -3.5124695 -3.051744 -2.4537573 -1.7975092 -1.2587676 -1.0009671 -1.1067406 -1.1747309 -0.83573651 -0.10025036 0.58021259 0.66379523 0.0635438][-3.79489 -3.2765877 -3.004643 -2.718719 -2.2636707 -1.652114 -1.1590756 -0.95268428 -1.02355 -1.244925 -0.98389006 -0.12302232 0.68664384 0.92593241 0.67042518]]...]
INFO - root - 2017-12-15 06:13:32.045192: step 2910, loss = 0.27, batch loss = 0.23 (34.5 examples/sec; 0.232 sec/batch; 21h:14m:15s remains)
INFO - root - 2017-12-15 06:13:34.301527: step 2920, loss = 0.25, batch loss = 0.21 (35.9 examples/sec; 0.223 sec/batch; 20h:24m:58s remains)
INFO - root - 2017-12-15 06:13:36.586862: step 2930, loss = 0.39, batch loss = 0.35 (36.2 examples/sec; 0.221 sec/batch; 20h:14m:07s remains)
INFO - root - 2017-12-15 06:13:38.831048: step 2940, loss = 0.21, batch loss = 0.17 (35.2 examples/sec; 0.227 sec/batch; 20h:49m:24s remains)
INFO - root - 2017-12-15 06:13:41.118796: step 2950, loss = 0.34, batch loss = 0.31 (35.3 examples/sec; 0.226 sec/batch; 20h:43m:34s remains)
INFO - root - 2017-12-15 06:13:43.376410: step 2960, loss = 0.43, batch loss = 0.39 (35.8 examples/sec; 0.223 sec/batch; 20h:26m:43s remains)
INFO - root - 2017-12-15 06:13:45.643028: step 2970, loss = 0.32, batch loss = 0.29 (35.5 examples/sec; 0.225 sec/batch; 20h:36m:26s remains)
INFO - root - 2017-12-15 06:13:47.905947: step 2980, loss = 0.30, batch loss = 0.26 (36.1 examples/sec; 0.221 sec/batch; 20h:15m:50s remains)
INFO - root - 2017-12-15 06:13:50.160057: step 2990, loss = 0.26, batch loss = 0.23 (36.7 examples/sec; 0.218 sec/batch; 19h:56m:56s remains)
INFO - root - 2017-12-15 06:13:52.418111: step 3000, loss = 0.28, batch loss = 0.24 (35.5 examples/sec; 0.225 sec/batch; 20h:35m:58s remains)
2017-12-15 06:13:52.715143: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.69317639 -1.4790381 -2.1402981 -2.26248 -2.6925976 -2.9377022 -2.0582628 -1.4158291 -0.6777581 -0.087418914 0.14268184 -0.24246216 -1.5059874 -2.3126755 -2.6470079][-1.1973023 -1.1223247 -1.8221689 -1.9142122 -2.4301829 -2.735467 -1.9712136 -1.3328457 -0.62674046 -0.036597252 0.022835016 -0.30761135 -1.5179585 -2.3458719 -2.707788][-1.9082949 -0.81711614 -1.6707662 -1.8448107 -2.4570549 -2.9109445 -2.445004 -2.0583494 -1.4993978 -0.85158956 -0.67077565 -0.90856469 -2.0256078 -2.7477627 -3.0258551][-2.2682672 -0.5528729 -1.4902453 -1.8115488 -2.4674859 -2.9903297 -2.9291859 -3.0190408 -2.7961235 -2.1908705 -1.970971 -2.1707962 -3.1213453 -3.6525607 -3.7799368][-2.3823519 -0.2733742 -1.0630462 -1.3781838 -1.9594747 -2.3825221 -2.4738648 -2.9674375 -3.2305284 -2.8767323 -2.80077 -3.0927067 -3.8880086 -4.3874636 -4.5420914][-2.1795406 -0.0978899 -0.54779267 -0.63108361 -0.87646556 -0.82348633 -0.70880008 -1.4758564 -2.2371368 -2.3035603 -2.5814245 -3.1502504 -3.873589 -4.3666372 -4.6067314][-2.3076971 -0.35540485 -0.14027154 0.33848023 0.7686193 1.6330538 2.2615004 1.3372641 -0.0035631657 -0.65980458 -1.3528435 -2.2981062 -3.02965 -3.5256796 -3.8213205][-3.5959792 -1.1782029 -0.13376188 0.99852204 2.1796808 3.8905621 5.2781544 4.4866662 2.692153 1.4399981 0.31067777 -0.99201429 -1.7399788 -2.1960237 -2.5170159][-5.5735745 -3.3294837 -1.7164927 0.0018031597 1.732156 3.9536691 5.9600697 5.6088128 3.7665639 2.3838539 1.340781 0.062255383 -0.62805486 -0.92217934 -1.0216259][-7.7432914 -5.9567471 -4.2290297 -2.4482362 -0.72688746 1.3503101 3.3448172 3.4069653 2.00282 1.220191 0.85121775 0.079046726 -0.23010385 -0.0044755936 0.41140008][-9.5528793 -8.33695 -7.0099216 -5.7156172 -4.5378075 -3.0598223 -1.360638 -0.83052886 -1.5517545 -1.5984707 -1.2451568 -1.3542576 -1.0584213 -0.11544561 0.9947412][-10.97156 -10.275924 -9.5186386 -8.7490711 -8.0960836 -7.1719146 -5.9426413 -5.2218885 -5.3256931 -4.7476034 -3.8330617 -3.2447577 -2.2235188 -0.41038132 1.3570786][-10.756376 -10.355873 -10.169893 -9.8725224 -9.5735874 -9.0523071 -8.3513737 -7.8501573 -7.8021274 -7.0285521 -5.900856 -4.8792424 -3.3739522 -0.9032861 1.3316078][-9.3311682 -9.036335 -9.3060579 -9.3984051 -9.3707581 -9.1495914 -8.8163176 -8.703846 -8.8699808 -8.4479141 -7.6791115 -6.6259189 -4.8921766 -1.9963247 0.50731087][-7.660624 -7.2500982 -7.7207551 -8.0535545 -8.25626 -8.2730312 -8.256959 -8.5785275 -9.1452045 -9.22962 -8.9210167 -8.0616388 -6.4626031 -3.7135677 -1.3164692]]...]
INFO - root - 2017-12-15 06:13:55.008544: step 3010, loss = 0.23, batch loss = 0.19 (34.5 examples/sec; 0.232 sec/batch; 21h:12m:50s remains)
INFO - root - 2017-12-15 06:13:57.303845: step 3020, loss = 0.21, batch loss = 0.18 (35.0 examples/sec; 0.229 sec/batch; 20h:56m:12s remains)
INFO - root - 2017-12-15 06:13:59.558721: step 3030, loss = 0.31, batch loss = 0.27 (35.7 examples/sec; 0.224 sec/batch; 20h:31m:45s remains)
INFO - root - 2017-12-15 06:14:01.820685: step 3040, loss = 0.21, batch loss = 0.17 (36.2 examples/sec; 0.221 sec/batch; 20h:14m:51s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 06:14:04.098644: step 3050, loss = 0.24, batch loss = 0.20 (34.9 examples/sec; 0.229 sec/batch; 20h:56m:55s remains)
INFO - root - 2017-12-15 06:14:06.379357: step 3060, loss = 0.33, batch loss = 0.29 (35.9 examples/sec; 0.223 sec/batch; 20h:23m:06s remains)
INFO - root - 2017-12-15 06:14:08.651299: step 3070, loss = 0.29, batch loss = 0.25 (34.6 examples/sec; 0.231 sec/batch; 21h:09m:51s remains)
INFO - root - 2017-12-15 06:14:10.955549: step 3080, loss = 0.37, batch loss = 0.34 (35.9 examples/sec; 0.223 sec/batch; 20h:23m:39s remains)
INFO - root - 2017-12-15 06:14:13.214737: step 3090, loss = 0.32, batch loss = 0.28 (36.1 examples/sec; 0.221 sec/batch; 20h:15m:46s remains)
INFO - root - 2017-12-15 06:14:15.467888: step 3100, loss = 0.31, batch loss = 0.28 (34.2 examples/sec; 0.234 sec/batch; 21h:24m:49s remains)
2017-12-15 06:14:15.741601: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5830066 -5.1162944 -5.4641895 -5.7419806 -5.9094963 -5.886929 -5.7898703 -5.9068956 -6.25157 -6.6072893 -6.9555821 -6.7216282 -6.0183296 -5.4980516 -5.1029472][-4.6801724 -5.9731231 -6.3785214 -6.600564 -6.623179 -6.3413363 -5.967165 -6.0243244 -6.5283127 -7.1329575 -7.7266154 -7.502492 -6.5664153 -5.76007 -4.9990129][-5.5111971 -6.4048319 -6.7274551 -6.6903734 -6.3291469 -5.5360088 -4.724184 -4.7201242 -5.5669489 -6.6973872 -7.7759972 -7.74998 -6.76902 -5.7733793 -4.6508694][-6.1137543 -6.4898205 -6.5924187 -6.11431 -5.1376548 -3.5675454 -2.1192551 -1.9458791 -3.20674 -4.9876003 -6.569684 -6.8219938 -6.0298185 -5.12634 -3.9616132][-6.1835284 -6.0253429 -5.8459225 -4.8932228 -3.2810154 -0.92244136 1.1467767 1.4612689 -0.2698406 -2.7289889 -4.7423897 -5.2844596 -4.8674459 -4.306675 -3.3383994][-5.8156843 -5.3802547 -4.8927908 -3.5516589 -1.4570692 1.4994781 4.0078773 4.38302 2.2446404 -0.76217723 -3.1019616 -3.9008846 -3.9836333 -3.8765025 -3.2304673][-5.3892455 -4.9287729 -4.2326574 -2.693373 -0.34361005 2.8736815 5.5063887 5.8272505 3.4539089 0.23540258 -2.1283414 -2.9832745 -3.3530233 -3.6140587 -3.37131][-5.3645086 -4.8175831 -4.0965066 -2.5972171 -0.2981 2.7634921 5.2389669 5.4582295 3.143343 0.14948463 -1.8995738 -2.643328 -3.1464486 -3.7161143 -3.9618454][-5.586009 -4.95758 -4.3218408 -3.0306034 -1.0178732 1.6073284 3.6604071 3.7643065 1.8372908 -0.55543685 -2.1034365 -2.73386 -3.3148367 -4.0039654 -4.5267982][-5.9930096 -5.3146219 -4.8222361 -3.8063264 -2.2048814 -0.15205467 1.3968074 1.4196885 -0.00915575 -1.7210228 -2.7296937 -3.2044544 -3.6336193 -4.1752772 -4.7554064][-6.4221239 -5.6499166 -5.3079891 -4.5688767 -3.4174867 -1.9523448 -0.8598814 -0.90268815 -1.8968188 -2.9892795 -3.516573 -3.7603524 -3.9625549 -4.2972555 -4.8806696][-6.5834002 -5.7691236 -5.5960417 -5.0970411 -4.3004313 -3.2938597 -2.596283 -2.7118738 -3.3705969 -4.0284405 -4.2894058 -4.3778939 -4.3841333 -4.5338378 -4.9706459][-6.3226671 -5.521101 -5.5120778 -5.2259803 -4.7142887 -4.0889416 -3.6946464 -3.8291469 -4.2727456 -4.6909471 -4.8900971 -4.9452925 -4.80124 -4.7269416 -4.8478775][-5.7639961 -4.9928913 -5.1428423 -5.0740366 -4.838974 -4.55166 -4.3704925 -4.4356632 -4.6901407 -4.96535 -5.1613874 -5.2019796 -5.0338974 -4.7721014 -4.5382471][-5.0088148 -4.2477694 -4.5519986 -4.7337127 -4.7734327 -4.7296534 -4.6989164 -4.7472095 -4.8798265 -5.0229754 -5.1115627 -5.0835667 -4.9205008 -4.5655222 -4.1495662]]...]
INFO - root - 2017-12-15 06:14:17.996839: step 3110, loss = 0.23, batch loss = 0.19 (35.6 examples/sec; 0.224 sec/batch; 20h:32m:24s remains)
INFO - root - 2017-12-15 06:14:20.229945: step 3120, loss = 0.31, batch loss = 0.27 (34.9 examples/sec; 0.229 sec/batch; 20h:57m:54s remains)
INFO - root - 2017-12-15 06:14:22.493735: step 3130, loss = 0.44, batch loss = 0.40 (35.7 examples/sec; 0.224 sec/batch; 20h:28m:59s remains)
INFO - root - 2017-12-15 06:14:24.788504: step 3140, loss = 0.35, batch loss = 0.31 (36.1 examples/sec; 0.222 sec/batch; 20h:15m:56s remains)
INFO - root - 2017-12-15 06:14:27.046640: step 3150, loss = 0.33, batch loss = 0.29 (33.3 examples/sec; 0.241 sec/batch; 22h:00m:19s remains)
INFO - root - 2017-12-15 06:14:29.320888: step 3160, loss = 0.22, batch loss = 0.18 (36.3 examples/sec; 0.221 sec/batch; 20h:10m:32s remains)
INFO - root - 2017-12-15 06:14:31.582635: step 3170, loss = 0.31, batch loss = 0.27 (35.3 examples/sec; 0.227 sec/batch; 20h:44m:56s remains)
INFO - root - 2017-12-15 06:14:33.871649: step 3180, loss = 0.47, batch loss = 0.43 (35.4 examples/sec; 0.226 sec/batch; 20h:40m:16s remains)
INFO - root - 2017-12-15 06:14:36.133829: step 3190, loss = 0.36, batch loss = 0.32 (34.8 examples/sec; 0.230 sec/batch; 21h:01m:48s remains)
INFO - root - 2017-12-15 06:14:38.401268: step 3200, loss = 0.28, batch loss = 0.24 (35.0 examples/sec; 0.229 sec/batch; 20h:55m:40s remains)
2017-12-15 06:14:38.670984: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4478517 -6.4909668 -6.5407562 -6.5247765 -6.5858173 -6.6861362 -6.6154613 -6.5930338 -6.634758 -6.739007 -6.9960804 -7.2409372 -7.1984348 -6.7636595 -5.9591866][-6.745966 -7.3040695 -7.2031441 -6.9608469 -6.7305861 -6.6361818 -6.596488 -6.806097 -7.0546284 -7.5335608 -8.33018 -9.0548372 -9.1463871 -8.6014338 -7.3487453][-8.5553255 -7.8446255 -7.6926069 -7.2597427 -6.5525417 -5.9785929 -5.7896881 -5.9999075 -6.1821156 -6.8514175 -8.2359838 -9.6392517 -10.078569 -9.6611748 -8.0979][-9.77051 -8.0976419 -7.9404821 -7.34085 -6.1439075 -4.9635882 -4.5316195 -4.709775 -4.8636007 -5.6820784 -7.604661 -9.6341228 -10.47398 -10.338524 -8.6412592][-9.7079391 -7.0945482 -6.7690611 -5.9688683 -4.3810358 -2.6192021 -1.8609741 -2.0107954 -2.1741016 -2.9421971 -5.2596645 -7.8893194 -9.3661375 -9.8733282 -8.50719][-8.6334171 -5.693224 -5.0937886 -4.0449276 -2.1886916 -0.060147524 0.95094633 0.80597019 0.57202792 -0.0448519 -2.5397835 -5.7156057 -8.0085869 -9.30766 -8.4799013][-8.1064091 -5.2500935 -4.4575806 -3.0887315 -0.94412971 1.549294 2.9904716 2.9252746 2.560714 2.0242379 -0.51888967 -4.1335149 -7.226697 -9.2702684 -8.9711609][-7.924161 -5.0473404 -4.0418148 -2.2635324 0.10577965 2.7335722 4.4933786 4.4839745 3.9657853 3.5439837 1.1340303 -2.7562757 -6.5498953 -9.2912216 -9.4824085][-8.0464888 -5.4212704 -4.4207664 -2.4479444 -0.095541358 2.3770301 4.2783442 4.4118414 3.7664034 3.324219 1.1661611 -2.6840549 -6.7819872 -9.7917538 -10.10714][-8.4198427 -6.2113352 -5.4334826 -3.5459242 -1.4306214 0.68781352 2.5972159 2.9179294 2.1766961 1.5813212 -0.30786669 -3.7707458 -7.6081762 -10.297532 -10.382492][-8.4137 -6.6374979 -6.1230288 -4.5141487 -2.8054016 -1.2932713 0.22836041 0.47978997 -0.40195286 -1.1245925 -2.7094865 -5.5588751 -8.6799231 -10.539912 -10.143989][-8.2521086 -6.9424877 -6.8012476 -5.6394153 -4.4130421 -3.5619068 -2.6138015 -2.5059934 -3.3130465 -4.0059338 -5.2109184 -7.3059254 -9.4845057 -10.4126 -9.6107187][-7.8312273 -6.7982578 -6.8678284 -6.1333761 -5.36055 -5.0214424 -4.5790319 -4.5821009 -5.1365967 -5.6625605 -6.495903 -7.8789062 -9.1521082 -9.3679276 -8.4653168][-6.721303 -5.7529888 -5.8741908 -5.4913392 -5.1026487 -5.065413 -4.9814878 -5.1002436 -5.4544525 -5.7982087 -6.3403635 -7.2027388 -7.8590431 -7.7484789 -6.9941654][-5.857255 -4.881484 -4.9776797 -4.8440847 -4.7391911 -4.8552046 -4.9567513 -5.0888453 -5.2576365 -5.4447374 -5.7779484 -6.2656994 -6.5416765 -6.3633175 -5.8448372]]...]
INFO - root - 2017-12-15 06:14:40.937072: step 3210, loss = 0.31, batch loss = 0.27 (35.8 examples/sec; 0.224 sec/batch; 20h:27m:47s remains)
INFO - root - 2017-12-15 06:14:43.195996: step 3220, loss = 0.22, batch loss = 0.18 (35.5 examples/sec; 0.225 sec/batch; 20h:36m:26s remains)
INFO - root - 2017-12-15 06:14:45.457119: step 3230, loss = 0.32, batch loss = 0.28 (35.6 examples/sec; 0.224 sec/batch; 20h:31m:44s remains)
INFO - root - 2017-12-15 06:14:47.706974: step 3240, loss = 0.29, batch loss = 0.25 (35.5 examples/sec; 0.225 sec/batch; 20h:35m:40s remains)
INFO - root - 2017-12-15 06:14:50.006998: step 3250, loss = 0.28, batch loss = 0.24 (34.4 examples/sec; 0.232 sec/batch; 21h:15m:43s remains)
INFO - root - 2017-12-15 06:14:52.257744: step 3260, loss = 0.33, batch loss = 0.29 (33.9 examples/sec; 0.236 sec/batch; 21h:35m:07s remains)
INFO - root - 2017-12-15 06:14:54.549151: step 3270, loss = 0.38, batch loss = 0.34 (35.7 examples/sec; 0.224 sec/batch; 20h:31m:16s remains)
INFO - root - 2017-12-15 06:14:56.825929: step 3280, loss = 0.21, batch loss = 0.17 (34.4 examples/sec; 0.233 sec/batch; 21h:17m:44s remains)
INFO - root - 2017-12-15 06:14:59.084498: step 3290, loss = 0.25, batch loss = 0.21 (36.1 examples/sec; 0.222 sec/batch; 20h:16m:25s remains)
INFO - root - 2017-12-15 06:15:01.344506: step 3300, loss = 0.28, batch loss = 0.24 (35.8 examples/sec; 0.223 sec/batch; 20h:26m:12s remains)
2017-12-15 06:15:01.622322: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7365274 -5.3440843 -6.267951 -7.104269 -7.4814034 -7.1147947 -6.1587534 -4.8296223 -3.6688013 -3.0049498 -2.8890495 -3.0542862 -3.5803914 -4.3406506 -4.3518219][-3.9875228 -5.1337743 -6.0584221 -6.7803545 -7.0846043 -6.8707247 -6.2024784 -5.3115807 -4.6173878 -4.2615519 -4.2136693 -4.2559452 -4.4810791 -4.7883892 -4.3772945][-3.4181404 -4.4017048 -5.3389788 -6.0047455 -6.1680574 -5.9221439 -5.4699278 -5.0769157 -5.0190573 -5.1737151 -5.2973113 -5.2169189 -4.9851384 -4.5983315 -3.6916614][-2.8777246 -3.3558397 -4.081183 -4.4114714 -4.1813588 -3.6268849 -3.1550422 -3.1663589 -3.8022065 -4.6146235 -5.1320066 -5.1980667 -4.77802 -3.9565678 -2.833817][-2.6889343 -2.4571204 -2.7321193 -2.4902203 -1.7075953 -0.63056159 0.16580057 0.023202419 -1.1801367 -2.751673 -4.0179911 -4.7733488 -4.7906551 -4.0748072 -2.9923592][-2.4646912 -1.7782407 -1.7187048 -1.0771122 0.18989182 1.8478363 3.1345856 3.0829089 1.5424883 -0.57419145 -2.4036486 -3.7421479 -4.2694187 -3.8831816 -3.0127025][-2.1688094 -1.4386829 -1.1464586 -0.28646743 1.27299 3.3438241 5.0012808 5.0581884 3.3617303 1.0409007 -0.94679868 -2.4584327 -3.2540095 -3.2202837 -2.6991708][-2.9482877 -2.0802691 -1.6489258 -0.66526365 1.0453851 3.3599389 5.2388306 5.4190674 3.7579381 1.4968362 -0.43511915 -1.9735187 -2.9694653 -3.2987533 -3.1149745][-4.0886612 -3.0692725 -2.5508089 -1.6289806 -0.0567981 2.1538532 3.9490888 4.1186724 2.596873 0.624351 -0.98412585 -2.2410719 -3.1470008 -3.6135101 -3.5857453][-4.9188108 -3.7820709 -3.217133 -2.473711 -1.2431124 0.56184459 1.9511511 1.9081516 0.56869578 -0.94127083 -2.01995 -2.7776375 -3.4540508 -3.9725189 -4.0860415][-5.165184 -4.1548228 -3.8288665 -3.4932346 -2.7991512 -1.5553968 -0.62145257 -0.78186679 -1.7511491 -2.6208417 -3.1058602 -3.3635187 -3.7946482 -4.3484364 -4.6184959][-4.8113046 -4.0982533 -4.11494 -4.2700567 -4.2120781 -3.5674062 -3.0578589 -3.2126017 -3.7086515 -3.8949547 -3.7713375 -3.590519 -3.7186656 -4.1719675 -4.5265713][-3.8140743 -3.2054629 -3.3489881 -3.8380241 -4.3703356 -4.3289208 -4.2750144 -4.4670286 -4.5634995 -4.178319 -3.5394163 -2.9681504 -2.7360969 -3.0781183 -3.5201774][-2.8019812 -2.281101 -2.516258 -3.2520356 -4.20812 -4.6032639 -4.840704 -5.0153289 -4.799129 -4.039638 -3.1470821 -2.3674123 -1.9198811 -2.1918178 -2.6793494][-2.1715281 -1.7377592 -2.0612798 -2.88616 -3.9879508 -4.5443134 -4.8095927 -4.8683982 -4.4638839 -3.587862 -2.7128706 -1.9895313 -1.5991879 -1.9341105 -2.4395211]]...]
INFO - root - 2017-12-15 06:15:03.853537: step 3310, loss = 0.29, batch loss = 0.25 (36.1 examples/sec; 0.222 sec/batch; 20h:16m:45s remains)
INFO - root - 2017-12-15 06:15:06.126708: step 3320, loss = 0.41, batch loss = 0.37 (35.5 examples/sec; 0.225 sec/batch; 20h:35m:47s remains)
INFO - root - 2017-12-15 06:15:08.461224: step 3330, loss = 0.29, batch loss = 0.25 (31.7 examples/sec; 0.253 sec/batch; 23h:06m:12s remains)
INFO - root - 2017-12-15 06:15:10.776266: step 3340, loss = 0.28, batch loss = 0.24 (34.3 examples/sec; 0.233 sec/batch; 21h:19m:49s remains)
INFO - root - 2017-12-15 06:15:13.084102: step 3350, loss = 0.33, batch loss = 0.29 (35.0 examples/sec; 0.229 sec/batch; 20h:55m:35s remains)
INFO - root - 2017-12-15 06:15:15.364587: step 3360, loss = 0.19, batch loss = 0.15 (36.0 examples/sec; 0.222 sec/batch; 20h:19m:49s remains)
INFO - root - 2017-12-15 06:15:17.628287: step 3370, loss = 0.37, batch loss = 0.33 (35.0 examples/sec; 0.229 sec/batch; 20h:53m:37s remains)
INFO - root - 2017-12-15 06:15:19.906350: step 3380, loss = 0.22, batch loss = 0.19 (35.5 examples/sec; 0.225 sec/batch; 20h:35m:21s remains)
INFO - root - 2017-12-15 06:15:22.184595: step 3390, loss = 0.28, batch loss = 0.24 (34.7 examples/sec; 0.231 sec/batch; 21h:05m:57s remains)
INFO - root - 2017-12-15 06:15:24.452149: step 3400, loss = 0.19, batch loss = 0.16 (35.0 examples/sec; 0.229 sec/batch; 20h:53m:53s remains)
2017-12-15 06:15:24.711172: I tensorflow/core/kernels/logging_ops.cc:79] [[[2.4063227 1.6884768 0.68723536 -0.61228442 -1.2259318 -1.5186923 -2.1445549 -2.449769 -2.5980375 -2.7192731 -2.7054958 -2.3446267 -1.4813561 -0.31570435 0.34052515][2.7375219 1.8906283 0.44993424 -1.0462191 -1.8064145 -2.1360753 -2.6788585 -2.8445895 -2.9201288 -2.9257464 -2.857259 -2.53824 -1.627547 -0.22947228 0.77786422][1.6633773 0.7333436 -0.91144931 -2.2221985 -2.8648968 -3.126519 -3.4192381 -3.3498075 -3.2718697 -3.1808825 -3.1424513 -2.9506302 -2.1132672 -0.68182766 0.51487279][-0.3519541 -1.1212323 -2.7001061 -3.5646484 -3.7987654 -3.6961188 -3.5000911 -3.0301189 -2.7963834 -2.7426946 -2.926981 -2.9456332 -2.3641815 -1.2455683 -0.24514544][-1.9848193 -2.3511734 -3.7037494 -4.0646038 -3.6682386 -2.9303873 -2.07164 -1.1538687 -0.81996906 -1.0076694 -1.6985894 -2.1347528 -2.0429659 -1.5671362 -1.0637946][-2.7523029 -2.7446594 -3.6288381 -3.4948564 -2.5279322 -1.1742411 0.28747582 1.5345447 1.8956721 1.4376857 0.26147223 -0.77092612 -1.4769852 -1.9397273 -2.1252611][-2.865766 -2.4403615 -2.8165071 -2.4278522 -1.2282546 0.51840806 2.3877084 3.822305 4.1065073 3.3408506 1.6103508 -0.20278287 -1.7696805 -2.9550605 -3.5549679][-3.0229361 -2.0854156 -2.1890841 -1.8925276 -0.71626759 1.3701599 3.6141031 5.098053 5.0875082 3.7675102 1.3580844 -1.1736053 -3.1882734 -4.4679637 -4.9158082][-3.8841467 -2.6258569 -2.5690539 -2.4305983 -1.4678633 0.65983272 2.9208338 4.1833448 3.8016856 2.0551975 -0.58745873 -3.1095414 -4.7321577 -5.3826418 -5.278883][-4.9781733 -3.509059 -3.4063289 -3.4607265 -2.929306 -1.3003724 0.42690992 1.2708178 0.70041776 -0.96965683 -3.2268949 -5.0007215 -5.49335 -4.8865004 -4.01161][-5.2424393 -3.6807556 -3.6430631 -3.9694965 -4.0359478 -3.1603813 -2.1056292 -1.6043329 -2.1477771 -3.4881742 -5.174159 -5.9939947 -5.0970573 -3.0560763 -1.4015033][-4.3107939 -2.8560069 -3.0209155 -3.5925355 -4.1526957 -3.9270046 -3.3484645 -2.9815707 -3.3883903 -4.4495649 -5.6711526 -5.6770334 -3.7312932 -0.70966315 1.3368354][-3.179729 -1.7708071 -1.9500874 -2.5488861 -3.3781581 -3.6338496 -3.4209602 -3.0191152 -3.0996027 -3.8313665 -4.8301277 -4.627656 -2.4720163 0.74789047 2.7818305][-2.4739211 -1.0635755 -1.1477408 -1.6573199 -2.5168204 -2.924449 -2.7686856 -2.1705458 -1.8488795 -2.3680751 -3.5037127 -3.729701 -2.0166192 0.95767212 2.9049056][-2.664407 -1.3787382 -1.3402965 -1.6495708 -2.2571838 -2.4772227 -2.2003496 -1.5218745 -1.0541614 -1.6283071 -3.0441277 -3.7570157 -2.5619712 0.0078656673 1.8917811]]...]
INFO - root - 2017-12-15 06:15:26.943086: step 3410, loss = 0.21, batch loss = 0.17 (35.5 examples/sec; 0.225 sec/batch; 20h:35m:40s remains)
INFO - root - 2017-12-15 06:15:29.200034: step 3420, loss = 0.30, batch loss = 0.26 (35.7 examples/sec; 0.224 sec/batch; 20h:29m:25s remains)
INFO - root - 2017-12-15 06:15:31.471858: step 3430, loss = 0.26, batch loss = 0.22 (35.3 examples/sec; 0.227 sec/batch; 20h:44m:27s remains)
INFO - root - 2017-12-15 06:15:33.745695: step 3440, loss = 0.27, batch loss = 0.23 (34.2 examples/sec; 0.234 sec/batch; 21h:23m:05s remains)
INFO - root - 2017-12-15 06:15:36.001945: step 3450, loss = 0.43, batch loss = 0.39 (35.9 examples/sec; 0.223 sec/batch; 20h:22m:19s remains)
INFO - root - 2017-12-15 06:15:38.277424: step 3460, loss = 0.23, batch loss = 0.20 (35.4 examples/sec; 0.226 sec/batch; 20h:40m:42s remains)
INFO - root - 2017-12-15 06:15:40.552692: step 3470, loss = 0.23, batch loss = 0.19 (35.4 examples/sec; 0.226 sec/batch; 20h:38m:46s remains)
INFO - root - 2017-12-15 06:15:42.799874: step 3480, loss = 0.39, batch loss = 0.36 (36.2 examples/sec; 0.221 sec/batch; 20h:12m:13s remains)
INFO - root - 2017-12-15 06:15:45.072523: step 3490, loss = 0.23, batch loss = 0.20 (34.2 examples/sec; 0.234 sec/batch; 21h:23m:59s remains)
INFO - root - 2017-12-15 06:15:47.364261: step 3500, loss = 0.21, batch loss = 0.17 (34.8 examples/sec; 0.230 sec/batch; 20h:59m:22s remains)
2017-12-15 06:15:47.667413: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3719242 -1.7315443 -1.7888033 -1.9219663 -2.1319163 -2.2426152 -2.1024501 -1.6109563 -1.4084765 -1.4430687 -1.5386187 -1.2444446 -0.94688952 -0.73796058 -0.46105433][-1.0672879 -1.2346079 -1.8002514 -2.3805807 -2.8161619 -2.9082763 -2.7404475 -2.3088949 -2.2463391 -2.276242 -2.4272633 -2.1766586 -1.6512969 -1.1424177 -0.66133475][-1.0984013 -0.84014785 -1.8013145 -2.73051 -3.3444624 -3.4755971 -3.2492728 -2.8680437 -2.9219921 -2.9309404 -3.1179724 -2.8821969 -2.1207254 -1.3012214 -0.58120382][-0.72054744 -0.23763132 -1.3691411 -2.4999926 -3.3294253 -3.5867066 -3.3594766 -3.0635276 -3.2118921 -3.2043421 -3.3762493 -3.2184319 -2.316473 -1.2601075 -0.25851703][-0.1969341 0.56865096 -0.50104213 -1.624774 -2.6240129 -2.9215755 -2.6532924 -2.282325 -2.4312027 -2.4753816 -2.6346738 -2.6269822 -1.8634539 -0.85522711 0.21616101][-0.4369967 0.57725835 -0.23732471 -1.0783217 -1.9849918 -2.0263145 -1.391027 -0.66067016 -0.61079276 -0.67813456 -0.8495636 -1.0937525 -0.81204617 -0.2968961 0.4210856][-0.50960541 0.38652992 -0.20292962 -0.79199648 -1.6301098 -1.3232887 -0.30401254 0.70945716 0.84796691 0.677624 0.48726368 0.087613344 -0.065021515 0.059908152 0.54094434][-1.0322438 0.26762271 -0.15843427 -0.54056609 -1.3714466 -0.87255228 0.38237166 1.5219214 1.6516745 1.3561432 1.1596999 0.79072261 0.42131186 0.31686831 0.58327842][-2.2837548 -0.72100925 -0.97047997 -1.1077989 -1.8801734 -1.3076044 0.10190034 1.2579782 1.3618219 0.95024061 0.6539278 0.28499246 -0.16644454 -0.38426828 -0.32148314][-3.4676268 -1.7835785 -1.9722791 -2.11016 -2.9218292 -2.5689445 -1.3581734 -0.44231141 -0.43494105 -0.89491677 -1.1955258 -1.4789129 -1.8214929 -1.8705837 -1.7508972][-4.3709259 -2.5742111 -2.7342634 -2.9258232 -3.7367835 -3.6557326 -2.8201432 -2.18124 -2.23963 -2.7113907 -2.9845436 -3.1830015 -3.4094183 -3.3060024 -3.1027386][-4.4557705 -2.6061831 -2.7585514 -2.955584 -3.6227279 -3.7422519 -3.2747796 -2.8581433 -2.9568119 -3.4730382 -3.8484924 -4.1842365 -4.5656319 -4.5916042 -4.4734368][-4.2253695 -2.44832 -2.576014 -2.8251953 -3.3811126 -3.6288755 -3.48112 -3.2114854 -3.3512678 -3.9884338 -4.5264773 -5.0228081 -5.5873203 -5.7600222 -5.6983519][-4.5056334 -2.7241375 -2.7831678 -3.0088632 -3.5159452 -3.8538134 -3.880172 -3.6245041 -3.7409375 -4.3775988 -4.9377165 -5.4409251 -6.0467334 -6.2963715 -6.2274661][-5.3620744 -3.5819449 -3.4940212 -3.5840557 -4.0467844 -4.4605207 -4.5548525 -4.3060541 -4.3804951 -4.904696 -5.3411055 -5.7358 -6.2957964 -6.5846114 -6.5815268]]...]
INFO - root - 2017-12-15 06:15:49.915786: step 3510, loss = 0.31, batch loss = 0.27 (35.0 examples/sec; 0.228 sec/batch; 20h:52m:03s remains)
INFO - root - 2017-12-15 06:15:52.178755: step 3520, loss = 0.23, batch loss = 0.19 (35.3 examples/sec; 0.227 sec/batch; 20h:42m:47s remains)
INFO - root - 2017-12-15 06:15:54.483976: step 3530, loss = 0.25, batch loss = 0.21 (34.6 examples/sec; 0.231 sec/batch; 21h:05m:55s remains)
INFO - root - 2017-12-15 06:15:56.747569: step 3540, loss = 0.33, batch loss = 0.29 (33.9 examples/sec; 0.236 sec/batch; 21h:34m:40s remains)
INFO - root - 2017-12-15 06:15:59.006145: step 3550, loss = 0.24, batch loss = 0.20 (35.7 examples/sec; 0.224 sec/batch; 20h:28m:35s remains)
INFO - root - 2017-12-15 06:16:01.245960: step 3560, loss = 0.34, batch loss = 0.30 (35.9 examples/sec; 0.223 sec/batch; 20h:22m:39s remains)
INFO - root - 2017-12-15 06:16:03.531529: step 3570, loss = 0.23, batch loss = 0.19 (35.3 examples/sec; 0.226 sec/batch; 20h:41m:02s remains)
INFO - root - 2017-12-15 06:16:05.796817: step 3580, loss = 0.20, batch loss = 0.16 (35.5 examples/sec; 0.226 sec/batch; 20h:36m:35s remains)
INFO - root - 2017-12-15 06:16:08.058886: step 3590, loss = 0.39, batch loss = 0.35 (32.0 examples/sec; 0.250 sec/batch; 22h:49m:30s remains)
INFO - root - 2017-12-15 06:16:10.344169: step 3600, loss = 0.34, batch loss = 0.30 (34.9 examples/sec; 0.229 sec/batch; 20h:55m:36s remains)
2017-12-15 06:16:10.624149: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8823662 -4.7835298 -4.7882447 -4.4629622 -3.8502841 -2.9391108 -1.5849491 -0.13695216 0.64766049 0.23889899 -1.1138351 -2.5396733 -3.7919946 -5.0650434 -5.6300206][-5.5343161 -5.7993093 -5.6633534 -5.4303007 -4.7863126 -4.0985065 -3.2529271 -2.011481 -1.1962355 -1.4713515 -2.4961891 -3.538795 -4.4820023 -5.5069127 -6.1346035][-5.7889757 -5.7219753 -5.5937738 -5.6267118 -5.0082703 -4.38741 -3.8674641 -2.9415915 -2.4515402 -2.8745472 -3.6887302 -4.2747707 -4.5899577 -5.1801691 -5.7861366][-5.3526497 -4.9921141 -4.8792992 -5.186017 -4.6083307 -3.9404318 -3.375423 -2.6438708 -2.5629895 -3.1750145 -3.8658371 -4.1061373 -3.899339 -4.2007284 -4.8486733][-4.4178495 -3.5967307 -3.314934 -3.6217942 -2.8271921 -1.7779564 -0.78677285 -0.052713275 -0.36379194 -1.2856915 -2.093291 -2.2406502 -1.8757668 -2.2593789 -3.2262769][-3.7279344 -2.455442 -1.8593605 -1.8741018 -0.6129837 1.0769536 2.7076027 3.5866983 3.0045259 1.6861441 0.50188565 0.17686105 0.44765782 -0.19416034 -1.4551721][-3.7487564 -2.1044424 -1.1350455 -0.70369315 1.0506525 3.2661202 5.4234743 6.4614372 5.694396 3.9269931 2.2830966 1.8844616 2.223932 1.5118454 0.27688408][-3.8473191 -2.2412713 -1.3225248 -0.83419919 0.92535758 3.1948082 5.52709 6.7348089 6.0128832 3.9592459 1.9544737 1.744127 2.4414461 1.929148 1.0862017][-4.2924433 -3.1081147 -2.6514089 -2.5456653 -1.2990785 0.52678037 2.7008374 4.1042614 3.7646453 1.8436141 -0.067095637 0.24150705 1.466126 1.2353723 0.83396339][-4.71415 -4.0401444 -4.1607485 -4.4762731 -3.7874835 -2.5652766 -0.77496338 0.60566235 0.57635188 -1.0127008 -2.5473289 -1.6794124 -0.15356755 -0.27709663 -0.40471232][-4.6731925 -4.5844584 -5.3375416 -5.9995766 -5.6718965 -4.8397818 -3.4169917 -2.2050967 -2.1202772 -3.4190669 -4.6038828 -3.3999841 -1.9858419 -2.2759466 -2.3710177][-4.747529 -5.0756025 -6.2453327 -7.0622549 -6.869319 -6.1875544 -5.005549 -3.9337859 -3.7665157 -4.7932825 -5.6820636 -4.3899269 -3.3112173 -3.9550529 -4.2712069][-4.7344708 -5.0306535 -6.2048874 -6.99903 -6.91278 -6.3942308 -5.5308285 -4.6592512 -4.45192 -5.2741008 -5.9882555 -4.8564196 -4.1773806 -5.1258049 -5.7211885][-4.4440718 -4.4024363 -5.3039618 -6.0241985 -6.0943041 -5.7178841 -5.0664349 -4.3212366 -4.0331326 -4.6564426 -5.33362 -4.6066122 -4.3379025 -5.49034 -6.29583][-4.1190081 -3.7007871 -4.2244411 -4.8513703 -5.0586042 -4.6895809 -4.0664463 -3.3048227 -2.7651381 -3.0996208 -3.8422308 -3.6665149 -3.7659659 -5.0069818 -6.0350876]]...]
INFO - root - 2017-12-15 06:16:12.849131: step 3610, loss = 0.39, batch loss = 0.36 (35.9 examples/sec; 0.223 sec/batch; 20h:22m:37s remains)
INFO - root - 2017-12-15 06:16:15.185997: step 3620, loss = 0.22, batch loss = 0.18 (34.5 examples/sec; 0.232 sec/batch; 21h:12m:21s remains)
INFO - root - 2017-12-15 06:16:17.475874: step 3630, loss = 0.27, batch loss = 0.23 (34.6 examples/sec; 0.231 sec/batch; 21h:07m:42s remains)
INFO - root - 2017-12-15 06:16:19.718890: step 3640, loss = 0.26, batch loss = 0.22 (35.1 examples/sec; 0.228 sec/batch; 20h:49m:43s remains)
INFO - root - 2017-12-15 06:16:22.010263: step 3650, loss = 0.35, batch loss = 0.31 (35.4 examples/sec; 0.226 sec/batch; 20h:36m:54s remains)
INFO - root - 2017-12-15 06:16:24.309074: step 3660, loss = 0.36, batch loss = 0.32 (35.4 examples/sec; 0.226 sec/batch; 20h:37m:15s remains)
INFO - root - 2017-12-15 06:16:26.587105: step 3670, loss = 0.31, batch loss = 0.27 (35.5 examples/sec; 0.225 sec/batch; 20h:34m:31s remains)
INFO - root - 2017-12-15 06:16:28.891294: step 3680, loss = 0.24, batch loss = 0.20 (35.6 examples/sec; 0.225 sec/batch; 20h:31m:58s remains)
INFO - root - 2017-12-15 06:16:31.180033: step 3690, loss = 0.28, batch loss = 0.24 (35.1 examples/sec; 0.228 sec/batch; 20h:48m:36s remains)
INFO - root - 2017-12-15 06:16:33.453087: step 3700, loss = 0.23, batch loss = 0.19 (35.2 examples/sec; 0.227 sec/batch; 20h:45m:22s remains)
2017-12-15 06:16:33.729204: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6368327 -3.4958968 -4.0482063 -4.528573 -4.5678616 -4.4211044 -4.2356653 -3.7395897 -2.912601 -2.3948588 -2.3243608 -2.5691111 -2.9534593 -3.1911817 -3.0205665][-2.0694828 -2.7990174 -3.1411333 -3.3918142 -3.4026704 -3.4522991 -3.1637087 -2.3718102 -1.3955916 -1.0318769 -0.9508363 -0.99391258 -1.4402776 -1.9009635 -1.8764486][-2.0473838 -1.7920541 -1.978999 -2.0853016 -2.092355 -2.371242 -2.1012676 -1.1639702 -0.24543047 -0.1758101 -0.2214905 -0.2006048 -0.67881143 -1.1190984 -1.0147252][-1.8901961 -0.87683082 -0.93589842 -0.94198155 -0.95752192 -1.3894746 -1.2067275 -0.32125723 0.25310445 -0.089805245 -0.3116802 -0.22847521 -0.52629817 -0.70431745 -0.37482178][-1.5063835 -0.15008152 -0.11462009 -0.007278204 0.01720047 -0.32655621 -0.18971217 0.42187762 0.41299057 -0.39641631 -0.90984285 -0.83325529 -0.86417413 -0.72545612 -0.20637774][-1.1105423 0.1413753 0.43300605 0.92569661 1.3874063 1.6057615 1.9056528 2.0804448 1.3719661 0.099071026 -0.85782278 -1.0830262 -1.120681 -0.81338692 -0.22473264][-0.98424292 0.059347153 0.78813338 1.8445115 2.9022374 3.8632464 4.4563451 4.2458959 2.9027715 1.176218 -0.3007586 -1.0529034 -1.3737564 -1.180336 -0.67875111][-0.8678782 0.27481914 1.4191604 2.8744087 4.283864 5.6909513 6.4991159 6.0636272 4.39476 2.4494419 0.64080453 -0.56321609 -1.3030965 -1.5133145 -1.241257][-1.380267 -0.21785772 0.99414015 2.4675779 3.9785981 5.5030394 6.4283266 6.1695871 4.8263392 3.125227 1.3685577 0.053039789 -0.94002771 -1.5095139 -1.5454497][-3.4168203 -2.416774 -1.4829415 -0.27732527 1.0747182 2.4878764 3.5138392 3.7050462 3.0459533 1.882462 0.5322473 -0.53317678 -1.3899155 -1.9907335 -2.199172][-5.5003595 -4.7136259 -4.2133017 -3.3693335 -2.3071437 -1.1661594 -0.20002639 0.26505756 0.1328063 -0.47552431 -1.2534742 -1.9101789 -2.4976401 -2.9719162 -3.1992702][-6.3158059 -5.5958247 -5.4145932 -4.8810873 -4.1159229 -3.3149443 -2.5849953 -2.1867852 -2.1693633 -2.4298568 -2.7589138 -3.059114 -3.3877554 -3.6602693 -3.7449284][-6.2264533 -5.4042654 -5.3383141 -5.0386267 -4.5813222 -4.1095743 -3.6235871 -3.3436508 -3.2966356 -3.3826878 -3.4654965 -3.5437865 -3.660491 -3.7706161 -3.73571][-5.4626141 -4.4791946 -4.50902 -4.4532495 -4.2912407 -4.1328626 -3.8943491 -3.7263222 -3.6568646 -3.6281052 -3.5872316 -3.5434632 -3.5262089 -3.5624971 -3.4974737][-4.8696203 -3.7371242 -3.8172092 -3.9140396 -3.9250398 -3.9733315 -3.9123807 -3.7999609 -3.6776013 -3.5417249 -3.4566932 -3.3956461 -3.3426442 -3.3467155 -3.2780657]]...]
INFO - root - 2017-12-15 06:16:36.040197: step 3710, loss = 0.27, batch loss = 0.23 (33.7 examples/sec; 0.237 sec/batch; 21h:41m:04s remains)
INFO - root - 2017-12-15 06:16:38.333112: step 3720, loss = 0.29, batch loss = 0.26 (34.5 examples/sec; 0.232 sec/batch; 21h:09m:57s remains)
INFO - root - 2017-12-15 06:16:40.614196: step 3730, loss = 0.36, batch loss = 0.33 (36.0 examples/sec; 0.222 sec/batch; 20h:19m:05s remains)
INFO - root - 2017-12-15 06:16:42.903652: step 3740, loss = 0.37, batch loss = 0.33 (35.9 examples/sec; 0.223 sec/batch; 20h:21m:42s remains)
INFO - root - 2017-12-15 06:16:45.190061: step 3750, loss = 0.31, batch loss = 0.28 (35.2 examples/sec; 0.227 sec/batch; 20h:46m:05s remains)
INFO - root - 2017-12-15 06:16:47.464559: step 3760, loss = 0.23, batch loss = 0.20 (33.7 examples/sec; 0.238 sec/batch; 21h:41m:54s remains)
INFO - root - 2017-12-15 06:16:49.763184: step 3770, loss = 0.33, batch loss = 0.30 (36.0 examples/sec; 0.222 sec/batch; 20h:18m:10s remains)
INFO - root - 2017-12-15 06:16:52.037580: step 3780, loss = 0.24, batch loss = 0.20 (35.5 examples/sec; 0.225 sec/batch; 20h:33m:45s remains)
INFO - root - 2017-12-15 06:16:54.323101: step 3790, loss = 0.21, batch loss = 0.17 (35.3 examples/sec; 0.226 sec/batch; 20h:40m:13s remains)
INFO - root - 2017-12-15 06:16:56.623558: step 3800, loss = 0.30, batch loss = 0.26 (35.0 examples/sec; 0.229 sec/batch; 20h:53m:53s remains)
2017-12-15 06:16:56.903067: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8600414 -4.5647559 -4.433939 -4.2161713 -3.8441181 -3.3065419 -2.8818574 -2.9056683 -3.3894913 -3.72096 -3.626708 -3.5324457 -3.571816 -3.6266215 -3.65827][-4.2721472 -5.6792107 -5.5889597 -5.2845488 -4.7500434 -3.9928932 -3.3173862 -3.2582483 -3.9226308 -4.5677147 -4.5681019 -4.5479403 -4.7292776 -4.883585 -4.8862696][-4.8235073 -5.9971418 -5.9474959 -5.6059623 -4.9788675 -4.0294733 -3.073936 -2.8389838 -3.5831053 -4.5565195 -4.7826486 -4.968071 -5.4419026 -5.8337288 -5.9151697][-4.7756987 -5.6374226 -5.6126032 -5.2136393 -4.4530897 -3.2211285 -1.9182169 -1.4426887 -2.1602919 -3.4205322 -3.9406703 -4.3961124 -5.1704984 -5.8537359 -6.1314826][-4.4727 -5.0662103 -4.9972057 -4.3849831 -3.3475583 -1.6450977 0.10046029 0.81537104 -0.0068452358 -1.6341263 -2.4913275 -3.2134051 -4.261878 -5.2541709 -5.7869425][-4.0168734 -4.2904925 -4.0275412 -3.0663981 -1.6217793 0.62371159 2.8111806 3.7151093 2.6784258 0.63976192 -0.54696727 -1.5015976 -2.7889087 -4.0823236 -4.8836565][-3.5527246 -3.9611139 -3.6017098 -2.3882124 -0.59025967 2.1517286 4.7519813 5.8735266 4.6569142 2.262 0.68760657 -0.47898281 -1.9381015 -3.4388962 -4.4353061][-3.6993728 -4.2010713 -3.9240055 -2.6585922 -0.6403054 2.3793416 5.24004 6.5930886 5.4463348 2.9174538 1.0141373 -0.39644921 -1.9719237 -3.5821784 -4.7292233][-3.928669 -4.627069 -4.6663923 -3.722826 -1.9171847 0.89885283 3.7516584 5.3484979 4.6127377 2.4513669 0.59391928 -0.8397193 -2.3808472 -4.0112038 -5.2499771][-4.2197661 -5.104836 -5.5838881 -5.2380714 -4.0005074 -1.7389405 0.83775806 2.5831075 2.4533358 0.97876549 -0.53829932 -1.8528489 -3.264641 -4.8056993 -5.9476643][-4.3979197 -5.3276291 -6.1663804 -6.3986115 -5.8341532 -4.3238878 -2.3296721 -0.74128067 -0.42184448 -1.2076812 -2.308852 -3.4242818 -4.5645342 -5.8143873 -6.6574116][-4.3936615 -5.2325945 -6.3056855 -7.0363188 -7.1193347 -6.4305015 -5.2325244 -4.0719128 -3.5363438 -3.7910819 -4.4391909 -5.2284374 -6.0052986 -6.873117 -7.2900825][-4.3543224 -4.9583139 -6.0212607 -6.9648628 -7.4809165 -7.4259448 -6.9331012 -6.2592907 -5.7289906 -5.6139441 -5.8545065 -6.2663288 -6.6868887 -7.216897 -7.3410473][-4.2466249 -4.4828296 -5.2650023 -6.0912347 -6.6948495 -6.9346814 -6.8837585 -6.5590982 -6.07931 -5.7707062 -5.742496 -5.8887935 -6.106688 -6.4277449 -6.4442406][-4.1798239 -4.0848665 -4.4827294 -4.95148 -5.3387365 -5.5621786 -5.6761541 -5.6003895 -5.3283262 -5.0719833 -4.9920893 -5.0239391 -5.1051903 -5.2555819 -5.2626195]]...]
INFO - root - 2017-12-15 06:16:59.227743: step 3810, loss = 0.31, batch loss = 0.27 (35.6 examples/sec; 0.225 sec/batch; 20h:32m:31s remains)
INFO - root - 2017-12-15 06:17:01.506873: step 3820, loss = 0.28, batch loss = 0.24 (34.7 examples/sec; 0.230 sec/batch; 21h:01m:43s remains)
INFO - root - 2017-12-15 06:17:03.814278: step 3830, loss = 0.33, batch loss = 0.30 (35.8 examples/sec; 0.224 sec/batch; 20h:24m:29s remains)
INFO - root - 2017-12-15 06:17:06.110368: step 3840, loss = 0.25, batch loss = 0.21 (33.1 examples/sec; 0.242 sec/batch; 22h:04m:50s remains)
INFO - root - 2017-12-15 06:17:08.425904: step 3850, loss = 0.32, batch loss = 0.28 (34.9 examples/sec; 0.229 sec/batch; 20h:56m:22s remains)
INFO - root - 2017-12-15 06:17:10.685685: step 3860, loss = 0.30, batch loss = 0.26 (36.0 examples/sec; 0.222 sec/batch; 20h:16m:02s remains)
INFO - root - 2017-12-15 06:17:12.981680: step 3870, loss = 0.34, batch loss = 0.30 (33.5 examples/sec; 0.239 sec/batch; 21h:49m:47s remains)
INFO - root - 2017-12-15 06:17:15.299426: step 3880, loss = 0.23, batch loss = 0.19 (35.8 examples/sec; 0.224 sec/batch; 20h:24m:17s remains)
INFO - root - 2017-12-15 06:17:17.608526: step 3890, loss = 0.33, batch loss = 0.29 (31.7 examples/sec; 0.252 sec/batch; 23h:01m:14s remains)
INFO - root - 2017-12-15 06:17:19.960107: step 3900, loss = 0.31, batch loss = 0.27 (32.7 examples/sec; 0.245 sec/batch; 22h:19m:47s remains)
2017-12-15 06:17:20.271066: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9222445 -3.8996048 -2.5921102 -1.6926033 -1.4246619 -1.6427041 -2.3519614 -3.1871529 -3.7768908 -3.8502569 -3.0552433 -1.8900487 -1.1402322 -0.80409777 -0.93832719][-3.6649208 -3.8482242 -2.197325 -0.97863936 -0.48722827 -0.79757071 -1.7592732 -2.8075066 -3.3863013 -3.4091616 -2.6693847 -1.5510105 -0.92759287 -0.85577321 -1.1426587][-3.8998213 -3.5609012 -1.701643 -0.27440941 0.2072165 -0.30446887 -1.4946153 -2.67597 -3.1864009 -3.0672827 -2.2721517 -1.1620864 -0.57996547 -0.70688939 -1.0075678][-4.1578336 -3.1410456 -1.1305343 0.31590939 0.72583413 0.19949794 -1.0362346 -2.2834625 -2.7590222 -2.5924418 -1.7794265 -0.62412179 -0.028880358 -0.16219735 -0.42726886][-5.057817 -2.6258705 -0.43138051 1.0133126 1.4914749 1.120471 -0.095870495 -1.3541033 -1.8118929 -1.7957293 -1.1187561 -0.0559628 0.54997325 0.43670559 0.1619556][-5.4696951 -2.228915 0.090520382 1.590517 2.2418568 1.9641368 0.86918068 -0.35829067 -1.0339987 -1.2494622 -0.69518077 0.21591091 0.96140766 0.98883033 0.62794542][-4.5009441 -2.0639555 0.33809137 1.8727248 2.6622846 2.5887163 1.696429 0.51283669 -0.44911468 -0.95948184 -0.7024076 0.026186228 0.79714632 1.0169945 0.59361053][-4.5790949 -2.0382755 0.38289022 1.9402018 2.7634623 2.8578212 2.142638 0.901268 -0.32580984 -1.1172434 -1.1472065 -0.54633284 0.18908095 0.57030439 0.35083437][-4.6954613 -2.3108685 -0.067882538 1.3313074 2.0335925 2.1738455 1.4952149 0.32080698 -0.94914007 -1.7171556 -1.5915188 -0.93634593 -0.249452 0.21801376 0.16744041][-4.9652967 -2.8474145 -0.94751358 0.22508526 0.76153135 0.8560493 0.28965783 -0.64230859 -1.6398427 -2.2199683 -2.0183203 -1.4174485 -0.89662766 -0.56492865 -0.5469445][-5.2149291 -3.3966255 -1.8552321 -0.87537861 -0.47642159 -0.41871059 -0.79937959 -1.4603057 -2.1690927 -2.6398988 -2.5046713 -1.9880891 -1.6446422 -1.5564117 -1.6353254][-5.4849811 -4.0128551 -2.8747461 -2.1073034 -1.8030334 -1.7518858 -1.9701601 -2.4941373 -3.066514 -3.5034637 -3.2194886 -2.6570492 -2.3894477 -2.4029818 -2.6293092][-5.8058991 -4.7307367 -4.0205917 -3.4797072 -3.2219434 -3.1499681 -3.2530706 -3.6389954 -4.1218815 -4.4923096 -4.1195498 -3.5968311 -3.3295166 -3.2576728 -3.4131451][-6.0653496 -5.3013105 -4.9496908 -4.60548 -4.4087381 -4.3689871 -4.4348803 -4.7191591 -5.0967827 -5.3743944 -5.0066662 -4.5513134 -4.3041611 -4.1810346 -4.2127275][-6.2275276 -5.6521864 -5.5259094 -5.3122139 -5.1800413 -5.2436752 -5.3380013 -5.5322905 -5.7971921 -5.9457703 -5.6165037 -5.29471 -5.0723743 -4.8934464 -4.8355675]]...]
INFO - root - 2017-12-15 06:17:22.570983: step 3910, loss = 0.43, batch loss = 0.39 (33.0 examples/sec; 0.242 sec/batch; 22h:07m:27s remains)
INFO - root - 2017-12-15 06:17:24.864841: step 3920, loss = 0.29, batch loss = 0.25 (35.7 examples/sec; 0.224 sec/batch; 20h:27m:41s remains)
INFO - root - 2017-12-15 06:17:27.157891: step 3930, loss = 0.41, batch loss = 0.37 (32.7 examples/sec; 0.245 sec/batch; 22h:20m:39s remains)
INFO - root - 2017-12-15 06:17:29.428161: step 3940, loss = 0.31, batch loss = 0.27 (36.2 examples/sec; 0.221 sec/batch; 20h:08m:57s remains)
INFO - root - 2017-12-15 06:17:31.693027: step 3950, loss = 0.40, batch loss = 0.37 (35.7 examples/sec; 0.224 sec/batch; 20h:28m:05s remains)
INFO - root - 2017-12-15 06:17:34.012780: step 3960, loss = 0.23, batch loss = 0.20 (34.9 examples/sec; 0.229 sec/batch; 20h:54m:36s remains)
INFO - root - 2017-12-15 06:17:36.315737: step 3970, loss = 0.33, batch loss = 0.29 (35.7 examples/sec; 0.224 sec/batch; 20h:27m:24s remains)
INFO - root - 2017-12-15 06:17:38.625678: step 3980, loss = 0.35, batch loss = 0.31 (34.7 examples/sec; 0.230 sec/batch; 21h:01m:43s remains)
INFO - root - 2017-12-15 06:17:40.975571: step 3990, loss = 0.32, batch loss = 0.29 (34.7 examples/sec; 0.231 sec/batch; 21h:02m:12s remains)
INFO - root - 2017-12-15 06:17:43.264911: step 4000, loss = 0.28, batch loss = 0.24 (35.1 examples/sec; 0.228 sec/batch; 20h:46m:11s remains)
2017-12-15 06:17:43.553172: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.5689826 -1.4535514 -2.1338508 -2.5343959 -2.7418292 -3.0683351 -3.6121297 -4.5909061 -5.2028103 -4.7098107 -4.02466 -3.5377345 -3.3892262 -3.2508636 -3.3972559][-1.2052907 -1.5589671 -1.877309 -2.0988865 -2.3677795 -2.8646722 -3.464988 -4.3191805 -4.7212515 -4.1820216 -3.7022529 -3.2584083 -2.8587973 -2.3886492 -2.2733271][-1.5909617 -1.3688769 -1.2597325 -1.2705581 -1.6004885 -2.2175417 -2.8532262 -3.6680686 -4.0252266 -3.5413713 -3.2540164 -2.8149188 -2.0698333 -1.1726235 -0.76589942][-1.6259415 -0.92054105 -0.56908321 -0.49899042 -0.84465945 -1.3561003 -1.864598 -2.6655483 -3.0902 -2.8464863 -2.8201747 -2.548378 -1.5893744 -0.27443802 0.45456219][-1.2908247 -0.27702379 0.12812376 0.17190123 -0.15741181 -0.48240232 -0.8605417 -1.6224327 -2.1388664 -2.1803114 -2.4020805 -2.381402 -1.4452033 0.094666481 1.0904021][-0.7069546 0.38441658 0.82565594 0.93486738 0.77567244 0.71392155 0.43786 -0.36857951 -1.118705 -1.527046 -2.0145698 -2.2720375 -1.5325474 0.0064358711 1.1429708][-0.46136081 0.57545471 1.1802399 1.5715783 1.842885 2.1602864 2.09135 1.3408461 0.35429239 -0.55802727 -1.5178174 -2.2108464 -1.888698 -0.57047796 0.64628649][-1.0462984 0.041292429 0.77028537 1.4935482 2.3072381 3.1277876 3.4298162 2.8789811 1.6701775 0.19258165 -1.3467689 -2.4681153 -2.5204835 -1.4878991 -0.18869019][-1.7340209 -0.78895211 -0.0618186 0.88707018 2.0383406 3.1961961 3.9111218 3.7430243 2.4681473 0.55878639 -1.4769739 -2.8710911 -3.0999279 -2.3191319 -1.0452654][-2.2998555 -1.586877 -1.0075102 -0.036883354 1.2029796 2.4184275 3.4428129 3.7446618 2.6789956 0.63230586 -1.6584549 -3.1291533 -3.4050832 -2.88966 -1.8709975][-2.9489341 -2.3566816 -1.9482965 -1.1173984 -0.027987719 1.0420914 2.1917233 2.8934412 2.1860971 0.28833842 -1.9411105 -3.2919393 -3.4983697 -3.2029781 -2.5186055][-3.9177935 -3.3826168 -3.0526681 -2.2906871 -1.3163936 -0.43445575 0.63242769 1.4308503 0.99454641 -0.59115279 -2.5302856 -3.6207232 -3.6897175 -3.4837952 -3.0413053][-5.0547085 -4.5123978 -4.0845375 -3.215297 -2.2059832 -1.4401931 -0.5681355 0.044165611 -0.39787912 -1.8213717 -3.5417297 -4.4138093 -4.313252 -3.9806209 -3.5118399][-5.7078915 -5.1442175 -4.5773029 -3.5838633 -2.5392213 -1.9073373 -1.3690624 -1.1317542 -1.713733 -3.003387 -4.5503159 -5.3637223 -5.2557297 -4.7388272 -4.0078773][-5.4614086 -4.8858624 -4.2127314 -3.1347802 -2.1127617 -1.6469245 -1.4416032 -1.5543773 -2.3001871 -3.4894314 -4.9245071 -5.8534226 -5.949544 -5.3894472 -4.39332]]...]
INFO - root - 2017-12-15 06:17:45.820137: step 4010, loss = 0.24, batch loss = 0.20 (34.8 examples/sec; 0.230 sec/batch; 20h:59m:32s remains)
INFO - root - 2017-12-15 06:17:48.118839: step 4020, loss = 0.24, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 20h:15m:37s remains)
INFO - root - 2017-12-15 06:17:50.436876: step 4030, loss = 0.20, batch loss = 0.16 (34.0 examples/sec; 0.236 sec/batch; 21h:29m:51s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 06:17:52.753379: step 4040, loss = 0.26, batch loss = 0.22 (36.3 examples/sec; 0.221 sec/batch; 20h:08m:05s remains)
INFO - root - 2017-12-15 06:17:55.061477: step 4050, loss = 0.21, batch loss = 0.18 (36.1 examples/sec; 0.222 sec/batch; 20h:14m:28s remains)
INFO - root - 2017-12-15 06:17:57.352114: step 4060, loss = 0.24, batch loss = 0.21 (34.4 examples/sec; 0.233 sec/batch; 21h:14m:51s remains)
INFO - root - 2017-12-15 06:17:59.614960: step 4070, loss = 0.31, batch loss = 0.27 (35.0 examples/sec; 0.228 sec/batch; 20h:50m:06s remains)
INFO - root - 2017-12-15 06:18:01.919895: step 4080, loss = 0.25, batch loss = 0.22 (34.8 examples/sec; 0.230 sec/batch; 20h:59m:28s remains)
INFO - root - 2017-12-15 06:18:04.227366: step 4090, loss = 0.24, batch loss = 0.20 (34.6 examples/sec; 0.231 sec/batch; 21h:05m:46s remains)
INFO - root - 2017-12-15 06:18:06.522508: step 4100, loss = 0.38, batch loss = 0.34 (36.0 examples/sec; 0.222 sec/batch; 20h:15m:07s remains)
2017-12-15 06:18:06.854496: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3872833 -2.5636952 -2.7789729 -2.8398061 -3.0132837 -3.4259777 -3.3434436 -3.093632 -3.1794369 -3.3231401 -3.2641487 -2.7483027 -2.2437634 -2.0267005 -2.4483192][-3.0488315 -3.0572462 -3.1621304 -2.9922378 -2.9531949 -3.0644927 -2.868257 -2.7944458 -3.1158705 -3.446301 -3.4191213 -2.8414662 -2.2089114 -1.8165766 -2.0081735][-3.9827542 -3.8186181 -3.6932273 -3.2120147 -2.7488742 -2.6730795 -2.464596 -2.5522339 -2.9799418 -3.4145777 -3.5196285 -3.0032933 -2.2718167 -1.5585878 -1.4418631][-4.9433537 -4.2263365 -3.690495 -2.701144 -1.9133072 -1.7338284 -1.7458534 -2.1081111 -2.7412329 -3.2272711 -3.34061 -2.9814975 -2.2839324 -1.4390092 -1.0182531][-5.1654758 -4.4202843 -3.4652309 -2.0238845 -0.83577335 -0.63123715 -0.88406479 -1.5520024 -2.5453858 -3.2375312 -3.4297109 -3.191355 -2.477222 -1.5195403 -0.87796462][-5.1308217 -3.9309168 -2.5458314 -0.78337729 0.49087334 0.65308046 0.10226202 -1.0129862 -2.5971494 -3.7030835 -3.9333837 -3.7230191 -2.9501972 -1.8895644 -1.0858587][-4.1664066 -3.3325706 -1.5843697 0.49872875 2.1221223 2.4557848 1.6538477 0.011447668 -2.1786788 -3.8072839 -4.14195 -4.0052881 -3.2670357 -2.2380307 -1.3912015][-4.2448578 -3.1533408 -1.132859 1.152349 3.0245013 3.7926745 3.2061911 1.2941372 -1.4921583 -3.71905 -4.3047786 -4.3395586 -3.7998812 -2.7919641 -1.9210168][-4.4745789 -3.4547739 -1.4612081 0.73361707 2.5912204 3.7278571 3.6117058 1.7608099 -1.0362782 -3.4300179 -4.2148438 -4.4292865 -4.0914249 -3.2724581 -2.4565859][-4.8465128 -4.1060076 -2.4591627 -0.62955165 1.0002954 2.3931518 2.8470941 1.5171673 -0.852227 -3.0015171 -3.987587 -4.4689465 -4.416585 -3.7697492 -3.1265097][-5.0428257 -4.5810814 -3.468153 -2.1865776 -0.96966934 0.42318654 1.2742269 0.39984608 -1.4374393 -3.2232385 -4.2281885 -4.7862024 -4.833087 -4.3640332 -3.8678761][-5.104454 -4.8486118 -4.2694569 -3.5131917 -2.7351656 -1.5725098 -0.68082082 -1.2064776 -2.5537372 -3.8615632 -4.7401204 -5.2994442 -5.3802109 -4.9250031 -4.5033426][-5.081111 -4.8749051 -4.7134838 -4.3581095 -3.9339304 -3.0924816 -2.3460436 -2.7913299 -3.7802694 -4.6616516 -5.2806149 -5.6303768 -5.5379066 -5.1318483 -4.8050089][-5.26307 -4.9781389 -5.1050568 -5.110158 -4.9909654 -4.4389839 -3.8938293 -4.2071056 -4.8044653 -5.2677355 -5.604898 -5.8049111 -5.6680522 -5.2781944 -4.9822226][-5.4974012 -5.0471354 -5.2602596 -5.4211187 -5.4523535 -5.1200824 -4.7431917 -4.9803314 -5.3531713 -5.6253424 -5.824481 -5.899045 -5.6532087 -5.261816 -4.971077]]...]
INFO - root - 2017-12-15 06:18:09.138482: step 4110, loss = 0.37, batch loss = 0.34 (34.4 examples/sec; 0.233 sec/batch; 21h:13m:30s remains)
INFO - root - 2017-12-15 06:18:11.422599: step 4120, loss = 0.28, batch loss = 0.25 (35.7 examples/sec; 0.224 sec/batch; 20h:25m:31s remains)
INFO - root - 2017-12-15 06:18:13.717627: step 4130, loss = 0.25, batch loss = 0.22 (34.7 examples/sec; 0.231 sec/batch; 21h:01m:39s remains)
INFO - root - 2017-12-15 06:18:16.036958: step 4140, loss = 0.23, batch loss = 0.20 (34.7 examples/sec; 0.231 sec/batch; 21h:02m:44s remains)
INFO - root - 2017-12-15 06:18:18.317019: step 4150, loss = 0.25, batch loss = 0.22 (34.9 examples/sec; 0.229 sec/batch; 20h:55m:32s remains)
INFO - root - 2017-12-15 06:18:20.573506: step 4160, loss = 0.31, batch loss = 0.28 (35.9 examples/sec; 0.223 sec/batch; 20h:19m:43s remains)
INFO - root - 2017-12-15 06:18:22.851551: step 4170, loss = 0.30, batch loss = 0.27 (34.8 examples/sec; 0.230 sec/batch; 20h:59m:40s remains)
INFO - root - 2017-12-15 06:18:25.159482: step 4180, loss = 0.27, batch loss = 0.23 (34.2 examples/sec; 0.234 sec/batch; 21h:21m:43s remains)
INFO - root - 2017-12-15 06:18:27.462386: step 4190, loss = 0.41, batch loss = 0.38 (34.0 examples/sec; 0.236 sec/batch; 21h:29m:05s remains)
INFO - root - 2017-12-15 06:18:29.740739: step 4200, loss = 0.35, batch loss = 0.31 (34.9 examples/sec; 0.229 sec/batch; 20h:55m:18s remains)
2017-12-15 06:18:30.022552: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1933 -3.5625153 -5.032104 -5.6545248 -4.7963037 -3.2985933 -1.7180729 -0.43118036 -0.00237751 -0.24205577 -0.98210418 -2.0623848 -2.7210603 -3.1375337 -2.9768209][-1.4279766 -3.2093804 -4.7761059 -5.3660851 -4.4574552 -2.9188712 -1.228688 0.19001961 0.66456604 0.4602375 -0.040469646 -0.81974435 -1.4062831 -1.8488314 -1.8137846][-1.6812525 -2.874691 -4.2572169 -4.5533924 -3.4323473 -1.7461786 -0.030983448 1.2441709 1.5130641 1.111254 0.65028381 0.0657959 -0.38888454 -0.72272968 -0.643824][-2.2194054 -2.9322042 -3.9247055 -3.6949883 -2.2059574 -0.26894093 1.4090593 2.3332219 2.1437092 1.2585428 0.63939452 0.17756438 -0.0017004013 -0.063406706 0.17533445][-3.1727614 -3.2739887 -3.6163268 -2.6338336 -0.62825096 1.6150708 3.3435125 4.038229 3.4205871 1.9675055 0.90271163 0.31817198 0.20727873 0.2185719 0.42751312][-4.1413908 -3.6422844 -3.220706 -1.5006561 1.0134075 3.5823264 5.5360236 6.3256159 5.4963775 3.5402913 1.7907422 0.61834931 0.15429544 -0.044925451 0.0065419674][-4.8027453 -3.8673825 -2.9019017 -0.69016361 2.2667651 5.1951189 7.4571991 8.4954071 7.7182446 5.5271006 3.2612691 1.4071159 0.27140474 -0.382007 -0.63368738][-5.6088381 -4.2756586 -3.0184491 -0.64349759 2.5033312 5.5924544 7.9991369 9.2700357 8.7159128 6.571084 4.0684748 1.7846117 0.071194649 -0.97461462 -1.4390398][-6.5770698 -5.1453323 -3.9088383 -1.7007897 1.3053224 4.2191029 6.5623169 7.9252491 7.7115984 5.9070764 3.5183167 1.102828 -0.93893707 -2.1568296 -2.6321025][-6.8312149 -5.7221985 -4.8728991 -3.1911745 -0.70311749 1.769592 3.9904251 5.4609065 5.7008948 4.4172478 2.3999524 0.1105125 -2.0258658 -3.2487893 -3.6226678][-6.7554922 -6.0374708 -5.6432781 -4.5113974 -2.5929897 -0.59694827 1.3742216 2.7796235 3.3257103 2.5745239 1.0424342 -0.93092477 -2.9855788 -4.2397728 -4.6147823][-7.0813422 -6.5950813 -6.4874206 -5.766201 -4.3274975 -2.7301383 -1.0637116 0.13318324 0.78444695 0.45463085 -0.61106837 -2.2062697 -4.0500383 -5.2782812 -5.6676116][-7.2507162 -6.934124 -7.059505 -6.7692561 -5.8920126 -4.743577 -3.4146733 -2.4159312 -1.7314773 -1.7299671 -2.3745821 -3.5466695 -5.0306158 -6.0840039 -6.4424634][-6.9459305 -6.7056565 -7.0007496 -7.0615549 -6.6917424 -6.0014191 -5.0925198 -4.3762684 -3.8183398 -3.6707141 -4.0058179 -4.730197 -5.6901436 -6.39384 -6.6552706][-6.260313 -5.9014072 -6.2138596 -6.4117079 -6.3369355 -6.0205355 -5.562789 -5.2081375 -4.8592224 -4.6763945 -4.7941751 -5.1494269 -5.6293731 -5.9915428 -6.1703286]]...]
INFO - root - 2017-12-15 06:18:32.318939: step 4210, loss = 0.31, batch loss = 0.27 (33.0 examples/sec; 0.243 sec/batch; 22h:08m:03s remains)
INFO - root - 2017-12-15 06:18:34.638738: step 4220, loss = 0.35, batch loss = 0.31 (35.3 examples/sec; 0.226 sec/batch; 20h:38m:37s remains)
INFO - root - 2017-12-15 06:18:36.964258: step 4230, loss = 0.26, batch loss = 0.22 (30.8 examples/sec; 0.259 sec/batch; 23h:38m:50s remains)
INFO - root - 2017-12-15 06:18:39.268522: step 4240, loss = 0.35, batch loss = 0.32 (35.2 examples/sec; 0.227 sec/batch; 20h:42m:17s remains)
INFO - root - 2017-12-15 06:18:41.541342: step 4250, loss = 0.18, batch loss = 0.15 (35.9 examples/sec; 0.223 sec/batch; 20h:19m:00s remains)
INFO - root - 2017-12-15 06:18:43.832542: step 4260, loss = 0.42, batch loss = 0.39 (35.5 examples/sec; 0.225 sec/batch; 20h:32m:06s remains)
INFO - root - 2017-12-15 06:18:46.113269: step 4270, loss = 0.27, batch loss = 0.23 (35.8 examples/sec; 0.224 sec/batch; 20h:22m:40s remains)
INFO - root - 2017-12-15 06:18:48.393260: step 4280, loss = 0.23, batch loss = 0.19 (36.5 examples/sec; 0.219 sec/batch; 19h:59m:06s remains)
INFO - root - 2017-12-15 06:18:50.674381: step 4290, loss = 0.27, batch loss = 0.23 (34.0 examples/sec; 0.235 sec/batch; 21h:25m:54s remains)
INFO - root - 2017-12-15 06:18:53.014551: step 4300, loss = 0.22, batch loss = 0.18 (34.8 examples/sec; 0.230 sec/batch; 20h:57m:16s remains)
2017-12-15 06:18:53.407914: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.111105 -6.4048958 -6.8671656 -6.3443408 -4.6997905 -2.6587658 -0.93048549 -0.3283478 -0.82406485 -1.2497592 -2.1873171 -3.1901588 -3.4777446 -3.7620363 -4.4412937][-4.6328673 -6.6642046 -7.1400647 -6.7008963 -5.1147356 -3.044961 -1.1749127 -0.53009319 -0.70531917 -1.3302122 -2.5786402 -3.9556918 -4.206912 -4.4397449 -4.8554869][-5.01722 -6.7525911 -7.29236 -6.8602438 -5.3604193 -3.4036758 -1.4199069 -0.61461723 -0.71817768 -1.4729564 -3.1089163 -4.659646 -4.7572184 -4.737226 -4.8045511][-5.70261 -6.8136492 -7.3859587 -6.9315372 -5.5484409 -3.7450252 -1.6856654 -0.66245306 -0.71262813 -1.6667112 -3.6813467 -5.1792431 -5.1371279 -4.9335804 -4.7580929][-6.2650776 -6.8373885 -7.3585277 -6.7887578 -5.390317 -3.6943853 -1.6053046 -0.5142504 -0.61119354 -1.6543967 -3.7178464 -4.9675093 -4.8662205 -4.7625532 -4.5246868][-6.4508696 -6.8920736 -7.3230023 -6.542984 -4.8946724 -2.9845834 -0.65713394 0.56602859 0.30680943 -1.0010498 -2.9579458 -4.1411657 -4.23699 -4.4785237 -4.271152][-6.1990538 -6.8418307 -7.1596828 -6.1034288 -4.0315371 -1.5700382 1.346457 3.0308886 2.5211425 0.580513 -1.4736497 -2.8069179 -3.1495681 -3.7360883 -3.5297065][-6.2030339 -6.8074222 -7.1237698 -5.875948 -3.4264779 -0.45292127 3.0476336 5.1687107 4.3070307 1.7885473 -0.4446913 -1.9029801 -2.3511586 -3.0405076 -2.8368268][-6.068521 -6.7730179 -7.2579408 -6.0626955 -3.5922742 -0.54964471 3.1582527 5.5834584 4.7880082 2.2856359 0.066560507 -1.53246 -2.0037503 -2.7173994 -2.6064718][-5.8630743 -6.7242484 -7.4500675 -6.5417776 -4.4432144 -1.7946894 1.688426 4.2298946 3.8908224 1.8411069 -0.2174921 -1.9495811 -2.3970437 -2.9599931 -2.8153706][-5.6359181 -6.5839009 -7.491354 -6.9953365 -5.5605869 -3.585259 -0.543224 2.034246 2.2995157 0.97247696 -0.73479772 -2.4309173 -2.8808913 -3.4313498 -3.1837039][-5.5233965 -6.4742112 -7.4954934 -7.4727769 -6.8091674 -5.6279888 -3.1948681 -0.76698089 -0.042402267 -0.56060851 -1.6342124 -2.9603443 -3.3031278 -3.8385756 -3.6179957][-5.622539 -6.5285072 -7.5789242 -7.8368773 -7.6411042 -7.0032835 -5.1985874 -3.212976 -2.3978236 -2.3899574 -3.0068011 -4.0320368 -4.1680613 -4.4379277 -4.1578951][-5.897974 -6.8119874 -7.8701448 -8.2481441 -8.2487125 -7.8657222 -6.6561933 -5.2950044 -4.7329268 -4.6088228 -5.116663 -6.0058031 -5.9661789 -5.6761069 -5.2415385][-6.0019836 -6.9325628 -7.9690657 -8.33046 -8.2812843 -7.7862263 -6.7673106 -5.8621011 -5.6398129 -5.7753992 -6.5818629 -7.6419911 -7.6051655 -7.1541977 -6.7527261]]...]
INFO - root - 2017-12-15 06:18:55.727781: step 4310, loss = 0.41, batch loss = 0.37 (34.7 examples/sec; 0.231 sec/batch; 21h:02m:00s remains)
INFO - root - 2017-12-15 06:18:58.039062: step 4320, loss = 0.50, batch loss = 0.46 (33.1 examples/sec; 0.241 sec/batch; 21h:59m:59s remains)
INFO - root - 2017-12-15 06:19:00.316291: step 4330, loss = 0.35, batch loss = 0.31 (36.5 examples/sec; 0.219 sec/batch; 19h:57m:21s remains)
INFO - root - 2017-12-15 06:19:02.564815: step 4340, loss = 0.37, batch loss = 0.34 (35.2 examples/sec; 0.227 sec/batch; 20h:41m:47s remains)
INFO - root - 2017-12-15 06:19:04.835138: step 4350, loss = 0.33, batch loss = 0.29 (34.9 examples/sec; 0.229 sec/batch; 20h:53m:10s remains)
INFO - root - 2017-12-15 06:19:07.088559: step 4360, loss = 0.36, batch loss = 0.32 (36.3 examples/sec; 0.220 sec/batch; 20h:05m:00s remains)
INFO - root - 2017-12-15 06:19:09.396926: step 4370, loss = 0.31, batch loss = 0.28 (32.9 examples/sec; 0.243 sec/batch; 22h:09m:42s remains)
INFO - root - 2017-12-15 06:19:11.678049: step 4380, loss = 0.28, batch loss = 0.24 (35.6 examples/sec; 0.225 sec/batch; 20h:29m:30s remains)
INFO - root - 2017-12-15 06:19:13.935279: step 4390, loss = 0.30, batch loss = 0.26 (35.2 examples/sec; 0.227 sec/batch; 20h:42m:44s remains)
INFO - root - 2017-12-15 06:19:16.226871: step 4400, loss = 0.28, batch loss = 0.24 (34.3 examples/sec; 0.233 sec/batch; 21h:15m:33s remains)
2017-12-15 06:19:16.494955: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.9125731 -1.6105545 -2.278878 -3.1095881 -3.7318764 -4.1573429 -4.478569 -4.4543667 -4.412519 -4.3766918 -4.1633978 -4.0365362 -4.4400368 -5.114707 -5.3950315][-2.1098955 -2.5678506 -3.3184414 -4.0111675 -4.4034019 -4.4751935 -4.5830908 -4.4596224 -4.3532672 -4.47787 -4.5024929 -4.5163536 -4.7217107 -4.8032055 -4.4491673][-2.9401126 -3.1258645 -3.7462349 -4.0628672 -3.9275873 -3.5713644 -3.4925389 -3.4210496 -3.4162731 -3.7254074 -3.9673085 -4.02985 -3.9395165 -3.4621282 -2.7034559][-3.0568292 -2.99228 -3.3858366 -3.2721531 -2.551852 -1.758853 -1.4530165 -1.405582 -1.5640208 -2.05604 -2.599225 -2.8937612 -2.7675667 -2.0293524 -1.1575563][-2.4182312 -2.149097 -2.3899617 -2.0203061 -0.81409442 0.3815279 1.0397208 1.2292414 0.92616844 0.32865262 -0.5343821 -1.3870554 -1.68446 -0.97201967 -0.0800761][-1.6480883 -1.5035783 -1.6461589 -1.1178658 0.40576863 1.961761 3.2104764 3.78154 3.470048 2.8023505 1.5594289 0.034028292 -0.835464 -0.36719561 0.34382343][-1.5197744 -1.6496236 -1.5498466 -0.74260259 0.98370528 2.8312449 4.6151748 5.63867 5.3707333 4.5312705 2.9158573 0.89709687 -0.37968516 -0.37008941 -0.14226282][-2.8538022 -2.7799726 -2.2834084 -1.1406357 0.70842552 2.7218533 4.8609371 6.2430062 5.9621329 4.7992315 2.8630438 0.66921806 -0.83864307 -1.3455014 -1.6271279][-5.0131655 -4.53042 -3.749712 -2.3960915 -0.58465838 1.2717736 3.2660298 4.6006427 4.2578592 2.9032412 0.97122478 -1.0048379 -2.4658082 -3.2745662 -3.8038518][-7.1617732 -6.368022 -5.6345148 -4.3252459 -2.7559268 -1.2753179 0.16572618 1.1126335 0.78902984 -0.4393791 -1.9963012 -3.5220654 -4.6724253 -5.4355793 -5.9264221][-8.482523 -7.5763788 -7.1971407 -6.2779212 -5.1527858 -4.1763649 -3.3777075 -2.8649848 -3.1225033 -4.0187941 -5.0471897 -5.9838991 -6.6443391 -7.0293231 -7.1868615][-8.6310158 -7.6828279 -7.7442007 -7.3766804 -6.7880068 -6.2989383 -5.9705477 -5.7845669 -5.9573884 -6.4855137 -7.0157585 -7.4429049 -7.6480713 -7.5917587 -7.3519711][-7.7975683 -6.7681794 -7.09392 -7.1538248 -7.0450816 -6.9686275 -7.0010018 -7.0596628 -7.180552 -7.3794417 -7.508482 -7.557611 -7.3987656 -7.1404648 -6.793519][-6.64096 -5.5135317 -5.8728714 -6.0947309 -6.247952 -6.40612 -6.5964303 -6.7519031 -6.8342142 -6.8508534 -6.7995162 -6.6395226 -6.3256721 -6.1219282 -5.9742708][-5.6146212 -4.380331 -4.64386 -4.8760462 -5.1302142 -5.3634844 -5.5380535 -5.6779838 -5.7507892 -5.7720051 -5.757453 -5.6125069 -5.3741379 -5.3577747 -5.4911351]]...]
INFO - root - 2017-12-15 06:19:18.760828: step 4410, loss = 0.31, batch loss = 0.28 (34.5 examples/sec; 0.232 sec/batch; 21h:08m:31s remains)
INFO - root - 2017-12-15 06:19:21.031765: step 4420, loss = 0.29, batch loss = 0.26 (35.4 examples/sec; 0.226 sec/batch; 20h:35m:48s remains)
INFO - root - 2017-12-15 06:19:23.320948: step 4430, loss = 0.34, batch loss = 0.30 (35.8 examples/sec; 0.223 sec/batch; 20h:20m:50s remains)
INFO - root - 2017-12-15 06:19:25.589861: step 4440, loss = 0.25, batch loss = 0.21 (35.0 examples/sec; 0.228 sec/batch; 20h:48m:11s remains)
INFO - root - 2017-12-15 06:19:27.876963: step 4450, loss = 0.25, batch loss = 0.21 (35.6 examples/sec; 0.225 sec/batch; 20h:29m:31s remains)
INFO - root - 2017-12-15 06:19:30.191162: step 4460, loss = 0.27, batch loss = 0.24 (33.6 examples/sec; 0.238 sec/batch; 21h:41m:26s remains)
INFO - root - 2017-12-15 06:19:32.474857: step 4470, loss = 0.24, batch loss = 0.20 (35.6 examples/sec; 0.225 sec/batch; 20h:28m:53s remains)
INFO - root - 2017-12-15 06:19:34.719210: step 4480, loss = 0.19, batch loss = 0.15 (35.1 examples/sec; 0.228 sec/batch; 20h:46m:15s remains)
INFO - root - 2017-12-15 06:19:36.991993: step 4490, loss = 0.27, batch loss = 0.23 (35.4 examples/sec; 0.226 sec/batch; 20h:36m:28s remains)
INFO - root - 2017-12-15 06:19:39.292744: step 4500, loss = 0.25, batch loss = 0.21 (34.9 examples/sec; 0.229 sec/batch; 20h:51m:46s remains)
2017-12-15 06:19:39.577267: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.053925 -5.1247749 -5.9171877 -6.4812326 -6.5687895 -5.8687763 -5.1994867 -4.8934355 -5.0424414 -5.1937151 -5.44874 -5.8238087 -6.2659245 -5.8839817 -4.8146191][-4.3226247 -4.9505215 -6.0697346 -7.0241938 -7.2350416 -6.6192732 -5.9923725 -5.6808681 -5.8223796 -6.2164717 -6.6659241 -6.9179869 -7.2177906 -6.5815744 -5.1219149][-4.5500212 -4.5023575 -5.6277385 -6.5108805 -6.5326691 -5.8192077 -5.2380013 -4.9952335 -5.2750406 -6.0424185 -6.865653 -7.1965485 -7.4562016 -6.6871281 -5.1846857][-4.3844943 -3.9869606 -4.7996283 -5.2801275 -4.8010554 -3.7487369 -3.1263247 -3.03349 -3.5442502 -4.6659756 -5.8752546 -6.5760746 -7.060915 -6.333621 -5.0381603][-4.0525632 -3.7036288 -4.24306 -4.2397981 -3.1269639 -1.587322 -0.64509141 -0.36950171 -0.93343437 -2.2395427 -3.8386962 -5.2861571 -6.2710867 -5.7318087 -4.745307][-3.8348336 -3.47961 -3.8709612 -3.503221 -1.9120927 0.074824095 1.7316587 2.7757475 2.4824979 1.0228038 -0.9464314 -3.1856408 -4.8576832 -4.8790312 -4.529829][-3.5325682 -3.3506515 -3.726651 -3.3412032 -1.6693147 0.581146 3.0197227 5.0454845 5.2320709 3.6287687 1.2755387 -1.476784 -3.5596378 -4.1206408 -4.401825][-3.5629661 -3.2148149 -3.7315545 -3.60851 -2.2147858 -0.16341424 2.454118 4.8408651 5.2373533 3.6426623 1.3959694 -1.2983952 -3.2509913 -3.9649918 -4.4693565][-3.1143022 -2.8425863 -3.5635488 -3.768116 -2.8409638 -1.3106488 0.81591105 2.7755687 3.0306017 1.7067022 -0.1741246 -2.3534944 -3.8130338 -4.3940258 -4.7950859][-1.6838447 -1.7682748 -2.8709269 -3.4630084 -3.0122395 -2.0613422 -0.6724261 0.64992642 0.69602537 -0.28150272 -1.677604 -3.144825 -3.9812944 -4.253191 -4.4168677][-0.15640533 -0.50998116 -1.962292 -2.8226829 -2.7585874 -2.20412 -1.4125752 -0.67181373 -0.741626 -1.2764378 -2.114902 -3.0368075 -3.3650923 -3.3256872 -3.26895][0.38938928 0.048872709 -1.516312 -2.3687184 -2.4400227 -2.1247652 -1.7293696 -1.3993216 -1.4101412 -1.5393476 -1.9151356 -2.3695691 -2.2905858 -2.0056388 -1.9210839][-0.15728641 -0.32880986 -1.7338161 -2.330204 -2.2899218 -2.140173 -2.148886 -2.1292157 -2.0659442 -1.9490628 -1.8900118 -1.7794662 -1.2107096 -0.6152823 -0.39738238][-1.5405064 -1.4506063 -2.3910782 -2.5819888 -2.331419 -2.1743419 -2.5060525 -2.7631812 -2.6633911 -2.45893 -2.1861143 -1.6509343 -0.78334379 -0.062464476 0.24911666][-3.0264468 -2.9799016 -3.533731 -3.3761263 -2.9200084 -2.5628004 -2.7847295 -3.1280379 -3.1238675 -2.9952428 -2.6800568 -2.0672295 -1.2476989 -0.61105263 -0.31924975]]...]
INFO - root - 2017-12-15 06:19:41.831943: step 4510, loss = 0.22, batch loss = 0.18 (35.8 examples/sec; 0.223 sec/batch; 20h:20m:31s remains)
INFO - root - 2017-12-15 06:19:44.118626: step 4520, loss = 0.27, batch loss = 0.23 (34.8 examples/sec; 0.230 sec/batch; 20h:55m:00s remains)
INFO - root - 2017-12-15 06:19:46.402390: step 4530, loss = 0.29, batch loss = 0.25 (35.9 examples/sec; 0.223 sec/batch; 20h:18m:15s remains)
INFO - root - 2017-12-15 06:19:48.684706: step 4540, loss = 0.23, batch loss = 0.19 (35.0 examples/sec; 0.229 sec/batch; 20h:49m:39s remains)
INFO - root - 2017-12-15 06:19:50.982739: step 4550, loss = 0.24, batch loss = 0.20 (35.0 examples/sec; 0.228 sec/batch; 20h:48m:26s remains)
INFO - root - 2017-12-15 06:19:53.240569: step 4560, loss = 0.29, batch loss = 0.25 (35.3 examples/sec; 0.227 sec/batch; 20h:39m:17s remains)
INFO - root - 2017-12-15 06:19:55.540826: step 4570, loss = 0.24, batch loss = 0.20 (35.6 examples/sec; 0.224 sec/batch; 20h:26m:38s remains)
INFO - root - 2017-12-15 06:19:57.805867: step 4580, loss = 0.25, batch loss = 0.21 (35.5 examples/sec; 0.225 sec/batch; 20h:30m:37s remains)
INFO - root - 2017-12-15 06:20:00.087396: step 4590, loss = 0.30, batch loss = 0.26 (35.6 examples/sec; 0.225 sec/batch; 20h:27m:41s remains)
INFO - root - 2017-12-15 06:20:02.353276: step 4600, loss = 0.29, batch loss = 0.25 (33.7 examples/sec; 0.237 sec/batch; 21h:37m:30s remains)
2017-12-15 06:20:02.639063: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6577641 -2.8888931 -2.9002745 -2.8099561 -2.6070361 -2.3440766 -2.1871645 -2.1502774 -1.9280434 -1.0752013 -0.58563137 -1.1532162 -2.1379619 -3.2936857 -4.5605197][-3.1784883 -3.6192365 -3.6296763 -3.5323439 -3.332119 -3.0155218 -2.7497375 -2.6410265 -2.3600619 -1.3900068 -0.84370816 -1.4522741 -2.5246036 -3.8858476 -5.4133139][-4.0027084 -3.8722949 -3.8658409 -3.7557721 -3.5615425 -3.2022972 -2.8320839 -2.657515 -2.4721065 -1.7104232 -1.2841903 -1.9086609 -3.0482459 -4.5769963 -6.223063][-4.2937193 -3.5304847 -3.5088649 -3.425225 -3.1764402 -2.6995306 -2.2001867 -1.9684842 -1.9893268 -1.6705397 -1.5203385 -2.1720624 -3.2817111 -4.7388363 -6.2612782][-3.85963 -2.576138 -2.4641159 -2.2318854 -1.7848718 -1.1269385 -0.39940655 -0.034559011 -0.44436204 -0.88830519 -1.2861664 -2.1995239 -3.3940902 -4.7181654 -6.0150318][-2.9288723 -1.2254906 -1.0636563 -0.70563912 -0.011567354 0.9104805 1.9894729 2.5655775 1.796315 0.62225533 -0.43986595 -1.8520658 -3.2764697 -4.466753 -5.5408945][-1.8833143 -0.25080419 -0.0088784695 0.48432422 1.4284837 2.6970644 4.2147 5.0690641 4.0905547 2.392417 0.72911453 -1.2977567 -3.0678263 -4.2384973 -5.2025261][-1.2461777 0.51008177 0.78362894 1.2128205 2.1904888 3.6605296 5.474472 6.5277877 5.5528684 3.7625332 1.7851758 -0.70346677 -2.7922826 -4.0447121 -5.0749178][-1.58855 0.13202095 0.38972902 0.71835113 1.5935183 3.0821018 5.0257111 6.2904334 5.6254315 4.1510978 2.2258215 -0.37169015 -2.5741408 -3.9809918 -5.250432][-2.9239483 -1.2661982 -0.96163404 -0.78546059 -0.12952566 1.1866977 2.9449 4.192595 3.9473815 3.0364304 1.4409227 -0.96133125 -3.0489123 -4.520895 -5.9200191][-4.1929779 -2.7609973 -2.5621276 -2.6306789 -2.2483816 -1.2246013 0.11261702 1.0886061 1.0468812 0.61476374 -0.55733132 -2.5622787 -4.3140688 -5.5846539 -6.8646493][-4.81793 -3.8392768 -3.8613129 -4.1467443 -4.0461369 -3.3538797 -2.4841418 -1.9050223 -1.9229705 -1.853129 -2.4151633 -3.8378592 -5.1526213 -6.1234007 -7.2196312][-5.1278362 -4.6948366 -5.0424623 -5.4862947 -5.5562596 -5.0158648 -4.3911452 -4.0878391 -4.054461 -3.3807077 -3.1947181 -4.0617495 -5.0036097 -5.7717991 -6.9050531][-5.0988064 -5.1466045 -5.67243 -6.117548 -6.2449241 -5.7642326 -5.2881517 -5.1862864 -5.0818872 -3.8142686 -2.9841366 -3.522584 -4.3517904 -5.1607261 -6.53279][-4.2912111 -4.6595187 -5.2972541 -5.7247372 -5.9040365 -5.5221395 -5.1831255 -5.2502351 -5.0857716 -3.3476086 -2.0370986 -2.3730857 -3.2462497 -4.2183247 -5.8798018]]...]
INFO - root - 2017-12-15 06:20:04.941996: step 4610, loss = 0.27, batch loss = 0.24 (35.8 examples/sec; 0.224 sec/batch; 20h:22m:25s remains)
INFO - root - 2017-12-15 06:20:07.249771: step 4620, loss = 0.40, batch loss = 0.36 (33.7 examples/sec; 0.237 sec/batch; 21h:36m:20s remains)
INFO - root - 2017-12-15 06:20:09.540191: step 4630, loss = 0.27, batch loss = 0.23 (35.9 examples/sec; 0.223 sec/batch; 20h:16m:03s remains)
INFO - root - 2017-12-15 06:20:11.822546: step 4640, loss = 0.31, batch loss = 0.27 (36.2 examples/sec; 0.221 sec/batch; 20h:08m:37s remains)
INFO - root - 2017-12-15 06:20:14.108906: step 4650, loss = 0.26, batch loss = 0.22 (35.8 examples/sec; 0.223 sec/batch; 20h:19m:28s remains)
INFO - root - 2017-12-15 06:20:16.381188: step 4660, loss = 0.26, batch loss = 0.22 (34.9 examples/sec; 0.229 sec/batch; 20h:52m:27s remains)
INFO - root - 2017-12-15 06:20:18.632451: step 4670, loss = 0.23, batch loss = 0.19 (35.8 examples/sec; 0.223 sec/batch; 20h:19m:50s remains)
INFO - root - 2017-12-15 06:20:20.901843: step 4680, loss = 0.28, batch loss = 0.24 (35.9 examples/sec; 0.223 sec/batch; 20h:18m:38s remains)
INFO - root - 2017-12-15 06:20:23.186308: step 4690, loss = 0.25, batch loss = 0.21 (35.9 examples/sec; 0.223 sec/batch; 20h:18m:15s remains)
INFO - root - 2017-12-15 06:20:25.503224: step 4700, loss = 0.25, batch loss = 0.21 (35.3 examples/sec; 0.227 sec/batch; 20h:39m:33s remains)
2017-12-15 06:20:25.786138: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.71407473 -1.674751 -2.4602053 -3.4809537 -4.4880996 -5.2335706 -5.4920053 -5.2937841 -5.1561241 -5.20825 -5.3388977 -5.5741005 -5.3473148 -4.7578897 -4.2951412][-0.76187837 -1.2252653 -2.0769627 -3.0769238 -4.04192 -4.7267084 -4.8581982 -4.52991 -4.3114195 -4.2526913 -4.409533 -4.844923 -4.8527956 -4.5136127 -4.3604856][-0.54889429 -0.5906477 -1.4524341 -2.3998883 -3.2613401 -3.739224 -3.61091 -3.2137208 -2.9957647 -2.9137344 -3.2435079 -3.9838271 -4.1522613 -3.9677582 -3.982522][-0.10781825 0.16814256 -0.57568538 -1.4396445 -2.1940866 -2.4003086 -2.0539427 -1.7220643 -1.7110287 -1.8095255 -2.3387415 -3.2896714 -3.4543502 -3.2589459 -3.3400159][-0.20997047 0.30251265 -0.14668489 -0.61568236 -0.86526513 -0.41198003 0.26230264 0.39590669 0.067675352 -0.2931118 -1.0366751 -2.1224246 -2.2499342 -2.065331 -2.3084106][-0.92396069 -0.1642127 -0.16540051 -0.05777216 0.37889957 1.5321813 2.4487672 2.4473238 1.941462 1.4407005 0.58010149 -0.6771853 -0.91249323 -0.91302109 -1.4700395][-2.4112964 -1.3391536 -0.87596035 -0.19957387 0.870296 2.5928473 3.6633315 3.63301 3.1389871 2.7067437 1.902272 0.55999517 0.15617061 -0.14223313 -1.0416099][-4.4831524 -3.0453176 -2.3823047 -1.4728076 -0.09071517 2.0166502 3.3363185 3.4431229 3.0524373 2.8302584 2.1400681 0.74534535 0.18597269 -0.30257523 -1.3312721][-5.4987979 -4.0860467 -3.6597381 -3.0429063 -1.9352342 0.11798286 1.6068757 2.0038805 1.8524475 1.887835 1.3509679 0.0093061924 -0.67913771 -1.2810587 -2.2916298][-5.3168993 -4.1331429 -4.1564064 -4.1201372 -3.6757221 -2.1563945 -0.79925156 -0.20758986 -0.1328069 0.1509409 -0.13004327 -1.1606539 -1.8084054 -2.4387028 -3.372175][-4.6274004 -3.7576251 -4.1975188 -4.6547422 -4.8232422 -3.9794874 -2.9469163 -2.3926563 -2.2823248 -1.9482329 -2.0688698 -2.7586114 -3.24013 -3.711885 -4.4435706][-3.9761734 -3.3372688 -4.0122209 -4.7955265 -5.3887243 -5.1158957 -4.4891038 -4.1347618 -4.1367121 -3.9598024 -4.0765524 -4.4906845 -4.711688 -4.8853855 -5.2723527][-3.6156354 -3.0136828 -3.7834649 -4.7235265 -5.5458221 -5.7164555 -5.5095234 -5.430974 -5.5670562 -5.5953341 -5.7732124 -5.9653988 -5.9047632 -5.7251453 -5.6925693][-3.6331649 -2.9542329 -3.6248724 -4.4724708 -5.3019514 -5.7150416 -5.8254309 -5.9774208 -6.2365761 -6.4501023 -6.718071 -6.8195515 -6.5721288 -6.1187119 -5.7290711][-3.7039957 -2.8885243 -3.3272016 -3.9229758 -4.5987921 -5.0524836 -5.3234072 -5.6017084 -5.8954258 -6.2080164 -6.5483675 -6.6767397 -6.39586 -5.83817 -5.297204]]...]
INFO - root - 2017-12-15 06:20:28.057318: step 4710, loss = 0.25, batch loss = 0.21 (34.8 examples/sec; 0.230 sec/batch; 20h:56m:20s remains)
INFO - root - 2017-12-15 06:20:30.316185: step 4720, loss = 0.21, batch loss = 0.17 (36.0 examples/sec; 0.222 sec/batch; 20h:13m:02s remains)
INFO - root - 2017-12-15 06:20:32.602780: step 4730, loss = 0.29, batch loss = 0.25 (35.2 examples/sec; 0.227 sec/batch; 20h:41m:28s remains)
INFO - root - 2017-12-15 06:20:34.849222: step 4740, loss = 0.26, batch loss = 0.22 (35.1 examples/sec; 0.228 sec/batch; 20h:46m:28s remains)
INFO - root - 2017-12-15 06:20:37.144764: step 4750, loss = 0.22, batch loss = 0.18 (35.2 examples/sec; 0.227 sec/batch; 20h:41m:02s remains)
INFO - root - 2017-12-15 06:20:39.427253: step 4760, loss = 0.32, batch loss = 0.29 (35.5 examples/sec; 0.226 sec/batch; 20h:31m:50s remains)
INFO - root - 2017-12-15 06:20:41.725133: step 4770, loss = 0.19, batch loss = 0.15 (35.9 examples/sec; 0.223 sec/batch; 20h:15m:48s remains)
INFO - root - 2017-12-15 06:20:44.017387: step 4780, loss = 0.48, batch loss = 0.44 (35.7 examples/sec; 0.224 sec/batch; 20h:24m:39s remains)
INFO - root - 2017-12-15 06:20:46.280755: step 4790, loss = 0.27, batch loss = 0.23 (34.7 examples/sec; 0.231 sec/batch; 21h:00m:21s remains)
INFO - root - 2017-12-15 06:20:48.579332: step 4800, loss = 0.20, batch loss = 0.17 (35.2 examples/sec; 0.227 sec/batch; 20h:41m:03s remains)
2017-12-15 06:20:48.864495: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6928144 -4.9172225 -4.6497946 -3.8966868 -2.8812876 -2.7787728 -4.3773232 -5.1303873 -4.3339887 -3.8548331 -3.892308 -4.055377 -3.9128733 -3.9735212 -4.1973314][-5.8576536 -5.0062609 -5.0628304 -4.5430384 -3.7480893 -3.6890182 -4.9361677 -5.2615328 -4.4489536 -4.0327883 -4.0865903 -4.2340937 -4.002852 -3.9001954 -3.9429533][-5.810276 -4.9949608 -5.3144832 -5.038342 -4.5016804 -4.4336 -5.2363334 -5.2484007 -4.5299387 -4.2667761 -4.4004021 -4.5622473 -4.2908082 -4.0946932 -4.0217962][-5.3913469 -4.5515647 -4.9400492 -4.8313031 -4.5330582 -4.5182996 -4.895875 -4.6251254 -4.0368643 -3.9746881 -4.2341084 -4.4388342 -4.1890073 -3.8839836 -3.6312485][-4.7612944 -3.6794672 -3.7564437 -3.5156596 -3.3028574 -3.3204422 -3.3230233 -2.8501492 -2.3218327 -2.4676526 -2.8855472 -3.0931659 -2.8452568 -2.4374523 -2.0077662][-4.8368487 -3.0526128 -2.4711597 -1.8753451 -1.6024079 -1.5144744 -1.1988163 -0.43111038 0.098002672 -0.21820915 -0.86857677 -1.2025211 -1.1684531 -0.9098655 -0.544899][-4.9654932 -2.817441 -1.6078942 -0.62430716 -0.20200801 0.1189847 0.68179536 1.6234808 2.2193871 1.7439823 0.75926542 0.12726116 -0.14717674 -0.17541933 0.036869526][-5.1322856 -2.5534406 -0.93141234 0.30121017 0.85225725 1.3190773 1.9210644 2.9404573 3.5468955 2.997097 1.7389705 0.84254527 0.34620452 0.069056034 0.13955545][-5.4358425 -2.7199664 -1.0381715 0.261209 0.88353539 1.3414931 1.6443083 2.4020548 2.9449897 2.467062 1.2310677 0.29211783 -0.19842124 -0.47150159 -0.4068737][-5.6798267 -3.003505 -1.5657277 -0.52290595 0.031573057 0.25109553 0.021217585 0.32589579 0.71977186 0.43267703 -0.45845103 -1.0620419 -1.227176 -1.2948 -1.1885302][-6.1849284 -3.7004669 -2.6476393 -1.9697688 -1.5497488 -1.5314381 -2.1392491 -2.2338839 -1.9216214 -2.0013435 -2.5149391 -2.7023494 -2.5075896 -2.3638182 -2.1240764][-6.2908382 -4.0264254 -3.3026004 -2.976007 -2.7680764 -2.9326439 -3.8123455 -4.2661848 -4.1067891 -4.1003008 -4.4081812 -4.3979735 -4.1237659 -3.9638317 -3.6877785][-6.2228994 -4.2187824 -3.6969919 -3.6228614 -3.5768898 -3.8327384 -4.8480062 -5.5398536 -5.5332103 -5.5514331 -5.8752 -5.9951239 -5.9673676 -6.0338163 -5.8357549][-6.4870653 -4.6334705 -4.1014633 -4.0129609 -3.9520602 -4.1786032 -5.2078114 -6.0174389 -6.09761 -6.0852752 -6.3738828 -6.5432186 -6.672091 -6.8885107 -6.8154559][-6.740375 -4.9051905 -4.2646 -4.0179844 -3.8831496 -3.9493408 -4.9180574 -5.7576938 -5.788373 -5.7071714 -5.8540249 -5.97099 -6.2074461 -6.5200405 -6.5475349]]...]
INFO - root - 2017-12-15 06:20:51.100942: step 4810, loss = 0.28, batch loss = 0.24 (35.8 examples/sec; 0.223 sec/batch; 20h:19m:43s remains)
INFO - root - 2017-12-15 06:20:53.379581: step 4820, loss = 0.35, batch loss = 0.31 (35.3 examples/sec; 0.226 sec/batch; 20h:36m:07s remains)
INFO - root - 2017-12-15 06:20:55.624424: step 4830, loss = 0.24, batch loss = 0.20 (35.6 examples/sec; 0.225 sec/batch; 20h:28m:04s remains)
INFO - root - 2017-12-15 06:20:57.923335: step 4840, loss = 0.36, batch loss = 0.32 (35.3 examples/sec; 0.227 sec/batch; 20h:39m:13s remains)
INFO - root - 2017-12-15 06:21:00.188081: step 4850, loss = 0.21, batch loss = 0.17 (34.6 examples/sec; 0.231 sec/batch; 21h:02m:35s remains)
INFO - root - 2017-12-15 06:21:02.515435: step 4860, loss = 0.28, batch loss = 0.25 (35.3 examples/sec; 0.227 sec/batch; 20h:38m:10s remains)
INFO - root - 2017-12-15 06:21:04.787473: step 4870, loss = 0.35, batch loss = 0.32 (34.9 examples/sec; 0.229 sec/batch; 20h:53m:10s remains)
INFO - root - 2017-12-15 06:21:07.046690: step 4880, loss = 0.28, batch loss = 0.24 (35.2 examples/sec; 0.227 sec/batch; 20h:39m:35s remains)
INFO - root - 2017-12-15 06:21:09.327760: step 4890, loss = 0.33, batch loss = 0.30 (34.6 examples/sec; 0.231 sec/batch; 21h:02m:36s remains)
INFO - root - 2017-12-15 06:21:11.606622: step 4900, loss = 0.29, batch loss = 0.25 (34.5 examples/sec; 0.232 sec/batch; 21h:05m:49s remains)
2017-12-15 06:21:11.882613: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7147963 -2.1949525 -2.497849 -2.4505773 -2.9476781 -3.8412504 -4.7866154 -5.5995722 -5.896698 -5.7608662 -5.5518751 -5.3719912 -5.1178989 -4.8332915 -4.6211071][-2.6771069 -2.9031515 -3.1159124 -3.044172 -3.6025612 -4.564 -5.4974809 -6.1918888 -6.5853438 -6.8770971 -6.9402094 -6.8517404 -6.6446748 -6.285358 -5.7923546][-3.2629471 -3.1666424 -3.3035555 -3.3070078 -3.909631 -4.76157 -5.4421978 -5.8172922 -6.202117 -6.8018193 -7.0265632 -7.1171794 -7.1304674 -6.8881483 -6.3293147][-3.0901523 -2.4610686 -2.5144918 -2.5490165 -3.1222377 -3.7437682 -4.0542617 -4.0961637 -4.3920565 -5.0904436 -5.3443937 -5.7692404 -6.2329206 -6.2856922 -5.818645][-1.7203176 -0.547935 -0.58164942 -0.71420574 -1.3087366 -1.8846872 -2.0655532 -1.9595411 -2.23121 -2.8156493 -2.8497567 -3.487042 -4.417696 -4.8633547 -4.6058083][-0.039313555 1.4723036 1.4181211 1.1763813 0.54391551 -0.11116672 -0.30797446 -0.12623096 -0.26663518 -0.53926492 -0.2602011 -0.91776419 -2.10897 -2.8991461 -2.8726485][0.75262833 2.3962202 2.362978 2.126873 1.5119667 0.85768127 0.71524572 1.0142899 1.0409193 0.97414327 1.3861952 0.82901859 -0.34590769 -1.2307045 -1.3136544][0.3980999 2.0533533 1.9146338 1.6797035 1.1324594 0.51253033 0.47199941 0.9383502 1.1920283 1.2564845 1.5973632 1.158035 0.16704726 -0.59539676 -0.67138791][-0.27092969 1.5032418 1.2199531 0.89483213 0.33459663 -0.36211538 -0.53689742 -0.13749313 0.22828984 0.46646047 0.80371118 0.44861007 -0.37410855 -1.0379946 -1.0538172][-0.65769064 1.3934426 1.1675541 0.80196285 0.089916706 -0.864926 -1.4409283 -1.5101357 -1.4797155 -1.4447989 -1.2000751 -1.4340285 -1.9061342 -2.2263134 -1.9824698][-0.88468015 1.5709794 1.5913622 1.3554306 0.5293777 -0.72630906 -1.7614591 -2.3737445 -2.8254614 -3.2360592 -3.3658853 -3.5966167 -3.610024 -3.4264183 -2.80474][-0.81444407 2.0963488 2.6569777 2.7915626 2.0354137 0.69603348 -0.67040384 -1.8882449 -2.8985734 -3.7046919 -4.2224741 -4.6913471 -4.5543156 -4.0987535 -3.0991044][-0.67933583 2.6900277 3.844327 4.4517684 3.8997569 2.6717429 1.2411621 -0.33517742 -1.8343897 -3.0596039 -3.8502722 -4.4327936 -4.2502584 -3.6701305 -2.3386478][-1.5831851 2.1097455 3.6399293 4.5889444 4.463201 3.7963552 2.8491974 1.5784416 0.192343 -1.1762716 -2.2044895 -2.9071758 -2.9175963 -2.5683784 -1.224269][-3.2650917 0.34341288 1.8390439 2.7742682 3.07626 3.2720323 3.2911835 2.7886376 1.9655132 0.88284016 -0.13748884 -0.919821 -1.2602258 -1.3590288 -0.25906634]]...]
INFO - root - 2017-12-15 06:21:14.157411: step 4910, loss = 0.36, batch loss = 0.32 (34.5 examples/sec; 0.232 sec/batch; 21h:05m:09s remains)
INFO - root - 2017-12-15 06:21:16.447824: step 4920, loss = 0.26, batch loss = 0.22 (34.8 examples/sec; 0.230 sec/batch; 20h:55m:37s remains)
INFO - root - 2017-12-15 06:21:18.703809: step 4930, loss = 0.27, batch loss = 0.24 (34.9 examples/sec; 0.229 sec/batch; 20h:51m:43s remains)
INFO - root - 2017-12-15 06:21:20.967203: step 4940, loss = 0.33, batch loss = 0.29 (34.6 examples/sec; 0.232 sec/batch; 21h:03m:56s remains)
INFO - root - 2017-12-15 06:21:23.244782: step 4950, loss = 0.28, batch loss = 0.24 (32.7 examples/sec; 0.245 sec/batch; 22h:16m:09s remains)
INFO - root - 2017-12-15 06:21:25.517258: step 4960, loss = 0.41, batch loss = 0.38 (36.2 examples/sec; 0.221 sec/batch; 20h:06m:28s remains)
INFO - root - 2017-12-15 06:21:27.771789: step 4970, loss = 0.39, batch loss = 0.35 (34.3 examples/sec; 0.233 sec/batch; 21h:13m:09s remains)
INFO - root - 2017-12-15 06:21:30.011634: step 4980, loss = 0.31, batch loss = 0.27 (35.3 examples/sec; 0.227 sec/batch; 20h:37m:11s remains)
INFO - root - 2017-12-15 06:21:32.311613: step 4990, loss = 0.24, batch loss = 0.20 (34.9 examples/sec; 0.229 sec/batch; 20h:51m:11s remains)
INFO - root - 2017-12-15 06:21:34.581773: step 5000, loss = 0.38, batch loss = 0.34 (35.0 examples/sec; 0.228 sec/batch; 20h:46m:18s remains)
2017-12-15 06:21:34.865204: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2957263 -4.7687564 -4.3493958 -3.4692602 -2.6224358 -1.4350215 -0.61405885 -0.61411619 -1.2933764 -2.1687851 -3.0134158 -3.5377798 -3.5520248 -3.4107957 -3.5276654][-3.5384712 -4.7646437 -4.4793262 -3.6055956 -2.7552688 -1.9343702 -1.4988222 -1.757658 -2.6021547 -3.2726841 -3.7505429 -4.2108049 -4.2507296 -3.8087883 -3.6555426][-2.6768746 -3.6769185 -3.5900624 -2.862103 -1.9910209 -1.3988583 -1.6095499 -2.5032473 -3.728833 -4.5328217 -4.885973 -5.1861954 -5.117672 -4.5662928 -4.3336697][-1.9440776 -2.9348621 -3.3784094 -3.027153 -2.0063624 -1.223605 -1.5511566 -2.6997123 -4.1553564 -5.1448669 -5.52782 -5.7688217 -5.6738195 -5.0628405 -4.7284493][-2.0951834 -3.1453145 -4.1014004 -4.0116611 -2.5942056 -1.1817605 -1.040732 -2.090071 -3.7608533 -5.0091162 -5.521132 -5.8896751 -6.0340271 -5.6865406 -5.2794676][-1.417316 -2.6089187 -3.9482732 -3.910768 -2.0610292 0.19920945 1.2341299 0.53221273 -1.5622368 -3.5743306 -4.8484221 -5.9398861 -6.7986894 -7.10529 -6.7954][-0.60018408 -1.9423105 -3.3678041 -3.1647305 -0.98093677 2.0663204 4.2372475 4.2660356 2.0406837 -0.6864295 -2.8199816 -4.6455631 -6.3603706 -7.4895539 -7.4946771][-0.89353883 -2.1794634 -3.485033 -3.1521366 -0.92679048 2.4592419 5.3563304 5.9668632 3.8664083 1.0163732 -1.2268057 -3.0404503 -4.8388987 -6.2503676 -6.5175686][-1.4869647 -2.6941743 -3.882755 -3.5640919 -1.6998713 1.3408566 4.20067 5.1099973 3.4147239 0.88745213 -1.1030748 -2.547132 -4.0894408 -5.3959136 -5.5147991][-3.0719037 -3.9910908 -4.7700005 -4.343605 -2.8449285 -0.31716716 2.2342482 3.2838659 2.0533195 -0.071475267 -1.6618416 -2.5530124 -3.6157193 -4.5357609 -4.3481407][-4.8182392 -5.3565955 -5.7362204 -5.2070751 -4.0949264 -2.2705104 -0.22683072 0.83364153 0.14781332 -1.2913516 -2.2414391 -2.3428581 -2.6369669 -3.1056113 -2.8353512][-5.4566031 -5.6181145 -5.8180242 -5.4623961 -4.887383 -3.8506708 -2.3745241 -1.3661723 -1.6381212 -2.4834898 -2.9271736 -2.5313132 -2.2514634 -2.3484349 -2.080029][-5.7879848 -5.6318083 -5.7386656 -5.4659934 -5.303658 -4.9547338 -4.0332136 -3.2132945 -3.2211561 -3.69411 -3.8921964 -3.2760568 -2.7257807 -2.5109744 -2.1054177][-6.137239 -5.7073965 -5.774384 -5.5888 -5.697176 -5.893919 -5.5562525 -5.0660353 -4.998189 -5.2079477 -5.2130442 -4.612752 -4.0292521 -3.6252992 -3.1594543][-5.8867207 -5.303895 -5.370121 -5.2186785 -5.3864059 -5.7586765 -5.729599 -5.550765 -5.6359911 -5.9184179 -6.1305842 -5.9978204 -5.7351074 -5.3582678 -4.9236679]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-5000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-5000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 06:21:37.421110: step 5010, loss = 0.49, batch loss = 0.45 (35.8 examples/sec; 0.224 sec/batch; 20h:20m:10s remains)
INFO - root - 2017-12-15 06:21:39.725976: step 5020, loss = 0.25, batch loss = 0.22 (35.1 examples/sec; 0.228 sec/batch; 20h:44m:14s remains)
INFO - root - 2017-12-15 06:21:41.954373: step 5030, loss = 0.29, batch loss = 0.25 (36.5 examples/sec; 0.219 sec/batch; 19h:56m:17s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 06:21:44.203029: step 5040, loss = 0.31, batch loss = 0.27 (35.6 examples/sec; 0.224 sec/batch; 20h:24m:52s remains)
INFO - root - 2017-12-15 06:21:46.517732: step 5050, loss = 0.35, batch loss = 0.31 (34.9 examples/sec; 0.229 sec/batch; 20h:49m:38s remains)
INFO - root - 2017-12-15 06:21:48.832970: step 5060, loss = 0.22, batch loss = 0.18 (36.0 examples/sec; 0.222 sec/batch; 20h:12m:33s remains)
INFO - root - 2017-12-15 06:21:51.085850: step 5070, loss = 0.32, batch loss = 0.28 (34.7 examples/sec; 0.230 sec/batch; 20h:57m:23s remains)
INFO - root - 2017-12-15 06:21:53.347108: step 5080, loss = 0.32, batch loss = 0.28 (35.7 examples/sec; 0.224 sec/batch; 20h:23m:09s remains)
INFO - root - 2017-12-15 06:21:55.608104: step 5090, loss = 0.37, batch loss = 0.33 (36.5 examples/sec; 0.219 sec/batch; 19h:55m:57s remains)
INFO - root - 2017-12-15 06:21:57.882726: step 5100, loss = 0.34, batch loss = 0.30 (35.4 examples/sec; 0.226 sec/batch; 20h:33m:43s remains)
2017-12-15 06:21:58.151587: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.40489709 -1.3571782 -0.84897804 -0.49132168 -0.23929226 -0.1624186 -0.62042642 -1.5624959 -2.6269503 -3.1671195 -3.16754 -2.7970829 -2.5346625 -2.9008484 -3.2988124][-0.63889766 -1.7239828 -1.5929551 -1.5060382 -1.2901804 -1.0714819 -1.1480333 -1.5488515 -2.3331356 -2.87273 -3.0114923 -2.797199 -2.6847916 -3.1470735 -3.4684467][-0.54942656 -1.7451119 -2.0776489 -2.307699 -2.1430867 -1.7103456 -1.4014837 -1.3852277 -2.0100493 -2.5372157 -2.7388229 -2.6647413 -2.6765187 -3.1471066 -3.3579378][-0.085847735 -1.2855505 -1.9245359 -2.3320966 -2.1293218 -1.4750198 -1.0695419 -1.0683059 -1.81535 -2.4856279 -2.7306693 -2.7246649 -2.6946883 -2.9448268 -2.9544725][0.26936126 -0.89775717 -1.6088486 -1.8847096 -1.3702177 -0.40132296 -0.046653271 -0.38925505 -1.5451185 -2.520787 -2.8676481 -2.9709408 -2.7631445 -2.6079376 -2.3746738][0.31763124 -0.84645927 -1.5308614 -1.5015583 -0.52878976 0.895494 1.2833321 0.620409 -0.90101457 -2.3072753 -3.0142603 -3.3478279 -2.9942465 -2.5068617 -2.06159][0.15569711 -1.2960148 -1.9121891 -1.4959705 0.0064616203 1.9150374 2.5237863 1.8217523 0.14154935 -1.7630692 -3.0926814 -3.7998502 -3.4638836 -2.78205 -2.1929564][-0.42014086 -1.984429 -2.4528704 -1.6131942 0.32048917 2.5871389 3.4879143 2.8083284 1.0031331 -1.3040612 -3.1132276 -4.0708847 -3.8595295 -3.1126914 -2.4150875][-1.0265896 -2.5556297 -2.8555393 -1.7222023 0.49151731 2.9133966 3.9330533 3.1963112 1.3375773 -1.0872917 -3.0628498 -4.0886517 -3.9991944 -3.2484818 -2.4967918][-1.477109 -2.8000107 -2.96352 -1.7114677 0.44596243 2.6298468 3.4387109 2.5301526 0.73585486 -1.4423528 -3.1649847 -4.0071273 -3.9688148 -3.2988377 -2.5815451][-1.8044167 -2.7865412 -2.906194 -1.8122252 -0.090812325 1.4485662 1.7652709 0.82465434 -0.54307663 -2.1510119 -3.413837 -3.9873323 -3.9508939 -3.4199061 -2.8352742][-2.1290562 -2.6350245 -2.7809405 -2.0199361 -0.88685358 -0.060432434 -0.24752009 -1.1334454 -2.0818238 -3.1815429 -3.9721556 -4.2198443 -4.0744085 -3.6333351 -3.1954737][-2.534399 -2.5238972 -2.6374588 -2.0925243 -1.3484178 -1.0851345 -1.6615918 -2.4491708 -3.1082156 -3.8633811 -4.325036 -4.3158865 -4.120204 -3.7791529 -3.4645066][-2.8870447 -2.4051557 -2.3744953 -1.8740495 -1.3050057 -1.365325 -2.1114545 -2.7138119 -3.2277534 -3.8253989 -4.110024 -3.9162378 -3.7425628 -3.5548701 -3.378262][-2.8601604 -1.9475653 -1.639681 -1.0929339 -0.67316258 -0.99653769 -1.8493942 -2.363086 -2.8198302 -3.3229551 -3.4825315 -3.2432904 -3.2206407 -3.1836743 -3.0779538]]...]
INFO - root - 2017-12-15 06:22:00.426249: step 5110, loss = 0.38, batch loss = 0.35 (35.6 examples/sec; 0.225 sec/batch; 20h:26m:20s remains)
INFO - root - 2017-12-15 06:22:02.721052: step 5120, loss = 0.28, batch loss = 0.24 (34.6 examples/sec; 0.231 sec/batch; 21h:01m:12s remains)
INFO - root - 2017-12-15 06:22:05.014410: step 5130, loss = 0.45, batch loss = 0.41 (34.9 examples/sec; 0.229 sec/batch; 20h:50m:18s remains)
INFO - root - 2017-12-15 06:22:07.296315: step 5140, loss = 0.20, batch loss = 0.17 (36.3 examples/sec; 0.220 sec/batch; 20h:01m:45s remains)
INFO - root - 2017-12-15 06:22:09.612102: step 5150, loss = 0.20, batch loss = 0.17 (36.2 examples/sec; 0.221 sec/batch; 20h:06m:00s remains)
INFO - root - 2017-12-15 06:22:11.881680: step 5160, loss = 0.30, batch loss = 0.26 (35.2 examples/sec; 0.227 sec/batch; 20h:39m:34s remains)
INFO - root - 2017-12-15 06:22:14.130557: step 5170, loss = 0.38, batch loss = 0.34 (36.7 examples/sec; 0.218 sec/batch; 19h:47m:57s remains)
INFO - root - 2017-12-15 06:22:16.427308: step 5180, loss = 0.21, batch loss = 0.17 (35.2 examples/sec; 0.227 sec/batch; 20h:40m:17s remains)
INFO - root - 2017-12-15 06:22:18.694110: step 5190, loss = 0.30, batch loss = 0.26 (35.7 examples/sec; 0.224 sec/batch; 20h:23m:59s remains)
INFO - root - 2017-12-15 06:22:20.959070: step 5200, loss = 0.29, batch loss = 0.25 (35.2 examples/sec; 0.227 sec/batch; 20h:39m:28s remains)
2017-12-15 06:22:21.260655: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4264438 -2.1514568 -2.4081523 -2.7627723 -3.6256402 -4.4351707 -4.1762543 -3.0349813 -1.8168068 -0.98218942 -0.68237853 -1.2632723 -2.0223103 -2.1511238 -1.7922819][-1.9694003 -2.3529024 -2.6464543 -3.0516095 -4.0362892 -5.001862 -4.8203049 -3.6379247 -2.5037553 -1.5814893 -1.0981765 -1.3819232 -1.7178738 -1.4925766 -1.0539271][-2.3808787 -2.6142547 -2.9628019 -3.3416219 -4.1391673 -4.800622 -4.4055762 -3.1950135 -2.3049328 -1.6600873 -1.4030854 -1.7232275 -1.8703911 -1.40294 -0.84332407][-3.0262568 -2.8754284 -3.1705086 -3.3864129 -3.8186703 -4.0342603 -3.3985643 -2.1758161 -1.5361824 -1.2747772 -1.3647544 -1.8303385 -1.9231113 -1.4254024 -0.85209894][-2.9329033 -2.7201581 -2.8298552 -2.836463 -2.9706879 -2.9693859 -2.354646 -1.2266753 -0.66199672 -0.64615583 -0.98457229 -1.6215045 -1.7681358 -1.4135802 -0.9295038][-2.8602509 -2.4820294 -2.3743517 -2.0979338 -1.8600321 -1.6429694 -1.1288254 -0.17596984 0.34974933 0.27419686 -0.10936785 -0.71838903 -0.920683 -0.73292267 -0.44222283][-2.6678784 -2.0822947 -1.5183555 -0.83315122 -0.21670115 0.1848495 0.61768246 1.2280352 1.4617815 1.2459562 0.83423352 0.36946273 0.17245674 0.21412516 0.3653574][-3.0702925 -2.1933532 -1.1646248 -0.057275772 0.87370896 1.4747143 1.9241896 2.3570855 2.2590339 1.8054113 1.4059517 1.1444709 0.96909332 0.91297078 0.95871735][-3.7194588 -2.8730693 -1.699518 -0.32183075 0.77124047 1.3907883 1.82827 2.18689 2.0053852 1.5278373 1.127212 0.94250321 0.72503734 0.59794784 0.68990469][-4.0709949 -3.3876925 -2.40583 -1.162554 -0.18174314 0.29260421 0.54488182 0.72892475 0.46161008 0.032264709 -0.31754148 -0.34626257 -0.3565768 -0.26422906 0.047101736][-4.294198 -3.6803668 -2.8970184 -1.8241501 -1.0934898 -0.87606311 -0.872828 -0.91905725 -1.3042285 -1.7192261 -2.0333323 -1.8914319 -1.5981238 -1.2026427 -0.77648938][-4.3603144 -3.705843 -2.9566169 -1.9828193 -1.5641558 -1.6667056 -1.9478447 -2.2247784 -2.6904855 -3.1170244 -3.4451268 -3.2993658 -2.8482704 -2.1623564 -1.530776][-4.7445168 -4.0776358 -3.2563474 -2.2040551 -1.9460741 -2.3108935 -2.7623832 -3.13417 -3.5313787 -3.8874536 -4.1786308 -4.0540195 -3.5178039 -2.6290088 -1.9179703][-5.0633388 -4.3431244 -3.5391884 -2.5935683 -2.4937685 -2.9983079 -3.448216 -3.7765522 -4.1126952 -4.4251566 -4.6712503 -4.5565825 -4.0759859 -3.2818027 -2.7304909][-5.2556143 -4.4408817 -3.7842469 -3.0512636 -2.9318204 -3.2847493 -3.4948373 -3.6993518 -3.9904375 -4.2566671 -4.4344192 -4.3571472 -4.0238786 -3.458539 -3.1203628]]...]
INFO - root - 2017-12-15 06:22:23.526621: step 5210, loss = 0.35, batch loss = 0.31 (35.7 examples/sec; 0.224 sec/batch; 20h:21m:31s remains)
INFO - root - 2017-12-15 06:22:25.784036: step 5220, loss = 0.34, batch loss = 0.30 (35.3 examples/sec; 0.227 sec/batch; 20h:36m:59s remains)
INFO - root - 2017-12-15 06:22:28.067202: step 5230, loss = 0.22, batch loss = 0.18 (35.8 examples/sec; 0.223 sec/batch; 20h:17m:25s remains)
INFO - root - 2017-12-15 06:22:30.326959: step 5240, loss = 0.27, batch loss = 0.23 (34.9 examples/sec; 0.229 sec/batch; 20h:50m:54s remains)
INFO - root - 2017-12-15 06:22:32.596878: step 5250, loss = 0.29, batch loss = 0.25 (35.5 examples/sec; 0.225 sec/batch; 20h:28m:24s remains)
INFO - root - 2017-12-15 06:22:34.855026: step 5260, loss = 0.30, batch loss = 0.27 (36.1 examples/sec; 0.222 sec/batch; 20h:09m:50s remains)
INFO - root - 2017-12-15 06:22:37.124201: step 5270, loss = 0.32, batch loss = 0.29 (35.5 examples/sec; 0.225 sec/batch; 20h:27m:40s remains)
INFO - root - 2017-12-15 06:22:39.417100: step 5280, loss = 0.26, batch loss = 0.22 (33.7 examples/sec; 0.237 sec/batch; 21h:32m:48s remains)
INFO - root - 2017-12-15 06:22:41.696304: step 5290, loss = 0.22, batch loss = 0.18 (36.0 examples/sec; 0.222 sec/batch; 20h:13m:07s remains)
INFO - root - 2017-12-15 06:22:43.985593: step 5300, loss = 0.33, batch loss = 0.29 (35.1 examples/sec; 0.228 sec/batch; 20h:42m:07s remains)
2017-12-15 06:22:44.280707: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1675158 -5.9535456 -5.8140774 -5.4737759 -5.4204445 -5.46385 -5.2300811 -4.8855267 -4.7178411 -4.9198503 -5.1718979 -5.631608 -6.2379446 -6.4097157 -6.1724811][-6.6210446 -6.2479825 -5.9627557 -5.5277576 -5.5839458 -5.8366361 -5.5507555 -5.0700264 -4.8592057 -5.0868845 -5.36652 -6.0280557 -6.9161453 -7.158833 -6.9329424][-6.6841769 -5.5041275 -5.0188761 -4.4104748 -4.4855957 -4.9412785 -4.6929808 -4.1769533 -4.048408 -4.3629866 -4.6650214 -5.46446 -6.51665 -6.8409715 -6.7090764][-5.8246574 -3.8710213 -3.203373 -2.51952 -2.6221528 -3.284272 -3.15521 -2.7793856 -2.9195714 -3.4572182 -3.8827546 -4.7746563 -5.8915925 -6.2655454 -6.2061038][-4.6401014 -1.9954711 -1.3524728 -0.70293605 -0.77087033 -1.5125713 -1.4029834 -1.0700378 -1.4471774 -2.2470505 -2.9068632 -3.9802716 -5.1926174 -5.6553278 -5.6995611][-2.9653752 0.070514441 0.65128446 1.2293034 1.2266052 0.55001545 0.76545548 1.2020867 0.65854764 -0.44282222 -1.4524004 -2.7662506 -4.1403332 -4.8080935 -5.0619507][-1.2826512 1.5893393 2.2136736 2.7837563 2.8604341 2.338695 2.7320709 3.3616219 2.8026624 1.5330942 0.25784874 -1.2519093 -2.7624729 -3.7396572 -4.2375011][-1.0143887 1.5828412 2.3226876 2.9739752 3.249218 3.0959263 3.8295078 4.7413225 4.2848139 2.999392 1.5617115 -0.051288605 -1.6875594 -2.9053202 -3.6078598][-2.3717804 0.1451838 0.99402475 1.7392023 2.1665816 2.3086038 3.2757044 4.3504105 4.0988712 3.0242972 1.6217723 0.012281179 -1.6442063 -2.9120653 -3.6089206][-4.5950527 -2.1816196 -1.290398 -0.575343 -0.31760979 -0.16410315 0.74456882 1.8181407 1.8293035 1.1246178 -0.014838457 -1.3720422 -2.7790725 -3.8238745 -4.2873168][-6.6501241 -4.3689651 -3.5258102 -3.0104077 -3.0876818 -3.0972877 -2.4255605 -1.5341148 -1.3662504 -1.7377204 -2.5076892 -3.4604812 -4.4370441 -5.0593462 -5.1198826][-7.8651223 -5.8360643 -5.1521206 -4.8430076 -5.1520061 -5.279767 -4.8730021 -4.2510748 -4.0533991 -4.2134233 -4.6734319 -5.2330785 -5.7875447 -5.9546518 -5.6276169][-8.2565956 -6.46539 -5.9160986 -5.7149839 -6.0723515 -6.2487082 -6.0832939 -5.7354183 -5.5812492 -5.6484928 -5.90164 -6.1413026 -6.3396544 -6.2057161 -5.6599631][-7.5447021 -5.9628563 -5.6003389 -5.5310793 -5.8974552 -6.104908 -6.0994387 -5.9358444 -5.860961 -5.927372 -6.0709925 -6.144475 -6.1633205 -5.932106 -5.3691154][-6.1166687 -4.7743106 -4.5457182 -4.5717959 -4.8974533 -5.0957475 -5.1524105 -5.1011391 -5.1163483 -5.2251987 -5.3521194 -5.4283562 -5.4619827 -5.303082 -4.8857288]]...]
INFO - root - 2017-12-15 06:22:46.503873: step 5310, loss = 0.42, batch loss = 0.39 (35.3 examples/sec; 0.227 sec/batch; 20h:36m:05s remains)
INFO - root - 2017-12-15 06:22:48.759933: step 5320, loss = 0.26, batch loss = 0.22 (35.1 examples/sec; 0.228 sec/batch; 20h:43m:06s remains)
INFO - root - 2017-12-15 06:22:51.025296: step 5330, loss = 0.40, batch loss = 0.36 (34.5 examples/sec; 0.232 sec/batch; 21h:06m:06s remains)
INFO - root - 2017-12-15 06:22:53.282774: step 5340, loss = 0.24, batch loss = 0.20 (35.8 examples/sec; 0.223 sec/batch; 20h:18m:20s remains)
INFO - root - 2017-12-15 06:22:55.530880: step 5350, loss = 0.25, batch loss = 0.21 (35.8 examples/sec; 0.223 sec/batch; 20h:17m:38s remains)
INFO - root - 2017-12-15 06:22:57.821133: step 5360, loss = 0.26, batch loss = 0.23 (34.6 examples/sec; 0.231 sec/batch; 21h:01m:56s remains)
INFO - root - 2017-12-15 06:23:00.068482: step 5370, loss = 0.22, batch loss = 0.18 (36.2 examples/sec; 0.221 sec/batch; 20h:06m:07s remains)
INFO - root - 2017-12-15 06:23:02.314419: step 5380, loss = 0.27, batch loss = 0.23 (36.0 examples/sec; 0.223 sec/batch; 20h:13m:13s remains)
INFO - root - 2017-12-15 06:23:04.586182: step 5390, loss = 0.29, batch loss = 0.25 (36.2 examples/sec; 0.221 sec/batch; 20h:04m:32s remains)
INFO - root - 2017-12-15 06:23:06.826273: step 5400, loss = 0.23, batch loss = 0.19 (35.4 examples/sec; 0.226 sec/batch; 20h:32m:18s remains)
2017-12-15 06:23:07.122579: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2534614 -5.9814172 -5.3309908 -4.6489906 -4.4128823 -4.6043844 -4.8882442 -4.9685183 -4.8869925 -4.8672233 -5.1924577 -5.9066381 -7.0479288 -7.3920479 -6.9601879][-7.098918 -7.9334259 -7.2095108 -6.2902966 -5.8962708 -6.1131344 -6.4827652 -6.8678389 -7.1364441 -7.1721973 -7.4589882 -8.152359 -9.4028673 -9.5323648 -8.5743694][-7.8607359 -7.9235506 -7.1445274 -6.1364965 -5.6861668 -5.8850765 -6.2664576 -6.9819508 -7.7293329 -7.9702473 -8.2504511 -8.79929 -10.02314 -10.130633 -9.0117979][-8.0574837 -7.2450018 -6.4959774 -5.4381342 -4.8071413 -4.7132883 -4.8820677 -5.8547478 -7.1415825 -7.7757816 -8.1009073 -8.5460434 -9.73337 -9.9691906 -9.0466595][-8.5912666 -6.75702 -5.8164196 -4.3785806 -3.1203129 -2.2901762 -1.8504837 -2.7432513 -4.3781443 -5.598495 -6.3770237 -7.2067184 -8.6464767 -9.2610025 -8.7142305][-7.5490704 -5.25595 -4.2603822 -2.5651248 -0.63830137 1.0321252 2.1344001 1.7215555 0.21955562 -1.4374461 -2.7741942 -4.2087908 -5.98475 -7.099185 -7.2120256][-5.6165771 -3.5972691 -2.5265553 -0.58963072 1.9336765 4.44343 6.0624771 6.1405735 5.0377026 3.0409467 0.91427445 -1.4328299 -3.8401165 -5.5187855 -6.2311897][-5.2323637 -3.2945073 -1.9617035 0.364475 3.5373333 6.8539562 8.9723406 9.6344023 8.942338 6.4877729 3.3487875 -0.097378731 -3.3959224 -5.5444078 -6.4457235][-6.1445227 -4.5437918 -3.2139964 -1.0065267 2.1611702 5.5681286 7.7333317 8.8032265 8.6346083 6.3100252 2.9273274 -0.77958155 -4.241538 -6.2032194 -6.7166862][-8.675909 -7.88789 -7.1262903 -5.5990763 -3.0490572 0.043079376 2.120878 3.476054 3.9573519 2.4825628 -0.2253207 -3.2125807 -5.9883518 -7.1951752 -6.9933929][-10.541399 -10.067155 -9.6560717 -8.7794237 -6.9287262 -4.4284554 -2.6652248 -1.3948691 -0.70717525 -1.4580671 -3.4540548 -5.6119194 -7.6182594 -8.1286974 -7.2546225][-10.98415 -10.434596 -10.204394 -9.8714056 -8.8175211 -7.1290321 -5.9482069 -5.1869259 -4.680428 -4.9427118 -6.2060223 -7.5676956 -8.936636 -9.1163349 -7.9647722][-11.406001 -10.854422 -10.857218 -10.883448 -10.384502 -9.2748833 -8.521946 -8.2625618 -8.0571671 -7.9282866 -8.3797255 -8.9981451 -9.83127 -9.9149961 -8.904995][-11.373361 -10.736034 -10.727129 -10.759345 -10.461419 -9.6444979 -9.1375265 -9.2305965 -9.3735294 -9.2110586 -9.2558718 -9.4214506 -9.7780151 -9.6858711 -8.8379707][-9.5188913 -8.7540836 -8.77876 -8.85481 -8.6725979 -8.0999985 -7.7693958 -7.9989967 -8.2867727 -8.2362909 -8.1687222 -8.0506277 -8.02974 -7.8937025 -7.4019203]]...]
INFO - root - 2017-12-15 06:23:09.400784: step 5410, loss = 0.30, batch loss = 0.26 (34.7 examples/sec; 0.230 sec/batch; 20h:55m:35s remains)
INFO - root - 2017-12-15 06:23:11.679824: step 5420, loss = 0.23, batch loss = 0.19 (35.7 examples/sec; 0.224 sec/batch; 20h:21m:21s remains)
INFO - root - 2017-12-15 06:23:13.968800: step 5430, loss = 0.21, batch loss = 0.17 (35.9 examples/sec; 0.223 sec/batch; 20h:13m:35s remains)
INFO - root - 2017-12-15 06:23:16.234481: step 5440, loss = 0.30, batch loss = 0.26 (35.8 examples/sec; 0.223 sec/batch; 20h:17m:50s remains)
INFO - root - 2017-12-15 06:23:18.492282: step 5450, loss = 0.22, batch loss = 0.18 (36.4 examples/sec; 0.220 sec/batch; 19h:57m:42s remains)
INFO - root - 2017-12-15 06:23:20.778260: step 5460, loss = 0.24, batch loss = 0.21 (34.6 examples/sec; 0.231 sec/batch; 20h:59m:13s remains)
INFO - root - 2017-12-15 06:23:23.022190: step 5470, loss = 0.29, batch loss = 0.25 (35.7 examples/sec; 0.224 sec/batch; 20h:20m:42s remains)
INFO - root - 2017-12-15 06:23:25.265357: step 5480, loss = 0.29, batch loss = 0.25 (35.5 examples/sec; 0.225 sec/batch; 20h:27m:09s remains)
INFO - root - 2017-12-15 06:23:27.520090: step 5490, loss = 0.26, batch loss = 0.23 (35.1 examples/sec; 0.228 sec/batch; 20h:42m:08s remains)
INFO - root - 2017-12-15 06:23:29.817565: step 5500, loss = 0.32, batch loss = 0.29 (34.7 examples/sec; 0.231 sec/batch; 20h:57m:59s remains)
2017-12-15 06:23:30.113759: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3479977 -5.2534838 -4.6295605 -4.0174856 -3.6405582 -3.68782 -3.6111698 -3.7179158 -3.9211802 -4.3205838 -4.5680685 -3.9334302 -2.634043 -1.2649245 -0.61603713][-4.6122847 -5.0811329 -4.482235 -3.9506793 -3.8225389 -4.1002693 -4.0881972 -4.2028017 -4.4800158 -4.9431028 -5.2389784 -4.6631269 -3.5182939 -2.3367348 -1.7770814][-4.524354 -4.3962736 -3.8067179 -3.3960333 -3.591187 -4.1856174 -4.3464403 -4.49685 -4.8518453 -5.4014664 -5.7326956 -5.2038183 -4.2850418 -3.4707026 -3.1234679][-4.5832138 -3.8472862 -3.2508879 -2.9052322 -3.2735 -3.9656851 -4.1019711 -4.1228619 -4.4583321 -5.1531954 -5.4991465 -4.8735776 -4.1473141 -3.8187037 -3.9001758][-4.8530006 -3.4523416 -2.818398 -2.4050503 -2.669909 -3.1836276 -3.0334647 -2.8238554 -3.3114498 -4.4179912 -4.9219246 -4.1199589 -3.3381834 -3.1846471 -3.5302863][-4.3366737 -2.692476 -2.1333289 -1.6123267 -1.5249484 -1.5271013 -0.77455723 -0.3136667 -1.2643338 -3.1451032 -4.0979552 -3.3205156 -2.3998592 -2.070076 -2.3188562][-3.5662303 -2.01011 -1.5248775 -0.91860342 -0.40107906 0.3306644 1.9941752 2.8288753 1.2967083 -1.5821419 -3.2160697 -2.7951887 -1.9137214 -1.3398213 -1.248096][-2.7242284 -1.2836692 -0.94301784 -0.32162666 0.47705913 1.8545797 4.3351583 5.5069351 3.5460813 0.021277189 -2.1066875 -2.1545877 -1.4856449 -0.94776857 -0.78705132][-2.1408756 -1.0535718 -0.93107641 -0.33131611 0.53948689 2.2002957 4.94059 6.2002449 4.1710911 0.58794522 -1.6379657 -2.0111346 -1.5759372 -1.2403791 -1.2413661][-2.1685491 -1.7426294 -1.9901799 -1.5301981 -0.77643669 0.79554844 3.3943231 4.772933 3.2662423 0.2466929 -1.6287262 -2.0556698 -1.8123891 -1.8270496 -2.1332335][-2.5416126 -2.8634183 -3.6691859 -3.4781919 -2.9733689 -1.6715114 0.49900889 1.9284713 1.2767351 -0.64079726 -1.7684691 -1.9908093 -1.9300917 -2.344723 -2.9521546][-3.0648203 -4.188488 -5.6798592 -5.9314771 -5.7759094 -4.9114504 -3.3393085 -1.9462806 -1.751943 -2.3997989 -2.4879775 -2.2151883 -2.2421196 -2.8293617 -3.4777293][-3.8932095 -5.4042215 -7.1060743 -7.6031504 -7.7139978 -7.2571545 -6.070385 -4.623353 -3.7806597 -3.4127624 -2.6995301 -2.1746504 -2.3676722 -3.10682 -3.7552357][-4.7196107 -5.88175 -7.2176228 -7.69116 -7.8862219 -7.6444216 -6.6802673 -5.2643375 -4.1477032 -3.2878332 -2.266813 -1.8497005 -2.3855371 -3.3641315 -4.1016245][-5.1331425 -5.7822566 -6.6669827 -7.0860548 -7.314106 -7.214488 -6.4827681 -5.3299227 -4.2468314 -3.2320738 -2.2630098 -2.1500731 -3.0254984 -4.1582527 -4.9332094]]...]
INFO - root - 2017-12-15 06:23:32.369011: step 5510, loss = 0.40, batch loss = 0.37 (34.6 examples/sec; 0.231 sec/batch; 21h:00m:43s remains)
INFO - root - 2017-12-15 06:23:34.627852: step 5520, loss = 0.25, batch loss = 0.21 (35.2 examples/sec; 0.227 sec/batch; 20h:39m:05s remains)
INFO - root - 2017-12-15 06:23:36.930032: step 5530, loss = 0.33, batch loss = 0.29 (35.2 examples/sec; 0.228 sec/batch; 20h:40m:03s remains)
INFO - root - 2017-12-15 06:23:39.211069: step 5540, loss = 0.25, batch loss = 0.22 (32.7 examples/sec; 0.244 sec/batch; 22h:11m:18s remains)
INFO - root - 2017-12-15 06:23:41.514062: step 5550, loss = 0.32, batch loss = 0.29 (33.0 examples/sec; 0.242 sec/batch; 21h:59m:43s remains)
INFO - root - 2017-12-15 06:23:43.818951: step 5560, loss = 0.30, batch loss = 0.26 (35.0 examples/sec; 0.229 sec/batch; 20h:46m:02s remains)
INFO - root - 2017-12-15 06:23:46.123055: step 5570, loss = 0.26, batch loss = 0.23 (35.2 examples/sec; 0.227 sec/batch; 20h:39m:15s remains)
INFO - root - 2017-12-15 06:23:48.368907: step 5580, loss = 0.36, batch loss = 0.32 (35.4 examples/sec; 0.226 sec/batch; 20h:32m:37s remains)
INFO - root - 2017-12-15 06:23:50.646587: step 5590, loss = 0.31, batch loss = 0.27 (35.0 examples/sec; 0.228 sec/batch; 20h:44m:01s remains)
INFO - root - 2017-12-15 06:23:53.144110: step 5600, loss = 0.30, batch loss = 0.26 (34.3 examples/sec; 0.233 sec/batch; 21h:09m:40s remains)
2017-12-15 06:23:53.451491: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8095021 -3.3795457 -3.4334562 -3.572618 -3.7502887 -3.9586525 -4.01904 -4.05558 -4.2550478 -4.5670452 -4.8191519 -4.7848339 -4.6478224 -4.5419016 -4.2040472][-3.3902059 -3.5297372 -3.6210675 -4.0127268 -4.3462224 -4.5386143 -4.4796238 -4.3458862 -4.43182 -4.7808371 -5.188674 -5.2222462 -5.128015 -5.0998592 -4.7058282][-4.6499333 -3.3755679 -3.6448257 -4.3378687 -4.7432594 -4.8249598 -4.6796637 -4.3564668 -4.1845217 -4.4510684 -4.9447165 -5.0423007 -5.0386338 -5.1149654 -4.6786289][-5.5042672 -3.2128277 -3.5985596 -4.3383474 -4.5214829 -4.3062396 -4.1065493 -3.6736689 -3.3004327 -3.6430054 -4.3620858 -4.6704621 -4.8328929 -5.0005441 -4.4998093][-5.7216091 -2.7395065 -3.1611371 -3.7508063 -3.6057091 -3.004699 -2.7099614 -2.226068 -1.8433368 -2.4252315 -3.3833768 -4.0161791 -4.4655457 -4.7240062 -4.1549425][-4.811553 -1.8280911 -2.1970694 -2.5652549 -2.1137712 -1.0380604 -0.36384642 0.31248379 0.58054972 -0.33973193 -1.5461724 -2.560883 -3.4449069 -3.9159203 -3.3966556][-3.5238581 -1.0257142 -1.3602567 -1.5461677 -0.81632006 0.69658184 1.9199362 2.9744749 3.1839571 1.9256506 0.44241333 -1.0369204 -2.3204865 -2.9436326 -2.5450115][-3.4293442 -0.83849955 -1.1364796 -1.1714449 -0.18802559 1.7072921 3.4196925 4.8934989 5.1506281 3.5899329 1.8675008 0.033278942 -1.5314488 -2.1683185 -1.8585587][-3.7683716 -1.3779888 -1.7527454 -1.8208919 -0.79942882 1.1479831 3.0100598 4.7041831 5.0438066 3.3622913 1.5781238 -0.17350316 -1.6371371 -2.054219 -1.6258943][-4.5565114 -2.3668122 -2.9360213 -3.2748175 -2.4591284 -0.82617462 0.82409096 2.371758 2.687665 1.1337726 -0.3788594 -1.6825104 -2.7600596 -2.8317378 -2.1042047][-5.5459118 -3.5197358 -4.2762952 -4.9756794 -4.5320053 -3.3312924 -2.0899382 -0.97805786 -0.69083107 -1.9179723 -3.0749447 -3.8271852 -4.4169354 -4.1196747 -3.1439528][-6.244216 -4.4138432 -5.2843151 -6.2709594 -6.2645168 -5.5438385 -4.7837386 -4.1875629 -3.8559675 -4.5327582 -5.3604 -5.7274423 -5.9570255 -5.4368734 -4.3950462][-6.4207468 -4.8154283 -5.676899 -6.7311935 -7.04765 -6.7693682 -6.393786 -6.0862932 -5.6838436 -5.869442 -6.3700557 -6.6789441 -6.7745538 -6.2097812 -5.3182478][-6.0293822 -4.5940843 -5.3288755 -6.2528787 -6.6757045 -6.6242042 -6.4500866 -6.270093 -5.8818364 -5.8152676 -6.132679 -6.462101 -6.5264077 -6.104043 -5.6266193][-5.4559646 -4.1359358 -4.6448846 -5.3084478 -5.66029 -5.66047 -5.5274253 -5.382103 -5.1214161 -5.0366468 -5.2547617 -5.5309234 -5.616477 -5.4895048 -5.4236217]]...]
INFO - root - 2017-12-15 06:23:55.754697: step 5610, loss = 0.28, batch loss = 0.24 (35.3 examples/sec; 0.226 sec/batch; 20h:33m:12s remains)
INFO - root - 2017-12-15 06:23:58.045895: step 5620, loss = 0.31, batch loss = 0.27 (33.0 examples/sec; 0.242 sec/batch; 22h:00m:03s remains)
INFO - root - 2017-12-15 06:24:00.294624: step 5630, loss = 0.40, batch loss = 0.36 (34.6 examples/sec; 0.231 sec/batch; 20h:58m:41s remains)
INFO - root - 2017-12-15 06:24:02.575470: step 5640, loss = 0.23, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 20h:15m:21s remains)
INFO - root - 2017-12-15 06:24:04.849088: step 5650, loss = 0.25, batch loss = 0.22 (35.3 examples/sec; 0.227 sec/batch; 20h:33m:54s remains)
INFO - root - 2017-12-15 06:24:07.140484: step 5660, loss = 0.37, batch loss = 0.34 (35.5 examples/sec; 0.225 sec/batch; 20h:26m:21s remains)
INFO - root - 2017-12-15 06:24:09.434354: step 5670, loss = 0.23, batch loss = 0.19 (35.1 examples/sec; 0.228 sec/batch; 20h:40m:44s remains)
INFO - root - 2017-12-15 06:24:11.719122: step 5680, loss = 0.23, batch loss = 0.19 (35.7 examples/sec; 0.224 sec/batch; 20h:20m:18s remains)
INFO - root - 2017-12-15 06:24:13.980864: step 5690, loss = 0.23, batch loss = 0.19 (35.4 examples/sec; 0.226 sec/batch; 20h:30m:28s remains)
INFO - root - 2017-12-15 06:24:16.243768: step 5700, loss = 0.24, batch loss = 0.20 (35.1 examples/sec; 0.228 sec/batch; 20h:41m:53s remains)
2017-12-15 06:24:16.542429: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.18925691 -0.252254 -0.24047732 -0.02714467 0.13515258 0.12742519 0.41732955 0.93128753 1.0283372 0.765903 0.44333124 0.24237871 0.21072698 0.28465223 1.0217135][-0.66934073 -0.34880102 -0.40831161 -0.09763658 0.23725915 0.16163039 0.3045969 0.81469011 1.0355477 0.88213468 0.44498372 0.10692215 0.01714015 0.12096381 1.0066471][-1.06349 -0.60424531 -0.83321738 -0.49558413 -0.053676605 -0.15894806 -0.062769651 0.48879576 0.86989975 0.88940263 0.48195386 0.12119865 -0.0051307678 -0.05197382 0.63185239][-1.7226069 -0.81449676 -1.2196046 -0.88233685 -0.43045223 -0.48246229 -0.367401 0.12709832 0.54661179 0.66059566 0.31435657 -0.090652108 -0.262123 -0.35753548 0.074818611][-2.3988023 -1.112927 -1.7245466 -1.465606 -1.0987017 -1.1543407 -0.97789121 -0.53944862 -0.158247 -0.072676659 -0.39424479 -0.75703633 -0.92364216 -1.0000299 -0.77436686][-2.7605233 -1.4423118 -2.1813934 -2.0334785 -1.8051881 -1.8826416 -1.7139466 -1.3319654 -1.0172231 -1.0032864 -1.2290843 -1.4948297 -1.5403194 -1.548708 -1.5200619][-2.4869099 -1.511151 -2.2274733 -2.1500874 -2.0589671 -2.1247621 -1.8443671 -1.4375558 -1.1223021 -1.1230119 -1.3151665 -1.6306112 -1.5782766 -1.6352861 -1.8494294][-2.4465833 -1.2372582 -1.8477405 -1.8552709 -1.8796777 -1.8857961 -1.4444462 -0.90591013 -0.54875255 -0.57003677 -0.81716132 -1.3432174 -1.3477479 -1.524966 -1.8849508][-2.3549554 -0.88999569 -1.3343062 -1.3389174 -1.3401477 -1.2746154 -0.78016567 -0.17731118 0.16096497 0.094957352 -0.1767621 -0.85797834 -0.95103753 -1.2382314 -1.6886876][-2.256988 -0.57817054 -0.9214406 -0.9004612 -0.88578939 -0.83291161 -0.37429142 0.27437353 0.58673191 0.51553512 0.22910452 -0.55774009 -0.68173575 -0.93850625 -1.3591113][-2.1594896 -0.33207095 -0.64751303 -0.66425455 -0.72538996 -0.77492905 -0.39195836 0.27451897 0.52706027 0.41582608 0.14187479 -0.55748713 -0.56073272 -0.68919539 -1.058955][-2.0511801 -0.21118951 -0.59420669 -0.68758893 -0.785308 -0.93118179 -0.62988245 0.042711735 0.20965052 0.037310362 -0.19383311 -0.67019784 -0.56952071 -0.5852927 -0.91902065][-2.0565708 -0.32652581 -0.84901893 -1.026692 -1.1224215 -1.3114424 -1.072511 -0.37406933 -0.24105251 -0.46099484 -0.673951 -0.92033875 -0.70826149 -0.62207162 -0.96985126][-2.1688213 -0.55762374 -1.1918383 -1.4454982 -1.5417941 -1.757874 -1.558902 -0.839502 -0.744472 -1.0705717 -1.2464144 -1.1942818 -0.87340474 -0.76967931 -1.1350332][-2.252202 -0.7442162 -1.4703832 -1.7887288 -1.8396107 -2.0168 -1.8346992 -1.1208403 -1.0710523 -1.4926579 -1.6174637 -1.3574915 -1.0222323 -0.9629997 -1.2955447]]...]
INFO - root - 2017-12-15 06:24:18.778458: step 5710, loss = 0.24, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 20h:21m:39s remains)
INFO - root - 2017-12-15 06:24:21.041303: step 5720, loss = 0.32, batch loss = 0.28 (35.5 examples/sec; 0.225 sec/batch; 20h:25m:42s remains)
INFO - root - 2017-12-15 06:24:23.305977: step 5730, loss = 0.20, batch loss = 0.16 (32.9 examples/sec; 0.243 sec/batch; 22h:06m:03s remains)
INFO - root - 2017-12-15 06:24:25.609771: step 5740, loss = 0.32, batch loss = 0.28 (35.9 examples/sec; 0.223 sec/batch; 20h:13m:38s remains)
INFO - root - 2017-12-15 06:24:27.855407: step 5750, loss = 0.35, batch loss = 0.31 (36.1 examples/sec; 0.222 sec/batch; 20h:08m:12s remains)
INFO - root - 2017-12-15 06:24:30.136157: step 5760, loss = 0.34, batch loss = 0.30 (35.1 examples/sec; 0.228 sec/batch; 20h:40m:21s remains)
INFO - root - 2017-12-15 06:24:32.412965: step 5770, loss = 0.32, batch loss = 0.28 (34.8 examples/sec; 0.230 sec/batch; 20h:50m:25s remains)
INFO - root - 2017-12-15 06:24:34.676865: step 5780, loss = 0.30, batch loss = 0.27 (35.4 examples/sec; 0.226 sec/batch; 20h:30m:22s remains)
INFO - root - 2017-12-15 06:24:36.974882: step 5790, loss = 0.24, batch loss = 0.20 (34.7 examples/sec; 0.231 sec/batch; 20h:57m:09s remains)
INFO - root - 2017-12-15 06:24:39.209302: step 5800, loss = 0.22, batch loss = 0.18 (35.3 examples/sec; 0.227 sec/batch; 20h:34m:11s remains)
2017-12-15 06:24:39.502945: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4433699 -3.9173703 -3.4673042 -3.8863926 -4.7359781 -5.4298048 -5.7424488 -5.8240509 -5.5394874 -5.1433868 -4.9431572 -4.9240823 -5.1584463 -5.1745939 -5.1514025][-4.6576147 -4.7265816 -4.4481258 -4.6819754 -5.0968981 -5.471242 -5.7303419 -5.9176435 -5.7832308 -5.6208758 -5.7492261 -5.9185219 -6.258378 -6.4877968 -6.66321][-5.225111 -4.63388 -4.4947853 -4.5398121 -4.5168729 -4.5072069 -4.7078738 -5.0570836 -5.1774654 -5.2577543 -5.5944357 -5.914587 -6.2254243 -6.4850159 -6.7228765][-5.09451 -3.8519149 -3.4891343 -3.1086836 -2.5374293 -2.059624 -2.0984368 -2.7063651 -3.3632314 -3.9801388 -4.6954393 -5.2512321 -5.5269766 -5.6081944 -5.69911][-4.0330772 -2.3089576 -1.6054822 -0.60349762 0.64661837 1.7257087 2.0345769 1.2950461 0.078384876 -1.3263969 -2.7139511 -3.77646 -4.1455173 -4.104856 -4.0146489][-3.0888207 -1.296105 -0.242993 1.3817499 3.417191 5.2556047 5.9524655 5.0984535 3.378541 1.206197 -0.84764695 -2.5122128 -3.1429944 -3.1321223 -2.9329987][-2.9172587 -1.2846806 0.1768837 2.4174857 5.2881508 7.892179 8.8363991 7.7226539 5.5610428 2.6212187 -0.074868679 -2.102088 -2.9424725 -2.9175072 -2.583046][-3.5046182 -2.0877464 -0.48591316 1.9870028 5.29955 8.3477592 9.3783379 8.1854382 5.941896 2.5958343 -0.46615922 -2.5398488 -3.3913002 -3.2686625 -2.720912][-4.9875774 -4.0142574 -2.4933097 -0.295079 2.7065997 5.5269632 6.397428 5.3557086 3.4905872 0.3803587 -2.4183688 -4.0280924 -4.6030397 -4.2905016 -3.4866056][-6.7593937 -5.9312296 -4.608633 -2.9440026 -0.78853178 1.2536943 1.7852042 0.89491224 -0.536564 -3.1687469 -5.33854 -6.2099476 -6.3219023 -5.7666273 -4.6775594][-8.2576447 -7.4634085 -6.5953712 -5.5928907 -4.3848658 -3.2998414 -3.1271639 -3.7074378 -4.5365734 -6.3907528 -7.7604561 -8.011898 -7.8168907 -7.21705 -6.0081115][-9.5007343 -8.6899605 -8.2898731 -7.8842249 -7.3787613 -6.9915471 -7.0470228 -7.3617287 -7.7273369 -8.852972 -9.50194 -9.3889618 -9.1452541 -8.6798687 -7.652452][-9.6897459 -8.9595528 -8.9450588 -8.9305267 -8.822197 -8.7814369 -8.9199667 -9.0952845 -9.211791 -9.7767887 -9.9863653 -9.8472672 -9.7563114 -9.5192242 -8.79991][-9.0598335 -8.5778275 -8.7704659 -8.91358 -8.9718533 -9.0808582 -9.17708 -9.158165 -9.06901 -9.1869221 -9.1790667 -9.1043186 -9.0799847 -8.9757023 -8.66482][-8.1606531 -7.9013143 -8.1255054 -8.257513 -8.3064852 -8.347352 -8.3403053 -8.2518768 -8.1352386 -8.05158 -7.9446697 -7.8616915 -7.8570719 -7.8768673 -7.9164467]]...]
INFO - root - 2017-12-15 06:24:41.789719: step 5810, loss = 0.32, batch loss = 0.28 (33.7 examples/sec; 0.237 sec/batch; 21h:31m:01s remains)
INFO - root - 2017-12-15 06:24:44.073571: step 5820, loss = 0.33, batch loss = 0.29 (34.7 examples/sec; 0.230 sec/batch; 20h:54m:20s remains)
INFO - root - 2017-12-15 06:24:46.362252: step 5830, loss = 0.24, batch loss = 0.20 (35.0 examples/sec; 0.229 sec/batch; 20h:44m:38s remains)
INFO - root - 2017-12-15 06:24:48.686839: step 5840, loss = 0.35, batch loss = 0.32 (35.0 examples/sec; 0.228 sec/batch; 20h:43m:18s remains)
INFO - root - 2017-12-15 06:24:50.955149: step 5850, loss = 0.26, batch loss = 0.23 (35.2 examples/sec; 0.228 sec/batch; 20h:38m:39s remains)
INFO - root - 2017-12-15 06:24:53.232828: step 5860, loss = 0.34, batch loss = 0.31 (35.8 examples/sec; 0.223 sec/batch; 20h:15m:00s remains)
INFO - root - 2017-12-15 06:24:55.481717: step 5870, loss = 0.23, batch loss = 0.19 (36.2 examples/sec; 0.221 sec/batch; 20h:02m:05s remains)
INFO - root - 2017-12-15 06:24:57.751670: step 5880, loss = 0.29, batch loss = 0.25 (35.4 examples/sec; 0.226 sec/batch; 20h:30m:23s remains)
INFO - root - 2017-12-15 06:25:00.071010: step 5890, loss = 0.23, batch loss = 0.19 (34.8 examples/sec; 0.230 sec/batch; 20h:52m:36s remains)
INFO - root - 2017-12-15 06:25:02.363989: step 5900, loss = 0.43, batch loss = 0.39 (34.5 examples/sec; 0.232 sec/batch; 21h:03m:38s remains)
2017-12-15 06:25:02.669390: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.674578 -1.9347235 -1.7511913 -1.6730604 -1.6635462 -2.1242654 -2.8905916 -3.2897487 -3.7061405 -3.8021853 -3.6030331 -3.4010341 -2.7371197 -2.1991949 -2.5823886][-2.2462444 -1.87191 -1.5847909 -1.2460424 -0.94174862 -1.2804832 -2.0931678 -2.6959789 -3.4383142 -3.7829385 -3.5867846 -3.3105123 -2.4555917 -1.7559862 -2.0546024][-2.2557189 -1.4835918 -1.2763956 -0.87064195 -0.35388064 -0.43778372 -1.1304123 -1.7770829 -2.740891 -3.3736398 -3.3385072 -3.0874479 -2.1402278 -1.300351 -1.385152][-2.5051494 -1.0308077 -1.0033144 -0.70313418 -0.073647022 0.15936017 -0.21447003 -0.71282935 -1.7332301 -2.6753612 -2.9733157 -2.8882389 -1.9501371 -0.96352041 -0.74454772][-1.985072 -0.24472618 -0.440014 -0.37485135 0.12080646 0.58110762 0.52882648 0.2284224 -0.78855872 -1.9704418 -2.6058779 -2.6794302 -1.7670791 -0.59527278 -0.031228542][-0.54557061 0.94255209 0.56816983 0.44779181 0.7946403 1.3388617 1.505204 1.2978318 0.31097794 -0.99274731 -1.8667221 -2.1814954 -1.482003 -0.24068797 0.58990836][0.583164 1.7155538 1.2906191 1.1358812 1.3434069 1.8468306 2.0151005 1.8127365 0.89225459 -0.33410084 -1.3446925 -1.9263313 -1.5041842 -0.30632961 0.66926384][0.51730013 2.0085974 1.6262288 1.5040858 1.6617796 2.083559 2.2184682 1.9838047 1.1285989 -0.017214537 -1.101961 -1.9570611 -1.7831531 -0.70197082 0.28595519][0.16593671 1.6734331 1.2951717 1.2136235 1.4072785 1.9196777 2.079144 1.8454487 1.0699115 0.049081802 -1.0772017 -2.1681306 -2.2097545 -1.3467607 -0.50007629][-1.1476252 0.26165271 -0.084938049 -0.099072933 0.19960427 0.67776632 0.71601224 0.36562753 -0.32077205 -1.0976338 -2.0120411 -3.0091078 -3.0603778 -2.2935989 -1.5844709][-2.8010569 -1.432986 -1.6856866 -1.6791334 -1.4396113 -1.2286205 -1.4419799 -1.8760759 -2.3778422 -2.7736681 -3.2868624 -3.9578624 -3.8701143 -3.1733851 -2.6255815][-3.8564782 -2.58851 -2.8333869 -2.901094 -2.7731674 -2.726269 -2.970938 -3.2169533 -3.4213023 -3.5508752 -3.825429 -4.306613 -4.2614717 -3.7823668 -3.4695969][-4.57218 -3.5229254 -3.7932329 -3.8988664 -3.7638288 -3.6708894 -3.762311 -3.8648081 -3.9528902 -4.0174327 -4.262774 -4.6526928 -4.6248379 -4.3266025 -4.1539011][-5.111609 -4.2150354 -4.4586983 -4.5586452 -4.4667196 -4.4130297 -4.4489603 -4.4730396 -4.4706936 -4.4776516 -4.6336975 -4.8282285 -4.7425189 -4.5652437 -4.4812965][-5.121068 -4.3497591 -4.54313 -4.6697149 -4.7072453 -4.7636127 -4.7923493 -4.754117 -4.6704788 -4.63011 -4.6710749 -4.7118616 -4.6002779 -4.5092793 -4.460557]]...]
INFO - root - 2017-12-15 06:25:04.948064: step 5910, loss = 0.40, batch loss = 0.36 (35.9 examples/sec; 0.223 sec/batch; 20h:11m:28s remains)
INFO - root - 2017-12-15 06:25:07.185038: step 5920, loss = 0.31, batch loss = 0.27 (36.3 examples/sec; 0.221 sec/batch; 20h:00m:48s remains)
INFO - root - 2017-12-15 06:25:09.457870: step 5930, loss = 0.32, batch loss = 0.28 (36.3 examples/sec; 0.220 sec/batch; 19h:58m:48s remains)
INFO - root - 2017-12-15 06:25:11.755316: step 5940, loss = 0.29, batch loss = 0.26 (32.7 examples/sec; 0.245 sec/batch; 22h:12m:20s remains)
INFO - root - 2017-12-15 06:25:14.058063: step 5950, loss = 0.23, batch loss = 0.19 (34.2 examples/sec; 0.234 sec/batch; 21h:13m:31s remains)
INFO - root - 2017-12-15 06:25:16.302234: step 5960, loss = 0.25, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 20h:18m:36s remains)
INFO - root - 2017-12-15 06:25:18.581283: step 5970, loss = 0.35, batch loss = 0.31 (34.2 examples/sec; 0.234 sec/batch; 21h:14m:18s remains)
INFO - root - 2017-12-15 06:25:20.839671: step 5980, loss = 0.25, batch loss = 0.22 (35.4 examples/sec; 0.226 sec/batch; 20h:29m:13s remains)
INFO - root - 2017-12-15 06:25:23.146288: step 5990, loss = 0.30, batch loss = 0.26 (33.9 examples/sec; 0.236 sec/batch; 21h:23m:08s remains)
INFO - root - 2017-12-15 06:25:25.422218: step 6000, loss = 0.32, batch loss = 0.28 (34.7 examples/sec; 0.230 sec/batch; 20h:53m:16s remains)
2017-12-15 06:25:25.722281: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1748919 -3.9776611 -3.6608415 -3.6188927 -4.459219 -5.6804628 -6.5480185 -6.9776 -7.2673206 -7.0131216 -5.7193651 -4.0726666 -2.7600026 -1.6100845 -0.70601654][-4.8442612 -3.8716218 -3.8551126 -4.1985121 -5.0637608 -6.0708275 -7.0195217 -7.5316935 -7.8955221 -7.7208118 -6.617135 -4.9663706 -3.3196881 -1.8291719 -1.0101204][-5.325285 -3.3002768 -3.2158489 -3.5703301 -4.4844151 -5.3864355 -6.23079 -6.8147411 -7.4040213 -7.4808865 -6.6655426 -5.1646566 -3.5824482 -2.1370461 -1.4760485][-5.7440233 -3.1413648 -3.0362067 -3.1277251 -3.7549119 -4.30833 -4.8806486 -5.4891815 -6.2023649 -6.4578667 -5.9534054 -4.758132 -3.5130053 -2.484726 -2.2395341][-5.7074523 -3.1021068 -2.910032 -2.7464566 -2.878197 -2.8527932 -3.0788717 -3.6182337 -4.3769159 -4.7682252 -4.5424809 -3.7376482 -2.9119077 -2.4132888 -2.7871423][-5.4093037 -2.8730006 -2.7992363 -2.5568123 -2.2208571 -1.5887163 -1.3234206 -1.6215711 -2.2727468 -2.7126179 -2.7634003 -2.4128497 -2.0945551 -2.0494628 -2.8635862][-4.6548891 -2.6657135 -2.6072936 -2.3354881 -1.7630774 -0.74103415 -0.031216383 0.15808797 -0.13995278 -0.56558537 -0.85840762 -1.0271801 -1.3107883 -1.7399554 -2.7614467][-4.5067244 -2.50762 -2.6594594 -2.4881723 -1.8023134 -0.53214 0.47974324 1.1864579 1.4345069 1.2853985 0.91803789 0.19303322 -0.73738897 -1.6786118 -2.93581][-4.3468738 -2.6697559 -3.1742067 -3.3017778 -2.8895669 -1.7023443 -0.53882432 0.67630339 1.6341383 2.1547396 2.0470908 1.1195607 -0.22202945 -1.6327605 -3.0856206][-4.2008638 -2.9219387 -3.9338658 -4.4369488 -4.4422226 -3.6060698 -2.4556282 -0.92384851 0.61026406 1.8235114 2.0329888 1.1480389 -0.20310938 -1.7343698 -3.1968794][-3.2223258 -2.5053661 -4.1048965 -5.0312328 -5.2999949 -4.738997 -3.7183442 -2.2719266 -0.54828238 0.94658065 1.2061031 0.41560507 -0.79449344 -2.149231 -3.3258758][-1.325897 -1.2079272 -3.2591424 -4.5280418 -5.0290675 -4.776967 -4.0408192 -2.9053898 -1.3818049 -0.078237295 -0.0639894 -0.78480768 -1.9092616 -3.0012386 -3.6502929][0.058838844 -0.016046047 -2.1706543 -3.5651612 -4.2482948 -4.3730469 -4.0774 -3.2420623 -1.977942 -1.1258171 -1.4056489 -2.0496039 -3.0489659 -3.8607407 -3.9660621][0.20673633 0.30205441 -1.650671 -2.9923911 -3.7185411 -4.0829577 -4.2410383 -3.7116179 -2.8092475 -2.4894433 -2.8709235 -3.2560773 -4.0451994 -4.5687361 -4.2296743][-0.6087209 -0.23963237 -1.9638926 -3.1067266 -3.5863097 -3.9506912 -4.4996333 -4.2728033 -3.7495122 -3.8232481 -4.1586037 -4.2397594 -4.7966828 -5.13741 -4.5850086]]...]
INFO - root - 2017-12-15 06:25:28.009510: step 6010, loss = 0.32, batch loss = 0.28 (36.3 examples/sec; 0.221 sec/batch; 20h:00m:09s remains)
INFO - root - 2017-12-15 06:25:30.310386: step 6020, loss = 0.40, batch loss = 0.36 (34.4 examples/sec; 0.233 sec/batch; 21h:05m:20s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 06:25:32.571242: step 6030, loss = 0.24, batch loss = 0.21 (34.5 examples/sec; 0.232 sec/batch; 21h:03m:17s remains)
INFO - root - 2017-12-15 06:25:34.901759: step 6040, loss = 0.32, batch loss = 0.28 (32.9 examples/sec; 0.244 sec/batch; 22h:04m:54s remains)
INFO - root - 2017-12-15 06:25:37.207371: step 6050, loss = 0.27, batch loss = 0.23 (34.4 examples/sec; 0.232 sec/batch; 21h:04m:51s remains)
INFO - root - 2017-12-15 06:25:39.460641: step 6060, loss = 0.35, batch loss = 0.31 (35.3 examples/sec; 0.226 sec/batch; 20h:31m:25s remains)
INFO - root - 2017-12-15 06:25:41.813801: step 6070, loss = 0.30, batch loss = 0.26 (34.7 examples/sec; 0.231 sec/batch; 20h:54m:27s remains)
INFO - root - 2017-12-15 06:25:44.100185: step 6080, loss = 0.23, batch loss = 0.19 (35.8 examples/sec; 0.224 sec/batch; 20h:17m:18s remains)
INFO - root - 2017-12-15 06:25:46.458267: step 6090, loss = 0.26, batch loss = 0.22 (32.3 examples/sec; 0.248 sec/batch; 22h:28m:29s remains)
INFO - root - 2017-12-15 06:25:48.878072: step 6100, loss = 0.26, batch loss = 0.23 (34.8 examples/sec; 0.230 sec/batch; 20h:50m:57s remains)
2017-12-15 06:25:49.142985: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7488375 -6.3841343 -7.6926775 -8.4244385 -8.4216194 -7.7803421 -6.9127626 -6.3352871 -6.2579489 -6.3778696 -6.598877 -6.88476 -6.9917488 -6.9344234 -6.8364782][-3.9230738 -6.9664 -8.4486761 -9.1017885 -8.8753777 -7.8740158 -6.4836197 -5.5022674 -5.2127218 -5.3817415 -5.8023291 -6.4015093 -6.8001518 -6.9821825 -6.9929619][-5.0799494 -7.2599783 -8.7616329 -9.131238 -8.4612207 -6.9483232 -5.08996 -3.7863486 -3.3595226 -3.5939462 -4.1144428 -4.8074713 -5.3909073 -5.8497229 -6.0395279][-6.2113953 -7.570859 -9.0091352 -8.9574451 -7.7021341 -5.5996389 -3.3190644 -1.802093 -1.3819295 -1.7839496 -2.3776581 -2.8684692 -3.3956504 -4.020474 -4.3178654][-7.0404425 -7.6528645 -8.8340034 -8.2008286 -6.3796244 -3.8517888 -1.3354969 0.18021679 0.43718052 -0.14563358 -0.72048175 -0.86788607 -1.1530485 -1.8018339 -2.2017419][-7.2397051 -7.5534754 -8.4297943 -7.2728682 -5.109046 -2.4143171 0.2309413 1.7402279 1.8276944 1.0306599 0.38360405 0.41771269 0.3132751 -0.21299744 -0.66364586][-6.8230705 -7.2568064 -8.0247488 -6.641674 -4.4296656 -1.7257895 1.0902157 2.8176849 2.9076827 1.828295 0.79965162 0.521703 0.24089718 -0.21819365 -0.6415571][-6.6101627 -7.1168146 -7.9415779 -6.5150852 -4.3833523 -1.704601 1.2667246 3.2546313 3.4031694 1.9886558 0.34615397 -0.64902961 -1.3135891 -1.6982645 -1.9794326][-6.6269178 -7.2985134 -8.1990433 -6.7775755 -4.7628841 -2.2274978 0.72216249 2.8696954 3.0733783 1.4998717 -0.5797739 -2.3036122 -3.3887041 -3.7275987 -3.8222361][-6.7216864 -7.5746517 -8.5588865 -7.2530375 -5.3977804 -3.100666 -0.32789528 1.9054801 2.2091782 0.63338447 -1.6883582 -4.0128078 -5.5009441 -5.8912849 -5.8644834][-6.8517904 -7.8586884 -8.9748545 -7.9019938 -6.1979017 -4.054441 -1.4438329 0.71472144 0.98811626 -0.55793405 -2.9712729 -5.5821414 -7.0760918 -7.327137 -7.1136169][-6.95973 -7.9654264 -9.1840782 -8.4603968 -6.9965868 -5.0098209 -2.6240802 -0.7290591 -0.5210247 -1.9035785 -4.1388726 -6.5596571 -7.6424246 -7.6186428 -7.2382135][-7.1012049 -7.9161263 -9.1540308 -8.842535 -7.7314625 -6.0643425 -4.1134634 -2.5771465 -2.2828643 -3.1625419 -4.8184552 -6.6109724 -7.1952991 -7.0933609 -6.8104887][-7.3232584 -7.9518061 -9.1781807 -9.207387 -8.3842344 -6.9993763 -5.4588976 -4.2347107 -3.8028126 -4.1637735 -5.2370319 -6.3965349 -6.6786747 -6.684948 -6.5955276][-7.279026 -7.7530079 -8.9548788 -9.2261095 -8.5957918 -7.4626236 -6.2663155 -5.3594255 -5.0057878 -5.2229347 -5.9469566 -6.6272116 -6.7259521 -6.7477016 -6.693553]]...]
INFO - root - 2017-12-15 06:25:51.426241: step 6110, loss = 0.25, batch loss = 0.21 (32.6 examples/sec; 0.246 sec/batch; 22h:16m:51s remains)
INFO - root - 2017-12-15 06:25:53.712024: step 6120, loss = 0.39, batch loss = 0.35 (36.4 examples/sec; 0.220 sec/batch; 19h:54m:54s remains)
INFO - root - 2017-12-15 06:25:56.009169: step 6130, loss = 0.27, batch loss = 0.23 (35.8 examples/sec; 0.223 sec/batch; 20h:14m:53s remains)
INFO - root - 2017-12-15 06:25:58.274744: step 6140, loss = 0.25, batch loss = 0.21 (34.5 examples/sec; 0.232 sec/batch; 21h:02m:00s remains)
INFO - root - 2017-12-15 06:26:00.559448: step 6150, loss = 0.23, batch loss = 0.19 (34.2 examples/sec; 0.234 sec/batch; 21h:11m:09s remains)
INFO - root - 2017-12-15 06:26:02.842920: step 6160, loss = 0.30, batch loss = 0.27 (35.9 examples/sec; 0.223 sec/batch; 20h:11m:25s remains)
INFO - root - 2017-12-15 06:26:05.106429: step 6170, loss = 0.28, batch loss = 0.25 (35.8 examples/sec; 0.223 sec/batch; 20h:14m:46s remains)
INFO - root - 2017-12-15 06:26:07.399200: step 6180, loss = 0.25, batch loss = 0.21 (34.2 examples/sec; 0.234 sec/batch; 21h:12m:53s remains)
INFO - root - 2017-12-15 06:26:09.718039: step 6190, loss = 0.41, batch loss = 0.38 (34.9 examples/sec; 0.229 sec/batch; 20h:47m:14s remains)
INFO - root - 2017-12-15 06:26:12.011211: step 6200, loss = 0.25, batch loss = 0.21 (34.8 examples/sec; 0.230 sec/batch; 20h:48m:33s remains)
2017-12-15 06:26:12.284123: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9414887 -4.4896741 -4.0512404 -4.3707609 -4.5845451 -4.9442682 -5.8148031 -6.1397791 -6.1059628 -5.9494486 -6.200593 -6.2748804 -6.0862741 -5.8756409 -5.4714417][-4.8680487 -3.1589499 -3.1024675 -3.7662196 -4.0419478 -4.0133963 -4.4938812 -4.6802568 -4.4464946 -3.9032259 -3.9349468 -4.2040706 -4.6445179 -5.0416689 -5.0577965][-4.5947657 -2.0111153 -2.1945436 -2.990386 -3.206003 -2.7278759 -2.7050564 -2.6314816 -2.2778916 -1.5502639 -1.1658177 -1.2702782 -2.0470793 -2.8973331 -3.4143963][-4.893281 -1.7634593 -1.855594 -2.4586194 -2.5497575 -1.8864954 -1.5757935 -1.3027775 -0.8638376 -0.0903306 0.68555546 0.99568439 0.361521 -0.63139439 -1.4985328][-5.6319337 -2.0614023 -2.0331485 -2.3677223 -2.3539138 -1.7995808 -1.5727935 -1.3687111 -0.89739907 0.017706394 1.1918252 2.0443532 1.8888977 1.1436517 0.094689369][-6.1220112 -2.6007884 -2.4659925 -2.4955897 -2.2691388 -1.7825413 -1.6850384 -1.4557521 -0.948362 -0.18265665 0.91460323 1.8859329 2.1023409 1.7646174 0.88649154][-5.8954391 -2.8586259 -2.6452193 -2.4510756 -1.8560985 -1.125315 -0.79238331 -0.46260381 -0.11807668 0.094641209 0.55610442 1.1414587 1.4569829 1.4587274 0.97476721][-5.5740929 -2.7403221 -2.4538026 -2.2132623 -1.3895893 -0.58243489 -0.096420646 0.27406096 0.61974239 0.59526968 0.41243267 0.38128114 0.57342267 0.86108971 0.75362778][-5.5107927 -2.8039463 -2.2905655 -1.9910494 -1.1830945 -0.47260356 0.043313742 0.46437144 0.801486 0.61491513 0.017172098 -0.39607751 -0.20759642 0.35523725 0.58840322][-5.855382 -3.0323551 -2.1374187 -1.7237381 -1.0214803 -0.58770096 -0.27059889 0.13265896 0.53179717 0.29583144 -0.54422688 -1.1931312 -0.9697299 -0.1530782 0.34106588][-6.3955603 -3.6799974 -2.8395519 -2.6559753 -2.1222036 -1.6584281 -1.2717547 -0.88143706 -0.55789554 -0.85905063 -1.6848812 -2.3845692 -2.0623257 -1.0969956 -0.5832448][-6.7690506 -4.5878181 -4.143393 -4.2371483 -3.887228 -3.4242835 -2.9591808 -2.5436702 -2.3292742 -2.5891337 -3.2227769 -3.7975984 -3.4269419 -2.5890009 -2.2243936][-7.0837059 -5.4008584 -5.282196 -5.6254673 -5.6081839 -5.2817116 -4.6940737 -4.146594 -3.9368324 -4.0908737 -4.5243177 -5.0000467 -4.738831 -4.2113986 -4.0021582][-7.3023281 -6.0670347 -6.3013783 -6.8480196 -7.00027 -6.6730728 -5.9809747 -5.3847923 -5.12071 -5.1144276 -5.3066583 -5.6138372 -5.40291 -5.105629 -5.0876708][-6.6543531 -5.8118706 -6.2760067 -6.877327 -7.1021891 -6.8093567 -6.109726 -5.5084171 -5.236474 -5.1329966 -5.1240239 -5.2124386 -5.0483503 -4.9576721 -5.1387596]]...]
INFO - root - 2017-12-15 06:26:14.572173: step 6210, loss = 0.25, batch loss = 0.22 (36.1 examples/sec; 0.221 sec/batch; 20h:04m:30s remains)
INFO - root - 2017-12-15 06:26:16.812588: step 6220, loss = 0.31, batch loss = 0.27 (34.8 examples/sec; 0.230 sec/batch; 20h:49m:26s remains)
INFO - root - 2017-12-15 06:26:19.125003: step 6230, loss = 0.29, batch loss = 0.26 (34.3 examples/sec; 0.233 sec/batch; 21h:08m:11s remains)
INFO - root - 2017-12-15 06:26:21.415672: step 6240, loss = 0.27, batch loss = 0.23 (35.5 examples/sec; 0.225 sec/batch; 20h:24m:08s remains)
INFO - root - 2017-12-15 06:26:23.702985: step 6250, loss = 0.26, batch loss = 0.22 (35.3 examples/sec; 0.227 sec/batch; 20h:33m:42s remains)
INFO - root - 2017-12-15 06:26:25.985100: step 6260, loss = 0.40, batch loss = 0.36 (35.8 examples/sec; 0.223 sec/batch; 20h:14m:09s remains)
INFO - root - 2017-12-15 06:26:28.265083: step 6270, loss = 0.22, batch loss = 0.18 (34.6 examples/sec; 0.231 sec/batch; 20h:58m:20s remains)
INFO - root - 2017-12-15 06:26:30.538181: step 6280, loss = 0.36, batch loss = 0.32 (34.7 examples/sec; 0.230 sec/batch; 20h:52m:25s remains)
INFO - root - 2017-12-15 06:26:32.806453: step 6290, loss = 0.25, batch loss = 0.22 (34.9 examples/sec; 0.229 sec/batch; 20h:46m:05s remains)
INFO - root - 2017-12-15 06:26:35.108345: step 6300, loss = 0.31, batch loss = 0.27 (35.2 examples/sec; 0.227 sec/batch; 20h:35m:47s remains)
2017-12-15 06:26:35.393437: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4461451 -4.0804758 -3.3702195 -2.8087878 -3.0741735 -3.8363078 -4.1223717 -3.9757056 -3.918149 -3.6632333 -3.4147875 -3.2909274 -2.9475596 -2.7580383 -2.865356][-3.9069595 -4.4240971 -4.1665192 -3.9163642 -4.298799 -4.8555374 -4.9980655 -4.995244 -5.210166 -5.2533751 -5.0811348 -4.8927784 -4.4262013 -3.8608375 -3.5258689][-4.2285242 -4.40471 -4.5151672 -4.502738 -4.8523951 -5.0215073 -4.8894482 -4.9769106 -5.4359722 -5.8398523 -5.9457808 -5.6369214 -4.814096 -3.8335066 -3.238553][-4.4986067 -4.150012 -4.4312692 -4.5288911 -4.6680183 -4.2928214 -3.7627578 -3.8886728 -4.5623817 -5.4433446 -6.1321268 -5.8830118 -4.6869364 -3.2313037 -2.3085155][-3.8160782 -3.0942709 -3.2713032 -3.1966925 -2.9124696 -1.9227468 -1.0501597 -1.401721 -2.5734529 -4.2295651 -5.693965 -5.6987362 -4.2647452 -2.4551916 -1.2810111][-2.9732895 -1.8552086 -1.7119728 -1.2569592 -0.47419465 1.1574945 2.4749224 2.0967886 0.36370468 -2.2397993 -4.52273 -4.9600077 -3.6890285 -1.8552078 -0.59604013][-1.9900577 -1.0080752 -0.473804 0.37557817 1.5913606 3.8385651 5.7352695 5.568387 3.3388708 -0.20695317 -3.1402006 -3.9827533 -3.1642628 -1.5746232 -0.2870189][-1.743737 -0.6117171 0.250252 1.2895386 2.711355 5.3280354 7.6531496 7.8929043 5.3651695 1.1039793 -2.1529863 -3.2837574 -3.0410912 -1.9394333 -0.72462404][-2.2820811 -1.1147954 -0.21452975 0.78940082 2.2580817 4.977067 7.6609373 8.4391785 6.06851 1.7638228 -1.4069264 -2.7378602 -3.1208327 -2.5788844 -1.5712363][-3.4320652 -2.3697581 -1.7245259 -1.0065191 0.15522408 2.4114349 4.8230505 5.7348728 3.8306568 0.34431911 -1.9891911 -2.9444761 -3.3761027 -3.0571132 -2.2409852][-4.2747765 -3.204124 -2.81011 -2.4113719 -1.7050685 -0.29369271 1.3463478 1.9288208 0.49981117 -1.8577992 -3.2204762 -3.5297186 -3.5676475 -3.0823231 -2.2305837][-5.2807174 -4.2531157 -4.1787977 -4.0664353 -3.7318354 -3.058805 -2.1280441 -1.6923189 -2.400903 -3.5153029 -3.9243274 -3.6325555 -3.2401695 -2.6390684 -1.8713754][-5.8569584 -4.7909265 -4.9247379 -4.9364648 -4.8101678 -4.68778 -4.3473539 -4.1018772 -4.2856932 -4.5009165 -4.2882361 -3.6302361 -2.9850953 -2.3254218 -1.7232695][-5.4644737 -4.3295269 -4.4813147 -4.5134478 -4.4401555 -4.589057 -4.6147985 -4.4714136 -4.4329357 -4.2903872 -3.9866433 -3.2528882 -2.4810023 -1.9643524 -1.62776][-5.4234581 -4.3006535 -4.4019437 -4.3584657 -4.2227106 -4.3756285 -4.5165753 -4.4441767 -4.4052839 -4.3058109 -4.1032372 -3.3745556 -2.4609849 -2.0226073 -1.9962958]]...]
INFO - root - 2017-12-15 06:26:37.656003: step 6310, loss = 0.29, batch loss = 0.25 (35.1 examples/sec; 0.228 sec/batch; 20h:38m:19s remains)
INFO - root - 2017-12-15 06:26:39.981033: step 6320, loss = 0.24, batch loss = 0.20 (34.9 examples/sec; 0.230 sec/batch; 20h:47m:48s remains)
INFO - root - 2017-12-15 06:26:42.275881: step 6330, loss = 0.25, batch loss = 0.21 (35.1 examples/sec; 0.228 sec/batch; 20h:37m:27s remains)
INFO - root - 2017-12-15 06:26:44.548765: step 6340, loss = 0.37, batch loss = 0.33 (34.6 examples/sec; 0.232 sec/batch; 20h:58m:35s remains)
INFO - root - 2017-12-15 06:26:46.801153: step 6350, loss = 0.33, batch loss = 0.29 (36.1 examples/sec; 0.221 sec/batch; 20h:03m:17s remains)
INFO - root - 2017-12-15 06:26:49.109606: step 6360, loss = 0.43, batch loss = 0.39 (34.3 examples/sec; 0.233 sec/batch; 21h:07m:13s remains)
INFO - root - 2017-12-15 06:26:51.395241: step 6370, loss = 0.29, batch loss = 0.25 (35.5 examples/sec; 0.226 sec/batch; 20h:25m:49s remains)
INFO - root - 2017-12-15 06:26:53.683520: step 6380, loss = 0.27, batch loss = 0.24 (34.9 examples/sec; 0.229 sec/batch; 20h:45m:02s remains)
INFO - root - 2017-12-15 06:26:55.976779: step 6390, loss = 0.28, batch loss = 0.24 (35.4 examples/sec; 0.226 sec/batch; 20h:28m:30s remains)
INFO - root - 2017-12-15 06:26:58.221492: step 6400, loss = 0.30, batch loss = 0.26 (36.4 examples/sec; 0.220 sec/batch; 19h:55m:43s remains)
2017-12-15 06:26:58.521847: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9914607 -4.1754851 -4.0202847 -3.9718583 -3.7982159 -3.1690998 -2.7421336 -2.5771897 -2.8686748 -3.4968109 -3.8100357 -3.5514743 -3.1157496 -2.3635542 -1.3207157][-3.5302229 -4.7095947 -4.6611466 -4.751997 -4.6973524 -4.056838 -3.4859862 -3.4188321 -3.7135513 -4.2027426 -4.3359747 -3.789139 -2.9424074 -1.7745012 -0.5976131][-4.7381954 -4.9883695 -5.01104 -5.1764269 -5.1461582 -4.4950876 -3.9325962 -3.8786283 -4.1669827 -4.5480404 -4.428524 -3.7676282 -2.6982112 -1.175612 0.23615599][-4.5167866 -4.1523056 -4.3438196 -4.7008634 -4.8487692 -4.4277744 -4.1567287 -4.1656485 -4.4073315 -4.6286535 -4.1095695 -3.2218461 -2.1266081 -0.62853384 0.939548][-3.3496635 -2.7218053 -3.0714056 -3.405324 -3.4451044 -3.3695815 -3.5499578 -3.7626443 -4.0082564 -4.233799 -3.5846028 -2.5612388 -1.5504193 -0.46578074 0.95102882][-1.6584849 -0.91472411 -1.1074917 -0.98462605 -0.56504393 -0.53874874 -1.1823404 -1.6798048 -2.0812483 -2.6966491 -2.4002514 -1.577585 -0.87439477 -0.27550888 0.8641479][-0.44084787 -0.053063393 0.045547962 0.84728217 1.9194863 2.0909741 1.2196569 0.51088142 -0.079286337 -1.0331839 -1.198065 -0.80520809 -0.56396222 -0.42596149 0.60463][-0.51160789 -0.19038188 0.34127355 1.9812324 3.7867582 4.2294912 3.29166 2.3769596 1.3714561 0.0972445 -0.39229727 -0.49948406 -0.55156827 -0.57143188 0.26590133][-1.0733079 -0.98489249 -0.22334146 1.9280417 4.1030664 4.7141542 3.9407694 2.8615673 1.4169846 -0.00087857246 -0.63930058 -0.99381554 -1.0183504 -0.75269151 0.13521338][-2.0756269 -2.0664666 -1.3515584 0.73813391 2.7667258 3.4258749 2.7967155 1.7116456 0.049979925 -1.2997117 -1.7435632 -2.034534 -1.8667207 -1.275188 -0.4771769][-3.5086505 -3.5039546 -3.1099453 -1.5294082 0.20203614 0.89106989 0.61239934 -0.37822151 -2.060405 -3.1707163 -3.4814453 -3.5326014 -2.9450207 -2.0586803 -1.5117235][-5.1616926 -5.1994209 -5.1051335 -4.0896306 -2.8096495 -2.120852 -2.2246943 -3.05306 -4.4005585 -5.1842713 -5.1886129 -4.8758774 -4.1089725 -3.265038 -3.0032237][-6.7931719 -6.7689266 -7.0098658 -6.6151648 -5.819766 -5.2826028 -5.3081169 -5.9556131 -6.8711138 -7.2559738 -7.0593033 -6.6276741 -6.02633 -5.381824 -5.1892643][-7.8169212 -7.586853 -7.9515753 -7.9842463 -7.601656 -7.247674 -7.2573833 -7.6673813 -8.23856 -8.4448977 -8.1667385 -7.7230978 -7.2415981 -6.828722 -6.6568642][-8.0927315 -7.5244513 -7.7909927 -7.9534178 -7.871088 -7.7099056 -7.6884785 -7.8685675 -8.11582 -8.1613922 -7.9265366 -7.5929451 -7.318655 -7.0441647 -6.82401]]...]
INFO - root - 2017-12-15 06:27:00.812072: step 6410, loss = 0.24, batch loss = 0.20 (34.8 examples/sec; 0.230 sec/batch; 20h:50m:47s remains)
INFO - root - 2017-12-15 06:27:03.091482: step 6420, loss = 0.41, batch loss = 0.38 (35.5 examples/sec; 0.226 sec/batch; 20h:26m:17s remains)
INFO - root - 2017-12-15 06:27:05.378693: step 6430, loss = 0.28, batch loss = 0.24 (34.8 examples/sec; 0.230 sec/batch; 20h:49m:32s remains)
INFO - root - 2017-12-15 06:27:07.632864: step 6440, loss = 0.28, batch loss = 0.24 (35.0 examples/sec; 0.229 sec/batch; 20h:42m:16s remains)
INFO - root - 2017-12-15 06:27:09.925067: step 6450, loss = 0.25, batch loss = 0.21 (35.1 examples/sec; 0.228 sec/batch; 20h:37m:33s remains)
INFO - root - 2017-12-15 06:27:12.203761: step 6460, loss = 0.31, batch loss = 0.27 (35.2 examples/sec; 0.227 sec/batch; 20h:35m:39s remains)
INFO - root - 2017-12-15 06:27:14.480030: step 6470, loss = 0.29, batch loss = 0.25 (34.5 examples/sec; 0.232 sec/batch; 20h:59m:30s remains)
INFO - root - 2017-12-15 06:27:16.747752: step 6480, loss = 0.27, batch loss = 0.24 (35.7 examples/sec; 0.224 sec/batch; 20h:19m:03s remains)
INFO - root - 2017-12-15 06:27:19.008139: step 6490, loss = 0.28, batch loss = 0.25 (35.4 examples/sec; 0.226 sec/batch; 20h:28m:59s remains)
INFO - root - 2017-12-15 06:27:21.301695: step 6500, loss = 0.25, batch loss = 0.22 (35.5 examples/sec; 0.225 sec/batch; 20h:23m:55s remains)
2017-12-15 06:27:21.589461: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8471074 -6.8980393 -7.1442857 -7.0716267 -7.071682 -6.3472395 -5.2268095 -4.3938608 -4.4039 -4.9483633 -5.8191051 -6.9875817 -8.0349665 -8.5528393 -8.3531389][-5.3240414 -7.005425 -7.2528181 -6.9682264 -6.9307995 -5.9567046 -4.4385433 -3.3608902 -3.3672719 -4.0768776 -5.3757696 -7.0837593 -8.3067808 -8.8928413 -8.8475933][-5.8453979 -6.7116556 -6.8011394 -6.2345915 -5.981122 -4.6175289 -2.7377043 -1.6095926 -1.7468467 -2.6436038 -4.3120689 -6.3738222 -7.7854648 -8.61865 -8.8699722][-6.6282473 -6.8034654 -6.5999575 -5.6318393 -4.8957381 -3.0864842 -0.95522404 0.16707397 0.16386414 -0.63503468 -2.4377637 -4.7030115 -6.4343944 -7.7115517 -8.5220509][-7.2717977 -7.0554619 -6.5005407 -5.1374044 -3.8685088 -1.7293198 0.511662 1.679184 1.9242435 1.3174653 -0.60204542 -3.0168357 -5.018652 -6.7178884 -7.9544458][-7.8662705 -7.2432184 -6.2569623 -4.5415931 -2.7779942 -0.44981015 1.8162487 2.966373 3.2310348 2.4499226 0.22080708 -2.1855061 -4.2649279 -6.1443896 -7.5108728][-7.9706197 -7.1839566 -5.8090253 -3.8345339 -1.5638089 1.0066483 3.4050455 4.6439695 4.8279166 3.5786042 0.79580784 -1.7359694 -3.8352947 -5.7107992 -6.8926344][-7.9688325 -6.9223208 -5.4115257 -3.4396548 -0.84611154 1.8591945 4.284039 5.5475092 5.6797256 4.1384892 1.1653583 -1.4039385 -3.6047759 -5.4697461 -6.3223686][-7.6851063 -6.6379442 -5.3142304 -3.5237374 -0.76678979 1.9739871 4.4083652 5.65996 5.6611533 3.9978762 1.1270456 -1.4334447 -3.8207438 -5.5868597 -5.9092512][-7.3015165 -6.4282513 -5.5711207 -4.0963693 -1.4022567 1.1602137 3.5401149 4.9291658 5.0342317 3.4558473 0.81056213 -1.843421 -4.3326478 -5.7135782 -5.4859486][-6.87885 -6.2673736 -6.034996 -5.0532503 -2.7276177 -0.58940852 1.6010263 2.9819775 3.0728478 1.74824 -0.4615041 -2.9790242 -5.2157955 -5.9625063 -5.2800827][-6.7441859 -6.3809333 -6.6523237 -6.1103563 -4.1944885 -2.4893682 -0.76377988 0.041600943 -0.24013925 -1.1970091 -2.5412672 -4.476 -6.1264725 -6.3029509 -5.4972954][-6.961112 -6.7163143 -7.1551952 -6.7791629 -5.2244959 -3.9213173 -2.8894095 -2.7621715 -3.2681737 -3.7451262 -4.4700422 -5.9517317 -7.0124688 -6.8364511 -6.1518488][-7.0897021 -6.892487 -7.34402 -7.1518688 -6.0358391 -5.1030574 -4.5855956 -4.8735332 -5.4150763 -5.6343565 -6.0997462 -7.1360993 -7.6034136 -7.2108159 -6.67971][-6.7801557 -6.5103874 -6.9386296 -6.9799709 -6.3436618 -5.6988511 -5.4694862 -5.9095283 -6.4644241 -6.7118521 -7.0293741 -7.5240984 -7.5189705 -7.0887175 -6.7126045]]...]
INFO - root - 2017-12-15 06:27:23.907279: step 6510, loss = 0.21, batch loss = 0.17 (34.9 examples/sec; 0.229 sec/batch; 20h:46m:11s remains)
INFO - root - 2017-12-15 06:27:26.182217: step 6520, loss = 0.35, batch loss = 0.31 (35.0 examples/sec; 0.229 sec/batch; 20h:43m:22s remains)
INFO - root - 2017-12-15 06:27:28.446933: step 6530, loss = 0.21, batch loss = 0.17 (33.9 examples/sec; 0.236 sec/batch; 21h:22m:18s remains)
INFO - root - 2017-12-15 06:27:30.751361: step 6540, loss = 0.20, batch loss = 0.16 (34.1 examples/sec; 0.235 sec/batch; 21h:14m:34s remains)
INFO - root - 2017-12-15 06:27:33.066150: step 6550, loss = 0.46, batch loss = 0.42 (36.0 examples/sec; 0.222 sec/batch; 20h:06m:59s remains)
INFO - root - 2017-12-15 06:27:35.327681: step 6560, loss = 0.42, batch loss = 0.38 (35.1 examples/sec; 0.228 sec/batch; 20h:37m:18s remains)
INFO - root - 2017-12-15 06:27:37.706145: step 6570, loss = 0.30, batch loss = 0.26 (36.5 examples/sec; 0.219 sec/batch; 19h:50m:14s remains)
INFO - root - 2017-12-15 06:27:39.977080: step 6580, loss = 0.26, batch loss = 0.23 (35.3 examples/sec; 0.227 sec/batch; 20h:32m:10s remains)
INFO - root - 2017-12-15 06:27:42.239248: step 6590, loss = 0.27, batch loss = 0.23 (36.0 examples/sec; 0.222 sec/batch; 20h:08m:31s remains)
INFO - root - 2017-12-15 06:27:44.505451: step 6600, loss = 0.22, batch loss = 0.18 (36.2 examples/sec; 0.221 sec/batch; 19h:59m:16s remains)
2017-12-15 06:27:44.826257: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3642631 -4.2893915 -3.5074725 -2.5334225 -2.3418453 -2.0483723 -1.4547236 -1.0674096 -1.2248828 -1.4767289 -1.4667627 -1.6217134 -1.3806698 -0.63391554 -0.59739137][-4.0210881 -4.1454191 -3.1994495 -2.0780957 -1.7980555 -1.561547 -1.1796563 -0.8423599 -0.84023213 -1.1642399 -1.3341131 -1.6091163 -1.5935822 -1.1322961 -1.1501293][-4.4089413 -4.1063137 -3.0808764 -1.831012 -1.3806639 -1.166274 -1.0469128 -0.85689032 -0.84809208 -1.1833215 -1.435739 -1.6934757 -1.8259263 -1.7512685 -1.9777769][-4.9780769 -4.2601471 -3.2792387 -1.9985315 -1.3760393 -1.075233 -1.0534883 -0.9746362 -0.986323 -1.3233101 -1.584722 -1.7874649 -1.9372882 -2.0426631 -2.3598843][-5.1906271 -4.4225554 -3.4715748 -2.1720948 -1.3250228 -0.81043577 -0.75264883 -0.73100185 -0.83348465 -1.1453891 -1.4693463 -1.7457623 -1.9516522 -2.1995878 -2.4879482][-5.2049131 -4.3083658 -3.2158144 -1.7495226 -0.595873 0.23036909 0.42397571 0.37296104 0.059277773 -0.41193998 -0.89019728 -1.3100672 -1.5295155 -1.878413 -2.1524673][-4.765501 -4.086175 -2.9544129 -1.4852529 -0.25157177 0.70563221 0.99876261 0.89025927 0.43349862 -0.12162638 -0.67773211 -1.0921003 -1.1360576 -1.3894069 -1.5415273][-4.7485642 -4.1037159 -3.019491 -1.6065066 -0.32726991 0.83402824 1.3873773 1.4696698 1.0588295 0.42163515 -0.28581393 -0.76439881 -0.69170594 -0.82530296 -0.83202124][-5.1117167 -4.6766338 -3.7889967 -2.5485511 -1.2933952 0.024343967 0.87249327 1.2299106 1.0279803 0.4682765 -0.32778335 -0.75945604 -0.52212918 -0.49035418 -0.3077929][-5.7988005 -5.6809921 -5.162569 -4.2930269 -3.2172832 -1.8410954 -0.7462095 -0.051655293 0.090497494 -0.21886075 -0.94108129 -1.1511436 -0.70127308 -0.49669659 -0.12706542][-6.494091 -6.6484394 -6.5150604 -6.0551944 -5.2415533 -3.9684403 -2.7722759 -1.9027321 -1.4362228 -1.485235 -2.1631343 -2.2158968 -1.75368 -1.6192324 -1.3164555][-7.024518 -7.3635693 -7.4788303 -7.2650681 -6.6421843 -5.5421114 -4.4648438 -3.5642996 -2.9162953 -2.7669933 -3.2634032 -3.0620131 -2.5445197 -2.4614856 -2.2793639][-7.1261187 -7.5752497 -7.8375225 -7.6685667 -7.0888386 -6.1623678 -5.3008518 -4.6287603 -4.0457449 -3.8721838 -4.2577739 -4.0087729 -3.5857389 -3.5388565 -3.4149477][-6.8164253 -7.3548765 -7.7944021 -7.7769003 -7.3446474 -6.6492033 -6.0071239 -5.4530058 -4.9514914 -4.8303671 -5.1551166 -5.0542779 -4.9089127 -5.015624 -5.0401125][-6.1858978 -6.6094751 -7.04525 -7.0852137 -6.8007746 -6.3116112 -5.8010607 -5.3517962 -4.9600534 -4.929759 -5.2455215 -5.3384385 -5.43211 -5.7056017 -5.9870758]]...]
INFO - root - 2017-12-15 06:27:47.102943: step 6610, loss = 0.35, batch loss = 0.31 (36.0 examples/sec; 0.222 sec/batch; 20h:06m:17s remains)
INFO - root - 2017-12-15 06:27:49.406914: step 6620, loss = 0.23, batch loss = 0.19 (36.3 examples/sec; 0.221 sec/batch; 19h:58m:12s remains)
INFO - root - 2017-12-15 06:27:51.688413: step 6630, loss = 0.26, batch loss = 0.23 (35.6 examples/sec; 0.225 sec/batch; 20h:21m:32s remains)
INFO - root - 2017-12-15 06:27:54.018880: step 6640, loss = 0.33, batch loss = 0.30 (34.5 examples/sec; 0.232 sec/batch; 21h:00m:02s remains)
INFO - root - 2017-12-15 06:27:56.332120: step 6650, loss = 0.19, batch loss = 0.16 (34.4 examples/sec; 0.232 sec/batch; 21h:02m:05s remains)
INFO - root - 2017-12-15 06:27:58.617024: step 6660, loss = 0.31, batch loss = 0.28 (35.6 examples/sec; 0.225 sec/batch; 20h:19m:50s remains)
INFO - root - 2017-12-15 06:28:00.880679: step 6670, loss = 0.22, batch loss = 0.19 (35.5 examples/sec; 0.226 sec/batch; 20h:25m:15s remains)
INFO - root - 2017-12-15 06:28:03.159451: step 6680, loss = 0.32, batch loss = 0.28 (35.6 examples/sec; 0.225 sec/batch; 20h:19m:29s remains)
INFO - root - 2017-12-15 06:28:05.424323: step 6690, loss = 0.25, batch loss = 0.21 (35.9 examples/sec; 0.223 sec/batch; 20h:11m:23s remains)
INFO - root - 2017-12-15 06:28:07.709206: step 6700, loss = 0.38, batch loss = 0.35 (33.3 examples/sec; 0.240 sec/batch; 21h:43m:57s remains)
2017-12-15 06:28:08.014670: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3026149 -2.1635826 -2.4985957 -3.0138748 -3.2903016 -3.1226737 -3.0451789 -3.3789015 -4.0154643 -4.6031809 -4.8569813 -4.6175957 -4.3687544 -4.0327611 -3.706778][-1.9604906 -1.9994602 -2.5208869 -3.2203403 -3.5950727 -3.3946855 -3.4288168 -4.0785732 -5.1163912 -6.1338596 -6.5338945 -6.1003847 -5.6342916 -5.0107355 -4.3770781][-2.2768795 -1.2937398 -1.8901701 -2.6773908 -3.0695319 -2.6844618 -2.6464789 -3.467504 -4.8804722 -6.4059553 -7.0917711 -6.7718372 -6.3563919 -5.6678419 -4.7939014][-2.5438271 -0.61009383 -1.0970248 -1.8462666 -2.0932138 -1.2541749 -0.826702 -1.5946378 -3.3729291 -5.4258852 -6.5218573 -6.6579132 -6.615921 -6.1037636 -5.1228247][-2.9564271 -0.51753175 -0.67046082 -1.0153242 -0.84236526 0.57249308 1.5004659 0.81443048 -1.2866324 -3.6877589 -5.1635332 -5.8658571 -6.3021345 -6.1356821 -5.2540636][-2.5366421 -0.34782112 -0.32413113 -0.22405529 0.59198451 2.8527491 4.5323048 4.0086346 1.5477211 -1.3544981 -3.5128527 -4.8549967 -5.7123985 -5.8719368 -5.1529121][-2.402669 -0.43557334 -0.2164501 0.2833631 1.619729 4.523098 6.7011518 6.2560911 3.471658 0.057207346 -2.8130207 -4.5926776 -5.6111078 -5.8510342 -5.1870127][-2.5269332 -0.61625552 -0.25704324 0.41811562 1.9110086 5.0193529 7.2754374 6.8667336 3.9914448 0.2705605 -3.1031542 -5.103857 -6.0725412 -6.1950464 -5.4274774][-3.3214993 -1.0797857 -0.72113216 -0.3893472 0.69790864 3.5466721 5.6333809 5.5153646 3.1062896 -0.45386922 -3.9559374 -5.9654756 -6.6881609 -6.4743137 -5.4523][-4.3151937 -1.8505034 -1.6258699 -1.7789292 -1.2757939 1.163234 2.9646766 3.0883758 1.348264 -1.5332383 -4.6662674 -6.5562315 -7.0821352 -6.6011696 -5.3619204][-4.7138243 -1.9847859 -1.9985038 -2.8289635 -3.0602729 -1.2128439 0.28259563 0.64494133 -0.39117956 -2.4999897 -5.1731977 -6.9169331 -7.2808943 -6.6283588 -5.2715654][-5.1348429 -2.4607565 -2.5313926 -3.6248837 -4.2664895 -3.0971904 -2.0571415 -1.6795105 -2.2186859 -3.5933051 -5.6456957 -7.0838661 -7.2665405 -6.4984465 -5.1818185][-5.4128323 -3.1198037 -3.2708638 -4.3598928 -5.1200142 -4.511651 -3.8983347 -3.7223506 -4.1045008 -4.9461756 -6.3050871 -7.2852869 -7.2419672 -6.3969193 -5.2352076][-5.1886287 -3.3156013 -3.4842722 -4.3442564 -4.9489174 -4.6754608 -4.3498087 -4.3410077 -4.675807 -5.260046 -6.1926937 -6.877511 -6.7517691 -6.0005255 -5.1527853][-5.2368288 -3.9230623 -4.1013851 -4.5855894 -4.8240557 -4.5934119 -4.3828158 -4.4796925 -4.809103 -5.2294512 -5.7841454 -6.172576 -6.0442047 -5.5197697 -5.0071163]]...]
INFO - root - 2017-12-15 06:28:10.286561: step 6710, loss = 0.27, batch loss = 0.23 (35.4 examples/sec; 0.226 sec/batch; 20h:26m:40s remains)
INFO - root - 2017-12-15 06:28:12.545503: step 6720, loss = 0.22, batch loss = 0.19 (34.7 examples/sec; 0.231 sec/batch; 20h:51m:53s remains)
INFO - root - 2017-12-15 06:28:14.814943: step 6730, loss = 0.23, batch loss = 0.19 (34.6 examples/sec; 0.231 sec/batch; 20h:55m:22s remains)
INFO - root - 2017-12-15 06:28:17.094060: step 6740, loss = 0.26, batch loss = 0.22 (34.0 examples/sec; 0.235 sec/batch; 21h:17m:03s remains)
INFO - root - 2017-12-15 06:28:19.361932: step 6750, loss = 0.29, batch loss = 0.25 (34.6 examples/sec; 0.231 sec/batch; 20h:54m:34s remains)
INFO - root - 2017-12-15 06:28:21.673606: step 6760, loss = 0.29, batch loss = 0.25 (35.5 examples/sec; 0.225 sec/batch; 20h:21m:54s remains)
INFO - root - 2017-12-15 06:28:23.988285: step 6770, loss = 0.24, batch loss = 0.21 (33.9 examples/sec; 0.236 sec/batch; 21h:20m:59s remains)
INFO - root - 2017-12-15 06:28:26.239216: step 6780, loss = 0.28, batch loss = 0.24 (35.4 examples/sec; 0.226 sec/batch; 20h:28m:28s remains)
INFO - root - 2017-12-15 06:28:28.506093: step 6790, loss = 0.30, batch loss = 0.26 (35.3 examples/sec; 0.227 sec/batch; 20h:30m:42s remains)
INFO - root - 2017-12-15 06:28:30.801642: step 6800, loss = 0.22, batch loss = 0.18 (34.8 examples/sec; 0.230 sec/batch; 20h:47m:44s remains)
2017-12-15 06:28:31.091678: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1556036 -5.6903811 -6.5659266 -6.0243406 -5.2997417 -4.6346722 -3.9108579 -3.3297622 -2.8116961 -2.8982127 -3.8072317 -4.1928825 -4.5171056 -4.7401018 -3.7173362][-3.2105527 -5.6778774 -6.3050642 -5.5027733 -4.5314713 -3.4532633 -2.5877457 -2.3500607 -2.454078 -2.8732097 -3.6772482 -3.9734693 -4.0501509 -3.9067378 -2.7978296][-4.3857737 -5.6176205 -6.0372605 -4.9859209 -3.7825651 -2.3401561 -1.2580857 -1.2309124 -1.8257117 -2.5708852 -3.3577559 -3.679183 -3.6304808 -3.1832964 -2.087513][-5.0150023 -5.5875211 -5.8552303 -4.6368747 -3.2554474 -1.4927863 -0.21791565 -0.24485612 -1.0038018 -1.855655 -2.6731739 -3.1303139 -3.1691809 -2.6989875 -1.8684083][-5.13134 -5.6041431 -5.7896686 -4.54579 -3.0738857 -1.0220243 0.53145337 0.62282181 -0.079682589 -0.98462105 -2.0314107 -2.7123582 -2.9501657 -2.6784253 -2.34465][-5.464932 -5.7573004 -6.0212488 -4.9440136 -3.4426155 -1.0984967 0.77054024 1.2288513 0.78640127 -0.24135137 -1.7238792 -2.6217949 -2.9765556 -2.8044691 -2.7070844][-5.1173091 -5.82677 -6.3235121 -5.5233355 -3.9827919 -1.3745184 0.83473158 1.9551013 2.1166656 1.0166876 -1.0975704 -2.4620733 -3.1390333 -3.190439 -3.1860776][-4.8518114 -5.7709503 -6.6117468 -6.1928205 -4.7008944 -1.9018239 0.75094295 2.7889297 3.793597 2.750499 -0.040023327 -2.0456827 -3.1804304 -3.56038 -3.5437341][-4.6422086 -5.6350036 -6.7760134 -6.7310266 -5.3365469 -2.409759 0.59073043 3.3988397 4.9660568 3.8810089 0.65727615 -1.8515282 -3.3740611 -4.0303674 -3.9573908][-4.5222483 -5.4733095 -6.8467636 -7.1498985 -5.9775572 -3.2712665 -0.45459604 2.5011356 4.2108736 3.2569196 0.2328732 -2.299233 -3.8626122 -4.6481743 -4.591125][-4.5036988 -5.2836456 -6.7063007 -7.2505245 -6.4500227 -4.3563848 -2.1329997 0.4587853 2.1302631 1.5822527 -0.8293519 -3.0104871 -4.4384708 -5.2942023 -5.4461284][-4.6021051 -5.0064421 -6.1383495 -6.6206474 -6.1901321 -4.906579 -3.4869061 -1.5661824 -0.20506036 -0.5237335 -2.2753379 -3.9089475 -5.0136504 -5.7723346 -6.0393114][-5.0205503 -5.0652752 -5.8648529 -6.2171192 -6.0528793 -5.4845428 -4.7851849 -3.5889685 -2.6989584 -2.8561 -3.9025779 -4.9167805 -5.6059828 -6.0912905 -6.2943916][-5.4666042 -5.2197785 -5.7491369 -5.9702606 -5.9272776 -5.7642279 -5.5182076 -4.9653711 -4.511651 -4.5442309 -5.0569339 -5.5606875 -5.8880019 -6.0938411 -6.1128411][-5.5665989 -5.0837784 -5.3558011 -5.45656 -5.4819632 -5.5117612 -5.495296 -5.3249187 -5.1329942 -5.1142931 -5.3371077 -5.5572033 -5.6713333 -5.711771 -5.6371379]]...]
INFO - root - 2017-12-15 06:28:33.368859: step 6810, loss = 0.22, batch loss = 0.18 (35.5 examples/sec; 0.226 sec/batch; 20h:24m:47s remains)
INFO - root - 2017-12-15 06:28:35.620647: step 6820, loss = 0.31, batch loss = 0.28 (35.7 examples/sec; 0.224 sec/batch; 20h:16m:02s remains)
INFO - root - 2017-12-15 06:28:37.862749: step 6830, loss = 0.25, batch loss = 0.22 (35.1 examples/sec; 0.228 sec/batch; 20h:37m:58s remains)
INFO - root - 2017-12-15 06:28:40.120997: step 6840, loss = 0.26, batch loss = 0.22 (36.7 examples/sec; 0.218 sec/batch; 19h:44m:10s remains)
INFO - root - 2017-12-15 06:28:42.373536: step 6850, loss = 0.27, batch loss = 0.23 (36.0 examples/sec; 0.222 sec/batch; 20h:07m:08s remains)
INFO - root - 2017-12-15 06:28:44.656421: step 6860, loss = 0.25, batch loss = 0.22 (34.9 examples/sec; 0.229 sec/batch; 20h:43m:27s remains)
INFO - root - 2017-12-15 06:28:46.904673: step 6870, loss = 0.22, batch loss = 0.19 (35.6 examples/sec; 0.225 sec/batch; 20h:20m:43s remains)
INFO - root - 2017-12-15 06:28:49.139575: step 6880, loss = 0.35, batch loss = 0.31 (35.9 examples/sec; 0.223 sec/batch; 20h:08m:06s remains)
INFO - root - 2017-12-15 06:28:51.405253: step 6890, loss = 0.26, batch loss = 0.22 (35.3 examples/sec; 0.227 sec/batch; 20h:30m:34s remains)
INFO - root - 2017-12-15 06:28:53.700734: step 6900, loss = 0.21, batch loss = 0.17 (35.0 examples/sec; 0.228 sec/batch; 20h:39m:57s remains)
2017-12-15 06:28:53.969597: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1575432 -5.8743854 -5.7312827 -5.4060612 -5.2812734 -5.3036489 -5.5291576 -5.9171057 -6.5731192 -7.1139574 -7.1897793 -6.5020123 -5.1340532 -3.6087923 -2.8979704][-4.7453074 -5.644475 -5.4892497 -4.9252658 -4.500803 -4.3712521 -4.4097767 -4.7894697 -5.8133144 -6.7402744 -7.014761 -6.2222328 -4.4103937 -2.2742903 -1.0783457][-4.9890051 -5.0502644 -4.6148973 -3.6762848 -2.9726758 -2.6609778 -2.3734376 -2.6725934 -4.0274119 -5.3281889 -5.8068357 -4.9951119 -3.0295024 -0.76185596 0.43152905][-6.2212038 -4.8067369 -4.0281277 -2.7534237 -1.7230812 -1.0485733 -0.13600314 -0.068715572 -1.4684119 -3.0952394 -4.1615648 -3.9927537 -2.535881 -0.75988829 0.066245794][-7.4394007 -5.0211983 -4.0539932 -2.4972556 -1.1022375 0.086419582 1.5684493 2.0248773 0.65282965 -1.5455365 -3.493583 -4.1857433 -3.4055622 -2.1255026 -1.3434596][-7.4157705 -4.9510946 -3.6078413 -1.3492701 1.0197945 3.1354716 5.2575817 5.9114656 4.022419 0.702399 -2.4118993 -4.0246258 -3.9316568 -3.1612484 -2.5095367][-6.6380377 -4.9083629 -3.3531609 -0.468634 2.948313 6.0428619 8.7916079 9.5100136 7.0410147 2.7669647 -1.228366 -3.6695213 -4.3583989 -4.1636705 -3.7899084][-6.7824783 -4.9941854 -3.6595998 -0.94664073 2.4357326 5.4071684 7.8278055 8.2361593 5.669776 1.5069621 -2.2369382 -4.4272027 -4.9924135 -4.7804346 -4.4064274][-6.6429882 -5.1287503 -4.1672649 -1.8826602 0.8733356 3.1424177 4.7823372 4.8226938 2.6691949 -0.77142894 -3.7194555 -4.9889965 -4.6637912 -3.9008491 -3.318378][-6.7107611 -5.6190028 -4.9581547 -3.0745006 -0.91723168 0.77413058 2.0293014 2.0884287 0.60554457 -1.7679428 -3.7481594 -3.9639452 -2.8830941 -1.9596908 -1.6097884][-6.9430189 -6.0608811 -5.4364543 -3.8189855 -2.3547428 -1.3840301 -0.68704033 -0.86567152 -1.6933426 -3.0305943 -3.7920532 -2.8524821 -1.0012562 0.11194658 0.17072082][-7.4209881 -6.6390591 -5.9189405 -4.3223476 -3.1780946 -2.70745 -2.5829148 -3.0402246 -3.4162703 -3.906913 -3.9322686 -2.5413933 -0.46968961 0.43067193 0.22495604][-8.33095 -7.9164104 -7.4363985 -6.0398259 -5.0430079 -4.6980662 -4.6244011 -4.8886104 -4.9797049 -5.1425123 -5.2009048 -4.1560783 -2.5418973 -1.9998407 -2.2428319][-8.7110052 -8.7149525 -8.7848139 -7.904458 -7.1650314 -7.0296307 -7.1177006 -7.3230505 -7.3919182 -7.467598 -7.6166325 -6.8021393 -5.4054127 -4.7869062 -4.7125311][-8.0469561 -8.1258812 -8.575985 -8.2073507 -7.757844 -7.6881542 -7.7738438 -7.9463396 -8.2725325 -8.6478939 -8.922636 -8.3582516 -7.3094988 -6.7630434 -6.453537]]...]
INFO - root - 2017-12-15 06:28:56.214486: step 6910, loss = 0.29, batch loss = 0.26 (34.9 examples/sec; 0.229 sec/batch; 20h:45m:22s remains)
INFO - root - 2017-12-15 06:28:58.478584: step 6920, loss = 0.21, batch loss = 0.17 (34.9 examples/sec; 0.229 sec/batch; 20h:44m:17s remains)
INFO - root - 2017-12-15 06:29:00.741091: step 6930, loss = 0.25, batch loss = 0.21 (34.5 examples/sec; 0.232 sec/batch; 20h:57m:06s remains)
INFO - root - 2017-12-15 06:29:03.026831: step 6940, loss = 0.21, batch loss = 0.18 (34.7 examples/sec; 0.231 sec/batch; 20h:52m:13s remains)
INFO - root - 2017-12-15 06:29:05.292809: step 6950, loss = 0.47, batch loss = 0.44 (34.8 examples/sec; 0.230 sec/batch; 20h:48m:53s remains)
INFO - root - 2017-12-15 06:29:07.596020: step 6960, loss = 0.28, batch loss = 0.24 (34.1 examples/sec; 0.235 sec/batch; 21h:13m:57s remains)
INFO - root - 2017-12-15 06:29:09.887050: step 6970, loss = 0.30, batch loss = 0.26 (34.8 examples/sec; 0.230 sec/batch; 20h:47m:46s remains)
INFO - root - 2017-12-15 06:29:12.147186: step 6980, loss = 0.22, batch loss = 0.19 (35.6 examples/sec; 0.225 sec/batch; 20h:19m:01s remains)
INFO - root - 2017-12-15 06:29:14.434251: step 6990, loss = 0.21, batch loss = 0.17 (34.7 examples/sec; 0.230 sec/batch; 20h:49m:17s remains)
INFO - root - 2017-12-15 06:29:16.725116: step 7000, loss = 0.27, batch loss = 0.24 (34.7 examples/sec; 0.230 sec/batch; 20h:49m:13s remains)
2017-12-15 06:29:16.990928: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9673834 -4.2256241 -4.4495468 -5.1033425 -5.5370789 -5.8313093 -5.61491 -4.92634 -4.3856468 -4.3197494 -4.7644367 -5.3113651 -5.3101988 -4.5832129 -4.0260696][-4.8900814 -3.8685305 -4.045898 -4.6990209 -5.0402365 -5.3179173 -5.094635 -4.39382 -3.9647622 -4.2550621 -5.2258549 -6.1214962 -6.2192841 -5.4542789 -4.8528013][-4.6366425 -2.5372286 -2.7165377 -3.2877493 -3.4310184 -3.6048536 -3.3602197 -2.6602407 -2.4782691 -3.2792046 -4.8301024 -5.9590244 -6.0266814 -5.295804 -4.815258][-3.8770776 -1.2095094 -1.3930109 -1.8051631 -1.5757716 -1.3877999 -0.7995106 0.12754393 0.079682112 -1.2671008 -3.3433843 -4.6567345 -4.7728472 -4.3914685 -4.3294358][-3.1690149 -0.3160255 -0.55205524 -0.8019098 -0.17454982 0.48350263 1.6496415 3.0734274 2.9160717 1.094084 -1.3859687 -2.8338206 -3.0880618 -3.2791867 -3.7111385][-1.86907 0.77586746 0.39995623 0.24703693 1.3346207 2.5715711 4.3572569 6.26001 5.9204416 3.629873 0.79263163 -0.94030023 -1.4443297 -2.1658957 -3.0646548][-0.66185677 1.3809032 0.76738667 0.66264153 2.1953657 3.8644097 5.8853731 7.8555412 7.170723 4.466011 1.5045376 -0.40461397 -1.1055473 -2.0938196 -3.1105828][-0.88652444 0.81071472 -0.15347004 -0.33196127 1.3189263 3.0955908 4.8665218 6.3884764 5.3474817 2.6984651 0.14166188 -1.6072754 -2.2984178 -3.1182866 -3.7522933][-1.9039227 -0.74109411 -2.0870786 -2.4589183 -0.99631512 0.60120821 1.8319907 2.6290486 1.4040368 -0.761466 -2.6289382 -4.0214319 -4.4832358 -4.7425966 -4.7146416][-3.2366619 -2.456182 -3.9583678 -4.3915195 -3.2507539 -2.0061402 -1.3291605 -1.1950533 -2.3603344 -3.8858473 -5.0685396 -6.1048098 -6.2801695 -5.9537487 -5.4070392][-4.5027409 -3.8738327 -5.2664008 -5.6154346 -4.812058 -3.9915955 -3.679116 -3.8180141 -4.7097216 -5.6707916 -6.3747721 -7.0913162 -7.0748892 -6.4613714 -5.6704283][-5.0652351 -4.3718805 -5.4775438 -5.7361927 -5.2614107 -4.8390732 -4.7361355 -4.8626175 -5.3557558 -5.9061546 -6.3603086 -6.7958488 -6.6052256 -5.9326811 -5.2164326][-4.7536125 -3.9411263 -4.7419081 -4.93295 -4.6973209 -4.5383682 -4.4680786 -4.4166088 -4.5734081 -4.9289 -5.3068705 -5.5313826 -5.2366123 -4.7011633 -4.26353][-4.0802593 -3.1449237 -3.603992 -3.718492 -3.644629 -3.5977118 -3.5034008 -3.3427141 -3.3809276 -3.6849151 -4.0035992 -4.07807 -3.7977924 -3.4795115 -3.300384][-3.5939262 -2.5478463 -2.7390168 -2.8065226 -2.8145411 -2.7926092 -2.6892347 -2.5481548 -2.596169 -2.8569233 -3.0724313 -3.0741773 -2.9108181 -2.7620366 -2.69367]]...]
INFO - root - 2017-12-15 06:29:19.254525: step 7010, loss = 0.22, batch loss = 0.19 (36.3 examples/sec; 0.220 sec/batch; 19h:55m:06s remains)
INFO - root - 2017-12-15 06:29:21.545475: step 7020, loss = 0.37, batch loss = 0.33 (35.3 examples/sec; 0.227 sec/batch; 20h:29m:19s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 06:29:23.838823: step 7030, loss = 0.22, batch loss = 0.18 (34.4 examples/sec; 0.232 sec/batch; 21h:00m:12s remains)
INFO - root - 2017-12-15 06:29:26.106378: step 7040, loss = 0.39, batch loss = 0.35 (34.9 examples/sec; 0.229 sec/batch; 20h:42m:47s remains)
INFO - root - 2017-12-15 06:29:28.392432: step 7050, loss = 0.29, batch loss = 0.25 (35.2 examples/sec; 0.228 sec/batch; 20h:34m:30s remains)
INFO - root - 2017-12-15 06:29:30.672208: step 7060, loss = 0.28, batch loss = 0.24 (36.6 examples/sec; 0.218 sec/batch; 19h:44m:31s remains)
INFO - root - 2017-12-15 06:29:32.943765: step 7070, loss = 0.33, batch loss = 0.29 (36.0 examples/sec; 0.222 sec/batch; 20h:04m:32s remains)
INFO - root - 2017-12-15 06:29:35.210006: step 7080, loss = 0.25, batch loss = 0.21 (34.6 examples/sec; 0.231 sec/batch; 20h:52m:55s remains)
INFO - root - 2017-12-15 06:29:37.488477: step 7090, loss = 0.31, batch loss = 0.28 (36.0 examples/sec; 0.222 sec/batch; 20h:04m:58s remains)
INFO - root - 2017-12-15 06:29:39.763213: step 7100, loss = 0.33, batch loss = 0.29 (34.9 examples/sec; 0.229 sec/batch; 20h:44m:28s remains)
2017-12-15 06:29:40.056842: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5419106 -5.95704 -6.0708866 -6.1188545 -5.8797517 -5.5280619 -5.2828612 -5.062479 -4.8539729 -4.5868893 -4.3638067 -4.059454 -3.6464815 -3.4117603 -3.6029255][-5.3650112 -5.199966 -5.142982 -5.1669064 -4.8631239 -4.4138327 -4.3523312 -4.5559864 -4.7298083 -4.5686989 -4.1636906 -3.6083827 -2.7762887 -2.1785016 -2.2924106][-5.4449883 -4.4768414 -4.1022534 -3.974544 -3.6794686 -3.178952 -3.3145335 -3.8965538 -4.4445696 -4.5474825 -4.1474342 -3.5032814 -2.4046154 -1.5163127 -1.4588778][-5.5084963 -3.6833634 -2.6979778 -2.0793552 -1.590431 -1.1622097 -1.6552747 -2.7442064 -3.8090444 -4.3983459 -4.31176 -3.8557739 -2.7633796 -1.7994775 -1.6487225][-4.9864092 -2.624501 -0.94957924 0.35931087 1.1826198 1.6491058 0.83628535 -0.73570251 -2.3850207 -3.67479 -4.2528009 -4.1444387 -3.1923771 -2.3657153 -2.2541959][-3.8934293 -1.1565301 1.0365701 2.7474186 3.7246983 4.0844383 2.9659464 1.1387386 -0.91088414 -2.6942568 -3.8909421 -4.1989565 -3.4660196 -2.7751153 -2.71161][-2.6041667 -0.1168834 2.1912439 3.9966519 5.0318413 5.2706623 3.9623024 2.0777829 -0.05506444 -2.0144722 -3.5102983 -4.0829773 -3.4338312 -2.8631947 -2.8473697][-2.649549 -0.34602082 1.6536708 3.2728231 4.2971926 4.4722109 3.2500184 1.6018226 -0.32518363 -2.1669874 -3.6635642 -4.288888 -3.5522804 -2.9426019 -2.8676994][-3.5927582 -1.8195708 -0.2778008 1.0732458 2.02433 2.2278068 1.2752194 0.033209085 -1.4271603 -2.9426384 -4.2241497 -4.6895618 -3.8820944 -3.1866083 -2.9779954][-4.8516321 -3.6753654 -2.5932908 -1.4251595 -0.5359205 -0.22322345 -0.85649633 -1.7972281 -2.9009693 -4.1138134 -5.0833549 -5.2654209 -4.3994379 -3.6064205 -3.2202477][-5.4442911 -4.8853207 -4.3334293 -3.4631991 -2.695946 -2.32077 -2.6410558 -3.3242476 -4.1736646 -5.1109228 -5.7547531 -5.550148 -4.6892138 -3.8203106 -3.2010317][-5.551899 -5.4738793 -5.4231372 -4.9031525 -4.2714257 -3.7936258 -3.7045374 -4.0064011 -4.5355587 -5.2433147 -5.6542997 -5.3032713 -4.6469145 -3.8611913 -3.148581][-5.6353054 -5.8325062 -6.1742535 -5.9205909 -5.296854 -4.58603 -4.019978 -3.7602773 -3.8536968 -4.3482432 -4.632895 -4.4198332 -4.1786313 -3.6978731 -3.1399493][-5.6952982 -5.9531054 -6.4988947 -6.4294291 -5.86077 -5.0297375 -3.9707875 -3.149684 -2.8526163 -2.9943767 -3.2270117 -3.4239933 -3.753675 -3.7937372 -3.611557][-5.7107139 -5.7269878 -6.3062248 -6.389204 -5.9092875 -5.0045834 -3.6328712 -2.4253402 -1.7517567 -1.5878634 -1.8464447 -2.5224123 -3.4657784 -4.1228876 -4.4914694]]...]
INFO - root - 2017-12-15 06:29:42.313958: step 7110, loss = 0.41, batch loss = 0.37 (35.8 examples/sec; 0.224 sec/batch; 20h:13m:00s remains)
INFO - root - 2017-12-15 06:29:44.608605: step 7120, loss = 0.21, batch loss = 0.17 (34.6 examples/sec; 0.231 sec/batch; 20h:53m:44s remains)
INFO - root - 2017-12-15 06:29:46.905445: step 7130, loss = 0.26, batch loss = 0.22 (33.7 examples/sec; 0.238 sec/batch; 21h:29m:04s remains)
INFO - root - 2017-12-15 06:29:49.204120: step 7140, loss = 0.41, batch loss = 0.37 (35.0 examples/sec; 0.229 sec/batch; 20h:39m:33s remains)
INFO - root - 2017-12-15 06:29:51.470423: step 7150, loss = 0.22, batch loss = 0.19 (34.2 examples/sec; 0.234 sec/batch; 21h:07m:45s remains)
INFO - root - 2017-12-15 06:29:53.781533: step 7160, loss = 0.26, batch loss = 0.23 (34.0 examples/sec; 0.235 sec/batch; 21h:15m:34s remains)
INFO - root - 2017-12-15 06:29:56.119089: step 7170, loss = 0.21, batch loss = 0.17 (34.6 examples/sec; 0.231 sec/batch; 20h:54m:29s remains)
INFO - root - 2017-12-15 06:29:58.448686: step 7180, loss = 0.29, batch loss = 0.25 (35.9 examples/sec; 0.223 sec/batch; 20h:07m:35s remains)
INFO - root - 2017-12-15 06:30:00.784582: step 7190, loss = 0.25, batch loss = 0.21 (32.1 examples/sec; 0.249 sec/batch; 22h:31m:19s remains)
INFO - root - 2017-12-15 06:30:03.128856: step 7200, loss = 0.27, batch loss = 0.23 (33.4 examples/sec; 0.240 sec/batch; 21h:39m:00s remains)
2017-12-15 06:30:03.434707: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5064163 -6.8348689 -6.4805317 -5.9510312 -5.5163155 -5.2507048 -5.2655654 -5.3201828 -5.2542782 -5.2704067 -5.4491158 -5.5899763 -5.4965563 -5.270647 -5.0533476][-6.356329 -7.080399 -6.4877243 -5.7975397 -5.12109 -4.6240711 -4.7212372 -5.0212636 -5.1305761 -5.3821836 -5.8599496 -6.1558704 -6.0721464 -5.7452383 -5.348455][-7.6577282 -6.6730437 -5.7513666 -4.8895082 -3.9128656 -3.0497079 -3.0565538 -3.4721787 -3.7010651 -4.291172 -5.1260176 -5.6705203 -5.7496 -5.5042591 -5.1544733][-8.30357 -5.9364452 -4.524035 -3.4121084 -2.100359 -0.84411633 -0.58759928 -1.0731335 -1.5510215 -2.7029068 -4.0271153 -4.9148707 -5.228384 -5.1168828 -4.8604221][-8.1012249 -4.9996104 -3.0954981 -1.7152634 -0.14056408 1.3800435 1.9263477 1.3762589 0.57268262 -1.1913615 -3.0874553 -4.4815884 -5.2081337 -5.2646441 -4.985837][-7.8156047 -3.9123995 -1.6233115 -0.056563139 1.5514796 3.1825504 3.9859362 3.5551958 2.7021942 0.70267272 -1.6077335 -3.6700933 -5.0186648 -5.4523392 -5.2048345][-7.1447148 -3.510654 -1.0343734 0.64347196 2.1761761 3.719274 4.6114078 4.3330622 3.7235923 1.9367156 -0.44602263 -2.8961771 -4.6498623 -5.32609 -5.0897903][-7.4753141 -3.7812052 -1.2564912 0.49377203 1.9295745 3.3682241 4.2140989 3.9424124 3.5647416 2.1770449 -0.073419571 -2.6766381 -4.4667759 -5.1547661 -4.9385853][-8.2868595 -4.7932472 -2.3831034 -0.65100563 0.71556687 2.1105533 2.7599235 2.3495698 2.1224389 1.0882378 -1.0559416 -3.5883007 -5.0474329 -5.4976778 -5.1810026][-9.3883629 -6.2299175 -4.1400781 -2.5807252 -1.2286301 0.045819759 0.40481186 -0.15739024 -0.24728346 -1.0261538 -3.031054 -5.3158054 -6.3044481 -6.3028231 -5.8118763][-10.17075 -7.3973918 -5.7656302 -4.5550127 -3.4180036 -2.5319958 -2.5471079 -3.1350818 -3.0524387 -3.6686537 -5.5341558 -7.381413 -7.8758821 -7.4973369 -6.76321][-10.539515 -8.2349873 -7.1848836 -6.4133935 -5.6564574 -5.2223177 -5.5529156 -5.9868307 -5.7486458 -6.2802439 -7.8838568 -9.1765575 -9.31316 -8.6708584 -7.622313][-10.448395 -8.5715466 -7.9572797 -7.4387336 -6.9610839 -6.967586 -7.4779449 -7.676024 -7.3225322 -7.7891445 -9.08061 -9.9519548 -9.9654789 -9.2194023 -7.9859867][-9.628891 -8.0742512 -7.6881294 -7.3457465 -7.1362872 -7.4122534 -7.9091015 -7.8642812 -7.4621215 -7.8208704 -8.7628126 -9.3259926 -9.2896042 -8.6369 -7.4925585][-8.5168514 -7.1500454 -6.9127541 -6.6859813 -6.6095753 -6.8631954 -7.2049794 -7.0807805 -6.7992187 -7.047359 -7.6169696 -7.9292226 -7.8713007 -7.3786616 -6.5274291]]...]
INFO - root - 2017-12-15 06:30:05.752782: step 7210, loss = 0.28, batch loss = 0.25 (35.6 examples/sec; 0.225 sec/batch; 20h:17m:37s remains)
INFO - root - 2017-12-15 06:30:08.085594: step 7220, loss = 0.27, batch loss = 0.24 (34.9 examples/sec; 0.229 sec/batch; 20h:43m:23s remains)
INFO - root - 2017-12-15 06:30:10.417912: step 7230, loss = 0.39, batch loss = 0.36 (33.4 examples/sec; 0.240 sec/batch; 21h:39m:29s remains)
INFO - root - 2017-12-15 06:30:12.763246: step 7240, loss = 0.25, batch loss = 0.22 (34.2 examples/sec; 0.234 sec/batch; 21h:06m:13s remains)
INFO - root - 2017-12-15 06:30:15.075035: step 7250, loss = 0.36, batch loss = 0.33 (33.6 examples/sec; 0.238 sec/batch; 21h:30m:53s remains)
INFO - root - 2017-12-15 06:30:17.400518: step 7260, loss = 0.23, batch loss = 0.19 (34.6 examples/sec; 0.231 sec/batch; 20h:53m:56s remains)
INFO - root - 2017-12-15 06:30:19.649324: step 7270, loss = 0.36, batch loss = 0.32 (35.1 examples/sec; 0.228 sec/batch; 20h:34m:05s remains)
INFO - root - 2017-12-15 06:30:21.974513: step 7280, loss = 0.29, batch loss = 0.26 (35.5 examples/sec; 0.225 sec/batch; 20h:21m:30s remains)
INFO - root - 2017-12-15 06:30:24.293670: step 7290, loss = 0.28, batch loss = 0.24 (34.6 examples/sec; 0.231 sec/batch; 20h:52m:33s remains)
INFO - root - 2017-12-15 06:30:26.603102: step 7300, loss = 0.33, batch loss = 0.30 (35.3 examples/sec; 0.227 sec/batch; 20h:28m:22s remains)
2017-12-15 06:30:26.914691: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.73762488 -1.9291222 -2.1042175 -2.3214939 -2.3737593 -2.2596359 -2.3592386 -2.7071853 -2.8400848 -2.6284394 -2.4073431 -2.4180183 -2.6082013 -2.4048207 -1.9261847][-2.0320091 -2.4248822 -2.5131328 -2.7697034 -2.838223 -2.6496959 -2.7807264 -3.3181469 -3.4828198 -3.1863651 -2.9733615 -3.1300206 -3.5354457 -3.2628479 -2.4157474][-3.1774449 -2.5846703 -2.468771 -2.6370034 -2.6759682 -2.5271997 -2.8955348 -3.7902579 -4.1095715 -3.8038294 -3.5875864 -3.758127 -4.2353582 -4.0460877 -3.1634746][-4.1414385 -2.6450973 -2.2559688 -2.2270031 -2.1750479 -1.910386 -2.2889326 -3.3655486 -3.9978762 -4.1051283 -4.2131195 -4.3855181 -4.7086411 -4.5571461 -3.8862233][-5.0474834 -2.7063622 -2.0121171 -1.7221314 -1.4062669 -0.67652488 -0.47927213 -1.2464273 -2.3287361 -3.4022355 -4.3171926 -4.7212892 -4.8199673 -4.5723848 -4.0741463][-5.2994261 -2.9160309 -2.0731542 -1.6259104 -0.99198008 0.43302298 1.5995424 1.5528765 0.11076117 -2.0607171 -3.9777145 -4.8618121 -4.9131336 -4.5074649 -3.91741][-5.1865711 -3.3183742 -2.5907476 -2.0511284 -0.99702609 1.279839 3.484174 4.2349253 2.8166449 -0.025823116 -2.7154274 -4.2677727 -4.7352791 -4.4466958 -3.7630486][-5.2884874 -3.5037031 -3.1048322 -2.6551361 -1.3609569 1.2832193 3.9186313 5.1578159 4.2577524 1.6748309 -1.0670483 -2.9998679 -4.0401011 -4.1135387 -3.4766028][-4.7326455 -3.1036661 -3.085537 -3.0441415 -2.0551469 0.2063756 2.3787472 3.6589715 3.6655738 2.2885978 0.44540644 -1.2459247 -2.6210008 -3.2034655 -2.9294446][-4.1707273 -2.3653452 -2.5509841 -2.9375472 -2.5323391 -1.0770457 0.12781072 0.91818523 1.6344519 1.5431502 0.84252882 -0.21989691 -1.4618154 -2.1717083 -2.1757023][-4.1827207 -1.988368 -1.9697217 -2.485889 -2.5246756 -1.9657569 -1.8025098 -1.7236456 -0.82031703 -0.22316325 -0.25101161 -0.79324532 -1.6734941 -2.0347855 -1.8658274][-4.6959715 -2.3292217 -1.935143 -2.217241 -2.3812692 -2.3907182 -2.9622319 -3.4575787 -2.7025628 -1.8155379 -1.5115825 -1.84495 -2.5832505 -2.7498186 -2.3557906][-5.7882328 -3.6403363 -3.0850813 -2.9531567 -2.8449571 -2.8627241 -3.6487074 -4.5407133 -4.20568 -3.3493633 -2.8717666 -3.0168495 -3.5834253 -3.6534839 -3.144778][-6.2685776 -4.5978951 -4.1171331 -3.6369312 -3.117022 -2.9275031 -3.5684192 -4.5094261 -4.6110034 -4.1308346 -3.7073789 -3.5955606 -3.8216615 -3.7304664 -3.3179166][-5.557797 -4.4337845 -4.2168884 -3.7080183 -3.0327871 -2.715301 -3.0772772 -3.7377257 -4.1135869 -4.1872959 -4.1243629 -3.9828551 -3.8962276 -3.695441 -3.3897204]]...]
INFO - root - 2017-12-15 06:30:29.225534: step 7310, loss = 0.27, batch loss = 0.23 (34.7 examples/sec; 0.231 sec/batch; 20h:51m:19s remains)
INFO - root - 2017-12-15 06:30:31.543668: step 7320, loss = 0.23, batch loss = 0.19 (34.0 examples/sec; 0.236 sec/batch; 21h:16m:49s remains)
INFO - root - 2017-12-15 06:30:33.883527: step 7330, loss = 0.22, batch loss = 0.18 (35.0 examples/sec; 0.229 sec/batch; 20h:40m:07s remains)
INFO - root - 2017-12-15 06:30:36.202921: step 7340, loss = 0.26, batch loss = 0.22 (35.7 examples/sec; 0.224 sec/batch; 20h:13m:29s remains)
INFO - root - 2017-12-15 06:30:38.494653: step 7350, loss = 0.34, batch loss = 0.30 (36.3 examples/sec; 0.221 sec/batch; 19h:55m:06s remains)
INFO - root - 2017-12-15 06:30:40.792615: step 7360, loss = 0.28, batch loss = 0.24 (34.9 examples/sec; 0.230 sec/batch; 20h:43m:52s remains)
INFO - root - 2017-12-15 06:30:43.065432: step 7370, loss = 0.24, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 20h:04m:38s remains)
INFO - root - 2017-12-15 06:30:45.340924: step 7380, loss = 0.29, batch loss = 0.25 (36.0 examples/sec; 0.222 sec/batch; 20h:05m:30s remains)
INFO - root - 2017-12-15 06:30:47.650754: step 7390, loss = 0.21, batch loss = 0.17 (35.8 examples/sec; 0.224 sec/batch; 20h:11m:08s remains)
INFO - root - 2017-12-15 06:30:49.901348: step 7400, loss = 0.25, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 20h:15m:40s remains)
2017-12-15 06:30:50.262088: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8001988 -2.046834 -1.9376938 -1.9893056 -2.1134794 -2.3825142 -2.9579031 -3.6438735 -4.2751713 -4.5360303 -4.28041 -3.9817286 -3.9532244 -3.92485 -3.6775537][-2.2615168 -1.1714835 -0.96189678 -1.0423704 -1.1610416 -1.4525416 -2.0660167 -2.8321931 -3.5437586 -3.9148526 -3.7018659 -3.4184141 -3.4309433 -3.5239418 -3.2934327][-3.1000781 -1.1945177 -0.90028691 -1.000581 -1.1174181 -1.3856051 -1.8729795 -2.4932444 -3.0950809 -3.4642353 -3.3363252 -3.0943916 -3.1609697 -3.4424622 -3.2415416][-3.9950824 -1.868715 -1.4232445 -1.4152441 -1.42668 -1.6070826 -1.7985632 -2.0778348 -2.4009912 -2.7144923 -2.685601 -2.4834785 -2.6602988 -3.1889486 -3.1017313][-4.7024884 -1.9862074 -1.3909496 -1.2321419 -1.0925083 -1.1562612 -0.909142 -0.73835433 -0.75914991 -1.1380042 -1.3413167 -1.275918 -1.6266497 -2.3707919 -2.3867457][-4.4757524 -1.4892952 -0.78235161 -0.53339088 -0.41845703 -0.41936874 0.17914295 0.55723119 0.56637669 -0.042410135 -0.63729882 -0.77916265 -1.1739416 -1.9412056 -1.945982][-3.5488477 -1.1959522 -0.4893682 -0.24900591 -0.19920349 -0.11703181 0.68736863 1.0575156 1.0022683 0.2350831 -0.63468313 -0.8737781 -1.1936193 -1.7400939 -1.6886147][-3.6627302 -1.3699633 -0.68911719 -0.35983598 -0.25104463 -0.020931482 0.90343642 1.2377737 1.1477575 0.35081816 -0.56924117 -0.79281127 -0.90338969 -1.0655439 -1.001066][-4.254076 -2.0541658 -1.362675 -0.88355124 -0.65968573 -0.26649642 0.56809497 0.60225534 0.34516525 -0.42061412 -1.2563391 -1.4223866 -1.2350984 -0.91554105 -0.75012994][-5.1677508 -3.1338525 -2.5071311 -1.9601712 -1.6877527 -1.2318001 -0.62380815 -0.92589843 -1.33091 -1.9745625 -2.6429179 -2.7417147 -2.2865095 -1.5306172 -1.2244557][-5.6272397 -3.8938789 -3.5148678 -3.1096532 -2.9099805 -2.5225773 -2.1726 -2.5422208 -2.8586164 -3.2514939 -3.7070513 -3.7808027 -3.2497058 -2.2336123 -1.7640616][-5.9707136 -4.6355629 -4.5050292 -4.2772369 -4.110981 -3.7172568 -3.4756074 -3.7643418 -3.963829 -4.178504 -4.4827943 -4.5596719 -4.0464048 -3.0073633 -2.4841692][-6.020761 -4.9112024 -4.8637261 -4.6984167 -4.4931192 -4.0963678 -3.9134269 -4.179759 -4.3473177 -4.496748 -4.6903553 -4.7338486 -4.3420606 -3.4937487 -3.0240126][-5.7047215 -4.705894 -4.6626339 -4.5300388 -4.3255281 -4.057128 -4.0130448 -4.2262983 -4.350471 -4.4246187 -4.4953289 -4.5129929 -4.3244853 -3.8109848 -3.4208617][-5.0853386 -4.1980786 -4.2072058 -4.18184 -4.1078181 -4.0365124 -4.0779157 -4.1656017 -4.2030621 -4.2255673 -4.2471981 -4.3007345 -4.3394661 -4.1458449 -3.8690107]]...]
INFO - root - 2017-12-15 06:30:52.579005: step 7410, loss = 0.31, batch loss = 0.28 (32.9 examples/sec; 0.243 sec/batch; 21h:57m:37s remains)
INFO - root - 2017-12-15 06:30:54.904217: step 7420, loss = 0.21, batch loss = 0.17 (36.7 examples/sec; 0.218 sec/batch; 19h:39m:26s remains)
INFO - root - 2017-12-15 06:30:57.166593: step 7430, loss = 0.44, batch loss = 0.40 (35.9 examples/sec; 0.223 sec/batch; 20h:05m:42s remains)
INFO - root - 2017-12-15 06:30:59.450099: step 7440, loss = 0.33, batch loss = 0.29 (34.3 examples/sec; 0.233 sec/batch; 21h:03m:46s remains)
INFO - root - 2017-12-15 06:31:01.735375: step 7450, loss = 0.25, batch loss = 0.22 (36.5 examples/sec; 0.219 sec/batch; 19h:48m:15s remains)
INFO - root - 2017-12-15 06:31:04.016129: step 7460, loss = 0.32, batch loss = 0.28 (34.9 examples/sec; 0.229 sec/batch; 20h:42m:09s remains)
INFO - root - 2017-12-15 06:31:06.313023: step 7470, loss = 0.36, batch loss = 0.33 (34.7 examples/sec; 0.231 sec/batch; 20h:49m:25s remains)
INFO - root - 2017-12-15 06:31:08.656965: step 7480, loss = 0.23, batch loss = 0.19 (32.2 examples/sec; 0.249 sec/batch; 22h:26m:47s remains)
INFO - root - 2017-12-15 06:31:10.950568: step 7490, loss = 0.30, batch loss = 0.27 (35.4 examples/sec; 0.226 sec/batch; 20h:24m:47s remains)
INFO - root - 2017-12-15 06:31:13.296440: step 7500, loss = 0.26, batch loss = 0.22 (32.5 examples/sec; 0.246 sec/batch; 22h:11m:50s remains)
2017-12-15 06:31:13.676393: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2367544 -5.1436205 -5.0515985 -4.8899479 -4.7329569 -4.628293 -4.7198133 -5.0082088 -5.2493773 -5.4012489 -5.3807306 -4.8341303 -4.0111933 -3.5040431 -3.6501887][-5.484314 -5.5720797 -5.4375677 -5.255455 -5.0584583 -4.9548225 -4.973043 -5.17626 -5.3637905 -5.373 -5.0697517 -4.3967381 -3.6825039 -3.2425554 -3.3712962][-5.4458361 -5.042943 -4.8260508 -4.5818319 -4.2718029 -4.1387906 -4.1371536 -4.306366 -4.4873295 -4.5350924 -4.2121496 -3.6160543 -2.9850574 -2.4476967 -2.4937291][-4.89736 -4.1493726 -3.8336334 -3.5020709 -2.9834242 -2.6334233 -2.4797835 -2.6860025 -3.0898371 -3.4626708 -3.4558539 -3.1493883 -2.6007023 -1.9908574 -1.7750551][-4.2860227 -3.3279214 -2.9852958 -2.5801082 -1.8189722 -1.059155 -0.59234715 -0.83979738 -1.5808418 -2.4382572 -2.8742228 -2.9994273 -2.5699308 -1.7689519 -1.2589421][-3.7396717 -2.6762195 -2.2575297 -1.6824304 -0.54687774 0.71328139 1.5539544 1.2612076 0.084913731 -1.3411644 -2.3488321 -2.9481823 -2.6756978 -1.7227466 -0.99184453][-3.5391793 -2.7023296 -2.0706983 -1.320714 0.068930387 1.7795613 3.1405256 3.1156242 1.7908757 -0.18272412 -1.7347231 -2.6367695 -2.5450501 -1.710459 -1.0337203][-4.2667971 -2.9749513 -2.1707087 -1.3817749 -0.052277088 1.6688433 3.309068 3.8069818 2.7638891 0.62082934 -1.0929877 -1.9996349 -2.0877333 -1.5785594 -1.1708089][-4.8332863 -3.152792 -2.1428356 -1.3285711 -0.30259454 0.86112022 2.1482012 2.9389837 2.419996 0.57395506 -0.8246218 -1.3967509 -1.3977041 -1.0007321 -0.89672828][-5.4559784 -3.4576688 -2.2460473 -1.3638721 -0.65374732 -0.14741421 0.58613467 1.3091242 1.0992019 -0.31196094 -1.2971656 -1.6272206 -1.584897 -1.2598895 -1.2994452][-5.7868886 -3.84015 -2.6540697 -1.7385049 -1.3105001 -1.4067653 -1.2499185 -0.76054275 -0.79927576 -1.7673848 -2.272121 -2.4089198 -2.3851085 -2.1490808 -2.2190046][-5.4156361 -3.6845045 -2.7191992 -1.9119699 -1.6398152 -1.9974959 -2.1242454 -1.904654 -2.0632694 -2.7160308 -2.9344342 -3.0730824 -3.1615906 -2.9561636 -2.8692281][-4.977211 -3.6612177 -3.0586867 -2.4908268 -2.3843629 -2.7484527 -2.9253881 -2.8813715 -3.1143796 -3.5631742 -3.712378 -3.9538643 -4.0760345 -3.7938786 -3.4491439][-4.6747 -3.7313147 -3.5674496 -3.4648738 -3.625926 -3.9261603 -4.0328984 -4.0110369 -4.1941395 -4.3461628 -4.37207 -4.6985378 -4.8978195 -4.6195078 -4.1276989][-4.1814957 -3.4154599 -3.4994485 -3.6645098 -3.9495754 -4.23194 -4.3415327 -4.3369131 -4.3770423 -4.334435 -4.2863197 -4.5406084 -4.7119284 -4.4747372 -3.99641]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-7500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-7500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 06:31:16.331683: step 7510, loss = 0.28, batch loss = 0.24 (34.8 examples/sec; 0.230 sec/batch; 20h:45m:28s remains)
INFO - root - 2017-12-15 06:31:18.602921: step 7520, loss = 0.28, batch loss = 0.25 (34.6 examples/sec; 0.232 sec/batch; 20h:54m:02s remains)
INFO - root - 2017-12-15 06:31:20.932322: step 7530, loss = 0.33, batch loss = 0.29 (35.4 examples/sec; 0.226 sec/batch; 20h:22m:43s remains)
INFO - root - 2017-12-15 06:31:23.191232: step 7540, loss = 0.31, batch loss = 0.27 (35.1 examples/sec; 0.228 sec/batch; 20h:32m:45s remains)
INFO - root - 2017-12-15 06:31:25.513984: step 7550, loss = 0.43, batch loss = 0.39 (35.3 examples/sec; 0.227 sec/batch; 20h:29m:00s remains)
INFO - root - 2017-12-15 06:31:27.798404: step 7560, loss = 0.29, batch loss = 0.26 (34.4 examples/sec; 0.233 sec/batch; 20h:59m:15s remains)
INFO - root - 2017-12-15 06:31:30.051921: step 7570, loss = 0.53, batch loss = 0.49 (34.2 examples/sec; 0.234 sec/batch; 21h:07m:12s remains)
INFO - root - 2017-12-15 06:31:32.396029: step 7580, loss = 0.31, batch loss = 0.28 (35.9 examples/sec; 0.223 sec/batch; 20h:06m:58s remains)
INFO - root - 2017-12-15 06:31:34.670577: step 7590, loss = 0.31, batch loss = 0.27 (35.7 examples/sec; 0.224 sec/batch; 20h:14m:33s remains)
INFO - root - 2017-12-15 06:31:36.940726: step 7600, loss = 0.27, batch loss = 0.23 (36.1 examples/sec; 0.222 sec/batch; 19h:59m:30s remains)
2017-12-15 06:31:37.255905: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1324267 -8.3599405 -8.9525194 -8.9091282 -8.6086931 -8.7454643 -8.845685 -8.733819 -8.6536732 -8.7016172 -8.5572805 -8.1516266 -7.8806219 -8.1728525 -8.596489][-6.4474678 -8.9045343 -9.6588249 -9.4681416 -8.9538879 -9.0500937 -9.0961952 -8.8853779 -8.7211666 -8.8805456 -8.7593508 -8.2874413 -7.9599466 -8.2195759 -8.6851463][-7.1101046 -8.5064306 -9.2863178 -8.8087072 -7.9727888 -7.9079075 -7.817131 -7.4344683 -7.1669254 -7.4809237 -7.5273333 -7.1724873 -6.8025751 -7.0234728 -7.5646105][-7.3627796 -7.8948936 -8.5600662 -7.6068172 -6.275631 -5.9283018 -5.6390905 -5.0160484 -4.6115746 -5.0917664 -5.4276123 -5.2315521 -4.6487293 -4.7321057 -5.4260187][-7.5201139 -7.2810526 -7.7169237 -6.2068453 -4.2679548 -3.4965334 -2.961133 -2.1427331 -1.6638242 -2.3041849 -3.0578952 -3.0207841 -2.051265 -1.8471049 -2.7287459][-7.2315969 -6.7242374 -6.8108358 -4.7445769 -2.1153135 -0.70557714 0.13282037 0.94361448 1.1869695 0.33591795 -0.678769 -0.64685273 0.666414 1.0719209 0.045348167][-6.7550364 -6.5121164 -6.32104 -3.913547 -0.71928692 1.3644509 2.5315707 3.124088 2.8728197 1.8440881 0.82936311 0.91406775 2.3621223 2.8673666 1.7981627][-6.986289 -6.5338793 -6.2573967 -3.8477015 -0.44603539 2.070648 3.4727037 3.7702768 2.9542215 1.882024 1.180243 1.4599283 2.8223598 3.2965767 2.2759283][-7.1516037 -6.7200708 -6.5419931 -4.4227324 -1.1982433 1.3532467 2.7832916 2.8520334 1.7319529 0.7916162 0.592541 1.0541816 2.1241972 2.4588263 1.670176][-7.2138996 -6.8254862 -6.8191681 -5.1318521 -2.3063271 -0.0057964325 1.2924631 1.1558518 -0.11420226 -0.90393507 -0.77873218 -0.41148293 0.24483585 0.45913744 0.1156826][-7.18608 -6.7995729 -6.9154835 -5.656476 -3.2277474 -1.1981585 -0.13178432 -0.44534683 -1.77384 -2.5597436 -2.5216427 -2.3757427 -1.9879735 -1.7274578 -1.5731795][-7.2782335 -6.8979664 -7.0957747 -6.2063322 -4.1126027 -2.3081334 -1.4499638 -1.7841202 -2.9000468 -3.7117624 -3.9673185 -4.0605822 -3.70076 -3.2885103 -2.7064936][-7.5007868 -7.1665192 -7.4299746 -6.8313084 -5.0585613 -3.4797652 -2.7801948 -3.0062234 -3.791431 -4.4576039 -4.9718022 -5.3710265 -4.9923978 -4.3728962 -3.5521598][-7.8698807 -7.6206284 -7.9796057 -7.5535116 -5.9468427 -4.441453 -3.79757 -3.826647 -4.1436834 -4.5396242 -5.2728577 -5.9649935 -5.6040835 -4.9463806 -4.0761094][-8.2124653 -8.065836 -8.6020012 -8.3644762 -6.8699808 -5.4234605 -4.749444 -4.5032673 -4.3374567 -4.4671345 -5.3079214 -6.1822968 -5.9137907 -5.3846169 -4.6524472]]...]
INFO - root - 2017-12-15 06:31:39.558468: step 7610, loss = 0.25, batch loss = 0.22 (35.6 examples/sec; 0.225 sec/batch; 20h:15m:47s remains)
INFO - root - 2017-12-15 06:31:41.866302: step 7620, loss = 0.23, batch loss = 0.20 (34.8 examples/sec; 0.230 sec/batch; 20h:43m:56s remains)
INFO - root - 2017-12-15 06:31:44.122251: step 7630, loss = 0.31, batch loss = 0.27 (36.4 examples/sec; 0.220 sec/batch; 19h:49m:21s remains)
INFO - root - 2017-12-15 06:31:46.409929: step 7640, loss = 0.28, batch loss = 0.24 (35.8 examples/sec; 0.224 sec/batch; 20h:10m:07s remains)
INFO - root - 2017-12-15 06:31:48.673891: step 7650, loss = 0.25, batch loss = 0.22 (33.4 examples/sec; 0.239 sec/batch; 21h:35m:12s remains)
INFO - root - 2017-12-15 06:31:50.954461: step 7660, loss = 0.36, batch loss = 0.33 (35.4 examples/sec; 0.226 sec/batch; 20h:22m:37s remains)
INFO - root - 2017-12-15 06:31:53.240725: step 7670, loss = 0.17, batch loss = 0.14 (33.9 examples/sec; 0.236 sec/batch; 21h:16m:20s remains)
INFO - root - 2017-12-15 06:31:55.521128: step 7680, loss = 0.33, batch loss = 0.29 (34.4 examples/sec; 0.233 sec/batch; 21h:00m:24s remains)
INFO - root - 2017-12-15 06:31:57.815932: step 7690, loss = 0.24, batch loss = 0.20 (34.9 examples/sec; 0.229 sec/batch; 20h:40m:08s remains)
INFO - root - 2017-12-15 06:32:00.055033: step 7700, loss = 0.36, batch loss = 0.33 (36.7 examples/sec; 0.218 sec/batch; 19h:41m:25s remains)
2017-12-15 06:32:00.392286: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8160381 -6.7506342 -7.0996618 -7.3816433 -7.0170822 -6.729188 -6.7461348 -7.0124664 -7.3617115 -7.6653891 -7.5935812 -7.0358438 -6.292841 -5.408926 -4.5329533][-6.4247417 -6.1227608 -6.8138418 -7.3965044 -7.1007853 -6.8852072 -6.794898 -6.8642178 -7.0711794 -7.4166718 -7.4213161 -6.9397888 -6.3109865 -5.6671352 -4.813735][-6.649457 -4.8569469 -5.877737 -6.69267 -6.5247135 -6.4014282 -6.1709023 -5.9392905 -5.9245071 -6.0975051 -6.046401 -5.589766 -5.0933485 -4.72542 -4.0240622][-6.5888624 -4.24137 -5.5240121 -6.2704582 -5.9350882 -5.66823 -5.2211266 -4.8310204 -4.6889892 -4.6967592 -4.6601429 -4.3763618 -4.0614452 -3.9556732 -3.5036457][-5.3665247 -3.1644201 -4.4198103 -4.662365 -3.7712593 -3.2397192 -2.6829653 -2.4976692 -2.8398063 -3.1324587 -3.40521 -3.4605925 -3.4136152 -3.4420419 -3.1163249][-4.1350546 -2.2248549 -3.1676521 -2.6769361 -1.1509402 -0.30177259 0.41344333 0.22857118 -0.76623523 -1.5626893 -2.2843301 -2.7880993 -3.0102727 -3.1903565 -3.0301991][-2.9703074 -1.9221901 -2.3594632 -1.1332207 0.84032559 1.961766 2.8382394 2.476505 1.0553877 -0.26486897 -1.4591429 -2.5085957 -3.1656728 -3.5234561 -3.4505374][-2.8580115 -1.8666904 -1.8959714 -0.20696962 1.9023554 3.2235777 4.2973089 3.8723023 2.2451522 0.4932251 -1.030663 -2.3873169 -3.3166952 -3.8511963 -4.0168591][-2.9874761 -2.2536597 -2.10459 -0.33246076 1.6263325 3.0023348 4.1408987 3.6915743 1.9786632 0.043251276 -1.5167476 -2.8639746 -3.8639088 -4.4302659 -4.6177187][-3.1723137 -2.753047 -2.9222841 -1.6686563 -0.2774899 1.0133967 2.0009801 1.4410172 -0.052033186 -1.6177932 -2.7638564 -3.8069432 -4.6720948 -5.148541 -5.2380142][-2.8060482 -2.8381298 -3.5166907 -3.0416274 -2.3563991 -1.5430067 -1.0028651 -1.6582243 -2.7930856 -3.7737794 -4.3777838 -4.9856853 -5.5540695 -5.8286705 -5.8056474][-2.4541445 -2.8089898 -3.9090276 -4.0364494 -3.9213631 -3.6821237 -3.8250327 -4.6397176 -5.441071 -5.8991442 -5.9904051 -6.046628 -6.1176472 -6.162138 -6.1401062][-3.0604255 -3.4517035 -4.6899891 -5.044425 -5.245759 -5.4656477 -6.125577 -6.935914 -7.3324575 -7.2540884 -6.8621578 -6.4714127 -6.2682047 -6.1728582 -6.1029339][-4.3285322 -4.4709148 -5.5877028 -5.8824043 -6.1880226 -6.5797806 -7.3288527 -7.8684893 -7.9101973 -7.6063461 -7.0771 -6.5721025 -6.2960739 -6.1867418 -6.1365328][-5.4290495 -5.1053057 -6.0112906 -6.1642313 -6.307333 -6.6724243 -7.2406578 -7.47435 -7.3239212 -7.0125751 -6.5599756 -6.1030602 -5.8529778 -5.7429523 -5.6463275]]...]
INFO - root - 2017-12-15 06:32:02.652634: step 7710, loss = 0.26, batch loss = 0.23 (35.3 examples/sec; 0.226 sec/batch; 20h:25m:55s remains)
INFO - root - 2017-12-15 06:32:04.937356: step 7720, loss = 0.36, batch loss = 0.32 (35.0 examples/sec; 0.228 sec/batch; 20h:36m:20s remains)
INFO - root - 2017-12-15 06:32:07.212343: step 7730, loss = 0.32, batch loss = 0.28 (34.0 examples/sec; 0.235 sec/batch; 21h:13m:52s remains)
INFO - root - 2017-12-15 06:32:09.495282: step 7740, loss = 0.35, batch loss = 0.32 (34.6 examples/sec; 0.231 sec/batch; 20h:51m:41s remains)
INFO - root - 2017-12-15 06:32:11.792982: step 7750, loss = 0.28, batch loss = 0.24 (35.1 examples/sec; 0.228 sec/batch; 20h:33m:25s remains)
INFO - root - 2017-12-15 06:32:14.036221: step 7760, loss = 0.44, batch loss = 0.40 (37.1 examples/sec; 0.216 sec/batch; 19h:28m:28s remains)
INFO - root - 2017-12-15 06:32:16.302678: step 7770, loss = 0.29, batch loss = 0.25 (35.9 examples/sec; 0.223 sec/batch; 20h:05m:48s remains)
INFO - root - 2017-12-15 06:32:18.545851: step 7780, loss = 0.29, batch loss = 0.26 (34.7 examples/sec; 0.230 sec/batch; 20h:47m:13s remains)
INFO - root - 2017-12-15 06:32:20.872995: step 7790, loss = 0.34, batch loss = 0.30 (35.9 examples/sec; 0.223 sec/batch; 20h:05m:15s remains)
INFO - root - 2017-12-15 06:32:23.140047: step 7800, loss = 0.27, batch loss = 0.23 (36.1 examples/sec; 0.222 sec/batch; 20h:00m:01s remains)
2017-12-15 06:32:23.536879: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8853962 -4.3307152 -4.9524708 -5.2356095 -5.2213793 -5.1756883 -5.1606545 -4.9464054 -4.6125255 -4.4475985 -4.3346062 -4.2784572 -4.2075109 -4.1472063 -4.0446348][-2.196928 -4.208559 -4.9908113 -5.4523325 -5.52456 -5.4503989 -5.3709993 -5.0615869 -4.5956516 -4.4246464 -4.4013948 -4.4540062 -4.2908525 -3.9970765 -3.7917695][-2.1870875 -3.7760191 -4.7659616 -5.4635339 -5.6315694 -5.4713011 -5.2980037 -4.9071178 -4.338273 -4.1381149 -4.2236919 -4.413353 -4.2638688 -3.7868013 -3.444623][-1.8692822 -3.0476322 -4.0397177 -4.6915474 -4.7688804 -4.5136633 -4.2947855 -3.92151 -3.5180321 -3.5707037 -3.9779711 -4.3501606 -4.3190508 -3.7581985 -3.1933422][-1.7653791 -2.5388291 -3.2146828 -3.3562541 -2.9574432 -2.2789924 -1.7309074 -1.2838192 -1.2006295 -1.9003644 -2.9396698 -3.736609 -4.1240034 -3.8025005 -3.2785051][-1.9889346 -2.4483831 -2.7913485 -2.4404197 -1.5243975 -0.25226688 0.92070913 1.711719 1.5145395 -0.00014185905 -1.781958 -3.1548896 -4.1086645 -4.1803865 -3.8428268][-2.0452931 -2.2757249 -2.3436954 -1.6727324 -0.42096758 1.3424559 3.1273582 4.1811628 3.6180489 1.3043022 -1.1694281 -2.9299548 -4.1987476 -4.5107536 -4.2195425][-2.8263237 -2.7103672 -2.4568198 -1.3948658 0.24323344 2.5818651 5.0084648 6.3782511 5.5189352 2.5989459 -0.36600029 -2.5021422 -3.97963 -4.5074539 -4.3003144][-3.9165072 -3.7491097 -3.5007014 -2.3665941 -0.6021452 1.9988215 4.8120136 6.3866262 5.5629311 2.5686638 -0.47313428 -2.7119505 -4.1365142 -4.6347389 -4.3234282][-4.7639966 -4.6059093 -4.6169238 -3.8842707 -2.5177059 -0.2530545 2.2911217 3.7146046 3.0141246 0.53260279 -2.0521975 -3.9335313 -4.8797736 -4.9924841 -4.2994804][-5.0170059 -4.7969828 -5.0642157 -4.7777724 -3.8830054 -2.2196703 -0.23955119 0.82351136 0.31791377 -1.4233255 -3.3580236 -4.7763786 -5.3055573 -5.1168513 -4.2844734][-4.8712225 -4.8632007 -5.5513554 -5.7515044 -5.1854067 -3.9253166 -2.4847696 -1.7119513 -1.8610775 -2.8221586 -4.066319 -5.0664611 -5.3578835 -5.1119037 -4.38451][-4.2612967 -4.4227219 -5.470191 -6.101203 -5.8193507 -4.9150963 -3.954824 -3.5187953 -3.4851718 -3.823781 -4.4736052 -5.0113134 -5.1026783 -4.8695278 -4.3549328][-3.4494004 -3.6949191 -4.9067583 -5.7216263 -5.5902948 -4.7979927 -4.1160307 -4.0042181 -4.0591063 -4.1487265 -4.4490986 -4.73341 -4.7870817 -4.6828294 -4.4489388][-3.0373061 -3.4349284 -4.837162 -5.7129612 -5.6250019 -4.82196 -4.2233562 -4.2282953 -4.288928 -4.1907139 -4.2327251 -4.358057 -4.4184155 -4.4246583 -4.4155078]]...]
INFO - root - 2017-12-15 06:32:25.807681: step 7810, loss = 0.22, batch loss = 0.18 (36.5 examples/sec; 0.219 sec/batch; 19h:47m:38s remains)
INFO - root - 2017-12-15 06:32:28.119364: step 7820, loss = 0.19, batch loss = 0.15 (35.7 examples/sec; 0.224 sec/batch; 20h:14m:09s remains)
INFO - root - 2017-12-15 06:32:30.365580: step 7830, loss = 0.23, batch loss = 0.19 (35.8 examples/sec; 0.223 sec/batch; 20h:08m:39s remains)
INFO - root - 2017-12-15 06:32:32.688482: step 7840, loss = 0.23, batch loss = 0.19 (33.3 examples/sec; 0.240 sec/batch; 21h:39m:56s remains)
INFO - root - 2017-12-15 06:32:34.951185: step 7850, loss = 0.27, batch loss = 0.23 (36.1 examples/sec; 0.221 sec/batch; 19h:57m:54s remains)
INFO - root - 2017-12-15 06:32:37.214673: step 7860, loss = 0.24, batch loss = 0.20 (36.5 examples/sec; 0.219 sec/batch; 19h:45m:59s remains)
INFO - root - 2017-12-15 06:32:39.451867: step 7870, loss = 0.28, batch loss = 0.24 (35.4 examples/sec; 0.226 sec/batch; 20h:21m:52s remains)
INFO - root - 2017-12-15 06:32:41.765998: step 7880, loss = 0.33, batch loss = 0.29 (35.4 examples/sec; 0.226 sec/batch; 20h:23m:47s remains)
INFO - root - 2017-12-15 06:32:44.030593: step 7890, loss = 0.43, batch loss = 0.39 (34.7 examples/sec; 0.230 sec/batch; 20h:46m:06s remains)
INFO - root - 2017-12-15 06:32:46.295374: step 7900, loss = 0.31, batch loss = 0.27 (36.3 examples/sec; 0.220 sec/batch; 19h:52m:51s remains)
2017-12-15 06:32:46.617623: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3869605 -4.3655963 -3.9792185 -4.2245169 -4.827487 -4.9158115 -4.7397995 -4.5071869 -4.5094762 -4.582058 -4.7551203 -4.8767929 -4.9044371 -4.698946 -4.3210177][-4.9027939 -4.1146431 -3.92096 -4.42642 -5.3082438 -5.4107647 -4.9987383 -4.4849162 -4.327692 -4.3910646 -4.734457 -5.1245203 -5.3850641 -5.3090839 -4.9255319][-5.1368666 -3.5781972 -3.6339869 -4.3566494 -5.2063484 -5.2536755 -4.6472244 -3.8971009 -3.5643854 -3.6397114 -4.1703753 -4.9074183 -5.516386 -5.7382669 -5.4477377][-5.0759358 -2.7850559 -3.0861173 -3.958158 -4.7912579 -4.6625051 -3.8549089 -2.9453783 -2.5110202 -2.6406107 -3.356936 -4.3810978 -5.3039303 -5.8445892 -5.810689][-3.7623327 -1.630584 -2.1531744 -3.1995032 -3.9942956 -3.8100553 -3.00077 -2.0917187 -1.5744725 -1.7176569 -2.4455867 -3.571548 -4.560112 -5.2390566 -5.3778706][-2.65557 -0.58634424 -1.2067875 -2.3725166 -3.1535416 -2.94505 -2.2740769 -1.4972053 -0.98846424 -1.1294849 -1.804611 -2.8629994 -3.7923908 -4.4054303 -4.55838][-1.3880024 -0.09157753 -0.73804629 -1.8419428 -2.3977365 -2.1489391 -1.6651419 -1.167354 -0.79304469 -0.93523431 -1.4998837 -2.4299343 -3.2067006 -3.7615795 -3.9053168][-0.87928665 0.57822943 -0.029905558 -1.000067 -1.44453 -1.4203742 -1.269087 -1.1047769 -0.87612641 -0.9732126 -1.3939613 -2.0253613 -2.4664586 -2.7942491 -2.7908087][-0.18637443 1.2833171 0.74372172 -0.11444712 -0.67938018 -1.0357035 -1.1627363 -1.1721113 -1.0037614 -1.0656298 -1.3473909 -1.6096852 -1.565938 -1.4951956 -1.2832649][0.11386251 1.8553662 1.4371529 0.50924969 -0.40511978 -1.1687832 -1.3996398 -1.4144869 -1.2598518 -1.2647765 -1.3578689 -1.246609 -0.832366 -0.43149507 0.050016642][-0.068648815 2.0565863 1.7160816 0.7016294 -0.57015884 -1.7130371 -2.0470917 -1.9949527 -1.7687579 -1.6112885 -1.4389471 -0.97577631 -0.35136747 0.23310804 0.88154912][-0.86779332 1.6537883 1.4606793 0.43683386 -1.0619524 -2.4870315 -2.9479403 -2.9514129 -2.6332686 -2.2314987 -1.7526579 -1.0652174 -0.37019062 0.21871734 1.0469236][-2.5656559 0.12162018 0.12598681 -0.65122151 -2.0709913 -3.5925541 -4.1941414 -4.2608385 -3.8036876 -3.1393433 -2.4873695 -1.7829456 -1.3062038 -0.92959213 -0.174137][-4.2319775 -1.6515819 -1.5092928 -2.098455 -3.3248537 -4.7321444 -5.3484497 -5.439909 -4.8919144 -4.0774212 -3.3449616 -2.7685792 -2.5015085 -2.424746 -1.8278117][-5.3451824 -3.1511655 -2.9729774 -3.3433628 -4.2793655 -5.3702149 -5.8103561 -5.8062048 -5.283474 -4.4536581 -3.7267504 -3.25467 -3.2669435 -3.41407 -3.0741134]]...]
INFO - root - 2017-12-15 06:32:48.892983: step 7910, loss = 0.26, batch loss = 0.22 (35.8 examples/sec; 0.224 sec/batch; 20h:09m:50s remains)
INFO - root - 2017-12-15 06:32:51.143012: step 7920, loss = 0.24, batch loss = 0.20 (35.0 examples/sec; 0.228 sec/batch; 20h:35m:37s remains)
INFO - root - 2017-12-15 06:32:53.448930: step 7930, loss = 0.31, batch loss = 0.27 (31.8 examples/sec; 0.252 sec/batch; 22h:41m:21s remains)
INFO - root - 2017-12-15 06:32:55.754904: step 7940, loss = 0.32, batch loss = 0.29 (34.4 examples/sec; 0.233 sec/batch; 20h:59m:10s remains)
INFO - root - 2017-12-15 06:32:58.054348: step 7950, loss = 0.67, batch loss = 0.64 (34.8 examples/sec; 0.230 sec/batch; 20h:44m:05s remains)
INFO - root - 2017-12-15 06:33:00.334389: step 7960, loss = 0.31, batch loss = 0.27 (35.5 examples/sec; 0.225 sec/batch; 20h:18m:42s remains)
INFO - root - 2017-12-15 06:33:02.611380: step 7970, loss = 0.23, batch loss = 0.20 (35.5 examples/sec; 0.225 sec/batch; 20h:19m:34s remains)
INFO - root - 2017-12-15 06:33:04.872685: step 7980, loss = 0.23, batch loss = 0.19 (36.0 examples/sec; 0.222 sec/batch; 20h:01m:13s remains)
INFO - root - 2017-12-15 06:33:07.137438: step 7990, loss = 0.25, batch loss = 0.21 (34.3 examples/sec; 0.233 sec/batch; 21h:01m:24s remains)
INFO - root - 2017-12-15 06:33:09.431376: step 8000, loss = 0.21, batch loss = 0.17 (36.3 examples/sec; 0.221 sec/batch; 19h:53m:19s remains)
2017-12-15 06:33:09.762318: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7914524 -1.929258 -1.5152166 -1.6594931 -1.9864066 -2.3791614 -2.5424047 -2.3261695 -2.1968913 -2.3711042 -2.9050186 -3.3401847 -3.7499247 -4.0788016 -3.905417][-2.7974918 -3.1126795 -2.7265868 -2.6248026 -2.6834219 -2.7446842 -2.5568924 -2.0720017 -1.9367332 -2.3658843 -3.1121035 -3.7671099 -4.3836479 -4.6908274 -4.4707336][-3.5840583 -3.4673007 -3.1188126 -2.7503448 -2.5040202 -2.2181909 -1.775717 -1.1210175 -0.92837 -1.4279195 -2.4188154 -3.4568596 -4.397718 -4.8272648 -4.7590761][-4.315383 -3.8453097 -3.42696 -2.8159289 -2.1886284 -1.4168303 -0.65974081 0.029678583 0.10134482 -0.59863734 -1.8980385 -3.3720241 -4.5596647 -5.1238875 -5.1889148][-5.0049171 -3.9760151 -3.5665762 -2.8783269 -2.0082386 -0.87037754 0.17748165 0.89802217 0.88316226 0.0071671009 -1.4481453 -3.160321 -4.46869 -5.1410275 -5.2854934][-5.2211432 -4.044292 -3.7105672 -3.0145156 -1.946234 -0.52033079 0.90284967 1.8031929 1.7633443 0.77238727 -0.73449039 -2.54513 -3.9569337 -4.6856184 -4.809669][-4.6822233 -4.1112094 -3.7898672 -3.0740664 -1.8207749 -0.10341954 1.6925387 2.8599646 2.8122642 1.6688237 0.081728458 -1.8428028 -3.383523 -4.2013049 -4.4225645][-4.6631718 -4.079649 -3.8126254 -3.1931384 -1.9846212 -0.28715825 1.5563717 2.7764251 2.7450106 1.7127843 0.30851316 -1.5249355 -2.9252746 -3.7490585 -4.0957479][-4.5411024 -3.89604 -3.654635 -3.1762652 -2.2497134 -0.92463672 0.61457205 1.669275 1.8149772 1.0729508 0.064880848 -1.3880813 -2.4670639 -3.0596614 -3.380177][-4.3450465 -3.5397639 -3.2716165 -2.9149103 -2.3289216 -1.4345229 -0.27084398 0.693521 0.96502733 0.47881341 -0.27018344 -1.4218075 -2.1574459 -2.4450479 -2.6950326][-4.3809671 -3.3743629 -3.0369914 -2.7357275 -2.4164047 -1.8437495 -1.0106473 -0.21874166 0.038975954 -0.32524443 -0.95423615 -1.6921779 -1.9790158 -1.9306688 -2.0749016][-4.5868998 -3.50519 -3.1250925 -2.8142221 -2.670157 -2.3507843 -1.835149 -1.3374686 -1.2005105 -1.5341191 -1.9514591 -2.1107762 -1.9106597 -1.5056467 -1.5221798][-4.8147907 -3.8358197 -3.4670715 -3.09211 -2.9696772 -2.7487903 -2.4708652 -2.1949844 -2.1569231 -2.4304268 -2.6429532 -2.2864919 -1.7695489 -1.1187199 -1.0717051][-4.9438953 -4.1824093 -3.8914957 -3.4823534 -3.3659902 -3.187952 -3.0980163 -2.9824791 -2.9907529 -3.0333211 -2.9617105 -2.186667 -1.4720817 -0.81521595 -0.91487813][-4.7271528 -4.2847319 -4.1491184 -3.8145692 -3.7790937 -3.6902475 -3.6924143 -3.7361357 -3.7174039 -3.5579147 -3.2268987 -2.1992371 -1.3803382 -0.76518238 -1.0486575]]...]
INFO - root - 2017-12-15 06:33:12.080276: step 8010, loss = 0.30, batch loss = 0.27 (34.2 examples/sec; 0.234 sec/batch; 21h:06m:51s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 06:33:14.340415: step 8020, loss = 0.22, batch loss = 0.18 (35.9 examples/sec; 0.223 sec/batch; 20h:05m:46s remains)
INFO - root - 2017-12-15 06:33:16.587139: step 8030, loss = 0.19, batch loss = 0.15 (35.3 examples/sec; 0.226 sec/batch; 20h:24m:30s remains)
INFO - root - 2017-12-15 06:33:18.824625: step 8040, loss = 0.24, batch loss = 0.21 (36.3 examples/sec; 0.220 sec/batch; 19h:52m:19s remains)
INFO - root - 2017-12-15 06:33:21.103981: step 8050, loss = 0.30, batch loss = 0.26 (34.9 examples/sec; 0.229 sec/batch; 20h:40m:56s remains)
INFO - root - 2017-12-15 06:33:23.355430: step 8060, loss = 0.25, batch loss = 0.22 (36.1 examples/sec; 0.222 sec/batch; 19h:59m:40s remains)
INFO - root - 2017-12-15 06:33:25.648547: step 8070, loss = 0.30, batch loss = 0.26 (35.4 examples/sec; 0.226 sec/batch; 20h:23m:28s remains)
INFO - root - 2017-12-15 06:33:27.886127: step 8080, loss = 0.33, batch loss = 0.29 (35.0 examples/sec; 0.229 sec/batch; 20h:35m:42s remains)
INFO - root - 2017-12-15 06:33:30.127630: step 8090, loss = 0.30, batch loss = 0.26 (33.8 examples/sec; 0.236 sec/batch; 21h:18m:20s remains)
INFO - root - 2017-12-15 06:33:32.403278: step 8100, loss = 0.35, batch loss = 0.31 (34.2 examples/sec; 0.234 sec/batch; 21h:04m:24s remains)
2017-12-15 06:33:32.744150: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2413867 -2.463021 -2.7177968 -3.3194189 -4.0198402 -4.5544319 -4.6436691 -4.2299805 -3.6735733 -3.1094954 -2.0587902 -0.99952435 -0.61273444 -0.90354943 -1.4857925][-2.9781322 -2.3105736 -2.5633984 -3.1410394 -3.7618861 -4.2117147 -4.3211408 -3.9646463 -3.5079472 -3.0818977 -2.0522161 -1.0060902 -0.651922 -1.0125846 -1.7514282][-3.3800011 -1.9801005 -2.2893956 -2.6856971 -2.894876 -2.9522507 -2.9865079 -2.9061792 -2.9013047 -2.9327812 -2.1257513 -1.088208 -0.70915473 -0.9791888 -1.7336875][-3.4605353 -1.6150212 -1.8648293 -2.0075281 -1.6881144 -1.1443827 -1.07273 -1.3837554 -2.0488343 -2.7673302 -2.4218693 -1.583025 -1.2967299 -1.440269 -2.0567524][-3.0959995 -1.1059719 -1.4332917 -1.4726896 -0.68660986 0.6000123 1.0517321 0.55723429 -0.62053025 -2.0370135 -2.2737379 -1.8548415 -1.850423 -1.9682858 -2.398942][-1.9492106 -0.2273128 -0.66867912 -0.73946047 0.43289924 2.5117762 3.5475328 3.1663139 1.7658846 -0.21246898 -1.0854973 -1.2333786 -1.7649782 -2.0184352 -2.3657591][-0.20192218 0.99973083 0.519495 0.38640523 1.7730055 4.4487371 6.0331764 5.8971834 4.4594707 2.1303675 0.75381517 0.089792728 -0.98214483 -1.572356 -1.9758399][0.89458466 1.7107222 1.1869466 0.99172974 2.3816898 5.3002033 7.2633057 7.4583178 6.1247234 3.6086867 1.9825637 1.1292629 -0.12482452 -1.0094483 -1.5464785][0.49531364 1.0614836 0.71386504 0.68250179 2.0600617 4.9277334 7.1084604 7.6219292 6.470521 3.9801719 2.4146597 1.6977136 0.52127314 -0.54583824 -1.2858][-1.2676375 -0.7714107 -0.78371394 -0.57791436 0.70767665 3.2226574 5.273941 5.9319878 5.0220461 2.9204209 1.718492 1.3780918 0.49780345 -0.65705574 -1.4935646][-3.5132396 -2.8996854 -2.7528758 -2.5321651 -1.6097736 0.20706415 1.7714567 2.343209 1.7448168 0.27882576 -0.34933448 -0.23327029 -0.60040259 -1.4713995 -2.1030114][-5.2071009 -4.4536839 -4.2346492 -4.098022 -3.5799048 -2.5186479 -1.5951294 -1.332653 -1.7038571 -2.5637183 -2.6711416 -2.1408007 -1.9839622 -2.3905072 -2.7321029][-5.8291216 -4.99575 -4.7627287 -4.6994305 -4.4925203 -4.0566139 -3.7433715 -3.8530717 -4.0792885 -4.4415445 -4.1949239 -3.463717 -2.9784 -3.0759077 -3.2359691][-5.401938 -4.7163696 -4.6292567 -4.6904726 -4.7849035 -4.8222303 -4.9413671 -5.234798 -5.3420267 -5.4226317 -5.1875844 -4.59235 -3.9969685 -3.9113531 -3.9066105][-4.5342522 -4.1125693 -4.2041593 -4.410902 -4.7861233 -5.1775527 -5.5927224 -5.8964062 -5.82525 -5.7553606 -5.6792507 -5.4380302 -4.9396181 -4.7733393 -4.6771717]]...]
INFO - root - 2017-12-15 06:33:34.976514: step 8110, loss = 0.23, batch loss = 0.20 (35.6 examples/sec; 0.225 sec/batch; 20h:16m:02s remains)
INFO - root - 2017-12-15 06:33:37.241348: step 8120, loss = 0.19, batch loss = 0.16 (33.9 examples/sec; 0.236 sec/batch; 21h:16m:27s remains)
INFO - root - 2017-12-15 06:33:39.554053: step 8130, loss = 0.38, batch loss = 0.35 (34.2 examples/sec; 0.234 sec/batch; 21h:05m:28s remains)
INFO - root - 2017-12-15 06:33:41.818414: step 8140, loss = 0.19, batch loss = 0.15 (36.2 examples/sec; 0.221 sec/batch; 19h:55m:59s remains)
INFO - root - 2017-12-15 06:33:44.104344: step 8150, loss = 0.21, batch loss = 0.18 (33.1 examples/sec; 0.242 sec/batch; 21h:47m:54s remains)
INFO - root - 2017-12-15 06:33:46.368405: step 8160, loss = 0.27, batch loss = 0.23 (35.0 examples/sec; 0.228 sec/batch; 20h:34m:15s remains)
INFO - root - 2017-12-15 06:33:48.633689: step 8170, loss = 0.22, batch loss = 0.19 (35.3 examples/sec; 0.226 sec/batch; 20h:24m:12s remains)
INFO - root - 2017-12-15 06:33:50.951176: step 8180, loss = 0.37, batch loss = 0.34 (35.0 examples/sec; 0.229 sec/batch; 20h:36m:54s remains)
INFO - root - 2017-12-15 06:33:53.246393: step 8190, loss = 0.20, batch loss = 0.16 (35.9 examples/sec; 0.223 sec/batch; 20h:03m:31s remains)
INFO - root - 2017-12-15 06:33:55.551237: step 8200, loss = 0.37, batch loss = 0.33 (35.5 examples/sec; 0.225 sec/batch; 20h:16m:56s remains)
2017-12-15 06:33:55.902329: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0800354 -3.1676154 -3.7505903 -4.2158794 -4.0612516 -3.8765984 -3.681457 -3.0277004 -2.2811472 -1.4976044 -0.80299687 -0.83593071 -1.3244183 -1.547945 -1.9908007][-3.0443666 -3.4533467 -3.8916006 -4.2180328 -4.0557475 -3.8052096 -3.5648005 -3.1888423 -2.7430413 -2.2966764 -1.9065137 -2.0191722 -2.3090565 -2.2826736 -2.4552283][-3.4820023 -3.5349131 -3.9458275 -4.2796292 -4.1129994 -3.7881415 -3.4819679 -3.1905379 -3.0477014 -3.0376649 -3.0700288 -3.4951239 -3.8251357 -3.67874 -3.5395799][-4.0805821 -3.6448054 -4.0461178 -4.3230348 -4.0248232 -3.5495846 -3.0548582 -2.7224627 -2.7962203 -3.1140652 -3.5008276 -4.2513013 -4.813838 -4.7690949 -4.5404921][-4.3965945 -3.8181376 -4.1899071 -4.2816868 -3.618371 -2.7138498 -1.7517719 -1.181915 -1.3141097 -1.9397502 -2.8247955 -4.0421667 -4.9051456 -5.1029882 -5.0059366][-4.4629893 -3.8862643 -4.2201686 -4.0500994 -2.8348322 -1.2037516 0.48906898 1.5995409 1.5678701 0.52858043 -1.1652203 -3.0769677 -4.3977976 -4.8856192 -5.0051122][-3.985646 -3.8390498 -4.2340574 -3.9464993 -2.3534875 -0.11341977 2.3323381 4.1883059 4.5627193 3.27577 0.86910152 -1.7498649 -3.5456729 -4.3031158 -4.6174006][-4.0202289 -3.8023853 -4.340065 -4.0890379 -2.3741119 0.16652393 3.135534 5.6578331 6.55425 5.2742739 2.5088866 -0.5211128 -2.7179916 -3.6651483 -4.1130061][-3.9746082 -3.8211236 -4.5133495 -4.4387436 -2.9058025 -0.4763279 2.5476277 5.2943964 6.4848118 5.4238825 2.8682888 -0.077376842 -2.3390496 -3.2415524 -3.6254518][-3.9467168 -3.8692098 -4.7508297 -4.9985542 -3.8691192 -1.8340441 0.86479187 3.4592044 4.72153 3.9833844 2.0231631 -0.48027396 -2.5382214 -3.2514241 -3.4138913][-3.8541961 -3.8448153 -4.9169931 -5.5325336 -4.9609442 -3.5469389 -1.4203799 0.71966958 1.8554654 1.5942149 0.44203949 -1.4367411 -2.9974229 -3.3908134 -3.2526588][-3.7011259 -3.6605341 -4.7606273 -5.5362778 -5.377234 -4.622406 -3.2601514 -1.7796329 -0.90143657 -0.79635489 -1.3320159 -2.6149282 -3.6361251 -3.6771393 -3.2045643][-3.596621 -3.4496734 -4.3925228 -5.0581608 -5.0180092 -4.7362418 -4.1629238 -3.4815812 -3.0749037 -3.0330737 -3.3739023 -4.2379751 -4.68954 -4.3072443 -3.4371333][-3.5830636 -3.3343017 -4.0784259 -4.5132256 -4.3138885 -4.0568981 -3.7640862 -3.448442 -3.3119297 -3.504827 -3.9321427 -4.7127833 -4.9783258 -4.5270004 -3.6230807][-3.630049 -3.3492599 -3.9666564 -4.1657715 -3.6721659 -3.1046793 -2.5254524 -2.0218196 -1.9033664 -2.2657979 -2.9229977 -3.8202968 -4.1901627 -3.9677441 -3.3750811]]...]
INFO - root - 2017-12-15 06:33:58.122917: step 8210, loss = 0.28, batch loss = 0.24 (36.7 examples/sec; 0.218 sec/batch; 19h:38m:36s remains)
INFO - root - 2017-12-15 06:34:00.377139: step 8220, loss = 0.33, batch loss = 0.29 (34.7 examples/sec; 0.231 sec/batch; 20h:46m:40s remains)
INFO - root - 2017-12-15 06:34:02.616334: step 8230, loss = 0.31, batch loss = 0.28 (35.7 examples/sec; 0.224 sec/batch; 20h:11m:25s remains)
INFO - root - 2017-12-15 06:34:04.861118: step 8240, loss = 0.30, batch loss = 0.26 (35.8 examples/sec; 0.223 sec/batch; 20h:07m:35s remains)
INFO - root - 2017-12-15 06:34:07.142021: step 8250, loss = 0.18, batch loss = 0.15 (36.0 examples/sec; 0.222 sec/batch; 20h:00m:04s remains)
INFO - root - 2017-12-15 06:34:09.401563: step 8260, loss = 0.33, batch loss = 0.29 (35.2 examples/sec; 0.228 sec/batch; 20h:29m:49s remains)
INFO - root - 2017-12-15 06:34:11.675266: step 8270, loss = 0.33, batch loss = 0.30 (33.8 examples/sec; 0.236 sec/batch; 21h:17m:30s remains)
INFO - root - 2017-12-15 06:34:13.921049: step 8280, loss = 0.28, batch loss = 0.25 (36.6 examples/sec; 0.219 sec/batch; 19h:41m:31s remains)
INFO - root - 2017-12-15 06:34:16.209794: step 8290, loss = 0.31, batch loss = 0.28 (33.6 examples/sec; 0.238 sec/batch; 21h:25m:31s remains)
INFO - root - 2017-12-15 06:34:18.482613: step 8300, loss = 0.31, batch loss = 0.28 (35.1 examples/sec; 0.228 sec/batch; 20h:30m:43s remains)
2017-12-15 06:34:18.788530: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6305395 -2.432348 -2.5003633 -2.99166 -3.4737449 -4.1754475 -5.3674932 -6.1852908 -6.5828815 -6.9421706 -7.0845151 -6.9185514 -6.2701964 -5.4535313 -4.8544073][-2.4171233 -2.8591089 -2.8058038 -2.9551647 -3.1303124 -3.6517022 -4.7395926 -5.7511854 -6.4241896 -6.9521804 -7.2232566 -7.2446775 -6.4024343 -5.06569 -3.9952159][-3.3935709 -3.4652646 -3.2546015 -3.1127748 -2.9775178 -3.1590734 -3.8405271 -4.7502384 -5.4864426 -6.1515141 -6.5345635 -6.8128004 -6.0603151 -4.4648457 -2.8931444][-4.3986511 -4.0619121 -3.5602489 -3.0476575 -2.500582 -2.1843059 -2.3636837 -3.0811462 -3.9258323 -4.7514687 -5.1850052 -5.7667456 -5.435483 -3.9936275 -2.0682628][-5.5452013 -4.6531439 -3.6272674 -2.6838486 -1.7119462 -0.89001131 -0.64395332 -1.1854038 -2.1867735 -3.1947453 -3.6857238 -4.5140204 -4.6801243 -3.6918488 -1.8035296][-6.5041714 -5.1824617 -3.5873747 -2.164053 -0.85085213 0.38765216 1.010046 0.71919322 -0.22081482 -1.3384073 -1.9366047 -2.9759767 -3.6161828 -3.1494732 -1.6660984][-6.945962 -5.6221156 -3.6509137 -1.7520605 -0.066770554 1.5806642 2.5233212 2.5420828 1.8585846 0.71341777 -0.098328352 -1.4247775 -2.4941957 -2.4404726 -1.5087991][-7.2519727 -5.9613047 -3.9314814 -1.8437068 0.040463209 1.9493217 3.0956292 3.3318748 3.0610909 2.1137085 1.2815826 -0.19345129 -1.5783962 -1.7958758 -1.3928146][-6.863863 -6.0223932 -4.36496 -2.5198987 -0.80792487 1.090909 2.22582 2.5749755 2.8323665 2.3418789 1.7412331 0.39741373 -1.0297533 -1.243506 -1.0151597][-6.0343795 -5.7235985 -4.6262131 -3.2216203 -1.8955833 -0.27660406 0.65810061 1.018193 1.6962326 1.6574709 1.433614 0.44587612 -0.79298151 -0.81479812 -0.46317196][-4.9512157 -5.1476865 -4.5587883 -3.6388798 -2.8379042 -1.6846603 -1.0370427 -0.66615 0.38825989 0.71201491 0.89434934 0.29499507 -0.568033 -0.1951822 0.50040007][-3.8851302 -4.4024057 -4.1372347 -3.5768232 -3.284627 -2.6587772 -2.1561515 -1.6683617 -0.30637491 0.2837131 0.6763761 0.36349678 -0.16741967 0.683378 1.7112439][-3.2030931 -3.6825557 -3.4626904 -3.0698872 -3.0720582 -2.802335 -2.3481581 -1.8123753 -0.39378572 0.33518839 0.79006553 0.48546576 0.061166763 1.1983976 2.4777699][-2.6750226 -3.0771363 -2.8839893 -2.5890129 -2.8373418 -2.8866775 -2.4394808 -1.9127898 -0.53264832 0.31539512 0.80727696 0.42519903 -0.10443711 1.0024126 2.3314562][-2.2788954 -2.5502205 -2.4317873 -2.2688289 -2.760313 -3.1048398 -2.6521766 -2.1717055 -1.0049803 -0.19995975 0.25405431 -0.25229406 -0.93217909 -0.080118179 1.165724]]...]
INFO - root - 2017-12-15 06:34:21.072981: step 8310, loss = 0.21, batch loss = 0.17 (35.3 examples/sec; 0.227 sec/batch; 20h:25m:35s remains)
INFO - root - 2017-12-15 06:34:23.353742: step 8320, loss = 0.30, batch loss = 0.26 (35.2 examples/sec; 0.227 sec/batch; 20h:26m:44s remains)
INFO - root - 2017-12-15 06:34:25.634513: step 8330, loss = 0.32, batch loss = 0.28 (36.0 examples/sec; 0.222 sec/batch; 19h:59m:05s remains)
INFO - root - 2017-12-15 06:34:27.913849: step 8340, loss = 0.29, batch loss = 0.26 (35.4 examples/sec; 0.226 sec/batch; 20h:20m:38s remains)
INFO - root - 2017-12-15 06:34:30.163764: step 8350, loss = 0.32, batch loss = 0.28 (36.0 examples/sec; 0.222 sec/batch; 19h:59m:29s remains)
INFO - root - 2017-12-15 06:34:32.432766: step 8360, loss = 0.32, batch loss = 0.28 (35.6 examples/sec; 0.225 sec/batch; 20h:14m:25s remains)
INFO - root - 2017-12-15 06:34:34.696443: step 8370, loss = 0.27, batch loss = 0.23 (34.2 examples/sec; 0.234 sec/batch; 21h:03m:31s remains)
INFO - root - 2017-12-15 06:34:36.951645: step 8380, loss = 0.30, batch loss = 0.27 (35.4 examples/sec; 0.226 sec/batch; 20h:19m:28s remains)
INFO - root - 2017-12-15 06:34:39.220588: step 8390, loss = 0.21, batch loss = 0.17 (34.7 examples/sec; 0.231 sec/batch; 20h:46m:13s remains)
INFO - root - 2017-12-15 06:34:41.514516: step 8400, loss = 0.33, batch loss = 0.29 (35.0 examples/sec; 0.229 sec/batch; 20h:36m:19s remains)
2017-12-15 06:34:41.889882: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3563623 -3.9141445 -3.7253919 -3.1194096 -2.254658 -1.1089702 0.52546453 1.3621628 0.494673 -1.0752326 -2.4931712 -3.6843421 -4.3178916 -4.3177958 -3.7041106][-3.6656618 -4.4528894 -4.5393353 -4.2016039 -3.3662624 -2.0460632 -0.12884128 0.95683503 0.33655334 -0.89220619 -2.362421 -3.7238054 -4.5313559 -4.628799 -4.0915909][-4.8416615 -4.9182897 -5.3015146 -5.2541595 -4.6020513 -3.2174029 -1.0664921 0.36476564 0.22637749 -0.48682964 -1.7869878 -3.3592825 -4.4580917 -4.807385 -4.525279][-5.5040045 -5.20454 -5.8138485 -5.9332843 -5.3300009 -3.7665489 -1.3725827 0.39699221 0.83724236 0.60505629 -0.53732121 -2.3397169 -3.8976116 -4.7416763 -4.89277][-6.0000167 -5.1963377 -5.82388 -5.8651061 -5.1007342 -3.2652831 -0.63887978 1.3386667 2.1724296 2.2043123 1.0236821 -1.0428276 -3.051857 -4.4514894 -5.1056604][-5.8818665 -4.982945 -5.5001068 -5.350256 -4.3360553 -2.1773639 0.62056565 2.7109251 3.7837553 3.8475862 2.4233904 0.14393449 -2.2065394 -4.0978489 -5.234293][-5.2310333 -4.687048 -5.0913291 -4.8389015 -3.7024255 -1.3052855 1.6117029 3.8563285 5.1969309 5.2584457 3.5848389 1.1170452 -1.5345322 -3.8726621 -5.3808222][-5.1622334 -4.4175539 -4.7555766 -4.542304 -3.4717498 -1.0632349 1.7142646 3.9607573 5.4644871 5.5248013 3.7755823 1.2409163 -1.4776862 -4.0112491 -5.6218691][-5.1217952 -4.3184853 -4.688611 -4.6680155 -3.8390131 -1.7200642 0.60374188 2.6337619 4.07793 4.0981116 2.4787602 0.19693899 -2.2801228 -4.6588879 -6.0452862][-5.2491293 -4.4811335 -5.011487 -5.2919035 -4.7940555 -3.1375251 -1.3031747 0.51055908 1.7846472 1.6913757 0.25787783 -1.6653029 -3.8663454 -5.8284397 -6.6349239][-5.5500259 -4.8673058 -5.5424833 -6.0526972 -5.8566093 -4.6945581 -3.3091173 -1.8628964 -0.908777 -1.1571589 -2.378654 -3.9685068 -5.74811 -7.0791583 -7.0955129][-5.8704596 -5.2315617 -5.9591665 -6.597168 -6.6502061 -5.9681535 -5.0603013 -4.14167 -3.6820846 -4.0949879 -5.0625868 -6.1330762 -7.2233438 -7.6745634 -6.884974][-6.058507 -5.4198675 -6.1149845 -6.794395 -7.0313282 -6.7477932 -6.2564631 -5.8379989 -5.8123341 -6.2653732 -6.9108605 -7.4319687 -7.7402625 -7.3461413 -5.9742336][-6.0539489 -5.3822231 -5.9595842 -6.5749207 -6.9160395 -6.937346 -6.8155427 -6.804461 -7.0325017 -7.3886967 -7.6744161 -7.6563859 -7.2584414 -6.3166571 -4.686048][-5.9026771 -5.2033024 -5.6551752 -6.172698 -6.5762172 -6.81424 -6.961586 -7.1653666 -7.4235697 -7.5502234 -7.373755 -6.7876015 -5.8298159 -4.6771536 -3.1803279]]...]
INFO - root - 2017-12-15 06:34:44.144141: step 8410, loss = 0.27, batch loss = 0.23 (34.8 examples/sec; 0.230 sec/batch; 20h:43m:19s remains)
INFO - root - 2017-12-15 06:34:46.423405: step 8420, loss = 0.22, batch loss = 0.18 (35.0 examples/sec; 0.228 sec/batch; 20h:34m:03s remains)
INFO - root - 2017-12-15 06:34:48.738423: step 8430, loss = 0.21, batch loss = 0.17 (33.9 examples/sec; 0.236 sec/batch; 21h:13m:15s remains)
INFO - root - 2017-12-15 06:34:51.009173: step 8440, loss = 0.42, batch loss = 0.38 (35.8 examples/sec; 0.224 sec/batch; 20h:08m:24s remains)
INFO - root - 2017-12-15 06:34:53.314866: step 8450, loss = 0.20, batch loss = 0.16 (35.6 examples/sec; 0.225 sec/batch; 20h:14m:19s remains)
INFO - root - 2017-12-15 06:34:55.578383: step 8460, loss = 0.35, batch loss = 0.32 (34.9 examples/sec; 0.229 sec/batch; 20h:38m:05s remains)
INFO - root - 2017-12-15 06:34:57.859281: step 8470, loss = 0.23, batch loss = 0.19 (36.1 examples/sec; 0.222 sec/batch; 19h:57m:22s remains)
INFO - root - 2017-12-15 06:35:00.145314: step 8480, loss = 0.27, batch loss = 0.23 (36.1 examples/sec; 0.222 sec/batch; 19h:57m:53s remains)
INFO - root - 2017-12-15 06:35:02.427758: step 8490, loss = 0.30, batch loss = 0.26 (36.2 examples/sec; 0.221 sec/batch; 19h:54m:49s remains)
INFO - root - 2017-12-15 06:35:04.702984: step 8500, loss = 0.23, batch loss = 0.20 (35.6 examples/sec; 0.225 sec/batch; 20h:15m:01s remains)
2017-12-15 06:35:05.067629: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3576757 -1.6457943 -1.3808826 -1.1186335 -1.1379213 -1.4393505 -2.0889471 -2.6258631 -2.5907979 -2.5225635 -2.5997689 -2.7294896 -2.9182751 -3.0279739 -2.8582218][-1.9877714 -2.2551877 -2.2491314 -2.0215671 -2.1302242 -2.4185266 -2.8869867 -3.1376996 -3.0193944 -2.9365208 -2.9832256 -3.0780678 -3.1629028 -3.1343946 -2.9363601][-2.5547707 -2.7778552 -2.797842 -2.3779721 -2.3019285 -2.4434152 -2.635787 -2.7384386 -2.7084932 -2.67339 -2.728476 -2.7787077 -2.7149429 -2.5159342 -2.2513564][-2.9017673 -2.654047 -2.4585476 -1.8490956 -1.6025374 -1.5480695 -1.596348 -1.7263397 -1.9587426 -2.084517 -2.241462 -2.2053213 -1.9536089 -1.5435152 -1.2013093][-2.9424746 -1.8737561 -1.4191689 -0.62229347 -0.12516773 0.14321971 0.19238472 -0.031282663 -0.60717416 -1.1442132 -1.7028135 -1.779639 -1.4266889 -0.74936819 -0.26714218][-2.5121603 -0.81925547 -0.1738466 0.76634812 1.4694161 1.9503887 2.1127961 1.775773 0.88069487 -0.19839108 -1.2411411 -1.6258376 -1.5414748 -0.81926012 -0.11730444][-1.5927682 -0.0013053417 0.68298888 1.487628 2.0108144 2.5152776 2.7483313 2.385874 1.3659158 0.0059399605 -1.1964656 -1.858312 -2.1423244 -1.6112907 -0.782997][-1.0856242 0.3594501 0.88139653 1.4198635 1.7193875 2.2218516 2.4297249 2.0491664 1.0481763 -0.31684065 -1.2911465 -1.8522512 -2.2226887 -1.8718798 -1.1741138][-0.93145323 0.29091716 0.45355153 0.65489841 0.80411649 1.3563926 1.5502326 1.242099 0.46829844 -0.63750947 -1.1749451 -1.3011148 -1.6473275 -1.5496027 -1.1719234][-1.1761072 -0.20926857 -0.44927788 -0.63266671 -0.70877731 -0.14526463 0.16596889 0.20125961 0.024777889 -0.54207039 -0.41734588 0.012970686 -0.34335697 -0.66881323 -0.81604683][-1.4893434 -0.73902 -1.1871219 -1.6231377 -1.8847662 -1.3745731 -0.93280196 -0.50528765 -0.023195267 -0.018497467 0.48735356 1.0962217 0.59695911 -0.13685572 -0.73096895][-1.6916404 -1.0467252 -1.4471061 -1.8986318 -2.2398775 -1.8280189 -1.4490042 -0.92093194 0.066928387 0.50598 0.97518587 1.3705385 0.6893065 -0.16895127 -0.84169257][-1.8047403 -1.1545203 -1.3118539 -1.6136875 -1.9315518 -1.7186284 -1.5982013 -1.2121173 -0.077083588 0.51769543 0.69060969 0.79860663 0.068100452 -0.65670288 -1.0547622][-1.8050267 -1.025448 -0.88183224 -0.94525015 -1.1495097 -1.1156447 -1.2659297 -1.2220912 -0.37333417 0.035166979 0.028268337 0.041784763 -0.44815862 -0.87579477 -0.96241248][-1.8770363 -0.95985663 -0.59279609 -0.4598552 -0.55387938 -0.674749 -1.034907 -1.3379933 -0.87898266 -0.7397356 -0.81951487 -0.6938529 -0.74968278 -0.7056241 -0.5823344]]...]
INFO - root - 2017-12-15 06:35:07.327910: step 8510, loss = 0.35, batch loss = 0.31 (36.0 examples/sec; 0.222 sec/batch; 20h:00m:33s remains)
INFO - root - 2017-12-15 06:35:09.618884: step 8520, loss = 0.40, batch loss = 0.36 (36.3 examples/sec; 0.220 sec/batch; 19h:49m:42s remains)
INFO - root - 2017-12-15 06:35:11.932301: step 8530, loss = 0.26, batch loss = 0.23 (33.6 examples/sec; 0.238 sec/batch; 21h:27m:02s remains)
INFO - root - 2017-12-15 06:35:14.207247: step 8540, loss = 0.24, batch loss = 0.20 (34.8 examples/sec; 0.230 sec/batch; 20h:39m:42s remains)
INFO - root - 2017-12-15 06:35:16.556133: step 8550, loss = 0.22, batch loss = 0.19 (35.4 examples/sec; 0.226 sec/batch; 20h:18m:45s remains)
INFO - root - 2017-12-15 06:35:18.819560: step 8560, loss = 0.20, batch loss = 0.16 (35.1 examples/sec; 0.228 sec/batch; 20h:30m:57s remains)
INFO - root - 2017-12-15 06:35:21.154408: step 8570, loss = 0.24, batch loss = 0.21 (33.5 examples/sec; 0.239 sec/batch; 21h:30m:07s remains)
INFO - root - 2017-12-15 06:35:23.419860: step 8580, loss = 0.33, batch loss = 0.29 (36.5 examples/sec; 0.219 sec/batch; 19h:43m:21s remains)
INFO - root - 2017-12-15 06:35:25.699184: step 8590, loss = 0.31, batch loss = 0.28 (35.6 examples/sec; 0.225 sec/batch; 20h:13m:23s remains)
INFO - root - 2017-12-15 06:35:27.925084: step 8600, loss = 0.24, batch loss = 0.20 (35.8 examples/sec; 0.223 sec/batch; 20h:04m:52s remains)
2017-12-15 06:35:28.266701: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.906456 -6.7256689 -6.6962204 -6.6140966 -6.4057016 -6.1698589 -5.8732085 -5.4541426 -5.0631561 -4.7392988 -4.0898695 -3.4269071 -3.1492834 -3.0119929 -2.9142351][-5.811811 -6.9634485 -6.9828825 -6.8740377 -6.5835104 -6.27308 -5.8990784 -5.4801493 -5.2191772 -5.0391064 -4.5983653 -4.1171169 -3.8906384 -3.7021017 -3.5352101][-6.7330141 -6.9590645 -6.9923363 -6.7185059 -6.0381489 -5.3585505 -4.7283363 -4.42097 -4.5842896 -5.0245123 -5.120357 -4.9756789 -4.91424 -4.8324957 -4.4720316][-6.8495655 -6.530489 -6.5964737 -6.084662 -4.7617307 -3.3247347 -2.1806173 -2.0398865 -2.8867819 -4.1778708 -4.9159837 -5.1370678 -5.4401278 -5.6867247 -5.2743483][-6.4179897 -5.6962256 -5.6270504 -4.712966 -2.5851293 -0.20601261 1.469105 1.2710342 -0.47931993 -2.6816196 -3.9800954 -4.4740334 -5.1930237 -5.9702063 -5.711977][-5.6850286 -4.4743223 -4.0441775 -2.7158372 -0.019570827 3.0700872 5.0405655 4.4543362 1.8416977 -1.1444137 -2.8366575 -3.5221419 -4.4358406 -5.5485764 -5.5506468][-4.8188362 -3.4713109 -2.5225403 -0.82375717 2.0490477 5.3664455 7.32753 6.3720179 3.2155797 -0.15535617 -2.0483131 -2.8904316 -3.8620167 -4.9887877 -5.17719][-4.7355146 -3.0239825 -1.6249986 0.25240684 2.7802436 5.5944386 7.0639105 5.8914804 2.8146603 -0.25825548 -1.9508905 -2.8192279 -3.7597327 -4.6374931 -4.82329][-5.0770903 -3.3119073 -1.7543733 -0.036868334 1.7296553 3.5837753 4.3053045 3.095587 0.67815328 -1.5665765 -2.6542957 -3.3400955 -4.1249437 -4.7263775 -4.8741665][-5.824564 -4.2142286 -2.6775088 -1.2135812 -0.12385488 0.73598814 0.66672373 -0.57930386 -2.2587829 -3.5317104 -3.921109 -4.2221265 -4.8384285 -5.3208113 -5.2981148][-6.5764894 -5.165307 -3.7021208 -2.4555783 -1.9321268 -1.8899751 -2.5030727 -3.6549871 -4.6493487 -5.1177106 -4.9346867 -4.9561653 -5.3737888 -5.6624088 -5.4667225][-7.1059456 -5.9682703 -4.7923326 -3.8216729 -3.6305604 -3.9323316 -4.6615467 -5.5398006 -6.12663 -6.13459 -5.6677217 -5.409009 -5.5653696 -5.6315422 -5.3176026][-7.1838369 -6.31114 -5.4826617 -4.769063 -4.6587658 -4.9649529 -5.546382 -6.1936684 -6.5674381 -6.3553543 -5.7997317 -5.5003242 -5.4468026 -5.2709517 -4.930088][-6.7136297 -5.9524517 -5.4606152 -5.0274391 -4.9742227 -5.1839533 -5.5709591 -5.9401274 -6.0536513 -5.7075367 -5.2186966 -4.94722 -4.8126707 -4.5667396 -4.2449694][-5.899663 -5.2280407 -5.0088816 -4.7668667 -4.7167721 -4.8207769 -5.0351191 -5.2192178 -5.2034216 -4.8496871 -4.4252219 -4.1771812 -3.992445 -3.7002735 -3.5500689]]...]
INFO - root - 2017-12-15 06:35:30.538785: step 8610, loss = 0.24, batch loss = 0.21 (34.8 examples/sec; 0.230 sec/batch; 20h:41m:25s remains)
INFO - root - 2017-12-15 06:35:32.829673: step 8620, loss = 0.23, batch loss = 0.19 (35.5 examples/sec; 0.226 sec/batch; 20h:17m:34s remains)
INFO - root - 2017-12-15 06:35:35.102551: step 8630, loss = 0.26, batch loss = 0.22 (35.2 examples/sec; 0.227 sec/batch; 20h:27m:09s remains)
INFO - root - 2017-12-15 06:35:37.342179: step 8640, loss = 0.23, batch loss = 0.19 (36.9 examples/sec; 0.217 sec/batch; 19h:29m:14s remains)
INFO - root - 2017-12-15 06:35:39.618973: step 8650, loss = 0.20, batch loss = 0.17 (35.6 examples/sec; 0.224 sec/batch; 20h:11m:25s remains)
INFO - root - 2017-12-15 06:35:41.864552: step 8660, loss = 0.24, batch loss = 0.20 (35.6 examples/sec; 0.224 sec/batch; 20h:11m:16s remains)
INFO - root - 2017-12-15 06:35:44.211497: step 8670, loss = 0.28, batch loss = 0.24 (33.3 examples/sec; 0.240 sec/batch; 21h:36m:53s remains)
INFO - root - 2017-12-15 06:35:46.490925: step 8680, loss = 0.25, batch loss = 0.21 (36.1 examples/sec; 0.221 sec/batch; 19h:54m:37s remains)
INFO - root - 2017-12-15 06:35:48.761103: step 8690, loss = 0.29, batch loss = 0.26 (34.4 examples/sec; 0.233 sec/batch; 20h:56m:50s remains)
INFO - root - 2017-12-15 06:35:51.070312: step 8700, loss = 0.23, batch loss = 0.19 (34.0 examples/sec; 0.235 sec/batch; 21h:09m:25s remains)
2017-12-15 06:35:51.452996: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1482086 -4.7725363 -5.2365789 -5.7360091 -5.9216108 -5.8763657 -5.6342206 -5.30252 -5.0406208 -4.9784818 -5.2673588 -5.3469362 -5.4319153 -5.5587511 -5.3525267][-4.9564047 -5.85804 -6.5464296 -7.1811409 -7.545146 -7.7749281 -7.5135612 -7.03751 -6.4966125 -6.193119 -6.5047626 -6.5015526 -6.4742279 -6.56847 -6.3216105][-5.991 -6.2182875 -6.9758959 -7.4538307 -7.7573366 -8.1929808 -7.9462185 -7.4292421 -6.7926979 -6.4376926 -6.9149742 -6.9807243 -7.0004549 -7.1650496 -6.9654236][-6.5244045 -6.0920544 -6.6186581 -6.6469336 -6.5673342 -6.8759422 -6.3995972 -5.7616405 -5.1769648 -5.1418953 -6.08696 -6.5680676 -6.9769716 -7.3727722 -7.3741875][-6.4720211 -5.6215591 -5.7197237 -5.1365385 -4.4392891 -4.1637335 -3.0951016 -2.1054959 -1.5428884 -1.9759728 -3.4995193 -4.6466007 -5.7253056 -6.6330204 -7.1471977][-6.4290581 -5.1080589 -4.7170753 -3.5400269 -2.1645381 -1.1322074 0.62947059 2.0149677 2.6405818 1.7654932 -0.21856785 -2.0509958 -3.7269056 -5.0177059 -6.0348883][-5.5383835 -4.3435297 -3.5691817 -1.9686791 -0.088268757 1.5739379 3.7507269 5.3681107 5.9878187 4.6961651 2.2391102 -0.22957993 -2.3965368 -3.8888538 -5.1851888][-5.0663819 -3.9625511 -3.1666217 -1.5094187 0.64143753 2.859916 5.3545027 7.19077 7.8218174 6.1589766 3.2952249 0.28529572 -2.1612782 -3.6527905 -4.9387589][-5.14181 -4.457675 -4.0359464 -2.7052593 -0.71275389 1.5617507 3.9109843 5.5772829 6.0327349 4.31022 1.5791616 -1.2303169 -3.2435892 -4.2993307 -5.2469053][-5.6137581 -5.3729858 -5.353529 -4.4556952 -2.7630482 -0.78548741 1.0934818 2.2948878 2.3829181 0.6545434 -1.703643 -3.8748293 -5.1164112 -5.5470896 -5.9283619][-6.3461704 -6.3467197 -6.6074333 -5.921011 -4.3347826 -2.44677 -0.76641476 0.12521434 -0.084391832 -1.7456512 -3.7991042 -5.4879532 -6.2962418 -6.4655619 -6.5461273][-7.0312672 -7.1250277 -7.5250931 -7.0384645 -5.6124411 -3.8702931 -2.311115 -1.6793945 -2.0776317 -3.4780731 -5.102808 -6.3223219 -6.9084835 -7.0745926 -7.0861616][-7.0851545 -7.1696415 -7.6639328 -7.4112749 -6.3386726 -5.0756226 -3.9328506 -3.745254 -4.2710404 -5.2580509 -6.2999277 -7.0387888 -7.4278126 -7.5411863 -7.4372139][-6.9169703 -7.0057659 -7.5437431 -7.4566174 -6.7608871 -5.9394655 -5.2587376 -5.3105631 -5.7123995 -6.2486744 -6.8182735 -7.1768246 -7.3804846 -7.4097366 -7.2619734][-6.3096943 -6.3703728 -6.9504118 -7.0497332 -6.720108 -6.3197365 -6.0160303 -6.1803846 -6.39592 -6.5356808 -6.6980038 -6.773881 -6.8101196 -6.7500458 -6.5831022]]...]
INFO - root - 2017-12-15 06:35:53.774279: step 8710, loss = 0.23, batch loss = 0.19 (34.1 examples/sec; 0.235 sec/batch; 21h:07m:07s remains)
INFO - root - 2017-12-15 06:35:56.110740: step 8720, loss = 0.29, batch loss = 0.25 (32.0 examples/sec; 0.250 sec/batch; 22h:28m:31s remains)
INFO - root - 2017-12-15 06:35:58.448737: step 8730, loss = 0.32, batch loss = 0.28 (34.3 examples/sec; 0.234 sec/batch; 21h:00m:04s remains)
INFO - root - 2017-12-15 06:36:00.725511: step 8740, loss = 0.27, batch loss = 0.24 (35.6 examples/sec; 0.225 sec/batch; 20h:11m:29s remains)
INFO - root - 2017-12-15 06:36:02.972443: step 8750, loss = 0.26, batch loss = 0.22 (35.9 examples/sec; 0.223 sec/batch; 20h:02m:42s remains)
INFO - root - 2017-12-15 06:36:05.261961: step 8760, loss = 0.24, batch loss = 0.20 (35.6 examples/sec; 0.224 sec/batch; 20h:10m:51s remains)
INFO - root - 2017-12-15 06:36:07.533705: step 8770, loss = 0.24, batch loss = 0.20 (35.6 examples/sec; 0.225 sec/batch; 20h:11m:21s remains)
INFO - root - 2017-12-15 06:36:09.844671: step 8780, loss = 0.33, batch loss = 0.29 (36.3 examples/sec; 0.220 sec/batch; 19h:48m:52s remains)
INFO - root - 2017-12-15 06:36:12.173738: step 8790, loss = 0.23, batch loss = 0.19 (36.0 examples/sec; 0.222 sec/batch; 19h:57m:39s remains)
INFO - root - 2017-12-15 06:36:14.471545: step 8800, loss = 0.38, batch loss = 0.35 (35.9 examples/sec; 0.223 sec/batch; 20h:03m:02s remains)
2017-12-15 06:36:14.790375: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5333276 -3.8276935 -4.3113642 -4.4763746 -3.9222469 -3.2106924 -3.0103893 -3.4716413 -4.5309782 -5.4246511 -5.7304664 -5.0307121 -3.653064 -2.2261205 -1.3587587][-3.5405097 -4.262042 -4.7629504 -4.9259443 -4.3211861 -3.5884581 -3.3353519 -3.6175632 -4.5008802 -5.3749933 -5.7948251 -5.41022 -4.3804555 -3.2479751 -2.4466157][-4.1618013 -4.1949749 -4.6383009 -4.6259956 -3.819241 -3.1517034 -2.9229679 -3.2495985 -4.1104975 -4.8739347 -5.3809042 -5.2019167 -4.3965626 -3.5612903 -2.9457116][-4.119113 -3.8545256 -4.1775818 -3.8824515 -2.8443832 -2.1650906 -1.7569349 -2.0380142 -2.9472728 -3.6794782 -4.4666462 -4.8081379 -4.4718037 -3.9968665 -3.5800312][-4.1073217 -4.0766091 -4.381546 -3.5580087 -2.0220997 -0.87931287 0.18542099 0.077315807 -1.1370463 -2.2191846 -3.6134064 -4.7756782 -5.081563 -4.9206219 -4.5085664][-4.2430763 -4.4071054 -4.5697813 -3.0432358 -0.75997663 1.1493726 2.9744306 3.0133133 1.1779683 -0.692677 -2.870919 -4.7219954 -5.5188856 -5.6190987 -5.2014971][-4.0116582 -4.316267 -4.0517626 -1.7999545 1.1884255 3.7346969 5.9603448 5.7539778 3.0867891 0.2568748 -2.5831385 -4.7085309 -5.6508889 -5.828496 -5.3489833][-4.210453 -4.1568317 -3.5155516 -0.95217741 2.3451948 5.1610646 7.2738996 6.6190276 3.4149475 0.14742517 -2.7393055 -4.4972272 -5.1326742 -5.1379428 -4.5222521][-4.2301407 -3.8377781 -3.0802419 -0.7865479 2.1912007 4.71345 6.2788081 5.3250842 2.2499428 -0.75665057 -3.2073746 -4.3022528 -4.5173883 -4.2605267 -3.5677271][-4.4028225 -3.6474977 -2.9809456 -1.3406074 0.90835786 2.8921747 3.8165259 2.6731896 -0.0059919357 -2.5811205 -4.5327358 -5.1316357 -4.9927039 -4.4367485 -3.5614657][-4.6223917 -3.5858123 -3.2007477 -2.4549503 -1.3315864 -0.13265848 0.26718688 -0.82628512 -2.8780107 -4.7949719 -6.1107569 -6.2646737 -5.8231754 -5.1144652 -4.1645832][-4.6037869 -3.5550508 -3.5798953 -3.7000327 -3.6569736 -3.232285 -2.9847257 -3.5853825 -4.7891197 -5.9586248 -6.6246166 -6.5026937 -6.0926809 -5.5540314 -4.74053][-4.1995888 -3.4263191 -3.8776159 -4.4776964 -4.90948 -4.871088 -4.634922 -4.7377996 -5.1698213 -5.6815434 -5.8473573 -5.7834396 -5.7715492 -5.6602726 -5.0558615][-3.948307 -3.5121188 -4.1929474 -4.6981554 -4.9087224 -4.7790866 -4.3321452 -3.9463191 -3.7553606 -3.8269839 -3.9569376 -4.340519 -4.9965553 -5.339344 -4.8034067][-4.2306848 -3.869339 -4.3582611 -4.430212 -4.2036033 -4.018342 -3.6007931 -3.0576208 -2.4919181 -2.1797507 -2.3252394 -3.0306816 -3.935859 -4.1945219 -3.4840059]]...]
INFO - root - 2017-12-15 06:36:17.073339: step 8810, loss = 0.26, batch loss = 0.23 (35.1 examples/sec; 0.228 sec/batch; 20h:28m:38s remains)
INFO - root - 2017-12-15 06:36:19.355633: step 8820, loss = 0.27, batch loss = 0.23 (34.4 examples/sec; 0.233 sec/batch; 20h:54m:30s remains)
INFO - root - 2017-12-15 06:36:21.619714: step 8830, loss = 0.19, batch loss = 0.15 (35.1 examples/sec; 0.228 sec/batch; 20h:28m:41s remains)
INFO - root - 2017-12-15 06:36:23.910742: step 8840, loss = 0.25, batch loss = 0.21 (36.0 examples/sec; 0.222 sec/batch; 19h:57m:09s remains)
INFO - root - 2017-12-15 06:36:26.176673: step 8850, loss = 0.27, batch loss = 0.24 (35.0 examples/sec; 0.229 sec/batch; 20h:34m:35s remains)
INFO - root - 2017-12-15 06:36:28.430457: step 8860, loss = 0.20, batch loss = 0.17 (35.9 examples/sec; 0.223 sec/batch; 20h:02m:15s remains)
INFO - root - 2017-12-15 06:36:30.692092: step 8870, loss = 0.25, batch loss = 0.21 (35.0 examples/sec; 0.229 sec/batch; 20h:33m:00s remains)
INFO - root - 2017-12-15 06:36:33.006708: step 8880, loss = 0.28, batch loss = 0.25 (35.2 examples/sec; 0.227 sec/batch; 20h:26m:23s remains)
INFO - root - 2017-12-15 06:36:35.265793: step 8890, loss = 0.27, batch loss = 0.24 (35.2 examples/sec; 0.227 sec/batch; 20h:26m:45s remains)
INFO - root - 2017-12-15 06:36:37.529265: step 8900, loss = 0.23, batch loss = 0.20 (35.4 examples/sec; 0.226 sec/batch; 20h:18m:51s remains)
2017-12-15 06:36:37.889096: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0693741 -5.8890605 -5.9044638 -5.735034 -5.0646286 -4.4207478 -4.1983562 -4.3990593 -4.9641948 -5.466651 -5.6263676 -5.4869919 -5.1240463 -4.8687887 -4.8055153][-4.8593941 -6.4894595 -6.619173 -6.5171657 -5.6490602 -4.7265978 -4.1638374 -4.1533871 -4.7233248 -5.4351206 -5.9452467 -6.1344552 -5.9407845 -5.5948167 -5.3243675][-5.8683429 -6.584218 -6.8824358 -7.0019197 -6.102303 -5.0090551 -4.1522331 -3.9972901 -4.5356121 -5.3499327 -6.0539169 -6.5014377 -6.4942193 -6.110137 -5.6347508][-6.4179006 -6.1447849 -6.5108013 -6.7956576 -5.8767796 -4.6481838 -3.5173645 -3.2715168 -4.0070233 -5.2093229 -6.1572361 -6.8347945 -7.02885 -6.6356382 -5.8919339][-6.581305 -5.3912516 -5.7481408 -6.0250707 -4.90767 -3.4016864 -1.8684434 -1.325359 -2.3049817 -4.2409663 -5.8305483 -6.9866118 -7.4875832 -7.186347 -6.1878529][-6.0901351 -4.7516756 -4.9488592 -4.8253822 -3.1882486 -1.2128386 0.77089095 1.650754 0.5043776 -2.1061673 -4.4806633 -6.3761616 -7.3287511 -7.3329797 -6.4261718][-5.5114312 -4.46504 -4.33407 -3.6590514 -1.3654077 1.3182783 3.7840824 4.7294979 3.1573229 -0.0066170692 -2.8523169 -5.1759987 -6.5220141 -6.9338374 -6.3389134][-6.04537 -4.7193975 -4.3340187 -3.3550396 -0.66297066 2.7169094 5.8553948 6.8419919 4.8053455 1.1636403 -1.9004204 -4.2775717 -5.6652265 -6.2150455 -5.8409557][-6.8792973 -5.587698 -5.0380621 -3.9418406 -1.3253157 2.1141286 5.5540366 6.8006749 4.8988147 1.2936907 -1.7547295 -4.0570068 -5.3332624 -5.7837391 -5.3599334][-7.6394038 -6.8331509 -6.4062071 -5.2972422 -2.86627 0.22915387 3.2410812 4.4340715 3.0003886 0.026964903 -2.4653704 -4.3990307 -5.5021749 -5.8452759 -5.2919931][-7.8886161 -7.7837939 -7.776454 -6.9170294 -4.7765632 -2.0639868 0.2659409 1.0885332 -0.034454107 -2.3051054 -3.9829121 -5.1665688 -5.8607349 -5.9709611 -5.3357315][-7.5830059 -7.9647164 -8.3686323 -8.0253983 -6.4971948 -4.2635212 -2.4892552 -1.8553679 -2.5730274 -4.20623 -5.4201961 -6.0806036 -6.3075471 -6.0441465 -5.3003654][-7.1049981 -7.7098193 -8.1567039 -8.07584 -7.1475182 -5.600009 -4.4778652 -4.1110229 -4.4873552 -5.4318695 -6.21913 -6.6186986 -6.6246605 -6.1538429 -5.2556925][-6.811697 -7.4675951 -7.7233458 -7.4179158 -6.7542086 -5.8870277 -5.33585 -5.2848282 -5.5809317 -6.1492519 -6.5307827 -6.6034355 -6.3759079 -5.7920122 -5.0209851][-6.6824431 -7.1491404 -7.2838645 -6.6604819 -5.8998194 -5.4653416 -5.2905464 -5.4110041 -5.6892538 -6.0850749 -6.3044615 -6.244308 -5.8416309 -5.3360205 -4.8828773]]...]
INFO - root - 2017-12-15 06:36:40.150231: step 8910, loss = 0.23, batch loss = 0.20 (35.9 examples/sec; 0.223 sec/batch; 20h:01m:01s remains)
INFO - root - 2017-12-15 06:36:42.457763: step 8920, loss = 0.27, batch loss = 0.23 (34.1 examples/sec; 0.234 sec/batch; 21h:03m:29s remains)
INFO - root - 2017-12-15 06:36:44.730657: step 8930, loss = 0.25, batch loss = 0.21 (34.9 examples/sec; 0.229 sec/batch; 20h:37m:25s remains)
INFO - root - 2017-12-15 06:36:47.072329: step 8940, loss = 0.26, batch loss = 0.22 (35.0 examples/sec; 0.229 sec/batch; 20h:33m:52s remains)
INFO - root - 2017-12-15 06:36:49.371540: step 8950, loss = 0.30, batch loss = 0.26 (35.6 examples/sec; 0.225 sec/batch; 20h:11m:11s remains)
INFO - root - 2017-12-15 06:36:51.645775: step 8960, loss = 0.31, batch loss = 0.27 (35.4 examples/sec; 0.226 sec/batch; 20h:19m:15s remains)
INFO - root - 2017-12-15 06:36:53.905439: step 8970, loss = 0.22, batch loss = 0.18 (36.1 examples/sec; 0.222 sec/batch; 19h:55m:46s remains)
INFO - root - 2017-12-15 06:36:56.179631: step 8980, loss = 0.23, batch loss = 0.20 (34.2 examples/sec; 0.234 sec/batch; 20h:59m:47s remains)
INFO - root - 2017-12-15 06:36:58.472329: step 8990, loss = 0.28, batch loss = 0.24 (35.7 examples/sec; 0.224 sec/batch; 20h:07m:08s remains)
INFO - root - 2017-12-15 06:37:00.754074: step 9000, loss = 0.27, batch loss = 0.23 (35.3 examples/sec; 0.226 sec/batch; 20h:21m:08s remains)
2017-12-15 06:37:01.091448: I tensorflow/core/kernels/logging_ops.cc:79] [[[3.2901633 1.8102436 0.37405205 -1.5090761 -3.1354823 -4.3824606 -4.60879 -3.8594952 -3.1983058 -2.9819756 -2.905462 -2.9280496 -2.2364085 -1.5076938 -1.3330823][1.5223522 1.1046448 -0.23646939 -1.99787 -3.5490668 -4.5499249 -4.4267044 -3.5236173 -2.8094552 -2.6713073 -2.9135032 -3.2323189 -2.707144 -2.0354497 -1.8961][-0.90256274 -0.41549754 -1.5350289 -2.8742723 -4.0732341 -4.59601 -4.0629187 -2.9950919 -2.2260022 -2.2975273 -2.9280591 -3.6125314 -3.4528093 -3.0539622 -2.9009647][-3.3922048 -1.7879097 -2.7442958 -3.6671576 -4.3757362 -4.2769957 -3.4183764 -2.362179 -1.7496916 -2.205729 -3.2359529 -4.2043157 -4.5354881 -4.4432697 -4.1907167][-4.4617214 -2.5231879 -3.477735 -4.0655613 -4.1079741 -3.2933345 -2.1715698 -1.1834581 -0.82294357 -1.8032408 -3.3077195 -4.564826 -5.3617277 -5.6752272 -5.4158525][-3.8404088 -2.292269 -3.4161143 -3.899024 -3.4601641 -2.0592051 -0.70214212 0.43242693 0.72687936 -0.85715377 -2.8178754 -4.2921138 -5.6250429 -6.3383665 -5.9881258][-2.2962487 -1.4413162 -2.8492475 -3.431242 -2.6768677 -0.82356596 0.94165182 2.3604758 2.5505598 0.44209528 -1.6692326 -3.3170695 -5.1593676 -6.2285271 -5.9153252][-1.1431264 -0.331591 -1.9714283 -2.7844689 -1.9420396 0.21851778 2.4239328 4.131525 4.1729231 1.8163826 -0.22882438 -2.0818624 -4.390029 -5.7248697 -5.5384707][0.12350845 0.86484218 -0.99760139 -2.0994463 -1.4597806 0.65291715 3.0986087 4.9140873 4.7046413 2.2153547 0.27443361 -1.7385588 -4.1084867 -5.3640509 -5.0799685][0.77795959 1.6916153 -0.16032374 -1.4901643 -1.3130113 0.40498853 2.6200998 4.1658907 3.5836713 1.0569296 -0.765648 -2.7366552 -4.7637997 -5.5348053 -4.9164858][0.52844787 1.8912084 0.3578 -0.94855297 -1.0833125 0.18695736 1.9034245 2.8591497 1.8968055 -0.44130278 -2.1137285 -3.8960972 -5.2396259 -5.2800808 -4.3465009][-0.32765865 1.5908909 0.42927265 -0.78252363 -1.284692 -0.6867069 0.28250265 0.65224886 -0.33960259 -2.1412868 -3.4770019 -4.8344975 -5.5119185 -5.0934019 -4.1373987][-2.1864421 0.10883927 -0.69290352 -1.7220381 -2.5504677 -2.3511484 -1.8067119 -1.6417449 -2.345314 -3.5379319 -4.5379181 -5.45558 -5.6898947 -5.1846533 -4.4982347][-4.8615217 -2.4841642 -2.968487 -3.6725821 -4.4221239 -4.3119574 -3.9705734 -3.8160152 -4.2157979 -4.9537206 -5.686903 -6.2202559 -6.161603 -5.6477313 -5.1660624][-7.5783052 -5.208231 -5.264185 -5.4879532 -5.9236007 -5.76161 -5.5452995 -5.4447479 -5.651052 -6.1375637 -6.627533 -6.8768387 -6.6698246 -6.252492 -5.9163284]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 06:37:03.407900: step 9010, loss = 0.22, batch loss = 0.19 (34.8 examples/sec; 0.230 sec/batch; 20h:39m:33s remains)
INFO - root - 2017-12-15 06:37:05.708684: step 9020, loss = 0.32, batch loss = 0.29 (33.3 examples/sec; 0.240 sec/batch; 21h:33m:54s remains)
INFO - root - 2017-12-15 06:37:07.992305: step 9030, loss = 0.22, batch loss = 0.18 (35.6 examples/sec; 0.225 sec/batch; 20h:11m:11s remains)
INFO - root - 2017-12-15 06:37:10.290031: step 9040, loss = 0.23, batch loss = 0.19 (33.8 examples/sec; 0.237 sec/batch; 21h:15m:56s remains)
INFO - root - 2017-12-15 06:37:12.536352: step 9050, loss = 0.24, batch loss = 0.20 (36.1 examples/sec; 0.222 sec/batch; 19h:56m:08s remains)
INFO - root - 2017-12-15 06:37:14.849213: step 9060, loss = 0.58, batch loss = 0.55 (35.2 examples/sec; 0.227 sec/batch; 20h:24m:27s remains)
INFO - root - 2017-12-15 06:37:17.164094: step 9070, loss = 0.32, batch loss = 0.29 (33.5 examples/sec; 0.239 sec/batch; 21h:26m:33s remains)
INFO - root - 2017-12-15 06:37:19.480805: step 9080, loss = 0.21, batch loss = 0.17 (34.3 examples/sec; 0.233 sec/batch; 20h:58m:29s remains)
INFO - root - 2017-12-15 06:37:21.794808: step 9090, loss = 0.27, batch loss = 0.24 (34.8 examples/sec; 0.230 sec/batch; 20h:37m:53s remains)
INFO - root - 2017-12-15 06:37:24.047954: step 9100, loss = 0.28, batch loss = 0.25 (35.9 examples/sec; 0.223 sec/batch; 19h:59m:44s remains)
2017-12-15 06:37:24.403365: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9085124 -4.2916307 -4.5757341 -3.6685762 -2.6624281 -2.1369491 -1.386379 -0.85575366 -1.0066487 -1.5020015 -1.9489497 -1.976303 -1.7809362 -1.9283869 -2.1551969][-3.1315668 -4.3859129 -4.5801029 -3.5702374 -2.4633327 -1.8396877 -0.9371134 -0.41852403 -0.64047861 -1.1174234 -1.3884614 -1.1397884 -0.91120327 -1.1814667 -1.5116497][-3.5182424 -4.0229745 -4.1072531 -3.0722997 -1.9803224 -1.361583 -0.42838681 -0.059362888 -0.46986485 -0.99274158 -1.1234796 -0.58599937 -0.3050617 -0.64242148 -1.0290506][-4.3735652 -3.8453836 -3.7446551 -2.6636987 -1.6170127 -1.010299 -0.13347006 0.057617664 -0.63679862 -1.4140847 -1.624259 -0.90967131 -0.45333314 -0.69301653 -1.064443][-5.4653563 -4.1540031 -3.7601972 -2.4145858 -1.1189431 -0.230245 0.79831195 0.91587877 -0.23735189 -1.6360449 -2.34706 -1.7775255 -1.1310313 -1.1002952 -1.3115288][-6.7347555 -4.7330613 -4.050024 -2.2888613 -0.38965738 1.1889014 2.66204 2.9076664 1.2861655 -0.97814095 -2.6286647 -2.6172435 -1.9361293 -1.5240448 -1.3391097][-6.8102245 -5.3101172 -4.5620041 -2.6314538 -0.23533118 2.0747397 4.3136721 5.0196543 3.2948673 0.33045912 -2.1998703 -2.9782391 -2.5039229 -1.8055353 -1.2609725][-6.7713375 -5.6149187 -5.0043974 -3.2622638 -0.85208452 1.7882366 4.5656538 5.8254881 4.5283585 1.4828622 -1.5278201 -3.0872483 -3.0288572 -2.3397784 -1.5430138][-6.8956137 -5.7453794 -5.3726835 -4.0652866 -2.0506938 0.28899217 2.9883811 4.5872564 4.0573626 1.6804321 -1.0861294 -3.113832 -3.6009004 -3.1854913 -2.3387334][-6.95716 -5.8405018 -5.7219162 -4.8012333 -3.2223897 -1.4090827 0.89457512 2.50831 2.542979 1.0399032 -1.0770832 -3.1048348 -4.0372791 -4.0876203 -3.4366827][-6.8091135 -5.65016 -5.621305 -4.7954321 -3.3488626 -1.9700291 -0.27281213 0.86492825 1.0080566 0.14841509 -1.1730146 -2.7828469 -3.7930014 -4.2902961 -3.9187415][-6.4407806 -5.150918 -5.0314379 -4.0899634 -2.5733433 -1.4200988 -0.30131066 0.26456285 0.11560154 -0.40777493 -1.1057414 -2.0936456 -2.9224505 -3.6172452 -3.4371219][-6.0569019 -4.681076 -4.4805136 -3.4156337 -1.7248108 -0.56622541 0.28289843 0.42228436 -0.1509831 -0.79212058 -1.3196638 -1.9223232 -2.513643 -3.1176729 -2.8395371][-5.8380184 -4.47012 -4.2822819 -3.140425 -1.3124704 -0.019936085 0.83747435 0.87403607 0.0028436184 -1.0238875 -1.6760876 -2.1544991 -2.5275853 -2.868058 -2.4021769][-5.8564591 -4.6504688 -4.697938 -3.7480361 -2.0411551 -0.6730423 0.42134309 0.61849117 -0.39926994 -1.7404623 -2.4940352 -3.0115743 -3.2517967 -3.3756909 -2.80093]]...]
INFO - root - 2017-12-15 06:37:26.716592: step 9110, loss = 0.28, batch loss = 0.25 (35.1 examples/sec; 0.228 sec/batch; 20h:29m:24s remains)
INFO - root - 2017-12-15 06:37:28.979519: step 9120, loss = 0.26, batch loss = 0.22 (35.7 examples/sec; 0.224 sec/batch; 20h:06m:12s remains)
INFO - root - 2017-12-15 06:37:31.285106: step 9130, loss = 0.22, batch loss = 0.18 (33.2 examples/sec; 0.241 sec/batch; 21h:37m:59s remains)
INFO - root - 2017-12-15 06:37:33.531852: step 9140, loss = 0.34, batch loss = 0.30 (35.9 examples/sec; 0.223 sec/batch; 20h:02m:27s remains)
INFO - root - 2017-12-15 06:37:35.834186: step 9150, loss = 0.23, batch loss = 0.19 (33.3 examples/sec; 0.240 sec/batch; 21h:34m:37s remains)
INFO - root - 2017-12-15 06:37:38.143850: step 9160, loss = 0.35, batch loss = 0.32 (33.5 examples/sec; 0.239 sec/batch; 21h:27m:58s remains)
INFO - root - 2017-12-15 06:37:40.412700: step 9170, loss = 0.32, batch loss = 0.28 (35.3 examples/sec; 0.227 sec/batch; 20h:22m:01s remains)
INFO - root - 2017-12-15 06:37:42.704561: step 9180, loss = 0.32, batch loss = 0.28 (34.4 examples/sec; 0.232 sec/batch; 20h:51m:32s remains)
INFO - root - 2017-12-15 06:37:44.978076: step 9190, loss = 0.23, batch loss = 0.19 (36.1 examples/sec; 0.222 sec/batch; 19h:53m:35s remains)
INFO - root - 2017-12-15 06:37:47.267013: step 9200, loss = 0.25, batch loss = 0.21 (35.1 examples/sec; 0.228 sec/batch; 20h:26m:52s remains)
2017-12-15 06:37:47.617408: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2496428 -5.9346423 -6.4250736 -6.5136309 -6.2110844 -5.9213233 -5.7229762 -5.503664 -5.5107851 -5.8343678 -6.0635142 -6.107008 -6.1494932 -5.95945 -5.4199986][-5.0433569 -7.1660309 -7.7382374 -7.7147655 -7.0763493 -6.4011431 -5.7505269 -5.337883 -5.5879488 -6.3749371 -6.81938 -6.9578304 -7.2115717 -7.1755915 -6.6787786][-6.515111 -7.6971703 -8.30437 -8.1075678 -7.0399313 -5.8632555 -4.6764941 -3.9784875 -4.4459085 -5.7481804 -6.6334238 -7.1665316 -7.72684 -7.8177729 -7.3091173][-7.7471952 -8.1728258 -8.6775322 -8.1867075 -6.6244173 -4.9441643 -3.1736059 -2.15035 -2.7503133 -4.4270377 -5.7982464 -7.0002069 -7.7900972 -7.926981 -7.4498348][-8.7982025 -8.6595078 -8.8827505 -7.984724 -5.8246956 -3.5318418 -1.0322045 0.38021016 -0.39592886 -2.3638082 -4.1535349 -5.992672 -6.8993177 -7.2000227 -6.9993887][-8.8461781 -8.7540131 -8.7343035 -7.6324577 -5.0960541 -2.334801 0.94893861 3.1173756 2.3909433 0.11800265 -2.1664 -4.5434623 -5.4079018 -5.88235 -6.0891466][-8.4302683 -8.764678 -8.5155172 -7.2096224 -4.3049774 -1.1461446 2.8105428 5.5756245 4.8874397 2.2283857 -0.57855654 -3.3433492 -4.1490936 -4.7037554 -5.1061878][-8.6788673 -8.8930988 -8.4761 -7.0008144 -3.8270636 -0.34809387 3.9951942 7.00809 6.2746248 3.2772896 0.11305237 -2.98511 -3.88019 -4.4616909 -4.7128987][-8.6517839 -9.02341 -8.71643 -7.3166122 -4.1842117 -0.5724144 3.8482172 6.8446436 6.2403927 3.2415106 0.018252134 -3.2161863 -4.2669997 -4.8572664 -4.7937937][-8.6672945 -9.3165932 -9.3858328 -8.375411 -5.6075153 -2.0526111 2.178189 4.8231764 4.5182381 2.0158961 -0.83999646 -3.8425934 -4.8775721 -5.4868736 -5.22999][-8.8372278 -9.7280264 -10.217052 -9.60763 -7.2270536 -3.9425156 -0.17098081 2.091572 2.1721923 0.31533408 -2.1153455 -4.6243315 -5.5400357 -6.1783257 -5.8791571][-8.8131552 -9.783289 -10.551807 -10.236454 -8.25352 -5.499258 -2.4852748 -0.65246367 -0.31825364 -1.5996385 -3.6791658 -5.5700817 -6.3081341 -6.8693514 -6.5900669][-8.4631863 -9.3407469 -10.299419 -10.307306 -8.8505831 -6.7663822 -4.5780544 -3.2618861 -2.9825015 -4.0219665 -5.6369247 -6.781621 -7.2108603 -7.5043745 -7.17562][-7.9274921 -8.5553913 -9.556077 -9.7698517 -8.8154478 -7.373909 -5.9850292 -5.2028961 -5.1462774 -5.9798279 -7.014802 -7.5391188 -7.6399307 -7.5615435 -7.1741133][-7.0966835 -7.3192739 -8.0681553 -8.1842289 -7.563035 -6.6695967 -5.943367 -5.6238289 -5.8026447 -6.4372177 -6.9796553 -7.06155 -6.9392347 -6.7109356 -6.3798819]]...]
INFO - root - 2017-12-15 06:37:49.869936: step 9210, loss = 0.20, batch loss = 0.16 (36.1 examples/sec; 0.221 sec/batch; 19h:53m:21s remains)
INFO - root - 2017-12-15 06:37:52.168231: step 9220, loss = 0.27, batch loss = 0.24 (34.7 examples/sec; 0.231 sec/batch; 20h:42m:21s remains)
INFO - root - 2017-12-15 06:37:54.460367: step 9230, loss = 0.34, batch loss = 0.30 (33.3 examples/sec; 0.240 sec/batch; 21h:33m:44s remains)
INFO - root - 2017-12-15 06:37:56.792070: step 9240, loss = 0.30, batch loss = 0.27 (34.4 examples/sec; 0.233 sec/batch; 20h:52m:55s remains)
INFO - root - 2017-12-15 06:37:59.089723: step 9250, loss = 0.22, batch loss = 0.19 (36.3 examples/sec; 0.220 sec/batch; 19h:46m:52s remains)
INFO - root - 2017-12-15 06:38:01.381975: step 9260, loss = 0.21, batch loss = 0.17 (34.0 examples/sec; 0.235 sec/batch; 21h:06m:32s remains)
INFO - root - 2017-12-15 06:38:03.638901: step 9270, loss = 0.20, batch loss = 0.17 (36.3 examples/sec; 0.220 sec/batch; 19h:47m:21s remains)
INFO - root - 2017-12-15 06:38:05.906702: step 9280, loss = 0.31, batch loss = 0.27 (34.3 examples/sec; 0.233 sec/batch; 20h:55m:25s remains)
INFO - root - 2017-12-15 06:38:08.235962: step 9290, loss = 0.33, batch loss = 0.29 (34.5 examples/sec; 0.232 sec/batch; 20h:49m:05s remains)
INFO - root - 2017-12-15 06:38:10.498220: step 9300, loss = 0.30, batch loss = 0.27 (35.5 examples/sec; 0.225 sec/batch; 20h:12m:12s remains)
2017-12-15 06:38:10.831959: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.408509 -2.6484339 -2.1558015 -1.7553945 -1.6464344 -1.9064064 -1.4036717 -0.99251485 -1.3131684 -1.7123269 -1.8072058 -2.513871 -3.5593147 -3.7383995 -3.4819951][-2.5391679 -2.0850132 -1.4729702 -1.0067625 -0.91134489 -1.3021233 -0.96158552 -0.563761 -0.75948346 -1.0293294 -1.118838 -1.9054283 -3.1031318 -3.48248 -3.3953066][-2.6911745 -1.7245567 -1.2278912 -0.98302007 -1.1132859 -1.6286678 -1.3211179 -0.6872077 -0.55698776 -0.61312735 -0.65267038 -1.5006804 -2.826405 -3.3743925 -3.3555994][-3.0637531 -1.7415222 -1.4803357 -1.6283196 -2.0056748 -2.5002525 -2.0243437 -0.9863106 -0.42677569 -0.25261414 -0.2722851 -1.1803508 -2.6448579 -3.3832114 -3.4249113][-3.5754814 -2.0695448 -1.9774148 -2.3372574 -2.6681876 -2.9115789 -2.1613019 -0.70904326 0.0893693 0.30872607 0.23494673 -0.74559915 -2.3271706 -3.2093263 -3.3243279][-3.453063 -2.1146581 -2.1277819 -2.5693293 -2.7675743 -2.779177 -1.8769431 -0.17732835 0.66863012 0.86087894 0.73286295 -0.25878978 -1.8472061 -2.7904768 -3.0414777][-2.554446 -1.6869197 -1.7778299 -2.1550663 -2.095823 -1.9061489 -1.0051149 0.62226081 1.2636862 1.3020105 1.0642965 0.062754393 -1.4550307 -2.4391475 -2.8499117][-1.7140548 -1.1418791 -1.3375145 -1.6289177 -1.2752018 -0.91907752 -0.16766739 1.1458657 1.4929326 1.3491418 0.96999788 -0.045886755 -1.4434893 -2.3755386 -2.8806553][-1.2878468 -0.7772119 -1.0329089 -1.360528 -0.99791634 -0.65757573 -0.17407179 0.69979525 0.82155919 0.60646105 0.17378521 -0.78969085 -1.9512323 -2.7182987 -3.1649992][-1.6954427 -1.0727369 -1.2722237 -1.6336699 -1.3464957 -1.0566957 -0.84827662 -0.44199467 -0.46168804 -0.66769266 -1.1149694 -1.9693583 -2.8582716 -3.40904 -3.6611443][-2.4757259 -1.7385929 -1.9389379 -2.2926557 -2.0909879 -1.8706837 -1.8400768 -1.730835 -1.7894999 -1.9381644 -2.3059812 -3.0301542 -3.7356329 -4.1131411 -4.1877794][-3.40948 -2.5811007 -2.7137709 -2.9807696 -2.8414834 -2.6468453 -2.6303656 -2.5488443 -2.5227952 -2.5570765 -2.8001635 -3.3915486 -3.9913549 -4.3004322 -4.3021908][-4.3859453 -3.43013 -3.4247346 -3.4812686 -3.2667296 -3.0625021 -2.9343719 -2.6355226 -2.4099982 -2.3088536 -2.3860016 -2.8498282 -3.4793355 -3.9132857 -4.0165653][-5.0480828 -3.8815496 -3.7008715 -3.5141246 -3.1768334 -2.9829147 -2.6936164 -2.075599 -1.691902 -1.5006177 -1.4455193 -1.8975437 -2.7172709 -3.3311257 -3.6154377][-5.2253876 -3.9426556 -3.6875052 -3.3637204 -3.014931 -2.92697 -2.4731541 -1.4794136 -0.97291255 -0.7859323 -0.67519951 -1.1911635 -2.2003367 -2.9024172 -3.2935948]]...]
INFO - root - 2017-12-15 06:38:13.107568: step 9310, loss = 0.43, batch loss = 0.39 (32.9 examples/sec; 0.243 sec/batch; 21h:50m:56s remains)
INFO - root - 2017-12-15 06:38:15.370705: step 9320, loss = 0.26, batch loss = 0.23 (35.6 examples/sec; 0.225 sec/batch; 20h:11m:08s remains)
INFO - root - 2017-12-15 06:38:17.632442: step 9330, loss = 0.27, batch loss = 0.24 (36.2 examples/sec; 0.221 sec/batch; 19h:49m:19s remains)
INFO - root - 2017-12-15 06:38:19.892844: step 9340, loss = 0.42, batch loss = 0.38 (33.9 examples/sec; 0.236 sec/batch; 21h:09m:13s remains)
INFO - root - 2017-12-15 06:38:22.191236: step 9350, loss = 0.20, batch loss = 0.17 (33.0 examples/sec; 0.242 sec/batch; 21h:45m:49s remains)
INFO - root - 2017-12-15 06:38:24.475861: step 9360, loss = 0.27, batch loss = 0.23 (36.0 examples/sec; 0.222 sec/batch; 19h:57m:02s remains)
INFO - root - 2017-12-15 06:38:26.774029: step 9370, loss = 0.44, batch loss = 0.40 (35.7 examples/sec; 0.224 sec/batch; 20h:06m:09s remains)
INFO - root - 2017-12-15 06:38:29.064386: step 9380, loss = 0.32, batch loss = 0.28 (33.4 examples/sec; 0.239 sec/batch; 21h:28m:11s remains)
INFO - root - 2017-12-15 06:38:31.325249: step 9390, loss = 0.22, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 19h:59m:35s remains)
INFO - root - 2017-12-15 06:38:33.574810: step 9400, loss = 0.32, batch loss = 0.28 (35.2 examples/sec; 0.227 sec/batch; 20h:25m:00s remains)
2017-12-15 06:38:33.878381: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6118493 -5.9338827 -5.9565277 -6.4599319 -7.0416179 -7.4227076 -7.6191721 -7.6749606 -7.40476 -7.0037279 -6.7087436 -6.3495631 -6.0083995 -5.6388535 -5.5423365][-6.2373166 -6.6394968 -6.8149533 -7.4342308 -8.0517006 -8.39968 -8.5819378 -8.6275311 -8.2506695 -7.7119474 -7.2299829 -6.6896873 -6.2974672 -5.705513 -5.4707351][-6.9688358 -5.9329448 -5.9915051 -6.5838223 -7.1911097 -7.5974483 -7.7107868 -7.6857438 -7.3145275 -6.8617744 -6.3842378 -5.8349133 -5.54725 -5.0399847 -4.91553][-6.8612676 -4.8990622 -4.9702778 -5.4133716 -5.8028927 -6.1120849 -6.0925264 -5.9642568 -5.6417837 -5.324439 -4.99929 -4.5350666 -4.3075371 -3.9092691 -3.9982958][-6.6999416 -4.2651758 -4.3549795 -4.4490643 -4.3217115 -4.0799761 -3.5633538 -3.1423874 -2.8878689 -2.8619485 -2.9315717 -2.7762451 -2.7553997 -2.5559864 -2.8004754][-6.0778904 -3.8055878 -3.8697119 -3.58005 -2.8260291 -1.8143194 -0.61915851 0.084946394 0.10197115 -0.30047333 -0.79894567 -0.89790189 -1.0571256 -1.1571715 -1.6792762][-5.6609583 -4.0084543 -3.8894591 -3.1328483 -1.6301258 0.26531339 1.9807792 2.8159442 2.4117861 1.4820092 0.6198442 0.28765297 -0.040079594 -0.39274 -1.2374136][-6.363009 -4.7394724 -4.2056036 -2.8411555 -0.67150927 1.8911252 4.0041537 5.0085626 4.2660432 2.7556281 1.5163512 0.95334673 0.50139 0.069414854 -0.91667557][-7.404048 -5.5538764 -4.450954 -2.4937806 0.15444684 3.064136 5.2064457 6.0769286 5.001451 2.9635921 1.4468043 0.76218653 0.40627146 0.25831294 -0.53867662][-8.3479986 -6.236176 -4.6756763 -2.3854589 0.35090709 3.0081496 4.6045828 5.0556197 3.8098254 1.5402777 -0.095715761 -0.85938096 -1.044993 -0.635877 -0.72616506][-8.5861759 -6.2772131 -4.7361536 -2.6367965 -0.27498329 1.8242421 2.8962746 3.0893879 2.0198836 -0.10660076 -1.5899631 -2.248013 -2.2442203 -1.3769103 -0.75785339][-8.1507378 -5.6681557 -4.3720503 -2.7520542 -1.0700828 0.25167322 0.77622509 0.71697259 -0.083368778 -1.7386392 -2.7661581 -3.1431956 -3.0033953 -2.0182865 -0.9580363][-7.6944723 -5.203517 -4.231988 -3.1346025 -2.1598592 -1.555867 -1.4418948 -1.6508753 -2.1107926 -3.1376514 -3.5637629 -3.57203 -3.4124749 -2.5984106 -1.4435207][-7.0991125 -4.877574 -4.3635941 -3.8485765 -3.5244207 -3.3844504 -3.3485208 -3.5395117 -3.831111 -4.4102435 -4.4239407 -4.1330671 -3.8421764 -3.15657 -2.0387585][-6.2883463 -4.4139061 -4.3561153 -4.4102607 -4.5822926 -4.7515221 -4.8625526 -5.1699085 -5.415904 -5.711133 -5.5436811 -5.1315756 -4.7647038 -4.1238608 -3.1766691]]...]
INFO - root - 2017-12-15 06:38:36.137296: step 9410, loss = 0.38, batch loss = 0.35 (34.9 examples/sec; 0.229 sec/batch; 20h:33m:17s remains)
INFO - root - 2017-12-15 06:38:38.414399: step 9420, loss = 0.30, batch loss = 0.27 (35.5 examples/sec; 0.225 sec/batch; 20h:12m:57s remains)
INFO - root - 2017-12-15 06:38:40.712240: step 9430, loss = 0.31, batch loss = 0.27 (31.7 examples/sec; 0.253 sec/batch; 22h:40m:55s remains)
INFO - root - 2017-12-15 06:38:42.975206: step 9440, loss = 0.31, batch loss = 0.27 (36.0 examples/sec; 0.222 sec/batch; 19h:57m:50s remains)
INFO - root - 2017-12-15 06:38:45.224678: step 9450, loss = 0.22, batch loss = 0.19 (35.3 examples/sec; 0.226 sec/batch; 20h:19m:22s remains)
INFO - root - 2017-12-15 06:38:47.488343: step 9460, loss = 0.36, batch loss = 0.33 (34.8 examples/sec; 0.230 sec/batch; 20h:36m:10s remains)
INFO - root - 2017-12-15 06:38:49.807020: step 9470, loss = 0.19, batch loss = 0.15 (33.5 examples/sec; 0.238 sec/batch; 21h:23m:56s remains)
INFO - root - 2017-12-15 06:38:52.090977: step 9480, loss = 0.39, batch loss = 0.36 (35.6 examples/sec; 0.225 sec/batch; 20h:09m:29s remains)
INFO - root - 2017-12-15 06:38:54.388743: step 9490, loss = 0.26, batch loss = 0.22 (33.4 examples/sec; 0.240 sec/batch; 21h:29m:56s remains)
INFO - root - 2017-12-15 06:38:56.690128: step 9500, loss = 0.28, batch loss = 0.25 (34.6 examples/sec; 0.231 sec/batch; 20h:44m:40s remains)
2017-12-15 06:38:57.080125: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5284481 -4.0810728 -5.0509572 -5.9505258 -5.869514 -4.5697231 -2.6909702 -1.3532429 -1.060583 -1.6057291 -2.9419417 -4.7480888 -5.58331 -5.9404798 -5.9610834][-3.2696059 -4.2335434 -5.22824 -6.1557779 -5.9679308 -4.352366 -2.1681187 -0.59509385 -0.029342413 -0.36706614 -1.5778381 -3.1858912 -4.0378981 -4.6145587 -4.7393489][-4.235425 -4.3109016 -5.2216325 -6.1657305 -5.8266392 -3.9884079 -1.6999643 0.030427933 0.756629 0.46552825 -0.66601467 -1.9886435 -2.8254445 -3.555414 -3.824352][-5.680665 -4.2977648 -4.9112821 -5.5370975 -4.9893188 -3.2593021 -1.253971 0.322793 0.92280722 0.45189023 -0.79534936 -1.7921598 -2.426805 -3.2898302 -3.663326][-6.0200109 -3.7997222 -3.9608805 -4.0995426 -3.3354077 -2.0002322 -0.49405181 0.67803788 1.0161085 0.27101088 -1.1974149 -1.9790525 -2.4299538 -3.3333173 -3.7449811][-4.9807363 -2.6424768 -2.2681174 -1.8357053 -0.94592893 0.00036668777 1.0387335 1.6660588 1.615855 0.46555853 -1.3891999 -2.1445291 -2.5758057 -3.3129039 -3.6134076][-3.5341284 -1.3896644 -0.6448468 0.021038055 0.74711442 1.5243104 2.5929458 3.2564633 3.1555598 1.7109058 -0.38372433 -1.417197 -1.9544126 -2.3926394 -2.5439894][-3.1492553 -0.72058988 0.10155272 0.60454249 0.88291311 1.3991406 2.5263889 3.6190493 3.8440573 2.4192336 0.48991823 -0.72477174 -1.1634881 -1.3126681 -1.3746305][-3.7758305 -1.1005858 -0.088332891 0.33317518 0.3303287 0.53354549 1.5989201 2.9353259 3.4055221 2.2365806 0.67768717 -0.60323393 -1.1412954 -1.1534278 -1.196413][-5.2302446 -2.4518721 -1.3098252 -0.8016243 -1.0083255 -1.1341546 -0.26652122 0.94617629 1.5579865 0.85763907 -0.014627218 -1.0015128 -1.6815336 -1.7729571 -1.8238647][-6.6544914 -4.26647 -3.2587764 -2.8039382 -3.1965337 -3.5318475 -2.952424 -2.1211679 -1.5789149 -1.7377237 -1.8135868 -2.2427893 -2.9118202 -3.1023917 -3.0258069][-7.2119923 -5.6146917 -5.1410074 -5.1159134 -5.7365141 -6.144207 -5.8878713 -5.4944077 -5.06355 -4.684453 -4.0859346 -4.0418262 -4.2604065 -4.2028065 -3.8676558][-6.9112453 -5.9538841 -6.0159125 -6.3643665 -7.0262041 -7.4783211 -7.5466547 -7.5334816 -7.2711592 -6.68896 -5.9416857 -5.7129431 -5.5033512 -5.0927305 -4.5924811][-6.0381942 -5.4510813 -5.80929 -6.3468666 -6.958744 -7.3633537 -7.5703621 -7.676178 -7.5224648 -7.07722 -6.6309223 -6.3782339 -6.0054541 -5.3876448 -4.9551239][-4.8524213 -4.3302541 -4.6290331 -5.1017923 -5.5610247 -5.8793178 -6.13122 -6.2759972 -6.2592964 -6.1363754 -6.0217004 -5.8535995 -5.4732227 -4.9347229 -4.6581664]]...]
INFO - root - 2017-12-15 06:38:59.355804: step 9510, loss = 0.19, batch loss = 0.15 (35.4 examples/sec; 0.226 sec/batch; 20h:17m:18s remains)
INFO - root - 2017-12-15 06:39:01.655184: step 9520, loss = 0.37, batch loss = 0.33 (34.7 examples/sec; 0.231 sec/batch; 20h:41m:46s remains)
INFO - root - 2017-12-15 06:39:03.904648: step 9530, loss = 0.44, batch loss = 0.41 (35.4 examples/sec; 0.226 sec/batch; 20h:15m:09s remains)
INFO - root - 2017-12-15 06:39:06.188634: step 9540, loss = 0.20, batch loss = 0.17 (34.2 examples/sec; 0.234 sec/batch; 20h:59m:36s remains)
INFO - root - 2017-12-15 06:39:08.445871: step 9550, loss = 0.25, batch loss = 0.22 (35.1 examples/sec; 0.228 sec/batch; 20h:27m:23s remains)
INFO - root - 2017-12-15 06:39:10.739654: step 9560, loss = 0.52, batch loss = 0.49 (35.8 examples/sec; 0.224 sec/batch; 20h:03m:17s remains)
INFO - root - 2017-12-15 06:39:12.984224: step 9570, loss = 0.24, batch loss = 0.20 (35.8 examples/sec; 0.223 sec/batch; 20h:02m:25s remains)
INFO - root - 2017-12-15 06:39:15.262216: step 9580, loss = 0.24, batch loss = 0.20 (34.1 examples/sec; 0.235 sec/batch; 21h:03m:56s remains)
INFO - root - 2017-12-15 06:39:17.556000: step 9590, loss = 0.40, batch loss = 0.37 (35.3 examples/sec; 0.226 sec/batch; 20h:18m:09s remains)
INFO - root - 2017-12-15 06:39:19.839585: step 9600, loss = 0.29, batch loss = 0.26 (36.4 examples/sec; 0.220 sec/batch; 19h:43m:32s remains)
2017-12-15 06:39:20.181021: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6419785 -5.8610697 -5.97598 -5.5551376 -4.723876 -3.7122931 -2.8762159 -1.8018144 -0.99486351 -0.23911691 -0.095896959 -0.78549242 -1.7152169 -2.102253 -1.2684145][-4.0541191 -5.4511375 -5.522378 -5.0422578 -3.9731977 -2.7380455 -1.8581543 -0.90578794 -0.25257957 0.39759707 0.495811 -0.18006599 -1.3654418 -2.0017996 -1.2974384][-3.8277593 -4.4437757 -4.5004439 -4.0658922 -2.9012182 -1.4967542 -0.70508623 -0.021212101 0.499722 1.0685701 1.0106127 -0.008939743 -1.6007841 -2.5944271 -1.9279279][-3.5594018 -3.5309343 -3.6417303 -3.3370905 -2.2050543 -0.68187618 0.0022549629 0.22141981 0.41764092 0.8689425 0.71658587 -0.46902931 -2.1425056 -3.134105 -2.4021006][-3.2793555 -2.6706891 -2.9217925 -2.7887874 -1.7231315 -0.026574135 0.619251 0.37543821 0.10468984 0.38931131 0.17966318 -1.1222678 -2.7819064 -3.7581403 -3.1683354][-2.115432 -1.4495356 -1.8088853 -1.7755744 -0.65217841 1.2759666 2.1587086 1.7415912 0.989696 0.887594 0.42404366 -1.257206 -3.1781411 -4.4768982 -4.320828][-0.60988057 -0.50054491 -0.90669024 -0.86061358 0.37285924 2.395699 3.4937963 3.1858606 2.1212282 1.6674869 0.95582104 -1.0353611 -3.1459384 -4.6220136 -4.8344259][-0.27450442 -0.28368318 -0.54571283 -0.37977314 0.95456934 2.88166 3.9700584 3.8174725 2.6060939 1.9365582 1.1664417 -0.81699312 -2.89789 -4.408164 -4.8692288][-0.57288182 -0.26184309 -0.057303429 0.38141966 1.8656375 3.7083559 4.7509017 4.748137 3.4994841 2.4782052 1.4880443 -0.57531464 -2.64175 -4.1784673 -4.7338495][-1.1001338 -0.36375582 0.43946695 1.1576779 2.5781307 4.1949553 5.0946441 5.0924416 3.8314385 2.5486622 1.3645158 -0.696731 -2.6174216 -3.9142926 -4.3946295][-1.7959201 -0.8706497 0.16289735 0.83751297 1.7657084 2.9565177 3.6181331 3.52073 2.3521366 1.146857 0.1249826 -1.6304005 -3.2324898 -4.1113014 -4.2669411][-2.6259806 -1.7237589 -0.78136623 -0.22878802 0.27309895 1.0518084 1.5711648 1.3878441 0.43011594 -0.53718996 -1.3491559 -2.780061 -4.0907097 -4.7257795 -4.5952616][-4.2421293 -3.1813169 -2.1485908 -1.4417154 -1.1463634 -0.74967086 -0.46911478 -0.76563847 -1.5282233 -2.3314421 -2.9589517 -4.04786 -4.9885311 -5.30331 -4.9462056][-6.0082769 -4.8283939 -3.9194031 -3.3354216 -3.3458588 -3.400435 -3.4514215 -3.8831747 -4.4739323 -5.0479226 -5.3918147 -5.9311361 -6.3659053 -6.327611 -5.7210016][-7.0121531 -5.8876009 -5.436161 -5.2118993 -5.4705381 -5.7796488 -5.930954 -6.1615667 -6.4155173 -6.6771841 -6.76212 -6.945982 -7.1182613 -6.9959497 -6.254879]]...]
INFO - root - 2017-12-15 06:39:22.484231: step 9610, loss = 0.30, batch loss = 0.27 (34.8 examples/sec; 0.230 sec/batch; 20h:36m:30s remains)
INFO - root - 2017-12-15 06:39:24.749434: step 9620, loss = 0.32, batch loss = 0.29 (35.0 examples/sec; 0.229 sec/batch; 20h:31m:45s remains)
INFO - root - 2017-12-15 06:39:26.981114: step 9630, loss = 0.36, batch loss = 0.33 (35.2 examples/sec; 0.227 sec/batch; 20h:21m:41s remains)
INFO - root - 2017-12-15 06:39:29.281856: step 9640, loss = 0.29, batch loss = 0.26 (35.8 examples/sec; 0.224 sec/batch; 20h:03m:53s remains)
INFO - root - 2017-12-15 06:39:31.527318: step 9650, loss = 0.27, batch loss = 0.24 (35.7 examples/sec; 0.224 sec/batch; 20h:06m:49s remains)
INFO - root - 2017-12-15 06:39:33.801924: step 9660, loss = 0.23, batch loss = 0.19 (34.2 examples/sec; 0.234 sec/batch; 20h:58m:07s remains)
INFO - root - 2017-12-15 06:39:36.106398: step 9670, loss = 0.24, batch loss = 0.20 (33.8 examples/sec; 0.236 sec/batch; 21h:12m:26s remains)
INFO - root - 2017-12-15 06:39:38.427942: step 9680, loss = 0.25, batch loss = 0.22 (33.3 examples/sec; 0.240 sec/batch; 21h:31m:03s remains)
INFO - root - 2017-12-15 06:39:40.709026: step 9690, loss = 0.44, batch loss = 0.41 (34.6 examples/sec; 0.231 sec/batch; 20h:43m:01s remains)
INFO - root - 2017-12-15 06:39:42.991572: step 9700, loss = 0.23, batch loss = 0.20 (34.4 examples/sec; 0.232 sec/batch; 20h:49m:42s remains)
2017-12-15 06:39:43.320937: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1671486 -7.1492753 -7.2911515 -7.0607519 -6.5632696 -5.9566574 -5.539782 -5.5600138 -6.1934352 -6.9262948 -7.0842919 -6.850924 -6.3975782 -5.7233343 -4.9965219][-5.60295 -7.2591105 -7.52795 -7.2627835 -6.7688427 -6.07876 -5.5430622 -5.5684729 -6.4537663 -7.5804052 -7.933465 -7.8041868 -7.3475041 -6.5022259 -5.4882536][-4.8484907 -6.3636017 -6.7842207 -6.3251252 -5.6649113 -4.718008 -3.9847589 -4.0417819 -5.3451071 -7.0816712 -7.872963 -8.1252728 -7.9152737 -6.9923348 -5.7112961][-4.7087908 -5.9261136 -6.4820995 -5.7700949 -4.8043633 -3.4777234 -2.397665 -2.4698174 -4.1640797 -6.5085654 -7.7556438 -8.5194778 -8.5759687 -7.587213 -6.0054731][-4.3007479 -5.2237625 -5.8693657 -4.9994688 -3.7371659 -1.9647461 -0.48680139 -0.52796757 -2.5626986 -5.3662043 -7.0181155 -8.3399353 -8.7967548 -7.9066963 -6.1531019][-3.9845374 -4.8334475 -5.4534464 -4.3693552 -2.7070372 -0.45905566 1.4882469 1.5292747 -0.68696463 -3.7970295 -5.8996973 -7.8585634 -8.7449322 -8.0689087 -6.2735548][-3.9649463 -4.7951984 -5.1364155 -3.9460201 -1.9319949 0.68930745 2.9659715 3.1893177 0.97399354 -2.3693297 -4.8839378 -7.3696122 -8.6180115 -8.1126652 -6.3555932][-3.9797895 -4.8084612 -5.0144234 -3.8934426 -1.5680918 1.3088486 3.8408103 4.28719 2.2439752 -1.1613011 -3.9408691 -6.7626553 -8.3075485 -7.9924622 -6.3461561][-4.1875644 -4.9946632 -5.3431864 -4.517942 -2.1563873 0.75068283 3.3138843 3.9270229 2.1477885 -1.0749378 -3.8626285 -6.607234 -8.2234583 -7.9795861 -6.4319553][-4.3101377 -4.8851714 -5.42717 -5.06151 -3.0247703 -0.36250091 2.0712261 2.719346 1.1276305 -1.8146348 -4.3910131 -6.7564378 -8.1920128 -7.8870878 -6.4147782][-4.0658646 -4.4959855 -5.2707815 -5.3937883 -3.8617635 -1.5443797 0.63578367 1.3062689 0.016317606 -2.5202096 -4.820354 -6.8101044 -8.03305 -7.6722646 -6.3369241][-3.3602076 -3.6620955 -4.5921016 -5.2068796 -4.4371986 -2.8436563 -1.1983256 -0.65957224 -1.6606389 -3.6840172 -5.5568838 -7.0588255 -7.859591 -7.3841405 -6.18356][-3.4683261 -3.5977705 -4.5270343 -5.3166466 -5.1092548 -4.1812277 -3.0067558 -2.5265889 -3.1953928 -4.7183781 -6.19146 -7.2530136 -7.7218056 -7.1243172 -6.0195379][-4.039454 -4.0362611 -4.8813267 -5.6397362 -5.7437649 -5.28403 -4.4637733 -4.039588 -4.468359 -5.4886446 -6.5239573 -7.1652942 -7.3621564 -6.6699038 -5.6998444][-4.5494375 -4.2264781 -4.8913746 -5.65909 -6.1147156 -6.0824733 -5.5086174 -5.1648283 -5.4526415 -6.0501828 -6.6312461 -6.9266496 -6.8399887 -6.06092 -5.2865696]]...]
INFO - root - 2017-12-15 06:39:45.591646: step 9710, loss = 0.24, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 20h:04m:26s remains)
INFO - root - 2017-12-15 06:39:47.860524: step 9720, loss = 0.32, batch loss = 0.29 (36.1 examples/sec; 0.222 sec/batch; 19h:52m:17s remains)
INFO - root - 2017-12-15 06:39:50.134579: step 9730, loss = 0.35, batch loss = 0.32 (34.8 examples/sec; 0.230 sec/batch; 20h:38m:00s remains)
INFO - root - 2017-12-15 06:39:52.407516: step 9740, loss = 0.31, batch loss = 0.28 (35.1 examples/sec; 0.228 sec/batch; 20h:27m:15s remains)
INFO - root - 2017-12-15 06:39:54.665797: step 9750, loss = 0.31, batch loss = 0.27 (35.1 examples/sec; 0.228 sec/batch; 20h:25m:37s remains)
INFO - root - 2017-12-15 06:39:56.929085: step 9760, loss = 0.23, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 19h:58m:43s remains)
INFO - root - 2017-12-15 06:39:59.199661: step 9770, loss = 0.26, batch loss = 0.23 (33.1 examples/sec; 0.242 sec/batch; 21h:40m:45s remains)
INFO - root - 2017-12-15 06:40:01.480178: step 9780, loss = 0.19, batch loss = 0.15 (35.1 examples/sec; 0.228 sec/batch; 20h:25m:12s remains)
INFO - root - 2017-12-15 06:40:03.780909: step 9790, loss = 0.25, batch loss = 0.22 (35.4 examples/sec; 0.226 sec/batch; 20h:13m:57s remains)
INFO - root - 2017-12-15 06:40:06.046806: step 9800, loss = 0.29, batch loss = 0.26 (35.7 examples/sec; 0.224 sec/batch; 20h:04m:01s remains)
2017-12-15 06:40:06.385054: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7579927 -4.03084 -3.8765049 -3.612905 -3.451175 -3.4613559 -3.5519016 -3.709712 -3.8670039 -3.9441581 -3.9322386 -4.0890765 -4.4075603 -4.6218719 -5.3163447][-4.1824789 -4.7024426 -4.3529773 -3.8913889 -3.6075826 -3.614377 -3.7765746 -4.1006842 -4.4531927 -4.7443218 -4.8881211 -4.9338479 -4.9739261 -4.88822 -5.3370242][-4.77713 -4.8214092 -4.2697916 -3.5966582 -3.1497035 -3.1069944 -3.2774718 -3.7151284 -4.2309747 -4.6831837 -4.9362454 -4.8618155 -4.7163534 -4.5237722 -4.8712687][-4.6086645 -3.8266678 -3.0765986 -2.1591518 -1.4901712 -1.3045807 -1.3763554 -1.8181159 -2.4083433 -3.0205173 -3.4474735 -3.4064655 -3.308949 -3.2576125 -3.6760492][-3.5620387 -2.2023709 -1.3744065 -0.31517828 0.49625349 0.84195828 1.033778 0.82745171 0.34967637 -0.289881 -0.87135947 -1.0210599 -1.19822 -1.5130973 -2.1140747][-2.4068346 -0.7803216 6.8187714e-05 1.0917811 1.9878771 2.5021894 3.0174115 3.1974738 3.0686438 2.5813868 1.8771293 1.3761632 0.76440239 -0.0095899105 -0.92225468][-2.6344109 -1.1695099 -0.4844048 0.53051758 1.3931725 1.9924147 2.7361276 3.2696106 3.4436233 3.1259573 2.377022 1.6572621 0.85896397 -0.094154358 -1.1214573][-4.0775385 -2.7773781 -2.320817 -1.5283194 -0.83574879 -0.25187933 0.51995015 1.1721048 1.5310996 1.4396319 0.86187506 0.17997956 -0.53811622 -1.3377409 -2.244251][-5.0239477 -3.9088817 -3.6735191 -3.0727725 -2.5280237 -2.0373402 -1.344411 -0.65604055 -0.20114994 -0.1160903 -0.49908912 -1.1405414 -1.8119503 -2.5049736 -3.3731136][-4.8426623 -3.876142 -3.7770791 -3.3134661 -2.9341617 -2.6283505 -2.2239437 -1.7646856 -1.4438596 -1.3328981 -1.5688555 -2.097353 -2.644439 -3.1258843 -3.9212632][-4.4353456 -3.4984913 -3.4042773 -3.0529723 -2.8119471 -2.6547272 -2.5353286 -2.3534129 -2.2544971 -2.183197 -2.2575581 -2.5388408 -2.8191373 -3.0519757 -3.7309506][-4.4927521 -3.6485467 -3.5830169 -3.2778018 -3.0050159 -2.6904202 -2.5208862 -2.3768394 -2.3789473 -2.309833 -2.2531238 -2.3354807 -2.4401481 -2.5236244 -3.1280813][-5.089838 -4.3921943 -4.4045658 -4.1963286 -3.9007668 -3.40162 -3.0544393 -2.7546544 -2.6162753 -2.3547647 -2.0246611 -1.8044435 -1.6564469 -1.6171532 -2.2161567][-5.7128825 -5.1139064 -5.1968136 -5.0693083 -4.7432141 -4.0713539 -3.5030012 -2.9524908 -2.5479655 -2.0084858 -1.4090424 -0.95145559 -0.64536428 -0.60907114 -1.308643][-6.1354208 -5.5073652 -5.5732484 -5.5162516 -5.2584925 -4.6363087 -4.0504975 -3.4221983 -2.8871527 -2.2628341 -1.6155713 -1.118826 -0.75545907 -0.71139812 -1.4303713]]...]
INFO - root - 2017-12-15 06:40:08.657643: step 9810, loss = 0.31, batch loss = 0.28 (34.7 examples/sec; 0.230 sec/batch; 20h:39m:39s remains)
INFO - root - 2017-12-15 06:40:10.910816: step 9820, loss = 0.28, batch loss = 0.24 (35.1 examples/sec; 0.228 sec/batch; 20h:25m:50s remains)
INFO - root - 2017-12-15 06:40:13.186598: step 9830, loss = 0.18, batch loss = 0.15 (34.7 examples/sec; 0.231 sec/batch; 20h:39m:43s remains)
INFO - root - 2017-12-15 06:40:15.472888: step 9840, loss = 0.27, batch loss = 0.23 (35.5 examples/sec; 0.225 sec/batch; 20h:12m:24s remains)
INFO - root - 2017-12-15 06:40:17.760629: step 9850, loss = 0.22, batch loss = 0.19 (34.6 examples/sec; 0.231 sec/batch; 20h:43m:08s remains)
INFO - root - 2017-12-15 06:40:20.048319: step 9860, loss = 0.25, batch loss = 0.21 (34.0 examples/sec; 0.235 sec/batch; 21h:04m:11s remains)
INFO - root - 2017-12-15 06:40:22.414337: step 9870, loss = 0.36, batch loss = 0.32 (33.8 examples/sec; 0.236 sec/batch; 21h:11m:17s remains)
INFO - root - 2017-12-15 06:40:24.689600: step 9880, loss = 0.27, batch loss = 0.23 (35.6 examples/sec; 0.225 sec/batch; 20h:09m:53s remains)
INFO - root - 2017-12-15 06:40:26.974698: step 9890, loss = 0.23, batch loss = 0.19 (34.9 examples/sec; 0.229 sec/batch; 20h:31m:42s remains)
INFO - root - 2017-12-15 06:40:29.279390: step 9900, loss = 0.24, batch loss = 0.20 (36.2 examples/sec; 0.221 sec/batch; 19h:46m:36s remains)
2017-12-15 06:40:29.573401: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3884435 -4.1709175 -4.7025557 -5.487711 -6.1794386 -6.1009045 -5.9253492 -5.5525208 -5.3671827 -5.5877295 -5.965198 -6.04712 -5.6674585 -5.4426432 -5.5358114][-3.6161008 -3.4553161 -4.0882273 -5.1010685 -5.8900065 -5.6552587 -5.3784628 -4.8896847 -4.8121338 -5.362587 -6.0523796 -6.260344 -5.6814432 -5.0504589 -4.7146187][-3.8297658 -2.3952694 -3.137059 -4.5066628 -5.3028641 -4.9978118 -4.6614666 -4.0041733 -3.9343607 -4.7269945 -5.6218004 -5.95617 -5.3320513 -4.4252939 -3.7014141][-3.6678429 -1.5513763 -2.1876848 -3.4838009 -4.0498934 -3.5533323 -3.2437208 -2.5715239 -2.5750279 -3.5878925 -4.5970454 -4.9826722 -4.3550072 -3.3101678 -2.377749][-3.3687649 -1.2282622 -1.6423488 -2.6609852 -2.6226332 -1.6784675 -1.2826794 -0.66000593 -0.91137075 -2.3398066 -3.6529531 -4.2402 -3.7266173 -2.7008386 -1.7176403][-3.4116058 -1.4202964 -1.7925195 -2.5792937 -2.016304 -0.71946216 -0.23592329 0.37886834 0.0352993 -1.5381703 -2.9550376 -3.7441621 -3.5274057 -2.7367084 -1.9654635][-2.7254102 -1.5761189 -1.8524313 -2.3310914 -1.5132341 -0.24768555 0.08830142 0.56422091 0.286232 -1.0187124 -2.2337132 -3.1823835 -3.283524 -2.8378294 -2.4582438][-2.1835659 -0.92098475 -0.91766405 -0.99777246 -0.16542482 0.72908592 0.71974659 0.84848046 0.54596448 -0.54440224 -1.5434284 -2.5063763 -2.6130955 -2.1686642 -1.8145753][-1.5545119 -0.087013245 0.19005823 0.30971074 0.899683 1.2641153 0.96921968 0.80180454 0.43941808 -0.58385742 -1.5616398 -2.4531789 -2.4587533 -1.8858057 -1.3836646][-1.0146717 0.84892511 1.4036629 1.5433557 1.6659343 1.5231619 0.94923639 0.49633074 0.032775164 -0.90903223 -1.7222091 -2.4927177 -2.4931448 -1.8827527 -1.3179433][-0.83633006 1.3362267 2.1589761 2.4115572 2.2657032 1.7810845 0.94214773 0.11963391 -0.46011078 -1.1760139 -1.8156904 -2.4131863 -2.3479655 -1.7328417 -0.99668324][-1.1497011 0.96457148 1.9014268 2.1313672 1.6918395 1.1248729 0.25963831 -0.62589455 -1.1509221 -1.6503094 -2.1075282 -2.5833158 -2.4634485 -1.7053919 -0.81828153][-1.8237408 0.0898025 0.85178828 0.82521558 0.1030879 -0.49977875 -1.1452577 -1.92228 -2.3286278 -2.5778873 -2.8620789 -3.1549664 -2.9202676 -2.1392362 -1.3173299][-2.8103797 -1.1466341 -0.58884656 -0.83897185 -1.7542459 -2.3255322 -2.7952485 -3.4206696 -3.7666941 -3.9026628 -3.9329138 -3.9832931 -3.7009802 -3.0744054 -2.5066357][-4.2346668 -2.9096591 -2.6312785 -2.9955883 -3.7893333 -4.2599192 -4.5527143 -4.9709578 -5.0892887 -4.9962416 -4.9204464 -4.8144684 -4.475596 -3.9864776 -3.6295314]]...]
INFO - root - 2017-12-15 06:40:31.851500: step 9910, loss = 0.30, batch loss = 0.26 (35.7 examples/sec; 0.224 sec/batch; 20h:03m:57s remains)
INFO - root - 2017-12-15 06:40:34.122340: step 9920, loss = 0.30, batch loss = 0.27 (35.1 examples/sec; 0.228 sec/batch; 20h:24m:32s remains)
INFO - root - 2017-12-15 06:40:36.410010: step 9930, loss = 0.34, batch loss = 0.30 (35.2 examples/sec; 0.227 sec/batch; 20h:21m:56s remains)
INFO - root - 2017-12-15 06:40:38.723413: step 9940, loss = 0.23, batch loss = 0.19 (33.7 examples/sec; 0.237 sec/batch; 21h:16m:22s remains)
INFO - root - 2017-12-15 06:40:41.039662: step 9950, loss = 0.24, batch loss = 0.20 (34.7 examples/sec; 0.231 sec/batch; 20h:39m:15s remains)
INFO - root - 2017-12-15 06:40:43.358330: step 9960, loss = 0.43, batch loss = 0.39 (35.3 examples/sec; 0.227 sec/batch; 20h:18m:28s remains)
INFO - root - 2017-12-15 06:40:45.616337: step 9970, loss = 0.33, batch loss = 0.30 (35.7 examples/sec; 0.224 sec/batch; 20h:04m:30s remains)
INFO - root - 2017-12-15 06:40:47.893641: step 9980, loss = 0.21, batch loss = 0.18 (36.0 examples/sec; 0.222 sec/batch; 19h:54m:48s remains)
INFO - root - 2017-12-15 06:40:50.189643: step 9990, loss = 0.30, batch loss = 0.27 (35.9 examples/sec; 0.223 sec/batch; 19h:57m:09s remains)
INFO - root - 2017-12-15 06:40:52.512763: step 10000, loss = 0.37, batch loss = 0.34 (34.6 examples/sec; 0.231 sec/batch; 20h:41m:41s remains)
2017-12-15 06:40:52.849984: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.623225 -4.1198616 -4.4901648 -5.1277785 -5.9084492 -6.3820238 -6.6106167 -6.7866745 -6.9716873 -6.5640926 -5.8710709 -5.5502877 -5.1950274 -4.8201537 -4.8051567][-4.1854076 -5.1651568 -5.6379251 -6.3102684 -7.0400515 -7.4004116 -7.3864803 -7.3021851 -7.342968 -6.9711657 -6.3960056 -6.0533948 -5.6645594 -5.4079523 -5.4597492][-5.9836407 -6.2918053 -6.7961636 -7.3383808 -7.7830172 -7.8360739 -7.4564486 -7.0658779 -6.9594641 -6.8440838 -6.7804308 -6.6645346 -6.2803307 -5.9883642 -5.9024138][-7.2196531 -6.8982449 -7.1590843 -7.2854919 -7.1734676 -6.6029329 -5.6327057 -4.9344578 -4.8984575 -5.4147358 -6.2729807 -6.826232 -6.84455 -6.7302647 -6.6106138][-8.061718 -6.7548504 -6.4672766 -5.9958758 -5.1173425 -3.6097832 -1.7844453 -0.58594513 -0.62920177 -1.8949856 -3.7535782 -5.2198782 -6.0719419 -6.587142 -6.905551][-7.7517309 -5.89876 -5.0360789 -3.9657633 -2.3232327 0.04115963 2.5834529 4.1975212 4.024229 2.1329186 -0.42679656 -2.5209885 -3.9947982 -5.0146646 -5.9217167][-6.8974552 -5.0261612 -3.7872987 -2.1952159 0.042571783 2.9732554 5.92564 7.6951876 7.2538834 4.8612289 2.1682537 0.0790205 -1.5016432 -2.6898763 -4.081851][-6.0183496 -4.37801 -2.9210646 -0.85099196 1.843019 5.0289822 8.0120935 9.76495 9.1304026 6.417264 3.8073709 1.9769003 0.59034729 -0.48539317 -2.1469669][-5.2455196 -4.2255397 -2.9992223 -1.0216527 1.5092726 4.3653297 6.9528904 8.5263462 7.9532022 5.5478115 3.6259911 2.4192226 1.4472296 0.53484225 -1.168473][-4.6758327 -4.4467387 -3.8023911 -2.4005737 -0.49256492 1.6582263 3.6518219 4.9259157 4.4709215 2.7361653 1.9125125 1.6090906 1.1351051 0.40465927 -1.2380179][-4.2483211 -4.7199316 -4.9528651 -4.5367289 -3.4829793 -2.0090356 -0.57101882 0.35297775 -0.041852236 -1.2112558 -1.2256546 -0.98943853 -1.2128563 -1.5732174 -2.6317225][-4.4185104 -5.4142466 -6.452282 -6.9177237 -6.5964279 -5.6217008 -4.5662785 -4.0060444 -4.4721689 -5.2455339 -4.548419 -3.8075495 -3.9218664 -4.1025009 -4.5988493][-4.5397034 -5.83867 -7.3995476 -8.1808767 -8.0404415 -7.2215538 -6.3815231 -6.1700773 -6.8699732 -7.5389605 -6.5518456 -5.552949 -5.7608194 -6.071558 -6.4969549][-4.0409966 -5.335516 -7.3041372 -8.1410637 -7.7427177 -6.683259 -5.9210958 -5.996664 -6.9612656 -7.8221741 -7.070817 -6.2155361 -6.7490768 -7.2928391 -7.7384939][-3.3628211 -4.4204044 -6.5522604 -7.3157654 -6.6409836 -5.3845882 -4.7245989 -4.8897462 -5.7986917 -6.7261992 -6.272089 -5.5283203 -6.1438851 -6.8774662 -7.5486088]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 06:40:55.719306: step 10010, loss = 0.23, batch loss = 0.20 (35.2 examples/sec; 0.227 sec/batch; 20h:22m:01s remains)
INFO - root - 2017-12-15 06:40:58.016767: step 10020, loss = 0.25, batch loss = 0.22 (35.4 examples/sec; 0.226 sec/batch; 20h:13m:58s remains)
INFO - root - 2017-12-15 06:41:00.285301: step 10030, loss = 0.33, batch loss = 0.30 (34.6 examples/sec; 0.231 sec/batch; 20h:42m:23s remains)
INFO - root - 2017-12-15 06:41:02.590353: step 10040, loss = 0.46, batch loss = 0.43 (34.2 examples/sec; 0.234 sec/batch; 20h:56m:28s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 06:41:04.855816: step 10050, loss = 0.39, batch loss = 0.35 (35.1 examples/sec; 0.228 sec/batch; 20h:23m:40s remains)
INFO - root - 2017-12-15 06:41:07.160712: step 10060, loss = 0.22, batch loss = 0.19 (34.9 examples/sec; 0.230 sec/batch; 20h:33m:29s remains)
INFO - root - 2017-12-15 06:41:09.432252: step 10070, loss = 0.26, batch loss = 0.23 (35.6 examples/sec; 0.224 sec/batch; 20h:06m:18s remains)
INFO - root - 2017-12-15 06:41:11.733930: step 10080, loss = 0.34, batch loss = 0.30 (33.9 examples/sec; 0.236 sec/batch; 21h:09m:15s remains)
INFO - root - 2017-12-15 06:41:14.060269: step 10090, loss = 0.24, batch loss = 0.20 (36.7 examples/sec; 0.218 sec/batch; 19h:31m:57s remains)
INFO - root - 2017-12-15 06:41:16.331350: step 10100, loss = 0.30, batch loss = 0.26 (35.3 examples/sec; 0.226 sec/batch; 20h:16m:17s remains)
2017-12-15 06:41:16.660291: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2804365 -5.1788116 -4.4415607 -4.4981141 -5.2005973 -6.2935953 -7.1239862 -7.1035652 -7.4185724 -7.1228027 -5.2516346 -3.7798247 -3.5874181 -3.4376056 -3.9390345][-5.9178543 -5.2512894 -4.8289046 -4.9386768 -5.5114384 -6.2117462 -6.6984415 -6.7185783 -7.2158871 -7.0374956 -5.3410816 -3.9963834 -3.6752768 -3.3551381 -3.6117697][-6.3543921 -5.5299797 -5.5807929 -5.7667465 -6.1071777 -5.9888611 -5.7066965 -5.5693741 -6.1718273 -6.3148413 -5.180171 -4.2685108 -3.9189038 -3.5083861 -3.6421189][-6.6368675 -5.5704193 -5.811986 -5.6421733 -5.4257245 -4.4137182 -3.3579507 -3.0418084 -3.8557911 -4.7203383 -4.4658318 -4.0907125 -3.7920475 -3.2746854 -3.2911754][-6.7388659 -5.4314461 -5.3742051 -4.4563866 -3.4262486 -1.4601977 0.2657938 0.59370875 -0.73874521 -2.6863229 -3.6113563 -3.9960546 -3.9268088 -3.4600518 -3.5100889][-6.8668833 -5.1894755 -4.474771 -2.5108843 -0.57711673 2.0343912 4.0856886 4.2654018 2.3026445 -0.74211931 -2.7849636 -3.8429966 -4.0987854 -3.9065764 -4.0225353][-6.0400915 -4.4938631 -3.2197671 -0.46450973 2.1140115 5.0768108 7.0855532 6.9037113 4.3954172 0.68855405 -2.0147998 -3.4043231 -3.8717356 -3.9743371 -4.1923866][-5.3986492 -4.0021915 -2.5994976 0.41067076 3.2601573 6.24992 8.0516691 7.596137 4.8326855 0.985034 -1.9788616 -3.4426727 -3.9887352 -4.3395391 -4.6583624][-5.03393 -4.1201191 -3.0980673 -0.45809805 2.1816342 4.9400492 6.5489941 6.0659666 3.5778615 0.21911168 -2.5288844 -3.9516182 -4.7238379 -5.4309063 -5.9308357][-5.3641586 -4.9990144 -4.608489 -2.7603991 -0.77762294 1.438519 2.7807372 2.4991109 0.76430655 -1.5927627 -3.5296116 -4.4677711 -5.2806587 -6.2741642 -7.0424976][-6.0844989 -6.25863 -6.4658089 -5.3704267 -4.0438223 -2.4633169 -1.4976988 -1.5872974 -2.4867249 -3.7549973 -4.7412233 -5.1904373 -5.9901385 -7.0996056 -7.9600229][-6.8094587 -7.4378862 -8.1057072 -7.5878248 -6.6881104 -5.5189142 -4.8061724 -4.615365 -4.7421288 -5.0329552 -5.3102283 -5.6020403 -6.5619078 -7.7619734 -8.5063829][-7.0527878 -8.0482321 -9.0416718 -8.8346672 -8.2233648 -7.334136 -6.6384945 -6.0671778 -5.4757643 -5.07247 -4.9798989 -5.3928032 -6.6038265 -7.7335405 -7.9707203][-6.9728012 -8.0676546 -9.2847471 -9.2902107 -8.7634563 -7.9353 -7.0652132 -6.068759 -4.9387932 -4.0781145 -3.8769538 -4.51525 -5.9276752 -6.89559 -6.6078539][-6.1994963 -7.1565962 -8.5042925 -8.6122179 -8.294796 -7.6000223 -6.6091013 -5.3051672 -3.7037783 -2.5209904 -2.3472135 -3.276094 -4.7990303 -5.4813833 -4.6842194]]...]
INFO - root - 2017-12-15 06:41:18.973258: step 10110, loss = 0.26, batch loss = 0.22 (34.8 examples/sec; 0.230 sec/batch; 20h:35m:47s remains)
INFO - root - 2017-12-15 06:41:21.266756: step 10120, loss = 0.29, batch loss = 0.25 (32.9 examples/sec; 0.243 sec/batch; 21h:47m:53s remains)
INFO - root - 2017-12-15 06:41:23.555534: step 10130, loss = 0.22, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 19h:56m:29s remains)
INFO - root - 2017-12-15 06:41:25.882843: step 10140, loss = 0.25, batch loss = 0.21 (34.8 examples/sec; 0.230 sec/batch; 20h:33m:23s remains)
INFO - root - 2017-12-15 06:41:28.170437: step 10150, loss = 0.19, batch loss = 0.15 (35.8 examples/sec; 0.224 sec/batch; 20h:01m:19s remains)
INFO - root - 2017-12-15 06:41:30.442076: step 10160, loss = 0.22, batch loss = 0.19 (35.8 examples/sec; 0.224 sec/batch; 20h:01m:15s remains)
INFO - root - 2017-12-15 06:41:32.707297: step 10170, loss = 0.25, batch loss = 0.21 (34.7 examples/sec; 0.230 sec/batch; 20h:38m:02s remains)
INFO - root - 2017-12-15 06:41:34.987893: step 10180, loss = 0.31, batch loss = 0.28 (35.1 examples/sec; 0.228 sec/batch; 20h:23m:23s remains)
INFO - root - 2017-12-15 06:41:37.283630: step 10190, loss = 0.23, batch loss = 0.20 (35.1 examples/sec; 0.228 sec/batch; 20h:23m:35s remains)
INFO - root - 2017-12-15 06:41:39.543611: step 10200, loss = 0.24, batch loss = 0.20 (35.4 examples/sec; 0.226 sec/batch; 20h:14m:37s remains)
2017-12-15 06:41:39.922942: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.72706 -6.6776676 -6.927228 -6.7136106 -6.4125528 -5.8814278 -5.1305294 -4.6165686 -4.6935105 -5.5250378 -6.5359735 -6.9061022 -6.7838726 -6.5760889 -6.0024614][-5.5275383 -6.6930504 -6.8748178 -6.4925346 -5.9603844 -5.4326382 -4.8222017 -4.335001 -4.5365295 -5.5442739 -6.6540117 -6.995738 -6.9635944 -7.1263695 -6.7709827][-6.0710278 -6.1137347 -6.0428724 -5.3544617 -4.48271 -3.897521 -3.4305274 -2.9772387 -3.3065548 -4.5707922 -5.8926487 -6.2784128 -6.3770285 -6.8974047 -6.7697239][-5.8225522 -4.8579559 -4.3211517 -3.2749548 -2.1203611 -1.5410504 -1.2779028 -0.93216825 -1.5305421 -3.1676314 -4.6724954 -5.1205196 -5.3985214 -6.27065 -6.4236841][-4.9732771 -3.0630312 -2.0447154 -0.75634193 0.52713513 1.096168 1.3201292 1.6739571 0.8914454 -0.99718976 -2.5624371 -3.125082 -3.8096404 -5.1351824 -5.6290574][-4.1964121 -1.486248 -0.052732706 1.4501736 2.8269427 3.4810407 3.8858411 4.5300722 3.7962964 1.7741606 0.13346148 -0.75657189 -2.0619013 -3.9090991 -4.831461][-3.2560935 -0.46161413 1.3216724 2.967293 4.3137932 4.9631224 5.4748583 6.2846432 5.5676365 3.4734375 1.7090743 0.54963803 -1.2787223 -3.4495864 -4.5763707][-3.0406141 -0.31821585 1.4399281 2.9957745 4.1847992 4.7556229 5.1691246 5.9633017 5.3440161 3.4199622 1.7271729 0.47378969 -1.5531007 -3.6790729 -4.8023081][-3.9403477 -1.760406 -0.34470069 0.95562363 1.9568913 2.5876162 2.9706481 3.7635863 3.4934571 2.0426943 0.61708784 -0.45614636 -2.3686509 -4.2264776 -5.154726][-5.0619736 -3.4990225 -2.4795985 -1.5065613 -0.794039 -0.10800767 0.22916913 0.92559052 0.99880576 -0.060391903 -1.3802812 -2.3216286 -3.9468408 -5.2634535 -5.7533436][-5.4836149 -4.3509159 -3.6774273 -3.111918 -2.8145056 -2.1785059 -1.8640325 -1.3295002 -1.1715872 -2.0438123 -3.3767183 -4.2986455 -5.646049 -6.4463329 -6.3638687][-5.6294785 -4.8366418 -4.4901447 -4.2697573 -4.3052092 -3.7237811 -3.3548751 -2.8934729 -2.7056825 -3.4674714 -4.7320251 -5.6627378 -6.7945547 -7.241991 -6.7920752][-5.656333 -5.1579056 -5.0880017 -5.1025739 -5.3282824 -4.8722439 -4.4219961 -4.0550423 -4.0231757 -4.765481 -5.88297 -6.7352715 -7.7021818 -7.9116545 -7.1790113][-5.4613385 -5.1182532 -5.1930294 -5.3149128 -5.625731 -5.3428397 -4.9381323 -4.7165236 -4.9364204 -5.7274132 -6.7373772 -7.5347366 -8.2195549 -8.1283274 -7.195591][-5.835124 -5.5128326 -5.6380863 -5.7613049 -6.0424871 -5.8181095 -5.3852448 -5.2678566 -5.6492748 -6.3728542 -7.1386003 -7.7310247 -8.0910645 -7.738163 -6.7799807]]...]
INFO - root - 2017-12-15 06:41:42.236576: step 10210, loss = 0.25, batch loss = 0.21 (34.8 examples/sec; 0.230 sec/batch; 20h:33m:22s remains)
INFO - root - 2017-12-15 06:41:44.532584: step 10220, loss = 0.27, batch loss = 0.23 (34.8 examples/sec; 0.230 sec/batch; 20h:36m:17s remains)
INFO - root - 2017-12-15 06:41:46.798654: step 10230, loss = 0.30, batch loss = 0.26 (36.2 examples/sec; 0.221 sec/batch; 19h:48m:08s remains)
INFO - root - 2017-12-15 06:41:49.060283: step 10240, loss = 0.27, batch loss = 0.24 (36.7 examples/sec; 0.218 sec/batch; 19h:31m:18s remains)
INFO - root - 2017-12-15 06:41:51.369802: step 10250, loss = 0.22, batch loss = 0.18 (34.2 examples/sec; 0.234 sec/batch; 20h:56m:23s remains)
INFO - root - 2017-12-15 06:41:53.629033: step 10260, loss = 0.34, batch loss = 0.31 (36.3 examples/sec; 0.221 sec/batch; 19h:44m:48s remains)
INFO - root - 2017-12-15 06:41:55.920057: step 10270, loss = 0.26, batch loss = 0.22 (36.0 examples/sec; 0.222 sec/batch; 19h:53m:47s remains)
INFO - root - 2017-12-15 06:41:58.168537: step 10280, loss = 0.22, batch loss = 0.18 (35.2 examples/sec; 0.227 sec/batch; 20h:20m:59s remains)
INFO - root - 2017-12-15 06:42:00.482432: step 10290, loss = 0.22, batch loss = 0.18 (36.2 examples/sec; 0.221 sec/batch; 19h:47m:57s remains)
INFO - root - 2017-12-15 06:42:02.751280: step 10300, loss = 0.19, batch loss = 0.16 (35.3 examples/sec; 0.226 sec/batch; 20h:16m:04s remains)
2017-12-15 06:42:03.176445: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.60981178 -0.017175436 -0.17397928 -0.43603635 -0.97722971 -1.3709533 -1.3570592 -1.0038972 -0.69483685 -0.78342235 -1.0794846 -1.2252378 -0.87055779 -0.49622059 -0.55758333][-1.4730501 -1.602156 -1.7357578 -2.0007546 -2.4117944 -2.7921968 -2.9486766 -2.9433806 -3.0719995 -3.3561502 -3.740448 -3.92599 -3.501091 -2.9133797 -2.626122][-3.2409258 -2.9321339 -3.1294394 -3.4224844 -3.5784249 -3.6166356 -3.6261902 -3.7642083 -4.3680353 -5.0379462 -5.6437626 -6.0733309 -5.6840496 -4.9443092 -4.2816324][-4.6363482 -4.1313744 -4.6700606 -5.1383915 -4.9114494 -4.327899 -3.8583932 -3.8771203 -4.6625738 -5.5035772 -6.3528848 -7.1987758 -7.0780292 -6.3362827 -5.3571935][-4.6630945 -4.4003544 -5.3665195 -6.0516667 -5.6223288 -4.5205631 -3.4585531 -3.1407979 -3.9389267 -4.9545455 -6.0178576 -7.099277 -7.1825385 -6.5662413 -5.6121349][-3.8535929 -3.8964748 -5.087872 -5.6761694 -4.8373566 -3.2274282 -1.7087046 -1.1571376 -2.0403256 -3.4360452 -4.8696909 -6.1687989 -6.3841848 -5.953948 -5.1610279][-2.4371939 -2.9577041 -4.1185212 -4.2957468 -2.9351702 -0.84136808 1.2268291 1.9833965 0.799953 -1.2585642 -3.2888479 -4.8767519 -5.1634965 -4.8687124 -4.331234][-1.1950747 -1.9442639 -3.0717235 -2.9562302 -1.2320144 1.3112435 3.7943707 4.8164248 3.6183805 1.1696336 -1.1904591 -2.9394126 -3.3763437 -3.3508677 -3.1698909][-0.47041225 -1.4548676 -2.6545477 -2.5468383 -0.85057569 1.7664368 4.4613233 5.6872363 4.5763583 2.2184324 -0.011380434 -1.6319962 -2.1166136 -2.3648283 -2.569226][-0.67139673 -1.788756 -3.0101941 -2.9676197 -1.5355387 0.83586121 3.1797805 4.219502 3.374752 1.4165869 -0.34124935 -1.6289464 -2.1871207 -2.64787 -3.0192862][-1.681987 -2.6548979 -3.7006483 -3.7072794 -2.6019692 -0.74412274 1.0748441 1.9183855 1.3470843 -0.04370904 -1.2732251 -2.2287257 -2.8079722 -3.4249029 -3.8517458][-2.9112234 -3.5617361 -4.2825184 -4.184968 -3.2200518 -1.6651988 -0.21964693 0.44358659 0.10677648 -0.89985478 -1.8016208 -2.5767393 -3.314158 -4.0537114 -4.443511][-3.6152048 -3.8625979 -4.1758928 -3.9045897 -2.9283853 -1.5771093 -0.39619732 0.1596849 -0.14539194 -1.0429758 -1.9203284 -2.7210803 -3.5250952 -4.3588095 -4.71196][-3.378787 -3.3068366 -3.4648213 -3.2257824 -2.47314 -1.4206424 -0.49945402 -0.14053464 -0.46227622 -1.282192 -2.0760946 -2.8221574 -3.5990241 -4.3911047 -4.6927996][-2.6955094 -2.4599137 -2.6896944 -2.740278 -2.4232366 -1.8352631 -1.2341609 -0.99776983 -1.2141048 -1.8153957 -2.4605143 -3.1396537 -3.8164058 -4.4541154 -4.6887007]]...]
INFO - root - 2017-12-15 06:42:05.478844: step 10310, loss = 0.29, batch loss = 0.25 (34.6 examples/sec; 0.231 sec/batch; 20h:41m:01s remains)
INFO - root - 2017-12-15 06:42:07.769909: step 10320, loss = 0.28, batch loss = 0.24 (33.9 examples/sec; 0.236 sec/batch; 21h:08m:32s remains)
INFO - root - 2017-12-15 06:42:10.084769: step 10330, loss = 0.30, batch loss = 0.27 (34.7 examples/sec; 0.230 sec/batch; 20h:36m:24s remains)
INFO - root - 2017-12-15 06:42:12.374271: step 10340, loss = 0.32, batch loss = 0.28 (36.8 examples/sec; 0.217 sec/batch; 19h:27m:07s remains)
INFO - root - 2017-12-15 06:42:14.656537: step 10350, loss = 0.33, batch loss = 0.29 (35.2 examples/sec; 0.227 sec/batch; 20h:19m:11s remains)
INFO - root - 2017-12-15 06:42:16.910924: step 10360, loss = 0.24, batch loss = 0.21 (36.3 examples/sec; 0.220 sec/batch; 19h:42m:18s remains)
INFO - root - 2017-12-15 06:42:19.178375: step 10370, loss = 0.22, batch loss = 0.19 (35.4 examples/sec; 0.226 sec/batch; 20h:14m:44s remains)
INFO - root - 2017-12-15 06:42:21.410687: step 10380, loss = 0.21, batch loss = 0.18 (37.0 examples/sec; 0.216 sec/batch; 19h:20m:36s remains)
INFO - root - 2017-12-15 06:42:23.661725: step 10390, loss = 0.25, batch loss = 0.21 (35.8 examples/sec; 0.224 sec/batch; 20h:00m:24s remains)
INFO - root - 2017-12-15 06:42:25.897549: step 10400, loss = 0.24, batch loss = 0.21 (36.6 examples/sec; 0.218 sec/batch; 19h:32m:22s remains)
2017-12-15 06:42:26.178344: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9814878 -4.1890717 -4.167417 -3.8612092 -3.5199106 -3.1911807 -2.8788338 -2.8693547 -3.2846417 -3.7818742 -4.0637169 -4.1957235 -4.1487122 -3.7314072 -3.4297261][-4.5066452 -4.9901791 -4.8416696 -4.4049821 -3.9067609 -3.4336152 -3.0563855 -3.1457844 -3.6776867 -4.2966828 -4.7080388 -4.9137726 -4.7978554 -4.3128104 -3.9519429][-5.700448 -5.4001818 -5.1232548 -4.5264273 -3.8347654 -3.1547832 -2.7047749 -2.8816926 -3.4885645 -4.1928177 -4.7593265 -5.1049175 -4.9965668 -4.5173659 -4.1665792][-5.8397293 -5.2040272 -4.8672576 -4.1815438 -3.4300389 -2.7377861 -2.3718538 -2.6592259 -3.3771148 -4.1661205 -4.8079834 -5.2659717 -5.1502504 -4.6646523 -4.3045788][-5.4295673 -4.1979108 -3.8638089 -3.1994965 -2.5177774 -1.8791115 -1.4970634 -1.6833106 -2.3538053 -3.2706671 -4.0680079 -4.800292 -4.9479113 -4.6381288 -4.3200588][-4.4084883 -2.718622 -2.4069471 -1.7973688 -1.1692145 -0.50200975 -0.0012154579 0.066539764 -0.46021581 -1.6971967 -2.8989296 -4.1441979 -4.7286296 -4.6644983 -4.29579][-2.3402097 -1.0578895 -0.72297311 -0.12812209 0.69097447 1.6194017 2.158376 2.4045975 1.8497031 -0.009437561 -1.8393571 -3.6655526 -4.6240644 -4.8305211 -4.3939152][-1.5963299 0.014503956 0.35264397 0.9470582 2.1170847 3.3855031 3.9457581 4.2325249 3.5882175 1.1243644 -1.2445984 -3.3750548 -4.5672832 -5.0241919 -4.5976257][-1.986269 -0.45756936 -0.25930393 0.26751041 1.7909675 3.4103124 4.0386009 4.3823862 3.8776042 1.2235138 -1.429158 -3.586071 -4.7881193 -5.3052082 -4.8084097][-3.3525386 -2.0134003 -1.9684473 -1.5784035 0.12521148 1.9164531 2.5198991 2.7936165 2.2675283 -0.33599675 -2.9322369 -4.6872029 -5.4493408 -5.6412525 -4.9424906][-4.9322877 -3.7719593 -3.8007739 -3.5237486 -1.9451122 -0.22831154 0.34486389 0.45313573 -0.060031414 -2.2016592 -4.3625135 -5.5948095 -5.930594 -5.84451 -5.0571938][-6.1643286 -5.3057761 -5.4664192 -5.3706665 -4.1820459 -2.8020744 -2.352869 -2.3376863 -2.7246895 -4.1390209 -5.5541029 -6.2020063 -6.2644262 -5.9145479 -5.1072378][-6.5513163 -5.8840904 -6.0533562 -6.072382 -5.3666143 -4.45708 -4.15554 -4.1852226 -4.5138168 -5.3108149 -6.07969 -6.3512793 -6.2234516 -5.7710667 -4.9905682][-6.3968782 -5.8237143 -6.0012188 -6.1200647 -5.90274 -5.4880648 -5.321259 -5.31573 -5.4471135 -5.6906567 -5.8678951 -5.9549627 -5.7663946 -5.3219085 -4.7505045][-5.6861572 -5.109386 -5.2710466 -5.4256911 -5.4764686 -5.3896084 -5.3482881 -5.382215 -5.4416351 -5.5148864 -5.5828056 -5.6318674 -5.5607386 -5.1703911 -4.68913]]...]
INFO - root - 2017-12-15 06:42:28.418943: step 10410, loss = 0.21, batch loss = 0.18 (35.7 examples/sec; 0.224 sec/batch; 20h:04m:16s remains)
INFO - root - 2017-12-15 06:42:30.643898: step 10420, loss = 0.29, batch loss = 0.26 (36.1 examples/sec; 0.222 sec/batch; 19h:49m:40s remains)
INFO - root - 2017-12-15 06:42:32.873293: step 10430, loss = 0.40, batch loss = 0.36 (35.6 examples/sec; 0.225 sec/batch; 20h:07m:28s remains)
INFO - root - 2017-12-15 06:42:35.107197: step 10440, loss = 0.39, batch loss = 0.35 (34.4 examples/sec; 0.233 sec/batch; 20h:48m:48s remains)
INFO - root - 2017-12-15 06:42:37.368124: step 10450, loss = 0.30, batch loss = 0.27 (35.0 examples/sec; 0.229 sec/batch; 20h:27m:39s remains)
INFO - root - 2017-12-15 06:42:39.666596: step 10460, loss = 0.25, batch loss = 0.21 (35.2 examples/sec; 0.227 sec/batch; 20h:19m:46s remains)
INFO - root - 2017-12-15 06:42:41.914836: step 10470, loss = 0.22, batch loss = 0.19 (36.1 examples/sec; 0.222 sec/batch; 19h:49m:39s remains)
INFO - root - 2017-12-15 06:42:44.143870: step 10480, loss = 0.24, batch loss = 0.20 (35.9 examples/sec; 0.223 sec/batch; 19h:55m:43s remains)
INFO - root - 2017-12-15 06:42:46.373591: step 10490, loss = 0.30, batch loss = 0.26 (34.8 examples/sec; 0.230 sec/batch; 20h:33m:28s remains)
INFO - root - 2017-12-15 06:42:48.607801: step 10500, loss = 0.23, batch loss = 0.20 (35.0 examples/sec; 0.229 sec/batch; 20h:26m:22s remains)
2017-12-15 06:42:48.871619: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1919346 -6.6649327 -6.5234513 -6.2809825 -6.0153117 -5.498538 -4.9529123 -4.884213 -5.2101312 -5.4636459 -5.8516555 -6.1467352 -6.0572233 -5.5381203 -5.0969105][-6.5327096 -7.7776523 -7.7412214 -7.5631523 -7.3170428 -6.7618914 -6.1556082 -5.970664 -6.1548572 -6.31953 -6.6505327 -7.0004888 -7.0777168 -6.6378222 -6.1971536][-7.346117 -8.4678288 -8.62018 -8.5432091 -8.4103689 -7.91862 -7.2069049 -6.7685308 -6.6440711 -6.6283479 -6.8881249 -7.2567158 -7.3930397 -7.0124578 -6.4936709][-7.6709862 -8.4501238 -8.778862 -8.8271208 -8.7905235 -8.3722248 -7.5654783 -6.8130245 -6.2544312 -6.0072966 -6.348444 -6.9462428 -7.401701 -7.2518296 -6.7600074][-7.3895445 -7.3373179 -7.5821805 -7.5119243 -7.4886551 -7.2359929 -6.4920778 -5.5576563 -4.6323218 -4.2843771 -4.8529458 -5.8582363 -6.7604275 -6.9508457 -6.570538][-6.304605 -5.7471 -5.8186979 -5.4692669 -5.2855539 -5.1948137 -4.4641891 -3.2784634 -2.1843538 -1.9633524 -2.8816109 -4.3188791 -5.6206713 -6.18034 -5.9974113][-4.064878 -3.3142152 -3.2926617 -2.9120131 -2.6916718 -2.6894395 -1.9028859 -0.55190158 0.5587182 0.60162568 -0.69491494 -2.6364908 -4.450593 -5.4227891 -5.498971][-2.2299156 -1.0837713 -0.96793962 -0.68155479 -0.41462636 -0.35431337 0.42357922 1.7594709 2.9092207 2.8104177 1.3145783 -0.95136917 -3.0908499 -4.243763 -4.384511][-1.558285 -0.16156077 0.053775072 0.31605625 0.7160852 0.9023459 1.6434827 2.9136434 3.8980279 3.5234871 1.8801379 -0.3930831 -2.4894884 -3.5943956 -3.6627102][-1.843635 -0.3445648 0.07754612 0.37127209 0.84722543 1.0924804 1.653513 2.689383 3.2851968 2.675427 1.1452725 -0.81024313 -2.5917916 -3.3561494 -3.0672674][-3.1083949 -1.6789694 -1.0850214 -0.69313073 -0.25248623 -0.036562443 0.22808957 0.96219635 1.4204369 0.80241513 -0.46509898 -2.0330167 -3.4109812 -3.6108522 -2.7578623][-4.8165617 -3.4044251 -2.6594574 -2.1148627 -1.7847142 -1.8543615 -1.984982 -1.4151725 -0.94154871 -1.3906496 -2.3489213 -3.453588 -4.4159665 -4.1984611 -3.1062522][-6.3860397 -4.9886208 -4.1038451 -3.4906693 -3.3481231 -3.7224083 -4.0601726 -3.51342 -3.065454 -3.2184002 -3.6738782 -4.2931919 -4.8702393 -4.5090308 -3.5244393][-7.5714221 -6.1984715 -5.2656727 -4.6570616 -4.6246438 -5.0734234 -5.480916 -5.0442133 -4.6347995 -4.5413976 -4.6364818 -5.0119462 -5.4964066 -5.2757959 -4.5405903][-7.6052976 -6.2025824 -5.3934894 -4.9121904 -5.0064287 -5.4336138 -5.8543344 -5.5880318 -5.2320576 -5.0028048 -4.9629974 -5.1690464 -5.618227 -5.7093592 -5.4337034]]...]
INFO - root - 2017-12-15 06:42:51.113597: step 10510, loss = 0.29, batch loss = 0.25 (35.3 examples/sec; 0.227 sec/batch; 20h:17m:06s remains)
INFO - root - 2017-12-15 06:42:53.361229: step 10520, loss = 0.19, batch loss = 0.16 (35.1 examples/sec; 0.228 sec/batch; 20h:23m:03s remains)
INFO - root - 2017-12-15 06:42:55.621460: step 10530, loss = 0.21, batch loss = 0.18 (36.4 examples/sec; 0.220 sec/batch; 19h:38m:32s remains)
INFO - root - 2017-12-15 06:42:57.862374: step 10540, loss = 0.30, batch loss = 0.26 (35.2 examples/sec; 0.227 sec/batch; 20h:18m:44s remains)
INFO - root - 2017-12-15 06:43:00.135026: step 10550, loss = 0.26, batch loss = 0.23 (34.2 examples/sec; 0.234 sec/batch; 20h:54m:15s remains)
INFO - root - 2017-12-15 06:43:02.408509: step 10560, loss = 0.33, batch loss = 0.29 (36.1 examples/sec; 0.221 sec/batch; 19h:48m:06s remains)
INFO - root - 2017-12-15 06:43:04.651956: step 10570, loss = 0.24, batch loss = 0.20 (36.2 examples/sec; 0.221 sec/batch; 19h:46m:43s remains)
INFO - root - 2017-12-15 06:43:06.913380: step 10580, loss = 0.26, batch loss = 0.22 (34.4 examples/sec; 0.233 sec/batch; 20h:47m:59s remains)
INFO - root - 2017-12-15 06:43:09.161560: step 10590, loss = 0.18, batch loss = 0.15 (34.1 examples/sec; 0.235 sec/batch; 20h:59m:18s remains)
INFO - root - 2017-12-15 06:43:11.417856: step 10600, loss = 0.24, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 20h:01m:11s remains)
2017-12-15 06:43:11.683305: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4653904 -1.5109863 -0.069951057 1.744978 2.0214427 0.91220045 -0.015576363 -1.4196625 -2.6985326 -2.9732747 -2.7077322 -2.4482665 -2.1530221 -2.3586128 -3.3845463][-3.0284727 -1.7997317 -0.52346647 0.98790526 0.997622 -0.34230876 -1.2892065 -2.538167 -3.640995 -3.7946796 -3.3179545 -3.0790961 -3.1957521 -3.6271381 -4.678165][-4.6959858 -3.4174 -2.6448689 -1.7328286 -1.9270878 -3.07584 -3.747458 -4.7177439 -5.5497088 -5.3185453 -4.5722485 -4.3317466 -4.6611166 -5.0200682 -5.6181216][-5.9280481 -4.770247 -4.3555307 -3.8631682 -3.8716826 -4.3949604 -4.6933618 -5.5638056 -6.5669222 -6.6394262 -6.1454096 -5.9719725 -6.13124 -5.96005 -5.8451614][-6.2795525 -5.1945839 -4.8780737 -4.4815774 -4.1745334 -3.9932113 -3.9095886 -4.684464 -5.9089332 -6.5464029 -6.8101568 -6.9997683 -6.9341955 -6.1920185 -5.3290229][-5.9817276 -5.221509 -5.0241084 -4.749301 -4.4291945 -3.683274 -2.9428682 -3.3684049 -4.6022549 -5.7148514 -6.6428957 -6.98551 -6.53808 -5.4289846 -4.0427847][-4.9553061 -4.7217336 -4.6469135 -4.2429752 -3.422823 -1.7870201 -0.19313037 -0.330132 -2.0959849 -4.1791372 -5.9282532 -6.6642237 -6.2222977 -4.9710073 -3.1968446][-3.8659973 -3.8058221 -3.8020372 -3.0359192 -1.227658 1.5207889 3.7942607 3.5178239 0.8134954 -2.3072803 -4.8898659 -6.204566 -6.0960474 -4.9861083 -3.0110326][-2.9900606 -2.8452442 -2.8791256 -1.9697437 0.31470275 3.5447462 6.1414585 5.7593479 2.5712388 -0.99415994 -4.0993509 -5.9365315 -6.1962519 -5.3164039 -3.4514141][-2.736753 -2.3985045 -2.4040475 -1.4753128 0.79645848 3.7355511 6.2475014 6.0042667 2.8506758 -0.79159224 -4.1359863 -6.2499123 -6.7015095 -6.0074835 -4.4453621][-2.18741 -1.7717805 -1.8673975 -1.3738475 0.16875672 2.076349 3.7342317 3.2023766 0.19048643 -3.006644 -5.83364 -7.4763422 -7.5513105 -6.7584114 -5.3949776][-1.3979323 -0.73474956 -0.93786967 -1.2248135 -0.95504141 -0.49865127 0.0955081 -0.804489 -3.5076268 -5.8162756 -7.53008 -8.3339777 -7.9341249 -7.0496645 -5.8942537][-0.80743241 0.16726851 -0.17978871 -1.2048745 -1.9048269 -2.4782977 -2.7234039 -3.7258017 -5.71678 -6.9752283 -7.7225389 -8.1007137 -7.6132641 -6.7248478 -5.8000574][-0.41940022 0.71260619 0.038682938 -1.5369298 -2.7175105 -3.9323015 -4.7823124 -5.8692865 -7.289712 -7.8185682 -7.9682055 -7.993926 -7.2593555 -6.1658983 -5.2286329][-0.72035134 0.54134679 -0.07736063 -1.6246438 -2.8633342 -4.4375181 -5.765111 -6.9331493 -7.8497572 -7.9615059 -7.7296667 -7.407589 -6.5338106 -5.4382133 -4.6154051]]...]
INFO - root - 2017-12-15 06:43:13.945610: step 10610, loss = 0.21, batch loss = 0.18 (34.3 examples/sec; 0.234 sec/batch; 20h:53m:02s remains)
INFO - root - 2017-12-15 06:43:16.228096: step 10620, loss = 0.25, batch loss = 0.21 (35.3 examples/sec; 0.227 sec/batch; 20h:16m:19s remains)
INFO - root - 2017-12-15 06:43:18.487752: step 10630, loss = 0.36, batch loss = 0.33 (35.4 examples/sec; 0.226 sec/batch; 20h:14m:00s remains)
INFO - root - 2017-12-15 06:43:20.749336: step 10640, loss = 0.20, batch loss = 0.17 (34.1 examples/sec; 0.234 sec/batch; 20h:57m:36s remains)
INFO - root - 2017-12-15 06:43:23.010128: step 10650, loss = 0.31, batch loss = 0.27 (35.0 examples/sec; 0.229 sec/batch; 20h:26m:38s remains)
INFO - root - 2017-12-15 06:43:25.232095: step 10660, loss = 0.31, batch loss = 0.28 (36.6 examples/sec; 0.219 sec/batch; 19h:33m:44s remains)
INFO - root - 2017-12-15 06:43:27.478261: step 10670, loss = 0.21, batch loss = 0.17 (36.0 examples/sec; 0.222 sec/batch; 19h:52m:32s remains)
INFO - root - 2017-12-15 06:43:29.768799: step 10680, loss = 0.25, batch loss = 0.22 (33.9 examples/sec; 0.236 sec/batch; 21h:06m:34s remains)
INFO - root - 2017-12-15 06:43:32.028155: step 10690, loss = 0.31, batch loss = 0.27 (36.2 examples/sec; 0.221 sec/batch; 19h:45m:39s remains)
INFO - root - 2017-12-15 06:43:34.291400: step 10700, loss = 0.34, batch loss = 0.30 (35.2 examples/sec; 0.227 sec/batch; 20h:18m:41s remains)
2017-12-15 06:43:34.564332: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.64121842 -0.74704039 -1.6496 -2.0727553 -2.1074972 -2.2251859 -2.4926815 -2.7263751 -2.7243714 -2.4811394 -2.0915082 -1.6033502 -1.2063947 -0.96737289 -0.75297761][-0.7195785 -1.4380083 -2.3237872 -2.6663318 -2.7425046 -2.9215984 -3.1284323 -3.2110367 -3.1489511 -2.8908243 -2.4330235 -1.8724617 -1.4672952 -1.2054982 -0.9103][-1.9484801 -1.9402617 -2.6912158 -3.03836 -3.1524234 -3.2532215 -3.3040483 -3.2344418 -3.1299934 -2.8861718 -2.41629 -1.8587281 -1.5118055 -1.3009716 -1.0374811][-2.9161606 -2.4635639 -3.089663 -3.3192797 -3.3405819 -3.3056419 -3.1906075 -3.0221429 -2.8732114 -2.6531019 -2.2044959 -1.6922382 -1.4189041 -1.2928028 -1.1504064][-3.7637377 -2.9084606 -3.213192 -3.1853437 -3.0108011 -2.8429298 -2.6943166 -2.4850316 -2.3748617 -2.2616658 -1.90732 -1.5050294 -1.3186417 -1.2698302 -1.2572688][-3.9809103 -2.9353318 -2.8780174 -2.6024046 -2.3055952 -2.1282625 -1.9574826 -1.794747 -1.8398693 -1.8889878 -1.6747766 -1.4068909 -1.2979501 -1.2913733 -1.3519059][-3.3771238 -2.6115456 -2.3618896 -2.0007255 -1.6264682 -1.3911703 -1.2767098 -1.2635621 -1.4875745 -1.6841611 -1.5950034 -1.4323809 -1.3726616 -1.3752148 -1.46523][-3.248286 -2.3879347 -2.0822618 -1.6300542 -1.2367346 -1.0584172 -1.0078731 -1.0938286 -1.4263546 -1.6973386 -1.6665668 -1.5504872 -1.5366118 -1.5525017 -1.64131][-3.2589221 -2.3371739 -2.0166237 -1.6209393 -1.3160111 -1.1555713 -1.1146934 -1.2422062 -1.5982268 -1.85698 -1.8291079 -1.7356339 -1.7673012 -1.7995603 -1.8510511][-3.3347528 -2.4142919 -2.183372 -1.8823799 -1.6448457 -1.504096 -1.46577 -1.5954742 -1.8841026 -2.0478277 -1.9853944 -1.9056392 -1.9940296 -2.055655 -2.0732844][-3.494031 -2.6181383 -2.4688554 -2.2619667 -2.0920014 -1.9626055 -1.9122725 -1.9908798 -2.1450622 -2.1939442 -2.1178308 -2.0730848 -2.2224021 -2.323724 -2.3384812][-3.6687863 -2.8268161 -2.7453933 -2.6235821 -2.5106354 -2.403018 -2.3561447 -2.3840139 -2.4219205 -2.3846917 -2.3198681 -2.3201303 -2.5028348 -2.63036 -2.65812][-3.7568507 -2.9357524 -2.8993697 -2.8526258 -2.7998719 -2.727962 -2.7052488 -2.7127676 -2.6776047 -2.5938148 -2.5406318 -2.5632482 -2.7294929 -2.8454142 -2.8725708][-3.7086329 -2.9089494 -2.9118173 -2.9358075 -2.9402056 -2.90203 -2.9024491 -2.9057322 -2.8389606 -2.7447696 -2.7053833 -2.7276165 -2.8306451 -2.904058 -2.91722][-3.5134759 -2.7337229 -2.7638307 -2.8451555 -2.8981643 -2.8793483 -2.8922148 -2.8969398 -2.8249259 -2.737505 -2.7057304 -2.6991398 -2.7224302 -2.7445045 -2.7431488]]...]
INFO - root - 2017-12-15 06:43:36.801155: step 10710, loss = 0.22, batch loss = 0.19 (36.5 examples/sec; 0.219 sec/batch; 19h:34m:06s remains)
INFO - root - 2017-12-15 06:43:39.059630: step 10720, loss = 0.20, batch loss = 0.16 (34.5 examples/sec; 0.232 sec/batch; 20h:43m:47s remains)
INFO - root - 2017-12-15 06:43:41.360041: step 10730, loss = 0.25, batch loss = 0.21 (33.4 examples/sec; 0.239 sec/batch; 21h:23m:52s remains)
INFO - root - 2017-12-15 06:43:43.630049: step 10740, loss = 0.24, batch loss = 0.21 (34.7 examples/sec; 0.231 sec/batch; 20h:37m:38s remains)
INFO - root - 2017-12-15 06:43:45.855836: step 10750, loss = 0.27, batch loss = 0.24 (36.3 examples/sec; 0.220 sec/batch; 19h:42m:03s remains)
INFO - root - 2017-12-15 06:43:48.091975: step 10760, loss = 0.24, batch loss = 0.21 (36.2 examples/sec; 0.221 sec/batch; 19h:46m:26s remains)
INFO - root - 2017-12-15 06:43:50.356873: step 10770, loss = 0.17, batch loss = 0.14 (35.1 examples/sec; 0.228 sec/batch; 20h:20m:29s remains)
INFO - root - 2017-12-15 06:43:52.611501: step 10780, loss = 0.27, batch loss = 0.23 (36.5 examples/sec; 0.219 sec/batch; 19h:34m:25s remains)
INFO - root - 2017-12-15 06:43:54.857040: step 10790, loss = 0.25, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 20h:01m:10s remains)
INFO - root - 2017-12-15 06:43:57.119182: step 10800, loss = 0.26, batch loss = 0.22 (35.9 examples/sec; 0.223 sec/batch; 19h:56m:25s remains)
2017-12-15 06:43:57.406304: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0815501 -3.2373061 -3.8135242 -3.6261721 -2.8375323 -2.318897 -2.1001074 -1.9237646 -1.7035409 -0.72228682 0.53876615 0.80394292 0.63488817 0.0033729076 -1.1179897][-1.4597716 -3.1497943 -3.7502375 -3.7051339 -2.9434011 -2.3969786 -2.1731033 -1.949718 -1.5874906 -0.56044352 0.67102265 0.86093616 0.51908755 -0.31173134 -1.5811627][-1.6806335 -2.8053665 -3.316268 -3.2592769 -2.5024109 -2.0049942 -1.9466059 -1.9553107 -1.7018638 -0.63984394 0.5849514 0.99464536 0.74824429 -0.17717838 -1.5267317][-1.8150787 -2.3673582 -2.8552897 -2.7625306 -2.0900762 -1.7587175 -1.830375 -1.9352584 -1.8270879 -0.73408616 0.51136184 0.968879 0.68508887 -0.38557875 -1.7963989][-2.32348 -2.4182324 -2.7769535 -2.3295286 -1.4552395 -0.98232985 -0.91176844 -1.1092701 -1.358104 -0.48499513 0.56817842 0.91314173 0.55126691 -0.54765141 -2.0216105][-2.2604911 -2.0210831 -2.287169 -1.5366772 -0.48266816 0.11783314 0.4056561 0.2438736 -0.44356894 -0.0041387081 0.70909 0.87684083 0.46533561 -0.6250236 -2.0512428][-2.1792595 -1.9979402 -1.9640092 -0.80545664 0.48273873 1.2942369 1.9559743 1.9118731 0.954968 0.97696304 1.254343 1.0872116 0.53865147 -0.65146053 -2.229461][-2.5561798 -2.1970875 -1.7340672 -0.14135551 1.3720591 2.3625605 3.189039 3.1023147 2.0264671 1.6221561 1.3895423 0.97460127 0.42800736 -0.70394278 -2.2709286][-2.92888 -2.5695097 -1.8062699 -0.089560032 1.3432429 2.1694853 2.8801367 2.611973 1.5633771 1.0670342 0.77682638 0.45645666 0.20955372 -0.60653925 -2.0936618][-3.3460011 -3.1274633 -2.366792 -0.81249309 0.39025497 1.0363727 1.5810688 1.277714 0.45254159 0.11923838 0.00735116 0.0015602112 0.24502897 -0.14475298 -1.3679481][-3.6990094 -3.6195025 -2.9426734 -1.622005 -0.658381 -0.17605019 0.10263824 -0.37340891 -1.0586594 -1.2757301 -1.1119597 -0.73931324 -0.0742805 -0.0740087 -0.84827876][-3.7393491 -3.7400112 -3.2752028 -2.3433802 -1.6461132 -1.2755959 -1.2057204 -1.6842389 -2.1698084 -2.2023163 -1.7486608 -1.0978816 -0.29874623 -0.25440347 -0.83517253][-4.0990911 -4.0504704 -3.6740746 -2.8509662 -2.1115546 -1.718214 -1.6965376 -2.1314955 -2.5223374 -2.4717507 -1.8658584 -1.0023499 -0.18517697 -0.32135344 -0.94484162][-4.4990683 -4.2915473 -3.9806077 -3.3244591 -2.6775937 -2.2771857 -2.0919051 -2.2448967 -2.4196651 -2.3021393 -1.7684813 -0.94210982 -0.25706494 -0.57469177 -1.298418][-4.7867241 -4.5219626 -4.2932854 -3.7970827 -3.2203174 -2.6852493 -2.1445448 -1.8813196 -1.8008037 -1.7450906 -1.4960837 -0.96449757 -0.58777368 -1.094238 -1.8733416]]...]
INFO - root - 2017-12-15 06:43:59.686994: step 10810, loss = 0.22, batch loss = 0.18 (35.8 examples/sec; 0.223 sec/batch; 19h:56m:33s remains)
INFO - root - 2017-12-15 06:44:01.946194: step 10820, loss = 0.22, batch loss = 0.19 (33.5 examples/sec; 0.239 sec/batch; 21h:22m:01s remains)
INFO - root - 2017-12-15 06:44:04.252022: step 10830, loss = 0.21, batch loss = 0.18 (34.3 examples/sec; 0.233 sec/batch; 20h:50m:23s remains)
INFO - root - 2017-12-15 06:44:06.526829: step 10840, loss = 0.24, batch loss = 0.20 (34.7 examples/sec; 0.231 sec/batch; 20h:36m:34s remains)
INFO - root - 2017-12-15 06:44:08.864533: step 10850, loss = 0.38, batch loss = 0.34 (35.1 examples/sec; 0.228 sec/batch; 20h:21m:50s remains)
INFO - root - 2017-12-15 06:44:11.127154: step 10860, loss = 0.25, batch loss = 0.21 (34.6 examples/sec; 0.231 sec/batch; 20h:38m:51s remains)
INFO - root - 2017-12-15 06:44:13.379654: step 10870, loss = 0.24, batch loss = 0.20 (35.9 examples/sec; 0.223 sec/batch; 19h:55m:07s remains)
INFO - root - 2017-12-15 06:44:15.627037: step 10880, loss = 0.32, batch loss = 0.29 (36.6 examples/sec; 0.219 sec/batch; 19h:32m:00s remains)
INFO - root - 2017-12-15 06:44:17.888080: step 10890, loss = 0.30, batch loss = 0.27 (35.6 examples/sec; 0.225 sec/batch; 20h:04m:20s remains)
INFO - root - 2017-12-15 06:44:20.201101: step 10900, loss = 0.31, batch loss = 0.28 (34.3 examples/sec; 0.233 sec/batch; 20h:48m:51s remains)
2017-12-15 06:44:20.476056: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1906853 -6.9864798 -7.16292 -6.9099121 -6.4230976 -6.1507344 -5.8880329 -5.7144728 -5.7949862 -5.7753453 -5.0734491 -2.9919627 -2.2646766 -3.1123753 -3.8393254][-6.9181452 -7.2377481 -7.4190521 -6.9887114 -6.4009285 -6.14495 -5.9160423 -5.833251 -6.065454 -6.0877161 -5.4445772 -3.5077615 -2.9567494 -3.8553808 -4.6205864][-6.5994463 -6.5404043 -6.6608028 -6.1606665 -5.6105337 -5.4651279 -5.3191128 -5.5075626 -6.0792923 -6.2413492 -5.8447628 -4.3060446 -3.9603477 -4.7835679 -5.4280105][-5.2461128 -4.8882685 -4.9936886 -4.5207853 -4.1104441 -4.1317987 -4.1765509 -4.6798463 -5.5892677 -5.9347649 -5.8255568 -4.6450558 -4.2199078 -4.7438149 -5.2183514][-3.4251106 -2.6060481 -2.6651776 -2.1627519 -1.8410325 -1.8875759 -2.0090227 -2.79631 -4.1289721 -4.7514081 -4.8403406 -3.8614941 -3.3431067 -3.625104 -4.1420383][-1.1127487 0.29917097 0.19668078 0.63422012 0.80478263 0.74041939 0.41118789 -0.79124188 -2.6019485 -3.589911 -3.821826 -2.8783121 -2.110759 -2.0903776 -2.6681008][1.7126741 3.2861216 2.972235 3.1736138 3.1925457 3.0200965 2.4340498 0.8774662 -1.2422929 -2.5407348 -2.8830845 -1.8744932 -0.90327942 -0.70268393 -1.456742][2.7988651 4.8567543 4.4343777 4.4670763 4.2853413 3.8015411 2.8739407 1.1791883 -0.86043441 -2.2292192 -2.5452425 -1.4251885 -0.31878352 -0.12089968 -1.1020018][1.5034387 3.9919188 3.6284521 3.483979 2.9433119 2.1426313 1.0524111 -0.39950871 -1.9982309 -3.12116 -3.0507989 -1.5595156 -0.20248139 0.092689037 -0.8710531][-0.68216968 1.8768284 1.6399143 1.3176844 0.61759806 -0.32456315 -1.4115441 -2.5996087 -3.7188044 -4.3794117 -3.9065304 -2.2356086 -0.80335951 -0.41444647 -1.1338367][-3.4232633 -1.2427716 -1.411334 -1.7379014 -2.3594387 -3.2761996 -4.298625 -5.2846022 -6.0755692 -6.3837385 -5.6205339 -3.8666623 -2.3294852 -1.7067726 -1.9720151][-5.8883505 -4.207036 -4.3292332 -4.5543175 -5.0330439 -5.7779808 -6.5633764 -7.2273359 -7.6640015 -7.5833149 -6.6664085 -5.0517673 -3.5792994 -2.869978 -2.82557][-6.6238203 -5.419879 -5.5470333 -5.7548141 -6.1292295 -6.6007576 -7.0061207 -7.2793255 -7.4421005 -7.23647 -6.4601278 -5.2052059 -3.9729309 -3.326544 -3.197928][-6.357193 -5.5849123 -5.7605014 -5.9220247 -6.0853472 -6.273201 -6.3417587 -6.3813858 -6.48174 -6.3459034 -5.860281 -4.9561949 -3.9502819 -3.4251337 -3.3305683][-5.9977207 -5.4365931 -5.6269507 -5.6724029 -5.6953421 -5.7115536 -5.6373234 -5.6245651 -5.7606645 -5.8019314 -5.6100912 -4.979393 -4.1458225 -3.7842929 -3.8784189]]...]
INFO - root - 2017-12-15 06:44:22.734876: step 10910, loss = 0.26, batch loss = 0.22 (36.6 examples/sec; 0.219 sec/batch; 19h:31m:43s remains)
INFO - root - 2017-12-15 06:44:25.045326: step 10920, loss = 0.24, batch loss = 0.21 (34.7 examples/sec; 0.230 sec/batch; 20h:34m:00s remains)
INFO - root - 2017-12-15 06:44:27.324780: step 10930, loss = 0.17, batch loss = 0.14 (34.4 examples/sec; 0.233 sec/batch; 20h:47m:08s remains)
INFO - root - 2017-12-15 06:44:29.568218: step 10940, loss = 0.23, batch loss = 0.20 (36.2 examples/sec; 0.221 sec/batch; 19h:45m:19s remains)
INFO - root - 2017-12-15 06:44:31.855144: step 10950, loss = 0.36, batch loss = 0.33 (36.1 examples/sec; 0.222 sec/batch; 19h:47m:29s remains)
INFO - root - 2017-12-15 06:44:34.152414: step 10960, loss = 0.32, batch loss = 0.29 (34.0 examples/sec; 0.235 sec/batch; 21h:00m:49s remains)
INFO - root - 2017-12-15 06:44:36.448514: step 10970, loss = 0.23, batch loss = 0.20 (35.7 examples/sec; 0.224 sec/batch; 20h:00m:28s remains)
INFO - root - 2017-12-15 06:44:38.747222: step 10980, loss = 0.23, batch loss = 0.20 (36.3 examples/sec; 0.220 sec/batch; 19h:39m:35s remains)
INFO - root - 2017-12-15 06:44:41.001543: step 10990, loss = 0.42, batch loss = 0.38 (35.6 examples/sec; 0.225 sec/batch; 20h:05m:32s remains)
INFO - root - 2017-12-15 06:44:43.258025: step 11000, loss = 0.22, batch loss = 0.19 (36.2 examples/sec; 0.221 sec/batch; 19h:42m:39s remains)
2017-12-15 06:44:43.534912: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5676532 -5.8525162 -6.0883512 -6.2849603 -6.3339858 -6.3644857 -6.3013616 -6.1494694 -5.8760347 -5.6934528 -5.7332 -5.9100838 -6.0442238 -6.16797 -6.2658339][-5.8965621 -6.7021551 -7.1879635 -7.5586667 -7.5864539 -7.3263693 -6.849854 -6.4059095 -5.9393682 -5.7678881 -6.0484858 -6.558104 -6.9371953 -7.2056751 -7.4431725][-6.4123645 -6.716423 -7.2992492 -7.7341852 -7.6817818 -7.0496907 -6.1454086 -5.6096377 -5.272172 -5.310667 -5.8902454 -6.6700344 -7.17251 -7.5543141 -7.9510503][-5.9087677 -5.4711046 -5.8808489 -6.230876 -6.0691843 -5.0752115 -3.8503757 -3.5600653 -3.84988 -4.3969607 -5.4390268 -6.5177221 -7.0430956 -7.4063077 -7.8796606][-5.5101843 -4.1311398 -4.2990689 -4.4204512 -3.9206288 -2.337142 -0.62272918 -0.37218857 -1.2372162 -2.4783816 -4.1613111 -5.5896578 -6.2954636 -6.7046762 -7.2152224][-5.1546812 -3.5502985 -3.3993344 -3.0441546 -1.8836061 0.45618892 2.673605 2.8657842 1.416873 -0.60672021 -2.7940936 -4.4085836 -5.1776037 -5.5774708 -6.0334749][-4.8149481 -3.3832433 -2.842802 -1.8885049 0.072022676 3.1552396 5.6380291 5.5208807 3.4591241 0.83002687 -1.6766949 -3.202353 -3.8139391 -4.1298404 -4.5978975][-5.1824217 -3.6230283 -3.0007823 -1.823272 0.56370711 3.9240012 6.3291125 5.9979053 3.7974615 1.0865119 -1.4128814 -2.7040539 -3.1651416 -3.466362 -3.9898648][-5.7429132 -4.3303661 -3.855505 -2.7686641 -0.41873121 2.6906023 4.7373371 4.3293839 2.3444653 -0.10407567 -2.2403357 -3.3075917 -3.7286978 -3.9965394 -4.4076648][-6.7696977 -5.5913992 -5.1886187 -4.1246147 -1.9013064 0.80579758 2.4021378 1.8431902 -0.0070161819 -1.9913788 -3.5659366 -4.3815436 -4.7304978 -4.9775739 -5.352438][-7.5816932 -6.6122904 -6.2820544 -5.3881769 -3.5465798 -1.5357414 -0.62779868 -1.4122365 -3.0536454 -4.35205 -5.1886048 -5.4810333 -5.6409473 -5.9943953 -6.5302968][-7.7443357 -6.9785142 -6.8181095 -6.2085376 -4.873251 -3.5886893 -3.1998329 -4.0576253 -5.5007377 -6.3944807 -6.7731404 -6.784234 -6.7805161 -7.0530539 -7.4403853][-7.4894953 -6.8449078 -6.7914095 -6.5017776 -5.7832174 -5.134778 -5.0620289 -5.8164358 -6.9361548 -7.4902382 -7.5963082 -7.50727 -7.4554572 -7.5843873 -7.7052765][-7.0224676 -6.2773771 -6.0507879 -5.8040743 -5.4635458 -5.3092084 -5.600492 -6.3343616 -7.133523 -7.440856 -7.4546943 -7.366735 -7.3273721 -7.3744707 -7.3448625][-6.5060105 -5.6423464 -5.2276173 -4.8751087 -4.6255856 -4.6306314 -5.0387545 -5.7033124 -6.3044691 -6.560113 -6.6479182 -6.6527066 -6.6953197 -6.7337813 -6.6409802]]...]
INFO - root - 2017-12-15 06:44:45.829199: step 11010, loss = 0.19, batch loss = 0.15 (35.6 examples/sec; 0.225 sec/batch; 20h:03m:32s remains)
INFO - root - 2017-12-15 06:44:48.080801: step 11020, loss = 0.26, batch loss = 0.23 (35.0 examples/sec; 0.228 sec/batch; 20h:23m:58s remains)
INFO - root - 2017-12-15 06:44:50.358779: step 11030, loss = 0.20, batch loss = 0.16 (35.6 examples/sec; 0.225 sec/batch; 20h:03m:52s remains)
INFO - root - 2017-12-15 06:44:52.644996: step 11040, loss = 0.33, batch loss = 0.30 (33.5 examples/sec; 0.239 sec/batch; 21h:19m:42s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 06:44:54.932832: step 11050, loss = 0.22, batch loss = 0.19 (34.9 examples/sec; 0.229 sec/batch; 20h:27m:03s remains)
INFO - root - 2017-12-15 06:44:57.211422: step 11060, loss = 0.17, batch loss = 0.14 (35.4 examples/sec; 0.226 sec/batch; 20h:09m:20s remains)
INFO - root - 2017-12-15 06:44:59.492315: step 11070, loss = 0.19, batch loss = 0.15 (34.1 examples/sec; 0.235 sec/batch; 20h:57m:54s remains)
INFO - root - 2017-12-15 06:45:01.759858: step 11080, loss = 0.40, batch loss = 0.36 (34.9 examples/sec; 0.229 sec/batch; 20h:26m:34s remains)
INFO - root - 2017-12-15 06:45:04.044239: step 11090, loss = 0.27, batch loss = 0.24 (35.4 examples/sec; 0.226 sec/batch; 20h:11m:50s remains)
INFO - root - 2017-12-15 06:45:06.321055: step 11100, loss = 0.25, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 19h:58m:58s remains)
2017-12-15 06:45:06.585592: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.79079556 -0.020895243 0.084855318 -0.29977167 -1.6292279 -2.6173534 -3.4247174 -4.3191185 -4.9181547 -4.6953015 -4.6005883 -4.2263489 -3.3120995 -2.1584609 -0.67364264][-0.79160428 -0.77933359 -0.84852827 -1.1658204 -2.2707148 -2.999784 -3.5926852 -4.320888 -4.8106632 -4.5924435 -4.2924137 -3.8616052 -3.0790906 -2.0990684 -0.862975][-2.5468497 -1.902382 -1.9445654 -2.0644474 -2.6765881 -2.8731782 -3.1065743 -3.5847342 -4.0116091 -3.9310389 -3.6387129 -3.2972493 -2.82941 -2.3397157 -1.5491225][-4.1611652 -2.5921681 -2.3817382 -2.0639257 -2.0571418 -1.885287 -1.9606749 -2.3620074 -2.7693548 -2.8799102 -2.7035069 -2.5378852 -2.5115745 -2.609642 -2.3786242][-4.6961703 -2.7129393 -2.06094 -1.317646 -0.84432971 -0.36760759 -0.38744962 -0.91544425 -1.4510468 -1.735821 -1.6693052 -1.65714 -1.9804215 -2.6210675 -2.9189048][-4.643055 -2.3424664 -1.3733952 -0.27579927 0.63440084 1.2250946 1.0655169 0.30318403 -0.41245317 -0.8320576 -0.73735094 -0.76316476 -1.2604012 -2.2316282 -3.0253348][-3.4483571 -1.6761273 -0.65386641 0.580765 1.6659663 2.3925948 2.2489481 1.4219816 0.57850432 0.071154594 0.1448772 0.026231766 -0.58572197 -1.6367822 -2.7401595][-3.1960325 -1.6249931 -0.6983974 0.59605861 1.8100193 2.586194 2.5133495 1.8002224 1.0634398 0.55780959 0.60067463 0.40173864 -0.28439033 -1.2957222 -2.3090737][-3.2170141 -1.9896035 -1.3862004 -0.18730211 0.93965077 1.7855952 1.9594531 1.5102065 0.94819903 0.51574469 0.52784562 0.27118015 -0.46731174 -1.277669 -2.0201802][-2.9935024 -2.310123 -2.2269313 -1.3702461 -0.40212405 0.37589645 0.79927969 0.69303679 0.44650626 0.25216079 0.20657063 -0.22815716 -1.1660753 -1.8656067 -2.2044218][-2.6758969 -2.3206177 -2.7902825 -2.2224088 -1.4981896 -0.9476701 -0.3228575 0.13317323 0.33716393 0.42702079 0.344378 -0.23226559 -1.3692131 -2.0003633 -2.1360731][-2.35602 -2.0493526 -2.5576854 -2.1791785 -1.6123924 -1.2794415 -0.54598033 0.14919233 0.49161863 0.60530806 0.44744372 -0.1142931 -1.290921 -1.9629817 -2.0058532][-2.1563144 -1.5441672 -1.8926632 -1.5281819 -1.0582297 -1.0256553 -0.65790451 -0.087080717 0.19540286 0.13545275 -0.01461482 -0.37120712 -1.4448591 -2.0465627 -2.0196123][-2.2563629 -1.2663033 -1.1300963 -0.6896286 -0.29314935 -0.50740874 -0.43744087 -0.051502705 0.050259829 -0.39859176 -0.60024559 -0.76548851 -1.6078684 -2.0948431 -1.9229708][-2.3355436 -0.9763993 -0.56319785 -0.0031380653 0.36235476 0.081460238 -0.20711541 -0.21301258 -0.38435519 -1.1753337 -1.4797314 -1.5025674 -2.0603631 -2.3023925 -1.9398655]]...]
INFO - root - 2017-12-15 06:45:08.826500: step 11110, loss = 0.40, batch loss = 0.36 (34.7 examples/sec; 0.231 sec/batch; 20h:35m:31s remains)
INFO - root - 2017-12-15 06:45:11.104455: step 11120, loss = 0.24, batch loss = 0.20 (35.4 examples/sec; 0.226 sec/batch; 20h:09m:37s remains)
INFO - root - 2017-12-15 06:45:13.350503: step 11130, loss = 0.22, batch loss = 0.19 (34.9 examples/sec; 0.229 sec/batch; 20h:28m:40s remains)
INFO - root - 2017-12-15 06:45:15.623523: step 11140, loss = 0.38, batch loss = 0.34 (34.4 examples/sec; 0.232 sec/batch; 20h:45m:02s remains)
INFO - root - 2017-12-15 06:45:17.897931: step 11150, loss = 0.27, batch loss = 0.24 (35.2 examples/sec; 0.227 sec/batch; 20h:16m:15s remains)
INFO - root - 2017-12-15 06:45:20.164095: step 11160, loss = 0.23, batch loss = 0.19 (34.5 examples/sec; 0.232 sec/batch; 20h:41m:18s remains)
INFO - root - 2017-12-15 06:45:22.419498: step 11170, loss = 0.26, batch loss = 0.22 (34.1 examples/sec; 0.235 sec/batch; 20h:57m:28s remains)
INFO - root - 2017-12-15 06:45:24.680437: step 11180, loss = 0.30, batch loss = 0.26 (35.8 examples/sec; 0.223 sec/batch; 19h:55m:57s remains)
INFO - root - 2017-12-15 06:45:26.925543: step 11190, loss = 0.24, batch loss = 0.21 (35.6 examples/sec; 0.224 sec/batch; 20h:01m:47s remains)
INFO - root - 2017-12-15 06:45:29.185913: step 11200, loss = 0.19, batch loss = 0.16 (35.7 examples/sec; 0.224 sec/batch; 19h:59m:58s remains)
2017-12-15 06:45:29.455189: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4003453 -6.8602009 -6.5209 -6.10468 -6.3501778 -6.4906616 -6.3254347 -5.9080677 -5.5151639 -5.2260051 -5.0683451 -4.9100132 -5.0169582 -5.2055354 -5.3020062][-5.7733526 -6.5139694 -6.4715748 -6.1357803 -6.0565195 -5.8475065 -5.3934884 -4.7724595 -4.45401 -4.4502249 -4.6782203 -4.7474508 -4.9970846 -5.3194137 -5.4178543][-5.7018518 -5.7309656 -5.8396616 -5.4060626 -4.7833543 -4.1214557 -3.5470219 -3.09157 -3.0443118 -3.37223 -4.0054922 -4.487277 -4.9184017 -5.2870555 -5.3286843][-5.6935453 -5.0119138 -5.1041708 -4.4533806 -3.0895488 -1.6858898 -0.92847681 -0.8353883 -1.3625236 -2.1159344 -3.1672058 -4.1150928 -4.8342633 -5.2623835 -5.2248249][-5.6630883 -4.4869776 -4.3048716 -3.2229185 -1.2931368 0.65343571 1.7260854 1.6306663 0.53250408 -0.82575858 -2.3312681 -3.7617164 -4.7515306 -5.2197371 -5.0800762][-5.5112376 -4.2958708 -3.5365057 -1.8184794 0.4633503 2.6027405 3.7700512 3.6690433 2.2812464 0.46052647 -1.4882395 -3.3226168 -4.61319 -5.1424751 -4.9489365][-5.3102446 -4.2708464 -3.1648116 -0.97371244 1.5997374 3.9886553 5.1622133 4.9779186 3.4468229 1.4106364 -0.75212049 -2.7838421 -4.1996903 -4.7447329 -4.5496445][-5.4035091 -4.4950895 -3.5909278 -1.4651768 1.0598352 3.5187738 4.7440405 4.643239 3.1426914 1.3164914 -0.41833425 -1.8394222 -2.8035233 -3.1659439 -3.1117926][-5.4962997 -4.8493052 -4.4222255 -2.83715 -0.87815654 1.1473644 2.3082898 2.5214064 1.6180537 0.4986403 -0.31626773 -0.78868115 -0.98540497 -0.99501073 -1.145695][-5.7319841 -5.354414 -5.4136395 -4.4585466 -3.1608214 -1.6696259 -0.79902208 -0.36679518 -0.38969719 -0.32036984 -0.11033797 0.29420567 0.78169465 1.2151649 0.85491228][-6.0207815 -5.9228325 -6.4101419 -6.0725636 -5.3783064 -4.2638006 -3.4831643 -2.9921086 -2.3413391 -1.240993 -0.054434776 1.2674172 2.4859679 3.2067387 2.5449345][-6.0474253 -6.2031288 -7.0740452 -7.272089 -7.0899076 -6.2891445 -5.5059748 -4.9325929 -4.0760794 -2.4242206 -0.67589831 1.1023705 2.7260473 3.6386302 2.7396348][-5.7667084 -5.9308171 -6.9310446 -7.4251804 -7.6422043 -7.217145 -6.5307531 -6.0334616 -5.3586245 -3.8860905 -2.2291379 -0.56539404 1.1049519 2.1243637 1.4727995][-5.3955755 -5.3617582 -6.1933317 -6.65609 -7.1017666 -7.0738783 -6.6529493 -6.4305849 -6.2066307 -5.2891312 -4.0395956 -2.6471624 -1.3183558 -0.40556192 -0.42971659][-5.1100063 -4.876194 -5.4290519 -5.6699162 -6.1760054 -6.5120687 -6.5068569 -6.7046776 -7.0356646 -6.8367748 -5.9493346 -4.6206331 -3.5739188 -2.8857183 -2.2117763]]...]
INFO - root - 2017-12-15 06:45:31.733104: step 11210, loss = 0.19, batch loss = 0.16 (34.7 examples/sec; 0.230 sec/batch; 20h:34m:12s remains)
INFO - root - 2017-12-15 06:45:33.999216: step 11220, loss = 0.35, batch loss = 0.32 (35.4 examples/sec; 0.226 sec/batch; 20h:09m:49s remains)
INFO - root - 2017-12-15 06:45:36.260404: step 11230, loss = 0.24, batch loss = 0.20 (34.0 examples/sec; 0.235 sec/batch; 20h:59m:40s remains)
INFO - root - 2017-12-15 06:45:38.516465: step 11240, loss = 0.26, batch loss = 0.22 (35.4 examples/sec; 0.226 sec/batch; 20h:09m:02s remains)
INFO - root - 2017-12-15 06:45:40.804051: step 11250, loss = 0.23, batch loss = 0.19 (33.1 examples/sec; 0.242 sec/batch; 21h:33m:24s remains)
INFO - root - 2017-12-15 06:45:43.065870: step 11260, loss = 0.30, batch loss = 0.27 (34.5 examples/sec; 0.232 sec/batch; 20h:43m:14s remains)
INFO - root - 2017-12-15 06:45:45.365316: step 11270, loss = 0.25, batch loss = 0.21 (34.5 examples/sec; 0.232 sec/batch; 20h:42m:44s remains)
INFO - root - 2017-12-15 06:45:47.652673: step 11280, loss = 0.31, batch loss = 0.28 (35.0 examples/sec; 0.229 sec/batch; 20h:24m:47s remains)
INFO - root - 2017-12-15 06:45:49.932922: step 11290, loss = 0.20, batch loss = 0.17 (35.0 examples/sec; 0.228 sec/batch; 20h:22m:05s remains)
INFO - root - 2017-12-15 06:45:52.258990: step 11300, loss = 0.22, batch loss = 0.18 (34.4 examples/sec; 0.233 sec/batch; 20h:44m:50s remains)
2017-12-15 06:45:52.522421: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0769877 -2.4657879 -3.4462752 -3.9566159 -3.8920112 -3.9845648 -4.0556593 -4.254487 -4.4597511 -3.0308022 -0.85816383 0.60054469 0.90589309 0.44533896 -1.2122393][-1.7004812 -1.9038118 -2.7424815 -3.3374796 -3.4890358 -3.7186229 -3.7458694 -4.0090904 -4.2281914 -2.9365382 -1.1285133 0.21013498 0.56567 -0.019257069 -1.8187164][-1.0966234 -1.5629148 -2.3449903 -2.9238391 -2.9887238 -3.1015286 -3.0710912 -3.3126037 -3.5685344 -2.5514455 -1.1446626 -0.072630405 0.20588064 -0.35648835 -2.0569327][-1.628701 -1.1943033 -1.9824947 -2.44273 -2.4323213 -2.4118702 -2.2306633 -2.4276643 -2.8391862 -2.1784766 -1.028897 -0.13353372 0.15451694 -0.22168219 -1.6619499][-2.1289046 -0.25805902 -1.1500151 -1.6333089 -1.6109035 -1.4637473 -1.0819192 -1.1717896 -1.7450042 -1.4401751 -0.57840991 0.15792012 0.3524034 0.061371326 -1.0110027][-1.5362312 0.48245859 -0.54549527 -1.1551297 -0.99116755 -0.56663883 0.15846801 0.2680428 -0.42643774 -0.4135375 0.26103926 0.85263205 0.92568588 0.55779219 -0.5290885][0.092607021 0.99535465 -0.27707303 -1.131791 -0.90950274 -0.18859529 0.973989 1.3779957 0.63176036 0.47155046 1.1699443 1.9119503 2.178993 1.6579635 0.18285966][0.43558741 1.1928875 -0.21741593 -1.264313 -1.1520879 -0.40321255 0.9838078 1.5813437 0.86349368 0.59258485 1.3228402 2.3406823 2.84428 2.3350027 0.69631243][0.44350958 1.287365 -0.17615855 -1.3375914 -1.2747245 -0.60476661 0.47961354 0.79628921 -0.070696115 -0.39649498 0.30836678 1.4080217 2.1447418 1.7324507 -0.023784637][0.027112961 0.87903976 -0.76455152 -2.0224383 -1.9265141 -1.2301564 -0.53960407 -0.53084266 -1.478357 -1.7053261 -0.94469953 0.19849443 0.99532223 0.637913 -1.2238606][-0.36315084 0.58397484 -1.160473 -2.7056007 -2.7993088 -2.3037636 -1.9803209 -2.0982389 -2.8426485 -2.8087947 -1.8716927 -0.73709846 -0.0553329 -0.51144123 -2.2977071][-0.61602843 0.59450912 -1.2765583 -3.2524862 -3.6567073 -3.4532332 -3.3752804 -3.3708403 -3.7850871 -3.4564095 -2.3082485 -1.2335048 -0.72533655 -1.3020601 -2.9675665][-0.96836507 0.30082178 -1.5327606 -3.658462 -4.3773966 -4.4055991 -4.3156013 -3.9296937 -3.9294782 -3.3372903 -2.157413 -1.1759973 -0.82856143 -1.5185137 -3.0537338][-0.93945813 0.23235202 -1.4194221 -3.6092305 -4.8335738 -5.1798024 -5.0823193 -4.3626151 -3.8514075 -2.9168272 -1.7691175 -0.96685755 -0.70029676 -1.5210345 -2.933207][-0.51584911 0.64906526 -0.97360516 -3.3607454 -4.8714571 -5.4140091 -5.372067 -4.6414094 -3.8086653 -2.627749 -1.4843788 -0.86599362 -0.49885845 -1.196174 -2.5327756]]...]
INFO - root - 2017-12-15 06:45:54.827214: step 11310, loss = 0.25, batch loss = 0.21 (33.4 examples/sec; 0.240 sec/batch; 21h:23m:08s remains)
INFO - root - 2017-12-15 06:45:57.092831: step 11320, loss = 0.20, batch loss = 0.16 (36.0 examples/sec; 0.222 sec/batch; 19h:48m:25s remains)
INFO - root - 2017-12-15 06:45:59.348595: step 11330, loss = 0.20, batch loss = 0.16 (35.5 examples/sec; 0.226 sec/batch; 20h:07m:38s remains)
INFO - root - 2017-12-15 06:46:01.656558: step 11340, loss = 0.25, batch loss = 0.21 (34.8 examples/sec; 0.230 sec/batch; 20h:30m:33s remains)
INFO - root - 2017-12-15 06:46:03.957276: step 11350, loss = 0.28, batch loss = 0.25 (33.5 examples/sec; 0.239 sec/batch; 21h:19m:20s remains)
INFO - root - 2017-12-15 06:46:06.200956: step 11360, loss = 0.31, batch loss = 0.28 (34.4 examples/sec; 0.232 sec/batch; 20h:43m:19s remains)
INFO - root - 2017-12-15 06:46:08.492613: step 11370, loss = 0.32, batch loss = 0.28 (35.3 examples/sec; 0.226 sec/batch; 20h:11m:53s remains)
INFO - root - 2017-12-15 06:46:10.779448: step 11380, loss = 0.31, batch loss = 0.27 (33.2 examples/sec; 0.241 sec/batch; 21h:31m:26s remains)
INFO - root - 2017-12-15 06:46:13.034363: step 11390, loss = 0.29, batch loss = 0.26 (36.6 examples/sec; 0.219 sec/batch; 19h:31m:09s remains)
INFO - root - 2017-12-15 06:46:15.319007: step 11400, loss = 0.22, batch loss = 0.18 (34.9 examples/sec; 0.229 sec/batch; 20h:27m:00s remains)
2017-12-15 06:46:15.612794: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5909629 -5.9235315 -6.0316744 -6.1444411 -6.3543291 -6.5717521 -6.5904908 -6.6064496 -6.6147795 -6.8054748 -7.0220647 -7.0331039 -7.009552 -6.7731819 -6.1679792][-6.3786087 -6.8878641 -7.0471106 -7.1431251 -7.2558341 -7.2734408 -6.969264 -6.7816486 -6.8613653 -7.3586183 -7.9355493 -8.1656895 -8.2293549 -7.9598784 -7.1680841][-6.9932446 -6.8341727 -7.0809731 -7.2088671 -7.12687 -6.6769485 -5.7102022 -5.1604939 -5.445466 -6.5183282 -7.6882005 -8.3032761 -8.52812 -8.3505945 -7.4690161][-6.9184885 -5.9739037 -6.1227756 -6.237442 -6.0198765 -5.185854 -3.5976515 -2.714448 -3.4224961 -5.31507 -7.2251019 -8.2113228 -8.4877768 -8.3929043 -7.4472761][-6.3909326 -4.8986883 -4.7825303 -4.6255231 -4.0491233 -2.6724758 -0.4226141 0.78439617 -0.31835902 -3.0753684 -5.8155875 -7.3209867 -7.8856897 -8.0407772 -7.1633024][-5.8673635 -3.885473 -3.4494643 -2.9732463 -1.8632295 0.22976637 3.3832297 5.0731215 3.5543609 -0.09036994 -3.7115455 -5.8486838 -6.7872949 -7.2490525 -6.5663781][-5.5202947 -3.687283 -2.8498468 -1.9905075 -0.41980088 2.2599926 5.9617219 7.917726 6.0005918 1.5587132 -2.724699 -5.2819777 -6.3693047 -6.9423985 -6.234529][-6.2906437 -4.0937786 -2.8894491 -1.8271109 -0.21409035 2.4659271 5.9404645 7.5602794 5.40321 0.88730717 -3.3697903 -5.8606486 -6.864398 -7.3172541 -6.3973608][-7.0533314 -4.3368111 -2.7542303 -1.6564269 -0.5226959 1.5284743 4.3432183 5.4037795 3.1865745 -1.0691974 -5.0302114 -7.2793288 -8.04514 -8.1994228 -6.8844285][-7.4986458 -4.4318705 -2.6228604 -1.6136153 -1.2391996 -0.34034359 1.6399844 2.3358335 0.323097 -3.3615701 -6.8548341 -8.6982 -9.0635071 -8.8176517 -7.13458][-7.9787555 -4.8230667 -2.9571352 -2.0786662 -2.3903441 -2.3142786 -0.94621062 -0.48400974 -2.0429521 -4.8773689 -7.611167 -8.9926853 -9.06177 -8.6045189 -6.8873334][-8.6323242 -5.5883942 -3.7172937 -2.9100099 -3.6029167 -4.0472665 -3.0608137 -2.7820513 -3.9523749 -5.9985628 -8.073391 -9.0852184 -9.0596561 -8.4770737 -6.836195][-8.735281 -5.9471941 -4.46278 -3.9738755 -4.785 -5.3788071 -4.75935 -4.6499143 -5.5336461 -6.9308076 -8.3949242 -9.0950365 -8.92098 -8.1802311 -6.6263533][-7.6993017 -5.2321215 -4.1215076 -3.9155838 -4.6488104 -5.1847563 -4.8777933 -4.9933996 -5.7590451 -6.6801391 -7.5509977 -7.9574633 -7.8179379 -7.1494222 -5.9880104][-6.2583189 -4.2712665 -3.5723114 -3.5463305 -3.9734297 -4.1725669 -4.0269547 -4.1971121 -4.7893581 -5.4666133 -6.0282249 -6.3915949 -6.4706769 -6.1078219 -5.4719534]]...]
INFO - root - 2017-12-15 06:46:17.849211: step 11410, loss = 0.27, batch loss = 0.24 (35.9 examples/sec; 0.223 sec/batch; 19h:51m:25s remains)
INFO - root - 2017-12-15 06:46:20.108845: step 11420, loss = 0.27, batch loss = 0.23 (35.3 examples/sec; 0.227 sec/batch; 20h:14m:08s remains)
INFO - root - 2017-12-15 06:46:22.396243: step 11430, loss = 0.39, batch loss = 0.35 (36.4 examples/sec; 0.220 sec/batch; 19h:37m:30s remains)
INFO - root - 2017-12-15 06:46:24.647424: step 11440, loss = 0.19, batch loss = 0.15 (35.7 examples/sec; 0.224 sec/batch; 19h:57m:50s remains)
INFO - root - 2017-12-15 06:46:26.915261: step 11450, loss = 0.20, batch loss = 0.16 (34.3 examples/sec; 0.234 sec/batch; 20h:49m:38s remains)
INFO - root - 2017-12-15 06:46:29.209353: step 11460, loss = 0.20, batch loss = 0.16 (34.1 examples/sec; 0.235 sec/batch; 20h:56m:35s remains)
INFO - root - 2017-12-15 06:46:31.480118: step 11470, loss = 0.25, batch loss = 0.22 (36.2 examples/sec; 0.221 sec/batch; 19h:43m:45s remains)
INFO - root - 2017-12-15 06:46:33.723906: step 11480, loss = 0.20, batch loss = 0.16 (36.1 examples/sec; 0.222 sec/batch; 19h:47m:08s remains)
INFO - root - 2017-12-15 06:46:36.006113: step 11490, loss = 0.29, batch loss = 0.25 (33.9 examples/sec; 0.236 sec/batch; 21h:01m:46s remains)
INFO - root - 2017-12-15 06:46:38.301438: step 11500, loss = 0.20, batch loss = 0.17 (34.8 examples/sec; 0.230 sec/batch; 20h:31m:32s remains)
2017-12-15 06:46:38.606428: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.8897896 -9.3234921 -8.9129286 -8.6832123 -8.4579878 -7.7786121 -6.8143773 -6.3309035 -6.3984118 -6.4765248 -6.5054646 -6.5786071 -7.2525873 -7.6360736 -6.8220677][-7.1658311 -9.3943748 -8.9257832 -8.5269966 -8.0968485 -7.3217711 -6.0152521 -5.4131594 -5.4133987 -5.6557283 -6.1068215 -6.4198093 -7.3971267 -8.1066875 -7.4532328][-7.2150726 -9.1877632 -8.7745609 -8.1142216 -7.445158 -6.4182534 -4.7797556 -3.9565582 -3.7922816 -4.1402888 -5.1274023 -5.8867493 -7.2298169 -8.3285084 -7.9512262][-7.4077716 -8.7157764 -8.3817883 -7.5066013 -6.6543307 -5.4998474 -3.6005359 -2.5585055 -2.3892953 -2.9146698 -4.3018851 -5.3738317 -6.8595691 -8.2101326 -8.27145][-7.7137179 -8.3705063 -8.2565041 -7.2168345 -6.0813322 -4.7101331 -2.6449761 -1.6609213 -1.885793 -2.7183447 -4.2406049 -5.515667 -6.8508358 -8.0027695 -8.3855858][-6.5179863 -7.0795355 -7.6022768 -6.7694259 -5.485199 -3.8323097 -1.4740045 -0.5837754 -1.4908149 -2.8501322 -4.6138372 -6.1483526 -7.3445811 -8.0601549 -8.46606][-3.8677931 -4.5609517 -5.908577 -5.6393681 -4.4615164 -2.344064 0.66337681 2.0022354 0.65563273 -1.2505195 -3.5732603 -5.6735935 -7.1908956 -7.7928748 -8.28766][-1.3081024 -1.9175906 -3.8723805 -4.3289928 -3.5314178 -1.146647 2.6317673 4.6098619 3.2118134 0.88575864 -1.8820003 -4.5333452 -6.4402633 -7.1927042 -7.849988][0.95487881 0.32759261 -1.8256247 -2.8176627 -2.4624584 -0.12897873 4.1208038 6.6096759 5.4232707 2.7970424 -0.23654568 -3.4143927 -5.7554245 -6.65911 -7.3641958][2.1076336 1.6690757 -0.37315667 -1.5903623 -1.5409607 0.53733754 4.6000023 7.1701779 6.1869559 3.4496765 0.35852551 -3.0792487 -5.7921314 -6.7661796 -7.1618924][1.3747914 1.2927024 -0.54313242 -1.6976216 -1.6976501 -0.020563126 3.3140082 5.3817081 4.4543438 1.8752809 -0.92687321 -4.0496435 -6.7665262 -7.5679665 -7.4509158][-0.86921978 -0.591519 -2.0156393 -2.9847527 -2.8903108 -1.5491912 0.93418741 2.2896504 1.239295 -0.82490027 -2.9480891 -5.3295941 -7.6981096 -8.3218632 -7.8632164][-3.9322896 -3.251153 -4.1584215 -4.7529507 -4.643013 -3.6088524 -1.9808164 -1.4130619 -2.4197111 -3.9216862 -5.2611384 -6.7167158 -8.3835506 -8.6386719 -7.9856124][-6.9866862 -6.0274124 -6.6755624 -7.1218996 -7.1560512 -6.3733988 -5.2882218 -5.2181625 -5.9093142 -6.855679 -7.5542026 -8.323473 -9.1571684 -8.85285 -7.9783359][-9.2467594 -8.0570679 -8.4302273 -8.8894424 -9.064661 -8.5488739 -7.8422484 -7.9611158 -8.2665482 -8.5328312 -8.7275038 -8.963707 -9.08503 -8.4090462 -7.5362492]]...]
INFO - root - 2017-12-15 06:46:40.916204: step 11510, loss = 0.26, batch loss = 0.22 (35.0 examples/sec; 0.228 sec/batch; 20h:22m:08s remains)
INFO - root - 2017-12-15 06:46:43.175886: step 11520, loss = 0.15, batch loss = 0.11 (35.8 examples/sec; 0.223 sec/batch; 19h:55m:17s remains)
INFO - root - 2017-12-15 06:46:45.432055: step 11530, loss = 0.20, batch loss = 0.17 (36.4 examples/sec; 0.220 sec/batch; 19h:34m:18s remains)
INFO - root - 2017-12-15 06:46:47.710174: step 11540, loss = 0.25, batch loss = 0.21 (34.3 examples/sec; 0.233 sec/batch; 20h:46m:14s remains)
INFO - root - 2017-12-15 06:46:49.977605: step 11550, loss = 0.32, batch loss = 0.28 (34.6 examples/sec; 0.231 sec/batch; 20h:38m:06s remains)
INFO - root - 2017-12-15 06:46:52.225879: step 11560, loss = 0.31, batch loss = 0.27 (36.0 examples/sec; 0.222 sec/batch; 19h:49m:36s remains)
INFO - root - 2017-12-15 06:46:54.521262: step 11570, loss = 0.18, batch loss = 0.14 (34.4 examples/sec; 0.232 sec/batch; 20h:43m:32s remains)
INFO - root - 2017-12-15 06:46:56.816182: step 11580, loss = 0.35, batch loss = 0.31 (36.2 examples/sec; 0.221 sec/batch; 19h:42m:44s remains)
INFO - root - 2017-12-15 06:46:59.085337: step 11590, loss = 0.25, batch loss = 0.21 (35.0 examples/sec; 0.228 sec/batch; 20h:21m:46s remains)
INFO - root - 2017-12-15 06:47:01.364957: step 11600, loss = 0.23, batch loss = 0.20 (36.1 examples/sec; 0.222 sec/batch; 19h:46m:47s remains)
2017-12-15 06:47:01.625567: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3774233 -6.9883604 -6.9186468 -6.9204483 -6.8998003 -6.7735329 -6.5542955 -6.3891497 -6.5161533 -6.7738733 -6.86648 -6.7926826 -6.6651831 -6.4771814 -6.4115391][-7.1441755 -7.9437952 -7.8345251 -7.8084354 -7.8250885 -7.7859783 -7.6176548 -7.2685833 -7.2512169 -7.6017942 -7.7973337 -7.8418331 -7.783206 -7.5588579 -7.4390087][-7.6304522 -7.994256 -7.8283243 -7.6095333 -7.4157095 -7.3005161 -7.1446276 -6.7230825 -6.7806578 -7.3682947 -7.6836653 -7.8099575 -7.87284 -7.6970081 -7.5599213][-7.4033842 -7.1814451 -7.0320473 -6.7722006 -6.3861952 -6.0776949 -5.8257213 -5.222765 -5.3151407 -6.2200737 -6.7420006 -7.1508722 -7.499651 -7.4539652 -7.258275][-6.2703886 -5.0860314 -4.8609438 -4.6251049 -4.2074366 -3.8426418 -3.7006702 -3.1600432 -3.256968 -4.2736807 -4.9798851 -5.7067008 -6.3509884 -6.55618 -6.46066][-4.5428162 -2.7412055 -2.2924941 -1.993697 -1.4932563 -0.9624536 -0.83763611 -0.45275831 -0.69040763 -1.966857 -2.9528675 -3.9237003 -4.630187 -4.9082966 -4.9349432][-2.7595887 -0.64645207 0.056323528 0.57230282 1.324002 2.0474365 2.1317327 2.360369 1.9057791 0.38417077 -0.930673 -2.1144958 -2.8181524 -3.0962515 -3.2848878][-1.9393344 0.49486995 1.3894162 2.1093652 3.1370852 4.0779362 4.2604818 4.4475193 4.0095654 2.5347893 0.98996043 -0.40305412 -1.197787 -1.628037 -2.1099405][-2.5454602 -0.0033609867 1.0018687 1.8660996 2.9638288 4.0621643 4.3722973 4.6462622 4.4517879 3.3297913 1.8726737 0.37581778 -0.45729887 -0.96784151 -1.6981049][-4.1701345 -1.794499 -0.87572205 -0.031560898 0.99875641 2.0485251 2.4045107 2.8007791 2.8506272 2.1438153 0.85810924 -0.61825454 -1.3967593 -1.8034749 -2.4356375][-5.754859 -3.7615192 -3.1188734 -2.4692149 -1.6365027 -0.75864768 -0.30343604 0.22495818 0.52228785 0.27199006 -0.8101989 -2.1747901 -2.843255 -3.2511883 -3.8899179][-6.4782372 -4.769249 -4.3391981 -3.9326119 -3.4652905 -2.7896481 -2.3671026 -1.8303872 -1.4162573 -1.4246271 -2.2536654 -3.4531686 -4.1248617 -4.7261009 -5.3996005][-6.5026731 -4.9013619 -4.5415764 -4.3243456 -4.2326555 -3.8852088 -3.6663203 -3.2722685 -2.9423826 -2.8686402 -3.3916693 -4.306242 -4.9501963 -5.6373582 -6.2429428][-6.5808039 -5.144968 -4.7971087 -4.659296 -4.7949896 -4.77359 -4.7260418 -4.5574694 -4.4091806 -4.3182449 -4.5075984 -4.9119167 -5.2533226 -5.8163376 -6.2969646][-6.5660639 -5.3548727 -5.0726762 -4.9842815 -5.2451224 -5.402472 -5.4744053 -5.4957705 -5.5305138 -5.4571557 -5.35405 -5.327282 -5.3937445 -5.7398777 -5.980957]]...]
INFO - root - 2017-12-15 06:47:03.873724: step 11610, loss = 0.18, batch loss = 0.14 (35.4 examples/sec; 0.226 sec/batch; 20h:08m:36s remains)
INFO - root - 2017-12-15 06:47:06.156749: step 11620, loss = 0.23, batch loss = 0.20 (34.7 examples/sec; 0.230 sec/batch; 20h:31m:31s remains)
INFO - root - 2017-12-15 06:47:08.446543: step 11630, loss = 0.24, batch loss = 0.21 (34.0 examples/sec; 0.235 sec/batch; 20h:59m:15s remains)
INFO - root - 2017-12-15 06:47:10.739646: step 11640, loss = 0.20, batch loss = 0.17 (35.4 examples/sec; 0.226 sec/batch; 20h:09m:02s remains)
INFO - root - 2017-12-15 06:47:13.024000: step 11650, loss = 0.24, batch loss = 0.20 (34.1 examples/sec; 0.235 sec/batch; 20h:55m:15s remains)
INFO - root - 2017-12-15 06:47:15.310391: step 11660, loss = 0.31, batch loss = 0.28 (34.6 examples/sec; 0.231 sec/batch; 20h:35m:37s remains)
INFO - root - 2017-12-15 06:47:17.586046: step 11670, loss = 0.19, batch loss = 0.16 (34.6 examples/sec; 0.231 sec/batch; 20h:36m:08s remains)
INFO - root - 2017-12-15 06:47:19.856743: step 11680, loss = 0.21, batch loss = 0.17 (33.5 examples/sec; 0.239 sec/batch; 21h:15m:32s remains)
INFO - root - 2017-12-15 06:47:22.100015: step 11690, loss = 0.49, batch loss = 0.45 (35.7 examples/sec; 0.224 sec/batch; 19h:57m:51s remains)
INFO - root - 2017-12-15 06:47:24.365722: step 11700, loss = 0.23, batch loss = 0.19 (35.7 examples/sec; 0.224 sec/batch; 19h:59m:34s remains)
2017-12-15 06:47:24.660853: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.32911897 -0.060148954 -0.59792328 -1.0974271 -1.2358695 -0.97742939 0.015430689 0.46058774 0.352005 0.52578044 -0.25132191 -1.1095707 -1.4492712 -1.1231925 -0.66877568][-0.28261304 -0.14897466 -0.88984406 -1.7722564 -2.1782846 -1.9901475 -0.9178859 -0.24363089 -0.29812944 -0.009873867 -0.68492818 -1.608171 -2.0454059 -1.8205647 -1.3182671][-0.59451544 -0.26741815 -1.2258351 -2.3036938 -2.8470192 -2.6175756 -1.3966749 -0.66593814 -0.73846853 -0.53369427 -1.2131841 -2.2146144 -2.9860687 -3.0160947 -2.5039263][-0.46656144 -0.011678696 -1.0086598 -1.9363198 -2.3986688 -1.9336426 -0.50542176 0.12138486 -0.0055785179 -0.040723324 -0.83653879 -1.9934134 -3.2167487 -3.6442976 -3.3091002][-0.33622837 0.29944396 -0.62213194 -1.2847309 -1.5703156 -0.85705924 0.666188 1.1271062 0.96821809 0.72526622 -0.33348918 -1.6228036 -3.1508448 -3.9378386 -3.728508][-0.080643177 0.5690465 -0.26254392 -0.525944 -0.44956732 0.563889 2.1845117 2.4523845 2.129447 1.5197642 0.12367344 -1.3187253 -3.0018456 -4.0202904 -3.8224106][0.069251537 0.580426 -0.043882608 0.20843291 0.76328683 2.0393553 3.6364241 3.6160293 2.8740621 1.8798318 0.35128379 -1.0304655 -2.7270579 -3.8060203 -3.4987948][-0.84727073 -0.30375242 -0.79869843 -0.25562191 0.62977839 1.9173942 3.4341712 3.4159217 2.6011472 1.6601794 0.31693578 -0.8358233 -2.2655029 -3.1698375 -2.7490404][-2.2981589 -1.6897026 -2.1254339 -1.6817343 -0.82967377 0.26499438 1.57197 1.6015167 0.89553976 0.37259936 -0.4166379 -1.2893472 -2.5268378 -3.3447967 -2.9184282][-3.9087012 -3.2544532 -3.6489177 -3.412133 -2.7344708 -1.8119469 -0.64001179 -0.42770064 -0.81551456 -0.90491688 -1.1575282 -1.7137535 -2.88065 -3.8060756 -3.6084151][-5.6400223 -4.86026 -5.1594257 -5.2193031 -4.8242197 -3.9674602 -2.7988887 -2.2492383 -2.2222416 -1.9015831 -1.7142615 -2.1693032 -3.2949991 -4.2536426 -4.32734][-6.7628622 -5.8498859 -5.979867 -6.1806197 -6.0442038 -5.2850485 -4.2013173 -3.5754228 -3.2308097 -2.4906838 -1.8628817 -2.30041 -3.5092063 -4.4808664 -4.7886124][-7.2439523 -6.1511011 -5.88832 -5.9472542 -5.7890015 -4.9568472 -3.9618704 -3.36835 -2.8187661 -1.9780554 -1.3013078 -1.7421279 -2.9374111 -3.9072495 -4.44613][-6.25931 -5.0032229 -4.3910871 -4.4000816 -4.4489741 -3.8061922 -2.9730449 -2.35973 -1.7599907 -1.0487716 -0.682436 -1.1895092 -2.2486937 -3.110105 -3.6352165][-4.1600294 -2.8637528 -2.2052388 -2.2878504 -2.5707576 -2.2230239 -1.6496758 -1.2327858 -0.79002547 -0.31285357 -0.22025204 -0.77664196 -1.6797498 -2.2937748 -2.5814629]]...]
INFO - root - 2017-12-15 06:47:26.930595: step 11710, loss = 0.27, batch loss = 0.23 (36.1 examples/sec; 0.222 sec/batch; 19h:45m:24s remains)
INFO - root - 2017-12-15 06:47:29.225005: step 11720, loss = 0.33, batch loss = 0.30 (33.5 examples/sec; 0.239 sec/batch; 21h:17m:20s remains)
INFO - root - 2017-12-15 06:47:31.501838: step 11730, loss = 0.30, batch loss = 0.26 (35.5 examples/sec; 0.226 sec/batch; 20h:06m:11s remains)
INFO - root - 2017-12-15 06:47:33.763519: step 11740, loss = 0.17, batch loss = 0.14 (35.5 examples/sec; 0.225 sec/batch; 20h:05m:07s remains)
INFO - root - 2017-12-15 06:47:36.034983: step 11750, loss = 0.26, batch loss = 0.22 (34.4 examples/sec; 0.233 sec/batch; 20h:43m:41s remains)
INFO - root - 2017-12-15 06:47:38.321658: step 11760, loss = 0.20, batch loss = 0.17 (35.2 examples/sec; 0.227 sec/batch; 20h:13m:16s remains)
INFO - root - 2017-12-15 06:47:40.624552: step 11770, loss = 0.28, batch loss = 0.24 (34.7 examples/sec; 0.231 sec/batch; 20h:32m:13s remains)
INFO - root - 2017-12-15 06:47:42.899458: step 11780, loss = 0.26, batch loss = 0.22 (34.2 examples/sec; 0.234 sec/batch; 20h:48m:58s remains)
INFO - root - 2017-12-15 06:47:45.162173: step 11790, loss = 0.27, batch loss = 0.23 (36.2 examples/sec; 0.221 sec/batch; 19h:40m:16s remains)
INFO - root - 2017-12-15 06:47:47.482535: step 11800, loss = 0.31, batch loss = 0.28 (35.0 examples/sec; 0.229 sec/batch; 20h:22m:31s remains)
2017-12-15 06:47:47.768877: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.91145 -5.3017216 -4.0894265 -3.6933148 -5.3480959 -6.190403 -5.5997882 -5.4333477 -5.8985815 -6.4458904 -7.1318388 -8.7773066 -9.0397663 -7.5504832 -6.1203761][-8.1677265 -4.8505607 -3.4358237 -3.1319656 -5.2374344 -6.307806 -5.8893976 -5.744751 -6.0374193 -6.3734245 -6.9064422 -8.4986267 -9.084938 -7.7813959 -6.1103916][-8.2968616 -3.7737303 -2.1851327 -1.9628303 -4.3232479 -5.372005 -5.1483 -5.1443129 -5.3860846 -5.7000933 -6.2064881 -7.6096492 -8.4760151 -7.5941739 -5.9716616][-7.5829391 -2.2708442 -0.51254678 -0.25421023 -2.5633552 -3.3597207 -3.2400928 -3.404489 -3.7970724 -4.3953323 -5.0813818 -6.3495884 -7.5078363 -7.1196208 -5.7483306][-6.7910371 -0.69596529 1.2718294 1.5500124 -0.60894287 -1.2770895 -1.1732247 -1.3070639 -1.9261551 -2.9332192 -3.8780541 -5.0196071 -6.2130094 -6.274909 -5.4060559][-5.6598825 0.47248316 2.3052106 2.3773828 0.3196795 -0.12958288 0.30420041 0.39552784 -0.34747553 -1.5687392 -2.5044394 -3.5003042 -4.5907512 -5.0108395 -4.7762785][-3.6473322 1.8970079 3.3004875 3.0217447 0.9483912 0.51847911 1.3419855 1.7987714 1.0828409 -0.24104571 -1.0753731 -2.039104 -3.0268462 -3.7332671 -4.1564293][-2.2289031 3.0145988 4.1061959 3.5734982 1.3932185 0.5700922 1.3890748 1.8934331 1.1078959 -0.0072171688 -0.38614416 -1.2162559 -2.1042714 -3.0335567 -3.8808031][-1.8538103 3.16645 4.124331 3.5420027 1.3720348 0.23584008 0.88158679 1.1606772 0.23620462 -0.66779995 -0.67582417 -1.3716719 -2.1734653 -3.2368908 -4.097528][-2.3216159 2.4130783 3.1849518 2.6018724 0.60676455 -0.57358217 -0.0047709942 0.064923525 -0.89877295 -1.5423971 -1.3157446 -2.0466971 -2.9546621 -3.9408398 -4.5641174][-3.9269009 0.61600637 1.4139822 0.9056046 -0.91979146 -2.068969 -1.505515 -1.4005598 -2.251308 -2.8053775 -2.6494908 -3.3431826 -4.1810117 -4.8222318 -4.9944477][-5.7534046 -1.4764709 -0.58908653 -0.93393445 -2.3752534 -3.1678579 -2.5942521 -2.5680413 -3.4174829 -4.0103683 -4.0444884 -4.7016745 -5.4376178 -5.7622976 -5.4523511][-6.8745522 -3.1183252 -2.3778021 -2.6036644 -3.6075006 -4.1431541 -3.7647181 -3.7252522 -4.2910538 -4.732296 -4.8801327 -5.4901819 -6.1016545 -6.1366386 -5.5478792][-7.3643456 -4.2175074 -3.6576562 -3.7979841 -4.4631186 -4.8515463 -4.704793 -4.6485233 -4.9199572 -5.1836224 -5.342298 -5.753469 -6.0564957 -5.8328295 -5.1381392][-6.8507576 -4.3359318 -3.9246988 -4.0025339 -4.3763685 -4.5975313 -4.558754 -4.4877539 -4.6322622 -4.8767185 -5.0818739 -5.3207846 -5.4081707 -5.1781321 -4.6004372]]...]
INFO - root - 2017-12-15 06:47:50.056442: step 11810, loss = 0.19, batch loss = 0.15 (36.2 examples/sec; 0.221 sec/batch; 19h:41m:31s remains)
INFO - root - 2017-12-15 06:47:52.330446: step 11820, loss = 0.25, batch loss = 0.21 (34.6 examples/sec; 0.231 sec/batch; 20h:34m:20s remains)
INFO - root - 2017-12-15 06:47:54.663573: step 11830, loss = 0.21, batch loss = 0.18 (34.7 examples/sec; 0.230 sec/batch; 20h:30m:48s remains)
INFO - root - 2017-12-15 06:47:56.913231: step 11840, loss = 0.27, batch loss = 0.24 (35.3 examples/sec; 0.227 sec/batch; 20h:10m:44s remains)
INFO - root - 2017-12-15 06:47:59.169987: step 11850, loss = 0.34, batch loss = 0.31 (35.1 examples/sec; 0.228 sec/batch; 20h:16m:56s remains)
INFO - root - 2017-12-15 06:48:01.429985: step 11860, loss = 0.20, batch loss = 0.16 (35.6 examples/sec; 0.225 sec/batch; 20h:02m:24s remains)
INFO - root - 2017-12-15 06:48:03.693053: step 11870, loss = 0.21, batch loss = 0.18 (35.9 examples/sec; 0.223 sec/batch; 19h:52m:08s remains)
INFO - root - 2017-12-15 06:48:05.933527: step 11880, loss = 0.18, batch loss = 0.14 (36.5 examples/sec; 0.219 sec/batch; 19h:32m:27s remains)
INFO - root - 2017-12-15 06:48:08.245439: step 11890, loss = 0.28, batch loss = 0.25 (34.1 examples/sec; 0.234 sec/batch; 20h:52m:32s remains)
INFO - root - 2017-12-15 06:48:10.527485: step 11900, loss = 0.21, batch loss = 0.18 (36.3 examples/sec; 0.220 sec/batch; 19h:37m:30s remains)
2017-12-15 06:48:10.831171: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7675474 -3.1605408 -3.2814603 -3.9307828 -4.3926926 -4.3148174 -3.9373288 -3.5502357 -3.0934489 -3.0892379 -3.607666 -4.2484984 -4.8931656 -5.3321443 -5.020647][-3.0623584 -4.31118 -4.4342775 -4.7066069 -4.8965511 -4.7310972 -4.3800712 -4.2376571 -4.0640688 -4.1202316 -4.3384733 -4.6389256 -5.1654558 -5.5315495 -5.1465807][-4.676568 -5.5126147 -5.70336 -5.7740049 -5.7546463 -5.4307451 -4.944005 -4.8307037 -4.9180694 -5.1633306 -5.2502184 -5.3779397 -5.73271 -5.7550097 -5.0210915][-5.6775646 -6.0246019 -6.2254829 -6.2586288 -6.185813 -5.8404446 -5.3189893 -5.142839 -5.3590384 -5.699676 -5.756999 -5.7828445 -5.9373012 -5.6859484 -4.7425027][-5.2835803 -5.0320463 -5.2121224 -5.3702273 -5.4053831 -5.21305 -4.6551685 -4.2641964 -4.4694376 -4.8948941 -5.2105603 -5.512681 -5.751298 -5.5608678 -4.6290436][-4.038177 -3.4513571 -3.4753108 -3.4912002 -3.1841297 -2.5407968 -1.4213109 -0.42671919 -0.46324813 -1.2590622 -2.5297801 -3.9093745 -4.8817196 -5.2919984 -4.6987104][-2.5990005 -2.17155 -1.9773591 -1.5241247 -0.4822582 0.99039149 2.9104965 4.51624 4.6505213 3.2943242 0.75100636 -1.9959745 -3.8927519 -5.0358577 -4.8313928][-1.8824728 -1.7744534 -1.6580244 -0.87102163 0.74431682 2.7808115 5.1048174 6.7599716 6.9099588 5.0997181 1.9012172 -1.3620104 -3.4687839 -4.8919373 -4.89612][-2.0017662 -2.4376497 -2.7827477 -1.8766999 0.17271423 2.4740398 4.688448 5.9842739 5.9857874 4.0151539 1.0434823 -1.8224568 -3.559144 -4.8580556 -4.8817148][-2.5778744 -3.6304564 -4.5417833 -3.7230468 -1.5420408 0.62180519 2.3294532 3.0997713 2.9689224 1.2870889 -0.91007161 -2.9243042 -4.0117311 -4.8948755 -4.7397976][-3.5443246 -4.8584738 -6.0806131 -5.5236492 -3.7154951 -2.2026184 -1.3488624 -1.2206192 -1.4014502 -2.3949654 -3.4875646 -4.5102959 -4.9024405 -5.1439338 -4.6182508][-5.0545764 -6.480958 -7.7256947 -7.3361063 -5.9725037 -4.9987617 -4.483057 -4.321023 -4.1976371 -4.3426223 -4.6020851 -5.1425152 -5.2984734 -5.1982765 -4.5048876][-6.1937943 -7.3701496 -8.2372265 -7.90139 -7.1104069 -6.6837025 -6.3899355 -6.0868821 -5.7317982 -5.3221827 -5.1670189 -5.4829059 -5.5196609 -5.1833773 -4.4010782][-6.1228466 -6.713006 -7.1938725 -7.0977039 -7.0239868 -7.2792873 -7.4457159 -7.4005709 -7.1491241 -6.5493803 -6.0523386 -5.8697524 -5.428287 -4.7421336 -3.9550638][-4.8863497 -4.956409 -5.2962618 -5.5636406 -6.0490465 -6.6950684 -7.0810041 -7.1119394 -6.7544765 -5.9781771 -5.280704 -4.81237 -4.2442117 -3.7064438 -3.2299538]]...]
INFO - root - 2017-12-15 06:48:13.060477: step 11910, loss = 0.28, batch loss = 0.25 (36.1 examples/sec; 0.221 sec/batch; 19h:42m:53s remains)
INFO - root - 2017-12-15 06:48:15.353132: step 11920, loss = 0.24, batch loss = 0.20 (32.9 examples/sec; 0.243 sec/batch; 21h:38m:31s remains)
INFO - root - 2017-12-15 06:48:17.654191: step 11930, loss = 0.22, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 19h:51m:44s remains)
INFO - root - 2017-12-15 06:48:19.926879: step 11940, loss = 0.22, batch loss = 0.18 (34.4 examples/sec; 0.233 sec/batch; 20h:42m:15s remains)
INFO - root - 2017-12-15 06:48:22.231099: step 11950, loss = 0.18, batch loss = 0.15 (35.7 examples/sec; 0.224 sec/batch; 19h:56m:58s remains)
INFO - root - 2017-12-15 06:48:24.527242: step 11960, loss = 0.35, batch loss = 0.32 (35.6 examples/sec; 0.224 sec/batch; 19h:59m:18s remains)
INFO - root - 2017-12-15 06:48:26.821271: step 11970, loss = 0.19, batch loss = 0.16 (33.9 examples/sec; 0.236 sec/batch; 21h:01m:15s remains)
INFO - root - 2017-12-15 06:48:29.080935: step 11980, loss = 0.28, batch loss = 0.24 (35.7 examples/sec; 0.224 sec/batch; 19h:57m:54s remains)
INFO - root - 2017-12-15 06:48:31.343331: step 11990, loss = 0.28, batch loss = 0.25 (35.1 examples/sec; 0.228 sec/batch; 20h:17m:23s remains)
INFO - root - 2017-12-15 06:48:33.618502: step 12000, loss = 0.23, batch loss = 0.20 (33.6 examples/sec; 0.238 sec/batch; 21h:13m:41s remains)
2017-12-15 06:48:33.885337: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0552354 -3.6502914 -3.8734441 -4.0221205 -4.0027666 -3.9784479 -3.7547774 -3.4489036 -3.1926994 -3.3069391 -4.0084205 -4.4759669 -4.4992781 -4.3265724 -4.0472145][-4.0417 -4.0377569 -4.4379168 -4.6004877 -4.464982 -4.2722921 -3.8124886 -3.2862029 -2.8058336 -2.8620138 -3.7977676 -4.4931765 -4.6779904 -4.6070776 -4.3173771][-4.506074 -3.9930739 -4.4393892 -4.5540438 -4.4276476 -4.1720462 -3.6573625 -3.1650522 -2.6590176 -2.682004 -3.5991907 -4.3546667 -4.678237 -4.7211227 -4.4230661][-4.1106386 -3.0746479 -3.3690581 -3.2994046 -3.1201792 -2.8894882 -2.3954725 -2.0464904 -1.6385481 -1.6651881 -2.4926574 -3.2387595 -3.7638845 -4.0537796 -3.9712434][-3.5715191 -2.1068721 -2.3356636 -2.2874498 -2.1998081 -1.9030398 -1.3206105 -0.96964717 -0.56372094 -0.54719841 -1.0971457 -1.7716913 -2.521816 -3.0723064 -3.1986017][-3.2492709 -1.665885 -1.829599 -1.8430874 -1.8287754 -1.4599863 -0.67938375 -0.095406055 0.42919683 0.48956537 0.3470006 -0.16559601 -1.1813711 -2.0572457 -2.5615821][-2.7849905 -1.475446 -1.3476262 -1.1888247 -1.0717144 -0.52920246 0.53033972 1.4254777 2.1915457 2.4315898 2.5802839 2.0714548 0.76354671 -0.47586274 -1.4550353][-3.2784829 -1.5829417 -1.3041396 -1.1491437 -0.93907392 -0.19790363 1.0498352 2.1491768 3.0617735 3.449229 3.7507579 3.2780263 1.9886949 0.59647489 -0.69770885][-3.8262236 -2.1374426 -1.9025959 -1.8930004 -1.6920021 -0.80795872 0.33671331 1.3375604 2.2388337 2.6690643 3.1227443 2.7677062 1.6550429 0.25680375 -1.1219574][-4.110662 -2.2755773 -1.8717161 -1.7235132 -1.378408 -0.60809028 0.080313921 0.62898421 1.1941156 1.5349169 1.9792755 1.7687881 1.0133173 -0.084643364 -1.3947415][-4.024919 -2.0290432 -1.3477057 -0.9014945 -0.60758209 -0.38012636 -0.36959147 -0.4887265 -0.40199733 -0.21673822 0.22001576 0.29644179 0.12729955 -0.49751496 -1.6300964][-4.0284863 -1.9257524 -1.0520129 -0.30949998 0.0055272579 -0.15064883 -0.64190888 -1.2919726 -1.6139376 -1.6296014 -1.2274281 -0.91472244 -0.71887755 -1.1059996 -2.095598][-4.2502108 -2.0854361 -0.96465778 0.13134718 0.64165974 0.30675769 -0.37407088 -1.283551 -1.8316154 -1.9618876 -1.597334 -1.1750438 -0.872978 -1.1248085 -1.8529081][-5.0194283 -3.1016436 -2.0976477 -0.86565661 -0.21620119 -0.57576215 -1.249169 -2.1422663 -2.6179667 -2.6696765 -2.4656553 -2.3022225 -2.1659105 -2.2612855 -2.520927][-5.8060932 -4.3062372 -3.5295453 -2.2729914 -1.5904357 -1.8267093 -2.2777903 -3.0308709 -3.3098729 -3.4019811 -3.4149156 -3.6384737 -3.7076509 -3.65238 -3.5360081]]...]
INFO - root - 2017-12-15 06:48:36.158572: step 12010, loss = 0.24, batch loss = 0.20 (35.7 examples/sec; 0.224 sec/batch; 19h:56m:09s remains)
INFO - root - 2017-12-15 06:48:38.463752: step 12020, loss = 0.27, batch loss = 0.23 (33.3 examples/sec; 0.240 sec/batch; 21h:24m:10s remains)
INFO - root - 2017-12-15 06:48:40.775119: step 12030, loss = 0.27, batch loss = 0.23 (35.0 examples/sec; 0.229 sec/batch; 20h:22m:22s remains)
INFO - root - 2017-12-15 06:48:43.082070: step 12040, loss = 0.23, batch loss = 0.19 (34.8 examples/sec; 0.230 sec/batch; 20h:27m:23s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 06:48:45.333841: step 12050, loss = 0.37, batch loss = 0.34 (35.7 examples/sec; 0.224 sec/batch; 19h:56m:30s remains)
INFO - root - 2017-12-15 06:48:47.579286: step 12060, loss = 0.28, batch loss = 0.24 (35.6 examples/sec; 0.225 sec/batch; 20h:00m:48s remains)
INFO - root - 2017-12-15 06:48:49.853356: step 12070, loss = 0.28, batch loss = 0.25 (34.8 examples/sec; 0.230 sec/batch; 20h:29m:00s remains)
INFO - root - 2017-12-15 06:48:52.159549: step 12080, loss = 0.30, batch loss = 0.26 (34.8 examples/sec; 0.230 sec/batch; 20h:27m:37s remains)
INFO - root - 2017-12-15 06:48:54.476820: step 12090, loss = 0.31, batch loss = 0.27 (33.8 examples/sec; 0.237 sec/batch; 21h:05m:18s remains)
INFO - root - 2017-12-15 06:48:56.800071: step 12100, loss = 0.24, batch loss = 0.20 (35.5 examples/sec; 0.226 sec/batch; 20h:05m:00s remains)
2017-12-15 06:48:57.112928: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4000843 -2.0946798 -2.9457037 -3.6782727 -4.0735774 -4.30607 -4.3196096 -4.1441603 -3.8043523 -3.6571553 -3.8511996 -4.0885563 -4.2488809 -4.5906487 -4.9740934][-2.7782502 -1.9119508 -2.6590486 -3.2646074 -3.662451 -4.0504375 -4.1103387 -4.0331273 -3.8894548 -3.7977648 -3.9393978 -3.7837462 -3.614203 -3.9358261 -4.36203][-3.3092039 -1.8847034 -2.320883 -2.5609498 -2.808434 -3.0870862 -3.029084 -3.0455427 -3.1380484 -3.3167748 -3.7246375 -3.5125561 -3.03714 -3.2001009 -3.4813068][-3.5408511 -1.8718624 -1.8627514 -1.7293836 -1.8091677 -1.9610524 -1.7061799 -1.6643803 -1.9473102 -2.5165534 -3.407804 -3.492353 -3.0799761 -3.2867358 -3.3455248][-4.1972351 -2.3725548 -1.8467462 -1.1862282 -0.89786446 -0.65476036 0.0022926331 0.31142902 -0.035989046 -0.99631381 -2.3775141 -2.8584027 -2.7559464 -3.3162236 -3.4974406][-5.1233029 -3.1193552 -2.2584214 -1.0867305 -0.09404707 0.88163805 2.2519717 3.0613847 2.739419 1.3589494 -0.609269 -1.6691265 -2.0303817 -3.0668631 -3.5138705][-5.3588934 -4.0441236 -3.3554707 -1.9517318 -0.18158746 1.7063377 3.9108276 5.442719 5.3644238 3.6887579 1.2088046 -0.47771418 -1.3749375 -2.7460833 -3.3634608][-5.8064361 -4.9693604 -4.6694922 -3.2811198 -1.0493072 1.3007088 3.9350877 5.975925 6.2763734 4.7538452 2.3360987 0.52872515 -0.64368844 -2.1954455 -2.9615629][-5.6903882 -5.4353471 -5.6485319 -4.4844503 -2.27712 -0.063755989 2.4054694 4.4511933 5.1007342 4.2059517 2.5404286 1.0976925 -0.16668606 -1.881332 -2.7543602][-5.4163027 -5.6882277 -6.3211427 -5.5811396 -3.7560892 -2.0070314 -0.15065908 1.5422068 2.3133631 2.071507 1.3126981 0.49453521 -0.50455356 -1.9698511 -2.6670356][-4.9384527 -5.5982246 -6.5251374 -6.2269034 -4.9033775 -3.6351309 -2.4599435 -1.3735932 -0.75436378 -0.77867341 -1.05189 -1.3101299 -1.8335651 -2.6643674 -2.873446][-4.8813605 -5.5754609 -6.2639537 -6.1106586 -5.2428765 -4.2982306 -3.5408907 -2.9358075 -2.6316071 -2.6720712 -2.8459485 -3.0591466 -3.4387898 -3.8348007 -3.6354015][-5.1010256 -5.6159382 -5.7657213 -5.4113226 -4.7469292 -3.9946773 -3.4055917 -3.0446124 -2.9485953 -3.0972428 -3.4512889 -4.0011859 -4.6696758 -4.9884934 -4.68141][-5.1063948 -5.4434791 -5.2933388 -4.7645268 -4.1294312 -3.4033618 -2.8078127 -2.4543526 -2.4503679 -2.7307467 -3.3520055 -4.2915154 -5.2372212 -5.7449055 -5.7866039][-5.06227 -5.2292037 -4.9771481 -4.3657341 -3.6556559 -2.7780628 -2.0225646 -1.6228659 -1.6735492 -1.986869 -2.6947458 -3.7930388 -4.8895659 -5.6511507 -6.1284232]]...]
INFO - root - 2017-12-15 06:48:59.380572: step 12110, loss = 0.40, batch loss = 0.37 (35.1 examples/sec; 0.228 sec/batch; 20h:17m:04s remains)
INFO - root - 2017-12-15 06:49:01.650114: step 12120, loss = 0.23, batch loss = 0.19 (35.0 examples/sec; 0.229 sec/batch; 20h:20m:51s remains)
INFO - root - 2017-12-15 06:49:03.982216: step 12130, loss = 0.32, batch loss = 0.29 (34.4 examples/sec; 0.232 sec/batch; 20h:40m:33s remains)
INFO - root - 2017-12-15 06:49:06.265136: step 12140, loss = 0.20, batch loss = 0.16 (32.8 examples/sec; 0.244 sec/batch; 21h:41m:45s remains)
INFO - root - 2017-12-15 06:49:08.549917: step 12150, loss = 0.31, batch loss = 0.27 (34.7 examples/sec; 0.230 sec/batch; 20h:30m:23s remains)
INFO - root - 2017-12-15 06:49:10.801342: step 12160, loss = 0.19, batch loss = 0.15 (36.2 examples/sec; 0.221 sec/batch; 19h:39m:09s remains)
INFO - root - 2017-12-15 06:49:13.060718: step 12170, loss = 0.16, batch loss = 0.13 (35.8 examples/sec; 0.224 sec/batch; 19h:53m:40s remains)
INFO - root - 2017-12-15 06:49:15.347679: step 12180, loss = 0.21, batch loss = 0.17 (35.7 examples/sec; 0.224 sec/batch; 19h:56m:09s remains)
INFO - root - 2017-12-15 06:49:17.636874: step 12190, loss = 0.16, batch loss = 0.13 (34.8 examples/sec; 0.230 sec/batch; 20h:27m:00s remains)
INFO - root - 2017-12-15 06:49:19.883486: step 12200, loss = 0.24, batch loss = 0.21 (35.4 examples/sec; 0.226 sec/batch; 20h:07m:52s remains)
2017-12-15 06:49:20.145097: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.82857 -4.5258722 -4.3711128 -4.515995 -4.8517556 -5.1105652 -5.1648045 -5.1175222 -4.8657222 -4.2431602 -3.6876671 -3.4900794 -3.66905 -3.9717569 -4.1000013][-4.8979864 -4.7577076 -4.4744253 -4.7399559 -5.2514229 -5.6141706 -5.7391729 -5.8193569 -5.6956964 -5.0203443 -4.369019 -4.1352181 -4.3807869 -4.7613077 -4.819087][-5.7576823 -4.9776268 -4.6738191 -4.98355 -5.4434137 -5.6869326 -5.844512 -6.0808458 -6.1760387 -5.5968285 -5.0292492 -4.93052 -5.3945088 -5.8576488 -5.6908636][-6.0463982 -5.0190372 -4.720459 -4.9772425 -5.2090073 -5.0587559 -5.02329 -5.295938 -5.6413927 -5.3797574 -5.0936265 -5.2353773 -6.0341768 -6.6191854 -6.3026285][-5.925107 -4.5242987 -4.1951208 -4.2832222 -4.1308384 -3.4358077 -3.0262427 -3.1906686 -3.6455572 -3.8432574 -4.0155563 -4.5055995 -5.643826 -6.4747953 -6.3500042][-4.9743729 -3.4323368 -2.9848559 -2.7720933 -2.1727521 -0.93707657 -0.16777086 -0.27073467 -0.761276 -1.4088271 -2.0369353 -2.8477795 -4.2274947 -5.4148026 -5.8199358][-3.5112681 -2.0452 -1.4718513 -0.94208944 0.027173519 1.7478044 2.8252485 2.7411997 2.2636325 1.2369578 0.10380745 -1.0438999 -2.5651786 -4.0357118 -4.9954157][-2.282393 -0.71319723 -0.069410563 0.66910648 1.8361666 3.8211706 5.1937246 5.2061558 4.6969051 3.3170311 1.6293757 0.15411377 -1.3777289 -3.0726142 -4.4484262][-1.5671358 0.080583096 0.67341638 1.4256799 2.5209873 4.3829794 5.8066225 5.9283428 5.4275084 3.8946221 1.9541619 0.37971091 -1.0709932 -2.8409503 -4.3941178][-1.7380533 -0.13093019 0.26088595 0.73816109 1.4062977 2.6873186 3.8279264 4.1736326 4.0131044 2.7622387 1.0108438 -0.37340879 -1.6933527 -3.3512855 -4.734108][-2.6845245 -1.3069401 -1.2524573 -1.2129517 -1.0908483 -0.48349273 0.31263494 0.89460421 1.2170589 0.56668663 -0.72559071 -1.8520048 -3.0683141 -4.5093431 -5.4664526][-3.827601 -2.7116294 -2.9069257 -3.1408062 -3.3574052 -3.2769151 -2.8756237 -2.3102067 -1.7425965 -1.8648616 -2.6204281 -3.4445062 -4.4840212 -5.5910439 -6.0939703][-4.4852915 -3.495209 -3.7078228 -4.00877 -4.33804 -4.5495739 -4.4731379 -4.061398 -3.4944291 -3.3606944 -3.7028587 -4.2533174 -5.0870957 -5.8201332 -5.9134908][-4.5709944 -3.6220489 -3.7271075 -3.9308343 -4.18642 -4.4291611 -4.5044069 -4.3078384 -3.9241972 -3.7456656 -3.9012194 -4.2573261 -4.7966046 -5.1619635 -5.0082026][-4.3138905 -3.3851895 -3.3920271 -3.4592843 -3.5761247 -3.7303042 -3.843812 -3.8454347 -3.7311363 -3.6823914 -3.8087053 -4.0527964 -4.3236971 -4.3957596 -4.1589603]]...]
INFO - root - 2017-12-15 06:49:22.424382: step 12210, loss = 0.27, batch loss = 0.23 (34.2 examples/sec; 0.234 sec/batch; 20h:48m:54s remains)
INFO - root - 2017-12-15 06:49:24.716291: step 12220, loss = 0.26, batch loss = 0.23 (35.1 examples/sec; 0.228 sec/batch; 20h:16m:27s remains)
INFO - root - 2017-12-15 06:49:26.988736: step 12230, loss = 0.31, batch loss = 0.28 (34.3 examples/sec; 0.233 sec/batch; 20h:43m:36s remains)
INFO - root - 2017-12-15 06:49:29.284749: step 12240, loss = 0.20, batch loss = 0.17 (35.1 examples/sec; 0.228 sec/batch; 20h:17m:28s remains)
INFO - root - 2017-12-15 06:49:31.534048: step 12250, loss = 0.26, batch loss = 0.22 (35.8 examples/sec; 0.223 sec/batch; 19h:51m:09s remains)
INFO - root - 2017-12-15 06:49:33.805551: step 12260, loss = 0.28, batch loss = 0.24 (35.4 examples/sec; 0.226 sec/batch; 20h:07m:31s remains)
INFO - root - 2017-12-15 06:49:36.098272: step 12270, loss = 0.25, batch loss = 0.21 (33.2 examples/sec; 0.241 sec/batch; 21h:26m:29s remains)
INFO - root - 2017-12-15 06:49:38.345716: step 12280, loss = 0.20, batch loss = 0.17 (36.2 examples/sec; 0.221 sec/batch; 19h:38m:09s remains)
INFO - root - 2017-12-15 06:49:40.636137: step 12290, loss = 0.29, batch loss = 0.25 (35.2 examples/sec; 0.228 sec/batch; 20h:14m:31s remains)
INFO - root - 2017-12-15 06:49:42.953551: step 12300, loss = 0.18, batch loss = 0.15 (34.8 examples/sec; 0.230 sec/batch; 20h:27m:25s remains)
2017-12-15 06:49:43.257624: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7961855 -4.6742616 -4.5578785 -4.7208633 -5.20557 -6.1888475 -6.9298506 -7.2295485 -6.9528837 -6.0835953 -5.5706129 -5.2327251 -5.243619 -5.3517494 -5.2742181][-5.3002396 -4.3284483 -4.1104684 -3.9753997 -4.168891 -4.8553877 -5.6274433 -6.4327831 -6.9399519 -6.8130217 -6.5323524 -6.0383072 -5.8447485 -5.8233747 -5.6021032][-5.93898 -3.7020459 -3.0641665 -2.5379257 -2.4456203 -2.9420233 -3.9271679 -5.3166246 -6.5332956 -7.2259493 -7.33004 -6.9415541 -6.6524448 -6.3297129 -5.7769947][-6.191988 -3.2037234 -2.067996 -1.1250682 -0.60124648 -0.82929277 -1.9745182 -3.7623885 -5.6256008 -6.954668 -7.2560673 -7.0133543 -6.7908878 -6.3640594 -5.6428428][-6.0534344 -2.8309255 -1.2785656 0.0040638447 0.84167624 0.87325692 -0.15969944 -2.0418761 -4.2494755 -5.8732882 -6.2583456 -6.1935825 -6.2497349 -5.9344306 -5.2145][-6.0953817 -2.4227939 -0.3944757 1.4961333 2.8728588 3.3133209 2.4367416 0.47647119 -2.0354517 -4.2117119 -5.12693 -5.4901228 -5.8317451 -5.6163793 -4.9321833][-5.9060965 -2.6175683 -0.16772199 2.4132841 4.5095453 5.7058935 5.4743881 3.6489236 0.75383067 -2.2696226 -4.1444674 -5.2139363 -5.8371286 -5.6931858 -5.1329184][-6.7408428 -3.6016319 -0.8765136 2.3112605 5.0081539 6.8164845 7.1228952 5.7299538 2.8300102 -0.66449845 -3.2806818 -5.0831261 -5.9189043 -5.8693552 -5.5383854][-7.8693228 -5.3361292 -2.735045 0.86691356 3.9158762 5.9784851 6.6971159 5.75321 3.0900934 -0.35772312 -3.1768527 -5.3179893 -6.3217297 -6.5321302 -6.4432383][-8.6472521 -7.1216421 -5.315721 -1.9830711 1.0447948 3.1234438 4.1385164 3.6664593 1.4108734 -1.6993506 -4.4116011 -6.6481676 -7.7009592 -7.9713697 -7.7408104][-8.7026825 -8.1202526 -7.3179808 -4.5855217 -1.9266195 -0.26979816 0.55784726 0.19403744 -1.6031072 -3.9099469 -5.9314871 -7.8486576 -8.7997 -9.00668 -8.5509481][-8.4286728 -8.3680811 -8.2679443 -6.2573252 -4.0998478 -2.9560213 -2.4926083 -2.881897 -4.1249647 -5.5975027 -6.8601122 -8.2075281 -8.7792826 -8.7816172 -8.1811638][-7.9829226 -8.0968771 -8.3455448 -6.9465265 -5.2759619 -4.6212749 -4.4942651 -4.9441519 -5.6661739 -6.1834745 -6.649 -7.477541 -7.7760725 -7.7787514 -7.1065035][-7.0103016 -7.2668447 -7.7054281 -6.9000854 -5.7441659 -5.3640876 -5.3743773 -5.7205715 -5.8143406 -5.4206333 -5.200861 -5.7170763 -6.0797195 -6.3733358 -5.673214][-5.9997492 -6.3657 -6.7710819 -6.3322048 -5.49634 -5.122859 -5.0319114 -5.0286713 -4.5471296 -3.7552509 -3.4810052 -3.8155403 -4.2124529 -4.6228423 -3.8262544]]...]
INFO - root - 2017-12-15 06:49:45.560619: step 12310, loss = 0.28, batch loss = 0.25 (35.7 examples/sec; 0.224 sec/batch; 19h:56m:17s remains)
INFO - root - 2017-12-15 06:49:47.817146: step 12320, loss = 0.22, batch loss = 0.18 (34.2 examples/sec; 0.234 sec/batch; 20h:49m:44s remains)
INFO - root - 2017-12-15 06:49:50.090080: step 12330, loss = 0.27, batch loss = 0.23 (35.0 examples/sec; 0.228 sec/batch; 20h:18m:04s remains)
INFO - root - 2017-12-15 06:49:52.359365: step 12340, loss = 0.36, batch loss = 0.33 (34.8 examples/sec; 0.230 sec/batch; 20h:26m:13s remains)
INFO - root - 2017-12-15 06:49:54.633799: step 12350, loss = 0.33, batch loss = 0.30 (35.6 examples/sec; 0.225 sec/batch; 19h:59m:12s remains)
INFO - root - 2017-12-15 06:49:56.899366: step 12360, loss = 0.26, batch loss = 0.22 (35.1 examples/sec; 0.228 sec/batch; 20h:15m:32s remains)
INFO - root - 2017-12-15 06:49:59.148083: step 12370, loss = 0.19, batch loss = 0.16 (35.6 examples/sec; 0.225 sec/batch; 19h:58m:55s remains)
INFO - root - 2017-12-15 06:50:01.439510: step 12380, loss = 0.26, batch loss = 0.23 (35.8 examples/sec; 0.224 sec/batch; 19h:53m:11s remains)
INFO - root - 2017-12-15 06:50:03.700786: step 12390, loss = 0.35, batch loss = 0.32 (34.5 examples/sec; 0.232 sec/batch; 20h:38m:06s remains)
INFO - root - 2017-12-15 06:50:05.968588: step 12400, loss = 0.20, batch loss = 0.16 (34.5 examples/sec; 0.232 sec/batch; 20h:37m:56s remains)
2017-12-15 06:50:06.241354: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9361751 -2.8179321 -2.9735589 -3.0699034 -2.9376688 -2.9186704 -2.7553971 -2.4826972 -2.1361794 -1.8406045 -2.0471802 -2.3689167 -2.4447436 -2.4249654 -2.3958783][-2.8412981 -3.3592715 -3.2759204 -3.2216766 -3.1668134 -3.243875 -3.2194004 -3.0760403 -2.792944 -2.6062899 -3.0315449 -3.4986911 -3.6287913 -3.7342987 -3.8142486][-3.68083 -3.5668857 -3.2218258 -3.0132017 -2.8546586 -2.8355823 -2.7479787 -2.5327919 -2.2582502 -2.2255847 -3.0070748 -3.735981 -4.0503397 -4.1985407 -4.1415577][-4.34099 -3.5464258 -2.9492877 -2.5496333 -2.3342223 -2.3808658 -2.3800404 -2.0583513 -1.601321 -1.4661059 -2.2539446 -3.0772173 -3.6138787 -3.9283452 -3.8142242][-4.6802969 -3.3178186 -2.5495219 -1.9706086 -1.7947197 -1.9904734 -2.1455796 -1.7461308 -1.0828834 -0.74356866 -1.3318632 -2.0397673 -2.7857807 -3.4476004 -3.4639392][-4.0403323 -2.5996423 -1.8379664 -1.2318205 -1.1924076 -1.5604459 -1.7633172 -1.2252665 -0.42818666 -0.0046088696 -0.4371531 -1.0074418 -1.859408 -2.7748692 -2.9073091][-2.8713231 -1.7874718 -1.1304916 -0.6433959 -0.88556111 -1.5007355 -1.6241677 -0.8452208 0.18069005 0.73712921 0.41059995 -0.034323931 -0.90641093 -1.9656693 -2.1502507][-2.1582084 -1.248686 -0.77391338 -0.50935435 -1.0619044 -1.8821446 -1.9210281 -1.0655704 0.057241917 0.70209169 0.47255874 0.0037355423 -0.89043975 -1.8252411 -1.8029766][-2.1328127 -1.3996222 -1.0337474 -0.85871315 -1.4205599 -2.2295527 -2.2531705 -1.6582479 -0.81717706 -0.302428 -0.50489926 -0.96150446 -1.7400824 -2.3157198 -1.8796569][-2.4036758 -1.8199216 -1.5669956 -1.4157015 -1.7827021 -2.3841219 -2.40141 -2.2200963 -1.8756132 -1.6732546 -1.8170655 -2.111763 -2.6760654 -3.0604208 -2.65799][-2.2840037 -1.8282895 -1.7134271 -1.5939887 -1.7804133 -2.1391795 -2.1754792 -2.3536403 -2.4018216 -2.5155826 -2.5887184 -2.6829762 -3.0689187 -3.5248566 -3.553647][-1.7757572 -1.3923104 -1.3684728 -1.3072741 -1.3776881 -1.5206473 -1.5579667 -1.9153118 -2.1344981 -2.3851292 -2.4059608 -2.3424926 -2.6288302 -3.2567902 -3.839788][-1.8303605 -1.4089031 -1.3589866 -1.3103361 -1.2959069 -1.2646829 -1.2080112 -1.4128075 -1.4466023 -1.4801688 -1.2734225 -1.1012 -1.3920643 -2.0834894 -3.0133073][-1.8563906 -1.5097675 -1.6109502 -1.6828192 -1.6237087 -1.4222505 -1.2185568 -1.1166595 -0.78457439 -0.50870609 -0.10633278 0.10640407 -0.24659598 -0.96235704 -2.0266469][-2.0347776 -1.8667214 -2.1686702 -2.3634052 -2.314501 -2.0095174 -1.5642092 -1.1034585 -0.42770386 0.065169573 0.59328556 0.78333378 0.38668323 -0.27403426 -1.2798834]]...]
INFO - root - 2017-12-15 06:50:08.538219: step 12410, loss = 0.20, batch loss = 0.16 (33.4 examples/sec; 0.240 sec/batch; 21h:18m:28s remains)
INFO - root - 2017-12-15 06:50:10.816746: step 12420, loss = 0.18, batch loss = 0.15 (33.6 examples/sec; 0.238 sec/batch; 21h:11m:10s remains)
INFO - root - 2017-12-15 06:50:13.138152: step 12430, loss = 0.19, batch loss = 0.15 (34.2 examples/sec; 0.234 sec/batch; 20h:48m:40s remains)
INFO - root - 2017-12-15 06:50:15.411193: step 12440, loss = 0.23, batch loss = 0.20 (35.0 examples/sec; 0.229 sec/batch; 20h:19m:33s remains)
INFO - root - 2017-12-15 06:50:17.657344: step 12450, loss = 0.27, batch loss = 0.24 (35.9 examples/sec; 0.223 sec/batch; 19h:49m:18s remains)
INFO - root - 2017-12-15 06:50:19.926891: step 12460, loss = 0.25, batch loss = 0.22 (35.1 examples/sec; 0.228 sec/batch; 20h:17m:15s remains)
INFO - root - 2017-12-15 06:50:22.210660: step 12470, loss = 0.23, batch loss = 0.20 (36.4 examples/sec; 0.220 sec/batch; 19h:33m:10s remains)
INFO - root - 2017-12-15 06:50:24.467612: step 12480, loss = 0.23, batch loss = 0.19 (35.2 examples/sec; 0.227 sec/batch; 20h:10m:58s remains)
INFO - root - 2017-12-15 06:50:26.748413: step 12490, loss = 0.34, batch loss = 0.30 (34.1 examples/sec; 0.234 sec/batch; 20h:50m:27s remains)
INFO - root - 2017-12-15 06:50:29.013666: step 12500, loss = 0.24, batch loss = 0.21 (34.4 examples/sec; 0.232 sec/batch; 20h:39m:26s remains)
2017-12-15 06:50:29.276542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.19892907 -1.5390687 -2.2837331 -2.3875442 -1.9668397 -1.4431537 -0.60563982 -0.20653784 -0.64080894 -1.8642021 -3.5219033 -4.5716496 -4.3881588 -3.5431461 -2.3501091][0.46776104 -0.29599094 -1.2187316 -1.7277193 -1.6689651 -1.3276966 -0.5120455 -0.086170673 -0.44033074 -1.5099411 -3.0323608 -4.0273476 -3.8630028 -3.1432424 -2.0430379][0.27787256 0.15738845 -0.86736476 -1.7761123 -2.1489635 -2.1006231 -1.3868611 -0.95291126 -1.2352326 -2.1119902 -3.3773088 -4.1056151 -3.80932 -3.0590806 -2.059854][-0.16956401 0.05968523 -0.8476814 -1.8584775 -2.4835103 -2.6426861 -2.1287558 -1.8303102 -2.1206102 -2.8254783 -3.7966223 -4.2173829 -3.7435641 -3.0283351 -2.2285604][-1.2159903 -0.61100781 -1.1125429 -1.791667 -2.1652596 -2.0995631 -1.5906093 -1.3967335 -1.9148597 -2.7777779 -3.7980869 -4.1631422 -3.6885505 -2.9563525 -2.1286628][-1.9345684 -1.1820662 -1.3185458 -1.4099755 -1.0769174 -0.25403845 0.74943113 0.98657227 -0.073684454 -1.6963084 -3.2922788 -3.979794 -3.640938 -2.9293168 -2.0541756][-2.2342589 -1.5519168 -1.3754843 -0.95357025 0.12299228 1.889931 3.6504428 4.0561352 2.4134567 -0.13593626 -2.4890916 -3.6877105 -3.711046 -3.2008524 -2.3901796][-2.2229648 -1.6442673 -1.3601475 -0.70098162 0.73290133 3.0095246 5.265872 5.9498377 4.1841974 1.2349942 -1.5032312 -3.1245546 -3.5485029 -3.294219 -2.5591755][-2.0260477 -1.8446139 -1.7813587 -1.2075884 0.1172514 2.2506034 4.5500374 5.5175171 4.1786709 1.602021 -0.91322124 -2.5591493 -3.1588955 -3.0367258 -2.4026031][-1.7285426 -1.8558248 -2.1025641 -1.90216 -1.1011921 0.42515421 2.3219988 3.3377645 2.6183307 0.87617755 -0.93468308 -2.1728239 -2.5794189 -2.3701754 -1.7644815][-1.9053473 -2.1481953 -2.5143447 -2.6077497 -2.1976044 -1.2142906 0.10896087 0.92636609 0.63378716 -0.36244357 -1.3875377 -1.9906869 -1.9028704 -1.3310606 -0.70624113][-2.674468 -2.8065345 -2.95366 -2.9471135 -2.5416536 -1.8423367 -1.0646915 -0.60364497 -0.75370646 -1.3410878 -1.9325296 -2.1690412 -1.7654238 -0.97010517 -0.44862318][-3.2915604 -3.2605257 -3.0751631 -2.8965943 -2.4317596 -1.8346491 -1.3882153 -1.3027868 -1.5500262 -2.1979654 -2.8703246 -3.1120162 -2.6380527 -1.8541245 -1.4709799][-3.6294394 -3.6650074 -3.3400035 -3.0394175 -2.4583614 -1.7630521 -1.4029078 -1.475253 -1.8493695 -2.6965733 -3.4787576 -3.7193866 -3.3119597 -2.710248 -2.5174174][-3.7410872 -3.9848511 -3.6770411 -3.3221512 -2.714273 -2.078743 -1.8457894 -2.0175135 -2.4104776 -3.3131523 -4.1288524 -4.3934717 -4.1539836 -3.784961 -3.631418]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-12500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-12500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 06:50:31.904474: step 12510, loss = 0.25, batch loss = 0.21 (35.0 examples/sec; 0.229 sec/batch; 20h:19m:43s remains)
INFO - root - 2017-12-15 06:50:34.178481: step 12520, loss = 0.29, batch loss = 0.26 (36.4 examples/sec; 0.220 sec/batch; 19h:30m:57s remains)
INFO - root - 2017-12-15 06:50:36.472959: step 12530, loss = 0.23, batch loss = 0.20 (33.5 examples/sec; 0.239 sec/batch; 21h:14m:16s remains)
INFO - root - 2017-12-15 06:50:38.723853: step 12540, loss = 0.19, batch loss = 0.16 (35.1 examples/sec; 0.228 sec/batch; 20h:15m:40s remains)
INFO - root - 2017-12-15 06:50:41.018047: step 12550, loss = 0.34, batch loss = 0.31 (35.8 examples/sec; 0.223 sec/batch; 19h:50m:52s remains)
INFO - root - 2017-12-15 06:50:43.340930: step 12560, loss = 0.35, batch loss = 0.32 (34.7 examples/sec; 0.230 sec/batch; 20h:28m:16s remains)
INFO - root - 2017-12-15 06:50:45.610441: step 12570, loss = 0.17, batch loss = 0.14 (35.1 examples/sec; 0.228 sec/batch; 20h:16m:37s remains)
INFO - root - 2017-12-15 06:50:47.895103: step 12580, loss = 0.21, batch loss = 0.17 (34.9 examples/sec; 0.230 sec/batch; 20h:23m:57s remains)
INFO - root - 2017-12-15 06:50:50.181807: step 12590, loss = 0.27, batch loss = 0.23 (34.0 examples/sec; 0.235 sec/batch; 20h:55m:38s remains)
INFO - root - 2017-12-15 06:50:52.457075: step 12600, loss = 0.24, batch loss = 0.21 (33.7 examples/sec; 0.238 sec/batch; 21h:07m:07s remains)
2017-12-15 06:50:52.742479: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3968029 -2.0970297 -3.1011815 -3.4376144 -2.8839393 -2.6298628 -2.6296556 -2.6857226 -3.1629293 -3.1769838 -3.214829 -4.4375005 -5.070569 -4.7524195 -5.2399864][-1.1441895 -1.3201108 -2.2683568 -2.3966861 -1.7634267 -1.3649716 -1.2558486 -1.2619269 -1.8275224 -2.017252 -2.1809506 -3.3827007 -4.3038216 -4.4859481 -5.16881][-1.1214404 -0.78922868 -1.5771304 -1.5053227 -0.85763061 -0.18493652 0.29237294 0.37778497 -0.48366308 -1.0873256 -1.4421812 -2.5047431 -3.502605 -4.0145416 -4.8409748][-1.9286884 -0.8820951 -1.4553273 -1.228929 -0.50263834 0.47161674 1.2737875 1.3879826 0.2326324 -0.66587996 -1.0715506 -1.8988714 -2.8676524 -3.6251636 -4.5067453][-2.4450543 -1.0983886 -1.4680088 -1.1367594 -0.31957591 0.81419325 1.778157 1.8254986 0.5062356 -0.53592765 -0.96243668 -1.6788228 -2.7306638 -3.6002491 -4.4486675][-2.778095 -1.2932208 -1.7250738 -1.5075095 -0.55313694 0.93519044 2.2453108 2.2420716 0.85622096 -0.31561458 -0.90349638 -1.6803775 -2.8754125 -3.8006167 -4.4979038][-2.6353617 -1.5390277 -2.0504255 -1.7132072 -0.58504152 1.2911761 2.9250283 2.9279075 1.5968475 0.24765968 -0.72089624 -1.9437386 -3.4489498 -4.4265184 -5.0134439][-3.0987518 -1.8115367 -2.3157532 -1.9859579 -0.83711028 1.1611857 2.8177605 2.7865548 1.7010961 0.43416381 -0.90899122 -2.6766758 -4.3400812 -5.14734 -5.6387272][-4.0614514 -2.6098597 -2.95697 -2.656168 -1.7144666 0.099029541 1.7081459 1.6700394 0.91410851 -0.10003543 -1.749293 -4.0146046 -5.5192332 -5.8451481 -6.1207628][-5.5636311 -3.92482 -3.8519411 -3.557267 -2.9488976 -1.5185694 -0.13320708 -0.20904946 -0.7355907 -1.4730769 -3.1620114 -5.5599694 -6.6701436 -6.339262 -6.3008695][-6.928844 -5.2611623 -5.0173717 -4.8195491 -4.5828686 -3.6367564 -2.4700716 -2.576761 -2.9731946 -3.4320712 -4.8040895 -6.8246317 -7.3305697 -6.4300413 -6.0952849][-7.4903188 -6.062736 -5.872611 -5.7771735 -5.7018147 -5.1921883 -4.3592081 -4.5830011 -5.0296783 -5.3165274 -6.2868872 -7.6782093 -7.6736622 -6.4921455 -6.0026436][-7.4009662 -6.2063088 -6.0292234 -5.9335194 -6.0110455 -5.8787007 -5.3747015 -5.6037064 -5.958456 -6.11845 -6.7182579 -7.5377054 -7.3083553 -6.3025327 -5.8423996][-6.80265 -5.939682 -5.9757538 -5.9149466 -5.9546061 -5.91728 -5.6123838 -5.6692357 -5.8085694 -5.8588619 -6.1344504 -6.4432349 -6.2015777 -5.6332836 -5.3798943][-5.7933807 -5.2048721 -5.361105 -5.3050966 -5.2964554 -5.3203545 -5.1549187 -5.100194 -5.1279955 -5.1561871 -5.2488966 -5.2486153 -5.0554862 -4.8354969 -4.6847291]]...]
INFO - root - 2017-12-15 06:50:55.073555: step 12610, loss = 0.23, batch loss = 0.19 (35.4 examples/sec; 0.226 sec/batch; 20h:05m:46s remains)
INFO - root - 2017-12-15 06:50:57.395504: step 12620, loss = 0.28, batch loss = 0.24 (34.4 examples/sec; 0.232 sec/batch; 20h:38m:29s remains)
INFO - root - 2017-12-15 06:50:59.720754: step 12630, loss = 0.28, batch loss = 0.24 (34.8 examples/sec; 0.230 sec/batch; 20h:23m:55s remains)
INFO - root - 2017-12-15 06:51:02.048408: step 12640, loss = 0.19, batch loss = 0.15 (31.8 examples/sec; 0.252 sec/batch; 22h:21m:41s remains)
INFO - root - 2017-12-15 06:51:04.349555: step 12650, loss = 0.21, batch loss = 0.18 (34.9 examples/sec; 0.229 sec/batch; 20h:20m:21s remains)
INFO - root - 2017-12-15 06:51:06.669187: step 12660, loss = 0.24, batch loss = 0.21 (35.2 examples/sec; 0.227 sec/batch; 20h:11m:06s remains)
INFO - root - 2017-12-15 06:51:08.977745: step 12670, loss = 0.35, batch loss = 0.32 (34.1 examples/sec; 0.235 sec/batch; 20h:51m:26s remains)
INFO - root - 2017-12-15 06:51:11.298986: step 12680, loss = 0.23, batch loss = 0.20 (34.3 examples/sec; 0.233 sec/batch; 20h:41m:33s remains)
INFO - root - 2017-12-15 06:51:13.610521: step 12690, loss = 0.30, batch loss = 0.26 (35.0 examples/sec; 0.228 sec/batch; 20h:16m:50s remains)
INFO - root - 2017-12-15 06:51:15.910426: step 12700, loss = 0.22, batch loss = 0.19 (34.7 examples/sec; 0.231 sec/batch; 20h:29m:30s remains)
2017-12-15 06:51:16.263057: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0367451 -6.0309677 -6.1657648 -6.1753693 -5.7006364 -5.1285295 -4.6988277 -4.2413654 -4.3143 -5.018002 -5.7859373 -6.2713771 -5.8998504 -5.0900421 -4.4653034][-5.5364237 -6.356544 -6.1895909 -5.9124465 -5.0297618 -4.2118678 -3.5914452 -3.1401613 -3.4242408 -4.4097857 -5.5448723 -6.3809266 -6.2834826 -5.445406 -4.8121533][-6.7512703 -6.4800587 -5.9615908 -5.1390409 -3.692945 -2.4541137 -1.6026582 -1.1971135 -1.8663285 -3.3248632 -4.8836107 -6.1123123 -6.23826 -5.3859453 -4.7054019][-7.5491676 -6.5165319 -5.7282915 -4.34723 -2.2720852 -0.53636611 0.719553 1.251734 0.26709151 -1.7533216 -4.0043659 -5.8121481 -6.3304281 -5.6127572 -4.8737993][-7.8479705 -6.5894256 -5.8026981 -4.1441703 -1.6159009 0.58158922 2.26375 3.012032 1.9781299 -0.45229709 -3.2801788 -5.6145191 -6.5022435 -6.0165176 -5.2391586][-8.1546183 -6.6881104 -6.0085664 -4.3583746 -1.6194588 0.92921662 2.9946356 4.0300536 3.0885472 0.50256038 -2.6959956 -5.4280715 -6.5710998 -6.3584118 -5.5892467][-6.9161181 -5.991497 -5.1051655 -3.2948079 -0.19150019 2.9142509 5.5210834 6.802309 5.8307362 2.9165978 -0.88056421 -4.2986803 -5.9722242 -6.2290978 -5.6370077][-6.4725776 -5.3824391 -4.52225 -2.9431429 -0.014304399 3.0976629 5.7873955 7.0702591 6.100563 3.221324 -0.41999948 -3.8813663 -5.58451 -5.9149389 -5.332346][-6.0532408 -5.0787859 -4.4321103 -3.242775 -0.83458817 1.9014049 4.29012 5.2164168 4.0632286 1.3251145 -1.8455323 -4.8803935 -6.0644712 -6.0088825 -5.0735383][-5.7009163 -4.8673229 -4.4655447 -3.6322393 -1.6353927 0.82711744 2.9642763 3.6846991 2.298337 -0.46423995 -3.4067433 -6.0795922 -6.8798437 -6.4421992 -5.054059][-5.7001257 -5.149334 -5.1763988 -4.8512363 -3.3685529 -1.2432996 0.69264507 1.3170917 -0.072778225 -2.7166438 -5.2235489 -7.3592186 -7.7338982 -6.9448824 -5.2163982][-5.8321409 -5.6021657 -6.0612612 -6.3394938 -5.481164 -3.8763149 -2.3292737 -1.7862626 -3.090404 -5.2787504 -7.10497 -8.5434675 -8.4333839 -7.226222 -5.3565807][-6.0124946 -6.0413942 -6.826601 -7.575129 -7.2826028 -6.1849618 -4.8874459 -4.3803816 -5.3790379 -6.9153795 -8.1955252 -8.99795 -8.5655718 -7.2056785 -5.4931154][-6.1166553 -6.2328558 -7.18733 -8.2414532 -8.5586958 -8.1154079 -7.2945943 -6.9720845 -7.64543 -8.5049553 -9.0798492 -9.251894 -8.5621624 -7.1115541 -5.6126842][-5.999404 -6.0474396 -6.9795513 -7.9726338 -8.51284 -8.4819221 -8.0721455 -8.0236406 -8.5712566 -8.9724045 -9.1402 -8.9234686 -8.0959558 -6.7338943 -5.5956936]]...]
INFO - root - 2017-12-15 06:51:18.554388: step 12710, loss = 0.22, batch loss = 0.18 (34.7 examples/sec; 0.231 sec/batch; 20h:29m:41s remains)
INFO - root - 2017-12-15 06:51:20.844873: step 12720, loss = 0.19, batch loss = 0.16 (35.6 examples/sec; 0.225 sec/batch; 19h:56m:50s remains)
INFO - root - 2017-12-15 06:51:23.115876: step 12730, loss = 0.24, batch loss = 0.21 (33.7 examples/sec; 0.238 sec/batch; 21h:06m:12s remains)
INFO - root - 2017-12-15 06:51:25.414581: step 12740, loss = 0.33, batch loss = 0.30 (36.4 examples/sec; 0.220 sec/batch; 19h:31m:54s remains)
INFO - root - 2017-12-15 06:51:27.674170: step 12750, loss = 0.26, batch loss = 0.23 (35.5 examples/sec; 0.225 sec/batch; 19h:59m:29s remains)
INFO - root - 2017-12-15 06:51:29.950653: step 12760, loss = 0.46, batch loss = 0.43 (35.3 examples/sec; 0.227 sec/batch; 20h:08m:10s remains)
INFO - root - 2017-12-15 06:51:32.251276: step 12770, loss = 0.20, batch loss = 0.17 (34.3 examples/sec; 0.233 sec/batch; 20h:42m:38s remains)
INFO - root - 2017-12-15 06:51:34.566398: step 12780, loss = 0.31, batch loss = 0.28 (34.4 examples/sec; 0.232 sec/batch; 20h:38m:45s remains)
INFO - root - 2017-12-15 06:51:36.848729: step 12790, loss = 0.29, batch loss = 0.26 (34.5 examples/sec; 0.232 sec/batch; 20h:35m:31s remains)
INFO - root - 2017-12-15 06:51:39.144286: step 12800, loss = 0.34, batch loss = 0.31 (32.8 examples/sec; 0.244 sec/batch; 21h:38m:51s remains)
2017-12-15 06:51:39.530791: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7570121 -4.5924015 -4.9981537 -5.5818596 -5.8564582 -6.0506573 -6.1175585 -6.137784 -6.5549936 -6.7779284 -7.119319 -7.0107508 -6.5816298 -5.9867716 -5.7314548][-3.5419652 -4.7187662 -5.1721067 -5.4508505 -5.3610334 -5.301384 -5.2101946 -4.8490505 -5.234097 -5.7936478 -6.5885992 -6.988905 -6.5547771 -5.6265497 -4.9430909][-4.4880767 -4.8932872 -5.2636156 -5.1413603 -4.6904325 -4.4055896 -4.1439261 -3.4655983 -3.6513538 -4.5057831 -5.794714 -6.6623936 -6.4135752 -5.3452353 -4.34319][-5.5638304 -5.3778782 -5.5638356 -5.040597 -4.2114568 -3.5874386 -3.0350282 -1.9876068 -1.7843332 -2.8403597 -4.6353059 -6.0195289 -6.117166 -5.1855617 -3.9800076][-6.7208309 -5.6861982 -5.5348434 -4.6646061 -3.4302306 -2.3672006 -1.4357507 -0.045345068 0.57108164 -0.60700476 -2.8346527 -4.820117 -5.4239569 -4.8690529 -3.6275668][-7.2338772 -5.680831 -5.1291208 -3.8924313 -2.1093903 -0.4020015 1.0959668 2.8109257 3.7294595 2.3049796 -0.45793593 -3.0462937 -4.2844634 -4.1228137 -3.0157988][-7.143796 -5.4438362 -4.6532297 -3.1090312 -0.72723532 1.7169988 3.8671172 5.7687292 6.7133961 4.8914051 1.6014845 -1.53192 -3.2101507 -3.3804626 -2.5223517][-7.0658875 -5.0608997 -4.4478674 -2.90155 -0.2216301 2.6900265 5.3921576 7.4360771 8.1888714 5.9609852 2.323817 -1.0762862 -2.9255984 -3.1926651 -2.4878592][-6.7326326 -4.7113485 -4.5030928 -3.2942402 -0.84367526 1.9556239 4.7395649 6.7885151 7.3595524 5.0837955 1.6167719 -1.5345204 -3.2534711 -3.4966989 -2.9361324][-6.1345134 -4.3243475 -4.659296 -4.0178986 -2.2229207 -0.051289558 2.4106996 4.2318544 4.6613417 2.7826884 -0.028342009 -2.5189617 -3.8766065 -4.0811977 -3.6720123][-5.479012 -3.9657416 -4.6871128 -4.681026 -3.7873163 -2.5355151 -0.72100389 0.80355644 1.282414 0.08353281 -1.8661171 -3.6065369 -4.5541487 -4.5959654 -4.2222505][-4.9402208 -3.6059384 -4.4443502 -4.8520584 -4.7523303 -4.2983155 -3.177489 -2.0447028 -1.5369655 -2.0893354 -3.2943788 -4.4311848 -5.0596533 -4.8910928 -4.5045443][-4.6016855 -3.3425741 -4.1376276 -4.76663 -5.1674948 -5.2705436 -4.7099733 -4.0031867 -3.6112475 -3.8199906 -4.5528336 -5.2612562 -5.5841842 -5.2833347 -4.8807917][-5.04691 -3.8196926 -4.4954338 -5.1157417 -5.7034335 -6.0778894 -5.932497 -5.6323156 -5.4164238 -5.4867225 -5.9574633 -6.295023 -6.2358503 -5.7008524 -5.2040071][-5.9547949 -4.7093706 -5.1673365 -5.6318493 -6.2213182 -6.7028322 -6.8452826 -6.8520374 -6.7977629 -6.8508635 -7.0808449 -7.1119595 -6.6964512 -5.9473143 -5.3453369]]...]
INFO - root - 2017-12-15 06:51:41.783430: step 12810, loss = 0.22, batch loss = 0.19 (35.5 examples/sec; 0.226 sec/batch; 20h:02m:14s remains)
INFO - root - 2017-12-15 06:51:44.123923: step 12820, loss = 0.26, batch loss = 0.23 (33.8 examples/sec; 0.237 sec/batch; 21h:02m:51s remains)
INFO - root - 2017-12-15 06:51:46.417792: step 12830, loss = 0.23, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 19h:45m:23s remains)
INFO - root - 2017-12-15 06:51:48.680428: step 12840, loss = 0.25, batch loss = 0.22 (35.1 examples/sec; 0.228 sec/batch; 20h:13m:12s remains)
INFO - root - 2017-12-15 06:51:50.988255: step 12850, loss = 0.33, batch loss = 0.29 (36.1 examples/sec; 0.222 sec/batch; 19h:41m:22s remains)
INFO - root - 2017-12-15 06:51:53.266114: step 12860, loss = 0.20, batch loss = 0.17 (36.5 examples/sec; 0.219 sec/batch; 19h:27m:50s remains)
INFO - root - 2017-12-15 06:51:55.545960: step 12870, loss = 0.25, batch loss = 0.21 (35.3 examples/sec; 0.227 sec/batch; 20h:07m:35s remains)
INFO - root - 2017-12-15 06:51:57.827778: step 12880, loss = 0.29, batch loss = 0.26 (35.0 examples/sec; 0.229 sec/batch; 20h:18m:38s remains)
INFO - root - 2017-12-15 06:52:00.115548: step 12890, loss = 0.21, batch loss = 0.17 (34.5 examples/sec; 0.232 sec/batch; 20h:34m:29s remains)
INFO - root - 2017-12-15 06:52:02.405007: step 12900, loss = 0.24, batch loss = 0.20 (35.4 examples/sec; 0.226 sec/batch; 20h:03m:06s remains)
2017-12-15 06:52:02.732036: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.934505 -6.5152369 -7.7127895 -8.5451269 -9.0173044 -8.9825287 -8.3657169 -7.5528526 -6.9475193 -6.9572134 -7.1063948 -6.9345875 -6.6648226 -6.0511684 -5.612731][-3.9259157 -5.9422584 -7.4229174 -8.5006924 -9.0404892 -8.8080168 -7.7604971 -6.4287748 -5.4657288 -5.5705366 -5.9869242 -5.9737358 -5.8284807 -5.0798488 -4.3690186][-3.8409762 -4.9137192 -6.5252628 -7.733717 -8.1634111 -7.58416 -6.0691595 -4.3662004 -3.2634912 -3.7222967 -4.7024212 -5.1718922 -5.3435583 -4.5333424 -3.5291514][-3.3180137 -3.6485658 -5.3608966 -6.4913378 -6.495532 -5.3211427 -3.3478782 -1.3468132 -0.21191549 -1.1734531 -3.010684 -4.2234592 -4.8187122 -4.100934 -2.9499073][-2.6996841 -2.59875 -4.2014966 -5.1420379 -4.6169415 -2.8509111 -0.50196564 1.8284996 3.109051 1.7121198 -1.0227356 -3.0834072 -4.2184076 -3.7281442 -2.6219633][-2.5372207 -2.0518816 -3.5833678 -4.3000937 -3.3561254 -1.1955585 1.4319916 4.1192875 5.6315908 4.0167379 0.55887175 -2.2366109 -3.82958 -3.6763048 -2.8462925][-2.4973133 -1.902477 -3.1636698 -3.7723026 -2.7674713 -0.46054721 2.2741544 5.0254593 6.5711823 4.8619471 1.1136761 -2.01932 -3.8306534 -3.93742 -3.3879564][-2.8475139 -1.8564212 -2.9236531 -3.4998143 -2.5900667 -0.61477304 1.6649721 3.9860208 5.3399143 3.8051736 0.36398053 -2.4941783 -4.0995092 -4.3458176 -4.022387][-2.9592347 -1.7396511 -2.7107728 -3.2979484 -2.7774587 -1.4988233 0.12750483 1.9446023 3.1366313 2.0405867 -0.67156839 -2.9886203 -4.2589688 -4.589344 -4.3814058][-2.8726714 -1.5338283 -2.4421952 -3.0428576 -2.8615541 -2.1762137 -1.2120492 0.10618043 1.2068028 0.58006549 -1.2781209 -2.9016256 -3.7700195 -4.0991406 -4.02293][-2.8565743 -1.3853009 -2.147325 -2.6476436 -2.6805239 -2.4351315 -2.0192127 -1.1719602 -0.38204265 -0.79778969 -1.9638608 -2.9901381 -3.4777331 -3.7716122 -3.7460511][-2.9384007 -1.356707 -1.7243479 -1.9331409 -2.0551908 -2.0686297 -2.036545 -1.7380878 -1.3905166 -1.7908199 -2.4302304 -2.9351258 -3.2070229 -3.4108272 -3.4091067][-2.8849289 -1.0978297 -1.0391563 -1.0101925 -1.2826025 -1.5604877 -1.8528967 -1.9635605 -1.9990655 -2.3329897 -2.5270569 -2.6218419 -2.6089559 -2.7475955 -2.7374487][-2.3712749 -0.38081098 -0.098381042 -0.13903356 -0.7407465 -1.4289987 -2.0623987 -2.3901751 -2.4739742 -2.5948565 -2.4909744 -2.315156 -2.2042911 -2.3883593 -2.456444][-1.5558581 0.76735568 1.1616452 0.825727 -0.322775 -1.5247122 -2.4072587 -2.6309149 -2.5003498 -2.4008861 -2.2180274 -2.0937698 -1.9900072 -2.2678056 -2.4643865]]...]
INFO - root - 2017-12-15 06:52:05.020760: step 12910, loss = 0.24, batch loss = 0.21 (35.5 examples/sec; 0.226 sec/batch; 20h:01m:21s remains)
INFO - root - 2017-12-15 06:52:07.333159: step 12920, loss = 0.22, batch loss = 0.19 (34.8 examples/sec; 0.230 sec/batch; 20h:25m:00s remains)
INFO - root - 2017-12-15 06:52:09.595417: step 12930, loss = 0.19, batch loss = 0.16 (35.9 examples/sec; 0.223 sec/batch; 19h:46m:27s remains)
INFO - root - 2017-12-15 06:52:11.851684: step 12940, loss = 0.19, batch loss = 0.16 (36.6 examples/sec; 0.218 sec/batch; 19h:22m:47s remains)
INFO - root - 2017-12-15 06:52:14.114065: step 12950, loss = 0.31, batch loss = 0.28 (36.9 examples/sec; 0.217 sec/batch; 19h:15m:50s remains)
INFO - root - 2017-12-15 06:52:16.408875: step 12960, loss = 0.38, batch loss = 0.34 (36.2 examples/sec; 0.221 sec/batch; 19h:35m:59s remains)
INFO - root - 2017-12-15 06:52:18.678253: step 12970, loss = 0.29, batch loss = 0.25 (35.6 examples/sec; 0.225 sec/batch; 19h:57m:04s remains)
INFO - root - 2017-12-15 06:52:20.999012: step 12980, loss = 0.17, batch loss = 0.13 (35.6 examples/sec; 0.225 sec/batch; 19h:56m:33s remains)
INFO - root - 2017-12-15 06:52:23.271852: step 12990, loss = 0.26, batch loss = 0.23 (34.5 examples/sec; 0.232 sec/batch; 20h:36m:22s remains)
INFO - root - 2017-12-15 06:52:25.556590: step 13000, loss = 0.28, batch loss = 0.24 (35.1 examples/sec; 0.228 sec/batch; 20h:14m:50s remains)
2017-12-15 06:52:25.931003: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7462234 -7.5160604 -6.1699181 -4.557044 -4.1527042 -3.9111028 -3.0086489 -2.7963824 -3.1782687 -3.67478 -3.6367912 -3.3889315 -3.6126537 -4.0276895 -4.4456897][-7.6909561 -6.9586596 -5.2625427 -3.5434785 -3.1233144 -2.9321742 -2.3741133 -2.5384786 -2.9881861 -3.5051072 -3.3736191 -2.7712011 -2.7317116 -2.8533332 -2.8207209][-8.2057533 -6.1198378 -4.3347073 -2.8504658 -2.5432203 -2.4120774 -2.1539617 -2.6032412 -3.18083 -3.6725821 -3.3718848 -2.470474 -1.9899207 -1.6086676 -1.1932144][-7.6794624 -4.7039576 -2.8887186 -1.4850466 -0.99324119 -0.68497455 -0.46683514 -1.1192245 -1.9781847 -2.5067897 -2.1342995 -0.94017756 0.040481806 0.64422178 1.01034][-7.9049964 -4.2912908 -2.6370063 -1.1648321 -0.26733005 0.489892 0.96557975 0.39823985 -0.53138018 -1.0507976 -0.7104491 0.56706905 1.7435105 2.0162058 1.6914623][-8.0958061 -4.4781847 -3.0871468 -1.5087527 -0.1164012 1.1651425 2.0192852 1.714098 0.83359408 0.17262888 0.37113762 1.5016992 2.5172439 2.3820772 1.4298158][-7.5803337 -4.6716771 -3.4686937 -1.6581975 0.35452318 2.2412539 3.4211507 3.4058228 2.6734986 1.9480462 1.9228549 2.597753 3.1192679 2.6146731 1.2516882][-8.12097 -5.3616014 -4.41796 -2.676378 -0.38926554 1.874959 3.4109726 3.8958335 3.3743172 2.546576 2.3916459 2.7786579 2.9004064 2.1318192 0.63660765][-8.3987064 -5.9148726 -5.3076725 -4.106771 -2.1457605 0.13163805 1.8498759 2.6066608 2.1191669 1.4621058 1.5456889 1.6928763 1.3686106 0.53897238 -0.65102565][-8.3123455 -6.2062325 -6.0252156 -5.5613937 -4.3267565 -2.5455954 -1.0732573 -0.43543863 -0.87732542 -1.1092871 -0.66891015 -0.65956104 -1.3135793 -2.0055542 -2.6838756][-8.805747 -7.1902242 -7.3143511 -7.3931823 -6.95679 -5.9688082 -4.9853678 -4.5074558 -4.7037497 -4.5197945 -3.9927254 -4.1592522 -4.9038215 -5.2707047 -5.4568996][-8.3918362 -7.0645561 -7.2455387 -7.6101217 -7.8088126 -7.5461693 -7.013001 -6.6267662 -6.5732584 -6.2258921 -5.8840637 -6.2553945 -6.9575286 -7.0188847 -6.8369541][-6.9531913 -5.7715034 -5.9941778 -6.52408 -7.0466352 -7.1863003 -7.0385523 -6.8590479 -6.6560941 -6.1502652 -5.8620796 -6.2876205 -6.8337216 -6.6967454 -6.4876904][-6.1215849 -5.1082907 -5.4383364 -5.9648776 -6.4225769 -6.6504245 -6.7072358 -6.7226496 -6.5411491 -6.1575241 -5.9039 -6.037787 -6.168335 -5.9996591 -6.0537119][-5.8896871 -4.8696995 -5.0792956 -5.3266096 -5.5050831 -5.5941887 -5.6828003 -5.827404 -5.8489151 -5.6964855 -5.5110273 -5.4270983 -5.3554926 -5.2812605 -5.4062457]]...]
INFO - root - 2017-12-15 06:52:28.213044: step 13010, loss = 0.18, batch loss = 0.15 (35.2 examples/sec; 0.227 sec/batch; 20h:11m:15s remains)
INFO - root - 2017-12-15 06:52:30.480068: step 13020, loss = 0.20, batch loss = 0.17 (35.8 examples/sec; 0.224 sec/batch; 19h:50m:32s remains)
INFO - root - 2017-12-15 06:52:32.741849: step 13030, loss = 0.25, batch loss = 0.22 (35.4 examples/sec; 0.226 sec/batch; 20h:03m:21s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 06:52:35.034622: step 13040, loss = 0.20, batch loss = 0.17 (33.6 examples/sec; 0.238 sec/batch; 21h:08m:10s remains)
INFO - root - 2017-12-15 06:52:37.333550: step 13050, loss = 0.31, batch loss = 0.27 (34.5 examples/sec; 0.232 sec/batch; 20h:35m:42s remains)
INFO - root - 2017-12-15 06:52:39.643440: step 13060, loss = 0.25, batch loss = 0.21 (36.1 examples/sec; 0.221 sec/batch; 19h:38m:43s remains)
INFO - root - 2017-12-15 06:52:41.868409: step 13070, loss = 0.20, batch loss = 0.16 (36.6 examples/sec; 0.218 sec/batch; 19h:22m:09s remains)
INFO - root - 2017-12-15 06:52:44.186132: step 13080, loss = 0.18, batch loss = 0.14 (32.9 examples/sec; 0.243 sec/batch; 21h:35m:35s remains)
INFO - root - 2017-12-15 06:52:46.470907: step 13090, loss = 0.22, batch loss = 0.19 (36.4 examples/sec; 0.220 sec/batch; 19h:29m:06s remains)
INFO - root - 2017-12-15 06:52:48.710501: step 13100, loss = 0.24, batch loss = 0.20 (34.3 examples/sec; 0.234 sec/batch; 20h:43m:06s remains)
2017-12-15 06:52:49.049526: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8425884 -6.7654886 -7.2084894 -7.4115734 -6.8732023 -6.1702585 -5.5452251 -5.13535 -5.1257381 -5.3786211 -5.6233668 -6.2681937 -6.60533 -6.5199347 -5.6420512][-5.8647356 -7.4148946 -7.8282886 -7.6072373 -6.755146 -5.8403358 -4.9205065 -4.6392479 -5.044951 -5.5643706 -5.6910305 -6.1295519 -6.4134569 -6.3470488 -5.3125968][-7.0825381 -7.818851 -7.9981875 -7.2854681 -6.1026 -4.7820859 -3.6216741 -3.6409333 -4.5920658 -5.61082 -5.7913952 -5.8382635 -5.8236995 -5.6470232 -4.6640921][-8.0695267 -7.8006115 -7.5765629 -6.2970128 -4.5866933 -2.7664673 -1.4815693 -1.879202 -3.3119655 -4.8765955 -5.3957071 -5.335928 -5.1163354 -4.8481131 -4.0850329][-8.5440521 -7.1786323 -6.2326159 -4.121748 -1.612479 0.56619072 1.5669451 0.64174724 -1.2312806 -3.1959381 -4.2158885 -4.2410769 -3.951972 -3.5676014 -3.06104][-8.4635372 -6.3258076 -4.5874171 -1.7889373 1.3482709 3.6902246 4.3843794 3.2102337 1.1535022 -1.1040448 -2.6814094 -3.0117643 -2.8459361 -2.4249742 -2.1363783][-7.8604403 -5.4926138 -3.3921456 -0.36914289 2.974329 5.2490454 5.8404036 4.6926088 2.6177921 0.10560131 -2.0072575 -2.7775903 -2.8647408 -2.5679274 -2.1957538][-7.6284065 -5.3477955 -3.2575212 -0.26163936 2.8830767 4.9070573 5.3654394 4.2024612 2.1215639 -0.36968124 -2.6958671 -3.729733 -3.9706473 -3.6677978 -2.8541372][-7.9400291 -6.0797024 -4.1082649 -1.2658423 1.3618021 2.9535599 3.2106547 2.0566254 0.24290514 -1.7363915 -3.6593132 -4.7522707 -5.2017069 -4.973382 -3.8628845][-8.253191 -7.0777969 -5.4986005 -3.0731227 -1.064304 0.2033391 0.54795671 -0.32157195 -1.611026 -2.8184125 -4.2058744 -5.4557962 -6.1946859 -6.094924 -5.0056729][-8.4746923 -8.0110416 -6.9879208 -5.1160927 -3.6111469 -2.438477 -1.972066 -2.4175146 -3.1583536 -3.6657245 -4.5650253 -5.8358293 -6.6270704 -6.5898681 -5.8629231][-8.9492359 -8.9989824 -8.4515381 -7.091445 -5.8012886 -4.5093727 -3.9570389 -3.9699917 -4.0523872 -4.0296926 -4.5120153 -5.5582647 -6.3172536 -6.5820394 -6.603951][-9.3046551 -9.62455 -9.556118 -8.7043867 -7.5043106 -6.1478624 -5.3359766 -4.76219 -4.2096348 -3.9759393 -4.4072595 -5.4182673 -6.2428217 -6.74274 -7.1394672][-8.7578812 -9.1211929 -9.503644 -9.174634 -8.2190742 -7.0152297 -6.0577021 -5.0981331 -4.2740173 -4.0346837 -4.4927521 -5.5430593 -6.4066367 -6.8459859 -7.0643425][-7.1674013 -7.3716788 -7.9825373 -8.0431681 -7.4336033 -6.6110487 -5.7745047 -4.8309536 -3.929044 -3.6352582 -4.0383511 -5.052515 -5.988245 -6.3498907 -6.0539665]]...]
INFO - root - 2017-12-15 06:52:51.355377: step 13110, loss = 0.30, batch loss = 0.27 (33.9 examples/sec; 0.236 sec/batch; 20h:55m:22s remains)
INFO - root - 2017-12-15 06:52:53.662937: step 13120, loss = 0.25, batch loss = 0.22 (33.9 examples/sec; 0.236 sec/batch; 20h:54m:53s remains)
INFO - root - 2017-12-15 06:52:55.927520: step 13130, loss = 0.23, batch loss = 0.20 (35.3 examples/sec; 0.227 sec/batch; 20h:06m:46s remains)
INFO - root - 2017-12-15 06:52:58.208994: step 13140, loss = 0.43, batch loss = 0.40 (32.3 examples/sec; 0.247 sec/batch; 21h:56m:20s remains)
INFO - root - 2017-12-15 06:53:00.495716: step 13150, loss = 0.26, batch loss = 0.23 (35.5 examples/sec; 0.225 sec/batch; 19h:58m:04s remains)
INFO - root - 2017-12-15 06:53:02.812124: step 13160, loss = 0.25, batch loss = 0.22 (34.2 examples/sec; 0.234 sec/batch; 20h:44m:43s remains)
INFO - root - 2017-12-15 06:53:05.121717: step 13170, loss = 0.61, batch loss = 0.58 (35.6 examples/sec; 0.224 sec/batch; 19h:54m:39s remains)
INFO - root - 2017-12-15 06:53:07.408632: step 13180, loss = 0.28, batch loss = 0.24 (34.4 examples/sec; 0.233 sec/batch; 20h:37m:43s remains)
INFO - root - 2017-12-15 06:53:09.708382: step 13190, loss = 0.18, batch loss = 0.14 (35.7 examples/sec; 0.224 sec/batch; 19h:54m:07s remains)
INFO - root - 2017-12-15 06:53:11.980580: step 13200, loss = 0.24, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 19h:42m:41s remains)
2017-12-15 06:53:12.320049: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8678792 -5.448875 -5.4450541 -5.33008 -5.0957251 -4.8669329 -4.6903038 -4.5267577 -4.5281057 -4.7160406 -4.8992052 -4.9745903 -4.8469605 -4.6294188 -4.4116015][-3.6337891 -5.31222 -5.797781 -5.8655725 -5.7082634 -5.5082231 -5.2850385 -5.1270657 -5.2594986 -5.6308546 -5.9375153 -5.9787946 -5.703455 -5.3244019 -5.0242672][-3.0220807 -4.5207891 -5.5012493 -5.6596956 -5.3343983 -4.8991003 -4.4354553 -4.2982683 -4.7284355 -5.4032574 -5.9538555 -6.1277061 -5.8628297 -5.4514484 -5.1569276][-3.0706162 -4.1026974 -5.0336342 -4.9583044 -4.1732221 -3.2946463 -2.484659 -2.3902845 -3.1800976 -4.3038254 -5.2076969 -5.6403947 -5.5834832 -5.3288536 -5.1091948][-4.0451202 -4.4263725 -4.7911949 -4.1700315 -2.7456324 -1.3541334 -0.32747293 -0.35250771 -1.4822513 -2.9849472 -4.1866541 -4.9557257 -5.3218241 -5.4006963 -5.2113018][-5.1471691 -4.7379408 -4.2561455 -3.0244241 -1.1078973 0.58265257 1.6498489 1.6422839 0.50819468 -1.3146306 -2.913846 -4.1051607 -5.0562959 -5.6854281 -5.6463108][-5.1120844 -4.36383 -3.3951883 -1.9004751 0.16973948 1.9558954 2.9944406 3.1379051 2.097126 0.05829072 -1.8225821 -3.3387506 -4.8142357 -6.1816092 -6.5122662][-4.3867579 -3.51461 -2.6576364 -1.3759142 0.42433071 1.9578962 2.7342443 3.0160341 2.2663202 0.23961592 -1.8082705 -3.5613647 -5.2880435 -6.9638529 -7.4721928][-2.8138528 -2.1648636 -2.1178489 -1.5455821 -0.35768819 0.60447073 1.068994 1.5066819 0.94745541 -0.92496586 -2.8897104 -4.5651555 -5.9834185 -7.2445045 -7.4334474][-1.0220034 -0.39478707 -1.1564336 -1.453793 -1.1024324 -0.86688507 -0.71799016 -0.10476375 -0.25107467 -1.8087314 -3.590081 -5.0792875 -5.96446 -6.4931412 -6.2033668][0.41236353 1.0692821 -0.44641936 -1.5404177 -1.92438 -2.2694097 -2.2097249 -1.2915778 -0.93742585 -2.0124779 -3.443265 -4.7741351 -5.1305828 -4.9245896 -4.1121697][0.46530271 1.1129158 -0.64363551 -2.0370908 -2.7945447 -3.3071253 -2.9046931 -1.5129957 -0.64026666 -1.2387179 -2.4635446 -3.6518645 -3.6227798 -2.8868585 -1.8453805][-0.69764721 0.10133243 -1.4750659 -2.7209969 -3.4125 -3.7094364 -2.7361033 -0.872282 0.38147306 0.16586208 -0.78136432 -1.868573 -1.7845025 -1.1387676 -0.38418365][-2.4981508 -1.7053825 -2.8614383 -3.5913069 -4.0545034 -4.0521412 -2.4624014 -0.23078394 1.2305214 1.4657781 0.764683 -0.54247105 -0.89736211 -0.66456771 -0.33986104][-4.3928595 -3.7250552 -4.2609062 -4.4570732 -4.6335468 -4.374229 -2.477138 -0.24849904 0.99454761 1.3616605 0.87753248 -0.53132629 -1.2989547 -1.5029666 -1.5121779]]...]
INFO - root - 2017-12-15 06:53:14.579464: step 13210, loss = 0.18, batch loss = 0.15 (36.1 examples/sec; 0.222 sec/batch; 19h:39m:36s remains)
INFO - root - 2017-12-15 06:53:16.889455: step 13220, loss = 0.35, batch loss = 0.32 (31.8 examples/sec; 0.252 sec/batch; 22h:18m:43s remains)
INFO - root - 2017-12-15 06:53:19.227060: step 13230, loss = 0.21, batch loss = 0.18 (31.3 examples/sec; 0.256 sec/batch; 22h:41m:07s remains)
INFO - root - 2017-12-15 06:53:21.515903: step 13240, loss = 0.26, batch loss = 0.23 (35.8 examples/sec; 0.224 sec/batch; 19h:49m:15s remains)
INFO - root - 2017-12-15 06:53:23.809831: step 13250, loss = 0.30, batch loss = 0.27 (33.9 examples/sec; 0.236 sec/batch; 20h:55m:03s remains)
INFO - root - 2017-12-15 06:53:26.075732: step 13260, loss = 0.24, batch loss = 0.21 (36.0 examples/sec; 0.222 sec/batch; 19h:41m:48s remains)
INFO - root - 2017-12-15 06:53:28.413687: step 13270, loss = 0.26, batch loss = 0.23 (33.4 examples/sec; 0.239 sec/batch; 21h:13m:50s remains)
INFO - root - 2017-12-15 06:53:30.694057: step 13280, loss = 0.26, batch loss = 0.22 (36.5 examples/sec; 0.219 sec/batch; 19h:27m:28s remains)
INFO - root - 2017-12-15 06:53:32.975538: step 13290, loss = 0.25, batch loss = 0.22 (36.5 examples/sec; 0.219 sec/batch; 19h:25m:55s remains)
INFO - root - 2017-12-15 06:53:35.234344: step 13300, loss = 0.31, batch loss = 0.28 (36.0 examples/sec; 0.222 sec/batch; 19h:42m:20s remains)
2017-12-15 06:53:35.513536: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.296365 -1.8790991 -2.6256685 -3.3863351 -4.2596221 -4.7133675 -5.4147716 -5.8368011 -6.0398364 -5.5439177 -4.5349112 -4.3361511 -4.45453 -4.9587393 -5.5416927][-4.1962266 -2.8114548 -3.4768894 -3.9952664 -4.2548656 -4.225769 -4.7752542 -5.3266468 -5.6601796 -5.1517506 -4.0240846 -3.7285991 -3.6560998 -4.3556504 -5.2091331][-5.8890676 -3.6948915 -4.1061583 -4.0549831 -3.6636138 -3.2537551 -3.8413918 -4.7062144 -5.2239904 -4.7359476 -3.2986789 -2.6204081 -2.2739544 -3.1943154 -4.4459372][-5.85341 -3.45337 -3.4176972 -2.9012232 -2.0613153 -1.2986124 -1.6978675 -2.7788205 -3.6874366 -3.5956802 -2.382432 -1.659256 -1.0923133 -1.9189514 -3.417413][-5.2502441 -3.0301006 -2.6460152 -1.8811778 -0.75526035 0.6020937 1.0520906 0.35054135 -0.91790843 -1.6473393 -1.2492828 -0.90137482 -0.44032705 -1.2234358 -2.9356925][-5.4822588 -3.1257055 -2.348527 -1.3427869 -0.032901287 1.8287036 3.3500607 3.3936565 1.8492591 0.34495807 -0.067374468 -0.38486767 -0.44812584 -1.4868476 -3.402864][-5.4581704 -3.2110705 -1.9075792 -0.45909059 1.1034036 3.3649986 5.879921 6.7012453 4.9478645 2.7350667 1.4279666 0.20953679 -0.64565361 -2.1388252 -4.1242309][-6.843646 -4.5082273 -2.9138422 -1.0203668 0.90697503 3.4984524 6.6259117 8.2026539 6.7375107 4.3473816 2.3253706 0.29027271 -1.3693718 -3.1910057 -5.0166578][-9.3074942 -7.092989 -5.5185852 -3.4603362 -1.4492434 0.86193633 3.6647913 5.448266 4.7863111 3.0457914 1.0908201 -1.0668851 -3.1306469 -4.9530468 -6.3521318][-10.757011 -8.6683645 -7.3057652 -5.433496 -3.6222968 -1.8895512 -0.10424089 1.255717 1.3087797 0.46544075 -1.08001 -3.0859745 -5.009531 -6.46352 -7.2677288][-11.298412 -9.57015 -8.6620827 -7.2239285 -5.742373 -4.4847536 -3.3719244 -2.4585381 -2.0392568 -2.3185182 -3.4131293 -5.0052023 -6.561017 -7.58202 -7.977458][-11.905925 -10.77588 -10.636053 -9.954257 -9.0552835 -8.2015657 -7.4517603 -6.7827148 -6.2604866 -6.0735731 -6.4133396 -7.231245 -8.1284008 -8.6119881 -8.6442509][-11.149804 -10.416907 -10.806371 -10.792693 -10.516519 -10.198007 -9.7653685 -9.2691345 -8.8298759 -8.3904324 -8.1130219 -8.151103 -8.3670034 -8.4325581 -8.1900072][-8.8751287 -8.2772579 -8.8087244 -9.0464544 -9.092308 -9.0452394 -8.8361483 -8.5667944 -8.3951635 -8.0956583 -7.6096649 -7.1685648 -6.9648409 -6.8627615 -6.5769253][-6.4863243 -5.886198 -6.3254347 -6.6031837 -6.7005491 -6.7083788 -6.6437368 -6.6076946 -6.6713228 -6.5527849 -6.118331 -5.5786247 -5.2688437 -5.1532388 -4.9785662]]...]
INFO - root - 2017-12-15 06:53:37.816906: step 13310, loss = 0.24, batch loss = 0.20 (35.7 examples/sec; 0.224 sec/batch; 19h:53m:02s remains)
INFO - root - 2017-12-15 06:53:40.130780: step 13320, loss = 0.25, batch loss = 0.21 (36.3 examples/sec; 0.221 sec/batch; 19h:33m:15s remains)
INFO - root - 2017-12-15 06:53:42.423141: step 13330, loss = 0.39, batch loss = 0.35 (35.4 examples/sec; 0.226 sec/batch; 20h:03m:38s remains)
INFO - root - 2017-12-15 06:53:44.720041: step 13340, loss = 0.22, batch loss = 0.18 (35.2 examples/sec; 0.228 sec/batch; 20h:10m:20s remains)
INFO - root - 2017-12-15 06:53:46.995510: step 13350, loss = 0.32, batch loss = 0.29 (34.7 examples/sec; 0.231 sec/batch; 20h:28m:00s remains)
INFO - root - 2017-12-15 06:53:49.285181: step 13360, loss = 0.30, batch loss = 0.26 (34.6 examples/sec; 0.231 sec/batch; 20h:30m:29s remains)
INFO - root - 2017-12-15 06:53:51.561245: step 13370, loss = 0.26, batch loss = 0.23 (35.8 examples/sec; 0.223 sec/batch; 19h:47m:21s remains)
INFO - root - 2017-12-15 06:53:53.868735: step 13380, loss = 0.33, batch loss = 0.29 (35.0 examples/sec; 0.228 sec/batch; 20h:15m:08s remains)
INFO - root - 2017-12-15 06:53:56.157900: step 13390, loss = 0.37, batch loss = 0.34 (34.3 examples/sec; 0.233 sec/batch; 20h:40m:09s remains)
INFO - root - 2017-12-15 06:53:58.398962: step 13400, loss = 0.19, batch loss = 0.15 (36.1 examples/sec; 0.222 sec/batch; 19h:39m:36s remains)
2017-12-15 06:53:58.717536: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.895126 -5.0589633 -5.7284365 -6.192359 -6.5591497 -6.3329611 -6.0931444 -5.4452887 -4.6178417 -4.0246348 -4.365037 -3.8949919 -3.4102883 -5.1325645 -6.4404221][-3.0623572 -5.4628048 -6.0633049 -6.5869513 -6.9781914 -6.6722059 -6.1311793 -5.4240122 -4.854002 -4.5263786 -4.9284096 -4.4249969 -4.2670555 -6.1355276 -7.3931236][-4.4503193 -5.6600542 -6.1658278 -6.7492237 -7.0473356 -6.6754227 -5.8420877 -4.9151082 -4.3960714 -4.2222137 -4.6545715 -4.2775507 -4.5038962 -6.4139051 -7.6631775][-5.2963991 -5.5973668 -5.9891891 -6.4069662 -6.2839842 -5.5394049 -4.1881285 -2.8906519 -2.3737772 -2.5641685 -3.4224634 -3.6383395 -4.4410362 -6.211812 -7.2257357][-5.9020634 -5.3346434 -5.6870365 -5.8055315 -5.1643186 -3.9450758 -2.1479075 -0.62058938 -0.1427319 -0.74513912 -2.0248296 -2.7665884 -3.9693036 -5.5602827 -6.436595][-5.9161415 -4.8555288 -5.0392118 -4.5455351 -3.1576693 -1.1834062 1.0841222 2.6798527 2.7904379 1.396899 -0.57129669 -1.9680357 -3.5108497 -4.9585991 -5.6627488][-5.3015828 -4.3860579 -4.2941461 -3.059314 -0.85050106 1.8989065 4.4933119 5.846343 5.2094164 2.7314212 0.061439753 -1.7763356 -3.3139653 -4.4844809 -4.9012184][-5.4225359 -4.3501892 -4.2300425 -2.6728873 -0.092373371 2.8662932 5.4347334 6.4712086 5.3975277 2.4460213 -0.35135829 -2.2144988 -3.5502491 -4.509285 -4.7488565][-5.6515427 -4.7174878 -4.8379164 -3.3489811 -0.70658576 2.1640399 4.6687841 5.6752748 4.5809278 1.4987309 -1.3890646 -3.43454 -4.6363349 -5.244946 -5.166419][-5.9508467 -5.2714949 -5.7562275 -4.5782647 -2.099467 0.41233468 2.6244004 3.5192711 2.5317848 -0.39645028 -3.1714497 -5.0434251 -5.7967105 -5.861846 -5.4704156][-6.2958336 -5.908103 -6.7804642 -6.2072411 -4.3630514 -2.669 -1.1486485 -0.56145382 -1.1453474 -3.253767 -5.2186174 -6.305645 -6.4762077 -6.2460394 -5.8056674][-6.4163332 -6.2374878 -7.355072 -7.3986568 -6.2670374 -5.2625093 -4.3209424 -3.8332343 -3.9278646 -5.1945333 -6.3787966 -6.8876543 -6.7857695 -6.4552174 -5.967031][-6.0958819 -5.9254065 -7.031888 -7.4476805 -6.8685493 -6.3235521 -5.8796148 -5.609271 -5.6456776 -6.3773279 -6.9667873 -7.0183353 -6.59869 -6.1506782 -5.6496167][-5.5337052 -5.2588968 -6.2384148 -6.9051514 -6.8516855 -6.7205477 -6.7035503 -6.7525892 -6.9044476 -7.1601753 -7.1012783 -6.6231661 -5.923254 -5.4666867 -5.0606527][-4.93618 -4.4592724 -5.1506557 -5.758153 -5.8623943 -5.7760935 -5.826931 -5.9534607 -6.2210035 -6.3274927 -6.0639639 -5.5545106 -5.0437927 -4.7203846 -4.3762035]]...]
INFO - root - 2017-12-15 06:54:01.004734: step 13410, loss = 0.23, batch loss = 0.20 (35.5 examples/sec; 0.225 sec/batch; 19h:57m:25s remains)
INFO - root - 2017-12-15 06:54:03.280307: step 13420, loss = 0.22, batch loss = 0.18 (34.6 examples/sec; 0.231 sec/batch; 20h:30m:52s remains)
INFO - root - 2017-12-15 06:54:05.583762: step 13430, loss = 0.22, batch loss = 0.19 (35.3 examples/sec; 0.226 sec/batch; 20h:03m:53s remains)
INFO - root - 2017-12-15 06:54:07.859317: step 13440, loss = 0.27, batch loss = 0.23 (34.2 examples/sec; 0.234 sec/batch; 20h:45m:38s remains)
INFO - root - 2017-12-15 06:54:10.140437: step 13450, loss = 0.20, batch loss = 0.16 (35.7 examples/sec; 0.224 sec/batch; 19h:51m:36s remains)
INFO - root - 2017-12-15 06:54:12.450047: step 13460, loss = 0.20, batch loss = 0.16 (33.0 examples/sec; 0.242 sec/batch; 21h:28m:19s remains)
INFO - root - 2017-12-15 06:54:14.728451: step 13470, loss = 0.24, batch loss = 0.21 (33.9 examples/sec; 0.236 sec/batch; 20h:53m:01s remains)
INFO - root - 2017-12-15 06:54:17.047660: step 13480, loss = 0.22, batch loss = 0.19 (35.8 examples/sec; 0.223 sec/batch; 19h:47m:14s remains)
INFO - root - 2017-12-15 06:54:19.332663: step 13490, loss = 0.19, batch loss = 0.16 (36.0 examples/sec; 0.222 sec/batch; 19h:42m:57s remains)
INFO - root - 2017-12-15 06:54:21.603044: step 13500, loss = 0.23, batch loss = 0.20 (35.6 examples/sec; 0.225 sec/batch; 19h:54m:07s remains)
2017-12-15 06:54:21.944245: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.378361 -3.0068023 -3.0727985 -3.1970532 -3.6755514 -3.7510755 -3.5709581 -3.2600875 -2.8976367 -2.5232165 -2.0389497 -1.5960262 -2.1083934 -3.0201902 -3.42331][-2.9840455 -2.931968 -2.891325 -2.9151096 -3.3383296 -3.4291759 -3.2379751 -2.8200493 -2.3514662 -2.1225333 -1.7853999 -1.4077795 -2.0088429 -3.0386732 -3.3643937][-3.276474 -2.9991193 -2.8588817 -2.7052937 -2.9700871 -2.9397612 -2.6960273 -2.1798639 -1.6391568 -1.4989203 -1.3157835 -1.1081232 -1.8985512 -3.0292535 -3.3410547][-3.3678682 -2.7213871 -2.5486963 -2.1935754 -2.0608389 -1.669821 -1.3331758 -0.89242375 -0.47339678 -0.54399741 -0.518162 -0.59360623 -1.5601633 -2.6764524 -3.0157139][-3.3496957 -2.1546762 -1.9330399 -1.4412444 -0.88622689 -0.14992189 0.15947819 0.34845686 0.5741775 0.36598849 0.27455974 0.019191742 -1.0423244 -2.1194868 -2.4975038][-2.8007381 -1.082761 -0.87475109 -0.38963878 0.4491961 1.5043061 1.8317053 1.8167479 1.8026161 1.4398603 1.2036617 0.82703829 -0.32574737 -1.5121543 -2.0001991][-2.1085913 -0.7936523 -0.562683 -0.022009134 1.0559227 2.3011439 2.6882565 2.5940359 2.3798401 1.880306 1.5203032 1.0099046 -0.28360164 -1.4784849 -2.0008283][-2.3991783 -1.0550138 -0.67563641 -0.063164949 0.87006044 1.8939741 2.1348608 1.9512064 1.789995 1.3454897 0.93675041 0.45775557 -0.79217565 -1.9645507 -2.3729594][-2.7379227 -1.3595771 -0.93988264 -0.24830627 0.37934446 0.97568417 0.893255 0.60438204 0.56973433 0.34296823 -0.01696682 -0.35767233 -1.4772141 -2.5463531 -2.7588964][-3.1225777 -1.5998176 -1.0665936 -0.31940591 -0.040773869 0.27474046 0.05916357 -0.20861328 -0.2975353 -0.44501567 -0.80669987 -1.0645978 -2.0564113 -3.083909 -3.153964][-3.1347463 -1.4711552 -0.85945868 -0.13650489 -0.14689875 0.053019524 -0.099544048 -0.32479393 -0.48465002 -0.70549035 -1.1360356 -1.458739 -2.502259 -3.5722845 -3.6188898][-3.017333 -1.4662368 -0.96326458 -0.42285955 -0.69385028 -0.4250319 -0.27721453 -0.42466271 -0.62568223 -0.84224427 -1.283167 -1.6016942 -2.6176922 -3.7334995 -3.8329558][-3.3792815 -2.0060568 -1.7362016 -1.4730506 -1.7282252 -1.3809654 -1.0742376 -1.1240973 -1.2752675 -1.4155742 -1.7336788 -1.9971477 -2.9002943 -3.9148793 -3.9473648][-3.7440867 -2.6011329 -2.5198975 -2.3742123 -2.4933562 -2.2283645 -1.9975115 -2.0636253 -2.1559319 -2.1858191 -2.3927743 -2.6466897 -3.4133837 -4.286613 -4.2107458][-3.48593 -2.5912759 -2.6930285 -2.7372866 -2.9280777 -2.9060216 -2.8804412 -3.0014517 -2.9611764 -2.8555508 -2.9185965 -3.0793812 -3.668972 -4.4119816 -4.2995205]]...]
INFO - root - 2017-12-15 06:54:24.213188: step 13510, loss = 0.21, batch loss = 0.18 (36.7 examples/sec; 0.218 sec/batch; 19h:20m:01s remains)
INFO - root - 2017-12-15 06:54:26.531332: step 13520, loss = 0.23, batch loss = 0.20 (36.2 examples/sec; 0.221 sec/batch; 19h:35m:20s remains)
INFO - root - 2017-12-15 06:54:28.795884: step 13530, loss = 0.28, batch loss = 0.25 (35.3 examples/sec; 0.227 sec/batch; 20h:05m:17s remains)
INFO - root - 2017-12-15 06:54:31.069954: step 13540, loss = 0.26, batch loss = 0.23 (35.5 examples/sec; 0.225 sec/batch; 19h:58m:34s remains)
INFO - root - 2017-12-15 06:54:33.379716: step 13550, loss = 0.27, batch loss = 0.24 (34.3 examples/sec; 0.234 sec/batch; 20h:41m:26s remains)
INFO - root - 2017-12-15 06:54:35.656745: step 13560, loss = 0.20, batch loss = 0.17 (34.9 examples/sec; 0.229 sec/batch; 20h:17m:57s remains)
INFO - root - 2017-12-15 06:54:37.944546: step 13570, loss = 0.21, batch loss = 0.18 (35.7 examples/sec; 0.224 sec/batch; 19h:50m:23s remains)
INFO - root - 2017-12-15 06:54:40.219359: step 13580, loss = 0.20, batch loss = 0.17 (33.8 examples/sec; 0.237 sec/batch; 20h:58m:01s remains)
INFO - root - 2017-12-15 06:54:42.499996: step 13590, loss = 0.24, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 19h:42m:20s remains)
INFO - root - 2017-12-15 06:54:44.767967: step 13600, loss = 0.25, batch loss = 0.22 (35.4 examples/sec; 0.226 sec/batch; 20h:00m:44s remains)
2017-12-15 06:54:45.093838: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9383855 -5.3761606 -6.0159397 -6.5622435 -7.0868435 -7.5592365 -7.5555038 -6.3369589 -5.8518209 -6.9460936 -7.3074827 -7.1458311 -8.6743679 -10.054003 -9.8035517][-2.8990657 -4.4061375 -4.985651 -5.5861187 -6.322485 -6.9731512 -7.2364216 -6.1655793 -5.57566 -6.6522741 -7.0907116 -7.0243826 -8.8010817 -10.604532 -10.571599][-2.7348623 -3.3120122 -3.6638732 -4.1059122 -4.864501 -5.46512 -5.84271 -4.974709 -4.3784771 -5.4190793 -6.0181484 -6.193841 -8.1160936 -10.317092 -10.743706][-2.0028155 -1.8694098 -1.9976442 -2.2923486 -3.0347216 -3.4506683 -3.7263861 -3.0055971 -2.4597688 -3.4761322 -4.2057467 -4.5671577 -6.3425417 -8.6808195 -9.6054277][-1.926229 -1.3673859 -1.3358018 -1.4069002 -1.9001422 -1.8751881 -1.6239803 -0.71199107 -0.1871295 -1.3516893 -2.3835354 -3.1260085 -4.7156849 -6.9981213 -8.2279243][-2.2302337 -1.4480023 -1.3783052 -1.1890795 -1.2206771 -0.66539812 0.32452083 1.6822526 1.9359467 0.21739006 -1.1549182 -2.3147244 -3.79102 -5.8212423 -7.1292505][-2.0841808 -1.2766995 -1.1986544 -0.76085663 -0.33598995 0.475657 2.0360925 3.9661639 3.7391961 1.0968504 -0.63328087 -2.1459398 -3.7226441 -5.4039669 -6.5496845][-1.3633785 -0.52770281 -0.61371803 -0.15566158 0.48554468 1.1601398 2.8231337 5.2309122 4.5142622 0.94997382 -1.0409272 -2.7651677 -4.5622091 -5.8235049 -6.5843124][-1.2055779 -0.41791832 -0.698423 -0.29339743 0.51387 1.0214226 2.4976199 5.1352329 4.1546154 0.05354166 -2.0818019 -3.7684672 -5.6703882 -6.5776806 -6.9489231][-1.9987776 -1.314198 -1.6792787 -1.3700309 -0.53691268 -0.20188892 0.85071206 3.2706406 2.3184378 -1.5783796 -3.6240017 -5.1120052 -6.920577 -7.627037 -7.6969357][-3.6935194 -3.1440701 -3.54621 -3.4024744 -2.7004056 -2.4733429 -1.8534734 0.057964087 -0.58805895 -3.5975084 -5.2763357 -6.4644651 -7.9347506 -8.4668512 -8.3741446][-5.5824337 -4.9918079 -5.2970061 -5.2636194 -4.7899585 -4.6711988 -4.3922286 -3.05151 -3.2887635 -5.1239314 -6.2502594 -7.071188 -8.0899105 -8.4627876 -8.3388472][-6.5565414 -5.8570404 -6.0420532 -6.0801907 -5.8348017 -5.7932405 -5.7232652 -4.9229741 -4.8627644 -5.6939754 -6.3452854 -6.8385878 -7.3667555 -7.5762205 -7.4720211][-6.4986811 -5.7212291 -5.8721695 -5.9731469 -5.8708572 -5.8532553 -5.9193192 -5.5878105 -5.438158 -5.6768885 -5.9750195 -6.1881227 -6.3049011 -6.3556833 -6.3115416][-5.8211555 -4.9533119 -5.0547252 -5.1384125 -5.0982413 -5.0924935 -5.1843643 -5.1035738 -4.9768896 -4.9738321 -5.0802708 -5.1870022 -5.2145653 -5.2277651 -5.1784325]]...]
INFO - root - 2017-12-15 06:54:47.388973: step 13610, loss = 0.32, batch loss = 0.29 (35.0 examples/sec; 0.228 sec/batch; 20h:13m:55s remains)
INFO - root - 2017-12-15 06:54:49.650600: step 13620, loss = 0.32, batch loss = 0.29 (35.2 examples/sec; 0.227 sec/batch; 20h:06m:35s remains)
INFO - root - 2017-12-15 06:54:51.965586: step 13630, loss = 0.17, batch loss = 0.14 (36.0 examples/sec; 0.222 sec/batch; 19h:41m:00s remains)
INFO - root - 2017-12-15 06:54:54.249232: step 13640, loss = 0.20, batch loss = 0.17 (34.5 examples/sec; 0.232 sec/batch; 20h:32m:42s remains)
INFO - root - 2017-12-15 06:54:56.581738: step 13650, loss = 0.20, batch loss = 0.16 (34.5 examples/sec; 0.232 sec/batch; 20h:33m:01s remains)
INFO - root - 2017-12-15 06:54:58.934255: step 13660, loss = 0.29, batch loss = 0.25 (34.5 examples/sec; 0.232 sec/batch; 20h:32m:43s remains)
INFO - root - 2017-12-15 06:55:01.202853: step 13670, loss = 0.24, batch loss = 0.21 (33.9 examples/sec; 0.236 sec/batch; 20h:54m:51s remains)
INFO - root - 2017-12-15 06:55:03.480067: step 13680, loss = 0.31, batch loss = 0.28 (35.5 examples/sec; 0.225 sec/batch; 19h:56m:43s remains)
INFO - root - 2017-12-15 06:55:05.754939: step 13690, loss = 0.30, batch loss = 0.27 (34.7 examples/sec; 0.230 sec/batch; 20h:24m:10s remains)
INFO - root - 2017-12-15 06:55:08.015638: step 13700, loss = 0.29, batch loss = 0.26 (35.3 examples/sec; 0.226 sec/batch; 20h:02m:42s remains)
2017-12-15 06:55:08.426234: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2375369 -4.5048022 -4.6260529 -4.543704 -4.6240406 -4.8711681 -5.2287893 -5.5457792 -6.0469851 -6.2590437 -6.2114296 -6.1908588 -5.8844318 -4.9597955 -4.1065845][-4.6264462 -5.134563 -5.090313 -4.8006406 -4.7538414 -4.9961863 -5.396349 -5.8531265 -6.602097 -7.0277538 -7.0563955 -6.9444165 -6.3588052 -5.0468817 -3.691005][-5.5396357 -5.4901247 -5.1038551 -4.49459 -4.1894364 -4.3156271 -4.7240562 -5.4265885 -6.5326076 -7.1898761 -7.2069116 -6.9669504 -6.1434484 -4.6341929 -3.0311041][-5.652976 -5.017 -4.0686336 -3.1637092 -2.7200112 -2.6836443 -2.9917009 -3.9156284 -5.3664188 -6.2816453 -6.38848 -6.2114491 -5.4028797 -3.8664207 -2.2403665][-5.0342994 -3.3539367 -1.8198266 -0.76066041 -0.40979135 -0.25605929 -0.59698308 -1.8689446 -3.6602302 -4.9262757 -5.2248716 -5.1708689 -4.5006256 -3.1141033 -1.606981][-4.3043442 -2.2812769 -0.35245883 0.76425934 1.0175903 1.4225502 1.1587417 -0.32375014 -2.2804646 -3.6867185 -4.1039557 -4.2972026 -3.9534819 -3.0036707 -1.7613256][-3.0917249 -1.5414665 0.5206604 1.6200941 1.8583426 2.5464358 2.3865623 0.8738575 -0.96906221 -2.3586512 -3.0539007 -3.6047652 -3.8100104 -3.3998575 -2.4492986][-2.3736069 -0.53190958 1.2770007 2.036458 2.0502186 2.8583508 2.8329992 1.6948287 0.37023735 -0.96490908 -2.1185756 -3.0764554 -3.6765976 -3.7459366 -3.0313418][-1.8194339 -0.1188333 1.0865452 1.3374295 1.1209869 2.0008516 2.3606868 1.9948196 1.357151 0.088854313 -1.4716928 -2.7439675 -3.5561931 -3.9840603 -3.4936695][-1.2504194 0.19278622 0.71677995 0.46640539 -0.11951399 0.45701003 1.0958235 1.5048571 1.5219138 0.62802196 -0.97250938 -2.3654094 -3.329103 -3.9615393 -3.67629][-0.8641212 0.25948286 0.16134739 -0.46735096 -1.3646663 -1.207088 -0.4611361 0.42557597 0.92548013 0.55405569 -0.69993341 -1.9144802 -2.8219259 -3.629848 -3.5387926][-1.0395898 -0.25848186 -0.64745843 -1.3859556 -2.4438288 -2.7616796 -2.1818502 -1.2135065 -0.4925921 -0.32083523 -0.95265937 -1.8637702 -2.5760028 -3.303664 -3.3396511][-1.4692305 -0.90517128 -1.3353405 -2.0864775 -3.356904 -4.2109079 -4.0066681 -3.1641769 -2.301996 -1.7604811 -1.8883038 -2.4757035 -2.9152474 -3.1766596 -3.1136594][-2.0700769 -1.7896062 -2.395869 -3.1802225 -4.5493522 -5.7651463 -5.911149 -5.248673 -4.3469524 -3.6009264 -3.2751727 -3.4206791 -3.3909514 -3.2107716 -2.9586139][-2.9206591 -2.9087944 -3.7083602 -4.554471 -5.9277906 -7.221673 -7.4701109 -6.9541349 -6.1408396 -5.2093058 -4.460907 -4.2491884 -3.8015656 -3.2413387 -2.7840803]]...]
INFO - root - 2017-12-15 06:55:10.732128: step 13710, loss = 0.17, batch loss = 0.14 (34.9 examples/sec; 0.229 sec/batch; 20h:17m:52s remains)
INFO - root - 2017-12-15 06:55:13.051219: step 13720, loss = 0.23, batch loss = 0.20 (33.9 examples/sec; 0.236 sec/batch; 20h:52m:59s remains)
INFO - root - 2017-12-15 06:55:15.349592: step 13730, loss = 0.37, batch loss = 0.33 (35.8 examples/sec; 0.223 sec/batch; 19h:45m:35s remains)
INFO - root - 2017-12-15 06:55:17.640861: step 13740, loss = 0.30, batch loss = 0.26 (35.5 examples/sec; 0.225 sec/batch; 19h:56m:48s remains)
INFO - root - 2017-12-15 06:55:19.884253: step 13750, loss = 0.22, batch loss = 0.19 (35.3 examples/sec; 0.227 sec/batch; 20h:04m:31s remains)
INFO - root - 2017-12-15 06:55:22.133441: step 13760, loss = 0.23, batch loss = 0.19 (34.0 examples/sec; 0.236 sec/batch; 20h:51m:17s remains)
INFO - root - 2017-12-15 06:55:24.426381: step 13770, loss = 0.23, batch loss = 0.20 (34.6 examples/sec; 0.231 sec/batch; 20h:26m:35s remains)
INFO - root - 2017-12-15 06:55:26.706487: step 13780, loss = 0.21, batch loss = 0.18 (36.0 examples/sec; 0.222 sec/batch; 19h:40m:49s remains)
INFO - root - 2017-12-15 06:55:29.012047: step 13790, loss = 0.23, batch loss = 0.20 (35.2 examples/sec; 0.227 sec/batch; 20h:08m:08s remains)
INFO - root - 2017-12-15 06:55:31.283673: step 13800, loss = 0.28, batch loss = 0.25 (36.1 examples/sec; 0.222 sec/batch; 19h:36m:49s remains)
2017-12-15 06:55:31.569672: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7181735 -6.6534777 -7.0716314 -7.1087089 -6.8762088 -6.3628035 -5.9506822 -6.3779488 -6.8694782 -7.0373473 -7.0670018 -7.5589638 -7.9907322 -7.7020464 -7.0875196][-7.22153 -5.91383 -6.0851703 -5.8685465 -5.5695195 -4.7712445 -4.1102591 -4.8903613 -6.0857711 -6.7189174 -6.9149461 -7.3240209 -7.7950807 -7.6181531 -7.18116][-6.7178206 -4.6616416 -4.6378832 -4.229845 -3.9337947 -2.8759425 -1.8136541 -2.7962227 -4.6595144 -5.7107043 -6.2028818 -6.7053189 -7.2774029 -7.1994481 -6.9145021][-5.936687 -3.5852063 -3.3317933 -2.7679482 -2.4658327 -1.0842702 0.55189919 -0.32156873 -2.5935485 -4.0177121 -4.998436 -5.9628043 -6.8147931 -6.8960061 -6.8000174][-5.2448387 -2.7729852 -2.1619315 -1.3424296 -0.88044 0.70233059 2.9052134 2.295815 0.030854702 -1.5374889 -2.9325194 -4.410718 -5.6091137 -5.8911552 -6.0039854][-4.7843943 -2.163306 -1.1352241 -0.10075712 0.42077851 1.9678516 4.2585626 3.7222433 1.6274045 -0.029345036 -1.6956792 -3.5579882 -5.0259366 -5.344182 -5.4723415][-4.1999865 -1.6018817 -0.20715714 0.91590261 1.3645985 2.6998086 4.7304697 4.1693087 2.4557734 0.84322333 -0.8846786 -2.8039155 -4.2964091 -4.617053 -4.798173][-4.2096772 -1.3874211 0.25101471 1.3357489 1.615447 2.5349488 3.9833603 3.2716961 1.9901071 0.66636753 -0.88992536 -2.4176486 -3.5237694 -3.8110938 -4.1058674][-4.6732225 -2.0478249 -0.4777838 0.54360938 0.66165709 1.2605081 2.1082745 1.3578336 0.78315687 0.0346632 -1.1404994 -2.1431937 -2.7827456 -3.1544185 -3.6528358][-5.69016 -3.5689125 -2.4337327 -1.6565268 -1.6182745 -1.1340809 -0.5014627 -0.91025341 -0.69343317 -0.77183282 -1.5691984 -2.2296805 -2.5439751 -3.0078633 -3.71162][-7.1869812 -5.6800756 -5.1571364 -4.6856127 -4.5721006 -3.8910222 -3.2552805 -3.4338589 -2.743089 -2.3980851 -3.0077133 -3.5023627 -3.5365384 -3.9370034 -4.7056575][-8.3749161 -7.314333 -7.2772951 -7.1058044 -6.8335133 -5.9628048 -5.3963132 -5.5315933 -4.6664457 -4.0936542 -4.4669228 -4.7589245 -4.6279225 -5.0212736 -5.9129796][-8.966114 -8.20681 -8.4965343 -8.5692167 -8.2845039 -7.4674177 -7.1476626 -7.3405418 -6.3816586 -5.6236577 -5.7170639 -5.7998629 -5.6125946 -6.0580082 -6.9878559][-8.5940113 -7.9710493 -8.4602871 -8.8454647 -8.73328 -8.2201986 -8.2917213 -8.6114407 -7.6840696 -6.7822604 -6.6173296 -6.5624285 -6.3356123 -6.6513495 -7.4162779][-7.3428297 -6.7658033 -7.3198743 -7.896451 -7.9743309 -7.7918873 -8.1360626 -8.4727249 -7.6758084 -6.7605252 -6.4164176 -6.2027254 -5.9726038 -6.2150631 -6.8010278]]...]
INFO - root - 2017-12-15 06:55:33.827119: step 13810, loss = 0.26, batch loss = 0.22 (37.3 examples/sec; 0.214 sec/batch; 18h:58m:57s remains)
INFO - root - 2017-12-15 06:55:36.082623: step 13820, loss = 0.21, batch loss = 0.17 (34.4 examples/sec; 0.233 sec/batch; 20h:35m:09s remains)
INFO - root - 2017-12-15 06:55:38.344004: step 13830, loss = 0.26, batch loss = 0.23 (34.7 examples/sec; 0.231 sec/batch; 20h:25m:54s remains)
INFO - root - 2017-12-15 06:55:40.622789: step 13840, loss = 0.23, batch loss = 0.19 (34.2 examples/sec; 0.234 sec/batch; 20h:43m:50s remains)
INFO - root - 2017-12-15 06:55:42.863290: step 13850, loss = 0.19, batch loss = 0.16 (34.4 examples/sec; 0.233 sec/batch; 20h:36m:49s remains)
INFO - root - 2017-12-15 06:55:45.156040: step 13860, loss = 0.22, batch loss = 0.18 (35.6 examples/sec; 0.225 sec/batch; 19h:54m:52s remains)
INFO - root - 2017-12-15 06:55:47.414293: step 13870, loss = 0.24, batch loss = 0.21 (35.2 examples/sec; 0.228 sec/batch; 20h:08m:38s remains)
INFO - root - 2017-12-15 06:55:49.684412: step 13880, loss = 0.21, batch loss = 0.17 (34.2 examples/sec; 0.234 sec/batch; 20h:42m:54s remains)
INFO - root - 2017-12-15 06:55:51.975312: step 13890, loss = 0.28, batch loss = 0.24 (34.6 examples/sec; 0.231 sec/batch; 20h:26m:41s remains)
INFO - root - 2017-12-15 06:55:54.278934: step 13900, loss = 0.26, batch loss = 0.23 (34.6 examples/sec; 0.231 sec/batch; 20h:27m:21s remains)
2017-12-15 06:55:54.607658: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6966043 -3.8796144 -2.9763269 -2.6052008 -2.4451923 -1.7900374 -1.501832 -1.5947003 -2.1634226 -3.0387573 -3.5079756 -3.6268704 -3.6588554 -3.5770836 -3.5088882][-6.4042788 -4.5315781 -3.5822601 -3.1248422 -2.8484325 -1.9852846 -1.5536258 -1.7743005 -2.7562985 -4.1051116 -4.9139795 -5.229248 -5.2869725 -5.0863543 -4.7623844][-7.4312754 -5.1810584 -4.1829815 -3.5403104 -3.0603352 -1.9270809 -1.3700612 -1.5609128 -2.738343 -4.4657917 -5.6217151 -6.1611671 -6.2923145 -6.0964031 -5.7175951][-8.3941956 -5.4164085 -4.2124195 -3.3193493 -2.6063833 -1.117189 -0.25035357 -0.0631094 -1.181937 -3.186533 -4.9255643 -6.0181446 -6.5473843 -6.6064777 -6.27831][-10.048635 -5.9169874 -4.3548403 -2.9928031 -1.8568851 0.2306242 1.9097321 2.851434 2.0369499 -0.092778683 -2.578378 -4.4770136 -5.654418 -6.2929249 -6.2310152][-10.863394 -6.7769117 -5.0676231 -3.4038332 -1.8480742 0.9082768 3.5391839 5.2520952 4.8045816 2.7046297 -0.39995396 -3.10976 -4.8562393 -6.0286541 -6.2217512][-10.507627 -7.0720129 -5.4270573 -3.7971206 -2.1941662 0.78932691 3.8697207 6.0903177 6.0469475 4.205945 0.99439979 -2.0601909 -4.1030483 -5.5296888 -5.7890477][-10.245683 -7.0175438 -5.5195432 -4.0698133 -2.7351267 -0.13198781 2.8398597 5.0851469 5.38573 4.0573788 1.2744563 -1.5878148 -3.537981 -4.9619017 -5.1819715][-10.48179 -7.4132891 -6.1248713 -4.9437742 -4.0141954 -2.0489905 0.3079617 2.1516378 2.6646931 1.9412997 -0.16084552 -2.62428 -4.2236547 -5.4337883 -5.6436558][-10.911003 -7.902298 -6.5957308 -5.4771814 -4.7953753 -3.4926033 -2.0626583 -1.0702924 -0.61573219 -0.914531 -2.2583404 -4.0980482 -5.3216629 -6.2693987 -6.3645916][-11.217645 -8.0015049 -6.4845314 -5.1688228 -4.5168662 -3.6398082 -3.2056341 -3.3185153 -3.2082582 -3.4743733 -4.2935328 -5.5000439 -6.2969332 -6.7705956 -6.5662565][-11.961986 -8.4493656 -6.6063747 -4.9936152 -4.141748 -3.4723897 -3.9152145 -4.85273 -5.0375967 -5.3455696 -5.8851914 -6.6566963 -7.0388193 -7.0875807 -6.7197704][-12.880744 -9.1819506 -7.0299158 -5.1776986 -4.06757 -3.48845 -4.47326 -5.8472223 -6.1529369 -6.4713326 -6.8028193 -7.2326508 -7.3657827 -7.2775974 -6.9946175][-13.208195 -9.6293755 -7.6119289 -5.8153281 -4.5497303 -3.9185658 -4.9885387 -6.3663454 -6.7043114 -7.0339718 -7.2974844 -7.5427017 -7.5546103 -7.4088612 -7.1712093][-12.907972 -9.7442083 -8.1280727 -6.59046 -5.2885895 -4.5710363 -5.4257536 -6.4448881 -6.7324543 -7.0214338 -7.2514124 -7.4260759 -7.4037685 -7.2269516 -6.9978752]]...]
INFO - root - 2017-12-15 06:55:56.904154: step 13910, loss = 0.28, batch loss = 0.24 (35.2 examples/sec; 0.227 sec/batch; 20h:05m:57s remains)
INFO - root - 2017-12-15 06:55:59.177375: step 13920, loss = 0.18, batch loss = 0.14 (35.7 examples/sec; 0.224 sec/batch; 19h:49m:32s remains)
INFO - root - 2017-12-15 06:56:01.459572: step 13930, loss = 0.29, batch loss = 0.26 (33.7 examples/sec; 0.238 sec/batch; 21h:01m:44s remains)
INFO - root - 2017-12-15 06:56:03.763413: step 13940, loss = 0.29, batch loss = 0.26 (33.6 examples/sec; 0.238 sec/batch; 21h:03m:59s remains)
INFO - root - 2017-12-15 06:56:06.036002: step 13950, loss = 0.27, batch loss = 0.23 (36.1 examples/sec; 0.221 sec/batch; 19h:35m:29s remains)
INFO - root - 2017-12-15 06:56:08.305966: step 13960, loss = 0.25, batch loss = 0.22 (34.9 examples/sec; 0.229 sec/batch; 20h:16m:20s remains)
INFO - root - 2017-12-15 06:56:10.608306: step 13970, loss = 0.26, batch loss = 0.23 (35.7 examples/sec; 0.224 sec/batch; 19h:51m:15s remains)
INFO - root - 2017-12-15 06:56:12.858903: step 13980, loss = 0.25, batch loss = 0.22 (35.6 examples/sec; 0.225 sec/batch; 19h:52m:45s remains)
INFO - root - 2017-12-15 06:56:15.163671: step 13990, loss = 0.26, batch loss = 0.23 (35.1 examples/sec; 0.228 sec/batch; 20h:09m:45s remains)
INFO - root - 2017-12-15 06:56:17.438001: step 14000, loss = 0.24, batch loss = 0.21 (34.5 examples/sec; 0.232 sec/batch; 20h:32m:18s remains)
2017-12-15 06:56:17.772233: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.6890831 -6.9459572 -6.7831793 -6.5848417 -6.7573328 -6.519906 -5.7865696 -5.389339 -5.4583731 -5.8076229 -6.1270466 -6.2311621 -6.0734091 -5.4336357 -5.0744438][-6.3104162 -5.8973541 -5.7286978 -5.569663 -5.8508711 -5.4802914 -4.3255553 -3.6260605 -3.5522094 -4.1132994 -4.8219986 -5.25257 -5.3088112 -4.7982807 -4.56178][-5.0397472 -4.1486921 -4.0969377 -4.1029072 -4.6352239 -4.2721338 -2.8574314 -1.8269668 -1.4579542 -2.0176606 -2.9871128 -3.7167182 -4.051044 -3.7279894 -3.6279807][-3.6802859 -2.3031192 -2.3826129 -2.5847127 -3.2734823 -2.9325163 -1.4054396 -0.20819438 0.27862048 -0.28861666 -1.4788806 -2.4908037 -3.1055727 -3.0015416 -3.0315392][-2.2379327 -0.47293568 -0.83174062 -1.3175895 -1.971712 -1.5631796 0.064341545 1.2588367 1.6168959 0.82823992 -0.5748986 -1.731424 -2.4306176 -2.4847698 -2.6323147][-0.93351686 0.86546445 0.27115631 -0.40108979 -0.80685127 -0.28073215 1.2792339 2.3288739 2.4694555 1.5781252 0.055311203 -1.3204135 -2.1545746 -2.3267088 -2.3271308][0.34443212 1.5775201 0.760381 -0.093074083 -0.264364 0.33828616 1.7074983 2.622072 2.6176035 1.706984 0.17856884 -1.3089533 -2.2788181 -2.4853084 -2.2072556][-0.043773413 1.1613631 0.13396788 -0.80684662 -0.71063638 0.074184656 1.3200607 2.0928218 1.9027116 1.0753195 -0.209777 -1.5460919 -2.4020364 -2.4517686 -1.9309764][-1.2610587 -0.12475109 -1.1174539 -1.9655676 -1.6922803 -0.79944193 0.35343623 0.99405241 0.78939795 0.1930337 -0.62434161 -1.3647259 -1.6835558 -1.3585968 -0.48166788][-2.7291989 -1.5446856 -2.4422734 -3.1698408 -2.9042666 -2.0485849 -1.0835816 -0.63609862 -0.80591869 -0.98516774 -1.1585768 -1.1248884 -0.81140816 -0.16933942 0.76207733][-3.8952429 -2.7864709 -3.5432887 -4.1829567 -4.0156941 -3.3683724 -2.7033184 -2.3364153 -2.22712 -1.9511244 -1.594294 -0.93329883 -0.15743995 0.58901429 1.2665651][-4.87393 -3.7372622 -4.2477784 -4.6822176 -4.5414877 -4.073607 -3.6586523 -3.3280029 -2.8045425 -2.0907125 -1.3857887 -0.32778633 0.7010572 1.3888984 1.7035315][-5.5327234 -4.2244072 -4.4530816 -4.7411256 -4.6387653 -4.3507938 -4.1500626 -3.9456682 -3.3330512 -2.5356936 -1.8506052 -0.68579292 0.49513483 1.0814981 0.98521852][-5.8266811 -4.4803543 -4.6079931 -4.895051 -4.7765522 -4.4642472 -4.4186683 -4.423234 -3.9833207 -3.3711109 -2.746387 -1.5954336 -0.44624043 -0.040691376 -0.51084387][-6.0388212 -4.8462172 -4.9340448 -5.1685348 -4.9372625 -4.4997845 -4.521399 -4.6142688 -4.2326136 -3.6530657 -3.1472809 -2.2917857 -1.5345773 -1.4407316 -1.9249599]]...]
INFO - root - 2017-12-15 06:56:20.104312: step 14010, loss = 0.24, batch loss = 0.20 (35.0 examples/sec; 0.229 sec/batch; 20h:14m:35s remains)
INFO - root - 2017-12-15 06:56:22.400941: step 14020, loss = 0.41, batch loss = 0.37 (36.2 examples/sec; 0.221 sec/batch; 19h:31m:29s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 06:56:24.676861: step 14030, loss = 0.24, batch loss = 0.20 (35.3 examples/sec; 0.226 sec/batch; 20h:02m:00s remains)
INFO - root - 2017-12-15 06:56:26.919622: step 14040, loss = 0.23, batch loss = 0.20 (36.2 examples/sec; 0.221 sec/batch; 19h:31m:43s remains)
INFO - root - 2017-12-15 06:56:29.206177: step 14050, loss = 0.31, batch loss = 0.28 (34.5 examples/sec; 0.232 sec/batch; 20h:30m:49s remains)
INFO - root - 2017-12-15 06:56:31.456170: step 14060, loss = 0.20, batch loss = 0.16 (35.8 examples/sec; 0.223 sec/batch; 19h:44m:51s remains)
INFO - root - 2017-12-15 06:56:33.718321: step 14070, loss = 0.31, batch loss = 0.28 (35.7 examples/sec; 0.224 sec/batch; 19h:50m:21s remains)
INFO - root - 2017-12-15 06:56:36.033181: step 14080, loss = 0.23, batch loss = 0.20 (33.2 examples/sec; 0.241 sec/batch; 21h:19m:35s remains)
INFO - root - 2017-12-15 06:56:38.358793: step 14090, loss = 0.27, batch loss = 0.24 (33.0 examples/sec; 0.242 sec/batch; 21h:26m:30s remains)
INFO - root - 2017-12-15 06:56:40.634766: step 14100, loss = 0.21, batch loss = 0.17 (33.4 examples/sec; 0.239 sec/batch; 21h:09m:46s remains)
2017-12-15 06:56:40.962163: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3559828 -5.0683832 -5.584487 -6.4835558 -6.9221878 -7.0606289 -6.7824926 -6.0789256 -5.35446 -5.1802759 -5.3038526 -5.80075 -6.5341845 -6.4457836 -5.9979539][-5.3771105 -4.7031708 -5.0947905 -6.2837477 -6.9002371 -7.0158014 -6.6099367 -5.6996183 -4.8824854 -4.8260136 -5.2509747 -6.1362715 -7.2525167 -7.2560434 -6.8227015][-5.2608647 -3.58146 -3.9217558 -5.1548662 -5.8216472 -5.9563513 -5.4645729 -4.4786453 -3.7456412 -3.9645503 -4.8129215 -5.9992981 -7.3824329 -7.517292 -7.120079][-5.1635323 -2.6930895 -2.9333127 -3.9362507 -4.384419 -4.3643589 -3.6996431 -2.6566219 -2.1287868 -2.7257493 -4.0898738 -5.57887 -7.101162 -7.3890285 -7.2110929][-5.09766 -2.0395951 -2.0530927 -2.5842376 -2.5460706 -2.1690686 -1.1678851 0.092494249 0.52091861 -0.42434645 -2.2804585 -4.0921793 -5.7790055 -6.51079 -6.7697325][-4.2250652 -1.2412384 -1.0262684 -1.1052276 -0.54052424 0.14638424 1.4639654 3.0757039 3.5066707 2.3094027 -0.034052134 -2.0761325 -3.8934305 -5.1003065 -5.7875967][-2.7125831 -0.17291188 0.11131644 0.41348219 1.5539107 2.564544 3.9683354 5.6532984 5.9761705 4.5796471 1.9359515 -0.31111968 -2.203099 -3.7488561 -4.7386484][-2.1206529 0.36759567 0.54491711 1.0290494 2.5266025 3.6910598 4.8665171 6.323801 6.4804535 5.0201025 2.4501603 0.22792006 -1.5973216 -3.2846529 -4.4062943][-2.893373 -0.68694186 -0.72599924 -0.23816848 1.296526 2.4865415 3.287226 4.2017918 4.0451803 2.8069437 0.68476224 -1.2413955 -2.8488202 -4.2464681 -4.9469552][-4.3216319 -2.5074704 -2.8487043 -2.4534404 -1.1215613 -0.0043017864 0.53516364 0.90458107 0.46511745 -0.41774321 -1.9701914 -3.6400237 -4.9469061 -5.7415886 -5.8146391][-5.7105269 -4.1457272 -4.6270308 -4.2646723 -3.2494154 -2.261188 -1.7338003 -1.6965632 -2.3557303 -3.0115838 -4.063621 -5.4803772 -6.4822617 -6.7853231 -6.3995304][-6.5470428 -5.08685 -5.5266995 -5.1906052 -4.6090231 -4.133791 -3.7757826 -3.8192508 -4.4247646 -4.8241215 -5.46874 -6.4800739 -7.1240683 -7.0851355 -6.4208126][-6.1817012 -4.8010511 -5.2709169 -5.1393986 -4.9501557 -4.9247179 -4.7842989 -4.7752247 -5.081903 -5.2727318 -5.7028623 -6.3669462 -6.6596003 -6.4202566 -5.7543335][-4.857234 -3.6201534 -4.0300941 -4.0401497 -4.0855703 -4.3437467 -4.3672142 -4.274024 -4.3323212 -4.4961281 -4.9468031 -5.4029083 -5.4223595 -5.1492128 -4.7039285][-3.6865149 -2.5950694 -2.9120765 -3.0164528 -3.2220898 -3.5474951 -3.6028304 -3.4585748 -3.4350996 -3.599627 -3.9943819 -4.2771511 -4.2211528 -4.0721359 -3.8297813]]...]
INFO - root - 2017-12-15 06:56:43.273073: step 14110, loss = 0.17, batch loss = 0.14 (35.4 examples/sec; 0.226 sec/batch; 19h:57m:44s remains)
INFO - root - 2017-12-15 06:56:45.530141: step 14120, loss = 0.27, batch loss = 0.24 (36.1 examples/sec; 0.222 sec/batch; 19h:36m:21s remains)
INFO - root - 2017-12-15 06:56:47.768257: step 14130, loss = 0.20, batch loss = 0.17 (35.6 examples/sec; 0.225 sec/batch; 19h:53m:19s remains)
INFO - root - 2017-12-15 06:56:50.040983: step 14140, loss = 0.28, batch loss = 0.25 (33.0 examples/sec; 0.242 sec/batch; 21h:24m:48s remains)
INFO - root - 2017-12-15 06:56:52.309730: step 14150, loss = 0.20, batch loss = 0.17 (35.1 examples/sec; 0.228 sec/batch; 20h:08m:32s remains)
INFO - root - 2017-12-15 06:56:54.594608: step 14160, loss = 0.19, batch loss = 0.15 (35.8 examples/sec; 0.224 sec/batch; 19h:46m:26s remains)
INFO - root - 2017-12-15 06:56:56.826325: step 14170, loss = 0.29, batch loss = 0.26 (36.4 examples/sec; 0.220 sec/batch; 19h:27m:35s remains)
INFO - root - 2017-12-15 06:56:59.121305: step 14180, loss = 0.22, batch loss = 0.19 (34.8 examples/sec; 0.230 sec/batch; 20h:19m:09s remains)
INFO - root - 2017-12-15 06:57:01.382575: step 14190, loss = 0.21, batch loss = 0.18 (36.1 examples/sec; 0.222 sec/batch; 19h:36m:43s remains)
INFO - root - 2017-12-15 06:57:03.644434: step 14200, loss = 0.27, batch loss = 0.24 (35.8 examples/sec; 0.223 sec/batch; 19h:44m:05s remains)
2017-12-15 06:57:03.968010: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6371386 -5.8019867 -5.7496862 -5.9301643 -6.0773506 -6.2865086 -6.454999 -6.5526867 -6.6987782 -6.6417093 -6.422081 -6.2857265 -6.2166109 -6.1307154 -5.8796206][-5.6235485 -6.5641565 -6.6382666 -6.9389467 -7.0780926 -7.4466734 -7.9453182 -8.1833563 -8.3390512 -8.1850271 -7.7559557 -7.4708118 -7.2873058 -7.1952181 -6.7962132][-7.6401072 -7.1822186 -7.49473 -7.8359308 -7.7498388 -8.1776028 -8.9041271 -9.1078444 -9.0907469 -8.8143854 -8.3289843 -8.0341015 -7.8734865 -7.9246454 -7.6060028][-8.7225161 -7.0911646 -7.6369362 -7.8805695 -7.44489 -7.74185 -8.398035 -8.1930017 -7.6819487 -7.1234484 -6.7194328 -6.7453337 -6.87869 -7.2464628 -7.3489084][-8.9998388 -6.271554 -6.8579559 -6.9698353 -6.4706116 -6.7370787 -7.1367788 -6.3339224 -5.0914836 -4.0466542 -3.5713177 -3.9255548 -4.5425243 -5.3571415 -6.0658484][-8.7520828 -5.707263 -6.1280375 -6.1529312 -5.7399559 -5.8064842 -5.643734 -4.3362226 -2.4057004 -0.76043713 -0.07889986 -0.63500237 -1.828367 -3.1629174 -4.5624552][-7.8561621 -4.9619279 -4.9951296 -4.9373579 -4.6156874 -4.2465539 -3.3011975 -1.6350757 0.69377756 2.7486989 3.4744961 2.7047179 1.0014722 -0.83720827 -2.8808277][-7.5356369 -4.4557266 -4.0722651 -4.0169029 -3.8789916 -3.1969769 -1.7255019 0.19755983 2.7111719 4.8432856 5.5880861 4.6165171 2.7150857 0.63814783 -1.7103319][-7.9844589 -4.8324518 -4.23818 -4.303072 -4.39229 -3.7621086 -2.240973 -0.17369509 2.3785102 4.4090662 4.9830303 3.9945595 2.3719161 0.48544383 -1.7377844][-8.2402477 -5.116416 -4.5436153 -4.7453218 -4.9705114 -4.7029505 -3.9185815 -2.0467546 0.31547213 2.1565254 2.5906956 1.6531641 0.46420741 -1.0848248 -2.7748764][-8.2704372 -5.2199049 -4.8675456 -5.1651545 -5.3982167 -5.4370008 -5.5142908 -4.0545235 -2.1192057 -0.66105342 -0.35986972 -1.1910524 -2.0556917 -3.2163539 -4.0464578][-8.368783 -5.2701063 -5.1629252 -5.5883665 -5.8425293 -6.022892 -6.6857939 -5.8564095 -4.7180338 -3.792325 -3.6931214 -4.2999992 -4.7858906 -5.3391232 -5.2541494][-8.6293793 -5.3585043 -5.3079419 -5.7027841 -5.8379622 -6.0871506 -7.020772 -6.6471329 -6.2218342 -5.8757486 -6.1173344 -6.5933094 -6.8108091 -6.7577133 -5.908309][-8.9516621 -5.5032043 -5.425848 -5.7742071 -5.76764 -6.0944891 -7.0875225 -6.7247467 -6.5402374 -6.45887 -6.7145262 -7.1377687 -7.3710003 -7.0292625 -5.8787422][-9.4205189 -5.8976564 -5.6997213 -5.864603 -5.6536312 -5.9817905 -6.9483347 -6.5090761 -6.408906 -6.4419174 -6.67947 -6.9919024 -7.2424183 -6.8083982 -5.7127819]]...]
INFO - root - 2017-12-15 06:57:06.223433: step 14210, loss = 0.43, batch loss = 0.39 (33.8 examples/sec; 0.237 sec/batch; 20h:55m:29s remains)
INFO - root - 2017-12-15 06:57:08.501914: step 14220, loss = 0.23, batch loss = 0.20 (33.7 examples/sec; 0.237 sec/batch; 20h:59m:25s remains)
INFO - root - 2017-12-15 06:57:10.801933: step 14230, loss = 0.37, batch loss = 0.34 (33.9 examples/sec; 0.236 sec/batch; 20h:50m:36s remains)
INFO - root - 2017-12-15 06:57:13.030664: step 14240, loss = 0.21, batch loss = 0.17 (35.0 examples/sec; 0.228 sec/batch; 20h:11m:47s remains)
INFO - root - 2017-12-15 06:57:15.279324: step 14250, loss = 0.19, batch loss = 0.16 (34.4 examples/sec; 0.233 sec/batch; 20h:35m:01s remains)
INFO - root - 2017-12-15 06:57:17.527092: step 14260, loss = 0.19, batch loss = 0.16 (35.6 examples/sec; 0.225 sec/batch; 19h:51m:27s remains)
INFO - root - 2017-12-15 06:57:19.810107: step 14270, loss = 0.17, batch loss = 0.14 (36.5 examples/sec; 0.219 sec/batch; 19h:20m:55s remains)
INFO - root - 2017-12-15 06:57:22.056862: step 14280, loss = 0.19, batch loss = 0.16 (35.3 examples/sec; 0.227 sec/batch; 20h:01m:33s remains)
INFO - root - 2017-12-15 06:57:24.291945: step 14290, loss = 0.29, batch loss = 0.26 (33.4 examples/sec; 0.239 sec/batch; 21h:08m:57s remains)
INFO - root - 2017-12-15 06:57:26.549927: step 14300, loss = 0.18, batch loss = 0.14 (35.0 examples/sec; 0.228 sec/batch; 20h:11m:20s remains)
2017-12-15 06:57:26.903253: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5914962 -4.7991433 -6.4199486 -7.0817719 -6.87241 -6.4986992 -6.2052746 -6.0773668 -5.8487024 -5.1361771 -4.8013687 -4.8684425 -3.9516659 -3.20465 -2.857445][-3.0576825 -6.0336432 -7.4742727 -7.7373581 -7.4617825 -7.0950851 -6.5156755 -5.8581781 -5.0993667 -4.3065643 -4.3745704 -4.7950578 -3.7061055 -2.7291043 -2.5695858][-4.6204734 -6.53895 -7.4215603 -7.1177664 -6.8121109 -6.4763937 -5.5465508 -4.2429605 -3.0804377 -2.508291 -3.1991019 -4.1726508 -3.3272438 -2.6119993 -2.8248513][-5.5055556 -6.1949463 -6.3626337 -5.5222654 -5.1889572 -4.8024116 -3.5691431 -1.6977828 -0.2682811 -0.2346983 -1.7464715 -3.5071068 -3.4116526 -3.2073929 -3.6774697][-5.6885338 -5.5864725 -5.1904821 -3.8666422 -3.291348 -2.5581074 -0.88100517 1.4555247 2.8119113 1.9781864 -0.53582883 -3.1706724 -3.9139252 -4.131505 -4.5249405][-6.0096488 -5.1898651 -4.4187708 -2.6242993 -1.5239837 -0.11423612 2.2259319 4.9599266 5.9171953 4.0145864 0.47636795 -2.7655802 -4.1470518 -4.7270041 -4.9576306][-5.6713524 -5.1277552 -4.2149944 -2.1508312 -0.53415215 1.6342599 4.7312908 7.6665106 8.0636911 5.2765455 0.98017287 -2.6028833 -4.4202623 -5.2213182 -5.3034525][-6.0014448 -5.3665543 -4.5127287 -2.4621353 -0.62066388 1.840564 5.0735722 7.5745754 7.2946157 4.1291361 -0.12556863 -3.294812 -4.8885503 -5.4109454 -5.3054886][-6.1870661 -5.5505247 -4.7579732 -2.8623538 -1.0727855 1.1862357 3.9273016 5.5436678 4.6696835 1.6729171 -1.9178997 -4.266089 -5.2665262 -5.28823 -4.9128752][-6.0991216 -5.4434571 -4.6165175 -2.8201869 -1.2293034 0.58736038 2.5944803 3.4371088 2.2714736 -0.37358713 -3.1665635 -4.7694807 -5.3258924 -5.0017805 -4.3978667][-6.0267076 -5.4329252 -4.5256209 -2.7440965 -1.4563055 -0.19466567 1.0929186 1.4381471 0.26015806 -1.8842577 -3.8726087 -4.9152622 -5.092329 -4.49677 -3.777668][-6.1015825 -5.5644975 -4.4590483 -2.568754 -1.5873722 -0.87252307 -0.17802215 -0.045009613 -1.0662608 -2.778651 -4.2851596 -4.9861536 -4.7955856 -4.0237069 -3.3511198][-6.4045286 -6.0505409 -4.9732304 -3.1511598 -2.3830111 -1.8792899 -1.3838413 -1.2558408 -2.059381 -3.34443 -4.4630222 -5.1039505 -4.78057 -3.9637895 -3.4527798][-6.796217 -6.7108431 -5.8964281 -4.3441405 -3.710072 -3.1935625 -2.64264 -2.4148607 -2.9693527 -3.8562508 -4.641242 -5.2657332 -4.8380961 -4.1099873 -3.9375424][-6.8065577 -6.8204861 -6.2584953 -5.0030794 -4.4281034 -3.9050314 -3.3666234 -3.1058037 -3.4357824 -3.869477 -4.3728752 -5.0748653 -4.8611174 -4.4466462 -4.5360756]]...]
INFO - root - 2017-12-15 06:57:29.173070: step 14310, loss = 0.33, batch loss = 0.29 (34.0 examples/sec; 0.235 sec/batch; 20h:46m:35s remains)
INFO - root - 2017-12-15 06:57:31.427879: step 14320, loss = 0.21, batch loss = 0.18 (35.6 examples/sec; 0.225 sec/batch; 19h:52m:20s remains)
INFO - root - 2017-12-15 06:57:33.698943: step 14330, loss = 0.26, batch loss = 0.22 (34.0 examples/sec; 0.236 sec/batch; 20h:48m:50s remains)
INFO - root - 2017-12-15 06:57:35.947846: step 14340, loss = 0.27, batch loss = 0.24 (35.9 examples/sec; 0.223 sec/batch; 19h:41m:59s remains)
INFO - root - 2017-12-15 06:57:38.192644: step 14350, loss = 0.27, batch loss = 0.24 (35.4 examples/sec; 0.226 sec/batch; 19h:59m:37s remains)
INFO - root - 2017-12-15 06:57:40.456593: step 14360, loss = 0.24, batch loss = 0.21 (36.7 examples/sec; 0.218 sec/batch; 19h:14m:30s remains)
INFO - root - 2017-12-15 06:57:42.705682: step 14370, loss = 0.22, batch loss = 0.18 (35.1 examples/sec; 0.228 sec/batch; 20h:07m:46s remains)
INFO - root - 2017-12-15 06:57:44.985949: step 14380, loss = 0.26, batch loss = 0.23 (35.1 examples/sec; 0.228 sec/batch; 20h:07m:36s remains)
INFO - root - 2017-12-15 06:57:47.229934: step 14390, loss = 0.30, batch loss = 0.26 (35.1 examples/sec; 0.228 sec/batch; 20h:06m:57s remains)
INFO - root - 2017-12-15 06:57:49.538588: step 14400, loss = 0.21, batch loss = 0.17 (35.2 examples/sec; 0.227 sec/batch; 20h:05m:35s remains)
2017-12-15 06:57:49.846491: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7677417 -7.2950621 -7.0530758 -7.4341488 -7.0968695 -6.6938143 -6.3901682 -6.1576047 -6.0424333 -6.8370695 -7.6408348 -7.8270531 -8.0881977 -8.2648411 -8.1467037][-5.347434 -7.7761965 -7.3700886 -7.5689144 -7.2010965 -6.7870369 -6.3078842 -5.9041824 -5.8763337 -6.9376736 -7.7579279 -8.0149708 -8.3891735 -8.620163 -8.3235493][-6.1898623 -7.8635015 -7.3081007 -7.3750076 -6.9371023 -6.3804016 -5.62227 -4.9072304 -4.8727484 -6.1099339 -7.122736 -7.6160145 -8.0993023 -8.34367 -7.8140717][-7.3833132 -8.0732441 -7.3866549 -7.3464289 -6.8569756 -6.0865231 -4.9573941 -3.7912455 -3.4661324 -4.8434429 -6.1262803 -6.9642191 -7.7491074 -8.0667686 -7.4782753][-7.6512423 -7.8496561 -7.1161366 -7.0892935 -6.6938219 -5.7530441 -4.1469555 -2.3406537 -1.5543289 -3.0700312 -4.9095907 -6.3489876 -7.6186361 -8.149271 -7.52682][-7.7592297 -7.2463589 -6.3622675 -6.2088156 -5.7434878 -4.5252628 -2.3610988 0.16805053 1.4133055 -0.34359455 -2.8654912 -4.9042063 -6.496912 -6.933116 -6.2740011][-6.956871 -6.4748678 -5.2908945 -4.6786766 -3.824064 -2.2073872 0.42372274 3.5351765 5.1673346 3.3586934 0.32830834 -2.334254 -4.3028507 -4.8039188 -4.1149983][-6.6852369 -6.048233 -4.6569347 -3.5848122 -2.3625226 -0.54839683 2.2749956 5.6794767 7.5961094 5.8154993 2.4550536 -0.73703957 -3.0633395 -3.6346817 -2.9158812][-6.8024912 -6.1725373 -4.9003067 -3.7774382 -2.6008921 -0.95305896 1.6800492 4.9156294 6.829154 5.3024673 2.2358267 -0.80171573 -2.9982049 -3.5964499 -2.80938][-7.0118847 -6.5496855 -5.7318172 -4.9873438 -4.2557325 -3.0024624 -0.70759809 2.2176392 3.954159 2.8123853 0.35704446 -2.2262948 -3.9758711 -4.3920426 -3.5346513][-7.098937 -6.8084 -6.4970579 -6.278121 -6.1193132 -5.32876 -3.4723701 -0.98902559 0.5388453 -0.21089685 -2.0297861 -4.1061311 -5.3614445 -5.6006618 -4.60014][-7.0356932 -6.6804228 -6.5679512 -6.6123257 -6.772912 -6.387044 -5.0368071 -3.0454786 -1.6693599 -2.0296414 -3.3810596 -5.1834044 -6.198348 -6.3829679 -5.4091215][-6.9813576 -6.44895 -6.2111254 -6.1890793 -6.2842636 -5.953167 -5.0291476 -3.6767697 -2.7632229 -3.0621812 -4.1644363 -5.6789112 -6.5606527 -6.8632736 -6.2528934][-6.998 -6.360013 -5.98732 -5.8399887 -5.7332029 -5.3827477 -4.8139787 -4.023387 -3.5485675 -3.920763 -4.8415775 -6.0656776 -6.7606249 -7.0968227 -6.8549356][-6.8421812 -6.1538553 -5.662899 -5.3901415 -5.0819759 -4.5915956 -4.1744852 -3.8765717 -3.9160981 -4.4994535 -5.3175964 -6.1439228 -6.5752878 -6.8292685 -6.8299961]]...]
INFO - root - 2017-12-15 06:57:52.122948: step 14410, loss = 0.27, batch loss = 0.23 (34.7 examples/sec; 0.231 sec/batch; 20h:22m:18s remains)
INFO - root - 2017-12-15 06:57:54.427845: step 14420, loss = 0.25, batch loss = 0.21 (35.9 examples/sec; 0.223 sec/batch; 19h:40m:49s remains)
INFO - root - 2017-12-15 06:57:56.664932: step 14430, loss = 0.20, batch loss = 0.16 (35.8 examples/sec; 0.224 sec/batch; 19h:45m:17s remains)
INFO - root - 2017-12-15 06:57:58.958279: step 14440, loss = 0.23, batch loss = 0.20 (35.3 examples/sec; 0.227 sec/batch; 20h:02m:35s remains)
INFO - root - 2017-12-15 06:58:01.216175: step 14450, loss = 0.37, batch loss = 0.34 (33.2 examples/sec; 0.241 sec/batch; 21h:15m:45s remains)
INFO - root - 2017-12-15 06:58:03.482657: step 14460, loss = 0.24, batch loss = 0.21 (34.2 examples/sec; 0.234 sec/batch; 20h:39m:39s remains)
INFO - root - 2017-12-15 06:58:05.813341: step 14470, loss = 0.22, batch loss = 0.18 (31.7 examples/sec; 0.253 sec/batch; 22h:18m:51s remains)
INFO - root - 2017-12-15 06:58:08.087438: step 14480, loss = 0.29, batch loss = 0.26 (34.6 examples/sec; 0.231 sec/batch; 20h:26m:58s remains)
INFO - root - 2017-12-15 06:58:10.344433: step 14490, loss = 0.24, batch loss = 0.21 (37.2 examples/sec; 0.215 sec/batch; 19h:00m:33s remains)
INFO - root - 2017-12-15 06:58:12.655676: step 14500, loss = 0.22, batch loss = 0.19 (35.1 examples/sec; 0.228 sec/batch; 20h:06m:42s remains)
2017-12-15 06:58:12.992275: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0707407 -5.1680984 -6.0282249 -6.5925756 -6.4161549 -6.2910714 -6.4432011 -6.1359158 -5.1493349 -4.311974 -4.272655 -4.7491922 -5.4286814 -6.35585 -7.0295258][-3.7865534 -5.0277939 -5.8898926 -6.7262397 -6.7557344 -6.6415329 -6.7554841 -6.6421752 -5.9869385 -5.5218291 -5.8370419 -6.48202 -6.8990493 -7.6367826 -7.9834652][-5.0017052 -5.0162554 -5.81061 -6.6805019 -6.8027592 -6.6518326 -6.5099535 -6.2523737 -5.7801332 -5.6798563 -6.2911677 -6.9702964 -7.1473503 -7.5017877 -7.4947343][-5.5872221 -5.1290197 -5.7442837 -6.4225287 -6.4550085 -6.0906167 -5.502161 -4.7882051 -4.3874893 -4.649663 -5.4954367 -6.2851243 -6.4827476 -6.4435582 -6.2211161][-5.8413067 -4.9688778 -5.2845082 -5.603776 -5.1726732 -4.1693563 -2.9298739 -1.5813396 -1.1371592 -1.8318533 -3.1602883 -4.4505138 -5.003459 -4.8886766 -4.4598022][-5.1947136 -3.9387927 -4.0177312 -3.8937721 -2.8272004 -1.0952913 0.80140758 2.780077 3.1544757 1.7939801 -0.19376111 -2.0962515 -3.3018081 -3.3012669 -2.7730784][-3.440722 -2.6060433 -2.5522275 -2.1468654 -0.57881486 1.8171229 4.4144139 6.7656779 6.8744311 4.9064412 2.194499 -0.42412591 -2.2769713 -2.4203622 -1.8953975][-2.3723958 -1.6606135 -1.5148213 -0.92733741 1.0332167 3.9696431 6.9071612 9.1029263 8.8704681 6.6180291 3.3962517 0.26011157 -1.8206387 -2.1477292 -1.7190208][-2.1325705 -1.6544492 -1.5280726 -0.89870512 1.1573741 4.1160135 6.9306026 8.6168194 8.0090141 5.7130556 2.5164084 -0.41732252 -2.1900244 -2.5502062 -2.1077087][-2.8458376 -2.5489788 -2.5384324 -1.9946613 -0.15519238 2.3885531 4.7284422 5.7143359 4.7749257 2.7845821 0.18240595 -2.0951757 -3.2908688 -3.481781 -2.8958564][-3.9037073 -3.661684 -3.7747953 -3.4108918 -2.0555542 -0.223876 1.3739011 1.6903329 0.71643591 -0.685619 -2.4971869 -4.0332842 -4.6232634 -4.5140753 -3.743238][-5.3909407 -5.0539856 -5.2480364 -5.0459857 -4.2284083 -3.2273698 -2.4531968 -2.5370295 -3.3256221 -4.2014122 -5.2505312 -6.0903425 -6.2591925 -5.9927092 -5.1804628][-6.4098992 -5.9586844 -6.1502333 -6.0841675 -5.7175646 -5.3238411 -5.0571175 -5.26401 -5.8542271 -6.3730388 -6.8927984 -7.2481837 -7.2470407 -7.0000362 -6.2751493][-6.5519748 -6.003952 -6.19034 -6.2659526 -6.2179842 -6.1696835 -6.155201 -6.3473845 -6.6965446 -6.92728 -7.0839128 -7.1753559 -7.1215439 -6.9038205 -6.3482809][-6.2880054 -5.7015738 -5.8616695 -5.995142 -6.0583487 -6.0879216 -6.1266456 -6.1868868 -6.2795911 -6.362318 -6.4181275 -6.4214849 -6.3912086 -6.26227 -5.9353342]]...]
INFO - root - 2017-12-15 06:58:15.254357: step 14510, loss = 0.24, batch loss = 0.21 (36.2 examples/sec; 0.221 sec/batch; 19h:29m:45s remains)
INFO - root - 2017-12-15 06:58:17.519432: step 14520, loss = 0.25, batch loss = 0.21 (34.8 examples/sec; 0.230 sec/batch; 20h:19m:52s remains)
INFO - root - 2017-12-15 06:58:19.761373: step 14530, loss = 0.26, batch loss = 0.23 (36.1 examples/sec; 0.221 sec/batch; 19h:33m:08s remains)
INFO - root - 2017-12-15 06:58:22.022458: step 14540, loss = 0.23, batch loss = 0.20 (35.5 examples/sec; 0.225 sec/batch; 19h:54m:48s remains)
INFO - root - 2017-12-15 06:58:24.316808: step 14550, loss = 0.19, batch loss = 0.16 (32.0 examples/sec; 0.250 sec/batch; 22h:06m:07s remains)
INFO - root - 2017-12-15 06:58:26.558226: step 14560, loss = 0.20, batch loss = 0.16 (36.0 examples/sec; 0.222 sec/batch; 19h:37m:14s remains)
INFO - root - 2017-12-15 06:58:28.801039: step 14570, loss = 0.24, batch loss = 0.21 (35.9 examples/sec; 0.223 sec/batch; 19h:39m:25s remains)
INFO - root - 2017-12-15 06:58:31.088172: step 14580, loss = 0.28, batch loss = 0.25 (35.2 examples/sec; 0.227 sec/batch; 20h:04m:15s remains)
INFO - root - 2017-12-15 06:58:33.312650: step 14590, loss = 0.23, batch loss = 0.20 (35.5 examples/sec; 0.225 sec/batch; 19h:54m:45s remains)
INFO - root - 2017-12-15 06:58:35.622718: step 14600, loss = 0.32, batch loss = 0.28 (33.9 examples/sec; 0.236 sec/batch; 20h:49m:00s remains)
2017-12-15 06:58:35.962628: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4758506 -4.8329949 -5.5493236 -5.7998919 -4.2910891 -2.2908792 -0.040279865 0.66639304 0.036239624 0.0723567 -0.26718414 -0.81748652 -1.3906906 -2.8043718 -4.8599668][-4.4083452 -6.1085367 -6.9427729 -7.002512 -5.3557038 -3.2383456 -1.1541297 -0.40586853 -0.80759144 -0.39815176 -0.22802198 -0.25830019 -0.48681486 -1.7956303 -4.0097075][-6.654479 -7.2137709 -8.2631721 -8.3420134 -6.6517315 -4.1339321 -1.9487388 -1.1939005 -1.6169734 -1.3134699 -0.83711088 -0.68651378 -0.78155875 -1.8010457 -3.5440106][-7.5425396 -7.4236221 -8.54985 -8.5804729 -6.8058977 -4.1580238 -1.8502002 -1.0218064 -1.686944 -1.8583763 -1.6285107 -1.5731325 -1.6742624 -2.4874861 -3.6218762][-7.9325037 -7.1085739 -7.8045435 -7.1608629 -4.7454238 -1.816802 0.60762095 1.40483 0.16340947 -0.90441525 -1.3661737 -1.8241881 -2.1534731 -3.0762043 -3.8879774][-7.3894062 -6.663126 -6.9105549 -5.391571 -2.1931529 1.1246142 3.7011268 4.3798466 2.5029128 0.32029295 -1.0875989 -2.1044075 -2.9442134 -4.0345879 -4.5976343][-6.117012 -5.9322267 -6.0076418 -3.8034658 0.15435719 3.8749979 6.4335289 6.90687 4.6057825 1.4232817 -1.0887573 -2.585896 -3.7021258 -4.7783828 -5.2014618][-6.2404356 -5.8104315 -5.65909 -3.2560573 0.98519778 4.9474106 7.5641127 8.1075239 5.9033794 2.1473715 -1.3648171 -3.254611 -4.535697 -5.4462729 -5.8893938][-6.40971 -5.7352471 -5.8904147 -4.2752533 -0.7304312 2.8925269 5.3745413 6.0264254 4.4432468 1.0454834 -2.6405635 -4.4821768 -6.0025606 -6.7585354 -7.1183882][-5.698205 -4.8419704 -5.9519248 -5.8648958 -3.6948326 -0.83905292 1.3776541 2.1167939 1.4233501 -0.90235496 -3.9743857 -5.4571438 -6.8867664 -7.42441 -7.6662807][-4.7173491 -3.4650288 -5.3117604 -6.6670523 -6.0018148 -4.1997848 -2.4440084 -1.6590729 -1.5581424 -2.7572436 -4.8990664 -5.9888048 -7.2568331 -7.6681128 -7.7641258][-4.0245228 -2.3617 -4.7170486 -7.2529507 -7.8115053 -7.0281487 -5.9031925 -5.3558655 -4.9752049 -5.3333631 -6.465169 -7.131217 -8.2248621 -8.5811977 -8.50261][-4.3849311 -2.1665573 -4.6420422 -7.67583 -8.8508806 -8.8006449 -8.30202 -7.9590874 -7.45325 -7.2437963 -7.5450306 -7.9240265 -8.6802473 -8.9413824 -8.7379313][-5.6310768 -3.1265988 -5.1899223 -7.7886705 -8.7642374 -9.0620842 -9.0178919 -8.7639732 -8.2294264 -7.751564 -7.64429 -7.8667765 -8.3498383 -8.543066 -8.3571653][-6.4897642 -4.108624 -5.5815372 -7.2882996 -7.8031311 -8.159646 -8.2949371 -8.0601168 -7.5861492 -7.1276321 -6.9645433 -7.1188822 -7.369792 -7.4061241 -7.2064562]]...]
INFO - root - 2017-12-15 06:58:38.296143: step 14610, loss = 0.18, batch loss = 0.15 (30.7 examples/sec; 0.261 sec/batch; 23h:01m:47s remains)
INFO - root - 2017-12-15 06:58:40.569642: step 14620, loss = 0.17, batch loss = 0.13 (35.9 examples/sec; 0.223 sec/batch; 19h:40m:33s remains)
INFO - root - 2017-12-15 06:58:42.855290: step 14630, loss = 0.23, batch loss = 0.19 (34.9 examples/sec; 0.229 sec/batch; 20h:12m:41s remains)
INFO - root - 2017-12-15 06:58:45.116415: step 14640, loss = 0.37, batch loss = 0.34 (35.7 examples/sec; 0.224 sec/batch; 19h:47m:33s remains)
INFO - root - 2017-12-15 06:58:47.415895: step 14650, loss = 0.19, batch loss = 0.16 (36.0 examples/sec; 0.222 sec/batch; 19h:37m:04s remains)
INFO - root - 2017-12-15 06:58:49.718310: step 14660, loss = 0.24, batch loss = 0.21 (32.4 examples/sec; 0.247 sec/batch; 21h:47m:47s remains)
INFO - root - 2017-12-15 06:58:51.960390: step 14670, loss = 0.20, batch loss = 0.16 (36.4 examples/sec; 0.220 sec/batch; 19h:24m:40s remains)
INFO - root - 2017-12-15 06:58:54.261311: step 14680, loss = 0.25, batch loss = 0.22 (35.2 examples/sec; 0.227 sec/batch; 20h:04m:49s remains)
INFO - root - 2017-12-15 06:58:56.503220: step 14690, loss = 0.22, batch loss = 0.19 (36.2 examples/sec; 0.221 sec/batch; 19h:30m:54s remains)
INFO - root - 2017-12-15 06:58:58.770571: step 14700, loss = 0.21, batch loss = 0.18 (35.7 examples/sec; 0.224 sec/batch; 19h:47m:49s remains)
2017-12-15 06:58:59.082871: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12313128 -1.8340139 -2.4761686 -3.3080652 -4.0713716 -4.8107023 -5.1862664 -4.7010889 -3.8644276 -3.6844912 -3.1359024 -2.1971419 -1.3085825 -0.65147543 -0.25660646][-2.1119528 -3.5598123 -4.0962648 -4.7014523 -5.4448318 -6.0028591 -5.9402008 -4.9709816 -3.7347231 -3.4161115 -2.9384284 -2.1464736 -1.3094363 -0.48160863 0.20372105][-3.6661196 -4.5124488 -4.7339158 -4.9438524 -5.41418 -5.617866 -5.2620683 -4.1591635 -3.0063941 -2.8603952 -2.7855611 -2.3155704 -1.7360995 -0.90506756 -0.086250782][-4.5928907 -4.8747492 -4.8607836 -4.7997894 -4.854444 -4.6971397 -4.2236495 -3.3021891 -2.4721472 -2.4841673 -2.6875844 -2.528393 -2.3501675 -1.7523613 -0.91340363][-4.9043264 -4.6206808 -4.3537412 -4.016593 -3.6943312 -3.2174754 -2.9062259 -2.41587 -1.869758 -1.8741999 -2.2967162 -2.3413773 -2.4115582 -2.0701947 -1.4286304][-4.5239182 -3.6559114 -3.2120965 -2.7761927 -2.1507571 -1.5427563 -1.5972385 -1.6599903 -1.2737683 -1.0504462 -1.5275259 -1.4550085 -1.5675513 -1.5493944 -1.2280445][-3.0877571 -2.1501245 -1.7138171 -1.3904483 -0.72950232 -0.15705609 -0.42829144 -0.85005546 -0.5152781 0.034725666 -0.22161353 0.086957216 0.039406061 -0.20902753 -0.346735][-2.2798357 -0.994838 -0.52192366 -0.22870898 0.31354284 0.81877661 0.62867951 0.13449097 0.42566037 1.0694621 0.92533636 1.4942007 1.6323397 1.1396441 0.58705258][-2.4822776 -1.0278274 -0.56831622 -0.29345047 0.14680362 0.65923071 0.74692512 0.34734821 0.49302173 1.1017282 1.0338876 1.6612358 1.9760342 1.4092455 0.60859752][-3.26273 -1.9486965 -1.6311858 -1.3737833 -1.0487673 -0.60404885 -0.34399343 -0.67749774 -0.71169186 -0.33692694 -0.45358276 0.15157843 0.64636946 0.17677236 -0.62039912][-4.0529189 -3.0145886 -2.8659205 -2.6903431 -2.4738312 -2.1219792 -1.8769166 -2.2143226 -2.4363735 -2.3084955 -2.465621 -2.0239348 -1.562326 -1.9544342 -2.6441324][-4.8945761 -4.1281686 -4.1190557 -4.0342369 -3.9257073 -3.740541 -3.6126873 -3.9218116 -4.1425438 -4.1663384 -4.4312344 -4.2262797 -3.9331412 -4.2253304 -4.6843147][-5.4710097 -4.8726912 -4.956265 -4.9501181 -4.9018831 -4.8705397 -4.9115276 -5.2183752 -5.4643788 -5.5249071 -5.7014546 -5.6301851 -5.4722033 -5.6609006 -5.9070215][-5.6099434 -5.1464224 -5.2627258 -5.2691507 -5.2342181 -5.2814751 -5.4282322 -5.6525908 -5.8115911 -5.9345737 -6.0540848 -5.9879751 -5.8987641 -6.0223303 -6.2118416][-5.473073 -4.9799461 -5.1172938 -5.1561294 -5.1344528 -5.1795044 -5.3152304 -5.470006 -5.5524869 -5.6034336 -5.6460066 -5.631515 -5.6400638 -5.7586737 -5.9061966]]...]
INFO - root - 2017-12-15 06:59:01.387222: step 14710, loss = 0.29, batch loss = 0.26 (36.6 examples/sec; 0.218 sec/batch; 19h:16m:51s remains)
INFO - root - 2017-12-15 06:59:03.636684: step 14720, loss = 0.24, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 19h:36m:46s remains)
INFO - root - 2017-12-15 06:59:05.898985: step 14730, loss = 0.27, batch loss = 0.23 (36.7 examples/sec; 0.218 sec/batch; 19h:13m:28s remains)
INFO - root - 2017-12-15 06:59:08.172966: step 14740, loss = 0.25, batch loss = 0.22 (33.7 examples/sec; 0.238 sec/batch; 20h:58m:04s remains)
INFO - root - 2017-12-15 06:59:10.478805: step 14750, loss = 0.23, batch loss = 0.19 (35.8 examples/sec; 0.224 sec/batch; 19h:44m:36s remains)
INFO - root - 2017-12-15 06:59:12.735132: step 14760, loss = 0.18, batch loss = 0.14 (35.9 examples/sec; 0.223 sec/batch; 19h:38m:33s remains)
INFO - root - 2017-12-15 06:59:15.016810: step 14770, loss = 0.22, batch loss = 0.19 (34.5 examples/sec; 0.232 sec/batch; 20h:27m:04s remains)
INFO - root - 2017-12-15 06:59:17.285058: step 14780, loss = 0.21, batch loss = 0.18 (33.6 examples/sec; 0.238 sec/batch; 20h:59m:21s remains)
INFO - root - 2017-12-15 06:59:19.587846: step 14790, loss = 0.32, batch loss = 0.28 (35.3 examples/sec; 0.227 sec/batch; 20h:01m:13s remains)
INFO - root - 2017-12-15 06:59:21.841163: step 14800, loss = 0.32, batch loss = 0.29 (36.0 examples/sec; 0.222 sec/batch; 19h:37m:11s remains)
2017-12-15 06:59:22.189375: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.230442 -4.609354 -4.3491077 -4.1254835 -4.062161 -3.689517 -2.981029 -2.4766054 -2.3180714 -2.6212828 -3.5839767 -4.8461075 -5.4664598 -5.6166945 -5.7999716][-4.54947 -5.1302266 -4.8913875 -4.6455946 -4.6098919 -4.3511572 -3.8159833 -3.4464154 -3.2644577 -3.4538627 -4.30282 -5.3130779 -5.7783546 -5.8878546 -5.9322796][-5.5120478 -5.464736 -5.2890096 -5.09037 -5.0758381 -4.9271107 -4.4560061 -4.0505571 -3.7491398 -3.749666 -4.4399147 -5.2916574 -5.7655554 -6.036871 -6.0674009][-5.8215361 -5.305851 -5.1999216 -5.0683646 -5.1083913 -5.0509415 -4.5040264 -3.9221561 -3.5133448 -3.3254929 -3.8843257 -4.733902 -5.3614941 -5.9404731 -6.0485249][-5.3127613 -4.3841834 -4.2943306 -4.2209034 -4.3520584 -4.3955212 -3.6454206 -2.8127952 -2.3756824 -2.1078455 -2.6945186 -3.8540821 -4.8370543 -5.7492104 -5.8870811][-3.8348632 -2.7483568 -2.6479878 -2.668061 -2.842623 -2.8543596 -1.7759788 -0.70672381 -0.38321352 -0.20055199 -0.94870377 -2.5803025 -3.9585519 -5.2577696 -5.5200605][-1.9725509 -1.2673938 -1.0970644 -1.045027 -1.1117673 -0.87656236 0.56763029 1.7856598 1.8569825 1.7861469 0.75457406 -1.454247 -3.3147388 -4.9134631 -5.2412157][-1.3469541 -0.57735884 -0.17419958 0.10589099 0.23864532 0.800822 2.5669076 3.873575 3.6346304 3.1927311 1.7128267 -1.090718 -3.3515606 -4.9829054 -5.2949762][-1.0056407 -0.10756803 0.34896994 0.58552194 0.76206803 1.5217421 3.4298475 4.6809359 4.1869373 3.4267795 1.6264095 -1.3736843 -3.7417853 -5.1465273 -5.4166803][-1.4130734 -0.54842055 -0.29840362 -0.25678241 -0.092787504 0.75068235 2.539695 3.6133168 3.0317705 2.1160667 0.29245305 -2.4465592 -4.5694933 -5.6006942 -5.6958179][-2.9470794 -2.0602863 -1.9876282 -2.0622725 -1.9465644 -1.2409661 0.18943644 1.034313 0.57187057 -0.25920188 -1.896387 -4.1724234 -5.9015622 -6.5069809 -6.3502254][-4.782835 -3.8082881 -3.8476346 -4.0148172 -4.0503421 -3.7535229 -2.8733 -2.303067 -2.5923045 -3.2352691 -4.497786 -6.1305923 -7.2449675 -7.3906708 -6.9385214][-5.9032774 -4.9935131 -5.1544628 -5.3587704 -5.471077 -5.4663267 -4.9820852 -4.5747976 -4.7253757 -5.155942 -5.947526 -7.010313 -7.7049541 -7.6288605 -6.993269][-5.7372751 -4.9048767 -5.07131 -5.1762428 -5.368289 -5.6616468 -5.4433584 -5.1286192 -5.2588539 -5.5304136 -5.9802561 -6.8087711 -7.3639402 -7.2308426 -6.5687428][-5.0290308 -4.2835903 -4.4123106 -4.4915857 -4.8316164 -5.3937635 -5.3078895 -4.9768367 -5.0495005 -5.1092539 -5.3077822 -6.0652895 -6.5825653 -6.4546518 -5.9227753]]...]
INFO - root - 2017-12-15 06:59:24.425098: step 14810, loss = 0.21, batch loss = 0.17 (35.2 examples/sec; 0.228 sec/batch; 20h:04m:45s remains)
INFO - root - 2017-12-15 06:59:26.722628: step 14820, loss = 0.19, batch loss = 0.16 (34.7 examples/sec; 0.230 sec/batch; 20h:20m:21s remains)
INFO - root - 2017-12-15 06:59:28.982903: step 14830, loss = 0.21, batch loss = 0.18 (33.7 examples/sec; 0.237 sec/batch; 20h:56m:28s remains)
INFO - root - 2017-12-15 06:59:31.245922: step 14840, loss = 0.25, batch loss = 0.22 (36.0 examples/sec; 0.222 sec/batch; 19h:37m:44s remains)
INFO - root - 2017-12-15 06:59:33.488982: step 14850, loss = 0.20, batch loss = 0.17 (34.2 examples/sec; 0.234 sec/batch; 20h:39m:18s remains)
INFO - root - 2017-12-15 06:59:35.750260: step 14860, loss = 0.28, batch loss = 0.24 (34.7 examples/sec; 0.230 sec/batch; 20h:20m:00s remains)
INFO - root - 2017-12-15 06:59:38.038526: step 14870, loss = 0.25, batch loss = 0.22 (35.3 examples/sec; 0.227 sec/batch; 20h:00m:36s remains)
INFO - root - 2017-12-15 06:59:40.325258: step 14880, loss = 0.18, batch loss = 0.15 (34.5 examples/sec; 0.232 sec/batch; 20h:27m:27s remains)
INFO - root - 2017-12-15 06:59:42.647464: step 14890, loss = 0.19, batch loss = 0.16 (34.8 examples/sec; 0.230 sec/batch; 20h:15m:36s remains)
INFO - root - 2017-12-15 06:59:44.922597: step 14900, loss = 0.24, batch loss = 0.21 (34.5 examples/sec; 0.232 sec/batch; 20h:27m:02s remains)
2017-12-15 06:59:45.250698: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7229102 -3.4520378 -2.1190615 -1.3442413 -0.89156044 -1.4273629 -2.2097046 -2.4597178 -1.9395285 -1.2670956 -0.78539777 -0.6430248 -1.0959398 -1.7916341 -2.6621819][-3.2269208 -2.5700791 -1.3467847 -0.84533739 -0.77929866 -1.5586838 -2.3376994 -2.5942149 -2.05536 -1.4606981 -0.81019342 -0.39619696 -0.805781 -1.599245 -2.5347569][-1.9778903 -1.0225898 -0.22414351 -0.28809786 -0.71559072 -1.5767624 -2.2336857 -2.2494287 -1.6081353 -1.1760997 -0.83406937 -0.52187264 -0.97465682 -1.7070485 -2.4889657][-0.75217628 0.47609568 1.0695744 0.66186094 -0.24090588 -1.2876909 -1.9221582 -1.8757879 -1.310588 -1.0508171 -0.98792791 -0.97420537 -1.6863481 -2.425812 -2.9803376][-0.058614492 1.2595773 1.8630819 1.7551408 1.1055717 0.13931751 -0.56877565 -0.94565 -0.81537426 -0.93176961 -1.2415516 -1.5763555 -2.5335402 -3.3425722 -3.7792277][-1.2279596 0.44723773 1.1662295 1.5980484 1.7484088 1.6702702 1.5556319 1.0750387 0.48828745 -0.41301787 -1.1833212 -1.9262688 -3.0675128 -3.7907271 -4.1452122][-2.5964901 -1.3793015 -0.62880182 0.37637591 1.4597204 2.3911762 3.1586256 3.0229244 2.052186 0.478395 -0.98010218 -2.1295741 -3.4219499 -4.0921717 -4.2078791][-4.0015535 -3.0111613 -2.3526142 -1.239378 0.12435937 1.5940406 3.040144 3.4236784 2.5594487 0.90916514 -0.75097263 -2.2269628 -3.6105943 -4.2579908 -4.1464415][-4.7089014 -4.0237851 -3.5705922 -2.6551242 -1.5449548 -0.19200802 1.3163536 2.0657749 1.6738863 0.51430154 -0.80904615 -2.0173302 -3.2555957 -3.9646859 -3.8561552][-4.8320308 -4.1241293 -3.8415585 -3.3321524 -2.5682044 -1.5051742 -0.34095025 0.46047235 0.52070904 -0.043057203 -0.82726347 -1.630538 -2.5475411 -3.2218032 -3.3245153][-4.6362429 -3.8155243 -3.6671805 -3.3617811 -2.718689 -1.8177389 -1.095865 -0.59708893 -0.604275 -0.89082503 -1.1863971 -1.3949248 -1.7320144 -2.1953404 -2.2539268][-4.0546036 -3.0857248 -3.0347531 -2.9512315 -2.4883847 -1.8620853 -1.6198225 -1.5608358 -1.7716453 -1.9633216 -1.9246154 -1.6758189 -1.4079559 -1.3017938 -1.0187831][-2.7959738 -1.5939478 -1.5363626 -1.6934245 -1.6396934 -1.4541382 -1.6518637 -2.0020435 -2.4758456 -2.8046699 -2.8601441 -2.6106627 -2.0973389 -1.4350311 -0.67959893][-2.2873943 -0.8702054 -0.52168524 -0.51711941 -0.6207844 -0.80214942 -1.2654409 -1.7261474 -2.2730732 -2.7171035 -3.002013 -3.1401634 -2.8881135 -2.2203045 -1.2966337][-2.9771366 -1.607172 -0.86357343 -0.494372 -0.56213474 -0.85148263 -1.1930096 -1.4037268 -1.642761 -1.8594508 -2.1694729 -2.5865214 -2.7608232 -2.4481711 -1.7780155]]...]
INFO - root - 2017-12-15 06:59:47.538187: step 14910, loss = 0.33, batch loss = 0.30 (35.6 examples/sec; 0.224 sec/batch; 19h:47m:59s remains)
INFO - root - 2017-12-15 06:59:49.857639: step 14920, loss = 0.19, batch loss = 0.16 (34.7 examples/sec; 0.231 sec/batch; 20h:21m:16s remains)
INFO - root - 2017-12-15 06:59:52.140753: step 14930, loss = 0.23, batch loss = 0.20 (35.2 examples/sec; 0.228 sec/batch; 20h:04m:17s remains)
INFO - root - 2017-12-15 06:59:54.429844: step 14940, loss = 0.19, batch loss = 0.16 (35.5 examples/sec; 0.226 sec/batch; 19h:53m:43s remains)
INFO - root - 2017-12-15 06:59:56.703825: step 14950, loss = 0.30, batch loss = 0.26 (34.8 examples/sec; 0.230 sec/batch; 20h:15m:05s remains)
INFO - root - 2017-12-15 06:59:59.010193: step 14960, loss = 0.26, batch loss = 0.23 (35.0 examples/sec; 0.228 sec/batch; 20h:09m:14s remains)
INFO - root - 2017-12-15 07:00:01.311981: step 14970, loss = 0.22, batch loss = 0.18 (33.6 examples/sec; 0.238 sec/batch; 21h:00m:54s remains)
INFO - root - 2017-12-15 07:00:03.557014: step 14980, loss = 0.25, batch loss = 0.22 (35.7 examples/sec; 0.224 sec/batch; 19h:45m:13s remains)
INFO - root - 2017-12-15 07:00:05.854283: step 14990, loss = 0.31, batch loss = 0.27 (33.2 examples/sec; 0.241 sec/batch; 21h:14m:30s remains)
INFO - root - 2017-12-15 07:00:08.127430: step 15000, loss = 0.23, batch loss = 0.20 (34.6 examples/sec; 0.231 sec/batch; 20h:23m:43s remains)
2017-12-15 07:00:08.459156: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3530807 -3.9516988 -4.3435707 -4.8823376 -5.2330613 -5.370532 -5.3207731 -5.0035343 -4.8180904 -4.8881769 -5.3727942 -5.5704703 -5.2438374 -4.5280619 -3.9060137][-3.4030757 -5.08166 -5.6371746 -6.1672378 -6.20932 -5.9542561 -5.7112083 -5.327323 -5.1289463 -5.429225 -6.2438145 -6.7066846 -6.6178622 -5.7929606 -4.6870861][-5.0715523 -5.6749191 -6.2966223 -6.5963297 -6.1559429 -5.4685168 -4.9921083 -4.561769 -4.4165292 -5.0127153 -6.1764746 -6.9920955 -7.1767564 -6.36143 -4.9632158][-6.237905 -6.0442247 -6.4800467 -6.3216286 -5.1915016 -3.8322139 -2.8244367 -2.3676558 -2.609787 -3.7327437 -5.247386 -6.5457172 -7.0915074 -6.4806175 -5.1043668][-6.7043 -6.0227213 -6.0353847 -5.3390083 -3.4545677 -1.3085971 0.47521782 1.1128817 0.37568116 -1.4873042 -3.5900664 -5.6830983 -6.6643381 -6.3820591 -5.1544681][-7.2785659 -5.804842 -5.1882963 -3.9458327 -1.3828856 1.604136 4.3159933 5.1743164 3.8115783 1.0576859 -1.9119239 -4.7246132 -6.3379583 -6.5455456 -5.4924793][-6.5712762 -5.2291479 -4.1098194 -2.4425125 0.61498785 4.43696 8.0109882 9.1095743 7.1147804 3.2747788 -0.7021687 -3.8070285 -5.8303161 -6.523818 -5.7010188][-6.5791116 -4.9068031 -3.7021117 -1.8723001 1.4057298 5.5472803 9.4520092 10.617463 8.1639252 3.6172571 -0.8250643 -3.5039787 -5.19713 -6.1201153 -5.5336733][-6.5248857 -4.8972845 -3.8299568 -2.0063443 1.0045912 4.5716047 7.8441973 8.6393089 6.0137978 1.4631369 -2.7196174 -4.415657 -5.3508978 -6.1128855 -5.595613][-6.7180748 -5.3526106 -4.6381006 -3.2111459 -0.88738966 1.677253 3.933279 4.4541974 2.2803125 -1.4137368 -4.9094243 -6.0043716 -6.3704672 -6.7787652 -6.1637411][-7.3342957 -6.2705073 -5.8566227 -4.8375669 -3.1854415 -1.5491288 -0.15998173 0.056485176 -1.4847155 -4.1428728 -6.7544489 -7.5457311 -7.5724897 -7.6504397 -6.9401431][-7.8380985 -6.9485283 -6.7076325 -6.12494 -5.2873421 -4.5158081 -3.9475188 -4.0520792 -5.0005312 -6.36825 -7.7604332 -8.1764832 -8.1961784 -8.1802654 -7.5537896][-7.77796 -7.0309811 -7.01939 -6.8620749 -6.6455464 -6.3942242 -6.1356373 -6.2421761 -6.7379131 -7.3047409 -7.8502712 -8.1059551 -8.2662888 -8.1703281 -7.587985][-7.167707 -6.4983721 -6.6586432 -6.6420493 -6.5440407 -6.5465922 -6.6663613 -6.9391413 -7.1735425 -7.2494235 -7.3337703 -7.4734344 -7.5776238 -7.3541856 -6.795321][-6.3093543 -5.5394878 -5.692688 -5.7631311 -5.8843064 -6.131784 -6.4127936 -6.5186391 -6.4218678 -6.2889242 -6.231061 -6.299901 -6.3430519 -6.1457205 -5.8131275]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-15000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-15000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 07:00:11.281266: step 15010, loss = 0.24, batch loss = 0.20 (35.4 examples/sec; 0.226 sec/batch; 19h:56m:31s remains)
INFO - root - 2017-12-15 07:00:13.589952: step 15020, loss = 0.32, batch loss = 0.28 (32.6 examples/sec; 0.246 sec/batch; 21h:40m:18s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 07:00:15.890164: step 15030, loss = 0.16, batch loss = 0.13 (35.3 examples/sec; 0.227 sec/batch; 19h:58m:51s remains)
INFO - root - 2017-12-15 07:00:18.219361: step 15040, loss = 0.25, batch loss = 0.21 (35.9 examples/sec; 0.223 sec/batch; 19h:37m:48s remains)
INFO - root - 2017-12-15 07:00:20.509584: step 15050, loss = 0.24, batch loss = 0.21 (34.8 examples/sec; 0.230 sec/batch; 20h:17m:24s remains)
INFO - root - 2017-12-15 07:00:22.771481: step 15060, loss = 0.29, batch loss = 0.25 (35.3 examples/sec; 0.226 sec/batch; 19h:57m:38s remains)
INFO - root - 2017-12-15 07:00:25.080367: step 15070, loss = 0.39, batch loss = 0.36 (35.1 examples/sec; 0.228 sec/batch; 20h:07m:03s remains)
INFO - root - 2017-12-15 07:00:27.398522: step 15080, loss = 0.20, batch loss = 0.17 (36.5 examples/sec; 0.219 sec/batch; 19h:18m:45s remains)
INFO - root - 2017-12-15 07:00:29.710060: step 15090, loss = 0.36, batch loss = 0.32 (35.0 examples/sec; 0.229 sec/batch; 20h:09m:18s remains)
INFO - root - 2017-12-15 07:00:31.989123: step 15100, loss = 0.23, batch loss = 0.20 (33.6 examples/sec; 0.238 sec/batch; 21h:01m:11s remains)
2017-12-15 07:00:32.304466: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9318054 -4.1706729 -4.2577562 -4.5630159 -5.1151743 -5.1958022 -4.8862095 -4.6210766 -4.559535 -4.7103219 -5.0306883 -5.6189928 -6.1110439 -6.1869588 -5.992075][-3.6304703 -3.5712013 -3.4056511 -3.4821792 -4.0558233 -4.1116748 -3.6666486 -3.3633962 -3.4098613 -3.6715221 -3.9951072 -4.7078838 -5.4969883 -5.8966718 -5.9632635][-3.7833464 -2.6079276 -2.1159897 -1.8702426 -2.3594046 -2.3172572 -1.7174896 -1.4436101 -1.7056917 -2.265497 -2.7917042 -3.7101188 -4.713542 -5.3037977 -5.5484157][-3.3022842 -1.3740001 -0.61900175 -0.1746316 -0.67937446 -0.69713926 -0.10779095 -0.012381077 -0.53757215 -1.3883734 -2.1545527 -3.3060706 -4.4249592 -5.0636773 -5.3186007][-3.1681781 -0.049367666 0.67861438 1.1037781 0.45252943 0.24094963 0.87209988 0.920285 0.2791779 -0.72488558 -1.5787656 -2.8760126 -4.1073895 -4.8261051 -5.0540438][-2.7547894 0.79519463 1.1891708 1.4670248 0.65209794 0.18849182 0.88980389 1.0707581 0.60641241 -0.42191148 -1.3118215 -2.6668806 -3.947588 -4.696557 -4.8257422][-2.5241625 0.73621511 1.0452297 1.3263166 0.48669553 -0.05820632 0.73178291 1.038065 0.82834792 -0.20498586 -1.3556695 -2.9298682 -4.2279139 -4.8552895 -4.7647591][-2.0327554 1.2447689 1.6329317 1.8647199 0.93058276 0.27400017 1.0802262 1.3873599 1.3298659 0.26493764 -1.2482915 -3.2332516 -4.6836419 -5.2034378 -4.8868718][-2.110631 1.4815462 2.0321083 2.1556616 1.0237148 0.23325992 0.94695544 1.1084867 1.0905094 0.048028469 -1.6518606 -3.9021785 -5.4440546 -5.8142395 -5.2531042][-3.6836715 -0.1214385 0.59024286 0.80857134 -0.22220397 -0.6403867 0.21978831 0.20044208 -0.036425352 -1.0745572 -2.8063803 -4.9532313 -6.2895322 -6.4181023 -5.6721339][-5.4637733 -2.09415 -1.1788737 -0.749897 -1.4675786 -1.4623002 -0.5068357 -0.7716068 -1.4391325 -2.4888415 -3.965519 -5.6556578 -6.6662745 -6.6161814 -5.7705574][-6.8372612 -3.773592 -2.8340535 -2.2994087 -2.6401863 -2.3640857 -1.5585892 -2.1475229 -3.2053151 -4.2216597 -5.2681513 -6.3788705 -7.0244451 -6.8011136 -5.8335781][-7.7794385 -5.1736283 -4.4784031 -4.1749325 -4.4018946 -3.9811687 -3.2775068 -3.8473644 -4.9000463 -5.7362094 -6.4775796 -7.1483426 -7.4758959 -7.147121 -6.15649][-7.9265466 -5.8665533 -5.4705858 -5.3973093 -5.4823475 -4.9219127 -4.249135 -4.6353664 -5.5423937 -6.2423553 -6.7796645 -7.1246266 -7.2259831 -6.9086714 -6.0538979][-7.06961 -5.5826616 -5.4920435 -5.4742818 -5.375761 -4.869401 -4.3942375 -4.6278491 -5.2444344 -5.7543182 -6.0690031 -6.2032328 -6.27143 -6.08315 -5.4621391]]...]
INFO - root - 2017-12-15 07:00:34.640416: step 15110, loss = 0.21, batch loss = 0.17 (34.9 examples/sec; 0.229 sec/batch; 20h:13m:59s remains)
INFO - root - 2017-12-15 07:00:36.889933: step 15120, loss = 0.27, batch loss = 0.24 (35.6 examples/sec; 0.225 sec/batch; 19h:48m:17s remains)
INFO - root - 2017-12-15 07:00:39.180966: step 15130, loss = 0.21, batch loss = 0.17 (31.3 examples/sec; 0.256 sec/batch; 22h:33m:25s remains)
INFO - root - 2017-12-15 07:00:41.452881: step 15140, loss = 0.30, batch loss = 0.27 (35.2 examples/sec; 0.227 sec/batch; 20h:03m:01s remains)
INFO - root - 2017-12-15 07:00:43.764453: step 15150, loss = 0.20, batch loss = 0.17 (34.3 examples/sec; 0.233 sec/batch; 20h:32m:12s remains)
INFO - root - 2017-12-15 07:00:46.069214: step 15160, loss = 0.23, batch loss = 0.20 (33.4 examples/sec; 0.239 sec/batch; 21h:05m:29s remains)
INFO - root - 2017-12-15 07:00:48.309717: step 15170, loss = 0.28, batch loss = 0.25 (36.5 examples/sec; 0.219 sec/batch; 19h:19m:27s remains)
INFO - root - 2017-12-15 07:00:50.567687: step 15180, loss = 0.22, batch loss = 0.19 (34.6 examples/sec; 0.231 sec/batch; 20h:22m:16s remains)
INFO - root - 2017-12-15 07:00:52.818312: step 15190, loss = 0.26, batch loss = 0.23 (34.5 examples/sec; 0.232 sec/batch; 20h:27m:57s remains)
INFO - root - 2017-12-15 07:00:55.083538: step 15200, loss = 0.25, batch loss = 0.22 (35.0 examples/sec; 0.228 sec/batch; 20h:07m:51s remains)
2017-12-15 07:00:55.423848: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2115285 -3.2676964 -3.027113 -2.1931815 -1.3764791 -1.5050075 -2.0177174 -2.3374374 -2.3366694 -2.3830469 -2.7810564 -3.8354568 -4.6686163 -5.1991949 -5.3631325][-3.7646406 -4.6889849 -4.5702353 -3.6635942 -2.6464837 -2.5356104 -3.0176344 -3.3697684 -3.5196559 -3.6844144 -4.470221 -5.8137751 -6.5274534 -6.5716782 -6.3549194][-4.5086436 -4.8149996 -4.7959118 -4.1737175 -3.1997113 -2.8175504 -3.0972328 -3.4302456 -3.5489862 -3.6882782 -4.6709957 -6.1270857 -6.9294119 -6.9969616 -6.6764574][-4.6827116 -4.3362627 -4.334518 -4.0504522 -3.26761 -2.8177853 -2.8356929 -3.0224028 -3.0891414 -3.3121095 -4.3625493 -5.7151918 -6.6327467 -6.9290628 -6.5979662][-4.7751555 -3.715538 -3.6938596 -3.7559972 -3.054513 -2.3050785 -2.0105355 -2.0113773 -2.0942709 -2.6844149 -3.8205144 -5.0965118 -6.1098428 -6.627183 -6.3324089][-4.2819014 -3.0538318 -3.0107806 -3.1625559 -2.3543537 -1.292601 -0.55853558 -0.2767235 -0.46237564 -1.4053638 -2.6000547 -3.9234364 -5.1492519 -5.8973336 -5.7631526][-2.9044757 -2.0878134 -2.0139611 -2.0419405 -1.1355075 0.021785259 1.0054643 1.5237374 1.3614516 0.32253695 -1.0031291 -2.5682383 -4.2106695 -5.2574196 -5.3474693][-2.4261804 -1.7685862 -1.5953829 -1.4532535 -0.48309004 0.62071228 1.7737863 2.5200469 2.4746025 1.4062536 -0.056125641 -1.9211059 -3.9016075 -4.9905834 -5.196034][-2.3692052 -2.0222251 -1.7379667 -1.4148617 -0.38684428 0.76314831 1.9950411 2.9162867 2.9526393 1.8175995 0.12071872 -2.02883 -4.1417494 -5.1306152 -5.3285375][-2.7264662 -2.6572845 -2.3915377 -2.0870936 -1.2318158 -0.24000013 0.93367052 2.0068514 2.1897476 1.0027144 -0.8946203 -3.0478218 -4.9254465 -5.6617045 -5.7092171][-3.4336085 -3.5130153 -3.3829265 -3.3607056 -2.979219 -2.3607655 -1.4522153 -0.35512817 0.03468895 -0.97217917 -2.730984 -4.620636 -5.9771786 -6.31559 -6.17648][-4.0331731 -4.1390605 -4.09805 -4.2997437 -4.2627773 -3.8276167 -3.1140726 -2.173002 -1.8961836 -2.6679552 -4.0301285 -5.5311284 -6.5281219 -6.6804342 -6.4094114][-4.7851014 -4.9879456 -4.9804554 -5.1948433 -5.2689915 -4.8322935 -4.3277597 -3.8633485 -3.7893105 -4.2785654 -5.2713852 -6.3793373 -6.9560022 -6.8814707 -6.4223614][-6.3834438 -6.3128376 -6.2634792 -6.5041838 -6.68631 -6.3579206 -6.0182457 -5.8046088 -5.8353319 -6.0584431 -6.5991898 -7.2309875 -7.3560743 -6.960906 -6.2912769][-6.9950123 -6.5160036 -6.4241228 -6.678216 -6.9187241 -6.7454023 -6.5424204 -6.49928 -6.5757837 -6.7368164 -7.1015081 -7.4310293 -7.2683992 -6.7030697 -5.9533558]]...]
INFO - root - 2017-12-15 07:00:57.697552: step 15210, loss = 0.26, batch loss = 0.22 (35.8 examples/sec; 0.224 sec/batch; 19h:42m:32s remains)
INFO - root - 2017-12-15 07:00:59.937651: step 15220, loss = 0.20, batch loss = 0.17 (35.8 examples/sec; 0.224 sec/batch; 19h:42m:26s remains)
INFO - root - 2017-12-15 07:01:02.253971: step 15230, loss = 0.28, batch loss = 0.24 (33.8 examples/sec; 0.236 sec/batch; 20h:49m:50s remains)
INFO - root - 2017-12-15 07:01:04.562940: step 15240, loss = 0.24, batch loss = 0.21 (35.1 examples/sec; 0.228 sec/batch; 20h:03m:31s remains)
INFO - root - 2017-12-15 07:01:06.833957: step 15250, loss = 0.25, batch loss = 0.21 (35.5 examples/sec; 0.225 sec/batch; 19h:50m:18s remains)
INFO - root - 2017-12-15 07:01:09.106378: step 15260, loss = 0.16, batch loss = 0.12 (34.0 examples/sec; 0.235 sec/batch; 20h:44m:38s remains)
INFO - root - 2017-12-15 07:01:11.401211: step 15270, loss = 0.21, batch loss = 0.18 (36.3 examples/sec; 0.221 sec/batch; 19h:26m:04s remains)
INFO - root - 2017-12-15 07:01:13.657181: step 15280, loss = 0.35, batch loss = 0.31 (32.7 examples/sec; 0.245 sec/batch; 21h:34m:20s remains)
INFO - root - 2017-12-15 07:01:15.963886: step 15290, loss = 0.29, batch loss = 0.26 (33.4 examples/sec; 0.240 sec/batch; 21h:06m:51s remains)
INFO - root - 2017-12-15 07:01:18.233536: step 15300, loss = 0.20, batch loss = 0.17 (33.6 examples/sec; 0.238 sec/batch; 20h:58m:08s remains)
2017-12-15 07:01:18.545745: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.743075 -0.88644671 -1.5518036 -3.0056236 -4.5993834 -5.7458611 -5.6583338 -5.4322081 -5.0480542 -4.4037285 -3.153739 -1.4782897 0.033923626 0.54400253 -0.020717621][-1.6094458 -0.58705819 -1.694294 -3.5087786 -5.213665 -6.2189331 -5.9237657 -5.2717247 -4.689517 -4.114397 -3.2358525 -1.9692988 -0.73396361 -0.34420574 -1.3080039][-1.9830909 -0.1998713 -1.4146187 -3.2015038 -4.8842611 -5.7845888 -5.42336 -4.3880939 -3.7736907 -3.4263182 -2.9160914 -2.1632135 -1.4636697 -1.5595493 -2.8533721][-1.9668772 0.20147514 -1.0408555 -2.5595756 -3.8090096 -4.3022132 -3.7855043 -2.8034189 -2.5177238 -2.4903231 -2.409313 -2.4110303 -2.2993646 -2.7940474 -4.3122463][-1.7837784 0.38136435 -0.63222611 -1.5681357 -2.2422178 -2.2749996 -1.4376528 -0.55947292 -0.79920292 -1.448987 -2.0533946 -2.6392496 -3.0733602 -3.8816454 -5.3711357][-2.2648392 -0.020815849 -0.55617261 -0.82996655 -0.80512452 -0.29210746 0.90834427 1.7245221 1.0164406 -0.44967878 -1.7801533 -2.7847958 -3.4256535 -4.3739362 -5.7330828][-2.8291223 -0.8971945 -1.0053532 -0.63333321 0.17446184 1.3450963 2.7494097 3.4174428 2.2868819 0.20059466 -1.6880286 -2.8350437 -3.3776135 -4.1178513 -5.136652][-4.2541838 -1.7815988 -1.5171759 -0.772107 0.63349652 2.2037935 3.512116 3.9499054 2.6227303 0.27111912 -1.9095021 -2.9968729 -3.3287663 -3.7990212 -4.3791251][-5.3527851 -2.5986764 -1.9328213 -0.932794 0.59941316 2.0863104 2.8780894 2.9722528 1.6389525 -0.45615518 -2.2732568 -3.0005946 -3.1513391 -3.536962 -3.9269094][-6.6237192 -3.5357137 -2.6168163 -1.6238514 -0.28123295 0.86270404 1.1250191 0.83249664 -0.29078496 -1.691798 -2.844964 -3.1545627 -3.1423962 -3.4369373 -3.8167291][-7.3322692 -4.298501 -3.2921238 -2.4260533 -1.4408584 -0.75273275 -0.914431 -1.4410878 -2.3536382 -2.99167 -3.3517213 -3.3301215 -3.1539388 -3.327213 -3.6692314][-6.983778 -4.3286409 -3.6087222 -3.1177638 -2.5863078 -2.258446 -2.4778202 -3.1218309 -3.9602337 -4.1118965 -3.9539766 -3.7161145 -3.5566459 -3.5466099 -3.6154637][-5.6976881 -3.5895243 -3.3495564 -3.4845893 -3.5182743 -3.4903955 -3.70997 -4.3484545 -5.069272 -5.0230289 -4.6257877 -4.3338614 -4.1917033 -4.0113134 -3.7413716][-4.1767931 -2.6440282 -2.7265849 -3.2536514 -3.6739614 -4.0050631 -4.3088064 -4.7318077 -5.2983265 -5.2260633 -4.8551579 -4.6401682 -4.611083 -4.4124947 -3.9081717][-3.044693 -1.9943359 -2.2445705 -2.829983 -3.2051251 -3.6132245 -3.9834862 -4.3226023 -4.73763 -4.7868605 -4.5910444 -4.4245024 -4.409677 -4.2188625 -3.7149863]]...]
INFO - root - 2017-12-15 07:01:20.834531: step 15310, loss = 0.27, batch loss = 0.24 (34.7 examples/sec; 0.230 sec/batch; 20h:18m:27s remains)
INFO - root - 2017-12-15 07:01:23.141208: step 15320, loss = 0.36, batch loss = 0.32 (33.9 examples/sec; 0.236 sec/batch; 20h:49m:12s remains)
INFO - root - 2017-12-15 07:01:25.425452: step 15330, loss = 0.24, batch loss = 0.21 (34.4 examples/sec; 0.232 sec/batch; 20h:27m:43s remains)
INFO - root - 2017-12-15 07:01:27.682139: step 15340, loss = 0.31, batch loss = 0.28 (35.7 examples/sec; 0.224 sec/batch; 19h:43m:07s remains)
INFO - root - 2017-12-15 07:01:29.987520: step 15350, loss = 0.34, batch loss = 0.30 (34.8 examples/sec; 0.230 sec/batch; 20h:14m:50s remains)
INFO - root - 2017-12-15 07:01:32.260663: step 15360, loss = 0.22, batch loss = 0.19 (34.9 examples/sec; 0.229 sec/batch; 20h:12m:35s remains)
INFO - root - 2017-12-15 07:01:34.552502: step 15370, loss = 0.25, batch loss = 0.21 (34.5 examples/sec; 0.232 sec/batch; 20h:25m:46s remains)
INFO - root - 2017-12-15 07:01:36.821201: step 15380, loss = 0.19, batch loss = 0.15 (34.0 examples/sec; 0.235 sec/batch; 20h:42m:14s remains)
INFO - root - 2017-12-15 07:01:39.149534: step 15390, loss = 0.29, batch loss = 0.25 (33.5 examples/sec; 0.239 sec/batch; 21h:01m:02s remains)
INFO - root - 2017-12-15 07:01:41.406548: step 15400, loss = 0.25, batch loss = 0.22 (36.1 examples/sec; 0.222 sec/batch; 19h:32m:11s remains)
2017-12-15 07:01:41.704089: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6375589 -4.6973495 -5.1783867 -5.5468521 -5.7446136 -5.8892603 -5.8073335 -5.4700775 -5.4725218 -6.1049042 -6.8521519 -7.1272612 -7.0027275 -6.525712 -5.5221415][-4.3098412 -5.6907768 -6.2019343 -6.42714 -6.4042339 -6.3702965 -6.1545248 -5.6827312 -5.7391005 -6.5561533 -7.1404953 -7.008935 -6.4689255 -5.6414618 -4.1757183][-5.304781 -6.240118 -6.6051207 -6.5199003 -6.2011652 -5.8621368 -5.3385181 -4.7516527 -5.0572519 -6.0433364 -6.5154085 -6.1673765 -5.4074669 -4.2686157 -2.2504842][-6.5211248 -6.5148811 -6.6295815 -6.0741978 -5.2972369 -4.4449577 -3.5932937 -2.9956667 -3.4998109 -4.5634522 -5.12925 -4.9464941 -4.281744 -3.0507579 -0.59540713][-6.9758768 -6.4574089 -6.184021 -5.0604353 -3.7292142 -2.2838273 -1.0451872 -0.47047257 -1.2328135 -2.6831281 -3.8098059 -4.1221991 -3.8181195 -2.7307472 -0.1336813][-6.8907547 -5.8464079 -5.2291369 -3.6537249 -1.8419427 0.25078034 1.8965218 2.5564373 1.5520403 -0.45575213 -2.430845 -3.506618 -3.8131132 -3.271029 -1.2910376][-5.7197523 -5.0611334 -4.3662968 -2.5750344 -0.3165381 2.4970977 4.8646126 5.8081121 4.5559244 1.9161642 -0.94337046 -2.9409375 -3.9858954 -4.0927544 -2.9182806][-5.186151 -4.6241941 -4.06561 -2.3296616 0.0762279 3.3433983 6.0185776 6.9999847 5.6064348 2.6862204 -0.61412942 -3.0294566 -4.4893551 -5.1281438 -4.7013493][-5.1589851 -4.8706255 -4.5621686 -3.1934056 -1.0763741 1.9307067 4.57345 5.6411963 4.5319118 1.856396 -1.2956858 -3.7107816 -5.2510905 -6.0952406 -6.0807133][-5.8252621 -5.8205976 -5.7608037 -4.7627316 -2.9909263 -0.26536882 2.1596239 3.1263902 2.2242868 0.032002211 -2.646184 -4.6042528 -5.8829575 -6.6604614 -6.6079531][-7.219265 -7.3665419 -7.3878365 -6.5843134 -5.0531492 -2.7238193 -0.61647511 0.2489965 -0.4181577 -2.0831358 -4.0253668 -5.3901434 -6.1053724 -6.2264128 -5.4500613][-8.5717125 -8.6449394 -8.6835241 -8.1040993 -6.9256182 -4.9245839 -3.1345398 -2.4837449 -3.0257969 -3.9747157 -5.0095215 -5.516223 -5.4368515 -4.613462 -3.0713468][-9.0996952 -9.0540457 -9.0761509 -8.492919 -7.3729868 -5.7301936 -4.5264006 -4.211812 -4.528481 -4.7586021 -4.968657 -4.8627338 -4.1683455 -2.4252872 -0.15132189][-9.0792112 -8.7144947 -8.3798237 -7.3907967 -5.9920535 -4.5266962 -3.8553276 -3.8930817 -4.3475084 -4.5403643 -4.6369762 -4.3197994 -3.2047997 -0.55227697 2.5847194][-8.2778673 -7.339262 -6.5752716 -4.9303164 -3.0085363 -1.7085789 -1.5793536 -2.2206085 -3.2731605 -3.9869051 -4.2649107 -4.0464344 -2.6846058 0.41695142 3.9746912]]...]
INFO - root - 2017-12-15 07:01:44.002975: step 15410, loss = 0.16, batch loss = 0.13 (35.9 examples/sec; 0.223 sec/batch; 19h:37m:55s remains)
INFO - root - 2017-12-15 07:01:46.290752: step 15420, loss = 0.17, batch loss = 0.14 (34.1 examples/sec; 0.234 sec/batch; 20h:38m:28s remains)
INFO - root - 2017-12-15 07:01:48.559958: step 15430, loss = 0.28, batch loss = 0.25 (35.7 examples/sec; 0.224 sec/batch; 19h:42m:57s remains)
INFO - root - 2017-12-15 07:01:50.842903: step 15440, loss = 0.24, batch loss = 0.20 (35.0 examples/sec; 0.229 sec/batch; 20h:07m:47s remains)
INFO - root - 2017-12-15 07:01:53.140665: step 15450, loss = 0.24, batch loss = 0.20 (33.8 examples/sec; 0.236 sec/batch; 20h:49m:22s remains)
INFO - root - 2017-12-15 07:01:55.449607: step 15460, loss = 0.26, batch loss = 0.22 (31.9 examples/sec; 0.251 sec/batch; 22h:06m:43s remains)
INFO - root - 2017-12-15 07:01:57.700228: step 15470, loss = 0.35, batch loss = 0.31 (36.8 examples/sec; 0.218 sec/batch; 19h:10m:05s remains)
INFO - root - 2017-12-15 07:01:59.955562: step 15480, loss = 0.21, batch loss = 0.18 (37.2 examples/sec; 0.215 sec/batch; 18h:56m:12s remains)
INFO - root - 2017-12-15 07:02:02.203852: step 15490, loss = 0.25, batch loss = 0.21 (35.4 examples/sec; 0.226 sec/batch; 19h:55m:17s remains)
INFO - root - 2017-12-15 07:02:04.485774: step 15500, loss = 0.24, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 19h:34m:03s remains)
2017-12-15 07:02:04.807786: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1921321 -2.9187348 -3.5859604 -3.924957 -3.9788175 -5.1299796 -6.3384342 -6.99104 -7.2296429 -6.7740593 -5.8586063 -5.6364021 -5.5452785 -5.17795 -4.2422395][-1.5597262 -2.7597852 -3.5842752 -4.2994528 -4.612114 -5.6545477 -6.6655836 -7.1016626 -7.2779942 -7.2384796 -6.6210279 -6.5815592 -6.5811415 -6.0769691 -4.6797385][-3.1919773 -3.7824464 -4.5881834 -5.4265089 -5.5987635 -5.896287 -6.1839333 -6.2878284 -6.5517139 -7.1777763 -7.1861172 -7.4601774 -7.7066545 -7.1485405 -5.3673291][-5.1444235 -5.2349863 -5.734446 -5.9721642 -5.3556614 -4.5537939 -3.8711126 -3.7759056 -4.4785805 -5.7581081 -6.5677872 -7.37521 -7.8679047 -7.5409994 -5.6378717][-6.4625063 -6.5138741 -6.4577942 -5.7590766 -4.0846415 -1.9944177 -0.21502173 -0.029159546 -1.2489202 -3.2060447 -4.8703957 -6.2348137 -7.0132847 -6.9240227 -5.214201][-8.05937 -7.3220539 -6.4750509 -4.8298936 -2.1709962 0.9318409 3.5445092 3.9026334 2.262305 -0.24482155 -2.7086761 -4.4763327 -5.4267464 -5.623457 -4.3389964][-7.9043093 -6.6027174 -5.0134459 -2.8115125 0.22680926 3.5997913 6.5906191 6.9623575 4.7789755 1.8263905 -1.0563021 -3.035929 -3.9901767 -4.4422159 -3.8340869][-7.9021559 -5.8031144 -3.6529932 -1.0984837 1.9934304 5.3514223 8.3616009 8.6737671 6.1013784 2.8676255 -0.22746909 -2.3317566 -3.4374905 -4.2453279 -4.2987094][-7.8192377 -5.2909813 -2.9361773 -0.39618397 2.3287032 5.0564814 7.5871696 7.6515837 5.1160126 1.822211 -1.16252 -3.1906238 -4.4520993 -5.5028429 -5.674427][-7.3600264 -4.9744625 -2.9298768 -0.66926908 1.2574582 2.8877227 4.4519787 4.2075062 1.9472382 -1.001684 -3.628593 -5.2761383 -6.4381046 -7.31155 -6.9418869][-7.155241 -4.957921 -3.2080383 -1.2771654 -0.041136026 0.55434704 1.0237834 0.30930805 -1.6745013 -4.317193 -6.288147 -7.3306656 -7.9631462 -8.1556206 -7.07069][-6.5807686 -4.5849109 -3.2063086 -1.8008718 -1.338104 -1.6423345 -1.960566 -2.8802109 -4.3975964 -6.3376236 -7.4107685 -7.7194538 -7.6101065 -7.1594772 -5.5993443][-5.98223 -4.226409 -3.1473536 -2.3113117 -2.5885079 -3.3968039 -3.8698859 -4.6244569 -5.7313638 -7.1136522 -7.7134285 -7.6446314 -7.0163584 -5.99479 -3.905138][-5.7486038 -3.9748859 -2.97233 -2.3673913 -2.9954953 -4.080019 -4.5195 -5.098917 -5.9080772 -6.7049379 -6.9497185 -6.5877819 -5.6004291 -4.3207016 -2.0524182][-5.8246984 -4.0624428 -3.0790689 -2.5542386 -3.2688789 -4.2349796 -4.538434 -5.02132 -5.4719582 -5.744648 -6.0047407 -5.6756053 -4.6900568 -3.6769125 -1.770606]]...]
INFO - root - 2017-12-15 07:02:07.086580: step 15510, loss = 0.21, batch loss = 0.17 (35.2 examples/sec; 0.228 sec/batch; 20h:02m:15s remains)
INFO - root - 2017-12-15 07:02:09.388900: step 15520, loss = 0.22, batch loss = 0.18 (34.6 examples/sec; 0.231 sec/batch; 20h:22m:48s remains)
INFO - root - 2017-12-15 07:02:11.673349: step 15530, loss = 0.28, batch loss = 0.25 (35.3 examples/sec; 0.227 sec/batch; 19h:56m:57s remains)
INFO - root - 2017-12-15 07:02:13.936004: step 15540, loss = 0.30, batch loss = 0.27 (36.3 examples/sec; 0.220 sec/batch; 19h:23m:00s remains)
INFO - root - 2017-12-15 07:02:16.245577: step 15550, loss = 0.28, batch loss = 0.25 (36.3 examples/sec; 0.221 sec/batch; 19h:25m:40s remains)
INFO - root - 2017-12-15 07:02:18.507210: step 15560, loss = 0.26, batch loss = 0.23 (34.7 examples/sec; 0.230 sec/batch; 20h:17m:20s remains)
INFO - root - 2017-12-15 07:02:20.762838: step 15570, loss = 0.35, batch loss = 0.32 (34.4 examples/sec; 0.233 sec/batch; 20h:28m:19s remains)
INFO - root - 2017-12-15 07:02:23.015927: step 15580, loss = 0.24, batch loss = 0.20 (35.9 examples/sec; 0.223 sec/batch; 19h:37m:06s remains)
INFO - root - 2017-12-15 07:02:25.283019: step 15590, loss = 0.36, batch loss = 0.32 (35.7 examples/sec; 0.224 sec/batch; 19h:43m:23s remains)
INFO - root - 2017-12-15 07:02:27.571540: step 15600, loss = 0.15, batch loss = 0.12 (34.9 examples/sec; 0.229 sec/batch; 20h:11m:13s remains)
2017-12-15 07:02:27.854465: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.25126529 -1.58803 -2.9130325 -3.8470635 -4.0073824 -3.821049 -3.7989855 -3.6675847 -3.283494 -2.6641457 -1.9098397 -1.4974036 -1.5569594 -1.7353188 -1.6638688][-0.83085406 -1.3997676 -2.6424122 -3.4941919 -3.3175263 -2.8485558 -2.5629086 -2.4754674 -2.5568073 -2.5894778 -2.2918487 -1.9432323 -2.0727451 -2.3392496 -2.3521388][-2.0545058 -1.8251027 -2.7677319 -3.1695235 -2.5576868 -1.8177762 -1.3043063 -1.3074522 -1.7980263 -2.4732356 -2.6657467 -2.5354171 -2.8859689 -3.4482279 -3.6332097][-2.9788945 -2.3430595 -2.6801186 -2.4550948 -1.4572377 -0.4265759 0.11043692 -0.11126924 -1.0349827 -2.2813382 -2.8722038 -3.0394537 -3.4628086 -3.9461951 -4.0542474][-4.0770059 -2.3910291 -2.0789623 -1.2220697 0.21742749 1.4326556 1.760021 1.1936562 -0.31968749 -2.1756198 -3.1432204 -3.3938193 -3.6128078 -3.773515 -3.7844403][-4.2002249 -2.0723922 -1.2713547 0.092192173 1.7820988 3.010829 3.1261082 2.2865391 0.29609895 -2.0260148 -3.2111902 -3.374022 -3.3552027 -3.4426508 -3.4845591][-3.49304 -1.3732648 -0.2854197 1.2189157 2.7846885 3.98315 3.8902125 2.8985505 0.69895005 -1.7889073 -2.943433 -2.9188218 -2.6014147 -2.7194865 -2.770195][-3.1461959 -0.900108 0.41990185 1.9704299 3.4426141 4.5001659 4.2064657 3.1966562 1.1336861 -1.1915568 -2.1937058 -1.868723 -1.2356507 -1.3312602 -1.5460677][-3.3589478 -0.83522213 0.61207557 1.9893174 3.2774076 4.2097 3.9178214 3.0600457 1.2921028 -0.67049897 -1.4067277 -0.75173843 0.18287873 0.11318302 -0.36039507][-3.6141777 -0.95873213 0.39335442 1.3741782 2.2194128 2.9382572 2.7064519 2.0465407 0.67554855 -0.76815331 -1.1623719 -0.19791913 0.96029425 1.0658088 0.56544662][-4.7548704 -2.114429 -0.82898951 -0.187958 0.23819709 0.71439672 0.64374232 0.26217961 -0.54907238 -1.4066372 -1.5600262 -0.45720994 0.91860819 1.4076729 1.2032645][-6.992878 -4.3756046 -3.3092303 -2.933073 -2.6936748 -2.2486739 -2.0521004 -2.0206482 -2.2585545 -2.5287726 -2.4873302 -1.4212501 0.10164261 1.0382671 1.1438544][-8.5431929 -6.3720384 -5.9122229 -5.8451514 -5.6915288 -5.2628422 -4.96772 -4.7097483 -4.4816628 -4.2643175 -4.0549173 -3.0756631 -1.5803123 -0.50044513 -0.24705088][-8.9809151 -7.440341 -7.4215856 -7.5769653 -7.622366 -7.4313979 -7.2909088 -6.9408937 -6.3290286 -5.7869892 -5.432106 -4.57455 -3.2203937 -2.2117047 -1.9474406][-8.4872169 -7.2900772 -7.3085203 -7.5794616 -7.870029 -7.8542304 -7.91641 -7.6096215 -6.8756866 -6.2903724 -5.95958 -5.4012861 -4.4885912 -3.7873275 -3.5507622]]...]
INFO - root - 2017-12-15 07:02:30.105281: step 15610, loss = 0.32, batch loss = 0.28 (35.0 examples/sec; 0.229 sec/batch; 20h:07m:45s remains)
INFO - root - 2017-12-15 07:02:32.367120: step 15620, loss = 0.27, batch loss = 0.24 (36.4 examples/sec; 0.220 sec/batch; 19h:20m:38s remains)
INFO - root - 2017-12-15 07:02:34.664103: step 15630, loss = 0.27, batch loss = 0.24 (35.0 examples/sec; 0.229 sec/batch; 20h:07m:46s remains)
INFO - root - 2017-12-15 07:02:36.960560: step 15640, loss = 0.25, batch loss = 0.21 (35.8 examples/sec; 0.223 sec/batch; 19h:39m:54s remains)
INFO - root - 2017-12-15 07:02:39.249586: step 15650, loss = 0.33, batch loss = 0.30 (35.8 examples/sec; 0.224 sec/batch; 19h:40m:52s remains)
INFO - root - 2017-12-15 07:02:41.540048: step 15660, loss = 0.26, batch loss = 0.22 (32.4 examples/sec; 0.247 sec/batch; 21h:44m:46s remains)
INFO - root - 2017-12-15 07:02:43.800267: step 15670, loss = 0.24, batch loss = 0.20 (36.1 examples/sec; 0.222 sec/batch; 19h:30m:21s remains)
INFO - root - 2017-12-15 07:02:46.106772: step 15680, loss = 0.21, batch loss = 0.17 (34.8 examples/sec; 0.230 sec/batch; 20h:12m:41s remains)
INFO - root - 2017-12-15 07:02:48.375048: step 15690, loss = 0.27, batch loss = 0.23 (35.6 examples/sec; 0.224 sec/batch; 19h:44m:56s remains)
INFO - root - 2017-12-15 07:02:50.622191: step 15700, loss = 0.24, batch loss = 0.20 (33.9 examples/sec; 0.236 sec/batch; 20h:44m:50s remains)
2017-12-15 07:02:50.961300: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5684013 -5.2128906 -5.1134171 -5.314548 -5.2452288 -5.4337454 -5.9648948 -6.5037351 -6.69838 -7.0094848 -6.886692 -6.5489511 -6.3245864 -6.0331578 -5.7758913][-5.2804651 -5.3986416 -5.5464444 -5.6520796 -5.2155561 -5.1526127 -5.5071177 -5.769433 -5.8716025 -6.457829 -6.2926311 -6.032855 -6.0463476 -5.9205246 -5.8163986][-5.8909245 -5.0269938 -5.2552748 -5.3661785 -4.5878329 -4.2014818 -4.3648081 -4.4417381 -4.5962386 -5.7497292 -5.8763971 -5.6608419 -5.6637292 -5.5642223 -5.5742226][-6.2220249 -4.25589 -4.4693065 -4.5912666 -3.5268483 -2.5683663 -2.3653183 -2.523706 -3.0498955 -4.8615036 -5.4337096 -5.353569 -5.2173815 -5.1510315 -5.3861094][-5.2906761 -3.0479505 -3.1341763 -3.1855111 -1.8537608 -0.035284758 0.928602 0.54729986 -0.73907077 -3.2345252 -4.3395672 -4.6094594 -4.4605703 -4.43466 -4.8971062][-4.8002234 -2.2382274 -2.1271188 -1.8478937 -0.22021401 2.4556017 4.2767677 3.6953526 1.6616826 -1.1616795 -2.7741787 -3.5543118 -3.5988188 -3.6961842 -4.2580051][-4.1255364 -1.8294971 -1.5945148 -0.87942851 1.1697214 4.470089 6.8001571 6.2258391 3.8766866 0.93915224 -1.0605505 -2.339848 -2.6962633 -2.8921771 -3.5332036][-4.4338894 -1.7717062 -1.6574794 -0.78711915 1.5039263 4.9122849 7.2267165 6.866611 4.6908894 2.0811439 0.05672431 -1.3993337 -2.0871091 -2.4417861 -3.0992146][-5.2069912 -2.6473966 -2.9515598 -2.4057908 -0.214208 2.912631 4.9129362 5.0240116 3.5808525 1.5198076 -0.15011239 -1.5702248 -2.5145531 -2.9227817 -3.4047112][-5.855875 -3.5030751 -4.2655973 -4.2133169 -2.4216437 0.11382413 1.6146576 2.1315808 1.4474354 -0.11628079 -1.4135187 -2.3687801 -3.3817539 -3.9086988 -4.0537558][-6.3610406 -4.2427025 -5.3575912 -5.9175773 -4.843967 -2.9201372 -1.942454 -1.1760466 -1.188069 -2.3489099 -3.2579434 -3.6300263 -4.5954905 -5.3330374 -5.2059207][-6.9078207 -5.1092443 -6.3068738 -7.1480227 -6.7219415 -5.2119761 -4.5158644 -3.7072682 -3.361619 -4.2132344 -5.0564442 -5.0872235 -5.82183 -6.6234822 -6.3402591][-7.1815166 -5.6773615 -6.6925373 -7.5505943 -7.4554448 -6.0076985 -5.3033609 -4.6615624 -4.451189 -5.217556 -6.1409845 -6.2546697 -6.8662314 -7.486146 -7.1839976][-7.3478646 -6.0925417 -6.9431162 -7.622745 -7.508791 -6.1318436 -5.3288832 -4.8760529 -4.91253 -5.6748629 -6.6276684 -7.0145531 -7.4370637 -7.5920525 -7.2047787][-7.3404975 -6.4318986 -7.1222258 -7.5614724 -7.2714458 -5.9836912 -5.029201 -4.6338534 -4.8758512 -5.6263733 -6.5598459 -7.0690112 -7.2983265 -7.1188197 -6.831481]]...]
INFO - root - 2017-12-15 07:02:53.215838: step 15710, loss = 0.22, batch loss = 0.18 (33.2 examples/sec; 0.241 sec/batch; 21h:12m:09s remains)
INFO - root - 2017-12-15 07:02:55.506809: step 15720, loss = 0.24, batch loss = 0.20 (36.1 examples/sec; 0.222 sec/batch; 19h:29m:32s remains)
INFO - root - 2017-12-15 07:02:57.774908: step 15730, loss = 0.29, batch loss = 0.25 (35.3 examples/sec; 0.227 sec/batch; 19h:57m:15s remains)
INFO - root - 2017-12-15 07:03:00.028832: step 15740, loss = 0.26, batch loss = 0.23 (35.0 examples/sec; 0.228 sec/batch; 20h:05m:06s remains)
INFO - root - 2017-12-15 07:03:02.265979: step 15750, loss = 0.28, batch loss = 0.25 (35.5 examples/sec; 0.225 sec/batch; 19h:50m:07s remains)
INFO - root - 2017-12-15 07:03:04.493647: step 15760, loss = 0.19, batch loss = 0.15 (35.8 examples/sec; 0.223 sec/batch; 19h:39m:10s remains)
INFO - root - 2017-12-15 07:03:06.752863: step 15770, loss = 0.19, batch loss = 0.16 (35.1 examples/sec; 0.228 sec/batch; 20h:04m:21s remains)
INFO - root - 2017-12-15 07:03:08.971963: step 15780, loss = 0.24, batch loss = 0.21 (35.2 examples/sec; 0.227 sec/batch; 19h:58m:39s remains)
INFO - root - 2017-12-15 07:03:11.233578: step 15790, loss = 0.31, batch loss = 0.28 (35.3 examples/sec; 0.227 sec/batch; 19h:55m:42s remains)
INFO - root - 2017-12-15 07:03:13.473654: step 15800, loss = 0.33, batch loss = 0.29 (35.4 examples/sec; 0.226 sec/batch; 19h:52m:12s remains)
2017-12-15 07:03:13.735835: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3358717 -7.3335686 -7.1274471 -6.734725 -6.4236803 -6.4696226 -6.5335083 -6.3207169 -6.0712805 -6.060956 -6.1475878 -5.9047685 -5.4703307 -4.9158306 -4.6533613][-6.51038 -7.9401865 -7.7964354 -7.3543549 -6.7368097 -6.5718279 -6.3823643 -5.9306631 -5.6180091 -5.6331425 -5.9566669 -5.9702024 -5.7479649 -5.3554945 -5.3661184][-7.2938156 -7.8842592 -7.7985258 -7.3698282 -6.5227308 -6.098876 -5.57874 -4.9640017 -4.7372794 -4.9463959 -5.6334295 -5.8950882 -5.8547716 -5.6929665 -6.0984263][-7.7385406 -7.860754 -7.7262697 -7.2463064 -6.2834396 -5.6164355 -4.6922574 -3.8291266 -3.5980577 -3.8923571 -4.9414 -5.592659 -5.93866 -6.1953154 -7.0297022][-7.848218 -7.6822119 -7.1626673 -6.2428246 -4.8816004 -3.5515492 -1.8548735 -0.38192821 0.049799442 -0.44361079 -1.9859281 -3.2464314 -4.2935162 -5.2566357 -6.6665106][-7.9602251 -7.4714975 -6.5227633 -5.1088295 -3.3697171 -1.4755104 0.8664093 2.9410498 3.6526206 2.8843005 0.78217173 -0.85154021 -2.3362722 -3.8110056 -5.5551639][-7.4030962 -7.3782415 -6.2069736 -4.4632549 -2.4876797 -0.31852019 2.37795 5.0086794 6.1270504 4.9696922 2.3345015 0.60364079 -1.0942153 -3.0123086 -5.0477304][-7.1541815 -7.1677322 -5.9154534 -3.7901068 -1.4263582 1.0472271 3.8781874 6.761754 7.929471 6.2046995 3.2580597 1.5362041 -0.3778019 -2.7520339 -5.0320873][-6.8450222 -7.1070452 -6.1572781 -4.1604452 -1.8293148 0.51950622 2.8432338 5.1926737 5.9449053 4.006424 1.4854679 0.30341172 -1.3558359 -3.6039419 -5.6353536][-6.6027803 -7.1076384 -6.7892628 -5.521287 -3.785713 -1.9255136 -0.35319591 1.2100239 1.5718932 0.054146051 -1.4895645 -1.9641082 -3.1798425 -5.0050755 -6.4458857][-6.3472557 -6.7700491 -6.8186369 -6.1821737 -5.1070747 -3.7053967 -2.613131 -1.6687927 -1.5272632 -2.7008698 -3.5029111 -3.5549822 -4.4332767 -5.8565159 -6.8544731][-6.1247673 -6.2495174 -6.4533482 -6.3950729 -6.1752796 -5.6208005 -5.1288853 -4.6587987 -4.7519393 -5.5101247 -5.6928663 -5.2926636 -5.5851059 -6.4400673 -6.8974533][-5.8426485 -5.6047 -5.7968478 -6.0955448 -6.4621096 -6.5401735 -6.4706173 -6.3976994 -6.4959688 -6.9315338 -6.7562513 -6.064106 -5.8681526 -6.15812 -6.24568][-5.4787989 -4.9123974 -4.9622231 -5.2578096 -5.755105 -6.0727777 -6.1319923 -6.1400142 -6.2118473 -6.4014482 -6.1194038 -5.4778509 -5.221962 -5.4013157 -5.5011272][-5.2459116 -4.527462 -4.5058022 -4.692266 -5.0390596 -5.2734394 -5.3883505 -5.4787245 -5.5859404 -5.6904807 -5.4817824 -5.0663762 -4.9216909 -5.0395336 -5.0963793]]...]
INFO - root - 2017-12-15 07:03:15.997138: step 15810, loss = 0.26, batch loss = 0.23 (34.9 examples/sec; 0.230 sec/batch; 20h:11m:32s remains)
INFO - root - 2017-12-15 07:03:18.246718: step 15820, loss = 0.24, batch loss = 0.20 (35.3 examples/sec; 0.227 sec/batch; 19h:55m:29s remains)
INFO - root - 2017-12-15 07:03:20.500348: step 15830, loss = 0.18, batch loss = 0.15 (35.7 examples/sec; 0.224 sec/batch; 19h:43m:32s remains)
INFO - root - 2017-12-15 07:03:22.747178: step 15840, loss = 0.28, batch loss = 0.24 (36.1 examples/sec; 0.222 sec/batch; 19h:30m:14s remains)
INFO - root - 2017-12-15 07:03:25.028186: step 15850, loss = 0.25, batch loss = 0.22 (33.6 examples/sec; 0.238 sec/batch; 20h:56m:55s remains)
INFO - root - 2017-12-15 07:03:27.269616: step 15860, loss = 0.24, batch loss = 0.21 (36.7 examples/sec; 0.218 sec/batch; 19h:10m:49s remains)
INFO - root - 2017-12-15 07:03:29.544025: step 15870, loss = 0.21, batch loss = 0.18 (35.3 examples/sec; 0.227 sec/batch; 19h:56m:58s remains)
INFO - root - 2017-12-15 07:03:31.767208: step 15880, loss = 0.25, batch loss = 0.22 (34.8 examples/sec; 0.230 sec/batch; 20h:13m:48s remains)
INFO - root - 2017-12-15 07:03:34.010348: step 15890, loss = 0.46, batch loss = 0.43 (35.0 examples/sec; 0.228 sec/batch; 20h:04m:52s remains)
INFO - root - 2017-12-15 07:03:36.271823: step 15900, loss = 0.28, batch loss = 0.24 (36.1 examples/sec; 0.222 sec/batch; 19h:29m:20s remains)
2017-12-15 07:03:36.534268: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.90515 -5.44709 -5.5343647 -5.59101 -5.6637478 -5.8373785 -6.0380383 -6.0248723 -5.653635 -5.454772 -5.5673094 -5.7540402 -5.8795042 -6.2325087 -6.5073452][-4.9810572 -5.4409761 -5.7833281 -5.8785391 -5.9379592 -5.9822378 -6.0995264 -5.97219 -5.5707655 -5.5035143 -5.7586918 -6.1049876 -6.3649006 -6.8447752 -7.1913223][-4.6927047 -4.7544832 -5.5497026 -5.74759 -5.6474905 -5.3564148 -5.0295315 -4.577363 -4.2567062 -4.5807357 -5.2680387 -5.9073887 -6.2584276 -6.8952541 -7.3680887][-4.1232796 -3.5732398 -4.6984224 -5.0279455 -4.7713165 -3.8555613 -2.7841251 -1.9517746 -1.9438444 -2.9986751 -4.3756404 -5.455369 -5.9561167 -6.671154 -7.2107906][-3.8259864 -2.4449611 -3.721868 -3.965611 -3.3249073 -1.5488281 0.42601848 1.8199193 1.368844 -0.64676964 -2.7950459 -4.2583809 -4.8408961 -5.568151 -6.1226759][-2.6598465 -1.3342772 -2.64468 -2.7430966 -1.6090326 0.78546715 3.4590976 5.2692528 4.4894066 1.7564189 -1.0660347 -2.6555295 -3.1121147 -3.8644378 -4.5483627][-1.9199884 -0.87906373 -1.9606949 -1.7812173 -0.31746662 2.3262188 5.3026361 7.2514009 6.360817 3.4648359 0.3862803 -1.1481113 -1.5897685 -2.4121494 -3.2650661][-2.7691696 -1.5440589 -2.2136075 -1.7846074 -0.33135426 2.1472824 4.9823551 6.7814026 6.0020704 3.4572732 0.84257722 -0.32432497 -0.78466189 -1.890157 -2.9004672][-4.0741763 -2.685601 -3.0354493 -2.5978801 -1.4863551 0.5022819 2.9418771 4.3944759 3.9612429 2.2110794 0.28355312 -0.54469645 -1.2824787 -2.7731769 -3.7443652][-5.4805994 -3.95571 -4.1947079 -3.9757791 -3.4929738 -2.2340567 -0.41654491 0.87397718 1.1015627 0.29345107 -0.9985832 -1.7491562 -2.9174638 -4.5128927 -5.2947178][-6.4358845 -4.825346 -4.984776 -5.0883031 -5.4045868 -4.9180031 -3.65983 -2.4410574 -1.715683 -1.8692993 -2.5602951 -3.3376317 -4.5802212 -5.9606714 -6.5058174][-6.70877 -5.0122256 -5.1905169 -5.6719189 -6.6623955 -6.6935778 -5.8154869 -4.8290186 -3.9864106 -3.8311119 -4.2198048 -5.0043674 -6.1365657 -7.2583141 -7.5274296][-6.6430607 -4.89021 -5.0865884 -5.7906694 -6.9563637 -7.2146864 -6.7692785 -6.1580658 -5.5644836 -5.40998 -5.7660866 -6.4584026 -7.3102283 -8.0041513 -8.0359879][-6.147109 -4.42727 -4.5872 -5.2695045 -6.2458134 -6.4986887 -6.3612642 -6.0932212 -5.8935986 -5.9986649 -6.4246216 -6.9569149 -7.3598995 -7.5252018 -7.4245195][-5.6132278 -4.09396 -4.2965841 -4.8979111 -5.599472 -5.6790752 -5.5718455 -5.4866161 -5.5568905 -5.8717432 -6.3248034 -6.6717234 -6.8005762 -6.7583275 -6.6756477]]...]
INFO - root - 2017-12-15 07:03:38.796643: step 15910, loss = 0.32, batch loss = 0.29 (34.4 examples/sec; 0.233 sec/batch; 20h:28m:01s remains)
INFO - root - 2017-12-15 07:03:41.057363: step 15920, loss = 0.23, batch loss = 0.20 (36.3 examples/sec; 0.220 sec/batch; 19h:21m:49s remains)
INFO - root - 2017-12-15 07:03:43.286522: step 15930, loss = 0.20, batch loss = 0.16 (36.5 examples/sec; 0.219 sec/batch; 19h:15m:29s remains)
INFO - root - 2017-12-15 07:03:45.577802: step 15940, loss = 0.27, batch loss = 0.23 (33.6 examples/sec; 0.238 sec/batch; 20h:57m:03s remains)
INFO - root - 2017-12-15 07:03:47.844850: step 15950, loss = 0.22, batch loss = 0.19 (34.9 examples/sec; 0.229 sec/batch; 20h:09m:16s remains)
INFO - root - 2017-12-15 07:03:50.089658: step 15960, loss = 0.22, batch loss = 0.18 (35.7 examples/sec; 0.224 sec/batch; 19h:42m:53s remains)
INFO - root - 2017-12-15 07:03:52.348989: step 15970, loss = 0.21, batch loss = 0.18 (35.7 examples/sec; 0.224 sec/batch; 19h:42m:02s remains)
INFO - root - 2017-12-15 07:03:54.608737: step 15980, loss = 0.22, batch loss = 0.18 (35.2 examples/sec; 0.227 sec/batch; 20h:00m:03s remains)
INFO - root - 2017-12-15 07:03:56.874680: step 15990, loss = 0.30, batch loss = 0.27 (34.8 examples/sec; 0.230 sec/batch; 20h:12m:27s remains)
INFO - root - 2017-12-15 07:03:59.131361: step 16000, loss = 0.20, batch loss = 0.16 (35.6 examples/sec; 0.225 sec/batch; 19h:46m:35s remains)
2017-12-15 07:03:59.421540: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.933641 -5.4519472 -5.03714 -5.9904771 -7.0016785 -7.3060074 -7.1407604 -6.8929653 -7.5392857 -8.4665337 -8.0481787 -7.1326895 -5.7761154 -4.8159809 -4.57628][-6.8052759 -5.6468468 -5.3510685 -6.44707 -7.6038275 -8.2094479 -7.8854828 -7.26626 -7.6274843 -8.7285185 -8.5428705 -7.6464128 -6.242219 -5.1103239 -4.8975382][-7.5818796 -5.988903 -6.0155263 -7.2608738 -8.47099 -8.9631233 -8.1315136 -6.8863392 -6.7899132 -7.8292618 -7.8305321 -7.1213293 -5.7342396 -4.4569626 -4.335722][-8.13979 -6.2003994 -6.340518 -7.6663504 -8.5812826 -8.4437494 -6.716095 -4.6369457 -3.9850576 -5.0726471 -5.7225327 -5.8103695 -4.9226346 -3.9414878 -4.0442691][-7.9812908 -5.2787766 -5.346714 -6.64087 -7.1757059 -6.1652646 -3.4100833 -0.53097367 0.41865873 -1.2336121 -3.1145124 -4.4340906 -4.345511 -3.6018815 -3.6059575][-6.2296638 -3.014323 -3.2277312 -4.8164873 -5.3791738 -3.9684072 -0.60448289 2.8559625 4.1678123 2.19442 -0.64571357 -2.8428905 -3.263947 -2.5394025 -2.4765861][-3.5162549 -0.29008365 -0.91356754 -3.1533399 -4.2028027 -2.8821778 0.79550076 4.8591604 6.8465042 4.9761457 1.5886347 -1.1136605 -1.9924549 -1.6246512 -1.8411825][-1.3219056 2.1457727 1.1853507 -1.4988215 -3.1361365 -2.2206631 1.3570178 5.5712385 7.8281622 5.907897 2.1659529 -0.79530048 -1.8835741 -1.5786171 -1.6031957][0.75583434 4.0907631 2.9997227 0.149225 -2.0861728 -2.1007452 0.49703789 4.0002003 5.9806843 4.2533569 0.90444279 -1.5576507 -2.2906692 -1.7086606 -1.4668326][1.3290403 4.0771179 2.8924887 0.098107338 -2.5293908 -3.4754398 -2.0359502 0.5403583 2.1495221 1.0253553 -1.3722523 -3.0666728 -3.4112215 -2.7400727 -2.3795755][-0.35371232 1.5818875 0.56261134 -1.7212086 -3.9805789 -4.956358 -4.1613455 -2.5427151 -1.7163279 -2.5745831 -4.2099576 -5.2156487 -5.1413765 -4.2591715 -3.6524992][-2.5200405 -1.1798331 -1.9075561 -3.5505154 -5.0608439 -5.6927395 -5.4211016 -4.819252 -4.7150345 -5.4244046 -6.5475836 -6.9249697 -6.3472872 -5.1975374 -4.3409615][-4.2455769 -3.4575293 -4.2192287 -5.5874853 -6.4623756 -6.656148 -6.6683874 -6.5005207 -6.4395504 -6.8414249 -7.6683149 -7.73252 -6.8996019 -5.7408981 -4.9251595][-5.9493852 -5.4533086 -6.2077141 -7.3021669 -7.6285095 -7.323102 -7.2824287 -7.0434265 -6.785387 -7.0787487 -7.84356 -7.7871857 -6.8554497 -5.8934836 -5.2992988][-6.7005968 -5.9645696 -6.3983841 -7.2469196 -7.452199 -6.9912548 -6.9741735 -6.699234 -6.3623447 -6.6107006 -7.1873722 -7.0197897 -6.1778021 -5.5448284 -5.3069267]]...]
INFO - root - 2017-12-15 07:04:01.697836: step 16010, loss = 0.24, batch loss = 0.21 (34.4 examples/sec; 0.233 sec/batch; 20h:27m:45s remains)
INFO - root - 2017-12-15 07:04:03.946283: step 16020, loss = 0.29, batch loss = 0.26 (36.3 examples/sec; 0.220 sec/batch; 19h:22m:42s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 07:04:06.210166: step 16030, loss = 0.20, batch loss = 0.17 (35.2 examples/sec; 0.227 sec/batch; 19h:59m:48s remains)
INFO - root - 2017-12-15 07:04:08.470396: step 16040, loss = 0.28, batch loss = 0.25 (35.2 examples/sec; 0.228 sec/batch; 19h:59m:57s remains)
INFO - root - 2017-12-15 07:04:10.724547: step 16050, loss = 0.24, batch loss = 0.20 (36.2 examples/sec; 0.221 sec/batch; 19h:24m:05s remains)
INFO - root - 2017-12-15 07:04:13.008749: step 16060, loss = 0.29, batch loss = 0.26 (35.2 examples/sec; 0.227 sec/batch; 19h:59m:45s remains)
INFO - root - 2017-12-15 07:04:15.293658: step 16070, loss = 0.23, batch loss = 0.20 (36.1 examples/sec; 0.222 sec/batch; 19h:28m:51s remains)
INFO - root - 2017-12-15 07:04:17.550952: step 16080, loss = 0.19, batch loss = 0.15 (36.7 examples/sec; 0.218 sec/batch; 19h:08m:25s remains)
INFO - root - 2017-12-15 07:04:19.806831: step 16090, loss = 0.27, batch loss = 0.24 (34.5 examples/sec; 0.232 sec/batch; 20h:21m:45s remains)
INFO - root - 2017-12-15 07:04:22.107762: step 16100, loss = 0.28, batch loss = 0.24 (35.8 examples/sec; 0.224 sec/batch; 19h:38m:39s remains)
2017-12-15 07:04:22.391416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5248022 -5.3637748 -5.5287628 -5.6656895 -5.7753105 -5.7191625 -5.784987 -5.771019 -5.6542025 -5.461422 -5.3068075 -5.209672 -5.2351322 -5.5376096 -5.7006397][-4.8320637 -5.6834869 -5.8366704 -5.9542322 -6.1669607 -6.2029228 -6.3261037 -6.2899423 -6.1588335 -5.9773688 -5.8474464 -5.7280378 -5.7112827 -5.9611268 -6.0266476][-5.9226985 -5.5704384 -5.6018748 -5.5488749 -5.6779995 -5.6731682 -5.7364407 -5.5829759 -5.4129524 -5.3261423 -5.3562341 -5.3075733 -5.3089924 -5.6095171 -5.7158141][-6.5149264 -5.19616 -5.0416985 -4.7070427 -4.5360656 -4.3081579 -4.1830773 -3.8886585 -3.6195693 -3.7051258 -4.1016469 -4.3222294 -4.4834967 -4.9064827 -5.1523428][-6.5663676 -4.383883 -3.9880123 -3.3795214 -2.7412903 -1.9411632 -1.3657188 -0.94758332 -0.7228235 -1.1742153 -2.1121387 -2.7838929 -3.2461627 -3.9170032 -4.4310884][-5.8790336 -3.323689 -2.666935 -1.8074555 -0.70357215 0.77028966 1.8894467 2.3230643 2.2615881 1.3648405 -0.17130303 -1.3681021 -2.1407502 -3.1037426 -3.8928685][-4.8185778 -2.4529 -1.6559758 -0.64122009 0.80383205 2.8611293 4.4887462 4.8684006 4.3937063 3.1190991 1.1215839 -0.55965555 -1.6764481 -2.8042054 -3.7042131][-4.9100676 -2.4986236 -1.6098762 -0.40043724 1.2037077 3.5656924 5.5086107 5.8862104 4.9772053 3.3936806 1.2767398 -0.44024658 -1.6401763 -2.8111644 -3.7572346][-5.566813 -3.3231616 -2.5214071 -1.1852888 0.41705775 2.6108923 4.406724 4.6222897 3.5058627 1.9809718 0.2466414 -1.0483496 -2.0537581 -3.2555871 -4.2861996][-6.298687 -4.401947 -3.8686533 -2.6078792 -1.1832231 0.53990293 1.9547377 1.9208312 1.0157306 -0.059675694 -1.245384 -2.1676626 -2.9912486 -4.0768509 -5.0147786][-6.889883 -5.3091269 -5.0106287 -3.9406881 -2.8910685 -1.8255069 -0.94781423 -1.3751218 -1.9465535 -2.4264052 -2.8882902 -3.3647463 -4.0089636 -5.0303612 -5.8654366][-7.2717023 -5.8911543 -5.7544174 -4.9500494 -4.2627239 -3.7877994 -3.5435991 -4.4445477 -4.6285706 -4.5046806 -4.2935147 -4.327991 -4.7091112 -5.6748009 -6.3866515][-7.2256126 -5.8470755 -5.714252 -5.2050829 -4.8609295 -4.8274527 -5.1347241 -6.3703423 -6.18211 -5.6584287 -5.1727924 -5.07083 -5.2507944 -5.9382396 -6.3183188][-7.007215 -5.3971138 -5.1420231 -4.8638592 -4.8823738 -5.20627 -5.9736757 -7.3023577 -6.9692092 -6.3628769 -5.9002304 -5.7268248 -5.6609879 -5.9478416 -6.0060205][-6.8597393 -4.9918771 -4.6368361 -4.5014563 -4.7846589 -5.3961477 -6.3753414 -7.4607396 -7.1180239 -6.6548972 -6.3160863 -6.0547156 -5.7892942 -5.7978678 -5.674283]]...]
INFO - root - 2017-12-15 07:04:24.662954: step 16110, loss = 0.29, batch loss = 0.26 (35.1 examples/sec; 0.228 sec/batch; 20h:00m:20s remains)
INFO - root - 2017-12-15 07:04:26.921514: step 16120, loss = 0.18, batch loss = 0.14 (35.9 examples/sec; 0.223 sec/batch; 19h:34m:09s remains)
INFO - root - 2017-12-15 07:04:29.170436: step 16130, loss = 0.35, batch loss = 0.32 (34.6 examples/sec; 0.232 sec/batch; 20h:20m:47s remains)
INFO - root - 2017-12-15 07:04:31.420058: step 16140, loss = 0.36, batch loss = 0.33 (35.8 examples/sec; 0.223 sec/batch; 19h:38m:20s remains)
INFO - root - 2017-12-15 07:04:33.670981: step 16150, loss = 0.29, batch loss = 0.26 (35.7 examples/sec; 0.224 sec/batch; 19h:42m:09s remains)
INFO - root - 2017-12-15 07:04:35.911171: step 16160, loss = 0.21, batch loss = 0.17 (34.8 examples/sec; 0.230 sec/batch; 20h:10m:23s remains)
INFO - root - 2017-12-15 07:04:38.198765: step 16170, loss = 0.24, batch loss = 0.20 (34.2 examples/sec; 0.234 sec/batch; 20h:31m:46s remains)
INFO - root - 2017-12-15 07:04:40.504131: step 16180, loss = 0.17, batch loss = 0.14 (36.1 examples/sec; 0.222 sec/batch; 19h:28m:32s remains)
INFO - root - 2017-12-15 07:04:42.775196: step 16190, loss = 0.49, batch loss = 0.46 (35.2 examples/sec; 0.227 sec/batch; 19h:58m:30s remains)
INFO - root - 2017-12-15 07:04:45.021698: step 16200, loss = 0.17, batch loss = 0.14 (36.5 examples/sec; 0.219 sec/batch; 19h:15m:18s remains)
2017-12-15 07:04:45.292010: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5412521 -5.6973829 -5.9407558 -6.0468173 -6.0707836 -6.0940843 -6.275106 -6.6238246 -7.0225716 -7.4227209 -7.3442912 -6.6324954 -5.4402571 -3.5402193 -1.0059901][-4.7950368 -6.5973406 -6.9300752 -7.0130324 -6.9364262 -6.791429 -6.655097 -6.6594419 -6.8877316 -7.231956 -7.0844641 -6.1747394 -4.7509022 -2.6198614 0.32597733][-5.982935 -7.0124593 -7.2660165 -7.1429095 -6.829309 -6.3743496 -5.7976594 -5.4575911 -5.6188145 -6.2307634 -6.5945511 -6.0285163 -4.6997318 -2.4436133 0.85250306][-6.8143663 -7.0283933 -7.0005121 -6.4928107 -5.7363043 -4.7656546 -3.7206287 -3.1879089 -3.5304503 -4.7363062 -6.0380449 -6.2850776 -5.4133067 -3.3526485 -0.050542831][-7.2978382 -6.5729647 -6.106957 -5.0719023 -3.684545 -2.0392623 -0.50738978 0.11905003 -0.49549651 -2.4595129 -4.9455385 -6.4719338 -6.5754123 -5.1935854 -2.3950479][-6.8176661 -5.619473 -4.6971064 -3.2450676 -1.3616955 0.85238647 2.7673059 3.4283581 2.6106381 0.090489388 -3.3184867 -6.1362948 -7.4948487 -7.1934819 -5.2934685][-5.7802353 -4.6367044 -3.4962206 -1.9065037 0.29627728 2.9391026 5.0570579 5.6332707 4.6274962 1.9192677 -1.8541323 -5.5153275 -7.8313408 -8.5466595 -7.63378][-5.4654226 -4.2380252 -3.0451393 -1.3798463 0.97121882 3.6792407 5.8128157 6.3301926 5.3075004 2.6490445 -1.1171242 -5.0087957 -7.6270509 -8.8923893 -8.8103485][-5.8187256 -4.7417636 -3.5962229 -1.8933599 0.41484022 2.8900814 4.8695989 5.2731109 4.2021623 1.7675154 -1.5951934 -5.0517092 -7.3886213 -8.6843281 -8.8503933][-6.659915 -5.97065 -5.131721 -3.6142 -1.540759 0.58567142 2.3284521 2.6797862 1.557178 -0.64117062 -3.3846946 -5.9360452 -7.5464697 -8.4419346 -8.3946667][-7.4265895 -7.1518793 -6.7555003 -5.6376734 -4.0896883 -2.5798483 -1.2198551 -0.87257385 -1.7753776 -3.4959671 -5.4365506 -6.9988179 -7.812429 -8.0333548 -7.3877687][-7.7280159 -7.6899962 -7.6704268 -7.0356431 -6.1365023 -5.3305569 -4.5175648 -4.2892265 -4.9179678 -5.9406681 -6.9215131 -7.53564 -7.6056924 -7.072062 -5.7094035][-7.6205411 -7.6723757 -7.9185405 -7.6825919 -7.2520056 -6.8701339 -6.4637804 -6.3891129 -6.8454475 -7.3127718 -7.5373869 -7.5539351 -7.183537 -6.1496711 -4.2518759][-7.1288891 -7.1264892 -7.4580755 -7.4405265 -7.2846231 -7.1658559 -7.0415926 -7.09288 -7.34388 -7.4766726 -7.4097209 -7.2769065 -6.7855945 -5.4484258 -3.3190451][-6.446876 -6.2982316 -6.6191506 -6.7197113 -6.708427 -6.7043571 -6.7007384 -6.7937179 -6.9711938 -7.0348821 -6.9964395 -7.0167923 -6.7267666 -5.5955534 -3.7256875]]...]
INFO - root - 2017-12-15 07:04:47.558835: step 16210, loss = 0.27, batch loss = 0.24 (34.5 examples/sec; 0.232 sec/batch; 20h:22m:18s remains)
INFO - root - 2017-12-15 07:04:49.802905: step 16220, loss = 0.24, batch loss = 0.20 (37.3 examples/sec; 0.214 sec/batch; 18h:50m:00s remains)
INFO - root - 2017-12-15 07:04:52.079723: step 16230, loss = 0.26, batch loss = 0.22 (34.5 examples/sec; 0.232 sec/batch; 20h:23m:13s remains)
INFO - root - 2017-12-15 07:04:54.356645: step 16240, loss = 0.22, batch loss = 0.19 (34.2 examples/sec; 0.234 sec/batch; 20h:34m:19s remains)
INFO - root - 2017-12-15 07:04:56.622745: step 16250, loss = 0.23, batch loss = 0.20 (34.9 examples/sec; 0.229 sec/batch; 20h:09m:24s remains)
INFO - root - 2017-12-15 07:04:58.884143: step 16260, loss = 0.24, batch loss = 0.21 (35.6 examples/sec; 0.225 sec/batch; 19h:44m:13s remains)
INFO - root - 2017-12-15 07:05:01.156064: step 16270, loss = 0.29, batch loss = 0.26 (35.3 examples/sec; 0.227 sec/batch; 19h:54m:44s remains)
INFO - root - 2017-12-15 07:05:03.434494: step 16280, loss = 0.29, batch loss = 0.26 (35.4 examples/sec; 0.226 sec/batch; 19h:52m:13s remains)
INFO - root - 2017-12-15 07:05:05.707983: step 16290, loss = 0.30, batch loss = 0.26 (35.1 examples/sec; 0.228 sec/batch; 20h:01m:10s remains)
INFO - root - 2017-12-15 07:05:07.956464: step 16300, loss = 0.26, batch loss = 0.22 (34.7 examples/sec; 0.230 sec/batch; 20h:13m:29s remains)
2017-12-15 07:05:08.260892: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.387501 -4.4957333 -4.2203813 -3.2806759 -2.89901 -2.2972229 -1.2994161 -0.05923295 1.2324123 1.4456098 1.0038371 0.8084228 -0.29132116 -1.6480103 -2.5703931][-3.9642367 -5.0622196 -4.8191557 -3.9445047 -3.5057082 -2.8368292 -1.7631085 -0.45678771 0.59047341 0.65580416 0.40059376 0.50761151 -0.61512244 -1.9783909 -3.0157368][-4.9579225 -5.4907093 -5.2678237 -4.5186949 -4.0749645 -3.3856134 -2.2105141 -0.74829853 0.09889102 -0.22264099 -0.55422473 -0.46142614 -1.4414549 -2.645175 -3.56499][-6.239759 -5.9800749 -5.7349 -5.1179695 -4.639092 -3.8947055 -2.635478 -1.0653366 -0.3683238 -1.1074109 -1.7623661 -1.9233648 -2.806201 -3.7323489 -4.194417][-6.8148689 -6.1427689 -5.8693752 -5.3374786 -4.64107 -3.6206393 -2.1462739 -0.3821696 0.22005725 -1.1894156 -2.5741658 -3.3103306 -4.1752567 -4.9077225 -4.9003716][-6.9419465 -5.9352951 -5.6823659 -5.1728053 -4.0631852 -2.5237632 -0.61872828 1.462739 2.1281447 0.14351439 -2.064503 -3.4499762 -4.5686283 -5.3957162 -5.207109][-6.1607847 -5.517992 -5.3017006 -4.7553687 -3.2460942 -1.1906432 1.1832981 3.7121801 4.6923709 2.4207072 -0.42268014 -2.4881656 -4.05489 -5.1238966 -4.9831543][-5.9085536 -5.1858444 -5.0121765 -4.4210682 -2.717433 -0.3883667 2.3150578 5.1818089 6.4763703 4.293479 1.2581503 -1.0808972 -2.9913387 -4.2720509 -4.2808924][-5.6915827 -5.1891923 -5.2179275 -4.7409782 -3.12765 -0.777853 2.0930705 5.0779972 6.6555953 4.8188658 1.9914532 -0.24047518 -2.1041586 -3.4021192 -3.3713369][-5.5216 -5.2820749 -5.6272621 -5.5758915 -4.396493 -2.2411504 0.53256464 3.3613791 4.88565 3.4265618 1.1226525 -0.4995743 -1.8198011 -2.7817035 -2.5788357][-5.3637877 -5.225687 -5.8321705 -6.1920729 -5.4347239 -3.6182189 -1.2493546 1.147979 2.5479093 1.4318712 -0.31805408 -1.2790227 -1.9971057 -2.6760979 -2.2460127][-5.0847654 -4.8258581 -5.4772053 -6.0242958 -5.6812716 -4.3360491 -2.5785749 -0.70386481 0.44643688 -0.34211028 -1.6159459 -2.0563316 -2.4165189 -2.8038101 -2.3230858][-4.7274523 -4.2456083 -4.7785878 -5.3529654 -5.3541722 -4.4900846 -3.1714497 -1.651798 -0.64563704 -1.236692 -1.9637833 -1.9979233 -2.4112144 -2.9516349 -2.5510035][-4.3612142 -3.5949228 -3.8686042 -4.3874769 -4.6818867 -4.184516 -3.0512791 -1.578862 -0.79171455 -1.462769 -1.8454599 -1.6459833 -2.216459 -2.9187348 -2.7652314][-4.0840263 -3.0550787 -3.1328638 -3.6758969 -4.1729841 -3.9674692 -2.8586538 -1.2909002 -0.64354265 -1.5272005 -1.6525992 -1.2393444 -2.0037704 -2.9536769 -2.9355702]]...]
INFO - root - 2017-12-15 07:05:10.569713: step 16310, loss = 0.25, batch loss = 0.22 (36.7 examples/sec; 0.218 sec/batch; 19h:09m:59s remains)
INFO - root - 2017-12-15 07:05:12.856503: step 16320, loss = 0.19, batch loss = 0.15 (35.9 examples/sec; 0.223 sec/batch; 19h:34m:30s remains)
INFO - root - 2017-12-15 07:05:15.102930: step 16330, loss = 0.27, batch loss = 0.23 (35.1 examples/sec; 0.228 sec/batch; 19h:59m:23s remains)
INFO - root - 2017-12-15 07:05:17.397792: step 16340, loss = 0.33, batch loss = 0.30 (34.7 examples/sec; 0.230 sec/batch; 20h:14m:08s remains)
INFO - root - 2017-12-15 07:05:19.631664: step 16350, loss = 0.28, batch loss = 0.25 (36.0 examples/sec; 0.222 sec/batch; 19h:31m:52s remains)
INFO - root - 2017-12-15 07:05:21.896515: step 16360, loss = 0.44, batch loss = 0.41 (35.0 examples/sec; 0.228 sec/batch; 20h:03m:34s remains)
INFO - root - 2017-12-15 07:05:24.170850: step 16370, loss = 0.23, batch loss = 0.20 (34.7 examples/sec; 0.230 sec/batch; 20h:13m:03s remains)
INFO - root - 2017-12-15 07:05:26.471541: step 16380, loss = 0.26, batch loss = 0.23 (35.6 examples/sec; 0.225 sec/batch; 19h:44m:21s remains)
INFO - root - 2017-12-15 07:05:28.763876: step 16390, loss = 0.22, batch loss = 0.19 (35.4 examples/sec; 0.226 sec/batch; 19h:52m:10s remains)
INFO - root - 2017-12-15 07:05:31.010678: step 16400, loss = 0.20, batch loss = 0.17 (36.1 examples/sec; 0.222 sec/batch; 19h:28m:56s remains)
2017-12-15 07:05:31.277349: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2465024 -5.4231358 -5.3900909 -5.3136492 -5.1985617 -4.3003254 -2.2038729 -1.1020386 -0.92033935 -1.877417 -4.234004 -6.0844526 -6.3798504 -6.1106749 -5.8695984][-3.0062819 -5.3257151 -4.9582644 -4.3637724 -3.8478742 -2.8194063 -0.75019109 0.30152273 0.20825863 -1.0399815 -3.488935 -5.2585669 -6.0372753 -6.261178 -6.3116622][-3.9909186 -5.7113223 -5.1793909 -4.2331085 -3.2159436 -1.9068406 0.023717642 0.95899796 0.56114006 -0.79100192 -2.9085298 -4.2832994 -5.2604494 -5.8491049 -6.1064577][-5.5254488 -6.25115 -5.6892767 -4.5188456 -3.0858324 -1.3722701 0.46880937 1.2558463 0.6943152 -0.70795774 -2.5112433 -3.470221 -4.4854517 -5.3242073 -5.6915922][-6.6313128 -6.4765315 -5.8459167 -4.5122786 -2.7865365 -0.68118179 1.2098446 1.976222 1.321743 -0.21370673 -1.9195826 -2.8363607 -3.9798121 -4.9976349 -5.3683629][-6.8305683 -6.2890782 -5.5495 -4.0868497 -2.171854 0.2583499 2.3552971 3.1147456 2.2847724 0.50453162 -1.349214 -2.5244935 -3.9378633 -5.0527239 -5.3623877][-6.1588578 -5.9340625 -5.115726 -3.3728933 -1.1828486 1.5291433 3.9760609 4.8298411 3.7131004 1.5811591 -0.64077437 -2.2304955 -3.9150522 -5.1383953 -5.4805322][-6.1879539 -5.72581 -4.8996506 -2.8943458 -0.532966 2.2824726 5.05682 6.2012 4.9762468 2.4826431 -0.018508196 -1.9083961 -3.9270427 -5.3132763 -5.6778688][-5.9964614 -5.5010605 -4.7921152 -2.8303628 -0.66559052 1.9075804 4.7265358 6.1628051 5.0790405 2.4873714 0.12358594 -1.8915215 -4.0916376 -5.4680166 -5.7595387][-5.3740911 -4.8545485 -4.3578358 -2.7324524 -1.0350854 1.1027532 3.7073898 5.2722712 4.4285417 1.9336896 -0.064292669 -2.0869715 -4.3139353 -5.5460653 -5.7185497][-4.8911338 -4.4334154 -4.219492 -2.9449153 -1.5530725 0.18868637 2.4628072 4.0240717 3.4342847 1.1325188 -0.51140189 -2.4789393 -4.6183414 -5.7165241 -5.8007336][-4.7275314 -4.3644629 -4.2155766 -2.9926343 -1.5933845 0.0013206005 1.9844441 3.1924939 2.6591115 0.50517607 -0.97041142 -2.9489565 -4.9526687 -5.910656 -5.9065886][-4.8568778 -4.6491404 -4.5433412 -3.3263681 -1.8241986 -0.23445678 1.5016301 2.4511871 1.9340601 -0.055565596 -1.5808229 -3.5740576 -5.3473091 -6.0931067 -5.9970331][-5.2082911 -5.2986674 -5.4987993 -4.5862122 -3.1572967 -1.523273 0.16150165 1.0994811 0.73213649 -0.97807837 -2.5555801 -4.4183912 -5.8259649 -6.2773352 -6.0625525][-5.4070144 -5.7413721 -6.3542051 -5.9708357 -4.8571649 -3.3717308 -1.756016 -0.76445162 -0.920635 -2.1525481 -3.5682278 -5.1074657 -6.13885 -6.3909125 -6.1380491]]...]
INFO - root - 2017-12-15 07:05:33.543930: step 16410, loss = 0.19, batch loss = 0.16 (33.8 examples/sec; 0.237 sec/batch; 20h:48m:31s remains)
INFO - root - 2017-12-15 07:05:35.849572: step 16420, loss = 0.20, batch loss = 0.17 (34.9 examples/sec; 0.229 sec/batch; 20h:08m:57s remains)
INFO - root - 2017-12-15 07:05:38.148337: step 16430, loss = 0.24, batch loss = 0.21 (35.4 examples/sec; 0.226 sec/batch; 19h:51m:36s remains)
INFO - root - 2017-12-15 07:05:40.420168: step 16440, loss = 0.22, batch loss = 0.19 (34.8 examples/sec; 0.230 sec/batch; 20h:09m:32s remains)
INFO - root - 2017-12-15 07:05:42.662412: step 16450, loss = 0.24, batch loss = 0.21 (35.6 examples/sec; 0.225 sec/batch; 19h:43m:27s remains)
INFO - root - 2017-12-15 07:05:44.940531: step 16460, loss = 0.27, batch loss = 0.24 (35.7 examples/sec; 0.224 sec/batch; 19h:41m:04s remains)
INFO - root - 2017-12-15 07:05:47.210452: step 16470, loss = 0.20, batch loss = 0.17 (34.9 examples/sec; 0.229 sec/batch; 20h:05m:44s remains)
INFO - root - 2017-12-15 07:05:49.480450: step 16480, loss = 0.21, batch loss = 0.18 (33.3 examples/sec; 0.240 sec/batch; 21h:04m:48s remains)
INFO - root - 2017-12-15 07:05:51.756837: step 16490, loss = 0.23, batch loss = 0.20 (33.9 examples/sec; 0.236 sec/batch; 20h:44m:11s remains)
INFO - root - 2017-12-15 07:05:54.004331: step 16500, loss = 0.30, batch loss = 0.27 (35.4 examples/sec; 0.226 sec/batch; 19h:51m:26s remains)
2017-12-15 07:05:54.296446: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.8553524 -8.8959923 -8.4002447 -8.033783 -7.4960427 -7.1794672 -7.6648874 -7.901701 -7.9898248 -7.96924 -8.3597813 -9.098629 -9.21977 -9.1943731 -8.9949512][-9.3104115 -8.0099277 -7.3078885 -6.8662095 -6.3778005 -6.2042904 -6.8352385 -6.9722409 -7.00428 -7.0545373 -7.55136 -8.43581 -8.6228914 -8.5739746 -8.19862][-8.44463 -6.3002448 -5.6153717 -5.2768674 -4.8608546 -4.6730976 -5.2440109 -5.1320515 -4.9466715 -5.0595551 -5.7998734 -6.8604856 -7.0994763 -6.945992 -6.311718][-7.3731027 -4.4966512 -3.9685035 -3.8119874 -3.4802861 -3.0845904 -3.3037667 -2.7875059 -2.2492709 -2.402163 -3.4242251 -4.7606235 -5.237731 -5.0934625 -4.2322922][-6.0732288 -2.8640165 -2.6407657 -2.8368316 -2.6461315 -1.7894773 -1.3932486 -0.60085583 0.15273786 -0.0099992752 -1.2166412 -2.7696054 -3.5596631 -3.589859 -2.6377437][-4.759654 -1.8208799 -1.8397104 -2.3616195 -2.0721292 -0.44785619 0.71645284 1.7669988 2.6095533 2.3311033 0.86951828 -0.903574 -2.1412818 -2.5126529 -1.7180443][-3.9515791 -1.458662 -1.597388 -2.1348004 -1.5024827 0.66832089 2.3075585 3.3787751 4.0190234 3.4061513 1.7904825 -0.037904978 -1.6202832 -2.33133 -1.8016065][-4.2756119 -1.8739822 -2.1097417 -2.5401874 -1.6064868 0.71780825 2.500196 3.3573084 3.5099998 2.5979152 0.97881269 -0.7187171 -2.2345333 -3.0074532 -2.6926332][-4.9385576 -2.877418 -3.0595243 -3.3274579 -2.3796451 -0.2578069 1.3543687 1.8508391 1.5320656 0.40542221 -1.158641 -2.5754988 -3.6576104 -4.1127386 -3.8019185][-5.3234024 -3.4946823 -3.5111139 -3.6232452 -2.8903232 -1.2876679 -0.065290928 -0.00960803 -0.75685477 -2.0529084 -3.4423347 -4.5812855 -5.2200012 -5.3381948 -5.0030222][-6.4256864 -4.8922663 -4.707068 -4.5208654 -3.9224622 -2.8755238 -2.1391814 -2.339962 -3.2246728 -4.4174156 -5.6012378 -6.5347729 -6.7896276 -6.695199 -6.4473996][-8.0504589 -6.7967253 -6.3137565 -5.7411585 -5.0615082 -4.3018675 -4.12393 -4.57934 -5.3869305 -6.2456522 -7.1321678 -7.7645388 -7.7187619 -7.4999266 -7.4970942][-8.9921913 -7.9898181 -7.2879958 -6.3835893 -5.5688038 -5.1253061 -5.51983 -6.2390151 -7.042901 -7.7445192 -8.25306 -8.4310341 -8.1501789 -7.8934722 -8.0608511][-9.3637047 -8.5922117 -8.0208044 -7.1666322 -6.4140906 -6.2078524 -6.8385944 -7.6153097 -8.3711834 -8.8285093 -8.9504795 -8.7869015 -8.4013138 -8.1589432 -8.2708168][-8.8430309 -8.09433 -7.6636219 -7.0392461 -6.52521 -6.4391308 -7.0503082 -7.70145 -8.1490555 -8.1978207 -8.0605278 -7.8228035 -7.5341277 -7.3680687 -7.3598614]]...]
INFO - root - 2017-12-15 07:05:56.579119: step 16510, loss = 0.21, batch loss = 0.17 (35.1 examples/sec; 0.228 sec/batch; 20h:00m:32s remains)
INFO - root - 2017-12-15 07:05:58.846817: step 16520, loss = 0.33, batch loss = 0.30 (34.3 examples/sec; 0.233 sec/batch; 20h:28m:01s remains)
INFO - root - 2017-12-15 07:06:01.107034: step 16530, loss = 0.25, batch loss = 0.22 (35.8 examples/sec; 0.223 sec/batch; 19h:36m:12s remains)
INFO - root - 2017-12-15 07:06:03.367309: step 16540, loss = 0.25, batch loss = 0.22 (35.7 examples/sec; 0.224 sec/batch; 19h:39m:54s remains)
INFO - root - 2017-12-15 07:06:05.636965: step 16550, loss = 0.17, batch loss = 0.14 (35.3 examples/sec; 0.227 sec/batch; 19h:53m:13s remains)
INFO - root - 2017-12-15 07:06:07.919801: step 16560, loss = 0.22, batch loss = 0.18 (36.5 examples/sec; 0.219 sec/batch; 19h:13m:25s remains)
INFO - root - 2017-12-15 07:06:10.221422: step 16570, loss = 0.29, batch loss = 0.25 (34.5 examples/sec; 0.232 sec/batch; 20h:20m:04s remains)
INFO - root - 2017-12-15 07:06:12.498658: step 16580, loss = 0.23, batch loss = 0.20 (34.8 examples/sec; 0.230 sec/batch; 20h:09m:04s remains)
INFO - root - 2017-12-15 07:06:14.759673: step 16590, loss = 0.25, batch loss = 0.22 (35.4 examples/sec; 0.226 sec/batch; 19h:50m:23s remains)
INFO - root - 2017-12-15 07:06:17.020292: step 16600, loss = 0.20, batch loss = 0.17 (36.0 examples/sec; 0.222 sec/batch; 19h:31m:08s remains)
2017-12-15 07:06:17.334225: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7554884 -4.7661924 -3.8323176 -2.6447694 -1.8306885 -2.6839161 -4.5426874 -5.569272 -4.9059544 -3.8940406 -3.1780894 -2.3453267 -1.9552715 -1.1923796 -0.64781666][-6.0521097 -5.3811078 -4.3523784 -3.0930979 -2.2260418 -3.0156806 -4.89392 -5.9121051 -5.4769897 -4.6705136 -3.9366145 -3.2156186 -2.6034968 -1.8747065 -1.2037932][-6.7700653 -5.4552088 -4.4086304 -3.0846591 -2.2425263 -2.8825874 -4.5070114 -5.4447289 -5.4119024 -5.1519461 -4.7830524 -4.3231807 -3.6829848 -2.7802682 -1.7428963][-7.2773123 -5.2646275 -4.1684451 -2.8077302 -1.7856207 -1.953837 -2.8372974 -3.4613061 -3.9461405 -4.6630845 -5.1058617 -5.1856461 -4.86078 -3.9166181 -2.4104807][-6.5888577 -4.5840058 -3.5154791 -2.1171362 -0.85613644 -0.48370373 -0.32783294 -0.29603636 -1.1395098 -2.9139783 -4.33506 -5.0515432 -5.304523 -4.7733173 -3.1452518][-5.6907063 -3.1501558 -2.0668769 -0.80546653 0.46816254 1.3205721 2.6055672 3.4958475 2.31017 -0.46158504 -2.7469647 -4.1292 -5.0987673 -5.1323128 -3.6399837][-3.7931442 -1.3705177 -0.28608453 0.75601649 1.9138205 2.8161638 4.6603365 6.164876 4.8686676 1.5615413 -1.2203014 -3.1480129 -4.7992563 -5.4577637 -4.4555917][-3.0458453 -0.3120904 0.79361296 1.6542752 2.4734547 3.1114695 5.1150837 7.1263971 5.9870539 2.859463 0.13027644 -2.1165555 -4.4425035 -5.72519 -5.16178][-2.566823 0.13051629 1.0491498 1.5213685 1.8134162 1.8202269 3.3962491 5.5752554 4.9947071 2.5443585 0.25781226 -2.0665796 -4.6600695 -6.0304914 -5.5878382][-2.7129951 -0.11515164 0.54382849 0.64093161 0.40247893 -0.30894506 0.61037946 2.6970608 2.5841458 0.90603256 -0.68312132 -2.5281096 -4.7938089 -5.8832803 -5.37956][-3.1747611 -0.5653652 -0.034388542 -0.052368879 -0.54637885 -1.8263265 -1.6381527 -0.036121607 0.31739664 -0.7183826 -1.6997491 -2.9904544 -4.7239523 -5.3507023 -4.7819834][-4.2181416 -1.5819582 -0.97213137 -0.73366404 -1.0772874 -2.5614543 -3.1333199 -2.3420966 -1.9549865 -2.5373089 -3.0676472 -3.7468948 -4.806397 -4.8090653 -3.8390679][-5.5782385 -2.9379139 -2.0501797 -1.2955924 -1.1818131 -2.593318 -3.8276358 -4.018024 -3.7041054 -3.8104987 -3.8093622 -3.9981539 -4.5965905 -4.3987694 -3.525846][-6.9165907 -4.2890787 -3.2087939 -2.12727 -1.685958 -2.7986374 -4.2440643 -5.0487285 -4.9395075 -4.8237567 -4.4872375 -4.2647853 -4.5305614 -4.317914 -3.7008259][-8.1013308 -5.7235479 -4.6075854 -3.4481125 -2.7503886 -3.4452457 -4.7530608 -5.6978731 -5.680891 -5.3820419 -4.7373009 -4.0975943 -3.9166651 -3.6148643 -3.3096447]]...]
INFO - root - 2017-12-15 07:06:19.587132: step 16610, loss = 0.24, batch loss = 0.21 (35.1 examples/sec; 0.228 sec/batch; 19h:58m:55s remains)
INFO - root - 2017-12-15 07:06:21.867170: step 16620, loss = 0.27, batch loss = 0.23 (33.4 examples/sec; 0.239 sec/batch; 21h:00m:35s remains)
INFO - root - 2017-12-15 07:06:24.172024: step 16630, loss = 0.28, batch loss = 0.25 (34.0 examples/sec; 0.235 sec/batch; 20h:37m:11s remains)
INFO - root - 2017-12-15 07:06:26.474899: step 16640, loss = 0.30, batch loss = 0.26 (35.5 examples/sec; 0.225 sec/batch; 19h:45m:47s remains)
INFO - root - 2017-12-15 07:06:28.724361: step 16650, loss = 0.19, batch loss = 0.16 (35.7 examples/sec; 0.224 sec/batch; 19h:40m:11s remains)
INFO - root - 2017-12-15 07:06:30.964879: step 16660, loss = 0.25, batch loss = 0.22 (34.5 examples/sec; 0.232 sec/batch; 20h:19m:54s remains)
INFO - root - 2017-12-15 07:06:33.260070: step 16670, loss = 0.21, batch loss = 0.18 (34.2 examples/sec; 0.234 sec/batch; 20h:32m:41s remains)
INFO - root - 2017-12-15 07:06:35.510951: step 16680, loss = 0.16, batch loss = 0.13 (36.0 examples/sec; 0.222 sec/batch; 19h:28m:42s remains)
INFO - root - 2017-12-15 07:06:37.841908: step 16690, loss = 0.20, batch loss = 0.16 (32.6 examples/sec; 0.245 sec/batch; 21h:30m:12s remains)
INFO - root - 2017-12-15 07:06:40.116045: step 16700, loss = 0.24, batch loss = 0.21 (35.5 examples/sec; 0.225 sec/batch; 19h:46m:20s remains)
2017-12-15 07:06:40.402520: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7961156 -3.9663262 -4.5194092 -5.0417147 -5.2591982 -4.936595 -4.5750656 -4.4520607 -4.5168715 -4.6071949 -4.6535707 -4.3545485 -3.5464473 -2.9330711 -2.9259934][-3.549345 -4.7244768 -5.2217631 -5.6972933 -6.0139561 -5.8961272 -5.7603064 -5.8447638 -6.0373611 -6.1726 -6.2530651 -5.980442 -5.070055 -4.3126011 -3.9763985][-4.8533726 -5.2074575 -5.4691119 -5.6982775 -5.9513631 -5.89036 -5.7513461 -5.8770213 -6.0754442 -6.2003474 -6.3077831 -6.1758256 -5.5331774 -4.9445667 -4.4735003][-5.8846865 -5.5070415 -5.3923721 -5.1280646 -5.0473766 -4.7425575 -4.4517403 -4.592279 -4.9614778 -5.4199219 -5.8317447 -5.9693274 -5.644022 -5.2082167 -4.6867061][-6.7065 -5.9368896 -5.3825984 -4.5682368 -4.0344114 -3.2799106 -2.4521894 -2.1973777 -2.727654 -3.8768375 -5.07469 -5.6896191 -5.4938869 -5.0858297 -4.6037636][-7.0444822 -5.913167 -4.9438753 -3.7746809 -2.963356 -1.7882663 -0.18766952 0.84472322 0.33272958 -1.4887168 -3.4651866 -4.5236187 -4.3055744 -3.8679001 -3.3499179][-6.2391825 -5.6018915 -4.2822132 -2.9351888 -1.8581254 -0.10069489 2.5859149 4.5867577 3.8993413 1.2030647 -1.7823787 -3.4755573 -3.2912626 -2.7629504 -2.0379663][-6.2049251 -5.5660686 -4.1821842 -2.8421917 -1.4540681 0.99271464 4.7224979 7.4827671 6.6219168 3.146383 -0.7067374 -3.0921183 -3.0164449 -2.4651291 -1.63904][-6.1881723 -5.7800355 -4.6545496 -3.6282163 -2.2309029 0.513705 4.8088951 7.7760754 6.8788671 3.4564998 -0.53385866 -3.1499643 -2.9747372 -2.3842916 -1.6028441][-6.4481707 -6.3793573 -5.6729565 -4.9935012 -3.7390785 -0.95062757 3.2665398 5.9544859 5.2743959 2.3870947 -1.1444312 -3.4865246 -3.221334 -2.6605141 -2.0351164][-6.9443588 -7.2217846 -6.9086037 -6.3935871 -5.2406015 -2.630147 1.0323319 3.4133637 3.2927639 1.2485735 -1.7470293 -3.7631469 -3.5564997 -3.1383386 -2.8329921][-7.5275717 -8.0771255 -8.1205158 -7.6838274 -6.5894175 -4.3651342 -1.620968 0.22237158 0.52090049 -0.84933162 -3.0839119 -4.5603194 -4.49279 -4.3042 -4.3176908][-7.8462381 -8.4620523 -8.7383 -8.39878 -7.5157776 -5.9437728 -4.2162166 -3.0453439 -2.8708611 -3.8117313 -5.295259 -6.1356611 -6.1009893 -6.031538 -6.0326777][-7.9554672 -8.5318651 -8.9709339 -8.7438135 -8.1076765 -7.1704426 -6.29926 -5.7086868 -5.6329794 -6.2135973 -7.0108213 -7.3365784 -7.3125973 -7.2642126 -7.1924028][-7.672307 -7.9805517 -8.392086 -8.267662 -7.9506493 -7.5947638 -7.3246202 -7.1596966 -7.1943359 -7.4576569 -7.7614079 -7.8374805 -7.7981663 -7.630455 -7.4277716]]...]
INFO - root - 2017-12-15 07:06:42.676104: step 16710, loss = 0.23, batch loss = 0.20 (35.1 examples/sec; 0.228 sec/batch; 19h:59m:57s remains)
INFO - root - 2017-12-15 07:06:44.936271: step 16720, loss = 0.33, batch loss = 0.30 (35.0 examples/sec; 0.229 sec/batch; 20h:03m:23s remains)
INFO - root - 2017-12-15 07:06:47.211223: step 16730, loss = 0.22, batch loss = 0.19 (35.7 examples/sec; 0.224 sec/batch; 19h:38m:21s remains)
INFO - root - 2017-12-15 07:06:49.487739: step 16740, loss = 0.24, batch loss = 0.20 (35.6 examples/sec; 0.225 sec/batch; 19h:42m:11s remains)
INFO - root - 2017-12-15 07:06:51.755230: step 16750, loss = 0.26, batch loss = 0.23 (35.2 examples/sec; 0.227 sec/batch; 19h:54m:30s remains)
INFO - root - 2017-12-15 07:06:54.041716: step 16760, loss = 0.26, batch loss = 0.22 (35.7 examples/sec; 0.224 sec/batch; 19h:39m:52s remains)
INFO - root - 2017-12-15 07:06:56.352244: step 16770, loss = 0.23, batch loss = 0.20 (34.7 examples/sec; 0.230 sec/batch; 20h:11m:40s remains)
INFO - root - 2017-12-15 07:06:58.625590: step 16780, loss = 0.25, batch loss = 0.22 (34.2 examples/sec; 0.234 sec/batch; 20h:32m:02s remains)
INFO - root - 2017-12-15 07:07:00.895667: step 16790, loss = 0.30, batch loss = 0.26 (34.4 examples/sec; 0.232 sec/batch; 20h:23m:18s remains)
INFO - root - 2017-12-15 07:07:03.179141: step 16800, loss = 0.23, batch loss = 0.19 (33.9 examples/sec; 0.236 sec/batch; 20h:43m:27s remains)
2017-12-15 07:07:03.484051: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9566307 -5.7425146 -5.7916584 -5.7675939 -5.9288664 -5.9559808 -5.6107254 -5.0884604 -5.0294819 -5.4715643 -5.5225024 -5.3570328 -4.9121065 -4.7793856 -4.9680762][-5.3483419 -6.3610315 -6.4737759 -6.5450869 -6.7789507 -6.6943483 -6.3132486 -5.8820076 -5.7945232 -6.20669 -6.2582531 -6.247817 -5.9091625 -5.8624811 -6.1678486][-6.0504956 -6.5059986 -6.502512 -6.4887962 -6.5938826 -6.3538361 -5.962882 -5.5045853 -5.2303572 -5.7594175 -6.0658545 -6.3590755 -6.19045 -6.214201 -6.7250452][-6.9278975 -6.5169954 -6.3891888 -6.2393093 -6.073679 -5.6014428 -5.1286511 -4.5934753 -4.2881536 -4.8575168 -5.3085394 -5.7622471 -5.8810186 -6.221097 -6.8398561][-6.5130157 -5.6341033 -5.4203949 -5.1679716 -4.7011528 -3.8868837 -3.2281895 -2.6485863 -2.4426565 -3.4463425 -4.3020496 -4.9632998 -5.415267 -5.9722176 -6.6219273][-5.6804342 -4.3949857 -4.0741553 -3.6009684 -2.7148645 -1.4686768 -0.45668411 0.14958429 0.12657595 -1.0356727 -2.2424598 -3.1617954 -3.9959824 -4.9070282 -5.7238665][-4.8493152 -3.8321054 -3.3651524 -2.5440085 -1.0510139 0.96332645 2.5193555 3.1459744 2.7244618 1.1672411 -0.38249791 -1.5989254 -2.7801759 -4.0157537 -4.9368863][-4.7568307 -3.4266016 -2.7319641 -1.4197538 0.8442266 3.842222 6.1750603 6.8807011 5.74687 3.5928447 1.6318319 0.051359415 -1.6052926 -3.3848281 -4.5042706][-5.5161328 -3.9372473 -3.0311785 -1.5257261 1.186691 4.7204676 7.4206171 8.1625137 6.6589994 4.14176 2.08836 0.40978098 -1.5671724 -3.6454077 -4.8169136][-7.0011711 -5.49234 -4.7103033 -3.4294472 -0.94475126 2.1748168 4.4835157 5.0515881 3.6966522 1.8170002 0.3225069 -0.89677703 -2.5870028 -4.7262745 -5.8858585][-8.3311052 -6.8292437 -5.9669657 -4.822547 -2.7232847 -0.35039377 1.3563924 1.7986691 0.67170763 -0.68957543 -1.5541835 -2.2376523 -3.833195 -6.0968723 -7.1599407][-9.3384914 -7.916791 -7.09147 -6.2179613 -4.6013927 -2.8460267 -1.7625582 -1.5805495 -2.4034648 -3.3605108 -3.9144866 -4.3632336 -5.8564577 -7.9010868 -8.6894884][-10.137133 -9.0615988 -8.4902458 -8.0072117 -7.0795536 -5.9926133 -5.1947737 -4.9005585 -5.2636924 -5.7282467 -5.9296317 -6.1826582 -7.3147526 -8.7103529 -9.06296][-9.7562485 -8.8993731 -8.5583277 -8.442543 -7.9750948 -7.2324095 -6.5665283 -6.1971092 -6.27649 -6.5093393 -6.7098274 -7.0276527 -7.6812677 -8.2590609 -8.2208881][-8.3253632 -7.7037048 -7.6098251 -7.6872931 -7.58078 -7.1564016 -6.6284466 -6.2430038 -6.305728 -6.6682491 -7.0391817 -7.4245625 -7.8005714 -7.7514858 -7.2735167]]...]
INFO - root - 2017-12-15 07:07:05.763846: step 16810, loss = 0.23, batch loss = 0.20 (32.1 examples/sec; 0.249 sec/batch; 21h:49m:53s remains)
INFO - root - 2017-12-15 07:07:08.017254: step 16820, loss = 0.21, batch loss = 0.18 (36.0 examples/sec; 0.222 sec/batch; 19h:28m:09s remains)
INFO - root - 2017-12-15 07:07:10.259265: step 16830, loss = 0.23, batch loss = 0.20 (36.1 examples/sec; 0.221 sec/batch; 19h:24m:21s remains)
INFO - root - 2017-12-15 07:07:12.554713: step 16840, loss = 0.43, batch loss = 0.39 (34.4 examples/sec; 0.232 sec/batch; 20h:22m:20s remains)
INFO - root - 2017-12-15 07:07:14.831881: step 16850, loss = 0.22, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 19h:32m:18s remains)
INFO - root - 2017-12-15 07:07:17.116168: step 16860, loss = 0.26, batch loss = 0.23 (35.0 examples/sec; 0.229 sec/batch; 20h:02m:36s remains)
INFO - root - 2017-12-15 07:07:19.436205: step 16870, loss = 0.19, batch loss = 0.16 (34.4 examples/sec; 0.232 sec/batch; 20h:21m:51s remains)
INFO - root - 2017-12-15 07:07:21.698763: step 16880, loss = 0.30, batch loss = 0.27 (35.7 examples/sec; 0.224 sec/batch; 19h:38m:06s remains)
INFO - root - 2017-12-15 07:07:24.019769: step 16890, loss = 0.35, batch loss = 0.32 (35.2 examples/sec; 0.227 sec/batch; 19h:56m:30s remains)
INFO - root - 2017-12-15 07:07:26.299265: step 16900, loss = 0.23, batch loss = 0.20 (35.0 examples/sec; 0.228 sec/batch; 20h:01m:53s remains)
2017-12-15 07:07:26.578379: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8795054 -4.3914452 -5.0824385 -5.4448671 -4.4792833 -2.9463329 -1.8219988 -0.82854891 -0.53703082 -0.54165125 -0.69438481 -1.7654952 -3.8243375 -5.0874557 -4.9040146][-4.3036737 -4.9238997 -5.6265287 -5.8986268 -4.6883698 -3.1787882 -2.2515264 -1.3767033 -1.2680609 -1.1276562 -1.0087595 -1.8767673 -3.5611293 -4.6475115 -4.0239139][-5.2230368 -4.8948984 -5.632658 -5.737957 -4.4104548 -3.1845126 -2.5814395 -1.7096968 -1.6525073 -1.4783257 -1.1715926 -1.8333901 -3.0298219 -3.8461485 -3.0960112][-5.74835 -4.7894931 -5.7585287 -5.6470079 -4.1136312 -3.1611948 -2.7679029 -1.8702538 -1.9891764 -2.0747817 -1.9444089 -2.3913758 -3.034657 -3.5661311 -3.0765789][-5.6184559 -3.9600563 -4.9617319 -4.4148173 -2.5652652 -1.8472371 -1.4834628 -0.80158842 -1.4321234 -2.1646581 -2.5523248 -2.8806181 -3.0239801 -3.2917957 -3.2123675][-4.7812266 -2.9707072 -3.7811728 -2.7454982 -0.4754715 0.2998414 1.0665579 1.5332761 0.19127512 -1.4592915 -2.7774825 -3.2717419 -3.0468125 -2.9772546 -3.1765678][-4.4761963 -2.5750158 -2.9595342 -1.5017898 0.96542454 1.8231523 3.2464545 3.931145 2.2892239 -0.065818548 -2.2980027 -3.0920901 -2.8584347 -2.5722342 -2.8379922][-4.6973629 -2.1813302 -2.244204 -0.72563291 1.5015135 2.2845838 4.2664452 5.5190115 4.1643381 1.6973364 -1.0540218 -2.2207632 -2.055656 -1.5897563 -1.5408108][-5.4144578 -2.5941327 -2.7145219 -1.7004101 -0.065226793 0.41754866 2.4676573 4.252676 3.665848 1.8827679 -0.55319512 -1.706722 -1.3936276 -0.89803112 -0.69629717][-5.9949389 -3.3372746 -3.9722362 -3.9206915 -2.8278539 -2.4572976 -0.70362926 1.1985178 1.3562019 0.56035638 -0.92794836 -1.5606015 -0.86077273 -0.28615761 0.04213047][-6.2660236 -4.2190604 -5.4365363 -6.0205874 -5.1444178 -4.7722831 -3.45431 -1.9389576 -1.3400977 -1.1656543 -1.6608363 -1.6639338 -0.52778912 0.13977647 0.49292374][-6.6081085 -5.1943836 -6.7962503 -7.7590628 -7.1768789 -6.9245806 -5.86374 -4.6003428 -3.6819196 -2.8835816 -2.7042978 -2.3887656 -1.1796035 -0.68927109 -0.56452942][-6.4055424 -5.5390196 -7.2172441 -8.2595558 -7.9280138 -7.7702475 -6.8736343 -5.6508803 -4.4858665 -3.5947881 -3.293504 -3.0283623 -2.1040485 -1.8669776 -1.9741933][-5.9420223 -5.2997894 -6.6309891 -7.4492664 -7.3207493 -7.1628237 -6.3937273 -5.272769 -4.1050181 -3.3466506 -3.1388357 -3.0498719 -2.5800455 -2.5875084 -2.9540458][-5.6760631 -5.0422831 -5.9601159 -6.5265894 -6.5018692 -6.3850431 -5.8429947 -4.8554611 -3.6346555 -2.7924187 -2.4633694 -2.4089942 -2.3904908 -2.7732286 -3.40208]]...]
INFO - root - 2017-12-15 07:07:28.906130: step 16910, loss = 0.19, batch loss = 0.16 (35.0 examples/sec; 0.228 sec/batch; 20h:00m:34s remains)
INFO - root - 2017-12-15 07:07:31.174174: step 16920, loss = 0.26, batch loss = 0.23 (35.3 examples/sec; 0.226 sec/batch; 19h:50m:52s remains)
INFO - root - 2017-12-15 07:07:33.455018: step 16930, loss = 0.22, batch loss = 0.19 (34.8 examples/sec; 0.230 sec/batch; 20h:08m:26s remains)
INFO - root - 2017-12-15 07:07:35.726446: step 16940, loss = 0.29, batch loss = 0.25 (33.8 examples/sec; 0.237 sec/batch; 20h:44m:39s remains)
INFO - root - 2017-12-15 07:07:38.000733: step 16950, loss = 0.18, batch loss = 0.15 (34.7 examples/sec; 0.230 sec/batch; 20h:12m:02s remains)
INFO - root - 2017-12-15 07:07:40.281956: step 16960, loss = 0.31, batch loss = 0.28 (36.2 examples/sec; 0.221 sec/batch; 19h:21m:29s remains)
INFO - root - 2017-12-15 07:07:42.536857: step 16970, loss = 0.22, batch loss = 0.19 (36.8 examples/sec; 0.217 sec/batch; 19h:02m:58s remains)
INFO - root - 2017-12-15 07:07:44.792602: step 16980, loss = 0.20, batch loss = 0.17 (35.7 examples/sec; 0.224 sec/batch; 19h:38m:31s remains)
INFO - root - 2017-12-15 07:07:47.060244: step 16990, loss = 0.27, batch loss = 0.23 (35.1 examples/sec; 0.228 sec/batch; 19h:57m:50s remains)
INFO - root - 2017-12-15 07:07:49.347147: step 17000, loss = 0.23, batch loss = 0.19 (36.1 examples/sec; 0.222 sec/batch; 19h:24m:55s remains)
2017-12-15 07:07:49.615809: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.015883 -6.7111645 -6.609519 -6.28992 -6.0569024 -5.6873255 -5.2958336 -5.2337017 -5.5292468 -6.005609 -6.4689264 -6.5009537 -6.364356 -6.0855675 -5.4754086][-8.2344809 -8.9757357 -8.8129435 -8.2757425 -7.7089825 -6.9533625 -6.4062843 -6.3309708 -6.785243 -7.5297794 -8.3083572 -8.2379217 -7.862319 -7.3565645 -6.3594837][-10.095694 -10.074337 -9.9542952 -9.3752384 -8.3677082 -7.04784 -6.277545 -6.1332026 -6.7445278 -7.8600435 -9.1134968 -9.0522184 -8.50593 -7.9515667 -6.6705055][-11.228665 -9.7353048 -9.6645956 -9.10579 -7.7085228 -5.7609844 -4.7355604 -4.6936646 -5.6270885 -7.1036043 -8.8280029 -8.8319244 -8.3434782 -7.9625568 -6.6320581][-10.773218 -8.30035 -7.8337469 -7.0298185 -5.0916543 -2.4074447 -0.73523962 -0.54455245 -2.1629391 -4.4244795 -6.7554874 -7.2189646 -7.2152252 -7.2129507 -6.0427556][-9.4135065 -6.7570143 -5.6005187 -4.1803436 -1.9205842 1.1367354 3.5203207 4.1923323 2.0182655 -1.2280614 -4.1646843 -5.3684139 -6.2206678 -6.6490726 -5.5681152][-7.7289467 -5.3337111 -3.5584502 -1.4419971 1.2593327 4.5530481 7.3849506 8.3120241 5.7650862 1.7804482 -1.5189272 -3.5363488 -5.2761941 -6.1556597 -5.2854543][-7.7069759 -5.3340974 -3.5135779 -1.106566 1.9089673 5.3416281 8.1110659 9.0365038 6.5251741 2.3873041 -0.94680107 -3.4452159 -5.6250648 -6.5685825 -5.7028656][-8.4380121 -6.4345322 -5.2411308 -3.2589328 -0.27486265 2.8781731 5.1020288 5.9481058 4.0630379 0.55597925 -2.1859856 -4.5903516 -6.6743526 -7.416626 -6.4821796][-9.0883274 -7.26425 -6.7715425 -5.4733438 -3.0971558 -0.68182528 0.70967674 1.1271875 -0.077543974 -2.5803475 -4.4635277 -6.3281393 -7.9988365 -8.3211489 -7.0633497][-9.8129883 -8.1088848 -7.9924412 -7.3811407 -5.7879915 -4.1222878 -3.3996696 -3.3954272 -4.2250524 -5.7846317 -6.8628893 -8.0505705 -9.1034365 -8.77025 -7.1053638][-9.8205242 -8.2961264 -8.3906593 -8.24136 -7.4131126 -6.4227371 -6.0626307 -6.3033257 -6.97501 -7.8351183 -8.2767611 -8.8606462 -9.4061012 -8.6307917 -6.7279654][-8.5863209 -7.2224369 -7.4906206 -7.8006744 -7.7675705 -7.5111389 -7.4319544 -7.8070641 -8.3779993 -8.7703886 -8.7355022 -8.8732462 -8.8701839 -7.7588482 -5.9789338][-7.2184095 -5.962019 -6.2018652 -6.6736555 -7.1810417 -7.4321766 -7.5742722 -7.9179268 -8.291954 -8.4155207 -8.272851 -8.2156115 -7.8829794 -6.7817326 -5.4671016][-6.133481 -5.0034494 -5.141036 -5.5415225 -6.1281638 -6.5388527 -6.7365255 -6.913249 -7.1228228 -7.1852789 -7.1749487 -7.1028948 -6.7352324 -5.9444704 -5.2117062]]...]
INFO - root - 2017-12-15 07:07:51.962449: step 17010, loss = 0.30, batch loss = 0.27 (33.9 examples/sec; 0.236 sec/batch; 20h:40m:00s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 07:07:54.260935: step 17020, loss = 0.22, batch loss = 0.19 (34.1 examples/sec; 0.235 sec/batch; 20h:33m:53s remains)
INFO - root - 2017-12-15 07:07:56.528210: step 17030, loss = 0.31, batch loss = 0.28 (35.5 examples/sec; 0.226 sec/batch; 19h:46m:00s remains)
INFO - root - 2017-12-15 07:07:58.821023: step 17040, loss = 0.24, batch loss = 0.20 (34.4 examples/sec; 0.232 sec/batch; 20h:22m:22s remains)
INFO - root - 2017-12-15 07:08:01.126619: step 17050, loss = 0.23, batch loss = 0.20 (34.3 examples/sec; 0.233 sec/batch; 20h:26m:30s remains)
INFO - root - 2017-12-15 07:08:03.426494: step 17060, loss = 0.22, batch loss = 0.19 (34.2 examples/sec; 0.234 sec/batch; 20h:28m:52s remains)
INFO - root - 2017-12-15 07:08:05.702726: step 17070, loss = 0.25, batch loss = 0.22 (36.1 examples/sec; 0.222 sec/batch; 19h:24m:39s remains)
INFO - root - 2017-12-15 07:08:07.967117: step 17080, loss = 0.22, batch loss = 0.18 (35.5 examples/sec; 0.226 sec/batch; 19h:46m:20s remains)
INFO - root - 2017-12-15 07:08:10.274741: step 17090, loss = 0.33, batch loss = 0.29 (35.5 examples/sec; 0.225 sec/batch; 19h:43m:35s remains)
INFO - root - 2017-12-15 07:08:12.548302: step 17100, loss = 0.26, batch loss = 0.23 (35.0 examples/sec; 0.229 sec/batch; 20h:01m:13s remains)
2017-12-15 07:08:12.892339: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5365252 -5.3466048 -5.4880342 -5.5913944 -5.7667 -5.8485994 -5.8511066 -5.8783307 -6.0265951 -6.05855 -5.9720984 -5.4253545 -4.8464842 -4.624012 -4.4507241][-4.636713 -5.6912227 -5.7869453 -5.7821603 -5.8440924 -5.8074923 -5.6955628 -5.7256207 -5.9546404 -6.0627685 -6.0272346 -5.5993757 -5.1418452 -4.8370476 -4.5109024][-5.5298443 -5.6227436 -5.6376448 -5.497344 -5.4083157 -5.2400451 -5.0427632 -5.0118437 -5.2920918 -5.4155331 -5.3293514 -4.9594364 -4.6557641 -4.6078882 -4.4320221][-6.3121605 -5.5223274 -5.2144232 -4.7996435 -4.4469213 -4.113203 -3.8458281 -3.816433 -4.2564311 -4.466404 -4.3224988 -3.9536362 -3.8326573 -4.0680733 -4.0561042][-6.0955968 -4.7749043 -4.1336141 -3.431699 -2.8543069 -2.380481 -2.0884688 -2.0647275 -2.6811938 -2.9290059 -2.6876762 -2.2439845 -2.3900847 -2.9312952 -3.214901][-5.0501041 -3.3194244 -2.3476741 -1.3050239 -0.47388887 0.17840958 0.60381746 0.65817308 -0.20103836 -0.74577057 -0.70730484 -0.44833779 -1.0528276 -1.9967911 -2.6587698][-3.4933581 -2.0133648 -0.99943113 0.12809372 1.0296254 1.9019251 2.646656 2.8631463 1.8163705 1.1571434 1.1797974 1.4109001 0.56701064 -0.45893764 -1.3251872][-1.9927442 -0.95846474 -0.20457768 0.79594612 1.6729407 2.6033354 3.6365614 4.0312648 3.0275207 2.3468862 2.4088736 2.6620669 1.8319149 0.98276687 0.0029973984][-2.0717254 -1.4118901 -0.73738635 0.26357722 1.171859 2.164392 3.3690271 3.7402611 2.4839716 1.4486942 1.3601882 1.7213743 1.2657816 0.76391172 -0.040481329][-3.5262125 -3.1607559 -2.569005 -1.7125639 -0.86182356 0.066373825 1.2902994 1.5608928 0.29812098 -0.75582659 -0.68353 -0.0062365532 0.058095217 -0.12103105 -0.6706171][-5.4481153 -5.0767879 -4.3787203 -3.4810507 -2.61438 -1.9085454 -1.1422467 -1.189397 -2.3388567 -3.1770725 -2.7975216 -1.6156654 -0.8955785 -0.72005773 -0.99671423][-6.616745 -5.9626837 -5.1930227 -4.3865304 -3.6617594 -3.3913841 -3.1973937 -3.3985717 -4.1055813 -4.6366816 -4.1930995 -2.8736744 -1.7827731 -1.3710408 -1.3992753][-6.6284337 -5.66363 -4.9767323 -4.4531717 -3.9810009 -4.0504255 -4.3024344 -4.5695539 -4.9500241 -5.3396211 -5.0646 -3.841464 -2.7146795 -2.2660823 -2.1355376][-5.881958 -4.4133434 -3.791141 -3.5139666 -3.2656381 -3.5914428 -4.1999774 -4.6475677 -4.8966742 -5.2488842 -5.2558451 -4.3920355 -3.4997737 -3.0515196 -2.834115][-4.8061008 -3.0438461 -2.7194107 -2.7786555 -2.6394932 -2.8258388 -3.286716 -3.6248546 -3.8346336 -4.3986673 -4.8856359 -4.5273733 -3.9085476 -3.4754148 -3.2252917]]...]
INFO - root - 2017-12-15 07:08:15.216218: step 17110, loss = 0.19, batch loss = 0.16 (33.2 examples/sec; 0.241 sec/batch; 21h:08m:11s remains)
INFO - root - 2017-12-15 07:08:17.502248: step 17120, loss = 0.17, batch loss = 0.14 (34.6 examples/sec; 0.231 sec/batch; 20h:13m:42s remains)
INFO - root - 2017-12-15 07:08:19.805778: step 17130, loss = 0.22, batch loss = 0.19 (34.7 examples/sec; 0.231 sec/batch; 20h:13m:09s remains)
INFO - root - 2017-12-15 07:08:22.071126: step 17140, loss = 0.38, batch loss = 0.35 (36.0 examples/sec; 0.222 sec/batch; 19h:28m:37s remains)
INFO - root - 2017-12-15 07:08:24.379863: step 17150, loss = 0.21, batch loss = 0.18 (34.8 examples/sec; 0.230 sec/batch; 20h:07m:29s remains)
INFO - root - 2017-12-15 07:08:26.716835: step 17160, loss = 0.21, batch loss = 0.18 (35.5 examples/sec; 0.225 sec/batch; 19h:44m:56s remains)
INFO - root - 2017-12-15 07:08:29.065726: step 17170, loss = 0.39, batch loss = 0.36 (34.3 examples/sec; 0.233 sec/batch; 20h:24m:01s remains)
INFO - root - 2017-12-15 07:08:31.332766: step 17180, loss = 0.20, batch loss = 0.17 (34.7 examples/sec; 0.230 sec/batch; 20h:10m:50s remains)
INFO - root - 2017-12-15 07:08:33.645218: step 17190, loss = 0.24, batch loss = 0.21 (36.1 examples/sec; 0.222 sec/batch; 19h:25m:38s remains)
INFO - root - 2017-12-15 07:08:35.932822: step 17200, loss = 0.21, batch loss = 0.18 (34.5 examples/sec; 0.232 sec/batch; 20h:18m:02s remains)
2017-12-15 07:08:36.301585: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0862753 -3.5991664 -3.8779669 -4.0392652 -3.9730122 -3.6230788 -3.4076772 -3.18445 -3.3168435 -3.9106498 -4.4226327 -4.6742477 -4.3344889 -3.9171715 -3.5161352][-2.6862085 -3.54316 -3.6609149 -3.5639682 -3.1214008 -2.3101459 -1.7634288 -1.3746897 -1.6881361 -2.8022051 -4.0045142 -4.8831253 -4.8779111 -4.5339584 -4.0015788][-3.3692164 -3.3602209 -3.3211911 -2.9795058 -2.1461477 -0.87061548 0.12080574 0.84402466 0.56577444 -0.95000565 -2.7837827 -4.3704348 -4.8400245 -4.6955976 -4.1345224][-3.8390641 -3.2082024 -3.0991406 -2.6313634 -1.5200865 0.110672 1.558171 2.7272081 2.6813574 1.0054092 -1.2847493 -3.5164008 -4.4645958 -4.6034555 -4.07865][-4.3875961 -3.1364646 -2.9942539 -2.4653411 -1.1773517 0.64277458 2.3760171 3.8838649 4.1066346 2.3917637 -0.19590759 -2.860117 -4.1642728 -4.5041842 -4.0561266][-4.464076 -3.1124883 -2.9677229 -2.4302554 -1.0703615 0.79831529 2.6172395 4.2631307 4.6784544 3.0045853 0.27847791 -2.6056747 -4.1246052 -4.5663462 -4.1613593][-4.2179751 -3.2195609 -3.0963001 -2.6055222 -1.2900037 0.51208496 2.2549582 3.8607769 4.3872957 2.883172 0.23474574 -2.6706643 -4.3050194 -4.7824926 -4.3979855][-4.6233158 -3.6044574 -3.4875093 -3.0514398 -1.8085842 -0.089243174 1.5453041 3.041461 3.6338019 2.3638759 -0.00961113 -2.7388666 -4.4396305 -4.9545708 -4.6039391][-5.191103 -4.177331 -4.0701656 -3.6871293 -2.5285108 -0.87310004 0.75643373 2.2160974 2.884057 1.9360909 -0.020297766 -2.4795663 -4.2193551 -4.8386412 -4.6081123][-5.623395 -4.7012582 -4.6306949 -4.3010855 -3.2065983 -1.5980556 0.082332611 1.5732214 2.3182588 1.6481762 0.095641613 -2.050864 -3.7327895 -4.4779372 -4.45753][-5.7403851 -5.0068254 -4.988039 -4.696372 -3.6518741 -2.1438241 -0.47314107 1.0166385 1.7504704 1.2699196 0.046638727 -1.806161 -3.3447242 -4.1380353 -4.2990823][-5.3892365 -4.938921 -5.053072 -4.8297615 -3.8929398 -2.5465097 -0.97200608 0.501709 1.2288716 0.86153388 -0.12165999 -1.7078264 -3.1011062 -3.8952861 -4.168211][-4.8248081 -4.6065521 -4.8154874 -4.6487532 -3.8428025 -2.7170942 -1.3418436 0.016066074 0.64460158 0.28317285 -0.63159657 -1.9804906 -3.2150855 -3.9187496 -4.1493864][-4.3427172 -4.22488 -4.402216 -4.2251058 -3.5537109 -2.6713607 -1.5423309 -0.35924792 0.12803531 -0.26577234 -1.2000041 -2.3671122 -3.4369335 -4.0609736 -4.1936927][-4.1636977 -3.9700813 -3.9807639 -3.74431 -3.2043378 -2.5599535 -1.6836549 -0.72128 -0.40117168 -0.83207881 -1.7838019 -2.7897139 -3.623446 -4.1149759 -4.1071005]]...]
INFO - root - 2017-12-15 07:08:38.582030: step 17210, loss = 0.16, batch loss = 0.13 (33.3 examples/sec; 0.240 sec/batch; 21h:00m:42s remains)
INFO - root - 2017-12-15 07:08:40.869832: step 17220, loss = 0.22, batch loss = 0.19 (34.6 examples/sec; 0.231 sec/batch; 20h:13m:41s remains)
INFO - root - 2017-12-15 07:08:43.155628: step 17230, loss = 0.19, batch loss = 0.16 (36.2 examples/sec; 0.221 sec/batch; 19h:22m:38s remains)
INFO - root - 2017-12-15 07:08:45.490064: step 17240, loss = 0.19, batch loss = 0.16 (34.6 examples/sec; 0.232 sec/batch; 20h:16m:27s remains)
INFO - root - 2017-12-15 07:08:47.767980: step 17250, loss = 0.40, batch loss = 0.37 (36.3 examples/sec; 0.220 sec/batch; 19h:18m:02s remains)
INFO - root - 2017-12-15 07:08:50.068144: step 17260, loss = 0.24, batch loss = 0.21 (34.6 examples/sec; 0.231 sec/batch; 20h:13m:52s remains)
INFO - root - 2017-12-15 07:08:52.389689: step 17270, loss = 0.26, batch loss = 0.22 (35.0 examples/sec; 0.228 sec/batch; 19h:59m:33s remains)
INFO - root - 2017-12-15 07:08:54.680387: step 17280, loss = 0.20, batch loss = 0.17 (32.5 examples/sec; 0.246 sec/batch; 21h:33m:59s remains)
INFO - root - 2017-12-15 07:08:56.936859: step 17290, loss = 0.30, batch loss = 0.27 (35.0 examples/sec; 0.229 sec/batch; 20h:01m:39s remains)
INFO - root - 2017-12-15 07:08:59.199562: step 17300, loss = 0.21, batch loss = 0.18 (35.2 examples/sec; 0.227 sec/batch; 19h:54m:12s remains)
2017-12-15 07:08:59.542371: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.6795063 -7.8086724 -6.410831 -5.1914053 -4.3311806 -4.0081491 -4.0801525 -4.7260847 -5.6519809 -6.2137175 -6.5626802 -6.9644642 -6.8739667 -6.031333 -5.2183552][-8.8789921 -9.4324474 -8.4449263 -7.4529681 -6.5291452 -5.99438 -5.90215 -6.4175014 -7.2784104 -7.8205571 -8.14101 -8.4956074 -8.4002666 -7.576581 -6.6811051][-10.454176 -10.114963 -9.2077618 -8.1527576 -6.9159546 -5.9439621 -5.5660048 -5.9381819 -6.7755318 -7.4203176 -7.8097854 -8.17918 -8.2971869 -7.8993635 -7.3550348][-10.890836 -9.771347 -8.7491837 -7.39754 -5.6075292 -4.0539041 -3.3542221 -3.5451112 -4.4383259 -5.4569221 -6.2134571 -6.8863773 -7.4241385 -7.5837731 -7.5340919][-10.256836 -8.5851908 -7.2319889 -5.3408904 -2.7682552 -0.49080527 0.66825128 0.6811626 -0.49189687 -2.2609158 -3.7919421 -5.2198391 -6.4590435 -7.2097859 -7.6148868][-9.2625523 -7.0139618 -5.26433 -2.7728415 0.51817083 3.4621773 5.1481271 5.3255072 3.7428193 1.2138755 -1.2041111 -3.5266972 -5.4401088 -6.6604905 -7.4239373][-7.7780719 -6.017756 -4.1947441 -1.5294365 2.0010719 5.348505 7.5519323 7.9599218 6.2285223 3.3120508 0.35231376 -2.5881417 -4.9065533 -6.333365 -7.2558565][-7.663527 -6.0666342 -4.5008688 -2.0802803 1.2081795 4.5301752 6.9661722 7.52738 5.8568573 3.1184082 0.17181921 -2.827172 -5.0224075 -6.3680081 -7.2674127][-8.1449833 -6.78894 -5.4440956 -3.3567948 -0.55846035 2.4086118 4.7004356 5.1050091 3.5139627 1.1319058 -1.4524446 -4.0530024 -5.8088961 -6.790988 -7.4403048][-8.887413 -7.7639675 -6.6907873 -5.0291772 -2.881412 -0.60266685 1.0726488 1.168833 -0.19884372 -1.9260647 -3.8476324 -5.7316332 -6.8615823 -7.4452496 -7.8047724][-9.2746887 -8.2940836 -7.4737606 -6.2482452 -4.7972736 -3.3504286 -2.435322 -2.6430345 -3.7065291 -4.800189 -5.9337492 -7.0036936 -7.547893 -7.7789469 -7.9176359][-9.00971 -8.0630951 -7.4291272 -6.56489 -5.6846695 -5.0478897 -4.8716311 -5.3392315 -6.1485586 -6.7103977 -7.177681 -7.5793233 -7.6715212 -7.6062508 -7.5850039][-8.4102306 -7.494803 -6.988915 -6.3669958 -5.8785028 -5.80888 -6.0768976 -6.6993294 -7.2868052 -7.4943395 -7.5799646 -7.6474681 -7.5099807 -7.2978258 -7.1998329][-7.6906872 -6.7810893 -6.2811594 -5.6960964 -5.3023796 -5.3593264 -5.6504068 -6.2300987 -6.7284694 -6.8674755 -6.9333138 -6.9715991 -6.7872081 -6.5366731 -6.4369035][-6.9707079 -5.9699268 -5.3613648 -4.6947117 -4.2421083 -4.2698469 -4.5033379 -5.0580854 -5.5325708 -5.6804314 -5.8441715 -5.9723787 -5.867135 -5.6695352 -5.5783291]]...]
INFO - root - 2017-12-15 07:09:01.839795: step 17310, loss = 0.33, batch loss = 0.30 (34.7 examples/sec; 0.231 sec/batch; 20h:12m:38s remains)
INFO - root - 2017-12-15 07:09:04.115786: step 17320, loss = 0.19, batch loss = 0.16 (34.6 examples/sec; 0.231 sec/batch; 20h:14m:15s remains)
INFO - root - 2017-12-15 07:09:06.448323: step 17330, loss = 0.16, batch loss = 0.13 (34.5 examples/sec; 0.232 sec/batch; 20h:19m:28s remains)
INFO - root - 2017-12-15 07:09:08.713457: step 17340, loss = 0.17, batch loss = 0.14 (35.2 examples/sec; 0.227 sec/batch; 19h:52m:29s remains)
INFO - root - 2017-12-15 07:09:11.021500: step 17350, loss = 0.20, batch loss = 0.17 (32.2 examples/sec; 0.248 sec/batch; 21h:44m:44s remains)
INFO - root - 2017-12-15 07:09:13.276024: step 17360, loss = 0.20, batch loss = 0.16 (34.6 examples/sec; 0.231 sec/batch; 20h:14m:39s remains)
INFO - root - 2017-12-15 07:09:15.616339: step 17370, loss = 0.22, batch loss = 0.19 (34.0 examples/sec; 0.235 sec/batch; 20h:36m:29s remains)
INFO - root - 2017-12-15 07:09:17.907188: step 17380, loss = 0.31, batch loss = 0.28 (36.0 examples/sec; 0.222 sec/batch; 19h:26m:10s remains)
INFO - root - 2017-12-15 07:09:20.214407: step 17390, loss = 0.23, batch loss = 0.19 (35.8 examples/sec; 0.224 sec/batch; 19h:34m:32s remains)
INFO - root - 2017-12-15 07:09:22.470149: step 17400, loss = 0.25, batch loss = 0.22 (34.8 examples/sec; 0.230 sec/batch; 20h:06m:50s remains)
2017-12-15 07:09:22.848030: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.70801842 -2.04572 -2.2757277 -2.271384 -2.4074876 -3.1668079 -4.1420288 -4.8684597 -5.6179161 -6.2880383 -6.8484898 -7.0735292 -6.9606352 -6.9601316 -6.8000822][-2.4575438 -3.5428386 -3.8275344 -3.6207163 -3.4583824 -3.95168 -4.6720991 -5.2434845 -5.8200731 -6.4168882 -6.9842434 -7.0777006 -6.9987745 -7.0588179 -7.0078382][-4.5646181 -5.411262 -5.736536 -5.2589569 -4.7866364 -4.8211985 -5.013412 -5.3843012 -5.81452 -6.209836 -6.462306 -6.2587395 -6.0802989 -6.0873718 -5.9310632][-6.7425032 -7.3731356 -7.5224285 -6.5411186 -5.43476 -4.7032661 -4.3824792 -4.6790237 -5.1746616 -5.4430189 -5.4524841 -4.9812508 -4.6200118 -4.5152526 -4.3934951][-8.5914764 -8.1232243 -7.5649529 -5.7742333 -3.8887339 -2.3948946 -1.5956078 -2.0262334 -3.0732541 -3.7494788 -3.9213309 -3.5031714 -3.1986029 -3.1412265 -3.2398407][-8.1852255 -7.1087151 -5.96539 -3.6232357 -1.084839 1.2554302 2.4774892 1.6274958 -0.336828 -1.710006 -2.2644324 -2.0399427 -1.7658137 -1.8019928 -2.0946493][-6.446373 -5.5427427 -4.1918516 -1.6605482 1.3405974 4.530633 6.1878185 4.8283682 2.0891473 0.094900608 -0.99612391 -0.998163 -0.536258 -0.62749064 -1.2085812][-5.5332947 -4.481513 -3.0289481 -0.33454967 2.9086359 6.5007534 8.3896685 6.8670311 3.7083447 1.1524072 -0.61689138 -0.8654201 -0.1949122 -0.29691577 -1.1407495][-5.2068739 -4.3458867 -3.179981 -0.72489858 2.0861943 5.1996546 6.8745337 5.5949783 2.8568056 0.32160807 -1.3676891 -1.4609052 -0.53143072 -0.49949336 -1.3955758][-5.5498242 -5.08302 -4.5768776 -2.7651825 -0.6715312 1.6546965 3.1419013 2.4637821 0.48061609 -1.3994231 -2.395936 -2.2326829 -1.4743394 -1.4421035 -2.2132404][-6.5667791 -6.2264032 -6.1999016 -4.8470397 -3.3522916 -1.7448697 -0.4213717 -0.54131877 -2.0030384 -3.4648352 -4.00401 -3.8379865 -3.3016436 -3.2207198 -3.8207192][-8.2605934 -7.7931833 -8.0589094 -7.1373291 -6.1646614 -5.1320758 -4.1671228 -4.0880313 -5.1835809 -6.1799145 -6.2578154 -5.9974928 -5.5155749 -5.1947851 -5.4554348][-9.872673 -9.3723488 -9.9772282 -9.6069794 -9.0825586 -8.4463062 -7.7015276 -7.4759884 -8.03403 -8.4506454 -8.1414833 -7.70238 -7.2697859 -6.8027668 -6.7982035][-10.406147 -9.8656769 -10.617817 -10.657327 -10.492918 -10.140847 -9.4738989 -9.0342484 -9.1608 -9.2697334 -8.941371 -8.6376 -8.4392395 -8.1189861 -8.1344881][-10.402094 -9.7187195 -10.269968 -10.399383 -10.462937 -10.307892 -9.6947536 -9.0816364 -8.95875 -9.0451508 -8.9740248 -9.0896692 -9.2885265 -9.226469 -9.2462177]]...]
INFO - root - 2017-12-15 07:09:25.117252: step 17410, loss = 0.28, batch loss = 0.24 (33.9 examples/sec; 0.236 sec/batch; 20h:38m:12s remains)
INFO - root - 2017-12-15 07:09:27.415878: step 17420, loss = 0.33, batch loss = 0.30 (34.2 examples/sec; 0.234 sec/batch; 20h:29m:39s remains)
INFO - root - 2017-12-15 07:09:29.675756: step 17430, loss = 0.25, batch loss = 0.22 (35.8 examples/sec; 0.223 sec/batch; 19h:33m:23s remains)
INFO - root - 2017-12-15 07:09:31.962491: step 17440, loss = 0.37, batch loss = 0.34 (35.5 examples/sec; 0.226 sec/batch; 19h:44m:47s remains)
INFO - root - 2017-12-15 07:09:34.273627: step 17450, loss = 0.22, batch loss = 0.19 (35.8 examples/sec; 0.223 sec/batch; 19h:33m:14s remains)
INFO - root - 2017-12-15 07:09:36.542093: step 17460, loss = 0.18, batch loss = 0.15 (35.4 examples/sec; 0.226 sec/batch; 19h:44m:55s remains)
INFO - root - 2017-12-15 07:09:38.840046: step 17470, loss = 0.16, batch loss = 0.12 (34.2 examples/sec; 0.234 sec/batch; 20h:26m:24s remains)
INFO - root - 2017-12-15 07:09:41.168128: step 17480, loss = 0.25, batch loss = 0.21 (34.0 examples/sec; 0.235 sec/batch; 20h:33m:48s remains)
INFO - root - 2017-12-15 07:09:43.447376: step 17490, loss = 0.29, batch loss = 0.26 (35.5 examples/sec; 0.225 sec/batch; 19h:43m:48s remains)
INFO - root - 2017-12-15 07:09:45.736223: step 17500, loss = 0.27, batch loss = 0.24 (35.8 examples/sec; 0.224 sec/batch; 19h:33m:35s remains)
2017-12-15 07:09:46.082744: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1716609 -5.3758378 -5.5638084 -5.7905 -6.01996 -6.2486248 -6.31143 -6.2259135 -6.2146206 -6.360136 -6.4402485 -6.4721951 -6.6489239 -6.6581464 -6.1820383][-3.8170798 -4.9881639 -5.4202275 -5.8630962 -6.1628389 -6.2467546 -6.0204411 -5.8064404 -6.0226159 -6.54123 -6.879137 -7.1031923 -7.5009179 -7.5160761 -6.93133][-2.6378241 -3.4855132 -4.2122631 -4.9950452 -5.4448633 -5.3351316 -4.6483116 -4.181488 -4.6685739 -5.7036233 -6.4319515 -6.9348769 -7.5131087 -7.4439096 -6.7573881][-1.6274641 -2.1030064 -3.0459447 -4.1539111 -4.7298684 -4.2268534 -2.9138529 -2.1764011 -3.0013027 -4.6473417 -5.8247776 -6.6549749 -7.3805447 -7.1523781 -6.3382092][-1.0123221 -1.1496011 -2.2580214 -3.6069164 -4.1841626 -3.114466 -1.0260308 0.064569 -1.1482654 -3.3346105 -4.8753929 -6.0003862 -6.8072705 -6.3960285 -5.5101185][-1.1286486 -0.870541 -1.8945519 -3.2048514 -3.5218754 -1.8054321 0.98710752 2.2829714 0.67525077 -2.0232806 -3.8270941 -5.024581 -5.7044859 -5.167181 -4.3828011][-1.8175384 -1.6019002 -2.4158902 -3.2957802 -2.9180288 -0.40945184 3.0341592 4.52602 2.5775957 -0.65552306 -2.7455585 -3.9396529 -4.4038763 -3.8402581 -3.2325287][-2.7704556 -2.5991161 -3.2193055 -3.5496917 -2.4259744 0.54769874 4.1192985 5.6288667 3.5892992 0.20407486 -1.9336448 -2.9146931 -3.195508 -2.7237236 -2.3966389][-3.0942745 -2.9941788 -3.507231 -3.4270232 -1.9419093 0.70177794 3.5528636 4.707891 2.9179506 0.030090094 -1.6990988 -2.2416615 -2.3211656 -2.0641513 -2.0920877][-2.7985489 -2.6591041 -3.128906 -2.8621142 -1.5649816 0.26107335 1.9662189 2.5864425 1.230999 -0.83241606 -1.8960124 -1.9345003 -1.8175397 -1.7944888 -2.1022925][-2.1083674 -1.8125477 -2.2505462 -2.032016 -1.178552 -0.27352798 0.37343717 0.45879316 -0.49573851 -1.6755185 -2.1097012 -1.7986915 -1.616292 -1.8268147 -2.3808763][-1.5978688 -1.1444901 -1.6102316 -1.4948697 -1.0849572 -0.86424983 -0.92497337 -1.2012441 -1.7815984 -2.3487594 -2.3546033 -1.8952842 -1.7437894 -2.204401 -2.9500918][-1.6416992 -1.01287 -1.4077446 -1.354841 -1.271522 -1.4581149 -1.8951932 -2.3911395 -2.7334559 -2.8362136 -2.5373251 -1.988346 -1.9652386 -2.7506943 -3.6870458][-2.1992044 -1.3706571 -1.6445179 -1.5601293 -1.6471043 -2.0547233 -2.7004697 -3.354358 -3.4916515 -3.285362 -2.8014224 -2.2594528 -2.3647366 -3.3491154 -4.3522272][-2.9682822 -2.1301887 -2.3872638 -2.3044181 -2.4205041 -2.8569474 -3.5152178 -4.08363 -4.0186572 -3.644628 -3.1211562 -2.6728148 -2.8652897 -3.8652539 -4.7544513]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-17500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-17500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 07:09:48.784038: step 17510, loss = 0.20, batch loss = 0.16 (35.6 examples/sec; 0.225 sec/batch; 19h:40m:00s remains)
INFO - root - 2017-12-15 07:09:51.075166: step 17520, loss = 0.28, batch loss = 0.24 (35.5 examples/sec; 0.226 sec/batch; 19h:44m:32s remains)
INFO - root - 2017-12-15 07:09:53.331605: step 17530, loss = 0.21, batch loss = 0.17 (34.6 examples/sec; 0.231 sec/batch; 20h:13m:06s remains)
INFO - root - 2017-12-15 07:09:55.591027: step 17540, loss = 0.40, batch loss = 0.37 (36.3 examples/sec; 0.220 sec/batch; 19h:16m:58s remains)
INFO - root - 2017-12-15 07:09:57.862134: step 17550, loss = 0.21, batch loss = 0.17 (34.3 examples/sec; 0.233 sec/batch; 20h:24m:00s remains)
INFO - root - 2017-12-15 07:10:00.137593: step 17560, loss = 0.25, batch loss = 0.21 (35.3 examples/sec; 0.227 sec/batch; 19h:49m:37s remains)
INFO - root - 2017-12-15 07:10:02.413958: step 17570, loss = 0.25, batch loss = 0.21 (34.9 examples/sec; 0.229 sec/batch; 20h:03m:44s remains)
INFO - root - 2017-12-15 07:10:04.652220: step 17580, loss = 0.19, batch loss = 0.16 (36.2 examples/sec; 0.221 sec/batch; 19h:20m:52s remains)
INFO - root - 2017-12-15 07:10:06.921985: step 17590, loss = 0.21, batch loss = 0.18 (35.6 examples/sec; 0.225 sec/batch; 19h:39m:10s remains)
INFO - root - 2017-12-15 07:10:09.226333: step 17600, loss = 0.25, batch loss = 0.22 (34.2 examples/sec; 0.234 sec/batch; 20h:25m:59s remains)
2017-12-15 07:10:09.587412: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.977362 -3.4696393 -4.3040133 -5.16269 -5.8891807 -6.59436 -6.9540377 -6.9901767 -6.9668045 -6.6640072 -6.3157144 -6.3631978 -6.6314754 -6.8348346 -6.806231][-2.5398018 -3.2557354 -4.2643538 -5.3058329 -6.0022488 -6.5133524 -6.8744216 -6.9601297 -6.9716287 -6.8592534 -6.772285 -6.7505603 -6.9746695 -7.2445526 -7.4075136][-4.0392628 -4.1663938 -5.1016264 -5.9982004 -6.3294821 -6.3527088 -6.3513846 -6.221632 -6.1584244 -6.170166 -6.1177835 -5.8572655 -5.8612165 -6.0694914 -6.3960934][-6.1144567 -5.59651 -6.3149633 -6.8588982 -6.6127915 -5.9110794 -5.2818136 -4.9571161 -5.1276722 -5.5924149 -5.7161169 -5.3053589 -5.0512934 -5.1120434 -5.5467186][-8.243515 -7.4366636 -7.7172222 -7.6600633 -6.4361658 -4.6094322 -3.1406589 -2.7666352 -3.543606 -4.9739819 -5.6549659 -5.489954 -5.124917 -4.969326 -5.3244076][-10.454129 -8.7349482 -8.3779163 -7.4985542 -5.2378569 -2.2066736 0.32862616 0.7597878 -0.94040847 -3.7258971 -5.3574362 -5.99538 -5.8413754 -5.5294542 -5.6157994][-10.261075 -8.5041656 -7.6906738 -6.1821051 -2.9733105 1.4423563 5.223598 5.7328138 2.9004421 -1.4832261 -4.3785768 -6.1768875 -6.4455252 -6.0157132 -5.6796074][-9.67355 -7.6859121 -6.6650953 -4.7900257 -0.9673934 4.5026913 9.3689108 10.058404 6.3175335 0.57713437 -3.4744811 -6.2797408 -6.903739 -6.4223552 -5.8155632][-9.0971222 -7.2417583 -6.2441831 -4.3711729 -0.36396015 5.5635066 11.024874 11.930765 7.7819643 1.3538857 -3.225132 -6.3886609 -7.2151465 -7.0157671 -6.6646385][-9.4190693 -8.0963335 -7.3386517 -5.5914488 -1.9250417 3.6462235 8.8891029 9.7791758 5.7847385 -0.2369765 -4.3591194 -7.0600233 -7.8970141 -8.09576 -8.0575447][-10.192883 -9.3399239 -8.8636913 -7.4513006 -4.4660692 -0.015673161 4.121398 4.7733808 1.4900105 -3.3471751 -6.503582 -8.4054108 -9.1123247 -9.5409889 -9.6867771][-10.653363 -10.209721 -10.022233 -8.9555244 -6.8508735 -3.8392003 -1.1273729 -0.78230846 -3.0965459 -6.4341149 -8.5098505 -9.7021475 -10.331989 -10.922089 -11.231218][-10.453235 -10.11026 -10.078428 -9.3054647 -7.9437604 -6.1655817 -4.6376209 -4.4937363 -5.912055 -7.9446864 -9.2013 -9.8482914 -10.271842 -10.780969 -10.987926][-9.2070436 -8.56613 -8.50231 -8.003891 -7.2876453 -6.4834528 -5.8754005 -5.9066334 -6.6605024 -7.7090869 -8.3828268 -8.7779865 -9.0798159 -9.3701439 -9.4344673][-7.590138 -6.6644783 -6.6254988 -6.4483786 -6.2247505 -6.0377116 -5.9300056 -5.9853282 -6.2501054 -6.6842089 -7.0700455 -7.3735938 -7.5550737 -7.6667919 -7.6349664]]...]
INFO - root - 2017-12-15 07:10:11.860938: step 17610, loss = 0.29, batch loss = 0.26 (34.8 examples/sec; 0.230 sec/batch; 20h:06m:47s remains)
INFO - root - 2017-12-15 07:10:14.140028: step 17620, loss = 0.30, batch loss = 0.27 (34.0 examples/sec; 0.236 sec/batch; 20h:36m:07s remains)
INFO - root - 2017-12-15 07:10:16.407495: step 17630, loss = 0.35, batch loss = 0.32 (34.2 examples/sec; 0.234 sec/batch; 20h:27m:33s remains)
INFO - root - 2017-12-15 07:10:18.667777: step 17640, loss = 0.30, batch loss = 0.26 (35.9 examples/sec; 0.223 sec/batch; 19h:29m:54s remains)
INFO - root - 2017-12-15 07:10:20.978833: step 17650, loss = 0.27, batch loss = 0.23 (31.6 examples/sec; 0.253 sec/batch; 22h:08m:51s remains)
INFO - root - 2017-12-15 07:10:23.308954: step 17660, loss = 0.29, batch loss = 0.26 (34.7 examples/sec; 0.231 sec/batch; 20h:10m:35s remains)
INFO - root - 2017-12-15 07:10:25.664253: step 17670, loss = 0.29, batch loss = 0.26 (34.2 examples/sec; 0.234 sec/batch; 20h:28m:21s remains)
INFO - root - 2017-12-15 07:10:27.937580: step 17680, loss = 0.27, batch loss = 0.24 (34.9 examples/sec; 0.229 sec/batch; 20h:02m:10s remains)
INFO - root - 2017-12-15 07:10:30.219343: step 17690, loss = 0.27, batch loss = 0.23 (34.7 examples/sec; 0.231 sec/batch; 20h:10m:39s remains)
INFO - root - 2017-12-15 07:10:32.476981: step 17700, loss = 0.24, batch loss = 0.21 (35.1 examples/sec; 0.228 sec/batch; 19h:55m:30s remains)
2017-12-15 07:10:32.811298: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2318523 -3.7418964 -3.513176 -3.1233988 -2.4273348 -1.9116535 -2.0816984 -3.0143998 -3.3982296 -4.04471 -4.3491945 -4.8378172 -5.5071373 -5.0886354 -4.7691612][-2.0457683 -2.6467886 -2.5951834 -2.1521463 -1.0278496 -0.30942214 -0.36700642 -1.4957167 -2.1033711 -2.9248607 -3.5638766 -4.2685385 -5.47663 -5.5646524 -5.4706678][-2.4335177 -2.1254289 -2.1315515 -1.5132022 0.0031335354 0.8810463 0.91057467 -0.35204685 -1.1187663 -1.9823384 -2.9067507 -3.6920941 -5.1714506 -5.7100735 -5.9162025][-3.13472 -2.052798 -1.9992661 -1.1512265 0.54847145 1.5854595 1.6525047 0.28932738 -0.55537009 -1.3896596 -2.4261363 -3.218802 -4.6688032 -5.459631 -5.9207458][-3.6183209 -1.9145286 -1.7189806 -0.77391922 0.81100178 1.7745166 1.8670483 0.61374474 -0.097134352 -0.87119532 -1.8727312 -2.5702713 -3.81028 -4.7193317 -5.2796679][-3.8232188 -1.9885008 -1.5391245 -0.39451218 0.89815474 1.4933572 1.3734617 0.17674613 -0.34304404 -0.74448383 -1.4305686 -1.7394705 -2.608098 -3.6976781 -4.3928313][-3.5618687 -1.8816515 -1.3006407 -0.09138155 0.83473897 1.0701206 0.67000675 -0.43743753 -0.78807831 -0.84379172 -0.97080636 -0.67826271 -1.1929026 -2.4832833 -3.5400429][-2.558707 -0.83390117 -0.42139304 0.527087 1.2316699 1.3590033 0.93096042 -0.030706644 -0.44587278 -0.57936788 -0.53935027 0.0924201 -0.21060896 -1.5464909 -2.7449744][-1.7381482 -0.20086813 -0.11503935 0.61394382 1.2261176 1.3384466 0.94597626 0.15491343 -0.39605296 -0.67858493 -0.78987992 -0.22228003 -0.35208416 -1.5363073 -2.4858561][-1.9678719 -0.23953652 -0.22439003 0.42746329 0.91064692 0.95080709 0.55395579 -0.16010857 -0.77453065 -1.1486291 -1.3223828 -0.87299728 -0.94974172 -1.9057683 -2.551101][-2.349318 -0.501534 -0.48385787 0.0076436996 0.35810375 0.49953556 0.24240017 -0.41377139 -1.077064 -1.5948628 -1.8128439 -1.4024007 -1.3186575 -1.9192691 -2.3075643][-2.4568803 -0.72011542 -0.912356 -0.663993 -0.46093965 -0.16364217 -0.15496087 -0.58564281 -1.2424316 -1.8589271 -2.2479072 -2.1652014 -1.9152644 -1.9831336 -1.9545401][-2.4151843 -0.91282308 -1.1828581 -1.1152961 -1.1697681 -0.91558361 -0.75322616 -0.88549829 -1.2616863 -1.7848928 -2.2256052 -2.4469039 -2.2447047 -2.0524576 -1.9136922][-1.4374756 -0.35842311 -0.81769967 -1.0145686 -1.289551 -1.2735159 -1.2389147 -1.2173555 -1.2339034 -1.4329345 -1.7663505 -2.0704386 -1.9193722 -1.5805755 -1.5421939][-0.62826574 0.0961175 -0.70495331 -1.1030298 -1.2878201 -1.2066599 -1.2286479 -1.2997702 -1.1415004 -1.1221018 -1.4418576 -1.8737732 -1.7352369 -1.3039107 -1.250535]]...]
INFO - root - 2017-12-15 07:10:35.092214: step 17710, loss = 0.27, batch loss = 0.24 (34.5 examples/sec; 0.232 sec/batch; 20h:17m:07s remains)
INFO - root - 2017-12-15 07:10:37.428168: step 17720, loss = 0.18, batch loss = 0.14 (35.6 examples/sec; 0.225 sec/batch; 19h:39m:08s remains)
INFO - root - 2017-12-15 07:10:39.701731: step 17730, loss = 0.23, batch loss = 0.19 (35.8 examples/sec; 0.224 sec/batch; 19h:32m:38s remains)
INFO - root - 2017-12-15 07:10:41.953919: step 17740, loss = 0.20, batch loss = 0.17 (35.6 examples/sec; 0.225 sec/batch; 19h:38m:13s remains)
INFO - root - 2017-12-15 07:10:44.215050: step 17750, loss = 0.32, batch loss = 0.29 (35.4 examples/sec; 0.226 sec/batch; 19h:45m:55s remains)
INFO - root - 2017-12-15 07:10:46.487864: step 17760, loss = 0.20, batch loss = 0.17 (34.3 examples/sec; 0.233 sec/batch; 20h:21m:50s remains)
INFO - root - 2017-12-15 07:10:48.761064: step 17770, loss = 0.20, batch loss = 0.16 (35.0 examples/sec; 0.229 sec/batch; 20h:00m:02s remains)
INFO - root - 2017-12-15 07:10:51.020758: step 17780, loss = 0.19, batch loss = 0.15 (35.3 examples/sec; 0.226 sec/batch; 19h:47m:09s remains)
INFO - root - 2017-12-15 07:10:53.272358: step 17790, loss = 0.25, batch loss = 0.22 (34.4 examples/sec; 0.233 sec/batch; 20h:20m:01s remains)
INFO - root - 2017-12-15 07:10:55.587316: step 17800, loss = 0.22, batch loss = 0.19 (36.5 examples/sec; 0.219 sec/batch; 19h:10m:14s remains)
2017-12-15 07:10:55.928711: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7777715 -6.4778614 -6.1263623 -5.8311853 -5.0439029 -4.5157948 -4.8584518 -5.2761254 -5.4590731 -6.3049369 -6.7059975 -6.1847897 -5.5832415 -3.6380911 -2.0976241][-5.6274881 -6.5198011 -6.3117027 -5.995286 -5.0275831 -4.2660737 -4.5922995 -5.3411055 -5.7432427 -6.6331763 -7.1979117 -6.4915366 -5.8258066 -4.1114407 -2.7155013][-6.7383075 -6.871748 -6.52486 -5.7509937 -4.2237244 -3.0566492 -3.1596794 -4.3051434 -5.319665 -6.5999336 -7.4961791 -6.956708 -6.453444 -5.150898 -3.8777096][-7.8039079 -7.276042 -6.8357358 -5.5601439 -3.3603945 -1.5422733 -1.0858144 -2.4397721 -4.1560783 -5.8717546 -7.1196918 -7.1574955 -7.2169127 -6.5570617 -5.51293][-9.0404491 -8.0155058 -7.4997482 -5.7482719 -2.8149288 -0.18114018 0.98811054 -0.46753836 -2.8963342 -4.9598475 -6.4928317 -7.1877632 -7.7673922 -7.5551157 -6.7574539][-8.6195755 -7.7200732 -7.0939174 -4.7837052 -0.93293607 2.6926394 4.6494336 3.0457568 -0.13708138 -2.7774472 -4.830575 -6.3048143 -7.3405128 -7.4109993 -6.8327823][-6.5845704 -6.1459808 -5.6216097 -3.0570171 1.4129951 5.8557024 8.4460821 6.7141228 2.9939327 -0.12812757 -2.5916071 -4.6160612 -6.0005045 -6.3673444 -6.1849117][-5.4562654 -5.0884676 -4.5342827 -1.7985185 3.1174283 8.1382627 10.998899 9.1305828 5.0677986 1.4758148 -1.3050811 -3.5124321 -4.9851418 -5.4596148 -5.5680933][-5.0090265 -4.7836747 -4.643733 -2.2507787 2.5367141 7.725193 10.786499 9.2421751 5.5088367 1.7399323 -1.2495611 -3.4086175 -4.9000664 -5.4417939 -5.6812811][-6.0572395 -5.7357903 -6.1507282 -4.7594686 -0.94384897 3.5621448 6.4329214 5.7367353 3.0919685 -0.19388819 -2.7852139 -4.4100418 -5.5533333 -5.9565258 -6.0600271][-7.6024733 -6.9934936 -7.4657159 -6.8526 -4.1895247 -0.68934488 1.6899302 1.6968768 0.079224825 -2.4842372 -4.4884515 -5.4113555 -6.0048032 -6.077733 -5.8386736][-8.3196945 -7.5433841 -8.0206442 -8.0480795 -6.4403405 -3.8483562 -1.8996708 -1.4115599 -2.3058977 -4.1269541 -5.489645 -5.8537178 -6.0736856 -6.0292077 -5.6857266][-9.216054 -8.50708 -8.9042854 -9.2230368 -8.330018 -6.3821578 -4.8444147 -4.252151 -4.7451639 -5.8257656 -6.46204 -6.3208981 -6.100359 -5.9847054 -5.6171331][-9.6806278 -9.151454 -9.4684715 -9.735136 -9.1657019 -7.6263666 -6.5369811 -6.1445141 -6.6501656 -7.3509393 -7.4928617 -6.9765887 -6.4391732 -6.2039213 -5.8015623][-9.3330984 -9.01604 -9.5256081 -9.9070206 -9.5859652 -8.3556185 -7.4838247 -7.1859341 -7.5801282 -7.9380903 -7.721725 -6.8653846 -6.128799 -5.9146738 -5.7164078]]...]
INFO - root - 2017-12-15 07:10:58.220557: step 17810, loss = 0.25, batch loss = 0.22 (35.0 examples/sec; 0.229 sec/batch; 20h:00m:25s remains)
INFO - root - 2017-12-15 07:11:00.494081: step 17820, loss = 0.22, batch loss = 0.19 (35.8 examples/sec; 0.223 sec/batch; 19h:31m:14s remains)
INFO - root - 2017-12-15 07:11:02.784002: step 17830, loss = 0.25, batch loss = 0.21 (34.9 examples/sec; 0.229 sec/batch; 20h:02m:56s remains)
INFO - root - 2017-12-15 07:11:05.081365: step 17840, loss = 0.28, batch loss = 0.25 (34.2 examples/sec; 0.234 sec/batch; 20h:25m:07s remains)
INFO - root - 2017-12-15 07:11:07.348380: step 17850, loss = 0.27, batch loss = 0.23 (35.6 examples/sec; 0.225 sec/batch; 19h:37m:52s remains)
INFO - root - 2017-12-15 07:11:09.619134: step 17860, loss = 0.17, batch loss = 0.14 (34.9 examples/sec; 0.229 sec/batch; 20h:02m:33s remains)
INFO - root - 2017-12-15 07:11:11.916287: step 17870, loss = 0.22, batch loss = 0.18 (36.1 examples/sec; 0.222 sec/batch; 19h:21m:33s remains)
INFO - root - 2017-12-15 07:11:14.163014: step 17880, loss = 0.29, batch loss = 0.25 (35.1 examples/sec; 0.228 sec/batch; 19h:53m:52s remains)
INFO - root - 2017-12-15 07:11:16.446995: step 17890, loss = 0.29, batch loss = 0.25 (35.8 examples/sec; 0.223 sec/batch; 19h:30m:59s remains)
INFO - root - 2017-12-15 07:11:18.721986: step 17900, loss = 0.23, batch loss = 0.20 (35.3 examples/sec; 0.227 sec/batch; 19h:48m:46s remains)
2017-12-15 07:11:19.041351: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5238104 -4.1407509 -4.3064461 -3.5151441 -2.8200328 -2.3980658 -2.0750558 -1.6131442 -1.6476179 -2.0740962 -2.5230558 -2.6793585 -3.0219734 -4.451118 -5.3850327][-3.0665939 -4.659234 -4.7623734 -4.0355458 -3.4425139 -2.9300935 -2.4779446 -2.1526868 -2.4178083 -3.0468845 -3.6251721 -4.0145473 -4.6096277 -6.1221762 -6.9979639][-4.1170206 -4.5398989 -4.2885871 -3.6537533 -3.3132484 -2.8687594 -2.2787282 -2.0167353 -2.4888089 -3.4189913 -4.2978535 -4.9323316 -5.7271156 -7.0045414 -7.7340765][-4.4871187 -3.67003 -2.8564124 -2.2251756 -2.0585093 -1.7585231 -1.1951543 -1.0442781 -1.7558522 -3.0082102 -4.2570028 -5.29063 -6.584908 -7.8810534 -8.4854355][-4.0695763 -2.6396704 -1.5913208 -0.84314847 -0.36709285 0.34690094 1.139379 1.142767 0.10628724 -1.7203462 -3.4634027 -4.9219484 -6.6168547 -7.8052545 -8.030941][-3.4420462 -2.2828913 -1.3583517 -0.39113665 0.70819736 2.392518 3.9251466 4.1338363 2.6875954 -0.024256229 -2.6175566 -4.5552778 -6.2363262 -6.93679 -6.6835589][-2.6621168 -2.1756082 -1.3745204 -0.23728478 1.4227469 3.8923664 6.0835447 6.6916704 4.86853 1.3394246 -2.0611045 -4.4020681 -5.7803192 -5.9616618 -5.3442478][-2.3852181 -2.3155906 -1.8304951 -0.6880635 1.3045793 4.153296 6.6076908 7.3692513 5.4247985 1.6165183 -2.1547983 -4.6558013 -5.5339441 -5.27496 -4.5080361][-2.6615789 -2.8168831 -2.6695013 -1.9279267 -0.16727281 2.4885325 4.7333727 5.4224234 3.7284231 0.37303329 -3.1457889 -5.3555017 -5.5807161 -4.866457 -4.1658378][-3.4902868 -3.57497 -3.5964432 -3.3669209 -2.2150254 -0.25589168 1.3975 1.8298759 0.44611144 -2.0789261 -4.9331322 -6.5147738 -5.9551821 -4.8708253 -4.2419052][-5.1369257 -5.0727377 -5.016747 -5.0378795 -4.4407063 -3.2872427 -2.2541571 -2.1442518 -3.2241507 -4.99957 -7.0307312 -7.9515471 -6.9113526 -5.6653366 -5.0984221][-7.401813 -7.1227531 -7.0136652 -7.0854769 -6.9128528 -6.3950043 -5.80513 -5.7369604 -6.2866054 -7.2866645 -8.4574852 -8.7032528 -7.469594 -6.2764173 -5.9070573][-8.3146124 -7.857306 -7.85385 -7.9253216 -8.0402679 -8.0334673 -7.7342873 -7.5037804 -7.6336069 -8.1027212 -8.5917645 -8.2719421 -7.0370979 -6.0400534 -5.7560406][-7.9622025 -7.242197 -7.2184153 -7.1391025 -7.252965 -7.5074911 -7.4512305 -7.2464328 -7.2600875 -7.552352 -7.7042179 -7.2266908 -6.2377114 -5.4732571 -5.1116438][-7.5410671 -6.4958215 -6.3805237 -6.2007127 -6.2394953 -6.5091786 -6.6097679 -6.5146027 -6.4954815 -6.5825343 -6.5333753 -6.1868591 -5.6162262 -5.0907774 -4.736464]]...]
INFO - root - 2017-12-15 07:11:21.336738: step 17910, loss = 0.21, batch loss = 0.17 (35.3 examples/sec; 0.226 sec/batch; 19h:46m:50s remains)
INFO - root - 2017-12-15 07:11:23.582703: step 17920, loss = 0.24, batch loss = 0.21 (36.3 examples/sec; 0.220 sec/batch; 19h:15m:26s remains)
INFO - root - 2017-12-15 07:11:25.818708: step 17930, loss = 0.22, batch loss = 0.18 (36.2 examples/sec; 0.221 sec/batch; 19h:19m:06s remains)
INFO - root - 2017-12-15 07:11:28.107244: step 17940, loss = 0.32, batch loss = 0.29 (35.9 examples/sec; 0.223 sec/batch; 19h:27m:25s remains)
INFO - root - 2017-12-15 07:11:30.336361: step 17950, loss = 0.21, batch loss = 0.17 (37.4 examples/sec; 0.214 sec/batch; 18h:42m:39s remains)
INFO - root - 2017-12-15 07:11:32.592542: step 17960, loss = 0.21, batch loss = 0.18 (36.4 examples/sec; 0.220 sec/batch; 19h:10m:58s remains)
INFO - root - 2017-12-15 07:11:34.844980: step 17970, loss = 0.26, batch loss = 0.23 (35.7 examples/sec; 0.224 sec/batch; 19h:35m:53s remains)
INFO - root - 2017-12-15 07:11:37.108334: step 17980, loss = 0.30, batch loss = 0.27 (35.6 examples/sec; 0.224 sec/batch; 19h:36m:29s remains)
INFO - root - 2017-12-15 07:11:39.422249: step 17990, loss = 0.25, batch loss = 0.21 (36.3 examples/sec; 0.221 sec/batch; 19h:16m:46s remains)
INFO - root - 2017-12-15 07:11:41.673835: step 18000, loss = 0.23, batch loss = 0.19 (35.4 examples/sec; 0.226 sec/batch; 19h:45m:16s remains)
2017-12-15 07:11:41.970953: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2393854 -4.4224052 -4.9831319 -5.6618147 -6.1653957 -6.211834 -5.8214369 -5.574501 -5.689043 -5.8272657 -6.0307288 -6.230597 -6.6781611 -6.7199807 -6.135922][-3.2040689 -4.6548667 -5.1144328 -5.6184082 -5.8799949 -5.6982861 -5.1437407 -4.845583 -5.1232986 -5.5181437 -6.0871382 -6.5846419 -7.126668 -7.187407 -6.3960705][-3.9650993 -4.5552783 -4.9330988 -5.2735457 -5.3059731 -4.8627005 -4.1762123 -3.8220513 -4.159112 -4.8344507 -5.8025951 -6.6329832 -7.2347527 -7.4739828 -6.665556][-4.5501223 -4.524694 -4.9610081 -5.252223 -5.0906038 -4.379838 -3.6082561 -3.1076751 -3.326632 -4.1009245 -5.3394117 -6.3947048 -7.0090585 -7.45189 -6.8388538][-5.0417066 -4.6410265 -5.2434549 -5.5438547 -5.1519041 -4.1236525 -3.2829785 -2.5594089 -2.5589139 -3.1982343 -4.5174408 -5.7856545 -6.4938264 -7.1217756 -6.7420821][-4.8973351 -4.7032218 -5.4458175 -5.719779 -5.0520239 -3.6690178 -2.79007 -1.891686 -1.5832257 -1.9292207 -3.0940275 -4.4907827 -5.5145922 -6.4184756 -6.404736][-4.2563334 -4.5853424 -5.3932362 -5.6825747 -4.7939849 -3.0657725 -2.134294 -1.0497584 -0.44926286 -0.43889415 -1.2769015 -2.7453628 -4.2084732 -5.5189438 -5.8331828][-4.0877323 -4.3444719 -5.0922852 -5.3691664 -4.2835169 -2.1314065 -0.9084481 0.38988519 1.1547549 1.3916214 0.70926166 -0.81643987 -2.725884 -4.5147467 -5.2152348][-3.880528 -4.0042429 -4.5469246 -4.6985159 -3.3732038 -0.72224915 0.93680978 2.4430237 3.2723284 3.4852338 2.8813028 1.2762868 -0.95989585 -3.2654953 -4.4416876][-3.7994156 -3.8328571 -4.2275553 -4.2525845 -2.7146411 0.35116792 2.3888159 3.9686527 4.6390872 4.6713452 4.12784 2.6948647 0.4579165 -2.1272774 -3.6215191][-3.8139262 -3.9100449 -4.3981056 -4.4567919 -2.8455889 0.41086555 2.6795793 4.2871547 4.8364263 4.7759075 4.3838687 3.2198782 1.2420764 -1.3674688 -2.980154][-3.881537 -4.2025433 -5.066678 -5.5030403 -4.3104696 -1.4900165 0.49047375 1.928225 2.5401359 2.7737904 2.8152127 2.1839666 0.69185019 -1.7606206 -3.3171344][-3.9873171 -4.5023603 -5.7139912 -6.5593934 -5.9522715 -3.8855224 -2.4515653 -1.3196869 -0.65856838 -0.17224622 0.11902261 0.025232315 -0.87933779 -3.042429 -4.3127556][-4.0871086 -4.6142564 -5.9423628 -6.9946728 -6.8901048 -5.5881443 -4.5891218 -3.5850475 -2.8115673 -2.2262268 -1.926043 -1.8189393 -2.4377365 -4.247016 -5.3056588][-4.187644 -4.5512342 -5.8055677 -6.977757 -7.3502417 -6.7523193 -6.132061 -5.3052053 -4.5810127 -4.0654426 -3.7174067 -3.408123 -3.899056 -5.3237171 -6.214747]]...]
INFO - root - 2017-12-15 07:11:44.271009: step 18010, loss = 0.27, batch loss = 0.24 (36.9 examples/sec; 0.217 sec/batch; 18h:56m:55s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 07:11:46.592980: step 18020, loss = 0.22, batch loss = 0.19 (35.0 examples/sec; 0.228 sec/batch; 19h:56m:24s remains)
INFO - root - 2017-12-15 07:11:48.900476: step 18030, loss = 0.22, batch loss = 0.19 (34.1 examples/sec; 0.235 sec/batch; 20h:29m:32s remains)
INFO - root - 2017-12-15 07:11:51.182077: step 18040, loss = 0.31, batch loss = 0.28 (33.9 examples/sec; 0.236 sec/batch; 20h:36m:46s remains)
INFO - root - 2017-12-15 07:11:53.440169: step 18050, loss = 0.23, batch loss = 0.19 (36.1 examples/sec; 0.222 sec/batch; 19h:20m:51s remains)
INFO - root - 2017-12-15 07:11:55.708026: step 18060, loss = 0.27, batch loss = 0.23 (33.9 examples/sec; 0.236 sec/batch; 20h:37m:50s remains)
INFO - root - 2017-12-15 07:11:57.984171: step 18070, loss = 0.25, batch loss = 0.22 (34.2 examples/sec; 0.234 sec/batch; 20h:24m:45s remains)
INFO - root - 2017-12-15 07:12:00.229375: step 18080, loss = 0.31, batch loss = 0.28 (36.6 examples/sec; 0.219 sec/batch; 19h:05m:15s remains)
INFO - root - 2017-12-15 07:12:02.522006: step 18090, loss = 0.20, batch loss = 0.16 (34.1 examples/sec; 0.235 sec/batch; 20h:30m:07s remains)
INFO - root - 2017-12-15 07:12:04.807024: step 18100, loss = 0.35, batch loss = 0.32 (35.0 examples/sec; 0.229 sec/batch; 19h:57m:34s remains)
2017-12-15 07:12:05.155841: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0371375 -5.2765436 -5.2069292 -5.5785675 -5.9572835 -5.9795589 -5.4986634 -5.3293772 -6.1789875 -6.403244 -5.83377 -5.604661 -5.6480103 -5.6604967 -5.3847742][-4.6600986 -5.2063465 -5.1760087 -5.6924 -6.1012516 -6.010674 -5.4248433 -5.0867581 -5.9903054 -6.1591234 -5.3507404 -5.1671991 -5.4862833 -5.8044119 -5.7533627][-4.7379665 -4.7140832 -4.7671409 -5.4731016 -5.9098439 -5.6842661 -5.1161036 -4.6845903 -5.4891825 -5.525867 -4.49996 -4.31946 -4.7528834 -5.2656527 -5.4117765][-4.9454718 -4.4247866 -4.5007372 -5.2973471 -5.565855 -5.1862311 -4.6982694 -4.1586275 -4.6092091 -4.427352 -3.4397426 -3.4567494 -4.010993 -4.6900339 -4.8756943][-4.6148834 -3.5249751 -3.45427 -4.0696287 -4.060955 -3.5819714 -3.3228076 -2.7789905 -2.8288538 -2.4488862 -1.6273978 -1.8214114 -2.4226267 -3.2784054 -3.4201462][-4.0122213 -2.4908738 -2.1137311 -2.5264738 -2.2875197 -1.7655902 -1.6834538 -1.1435891 -0.90491009 -0.41180575 0.14356327 -0.15267086 -0.61561692 -1.5641317 -1.7209729][-3.0299344 -1.3657472 -0.7672255 -0.99569392 -0.58569729 -0.12446189 -0.11950397 0.41759682 0.85345984 1.3257091 1.5531716 1.1553564 0.78043175 -0.25479543 -0.45084834][-2.0624909 -0.45622122 0.13282537 -0.0089132786 0.44183636 0.73460364 0.56788826 1.0535259 1.6411786 2.1740975 2.3266702 1.9432168 1.5971961 0.52465343 0.16419387][-2.1217408 -0.763451 -0.22180867 -0.20479774 0.33849 0.54375982 0.28897524 0.59739995 1.1658936 1.8674774 2.1598573 1.6632178 0.98854876 -0.15890479 -0.68325913][-2.8239043 -1.7218003 -1.2563925 -1.1551206 -0.66774392 -0.564739 -0.74239671 -0.45679414 0.10017538 0.93011737 1.2759504 0.54014993 -0.37450516 -1.3198721 -1.751967][-3.8684177 -3.1243851 -2.9229012 -2.9480639 -2.6476276 -2.5788932 -2.5917432 -2.194984 -1.6143706 -0.84929061 -0.59877837 -1.424602 -2.3758993 -2.9485712 -3.1497698][-4.9642868 -4.5798483 -4.5126524 -4.4409 -4.1384358 -4.0303831 -3.9652543 -3.5837631 -3.1054778 -2.5022724 -2.40097 -3.2831898 -4.2269683 -4.5326471 -4.5829554][-5.422287 -5.3456526 -5.4167981 -5.2942796 -5.0208397 -4.8816409 -4.7702265 -4.4033175 -4.0186691 -3.4862132 -3.3171306 -4.0537767 -4.9229379 -5.0885744 -5.0850019][-5.5783191 -5.6384239 -5.7666163 -5.64172 -5.41553 -5.27861 -5.1596727 -4.73137 -4.3281784 -3.9000783 -3.759387 -4.3731022 -5.0697508 -5.0758924 -5.0470314][-5.3388481 -5.3457823 -5.3610735 -5.0773439 -4.7670603 -4.6040998 -4.5415792 -4.0552235 -3.609314 -3.3054159 -3.1850195 -3.7804365 -4.4459515 -4.4395323 -4.4888525]]...]
INFO - root - 2017-12-15 07:12:07.399166: step 18110, loss = 0.24, batch loss = 0.20 (33.6 examples/sec; 0.238 sec/batch; 20h:46m:14s remains)
INFO - root - 2017-12-15 07:12:09.676577: step 18120, loss = 0.34, batch loss = 0.30 (36.2 examples/sec; 0.221 sec/batch; 19h:19m:08s remains)
INFO - root - 2017-12-15 07:12:11.948167: step 18130, loss = 0.23, batch loss = 0.19 (35.3 examples/sec; 0.227 sec/batch; 19h:48m:33s remains)
INFO - root - 2017-12-15 07:12:14.206936: step 18140, loss = 0.21, batch loss = 0.18 (36.3 examples/sec; 0.220 sec/batch; 19h:14m:07s remains)
INFO - root - 2017-12-15 07:12:16.510723: step 18150, loss = 0.27, batch loss = 0.24 (34.7 examples/sec; 0.230 sec/batch; 20h:07m:26s remains)
INFO - root - 2017-12-15 07:12:18.802272: step 18160, loss = 0.23, batch loss = 0.20 (35.8 examples/sec; 0.223 sec/batch; 19h:30m:48s remains)
INFO - root - 2017-12-15 07:12:21.044885: step 18170, loss = 0.30, batch loss = 0.26 (35.5 examples/sec; 0.225 sec/batch; 19h:39m:53s remains)
INFO - root - 2017-12-15 07:12:23.390815: step 18180, loss = 0.21, batch loss = 0.18 (35.3 examples/sec; 0.227 sec/batch; 19h:48m:25s remains)
INFO - root - 2017-12-15 07:12:25.666102: step 18190, loss = 0.19, batch loss = 0.16 (34.2 examples/sec; 0.234 sec/batch; 20h:25m:20s remains)
INFO - root - 2017-12-15 07:12:27.985158: step 18200, loss = 0.32, batch loss = 0.29 (34.8 examples/sec; 0.230 sec/batch; 20h:03m:06s remains)
2017-12-15 07:12:28.327352: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8499382 -5.1356163 -5.5621004 -5.747242 -5.5690718 -5.4918084 -4.8551288 -4.624804 -4.4196057 -4.0605254 -4.1882687 -4.6673708 -5.3061175 -5.6103487 -6.1826315][-3.8249345 -5.38546 -5.770607 -5.8388305 -5.5714431 -5.4191113 -4.6039639 -4.2419276 -3.9672742 -3.5750091 -3.6867492 -4.1761146 -4.768034 -4.9536643 -5.4865952][-4.3673134 -5.3359909 -5.6560235 -5.4870272 -4.9288368 -4.7315912 -3.9569435 -3.5834222 -3.3525951 -3.1970544 -3.4244335 -3.7506876 -4.1175714 -4.1503944 -4.455039][-4.8654642 -5.1599369 -5.3380442 -4.7903843 -3.7337289 -3.3687735 -2.7802339 -2.6519125 -2.6177773 -2.7828851 -3.1146457 -3.1054077 -3.2515328 -3.2239618 -3.2494478][-5.6726589 -4.9947739 -4.9922905 -3.9951653 -2.2655549 -1.4864793 -0.919436 -0.97116411 -1.1393579 -1.6841428 -2.3650861 -2.4034419 -2.4273725 -2.4327314 -2.217428][-5.7862053 -4.7895241 -4.6149426 -3.1983733 -0.79520488 0.53102207 1.2906406 1.3192177 1.1098585 0.074514151 -1.5230207 -2.1540132 -2.3207572 -2.4617939 -2.0355222][-5.4876719 -4.87712 -4.742281 -3.1810882 -0.38537657 1.472033 2.7772803 3.319756 3.3835092 2.0336394 -0.39911282 -1.4998193 -1.8574649 -2.1814516 -1.8511505][-5.8438625 -5.3752713 -5.5054321 -4.0545354 -1.1805569 0.97297955 2.9640002 4.2735372 4.7983794 3.5092192 0.84406447 -0.58121169 -1.3437068 -1.8498724 -1.7905885][-6.0059729 -5.7558632 -6.1685643 -4.9038029 -2.2068114 -0.1598146 2.1619453 3.9477453 4.723 3.7294827 1.386631 -0.2193737 -1.3556976 -2.0575829 -2.2833633][-5.9600153 -5.87449 -6.515749 -5.5039034 -3.1467886 -1.4328 0.82879353 2.6729918 3.3305635 2.506187 0.53268242 -0.96895623 -2.0402515 -2.5553317 -2.6926169][-5.8048553 -5.8086119 -6.5454984 -5.7548418 -3.7447693 -2.424932 -0.51535487 1.1018336 1.5164194 0.67285013 -0.9439615 -2.1744804 -3.0704136 -3.2768278 -3.1930494][-5.6631765 -5.7408423 -6.505682 -5.9774041 -4.4467907 -3.6145113 -2.3175452 -1.1489129 -0.88237762 -1.5780343 -2.5804262 -3.3754597 -4.1135674 -4.2420263 -4.2412863][-5.5777063 -5.7651806 -6.6054182 -6.462461 -5.5021811 -5.0413551 -4.27423 -3.5141702 -3.3610573 -3.8971374 -4.3626184 -4.58502 -4.8989849 -5.0106096 -5.0079317][-5.4259663 -5.6170206 -6.3974252 -6.5395541 -6.094326 -5.9021778 -5.5252085 -5.142251 -5.1314411 -5.5807161 -5.7559495 -5.5414481 -5.2972412 -4.99445 -4.61467][-5.23306 -5.2885981 -5.8736053 -6.1528683 -6.1147194 -6.2421436 -6.3008447 -6.3392 -6.4340754 -6.629837 -6.5084386 -6.0569963 -5.3448973 -4.5492268 -3.9324608]]...]
INFO - root - 2017-12-15 07:12:30.628428: step 18210, loss = 0.28, batch loss = 0.24 (34.2 examples/sec; 0.234 sec/batch; 20h:26m:31s remains)
INFO - root - 2017-12-15 07:12:32.928378: step 18220, loss = 0.23, batch loss = 0.20 (32.9 examples/sec; 0.243 sec/batch; 21h:13m:10s remains)
INFO - root - 2017-12-15 07:12:35.254905: step 18230, loss = 0.17, batch loss = 0.14 (34.2 examples/sec; 0.234 sec/batch; 20h:23m:48s remains)
INFO - root - 2017-12-15 07:12:37.572717: step 18240, loss = 0.40, batch loss = 0.37 (35.5 examples/sec; 0.226 sec/batch; 19h:41m:21s remains)
INFO - root - 2017-12-15 07:12:39.853684: step 18250, loss = 0.23, batch loss = 0.20 (35.8 examples/sec; 0.224 sec/batch; 19h:31m:16s remains)
INFO - root - 2017-12-15 07:12:42.155952: step 18260, loss = 0.21, batch loss = 0.18 (34.5 examples/sec; 0.232 sec/batch; 20h:13m:03s remains)
INFO - root - 2017-12-15 07:12:44.482465: step 18270, loss = 0.23, batch loss = 0.20 (34.2 examples/sec; 0.234 sec/batch; 20h:26m:12s remains)
INFO - root - 2017-12-15 07:12:46.780572: step 18280, loss = 0.19, batch loss = 0.16 (36.4 examples/sec; 0.220 sec/batch; 19h:12m:10s remains)
INFO - root - 2017-12-15 07:12:49.019133: step 18290, loss = 0.23, batch loss = 0.20 (35.3 examples/sec; 0.227 sec/batch; 19h:48m:08s remains)
INFO - root - 2017-12-15 07:12:51.337013: step 18300, loss = 0.19, batch loss = 0.15 (33.7 examples/sec; 0.237 sec/batch; 20h:42m:11s remains)
2017-12-15 07:12:51.708151: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.89676976 -1.7990487 -3.5719738 -4.9216695 -5.6704392 -6.0574694 -6.2273116 -6.3838472 -6.080842 -6.157196 -6.4443564 -6.9582515 -7.3671055 -7.5192327 -7.6089869][-0.77977622 -3.0039363 -4.7277031 -5.9255095 -6.3159733 -6.24221 -6.1134291 -6.1179295 -5.9525838 -6.2743092 -6.9823346 -7.7946134 -8.3190517 -8.3856506 -8.3126526][-2.9856508 -4.1029778 -5.759016 -6.6143932 -6.4344158 -5.8133993 -5.2124262 -4.7279882 -4.5822906 -5.3313665 -6.5729604 -7.781373 -8.5546484 -8.77474 -8.709446][-4.6071959 -5.0028944 -6.4118934 -6.7500553 -5.8020778 -4.5584755 -3.4500766 -2.46465 -2.4423549 -3.7609277 -5.6547642 -7.3231473 -8.33507 -8.7477312 -8.76778][-5.04012 -5.1109848 -6.18073 -5.9097776 -4.1851954 -2.382431 -0.72766805 0.8835535 0.9257102 -0.88809729 -3.4875076 -5.853302 -7.3710794 -8.186595 -8.4839334][-5.3751287 -5.1069784 -5.8666949 -5.0202475 -2.743773 -0.51200306 1.6868384 3.9821639 4.343307 2.343267 -0.81317616 -3.9113348 -6.0240765 -7.3234315 -8.0028877][-5.3422747 -5.0289721 -5.4348745 -4.1113591 -1.4882143 1.1136429 3.7847919 6.6188655 7.2608533 5.183619 1.5710592 -2.2081797 -4.892765 -6.5298834 -7.3415003][-6.21011 -5.3897324 -5.556673 -4.0711975 -1.5167098 1.2360339 4.2196589 7.4638944 8.4055367 6.4086671 2.6263843 -1.551718 -4.65732 -6.4503188 -7.1702127][-7.5394506 -6.2628808 -6.1458244 -4.6048889 -2.4129679 0.10298705 2.9097 5.9884725 7.0303187 5.4410448 1.9952359 -2.0106182 -5.0088258 -6.5872693 -7.0981684][-7.8374453 -5.9876151 -5.6002989 -4.4861741 -3.2297258 -1.5706861 0.49740219 2.8285232 3.7604637 2.823585 0.25761437 -2.853286 -5.1756763 -6.2374549 -6.5950475][-7.2849979 -4.6890497 -3.9521861 -3.3452249 -3.1287782 -2.7286708 -1.6977699 -0.38069689 0.31828833 0.010372162 -1.3230171 -3.192327 -4.6049109 -5.1746855 -5.5166292][-6.023962 -2.7600186 -1.7744315 -1.6027496 -2.2697196 -2.944993 -3.0409496 -2.7629085 -2.2782335 -1.994961 -2.1920559 -2.7549787 -3.3330736 -3.534996 -3.9114108][-4.75295 -1.3292367 -0.56877768 -0.81839025 -1.9753124 -3.2547622 -4.0398378 -4.3185949 -3.7484984 -3.1017551 -2.477319 -2.0827122 -2.2324932 -2.5143425 -2.8448498][-4.4136767 -1.1419784 -0.57473922 -0.78480494 -1.9339137 -3.3733306 -4.4626522 -4.8919964 -4.1142063 -3.2709298 -2.2982695 -1.5406175 -1.7319212 -2.305567 -2.5256634][-4.7970772 -1.9509659 -1.7008889 -1.8488004 -2.6292944 -3.6292953 -4.5174885 -4.9160728 -4.0633707 -3.2877216 -2.3996518 -1.5849437 -2.0115438 -2.9590187 -3.0139079]]...]
INFO - root - 2017-12-15 07:12:53.981697: step 18310, loss = 0.25, batch loss = 0.22 (35.7 examples/sec; 0.224 sec/batch; 19h:34m:40s remains)
INFO - root - 2017-12-15 07:12:56.239369: step 18320, loss = 0.34, batch loss = 0.30 (36.3 examples/sec; 0.220 sec/batch; 19h:13m:44s remains)
INFO - root - 2017-12-15 07:12:58.509814: step 18330, loss = 0.23, batch loss = 0.19 (34.3 examples/sec; 0.233 sec/batch; 20h:21m:27s remains)
INFO - root - 2017-12-15 07:13:00.800188: step 18340, loss = 0.19, batch loss = 0.16 (33.1 examples/sec; 0.242 sec/batch; 21h:05m:40s remains)
INFO - root - 2017-12-15 07:13:03.109290: step 18350, loss = 0.20, batch loss = 0.17 (34.5 examples/sec; 0.232 sec/batch; 20h:12m:33s remains)
INFO - root - 2017-12-15 07:13:05.373325: step 18360, loss = 0.26, batch loss = 0.23 (35.5 examples/sec; 0.226 sec/batch; 19h:41m:12s remains)
INFO - root - 2017-12-15 07:13:07.675030: step 18370, loss = 0.23, batch loss = 0.19 (34.9 examples/sec; 0.229 sec/batch; 19h:59m:07s remains)
INFO - root - 2017-12-15 07:13:09.986092: step 18380, loss = 0.18, batch loss = 0.15 (35.2 examples/sec; 0.228 sec/batch; 19h:51m:14s remains)
INFO - root - 2017-12-15 07:13:12.249201: step 18390, loss = 0.19, batch loss = 0.16 (36.0 examples/sec; 0.222 sec/batch; 19h:21m:47s remains)
INFO - root - 2017-12-15 07:13:14.529573: step 18400, loss = 0.36, batch loss = 0.33 (34.7 examples/sec; 0.231 sec/batch; 20h:07m:39s remains)
2017-12-15 07:13:14.864258: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1872442 -4.3058739 -4.0420561 -4.631803 -5.0174527 -4.6158981 -3.7531066 -3.6603556 -3.8020556 -3.6452551 -3.6212144 -4.1415434 -4.49234 -4.951498 -4.7134433][-2.973525 -4.3810978 -4.2304211 -5.060708 -5.6448073 -5.1881003 -4.3051472 -4.1670938 -4.4732022 -4.487093 -4.2922688 -4.6545897 -5.0552263 -5.5314794 -5.225409][-3.5110765 -4.509932 -4.5325937 -5.2810135 -5.854641 -5.2724857 -4.2298036 -3.8467245 -4.1284261 -4.3270531 -4.1238012 -4.4683924 -4.86788 -5.3863182 -5.0908051][-4.2374897 -4.9612112 -5.2444239 -5.680625 -5.9478197 -5.0958662 -3.6633332 -2.839165 -2.9780388 -3.408906 -3.4595246 -4.1087446 -4.8123226 -5.456315 -5.2227755][-5.0556431 -5.5121055 -5.7471628 -5.6897345 -5.6779861 -4.5407534 -2.5427165 -1.1889927 -1.1846261 -1.8684766 -2.306581 -3.559016 -4.8109345 -5.5925751 -5.4811921][-5.11596 -5.5335665 -5.7199087 -5.2219915 -4.8165894 -3.3285503 -0.91929936 0.78754354 0.74044061 -0.25531864 -1.2883434 -3.0592468 -4.80538 -5.7775536 -5.8462143][-4.9181867 -5.1429162 -5.161 -4.3593731 -3.5004637 -1.7782177 0.7049799 2.4090869 2.1858933 0.95173979 -0.7004168 -2.8683329 -4.8810406 -5.9671 -6.3672752][-4.6558323 -4.4273348 -4.1604195 -3.2390137 -2.1170216 -0.39486289 1.9520791 3.5582025 3.106405 1.6822941 -0.33937323 -2.6944497 -4.7197571 -5.8521538 -6.6333752][-4.5132985 -3.8205571 -3.41141 -2.5585501 -1.3570118 0.14070821 2.2002628 3.6721179 3.1072176 1.5850608 -0.55247211 -2.8563528 -4.7851729 -5.9254713 -6.8601637][-4.5835381 -3.5293531 -3.1076031 -2.5754557 -1.6033398 -0.47857666 1.2502668 2.4780157 1.9460137 0.50140882 -1.5525091 -3.6510582 -5.2434373 -6.1896439 -6.98773][-4.8026304 -3.8696208 -3.5662818 -3.2957864 -2.5982547 -1.7457533 -0.35453141 0.52179193 0.0031356812 -1.2841561 -3.0130763 -4.7765942 -5.9370432 -6.5248232 -7.0667534][-5.3779049 -4.7907834 -4.6504908 -4.6611767 -4.2650971 -3.5343227 -2.3871891 -1.79484 -2.1600292 -3.1166277 -4.4386997 -5.76373 -6.4576826 -6.7275906 -6.9818153][-5.9075336 -5.6110129 -5.6755919 -5.923563 -5.7155161 -4.93134 -3.8739505 -3.4610634 -3.6402764 -4.1552377 -5.0547314 -6.0249338 -6.3538332 -6.3306093 -6.4201126][-6.3236837 -5.96119 -6.0532789 -6.3213091 -6.215343 -5.5506058 -4.8206892 -4.6911788 -4.7022443 -4.7209911 -5.276825 -6.0295882 -6.0246124 -5.7557445 -5.7999349][-6.4953632 -5.9495096 -6.0225487 -6.2070427 -6.1571975 -5.7160254 -5.340477 -5.27468 -4.9812365 -4.666996 -5.0546575 -5.6055746 -5.3697538 -5.1541858 -5.3549204]]...]
INFO - root - 2017-12-15 07:13:17.117967: step 18410, loss = 0.23, batch loss = 0.19 (34.8 examples/sec; 0.230 sec/batch; 20h:04m:48s remains)
INFO - root - 2017-12-15 07:13:19.430934: step 18420, loss = 0.23, batch loss = 0.20 (35.1 examples/sec; 0.228 sec/batch; 19h:54m:00s remains)
INFO - root - 2017-12-15 07:13:21.717652: step 18430, loss = 0.24, batch loss = 0.21 (34.9 examples/sec; 0.229 sec/batch; 19h:59m:03s remains)
INFO - root - 2017-12-15 07:13:24.018669: step 18440, loss = 0.23, batch loss = 0.19 (34.9 examples/sec; 0.229 sec/batch; 19h:59m:45s remains)
INFO - root - 2017-12-15 07:13:26.319585: step 18450, loss = 0.23, batch loss = 0.19 (34.5 examples/sec; 0.232 sec/batch; 20h:13m:16s remains)
INFO - root - 2017-12-15 07:13:28.576203: step 18460, loss = 0.19, batch loss = 0.16 (36.1 examples/sec; 0.222 sec/batch; 19h:19m:21s remains)
INFO - root - 2017-12-15 07:13:30.874519: step 18470, loss = 0.22, batch loss = 0.19 (32.4 examples/sec; 0.247 sec/batch; 21h:30m:50s remains)
INFO - root - 2017-12-15 07:13:33.119662: step 18480, loss = 0.29, batch loss = 0.25 (34.8 examples/sec; 0.230 sec/batch; 20h:03m:27s remains)
INFO - root - 2017-12-15 07:13:35.400183: step 18490, loss = 0.20, batch loss = 0.16 (35.5 examples/sec; 0.225 sec/batch; 19h:38m:30s remains)
INFO - root - 2017-12-15 07:13:37.676188: step 18500, loss = 0.25, batch loss = 0.22 (35.7 examples/sec; 0.224 sec/batch; 19h:31m:59s remains)
2017-12-15 07:13:38.084876: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8652024 -4.15864 -4.4781313 -4.5544291 -4.6837969 -4.8515387 -5.0284109 -5.2897353 -5.4947314 -5.5451202 -5.3203754 -5.0178652 -4.5942373 -4.0962152 -3.3367076][-4.5350924 -4.8268728 -5.0729771 -5.0595694 -5.1083488 -5.1053553 -5.11783 -5.4287834 -5.8447971 -6.1764479 -6.20977 -5.9228716 -5.408762 -4.7295532 -3.7389541][-5.3826294 -4.8469582 -4.83871 -4.5951152 -4.4425659 -4.1125631 -3.800173 -4.039588 -4.6851263 -5.4857087 -6.0098372 -5.9565296 -5.4915042 -4.775836 -3.7555668][-5.5350847 -4.2230511 -3.8413107 -3.1927278 -2.5921946 -1.7524714 -1.0820782 -1.2791382 -2.2589374 -3.6635485 -4.8180056 -5.244525 -5.0120292 -4.41288 -3.5182381][-5.3216109 -3.2908928 -2.6125906 -1.5224819 -0.29270685 1.2228954 2.3834496 2.3360367 1.092638 -0.87564433 -2.6507835 -3.6780224 -3.8272891 -3.5128989 -2.9188991][-4.7270851 -2.3973203 -1.4929168 -0.039327383 1.753226 3.7833061 5.3324027 5.51846 4.1981683 1.8603063 -0.35808754 -2.0122118 -2.6141589 -2.5955305 -2.2970543][-3.7545056 -1.6839001 -0.66820097 0.98907375 3.1686773 5.435154 7.0107608 7.2102036 5.8423629 3.4091372 1.03058 -1.0848492 -2.0313365 -2.1978033 -2.0833788][-3.5185449 -1.4690598 -0.5900135 0.92085814 3.1487341 5.3746986 6.8512082 7.0095496 5.6772146 3.3680568 1.028368 -1.2575393 -2.3373148 -2.5032737 -2.3882239][-3.7088594 -1.7579533 -1.1790096 -0.099543095 1.7585282 3.6744509 4.9154773 5.0346684 3.9144888 1.9691777 -0.15637589 -2.3779132 -3.3434148 -3.2859714 -2.9559598][-4.4146729 -2.6321418 -2.3626678 -1.7814121 -0.45979297 1.0193148 1.8976049 1.9079614 1.0222914 -0.39680898 -2.1028347 -3.984436 -4.5680723 -4.1023459 -3.427639][-5.1626315 -3.7473488 -3.8548584 -3.7723746 -3.0278962 -2.0166614 -1.453832 -1.5289606 -2.192765 -3.1334095 -4.4422207 -5.7863722 -5.7939606 -4.8952618 -3.8836732][-5.9339104 -4.940011 -5.3396559 -5.6555576 -5.4223251 -4.83119 -4.4882984 -4.5132208 -4.8530579 -5.3354926 -6.1924505 -6.952857 -6.4062705 -5.2121987 -3.9824214][-6.2921338 -5.7134848 -6.2479711 -6.7846475 -6.9413123 -6.6850905 -6.44787 -6.3127761 -6.3124571 -6.3890691 -6.8002028 -7.0663533 -6.0783329 -4.6560788 -3.2313652][-5.6998582 -5.4264746 -6.1180658 -6.8818493 -7.2949252 -7.2850466 -7.1571774 -6.8774295 -6.6141348 -6.461627 -6.5523777 -6.4667997 -5.1699319 -3.5587759 -1.9434264][-4.4744968 -4.23256 -5.1198063 -6.193922 -6.9421887 -7.1213751 -7.0291305 -6.6347647 -6.2399707 -5.99929 -5.88196 -5.6045065 -4.1571479 -2.440753 -0.75493884]]...]
INFO - root - 2017-12-15 07:13:40.364495: step 18510, loss = 0.30, batch loss = 0.27 (33.6 examples/sec; 0.238 sec/batch; 20h:47m:33s remains)
INFO - root - 2017-12-15 07:13:42.652043: step 18520, loss = 0.20, batch loss = 0.17 (34.7 examples/sec; 0.230 sec/batch; 20h:05m:38s remains)
INFO - root - 2017-12-15 07:13:44.918787: step 18530, loss = 0.25, batch loss = 0.22 (34.4 examples/sec; 0.232 sec/batch; 20h:16m:23s remains)
INFO - root - 2017-12-15 07:13:47.193014: step 18540, loss = 0.21, batch loss = 0.18 (35.5 examples/sec; 0.225 sec/batch; 19h:38m:10s remains)
INFO - root - 2017-12-15 07:13:49.444996: step 18550, loss = 0.17, batch loss = 0.14 (35.6 examples/sec; 0.225 sec/batch; 19h:36m:01s remains)
INFO - root - 2017-12-15 07:13:51.727713: step 18560, loss = 0.24, batch loss = 0.21 (34.8 examples/sec; 0.230 sec/batch; 20h:02m:41s remains)
INFO - root - 2017-12-15 07:13:54.068181: step 18570, loss = 0.18, batch loss = 0.15 (34.2 examples/sec; 0.234 sec/batch; 20h:25m:13s remains)
INFO - root - 2017-12-15 07:13:56.353642: step 18580, loss = 0.22, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 19h:26m:22s remains)
INFO - root - 2017-12-15 07:13:58.631565: step 18590, loss = 0.19, batch loss = 0.16 (35.6 examples/sec; 0.225 sec/batch; 19h:37m:05s remains)
INFO - root - 2017-12-15 07:14:00.898651: step 18600, loss = 0.18, batch loss = 0.15 (35.2 examples/sec; 0.227 sec/batch; 19h:49m:51s remains)
2017-12-15 07:14:01.237350: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3606017 -3.877192 -5.062324 -6.6834335 -7.57644 -7.527832 -7.0622272 -6.8714151 -6.6831894 -6.6088867 -6.7063942 -6.6340904 -6.4425774 -5.4779096 -3.3557994][-3.050648 -3.8188713 -5.3331223 -7.4677658 -8.4017916 -8.0571022 -7.2364273 -6.7619314 -6.5588951 -6.6395435 -6.9092665 -6.8774462 -6.6943607 -5.5264926 -2.8896012][-4.0418525 -3.7595 -5.344193 -7.70656 -8.49732 -7.8114014 -6.7666883 -6.1102629 -6.039216 -6.4882069 -7.1023035 -7.1438866 -6.9446974 -5.551198 -2.6122875][-4.7557673 -3.8635628 -5.4246626 -7.6509056 -8.0068016 -6.8757248 -5.6369395 -4.8648186 -4.9518418 -5.7684641 -6.7199163 -6.922761 -6.7955933 -5.3187389 -2.4604104][-5.604248 -4.33228 -5.5123553 -6.9758615 -6.6655397 -5.1568604 -3.7378285 -2.9371285 -3.1836877 -4.2379236 -5.3296852 -5.7214193 -5.7925158 -4.4455805 -1.9840829][-6.1149511 -4.4039822 -4.9460258 -5.34972 -4.2914262 -2.2894089 -0.47548008 0.52949023 0.0082454681 -1.4449952 -3.0503435 -4.020525 -4.5176754 -3.6173356 -1.7291001][-5.7028427 -3.7584739 -3.8031652 -3.4982226 -2.1781096 0.031824589 2.2419426 3.5681999 2.8206136 0.98191 -1.2069099 -2.7739482 -3.5656185 -3.0649142 -1.7660024][-5.3332291 -3.0138922 -2.7201066 -2.157486 -0.950006 1.2594531 3.6927521 5.2683229 4.39851 2.2699134 -0.33787537 -2.1887655 -2.9172649 -2.6052413 -1.6790043][-4.2818646 -1.5612364 -0.89734888 -0.468184 0.098096371 1.7795947 3.9945271 5.6482458 4.7730818 2.552068 -0.34963059 -2.3026822 -2.9541972 -2.7061603 -1.9364365][-2.9209285 -0.20595121 0.34738255 0.11176896 -0.23190093 0.56896782 2.3117449 3.7301681 2.9376943 0.92561412 -1.6194757 -3.2202501 -3.4729302 -2.9031572 -1.9054456][-2.2631953 0.15449572 0.2898674 -0.57226396 -1.6557732 -1.6164343 -0.45810103 0.43767643 -0.27691913 -1.7540694 -3.3869596 -4.1219773 -3.8104281 -2.791858 -1.5553755][-2.2476866 -0.30122328 -0.6947341 -1.858664 -3.0190973 -3.0850856 -2.2299893 -1.8112023 -2.3532925 -3.3083067 -4.1678386 -4.4045773 -3.8894649 -2.8700891 -1.7142353][-3.0140593 -1.6789153 -2.5431302 -3.8204155 -4.8235917 -4.7901664 -4.1896429 -4.1623054 -4.51558 -4.9825439 -5.2644277 -5.1580048 -4.6530008 -3.7477996 -2.7293398][-4.782546 -3.9954507 -5.1457033 -6.3621311 -7.1829872 -7.031333 -6.6868229 -6.8666382 -6.95854 -6.926897 -6.6662636 -6.2733345 -5.7054605 -4.8770194 -3.9206798][-6.7376347 -6.222527 -7.1827269 -8.088912 -8.563695 -8.2586508 -8.02804 -8.1571665 -8.07235 -7.7414255 -7.3313494 -6.9021606 -6.4198866 -5.6544557 -4.7077169]]...]
INFO - root - 2017-12-15 07:14:03.501008: step 18610, loss = 0.33, batch loss = 0.30 (34.7 examples/sec; 0.230 sec/batch; 20h:04m:36s remains)
INFO - root - 2017-12-15 07:14:05.769621: step 18620, loss = 0.26, batch loss = 0.22 (35.2 examples/sec; 0.227 sec/batch; 19h:49m:20s remains)
INFO - root - 2017-12-15 07:14:08.051988: step 18630, loss = 0.23, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 19h:22m:20s remains)
INFO - root - 2017-12-15 07:14:10.370507: step 18640, loss = 0.34, batch loss = 0.31 (36.2 examples/sec; 0.221 sec/batch; 19h:15m:41s remains)
INFO - root - 2017-12-15 07:14:12.647717: step 18650, loss = 0.29, batch loss = 0.26 (35.3 examples/sec; 0.227 sec/batch; 19h:47m:06s remains)
INFO - root - 2017-12-15 07:14:14.967324: step 18660, loss = 0.26, batch loss = 0.23 (33.1 examples/sec; 0.241 sec/batch; 21h:02m:57s remains)
INFO - root - 2017-12-15 07:14:17.268582: step 18670, loss = 0.27, batch loss = 0.24 (34.8 examples/sec; 0.230 sec/batch; 20h:01m:03s remains)
INFO - root - 2017-12-15 07:14:19.568040: step 18680, loss = 0.22, batch loss = 0.18 (34.4 examples/sec; 0.232 sec/batch; 20h:15m:34s remains)
INFO - root - 2017-12-15 07:14:21.848484: step 18690, loss = 0.18, batch loss = 0.15 (34.4 examples/sec; 0.233 sec/batch; 20h:16m:12s remains)
INFO - root - 2017-12-15 07:14:24.142094: step 18700, loss = 0.27, batch loss = 0.23 (35.7 examples/sec; 0.224 sec/batch; 19h:32m:02s remains)
2017-12-15 07:14:24.478087: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7682419 -4.5371828 -4.2829895 -3.7160239 -2.9918308 -2.7763367 -3.6164408 -5.0924234 -6.2723303 -6.3154297 -5.5794687 -5.2165709 -5.3562117 -5.8513384 -6.3052292][-4.23893 -2.651052 -2.34443 -1.8661244 -1.380096 -1.3259113 -2.231153 -3.7707081 -5.1660852 -5.5006938 -4.856154 -4.3213787 -4.1278973 -3.9604907 -3.8351858][-3.4586639 -0.49406016 0.095467567 0.61901188 0.77830935 0.39901733 -0.71631992 -2.4645326 -4.0868011 -4.8137741 -4.3961105 -3.6510692 -3.1135859 -2.5250187 -1.9424207][-3.2839346 0.76465654 1.6217346 2.3169043 2.5541389 2.1050723 1.010747 -0.92782187 -2.9236341 -4.1944156 -4.2531357 -3.5260434 -2.9596319 -2.3596141 -1.5277123][-3.1023688 1.3794796 2.4271123 3.1683981 3.3650839 2.8545058 1.9345829 0.10603738 -1.9681535 -3.6091533 -4.1169424 -3.5250797 -3.1064146 -2.8563416 -2.1158686][-2.97713 1.1762252 2.4151514 3.3824756 3.673424 3.0353372 2.2081821 0.73529124 -0.95462608 -2.4561336 -3.2237515 -3.0705638 -3.0524883 -3.3358808 -3.0739009][-3.7145839 -0.43024862 1.0500312 2.3109338 2.8570387 2.2917545 1.7752287 0.98713636 0.022351742 -1.0826683 -1.9634178 -2.2166266 -2.7151818 -3.5684655 -3.9060779][-5.3583031 -2.539139 -1.1043638 0.15485311 0.86206675 0.56656384 0.68494987 0.96736288 0.89139438 0.22194171 -0.7881602 -1.5457137 -2.4696474 -3.590003 -4.3803549][-6.3986111 -4.2552443 -3.0866127 -2.0362492 -1.323211 -1.3483233 -0.59295583 0.67431927 1.3802242 1.145364 0.13031077 -1.05205 -2.3563619 -3.5862677 -4.5612187][-6.737608 -5.3562765 -4.6302652 -4.0295296 -3.4787478 -3.2592773 -2.0472367 -0.16784906 0.94382095 1.0252371 0.1715095 -1.1161278 -2.5796709 -3.8132944 -4.756813][-6.3749924 -5.6600275 -5.2596588 -5.0301285 -4.6370015 -4.3086061 -2.9386315 -0.89881706 0.25818729 0.3314302 -0.47000277 -1.6635853 -3.0617652 -4.2963209 -5.1934986][-5.8679042 -5.6738529 -5.3992214 -5.3278255 -5.0940619 -4.8356113 -3.7191505 -2.0185564 -1.1202923 -1.1370641 -1.8230443 -2.7326183 -3.887424 -5.0673437 -5.8730869][-5.9548016 -6.1269617 -5.892231 -5.8884645 -5.8127022 -5.6960688 -4.9442739 -3.7598815 -3.123292 -3.2010641 -3.6927347 -4.1856937 -4.9644966 -5.8768587 -6.470645][-6.2518797 -6.4257412 -6.2609091 -6.2631769 -6.2159281 -6.1111135 -5.6246376 -4.8861551 -4.5077314 -4.7059484 -5.0767932 -5.2234335 -5.5929871 -6.1281986 -6.5547791][-6.0527639 -6.0960917 -5.9761648 -5.9417477 -5.9015636 -5.8707085 -5.6842747 -5.3825259 -5.1872854 -5.3202391 -5.4889164 -5.4361768 -5.5784984 -5.8572321 -6.0985565]]...]
INFO - root - 2017-12-15 07:14:26.745943: step 18710, loss = 0.32, batch loss = 0.29 (34.4 examples/sec; 0.232 sec/batch; 20h:15m:39s remains)
INFO - root - 2017-12-15 07:14:29.044844: step 18720, loss = 0.30, batch loss = 0.27 (36.1 examples/sec; 0.222 sec/batch; 19h:18m:54s remains)
INFO - root - 2017-12-15 07:14:31.304445: step 18730, loss = 0.21, batch loss = 0.18 (35.8 examples/sec; 0.224 sec/batch; 19h:29m:06s remains)
INFO - root - 2017-12-15 07:14:33.568442: step 18740, loss = 0.18, batch loss = 0.14 (35.0 examples/sec; 0.229 sec/batch; 19h:56m:54s remains)
INFO - root - 2017-12-15 07:14:35.848603: step 18750, loss = 0.27, batch loss = 0.24 (35.8 examples/sec; 0.223 sec/batch; 19h:27m:52s remains)
INFO - root - 2017-12-15 07:14:38.133700: step 18760, loss = 0.22, batch loss = 0.19 (34.7 examples/sec; 0.231 sec/batch; 20h:05m:58s remains)
INFO - root - 2017-12-15 07:14:40.435926: step 18770, loss = 0.26, batch loss = 0.22 (34.9 examples/sec; 0.229 sec/batch; 19h:59m:03s remains)
INFO - root - 2017-12-15 07:14:42.712521: step 18780, loss = 0.22, batch loss = 0.19 (34.8 examples/sec; 0.230 sec/batch; 20h:02m:39s remains)
INFO - root - 2017-12-15 07:14:45.050792: step 18790, loss = 0.26, batch loss = 0.22 (35.1 examples/sec; 0.228 sec/batch; 19h:51m:10s remains)
INFO - root - 2017-12-15 07:14:47.320959: step 18800, loss = 0.20, batch loss = 0.17 (36.4 examples/sec; 0.220 sec/batch; 19h:09m:44s remains)
2017-12-15 07:14:47.605613: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.25154507 -2.5134194 -3.8634644 -4.3701882 -4.1004715 -3.6692071 -3.2080598 -2.7342079 -2.5474343 -2.31681 -1.9256225 -1.6725606 -1.5680151 -1.337877 -1.7995007][-1.0762595 -2.0637507 -3.5083745 -4.4165258 -4.46302 -4.1647644 -3.8122926 -3.3230762 -2.8412917 -2.4558766 -1.9271169 -1.4241482 -1.104952 -0.771104 -1.2860919][-1.7798594 -1.6207105 -3.1059942 -4.3085852 -4.709219 -4.5299191 -4.1294332 -3.4641426 -2.8709836 -2.6056597 -2.2465713 -1.6822102 -1.2845237 -0.85816014 -1.1681238][-2.4377155 -1.753643 -3.0488248 -4.131804 -4.46817 -4.044137 -3.2783709 -2.4055667 -1.9461021 -2.1646831 -2.2012966 -1.7910551 -1.4624878 -1.0511453 -1.2950399][-3.7695453 -1.9780598 -2.8319921 -3.5177002 -3.4381025 -2.4068978 -1.0877359 -0.0041904449 0.14762211 -0.73785365 -1.4270777 -1.5458186 -1.6628697 -1.7471774 -2.26442][-3.904757 -2.1434958 -2.670028 -3.0225585 -2.4822772 -0.8196646 1.1562178 2.5314837 2.5307231 1.2339523 -0.022908688 -0.81571472 -1.6330435 -2.3653305 -3.1248102][-4.0860577 -2.8229852 -2.9261336 -2.5927629 -1.312276 0.96846247 3.560607 5.2359905 5.1353936 3.4339223 1.6692793 0.14750624 -1.3576112 -2.6099577 -3.5915792][-4.77168 -3.0795004 -2.7826362 -1.9145536 0.0014331341 2.754127 5.819078 7.7877212 7.5312314 5.3837404 3.0603828 0.907403 -1.097851 -2.7709014 -3.997493][-4.8421583 -3.2112772 -2.9594998 -2.0992982 -0.022126436 2.7692318 5.8844409 7.8084979 7.506803 5.3717494 3.1047411 1.0439711 -0.90158737 -2.5826125 -3.7851319][-5.1174765 -3.6012282 -3.3858743 -2.475672 -0.63353217 1.5889902 4.1153703 5.7539668 5.707211 4.0324683 2.30654 0.81472564 -0.6619482 -1.9938978 -2.8916943][-5.0108843 -3.4153843 -3.3131933 -2.5592418 -1.0838948 0.59260178 2.585371 3.9942236 4.0790009 2.7985454 1.4165978 0.295043 -0.836746 -1.8455976 -2.511358][-5.332818 -3.7990155 -3.8961396 -3.4070945 -2.1975484 -0.86827672 0.60306 1.6329763 1.7401631 0.94402933 0.034904242 -0.66253853 -1.4952974 -2.1443598 -2.4534078][-6.1325912 -4.4881077 -4.4883862 -4.1291952 -3.431541 -2.6826684 -1.7285172 -0.8758235 -0.539116 -0.65417385 -1.0034523 -1.3632379 -1.973874 -2.3746552 -2.5193002][-6.3345838 -4.5537977 -4.6206779 -4.6692963 -4.576828 -4.1380124 -3.1700358 -2.1097569 -1.6376076 -1.2765481 -1.2998729 -1.7834493 -2.6640182 -3.193481 -3.3937819][-6.6281281 -4.9316044 -5.112741 -5.4118757 -5.5748839 -5.2017956 -4.1581364 -3.0172834 -2.6330175 -2.2114968 -2.3089066 -3.1276124 -4.1446891 -4.500627 -4.3668423]]...]
INFO - root - 2017-12-15 07:14:49.894165: step 18810, loss = 0.21, batch loss = 0.18 (34.7 examples/sec; 0.231 sec/batch; 20h:05m:27s remains)
INFO - root - 2017-12-15 07:14:52.139200: step 18820, loss = 0.23, batch loss = 0.19 (35.8 examples/sec; 0.224 sec/batch; 19h:29m:02s remains)
INFO - root - 2017-12-15 07:14:54.433218: step 18830, loss = 0.21, batch loss = 0.17 (34.7 examples/sec; 0.230 sec/batch; 20h:04m:17s remains)
INFO - root - 2017-12-15 07:14:56.738013: step 18840, loss = 0.33, batch loss = 0.30 (35.5 examples/sec; 0.226 sec/batch; 19h:39m:00s remains)
INFO - root - 2017-12-15 07:14:59.026702: step 18850, loss = 0.32, batch loss = 0.29 (35.7 examples/sec; 0.224 sec/batch; 19h:32m:13s remains)
INFO - root - 2017-12-15 07:15:01.266030: step 18860, loss = 0.21, batch loss = 0.17 (35.2 examples/sec; 0.227 sec/batch; 19h:47m:27s remains)
INFO - root - 2017-12-15 07:15:03.531454: step 18870, loss = 0.24, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 19h:30m:51s remains)
INFO - root - 2017-12-15 07:15:05.787575: step 18880, loss = 0.20, batch loss = 0.17 (35.8 examples/sec; 0.223 sec/batch; 19h:26m:39s remains)
INFO - root - 2017-12-15 07:15:08.033234: step 18890, loss = 0.28, batch loss = 0.25 (36.9 examples/sec; 0.217 sec/batch; 18h:53m:07s remains)
INFO - root - 2017-12-15 07:15:10.320173: step 18900, loss = 0.18, batch loss = 0.15 (36.1 examples/sec; 0.222 sec/batch; 19h:18m:52s remains)
2017-12-15 07:15:10.669133: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5912161 -6.577857 -6.5081749 -6.3709459 -6.1144652 -5.8251591 -5.6605587 -5.6552362 -5.670795 -5.715364 -5.8598843 -5.9209185 -5.7946339 -5.6461639 -5.484086][-6.4083147 -7.2020988 -7.04467 -6.8251162 -6.388505 -5.9548149 -5.7941604 -5.9228086 -6.0969787 -6.1475992 -6.2732277 -6.2194481 -6.0512147 -5.9262686 -5.6927791][-8.476038 -8.0551558 -7.9280729 -7.6531897 -7.0998135 -6.5430822 -6.1585903 -6.10515 -6.3813915 -6.4269114 -6.3816853 -6.0387254 -6.0263534 -6.2689495 -6.1191177][-9.8515434 -8.6380587 -8.2658386 -7.4760695 -6.4510946 -5.6420364 -4.893959 -4.4783506 -4.9824381 -5.2459974 -5.0891633 -4.5237813 -4.9329691 -5.7956281 -5.8860731][-9.6908369 -8.044302 -7.3826761 -5.9633508 -4.2146025 -2.9020171 -1.6841156 -1.1838815 -2.4132113 -3.3679614 -3.2425613 -2.5619719 -3.2954535 -4.5630894 -4.8983016][-9.31037 -7.2538157 -6.321599 -4.4271221 -2.0165617 -0.057037115 1.6593301 2.0247285 0.027427912 -1.6228698 -1.5465059 -0.93151486 -1.8169992 -3.2660003 -3.95732][-8.843502 -6.8229284 -5.498558 -3.016562 0.24960876 3.0223439 5.0416765 5.082303 2.4850175 0.24243999 0.041227579 0.31429935 -0.66769016 -1.9458569 -2.8730943][-8.3146343 -5.9590206 -4.442944 -1.7596914 1.9569151 5.2321091 7.2125616 6.7947435 3.7303655 1.0549548 0.48696303 0.50652289 -0.17906356 -0.94802558 -1.9124146][-8.1197319 -5.7426453 -4.3504004 -2.1348512 1.1816547 4.2099047 5.8704958 5.2863359 2.3965423 -0.13178515 -0.71115994 -0.63395023 -0.80678236 -1.2186697 -2.1125059][-8.2975368 -6.1206923 -4.9623671 -3.3166962 -0.72271478 1.6544185 2.7691195 2.2651761 -0.087977886 -1.9807328 -2.21846 -1.9100481 -1.7873687 -2.1115243 -2.9663532][-8.3836746 -6.5031576 -5.5785294 -4.477365 -2.7727697 -1.2062685 -0.52875805 -0.92401087 -2.8012378 -4.0657072 -3.9007773 -3.4318573 -3.3161535 -3.6294036 -4.2680569][-8.48398 -7.1510153 -6.4921284 -5.8512869 -4.8536482 -3.9673479 -3.5914912 -3.9031978 -5.3046865 -5.9469042 -5.3688536 -4.772624 -4.7196865 -5.0193033 -5.437592][-7.9799585 -7.3284283 -7.101594 -6.7272511 -6.1344023 -5.5883069 -5.3242168 -5.5406609 -6.4226618 -6.6573963 -5.9271193 -5.3834367 -5.5250387 -5.7365794 -5.9081907][-7.0870447 -6.7387304 -6.8454828 -6.7073565 -6.3893261 -6.0625372 -5.7645812 -5.7973542 -6.301074 -6.3279018 -5.6309347 -5.1810441 -5.3202477 -5.3534374 -5.3791952][-6.4576588 -6.1277413 -6.2815619 -6.281724 -6.1346674 -5.8878412 -5.5405593 -5.44536 -5.6936226 -5.7009306 -5.2190914 -4.911778 -4.9801607 -4.9130187 -4.9234557]]...]
INFO - root - 2017-12-15 07:15:12.965820: step 18910, loss = 0.21, batch loss = 0.18 (33.0 examples/sec; 0.242 sec/batch; 21h:05m:20s remains)
INFO - root - 2017-12-15 07:15:15.270832: step 18920, loss = 0.22, batch loss = 0.18 (34.9 examples/sec; 0.230 sec/batch; 19h:59m:38s remains)
INFO - root - 2017-12-15 07:15:17.530687: step 18930, loss = 0.17, batch loss = 0.14 (36.1 examples/sec; 0.222 sec/batch; 19h:17m:51s remains)
INFO - root - 2017-12-15 07:15:19.811996: step 18940, loss = 0.19, batch loss = 0.15 (35.3 examples/sec; 0.227 sec/batch; 19h:44m:03s remains)
INFO - root - 2017-12-15 07:15:22.100352: step 18950, loss = 0.22, batch loss = 0.19 (34.7 examples/sec; 0.230 sec/batch; 20h:03m:34s remains)
INFO - root - 2017-12-15 07:15:24.392213: step 18960, loss = 0.22, batch loss = 0.18 (35.4 examples/sec; 0.226 sec/batch; 19h:41m:31s remains)
INFO - root - 2017-12-15 07:15:26.634805: step 18970, loss = 0.40, batch loss = 0.37 (35.3 examples/sec; 0.227 sec/batch; 19h:44m:37s remains)
INFO - root - 2017-12-15 07:15:28.891069: step 18980, loss = 0.24, batch loss = 0.21 (35.1 examples/sec; 0.228 sec/batch; 19h:50m:51s remains)
INFO - root - 2017-12-15 07:15:31.162809: step 18990, loss = 0.22, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 19h:25m:32s remains)
INFO - root - 2017-12-15 07:15:33.521005: step 19000, loss = 0.18, batch loss = 0.15 (35.0 examples/sec; 0.228 sec/batch; 19h:53m:10s remains)
2017-12-15 07:15:33.863041: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.973042 -7.4534311 -7.5971146 -7.13556 -6.0115266 -5.3462687 -5.1080112 -5.3354778 -6.205142 -7.5465155 -8.5236759 -8.9967451 -8.3320971 -6.838954 -5.5339928][-5.6951818 -7.4007778 -7.7109804 -7.3536224 -6.043232 -4.8957767 -4.2305617 -4.2000532 -4.9025087 -6.3342409 -7.7053289 -8.5761814 -8.25981 -6.9570866 -5.7224126][-6.3464985 -7.07316 -7.5043974 -7.0924397 -5.4651566 -3.798492 -2.7074401 -2.4671757 -3.1428084 -4.7757921 -6.5284605 -7.8167963 -7.9634705 -6.889782 -5.6956682][-6.1323338 -6.0645161 -6.5456858 -5.9472151 -4.0161505 -1.9320443 -0.60381925 -0.29078984 -1.0120895 -2.7884774 -4.8473883 -6.6535568 -7.3910761 -6.6499929 -5.5475507][-5.8739591 -4.9252763 -5.1946144 -4.2803917 -1.9557288 0.61359048 2.1377099 2.3118269 1.3525321 -0.52036738 -2.8078141 -5.2149734 -6.5492306 -6.1647115 -5.2471519][-5.283638 -3.5273979 -3.3751137 -2.0675216 0.63359046 3.6096117 5.1518412 4.98757 3.6191113 1.5226223 -1.0428929 -4.0078993 -5.8150477 -5.7764344 -5.0521507][-4.3322992 -2.3336661 -1.9204543 -0.48411119 2.359956 5.4934788 6.8351536 6.2588415 4.6073971 2.37836 -0.38771641 -3.6944118 -5.7148786 -5.8139076 -5.1582971][-3.7215114 -1.8104131 -1.5311357 -0.34446645 2.3303168 5.3941135 6.4691172 5.5328579 3.791661 1.6081746 -1.0727491 -4.3167672 -6.1812954 -6.2236071 -5.5084877][-3.9299419 -2.3485367 -2.2782552 -1.5053197 0.68768597 3.3856208 4.2883158 3.3953478 1.9058549 0.09052515 -2.1418855 -5.0578165 -6.5974388 -6.602314 -5.8511333][-4.3882933 -3.2430806 -3.3816929 -3.0548828 -1.5371771 0.66714168 1.4190676 0.82567739 -0.25667322 -1.4297535 -2.979 -5.390059 -6.5969381 -6.6494112 -5.9020171][-4.80828 -4.2486973 -4.6781335 -4.8737535 -3.958024 -2.1796088 -1.4215348 -1.6319752 -2.429739 -3.1661413 -4.1116505 -5.9813919 -6.7562742 -6.7078123 -5.8325453][-5.2103519 -5.1460857 -5.7694988 -6.3469696 -5.7549257 -4.2464752 -3.4370356 -3.2552888 -3.9561446 -4.5469503 -5.0754528 -6.4149637 -6.8123636 -6.6782169 -5.693984][-5.6166592 -5.6103506 -6.0474052 -6.7974424 -6.3563404 -5.030077 -4.3212686 -3.9341359 -4.5711188 -5.1451664 -5.4528246 -6.3302937 -6.5407829 -6.4068108 -5.4608488][-5.6948342 -5.5307903 -5.7945461 -6.5940781 -6.12691 -4.9868731 -4.4203229 -4.0496588 -4.6239319 -5.2612343 -5.5365067 -6.1342878 -6.3561153 -6.2507858 -5.3230796][-5.6840706 -5.4459386 -5.6126094 -6.373517 -5.6830821 -4.5812283 -4.1173143 -3.9296217 -4.6358371 -5.4939537 -5.9316483 -6.3719473 -6.438488 -6.1618567 -5.2290659]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 07:15:36.189636: step 19010, loss = 0.18, batch loss = 0.15 (34.3 examples/sec; 0.233 sec/batch; 20h:19m:02s remains)
INFO - root - 2017-12-15 07:15:38.481401: step 19020, loss = 0.19, batch loss = 0.15 (35.4 examples/sec; 0.226 sec/batch; 19h:41m:11s remains)
INFO - root - 2017-12-15 07:15:40.748899: step 19030, loss = 0.26, batch loss = 0.23 (35.7 examples/sec; 0.224 sec/batch; 19h:31m:25s remains)
INFO - root - 2017-12-15 07:15:43.043588: step 19040, loss = 0.24, batch loss = 0.20 (35.1 examples/sec; 0.228 sec/batch; 19h:50m:33s remains)
INFO - root - 2017-12-15 07:15:45.306517: step 19050, loss = 0.21, batch loss = 0.18 (36.1 examples/sec; 0.222 sec/batch; 19h:18m:52s remains)
INFO - root - 2017-12-15 07:15:47.576778: step 19060, loss = 0.20, batch loss = 0.16 (34.6 examples/sec; 0.231 sec/batch; 20h:06m:09s remains)
INFO - root - 2017-12-15 07:15:49.877794: step 19070, loss = 0.22, batch loss = 0.19 (34.1 examples/sec; 0.235 sec/batch; 20h:25m:03s remains)
INFO - root - 2017-12-15 07:15:52.137767: step 19080, loss = 0.27, batch loss = 0.23 (36.2 examples/sec; 0.221 sec/batch; 19h:15m:50s remains)
INFO - root - 2017-12-15 07:15:54.406937: step 19090, loss = 0.24, batch loss = 0.20 (36.1 examples/sec; 0.221 sec/batch; 19h:16m:08s remains)
INFO - root - 2017-12-15 07:15:56.720645: step 19100, loss = 0.18, batch loss = 0.14 (34.7 examples/sec; 0.231 sec/batch; 20h:04m:52s remains)
2017-12-15 07:15:57.007402: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9350278 -5.0291929 -5.2344303 -5.3241158 -5.16976 -4.7582092 -4.7027583 -5.0538788 -5.3896446 -5.8656697 -6.2003794 -6.0920424 -5.8097992 -5.3089776 -4.6551404][-5.4059515 -6.2850876 -6.4371634 -6.4627371 -6.1188679 -5.35787 -5.16847 -5.6805911 -6.0668049 -6.78341 -7.4115052 -7.301836 -6.9408736 -6.2582827 -5.3978062][-6.9733014 -6.461246 -6.4044795 -6.3650217 -5.9646235 -4.8984308 -4.556519 -5.3515291 -5.9296145 -6.90253 -7.8991032 -7.8888741 -7.5534005 -6.7006269 -5.7009139][-7.5507326 -5.8183436 -5.5642323 -5.5425777 -5.2070475 -3.8981421 -3.2997322 -4.3903875 -5.1749258 -6.3543105 -7.6945934 -7.8787041 -7.7262969 -6.8083115 -5.8086133][-7.2989244 -4.6113758 -4.0914454 -3.8914766 -3.5042591 -1.9092485 -0.95013189 -2.2899439 -3.3416898 -4.7933807 -6.51888 -7.0234966 -7.144825 -6.3359394 -5.558116][-6.1881361 -3.1902881 -2.2799463 -1.6796447 -1.0855576 0.79586649 1.9939332 0.35877872 -1.0677652 -2.8548794 -4.9564013 -5.8322239 -6.3975039 -5.7674694 -5.12586][-4.6074467 -1.761552 -0.48380983 0.50894 1.2807448 3.2473936 4.40269 2.3645806 0.618274 -1.409821 -3.7788918 -4.9119043 -5.8007197 -5.3478136 -4.7308989][-4.4150372 -1.2625364 0.36730313 1.7765784 2.7180934 4.6624227 5.5998788 3.2611127 1.3206806 -0.89501381 -3.4162288 -4.5765543 -5.5691953 -5.2436166 -4.5472188][-5.1833978 -2.0836773 -0.38193667 1.2136333 2.3101659 4.1613169 4.8465066 2.4592552 0.38138008 -1.8446054 -4.3050308 -5.3636231 -6.2405052 -5.8197002 -4.7456951][-6.553246 -3.7304685 -2.2476244 -0.73095179 0.50326586 2.2610297 2.7057443 0.40562487 -1.6667831 -3.726748 -5.7899003 -6.6068926 -7.20709 -6.4924865 -4.9747915][-7.94621 -5.6350555 -4.6512728 -3.3486452 -1.9366976 -0.25298011 -0.030262709 -2.104764 -4.0126328 -5.7693658 -7.2040954 -7.6123719 -7.7002759 -6.759438 -5.1062212][-9.1158962 -7.2353282 -6.660594 -5.6374235 -4.3438215 -3.0739653 -3.1201754 -4.8479815 -6.4752045 -7.6640692 -8.2735157 -8.1616459 -7.775208 -6.6689434 -5.027667][-9.187151 -7.6097274 -7.2811604 -6.6458769 -5.7519121 -5.005518 -5.1721039 -6.3227806 -7.349411 -7.9525671 -8.01145 -7.6840439 -7.0944133 -6.0749 -4.7524376][-8.2764559 -7.0381484 -6.9111824 -6.6207023 -6.1672897 -5.7783728 -5.8731275 -6.4247446 -6.9163074 -7.1330528 -6.9853964 -6.6462545 -6.1400213 -5.3982229 -4.5334606][-6.8844438 -5.8951821 -5.8513489 -5.7838526 -5.6911325 -5.6168795 -5.6460133 -5.8066673 -5.9750013 -5.9949956 -5.8667865 -5.6943493 -5.4108195 -4.9761715 -4.4424314]]...]
INFO - root - 2017-12-15 07:15:59.238660: step 19110, loss = 0.35, batch loss = 0.31 (36.5 examples/sec; 0.219 sec/batch; 19h:03m:34s remains)
INFO - root - 2017-12-15 07:16:01.510558: step 19120, loss = 0.30, batch loss = 0.26 (35.8 examples/sec; 0.223 sec/batch; 19h:27m:00s remains)
INFO - root - 2017-12-15 07:16:03.770704: step 19130, loss = 0.27, batch loss = 0.23 (36.0 examples/sec; 0.222 sec/batch; 19h:20m:54s remains)
INFO - root - 2017-12-15 07:16:06.047015: step 19140, loss = 0.22, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 19h:22m:43s remains)
INFO - root - 2017-12-15 07:16:08.324077: step 19150, loss = 0.23, batch loss = 0.20 (33.8 examples/sec; 0.237 sec/batch; 20h:37m:29s remains)
INFO - root - 2017-12-15 07:16:10.619761: step 19160, loss = 0.20, batch loss = 0.17 (34.4 examples/sec; 0.232 sec/batch; 20h:12m:51s remains)
INFO - root - 2017-12-15 07:16:12.888522: step 19170, loss = 0.19, batch loss = 0.15 (35.4 examples/sec; 0.226 sec/batch; 19h:40m:36s remains)
INFO - root - 2017-12-15 07:16:15.139902: step 19180, loss = 0.29, batch loss = 0.26 (34.8 examples/sec; 0.230 sec/batch; 20h:00m:37s remains)
INFO - root - 2017-12-15 07:16:17.429364: step 19190, loss = 0.29, batch loss = 0.25 (35.0 examples/sec; 0.228 sec/batch; 19h:52m:38s remains)
INFO - root - 2017-12-15 07:16:19.702881: step 19200, loss = 0.16, batch loss = 0.13 (35.6 examples/sec; 0.225 sec/batch; 19h:33m:13s remains)
2017-12-15 07:16:19.990045: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.011749 -2.6596975 -2.4158328 -2.4395411 -2.7678671 -2.7662106 -2.4824514 -1.9707996 -1.4309394 -1.4742765 -1.8716068 -2.5407324 -2.8601937 -2.950434 -3.0787072][-0.74445486 -0.46073115 -0.3289988 -0.69172943 -1.4435647 -1.8981415 -2.0162733 -1.809968 -1.276844 -1.3244272 -1.7776816 -2.6419024 -2.9119256 -2.7695494 -2.815753][-0.0872674 0.63979483 0.72595024 0.27286863 -0.61542439 -1.3699384 -1.9931823 -2.2534719 -1.9697615 -1.7820649 -1.9216874 -2.7536607 -2.890574 -2.4537351 -2.3604786][-0.28503358 0.94741654 0.92753029 0.54362392 -0.088233471 -0.76951075 -1.5550476 -2.1829765 -2.2713017 -2.1965611 -2.5017135 -3.5223546 -3.5789065 -2.8175619 -2.445863][-0.3792969 1.1799791 1.2017181 0.94753861 0.45487785 -0.11167264 -0.64234293 -1.1363449 -1.4118544 -1.6245979 -2.2162893 -3.520308 -3.8618035 -3.2170885 -2.68629][-0.74918878 0.55173135 0.53732085 0.36403894 0.30742502 0.66832232 0.92014933 0.51585984 -0.34335017 -1.1012638 -1.7678618 -2.8884013 -3.4803259 -3.1676111 -2.8122082][-1.6958038 -0.56961656 -0.46873558 -0.44688487 0.0092456341 1.149344 2.1941557 2.197813 1.0602422 -0.0940547 -0.80416179 -1.6405523 -2.2455232 -2.3146174 -2.2869704][-2.4822776 -0.58106291 -0.11704659 0.098524332 0.87468457 2.51258 3.8101788 3.9123034 2.6023598 1.069237 0.015413523 -0.97511327 -1.6469646 -1.9436064 -2.2220402][-3.2511482 -1.2207524 -0.56490588 -0.17679977 0.64793921 2.199728 3.2784724 3.5934815 2.3507905 0.48269629 -0.69143438 -1.4500668 -1.9069405 -1.96042 -2.2738433][-4.3172841 -2.709336 -2.0954111 -1.4450029 -0.63339877 0.82868505 1.8114781 2.068069 1.1563282 -0.28577888 -1.1775807 -1.589087 -2.0745852 -2.3063715 -2.7415009][-4.9877238 -3.7743912 -3.2216802 -2.5394878 -1.7501451 -0.43954849 0.61806512 0.88636613 0.088605881 -1.0645909 -1.7730708 -2.1924384 -2.869549 -3.2838736 -3.754797][-5.2477894 -4.4002266 -4.0537348 -3.6809573 -3.1997187 -2.28223 -1.5221713 -1.2370081 -1.8065443 -2.6934168 -3.1268525 -3.2348876 -3.6752415 -4.04735 -4.5021958][-5.6305542 -5.0925751 -4.9355922 -4.5831566 -4.229445 -3.79812 -3.3916671 -3.3643136 -3.6379759 -4.0657792 -4.1047435 -3.8358951 -4.0789843 -4.4644632 -5.0098877][-5.5812492 -4.8801908 -4.4448314 -3.8479514 -3.4937284 -3.3878856 -3.3689253 -3.7384937 -4.2483492 -4.8225689 -4.8976703 -4.5197639 -4.6489687 -4.9111619 -5.2046437][-5.5859919 -4.6585293 -4.0618296 -3.6448016 -3.4163928 -3.4935405 -3.8977458 -4.5599427 -5.1750269 -5.63916 -5.8333607 -5.614604 -5.3666239 -5.0404606 -4.9888229]]...]
INFO - root - 2017-12-15 07:16:22.258191: step 19210, loss = 0.23, batch loss = 0.19 (34.8 examples/sec; 0.230 sec/batch; 20h:00m:05s remains)
INFO - root - 2017-12-15 07:16:24.558932: step 19220, loss = 0.26, batch loss = 0.23 (36.0 examples/sec; 0.222 sec/batch; 19h:18m:53s remains)
INFO - root - 2017-12-15 07:16:26.881027: step 19230, loss = 0.22, batch loss = 0.19 (35.3 examples/sec; 0.227 sec/batch; 19h:44m:18s remains)
INFO - root - 2017-12-15 07:16:29.164274: step 19240, loss = 0.41, batch loss = 0.38 (35.5 examples/sec; 0.226 sec/batch; 19h:37m:25s remains)
INFO - root - 2017-12-15 07:16:31.427917: step 19250, loss = 0.19, batch loss = 0.16 (36.1 examples/sec; 0.222 sec/batch; 19h:18m:23s remains)
INFO - root - 2017-12-15 07:16:33.720388: step 19260, loss = 0.23, batch loss = 0.19 (34.1 examples/sec; 0.235 sec/batch; 20h:26m:09s remains)
INFO - root - 2017-12-15 07:16:36.001129: step 19270, loss = 0.20, batch loss = 0.17 (36.0 examples/sec; 0.222 sec/batch; 19h:18m:50s remains)
INFO - root - 2017-12-15 07:16:38.303433: step 19280, loss = 0.23, batch loss = 0.19 (36.1 examples/sec; 0.221 sec/batch; 19h:15m:49s remains)
INFO - root - 2017-12-15 07:16:40.627219: step 19290, loss = 0.29, batch loss = 0.26 (35.1 examples/sec; 0.228 sec/batch; 19h:49m:59s remains)
INFO - root - 2017-12-15 07:16:42.894735: step 19300, loss = 0.30, batch loss = 0.26 (35.3 examples/sec; 0.227 sec/batch; 19h:43m:15s remains)
2017-12-15 07:16:43.246746: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7452246 -1.0998065 -0.69905651 -1.4466085 -2.3303065 -2.8844128 -3.2882171 -4.0333681 -5.4422274 -6.2621384 -5.8605218 -5.0226307 -4.289392 -3.7035134 -3.1972528][-2.0265224 -1.1112967 -0.91625261 -1.7585063 -2.5519707 -3.1077647 -3.4603992 -3.9136412 -4.9587193 -5.4898062 -5.1014085 -4.463582 -4.0138144 -3.4435806 -2.7579687][-1.9825603 -1.0621064 -1.1926061 -1.9266505 -2.4111557 -2.7695675 -3.0069232 -3.3382382 -4.0582604 -4.5517178 -4.3328114 -4.0434942 -4.0300732 -3.5288787 -2.8059964][-1.5541847 -0.65781069 -0.95331562 -1.3496053 -1.3457088 -1.4405247 -1.5892366 -2.0074737 -2.69056 -3.4252663 -3.6445205 -3.9537833 -4.5634909 -4.3498964 -3.5794258][-1.6011678 -0.41527534 -0.50987494 -0.24443471 0.40221357 0.57014728 0.39294314 -0.2170682 -1.058777 -2.1159918 -2.8319888 -3.8202977 -5.1525612 -5.3445053 -4.5283546][-2.1159003 -0.74081945 -0.36768889 0.54553843 1.7848239 2.3620963 2.3233356 1.5973964 0.65887451 -0.60076845 -1.6449572 -3.2358265 -5.1664133 -5.8566236 -5.2292562][-2.3231962 -1.0910515 -0.45060384 0.76403809 2.3662386 3.3463368 3.5224953 2.8254089 1.9136801 0.54909754 -0.83044589 -2.968462 -5.3143797 -6.4110231 -6.0551109][-1.5876198 -0.52732885 -0.22427678 0.59006453 2.1454639 3.435349 3.804759 3.1962881 2.3573198 0.98415256 -0.67094779 -3.1551361 -5.6385088 -6.8900352 -6.7710404][-0.37120247 0.36497784 -0.059904337 -0.11958122 0.99891996 2.3903913 2.8333907 2.2173753 1.4625869 0.25087547 -1.3496625 -3.6646533 -5.9751806 -7.2470312 -7.4726648][0.4692831 0.85550785 -0.34638071 -1.2491847 -0.66769814 0.52709556 0.84448051 0.22236419 -0.44507217 -1.4018385 -2.692822 -4.434577 -6.22548 -7.3761492 -7.824923][0.87055564 0.94803047 -0.69931722 -2.0840583 -2.0316892 -1.3160129 -1.1735995 -1.6861169 -2.2530825 -3.0392978 -4.0759468 -5.1776724 -6.31995 -7.10135 -7.4080243][-0.067104578 0.0671165 -1.4135721 -2.8142507 -3.2275257 -3.13232 -3.1517315 -3.5150654 -3.9732695 -4.6212912 -5.3667407 -5.8584871 -6.2997694 -6.532948 -6.4113073][-2.1033227 -1.7560606 -2.8702095 -4.0239372 -4.631526 -4.9365044 -5.018712 -5.1623816 -5.4249496 -5.8645177 -6.3106456 -6.3187613 -6.2002559 -5.9115982 -5.3193264][-3.8701892 -3.5900681 -4.5090976 -5.399785 -6.0004344 -6.4889908 -6.5310097 -6.4280686 -6.3890371 -6.5463858 -6.6665068 -6.3414063 -5.914948 -5.435607 -4.6961145][-5.4077697 -5.2696085 -6.0217757 -6.6196513 -6.9980164 -7.3807092 -7.3437276 -7.1457863 -6.9137278 -6.8353972 -6.6617265 -6.1495752 -5.6404819 -5.1882977 -4.5945539]]...]
INFO - root - 2017-12-15 07:16:45.534418: step 19310, loss = 0.22, batch loss = 0.19 (36.3 examples/sec; 0.221 sec/batch; 19h:11m:05s remains)
INFO - root - 2017-12-15 07:16:47.774404: step 19320, loss = 0.21, batch loss = 0.18 (35.8 examples/sec; 0.223 sec/batch; 19h:26m:20s remains)
INFO - root - 2017-12-15 07:16:50.069607: step 19330, loss = 0.23, batch loss = 0.20 (35.1 examples/sec; 0.228 sec/batch; 19h:48m:40s remains)
INFO - root - 2017-12-15 07:16:52.329039: step 19340, loss = 0.29, batch loss = 0.25 (33.8 examples/sec; 0.236 sec/batch; 20h:33m:31s remains)
INFO - root - 2017-12-15 07:16:54.571386: step 19350, loss = 0.36, batch loss = 0.33 (34.1 examples/sec; 0.235 sec/batch; 20h:24m:26s remains)
INFO - root - 2017-12-15 07:16:56.833252: step 19360, loss = 0.19, batch loss = 0.16 (35.7 examples/sec; 0.224 sec/batch; 19h:28m:12s remains)
INFO - root - 2017-12-15 07:16:59.147808: step 19370, loss = 0.27, batch loss = 0.24 (35.6 examples/sec; 0.225 sec/batch; 19h:32m:00s remains)
INFO - root - 2017-12-15 07:17:01.423578: step 19380, loss = 0.29, batch loss = 0.26 (34.4 examples/sec; 0.233 sec/batch; 20h:14m:59s remains)
INFO - root - 2017-12-15 07:17:03.697476: step 19390, loss = 0.26, batch loss = 0.22 (36.2 examples/sec; 0.221 sec/batch; 19h:12m:13s remains)
INFO - root - 2017-12-15 07:17:06.013868: step 19400, loss = 0.19, batch loss = 0.16 (34.0 examples/sec; 0.235 sec/batch; 20h:28m:36s remains)
2017-12-15 07:17:06.358862: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6366653 -7.8882442 -8.2379208 -7.9156165 -7.153986 -6.4602304 -6.0280223 -5.9110403 -5.7146983 -5.6541152 -5.5575647 -5.7866554 -6.2613339 -6.79413 -7.1320662][-7.5037704 -9.6262226 -9.9471827 -9.5640125 -8.866436 -8.0819588 -7.4570527 -7.2969604 -7.1218977 -6.868567 -6.6894836 -6.9793096 -7.5008187 -8.0703554 -8.3663578][-8.3200493 -9.860651 -9.8873262 -9.5557795 -8.9394188 -7.9072237 -7.0646005 -6.9444094 -6.8784404 -6.5085568 -6.3245821 -6.6558013 -7.103385 -7.5731645 -7.9188547][-8.0839853 -8.8849182 -8.7335062 -8.3789177 -7.66019 -6.1920385 -4.9021311 -4.588069 -4.56939 -4.4052706 -4.6900978 -5.420743 -5.9521437 -6.4020476 -6.8891397][-7.4199533 -7.8397989 -7.3620949 -6.6958828 -5.5983706 -3.4818592 -1.5772121 -1.0280974 -1.167563 -1.5060195 -2.5847631 -3.8720553 -4.553091 -5.0571346 -5.6486311][-6.3948851 -6.4655781 -5.7241411 -4.6674261 -3.1585214 -0.6046778 1.6906931 2.3565106 1.9755511 1.0996439 -0.59822309 -2.35118 -3.2259455 -3.8071165 -4.5055037][-5.3944626 -5.3655443 -4.3461928 -2.7250161 -0.75123703 1.9725728 4.3784423 5.047987 4.323575 2.9676237 0.86758137 -1.3020821 -2.4349163 -3.0673206 -3.8314691][-5.3768015 -5.0230436 -3.7774706 -1.6597626 0.77207971 3.4697704 5.6881528 6.3133135 5.3266754 3.7685413 1.6540408 -0.67090178 -2.0324469 -2.8166118 -3.7207894][-5.9215984 -5.3171911 -4.03677 -1.7847096 0.65576887 2.9124928 4.6312308 5.1711674 4.253684 2.9575171 1.2304184 -0.93025255 -2.3880589 -3.2003074 -4.0804653][-6.5569143 -5.5878029 -4.4212604 -2.4998672 -0.40246606 1.2987719 2.4231744 2.7861428 1.9922867 0.95929885 -0.39156616 -2.3161447 -3.7788377 -4.6021919 -5.3299756][-6.9533796 -5.6227913 -4.6760726 -3.3545885 -1.8522753 -0.74021018 -0.0932107 0.27012753 -0.21984076 -1.0891182 -2.3435817 -4.1344914 -5.6636982 -6.5178123 -6.9392605][-7.1729326 -5.5992813 -4.9329019 -4.196732 -3.3923991 -2.9492078 -2.7309134 -2.4264858 -2.56828 -3.1507492 -4.308094 -5.8848257 -7.15341 -7.6565228 -7.6374631][-7.181788 -5.4785542 -4.8179564 -4.4431944 -4.2578259 -4.5137753 -4.7133503 -4.3789673 -4.1156874 -4.3819556 -5.2724981 -6.631393 -7.6919494 -7.9421396 -7.6474528][-6.7355928 -5.1080694 -4.466105 -4.2016754 -4.2587223 -4.8680239 -5.3514929 -5.0968742 -4.7598839 -4.8711452 -5.5151081 -6.573698 -7.3601284 -7.4277573 -7.1001172][-6.2650623 -4.7439914 -4.0309019 -3.6653233 -3.8345392 -4.7104015 -5.416255 -5.2427473 -4.8547907 -4.8235126 -5.2164164 -5.9803877 -6.5103059 -6.4626126 -6.1567512]]...]
INFO - root - 2017-12-15 07:17:08.669627: step 19410, loss = 0.21, batch loss = 0.18 (33.4 examples/sec; 0.239 sec/batch; 20h:49m:41s remains)
INFO - root - 2017-12-15 07:17:10.956784: step 19420, loss = 0.27, batch loss = 0.23 (34.7 examples/sec; 0.230 sec/batch; 20h:01m:52s remains)
INFO - root - 2017-12-15 07:17:13.259063: step 19430, loss = 0.19, batch loss = 0.16 (33.6 examples/sec; 0.238 sec/batch; 20h:42m:09s remains)
INFO - root - 2017-12-15 07:17:15.574463: step 19440, loss = 0.21, batch loss = 0.18 (33.7 examples/sec; 0.237 sec/batch; 20h:38m:57s remains)
INFO - root - 2017-12-15 07:17:17.833759: step 19450, loss = 0.34, batch loss = 0.30 (34.6 examples/sec; 0.231 sec/batch; 20h:04m:46s remains)
INFO - root - 2017-12-15 07:17:20.126799: step 19460, loss = 0.22, batch loss = 0.19 (35.8 examples/sec; 0.223 sec/batch; 19h:25m:21s remains)
INFO - root - 2017-12-15 07:17:22.367205: step 19470, loss = 0.22, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 19h:22m:37s remains)
INFO - root - 2017-12-15 07:17:24.628654: step 19480, loss = 0.20, batch loss = 0.16 (34.0 examples/sec; 0.236 sec/batch; 20h:28m:46s remains)
INFO - root - 2017-12-15 07:17:26.894942: step 19490, loss = 0.41, batch loss = 0.37 (34.4 examples/sec; 0.233 sec/batch; 20h:14m:21s remains)
INFO - root - 2017-12-15 07:17:29.177741: step 19500, loss = 0.33, batch loss = 0.29 (35.4 examples/sec; 0.226 sec/batch; 19h:40m:32s remains)
2017-12-15 07:17:29.499639: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1625179 -1.3195369 -1.2127532 -1.1137619 -0.70368588 -0.78961015 -1.5858238 -2.0216756 -1.4883008 -0.78711331 -0.033063412 0.14618826 0.13000917 0.044786692 -0.59378731][-2.3569794 -1.9770267 -2.0330319 -2.0130191 -1.6992745 -1.6719847 -2.0886528 -2.2596984 -1.6467946 -0.77839088 0.22579193 0.72278166 0.82803726 0.70992088 0.0075120926][-3.446507 -2.8387733 -3.1061692 -3.1989522 -2.9996328 -2.9371345 -3.0428507 -2.9772062 -2.2982423 -1.4181323 -0.26035058 0.55147409 0.8639245 0.91134 0.31209826][-4.5989981 -3.861917 -4.2577105 -4.233604 -3.9837782 -3.9055922 -3.7418022 -3.3280573 -2.6148858 -1.834552 -0.70670187 0.21487308 0.76571345 1.1513822 0.90922952][-5.4714928 -4.7479506 -5.0823174 -4.7514467 -4.0990906 -3.5283849 -2.7801387 -1.9737509 -1.3716332 -0.90811312 -0.14217973 0.44779444 0.92690134 1.4016433 1.2475874][-5.7455463 -4.7228708 -4.759594 -4.0889721 -2.9330225 -1.7618153 -0.61813545 0.3637867 0.65043855 0.60778213 0.66558266 0.71571994 0.787086 0.92796087 0.58764791][-4.7738094 -3.6575885 -3.4263144 -2.5330243 -1.0373299 0.48332858 1.7431505 2.5456054 2.3043382 1.7356005 1.0644922 0.5193 0.080667496 -0.081300974 -0.47806931][-4.1759605 -2.6271245 -2.1196167 -1.01845 0.5119977 1.9728682 2.973594 3.3898947 2.7066133 1.7793734 0.55263853 -0.25400245 -0.80194223 -0.94683278 -1.1747012][-3.5244207 -1.7284918 -1.135165 -0.18092561 0.9875865 2.0725706 2.6953828 2.6465881 1.6912389 0.67441249 -0.64649987 -1.3743973 -1.7056726 -1.6144848 -1.5303199][-3.3617373 -1.5813442 -1.1263813 -0.48968828 0.26292396 1.0556252 1.4286866 1.2141519 0.30552387 -0.68424416 -2.0848725 -2.7054026 -2.8605444 -2.5999877 -2.2808287][-3.7068517 -2.2565467 -1.9908739 -1.4529977 -0.8386153 -0.12504101 0.28850317 0.060553312 -0.76935136 -1.6948483 -2.9829388 -3.650548 -4.0105124 -3.9512095 -3.5967209][-4.519722 -3.34175 -3.0433068 -2.4145274 -1.7823998 -1.1160047 -0.72901523 -0.95695245 -1.7405891 -2.6389844 -3.7628469 -4.4006767 -4.7694788 -4.8803177 -4.5945406][-5.4345427 -4.5273671 -4.0479894 -3.2979271 -2.6194553 -2.1002922 -1.9209281 -2.1101656 -2.7813463 -3.633625 -4.4091434 -4.826128 -5.0694809 -5.1478734 -4.8506956][-4.7797451 -3.9750614 -3.4292436 -2.7746081 -2.2384489 -1.983652 -2.1129973 -2.2579443 -2.726681 -3.3151412 -3.7607079 -4.0811586 -4.3912868 -4.661334 -4.5608511][-3.7461984 -2.8643556 -2.329725 -1.9596487 -1.6668284 -1.6209435 -1.9968503 -2.1522932 -2.3347118 -2.5790267 -2.6548591 -2.784585 -3.0875897 -3.4558477 -3.5481014]]...]
INFO - root - 2017-12-15 07:17:31.777357: step 19510, loss = 0.20, batch loss = 0.17 (35.4 examples/sec; 0.226 sec/batch; 19h:38m:44s remains)
INFO - root - 2017-12-15 07:17:34.043135: step 19520, loss = 0.25, batch loss = 0.22 (35.3 examples/sec; 0.227 sec/batch; 19h:41m:33s remains)
INFO - root - 2017-12-15 07:17:36.324429: step 19530, loss = 0.24, batch loss = 0.20 (36.1 examples/sec; 0.222 sec/batch; 19h:16m:41s remains)
INFO - root - 2017-12-15 07:17:38.585965: step 19540, loss = 0.23, batch loss = 0.20 (34.5 examples/sec; 0.232 sec/batch; 20h:09m:09s remains)
INFO - root - 2017-12-15 07:17:40.892848: step 19550, loss = 0.32, batch loss = 0.29 (35.9 examples/sec; 0.223 sec/batch; 19h:21m:44s remains)
INFO - root - 2017-12-15 07:17:43.197430: step 19560, loss = 0.14, batch loss = 0.11 (35.4 examples/sec; 0.226 sec/batch; 19h:37m:53s remains)
INFO - root - 2017-12-15 07:17:45.464184: step 19570, loss = 0.19, batch loss = 0.15 (34.8 examples/sec; 0.230 sec/batch; 20h:00m:34s remains)
INFO - root - 2017-12-15 07:17:47.725742: step 19580, loss = 0.26, batch loss = 0.23 (34.6 examples/sec; 0.231 sec/batch; 20h:05m:27s remains)
INFO - root - 2017-12-15 07:17:50.052611: step 19590, loss = 0.20, batch loss = 0.17 (34.2 examples/sec; 0.234 sec/batch; 20h:19m:30s remains)
INFO - root - 2017-12-15 07:17:52.319866: step 19600, loss = 0.29, batch loss = 0.26 (36.0 examples/sec; 0.222 sec/batch; 19h:20m:09s remains)
2017-12-15 07:17:52.633773: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0562553 -4.8830929 -4.8854012 -4.4354243 -3.7704506 -3.6427178 -3.8202581 -4.5293231 -5.3837872 -5.9599271 -6.2780647 -6.0014744 -5.3783031 -4.6989026 -4.2084875][-3.2815797 -4.82431 -5.3769016 -5.3287706 -5.0662804 -5.1001077 -5.3280463 -5.6043587 -6.0644732 -6.4210062 -6.5708237 -5.9099312 -5.2590485 -4.7154093 -4.3131971][-3.8051071 -5.076972 -5.6669083 -5.6252508 -5.3515587 -5.1868267 -5.1030397 -4.9991026 -5.1867404 -5.5494943 -5.799325 -5.0658145 -4.4508238 -4.01548 -3.6939297][-4.3258648 -5.0814228 -5.4248505 -5.1244383 -4.677124 -4.3421574 -3.9909763 -3.556736 -3.646724 -4.2117071 -4.7542729 -4.0082383 -3.2702007 -2.8086185 -2.65503][-5.0132141 -4.7301273 -4.5419121 -3.95344 -3.394845 -2.865509 -2.3732448 -1.6491387 -1.6773119 -2.6154013 -3.6591771 -3.0680428 -2.4623179 -2.1347673 -1.9694796][-4.992815 -4.0626483 -3.438086 -2.5155156 -1.8000255 -0.9695704 -0.21182418 0.55748177 0.49169421 -0.84968293 -2.5229735 -2.4418561 -2.2443123 -2.0233402 -1.4911439][-4.3805442 -3.4534514 -2.4993286 -1.1773816 -0.10609126 1.0981371 1.9008973 2.3792737 1.9646356 0.24124813 -1.8702309 -2.1584 -2.0993223 -1.7453349 -1.0233766][-4.1236811 -3.1010671 -1.9992288 -0.48333406 0.97828913 2.4544208 3.1088421 3.1663873 2.377013 0.49944353 -1.6552567 -2.0452838 -2.015125 -1.7625058 -0.84494674][-3.8514585 -2.9599388 -2.0740988 -0.62795389 0.89003086 2.3123028 3.0239503 2.8698628 1.856159 -0.05755043 -2.0449684 -2.4368405 -2.4214971 -2.2941468 -1.036898][-3.7860708 -3.2533979 -2.6692812 -1.4338586 -0.21403623 0.98144531 1.6141369 1.3574095 0.26052737 -1.4391655 -2.8988588 -2.96722 -2.6910729 -2.4619625 -1.294319][-4.3079395 -4.0883985 -3.7361479 -2.8575954 -1.8656917 -0.74125934 -0.32763541 -0.71539772 -1.7515128 -2.947814 -3.5755181 -3.1483276 -2.6321034 -2.5061774 -1.5588744][-5.1259527 -5.17958 -5.142478 -4.6063595 -3.7842603 -2.7665055 -2.3786461 -2.697943 -3.3906355 -3.9978037 -3.9242752 -3.2270052 -2.7272365 -2.6728766 -1.6987956][-5.7982693 -5.8634968 -5.9893842 -5.7008829 -5.169795 -4.5127573 -4.171432 -4.1021447 -4.3146915 -4.6068678 -4.2512546 -3.5043674 -3.0127175 -2.9476092 -2.2478383][-5.6704912 -5.4214754 -5.589119 -5.5785637 -5.4567175 -5.3064394 -5.1741953 -4.966341 -4.9544516 -5.0485106 -4.6124754 -3.9733696 -3.6991463 -3.9188268 -3.5416169][-4.4869051 -3.822155 -3.9593577 -4.1882133 -4.5990028 -5.0761189 -5.3297253 -5.3514 -5.3187985 -5.3280106 -4.9056048 -4.5165944 -4.607233 -5.0757518 -4.9415846]]...]
INFO - root - 2017-12-15 07:17:54.907323: step 19610, loss = 0.23, batch loss = 0.20 (36.1 examples/sec; 0.222 sec/batch; 19h:15m:26s remains)
INFO - root - 2017-12-15 07:17:57.191634: step 19620, loss = 0.21, batch loss = 0.18 (33.7 examples/sec; 0.238 sec/batch; 20h:38m:39s remains)
INFO - root - 2017-12-15 07:17:59.476717: step 19630, loss = 0.25, batch loss = 0.21 (35.5 examples/sec; 0.226 sec/batch; 19h:36m:14s remains)
INFO - root - 2017-12-15 07:18:01.742079: step 19640, loss = 0.20, batch loss = 0.17 (34.8 examples/sec; 0.230 sec/batch; 19h:58m:17s remains)
INFO - root - 2017-12-15 07:18:04.045951: step 19650, loss = 0.23, batch loss = 0.20 (34.4 examples/sec; 0.233 sec/batch; 20h:14m:19s remains)
INFO - root - 2017-12-15 07:18:06.339291: step 19660, loss = 0.18, batch loss = 0.15 (34.8 examples/sec; 0.230 sec/batch; 19h:57m:28s remains)
INFO - root - 2017-12-15 07:18:08.660588: step 19670, loss = 0.34, batch loss = 0.31 (34.1 examples/sec; 0.235 sec/batch; 20h:24m:52s remains)
INFO - root - 2017-12-15 07:18:10.922801: step 19680, loss = 0.17, batch loss = 0.14 (35.2 examples/sec; 0.227 sec/batch; 19h:45m:09s remains)
INFO - root - 2017-12-15 07:18:13.190249: step 19690, loss = 0.22, batch loss = 0.19 (34.8 examples/sec; 0.230 sec/batch; 19h:59m:40s remains)
INFO - root - 2017-12-15 07:18:15.507470: step 19700, loss = 0.18, batch loss = 0.15 (34.5 examples/sec; 0.232 sec/batch; 20h:08m:56s remains)
2017-12-15 07:18:15.873376: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.872573 -1.8053241 -2.0782084 -2.1088698 -1.9492071 -2.2095561 -2.5619879 -3.2354898 -4.52878 -5.3038797 -4.9787378 -4.2929192 -3.8986502 -3.5731652 -4.0383396][-2.9326344 -1.5269442 -1.9017181 -2.2022305 -2.0209901 -2.1702924 -2.5803189 -3.4462042 -4.7675176 -5.2846308 -4.8176937 -3.9581358 -3.2411916 -2.7792742 -3.0784473][-3.4356458 -1.1099209 -1.4551016 -1.9001219 -1.6441734 -1.8829012 -2.5399184 -3.7209554 -5.3303146 -5.9329033 -5.6259947 -4.769949 -3.6810241 -2.8474979 -2.5773757][-3.3138466 -0.35958827 -0.72800112 -1.2844739 -0.865132 -1.1055951 -1.9063654 -3.4012024 -5.3426852 -6.2084141 -6.2184982 -5.4549494 -4.1338282 -3.0234103 -2.2919948][-3.5880351 0.12799215 0.052392721 -0.22369885 0.78670192 1.0170937 0.31451702 -1.5543737 -3.9661045 -5.3252692 -5.8804274 -5.4830918 -4.2007504 -2.9522347 -1.9540755][-4.64342 -0.44985342 0.0779624 0.59192014 2.6535935 3.8966994 3.5016437 1.1690216 -1.907068 -4.013751 -5.3874846 -5.6166182 -4.6393428 -3.3544388 -2.1450305][-5.0494261 -0.89176118 0.24110556 1.5941641 4.64472 6.9860487 7.1227779 4.6150079 1.0627034 -1.9204146 -4.2667274 -5.4385633 -5.2423544 -4.2464552 -3.0886488][-5.5620508 -1.6887796 -0.29939926 1.492178 4.8029265 7.7819862 8.5697107 6.5408506 3.2718854 0.058414936 -2.7731581 -4.6785307 -5.4256706 -4.9491091 -4.0532608][-7.1349196 -3.8729463 -2.534631 -0.81875205 1.9424553 4.8758221 6.0972595 4.8180771 2.3645158 -0.24203157 -2.654408 -4.5466084 -5.7721767 -5.6795692 -5.0378265][-8.60113 -6.0194216 -4.7970557 -3.2565026 -1.1223376 1.4098344 2.8646197 2.194829 0.46438003 -1.4867762 -3.2632031 -4.9367132 -6.3530478 -6.4206667 -5.9357233][-8.6762486 -6.7936053 -5.8960142 -4.671526 -3.1277435 -1.2774173 -0.044353962 -0.43172419 -1.6793423 -3.0951548 -4.3868914 -5.89506 -7.2983756 -7.3765888 -6.8868446][-8.5639286 -7.5793862 -7.2865562 -6.5705113 -5.725893 -4.6085482 -3.6888981 -3.8572712 -4.6693091 -5.4746151 -6.1951036 -7.2525005 -8.24102 -8.1578026 -7.5355492][-8.5269623 -7.9692597 -8.1379414 -7.9148607 -7.6192207 -7.07423 -6.4569674 -6.49888 -6.9464927 -7.202405 -7.3631659 -7.897439 -8.3125963 -7.9377651 -7.2317162][-7.9501657 -7.1976123 -7.5614619 -7.6433463 -7.6099815 -7.4272294 -7.1453433 -7.2210712 -7.4782495 -7.4048495 -7.26834 -7.510571 -7.5320535 -7.0102944 -6.4356604][-6.9736671 -5.9246993 -6.3186846 -6.4134316 -6.4162712 -6.4182749 -6.4148574 -6.5360327 -6.7043004 -6.5686831 -6.4129286 -6.5616117 -6.4398155 -5.9703145 -5.5826159]]...]
INFO - root - 2017-12-15 07:18:18.186562: step 19710, loss = 0.27, batch loss = 0.23 (36.3 examples/sec; 0.221 sec/batch; 19h:09m:56s remains)
INFO - root - 2017-12-15 07:18:20.439069: step 19720, loss = 0.24, batch loss = 0.21 (35.0 examples/sec; 0.228 sec/batch; 19h:49m:57s remains)
INFO - root - 2017-12-15 07:18:22.678195: step 19730, loss = 0.24, batch loss = 0.21 (35.6 examples/sec; 0.225 sec/batch; 19h:31m:21s remains)
INFO - root - 2017-12-15 07:18:25.018799: step 19740, loss = 0.31, batch loss = 0.27 (35.2 examples/sec; 0.227 sec/batch; 19h:45m:40s remains)
INFO - root - 2017-12-15 07:18:27.313515: step 19750, loss = 0.17, batch loss = 0.14 (34.4 examples/sec; 0.232 sec/batch; 20h:11m:43s remains)
INFO - root - 2017-12-15 07:18:29.584992: step 19760, loss = 0.16, batch loss = 0.13 (34.4 examples/sec; 0.233 sec/batch; 20h:12m:21s remains)
INFO - root - 2017-12-15 07:18:31.867111: step 19770, loss = 0.27, batch loss = 0.24 (34.9 examples/sec; 0.229 sec/batch; 19h:55m:46s remains)
INFO - root - 2017-12-15 07:18:34.131137: step 19780, loss = 0.48, batch loss = 0.45 (36.0 examples/sec; 0.222 sec/batch; 19h:19m:26s remains)
INFO - root - 2017-12-15 07:18:36.442419: step 19790, loss = 0.26, batch loss = 0.23 (34.8 examples/sec; 0.230 sec/batch; 19h:59m:28s remains)
INFO - root - 2017-12-15 07:18:38.742381: step 19800, loss = 0.30, batch loss = 0.27 (34.2 examples/sec; 0.234 sec/batch; 20h:19m:55s remains)
2017-12-15 07:18:39.068000: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3896625 -2.7696488 -3.40663 -4.3849049 -5.0600491 -6.144784 -7.31848 -7.7913108 -7.73827 -7.5938416 -7.2331038 -6.314827 -5.3626137 -4.3541393 -3.7871792][-3.8513882 -3.9439809 -4.6971111 -5.530098 -6.047349 -6.8527741 -7.5828786 -8.009223 -8.1274929 -7.8622561 -7.0129795 -5.7802548 -4.6323748 -3.2991781 -2.1947143][-5.578248 -5.0643282 -5.6211343 -6.0368929 -6.16626 -6.3705711 -6.4853177 -6.7577147 -7.1488323 -7.2338333 -6.6404195 -5.5804119 -4.3826933 -2.5274153 -0.72846007][-7.0519691 -6.2167645 -6.3393 -6.1770229 -5.7315664 -5.0435457 -4.2864642 -4.2497463 -4.8049741 -5.3017917 -5.2514534 -4.7936139 -3.9054956 -1.994565 0.24496651][-8.2286129 -6.7913408 -6.3464675 -5.6666865 -4.6857653 -3.3377693 -2.0622766 -1.5955895 -1.8004228 -2.3166351 -2.8548684 -3.1646085 -2.8711765 -1.4375956 0.6466012][-8.1250725 -6.3730321 -5.6052303 -4.6858845 -3.3341358 -1.546607 -0.026905537 0.73476982 0.78922963 0.066153765 -1.0690247 -1.8908259 -1.9783018 -1.1284587 0.46914315][-7.268096 -5.805943 -4.7119064 -3.5500336 -1.8277754 0.35563159 2.1116061 3.1095252 3.0574331 1.7752047 -0.031080008 -1.2953496 -1.6498923 -1.2585416 -0.1192224][-7.0503159 -5.6221828 -4.2778764 -2.9234874 -0.93604457 1.5327983 3.5099115 4.7393608 4.5220656 2.6937737 0.5466454 -1.1218141 -1.7823715 -1.7725309 -0.89966846][-7.3085566 -6.1694522 -4.8284316 -3.3704896 -1.3399149 1.1917005 3.2378931 4.5455017 4.2338934 2.3062577 0.37647367 -1.3421032 -2.2447145 -2.2910612 -1.5156269][-7.7983589 -7.0386915 -5.9438763 -4.684845 -2.8808064 -0.42295682 1.7041597 3.125639 2.8678794 1.1042106 -0.37953961 -1.7479476 -2.5933161 -2.6056223 -1.9841673][-8.2846527 -7.8783331 -7.0799055 -6.0797644 -4.6676621 -2.5503464 -0.71823835 0.452672 0.085211515 -1.5572979 -2.6251776 -3.3990333 -3.6817622 -3.2497783 -2.5083265][-8.5533161 -8.5138092 -8.3270893 -7.7073116 -6.6635361 -4.8588991 -3.3762522 -2.3545523 -2.5527709 -3.8106258 -4.7025862 -5.0029354 -4.6524906 -3.8521125 -3.0734558][-8.26561 -8.4994535 -9.004528 -8.8919182 -8.1350842 -6.5232139 -5.1130219 -4.0672503 -3.9432592 -4.8465409 -5.5385828 -5.39952 -4.6074195 -3.8288476 -3.3484263][-7.3787541 -7.5304585 -8.4525852 -8.8391609 -8.5515938 -7.312459 -6.116003 -5.0327616 -4.626544 -5.230238 -5.6687403 -5.2462168 -4.3383126 -3.6807628 -3.3539727][-6.7511926 -6.4620509 -7.424798 -8.0964108 -8.134676 -7.2126985 -6.26353 -5.2689228 -4.8097219 -5.3765826 -5.7253942 -5.1245084 -4.2206039 -3.4143271 -2.8691778]]...]
INFO - root - 2017-12-15 07:18:41.308849: step 19810, loss = 0.26, batch loss = 0.22 (36.2 examples/sec; 0.221 sec/batch; 19h:11m:05s remains)
INFO - root - 2017-12-15 07:18:43.577310: step 19820, loss = 0.20, batch loss = 0.17 (36.0 examples/sec; 0.222 sec/batch; 19h:17m:51s remains)
INFO - root - 2017-12-15 07:18:45.840463: step 19830, loss = 0.20, batch loss = 0.17 (36.2 examples/sec; 0.221 sec/batch; 19h:10m:19s remains)
INFO - root - 2017-12-15 07:18:48.101481: step 19840, loss = 0.24, batch loss = 0.20 (34.9 examples/sec; 0.229 sec/batch; 19h:52m:52s remains)
INFO - root - 2017-12-15 07:18:50.345812: step 19850, loss = 0.22, batch loss = 0.19 (36.1 examples/sec; 0.222 sec/batch; 19h:14m:19s remains)
INFO - root - 2017-12-15 07:18:52.620117: step 19860, loss = 0.21, batch loss = 0.18 (34.6 examples/sec; 0.231 sec/batch; 20h:04m:52s remains)
INFO - root - 2017-12-15 07:18:54.899362: step 19870, loss = 0.24, batch loss = 0.21 (36.0 examples/sec; 0.222 sec/batch; 19h:16m:43s remains)
INFO - root - 2017-12-15 07:18:57.152953: step 19880, loss = 0.20, batch loss = 0.16 (35.0 examples/sec; 0.228 sec/batch; 19h:49m:49s remains)
INFO - root - 2017-12-15 07:18:59.456236: step 19890, loss = 0.45, batch loss = 0.42 (35.3 examples/sec; 0.227 sec/batch; 19h:42m:17s remains)
INFO - root - 2017-12-15 07:19:01.694804: step 19900, loss = 0.32, batch loss = 0.28 (36.1 examples/sec; 0.222 sec/batch; 19h:15m:04s remains)
2017-12-15 07:19:02.070759: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9442697 -1.9978657 -1.1758671 -0.84730375 -0.8458612 -1.3146393 -2.0891397 -2.4458148 -2.6053309 -2.7332594 -3.1531043 -3.4259524 -3.0852766 -2.3790545 -1.8659569][-3.8180583 -2.1201792 -1.3161032 -1.1098089 -1.2245984 -1.781618 -2.3963566 -2.4129319 -2.3444626 -2.5495253 -2.9942372 -3.1202524 -2.5274706 -1.5532434 -1.0497222][-5.0483074 -2.6481655 -1.9932983 -2.0426857 -2.3308198 -2.7154908 -2.797004 -2.2815251 -2.0005043 -2.2723773 -2.5472248 -2.3263788 -1.4730678 -0.27850187 0.32322335][-5.7445593 -3.1269605 -2.7559547 -3.1798825 -3.6020308 -3.623158 -3.1612453 -2.2272704 -1.6203663 -1.5983616 -1.5379301 -1.0553691 -0.028286457 1.2260814 1.816931][-5.5499716 -3.1456966 -3.0548694 -3.7157736 -4.0737848 -3.601243 -2.6449602 -1.3798249 -0.35922647 -0.023695707 0.26129889 0.8107121 1.7365482 2.8094375 3.1773417][-5.0158119 -2.6576228 -2.7261 -3.3908291 -3.5670128 -2.7274942 -1.4687873 -0.024461031 1.207073 1.689549 1.8261077 2.085871 2.671221 3.3572609 3.3089821][-4.1502433 -2.4184213 -2.2968128 -2.545388 -2.3548985 -1.2148165 0.20576167 1.6919069 2.8535211 3.0575392 2.7410448 2.3578575 2.3824074 2.5010879 1.9469416][-4.0977273 -2.285197 -1.8792444 -1.6249251 -1.2042047 -0.038117647 1.2934492 2.6055262 3.4937031 3.3969166 2.76109 1.945626 1.5624907 1.3459632 0.47264576][-4.0703177 -2.4551783 -1.8813834 -1.2973301 -0.82525909 0.015353441 0.96450019 1.91168 2.5272043 2.2473872 1.5525444 0.65854144 0.1942122 -0.12963653 -1.0634668][-4.3307986 -2.9122384 -2.3242271 -1.5397494 -1.0911849 -0.64825535 -0.19520569 0.36491108 0.6649425 0.37205911 -0.17622137 -0.96909821 -1.3088065 -1.4779084 -2.1794481][-4.9037094 -3.6185498 -3.1031847 -2.3312132 -1.9549859 -1.8380116 -1.9005919 -1.7620345 -1.7047158 -1.8806412 -2.1281369 -2.6180577 -2.681535 -2.6410227 -3.156261][-5.1761041 -3.85534 -3.4790409 -2.9651489 -2.8386393 -3.0373871 -3.47731 -3.57464 -3.6242597 -3.5611715 -3.5066311 -3.6721146 -3.4823964 -3.2946291 -3.6877575][-5.0492506 -3.7609367 -3.6933353 -3.6078053 -3.7111788 -4.0158467 -4.4915891 -4.6107664 -4.5608196 -4.3001537 -4.1376719 -4.1333027 -3.8642468 -3.7214084 -4.0785289][-5.1008711 -4.029995 -4.3073463 -4.5770569 -4.8500748 -5.1425829 -5.4767485 -5.3952446 -5.1759191 -4.8458424 -4.6867056 -4.5785093 -4.3449326 -4.2881584 -4.5675831][-5.0107241 -4.2094674 -4.6502333 -5.0753164 -5.3270683 -5.4544806 -5.4460392 -5.14992 -4.9131122 -4.7771626 -4.7205381 -4.6167212 -4.5373731 -4.6052928 -4.8202705]]...]
INFO - root - 2017-12-15 07:19:04.351943: step 19910, loss = 0.31, batch loss = 0.28 (35.4 examples/sec; 0.226 sec/batch; 19h:38m:12s remains)
INFO - root - 2017-12-15 07:19:06.661379: step 19920, loss = 0.17, batch loss = 0.13 (33.5 examples/sec; 0.239 sec/batch; 20h:44m:35s remains)
INFO - root - 2017-12-15 07:19:08.924658: step 19930, loss = 0.22, batch loss = 0.19 (35.5 examples/sec; 0.226 sec/batch; 19h:35m:15s remains)
INFO - root - 2017-12-15 07:19:11.188861: step 19940, loss = 0.49, batch loss = 0.46 (34.9 examples/sec; 0.229 sec/batch; 19h:54m:00s remains)
INFO - root - 2017-12-15 07:19:13.478436: step 19950, loss = 0.23, batch loss = 0.20 (34.8 examples/sec; 0.230 sec/batch; 19h:57m:07s remains)
INFO - root - 2017-12-15 07:19:15.770924: step 19960, loss = 0.26, batch loss = 0.22 (36.3 examples/sec; 0.220 sec/batch; 19h:08m:28s remains)
INFO - root - 2017-12-15 07:19:18.059451: step 19970, loss = 0.21, batch loss = 0.17 (33.2 examples/sec; 0.241 sec/batch; 20h:53m:36s remains)
INFO - root - 2017-12-15 07:19:20.312854: step 19980, loss = 0.19, batch loss = 0.16 (35.7 examples/sec; 0.224 sec/batch; 19h:27m:17s remains)
INFO - root - 2017-12-15 07:19:22.646746: step 19990, loss = 0.21, batch loss = 0.18 (34.2 examples/sec; 0.234 sec/batch; 20h:19m:55s remains)
INFO - root - 2017-12-15 07:19:24.925206: step 20000, loss = 0.24, batch loss = 0.21 (36.0 examples/sec; 0.222 sec/batch; 19h:18m:18s remains)
2017-12-15 07:19:25.283277: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7877145 -6.1059008 -5.7628489 -5.6461287 -5.8623562 -6.2004938 -6.3341737 -5.9736166 -5.7150068 -5.8053474 -5.9046006 -5.9529915 -6.1414509 -6.4107265 -6.4871192][-6.7041845 -6.6011372 -6.2838259 -6.3093324 -6.7642264 -7.2425714 -7.4581952 -7.0193238 -6.7988162 -7.0338149 -7.2776623 -7.4273796 -7.6035776 -7.7117691 -7.5943284][-7.682373 -6.3664327 -6.0713406 -6.2400475 -6.7655129 -7.0590229 -7.13295 -6.5654874 -6.3897657 -6.8823075 -7.5649157 -8.1332922 -8.4527721 -8.4664841 -8.3056259][-8.6528664 -6.4272895 -6.1517005 -6.2516642 -6.4523025 -6.0824318 -5.5571404 -4.6516795 -4.527174 -5.4571877 -6.8076086 -8.0009022 -8.5894852 -8.6949539 -8.7015924][-8.8443871 -6.1067424 -5.8167467 -5.6408472 -5.2806168 -4.1539907 -2.8412049 -1.4184988 -1.3693051 -2.805681 -4.8162622 -6.5841393 -7.387991 -7.7811966 -8.3628044][-7.7173247 -5.1773567 -4.8975716 -4.3125725 -3.3290102 -1.6274008 0.42044187 2.29536 2.0798531 0.063587666 -2.4163306 -4.6035166 -5.5718403 -6.283493 -7.4106574][-6.9135275 -4.9421086 -4.5147753 -3.2975864 -1.7040842 0.48125768 3.2953153 5.5918922 4.9990435 2.3693218 -0.46861911 -2.8698242 -3.9756713 -5.017755 -6.4216046][-7.1763763 -5.2336164 -4.4886546 -2.5421388 -0.45354021 2.0508609 5.468483 8.1831455 7.1901641 4.1724253 1.2032316 -1.3780417 -2.8911672 -4.2748961 -5.9594507][-7.679481 -5.5239129 -4.4435191 -2.1125312 -0.010706186 2.1657009 5.3030758 7.7220159 6.3288302 3.2893162 0.65908861 -1.7763758 -3.5284135 -4.9030156 -6.3707085][-9.2247629 -6.8508973 -5.4641495 -3.0453792 -1.1533482 0.47102189 2.7160807 4.3784409 2.7748713 0.22239494 -1.6984267 -3.723356 -5.46469 -6.3958383 -7.2567964][-10.909983 -8.4039173 -6.69628 -4.2667 -2.5058455 -1.3104062 -0.093866348 0.94605255 -0.43783891 -2.2578661 -3.645381 -5.4415751 -7.1787195 -7.7128849 -8.1766453][-11.763046 -9.30661 -7.6916232 -5.7644739 -4.4274635 -3.8928094 -3.6917384 -3.1543455 -4.0285597 -5.0465021 -5.874279 -7.2568173 -8.664669 -8.82526 -8.8706017][-12.200813 -9.843257 -8.4927311 -7.08962 -6.2421846 -6.3383131 -6.8261347 -6.5666575 -6.7920184 -7.0879331 -7.5119491 -8.4701862 -9.3275509 -9.21027 -9.0057926][-11.838404 -9.5050077 -8.3375015 -7.3013644 -6.8674035 -7.3062458 -8.0448637 -7.850738 -7.6407027 -7.6053438 -8.0463448 -8.8302879 -9.3314533 -9.101758 -8.8060246][-10.054668 -7.9160209 -7.0654221 -6.4106555 -6.3375449 -6.9150066 -7.5525527 -7.3505068 -7.0129781 -7.0100331 -7.4980426 -8.0468979 -8.2581644 -8.06855 -7.7738523]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 07:19:28.157713: step 20010, loss = 0.21, batch loss = 0.17 (34.6 examples/sec; 0.231 sec/batch; 20h:02m:58s remains)
INFO - root - 2017-12-15 07:19:30.395581: step 20020, loss = 0.18, batch loss = 0.15 (35.9 examples/sec; 0.223 sec/batch; 19h:20m:05s remains)
INFO - root - 2017-12-15 07:19:32.644196: step 20030, loss = 0.24, batch loss = 0.20 (36.4 examples/sec; 0.220 sec/batch; 19h:05m:09s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 07:19:34.906281: step 20040, loss = 0.24, batch loss = 0.20 (36.1 examples/sec; 0.221 sec/batch; 19h:13m:19s remains)
INFO - root - 2017-12-15 07:19:37.181660: step 20050, loss = 0.39, batch loss = 0.35 (35.8 examples/sec; 0.224 sec/batch; 19h:24m:19s remains)
INFO - root - 2017-12-15 07:19:39.455532: step 20060, loss = 0.15, batch loss = 0.12 (34.5 examples/sec; 0.232 sec/batch; 20h:06m:05s remains)
INFO - root - 2017-12-15 07:19:41.740165: step 20070, loss = 0.30, batch loss = 0.27 (34.8 examples/sec; 0.230 sec/batch; 19h:58m:40s remains)
INFO - root - 2017-12-15 07:19:44.020287: step 20080, loss = 0.36, batch loss = 0.32 (33.8 examples/sec; 0.236 sec/batch; 20h:30m:48s remains)
INFO - root - 2017-12-15 07:19:46.294835: step 20090, loss = 0.24, batch loss = 0.21 (35.5 examples/sec; 0.225 sec/batch; 19h:33m:29s remains)
INFO - root - 2017-12-15 07:19:48.562895: step 20100, loss = 0.27, batch loss = 0.24 (35.3 examples/sec; 0.226 sec/batch; 19h:39m:01s remains)
2017-12-15 07:19:48.902843: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9200592 -5.0695829 -5.0713859 -5.4471359 -5.8663912 -5.8684568 -5.8362255 -5.6632934 -5.4211764 -5.3435483 -5.7704 -5.8709846 -5.529912 -5.3836584 -4.6676884][-5.7214336 -5.4577589 -5.31017 -5.8221774 -6.5350952 -6.4715691 -6.3200331 -6.0611539 -5.8098826 -6.0063419 -6.902235 -7.185955 -6.6527061 -6.3711634 -5.3891621][-6.8316631 -5.279355 -5.0153189 -5.5565472 -6.5024328 -6.3468752 -6.0699854 -5.7724881 -5.6447282 -6.2476397 -7.6065378 -8.0602226 -7.4528646 -7.0988808 -5.8571587][-7.3140059 -4.9892864 -4.7294121 -4.8975148 -5.6995478 -5.427393 -5.1754875 -4.994998 -5.02967 -6.0789652 -7.9500084 -8.8451748 -8.3683634 -7.9918175 -6.5581436][-7.4616766 -4.3303351 -4.1473675 -3.8034346 -4.1428652 -3.6547709 -3.5737562 -3.5428391 -3.8264532 -5.307373 -7.4799719 -8.7368517 -8.4424047 -8.1124458 -6.7926636][-6.639451 -3.1222413 -3.3237934 -2.8139148 -2.9022722 -2.2648733 -2.1374023 -2.0626194 -2.3894334 -4.0057917 -5.9975681 -7.2156496 -7.1635952 -7.0232811 -6.1217289][-5.1543527 -2.024425 -2.6764894 -2.3625286 -2.3067148 -1.4025531 -0.99641526 -0.83599281 -1.3200551 -3.0367353 -4.8270569 -5.9018788 -6.060513 -5.9958134 -5.3058438][-4.8643503 -1.2897658 -2.0009165 -1.7132992 -1.3224635 0.041618109 0.7095921 0.76536059 -0.12718153 -2.1418359 -4.129982 -5.2187448 -5.3768106 -5.3209386 -4.8561974][-4.1163988 -0.13825631 -0.87360537 -0.90203488 -0.48772264 0.94730043 1.6222632 1.5451179 0.4972384 -1.5196395 -3.6793356 -4.6592674 -4.7975368 -4.9673367 -4.8907309][-3.2960353 0.62365961 -0.34960437 -0.81663954 -0.62156868 0.45923591 1.045959 0.9456706 0.28737235 -1.3567877 -3.6037278 -4.5944362 -4.8852186 -5.3482518 -5.4017816][-3.3902166 0.55447674 -0.26542342 -0.574155 -0.27231288 0.63222218 1.1179855 0.90292931 0.26020145 -1.3598474 -3.9838676 -5.3172693 -5.840312 -6.4409637 -6.2874107][-4.221961 -0.1721139 -0.48957086 -0.42777359 0.024227619 0.91565204 1.3434761 0.97204375 0.25769353 -1.4003878 -4.2797656 -5.8511672 -6.4922638 -7.1482449 -6.8359394][-6.1008778 -2.1986194 -2.1898534 -1.9359238 -1.3899267 -0.51475275 0.048794508 -0.077818155 -0.42257237 -1.8661562 -4.44079 -5.921896 -6.6912575 -7.5389471 -7.1563673][-7.8876505 -4.4633493 -4.31443 -3.868834 -3.2264414 -2.1976669 -1.2924635 -1.0309291 -1.1361002 -2.5358744 -4.8157997 -6.1391487 -6.9621286 -7.8060532 -7.2773552][-8.4715185 -5.7584419 -5.7116275 -5.3466368 -4.773798 -3.6808486 -2.736295 -2.4532626 -2.6784377 -3.9913402 -5.7586346 -6.7029514 -7.2449207 -7.7049952 -7.0114]]...]
INFO - root - 2017-12-15 07:19:51.162652: step 20110, loss = 0.19, batch loss = 0.15 (35.8 examples/sec; 0.223 sec/batch; 19h:22m:28s remains)
INFO - root - 2017-12-15 07:19:53.430953: step 20120, loss = 0.36, batch loss = 0.32 (35.8 examples/sec; 0.223 sec/batch; 19h:22m:16s remains)
INFO - root - 2017-12-15 07:19:55.696928: step 20130, loss = 0.19, batch loss = 0.16 (35.7 examples/sec; 0.224 sec/batch; 19h:25m:01s remains)
INFO - root - 2017-12-15 07:19:57.944525: step 20140, loss = 0.26, batch loss = 0.22 (35.4 examples/sec; 0.226 sec/batch; 19h:34m:54s remains)
INFO - root - 2017-12-15 07:20:00.216022: step 20150, loss = 0.20, batch loss = 0.16 (36.4 examples/sec; 0.220 sec/batch; 19h:04m:25s remains)
INFO - root - 2017-12-15 07:20:02.515669: step 20160, loss = 0.18, batch loss = 0.14 (33.4 examples/sec; 0.239 sec/batch; 20h:45m:38s remains)
INFO - root - 2017-12-15 07:20:04.766021: step 20170, loss = 0.24, batch loss = 0.21 (35.1 examples/sec; 0.228 sec/batch; 19h:47m:22s remains)
INFO - root - 2017-12-15 07:20:07.019210: step 20180, loss = 0.20, batch loss = 0.17 (35.3 examples/sec; 0.227 sec/batch; 19h:39m:36s remains)
INFO - root - 2017-12-15 07:20:09.283189: step 20190, loss = 0.21, batch loss = 0.17 (36.1 examples/sec; 0.221 sec/batch; 19h:12m:49s remains)
INFO - root - 2017-12-15 07:20:11.541839: step 20200, loss = 0.24, batch loss = 0.21 (36.0 examples/sec; 0.222 sec/batch; 19h:16m:20s remains)
2017-12-15 07:20:11.833443: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0525675 -4.4936447 -4.9725714 -6.3449965 -6.2312365 -4.9123011 -3.7461853 -2.6065421 -1.8497447 -2.0331504 -2.7767353 -3.1315885 -3.12887 -3.1236048 -2.7262981][-4.8576889 -5.4964423 -6.0771351 -7.133863 -6.9335246 -5.743053 -4.6447544 -3.5585189 -2.996841 -3.3357141 -3.915096 -4.191288 -4.1102891 -4.0256395 -3.6573896][-5.3222919 -5.5599251 -6.124238 -6.7489672 -6.430419 -5.5259018 -4.7313337 -4.2686892 -4.3049145 -4.8990445 -5.4935656 -5.6446476 -5.3905931 -4.9901586 -4.3325081][-5.4997258 -4.8316054 -5.2909689 -5.4799423 -5.0581 -4.1688328 -3.5204787 -3.5867095 -4.2671766 -5.3250666 -6.2945004 -6.5675864 -6.2273417 -5.7131605 -4.8424759][-6.0645485 -4.1182761 -4.10079 -3.7130909 -2.7437611 -1.5208333 -0.48838842 -0.70664454 -1.9917016 -3.5692554 -5.1573915 -6.0988426 -6.201263 -6.0535154 -5.4080725][-5.6858974 -3.5421283 -3.1367664 -2.2479341 -0.50531006 1.5261922 3.2742081 3.18964 1.4114859 -0.63524485 -2.7923818 -4.4066963 -5.0777769 -5.5709434 -5.3506708][-5.1115055 -2.9844437 -2.5862637 -1.7618124 0.41239858 3.224432 5.7816777 6.2550673 4.2274346 1.8887691 -0.61160243 -2.5602779 -3.4388552 -4.2663155 -4.3285761][-5.7365451 -3.132941 -2.8126369 -2.2465336 0.066411972 3.085959 6.0631328 7.2315745 5.5467305 3.3286667 0.91359138 -0.90924585 -1.7261459 -2.4887276 -2.4508793][-5.3313155 -2.8147583 -2.8395829 -2.7580619 -0.84821749 1.8331761 4.45159 5.8367391 4.7172189 3.0059013 1.1926134 -0.093317747 -0.75919724 -1.1365949 -0.64962161][-3.9531608 -1.6644655 -2.2017245 -2.9467268 -1.9648438 -0.17002201 1.5295944 2.4756703 1.7538249 0.616081 -0.22401905 -0.76911438 -0.95520878 -0.72268283 0.25270224][-2.53327 -0.44021392 -1.2933176 -2.6738 -2.7010019 -2.0390835 -1.3731267 -0.92876577 -1.2853299 -1.8855014 -2.1354423 -1.9543808 -1.6082973 -0.94334495 0.11984205][-1.4865025 0.53742194 -0.34638083 -2.0164742 -2.5706463 -2.6689727 -2.7248092 -2.8075726 -2.810396 -3.0097008 -2.9770126 -2.5875573 -2.0949318 -1.21194 -0.3189894][-1.8063966 0.64545751 0.056871891 -1.5588899 -2.1463709 -2.6030807 -2.8874233 -3.1780672 -3.0851183 -3.2838254 -3.2682395 -2.9748285 -2.6276627 -1.8188856 -1.2570897][-3.0337176 -0.17231345 -0.19886518 -1.4087197 -1.6359959 -2.1572309 -2.3409264 -2.6688938 -2.7330117 -3.0874536 -3.3994017 -3.2556028 -3.1325078 -2.5859768 -2.38826][-4.622674 -1.8010894 -1.6166176 -2.3974941 -2.0586493 -2.1616733 -2.026536 -2.2790029 -2.307456 -2.8155503 -3.3962607 -3.474586 -3.5098755 -3.1842427 -3.2224813]]...]
INFO - root - 2017-12-15 07:20:14.054459: step 20210, loss = 0.31, batch loss = 0.28 (36.3 examples/sec; 0.220 sec/batch; 19h:06m:39s remains)
INFO - root - 2017-12-15 07:20:16.286568: step 20220, loss = 0.26, batch loss = 0.23 (36.2 examples/sec; 0.221 sec/batch; 19h:11m:17s remains)
INFO - root - 2017-12-15 07:20:18.516704: step 20230, loss = 0.17, batch loss = 0.14 (35.0 examples/sec; 0.229 sec/batch; 19h:50m:20s remains)
INFO - root - 2017-12-15 07:20:20.749219: step 20240, loss = 0.30, batch loss = 0.27 (34.9 examples/sec; 0.229 sec/batch; 19h:53m:09s remains)
INFO - root - 2017-12-15 07:20:23.005907: step 20250, loss = 0.24, batch loss = 0.20 (34.7 examples/sec; 0.230 sec/batch; 19h:59m:17s remains)
INFO - root - 2017-12-15 07:20:25.231292: step 20260, loss = 0.30, batch loss = 0.27 (37.0 examples/sec; 0.216 sec/batch; 18h:43m:45s remains)
INFO - root - 2017-12-15 07:20:27.487138: step 20270, loss = 0.33, batch loss = 0.29 (36.6 examples/sec; 0.219 sec/batch; 18h:58m:43s remains)
INFO - root - 2017-12-15 07:20:29.730822: step 20280, loss = 0.30, batch loss = 0.27 (35.2 examples/sec; 0.227 sec/batch; 19h:43m:23s remains)
INFO - root - 2017-12-15 07:20:31.987202: step 20290, loss = 0.22, batch loss = 0.19 (36.3 examples/sec; 0.220 sec/batch; 19h:06m:44s remains)
INFO - root - 2017-12-15 07:20:34.219196: step 20300, loss = 0.32, batch loss = 0.29 (35.7 examples/sec; 0.224 sec/batch; 19h:25m:26s remains)
2017-12-15 07:20:34.482713: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6822698 -3.1723528 -3.4047494 -3.3700638 -2.7758796 -2.2680042 -1.8528893 -1.6469671 -1.9874096 -2.286957 -3.0055223 -3.6531749 -3.4295986 -3.1112638 -2.9241071][-3.1548762 -3.6221681 -3.9476538 -4.1774955 -3.8304653 -3.5431688 -3.124526 -2.8649464 -3.0538781 -3.1972933 -3.6858854 -4.2706404 -4.0915775 -3.6708441 -3.4264674][-3.4439449 -3.6970477 -4.1885185 -4.563448 -4.4062972 -4.4212818 -4.16308 -4.0422359 -4.2190709 -4.3578882 -4.6969738 -5.2744637 -5.0810275 -4.3921061 -3.9167104][-3.4047952 -3.1659646 -3.6117852 -3.8624864 -3.733716 -3.9037442 -3.8499584 -3.951889 -4.1978602 -4.415235 -4.6976008 -5.2722731 -5.2384253 -4.5567703 -4.1084552][-2.5626934 -2.1144125 -2.3899996 -2.3218696 -1.9833106 -2.04553 -1.9005858 -1.8990715 -2.190397 -2.6721544 -3.1607003 -4.003437 -4.4170275 -4.1386404 -3.8663316][-2.32113 -1.4180374 -1.4183209 -1.0078319 -0.44414032 -0.23942399 0.12897038 0.39105105 0.29569817 -0.35340059 -1.167205 -2.2577045 -3.1076794 -3.4139228 -3.4805832][-1.5769162 -0.65522766 -0.52014124 -0.060092449 0.48587132 0.78024769 1.3551347 2.0148444 2.4502897 2.1238155 1.3097203 0.070073605 -1.2208221 -2.3098533 -2.9927313][-1.8928177 -0.85051429 -0.54471576 0.025430202 0.58901286 1.0604224 1.8390431 2.7078238 3.6388659 4.01857 3.6921463 2.61588 1.0451691 -0.55689955 -1.7238729][-3.0829437 -2.046181 -1.6793371 -1.072631 -0.43638051 0.11322665 0.83711791 1.4657652 2.3524847 3.1699038 3.5630074 3.0669465 1.8811793 0.34914613 -0.98486412][-4.2598763 -3.3404131 -3.0696974 -2.4927566 -1.7762651 -1.1133207 -0.30378234 0.1981132 0.98447371 2.0439725 3.0245819 3.1628642 2.3895969 1.1807606 -0.11455894][-5.0172739 -3.9514537 -3.6875429 -3.1997802 -2.4469535 -1.7806468 -0.94931161 -0.49616671 0.1583004 1.0201614 2.0607419 2.4811864 2.0511551 1.2142072 0.050487995][-5.3114557 -4.1238613 -3.9750614 -3.6849797 -3.1251216 -2.7222447 -2.1575804 -1.774541 -1.2635462 -0.6234442 0.34445405 1.010397 0.97735977 0.5833199 -0.23865151][-5.5495715 -4.3482728 -4.3741241 -4.3932896 -4.0718718 -3.7610583 -3.3300445 -2.9789708 -2.4297476 -1.7780778 -0.82928336 0.013280392 0.34978962 0.35980773 -0.19652724][-5.6016178 -4.3152065 -4.4507504 -4.6761885 -4.6336031 -4.57847 -4.353447 -4.0721188 -3.6597326 -3.2549012 -2.5893414 -1.8004813 -1.2593479 -0.93401015 -1.1110275][-6.0187888 -4.7303057 -4.7800179 -5.0588541 -5.27906 -5.523232 -5.5423021 -5.4415197 -5.1933832 -4.9620709 -4.5084057 -3.8616462 -3.3573768 -3.0133495 -2.9615898]]...]
INFO - root - 2017-12-15 07:20:36.731317: step 20310, loss = 0.21, batch loss = 0.18 (36.0 examples/sec; 0.222 sec/batch; 19h:17m:26s remains)
INFO - root - 2017-12-15 07:20:38.997162: step 20320, loss = 0.19, batch loss = 0.16 (34.4 examples/sec; 0.232 sec/batch; 20h:09m:30s remains)
INFO - root - 2017-12-15 07:20:41.279838: step 20330, loss = 0.23, batch loss = 0.20 (35.9 examples/sec; 0.223 sec/batch; 19h:20m:04s remains)
INFO - root - 2017-12-15 07:20:43.518438: step 20340, loss = 0.19, batch loss = 0.16 (34.9 examples/sec; 0.229 sec/batch; 19h:51m:35s remains)
INFO - root - 2017-12-15 07:20:45.766897: step 20350, loss = 0.17, batch loss = 0.13 (34.9 examples/sec; 0.229 sec/batch; 19h:51m:05s remains)
INFO - root - 2017-12-15 07:20:48.040076: step 20360, loss = 0.18, batch loss = 0.15 (33.5 examples/sec; 0.239 sec/batch; 20h:41m:05s remains)
INFO - root - 2017-12-15 07:20:50.311824: step 20370, loss = 0.32, batch loss = 0.29 (35.2 examples/sec; 0.227 sec/batch; 19h:41m:56s remains)
INFO - root - 2017-12-15 07:20:52.567001: step 20380, loss = 0.23, batch loss = 0.20 (35.1 examples/sec; 0.228 sec/batch; 19h:45m:52s remains)
INFO - root - 2017-12-15 07:20:54.836171: step 20390, loss = 0.25, batch loss = 0.22 (34.7 examples/sec; 0.231 sec/batch; 19h:59m:50s remains)
INFO - root - 2017-12-15 07:20:57.107143: step 20400, loss = 0.41, batch loss = 0.37 (35.6 examples/sec; 0.225 sec/batch; 19h:29m:21s remains)
2017-12-15 07:20:57.418757: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.95906663 0.64936876 0.23208809 -0.20017362 -0.55070305 -0.43945515 -0.42174613 -0.66200268 -0.64845312 -0.72914886 -0.87389994 -0.88011646 -0.74290919 -0.84677148 -1.0700306][0.89305019 1.5660105 1.4597166 1.2119305 0.80261993 0.86364484 0.87605548 0.68180394 0.78173852 0.89255977 0.71199608 0.45848703 0.23044586 -0.39329422 -1.1321337][-0.49968922 0.95060086 1.0643747 1.0704739 0.80738878 0.91264009 1.0316577 1.2185779 1.7728491 2.3132238 2.1812944 1.6380537 0.83597112 -0.35276353 -1.501662][-1.9899309 0.76804042 1.1511204 1.277091 1.0670028 1.0900886 1.141464 1.6661692 2.5805588 3.5494361 3.5899849 2.8188844 1.4798982 -0.27654266 -1.8018652][-3.2236564 0.14168715 0.55760217 0.59944654 0.43622613 0.46502852 0.7489872 1.8206401 3.2822981 4.6942029 4.9042244 4.0121055 2.2935405 0.056796074 -1.9141744][-4.7327266 -1.7236289 -1.4084399 -1.4884079 -1.5785034 -1.349921 -0.63858986 1.0605969 2.9812784 4.6779156 5.0105562 4.1098938 2.2870989 -0.17414951 -2.4477513][-5.670063 -3.549159 -3.3217204 -3.4319644 -3.3691583 -2.9086127 -1.9222206 0.041171551 2.1184607 3.8335447 4.1506677 3.256237 1.4439149 -1.0518932 -3.3850734][-6.8522506 -5.0631647 -4.9255648 -4.940876 -4.6458845 -3.9688137 -2.7651079 -0.67439795 1.3503928 2.8905816 3.0232677 1.9420652 0.12675762 -2.2318983 -4.360404][-6.7259059 -5.5624962 -5.59818 -5.5550747 -5.1881065 -4.5640211 -3.4373617 -1.5758348 0.19475317 1.3466077 1.232003 0.19684052 -1.3255163 -3.2548673 -4.938962][-5.4039125 -4.8391337 -5.1098938 -5.2538118 -5.0522575 -4.6935396 -3.9074531 -2.508903 -1.313078 -0.69102943 -0.99739873 -1.8622508 -2.8673737 -4.049859 -5.0262203][-3.4808469 -3.488122 -4.1075754 -4.5613637 -4.7256932 -4.8729582 -4.6058893 -3.8113728 -3.1821709 -2.9256027 -3.2349372 -3.7341099 -4.1454449 -4.5956254 -4.8658915][-2.354702 -2.5732756 -3.298631 -3.9017441 -4.3029079 -4.8014851 -4.9198093 -4.5012112 -4.1353736 -4.021081 -4.2216334 -4.4502525 -4.5017786 -4.4989214 -4.3337426][-1.86985 -2.0270817 -2.5860627 -3.0526323 -3.4195101 -3.9498818 -4.1513791 -3.843467 -3.5062487 -3.4147849 -3.4995775 -3.5807695 -3.563107 -3.5294206 -3.3383875][-1.3243755 -1.3319533 -1.7669137 -2.1152575 -2.3503947 -2.7241142 -2.8309052 -2.4933949 -2.0718236 -1.82961 -1.7283466 -1.7651153 -1.8691142 -2.0201879 -2.1127987][-1.2475297 -1.0369023 -1.3099818 -1.4908364 -1.5255849 -1.6175425 -1.4627049 -1.0020097 -0.50984645 -0.12460709 0.10060525 0.066108227 -0.23970127 -0.69012749 -1.1378976]]...]
INFO - root - 2017-12-15 07:20:59.666959: step 20410, loss = 0.21, batch loss = 0.18 (36.4 examples/sec; 0.220 sec/batch; 19h:02m:15s remains)
INFO - root - 2017-12-15 07:21:01.952746: step 20420, loss = 0.30, batch loss = 0.26 (35.1 examples/sec; 0.228 sec/batch; 19h:47m:01s remains)
INFO - root - 2017-12-15 07:21:04.229785: step 20430, loss = 0.20, batch loss = 0.17 (34.1 examples/sec; 0.235 sec/batch; 20h:20m:10s remains)
INFO - root - 2017-12-15 07:21:06.502708: step 20440, loss = 0.19, batch loss = 0.15 (35.2 examples/sec; 0.227 sec/batch; 19h:42m:00s remains)
INFO - root - 2017-12-15 07:21:08.772188: step 20450, loss = 0.22, batch loss = 0.19 (34.8 examples/sec; 0.230 sec/batch; 19h:54m:19s remains)
INFO - root - 2017-12-15 07:21:11.029389: step 20460, loss = 0.20, batch loss = 0.17 (34.0 examples/sec; 0.235 sec/batch; 20h:22m:56s remains)
INFO - root - 2017-12-15 07:21:13.287711: step 20470, loss = 0.25, batch loss = 0.22 (35.8 examples/sec; 0.223 sec/batch; 19h:20m:53s remains)
INFO - root - 2017-12-15 07:21:15.551037: step 20480, loss = 0.25, batch loss = 0.22 (34.7 examples/sec; 0.231 sec/batch; 19h:58m:45s remains)
INFO - root - 2017-12-15 07:21:17.840812: step 20490, loss = 0.18, batch loss = 0.15 (35.2 examples/sec; 0.227 sec/batch; 19h:40m:43s remains)
INFO - root - 2017-12-15 07:21:20.089271: step 20500, loss = 0.17, batch loss = 0.14 (36.3 examples/sec; 0.220 sec/batch; 19h:05m:36s remains)
2017-12-15 07:21:20.381907: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4245377 -2.3552663 -2.6882968 -3.4719362 -5.2789335 -5.6614857 -5.7114778 -6.3086939 -6.1656003 -5.7301617 -5.6578608 -5.5235839 -4.9836164 -4.6752644 -4.2783985][-3.7246182 -3.8950562 -4.4641914 -5.0104251 -6.6159964 -6.8784466 -6.6221375 -7.204731 -7.2652473 -7.0621157 -7.0796542 -6.8466325 -6.2467194 -5.7298279 -5.3182449][-5.745492 -5.1139975 -5.7230434 -5.9053626 -6.6915421 -6.5096936 -5.9590497 -6.566596 -6.8546858 -6.9631176 -7.2241673 -7.0187092 -6.5371995 -6.1171069 -5.9637713][-6.7163196 -5.67091 -5.8739848 -5.3305244 -5.0807943 -4.2196474 -3.4500189 -4.3616829 -5.0585461 -5.64092 -6.0719519 -5.5818725 -5.0728512 -4.68629 -4.6812229][-7.2151866 -5.4699216 -5.0366631 -3.6624296 -2.365623 -0.7921375 0.22667241 -1.0005956 -2.2925534 -3.5846863 -4.4088368 -3.8239532 -3.3849874 -3.0038409 -3.0479522][-6.7508287 -4.9072647 -3.9207864 -1.850076 0.46590781 3.0880432 4.8258548 3.3839898 1.1415968 -1.1511354 -2.662477 -2.228615 -1.8507848 -1.4114552 -1.4336947][-6.124032 -4.60051 -3.3112488 -0.76572728 2.3918605 5.9553943 8.5071411 7.1835265 4.2740345 1.044868 -1.1327814 -0.85984182 -0.29070795 0.46825767 0.79396248][-6.1287737 -4.6742477 -3.6063485 -1.3538764 2.11872 6.1644015 9.2574558 8.45499 5.596508 2.2068248 -0.21166754 -0.090681076 0.79260778 1.8750625 2.584198][-5.8037877 -4.7487135 -4.264719 -2.7942998 0.18694305 3.8861933 6.9911723 7.0086122 4.876452 2.0052676 -0.20500517 -0.12277532 0.99370646 2.1058021 2.8749971][-5.1833925 -4.7945867 -5.0851364 -4.5146837 -2.4499393 0.3544817 3.1318069 3.95255 2.785964 0.5314045 -1.3084761 -1.0745053 0.086934566 0.89448833 1.5622001][-4.36091 -4.3087358 -5.13195 -5.5684257 -4.7070322 -3.2109673 -1.1253085 0.31863856 0.12976122 -1.2296653 -2.5322287 -2.2014542 -1.0765599 -0.67550838 -0.1543448][-3.5841942 -3.4709122 -4.3948956 -5.4862971 -5.730834 -5.4114685 -4.4495487 -3.3245742 -3.1226587 -3.6100237 -4.1228242 -3.624758 -2.6392384 -2.4037266 -1.9685636][-3.2530394 -2.7228761 -3.2504787 -4.1900473 -5.0171013 -5.5418911 -5.5199394 -5.0256271 -5.0351191 -5.6842175 -6.1583424 -5.6820583 -4.849637 -4.5394688 -4.0374184][-3.0590191 -2.1137705 -2.2846403 -2.9150467 -3.7136302 -4.5357485 -5.1820459 -5.2408981 -5.4081511 -6.1382127 -6.8870845 -6.9144611 -6.50284 -6.3492861 -5.9551029][-3.1390941 -1.9819376 -1.9609009 -2.2652574 -2.640029 -3.0961502 -3.81999 -4.0874538 -4.4228554 -5.2333975 -6.1619296 -6.7826724 -6.90589 -7.1572766 -7.1933784]]...]
INFO - root - 2017-12-15 07:21:22.644910: step 20510, loss = 0.25, batch loss = 0.22 (35.6 examples/sec; 0.225 sec/batch; 19h:27m:34s remains)
INFO - root - 2017-12-15 07:21:24.928675: step 20520, loss = 0.19, batch loss = 0.16 (35.0 examples/sec; 0.229 sec/batch; 19h:49m:24s remains)
INFO - root - 2017-12-15 07:21:27.184766: step 20530, loss = 0.19, batch loss = 0.16 (33.4 examples/sec; 0.240 sec/batch; 20h:46m:05s remains)
INFO - root - 2017-12-15 07:21:29.453951: step 20540, loss = 0.28, batch loss = 0.25 (36.5 examples/sec; 0.219 sec/batch; 18h:59m:02s remains)
INFO - root - 2017-12-15 07:21:31.714700: step 20550, loss = 0.27, batch loss = 0.24 (35.0 examples/sec; 0.228 sec/batch; 19h:46m:44s remains)
INFO - root - 2017-12-15 07:21:33.987156: step 20560, loss = 0.21, batch loss = 0.17 (36.2 examples/sec; 0.221 sec/batch; 19h:08m:16s remains)
INFO - root - 2017-12-15 07:21:36.230643: step 20570, loss = 0.22, batch loss = 0.19 (35.8 examples/sec; 0.223 sec/batch; 19h:21m:37s remains)
INFO - root - 2017-12-15 07:21:38.494407: step 20580, loss = 0.28, batch loss = 0.25 (36.7 examples/sec; 0.218 sec/batch; 18h:54m:32s remains)
INFO - root - 2017-12-15 07:21:40.780160: step 20590, loss = 0.34, batch loss = 0.31 (35.5 examples/sec; 0.225 sec/batch; 19h:31m:26s remains)
INFO - root - 2017-12-15 07:21:43.023448: step 20600, loss = 0.30, batch loss = 0.27 (36.0 examples/sec; 0.222 sec/batch; 19h:16m:01s remains)
2017-12-15 07:21:43.315520: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2745571 -6.4060578 -6.4231787 -6.44725 -6.6017194 -6.8373857 -6.8139524 -6.5166321 -6.2541695 -6.2856317 -6.5565572 -6.9057422 -7.1582279 -7.234127 -7.1227727][-5.5584574 -7.0109944 -6.9669766 -6.9531364 -7.0186868 -7.2503347 -7.2895408 -6.92462 -6.4916506 -6.6033144 -7.2597618 -7.8850079 -8.1205988 -8.1401978 -8.0398369][-5.8300638 -6.7327862 -6.4818735 -6.2452288 -5.933537 -5.8588362 -5.76521 -5.3595123 -4.8344693 -5.1719475 -6.4571319 -7.5473528 -7.8415442 -7.8038659 -7.7014856][-6.3349113 -6.5950708 -6.1589041 -5.6868963 -4.837615 -4.2058744 -3.8832455 -3.4347796 -2.9178522 -3.4906569 -5.1654854 -6.5026493 -6.7879581 -6.6326303 -6.4914622][-6.8109317 -6.2846708 -5.4305124 -4.5733819 -3.0675173 -1.7715797 -1.0948184 -0.67670369 -0.3454634 -1.2722771 -3.2002435 -4.727561 -5.1457729 -5.1254129 -5.0980177][-7.1862612 -6.1464977 -4.7891083 -3.505949 -1.4305613 0.5727551 1.7635293 2.2368917 2.2688694 0.93613625 -1.0613451 -2.6498365 -3.4612322 -3.8903682 -4.0675983][-6.9800177 -6.0436954 -4.3592081 -2.8207219 -0.54116106 1.8119035 3.3612595 3.9666181 3.626225 1.7131302 -0.19705701 -1.5473224 -2.4565763 -3.0133798 -3.2231126][-7.1011147 -5.9629154 -4.0948048 -2.5001743 -0.47518218 1.7516165 3.390059 4.011302 3.4941106 1.366596 -0.205549 -1.2164761 -2.0872176 -2.5568702 -2.5915895][-7.1467919 -5.95092 -4.1151123 -2.6288676 -1.0428641 0.65566325 2.0658088 2.7456164 2.348166 0.58763361 -0.4119637 -1.0538636 -1.6823242 -1.8552868 -1.6715895][-6.9538164 -5.7987432 -4.2036262 -3.0488651 -1.9970646 -0.95707607 0.11571598 0.81843066 0.70209932 -0.4199115 -0.82140362 -0.9908576 -1.1833459 -1.1122994 -0.83915186][-6.4278464 -5.3627291 -4.1413651 -3.4407136 -2.8103786 -2.2130456 -1.2315243 -0.43617022 -0.21017027 -0.74460578 -0.74430633 -0.63817608 -0.37967145 -0.012136459 0.32878351][-6.1081514 -5.1047864 -4.2068329 -3.8037748 -3.4174495 -3.0263722 -1.8947471 -0.85279822 -0.13858175 -0.069585562 0.081305742 0.10343361 0.55668449 1.0150192 1.1730154][-6.3279228 -5.5503311 -4.9482236 -4.6674538 -4.2735157 -3.7790065 -2.3941939 -0.96475947 0.18510079 0.6067977 0.69326067 0.54721403 0.95481706 1.2451088 1.0396187][-6.5829725 -6.0747852 -5.7276506 -5.377677 -4.7425966 -4.051106 -2.5466106 -1.0518842 0.20739985 0.67879081 0.48298645 0.09183979 0.2594986 0.25500917 -0.26675856][-6.9799953 -6.7056866 -6.4980836 -5.9377747 -5.0388021 -4.331563 -3.1355219 -1.9713638 -0.94540274 -0.61126959 -0.99149156 -1.4610068 -1.576936 -1.8374726 -2.4592297]]...]
INFO - root - 2017-12-15 07:21:45.593033: step 20610, loss = 0.25, batch loss = 0.22 (35.2 examples/sec; 0.227 sec/batch; 19h:41m:10s remains)
INFO - root - 2017-12-15 07:21:47.873094: step 20620, loss = 0.24, batch loss = 0.20 (34.8 examples/sec; 0.230 sec/batch; 19h:55m:50s remains)
INFO - root - 2017-12-15 07:21:50.140981: step 20630, loss = 0.23, batch loss = 0.20 (35.1 examples/sec; 0.228 sec/batch; 19h:45m:40s remains)
INFO - root - 2017-12-15 07:21:52.404340: step 20640, loss = 0.20, batch loss = 0.17 (35.9 examples/sec; 0.223 sec/batch; 19h:19m:47s remains)
INFO - root - 2017-12-15 07:21:54.713959: step 20650, loss = 0.27, batch loss = 0.23 (34.4 examples/sec; 0.232 sec/batch; 20h:08m:00s remains)
INFO - root - 2017-12-15 07:21:56.960053: step 20660, loss = 0.21, batch loss = 0.18 (36.8 examples/sec; 0.217 sec/batch; 18h:48m:21s remains)
INFO - root - 2017-12-15 07:21:59.230147: step 20670, loss = 0.23, batch loss = 0.20 (35.3 examples/sec; 0.226 sec/batch; 19h:37m:05s remains)
INFO - root - 2017-12-15 07:22:01.511930: step 20680, loss = 0.21, batch loss = 0.18 (34.5 examples/sec; 0.232 sec/batch; 20h:03m:33s remains)
INFO - root - 2017-12-15 07:22:03.821117: step 20690, loss = 0.25, batch loss = 0.22 (35.1 examples/sec; 0.228 sec/batch; 19h:42m:57s remains)
INFO - root - 2017-12-15 07:22:06.120936: step 20700, loss = 0.22, batch loss = 0.19 (32.7 examples/sec; 0.244 sec/batch; 21h:10m:10s remains)
2017-12-15 07:22:06.400239: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1650977 -5.623313 -5.0542951 -4.2888293 -3.7814007 -3.4595327 -3.0336602 -2.4982533 -2.2407289 -2.2546659 -2.236129 -2.2689793 -2.2685907 -2.2848616 -2.0028419][-4.2133117 -5.1098585 -4.64876 -4.1121788 -3.8253617 -3.4629292 -2.9325719 -2.4024541 -2.3677497 -2.5623772 -2.6376903 -2.6167204 -2.6770954 -2.9117608 -2.9945726][-4.0298986 -4.2471457 -3.8913321 -3.6033058 -3.4066191 -3.0916548 -2.7289317 -2.5255663 -2.8242691 -3.2129653 -3.3078763 -3.2347674 -3.1909595 -3.3485584 -3.4813943][-4.1599293 -3.8327067 -3.4276948 -3.0883081 -2.653723 -2.1704302 -1.8188642 -1.7906821 -2.34338 -3.0047944 -3.4882548 -3.7470095 -3.7950766 -3.9109492 -3.9053695][-3.9420073 -3.1212978 -2.4857919 -1.8881661 -1.2606611 -0.77852416 -0.52443433 -0.50993896 -0.925823 -1.6798487 -2.7116809 -3.5203743 -3.882237 -4.12202 -4.0634475][-3.2802763 -2.3509662 -1.5913539 -0.76200223 0.019163847 0.5228579 0.78074169 0.8739562 0.67205215 -0.13386369 -1.4577669 -2.7230997 -3.493629 -3.9558604 -3.8309145][-2.7092931 -2.0850954 -1.5216916 -0.74419212 0.11868954 0.79101777 1.2674778 1.582186 1.5410566 0.81611753 -0.43896031 -1.742195 -2.6763115 -3.4768791 -3.395545][-3.2384562 -2.6052613 -2.2867296 -1.7300724 -0.87199533 0.053015947 0.74343705 1.1353598 1.1208239 0.49875164 -0.38061512 -1.194312 -1.8720578 -2.6728733 -2.5519328][-3.6972632 -3.1883805 -3.2299 -3.0378013 -2.3015189 -0.99262989 0.027910709 0.37841129 0.15836263 -0.61184454 -1.1599618 -1.4811976 -1.7154859 -2.1771324 -1.8121089][-4.5626526 -4.0668244 -4.111814 -3.9953897 -3.4220791 -2.0588758 -0.8213532 -0.44373274 -0.83552814 -1.645669 -2.0811331 -2.1204317 -2.0388424 -2.2318487 -1.6146319][-5.6672544 -5.3585219 -5.0317078 -4.3757749 -3.450356 -2.3871164 -1.5395708 -1.460748 -2.0673079 -2.9348421 -3.3914323 -3.3491497 -3.1229038 -2.9699955 -2.0543244][-6.3842545 -6.3734474 -5.7075372 -4.4724789 -3.3119588 -2.50179 -1.9960356 -2.0682294 -2.8566782 -3.7351189 -4.2303514 -4.3468103 -4.2741613 -3.9686997 -3.0560062][-6.2009735 -6.4218397 -5.5075097 -3.7804849 -2.6209533 -2.2568877 -1.9993461 -2.1540663 -2.89954 -3.6642046 -4.1688271 -4.4267907 -4.49017 -4.4152441 -3.8188257][-5.1723032 -5.3803687 -4.486959 -2.8165007 -1.7788 -1.6784443 -1.6798739 -2.1683691 -3.1683724 -3.994236 -4.4020014 -4.3781996 -4.2798624 -4.31785 -4.1294379][-3.9243445 -3.6788237 -2.8525355 -1.5871193 -0.94694269 -1.0508529 -1.3526058 -2.364387 -3.7958689 -4.8974876 -5.3245497 -5.0329289 -4.5300646 -4.3035564 -4.3631673]]...]
INFO - root - 2017-12-15 07:22:08.682319: step 20710, loss = 0.25, batch loss = 0.21 (35.4 examples/sec; 0.226 sec/batch; 19h:35m:45s remains)
INFO - root - 2017-12-15 07:22:10.970107: step 20720, loss = 0.21, batch loss = 0.18 (35.9 examples/sec; 0.223 sec/batch; 19h:17m:04s remains)
INFO - root - 2017-12-15 07:22:13.201035: step 20730, loss = 0.23, batch loss = 0.19 (36.7 examples/sec; 0.218 sec/batch; 18h:52m:48s remains)
INFO - root - 2017-12-15 07:22:15.457634: step 20740, loss = 0.30, batch loss = 0.27 (34.6 examples/sec; 0.231 sec/batch; 20h:02m:12s remains)
INFO - root - 2017-12-15 07:22:17.717269: step 20750, loss = 0.26, batch loss = 0.23 (35.4 examples/sec; 0.226 sec/batch; 19h:33m:07s remains)
INFO - root - 2017-12-15 07:22:19.958939: step 20760, loss = 0.16, batch loss = 0.12 (36.2 examples/sec; 0.221 sec/batch; 19h:07m:30s remains)
INFO - root - 2017-12-15 07:22:22.191135: step 20770, loss = 0.23, batch loss = 0.20 (35.1 examples/sec; 0.228 sec/batch; 19h:42m:48s remains)
INFO - root - 2017-12-15 07:22:24.474225: step 20780, loss = 0.29, batch loss = 0.25 (35.4 examples/sec; 0.226 sec/batch; 19h:32m:51s remains)
INFO - root - 2017-12-15 07:22:26.713696: step 20790, loss = 0.46, batch loss = 0.43 (35.6 examples/sec; 0.224 sec/batch; 19h:25m:54s remains)
INFO - root - 2017-12-15 07:22:28.954420: step 20800, loss = 0.24, batch loss = 0.21 (36.1 examples/sec; 0.222 sec/batch; 19h:12m:02s remains)
2017-12-15 07:22:29.220804: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.93644249 -3.288753 -3.6321914 -3.7621474 -3.9443889 -4.1684041 -4.24259 -4.31773 -4.367939 -4.3575106 -4.2088366 -3.8964148 -3.5219264 -3.089437 -2.7797539][-2.5183768 -3.9186625 -4.1882687 -4.2330441 -4.3168907 -4.4331689 -4.3001127 -4.2326932 -4.3563461 -4.5247107 -4.5992217 -4.3443675 -3.9825959 -3.5312762 -3.1533175][-3.4220471 -3.823559 -3.8740931 -3.6687083 -3.4681463 -3.4016609 -3.1573002 -3.1015813 -3.4329178 -3.9369831 -4.369401 -4.2303557 -3.9263859 -3.5508006 -3.1374462][-3.4771523 -2.928853 -2.6475222 -2.0721142 -1.4376681 -1.0885642 -0.83263087 -0.92617977 -1.6120408 -2.6498997 -3.5865848 -3.6890113 -3.5814338 -3.4027295 -2.9546928][-3.0144403 -1.5551155 -0.91120172 0.07730341 1.2408597 1.9410448 2.1779051 1.9013047 0.84433293 -0.75697756 -2.2596769 -2.6677113 -2.7834756 -2.8624666 -2.4226236][-2.2434666 -0.43049681 0.3990891 1.6277573 3.1858392 4.1580505 4.2840519 3.8495383 2.5559335 0.58905959 -1.2739727 -1.875067 -2.1272037 -2.3362105 -1.8726153][-1.5164185 -0.16606045 0.66049933 1.9368229 3.6592412 4.7534795 4.7739367 4.2922177 2.9984818 0.96819854 -0.87406421 -1.4759867 -1.8352414 -2.0496273 -1.4997278][-2.1242564 -0.8892597 -0.22877836 0.87553072 2.5025887 3.6646423 3.8210516 3.4868908 2.3306327 0.4457972 -1.2481599 -1.8263273 -2.1532094 -2.171102 -1.5345058][-3.069273 -2.0883384 -1.6886423 -0.90858769 0.40815425 1.4888201 1.8019881 1.5808384 0.56532669 -1.0373919 -2.455302 -2.8974717 -3.0006824 -2.6990213 -1.9813701][-3.9690433 -3.2528076 -3.1135321 -2.6944461 -1.8421464 -1.010258 -0.69909704 -0.89574468 -1.6607323 -2.8192677 -3.7517886 -3.9313326 -3.7805371 -3.2415891 -2.5421555][-4.3346586 -3.7946744 -3.8749433 -3.8056664 -3.4111938 -2.9030488 -2.6867111 -2.8801606 -3.3654256 -4.0432482 -4.5012178 -4.454092 -4.1114764 -3.4886112 -2.9013035][-4.2368755 -3.7473073 -3.9364138 -4.0865364 -4.0247092 -3.817528 -3.728415 -3.9017639 -4.2017937 -4.4965281 -4.6082649 -4.3893528 -3.9065249 -3.3061438 -2.8750234][-3.8737888 -3.3571782 -3.5844064 -3.8204949 -3.9650743 -4.015274 -4.1099544 -4.2779021 -4.3969631 -4.4107542 -4.2399654 -3.9080451 -3.4596796 -3.0193281 -2.7458479][-3.456538 -2.8633029 -3.0672128 -3.2927647 -3.520206 -3.7090898 -3.8955669 -4.0529709 -4.0354533 -3.8846011 -3.6084895 -3.3077719 -3.0010223 -2.72613 -2.564992][-3.4325233 -2.7569263 -2.9025378 -3.0761518 -3.2847345 -3.4632559 -3.5934184 -3.6777472 -3.6280038 -3.4838307 -3.2574573 -3.0692611 -2.8939235 -2.7277844 -2.6232882]]...]
INFO - root - 2017-12-15 07:22:31.504807: step 20810, loss = 0.19, batch loss = 0.15 (36.0 examples/sec; 0.222 sec/batch; 19h:14m:17s remains)
INFO - root - 2017-12-15 07:22:33.782412: step 20820, loss = 0.18, batch loss = 0.14 (34.8 examples/sec; 0.230 sec/batch; 19h:54m:20s remains)
INFO - root - 2017-12-15 07:22:36.029863: step 20830, loss = 0.16, batch loss = 0.13 (35.6 examples/sec; 0.225 sec/batch; 19h:28m:02s remains)
INFO - root - 2017-12-15 07:22:38.283701: step 20840, loss = 0.24, batch loss = 0.21 (34.5 examples/sec; 0.232 sec/batch; 20h:03m:48s remains)
INFO - root - 2017-12-15 07:22:40.575729: step 20850, loss = 0.16, batch loss = 0.13 (35.8 examples/sec; 0.224 sec/batch; 19h:20m:55s remains)
INFO - root - 2017-12-15 07:22:42.838991: step 20860, loss = 0.20, batch loss = 0.17 (35.5 examples/sec; 0.225 sec/batch; 19h:31m:13s remains)
INFO - root - 2017-12-15 07:22:45.090795: step 20870, loss = 0.20, batch loss = 0.17 (35.0 examples/sec; 0.228 sec/batch; 19h:45m:40s remains)
INFO - root - 2017-12-15 07:22:47.386946: step 20880, loss = 0.20, batch loss = 0.17 (36.2 examples/sec; 0.221 sec/batch; 19h:06m:49s remains)
INFO - root - 2017-12-15 07:22:49.633176: step 20890, loss = 0.21, batch loss = 0.18 (36.0 examples/sec; 0.222 sec/batch; 19h:15m:05s remains)
INFO - root - 2017-12-15 07:22:51.899740: step 20900, loss = 0.20, batch loss = 0.17 (34.9 examples/sec; 0.229 sec/batch; 19h:51m:34s remains)
2017-12-15 07:22:52.175408: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.028353214 -3.4858847 -6.0478296 -7.2130942 -7.5389848 -6.8881989 -6.0692415 -5.9263105 -6.147419 -6.2331867 -6.3123574 -6.3655519 -6.234827 -5.9900856 -5.6785355][-1.9606931 -4.5277042 -6.7179985 -7.7200561 -8.0971432 -7.4631705 -6.5214863 -6.0923696 -6.2529311 -6.5142708 -6.7583179 -6.8771358 -6.7836733 -6.5430689 -5.9902744][-4.991931 -6.1419182 -7.5666351 -7.9301195 -8.074707 -7.2656951 -6.0219183 -5.3960595 -5.5262418 -5.9400568 -6.4587312 -6.6505218 -6.6092815 -6.5085306 -5.9638577][-7.2752743 -7.5650496 -8.1890774 -7.8635631 -7.5312252 -6.2225666 -4.4830885 -3.7163854 -3.9556727 -4.6244187 -5.5491352 -5.9351444 -6.1149893 -6.1401162 -5.576251][-8.5020046 -8.0986071 -8.1329727 -7.4408927 -6.3355417 -4.1789594 -1.8423872 -0.86927211 -1.208559 -2.4469028 -4.162077 -5.112586 -5.68297 -5.7541122 -5.0717278][-8.9804535 -7.6610785 -7.3258281 -6.3022246 -4.5340424 -1.9009067 0.89478183 2.1184726 1.7845554 0.0073065758 -2.4640961 -4.1773496 -5.2772646 -5.5315695 -4.8127441][-7.7636867 -6.4514389 -5.814249 -4.6137705 -2.4945922 0.40632606 3.2627892 4.6255097 4.3814635 2.3324027 -0.64998627 -3.1258013 -4.9548874 -5.5249381 -4.8968678][-7.216764 -5.6102428 -4.7299519 -3.3087854 -0.93543041 1.9103856 4.6484365 6.0111542 5.6921597 3.2892756 0.059651613 -2.7047698 -4.8515816 -5.6172934 -5.2095189][-7.0726266 -5.6721787 -4.7864084 -3.3228388 -1.1095842 1.4394822 3.9803176 5.326726 4.969245 2.5159197 -0.70826244 -3.3298051 -5.1086888 -5.6899409 -5.366209][-7.173131 -6.2149229 -5.6119318 -4.4015894 -2.5098951 -0.26365876 1.9664588 3.1111946 2.603837 0.17200899 -2.8151886 -4.8909149 -6.0239043 -6.1794491 -5.8784122][-7.2194076 -6.5616746 -6.1855564 -5.3062606 -3.9166141 -2.2256069 -0.55661917 0.2597456 -0.31980717 -2.4523544 -4.8879147 -6.4568763 -7.0203533 -6.7803903 -6.5164566][-7.064888 -6.4763036 -6.2134776 -5.64664 -4.860085 -3.9989939 -3.1244736 -2.6242611 -2.9673388 -4.5252666 -6.2679377 -7.298171 -7.51385 -7.1136041 -6.6709352][-6.9166451 -6.4163895 -6.2628222 -5.8698568 -5.4538817 -5.1959748 -4.9878073 -4.8390656 -5.0069733 -5.886281 -6.9571071 -7.56799 -7.5551319 -7.023921 -6.4020243][-6.4922595 -6.0740805 -6.0587921 -5.8069119 -5.6064529 -5.567276 -5.7262068 -5.8594708 -5.9562969 -6.4128232 -6.9661074 -7.2467537 -7.1620212 -6.6780891 -5.9999571][-5.738091 -5.3683262 -5.5669069 -5.5569363 -5.5491982 -5.5748696 -5.7350187 -5.8935204 -6.0053072 -6.3341179 -6.6806169 -6.8302751 -6.665772 -6.2415957 -5.5728755]]...]
INFO - root - 2017-12-15 07:22:54.466209: step 20910, loss = 0.16, batch loss = 0.13 (35.3 examples/sec; 0.227 sec/batch; 19h:37m:11s remains)
INFO - root - 2017-12-15 07:22:56.711817: step 20920, loss = 0.23, batch loss = 0.20 (36.8 examples/sec; 0.217 sec/batch; 18h:49m:01s remains)
INFO - root - 2017-12-15 07:22:58.957450: step 20930, loss = 0.25, batch loss = 0.22 (34.8 examples/sec; 0.230 sec/batch; 19h:54m:01s remains)
INFO - root - 2017-12-15 07:23:01.205508: step 20940, loss = 0.29, batch loss = 0.25 (35.6 examples/sec; 0.225 sec/batch; 19h:28m:13s remains)
INFO - root - 2017-12-15 07:23:03.443311: step 20950, loss = 0.36, batch loss = 0.32 (34.8 examples/sec; 0.230 sec/batch; 19h:52m:42s remains)
INFO - root - 2017-12-15 07:23:05.696533: step 20960, loss = 0.28, batch loss = 0.25 (36.0 examples/sec; 0.223 sec/batch; 19h:15m:22s remains)
INFO - root - 2017-12-15 07:23:07.967349: step 20970, loss = 0.24, batch loss = 0.20 (34.8 examples/sec; 0.230 sec/batch; 19h:53m:47s remains)
INFO - root - 2017-12-15 07:23:10.284308: step 20980, loss = 0.32, batch loss = 0.29 (35.1 examples/sec; 0.228 sec/batch; 19h:42m:58s remains)
INFO - root - 2017-12-15 07:23:12.612199: step 20990, loss = 0.43, batch loss = 0.40 (33.9 examples/sec; 0.236 sec/batch; 20h:26m:42s remains)
INFO - root - 2017-12-15 07:23:14.866687: step 21000, loss = 0.20, batch loss = 0.17 (34.7 examples/sec; 0.231 sec/batch; 19h:58m:03s remains)
2017-12-15 07:23:15.166390: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1326485 -5.1672373 -5.2768297 -5.8111 -6.5728936 -7.0973659 -7.1647 -6.724731 -6.2503004 -5.6410284 -5.4238219 -5.4928703 -5.908556 -6.226891 -5.880826][-4.7945414 -4.9711614 -5.2319179 -5.9116344 -6.526423 -6.924675 -6.897243 -6.3499885 -5.867033 -5.2921009 -5.0737066 -5.2236524 -5.6191173 -5.815609 -5.3582525][-5.4849749 -4.3888378 -4.8474064 -5.68539 -6.1254311 -6.4156952 -6.1982069 -5.1888618 -4.5029297 -3.9947433 -4.0351992 -4.4867887 -5.1331453 -5.3834848 -5.0426579][-5.4965744 -3.5070004 -4.1035366 -5.1329165 -5.6084738 -5.7893467 -5.1421614 -3.5291138 -2.6261547 -2.3388834 -2.8627794 -3.8222106 -4.8912678 -5.3320217 -5.2627788][-5.0051 -2.4165475 -3.1059794 -4.2947731 -4.8454547 -4.6178284 -3.1531768 -0.7991457 0.39533186 0.25031304 -1.1962312 -3.0857592 -4.7594457 -5.5931664 -5.87278][-4.0084481 -1.3737469 -1.9005787 -2.9933763 -3.5508866 -2.8693812 -0.59461462 2.4003778 4.0146112 3.3742728 0.91607857 -2.0805492 -4.5328755 -5.8690963 -6.4138641][-2.8083968 -0.54659247 -0.84094393 -1.7569988 -2.3496635 -1.2029544 1.7539825 5.2838244 7.1736035 6.0055556 2.7710681 -0.95595038 -3.998687 -5.7692676 -6.4428525][-3.0046296 -0.56148767 -0.61729407 -1.3058221 -1.7953634 -0.39993525 2.8108482 6.4675937 8.3940277 6.9387422 3.5564809 -0.25954556 -3.3573613 -5.1916981 -5.8180609][-3.5200129 -1.1102424 -1.0824775 -1.6465914 -2.0700879 -0.87426126 1.8774533 5.0639529 6.6427245 5.2190619 2.3162861 -0.86424756 -3.2797174 -4.498929 -4.6678014][-4.0241737 -2.0215974 -2.2881529 -2.945004 -3.4780693 -2.7729099 -0.76492631 1.6904969 2.7716403 1.6927905 -0.33919692 -2.3024974 -3.6324985 -3.8744478 -3.4866681][-4.4073124 -2.9509521 -3.5216451 -4.2567177 -4.7842321 -4.4853816 -3.2193422 -1.6224651 -0.94745755 -1.5124581 -2.711545 -3.6106386 -4.0157065 -3.5956082 -2.8739033][-4.323669 -3.2967491 -3.9349673 -4.6387424 -5.0313892 -4.7872372 -3.9808955 -2.9445176 -2.4159169 -2.6084113 -3.4581885 -3.886044 -4.1157269 -3.6826875 -3.0050974][-3.905582 -3.036108 -3.4630451 -4.0221395 -4.1453533 -3.6142285 -2.8479972 -1.9824834 -1.3450179 -1.3706636 -2.2726 -2.8618016 -3.6554227 -3.8308237 -3.5206981][-4.0484242 -2.8313496 -2.7146912 -2.8810494 -2.6640003 -1.8035697 -0.95947242 -0.080918074 0.82070541 0.83334708 -0.19994354 -1.1978575 -2.7852287 -3.7589316 -3.8762069][-4.3982506 -2.6571498 -2.1178191 -1.9105508 -1.5167379 -0.70348561 0.016358614 1.0398035 2.3332424 2.45997 1.3978517 0.05990696 -2.1149633 -3.5108848 -3.82535]]...]
INFO - root - 2017-12-15 07:23:17.462629: step 21010, loss = 0.19, batch loss = 0.16 (35.8 examples/sec; 0.223 sec/batch; 19h:19m:08s remains)
INFO - root - 2017-12-15 07:23:19.752586: step 21020, loss = 0.24, batch loss = 0.21 (35.2 examples/sec; 0.228 sec/batch; 19h:41m:03s remains)
INFO - root - 2017-12-15 07:23:22.032339: step 21030, loss = 0.22, batch loss = 0.19 (34.4 examples/sec; 0.233 sec/batch; 20h:07m:28s remains)
INFO - root - 2017-12-15 07:23:24.294993: step 21040, loss = 0.27, batch loss = 0.24 (34.5 examples/sec; 0.232 sec/batch; 20h:03m:56s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 07:23:26.532409: step 21050, loss = 0.27, batch loss = 0.24 (35.6 examples/sec; 0.225 sec/batch; 19h:27m:21s remains)
INFO - root - 2017-12-15 07:23:28.784002: step 21060, loss = 0.27, batch loss = 0.24 (35.7 examples/sec; 0.224 sec/batch; 19h:24m:15s remains)
INFO - root - 2017-12-15 07:23:31.025725: step 21070, loss = 0.32, batch loss = 0.28 (36.3 examples/sec; 0.220 sec/batch; 19h:04m:22s remains)
INFO - root - 2017-12-15 07:23:33.285968: step 21080, loss = 0.35, batch loss = 0.32 (34.5 examples/sec; 0.232 sec/batch; 20h:02m:57s remains)
INFO - root - 2017-12-15 07:23:35.531688: step 21090, loss = 0.17, batch loss = 0.14 (36.1 examples/sec; 0.222 sec/batch; 19h:09m:37s remains)
INFO - root - 2017-12-15 07:23:37.795660: step 21100, loss = 0.15, batch loss = 0.12 (35.3 examples/sec; 0.227 sec/batch; 19h:35m:49s remains)
2017-12-15 07:23:38.095960: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7323029 -4.7425823 -5.4691935 -5.9002314 -5.9910889 -5.9051647 -5.7850523 -5.3843641 -4.8858309 -4.3785391 -4.1396546 -4.3608732 -4.9424458 -5.2571926 -5.0797358][-5.0791807 -5.82522 -6.56535 -6.945313 -6.8318276 -6.5938182 -6.4319825 -6.0531058 -5.6260662 -5.3136482 -5.160181 -5.3285255 -5.810473 -5.8870611 -5.4849424][-6.2740006 -6.2268133 -6.8614583 -7.1005135 -6.7014117 -6.2446918 -5.947238 -5.5567217 -5.2686052 -5.2796268 -5.3063378 -5.4757853 -5.8995013 -5.7861929 -5.2301559][-6.7078795 -6.4084435 -6.845253 -6.7795095 -5.882503 -4.9522467 -4.3603191 -3.8843496 -3.7705841 -4.2409525 -4.6904397 -5.1429234 -5.7431483 -5.6856651 -5.1528339][-6.8680229 -6.2816238 -6.4138069 -5.8709412 -4.2023406 -2.5602057 -1.6266112 -1.0027032 -1.1177893 -2.1773882 -3.2505264 -4.170609 -5.1259432 -5.3150654 -4.9259071][-6.467402 -5.7940011 -5.5745859 -4.4334412 -1.8482296 0.549207 1.8918209 2.8861737 2.5536027 0.773963 -0.97423542 -2.4886677 -3.8523116 -4.3401284 -4.1869311][-5.6793833 -5.08727 -4.3969021 -2.6089079 0.75144219 3.7744217 5.4521003 6.623085 5.8791037 3.4950671 1.2805655 -0.76791275 -2.6204338 -3.35039 -3.3803678][-5.2981029 -4.4880533 -3.409622 -1.278299 2.2886925 5.4084158 7.0386391 8.1254482 7.106986 4.4968948 2.2755003 0.16202521 -1.7632246 -2.4388669 -2.542794][-5.1066589 -4.207458 -2.9763331 -0.87633288 2.28645 4.8662934 6.0761175 6.7975063 5.6850014 3.3759322 1.579191 -0.16865015 -1.7892255 -2.0236876 -1.9714229][-4.9533253 -4.005847 -2.8275638 -1.0301906 1.3857818 3.2396054 3.8844504 3.9802742 2.7984104 1.1294861 0.072756529 -1.1194332 -2.2738192 -2.1381917 -1.8583384][-4.8839588 -3.9283404 -2.9950109 -1.8047839 -0.31576788 0.76100731 0.84865427 0.44142532 -0.58801782 -1.5851003 -1.9493225 -2.4597752 -3.0313818 -2.5752907 -2.0158882][-4.7513933 -3.8711414 -3.2562475 -2.6218865 -1.8443477 -1.2689354 -1.3847094 -1.8274727 -2.5809178 -3.1668944 -3.0594449 -3.0147567 -3.225338 -2.8913043 -2.3311443][-4.646111 -3.9311137 -3.6483631 -3.4056394 -2.9357643 -2.4716442 -2.4873886 -2.7138071 -3.2148776 -3.5501561 -3.0854878 -2.89435 -3.049742 -2.9034615 -2.4703813][-4.5587344 -3.9794073 -3.9981494 -4.06988 -3.8206258 -3.3951206 -3.1976357 -2.9972405 -3.2548726 -3.5193889 -2.9080169 -2.7320461 -2.8501954 -2.9040473 -2.6324995][-4.8942804 -4.3621368 -4.4511003 -4.5446234 -4.2202158 -3.7304387 -3.2957284 -2.687433 -2.7100797 -2.8958373 -2.3970256 -2.4958954 -2.7442396 -3.0029793 -2.8993003]]...]
INFO - root - 2017-12-15 07:23:40.383458: step 21110, loss = 0.21, batch loss = 0.18 (35.3 examples/sec; 0.227 sec/batch; 19h:36m:20s remains)
INFO - root - 2017-12-15 07:23:42.645783: step 21120, loss = 0.24, batch loss = 0.21 (35.2 examples/sec; 0.227 sec/batch; 19h:38m:11s remains)
INFO - root - 2017-12-15 07:23:44.927479: step 21130, loss = 0.20, batch loss = 0.17 (35.0 examples/sec; 0.229 sec/batch; 19h:47m:18s remains)
INFO - root - 2017-12-15 07:23:47.238513: step 21140, loss = 0.27, batch loss = 0.24 (35.5 examples/sec; 0.225 sec/batch; 19h:28m:30s remains)
INFO - root - 2017-12-15 07:23:49.494847: step 21150, loss = 0.39, batch loss = 0.36 (35.1 examples/sec; 0.228 sec/batch; 19h:43m:46s remains)
INFO - root - 2017-12-15 07:23:51.777431: step 21160, loss = 0.30, batch loss = 0.27 (34.6 examples/sec; 0.231 sec/batch; 20h:00m:04s remains)
INFO - root - 2017-12-15 07:23:54.116184: step 21170, loss = 0.20, batch loss = 0.17 (35.4 examples/sec; 0.226 sec/batch; 19h:33m:46s remains)
INFO - root - 2017-12-15 07:23:56.378678: step 21180, loss = 0.32, batch loss = 0.29 (35.3 examples/sec; 0.227 sec/batch; 19h:35m:44s remains)
INFO - root - 2017-12-15 07:23:58.630150: step 21190, loss = 0.29, batch loss = 0.26 (37.0 examples/sec; 0.216 sec/batch; 18h:41m:39s remains)
INFO - root - 2017-12-15 07:24:00.888081: step 21200, loss = 0.20, batch loss = 0.17 (36.2 examples/sec; 0.221 sec/batch; 19h:07m:42s remains)
2017-12-15 07:24:01.177114: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5902259 -4.2686172 -4.1361022 -4.4232569 -5.1460376 -6.0724983 -6.5966511 -6.3372087 -5.9739332 -5.7095227 -5.3727155 -5.6385808 -5.7654982 -5.5031557 -5.4135833][-3.8330359 -4.6817932 -4.826087 -5.3707528 -6.4804239 -7.5716562 -8.0288048 -7.6707983 -7.2210836 -7.0850182 -6.7858639 -7.0478086 -7.18073 -6.7956495 -6.8440676][-4.8567219 -5.2154336 -5.8588762 -6.4071875 -7.4399061 -8.2195873 -8.3165855 -7.7848177 -7.2894096 -7.5902643 -7.6715422 -8.0458393 -8.209671 -7.6224203 -7.6640854][-5.2947311 -5.598701 -6.6179361 -6.9300661 -7.3927727 -7.2593479 -6.5569878 -5.5846834 -4.9727812 -5.9387417 -6.6925249 -7.4948778 -8.0598545 -7.3855543 -7.204196][-5.4469128 -5.6464605 -6.6407537 -6.4148874 -5.8554363 -4.3336048 -2.6938226 -1.4009941 -0.94916093 -2.9644876 -4.9232216 -6.5013494 -7.6221151 -6.8574123 -6.1403913][-5.6869793 -5.6259565 -6.2419863 -5.45809 -3.7444096 -0.87803257 1.5136273 2.9393046 3.0450881 -0.013856649 -3.3269219 -6.0499392 -7.7760506 -6.9574585 -5.5772104][-5.5293527 -5.5636163 -5.7600346 -4.7825823 -2.1556501 1.8865621 4.9119186 6.3139906 6.0577726 2.4232256 -1.9204351 -5.693346 -7.7843657 -6.8108416 -4.804925][-5.724834 -5.6386909 -5.712018 -4.88342 -1.8972118 2.7745349 6.0896749 7.2942858 6.7869558 3.122195 -1.6432662 -5.7496152 -7.39095 -5.748271 -3.0808473][-6.0750504 -6.2195406 -6.2909079 -5.7012043 -2.8662474 1.6126618 4.6796808 5.4715176 4.7157192 1.5264504 -2.7280664 -6.21819 -6.752099 -4.2767477 -1.2339617][-6.6570492 -6.9187956 -6.8796787 -6.6380129 -4.343317 -0.50043654 2.1384766 2.3779743 1.2540195 -1.1751837 -4.2109137 -6.3789263 -5.6361074 -2.6431675 0.25045228][-7.4624224 -7.5964193 -7.4803123 -7.3774776 -5.6255236 -2.4709117 -0.25105202 -0.48488092 -1.8785064 -3.6364341 -5.300806 -6.1029549 -4.3078728 -1.0004752 1.7362249][-8.1430664 -7.8423548 -7.4630022 -7.3294449 -6.1620159 -3.883642 -2.1822176 -2.726438 -4.1131315 -5.1774416 -5.5568237 -5.2490139 -3.0353291 0.20920372 2.8269274][-7.8664427 -6.9822493 -6.478755 -6.4962635 -5.9028759 -4.3845367 -3.2442026 -4.0578165 -5.3562956 -5.8571248 -5.3155556 -4.3006787 -2.0013244 0.90038943 3.4073904][-7.0279894 -5.8264723 -5.5499573 -5.7251024 -5.4969568 -4.501091 -3.8196335 -4.6342573 -5.6561871 -5.9433603 -4.9914451 -3.5929914 -1.3310783 1.4230082 3.756057][-6.3623009 -5.0514641 -5.093308 -5.4145155 -5.3691196 -4.7141676 -4.3348656 -4.8970571 -5.3763161 -5.5258155 -4.5624685 -3.0797741 -1.017442 1.5615673 3.6371024]]...]
INFO - root - 2017-12-15 07:24:03.450842: step 21210, loss = 0.18, batch loss = 0.15 (34.3 examples/sec; 0.233 sec/batch; 20h:08m:52s remains)
INFO - root - 2017-12-15 07:24:05.696258: step 21220, loss = 0.20, batch loss = 0.17 (34.9 examples/sec; 0.230 sec/batch; 19h:50m:48s remains)
INFO - root - 2017-12-15 07:24:07.947174: step 21230, loss = 0.22, batch loss = 0.19 (34.7 examples/sec; 0.231 sec/batch; 19h:56m:48s remains)
INFO - root - 2017-12-15 07:24:10.248161: step 21240, loss = 0.33, batch loss = 0.30 (34.5 examples/sec; 0.232 sec/batch; 20h:03m:14s remains)
INFO - root - 2017-12-15 07:24:12.489761: step 21250, loss = 0.22, batch loss = 0.19 (35.8 examples/sec; 0.223 sec/batch; 19h:18m:30s remains)
INFO - root - 2017-12-15 07:24:14.752472: step 21260, loss = 0.25, batch loss = 0.22 (36.0 examples/sec; 0.222 sec/batch; 19h:11m:58s remains)
INFO - root - 2017-12-15 07:24:17.013506: step 21270, loss = 0.33, batch loss = 0.30 (36.2 examples/sec; 0.221 sec/batch; 19h:06m:04s remains)
INFO - root - 2017-12-15 07:24:19.299352: step 21280, loss = 0.31, batch loss = 0.27 (35.1 examples/sec; 0.228 sec/batch; 19h:43m:21s remains)
INFO - root - 2017-12-15 07:24:21.574536: step 21290, loss = 0.21, batch loss = 0.18 (34.9 examples/sec; 0.229 sec/batch; 19h:48m:22s remains)
INFO - root - 2017-12-15 07:24:23.831740: step 21300, loss = 0.28, batch loss = 0.25 (36.1 examples/sec; 0.222 sec/batch; 19h:08m:57s remains)
2017-12-15 07:24:24.121683: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7082143 -6.7546744 -6.6799397 -6.3788385 -6.0949225 -5.9010582 -5.4491711 -5.1834559 -5.8635025 -6.6192074 -6.6718678 -5.7781782 -4.4588518 -3.1651745 -2.0668221][-4.7720804 -6.2510257 -6.4057164 -6.224175 -5.7828226 -5.3935776 -4.6759396 -4.3286166 -4.9675283 -5.7929454 -6.1866469 -5.5946455 -4.5765629 -3.6555467 -2.5900211][-4.3967671 -5.1893411 -5.510787 -5.4664946 -4.9072638 -4.3257418 -3.4249868 -3.0291641 -3.5649471 -4.3293467 -5.0108223 -4.8889914 -4.2782173 -3.7049916 -2.6941364][-4.0471969 -4.1423559 -4.5599308 -4.4953394 -3.6274426 -2.6093771 -1.4022666 -1.0483147 -1.7099199 -2.6738882 -3.8223915 -4.3447638 -4.1183825 -3.7380924 -2.7840514][-4.6344819 -4.1023765 -4.3177004 -3.9204006 -2.6810918 -1.3250574 -0.068316221 0.10721898 -0.66212726 -1.6904944 -3.1225715 -4.1132975 -4.146028 -3.7832184 -2.8676014][-5.10715 -4.1776042 -4.1625552 -3.6604834 -2.4004912 -0.9490068 0.22854471 0.3724339 -0.31003654 -1.2758077 -2.7536607 -4.0027938 -4.1919508 -3.7985482 -2.9166746][-4.0911951 -2.8581784 -2.5393112 -2.0869493 -1.1447474 0.19018126 1.1335931 1.1801829 0.378613 -0.66580188 -2.1683021 -3.5894003 -3.8895426 -3.5571313 -2.8527718][-2.2757425 -0.66591585 -0.32431662 -0.31007457 -0.086363554 0.76302886 1.2423842 0.942178 -0.1954689 -1.2410309 -2.3736432 -3.5331945 -3.6741714 -3.2591159 -2.6903198][-0.71631205 0.61815524 0.57018971 0.065232515 -0.27517724 0.13306665 0.083559275 -0.63554609 -2.0594087 -2.9908283 -3.5388644 -4.0416045 -3.7467847 -3.03121 -2.5009224][-0.64587128 0.25426149 -0.2132771 -1.0056536 -1.4613991 -1.1699189 -1.474737 -2.3364291 -3.7387342 -4.4442682 -4.4513354 -4.4391613 -3.8187938 -2.8327007 -2.3338122][-1.2611713 -0.58921826 -1.1749461 -2.0126333 -2.3815143 -2.211719 -2.7447972 -3.7181375 -5.0746174 -5.5260458 -5.04895 -4.6813326 -3.8937931 -2.7253048 -2.2712431][-2.6124203 -2.4632449 -3.1297269 -3.9723055 -4.3043957 -4.246388 -4.6677861 -5.1597142 -5.9412642 -6.0102754 -5.1950274 -4.6405773 -3.9399037 -2.7665532 -2.3162186][-4.8324718 -4.877893 -5.2971544 -5.9273415 -6.1358757 -6.1098719 -6.4004135 -6.255969 -6.2602372 -5.997571 -4.9559479 -4.2657423 -3.7193363 -2.7963512 -2.4169977][-6.677846 -6.4329057 -6.4745746 -6.8496518 -6.8105688 -6.7697783 -7.208674 -6.8907065 -6.4808989 -6.1308956 -5.0383673 -4.2278886 -3.6599164 -2.8769388 -2.4880085][-6.8534265 -6.3564024 -6.1553659 -6.3072739 -6.1047449 -6.2169571 -7.1308942 -7.1502175 -6.7465687 -6.5837722 -5.6786942 -4.7517967 -3.9037032 -2.9984851 -2.4653831]]...]
INFO - root - 2017-12-15 07:24:26.387075: step 21310, loss = 0.20, batch loss = 0.17 (35.9 examples/sec; 0.223 sec/batch; 19h:14m:16s remains)
INFO - root - 2017-12-15 07:24:28.700426: step 21320, loss = 0.34, batch loss = 0.31 (35.4 examples/sec; 0.226 sec/batch; 19h:31m:40s remains)
INFO - root - 2017-12-15 07:24:30.977653: step 21330, loss = 0.23, batch loss = 0.20 (35.4 examples/sec; 0.226 sec/batch; 19h:30m:50s remains)
INFO - root - 2017-12-15 07:24:33.234585: step 21340, loss = 0.24, batch loss = 0.21 (36.2 examples/sec; 0.221 sec/batch; 19h:06m:38s remains)
INFO - root - 2017-12-15 07:24:35.496049: step 21350, loss = 0.30, batch loss = 0.26 (36.0 examples/sec; 0.222 sec/batch; 19h:11m:16s remains)
INFO - root - 2017-12-15 07:24:37.742091: step 21360, loss = 0.21, batch loss = 0.17 (35.7 examples/sec; 0.224 sec/batch; 19h:21m:44s remains)
INFO - root - 2017-12-15 07:24:40.043554: step 21370, loss = 0.19, batch loss = 0.16 (36.6 examples/sec; 0.218 sec/batch; 18h:51m:54s remains)
INFO - root - 2017-12-15 07:24:42.353318: step 21380, loss = 0.28, batch loss = 0.25 (32.6 examples/sec; 0.245 sec/batch; 21h:12m:03s remains)
INFO - root - 2017-12-15 07:24:44.641159: step 21390, loss = 0.41, batch loss = 0.38 (34.6 examples/sec; 0.231 sec/batch; 20h:00m:03s remains)
INFO - root - 2017-12-15 07:24:46.922377: step 21400, loss = 0.23, batch loss = 0.19 (35.3 examples/sec; 0.227 sec/batch; 19h:35m:39s remains)
2017-12-15 07:24:47.232231: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.4486861 -8.33268 -7.9307055 -7.3072071 -6.4471135 -5.992867 -6.1832585 -6.4100313 -6.4521718 -6.50879 -7.0338588 -7.7881665 -8.4283447 -8.3889847 -7.928174][-10.250534 -9.5057373 -8.9297256 -8.0912046 -6.7798047 -6.0531759 -6.1628189 -6.37221 -6.6206322 -7.0659 -7.9045377 -8.7193613 -9.3885441 -9.4525623 -8.8160934][-11.355915 -9.5919323 -8.9984322 -7.9945335 -6.3593454 -5.2052865 -4.7454205 -4.6334343 -5.2066402 -6.2268314 -7.4824457 -8.4592543 -9.31542 -9.5471287 -8.71538][-11.650515 -9.264204 -8.8343906 -7.7668595 -5.9090223 -4.21517 -2.8448932 -2.0732975 -3.0010643 -4.6920595 -6.3252358 -7.64245 -8.8276672 -9.1076431 -8.1567726][-9.9306164 -7.0135655 -6.7710414 -5.7925072 -3.9504642 -1.9709936 0.11594343 1.4174542 0.0001244545 -2.3739452 -4.2678928 -5.7333593 -6.8791928 -6.8889308 -5.8491907][-6.65647 -3.9193044 -3.8883536 -3.0781622 -1.479447 0.58075047 3.2537003 4.9650931 3.1255765 0.18097067 -2.1030838 -3.7605395 -4.7651968 -4.4962387 -3.5682752][-4.189157 -2.1963525 -2.1406429 -1.2898424 0.28320503 2.6476555 5.846745 7.898211 5.8670192 2.4536905 -0.3092159 -2.4798458 -3.6304862 -3.3859167 -2.8976824][-4.0347843 -2.0982316 -1.7465436 -0.71532989 1.1070695 3.6647921 6.7460141 8.6722088 6.6502624 2.9527082 -0.28123045 -2.7989631 -4.0138612 -3.8749745 -3.6267252][-5.3653812 -3.3851185 -2.7417235 -1.6336098 0.1529336 2.3873439 4.8994589 6.3465209 4.5892324 1.0364466 -2.4059741 -4.7907305 -5.7094779 -5.5957108 -5.3721733][-7.4104514 -5.3404112 -4.6493387 -3.6774747 -2.1480267 -0.44826102 1.2825789 2.2652493 1.062376 -1.8494473 -4.9022694 -6.8934231 -7.4514437 -7.3677645 -7.2165713][-9.2657776 -7.1601524 -6.4567394 -5.5934758 -4.3790665 -3.3939981 -2.636683 -2.0152104 -2.5349655 -4.4872904 -6.6820331 -8.14583 -8.5866432 -8.7023125 -8.5717926][-9.951725 -7.8416467 -7.3690581 -6.8170323 -6.0990515 -5.7975268 -5.6203713 -5.1299396 -5.1061535 -6.1480188 -7.4041758 -8.3057995 -8.7369232 -9.0200644 -8.9085026][-9.9968119 -7.9674363 -7.7621131 -7.5183716 -7.199832 -7.1772037 -7.0882025 -6.5727091 -6.34239 -6.8444982 -7.5095124 -8.19409 -8.6132269 -8.8066826 -8.6737776][-9.1212187 -7.2835374 -7.2350178 -7.1550579 -7.0345235 -7.0638137 -6.92439 -6.4170413 -6.172925 -6.3947811 -6.7821722 -7.4022737 -7.8223205 -7.8476825 -7.6749611][-7.5912576 -6.1166553 -6.1583471 -6.14005 -6.1845856 -6.2931342 -6.05376 -5.57786 -5.3954887 -5.448101 -5.6577258 -6.1339741 -6.425004 -6.3659916 -6.3311944]]...]
INFO - root - 2017-12-15 07:24:49.527403: step 21410, loss = 0.22, batch loss = 0.19 (36.7 examples/sec; 0.218 sec/batch; 18h:50m:30s remains)
INFO - root - 2017-12-15 07:24:51.797887: step 21420, loss = 0.25, batch loss = 0.22 (35.0 examples/sec; 0.228 sec/batch; 19h:43m:23s remains)
INFO - root - 2017-12-15 07:24:54.082573: step 21430, loss = 0.24, batch loss = 0.20 (33.3 examples/sec; 0.240 sec/batch; 20h:45m:09s remains)
INFO - root - 2017-12-15 07:24:56.385722: step 21440, loss = 0.24, batch loss = 0.21 (35.2 examples/sec; 0.227 sec/batch; 19h:37m:54s remains)
INFO - root - 2017-12-15 07:24:58.640758: step 21450, loss = 0.26, batch loss = 0.23 (35.2 examples/sec; 0.227 sec/batch; 19h:37m:11s remains)
INFO - root - 2017-12-15 07:25:00.933628: step 21460, loss = 0.21, batch loss = 0.18 (34.9 examples/sec; 0.229 sec/batch; 19h:47m:18s remains)
INFO - root - 2017-12-15 07:25:03.195629: step 21470, loss = 0.24, batch loss = 0.20 (35.3 examples/sec; 0.227 sec/batch; 19h:36m:12s remains)
INFO - root - 2017-12-15 07:25:05.472038: step 21480, loss = 0.22, batch loss = 0.18 (34.4 examples/sec; 0.233 sec/batch; 20h:06m:51s remains)
INFO - root - 2017-12-15 07:25:07.752305: step 21490, loss = 0.18, batch loss = 0.15 (34.8 examples/sec; 0.230 sec/batch; 19h:52m:11s remains)
INFO - root - 2017-12-15 07:25:10.022890: step 21500, loss = 0.27, batch loss = 0.24 (35.8 examples/sec; 0.224 sec/batch; 19h:19m:37s remains)
2017-12-15 07:25:10.312978: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9920235 -3.942997 -5.13472 -5.9459872 -5.4000683 -3.8855629 -2.610486 -2.0532207 -2.3793356 -3.5629039 -5.0524197 -6.7772846 -8.0150824 -7.4679718 -5.5842328][-2.9316776 -3.4846005 -4.4266124 -4.8774462 -4.4303241 -2.9245529 -1.9491041 -1.7033944 -2.3093481 -3.6931911 -5.40268 -7.1391354 -8.2294874 -7.6312094 -5.829298][-2.6848891 -2.2188549 -2.7460973 -2.9352582 -2.721205 -1.4785167 -0.8007524 -0.77040124 -1.4534266 -2.874814 -4.7325754 -6.5072508 -7.5112486 -7.065177 -5.518671][-2.5230393 -0.60789239 -0.60851896 -0.60886979 -0.78752756 -0.017581224 0.44512057 0.37309742 -0.34818172 -1.7621686 -3.8668504 -5.8705812 -7.0099506 -6.7391758 -5.4006376][-2.9291937 0.19053292 0.47741413 0.61171675 0.39521837 0.95656157 1.2487113 1.0595906 0.27318382 -1.0891489 -3.3757505 -5.6207619 -6.7944078 -6.5192871 -5.2713413][-3.6198869 0.177814 0.61894059 0.88381195 0.92926311 1.607619 1.947171 1.6965737 0.73690367 -0.78298533 -3.2725618 -5.6929264 -6.8354759 -6.5323591 -5.2387428][-3.7184956 -0.50877678 0.018029213 0.34106278 0.82926655 1.8720872 2.5865247 2.6654403 1.7186289 -0.026894093 -2.8092976 -5.4499393 -6.7852526 -6.5976582 -5.3886905][-4.7145891 -2.0836234 -1.4413071 -0.96864712 -0.048945427 1.4510028 2.6573365 3.2210815 2.6118691 0.71885371 -2.2488804 -4.9749675 -6.6125708 -6.6993685 -5.6158552][-6.1216969 -4.003273 -3.3518753 -2.8596039 -1.6104968 0.32916498 1.992347 2.9408453 2.6144826 0.71611071 -2.22268 -4.7247996 -6.4760857 -6.8327227 -5.8059311][-7.5558958 -6.100554 -5.8010988 -5.6235495 -4.2948227 -2.0557733 -0.0419569 1.2381539 1.2258618 -0.44495571 -2.8270051 -4.6410484 -6.3347464 -6.8815374 -5.8758068][-8.4398823 -7.3307753 -7.4732456 -7.9304266 -6.8275461 -4.4624519 -2.327877 -1.05685 -0.86466861 -2.0353673 -3.5945802 -4.7010841 -6.2875109 -6.88019 -5.962203][-8.1520271 -7.007947 -7.5233116 -8.5798025 -7.7738371 -5.6385159 -3.7388468 -2.6865947 -2.438262 -3.3671706 -4.2831821 -4.8874674 -6.4065962 -7.0703526 -6.2680626][-6.2916727 -5.0196123 -5.9706354 -7.6899519 -7.2327614 -5.3508158 -3.730135 -3.0354471 -3.0143349 -4.057086 -4.63929 -5.18359 -6.8300686 -7.5680008 -6.8078051][-3.4178114 -2.0553849 -3.5226402 -5.8275166 -5.6109972 -3.836868 -2.4271286 -2.1438336 -2.463634 -3.8742342 -4.5513096 -5.3888741 -7.2150431 -8.0197945 -7.2686949][-1.1467203 0.25087714 -1.6312428 -4.2418571 -4.225821 -2.6906393 -1.5040439 -1.4324481 -2.062238 -3.9755211 -4.8543983 -5.7703466 -7.4900589 -8.2018366 -7.3652525]]...]
INFO - root - 2017-12-15 07:25:12.558404: step 21510, loss = 0.20, batch loss = 0.17 (35.7 examples/sec; 0.224 sec/batch; 19h:20m:16s remains)
INFO - root - 2017-12-15 07:25:14.831218: step 21520, loss = 0.27, batch loss = 0.24 (34.4 examples/sec; 0.233 sec/batch; 20h:05m:20s remains)
INFO - root - 2017-12-15 07:25:17.086114: step 21530, loss = 0.20, batch loss = 0.17 (36.3 examples/sec; 0.221 sec/batch; 19h:03m:24s remains)
INFO - root - 2017-12-15 07:25:19.345926: step 21540, loss = 0.22, batch loss = 0.19 (34.4 examples/sec; 0.233 sec/batch; 20h:05m:03s remains)
INFO - root - 2017-12-15 07:25:21.613713: step 21550, loss = 0.19, batch loss = 0.16 (35.4 examples/sec; 0.226 sec/batch; 19h:29m:44s remains)
INFO - root - 2017-12-15 07:25:23.908003: step 21560, loss = 0.23, batch loss = 0.20 (34.7 examples/sec; 0.231 sec/batch; 19h:55m:42s remains)
INFO - root - 2017-12-15 07:25:26.179242: step 21570, loss = 0.25, batch loss = 0.22 (35.0 examples/sec; 0.228 sec/batch; 19h:43m:55s remains)
INFO - root - 2017-12-15 07:25:28.446229: step 21580, loss = 0.23, batch loss = 0.20 (34.7 examples/sec; 0.231 sec/batch; 19h:54m:39s remains)
INFO - root - 2017-12-15 07:25:30.690288: step 21590, loss = 0.24, batch loss = 0.21 (36.0 examples/sec; 0.222 sec/batch; 19h:11m:47s remains)
INFO - root - 2017-12-15 07:25:32.935208: step 21600, loss = 0.16, batch loss = 0.13 (35.5 examples/sec; 0.225 sec/batch; 19h:26m:51s remains)
2017-12-15 07:25:33.211670: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7707477 -6.6171436 -6.0210376 -5.3036613 -4.4435039 -3.6672831 -3.0307088 -3.1850712 -4.0451732 -4.8761072 -5.5780392 -6.260148 -5.8554621 -4.9499254 -4.1483603][-4.9714532 -5.730567 -5.0149937 -4.3520093 -3.5825505 -2.9768348 -2.718677 -3.1300445 -4.2809868 -5.4728203 -6.4331803 -7.2765985 -6.9497008 -5.9526525 -4.9308777][-3.9568052 -4.0329294 -3.5001569 -3.0279789 -2.256809 -1.7570609 -1.9126453 -2.678508 -4.1517177 -5.6425705 -6.7712731 -7.7008896 -7.4383755 -6.3193569 -5.092463][-2.8001137 -2.2680883 -2.0063396 -1.7451954 -0.91917109 -0.42190158 -0.86469483 -1.8936948 -3.6956587 -5.6025896 -7.0471053 -8.1497326 -8.0478325 -6.834754 -5.3897939][-1.8108561 -0.60950506 -0.5340234 -0.36957073 0.62340832 1.2172909 0.65765166 -0.42724586 -2.3798892 -4.669764 -6.5467334 -8.0555668 -8.3085861 -7.1887722 -5.6506815][-0.49289656 1.0820074 0.969887 1.1413431 2.293304 2.9155164 2.2906637 1.0718939 -1.0496483 -3.6741281 -5.8400846 -7.6645627 -8.17787 -7.2163415 -5.7608037][0.4592402 1.924859 1.8982773 2.2538323 3.6111684 4.2817979 3.6372957 2.1685524 -0.13790011 -2.9420638 -5.2232895 -7.20689 -7.7888651 -6.9424953 -5.6547666][-0.39999843 1.2592449 1.4277902 2.0334911 3.6657033 4.6880217 4.2616143 2.6107168 0.2397244 -2.5566335 -4.8041577 -6.8742981 -7.4622693 -6.5964551 -5.4482884][-1.7542393 -0.12400866 0.049017668 0.68624783 2.4624658 3.8587465 3.7899818 2.0848074 -0.11241174 -2.6021197 -4.6714563 -6.7144656 -7.1594477 -6.1800785 -5.1726103][-3.0634584 -1.5098388 -1.3787494 -0.91956615 0.64962053 2.1036191 2.2413712 0.67734933 -1.1336386 -3.1060166 -4.8756285 -6.6945868 -6.9395838 -5.9050159 -5.0881243][-4.6851063 -3.4083221 -3.4706197 -3.3017817 -2.056391 -0.64447653 -0.37103403 -1.6064368 -2.8723872 -4.2168436 -5.5659657 -7.0049047 -7.0108652 -5.941555 -5.1606789][-6.7101073 -5.8062921 -6.17109 -6.2195435 -5.1986761 -3.7713127 -3.2889185 -4.1173363 -4.931663 -5.7729921 -6.6628981 -7.6111679 -7.2664957 -6.0575352 -5.2292304][-7.57027 -6.8358474 -7.1944008 -7.2774839 -6.5337849 -5.4051161 -4.9970145 -5.6765041 -6.3568368 -7.009408 -7.6611152 -8.2336826 -7.5737004 -6.214694 -5.2964292][-6.4038277 -5.7006445 -6.0124741 -6.1264749 -5.7484403 -5.0360746 -4.8023739 -5.4352112 -6.2112293 -6.970499 -7.5897713 -8.05913 -7.4321589 -6.1737041 -5.2643185][-3.7379379 -3.0615165 -3.3964071 -3.581193 -3.4982524 -3.1056662 -3.0051055 -3.7048306 -4.7496939 -5.8244629 -6.6607661 -7.3324666 -7.0145016 -5.9478827 -5.0585957]]...]
INFO - root - 2017-12-15 07:25:35.481341: step 21610, loss = 0.23, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 19h:10m:11s remains)
INFO - root - 2017-12-15 07:25:37.769717: step 21620, loss = 0.30, batch loss = 0.26 (35.7 examples/sec; 0.224 sec/batch; 19h:20m:48s remains)
INFO - root - 2017-12-15 07:25:40.077955: step 21630, loss = 0.36, batch loss = 0.33 (31.0 examples/sec; 0.258 sec/batch; 22h:17m:22s remains)
INFO - root - 2017-12-15 07:25:42.341519: step 21640, loss = 0.24, batch loss = 0.20 (34.6 examples/sec; 0.232 sec/batch; 19h:59m:24s remains)
INFO - root - 2017-12-15 07:25:44.617017: step 21650, loss = 0.20, batch loss = 0.17 (34.2 examples/sec; 0.234 sec/batch; 20h:12m:28s remains)
INFO - root - 2017-12-15 07:25:46.884918: step 21660, loss = 0.22, batch loss = 0.19 (36.2 examples/sec; 0.221 sec/batch; 19h:04m:41s remains)
INFO - root - 2017-12-15 07:25:49.181109: step 21670, loss = 0.22, batch loss = 0.19 (34.8 examples/sec; 0.230 sec/batch; 19h:50m:52s remains)
INFO - root - 2017-12-15 07:25:51.441865: step 21680, loss = 0.20, batch loss = 0.17 (36.3 examples/sec; 0.220 sec/batch; 19h:00m:31s remains)
INFO - root - 2017-12-15 07:25:53.681910: step 21690, loss = 0.20, batch loss = 0.16 (36.2 examples/sec; 0.221 sec/batch; 19h:04m:11s remains)
INFO - root - 2017-12-15 07:25:55.964120: step 21700, loss = 0.38, batch loss = 0.35 (34.9 examples/sec; 0.229 sec/batch; 19h:47m:00s remains)
2017-12-15 07:25:56.256637: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3171549 -2.8482523 -2.7058473 -3.3355691 -3.600513 -3.7221065 -4.3463764 -4.5785828 -4.2429495 -4.3029509 -4.5994949 -4.2750072 -3.5524201 -3.1806242 -3.0477936][-2.2088182 -2.25218 -2.1830647 -2.5399156 -2.7747724 -3.0004697 -3.744204 -4.1329947 -3.9308691 -4.12481 -4.5682831 -4.2471018 -3.3066609 -2.6100273 -2.240752][-2.0839133 -2.0249631 -2.1259558 -2.2259071 -2.1935174 -2.3290117 -3.0076044 -3.5621738 -3.595417 -4.0933943 -4.8642478 -4.742816 -3.6838117 -2.6646726 -1.763083][-2.5446723 -2.2238355 -2.6115086 -2.4519734 -1.9195431 -1.6160989 -1.7987282 -2.2064159 -2.3819299 -3.1701791 -4.3600435 -4.4636264 -3.4073696 -2.1798577 -0.77221715][-2.636843 -1.9561845 -2.6262231 -2.3723066 -1.3011569 -0.44856 0.14576817 0.27009058 0.065383673 -1.1736764 -2.9224637 -3.3309383 -2.4537079 -1.2191324 0.5037744][-2.1330764 -1.6863642 -2.443188 -2.0755978 -0.56488729 0.76293921 1.9706147 2.615854 2.4818165 0.83897424 -1.3739238 -2.0655298 -1.5119026 -0.454067 1.3999438][-1.8434978 -1.747622 -2.3606365 -1.9221469 -0.19297528 1.4546545 2.9945133 3.8907588 3.7101071 1.866616 -0.44460166 -1.1273943 -0.91694784 -0.14043069 1.6252041][-1.8381717 -1.735661 -2.1435585 -1.7277732 -0.03996563 1.7630045 3.3911493 4.3097963 3.8592389 1.9119203 -0.3477025 -1.0169024 -1.1835861 -0.71916485 0.65190458][-1.957804 -1.8948073 -2.0609357 -1.8257813 -0.48302841 1.1183951 2.5188658 3.1872451 2.635165 0.84241056 -1.072541 -1.6407988 -2.0959063 -1.9453007 -0.98902512][-2.4923518 -2.3782735 -2.5002694 -2.6514966 -1.9039825 -0.71240366 0.36455607 0.77032828 0.23364234 -1.1698991 -2.4220738 -2.5965915 -3.0070231 -3.0425727 -2.3430865][-2.7075989 -2.6465669 -2.9535866 -3.6684873 -3.5078716 -2.6803193 -1.7097033 -1.2784014 -1.8039906 -2.99314 -3.7909415 -3.7299485 -4.0116506 -4.1375904 -3.6158116][-2.5866461 -2.760062 -3.3335748 -4.3838873 -4.5917826 -4.103631 -3.2474141 -2.8722594 -3.3806736 -4.4205775 -4.9993191 -4.9600449 -5.2308912 -5.3699684 -4.8534102][-2.6709876 -2.920383 -3.8138909 -5.1132059 -5.61647 -5.3842821 -4.64334 -4.3615079 -4.7357254 -5.4622588 -5.6780481 -5.4417391 -5.5482626 -5.5943804 -4.9862528][-2.5702183 -2.8436546 -4.0460224 -5.5310965 -6.1502252 -5.9078827 -5.1953812 -4.7832437 -4.9445877 -5.3094578 -5.2229567 -4.8427863 -4.82359 -4.7851768 -4.2231655][-2.4037197 -2.6932764 -4.003891 -5.312418 -5.6528625 -5.1858492 -4.5472231 -4.1472273 -4.2433515 -4.4445333 -4.3331776 -4.069356 -4.05676 -3.9022388 -3.3886383]]...]
INFO - root - 2017-12-15 07:25:58.537630: step 21710, loss = 0.29, batch loss = 0.26 (36.2 examples/sec; 0.221 sec/batch; 19h:04m:57s remains)
INFO - root - 2017-12-15 07:26:00.803881: step 21720, loss = 0.21, batch loss = 0.18 (35.4 examples/sec; 0.226 sec/batch; 19h:29m:08s remains)
INFO - root - 2017-12-15 07:26:03.066326: step 21730, loss = 0.23, batch loss = 0.20 (35.5 examples/sec; 0.225 sec/batch; 19h:25m:54s remains)
INFO - root - 2017-12-15 07:26:05.330352: step 21740, loss = 0.31, batch loss = 0.28 (35.3 examples/sec; 0.227 sec/batch; 19h:34m:06s remains)
INFO - root - 2017-12-15 07:26:07.612088: step 21750, loss = 0.25, batch loss = 0.22 (33.3 examples/sec; 0.240 sec/batch; 20h:44m:32s remains)
INFO - root - 2017-12-15 07:26:09.863922: step 21760, loss = 0.23, batch loss = 0.20 (36.4 examples/sec; 0.220 sec/batch; 18h:58m:29s remains)
INFO - root - 2017-12-15 07:26:12.112101: step 21770, loss = 0.22, batch loss = 0.18 (35.4 examples/sec; 0.226 sec/batch; 19h:30m:01s remains)
INFO - root - 2017-12-15 07:26:14.372020: step 21780, loss = 0.19, batch loss = 0.16 (36.0 examples/sec; 0.222 sec/batch; 19h:09m:23s remains)
INFO - root - 2017-12-15 07:26:16.622961: step 21790, loss = 0.22, batch loss = 0.19 (34.2 examples/sec; 0.234 sec/batch; 20h:10m:06s remains)
INFO - root - 2017-12-15 07:26:18.909624: step 21800, loss = 0.23, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 19h:11m:17s remains)
2017-12-15 07:26:19.214724: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4050477 -3.3206258 -3.1158152 -1.6262954 -0.30084312 -0.19813108 -0.60827971 -1.0860405 -1.5893431 -2.374547 -3.2335477 -4.0839972 -4.3935509 -3.8871655 -3.6309619][-3.4357138 -2.8357935 -2.6156344 -0.95142567 0.29971242 0.23018885 -0.19072223 -0.52500176 -1.0272344 -2.0815966 -3.033921 -4.1532025 -4.8480778 -4.5001888 -4.4668183][-3.1377578 -2.1354752 -1.9702682 -0.41391444 0.57695246 0.43217206 0.27081251 0.27168226 -0.072933674 -1.2137295 -2.4185061 -3.9221611 -5.1161232 -5.0097017 -5.156332][-2.397929 -1.1924437 -1.1947904 0.090901375 0.93315077 0.85579443 0.7746408 0.85077143 0.50681591 -0.66427803 -2.1140535 -3.9098916 -5.1727366 -5.027544 -4.9909358][-2.6527414 -0.83367908 -0.9114188 -0.0676496 0.535522 0.78599238 1.0539153 1.2557726 1.0627508 -0.10780287 -1.6719891 -3.6257818 -4.733285 -4.5488129 -4.34758][-3.4567485 -1.3811357 -1.0665964 -0.36626303 0.341367 1.129307 1.7156768 1.8073177 1.4662528 0.3926146 -1.12498 -3.109333 -4.01634 -3.9041264 -3.6401243][-4.0565052 -1.929458 -1.1986705 -0.47218215 0.57034087 1.7725329 2.2626681 2.0223026 1.2860286 0.1283958 -1.273495 -2.9805927 -3.4723129 -3.2552664 -2.989723][-4.8554292 -2.3500996 -1.1911323 -0.37237942 0.68488955 1.8679695 2.261879 1.8387928 0.94271731 -0.14318871 -1.3082449 -2.4652092 -2.559139 -2.2762671 -1.9790142][-5.372777 -2.6788011 -1.1952692 -0.060184717 1.2147884 2.1366868 2.135819 1.5154154 0.54580212 -0.42671883 -1.3454863 -2.0858428 -1.9521095 -1.7132294 -1.4645007][-5.0935349 -2.3985121 -0.98339248 0.34606194 1.6709228 2.0856423 1.3936822 0.51428747 -0.48984683 -1.4147942 -2.16441 -2.5286527 -2.2921865 -1.9703459 -1.665092][-4.4485493 -1.8512751 -0.69447815 0.36129832 1.2223423 0.99971032 -0.17598057 -1.2758737 -2.1908369 -2.9313049 -3.4680157 -3.4639018 -3.1856654 -2.7015367 -2.2268579][-3.6225986 -1.201448 -0.3964144 0.16146064 0.43449426 -0.25045133 -1.5649952 -2.8071027 -3.7153087 -4.3777819 -4.820663 -4.7101393 -4.4710994 -3.8291547 -3.2087338][-3.1983495 -0.95302463 -0.45210922 -0.19030285 -0.42076743 -1.4819849 -2.8246427 -3.9928727 -4.8012667 -5.3557158 -5.6899128 -5.5302067 -5.32124 -4.5692167 -3.8649955][-2.9022956 -0.77047169 -0.62741137 -0.63319278 -1.1690569 -2.4629371 -3.7667196 -4.694129 -5.1950493 -5.59239 -5.8638554 -5.6997795 -5.5636921 -4.8877439 -4.2451382][-2.5914133 -0.597463 -0.81531703 -1.0696461 -1.7086726 -3.0458498 -4.1345973 -4.721786 -4.9667454 -5.3699408 -5.7567282 -5.6500716 -5.5476131 -5.0120091 -4.5178308]]...]
INFO - root - 2017-12-15 07:26:21.445712: step 21810, loss = 0.23, batch loss = 0.19 (36.6 examples/sec; 0.218 sec/batch; 18h:50m:22s remains)
INFO - root - 2017-12-15 07:26:23.761345: step 21820, loss = 0.22, batch loss = 0.18 (34.1 examples/sec; 0.235 sec/batch; 20h:16m:08s remains)
INFO - root - 2017-12-15 07:26:26.060918: step 21830, loss = 0.18, batch loss = 0.15 (34.6 examples/sec; 0.231 sec/batch; 19h:55m:52s remains)
INFO - root - 2017-12-15 07:26:28.302419: step 21840, loss = 0.23, batch loss = 0.20 (35.1 examples/sec; 0.228 sec/batch; 19h:40m:39s remains)
INFO - root - 2017-12-15 07:26:30.573848: step 21850, loss = 0.25, batch loss = 0.22 (34.5 examples/sec; 0.232 sec/batch; 19h:59m:30s remains)
INFO - root - 2017-12-15 07:26:32.839882: step 21860, loss = 0.21, batch loss = 0.18 (35.8 examples/sec; 0.223 sec/batch; 19h:17m:07s remains)
INFO - root - 2017-12-15 07:26:35.101329: step 21870, loss = 0.19, batch loss = 0.16 (35.6 examples/sec; 0.224 sec/batch; 19h:22m:13s remains)
INFO - root - 2017-12-15 07:26:37.330992: step 21880, loss = 0.25, batch loss = 0.22 (36.5 examples/sec; 0.219 sec/batch; 18h:54m:37s remains)
INFO - root - 2017-12-15 07:26:39.622579: step 21890, loss = 0.31, batch loss = 0.28 (35.9 examples/sec; 0.223 sec/batch; 19h:14m:05s remains)
INFO - root - 2017-12-15 07:26:41.891893: step 21900, loss = 0.20, batch loss = 0.17 (35.2 examples/sec; 0.227 sec/batch; 19h:37m:22s remains)
2017-12-15 07:26:42.177902: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6979361 -4.3702803 -3.6115832 -3.3374848 -2.867758 -2.779793 -3.3872228 -3.4317565 -3.0243092 -2.7172494 -3.2092085 -3.1582332 -2.7212613 -2.6757233 -3.1296458][-3.582443 -3.4438493 -2.9346404 -2.9880817 -2.7042227 -2.8168435 -3.4846826 -3.1411614 -2.2950339 -1.816823 -2.4795418 -3.024071 -3.3502707 -3.9972682 -4.9449644][-3.4329331 -2.12302 -2.1768322 -2.8140881 -2.8737068 -3.0289516 -3.5031962 -2.74346 -1.5663147 -0.97712374 -1.8326843 -3.0020006 -4.0826263 -5.3283706 -6.4373031][-2.5146451 -0.250219 -0.74781871 -1.7636747 -2.1194868 -2.1959438 -2.6616802 -1.7917931 -0.59702957 -0.0033502579 -1.0393891 -2.7270594 -4.2905264 -5.7214341 -6.6788635][-1.5113223 1.4830029 0.71851206 -0.46898949 -0.9323926 -1.1144861 -1.7527418 -0.956002 0.15906763 0.77388287 -0.2991966 -2.1294792 -3.7908072 -5.0986338 -5.6791525][-0.57496417 2.3875861 1.5786545 0.51764154 0.1616745 0.017277002 -0.60830891 0.15675569 1.1285408 1.6652982 0.50729132 -1.3130702 -2.8532872 -3.9913616 -4.2090206][0.12227416 2.467936 1.9127927 1.2878923 1.1647325 1.1333725 0.66165662 1.4078212 2.1719122 2.4498825 1.099226 -0.61277282 -2.0584958 -3.0864232 -3.1556907][0.024247408 2.2414112 1.9707947 1.5218344 1.4545312 1.354867 0.93074656 1.5035892 2.0399952 1.9830675 0.64211607 -0.7034502 -1.9386327 -2.8733983 -2.9429519][-1.0777112 0.79640031 0.6166122 0.27581906 0.15255642 -0.017387867 -0.29049993 0.20074511 0.66682196 0.64277959 -0.229913 -1.1031429 -2.2031705 -3.1856127 -3.3166418][-2.5485833 -0.77881622 -0.86029029 -1.0193359 -1.1321583 -1.276916 -1.477252 -1.2320361 -0.81107569 -0.619146 -1.0553129 -1.7542126 -2.9009995 -3.8884413 -4.0716782][-3.0681748 -1.5201318 -1.7266195 -1.846688 -2.0263119 -2.1344209 -2.4601557 -2.50777 -2.1097147 -1.7325374 -1.914986 -2.6752057 -3.8598056 -4.660078 -4.7362261][-4.0255504 -3.0014448 -3.4494815 -3.6078422 -3.8299603 -3.8122697 -4.0217505 -3.9192595 -3.1754642 -2.3724768 -2.2163219 -3.0494258 -4.2234354 -4.8228884 -4.7815695][-4.9528031 -4.33562 -4.8709884 -5.0015993 -5.2009034 -5.0901275 -5.0671959 -4.660809 -3.5883429 -2.359592 -1.8296283 -2.6065121 -3.6572523 -4.1601229 -4.2394514][-5.1538444 -4.7366257 -5.2980833 -5.5664558 -5.8322415 -5.6600704 -5.4299841 -4.8298531 -3.7042093 -2.3457909 -1.5574811 -2.2862887 -3.2579691 -3.705091 -3.9504371][-5.4910116 -5.0172405 -5.5227876 -5.8468304 -5.9524012 -5.6653557 -5.30856 -4.6778593 -3.8135123 -2.6664913 -1.835811 -2.5076962 -3.3903358 -3.7751894 -4.0459771]]...]
INFO - root - 2017-12-15 07:26:44.437186: step 21910, loss = 0.28, batch loss = 0.25 (36.1 examples/sec; 0.222 sec/batch; 19h:07m:43s remains)
INFO - root - 2017-12-15 07:26:46.679449: step 21920, loss = 0.32, batch loss = 0.29 (34.4 examples/sec; 0.232 sec/batch; 20h:02m:59s remains)
INFO - root - 2017-12-15 07:26:48.952431: step 21930, loss = 0.19, batch loss = 0.16 (34.5 examples/sec; 0.232 sec/batch; 20h:01m:28s remains)
INFO - root - 2017-12-15 07:26:51.207688: step 21940, loss = 0.28, batch loss = 0.25 (35.8 examples/sec; 0.224 sec/batch; 19h:17m:03s remains)
INFO - root - 2017-12-15 07:26:53.476572: step 21950, loss = 0.22, batch loss = 0.19 (34.0 examples/sec; 0.235 sec/batch; 20h:16m:43s remains)
INFO - root - 2017-12-15 07:26:55.759955: step 21960, loss = 0.33, batch loss = 0.30 (34.5 examples/sec; 0.232 sec/batch; 19h:59m:16s remains)
INFO - root - 2017-12-15 07:26:58.030359: step 21970, loss = 0.24, batch loss = 0.21 (36.4 examples/sec; 0.220 sec/batch; 18h:58m:23s remains)
INFO - root - 2017-12-15 07:27:00.323195: step 21980, loss = 0.32, batch loss = 0.29 (36.0 examples/sec; 0.222 sec/batch; 19h:11m:08s remains)
INFO - root - 2017-12-15 07:27:02.601473: step 21990, loss = 0.23, batch loss = 0.20 (34.4 examples/sec; 0.232 sec/batch; 20h:02m:18s remains)
INFO - root - 2017-12-15 07:27:04.943058: step 22000, loss = 0.16, batch loss = 0.13 (33.5 examples/sec; 0.239 sec/batch; 20h:36m:41s remains)
2017-12-15 07:27:05.231786: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4457712 -4.4784012 -4.6329 -3.7913237 -3.8618846 -3.9321964 -3.6364827 -2.6429896 -2.3530741 -2.1082571 -1.119252 -0.68208408 -1.1052756 -1.5676522 -2.2413089][-3.9085286 -5.4659872 -5.8621254 -5.2558928 -5.2585349 -4.9682856 -4.1476793 -2.8314979 -2.2425551 -1.9581335 -1.1367719 -0.92372584 -1.7253339 -2.6797314 -3.2609026][-4.721715 -6.008091 -6.6584687 -6.2732644 -6.2015891 -5.5994077 -4.396327 -2.8554692 -2.3303986 -2.3213575 -1.9230523 -2.0972667 -3.0832512 -3.899539 -4.1261587][-5.7089367 -6.3414831 -6.9349113 -6.374126 -5.8835344 -4.7420363 -3.0197177 -1.4480135 -1.5194981 -2.3649158 -2.76353 -3.2477479 -4.1126232 -4.7332239 -4.57071][-6.2122355 -6.644043 -6.8945436 -5.6767893 -4.2331891 -2.0393806 0.50733948 1.9993505 0.9018333 -1.2894902 -2.835619 -3.7644846 -4.3558021 -4.5533972 -4.1374822][-6.5159292 -6.6537242 -6.4824891 -4.394104 -1.8468256 1.3506467 4.4662414 5.694941 3.7319517 0.54162121 -1.8196497 -3.1370821 -3.64287 -3.6036248 -3.2580929][-6.1298518 -6.6039476 -6.2500386 -3.5766392 -0.33339393 3.35217 6.6349549 7.7278781 5.4481516 2.1514907 -0.54868615 -2.2962296 -2.8009408 -2.6511197 -2.5168855][-6.0287294 -6.4845715 -6.1301365 -3.1740077 0.25036621 3.9025474 6.9522614 7.7832274 5.6332278 2.8926768 0.54266119 -1.1529486 -1.5769384 -1.142723 -1.0774794][-6.0381346 -6.6484394 -6.5001569 -3.8006392 -0.89965522 2.0975761 4.5087333 5.0877242 3.3204565 1.5289249 -0.094090223 -1.5708536 -1.8937323 -1.2991986 -1.0408701][-6.2800651 -6.99039 -7.153152 -5.0518255 -2.935425 -0.68317568 1.1021841 1.4632909 0.10158014 -1.0363697 -1.9956427 -3.1019685 -3.4757771 -2.6641128 -2.0652347][-6.4592071 -7.1657772 -7.658555 -6.2311697 -4.7452211 -2.9858465 -1.5910251 -1.3965242 -2.4154816 -3.0300608 -3.43089 -4.1270657 -4.4186192 -3.6292286 -2.8235121][-6.6693935 -7.4016275 -8.2501116 -7.5292668 -6.5186214 -5.1855907 -4.2893562 -4.4697342 -5.2738886 -5.6404386 -5.7023368 -6.0492983 -6.26521 -5.4698029 -4.5732126][-6.9289951 -7.5940003 -8.528923 -8.1809082 -7.4070559 -6.4406695 -5.9858046 -6.4457445 -7.1182737 -7.3159885 -7.2820334 -7.3865519 -7.4014659 -6.7685957 -6.1111097][-6.8175611 -7.259182 -8.0722847 -8.0453377 -7.6258211 -7.1319828 -6.9583187 -7.3623943 -7.7600431 -7.9192472 -8.0044079 -8.1440821 -8.0977736 -7.6121163 -7.1163354][-6.3132477 -6.4560976 -7.0768843 -7.3206091 -7.2973204 -7.1460509 -7.0872979 -7.3262663 -7.5658956 -7.722158 -7.8962474 -8.0002918 -7.8423738 -7.4734445 -7.1837573]]...]
INFO - root - 2017-12-15 07:27:07.521050: step 22010, loss = 0.20, batch loss = 0.17 (33.5 examples/sec; 0.239 sec/batch; 20h:34m:33s remains)
INFO - root - 2017-12-15 07:27:09.812004: step 22020, loss = 0.18, batch loss = 0.14 (36.1 examples/sec; 0.221 sec/batch; 19h:05m:47s remains)
INFO - root - 2017-12-15 07:27:12.075628: step 22030, loss = 0.19, batch loss = 0.16 (34.3 examples/sec; 0.234 sec/batch; 20h:08m:29s remains)
INFO - root - 2017-12-15 07:27:14.338070: step 22040, loss = 0.20, batch loss = 0.16 (36.4 examples/sec; 0.219 sec/batch; 18h:55m:40s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 07:27:16.621641: step 22050, loss = 0.25, batch loss = 0.21 (35.9 examples/sec; 0.223 sec/batch; 19h:13m:32s remains)
INFO - root - 2017-12-15 07:27:18.934436: step 22060, loss = 0.21, batch loss = 0.17 (33.5 examples/sec; 0.239 sec/batch; 20h:34m:51s remains)
INFO - root - 2017-12-15 07:27:21.235692: step 22070, loss = 0.25, batch loss = 0.22 (34.9 examples/sec; 0.229 sec/batch; 19h:45m:45s remains)
INFO - root - 2017-12-15 07:27:23.515816: step 22080, loss = 0.18, batch loss = 0.14 (33.6 examples/sec; 0.238 sec/batch; 20h:32m:26s remains)
INFO - root - 2017-12-15 07:27:25.809648: step 22090, loss = 0.33, batch loss = 0.30 (35.7 examples/sec; 0.224 sec/batch; 19h:18m:29s remains)
INFO - root - 2017-12-15 07:27:28.141557: step 22100, loss = 0.30, batch loss = 0.26 (34.8 examples/sec; 0.230 sec/batch; 19h:49m:04s remains)
2017-12-15 07:27:28.440111: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5370684 -9.7071152 -10.541845 -10.580793 -10.292631 -9.6686115 -9.1815758 -8.9624624 -8.9941874 -9.81053 -10.41914 -10.035585 -9.511425 -9.156023 -8.8167944][-7.524601 -10.660921 -11.105377 -10.567904 -9.6882153 -8.5998058 -8.0370274 -7.9487314 -8.093277 -9.2975521 -10.195522 -9.9356117 -9.5445747 -9.2717876 -9.0713406][-8.8557444 -10.710934 -10.617519 -9.3308067 -7.8383961 -6.532557 -6.0455923 -6.166647 -6.6540036 -8.2579651 -9.1301575 -8.7358332 -8.3373013 -8.0002594 -7.7045736][-9.5056524 -10.375229 -9.6148825 -7.4707661 -5.25893 -3.6656077 -3.1697807 -3.5494952 -4.699194 -6.7462 -7.6578951 -7.1710134 -6.8132033 -6.3158956 -5.7682896][-9.3286858 -9.45706 -7.9152102 -4.9924564 -2.0652606 -0.15393877 0.502285 0.021291494 -1.8779554 -4.52404 -5.8016634 -5.3152809 -4.9137988 -4.0578718 -3.0505188][-9.244278 -8.6577034 -6.6031704 -3.337249 0.013574123 2.3957567 3.6020322 3.2266951 0.80048633 -2.3627789 -3.912775 -3.5072317 -3.0663586 -1.8254917 -0.24339962][-8.6020279 -8.212286 -6.0473871 -2.9647617 0.45829225 3.3584809 5.2378373 5.3147388 2.7945127 -0.52812338 -2.3568022 -2.1374276 -1.7036656 -0.18299723 1.8006139][-8.3680019 -8.1707878 -6.1814137 -3.5946703 -0.44395471 2.5279484 4.8170934 5.3938475 3.3157635 0.15962386 -1.951075 -1.8232133 -1.245033 0.64533997 2.8031311][-8.4923372 -8.6427011 -7.0665908 -5.0359573 -2.437588 0.054363728 2.27358 3.386941 2.1706171 -0.3873266 -2.6538253 -2.683521 -1.8579067 0.28073025 2.2092123][-8.8153152 -9.3643246 -8.30562 -6.8109403 -4.8335762 -2.9385705 -0.8955766 0.64127135 0.43130112 -1.4268103 -3.9460864 -4.3469591 -3.4791982 -1.5054879 -0.10660291][-9.05921 -9.82814 -9.2493019 -8.1809244 -6.7525463 -5.5142441 -3.84896 -2.190701 -1.7657951 -3.0568233 -5.5834475 -6.1557484 -5.2054386 -3.445792 -2.542089][-9.1948442 -10.050392 -9.915329 -9.2647877 -8.4035149 -7.7095175 -6.4895234 -4.9980268 -4.1960568 -5.064559 -7.1782088 -7.6184139 -6.5790038 -5.1367912 -4.6308737][-9.14292 -10.049696 -10.346527 -10.068137 -9.5984678 -9.1887121 -8.2025261 -6.8135204 -5.8711076 -6.4404879 -8.0567017 -8.3663006 -7.5926156 -6.7199965 -6.5089641][-8.6640291 -9.4856911 -9.9916325 -9.8920164 -9.547574 -9.07914 -8.053297 -6.6875811 -5.8609438 -6.4547615 -7.7935743 -8.1284981 -7.7710156 -7.3464947 -7.2341595][-7.8478031 -8.47348 -9.0140724 -8.9354362 -8.52881 -7.9060879 -6.8533177 -5.5906076 -4.9947023 -5.6349916 -6.686995 -6.9490371 -6.7693181 -6.515697 -6.4287543]]...]
INFO - root - 2017-12-15 07:27:30.718689: step 22110, loss = 0.23, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 19h:10m:32s remains)
INFO - root - 2017-12-15 07:27:32.984647: step 22120, loss = 0.25, batch loss = 0.22 (35.2 examples/sec; 0.228 sec/batch; 19h:37m:01s remains)
INFO - root - 2017-12-15 07:27:35.294747: step 22130, loss = 0.27, batch loss = 0.24 (35.0 examples/sec; 0.229 sec/batch; 19h:42m:55s remains)
INFO - root - 2017-12-15 07:27:37.554523: step 22140, loss = 0.21, batch loss = 0.18 (34.5 examples/sec; 0.232 sec/batch; 19h:58m:53s remains)
INFO - root - 2017-12-15 07:27:39.840999: step 22150, loss = 0.27, batch loss = 0.24 (35.0 examples/sec; 0.229 sec/batch; 19h:43m:35s remains)
INFO - root - 2017-12-15 07:27:42.092353: step 22160, loss = 0.25, batch loss = 0.22 (34.7 examples/sec; 0.230 sec/batch; 19h:51m:51s remains)
INFO - root - 2017-12-15 07:27:44.362068: step 22170, loss = 0.30, batch loss = 0.26 (34.8 examples/sec; 0.230 sec/batch; 19h:49m:21s remains)
INFO - root - 2017-12-15 07:27:46.672526: step 22180, loss = 0.16, batch loss = 0.13 (33.7 examples/sec; 0.238 sec/batch; 20h:29m:13s remains)
INFO - root - 2017-12-15 07:27:48.963272: step 22190, loss = 0.21, batch loss = 0.18 (34.5 examples/sec; 0.232 sec/batch; 19h:58m:51s remains)
INFO - root - 2017-12-15 07:27:51.230361: step 22200, loss = 0.26, batch loss = 0.23 (35.7 examples/sec; 0.224 sec/batch; 19h:18m:50s remains)
2017-12-15 07:27:51.513956: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31891322 -0.3407824 -0.12141824 -0.019960642 -1.2173436 -2.8005073 -3.9216642 -4.9687524 -5.1786509 -5.0106788 -4.5213137 -3.9510922 -4.4202595 -4.7890253 -4.7944851][-0.31406856 0.063989878 -0.030999899 -0.16898322 -1.5750189 -3.0650287 -3.9464679 -5.0551915 -5.5217409 -5.5399485 -5.1464205 -4.6343865 -5.1359072 -5.4085407 -5.3331065][-1.2297032 0.051086903 -0.23072481 -0.64439976 -2.1336715 -3.3751464 -3.9384067 -4.9478436 -5.6032891 -5.9249296 -5.7793751 -5.3700495 -5.7827473 -5.84262 -5.6291122][-2.0427687 -0.12519884 -0.46214354 -1.134064 -2.7296939 -3.7580764 -4.0579424 -4.78578 -5.4465985 -6.0520034 -6.2338581 -5.9423161 -6.1443319 -5.9633193 -5.6493874][-2.9411938 -0.55078661 -0.81927681 -1.6169481 -3.1272616 -3.7914698 -3.7170749 -4.0018449 -4.5093665 -5.3405523 -6.0053253 -5.9513063 -5.9687986 -5.5558615 -5.0941219][-3.9299335 -1.4686403 -1.4150381 -1.930166 -3.0169878 -3.1426 -2.6041813 -2.4490144 -2.7398999 -3.8020296 -5.0377445 -5.413476 -5.4086533 -4.9179111 -4.3496351][-4.5990572 -2.3665016 -1.9862273 -2.0065031 -2.3848355 -1.8580242 -0.82580328 -0.164716 -0.22759485 -1.4863001 -3.3527503 -4.3211823 -4.5375557 -4.2757559 -3.708992][-5.6437621 -3.2107537 -2.6653247 -2.1813018 -1.7953805 -0.56888926 1.0646622 2.2520792 2.3700197 1.0095904 -1.2779328 -2.7532239 -3.2367213 -3.338614 -2.8756571][-6.5863791 -4.1580915 -3.5869765 -2.5833771 -1.4089471 0.44925666 2.4797347 3.8951681 3.94889 2.5455539 0.17025876 -1.6294633 -2.355572 -2.68558 -2.2898571][-6.92206 -4.5321546 -4.0405154 -2.8191855 -1.254334 0.74894023 2.7707059 4.1829948 4.065608 2.6509473 0.44916344 -1.4220726 -2.3687787 -2.8871031 -2.5733449][-7.0037394 -4.7192221 -4.3893042 -3.2907772 -1.8327541 -0.10359883 1.594892 2.8157022 2.5565135 1.291728 -0.5557729 -2.2613103 -3.2953119 -3.8756046 -3.5888546][-7.6534185 -5.4876957 -5.2579169 -4.3517432 -3.2063918 -1.8292723 -0.47929704 0.47219038 0.15982008 -0.932467 -2.3771698 -3.7795796 -4.6794391 -5.0578375 -4.6259184][-8.416297 -6.3834791 -6.2455497 -5.5842638 -4.6770754 -3.6441505 -2.6820087 -2.0276718 -2.2909517 -3.141665 -4.2050834 -5.2436457 -5.8658943 -5.9002814 -5.298439][-8.6144848 -6.7294025 -6.6798887 -6.3345022 -5.7867727 -5.1864815 -4.6207571 -4.2192307 -4.3824883 -4.9974279 -5.8086166 -6.4510069 -6.6246014 -6.2759571 -5.4603653][-8.3948183 -6.662447 -6.6820955 -6.6119833 -6.4578981 -6.2876654 -6.1689806 -5.9955907 -5.9873476 -6.3065214 -6.8809519 -7.1854906 -6.9805355 -6.3701668 -5.4216232]]...]
INFO - root - 2017-12-15 07:27:53.778384: step 22210, loss = 0.20, batch loss = 0.17 (34.7 examples/sec; 0.230 sec/batch; 19h:51m:42s remains)
INFO - root - 2017-12-15 07:27:56.092900: step 22220, loss = 0.22, batch loss = 0.19 (35.6 examples/sec; 0.225 sec/batch; 19h:23m:18s remains)
INFO - root - 2017-12-15 07:27:58.327230: step 22230, loss = 0.24, batch loss = 0.21 (35.9 examples/sec; 0.223 sec/batch; 19h:12m:59s remains)
INFO - root - 2017-12-15 07:28:00.575963: step 22240, loss = 0.19, batch loss = 0.16 (35.6 examples/sec; 0.225 sec/batch; 19h:23m:19s remains)
INFO - root - 2017-12-15 07:28:02.872214: step 22250, loss = 0.21, batch loss = 0.17 (34.1 examples/sec; 0.234 sec/batch; 20h:11m:36s remains)
INFO - root - 2017-12-15 07:28:05.170499: step 22260, loss = 0.20, batch loss = 0.16 (35.4 examples/sec; 0.226 sec/batch; 19h:29m:38s remains)
INFO - root - 2017-12-15 07:28:07.452529: step 22270, loss = 0.17, batch loss = 0.13 (35.4 examples/sec; 0.226 sec/batch; 19h:28m:02s remains)
INFO - root - 2017-12-15 07:28:09.758378: step 22280, loss = 0.23, batch loss = 0.20 (35.9 examples/sec; 0.223 sec/batch; 19h:12m:43s remains)
INFO - root - 2017-12-15 07:28:12.063543: step 22290, loss = 0.17, batch loss = 0.14 (35.4 examples/sec; 0.226 sec/batch; 19h:30m:03s remains)
INFO - root - 2017-12-15 07:28:14.322756: step 22300, loss = 0.23, batch loss = 0.20 (36.1 examples/sec; 0.222 sec/batch; 19h:06m:52s remains)
2017-12-15 07:28:14.648158: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.230567 -8.0003567 -8.1871815 -8.3585958 -8.2762547 -8.0292625 -7.4609509 -6.9837351 -6.4443045 -6.5249844 -6.6297455 -6.8810167 -7.3463154 -7.7261038 -7.4393587][-7.0721197 -8.2742157 -8.5219908 -8.6915874 -8.4429722 -7.879077 -6.9312744 -6.0442467 -5.3147526 -5.7379217 -6.1584392 -6.6469116 -7.3740416 -7.9740362 -7.7885408][-7.3226957 -7.8419161 -8.1237879 -8.2127819 -7.7982416 -6.9586487 -5.5563545 -4.300746 -3.758718 -4.828228 -5.6347885 -6.3137617 -7.2176838 -7.9687328 -7.8223619][-7.6891356 -7.1515322 -7.2495947 -7.1151838 -6.5441113 -5.3790426 -3.3976097 -1.7963514 -1.7658911 -3.5313952 -4.6351118 -5.2820768 -6.2076139 -7.25298 -7.3320761][-7.4398966 -6.1443195 -5.9676275 -5.6680937 -4.9075127 -3.3081725 -0.96156871 0.71387935 0.15757227 -2.2785683 -3.8046114 -4.3464718 -5.2764139 -6.4568377 -6.6586266][-6.7203951 -5.33193 -4.9999771 -4.5807381 -3.4230504 -1.2198697 1.4969299 3.3551853 2.4683092 -0.34846568 -2.2754848 -2.9012585 -3.9483085 -5.1952639 -5.606802][-6.0548158 -5.0370421 -4.6668549 -4.051569 -2.2406957 0.85502458 4.221159 6.5328016 5.8943443 3.1417263 1.0208998 0.1282959 -1.3345157 -2.9303916 -3.8473716][-6.2771406 -5.2610517 -4.9388075 -4.1788359 -1.9403051 1.5608346 5.0380096 7.5357094 7.1927567 4.6923437 2.5554059 1.2542558 -0.37068236 -1.9271767 -3.03899][-6.5685644 -5.6604495 -5.4920626 -4.8494945 -2.8508725 0.19994473 3.2201197 5.767086 5.9426336 4.0274105 2.1831677 0.717134 -1.0115039 -2.2956843 -3.2654004][-7.0810785 -6.282073 -6.2673712 -5.8504291 -4.4136057 -2.1816785 0.35469055 3.048898 3.7802751 2.592406 1.2413149 -0.33209836 -2.2370958 -3.5364285 -4.4314094][-7.7084913 -6.7781925 -6.7385521 -6.3755331 -5.5047464 -4.2358651 -2.4198031 -0.11385298 0.74623275 -0.020530939 -0.9842838 -2.4194729 -4.4385862 -5.7389917 -6.4151535][-7.6011267 -6.4255514 -6.3599243 -6.1491013 -5.87051 -5.639678 -4.8513012 -3.2896633 -2.5814621 -3.0944154 -3.7607427 -4.9101019 -6.7865496 -7.86034 -8.1460323][-7.2745543 -6.004632 -6.03826 -6.0085773 -6.0865421 -6.5618744 -6.6489792 -5.8239059 -5.2235055 -5.5176382 -6.0100994 -6.8325129 -8.3367538 -9.00654 -8.93084][-7.0376072 -5.6938825 -5.7523794 -5.8378825 -6.1096468 -6.9854536 -7.7910595 -7.6816282 -7.4868279 -7.7458105 -8.0611887 -8.574275 -9.47448 -9.6709948 -9.3999853][-6.2671127 -4.864337 -4.9361506 -5.1055961 -5.4794312 -6.4453831 -7.5329762 -7.8494043 -8.0166607 -8.3742237 -8.7026272 -8.98365 -9.2758732 -9.122406 -8.8997478]]...]
INFO - root - 2017-12-15 07:28:16.946630: step 22310, loss = 0.30, batch loss = 0.27 (34.1 examples/sec; 0.234 sec/batch; 20h:11m:59s remains)
INFO - root - 2017-12-15 07:28:19.250042: step 22320, loss = 0.21, batch loss = 0.18 (35.3 examples/sec; 0.227 sec/batch; 19h:32m:20s remains)
INFO - root - 2017-12-15 07:28:21.567379: step 22330, loss = 0.24, batch loss = 0.21 (35.0 examples/sec; 0.229 sec/batch; 19h:41m:47s remains)
INFO - root - 2017-12-15 07:28:23.842031: step 22340, loss = 0.39, batch loss = 0.36 (34.5 examples/sec; 0.232 sec/batch; 19h:57m:43s remains)
INFO - root - 2017-12-15 07:28:26.115092: step 22350, loss = 0.22, batch loss = 0.19 (34.8 examples/sec; 0.230 sec/batch; 19h:46m:56s remains)
INFO - root - 2017-12-15 07:28:28.396091: step 22360, loss = 0.19, batch loss = 0.15 (34.4 examples/sec; 0.233 sec/batch; 20h:02m:01s remains)
INFO - root - 2017-12-15 07:28:30.664872: step 22370, loss = 0.22, batch loss = 0.18 (34.9 examples/sec; 0.229 sec/batch; 19h:46m:09s remains)
INFO - root - 2017-12-15 07:28:32.930149: step 22380, loss = 0.16, batch loss = 0.13 (34.4 examples/sec; 0.233 sec/batch; 20h:02m:52s remains)
INFO - root - 2017-12-15 07:28:35.242865: step 22390, loss = 0.26, batch loss = 0.23 (34.4 examples/sec; 0.233 sec/batch; 20h:02m:00s remains)
INFO - root - 2017-12-15 07:28:37.525060: step 22400, loss = 0.18, batch loss = 0.15 (35.2 examples/sec; 0.227 sec/batch; 19h:34m:32s remains)
2017-12-15 07:28:37.795053: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7953348 -8.6883421 -8.1363153 -7.5884438 -6.682148 -5.9012957 -5.3640862 -5.1813564 -5.528192 -6.4669266 -7.1269827 -6.9141893 -6.9805441 -6.3831635 -4.9698253][-5.8808737 -8.5674725 -8.12833 -7.6301661 -6.5284863 -5.4584122 -4.4225817 -3.7292695 -3.9675794 -5.3860626 -6.5820932 -6.666482 -6.7719879 -6.0949945 -4.584959][-5.7536097 -7.7349396 -7.4556408 -7.1362715 -5.86442 -4.4203868 -3.0017228 -1.86287 -1.8194654 -3.5707979 -5.5913782 -6.4620066 -6.8391809 -6.0118484 -4.1644368][-5.4244061 -6.5617809 -6.4531927 -6.2781544 -4.7433758 -2.7336371 -0.90131986 0.53612041 0.6929419 -1.2406359 -3.9634306 -5.8908396 -6.8754683 -6.0493402 -3.8305254][-5.2611485 -5.6307068 -5.5753469 -5.3162622 -3.5141907 -0.81195319 1.6891611 3.3878934 3.2680228 0.9144702 -2.3879881 -5.2139254 -6.8019485 -6.3054848 -3.9874043][-4.9414625 -5.0746479 -5.1609106 -4.8237853 -2.9862981 0.043907404 3.3108394 5.60528 5.4529858 2.5909817 -1.3120295 -4.8948975 -6.9148989 -6.8476715 -4.8094378][-4.6275163 -5.054337 -5.3610239 -4.9605007 -3.1282973 -0.18682766 3.3793933 6.3068 6.4601965 3.5490782 -0.56133986 -4.637104 -6.8135586 -6.8888569 -5.1793923][-5.1063433 -5.4993782 -6.081769 -5.762074 -3.9936783 -1.1214871 2.3253214 5.3451366 5.598814 2.8981111 -0.85026538 -4.777359 -6.7691269 -6.6751785 -4.9818606][-5.882988 -6.056097 -6.8893976 -6.8756809 -5.4789429 -2.9350495 0.11973548 2.7732837 3.0129855 0.68859529 -2.3311963 -5.4191933 -6.9271755 -6.6578264 -4.865562][-6.3159933 -6.1775622 -7.1192427 -7.489253 -6.7501526 -4.8582387 -2.5695708 -0.56864965 -0.30646098 -1.9316756 -4.0443034 -6.1462631 -6.9635849 -6.5335054 -4.9072104][-6.4314718 -5.8996272 -6.6287651 -7.174859 -7.1248484 -6.0478005 -4.5941153 -3.3551319 -3.1700265 -4.1798592 -5.65479 -7.018117 -7.2835636 -6.7078466 -5.3675671][-6.4576921 -5.6290879 -6.053061 -6.5086241 -6.8381462 -6.5329351 -5.8238864 -5.1621695 -5.1307659 -5.7733979 -6.658493 -7.4553947 -7.5460653 -7.0722008 -6.0917907][-6.4153605 -5.5989952 -5.8906941 -6.165863 -6.4967222 -6.5705881 -6.4188013 -6.1217928 -6.0852652 -6.3553877 -6.7248421 -7.0775909 -7.1785345 -7.0190687 -6.5290318][-6.4187279 -5.7669125 -5.9180927 -6.0418763 -6.2698336 -6.3673658 -6.3930321 -6.2474422 -6.0961604 -6.036531 -6.0716729 -6.2735367 -6.5030718 -6.705327 -6.785244][-6.2833261 -5.7882843 -5.8662586 -5.9377823 -6.0700507 -6.1057911 -6.0417938 -5.8570518 -5.6203542 -5.4141769 -5.314436 -5.5151429 -5.9087362 -6.3781643 -6.8201561]]...]
INFO - root - 2017-12-15 07:28:40.087936: step 22410, loss = 0.22, batch loss = 0.19 (34.9 examples/sec; 0.229 sec/batch; 19h:43m:00s remains)
INFO - root - 2017-12-15 07:28:42.366576: step 22420, loss = 0.32, batch loss = 0.29 (35.8 examples/sec; 0.224 sec/batch; 19h:15m:59s remains)
INFO - root - 2017-12-15 07:28:44.617809: step 22430, loss = 0.16, batch loss = 0.12 (36.0 examples/sec; 0.222 sec/batch; 19h:09m:02s remains)
INFO - root - 2017-12-15 07:28:46.921020: step 22440, loss = 0.18, batch loss = 0.14 (33.1 examples/sec; 0.242 sec/batch; 20h:49m:24s remains)
INFO - root - 2017-12-15 07:28:49.195165: step 22450, loss = 0.19, batch loss = 0.16 (34.8 examples/sec; 0.230 sec/batch; 19h:47m:52s remains)
INFO - root - 2017-12-15 07:28:51.475762: step 22460, loss = 0.25, batch loss = 0.22 (35.2 examples/sec; 0.227 sec/batch; 19h:34m:27s remains)
INFO - root - 2017-12-15 07:28:53.742949: step 22470, loss = 0.29, batch loss = 0.26 (32.6 examples/sec; 0.245 sec/batch; 21h:07m:59s remains)
INFO - root - 2017-12-15 07:28:56.038319: step 22480, loss = 0.27, batch loss = 0.24 (35.0 examples/sec; 0.228 sec/batch; 19h:40m:28s remains)
INFO - root - 2017-12-15 07:28:58.340668: step 22490, loss = 0.21, batch loss = 0.17 (36.6 examples/sec; 0.219 sec/batch; 18h:50m:12s remains)
INFO - root - 2017-12-15 07:29:00.592035: step 22500, loss = 0.18, batch loss = 0.15 (36.2 examples/sec; 0.221 sec/batch; 19h:02m:11s remains)
2017-12-15 07:29:00.869536: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7489815 -3.52984 -3.4063895 -3.3636327 -3.5753829 -3.8307037 -4.1154919 -4.4474592 -4.7856431 -5.0278883 -5.0071096 -4.8743644 -4.5394626 -3.8619442 -3.0722091][-2.4071524 -3.3676693 -3.0921156 -2.9334707 -3.0876267 -3.426646 -3.9328003 -4.383007 -4.7642384 -5.0595927 -5.2811465 -5.5174971 -5.2834325 -4.4979835 -3.6527333][-3.2140303 -3.4558721 -3.0832803 -2.847095 -2.9418252 -3.2696342 -3.8178124 -4.226984 -4.5163031 -4.8006692 -5.2302246 -5.750093 -5.5944433 -4.6987715 -3.7885356][-4.3623509 -4.0964937 -3.7196584 -3.4069514 -3.3889208 -3.5776114 -3.9539309 -4.140944 -4.2023673 -4.4111357 -4.9485407 -5.5087128 -5.3026052 -4.3102179 -3.349906][-5.4779048 -4.7762241 -4.4163413 -3.9900846 -3.7704046 -3.6470833 -3.6096048 -3.4221075 -3.2067556 -3.3180704 -3.8522429 -4.4088559 -4.3066483 -3.4784184 -2.6978824][-6.0933838 -5.1616182 -4.751214 -4.1639829 -3.6776726 -3.1142082 -2.4872456 -1.9374775 -1.5755906 -1.6529716 -2.2973795 -3.0158489 -3.3039043 -2.95183 -2.3722327][-6.1346903 -5.2731829 -4.8830695 -4.3126922 -3.7457569 -2.8961322 -1.7950741 -1.0293287 -0.4046762 -0.26312065 -0.85775185 -1.6166596 -2.279218 -2.3673875 -1.8774576][-6.2060308 -5.3388977 -5.102067 -4.7336025 -4.21336 -3.1926694 -1.7356414 -0.69455338 0.40711427 1.0442731 0.6463716 -0.13110089 -1.1244156 -1.5521206 -1.1006626][-6.3145537 -5.3990154 -5.1758423 -4.8795557 -4.3817897 -3.2741661 -1.5806644 -0.33464825 1.1352229 2.2155724 2.1307259 1.3617134 0.02226162 -0.77617908 -0.47450721][-6.4842339 -5.4322443 -5.051837 -4.7861156 -4.5145631 -3.5662127 -1.8327786 -0.47383678 1.2197309 2.5994725 2.9187942 2.3508034 0.85801387 -0.15451908 -0.080165625][-6.2077508 -5.0052791 -4.4649372 -4.2942495 -4.398982 -3.74504 -2.1470976 -0.7193706 1.1173224 2.7367072 3.5897064 3.2350688 1.6704559 0.49119949 0.13672233][-5.5960126 -4.3863144 -3.9092441 -3.9476547 -4.3897152 -3.9903624 -2.5866733 -1.1126955 0.73288465 2.4003797 3.6108918 3.4585366 2.0449624 0.93281412 0.30835247][-4.979917 -3.7921257 -3.4563825 -3.7045016 -4.4413652 -4.4073629 -3.4033623 -2.1038361 -0.41201937 1.0743108 2.4398217 2.5969915 1.6840851 1.0083563 0.44122148][-4.6485367 -3.3843396 -3.1894479 -3.6205049 -4.6750216 -5.1375532 -4.6867561 -3.6775091 -2.1979074 -0.85180032 0.69651103 1.2524905 0.99494314 0.77215171 0.39288998][-4.5666513 -3.1805196 -3.1803145 -3.7028193 -4.7787962 -5.4785423 -5.3785038 -4.5823584 -3.3462455 -2.2044935 -0.72543621 -0.040548563 0.10992098 0.14064002 0.10301328]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-22500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-22500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 07:29:03.440207: step 22510, loss = 0.25, batch loss = 0.22 (34.7 examples/sec; 0.231 sec/batch; 19h:51m:16s remains)
INFO - root - 2017-12-15 07:29:05.739924: step 22520, loss = 0.32, batch loss = 0.29 (34.9 examples/sec; 0.229 sec/batch; 19h:42m:53s remains)
INFO - root - 2017-12-15 07:29:08.051491: step 22530, loss = 0.18, batch loss = 0.15 (36.2 examples/sec; 0.221 sec/batch; 19h:01m:39s remains)
INFO - root - 2017-12-15 07:29:10.385571: step 22540, loss = 0.22, batch loss = 0.19 (34.9 examples/sec; 0.229 sec/batch; 19h:44m:15s remains)
INFO - root - 2017-12-15 07:29:12.646908: step 22550, loss = 0.27, batch loss = 0.23 (36.2 examples/sec; 0.221 sec/batch; 19h:00m:04s remains)
INFO - root - 2017-12-15 07:29:14.887465: step 22560, loss = 0.20, batch loss = 0.17 (35.7 examples/sec; 0.224 sec/batch; 19h:16m:32s remains)
INFO - root - 2017-12-15 07:29:17.131811: step 22570, loss = 0.16, batch loss = 0.13 (34.9 examples/sec; 0.230 sec/batch; 19h:45m:40s remains)
INFO - root - 2017-12-15 07:29:19.414927: step 22580, loss = 0.29, batch loss = 0.26 (36.0 examples/sec; 0.222 sec/batch; 19h:07m:07s remains)
INFO - root - 2017-12-15 07:29:21.670482: step 22590, loss = 0.27, batch loss = 0.24 (35.4 examples/sec; 0.226 sec/batch; 19h:27m:24s remains)
INFO - root - 2017-12-15 07:29:23.941078: step 22600, loss = 0.22, batch loss = 0.19 (35.0 examples/sec; 0.229 sec/batch; 19h:40m:25s remains)
2017-12-15 07:29:24.257526: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0276949 -5.8489094 -5.9796815 -5.7164431 -5.3567934 -5.0823245 -5.0701327 -5.2663889 -5.40933 -5.5491047 -5.6414547 -5.644268 -5.7863827 -5.855546 -5.6769924][-5.2039843 -7.5358438 -7.7496796 -7.2241211 -6.6273108 -6.144557 -5.99523 -6.1847019 -6.4030094 -6.7297173 -7.0178757 -7.098403 -7.3960857 -7.4944925 -7.0733843][-7.3064156 -8.9910812 -9.2953548 -8.541049 -7.8248234 -7.0497742 -6.3997068 -6.287488 -6.6205244 -7.2819529 -7.9155664 -8.0995274 -8.4567413 -8.543437 -8.0342522][-8.66077 -9.7643471 -10.097497 -9.0958414 -8.2310238 -6.9896431 -5.3700614 -4.632616 -5.1913815 -6.4190674 -7.6652 -8.0295038 -8.3549471 -8.4349689 -7.998024][-9.226799 -9.7189026 -9.9182386 -8.5277691 -7.2731266 -5.3688073 -2.5227108 -1.2055652 -2.215641 -4.1385708 -6.0186958 -6.6968031 -7.1129751 -7.3609648 -7.217701][-9.1514711 -9.6608219 -9.660223 -7.8983555 -6.1715164 -3.5531526 0.53741789 2.5032923 1.1430483 -1.4156314 -3.9475369 -5.16316 -5.6819735 -6.0844669 -6.3759174][-8.422636 -8.7432261 -8.2423964 -6.1565418 -4.1505117 -1.2275434 3.6277921 6.1427212 4.7633333 1.7550085 -1.4871601 -3.5159187 -4.413599 -5.2571955 -6.1413465][-7.8006716 -7.5814772 -6.7181091 -4.5702915 -2.4312706 0.62426233 5.8338776 8.7383757 7.4937553 4.1723795 0.61466694 -1.855503 -3.1645813 -4.6165533 -6.0665264][-7.6099997 -7.2615604 -6.5594168 -4.8409882 -2.9218516 -0.1114192 4.7928724 7.8281689 6.6999359 3.2105772 -0.30172551 -2.6527371 -4.06805 -5.7395544 -7.1288414][-7.6670933 -7.4497089 -7.1680918 -6.0537944 -4.64501 -2.4433694 1.5708623 4.4669971 3.8665679 0.86229205 -2.1767981 -4.0971127 -5.562994 -7.3750019 -8.3318644][-7.8433695 -7.9120274 -8.0793114 -7.5062857 -6.589076 -5.0593815 -2.0549273 0.55670667 0.60495949 -1.705651 -4.151216 -5.5792141 -7.0924854 -8.7712793 -9.064189][-7.9877787 -8.4007664 -9.0754538 -9.0186033 -8.5892735 -7.6384239 -5.5864854 -3.4177904 -3.0417891 -4.6499977 -6.3988867 -7.4071922 -8.7112789 -9.80579 -9.5067778][-7.6642895 -8.1214685 -8.8746586 -8.8531313 -8.5091829 -7.9031754 -6.6724186 -5.3399038 -5.1387739 -6.2295976 -7.3037739 -8.07328 -9.1610785 -9.8813953 -9.4719677][-7.1927872 -7.4470882 -7.9762459 -7.7764854 -7.3960667 -7.0340157 -6.5162554 -6.0104589 -6.0615673 -6.5863991 -7.0014343 -7.5345135 -8.4265537 -8.96386 -8.6857414][-6.5701151 -6.49098 -6.8176203 -6.6840067 -6.51472 -6.4434919 -6.3944721 -6.3934917 -6.5158558 -6.5674744 -6.528039 -6.76285 -7.3182983 -7.6555834 -7.54064]]...]
INFO - root - 2017-12-15 07:29:26.545191: step 22610, loss = 0.27, batch loss = 0.24 (35.4 examples/sec; 0.226 sec/batch; 19h:27m:30s remains)
INFO - root - 2017-12-15 07:29:28.838712: step 22620, loss = 0.35, batch loss = 0.32 (35.3 examples/sec; 0.227 sec/batch; 19h:30m:04s remains)
INFO - root - 2017-12-15 07:29:31.099089: step 22630, loss = 0.22, batch loss = 0.19 (35.0 examples/sec; 0.228 sec/batch; 19h:38m:58s remains)
INFO - root - 2017-12-15 07:29:33.362199: step 22640, loss = 0.24, batch loss = 0.21 (33.0 examples/sec; 0.242 sec/batch; 20h:51m:58s remains)
INFO - root - 2017-12-15 07:29:35.662257: step 22650, loss = 0.26, batch loss = 0.23 (35.2 examples/sec; 0.227 sec/batch; 19h:34m:18s remains)
INFO - root - 2017-12-15 07:29:37.951570: step 22660, loss = 0.22, batch loss = 0.19 (34.6 examples/sec; 0.231 sec/batch; 19h:54m:50s remains)
INFO - root - 2017-12-15 07:29:40.226081: step 22670, loss = 0.18, batch loss = 0.15 (34.8 examples/sec; 0.230 sec/batch; 19h:46m:40s remains)
INFO - root - 2017-12-15 07:29:42.503478: step 22680, loss = 0.19, batch loss = 0.16 (35.2 examples/sec; 0.227 sec/batch; 19h:32m:35s remains)
INFO - root - 2017-12-15 07:29:44.785343: step 22690, loss = 0.19, batch loss = 0.16 (34.5 examples/sec; 0.232 sec/batch; 19h:56m:54s remains)
INFO - root - 2017-12-15 07:29:47.078902: step 22700, loss = 0.17, batch loss = 0.14 (36.9 examples/sec; 0.217 sec/batch; 18h:40m:20s remains)
2017-12-15 07:29:47.378498: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8245907 -5.80282 -5.3613214 -5.1331429 -5.0598426 -5.0825396 -5.2847652 -5.6623049 -6.1713834 -6.3664207 -6.1657019 -5.8761139 -5.5390778 -5.1549311 -4.7215624][-5.6552372 -6.6567745 -6.107192 -5.6530609 -5.3709083 -5.2297125 -5.35665 -5.9420309 -6.8929396 -7.6376905 -7.6668816 -7.4899168 -7.1191711 -6.4162807 -5.6211381][-7.7587738 -7.5023146 -6.8540106 -6.1213808 -5.5898581 -5.1542568 -5.1048613 -5.7470803 -6.9216442 -8.1002607 -8.36536 -8.4898081 -8.2553835 -7.3620977 -6.3567715][-8.6989241 -7.319828 -6.5857353 -5.5851355 -4.7939339 -4.0117493 -3.6353755 -4.1056981 -5.2995377 -6.8455429 -7.3835516 -7.9861889 -8.0979843 -7.2945442 -6.3327646][-8.38647 -6.2839956 -5.3743916 -4.0233331 -2.8381889 -1.4456027 -0.5874964 -1.0195488 -2.3728342 -4.3464417 -5.3155947 -6.4334955 -6.9727535 -6.5020313 -5.7750716][-8.2940493 -5.5703049 -4.4537878 -2.8167388 -1.1103032 1.0750155 2.5080512 2.2194698 0.81354189 -1.687638 -3.1281943 -4.7096691 -5.7531929 -5.8426337 -5.384593][-7.5715475 -5.055037 -3.6446495 -1.7624257 0.33750963 3.1675532 4.9946594 4.84251 3.3542759 0.47301626 -1.2502744 -3.0953095 -4.5941176 -5.18204 -5.0732403][-8.16522 -5.8293958 -4.2920589 -2.2737403 0.22536969 3.4251635 5.551692 5.6351948 4.3008757 1.3972948 -0.40972018 -2.2577989 -3.9396777 -4.8828578 -5.0469923][-9.0788393 -7.3469486 -6.0956869 -4.2875309 -1.7891446 1.5312409 3.9275453 4.5575047 3.7154467 1.1438935 -0.61239612 -2.449033 -4.2335958 -5.3254828 -5.5041471][-9.1118622 -7.9517431 -7.1989412 -5.9235272 -3.96584 -1.1785415 1.0497911 2.0907142 1.5856426 -0.53630328 -1.9937859 -3.6237495 -5.1620216 -6.0732474 -6.1429844][-8.5259991 -7.843174 -7.6609373 -7.0696363 -5.9353123 -3.999109 -2.1160474 -0.97253823 -1.1940067 -2.6994259 -3.768939 -5.0694776 -6.3056068 -6.9301414 -6.7975769][-8.2267475 -7.8024607 -7.9102621 -7.7540131 -7.2455978 -6.1096888 -4.7642331 -3.7002711 -3.6995673 -4.711648 -5.437788 -6.3830481 -7.1941118 -7.5277624 -7.1698189][-7.5918941 -7.2114744 -7.3600168 -7.236187 -6.9495029 -6.3423862 -5.497714 -4.7363052 -4.6793361 -5.2410278 -5.6637354 -6.33706 -6.9260283 -7.1210241 -6.8473186][-6.290761 -5.7454114 -5.7946186 -5.6288185 -5.3121462 -4.9028463 -4.423161 -4.0547619 -4.0928035 -4.5082321 -4.9163151 -5.5043373 -6.0495119 -6.3042097 -6.1479712][-4.8618331 -3.9766364 -3.8950977 -3.7287173 -3.5491002 -3.4859061 -3.4867654 -3.5550904 -3.7827966 -4.0902967 -4.4152274 -4.84686 -5.2478309 -5.5098505 -5.4943504]]...]
INFO - root - 2017-12-15 07:29:49.679057: step 22710, loss = 0.21, batch loss = 0.17 (34.7 examples/sec; 0.230 sec/batch; 19h:49m:26s remains)
INFO - root - 2017-12-15 07:29:51.920777: step 22720, loss = 0.27, batch loss = 0.23 (35.4 examples/sec; 0.226 sec/batch; 19h:25m:35s remains)
INFO - root - 2017-12-15 07:29:54.218586: step 22730, loss = 0.17, batch loss = 0.14 (34.9 examples/sec; 0.229 sec/batch; 19h:44m:46s remains)
INFO - root - 2017-12-15 07:29:56.487708: step 22740, loss = 0.39, batch loss = 0.36 (35.4 examples/sec; 0.226 sec/batch; 19h:25m:43s remains)
INFO - root - 2017-12-15 07:29:58.790450: step 22750, loss = 0.20, batch loss = 0.17 (36.3 examples/sec; 0.220 sec/batch; 18h:57m:16s remains)
INFO - root - 2017-12-15 07:30:01.058118: step 22760, loss = 0.16, batch loss = 0.13 (35.9 examples/sec; 0.223 sec/batch; 19h:11m:10s remains)
INFO - root - 2017-12-15 07:30:03.333879: step 22770, loss = 0.30, batch loss = 0.27 (35.4 examples/sec; 0.226 sec/batch; 19h:27m:51s remains)
INFO - root - 2017-12-15 07:30:05.639221: step 22780, loss = 0.16, batch loss = 0.13 (35.4 examples/sec; 0.226 sec/batch; 19h:27m:45s remains)
INFO - root - 2017-12-15 07:30:07.978104: step 22790, loss = 0.19, batch loss = 0.16 (33.5 examples/sec; 0.239 sec/batch; 20h:34m:05s remains)
INFO - root - 2017-12-15 07:30:10.263900: step 22800, loss = 0.19, batch loss = 0.16 (34.7 examples/sec; 0.230 sec/batch; 19h:49m:41s remains)
2017-12-15 07:30:10.545071: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.96597 -6.39769 -7.2583609 -7.1066294 -6.6910038 -6.447073 -6.4230042 -6.1551819 -5.5379581 -4.7636409 -4.3834219 -4.4506874 -4.9883757 -5.0603085 -4.7809982][-5.7179794 -7.1826334 -8.032321 -7.8127823 -7.3202047 -6.9995003 -7.0487061 -7.0369043 -6.681386 -6.0343647 -5.72093 -5.7884932 -6.1124253 -5.9755387 -5.7266073][-7.2496834 -7.4596319 -8.0957966 -7.8401012 -7.2098207 -6.6510582 -6.8199186 -7.1594467 -7.0244122 -6.4038267 -6.0135202 -5.8049498 -5.796381 -5.6213164 -5.5561371][-8.1011505 -7.3015461 -7.5206013 -7.0338087 -5.9878435 -5.2595396 -5.6663866 -6.5172577 -6.8698969 -6.6828671 -6.3500433 -5.8929491 -5.5905867 -5.4058013 -5.332715][-7.9578848 -6.5034018 -6.1012621 -4.980155 -3.1563082 -2.1845112 -2.8565905 -4.4696379 -5.79913 -6.4686317 -6.5494566 -6.1224942 -5.6072826 -5.4044371 -5.1865149][-7.6360655 -5.4994736 -4.4021196 -2.4580879 0.24362087 1.6092947 1.0132821 -0.9433111 -3.1907911 -4.8632517 -5.5926619 -5.552053 -5.1613827 -5.0633993 -4.8686442][-6.3846521 -4.491106 -2.9577279 -0.43114305 2.84257 4.7550364 4.5536041 2.6844461 -0.29494321 -2.8336039 -4.2725592 -4.7604208 -4.5164113 -4.3840566 -4.24045][-5.7480836 -3.6557939 -1.9463596 0.75465393 4.180048 6.5831881 6.9332705 5.25191 1.7384105 -1.4823273 -3.4912817 -4.4496489 -4.1759415 -3.8227248 -3.7327375][-5.3912458 -3.4254422 -1.9137107 0.61551738 4.07909 6.9817705 7.8442631 6.3494196 2.6614039 -0.82847905 -3.3011894 -4.6564517 -4.3520164 -3.8196254 -3.7302446][-5.8741455 -4.2539988 -3.2852802 -1.2515225 2.0943863 5.1895 6.3210926 5.3872604 2.2970383 -0.83378482 -3.2519388 -4.7387762 -4.5143328 -3.9431925 -3.8921099][-7.137639 -5.629982 -5.0205 -3.3148546 -0.29077494 2.444958 3.5894005 3.0366709 0.69092274 -1.8624126 -3.9900951 -5.102272 -4.7300911 -4.1093378 -4.0430174][-8.249301 -6.7572536 -6.3723249 -5.0590305 -2.5506124 -0.42847872 0.39740396 0.065787315 -1.6451828 -3.6383383 -5.0981407 -5.6556354 -5.1139779 -4.4482431 -4.2634864][-8.8735685 -7.4185448 -7.2591381 -6.4057112 -4.6264515 -3.1222057 -2.5330546 -2.6115487 -3.8783522 -5.285665 -5.9246111 -5.878684 -5.219985 -4.6813488 -4.3771749][-8.9153328 -7.485836 -7.2387943 -6.6758852 -5.6537895 -4.70383 -4.3975406 -4.5503864 -5.5770621 -6.5606165 -6.6340389 -6.2835236 -5.5824003 -4.8697958 -4.2802591][-8.7161169 -7.240325 -6.6587181 -6.1200123 -5.6117458 -5.1780376 -5.332222 -5.8715649 -6.9182606 -7.504178 -6.9937067 -6.341856 -5.6417351 -4.8291616 -4.1498561]]...]
INFO - root - 2017-12-15 07:30:12.807742: step 22810, loss = 0.20, batch loss = 0.17 (34.7 examples/sec; 0.230 sec/batch; 19h:48m:43s remains)
INFO - root - 2017-12-15 07:30:15.068399: step 22820, loss = 0.28, batch loss = 0.25 (35.5 examples/sec; 0.225 sec/batch; 19h:22m:40s remains)
INFO - root - 2017-12-15 07:30:17.317731: step 22830, loss = 0.25, batch loss = 0.21 (35.5 examples/sec; 0.225 sec/batch; 19h:23m:26s remains)
INFO - root - 2017-12-15 07:30:19.559926: step 22840, loss = 0.26, batch loss = 0.23 (35.9 examples/sec; 0.223 sec/batch; 19h:11m:36s remains)
INFO - root - 2017-12-15 07:30:21.813416: step 22850, loss = 0.16, batch loss = 0.13 (35.3 examples/sec; 0.227 sec/batch; 19h:29m:43s remains)
INFO - root - 2017-12-15 07:30:24.123927: step 22860, loss = 0.39, batch loss = 0.35 (34.9 examples/sec; 0.229 sec/batch; 19h:42m:20s remains)
INFO - root - 2017-12-15 07:30:26.402751: step 22870, loss = 0.16, batch loss = 0.13 (35.4 examples/sec; 0.226 sec/batch; 19h:26m:22s remains)
INFO - root - 2017-12-15 07:30:28.664338: step 22880, loss = 0.46, batch loss = 0.43 (35.4 examples/sec; 0.226 sec/batch; 19h:25m:44s remains)
INFO - root - 2017-12-15 07:30:30.969058: step 22890, loss = 0.37, batch loss = 0.34 (35.6 examples/sec; 0.224 sec/batch; 19h:18m:20s remains)
INFO - root - 2017-12-15 07:30:33.230298: step 22900, loss = 0.20, batch loss = 0.17 (35.8 examples/sec; 0.224 sec/batch; 19h:14m:07s remains)
2017-12-15 07:30:33.529339: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2835131 -6.3026013 -6.164362 -6.1062279 -5.9710107 -5.8617268 -5.8134356 -5.696744 -5.6241341 -5.6200352 -5.7318726 -5.9390469 -6.0851393 -6.0540996 -5.9098248][-5.8503842 -6.2987986 -6.0664587 -5.8633842 -5.6136255 -5.4858685 -5.5104532 -5.5741024 -5.7815523 -6.0016947 -6.1842666 -6.4135056 -6.5796723 -6.5509391 -6.4234142][-5.4763012 -5.2012296 -4.6415548 -4.053607 -3.4393623 -2.9946837 -2.9971097 -3.334955 -4.0562968 -4.6810942 -5.136806 -5.5911131 -5.9221044 -5.9060707 -5.7496471][-4.7275591 -3.9069982 -3.1003029 -2.272403 -1.3598189 -0.40700495 -0.22651172 -0.87101889 -2.1203492 -3.1302929 -3.8272452 -4.448348 -4.8788295 -4.8579454 -4.6038613][-4.03479 -2.9523408 -1.978657 -0.88253546 0.49115825 2.2506733 2.792695 1.628196 -0.46716404 -2.2866876 -3.5376678 -4.2908058 -4.6612978 -4.3833008 -3.6694338][-3.1762357 -2.1826582 -1.0797395 0.32215047 2.1791129 4.8758264 5.9966564 4.5529242 1.7440581 -0.8635633 -2.8372238 -3.9408336 -4.514194 -4.1415339 -3.107332][-2.5936124 -1.9359481 -0.84494567 0.52037716 2.3604755 5.3938875 6.8453112 5.4841461 2.6056104 -0.25461972 -2.6124158 -3.8826585 -4.4938221 -4.1018772 -2.9631565][-2.9173126 -2.1270654 -1.0960062 0.023715973 1.3962588 4.0507312 5.4795356 4.3026371 1.6649756 -1.1471075 -3.5050297 -4.4800215 -4.7059679 -4.1698565 -2.9876044][-3.939784 -3.1169102 -2.0939167 -1.0201793 -0.012691259 2.0202956 3.2436895 2.410419 0.26988697 -2.1357007 -4.3209906 -5.196599 -5.3066072 -4.6918011 -3.5472984][-5.4589834 -4.6455107 -3.7900953 -2.8316975 -2.1869175 -0.91461253 -0.11647296 -0.62981212 -2.0998793 -3.8290126 -5.5111418 -6.1456604 -6.0683823 -5.3915234 -4.4430952][-6.0527172 -5.2082496 -4.6129808 -3.9816251 -3.7802415 -3.2757378 -2.9797044 -3.3770432 -4.2370586 -5.2304382 -6.2684865 -6.5778756 -6.2121372 -5.4904232 -4.7102928][-6.1694918 -5.3171444 -4.9335661 -4.5256066 -4.5168009 -4.4047689 -4.3474441 -4.6535416 -5.1132588 -5.483182 -6.0004296 -6.0783243 -5.6924648 -5.0943689 -4.5805111][-5.69613 -4.8350897 -4.5914373 -4.3348637 -4.3432331 -4.35176 -4.3734446 -4.5457435 -4.6916628 -4.7170038 -4.9971619 -5.118681 -4.8586836 -4.4363861 -4.1523118][-4.9245186 -4.1105547 -4.0284595 -3.9450641 -3.9424789 -3.8729086 -3.8593645 -3.9966998 -4.0840125 -4.079031 -4.2550983 -4.290452 -4.0330548 -3.7952785 -3.6626754][-4.4789042 -3.7685847 -3.7279637 -3.686244 -3.6432605 -3.5777416 -3.591074 -3.7246416 -3.7873752 -3.7300677 -3.7844875 -3.8057752 -3.6956282 -3.6510925 -3.6295328]]...]
INFO - root - 2017-12-15 07:30:35.796898: step 22910, loss = 0.21, batch loss = 0.18 (35.2 examples/sec; 0.227 sec/batch; 19h:32m:59s remains)
INFO - root - 2017-12-15 07:30:38.087901: step 22920, loss = 0.23, batch loss = 0.20 (35.1 examples/sec; 0.228 sec/batch; 19h:35m:59s remains)
INFO - root - 2017-12-15 07:30:40.345497: step 22930, loss = 0.23, batch loss = 0.20 (35.6 examples/sec; 0.225 sec/batch; 19h:19m:47s remains)
INFO - root - 2017-12-15 07:30:42.628986: step 22940, loss = 0.46, batch loss = 0.43 (35.4 examples/sec; 0.226 sec/batch; 19h:24m:23s remains)
INFO - root - 2017-12-15 07:30:44.894654: step 22950, loss = 0.25, batch loss = 0.22 (35.9 examples/sec; 0.223 sec/batch; 19h:09m:36s remains)
INFO - root - 2017-12-15 07:30:47.160878: step 22960, loss = 0.35, batch loss = 0.32 (36.0 examples/sec; 0.222 sec/batch; 19h:07m:04s remains)
INFO - root - 2017-12-15 07:30:49.465178: step 22970, loss = 0.22, batch loss = 0.19 (35.8 examples/sec; 0.223 sec/batch; 19h:11m:22s remains)
INFO - root - 2017-12-15 07:30:51.754746: step 22980, loss = 0.23, batch loss = 0.19 (36.0 examples/sec; 0.222 sec/batch; 19h:05m:10s remains)
INFO - root - 2017-12-15 07:30:54.083530: step 22990, loss = 0.36, batch loss = 0.33 (33.5 examples/sec; 0.239 sec/batch; 20h:30m:22s remains)
INFO - root - 2017-12-15 07:30:56.379698: step 23000, loss = 0.19, batch loss = 0.15 (34.7 examples/sec; 0.230 sec/batch; 19h:48m:03s remains)
2017-12-15 07:30:56.655170: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5942508 -3.3542237 -4.7279878 -5.2645082 -4.1568813 -2.8813672 -1.9283966 -1.1767269 -0.89797497 -0.91053319 -1.2698722 -1.8147764 -2.1122458 -2.404587 -2.0600524][-2.2945702 -3.4322314 -5.0607157 -5.9243436 -4.9747705 -3.6577067 -2.5423493 -1.5408163 -1.4177396 -1.6350613 -2.0343211 -2.6216853 -2.9354982 -3.1534011 -2.5324168][-2.5900631 -3.373333 -5.1414142 -6.0600405 -5.0440707 -3.5894651 -2.2831397 -1.0541743 -0.96187067 -1.2331591 -1.6828783 -2.550642 -3.1264071 -3.3099704 -2.6835787][-2.8667562 -2.7897067 -4.4775057 -5.3789787 -4.4286594 -2.9723501 -1.43295 -0.018428087 0.10014105 -0.25395417 -0.88243043 -1.9969847 -2.7920532 -3.0248938 -2.7024462][-3.5308547 -2.5814817 -3.803946 -4.5191684 -3.7943449 -2.4459648 -0.82344246 0.731966 1.0431187 0.63689804 -0.16411567 -1.4805522 -2.588511 -2.8292251 -2.7602892][-3.9921546 -2.5312831 -3.1545095 -3.5013916 -2.9797354 -1.9173763 -0.31754732 1.163924 1.4863687 0.95277214 0.085631847 -1.3920581 -2.7671936 -3.0717697 -3.2991936][-3.9433415 -2.405751 -2.417331 -2.2370436 -1.7111237 -0.79746878 0.5868814 1.490442 1.4528399 0.77203488 -0.089677334 -1.6125587 -3.0549622 -3.3546143 -3.7841742][-4.3305397 -2.4446814 -1.9113955 -1.2607858 -0.624992 0.19362712 1.3254895 1.7240231 1.3454058 0.43726277 -0.55652344 -2.0622404 -3.4309673 -3.8613272 -4.3480492][-4.5147715 -2.0695362 -1.0756208 -0.1590364 0.55344892 1.2674124 2.0271611 1.9997258 1.6321228 0.54399419 -0.81911814 -2.3924718 -3.7482681 -4.3570886 -4.9187822][-4.2801619 -1.3432063 -0.13676381 0.76747441 1.2697365 1.5844698 1.9331288 1.7318153 1.4108803 0.40188503 -1.0443997 -2.5147655 -3.8141241 -4.4467111 -4.9079409][-4.1321039 -0.981454 0.29890203 1.0248919 1.049094 0.89624524 0.78848934 0.51510358 0.47780085 -0.2413528 -1.5442971 -2.7555306 -3.8202875 -4.2990174 -4.60863][-4.0988913 -0.76966727 0.69454885 1.4037344 1.0758495 0.38618898 -0.21677232 -0.67787957 -0.51492333 -0.86194336 -1.8270296 -2.7617698 -3.6462226 -4.0203514 -4.3195763][-4.1412649 -0.75449073 0.81593227 1.4902136 0.73283553 -0.44601607 -1.3745662 -1.9469197 -1.6933639 -1.7084825 -2.1165161 -2.719502 -3.3073931 -3.3689194 -3.6220412][-4.0828171 -0.70788574 0.89416075 1.5697608 0.54192138 -1.0153599 -2.1233163 -2.7399042 -2.6673808 -2.6253426 -2.542825 -2.6614897 -2.6875768 -2.3283727 -2.5107994][-4.1522493 -0.76336789 0.98565054 1.5303228 0.38953567 -1.1567314 -2.1177039 -2.7184498 -2.9643879 -3.1296468 -2.7857146 -2.5410044 -1.9463376 -1.0566289 -1.1113032]]...]
INFO - root - 2017-12-15 07:30:58.949018: step 23010, loss = 0.31, batch loss = 0.28 (33.8 examples/sec; 0.237 sec/batch; 20h:20m:33s remains)
INFO - root - 2017-12-15 07:31:01.230648: step 23020, loss = 0.21, batch loss = 0.18 (36.0 examples/sec; 0.222 sec/batch; 19h:04m:55s remains)
INFO - root - 2017-12-15 07:31:03.471693: step 23030, loss = 0.27, batch loss = 0.24 (36.1 examples/sec; 0.222 sec/batch; 19h:03m:23s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 07:31:05.712920: step 23040, loss = 0.19, batch loss = 0.15 (35.9 examples/sec; 0.223 sec/batch; 19h:09m:47s remains)
INFO - root - 2017-12-15 07:31:07.955015: step 23050, loss = 0.28, batch loss = 0.24 (36.1 examples/sec; 0.221 sec/batch; 19h:02m:17s remains)
INFO - root - 2017-12-15 07:31:10.212558: step 23060, loss = 0.21, batch loss = 0.18 (33.7 examples/sec; 0.238 sec/batch; 20h:25m:25s remains)
INFO - root - 2017-12-15 07:31:12.494302: step 23070, loss = 0.22, batch loss = 0.18 (35.1 examples/sec; 0.228 sec/batch; 19h:36m:16s remains)
INFO - root - 2017-12-15 07:31:14.751695: step 23080, loss = 0.25, batch loss = 0.22 (35.8 examples/sec; 0.224 sec/batch; 19h:13m:30s remains)
INFO - root - 2017-12-15 07:31:17.011410: step 23090, loss = 0.18, batch loss = 0.15 (35.9 examples/sec; 0.223 sec/batch; 19h:10m:14s remains)
INFO - root - 2017-12-15 07:31:19.273785: step 23100, loss = 0.18, batch loss = 0.15 (34.7 examples/sec; 0.230 sec/batch; 19h:48m:22s remains)
2017-12-15 07:31:19.553638: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0779443 -2.0452979 -2.6978552 -3.6324267 -4.2930527 -4.3558311 -3.7977223 -3.2101789 -3.0946331 -3.4360752 -4.2684793 -5.43754 -5.8931708 -5.8627348 -5.4327092][-2.9440618 -3.5244379 -4.2263393 -5.0395451 -5.5656013 -5.3680191 -4.4219337 -3.5178571 -3.2156343 -3.5203648 -4.4346848 -5.7194853 -6.3351364 -6.5270329 -6.3097353][-4.9808426 -5.0021973 -5.51314 -5.9494791 -6.1869736 -5.7742043 -4.5987768 -3.5480237 -3.1646106 -3.4209614 -4.3469219 -5.7748604 -6.5650477 -6.9432263 -6.8890009][-5.8173141 -5.1231585 -5.4998579 -5.8912964 -6.1596975 -5.6123996 -4.1380873 -2.9292197 -2.4762659 -2.675859 -3.6313643 -5.2058735 -6.12483 -6.7415347 -6.9714551][-4.8901739 -4.1324635 -4.6391239 -5.0718365 -5.2378254 -4.355907 -2.3654389 -0.93048847 -0.5356226 -0.99424779 -2.3840842 -4.412034 -5.5850935 -6.5229287 -6.9605546][-3.8575821 -2.8769436 -3.6594117 -4.0874076 -3.9294944 -2.5280972 0.0024590492 1.5265958 1.6235433 0.606709 -1.4187183 -3.7941933 -5.0934696 -6.2620997 -6.8150673][-1.8096547 -1.3389177 -2.5441098 -2.980504 -2.5181465 -0.86139083 1.9526582 3.6589136 3.7182631 2.2964592 -0.15485144 -2.7786236 -4.4394245 -6.0023279 -6.6502466][-0.11195302 0.65784883 -0.89304805 -1.5112447 -1.1878086 0.2905848 3.1587024 5.1363106 5.1731448 3.4429035 0.58100796 -2.3210802 -4.3296967 -6.14161 -6.7699738][0.76364756 1.8646069 0.086520433 -0.79319978 -0.67096817 0.542737 3.1768527 5.1770573 5.0469594 3.0684786 0.17801428 -2.6855335 -4.8978333 -6.8070755 -7.3275094][-0.11814737 1.0848761 -0.64955211 -1.7486905 -1.9059649 -0.90498066 1.3231833 3.0983272 2.8527298 1.0691109 -1.4580371 -4.0524688 -6.22225 -7.9191594 -8.1150932][-1.7609707 -0.67796504 -2.1623526 -3.2311246 -3.4199922 -2.59938 -0.9417367 0.38919258 0.13363481 -1.287532 -3.3316064 -5.5834045 -7.5496764 -8.7865009 -8.5918446][-3.3456678 -2.582715 -3.7034659 -4.5694647 -4.78607 -4.2250857 -3.2436838 -2.4634018 -2.737288 -3.7939162 -5.3989177 -7.1383367 -8.5317822 -9.1009178 -8.5025311][-4.6054726 -4.1320143 -5.0101357 -5.7763739 -6.0439172 -5.7116957 -5.1945353 -4.790009 -5.063179 -5.7888985 -6.8359351 -7.8784184 -8.63888 -8.663599 -7.8754086][-5.407609 -5.159955 -5.8107319 -6.2566638 -6.3183246 -6.0607557 -5.8218694 -5.7004404 -5.8884454 -6.313591 -6.92255 -7.4727736 -7.7729297 -7.5710206 -6.8220587][-5.4733877 -5.1750197 -5.4981728 -5.6366167 -5.585639 -5.49216 -5.432663 -5.3954535 -5.5033464 -5.7240562 -6.0174303 -6.288878 -6.4027448 -6.2396507 -5.6738625]]...]
INFO - root - 2017-12-15 07:31:21.846398: step 23110, loss = 0.26, batch loss = 0.22 (34.6 examples/sec; 0.231 sec/batch; 19h:51m:33s remains)
INFO - root - 2017-12-15 07:31:24.157006: step 23120, loss = 0.21, batch loss = 0.18 (34.3 examples/sec; 0.233 sec/batch; 20h:03m:52s remains)
INFO - root - 2017-12-15 07:31:26.436886: step 23130, loss = 0.19, batch loss = 0.15 (34.9 examples/sec; 0.229 sec/batch; 19h:41m:45s remains)
INFO - root - 2017-12-15 07:31:28.736973: step 23140, loss = 0.26, batch loss = 0.23 (35.2 examples/sec; 0.227 sec/batch; 19h:30m:31s remains)
INFO - root - 2017-12-15 07:31:31.007827: step 23150, loss = 0.24, batch loss = 0.21 (36.5 examples/sec; 0.219 sec/batch; 18h:50m:08s remains)
INFO - root - 2017-12-15 07:31:33.256831: step 23160, loss = 0.29, batch loss = 0.26 (35.4 examples/sec; 0.226 sec/batch; 19h:24m:32s remains)
INFO - root - 2017-12-15 07:31:35.525716: step 23170, loss = 0.20, batch loss = 0.16 (34.3 examples/sec; 0.233 sec/batch; 20h:02m:30s remains)
INFO - root - 2017-12-15 07:31:37.810992: step 23180, loss = 0.24, batch loss = 0.21 (35.2 examples/sec; 0.227 sec/batch; 19h:32m:40s remains)
INFO - root - 2017-12-15 07:31:40.135629: step 23190, loss = 0.18, batch loss = 0.15 (34.4 examples/sec; 0.232 sec/batch; 19h:58m:29s remains)
INFO - root - 2017-12-15 07:31:42.433333: step 23200, loss = 0.22, batch loss = 0.19 (34.9 examples/sec; 0.229 sec/batch; 19h:42m:29s remains)
2017-12-15 07:31:42.710771: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3200614 -4.0553808 -3.9896092 -3.9356589 -3.933907 -4.0716004 -4.3295393 -4.6407895 -4.9225688 -5.1342068 -5.2580891 -5.1154985 -4.8296032 -4.4722595 -4.2154226][-3.0286138 -4.3564982 -4.3967152 -4.4150429 -4.4089441 -4.5470891 -4.8109856 -5.1281719 -5.456883 -5.7763681 -5.9820118 -5.9088712 -5.5690756 -5.0565948 -4.5944786][-4.1881547 -4.8214111 -4.9316082 -4.9349179 -4.8190866 -4.8279986 -4.9295387 -5.0932703 -5.4008369 -5.8311057 -6.2224445 -6.3855734 -6.2068129 -5.6956158 -5.0802188][-5.0661316 -5.0414987 -5.1375937 -5.0102749 -4.6734285 -4.3980341 -4.153718 -4.0166435 -4.1982045 -4.7186975 -5.36989 -5.9099636 -6.1111789 -5.8449593 -5.283206][-5.660841 -4.9045992 -4.806222 -4.4459133 -3.8440914 -3.1974134 -2.4948065 -2.0047238 -2.0344272 -2.6731975 -3.5857892 -4.5320568 -5.1985431 -5.3361344 -5.0374689][-5.7318363 -4.6117063 -4.1952381 -3.4967482 -2.4684939 -1.2463773 0.016818285 0.70568895 0.63664079 -0.22538662 -1.4284415 -2.8238416 -3.9340773 -4.4551706 -4.5036192][-5.4302483 -4.4542017 -3.7166276 -2.663909 -1.1456349 0.65857744 2.3749268 3.12299 3.0110648 1.9955351 0.57378888 -1.2094719 -2.6877711 -3.5320117 -3.9388065][-5.6390829 -4.5093632 -3.5931396 -2.30505 -0.4078474 1.7996461 3.7874119 4.5342417 4.3690596 3.2331908 1.6852095 -0.2744298 -1.8985929 -2.9450881 -3.6188025][-5.832737 -4.8401728 -4.0086679 -2.7470806 -0.73976827 1.5849204 3.5974967 4.2399187 3.9600275 2.7361767 1.2000623 -0.57813752 -2.0033307 -3.0090892 -3.7282295][-6.0324841 -5.3860245 -4.8349819 -3.8140395 -2.0691881 -0.047415972 1.6878436 2.2648909 2.0499508 0.99458027 -0.31308806 -1.6879098 -2.7678308 -3.576426 -4.1106262][-6.0544434 -5.8498154 -5.6723394 -4.9835262 -3.6558051 -2.0819454 -0.67521691 -0.087226629 -0.27002215 -1.2603798 -2.342864 -3.3025992 -4.0130739 -4.4893045 -4.5741363][-5.8561988 -6.0486984 -6.2079296 -5.8690114 -5.0603328 -4.0506725 -3.0841901 -2.5381014 -2.6647329 -3.4494152 -4.2164121 -4.7514381 -5.1070929 -5.2275066 -4.8767467][-5.3864431 -5.9170618 -6.4253855 -6.4076977 -6.0306287 -5.46087 -4.8268924 -4.2471128 -4.1895676 -4.7349949 -5.2617593 -5.5492611 -5.6866145 -5.543643 -4.985229][-4.6874161 -5.320169 -6.09789 -6.4006996 -6.3162537 -5.9686346 -5.5068836 -4.9270592 -4.7098475 -5.0657783 -5.437026 -5.5339155 -5.5449085 -5.2921281 -4.7935019][-3.9392533 -4.4017324 -5.2854362 -5.808105 -5.9564304 -5.8112249 -5.5251451 -4.9849615 -4.5510111 -4.594511 -4.7457604 -4.7834883 -4.8978014 -4.8513288 -4.6609139]]...]
INFO - root - 2017-12-15 07:31:44.974932: step 23210, loss = 0.22, batch loss = 0.19 (36.0 examples/sec; 0.222 sec/batch; 19h:04m:47s remains)
INFO - root - 2017-12-15 07:31:47.253342: step 23220, loss = 0.19, batch loss = 0.16 (36.2 examples/sec; 0.221 sec/batch; 19h:00m:25s remains)
INFO - root - 2017-12-15 07:31:49.525510: step 23230, loss = 0.35, batch loss = 0.32 (34.5 examples/sec; 0.232 sec/batch; 19h:55m:38s remains)
INFO - root - 2017-12-15 07:31:51.810394: step 23240, loss = 0.23, batch loss = 0.19 (35.8 examples/sec; 0.223 sec/batch; 19h:10m:53s remains)
INFO - root - 2017-12-15 07:31:54.069195: step 23250, loss = 0.21, batch loss = 0.17 (35.0 examples/sec; 0.228 sec/batch; 19h:37m:29s remains)
INFO - root - 2017-12-15 07:31:56.332160: step 23260, loss = 0.21, batch loss = 0.17 (34.8 examples/sec; 0.230 sec/batch; 19h:43m:55s remains)
INFO - root - 2017-12-15 07:31:58.616143: step 23270, loss = 0.19, batch loss = 0.16 (35.2 examples/sec; 0.227 sec/batch; 19h:32m:17s remains)
INFO - root - 2017-12-15 07:32:00.859779: step 23280, loss = 0.23, batch loss = 0.20 (36.5 examples/sec; 0.219 sec/batch; 18h:49m:54s remains)
INFO - root - 2017-12-15 07:32:03.133935: step 23290, loss = 0.20, batch loss = 0.17 (35.0 examples/sec; 0.228 sec/batch; 19h:37m:03s remains)
INFO - root - 2017-12-15 07:32:05.434101: step 23300, loss = 0.28, batch loss = 0.25 (34.6 examples/sec; 0.231 sec/batch; 19h:50m:53s remains)
2017-12-15 07:32:05.734784: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9419351 -5.6201096 -5.5823474 -5.7162085 -6.0916529 -6.2522826 -6.1646414 -6.1338234 -6.1693707 -6.2143126 -6.1604548 -6.205441 -6.2889557 -6.0384378 -5.4879694][-6.8469038 -6.9519186 -6.6252851 -6.4955006 -6.8641353 -7.0191922 -6.8441725 -6.8260455 -6.8017035 -6.7583437 -6.8274703 -6.9284949 -6.8773994 -6.5034418 -5.8603725][-8.4430676 -7.3044276 -6.4913611 -5.900424 -6.2131395 -6.5228629 -6.5025611 -6.7189269 -6.8007193 -6.7795272 -7.2436285 -7.586441 -7.2597594 -6.6865034 -5.8870344][-9.6450291 -7.165863 -5.6695881 -4.3895421 -4.4380522 -4.7750974 -4.9097052 -5.6444073 -6.1207933 -6.344451 -7.608191 -8.6366177 -8.2424641 -7.3721189 -6.2111444][-9.7110577 -6.4361033 -4.2386575 -2.2975507 -1.9855728 -2.1850014 -2.2306159 -3.2592969 -4.0821514 -4.4580555 -6.4302521 -8.3539333 -8.3133335 -7.5753684 -6.3789196][-8.9093218 -4.9771657 -2.2087348 0.20715523 0.792289 0.85372043 1.0639417 -0.0051348209 -1.0571784 -1.48382 -4.012136 -6.8454928 -7.5524197 -7.2375169 -6.23579][-7.7883916 -4.0597534 -1.0754145 1.6952345 2.6852312 3.3071675 4.2174029 3.589118 2.6553931 2.1619754 -0.92179096 -4.7444391 -6.3287792 -6.6057525 -5.928834][-7.9667535 -4.5652771 -2.0984607 0.45282149 1.6387374 2.9386363 4.6558433 4.7656207 4.3802652 4.0820093 0.78870821 -3.5266488 -5.7157722 -6.5647655 -6.0607109][-8.4436951 -5.6311436 -3.9923129 -2.0113487 -0.89885104 0.62773442 2.682157 3.198462 3.1419749 2.8728037 -0.34934103 -4.4207354 -6.3368421 -7.1011682 -6.433322][-9.20387 -6.902216 -5.9073033 -4.40662 -3.3702822 -1.922538 -0.24735546 0.22312498 0.30258369 -0.051204443 -2.9794686 -6.2830138 -7.5901432 -7.970871 -6.9936514][-9.53236 -7.87053 -7.6143684 -6.8454938 -6.2208281 -5.2555671 -4.1055374 -3.7040324 -3.36623 -3.5140629 -5.7166758 -8.0093555 -8.6267519 -8.4695683 -7.211091][-9.0095806 -7.9291172 -8.2366047 -8.1862125 -8.1274643 -7.7644081 -7.3890691 -7.2188139 -6.78282 -6.6291533 -7.8435116 -8.938448 -8.933485 -8.4023342 -7.123199][-7.9019775 -7.0759783 -7.5314035 -7.827785 -8.0983028 -8.2024355 -8.3263407 -8.4243679 -8.1521187 -7.9633131 -8.4919662 -8.8305378 -8.5458508 -7.8263373 -6.7224345][-6.7535133 -6.0249257 -6.3944383 -6.7248306 -7.0615907 -7.2731276 -7.5534925 -7.7051258 -7.584075 -7.4086108 -7.4625144 -7.3793745 -7.0930166 -6.5656562 -5.897995][-5.6994338 -4.9978604 -5.2229309 -5.4874897 -5.767292 -5.9800205 -6.1912651 -6.2878175 -6.2206736 -6.0487223 -5.8935003 -5.7403965 -5.5893831 -5.3338785 -5.0545073]]...]
INFO - root - 2017-12-15 07:32:08.012846: step 23310, loss = 0.21, batch loss = 0.17 (34.7 examples/sec; 0.231 sec/batch; 19h:49m:01s remains)
INFO - root - 2017-12-15 07:32:10.265423: step 23320, loss = 0.28, batch loss = 0.25 (35.5 examples/sec; 0.225 sec/batch; 19h:20m:25s remains)
INFO - root - 2017-12-15 07:32:12.527987: step 23330, loss = 0.17, batch loss = 0.13 (35.1 examples/sec; 0.228 sec/batch; 19h:34m:34s remains)
INFO - root - 2017-12-15 07:32:14.792613: step 23340, loss = 0.23, batch loss = 0.20 (35.5 examples/sec; 0.226 sec/batch; 19h:22m:04s remains)
INFO - root - 2017-12-15 07:32:17.078107: step 23350, loss = 0.28, batch loss = 0.25 (35.4 examples/sec; 0.226 sec/batch; 19h:25m:53s remains)
INFO - root - 2017-12-15 07:32:19.339908: step 23360, loss = 0.22, batch loss = 0.19 (35.5 examples/sec; 0.225 sec/batch; 19h:21m:30s remains)
INFO - root - 2017-12-15 07:32:21.576664: step 23370, loss = 0.24, batch loss = 0.20 (36.3 examples/sec; 0.220 sec/batch; 18h:55m:58s remains)
INFO - root - 2017-12-15 07:32:23.842150: step 23380, loss = 0.26, batch loss = 0.23 (35.7 examples/sec; 0.224 sec/batch; 19h:14m:06s remains)
INFO - root - 2017-12-15 07:32:26.118185: step 23390, loss = 0.28, batch loss = 0.25 (34.3 examples/sec; 0.233 sec/batch; 20h:02m:06s remains)
INFO - root - 2017-12-15 07:32:28.372080: step 23400, loss = 0.29, batch loss = 0.26 (36.2 examples/sec; 0.221 sec/batch; 18h:58m:51s remains)
2017-12-15 07:32:28.660205: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3628612 -4.5153065 -5.168685 -5.7794485 -5.5659146 -4.0589743 -2.7023664 -2.6749322 -3.1925166 -3.8391342 -4.8568411 -5.5509028 -5.7334423 -5.445323 -4.72014][-3.2205815 -4.4957747 -4.9060726 -5.3514996 -4.9673138 -3.2192016 -1.7086774 -1.6532929 -2.1962402 -2.9565606 -4.2275052 -5.1786079 -5.5406618 -5.43503 -5.0040436][-3.6347928 -4.265439 -4.6039448 -4.9075704 -4.0458217 -1.9747374 -0.41240358 -0.19933033 -0.70584726 -1.6686 -3.2222333 -4.4838343 -5.0383148 -5.0075655 -4.7734947][-4.0705404 -4.2965479 -4.4553585 -4.2697906 -2.7818811 -0.42811382 1.1100655 1.4666741 0.98943138 -0.18907452 -2.0439665 -3.6727958 -4.4815083 -4.5737972 -4.424058][-4.7684879 -4.6416531 -4.294003 -3.4189644 -1.4063394 1.096595 2.5172453 2.9927034 2.5748062 1.298805 -0.77636063 -2.7589097 -3.965265 -4.351872 -4.2523975][-5.5324564 -5.1783919 -4.3584976 -2.9764998 -0.63435352 1.8843474 3.3844414 3.983954 3.6215 2.3301206 0.24185348 -1.9558727 -3.5566506 -4.3183284 -4.2936373][-5.7686391 -5.5971084 -4.7098227 -3.1288962 -0.80886495 1.6594207 3.4245305 4.2231412 3.8921008 2.7388873 0.79171157 -1.4770775 -3.3212142 -4.3966374 -4.4307351][-5.94047 -5.7151003 -4.9721041 -3.5097702 -1.3700707 0.764766 2.6660562 3.6004987 3.3566947 2.3207316 0.57884049 -1.5472208 -3.306201 -4.4775996 -4.4674168][-6.0975647 -5.6452541 -5.0342493 -3.7912741 -1.9515634 -0.17362022 1.6222908 2.5613952 2.2852955 1.2933273 -0.23566937 -2.0397203 -3.4857204 -4.495131 -4.431612][-5.8566427 -5.1684666 -4.7812753 -3.7955213 -2.2794597 -0.80705643 0.72874904 1.4759786 1.1146979 0.29079771 -0.99295342 -2.4651802 -3.5779572 -4.3851986 -4.368309][-5.3788557 -4.4035673 -4.2002978 -3.4984674 -2.3391616 -1.0850222 0.19145751 0.62774444 0.10424399 -0.68702281 -1.9005741 -3.1787312 -4.0941896 -4.71358 -4.7258172][-5.2103176 -4.0064616 -3.8914733 -3.40517 -2.5765655 -1.6389899 -0.54881155 -0.27489281 -0.95695972 -1.8134081 -2.944521 -4.0229678 -4.7324505 -5.1279392 -5.1919289][-5.314002 -4.1089249 -4.2289586 -4.1104193 -3.6080887 -2.9013305 -1.9039632 -1.6521578 -2.3908567 -3.1740141 -4.0646195 -4.8343425 -5.2873297 -5.5700006 -5.7558317][-6.2704449 -5.1848021 -5.4588737 -5.7803526 -5.527379 -4.7771673 -3.7568755 -3.5524569 -4.0909944 -4.4739361 -4.8709273 -5.1899958 -5.3848209 -5.6560326 -6.0346222][-7.9572134 -7.0488658 -7.4003906 -7.9127955 -7.6353488 -6.7059131 -5.5707617 -5.2976027 -5.4124093 -5.2993574 -5.204361 -5.1137753 -5.0299482 -5.2218437 -5.77995]]...]
INFO - root - 2017-12-15 07:32:30.924695: step 23410, loss = 0.20, batch loss = 0.17 (35.9 examples/sec; 0.223 sec/batch; 19h:09m:23s remains)
INFO - root - 2017-12-15 07:32:33.185517: step 23420, loss = 0.19, batch loss = 0.15 (35.7 examples/sec; 0.224 sec/batch; 19h:13m:29s remains)
INFO - root - 2017-12-15 07:32:35.452352: step 23430, loss = 0.25, batch loss = 0.22 (35.8 examples/sec; 0.223 sec/batch; 19h:10m:28s remains)
INFO - root - 2017-12-15 07:32:37.764408: step 23440, loss = 0.21, batch loss = 0.18 (35.3 examples/sec; 0.227 sec/batch; 19h:27m:36s remains)
INFO - root - 2017-12-15 07:32:40.074785: step 23450, loss = 0.20, batch loss = 0.17 (35.0 examples/sec; 0.229 sec/batch; 19h:38m:22s remains)
INFO - root - 2017-12-15 07:32:42.359661: step 23460, loss = 0.26, batch loss = 0.23 (35.9 examples/sec; 0.223 sec/batch; 19h:06m:21s remains)
INFO - root - 2017-12-15 07:32:44.679773: step 23470, loss = 0.20, batch loss = 0.17 (33.5 examples/sec; 0.239 sec/batch; 20h:28m:28s remains)
INFO - root - 2017-12-15 07:32:46.949948: step 23480, loss = 0.28, batch loss = 0.25 (34.1 examples/sec; 0.235 sec/batch; 20h:09m:33s remains)
INFO - root - 2017-12-15 07:32:49.252753: step 23490, loss = 0.21, batch loss = 0.18 (35.5 examples/sec; 0.226 sec/batch; 19h:21m:57s remains)
INFO - root - 2017-12-15 07:32:51.514643: step 23500, loss = 0.19, batch loss = 0.15 (35.8 examples/sec; 0.224 sec/batch; 19h:11m:34s remains)
2017-12-15 07:32:51.778080: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8110576 -5.1483841 -5.8259325 -6.0707068 -5.8126535 -5.3581867 -5.1875696 -4.9145451 -4.620368 -4.8225746 -5.4718828 -6.0852108 -6.2483625 -6.1562653 -6.1375732][-4.2795744 -4.6002588 -5.5749531 -5.9747066 -5.6758404 -5.2320948 -4.980237 -4.5139775 -3.9974275 -4.2595854 -5.1821995 -6.0706625 -6.5183516 -6.5395651 -6.5437851][-4.2746673 -3.5503874 -4.6624289 -5.0936279 -4.6614332 -4.16107 -3.888114 -3.4215076 -2.7810373 -3.0181146 -4.0428128 -5.1760731 -5.9058075 -6.0960059 -6.1991587][-4.4202471 -2.7173028 -3.7687571 -4.0102205 -3.2310877 -2.2726493 -1.5672098 -1.1251237 -0.77936625 -1.2578764 -2.4015024 -3.7919927 -4.8350096 -5.4230628 -5.737277][-4.6370659 -2.0558279 -2.9888477 -3.1102159 -2.1487978 -0.74789977 0.60195756 1.1051033 0.92552543 0.016993523 -1.1904353 -2.5980787 -3.6856463 -4.5041008 -5.0233307][-4.2391682 -1.5984638 -2.5369351 -2.6544194 -1.6655269 0.20643997 2.3977625 3.1836712 2.4443023 1.0112729 -0.27325308 -1.6706436 -2.8374426 -3.73987 -4.3502374][-3.1354074 -0.88366079 -1.7520847 -1.8169035 -0.76724112 1.496531 4.4671669 5.7049789 4.4351339 2.2447526 0.59547782 -1.0107321 -2.5116119 -3.5780354 -4.1961241][-2.4827747 -0.38142693 -1.0736873 -0.97144866 0.11447954 2.2386663 5.1721344 6.4807959 4.9412441 2.2897613 0.3611753 -1.385314 -3.0871103 -4.0324726 -4.4321318][-2.0405917 -0.26421833 -0.96566916 -0.83016813 -0.041208744 1.4525533 3.616987 4.57172 3.1514828 0.65967083 -1.0375395 -2.5799565 -4.27422 -5.1069994 -5.2763219][-1.6681461 -0.2226584 -1.1311687 -1.0650185 -0.58590627 0.33633661 1.7407429 2.4165213 1.2866919 -0.94830644 -2.4290423 -3.7550042 -5.42591 -6.3303013 -6.3952885][-1.5886201 -0.56355488 -1.6481812 -1.6491363 -1.3903136 -0.88010347 -0.027285337 0.47226334 -0.38134241 -2.4033875 -3.7666831 -4.8781586 -6.2561884 -7.084444 -7.0645437][-2.4388847 -1.7710892 -2.8326337 -2.8081882 -2.670413 -2.531 -2.0154181 -1.6032965 -2.2125561 -3.9033251 -5.1156349 -5.87261 -6.6785297 -7.283617 -7.2432013][-3.4497662 -3.092078 -3.9310489 -3.7908235 -3.838897 -4.0078349 -3.6791127 -3.2169054 -3.6074429 -5.0061445 -6.0830388 -6.5087433 -6.7646475 -7.126791 -7.0614662][-4.7285624 -4.7823787 -5.3643179 -5.0713396 -5.0681725 -5.2914238 -5.143496 -4.7334943 -4.9224091 -5.9588671 -6.721817 -6.7031984 -6.4304857 -6.5006123 -6.3711433][-6.4496193 -6.5671864 -6.9308958 -6.5775852 -6.4094219 -6.5021677 -6.3707104 -6.0369415 -6.0584679 -6.6121597 -6.87803 -6.4623604 -5.8889689 -5.7435923 -5.5422611]]...]
INFO - root - 2017-12-15 07:32:54.093663: step 23510, loss = 0.24, batch loss = 0.21 (34.0 examples/sec; 0.235 sec/batch; 20h:12m:40s remains)
INFO - root - 2017-12-15 07:32:56.374288: step 23520, loss = 0.30, batch loss = 0.26 (34.4 examples/sec; 0.233 sec/batch; 19h:57m:40s remains)
INFO - root - 2017-12-15 07:32:58.654318: step 23530, loss = 0.22, batch loss = 0.19 (34.4 examples/sec; 0.232 sec/batch; 19h:56m:55s remains)
INFO - root - 2017-12-15 07:33:00.943925: step 23540, loss = 0.24, batch loss = 0.20 (34.6 examples/sec; 0.231 sec/batch; 19h:51m:01s remains)
INFO - root - 2017-12-15 07:33:03.245579: step 23550, loss = 0.17, batch loss = 0.14 (34.4 examples/sec; 0.232 sec/batch; 19h:56m:36s remains)
INFO - root - 2017-12-15 07:33:05.508727: step 23560, loss = 0.20, batch loss = 0.17 (35.1 examples/sec; 0.228 sec/batch; 19h:34m:47s remains)
INFO - root - 2017-12-15 07:33:07.834695: step 23570, loss = 0.20, batch loss = 0.17 (34.5 examples/sec; 0.232 sec/batch; 19h:53m:15s remains)
INFO - root - 2017-12-15 07:33:10.201037: step 23580, loss = 0.25, batch loss = 0.21 (33.2 examples/sec; 0.241 sec/batch; 20h:39m:37s remains)
INFO - root - 2017-12-15 07:33:12.449272: step 23590, loss = 0.23, batch loss = 0.20 (37.1 examples/sec; 0.216 sec/batch; 18h:30m:57s remains)
INFO - root - 2017-12-15 07:33:14.700576: step 23600, loss = 0.22, batch loss = 0.19 (36.0 examples/sec; 0.222 sec/batch; 19h:05m:29s remains)
2017-12-15 07:33:14.979211: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7325485 -4.932785 -4.9326735 -5.1418676 -5.2419109 -5.403059 -5.3704586 -5.3229523 -5.4445496 -5.2437248 -4.9053354 -4.5480452 -4.4882569 -4.6723294 -4.8701754][-4.5452418 -4.7676167 -4.6726794 -4.9301496 -4.9922657 -5.11149 -5.0915947 -5.0420523 -5.1397 -4.9738483 -4.5763483 -4.0765691 -4.0037985 -4.3605623 -4.7943664][-4.7844734 -4.4434967 -4.3208408 -4.6282754 -4.621964 -4.5506115 -4.4392805 -4.3585014 -4.3528748 -4.18757 -3.810133 -3.2853851 -3.29007 -3.881129 -4.4883103][-5.000844 -4.1387534 -3.9144726 -4.1522717 -3.9623156 -3.6316171 -3.3874757 -3.3062048 -3.2372532 -3.1951299 -2.8484874 -2.3058898 -2.4775929 -3.3135626 -4.130496][-4.7609959 -3.2403674 -2.8774462 -3.0179684 -2.7525959 -2.1714823 -1.7128524 -1.5927263 -1.59487 -1.8008659 -1.5535457 -1.0728229 -1.4383135 -2.5218368 -3.513891][-4.1734304 -2.0332832 -1.570462 -1.6516593 -1.3078163 -0.39170861 0.31891584 0.49408031 0.45806813 -0.048794985 -0.071590424 0.2091496 -0.4075681 -1.6965208 -2.7946591][-3.2755051 -1.1294495 -0.6681025 -0.70480692 -0.15245175 1.2241073 2.3780518 2.7185354 2.5748014 1.7135458 1.2653301 1.2901156 0.48657203 -0.95952535 -2.1627767][-2.7415853 -0.65906394 -0.20214224 -0.10492516 0.74176407 2.4165297 3.8238888 4.3266568 4.23497 3.2046909 2.4954095 2.3659506 1.4075682 -0.15740824 -1.4664453][-2.6892788 -0.7588079 -0.32008779 -0.099378109 0.7497642 2.2186632 3.5040855 4.03191 4.083849 3.2050982 2.471807 2.2947359 1.271734 -0.42821586 -1.7563052][-3.1421885 -1.4100653 -1.0792292 -0.87071037 -0.041538239 1.1782322 2.1985269 2.6101842 2.8337488 2.155395 1.5019383 1.3610935 0.24635148 -1.5879695 -2.9341323][-3.7996566 -2.1708183 -1.8299896 -1.4777329 -0.50273371 0.51568055 1.3270214 1.698226 2.0308132 1.368407 0.64996529 0.40256929 -0.81831563 -2.890101 -4.2994876][-4.2242508 -2.606288 -2.2782109 -1.8914952 -0.98546112 -0.23580337 0.43320489 0.81467175 1.1465058 0.41870832 -0.35813606 -0.75921893 -2.1471643 -4.3789349 -5.6752834][-4.7562218 -3.3508511 -3.1238947 -2.9145715 -2.3235571 -1.8607579 -1.2816919 -0.754395 -0.3220638 -0.96586967 -1.7488704 -2.3030577 -3.7675552 -5.9552636 -6.9780426][-5.5864677 -4.5266562 -4.3805213 -4.3726349 -4.0566306 -3.7254903 -3.152612 -2.544044 -2.0722227 -2.5595403 -3.2168226 -3.7338052 -5.0707512 -7.0240393 -7.7719369][-5.9869986 -5.2023869 -5.1531854 -5.2551889 -5.0303221 -4.7777452 -4.3489413 -3.9623046 -3.6608768 -3.9849367 -4.4584937 -4.8766308 -5.9603939 -7.515398 -7.9628234]]...]
INFO - root - 2017-12-15 07:33:17.223319: step 23610, loss = 0.17, batch loss = 0.14 (35.0 examples/sec; 0.229 sec/batch; 19h:38m:02s remains)
INFO - root - 2017-12-15 07:33:19.497913: step 23620, loss = 0.22, batch loss = 0.19 (35.4 examples/sec; 0.226 sec/batch; 19h:23m:48s remains)
INFO - root - 2017-12-15 07:33:21.788397: step 23630, loss = 0.16, batch loss = 0.13 (35.3 examples/sec; 0.227 sec/batch; 19h:26m:59s remains)
INFO - root - 2017-12-15 07:33:24.071334: step 23640, loss = 0.20, batch loss = 0.17 (35.4 examples/sec; 0.226 sec/batch; 19h:24m:17s remains)
INFO - root - 2017-12-15 07:33:26.324159: step 23650, loss = 0.28, batch loss = 0.25 (35.3 examples/sec; 0.226 sec/batch; 19h:25m:48s remains)
INFO - root - 2017-12-15 07:33:28.605371: step 23660, loss = 0.20, batch loss = 0.16 (35.2 examples/sec; 0.228 sec/batch; 19h:31m:22s remains)
INFO - root - 2017-12-15 07:33:30.898499: step 23670, loss = 0.24, batch loss = 0.21 (35.3 examples/sec; 0.227 sec/batch; 19h:26m:14s remains)
INFO - root - 2017-12-15 07:33:33.158205: step 23680, loss = 0.21, batch loss = 0.17 (35.2 examples/sec; 0.227 sec/batch; 19h:29m:34s remains)
INFO - root - 2017-12-15 07:33:35.452854: step 23690, loss = 0.26, batch loss = 0.23 (34.7 examples/sec; 0.231 sec/batch; 19h:47m:03s remains)
INFO - root - 2017-12-15 07:33:37.692680: step 23700, loss = 0.27, batch loss = 0.23 (35.7 examples/sec; 0.224 sec/batch; 19h:13m:34s remains)
2017-12-15 07:33:37.978331: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6424637 -1.6336012 -1.4278326 -1.5908997 -1.9599756 -2.742337 -3.3950648 -4.6224775 -5.628027 -5.7974215 -5.388536 -4.4847565 -3.5453722 -2.7485266 -2.5395465][-4.4861188 -2.7661309 -2.715194 -2.9363191 -3.5334272 -4.1085052 -4.5365677 -5.328804 -5.9065504 -5.9024515 -5.5014491 -4.8041263 -4.0787034 -3.4563966 -3.3332582][-6.4259052 -4.3913488 -4.3051939 -4.3618507 -4.826725 -4.8509731 -4.84028 -5.190002 -5.5850439 -5.7286587 -5.7384682 -5.5147176 -5.1593003 -4.7614555 -4.6823454][-8.2497835 -5.2301846 -5.1485868 -5.137362 -5.3881817 -5.0286436 -4.896873 -4.9783077 -5.2061691 -5.534523 -5.9253092 -6.1652484 -6.067153 -5.8301177 -5.8634658][-9.342474 -5.1490068 -5.0547581 -5.0328503 -4.8934479 -4.1235733 -3.5852656 -3.1877544 -3.1793673 -3.7135582 -4.3761659 -4.9995689 -5.1719108 -5.0649176 -5.2990875][-7.7671003 -4.2587142 -4.1846395 -4.0368266 -3.4441662 -2.1806448 -1.1021771 -0.21522689 -0.1618154 -1.0430474 -2.196327 -3.2353625 -3.54756 -3.4997551 -3.7405522][-5.282938 -2.8406224 -2.9220004 -2.6932719 -1.603744 -0.0024945736 1.4412603 2.5697048 2.5901263 1.3625264 -0.19459939 -1.3608145 -1.5493586 -1.4460201 -1.6941644][-4.04016 -1.8006914 -2.1596096 -1.9700375 -0.5146507 1.3346252 3.2007329 4.7621622 4.98934 3.7430847 2.1101472 0.82501364 0.39666009 0.3153882 0.056656122][-3.5131521 -1.5439922 -2.0823851 -1.8889607 -0.38282526 1.5070133 3.4729335 5.0984783 5.4968033 4.6234913 3.3400486 2.0980103 1.4917896 1.3072755 1.1230149][-3.6429777 -1.9533951 -2.6680696 -2.5067358 -1.2626376 0.43348527 2.138891 3.4709957 3.7692273 3.3086321 2.5925944 1.8523881 1.5703344 1.6739068 1.4627926][-3.872184 -2.4336569 -3.3774958 -3.4602246 -2.9287126 -1.7432673 -0.49283528 0.38747907 0.47242618 0.48841262 0.43417192 0.4211762 0.66781092 1.2859795 1.1418757][-3.925355 -2.6806498 -3.8204336 -4.2437854 -4.5339537 -3.8048685 -2.950139 -2.640563 -2.7283645 -2.2406063 -1.8509387 -1.3576577 -0.73798954 0.44753075 0.59285569][-3.5230513 -2.252836 -3.5402772 -4.4076853 -5.4105043 -4.9525776 -4.235671 -4.2288127 -4.4162345 -3.7469676 -3.2211304 -2.5264697 -1.747494 -0.17254925 0.12252641][-3.0672321 -1.5342722 -2.9452953 -4.2508416 -5.9972115 -5.9161549 -5.2081652 -5.1427317 -5.1737571 -4.4863548 -4.0223064 -3.39973 -2.6429865 -1.1057525 -0.75700569][-2.8964193 -1.1105505 -2.6348553 -4.3151646 -6.7407379 -7.1463156 -6.5351639 -6.2867355 -5.9605417 -5.0821609 -4.5181608 -3.9605963 -3.4175987 -2.2219095 -1.8433216]]...]
INFO - root - 2017-12-15 07:33:40.230365: step 23710, loss = 0.21, batch loss = 0.18 (34.7 examples/sec; 0.231 sec/batch; 19h:47m:58s remains)
INFO - root - 2017-12-15 07:33:42.478794: step 23720, loss = 0.25, batch loss = 0.22 (35.0 examples/sec; 0.229 sec/batch; 19h:36m:23s remains)
INFO - root - 2017-12-15 07:33:44.785962: step 23730, loss = 0.21, batch loss = 0.17 (35.3 examples/sec; 0.227 sec/batch; 19h:25m:40s remains)
INFO - root - 2017-12-15 07:33:47.043633: step 23740, loss = 0.22, batch loss = 0.18 (35.6 examples/sec; 0.224 sec/batch; 19h:14m:57s remains)
INFO - root - 2017-12-15 07:33:49.276259: step 23750, loss = 0.26, batch loss = 0.23 (35.8 examples/sec; 0.224 sec/batch; 19h:10m:12s remains)
INFO - root - 2017-12-15 07:33:51.540491: step 23760, loss = 0.22, batch loss = 0.19 (35.2 examples/sec; 0.227 sec/batch; 19h:28m:28s remains)
INFO - root - 2017-12-15 07:33:53.845700: step 23770, loss = 0.22, batch loss = 0.18 (34.6 examples/sec; 0.231 sec/batch; 19h:50m:58s remains)
INFO - root - 2017-12-15 07:33:56.122985: step 23780, loss = 0.28, batch loss = 0.25 (34.2 examples/sec; 0.234 sec/batch; 20h:04m:13s remains)
INFO - root - 2017-12-15 07:33:58.404945: step 23790, loss = 0.22, batch loss = 0.19 (36.1 examples/sec; 0.222 sec/batch; 19h:00m:35s remains)
INFO - root - 2017-12-15 07:34:00.677371: step 23800, loss = 0.17, batch loss = 0.14 (35.5 examples/sec; 0.225 sec/batch; 19h:18m:37s remains)
2017-12-15 07:34:00.963879: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8410478 -4.4616466 -4.7595739 -5.1186461 -5.0322556 -4.9730597 -5.0340862 -4.8876271 -4.6631451 -4.3483648 -3.7771282 -3.2931356 -2.6943612 -2.0960357 -2.0869598][-4.4964848 -4.9530945 -5.3096056 -5.5935621 -5.4203382 -5.3196726 -5.2362852 -5.010272 -5.0286207 -5.0533047 -4.6569667 -4.241106 -3.8041658 -3.3291802 -3.2478111][-4.7740088 -5.0872812 -5.4547639 -5.6361837 -5.3205276 -4.987957 -4.5889082 -4.288084 -4.5887785 -5.0705767 -5.154355 -5.0428953 -4.8579845 -4.5481596 -4.3823824][-4.4971437 -4.5509143 -4.9716234 -5.054615 -4.5028658 -3.8004534 -3.0159001 -2.6674714 -3.2637365 -4.2456036 -4.9440975 -5.2622375 -5.2904291 -5.1000462 -4.8883996][-3.4935851 -3.541714 -3.9985709 -3.8852258 -2.9497826 -1.6816607 -0.55081046 -0.16756082 -0.94747758 -2.3569252 -3.7584996 -4.5619779 -4.7371116 -4.5541306 -4.2903891][-2.2234757 -2.2799239 -2.8425395 -2.5493073 -1.1594696 0.6227417 1.8514669 2.2585943 1.4482012 -0.18063092 -2.1506033 -3.4823842 -3.7197726 -3.6757176 -3.6234674][-0.44998276 -0.81453633 -1.5287951 -1.1383781 0.43932462 2.3919938 3.5877168 3.9956887 3.2121966 1.5860827 -0.5662744 -2.0849471 -2.4286876 -2.5291021 -2.6070297][0.74755478 0.44734716 -0.43035471 -0.20711803 1.1818483 2.9800942 3.905318 4.3091755 3.6009333 2.1561015 0.21040297 -1.30995 -1.8064877 -2.1628981 -2.480495][0.7696588 0.7031827 -0.26270449 -0.35986018 0.49669051 1.9224408 2.6703551 3.1013043 2.493706 1.4556243 -0.041187286 -1.41749 -2.088825 -2.6097832 -3.0736089][-0.61707497 -0.35597467 -1.4344542 -1.8906014 -1.470448 -0.3305366 0.35448933 0.87116671 0.53323126 -0.069072962 -1.0503002 -2.1205785 -2.734581 -3.1402328 -3.3989706][-2.5728636 -2.0308857 -3.1061032 -3.8186483 -3.7175593 -2.8282986 -2.1166325 -1.3889163 -1.4067404 -1.6426753 -2.2670233 -3.0462956 -3.6105065 -3.9614978 -3.9911933][-4.5672808 -3.6996946 -4.555408 -5.1858988 -5.0976686 -4.4861441 -3.9370279 -3.2130189 -2.911319 -2.9018209 -3.2628794 -3.725359 -4.1200666 -4.422967 -4.2890229][-6.1847253 -5.3108339 -5.9541368 -6.3325014 -6.1105108 -5.7418642 -5.4171219 -4.88026 -4.5194745 -4.4542637 -4.5555639 -4.6025057 -4.709115 -4.8891373 -4.4474297][-6.9151754 -6.2251673 -6.8331385 -7.1102428 -6.9108915 -6.7731695 -6.677577 -6.4200773 -6.1187716 -5.9327455 -5.591054 -5.1828194 -5.0968723 -5.1367121 -4.5335503][-6.5735226 -6.1624775 -6.9274683 -7.2923279 -7.1689348 -7.1244678 -7.08181 -7.0027256 -6.7816248 -6.4973326 -5.7439976 -4.9535408 -4.6917729 -4.6881514 -4.1139293]]...]
INFO - root - 2017-12-15 07:34:03.213764: step 23810, loss = 0.22, batch loss = 0.18 (34.5 examples/sec; 0.232 sec/batch; 19h:54m:10s remains)
INFO - root - 2017-12-15 07:34:05.513403: step 23820, loss = 0.24, batch loss = 0.20 (34.3 examples/sec; 0.233 sec/batch; 20h:00m:02s remains)
INFO - root - 2017-12-15 07:34:07.834328: step 23830, loss = 0.27, batch loss = 0.24 (34.9 examples/sec; 0.229 sec/batch; 19h:40m:39s remains)
INFO - root - 2017-12-15 07:34:10.108318: step 23840, loss = 0.29, batch loss = 0.25 (34.6 examples/sec; 0.231 sec/batch; 19h:48m:08s remains)
INFO - root - 2017-12-15 07:34:12.404188: step 23850, loss = 0.19, batch loss = 0.16 (34.8 examples/sec; 0.230 sec/batch; 19h:44m:14s remains)
INFO - root - 2017-12-15 07:34:14.652226: step 23860, loss = 0.23, batch loss = 0.20 (34.9 examples/sec; 0.229 sec/batch; 19h:37m:29s remains)
INFO - root - 2017-12-15 07:34:16.905052: step 23870, loss = 0.30, batch loss = 0.27 (35.3 examples/sec; 0.227 sec/batch; 19h:25m:33s remains)
INFO - root - 2017-12-15 07:34:19.199891: step 23880, loss = 0.23, batch loss = 0.19 (34.0 examples/sec; 0.235 sec/batch; 20h:11m:02s remains)
INFO - root - 2017-12-15 07:34:21.479373: step 23890, loss = 0.26, batch loss = 0.22 (35.8 examples/sec; 0.223 sec/batch; 19h:08m:28s remains)
INFO - root - 2017-12-15 07:34:23.780406: step 23900, loss = 0.36, batch loss = 0.32 (33.3 examples/sec; 0.240 sec/batch; 20h:34m:02s remains)
2017-12-15 07:34:24.058717: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3317542 -6.4602385 -6.5886841 -6.2586737 -5.66919 -5.2046003 -4.9403076 -4.6993217 -4.7856436 -5.269846 -5.9588027 -6.8713903 -6.8222809 -6.1139479 -5.5596771][-5.0794282 -6.6339817 -6.6575766 -5.6834497 -4.6377916 -3.9904971 -3.5903072 -3.3780284 -3.6073587 -4.0643806 -4.86145 -6.1903486 -6.3160191 -5.3104792 -4.4014182][-5.1520195 -6.2488489 -6.1294374 -4.7197957 -3.2088895 -2.2242119 -1.6446792 -1.5688431 -2.0822213 -2.7068288 -3.6216049 -4.9704361 -5.0464716 -3.7477851 -2.4586034][-5.5005665 -5.7068634 -5.4958639 -3.7706811 -1.8009107 -0.37428451 0.509145 0.58019924 -0.34816539 -1.4962077 -2.6502552 -3.8778925 -3.6970916 -2.0985625 -0.58675992][-5.6723642 -5.214325 -4.840456 -2.8074117 -0.42841768 1.3992615 2.5412476 2.6312826 1.3137314 -0.46716702 -2.0365756 -3.152132 -2.5956314 -0.87518787 0.47236204][-5.5329466 -4.8555269 -4.2494173 -1.8594348 0.91169143 3.0104921 4.2234783 4.1572666 2.3920009 0.051913023 -1.8227139 -2.8208313 -1.9071631 -0.13662338 0.96362019][-4.9925776 -4.6660595 -3.9631817 -1.3541753 1.7704494 4.0346317 5.0613642 4.7529058 2.6753004 -0.25999284 -2.3463795 -3.0762103 -1.9831104 -0.22281647 0.70821571][-5.08766 -4.5861053 -3.9361577 -1.3030053 1.8542712 3.9298737 4.7002859 4.118413 1.8246338 -1.2305404 -3.0029645 -3.3513122 -2.3471663 -0.65975606 0.36719632][-4.947422 -4.3768606 -3.7812161 -1.3378224 1.419301 2.9750879 3.3911397 2.7056143 0.75782466 -1.9208875 -3.1075952 -2.9839978 -2.0673926 -0.54272306 0.53344226][-4.8418694 -4.1844997 -3.7148461 -1.7016976 0.37925816 1.3048921 1.5155983 1.0921693 -0.10301971 -2.0405228 -2.4419734 -1.8149344 -1.0387968 0.20677662 1.1979449][-4.928472 -4.3496041 -4.1416016 -2.6247363 -1.1518629 -0.66651642 -0.50368 -0.44847345 -0.73985279 -1.8242173 -1.7455314 -0.82050431 -0.23492122 0.624809 1.5140891][-5.1598153 -4.8374643 -4.9282846 -3.7356274 -2.5100861 -2.2191007 -2.0457892 -1.6822859 -1.3426563 -1.9008615 -1.6061838 -0.49875128 -0.21368098 0.21200681 0.91068959][-5.2891979 -5.179594 -5.4915752 -4.5824986 -3.4830928 -3.314893 -3.3116031 -2.9734397 -2.3876145 -2.5545232 -2.1672623 -0.90455973 -0.562619 -0.43775642 -0.0852561][-5.2498031 -5.1964836 -5.642386 -5.0804644 -4.1996579 -4.1096153 -4.2322106 -4.0203753 -3.4556167 -3.3358383 -2.8107467 -1.5566874 -1.2674327 -1.3287199 -1.3020597][-5.1261511 -5.064599 -5.5774708 -5.32687 -4.6652641 -4.5674696 -4.8250723 -4.8438931 -4.4591551 -4.1591411 -3.6205716 -2.5686836 -2.3605804 -2.5888181 -2.7153041]]...]
INFO - root - 2017-12-15 07:34:26.346713: step 23910, loss = 0.22, batch loss = 0.18 (34.4 examples/sec; 0.233 sec/batch; 19h:56m:23s remains)
INFO - root - 2017-12-15 07:34:28.612057: step 23920, loss = 0.18, batch loss = 0.15 (35.8 examples/sec; 0.223 sec/batch; 19h:08m:01s remains)
INFO - root - 2017-12-15 07:34:30.919298: step 23930, loss = 0.21, batch loss = 0.18 (33.8 examples/sec; 0.237 sec/batch; 20h:16m:50s remains)
INFO - root - 2017-12-15 07:34:33.229149: step 23940, loss = 0.24, batch loss = 0.21 (35.6 examples/sec; 0.225 sec/batch; 19h:17m:12s remains)
INFO - root - 2017-12-15 07:34:35.470989: step 23950, loss = 0.22, batch loss = 0.18 (34.9 examples/sec; 0.229 sec/batch; 19h:39m:53s remains)
INFO - root - 2017-12-15 07:34:37.753977: step 23960, loss = 0.20, batch loss = 0.17 (35.4 examples/sec; 0.226 sec/batch; 19h:23m:15s remains)
INFO - root - 2017-12-15 07:34:40.035703: step 23970, loss = 0.26, batch loss = 0.23 (36.1 examples/sec; 0.222 sec/batch; 19h:01m:06s remains)
INFO - root - 2017-12-15 07:34:42.328123: step 23980, loss = 0.21, batch loss = 0.17 (35.0 examples/sec; 0.229 sec/batch; 19h:35m:32s remains)
INFO - root - 2017-12-15 07:34:44.600873: step 23990, loss = 0.33, batch loss = 0.30 (34.5 examples/sec; 0.232 sec/batch; 19h:52m:41s remains)
INFO - root - 2017-12-15 07:34:46.880760: step 24000, loss = 0.27, batch loss = 0.24 (33.6 examples/sec; 0.238 sec/batch; 20h:22m:59s remains)
2017-12-15 07:34:47.169440: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2036042 -4.53062 -4.8079681 -5.3844357 -5.9230413 -6.495141 -6.5400009 -6.018908 -5.1856813 -4.8573327 -5.0192337 -5.2119193 -5.4470563 -5.6363015 -5.3699026][-5.3218637 -4.8278255 -5.1158962 -5.6631632 -6.2156229 -6.7293167 -6.821003 -6.5147448 -5.8423405 -5.4691997 -5.6135015 -5.9529934 -6.2909927 -6.3033276 -5.7164121][-5.7530031 -5.3243752 -5.6665144 -5.948226 -6.0492249 -6.0580368 -5.8127742 -5.5830364 -5.294157 -5.2325606 -5.5093837 -5.9927416 -6.424592 -6.2488546 -5.4054847][-5.9639397 -5.4563737 -5.6690636 -5.507555 -4.9827094 -4.2538242 -3.3433137 -2.9887891 -3.2077503 -3.6250257 -4.1359205 -4.991395 -5.7228889 -5.7579784 -5.1446333][-5.9088345 -5.0070362 -5.0164285 -4.485219 -3.2753003 -1.647635 0.046265602 0.5533309 -0.38428688 -1.6547413 -2.7222466 -3.928021 -4.9109297 -5.21125 -4.9541807][-4.8210011 -3.9883142 -4.0057545 -3.2355659 -1.4057801 1.0249994 3.3446825 3.8118527 2.1075881 -0.13285351 -1.815863 -3.2517967 -4.1675696 -4.5012217 -4.529952][-3.3688369 -2.9039018 -2.7721477 -1.7215238 0.50983644 3.4604461 6.1404104 6.438324 4.1539764 1.3331053 -0.79049671 -2.37329 -3.1694114 -3.617949 -4.0334396][-2.8844631 -2.3495982 -1.9857562 -0.92663479 1.1327119 4.139472 6.8601046 7.0733871 4.6794319 1.7286024 -0.58901215 -2.1581662 -2.8484495 -3.388422 -4.1321349][-3.035862 -2.4976304 -2.1975226 -1.3693762 0.33483219 3.0648863 5.6212826 5.8252668 3.8318498 1.050211 -1.2798697 -2.8656125 -3.5201364 -3.9919097 -4.6957388][-3.6771545 -3.224355 -2.9453692 -2.1980402 -0.8105514 1.4778178 3.5969007 3.7001197 2.1981356 -0.14618087 -2.195209 -3.4818423 -4.0220757 -4.3476677 -4.8249345][-5.3442574 -5.0664687 -4.8329058 -4.2289534 -3.1923406 -1.4184468 0.17243099 0.39109087 -0.4426769 -2.0046008 -3.3868496 -3.9430351 -4.1403503 -4.3012867 -4.4923725][-6.6719351 -6.4172935 -6.42076 -6.0140634 -5.3042555 -4.1661873 -3.1255269 -2.9502294 -3.2804065 -4.0457373 -4.5012579 -4.3551235 -4.1424012 -4.0695915 -3.9707198][-7.0280256 -6.7058325 -6.8172073 -6.3302593 -5.6294127 -4.8620615 -4.250083 -4.3355274 -4.7140856 -5.1842852 -5.120276 -4.5655241 -4.1065822 -3.8264794 -3.3685524][-6.559473 -6.020628 -6.2044 -5.6798096 -5.0527 -4.6402826 -4.4267521 -4.7686405 -5.3951907 -5.71583 -5.36032 -4.6293859 -4.094214 -3.7406795 -3.0766733][-5.4545479 -4.4820967 -4.6677608 -4.2200747 -3.8778954 -3.9032564 -4.0688496 -4.6160874 -5.2842703 -5.3245258 -4.7219744 -4.1472669 -3.8632846 -3.6257253 -2.942626]]...]
INFO - root - 2017-12-15 07:34:49.460147: step 24010, loss = 0.25, batch loss = 0.21 (35.2 examples/sec; 0.227 sec/batch; 19h:27m:47s remains)
INFO - root - 2017-12-15 07:34:51.747636: step 24020, loss = 0.19, batch loss = 0.16 (35.8 examples/sec; 0.224 sec/batch; 19h:10m:23s remains)
INFO - root - 2017-12-15 07:34:54.044234: step 24030, loss = 0.20, batch loss = 0.17 (33.1 examples/sec; 0.241 sec/batch; 20h:41m:01s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 07:34:56.313825: step 24040, loss = 0.25, batch loss = 0.22 (35.6 examples/sec; 0.225 sec/batch; 19h:16m:39s remains)
INFO - root - 2017-12-15 07:34:58.631549: step 24050, loss = 0.22, batch loss = 0.18 (33.3 examples/sec; 0.240 sec/batch; 20h:33m:49s remains)
INFO - root - 2017-12-15 07:35:00.913882: step 24060, loss = 0.33, batch loss = 0.29 (34.9 examples/sec; 0.230 sec/batch; 19h:39m:50s remains)
INFO - root - 2017-12-15 07:35:03.191306: step 24070, loss = 0.17, batch loss = 0.14 (34.8 examples/sec; 0.230 sec/batch; 19h:40m:11s remains)
INFO - root - 2017-12-15 07:35:05.515504: step 24080, loss = 0.18, batch loss = 0.15 (35.8 examples/sec; 0.223 sec/batch; 19h:07m:36s remains)
INFO - root - 2017-12-15 07:35:07.831114: step 24090, loss = 0.27, batch loss = 0.24 (33.9 examples/sec; 0.236 sec/batch; 20h:12m:33s remains)
INFO - root - 2017-12-15 07:35:10.115880: step 24100, loss = 0.31, batch loss = 0.28 (35.3 examples/sec; 0.227 sec/batch; 19h:26m:21s remains)
2017-12-15 07:35:10.470768: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2173982 -4.3128929 -4.356926 -4.4178076 -4.6100979 -4.697751 -4.3687239 -3.8349729 -3.3183494 -3.2416842 -3.9036789 -4.9348154 -5.6496177 -5.5256205 -4.553524][-3.7379098 -4.4574118 -4.49667 -4.4468722 -4.357708 -4.2519064 -3.7598763 -3.1688557 -2.6295292 -2.460587 -3.0403881 -4.1464143 -5.0733 -5.1637349 -4.2819338][-3.6063521 -4.41994 -4.7500429 -4.6165113 -4.17718 -3.7003117 -2.8927541 -2.1101379 -1.4807544 -1.2260373 -1.7118313 -2.9233143 -4.0942235 -4.5725908 -4.02659][-3.2699339 -3.9624038 -4.508193 -4.2353811 -3.452024 -2.6581066 -1.5666914 -0.54288721 0.15687633 0.51766276 0.24851108 -0.94174957 -2.3530319 -3.3385046 -3.3015847][-3.000247 -3.4317708 -4.0853891 -3.6689377 -2.6274452 -1.6515694 -0.47490692 0.57126212 1.2257719 1.6572194 1.5954299 0.557364 -0.85839546 -2.1526651 -2.6167231][-3.0594203 -3.194808 -3.743679 -3.2846515 -2.193949 -1.1333337 0.0037271976 0.94233346 1.5831752 2.113441 2.2286038 1.5033512 0.36617422 -0.94788647 -1.790906][-3.2182813 -3.4459834 -3.9313779 -3.5278697 -2.4943378 -1.4297853 -0.35095274 0.47005725 1.0794835 1.6252227 1.8371668 1.4139237 0.64273024 -0.48606253 -1.3939841][-3.676204 -3.8242621 -4.3410578 -4.1257176 -3.2721686 -2.2445922 -1.20924 -0.46196496 0.18419242 0.74147058 1.0413237 0.89008236 0.39053774 -0.58120871 -1.5979205][-4.124362 -4.1821022 -4.6755686 -4.6176176 -4.0162196 -3.0820568 -2.113807 -1.4575034 -0.88228905 -0.45101559 -0.14202571 -0.0706408 -0.26605487 -0.92562628 -1.8574398][-4.4491367 -4.4322014 -4.8642979 -4.9067674 -4.5720606 -3.8718019 -3.1209772 -2.6071825 -2.1424184 -1.8651196 -1.5909812 -1.4039025 -1.4203814 -1.8850495 -2.6516922][-4.9863582 -4.7579594 -5.0872421 -5.1184545 -4.9854178 -4.5781674 -4.1058178 -3.7335474 -3.4295387 -3.2937708 -3.0391111 -2.7556484 -2.651047 -2.8747964 -3.3872166][-5.4786034 -4.9911079 -5.2025084 -5.1837835 -5.0943632 -4.7893 -4.4550018 -4.2226877 -4.0349607 -3.940958 -3.7447028 -3.5688877 -3.5519147 -3.6363473 -3.9487534][-5.576129 -4.8682275 -4.985743 -4.9085355 -4.8087 -4.5660782 -4.305089 -4.1694055 -4.1081548 -4.0650239 -3.9236288 -3.7996693 -3.804626 -3.8864093 -4.1625304][-5.2303586 -4.5282822 -4.6241503 -4.5112915 -4.2966518 -3.9426248 -3.6280155 -3.5407195 -3.5642595 -3.590476 -3.5987177 -3.6187668 -3.6597061 -3.7203255 -3.9759445][-4.4183722 -3.8930974 -4.0139084 -3.876142 -3.5245166 -2.9952269 -2.5463278 -2.4115875 -2.5053425 -2.70149 -2.9317982 -3.1150289 -3.2436435 -3.4049497 -3.7376211]]...]
INFO - root - 2017-12-15 07:35:12.789488: step 24110, loss = 0.23, batch loss = 0.20 (34.7 examples/sec; 0.231 sec/batch; 19h:46m:33s remains)
INFO - root - 2017-12-15 07:35:15.135270: step 24120, loss = 0.19, batch loss = 0.16 (35.2 examples/sec; 0.227 sec/batch; 19h:27m:59s remains)
INFO - root - 2017-12-15 07:35:17.460695: step 24130, loss = 0.27, batch loss = 0.23 (33.6 examples/sec; 0.238 sec/batch; 20h:25m:20s remains)
INFO - root - 2017-12-15 07:35:19.780941: step 24140, loss = 0.22, batch loss = 0.19 (36.0 examples/sec; 0.222 sec/batch; 19h:02m:29s remains)
INFO - root - 2017-12-15 07:35:22.097970: step 24150, loss = 0.19, batch loss = 0.16 (35.2 examples/sec; 0.228 sec/batch; 19h:29m:26s remains)
INFO - root - 2017-12-15 07:35:24.432556: step 24160, loss = 0.21, batch loss = 0.18 (33.5 examples/sec; 0.239 sec/batch; 20h:27m:26s remains)
INFO - root - 2017-12-15 07:35:26.784689: step 24170, loss = 0.28, batch loss = 0.24 (33.4 examples/sec; 0.239 sec/batch; 20h:30m:39s remains)
INFO - root - 2017-12-15 07:35:29.077831: step 24180, loss = 0.18, batch loss = 0.15 (34.1 examples/sec; 0.235 sec/batch; 20h:05m:31s remains)
INFO - root - 2017-12-15 07:35:31.361912: step 24190, loss = 0.18, batch loss = 0.14 (34.1 examples/sec; 0.234 sec/batch; 20h:04m:11s remains)
INFO - root - 2017-12-15 07:35:33.677405: step 24200, loss = 0.29, batch loss = 0.26 (35.1 examples/sec; 0.228 sec/batch; 19h:31m:30s remains)
2017-12-15 07:35:34.008679: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.0914335 -7.7570305 -7.0079031 -6.15234 -5.4855905 -5.549181 -6.0310144 -6.1579342 -6.2804079 -6.276289 -5.8949442 -4.9302225 -4.0383344 -4.3573141 -4.5439625][-9.1489563 -8.2806435 -7.2994595 -6.3621683 -5.6034317 -5.3718996 -5.5803194 -5.606432 -5.9729295 -6.4620447 -6.4706278 -5.7170944 -4.6551132 -4.8419142 -4.873045][-10.260253 -8.4543991 -7.3680286 -6.3442745 -5.3883371 -4.9109154 -4.9389029 -4.7703824 -5.1875553 -6.0219975 -6.5996342 -6.3777695 -5.3227286 -5.0894423 -4.8839388][-10.102044 -7.5378036 -6.616333 -5.65924 -4.462461 -3.7280858 -3.5346308 -3.0841494 -3.4310131 -4.4666395 -5.7249269 -6.3071461 -5.532073 -5.045866 -4.683413][-8.7482586 -5.7449288 -5.2372608 -4.6013527 -3.1509361 -1.9209589 -1.089272 -0.36373162 -0.90313327 -2.2292876 -4.127449 -5.4196339 -5.0014944 -4.4439144 -4.1556869][-6.7689524 -4.0828547 -3.9262 -3.5381415 -1.8191657 0.010227203 1.7164786 2.7600195 1.703733 -0.31142664 -2.9077449 -4.7144504 -4.6527157 -4.0615411 -3.8250952][-4.9075918 -2.7102654 -2.6406119 -2.2633991 -0.18726015 2.2753446 4.8440905 6.2589884 4.4590178 1.4197595 -1.8599596 -4.0234575 -4.3333483 -3.8701763 -3.6381466][-4.5624447 -2.5272799 -2.5406871 -2.0741165 0.28598595 3.2433364 6.4553776 8.0051908 5.4834805 1.604341 -1.5717943 -3.6210577 -4.0328159 -3.9368711 -3.7904963][-5.2593908 -3.6247544 -3.6697378 -3.0310724 -0.76475263 2.211189 5.4043446 6.6739893 3.8365085 -0.40510225 -2.7753062 -3.962534 -4.2246904 -4.2076111 -3.9643281][-6.3697081 -4.8026953 -4.7722216 -4.01845 -2.1649237 0.36489224 2.7653892 3.4425271 0.71122575 -3.3316605 -4.7100105 -5.1181607 -5.2041063 -4.8451662 -3.9900808][-7.5479422 -5.7536612 -5.7169194 -5.0883045 -3.5944653 -1.5326018 -0.21844625 -0.2524699 -2.3418117 -5.4965191 -6.0641375 -6.1509209 -6.23905 -5.5785093 -4.3144779][-8.4388485 -6.4108171 -6.3819618 -5.9338007 -4.7937593 -3.327569 -2.9968929 -3.4310083 -4.6415949 -6.6140089 -6.8143625 -7.0220108 -7.2678022 -6.5274038 -5.2363663][-9.1355963 -7.2843189 -7.4972029 -7.3245955 -6.5900021 -5.7853308 -6.0877457 -6.641695 -7.3771591 -8.5504684 -8.785346 -8.8440351 -8.722784 -7.7141533 -6.5221186][-9.8073282 -8.119051 -8.297431 -8.1093264 -7.5987768 -7.2431021 -7.7485466 -8.3141947 -8.8648281 -9.5571108 -9.7688684 -9.558404 -8.9248772 -7.8024445 -6.9971523][-9.0463257 -7.355649 -7.2596855 -7.0514956 -6.8705997 -7.0181055 -7.5617676 -7.895299 -8.1094246 -8.3775311 -8.4602585 -8.137866 -7.4665327 -6.7218323 -6.4915791]]...]
INFO - root - 2017-12-15 07:35:36.333319: step 24210, loss = 0.20, batch loss = 0.17 (35.2 examples/sec; 0.228 sec/batch; 19h:29m:02s remains)
INFO - root - 2017-12-15 07:35:38.663001: step 24220, loss = 0.21, batch loss = 0.18 (35.6 examples/sec; 0.224 sec/batch; 19h:13m:02s remains)
INFO - root - 2017-12-15 07:35:40.986044: step 24230, loss = 0.22, batch loss = 0.19 (32.7 examples/sec; 0.245 sec/batch; 20h:57m:01s remains)
INFO - root - 2017-12-15 07:35:43.275855: step 24240, loss = 0.40, batch loss = 0.36 (35.5 examples/sec; 0.225 sec/batch; 19h:17m:06s remains)
INFO - root - 2017-12-15 07:35:45.591031: step 24250, loss = 0.22, batch loss = 0.18 (34.9 examples/sec; 0.229 sec/batch; 19h:38m:05s remains)
INFO - root - 2017-12-15 07:35:47.913879: step 24260, loss = 0.32, batch loss = 0.29 (34.6 examples/sec; 0.231 sec/batch; 19h:47m:34s remains)
INFO - root - 2017-12-15 07:35:50.236220: step 24270, loss = 0.25, batch loss = 0.22 (35.7 examples/sec; 0.224 sec/batch; 19h:11m:16s remains)
INFO - root - 2017-12-15 07:35:52.528013: step 24280, loss = 0.26, batch loss = 0.23 (33.9 examples/sec; 0.236 sec/batch; 20h:11m:26s remains)
INFO - root - 2017-12-15 07:35:54.839132: step 24290, loss = 0.20, batch loss = 0.17 (34.9 examples/sec; 0.229 sec/batch; 19h:37m:26s remains)
INFO - root - 2017-12-15 07:35:57.108274: step 24300, loss = 0.20, batch loss = 0.17 (35.1 examples/sec; 0.228 sec/batch; 19h:31m:30s remains)
2017-12-15 07:35:57.399965: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.423152 -5.9668331 -4.4344993 -3.1611288 -1.1997192 0.0798738 0.014890671 -1.5558181 -3.6013155 -4.985507 -5.8228922 -5.3453655 -4.2776747 -3.2069502 -2.4197042][-5.6260409 -6.3901229 -4.6238275 -3.1622634 -1.2383018 -0.16336632 -0.39599967 -1.8373599 -3.5823345 -4.6843338 -5.1757278 -4.5323315 -3.6362629 -3.0618045 -2.6645191][-6.4818125 -6.5213718 -4.7164578 -3.267879 -1.4360608 -0.388736 -0.52036536 -1.6816993 -2.8984506 -3.5948415 -3.7632809 -2.9770305 -2.2942996 -2.2448192 -2.2230036][-7.1073146 -6.4061904 -4.6119404 -3.269383 -1.5930371 -0.59984338 -0.56325233 -1.4407094 -2.2000515 -2.4765029 -2.3370221 -1.3985167 -0.94280457 -1.2643516 -1.4510285][-7.5114737 -6.0976009 -4.3380814 -3.0688944 -1.499687 -0.46700156 -0.11161876 -0.6302247 -1.1823295 -1.2709939 -1.2249358 -0.275589 0.038443804 -0.27524626 -0.45309317][-7.4480948 -5.6673775 -3.8744607 -2.6049461 -1.0262102 0.11225629 0.94011116 0.81704879 0.32870245 0.086128235 -0.45355535 0.24681735 0.59122491 0.46830988 0.46722794][-6.89245 -5.3542495 -3.4681907 -2.1509526 -0.50586236 0.81985044 2.105547 2.3308058 1.8223314 1.3141699 -0.052026987 0.23666501 0.48234344 0.54943848 0.75264][-7.0972576 -5.5151958 -3.5720325 -2.1894553 -0.43800509 1.0582438 2.6925073 3.1834617 2.6358285 1.900475 -0.20348954 -0.36684418 -0.37923861 -0.20572495 0.18640065][-7.6526542 -6.1577721 -4.219945 -2.8811953 -1.0911489 0.54742527 2.370986 2.966146 2.5554986 1.6627812 -0.95307469 -1.5274594 -1.8962793 -1.7879672 -1.3072339][-8.2441425 -7.1203556 -5.5347385 -4.4276876 -2.742115 -0.97655582 0.9454267 1.6384277 1.5354292 0.64364982 -2.1330481 -2.9898405 -3.6243753 -3.7151167 -3.3563807][-8.5908051 -7.9897552 -6.9687796 -6.2282982 -4.8493652 -3.200758 -1.4184403 -0.65337062 -0.51366258 -1.2018642 -3.557049 -4.362721 -5.0231853 -5.2189026 -5.0494471][-8.512392 -8.3526459 -7.8806024 -7.5397139 -6.6543336 -5.4097552 -3.9281363 -3.0918591 -2.5713613 -2.8840413 -4.5021024 -5.1175795 -5.6407518 -5.9224048 -5.9748669][-8.0763969 -8.2254772 -8.2945576 -8.3628979 -7.9644346 -7.086175 -5.8711939 -4.9699812 -4.2563171 -4.3711925 -5.3995433 -5.7958956 -6.06851 -6.2690039 -6.3491755][-7.1672511 -7.3870025 -7.7171454 -7.9313326 -7.7966704 -7.2216659 -6.3767934 -5.7427063 -5.3192244 -5.4517317 -5.9821873 -6.0892959 -6.0402184 -5.9910803 -5.9521446][-5.77872 -5.7656264 -6.081809 -6.3468928 -6.4347963 -6.1480856 -5.6828852 -5.3899174 -5.2578716 -5.3673286 -5.529264 -5.4956064 -5.3338928 -5.1734209 -5.0622673]]...]
INFO - root - 2017-12-15 07:35:59.710048: step 24310, loss = 0.24, batch loss = 0.20 (34.8 examples/sec; 0.230 sec/batch; 19h:40m:46s remains)
INFO - root - 2017-12-15 07:36:02.040068: step 24320, loss = 0.28, batch loss = 0.25 (34.2 examples/sec; 0.234 sec/batch; 20h:03m:07s remains)
INFO - root - 2017-12-15 07:36:04.350965: step 24330, loss = 0.25, batch loss = 0.21 (34.6 examples/sec; 0.231 sec/batch; 19h:48m:59s remains)
INFO - root - 2017-12-15 07:36:06.685424: step 24340, loss = 0.31, batch loss = 0.28 (35.7 examples/sec; 0.224 sec/batch; 19h:11m:06s remains)
INFO - root - 2017-12-15 07:36:09.010728: step 24350, loss = 0.22, batch loss = 0.19 (35.1 examples/sec; 0.228 sec/batch; 19h:28m:59s remains)
INFO - root - 2017-12-15 07:36:11.347004: step 24360, loss = 0.23, batch loss = 0.20 (35.0 examples/sec; 0.229 sec/batch; 19h:34m:33s remains)
INFO - root - 2017-12-15 07:36:13.647657: step 24370, loss = 0.19, batch loss = 0.15 (34.3 examples/sec; 0.233 sec/batch; 19h:58m:51s remains)
INFO - root - 2017-12-15 07:36:15.950779: step 24380, loss = 0.19, batch loss = 0.16 (36.4 examples/sec; 0.220 sec/batch; 18h:47m:27s remains)
INFO - root - 2017-12-15 07:36:18.221566: step 24390, loss = 0.38, batch loss = 0.34 (35.8 examples/sec; 0.224 sec/batch; 19h:07m:44s remains)
INFO - root - 2017-12-15 07:36:20.510206: step 24400, loss = 0.30, batch loss = 0.26 (35.4 examples/sec; 0.226 sec/batch; 19h:21m:05s remains)
2017-12-15 07:36:20.842866: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2116661 -6.0103159 -5.81391 -5.3915234 -5.0532703 -4.8650064 -3.8576589 -3.712101 -4.3881154 -4.891633 -5.2382693 -5.8273606 -6.4054213 -6.3532629 -5.8591809][-6.0694609 -6.4370346 -6.1596456 -5.7520432 -5.4168186 -5.2266011 -3.8576331 -3.8464622 -5.1066036 -5.7773924 -6.1984406 -6.8471947 -7.3632612 -7.1485472 -6.3361368][-7.0708294 -6.197299 -5.7766514 -5.3683767 -4.9431024 -4.5916662 -2.9349079 -3.0626428 -4.8677359 -5.7520828 -6.2891159 -7.0592051 -7.6409388 -7.2866793 -6.2839365][-8.0437651 -6.0276661 -5.3438425 -4.7746058 -4.0897083 -3.3740196 -1.5225168 -1.720629 -3.973067 -5.176157 -5.9669638 -6.9682055 -7.7340212 -7.3740931 -6.3078651][-8.0990152 -5.4529276 -4.7211905 -4.0518184 -3.0884449 -1.9972657 -0.058265686 -0.3752445 -2.9211857 -4.3858805 -5.3179851 -6.3195734 -7.0481606 -6.6419868 -5.5592837][-7.20265 -4.2710733 -3.1936545 -2.1274831 -0.73465967 0.76093054 2.7752602 2.3672545 -0.51088214 -2.4912543 -3.8747954 -5.1771717 -6.1491203 -5.83407 -4.7779617][-5.7414083 -2.9447286 -1.6465966 -0.36551738 1.2570596 3.1312811 5.3855209 4.9656067 1.9319522 -0.45043278 -2.2360075 -3.8150506 -5.0862951 -4.9800377 -3.9758639][-4.5838289 -2.0142481 -0.74131095 0.6139617 2.2679775 4.1677828 6.6154385 6.1912374 3.087188 0.48972893 -1.4823872 -3.1801214 -4.525012 -4.4242668 -3.399291][-4.1774254 -1.8454368 -0.64774 0.59621143 2.012243 3.6073973 6.089016 5.7408905 2.7989838 0.26723385 -1.63677 -3.3519063 -4.6310735 -4.378623 -3.2340839][-4.5876389 -2.4545679 -1.3700997 -0.3902154 0.52915597 1.5706363 3.7722642 3.5092652 0.96816444 -1.1455193 -2.7434287 -4.3055272 -5.348105 -4.8818011 -3.5691719][-5.7399397 -3.7293441 -2.7374072 -1.9310176 -1.3835959 -0.91465664 0.78140545 0.48115373 -1.5045483 -3.039458 -4.2125988 -5.5089121 -6.2280321 -5.572794 -4.3766751][-6.6530418 -4.8667068 -4.0787425 -3.440784 -3.0316074 -2.9094872 -1.6975338 -1.929116 -3.319176 -4.2336836 -4.98361 -5.9919252 -6.5082483 -5.9211211 -4.9966831][-7.103776 -5.4857297 -4.9264994 -4.4890308 -4.2892532 -4.487793 -3.737515 -3.8795285 -4.6551552 -4.9605389 -5.2832661 -5.986515 -6.4137268 -6.0968847 -5.527545][-7.2957535 -5.8427649 -5.5261374 -5.2663 -5.2218275 -5.4480686 -4.7806969 -4.6393909 -4.8569546 -4.8451376 -5.0493116 -5.6861362 -6.2101984 -6.159667 -5.8516941][-8.1158228 -6.553236 -6.1464472 -5.7751484 -5.6115494 -5.6124668 -4.899909 -4.5936766 -4.6107054 -4.6810822 -4.9976797 -5.6764321 -6.3759651 -6.6224308 -6.6091585]]...]
INFO - root - 2017-12-15 07:36:23.168098: step 24410, loss = 0.30, batch loss = 0.27 (33.3 examples/sec; 0.240 sec/batch; 20h:32m:45s remains)
INFO - root - 2017-12-15 07:36:25.431419: step 24420, loss = 0.27, batch loss = 0.24 (35.1 examples/sec; 0.228 sec/batch; 19h:30m:51s remains)
INFO - root - 2017-12-15 07:36:27.762688: step 24430, loss = 0.56, batch loss = 0.53 (32.9 examples/sec; 0.243 sec/batch; 20h:47m:00s remains)
INFO - root - 2017-12-15 07:36:30.028875: step 24440, loss = 0.17, batch loss = 0.14 (35.7 examples/sec; 0.224 sec/batch; 19h:10m:08s remains)
INFO - root - 2017-12-15 07:36:32.281469: step 24450, loss = 0.18, batch loss = 0.14 (34.5 examples/sec; 0.232 sec/batch; 19h:51m:15s remains)
INFO - root - 2017-12-15 07:36:34.577194: step 24460, loss = 0.20, batch loss = 0.17 (34.9 examples/sec; 0.229 sec/batch; 19h:37m:12s remains)
INFO - root - 2017-12-15 07:36:36.898548: step 24470, loss = 0.24, batch loss = 0.21 (35.0 examples/sec; 0.229 sec/batch; 19h:33m:57s remains)
INFO - root - 2017-12-15 07:36:39.152512: step 24480, loss = 0.25, batch loss = 0.22 (35.2 examples/sec; 0.227 sec/batch; 19h:25m:45s remains)
INFO - root - 2017-12-15 07:36:41.406841: step 24490, loss = 0.22, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 19h:02m:24s remains)
INFO - root - 2017-12-15 07:36:43.678732: step 24500, loss = 0.26, batch loss = 0.23 (34.7 examples/sec; 0.231 sec/batch; 19h:44m:35s remains)
2017-12-15 07:36:43.983357: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7708948 -5.223299 -5.5576525 -5.51526 -5.4593983 -5.2351084 -4.9642973 -4.8554764 -4.9221392 -5.2218113 -5.6049933 -5.9212155 -6.1750431 -6.250617 -6.1802711][-3.2946894 -6.2377319 -6.8622465 -6.8923965 -6.8215985 -6.43143 -6.06546 -5.9651022 -6.0413465 -6.2762728 -6.5581055 -6.9018106 -7.2207189 -7.3617978 -7.4797153][-4.8939705 -6.8630886 -7.8054595 -7.8700023 -7.6891184 -7.0200415 -6.4228783 -6.1779208 -6.1602764 -6.1646137 -6.2915859 -6.6777577 -7.0774875 -7.3041778 -7.66855][-6.1698828 -7.4417219 -8.6170912 -8.5964432 -8.1433325 -7.0291133 -5.9468803 -5.3227539 -5.0617447 -4.7951689 -4.9055357 -5.5160184 -6.1601205 -6.6355152 -7.20125][-7.1770144 -7.7687674 -8.9677391 -8.6280193 -7.7022114 -6.0063453 -4.2683105 -3.1159625 -2.5700486 -2.2253625 -2.6174836 -3.6842277 -4.683826 -5.26575 -5.7200689][-7.0694485 -7.5531263 -8.50637 -7.7180452 -6.4081807 -4.3086987 -1.9915621 -0.31003702 0.46615553 0.6025877 -0.2677362 -1.6781359 -2.9212749 -3.383687 -3.6746006][-6.1522617 -7.1854682 -7.9037471 -6.8779378 -5.4217682 -3.004076 -0.1494844 2.0405025 3.0699573 3.0244088 1.7061634 0.085575819 -1.4071106 -1.8686074 -2.1270022][-6.0939894 -6.9840245 -7.6454048 -6.663259 -5.1144295 -2.3872082 0.82416391 3.4058628 4.6660018 4.5104127 2.8893676 1.1786714 -0.47584677 -0.83216929 -1.0100644][-5.9681687 -6.7958584 -7.5212765 -6.8428321 -5.3697319 -2.6139438 0.511945 3.0623312 4.3036213 4.0188208 2.2271361 0.64445662 -0.90312195 -1.0824353 -1.141112][-5.8956552 -6.5623555 -7.4428406 -7.2958131 -6.2210379 -3.8900251 -1.3061894 0.86814809 2.0257077 1.8451223 0.15816617 -1.1290317 -2.2848539 -2.295939 -2.3222072][-5.8490429 -6.28269 -7.2900677 -7.6638293 -7.176002 -5.6778059 -4.0515804 -2.6459086 -1.6942551 -1.5042102 -2.68441 -3.582705 -4.2530527 -4.1057625 -4.0781841][-5.822474 -6.0116091 -6.9833312 -7.64896 -7.754828 -7.1734266 -6.5124259 -5.9247341 -5.3057861 -4.9370546 -5.5222678 -6.1579475 -6.5863304 -6.3977075 -6.3849869][-5.7436409 -5.6763754 -6.4490347 -7.1351161 -7.5994844 -7.6546259 -7.5752397 -7.4808207 -7.1982045 -6.8767853 -7.1097288 -7.4569588 -7.7062159 -7.698885 -7.6890121][-5.607883 -5.2630272 -5.7412024 -6.228055 -6.6797466 -6.9153509 -7.0065479 -7.1168275 -7.0751619 -6.9259758 -7.0714626 -7.35567 -7.5253015 -7.5066977 -7.4126787][-5.5536094 -5.0238152 -5.269763 -5.527873 -5.7724118 -5.8781676 -5.8621426 -5.8556576 -5.8118935 -5.7689967 -5.9299498 -6.2308388 -6.4786267 -6.5321622 -6.4172969]]...]
INFO - root - 2017-12-15 07:36:46.294140: step 24510, loss = 0.18, batch loss = 0.15 (34.2 examples/sec; 0.234 sec/batch; 20h:02m:00s remains)
INFO - root - 2017-12-15 07:36:48.556687: step 24520, loss = 0.28, batch loss = 0.25 (35.3 examples/sec; 0.226 sec/batch; 19h:22m:08s remains)
INFO - root - 2017-12-15 07:36:50.852453: step 24530, loss = 0.49, batch loss = 0.45 (33.3 examples/sec; 0.240 sec/batch; 20h:33m:59s remains)
INFO - root - 2017-12-15 07:36:53.111431: step 24540, loss = 0.18, batch loss = 0.15 (35.7 examples/sec; 0.224 sec/batch; 19h:09m:07s remains)
INFO - root - 2017-12-15 07:36:55.438303: step 24550, loss = 0.26, batch loss = 0.23 (34.9 examples/sec; 0.229 sec/batch; 19h:37m:08s remains)
INFO - root - 2017-12-15 07:36:57.740759: step 24560, loss = 0.16, batch loss = 0.13 (33.3 examples/sec; 0.240 sec/batch; 20h:33m:48s remains)
INFO - root - 2017-12-15 07:37:00.058109: step 24570, loss = 0.19, batch loss = 0.16 (34.9 examples/sec; 0.229 sec/batch; 19h:35m:29s remains)
INFO - root - 2017-12-15 07:37:02.348200: step 24580, loss = 0.28, batch loss = 0.25 (35.0 examples/sec; 0.229 sec/batch; 19h:33m:53s remains)
INFO - root - 2017-12-15 07:37:04.606984: step 24590, loss = 0.22, batch loss = 0.19 (35.8 examples/sec; 0.224 sec/batch; 19h:07m:40s remains)
INFO - root - 2017-12-15 07:37:06.885772: step 24600, loss = 0.31, batch loss = 0.28 (36.0 examples/sec; 0.222 sec/batch; 18h:59m:44s remains)
2017-12-15 07:37:07.354440: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0968571 -6.6317759 -7.4243579 -7.6087379 -7.7184424 -7.6915994 -7.6129375 -7.6124291 -7.5202861 -7.5311637 -7.5031228 -7.0775905 -6.68612 -6.5182786 -6.6176448][-5.8803062 -7.77638 -8.8580484 -8.95297 -9.024828 -8.9024315 -8.5538521 -8.37969 -8.4014864 -8.6924362 -8.8111992 -8.2454157 -7.7948475 -7.47466 -7.2945428][-7.023993 -8.1852779 -9.3621988 -9.1177149 -8.9011593 -8.3533363 -7.400938 -6.7857308 -6.8491044 -7.44845 -7.8794346 -7.4412217 -7.2420168 -6.9962654 -6.6539068][-7.1797209 -8.1283646 -9.1185856 -8.4488983 -7.9472046 -6.986083 -5.4983559 -4.6218033 -4.9846745 -6.0389705 -6.988451 -6.8347125 -6.7194967 -6.390348 -5.88888][-7.2844629 -7.6316962 -8.18781 -7.0500965 -6.1882353 -4.7658319 -2.8595152 -1.9308178 -2.7321746 -4.3758316 -5.94276 -6.1062636 -5.9990988 -5.6209393 -5.1576738][-7.2181935 -6.9500308 -6.9067612 -5.1823435 -3.6693811 -1.5899162 0.67140341 1.6206892 0.57452655 -1.4372784 -3.43888 -3.8138409 -3.5291991 -3.1399775 -2.9523234][-6.7138419 -6.5220709 -5.977386 -3.8957868 -1.8428743 0.79737353 3.3374195 4.3455133 3.1072412 0.80465436 -1.547405 -1.9500268 -1.2138231 -0.831347 -1.0138339][-6.78594 -6.2120705 -5.2958374 -3.0852036 -0.76916063 2.0875149 4.6390395 5.5856729 4.1783886 1.5737624 -0.754509 -0.82516825 0.28941035 0.48902631 -0.11008835][-7.0319104 -6.4226112 -5.48666 -3.4425406 -1.1094409 1.6915576 3.9934974 4.9405012 3.7669573 1.4875715 -0.25388372 0.19341159 1.6469529 1.837317 1.0497892][-7.5039449 -7.1900125 -6.6200662 -5.1491613 -3.2610946 -0.87450945 1.0847485 2.1287584 1.4546158 -0.082033396 -1.1446809 -0.13391733 1.7493329 2.1954136 1.4904735][-7.7388306 -7.8283691 -7.8141236 -6.9775028 -5.61464 -3.785718 -2.2585227 -1.20712 -1.4330597 -2.4136539 -2.9105492 -1.4540493 0.52675247 1.1942446 0.52659559][-7.7042894 -8.0387468 -8.4067268 -7.9905233 -7.1301451 -5.7835188 -4.5309505 -3.4345088 -3.2500415 -3.8608763 -4.2136564 -2.9414444 -1.3605545 -0.66667438 -1.2162366][-7.6834912 -8.15453 -8.8044624 -8.7569046 -8.3243065 -7.3249092 -6.1508093 -5.0343218 -4.6530018 -5.0584574 -5.48857 -4.5746093 -3.4547544 -2.7617462 -3.1393857][-7.5034928 -7.9814668 -8.8610268 -9.1856279 -9.1734123 -8.577651 -7.5184951 -6.3621726 -5.8265953 -6.168745 -6.6987185 -6.2111025 -5.4541283 -4.9179974 -5.1203709][-6.9461255 -7.2079406 -8.0677547 -8.5901012 -8.90678 -8.6233654 -7.6966667 -6.6043348 -6.0453482 -6.3980331 -7.1563921 -7.2402873 -6.9849372 -6.64733 -6.645998]]...]
INFO - root - 2017-12-15 07:37:09.677052: step 24610, loss = 0.19, batch loss = 0.16 (34.9 examples/sec; 0.229 sec/batch; 19h:36m:22s remains)
INFO - root - 2017-12-15 07:37:11.975326: step 24620, loss = 0.23, batch loss = 0.20 (34.7 examples/sec; 0.231 sec/batch; 19h:43m:03s remains)
INFO - root - 2017-12-15 07:37:14.261280: step 24630, loss = 0.21, batch loss = 0.18 (33.8 examples/sec; 0.236 sec/batch; 20h:13m:24s remains)
INFO - root - 2017-12-15 07:37:16.552579: step 24640, loss = 0.29, batch loss = 0.26 (34.6 examples/sec; 0.231 sec/batch; 19h:46m:32s remains)
INFO - root - 2017-12-15 07:37:18.818896: step 24650, loss = 0.25, batch loss = 0.21 (36.2 examples/sec; 0.221 sec/batch; 18h:52m:46s remains)
INFO - root - 2017-12-15 07:37:21.114053: step 24660, loss = 0.19, batch loss = 0.16 (33.0 examples/sec; 0.242 sec/batch; 20h:43m:34s remains)
INFO - root - 2017-12-15 07:37:23.435967: step 24670, loss = 0.25, batch loss = 0.22 (34.9 examples/sec; 0.229 sec/batch; 19h:35m:54s remains)
INFO - root - 2017-12-15 07:37:25.696161: step 24680, loss = 0.25, batch loss = 0.22 (36.2 examples/sec; 0.221 sec/batch; 18h:53m:22s remains)
INFO - root - 2017-12-15 07:37:27.934380: step 24690, loss = 0.20, batch loss = 0.16 (35.4 examples/sec; 0.226 sec/batch; 19h:18m:13s remains)
INFO - root - 2017-12-15 07:37:30.215231: step 24700, loss = 0.25, batch loss = 0.22 (34.8 examples/sec; 0.230 sec/batch; 19h:40m:09s remains)
2017-12-15 07:37:30.549615: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8017902 -6.9037509 -7.3063498 -6.7236876 -5.4745474 -3.88547 -2.8174434 -2.7505105 -3.5761695 -4.3338575 -4.0509462 -2.6659985 -1.2835033 -0.75324178 -0.54437125][-5.5250049 -7.7734752 -7.9986172 -7.2243652 -5.9619102 -4.45971 -3.2029014 -2.6347594 -3.0946026 -3.7824917 -3.7062216 -2.8668962 -1.9671592 -1.6324444 -1.365167][-6.8316855 -8.1331749 -8.1858521 -7.1729846 -5.8669586 -4.2528009 -2.7928741 -1.8923618 -2.1955388 -3.1558352 -3.4500823 -3.2504282 -2.92762 -2.8223138 -2.5249236][-7.5323625 -7.9188657 -7.8683763 -6.5763936 -5.0512152 -3.1523607 -1.3935935 -0.0945878 -0.38342261 -1.9498453 -2.8914657 -3.4519887 -3.7741537 -3.8624988 -3.5382276][-7.4398785 -6.95521 -6.8993411 -5.4262047 -3.6302216 -1.2938519 0.91313481 2.7352848 2.366715 -0.065769434 -1.7491214 -2.9203863 -3.7715263 -4.2049561 -3.9670489][-6.7736969 -5.7341948 -5.701931 -4.0730095 -1.9996363 0.73516774 3.4802704 5.7639532 5.1299114 1.7853436 -0.59204733 -2.1555784 -3.1415606 -3.7238288 -3.6414723][-6.1324391 -4.9875169 -4.9633641 -3.2690487 -0.94633627 2.0549841 5.2487669 7.9672861 7.1616411 3.1424527 0.19053006 -1.605566 -2.5727768 -3.1704819 -3.2049561][-6.2548637 -4.8964386 -4.9120059 -3.2654662 -0.89023316 2.1043773 5.5224986 8.5656672 7.6951685 3.4054828 0.24417448 -1.5296639 -2.3015344 -2.8752522 -3.1149542][-6.8676982 -5.6172404 -5.6769428 -4.1224861 -1.9327905 0.66841912 3.8966908 6.7736006 5.8765221 1.8829184 -0.95313656 -2.3554111 -2.7276406 -3.225023 -3.673732][-7.4433732 -6.4560337 -6.5620966 -5.278614 -3.5378132 -1.5829272 1.0821795 3.3944445 2.4864779 -0.77336895 -3.0094562 -3.9297347 -3.9647665 -4.3308353 -4.77962][-7.8928304 -7.2015648 -7.3532853 -6.5222969 -5.3691621 -4.0375981 -2.094317 -0.43123639 -1.2488035 -3.5841608 -5.101234 -5.5529337 -5.3468504 -5.5390468 -5.78561][-7.99368 -7.4650578 -7.5893459 -7.0792222 -6.3835831 -5.56266 -4.2910748 -3.3280547 -4.0449739 -5.490191 -6.3172221 -6.4343538 -6.12574 -6.1766171 -6.2224116][-7.3736167 -6.815105 -6.8745346 -6.5856891 -6.1757269 -5.7205667 -5.0617876 -4.7113485 -5.2475777 -5.9365182 -6.2974005 -6.2832913 -6.0737658 -6.0511818 -5.9534264][-6.4752121 -5.851428 -5.9282284 -5.8593225 -5.6878867 -5.4823475 -5.2038088 -5.0773559 -5.2715254 -5.4743204 -5.6244378 -5.6649065 -5.62342 -5.5803766 -5.4385357][-5.8817015 -5.2216825 -5.32069 -5.3228397 -5.2423811 -5.1399212 -5.0387545 -5.0125761 -5.051774 -5.090436 -5.1566553 -5.2235255 -5.2274151 -5.1606464 -5.04372]]...]
INFO - root - 2017-12-15 07:37:32.803177: step 24710, loss = 0.24, batch loss = 0.21 (35.9 examples/sec; 0.223 sec/batch; 19h:04m:30s remains)
INFO - root - 2017-12-15 07:37:35.058271: step 24720, loss = 0.20, batch loss = 0.17 (35.6 examples/sec; 0.225 sec/batch; 19h:12m:38s remains)
INFO - root - 2017-12-15 07:37:37.312298: step 24730, loss = 0.23, batch loss = 0.20 (34.2 examples/sec; 0.234 sec/batch; 20h:01m:18s remains)
INFO - root - 2017-12-15 07:37:39.622319: step 24740, loss = 0.22, batch loss = 0.19 (35.5 examples/sec; 0.225 sec/batch; 19h:16m:09s remains)
INFO - root - 2017-12-15 07:37:41.861304: step 24750, loss = 0.20, batch loss = 0.17 (34.9 examples/sec; 0.229 sec/batch; 19h:35m:30s remains)
INFO - root - 2017-12-15 07:37:44.145311: step 24760, loss = 0.32, batch loss = 0.29 (36.3 examples/sec; 0.220 sec/batch; 18h:50m:15s remains)
INFO - root - 2017-12-15 07:37:46.426392: step 24770, loss = 0.30, batch loss = 0.27 (35.6 examples/sec; 0.225 sec/batch; 19h:12m:43s remains)
INFO - root - 2017-12-15 07:37:48.673954: step 24780, loss = 0.32, batch loss = 0.29 (35.6 examples/sec; 0.225 sec/batch; 19h:12m:59s remains)
INFO - root - 2017-12-15 07:37:50.940450: step 24790, loss = 0.19, batch loss = 0.16 (36.4 examples/sec; 0.219 sec/batch; 18h:45m:39s remains)
INFO - root - 2017-12-15 07:37:53.216036: step 24800, loss = 0.25, batch loss = 0.22 (36.2 examples/sec; 0.221 sec/batch; 18h:52m:43s remains)
2017-12-15 07:37:53.542305: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.7374821 0.08056879 -1.1502388 -1.8433435 -2.2294075 -2.737036 -3.1327929 -3.0454369 -3.120842 -3.3087511 -3.5136456 -3.5170417 -3.8281889 -3.798142 -3.5712161][0.15065479 -1.6469991 -2.7046974 -3.2332196 -3.2347956 -3.1625717 -2.7684777 -2.1329818 -2.3053231 -2.7720447 -3.1878574 -3.5271502 -3.9041867 -3.9976325 -3.9545774][-1.9118363 -3.4166498 -4.3759089 -4.5840292 -4.0360947 -3.161706 -1.9412942 -0.69384241 -0.98275197 -1.8180408 -2.4502957 -3.0239468 -3.472466 -3.6388438 -3.8151908][-3.8206668 -4.5230255 -5.2300253 -5.1201048 -4.1187305 -2.7575758 -1.1588714 0.53878736 -0.014505863 -1.2481506 -2.141983 -2.9409981 -3.3840256 -3.5283837 -3.7078719][-4.4221926 -4.4813089 -5.0545554 -4.7886562 -3.6200166 -2.1267979 -0.37442327 1.4416318 0.47983503 -1.2410113 -2.4711182 -3.4344916 -3.8502891 -3.8864024 -3.8035803][-3.8748293 -3.7256587 -4.3392706 -3.9931674 -2.5807328 -0.79537606 0.94013929 2.4057953 0.724674 -1.5809455 -2.9731967 -3.8401055 -4.1734104 -4.1525822 -3.8727167][-2.8210373 -2.6714044 -3.0351684 -2.3706787 -0.53201008 1.5535843 3.0481398 3.901428 1.5814521 -1.1406174 -2.609879 -3.496619 -3.9649687 -4.1203184 -3.8967152][-3.1366334 -2.6806192 -2.5947266 -1.5599082 0.6261611 2.8703358 4.1246157 4.3073044 1.5893581 -0.988091 -2.1892822 -2.9321108 -3.5745258 -3.9378185 -4.00124][-3.8621016 -3.3490424 -3.2253349 -2.2010806 -0.03430438 2.1140716 3.3422759 3.3325822 1.0301096 -0.82893467 -1.7033573 -2.459347 -3.212692 -3.5146246 -3.877233][-4.2153883 -4.1004448 -4.0718417 -3.1588173 -1.2737839 0.60941386 1.838824 1.875056 0.28653765 -0.73613906 -1.4505708 -2.3113782 -3.2342505 -3.4947119 -4.251195][-4.2984438 -4.8107605 -5.0788765 -4.47746 -3.1099944 -1.6457918 -0.49439251 -0.35004961 -1.1242754 -1.3574165 -1.8196409 -2.7669127 -3.867847 -4.2648134 -5.2687449][-4.4878778 -5.5716848 -6.3673306 -6.30534 -5.5565577 -4.4106765 -3.2988863 -3.0177875 -3.1967576 -2.8284383 -2.9996018 -3.8185122 -4.9764891 -5.4182334 -6.2723761][-4.3975048 -5.6007695 -6.76894 -7.1446581 -6.8558397 -5.9903913 -5.1891375 -5.151185 -5.1767063 -4.6338816 -4.6277256 -5.2099996 -6.0541658 -6.2837639 -6.6789083][-4.7447529 -5.7853255 -7.0151081 -7.5617037 -7.5371828 -6.8510485 -6.4463358 -6.7179565 -6.8629236 -6.3518267 -6.167738 -6.3236904 -6.7625246 -6.795887 -6.8521271][-5.9205303 -6.6014776 -7.7013359 -8.193964 -8.2250128 -7.5589123 -7.2834768 -7.5328903 -7.5307913 -6.9578028 -6.5683985 -6.5424843 -6.7599173 -6.6532288 -6.4023609]]...]
INFO - root - 2017-12-15 07:37:55.874925: step 24810, loss = 0.24, batch loss = 0.21 (33.9 examples/sec; 0.236 sec/batch; 20h:08m:46s remains)
INFO - root - 2017-12-15 07:37:58.108684: step 24820, loss = 0.20, batch loss = 0.17 (35.5 examples/sec; 0.225 sec/batch; 19h:15m:57s remains)
INFO - root - 2017-12-15 07:38:00.389555: step 24830, loss = 0.23, batch loss = 0.20 (34.5 examples/sec; 0.232 sec/batch; 19h:47m:53s remains)
INFO - root - 2017-12-15 07:38:02.670545: step 24840, loss = 0.17, batch loss = 0.13 (34.4 examples/sec; 0.233 sec/batch; 19h:53m:09s remains)
INFO - root - 2017-12-15 07:38:04.943970: step 24850, loss = 0.24, batch loss = 0.21 (34.8 examples/sec; 0.230 sec/batch; 19h:37m:05s remains)
INFO - root - 2017-12-15 07:38:07.209953: step 24860, loss = 0.27, batch loss = 0.24 (35.2 examples/sec; 0.227 sec/batch; 19h:26m:05s remains)
INFO - root - 2017-12-15 07:38:09.474462: step 24870, loss = 0.23, batch loss = 0.20 (35.0 examples/sec; 0.229 sec/batch; 19h:32m:26s remains)
INFO - root - 2017-12-15 07:38:11.781303: step 24880, loss = 0.25, batch loss = 0.22 (34.8 examples/sec; 0.230 sec/batch; 19h:39m:23s remains)
INFO - root - 2017-12-15 07:38:14.022218: step 24890, loss = 0.20, batch loss = 0.17 (36.5 examples/sec; 0.219 sec/batch; 18h:44m:20s remains)
INFO - root - 2017-12-15 07:38:16.297535: step 24900, loss = 0.19, batch loss = 0.16 (35.0 examples/sec; 0.228 sec/batch; 19h:30m:30s remains)
2017-12-15 07:38:16.639644: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8613443 -5.9633336 -5.993022 -6.1802945 -5.727448 -5.3651452 -4.8900785 -4.2516346 -4.02575 -3.9228139 -3.9300866 -3.9451995 -4.0411773 -3.8494465 -3.4569075][-5.6789861 -6.9069896 -6.6926632 -6.4794383 -6.0649524 -5.8394389 -5.3200722 -4.6998854 -4.6083069 -4.7418828 -4.9154196 -5.1566219 -5.2513285 -5.0715122 -4.7102237][-7.1181359 -7.4977622 -7.1188059 -6.433578 -5.6411462 -5.2900162 -4.5314717 -3.8947871 -4.1427193 -4.7248721 -5.3808908 -6.0912056 -6.3100266 -6.0655193 -5.77969][-7.4221225 -7.0460186 -6.4369936 -5.2163019 -4.1109209 -3.5087428 -2.3853836 -1.7451365 -2.4874129 -3.7134483 -5.1720114 -6.5550437 -6.9683714 -6.6302729 -6.3334904][-6.7350187 -5.8272781 -4.9708786 -3.369935 -2.1168523 -1.1213974 0.49458909 1.1251903 -0.055698633 -1.9191315 -4.1795168 -6.0643549 -6.5573425 -6.1928158 -6.0628014][-5.7632294 -4.4180312 -3.4355483 -1.6200297 -0.14663029 1.4144297 3.3573947 3.7730398 2.2044082 0.021682262 -2.7430868 -4.8954868 -5.3395882 -4.8763218 -4.8059721][-4.2939491 -3.3073423 -2.4407389 -0.6788367 1.0802774 3.2464433 5.3435712 5.5043812 3.7952018 1.4738483 -1.6577704 -3.8289037 -4.0502872 -3.3656397 -3.267092][-3.6504254 -2.9467707 -2.4230433 -1.0054016 0.91476583 3.3994937 5.4143357 5.4426327 4.007421 1.9064317 -1.1908242 -3.1435773 -3.1225772 -2.3861437 -2.228967][-3.6332808 -3.2951338 -3.1661656 -2.2836211 -0.48844135 1.8744688 3.5108962 3.579617 2.6149359 0.85371637 -1.9328455 -3.2991438 -2.9480002 -2.3075991 -2.17428][-4.2893991 -4.3026438 -4.4631424 -3.9897161 -2.4992959 -0.59910369 0.57451272 0.60846305 -0.13170362 -1.6561093 -3.8080108 -4.4928493 -3.873378 -3.349328 -3.2246518][-5.4166465 -5.7105751 -6.0115767 -5.7374816 -4.5417166 -3.1802731 -2.4523537 -2.522367 -3.2485502 -4.5771761 -5.989953 -6.0453663 -5.3029046 -4.8091497 -4.5177584][-6.1570559 -6.4846435 -6.7934651 -6.56043 -5.6552315 -4.8630686 -4.574923 -4.8207531 -5.5396013 -6.54696 -7.2315373 -6.8607969 -6.0955396 -5.6472936 -5.3187418][-6.7354507 -6.7619057 -6.9327793 -6.7148838 -6.1618357 -5.8368235 -5.9011335 -6.3293414 -7.0156312 -7.5213928 -7.6463666 -7.1719384 -6.6453481 -6.3986759 -6.1457229][-6.65818 -6.2570381 -6.1446629 -5.7730308 -5.4290562 -5.5150895 -5.8746357 -6.499516 -7.145154 -7.3941474 -7.3214588 -7.0087843 -6.8377395 -6.8599949 -6.7949839][-6.0800929 -5.3644018 -5.0078831 -4.4104381 -4.0768752 -4.3127375 -4.8463297 -5.5140581 -6.0428076 -6.0504737 -5.918684 -5.7724843 -5.8918347 -6.1387248 -6.2686172]]...]
INFO - root - 2017-12-15 07:38:18.910194: step 24910, loss = 0.18, batch loss = 0.15 (35.1 examples/sec; 0.228 sec/batch; 19h:27m:19s remains)
INFO - root - 2017-12-15 07:38:21.175166: step 24920, loss = 0.25, batch loss = 0.21 (34.9 examples/sec; 0.229 sec/batch; 19h:35m:43s remains)
INFO - root - 2017-12-15 07:38:23.447097: step 24930, loss = 0.26, batch loss = 0.23 (36.4 examples/sec; 0.220 sec/batch; 18h:46m:26s remains)
INFO - root - 2017-12-15 07:38:25.722675: step 24940, loss = 0.22, batch loss = 0.18 (36.0 examples/sec; 0.222 sec/batch; 19h:00m:17s remains)
INFO - root - 2017-12-15 07:38:27.990373: step 24950, loss = 0.26, batch loss = 0.23 (34.6 examples/sec; 0.232 sec/batch; 19h:46m:47s remains)
INFO - root - 2017-12-15 07:38:30.277064: step 24960, loss = 0.21, batch loss = 0.18 (34.7 examples/sec; 0.231 sec/batch; 19h:43m:11s remains)
INFO - root - 2017-12-15 07:38:32.555412: step 24970, loss = 0.19, batch loss = 0.16 (35.5 examples/sec; 0.225 sec/batch; 19h:14m:30s remains)
INFO - root - 2017-12-15 07:38:34.813052: step 24980, loss = 0.19, batch loss = 0.16 (36.3 examples/sec; 0.220 sec/batch; 18h:49m:53s remains)
INFO - root - 2017-12-15 07:38:37.085226: step 24990, loss = 0.20, batch loss = 0.16 (35.7 examples/sec; 0.224 sec/batch; 19h:07m:37s remains)
INFO - root - 2017-12-15 07:38:39.372607: step 25000, loss = 0.36, batch loss = 0.33 (35.1 examples/sec; 0.228 sec/batch; 19h:26m:36s remains)
2017-12-15 07:38:39.722591: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8047936 -5.7339592 -7.4632668 -8.0845394 -6.9403439 -4.7130547 -3.9204979 -3.6414204 -3.087148 -2.8981028 -3.442965 -4.3562851 -5.359334 -5.7762108 -5.5457869][-3.3126822 -4.8724794 -6.1076441 -6.35201 -5.3072462 -3.556004 -2.943881 -2.4167471 -1.7066219 -1.6294866 -2.3211005 -3.41555 -4.7694931 -5.7561588 -6.1017447][-3.8920751 -4.0413241 -4.6616268 -4.5130358 -3.5866315 -2.4799128 -2.2108936 -1.8878877 -1.6194355 -2.0709767 -2.9820342 -3.9167128 -5.2526307 -6.3046503 -6.6934738][-3.9585853 -3.0541475 -3.0781751 -2.4590364 -1.4904931 -0.66359723 -0.62433803 -0.75740445 -1.2871251 -2.5849915 -3.9182496 -4.8489809 -6.0575542 -6.9157019 -7.1542406][-4.3790836 -2.8377004 -2.4660263 -1.6313345 -0.5543077 0.59759784 1.1237218 1.169452 0.15882421 -1.8627639 -3.6414585 -4.624917 -5.7082119 -6.5666051 -7.0229883][-4.9202185 -3.1642914 -2.4741545 -1.3659655 0.0069959164 1.6458981 2.7686095 3.243228 2.1562815 -0.3845737 -2.7637639 -4.1258507 -5.3083382 -6.3161755 -6.9593797][-4.762568 -3.2798519 -2.2232096 -0.64703071 1.1828032 3.22374 4.6406217 5.300561 4.1914172 1.1015444 -2.0623546 -4.0265121 -5.4452062 -6.5566025 -7.1553741][-4.8824482 -3.6208167 -2.2181988 -0.36550391 1.8182087 4.120399 5.4358864 6.0064244 4.9533935 1.5860143 -1.9405597 -4.2009263 -5.5875993 -6.624042 -7.1303353][-4.4951506 -3.7488942 -2.3512578 -0.6639955 1.4192126 3.6144438 4.5914364 4.7890043 3.5495248 0.10844398 -3.2092171 -5.1156912 -6.0220175 -6.6786385 -6.8808079][-3.5222054 -3.3055968 -2.2670741 -1.0558105 0.60818982 2.4707017 2.957149 2.5885806 1.0875213 -2.31758 -5.2185216 -6.6387463 -7.0088563 -7.1358781 -6.8733125][-2.6959553 -3.1365612 -2.4759223 -1.846958 -0.75786316 0.81121445 1.044322 0.40774035 -1.1701088 -4.2614574 -6.5777736 -7.6576023 -7.7587557 -7.51218 -6.8604388][-2.814693 -3.8280945 -3.4047894 -3.0051243 -2.2137358 -0.70616674 -0.49274862 -1.1366898 -2.6526003 -5.3784037 -7.3230653 -8.3404865 -8.3008595 -7.8036532 -6.9225922][-3.35806 -4.5145521 -4.3112006 -3.9956567 -3.3764353 -2.0122845 -1.781539 -2.4235847 -3.8368258 -6.1484652 -7.839057 -8.7738495 -8.62639 -7.9732432 -6.9981632][-4.0225291 -4.9328461 -4.9503155 -4.8169217 -4.374649 -3.3381672 -3.2494135 -3.8390515 -4.9050775 -6.5422344 -7.7869973 -8.5164185 -8.3296413 -7.7257514 -6.912106][-5.5209413 -5.8937244 -5.8885174 -5.8233862 -5.5224447 -4.7918997 -4.7340536 -5.0765619 -5.691236 -6.69675 -7.5905118 -8.1508026 -7.9052567 -7.3621902 -6.7305613]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-25000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-25000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 07:38:42.477782: step 25010, loss = 0.20, batch loss = 0.16 (35.5 examples/sec; 0.225 sec/batch; 19h:13m:26s remains)
INFO - root - 2017-12-15 07:38:44.717607: step 25020, loss = 0.23, batch loss = 0.20 (35.9 examples/sec; 0.223 sec/batch; 19h:01m:06s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 07:38:46.979544: step 25030, loss = 0.19, batch loss = 0.16 (33.9 examples/sec; 0.236 sec/batch; 20h:07m:51s remains)
INFO - root - 2017-12-15 07:38:49.216643: step 25040, loss = 0.27, batch loss = 0.23 (36.6 examples/sec; 0.219 sec/batch; 18h:40m:13s remains)
INFO - root - 2017-12-15 07:38:51.563503: step 25050, loss = 0.24, batch loss = 0.21 (34.6 examples/sec; 0.232 sec/batch; 19h:46m:26s remains)
INFO - root - 2017-12-15 07:38:53.862612: step 25060, loss = 0.28, batch loss = 0.24 (34.6 examples/sec; 0.231 sec/batch; 19h:44m:07s remains)
INFO - root - 2017-12-15 07:38:56.141707: step 25070, loss = 0.18, batch loss = 0.15 (34.4 examples/sec; 0.233 sec/batch; 19h:51m:27s remains)
INFO - root - 2017-12-15 07:38:58.438761: step 25080, loss = 0.24, batch loss = 0.21 (34.4 examples/sec; 0.232 sec/batch; 19h:50m:58s remains)
INFO - root - 2017-12-15 07:39:00.738899: step 25090, loss = 0.24, batch loss = 0.21 (33.4 examples/sec; 0.240 sec/batch; 20h:27m:47s remains)
INFO - root - 2017-12-15 07:39:03.011487: step 25100, loss = 0.34, batch loss = 0.31 (35.8 examples/sec; 0.223 sec/batch; 19h:03m:17s remains)
2017-12-15 07:39:03.363529: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30341411 -1.4390455 -1.0211611 -1.6081648 -2.004982 -2.7482529 -3.1566839 -3.0222216 -3.0958655 -4.3695469 -6.57262 -7.3282518 -7.5137386 -6.7648382 -5.9240265][-1.6069192 -2.1053302 -1.8791895 -2.492429 -2.7615347 -3.0730813 -3.3127713 -3.5107589 -3.942553 -5.0878439 -7.3339429 -8.5693512 -9.0017357 -8.3714371 -7.4813843][-5.5377479 -4.8099766 -4.4535074 -4.5710821 -4.3013029 -3.9436905 -3.8609116 -3.9735785 -4.3265138 -5.1057415 -7.0189371 -8.6526756 -9.4964333 -8.9402571 -8.0890322][-8.0832882 -6.7192864 -6.4550357 -6.010725 -5.0801597 -4.2457991 -3.6571569 -3.5102496 -3.8549974 -4.44373 -6.1459584 -7.9744215 -9.2585192 -8.9650421 -8.2876139][-8.7365723 -7.228385 -7.0390224 -6.05388 -4.433691 -3.1270347 -1.8204956 -1.0744885 -1.0616784 -1.7879689 -3.9060407 -6.3427215 -8.360486 -8.6267929 -8.2996683][-8.9007988 -7.3572226 -7.396492 -6.213769 -4.1666985 -2.34514 -0.26845205 1.5724511 2.6219592 2.2524815 -0.27131081 -3.3692284 -6.3885388 -7.5694218 -8.0132647][-6.8633842 -6.4468579 -7.1068149 -6.1319766 -3.8727176 -1.6300039 0.56502819 3.0185165 5.0798888 5.3597736 2.7482104 -0.52773833 -4.0297537 -6.01294 -7.1729488][-5.4931078 -5.4290104 -6.6391044 -6.1999016 -3.960254 -1.3143307 1.0666063 3.8203049 6.3853812 6.8419929 3.8165288 0.60516858 -2.8973784 -5.2197037 -6.5604415][-4.2798934 -4.4220505 -5.9031143 -6.0279579 -4.1878748 -1.3786778 1.3470805 4.4014182 7.2908807 7.4619145 3.7380061 0.50988865 -2.9264584 -5.280283 -6.5505133][-3.362855 -3.6816106 -5.1932235 -5.9702039 -5.0213656 -2.6887505 -0.15521526 2.9121275 5.8056188 5.8570228 2.0830069 -0.84504175 -3.8596191 -5.8346071 -6.7475171][-3.080627 -3.5215487 -5.1029415 -6.3521261 -6.0155191 -4.1539211 -2.1697285 0.33863807 2.7295399 2.4829454 -1.0095117 -3.6782193 -6.0449018 -7.284452 -7.3257022][-3.5682454 -4.21649 -5.9213428 -7.5747046 -7.608129 -6.0168657 -4.4044695 -2.4139717 -0.42897058 -0.665274 -3.5065079 -5.9084435 -7.8641939 -8.5298462 -7.9582524][-4.3472662 -4.9082537 -6.3784719 -8.1323566 -8.5373344 -7.4029222 -6.4092278 -5.1649613 -3.6863539 -3.5824738 -5.3757057 -7.2543921 -8.6914949 -8.9376822 -8.0964241][-5.3570275 -5.2233648 -6.01484 -7.3079247 -7.7693191 -7.0400453 -6.5982089 -6.1154242 -5.1436567 -5.0234518 -6.1367722 -7.5430059 -8.4863386 -8.2525024 -7.3244166][-5.3213663 -4.7664528 -5.1721115 -5.8864584 -6.0650687 -5.5795479 -5.5021982 -5.6054077 -5.0852442 -5.0870643 -5.8134041 -6.7400136 -7.2180343 -6.6433511 -5.8151369]]...]
INFO - root - 2017-12-15 07:39:05.653227: step 25110, loss = 0.31, batch loss = 0.28 (35.6 examples/sec; 0.225 sec/batch; 19h:11m:31s remains)
INFO - root - 2017-12-15 07:39:07.946490: step 25120, loss = 0.22, batch loss = 0.19 (34.6 examples/sec; 0.231 sec/batch; 19h:45m:23s remains)
INFO - root - 2017-12-15 07:39:10.281284: step 25130, loss = 0.24, batch loss = 0.21 (34.7 examples/sec; 0.230 sec/batch; 19h:40m:32s remains)
INFO - root - 2017-12-15 07:39:12.577422: step 25140, loss = 0.21, batch loss = 0.17 (35.0 examples/sec; 0.229 sec/batch; 19h:31m:54s remains)
INFO - root - 2017-12-15 07:39:14.831913: step 25150, loss = 0.26, batch loss = 0.23 (35.9 examples/sec; 0.223 sec/batch; 19h:01m:48s remains)
INFO - root - 2017-12-15 07:39:17.087258: step 25160, loss = 0.32, batch loss = 0.28 (35.7 examples/sec; 0.224 sec/batch; 19h:06m:44s remains)
INFO - root - 2017-12-15 07:39:19.358152: step 25170, loss = 0.28, batch loss = 0.25 (35.6 examples/sec; 0.225 sec/batch; 19h:09m:59s remains)
INFO - root - 2017-12-15 07:39:21.664527: step 25180, loss = 0.24, batch loss = 0.20 (33.9 examples/sec; 0.236 sec/batch; 20h:10m:27s remains)
INFO - root - 2017-12-15 07:39:23.963783: step 25190, loss = 0.23, batch loss = 0.20 (33.2 examples/sec; 0.241 sec/batch; 20h:35m:26s remains)
INFO - root - 2017-12-15 07:39:26.262423: step 25200, loss = 0.17, batch loss = 0.13 (34.7 examples/sec; 0.231 sec/batch; 19h:40m:44s remains)
2017-12-15 07:39:26.581650: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2288709 -6.8544445 -7.2598925 -6.6244469 -5.6707916 -4.6528053 -3.3524895 -2.4106143 -2.5378957 -3.2362802 -3.8653252 -5.3646441 -5.8696041 -5.635787 -5.5421829][-5.0489712 -6.833003 -7.0909958 -6.2270613 -5.1968174 -4.0752978 -2.424051 -1.1370351 -1.3357095 -2.3343492 -3.0970891 -4.7221465 -5.3743486 -5.6540375 -5.8125563][-5.5593266 -6.6243629 -6.6895132 -5.6516047 -4.4721341 -3.2584372 -1.4250695 -0.091360331 -0.35622263 -1.4339502 -2.2338243 -3.8549843 -4.6603107 -5.4914761 -5.7932615][-6.3567781 -6.3325682 -6.1799321 -4.8659506 -3.4141226 -2.1132419 -0.15542674 1.3435502 1.0569074 -0.25409961 -1.3300166 -3.0608902 -3.9942279 -5.0340691 -5.2671776][-6.5311146 -6.1529722 -5.7622614 -4.1310992 -2.3767416 -0.95937872 1.0830233 2.7560408 2.4334872 0.65701461 -0.92606294 -2.813051 -3.7823961 -4.5951071 -4.459199][-6.6961527 -5.9161386 -5.4230547 -3.6780496 -1.7235469 -0.22643399 1.9851425 3.975455 3.7322557 1.728883 -0.066359043 -2.0094926 -3.1594386 -3.7729762 -3.4041357][-5.9204426 -5.5569639 -5.0537767 -3.3468945 -1.2998198 0.27809024 2.4505517 4.5492935 4.3450556 2.3807051 0.4845922 -1.5224129 -2.7167988 -3.0957522 -2.6526966][-5.7869539 -5.3021975 -4.8380933 -3.2921317 -1.4487827 -0.15812063 1.5802994 3.4576185 3.4186757 1.9034021 0.4446044 -1.165782 -2.0412481 -2.1375842 -1.6903558][-5.69034 -5.311748 -4.9918346 -3.7709327 -2.2180994 -1.0166254 0.4691546 2.2066171 2.4036434 1.526104 0.499763 -0.90379226 -1.5498576 -1.6090136 -1.3403835][-5.73592 -5.5237188 -5.3395824 -4.3457918 -2.9842176 -1.7912474 -0.50411272 0.80945063 1.0953944 0.60382032 -0.17341518 -1.4038277 -2.043561 -2.2625582 -2.1432571][-5.6682711 -5.5544453 -5.4385338 -4.7034025 -3.6704268 -2.7187908 -1.9286603 -1.0103763 -0.73248661 -0.80259585 -1.1215229 -2.0921633 -2.6278725 -3.0402246 -3.1981297][-5.485219 -5.4158173 -5.3687992 -4.842237 -4.0177479 -3.2567244 -2.7103078 -2.0865977 -1.9410576 -2.0093522 -2.2342095 -2.9561319 -3.2816362 -3.7619889 -4.3557272][-5.1882858 -5.1300135 -5.0836849 -4.6501217 -3.9563112 -3.4020655 -3.1602654 -2.8211055 -2.903939 -3.0705266 -3.3648582 -3.9781127 -4.1181688 -4.4147615 -4.994792][-4.7963548 -4.7023592 -4.6476374 -4.3025517 -3.794961 -3.4835148 -3.3996863 -3.1064343 -3.162683 -3.4073391 -3.8550115 -4.4242439 -4.4568968 -4.604425 -5.172533][-4.5621071 -4.45081 -4.3985186 -4.1095009 -3.721992 -3.5009775 -3.4286983 -3.1801393 -3.3520257 -3.7503736 -4.2163262 -4.5886822 -4.5899448 -4.8110619 -5.2153964]]...]
INFO - root - 2017-12-15 07:39:28.850504: step 25210, loss = 0.20, batch loss = 0.17 (37.1 examples/sec; 0.215 sec/batch; 18h:23m:05s remains)
INFO - root - 2017-12-15 07:39:31.119028: step 25220, loss = 0.22, batch loss = 0.19 (35.6 examples/sec; 0.225 sec/batch; 19h:12m:23s remains)
INFO - root - 2017-12-15 07:39:33.389880: step 25230, loss = 0.16, batch loss = 0.13 (35.0 examples/sec; 0.228 sec/batch; 19h:29m:55s remains)
INFO - root - 2017-12-15 07:39:35.697295: step 25240, loss = 0.33, batch loss = 0.30 (34.1 examples/sec; 0.235 sec/batch; 20h:02m:24s remains)
INFO - root - 2017-12-15 07:39:37.970990: step 25250, loss = 0.24, batch loss = 0.21 (35.3 examples/sec; 0.227 sec/batch; 19h:20m:58s remains)
INFO - root - 2017-12-15 07:39:40.259382: step 25260, loss = 0.21, batch loss = 0.18 (35.4 examples/sec; 0.226 sec/batch; 19h:17m:04s remains)
INFO - root - 2017-12-15 07:39:42.585685: step 25270, loss = 0.25, batch loss = 0.21 (34.0 examples/sec; 0.235 sec/batch; 20h:04m:37s remains)
INFO - root - 2017-12-15 07:39:44.840489: step 25280, loss = 0.22, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 19h:01m:38s remains)
INFO - root - 2017-12-15 07:39:47.122361: step 25290, loss = 0.41, batch loss = 0.38 (35.6 examples/sec; 0.225 sec/batch; 19h:11m:19s remains)
INFO - root - 2017-12-15 07:39:49.439822: step 25300, loss = 0.34, batch loss = 0.30 (36.2 examples/sec; 0.221 sec/batch; 18h:52m:38s remains)
2017-12-15 07:39:49.780300: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.8741241 -7.2473745 -6.54574 -5.6054449 -4.2524767 -3.7402811 -4.3902168 -5.4798012 -6.5351815 -7.2699761 -7.5671406 -8.2199345 -8.6610956 -8.5287724 -7.8617878][-9.516881 -8.2920532 -7.3796725 -6.2981071 -4.7871952 -4.0556979 -4.4775209 -5.4271517 -6.594739 -7.45686 -7.7091055 -8.4338284 -8.9940357 -8.834177 -8.1250858][-10.722073 -7.9370384 -6.6742516 -5.6092558 -4.0218606 -3.1095462 -3.1791873 -3.9566226 -5.1856956 -6.2181158 -6.5596237 -7.6223464 -8.40967 -8.3262138 -7.7227845][-10.487312 -6.7049565 -5.0391617 -3.9768124 -2.2259486 -0.89312637 -0.43461788 -1.0000752 -2.4705293 -4.1432605 -5.0226917 -6.7353592 -7.929482 -8.0108194 -7.4583535][-9.2992249 -5.0031061 -3.253552 -2.2867627 -0.30444503 1.4782822 2.5971823 2.4580698 0.69013596 -1.883812 -3.5821137 -6.0732479 -7.6571054 -7.8531971 -7.284102][-8.0897388 -3.5653605 -1.9343269 -0.89359057 1.3740571 3.5807381 5.4281712 5.8528838 3.7711153 0.31657529 -2.2013962 -5.3970184 -7.4191775 -7.8399434 -7.245163][-6.6976953 -2.8130691 -1.3090576 -0.029136419 2.4979053 4.9627481 7.3469024 8.1978931 5.8938518 1.7366002 -1.5346391 -5.220808 -7.5251169 -8.0482292 -7.3994217][-6.8151197 -3.1625638 -1.9542408 -0.5826751 1.8576937 4.2839079 6.9031072 8.1779022 5.8576951 1.3618171 -2.1259842 -5.7831974 -8.0289135 -8.4794579 -7.7054348][-8.0273685 -4.8568087 -3.9092321 -2.6147478 -0.41560841 1.7906013 4.4736147 5.7470808 3.4730277 -0.712885 -3.9025903 -7.1119657 -8.8766 -8.9439049 -7.983407][-9.1239052 -6.3205462 -5.5279961 -4.3845482 -2.563375 -0.75620615 1.4075968 2.227931 0.14570308 -3.3304403 -5.9545755 -8.5329161 -9.690671 -9.3805656 -8.4068851][-9.9754124 -7.6246634 -7.1082473 -6.2987137 -5.0022869 -3.7752984 -2.3860679 -2.034806 -3.6167524 -6.0366154 -7.941205 -9.78598 -10.31382 -9.6525574 -8.5788288][-10.460876 -8.5038624 -8.1990376 -7.769155 -7.014523 -6.2934027 -5.3647451 -5.0623584 -6.0122375 -7.4377146 -8.6522694 -9.8743134 -10.022261 -9.2973957 -8.3350868][-9.6742287 -7.84886 -7.5747294 -7.3916969 -7.0839033 -6.7600904 -6.2417073 -5.9901676 -6.4237828 -7.1103172 -7.8941121 -8.720787 -8.7748356 -8.2617 -7.5329914][-8.225893 -6.5494061 -6.2566123 -6.2139511 -6.2290306 -6.2658906 -6.0873041 -5.8581929 -5.934371 -6.228838 -6.6931286 -7.168117 -7.2080398 -6.9536443 -6.5078378][-6.8577113 -5.5413117 -5.3583517 -5.3963795 -5.5186377 -5.622653 -5.5088949 -5.254982 -5.1871114 -5.318325 -5.5614061 -5.8120356 -5.890399 -5.8251333 -5.5596848]]...]
INFO - root - 2017-12-15 07:39:52.084435: step 25310, loss = 0.20, batch loss = 0.17 (32.5 examples/sec; 0.246 sec/batch; 20h:58m:54s remains)
INFO - root - 2017-12-15 07:39:54.382987: step 25320, loss = 0.28, batch loss = 0.25 (36.0 examples/sec; 0.223 sec/batch; 18h:59m:12s remains)
INFO - root - 2017-12-15 07:39:56.670373: step 25330, loss = 0.27, batch loss = 0.24 (34.6 examples/sec; 0.231 sec/batch; 19h:44m:48s remains)
INFO - root - 2017-12-15 07:39:58.947736: step 25340, loss = 0.22, batch loss = 0.19 (35.8 examples/sec; 0.223 sec/batch; 19h:03m:16s remains)
INFO - root - 2017-12-15 07:40:01.239773: step 25350, loss = 0.22, batch loss = 0.19 (34.3 examples/sec; 0.233 sec/batch; 19h:54m:41s remains)
INFO - root - 2017-12-15 07:40:03.538273: step 25360, loss = 0.35, batch loss = 0.32 (35.6 examples/sec; 0.225 sec/batch; 19h:09m:13s remains)
INFO - root - 2017-12-15 07:40:05.799425: step 25370, loss = 0.26, batch loss = 0.22 (36.1 examples/sec; 0.222 sec/batch; 18h:55m:07s remains)
INFO - root - 2017-12-15 07:40:08.054722: step 25380, loss = 0.22, batch loss = 0.19 (34.4 examples/sec; 0.233 sec/batch; 19h:51m:43s remains)
INFO - root - 2017-12-15 07:40:10.344635: step 25390, loss = 0.43, batch loss = 0.39 (35.7 examples/sec; 0.224 sec/batch; 19h:06m:33s remains)
INFO - root - 2017-12-15 07:40:12.589637: step 25400, loss = 0.22, batch loss = 0.19 (34.7 examples/sec; 0.231 sec/batch; 19h:41m:01s remains)
2017-12-15 07:40:12.923280: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5533168 -4.25557 -4.9744167 -5.2334847 -4.9899487 -4.8291521 -4.8866229 -5.0048604 -4.939023 -5.0539618 -5.3601093 -5.5500212 -5.4367104 -4.8843646 -3.8468726][-3.9326272 -5.066031 -5.8542595 -6.0261564 -5.6946173 -5.4331656 -5.2712336 -5.392096 -5.4639931 -5.6811161 -5.97394 -6.1151562 -6.0976095 -5.7121487 -4.4120855][-4.5163889 -4.96607 -5.8168983 -5.9774942 -5.5637779 -5.09303 -4.4919958 -4.5522366 -4.9068637 -5.2967892 -5.6771803 -5.8970032 -6.0265841 -5.8937845 -4.50075][-4.627737 -4.3228078 -5.2464457 -5.2994366 -4.6199532 -3.8061709 -2.6630528 -2.6401055 -3.3493309 -4.0320406 -4.7484417 -5.2372169 -5.539444 -5.706502 -4.5408111][-4.1088371 -2.9340894 -3.8867865 -3.7197661 -2.5910275 -1.4781595 0.088796139 0.18801832 -0.91458941 -2.0386717 -3.3155007 -4.3156881 -4.82448 -5.2552433 -4.5518732][-3.2517338 -1.399626 -2.3214054 -1.9517276 -0.46616542 0.95622444 2.9675941 3.2892909 2.0140176 0.41524625 -1.5397279 -3.13197 -3.985817 -4.71316 -4.577775][-2.2132883 -0.10964918 -0.97301245 -0.56433368 1.0501986 2.7348576 5.0351982 5.6123285 4.3312464 2.1992722 -0.35061836 -2.3197598 -3.491869 -4.4182577 -4.660387][-1.7049116 0.49810076 -0.35156417 0.088794947 1.6716635 3.3447485 5.6485763 6.3666253 5.1060963 2.5439639 -0.18755603 -2.1483603 -3.4300852 -4.4258466 -4.7398548][-1.8873241 -0.023366451 -0.92660248 -0.45959258 0.84895992 2.1659565 4.161201 4.8696733 3.6289253 1.0349271 -1.3351074 -2.8552856 -3.9221144 -4.7413049 -4.8101487][-2.3811009 -1.0809413 -2.0473971 -1.5290349 -0.57292879 0.29973197 1.8568959 2.4171457 1.2248034 -1.1756934 -2.9680996 -4.0362339 -4.7358313 -5.206378 -4.8881407][-2.825422 -2.2003849 -3.1372149 -2.6442153 -1.9879189 -1.4838094 -0.362965 0.023853779 -1.1945777 -3.3157036 -4.4740448 -5.1377106 -5.4406981 -5.5486565 -4.921423][-3.943542 -3.8773618 -4.7365375 -4.3547888 -3.8164973 -3.3652868 -2.4238195 -2.0613291 -3.2562733 -4.9799986 -5.5673428 -5.8428288 -5.8659744 -5.7290387 -4.9187889][-4.8972936 -5.0892477 -5.8495073 -5.6648512 -5.2721205 -4.8545122 -4.1210055 -3.8703728 -4.95286 -6.28865 -6.4585505 -6.4019804 -6.1880441 -5.7898211 -4.8471074][-5.3591037 -5.6506758 -6.3279204 -6.3673887 -6.1326828 -5.7844543 -5.3199797 -5.2262673 -6.073843 -6.9985762 -6.8895926 -6.6073642 -6.241715 -5.6307745 -4.6465511][-5.9377456 -6.0853467 -6.5919223 -6.7041378 -6.5490494 -6.2711859 -5.9874058 -5.9321914 -6.4814277 -7.0111108 -6.7847266 -6.348783 -5.8644123 -5.1712828 -4.2719469]]...]
INFO - root - 2017-12-15 07:40:15.171442: step 25410, loss = 0.21, batch loss = 0.18 (35.7 examples/sec; 0.224 sec/batch; 19h:07m:44s remains)
INFO - root - 2017-12-15 07:40:17.414866: step 25420, loss = 0.20, batch loss = 0.17 (35.5 examples/sec; 0.225 sec/batch; 19h:11m:53s remains)
INFO - root - 2017-12-15 07:40:19.695241: step 25430, loss = 0.21, batch loss = 0.18 (35.4 examples/sec; 0.226 sec/batch; 19h:16m:53s remains)
INFO - root - 2017-12-15 07:40:22.001870: step 25440, loss = 0.27, batch loss = 0.24 (34.2 examples/sec; 0.234 sec/batch; 19h:56m:03s remains)
INFO - root - 2017-12-15 07:40:24.283196: step 25450, loss = 0.18, batch loss = 0.15 (33.0 examples/sec; 0.242 sec/batch; 20h:38m:46s remains)
INFO - root - 2017-12-15 07:40:26.570599: step 25460, loss = 0.29, batch loss = 0.26 (35.8 examples/sec; 0.223 sec/batch; 19h:03m:27s remains)
INFO - root - 2017-12-15 07:40:28.861030: step 25470, loss = 0.18, batch loss = 0.15 (34.1 examples/sec; 0.234 sec/batch; 19h:58m:52s remains)
INFO - root - 2017-12-15 07:40:31.199964: step 25480, loss = 0.21, batch loss = 0.18 (34.8 examples/sec; 0.230 sec/batch; 19h:35m:20s remains)
INFO - root - 2017-12-15 07:40:33.504259: step 25490, loss = 0.23, batch loss = 0.20 (34.3 examples/sec; 0.233 sec/batch; 19h:51m:56s remains)
INFO - root - 2017-12-15 07:40:35.829140: step 25500, loss = 0.26, batch loss = 0.23 (34.5 examples/sec; 0.232 sec/batch; 19h:48m:03s remains)
2017-12-15 07:40:36.172674: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2876673 -6.32693 -6.3466034 -6.3655081 -6.66508 -6.9757471 -7.0149727 -6.8709669 -6.6621771 -6.3510323 -5.9678373 -5.62873 -5.4429178 -5.3452659 -5.2871933][-7.4087868 -7.3943853 -7.3609715 -7.3344135 -7.7538671 -8.3110771 -8.500042 -8.5393257 -8.454464 -8.2069445 -7.9668322 -7.6973877 -7.4833956 -7.2822876 -7.151041][-7.1330481 -6.5915585 -6.680109 -6.7278996 -7.3065319 -8.1281109 -8.6915684 -9.1527309 -9.4194374 -9.3857412 -9.37072 -9.3352165 -9.117691 -8.6781321 -8.2242737][-6.0990005 -4.6162729 -4.9252615 -5.233027 -6.0372128 -6.9539919 -7.756423 -8.7368946 -9.5281458 -9.739665 -9.9983482 -10.283386 -10.128607 -9.5090933 -8.68192][-4.7230349 -2.4219458 -2.4442697 -2.5965931 -3.2930899 -4.1441689 -5.0121589 -6.4134388 -7.9289684 -8.8140841 -9.6575994 -10.466414 -10.552442 -10.004648 -8.9290581][-3.3472328 -0.96190321 -0.84272659 -0.62604034 -0.794137 -1.1058934 -1.628263 -3.0967684 -5.1367617 -6.7314634 -8.251524 -9.6615887 -10.063039 -9.6579742 -8.4121161][-2.6577129 -0.38202131 -0.0011303425 0.58585072 1.0739441 1.5200198 1.6216061 0.43036938 -1.7278904 -3.6831908 -5.6314449 -7.5788994 -8.4638891 -8.584549 -7.634737][-3.1778321 -0.88814104 -0.094320536 1.0711858 2.3288138 3.4450409 4.1118336 3.2961509 1.2063713 -0.91150987 -3.0805225 -5.2184782 -6.4091749 -6.9971838 -6.5354872][-4.3944273 -2.4920263 -1.6340747 -0.11787939 1.9585536 3.7607367 4.9059219 4.5276356 2.8121717 0.80754662 -1.4009075 -3.6020427 -4.8337803 -5.4984636 -5.1833878][-5.649765 -4.030663 -3.3572941 -2.0708663 0.24900222 2.5285976 4.0949707 4.253499 3.2033894 1.8185356 -0.2035501 -2.5099208 -3.8155811 -4.5335207 -4.2770195][-6.7416325 -5.3993073 -4.9461913 -4.0354624 -1.9229587 0.28040409 1.8336499 2.3268373 1.9843986 1.4806256 0.10106301 -1.7662653 -2.8223555 -3.5416131 -3.4897122][-7.5775518 -6.7495804 -6.6629972 -6.2130075 -4.6689267 -2.8464613 -1.36869 -0.52766228 -0.24163818 0.053465128 -0.50961339 -1.5474223 -2.0335066 -2.6768599 -2.9532528][-7.6135273 -7.417017 -7.8873854 -8.1147881 -7.3245068 -5.9144235 -4.4496326 -3.2784376 -2.4884722 -1.540239 -1.245607 -1.4326316 -1.2680533 -1.5709524 -1.9583688][-7.0408211 -7.1557493 -7.9415808 -8.6491051 -8.556325 -7.75453 -6.7000179 -5.5669079 -4.6056986 -3.3956556 -2.4092984 -1.7535872 -1.0119935 -0.70961678 -0.72884822][-6.5230856 -6.6900492 -7.59136 -8.40688 -8.6122713 -8.2381163 -7.6885214 -6.8272572 -5.8030577 -4.553102 -3.2077212 -2.0518839 -1.0800879 -0.39325714 -0.00073075294]]...]
INFO - root - 2017-12-15 07:40:38.419811: step 25510, loss = 0.17, batch loss = 0.14 (35.3 examples/sec; 0.226 sec/batch; 19h:18m:40s remains)
INFO - root - 2017-12-15 07:40:40.686602: step 25520, loss = 0.28, batch loss = 0.25 (36.7 examples/sec; 0.218 sec/batch; 18h:34m:42s remains)
INFO - root - 2017-12-15 07:40:42.989695: step 25530, loss = 0.23, batch loss = 0.20 (33.8 examples/sec; 0.237 sec/batch; 20h:11m:57s remains)
INFO - root - 2017-12-15 07:40:45.279967: step 25540, loss = 0.24, batch loss = 0.20 (33.3 examples/sec; 0.241 sec/batch; 20h:30m:49s remains)
INFO - root - 2017-12-15 07:40:47.527450: step 25550, loss = 0.28, batch loss = 0.25 (35.6 examples/sec; 0.225 sec/batch; 19h:10m:12s remains)
INFO - root - 2017-12-15 07:40:49.818748: step 25560, loss = 0.37, batch loss = 0.34 (35.3 examples/sec; 0.227 sec/batch; 19h:19m:12s remains)
INFO - root - 2017-12-15 07:40:52.113086: step 25570, loss = 0.24, batch loss = 0.21 (34.7 examples/sec; 0.231 sec/batch; 19h:39m:41s remains)
INFO - root - 2017-12-15 07:40:54.399080: step 25580, loss = 0.20, batch loss = 0.17 (35.2 examples/sec; 0.227 sec/batch; 19h:21m:07s remains)
INFO - root - 2017-12-15 07:40:56.719434: step 25590, loss = 0.28, batch loss = 0.25 (34.0 examples/sec; 0.235 sec/batch; 20h:04m:22s remains)
INFO - root - 2017-12-15 07:40:59.007000: step 25600, loss = 0.20, batch loss = 0.17 (35.5 examples/sec; 0.225 sec/batch; 19h:12m:38s remains)
2017-12-15 07:40:59.372462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6554029 -4.3465328 -4.4855022 -4.8762283 -5.2120533 -5.3931046 -5.7572556 -5.8620081 -6.1592379 -5.4809442 -4.3097315 -3.906975 -4.2280941 -4.0460753 -3.5307188][-1.8647579 -3.6194825 -3.8214998 -4.314115 -4.6089239 -4.6124287 -4.7046027 -4.7140169 -4.8719625 -3.9409356 -2.9129119 -2.9966676 -3.7475052 -3.8842378 -3.4820125][-2.4921529 -2.8777206 -2.9592407 -3.4603844 -3.4922669 -3.162499 -2.9254353 -2.941287 -3.236372 -2.4105103 -1.595782 -2.1466968 -3.24541 -3.661459 -3.4876442][-2.9694641 -2.5144737 -2.3791595 -2.6670542 -2.3543823 -1.7594869 -1.3172743 -1.4309061 -1.9509089 -1.2608111 -0.39155436 -0.92468762 -2.0097432 -2.5792212 -2.5528886][-3.3417802 -2.4333544 -2.1544802 -2.1645887 -1.4670281 -0.59076452 0.051536083 -0.096964121 -0.78845084 -0.24388719 0.73270488 0.28341079 -0.92105782 -1.8304739 -2.1430664][-4.0434785 -2.5868819 -2.039114 -1.7592556 -0.82723653 0.27161908 1.1739085 1.0854428 0.10647368 0.28998184 1.134187 0.6933744 -0.62050116 -1.8176548 -2.4022222][-4.6611805 -3.0122581 -2.3106835 -1.9239478 -1.071584 0.162673 1.4269984 1.4764037 0.3802681 0.42673087 1.1051292 0.74719763 -0.56064939 -1.867959 -2.588917][-5.3766184 -3.2564549 -2.3863058 -2.015203 -1.5421176 -0.42646062 1.0604174 1.3162363 0.39517903 0.54845357 1.0294936 0.65931249 -0.6431669 -2.0415905 -2.8308468][-5.6360059 -3.2435737 -2.2627311 -1.8753803 -1.5239277 -0.46939254 1.2473364 1.7021618 0.93608165 1.1323073 1.4876006 1.0648103 -0.27903855 -1.8051348 -2.6561458][-5.8221092 -3.4017239 -2.6223311 -2.3826241 -2.0555053 -1.0023073 0.83703613 1.5823288 1.0901172 1.3199081 1.654912 1.3966925 0.24141955 -1.0813739 -1.8622084][-5.6589031 -3.3986659 -2.8322775 -2.8091562 -2.4180589 -1.380156 0.39155984 1.2189779 0.9197104 1.0671961 1.4185693 1.4407 0.688663 -0.0785532 -0.44956338][-5.75806 -3.8710966 -3.60028 -3.7160177 -3.2304766 -2.2218947 -0.4761374 0.49346685 0.40165758 0.52559924 0.85283589 1.0825155 0.74984026 0.50077248 0.45548415][-5.3996 -3.744982 -3.9148245 -4.1878347 -3.5676804 -2.6539598 -1.2620397 -0.46252024 -0.62745488 -0.62457538 -0.35369921 -0.0087509155 -0.038571358 -0.043076277 -0.091132164][-5.4254179 -3.8966427 -4.4171181 -4.8037004 -3.9727585 -3.1510024 -2.332335 -1.8958633 -2.2030921 -2.3862519 -2.1175129 -1.5639095 -1.2472155 -1.0666906 -1.1419642][-5.816855 -4.6064281 -5.3183241 -5.5113277 -4.3714609 -3.549377 -3.2383816 -3.2536316 -3.7919149 -4.2387085 -3.9483843 -3.0138874 -2.2753332 -1.9707046 -2.1612358]]...]
INFO - root - 2017-12-15 07:41:01.669242: step 25610, loss = 0.28, batch loss = 0.25 (35.7 examples/sec; 0.224 sec/batch; 19h:05m:23s remains)
INFO - root - 2017-12-15 07:41:03.942067: step 25620, loss = 0.23, batch loss = 0.20 (35.6 examples/sec; 0.225 sec/batch; 19h:09m:40s remains)
INFO - root - 2017-12-15 07:41:06.216519: step 25630, loss = 0.22, batch loss = 0.19 (35.3 examples/sec; 0.227 sec/batch; 19h:19m:17s remains)
INFO - root - 2017-12-15 07:41:08.510766: step 25640, loss = 0.20, batch loss = 0.17 (35.7 examples/sec; 0.224 sec/batch; 19h:05m:10s remains)
INFO - root - 2017-12-15 07:41:10.809283: step 25650, loss = 0.19, batch loss = 0.16 (36.2 examples/sec; 0.221 sec/batch; 18h:50m:43s remains)
INFO - root - 2017-12-15 07:41:13.081342: step 25660, loss = 0.27, batch loss = 0.23 (35.7 examples/sec; 0.224 sec/batch; 19h:07m:29s remains)
INFO - root - 2017-12-15 07:41:15.322632: step 25670, loss = 0.27, batch loss = 0.23 (35.9 examples/sec; 0.223 sec/batch; 18h:58m:40s remains)
INFO - root - 2017-12-15 07:41:17.580325: step 25680, loss = 0.17, batch loss = 0.14 (35.9 examples/sec; 0.223 sec/batch; 18h:59m:27s remains)
INFO - root - 2017-12-15 07:41:19.848068: step 25690, loss = 0.23, batch loss = 0.19 (34.0 examples/sec; 0.235 sec/batch; 20h:04m:07s remains)
INFO - root - 2017-12-15 07:41:22.104101: step 25700, loss = 0.19, batch loss = 0.16 (36.1 examples/sec; 0.221 sec/batch; 18h:52m:29s remains)
2017-12-15 07:41:22.419158: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0563078 -5.5705018 -4.1024132 -4.0877504 -3.9261909 -3.4063411 -3.0597804 -3.182056 -3.340909 -3.3772328 -3.7734313 -3.4705977 -3.49157 -4.1452188 -5.108027][-5.5739264 -6.1299238 -4.6774092 -4.4832039 -3.97845 -3.4355721 -3.1401455 -3.3549709 -3.6042013 -3.7944469 -4.4457521 -4.1830635 -4.0705433 -4.793191 -5.6668024][-6.9496546 -6.5934587 -5.2226448 -4.9005003 -4.1587043 -3.419817 -3.0535681 -3.2181215 -3.4820757 -3.7904496 -4.7223206 -4.7348948 -4.5882549 -5.1302247 -5.6633325][-7.9870687 -6.9563966 -5.5824995 -5.0189567 -3.9761951 -3.0323088 -2.5008979 -2.4652472 -2.7841959 -3.3981209 -4.6211653 -4.9095221 -4.7750425 -4.9951172 -5.1025534][-8.4213438 -7.1752834 -5.7264466 -4.8737764 -3.5526042 -2.3616295 -1.4469806 -1.0279778 -1.4660692 -2.4475429 -3.8531337 -4.463912 -4.4427967 -4.3309555 -3.9114172][-8.8351946 -7.2242455 -5.5954533 -4.3837843 -2.7419088 -1.1985884 0.3066721 1.1263669 0.45530891 -0.883031 -2.4808061 -3.4534705 -3.7222977 -3.4701076 -2.5786216][-8.2678795 -6.9857025 -5.0992632 -3.5695891 -1.7132076 0.10868621 2.14919 3.2004142 2.1401768 0.4638741 -1.2913957 -2.5296066 -3.0312624 -2.5834732 -1.2312864][-8.13854 -6.6706 -4.6019745 -2.8852549 -0.88848114 1.0997462 3.5110307 4.54243 2.9473968 1.0583944 -0.80755043 -2.2509351 -2.7930002 -1.9513711 -0.08324337][-8.0188313 -6.4460306 -4.2723322 -2.494132 -0.50166321 1.4419169 3.8698468 4.6320333 2.6272063 0.6910696 -1.2335346 -2.7112448 -2.9581428 -1.6061423 0.56831813][-7.9461918 -6.2827063 -4.0502672 -2.3423233 -0.55178845 1.0441735 3.0725675 3.461648 1.3312268 -0.47135508 -2.3044906 -3.5356402 -3.3120322 -1.6893599 0.38955641][-7.9162493 -6.1599817 -3.8521242 -2.247772 -0.82904482 0.25677609 1.6421227 1.59004 -0.45733798 -2.1125205 -3.7623808 -4.6703978 -3.9310071 -2.0873783 -0.26560807][-7.8042631 -5.9449596 -3.5857983 -2.0890763 -1.0958978 -0.53972113 0.24338055 -0.1760664 -1.992702 -3.404088 -4.8402061 -5.4244003 -4.3471766 -2.6025352 -1.2707952][-7.6050258 -5.6534915 -3.2280354 -1.7340536 -0.98726416 -0.699304 -0.3528676 -1.053203 -2.6829977 -3.9417734 -5.2895322 -5.7752104 -4.8013263 -3.3872657 -2.4969153][-7.3298283 -5.3018575 -2.8059239 -1.2411081 -0.53883743 -0.22682381 -0.038317919 -0.95159674 -2.4801404 -3.7457883 -5.1570821 -5.7501993 -5.01958 -3.9234476 -3.3628385][-7.0333557 -4.97367 -2.5055408 -0.96153748 -0.18096185 0.35795641 0.60742545 -0.3187145 -1.702463 -3.0452518 -4.610929 -5.3380337 -4.9870443 -4.3599453 -4.0728068]]...]
INFO - root - 2017-12-15 07:41:24.699111: step 25710, loss = 0.21, batch loss = 0.18 (35.8 examples/sec; 0.224 sec/batch; 19h:03m:59s remains)
INFO - root - 2017-12-15 07:41:26.973009: step 25720, loss = 0.23, batch loss = 0.20 (36.8 examples/sec; 0.217 sec/batch; 18h:31m:24s remains)
INFO - root - 2017-12-15 07:41:29.279974: step 25730, loss = 0.31, batch loss = 0.27 (34.1 examples/sec; 0.235 sec/batch; 19h:59m:28s remains)
INFO - root - 2017-12-15 07:41:31.580077: step 25740, loss = 0.25, batch loss = 0.21 (33.7 examples/sec; 0.238 sec/batch; 20h:15m:14s remains)
INFO - root - 2017-12-15 07:41:33.844013: step 25750, loss = 0.29, batch loss = 0.25 (35.5 examples/sec; 0.225 sec/batch; 19h:12m:06s remains)
INFO - root - 2017-12-15 07:41:36.116300: step 25760, loss = 0.20, batch loss = 0.16 (34.9 examples/sec; 0.229 sec/batch; 19h:33m:00s remains)
INFO - root - 2017-12-15 07:41:38.406083: step 25770, loss = 0.19, batch loss = 0.16 (33.0 examples/sec; 0.242 sec/batch; 20h:38m:55s remains)
INFO - root - 2017-12-15 07:41:40.685626: step 25780, loss = 0.27, batch loss = 0.23 (34.0 examples/sec; 0.236 sec/batch; 20h:04m:02s remains)
INFO - root - 2017-12-15 07:41:42.967398: step 25790, loss = 0.19, batch loss = 0.16 (34.6 examples/sec; 0.231 sec/batch; 19h:42m:00s remains)
INFO - root - 2017-12-15 07:41:45.271561: step 25800, loss = 0.18, batch loss = 0.14 (33.9 examples/sec; 0.236 sec/batch; 20h:05m:43s remains)
2017-12-15 07:41:45.628679: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6393414 -3.6614223 -3.0459824 -3.5663848 -4.873735 -6.1221838 -6.7107172 -6.8202682 -6.2575922 -5.2167764 -4.2913666 -3.6089683 -3.1901457 -3.0778265 -3.3878446][-3.048254 -1.7945346 -1.2524577 -2.12895 -3.5352616 -4.6434746 -4.9872928 -5.0276461 -4.6838093 -4.0668926 -3.4494424 -2.8288474 -2.2972844 -1.9400676 -1.9110658][-2.5972054 -0.40777433 -0.033342838 -1.0813445 -2.391463 -3.2238574 -3.3003187 -3.2726641 -3.2352271 -3.1738305 -2.9839692 -2.4040625 -1.6454532 -0.93267417 -0.45894945][-2.5078897 0.23352504 0.42178011 -0.55067861 -1.4910698 -1.8663359 -1.6033772 -1.3924987 -1.6108193 -2.1610625 -2.5195022 -2.11772 -1.2395036 -0.25013924 0.53444052][-2.9917965 -0.052856922 -0.0755198 -0.59526408 -0.7830373 -0.42176068 0.4498632 1.0373027 0.67633367 -0.41343021 -1.422482 -1.6188726 -1.1238669 -0.30650187 0.40683341][-3.1998658 -0.69433403 -0.8355242 -0.74395633 -0.10101318 1.0193424 2.5811653 3.5693069 3.1275353 1.5864117 -0.054688454 -1.0435833 -1.3815079 -1.1947495 -0.946365][-2.9752231 -1.1057539 -1.2809091 -0.606457 0.68299866 2.3336167 4.3892326 5.6692419 5.0297375 3.0815549 0.92061782 -0.85014653 -2.271673 -2.8335881 -3.1313255][-3.2406831 -1.7762556 -2.0892668 -1.0831662 0.58063745 2.4729171 4.8846059 6.3123264 5.4393792 3.1942506 0.739836 -1.5792753 -3.7948775 -4.7018924 -5.0472708][-3.8424759 -2.8944066 -3.4779153 -2.3691571 -0.60807276 1.2919903 3.7277856 4.9081216 3.8171625 1.6116607 -0.74558294 -3.1265175 -5.4263124 -6.1816654 -6.211319][-4.3251352 -3.5608351 -4.35244 -3.2363193 -1.6517336 -0.053739309 1.9305849 2.6114502 1.4095442 -0.50620413 -2.475637 -4.574192 -6.3763146 -6.72097 -6.3842325][-4.7444868 -4.024262 -5.0041213 -4.0615325 -2.8987012 -1.7349166 -0.25188208 0.028905392 -1.0921472 -2.4838059 -3.9465196 -5.7788153 -7.0139556 -7.0497284 -6.4417057][-5.373003 -4.5927429 -5.6181712 -4.8212137 -3.9443626 -3.0901155 -2.0238433 -2.0350771 -3.078563 -4.1580625 -5.3379889 -6.9798141 -7.7552962 -7.6081128 -6.909296][-5.9742975 -5.0853939 -5.8535061 -5.0382042 -4.2998209 -3.7265179 -3.1148479 -3.4000793 -4.4486618 -5.3291893 -6.2913508 -7.6246939 -8.0864382 -7.9533396 -7.4128208][-6.535718 -5.6762676 -6.0720167 -5.3409948 -4.7220616 -4.3220692 -4.0456295 -4.444272 -5.3571968 -5.9832735 -6.6504083 -7.578476 -7.8950238 -7.8551645 -7.5211282][-6.5397682 -5.8069191 -6.0019026 -5.5227737 -5.0405607 -4.7509336 -4.6585841 -5.0139256 -5.64589 -5.9699793 -6.3526354 -6.8924513 -7.1929193 -7.3704844 -7.3813119]]...]
INFO - root - 2017-12-15 07:41:47.903631: step 25810, loss = 0.25, batch loss = 0.22 (34.3 examples/sec; 0.233 sec/batch; 19h:51m:35s remains)
INFO - root - 2017-12-15 07:41:50.162421: step 25820, loss = 0.32, batch loss = 0.28 (36.9 examples/sec; 0.217 sec/batch; 18h:27m:22s remains)
INFO - root - 2017-12-15 07:41:52.421049: step 25830, loss = 0.31, batch loss = 0.28 (35.5 examples/sec; 0.225 sec/batch; 19h:10m:56s remains)
INFO - root - 2017-12-15 07:41:54.698621: step 25840, loss = 0.19, batch loss = 0.16 (35.7 examples/sec; 0.224 sec/batch; 19h:06m:01s remains)
INFO - root - 2017-12-15 07:41:56.963929: step 25850, loss = 0.19, batch loss = 0.16 (36.2 examples/sec; 0.221 sec/batch; 18h:50m:43s remains)
INFO - root - 2017-12-15 07:41:59.262788: step 25860, loss = 0.34, batch loss = 0.31 (35.8 examples/sec; 0.224 sec/batch; 19h:03m:32s remains)
INFO - root - 2017-12-15 07:42:01.528857: step 25870, loss = 0.23, batch loss = 0.20 (35.2 examples/sec; 0.227 sec/batch; 19h:20m:33s remains)
INFO - root - 2017-12-15 07:42:03.814642: step 25880, loss = 0.22, batch loss = 0.19 (35.2 examples/sec; 0.227 sec/batch; 19h:22m:01s remains)
INFO - root - 2017-12-15 07:42:06.108926: step 25890, loss = 0.20, batch loss = 0.17 (34.2 examples/sec; 0.234 sec/batch; 19h:54m:15s remains)
INFO - root - 2017-12-15 07:42:08.380635: step 25900, loss = 0.25, batch loss = 0.22 (34.3 examples/sec; 0.233 sec/batch; 19h:51m:06s remains)
2017-12-15 07:42:08.717814: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3911877 -5.2036209 -5.01429 -5.0920177 -5.3830957 -5.4586005 -5.6798491 -5.9456773 -6.2275476 -6.3154683 -6.178339 -6.1049614 -6.0362172 -5.9709826 -5.8988495][-7.4853363 -6.8572321 -6.6219282 -6.8098516 -7.3012338 -7.2480445 -7.19719 -7.2922039 -7.7180972 -8.0566187 -8.08693 -8.0342979 -7.8740692 -7.64547 -7.3021836][-9.477232 -7.2203226 -7.0226545 -7.4469905 -8.1821289 -7.9559135 -7.5136609 -7.2525105 -7.702692 -8.3703918 -8.7872257 -8.9960384 -8.9349909 -8.7489529 -8.2408466][-10.721607 -7.4777012 -7.3958735 -7.8925104 -8.5130959 -7.8976483 -7.063199 -6.5158529 -7.0145006 -8.1238031 -9.1590624 -9.7615471 -9.8583546 -9.7569456 -9.13139][-11.828688 -7.9445734 -7.9036865 -8.0319948 -7.969955 -6.4999623 -4.9737692 -4.0010028 -4.5738955 -6.349267 -8.3477478 -9.5307522 -9.9342384 -10.089743 -9.6741047][-11.442106 -8.1637077 -8.0991116 -7.5554881 -6.5209832 -3.9096181 -1.3878284 0.24643874 -0.25666988 -2.7143695 -5.7450109 -7.7054195 -8.652914 -9.3330879 -9.4375429][-10.225345 -7.8680229 -7.5561428 -6.296526 -4.3959522 -0.85676241 2.5603347 4.5884914 3.9367547 0.89682937 -2.9397583 -5.6194696 -7.0707183 -8.07987 -8.5783234][-9.99357 -7.1358118 -6.3545742 -4.661622 -2.3337119 1.739959 5.8567214 8.0668831 7.15408 3.6924219 -0.78209341 -4.015481 -5.7845259 -6.9524188 -7.6611023][-9.6041651 -6.1884141 -5.0632377 -3.6308167 -1.7697896 1.9083562 6.02251 8.2039623 7.3698363 4.1256962 -0.21406364 -3.3891382 -5.09011 -6.3239245 -7.07025][-8.839529 -4.7968745 -3.5740252 -2.8494086 -2.1597836 0.16709566 3.4985013 5.3368878 4.76086 2.069128 -1.5614374 -4.1326933 -5.4291697 -6.5491991 -7.0723352][-8.6009951 -4.2042418 -3.1750209 -3.1404262 -3.7457485 -2.8146479 -0.14045572 1.613435 1.4856019 -0.50434744 -3.4175966 -5.418952 -6.3626404 -7.5537434 -7.860229][-9.3490124 -4.6698055 -3.8084021 -4.2131162 -5.9056792 -6.07358 -3.9794283 -2.2454202 -1.9809318 -3.2626739 -5.4746017 -6.9628386 -7.7097764 -8.9829741 -9.1008577][-10.253658 -5.4502029 -4.3753424 -4.7129164 -7.016736 -7.9384928 -6.5477252 -5.3571072 -5.1155682 -5.9119697 -7.4330754 -8.4728165 -9.1418161 -10.285572 -10.061124][-10.990314 -6.5173478 -5.331212 -5.5878181 -7.9267855 -9.0211086 -8.39928 -8.0337677 -8.00285 -8.189352 -8.8570251 -9.3609781 -9.8545876 -10.648207 -10.208906][-10.697399 -6.7904696 -5.8603811 -6.4111395 -8.4517679 -9.2337494 -9.1008415 -9.2827959 -9.3301563 -9.0525551 -8.9352894 -9.0212231 -9.3217936 -9.718441 -9.2915573]]...]
INFO - root - 2017-12-15 07:42:10.990722: step 25910, loss = 0.24, batch loss = 0.20 (35.3 examples/sec; 0.227 sec/batch; 19h:17m:51s remains)
INFO - root - 2017-12-15 07:42:13.266341: step 25920, loss = 0.24, batch loss = 0.21 (34.0 examples/sec; 0.235 sec/batch; 20h:02m:54s remains)
INFO - root - 2017-12-15 07:42:15.509128: step 25930, loss = 0.24, batch loss = 0.21 (34.6 examples/sec; 0.231 sec/batch; 19h:42m:31s remains)
INFO - root - 2017-12-15 07:42:17.800094: step 25940, loss = 0.23, batch loss = 0.20 (34.9 examples/sec; 0.229 sec/batch; 19h:32m:27s remains)
INFO - root - 2017-12-15 07:42:20.091155: step 25950, loss = 0.20, batch loss = 0.17 (35.0 examples/sec; 0.229 sec/batch; 19h:28m:01s remains)
INFO - root - 2017-12-15 07:42:22.350014: step 25960, loss = 0.25, batch loss = 0.22 (34.6 examples/sec; 0.231 sec/batch; 19h:41m:45s remains)
INFO - root - 2017-12-15 07:42:24.666193: step 25970, loss = 0.21, batch loss = 0.18 (35.4 examples/sec; 0.226 sec/batch; 19h:13m:17s remains)
INFO - root - 2017-12-15 07:42:26.932020: step 25980, loss = 0.25, batch loss = 0.22 (34.5 examples/sec; 0.232 sec/batch; 19h:43m:31s remains)
INFO - root - 2017-12-15 07:42:29.231487: step 25990, loss = 0.21, batch loss = 0.18 (36.0 examples/sec; 0.222 sec/batch; 18h:53m:56s remains)
INFO - root - 2017-12-15 07:42:31.503245: step 26000, loss = 0.20, batch loss = 0.16 (35.9 examples/sec; 0.223 sec/batch; 18h:58m:04s remains)
2017-12-15 07:42:31.872689: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.43936908 -1.1204916 -2.3057857 -3.3169718 -3.4359744 -3.1852956 -2.7484622 -2.4218109 -2.3720045 -2.5360374 -2.6871805 -2.5419321 -2.1247263 -1.7496634 -1.2910748][-3.2271109 -3.4172943 -4.3504238 -5.3056021 -5.4305897 -5.1240616 -4.5599589 -4.1225882 -3.9417028 -3.9429417 -4.0454736 -4.0468063 -3.8676231 -3.5142992 -2.962666][-5.9662938 -5.415699 -6.0681586 -6.7957907 -6.869432 -6.4995756 -5.9411626 -5.5342722 -5.3936625 -5.4439354 -5.606884 -5.7713757 -5.8827939 -5.6961508 -5.1905651][-7.792603 -6.3915176 -6.6379781 -6.8911009 -6.7430248 -6.3823504 -5.9505291 -5.6858611 -5.6340818 -5.8823566 -6.3190961 -6.7991304 -7.2847676 -7.4374557 -7.2363915][-8.1239748 -6.1089621 -5.7929821 -5.4394646 -4.9163933 -4.3025827 -3.883924 -3.6365314 -3.6904802 -4.354207 -5.3251734 -6.2498713 -7.1489906 -7.7373638 -7.9904308][-7.5201769 -5.1047826 -4.3185711 -3.4048867 -2.3407843 -1.1693897 -0.40122485 0.11921382 0.20967484 -0.75596082 -2.3812244 -3.9604585 -5.239296 -6.249548 -7.0360837][-6.2291489 -3.9499257 -2.9165571 -1.6398544 -0.13520622 1.6642096 2.9609179 3.87017 4.22917 3.1788082 1.0514784 -1.129922 -2.7184021 -4.0704556 -5.3537378][-5.528595 -3.1746936 -2.0725811 -0.69755828 1.0283403 3.1568651 4.8629713 6.1020045 6.7011876 5.908464 3.6951289 1.1556215 -0.49769163 -1.9686654 -3.5091491][-5.4947567 -3.2970724 -2.4086282 -1.1920568 0.41970849 2.4228387 4.2985735 5.7559152 6.5249295 6.1137462 4.2964926 1.967824 0.5497303 -0.70235991 -2.0090227][-6.2537866 -4.390862 -3.8510692 -3.033936 -1.7057014 -0.10346699 1.5836608 2.8279572 3.5222821 3.5505838 2.5170183 0.77563262 0.020200968 -0.63138568 -1.3336067][-6.6617074 -5.0669231 -5.0219841 -4.7242279 -3.7495146 -2.5539668 -1.2221445 -0.31133735 0.17700791 0.39509511 0.065578938 -1.0095476 -1.1119382 -1.1271927 -1.1536503][-6.7319274 -5.3873038 -5.82764 -6.1021371 -5.4771762 -4.5807514 -3.4855494 -2.8349857 -2.5989146 -2.3872526 -2.4037175 -3.0009298 -2.5048306 -1.9879279 -1.4997886][-6.616457 -5.5134764 -6.4226804 -7.0790677 -6.70861 -6.0444431 -5.2631559 -4.8546333 -4.8120475 -4.7801061 -4.6332378 -4.791019 -3.9363661 -3.1050327 -2.3796158][-5.5092087 -4.5082603 -5.6196251 -6.4187751 -6.2143149 -5.9362984 -5.64701 -5.5249691 -5.62646 -5.7150974 -5.5737419 -5.49489 -4.5004644 -3.6510439 -3.0335493][-4.2507572 -3.2961311 -4.401803 -5.0896378 -4.9344087 -4.9073997 -5.0278234 -5.1490703 -5.466836 -5.7743921 -5.7537603 -5.5564289 -4.7028351 -3.9667966 -3.6000071]]...]
INFO - root - 2017-12-15 07:42:34.176528: step 26010, loss = 0.28, batch loss = 0.25 (34.5 examples/sec; 0.232 sec/batch; 19h:44m:04s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 07:42:36.445819: step 26020, loss = 0.20, batch loss = 0.17 (36.1 examples/sec; 0.222 sec/batch; 18h:51m:50s remains)
INFO - root - 2017-12-15 07:42:38.678548: step 26030, loss = 0.24, batch loss = 0.21 (36.3 examples/sec; 0.221 sec/batch; 18h:46m:17s remains)
INFO - root - 2017-12-15 07:42:40.970019: step 26040, loss = 0.27, batch loss = 0.24 (34.9 examples/sec; 0.229 sec/batch; 19h:31m:09s remains)
INFO - root - 2017-12-15 07:42:43.278113: step 26050, loss = 0.26, batch loss = 0.22 (34.9 examples/sec; 0.229 sec/batch; 19h:29m:59s remains)
INFO - root - 2017-12-15 07:42:45.605077: step 26060, loss = 0.29, batch loss = 0.25 (34.3 examples/sec; 0.233 sec/batch; 19h:51m:51s remains)
INFO - root - 2017-12-15 07:42:47.888049: step 26070, loss = 0.24, batch loss = 0.21 (34.9 examples/sec; 0.229 sec/batch; 19h:29m:37s remains)
INFO - root - 2017-12-15 07:42:50.146076: step 26080, loss = 0.23, batch loss = 0.20 (36.4 examples/sec; 0.220 sec/batch; 18h:41m:54s remains)
INFO - root - 2017-12-15 07:42:52.436612: step 26090, loss = 0.33, batch loss = 0.30 (35.7 examples/sec; 0.224 sec/batch; 19h:04m:37s remains)
INFO - root - 2017-12-15 07:42:54.809744: step 26100, loss = 0.21, batch loss = 0.18 (34.2 examples/sec; 0.234 sec/batch; 19h:53m:36s remains)
2017-12-15 07:42:55.203573: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1130204 -3.3817792 -3.7493515 -4.3493814 -5.4047642 -6.3521576 -6.5458508 -4.9453373 -3.0538678 -1.1676875 0.07119441 -0.11480212 -0.38458085 -0.42126095 -1.3678734][-1.8866959 -2.5813422 -2.9111137 -3.5199218 -4.7166386 -5.9038687 -6.2070074 -4.7829504 -3.0814724 -1.1913612 0.39901614 0.48290491 0.24189353 0.16584969 -0.90469134][-3.0995975 -2.1331522 -2.582123 -3.177062 -4.2190704 -5.3567386 -5.5764132 -4.3007593 -2.8589635 -1.3493251 0.25572371 0.47866607 0.48312902 0.37213826 -0.72595251][-4.166934 -2.2003186 -2.6673813 -3.0362859 -3.8748 -5.0750771 -5.2512026 -4.1051903 -2.9434536 -1.7194767 -0.36966813 -0.18729687 -0.1128757 -0.43306828 -1.4779677][-4.2800808 -1.6527476 -2.00746 -2.1753244 -2.6613991 -4.0866241 -4.49898 -3.6229019 -2.8455787 -2.10278 -1.0830041 -0.66460466 -0.36246645 -0.88922226 -2.0150025][-3.7217464 -0.888217 -1.066851 -0.9307282 -1.1498508 -2.6545305 -3.0543959 -2.5127199 -2.1424017 -1.6982648 -0.73401761 -0.14356542 0.19137311 -0.4365828 -1.8044386][-2.4469597 0.034192562 0.068346262 0.50862551 0.44869685 -1.1111735 -1.7163692 -1.8106532 -2.0205441 -1.6867278 -0.50669813 0.47351384 1.0119231 0.31686044 -1.1818246][-1.1095338 1.356878 1.2689824 1.6306808 1.375093 -0.20182419 -1.0020239 -1.6950612 -2.4170125 -2.0839908 -0.51936471 0.8409586 1.6999595 1.2010639 -0.34758806][-0.52509367 1.6748273 1.5500534 1.8203137 1.3625326 0.0046703815 -0.74639082 -1.880634 -2.8800983 -2.3038056 -0.38648379 0.93794131 1.8381665 1.5530446 0.1316545][-1.1585137 0.94214272 1.1237831 1.6658249 1.1316454 0.027107 -0.78760839 -2.1923885 -3.33364 -2.5957718 -0.572528 0.69754791 1.7343554 1.5052564 0.02285409][-2.2922215 -0.044032335 0.663455 1.5824397 1.2857585 0.38694692 -0.63592923 -2.3170395 -3.3868537 -2.5900116 -0.8142786 0.083434105 0.964664 0.75705624 -0.76498842][-3.881722 -1.4255595 -0.1177187 1.2028985 1.3846438 0.79961395 -0.36327279 -2.1168401 -3.07183 -2.4569294 -1.0939249 -0.47235763 0.20595312 -0.18209314 -1.8119885][-5.8152933 -3.124907 -1.4118506 0.27841043 0.918854 0.47357178 -0.87642384 -2.4934549 -3.2386692 -2.889739 -1.9831866 -1.5952773 -1.2462007 -1.8886132 -3.5515819][-7.7166319 -4.9030132 -3.0735369 -1.4242077 -0.81246579 -1.4136914 -2.8726544 -4.2111287 -4.6372643 -4.446034 -3.9016118 -3.7300653 -3.6321912 -4.2837515 -5.6345153][-9.1891928 -6.2327905 -4.6751838 -3.4491718 -3.1914215 -4.0015221 -5.2604852 -6.008769 -6.0702496 -5.8956175 -5.5326614 -5.4568949 -5.5176773 -6.0252633 -6.9498806]]...]
INFO - root - 2017-12-15 07:42:57.541252: step 26110, loss = 0.20, batch loss = 0.16 (33.7 examples/sec; 0.237 sec/batch; 20h:11m:00s remains)
INFO - root - 2017-12-15 07:42:59.839759: step 26120, loss = 0.18, batch loss = 0.14 (35.2 examples/sec; 0.227 sec/batch; 19h:19m:06s remains)
INFO - root - 2017-12-15 07:43:02.138134: step 26130, loss = 0.21, batch loss = 0.18 (34.2 examples/sec; 0.234 sec/batch; 19h:52m:45s remains)
INFO - root - 2017-12-15 07:43:04.422642: step 26140, loss = 0.21, batch loss = 0.17 (34.9 examples/sec; 0.230 sec/batch; 19h:32m:02s remains)
INFO - root - 2017-12-15 07:43:06.709547: step 26150, loss = 0.22, batch loss = 0.19 (33.6 examples/sec; 0.238 sec/batch; 20h:14m:18s remains)
INFO - root - 2017-12-15 07:43:09.025680: step 26160, loss = 0.22, batch loss = 0.19 (34.1 examples/sec; 0.235 sec/batch; 19h:59m:24s remains)
INFO - root - 2017-12-15 07:43:11.325514: step 26170, loss = 0.24, batch loss = 0.21 (35.0 examples/sec; 0.229 sec/batch; 19h:27m:35s remains)
INFO - root - 2017-12-15 07:43:13.631452: step 26180, loss = 0.19, batch loss = 0.15 (34.1 examples/sec; 0.235 sec/batch; 19h:58m:39s remains)
INFO - root - 2017-12-15 07:43:15.902471: step 26190, loss = 0.22, batch loss = 0.18 (34.5 examples/sec; 0.232 sec/batch; 19h:44m:31s remains)
INFO - root - 2017-12-15 07:43:18.197151: step 26200, loss = 0.30, batch loss = 0.27 (35.4 examples/sec; 0.226 sec/batch; 19h:13m:53s remains)
2017-12-15 07:43:18.558966: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5274055 -5.3949814 -5.29418 -5.2723336 -5.2767239 -5.1795583 -5.2259312 -5.3863263 -5.4879351 -5.4528317 -5.2159653 -5.07001 -5.0662537 -5.2087727 -5.3891387][-4.9594908 -6.8663654 -6.7440653 -6.7405596 -6.6275606 -6.3626575 -6.3409348 -6.5131235 -6.6741352 -6.9168682 -6.8877211 -6.7340736 -6.5611172 -6.5415773 -6.5987997][-7.51011 -8.4046488 -8.2199183 -8.0819321 -7.5444469 -6.8493834 -6.5933628 -6.637475 -6.8193703 -7.4559555 -7.7241564 -7.6153193 -7.3950815 -7.4194651 -7.3531542][-9.0907621 -9.3810339 -9.04659 -8.6811295 -7.5418577 -6.2175136 -5.5559835 -5.4029303 -5.773035 -7.0335159 -7.7276564 -7.6895037 -7.5059614 -7.7150383 -7.6453533][-9.803936 -9.409584 -8.7516737 -7.992568 -6.1508074 -3.987973 -2.652668 -2.2741554 -3.0682435 -5.2515173 -6.7405186 -7.0665026 -7.1685934 -7.5996275 -7.6330118][-9.7210274 -9.2689247 -8.2159767 -6.9506092 -4.4516268 -1.3333158 0.99290085 1.7118015 0.38348055 -2.7839694 -5.168592 -6.0849133 -6.6554127 -7.2849312 -7.4069004][-9.4081345 -9.3752222 -7.8895645 -5.9863658 -2.7918038 1.3201506 4.824501 6.1204596 4.4745693 0.52288556 -2.8081768 -4.5341535 -5.6651068 -6.4585953 -6.7576275][-9.780118 -9.4944239 -7.7943163 -5.5001397 -1.8946478 2.8769677 7.2526121 9.03569 7.3035049 3.0186188 -1.0733256 -3.4988382 -4.976944 -5.8286042 -6.3346586][-9.6344471 -9.4106693 -8.0247355 -5.972198 -2.6048889 2.0373533 6.5537567 8.57979 7.0701122 3.0981209 -1.0419436 -3.6710272 -5.062799 -5.9675541 -6.5490885][-9.4519472 -9.2752752 -8.3009634 -6.8407593 -4.2699685 -0.47398818 3.5676658 5.7908726 5.06437 2.0357964 -1.7651036 -4.4542322 -5.6653347 -6.5585251 -7.1622829][-9.501152 -9.3490648 -8.6657829 -7.7434406 -6.111743 -3.434999 -0.34720004 1.7359309 1.6275413 -0.40746176 -3.5817356 -6.0374556 -7.0028467 -7.6637087 -8.1654177][-9.5579 -9.6420908 -9.4279556 -8.9485788 -7.9740076 -6.2125759 -4.1018152 -2.5593836 -2.5203531 -3.7889063 -6.0579758 -7.9717321 -8.5849218 -8.8363161 -8.9364538][-9.3040733 -9.7317505 -10.144085 -10.087503 -9.4413986 -8.13452 -6.573863 -5.4443474 -5.3708463 -6.2358093 -7.7223463 -8.9693613 -9.2021246 -9.0010624 -8.67692][-8.3516474 -8.7001715 -9.40181 -9.6385584 -9.3353157 -8.4148951 -7.2832632 -6.4387894 -6.4480238 -7.097682 -7.982069 -8.5999708 -8.5766258 -8.1576862 -7.7216921][-7.2258167 -7.097683 -7.6803913 -8.0009995 -8.0346479 -7.70644 -7.1894503 -6.7674437 -6.7978697 -7.1038208 -7.3955517 -7.566411 -7.4484367 -7.1646004 -6.7884245]]...]
INFO - root - 2017-12-15 07:43:20.834038: step 26210, loss = 0.17, batch loss = 0.14 (34.5 examples/sec; 0.232 sec/batch; 19h:43m:48s remains)
INFO - root - 2017-12-15 07:43:23.114208: step 26220, loss = 0.28, batch loss = 0.25 (32.3 examples/sec; 0.248 sec/batch; 21h:03m:44s remains)
INFO - root - 2017-12-15 07:43:25.450810: step 26230, loss = 0.20, batch loss = 0.17 (34.7 examples/sec; 0.231 sec/batch; 19h:38m:22s remains)
INFO - root - 2017-12-15 07:43:27.689445: step 26240, loss = 0.26, batch loss = 0.22 (36.1 examples/sec; 0.222 sec/batch; 18h:52m:35s remains)
INFO - root - 2017-12-15 07:43:29.955832: step 26250, loss = 0.22, batch loss = 0.19 (35.4 examples/sec; 0.226 sec/batch; 19h:12m:35s remains)
INFO - root - 2017-12-15 07:43:32.222279: step 26260, loss = 0.19, batch loss = 0.16 (36.2 examples/sec; 0.221 sec/batch; 18h:46m:48s remains)
INFO - root - 2017-12-15 07:43:34.496980: step 26270, loss = 0.28, batch loss = 0.24 (35.1 examples/sec; 0.228 sec/batch; 19h:24m:46s remains)
INFO - root - 2017-12-15 07:43:36.772285: step 26280, loss = 0.21, batch loss = 0.17 (35.4 examples/sec; 0.226 sec/batch; 19h:12m:10s remains)
INFO - root - 2017-12-15 07:43:39.062309: step 26290, loss = 0.31, batch loss = 0.28 (33.3 examples/sec; 0.240 sec/batch; 20h:27m:07s remains)
INFO - root - 2017-12-15 07:43:41.336496: step 26300, loss = 0.24, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 19h:02m:44s remains)
2017-12-15 07:43:41.675312: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4584887 -3.8320403 -4.93746 -5.5302014 -5.9386282 -6.4078379 -6.0867758 -5.3431454 -5.086988 -4.7375865 -4.7937679 -5.3919063 -6.2369814 -6.0534277 -4.99597][-1.4879194 -3.0008414 -4.4231906 -5.5237942 -6.1225729 -6.4226322 -5.9809241 -5.2822227 -4.9903479 -4.7545304 -5.0186195 -5.8995667 -6.9206657 -6.8157072 -5.66737][-2.0521374 -2.6618283 -4.4711323 -5.97398 -6.5957766 -6.4580946 -5.8167038 -5.1313505 -4.6434565 -4.4085889 -4.7556243 -5.8129997 -7.074502 -7.35067 -6.5274115][-3.687108 -3.489408 -5.3688731 -6.8753653 -7.23713 -6.4714775 -5.4043007 -4.3798361 -3.5114369 -3.3428063 -3.9466462 -5.2982645 -6.8458824 -7.6189594 -7.3206549][-5.86025 -4.9022417 -6.4748268 -7.6480646 -7.4935274 -6.13829 -4.3805003 -2.7268379 -1.4917786 -1.4426074 -2.4296522 -4.0694022 -5.8538876 -7.1668463 -7.4693718][-6.6912212 -5.862586 -6.7962837 -7.3269215 -6.4165611 -4.3342304 -1.767082 0.61463428 2.1674616 1.8685248 0.3454957 -1.7158284 -3.8631995 -5.7566414 -6.68058][-6.4815359 -6.0649872 -6.4443827 -6.3707314 -4.9411049 -2.1890869 0.9994092 4.1236877 5.9811106 5.2745056 3.0702555 0.43119597 -2.0052872 -4.12498 -5.2887068][-6.2693138 -5.7303343 -5.7796559 -5.3465257 -3.7883072 -0.85958827 2.5345323 6.0216627 7.9266605 7.0116434 4.378212 1.4164264 -1.1003004 -3.046591 -4.14976][-5.2682233 -4.9195414 -4.89678 -4.3434286 -3.045013 -0.42178798 2.7440436 6.0855684 7.8538485 6.8876123 4.2325096 1.3675318 -1.0272707 -2.6496189 -3.5358233][-4.3125458 -4.3132381 -4.497611 -3.9977727 -3.1557631 -1.0905 1.6983402 4.7053165 6.2153082 5.2265444 2.688 0.19360113 -1.7372347 -2.929141 -3.5295615][-3.8088851 -4.0804911 -4.628191 -4.2982025 -3.9580855 -2.598716 -0.3392365 2.1574667 3.2878807 2.4348037 0.43611979 -1.3879243 -2.756114 -3.5116563 -3.8882165][-3.8017378 -4.1542339 -4.9835572 -4.8243885 -4.8599691 -4.1288691 -2.3217757 -0.30751669 0.63261104 0.11566949 -1.127933 -2.3997276 -3.4180655 -3.8875446 -4.0249276][-4.4004669 -4.6999254 -5.6286259 -5.5251255 -5.7053514 -5.2550855 -3.7522302 -2.1474304 -1.2162292 -1.2898732 -1.9064175 -2.8799691 -3.7156477 -4.0218554 -4.1406407][-5.1336031 -5.2697182 -6.2146883 -6.1835542 -6.4052296 -6.0137596 -4.6425867 -3.3658257 -2.2097406 -1.6506124 -1.7973049 -2.6086605 -3.4191942 -4.0070019 -4.5932455][-5.7513165 -5.7400484 -6.673317 -6.7274132 -6.8409262 -6.327899 -5.0822158 -3.9981265 -2.5632868 -1.6185775 -1.6828415 -2.5213091 -3.3845153 -4.384347 -5.3209667]]...]
INFO - root - 2017-12-15 07:43:44.008043: step 26310, loss = 0.31, batch loss = 0.28 (33.6 examples/sec; 0.238 sec/batch; 20h:16m:32s remains)
INFO - root - 2017-12-15 07:43:46.275692: step 26320, loss = 0.22, batch loss = 0.19 (36.3 examples/sec; 0.220 sec/batch; 18h:44m:27s remains)
INFO - root - 2017-12-15 07:43:48.541521: step 26330, loss = 0.17, batch loss = 0.14 (34.4 examples/sec; 0.232 sec/batch; 19h:45m:45s remains)
INFO - root - 2017-12-15 07:43:50.777341: step 26340, loss = 0.22, batch loss = 0.19 (36.1 examples/sec; 0.221 sec/batch; 18h:49m:48s remains)
INFO - root - 2017-12-15 07:43:53.025514: step 26350, loss = 0.20, batch loss = 0.17 (35.0 examples/sec; 0.229 sec/batch; 19h:27m:38s remains)
INFO - root - 2017-12-15 07:43:55.369451: step 26360, loss = 0.23, batch loss = 0.19 (34.9 examples/sec; 0.229 sec/batch; 19h:29m:26s remains)
INFO - root - 2017-12-15 07:43:57.640617: step 26370, loss = 0.17, batch loss = 0.13 (34.8 examples/sec; 0.230 sec/batch; 19h:31m:44s remains)
INFO - root - 2017-12-15 07:44:00.031916: step 26380, loss = 0.25, batch loss = 0.21 (33.7 examples/sec; 0.237 sec/batch; 20h:10m:04s remains)
INFO - root - 2017-12-15 07:44:02.297383: step 26390, loss = 0.23, batch loss = 0.20 (36.2 examples/sec; 0.221 sec/batch; 18h:46m:41s remains)
INFO - root - 2017-12-15 07:44:04.614733: step 26400, loss = 0.26, batch loss = 0.23 (33.9 examples/sec; 0.236 sec/batch; 20h:03m:33s remains)
2017-12-15 07:44:04.988405: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4914804 -6.4092522 -6.4216757 -6.0241375 -5.4914856 -5.2010517 -5.3784733 -5.7071257 -5.9733543 -6.0027027 -5.7247815 -5.212677 -4.5830116 -4.1348529 -4.0206981][-4.9901247 -6.8198862 -6.5735803 -6.04156 -5.5150738 -5.1141205 -5.1125321 -5.4293966 -6.119132 -6.7085924 -6.85731 -6.6565981 -6.1194973 -5.7133303 -5.425127][-5.9425526 -6.2824731 -5.6585793 -4.9690175 -4.4440823 -3.8705864 -3.6087618 -3.8101301 -4.9328413 -6.2365336 -7.1060686 -7.5035043 -7.2217193 -6.8787122 -6.3681273][-6.3395376 -5.4311261 -4.4634361 -3.6356239 -2.8816054 -1.8949577 -1.1509084 -1.0043073 -2.3977113 -4.4619207 -6.2703571 -7.4731236 -7.70287 -7.6589537 -7.103406][-6.7166595 -4.87403 -3.7321205 -2.7921369 -1.5267904 0.12068439 1.544915 2.2967772 0.92883015 -1.5946864 -4.1885786 -6.2142181 -7.1619043 -7.6904216 -7.3335967][-6.792552 -4.8361812 -3.6464152 -2.5041244 -0.44331288 2.0321202 4.1188612 5.3024716 3.9671721 1.2314861 -1.694412 -4.2210107 -5.7712259 -6.8758926 -6.836401][-6.5963693 -5.3371482 -4.1959004 -2.8339889 0.058448792 3.3935442 6.0961847 7.5419941 6.1035347 3.0181084 -0.064541817 -2.6950018 -4.4211326 -5.74778 -6.0251675][-6.9557905 -5.9264688 -4.9449062 -3.6076128 -0.3892889 3.4263058 6.5325127 8.13642 6.6950369 3.5723348 0.64675689 -1.8868287 -3.6375871 -4.8910851 -5.2824078][-7.1074605 -6.4108634 -5.7778721 -4.8025079 -1.9679222 1.6706321 4.7801533 6.3304749 5.1821651 2.5487452 0.2563529 -1.8402389 -3.3507996 -4.3456755 -4.7770529][-7.0419431 -6.648766 -6.5335374 -6.2282839 -4.2237725 -1.2253033 1.4679179 2.7665524 2.0648241 0.22734571 -1.3431325 -2.7946136 -3.725059 -4.2171154 -4.5852356][-6.7686858 -6.4594674 -6.7613177 -7.1544118 -6.1701956 -4.0897131 -2.0875604 -1.1549938 -1.4369996 -2.4448338 -3.3996711 -4.3129272 -4.5980263 -4.4982548 -4.6646867][-6.3293815 -5.9199886 -6.4136028 -7.2453675 -7.0777621 -5.92135 -4.7312632 -4.2552247 -4.294312 -4.6712828 -5.1907883 -5.6412973 -5.4117923 -4.9164963 -4.8733306][-5.9280958 -5.3452454 -5.823802 -6.74428 -6.9369407 -6.4297175 -5.9498186 -5.8297362 -5.8412561 -5.9562287 -6.2556744 -6.3763294 -5.8443832 -5.2380147 -5.1416321][-5.5046406 -4.8165288 -5.2141762 -5.9617343 -6.1577148 -6.0150547 -5.9797068 -6.0927453 -6.2230968 -6.3310242 -6.5085678 -6.4853683 -5.9339972 -5.38139 -5.2752895][-4.9496584 -4.2300186 -4.5094385 -4.9653759 -5.0356927 -5.0489283 -5.1824603 -5.4015779 -5.6744308 -5.9258623 -6.0633364 -5.9348316 -5.53405 -5.1548204 -5.06499]]...]
INFO - root - 2017-12-15 07:44:07.261445: step 26410, loss = 0.36, batch loss = 0.33 (35.0 examples/sec; 0.229 sec/batch; 19h:26m:51s remains)
INFO - root - 2017-12-15 07:44:09.588805: step 26420, loss = 0.21, batch loss = 0.18 (34.5 examples/sec; 0.232 sec/batch; 19h:42m:07s remains)
INFO - root - 2017-12-15 07:44:11.902566: step 26430, loss = 0.19, batch loss = 0.16 (33.7 examples/sec; 0.237 sec/batch; 20h:09m:31s remains)
INFO - root - 2017-12-15 07:44:14.202080: step 26440, loss = 0.21, batch loss = 0.18 (35.0 examples/sec; 0.229 sec/batch; 19h:26m:13s remains)
INFO - root - 2017-12-15 07:44:16.497064: step 26450, loss = 0.17, batch loss = 0.14 (36.2 examples/sec; 0.221 sec/batch; 18h:46m:14s remains)
INFO - root - 2017-12-15 07:44:18.752415: step 26460, loss = 0.19, batch loss = 0.16 (36.1 examples/sec; 0.221 sec/batch; 18h:49m:24s remains)
INFO - root - 2017-12-15 07:44:21.056797: step 26470, loss = 0.20, batch loss = 0.17 (32.8 examples/sec; 0.244 sec/batch; 20h:43m:50s remains)
INFO - root - 2017-12-15 07:44:23.344136: step 26480, loss = 0.21, batch loss = 0.17 (34.5 examples/sec; 0.232 sec/batch; 19h:41m:03s remains)
INFO - root - 2017-12-15 07:44:25.618876: step 26490, loss = 0.22, batch loss = 0.18 (35.2 examples/sec; 0.228 sec/batch; 19h:20m:43s remains)
INFO - root - 2017-12-15 07:44:27.908001: step 26500, loss = 0.22, batch loss = 0.18 (36.0 examples/sec; 0.222 sec/batch; 18h:53m:54s remains)
2017-12-15 07:44:28.238635: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6192422 -3.7774315 -3.7686973 -3.2351487 -2.9848928 -3.2825277 -4.2214165 -5.1577835 -5.7291136 -6.336009 -6.2182951 -5.8337331 -5.53454 -4.6536541 -3.6832039][-2.5145068 -4.0918818 -3.7532709 -2.9253032 -2.7877164 -3.1401825 -4.0510592 -4.8919268 -5.4189405 -6.2956152 -6.446393 -6.2244844 -6.0792136 -5.3501663 -4.5252581][-3.1654723 -3.7432 -3.0138481 -1.9505206 -1.712702 -1.9113419 -2.6110454 -3.321455 -3.9233179 -5.0185757 -5.41165 -5.4827852 -5.5787654 -4.9275022 -4.1123137][-4.147851 -3.7896585 -2.6944931 -1.3839926 -0.78026438 -0.47074425 -0.79417586 -1.3816769 -1.979676 -3.1386673 -3.78315 -4.2498951 -4.6647887 -4.129518 -3.4570968][-4.7561989 -3.4973466 -2.2170289 -0.7797488 0.30440235 1.2541447 1.4360812 1.1059792 0.61957812 -0.642822 -1.8038181 -3.065062 -3.9579306 -3.6369088 -3.1157162][-4.6650524 -3.1100917 -1.9657056 -0.69742453 0.59798479 2.0034125 2.7859643 2.9748509 2.6591241 1.2892795 -0.24322391 -1.8763812 -2.8955231 -2.8621125 -2.428606][-4.8299942 -3.4705338 -2.3419912 -1.0544215 0.57994151 2.4367645 3.8679702 4.6306534 4.4307775 2.9048245 1.1975157 -0.52364159 -1.546922 -1.9284526 -1.6897925][-4.4478278 -3.1335785 -2.2230394 -1.224746 0.29418278 2.1736882 3.9169309 4.8871527 4.5654821 3.017101 1.4472067 -0.043319702 -0.96587181 -1.5058317 -1.2761977][-3.6559138 -2.6093161 -2.1223867 -1.7236638 -0.84009373 0.522022 1.9945519 2.9331686 2.6894076 1.6099949 0.5644455 -0.5309087 -1.4355665 -2.058918 -1.7892319][-3.6546621 -2.6125965 -2.308744 -2.2770641 -1.8792784 -0.95361912 0.14958835 0.95037031 0.7690227 -0.054380655 -0.84556913 -1.7584857 -2.6890519 -3.2614923 -3.1265206][-3.8640575 -2.6056733 -2.3302119 -2.385324 -2.2014563 -1.5435047 -0.8161025 -0.18036723 -0.33803153 -1.0531862 -1.7189636 -2.5532782 -3.5737476 -4.1273284 -4.2559023][-4.1670237 -2.6936202 -2.2140379 -2.0963488 -1.9718001 -1.5632708 -1.1925976 -0.61085689 -0.67107034 -1.2475476 -1.7850091 -2.5349481 -3.5087941 -4.1669393 -4.5809817][-4.1296015 -2.2674592 -1.4249847 -0.97379208 -0.753608 -0.58964181 -0.63955057 -0.30809832 -0.52641022 -1.1340311 -1.5571578 -2.0732903 -2.8485036 -3.6360655 -4.2835155][-3.6908641 -1.7834138 -0.81818044 -0.097397089 0.40660405 0.56600547 0.32030034 0.36647916 -0.035635233 -0.5934602 -0.79521239 -0.96982419 -1.492733 -2.3000574 -3.0885265][-3.4874454 -1.7819806 -0.91145313 -0.16632628 0.47427273 0.59754062 0.12017965 -0.22830582 -0.84461856 -1.2986332 -1.3162466 -1.1755915 -1.2543681 -1.7378674 -2.3295608]]...]
INFO - root - 2017-12-15 07:44:30.508789: step 26510, loss = 0.26, batch loss = 0.23 (35.8 examples/sec; 0.223 sec/batch; 18h:59m:38s remains)
INFO - root - 2017-12-15 07:44:32.786862: step 26520, loss = 0.28, batch loss = 0.25 (33.9 examples/sec; 0.236 sec/batch; 20h:03m:36s remains)
INFO - root - 2017-12-15 07:44:35.048649: step 26530, loss = 0.29, batch loss = 0.26 (35.2 examples/sec; 0.227 sec/batch; 19h:19m:51s remains)
INFO - root - 2017-12-15 07:44:37.334525: step 26540, loss = 0.20, batch loss = 0.16 (35.3 examples/sec; 0.227 sec/batch; 19h:15m:50s remains)
INFO - root - 2017-12-15 07:44:39.653118: step 26550, loss = 0.25, batch loss = 0.21 (36.8 examples/sec; 0.217 sec/batch; 18h:27m:21s remains)
INFO - root - 2017-12-15 07:44:41.923574: step 26560, loss = 0.17, batch loss = 0.13 (36.1 examples/sec; 0.222 sec/batch; 18h:50m:45s remains)
INFO - root - 2017-12-15 07:44:44.191631: step 26570, loss = 0.18, batch loss = 0.15 (34.4 examples/sec; 0.233 sec/batch; 19h:46m:02s remains)
INFO - root - 2017-12-15 07:44:46.515469: step 26580, loss = 0.22, batch loss = 0.18 (36.2 examples/sec; 0.221 sec/batch; 18h:46m:05s remains)
INFO - root - 2017-12-15 07:44:48.789361: step 26590, loss = 0.15, batch loss = 0.12 (36.2 examples/sec; 0.221 sec/batch; 18h:46m:42s remains)
INFO - root - 2017-12-15 07:44:51.079760: step 26600, loss = 0.25, batch loss = 0.22 (34.5 examples/sec; 0.232 sec/batch; 19h:41m:29s remains)
2017-12-15 07:44:51.419079: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2051826 -5.640173 -5.4395714 -5.3948526 -5.5884953 -5.5894146 -5.2423382 -4.8036814 -4.3554955 -4.1239433 -4.283267 -4.7030878 -5.1840277 -5.7570705 -6.0169029][-5.6599493 -6.4004316 -5.8990064 -5.5490904 -5.6750226 -5.8243942 -5.6604176 -5.206429 -4.6839862 -4.3974409 -4.3874841 -4.6844788 -5.116888 -5.5957184 -5.8488154][-7.3988686 -7.4576712 -6.8048773 -6.2260408 -6.0970621 -6.1426306 -6.0354552 -5.6716366 -5.3302503 -4.9019952 -4.6768742 -4.6947947 -4.8318539 -5.2939034 -5.6704531][-7.1536875 -7.0459967 -6.0231338 -5.0513144 -4.3726549 -3.9789176 -3.7833145 -3.6913595 -3.8200445 -3.6622541 -3.4142528 -3.2386308 -3.0871949 -3.5997312 -4.4356856][-7.0909729 -6.3774137 -4.9484344 -3.601131 -2.3232734 -1.39849 -1.0434921 -1.3471903 -2.0996134 -2.3986595 -2.1855624 -1.6487997 -1.1687933 -1.4517033 -2.5738347][-7.1490631 -6.0750275 -4.4620328 -2.787771 -0.936025 0.67548203 1.4873633 1.1011508 -0.17198277 -0.86993504 -0.85424936 0.0048441887 0.94172096 0.983603 -0.30847597][-6.2465816 -5.2983251 -3.543014 -1.5784054 0.79895782 3.1450398 4.5048494 4.16453 2.6660497 1.5585334 1.181432 1.7790406 2.7202981 3.0089467 1.6674361][-5.5201545 -4.3983 -2.5888944 -0.57281613 1.812803 4.2285032 5.8103914 5.6446543 4.2566938 3.0257981 2.3575828 2.6216786 3.2905285 3.7265146 2.5133049][-5.7948227 -4.878438 -3.3810353 -1.5861592 0.35943651 2.3295791 3.8056982 4.0066013 3.0938017 1.991986 1.3102727 1.3972378 1.8588173 2.446152 1.5734015][-6.4204969 -5.8710561 -4.8044739 -3.2763209 -1.6906592 -0.084003925 1.1396499 1.4551344 0.92309904 0.081929445 -0.45439959 -0.520496 -0.27931774 0.27042985 -0.31503427][-6.7485485 -6.4203854 -5.8395605 -4.7395115 -3.56843 -2.5660162 -1.9924312 -1.9010441 -2.2598414 -2.7022772 -2.9780617 -3.0925894 -3.0307662 -2.5377476 -2.765995][-6.9437246 -6.7468615 -6.6284113 -5.9574366 -5.2865238 -4.8713641 -4.8430262 -4.9657598 -5.1210394 -5.3082104 -5.4337721 -5.5975018 -5.6562347 -5.2725387 -5.2400455][-6.8685703 -6.4300909 -6.3199472 -5.7071505 -5.2563739 -5.2138834 -5.5248146 -5.8015537 -5.9916587 -6.1715326 -6.3134775 -6.5143862 -6.6588564 -6.5095692 -6.3894386][-6.6896372 -5.8117261 -5.4134145 -4.8542504 -4.6836181 -4.9903078 -5.5394487 -5.9195638 -6.1631823 -6.3212347 -6.4732742 -6.7152147 -6.9761467 -6.9501629 -6.7208166][-7.0628748 -6.0970306 -5.51356 -4.9858084 -4.9564848 -5.2693157 -5.6901808 -5.9848738 -6.1238403 -6.2020826 -6.3709087 -6.5968542 -6.7565365 -6.66337 -6.3745022]]...]
INFO - root - 2017-12-15 07:44:53.651520: step 26610, loss = 0.33, batch loss = 0.29 (35.9 examples/sec; 0.223 sec/batch; 18h:54m:51s remains)
INFO - root - 2017-12-15 07:44:55.917200: step 26620, loss = 0.15, batch loss = 0.12 (35.3 examples/sec; 0.227 sec/batch; 19h:14m:45s remains)
INFO - root - 2017-12-15 07:44:58.206144: step 26630, loss = 0.22, batch loss = 0.19 (34.9 examples/sec; 0.229 sec/batch; 19h:26m:59s remains)
INFO - root - 2017-12-15 07:45:00.472679: step 26640, loss = 0.24, batch loss = 0.20 (36.2 examples/sec; 0.221 sec/batch; 18h:45m:25s remains)
INFO - root - 2017-12-15 07:45:02.715908: step 26650, loss = 0.19, batch loss = 0.15 (36.0 examples/sec; 0.222 sec/batch; 18h:53m:20s remains)
INFO - root - 2017-12-15 07:45:05.074146: step 26660, loss = 0.24, batch loss = 0.20 (33.5 examples/sec; 0.239 sec/batch; 20h:16m:56s remains)
INFO - root - 2017-12-15 07:45:07.325539: step 26670, loss = 0.40, batch loss = 0.37 (34.2 examples/sec; 0.234 sec/batch; 19h:53m:49s remains)
INFO - root - 2017-12-15 07:45:09.681701: step 26680, loss = 0.18, batch loss = 0.15 (33.6 examples/sec; 0.238 sec/batch; 20h:15m:08s remains)
INFO - root - 2017-12-15 07:45:11.987449: step 26690, loss = 0.19, batch loss = 0.16 (35.6 examples/sec; 0.225 sec/batch; 19h:06m:23s remains)
INFO - root - 2017-12-15 07:45:14.245327: step 26700, loss = 0.22, batch loss = 0.19 (35.1 examples/sec; 0.228 sec/batch; 19h:20m:38s remains)
2017-12-15 07:45:14.571470: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7667632 -8.548748 -8.9106121 -9.1474428 -9.0639029 -8.8663483 -8.6154327 -8.6212864 -8.8033695 -8.7083874 -8.477066 -8.535327 -8.4724541 -8.0377388 -7.7669206][-9.2577982 -10.016191 -10.408618 -10.603531 -10.398216 -10.012087 -9.44912 -9.2921638 -9.4861641 -9.3855839 -9.0387 -9.0760536 -9.0298414 -8.5230875 -8.3217735][-10.212789 -10.110173 -10.5002 -10.653786 -10.312976 -9.7216434 -8.8565311 -8.4154377 -8.4644585 -8.3144236 -7.93919 -8.15703 -8.3235168 -8.0393238 -8.2681923][-11.563505 -10.488749 -10.712444 -10.634548 -9.9548435 -8.8897047 -7.5538411 -6.6591072 -6.464736 -6.3933439 -6.3100443 -6.9953957 -7.5640116 -7.6701527 -8.4278212][-12.05932 -9.8285885 -9.7099457 -9.3651791 -8.32181 -6.7734389 -5.1567097 -4.1118746 -3.993259 -4.2788019 -4.6553 -5.7989616 -6.6119704 -6.9020729 -8.0432863][-10.784063 -8.0928612 -7.6828861 -7.109972 -5.7326155 -3.652565 -1.8493218 -0.789778 -1.0455483 -1.9148469 -2.8045692 -4.3900647 -5.34096 -5.6358852 -6.9129877][-8.2195415 -5.8159137 -5.2078104 -4.4511509 -2.7260504 0.085956812 2.4514775 3.676836 2.9088035 1.0817251 -0.75371838 -2.9184568 -4.0820637 -4.3436193 -5.4679527][-6.0829043 -3.5645139 -2.8388071 -1.98678 -0.09715724 3.1450448 5.9596443 7.3007364 5.9133291 3.0753846 0.43865418 -2.0546868 -3.1605399 -3.200634 -3.9313903][-4.6414633 -2.0731635 -1.2343235 -0.29296577 1.5219953 4.6038761 7.2312088 8.2705059 6.4346132 3.1676989 0.38320541 -1.9682509 -2.7082698 -2.4783263 -2.9232521][-4.5632844 -2.0185072 -1.0199513 0.078410864 1.5595326 3.9322433 5.6810331 5.9167261 3.9709516 1.0991127 -1.1115046 -2.9193304 -3.2209411 -2.7822239 -3.0181563][-6.1396427 -3.7037742 -2.4858124 -1.24038 -0.074121237 1.4340351 1.9844475 1.2697833 -0.53214931 -2.6856165 -4.18911 -5.4220085 -5.0793433 -4.2449307 -4.1491451][-7.9015136 -5.49193 -4.2036428 -2.9136689 -1.8790829 -0.93235421 -1.3519379 -2.6842973 -4.3325906 -5.88266 -6.7880807 -7.4782982 -6.4984407 -5.40635 -5.3399243][-8.9710369 -6.6301394 -5.5081625 -4.3731184 -3.4772725 -2.9862385 -3.9294207 -5.3984628 -6.6295004 -7.4522982 -7.9101305 -8.1403923 -6.9101195 -6.005435 -6.378387][-9.8897142 -7.796555 -7.1058149 -6.3413496 -5.6358476 -5.5266056 -6.7106848 -7.9342833 -8.496273 -8.5596151 -8.6076117 -8.5511379 -7.3126478 -6.6127071 -7.2086782][-9.8042049 -8.1015148 -7.8184261 -7.3856311 -6.9348536 -7.261013 -8.5930939 -9.4489994 -9.5400553 -9.1621075 -8.8734894 -8.4725246 -7.2487612 -6.7392769 -7.3037119]]...]
INFO - root - 2017-12-15 07:45:16.827218: step 26710, loss = 0.24, batch loss = 0.21 (35.3 examples/sec; 0.227 sec/batch; 19h:14m:57s remains)
INFO - root - 2017-12-15 07:45:19.101473: step 26720, loss = 0.22, batch loss = 0.19 (34.4 examples/sec; 0.232 sec/batch; 19h:44m:48s remains)
INFO - root - 2017-12-15 07:45:21.390898: step 26730, loss = 0.24, batch loss = 0.21 (35.0 examples/sec; 0.228 sec/batch; 19h:24m:10s remains)
INFO - root - 2017-12-15 07:45:23.694384: step 26740, loss = 0.23, batch loss = 0.20 (33.6 examples/sec; 0.238 sec/batch; 20h:11m:59s remains)
INFO - root - 2017-12-15 07:45:25.977212: step 26750, loss = 0.23, batch loss = 0.20 (35.7 examples/sec; 0.224 sec/batch; 19h:01m:06s remains)
INFO - root - 2017-12-15 07:45:28.284064: step 26760, loss = 0.21, batch loss = 0.18 (35.6 examples/sec; 0.225 sec/batch; 19h:05m:19s remains)
INFO - root - 2017-12-15 07:45:30.536729: step 26770, loss = 0.24, batch loss = 0.21 (36.1 examples/sec; 0.222 sec/batch; 18h:49m:16s remains)
INFO - root - 2017-12-15 07:45:32.838056: step 26780, loss = 0.24, batch loss = 0.20 (33.6 examples/sec; 0.238 sec/batch; 20h:12m:53s remains)
INFO - root - 2017-12-15 07:45:35.145434: step 26790, loss = 0.18, batch loss = 0.15 (35.2 examples/sec; 0.227 sec/batch; 19h:16m:45s remains)
INFO - root - 2017-12-15 07:45:37.443871: step 26800, loss = 0.37, batch loss = 0.34 (35.3 examples/sec; 0.226 sec/batch; 19h:13m:55s remains)
2017-12-15 07:45:37.829742: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.074692488 -1.0905054 -2.0671804 -2.1037076 -0.8408823 -0.16731238 -1.3570544 -2.7031651 -3.3693805 -4.1729026 -4.0495758 -3.14047 -1.8398075 -0.75873315 0.29394889][-1.2345225 -1.169981 -2.1868906 -2.292912 -1.0002248 -0.24123383 -1.3092439 -2.7165565 -3.5763564 -4.3083105 -4.1074104 -2.9489827 -1.4089606 -0.22441888 0.84368467][-2.7320817 -1.846535 -2.5934849 -2.6660635 -1.3980558 -0.59366286 -1.5023968 -2.8772635 -3.7996564 -4.2200193 -3.8537836 -2.7115538 -1.2776495 -0.18898606 0.87121058][-4.0196328 -2.4102266 -2.605104 -2.5602207 -1.4769788 -0.745206 -1.5452781 -2.9092772 -3.9047465 -4.0287838 -3.5710099 -2.7297559 -1.5388129 -0.4059999 0.74386477][-4.8382893 -2.1371002 -1.6502206 -1.5262871 -0.91092253 -0.48092818 -1.4085386 -2.9795022 -4.2743764 -4.3328743 -3.7330713 -3.2061851 -2.3079181 -1.0229735 0.36010718][-4.740077 -1.0431298 0.040020227 0.19877291 0.30852675 0.29084444 -0.80021 -2.6196308 -4.2601471 -4.5333996 -3.9241881 -3.7570722 -3.3361895 -2.0936131 -0.55553269][-3.7055898 0.45618844 2.094907 2.3548048 2.0320184 1.706913 0.42656064 -1.6441395 -3.6292019 -4.2506356 -3.8293557 -4.1205349 -4.1635604 -3.1489067 -1.7181087][-2.6948659 1.8112023 3.7011178 3.9407547 3.2926433 2.7939975 1.4006219 -0.75467634 -2.9493926 -3.8978095 -3.7812762 -4.3470435 -4.6982355 -4.0884805 -3.16205][-2.8127446 1.7239468 3.8215115 4.0569344 3.2378981 2.4931014 1.0204909 -0.75294971 -2.7422967 -3.8826065 -4.1415396 -5.046834 -5.6882429 -5.3736963 -4.8205924][-4.4652834 -0.24319792 1.8717768 2.0709088 1.2071657 0.30354166 -1.0066338 -2.0900476 -3.5206389 -4.5123796 -4.9387965 -5.9319692 -6.5903196 -6.2997351 -5.9355369][-6.5855713 -2.8729191 -1.0278441 -1.0287389 -1.963611 -2.7924263 -3.7078958 -4.1540041 -4.9984541 -5.7143846 -6.0781918 -6.72892 -6.9723463 -6.4206219 -5.9075947][-8.1488371 -5.1748238 -3.8177886 -4.0844336 -5.107079 -5.9724212 -6.5378542 -6.4290838 -6.5850649 -6.825696 -6.9062748 -7.0465822 -6.7731228 -5.8773708 -5.0721426][-8.915329 -6.6907539 -5.764699 -6.1328573 -7.1125584 -7.8565273 -8.0787172 -7.5621891 -7.1389532 -7.051157 -6.9975939 -6.8240757 -6.1351719 -4.9250059 -3.784735][-8.7280369 -7.2253895 -6.7577791 -7.1161842 -7.8535023 -8.3627758 -8.4773369 -8.0497179 -7.515667 -7.3108358 -7.1088123 -6.4584265 -5.2299604 -3.7229495 -2.4555717][-7.5777836 -6.7298536 -6.7431726 -7.0600348 -7.4992371 -7.9379282 -8.2127552 -7.9275494 -7.3663445 -7.0077162 -6.5461254 -5.615931 -4.2336488 -2.6789827 -1.567844]]...]
INFO - root - 2017-12-15 07:45:40.108858: step 26810, loss = 0.17, batch loss = 0.14 (34.6 examples/sec; 0.231 sec/batch; 19h:38m:32s remains)
INFO - root - 2017-12-15 07:45:42.392657: step 26820, loss = 0.20, batch loss = 0.17 (35.3 examples/sec; 0.227 sec/batch; 19h:15m:57s remains)
INFO - root - 2017-12-15 07:45:44.700451: step 26830, loss = 0.21, batch loss = 0.17 (34.7 examples/sec; 0.231 sec/batch; 19h:34m:45s remains)
INFO - root - 2017-12-15 07:45:46.982779: step 26840, loss = 0.17, batch loss = 0.13 (35.2 examples/sec; 0.227 sec/batch; 19h:17m:38s remains)
INFO - root - 2017-12-15 07:45:49.293508: step 26850, loss = 0.22, batch loss = 0.18 (35.1 examples/sec; 0.228 sec/batch; 19h:21m:23s remains)
INFO - root - 2017-12-15 07:45:51.588682: step 26860, loss = 0.21, batch loss = 0.18 (35.2 examples/sec; 0.227 sec/batch; 19h:16m:09s remains)
INFO - root - 2017-12-15 07:45:53.833141: step 26870, loss = 0.23, batch loss = 0.19 (35.3 examples/sec; 0.226 sec/batch; 19h:13m:02s remains)
INFO - root - 2017-12-15 07:45:56.112225: step 26880, loss = 0.28, batch loss = 0.24 (35.6 examples/sec; 0.225 sec/batch; 19h:05m:53s remains)
INFO - root - 2017-12-15 07:45:58.387723: step 26890, loss = 0.27, batch loss = 0.24 (35.1 examples/sec; 0.228 sec/batch; 19h:20m:04s remains)
INFO - root - 2017-12-15 07:46:00.688177: step 26900, loss = 0.20, batch loss = 0.16 (34.5 examples/sec; 0.232 sec/batch; 19h:41m:52s remains)
2017-12-15 07:46:01.053626: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0848408 -5.7201729 -4.7262592 -4.07421 -3.1868038 -2.4627538 -1.9954573 -1.9022641 -1.8561136 -1.9922816 -2.4223733 -3.0956895 -3.7031255 -4.1027236 -4.5347638][-6.6515656 -5.1591277 -4.1554546 -3.8253255 -3.3420362 -2.6314895 -1.6096448 -0.80245781 -0.081115246 0.21105337 -0.15999031 -1.0080652 -1.837815 -2.5700572 -3.3523703][-6.7413445 -4.6872196 -3.8538847 -3.9096828 -3.8063328 -3.1104267 -1.6966009 -0.23831964 1.1639981 2.0306203 1.7524674 0.71775126 -0.32567191 -1.472854 -2.5036345][-7.2405567 -4.3488269 -3.872555 -4.3927917 -4.7595091 -4.0730934 -2.2929957 -0.43088651 1.2626078 2.4378178 2.3248928 1.4338596 0.34428787 -1.0687512 -2.1861846][-7.2724991 -4.0961318 -3.8474283 -4.6380324 -5.3182745 -4.701848 -2.7087531 -0.64551151 1.0151393 2.1397784 2.0359232 1.2926011 0.25517154 -1.3590901 -2.6005421][-6.4235115 -3.4548512 -3.2725224 -4.2082748 -5.0487165 -4.6116233 -2.6548829 -0.63143921 0.75640893 1.4255409 1.0404932 0.25324702 -0.83283675 -2.4435179 -3.5603924][-5.0176592 -2.3597505 -2.1339066 -2.9932113 -3.7627759 -3.518898 -1.8680079 -0.12329173 1.0034034 1.1530147 0.34018493 -0.69687772 -1.9685594 -3.5526876 -4.2947054][-4.6091905 -1.6238639 -1.2257137 -1.8354858 -2.4105141 -2.2373128 -0.75757849 0.7623291 1.5561666 1.3059714 0.071038723 -1.3379703 -2.8507197 -4.5067577 -5.1031675][-4.652688 -1.6817169 -1.1239936 -1.5501363 -2.0864415 -1.8775361 -0.47771442 0.82436943 1.2730083 0.76840472 -0.58367348 -1.9975218 -3.3165965 -4.6485405 -4.9446278][-5.0207334 -2.1615863 -1.5060601 -1.8456881 -2.5340893 -2.3969 -1.1107117 -0.018063545 0.25066686 -0.37424326 -1.6142602 -2.7646139 -3.6911068 -4.4306498 -4.4117327][-5.5393171 -2.8619289 -2.2154992 -2.4941926 -3.2396042 -3.125838 -2.0702786 -1.0541908 -0.78963757 -1.5096972 -2.5922685 -3.410399 -3.8433185 -3.9433756 -3.6065598][-6.3050938 -3.6725273 -3.0317159 -3.15865 -3.7728877 -3.70139 -3.0896637 -2.4522517 -2.3416882 -2.9863858 -3.6564775 -3.9614506 -3.8115029 -3.2416253 -2.6700313][-7.4073868 -4.8263869 -4.18381 -4.1249752 -4.489162 -4.4129996 -4.2368231 -4.1348228 -4.0897503 -4.5623055 -4.7918558 -4.580183 -4.0563908 -3.0675354 -2.4399621][-8.3641691 -5.9901428 -5.2939687 -4.9759564 -4.9381552 -4.7629247 -4.8634005 -4.9447622 -4.7877188 -5.0593243 -5.1413054 -4.8118839 -4.2803745 -3.2498095 -2.5014415][-8.8369217 -6.8316631 -6.041152 -5.4080191 -4.8857346 -4.5169225 -4.7189226 -4.8331718 -4.8416038 -5.3225679 -5.6028423 -5.4088688 -5.0273342 -4.1162825 -3.1752605]]...]
INFO - root - 2017-12-15 07:46:03.338347: step 26910, loss = 0.24, batch loss = 0.21 (36.5 examples/sec; 0.219 sec/batch; 18h:35m:52s remains)
INFO - root - 2017-12-15 07:46:05.582054: step 26920, loss = 0.20, batch loss = 0.17 (36.0 examples/sec; 0.222 sec/batch; 18h:52m:07s remains)
INFO - root - 2017-12-15 07:46:07.871471: step 26930, loss = 0.28, batch loss = 0.25 (35.9 examples/sec; 0.223 sec/batch; 18h:56m:13s remains)
INFO - root - 2017-12-15 07:46:10.175012: step 26940, loss = 0.16, batch loss = 0.13 (36.9 examples/sec; 0.217 sec/batch; 18h:23m:40s remains)
INFO - root - 2017-12-15 07:46:12.470043: step 26950, loss = 0.20, batch loss = 0.16 (35.5 examples/sec; 0.225 sec/batch; 19h:06m:21s remains)
INFO - root - 2017-12-15 07:46:14.731440: step 26960, loss = 0.16, batch loss = 0.12 (34.6 examples/sec; 0.231 sec/batch; 19h:37m:52s remains)
INFO - root - 2017-12-15 07:46:17.020279: step 26970, loss = 0.25, batch loss = 0.22 (36.3 examples/sec; 0.220 sec/batch; 18h:40m:42s remains)
INFO - root - 2017-12-15 07:46:19.284883: step 26980, loss = 0.23, batch loss = 0.20 (34.8 examples/sec; 0.230 sec/batch; 19h:31m:10s remains)
INFO - root - 2017-12-15 07:46:21.572851: step 26990, loss = 0.30, batch loss = 0.27 (36.0 examples/sec; 0.222 sec/batch; 18h:51m:30s remains)
INFO - root - 2017-12-15 07:46:23.864607: step 27000, loss = 0.26, batch loss = 0.23 (35.1 examples/sec; 0.228 sec/batch; 19h:21m:42s remains)
2017-12-15 07:46:24.225782: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6800931 -3.6310661 -4.4761152 -5.0472803 -5.1725564 -4.5569839 -3.5674214 -3.0043511 -2.4228985 -2.5512547 -3.4735966 -4.2098413 -4.471818 -4.6240535 -5.3957472][-3.3214042 -3.8612521 -4.6287794 -5.22073 -5.3775716 -4.8142719 -3.8421669 -3.2508981 -2.5166266 -2.3210955 -3.1111836 -3.9118514 -4.4359841 -4.9269381 -5.8396387][-5.0474014 -4.3594837 -4.9975195 -5.5858736 -5.668014 -5.0754747 -4.0072122 -3.2335219 -2.2343845 -1.8081729 -2.6043553 -3.6479373 -4.5140538 -5.1717539 -6.0507264][-6.1181231 -4.82425 -5.3454113 -5.6659231 -5.4071646 -4.5593891 -3.3038785 -2.2853465 -1.2812612 -1.0974911 -2.2169368 -3.6509547 -4.7522025 -5.4266968 -6.3314][-6.4873304 -5.052515 -5.371717 -5.174798 -4.3640661 -3.0744076 -1.3516793 0.074842215 0.94459677 0.70191193 -0.88581634 -2.8623919 -4.2608204 -5.1468048 -6.2842178][-6.1941614 -4.7769151 -4.9164591 -4.2730541 -3.095283 -1.4160187 0.81799436 2.6629784 3.3871968 2.7024434 0.61183858 -1.9007463 -3.6363239 -4.8438997 -6.2226033][-4.7578878 -3.6395998 -3.6422935 -2.7146449 -1.2974355 0.67319894 3.3158343 5.4671373 6.0029516 4.821991 2.1544616 -0.9845351 -3.0885308 -4.6973033 -6.2281265][-4.1761785 -2.7115533 -2.58144 -1.5136175 0.01044488 2.2589214 5.2723503 7.5822277 7.9539948 6.3895912 3.3111293 -0.2017529 -2.634721 -4.6104026 -6.2179136][-4.3642797 -2.7963643 -2.6858435 -1.7372606 -0.37599337 1.7937219 4.7535896 6.8463526 7.0843229 5.4276514 2.3761117 -0.88542438 -3.1403873 -5.0800891 -6.5632696][-5.1191235 -3.5046446 -3.2689562 -2.4839942 -1.487793 0.27293015 2.6843541 4.3459044 4.5072842 3.0063641 0.25707531 -2.4473908 -4.1719365 -5.8119678 -7.0431423][-6.4269247 -4.8340864 -4.62425 -4.0664606 -3.3199463 -1.8841722 -0.011962891 1.2513039 1.3323464 0.063990355 -2.2729282 -4.2905655 -5.3777895 -6.49759 -7.3836803][-8.2587652 -7.0589571 -7.0534563 -6.6196442 -5.9638577 -4.8749385 -3.59051 -2.7597303 -2.7723732 -3.7246335 -5.4274392 -6.612607 -6.844367 -7.128994 -7.4092216][-8.9167643 -8.168663 -8.4743385 -8.327383 -7.921114 -7.2621031 -6.5638542 -6.1630688 -6.2860851 -6.8289242 -7.6762877 -8.0506058 -7.5259905 -7.001832 -6.7366018][-8.2605391 -7.6767054 -8.1792173 -8.3004284 -8.1159592 -7.8245907 -7.5906076 -7.5852785 -7.77466 -7.8934593 -7.9771729 -7.8794518 -7.0701551 -6.1774826 -5.7162933][-7.3899555 -6.7110739 -7.1278076 -7.3124046 -7.2710295 -7.2065268 -7.1251011 -7.2325859 -7.4568872 -7.3899612 -7.1943016 -7.1375675 -6.3680763 -5.2770452 -4.7063684]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 07:46:26.555762: step 27010, loss = 0.24, batch loss = 0.21 (34.7 examples/sec; 0.231 sec/batch; 19h:34m:54s remains)
INFO - root - 2017-12-15 07:46:28.812008: step 27020, loss = 0.22, batch loss = 0.18 (36.1 examples/sec; 0.221 sec/batch; 18h:47m:40s remains)
INFO - root - 2017-12-15 07:46:31.111782: step 27030, loss = 0.26, batch loss = 0.23 (35.7 examples/sec; 0.224 sec/batch; 19h:02m:09s remains)
INFO - root - 2017-12-15 07:46:33.348632: step 27040, loss = 0.24, batch loss = 0.21 (35.5 examples/sec; 0.225 sec/batch; 19h:05m:39s remains)
INFO - root - 2017-12-15 07:46:35.645735: step 27050, loss = 0.21, batch loss = 0.18 (34.3 examples/sec; 0.233 sec/batch; 19h:47m:02s remains)
INFO - root - 2017-12-15 07:46:37.940243: step 27060, loss = 0.31, batch loss = 0.27 (35.2 examples/sec; 0.227 sec/batch; 19h:17m:44s remains)
INFO - root - 2017-12-15 07:46:40.241786: step 27070, loss = 0.20, batch loss = 0.17 (34.9 examples/sec; 0.230 sec/batch; 19h:28m:29s remains)
INFO - root - 2017-12-15 07:46:42.484805: step 27080, loss = 0.25, batch loss = 0.22 (35.2 examples/sec; 0.228 sec/batch; 19h:18m:25s remains)
INFO - root - 2017-12-15 07:46:44.763166: step 27090, loss = 0.25, batch loss = 0.22 (35.5 examples/sec; 0.226 sec/batch; 19h:07m:54s remains)
INFO - root - 2017-12-15 07:46:47.047377: step 27100, loss = 0.19, batch loss = 0.15 (35.8 examples/sec; 0.223 sec/batch; 18h:56m:48s remains)
2017-12-15 07:46:47.410130: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1813467 -3.1283638 -3.2725554 -3.3949056 -3.3939009 -3.302839 -3.1679077 -3.0790949 -3.0015349 -2.9562948 -3.0036769 -3.1350212 -3.3161421 -3.4418454 -3.4188962][-3.3455517 -3.5013821 -3.6485136 -3.8353233 -3.8355923 -3.6823328 -3.4638028 -3.3892202 -3.3357625 -3.3039317 -3.4097424 -3.6957586 -4.0544734 -4.1817417 -4.0030403][-4.0000768 -3.7081819 -3.8193285 -4.0289173 -4.020853 -3.6431372 -3.1121845 -2.9595048 -2.9976671 -3.0784473 -3.3323817 -3.7893229 -4.32605 -4.5181627 -4.2377815][-4.6276817 -3.7054725 -3.7551064 -3.9443121 -3.888895 -3.1391497 -2.0677915 -1.6728407 -1.7426875 -1.9872255 -2.4583888 -3.1414566 -3.911819 -4.2662168 -3.9568934][-5.4780474 -3.4748321 -3.3253136 -3.4134002 -3.2752028 -2.1656821 -0.59886849 0.00310421 -0.10417032 -0.52558815 -1.187986 -2.0226603 -3.0947089 -3.6824913 -3.4331975][-5.7269278 -3.385354 -2.9952273 -2.836884 -2.5749133 -1.2198907 0.664515 1.3555207 1.0806813 0.43249893 -0.34152424 -1.0909129 -2.2298653 -2.9985836 -2.9118814][-5.4637966 -3.4130688 -2.9327998 -2.5476482 -2.0441334 -0.44870126 1.5516322 2.1850629 1.8618264 1.1280189 0.27088428 -0.39972985 -1.5435528 -2.3579884 -2.4745381][-5.583209 -3.6039143 -3.0359395 -2.4298627 -1.7729778 -0.22394061 1.5352972 1.9832916 1.6707058 0.96367955 0.10090637 -0.57608342 -1.6620665 -2.2778628 -2.3478992][-5.5482197 -3.6829481 -3.0645528 -2.25511 -1.4485095 -0.21045971 0.91512012 0.99734259 0.540385 -0.14224839 -0.8493017 -1.348725 -2.1801648 -2.4231582 -2.2568784][-5.2615471 -3.4141831 -2.849577 -2.0560641 -1.2418188 -0.38620377 0.054986477 -0.22268748 -0.86409009 -1.4227477 -1.7892148 -2.0580921 -2.7708609 -2.8154979 -2.4411721][-4.9982567 -3.0533881 -2.5110624 -1.7474301 -1.0696957 -0.66588116 -0.75592756 -1.2327861 -1.756458 -2.0738051 -2.1757283 -2.5794082 -3.3508203 -3.3481503 -2.898864][-4.9857121 -2.8183784 -2.2508137 -1.4796346 -0.82980847 -0.492306 -0.66476214 -0.99663365 -1.3251245 -1.5071806 -1.5974416 -2.2372854 -3.1018295 -3.0987759 -2.634752][-5.2591081 -2.914999 -2.3301466 -1.6501607 -0.976424 -0.51921785 -0.39960921 -0.21955109 -0.28370404 -0.37613285 -0.41797972 -1.0295938 -1.9142845 -2.0453825 -1.7354225][-5.4181714 -3.1584742 -2.6405308 -2.2441077 -1.5977256 -1.0313482 -0.49810112 0.30960226 0.74907494 0.74081969 0.66826868 -0.10856342 -1.1902442 -1.5078688 -1.4398816][-5.1521678 -3.1228359 -2.7993355 -2.8107324 -2.4676824 -1.7110981 -0.75020397 0.54566383 1.3276598 1.3329298 1.1255152 0.17569542 -1.0843062 -1.5560513 -1.6359601]]...]
INFO - root - 2017-12-15 07:46:49.679349: step 27110, loss = 0.25, batch loss = 0.22 (34.9 examples/sec; 0.229 sec/batch; 19h:26m:05s remains)
INFO - root - 2017-12-15 07:46:51.942522: step 27120, loss = 0.19, batch loss = 0.15 (35.4 examples/sec; 0.226 sec/batch; 19h:09m:33s remains)
INFO - root - 2017-12-15 07:46:54.239032: step 27130, loss = 0.26, batch loss = 0.23 (35.6 examples/sec; 0.225 sec/batch; 19h:02m:54s remains)
INFO - root - 2017-12-15 07:46:56.524952: step 27140, loss = 0.20, batch loss = 0.17 (36.0 examples/sec; 0.222 sec/batch; 18h:52m:00s remains)
INFO - root - 2017-12-15 07:46:58.767129: step 27150, loss = 0.14, batch loss = 0.11 (35.1 examples/sec; 0.228 sec/batch; 19h:20m:03s remains)
INFO - root - 2017-12-15 07:47:01.092885: step 27160, loss = 0.24, batch loss = 0.20 (33.8 examples/sec; 0.237 sec/batch; 20h:05m:19s remains)
INFO - root - 2017-12-15 07:47:03.324787: step 27170, loss = 0.20, batch loss = 0.17 (36.0 examples/sec; 0.222 sec/batch; 18h:50m:09s remains)
INFO - root - 2017-12-15 07:47:05.587037: step 27180, loss = 0.18, batch loss = 0.15 (34.8 examples/sec; 0.230 sec/batch; 19h:30m:25s remains)
INFO - root - 2017-12-15 07:47:07.852294: step 27190, loss = 0.28, batch loss = 0.25 (35.0 examples/sec; 0.228 sec/batch; 19h:21m:28s remains)
INFO - root - 2017-12-15 07:47:10.156406: step 27200, loss = 0.20, batch loss = 0.17 (36.7 examples/sec; 0.218 sec/batch; 18h:29m:25s remains)
2017-12-15 07:47:10.509369: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2105074 -6.7224922 -7.42515 -7.4987 -7.6300964 -7.7437773 -7.750123 -7.5298882 -7.2097721 -6.736331 -6.527379 -6.4604645 -6.284194 -6.2130961 -6.3779106][-5.2383666 -7.7120953 -8.5219908 -8.4717169 -8.5629959 -8.5223179 -8.3760118 -8.1765223 -8.013175 -7.6325216 -7.563098 -7.4892406 -6.9957247 -6.7864227 -6.9604206][-6.5857573 -7.8418045 -8.6045771 -8.197751 -8.0095892 -7.6006413 -7.2646275 -7.1856337 -7.2704172 -7.1442204 -7.3234205 -7.4007921 -6.6443539 -6.273551 -6.3838463][-7.091032 -7.55478 -8.1938286 -7.2587614 -6.5581236 -5.61007 -5.034862 -5.0010509 -5.270678 -5.4083209 -5.893486 -6.2608366 -5.3695817 -4.9552336 -5.0449219][-7.1106319 -6.7558966 -7.0074253 -5.2815289 -3.8353283 -2.1653161 -1.1833479 -1.1067281 -1.6056275 -2.2162325 -3.1484575 -3.9057424 -3.0132174 -2.7922134 -2.9860547][-6.4762125 -6.0131407 -5.9435987 -3.6706681 -1.6827711 0.53493667 2.0675986 2.4905221 1.8630373 0.660151 -0.83160651 -1.8891475 -0.97260559 -0.95037973 -1.1969681][-5.7171888 -5.6062393 -5.3514404 -2.7961786 -0.5897851 1.7191305 3.5712492 4.3422022 3.7163441 2.2440603 0.46011829 -0.56721926 0.44941497 0.44743133 0.25397611][-5.8291025 -5.5843444 -5.2743587 -2.6901784 -0.50119555 1.6776168 3.7362692 4.61905 3.933521 2.4916179 0.54772758 -0.39343357 0.79723239 1.122853 1.2651067][-6.2930603 -6.1717548 -6.1290574 -4.0131865 -2.2271531 -0.31536913 1.8431928 2.7418239 2.0525334 0.79104638 -1.1270022 -2.0747991 -0.9334265 0.0049595833 0.69004917][-6.8263807 -6.8677053 -7.2065926 -5.805151 -4.5151215 -2.942219 -0.86405885 0.14347458 -0.39293504 -1.3977046 -3.11196 -4.0020943 -3.0143516 -1.6011252 -0.62980175][-7.07652 -7.2327995 -7.8506813 -7.1103115 -6.2783289 -5.142375 -3.3977556 -2.2977881 -2.5065851 -3.2712111 -4.6608896 -5.4259243 -4.5853176 -3.0396309 -2.1827621][-6.9859276 -7.2182655 -8.0177708 -7.8279982 -7.4392519 -6.7985497 -5.6586876 -4.8347149 -4.9617662 -5.6903205 -6.7696419 -7.3235044 -6.5988436 -5.1971579 -4.4923267][-6.6202517 -6.8147926 -7.6751528 -7.852294 -7.7506104 -7.4421721 -6.8564234 -6.4627752 -6.7405844 -7.5107794 -8.2939472 -8.5538921 -7.8353605 -6.7211428 -6.2510486][-5.9536314 -5.8843794 -6.5812521 -6.8542805 -6.8490973 -6.6965847 -6.445693 -6.4099035 -6.7756119 -7.34208 -7.73028 -7.7342615 -7.186573 -6.5548658 -6.27833][-5.317132 -4.8707008 -5.247221 -5.3852034 -5.3669252 -5.270216 -5.2060156 -5.3966312 -5.7696 -6.0873137 -6.1437969 -6.0016284 -5.6737528 -5.425 -5.29153]]...]
INFO - root - 2017-12-15 07:47:12.786842: step 27210, loss = 0.20, batch loss = 0.17 (35.2 examples/sec; 0.227 sec/batch; 19h:17m:29s remains)
INFO - root - 2017-12-15 07:47:15.051971: step 27220, loss = 0.25, batch loss = 0.21 (35.2 examples/sec; 0.228 sec/batch; 19h:17m:55s remains)
INFO - root - 2017-12-15 07:47:17.350955: step 27230, loss = 0.19, batch loss = 0.15 (35.6 examples/sec; 0.224 sec/batch; 19h:02m:00s remains)
INFO - root - 2017-12-15 07:47:19.612722: step 27240, loss = 0.36, batch loss = 0.33 (34.3 examples/sec; 0.233 sec/batch; 19h:45m:07s remains)
INFO - root - 2017-12-15 07:47:21.876242: step 27250, loss = 0.38, batch loss = 0.34 (36.0 examples/sec; 0.222 sec/batch; 18h:51m:53s remains)
INFO - root - 2017-12-15 07:47:24.165371: step 27260, loss = 0.23, batch loss = 0.20 (35.0 examples/sec; 0.229 sec/batch; 19h:22m:59s remains)
INFO - root - 2017-12-15 07:47:26.453735: step 27270, loss = 0.19, batch loss = 0.15 (34.3 examples/sec; 0.234 sec/batch; 19h:48m:05s remains)
INFO - root - 2017-12-15 07:47:28.737793: step 27280, loss = 0.27, batch loss = 0.24 (34.9 examples/sec; 0.229 sec/batch; 19h:26m:24s remains)
INFO - root - 2017-12-15 07:47:30.974747: step 27290, loss = 0.19, batch loss = 0.16 (35.2 examples/sec; 0.227 sec/batch; 19h:16m:43s remains)
INFO - root - 2017-12-15 07:47:33.248054: step 27300, loss = 0.22, batch loss = 0.19 (35.3 examples/sec; 0.227 sec/batch; 19h:13m:14s remains)
2017-12-15 07:47:33.590605: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8462629 -3.4207397 -3.6715007 -3.794322 -4.5688772 -5.0124979 -5.5521045 -6.140729 -6.2858076 -5.968864 -6.0372114 -6.3224697 -6.0561004 -5.4940529 -5.2953196][-3.4109185 -2.7988122 -3.3329275 -3.7174397 -4.5206194 -4.952 -5.5104465 -6.0010757 -6.0288849 -5.6605978 -5.67069 -5.8570728 -5.3545856 -4.6006136 -4.2189655][-3.9636927 -2.2986021 -2.9832864 -3.4911079 -4.247591 -4.4453831 -4.7993183 -5.0877228 -5.0943627 -4.8530536 -4.9560776 -5.19105 -4.64839 -3.8213811 -3.3341112][-4.1279268 -1.6500672 -2.5152366 -3.1752326 -3.7552876 -3.5830708 -3.5006194 -3.5032368 -3.6214046 -3.7453561 -4.1654792 -4.5003858 -3.9261558 -3.0751805 -2.5275846][-4.2824306 -1.0928708 -1.8911927 -2.4727106 -2.7169585 -2.1417594 -1.5778141 -1.1728547 -1.3852704 -2.1348841 -3.1900721 -3.8842673 -3.5249739 -2.7970529 -2.0681095][-3.7851627 -0.74653316 -1.473273 -1.875011 -1.7188549 -0.68565178 0.46362138 1.3217638 1.1102176 -0.22035742 -1.9936666 -3.2239647 -3.138931 -2.5174503 -1.6638283][-3.0652366 -0.49522209 -0.99982584 -1.1657228 -0.67813611 0.8164773 2.52648 3.6981013 3.375828 1.4847999 -0.90243506 -2.5066359 -2.6886325 -2.2388961 -1.4225018][-3.4150918 -0.46707368 -0.82682753 -0.87195754 -0.18540907 1.4545729 3.4763973 5.0414906 4.76033 2.5029604 -0.22798634 -2.0230124 -2.5369861 -2.5467319 -1.946228][-3.4221513 -0.57020628 -0.99962735 -1.14836 -0.66193473 0.61723638 2.569628 4.4224691 4.3283596 2.2454517 -0.14493823 -1.7876704 -2.3969774 -2.4499156 -1.8194592][-3.9025633 -1.0908496 -1.5867193 -1.8622875 -1.6914107 -0.95542991 0.61452484 2.3294628 2.3831351 0.94615054 -0.669392 -1.814117 -2.3496556 -2.3720255 -1.7759886][-4.8635435 -2.1838932 -2.5244272 -2.660203 -2.627774 -2.5130863 -1.645429 -0.47023344 -0.39121008 -1.1167853 -1.6854482 -2.054523 -2.4247818 -2.4831009 -2.1231859][-5.6804476 -3.4613838 -3.6790209 -3.663013 -3.568851 -3.6144178 -3.3199496 -2.9463005 -3.1298156 -3.3393314 -3.0784183 -2.8607883 -2.9127293 -2.9222419 -2.6808066][-6.5712996 -4.8719311 -5.1119895 -5.1316862 -4.9636064 -4.8161688 -4.649538 -4.6888571 -4.9714813 -5.0143147 -4.623385 -4.2198186 -4.1077404 -4.161715 -3.8867626][-7.4611044 -6.088562 -6.2457266 -6.29838 -6.0098314 -5.6818705 -5.5896878 -5.6848259 -5.8915939 -6.0489988 -5.8817306 -5.4413557 -5.0796208 -4.9765854 -4.5734367][-7.8793516 -6.8965187 -7.1448669 -7.3109884 -6.9971704 -6.6728182 -6.6565723 -6.6612577 -6.7796383 -7.0540295 -7.0353689 -6.5241318 -5.8816948 -5.5124779 -4.9639912]]...]
INFO - root - 2017-12-15 07:47:35.823615: step 27310, loss = 0.26, batch loss = 0.23 (35.2 examples/sec; 0.227 sec/batch; 19h:15m:38s remains)
INFO - root - 2017-12-15 07:47:38.074762: step 27320, loss = 0.26, batch loss = 0.23 (34.8 examples/sec; 0.230 sec/batch; 19h:30m:24s remains)
INFO - root - 2017-12-15 07:47:40.355349: step 27330, loss = 0.20, batch loss = 0.17 (36.5 examples/sec; 0.219 sec/batch; 18h:36m:07s remains)
INFO - root - 2017-12-15 07:47:42.605151: step 27340, loss = 0.23, batch loss = 0.19 (35.7 examples/sec; 0.224 sec/batch; 18h:58m:11s remains)
INFO - root - 2017-12-15 07:47:44.828047: step 27350, loss = 0.29, batch loss = 0.26 (36.1 examples/sec; 0.222 sec/batch; 18h:46m:58s remains)
INFO - root - 2017-12-15 07:47:47.086320: step 27360, loss = 0.18, batch loss = 0.15 (35.7 examples/sec; 0.224 sec/batch; 19h:01m:11s remains)
INFO - root - 2017-12-15 07:47:49.354413: step 27370, loss = 0.19, batch loss = 0.15 (35.2 examples/sec; 0.227 sec/batch; 19h:14m:27s remains)
INFO - root - 2017-12-15 07:47:51.589999: step 27380, loss = 0.25, batch loss = 0.22 (35.5 examples/sec; 0.225 sec/batch; 19h:06m:41s remains)
INFO - root - 2017-12-15 07:47:53.853800: step 27390, loss = 0.25, batch loss = 0.21 (35.0 examples/sec; 0.228 sec/batch; 19h:21m:28s remains)
INFO - root - 2017-12-15 07:47:56.105717: step 27400, loss = 0.18, batch loss = 0.15 (34.8 examples/sec; 0.230 sec/batch; 19h:30m:11s remains)
2017-12-15 07:47:56.390456: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1147623 -4.55746 -3.5459218 -1.9754996 -0.81342971 -0.6681838 -2.4203236 -4.662569 -5.4799008 -5.3804226 -4.6606359 -3.881577 -2.9955547 -2.466249 -2.2556674][-4.0507169 -5.2789345 -4.4937239 -2.8909492 -1.7008359 -1.3866682 -2.86177 -4.4409213 -4.7260909 -4.6188345 -4.3808727 -4.1131721 -3.5412269 -2.9449732 -2.7252774][-4.8756433 -5.6742015 -5.106657 -3.6234069 -2.4072433 -1.8480998 -2.8351431 -3.7331979 -3.6784692 -3.7858586 -4.2033353 -4.5925674 -4.5668077 -3.9484007 -3.5317242][-5.7108345 -6.1029634 -5.8180857 -4.505681 -3.1039929 -1.9877583 -2.1586962 -2.3785567 -2.168051 -2.6013203 -3.5824559 -4.6362047 -5.1677041 -4.5340858 -3.8723812][-5.9736996 -5.9149761 -6.0568295 -5.0409803 -3.3059235 -1.4213779 -0.64998817 -0.2699908 -0.21746898 -1.3009584 -3.036644 -4.7067523 -5.5454345 -4.8906159 -4.1526713][-5.3755322 -5.2260218 -5.6459951 -4.7280679 -2.5878937 -0.24689221 1.2317166 2.1212258 1.812377 -0.10256815 -2.5472634 -4.525897 -5.4045038 -4.8954334 -4.364994][-4.6766863 -4.6897783 -5.2377634 -4.2335577 -1.8507063 0.5777874 2.5570583 3.7969909 3.1634569 0.7708993 -1.9224334 -3.8900321 -4.4335117 -4.0750093 -3.8076782][-4.5076046 -4.5746841 -5.1427212 -4.1576929 -1.9565873 0.27364039 2.4989557 3.8059525 2.9625306 0.60450435 -1.7992046 -3.1951602 -3.18334 -2.9878445 -3.0775352][-4.6904354 -4.5812922 -5.1131697 -4.5147409 -2.9877548 -1.0256418 1.3014009 2.6617293 1.9430366 0.055690765 -1.7092009 -2.2721057 -1.7366366 -1.7404861 -2.2735267][-5.127748 -4.53768 -5.0042405 -5.0310802 -4.2838721 -2.5929463 -0.28100431 1.2352765 1.04831 -0.015709877 -0.96273482 -0.85415888 -0.12356806 -0.49452043 -1.4574418][-5.4415932 -4.2589154 -4.852077 -5.6271658 -5.5514722 -4.1162291 -2.0087051 -0.57150793 -0.30028403 -0.52369893 -0.55993056 0.20441461 1.0027921 0.39161897 -0.77781463][-5.5553455 -4.0238256 -5.004261 -6.405828 -6.6873512 -5.306118 -3.4386139 -2.2075522 -1.7213042 -1.4015455 -0.75138783 0.44344306 1.2358069 0.570179 -0.53249037][-6.0145507 -4.3381643 -5.5593023 -7.1165681 -7.3853626 -6.0041542 -4.2388377 -2.9849002 -2.2696009 -1.6481049 -0.77654922 0.32212782 0.96023083 0.35820556 -0.47186565][-5.9893007 -4.2237754 -5.2814674 -6.5945582 -6.8018351 -5.8182669 -4.3635545 -3.2298896 -2.4545474 -1.6970422 -0.78656411 0.135499 0.58123803 0.090986252 -0.47269988][-5.7914386 -4.0058427 -4.5370884 -5.2234592 -5.3139386 -4.70212 -3.6798654 -2.8877847 -2.3250875 -1.7088161 -0.91077781 -0.19097686 0.061845064 -0.29148722 -0.73762846]]...]
INFO - root - 2017-12-15 07:47:58.626915: step 27410, loss = 0.18, batch loss = 0.15 (36.9 examples/sec; 0.217 sec/batch; 18h:23m:27s remains)
INFO - root - 2017-12-15 07:48:00.878727: step 27420, loss = 0.23, batch loss = 0.20 (35.5 examples/sec; 0.225 sec/batch; 19h:04m:15s remains)
INFO - root - 2017-12-15 07:48:03.118650: step 27430, loss = 0.47, batch loss = 0.44 (34.5 examples/sec; 0.232 sec/batch; 19h:38m:46s remains)
INFO - root - 2017-12-15 07:48:05.396950: step 27440, loss = 0.27, batch loss = 0.24 (34.6 examples/sec; 0.231 sec/batch; 19h:35m:34s remains)
INFO - root - 2017-12-15 07:48:07.674360: step 27450, loss = 0.17, batch loss = 0.14 (34.5 examples/sec; 0.232 sec/batch; 19h:38m:31s remains)
INFO - root - 2017-12-15 07:48:09.934565: step 27460, loss = 0.19, batch loss = 0.15 (35.2 examples/sec; 0.227 sec/batch; 19h:15m:05s remains)
INFO - root - 2017-12-15 07:48:12.188153: step 27470, loss = 0.29, batch loss = 0.26 (35.9 examples/sec; 0.223 sec/batch; 18h:52m:49s remains)
INFO - root - 2017-12-15 07:48:14.450358: step 27480, loss = 0.24, batch loss = 0.21 (35.2 examples/sec; 0.227 sec/batch; 19h:15m:51s remains)
INFO - root - 2017-12-15 07:48:16.717195: step 27490, loss = 0.22, batch loss = 0.19 (33.6 examples/sec; 0.238 sec/batch; 20h:12m:08s remains)
INFO - root - 2017-12-15 07:48:18.963853: step 27500, loss = 0.25, batch loss = 0.21 (35.2 examples/sec; 0.228 sec/batch; 19h:16m:48s remains)
2017-12-15 07:48:19.266479: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.38525665 -2.3874393 -2.9052353 -3.2311316 -3.5891967 -3.837038 -3.7119145 -3.3462067 -2.4736788 -2.0027366 -1.949568 -1.5424746 -1.1904303 -0.75770462 -0.869825][-0.32079136 -1.4652505 -2.0361788 -2.2986004 -2.6704364 -3.1818409 -3.1041913 -2.6351008 -1.5541308 -0.7864989 -0.8525362 -0.98046815 -1.010473 -0.72112691 -0.93808997][0.11159492 -0.2999481 -0.8597821 -1.0066395 -1.1638563 -1.7809031 -1.8645859 -1.5949545 -0.58453786 0.37035084 0.38505054 -0.23985624 -0.92546308 -1.1673555 -1.7590401][0.47578645 0.92710805 0.39111829 0.24823356 0.45533609 0.22111392 0.11994743 -0.096618414 0.28724337 1.0030887 0.98945975 0.15822697 -0.99891126 -1.870243 -2.9225907][0.17603636 1.3118129 0.87640214 0.73260617 1.2428076 1.6629457 2.0430813 1.5468011 1.1419969 1.4071164 1.3087165 0.59211946 -0.61414993 -1.9440861 -3.3605723][-0.11926079 1.2143528 0.89667678 0.724092 1.2878978 2.2381897 3.4134121 3.2195206 2.2090149 1.7318461 1.4599524 0.894042 -0.084693432 -1.3926523 -2.8138511][0.073540211 1.0410125 0.67573547 0.50476694 1.0543785 2.2966509 4.1650333 4.5771432 3.2366281 1.9837699 1.3531604 0.82358766 0.19359207 -0.87061346 -2.1107838][-0.3037498 0.35374022 -0.2805028 -0.56195164 -0.1339519 1.0640187 3.0836496 3.899683 2.4564314 0.56202459 -0.44274855 -0.75398076 -0.80716789 -1.3316712 -2.0890927][-1.4202631 -1.0182136 -1.9984993 -2.5856922 -2.5019257 -1.6720427 0.11325598 1.2517543 0.18046761 -1.7639483 -2.8375964 -2.8798451 -2.3598161 -2.2967079 -2.6575708][-2.4299483 -2.2138252 -3.4368682 -4.335393 -4.7353086 -4.3856936 -2.8290844 -1.3220551 -1.6806822 -3.2308195 -4.2291479 -4.1419735 -3.345912 -2.9585433 -3.1533635][-3.4043875 -3.3674488 -4.51165 -5.4496441 -6.168951 -6.2670746 -4.9719887 -3.3348694 -3.1687081 -4.2637186 -5.1349521 -5.0180087 -4.1727567 -3.6113946 -3.6486878][-4.9991627 -5.014204 -5.6758728 -6.2674155 -7.0031447 -7.4189639 -6.4729228 -4.8854246 -4.2944622 -4.8995428 -5.5374331 -5.3530679 -4.61734 -4.1163931 -4.0896792][-5.5539494 -5.6754313 -5.9954472 -6.2466116 -6.7908936 -7.208889 -6.525208 -5.1952276 -4.5224543 -4.7661171 -5.09978 -4.8422318 -4.2342739 -3.9048977 -3.9648218][-4.8847618 -5.02855 -5.2335682 -5.3211055 -5.653039 -5.8532295 -5.3166347 -4.3760729 -3.8622913 -3.9578915 -4.1902452 -4.045083 -3.6586425 -3.5147343 -3.6212978][-3.9364533 -3.846782 -3.9761076 -4.0557647 -4.2849169 -4.3685327 -4.017837 -3.4632421 -3.1827853 -3.2574379 -3.386394 -3.3207903 -3.1692467 -3.1767628 -3.2955453]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-27500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-27500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 07:48:21.980406: step 27510, loss = 0.22, batch loss = 0.19 (35.7 examples/sec; 0.224 sec/batch; 18h:59m:54s remains)
INFO - root - 2017-12-15 07:48:24.224551: step 27520, loss = 0.19, batch loss = 0.16 (35.5 examples/sec; 0.225 sec/batch; 19h:05m:40s remains)
INFO - root - 2017-12-15 07:48:26.480408: step 27530, loss = 0.27, batch loss = 0.24 (36.6 examples/sec; 0.219 sec/batch; 18h:31m:16s remains)
INFO - root - 2017-12-15 07:48:28.734916: step 27540, loss = 0.21, batch loss = 0.18 (35.2 examples/sec; 0.227 sec/batch; 19h:14m:27s remains)
INFO - root - 2017-12-15 07:48:30.999760: step 27550, loss = 0.27, batch loss = 0.23 (34.5 examples/sec; 0.232 sec/batch; 19h:39m:39s remains)
INFO - root - 2017-12-15 07:48:33.268917: step 27560, loss = 0.23, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 18h:48m:47s remains)
INFO - root - 2017-12-15 07:48:35.538356: step 27570, loss = 0.19, batch loss = 0.16 (34.5 examples/sec; 0.232 sec/batch; 19h:39m:37s remains)
INFO - root - 2017-12-15 07:48:37.792879: step 27580, loss = 0.25, batch loss = 0.22 (34.7 examples/sec; 0.231 sec/batch; 19h:32m:02s remains)
INFO - root - 2017-12-15 07:48:40.090943: step 27590, loss = 0.21, batch loss = 0.17 (35.0 examples/sec; 0.228 sec/batch; 19h:21m:08s remains)
INFO - root - 2017-12-15 07:48:42.351753: step 27600, loss = 0.34, batch loss = 0.31 (34.7 examples/sec; 0.230 sec/batch; 19h:30m:59s remains)
2017-12-15 07:48:42.652678: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3674631 -7.2204208 -7.0124693 -7.1685295 -7.5815258 -7.9471922 -7.5316868 -6.686059 -5.5282464 -4.9424334 -5.3225884 -5.8572178 -6.5672932 -7.426384 -7.6151972][-7.870245 -7.8849735 -7.8256025 -8.0376444 -8.41302 -8.687829 -7.8295813 -6.4892492 -5.0159225 -4.2726617 -4.6687212 -5.2483044 -6.2500877 -7.5014153 -7.9141417][-8.86984 -8.0355148 -8.0370007 -8.214118 -8.4049454 -8.2504625 -6.8196583 -5.0540543 -3.5593483 -3.0118597 -3.5684896 -4.1691184 -5.2032633 -6.5950747 -7.2707214][-9.4936867 -7.6518936 -7.8246355 -8.0585136 -8.0124512 -7.2287703 -5.1655908 -3.083777 -1.7758731 -1.7261956 -2.6682653 -3.4200408 -4.10896 -5.2044916 -5.9671173][-9.766964 -7.0910978 -7.52715 -7.8818045 -7.4657707 -6.016119 -3.3752608 -0.91494894 0.25169897 -0.37892067 -1.928378 -3.0382063 -3.3538609 -3.9427497 -4.5686326][-8.67709 -6.0115509 -6.5395737 -6.8817043 -5.836812 -3.6906495 -0.62273681 2.1846807 3.2645323 1.8327267 -0.63525212 -2.5980625 -2.8883922 -3.193526 -3.6923132][-6.0773573 -3.908556 -4.6020575 -4.9050069 -3.4254618 -0.90957165 2.4891884 5.5899982 6.6318083 4.4572678 0.9522016 -2.0966163 -2.7776737 -3.0968285 -3.5348206][-3.3586826 -1.180832 -2.0795546 -2.5690939 -1.1627545 1.1487772 4.450037 7.600812 8.7075109 6.2503309 2.1686604 -1.5668464 -2.9042342 -3.4310343 -3.8238406][-1.583849 0.46710515 -0.55694234 -1.3916254 -0.40500057 1.4018836 4.2109232 6.9568481 7.9745398 5.7619762 1.941653 -1.8410883 -3.6696389 -4.3728142 -4.7687907][-1.4221373 0.72658157 -0.15816927 -1.0689154 -0.57237113 0.47505045 2.545078 4.6043806 5.510355 3.7953055 0.64789128 -2.6409836 -4.4780712 -5.2488689 -5.7609897][-2.4991207 -0.018249273 -0.41784561 -1.0855728 -0.74963117 -0.38621283 0.7399931 2.1305902 3.1287868 1.9215167 -0.56265628 -3.147665 -4.8539143 -5.7445946 -6.4760952][-4.7825041 -1.8875334 -1.8030698 -2.1260016 -1.9839555 -2.0705373 -1.4144659 -0.39087451 0.68001723 0.0087590218 -1.8926772 -3.745059 -5.2701774 -6.3799181 -7.36154][-7.35546 -4.0549812 -3.4153361 -3.2782176 -3.4595544 -3.934473 -3.4477844 -2.5254188 -1.4224932 -1.7129099 -3.1219494 -4.3148837 -5.5582485 -6.745882 -7.9589558][-9.356946 -6.0127106 -5.0391707 -4.5007114 -4.8469687 -5.5536456 -5.2114687 -4.3565197 -3.3690894 -3.4022646 -4.3505344 -5.0129128 -6.0341644 -7.2429924 -8.4038382][-10.473979 -7.4778242 -6.4350033 -5.881712 -6.4572029 -7.1480885 -6.8758631 -6.1129279 -5.2560034 -5.0674925 -5.493978 -5.7514896 -6.5457573 -7.7119741 -8.5735407]]...]
INFO - root - 2017-12-15 07:48:44.906695: step 27610, loss = 0.22, batch loss = 0.19 (35.5 examples/sec; 0.225 sec/batch; 19h:03m:45s remains)
INFO - root - 2017-12-15 07:48:47.180295: step 27620, loss = 0.28, batch loss = 0.25 (33.6 examples/sec; 0.238 sec/batch; 20h:09m:11s remains)
INFO - root - 2017-12-15 07:48:49.431819: step 27630, loss = 0.24, batch loss = 0.21 (34.0 examples/sec; 0.236 sec/batch; 19h:56m:59s remains)
INFO - root - 2017-12-15 07:48:51.723895: step 27640, loss = 0.27, batch loss = 0.23 (34.7 examples/sec; 0.230 sec/batch; 19h:29m:54s remains)
INFO - root - 2017-12-15 07:48:54.006301: step 27650, loss = 0.22, batch loss = 0.19 (33.8 examples/sec; 0.237 sec/batch; 20h:03m:17s remains)
INFO - root - 2017-12-15 07:48:56.278264: step 27660, loss = 0.22, batch loss = 0.19 (35.7 examples/sec; 0.224 sec/batch; 18h:58m:06s remains)
INFO - root - 2017-12-15 07:48:58.568419: step 27670, loss = 0.45, batch loss = 0.42 (35.5 examples/sec; 0.225 sec/batch; 19h:03m:18s remains)
INFO - root - 2017-12-15 07:49:00.805336: step 27680, loss = 0.18, batch loss = 0.15 (35.1 examples/sec; 0.228 sec/batch; 19h:18m:27s remains)
INFO - root - 2017-12-15 07:49:03.054827: step 27690, loss = 0.19, batch loss = 0.15 (35.6 examples/sec; 0.225 sec/batch; 19h:02m:52s remains)
INFO - root - 2017-12-15 07:49:05.289689: step 27700, loss = 0.22, batch loss = 0.19 (36.6 examples/sec; 0.219 sec/batch; 18h:31m:10s remains)
2017-12-15 07:49:05.590750: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8899474 -4.1870933 -3.5779667 -3.952769 -3.9487691 -3.5944316 -3.1556642 -3.4725552 -4.1908889 -3.944581 -3.6595194 -3.5069566 -3.8741667 -4.9265127 -5.1599655][-4.4673929 -5.019145 -4.4523025 -4.659059 -4.6219296 -4.5511379 -4.1721878 -4.2909489 -4.7673168 -4.3558369 -4.2784185 -4.4548616 -4.9700632 -6.0101566 -6.1445031][-6.0231509 -5.6069989 -5.0180902 -5.0092888 -4.923924 -4.9323397 -4.5768838 -4.47377 -4.657155 -4.38058 -4.6905508 -5.3868675 -6.1263218 -7.1279454 -7.1107278][-7.1691179 -6.0775881 -5.3001723 -4.9093804 -4.6313353 -4.5137024 -4.0437655 -3.6723104 -3.6365676 -3.811137 -4.6539459 -5.8663793 -6.8625298 -7.76259 -7.3858066][-7.7996058 -6.0576258 -4.8692451 -3.8334932 -3.0056794 -2.3643634 -1.4536047 -0.75478578 -0.92361212 -2.123513 -3.630549 -5.3353066 -6.4990082 -7.335896 -6.7450376][-7.7849121 -5.5936451 -4.1748261 -2.715903 -1.3057888 0.16509247 1.7742386 2.8498755 2.2312393 -0.098777771 -2.2990777 -4.4871531 -5.9080648 -6.6690655 -5.9595709][-6.91505 -5.0115595 -3.5857222 -2.0372589 -0.17909455 2.2038636 4.6852856 6.1726055 5.1581135 1.889256 -0.7365346 -3.1497753 -4.807653 -5.604743 -4.9202442][-6.6829271 -4.5398111 -3.1134336 -1.5351678 0.59307694 3.5793061 6.554925 8.251297 7.1022978 3.2577477 0.24683022 -2.1612504 -3.6388888 -4.1024809 -3.5628934][-6.6343508 -4.7250028 -3.6012712 -2.2471497 -0.25125313 2.7176347 5.6382523 7.2504435 6.242054 2.533298 -0.45487 -2.6981983 -3.8871155 -3.9246819 -3.315511][-6.8877506 -5.4641337 -4.8442116 -3.8493686 -2.1680493 0.5241363 3.0626731 4.3024096 3.4682913 0.27722263 -2.3239009 -4.1231523 -4.8540516 -4.5833807 -4.0408621][-7.241797 -6.0905471 -5.7378292 -5.036901 -3.7638359 -1.5701107 0.44147658 1.2082336 0.32585883 -2.3152707 -4.3466768 -5.6566582 -6.0015192 -5.560976 -5.11635][-7.5272679 -6.4126797 -6.12827 -5.6535563 -4.80805 -3.4279976 -2.2506263 -2.1434841 -3.2310953 -5.3307652 -6.8188634 -7.6439877 -7.6284971 -7.0285697 -6.4591484][-7.6827593 -6.5454454 -6.195961 -5.9264059 -5.4922171 -4.9262733 -4.6419277 -5.1277623 -6.1897049 -7.4204726 -8.1610994 -8.3894739 -8.0550575 -7.3630152 -6.710392][-7.2706242 -6.0448689 -5.6147232 -5.5020103 -5.2608814 -5.1190357 -5.3912897 -6.1223593 -6.9106488 -7.4532504 -7.6630836 -7.5182552 -7.1363955 -6.612711 -6.1314058][-6.3681192 -5.0175695 -4.5161152 -4.5074749 -4.3858271 -4.5708284 -5.2024503 -5.9173026 -6.4277811 -6.4920378 -6.4749546 -6.2395835 -5.9700785 -5.6840687 -5.4249163]]...]
INFO - root - 2017-12-15 07:49:07.889992: step 27710, loss = 0.27, batch loss = 0.24 (34.0 examples/sec; 0.235 sec/batch; 19h:54m:55s remains)
INFO - root - 2017-12-15 07:49:10.211351: step 27720, loss = 0.26, batch loss = 0.23 (35.3 examples/sec; 0.226 sec/batch; 19h:09m:46s remains)
INFO - root - 2017-12-15 07:49:12.471849: step 27730, loss = 0.24, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 18h:47m:13s remains)
INFO - root - 2017-12-15 07:49:14.757718: step 27740, loss = 0.26, batch loss = 0.23 (34.1 examples/sec; 0.234 sec/batch; 19h:50m:53s remains)
INFO - root - 2017-12-15 07:49:17.026180: step 27750, loss = 0.19, batch loss = 0.16 (35.9 examples/sec; 0.223 sec/batch; 18h:51m:30s remains)
INFO - root - 2017-12-15 07:49:19.275821: step 27760, loss = 0.18, batch loss = 0.15 (35.9 examples/sec; 0.223 sec/batch; 18h:53m:15s remains)
INFO - root - 2017-12-15 07:49:21.561419: step 27770, loss = 0.27, batch loss = 0.23 (34.5 examples/sec; 0.232 sec/batch; 19h:37m:08s remains)
INFO - root - 2017-12-15 07:49:23.841977: step 27780, loss = 0.23, batch loss = 0.20 (33.9 examples/sec; 0.236 sec/batch; 19h:57m:18s remains)
INFO - root - 2017-12-15 07:49:26.113582: step 27790, loss = 0.23, batch loss = 0.20 (35.7 examples/sec; 0.224 sec/batch; 18h:57m:18s remains)
INFO - root - 2017-12-15 07:49:28.414449: step 27800, loss = 0.22, batch loss = 0.19 (35.7 examples/sec; 0.224 sec/batch; 18h:56m:59s remains)
2017-12-15 07:49:28.680718: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.444757 -5.1166453 -6.1011343 -6.8745909 -7.1705341 -6.4368887 -6.0517635 -6.0946932 -5.3917646 -5.8666244 -7.0426083 -7.4960985 -6.8980742 -5.8512549 -4.3944116][-1.892077 -3.1386197 -4.3929477 -5.3595066 -5.9027629 -5.4373541 -5.3464046 -5.7227249 -5.3349485 -6.2989254 -7.5488863 -7.7047386 -6.9868145 -5.8055124 -4.067481][-0.47907424 -1.3009101 -2.6124682 -3.5008855 -3.9745817 -3.5077362 -3.5637183 -4.02401 -3.9232802 -5.410203 -6.8901567 -7.0439234 -6.3248787 -5.0034666 -3.0718384][0.14715052 -0.1720891 -1.3063765 -1.9233181 -1.9963132 -1.1075165 -0.94919372 -1.4283284 -1.6184247 -3.5695305 -5.5327287 -5.9864054 -5.4980659 -4.2747583 -2.3055303][0.11417365 0.23127794 -0.60028493 -0.74462736 -0.12115431 1.4188659 1.9223688 1.2936692 0.51145172 -1.6805776 -3.9759357 -4.7948074 -4.5941572 -3.5622568 -1.6610956][0.10015583 0.27421618 -0.24977493 0.1291585 1.4776807 3.6498272 4.395133 3.5165365 2.0683057 -0.38003981 -2.8721726 -4.0087152 -3.93923 -3.0670135 -1.3420839][-0.26613617 -0.19524431 -0.20005369 0.78996706 2.7137372 5.184597 6.0562429 4.9097261 2.9083755 0.41873312 -2.0406439 -3.2923186 -3.4230757 -2.8623428 -1.5627507][-1.1295058 -0.96020329 -0.54519641 0.93090177 3.0386059 5.2191658 5.8387251 4.4050283 2.093075 -0.10996819 -2.082298 -3.0332372 -3.1812081 -2.9445665 -2.0820837][-2.0583129 -1.9224142 -1.4623415 0.047771692 1.8717725 3.3269494 3.4096859 1.9649756 -0.17982554 -1.8528131 -3.0975387 -3.5740075 -3.53151 -3.4225073 -2.9341516][-2.9708796 -2.9502563 -2.6995463 -1.4318153 -0.061706066 0.74640727 0.49849677 -0.69320548 -2.3135297 -3.5025494 -4.1671143 -4.2437377 -4.0585222 -4.0609531 -3.9283533][-4.0878878 -3.9296589 -3.7816114 -2.8199532 -1.8439479 -1.4518657 -1.8668033 -2.9170375 -4.1329088 -4.7661362 -4.8396721 -4.5113382 -4.3242702 -4.49874 -4.6805506][-5.2402582 -4.9314108 -4.8236837 -4.1672034 -3.5798826 -3.4844685 -3.9702003 -4.8987236 -5.7732525 -5.9640713 -5.5299683 -4.8264809 -4.4666877 -4.6182737 -5.0403447][-5.5555611 -5.0386076 -4.8689942 -4.5396733 -4.51054 -4.8534517 -5.4055524 -6.1675806 -6.7943373 -6.6689892 -5.877594 -4.780787 -4.1376982 -4.18199 -4.6732879][-5.64859 -5.013484 -4.9797635 -5.01797 -5.4423046 -6.0603752 -6.4230127 -6.759203 -6.9813871 -6.5522184 -5.54562 -4.3947673 -3.7598367 -3.8687372 -4.3715034][-5.4053059 -4.6579251 -4.828392 -5.14785 -5.7604733 -6.517415 -6.8603697 -6.9150009 -6.7988005 -6.2078977 -5.21327 -4.2362471 -3.6770921 -3.761178 -4.1334162]]...]
INFO - root - 2017-12-15 07:49:30.930087: step 27810, loss = 0.18, batch loss = 0.15 (34.7 examples/sec; 0.231 sec/batch; 19h:32m:14s remains)
INFO - root - 2017-12-15 07:49:33.194485: step 27820, loss = 0.19, batch loss = 0.16 (36.6 examples/sec; 0.219 sec/batch; 18h:31m:18s remains)
INFO - root - 2017-12-15 07:49:35.464898: step 27830, loss = 0.21, batch loss = 0.18 (34.5 examples/sec; 0.232 sec/batch; 19h:36m:35s remains)
INFO - root - 2017-12-15 07:49:37.724718: step 27840, loss = 0.24, batch loss = 0.20 (34.4 examples/sec; 0.233 sec/batch; 19h:42m:21s remains)
INFO - root - 2017-12-15 07:49:40.035999: step 27850, loss = 0.21, batch loss = 0.18 (35.6 examples/sec; 0.225 sec/batch; 19h:02m:31s remains)
INFO - root - 2017-12-15 07:49:42.298242: step 27860, loss = 0.25, batch loss = 0.22 (36.1 examples/sec; 0.222 sec/batch; 18h:45m:11s remains)
INFO - root - 2017-12-15 07:49:44.577413: step 27870, loss = 0.27, batch loss = 0.24 (35.9 examples/sec; 0.223 sec/batch; 18h:51m:53s remains)
INFO - root - 2017-12-15 07:49:46.844616: step 27880, loss = 0.17, batch loss = 0.14 (35.8 examples/sec; 0.224 sec/batch; 18h:55m:26s remains)
INFO - root - 2017-12-15 07:49:49.078515: step 27890, loss = 0.20, batch loss = 0.17 (35.6 examples/sec; 0.224 sec/batch; 18h:59m:39s remains)
INFO - root - 2017-12-15 07:49:51.368394: step 27900, loss = 0.29, batch loss = 0.26 (31.0 examples/sec; 0.258 sec/batch; 21h:48m:51s remains)
2017-12-15 07:49:51.640505: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8274477 -1.8096529 -1.7190623 -1.9819841 -2.5515366 -3.01027 -2.8053827 -2.5710645 -2.6536562 -2.7493391 -2.4513073 -1.8908702 -1.1150185 -0.53766191 -0.047335148][-3.2230415 -2.5009665 -2.610106 -3.0388112 -3.6547897 -4.2422152 -4.057755 -3.6706896 -3.6582613 -3.6866188 -3.2700582 -2.5420337 -1.6538442 -1.0345837 -0.52829039][-4.3136559 -2.9704654 -3.1990035 -3.6471682 -4.1676726 -4.7412758 -4.477603 -3.9746935 -3.983474 -4.120183 -3.7876363 -3.0910754 -2.1988659 -1.5241926 -0.97278404][-3.7530718 -2.3350346 -2.627368 -3.1497397 -3.715867 -4.2906389 -3.9338348 -3.3186009 -3.3608918 -3.6731422 -3.6013508 -3.1756759 -2.5064323 -1.9659709 -1.5717806][-2.8116593 -1.2250994 -1.4910262 -2.0846782 -2.8125372 -3.3706746 -2.933749 -2.2508061 -2.2289205 -2.6488278 -2.8122544 -2.7835257 -2.6206462 -2.528414 -2.4686897][-2.0884624 -0.53542256 -0.59107006 -0.95136428 -1.5278726 -1.8350967 -1.2030547 -0.39306176 -0.3249898 -0.84207475 -1.2498959 -1.6331434 -1.974694 -2.2612526 -2.5393231][-1.2382295 -0.21001816 -0.136163 -0.28308785 -0.72721219 -0.784721 0.19212008 1.3840084 1.6728067 1.138365 0.42063665 -0.4898448 -1.272855 -1.8044859 -2.3071847][-0.5936904 0.29116368 0.4327836 0.41224074 -0.040211439 -0.081139088 1.0187647 2.4263914 2.9523394 2.4811981 1.6108723 0.44863343 -0.59698296 -1.421802 -2.1939855][-0.65346873 0.26854467 0.50507045 0.60251737 0.011040449 -0.36150265 0.43085122 1.7029777 2.3493545 2.1731675 1.635654 0.82603455 -0.040850639 -0.85382378 -1.6794238][-1.2743771 -0.17592144 0.30598378 0.54582429 -0.182333 -0.76852393 -0.14262152 0.90037179 1.4878113 1.5283356 1.3923457 0.89358306 0.2328248 -0.39664054 -1.0700239][-1.9535316 -0.74410665 -0.12043643 0.07668829 -0.85657382 -1.5474144 -0.91240013 0.054601669 0.718318 1.201776 1.5573344 1.2157958 0.48807096 -0.27036989 -1.09902][-2.4500148 -1.2440906 -0.61386025 -0.51756322 -1.6799226 -2.5328515 -1.988922 -1.1546032 -0.4527446 0.32822752 1.1013851 1.1395776 0.59303284 -0.16942573 -1.150866][-2.9508698 -1.7521362 -1.1122452 -1.0415657 -2.2069402 -3.1055863 -2.6491008 -1.993042 -1.4548163 -0.81304026 -0.04897356 0.18903732 -0.18521595 -0.69207227 -1.4831132][-3.3424668 -2.1765895 -1.5668442 -1.3953252 -2.2883909 -3.0690434 -2.7016959 -2.1634939 -1.7522213 -1.258215 -0.72587788 -0.58160865 -0.95703018 -1.4677031 -2.2138598][-3.2113552 -2.1795497 -1.7203538 -1.4257014 -2.061933 -2.7886071 -2.6901596 -2.3814697 -2.1326809 -1.7765024 -1.3648615 -1.1923074 -1.5433536 -2.1522937 -2.9819014]]...]
INFO - root - 2017-12-15 07:49:53.876226: step 27910, loss = 0.19, batch loss = 0.16 (35.6 examples/sec; 0.224 sec/batch; 18h:59m:12s remains)
INFO - root - 2017-12-15 07:49:56.147969: step 27920, loss = 0.25, batch loss = 0.22 (35.5 examples/sec; 0.225 sec/batch; 19h:03m:42s remains)
INFO - root - 2017-12-15 07:49:58.409384: step 27930, loss = 0.27, batch loss = 0.24 (36.2 examples/sec; 0.221 sec/batch; 18h:42m:25s remains)
INFO - root - 2017-12-15 07:50:00.656741: step 27940, loss = 0.35, batch loss = 0.32 (35.8 examples/sec; 0.223 sec/batch; 18h:54m:16s remains)
INFO - root - 2017-12-15 07:50:02.917080: step 27950, loss = 0.30, batch loss = 0.26 (34.2 examples/sec; 0.234 sec/batch; 19h:46m:22s remains)
INFO - root - 2017-12-15 07:50:05.221874: step 27960, loss = 0.26, batch loss = 0.23 (35.8 examples/sec; 0.224 sec/batch; 18h:54m:28s remains)
INFO - root - 2017-12-15 07:50:07.510657: step 27970, loss = 0.28, batch loss = 0.25 (34.2 examples/sec; 0.234 sec/batch; 19h:45m:36s remains)
INFO - root - 2017-12-15 07:50:09.776129: step 27980, loss = 0.28, batch loss = 0.24 (36.2 examples/sec; 0.221 sec/batch; 18h:40m:10s remains)
INFO - root - 2017-12-15 07:50:12.020807: step 27990, loss = 0.24, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 18h:58m:07s remains)
INFO - root - 2017-12-15 07:50:14.253333: step 28000, loss = 0.25, batch loss = 0.21 (36.0 examples/sec; 0.222 sec/batch; 18h:48m:36s remains)
2017-12-15 07:50:14.530674: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3743141 -2.8207819 -3.5665979 -3.5382056 -3.8070579 -4.1229734 -4.3385744 -4.1605253 -3.7633464 -3.2918525 -3.2549891 -3.9786727 -5.1610541 -5.9552584 -5.9584336][-3.3771362 -4.015996 -4.2574835 -3.749939 -3.6914816 -3.8362927 -4.0618563 -4.2117386 -4.0268736 -3.4531047 -3.4154372 -4.4219732 -5.7716274 -6.4912872 -5.992795][-4.9620056 -4.5152206 -4.2387118 -3.3353956 -2.964402 -2.9200864 -2.9346585 -3.24623 -3.3321881 -2.9551752 -3.1133351 -4.28668 -5.6174049 -6.1982989 -5.2569656][-5.7245574 -4.5174131 -4.07499 -2.9126959 -2.1668549 -1.7202182 -1.2484217 -1.4909205 -1.8876919 -2.0406079 -2.757654 -4.2606511 -5.5630207 -5.9461603 -4.511817][-6.288723 -4.2819605 -3.6781337 -2.2297492 -0.97340548 0.13847136 1.1117327 1.0612845 0.26881289 -0.69304287 -2.3881414 -4.4164009 -5.6386576 -5.7481213 -3.8977437][-6.1237659 -4.0529079 -3.1720219 -1.3389935 0.6356883 2.6544027 4.2739806 4.3868537 2.8637614 0.65463662 -2.1670802 -4.6241112 -5.7994413 -5.7068968 -3.591939][-5.4342976 -3.6541796 -2.7933545 -0.90773582 1.5209994 4.4251342 6.5255251 6.6727419 4.3332448 1.0473616 -2.516911 -5.15574 -6.3279762 -6.0426426 -3.7193334][-5.3133078 -3.6365337 -3.1681151 -1.6723934 0.60579777 3.6121254 5.8311539 6.0830736 3.5874443 -0.01650095 -3.5122042 -5.7880588 -6.6728578 -6.1289897 -3.8252916][-5.3012638 -4.0075383 -3.8529334 -2.8400793 -1.2608566 1.1774797 3.2584877 3.6831164 1.679769 -1.3721044 -4.0992126 -5.57666 -5.9516544 -5.2827682 -3.3178177][-5.3491793 -4.2988281 -4.4113951 -3.8267012 -2.8984928 -1.0678003 0.93974948 1.6568189 0.36987591 -2.0031769 -4.0461588 -4.8988719 -4.8826628 -4.2762012 -2.7809052][-4.2911987 -3.450973 -3.9416902 -3.7142406 -3.2573738 -1.891481 0.0096707344 0.90376616 0.1559875 -1.8430355 -3.6557641 -4.2378988 -4.0776148 -3.5771222 -2.4570491][-2.6352868 -1.973465 -2.8171918 -2.8726707 -2.6712511 -1.7082332 -0.020254135 0.86452341 0.5385375 -1.3095863 -3.0870452 -3.6583467 -3.494585 -3.0886474 -2.195441][-1.2749337 -0.64945614 -1.7785606 -2.1332622 -2.1993041 -1.5570226 -0.26557982 0.44759393 0.25870538 -1.3830779 -2.9942119 -3.6299183 -3.6586037 -3.3429384 -2.4946127][-0.81161189 -0.087363958 -1.1825212 -1.5536339 -1.9539428 -1.8907142 -1.1888298 -0.68934059 -0.78157878 -2.0991197 -3.3443582 -3.9090912 -4.081634 -3.8891659 -3.1573951][-1.4565555 -0.1562283 -0.90017283 -1.102035 -1.5953987 -1.8212172 -1.4078414 -1.0906963 -1.2187673 -2.2702427 -3.1385679 -3.6571889 -3.9920177 -4.0560675 -3.6717443]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 07:50:16.808187: step 28010, loss = 0.21, batch loss = 0.18 (35.3 examples/sec; 0.227 sec/batch; 19h:10m:52s remains)
INFO - root - 2017-12-15 07:50:19.059722: step 28020, loss = 0.23, batch loss = 0.20 (35.4 examples/sec; 0.226 sec/batch; 19h:05m:54s remains)
INFO - root - 2017-12-15 07:50:21.296698: step 28030, loss = 0.19, batch loss = 0.16 (34.7 examples/sec; 0.230 sec/batch; 19h:29m:26s remains)
INFO - root - 2017-12-15 07:50:23.550469: step 28040, loss = 0.24, batch loss = 0.21 (34.6 examples/sec; 0.231 sec/batch; 19h:32m:08s remains)
INFO - root - 2017-12-15 07:50:25.809958: step 28050, loss = 0.17, batch loss = 0.13 (36.0 examples/sec; 0.222 sec/batch; 18h:47m:13s remains)
INFO - root - 2017-12-15 07:50:28.089952: step 28060, loss = 0.28, batch loss = 0.25 (34.9 examples/sec; 0.229 sec/batch; 19h:21m:45s remains)
INFO - root - 2017-12-15 07:50:30.362059: step 28070, loss = 0.21, batch loss = 0.18 (34.9 examples/sec; 0.229 sec/batch; 19h:21m:34s remains)
INFO - root - 2017-12-15 07:50:32.625633: step 28080, loss = 0.25, batch loss = 0.22 (35.6 examples/sec; 0.224 sec/batch; 18h:58m:36s remains)
INFO - root - 2017-12-15 07:50:34.887004: step 28090, loss = 0.23, batch loss = 0.20 (35.1 examples/sec; 0.228 sec/batch; 19h:17m:53s remains)
INFO - root - 2017-12-15 07:50:37.172399: step 28100, loss = 0.32, batch loss = 0.29 (35.6 examples/sec; 0.225 sec/batch; 19h:01m:03s remains)
2017-12-15 07:50:37.439841: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.246201 -4.1483946 -3.8719511 -3.8885355 -4.6523094 -5.0820045 -5.0973086 -5.1362653 -5.2549295 -5.2588758 -5.0264091 -4.6569252 -4.2946138 -4.1772885 -4.1770482][-5.3030338 -4.0209012 -3.7164392 -3.6262236 -4.6641626 -5.4020004 -5.4239531 -5.4876351 -5.661211 -5.6007824 -5.2674069 -4.8844128 -4.5730176 -4.6028986 -4.7172141][-5.8775959 -3.3978858 -3.1083808 -2.9160371 -4.1179147 -5.1653872 -5.0892239 -5.0500946 -5.1558423 -4.9082026 -4.4381762 -4.152524 -4.0749421 -4.3952847 -4.76163][-5.5163283 -2.56244 -2.4177709 -2.1760266 -3.3760667 -4.6046839 -4.3221951 -4.0335755 -3.9217777 -3.40376 -2.7551727 -2.5729029 -2.7589078 -3.4342487 -4.1148224][-4.5794868 -1.4265157 -1.4716618 -1.3031483 -2.4237525 -3.7113175 -3.2322848 -2.601568 -2.16003 -1.3393022 -0.52182579 -0.42692256 -0.88225615 -1.9459345 -2.9611974][-3.7914786 -0.62010586 -0.71992147 -0.48128295 -1.3584107 -2.5538676 -1.9821969 -1.0552704 -0.32363844 0.672673 1.5527728 1.5680335 0.90784764 -0.3982991 -1.6185279][-3.0085702 -0.2832762 -0.27075219 0.20965695 -0.31591165 -1.2978823 -0.79312658 0.1903975 1.0389311 2.0277534 2.771317 2.638515 1.8572555 0.56292987 -0.63594222][-2.5405259 0.22383451 0.4478941 1.0546277 0.69025278 -0.070981741 0.27913642 1.0839803 1.8264284 2.6641054 3.1994314 2.878933 2.1009445 0.99749041 -0.0463984][-2.0499172 0.724283 1.1145554 1.6480653 1.1671181 0.58417392 0.84859824 1.3889894 1.8835354 2.4598198 2.7201729 2.2732582 1.6410491 0.87222028 0.14279008][-1.7802638 0.88009691 1.3422801 1.6358402 0.8764627 0.49738789 0.82092309 1.2178841 1.501071 1.873724 1.861002 1.27826 0.71538353 0.12577415 -0.28730404][-1.1420779 1.3691127 1.6979458 1.4480543 0.1026895 -0.25172138 0.13371396 0.48630023 0.69939995 0.998014 0.82349324 0.20836186 -0.31819332 -0.81891429 -0.93612361][-0.64544809 1.8026042 2.0592833 1.2469685 -0.83170307 -1.3263897 -0.91717136 -0.59241128 -0.37409365 -0.021549463 -0.10505056 -0.53831196 -0.98737586 -1.4486066 -1.4033206][-0.81187582 1.7244341 2.038969 0.84427047 -1.8565005 -2.5743968 -2.131727 -1.6897559 -1.2786174 -0.78107452 -0.66922104 -0.85691977 -1.3012509 -1.8076288 -1.7343223][-1.7657955 0.9871819 1.5315773 0.26781464 -2.7639689 -3.738462 -3.3877182 -2.8945003 -2.3235848 -1.7215078 -1.3724716 -1.2624152 -1.6579598 -2.2099814 -2.2661722][-3.528707 -0.44190085 0.47817802 -0.48700082 -3.3699374 -4.446825 -4.2494345 -3.8566949 -3.3427072 -2.8569083 -2.4369578 -2.1035223 -2.3591709 -2.7869582 -2.9172494]]...]
INFO - root - 2017-12-15 07:50:39.717912: step 28110, loss = 0.23, batch loss = 0.19 (35.8 examples/sec; 0.224 sec/batch; 18h:53m:52s remains)
INFO - root - 2017-12-15 07:50:41.957947: step 28120, loss = 0.30, batch loss = 0.27 (35.6 examples/sec; 0.225 sec/batch; 18h:59m:00s remains)
INFO - root - 2017-12-15 07:50:44.220559: step 28130, loss = 0.38, batch loss = 0.34 (35.5 examples/sec; 0.226 sec/batch; 19h:04m:39s remains)
INFO - root - 2017-12-15 07:50:46.510346: step 28140, loss = 0.26, batch loss = 0.23 (32.8 examples/sec; 0.244 sec/batch; 20h:37m:01s remains)
INFO - root - 2017-12-15 07:50:48.773976: step 28150, loss = 0.25, batch loss = 0.21 (35.5 examples/sec; 0.225 sec/batch; 19h:02m:39s remains)
INFO - root - 2017-12-15 07:50:51.020057: step 28160, loss = 0.28, batch loss = 0.25 (34.9 examples/sec; 0.229 sec/batch; 19h:22m:47s remains)
INFO - root - 2017-12-15 07:50:53.310447: step 28170, loss = 0.19, batch loss = 0.16 (34.6 examples/sec; 0.231 sec/batch; 19h:31m:04s remains)
INFO - root - 2017-12-15 07:50:55.562094: step 28180, loss = 0.20, batch loss = 0.16 (36.4 examples/sec; 0.220 sec/batch; 18h:34m:10s remains)
INFO - root - 2017-12-15 07:50:57.857168: step 28190, loss = 0.23, batch loss = 0.20 (36.1 examples/sec; 0.221 sec/batch; 18h:42m:54s remains)
INFO - root - 2017-12-15 07:51:00.129544: step 28200, loss = 0.27, batch loss = 0.24 (34.4 examples/sec; 0.232 sec/batch; 19h:37m:46s remains)
2017-12-15 07:51:00.411085: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5764637 -4.9047346 -5.885848 -5.8932314 -5.9691749 -6.513679 -6.1535211 -5.4734869 -4.8372736 -3.7440963 -3.1284385 -1.7698914 -0.33924317 0.25428009 0.20331216][-1.8843921 -4.6564274 -5.9203644 -6.4092159 -6.7572217 -7.0155673 -6.5237513 -5.7575188 -5.054482 -4.0876293 -3.471921 -2.0776048 -0.8468051 -0.49058437 -0.81125736][-2.6612222 -4.5492253 -5.9685488 -6.7004437 -7.0604982 -6.98227 -6.4194112 -5.6141129 -4.986084 -4.4567318 -4.020205 -2.802887 -1.7778109 -1.4594371 -1.7771546][-3.4445848 -4.5526137 -5.8156967 -6.4448318 -6.5709472 -6.1735954 -5.4827833 -4.558588 -4.1255331 -4.2602749 -4.2688336 -3.6209657 -3.136636 -2.9941387 -3.1773427][-3.8927777 -4.3223248 -5.3547745 -5.7798672 -5.3916788 -4.378109 -3.1800909 -1.8435977 -1.6094472 -2.5505722 -3.3658948 -3.7899642 -4.0015855 -4.0194812 -4.0126181][-4.1674323 -3.9397669 -4.6643982 -4.7333198 -3.7174454 -2.1768372 -0.48070467 1.4097724 1.5309939 -0.17493916 -1.7683485 -3.2915258 -4.0826249 -4.1631107 -3.8971825][-3.9760995 -3.6846614 -4.1320887 -3.916594 -2.23303 -0.14136839 2.1379023 4.5389013 4.5514784 2.2802582 0.063030958 -2.362407 -3.3734291 -3.2383785 -2.720906][-4.4630508 -3.9332581 -4.1947327 -3.8289983 -1.7921762 0.9399004 3.7848668 6.4163203 6.1555486 3.4701138 0.86724186 -1.8979868 -2.6428876 -2.1212754 -1.2665694][-5.4843383 -4.9638033 -5.1496859 -4.7076406 -2.6546042 0.17004633 3.0535364 5.338479 4.9697814 2.4694066 -0.14199448 -2.6400828 -2.9375155 -2.1236274 -1.1302625][-6.4694109 -6.1632423 -6.3378782 -5.7509403 -3.7957778 -1.1641713 1.3210573 2.9405708 2.464313 0.43444681 -1.7978815 -3.4791789 -3.0748067 -1.8335029 -0.46743476][-7.3880553 -7.2754292 -7.5178409 -7.0110469 -5.37632 -3.1505549 -1.1903526 -0.14254522 -0.54507637 -1.908011 -3.417038 -4.0015159 -3.0503168 -1.5118097 0.21906924][-8.0807 -8.06397 -8.5060234 -8.2336216 -7.053597 -5.4252591 -4.0748062 -3.3562002 -3.5525041 -4.3563013 -4.9434185 -4.4428854 -3.319406 -1.7647612 0.20885491][-8.0196524 -7.8974161 -8.3883371 -8.3871965 -7.7886629 -6.9151359 -6.2103405 -5.8282895 -5.7659454 -6.0515251 -5.9215374 -4.7931576 -3.6316729 -2.1979659 -0.34875762][-7.6513352 -7.2844081 -7.6731405 -7.791748 -7.57901 -7.3231483 -7.2047777 -7.0460067 -6.8812304 -6.9082117 -6.317863 -4.9071817 -3.8811479 -2.7604673 -1.2217815][-7.2692542 -6.5744276 -6.7911777 -6.910038 -6.9156861 -6.9715819 -7.1891336 -7.3426247 -7.4478951 -7.7165384 -7.2640176 -6.0011597 -5.0726213 -4.2013283 -3.1769495]]...]
INFO - root - 2017-12-15 07:51:02.676567: step 28210, loss = 0.29, batch loss = 0.26 (35.1 examples/sec; 0.228 sec/batch; 19h:16m:47s remains)
INFO - root - 2017-12-15 07:51:04.975458: step 28220, loss = 0.27, batch loss = 0.24 (34.3 examples/sec; 0.233 sec/batch; 19h:41m:56s remains)
INFO - root - 2017-12-15 07:51:07.239461: step 28230, loss = 0.19, batch loss = 0.16 (35.5 examples/sec; 0.225 sec/batch; 19h:01m:38s remains)
INFO - root - 2017-12-15 07:51:09.495394: step 28240, loss = 0.22, batch loss = 0.18 (36.2 examples/sec; 0.221 sec/batch; 18h:41m:27s remains)
INFO - root - 2017-12-15 07:51:11.785369: step 28250, loss = 0.18, batch loss = 0.15 (34.6 examples/sec; 0.231 sec/batch; 19h:31m:00s remains)
INFO - root - 2017-12-15 07:51:14.030048: step 28260, loss = 0.21, batch loss = 0.17 (35.6 examples/sec; 0.225 sec/batch; 18h:58m:40s remains)
INFO - root - 2017-12-15 07:51:16.285506: step 28270, loss = 0.24, batch loss = 0.21 (33.6 examples/sec; 0.238 sec/batch; 20h:07m:00s remains)
INFO - root - 2017-12-15 07:51:18.567761: step 28280, loss = 0.24, batch loss = 0.21 (34.9 examples/sec; 0.229 sec/batch; 19h:21m:05s remains)
INFO - root - 2017-12-15 07:51:20.840907: step 28290, loss = 0.23, batch loss = 0.20 (35.5 examples/sec; 0.225 sec/batch; 19h:02m:53s remains)
INFO - root - 2017-12-15 07:51:23.106536: step 28300, loss = 0.23, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 18h:46m:58s remains)
2017-12-15 07:51:23.419063: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4574294 -3.900506 -3.5748515 -3.3532269 -3.7879958 -5.0971327 -6.4817991 -6.8395958 -7.0335636 -7.1072063 -7.0820484 -6.9752789 -6.6615353 -6.5396233 -6.943244][-4.4264421 -4.5155044 -5.0029635 -4.9409728 -4.8750763 -5.4732366 -6.1274652 -6.2748613 -6.4575682 -6.6824245 -7.1358938 -7.5643311 -7.5644331 -7.6561937 -7.9764338][-4.9519053 -4.5702305 -5.6816931 -5.62513 -4.9358139 -4.686367 -4.37647 -4.1429024 -4.3427372 -4.7423754 -5.6932397 -6.7461386 -7.0441875 -7.1297054 -7.213028][-4.6909647 -4.035749 -5.6380625 -5.6088681 -4.3361669 -3.3246231 -2.1145957 -1.4448953 -1.6379833 -2.2497191 -3.7068098 -5.3547173 -5.9393525 -5.8494282 -5.5815845][-4.2274733 -3.307972 -4.8794007 -4.6430717 -2.7792649 -1.1285083 0.83345938 2.0209939 1.78845 0.59450722 -1.7007313 -4.1547322 -5.2234755 -4.9545679 -4.3187871][-4.0274019 -3.1190381 -4.0847592 -3.3384838 -0.89271677 1.3133686 3.8622615 5.6240768 5.3003187 3.3222597 0.28091645 -2.8172152 -4.329731 -3.9293489 -3.0054905][-4.3620572 -3.5834682 -3.5544019 -2.2448444 0.72961664 3.3682539 6.2016783 8.3031082 7.8395634 5.1023474 1.5187345 -1.8778172 -3.6602669 -3.3199165 -2.2515943][-5.4429 -4.51173 -3.6501958 -2.1194556 0.96592236 3.8325226 6.6505156 8.7828741 8.2425508 5.1285667 1.4812088 -1.7612987 -3.6063862 -3.587554 -2.7022669][-6.6501703 -5.9003444 -4.9547176 -3.5942018 -0.79799116 1.8871834 4.367528 6.3146944 5.8422308 2.9707258 -0.2150135 -2.9237113 -4.4728985 -4.5886 -3.9231482][-7.3792343 -7.0366259 -6.4947882 -5.5344505 -3.2721477 -1.1098139 0.80559874 2.363776 2.0762775 -0.19145823 -2.7877309 -4.9396715 -6.1690626 -6.4079494 -5.8724627][-7.7711763 -8.0789347 -8.1173267 -7.643033 -5.9311228 -4.1612396 -2.6382391 -1.4296765 -1.5008304 -3.1418447 -5.2401915 -6.9608326 -7.8570766 -8.1310329 -7.6967649][-8.0938921 -9.0118618 -9.5846691 -9.46705 -8.2329369 -6.8183765 -5.5797434 -4.7153082 -4.7869329 -6.0340214 -7.7571297 -9.1789122 -9.7653923 -9.8245144 -9.3215361][-8.3656225 -9.531352 -10.364951 -10.520307 -9.7642679 -8.7570515 -7.8662214 -7.3385286 -7.5226393 -8.5243425 -9.8673782 -10.81336 -10.936177 -10.576449 -9.8212585][-8.31015 -9.1353388 -9.8208418 -10.075192 -9.7118092 -9.09288 -8.5521755 -8.3183975 -8.5964375 -9.3633022 -10.257832 -10.691952 -10.41596 -9.8147211 -9.064045][-7.3512468 -7.4673548 -7.7792187 -7.9273472 -7.8670683 -7.6872053 -7.5507917 -7.5875177 -7.8899155 -8.3700771 -8.8112154 -8.8969336 -8.5021725 -7.928401 -7.40767]]...]
INFO - root - 2017-12-15 07:51:25.695766: step 28310, loss = 0.20, batch loss = 0.17 (34.8 examples/sec; 0.230 sec/batch; 19h:26m:07s remains)
INFO - root - 2017-12-15 07:51:28.008656: step 28320, loss = 0.20, batch loss = 0.16 (35.5 examples/sec; 0.225 sec/batch; 19h:01m:30s remains)
INFO - root - 2017-12-15 07:51:30.248864: step 28330, loss = 0.25, batch loss = 0.22 (35.7 examples/sec; 0.224 sec/batch; 18h:56m:48s remains)
INFO - root - 2017-12-15 07:51:32.513341: step 28340, loss = 0.16, batch loss = 0.13 (36.4 examples/sec; 0.220 sec/batch; 18h:33m:58s remains)
INFO - root - 2017-12-15 07:51:34.769424: step 28350, loss = 0.17, batch loss = 0.13 (35.0 examples/sec; 0.229 sec/batch; 19h:19m:33s remains)
INFO - root - 2017-12-15 07:51:37.026111: step 28360, loss = 0.20, batch loss = 0.16 (35.7 examples/sec; 0.224 sec/batch; 18h:55m:16s remains)
INFO - root - 2017-12-15 07:51:39.331500: step 28370, loss = 0.24, batch loss = 0.21 (36.0 examples/sec; 0.222 sec/batch; 18h:45m:22s remains)
INFO - root - 2017-12-15 07:51:41.626900: step 28380, loss = 0.22, batch loss = 0.19 (33.5 examples/sec; 0.239 sec/batch; 20h:09m:36s remains)
INFO - root - 2017-12-15 07:51:43.886513: step 28390, loss = 0.20, batch loss = 0.16 (34.9 examples/sec; 0.230 sec/batch; 19h:23m:15s remains)
INFO - root - 2017-12-15 07:51:46.152642: step 28400, loss = 0.18, batch loss = 0.15 (35.6 examples/sec; 0.225 sec/batch; 18h:57m:59s remains)
2017-12-15 07:51:46.449593: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6725638 -4.4703684 -4.53095 -4.8179841 -4.9742208 -5.1423655 -5.4193826 -5.4031849 -5.5453448 -5.96735 -5.7632217 -5.2961454 -5.06962 -5.2548 -5.9553275][-4.8886409 -4.2542315 -4.246664 -4.593173 -4.8689938 -5.165421 -5.5603943 -5.5424261 -5.6488028 -6.0689106 -5.8326912 -5.3275633 -5.17453 -5.3223124 -5.9133387][-5.024066 -3.655899 -3.4247303 -3.7039485 -4.109663 -4.587141 -5.2113876 -5.4386353 -5.7199697 -6.2382393 -5.971838 -5.461184 -5.4132109 -5.5299911 -5.9119511][-5.4643855 -3.0789883 -2.5295496 -2.370533 -2.5602016 -2.9645133 -3.5519493 -4.0462 -4.7760468 -5.6760578 -5.6316924 -5.196619 -5.2645369 -5.3297071 -5.3627224][-5.6678362 -2.19778 -1.3943672 -0.655118 -0.35968959 -0.23442721 -0.41175652 -1.0915475 -2.4365048 -4.048615 -4.5584326 -4.3523273 -4.5370831 -4.6430244 -4.3435392][-5.9315796 -2.4215236 -1.3822622 0.08173871 1.3231733 2.471122 2.8669579 2.0306923 0.022207737 -2.3596632 -3.6009476 -3.7485242 -4.1474609 -4.2984858 -3.6474419][-6.212832 -3.1183541 -2.0813086 0.041989088 2.3414814 4.3594122 5.2603273 4.400856 2.0242488 -0.89343178 -2.7484059 -3.1546333 -3.7275491 -3.8808141 -2.9333835][-6.1753025 -3.7359693 -2.916429 -0.5968343 2.1486962 4.5976009 5.8104362 5.0385017 2.6365273 -0.36248255 -2.4686859 -2.9689007 -3.624958 -3.7503605 -2.578943][-6.7120628 -4.8223782 -4.3262997 -2.2681351 0.2249136 2.7033689 4.0582037 3.4073637 1.2536798 -1.3312842 -2.9595232 -3.2018614 -3.8484282 -3.7588539 -2.2925742][-7.2434883 -5.6418047 -5.3361607 -3.7879581 -1.953693 0.11313248 1.4689252 1.0730085 -0.59518075 -2.5117817 -3.4327672 -3.2367511 -3.6528289 -3.3040047 -1.8813398][-7.3914928 -5.8956032 -5.5480537 -4.2960448 -2.9895329 -1.422976 -0.40013528 -0.72699082 -1.8517146 -3.0758331 -3.3057833 -2.9023395 -3.2765296 -3.0207114 -2.0374207][-7.6591072 -6.301321 -5.9697428 -4.9096022 -4.0373259 -2.9305091 -2.2063363 -2.491586 -3.0939786 -3.8535151 -3.4921618 -3.062505 -3.4640164 -3.2957091 -2.659003][-7.5586643 -6.3528872 -6.303442 -5.6241541 -5.1687679 -4.55663 -4.2791328 -4.5761423 -4.7830067 -5.0479574 -4.1524096 -3.4533892 -3.5521383 -3.3782988 -3.1301165][-6.7914362 -5.5421271 -5.6616354 -5.2935228 -5.1402593 -4.8959455 -5.0638523 -5.5233154 -5.693119 -5.6754065 -4.5650511 -3.7246451 -3.6337848 -3.6052341 -3.7140863][-6.1725874 -4.7263432 -4.7653685 -4.5616465 -4.6174984 -4.6272349 -4.9377389 -5.3516493 -5.5377045 -5.5527854 -4.7192006 -4.1450911 -4.2567673 -4.5394211 -4.8467731]]...]
INFO - root - 2017-12-15 07:51:48.710487: step 28410, loss = 0.25, batch loss = 0.21 (34.8 examples/sec; 0.230 sec/batch; 19h:24m:06s remains)
INFO - root - 2017-12-15 07:51:50.987870: step 28420, loss = 0.29, batch loss = 0.26 (34.3 examples/sec; 0.233 sec/batch; 19h:41m:02s remains)
INFO - root - 2017-12-15 07:51:53.258880: step 28430, loss = 0.41, batch loss = 0.38 (35.3 examples/sec; 0.227 sec/batch; 19h:08m:24s remains)
INFO - root - 2017-12-15 07:51:55.570843: step 28440, loss = 0.19, batch loss = 0.16 (34.1 examples/sec; 0.234 sec/batch; 19h:47m:29s remains)
INFO - root - 2017-12-15 07:51:57.869688: step 28450, loss = 0.22, batch loss = 0.18 (36.0 examples/sec; 0.222 sec/batch; 18h:45m:06s remains)
INFO - root - 2017-12-15 07:52:00.137075: step 28460, loss = 0.20, batch loss = 0.16 (36.2 examples/sec; 0.221 sec/batch; 18h:38m:55s remains)
INFO - root - 2017-12-15 07:52:02.437145: step 28470, loss = 0.21, batch loss = 0.18 (33.5 examples/sec; 0.239 sec/batch; 20h:09m:12s remains)
INFO - root - 2017-12-15 07:52:04.719358: step 28480, loss = 0.19, batch loss = 0.16 (35.2 examples/sec; 0.227 sec/batch; 19h:10m:11s remains)
INFO - root - 2017-12-15 07:52:06.971106: step 28490, loss = 0.27, batch loss = 0.23 (35.5 examples/sec; 0.225 sec/batch; 19h:00m:27s remains)
INFO - root - 2017-12-15 07:52:09.249571: step 28500, loss = 0.24, batch loss = 0.21 (35.1 examples/sec; 0.228 sec/batch; 19h:13m:58s remains)
2017-12-15 07:52:09.522668: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.978528 -6.7728004 -6.6625652 -6.8002706 -7.1045656 -7.4567938 -7.6661029 -7.6419821 -7.4520774 -7.1775723 -7.4045115 -7.8168025 -8.0427713 -7.8071356 -7.1417389][-8.2648544 -8.6749687 -8.4365368 -8.2556477 -8.0850143 -8.0108423 -8.0451984 -8.055891 -7.9841828 -7.8547378 -8.1976471 -8.8354225 -9.3469467 -9.191637 -8.3764668][-10.955952 -9.9584675 -9.6649675 -9.2654915 -8.6665487 -7.9751248 -7.5591006 -7.4311757 -7.5708389 -7.7301621 -8.1179943 -8.8544769 -9.5319176 -9.4440918 -8.6057158][-12.103111 -10.192298 -9.6886377 -9.0067921 -8.0090294 -6.68513 -5.8189697 -5.6661239 -6.2246904 -6.8945742 -7.3793368 -8.1335278 -8.8616915 -8.9458675 -8.316103][-11.909808 -9.5298119 -8.7715921 -7.7402868 -6.1867647 -4.0141497 -2.5793364 -2.5349429 -3.7797933 -5.3332672 -6.3457055 -7.2165909 -8.0002146 -8.3617344 -8.0036783][-10.769717 -8.0302382 -7.0744262 -5.9064651 -3.9862981 -0.9393785 1.39111 1.6026373 -0.19042158 -2.6777916 -4.5753794 -5.7683425 -6.8668776 -7.7083907 -7.7741051][-9.121109 -6.4956961 -5.3955221 -4.1487718 -1.8416703 2.0306196 5.2103739 5.6754646 3.6155906 0.26027346 -2.703548 -4.30897 -5.7830253 -7.07272 -7.5916648][-7.6560845 -5.1075382 -4.2161384 -3.1135583 -0.841676 3.2284636 6.775403 7.4216681 5.4103513 1.4411387 -2.3110597 -4.241086 -5.8246326 -7.2645993 -7.8964424][-6.9686613 -4.8836946 -4.4575243 -3.6881213 -1.9904064 1.4551861 4.796217 5.7113357 4.3259225 0.63552523 -3.1319613 -5.0915937 -6.6971993 -8.0459757 -8.4647923][-7.1929522 -5.705008 -5.6388655 -5.0726972 -3.8678885 -1.3357493 1.3952682 2.4492974 1.8193693 -1.1540672 -4.4463682 -6.3164568 -7.8504715 -8.9399033 -9.0479469][-7.5916681 -6.523025 -6.7808433 -6.3856049 -5.5696745 -4.0863056 -2.3579612 -1.5031226 -1.7456529 -3.9505718 -6.4929328 -8.0158939 -9.1607227 -9.6713467 -9.3463907][-7.9591727 -7.2453151 -7.8044219 -7.6513114 -7.1131115 -6.3891544 -5.4778829 -4.8706083 -4.8649178 -6.1681433 -7.7120075 -8.6382122 -9.3547878 -9.47538 -8.919425][-7.8516855 -7.1797915 -7.7922473 -7.8264933 -7.4466863 -7.0863519 -6.6453862 -6.2435408 -6.1612358 -6.7680154 -7.4993253 -8.0061264 -8.5266476 -8.5428677 -8.0420837][-7.371479 -6.4975877 -6.89522 -6.9651113 -6.7973409 -6.7588077 -6.6073046 -6.3748531 -6.2314429 -6.4087896 -6.7107425 -7.046876 -7.3716578 -7.31363 -6.9860034][-6.8068585 -5.7529669 -5.7844343 -5.7293558 -5.6990376 -5.8793893 -6.0168152 -5.9841328 -5.9058638 -5.933526 -6.0166507 -6.1137266 -6.2471924 -6.224566 -6.1001387]]...]
INFO - root - 2017-12-15 07:52:11.840628: step 28510, loss = 0.20, batch loss = 0.16 (34.1 examples/sec; 0.235 sec/batch; 19h:49m:19s remains)
INFO - root - 2017-12-15 07:52:14.151987: step 28520, loss = 0.16, batch loss = 0.13 (33.9 examples/sec; 0.236 sec/batch; 19h:53m:53s remains)
INFO - root - 2017-12-15 07:52:16.418901: step 28530, loss = 0.21, batch loss = 0.18 (34.4 examples/sec; 0.232 sec/batch; 19h:37m:24s remains)
INFO - root - 2017-12-15 07:52:18.751306: step 28540, loss = 0.26, batch loss = 0.23 (34.2 examples/sec; 0.234 sec/batch; 19h:43m:27s remains)
INFO - root - 2017-12-15 07:52:21.004739: step 28550, loss = 0.19, batch loss = 0.16 (34.0 examples/sec; 0.235 sec/batch; 19h:50m:20s remains)
INFO - root - 2017-12-15 07:52:23.262064: step 28560, loss = 0.31, batch loss = 0.28 (35.2 examples/sec; 0.227 sec/batch; 19h:11m:07s remains)
INFO - root - 2017-12-15 07:52:25.524993: step 28570, loss = 0.22, batch loss = 0.19 (34.7 examples/sec; 0.230 sec/batch; 19h:26m:28s remains)
INFO - root - 2017-12-15 07:52:27.800558: step 28580, loss = 0.18, batch loss = 0.15 (34.5 examples/sec; 0.232 sec/batch; 19h:35m:25s remains)
INFO - root - 2017-12-15 07:52:30.066836: step 28590, loss = 0.18, batch loss = 0.15 (36.5 examples/sec; 0.219 sec/batch; 18h:30m:08s remains)
INFO - root - 2017-12-15 07:52:32.350109: step 28600, loss = 0.21, batch loss = 0.17 (35.2 examples/sec; 0.227 sec/batch; 19h:11m:38s remains)
2017-12-15 07:52:32.653084: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.565964 -6.4240875 -5.6154666 -4.17591 -2.1603391 -0.92122889 0.31972146 0.41252637 -0.91819704 -2.0342531 -2.9790924 -3.7611861 -4.3375139 -5.3483553 -5.0554552][-3.9767942 -6.3930168 -5.7338133 -4.1789794 -1.986161 -0.39853358 1.2014239 1.567523 0.12283111 -1.3586562 -2.5478592 -3.1566544 -4.0929971 -5.4448833 -5.2847061][-4.5756826 -6.3171124 -5.7468128 -4.0118847 -1.5926251 0.42051864 2.3830123 3.040966 1.5463386 -0.3347398 -1.993439 -2.7946973 -3.9637303 -5.2639742 -5.1179414][-5.6970506 -6.3635521 -5.6844683 -3.6487617 -1.0179451 1.2749941 3.3093448 4.0013018 2.3744211 0.15385151 -1.7633121 -2.5788026 -3.7486534 -4.8762064 -4.7940221][-6.9102435 -6.8702612 -5.9976382 -3.6620927 -0.8818326 1.7686143 4.0648866 4.8305569 3.2216096 0.76781988 -1.3111329 -2.2563274 -3.5810466 -4.6424165 -4.5673637][-7.9314222 -7.5229478 -6.4461923 -3.9984291 -1.3380437 1.5486314 4.1753292 5.0056615 3.5129366 1.0608633 -1.045314 -2.0742526 -3.3659296 -4.3782005 -4.2590389][-8.3483391 -8.0000381 -6.868011 -4.5732541 -1.9782908 1.1584251 4.0569391 4.8815813 3.5062523 1.1054976 -0.929667 -1.9893534 -3.078475 -3.9939594 -3.8687196][-8.56868 -8.2055779 -7.1769786 -5.1335177 -2.6935241 0.59977651 3.7887859 4.7684574 3.6491036 1.4079165 -0.61803341 -1.6718044 -2.524575 -3.4702356 -3.5521445][-8.3949986 -7.8219566 -7.1084514 -5.6963606 -3.647759 -0.59682643 2.7600808 4.1408486 3.5927448 1.8665771 -0.025749445 -1.1793743 -1.9297609 -3.0046356 -3.444685][-8.0148621 -7.2199554 -6.8629389 -6.0348372 -4.4340911 -1.9393212 1.1789107 2.6224456 2.5980263 1.7012482 0.29592538 -0.89115071 -1.6549771 -2.7328882 -3.4528308][-7.1331916 -6.3240762 -6.386601 -6.1053996 -4.9591594 -3.3354802 -0.84461379 0.45107079 0.74432969 0.65428472 -0.18254256 -1.2959591 -1.8984702 -2.6202605 -3.398613][-6.220191 -5.5243521 -6.134304 -6.1649923 -5.3994374 -4.7013855 -2.9930172 -1.9680107 -1.4825706 -1.0000287 -1.1653157 -1.9420543 -2.3938725 -2.7880702 -3.6783891][-5.3859339 -4.9479094 -6.2488866 -6.4634905 -6.0312948 -6.1019011 -5.0485158 -4.2182088 -3.7924995 -2.9000795 -2.4348295 -2.7342751 -3.0134401 -3.2699807 -4.2719913][-4.6435828 -4.3018179 -6.3810892 -6.8117571 -6.6307921 -7.1993384 -6.62069 -5.8455553 -5.5745215 -4.7369342 -3.9337344 -3.8429952 -3.9598227 -4.1319442 -5.051363][-4.4254751 -3.8779695 -6.4921522 -7.0235839 -6.8339748 -7.3546429 -6.9831929 -6.1824675 -5.9887128 -5.3865552 -4.6008148 -4.4211359 -4.5726376 -4.8556309 -5.732502]]...]
INFO - root - 2017-12-15 07:52:34.904127: step 28610, loss = 0.30, batch loss = 0.27 (35.5 examples/sec; 0.225 sec/batch; 19h:00m:56s remains)
INFO - root - 2017-12-15 07:52:37.186037: step 28620, loss = 0.22, batch loss = 0.19 (36.4 examples/sec; 0.220 sec/batch; 18h:33m:22s remains)
INFO - root - 2017-12-15 07:52:39.501338: step 28630, loss = 0.27, batch loss = 0.23 (34.0 examples/sec; 0.235 sec/batch; 19h:50m:24s remains)
INFO - root - 2017-12-15 07:52:41.780710: step 28640, loss = 0.22, batch loss = 0.19 (34.7 examples/sec; 0.230 sec/batch; 19h:26m:14s remains)
INFO - root - 2017-12-15 07:52:44.031851: step 28650, loss = 0.31, batch loss = 0.28 (35.7 examples/sec; 0.224 sec/batch; 18h:53m:19s remains)
INFO - root - 2017-12-15 07:52:46.315869: step 28660, loss = 0.21, batch loss = 0.17 (33.5 examples/sec; 0.239 sec/batch; 20h:09m:00s remains)
INFO - root - 2017-12-15 07:52:48.563521: step 28670, loss = 0.20, batch loss = 0.17 (36.3 examples/sec; 0.220 sec/batch; 18h:35m:28s remains)
INFO - root - 2017-12-15 07:52:50.816219: step 28680, loss = 0.24, batch loss = 0.20 (35.9 examples/sec; 0.223 sec/batch; 18h:49m:31s remains)
INFO - root - 2017-12-15 07:52:53.109847: step 28690, loss = 0.26, batch loss = 0.23 (33.7 examples/sec; 0.238 sec/batch; 20h:03m:31s remains)
INFO - root - 2017-12-15 07:52:55.374492: step 28700, loss = 0.20, batch loss = 0.17 (35.3 examples/sec; 0.227 sec/batch; 19h:06m:59s remains)
2017-12-15 07:52:55.647324: I tensorflow/core/kernels/logging_ops.cc:79] [[[5.198864 2.0746262 -0.65378487 -3.1958492 -5.3523521 -6.4841576 -6.7326908 -6.5736179 -6.7425241 -6.3499565 -6.1560555 -5.8843865 -5.1407862 -4.3192234 -3.6997485][1.2555635 -1.21352 -3.385325 -5.1439381 -6.7434521 -7.2466631 -7.0598259 -6.8202 -7.0881176 -6.9405146 -6.7934208 -6.5192423 -5.9757662 -5.175468 -4.3551521][-2.6649692 -4.0397267 -5.5669332 -6.60104 -7.4753075 -7.548542 -7.1017132 -6.7132187 -6.8944349 -6.8555336 -6.9101229 -6.77518 -6.4501152 -5.7947397 -5.0990095][-5.30153 -5.7176895 -6.5581589 -6.8835869 -7.2294197 -6.9210663 -6.2320137 -5.6566982 -5.6938467 -5.9709988 -6.4771938 -6.5896339 -6.4875154 -5.9679661 -5.4673438][-6.7387571 -6.4492769 -7.1283979 -6.9849997 -6.6516075 -5.7002077 -4.40905 -3.2417035 -3.1032271 -3.9640517 -5.3895268 -6.1980739 -6.4775066 -6.1325626 -5.6461887][-6.7116871 -6.3311453 -7.1936026 -7.0811682 -6.0081139 -4.133153 -1.7986896 -0.091176987 -0.22410059 -1.5833526 -3.7618604 -5.4757533 -6.5188665 -6.3987112 -5.8937149][-5.2701454 -5.25366 -6.1377659 -5.9129329 -4.3720531 -1.6977371 1.591548 3.2041576 2.4851735 0.70322108 -1.9653655 -4.3799572 -6.1657352 -6.5850878 -6.0378876][-3.6422553 -3.6717727 -5.085393 -4.9329987 -2.9607475 0.23994517 4.3638029 6.0484228 4.8009291 2.5205944 -0.38382733 -3.1778619 -5.2646723 -6.3005571 -6.0153856][-2.66343 -2.7019413 -4.5399446 -4.6902447 -2.6226778 0.81065631 5.28539 6.8782635 5.2232141 2.8252623 0.052781105 -2.739974 -4.7354927 -5.8502293 -5.6347761][-2.8199673 -2.9390857 -4.7735357 -5.0250645 -3.2307718 -0.23670602 3.782182 5.0111456 3.1917603 0.95834708 -1.634985 -4.1230688 -5.5094624 -6.1780376 -5.6709847][-4.0601349 -4.09688 -5.5872784 -5.8230247 -4.4092369 -2.1123431 1.1205482 2.0769255 0.33360243 -1.6252365 -3.9720316 -6.0019717 -6.7745953 -6.9437227 -6.3261747][-5.680089 -5.49652 -6.4699059 -6.578701 -5.5127969 -3.6370943 -1.0985339 -0.348598 -1.7409518 -3.3744826 -5.3911457 -6.9148607 -7.4008961 -7.4174814 -6.967433][-6.992631 -6.5966978 -7.09266 -6.9431753 -5.8039346 -4.1915889 -2.0496869 -1.2716389 -2.3852146 -4.0058861 -5.7934222 -7.12901 -7.60643 -7.5924339 -7.25725][-7.5463171 -7.1475611 -7.4409466 -7.0506277 -5.5252128 -3.7755055 -1.6800718 -0.96578765 -2.128324 -4.1747012 -6.05052 -7.3776922 -7.9291468 -7.7664938 -7.2478542][-7.6560411 -7.1500053 -7.118165 -6.2435923 -4.0404177 -1.6593708 0.41508007 0.54527044 -1.173689 -3.8022704 -6.0829678 -7.5039358 -8.1576366 -7.7856455 -6.8226881]]...]
INFO - root - 2017-12-15 07:52:57.961563: step 28710, loss = 0.22, batch loss = 0.19 (35.0 examples/sec; 0.229 sec/batch; 19h:17m:07s remains)
INFO - root - 2017-12-15 07:53:00.262904: step 28720, loss = 0.25, batch loss = 0.22 (32.2 examples/sec; 0.249 sec/batch; 20h:59m:44s remains)
INFO - root - 2017-12-15 07:53:02.548089: step 28730, loss = 0.25, batch loss = 0.22 (35.4 examples/sec; 0.226 sec/batch; 19h:03m:45s remains)
INFO - root - 2017-12-15 07:53:04.832936: step 28740, loss = 0.23, batch loss = 0.20 (34.6 examples/sec; 0.231 sec/batch; 19h:31m:53s remains)
INFO - root - 2017-12-15 07:53:07.099031: step 28750, loss = 0.15, batch loss = 0.12 (35.1 examples/sec; 0.228 sec/batch; 19h:13m:48s remains)
INFO - root - 2017-12-15 07:53:09.385903: step 28760, loss = 0.21, batch loss = 0.17 (35.7 examples/sec; 0.224 sec/batch; 18h:53m:20s remains)
INFO - root - 2017-12-15 07:53:11.647512: step 28770, loss = 0.19, batch loss = 0.15 (35.6 examples/sec; 0.225 sec/batch; 18h:58m:17s remains)
INFO - root - 2017-12-15 07:53:13.936820: step 28780, loss = 0.17, batch loss = 0.13 (35.7 examples/sec; 0.224 sec/batch; 18h:53m:47s remains)
INFO - root - 2017-12-15 07:53:16.196944: step 28790, loss = 0.24, batch loss = 0.21 (36.1 examples/sec; 0.222 sec/batch; 18h:41m:17s remains)
INFO - root - 2017-12-15 07:53:18.456247: step 28800, loss = 0.21, batch loss = 0.18 (35.6 examples/sec; 0.225 sec/batch; 18h:57m:28s remains)
2017-12-15 07:53:18.738068: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7060008 -3.1498587 -1.7677376 -0.85577917 -1.1085111 -1.2736003 -1.6456618 -3.2927389 -4.6457887 -5.2885747 -6.3679414 -7.1874261 -7.6874137 -6.9205451 -5.4882317][-2.7484765 -2.5667779 -1.7746431 -1.4786193 -1.9904246 -2.2536182 -2.4313645 -4.0152674 -5.2269425 -5.7597585 -7.0491271 -7.8329911 -8.2700148 -7.3857255 -5.9942074][-3.0045021 -1.7189729 -1.404242 -1.5439391 -2.1737754 -2.446918 -2.4510155 -3.7729836 -4.6999717 -5.1736231 -6.8904591 -7.679122 -8.22154 -7.3191671 -5.9248629][-2.9079127 -0.61890256 -0.48040462 -0.85065675 -1.5341845 -1.5353851 -1.1493856 -1.9591928 -2.5929596 -3.3251829 -5.856163 -7.0262823 -7.7675624 -7.0838919 -5.8374567][-2.7971339 0.23067284 0.42886209 -0.1359992 -0.95930839 -0.55957758 0.34492946 -0.18499184 -0.79283583 -1.8172377 -4.9454708 -6.1580758 -7.0862327 -6.8437982 -5.8054934][-3.0574 -0.022875071 0.29582095 -0.14747429 -0.81432843 0.085223913 1.5437968 1.2385414 0.64884067 -0.55925858 -4.0573297 -5.6668005 -6.9873018 -7.0519943 -6.101511][-3.2734501 -1.0492527 -0.58728468 -0.81769896 -1.2169321 0.36666083 2.459796 2.413898 1.8458147 0.49042845 -3.3200793 -5.4011312 -6.97796 -7.2883997 -6.4335904][-4.6535821 -2.7045891 -2.1265037 -2.0103385 -1.9922901 0.095384121 2.5258741 2.8246818 2.598927 1.3325329 -2.3525822 -4.61648 -6.54446 -7.2092161 -6.4683805][-6.4218349 -4.7436008 -4.1980324 -3.9217479 -3.4219389 -0.87974763 1.6408074 2.2163167 2.3096957 1.1381731 -2.2107317 -4.2475386 -6.013268 -6.7552757 -6.106205][-8.1978025 -7.0159111 -6.6930456 -6.3337831 -5.396729 -2.4881203 -0.088205814 0.47263408 0.64740133 -0.48040342 -3.1303077 -4.4467349 -5.8425465 -6.465354 -5.8755045][-9.2827168 -8.5108671 -8.3821764 -7.9782667 -6.8644805 -3.9794436 -1.7809094 -1.1814966 -0.8052696 -1.9864248 -4.0080342 -4.7388391 -5.8572311 -6.466352 -5.8888865][-8.7918606 -8.1400509 -8.1021185 -7.7546215 -6.8731046 -4.4218249 -2.6910338 -2.2781277 -1.9359155 -3.1958623 -4.5459256 -4.8781986 -5.9370852 -6.6518664 -6.1985197][-7.0045776 -6.0263457 -5.9640932 -5.7970915 -5.4267592 -3.7188635 -2.6495049 -2.6581075 -2.5988927 -4.0249557 -4.8886251 -5.1172152 -6.3847036 -7.2912335 -6.874167][-4.3119287 -2.9258041 -3.01231 -3.1885915 -3.3643155 -2.4252558 -2.064646 -2.4562941 -2.6633399 -4.427947 -5.3233047 -5.7853389 -7.2758145 -8.1676636 -7.5954003][-1.8976165 -0.078238249 -0.41724336 -1.0428234 -1.6501048 -1.3256594 -1.5739719 -2.226855 -2.702189 -4.8837605 -5.9096842 -6.3918824 -7.762958 -8.4753494 -7.7300792]]...]
INFO - root - 2017-12-15 07:53:20.999253: step 28810, loss = 0.18, batch loss = 0.15 (34.0 examples/sec; 0.236 sec/batch; 19h:52m:15s remains)
INFO - root - 2017-12-15 07:53:23.325013: step 28820, loss = 0.25, batch loss = 0.22 (35.7 examples/sec; 0.224 sec/batch; 18h:54m:44s remains)
INFO - root - 2017-12-15 07:53:25.597377: step 28830, loss = 0.22, batch loss = 0.19 (35.3 examples/sec; 0.227 sec/batch; 19h:06m:46s remains)
INFO - root - 2017-12-15 07:53:27.855374: step 28840, loss = 0.26, batch loss = 0.23 (35.2 examples/sec; 0.227 sec/batch; 19h:10m:41s remains)
INFO - root - 2017-12-15 07:53:30.145212: step 28850, loss = 0.33, batch loss = 0.29 (36.4 examples/sec; 0.220 sec/batch; 18h:32m:43s remains)
INFO - root - 2017-12-15 07:53:32.419469: step 28860, loss = 0.24, batch loss = 0.21 (35.1 examples/sec; 0.228 sec/batch; 19h:13m:03s remains)
INFO - root - 2017-12-15 07:53:34.679080: step 28870, loss = 0.23, batch loss = 0.20 (34.6 examples/sec; 0.231 sec/batch; 19h:28m:24s remains)
INFO - root - 2017-12-15 07:53:36.972804: step 28880, loss = 0.26, batch loss = 0.23 (35.0 examples/sec; 0.229 sec/batch; 19h:17m:31s remains)
INFO - root - 2017-12-15 07:53:39.292018: step 28890, loss = 0.21, batch loss = 0.18 (35.9 examples/sec; 0.223 sec/batch; 18h:46m:22s remains)
INFO - root - 2017-12-15 07:53:41.576841: step 28900, loss = 0.30, batch loss = 0.27 (34.9 examples/sec; 0.229 sec/batch; 19h:18m:29s remains)
2017-12-15 07:53:41.865633: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7997622 -6.8935127 -7.2874432 -7.4925861 -7.4366179 -7.2473006 -7.0128655 -6.8065152 -6.76647 -6.9209394 -7.1139541 -7.2331338 -7.1241527 -6.8381405 -6.5027771][-5.1828966 -7.2591457 -7.5484209 -7.4723487 -6.9861035 -6.3697333 -5.8649249 -5.5914946 -5.7443647 -6.2321315 -6.7350812 -7.0765743 -7.0476913 -6.7706451 -6.4727716][-6.5019617 -7.4604683 -7.6462674 -7.2321877 -6.2171831 -5.0707536 -4.1820188 -3.7889342 -4.185174 -5.0610557 -5.9778142 -6.6837831 -6.882185 -6.6473336 -6.2799869][-7.6436191 -7.5444531 -7.4872236 -6.5862765 -4.9515429 -3.208128 -1.8048282 -1.1768984 -1.708261 -2.9195313 -4.4080772 -5.7389975 -6.4435511 -6.4951715 -6.16389][-7.7971935 -7.1520395 -6.6677327 -5.1808348 -2.9436233 -0.71295488 1.0889971 1.9489913 1.3520017 -0.1406703 -2.1044283 -4.0390973 -5.3919516 -5.9571447 -5.8532362][-7.8967237 -6.3753181 -5.47131 -3.6049004 -1.1238419 1.1826255 3.0914431 4.0441179 3.4010143 1.7408543 -0.57028663 -2.9766443 -4.7017803 -5.574008 -5.6204815][-7.4624124 -5.9532666 -4.8372526 -2.8922455 -0.43789291 1.7303061 3.5332336 4.4105706 3.6511989 1.7978482 -0.65853751 -3.1157026 -4.9121518 -5.8543735 -5.9247494][-7.1722851 -5.9907532 -5.291975 -3.7813425 -1.6455863 0.36236906 2.0934353 2.934473 2.2729664 0.45573664 -1.9834332 -4.3248653 -5.9579296 -6.6546063 -6.5403366][-6.1286821 -5.4237576 -5.4655724 -4.8496408 -3.5031214 -1.9681283 -0.55231476 0.11308217 -0.532758 -2.1970575 -4.2636743 -6.0553589 -7.1391468 -7.3833408 -7.058455][-5.5598021 -4.9940481 -5.316783 -5.2390966 -4.5588212 -3.5073168 -2.5324872 -2.3010106 -3.0514357 -4.5453205 -6.2537136 -7.5418606 -8.0953627 -7.9056535 -7.3703532][-5.0825009 -4.5871058 -5.0065279 -5.1611633 -4.9080868 -4.3342085 -3.8244834 -3.8501062 -4.4441671 -5.6144447 -6.8907952 -7.7698307 -8.0660973 -7.7930527 -7.3592296][-4.1735806 -3.7973528 -4.4989638 -4.9065876 -4.9802222 -4.82218 -4.8013692 -5.1434975 -5.5275536 -6.1991749 -6.8226495 -7.2417479 -7.34367 -7.0156479 -6.6729383][-3.0164733 -2.5402603 -3.5041165 -4.1842809 -4.575325 -4.9116735 -5.4246011 -5.9786382 -6.1592808 -6.4019814 -6.4365134 -6.4237618 -6.3830738 -5.9900427 -5.7299347][-2.3874748 -1.4301889 -2.2793198 -2.9250283 -3.3841295 -4.0041018 -4.7967815 -5.4100151 -5.3834476 -5.3540478 -5.0265713 -4.8980618 -5.1490464 -5.0330186 -4.9219656][-3.0480311 -1.4513538 -1.8144734 -2.0547068 -2.2155936 -2.7711866 -3.6438909 -4.145268 -3.9506462 -3.8116784 -3.3905783 -3.3663731 -3.9774749 -4.1419182 -4.1665845]]...]
INFO - root - 2017-12-15 07:53:44.105046: step 28910, loss = 0.19, batch loss = 0.16 (35.9 examples/sec; 0.223 sec/batch; 18h:48m:38s remains)
INFO - root - 2017-12-15 07:53:46.372279: step 28920, loss = 0.24, batch loss = 0.21 (34.8 examples/sec; 0.230 sec/batch; 19h:24m:11s remains)
INFO - root - 2017-12-15 07:53:48.666035: step 28930, loss = 0.26, batch loss = 0.23 (34.9 examples/sec; 0.229 sec/batch; 19h:18m:32s remains)
INFO - root - 2017-12-15 07:53:50.936828: step 28940, loss = 0.15, batch loss = 0.11 (35.9 examples/sec; 0.223 sec/batch; 18h:47m:51s remains)
INFO - root - 2017-12-15 07:53:53.206024: step 28950, loss = 0.20, batch loss = 0.17 (34.1 examples/sec; 0.234 sec/batch; 19h:45m:44s remains)
INFO - root - 2017-12-15 07:53:55.507222: step 28960, loss = 0.24, batch loss = 0.20 (36.2 examples/sec; 0.221 sec/batch; 18h:39m:04s remains)
INFO - root - 2017-12-15 07:53:57.814759: step 28970, loss = 0.22, batch loss = 0.19 (32.8 examples/sec; 0.244 sec/batch; 20h:31m:59s remains)
INFO - root - 2017-12-15 07:54:00.085098: step 28980, loss = 0.21, batch loss = 0.18 (35.0 examples/sec; 0.228 sec/batch; 19h:14m:43s remains)
INFO - root - 2017-12-15 07:54:02.373837: step 28990, loss = 0.23, batch loss = 0.20 (35.1 examples/sec; 0.228 sec/batch; 19h:13m:30s remains)
INFO - root - 2017-12-15 07:54:04.646701: step 29000, loss = 0.21, batch loss = 0.17 (36.1 examples/sec; 0.222 sec/batch; 18h:40m:49s remains)
2017-12-15 07:54:04.945851: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.036305 -2.8796203 -2.7749419 -2.7778006 -2.9714825 -3.2688794 -4.4506049 -5.35771 -5.0594006 -4.9720354 -5.321044 -6.199008 -6.7974396 -6.8796272 -7.0911202][-2.5452147 -2.7532377 -3.0207686 -3.2801003 -3.4555721 -3.6237442 -4.5538912 -5.3444438 -5.0583315 -4.9631777 -5.3380165 -6.558249 -7.3754191 -7.5385828 -7.6945167][-3.2458987 -3.0001061 -3.394067 -3.8061161 -3.7259657 -3.4766521 -3.8348379 -4.3141561 -4.1745887 -4.23816 -4.5990815 -6.0806837 -7.1857638 -7.5026283 -7.6694427][-3.9040971 -3.1333129 -3.5734463 -4.0784378 -3.7465162 -3.134912 -2.8248615 -2.7629652 -2.7141371 -2.8469419 -2.9427528 -4.417522 -5.8421955 -6.5270987 -7.0031967][-4.2707725 -2.9881804 -3.3442934 -3.8223262 -3.3641491 -2.5115097 -1.5561743 -0.81997192 -0.75203812 -1.0648154 -0.97690868 -2.2863095 -3.8356004 -4.8176155 -5.6949644][-4.3875017 -2.7652113 -2.8551936 -3.0797215 -2.4297624 -1.383284 -0.071315765 1.1777825 1.446955 0.97284579 1.0000858 -0.26567197 -1.8757232 -3.0919104 -4.3314104][-4.2517447 -2.5683 -2.4907441 -2.5481718 -1.8481778 -0.74100733 0.71897817 2.2711735 2.695653 2.1592941 2.0886321 0.82272625 -0.7999537 -2.1268132 -3.5646236][-4.3551397 -2.7440064 -2.6052825 -2.526629 -1.8733574 -0.8994298 0.35484934 1.7876992 2.2200556 1.6786015 1.4252694 0.18127227 -1.2470187 -2.5281479 -3.842869][-4.6544065 -3.0445006 -2.8235428 -2.7557566 -2.468976 -1.8343198 -0.81853056 0.43385673 0.93483591 0.62882924 0.39647102 -0.75865722 -2.054775 -3.2690673 -4.2664576][-4.84565 -3.3374522 -3.1330576 -3.134228 -3.1861973 -2.8177214 -2.0352123 -1.0198623 -0.57556927 -0.74876058 -1.0902274 -2.2082343 -3.2603364 -4.2459431 -4.8244672][-4.9786844 -3.4423573 -3.1180017 -3.0483928 -3.4088857 -3.3346267 -2.8298764 -2.1436803 -1.936862 -2.1470578 -2.6286564 -3.617043 -4.3948278 -4.9955235 -5.1398058][-4.8925724 -3.3123894 -2.8394313 -2.630779 -3.2198143 -3.3946457 -3.1274214 -2.7725718 -2.8047411 -2.9946713 -3.3998542 -4.102211 -4.6556354 -5.023747 -5.0335836][-4.0783281 -2.3046241 -1.5514638 -1.1696316 -2.0107985 -2.4854105 -2.6143699 -2.7286966 -3.0390532 -3.2042358 -3.4352436 -3.9355922 -4.4817119 -4.8188715 -4.7948828][-3.0485833 -1.0878885 -0.097405434 0.36793208 -0.7041539 -1.3817909 -1.7170097 -2.1572511 -2.6052341 -2.7048454 -2.7394981 -3.1721942 -3.8056502 -4.1585917 -4.1919136][-2.1216104 -0.24616909 0.83738661 1.3205466 0.22350311 -0.515558 -0.926955 -1.5555794 -2.2305477 -2.5114043 -2.4931285 -2.7879522 -3.3641372 -3.7261152 -3.8369093]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 07:54:07.277496: step 29010, loss = 0.25, batch loss = 0.21 (34.0 examples/sec; 0.235 sec/batch; 19h:48m:25s remains)
INFO - root - 2017-12-15 07:54:09.578039: step 29020, loss = 0.21, batch loss = 0.18 (35.5 examples/sec; 0.225 sec/batch; 18h:58m:17s remains)
INFO - root - 2017-12-15 07:54:11.841114: step 29030, loss = 0.25, batch loss = 0.22 (35.8 examples/sec; 0.224 sec/batch; 18h:50m:50s remains)
INFO - root - 2017-12-15 07:54:14.109224: step 29040, loss = 0.27, batch loss = 0.24 (34.9 examples/sec; 0.229 sec/batch; 19h:19m:50s remains)
INFO - root - 2017-12-15 07:54:16.360403: step 29050, loss = 0.34, batch loss = 0.31 (35.7 examples/sec; 0.224 sec/batch; 18h:53m:19s remains)
INFO - root - 2017-12-15 07:54:18.619203: step 29060, loss = 0.25, batch loss = 0.22 (35.0 examples/sec; 0.229 sec/batch; 19h:15m:49s remains)
INFO - root - 2017-12-15 07:54:20.905109: step 29070, loss = 0.20, batch loss = 0.17 (35.1 examples/sec; 0.228 sec/batch; 19h:11m:36s remains)
INFO - root - 2017-12-15 07:54:23.212868: step 29080, loss = 0.24, batch loss = 0.21 (34.5 examples/sec; 0.232 sec/batch; 19h:31m:37s remains)
INFO - root - 2017-12-15 07:54:25.451256: step 29090, loss = 0.18, batch loss = 0.15 (36.3 examples/sec; 0.220 sec/batch; 18h:34m:00s remains)
INFO - root - 2017-12-15 07:54:27.722223: step 29100, loss = 0.20, batch loss = 0.17 (34.6 examples/sec; 0.231 sec/batch; 19h:28m:33s remains)
2017-12-15 07:54:28.024451: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4360409 -4.977807 -4.9661689 -5.0853958 -5.2718272 -5.3201284 -5.3601837 -5.4727917 -5.4712749 -5.4633102 -5.5389977 -5.3692131 -4.873445 -4.229321 -3.4346347][-4.2123723 -5.4270563 -5.5263591 -5.5838032 -5.6444616 -5.5592365 -5.4485674 -5.5442543 -5.7083569 -5.9739056 -6.3336325 -6.3815289 -6.0247393 -5.3915262 -4.4596405][-4.2143683 -5.2790728 -5.6793213 -5.6695423 -5.455307 -5.2332034 -4.9881253 -4.9319563 -5.1754522 -5.6809092 -6.2936821 -6.5867271 -6.4856691 -6.011467 -5.1768389][-4.535635 -5.0994987 -5.6248245 -5.2889414 -4.5915847 -4.2393723 -4.0168858 -3.8191152 -4.253469 -5.1635466 -6.0565815 -6.5666876 -6.6370316 -6.2864695 -5.6186428][-4.6719093 -4.5443645 -4.9292054 -4.085463 -2.7972469 -2.2126329 -1.726738 -1.2190014 -1.9661973 -3.5010171 -4.9404659 -5.9098911 -6.3147106 -6.2845478 -5.9192362][-4.3829031 -3.8362527 -4.0849819 -2.8227537 -1.0699804 -0.081392288 1.0354841 2.0811384 1.1328177 -0.86416173 -2.7472584 -4.3058825 -5.30737 -5.79325 -5.9076142][-4.0279231 -3.616662 -3.6071513 -1.9414563 0.27804875 1.6961896 3.5512893 5.12523 3.9979746 1.69279 -0.52281523 -2.6423697 -4.1450334 -5.0461378 -5.6618977][-4.2702913 -3.5593162 -3.3964291 -1.6012139 0.7980566 2.5055254 4.8224268 6.7209005 5.6655703 3.4312675 1.2498581 -1.1176139 -2.8737068 -3.9917407 -4.959794][-4.5662661 -3.6080737 -3.4096816 -1.8061463 0.48353267 2.0991681 4.2380085 5.9246025 5.1271076 3.4473588 1.7704618 -0.1562345 -1.6957036 -2.6787407 -3.7954922][-4.9725447 -3.8319921 -3.5457501 -2.2136049 -0.37435114 0.7224412 2.1376154 3.4894936 3.2253115 2.3534729 1.4326885 0.13119245 -1.0769058 -1.7546332 -2.7780743][-5.5884504 -4.3587322 -3.9592581 -3.0659552 -1.8954151 -1.4263766 -0.63225329 0.60546613 1.0906982 1.0914946 0.86397266 0.076745987 -0.88154709 -1.4155669 -2.3923101][-6.0637231 -4.6333909 -4.1331892 -3.6594744 -3.1137486 -3.0509965 -2.7524481 -1.7150204 -0.84785593 -0.45339251 -0.32747686 -0.73572922 -1.5479054 -2.0233097 -2.9100163][-6.0610027 -4.4901428 -4.0079031 -4.0028343 -3.9474764 -3.9358785 -3.8051233 -3.1548851 -2.41642 -1.9894077 -1.859906 -2.23566 -3.0646739 -3.6005518 -4.4880953][-5.4692135 -3.7738781 -3.3867159 -3.7587953 -4.0324802 -3.964551 -3.9998753 -3.7740357 -3.3578124 -3.0911069 -3.1488101 -3.7662296 -4.6881361 -5.3479967 -6.3653469][-4.3507786 -2.5098395 -2.2833452 -2.7521052 -2.9688983 -2.7509496 -2.8554072 -2.9933054 -2.95788 -3.1077969 -3.6339383 -4.5619488 -5.4856381 -6.2621193 -7.3289175]]...]
INFO - root - 2017-12-15 07:54:30.274365: step 29110, loss = 0.30, batch loss = 0.27 (35.1 examples/sec; 0.228 sec/batch; 19h:13m:56s remains)
INFO - root - 2017-12-15 07:54:32.524497: step 29120, loss = 0.34, batch loss = 0.31 (36.4 examples/sec; 0.220 sec/batch; 18h:31m:58s remains)
INFO - root - 2017-12-15 07:54:34.793028: step 29130, loss = 0.23, batch loss = 0.19 (35.4 examples/sec; 0.226 sec/batch; 19h:03m:25s remains)
INFO - root - 2017-12-15 07:54:37.063075: step 29140, loss = 0.34, batch loss = 0.30 (35.2 examples/sec; 0.227 sec/batch; 19h:08m:27s remains)
INFO - root - 2017-12-15 07:54:39.397671: step 29150, loss = 0.26, batch loss = 0.22 (35.5 examples/sec; 0.225 sec/batch; 18h:59m:09s remains)
INFO - root - 2017-12-15 07:54:41.666605: step 29160, loss = 0.22, batch loss = 0.18 (34.5 examples/sec; 0.232 sec/batch; 19h:33m:54s remains)
INFO - root - 2017-12-15 07:54:43.966442: step 29170, loss = 0.23, batch loss = 0.19 (33.5 examples/sec; 0.239 sec/batch; 20h:08m:11s remains)
INFO - root - 2017-12-15 07:54:46.278649: step 29180, loss = 0.29, batch loss = 0.25 (35.4 examples/sec; 0.226 sec/batch; 19h:02m:52s remains)
INFO - root - 2017-12-15 07:54:48.543789: step 29190, loss = 0.23, batch loss = 0.19 (36.0 examples/sec; 0.222 sec/batch; 18h:43m:05s remains)
INFO - root - 2017-12-15 07:54:50.789928: step 29200, loss = 0.20, batch loss = 0.16 (35.5 examples/sec; 0.225 sec/batch; 18h:59m:45s remains)
2017-12-15 07:54:51.069903: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1210804 -5.5300174 -5.910162 -6.2060909 -6.2684903 -6.0186396 -5.9557343 -5.8296909 -5.3102713 -5.2890253 -5.6689405 -6.2806911 -7.304275 -8.2106285 -7.1526494][-4.337265 -6.1291671 -6.4532089 -6.5675035 -6.4687753 -6.0852356 -5.9822669 -5.866333 -5.4986219 -5.7678852 -6.2352762 -6.6397195 -7.45321 -8.1841946 -7.1234031][-5.4508877 -6.1700196 -6.3806949 -6.2766762 -6.035028 -5.5545931 -5.2685947 -5.0286174 -4.7679086 -5.3489532 -5.964529 -6.1469851 -6.64544 -7.2843232 -6.6635022][-5.8378491 -5.6985636 -5.7085114 -5.256896 -4.7171297 -4.1835017 -3.6913295 -3.2436242 -3.1650972 -4.2414346 -5.2068796 -5.3041172 -5.5298071 -6.08391 -5.9154949][-5.5400906 -4.9229903 -4.7535686 -4.0077081 -2.996367 -2.1861327 -1.1155438 -0.21074939 -0.26118994 -2.0328956 -3.8542166 -4.4116049 -4.7008066 -5.2733665 -5.3996372][-5.0611315 -4.3137565 -4.0810966 -3.1618209 -1.6027364 -0.31783903 1.4646471 2.9621224 2.8153806 0.26648068 -2.5018992 -3.8367884 -4.5629492 -5.1691389 -5.4235244][-4.2584229 -3.8555584 -3.5567832 -2.5532918 -0.5556109 1.2335238 3.6750526 5.7977548 5.53566 2.2489991 -1.3405263 -3.3274624 -4.4930887 -5.3282576 -5.7121172][-3.8701339 -3.591507 -3.3480418 -2.5083909 -0.46432018 1.5992949 4.3555651 6.7834287 6.55689 2.9373512 -0.93404233 -3.1072714 -4.4168658 -5.4124565 -5.9845877][-3.7612562 -3.7854154 -3.701263 -3.1519213 -1.5458499 0.23385072 2.7057095 5.061697 5.0875454 1.8493214 -1.5600924 -3.3664596 -4.4495897 -5.4061909 -5.9327211][-4.1170769 -4.3977318 -4.4230323 -4.0744853 -3.0048981 -1.6753054 0.33841705 2.4530005 2.6825542 -0.011284828 -2.7456572 -4.07843 -4.7695985 -5.4708195 -5.6740294][-4.7634888 -5.2652407 -5.3815579 -5.1243386 -4.5059433 -3.5883198 -2.0756452 -0.42789638 -0.27790308 -2.4274523 -4.4479084 -5.2889204 -5.5178614 -5.8767929 -5.5889874][-5.4717636 -6.0403562 -6.1775789 -6.0313034 -5.7376642 -5.1333427 -4.1300173 -3.13838 -3.2498121 -4.786644 -5.8617096 -6.1154575 -6.1642938 -6.3776851 -5.68122][-5.9202261 -6.4527254 -6.604373 -6.6538968 -6.7845173 -6.5787215 -6.0282574 -5.620945 -5.8448181 -6.676662 -6.8545113 -6.6456237 -6.5962167 -6.7624564 -5.7071457][-5.8962307 -6.3227639 -6.4613481 -6.69762 -7.1632352 -7.3793464 -7.1880474 -7.0151525 -7.0908046 -7.3359666 -7.0671239 -6.7402263 -6.7874937 -6.9680796 -5.9678035][-5.3869162 -5.5862303 -5.6594591 -5.9327583 -6.4413524 -6.7768455 -6.7118564 -6.6548314 -6.6043606 -6.5915251 -6.2251635 -6.0052443 -6.2259064 -6.6844659 -6.0837812]]...]
INFO - root - 2017-12-15 07:54:53.388683: step 29210, loss = 0.20, batch loss = 0.17 (34.7 examples/sec; 0.231 sec/batch; 19h:26m:19s remains)
INFO - root - 2017-12-15 07:54:55.673044: step 29220, loss = 0.20, batch loss = 0.17 (34.7 examples/sec; 0.231 sec/batch; 19h:26m:15s remains)
INFO - root - 2017-12-15 07:54:57.954492: step 29230, loss = 0.37, batch loss = 0.34 (36.0 examples/sec; 0.222 sec/batch; 18h:43m:29s remains)
INFO - root - 2017-12-15 07:55:00.282671: step 29240, loss = 0.22, batch loss = 0.19 (35.4 examples/sec; 0.226 sec/batch; 19h:01m:43s remains)
INFO - root - 2017-12-15 07:55:02.562435: step 29250, loss = 0.15, batch loss = 0.11 (34.5 examples/sec; 0.232 sec/batch; 19h:32m:10s remains)
INFO - root - 2017-12-15 07:55:04.839430: step 29260, loss = 0.22, batch loss = 0.19 (34.1 examples/sec; 0.235 sec/batch; 19h:46m:14s remains)
INFO - root - 2017-12-15 07:55:07.136246: step 29270, loss = 0.27, batch loss = 0.24 (35.9 examples/sec; 0.223 sec/batch; 18h:44m:52s remains)
INFO - root - 2017-12-15 07:55:09.423095: step 29280, loss = 0.24, batch loss = 0.21 (34.3 examples/sec; 0.233 sec/batch; 19h:39m:35s remains)
INFO - root - 2017-12-15 07:55:11.687552: step 29290, loss = 0.23, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 18h:41m:53s remains)
INFO - root - 2017-12-15 07:55:13.954580: step 29300, loss = 0.27, batch loss = 0.24 (35.6 examples/sec; 0.225 sec/batch; 18h:55m:37s remains)
2017-12-15 07:55:14.255517: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.270437 -3.4930434 -3.8355536 -4.1754255 -4.3118162 -4.4541788 -4.6853638 -4.7344885 -4.8185821 -5.0382605 -5.1746969 -5.2421689 -5.0202794 -4.774581 -4.4689035][-3.4271889 -4.2716069 -4.8227224 -5.3478994 -5.5559707 -5.783143 -6.1942525 -6.2978296 -6.41545 -6.7669182 -7.0231543 -7.0655527 -6.5981731 -6.0743141 -5.502492][-3.9989457 -4.1865244 -4.7122755 -5.211915 -5.2959042 -5.4323587 -5.9006128 -6.0498214 -6.24117 -6.8999634 -7.4605694 -7.6373024 -7.1678762 -6.5929947 -5.9710684][-4.1935573 -3.4474566 -3.7051723 -3.9518404 -3.6718011 -3.4379125 -3.7006588 -3.8395548 -4.23578 -5.2954321 -6.2833967 -6.8426285 -6.7441807 -6.4789619 -6.0719333][-3.9079962 -2.3359642 -2.3679709 -2.3542004 -1.6555862 -0.94210565 -0.79377782 -0.86920619 -1.537079 -2.9341695 -4.2899837 -5.2087 -5.6232452 -5.7862864 -5.7423][-3.4607081 -1.2242035 -0.9100492 -0.53984118 0.47866631 1.561321 2.1600902 2.1145008 1.1377015 -0.50916159 -2.0653167 -3.2410409 -4.1355314 -4.7487955 -5.0955143][-2.256778 -0.41360331 0.097894192 0.64616585 1.8116415 3.0513966 3.8668354 3.8127944 2.6893904 0.92571449 -0.60924745 -1.8209754 -3.0151536 -3.9436049 -4.5930347][-2.4579337 -0.30543935 0.40770149 1.1338558 2.2354062 3.2641537 3.8803265 3.6902206 2.538028 0.928339 -0.41127789 -1.5202535 -2.6481919 -3.603991 -4.3215256][-3.1587865 -1.1594528 -0.45273054 0.42017841 1.420157 2.1143491 2.4223187 2.0125029 0.93998551 -0.45719302 -1.6448996 -2.5356932 -3.3672693 -3.9654269 -4.4159813][-3.6193118 -1.9765545 -1.5309795 -0.83065033 -0.086049557 0.25752282 0.13030672 -0.53009796 -1.3926923 -2.4672959 -3.2629266 -3.8894854 -4.4629416 -4.7561131 -4.8400707][-3.9907866 -2.6926572 -2.5003662 -2.0352163 -1.5416334 -1.48209 -1.9856627 -2.8127308 -3.4878366 -4.1765432 -4.6280332 -5.0598583 -5.3554983 -5.2392931 -4.9537706][-4.6119709 -3.5855703 -3.5699935 -3.3649931 -3.0881979 -3.1870914 -3.869987 -4.5991235 -4.9597836 -5.2665215 -5.4005303 -5.6482887 -5.6655874 -5.2647886 -4.7999969][-5.8403754 -4.8999424 -4.8503819 -4.7132134 -4.4547224 -4.636055 -5.3290911 -5.8932052 -6.1048961 -6.1815815 -6.1047511 -6.1322975 -5.9329972 -5.3336172 -4.7787733][-7.1993122 -6.2976427 -6.1276159 -5.9069214 -5.4601488 -5.5807281 -6.2651482 -6.6798763 -6.8036165 -6.736474 -6.44639 -6.39085 -6.06685 -5.4573641 -4.8757076][-7.7922325 -7.005847 -6.7595673 -6.3744726 -5.8424573 -6.006062 -6.6715736 -6.9226875 -7.0178614 -6.895752 -6.6038504 -6.4416018 -6.02666 -5.453824 -4.8735781]]...]
INFO - root - 2017-12-15 07:55:16.507407: step 29310, loss = 0.21, batch loss = 0.18 (35.3 examples/sec; 0.227 sec/batch; 19h:05m:47s remains)
INFO - root - 2017-12-15 07:55:18.767076: step 29320, loss = 0.25, batch loss = 0.22 (35.4 examples/sec; 0.226 sec/batch; 19h:02m:46s remains)
INFO - root - 2017-12-15 07:55:21.024381: step 29330, loss = 0.27, batch loss = 0.24 (35.0 examples/sec; 0.229 sec/batch; 19h:16m:14s remains)
INFO - root - 2017-12-15 07:55:23.337512: step 29340, loss = 0.33, batch loss = 0.30 (33.5 examples/sec; 0.239 sec/batch; 20h:06m:47s remains)
INFO - root - 2017-12-15 07:55:25.612271: step 29350, loss = 0.19, batch loss = 0.16 (35.9 examples/sec; 0.223 sec/batch; 18h:45m:16s remains)
INFO - root - 2017-12-15 07:55:27.900660: step 29360, loss = 0.35, batch loss = 0.32 (35.7 examples/sec; 0.224 sec/batch; 18h:51m:47s remains)
INFO - root - 2017-12-15 07:55:30.210933: step 29370, loss = 0.27, batch loss = 0.24 (34.0 examples/sec; 0.235 sec/batch; 19h:47m:23s remains)
INFO - root - 2017-12-15 07:55:32.506295: step 29380, loss = 0.21, batch loss = 0.18 (36.0 examples/sec; 0.222 sec/batch; 18h:43m:15s remains)
INFO - root - 2017-12-15 07:55:34.775340: step 29390, loss = 0.38, batch loss = 0.35 (34.5 examples/sec; 0.232 sec/batch; 19h:31m:09s remains)
INFO - root - 2017-12-15 07:55:37.048734: step 29400, loss = 0.22, batch loss = 0.19 (34.5 examples/sec; 0.232 sec/batch; 19h:32m:13s remains)
2017-12-15 07:55:37.339812: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.7289021 -2.6162534 -3.6988969 -4.7339888 -4.91479 -4.4586358 -3.9399681 -3.969872 -4.2766213 -4.6021762 -4.7332392 -4.6086593 -4.3914552 -3.9256527 -3.4112487][-2.8666437 -3.261786 -4.737246 -6.1172256 -6.1441965 -5.2938814 -4.5701571 -4.6370745 -5.176878 -5.5675917 -5.8456717 -5.7262449 -5.5532665 -4.9743586 -4.0376244][-5.1554585 -3.9396698 -5.4585986 -6.8550434 -6.6957731 -5.68587 -4.9080973 -5.0000706 -5.736557 -6.15182 -6.6006222 -6.5194263 -6.5128584 -6.0361443 -4.87743][-7.269948 -4.6693835 -5.6659608 -6.6943779 -6.2797308 -5.2909393 -4.5397596 -4.6493044 -5.6256514 -6.1439915 -6.6678686 -6.4997835 -6.6215477 -6.388133 -5.4136505][-8.4144716 -4.942894 -5.4358854 -5.9980841 -5.2809038 -4.3171482 -3.5493665 -3.6339898 -4.7798896 -5.5395856 -6.2195344 -6.0499525 -6.146718 -6.0528021 -5.4246807][-7.3987627 -3.9398043 -4.2105803 -4.39818 -3.1632435 -1.9604536 -1.1002942 -1.283173 -2.5605774 -3.7588058 -4.81444 -4.983758 -5.2005572 -5.2870779 -5.0548744][-5.3162708 -2.5937316 -3.0793667 -3.0511303 -1.3794477 0.1599369 1.2106168 0.99236155 -0.36434841 -2.1508207 -3.74062 -4.3396187 -4.6379623 -4.7924824 -4.7636471][-3.8856628 -1.2633332 -2.2882626 -2.1079466 -0.090914965 1.9886086 3.4808977 3.3687513 1.8981745 -0.57158816 -3.0017242 -4.1946344 -4.7444706 -4.9264526 -4.8228669][-3.0147352 -0.34458935 -1.7954161 -1.4896648 0.77687669 3.3918178 5.295043 5.0921021 3.24943 0.12041306 -2.9815993 -4.5933352 -5.33646 -5.4109359 -5.0669403][-3.6309953 -0.84300625 -2.3044798 -1.9248857 0.38746428 3.1624181 5.10989 4.834692 2.6458352 -0.87582445 -4.2442451 -5.8270354 -6.46249 -6.1536264 -5.4889231][-5.101531 -2.2797897 -3.4246984 -3.0059166 -0.86351788 1.736479 3.4667757 3.1485388 1.0351663 -2.4171274 -5.5991793 -7.0008516 -7.4248915 -6.8161469 -5.8676453][-6.8015671 -4.2612181 -4.9873667 -4.4738169 -2.5295196 -0.44121671 0.81288576 0.4102428 -1.4191563 -4.3811684 -6.902091 -7.9891663 -8.1574669 -7.3305817 -6.1720943][-8.1569071 -6.2064409 -6.6507025 -6.0862613 -4.4529963 -2.9879837 -2.2730007 -2.7955253 -4.2549038 -6.4661674 -8.1035051 -8.6229525 -8.3558769 -7.3839846 -6.14221][-8.592061 -7.2242126 -7.5177431 -7.0041628 -5.8059988 -4.895721 -4.5896778 -5.1851897 -6.2687225 -7.6825991 -8.5010872 -8.4743977 -7.8915806 -6.9424286 -5.8237619][-8.209425 -7.1641808 -7.3259563 -6.9138956 -6.0631981 -5.5397816 -5.5157747 -6.0951695 -6.9230461 -7.7204742 -7.9702206 -7.63256 -6.911849 -6.0453444 -5.1399221]]...]
INFO - root - 2017-12-15 07:55:39.584429: step 29410, loss = 0.17, batch loss = 0.14 (35.1 examples/sec; 0.228 sec/batch; 19h:10m:06s remains)
INFO - root - 2017-12-15 07:55:41.840065: step 29420, loss = 0.20, batch loss = 0.17 (34.9 examples/sec; 0.229 sec/batch; 19h:17m:50s remains)
INFO - root - 2017-12-15 07:55:44.081360: step 29430, loss = 0.21, batch loss = 0.18 (35.3 examples/sec; 0.226 sec/batch; 19h:03m:53s remains)
INFO - root - 2017-12-15 07:55:46.378817: step 29440, loss = 0.27, batch loss = 0.24 (34.8 examples/sec; 0.230 sec/batch; 19h:19m:40s remains)
INFO - root - 2017-12-15 07:55:48.649419: step 29450, loss = 0.28, batch loss = 0.25 (35.8 examples/sec; 0.223 sec/batch; 18h:47m:28s remains)
INFO - root - 2017-12-15 07:55:50.888869: step 29460, loss = 0.20, batch loss = 0.17 (35.0 examples/sec; 0.229 sec/batch; 19h:14m:51s remains)
INFO - root - 2017-12-15 07:55:53.183371: step 29470, loss = 0.22, batch loss = 0.19 (34.3 examples/sec; 0.233 sec/batch; 19h:39m:13s remains)
INFO - root - 2017-12-15 07:55:55.453903: step 29480, loss = 0.22, batch loss = 0.19 (35.6 examples/sec; 0.225 sec/batch; 18h:55m:40s remains)
INFO - root - 2017-12-15 07:55:57.699329: step 29490, loss = 0.27, batch loss = 0.24 (35.4 examples/sec; 0.226 sec/batch; 19h:02m:08s remains)
INFO - root - 2017-12-15 07:55:59.988743: step 29500, loss = 0.24, batch loss = 0.21 (36.3 examples/sec; 0.220 sec/batch; 18h:31m:26s remains)
2017-12-15 07:56:00.294287: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9247875 -5.70208 -5.2912254 -5.4111948 -6.1077681 -6.8319941 -6.839345 -6.067368 -5.2944021 -5.8473473 -6.7144794 -7.0762892 -6.3317986 -5.5953665 -5.0107751][-6.06747 -3.755507 -3.3746696 -3.61475 -4.1619239 -4.76797 -4.5758257 -3.8143368 -3.2966964 -4.1254387 -5.1997809 -5.4224234 -4.492157 -3.7439036 -3.2695346][-6.0515785 -2.3456721 -1.9507055 -2.1118071 -2.5111127 -3.016643 -2.700978 -2.2112744 -2.1047838 -3.2041354 -4.32261 -4.2371235 -3.0914035 -2.4248946 -2.2147524][-6.514535 -1.7998829 -1.4395337 -1.5446696 -1.9038248 -2.1246152 -1.5363419 -1.1683589 -1.2768149 -2.4008415 -3.4086118 -3.0641391 -1.9023134 -1.3781538 -1.42469][-6.3049574 -1.0022615 -0.78386664 -1.0798348 -1.5481429 -1.3054974 -0.41420805 -0.19239688 -0.25182772 -1.2003355 -2.1602361 -1.8564792 -0.88657212 -0.62687528 -0.86330056][-4.62587 0.34986138 0.4331665 -0.10693312 -0.59804738 0.17272234 1.2093589 1.2731822 1.2398233 0.30210686 -0.82150388 -0.63431227 -0.0025784969 -0.050868511 -0.34965217][-3.5201221 0.677325 0.54050636 -0.27061212 -0.600703 0.73650336 2.0366232 2.0883176 2.0374238 1.2006557 -0.041828394 0.063332081 0.33335352 -0.1047442 -0.41814208][-3.0237098 0.70608044 0.34759927 -0.64131331 -0.67853141 1.0808761 2.4935429 2.4300153 2.2089541 1.5097532 0.52301741 0.79525256 0.8751688 0.056574583 -0.26941359][-3.1536613 0.50116348 0.18551612 -0.64010453 -0.5609349 0.968832 2.0240524 1.6344604 0.94739223 0.23731112 -0.22183609 0.49120808 0.66757727 -0.23082304 -0.51376104][-4.2941895 -0.60703635 -0.61515832 -1.1230279 -1.173334 -0.36486185 0.22174716 -0.30719709 -1.2793384 -1.9613782 -1.8168749 -0.62255371 -0.24166417 -1.1174062 -1.4301081][-5.4663725 -1.8394322 -1.6115463 -1.7147863 -1.8377581 -1.6234548 -1.2758148 -1.6240578 -2.5735683 -3.1549349 -2.605809 -1.2748185 -0.86728573 -1.7995842 -2.3619316][-6.7629347 -3.4259205 -3.119715 -2.9389079 -3.1210876 -3.2236714 -3.0900922 -3.3604307 -4.1550884 -4.5426288 -3.7790775 -2.5336885 -2.1717415 -2.9504714 -3.5544429][-8.4579144 -5.6095982 -5.4286518 -5.2928381 -5.6341276 -5.9300404 -5.9005146 -6.0624266 -6.5672283 -6.6269703 -5.7012262 -4.6044064 -4.2802944 -4.7806187 -5.2615356][-8.9552908 -6.6315765 -6.5642204 -6.5859718 -7.0114384 -7.2905979 -7.2202082 -7.2520227 -7.5657854 -7.469615 -6.6110106 -5.7710772 -5.5970631 -5.9037724 -6.2341108][-7.8621149 -6.0350494 -6.0094142 -6.0804796 -6.4678459 -6.655345 -6.5912371 -6.6604528 -6.9729319 -6.9861078 -6.4027739 -5.8723736 -5.8202305 -5.952323 -6.0618925]]...]
INFO - root - 2017-12-15 07:56:02.561823: step 29510, loss = 0.31, batch loss = 0.28 (33.4 examples/sec; 0.239 sec/batch; 20h:09m:18s remains)
INFO - root - 2017-12-15 07:56:04.866510: step 29520, loss = 0.20, batch loss = 0.16 (35.7 examples/sec; 0.224 sec/batch; 18h:53m:01s remains)
INFO - root - 2017-12-15 07:56:07.163758: step 29530, loss = 0.25, batch loss = 0.22 (34.4 examples/sec; 0.232 sec/batch; 19h:33m:05s remains)
INFO - root - 2017-12-15 07:56:09.460086: step 29540, loss = 0.21, batch loss = 0.18 (34.9 examples/sec; 0.229 sec/batch; 19h:17m:29s remains)
INFO - root - 2017-12-15 07:56:11.723219: step 29550, loss = 0.25, batch loss = 0.22 (34.7 examples/sec; 0.230 sec/batch; 19h:22m:36s remains)
INFO - root - 2017-12-15 07:56:14.000472: step 29560, loss = 0.17, batch loss = 0.13 (36.5 examples/sec; 0.219 sec/batch; 18h:26m:59s remains)
INFO - root - 2017-12-15 07:56:16.253337: step 29570, loss = 0.36, batch loss = 0.33 (35.8 examples/sec; 0.223 sec/batch; 18h:46m:42s remains)
INFO - root - 2017-12-15 07:56:18.518036: step 29580, loss = 0.17, batch loss = 0.14 (34.3 examples/sec; 0.233 sec/batch; 19h:36m:34s remains)
INFO - root - 2017-12-15 07:56:20.781768: step 29590, loss = 0.20, batch loss = 0.16 (35.8 examples/sec; 0.223 sec/batch; 18h:47m:18s remains)
INFO - root - 2017-12-15 07:56:23.022362: step 29600, loss = 0.23, batch loss = 0.20 (34.6 examples/sec; 0.231 sec/batch; 19h:26m:32s remains)
2017-12-15 07:56:23.300357: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.37220693 -0.057676077 -0.72662687 -1.9050757 -3.7571654 -5.4194732 -6.4573412 -6.1772971 -5.4143791 -4.8250456 -4.4712982 -4.06872 -3.0766721 -2.5188169 -2.4053969][-2.7997267 -2.6392076 -3.2085841 -4.0204048 -5.4536791 -6.8856621 -7.8277063 -7.4951611 -6.7042408 -5.7882447 -4.9857616 -4.3384466 -3.3839464 -2.8195095 -2.7874498][-5.2235718 -4.3508625 -4.6036692 -4.9596362 -5.8949647 -7.0672169 -8.0670214 -7.9611931 -7.415473 -6.658742 -5.9200854 -5.3401556 -4.5001497 -3.9185328 -3.7215509][-6.4383039 -5.0973253 -4.9899788 -4.8394623 -5.2097979 -6.0675154 -7.1582241 -7.3949633 -7.2522974 -6.8813763 -6.5637789 -6.4213085 -5.9618349 -5.4159355 -4.9810581][-6.7843561 -4.9821577 -4.5863 -4.0342703 -3.8818345 -4.2309384 -5.0717487 -5.524662 -5.7727766 -5.9499793 -6.1008329 -6.2767429 -6.1088095 -5.7864389 -5.4226608][-6.3488913 -4.4845076 -3.9032078 -2.9398203 -2.134228 -1.6756642 -1.8776199 -2.2378986 -2.9009862 -3.8662574 -4.7159615 -5.2349477 -5.2660284 -5.3380709 -5.3206797][-5.2059751 -3.7518902 -3.1250296 -1.8039514 -0.41182423 0.90514255 1.5588026 1.5277095 0.5704515 -1.2475646 -2.886471 -3.8617969 -4.1466436 -4.5565314 -4.9431486][-4.0653872 -2.6844759 -2.1265812 -0.66724205 1.0780706 2.9938242 4.2824583 4.5074663 3.203819 0.68333673 -1.526556 -2.7506008 -3.1812088 -3.6848524 -4.2300382][-3.1765811 -2.0087051 -1.5535709 -0.057159185 1.7892573 3.9187109 5.5176258 5.8760233 4.3306627 1.3039517 -1.2903875 -2.5832837 -3.1110501 -3.5382185 -3.9623904][-3.4105384 -2.7179623 -2.4002271 -0.99305272 0.81536055 2.9349053 4.7662888 5.3684549 3.7632496 0.55895734 -1.9991391 -3.1867673 -3.7133017 -3.7851839 -3.8057723][-4.5614986 -4.204484 -3.8266897 -2.3988152 -0.50466704 1.5307391 3.2119815 3.5591137 1.8427994 -1.0447934 -3.0585356 -3.9369521 -4.3762169 -4.0892124 -3.8366838][-5.5370293 -5.3569512 -4.8130379 -3.2722175 -1.3607545 0.36326957 1.5460904 1.3978436 -0.36713338 -2.6385648 -4.0567951 -4.7205582 -5.0969253 -4.6554022 -4.2791309][-6.2372646 -6.2456584 -5.6704779 -4.1920161 -2.3474774 -0.89180112 -0.029807329 -0.46142864 -1.9636431 -3.4893098 -4.3799849 -4.9731588 -5.3294668 -4.7975554 -4.2737126][-6.8245659 -6.5654631 -5.7552366 -4.319232 -2.6444507 -1.5249505 -1.0424799 -1.6133978 -2.7474685 -3.6670711 -4.2393661 -4.8118763 -5.1817923 -4.7279987 -4.1031923][-6.9586678 -6.1586304 -5.1527424 -3.8081329 -2.4403744 -1.6564996 -1.5642803 -2.251966 -3.0929751 -3.5807776 -4.0574946 -4.5745907 -4.9283762 -4.5996828 -3.8352776]]...]
INFO - root - 2017-12-15 07:56:25.551930: step 29610, loss = 0.21, batch loss = 0.18 (36.4 examples/sec; 0.220 sec/batch; 18h:28m:15s remains)
INFO - root - 2017-12-15 07:56:27.831077: step 29620, loss = 0.21, batch loss = 0.18 (35.7 examples/sec; 0.224 sec/batch; 18h:52m:34s remains)
INFO - root - 2017-12-15 07:56:30.078819: step 29630, loss = 0.21, batch loss = 0.18 (35.7 examples/sec; 0.224 sec/batch; 18h:51m:01s remains)
INFO - root - 2017-12-15 07:56:32.375694: step 29640, loss = 0.25, batch loss = 0.22 (34.9 examples/sec; 0.229 sec/batch; 19h:17m:50s remains)
INFO - root - 2017-12-15 07:56:34.631381: step 29650, loss = 0.21, batch loss = 0.17 (35.3 examples/sec; 0.227 sec/batch; 19h:04m:03s remains)
INFO - root - 2017-12-15 07:56:36.937107: step 29660, loss = 0.34, batch loss = 0.31 (35.4 examples/sec; 0.226 sec/batch; 19h:00m:48s remains)
INFO - root - 2017-12-15 07:56:39.204804: step 29670, loss = 0.19, batch loss = 0.16 (33.7 examples/sec; 0.237 sec/batch; 19h:57m:27s remains)
INFO - root - 2017-12-15 07:56:41.463201: step 29680, loss = 0.16, batch loss = 0.13 (36.4 examples/sec; 0.220 sec/batch; 18h:29m:38s remains)
INFO - root - 2017-12-15 07:56:43.732326: step 29690, loss = 0.25, batch loss = 0.21 (35.3 examples/sec; 0.227 sec/batch; 19h:04m:48s remains)
INFO - root - 2017-12-15 07:56:46.014500: step 29700, loss = 0.19, batch loss = 0.16 (35.7 examples/sec; 0.224 sec/batch; 18h:52m:11s remains)
2017-12-15 07:56:46.299844: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8098986 -5.1637383 -5.6159525 -6.0481606 -6.4717803 -6.3334322 -4.9524851 -4.1899433 -3.8610873 -3.3459721 -3.5644312 -3.9077916 -3.5934536 -2.730571 -2.149394][-3.9276457 -5.3022952 -5.8173494 -6.3117619 -6.7236094 -6.3428922 -4.2982178 -3.3246083 -3.0097408 -2.452687 -2.6631711 -2.85843 -2.18996 -0.90460992 -0.3670578][-4.8332868 -5.1345224 -5.59327 -6.0853262 -6.434916 -5.8232946 -3.2379453 -1.9363618 -1.5888652 -1.343689 -1.9601585 -2.359803 -1.6565524 -0.25858557 0.22958994][-5.3652353 -4.9470606 -5.2450905 -5.5669045 -5.7578259 -4.9629259 -1.9327408 -0.2335937 0.15108252 -0.19545221 -1.2865274 -2.0220976 -1.666056 -0.64773321 -0.31765747][-6.0501051 -4.8156223 -4.8711581 -4.9022646 -4.8920693 -3.8335834 -0.38701367 1.6688068 2.0380683 1.0603349 -0.60807669 -1.7804933 -1.9411902 -1.5645853 -1.577456][-6.0336246 -4.7961569 -4.617219 -4.3087053 -4.0580578 -2.6265929 1.2399709 3.5721989 3.8346705 2.328074 0.17106414 -1.5280528 -2.2850137 -2.5835865 -3.1768956][-5.5023127 -4.8840733 -4.5841484 -4.0619788 -3.6709018 -1.9457827 2.2258034 4.7517009 5.0224919 3.2522254 0.8689611 -1.0376322 -2.2228293 -3.056834 -4.0615568][-5.749639 -5.0283241 -4.760498 -4.1980667 -3.8075383 -2.048182 2.20932 4.9013414 5.4172773 3.6929865 1.353025 -0.5870024 -1.7501442 -2.7454028 -3.6148624][-5.8040409 -5.1612091 -4.9960718 -4.5035348 -4.2243629 -2.7131763 1.2940073 3.9383135 4.7508368 3.3610182 1.4506974 -0.28636885 -1.2286695 -2.051966 -2.5946069][-5.8126249 -5.228775 -5.16354 -4.7452536 -4.5524273 -3.3077297 0.30262923 2.6874213 3.5759177 2.5293474 1.0782902 -0.39500916 -1.2116431 -1.9565411 -2.0111384][-5.7716346 -5.2018161 -5.2070451 -4.8711414 -4.7247314 -3.606741 -0.33186591 1.6684334 2.4372153 1.813035 0.85115385 -0.499066 -1.3534267 -2.3608 -2.1660907][-5.6858144 -5.0747061 -5.1150274 -4.9072533 -4.8691044 -3.9076157 -1.0795486 0.29874969 0.70864654 0.44183731 0.1624763 -0.82197642 -1.5557802 -2.7580144 -2.716929][-5.5829005 -4.8993359 -4.9356623 -4.8558245 -4.9382362 -4.1104984 -1.6952595 -0.80941391 -0.76784348 -0.933156 -0.719386 -1.1263795 -1.6506822 -2.965589 -2.9956937][-5.5004215 -4.7558656 -4.7720232 -4.7771425 -4.9340677 -4.2276082 -2.1558127 -1.4914043 -1.6002288 -1.9246914 -1.6840754 -1.7782719 -2.1038675 -3.5890102 -3.7638302][-5.4771547 -4.7086487 -4.7048035 -4.7584934 -4.9828587 -4.4536037 -2.7008562 -1.9833086 -1.9727721 -2.530771 -2.5711546 -2.7114584 -3.2053587 -4.6959562 -4.862422]]...]
INFO - root - 2017-12-15 07:56:48.590164: step 29710, loss = 0.23, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 18h:44m:38s remains)
INFO - root - 2017-12-15 07:56:50.865003: step 29720, loss = 0.26, batch loss = 0.23 (35.3 examples/sec; 0.227 sec/batch; 19h:03m:48s remains)
INFO - root - 2017-12-15 07:56:53.146830: step 29730, loss = 0.30, batch loss = 0.27 (34.0 examples/sec; 0.235 sec/batch; 19h:46m:14s remains)
INFO - root - 2017-12-15 07:56:55.407729: step 29740, loss = 0.26, batch loss = 0.22 (36.2 examples/sec; 0.221 sec/batch; 18h:36m:12s remains)
INFO - root - 2017-12-15 07:56:57.708779: step 29750, loss = 0.37, batch loss = 0.34 (34.2 examples/sec; 0.234 sec/batch; 19h:38m:49s remains)
INFO - root - 2017-12-15 07:56:59.963366: step 29760, loss = 0.41, batch loss = 0.38 (36.0 examples/sec; 0.222 sec/batch; 18h:42m:33s remains)
INFO - root - 2017-12-15 07:57:02.250334: step 29770, loss = 0.27, batch loss = 0.24 (34.4 examples/sec; 0.233 sec/batch; 19h:33m:31s remains)
INFO - root - 2017-12-15 07:57:04.519726: step 29780, loss = 0.23, batch loss = 0.20 (34.7 examples/sec; 0.230 sec/batch; 19h:22m:21s remains)
INFO - root - 2017-12-15 07:57:06.809875: step 29790, loss = 0.21, batch loss = 0.18 (35.2 examples/sec; 0.228 sec/batch; 19h:08m:14s remains)
INFO - root - 2017-12-15 07:57:09.092245: step 29800, loss = 0.22, batch loss = 0.19 (34.1 examples/sec; 0.235 sec/batch; 19h:43m:48s remains)
2017-12-15 07:57:09.385729: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5432491 -5.5915976 -5.3772392 -4.9494638 -4.608685 -4.5751886 -4.914156 -5.12561 -5.2746754 -5.4843187 -5.5674095 -5.360837 -4.8965688 -4.4160242 -3.8369431][-5.8189816 -6.1794524 -5.8152637 -5.2318077 -4.6426487 -4.26278 -4.3186731 -4.5059843 -4.8081207 -5.3198195 -5.7661467 -5.8602705 -5.4169235 -4.8078241 -4.01532][-6.6516237 -6.1809416 -5.8759465 -5.3687878 -4.6349697 -3.8973823 -3.7491016 -4.0406337 -4.6701469 -5.4799209 -6.2523813 -6.6880341 -6.2346663 -5.4620609 -4.4010706][-7.3810749 -6.3788195 -5.9130492 -5.1301756 -3.7152483 -2.1440842 -1.49704 -1.9343972 -3.1447723 -4.4844313 -5.8827896 -6.9120665 -6.7756348 -6.2509389 -5.2087269][-7.858057 -6.5273848 -5.8078065 -4.5681071 -2.2492986 0.39676404 1.6238775 0.99424505 -0.84953451 -2.8057261 -4.8410635 -6.5304165 -6.829217 -6.6693754 -5.7419691][-8.1000957 -6.2772856 -5.2996621 -3.5433521 -0.21196103 3.3588459 4.9494038 3.8890569 1.2978315 -1.3245072 -3.8003383 -5.9370937 -6.4690084 -6.5115519 -5.6521459][-7.8317003 -6.0306649 -4.5588865 -2.0455458 2.2306807 6.4124269 7.8915348 6.345439 3.2145402 -0.067617655 -2.9593329 -5.4607325 -6.2934084 -6.5500116 -5.710784][-7.6871386 -6.1269531 -4.4904671 -1.7946877 2.7713754 6.8951626 8.0482826 6.3390512 3.2147691 -0.37822258 -3.3219566 -5.7438965 -6.6668062 -6.8906507 -5.9225903][-7.8841496 -6.6673689 -5.0971994 -2.6390979 1.4731729 4.9367466 5.4702749 3.7579873 1.1644931 -2.0174012 -4.4573183 -6.4907093 -7.4461107 -7.4939871 -6.2367887][-8.1907759 -7.0385189 -5.4628296 -3.2943311 -0.00092434883 2.2990444 2.014415 0.3547709 -1.6578002 -4.0753326 -5.7182007 -7.1402712 -7.7627611 -7.5263014 -6.1804476][-8.3086243 -7.479641 -6.133328 -4.506475 -2.2897038 -1.1697291 -2.0247462 -3.2617545 -4.5122566 -6.1483936 -7.2937517 -8.30187 -8.4875 -7.8389444 -6.4481192][-8.442667 -7.8813019 -6.8919497 -5.8121185 -4.5892334 -4.3556967 -5.4592466 -6.5226669 -7.363368 -8.3557119 -8.9694633 -9.4798622 -9.2796135 -8.3130922 -6.9510784][-8.7505646 -8.00905 -7.0010376 -6.1641707 -5.7190228 -6.0188823 -7.0468884 -7.7991257 -8.15843 -8.415309 -8.5821133 -8.8249483 -8.578043 -7.775178 -6.8526258][-8.9904613 -8.022563 -7.0212793 -6.2566633 -5.9744306 -6.0532675 -6.55357 -6.9046755 -7.0830488 -7.1786823 -7.2174168 -7.3485737 -7.1530266 -6.6246128 -6.1771684][-9.119585 -7.8418961 -6.9090443 -6.267724 -6.0465994 -5.9818468 -6.1097994 -6.1988964 -6.2713203 -6.3126497 -6.2825451 -6.2349687 -6.1167879 -5.8646421 -5.7333031]]...]
INFO - root - 2017-12-15 07:57:11.670220: step 29810, loss = 0.33, batch loss = 0.30 (36.1 examples/sec; 0.222 sec/batch; 18h:38m:56s remains)
INFO - root - 2017-12-15 07:57:13.909172: step 29820, loss = 0.22, batch loss = 0.19 (36.4 examples/sec; 0.220 sec/batch; 18h:29m:33s remains)
INFO - root - 2017-12-15 07:57:16.208388: step 29830, loss = 0.19, batch loss = 0.16 (35.7 examples/sec; 0.224 sec/batch; 18h:49m:45s remains)
INFO - root - 2017-12-15 07:57:18.502762: step 29840, loss = 0.23, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 18h:41m:42s remains)
INFO - root - 2017-12-15 07:57:20.794550: step 29850, loss = 0.28, batch loss = 0.25 (34.6 examples/sec; 0.231 sec/batch; 19h:25m:11s remains)
INFO - root - 2017-12-15 07:57:23.065308: step 29860, loss = 0.30, batch loss = 0.26 (34.5 examples/sec; 0.232 sec/batch; 19h:28m:01s remains)
INFO - root - 2017-12-15 07:57:25.312764: step 29870, loss = 0.24, batch loss = 0.21 (37.1 examples/sec; 0.216 sec/batch; 18h:07m:02s remains)
INFO - root - 2017-12-15 07:57:27.579689: step 29880, loss = 0.25, batch loss = 0.21 (35.9 examples/sec; 0.223 sec/batch; 18h:42m:36s remains)
INFO - root - 2017-12-15 07:57:29.862490: step 29890, loss = 0.25, batch loss = 0.22 (34.1 examples/sec; 0.234 sec/batch; 19h:42m:35s remains)
INFO - root - 2017-12-15 07:57:32.178107: step 29900, loss = 0.20, batch loss = 0.17 (34.9 examples/sec; 0.229 sec/batch; 19h:16m:39s remains)
2017-12-15 07:57:32.493162: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7409573 -4.7516561 -6.0386496 -6.8469706 -6.8330975 -6.1759691 -4.830884 -4.1065044 -4.7330217 -5.193634 -4.6300888 -3.4625721 -2.7410872 -2.5082555 -2.2746542][-4.2886553 -5.120038 -5.9230556 -6.6315727 -6.5614147 -5.7187481 -4.0357356 -3.1152127 -3.9280908 -4.8260918 -4.606431 -3.5170355 -2.7342386 -2.5863605 -2.3035812][-5.388607 -4.952837 -5.1843643 -5.5204225 -5.512619 -4.7489891 -2.9614229 -1.9271791 -3.0713665 -4.4761209 -4.601191 -3.5974638 -2.8979995 -2.6788023 -2.082984][-5.7033415 -4.5789485 -4.100543 -3.9722338 -3.6859124 -2.84955 -1.112498 -0.2573297 -1.7793968 -3.6818306 -4.3360682 -3.4708238 -2.9183855 -2.5262721 -1.6678972][-5.7323642 -4.416831 -3.453588 -2.7335062 -1.8083706 -0.7023468 1.0459569 1.5287716 -0.40188932 -2.8072364 -4.0715523 -3.3897829 -2.8868871 -2.2366717 -1.2405046][-5.3019128 -4.200563 -3.2299111 -1.8830525 -0.072964907 1.5831189 3.4111998 3.4599626 0.83779287 -2.1436028 -4.0659494 -3.6724591 -3.2283058 -2.4896805 -1.4991788][-4.8030233 -4.005147 -3.012764 -1.2518651 1.3468113 3.8065312 6.0058384 5.8282185 2.5764792 -1.0282878 -3.6273389 -3.767597 -3.5220053 -2.8780479 -2.1003945][-4.6759663 -3.8911729 -3.1568773 -1.4067771 1.2839682 4.0648489 6.6085682 6.5686378 3.2932737 -0.52248478 -3.4130645 -3.9497204 -3.9604607 -3.5195384 -3.0118563][-4.7668071 -3.5372384 -3.1124325 -1.829865 0.27075195 2.4023297 4.6366768 4.8494625 2.2083538 -1.1491549 -3.7737894 -4.3822412 -4.4690146 -4.1835294 -3.8548551][-5.054265 -2.8344619 -2.3722539 -1.7875143 -0.54768348 0.7770679 2.1880739 2.3539021 0.51638937 -2.1494913 -4.2796321 -4.6872807 -4.5943274 -4.1992674 -3.8799572][-5.894496 -2.8160133 -2.0234547 -1.9076338 -1.4154708 -0.74080765 -0.014616489 0.056133986 -0.98229671 -2.9095085 -4.4939404 -4.6143489 -4.3019333 -3.7155089 -3.4405065][-6.9021587 -3.3860693 -2.3222117 -2.3356218 -2.207166 -2.0583587 -1.7531607 -1.4253104 -1.5427709 -2.6921928 -3.770422 -3.6822994 -3.410852 -2.9391634 -2.7939639][-7.3242636 -3.9392552 -2.8129597 -2.6725476 -2.6251929 -2.5883408 -2.4397991 -2.0225227 -1.5717533 -2.1253364 -2.8199744 -2.7616203 -2.7433548 -2.4463656 -2.1526446][-7.4752731 -4.7194586 -3.6685584 -3.3306704 -3.2730169 -3.248543 -3.2709618 -2.8593369 -2.1580927 -2.2356865 -2.55832 -2.4978554 -2.5914831 -2.3214571 -1.5487249][-6.9677773 -4.8328981 -4.1513462 -3.8730111 -3.8089111 -3.8203516 -3.9101768 -3.489758 -2.6467619 -2.23824 -2.3180304 -2.5097735 -2.823751 -2.6688604 -1.5124731]]...]
INFO - root - 2017-12-15 07:57:34.790354: step 29910, loss = 0.32, batch loss = 0.29 (33.9 examples/sec; 0.236 sec/batch; 19h:48m:44s remains)
INFO - root - 2017-12-15 07:57:37.065423: step 29920, loss = 0.35, batch loss = 0.32 (35.8 examples/sec; 0.223 sec/batch; 18h:46m:36s remains)
INFO - root - 2017-12-15 07:57:39.354981: step 29930, loss = 0.35, batch loss = 0.31 (34.7 examples/sec; 0.230 sec/batch; 19h:21m:26s remains)
INFO - root - 2017-12-15 07:57:41.639532: step 29940, loss = 0.27, batch loss = 0.24 (34.3 examples/sec; 0.234 sec/batch; 19h:37m:39s remains)
INFO - root - 2017-12-15 07:57:43.888756: step 29950, loss = 0.42, batch loss = 0.38 (36.1 examples/sec; 0.222 sec/batch; 18h:38m:48s remains)
INFO - root - 2017-12-15 07:57:46.187569: step 29960, loss = 0.18, batch loss = 0.15 (33.0 examples/sec; 0.242 sec/batch; 20h:22m:22s remains)
INFO - root - 2017-12-15 07:57:48.503995: step 29970, loss = 0.21, batch loss = 0.18 (34.7 examples/sec; 0.231 sec/batch; 19h:23m:40s remains)
INFO - root - 2017-12-15 07:57:50.800532: step 29980, loss = 0.20, batch loss = 0.17 (35.1 examples/sec; 0.228 sec/batch; 19h:09m:35s remains)
INFO - root - 2017-12-15 07:57:53.089939: step 29990, loss = 0.25, batch loss = 0.21 (35.8 examples/sec; 0.224 sec/batch; 18h:47m:47s remains)
INFO - root - 2017-12-15 07:57:55.372860: step 30000, loss = 0.27, batch loss = 0.24 (35.1 examples/sec; 0.228 sec/batch; 19h:09m:28s remains)
2017-12-15 07:57:55.653789: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7324638 -5.4952955 -4.6222677 -3.1437135 -1.8514755 -1.2686341 -2.2958066 -3.4986548 -3.6252894 -3.1329184 -2.765496 -2.8063569 -3.1636171 -2.7221272 -3.3055251][-3.7872834 -4.9162464 -3.8305619 -2.3952906 -1.2437588 -0.89338589 -2.0872097 -3.1333189 -3.2215884 -2.7627771 -2.3604553 -2.5080571 -2.7926495 -2.6340232 -3.2793975][-3.5205975 -4.2281704 -3.2130094 -2.2858059 -1.5999501 -1.2774491 -2.1578069 -2.9055188 -2.9752233 -2.6255221 -2.2611837 -2.4644268 -2.6530993 -2.7949913 -3.6748748][-3.5261414 -3.8071642 -3.0297427 -2.5800228 -2.1865749 -1.6878376 -2.0789554 -2.5009117 -2.62777 -2.3818762 -2.2293069 -2.5442924 -2.6223812 -2.9521973 -3.9866238][-3.821805 -3.3500509 -2.9876094 -2.967108 -2.6665003 -1.8653386 -1.814733 -2.0136883 -2.1550276 -1.9967043 -2.1039798 -2.5475976 -2.7372162 -3.2048969 -4.2022762][-3.8568935 -2.8869369 -3.0398479 -3.4271321 -3.2223291 -2.3170424 -2.0007749 -2.0434704 -2.1265244 -1.9711568 -2.1131258 -2.3475344 -2.2904966 -2.6990287 -3.7225587][-3.469939 -2.2911546 -2.7412183 -3.2844188 -3.1089609 -2.1738129 -1.6571739 -1.5340679 -1.5304956 -1.3885133 -1.6397827 -1.7314966 -1.6549094 -2.0218489 -3.0686464][-3.432569 -1.9262719 -2.33806 -2.789067 -2.6202595 -1.6610109 -1.1124961 -0.82560658 -0.7702955 -0.97476685 -1.4312139 -1.4249778 -1.401499 -1.7071598 -2.5724435][-4.2956161 -2.2926142 -2.3641686 -2.4205844 -2.1304419 -1.4842076 -1.1383953 -0.64196956 -0.56085145 -1.0576413 -1.5000613 -1.318658 -1.316505 -1.528564 -2.3271906][-4.5462828 -2.106849 -1.6069014 -1.2094545 -1.0814058 -1.0751086 -0.8947314 -0.32528913 -0.21181893 -0.80305052 -1.1424263 -0.98234928 -1.235589 -1.3784578 -2.157079][-4.57041 -1.9447688 -1.0436103 -0.26958811 -0.23750019 -0.58961594 -0.52568424 -0.069247723 -0.086960554 -0.80557859 -1.0347251 -0.96076512 -1.2049366 -1.3438568 -2.2821813][-4.9878883 -2.4812636 -1.4875642 -0.58194649 -0.64867663 -1.1790735 -1.1054688 -0.53278065 -0.50117493 -1.1227477 -1.2630053 -1.1984479 -1.3216827 -1.5409062 -2.7580237][-5.2411823 -3.2447577 -2.3645833 -1.4637886 -1.4143705 -1.8206112 -1.5184996 -1.0533447 -1.2620194 -1.9090518 -2.1263695 -2.0253725 -2.0518756 -2.402679 -3.8961759][-6.149435 -4.8007784 -4.0532351 -3.1305017 -2.8755112 -3.1248531 -2.7652659 -2.6326551 -3.0878646 -3.8023477 -3.9974716 -3.7824285 -3.6734393 -4.0575194 -5.5345883][-7.0645676 -5.962327 -5.4328337 -4.6735415 -4.3709927 -4.5100212 -4.349256 -4.5643034 -5.1470003 -5.7521372 -5.7267904 -5.290452 -5.064662 -5.3455853 -6.5954142]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 07:57:58.281693: step 30010, loss = 0.16, batch loss = 0.12 (34.7 examples/sec; 0.230 sec/batch; 19h:20m:53s remains)
INFO - root - 2017-12-15 07:58:00.533688: step 30020, loss = 0.37, batch loss = 0.34 (35.9 examples/sec; 0.223 sec/batch; 18h:43m:22s remains)
INFO - root - 2017-12-15 07:58:02.823181: step 30030, loss = 0.26, batch loss = 0.23 (34.4 examples/sec; 0.233 sec/batch; 19h:32m:43s remains)
INFO - root - 2017-12-15 07:58:05.082920: step 30040, loss = 0.31, batch loss = 0.28 (34.1 examples/sec; 0.235 sec/batch; 19h:42m:27s remains)
INFO - root - 2017-12-15 07:58:07.385907: step 30050, loss = 0.18, batch loss = 0.15 (35.7 examples/sec; 0.224 sec/batch; 18h:49m:16s remains)
INFO - root - 2017-12-15 07:58:09.676568: step 30060, loss = 0.27, batch loss = 0.24 (33.6 examples/sec; 0.238 sec/batch; 20h:01m:10s remains)
INFO - root - 2017-12-15 07:58:11.939428: step 30070, loss = 0.21, batch loss = 0.18 (34.7 examples/sec; 0.230 sec/batch; 19h:20m:26s remains)
INFO - root - 2017-12-15 07:58:14.226393: step 30080, loss = 0.28, batch loss = 0.24 (35.8 examples/sec; 0.223 sec/batch; 18h:46m:16s remains)
INFO - root - 2017-12-15 07:58:16.522027: step 30090, loss = 0.27, batch loss = 0.24 (34.1 examples/sec; 0.235 sec/batch; 19h:43m:04s remains)
INFO - root - 2017-12-15 07:58:18.839929: step 30100, loss = 0.20, batch loss = 0.17 (34.5 examples/sec; 0.232 sec/batch; 19h:30m:09s remains)
2017-12-15 07:58:19.134733: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6918378 -4.9952135 -4.0123024 -3.3037639 -2.8058317 -3.1674287 -3.8567224 -4.3691111 -4.7113791 -4.5092306 -3.9937122 -3.7961245 -3.7718306 -3.5183411 -3.5190258][-5.5826912 -5.2264776 -4.1989989 -3.4263742 -3.0573976 -3.520577 -4.5055761 -5.3311214 -6.0006886 -6.0895829 -5.7692337 -5.49165 -5.2579079 -4.9643316 -4.8395748][-5.4563036 -4.8073149 -3.8488963 -2.9899955 -2.6208265 -3.2884221 -4.3953695 -5.2913885 -6.2216697 -6.6696415 -6.5727253 -6.2220449 -5.8750463 -5.538765 -5.1657567][-4.9614544 -3.8559785 -2.9156594 -1.9297576 -1.4322947 -2.0494845 -3.0883369 -4.022893 -5.1910048 -6.0329494 -6.215415 -5.8202324 -5.4015083 -5.08263 -4.6675997][-4.3849745 -2.6466792 -1.7125318 -0.71320939 0.034353971 -0.29349864 -1.0585138 -1.8894806 -3.2850182 -4.603723 -5.1714363 -4.8877344 -4.4934711 -4.1072035 -3.4842792][-4.0366259 -1.8118905 -0.99618816 -0.0795598 0.91759157 1.0568388 0.84433508 0.29589224 -1.260926 -3.1103704 -4.1147976 -4.0369205 -3.7298613 -3.0654154 -1.9154463][-3.8816702 -1.9847337 -1.2692871 -0.40529346 0.69496465 1.1814885 1.5046134 1.4285686 -0.0162704 -2.097307 -3.3123424 -3.2520075 -2.950346 -2.0795264 -0.51331377][-4.9807816 -3.0192723 -2.1660397 -1.157621 -0.026084423 0.5463779 1.1339636 1.3873513 0.29349518 -1.6749141 -2.8229251 -2.849813 -2.7427487 -1.7950983 0.0064826012][-6.1065087 -4.3879452 -3.5502939 -2.4492693 -1.2715728 -0.62402833 0.074988127 0.49547577 -0.26058352 -1.9724958 -3.0042224 -3.1766882 -3.2653041 -2.4579988 -0.764838][-6.1595163 -4.6736412 -3.9630942 -2.8562598 -1.8310798 -1.3022702 -0.81143463 -0.54814374 -1.3106947 -2.79846 -3.7664957 -4.1009431 -4.4431872 -3.9778109 -2.79073][-5.269918 -3.7545626 -3.1106696 -2.1750088 -1.6217108 -1.6609849 -1.7891804 -1.9654136 -2.8324206 -3.9871373 -4.7096186 -5.0836296 -5.5568171 -5.3637276 -4.6368337][-4.1887569 -2.332267 -1.5443655 -0.73705649 -0.65413308 -1.4764752 -2.2825801 -2.7755103 -3.7592459 -4.7548342 -5.3837519 -5.851202 -6.4197617 -6.34272 -5.8074465][-3.5860147 -1.3995099 -0.4358561 0.38728142 0.27839661 -0.92220294 -2.23432 -2.9927955 -4.0534482 -4.917182 -5.5189352 -6.0356922 -6.5917559 -6.6481972 -6.4744186][-3.659749 -1.3264771 -0.29210234 0.56657434 0.58378816 -0.51054776 -1.9440076 -2.7863102 -3.915699 -4.65609 -5.1897478 -5.7014132 -6.2439127 -6.4674711 -6.6166763][-4.45965 -2.2626545 -1.3458931 -0.62505019 -0.477306 -1.2931583 -2.5520017 -3.2830136 -4.18478 -4.5284967 -4.5975533 -4.85458 -5.2728834 -5.5631289 -5.7714481]]...]
INFO - root - 2017-12-15 07:58:21.442350: step 30110, loss = 0.22, batch loss = 0.18 (36.2 examples/sec; 0.221 sec/batch; 18h:32m:35s remains)
INFO - root - 2017-12-15 07:58:23.727157: step 30120, loss = 0.27, batch loss = 0.24 (34.7 examples/sec; 0.230 sec/batch; 19h:20m:18s remains)
INFO - root - 2017-12-15 07:58:26.029338: step 30130, loss = 0.20, batch loss = 0.17 (35.1 examples/sec; 0.228 sec/batch; 19h:08m:57s remains)
INFO - root - 2017-12-15 07:58:28.280852: step 30140, loss = 0.24, batch loss = 0.21 (37.0 examples/sec; 0.216 sec/batch; 18h:08m:15s remains)
INFO - root - 2017-12-15 07:58:30.534295: step 30150, loss = 0.20, batch loss = 0.17 (35.7 examples/sec; 0.224 sec/batch; 18h:47m:51s remains)
INFO - root - 2017-12-15 07:58:32.800364: step 30160, loss = 0.30, batch loss = 0.27 (35.8 examples/sec; 0.224 sec/batch; 18h:46m:17s remains)
INFO - root - 2017-12-15 07:58:35.086080: step 30170, loss = 0.32, batch loss = 0.28 (36.7 examples/sec; 0.218 sec/batch; 18h:18m:39s remains)
INFO - root - 2017-12-15 07:58:37.368572: step 30180, loss = 0.32, batch loss = 0.29 (36.3 examples/sec; 0.220 sec/batch; 18h:30m:47s remains)
INFO - root - 2017-12-15 07:58:39.685676: step 30190, loss = 0.19, batch loss = 0.16 (34.1 examples/sec; 0.235 sec/batch; 19h:42m:20s remains)
INFO - root - 2017-12-15 07:58:41.984369: step 30200, loss = 0.22, batch loss = 0.19 (34.3 examples/sec; 0.234 sec/batch; 19h:36m:48s remains)
2017-12-15 07:58:42.283718: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5056314 -5.3348475 -4.5213675 -4.9807768 -5.917943 -6.5181322 -6.5818262 -6.2325253 -4.3846936 -2.0616391 -0.32480991 -0.21921921 -1.5783776 -2.8349662 -3.7606568][-5.1149988 -4.7547336 -3.6699347 -3.7771204 -4.5051804 -4.9614611 -5.1174421 -5.1301475 -3.8902922 -2.2630186 -0.9782232 -0.80613887 -1.8227521 -2.9877555 -3.9883039][-6.4235759 -4.8750458 -3.5589538 -3.3027587 -3.661921 -3.7613235 -3.8318386 -4.0284004 -3.4008975 -2.6281762 -2.066251 -1.866804 -2.4452257 -3.3810167 -4.3005209][-7.5897789 -5.2252183 -3.727725 -3.1774979 -3.1513863 -2.8485169 -2.7827644 -3.1769595 -3.1166797 -3.046051 -3.0775678 -2.9684594 -3.25479 -3.9842677 -4.8328905][-8.1831522 -5.4692831 -3.6683912 -2.6396511 -2.0021753 -1.1232507 -0.76388121 -1.4379172 -2.1951389 -2.8768692 -3.4113064 -3.4058383 -3.4998055 -4.0349488 -4.9503908][-8.7633953 -5.6284981 -3.4640899 -1.8174052 -0.31932902 1.2459641 1.8935847 0.81292629 -0.82922125 -2.2741556 -3.1554961 -3.1964111 -3.0073581 -3.3321309 -4.4155741][-8.4193153 -5.7088737 -3.2953105 -1.0599262 1.3466287 3.6177254 4.4204121 2.900672 0.40415311 -1.7001443 -2.7474487 -2.6345272 -1.9968243 -2.0420346 -3.3108125][-8.4440517 -5.8016472 -3.2785246 -0.64729345 2.4204464 5.2161169 6.0912819 4.282155 1.3224945 -1.1350349 -2.1931286 -1.7304521 -0.63440943 -0.46323824 -1.8232281][-8.4849758 -6.2374516 -3.847909 -1.1604259 1.9968419 4.8319669 5.540637 3.6809988 0.69027734 -1.6962199 -2.4172881 -1.5776162 0.024518013 0.59208417 -0.64405036][-8.4964352 -6.6941581 -4.5182238 -2.0414066 0.6421566 3.007977 3.3616428 1.5812199 -0.91886795 -2.8217759 -3.0470147 -1.8490423 -0.19744754 0.46484542 -0.58947766][-8.6979923 -7.3123288 -5.4227738 -3.2861362 -1.133689 0.69993258 0.79372835 -0.82417679 -3.0203545 -4.5479641 -4.3420954 -2.9302044 -1.2821124 -0.50541663 -1.3377433][-8.94552 -7.9279313 -6.4697762 -4.929472 -3.5356879 -2.457293 -2.6510954 -4.0997124 -5.811204 -6.7583284 -6.1102629 -4.6789427 -3.3276122 -2.7036335 -3.3717766][-8.9026794 -8.1625595 -7.16955 -6.22869 -5.48006 -4.9819727 -5.3743877 -6.5261011 -7.7397103 -8.2110081 -7.4343581 -6.1745338 -5.3010969 -5.0495453 -5.6575251][-7.9935637 -7.349401 -6.7138891 -6.19928 -5.9195271 -5.90016 -6.3877711 -7.2892847 -8.0979948 -8.2047424 -7.4440756 -6.4766836 -5.9036241 -5.9399328 -6.49057][-6.7448716 -6.1059122 -5.79046 -5.635582 -5.6569128 -5.8816013 -6.3500795 -6.8395982 -7.1279383 -7.0084949 -6.5464544 -6.0737538 -5.9013271 -6.0469284 -6.3598065]]...]
INFO - root - 2017-12-15 07:58:44.592435: step 30210, loss = 0.18, batch loss = 0.15 (33.6 examples/sec; 0.238 sec/batch; 19h:58m:20s remains)
INFO - root - 2017-12-15 07:58:46.885839: step 30220, loss = 0.27, batch loss = 0.24 (34.6 examples/sec; 0.231 sec/batch; 19h:24m:26s remains)
INFO - root - 2017-12-15 07:58:49.131355: step 30230, loss = 0.19, batch loss = 0.16 (36.0 examples/sec; 0.222 sec/batch; 18h:40m:32s remains)
INFO - root - 2017-12-15 07:58:51.381854: step 30240, loss = 0.33, batch loss = 0.30 (34.3 examples/sec; 0.233 sec/batch; 19h:35m:44s remains)
INFO - root - 2017-12-15 07:58:53.657041: step 30250, loss = 0.26, batch loss = 0.23 (35.0 examples/sec; 0.229 sec/batch; 19h:12m:28s remains)
INFO - root - 2017-12-15 07:58:55.921856: step 30260, loss = 0.28, batch loss = 0.24 (36.2 examples/sec; 0.221 sec/batch; 18h:32m:09s remains)
INFO - root - 2017-12-15 07:58:58.229840: step 30270, loss = 0.25, batch loss = 0.21 (35.8 examples/sec; 0.224 sec/batch; 18h:46m:23s remains)
INFO - root - 2017-12-15 07:59:00.484821: step 30280, loss = 0.31, batch loss = 0.27 (34.7 examples/sec; 0.231 sec/batch; 19h:22m:16s remains)
INFO - root - 2017-12-15 07:59:02.784170: step 30290, loss = 0.18, batch loss = 0.15 (34.2 examples/sec; 0.234 sec/batch; 19h:39m:25s remains)
INFO - root - 2017-12-15 07:59:05.047248: step 30300, loss = 0.22, batch loss = 0.19 (35.5 examples/sec; 0.226 sec/batch; 18h:56m:23s remains)
2017-12-15 07:59:05.323100: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8617969 -5.3217983 -5.5563707 -5.6654644 -5.6908584 -5.4643717 -5.2490311 -5.2777548 -5.3172016 -5.4412165 -5.4797196 -5.422616 -5.2832241 -5.323411 -5.4629645][-4.8123012 -6.7011442 -7.1362758 -7.3430924 -7.2383375 -6.6270871 -5.98759 -5.8310642 -6.0217228 -6.3456211 -6.3661084 -5.9865475 -5.3660269 -5.2632608 -5.6394978][-6.1603665 -7.638104 -8.2325525 -8.5394421 -8.2806454 -7.2708483 -6.1395307 -5.7242565 -6.0697975 -6.6987143 -6.9145727 -6.4038296 -5.4015427 -5.1306171 -5.5511179][-7.0376511 -8.0587988 -8.6470127 -8.8748474 -8.4035835 -7.1394167 -5.6802568 -4.9387908 -5.2384915 -5.8970413 -6.1611733 -5.6103191 -4.5801744 -4.5385017 -5.3080425][-8.0076923 -8.6125755 -9.1104736 -9.1370153 -8.4543982 -6.9952688 -5.1418147 -3.903784 -3.9188321 -4.4267864 -4.6592789 -4.0956411 -3.1305676 -3.3200717 -4.4355955][-7.9236989 -8.2911539 -8.4718266 -8.0354271 -6.9721518 -5.3828249 -3.4710174 -2.2199414 -2.2084122 -2.6858006 -2.958487 -2.4286642 -1.5270295 -1.7139176 -2.8679197][-7.1981478 -7.736515 -7.8330522 -7.059155 -5.63846 -3.8455405 -1.8586252 -0.72932661 -0.82264125 -1.3706595 -1.7398596 -1.2676065 -0.27748871 -0.13568711 -0.87029994][-6.75898 -7.0427418 -7.0395746 -6.119421 -4.6572475 -2.9131222 -0.95079803 0.086050987 -0.11122775 -0.68561614 -1.1772966 -0.84635496 0.23436093 0.76699352 0.60535264][-6.389523 -6.4742608 -6.3723664 -5.429728 -4.1074405 -2.4975712 -0.55750585 0.47722125 0.30792856 -0.12764025 -0.63508117 -0.35821533 0.89871883 1.8310142 2.1279812][-6.3324823 -6.4316549 -6.4460092 -5.7932234 -4.6778841 -3.1048455 -1.1357627 0.004951 0.096508265 -0.0256536 -0.69084 -0.46050429 0.97771287 2.290513 2.9227734][-6.2056694 -6.3558507 -6.59022 -6.3231812 -5.3398409 -3.8527906 -2.0899408 -0.91351581 -0.48571527 -0.48423386 -1.6278309 -1.5055397 -0.074134111 1.3516932 2.0934181][-6.048646 -6.1441951 -6.546639 -6.5755796 -5.7165971 -4.4734788 -3.1195602 -1.9344227 -1.1994466 -1.2713733 -2.6314187 -2.4471724 -1.1722159 0.16199946 0.84013462][-6.0948448 -6.0751772 -6.4980226 -6.5626664 -5.8601789 -5.1068039 -4.293746 -3.350421 -2.6279516 -2.9396708 -4.2445335 -3.881803 -2.7571182 -1.671617 -1.2149523][-6.0090747 -5.9650855 -6.4662218 -6.5789938 -6.0724282 -5.6892176 -5.2559667 -4.55266 -4.0817194 -4.636343 -5.6671133 -5.2248535 -4.3666835 -3.5843701 -3.3126192][-5.6307945 -5.5071182 -6.0353937 -6.2222137 -6.0258842 -5.9236965 -5.6910248 -5.1634846 -4.9178534 -5.4731236 -6.0113907 -5.669755 -5.140954 -4.6951275 -4.6811562]]...]
INFO - root - 2017-12-15 07:59:07.566160: step 30310, loss = 0.33, batch loss = 0.30 (35.4 examples/sec; 0.226 sec/batch; 18h:59m:13s remains)
INFO - root - 2017-12-15 07:59:09.856510: step 30320, loss = 0.28, batch loss = 0.25 (35.6 examples/sec; 0.225 sec/batch; 18h:53m:20s remains)
INFO - root - 2017-12-15 07:59:12.148082: step 30330, loss = 0.20, batch loss = 0.16 (35.2 examples/sec; 0.227 sec/batch; 19h:03m:21s remains)
INFO - root - 2017-12-15 07:59:14.419615: step 30340, loss = 0.23, batch loss = 0.20 (35.3 examples/sec; 0.227 sec/batch; 19h:01m:51s remains)
INFO - root - 2017-12-15 07:59:16.666262: step 30350, loss = 0.32, batch loss = 0.29 (34.8 examples/sec; 0.230 sec/batch; 19h:16m:25s remains)
INFO - root - 2017-12-15 07:59:18.965760: step 30360, loss = 0.21, batch loss = 0.18 (34.5 examples/sec; 0.232 sec/batch; 19h:28m:40s remains)
INFO - root - 2017-12-15 07:59:21.237111: step 30370, loss = 0.20, batch loss = 0.16 (34.8 examples/sec; 0.230 sec/batch; 19h:19m:05s remains)
INFO - root - 2017-12-15 07:59:23.521557: step 30380, loss = 0.24, batch loss = 0.21 (33.9 examples/sec; 0.236 sec/batch; 19h:46m:32s remains)
INFO - root - 2017-12-15 07:59:25.799738: step 30390, loss = 0.29, batch loss = 0.25 (35.3 examples/sec; 0.227 sec/batch; 19h:00m:54s remains)
INFO - root - 2017-12-15 07:59:28.091136: step 30400, loss = 0.31, batch loss = 0.28 (36.2 examples/sec; 0.221 sec/batch; 18h:32m:37s remains)
2017-12-15 07:59:28.379062: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.6012367 -2.7535448 -3.3253016 -3.7366869 -4.0699911 -4.2900791 -4.2813234 -4.1672845 -3.9995067 -3.8560176 -3.6300254 -3.3062358 -2.9423714 -2.6520727 -2.4501293][-2.5668011 -4.11871 -4.59894 -4.8834314 -5.13789 -5.2981791 -5.258585 -5.1378794 -5.0363865 -4.9451785 -4.6886415 -4.29331 -3.794194 -3.3378022 -2.9882724][-4.4707417 -5.21982 -5.482029 -5.498745 -5.5871067 -5.6844883 -5.6334815 -5.5433574 -5.6165333 -5.7376242 -5.6214027 -5.3085461 -4.8258395 -4.3847661 -3.9853721][-5.6699495 -5.7754669 -5.716085 -5.2959251 -4.9986205 -4.8490353 -4.7210445 -4.6281633 -4.9080524 -5.3756237 -5.6256928 -5.6359758 -5.3357458 -5.1211634 -4.9065933][-6.101141 -5.61546 -5.202714 -4.2470779 -3.3610477 -2.768239 -2.4079487 -2.2284033 -2.6997163 -3.579545 -4.3143053 -4.7863874 -4.8689551 -5.0334597 -5.1424341][-5.5063715 -4.6251774 -3.9773326 -2.6674571 -1.3479743 -0.30501151 0.38253903 0.787462 0.2941947 -0.91915011 -2.0952785 -2.9963508 -3.524189 -4.1633296 -4.6402984][-4.1664467 -3.4700131 -2.7839384 -1.4038135 0.18843246 1.7539551 2.8831489 3.5810511 3.2030571 1.8359039 0.39454985 -0.74817848 -1.5743128 -2.59409 -3.3767362][-3.3684428 -2.621161 -2.054373 -0.82537329 0.88578033 2.8175218 4.265295 5.2513533 5.172966 3.9560549 2.5595386 1.4187968 0.37313056 -0.936319 -1.8313032][-3.2108164 -2.3747981 -1.8967072 -0.91588676 0.64848733 2.5802667 4.1158772 5.245821 5.4412909 4.4658852 3.3099182 2.4374077 1.4196975 0.066850424 -0.58638406][-3.655211 -2.7744281 -2.3503225 -1.6277763 -0.32918823 1.4224501 2.8449152 3.9100683 4.2206459 3.5060532 2.6984937 2.2257693 1.4054325 0.18079138 -0.1326232][-4.2835321 -3.5859129 -3.3179326 -2.8680058 -1.9189768 -0.63285053 0.39311147 1.17535 1.4953558 1.1704621 0.90002584 0.92879009 0.43215466 -0.487718 -0.52928984][-4.8317585 -4.4308796 -4.4145918 -4.223506 -3.6048322 -2.809444 -2.1785927 -1.7215009 -1.492703 -1.5439878 -1.3875241 -0.97587669 -1.0729641 -1.5742055 -1.4437276][-4.8554087 -4.7563505 -5.0435247 -5.0453091 -4.6130838 -4.0820379 -3.6443117 -3.4163377 -3.2907436 -3.2295601 -2.8965333 -2.3433487 -2.1580377 -2.3157198 -2.1445386][-4.4145522 -4.58218 -5.1696033 -5.2044749 -4.7653971 -4.2861576 -3.8975735 -3.7872972 -3.7130091 -3.6112285 -3.2527869 -2.7144337 -2.4411683 -2.4732428 -2.5179074][-3.9955239 -4.2789822 -5.0620623 -5.0612278 -4.58305 -4.0690794 -3.569911 -3.3306746 -3.1009812 -2.9911346 -2.8143022 -2.5083606 -2.2688987 -2.2863212 -2.5726366]]...]
INFO - root - 2017-12-15 07:59:30.625923: step 30410, loss = 0.17, batch loss = 0.14 (35.2 examples/sec; 0.228 sec/batch; 19h:05m:25s remains)
INFO - root - 2017-12-15 07:59:32.896511: step 30420, loss = 0.29, batch loss = 0.25 (34.5 examples/sec; 0.232 sec/batch; 19h:28m:28s remains)
INFO - root - 2017-12-15 07:59:35.148786: step 30430, loss = 0.34, batch loss = 0.31 (36.5 examples/sec; 0.219 sec/batch; 18h:23m:19s remains)
INFO - root - 2017-12-15 07:59:37.417951: step 30440, loss = 0.24, batch loss = 0.20 (35.8 examples/sec; 0.223 sec/batch; 18h:43m:26s remains)
INFO - root - 2017-12-15 07:59:39.743630: step 30450, loss = 0.19, batch loss = 0.15 (35.8 examples/sec; 0.223 sec/batch; 18h:44m:52s remains)
INFO - root - 2017-12-15 07:59:42.008483: step 30460, loss = 0.19, batch loss = 0.16 (36.4 examples/sec; 0.220 sec/batch; 18h:26m:03s remains)
INFO - root - 2017-12-15 07:59:44.250237: step 30470, loss = 0.23, batch loss = 0.20 (34.5 examples/sec; 0.232 sec/batch; 19h:27m:42s remains)
INFO - root - 2017-12-15 07:59:46.566030: step 30480, loss = 0.20, batch loss = 0.16 (35.8 examples/sec; 0.224 sec/batch; 18h:45m:58s remains)
INFO - root - 2017-12-15 07:59:48.839306: step 30490, loss = 0.17, batch loss = 0.14 (35.8 examples/sec; 0.223 sec/batch; 18h:44m:43s remains)
INFO - root - 2017-12-15 07:59:51.105315: step 30500, loss = 0.26, batch loss = 0.23 (35.2 examples/sec; 0.227 sec/batch; 19h:04m:11s remains)
2017-12-15 07:59:51.401131: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0245347 -6.5939379 -6.8487387 -7.1715126 -7.4310474 -7.4557304 -7.1334429 -6.5640965 -6.2333322 -5.7328315 -5.1276131 -4.2646527 -3.2130785 -2.4255767 -1.5255712][-6.3948774 -7.5132418 -7.6011248 -7.6081696 -7.719162 -7.4924526 -6.8804426 -6.0363417 -5.6290231 -5.6038 -5.516459 -4.7953467 -3.7838843 -2.9629934 -1.9222003][-6.5758057 -7.2794886 -7.504653 -7.4084063 -7.3919516 -6.8682785 -6.013257 -4.8936362 -4.4590826 -4.986958 -5.5474987 -5.117537 -4.2934413 -3.5058534 -2.5209694][-7.0489922 -7.2360773 -7.5524292 -7.262085 -6.7749891 -5.5342584 -4.2981873 -2.8387129 -2.3946221 -3.5978961 -5.0342846 -5.2053871 -4.8348522 -4.2812309 -3.4717193][-7.4181128 -7.1525497 -7.2704992 -6.6268167 -5.4241629 -3.1137435 -1.1183177 0.86982059 1.3081319 -0.57258534 -3.071774 -4.1043606 -4.4701052 -4.618402 -4.1451712][-7.223958 -6.7595882 -6.7636318 -5.829957 -4.0141411 -0.75663924 2.0480897 4.7276812 5.258831 2.9053161 -0.37494171 -2.0703733 -2.9232988 -3.5973961 -3.1942692][-6.572484 -6.49393 -6.3025351 -4.9898214 -2.5355828 1.392349 4.8774862 8.0417051 8.2700586 5.3783379 1.372946 -0.73996508 -1.6589382 -2.5626495 -1.8488092][-6.2760935 -6.466732 -6.2648754 -4.7695904 -2.0091844 1.9902031 5.4541435 8.5238495 8.2753849 5.0131598 0.77224207 -1.1572856 -1.5686208 -2.1202948 -0.7695241][-5.9067688 -6.3131266 -6.382823 -5.1390438 -2.5551298 0.75380635 3.4057839 5.8113413 5.101819 2.0632107 -1.7247999 -3.0561595 -2.8273196 -2.8595095 -0.84736454][-5.5587864 -5.9438639 -6.2123151 -5.3532629 -3.1452615 -0.60781574 1.009269 2.5244887 1.6225069 -0.96961212 -4.2208209 -5.1494417 -4.6845036 -4.4900494 -2.3140905][-5.1166859 -5.5545568 -6.0917745 -5.6810942 -4.1268234 -2.4382491 -1.4183975 -0.41114044 -1.3784785 -3.6243832 -6.3241234 -7.1257839 -6.6523762 -6.1046686 -3.9071059][-4.683877 -5.0535469 -5.6344948 -5.4844971 -4.3691959 -3.2031889 -2.7020271 -2.2601545 -3.2737477 -5.2218304 -7.4405088 -8.2887115 -8.0373678 -7.4715586 -5.8549609][-4.3838892 -4.5363426 -5.0419421 -5.1050949 -4.4088831 -3.5528641 -3.2028737 -3.0537021 -4.0739574 -5.8432016 -7.7723417 -8.7015171 -8.734417 -8.3479385 -7.3412457][-4.2387056 -4.1382952 -4.5594482 -4.8737984 -4.658082 -4.155962 -4.1424446 -4.320312 -5.2496514 -6.6854343 -8.1634407 -8.8843689 -8.8679876 -8.4925137 -7.8193321][-4.5472522 -4.2396288 -4.4491587 -4.7641954 -4.810853 -4.6237803 -4.8386574 -5.0799427 -5.7956476 -6.915062 -8.0328274 -8.479455 -8.3433628 -7.9299316 -7.3754749]]...]
INFO - root - 2017-12-15 07:59:53.680078: step 30510, loss = 0.26, batch loss = 0.23 (35.5 examples/sec; 0.226 sec/batch; 18h:55m:46s remains)
INFO - root - 2017-12-15 07:59:55.939941: step 30520, loss = 0.23, batch loss = 0.20 (34.5 examples/sec; 0.232 sec/batch; 19h:27m:22s remains)
INFO - root - 2017-12-15 07:59:58.225790: step 30530, loss = 0.19, batch loss = 0.16 (36.0 examples/sec; 0.222 sec/batch; 18h:38m:06s remains)
INFO - root - 2017-12-15 08:00:00.493695: step 30540, loss = 0.20, batch loss = 0.17 (34.5 examples/sec; 0.232 sec/batch; 19h:26m:35s remains)
INFO - root - 2017-12-15 08:00:02.747634: step 30550, loss = 0.19, batch loss = 0.16 (35.7 examples/sec; 0.224 sec/batch; 18h:46m:57s remains)
INFO - root - 2017-12-15 08:00:05.035023: step 30560, loss = 0.23, batch loss = 0.20 (34.2 examples/sec; 0.234 sec/batch; 19h:35m:34s remains)
INFO - root - 2017-12-15 08:00:07.322204: step 30570, loss = 0.17, batch loss = 0.14 (34.5 examples/sec; 0.232 sec/batch; 19h:25m:51s remains)
INFO - root - 2017-12-15 08:00:09.639572: step 30580, loss = 0.31, batch loss = 0.27 (35.9 examples/sec; 0.223 sec/batch; 18h:41m:37s remains)
INFO - root - 2017-12-15 08:00:11.876852: step 30590, loss = 0.20, batch loss = 0.17 (34.9 examples/sec; 0.229 sec/batch; 19h:12m:45s remains)
INFO - root - 2017-12-15 08:00:14.144271: step 30600, loss = 0.23, batch loss = 0.20 (35.4 examples/sec; 0.226 sec/batch; 18h:56m:02s remains)
2017-12-15 08:00:14.413229: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.66390383 -2.5647397 -3.447794 -4.0461407 -4.0555048 -3.8363867 -3.7685206 -3.7746725 -4.1652131 -4.6640406 -4.7564087 -4.5742617 -4.3885202 -4.3512573 -4.3092618][-2.754457 -4.1598554 -5.1788549 -5.7778215 -5.6673021 -5.1602149 -4.8236871 -4.7407551 -5.2580104 -5.9496355 -6.2317514 -6.0652413 -5.715673 -5.4438658 -5.30582][-5.3889751 -5.853724 -6.7332964 -7.0407143 -6.6212444 -5.7207007 -4.9998703 -4.6985273 -5.4028125 -6.5156679 -7.0957527 -7.0396805 -6.58092 -6.1241913 -5.8703117][-7.2486935 -7.1029205 -7.6785593 -7.5931597 -6.8005543 -5.3034544 -3.889277 -3.1612995 -4.1284184 -5.8768587 -6.9600754 -7.184288 -6.77851 -6.2980509 -5.9996295][-7.9251652 -7.3694582 -7.5683651 -7.1184111 -6.0458784 -4.1110053 -1.9964482 -0.74824548 -1.8791198 -4.2777853 -5.9496665 -6.5327911 -6.3050661 -5.9767818 -5.789423][-7.8534241 -6.3950844 -6.0144577 -5.1571493 -3.828728 -1.4043467 1.381413 3.032923 1.7275469 -1.3192722 -3.6810546 -4.8448992 -5.0645089 -4.99871 -4.8778744][-6.7912149 -5.2102885 -4.3382859 -3.1638932 -1.56797 1.1648269 4.3758116 6.4519329 5.2408419 1.7939851 -1.1035256 -2.8213363 -3.4505291 -3.5704134 -3.438307][-6.4772749 -4.6880302 -3.5748343 -2.2653019 -0.52787328 2.3244803 5.7036943 8.1297779 7.34241 4.022357 0.92101836 -1.2816775 -2.4295509 -2.7101691 -2.5803328][-6.7497683 -4.9777746 -3.8125572 -2.4841094 -0.62592018 2.2101815 5.37971 7.9040117 7.4669571 4.4636106 1.4089472 -1.0917778 -2.6740332 -3.0540402 -2.915108][-7.4262524 -5.9199257 -5.0049562 -3.8626542 -2.2622862 0.15924144 2.8662612 5.1127462 4.9879761 2.5850794 -0.020908117 -2.3662772 -3.9425545 -4.223166 -3.9004855][-8.2560329 -7.2533054 -6.6897116 -5.8651009 -4.5132847 -2.5511255 -0.41753769 1.4480574 1.4892585 -0.21727371 -2.3128257 -4.4043064 -5.7832117 -5.9325809 -5.3983088][-8.4127178 -7.9618368 -7.6830091 -7.0722275 -5.9967079 -4.4809704 -2.8525467 -1.6220336 -1.662429 -2.8132896 -4.3924246 -6.3621378 -7.6612654 -7.6834984 -7.1000929][-7.9434223 -7.7992716 -7.8889055 -7.7024279 -7.1409779 -6.1757808 -5.1364107 -4.3605561 -4.3313036 -4.964941 -6.0838552 -7.6643553 -8.595849 -8.3955164 -7.6776714][-7.41057 -7.2825289 -7.4512224 -7.4849796 -7.4533405 -7.1762505 -6.7793303 -6.4343452 -6.1973 -6.2416964 -6.7144938 -7.6936426 -8.1011171 -7.6571035 -6.9286509][-7.2512341 -6.9071493 -6.9207964 -6.8732529 -6.94112 -6.9432583 -6.965024 -6.8536358 -6.5151634 -6.2671261 -6.4213238 -6.9405222 -7.0658627 -6.6853695 -6.17241]]...]
INFO - root - 2017-12-15 08:00:16.653136: step 30610, loss = 0.20, batch loss = 0.17 (35.6 examples/sec; 0.225 sec/batch; 18h:51m:55s remains)
INFO - root - 2017-12-15 08:00:18.904816: step 30620, loss = 0.28, batch loss = 0.25 (35.9 examples/sec; 0.223 sec/batch; 18h:42m:32s remains)
INFO - root - 2017-12-15 08:00:21.142720: step 30630, loss = 0.21, batch loss = 0.17 (36.0 examples/sec; 0.222 sec/batch; 18h:38m:45s remains)
INFO - root - 2017-12-15 08:00:23.416127: step 30640, loss = 0.24, batch loss = 0.21 (34.0 examples/sec; 0.235 sec/batch; 19h:42m:31s remains)
INFO - root - 2017-12-15 08:00:25.667368: step 30650, loss = 0.18, batch loss = 0.15 (34.5 examples/sec; 0.232 sec/batch; 19h:27m:49s remains)
INFO - root - 2017-12-15 08:00:27.945527: step 30660, loss = 0.20, batch loss = 0.17 (34.5 examples/sec; 0.232 sec/batch; 19h:28m:00s remains)
INFO - root - 2017-12-15 08:00:30.210049: step 30670, loss = 0.22, batch loss = 0.19 (34.7 examples/sec; 0.231 sec/batch; 19h:19m:59s remains)
INFO - root - 2017-12-15 08:00:32.471861: step 30680, loss = 0.21, batch loss = 0.17 (35.0 examples/sec; 0.228 sec/batch; 19h:08m:25s remains)
INFO - root - 2017-12-15 08:00:34.741668: step 30690, loss = 0.23, batch loss = 0.19 (35.7 examples/sec; 0.224 sec/batch; 18h:48m:10s remains)
INFO - root - 2017-12-15 08:00:37.065984: step 30700, loss = 0.18, batch loss = 0.15 (34.6 examples/sec; 0.231 sec/batch; 19h:22m:15s remains)
2017-12-15 08:00:37.371299: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1015005 -5.6021428 -5.6269703 -5.6928463 -5.6109715 -5.701848 -6.1048851 -6.650249 -6.7982378 -6.5752234 -6.224638 -5.8672066 -5.7081637 -5.8292093 -6.2067385][-5.51748 -5.82273 -5.8879094 -5.8623714 -5.6129065 -5.5550756 -6.0754943 -7.1273513 -7.67358 -7.4718037 -6.9861689 -6.4870634 -6.2571716 -6.4821019 -7.1642532][-6.9584222 -6.343163 -6.4132371 -6.1677675 -5.5392623 -4.9543543 -5.1157408 -6.2903333 -7.0763087 -6.9273915 -6.4219074 -5.8977528 -5.733098 -6.2211332 -7.3011546][-7.5166149 -6.0699778 -6.0397339 -5.6521921 -4.8871088 -3.9459062 -3.6363258 -4.6936903 -5.591301 -5.5239367 -5.0700884 -4.5533371 -4.4787641 -5.2106085 -6.5706987][-6.9540253 -5.0150604 -4.9121327 -4.5609818 -3.9129562 -2.8053145 -2.0501664 -2.7504573 -3.6191709 -3.715929 -3.430006 -3.0229273 -3.1683671 -4.2622623 -5.8702803][-5.661685 -3.551064 -3.4078088 -3.1123419 -2.5577509 -1.3487542 -0.21467042 -0.50979936 -1.2955037 -1.7407324 -1.7265761 -1.4145011 -1.6268705 -2.8721387 -4.5239515][-4.3179469 -2.3865283 -2.0125997 -1.5578504 -0.83779144 0.49207234 1.9152451 2.027564 1.3952632 0.66362667 0.35944629 0.53975892 0.26572204 -1.0118681 -2.5719376][-4.0717092 -1.7758902 -1.1012397 -0.41226041 0.56892157 1.9540534 3.4954638 3.9000826 3.3905845 2.3127642 1.5289571 1.4026043 0.99427032 -0.31397521 -1.8712175][-4.0086346 -1.7011739 -0.9678899 -0.24968433 0.86571574 2.1832466 3.5036874 3.8781567 3.4745874 2.26574 1.2565718 1.0069718 0.68681121 -0.57516241 -2.1981313][-4.8180189 -2.925462 -2.3377452 -1.5589516 -0.23507071 1.0860341 2.2297382 2.3996415 1.9203806 0.59830189 -0.45053434 -0.61447203 -0.72999191 -1.9014835 -3.6389976][-5.653553 -4.2788038 -3.9761968 -3.3209751 -2.0222895 -0.75353634 0.18576908 0.11087203 -0.50970626 -1.7641724 -2.628202 -2.5550776 -2.4131584 -3.4889126 -5.284658][-5.9463539 -4.9964757 -5.1041975 -4.8577752 -3.8657534 -2.802881 -2.0229578 -2.1314666 -2.6554184 -3.6837223 -4.2664013 -4.0880356 -3.7956007 -4.6474304 -6.2669721][-6.1856594 -5.4878092 -5.9101791 -6.0116091 -5.3741055 -4.5770178 -4.0299811 -4.1492534 -4.5549669 -5.364604 -5.7924166 -5.679491 -5.3678131 -6.0316505 -7.3528585][-6.5261984 -5.7993579 -6.1955261 -6.4396386 -6.1798797 -5.8430138 -5.6080837 -5.6660686 -5.87773 -6.3894205 -6.6494527 -6.5873623 -6.3107343 -6.7439818 -7.6585665][-6.3861628 -5.5464554 -5.7697258 -6.0210381 -6.0402951 -6.0527143 -6.0567217 -6.0519657 -6.0751309 -6.2668705 -6.3266325 -6.321703 -6.2316055 -6.5538759 -7.1400609]]...]
INFO - root - 2017-12-15 08:00:39.658218: step 30710, loss = 0.24, batch loss = 0.20 (34.4 examples/sec; 0.233 sec/batch; 19h:30m:26s remains)
INFO - root - 2017-12-15 08:00:41.957232: step 30720, loss = 0.25, batch loss = 0.22 (34.3 examples/sec; 0.233 sec/batch; 19h:31m:51s remains)
INFO - root - 2017-12-15 08:00:44.207711: step 30730, loss = 0.26, batch loss = 0.23 (35.6 examples/sec; 0.225 sec/batch; 18h:50m:40s remains)
INFO - root - 2017-12-15 08:00:46.468885: step 30740, loss = 0.22, batch loss = 0.19 (36.0 examples/sec; 0.222 sec/batch; 18h:36m:09s remains)
INFO - root - 2017-12-15 08:00:48.714006: step 30750, loss = 0.20, batch loss = 0.17 (36.7 examples/sec; 0.218 sec/batch; 18h:16m:55s remains)
INFO - root - 2017-12-15 08:00:50.963211: step 30760, loss = 0.32, batch loss = 0.29 (36.2 examples/sec; 0.221 sec/batch; 18h:30m:43s remains)
INFO - root - 2017-12-15 08:00:53.222742: step 30770, loss = 0.26, batch loss = 0.23 (35.3 examples/sec; 0.227 sec/batch; 18h:59m:46s remains)
INFO - root - 2017-12-15 08:00:55.529140: step 30780, loss = 0.24, batch loss = 0.20 (35.1 examples/sec; 0.228 sec/batch; 19h:04m:40s remains)
INFO - root - 2017-12-15 08:00:57.797870: step 30790, loss = 0.15, batch loss = 0.12 (35.0 examples/sec; 0.229 sec/batch; 19h:09m:14s remains)
INFO - root - 2017-12-15 08:01:00.073637: step 30800, loss = 0.20, batch loss = 0.17 (33.8 examples/sec; 0.237 sec/batch; 19h:49m:36s remains)
2017-12-15 08:01:00.348950: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0348711 -7.6505833 -7.5884876 -7.2394943 -6.8580704 -6.8239136 -6.9055967 -6.8536243 -6.6954527 -6.7798557 -7.1055164 -7.0037775 -6.4599228 -6.2897081 -6.782917][-7.9062796 -8.7791672 -8.7356167 -8.4019709 -7.9250135 -7.6508446 -7.491127 -7.1882715 -6.9385824 -7.111392 -7.7335119 -7.7850771 -7.0441952 -6.7761631 -7.5194588][-8.790741 -8.9034519 -8.77103 -8.3300018 -7.6088696 -6.9818192 -6.5338211 -5.9076481 -5.5206265 -5.840445 -6.7398624 -7.0280552 -6.3176489 -6.1398659 -7.0374222][-9.10068 -8.6856976 -8.5058565 -7.9010515 -6.7529745 -5.7515717 -5.1206384 -4.1431518 -3.528089 -4.01737 -5.1479139 -5.6119895 -5.1207714 -5.1326342 -6.0005875][-9.2453117 -8.54253 -8.0816689 -7.0215263 -5.2811832 -3.9433122 -3.3434258 -2.2207959 -1.4393241 -2.3051617 -3.8002057 -4.4281206 -4.2797637 -4.50334 -5.2169647][-8.7733278 -7.7440367 -6.8196287 -5.2627091 -3.0526385 -1.2820652 -0.56002545 0.7440598 1.5412307 0.28055286 -1.4912838 -2.2221882 -2.4107003 -2.7833631 -3.3908281][-7.2269382 -6.2527666 -4.86928 -2.9550767 -0.54789841 1.4809167 2.4778454 3.9487126 4.4827127 2.6793225 0.77054214 0.067207813 -0.079614878 -0.12429619 -0.43885922][-6.6948414 -5.5385237 -3.7041302 -1.4771924 0.95377254 3.1577389 4.4519968 5.9465857 6.057478 3.8775136 1.9799197 1.2039485 1.0228231 1.2667961 1.1890867][-6.1691413 -4.9903965 -3.0290422 -0.92158246 1.0909469 2.9483502 4.1557169 5.2927265 4.9643612 2.728946 1.3448083 1.1011996 1.4554837 1.9380982 1.8803642][-6.0373707 -4.9807167 -3.1607318 -1.3836315 0.04236865 1.158205 1.9012897 2.421191 1.6282146 -0.421211 -1.1218971 -0.65846372 0.28859711 0.78898168 0.6211915][-6.7011623 -5.9183707 -4.4485507 -3.0610294 -2.1277719 -1.5560975 -0.88212574 -0.4184823 -1.0635817 -2.7661438 -3.1969287 -2.5304363 -1.3918233 -1.1290574 -1.5294526][-7.1018896 -6.6139088 -5.6325531 -4.5809879 -3.9482884 -3.4073257 -2.3800395 -1.6253247 -2.1657317 -3.6161492 -3.9768991 -3.337513 -2.2453504 -2.1646297 -2.7142172][-7.4924259 -7.4171925 -6.8946819 -6.0942 -5.5460386 -5.0199184 -4.0726161 -3.5909457 -4.27672 -5.4119539 -5.4456067 -4.7071495 -3.7381892 -3.7349217 -4.3697233][-7.8181953 -7.9142532 -7.6501293 -7.0010595 -6.5772276 -6.2597485 -5.7429137 -5.7428646 -6.5312777 -7.2580261 -7.0312662 -6.4260406 -5.8478966 -5.8769817 -6.2719069][-7.1972208 -7.1644545 -7.0043554 -6.5107565 -6.2430158 -6.16718 -6.0939121 -6.4875126 -7.2726145 -7.7897692 -7.6523075 -7.3954439 -7.1251478 -6.9994345 -7.0527172]]...]
INFO - root - 2017-12-15 08:01:02.613740: step 30810, loss = 0.21, batch loss = 0.17 (34.8 examples/sec; 0.230 sec/batch; 19h:14m:36s remains)
INFO - root - 2017-12-15 08:01:04.888817: step 30820, loss = 0.24, batch loss = 0.21 (35.6 examples/sec; 0.225 sec/batch; 18h:50m:43s remains)
INFO - root - 2017-12-15 08:01:07.180143: step 30830, loss = 0.33, batch loss = 0.29 (35.3 examples/sec; 0.227 sec/batch; 19h:00m:20s remains)
INFO - root - 2017-12-15 08:01:09.438644: step 30840, loss = 0.21, batch loss = 0.17 (34.8 examples/sec; 0.230 sec/batch; 19h:16m:46s remains)
INFO - root - 2017-12-15 08:01:11.690912: step 30850, loss = 0.17, batch loss = 0.13 (36.2 examples/sec; 0.221 sec/batch; 18h:30m:11s remains)
INFO - root - 2017-12-15 08:01:13.934500: step 30860, loss = 0.27, batch loss = 0.24 (35.3 examples/sec; 0.226 sec/batch; 18h:57m:45s remains)
INFO - root - 2017-12-15 08:01:16.218735: step 30870, loss = 0.23, batch loss = 0.20 (35.2 examples/sec; 0.227 sec/batch; 19h:02m:14s remains)
INFO - root - 2017-12-15 08:01:18.481363: step 30880, loss = 0.23, batch loss = 0.19 (35.2 examples/sec; 0.227 sec/batch; 19h:01m:01s remains)
INFO - root - 2017-12-15 08:01:20.728643: step 30890, loss = 0.32, batch loss = 0.29 (36.0 examples/sec; 0.222 sec/batch; 18h:35m:58s remains)
INFO - root - 2017-12-15 08:01:23.020554: step 30900, loss = 0.39, batch loss = 0.36 (34.8 examples/sec; 0.230 sec/batch; 19h:16m:58s remains)
2017-12-15 08:01:23.329852: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0595069 -4.6670361 -4.4898977 -4.2344 -3.7900326 -3.5876575 -4.0607719 -4.4305096 -4.47814 -4.3201513 -4.4861574 -3.7140083 -2.3251565 -1.8293626 -1.8001137][-3.0092716 -4.7491789 -4.4794846 -4.021822 -3.4039326 -3.1673574 -3.5688639 -3.7835944 -3.3922091 -3.016118 -3.5706482 -3.3134596 -2.2074349 -1.780905 -1.5085235][-3.506166 -4.6660309 -4.2931552 -3.7957745 -3.1943452 -2.8568041 -2.9370944 -2.929805 -2.4209585 -2.07305 -2.8242984 -3.0725031 -2.2102919 -1.7895477 -1.1614575][-3.6903033 -4.6709423 -4.2121868 -3.5400562 -2.9326506 -2.4489887 -1.9444605 -1.533852 -1.1340479 -1.0774764 -2.1133544 -2.6840434 -2.2491422 -1.9516803 -0.986596][-3.9693768 -4.0357852 -3.3653724 -2.7301984 -2.401947 -1.969483 -1.0654658 -0.30849433 0.17389369 -0.056857824 -1.4934955 -2.4067612 -2.2695775 -2.0519724 -0.82640064][-3.180644 -2.5984397 -1.7158957 -1.2000036 -1.1613686 -0.87542617 0.15173817 1.0203018 1.3740523 0.81295156 -0.91250563 -2.3123891 -2.4904816 -2.2675905 -0.91162181][-1.9486742 -1.1130263 -0.26705492 0.25311494 0.23092771 0.31021595 1.1396384 1.9478796 2.2562659 1.4477854 -0.47683907 -2.1812358 -2.6042545 -2.3662095 -1.0562632][-1.3771226 -0.34805393 0.29758763 0.81084657 0.80174065 0.6564784 1.151927 1.8358338 2.1625645 1.1185436 -0.82975972 -2.448663 -2.8732533 -2.7839093 -1.6412202][-2.0435362 -0.93511641 -0.550717 0.033524275 0.31037903 0.26278186 0.55582929 1.0871019 1.134887 -0.14711261 -1.8201869 -3.1238487 -3.5732477 -3.6315973 -2.7769611][-3.7761734 -2.7804022 -2.629415 -2.0354095 -1.5557721 -1.4530593 -1.1568608 -0.568913 -0.65627182 -2.0452752 -3.4056163 -4.3114619 -4.6770687 -4.776597 -4.2524][-5.9087687 -4.9873409 -5.0226812 -4.5635953 -4.206284 -4.1480422 -3.7743363 -3.0714355 -3.2973714 -4.7275848 -5.8085489 -6.2591934 -6.4953623 -6.7598772 -6.6780424][-7.0712547 -6.2405167 -6.4096775 -6.1392279 -5.9400125 -6.0001278 -5.6812644 -5.0022049 -5.2748203 -6.46751 -7.1190968 -7.1806355 -7.3446226 -7.6439362 -7.733983][-7.1883936 -6.4073458 -6.5967684 -6.51851 -6.50161 -6.701787 -6.6636276 -6.2262545 -6.3557568 -7.089489 -7.4360476 -7.2771368 -7.2899218 -7.5221815 -7.6681385][-6.6692066 -5.7741494 -5.9451389 -6.0099325 -6.0907154 -6.2752466 -6.3462143 -6.1407146 -6.1659155 -6.5217266 -6.7259426 -6.5712671 -6.5006666 -6.698143 -6.9335403][-6.1089506 -5.2121172 -5.3380556 -5.4472837 -5.5172443 -5.5928288 -5.6398106 -5.611846 -5.6325774 -5.7575212 -5.8366852 -5.7877717 -5.793457 -5.9665537 -6.1481934]]...]
INFO - root - 2017-12-15 08:01:25.595269: step 30910, loss = 0.28, batch loss = 0.25 (34.5 examples/sec; 0.232 sec/batch; 19h:25m:29s remains)
INFO - root - 2017-12-15 08:01:27.881815: step 30920, loss = 0.21, batch loss = 0.18 (35.1 examples/sec; 0.228 sec/batch; 19h:06m:10s remains)
INFO - root - 2017-12-15 08:01:30.148616: step 30930, loss = 0.19, batch loss = 0.16 (35.2 examples/sec; 0.228 sec/batch; 19h:03m:47s remains)
INFO - root - 2017-12-15 08:01:32.399804: step 30940, loss = 0.25, batch loss = 0.21 (33.8 examples/sec; 0.236 sec/batch; 19h:48m:19s remains)
INFO - root - 2017-12-15 08:01:34.672626: step 30950, loss = 0.34, batch loss = 0.31 (35.5 examples/sec; 0.225 sec/batch; 18h:52m:30s remains)
INFO - root - 2017-12-15 08:01:36.975472: step 30960, loss = 0.21, batch loss = 0.18 (34.1 examples/sec; 0.235 sec/batch; 19h:39m:50s remains)
INFO - root - 2017-12-15 08:01:39.284274: step 30970, loss = 0.23, batch loss = 0.20 (31.6 examples/sec; 0.253 sec/batch; 21h:11m:59s remains)
INFO - root - 2017-12-15 08:01:41.573710: step 30980, loss = 0.19, batch loss = 0.16 (35.1 examples/sec; 0.228 sec/batch; 19h:04m:03s remains)
INFO - root - 2017-12-15 08:01:43.818611: step 30990, loss = 0.27, batch loss = 0.24 (35.7 examples/sec; 0.224 sec/batch; 18h:46m:13s remains)
INFO - root - 2017-12-15 08:01:46.065758: step 31000, loss = 0.26, batch loss = 0.23 (35.6 examples/sec; 0.225 sec/batch; 18h:50m:08s remains)
2017-12-15 08:01:46.332211: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8591843 -8.4292622 -8.249094 -8.0862856 -7.8043289 -7.6124668 -7.5148773 -7.3624973 -7.3400564 -7.322403 -7.3070793 -7.1212053 -6.7536039 -6.3953314 -6.0874748][-7.1948824 -9.4627714 -9.2206135 -8.8133726 -8.0844212 -7.4510279 -6.998023 -6.6776896 -6.8640671 -7.1533089 -7.460968 -7.6047258 -7.3262615 -6.9925508 -6.6897469][-7.5084305 -9.3132858 -9.06175 -8.410779 -7.3311996 -6.1724815 -5.1622829 -4.567584 -5.0181971 -5.6601934 -6.2966557 -6.85361 -6.7279491 -6.5705805 -6.4524784][-6.8843575 -8.3298349 -8.2443218 -7.3335896 -5.8185472 -3.8583548 -2.0963979 -1.1647806 -1.9824413 -3.093267 -4.2379169 -5.4500351 -5.6190643 -5.7421865 -5.8869576][-5.8989725 -7.0619888 -7.1815519 -5.9807692 -3.8804333 -0.9074415 1.5380149 2.6468112 1.4045773 -0.29856014 -2.0319476 -3.8607926 -4.3118081 -4.6699409 -4.9969044][-4.8650942 -5.9654255 -6.1417522 -4.627759 -1.945292 1.8258827 4.5680752 5.5788231 3.9587567 1.8194025 -0.44279706 -2.7465355 -3.4981217 -4.057652 -4.4771185][-3.5743728 -4.7829285 -5.1367059 -3.5586765 -0.57014453 3.53887 6.1681271 6.9440718 5.2249727 3.0099995 0.28209543 -2.351356 -3.3802202 -4.0576777 -4.5057564][-3.3626609 -4.4485197 -4.8318624 -3.1528244 0.053678036 4.055954 6.1831856 6.5485716 4.8522921 2.5460913 -0.42163527 -2.92118 -3.9906631 -4.6266785 -4.9947462][-4.263423 -5.233707 -5.4868464 -3.6444683 -0.48689783 2.8872344 4.3662939 4.454483 2.8782361 0.59394646 -2.1976283 -4.2061486 -5.0363789 -5.4023571 -5.622117][-5.7064524 -6.3671322 -6.4799366 -4.7810698 -2.1791213 0.23628116 1.114922 1.1129365 -0.15551853 -2.0347338 -4.2193842 -5.5375004 -5.991621 -6.1136179 -6.1794219][-7.2509146 -7.5601587 -7.6627178 -6.3721943 -4.4296236 -2.7943704 -2.2909563 -2.3301148 -3.2978609 -4.7251167 -6.1086569 -6.6905556 -6.7287245 -6.561326 -6.3204308][-8.655961 -8.592392 -8.6755075 -7.7153654 -6.2941189 -5.2589941 -4.9859848 -5.0269775 -5.6878929 -6.5428696 -7.0852785 -7.1169291 -6.8074684 -6.4092093 -6.0117645][-9.0250616 -8.58329 -8.5153522 -7.7830219 -6.8394794 -6.2536721 -6.1771851 -6.2630367 -6.6341944 -6.9661837 -7.0311604 -6.8035936 -6.3708372 -5.9917679 -5.5842991][-8.5142946 -7.8502474 -7.6906023 -7.2059784 -6.678215 -6.4095612 -6.4207535 -6.464921 -6.6122408 -6.6715384 -6.55787 -6.3163643 -5.9454794 -5.6618228 -5.3139091][-7.8092775 -6.9987078 -6.7580795 -6.4613008 -6.204567 -6.0845947 -6.127718 -6.158494 -6.2215767 -6.2139072 -6.1033268 -5.9485674 -5.6504107 -5.4471283 -5.21312]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 08:01:48.593664: step 31010, loss = 0.21, batch loss = 0.18 (35.3 examples/sec; 0.227 sec/batch; 18h:58m:41s remains)
INFO - root - 2017-12-15 08:01:50.876205: step 31020, loss = 0.22, batch loss = 0.19 (36.1 examples/sec; 0.222 sec/batch; 18h:33m:37s remains)
INFO - root - 2017-12-15 08:01:53.133691: step 31030, loss = 0.25, batch loss = 0.21 (34.8 examples/sec; 0.230 sec/batch; 19h:14m:36s remains)
INFO - root - 2017-12-15 08:01:55.440243: step 31040, loss = 0.21, batch loss = 0.18 (34.6 examples/sec; 0.231 sec/batch; 19h:20m:05s remains)
INFO - root - 2017-12-15 08:01:57.728923: step 31050, loss = 0.18, batch loss = 0.15 (34.6 examples/sec; 0.231 sec/batch; 19h:21m:32s remains)
INFO - root - 2017-12-15 08:02:00.019300: step 31060, loss = 0.19, batch loss = 0.16 (34.7 examples/sec; 0.231 sec/batch; 19h:18m:19s remains)
INFO - root - 2017-12-15 08:02:02.320431: step 31070, loss = 0.21, batch loss = 0.18 (35.9 examples/sec; 0.223 sec/batch; 18h:38m:47s remains)
INFO - root - 2017-12-15 08:02:04.576126: step 31080, loss = 0.25, batch loss = 0.22 (33.3 examples/sec; 0.240 sec/batch; 20h:06m:13s remains)
INFO - root - 2017-12-15 08:02:06.852329: step 31090, loss = 0.21, batch loss = 0.18 (35.4 examples/sec; 0.226 sec/batch; 18h:53m:39s remains)
INFO - root - 2017-12-15 08:02:09.102720: step 31100, loss = 0.22, batch loss = 0.19 (34.9 examples/sec; 0.229 sec/batch; 19h:12m:20s remains)
2017-12-15 08:02:09.386944: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2019606 -3.2814717 -3.1838913 -3.0307219 -2.7120907 -2.3601067 -1.9255956 -2.28324 -2.9560759 -4.3904109 -5.5657482 -5.7440438 -5.6440783 -4.9724345 -3.9425743][-6.3870115 -4.4062562 -4.4330444 -4.4973187 -4.484 -4.1611018 -3.5895443 -3.8073792 -4.5420022 -6.0765376 -7.0606461 -7.012763 -6.914423 -6.3390331 -5.401762][-7.9486895 -4.7765088 -4.8770266 -5.1538668 -5.1759791 -4.6809406 -3.863265 -4.0515795 -5.1751261 -7.1578684 -8.1048012 -7.8203821 -7.582305 -7.1286469 -6.2613068][-8.4706173 -4.9009485 -5.0591516 -5.2846713 -4.8832 -3.7555532 -2.3136871 -2.2432721 -3.7695456 -6.372757 -7.7518778 -7.863905 -7.7737131 -7.3939486 -6.6344709][-8.2131586 -4.5999565 -4.569766 -4.3841271 -3.3330498 -1.3667011 0.68344665 0.9948709 -0.94558263 -3.9581456 -6.0114179 -7.0114412 -7.5452223 -7.3594894 -6.61376][-8.0248852 -3.7952349 -3.3497839 -2.4031868 -0.50419867 2.1343019 4.679306 4.9765453 2.6077774 -0.78568006 -3.5959048 -5.5498314 -6.8089142 -6.9019213 -6.1448569][-7.1388321 -3.6990762 -2.8829918 -1.5098388 0.82464743 3.6985857 6.5962954 7.0326967 4.8061628 1.3762987 -2.0247355 -4.5686674 -6.2471008 -6.5967112 -6.0247755][-7.3776331 -3.7582903 -2.7950957 -1.4096482 0.56682634 2.9883711 5.7459192 6.2919846 4.5766172 1.4997392 -2.0216835 -4.5754957 -6.1668987 -6.3264523 -5.5400162][-6.9880295 -3.1564107 -1.8821877 -0.81663203 0.40508223 1.7870753 3.8623126 4.1696653 2.6826756 -0.036715984 -3.2808237 -5.3891106 -6.5270548 -6.4874725 -5.6775947][-7.545578 -3.6157293 -2.1160886 -1.253071 -0.47460663 0.056594849 1.3373759 1.250474 -0.2357018 -2.6605463 -5.3047876 -6.7460575 -7.2353735 -7.0022507 -6.1950607][-8.0306835 -4.3157463 -3.14276 -2.7753687 -2.3570285 -2.2572203 -1.7725877 -2.3760011 -4.1086335 -6.071104 -7.6686964 -8.1612139 -7.923563 -7.1474018 -6.2050438][-8.2149763 -4.5994291 -4.0562553 -4.1678786 -3.9997375 -4.4647331 -4.676218 -5.462718 -7.0357475 -8.2489986 -8.7091579 -8.4140205 -7.7958555 -6.9142733 -6.1825895][-8.6585007 -5.1771421 -4.9855008 -5.1534648 -5.0663328 -5.7225008 -6.3143606 -6.9411526 -8.0606976 -8.6770506 -8.5492144 -7.8723965 -7.27466 -6.5790386 -6.0503893][-9.048871 -5.931571 -6.058177 -5.9936142 -5.5371881 -5.9765854 -6.648572 -7.1418228 -7.9381981 -8.2069511 -7.8020725 -6.9483223 -6.4161692 -5.9982181 -5.7439327][-9.209898 -6.5494146 -7.1239815 -7.0875311 -6.5900259 -6.647809 -6.9377823 -7.0797119 -7.6504669 -7.7439356 -7.33574 -6.4582138 -5.8536654 -5.5658274 -5.42824]]...]
INFO - root - 2017-12-15 08:02:11.673663: step 31110, loss = 0.32, batch loss = 0.29 (35.7 examples/sec; 0.224 sec/batch; 18h:46m:42s remains)
INFO - root - 2017-12-15 08:02:13.936090: step 31120, loss = 0.29, batch loss = 0.26 (35.6 examples/sec; 0.225 sec/batch; 18h:49m:42s remains)
INFO - root - 2017-12-15 08:02:16.192644: step 31130, loss = 0.23, batch loss = 0.20 (36.8 examples/sec; 0.217 sec/batch; 18h:11m:48s remains)
INFO - root - 2017-12-15 08:02:18.446055: step 31140, loss = 0.22, batch loss = 0.18 (36.1 examples/sec; 0.222 sec/batch; 18h:34m:12s remains)
INFO - root - 2017-12-15 08:02:20.713288: step 31150, loss = 0.20, batch loss = 0.17 (34.1 examples/sec; 0.234 sec/batch; 19h:37m:39s remains)
INFO - root - 2017-12-15 08:02:22.990854: step 31160, loss = 0.32, batch loss = 0.28 (34.2 examples/sec; 0.234 sec/batch; 19h:34m:41s remains)
INFO - root - 2017-12-15 08:02:25.244205: step 31170, loss = 0.19, batch loss = 0.16 (35.8 examples/sec; 0.223 sec/batch; 18h:42m:05s remains)
INFO - root - 2017-12-15 08:02:27.475052: step 31180, loss = 0.25, batch loss = 0.21 (35.5 examples/sec; 0.226 sec/batch; 18h:53m:06s remains)
INFO - root - 2017-12-15 08:02:29.766117: step 31190, loss = 0.23, batch loss = 0.19 (33.4 examples/sec; 0.239 sec/batch; 20h:01m:07s remains)
INFO - root - 2017-12-15 08:02:32.043081: step 31200, loss = 0.19, batch loss = 0.16 (35.0 examples/sec; 0.229 sec/batch; 19h:09m:22s remains)
2017-12-15 08:02:32.327131: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1088028 -6.7244048 -7.0615034 -6.9943252 -6.691144 -6.2219625 -6.0260181 -6.0698271 -5.7466135 -5.9500775 -6.4048581 -5.687706 -4.9238505 -4.91758 -4.9312277][-5.7260437 -7.55751 -7.9016304 -7.9095678 -7.5758686 -7.0348816 -6.9178395 -7.0311408 -6.8166909 -7.0430074 -7.5634003 -6.5563641 -5.3756733 -5.1930246 -4.9899569][-6.7800994 -7.7466841 -8.1941528 -8.2496338 -7.8085032 -7.0026236 -6.7575431 -6.9239039 -7.0447974 -7.6905465 -8.4371281 -7.2296333 -5.5165591 -4.8750677 -4.2686782][-7.2419157 -7.3968382 -7.9984322 -7.9812932 -7.1666188 -5.9086609 -5.52401 -5.9158316 -6.6416287 -7.96848 -9.0629654 -7.8714576 -5.8130856 -4.5928464 -3.488214][-7.4683166 -6.8736014 -7.3522072 -6.7607155 -5.0027752 -2.85265 -2.312382 -3.1731844 -4.8469062 -7.0897222 -8.616313 -7.7987332 -5.9642096 -4.5024529 -3.095192][-7.1178942 -6.6439567 -6.7605124 -5.3383865 -2.4636202 0.82958341 1.7978306 0.65447021 -1.856335 -5.0290875 -7.1199059 -6.8570094 -5.5411363 -4.0705462 -2.5544853][-6.71315 -6.6402698 -6.4233193 -4.2260084 -0.34625232 3.9165506 5.2823124 4.0604091 1.1279407 -2.7577744 -5.54424 -5.9901228 -5.2835908 -3.8986156 -2.2290432][-6.519917 -6.4668393 -6.1143475 -3.5838971 0.75763845 5.5143156 7.0896888 6.01697 2.9466257 -1.2739916 -4.5372038 -5.6640863 -5.3920679 -4.2612009 -2.6201837][-5.7082167 -6.002986 -5.8580008 -3.7740941 0.031644344 4.4541407 6.0078712 5.119627 2.2534285 -1.6902044 -4.6821394 -5.9320664 -5.7145143 -4.7926788 -3.29911][-4.932158 -5.5128 -5.8860884 -4.7417459 -1.9384413 1.6367893 3.0465989 2.3683443 -0.21493411 -3.5609899 -5.8258791 -6.6017857 -6.1499214 -5.3927641 -4.1925197][-4.3559403 -5.022985 -5.886353 -5.6893282 -3.9868405 -1.3339512 -0.10772991 -0.5492847 -2.7314613 -5.3744698 -6.8026 -7.1275945 -6.5014167 -5.8839664 -5.147584][-4.8841114 -5.4725571 -6.5416403 -6.8140812 -5.978879 -4.3120079 -3.297544 -3.5175178 -5.1183052 -6.9603705 -7.6199751 -7.566236 -6.8618159 -6.4046793 -6.1916924][-5.8080759 -6.1596527 -7.2459283 -7.7470503 -7.5293741 -6.6412191 -5.75191 -5.6407375 -6.5549011 -7.6020155 -7.7366061 -7.6106186 -7.0702462 -6.7737293 -6.9449458][-6.5083547 -6.527976 -7.4012003 -7.948319 -8.0338793 -7.5750532 -6.8603263 -6.6017437 -7.0604868 -7.6096725 -7.5763192 -7.5235057 -7.2198954 -7.032618 -7.4469624][-7.429956 -7.0684681 -7.502594 -7.8267751 -8.0387535 -7.8362689 -7.3329582 -7.1354079 -7.3934083 -7.7685428 -7.7614307 -7.7703114 -7.5492358 -7.4085007 -7.7754469]]...]
INFO - root - 2017-12-15 08:02:34.626788: step 31210, loss = 0.19, batch loss = 0.16 (35.9 examples/sec; 0.223 sec/batch; 18h:37m:29s remains)
INFO - root - 2017-12-15 08:02:36.933641: step 31220, loss = 0.22, batch loss = 0.18 (35.4 examples/sec; 0.226 sec/batch; 18h:55m:19s remains)
INFO - root - 2017-12-15 08:02:39.235309: step 31230, loss = 0.26, batch loss = 0.23 (36.2 examples/sec; 0.221 sec/batch; 18h:29m:57s remains)
INFO - root - 2017-12-15 08:02:41.523621: step 31240, loss = 0.16, batch loss = 0.13 (35.5 examples/sec; 0.225 sec/batch; 18h:52m:01s remains)
INFO - root - 2017-12-15 08:02:43.756036: step 31250, loss = 0.24, batch loss = 0.21 (36.8 examples/sec; 0.217 sec/batch; 18h:11m:33s remains)
INFO - root - 2017-12-15 08:02:46.031506: step 31260, loss = 0.26, batch loss = 0.22 (35.1 examples/sec; 0.228 sec/batch; 19h:04m:47s remains)
INFO - root - 2017-12-15 08:02:48.308409: step 31270, loss = 0.18, batch loss = 0.15 (34.1 examples/sec; 0.234 sec/batch; 19h:36m:38s remains)
INFO - root - 2017-12-15 08:02:50.598093: step 31280, loss = 0.25, batch loss = 0.22 (35.7 examples/sec; 0.224 sec/batch; 18h:43m:52s remains)
INFO - root - 2017-12-15 08:02:52.862440: step 31290, loss = 0.22, batch loss = 0.19 (36.0 examples/sec; 0.222 sec/batch; 18h:36m:02s remains)
INFO - root - 2017-12-15 08:02:55.165039: step 31300, loss = 0.29, batch loss = 0.26 (36.0 examples/sec; 0.222 sec/batch; 18h:35m:32s remains)
2017-12-15 08:02:55.436012: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.90168214 -1.3788793 -1.5089054 -1.4403508 -1.2626534 -1.0727485 -1.2611263 -1.874334 -2.6152143 -2.6495495 -2.1447151 -1.9626372 -1.9474417 -1.4436876 -1.0298917][-0.35201931 -0.3790133 -0.75249922 -1.032041 -1.1821306 -1.0825577 -1.2657793 -1.9523667 -2.7345273 -2.7703137 -2.2877362 -2.0951757 -2.1022274 -1.6931422 -1.506191][-0.56689632 0.042797804 -0.59469926 -1.1555324 -1.4477117 -1.2793107 -1.3027008 -1.8508444 -2.5714304 -2.7192037 -2.4709761 -2.5157268 -2.6635778 -2.5045254 -2.5486705][-1.1035682 -0.28449416 -1.1342865 -1.8877335 -2.1364672 -1.7562861 -1.5267382 -1.7676136 -2.3250997 -2.6482303 -2.8879118 -3.4859591 -4.00178 -4.0495138 -4.0495758][-1.7910072 -0.94334912 -1.9026551 -2.6997402 -2.623955 -1.7545265 -1.0823212 -0.89570785 -1.2621548 -1.8650174 -2.6752641 -3.895309 -4.8147755 -5.1585312 -5.1104465][-2.2962763 -1.7158848 -2.5320325 -3.0149317 -2.4189517 -1.0371914 0.042902231 0.65984082 0.53708482 -0.16375208 -1.2105896 -2.7775853 -4.1125073 -4.6717076 -4.57477][-2.4911325 -2.1915829 -2.7485631 -2.8895361 -1.9240389 -0.19713426 1.236809 2.1456041 2.1410427 1.3214619 -0.0030469894 -1.9573027 -3.4288297 -3.8123307 -3.4587755][-3.586772 -3.2860222 -3.5376804 -3.1916876 -1.897902 0.093590736 1.8367381 2.9449205 2.9575238 1.9226909 0.20929503 -2.0275056 -3.4853749 -3.6523149 -3.1051986][-4.8007336 -4.2430906 -4.1487226 -3.4971952 -2.2214773 -0.27541149 1.5321376 2.667273 2.7776666 1.8632908 0.14275432 -2.0795634 -3.3261445 -3.2983418 -2.7623582][-5.3658891 -4.6901493 -4.4870968 -3.90763 -2.9276276 -1.3105212 0.23668265 1.201561 1.5540721 0.99542308 -0.42835772 -2.2152987 -2.8330426 -2.4661093 -2.102864][-5.6521215 -4.7918835 -4.5151539 -4.0786505 -3.2922239 -2.0468266 -1.0714958 -0.43596065 0.033963203 -0.3437022 -1.4725208 -2.6000617 -2.4783938 -1.7816529 -1.477428][-5.5294619 -4.3989949 -4.096941 -3.8991103 -3.260056 -2.4513435 -2.0901716 -1.7397635 -1.2597318 -1.5934266 -2.4893246 -3.0195084 -2.3136 -1.3441525 -1.082368][-5.0735722 -3.7905812 -3.6319034 -3.6280456 -3.0878882 -2.7044508 -2.8886907 -2.7161553 -2.2890182 -2.6392279 -3.2211027 -3.2560549 -2.2367442 -1.189247 -0.94268239][-4.77109 -3.4980469 -3.4800448 -3.5648541 -3.1556115 -3.1553211 -3.6029856 -3.4097598 -2.9676085 -3.1829991 -3.4422405 -3.1230004 -2.1278739 -1.2109843 -1.0203576][-4.8318052 -3.5386271 -3.5905423 -3.6632891 -3.4014719 -3.5832434 -3.9361181 -3.6379938 -3.3070388 -3.4788156 -3.5764313 -3.1279564 -2.3339207 -1.7918327 -1.8311186]]...]
INFO - root - 2017-12-15 08:02:57.698275: step 31310, loss = 0.24, batch loss = 0.21 (36.4 examples/sec; 0.220 sec/batch; 18h:21m:53s remains)
INFO - root - 2017-12-15 08:02:59.934458: step 31320, loss = 0.30, batch loss = 0.26 (36.2 examples/sec; 0.221 sec/batch; 18h:29m:20s remains)
INFO - root - 2017-12-15 08:03:02.209666: step 31330, loss = 0.18, batch loss = 0.15 (34.4 examples/sec; 0.232 sec/batch; 19h:26m:39s remains)
INFO - root - 2017-12-15 08:03:04.518787: step 31340, loss = 0.32, batch loss = 0.28 (35.6 examples/sec; 0.225 sec/batch; 18h:48m:49s remains)
INFO - root - 2017-12-15 08:03:06.800631: step 31350, loss = 0.21, batch loss = 0.17 (36.1 examples/sec; 0.222 sec/batch; 18h:33m:12s remains)
INFO - root - 2017-12-15 08:03:09.118952: step 31360, loss = 0.22, batch loss = 0.18 (36.0 examples/sec; 0.222 sec/batch; 18h:34m:55s remains)
INFO - root - 2017-12-15 08:03:11.390425: step 31370, loss = 0.18, batch loss = 0.15 (35.8 examples/sec; 0.223 sec/batch; 18h:41m:28s remains)
INFO - root - 2017-12-15 08:03:13.674374: step 31380, loss = 0.34, batch loss = 0.30 (34.9 examples/sec; 0.229 sec/batch; 19h:11m:00s remains)
INFO - root - 2017-12-15 08:03:15.921834: step 31390, loss = 0.18, batch loss = 0.15 (36.0 examples/sec; 0.222 sec/batch; 18h:35m:23s remains)
INFO - root - 2017-12-15 08:03:18.221355: step 31400, loss = 0.21, batch loss = 0.17 (34.7 examples/sec; 0.231 sec/batch; 19h:17m:14s remains)
2017-12-15 08:03:18.510452: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1438359 -4.4063568 -5.4325047 -6.1541948 -6.9845428 -6.9291658 -6.4700665 -6.3220248 -6.5765333 -6.9727859 -7.8370781 -8.7071762 -8.7125282 -8.1790447 -7.46983][-2.7746735 -5.0457535 -6.0888739 -6.5077605 -7.0888085 -6.9365869 -6.4554467 -6.20751 -6.1714025 -6.2891245 -7.3321228 -8.3538437 -8.3389282 -7.8546796 -7.0558381][-4.1510496 -5.6102438 -6.6439209 -6.7937441 -7.078465 -6.7722449 -6.2096376 -5.8531504 -5.5044842 -5.2601051 -6.2838411 -7.2715821 -7.1933947 -6.8273153 -6.0468664][-5.1831617 -5.9334764 -6.981709 -6.877964 -6.8792453 -6.4843917 -5.9101372 -5.5007677 -4.9217157 -4.3667407 -5.1676755 -5.9338865 -5.7061539 -5.5311246 -4.8473434][-5.8496008 -5.9529238 -6.95324 -6.4619412 -6.0078311 -5.4386959 -4.8510337 -4.5409169 -4.1191645 -3.8106694 -4.662632 -5.2303419 -4.802712 -4.6721258 -3.8396292][-5.7455897 -5.5378618 -6.3516474 -5.360405 -4.3836336 -3.6410389 -2.9876008 -2.7782419 -2.7497056 -3.0828462 -4.2779074 -4.8032079 -4.157248 -3.7251987 -2.5065465][-5.0318589 -5.070118 -5.623436 -4.150888 -2.6362762 -1.6223445 -0.893965 -0.80193341 -1.2552301 -2.33285 -4.0810375 -4.7844124 -4.0839691 -3.2382131 -1.7450004][-4.875421 -4.7242002 -5.0700512 -3.30895 -1.2998383 0.17069316 1.0723076 1.0744624 0.38815951 -1.2913792 -3.6893079 -4.593833 -4.0235491 -3.1130352 -1.6569011][-4.6650982 -4.4044485 -4.6302862 -2.8684075 -0.64115942 1.2168877 2.3335667 2.2526617 1.4444292 -0.38503981 -2.9872572 -3.9960647 -3.6280758 -2.7748094 -1.5209012][-4.6483755 -4.4843616 -4.9042807 -3.5547967 -1.5665735 0.44147491 1.6885676 1.6361623 0.94558 -0.52621925 -2.9884984 -3.819159 -3.6356158 -2.8544211 -1.7496974][-4.7249403 -4.826561 -5.6060839 -4.8490362 -3.3498096 -1.4622014 -0.14191031 0.031881809 -0.17574 -1.0867617 -3.355257 -3.9454474 -3.7552204 -2.9626031 -1.8578045][-4.6780853 -5.0308642 -6.147964 -5.9761496 -5.1355743 -3.6185193 -2.4051728 -1.8950084 -1.4131515 -1.6688349 -3.6201792 -4.0260148 -3.7740405 -2.9613008 -1.7524536][-4.652236 -5.2200732 -6.6605363 -7.033215 -6.8492651 -5.7743006 -4.6327457 -3.7063527 -2.748435 -2.5323496 -4.0133414 -4.1233449 -3.6279922 -2.7451646 -1.714494][-4.6294675 -5.187201 -6.6928492 -7.4207382 -7.7671928 -7.1110826 -6.0584774 -4.8867722 -3.8069353 -3.5977087 -4.8009739 -4.5250978 -3.7358806 -2.9073098 -2.262289][-4.5065756 -4.8270407 -6.1533628 -7.0320387 -7.665019 -7.2863083 -6.4542384 -5.4638233 -4.7876415 -4.9260817 -5.8839107 -5.5472927 -4.7897649 -4.1016049 -3.648571]]...]
INFO - root - 2017-12-15 08:03:20.796175: step 31410, loss = 0.27, batch loss = 0.23 (35.0 examples/sec; 0.228 sec/batch; 19h:05m:25s remains)
INFO - root - 2017-12-15 08:03:23.034159: step 31420, loss = 0.31, batch loss = 0.28 (35.1 examples/sec; 0.228 sec/batch; 19h:05m:14s remains)
INFO - root - 2017-12-15 08:03:25.308137: step 31430, loss = 0.25, batch loss = 0.22 (34.9 examples/sec; 0.229 sec/batch; 19h:09m:47s remains)
INFO - root - 2017-12-15 08:03:27.587226: step 31440, loss = 0.20, batch loss = 0.17 (35.0 examples/sec; 0.229 sec/batch; 19h:07m:09s remains)
INFO - root - 2017-12-15 08:03:29.878562: step 31450, loss = 0.22, batch loss = 0.19 (35.3 examples/sec; 0.226 sec/batch; 18h:56m:05s remains)
INFO - root - 2017-12-15 08:03:32.139158: step 31460, loss = 0.29, batch loss = 0.26 (34.5 examples/sec; 0.232 sec/batch; 19h:21m:48s remains)
INFO - root - 2017-12-15 08:03:34.373028: step 31470, loss = 0.23, batch loss = 0.20 (36.3 examples/sec; 0.220 sec/batch; 18h:24m:16s remains)
INFO - root - 2017-12-15 08:03:36.637685: step 31480, loss = 0.18, batch loss = 0.15 (34.7 examples/sec; 0.230 sec/batch; 19h:16m:20s remains)
INFO - root - 2017-12-15 08:03:38.904052: step 31490, loss = 0.22, batch loss = 0.19 (34.4 examples/sec; 0.232 sec/batch; 19h:25m:51s remains)
INFO - root - 2017-12-15 08:03:41.197875: step 31500, loss = 0.22, batch loss = 0.19 (35.8 examples/sec; 0.223 sec/batch; 18h:39m:42s remains)
2017-12-15 08:03:41.498740: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5937381 -2.9659925 -3.0783496 -2.8059886 -3.3726487 -3.3532009 -2.942246 -2.6238823 -2.3859084 -2.4020679 -2.4282365 -1.9698563 -2.0364325 -2.8354635 -3.7238481][-3.2656813 -2.678411 -2.9651251 -2.6824379 -3.140595 -2.9558148 -2.5328925 -2.3547978 -2.2265518 -2.4146414 -2.595232 -2.3268805 -2.5748329 -3.2817504 -3.9918542][-3.1240911 -1.8611782 -2.3048067 -2.0356679 -2.3805792 -2.1523287 -1.8984542 -1.9670985 -2.0488567 -2.4526222 -2.7457225 -2.63947 -2.96889 -3.3471892 -3.6559644][-3.9849586 -2.1433313 -2.546289 -2.1514673 -2.1173282 -1.6600869 -1.4264121 -1.596131 -1.7607846 -2.2930553 -2.6492617 -2.6964321 -3.1439233 -3.2865965 -3.2960095][-4.3648777 -2.3040304 -2.7299392 -2.2059338 -1.8637946 -1.0736256 -0.70243025 -0.87360442 -1.0766724 -1.6956329 -2.1671014 -2.4521184 -3.101186 -3.1419938 -2.9408803][-3.9178658 -2.0534773 -2.6896198 -2.168798 -1.5945833 -0.50690603 0.19004416 0.32014155 0.19769359 -0.58608782 -1.2853218 -1.7773935 -2.5224719 -2.4856517 -2.1210728][-2.4332654 -1.0537113 -1.5380682 -0.98797536 -0.34539831 0.78428364 1.670213 2.1119804 2.101397 1.1674101 0.22253418 -0.40582633 -1.1087962 -1.132699 -0.94099772][-1.4871554 0.085208416 -0.17407084 0.27288437 0.71568012 1.6984069 2.5624604 3.1211419 3.1666832 2.1874008 1.1888385 0.74424911 0.33363867 0.20578718 0.016845703][-2.0367599 -0.23187232 -0.33051419 0.0031034946 0.28419042 1.1140668 1.8568974 2.4089694 2.4754848 1.6035981 0.74369669 0.62964106 0.53377342 0.22519779 -0.26894677][-3.350481 -1.6380746 -1.6736656 -1.5025789 -1.2922711 -0.53618 0.12189817 0.69125772 0.78286815 0.04372406 -0.654073 -0.39034915 -0.12530518 -0.5373019 -1.1698426][-4.4277515 -2.9645505 -3.0315363 -3.0495102 -2.964201 -2.2861438 -1.673566 -1.1753355 -1.103866 -1.6787748 -2.2620428 -1.8150092 -1.347962 -1.8513812 -2.5869694][-4.8514862 -3.5898719 -3.6691704 -3.7670059 -3.720315 -3.2167251 -2.7134542 -2.3387163 -2.3251095 -2.7634749 -3.277796 -2.8004398 -2.3631418 -3.0832624 -3.9858031][-5.0622625 -4.0389872 -4.0888095 -4.1462684 -4.0245986 -3.6568866 -3.2784762 -3.0546935 -3.0885675 -3.4441149 -3.9520378 -3.510179 -3.1526058 -4.0015736 -4.8734722][-5.2353854 -4.3082581 -4.2197762 -4.2079172 -4.1086297 -3.9261479 -3.6503696 -3.495079 -3.5551102 -3.8943934 -4.4388475 -4.0323563 -3.678566 -4.4842863 -5.1659079][-5.5442047 -4.6104555 -4.4278116 -4.3970947 -4.3806577 -4.4870191 -4.3907137 -4.2433958 -4.1901751 -4.3759069 -4.7623491 -4.1760921 -3.5982051 -4.1155815 -4.5443912]]...]
INFO - root - 2017-12-15 08:03:43.774242: step 31510, loss = 0.19, batch loss = 0.16 (35.4 examples/sec; 0.226 sec/batch; 18h:54m:44s remains)
INFO - root - 2017-12-15 08:03:46.063499: step 31520, loss = 0.19, batch loss = 0.16 (31.5 examples/sec; 0.254 sec/batch; 21h:13m:08s remains)
INFO - root - 2017-12-15 08:03:48.357524: step 31530, loss = 0.18, batch loss = 0.14 (34.3 examples/sec; 0.233 sec/batch; 19h:30m:20s remains)
INFO - root - 2017-12-15 08:03:50.658126: step 31540, loss = 0.23, batch loss = 0.19 (36.3 examples/sec; 0.220 sec/batch; 18h:24m:13s remains)
INFO - root - 2017-12-15 08:03:52.925961: step 31550, loss = 0.27, batch loss = 0.24 (34.8 examples/sec; 0.230 sec/batch; 19h:11m:53s remains)
INFO - root - 2017-12-15 08:03:55.196756: step 31560, loss = 0.24, batch loss = 0.21 (34.1 examples/sec; 0.234 sec/batch; 19h:35m:04s remains)
INFO - root - 2017-12-15 08:03:57.450539: step 31570, loss = 0.45, batch loss = 0.41 (35.9 examples/sec; 0.223 sec/batch; 18h:38m:41s remains)
INFO - root - 2017-12-15 08:03:59.696400: step 31580, loss = 0.22, batch loss = 0.18 (35.9 examples/sec; 0.223 sec/batch; 18h:39m:03s remains)
INFO - root - 2017-12-15 08:04:01.967650: step 31590, loss = 0.26, batch loss = 0.23 (34.8 examples/sec; 0.230 sec/batch; 19h:14m:23s remains)
INFO - root - 2017-12-15 08:04:04.259761: step 31600, loss = 0.18, batch loss = 0.15 (36.6 examples/sec; 0.218 sec/batch; 18h:15m:27s remains)
2017-12-15 08:04:04.531205: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4508753 -6.8177538 -7.0334992 -7.1392527 -7.041275 -6.88544 -6.981524 -6.9889793 -6.8703651 -6.8140287 -6.6708455 -6.2751265 -5.8733578 -5.8785553 -6.0974836][-5.5443058 -7.3875303 -7.8415461 -8.2715588 -8.358737 -8.1689577 -8.0426712 -7.6112766 -7.033967 -6.5596104 -6.2563176 -5.7375526 -5.39134 -5.49897 -5.6209769][-6.13599 -7.2375088 -7.8339782 -8.185915 -8.1220894 -7.7044792 -7.0801497 -6.1405115 -5.2594519 -4.6969128 -4.7063365 -4.6449356 -4.6853571 -5.0667887 -5.0532][-5.9680986 -6.2807074 -6.7802691 -6.6929412 -6.1499043 -5.2057104 -3.9380312 -2.5679898 -1.7074175 -1.4374496 -2.0659859 -2.7557874 -3.2567854 -4.0873652 -4.2234697][-5.56841 -5.1066418 -5.2283378 -4.3815241 -3.1989398 -1.5327721 0.43983626 1.9978156 2.2978377 1.8139019 0.34139204 -0.97933877 -1.6243309 -2.7812963 -3.1724582][-4.9735327 -3.81842 -3.6069925 -2.2102647 -0.49557567 1.7807798 4.3026562 5.7911458 5.3003359 3.8758068 1.5859828 -0.19584918 -0.841565 -2.0740781 -2.6761975][-3.8653195 -2.6503844 -2.3610821 -0.82030118 1.1090951 3.7735372 6.6784406 8.0287819 6.9375596 4.7464437 1.8263783 -0.30721033 -1.0941647 -2.3800619 -3.1257589][-3.6130676 -2.3601439 -2.2403851 -0.77388322 1.2757885 4.0117159 6.9323277 8.1060886 6.7878642 4.1930285 0.87048483 -1.4144369 -2.4006629 -3.6513855 -4.4140348][-3.841342 -2.84937 -2.988951 -1.8043419 0.031030655 2.383832 4.9756145 5.8758721 4.50864 1.85254 -1.490059 -3.659621 -4.6359234 -5.5804586 -6.0057034][-4.3402314 -3.6423547 -4.1106405 -3.4337726 -2.0962882 -0.34379435 1.6852946 2.4070573 1.3300989 -1.0572993 -4.0224032 -5.8894005 -6.7348795 -7.2382107 -7.1031032][-4.9203806 -4.3713865 -5.0880251 -5.0021515 -4.3875604 -3.4312553 -2.0535514 -1.5024282 -2.2759342 -4.1160865 -6.4315729 -7.7306428 -8.2045708 -8.155777 -7.4215269][-5.0718427 -4.6278172 -5.4483352 -5.8130479 -5.968492 -5.8026485 -5.0983858 -4.8368082 -5.4413362 -6.7080917 -8.12561 -8.7001476 -8.6023464 -7.9123559 -6.6346169][-4.8169074 -4.4058352 -5.2367163 -5.8858314 -6.4774485 -6.7555523 -6.5002317 -6.3691492 -6.6871119 -7.3032355 -7.8739829 -7.7418303 -7.0878167 -5.8789806 -4.3690653][-4.735323 -4.3393908 -5.0891786 -5.749125 -6.4697037 -6.9151039 -6.8585181 -6.7438765 -6.819942 -6.8758678 -6.7448139 -6.1285625 -5.0078306 -3.4652309 -1.8839217][-5.1771469 -4.7553363 -5.1637597 -5.53516 -6.0701237 -6.4854264 -6.5617809 -6.5210552 -6.5829234 -6.399066 -5.8751125 -4.924902 -3.398128 -1.5727029 -0.056517363]]...]
INFO - root - 2017-12-15 08:04:06.794328: step 31610, loss = 0.24, batch loss = 0.21 (36.1 examples/sec; 0.221 sec/batch; 18h:30m:34s remains)
INFO - root - 2017-12-15 08:04:09.098471: step 31620, loss = 0.17, batch loss = 0.14 (35.5 examples/sec; 0.225 sec/batch; 18h:48m:56s remains)
INFO - root - 2017-12-15 08:04:11.370676: step 31630, loss = 0.30, batch loss = 0.27 (32.9 examples/sec; 0.243 sec/batch; 20h:19m:51s remains)
INFO - root - 2017-12-15 08:04:13.654699: step 31640, loss = 0.18, batch loss = 0.15 (35.1 examples/sec; 0.228 sec/batch; 19h:02m:44s remains)
INFO - root - 2017-12-15 08:04:15.938671: step 31650, loss = 0.18, batch loss = 0.15 (35.3 examples/sec; 0.227 sec/batch; 18h:56m:35s remains)
INFO - root - 2017-12-15 08:04:18.202259: step 31660, loss = 0.18, batch loss = 0.15 (35.3 examples/sec; 0.226 sec/batch; 18h:55m:12s remains)
INFO - root - 2017-12-15 08:04:20.446951: step 31670, loss = 0.23, batch loss = 0.20 (35.6 examples/sec; 0.225 sec/batch; 18h:46m:00s remains)
INFO - root - 2017-12-15 08:04:22.710916: step 31680, loss = 0.23, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 18h:33m:14s remains)
INFO - root - 2017-12-15 08:04:25.036861: step 31690, loss = 0.45, batch loss = 0.42 (34.6 examples/sec; 0.231 sec/batch; 19h:20m:04s remains)
INFO - root - 2017-12-15 08:04:27.292203: step 31700, loss = 0.29, batch loss = 0.26 (35.7 examples/sec; 0.224 sec/batch; 18h:43m:36s remains)
2017-12-15 08:04:27.585077: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2921128 -3.8620534 -3.3857784 -3.8376932 -3.1188786 -1.5101159 -0.51903784 -0.62293112 -0.94460607 -1.6329288 -2.5336947 -2.8682384 -3.2717845 -3.9686351 -4.0966272][-3.6611052 -4.1182566 -3.6778746 -3.5909176 -2.6597812 -0.78226197 0.58633733 0.59667611 0.0038211346 -1.6189656 -3.3426652 -4.0854979 -4.67412 -5.091053 -4.9242363][-4.7051306 -4.1160793 -3.7168193 -3.1733546 -1.9485922 0.18009925 1.9106092 2.1491284 1.2836215 -1.1834738 -3.7923722 -5.1872625 -5.8254089 -5.9241896 -5.3559628][-5.1803112 -3.9367652 -3.7006316 -2.8575783 -1.264668 1.1149487 3.1978502 3.7257681 2.7054348 -0.23659611 -3.4582369 -5.3971786 -6.1933889 -6.0099878 -5.3586946][-5.3715372 -3.9177368 -4.0279908 -3.2718937 -1.5135695 1.100467 3.5118742 4.4679837 3.5477448 0.60013747 -2.6852486 -4.8542719 -5.7208815 -5.4001427 -4.9429131][-4.5523338 -3.4184723 -3.9817262 -3.7370987 -2.3243825 0.28314114 2.7566752 4.22566 3.4550371 0.80844784 -2.3243535 -4.4315524 -5.140089 -4.7512884 -4.7052455][-3.2197938 -2.3167219 -3.2236691 -3.6683526 -2.6791916 -0.38752961 2.0073514 3.7229323 3.0716033 0.43251228 -2.439007 -4.4084439 -4.8489895 -4.4281554 -4.6724186][-2.6215765 -1.2610837 -2.4842024 -3.5876169 -2.976558 -1.0597094 1.2077143 3.1015825 2.5292926 -0.047718525 -2.7293274 -4.5864158 -4.8294964 -4.3629017 -4.7336721][-1.8266535 -0.37180734 -1.9012706 -3.6156378 -3.5008941 -2.1390982 0.030006647 2.0635834 1.6339953 -0.814301 -3.2920027 -4.97428 -4.9934788 -4.5894952 -4.9199476][-0.96425617 0.56788874 -1.2215549 -3.4565072 -3.7695587 -2.7988594 -0.7098645 1.1730578 0.91089511 -1.2292752 -3.4635181 -4.991467 -5.04829 -4.7822618 -4.9637585][-0.6067394 1.1568327 -0.76874244 -3.3393078 -3.9139421 -3.0253687 -0.92173839 0.64029908 0.41485071 -1.3899755 -3.3162441 -4.8104963 -5.1144009 -5.0171347 -5.0955243][-1.4950356 0.67848659 -1.1988535 -3.9251041 -4.607635 -3.5066934 -1.390579 -0.1547153 -0.41819 -2.0640938 -3.7697573 -5.1849852 -5.5747118 -5.4953823 -5.3745403][-3.078826 -0.5095855 -2.1387062 -4.7038612 -5.2531686 -4.0606151 -2.1701081 -1.2988448 -1.5451481 -2.9503763 -4.3792467 -5.4885473 -5.7370915 -5.6339417 -5.4092512][-4.3131127 -1.6664222 -3.0653129 -5.1871223 -5.5131674 -4.4938273 -3.1268647 -2.5721836 -2.791667 -3.7155261 -4.608201 -5.2706475 -5.4468312 -5.4835567 -5.2696538][-5.0582409 -2.8966591 -4.0206618 -5.6095142 -5.7890048 -5.109107 -4.2263288 -3.7849822 -3.9439309 -4.4802227 -5.05966 -5.2614403 -5.2004633 -5.15343 -4.87414]]...]
INFO - root - 2017-12-15 08:04:29.851301: step 31710, loss = 0.17, batch loss = 0.14 (35.6 examples/sec; 0.225 sec/batch; 18h:46m:55s remains)
INFO - root - 2017-12-15 08:04:32.113608: step 31720, loss = 0.20, batch loss = 0.17 (35.0 examples/sec; 0.229 sec/batch; 19h:07m:02s remains)
INFO - root - 2017-12-15 08:04:34.373711: step 31730, loss = 0.17, batch loss = 0.14 (36.6 examples/sec; 0.218 sec/batch; 18h:14m:46s remains)
INFO - root - 2017-12-15 08:04:36.659693: step 31740, loss = 0.25, batch loss = 0.22 (35.2 examples/sec; 0.228 sec/batch; 19h:00m:45s remains)
INFO - root - 2017-12-15 08:04:38.933582: step 31750, loss = 0.21, batch loss = 0.18 (34.5 examples/sec; 0.232 sec/batch; 19h:22m:49s remains)
INFO - root - 2017-12-15 08:04:41.270826: step 31760, loss = 0.20, batch loss = 0.17 (35.2 examples/sec; 0.227 sec/batch; 18h:58m:50s remains)
INFO - root - 2017-12-15 08:04:43.542948: step 31770, loss = 0.21, batch loss = 0.17 (35.8 examples/sec; 0.224 sec/batch; 18h:40m:33s remains)
INFO - root - 2017-12-15 08:04:45.814295: step 31780, loss = 0.22, batch loss = 0.19 (35.5 examples/sec; 0.226 sec/batch; 18h:50m:15s remains)
INFO - root - 2017-12-15 08:04:48.092043: step 31790, loss = 0.21, batch loss = 0.18 (35.8 examples/sec; 0.223 sec/batch; 18h:39m:30s remains)
INFO - root - 2017-12-15 08:04:50.352736: step 31800, loss = 0.24, batch loss = 0.21 (33.7 examples/sec; 0.237 sec/batch; 19h:49m:47s remains)
2017-12-15 08:04:50.643913: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3026257 -5.5738993 -5.6227155 -5.4333687 -5.0439472 -5.0057096 -4.764205 -4.8993292 -5.4943514 -5.2453289 -4.1677389 -3.1101177 -2.0347762 -0.69659829 -0.3204968][-4.6998496 -5.5202379 -5.41106 -5.2804794 -4.8069544 -4.3968 -3.9126191 -4.4668317 -5.4582653 -5.5277443 -4.7034292 -3.6648884 -2.3048964 -0.68876815 -0.091133833][-4.7380676 -5.2329493 -4.97609 -4.8387346 -4.2212181 -3.2826953 -2.4790277 -3.1915359 -4.471303 -4.93816 -4.7143145 -4.109477 -2.8537631 -1.1822569 -0.56546795][-5.1995392 -4.8433037 -4.2412891 -3.9077103 -3.1078129 -1.63953 -0.35921419 -1.1341021 -2.6992776 -3.5986266 -4.1842318 -4.4222059 -3.6820958 -2.118505 -1.515367][-5.3378115 -4.0867348 -3.1245503 -2.3224046 -1.1304429 0.8916719 2.7487111 1.9973998 0.0558908 -1.5014384 -2.9106579 -3.9300337 -4.0122747 -2.8720803 -2.3958988][-5.0639715 -3.6242919 -2.2888463 -0.81915939 1.0570478 3.660347 6.0492516 5.3569942 3.0392027 0.80566931 -1.445641 -3.098371 -3.8270979 -3.2439384 -3.0413537][-4.5408106 -3.3416309 -1.9967318 -0.1963644 2.1497726 5.1148024 7.7791615 7.3509722 5.1026845 2.6205988 -0.070104122 -2.0579011 -3.1342747 -2.9419403 -2.9801166][-4.7459383 -3.7112479 -2.4688673 -0.62594545 1.8473301 4.8266554 7.6600275 7.6175852 5.7020226 3.4780383 0.48426247 -1.702741 -2.9411705 -3.0772805 -3.1949434][-5.3320856 -4.6501818 -3.6356814 -2.1326542 0.037989616 2.851604 5.5585675 5.6632276 4.0931935 2.2401586 -0.689605 -2.5850315 -3.350004 -3.4532974 -3.4127405][-6.0644693 -5.8505869 -5.125741 -4.0435815 -2.3042705 0.24145579 2.5550609 2.5460267 1.2331388 -0.12171364 -2.6769183 -3.974386 -3.9606152 -3.6826124 -3.38094][-6.7876139 -6.815836 -6.2245617 -5.4196005 -4.020772 -1.9353681 -0.2050209 -0.33094072 -1.2550887 -2.191144 -4.3711367 -5.3852005 -5.1020689 -4.6021838 -4.0360389][-7.3461967 -7.562047 -7.1430888 -6.5083714 -5.3749142 -3.8578053 -2.7107668 -2.8284192 -3.3254998 -3.9151516 -5.6751466 -6.4952064 -6.0185266 -5.2597413 -4.4333811][-7.7942019 -7.8209262 -7.5167131 -6.9836416 -5.85449 -4.5879412 -3.9942455 -4.2567663 -4.5203114 -5.0576043 -6.3693652 -6.9819474 -6.4274483 -5.5283604 -4.6829638][-7.6819377 -7.1717672 -6.8449984 -6.4282308 -5.4939609 -4.6325588 -4.5400009 -4.8747892 -5.0955486 -5.6781864 -6.601347 -6.8849769 -6.33514 -5.3633471 -4.4643974][-7.4957585 -6.5393157 -6.2078047 -5.9970951 -5.3616357 -4.8480053 -5.0279222 -5.3809266 -5.482501 -5.887207 -6.3243222 -6.2304745 -5.6927204 -4.8275514 -3.9732575]]...]
INFO - root - 2017-12-15 08:04:52.940594: step 31810, loss = 0.17, batch loss = 0.14 (31.8 examples/sec; 0.251 sec/batch; 21h:00m:11s remains)
INFO - root - 2017-12-15 08:04:55.221860: step 31820, loss = 0.27, batch loss = 0.24 (35.5 examples/sec; 0.225 sec/batch; 18h:48m:07s remains)
INFO - root - 2017-12-15 08:04:57.542140: step 31830, loss = 0.16, batch loss = 0.13 (31.8 examples/sec; 0.252 sec/batch; 21h:00m:58s remains)
INFO - root - 2017-12-15 08:04:59.792335: step 31840, loss = 0.21, batch loss = 0.18 (36.5 examples/sec; 0.219 sec/batch; 18h:17m:17s remains)
INFO - root - 2017-12-15 08:05:02.060280: step 31850, loss = 0.21, batch loss = 0.17 (35.2 examples/sec; 0.227 sec/batch; 18h:57m:28s remains)
INFO - root - 2017-12-15 08:05:04.366938: step 31860, loss = 0.24, batch loss = 0.21 (35.5 examples/sec; 0.226 sec/batch; 18h:50m:11s remains)
INFO - root - 2017-12-15 08:05:06.640287: step 31870, loss = 0.29, batch loss = 0.26 (35.7 examples/sec; 0.224 sec/batch; 18h:42m:05s remains)
INFO - root - 2017-12-15 08:05:08.972386: step 31880, loss = 0.24, batch loss = 0.21 (36.2 examples/sec; 0.221 sec/batch; 18h:26m:47s remains)
INFO - root - 2017-12-15 08:05:11.275967: step 31890, loss = 0.23, batch loss = 0.20 (35.0 examples/sec; 0.229 sec/batch; 19h:06m:01s remains)
INFO - root - 2017-12-15 08:05:13.557624: step 31900, loss = 0.21, batch loss = 0.18 (34.8 examples/sec; 0.230 sec/batch; 19h:12m:11s remains)
2017-12-15 08:05:13.872990: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3920815 -3.9176431 -3.2135034 -2.507839 -1.7370236 -1.7421815 -2.5698128 -2.6114113 -2.0588961 -1.8023242 -2.2544379 -2.2974651 -2.0041053 -2.6499012 -4.0541229][-3.1412308 -4.3083797 -3.7767019 -2.9842336 -2.0282297 -1.9280627 -2.392689 -2.0792468 -1.6820233 -1.8705401 -2.6097634 -2.9544725 -2.8029947 -3.5727489 -5.0025644][-4.0363989 -4.9220886 -4.6435709 -3.9015393 -2.6500671 -2.4280875 -2.5387785 -1.9430465 -1.6101205 -2.1472738 -3.0824029 -3.5097294 -3.4095321 -4.2359567 -5.5996566][-5.0031137 -5.306262 -5.0376797 -4.1264076 -2.4095998 -1.9037945 -1.4278373 -0.41287148 -0.23508811 -1.2464713 -2.3855786 -3.0919967 -3.3759258 -4.6038952 -5.9539685][-5.925477 -5.5399504 -5.12029 -3.8932519 -1.7571832 -0.90436852 0.31682539 1.8821845 1.8315511 0.12529159 -1.4911121 -2.7658191 -3.5804782 -5.1816244 -6.4979463][-5.875392 -5.1745625 -4.5491381 -2.9239597 -0.54050767 0.58557391 2.506916 4.5672092 4.2026014 1.85115 -0.2591846 -2.161236 -3.5199442 -5.4171929 -6.653738][-5.3132772 -4.576911 -3.9082613 -2.0691917 0.26278853 1.5502076 3.8990779 6.2431111 5.5878134 2.9173846 0.50916624 -1.7416528 -3.1602204 -5.0967913 -6.2078381][-4.4903116 -3.684237 -3.1512749 -1.4068024 0.644094 2.1095452 4.8060441 7.3609157 6.588553 3.8211465 1.0925124 -1.5016476 -2.7990141 -4.3933949 -5.2500825][-4.2673569 -3.2003975 -2.7124343 -1.1005235 0.65652227 2.2879987 5.0583105 7.5691991 6.82567 4.2085004 1.276165 -1.4404854 -2.5344179 -3.8303933 -4.5921106][-4.9386873 -3.8786871 -3.6642199 -2.5302045 -1.298178 0.1571281 2.5104685 4.5923066 4.0664124 2.0129352 -0.6110276 -2.9010365 -3.4720154 -4.2026443 -4.7757215][-5.9164109 -4.8178973 -4.8893194 -4.3228579 -3.6419399 -2.4903748 -0.68145955 0.85886216 0.68595243 -0.629313 -2.6578219 -4.4239888 -4.687335 -4.89652 -5.1974273][-6.7662544 -5.6800647 -5.9518795 -5.7845573 -5.4289675 -4.5840931 -3.3922415 -2.4130619 -2.3357003 -3.0828323 -4.5093422 -5.7179213 -5.7841229 -5.6880016 -5.7046852][-6.8205667 -5.8849525 -6.392694 -6.59699 -6.513443 -6.0777512 -5.511044 -5.0262184 -4.8928108 -5.3615494 -6.2901936 -6.930759 -6.8207507 -6.4521384 -6.14867][-6.4615612 -5.4255381 -5.9448814 -6.3153272 -6.3827906 -6.28959 -6.2126093 -6.1194553 -6.0816126 -6.4497585 -7.0255055 -7.284142 -7.0604119 -6.5999632 -6.1396465][-5.8321829 -4.6508665 -5.1244125 -5.6091094 -5.8316259 -5.9441595 -6.15921 -6.3299789 -6.4379191 -6.7430291 -7.0070047 -6.9757605 -6.6251869 -6.0823917 -5.5103493]]...]
INFO - root - 2017-12-15 08:05:16.116929: step 31910, loss = 0.33, batch loss = 0.30 (35.2 examples/sec; 0.227 sec/batch; 18h:57m:21s remains)
INFO - root - 2017-12-15 08:05:18.419366: step 31920, loss = 0.25, batch loss = 0.22 (34.2 examples/sec; 0.234 sec/batch; 19h:33m:26s remains)
INFO - root - 2017-12-15 08:05:20.703609: step 31930, loss = 0.25, batch loss = 0.22 (34.8 examples/sec; 0.230 sec/batch; 19h:13m:15s remains)
INFO - root - 2017-12-15 08:05:22.951370: step 31940, loss = 0.23, batch loss = 0.20 (36.1 examples/sec; 0.221 sec/batch; 18h:29m:27s remains)
INFO - root - 2017-12-15 08:05:25.241944: step 31950, loss = 0.21, batch loss = 0.17 (32.9 examples/sec; 0.243 sec/batch; 20h:19m:29s remains)
INFO - root - 2017-12-15 08:05:27.516796: step 31960, loss = 0.21, batch loss = 0.17 (34.6 examples/sec; 0.231 sec/batch; 19h:18m:13s remains)
INFO - root - 2017-12-15 08:05:29.794480: step 31970, loss = 0.24, batch loss = 0.21 (35.5 examples/sec; 0.225 sec/batch; 18h:47m:11s remains)
INFO - root - 2017-12-15 08:05:32.085085: step 31980, loss = 0.27, batch loss = 0.24 (35.3 examples/sec; 0.227 sec/batch; 18h:55m:51s remains)
INFO - root - 2017-12-15 08:05:34.355166: step 31990, loss = 0.20, batch loss = 0.17 (35.1 examples/sec; 0.228 sec/batch; 18h:59m:56s remains)
INFO - root - 2017-12-15 08:05:36.607760: step 32000, loss = 0.23, batch loss = 0.20 (35.0 examples/sec; 0.228 sec/batch; 19h:03m:37s remains)
2017-12-15 08:05:36.905437: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.17696 -7.1064625 -7.505744 -7.8542156 -8.0818453 -8.2215862 -8.3492975 -8.5000973 -8.5583658 -8.5085707 -7.965641 -7.1820831 -6.542552 -5.9475327 -5.5785222][-6.1840153 -7.5290632 -7.76797 -7.8536015 -7.83346 -7.860775 -7.9752607 -8.3133183 -8.8167315 -9.1460524 -8.69111 -7.7848749 -7.0071678 -6.2694464 -5.80451][-7.1661034 -7.4912529 -7.4413404 -7.1367569 -6.7283468 -6.4530239 -6.3161736 -6.6018734 -7.4372358 -8.22529 -8.0546255 -7.2058344 -6.3361778 -5.5496311 -5.1295948][-7.7575483 -7.1639185 -6.7353992 -5.9658966 -5.060699 -4.3545489 -3.9016886 -4.065866 -5.1042366 -6.1783981 -6.3637304 -5.8650231 -5.0881319 -4.3411722 -4.0152946][-7.8465214 -6.7029533 -5.89914 -4.7568197 -3.3226726 -1.9324857 -0.9587785 -1.0658433 -2.4576013 -3.880507 -4.3925185 -4.2487922 -3.6369691 -3.0217724 -2.8484998][-7.5259066 -6.1750031 -5.0753856 -3.6681089 -1.8143336 0.25340295 1.870672 1.9610369 0.26588249 -1.6153622 -2.5782433 -2.8995433 -2.4030693 -1.854556 -1.7519112][-6.6339369 -5.5334706 -4.1694474 -2.5033457 -0.39376915 2.0479748 4.0382204 4.4015112 2.7389653 0.52509928 -0.980487 -1.7882956 -1.5453253 -1.103832 -1.0460854][-5.9516935 -4.9476132 -3.5586488 -1.8865501 0.146698 2.529741 4.5580807 5.1701765 3.8809326 1.7927563 0.081958771 -1.0441598 -1.1186465 -0.85687566 -0.9827137][-5.4300442 -4.4817381 -3.3212805 -2.0581007 -0.5118407 1.5643995 3.5072353 4.3117638 3.4339297 1.7077858 0.18335843 -0.93715739 -1.2221572 -1.0786732 -1.3893917][-5.1439648 -4.2086024 -3.3401341 -2.577085 -1.633094 0.006964922 1.8180511 2.6347225 1.985039 0.66535187 -0.46555483 -1.3117571 -1.6942441 -1.6708136 -2.1930935][-5.1745596 -4.2276754 -3.5847249 -3.1539845 -2.6902127 -1.4556432 0.012947798 0.687124 0.21041083 -0.69873667 -1.5207386 -2.0716743 -2.3601847 -2.4156864 -3.1544795][-5.3242321 -4.4683065 -4.1187568 -3.8855104 -3.6332669 -2.76722 -1.7594604 -1.3344862 -1.557604 -2.0528419 -2.7032773 -3.1122193 -3.1952527 -3.1981006 -3.9846377][-5.7695665 -5.0404944 -4.9867125 -4.8788958 -4.6529484 -4.0089369 -3.3302388 -2.9834166 -2.896162 -3.0799332 -3.8796949 -4.3201904 -4.2892208 -4.2500863 -4.9202433][-5.8176455 -5.2366495 -5.3468437 -5.2852583 -5.0101671 -4.4709244 -4.1144485 -3.8864927 -3.6231451 -3.6944027 -4.5730371 -4.9947848 -4.9284225 -4.9460583 -5.5909362][-5.9643245 -5.5293827 -5.6407404 -5.6003809 -5.3355474 -4.9351873 -4.7527738 -4.599216 -4.36388 -4.4782333 -5.2921162 -5.6219826 -5.5403109 -5.609251 -6.1592884]]...]
INFO - root - 2017-12-15 08:05:39.210254: step 32010, loss = 0.23, batch loss = 0.19 (35.2 examples/sec; 0.227 sec/batch; 18h:58m:02s remains)
INFO - root - 2017-12-15 08:05:41.497185: step 32020, loss = 0.27, batch loss = 0.23 (34.7 examples/sec; 0.231 sec/batch; 19h:14m:30s remains)
INFO - root - 2017-12-15 08:05:43.805567: step 32030, loss = 0.27, batch loss = 0.24 (35.3 examples/sec; 0.227 sec/batch; 18h:55m:11s remains)
INFO - root - 2017-12-15 08:05:46.048748: step 32040, loss = 0.28, batch loss = 0.25 (36.4 examples/sec; 0.220 sec/batch; 18h:21m:59s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 08:05:48.313371: step 32050, loss = 0.24, batch loss = 0.20 (34.5 examples/sec; 0.232 sec/batch; 19h:20m:18s remains)
INFO - root - 2017-12-15 08:05:50.568570: step 32060, loss = 0.25, batch loss = 0.22 (35.3 examples/sec; 0.227 sec/batch; 18h:55m:22s remains)
INFO - root - 2017-12-15 08:05:52.848081: step 32070, loss = 0.26, batch loss = 0.22 (35.7 examples/sec; 0.224 sec/batch; 18h:42m:07s remains)
INFO - root - 2017-12-15 08:05:55.153858: step 32080, loss = 0.20, batch loss = 0.17 (34.3 examples/sec; 0.233 sec/batch; 19h:27m:26s remains)
INFO - root - 2017-12-15 08:05:57.436538: step 32090, loss = 0.23, batch loss = 0.20 (34.7 examples/sec; 0.231 sec/batch; 19h:15m:39s remains)
INFO - root - 2017-12-15 08:05:59.731731: step 32100, loss = 0.18, batch loss = 0.14 (34.6 examples/sec; 0.231 sec/batch; 19h:18m:36s remains)
2017-12-15 08:06:00.041887: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9383318 -1.405539 -0.21478772 -0.090296984 -0.40761089 -0.44329965 -0.77063334 -0.97755134 -1.1232386 -1.5295718 -2.4908628 -4.0895333 -5.1574593 -5.7970448 -6.2988882][-2.6893878 -1.4799695 -0.1525476 0.13664412 -0.072348356 -0.095732689 -0.49712884 -0.95088279 -1.5385453 -2.2746525 -3.3828497 -4.9337535 -5.8261738 -6.3774948 -6.8054962][-3.9302616 -1.95429 -0.5442549 0.014462233 0.020421743 0.097221375 -0.20017171 -0.86203384 -1.8431765 -2.7089834 -3.8105402 -5.1032343 -5.6959391 -6.1481543 -6.6079693][-5.2928414 -3.0427215 -1.4540229 -0.46097994 -0.22447968 -0.13021708 -0.16890955 -0.88845837 -2.1750002 -2.8730607 -3.8719649 -4.8286572 -5.0743589 -5.4718275 -5.9340339][-6.9687414 -4.2300606 -2.2610843 -0.63412523 -0.04053092 0.12730384 0.41134 -0.53814054 -2.34698 -2.8437998 -3.8050828 -4.4287767 -4.3639488 -4.6393781 -4.9713187][-7.6604195 -4.8169632 -2.5513797 -0.3859489 0.53060389 0.80012226 1.3499002 -0.018261194 -2.3957679 -2.681921 -3.6283565 -4.0517373 -3.9291887 -4.1856604 -4.3624654][-6.90396 -4.6118793 -2.1692224 0.25876379 1.2871614 1.5770392 2.2634442 0.49791813 -2.4301147 -2.5196786 -3.4485798 -3.6866288 -3.5468137 -3.6763539 -3.6035423][-6.7152147 -4.5023341 -2.0242732 0.48549294 1.5610225 1.8661673 2.633868 0.74020338 -2.5681839 -2.567059 -3.4810281 -3.5879924 -3.4821944 -3.5558269 -3.4001002][-7.1832237 -5.0561347 -2.7237954 -0.34727883 0.7402761 1.0517604 1.8272083 -0.08552289 -3.5310361 -3.6446905 -4.4963322 -4.4668531 -4.379076 -4.389864 -4.16325][-7.3547411 -5.2762165 -3.3304281 -1.4316216 -0.48797739 -0.18147779 0.41284394 -1.4012895 -4.6266832 -4.9355431 -5.6116009 -5.4072542 -5.13271 -4.9425 -4.6570721][-8.0209837 -6.1254525 -4.7917256 -3.4384305 -2.561832 -2.1306796 -1.6433713 -3.0202088 -5.552382 -5.9765053 -6.5064688 -6.2488718 -5.9146028 -5.6956573 -5.5042992][-8.6108732 -6.9096041 -6.0670786 -5.1530743 -4.4071074 -4.0041261 -3.7638197 -4.7607183 -6.5705204 -7.0968943 -7.5233593 -7.3050108 -6.8637133 -6.5137281 -6.2400103][-7.5804281 -6.153223 -5.7803564 -5.362071 -4.9618282 -4.7530136 -4.7017694 -5.3204679 -6.3412209 -6.7961988 -7.0504045 -6.8125582 -6.3225946 -5.9193053 -5.6614304][-6.2911987 -5.14682 -5.1116924 -5.0338135 -4.9443946 -4.9116583 -4.9988327 -5.3086643 -5.769526 -6.1307039 -6.3049231 -6.1088047 -5.6952143 -5.3720312 -5.1384048][-5.5179691 -4.4806213 -4.4511423 -4.4469872 -4.4782572 -4.5550909 -4.7150512 -4.9674697 -5.2695866 -5.5715075 -5.7106977 -5.5963478 -5.3480568 -5.0713859 -4.7823133]]...]
INFO - root - 2017-12-15 08:06:02.285255: step 32110, loss = 0.18, batch loss = 0.15 (35.7 examples/sec; 0.224 sec/batch; 18h:43m:22s remains)
INFO - root - 2017-12-15 08:06:04.590273: step 32120, loss = 0.21, batch loss = 0.18 (36.0 examples/sec; 0.222 sec/batch; 18h:31m:00s remains)
INFO - root - 2017-12-15 08:06:06.901217: step 32130, loss = 0.19, batch loss = 0.16 (35.3 examples/sec; 0.227 sec/batch; 18h:53m:54s remains)
INFO - root - 2017-12-15 08:06:09.232419: step 32140, loss = 0.44, batch loss = 0.41 (35.3 examples/sec; 0.227 sec/batch; 18h:53m:52s remains)
INFO - root - 2017-12-15 08:06:11.518490: step 32150, loss = 0.24, batch loss = 0.21 (33.5 examples/sec; 0.239 sec/batch; 19h:55m:03s remains)
INFO - root - 2017-12-15 08:06:13.787532: step 32160, loss = 0.31, batch loss = 0.28 (35.1 examples/sec; 0.228 sec/batch; 19h:01m:46s remains)
INFO - root - 2017-12-15 08:06:16.042265: step 32170, loss = 0.16, batch loss = 0.13 (36.2 examples/sec; 0.221 sec/batch; 18h:26m:46s remains)
INFO - root - 2017-12-15 08:06:18.305969: step 32180, loss = 0.22, batch loss = 0.19 (36.0 examples/sec; 0.222 sec/batch; 18h:32m:21s remains)
INFO - root - 2017-12-15 08:06:20.603580: step 32190, loss = 0.41, batch loss = 0.37 (34.0 examples/sec; 0.235 sec/batch; 19h:37m:38s remains)
INFO - root - 2017-12-15 08:06:22.893199: step 32200, loss = 0.27, batch loss = 0.24 (34.4 examples/sec; 0.232 sec/batch; 19h:22m:34s remains)
2017-12-15 08:06:23.210446: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7328477 -5.3854256 -5.5817375 -5.6793947 -5.6946678 -5.6689696 -5.671279 -5.6697035 -5.5796204 -5.4649773 -5.389987 -5.3999929 -5.3675613 -5.2518253 -5.1898007][-4.5580678 -5.6598396 -5.9724946 -6.0661373 -5.9952536 -5.9014158 -5.8889303 -5.898056 -5.880157 -5.895174 -5.9283857 -6.0102654 -5.9792495 -5.8110533 -5.6287861][-5.5640478 -5.9032059 -6.1088676 -5.9594703 -5.5795255 -5.2617755 -5.1091738 -5.1122985 -5.2816596 -5.5966043 -5.9406915 -6.240406 -6.3002625 -6.1642847 -5.9704275][-6.4391608 -6.0278807 -5.9041214 -5.3393831 -4.5512714 -3.9560537 -3.6250019 -3.7316389 -4.2498951 -5.007174 -5.7855539 -6.3486757 -6.5118032 -6.3849273 -6.2021384][-7.109901 -5.8300195 -5.1984067 -4.0612936 -2.7474864 -1.7626477 -1.2215458 -1.5482323 -2.5802319 -3.9444475 -5.2523556 -6.1189175 -6.4286661 -6.3388443 -6.1312084][-7.0229721 -5.2371306 -3.9946611 -2.1875255 -0.26029611 1.1979771 1.9590187 1.358073 -0.21380496 -2.2240553 -4.0510769 -5.2854214 -5.811244 -5.835885 -5.5853291][-6.3387914 -4.4025059 -2.5748539 -0.16250849 2.308919 4.1652493 5.0394983 4.0781198 1.9964271 -0.56337285 -2.8145022 -4.3627319 -5.113955 -5.2770147 -4.9496641][-6.0247397 -3.9080334 -1.8576617 0.78454304 3.5244861 5.6641283 6.6424494 5.5099716 3.1525378 0.36024833 -2.0258555 -3.6531937 -4.4936838 -4.7872143 -4.4411154][-6.1887832 -4.2519441 -2.4379046 -0.019332886 2.606586 4.7579784 5.756772 4.7577248 2.5146217 -0.088530064 -2.2221611 -3.6554601 -4.3702874 -4.6115866 -4.1760569][-6.5739179 -4.9229569 -3.5193927 -1.5457056 0.69243574 2.551281 3.4036012 2.5730386 0.62499142 -1.5638278 -3.2694788 -4.3222451 -4.7342968 -4.6913605 -4.0798664][-7.190814 -5.5953751 -4.3548784 -2.6879303 -0.92803442 0.4045105 0.88593912 0.11482406 -1.596873 -3.4005561 -4.742074 -5.4109783 -5.4755907 -5.0990286 -4.3401423][-7.9421272 -6.2630234 -4.9624376 -3.3767815 -1.9472967 -1.026419 -0.95435846 -1.7992072 -3.3391714 -4.8553143 -5.9275126 -6.3377018 -6.1923156 -5.60514 -4.7776804][-7.85324 -6.1723309 -4.8431826 -3.2895927 -2.064914 -1.4646983 -1.6958393 -2.6732779 -4.1260252 -5.4314032 -6.4655046 -6.74327 -6.4783883 -5.7327614 -4.8638787][-6.8132362 -5.02727 -3.6866682 -2.2879167 -1.395745 -1.2681979 -1.8866341 -3.0603986 -4.4501534 -5.5463915 -6.6156459 -6.8132062 -6.445735 -5.6137815 -4.8082209][-5.3690891 -3.3973346 -2.0326533 -0.77385592 -0.1366117 -0.41065216 -1.4309134 -2.880579 -4.3012419 -5.2562656 -6.3468266 -6.514255 -6.2253571 -5.5381064 -4.8646812]]...]
INFO - root - 2017-12-15 08:06:25.481800: step 32210, loss = 0.23, batch loss = 0.20 (35.5 examples/sec; 0.225 sec/batch; 18h:48m:12s remains)
INFO - root - 2017-12-15 08:06:27.750133: step 32220, loss = 0.21, batch loss = 0.18 (35.7 examples/sec; 0.224 sec/batch; 18h:43m:03s remains)
INFO - root - 2017-12-15 08:06:30.016536: step 32230, loss = 0.41, batch loss = 0.37 (34.6 examples/sec; 0.231 sec/batch; 19h:15m:38s remains)
INFO - root - 2017-12-15 08:06:32.305487: step 32240, loss = 0.21, batch loss = 0.18 (35.9 examples/sec; 0.223 sec/batch; 18h:33m:43s remains)
INFO - root - 2017-12-15 08:06:34.568979: step 32250, loss = 0.18, batch loss = 0.15 (35.2 examples/sec; 0.228 sec/batch; 18h:58m:50s remains)
INFO - root - 2017-12-15 08:06:36.835901: step 32260, loss = 0.22, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 18h:36m:33s remains)
INFO - root - 2017-12-15 08:06:39.094262: step 32270, loss = 0.19, batch loss = 0.16 (34.7 examples/sec; 0.230 sec/batch; 19h:13m:21s remains)
INFO - root - 2017-12-15 08:06:41.359153: step 32280, loss = 0.30, batch loss = 0.27 (36.5 examples/sec; 0.219 sec/batch; 18h:17m:13s remains)
INFO - root - 2017-12-15 08:06:43.636411: step 32290, loss = 0.26, batch loss = 0.23 (34.2 examples/sec; 0.234 sec/batch; 19h:30m:26s remains)
INFO - root - 2017-12-15 08:06:45.876277: step 32300, loss = 0.19, batch loss = 0.16 (35.6 examples/sec; 0.225 sec/batch; 18h:44m:18s remains)
2017-12-15 08:06:46.169885: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5394232 -0.48082435 -0.041859388 0.011257648 -0.51083767 -1.2546667 -2.3348789 -3.1109874 -3.548079 -3.5204391 -2.5926366 -1.729049 -0.79511619 0.18223667 0.64607239][-2.2973757 -0.578779 -0.062301874 0.11790609 -0.24136925 -0.78324056 -1.9661107 -2.7645962 -3.261795 -3.2482693 -2.5143688 -1.807285 -0.78486359 0.49925661 1.1796448][-3.3979783 -1.1431729 -0.69412673 -0.33336473 -0.26645696 -0.38248026 -1.2788537 -2.0256631 -2.7268231 -2.9526567 -2.4692147 -2.0020533 -0.98533738 0.58938289 1.5210788][-3.8174653 -1.8413877 -1.5181034 -1.1284395 -0.70196509 -0.2836324 -0.7778697 -1.3737628 -2.1343019 -2.619864 -2.4954817 -2.3554218 -1.5034428 0.10201144 1.1397166][-4.1786051 -2.4727855 -2.2895596 -1.951072 -1.2707527 -0.36275315 -0.34766722 -0.62139988 -1.3970242 -2.1627345 -2.5199287 -2.8530588 -2.376008 -1.0162845 -0.039045811][-4.3423948 -2.4744658 -2.322109 -2.0464122 -1.2442911 -0.10761786 0.22175074 0.020730257 -0.84089017 -1.9084754 -2.78577 -3.4978404 -3.3449454 -2.2435253 -1.4779541][-3.5176282 -2.1769474 -1.8693786 -1.4828167 -0.55122066 0.72657776 1.174624 0.93127632 -0.046401978 -1.3296227 -2.4576979 -3.4005852 -3.5710096 -2.8257635 -2.4142025][-3.5094357 -2.2127478 -1.6097305 -0.86844945 0.40855503 1.9400465 2.6190889 2.3915584 1.1987143 -0.46177232 -1.9356905 -3.1660414 -3.5740423 -3.0013368 -2.8542807][-3.750165 -2.6599746 -1.9326928 -0.97885287 0.56650567 2.2075984 3.1504872 3.1336181 1.88112 0.041290522 -1.7560086 -3.2057459 -3.7858491 -3.2388391 -3.1212471][-4.1256075 -3.4084704 -2.7772322 -1.9008454 -0.50830328 0.95755267 2.0248811 2.3793848 1.4648759 -0.16155505 -1.8802828 -3.5187011 -4.2920766 -3.6745186 -3.38842][-4.2506876 -3.7500839 -3.183435 -2.5650408 -1.4945166 -0.41175878 0.35500622 0.7696538 0.15012193 -1.056478 -2.43195 -4.0320411 -4.7915444 -4.0768251 -3.3817942][-4.0780954 -3.6551118 -3.1632707 -2.7496953 -1.9718944 -1.3258909 -0.99358666 -0.71350348 -1.2463346 -2.0568385 -3.0266044 -4.3474197 -4.9574671 -4.1962342 -3.2038238][-3.8888426 -3.2972212 -2.7988505 -2.609267 -2.119025 -1.6817927 -1.6157562 -1.4132581 -1.9002761 -2.44115 -3.1804564 -4.3505812 -4.9204016 -4.2056818 -3.0096755][-3.7696464 -2.9282284 -2.3488069 -2.2715979 -1.9075558 -1.4496906 -1.2905493 -1.0327479 -1.4468784 -1.944272 -2.6957891 -3.896606 -4.5230503 -4.1330242 -3.0661569][-3.6230035 -2.5402088 -1.8957614 -1.9241558 -1.7144 -1.2434969 -0.899091 -0.4314065 -0.57165158 -0.93624258 -1.7912279 -3.0191166 -3.7386341 -3.8497787 -3.4377685]]...]
INFO - root - 2017-12-15 08:06:48.447136: step 32310, loss = 0.26, batch loss = 0.22 (35.2 examples/sec; 0.227 sec/batch; 18h:57m:13s remains)
INFO - root - 2017-12-15 08:06:50.710232: step 32320, loss = 0.34, batch loss = 0.31 (36.1 examples/sec; 0.222 sec/batch; 18h:28m:21s remains)
INFO - root - 2017-12-15 08:06:52.996513: step 32330, loss = 0.26, batch loss = 0.23 (36.1 examples/sec; 0.222 sec/batch; 18h:29m:09s remains)
INFO - root - 2017-12-15 08:06:55.295821: step 32340, loss = 0.26, batch loss = 0.23 (33.0 examples/sec; 0.242 sec/batch; 20h:12m:39s remains)
INFO - root - 2017-12-15 08:06:57.583525: step 32350, loss = 0.20, batch loss = 0.17 (34.9 examples/sec; 0.230 sec/batch; 19h:08m:10s remains)
INFO - root - 2017-12-15 08:06:59.817638: step 32360, loss = 0.26, batch loss = 0.23 (36.4 examples/sec; 0.220 sec/batch; 18h:20m:22s remains)
INFO - root - 2017-12-15 08:07:02.122884: step 32370, loss = 0.16, batch loss = 0.13 (34.4 examples/sec; 0.233 sec/batch; 19h:24m:21s remains)
INFO - root - 2017-12-15 08:07:04.377445: step 32380, loss = 0.21, batch loss = 0.18 (35.6 examples/sec; 0.225 sec/batch; 18h:43m:15s remains)
INFO - root - 2017-12-15 08:07:06.680503: step 32390, loss = 0.18, batch loss = 0.15 (35.5 examples/sec; 0.225 sec/batch; 18h:46m:03s remains)
INFO - root - 2017-12-15 08:07:09.011397: step 32400, loss = 0.33, batch loss = 0.30 (34.0 examples/sec; 0.235 sec/batch; 19h:36m:49s remains)
2017-12-15 08:07:09.287059: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2801709 -3.5720749 -2.9124839 -3.203635 -4.2859764 -5.4149575 -6.0449538 -6.2108793 -6.2273459 -6.3820047 -6.4030867 -6.1130953 -5.7913918 -5.5083475 -5.030777][-4.5502014 -3.4291453 -2.6306691 -2.9800851 -3.8149779 -4.8647671 -5.6769972 -5.9926987 -6.1375647 -6.6190529 -6.9293489 -6.910203 -6.6575994 -6.3552227 -5.7978497][-4.8438673 -2.7610588 -2.0838413 -2.5096152 -2.8858755 -3.5824044 -4.4952374 -4.7785635 -4.864377 -5.5745144 -6.2504005 -6.5499611 -6.42025 -6.1062303 -5.5441761][-4.7359529 -1.7754009 -1.2061008 -1.5435891 -1.5308261 -1.9538567 -2.8981481 -3.1603274 -3.4217544 -4.3630705 -5.4002233 -5.94724 -5.9052148 -5.6060982 -5.11084][-5.3470397 -2.1471238 -1.7200036 -1.7111648 -1.0558791 -0.75179148 -1.0626769 -0.99933934 -1.7122519 -3.1557674 -4.5599442 -5.3880329 -5.5542469 -5.4210353 -5.0867443][-4.1589189 -1.6124568 -1.4365902 -1.1700261 -0.18782759 0.7762506 1.5425422 2.2381675 1.2191126 -0.64060378 -2.4293694 -3.6037092 -4.2445755 -4.5977507 -4.6683254][-3.0594029 -1.2396119 -1.2675267 -0.76917386 0.51232028 1.9523618 3.5739286 4.6701326 3.1684787 0.9427352 -1.1606116 -2.5156391 -3.5043914 -4.1798563 -4.5744619][-2.720773 -1.2710931 -1.6578834 -1.1333475 0.34640002 1.9381216 4.1145344 5.3339043 3.6183035 1.45084 -0.47088921 -1.7585189 -2.8662429 -3.807343 -4.5542731][-2.9631736 -1.4932907 -2.0638764 -1.5601383 -0.10759139 1.1179528 2.9758842 4.0202417 2.7229917 1.116302 -0.37370181 -1.4366148 -2.5213177 -3.6412458 -4.5577483][-4.1846318 -2.546236 -3.0207825 -2.5762298 -1.2887857 -0.54833007 0.63405395 1.5035329 0.83151031 -0.13766932 -1.2097857 -1.8632146 -2.6847403 -3.8284507 -4.8708825][-5.20621 -3.25178 -3.5930252 -3.4442592 -2.6565361 -2.3979814 -1.7432482 -0.97409928 -1.1353869 -1.6449066 -2.3569181 -2.4866204 -2.7406175 -3.7761703 -4.9067011][-6.2352242 -4.1438403 -4.3588591 -4.4698973 -4.1377611 -4.2836361 -4.069685 -3.7288723 -3.8766332 -4.074368 -4.2672997 -3.8304274 -3.6263428 -4.4618483 -5.3910875][-7.1998892 -5.5263195 -5.8392253 -6.0647392 -5.9933882 -6.3827477 -6.4233294 -6.4766159 -6.68027 -6.4544373 -6.0625591 -5.1950388 -4.6436377 -5.0930128 -5.7251797][-7.4134932 -6.1202588 -6.4047461 -6.6573296 -6.7424297 -7.261858 -7.4363689 -7.6396179 -7.7259817 -7.2202377 -6.5616055 -5.6820493 -5.129879 -5.3679805 -5.7723436][-6.8269796 -5.9347954 -6.1205978 -6.2223539 -6.2657185 -6.7948732 -7.0383625 -7.25 -7.3020182 -6.7440004 -6.0394282 -5.4104567 -5.1146231 -5.2641311 -5.46136]]...]
INFO - root - 2017-12-15 08:07:11.550833: step 32410, loss = 0.15, batch loss = 0.12 (35.8 examples/sec; 0.224 sec/batch; 18h:38m:07s remains)
INFO - root - 2017-12-15 08:07:13.834730: step 32420, loss = 0.25, batch loss = 0.21 (33.3 examples/sec; 0.240 sec/batch; 20h:02m:22s remains)
INFO - root - 2017-12-15 08:07:16.123376: step 32430, loss = 0.28, batch loss = 0.25 (34.2 examples/sec; 0.234 sec/batch; 19h:29m:36s remains)
INFO - root - 2017-12-15 08:07:18.374242: step 32440, loss = 0.20, batch loss = 0.17 (35.9 examples/sec; 0.223 sec/batch; 18h:35m:36s remains)
INFO - root - 2017-12-15 08:07:20.636414: step 32450, loss = 0.20, batch loss = 0.17 (34.3 examples/sec; 0.233 sec/batch; 19h:27m:41s remains)
INFO - root - 2017-12-15 08:07:22.915906: step 32460, loss = 0.35, batch loss = 0.32 (35.5 examples/sec; 0.225 sec/batch; 18h:47m:07s remains)
INFO - root - 2017-12-15 08:07:25.189551: step 32470, loss = 0.35, batch loss = 0.32 (35.6 examples/sec; 0.225 sec/batch; 18h:44m:13s remains)
INFO - root - 2017-12-15 08:07:27.475890: step 32480, loss = 0.23, batch loss = 0.20 (34.7 examples/sec; 0.231 sec/batch; 19h:12m:46s remains)
INFO - root - 2017-12-15 08:07:29.758871: step 32490, loss = 0.26, batch loss = 0.23 (35.8 examples/sec; 0.224 sec/batch; 18h:37m:50s remains)
INFO - root - 2017-12-15 08:07:32.039233: step 32500, loss = 0.31, batch loss = 0.28 (35.3 examples/sec; 0.226 sec/batch; 18h:51m:36s remains)
2017-12-15 08:07:32.335013: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2894158 -5.3858371 -4.9285412 -3.4432893 -1.8201308 -1.3490255 -1.3943529 -1.638523 -2.2277443 -2.3589008 -1.3606193 -1.024889 -0.96021414 -1.9829127 -3.78995][-5.9105067 -6.445652 -6.0918627 -4.550303 -2.739783 -1.8483098 -1.6617411 -1.9213575 -2.7180789 -2.9909551 -2.3274393 -2.1293178 -2.145865 -2.9976974 -4.4642773][-6.8881006 -7.0790319 -6.8358774 -5.4470387 -3.5388865 -2.1356251 -1.6087526 -1.7966222 -2.7926211 -3.4856973 -3.3593121 -3.2864542 -3.0556417 -3.190135 -4.1061568][-7.4968328 -7.5051594 -7.3104067 -6.0329914 -4.0116062 -2.1912653 -1.3215771 -1.4196428 -2.4227555 -3.4592657 -3.9300478 -4.0096989 -3.5552568 -3.1035051 -3.6705174][-8.0117941 -7.5354204 -7.2038441 -5.7004666 -3.3462934 -1.165056 0.11781597 0.21995115 -0.83227861 -2.2922621 -3.3828692 -3.5031414 -2.9203453 -2.1238506 -2.4904532][-8.0784369 -7.0758934 -6.3892756 -4.4756718 -1.9579308 0.23116636 1.7271254 1.9756353 0.84964991 -0.80946195 -2.0991349 -2.3855844 -1.8996674 -0.94696462 -1.1149093][-7.2114878 -6.411334 -5.2759933 -2.9984853 -0.47641802 1.7474277 3.4535582 3.8113043 2.9526417 1.3699954 -0.046700478 -0.48347962 -0.22640657 0.39914656 0.056762695][-7.0573092 -6.2523947 -4.8586788 -2.4115219 0.047130823 2.290951 3.9815881 4.2717094 3.5232027 2.0671732 0.57864952 0.0258255 0.14473271 0.455117 -0.050785542][-7.6633987 -7.2182846 -6.0202026 -3.8655014 -1.6424689 0.65540051 2.3668354 2.5657399 1.9005778 0.668221 -0.74693489 -1.1656752 -0.93385124 -0.6372813 -0.97596025][-8.3125648 -8.1467743 -7.2520838 -5.5870609 -3.6977136 -1.3995578 0.251688 0.36701393 -0.088927031 -0.77927935 -1.8753138 -2.0748649 -1.7438688 -1.4934614 -1.8555669][-8.5603914 -8.553484 -7.989315 -6.9115925 -5.45028 -3.1965091 -1.5576723 -1.3106892 -1.5244737 -1.9308968 -2.8314409 -2.8923688 -2.4490736 -2.2873392 -2.7729762][-8.47952 -8.62729 -8.3701067 -7.7980442 -6.7113714 -4.6376634 -3.0371087 -2.6190886 -2.7277217 -2.8877938 -3.4940686 -3.4131174 -2.8825996 -2.8274593 -3.5297487][-7.9513149 -8.1122484 -7.9666615 -7.6612911 -6.7691355 -5.0205164 -3.5944591 -3.13269 -3.2211211 -3.4216113 -3.8547692 -3.6600418 -3.1834631 -3.3379567 -4.0908976][-7.3292542 -7.317255 -6.9657373 -6.5551586 -5.69688 -4.4118881 -3.3202052 -2.8983526 -3.0193789 -3.3771567 -3.7106075 -3.4103327 -3.115922 -3.499949 -4.3970823][-7.2442036 -7.0444355 -6.4288321 -5.8245668 -4.8758426 -3.784569 -2.7996073 -2.3127809 -2.3676095 -2.7946324 -3.0482771 -2.8525939 -2.9473195 -3.7031522 -4.7561069]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-32500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-32500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 08:07:35.074978: step 32510, loss = 0.28, batch loss = 0.25 (34.7 examples/sec; 0.231 sec/batch; 19h:13m:49s remains)
INFO - root - 2017-12-15 08:07:37.347493: step 32520, loss = 0.19, batch loss = 0.16 (34.7 examples/sec; 0.231 sec/batch; 19h:12m:35s remains)
INFO - root - 2017-12-15 08:07:39.619871: step 32530, loss = 0.34, batch loss = 0.31 (37.2 examples/sec; 0.215 sec/batch; 17h:53m:56s remains)
INFO - root - 2017-12-15 08:07:41.922137: step 32540, loss = 0.16, batch loss = 0.13 (34.9 examples/sec; 0.229 sec/batch; 19h:06m:20s remains)
INFO - root - 2017-12-15 08:07:44.178934: step 32550, loss = 0.23, batch loss = 0.20 (35.7 examples/sec; 0.224 sec/batch; 18h:39m:44s remains)
INFO - root - 2017-12-15 08:07:46.489729: step 32560, loss = 0.21, batch loss = 0.18 (35.1 examples/sec; 0.228 sec/batch; 18h:57m:51s remains)
INFO - root - 2017-12-15 08:07:48.789859: step 32570, loss = 0.22, batch loss = 0.19 (35.6 examples/sec; 0.225 sec/batch; 18h:43m:49s remains)
INFO - root - 2017-12-15 08:07:51.045360: step 32580, loss = 0.18, batch loss = 0.15 (35.1 examples/sec; 0.228 sec/batch; 19h:00m:35s remains)
INFO - root - 2017-12-15 08:07:53.333976: step 32590, loss = 0.21, batch loss = 0.17 (34.6 examples/sec; 0.231 sec/batch; 19h:15m:39s remains)
INFO - root - 2017-12-15 08:07:55.629202: step 32600, loss = 0.22, batch loss = 0.19 (36.0 examples/sec; 0.222 sec/batch; 18h:29m:14s remains)
2017-12-15 08:07:55.915514: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9019592 -2.6222479 -3.4976058 -4.3634481 -5.5025916 -6.2369061 -5.9800739 -3.3110008 -1.0860987 -1.1206586 -2.2305906 -2.8327405 -2.9881995 -4.1014147 -5.4577732][-3.082648 -3.1230097 -3.9503403 -4.4370494 -5.3264222 -5.6371346 -4.583437 -1.533587 0.5127058 0.28487611 -1.0237544 -2.0846367 -2.6403713 -3.9145665 -5.3612027][-3.4136887 -2.8960602 -3.5586348 -3.7742105 -4.3577671 -4.3385911 -2.9766312 -0.022625208 1.5551775 1.1563575 -0.057969093 -1.4770622 -2.290169 -3.4006267 -4.850225][-3.3788898 -2.5773113 -3.1285036 -3.1321561 -3.4786894 -3.0992775 -1.5242739 1.275106 2.4619718 1.9961276 0.7851696 -1.0524261 -2.1590016 -3.1100628 -4.6210184][-3.1098206 -1.8998331 -2.3609834 -2.5021477 -2.9271746 -2.4469719 -0.96006513 1.5501788 2.5016518 2.0890388 0.92408776 -1.2267781 -2.4757595 -3.1338336 -4.468648][-2.5363164 -1.1383317 -1.3479731 -1.5869546 -2.0086043 -1.3910768 -0.0037224293 2.1321511 2.8211274 2.3662395 1.07744 -1.287848 -2.6396494 -3.047117 -4.10229][-2.9460344 -1.4026136 -1.048732 -0.88473773 -0.78201723 0.31600451 1.8272886 3.5004406 3.82691 3.063756 1.4380276 -1.0115609 -2.4750156 -2.8507829 -3.7200541][-4.2133303 -2.1674476 -1.3101252 -0.83045816 -0.27490187 1.1974466 2.8896866 4.1729851 4.1299548 2.9337053 1.0793073 -1.2358218 -2.6222205 -3.0136039 -3.6551361][-4.77587 -2.6122947 -1.8606721 -1.4472404 -0.7893008 0.88152504 2.7964745 4.053853 3.8177114 2.1398911 0.11254692 -1.9659672 -3.1580806 -3.3439946 -3.578912][-5.1621323 -3.1376266 -2.6069102 -2.2589414 -1.5684128 0.093524933 1.9819145 3.1753664 2.7744532 0.64298153 -1.5255187 -3.2276349 -3.8949804 -3.7339592 -3.581605][-5.3954573 -3.7200422 -3.216033 -2.8374159 -2.3445208 -1.2068352 0.12241793 0.98367929 0.49775505 -1.6212766 -3.5019431 -4.4883833 -4.4662895 -3.9985294 -3.6285233][-5.2694511 -4.0042968 -3.5861611 -3.2387719 -3.013968 -2.5257511 -1.7840195 -1.1813185 -1.5697486 -3.1708136 -4.4908495 -4.7758346 -4.2460265 -3.7484245 -3.4664636][-5.2387791 -4.1933656 -3.8175535 -3.5351243 -3.5278513 -3.315002 -2.889432 -2.5055456 -2.6749306 -3.6270084 -4.4171829 -4.338501 -3.5781634 -3.2934275 -3.2245793][-5.6601038 -4.4389582 -3.9454861 -3.670054 -3.8036408 -3.666085 -3.4179766 -3.3811183 -3.4700346 -3.8301787 -4.2612753 -3.9751573 -3.2080677 -3.2047181 -3.3976114][-6.1680784 -4.4958119 -4.0535874 -3.8565872 -4.0086946 -3.8759651 -3.6284118 -3.9268296 -3.9936666 -3.983325 -4.2272606 -4.0396252 -3.3933876 -3.6430812 -4.0324111]]...]
INFO - root - 2017-12-15 08:07:58.176538: step 32610, loss = 0.28, batch loss = 0.24 (36.1 examples/sec; 0.222 sec/batch; 18h:27m:30s remains)
INFO - root - 2017-12-15 08:08:00.456782: step 32620, loss = 0.21, batch loss = 0.18 (33.9 examples/sec; 0.236 sec/batch; 19h:39m:29s remains)
INFO - root - 2017-12-15 08:08:02.728098: step 32630, loss = 0.17, batch loss = 0.14 (33.2 examples/sec; 0.241 sec/batch; 20h:03m:38s remains)
INFO - root - 2017-12-15 08:08:05.016272: step 32640, loss = 0.29, batch loss = 0.26 (34.9 examples/sec; 0.230 sec/batch; 19h:07m:11s remains)
INFO - root - 2017-12-15 08:08:07.305453: step 32650, loss = 0.22, batch loss = 0.18 (32.5 examples/sec; 0.246 sec/batch; 20h:29m:05s remains)
INFO - root - 2017-12-15 08:08:09.646466: step 32660, loss = 0.20, batch loss = 0.17 (35.1 examples/sec; 0.228 sec/batch; 18h:59m:37s remains)
INFO - root - 2017-12-15 08:08:11.893348: step 32670, loss = 0.26, batch loss = 0.23 (37.8 examples/sec; 0.212 sec/batch; 17h:37m:40s remains)
INFO - root - 2017-12-15 08:08:14.174619: step 32680, loss = 0.25, batch loss = 0.22 (34.4 examples/sec; 0.233 sec/batch; 19h:23m:30s remains)
INFO - root - 2017-12-15 08:08:16.462630: step 32690, loss = 0.32, batch loss = 0.29 (34.5 examples/sec; 0.232 sec/batch; 19h:17m:26s remains)
INFO - root - 2017-12-15 08:08:18.748219: step 32700, loss = 0.28, batch loss = 0.25 (34.3 examples/sec; 0.233 sec/batch; 19h:24m:27s remains)
2017-12-15 08:08:19.064411: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6624613 -8.1625462 -9.2920418 -9.4149361 -8.679841 -7.4559431 -6.0523915 -5.2233434 -4.866991 -4.8443308 -5.4106574 -5.1894789 -4.5166755 -4.4763241 -5.0950775][-5.3914132 -8.58445 -9.6752 -9.74687 -9.1123743 -7.7476435 -6.1282167 -5.1098294 -4.5782804 -4.4559631 -4.9518232 -4.4856844 -3.3793578 -3.0707884 -3.6183128][-5.4750338 -7.7151604 -8.5933485 -8.548336 -8.037899 -6.6859426 -5.1062307 -4.1396556 -3.5878968 -3.500689 -3.8830256 -3.2404346 -1.8814049 -1.4554474 -1.8135625][-5.5795212 -6.90903 -7.6229067 -7.4588041 -6.9182987 -5.379591 -3.7168345 -2.664269 -2.1378908 -2.2049119 -2.5077827 -1.7960536 -0.604319 -0.42185533 -0.71902227][-6.015811 -6.54679 -7.0591049 -6.6462631 -5.8514218 -4.0594745 -2.1002259 -0.91567647 -0.535933 -0.93418896 -1.434988 -1.053827 -0.3485148 -0.47076428 -0.703807][-6.01533 -6.1210594 -6.259181 -5.3804145 -4.1967726 -2.0798595 0.12964964 1.2921669 1.2903349 0.35843754 -0.51081538 -0.69202733 -0.70048034 -1.2687699 -1.5113769][-5.3832016 -5.543211 -5.3285789 -4.0020232 -2.4612479 -0.012849569 2.4370253 3.6031072 3.0690024 1.5301075 -0.0060813427 -0.994416 -1.9008306 -2.9650273 -3.3122649][-5.0201545 -5.2575326 -4.9267836 -3.4563861 -1.7536669 0.88777995 3.599622 4.7456255 3.7733877 1.769891 -0.27490985 -1.9856009 -3.5907369 -4.7948532 -5.2821903][-4.8633 -5.2083817 -4.938817 -3.5791492 -2.0450761 0.4911418 3.1425183 4.1892309 3.115948 1.1561246 -1.0609825 -3.1818829 -4.8758755 -5.7435274 -6.2561588][-4.74996 -5.0915341 -4.9118748 -3.786427 -2.5684891 -0.44942307 1.8265235 2.7105243 1.8450348 0.10130763 -2.17808 -4.5022621 -5.858058 -6.3525572 -6.9035368][-4.5518017 -4.8954268 -4.8635068 -3.9768496 -3.0778377 -1.4730117 0.32757449 1.1480246 0.52307343 -1.0516676 -3.4533665 -5.84198 -6.8788595 -7.212635 -7.6969919][-4.2893648 -4.6376505 -4.7819214 -4.1363792 -3.6721287 -2.8299737 -1.6101955 -0.89447272 -1.3429048 -2.7342358 -4.9872665 -6.9798737 -7.6863966 -7.988513 -8.2116594][-4.0106764 -4.2904863 -4.5405722 -4.1023445 -4.2094836 -4.24963 -3.6823244 -3.2284851 -3.6091819 -4.6053667 -6.2675867 -7.479372 -7.8001223 -8.033021 -8.1334839][-3.8826611 -4.0335703 -4.2725534 -3.9691932 -4.5748787 -5.2852354 -5.2099972 -4.9824457 -5.3168249 -5.6974387 -6.4333754 -6.8596973 -6.923214 -7.1464996 -7.4598064][-3.9390988 -4.0107918 -4.2152872 -4.0209579 -4.9508772 -5.9182839 -6.0553308 -5.8429422 -5.8481469 -5.4231844 -5.2767482 -5.2292223 -5.3129821 -5.6920729 -6.5343971]]...]
INFO - root - 2017-12-15 08:08:21.346907: step 32710, loss = 0.18, batch loss = 0.15 (35.8 examples/sec; 0.223 sec/batch; 18h:36m:01s remains)
INFO - root - 2017-12-15 08:08:23.626583: step 32720, loss = 0.38, batch loss = 0.35 (35.2 examples/sec; 0.227 sec/batch; 18h:55m:59s remains)
INFO - root - 2017-12-15 08:08:25.915296: step 32730, loss = 0.20, batch loss = 0.17 (34.6 examples/sec; 0.231 sec/batch; 19h:14m:50s remains)
INFO - root - 2017-12-15 08:08:28.190298: step 32740, loss = 0.20, batch loss = 0.17 (34.2 examples/sec; 0.234 sec/batch; 19h:29m:18s remains)
INFO - root - 2017-12-15 08:08:30.468360: step 32750, loss = 0.25, batch loss = 0.22 (34.5 examples/sec; 0.232 sec/batch; 19h:17m:51s remains)
INFO - root - 2017-12-15 08:08:32.735144: step 32760, loss = 0.29, batch loss = 0.25 (35.3 examples/sec; 0.227 sec/batch; 18h:52m:34s remains)
INFO - root - 2017-12-15 08:08:34.999945: step 32770, loss = 0.29, batch loss = 0.25 (34.7 examples/sec; 0.231 sec/batch; 19h:13m:05s remains)
INFO - root - 2017-12-15 08:08:37.301811: step 32780, loss = 0.22, batch loss = 0.19 (33.8 examples/sec; 0.237 sec/batch; 19h:42m:04s remains)
INFO - root - 2017-12-15 08:08:39.554120: step 32790, loss = 0.17, batch loss = 0.14 (35.7 examples/sec; 0.224 sec/batch; 18h:37m:50s remains)
INFO - root - 2017-12-15 08:08:41.838905: step 32800, loss = 0.27, batch loss = 0.24 (35.6 examples/sec; 0.225 sec/batch; 18h:42m:19s remains)
2017-12-15 08:08:42.104715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3797331 -3.170464 -3.3433623 -3.6020722 -3.4244153 -3.150182 -2.9323649 -2.6525002 -1.9609077 -1.2959523 -1.4003884 -1.9422309 -2.5479529 -2.875962 -2.8956914][-2.666539 -3.3556433 -3.5716796 -3.9113545 -3.8065405 -3.4310031 -3.085839 -3.0101774 -2.4440873 -1.4881234 -1.3666682 -1.8765862 -2.504076 -2.9685977 -2.9274154][-3.6238828 -3.3647494 -3.5418382 -3.7876801 -3.5872545 -3.0122676 -2.6853969 -3.0754795 -2.9674907 -2.0355196 -1.7119541 -2.0442557 -2.5494416 -2.9759467 -2.9023297][-5.0089655 -3.8462877 -3.7824554 -3.4551883 -2.6539385 -1.6886243 -1.2729071 -2.1287684 -2.7135596 -2.2208753 -1.9968147 -2.384388 -2.7974224 -2.9736993 -2.7516198][-5.622551 -3.8691044 -3.4851437 -2.4939137 -0.92373109 0.62215376 1.341877 0.14726448 -1.212817 -1.5630412 -1.9231638 -2.6205831 -2.9724674 -2.8610206 -2.3982306][-4.8709626 -3.0553908 -2.4791651 -0.86089957 1.5219102 3.6318657 4.5512524 3.1021287 0.98864889 -0.52009487 -1.6645005 -2.7402067 -3.1118925 -2.7554233 -2.1684549][-4.2403641 -2.4876585 -1.4611297 0.70319867 3.5930617 5.88824 6.8488245 5.3597775 2.8762858 0.43490076 -1.5473133 -3.0594862 -3.449028 -2.9477322 -2.2044327][-3.8983798 -2.2822902 -1.0646569 1.0781293 3.876138 6.1554508 7.2481327 6.0213375 3.6059921 0.63899994 -2.060111 -3.8647933 -4.2204247 -3.6700115 -2.7371483][-3.7710342 -2.7818427 -1.9855158 -0.33476233 2.0919278 4.3267975 5.7061749 5.1522827 3.3840997 0.64295793 -2.2334304 -4.08412 -4.5494204 -4.4062309 -3.7782502][-4.4491534 -4.1676865 -3.8969727 -2.8713078 -0.96981263 1.1022999 2.5675428 2.5665977 1.7473919 -0.1701901 -2.6807315 -4.220644 -4.6948533 -4.8771315 -4.5796986][-5.4571371 -5.8610859 -6.1315184 -5.6616697 -4.2446504 -2.552247 -1.2293346 -0.85598791 -0.95524085 -2.0387015 -3.961174 -5.0053325 -5.378068 -5.7560196 -5.7689648][-6.5853443 -7.4865789 -8.1269255 -7.9285984 -6.9021358 -5.6199207 -4.5262423 -3.9460337 -3.5591865 -4.0130167 -5.4064045 -6.1850519 -6.6421118 -7.2210693 -7.4450779][-7.1902561 -8.1917143 -8.7951946 -8.600421 -7.9088631 -7.1510725 -6.4417944 -5.8966966 -5.4072008 -5.6132832 -6.5904646 -7.1962576 -7.7157764 -8.2520351 -8.3353529][-7.2724209 -8.0366068 -8.4169722 -8.1448984 -7.7283516 -7.4680328 -7.208025 -6.9381609 -6.6312237 -6.7762413 -7.3483887 -7.7113152 -8.116745 -8.406888 -8.2585926][-7.4660549 -7.8348036 -7.97684 -7.6916876 -7.4498711 -7.4663134 -7.5174837 -7.5548763 -7.562542 -7.7515564 -7.999773 -8.0780115 -8.1877918 -8.0675268 -7.6185846]]...]
INFO - root - 2017-12-15 08:08:44.389503: step 32810, loss = 0.21, batch loss = 0.18 (35.2 examples/sec; 0.228 sec/batch; 18h:56m:35s remains)
INFO - root - 2017-12-15 08:08:46.670076: step 32820, loss = 0.18, batch loss = 0.15 (36.0 examples/sec; 0.222 sec/batch; 18h:30m:01s remains)
INFO - root - 2017-12-15 08:08:48.982532: step 32830, loss = 0.27, batch loss = 0.23 (34.9 examples/sec; 0.229 sec/batch; 19h:06m:07s remains)
INFO - root - 2017-12-15 08:08:51.278808: step 32840, loss = 0.30, batch loss = 0.26 (35.6 examples/sec; 0.225 sec/batch; 18h:42m:42s remains)
INFO - root - 2017-12-15 08:08:53.529643: step 32850, loss = 0.21, batch loss = 0.18 (35.0 examples/sec; 0.228 sec/batch; 19h:01m:04s remains)
INFO - root - 2017-12-15 08:08:55.823213: step 32860, loss = 0.23, batch loss = 0.20 (35.0 examples/sec; 0.229 sec/batch; 19h:02m:00s remains)
INFO - root - 2017-12-15 08:08:58.077080: step 32870, loss = 0.23, batch loss = 0.20 (34.7 examples/sec; 0.230 sec/batch; 19h:10m:37s remains)
INFO - root - 2017-12-15 08:09:00.375606: step 32880, loss = 0.35, batch loss = 0.32 (33.3 examples/sec; 0.240 sec/batch; 19h:58m:57s remains)
INFO - root - 2017-12-15 08:09:02.644470: step 32890, loss = 0.19, batch loss = 0.16 (35.5 examples/sec; 0.226 sec/batch; 18h:46m:26s remains)
INFO - root - 2017-12-15 08:09:04.910789: step 32900, loss = 0.23, batch loss = 0.20 (35.5 examples/sec; 0.225 sec/batch; 18h:45m:32s remains)
2017-12-15 08:09:05.198977: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9839067 -8.538415 -9.1504145 -9.0773735 -9.1051073 -9.3208179 -9.2383928 -8.75962 -7.3466873 -6.0869689 -6.1010647 -7.1265154 -8.6753931 -10.362577 -10.842463][-5.1376896 -7.53168 -8.1870975 -8.2221031 -8.326005 -8.5972691 -8.6783085 -8.4881363 -7.316433 -6.2195253 -6.3074741 -7.1270084 -8.5746222 -10.35659 -10.926661][-4.4639621 -5.637145 -6.2332382 -6.3709192 -6.4695072 -6.7019911 -6.7935991 -6.7265024 -5.92208 -5.3876739 -5.9378939 -6.7324734 -7.9589949 -9.6894054 -10.447444][-4.053463 -4.1535544 -4.653348 -4.7798204 -4.6014357 -4.5897579 -4.4102621 -4.1202431 -3.6657271 -3.9695616 -5.1783123 -6.1793823 -7.1555262 -8.7834988 -9.6819048][-3.6271915 -2.8537395 -3.1203561 -3.0431924 -2.2374232 -1.6547303 -0.71479523 0.2590642 0.56817317 -0.69079781 -2.8586681 -4.5415773 -5.7750883 -7.5139875 -8.7146263][-3.3833199 -2.2248724 -2.2548406 -1.8994293 -0.38465774 0.88099885 2.7770293 4.706233 5.1385851 2.9538896 -0.051193 -2.437608 -4.23526 -6.174325 -7.8643427][-3.7647991 -2.7250428 -2.5822654 -2.0779984 -0.059080362 1.8445418 4.6280403 7.5361013 8.4263744 5.63702 1.9137595 -1.0455425 -3.3426695 -5.3793774 -7.2536631][-5.3141856 -4.1973634 -4.02806 -3.6247687 -1.6227319 0.55314636 3.8570926 7.5540085 9.1120529 6.3286171 2.3758123 -0.67236495 -3.1071415 -5.0718803 -7.0308733][-6.6042862 -5.4086456 -5.37232 -5.3797 -3.9739056 -2.2104571 0.80434346 4.455699 6.3492146 4.1259413 0.821924 -1.5665269 -3.6733317 -5.4197745 -7.3373404][-7.9826021 -6.7903833 -6.8043861 -7.0185242 -6.1725507 -4.978446 -2.8370636 -0.020685196 1.7917078 0.37432027 -2.0423326 -3.7809691 -5.5357761 -7.10052 -8.5122786][-8.8019915 -7.78325 -7.8239708 -8.0196629 -7.53277 -6.92453 -5.7371769 -3.8906927 -2.4713886 -3.2767487 -5.0234714 -6.3816195 -7.8107662 -9.2160559 -9.9940481][-9.0303135 -8.1654892 -8.2245045 -8.3653049 -8.1213169 -7.9167805 -7.4219704 -6.4021435 -5.4282465 -5.9202657 -7.1635666 -8.20737 -9.2661591 -10.406156 -10.815657][-8.7798243 -8.0036325 -8.0184069 -8.11609 -8.11282 -8.1617384 -8.039278 -7.5617609 -6.9054651 -7.1120129 -7.9304733 -8.6892433 -9.4851332 -10.583625 -10.97237][-8.28434 -7.5270319 -7.5253811 -7.6027832 -7.7306128 -7.9164953 -8.0137148 -7.8983822 -7.5444193 -7.6345263 -8.163413 -8.7308254 -9.332983 -10.244905 -10.58567][-8.0422153 -7.1277561 -7.1049786 -7.1459074 -7.2552223 -7.4078255 -7.5483255 -7.6130152 -7.4498482 -7.5515137 -7.9401922 -8.380619 -8.7905169 -9.4258 -9.7143984]]...]
INFO - root - 2017-12-15 08:09:07.442646: step 32910, loss = 0.26, batch loss = 0.23 (34.3 examples/sec; 0.233 sec/batch; 19h:25m:19s remains)
INFO - root - 2017-12-15 08:09:09.768983: step 32920, loss = 0.28, batch loss = 0.25 (33.8 examples/sec; 0.237 sec/batch; 19h:43m:10s remains)
INFO - root - 2017-12-15 08:09:12.084306: step 32930, loss = 0.20, batch loss = 0.17 (33.9 examples/sec; 0.236 sec/batch; 19h:36m:48s remains)
INFO - root - 2017-12-15 08:09:14.349087: step 32940, loss = 0.22, batch loss = 0.19 (35.0 examples/sec; 0.228 sec/batch; 19h:00m:10s remains)
INFO - root - 2017-12-15 08:09:16.608440: step 32950, loss = 0.17, batch loss = 0.14 (34.8 examples/sec; 0.230 sec/batch; 19h:08m:58s remains)
INFO - root - 2017-12-15 08:09:18.849139: step 32960, loss = 0.17, batch loss = 0.14 (35.1 examples/sec; 0.228 sec/batch; 18h:56m:19s remains)
INFO - root - 2017-12-15 08:09:21.116979: step 32970, loss = 0.27, batch loss = 0.24 (34.3 examples/sec; 0.233 sec/batch; 19h:25m:09s remains)
INFO - root - 2017-12-15 08:09:23.433331: step 32980, loss = 0.24, batch loss = 0.21 (32.3 examples/sec; 0.248 sec/batch; 20h:37m:49s remains)
INFO - root - 2017-12-15 08:09:25.750678: step 32990, loss = 0.28, batch loss = 0.24 (34.9 examples/sec; 0.229 sec/batch; 19h:03m:22s remains)
INFO - root - 2017-12-15 08:09:28.029403: step 33000, loss = 0.27, batch loss = 0.24 (36.2 examples/sec; 0.221 sec/batch; 18h:23m:43s remains)
2017-12-15 08:09:28.317769: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7489405 -2.0949147 -2.4203589 -2.4011569 -2.2626767 -2.3802152 -2.90905 -3.2858074 -3.3059604 -3.3366475 -3.4456389 -3.9238453 -4.808075 -4.7117014 -3.8520789][-2.2004821 -1.9906672 -2.6737173 -3.0934963 -3.1769395 -3.1432986 -3.4659345 -3.9458818 -4.3728161 -4.7168379 -4.8478565 -5.10611 -5.6423192 -5.3585958 -4.1980634][-2.7465787 -1.9920073 -2.9956682 -3.8300633 -3.9095285 -3.4218063 -3.1113033 -3.4129343 -4.1314 -4.7559314 -4.9492121 -5.0559454 -5.3055792 -5.0867181 -4.04341][-3.2572803 -1.8961023 -2.933223 -3.9263856 -3.8634157 -2.7240515 -1.5364716 -1.5139709 -2.50509 -3.5988121 -4.1882706 -4.3547211 -4.3932838 -4.3935137 -3.6441951][-3.7246866 -1.9560989 -2.867743 -3.7250428 -3.387569 -1.6706897 0.28769684 0.65207767 -0.352939 -1.7125192 -2.7751365 -3.2095037 -3.3169339 -3.6457253 -3.3730276][-3.9549708 -1.6809089 -2.1539435 -2.7687483 -2.2138245 -0.1583941 2.3122923 2.9708951 2.0280917 0.46295428 -1.131112 -1.9724349 -2.3859117 -2.9838715 -3.1123974][-4.23773 -1.7232012 -1.4925388 -1.7202001 -1.1716262 0.69892645 3.0656002 3.7634723 2.7950232 1.1408145 -0.71326113 -1.8530765 -2.5270922 -3.0704529 -3.1382148][-4.9684086 -2.1033602 -1.2383512 -1.1488655 -0.91659439 0.26089311 2.0146773 2.5942943 1.7592041 0.23984408 -1.7143319 -2.9233439 -3.56222 -3.8535213 -3.5734761][-5.7269058 -2.6985781 -1.4716393 -1.1796243 -1.2152352 -0.88480639 0.010223389 0.43385983 -0.14620495 -1.2963817 -2.9850245 -4.1589546 -4.7717695 -4.9621358 -4.5516081][-6.02444 -2.9329543 -1.8509552 -1.5013171 -1.7205577 -2.1610096 -2.0910895 -1.7664078 -1.9769143 -2.65154 -3.9498599 -5.0673056 -5.6495042 -5.8037558 -5.3749094][-5.4752493 -2.6279449 -2.2761674 -2.1359813 -2.4558794 -3.3970008 -3.9788232 -3.7899382 -3.630446 -3.8139842 -4.6064014 -5.3771129 -5.7917342 -5.9694042 -5.5318584][-5.0885477 -2.5702307 -2.9235759 -3.0604515 -3.423136 -4.6347437 -5.5165672 -5.25243 -4.5883093 -4.23682 -4.5611243 -4.9905758 -5.2185879 -5.4305105 -4.9752808][-4.3241353 -2.0966721 -2.9192023 -3.2697635 -3.7690687 -5.2533255 -6.4393072 -6.1767054 -5.055933 -4.304935 -4.2440982 -4.3172636 -4.3869805 -4.7380762 -4.3276558][-3.9923868 -1.8961667 -2.6488419 -2.8309066 -3.3601272 -5.1879621 -6.731493 -6.591898 -5.2558527 -4.1932783 -3.9060469 -3.7415709 -3.7736855 -4.26037 -3.7515533][-4.041482 -1.9585755 -2.350805 -2.1190357 -2.4125702 -4.3022356 -6.0770416 -6.2339396 -5.0700312 -3.957603 -3.53863 -3.2997861 -3.3078382 -3.6822336 -2.9922857]]...]
INFO - root - 2017-12-15 08:09:30.603734: step 33010, loss = 0.24, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 18h:40m:01s remains)
INFO - root - 2017-12-15 08:09:32.895820: step 33020, loss = 0.23, batch loss = 0.20 (35.3 examples/sec; 0.227 sec/batch; 18h:50m:34s remains)
INFO - root - 2017-12-15 08:09:35.147610: step 33030, loss = 0.17, batch loss = 0.14 (34.9 examples/sec; 0.229 sec/batch; 19h:02m:51s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 08:09:37.424318: step 33040, loss = 0.21, batch loss = 0.18 (36.1 examples/sec; 0.221 sec/batch; 18h:24m:37s remains)
INFO - root - 2017-12-15 08:09:39.700197: step 33050, loss = 0.18, batch loss = 0.14 (34.0 examples/sec; 0.235 sec/batch; 19h:35m:18s remains)
INFO - root - 2017-12-15 08:09:41.972653: step 33060, loss = 0.23, batch loss = 0.20 (35.5 examples/sec; 0.226 sec/batch; 18h:45m:36s remains)
INFO - root - 2017-12-15 08:09:44.252568: step 33070, loss = 0.22, batch loss = 0.19 (34.1 examples/sec; 0.235 sec/batch; 19h:32m:18s remains)
INFO - root - 2017-12-15 08:09:46.526857: step 33080, loss = 0.20, batch loss = 0.17 (35.5 examples/sec; 0.226 sec/batch; 18h:45m:59s remains)
INFO - root - 2017-12-15 08:09:48.783677: step 33090, loss = 0.24, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 18h:38m:27s remains)
INFO - root - 2017-12-15 08:09:51.024726: step 33100, loss = 0.22, batch loss = 0.19 (35.2 examples/sec; 0.227 sec/batch; 18h:54m:10s remains)
2017-12-15 08:09:51.312917: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6522846 -4.7642584 -4.2544527 -3.9086492 -3.8205576 -4.3546538 -4.4388514 -3.6070673 -2.6469648 -1.7679027 -1.9249132 -2.6907244 -3.5280242 -3.9410052 -4.0614548][-4.7590513 -3.9664054 -3.376487 -3.3274527 -3.3371987 -3.6583004 -3.5607457 -2.4732895 -1.3543029 -0.53147662 -1.0953587 -2.1509748 -3.0440114 -3.429395 -3.3728049][-4.7254581 -2.9083071 -2.4703808 -2.8142934 -2.8457122 -2.9608216 -2.5913029 -1.2346869 -0.0026185513 0.59847 -0.18988395 -1.2864394 -2.0388422 -2.3335721 -2.1607566][-4.3700595 -1.7159922 -1.4276803 -2.0076771 -2.0201464 -2.1729338 -1.7319639 -0.32728839 0.78244615 0.97430778 0.071784735 -0.82799053 -1.2930415 -1.4139974 -1.1741457][-4.3961535 -0.88462663 -0.53004766 -1.0199999 -1.0273814 -1.3800371 -0.98286784 0.31989789 1.0974991 1.0338247 0.27497888 -0.43602633 -0.76469946 -0.98637378 -0.95767963][-4.1180429 -0.68176222 -0.19063067 -0.38842297 -0.35630178 -0.67662978 -0.1198597 0.86536407 1.2128675 1.0946233 0.57986879 -0.10460472 -0.48605967 -0.83803511 -0.96922791][-3.4404573 -0.80363381 -0.47985542 -0.66815829 -0.68061078 -0.67997646 0.16568685 0.9594605 1.1094425 1.0518465 0.66213894 0.017983675 -0.42109311 -0.75374568 -0.86028361][-3.3113475 -0.74069989 -0.66680229 -1.1658911 -1.5002328 -1.3943487 -0.41620231 0.39478421 0.79104948 1.0394616 0.80260658 0.13456059 -0.45572662 -0.87782347 -1.0418104][-3.2132528 -0.56115687 -0.41977334 -0.964326 -1.6360317 -1.8281969 -1.2085665 -0.5891664 0.024742603 0.52959156 0.40854025 -0.17073083 -0.7636627 -1.2014208 -1.2976487][-3.1405339 -0.37036693 -0.016402721 -0.32457352 -1.0888946 -1.7023661 -1.6432283 -1.4380984 -0.91712439 -0.47170997 -0.62107337 -0.93230236 -1.263643 -1.4844847 -1.35475][-2.6710355 0.20602822 0.74800229 0.70075226 -0.11847711 -1.0498798 -1.444171 -1.7161705 -1.5023711 -1.2594606 -1.3871782 -1.4725592 -1.5240045 -1.5557292 -1.163767][-2.1043377 0.98093438 1.7988963 2.0395265 1.25224 0.017887354 -0.83858871 -1.6440767 -2.0063441 -2.0456042 -2.0476336 -2.0016315 -1.9701135 -2.0045474 -1.4069967][-2.2216506 0.9238 2.0263839 2.5964584 2.0394759 0.75871277 -0.37550104 -1.6223395 -2.5598505 -2.9278991 -2.8397274 -2.67007 -2.58424 -2.6399636 -1.8663363][-3.2138717 -0.3891108 0.73383355 1.5967548 1.3833766 0.4378016 -0.56918418 -1.8833495 -3.0984118 -3.602828 -3.4419789 -3.1489215 -2.8467331 -2.8677828 -2.0109539][-4.40446 -1.9384261 -0.97712946 -0.092838287 -0.12813807 -0.6813513 -1.2837769 -2.2608442 -3.349391 -3.8075986 -3.6117551 -3.2672286 -2.7957683 -2.7683358 -1.9615314]]...]
INFO - root - 2017-12-15 08:09:53.579743: step 33110, loss = 0.33, batch loss = 0.30 (35.0 examples/sec; 0.229 sec/batch; 19h:01m:06s remains)
INFO - root - 2017-12-15 08:09:55.858065: step 33120, loss = 0.24, batch loss = 0.21 (33.8 examples/sec; 0.237 sec/batch; 19h:40m:40s remains)
INFO - root - 2017-12-15 08:09:58.131302: step 33130, loss = 0.23, batch loss = 0.19 (35.7 examples/sec; 0.224 sec/batch; 18h:36m:47s remains)
INFO - root - 2017-12-15 08:10:00.417498: step 33140, loss = 0.25, batch loss = 0.22 (35.9 examples/sec; 0.223 sec/batch; 18h:30m:24s remains)
INFO - root - 2017-12-15 08:10:02.683616: step 33150, loss = 0.27, batch loss = 0.24 (35.1 examples/sec; 0.228 sec/batch; 18h:56m:10s remains)
INFO - root - 2017-12-15 08:10:04.962370: step 33160, loss = 0.30, batch loss = 0.26 (34.5 examples/sec; 0.232 sec/batch; 19h:16m:23s remains)
INFO - root - 2017-12-15 08:10:07.258178: step 33170, loss = 0.22, batch loss = 0.19 (34.2 examples/sec; 0.234 sec/batch; 19h:28m:29s remains)
INFO - root - 2017-12-15 08:10:09.540417: step 33180, loss = 0.30, batch loss = 0.27 (36.0 examples/sec; 0.222 sec/batch; 18h:29m:57s remains)
INFO - root - 2017-12-15 08:10:11.782503: step 33190, loss = 0.23, batch loss = 0.19 (36.3 examples/sec; 0.220 sec/batch; 18h:18m:49s remains)
INFO - root - 2017-12-15 08:10:14.027820: step 33200, loss = 0.19, batch loss = 0.15 (35.9 examples/sec; 0.223 sec/batch; 18h:30m:46s remains)
2017-12-15 08:10:14.329075: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7585187 -5.1493816 -5.3662968 -5.4874768 -5.6002321 -5.65376 -5.5850692 -5.5477715 -5.6082258 -5.5926914 -5.5442233 -5.4134221 -5.1978168 -4.9345665 -4.63853][-4.8205705 -5.7388268 -5.95578 -6.0306988 -6.0055809 -5.8259106 -5.610076 -5.619523 -5.8093987 -5.9907284 -6.1924267 -6.2102332 -5.9684191 -5.5960894 -5.1237321][-5.7848444 -6.2228451 -6.4250379 -6.4784627 -6.2794213 -5.7859354 -5.3948193 -5.3687725 -5.6190877 -6.0131311 -6.5425982 -6.8054209 -6.56938 -6.111012 -5.6061821][-6.7343297 -6.5394969 -6.8116455 -6.9088049 -6.46359 -5.5017514 -4.8206739 -4.6812696 -4.9593916 -5.597074 -6.5220175 -7.1047935 -6.961483 -6.4597683 -5.9569135][-6.5395246 -6.3928819 -6.9383354 -7.1572633 -6.4993191 -5.1883173 -4.3402586 -4.0172558 -4.3458395 -5.2149463 -6.3859839 -7.2647371 -7.2606378 -6.61021 -6.0607872][-6.223629 -6.1337194 -6.7428145 -6.7162728 -5.6472588 -3.9578276 -2.6647537 -1.897925 -2.3425412 -3.5984068 -5.1222773 -6.392189 -6.9268436 -6.554122 -6.0285363][-5.1706018 -5.3810196 -5.7319908 -5.2895594 -3.91067 -1.9050934 0.038663149 1.5328062 0.9949584 -0.847041 -2.800395 -4.4109674 -5.4741182 -5.6582479 -5.5300684][-4.1621566 -4.097023 -4.2304459 -3.7367811 -2.30434 -0.061525822 2.6190877 4.6593151 4.0238943 1.7261651 -0.61211276 -2.2897506 -3.6023409 -4.2905455 -4.3574886][-3.1669352 -2.9708457 -3.124059 -2.8643615 -1.3818729 1.3675184 4.5403824 6.6047578 5.8486438 3.321713 0.73021793 -0.90253711 -1.9577191 -2.6341217 -2.947433][-2.8977661 -2.8590372 -3.3967481 -3.7232513 -2.429841 0.51984882 3.7309422 5.5374637 4.9212952 2.7341542 0.29613972 -1.1024791 -1.6291449 -1.8447195 -1.9788527][-3.6495948 -3.7453465 -4.6711984 -5.4395218 -4.3375916 -1.1841241 1.8955994 3.4439921 3.0613074 1.1319623 -1.1922957 -2.4959931 -2.492594 -1.9642097 -1.7073307][-4.4156036 -4.6895709 -5.9220772 -6.8722906 -5.9332647 -3.1096506 -0.47519898 0.78422308 0.52506685 -1.0554504 -2.8996232 -3.8062911 -3.5033944 -2.5855918 -2.0099514][-5.1099753 -5.6827364 -7.1703572 -8.0505219 -7.26526 -4.9929562 -2.7835162 -1.7242098 -1.8175317 -3.1328242 -4.4711504 -5.010406 -4.4860687 -3.2549396 -2.226579][-5.2612906 -6.0222979 -7.4879451 -8.0771446 -7.425118 -6.0062304 -4.4635777 -3.8120944 -4.0315719 -5.1479454 -6.0758619 -6.1298542 -5.4095535 -4.2742171 -3.2072718][-5.1632538 -6.0989933 -7.5529051 -8.0379982 -7.7826958 -7.20125 -6.3199472 -6.0114775 -6.1626406 -6.8372822 -7.2507558 -7.0336332 -6.2964063 -5.3320713 -4.4360285]]...]
INFO - root - 2017-12-15 08:10:16.638642: step 33210, loss = 0.15, batch loss = 0.12 (33.1 examples/sec; 0.242 sec/batch; 20h:07m:19s remains)
INFO - root - 2017-12-15 08:10:18.917594: step 33220, loss = 0.17, batch loss = 0.14 (34.4 examples/sec; 0.233 sec/batch; 19h:20m:00s remains)
INFO - root - 2017-12-15 08:10:21.206927: step 33230, loss = 0.34, batch loss = 0.30 (35.2 examples/sec; 0.228 sec/batch; 18h:55m:08s remains)
INFO - root - 2017-12-15 08:10:23.483318: step 33240, loss = 0.22, batch loss = 0.19 (32.3 examples/sec; 0.248 sec/batch; 20h:34m:54s remains)
INFO - root - 2017-12-15 08:10:25.799660: step 33250, loss = 0.23, batch loss = 0.20 (34.7 examples/sec; 0.231 sec/batch; 19h:09m:55s remains)
INFO - root - 2017-12-15 08:10:28.039944: step 33260, loss = 0.16, batch loss = 0.13 (35.9 examples/sec; 0.223 sec/batch; 18h:29m:51s remains)
INFO - root - 2017-12-15 08:10:30.306958: step 33270, loss = 0.21, batch loss = 0.18 (35.1 examples/sec; 0.228 sec/batch; 18h:55m:36s remains)
INFO - root - 2017-12-15 08:10:32.570152: step 33280, loss = 0.27, batch loss = 0.24 (35.5 examples/sec; 0.225 sec/batch; 18h:44m:29s remains)
INFO - root - 2017-12-15 08:10:34.828826: step 33290, loss = 0.24, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 18h:36m:18s remains)
INFO - root - 2017-12-15 08:10:37.081196: step 33300, loss = 0.22, batch loss = 0.19 (35.1 examples/sec; 0.228 sec/batch; 18h:56m:20s remains)
2017-12-15 08:10:37.348565: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.227324 -6.9586716 -7.3074226 -7.3746281 -6.9448557 -6.4794159 -5.9031749 -5.500536 -5.8007936 -6.8040762 -7.6200714 -8.2655334 -8.2531538 -7.4053621 -6.1997719][-7.4619732 -8.3890591 -8.6317348 -8.4150791 -7.5726166 -6.6961613 -5.7126932 -4.9229312 -5.0308456 -6.15224 -7.3412189 -8.4405756 -8.70057 -7.921524 -6.7312384][-8.3030834 -8.2440882 -8.4076767 -8.0884972 -6.9849825 -5.8044915 -4.5583916 -3.4501286 -3.4782915 -4.770113 -6.3506718 -7.8750296 -8.4590139 -7.9622707 -6.8497581][-8.0428162 -7.0229416 -7.131237 -6.8374987 -5.6174431 -4.362843 -2.9657724 -1.62785 -1.7571484 -3.3486137 -5.3528118 -7.222271 -8.0963163 -7.9225931 -6.9672232][-7.8629436 -6.1280637 -6.1975846 -5.7331257 -4.1834774 -2.6306996 -0.90182781 0.64553976 0.26598692 -1.7315093 -4.2859359 -6.6159792 -7.7868652 -7.9422455 -7.10658][-7.1509933 -5.2697768 -5.1382055 -4.3748069 -2.5188427 -0.67411339 1.4696789 3.1129973 2.3650606 -0.087676525 -3.259897 -6.1339474 -7.6643734 -8.146946 -7.3914967][-5.2924089 -3.848577 -3.4383187 -2.2835331 -0.092508078 2.0709689 4.5161228 6.011713 4.6858692 1.5844414 -2.3184352 -5.7450609 -7.6695929 -8.352396 -7.63414][-4.6695471 -3.2533047 -2.6505399 -1.2176894 1.0874717 3.4215171 6.0411263 7.3726339 5.7315845 2.4476917 -1.7085977 -5.3266764 -7.4284239 -8.0765018 -7.3861942][-4.92269 -3.6307888 -2.9715865 -1.4498088 0.72023845 2.8748558 5.3772087 6.656661 5.186801 2.4173663 -1.2900144 -4.7305846 -6.7798548 -7.3708925 -6.8248549][-4.5311189 -3.5093341 -3.0685072 -1.8915842 -0.2339921 1.4831538 3.7165277 5.0598221 4.0577459 2.079704 -1.0242746 -4.1397648 -6.0978274 -6.6741829 -6.3023129][-4.23325 -3.3031673 -3.0650496 -2.2306528 -1.2013971 -0.091263533 1.6526785 2.8351195 2.256377 0.86973548 -1.6230245 -4.2787409 -5.9369526 -6.3521681 -5.9756608][-4.6362667 -3.5671823 -3.3235986 -2.6403017 -1.9275357 -1.2808897 -0.24007988 0.47303128 0.012530088 -1.1050136 -3.08328 -5.2124624 -6.3725691 -6.4083796 -5.8247528][-5.3152452 -4.2031121 -3.9301667 -3.2693489 -2.5818951 -2.0918622 -1.386673 -0.88868821 -1.2721487 -2.3670154 -4.0945668 -5.9880753 -6.86217 -6.65862 -5.9423752][-6.1545448 -5.0815392 -4.8586745 -4.2865582 -3.515501 -3.0380833 -2.5071716 -2.0297227 -2.2206893 -3.2055531 -4.71128 -6.35326 -7.0350714 -6.767705 -6.104722][-6.4951706 -5.6100645 -5.7016039 -5.4571328 -4.8356447 -4.4465065 -4.000205 -3.3101573 -3.2393405 -3.990324 -5.2144918 -6.5340538 -7.0231752 -6.7056637 -6.1585083]]...]
INFO - root - 2017-12-15 08:10:39.637838: step 33310, loss = 0.22, batch loss = 0.19 (36.0 examples/sec; 0.222 sec/batch; 18h:26m:56s remains)
INFO - root - 2017-12-15 08:10:41.882162: step 33320, loss = 0.25, batch loss = 0.21 (35.8 examples/sec; 0.223 sec/batch; 18h:33m:01s remains)
INFO - root - 2017-12-15 08:10:44.172170: step 33330, loss = 0.19, batch loss = 0.16 (34.1 examples/sec; 0.235 sec/batch; 19h:29m:25s remains)
INFO - root - 2017-12-15 08:10:46.467738: step 33340, loss = 0.20, batch loss = 0.17 (32.2 examples/sec; 0.248 sec/batch; 20h:37m:30s remains)
INFO - root - 2017-12-15 08:10:48.741837: step 33350, loss = 0.18, batch loss = 0.15 (36.3 examples/sec; 0.220 sec/batch; 18h:17m:42s remains)
INFO - root - 2017-12-15 08:10:51.026331: step 33360, loss = 0.18, batch loss = 0.14 (36.1 examples/sec; 0.222 sec/batch; 18h:24m:37s remains)
INFO - root - 2017-12-15 08:10:53.333041: step 33370, loss = 0.27, batch loss = 0.24 (35.4 examples/sec; 0.226 sec/batch; 18h:47m:49s remains)
INFO - root - 2017-12-15 08:10:55.618808: step 33380, loss = 0.26, batch loss = 0.23 (36.1 examples/sec; 0.221 sec/batch; 18h:23m:59s remains)
INFO - root - 2017-12-15 08:10:57.887749: step 33390, loss = 0.26, batch loss = 0.23 (35.2 examples/sec; 0.227 sec/batch; 18h:51m:24s remains)
INFO - root - 2017-12-15 08:11:00.175544: step 33400, loss = 0.30, batch loss = 0.27 (35.2 examples/sec; 0.227 sec/batch; 18h:53m:04s remains)
2017-12-15 08:11:00.467386: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3773737 -4.2233286 -4.3436861 -3.9380016 -3.8758736 -4.3570013 -5.1258216 -6.3212409 -6.7217107 -6.3984089 -6.1045141 -6.3145623 -6.3902807 -6.2278118 -4.7273879][-2.3363075 -3.1318135 -3.5467062 -3.5216033 -3.9080806 -4.7736778 -5.7775011 -7.0520334 -7.5853338 -7.3348403 -6.9967704 -7.1336684 -7.0880823 -6.6796951 -4.6630778][-2.9869423 -2.9026732 -3.6117966 -4.0947714 -4.7151918 -5.5007424 -6.2236843 -7.1750793 -7.7516718 -7.8085294 -7.6994977 -7.9045334 -7.9205022 -7.4256654 -5.0088081][-4.5784712 -3.9577031 -4.80243 -5.3208766 -5.40088 -5.4104033 -5.3704414 -5.8230381 -6.593359 -7.2207866 -7.5265875 -7.9906683 -8.3216667 -7.9322228 -5.3509264][-5.464684 -4.8417015 -5.4237208 -5.4649215 -4.7077045 -3.6155546 -2.6105947 -2.8114061 -3.9520607 -5.1162653 -6.020112 -6.881804 -7.5592532 -7.4973178 -5.1884613][-6.6551261 -5.8280296 -5.9106636 -5.2571392 -3.2904265 -0.86137104 1.1164513 1.1503799 -0.293738 -2.0724766 -3.7422552 -5.1278448 -6.2665987 -6.6108441 -4.7376909][-7.2676711 -6.380424 -5.7596874 -4.3670349 -1.6847659 1.5300062 4.2716174 4.475132 2.7304044 0.53338289 -1.7814908 -3.5375092 -4.8234444 -5.4184518 -4.1274538][-7.091495 -5.6486692 -4.3599391 -2.6762886 0.017746687 3.3886542 6.3676186 6.6454268 4.6430478 2.1563821 -0.5170933 -2.3844028 -3.6042542 -4.4391456 -3.9189892][-7.0423384 -5.0883503 -3.2868607 -1.3119594 1.2206907 4.2654562 7.26237 7.5753593 5.3772855 2.6870565 -0.083717823 -1.8442394 -3.032969 -4.1449742 -4.3462534][-6.8683133 -4.55863 -2.4730096 -0.45842206 1.4375894 3.65279 6.02542 6.1882896 3.9589338 1.3332651 -1.190148 -2.7452967 -3.9965212 -5.2351065 -5.5153008][-6.3835597 -4.0357709 -2.018867 -0.23954487 0.88625574 1.8659825 3.215631 3.0954919 1.0541394 -1.1427566 -3.3395436 -4.7270613 -6.1065459 -7.1738038 -6.9878178][-6.1024189 -3.7667725 -1.9668504 -0.5810889 -0.13925505 -0.2166996 0.076326132 -0.32449973 -2.1385136 -3.8928022 -5.5925469 -6.7047262 -7.7234545 -7.9752269 -7.0583687][-5.6474705 -3.2397346 -1.7196724 -0.94143951 -1.1476101 -1.9954401 -2.4299769 -3.0031581 -4.5409145 -5.6471748 -6.6450357 -7.2708578 -7.5638418 -6.992908 -5.535111][-5.5679445 -3.1374359 -1.863961 -1.7044463 -2.3428266 -3.4872661 -4.1106234 -4.7009854 -6.0366054 -6.7858515 -7.4232287 -7.6782393 -7.3806038 -6.0997133 -4.1298819][-5.7840204 -3.3304718 -2.251085 -2.4225061 -3.1879468 -4.1247797 -4.6607475 -5.2091837 -6.2555594 -6.7885842 -7.1056919 -6.8894548 -6.1150713 -4.6125627 -2.6618686]]...]
INFO - root - 2017-12-15 08:11:02.752929: step 33410, loss = 0.26, batch loss = 0.23 (35.7 examples/sec; 0.224 sec/batch; 18h:36m:17s remains)
INFO - root - 2017-12-15 08:11:04.993920: step 33420, loss = 0.23, batch loss = 0.20 (35.9 examples/sec; 0.223 sec/batch; 18h:31m:37s remains)
INFO - root - 2017-12-15 08:11:07.264548: step 33430, loss = 0.21, batch loss = 0.18 (34.7 examples/sec; 0.231 sec/batch; 19h:09m:37s remains)
INFO - root - 2017-12-15 08:11:09.560949: step 33440, loss = 0.24, batch loss = 0.21 (35.2 examples/sec; 0.227 sec/batch; 18h:52m:42s remains)
INFO - root - 2017-12-15 08:11:11.849173: step 33450, loss = 0.23, batch loss = 0.19 (34.6 examples/sec; 0.231 sec/batch; 19h:11m:17s remains)
INFO - root - 2017-12-15 08:11:14.135696: step 33460, loss = 0.17, batch loss = 0.14 (34.7 examples/sec; 0.230 sec/batch; 19h:08m:16s remains)
INFO - root - 2017-12-15 08:11:16.416469: step 33470, loss = 0.19, batch loss = 0.16 (34.5 examples/sec; 0.232 sec/batch; 19h:17m:17s remains)
INFO - root - 2017-12-15 08:11:18.696471: step 33480, loss = 0.20, batch loss = 0.16 (34.7 examples/sec; 0.230 sec/batch; 19h:07m:58s remains)
INFO - root - 2017-12-15 08:11:20.971654: step 33490, loss = 0.22, batch loss = 0.19 (34.9 examples/sec; 0.229 sec/batch; 19h:02m:57s remains)
INFO - root - 2017-12-15 08:11:23.300785: step 33500, loss = 0.18, batch loss = 0.15 (35.8 examples/sec; 0.223 sec/batch; 18h:33m:22s remains)
2017-12-15 08:11:23.603900: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0028491 -6.9632378 -7.3113513 -7.1005 -6.62606 -5.4747438 -4.0967526 -3.599143 -4.16347 -5.0794091 -5.6026812 -5.928422 -5.8834515 -5.5352659 -4.9506755][-5.7114587 -7.8290019 -8.02962 -7.3415213 -6.3188314 -4.6868763 -2.9687281 -2.4497154 -3.1587696 -4.2902527 -5.0683408 -5.53909 -5.5150776 -5.0621247 -4.4028792][-7.1206994 -8.0024948 -7.8200793 -6.3920174 -4.6983786 -2.4886589 -0.54892123 -0.23614717 -1.3717477 -3.0270307 -4.2674112 -4.9278049 -4.858645 -4.2190571 -3.6036863][-7.9380336 -7.9992003 -7.4400997 -5.3370762 -3.0586877 -0.33681285 1.8806491 2.169518 0.72124982 -1.4204698 -3.1221313 -4.1267471 -4.0460343 -3.2357497 -2.6903491][-8.0287056 -7.83099 -6.9425054 -4.18715 -1.3327291 1.8088937 4.2226996 4.5609322 2.9168139 0.28629088 -1.9204452 -3.3073418 -3.3310683 -2.5375867 -2.201431][-8.13682 -7.5875969 -6.4618087 -3.1562974 0.15921164 3.4913387 5.9414039 6.3491225 4.7207203 1.6623383 -1.0168591 -2.8089027 -2.9822671 -2.4421241 -2.4745095][-7.4516582 -7.3795753 -6.17809 -2.6066391 0.90826678 4.1428308 6.4517093 6.91964 5.3825259 2.074563 -0.94068944 -3.0126731 -3.2636433 -3.0289202 -3.4028106][-7.1433048 -6.9886684 -5.9385281 -2.5708237 0.76790023 3.6958027 5.6675415 5.9675879 4.4022689 1.0571733 -1.943652 -3.9474225 -4.2556505 -4.362205 -4.9707546][-6.945375 -6.7938781 -6.1379728 -3.3591242 -0.46736574 2.0400376 3.613533 3.7428741 2.2623277 -0.78636253 -3.4771981 -5.3056188 -5.6439266 -6.0252857 -6.621335][-7.0495677 -6.8884363 -6.6298547 -4.4913807 -2.0647237 -0.01692009 1.1382835 1.0514116 -0.36046207 -3.0123811 -5.4406829 -7.07027 -7.2754083 -7.6564751 -7.9672351][-7.0064659 -6.7180009 -6.7539 -5.3103771 -3.4855578 -2.0021605 -1.2322955 -1.533277 -2.8860061 -5.1053553 -7.23871 -8.4279709 -8.321991 -8.6103249 -8.5359058][-6.749886 -6.3093214 -6.5477047 -5.7518396 -4.5182867 -3.6008065 -3.0699179 -3.3526678 -4.5727558 -6.4661913 -8.225563 -8.9813566 -8.6763935 -8.779398 -8.2257214][-6.479475 -5.9309349 -6.2484512 -5.9234743 -5.2132154 -4.8289638 -4.4907508 -4.6132946 -5.5116959 -6.9056368 -8.2763224 -8.59634 -8.1513481 -8.0724831 -7.13093][-6.4202447 -5.9432039 -6.4200954 -6.5521727 -6.3288617 -6.3147058 -6.0146937 -5.8411446 -6.2484045 -7.0825353 -7.7480206 -7.5250645 -6.9407082 -6.730711 -5.5088639][-6.3761864 -5.9919276 -6.5847764 -7.0278516 -7.1158671 -7.1289015 -6.7070031 -6.3432879 -6.5236173 -6.9867439 -7.0969677 -6.5941019 -6.00798 -5.6655321 -4.2699242]]...]
INFO - root - 2017-12-15 08:11:25.953672: step 33510, loss = 0.23, batch loss = 0.20 (35.3 examples/sec; 0.227 sec/batch; 18h:48m:59s remains)
INFO - root - 2017-12-15 08:11:28.256434: step 33520, loss = 0.20, batch loss = 0.16 (34.9 examples/sec; 0.229 sec/batch; 19h:03m:04s remains)
INFO - root - 2017-12-15 08:11:30.520719: step 33530, loss = 0.26, batch loss = 0.23 (35.4 examples/sec; 0.226 sec/batch; 18h:46m:53s remains)
INFO - root - 2017-12-15 08:11:32.803001: step 33540, loss = 0.18, batch loss = 0.15 (36.5 examples/sec; 0.219 sec/batch; 18h:12m:44s remains)
INFO - root - 2017-12-15 08:11:35.073434: step 33550, loss = 0.36, batch loss = 0.33 (33.9 examples/sec; 0.236 sec/batch; 19h:34m:51s remains)
INFO - root - 2017-12-15 08:11:37.374280: step 33560, loss = 0.23, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 18h:27m:17s remains)
INFO - root - 2017-12-15 08:11:39.693686: step 33570, loss = 0.22, batch loss = 0.19 (33.3 examples/sec; 0.241 sec/batch; 19h:58m:37s remains)
INFO - root - 2017-12-15 08:11:41.943191: step 33580, loss = 0.19, batch loss = 0.16 (35.1 examples/sec; 0.228 sec/batch; 18h:57m:06s remains)
INFO - root - 2017-12-15 08:11:44.213257: step 33590, loss = 0.34, batch loss = 0.30 (34.8 examples/sec; 0.230 sec/batch; 19h:05m:18s remains)
INFO - root - 2017-12-15 08:11:46.503792: step 33600, loss = 0.18, batch loss = 0.15 (35.2 examples/sec; 0.227 sec/batch; 18h:51m:25s remains)
2017-12-15 08:11:46.774549: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6944242 -5.7087774 -6.5080481 -6.5832925 -6.273088 -5.8453121 -5.2104836 -4.9758935 -5.5251727 -6.1063271 -6.6608844 -6.8129 -6.4478755 -5.6901321 -4.9312663][-4.6040878 -6.3051095 -7.5491934 -7.7879987 -7.3433261 -6.7660527 -6.0807953 -5.8861856 -6.48817 -7.050107 -7.7269864 -7.9679966 -7.4747953 -6.518476 -5.5515423][-4.7350483 -6.1391993 -7.5571504 -7.7440209 -7.0271864 -6.1858053 -5.3346028 -5.1742988 -5.8820238 -6.5539861 -7.5500712 -8.1126862 -7.6276894 -6.5138378 -5.4257994][-5.074492 -5.7411642 -7.0406084 -7.015821 -6.1621647 -5.0332413 -3.8845317 -3.7708647 -4.7391105 -5.6123209 -6.7243962 -7.4021335 -6.9162545 -5.8065195 -4.8228807][-4.7443666 -4.9713335 -5.7591972 -5.2548447 -4.17222 -2.7327352 -1.2173399 -1.1594281 -2.5089524 -3.6514864 -4.8855247 -5.6846824 -5.3570328 -4.4516397 -3.7676547][-4.2631245 -3.7011631 -3.7865005 -2.7542915 -1.4451931 0.38509083 2.3438053 2.3151441 0.34823251 -1.2892053 -2.6857285 -3.6054974 -3.6235633 -3.2238638 -2.9526978][-3.5239685 -2.6333954 -2.2679563 -0.97956836 0.50622344 2.598805 4.896759 4.8345819 2.4768319 0.7087388 -0.63786757 -1.6193821 -2.0718954 -2.2942672 -2.4802845][-2.8987179 -1.4243484 -0.93995118 0.28419852 1.7856178 3.9730434 6.3992972 6.46104 4.0160875 2.1742039 0.84975553 -0.22313404 -0.91608179 -1.5567064 -2.1153505][-2.1658521 -0.65565252 -0.5149256 0.26734161 1.4594843 3.2968783 5.5085144 5.7468991 3.718565 2.075211 0.83965135 -0.062218666 -0.64222062 -1.2869709 -1.8600824][-2.2399497 -0.89058244 -1.074374 -0.69822896 0.082514763 1.5155876 3.372107 3.8446784 2.4731493 1.0118048 -0.082734346 -0.82329452 -1.2998409 -1.6801744 -2.0139029][-2.2783027 -1.184441 -1.6058486 -1.5019082 -0.96384454 0.065668344 1.5128777 1.842514 0.75431991 -0.52312684 -1.5486429 -2.2520075 -2.627398 -2.5843341 -2.3548176][-3.0175345 -2.1281242 -2.6320934 -2.6302729 -2.2797337 -1.3442435 -0.28084874 -0.21785569 -1.1125774 -2.160111 -3.1164858 -4.0255246 -4.3589373 -3.833638 -3.1300087][-4.2946396 -3.1908848 -3.4583588 -3.4051023 -3.0533001 -2.1132197 -1.1917349 -1.3605757 -2.2085712 -3.10217 -4.2676172 -5.3798938 -5.6802096 -4.9697495 -4.0204082][-5.6571336 -4.1966004 -4.1497979 -4.1234803 -3.9856124 -3.1177998 -2.363076 -2.6551521 -3.3915606 -4.2719374 -5.4003992 -6.3904977 -6.5158057 -5.6480818 -4.7646523][-7.2358565 -5.5546093 -5.2926626 -5.2671175 -5.1591859 -4.4202242 -3.8346043 -4.0276513 -4.5241823 -5.160388 -6.035079 -6.6730289 -6.5973225 -5.902863 -5.2749262]]...]
INFO - root - 2017-12-15 08:11:49.037790: step 33610, loss = 0.22, batch loss = 0.19 (33.9 examples/sec; 0.236 sec/batch; 19h:35m:20s remains)
INFO - root - 2017-12-15 08:11:51.309771: step 33620, loss = 0.27, batch loss = 0.23 (36.2 examples/sec; 0.221 sec/batch; 18h:21m:54s remains)
INFO - root - 2017-12-15 08:11:53.572713: step 33630, loss = 0.25, batch loss = 0.22 (35.4 examples/sec; 0.226 sec/batch; 18h:44m:55s remains)
INFO - root - 2017-12-15 08:11:55.831663: step 33640, loss = 0.16, batch loss = 0.13 (35.1 examples/sec; 0.228 sec/batch; 18h:56m:53s remains)
INFO - root - 2017-12-15 08:11:58.144031: step 33650, loss = 0.27, batch loss = 0.24 (35.2 examples/sec; 0.227 sec/batch; 18h:51m:56s remains)
INFO - root - 2017-12-15 08:12:00.416642: step 33660, loss = 0.21, batch loss = 0.18 (35.8 examples/sec; 0.223 sec/batch; 18h:32m:13s remains)
INFO - root - 2017-12-15 08:12:02.715790: step 33670, loss = 0.24, batch loss = 0.21 (34.5 examples/sec; 0.232 sec/batch; 19h:15m:41s remains)
INFO - root - 2017-12-15 08:12:04.974022: step 33680, loss = 0.25, batch loss = 0.22 (35.4 examples/sec; 0.226 sec/batch; 18h:46m:53s remains)
INFO - root - 2017-12-15 08:12:07.242022: step 33690, loss = 0.22, batch loss = 0.19 (35.1 examples/sec; 0.228 sec/batch; 18h:53m:33s remains)
INFO - root - 2017-12-15 08:12:09.514421: step 33700, loss = 0.35, batch loss = 0.32 (35.9 examples/sec; 0.223 sec/batch; 18h:29m:23s remains)
2017-12-15 08:12:09.779189: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.01828 -5.4315968 -4.7750254 -4.484446 -4.2566948 -3.9328713 -3.6110473 -3.5532496 -3.6824257 -4.4351459 -5.1158381 -5.0493956 -5.0220013 -4.9708023 -4.7312007][-3.8575156 -5.2308779 -4.173872 -3.6601996 -3.5341887 -3.4921985 -3.3997087 -3.4996567 -3.8083003 -4.7001238 -5.6025906 -5.4937215 -5.3870926 -5.3495173 -5.1596556][-4.7393885 -4.9828463 -3.632086 -2.8589377 -2.5278749 -2.4818656 -2.3015592 -2.4774866 -2.9585888 -3.9591155 -5.059835 -5.0722103 -4.8309388 -4.6753626 -4.4060211][-5.5554581 -5.2633581 -3.9176223 -3.0832627 -2.5917888 -2.3270781 -1.7270062 -1.4933619 -1.8149778 -2.7694581 -4.1140079 -4.5173912 -4.5020332 -4.2896271 -3.8419063][-5.7585087 -5.3278236 -4.0243368 -3.2504606 -2.6615632 -2.1115184 -0.97882867 -0.24568343 -0.20095038 -1.0296255 -2.5983825 -3.5744205 -4.0961494 -4.0716314 -3.6237605][-5.9187527 -4.9890337 -3.5043168 -2.5164549 -1.5916187 -0.56311643 0.98636413 2.0914743 2.220618 1.1152189 -1.0459843 -2.7330265 -3.8531234 -3.9522805 -3.4292588][-6.2775784 -5.4399462 -3.8088503 -2.5275478 -1.0793883 0.61338234 2.6956556 4.2212324 4.2476425 2.7234008 -0.063857317 -2.3641357 -3.9423137 -3.991574 -3.32442][-6.6420555 -6.27674 -4.7930984 -3.4767697 -1.6338351 0.60290432 3.1237204 4.9690533 4.9479837 3.2403433 0.16559386 -2.3160691 -4.1284728 -4.1716671 -3.4370642][-6.5894003 -6.2642069 -4.7515359 -3.4739947 -1.6493773 0.56532812 2.8690732 4.5427608 4.4015961 2.5312464 -0.45383859 -2.7379584 -4.3811793 -4.2742043 -3.4257414][-6.1415167 -5.7129269 -4.0816507 -2.819159 -1.3054334 0.39181447 2.0547826 3.3729527 3.0480201 1.2291729 -1.379667 -3.1965697 -4.4052868 -4.0175 -3.0709934][-6.0613489 -5.6871657 -4.0627608 -2.9175408 -1.7739244 -0.52463031 0.8215816 1.7880504 1.4186506 -0.15526772 -2.3194299 -3.7461774 -4.5550814 -4.1313467 -3.0960846][-6.05867 -5.7723589 -4.3102465 -3.4059327 -2.6175225 -1.6278999 -0.49006343 0.12893128 -0.26939821 -1.5629697 -3.3173094 -4.3680892 -4.7847557 -4.3002214 -3.3139682][-5.898881 -5.648242 -4.3374619 -3.6318269 -3.0841925 -2.248086 -1.3496939 -1.0101411 -1.3509027 -2.2909865 -3.6112976 -4.3316407 -4.4315662 -4.0538583 -3.4159508][-5.9892311 -5.7890019 -4.5594292 -3.8988569 -3.3698783 -2.5709281 -1.8696729 -1.58324 -1.6688426 -2.2287343 -3.2492342 -3.814266 -3.7971983 -3.6198754 -3.3269877][-6.0984759 -6.0629282 -5.1746416 -4.7330866 -4.3081303 -3.5823672 -2.9110789 -2.4934731 -2.3083451 -2.5912597 -3.3722577 -3.8355119 -3.8209333 -3.8470168 -3.7932281]]...]
INFO - root - 2017-12-15 08:12:12.048268: step 33710, loss = 0.30, batch loss = 0.27 (34.6 examples/sec; 0.231 sec/batch; 19h:10m:28s remains)
INFO - root - 2017-12-15 08:12:14.342868: step 33720, loss = 0.33, batch loss = 0.30 (35.1 examples/sec; 0.228 sec/batch; 18h:55m:26s remains)
INFO - root - 2017-12-15 08:12:16.601754: step 33730, loss = 0.22, batch loss = 0.19 (35.5 examples/sec; 0.225 sec/batch; 18h:42m:28s remains)
INFO - root - 2017-12-15 08:12:18.875516: step 33740, loss = 0.24, batch loss = 0.20 (36.1 examples/sec; 0.222 sec/batch; 18h:24m:09s remains)
INFO - root - 2017-12-15 08:12:21.148003: step 33750, loss = 0.27, batch loss = 0.24 (35.9 examples/sec; 0.223 sec/batch; 18h:29m:54s remains)
INFO - root - 2017-12-15 08:12:23.442824: step 33760, loss = 0.22, batch loss = 0.19 (33.2 examples/sec; 0.241 sec/batch; 20h:00m:21s remains)
INFO - root - 2017-12-15 08:12:25.690367: step 33770, loss = 0.29, batch loss = 0.26 (36.3 examples/sec; 0.221 sec/batch; 18h:17m:59s remains)
INFO - root - 2017-12-15 08:12:27.942215: step 33780, loss = 0.24, batch loss = 0.20 (35.8 examples/sec; 0.224 sec/batch; 18h:33m:24s remains)
INFO - root - 2017-12-15 08:12:30.188140: step 33790, loss = 0.18, batch loss = 0.15 (35.1 examples/sec; 0.228 sec/batch; 18h:55m:24s remains)
INFO - root - 2017-12-15 08:12:32.463611: step 33800, loss = 0.36, batch loss = 0.33 (35.4 examples/sec; 0.226 sec/batch; 18h:45m:20s remains)
2017-12-15 08:12:32.739764: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7556925 -5.9518781 -6.0051303 -5.7857132 -5.048192 -4.1030354 -3.3985543 -3.1706614 -3.6289535 -4.4568286 -4.9208522 -4.6142664 -4.1687231 -4.0498643 -4.5403442][-4.3812456 -5.8524323 -5.7525992 -5.2131233 -4.0410838 -2.8416221 -2.2941427 -2.4205334 -3.2651944 -4.4156651 -5.0034838 -4.7159319 -4.2356911 -4.1394424 -4.7154064][-5.0157404 -5.677824 -5.2925372 -4.3474283 -2.7570682 -1.3813273 -0.97909951 -1.4568012 -2.6919901 -4.0937233 -4.6664371 -4.4725609 -4.0594468 -3.9832296 -4.5103445][-5.2687831 -5.1281519 -4.4755168 -3.4551735 -1.8186786 -0.3237015 0.28412676 -0.18925381 -1.7464478 -3.3677871 -3.9941192 -3.9692755 -3.6782613 -3.5741272 -3.9313912][-5.3436794 -4.5152345 -3.8939195 -3.0479712 -1.4102521 0.41805339 1.5463738 1.2091088 -0.58436763 -2.4474659 -3.145009 -3.351397 -3.2694249 -3.0955281 -3.3071806][-5.1059985 -3.9940715 -3.5033484 -2.8475866 -1.4390309 0.48346305 2.10908 2.2359297 0.4708488 -1.5700722 -2.3401911 -2.66297 -2.6554892 -2.4990637 -2.7101097][-4.1389 -3.2429993 -3.0466664 -2.8574066 -1.8802634 -0.0071854591 2.0891311 2.8836219 1.4804988 -0.53812551 -1.4446623 -1.8340125 -1.7721633 -1.5267329 -1.715341][-3.4222012 -2.6959331 -2.9751763 -3.1805673 -2.6308367 -0.98384333 1.195925 2.4485776 1.6293495 0.0028600693 -0.79655695 -1.0820036 -0.922066 -0.62936556 -0.94853985][-2.4677846 -2.0908108 -2.7579682 -3.0650969 -2.7929287 -1.6934758 -0.11353779 1.0204134 0.70125365 -0.35750806 -0.78908467 -0.92824495 -0.57784605 -0.24416971 -0.70284355][-1.9155095 -1.82492 -2.5830557 -2.791609 -2.6731942 -2.1444616 -1.3388858 -0.65790558 -0.77222586 -1.2778139 -1.3250363 -1.1599327 -0.49303246 -0.12367034 -0.77333951][-1.7166681 -1.7237036 -2.475121 -2.6458907 -2.5280893 -2.3180056 -2.2239518 -2.3516433 -2.5817404 -2.6707764 -2.2933941 -1.7054315 -0.88386357 -0.483701 -1.2657721][-1.9512058 -1.8335555 -2.484961 -2.4805026 -2.221339 -2.1854942 -2.6935272 -3.6485209 -4.0294514 -3.942523 -3.3440261 -2.5266421 -1.7065356 -1.4161124 -2.2223973][-2.8450894 -2.4397137 -2.7393541 -2.5382047 -2.1563365 -2.3434098 -3.2666626 -4.6279683 -5.041132 -4.9145832 -4.2407618 -3.3934357 -2.7144632 -2.5809648 -3.3633497][-4.0860181 -3.2943187 -3.268153 -2.9320073 -2.627661 -3.1272879 -4.2007294 -5.4394126 -5.6646395 -5.5048847 -4.891377 -4.0744414 -3.6118331 -3.589859 -4.239223][-5.2437458 -4.2781034 -4.0545812 -3.6711669 -3.402725 -3.9403815 -4.8673372 -5.7405806 -5.7135396 -5.5641117 -5.0654883 -4.340642 -4.076097 -4.1033945 -4.575026]]...]
INFO - root - 2017-12-15 08:12:34.999558: step 33810, loss = 0.20, batch loss = 0.17 (35.3 examples/sec; 0.227 sec/batch; 18h:47m:42s remains)
INFO - root - 2017-12-15 08:12:37.273549: step 33820, loss = 0.23, batch loss = 0.20 (34.4 examples/sec; 0.233 sec/batch; 19h:17m:46s remains)
INFO - root - 2017-12-15 08:12:39.548977: step 33830, loss = 0.27, batch loss = 0.24 (35.0 examples/sec; 0.229 sec/batch; 18h:58m:46s remains)
INFO - root - 2017-12-15 08:12:41.814942: step 33840, loss = 0.19, batch loss = 0.16 (35.0 examples/sec; 0.229 sec/batch; 18h:58m:56s remains)
INFO - root - 2017-12-15 08:12:44.070280: step 33850, loss = 0.25, batch loss = 0.22 (36.4 examples/sec; 0.220 sec/batch; 18h:14m:55s remains)
INFO - root - 2017-12-15 08:12:46.321744: step 33860, loss = 0.25, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 18h:35m:27s remains)
INFO - root - 2017-12-15 08:12:48.590571: step 33870, loss = 0.18, batch loss = 0.15 (35.8 examples/sec; 0.224 sec/batch; 18h:33m:26s remains)
INFO - root - 2017-12-15 08:12:50.840473: step 33880, loss = 0.25, batch loss = 0.22 (33.8 examples/sec; 0.237 sec/batch; 19h:37m:51s remains)
INFO - root - 2017-12-15 08:12:53.111407: step 33890, loss = 0.23, batch loss = 0.19 (36.0 examples/sec; 0.222 sec/batch; 18h:26m:25s remains)
INFO - root - 2017-12-15 08:12:55.380448: step 33900, loss = 0.25, batch loss = 0.22 (35.7 examples/sec; 0.224 sec/batch; 18h:36m:44s remains)
2017-12-15 08:12:55.683838: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8047509 -4.8024082 -5.5699358 -6.6127682 -7.082056 -6.9646425 -6.5609255 -6.29084 -5.9566145 -5.5786834 -5.7700415 -6.4415722 -7.3401213 -7.6168861 -6.5610075][-3.3769035 -3.7416015 -4.7666416 -6.0212145 -6.5372438 -6.1894031 -5.5550346 -5.2061663 -4.9912844 -4.618681 -5.02098 -5.9851952 -6.9944105 -7.4558744 -6.4774265][-3.4207907 -3.01939 -4.3407803 -5.66488 -6.0348825 -5.2987981 -4.2430973 -3.7566991 -3.6784129 -3.4245973 -3.96427 -4.9934521 -6.0192366 -6.6575727 -5.898942][-3.6456177 -3.0220573 -4.4656959 -5.3750334 -5.1093497 -3.9099927 -2.4658127 -1.8834896 -2.022963 -2.089504 -2.787039 -3.7523408 -4.737999 -5.6434546 -5.2815313][-4.1212015 -2.9096732 -4.053308 -4.2428236 -3.2530327 -1.6184607 0.24737549 0.86200213 0.24711418 -0.310987 -1.1828986 -2.0621958 -3.0261731 -4.24526 -4.3527489][-4.0520639 -2.3878326 -2.8705521 -2.2842278 -0.74268532 1.0888996 3.0903051 3.4872525 2.3249447 1.3030241 0.27504349 -0.4036442 -1.2750518 -2.6683795 -3.1990535][-2.9433331 -1.2110554 -1.0478408 0.076903582 1.8129518 3.4582398 5.2596149 5.2907534 3.6801789 2.2348077 1.0388536 0.49213648 -0.30197608 -1.691579 -2.4549508][-2.4630349 -0.30102682 0.30024862 1.4679039 2.8609803 3.9607732 5.331666 5.1123571 3.4076164 1.8623641 0.68308353 0.16867971 -0.61012661 -1.8974704 -2.6951377][-2.7783208 -0.30202579 0.36965704 1.2452478 2.0232136 2.4364021 3.324693 3.0479176 1.7237551 0.60984135 -0.26832926 -0.67845035 -1.4453638 -2.540905 -3.4194951][-3.9601364 -1.4920704 -0.95512295 -0.34442997 -0.0024218559 -0.17509365 0.23466682 0.062726736 -0.65556121 -1.1610898 -1.6615546 -1.9690965 -2.529341 -3.4100156 -4.4337654][-6.075861 -3.872725 -3.5226622 -3.0451498 -2.7934852 -3.0825818 -2.8312154 -2.882303 -3.1421947 -3.2780213 -3.4817414 -3.5347762 -3.7226205 -4.3914251 -5.4877334][-7.6226597 -5.7729268 -5.562747 -5.0998058 -4.7201605 -4.8527451 -4.6706543 -4.6425529 -4.7393618 -4.7777157 -4.7512865 -4.5338326 -4.3764172 -4.8631706 -5.9879422][-7.6415253 -6.0778408 -5.866293 -5.4365444 -5.1450014 -5.3426342 -5.4317117 -5.5276279 -5.6659904 -5.7168188 -5.5912476 -5.2821941 -4.9443431 -5.2799387 -6.3005505][-7.7568388 -6.2725039 -5.8997054 -5.4795084 -5.269124 -5.5040216 -5.7333546 -5.9343519 -6.1016932 -6.1448288 -6.0179634 -5.8592529 -5.5644178 -5.7938147 -6.5571089][-7.1581335 -5.6143427 -5.1242704 -4.7380929 -4.5929332 -4.8135271 -5.1364756 -5.4593325 -5.6470213 -5.6089787 -5.5525661 -5.624527 -5.5408783 -5.7827749 -6.3063107]]...]
INFO - root - 2017-12-15 08:12:57.942569: step 33910, loss = 0.28, batch loss = 0.25 (34.6 examples/sec; 0.231 sec/batch; 19h:09m:14s remains)
INFO - root - 2017-12-15 08:13:00.228734: step 33920, loss = 0.19, batch loss = 0.16 (35.3 examples/sec; 0.227 sec/batch; 18h:49m:22s remains)
INFO - root - 2017-12-15 08:13:02.530141: step 33930, loss = 0.23, batch loss = 0.19 (34.9 examples/sec; 0.229 sec/batch; 19h:00m:43s remains)
INFO - root - 2017-12-15 08:13:04.789724: step 33940, loss = 0.16, batch loss = 0.13 (35.2 examples/sec; 0.227 sec/batch; 18h:49m:50s remains)
INFO - root - 2017-12-15 08:13:07.063113: step 33950, loss = 0.27, batch loss = 0.24 (35.7 examples/sec; 0.224 sec/batch; 18h:36m:19s remains)
INFO - root - 2017-12-15 08:13:09.348218: step 33960, loss = 0.22, batch loss = 0.19 (35.5 examples/sec; 0.226 sec/batch; 18h:42m:42s remains)
INFO - root - 2017-12-15 08:13:11.598637: step 33970, loss = 0.23, batch loss = 0.19 (34.1 examples/sec; 0.235 sec/batch; 19h:27m:19s remains)
INFO - root - 2017-12-15 08:13:13.850724: step 33980, loss = 0.21, batch loss = 0.18 (35.7 examples/sec; 0.224 sec/batch; 18h:34m:13s remains)
INFO - root - 2017-12-15 08:13:16.117433: step 33990, loss = 0.26, batch loss = 0.23 (36.2 examples/sec; 0.221 sec/batch; 18h:18m:10s remains)
INFO - root - 2017-12-15 08:13:18.394100: step 34000, loss = 0.17, batch loss = 0.14 (34.9 examples/sec; 0.229 sec/batch; 19h:01m:36s remains)
2017-12-15 08:13:18.678382: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.918875 -5.787797 -5.9997377 -5.6796589 -5.227797 -4.8434429 -4.4786396 -4.2619543 -4.5011435 -5.0488644 -5.5219574 -5.387413 -5.1898184 -5.193181 -5.3019724][-4.7342262 -6.7323847 -7.1856985 -6.8355522 -6.2709017 -5.7444658 -5.1382523 -4.7621021 -5.0882053 -5.8809285 -6.5074377 -6.069788 -5.5871654 -5.5179396 -5.6234121][-6.1260853 -7.2453842 -7.7333708 -7.3447218 -6.7916012 -6.257308 -5.387001 -4.7814074 -5.1340489 -6.1459169 -6.9050369 -6.2198839 -5.4463921 -5.2188311 -5.248497][-7.1868858 -7.4589214 -7.7540321 -7.2742758 -6.6753368 -6.0194111 -4.8108263 -3.9772069 -4.3241425 -5.4534211 -6.3276253 -5.564199 -4.6185932 -4.2219563 -4.14332][-7.5955067 -6.786314 -6.6872435 -5.9591694 -5.0951958 -4.0996046 -2.5273535 -1.7607863 -2.4531696 -3.8091924 -4.8626633 -4.1067271 -3.207782 -2.7172794 -2.4814806][-7.0036039 -5.7589693 -5.2657518 -4.1729784 -2.8378942 -1.3178939 0.51414657 1.0156295 -0.22477961 -1.9658656 -3.1134171 -2.373158 -1.507479 -0.98615277 -0.7891959][-6.2115 -5.260664 -4.4984274 -2.9805655 -0.91059542 1.3224893 3.439872 3.679405 1.8301179 -0.35226464 -1.5218775 -0.87562966 -0.31412315 0.043081045 -0.0080571175][-6.0743856 -5.0291753 -4.1166267 -2.2275255 0.60866022 3.5653307 5.8753128 5.7890644 3.191458 0.52025723 -0.5715363 -0.047218323 0.17347169 0.18121052 -0.11492229][-6.1766458 -5.3383179 -4.5641155 -2.652951 0.4156332 3.5449297 5.7245846 5.4733191 2.5517285 -0.13752198 -0.84342968 -0.2497921 -0.090752125 -0.32519913 -0.77932405][-6.8122911 -6.2960987 -5.7538791 -4.0634103 -1.0771929 1.8838866 3.7904232 3.5707524 0.77461481 -1.4865232 -1.7346754 -1.207891 -1.2183479 -1.7227519 -2.3090594][-7.55824 -7.2717876 -6.870204 -5.4863348 -2.8904276 -0.41895139 1.0620337 0.76190686 -1.7090282 -3.3332381 -3.213151 -2.6181209 -2.5890353 -3.0995929 -3.5454352][-7.9673195 -7.8192091 -7.6727638 -6.8464937 -5.0187407 -3.3184757 -2.4848955 -2.9644945 -4.8508945 -5.7565002 -5.2755446 -4.5118294 -4.23627 -4.481504 -4.6937075][-8.0820837 -7.9985723 -8.0398922 -7.6740761 -6.5427971 -5.5514059 -5.2392826 -5.7332425 -6.9771833 -7.3384809 -6.8283262 -6.1891365 -5.9917994 -6.0376687 -6.1030116][-7.5983696 -7.3510017 -7.358634 -7.2051868 -6.5837564 -6.1370449 -6.1850009 -6.7347107 -7.5202332 -7.6294069 -7.2163992 -6.7794037 -6.6520548 -6.531497 -6.4432182][-6.7368469 -6.3233881 -6.2806206 -6.1970139 -5.9143338 -5.818469 -6.0499115 -6.5197344 -6.8910809 -6.8952751 -6.6326056 -6.3380775 -6.2258973 -6.09325 -6.0219173]]...]
INFO - root - 2017-12-15 08:13:20.972539: step 34010, loss = 0.29, batch loss = 0.26 (33.6 examples/sec; 0.238 sec/batch; 19h:43m:55s remains)
INFO - root - 2017-12-15 08:13:23.252902: step 34020, loss = 0.25, batch loss = 0.22 (35.1 examples/sec; 0.228 sec/batch; 18h:52m:22s remains)
INFO - root - 2017-12-15 08:13:25.543361: step 34030, loss = 0.20, batch loss = 0.16 (35.6 examples/sec; 0.225 sec/batch; 18h:36m:47s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 08:13:27.838033: step 34040, loss = 0.22, batch loss = 0.19 (34.5 examples/sec; 0.232 sec/batch; 19h:12m:53s remains)
INFO - root - 2017-12-15 08:13:30.138462: step 34050, loss = 0.24, batch loss = 0.21 (34.1 examples/sec; 0.234 sec/batch; 19h:25m:19s remains)
INFO - root - 2017-12-15 08:13:32.421486: step 34060, loss = 0.24, batch loss = 0.21 (35.8 examples/sec; 0.224 sec/batch; 18h:32m:04s remains)
INFO - root - 2017-12-15 08:13:34.674548: step 34070, loss = 0.24, batch loss = 0.20 (35.4 examples/sec; 0.226 sec/batch; 18h:45m:04s remains)
INFO - root - 2017-12-15 08:13:36.930339: step 34080, loss = 0.32, batch loss = 0.28 (35.1 examples/sec; 0.228 sec/batch; 18h:53m:53s remains)
INFO - root - 2017-12-15 08:13:39.221282: step 34090, loss = 0.24, batch loss = 0.21 (32.0 examples/sec; 0.250 sec/batch; 20h:44m:27s remains)
INFO - root - 2017-12-15 08:13:41.494173: step 34100, loss = 0.32, batch loss = 0.29 (36.1 examples/sec; 0.222 sec/batch; 18h:22m:48s remains)
2017-12-15 08:13:41.779290: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4880123 -6.1653905 -6.2603984 -6.1909637 -5.7964373 -5.3096781 -4.8869143 -4.7545614 -4.87551 -5.3098574 -5.9940429 -6.7692389 -7.28441 -7.4823403 -7.3295832][-4.4687395 -6.3303852 -6.5473833 -6.5623822 -6.1731334 -5.6962242 -5.3455825 -5.1326838 -5.1958809 -5.6174254 -6.3018556 -7.0383468 -7.5582957 -7.6390524 -7.388927][-5.7368069 -6.6658311 -6.7639675 -6.5795903 -5.9113398 -5.1810589 -4.6577387 -4.4050989 -4.5850854 -5.2014585 -6.0867243 -7.0333853 -7.7396636 -7.9024158 -7.5675387][-6.6387725 -6.8475895 -6.86161 -6.4645367 -5.4871187 -4.4422083 -3.5900626 -3.1594262 -3.4253449 -4.2282047 -5.2760267 -6.4154248 -7.4128337 -7.7330103 -7.2406406][-7.20689 -6.6888404 -6.4928489 -5.7326584 -4.3560925 -2.9204764 -1.6591946 -0.99239886 -1.3468122 -2.364218 -3.5768054 -4.8157506 -6.0362778 -6.4538612 -5.754447][-6.7668943 -5.8818264 -5.3632879 -4.1627126 -2.450654 -0.7168963 0.93996811 1.8746061 1.4438016 0.21670341 -1.142939 -2.5238137 -4.0630431 -4.703 -3.9994659][-5.4231911 -4.573411 -3.8278484 -2.4166224 -0.633878 1.1703916 2.9635272 3.9767218 3.484971 2.2086325 0.81428838 -0.70856762 -2.6743741 -3.7271981 -3.3082905][-4.8987551 -3.9114182 -3.0909076 -1.619989 0.24542665 2.1509862 3.946085 4.9324427 4.4093084 3.1186376 1.5922923 -0.11743331 -2.3837295 -3.7933984 -3.7401168][-4.7336655 -3.9996653 -3.5435822 -2.4926872 -0.86700177 0.92223358 2.5932565 3.6022458 3.225955 2.1412745 0.67671609 -1.0113038 -3.1822209 -4.5636187 -4.6504803][-4.8969502 -4.5026174 -4.5226164 -4.1122761 -2.9672444 -1.4915442 -0.14455986 0.68977618 0.38742018 -0.45170045 -1.6734282 -3.0086915 -4.6296959 -5.5847855 -5.5204544][-5.45763 -5.1931014 -5.4675293 -5.5583382 -4.9500532 -3.9052892 -2.8755009 -2.2116132 -2.3844028 -2.9343202 -3.9276667 -4.9937358 -6.0790281 -6.6501431 -6.4833531][-5.8440685 -5.52954 -5.8407779 -6.1886544 -6.0248132 -5.4937582 -4.8437605 -4.3477058 -4.3903961 -4.7060061 -5.3958693 -6.1478477 -6.861001 -7.2477927 -7.2159677][-5.9841986 -5.5760651 -5.8434896 -6.2251244 -6.3279476 -6.1934681 -5.8471785 -5.5553493 -5.5698428 -5.7339859 -6.0924954 -6.5227375 -6.9451065 -7.2509255 -7.3537664][-5.8366909 -5.3354959 -5.6274529 -6.0489759 -6.4001961 -6.6604109 -6.5492196 -6.3572369 -6.2952886 -6.2561693 -6.2625117 -6.3247433 -6.4423747 -6.5297985 -6.6205463][-5.153914 -4.4930096 -4.8108959 -5.2273636 -5.6754208 -6.1335554 -6.2202969 -6.1250944 -6.1019173 -5.9844122 -5.7573161 -5.5475616 -5.4365907 -5.3120241 -5.43789]]...]
INFO - root - 2017-12-15 08:13:44.025521: step 34110, loss = 0.27, batch loss = 0.24 (36.0 examples/sec; 0.222 sec/batch; 18h:24m:56s remains)
INFO - root - 2017-12-15 08:13:46.314308: step 34120, loss = 0.21, batch loss = 0.18 (34.0 examples/sec; 0.235 sec/batch; 19h:29m:55s remains)
INFO - root - 2017-12-15 08:13:48.567411: step 34130, loss = 0.18, batch loss = 0.15 (36.3 examples/sec; 0.220 sec/batch; 18h:15m:16s remains)
INFO - root - 2017-12-15 08:13:50.837352: step 34140, loss = 0.22, batch loss = 0.19 (35.5 examples/sec; 0.225 sec/batch; 18h:39m:50s remains)
INFO - root - 2017-12-15 08:13:53.116279: step 34150, loss = 0.19, batch loss = 0.16 (35.8 examples/sec; 0.223 sec/batch; 18h:29m:37s remains)
INFO - root - 2017-12-15 08:13:55.473233: step 34160, loss = 0.29, batch loss = 0.26 (35.0 examples/sec; 0.229 sec/batch; 18h:56m:27s remains)
INFO - root - 2017-12-15 08:13:57.712712: step 34170, loss = 0.22, batch loss = 0.19 (34.6 examples/sec; 0.232 sec/batch; 19h:11m:10s remains)
INFO - root - 2017-12-15 08:13:59.994980: step 34180, loss = 0.25, batch loss = 0.22 (35.5 examples/sec; 0.225 sec/batch; 18h:39m:02s remains)
INFO - root - 2017-12-15 08:14:02.247998: step 34190, loss = 0.28, batch loss = 0.25 (35.6 examples/sec; 0.225 sec/batch; 18h:36m:27s remains)
INFO - root - 2017-12-15 08:14:04.515552: step 34200, loss = 0.22, batch loss = 0.18 (35.6 examples/sec; 0.225 sec/batch; 18h:37m:19s remains)
2017-12-15 08:14:04.811129: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4979668 -4.7759256 -5.4812427 -5.3517079 -4.7947807 -3.8390141 -2.7968976 -2.3687074 -2.2893963 -2.3811424 -2.797791 -3.2037385 -2.9616265 -2.451304 -2.0496778][-5.5642281 -5.08809 -5.7340155 -5.5429988 -4.6836834 -3.4769754 -2.3242583 -1.7780484 -1.6334357 -1.8212477 -2.3117983 -2.8559065 -2.8003092 -2.4367692 -2.1919086][-6.3490353 -5.1600161 -5.7537394 -5.4135914 -4.2246461 -2.8874063 -1.8755172 -1.436486 -1.3665929 -1.680491 -2.2387249 -2.928087 -3.1430571 -2.9596567 -2.8028498][-6.3924656 -4.6621737 -5.1878767 -4.7384863 -3.3139338 -1.9436555 -1.1148204 -0.80806375 -0.84283376 -1.353044 -2.1208851 -3.1295445 -3.6407404 -3.5175045 -3.2726502][-5.9061484 -3.8275967 -4.3028717 -3.8354597 -2.3756604 -1.0741922 -0.36631429 -0.13231421 -0.25921786 -0.94267344 -1.9731802 -3.3170457 -4.0835714 -3.9061341 -3.5004213][-4.3101282 -2.3880854 -2.834156 -2.4156 -1.0913626 0.15553355 0.96209884 1.2501013 0.98390841 0.024696589 -1.4774866 -3.2616844 -4.2701106 -4.1225677 -3.5722358][-1.9950998 -0.62439716 -1.0487866 -0.69423032 0.48273849 1.8688157 3.137953 3.6702211 3.2719347 1.9431603 -0.13498116 -2.4953604 -3.8840263 -3.9592085 -3.3421016][-0.96126676 0.46047044 0.15897131 0.50768638 1.549789 3.0644624 4.7977848 5.6321316 5.2563076 3.7414243 1.4333122 -1.0936452 -2.6329172 -2.8640041 -2.2074342][-0.90914309 0.41046095 0.20556474 0.41142821 1.2277679 2.7235014 4.718235 5.8263187 5.7273407 4.4091797 2.2518771 -0.061083078 -1.4877465 -1.6516778 -0.94033742][-1.8136406 -0.49995828 -0.74614334 -0.81044996 -0.29822516 1.0773704 3.1199849 4.37486 4.6163931 3.6620638 1.8746479 -0.075740814 -1.2790325 -1.3064957 -0.62275565][-3.4642174 -2.1171968 -2.5656633 -2.991878 -2.7842445 -1.6337433 0.19298601 1.4073186 1.8947308 1.3719628 0.12761545 -1.3291615 -2.2287683 -2.1425674 -1.6553204][-4.7261071 -3.3609734 -4.1121521 -4.9343476 -4.9392047 -4.0144973 -2.5175946 -1.5511999 -1.0844097 -1.3906121 -2.294095 -3.3448305 -3.8873987 -3.6948705 -3.388206][-5.1134272 -3.7926321 -4.7533197 -5.8055325 -5.8010569 -4.9757309 -3.7665336 -3.146117 -2.8544369 -3.0890155 -3.7695374 -4.6292458 -5.0613651 -4.9224482 -4.8501606][-4.8492804 -3.5763297 -4.5041738 -5.5674124 -5.5161724 -4.80642 -3.7803226 -3.3808861 -3.2562368 -3.4669647 -3.9152179 -4.6292229 -5.0657997 -5.1234446 -5.4250784][-4.1669726 -2.9996078 -3.8180747 -4.8444238 -4.7703142 -4.1836343 -3.219064 -2.9300637 -2.9438729 -3.162672 -3.3419375 -3.8400271 -4.3320756 -4.6144257 -5.2001276]]...]
INFO - root - 2017-12-15 08:14:07.071222: step 34210, loss = 0.27, batch loss = 0.24 (34.5 examples/sec; 0.232 sec/batch; 19h:13m:20s remains)
INFO - root - 2017-12-15 08:14:09.322867: step 34220, loss = 0.22, batch loss = 0.19 (34.6 examples/sec; 0.231 sec/batch; 19h:09m:28s remains)
INFO - root - 2017-12-15 08:14:11.597475: step 34230, loss = 0.19, batch loss = 0.16 (36.6 examples/sec; 0.219 sec/batch; 18h:07m:07s remains)
INFO - root - 2017-12-15 08:14:13.868933: step 34240, loss = 0.17, batch loss = 0.14 (34.8 examples/sec; 0.230 sec/batch; 19h:02m:56s remains)
INFO - root - 2017-12-15 08:14:16.112403: step 34250, loss = 0.23, batch loss = 0.20 (34.8 examples/sec; 0.230 sec/batch; 19h:01m:07s remains)
INFO - root - 2017-12-15 08:14:18.410609: step 34260, loss = 0.29, batch loss = 0.26 (35.0 examples/sec; 0.228 sec/batch; 18h:54m:53s remains)
INFO - root - 2017-12-15 08:14:20.691879: step 34270, loss = 0.34, batch loss = 0.31 (33.7 examples/sec; 0.238 sec/batch; 19h:41m:09s remains)
INFO - root - 2017-12-15 08:14:22.970720: step 34280, loss = 0.21, batch loss = 0.17 (34.3 examples/sec; 0.233 sec/batch; 19h:18m:28s remains)
INFO - root - 2017-12-15 08:14:25.317504: step 34290, loss = 0.33, batch loss = 0.30 (32.5 examples/sec; 0.246 sec/batch; 20h:24m:31s remains)
INFO - root - 2017-12-15 08:14:27.624241: step 34300, loss = 0.24, batch loss = 0.20 (35.8 examples/sec; 0.224 sec/batch; 18h:31m:41s remains)
2017-12-15 08:14:27.913192: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9574273 -4.6926541 -5.0084343 -4.907052 -4.4684591 -4.3030987 -4.2041154 -4.363471 -4.6785913 -4.6890268 -4.2205248 -3.3930063 -2.7646246 -2.6831353 -2.7635884][-3.298882 -4.4201727 -4.8212528 -4.8576365 -4.3837719 -4.1335745 -4.0998745 -4.3689842 -4.596633 -4.5608006 -4.356472 -3.8436587 -3.2856464 -3.2163091 -3.3556595][-2.5910945 -2.9917679 -3.3798821 -3.7414351 -3.4665453 -3.162653 -3.1882644 -3.5566757 -3.7623482 -3.7092237 -3.6999793 -3.6047854 -3.4498746 -3.6464953 -3.9023051][-1.7738351 -1.5626543 -2.1881061 -3.0886214 -3.0768898 -2.8729846 -3.0051546 -3.330585 -3.4112837 -3.2035623 -3.0609028 -3.1384439 -3.4352355 -4.0135145 -4.4347229][-0.83965039 -0.18852091 -0.89945292 -1.9389445 -1.9727662 -1.7128644 -1.8908634 -2.2901795 -2.4906278 -2.4372423 -2.2588382 -2.5101128 -3.3017497 -4.2834864 -4.8976011][-0.56794834 0.53343463 -0.015169382 -0.92348981 -0.83178043 -0.54903185 -0.68714213 -0.91515434 -1.2136127 -1.5117081 -1.6056437 -1.9452343 -2.78255 -3.9167838 -4.7085938][-0.5231576 0.47329187 0.16270399 -0.58400345 -0.38249493 0.043127537 0.0050034523 0.11917639 0.0013253689 -0.4613893 -0.85149622 -1.2558138 -1.9871641 -3.1570864 -4.0659642][-1.3897409 -0.056843042 -0.085390806 -0.72368944 -0.4380697 0.36909342 0.53482008 0.87983513 0.99972534 0.54276705 -0.075074196 -0.41577446 -0.887954 -1.8073664 -2.5415549][-2.6269815 -1.1587195 -0.92098653 -1.5534627 -1.5530188 -0.55505943 0.0060954094 0.64758658 1.1375339 1.0183532 0.49266481 0.30932713 0.093358994 -0.42522764 -0.80450761][-3.8234558 -2.3399749 -1.8810943 -2.5129519 -2.8797207 -2.058537 -1.3360804 -0.21302128 0.7453475 1.032903 0.83905411 1.0146492 1.0817044 1.067194 1.0766089][-4.7141266 -3.4591427 -2.92561 -3.3808768 -3.8700354 -3.1768618 -2.4051974 -1.053462 0.28708792 0.94568968 1.1975052 1.7152717 1.9113672 2.1068456 2.2086756][-5.1327057 -4.3377156 -4.0150089 -4.3351593 -4.7315617 -4.1548662 -3.3625526 -2.0274527 -0.59251153 0.22205257 0.6595335 1.2036555 1.3323596 1.5506361 1.6462348][-5.2229357 -4.8453188 -4.7456946 -4.8531914 -4.9701061 -4.445343 -3.8540637 -2.8282139 -1.6988268 -0.93584335 -0.39161277 0.059525013 0.22642016 0.4061017 0.30451417][-5.1369362 -4.9964628 -4.9856195 -5.0047913 -4.9268322 -4.4129725 -4.0488119 -3.4754555 -2.8511076 -2.3542817 -1.8544211 -1.524555 -1.4264752 -1.367507 -1.5713698][-4.7287116 -4.5771775 -4.5087223 -4.4973989 -4.3679113 -3.9080923 -3.7829816 -3.7624803 -3.7367268 -3.6579418 -3.397233 -3.2557783 -3.2176673 -3.2990742 -3.617342]]...]
INFO - root - 2017-12-15 08:14:30.156162: step 34310, loss = 0.17, batch loss = 0.14 (35.7 examples/sec; 0.224 sec/batch; 18h:34m:14s remains)
INFO - root - 2017-12-15 08:14:32.460761: step 34320, loss = 0.26, batch loss = 0.23 (35.7 examples/sec; 0.224 sec/batch; 18h:33m:24s remains)
INFO - root - 2017-12-15 08:14:34.709930: step 34330, loss = 0.24, batch loss = 0.21 (35.2 examples/sec; 0.228 sec/batch; 18h:50m:59s remains)
INFO - root - 2017-12-15 08:14:37.011320: step 34340, loss = 0.26, batch loss = 0.23 (34.3 examples/sec; 0.233 sec/batch; 19h:19m:56s remains)
INFO - root - 2017-12-15 08:14:39.314115: step 34350, loss = 0.19, batch loss = 0.16 (36.3 examples/sec; 0.220 sec/batch; 18h:13m:40s remains)
INFO - root - 2017-12-15 08:14:41.558284: step 34360, loss = 0.23, batch loss = 0.20 (36.2 examples/sec; 0.221 sec/batch; 18h:16m:55s remains)
INFO - root - 2017-12-15 08:14:43.840912: step 34370, loss = 0.19, batch loss = 0.16 (35.4 examples/sec; 0.226 sec/batch; 18h:41m:35s remains)
INFO - root - 2017-12-15 08:14:46.112313: step 34380, loss = 0.28, batch loss = 0.24 (35.9 examples/sec; 0.223 sec/batch; 18h:27m:37s remains)
INFO - root - 2017-12-15 08:14:48.414890: step 34390, loss = 0.22, batch loss = 0.19 (34.4 examples/sec; 0.233 sec/batch; 19h:17m:03s remains)
INFO - root - 2017-12-15 08:14:50.717061: step 34400, loss = 0.22, batch loss = 0.19 (35.3 examples/sec; 0.226 sec/batch; 18h:44m:53s remains)
2017-12-15 08:14:50.997343: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9500449 -3.3278034 -2.0472913 -1.0576642 -1.3697242 -2.364311 -2.57737 -2.7758007 -3.0108182 -3.3224039 -3.5936313 -3.7492459 -3.9136388 -3.9578247 -3.7126398][-3.2408295 -4.0113263 -2.9870138 -2.0388274 -2.3552239 -3.6440601 -3.950146 -4.1298227 -4.298048 -4.5695763 -4.8363237 -4.9973927 -5.1527472 -5.1218987 -4.6742353][-4.8779917 -4.9882364 -4.3337955 -3.7062912 -3.9099402 -5.0982285 -5.299531 -5.2561369 -5.1875811 -5.2579217 -5.4085612 -5.5249033 -5.6373081 -5.61669 -5.1221581][-6.1283851 -5.8373451 -5.5388508 -5.0748577 -5.0153236 -5.8210125 -5.6640759 -5.1748281 -4.7434263 -4.5233245 -4.5404434 -4.6629677 -4.76868 -4.8011212 -4.4122739][-7.02392 -6.4426384 -6.2575407 -5.7605171 -5.3223567 -5.5197983 -4.8098893 -3.7552466 -2.9916329 -2.6446846 -2.7017457 -3.0646834 -3.2322493 -3.3047447 -3.0550227][-7.37111 -6.5962753 -6.2142959 -5.3567686 -4.4218435 -4.02384 -2.699249 -1.0135195 -0.013264894 0.18886948 -0.16796637 -1.0833936 -1.4874617 -1.5935667 -1.4855832][-6.8654156 -6.2569227 -5.4917145 -4.100563 -2.6686163 -1.7727721 0.102283 2.3546698 3.3298318 2.9603512 2.1569822 0.67715025 -0.074336052 -0.21636391 -0.18997717][-6.4840622 -5.6158261 -4.3783641 -2.4381695 -0.64388931 0.49312091 2.6919 5.3278542 6.086133 4.906436 3.5710089 1.6473293 0.40676713 0.18650579 0.26561165][-5.7614021 -4.646327 -3.1066809 -0.85833073 1.1101468 2.3447154 4.4798565 7.0359907 7.3425856 5.3014574 3.44752 1.2893741 -0.39916611 -0.83030272 -0.5843519][-5.3199205 -4.2031169 -2.7272639 -0.5890125 1.2161272 2.1337993 3.7430623 5.92031 5.928628 3.5457056 1.6082497 -0.21022606 -1.9773551 -2.6451952 -2.3199294][-5.6508951 -4.8528852 -3.7890558 -2.1487246 -0.68607795 -0.065571308 1.0755601 2.7580764 2.6077693 0.39585137 -1.2554771 -2.6240005 -4.2330046 -5.0195355 -4.5395412][-6.5085917 -6.2086697 -5.6610661 -4.5496044 -3.3721652 -2.9291275 -2.1741683 -1.1064876 -1.4074047 -3.1398518 -4.44051 -5.4080033 -6.6419535 -7.1751528 -6.3823986][-7.0150309 -6.9839773 -6.814394 -6.1178303 -5.259346 -5.0773988 -4.6751966 -4.1779947 -4.5939474 -5.7921224 -6.5933628 -7.1408024 -7.8772726 -7.9734726 -6.8505793][-6.7424765 -6.6885691 -6.6543188 -6.1609983 -5.621191 -5.7936249 -5.6815405 -5.637845 -6.0751543 -6.7219515 -7.1145582 -7.3523331 -7.6217613 -7.3372755 -6.0564485][-6.069582 -5.937058 -5.7884092 -5.2283278 -4.928359 -5.4666052 -5.5072918 -5.6407919 -5.9290791 -6.201663 -6.3345814 -6.3859196 -6.3911581 -5.8941259 -4.6602163]]...]
INFO - root - 2017-12-15 08:14:53.270284: step 34410, loss = 0.30, batch loss = 0.26 (36.1 examples/sec; 0.222 sec/batch; 18h:21m:38s remains)
INFO - root - 2017-12-15 08:14:55.551641: step 34420, loss = 0.36, batch loss = 0.33 (35.2 examples/sec; 0.228 sec/batch; 18h:50m:40s remains)
INFO - root - 2017-12-15 08:14:57.820028: step 34430, loss = 0.26, batch loss = 0.23 (35.2 examples/sec; 0.227 sec/batch; 18h:49m:15s remains)
INFO - root - 2017-12-15 08:15:00.116310: step 34440, loss = 0.25, batch loss = 0.21 (34.4 examples/sec; 0.232 sec/batch; 19h:14m:01s remains)
INFO - root - 2017-12-15 08:15:02.418908: step 34450, loss = 0.23, batch loss = 0.20 (35.2 examples/sec; 0.227 sec/batch; 18h:49m:39s remains)
INFO - root - 2017-12-15 08:15:04.688239: step 34460, loss = 0.36, batch loss = 0.32 (35.6 examples/sec; 0.225 sec/batch; 18h:36m:02s remains)
INFO - root - 2017-12-15 08:15:06.935807: step 34470, loss = 0.21, batch loss = 0.18 (35.1 examples/sec; 0.228 sec/batch; 18h:50m:34s remains)
INFO - root - 2017-12-15 08:15:09.199085: step 34480, loss = 0.26, batch loss = 0.23 (34.4 examples/sec; 0.233 sec/batch; 19h:14m:57s remains)
INFO - root - 2017-12-15 08:15:11.493751: step 34490, loss = 0.29, batch loss = 0.26 (35.9 examples/sec; 0.223 sec/batch; 18h:26m:25s remains)
INFO - root - 2017-12-15 08:15:13.756112: step 34500, loss = 0.24, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 18h:31m:46s remains)
2017-12-15 08:15:14.024486: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7869692 -5.3143516 -5.3234472 -5.4021349 -5.4297981 -5.3878388 -5.3714781 -5.3653564 -5.3882418 -5.4236445 -5.3984261 -5.3286333 -5.2177796 -5.0553484 -4.8237333][-5.3025265 -5.9106741 -5.850481 -5.8773241 -5.7979317 -5.6465025 -5.5692353 -5.5332842 -5.5475388 -5.5717154 -5.6094623 -5.6650133 -5.6762218 -5.6077867 -5.4809542][-6.3143129 -5.8940754 -5.6992683 -5.5712962 -5.30312 -5.0442429 -4.925024 -4.9102278 -5.0178232 -5.1053963 -5.2493868 -5.485971 -5.6730528 -5.7101283 -5.722436][-6.6950178 -5.2453632 -4.8265524 -4.451848 -3.9718099 -3.6535969 -3.5501482 -3.6324892 -3.9538848 -4.2071371 -4.5335445 -5.0499263 -5.4765606 -5.6500854 -5.8557987][-6.1635518 -3.6782804 -3.0631711 -2.4681642 -1.8474356 -1.5507054 -1.5154078 -1.6855359 -2.2145617 -2.650141 -3.1880209 -4.0280695 -4.7043791 -5.0562344 -5.4975891][-4.7207041 -1.8854119 -1.1327822 -0.365811 0.34676528 0.60469818 0.60637808 0.38697267 -0.35170674 -0.99071515 -1.7173448 -2.7710941 -3.5910568 -4.037509 -4.5964289][-2.9911804 -0.48806679 0.34688878 1.2206178 1.9911664 2.2323859 2.2039382 1.9078557 0.97549462 0.17820096 -0.62414372 -1.6902528 -2.4638827 -2.8879762 -3.4491224][-2.5763738 -0.062238693 0.69334579 1.5395415 2.3367517 2.6370409 2.6112912 2.3000891 1.3357732 0.51734209 -0.22359753 -1.0653012 -1.5625074 -1.8285003 -2.2601671][-2.8702021 -0.79035032 -0.28529954 0.35838747 1.0838079 1.3941286 1.4158552 1.2073641 0.42908716 -0.3185966 -0.95590866 -1.4146396 -1.5462393 -1.5618081 -1.6986096][-3.5953798 -1.9742919 -1.7163879 -1.3285217 -0.81604052 -0.6437732 -0.70155275 -0.84960032 -1.4126738 -2.1437688 -2.7105019 -2.7914476 -2.5056956 -2.1495681 -1.9262165][-4.1379147 -2.7040839 -2.5370822 -2.3446667 -2.1423414 -2.2538948 -2.5803523 -2.7592273 -3.1551626 -3.8310719 -4.3106961 -4.1172056 -3.4772527 -2.7994235 -2.3096807][-4.4815779 -2.9149921 -2.6386094 -2.5358038 -2.6625867 -3.1339321 -3.7472608 -4.0344968 -4.3882885 -4.9720821 -5.3460765 -4.9483175 -4.1124449 -3.2034204 -2.5155716][-4.682847 -2.8513503 -2.3517985 -2.2222552 -2.5651474 -3.2618291 -4.0433664 -4.4976883 -4.9807539 -5.5688591 -5.9084225 -5.4891357 -4.64709 -3.6391339 -2.8172514][-4.8967991 -2.9097824 -2.1569042 -1.8276089 -2.1151121 -2.7473416 -3.4937668 -4.0832291 -4.7595744 -5.4329157 -5.8701458 -5.5984411 -4.8792553 -3.9274776 -3.1244481][-5.334878 -3.4466896 -2.5458674 -1.9408677 -1.9399474 -2.2756808 -2.772444 -3.3634429 -4.1602826 -4.9163427 -5.4560909 -5.3589454 -4.8250208 -4.0938849 -3.4623485]]...]
INFO - root - 2017-12-15 08:15:16.331678: step 34510, loss = 0.29, batch loss = 0.25 (32.8 examples/sec; 0.244 sec/batch; 20h:12m:35s remains)
INFO - root - 2017-12-15 08:15:18.592584: step 34520, loss = 0.22, batch loss = 0.18 (36.2 examples/sec; 0.221 sec/batch; 18h:18m:29s remains)
INFO - root - 2017-12-15 08:15:20.835473: step 34530, loss = 0.17, batch loss = 0.14 (36.0 examples/sec; 0.222 sec/batch; 18h:23m:09s remains)
INFO - root - 2017-12-15 08:15:23.140167: step 34540, loss = 0.24, batch loss = 0.21 (34.8 examples/sec; 0.230 sec/batch; 19h:00m:24s remains)
INFO - root - 2017-12-15 08:15:25.465134: step 34550, loss = 0.24, batch loss = 0.20 (34.3 examples/sec; 0.233 sec/batch; 19h:19m:15s remains)
INFO - root - 2017-12-15 08:15:27.750051: step 34560, loss = 0.24, batch loss = 0.21 (34.2 examples/sec; 0.234 sec/batch; 19h:22m:19s remains)
INFO - root - 2017-12-15 08:15:30.043178: step 34570, loss = 0.24, batch loss = 0.21 (34.6 examples/sec; 0.231 sec/batch; 19h:08m:13s remains)
INFO - root - 2017-12-15 08:15:32.333934: step 34580, loss = 0.32, batch loss = 0.29 (35.1 examples/sec; 0.228 sec/batch; 18h:52m:06s remains)
INFO - root - 2017-12-15 08:15:34.607329: step 34590, loss = 0.29, batch loss = 0.26 (35.6 examples/sec; 0.225 sec/batch; 18h:37m:09s remains)
INFO - root - 2017-12-15 08:15:36.875988: step 34600, loss = 0.21, batch loss = 0.17 (34.8 examples/sec; 0.230 sec/batch; 19h:01m:44s remains)
2017-12-15 08:15:37.169122: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.791085 -5.5134349 -5.8543744 -6.77199 -7.6769533 -8.2310085 -8.6266451 -8.8590422 -8.685358 -7.9627886 -7.3163071 -7.4280634 -7.8860159 -8.4611979 -8.8080158][-4.5687332 -5.4394035 -5.7206697 -6.59441 -7.2423496 -7.5180159 -7.832263 -8.1938477 -8.3805208 -8.1512213 -7.7504139 -7.82333 -8.1969833 -8.7514343 -9.1596441][-4.8256254 -4.8476133 -5.1284695 -5.8847 -6.0658374 -5.7790284 -5.5271988 -5.6742454 -6.221375 -6.8116894 -7.0878935 -7.3930092 -7.7330952 -8.2370186 -8.6475229][-4.5872421 -4.14266 -4.5299568 -5.197381 -4.8084641 -3.8231606 -2.6595929 -2.2275453 -2.8871303 -4.205308 -5.3100991 -6.0069532 -6.36753 -6.8693314 -7.2902975][-4.129034 -3.2903256 -3.6988184 -4.2468371 -3.2975516 -1.4967504 0.6679132 1.9207017 1.3465164 -0.6230104 -2.7272532 -4.0600166 -4.7837267 -5.4135027 -5.7642179][-3.710371 -2.6259456 -2.854701 -3.1498601 -1.6987745 0.83697486 3.7556517 5.6343784 5.2231426 2.751116 -0.18847203 -2.0965979 -3.20398 -3.972712 -4.37317][-3.4503369 -2.5945733 -2.7719791 -2.8502426 -1.131579 1.8277266 5.0842714 7.2436314 7.03671 4.5498085 1.3555341 -0.80745387 -2.1904955 -3.0123208 -3.4762664][-3.9378326 -3.2277031 -3.4337523 -3.358705 -1.6170866 1.4044676 4.3925571 6.2791214 6.0886927 3.9098818 1.0336864 -0.97689176 -2.3101211 -2.8765321 -3.2981448][-5.0991926 -4.53689 -4.7058187 -4.3871965 -2.6623135 0.045830488 2.2529161 3.4828374 3.1649926 1.3925478 -0.93265474 -2.6175635 -3.5682404 -3.55206 -3.6826646][-6.3851595 -6.1374741 -6.3498373 -5.8421078 -4.2171164 -2.1047077 -0.70718646 -0.089247227 -0.50581408 -1.8522098 -3.5722361 -4.8805833 -5.3963966 -4.8396311 -4.558197][-7.5794821 -7.6018972 -7.8890886 -7.4006367 -6.1011648 -4.5739985 -3.7070153 -3.37107 -3.7633083 -4.7714467 -6.0055208 -6.8883228 -6.9975138 -6.2144055 -5.773694][-8.1854753 -8.326952 -8.7421618 -8.4840231 -7.5703783 -6.4986553 -5.9308367 -5.7166409 -6.0059414 -6.7316628 -7.6209984 -8.1600094 -7.8722219 -7.1234064 -6.7329044][-8.2157993 -8.3089266 -8.8106642 -8.7546492 -8.1729832 -7.4448862 -7.066927 -6.9155807 -7.0983448 -7.5515375 -8.0803957 -8.26144 -7.725955 -7.0939913 -6.7948437][-7.5974808 -7.4653025 -7.9250088 -8.0182724 -7.69048 -7.0994177 -6.7410717 -6.5965586 -6.6578255 -6.9336452 -7.2583647 -7.25379 -6.7167387 -6.2892962 -6.1183367][-7.2584553 -6.8123045 -7.1677103 -7.3184347 -7.0758486 -6.5083208 -6.1149893 -5.89496 -5.822186 -6.0263529 -6.3870478 -6.4859095 -6.1012354 -5.8272738 -5.7463384]]...]
INFO - root - 2017-12-15 08:15:39.429689: step 34610, loss = 0.19, batch loss = 0.16 (35.9 examples/sec; 0.223 sec/batch; 18h:27m:34s remains)
INFO - root - 2017-12-15 08:15:41.695835: step 34620, loss = 0.24, batch loss = 0.21 (34.9 examples/sec; 0.229 sec/batch; 18h:59m:08s remains)
INFO - root - 2017-12-15 08:15:43.980794: step 34630, loss = 0.20, batch loss = 0.17 (35.3 examples/sec; 0.227 sec/batch; 18h:45m:10s remains)
INFO - root - 2017-12-15 08:15:46.246307: step 34640, loss = 0.20, batch loss = 0.17 (35.5 examples/sec; 0.225 sec/batch; 18h:38m:00s remains)
INFO - root - 2017-12-15 08:15:48.506384: step 34650, loss = 0.25, batch loss = 0.21 (35.5 examples/sec; 0.226 sec/batch; 18h:39m:51s remains)
INFO - root - 2017-12-15 08:15:50.759034: step 34660, loss = 0.20, batch loss = 0.17 (35.1 examples/sec; 0.228 sec/batch; 18h:50m:44s remains)
INFO - root - 2017-12-15 08:15:53.023390: step 34670, loss = 0.23, batch loss = 0.20 (36.2 examples/sec; 0.221 sec/batch; 18h:18m:12s remains)
INFO - root - 2017-12-15 08:15:55.314233: step 34680, loss = 0.27, batch loss = 0.24 (35.7 examples/sec; 0.224 sec/batch; 18h:30m:55s remains)
INFO - root - 2017-12-15 08:15:57.595254: step 34690, loss = 0.20, batch loss = 0.17 (35.0 examples/sec; 0.229 sec/batch; 18h:54m:17s remains)
INFO - root - 2017-12-15 08:15:59.891140: step 34700, loss = 0.20, batch loss = 0.17 (33.9 examples/sec; 0.236 sec/batch; 19h:31m:35s remains)
2017-12-15 08:16:00.192091: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0998726 -5.9562287 -6.2153625 -6.3575659 -6.4322968 -6.4779859 -6.4983177 -6.5013132 -6.4418111 -6.3171573 -6.0868106 -5.7952895 -5.6222677 -5.5885897 -5.62248][-5.4686279 -6.6441245 -6.8248043 -6.8661351 -6.9691706 -7.1007514 -7.1918883 -7.2726216 -7.2595696 -7.2154331 -7.0971842 -6.8727555 -6.7543917 -6.7699108 -6.7856174][-6.6052113 -6.9651656 -6.9714527 -6.8189654 -6.7709517 -6.7364922 -6.6978383 -6.7225027 -6.6997905 -6.7339764 -6.8542318 -6.9276824 -7.0096488 -7.1374679 -7.2167583][-7.1763196 -6.8261585 -6.6237831 -6.2305455 -5.9234743 -5.473258 -5.0220203 -4.8406124 -4.85966 -5.112648 -5.6155782 -6.2206168 -6.6943545 -6.9932213 -7.1438322][-7.2106051 -6.1133881 -5.7383318 -5.2149191 -4.7252617 -3.7338247 -2.5872431 -1.9752766 -2.0716345 -2.7564597 -3.8757668 -5.1334953 -6.0167313 -6.4110551 -6.5553179][-6.2930274 -4.8483152 -4.3693 -3.7522359 -3.1123159 -1.6421723 0.24334502 1.4074929 1.2554345 0.058021307 -1.8073701 -3.790024 -5.0121546 -5.42101 -5.5423946][-5.0024819 -3.6812582 -3.260849 -2.5859332 -1.7149388 0.17868233 2.7274237 4.4888711 4.4073863 2.7362051 0.18611503 -2.3656869 -3.8376861 -4.2853775 -4.4108095][-4.1989689 -2.9213066 -2.5965707 -1.8447735 -0.7500664 1.3244803 4.1940522 6.3508644 6.3644848 4.3689852 1.3354173 -1.5153332 -3.1005402 -3.5851583 -3.655848][-3.9672248 -2.8201602 -2.5773213 -1.8487728 -0.68283844 1.2118158 3.6888175 5.5941744 5.6697187 3.798933 0.90095019 -1.756049 -3.2688878 -3.7974048 -3.7953098][-4.3435 -3.4434915 -3.3923397 -2.8813989 -1.9379719 -0.49478304 1.3449016 2.7174196 2.7751346 1.321907 -0.87649 -2.9062049 -4.1289148 -4.4804354 -4.2320123][-4.8676586 -4.3070974 -4.4362345 -4.2049451 -3.6713128 -2.835566 -1.6508052 -0.71037281 -0.56498659 -1.4376214 -2.7080605 -3.8731816 -4.6522779 -4.7002754 -4.1729989][-5.212605 -4.8841333 -5.0641065 -4.9588041 -4.781723 -4.4948254 -3.8878117 -3.3025093 -3.07904 -3.4339175 -3.9050426 -4.4245839 -4.8865848 -4.5800004 -3.7065384][-4.8703365 -4.770113 -5.0947523 -5.1675138 -5.2145391 -5.1233864 -4.7558484 -4.4125767 -4.1594305 -4.2148328 -4.2056065 -4.264472 -4.3412838 -3.6565392 -2.5749786][-4.2404804 -4.1888351 -4.6531973 -4.8668432 -5.0047593 -4.9354191 -4.5951629 -4.2930422 -4.0937467 -4.0302649 -3.8442471 -3.7759714 -3.720592 -2.9766858 -2.0868981][-3.6894016 -3.3411524 -3.734931 -3.9960728 -4.1261158 -4.0027008 -3.726541 -3.5207729 -3.3897276 -3.3498726 -3.2358296 -3.243109 -3.1783822 -2.5844824 -2.0699573]]...]
INFO - root - 2017-12-15 08:16:02.438791: step 34710, loss = 0.26, batch loss = 0.23 (35.1 examples/sec; 0.228 sec/batch; 18h:49m:48s remains)
INFO - root - 2017-12-15 08:16:04.727132: step 34720, loss = 0.26, batch loss = 0.22 (34.7 examples/sec; 0.230 sec/batch; 19h:03m:39s remains)
INFO - root - 2017-12-15 08:16:07.016106: step 34730, loss = 0.28, batch loss = 0.24 (34.8 examples/sec; 0.230 sec/batch; 19h:02m:30s remains)
INFO - root - 2017-12-15 08:16:09.322815: step 34740, loss = 0.21, batch loss = 0.17 (34.0 examples/sec; 0.235 sec/batch; 19h:28m:21s remains)
INFO - root - 2017-12-15 08:16:11.621419: step 34750, loss = 0.23, batch loss = 0.20 (35.3 examples/sec; 0.227 sec/batch; 18h:45m:19s remains)
INFO - root - 2017-12-15 08:16:13.878488: step 34760, loss = 0.22, batch loss = 0.19 (32.8 examples/sec; 0.244 sec/batch; 20h:09m:01s remains)
INFO - root - 2017-12-15 08:16:16.150302: step 34770, loss = 0.20, batch loss = 0.17 (35.3 examples/sec; 0.226 sec/batch; 18h:42m:59s remains)
INFO - root - 2017-12-15 08:16:18.414929: step 34780, loss = 0.27, batch loss = 0.23 (35.9 examples/sec; 0.223 sec/batch; 18h:26m:16s remains)
INFO - root - 2017-12-15 08:16:20.732957: step 34790, loss = 0.28, batch loss = 0.25 (33.8 examples/sec; 0.237 sec/batch; 19h:35m:19s remains)
INFO - root - 2017-12-15 08:16:22.978850: step 34800, loss = 0.29, batch loss = 0.26 (36.0 examples/sec; 0.222 sec/batch; 18h:23m:54s remains)
2017-12-15 08:16:23.307152: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4960943 -2.615968 -2.0686247 -0.58145308 -0.10543847 -0.55246627 -0.68631589 -0.036569834 0.10679269 0.25975752 -0.052072763 -1.5514257 -3.5726666 -3.7524042 -3.5496135][-2.4176328 -3.2663884 -2.6494365 -0.83162594 -0.13445687 -0.34511566 -0.44056714 -0.25672567 -0.62943888 -0.61956346 -1.0364839 -2.1663256 -3.7105861 -3.5291061 -2.9624939][-3.2667644 -3.9949579 -3.6612437 -1.636062 -0.62078977 -0.437474 -0.4926604 -0.576949 -1.3842535 -1.7291828 -2.393966 -3.4126689 -4.4781556 -3.5903659 -2.5906096][-4.9829292 -5.5189438 -5.5061064 -3.3608656 -1.7252479 -0.96946728 -0.45053482 -0.31454253 -1.4204769 -2.307972 -3.3363361 -4.5479169 -5.30532 -3.9613891 -2.7770753][-6.8553238 -7.1297026 -7.3582783 -4.8998628 -2.5030532 -0.94303465 0.54176378 1.1396794 -0.27709508 -1.5476142 -2.8484812 -4.4299498 -5.2024879 -4.1262312 -3.1684756][-7.822854 -7.7705059 -7.7701931 -4.9835372 -2.173008 -0.028015137 2.3950403 3.3289154 1.5592599 -0.13969874 -1.7680306 -3.8647852 -5.1085873 -4.6416955 -3.9428453][-7.0150232 -6.9855824 -6.518487 -3.7908075 -1.1131687 1.5473762 4.6640825 5.69853 3.5279396 1.2878375 -0.97116959 -3.6938448 -5.4595757 -5.4666538 -4.7832632][-5.8659234 -5.3932447 -4.7122936 -2.3864775 -0.1066792 2.8283255 6.3127193 7.2701759 4.9133329 2.1990025 -0.77355003 -3.8092427 -5.6974397 -6.1520624 -5.6238489][-4.5338106 -3.8216779 -3.3055608 -1.8921406 -0.44597459 2.2448723 5.5374975 6.4347343 4.3230743 1.6496189 -1.5716746 -4.3999267 -6.1472321 -6.9354229 -6.50607][-3.6768312 -3.026041 -2.7757764 -2.1845956 -1.5630206 0.40312266 2.8747857 3.8943207 2.6417129 0.26829553 -3.0081966 -5.2662182 -6.5387983 -7.3076773 -6.7856979][-3.4409938 -2.8762102 -2.9646 -2.9100523 -2.8739502 -1.8356464 -0.21689272 0.97717047 0.41908789 -1.5978634 -4.5838051 -6.0197487 -6.5977449 -7.0898933 -6.5625277][-3.9579983 -3.4859567 -3.9315152 -4.2103934 -4.4820528 -4.163219 -3.230732 -2.0838261 -2.2653151 -4.0187311 -6.2384582 -6.7608519 -6.7390842 -7.0129013 -6.6480246][-4.7227082 -4.3371391 -4.85144 -5.0555906 -5.1202021 -5.1212196 -4.6926656 -3.9561651 -4.0161057 -5.219862 -6.5356007 -6.5679579 -6.617322 -6.930995 -6.7185659][-5.4630966 -5.0936642 -5.4822416 -5.5346069 -5.4666538 -5.5503531 -5.4178867 -5.0459523 -4.98666 -5.5704794 -6.369132 -6.2813368 -6.4811964 -6.749733 -6.5297995][-6.6000404 -6.0312438 -6.1973305 -6.1719494 -6.128582 -6.2446647 -6.163774 -5.9326582 -5.9248152 -6.113318 -6.3838186 -6.4226246 -6.5962934 -6.7299919 -6.6437969]]...]
INFO - root - 2017-12-15 08:16:25.625580: step 34810, loss = 0.20, batch loss = 0.17 (34.2 examples/sec; 0.234 sec/batch; 19h:19m:11s remains)
INFO - root - 2017-12-15 08:16:27.881103: step 34820, loss = 0.29, batch loss = 0.25 (35.0 examples/sec; 0.229 sec/batch; 18h:54m:52s remains)
INFO - root - 2017-12-15 08:16:30.158529: step 34830, loss = 0.22, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 18h:25m:51s remains)
INFO - root - 2017-12-15 08:16:32.450038: step 34840, loss = 0.17, batch loss = 0.14 (36.3 examples/sec; 0.220 sec/batch; 18h:13m:27s remains)
INFO - root - 2017-12-15 08:16:34.706071: step 34850, loss = 0.21, batch loss = 0.18 (35.6 examples/sec; 0.225 sec/batch; 18h:35m:06s remains)
INFO - root - 2017-12-15 08:16:36.944689: step 34860, loss = 0.24, batch loss = 0.21 (35.0 examples/sec; 0.229 sec/batch; 18h:54m:02s remains)
INFO - root - 2017-12-15 08:16:39.257016: step 34870, loss = 0.17, batch loss = 0.14 (32.4 examples/sec; 0.247 sec/batch; 20h:23m:05s remains)
INFO - root - 2017-12-15 08:16:41.519190: step 34880, loss = 0.24, batch loss = 0.21 (35.6 examples/sec; 0.225 sec/batch; 18h:34m:56s remains)
INFO - root - 2017-12-15 08:16:43.759687: step 34890, loss = 0.18, batch loss = 0.15 (35.5 examples/sec; 0.226 sec/batch; 18h:38m:33s remains)
INFO - root - 2017-12-15 08:16:46.067774: step 34900, loss = 0.22, batch loss = 0.19 (35.7 examples/sec; 0.224 sec/batch; 18h:32m:24s remains)
2017-12-15 08:16:46.331236: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.616629 0.12345481 0.51380825 0.400553 -0.60620844 -1.6422614 -2.3500245 -3.1334913 -3.5835381 -2.4369135 -1.0461631 -0.37532389 -0.03032589 -0.73451495 -2.4701407][-1.6231098 -0.4669776 0.43714 0.84840417 0.17364597 -0.7026484 -1.2344748 -1.934648 -2.4343696 -1.6635838 -0.58420467 0.23659039 0.65053391 -0.13128805 -2.0397477][-3.4139876 -1.584723 -0.094368935 0.93792129 0.75359368 0.1113894 -0.5372436 -1.369225 -1.9919538 -1.730559 -0.97221315 -0.0029563904 0.54634118 -0.299747 -2.1814928][-5.0139151 -2.8651562 -1.0058007 0.57084084 0.85916257 0.58661056 -0.047787428 -0.92531288 -1.6188419 -1.7295072 -1.3499155 -0.42133737 0.11814237 -0.71958959 -2.3807008][-5.6676054 -3.2053261 -1.3353236 0.42566252 0.92437768 0.96710944 0.45900655 -0.42275763 -1.1870373 -1.5741398 -1.5997789 -0.75817275 -0.36550832 -1.1955353 -2.5430524][-5.978374 -3.2919641 -1.3867919 0.45960975 1.0762541 1.345036 1.1631708 0.33650303 -0.52428806 -1.1018599 -1.4400389 -0.81170487 -0.53936028 -1.3009633 -2.2493119][-6.1432242 -3.6351261 -1.7737486 0.17513919 0.90027952 1.3828378 1.655756 1.0299568 0.17689562 -0.40730691 -0.84138083 -0.45711637 -0.21928763 -0.78414416 -1.3780587][-6.7913442 -4.3580642 -2.6429689 -0.68140554 0.32117677 1.0406594 1.7215765 1.2999198 0.48933482 -0.081119537 -0.63778317 -0.57957637 -0.40313447 -0.70767653 -1.0522791][-7.1664429 -4.9776688 -3.7221637 -2.0467761 -0.83333278 0.016047239 0.94010639 0.64978004 -0.18509531 -0.71038234 -1.2478257 -1.3650854 -1.0979527 -1.0714719 -1.1402175][-6.5572114 -4.5628152 -3.8721519 -2.7927094 -1.7536209 -0.98591995 -0.0047850609 -0.32121658 -1.3207884 -1.9323485 -2.3703709 -2.4667559 -2.0249598 -1.588506 -1.402912][-5.4474478 -3.5041211 -3.3106632 -3.0193253 -2.4851766 -1.9198147 -0.99956608 -1.2022752 -2.087765 -2.6366966 -3.0332878 -3.1219792 -2.6766019 -2.1187572 -1.9079926][-4.1652088 -2.1970658 -2.4621625 -2.9041898 -2.9814873 -2.8387024 -2.2269685 -2.336467 -3.0998716 -3.6290464 -4.0955377 -4.2814856 -4.024394 -3.4433689 -2.9760807][-3.7813673 -1.4596112 -1.7690341 -2.4224637 -2.9122357 -3.0008357 -2.6736138 -2.8220994 -3.7422853 -4.4806376 -5.2166538 -5.6329956 -5.6341286 -5.17664 -4.4949112][-4.1537547 -1.512749 -1.6339087 -2.2153502 -2.8445086 -2.8248634 -2.502177 -2.491195 -3.4495718 -4.5049877 -5.5698066 -6.2455158 -6.4807048 -6.1055822 -5.301825][-4.7056327 -2.2250104 -2.1246867 -2.4225078 -2.9612687 -2.7066281 -2.2870579 -2.0358169 -2.8696558 -4.1637917 -5.502203 -6.4426889 -6.8272185 -6.3482361 -5.2799034]]...]
INFO - root - 2017-12-15 08:16:48.576418: step 34910, loss = 0.19, batch loss = 0.16 (33.7 examples/sec; 0.237 sec/batch; 19h:36m:46s remains)
INFO - root - 2017-12-15 08:16:50.875452: step 34920, loss = 0.25, batch loss = 0.22 (36.0 examples/sec; 0.222 sec/batch; 18h:23m:16s remains)
INFO - root - 2017-12-15 08:16:53.142408: step 34930, loss = 0.21, batch loss = 0.18 (34.3 examples/sec; 0.233 sec/batch; 19h:15m:18s remains)
INFO - root - 2017-12-15 08:16:55.437397: step 34940, loss = 0.23, batch loss = 0.20 (35.0 examples/sec; 0.228 sec/batch; 18h:52m:08s remains)
INFO - root - 2017-12-15 08:16:57.705601: step 34950, loss = 0.33, batch loss = 0.29 (35.3 examples/sec; 0.226 sec/batch; 18h:43m:08s remains)
INFO - root - 2017-12-15 08:16:59.995489: step 34960, loss = 0.18, batch loss = 0.15 (35.6 examples/sec; 0.225 sec/batch; 18h:34m:07s remains)
INFO - root - 2017-12-15 08:17:02.284891: step 34970, loss = 0.39, batch loss = 0.36 (36.1 examples/sec; 0.222 sec/batch; 18h:19m:43s remains)
INFO - root - 2017-12-15 08:17:04.568390: step 34980, loss = 0.29, batch loss = 0.26 (34.1 examples/sec; 0.235 sec/batch; 19h:23m:04s remains)
INFO - root - 2017-12-15 08:17:06.857758: step 34990, loss = 0.25, batch loss = 0.22 (35.1 examples/sec; 0.228 sec/batch; 18h:50m:58s remains)
INFO - root - 2017-12-15 08:17:09.176703: step 35000, loss = 0.28, batch loss = 0.25 (35.2 examples/sec; 0.227 sec/batch; 18h:45m:48s remains)
2017-12-15 08:17:09.444857: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3873919 -4.2381735 -4.6481271 -5.2026777 -5.5790329 -5.7634521 -5.7020874 -5.6192279 -5.500617 -5.3603687 -5.4044642 -5.7020121 -5.926455 -5.9740076 -5.7105703][-2.6155186 -4.896678 -5.4552712 -6.0930748 -6.3623152 -6.3858523 -6.1343956 -5.7852907 -5.5025954 -5.3945837 -5.45616 -5.71614 -5.750926 -5.5680537 -5.1381989][-4.1588798 -5.5328093 -6.1428552 -6.716692 -6.6777582 -6.2810159 -5.5528965 -4.7444477 -4.3241138 -4.4707794 -4.7719049 -5.1942673 -5.256166 -5.0760822 -4.780951][-5.6394405 -6.1658173 -6.7076135 -6.9781 -6.3205886 -5.1509504 -3.628911 -2.2861962 -1.9406104 -2.6376891 -3.3797624 -4.1377554 -4.4938936 -4.6009893 -4.6870155][-7.0955319 -6.6829762 -6.9592009 -6.7095594 -5.2689376 -3.2873714 -1.1024067 0.46003509 0.397043 -0.88185942 -2.1044891 -3.373384 -4.076705 -4.4228621 -4.8789043][-7.4101233 -6.4563322 -6.3386683 -5.5995774 -3.6805406 -1.3314298 1.2158949 2.9821939 2.6810727 1.1580155 -0.50891566 -2.3215065 -3.5108707 -4.1777763 -5.0007458][-6.7256522 -5.6783109 -5.3144178 -4.3619537 -2.2598314 0.13327861 2.8704419 4.858532 4.56712 3.0172396 0.95004392 -1.4668567 -3.2060719 -4.1791482 -5.1937418][-6.3655047 -5.1237097 -4.7617445 -3.7963572 -1.5894403 0.63033724 3.3887591 5.6328034 5.4008374 3.6542573 1.228909 -1.5349126 -3.4095778 -4.3475523 -5.198123][-5.5918355 -4.5580735 -4.4601455 -3.6258731 -1.4660513 0.39199686 3.0213022 5.3417497 5.011281 3.0081282 0.57503176 -1.9809796 -3.554121 -4.1917105 -4.5923429][-4.9604921 -4.3879938 -4.6547174 -4.0649052 -2.119432 -0.59153092 1.8468838 3.9543071 3.4987435 1.3958852 -0.85281467 -2.9175234 -3.8862395 -4.0524988 -3.961473][-4.6326566 -4.4312716 -4.9899492 -4.6424341 -2.9524593 -1.5704772 0.46102047 1.8800402 1.2197335 -0.83027411 -2.6874838 -4.0170803 -4.2607107 -3.8551903 -3.3693349][-4.4269342 -4.7182474 -5.5190344 -5.1888075 -3.5973921 -2.2331867 -0.831465 -0.23847008 -1.0647055 -2.7736812 -3.9865687 -4.51495 -4.1564355 -3.4637184 -3.0495713][-4.6972122 -5.2164364 -5.9819131 -5.5676174 -4.1653509 -3.0492089 -2.2371871 -2.1642175 -2.8661931 -3.9253101 -4.4373255 -4.40304 -3.7943194 -3.3039193 -3.2912195][-5.1771812 -5.4737916 -5.8761868 -5.3521547 -4.2546654 -3.4222167 -2.9385624 -3.0202446 -3.3805971 -3.8044596 -3.8224533 -3.6714735 -3.3935914 -3.3484848 -3.7164001][-5.4495654 -5.3029709 -5.3065019 -4.7453418 -3.8849387 -3.1808743 -2.8677649 -2.8579452 -2.883075 -2.9325817 -2.7752659 -2.8878765 -3.1437407 -3.5702872 -4.1026897]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-35000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-35000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 08:17:12.058472: step 35010, loss = 0.21, batch loss = 0.18 (33.8 examples/sec; 0.237 sec/batch; 19h:34m:07s remains)
INFO - root - 2017-12-15 08:17:14.306068: step 35020, loss = 0.17, batch loss = 0.14 (35.9 examples/sec; 0.223 sec/batch; 18h:23m:52s remains)
INFO - root - 2017-12-15 08:17:16.561466: step 35030, loss = 0.23, batch loss = 0.20 (35.1 examples/sec; 0.228 sec/batch; 18h:49m:13s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 08:17:18.793600: step 35040, loss = 0.25, batch loss = 0.22 (35.3 examples/sec; 0.227 sec/batch; 18h:44m:38s remains)
INFO - root - 2017-12-15 08:17:21.058930: step 35050, loss = 0.24, batch loss = 0.21 (34.7 examples/sec; 0.230 sec/batch; 19h:02m:34s remains)
INFO - root - 2017-12-15 08:17:23.347339: step 35060, loss = 0.30, batch loss = 0.26 (34.4 examples/sec; 0.232 sec/batch; 19h:11m:50s remains)
INFO - root - 2017-12-15 08:17:25.614301: step 35070, loss = 0.40, batch loss = 0.37 (35.9 examples/sec; 0.223 sec/batch; 18h:25m:43s remains)
INFO - root - 2017-12-15 08:17:27.867107: step 35080, loss = 0.28, batch loss = 0.24 (35.3 examples/sec; 0.227 sec/batch; 18h:42m:47s remains)
INFO - root - 2017-12-15 08:17:30.155201: step 35090, loss = 0.31, batch loss = 0.28 (36.3 examples/sec; 0.220 sec/batch; 18h:12m:07s remains)
INFO - root - 2017-12-15 08:17:32.424105: step 35100, loss = 0.20, batch loss = 0.16 (35.4 examples/sec; 0.226 sec/batch; 18h:40m:41s remains)
2017-12-15 08:17:32.726799: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1712284 -7.149642 -7.3987179 -8.007596 -8.4028044 -8.3795567 -8.3040056 -7.7927475 -7.1882505 -6.2062731 -4.8033891 -3.8984456 -3.8798914 -3.4775958 -3.6060598][-4.9057989 -7.4121332 -7.621233 -8.1454105 -8.5954647 -8.8569031 -8.7141924 -8.085475 -7.3716764 -6.2524323 -4.7580681 -3.9491699 -4.2230864 -4.1765079 -4.5241489][-5.8407421 -7.4590588 -7.4345341 -7.8209181 -8.2923641 -8.49874 -8.2562323 -7.5458889 -6.7948914 -5.7233543 -4.363039 -3.7294912 -4.1029153 -4.0740457 -4.5193033][-6.4720058 -7.225522 -7.1256723 -7.3751793 -7.5768528 -7.2876282 -6.6244993 -5.6862078 -4.8738394 -3.9554863 -3.0183959 -2.725425 -3.172812 -3.1087232 -3.474824][-6.8182468 -6.8518476 -6.8838625 -6.967803 -6.542325 -5.4146605 -4.204319 -2.9992671 -2.1738307 -1.6862698 -1.46976 -1.5389876 -2.0824573 -1.9457369 -2.273613][-6.2237711 -6.0801 -6.2349663 -6.2577181 -5.3835678 -3.4523461 -1.6814859 -0.033720255 0.86749649 0.90455365 0.40984416 -0.20561695 -0.78406394 -0.55377066 -0.79004824][-5.1881256 -5.2743063 -5.4679012 -5.4876356 -4.385581 -1.8765434 0.53581619 2.6291378 3.6171386 3.1721375 1.8893826 0.59050131 0.066108465 0.45703316 0.59770989][-4.7283106 -4.7477918 -4.8918219 -4.9630852 -3.7712965 -0.8664186 2.0907524 4.54082 5.5356579 4.5547266 2.3770449 0.39059377 -0.21034455 0.2883091 0.89781475][-4.6381469 -4.6434031 -4.7369814 -4.7903214 -3.7022233 -0.81582904 2.3016102 4.6948404 5.4525185 4.1419182 1.6028404 -0.74646115 -1.3059525 -0.55739939 0.35085964][-5.104897 -5.1857071 -5.2933774 -5.2589984 -4.3106418 -1.8130672 0.894824 2.7662852 3.0321128 1.6775143 -0.66557014 -2.8535616 -3.2060747 -2.2089341 -1.1330998][-5.6708107 -6.0235577 -6.2767115 -6.1416674 -5.4052248 -3.5009315 -1.4883091 -0.18486834 -0.12281418 -1.2535721 -3.1771445 -4.9645491 -5.2068586 -4.1979084 -3.0796502][-6.3041077 -6.7152414 -6.9765778 -6.6874332 -6.1206608 -4.9531112 -3.9303684 -3.0938659 -3.0757489 -3.9464216 -5.3282886 -6.5780888 -6.7344246 -5.755846 -4.6531773][-6.6796865 -7.1708403 -7.3665113 -7.0134335 -6.5630913 -5.8528023 -5.3841085 -4.6792455 -4.4581761 -5.0654259 -5.9577284 -6.9292984 -7.165554 -6.3920727 -5.45584][-6.6430254 -6.9044743 -7.0341253 -6.7727442 -6.477623 -5.9343705 -5.5120144 -4.7336063 -4.2821455 -4.604701 -5.324604 -6.3560085 -6.7384787 -6.1938772 -5.5546107][-5.6938429 -5.5090046 -5.6249123 -5.4427338 -5.3238573 -4.9307451 -4.622035 -4.0996838 -3.7283254 -3.8560705 -4.3720765 -5.3852444 -5.8536568 -5.5453615 -5.1548395]]...]
INFO - root - 2017-12-15 08:17:34.982068: step 35110, loss = 0.22, batch loss = 0.19 (35.0 examples/sec; 0.229 sec/batch; 18h:52m:49s remains)
INFO - root - 2017-12-15 08:17:37.260878: step 35120, loss = 0.21, batch loss = 0.18 (33.8 examples/sec; 0.237 sec/batch; 19h:34m:31s remains)
INFO - root - 2017-12-15 08:17:39.556745: step 35130, loss = 0.24, batch loss = 0.21 (35.9 examples/sec; 0.223 sec/batch; 18h:25m:47s remains)
INFO - root - 2017-12-15 08:17:41.824507: step 35140, loss = 0.29, batch loss = 0.26 (35.6 examples/sec; 0.225 sec/batch; 18h:35m:15s remains)
INFO - root - 2017-12-15 08:17:44.100289: step 35150, loss = 0.27, batch loss = 0.24 (33.5 examples/sec; 0.239 sec/batch; 19h:44m:50s remains)
INFO - root - 2017-12-15 08:17:46.366378: step 35160, loss = 0.24, batch loss = 0.21 (35.0 examples/sec; 0.229 sec/batch; 18h:53m:34s remains)
INFO - root - 2017-12-15 08:17:48.656146: step 35170, loss = 0.18, batch loss = 0.15 (35.5 examples/sec; 0.225 sec/batch; 18h:35m:37s remains)
INFO - root - 2017-12-15 08:17:50.913560: step 35180, loss = 0.25, batch loss = 0.22 (36.5 examples/sec; 0.219 sec/batch; 18h:04m:54s remains)
INFO - root - 2017-12-15 08:17:53.172778: step 35190, loss = 0.22, batch loss = 0.19 (34.3 examples/sec; 0.233 sec/batch; 19h:15m:52s remains)
INFO - root - 2017-12-15 08:17:55.491821: step 35200, loss = 0.25, batch loss = 0.22 (34.8 examples/sec; 0.230 sec/batch; 18h:59m:05s remains)
2017-12-15 08:17:55.794907: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0983377 -5.3202925 -5.2832937 -4.7626829 -3.5426283 -2.5280626 -2.0935435 -2.0555782 -1.5345645 -0.75353265 -0.22790837 -0.67634034 -2.0173719 -3.387342 -4.1272774][-5.0129538 -5.6330357 -5.4827919 -4.8902655 -3.5638046 -2.3480563 -1.7499878 -1.8312945 -1.6747775 -1.3475354 -1.1074319 -1.5409653 -2.3788898 -3.2320244 -3.7083135][-5.3190041 -5.5148106 -5.0992651 -4.4937372 -3.2936831 -2.0491002 -1.2466675 -1.4172359 -1.8780911 -2.2198112 -2.3758588 -2.590462 -2.7276239 -2.9174011 -2.9127152][-5.80052 -5.4893789 -4.6501474 -3.6350887 -2.2939157 -1.0159749 -0.14525318 -0.43389833 -1.5409343 -2.5979462 -3.3048391 -3.3454943 -3.0378108 -2.6361017 -2.1201499][-6.6577091 -5.7613688 -4.6983194 -3.1847003 -1.469137 0.088906527 1.2463658 1.0991991 -0.29385853 -1.9971135 -3.3342257 -3.5474815 -3.0897207 -2.1204212 -1.0691534][-6.8274288 -5.8404222 -4.7543116 -3.0253744 -1.0836084 0.63801813 2.0946362 2.2655828 0.61206579 -1.6599874 -3.5483592 -3.939312 -3.165586 -1.4796079 0.23719192][-6.5318089 -5.4186115 -4.373929 -2.6881263 -0.719069 1.1295393 2.9207299 3.4310396 1.6443682 -1.0300883 -3.3179107 -3.9501238 -2.9967222 -0.9355253 1.1072659][-6.0026827 -4.9394441 -3.9325423 -2.258749 -0.19335508 1.9082911 4.0227709 4.6744556 2.8345659 -0.17068195 -2.7145684 -3.5356083 -2.6453757 -0.55651963 1.3230414][-5.5267735 -4.5014067 -3.6345673 -2.074122 -0.0021612644 2.25396 4.4368877 5.1216373 3.3861706 0.47360158 -2.0563495 -3.0179775 -2.3611252 -0.79212928 0.77030468][-5.2557116 -4.2003565 -3.3672149 -2.0508306 -0.25448823 1.7483113 3.5886714 4.2751856 3.0457594 0.64033031 -1.6078309 -2.6607623 -2.1815157 -1.1732497 -0.090357065][-5.0757065 -3.883728 -2.9854 -1.9342489 -0.49439323 1.0557969 2.397227 2.9278443 2.1823862 0.45895648 -1.3232683 -2.2592082 -1.9416476 -1.350667 -0.62778771][-5.1068664 -3.8551245 -2.8598135 -2.076273 -0.97140718 0.10987306 1.0550883 1.4483912 1.0115993 -0.250947 -1.6153729 -2.1578751 -1.8211267 -1.5246139 -1.0193509][-5.1539168 -3.8422585 -2.7992082 -2.3201661 -1.4550979 -0.6971426 0.0477705 0.44565678 0.21429729 -0.78155875 -1.9059567 -2.2366486 -1.8847606 -1.7840588 -1.3875322][-4.9994574 -3.5279102 -2.4008431 -2.0921457 -1.4476974 -0.95210147 -0.38663828 0.10877085 0.19732261 -0.59133995 -1.4859306 -1.7176102 -1.4951005 -1.65195 -1.3702264][-4.798378 -3.3011327 -2.0543518 -1.6070999 -0.94016373 -0.71381462 -0.39399242 -0.009175539 0.22384524 -0.43239558 -1.3140321 -1.4758726 -1.1349671 -1.3936017 -1.2276517]]...]
INFO - root - 2017-12-15 08:17:58.050380: step 35210, loss = 0.22, batch loss = 0.19 (35.7 examples/sec; 0.224 sec/batch; 18h:29m:17s remains)
INFO - root - 2017-12-15 08:18:00.362123: step 35220, loss = 0.23, batch loss = 0.20 (34.4 examples/sec; 0.233 sec/batch; 19h:12m:23s remains)
INFO - root - 2017-12-15 08:18:02.674208: step 35230, loss = 0.18, batch loss = 0.15 (34.7 examples/sec; 0.231 sec/batch; 19h:03m:33s remains)
INFO - root - 2017-12-15 08:18:04.973634: step 35240, loss = 0.26, batch loss = 0.23 (34.0 examples/sec; 0.235 sec/batch; 19h:25m:57s remains)
INFO - root - 2017-12-15 08:18:07.257778: step 35250, loss = 0.25, batch loss = 0.22 (35.3 examples/sec; 0.226 sec/batch; 18h:41m:10s remains)
INFO - root - 2017-12-15 08:18:09.537893: step 35260, loss = 0.28, batch loss = 0.25 (36.2 examples/sec; 0.221 sec/batch; 18h:15m:04s remains)
INFO - root - 2017-12-15 08:18:11.812194: step 35270, loss = 0.36, batch loss = 0.33 (34.0 examples/sec; 0.235 sec/batch; 19h:25m:01s remains)
INFO - root - 2017-12-15 08:18:14.083925: step 35280, loss = 0.24, batch loss = 0.21 (35.8 examples/sec; 0.223 sec/batch; 18h:25m:38s remains)
INFO - root - 2017-12-15 08:18:16.371228: step 35290, loss = 0.33, batch loss = 0.30 (35.0 examples/sec; 0.229 sec/batch; 18h:52m:25s remains)
INFO - root - 2017-12-15 08:18:18.643828: step 35300, loss = 0.41, batch loss = 0.38 (35.6 examples/sec; 0.225 sec/batch; 18h:33m:22s remains)
2017-12-15 08:18:18.927094: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4320269 -7.0205512 -7.7829933 -8.0277843 -8.0189257 -7.598772 -6.7336187 -6.161293 -6.2208743 -6.37303 -6.0107851 -5.0972166 -4.4222431 -3.4569893 -1.7300217][-5.9338903 -7.9984303 -8.8928289 -9.12291 -8.962204 -8.2446766 -7.1651654 -6.5112691 -6.6946392 -6.9703894 -6.6770229 -5.7325578 -4.8785009 -3.9119627 -2.3384557][-7.2771535 -8.3678045 -9.2554169 -9.4099751 -9.0714369 -8.0007238 -6.4809065 -5.6296568 -6.1303911 -6.7169352 -6.8871017 -6.3716207 -5.6523447 -4.9392061 -3.9101477][-7.5618224 -7.6280928 -7.928277 -7.4801178 -6.545754 -4.7345629 -2.4847224 -1.6794622 -3.046001 -4.8044639 -6.1824651 -6.5541511 -6.0686193 -5.470017 -4.88239][-7.0416503 -6.6353583 -6.4167595 -5.40469 -3.7324102 -1.0302042 2.1264129 3.1539483 1.066349 -2.1171827 -5.0382061 -6.4361281 -6.1448693 -5.7928877 -5.7182741][-6.9898629 -6.34036 -5.8784409 -4.5228748 -2.1716452 1.6682367 6.0619659 7.8370686 5.6474814 1.2693186 -3.0761092 -5.4607859 -5.6345682 -5.8395977 -6.3589673][-6.8239336 -6.2210054 -5.2713475 -3.5245762 -0.68687427 3.9221697 9.1458693 11.533728 9.361599 4.169148 -1.1397147 -4.1910505 -4.7775974 -5.4737654 -6.374784][-6.7982078 -5.8164225 -4.6605997 -3.0979486 -0.56727052 3.7262416 8.6178083 10.973624 8.9501343 3.7349706 -1.4133496 -4.1988273 -4.8653917 -5.5906763 -6.3347926][-7.1542516 -6.1296167 -5.2540703 -4.396142 -2.6247272 0.89130926 5.0200167 7.1968 5.6311874 1.4457009 -2.4813955 -4.4814897 -5.073348 -5.8010607 -6.2441158][-7.6844563 -6.4911823 -5.6648932 -5.0075188 -3.7061691 -1.1021987 1.9881606 3.4849572 2.0668845 -1.3220142 -4.1969843 -5.66239 -6.2127113 -6.4981351 -6.3707228][-7.4060373 -6.1409698 -5.57191 -5.2524829 -4.5731716 -3.1671927 -1.5353695 -1.1709141 -2.8817797 -5.5171404 -7.3622108 -8.11875 -7.8965631 -7.1158295 -6.1826735][-6.5050492 -5.4876795 -5.5140471 -5.8174057 -5.7625504 -5.243392 -4.595211 -5.0324349 -6.7474813 -8.5326376 -9.260479 -9.1360416 -7.8007827 -5.8796811 -4.4750528][-5.9342875 -5.0568247 -5.3061318 -5.9061852 -6.0974364 -5.7414665 -5.2324872 -5.7118316 -7.2530522 -8.4921932 -8.7983961 -8.3580027 -6.2927585 -3.7235143 -2.1882875][-5.6905632 -4.6643734 -4.8662243 -5.4366465 -5.6805725 -5.3045259 -4.757977 -5.2866869 -6.5716763 -7.4464936 -7.7045903 -7.2351227 -4.8915668 -2.1137078 -0.62886405][-5.8744907 -4.7022686 -4.7791853 -5.4332566 -5.8418946 -5.5973177 -5.0912123 -5.3085365 -6.0345535 -6.5309186 -6.6675086 -6.1506367 -3.9917402 -1.6572697 -0.55064845]]...]
INFO - root - 2017-12-15 08:18:21.219208: step 35310, loss = 0.48, batch loss = 0.44 (34.8 examples/sec; 0.230 sec/batch; 18h:57m:03s remains)
INFO - root - 2017-12-15 08:18:23.512999: step 35320, loss = 0.25, batch loss = 0.22 (34.9 examples/sec; 0.229 sec/batch; 18h:55m:39s remains)
INFO - root - 2017-12-15 08:18:25.794963: step 35330, loss = 0.24, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 18h:28m:39s remains)
INFO - root - 2017-12-15 08:18:28.059474: step 35340, loss = 0.25, batch loss = 0.21 (35.0 examples/sec; 0.228 sec/batch; 18h:50m:58s remains)
INFO - root - 2017-12-15 08:18:30.319528: step 35350, loss = 0.22, batch loss = 0.19 (35.4 examples/sec; 0.226 sec/batch; 18h:40m:42s remains)
INFO - root - 2017-12-15 08:18:32.607563: step 35360, loss = 0.24, batch loss = 0.21 (36.0 examples/sec; 0.222 sec/batch; 18h:20m:13s remains)
INFO - root - 2017-12-15 08:18:34.870358: step 35370, loss = 0.18, batch loss = 0.15 (35.3 examples/sec; 0.226 sec/batch; 18h:40m:44s remains)
INFO - root - 2017-12-15 08:18:37.145842: step 35380, loss = 0.18, batch loss = 0.15 (34.0 examples/sec; 0.235 sec/batch; 19h:23m:48s remains)
INFO - root - 2017-12-15 08:18:39.416282: step 35390, loss = 0.21, batch loss = 0.18 (35.1 examples/sec; 0.228 sec/batch; 18h:48m:47s remains)
INFO - root - 2017-12-15 08:18:41.672515: step 35400, loss = 0.16, batch loss = 0.12 (35.8 examples/sec; 0.223 sec/batch; 18h:25m:22s remains)
2017-12-15 08:18:41.955151: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.03329 -6.8641157 -6.3545275 -5.5044346 -4.2598982 -3.2873371 -2.9595757 -3.2536609 -3.9541698 -4.5719967 -5.0921574 -5.5547457 -5.660718 -5.286211 -4.5021095][-5.0900741 -6.8728533 -6.0673537 -5.1225853 -3.6376238 -2.5392613 -2.0583732 -2.4096756 -3.4238372 -4.5407991 -5.3282261 -5.6660638 -5.7074146 -5.3791428 -4.6067286][-6.1732759 -6.658145 -5.7006264 -4.76904 -3.1701872 -1.8601238 -1.0482315 -1.3350074 -2.628561 -4.3763218 -5.3484583 -5.3078589 -5.0426145 -4.6288571 -4.0614834][-6.9197493 -6.6953621 -5.5595026 -4.4042053 -2.4545047 -0.59363878 0.80201554 0.6221385 -1.1948658 -3.8829517 -5.223135 -4.7577181 -4.2408175 -3.8285463 -3.5035682][-7.6469603 -7.0510406 -5.5851583 -3.9376373 -1.2423834 1.4529846 3.4770458 3.3096745 0.78417492 -2.728682 -4.3245077 -3.8099985 -3.3152556 -3.1848712 -3.2204232][-8.3526955 -7.4178553 -5.5282316 -3.3014297 0.014189959 3.2989476 5.8459606 5.7253962 2.7838609 -1.1031159 -3.0058138 -2.7168322 -2.4602938 -2.5781586 -2.8348498][-7.8653355 -7.1782122 -4.8874626 -2.4387879 1.0356925 4.645731 7.618413 7.6340637 4.412693 0.22877884 -1.9042342 -1.9859904 -1.9916431 -2.2597656 -2.6844966][-7.7558823 -6.7725906 -4.4109325 -2.1417227 1.0976665 4.7475014 7.8067636 7.8597193 4.5240803 0.51980805 -1.4719515 -1.7027482 -1.8094176 -2.1351376 -2.693145][-7.6595821 -6.7438269 -4.7795391 -2.9887371 -0.31098592 2.8335145 5.4368896 5.359066 2.59432 -0.38891017 -1.6339744 -1.6784629 -1.65153 -1.8000362 -2.4742687][-7.5423937 -6.8422709 -5.4188633 -4.2206783 -2.2290044 0.19547677 2.0833981 1.9958737 0.074263096 -1.7307956 -2.2018454 -1.8447068 -1.5719882 -1.6134971 -2.347862][-7.4518614 -6.9515944 -5.9851255 -5.2391644 -3.7495375 -1.8917096 -0.67846835 -0.9305917 -2.198947 -3.0400484 -2.8202591 -2.0729439 -1.6204454 -1.7050387 -2.395936][-7.5706367 -7.1705923 -6.3278179 -5.6987495 -4.4750328 -3.1490352 -2.5369818 -2.9707062 -3.7766066 -3.9608877 -3.2238588 -2.1806684 -1.7130996 -1.7982041 -2.347765][-7.6122208 -7.1695423 -6.1268568 -5.2639914 -4.0850868 -3.0825369 -2.7844338 -3.2657669 -3.952637 -4.1070209 -3.343884 -2.2270641 -1.8260276 -2.0789227 -2.433501][-7.3867426 -6.8381987 -5.5523353 -4.3945951 -3.0831747 -2.1060085 -1.7977319 -2.366832 -3.2629642 -3.796119 -3.1513309 -2.0251977 -1.8154435 -2.2531028 -2.5556848][-7.130229 -6.5893269 -5.2257686 -3.8639576 -2.3434982 -1.2807235 -0.77247667 -1.2995453 -2.341043 -3.184375 -2.8562319 -1.9282339 -1.8176082 -2.3389509 -2.6905928]]...]
INFO - root - 2017-12-15 08:18:44.206118: step 35410, loss = 0.21, batch loss = 0.18 (35.4 examples/sec; 0.226 sec/batch; 18h:39m:21s remains)
INFO - root - 2017-12-15 08:18:46.471531: step 35420, loss = 0.23, batch loss = 0.20 (35.8 examples/sec; 0.224 sec/batch; 18h:27m:05s remains)
INFO - root - 2017-12-15 08:18:48.775254: step 35430, loss = 0.30, batch loss = 0.26 (35.8 examples/sec; 0.223 sec/batch; 18h:25m:48s remains)
INFO - root - 2017-12-15 08:18:51.045837: step 35440, loss = 0.27, batch loss = 0.24 (35.6 examples/sec; 0.224 sec/batch; 18h:31m:20s remains)
INFO - root - 2017-12-15 08:18:53.337239: step 35450, loss = 0.19, batch loss = 0.16 (33.1 examples/sec; 0.242 sec/batch; 19h:56m:36s remains)
INFO - root - 2017-12-15 08:18:55.631197: step 35460, loss = 0.25, batch loss = 0.22 (36.4 examples/sec; 0.220 sec/batch; 18h:07m:32s remains)
INFO - root - 2017-12-15 08:18:57.892762: step 35470, loss = 0.30, batch loss = 0.27 (34.4 examples/sec; 0.233 sec/batch; 19h:11m:55s remains)
INFO - root - 2017-12-15 08:19:00.187424: step 35480, loss = 0.31, batch loss = 0.28 (35.1 examples/sec; 0.228 sec/batch; 18h:48m:10s remains)
INFO - root - 2017-12-15 08:19:02.454899: step 35490, loss = 0.24, batch loss = 0.21 (36.4 examples/sec; 0.220 sec/batch; 18h:07m:12s remains)
INFO - root - 2017-12-15 08:19:04.748874: step 35500, loss = 0.16, batch loss = 0.13 (35.4 examples/sec; 0.226 sec/batch; 18h:38m:53s remains)
2017-12-15 08:19:05.011757: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6888533 -8.215538 -8.3323383 -7.9717636 -7.2700438 -6.2039471 -5.08407 -4.7208652 -4.7289987 -5.4542923 -6.704432 -6.7798619 -6.4994512 -6.6720357 -7.0151763][-6.0702438 -9.0824146 -9.0659885 -8.5179491 -7.4744964 -6.058629 -4.4919124 -4.0107474 -4.0194263 -4.8648434 -6.4611912 -6.6209807 -6.3819637 -6.8932018 -7.63729][-7.5665979 -9.2332973 -9.1391277 -8.5717735 -7.4704447 -5.753397 -3.761858 -3.1349983 -3.1351006 -3.9850707 -5.7916946 -5.9301882 -5.7393274 -6.6997137 -7.9540348][-8.284503 -8.9401627 -8.7018642 -8.1281528 -6.9952869 -5.151227 -2.9363153 -2.0539858 -2.0897934 -2.9070358 -4.6734114 -4.7313375 -4.5978718 -5.9709911 -7.4822021][-8.504694 -8.6898508 -8.20099 -7.3597236 -5.9749651 -4.01365 -1.7082381 -0.46182072 -0.36910594 -0.86482227 -2.2125618 -2.0491271 -2.130851 -4.0778627 -5.8502016][-8.7121353 -8.5930824 -8.0263014 -6.9212489 -5.0926414 -2.9007533 -0.43633926 1.0964608 1.2435207 1.1436784 0.51837921 0.84242487 0.39859366 -1.8577673 -3.6318073][-7.7893133 -8.0762434 -7.425632 -6.2157631 -4.1153941 -1.7153075 0.99867105 2.9060025 3.0186696 2.9216151 2.7048054 3.0336423 2.2075486 -0.061177254 -1.6083419][-7.5558534 -7.4951491 -6.63713 -5.335299 -3.1444173 -0.64319015 2.1695175 4.269526 4.3793955 3.9405084 3.950068 4.0545216 2.908617 0.77567792 -0.29144061][-7.5278807 -7.39015 -6.560317 -5.36537 -3.4233465 -1.1260532 1.2405934 3.0469842 3.1428523 2.5699334 2.8092046 3.0435572 1.8498015 -0.080020905 -0.753736][-7.2489905 -7.1687183 -6.7025285 -5.899035 -4.4222627 -2.6538081 -0.99990213 0.30679798 0.35888529 -0.15842724 0.44886994 0.96735311 -0.21317434 -2.1124585 -2.486002][-6.7487283 -6.5809011 -6.4324226 -6.0363312 -5.0886536 -3.8618064 -2.8255506 -1.8045847 -1.7372499 -2.3392069 -1.780674 -1.2298299 -2.1947191 -3.7173948 -3.8335533][-6.4940567 -6.2352381 -6.3382635 -6.2842717 -5.935174 -5.4445081 -5.0017228 -4.3811393 -4.3745627 -4.8635845 -4.5561056 -4.1308165 -4.8482141 -5.8131032 -5.6940255][-6.3120027 -6.0071135 -6.2224331 -6.3564186 -6.443584 -6.6592059 -6.8356175 -6.5507751 -6.5113268 -6.7557693 -6.5895977 -6.5536118 -7.039711 -7.3941636 -7.073741][-5.9140539 -5.5101728 -5.7275181 -5.9110117 -6.1412 -6.5575371 -6.9205146 -6.9359 -6.9661021 -7.0747805 -7.0079913 -7.0693846 -7.31089 -7.4158926 -7.1529832][-5.6184096 -5.0987329 -5.2362981 -5.3544269 -5.5065074 -5.7955046 -6.1049833 -6.2806916 -6.4156365 -6.4551153 -6.4568863 -6.600636 -6.7636971 -6.7421222 -6.6017222]]...]
INFO - root - 2017-12-15 08:19:07.272904: step 35510, loss = 0.21, batch loss = 0.18 (34.9 examples/sec; 0.229 sec/batch; 18h:53m:11s remains)
INFO - root - 2017-12-15 08:19:09.548280: step 35520, loss = 0.26, batch loss = 0.23 (37.3 examples/sec; 0.214 sec/batch; 17h:40m:58s remains)
INFO - root - 2017-12-15 08:19:11.849984: step 35530, loss = 0.29, batch loss = 0.26 (34.7 examples/sec; 0.230 sec/batch; 18h:59m:33s remains)
INFO - root - 2017-12-15 08:19:14.119190: step 35540, loss = 0.30, batch loss = 0.27 (35.4 examples/sec; 0.226 sec/batch; 18h:39m:27s remains)
INFO - root - 2017-12-15 08:19:16.410445: step 35550, loss = 0.22, batch loss = 0.18 (36.2 examples/sec; 0.221 sec/batch; 18h:15m:03s remains)
INFO - root - 2017-12-15 08:19:18.709182: step 35560, loss = 0.21, batch loss = 0.18 (35.2 examples/sec; 0.227 sec/batch; 18h:44m:27s remains)
INFO - root - 2017-12-15 08:19:21.004831: step 35570, loss = 0.20, batch loss = 0.17 (35.0 examples/sec; 0.229 sec/batch; 18h:51m:34s remains)
INFO - root - 2017-12-15 08:19:23.280903: step 35580, loss = 0.17, batch loss = 0.14 (34.3 examples/sec; 0.233 sec/batch; 19h:14m:01s remains)
INFO - root - 2017-12-15 08:19:25.547504: step 35590, loss = 0.23, batch loss = 0.20 (34.8 examples/sec; 0.230 sec/batch; 18h:58m:46s remains)
INFO - root - 2017-12-15 08:19:27.805767: step 35600, loss = 0.24, batch loss = 0.21 (36.1 examples/sec; 0.222 sec/batch; 18h:17m:58s remains)
2017-12-15 08:19:28.107708: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2128615 -7.1131477 -7.0027709 -6.3435893 -5.5251679 -4.7244186 -4.0505738 -4.5276442 -6.0140738 -6.9517527 -7.1660113 -7.050312 -6.5370693 -5.3592758 -3.8806586][-4.7956424 -6.8436375 -6.3664393 -5.3674412 -4.5241942 -3.3574553 -2.5947385 -3.4415407 -5.0921955 -6.0691433 -6.53011 -6.7274065 -6.5721831 -5.5049477 -3.8622468][-5.7372408 -6.5284743 -5.79261 -4.6745582 -3.7740974 -2.2723863 -1.4006033 -2.4756033 -4.1458273 -5.1205468 -5.7190828 -6.2803659 -6.5776753 -5.6213684 -3.9577322][-6.4680481 -6.2593279 -5.4014606 -4.2084537 -3.2277436 -1.5270643 -0.57183743 -1.7712705 -3.4467592 -4.5663385 -5.2370234 -6.04252 -6.6152496 -5.8678875 -4.2915516][-6.8876433 -5.7817545 -4.7784829 -3.3551819 -2.1201208 -0.29105437 0.77354813 -0.29671431 -2.0380926 -3.4773822 -4.4250546 -5.4688635 -6.238677 -5.7837338 -4.4207344][-6.4898906 -5.2392912 -4.0010939 -2.0805991 -0.23558998 1.728369 2.9667764 2.0558038 0.026992559 -1.94805 -3.4162545 -4.7895241 -5.6859856 -5.4820824 -4.2305431][-5.562645 -4.620121 -3.1904194 -0.6638999 2.0182724 4.2082376 5.5128269 4.6159816 2.075336 -0.57996213 -2.6194797 -4.1945658 -5.1884794 -5.13749 -3.8670461][-5.5506878 -4.3632278 -2.9636352 -0.11386943 3.2361202 5.7448149 7.0993509 6.2065115 3.2885995 0.24341941 -2.119844 -3.7833443 -4.910152 -4.9329166 -3.5650885][-6.1607 -4.8248625 -3.6950328 -0.99027419 2.5450897 5.1428576 6.5101156 5.8400869 2.965291 0.0023915768 -2.2720573 -3.9947636 -5.2366438 -5.3178558 -3.8585615][-6.8921022 -5.6113052 -4.9424415 -2.8107677 0.31697774 2.6594033 3.8572254 3.3997746 0.94609571 -1.4335418 -3.1640086 -4.5820837 -5.7150059 -5.6799312 -4.2332745][-7.2689443 -6.2805986 -6.071135 -4.6259375 -2.1969061 -0.34023225 0.57222867 0.27871227 -1.5713086 -3.1579084 -4.2061286 -5.1405973 -5.9967585 -5.7541289 -4.4528656][-7.0955858 -6.5301008 -6.7508035 -5.9816904 -4.1270852 -2.5119841 -1.7506828 -1.84213 -3.1320984 -4.1511993 -4.7595797 -5.3728504 -6.1152282 -5.8236208 -4.7824421][-6.5540023 -6.394875 -6.980546 -6.8608031 -5.587429 -4.1559649 -3.5201406 -3.5759854 -4.4549146 -5.1380024 -5.4392638 -5.8001404 -6.3520513 -5.9420185 -5.0486298][-6.0211916 -6.0269537 -6.8567615 -7.2454405 -6.54051 -5.3931 -4.8517303 -4.9043674 -5.4788318 -5.9473982 -6.0891247 -6.1727896 -6.459332 -5.9888449 -5.2038441][-5.3524728 -5.2080717 -6.0981612 -6.7459731 -6.4850621 -5.7236996 -5.3787532 -5.4321556 -5.7502165 -6.1044559 -6.1592894 -6.1060734 -6.1022291 -5.6318645 -4.9889984]]...]
INFO - root - 2017-12-15 08:19:30.361481: step 35610, loss = 0.31, batch loss = 0.27 (35.5 examples/sec; 0.225 sec/batch; 18h:35m:17s remains)
INFO - root - 2017-12-15 08:19:32.635089: step 35620, loss = 0.27, batch loss = 0.24 (34.0 examples/sec; 0.235 sec/batch; 19h:23m:40s remains)
INFO - root - 2017-12-15 08:19:34.907170: step 35630, loss = 0.26, batch loss = 0.23 (36.5 examples/sec; 0.219 sec/batch; 18h:05m:20s remains)
INFO - root - 2017-12-15 08:19:37.173249: step 35640, loss = 0.31, batch loss = 0.28 (36.8 examples/sec; 0.218 sec/batch; 17h:56m:57s remains)
INFO - root - 2017-12-15 08:19:39.476217: step 35650, loss = 0.26, batch loss = 0.23 (34.9 examples/sec; 0.229 sec/batch; 18h:53m:24s remains)
INFO - root - 2017-12-15 08:19:41.728220: step 35660, loss = 0.26, batch loss = 0.23 (36.0 examples/sec; 0.222 sec/batch; 18h:18m:59s remains)
INFO - root - 2017-12-15 08:19:44.000622: step 35670, loss = 0.24, batch loss = 0.21 (34.6 examples/sec; 0.231 sec/batch; 19h:03m:51s remains)
INFO - root - 2017-12-15 08:19:46.290084: step 35680, loss = 0.20, batch loss = 0.17 (35.9 examples/sec; 0.223 sec/batch; 18h:22m:20s remains)
INFO - root - 2017-12-15 08:19:48.524766: step 35690, loss = 0.21, batch loss = 0.17 (35.8 examples/sec; 0.223 sec/batch; 18h:24m:30s remains)
INFO - root - 2017-12-15 08:19:50.774539: step 35700, loss = 0.23, batch loss = 0.19 (35.7 examples/sec; 0.224 sec/batch; 18h:28m:50s remains)
2017-12-15 08:19:51.065506: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.75002742 0.36444235 0.49442434 0.053986788 -0.6354053 -0.62765229 -1.2911702 -2.1235056 -2.9371204 -4.2204838 -5.16628 -5.3946152 -5.5135317 -5.5522051 -5.5046244][-0.18869781 0.58724332 0.82632875 0.43674135 0.04694581 0.20889068 -0.37823081 -1.2055013 -2.1623073 -3.3520126 -4.1687641 -4.3506346 -4.6768379 -4.6684527 -4.5405211][-2.30082 -0.95140207 -0.55848312 -0.22489619 0.088665962 0.64353275 0.31120229 -0.30124509 -1.2787566 -2.5862992 -3.3893428 -3.4633284 -3.7201018 -3.5331671 -3.2537754][-4.712985 -3.1988192 -2.873363 -1.7345006 -0.38101494 0.85141206 1.2382793 1.0664327 0.28677249 -1.2963054 -2.4928019 -2.8712449 -3.0929732 -2.708853 -2.4273963][-6.3739114 -4.6861534 -4.580008 -2.9030623 -0.66615427 1.3807526 2.7126384 3.1784453 2.5138721 0.63634729 -1.1558628 -2.160706 -2.6308465 -2.3296857 -2.1633117][-6.4767647 -5.2793283 -5.3970356 -3.4907041 -0.87540662 1.6911886 3.6268773 4.5807052 3.9673424 1.8884659 -0.46837032 -2.0466647 -2.7078218 -2.6479225 -2.5612042][-5.4308662 -5.0180473 -5.2844181 -3.5087271 -0.85416019 1.865757 3.9939318 5.0402732 4.3536963 2.0444837 -0.72906196 -2.611269 -3.3244247 -3.4934702 -3.553664][-4.6171427 -4.5867176 -4.8967886 -3.1694441 -0.46508181 2.2603173 4.4174232 5.3279157 4.319891 1.7649345 -1.3401544 -3.5184822 -4.1784172 -4.5735245 -4.5529518][-4.3236623 -4.4589033 -4.7342072 -2.8801937 -0.10257912 2.5420694 4.5776963 5.18584 3.8591747 1.086961 -2.2915244 -4.576581 -5.1723948 -5.4419451 -5.0755987][-4.5315742 -4.6190667 -4.7709994 -2.9252753 -0.26032984 2.012589 3.7969203 4.1528196 2.9470916 0.26431704 -3.0923917 -5.3104095 -5.97534 -6.1755009 -5.8453083][-5.2408075 -5.1427593 -5.1191964 -3.3229046 -0.95145404 0.72561145 2.0698195 2.3268995 1.4221549 -0.76849806 -3.6016946 -5.5555735 -6.3308749 -6.4739604 -6.3165154][-5.685462 -5.4023972 -5.3332624 -3.6274884 -1.597435 -0.44781208 0.29335737 0.22967339 -0.51197016 -2.0706441 -4.0017676 -5.2855372 -5.7127132 -5.6841869 -5.74601][-5.9637032 -5.5213518 -5.4202938 -3.6872122 -1.7905862 -0.98038137 -0.89079952 -1.3952878 -2.0186899 -2.9607391 -3.9488664 -4.5199881 -4.6546459 -4.8996439 -5.3231535][-5.9362693 -5.1560407 -4.6378188 -2.8714719 -1.205104 -0.79687738 -1.3113379 -2.2150257 -2.862097 -3.5164671 -4.0480804 -4.5010452 -4.5476503 -4.7549157 -5.09406][-5.432826 -4.3919535 -3.4676273 -1.9630976 -0.75387406 -0.54186594 -1.352245 -2.5962365 -3.4032245 -4.0587177 -4.5350075 -5.0646605 -4.9783907 -4.6915159 -4.4616461]]...]
INFO - root - 2017-12-15 08:19:53.321168: step 35710, loss = 0.16, batch loss = 0.12 (34.6 examples/sec; 0.231 sec/batch; 19h:02m:05s remains)
INFO - root - 2017-12-15 08:19:55.604471: step 35720, loss = 0.27, batch loss = 0.24 (34.9 examples/sec; 0.229 sec/batch; 18h:52m:21s remains)
INFO - root - 2017-12-15 08:19:57.870695: step 35730, loss = 0.19, batch loss = 0.16 (32.5 examples/sec; 0.246 sec/batch; 20h:17m:55s remains)
INFO - root - 2017-12-15 08:20:00.127695: step 35740, loss = 0.23, batch loss = 0.20 (35.8 examples/sec; 0.223 sec/batch; 18h:24m:37s remains)
INFO - root - 2017-12-15 08:20:02.413087: step 35750, loss = 0.27, batch loss = 0.24 (34.4 examples/sec; 0.233 sec/batch; 19h:10m:17s remains)
INFO - root - 2017-12-15 08:20:04.681295: step 35760, loss = 0.19, batch loss = 0.16 (36.0 examples/sec; 0.222 sec/batch; 18h:17m:49s remains)
INFO - root - 2017-12-15 08:20:06.988579: step 35770, loss = 0.17, batch loss = 0.14 (35.0 examples/sec; 0.229 sec/batch; 18h:51m:30s remains)
INFO - root - 2017-12-15 08:20:09.308101: step 35780, loss = 0.25, batch loss = 0.22 (35.4 examples/sec; 0.226 sec/batch; 18h:37m:26s remains)
INFO - root - 2017-12-15 08:20:11.595812: step 35790, loss = 0.38, batch loss = 0.34 (35.7 examples/sec; 0.224 sec/batch; 18h:27m:08s remains)
INFO - root - 2017-12-15 08:20:13.879759: step 35800, loss = 0.19, batch loss = 0.15 (36.5 examples/sec; 0.219 sec/batch; 18h:04m:20s remains)
2017-12-15 08:20:14.144850: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.51804447 -2.2942355 -4.3358054 -5.6134748 -6.0069771 -5.4611053 -4.263689 -1.0063541 0.90100694 1.8934228 1.1457174 -0.52457809 -2.5794334 -4.6192064 -6.0206842][-2.6084552 -4.17892 -6.0566216 -7.2412815 -7.8665485 -7.7069807 -6.5794668 -3.7957749 -2.2155635 -1.3477581 -1.8974702 -3.1104364 -4.7670889 -6.1981435 -6.82693][-5.2378135 -5.8801951 -7.4470682 -8.7059536 -9.3833008 -9.4257126 -8.3232288 -6.1716309 -5.0947256 -4.64529 -5.0330887 -5.9101944 -7.2479792 -8.1113338 -8.0417585][-6.6855364 -6.74051 -7.9810915 -8.9401674 -9.3117647 -9.1802769 -8.0098515 -6.58008 -6.2928228 -6.5287333 -6.9578438 -7.717329 -8.9518728 -9.5511227 -9.1789265][-7.3080444 -7.213439 -7.850894 -8.3611965 -8.1463413 -7.3577194 -5.6093054 -4.1647053 -4.3050365 -5.3041525 -6.4266729 -7.7135954 -9.3222885 -9.7616272 -9.24645][-8.036478 -7.4586411 -7.4189243 -6.9565506 -5.8970561 -4.40405 -1.9584033 -0.23019266 -0.63130188 -2.153971 -3.8231978 -6.1010327 -8.4769049 -9.1743879 -8.7626257][-7.464983 -6.8583479 -6.2987137 -4.8433814 -2.7348471 -0.35157752 2.6414039 4.599884 3.64912 1.3372178 -1.0158278 -4.0999227 -7.0354567 -8.1658726 -8.1146][-6.9892197 -6.1702995 -5.7291293 -3.7938418 -0.92776406 2.0496514 5.6052122 8.1068029 7.0401554 4.2094183 1.3770337 -2.2076473 -5.2386026 -6.6533165 -6.9758825][-6.4928989 -5.6487236 -5.72272 -4.0347242 -1.2551278 1.5952299 4.9228773 7.4212904 6.5627241 3.8875172 1.2745469 -1.8256477 -4.1931109 -5.5228 -6.0108333][-6.2872896 -5.49798 -5.9235244 -4.6111269 -2.0881596 0.29680204 2.7649038 4.5503225 3.5550911 0.87709713 -1.4376595 -3.6493671 -4.7876964 -5.4098172 -5.6346664][-6.47482 -5.8877482 -6.6419792 -5.8102283 -3.6205659 -1.497148 0.45285583 1.715565 0.82491755 -1.5210321 -3.7681208 -5.3663249 -5.7032852 -5.7749739 -5.5608664][-6.7865791 -6.4564905 -7.416194 -7.1177158 -5.5468969 -3.746223 -2.2573044 -1.3872478 -1.8591623 -3.3368497 -4.9187727 -6.0541863 -6.010612 -5.8705893 -5.4866114][-6.99489 -6.677721 -7.5917892 -7.6949186 -6.8070173 -5.4388256 -4.2746334 -3.5907774 -3.6888952 -4.2383881 -4.9047713 -5.411725 -5.436502 -5.5356493 -5.272902][-6.7574205 -6.3163395 -7.0471725 -7.3080864 -6.9987507 -6.0570421 -5.0984831 -4.4497137 -4.0905752 -3.8241482 -3.731647 -3.655726 -3.8287108 -4.3880939 -4.6011539][-5.9699111 -5.3514085 -6.0032492 -6.3669567 -6.3305063 -5.782948 -5.1585426 -4.6244 -3.9308906 -3.1844525 -2.652209 -2.092643 -2.2209637 -3.263006 -4.0086522]]...]
INFO - root - 2017-12-15 08:20:16.393333: step 35810, loss = 0.22, batch loss = 0.19 (34.3 examples/sec; 0.234 sec/batch; 19h:14m:56s remains)
INFO - root - 2017-12-15 08:20:18.687160: step 35820, loss = 0.26, batch loss = 0.23 (34.5 examples/sec; 0.232 sec/batch; 19h:07m:07s remains)
INFO - root - 2017-12-15 08:20:20.965189: step 35830, loss = 0.26, batch loss = 0.23 (35.2 examples/sec; 0.227 sec/batch; 18h:42m:39s remains)
INFO - root - 2017-12-15 08:20:23.243419: step 35840, loss = 0.16, batch loss = 0.13 (33.3 examples/sec; 0.240 sec/batch; 19h:48m:05s remains)
INFO - root - 2017-12-15 08:20:25.497433: step 35850, loss = 0.27, batch loss = 0.24 (35.9 examples/sec; 0.223 sec/batch; 18h:22m:14s remains)
INFO - root - 2017-12-15 08:20:27.765569: step 35860, loss = 0.25, batch loss = 0.22 (34.4 examples/sec; 0.232 sec/batch; 19h:08m:09s remains)
INFO - root - 2017-12-15 08:20:30.075259: step 35870, loss = 0.20, batch loss = 0.17 (35.3 examples/sec; 0.227 sec/batch; 18h:40m:50s remains)
INFO - root - 2017-12-15 08:20:32.370140: step 35880, loss = 0.24, batch loss = 0.21 (33.9 examples/sec; 0.236 sec/batch; 19h:27m:47s remains)
INFO - root - 2017-12-15 08:20:34.636646: step 35890, loss = 0.26, batch loss = 0.23 (35.8 examples/sec; 0.223 sec/batch; 18h:24m:07s remains)
INFO - root - 2017-12-15 08:20:36.874876: step 35900, loss = 0.30, batch loss = 0.27 (36.2 examples/sec; 0.221 sec/batch; 18h:11m:11s remains)
2017-12-15 08:20:37.170528: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0962152 -3.9133642 -3.8304858 -3.7589352 -3.6980643 -3.6567159 -3.6284904 -3.5902295 -3.5476518 -3.5631058 -3.6276495 -3.6926637 -3.7162309 -3.7516389 -3.8077126][-3.7932072 -5.4202175 -5.3532286 -5.171957 -4.9600525 -4.8448133 -4.8196125 -4.8118305 -4.7652993 -4.7675896 -4.8411674 -4.9578152 -5.0250568 -5.0830841 -5.1470671][-5.2013173 -6.1910658 -6.2652664 -5.9159708 -5.3375988 -5.0061417 -4.8388882 -4.8310966 -5.0234184 -5.14663 -5.2579832 -5.4968081 -5.7036533 -5.7816696 -5.8407197][-6.5263767 -6.8219709 -6.8441019 -6.3242092 -5.4221406 -4.9089289 -4.4518185 -4.3144894 -4.7984247 -5.0633869 -5.24146 -5.7796345 -6.2760849 -6.4388561 -6.5448561][-7.2348347 -6.9693937 -6.8806839 -6.1561484 -4.9620008 -4.314302 -3.6100478 -3.3430405 -4.0800347 -4.3996696 -4.6348639 -5.4600439 -6.2746944 -6.5175323 -6.6771479][-6.2932539 -5.4048381 -5.3899956 -4.6879416 -3.3288326 -2.5779169 -1.6486983 -1.2333907 -2.1170359 -2.2746139 -2.3683734 -3.3473182 -4.3686934 -4.6572447 -4.9554033][-4.1648245 -3.4382229 -3.556119 -3.0125151 -1.6406821 -0.77174163 0.34503579 0.9931345 0.086838245 0.0029029846 -0.15142441 -1.3345821 -2.41709 -2.6957192 -2.9781413][-3.2090402 -2.3400779 -2.4793596 -2.0004251 -0.61942708 0.34388518 1.1503823 1.541327 0.89421821 0.94804144 0.7619338 -0.35290265 -1.2053638 -1.494204 -1.6979256][-2.485431 -1.3004463 -1.6399822 -1.4510751 -0.38377953 0.510159 1.0630491 1.4588878 1.1650691 1.5447803 1.436914 0.58117628 -0.13610864 -0.50261223 -0.59814119][-2.9467263 -1.4842095 -1.9963584 -2.0940351 -1.4429007 -0.89494646 -0.61863053 -0.092021465 0.08595252 0.73736024 0.85015726 0.25317478 -0.24365449 -0.65764451 -0.75124538][-4.3572316 -2.5953951 -3.0941405 -3.2823696 -3.0134232 -2.8186693 -2.6884806 -1.9923195 -1.4534893 -0.59798956 -0.33833098 -0.56003594 -0.898811 -1.3340676 -1.3959312][-5.29887 -3.3959382 -3.9212816 -4.22011 -4.2012835 -4.3572288 -4.2065654 -3.2787094 -2.6928267 -1.8812097 -1.3065453 -1.030507 -1.1659881 -1.6799612 -1.9423687][-5.8789415 -4.2626109 -5.0501881 -5.4137406 -5.48063 -5.9337854 -5.6919141 -4.5377622 -3.9896266 -3.1673102 -2.5429466 -2.160491 -2.2431729 -2.7716906 -3.2868872][-5.3219028 -3.9575262 -5.1161036 -5.894249 -6.1768713 -6.7206278 -6.3107948 -4.801393 -4.0631552 -3.4985108 -3.264081 -3.1882555 -3.2538478 -3.596231 -4.1149225][-3.5349336 -2.2162836 -3.6074123 -4.8030281 -5.4361439 -6.103693 -5.643548 -3.9562933 -2.9245145 -2.3951094 -2.4985056 -2.8559525 -3.2714109 -3.8364487 -4.5379896]]...]
INFO - root - 2017-12-15 08:20:39.465534: step 35910, loss = 0.43, batch loss = 0.39 (35.7 examples/sec; 0.224 sec/batch; 18h:27m:55s remains)
INFO - root - 2017-12-15 08:20:41.748813: step 35920, loss = 0.25, batch loss = 0.22 (34.8 examples/sec; 0.230 sec/batch; 18h:55m:21s remains)
INFO - root - 2017-12-15 08:20:43.997256: step 35930, loss = 0.21, batch loss = 0.18 (36.8 examples/sec; 0.217 sec/batch; 17h:54m:50s remains)
INFO - root - 2017-12-15 08:20:46.286901: step 35940, loss = 0.44, batch loss = 0.41 (34.9 examples/sec; 0.229 sec/batch; 18h:53m:49s remains)
INFO - root - 2017-12-15 08:20:48.581439: step 35950, loss = 0.24, batch loss = 0.21 (35.0 examples/sec; 0.229 sec/batch; 18h:50m:12s remains)
INFO - root - 2017-12-15 08:20:50.841391: step 35960, loss = 0.29, batch loss = 0.26 (35.7 examples/sec; 0.224 sec/batch; 18h:27m:46s remains)
INFO - root - 2017-12-15 08:20:53.103876: step 35970, loss = 0.24, batch loss = 0.21 (35.2 examples/sec; 0.227 sec/batch; 18h:43m:41s remains)
INFO - root - 2017-12-15 08:20:55.418855: step 35980, loss = 0.18, batch loss = 0.15 (36.2 examples/sec; 0.221 sec/batch; 18h:13m:12s remains)
INFO - root - 2017-12-15 08:20:57.686413: step 35990, loss = 0.20, batch loss = 0.17 (34.8 examples/sec; 0.230 sec/batch; 18h:55m:21s remains)
INFO - root - 2017-12-15 08:21:00.002988: step 36000, loss = 0.22, batch loss = 0.19 (34.9 examples/sec; 0.229 sec/batch; 18h:53m:13s remains)
2017-12-15 08:21:00.273318: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.80509579 -3.4907956 -3.3480477 -3.1093388 -3.6758208 -4.1695642 -3.1592908 -3.0977097 -3.5030026 -3.4190483 -3.3794355 -3.2683778 -2.8753152 -2.5962892 -2.627845][-1.6398342 -3.5173426 -3.3528566 -2.8576784 -3.2892475 -3.6182532 -2.246762 -2.1040828 -2.6940386 -2.7323666 -2.8310115 -2.8936906 -2.6381562 -2.3415947 -2.3388245][-2.4557996 -3.4906077 -3.40273 -2.7569604 -3.0500202 -3.14738 -1.5596868 -1.2941444 -1.9021127 -2.0697103 -2.423713 -2.8144031 -2.83728 -2.7067332 -2.7089055][-2.9250951 -3.3683681 -3.3116255 -2.6176763 -2.7930124 -2.6341777 -0.86361039 -0.35571289 -0.75348389 -1.0106155 -1.627702 -2.3586085 -2.7130837 -2.8717029 -2.8922453][-3.4403887 -3.213438 -3.1361475 -2.5190835 -2.7068994 -2.2886665 -0.16596508 0.8314867 0.91522789 0.63546085 -0.34829354 -1.5583245 -2.3381402 -2.7636466 -2.9342937][-3.9318264 -3.0813599 -2.9171021 -2.3657372 -2.6475804 -2.0242658 0.52061105 2.0602255 2.7225266 2.5111232 1.2978387 -0.24686241 -1.3386738 -2.0044816 -2.3086169][-3.5082302 -3.0685241 -2.8206191 -2.2923224 -2.7147894 -2.0373 0.80057168 2.6625996 3.628119 3.580019 2.3682165 0.75836468 -0.39065003 -0.9261384 -1.2585647][-3.5492492 -3.1760166 -2.9101286 -2.4268031 -3.0320323 -2.4881606 0.43862128 2.3343701 3.2872634 3.2429748 2.1162014 0.5553143 -0.41255462 -0.4926964 -0.5946033][-3.6285348 -3.3115561 -3.0632987 -2.6464314 -3.3958287 -2.9370456 0.026049614 1.8137116 2.5524759 2.3630109 1.2549295 -0.42496026 -1.427923 -1.1063761 -0.88248754][-3.6885271 -3.3923488 -3.1382105 -2.7451022 -3.4657953 -2.9559875 -0.11324787 1.3229749 1.6379774 1.1199505 0.001210928 -1.6986852 -2.5501966 -1.8870265 -1.4143193][-3.7346892 -3.4165564 -3.11438 -2.6616862 -3.2130141 -2.5150075 0.13982391 1.0735691 0.85973454 -0.076942205 -1.2702706 -2.8724046 -3.4446292 -2.4845088 -1.5640593][-3.7717981 -3.388886 -3.0141764 -2.4531519 -2.7812672 -1.8570889 0.62212682 1.1744735 0.72530365 -0.44326437 -1.7078878 -3.113806 -3.4143128 -2.2820597 -1.1402948][-3.8144531 -3.31033 -2.8003857 -2.0884981 -2.2914774 -1.2976156 1.0454428 1.4735668 1.0557859 0.030326843 -1.1110129 -2.2595842 -2.459759 -1.4883821 -0.44417775][-3.884316 -3.2200904 -2.4931073 -1.5384264 -1.6553295 -0.73368454 1.4630952 1.789629 1.5102885 0.86936808 0.0079188347 -0.73064661 -0.80866432 0.0095257759 1.0363791][-3.9727731 -3.1904607 -2.2805588 -1.1590182 -1.3398125 -0.68156826 1.2432199 1.413615 1.1927166 0.75078845 0.069824934 -0.34350097 -0.45977163 0.22862959 1.3447089]]...]
INFO - root - 2017-12-15 08:21:02.559457: step 36010, loss = 0.21, batch loss = 0.18 (35.9 examples/sec; 0.223 sec/batch; 18h:19m:42s remains)
INFO - root - 2017-12-15 08:21:04.856980: step 36020, loss = 0.25, batch loss = 0.21 (36.3 examples/sec; 0.221 sec/batch; 18h:10m:09s remains)
INFO - root - 2017-12-15 08:21:07.117839: step 36030, loss = 0.25, batch loss = 0.22 (35.8 examples/sec; 0.224 sec/batch; 18h:25m:36s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 08:21:09.427910: step 36040, loss = 0.32, batch loss = 0.29 (34.5 examples/sec; 0.232 sec/batch; 19h:06m:05s remains)
INFO - root - 2017-12-15 08:21:11.729170: step 36050, loss = 0.23, batch loss = 0.20 (34.5 examples/sec; 0.232 sec/batch; 19h:05m:18s remains)
INFO - root - 2017-12-15 08:21:13.999259: step 36060, loss = 0.16, batch loss = 0.13 (34.9 examples/sec; 0.229 sec/batch; 18h:52m:11s remains)
INFO - root - 2017-12-15 08:21:16.269867: step 36070, loss = 0.35, batch loss = 0.32 (35.6 examples/sec; 0.225 sec/batch; 18h:29m:29s remains)
INFO - root - 2017-12-15 08:21:18.552607: step 36080, loss = 0.23, batch loss = 0.19 (32.6 examples/sec; 0.245 sec/batch; 20h:11m:37s remains)
INFO - root - 2017-12-15 08:21:20.860023: step 36090, loss = 0.33, batch loss = 0.30 (34.1 examples/sec; 0.235 sec/batch; 19h:20m:01s remains)
INFO - root - 2017-12-15 08:21:23.119352: step 36100, loss = 0.27, batch loss = 0.24 (35.0 examples/sec; 0.229 sec/batch; 18h:49m:32s remains)
2017-12-15 08:21:23.421618: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7840495 -6.6910596 -7.1685705 -7.2066674 -6.782217 -5.7663422 -4.7628908 -4.4101353 -4.6286459 -5.5303259 -6.9615641 -7.5710325 -7.4651542 -6.979259 -5.7854509][-6.3083925 -7.96447 -8.5118141 -8.4209681 -7.7131147 -6.3808365 -5.1940546 -4.7722673 -5.0240345 -6.1719704 -7.6324472 -8.10733 -8.0498114 -7.4389553 -6.0038147][-7.4187894 -8.0971584 -8.394207 -7.9776368 -6.9752584 -5.4574766 -4.2457204 -3.890075 -4.3792334 -5.9393382 -7.4663911 -8.0047846 -8.2648945 -7.8170156 -6.2654409][-7.6901412 -7.3618026 -7.2010288 -6.4142065 -5.1124959 -3.398633 -2.1854396 -1.8924222 -2.834353 -4.9927292 -6.7074046 -7.4409037 -7.9903216 -7.8254838 -6.4690061][-6.7364888 -5.6419439 -5.0617437 -4.1097336 -2.7175927 -0.79746711 0.64905548 1.0032475 -0.43612552 -3.1635618 -5.1905832 -6.4022903 -7.3119478 -7.5069528 -6.4969234][-5.6656966 -4.0804605 -3.2876077 -2.2973237 -0.63055015 1.7716479 3.5459886 3.9862442 2.1532273 -1.0133514 -3.4576283 -5.2142024 -6.493247 -7.1519761 -6.5579867][-4.5168619 -3.463486 -2.6482918 -1.3220403 1.0814841 4.1885939 6.4841461 6.9369297 4.6759758 1.0650909 -1.8434843 -4.2249947 -5.79299 -6.864151 -6.6202564][-4.8279743 -3.8834951 -3.039546 -1.3751607 1.4568496 4.8050394 7.0612555 7.3336267 5.0467172 1.3915577 -1.8010337 -4.4665661 -6.0027971 -7.1154919 -6.9940286][-5.3181124 -4.7074695 -4.1556964 -2.6806762 -0.2098968 2.5548382 4.418664 4.761508 2.8725157 -0.36180747 -3.2592707 -5.520977 -6.754189 -7.667109 -7.5221057][-5.8827515 -5.5731392 -5.2192669 -3.8954229 -1.8610655 0.18596768 1.662159 1.8477259 0.15173197 -2.5734036 -4.9858284 -6.6638365 -7.5154066 -8.127491 -7.9008226][-6.2146721 -5.9646516 -5.6575027 -4.6049027 -3.3119969 -2.205085 -1.4334228 -1.5435812 -2.9375119 -4.9377232 -6.606379 -7.6178041 -8.08745 -8.315052 -7.9399366][-6.5550809 -6.3299155 -6.18719 -5.5824156 -4.9528427 -4.4856997 -4.1685715 -4.5010076 -5.5086966 -6.6848803 -7.5642385 -7.9892387 -8.1386232 -8.1396446 -7.6797514][-6.47938 -6.1148882 -6.1618948 -5.9772167 -5.8085294 -5.7154474 -5.572813 -5.8143611 -6.2606864 -6.5617309 -6.8633366 -7.1198597 -7.227684 -7.3715429 -7.0475535][-6.2485209 -5.6859903 -5.8248129 -5.8870969 -6.0792618 -6.1641607 -5.87209 -5.7991486 -5.6445084 -5.3152709 -5.3940964 -5.7881031 -6.0210686 -6.4424319 -6.3588305][-5.6528549 -5.1195145 -5.3307152 -5.5007825 -5.8468103 -5.9458418 -5.4735546 -5.0968752 -4.4971242 -3.6928768 -3.7460518 -4.2184315 -4.6424217 -5.4467015 -5.6794863]]...]
INFO - root - 2017-12-15 08:21:25.690700: step 36110, loss = 0.17, batch loss = 0.14 (34.9 examples/sec; 0.229 sec/batch; 18h:52m:46s remains)
INFO - root - 2017-12-15 08:21:27.974274: step 36120, loss = 0.21, batch loss = 0.17 (34.7 examples/sec; 0.231 sec/batch; 18h:59m:01s remains)
INFO - root - 2017-12-15 08:21:30.247750: step 36130, loss = 0.22, batch loss = 0.19 (34.8 examples/sec; 0.230 sec/batch; 18h:55m:48s remains)
INFO - root - 2017-12-15 08:21:32.538481: step 36140, loss = 0.33, batch loss = 0.30 (36.4 examples/sec; 0.220 sec/batch; 18h:04m:37s remains)
INFO - root - 2017-12-15 08:21:34.828354: step 36150, loss = 0.24, batch loss = 0.21 (34.0 examples/sec; 0.236 sec/batch; 19h:23m:29s remains)
INFO - root - 2017-12-15 08:21:37.124625: step 36160, loss = 0.20, batch loss = 0.17 (35.3 examples/sec; 0.226 sec/batch; 18h:37m:46s remains)
INFO - root - 2017-12-15 08:21:39.426295: step 36170, loss = 0.23, batch loss = 0.20 (35.5 examples/sec; 0.225 sec/batch; 18h:32m:10s remains)
INFO - root - 2017-12-15 08:21:41.695186: step 36180, loss = 0.27, batch loss = 0.23 (35.3 examples/sec; 0.227 sec/batch; 18h:39m:06s remains)
INFO - root - 2017-12-15 08:21:43.965473: step 36190, loss = 0.30, batch loss = 0.26 (34.4 examples/sec; 0.232 sec/batch; 19h:06m:54s remains)
INFO - root - 2017-12-15 08:21:46.277759: step 36200, loss = 0.21, batch loss = 0.18 (36.1 examples/sec; 0.222 sec/batch; 18h:15m:43s remains)
2017-12-15 08:21:46.594746: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2145078 -5.24352 -5.8257189 -6.2071977 -6.0959721 -5.683197 -5.355474 -5.4497962 -5.5039315 -5.4798346 -5.4712434 -5.2166643 -4.5832605 -3.949367 -3.6203761][-3.4992833 -5.3514519 -6.3951159 -6.9281287 -6.8556509 -6.3171477 -5.8699083 -5.9939919 -6.0382361 -6.0173111 -5.9873171 -5.706923 -4.996089 -4.2135177 -3.8116221][-3.1862772 -4.738203 -6.0818558 -6.6364484 -6.4135056 -5.6805778 -5.0415478 -5.14843 -5.4123988 -5.8335624 -5.9507689 -5.6367235 -4.8720756 -3.9702458 -3.65062][-2.1989527 -3.4484677 -4.929625 -5.32106 -4.8982105 -4.0758643 -3.3166718 -3.31945 -3.9981942 -5.108326 -5.524477 -5.2502069 -4.3992815 -3.287281 -2.9344232][-1.6363738 -2.6522739 -3.9366186 -4.04683 -3.3351135 -2.1747203 -0.95429552 -0.70928371 -1.7547147 -3.6269221 -4.5958037 -4.5657187 -3.8130889 -2.6130302 -2.2180717][-1.7161108 -2.3795888 -3.1523223 -2.6836865 -1.5661327 0.047898054 1.7417116 2.3311062 1.0361302 -1.5387987 -3.1941588 -3.7661843 -3.3438427 -2.2989035 -1.8210332][-2.1118679 -2.5558009 -2.5653837 -1.5711367 -0.1580956 1.8369665 4.0467424 4.8078356 3.4170418 0.56076741 -1.8022889 -3.1036625 -3.1515574 -2.2367911 -1.7803271][-3.2057819 -2.9761806 -2.415992 -1.1622626 0.42681026 2.71626 5.3466883 6.4523625 5.0867167 2.2038746 -0.6582495 -2.5606127 -2.9773717 -2.2817285 -1.8435513][-4.0547042 -3.4346595 -2.6186588 -1.4855156 0.055662155 2.3663268 5.1331491 6.4725614 5.3322177 2.6330223 -0.2302556 -2.2152822 -2.7841258 -2.2717309 -1.906841][-4.6351461 -4.0819325 -3.6467791 -3.002975 -1.7553065 0.32421541 2.8026977 4.2697554 3.5920582 1.3595347 -1.16292 -2.8992679 -3.4323344 -3.0902095 -2.7076757][-4.7177982 -4.4024544 -4.6021051 -4.5703092 -3.712585 -2.0625327 -0.017168283 1.4988146 1.2845314 -0.52924836 -2.5348594 -3.9511843 -4.3829317 -4.1423969 -3.8340812][-4.3551731 -4.374784 -5.215507 -5.7623358 -5.6068125 -4.5519686 -2.9593863 -1.4655004 -1.1840999 -2.49566 -3.7604749 -4.6464739 -4.8437185 -4.7587543 -4.8292093][-4.3172126 -4.5933161 -5.7440376 -6.5494251 -6.8677411 -6.4253817 -5.41191 -4.19046 -3.6008794 -4.2315311 -4.657239 -4.9574051 -4.9180036 -5.0044427 -5.3690815][-4.5409179 -4.9581547 -6.2663479 -6.98868 -7.4823337 -7.4234872 -6.8042078 -5.8885727 -5.30709 -5.5543976 -5.3494034 -5.251049 -5.0366793 -5.2254162 -5.839077][-4.6175823 -5.1965995 -6.4309235 -7.06056 -7.4750967 -7.3513122 -6.8918595 -6.3175015 -5.8396912 -5.8891611 -5.5859413 -5.3686972 -5.235775 -5.6420631 -6.4989481]]...]
INFO - root - 2017-12-15 08:21:48.859556: step 36210, loss = 0.34, batch loss = 0.31 (36.9 examples/sec; 0.217 sec/batch; 17h:51m:39s remains)
INFO - root - 2017-12-15 08:21:51.136778: step 36220, loss = 0.20, batch loss = 0.17 (35.4 examples/sec; 0.226 sec/batch; 18h:36m:21s remains)
INFO - root - 2017-12-15 08:21:53.442146: step 36230, loss = 0.27, batch loss = 0.23 (35.0 examples/sec; 0.228 sec/batch; 18h:47m:12s remains)
INFO - root - 2017-12-15 08:21:55.731611: step 36240, loss = 0.51, batch loss = 0.48 (35.8 examples/sec; 0.223 sec/batch; 18h:22m:12s remains)
INFO - root - 2017-12-15 08:21:58.009414: step 36250, loss = 0.26, batch loss = 0.23 (34.5 examples/sec; 0.232 sec/batch; 19h:04m:01s remains)
INFO - root - 2017-12-15 08:22:00.276471: step 36260, loss = 0.23, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 18h:19m:46s remains)
INFO - root - 2017-12-15 08:22:02.583491: step 36270, loss = 0.20, batch loss = 0.17 (35.6 examples/sec; 0.225 sec/batch; 18h:29m:29s remains)
INFO - root - 2017-12-15 08:22:04.831771: step 36280, loss = 0.22, batch loss = 0.19 (35.2 examples/sec; 0.227 sec/batch; 18h:42m:20s remains)
INFO - root - 2017-12-15 08:22:07.115559: step 36290, loss = 0.16, batch loss = 0.13 (35.4 examples/sec; 0.226 sec/batch; 18h:34m:42s remains)
INFO - root - 2017-12-15 08:22:09.449352: step 36300, loss = 0.21, batch loss = 0.18 (33.1 examples/sec; 0.241 sec/batch; 19h:51m:59s remains)
2017-12-15 08:22:09.720165: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.813823 -5.6234827 -5.7165337 -5.4334908 -4.7361584 -4.4880714 -4.0156531 -2.4107931 -2.50766 -4.2438679 -5.0608788 -5.8922086 -7.87675 -8.38744 -7.0963326][-6.0915842 -6.7973766 -6.9355416 -6.4652576 -5.772789 -5.4284749 -4.7595959 -3.2387877 -3.5779369 -5.3555927 -6.2482595 -7.2095203 -9.104805 -9.3968639 -7.9943404][-6.8607683 -6.8319583 -7.0950451 -6.5076966 -5.6381702 -4.9628143 -4.0529652 -2.8281624 -3.6018863 -5.5859942 -6.6038947 -7.7723055 -9.3704967 -9.2614212 -7.8417692][-7.1058464 -6.4399085 -6.72132 -6.0294809 -5.024519 -3.9609714 -2.6930487 -1.5744517 -2.6883025 -4.9171147 -6.0741253 -7.3465571 -8.669796 -8.4892693 -7.4291377][-6.8110747 -5.5560751 -5.8039947 -5.003377 -3.8956439 -2.4785109 -0.87375462 0.32445168 -0.83869362 -3.2812059 -4.73351 -6.1830182 -7.4099832 -7.3462048 -6.8578739][-5.6277065 -4.4216356 -4.6576309 -3.6756842 -2.2367008 -0.32024169 1.5996161 2.9848228 1.8783693 -0.90722561 -2.7998266 -4.432703 -5.8327112 -6.11826 -6.2835555][-3.5777874 -2.8643062 -3.1937532 -2.1173096 -0.39065742 1.8625889 4.0768342 5.7655177 4.7164745 1.607527 -0.69811845 -2.5900528 -4.4874058 -5.3038273 -6.0424919][-2.8383138 -2.2522819 -2.5505924 -1.2936554 0.62791395 2.9995184 5.3158956 7.2021775 6.1647248 2.9496932 0.48807621 -1.528726 -3.8504815 -5.023675 -6.1130805][-3.1577642 -2.8116977 -3.1180754 -1.791158 0.23553777 2.4718781 4.5611582 6.2702918 5.2383671 2.2880917 0.029935837 -1.9784778 -4.5199957 -5.811727 -6.8586693][-3.9518838 -3.821938 -4.2247772 -3.1126394 -1.3607371 0.33595347 1.9242883 3.2929668 2.3051805 -0.17017412 -1.9973558 -3.6593099 -5.8240538 -6.9035134 -7.631938][-5.1448112 -5.0908566 -5.4818659 -4.6661782 -3.3938043 -2.3137078 -1.2906952 -0.3067174 -1.0080724 -2.7791319 -4.1572618 -5.4817781 -7.0322337 -7.7391386 -8.111536][-6.4429159 -6.2997751 -6.614645 -6.1195126 -5.3681407 -4.7962618 -4.229434 -3.577142 -3.8813524 -4.9151659 -5.8675079 -6.7879128 -7.6246109 -7.9335351 -7.9768748][-7.0510812 -6.7444253 -6.977767 -6.7017083 -6.3311934 -6.1447325 -5.9006157 -5.51151 -5.5592422 -6.0505605 -6.5917187 -7.0443888 -7.2904873 -7.354486 -7.2644763][-6.8876057 -6.3863516 -6.5416083 -6.4083796 -6.292551 -6.3256149 -6.2415614 -5.9865913 -5.9253883 -6.1312113 -6.3840384 -6.5112295 -6.4981089 -6.483839 -6.4183869][-6.4322848 -5.7009244 -5.7748756 -5.710722 -5.6851416 -5.7194247 -5.6652045 -5.540771 -5.5003586 -5.5809259 -5.6772652 -5.6861792 -5.6629467 -5.6817155 -5.6785407]]...]
INFO - root - 2017-12-15 08:22:12.007504: step 36310, loss = 0.29, batch loss = 0.26 (35.3 examples/sec; 0.226 sec/batch; 18h:37m:19s remains)
INFO - root - 2017-12-15 08:22:14.287424: step 36320, loss = 0.25, batch loss = 0.21 (36.4 examples/sec; 0.220 sec/batch; 18h:06m:05s remains)
INFO - root - 2017-12-15 08:22:16.612360: step 36330, loss = 0.26, batch loss = 0.22 (33.2 examples/sec; 0.241 sec/batch; 19h:48m:38s remains)
INFO - root - 2017-12-15 08:22:18.861695: step 36340, loss = 0.30, batch loss = 0.27 (35.8 examples/sec; 0.223 sec/batch; 18h:22m:00s remains)
INFO - root - 2017-12-15 08:22:21.105279: step 36350, loss = 0.22, batch loss = 0.18 (35.7 examples/sec; 0.224 sec/batch; 18h:26m:36s remains)
INFO - root - 2017-12-15 08:22:23.390623: step 36360, loss = 0.29, batch loss = 0.26 (34.3 examples/sec; 0.233 sec/batch; 19h:11m:44s remains)
INFO - root - 2017-12-15 08:22:25.705202: step 36370, loss = 0.21, batch loss = 0.18 (35.1 examples/sec; 0.228 sec/batch; 18h:45m:29s remains)
INFO - root - 2017-12-15 08:22:27.987379: step 36380, loss = 0.20, batch loss = 0.16 (35.5 examples/sec; 0.225 sec/batch; 18h:32m:31s remains)
INFO - root - 2017-12-15 08:22:30.262836: step 36390, loss = 0.26, batch loss = 0.23 (35.7 examples/sec; 0.224 sec/batch; 18h:26m:10s remains)
INFO - root - 2017-12-15 08:22:32.547782: step 36400, loss = 0.22, batch loss = 0.18 (34.5 examples/sec; 0.232 sec/batch; 19h:02m:45s remains)
2017-12-15 08:22:32.860259: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0874214 -5.4061966 -6.5513134 -6.7781439 -6.9667168 -6.9577036 -6.6734705 -6.579586 -6.7784224 -6.5995979 -6.2741756 -6.3738012 -6.6419592 -6.6214266 -6.5066471][-4.1225643 -5.9361744 -7.0757427 -7.368412 -7.5869055 -7.510293 -7.1191673 -7.0287256 -7.2937851 -7.23083 -7.0671282 -7.2299767 -7.5518055 -7.62352 -7.5778627][-5.1948185 -6.3251057 -7.36209 -7.6435175 -7.6187563 -7.1611347 -6.4430189 -6.2644119 -6.8056688 -7.0674906 -7.14952 -7.3965349 -7.8590422 -8.1226759 -8.0398092][-5.6311493 -6.25325 -6.796629 -6.8050785 -6.3909349 -5.5550475 -4.4534931 -4.2338619 -5.2296906 -5.9469576 -6.325882 -6.59138 -7.164978 -7.696888 -7.5665903][-5.8175106 -5.6538906 -5.6634388 -5.5130968 -4.8082895 -3.6138272 -2.1099684 -1.7428106 -2.9735343 -4.2072988 -4.9188585 -5.0012097 -5.3052936 -5.7849417 -5.5356121][-5.6341639 -4.6544104 -4.3981829 -4.3488016 -3.4486179 -1.9612125 -0.11204362 0.53840089 -0.55252564 -2.2114642 -3.4168582 -3.5172434 -3.5072179 -3.6395965 -3.1116371][-4.2604752 -2.6752796 -2.1984746 -2.4093971 -1.5775574 0.092267275 2.3080795 3.3305061 2.5027936 0.51485181 -1.4099889 -2.0091815 -1.9580185 -1.7620978 -0.90218687][-2.6351538 -0.32865691 0.36544514 -0.17633152 0.27108407 1.8399723 4.0215292 5.1558056 4.5918674 2.5704663 0.33791828 -0.59000993 -0.58694565 -0.067770481 1.1486232][-1.6691451 0.94744873 1.2819097 0.07449317 -0.20325351 0.88785839 2.7336318 3.9323394 3.8747241 2.2847121 0.36559415 -0.43671632 -0.25870717 0.64800739 2.1560681][-1.0110569 1.6701388 1.5846732 -0.11262798 -1.0076571 -0.52756262 0.70902109 1.7498176 1.9272768 0.76590729 -0.74674356 -1.4512293 -1.2741609 -0.28777921 1.299387][-0.47667813 2.1252491 1.6393843 -0.45518982 -1.8628201 -1.9975374 -1.4264286 -0.65701032 -0.45048273 -1.2524983 -2.3098793 -2.7711406 -2.5556483 -1.5160435 -0.018078327][-0.62258673 1.6006694 0.58703423 -1.7338808 -3.2810378 -3.6928558 -3.5067225 -2.9277139 -2.7233903 -3.1971397 -3.7196221 -3.8771727 -3.6790752 -2.802371 -1.739321][-0.86652553 0.8062346 -0.48704171 -2.6513493 -4.1627164 -4.8105974 -4.9078951 -4.3995676 -4.1732469 -4.3889494 -4.5574684 -4.64005 -4.5577011 -3.9588206 -3.3758264][-1.2333301 0.012759447 -1.307816 -3.2318282 -4.6843209 -5.5362611 -5.8680325 -5.50263 -5.4082365 -5.5489798 -5.5262346 -5.4744825 -5.2666717 -4.680912 -4.3807497][-2.4287698 -1.361025 -2.4362712 -3.9198685 -4.9882174 -5.6287737 -5.8602858 -5.5329361 -5.5235405 -5.6200247 -5.5493059 -5.5018473 -5.2484236 -4.7069559 -4.5811758]]...]
INFO - root - 2017-12-15 08:22:35.134380: step 36410, loss = 0.20, batch loss = 0.16 (33.1 examples/sec; 0.242 sec/batch; 19h:54m:09s remains)
INFO - root - 2017-12-15 08:22:37.425747: step 36420, loss = 0.27, batch loss = 0.24 (35.6 examples/sec; 0.225 sec/batch; 18h:30m:05s remains)
INFO - root - 2017-12-15 08:22:39.696898: step 36430, loss = 0.21, batch loss = 0.18 (35.5 examples/sec; 0.225 sec/batch; 18h:31m:51s remains)
INFO - root - 2017-12-15 08:22:41.976131: step 36440, loss = 0.25, batch loss = 0.22 (34.6 examples/sec; 0.231 sec/batch; 18h:59m:17s remains)
INFO - root - 2017-12-15 08:22:44.247650: step 36450, loss = 0.18, batch loss = 0.15 (35.3 examples/sec; 0.227 sec/batch; 18h:38m:22s remains)
INFO - root - 2017-12-15 08:22:46.516317: step 36460, loss = 0.19, batch loss = 0.15 (34.6 examples/sec; 0.231 sec/batch; 18h:59m:29s remains)
INFO - root - 2017-12-15 08:22:48.807521: step 36470, loss = 0.21, batch loss = 0.17 (35.6 examples/sec; 0.225 sec/batch; 18h:29m:28s remains)
INFO - root - 2017-12-15 08:22:51.099326: step 36480, loss = 0.15, batch loss = 0.11 (35.2 examples/sec; 0.227 sec/batch; 18h:40m:57s remains)
INFO - root - 2017-12-15 08:22:53.396912: step 36490, loss = 0.21, batch loss = 0.17 (31.5 examples/sec; 0.254 sec/batch; 20h:54m:06s remains)
INFO - root - 2017-12-15 08:22:55.688444: step 36500, loss = 0.22, batch loss = 0.18 (34.9 examples/sec; 0.229 sec/batch; 18h:50m:24s remains)
2017-12-15 08:22:55.969506: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4318039 -3.0349555 -3.1656523 -3.5268393 -4.1630363 -4.2734327 -3.7226281 -2.5539353 -1.8953054 -2.0129547 -2.8803306 -3.5748186 -4.1444511 -4.9220595 -5.1824737][-1.4943531 -2.6068788 -2.8829789 -3.3980606 -4.29149 -4.4477348 -3.8926344 -2.8647897 -2.3982067 -2.6331816 -3.4035659 -4.0496383 -4.6309586 -5.3808241 -5.6834164][-1.889825 -2.166393 -2.3383183 -2.8068917 -3.7981665 -4.0142059 -3.4304621 -2.6418855 -2.5542996 -3.0941174 -3.9062495 -4.4754839 -5.0677481 -5.7049541 -5.93128][-2.35941 -1.7275925 -1.8488747 -2.12577 -2.9365759 -2.9472151 -2.0901875 -1.391726 -1.6879411 -2.7370944 -3.9214931 -4.6095448 -5.30466 -5.8924713 -6.0849915][-2.2613494 -1.0651811 -1.0855536 -1.129922 -1.5218147 -0.96536267 0.4236691 1.2081144 0.49667668 -1.2232099 -3.05012 -4.0650554 -5.0647717 -5.8471689 -6.1559176][-2.1880832 -0.64718485 -0.43861878 -0.16672111 -0.024634361 1.1688402 3.0608416 3.959897 2.9889703 0.78845572 -1.6900859 -3.1418407 -4.5521588 -5.6519089 -6.2279005][-2.2080081 -0.6258595 -0.16427279 0.502265 1.2785017 3.0798678 5.2604561 6.1570077 5.0035906 2.4678159 -0.48078394 -2.3048964 -4.0922885 -5.5444403 -6.346602][-3.0320411 -1.3911505 -0.70249283 0.26637697 1.6322172 3.9321122 6.1357245 6.843596 5.5465264 2.9153013 -0.10939837 -2.1334145 -4.0563116 -5.5950055 -6.4820518][-4.6024332 -3.0004916 -2.294394 -1.2269833 0.5159204 3.0176082 5.004416 5.5042338 4.2678337 1.9070535 -0.7305696 -2.7304688 -4.5382037 -5.9317894 -6.6840358][-5.8981123 -4.4359336 -3.7895012 -2.835638 -1.0558459 1.20965 2.6864119 2.906004 1.8618097 -0.0905025 -2.1252797 -3.840683 -5.269968 -6.389214 -6.8242779][-6.34265 -5.0653524 -4.5626988 -3.8387442 -2.3326542 -0.533514 0.37674308 0.33204508 -0.62355673 -2.2005875 -3.7201633 -5.1550741 -6.108283 -6.8187137 -6.8223696][-5.7788329 -4.8321037 -4.5005274 -4.0887785 -3.0563543 -1.8137836 -1.3650948 -1.6737008 -2.7009444 -4.1096296 -5.4395056 -6.6957145 -7.1908913 -7.4004722 -6.9757366][-4.746913 -3.9086452 -3.7926259 -3.776257 -3.2837381 -2.470572 -2.292012 -2.7613187 -3.9254169 -5.2603436 -6.5575337 -7.746119 -8.0164185 -7.959857 -7.3422647][-3.8834431 -2.8674784 -2.8656342 -3.196835 -3.0780177 -2.461298 -2.3149016 -2.8416052 -4.1581411 -5.5636773 -6.8771996 -8.100069 -8.41449 -8.3250656 -7.6636481][-4.0931363 -2.8997102 -2.980653 -3.5308354 -3.6729248 -3.1859188 -3.0033929 -3.4897227 -4.8360662 -6.1468883 -7.2619061 -8.3989363 -8.6855125 -8.4409246 -7.6489658]]...]
INFO - root - 2017-12-15 08:22:58.208522: step 36510, loss = 0.30, batch loss = 0.27 (36.1 examples/sec; 0.222 sec/batch; 18h:14m:42s remains)
INFO - root - 2017-12-15 08:23:00.505302: step 36520, loss = 0.22, batch loss = 0.19 (34.4 examples/sec; 0.233 sec/batch; 19h:07m:16s remains)
INFO - root - 2017-12-15 08:23:02.778427: step 36530, loss = 0.24, batch loss = 0.21 (35.8 examples/sec; 0.224 sec/batch; 18h:23m:08s remains)
INFO - root - 2017-12-15 08:23:05.089369: step 36540, loss = 0.32, batch loss = 0.29 (35.2 examples/sec; 0.227 sec/batch; 18h:41m:58s remains)
INFO - root - 2017-12-15 08:23:07.397013: step 36550, loss = 0.24, batch loss = 0.21 (34.0 examples/sec; 0.235 sec/batch; 19h:19m:48s remains)
INFO - root - 2017-12-15 08:23:09.670025: step 36560, loss = 0.29, batch loss = 0.25 (34.7 examples/sec; 0.231 sec/batch; 18h:58m:36s remains)
INFO - root - 2017-12-15 08:23:11.932351: step 36570, loss = 0.26, batch loss = 0.23 (36.3 examples/sec; 0.221 sec/batch; 18h:07m:49s remains)
INFO - root - 2017-12-15 08:23:14.228322: step 36580, loss = 0.19, batch loss = 0.16 (35.7 examples/sec; 0.224 sec/batch; 18h:25m:46s remains)
INFO - root - 2017-12-15 08:23:16.489282: step 36590, loss = 0.31, batch loss = 0.27 (36.1 examples/sec; 0.222 sec/batch; 18h:14m:16s remains)
INFO - root - 2017-12-15 08:23:18.756189: step 36600, loss = 0.23, batch loss = 0.20 (34.5 examples/sec; 0.232 sec/batch; 19h:04m:28s remains)
2017-12-15 08:23:19.036979: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2711525 -5.1508126 -5.3548508 -5.5177507 -5.7180023 -5.8070364 -5.7920227 -5.7208929 -5.7033186 -5.8264275 -6.0139766 -5.9855442 -5.81482 -5.5710955 -5.4162054][-4.0096188 -5.2782288 -5.5879579 -5.8646393 -6.1368632 -6.2618628 -6.115891 -5.9487648 -6.020998 -6.496789 -7.132782 -7.33152 -7.2618389 -7.0435963 -6.8432312][-4.1790442 -5.1085377 -5.4569244 -5.7658243 -5.9991078 -6.0223036 -5.5770564 -5.1606569 -5.2339087 -6.0603814 -7.2993469 -7.8984203 -8.116745 -8.0600777 -7.9478817][-4.3073163 -4.8444881 -5.1069374 -5.34161 -5.3158455 -4.9865718 -3.9884872 -3.0895977 -3.0118492 -4.1556644 -5.963666 -6.9617205 -7.5856304 -7.8780279 -8.0483427][-4.7756281 -4.6196003 -4.6642113 -4.6827726 -4.2308574 -3.3507972 -1.5585893 -0.08590889 0.069291353 -1.3383415 -3.6017618 -4.9206858 -5.8502431 -6.55667 -7.0572729][-4.7930803 -4.410347 -4.121439 -3.7768614 -2.9212673 -1.3932149 1.292645 3.3847492 3.4997108 1.7619212 -0.923666 -2.5791483 -3.795049 -4.84299 -5.604846][-4.5783272 -4.262207 -3.6314425 -2.9041772 -1.6428304 0.52668357 3.8768971 6.3512592 6.2025185 4.2159548 1.4935541 -0.27951479 -1.8335638 -3.2811105 -4.3536081][-4.6836548 -4.2523947 -3.4744935 -2.5490148 -1.0171305 1.5206263 5.0791769 7.4202652 6.7932425 4.7828512 2.3557293 0.73426485 -1.072439 -2.9688227 -4.4448509][-4.662807 -4.2942638 -3.6588991 -2.8215706 -1.3965793 1.0142224 4.1741953 5.986598 4.9940481 3.2809308 1.4958227 0.15885234 -1.7336948 -3.7157955 -5.1166124][-4.7741642 -4.5932446 -4.3171263 -3.7928424 -2.6961112 -0.72939515 1.7532489 3.03215 2.0939896 1.0230811 0.073292971 -0.95675349 -2.8306375 -4.7190952 -6.0143366][-5.0177069 -4.9862871 -5.0429134 -4.8582077 -4.1352181 -2.7030182 -0.89048755 -0.057405472 -0.79528725 -1.2036916 -1.5519122 -2.3679047 -4.1504493 -5.8907509 -7.1336689][-5.1940522 -5.1645927 -5.33767 -5.3329482 -4.9537296 -4.1085939 -2.957562 -2.4741018 -3.0101948 -3.1759243 -3.2856379 -3.9527531 -5.3978939 -6.6760597 -7.5130749][-5.465704 -5.4122534 -5.6342268 -5.7079811 -5.56231 -5.1587763 -4.5564032 -4.3416424 -4.6693535 -4.7581682 -4.6702023 -5.0592213 -5.9499321 -6.6647434 -7.042984][-5.603581 -5.4812436 -5.6845608 -5.728713 -5.6628675 -5.4638495 -5.1572981 -5.0478563 -5.11862 -5.099309 -4.9524488 -5.0866318 -5.5156174 -5.8909307 -6.1109943][-5.509057 -5.2516603 -5.37046 -5.3574705 -5.3093357 -5.227829 -5.0660605 -4.9723339 -4.9219432 -4.9014125 -4.755795 -4.7008486 -4.823617 -5.0261946 -5.17537]]...]
INFO - root - 2017-12-15 08:23:21.330456: step 36610, loss = 0.38, batch loss = 0.35 (35.4 examples/sec; 0.226 sec/batch; 18h:35m:03s remains)
INFO - root - 2017-12-15 08:23:23.620649: step 36620, loss = 0.25, batch loss = 0.22 (34.4 examples/sec; 0.233 sec/batch; 19h:06m:58s remains)
INFO - root - 2017-12-15 08:23:25.886603: step 36630, loss = 0.37, batch loss = 0.34 (35.5 examples/sec; 0.225 sec/batch; 18h:29m:49s remains)
INFO - root - 2017-12-15 08:23:28.143623: step 36640, loss = 0.26, batch loss = 0.23 (35.8 examples/sec; 0.224 sec/batch; 18h:22m:26s remains)
INFO - root - 2017-12-15 08:23:30.406752: step 36650, loss = 0.24, batch loss = 0.21 (35.4 examples/sec; 0.226 sec/batch; 18h:32m:54s remains)
INFO - root - 2017-12-15 08:23:32.651218: step 36660, loss = 0.24, batch loss = 0.21 (34.2 examples/sec; 0.234 sec/batch; 19h:12m:19s remains)
INFO - root - 2017-12-15 08:23:34.952418: step 36670, loss = 0.20, batch loss = 0.16 (34.9 examples/sec; 0.229 sec/batch; 18h:48m:37s remains)
INFO - root - 2017-12-15 08:23:37.235552: step 36680, loss = 0.31, batch loss = 0.28 (34.7 examples/sec; 0.230 sec/batch; 18h:55m:52s remains)
INFO - root - 2017-12-15 08:23:39.526072: step 36690, loss = 0.18, batch loss = 0.15 (35.2 examples/sec; 0.227 sec/batch; 18h:41m:14s remains)
INFO - root - 2017-12-15 08:23:41.765484: step 36700, loss = 0.17, batch loss = 0.14 (35.4 examples/sec; 0.226 sec/batch; 18h:32m:47s remains)
2017-12-15 08:23:42.070262: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.9165483 -6.9971318 -6.5530062 -6.5829105 -6.9315939 -7.0843687 -7.9356828 -8.4491253 -8.3500175 -8.3825378 -8.8135729 -8.9202118 -8.4679813 -8.2445412 -8.598629][-8.1398535 -6.5847626 -6.010818 -6.0150642 -6.3821754 -6.1847763 -6.7364259 -7.2131581 -7.21928 -7.5283813 -8.4396477 -8.962616 -8.5456257 -8.3005571 -8.8954191][-8.5086517 -5.8619671 -5.1514912 -5.0289626 -5.203114 -4.5320816 -4.573431 -4.7431526 -4.8473067 -5.5864973 -6.9210167 -7.802772 -7.5879827 -7.4720464 -8.26582][-8.3571606 -5.111227 -4.2986746 -4.0198975 -3.9562397 -2.762917 -2.03489 -1.6459219 -1.7644365 -2.9511583 -4.6209607 -5.7364345 -5.8948755 -6.1451674 -7.180059][-8.6401739 -4.8000774 -3.7350721 -3.1501386 -2.7946415 -0.97573733 0.54153562 1.3765001 1.2057183 -0.35539162 -2.2883723 -3.6992245 -4.3453922 -4.9845705 -6.1479979][-8.427 -4.6435137 -3.274229 -2.2551825 -1.4142311 0.99618983 2.994875 3.8739562 3.40386 1.3420713 -0.98152792 -2.7758744 -3.9487021 -4.931674 -6.113802][-8.1615086 -4.6493492 -2.8966823 -1.2789937 0.17290878 2.8055286 4.7039366 5.2303362 4.3585668 1.9612198 -0.6781162 -2.7326624 -4.1293616 -5.1728077 -6.2312579][-8.4062614 -4.7404671 -2.7643237 -0.70672321 1.2583165 3.8158164 5.2995768 5.3501215 4.2971945 1.9290876 -0.73124242 -2.9751186 -4.4681406 -5.4886494 -6.7084317][-8.5337515 -5.0035949 -3.1495867 -0.98680031 1.371635 3.7906265 4.8558373 4.5168929 3.1150265 0.65011668 -1.8874662 -4.0559587 -5.4151096 -6.2903085 -7.5875006][-8.5062914 -5.44946 -4.1059837 -2.2625067 0.086089849 2.1104317 2.8180199 2.1915798 0.46788573 -2.0074141 -4.219902 -5.9128237 -6.7373695 -7.0877066 -7.9732695][-8.5588446 -6.1109791 -5.32524 -3.985795 -2.0302131 -0.43365884 0.016636848 -0.70819342 -2.4004364 -4.4444046 -6.0753546 -7.3803267 -7.9117217 -8.0290718 -8.5611076][-8.3258648 -6.1564808 -5.4766421 -4.4546289 -3.0837324 -1.9350629 -1.6887932 -2.4268343 -4.0432458 -5.6713448 -6.8700795 -7.8336167 -8.23664 -8.3944607 -8.6812086][-8.7703838 -6.8455424 -6.1665392 -5.3343325 -4.4701843 -3.6656766 -3.4720371 -4.038867 -5.4149857 -6.6829233 -7.5726428 -8.10169 -8.2413015 -8.2687216 -8.1882648][-9.7420549 -8.15066 -7.5166163 -6.801949 -6.2409282 -5.6681218 -5.5115018 -5.7343168 -6.5535755 -7.3389735 -7.8727541 -7.9433107 -7.7449651 -7.614428 -7.3719168][-9.6874714 -8.2748432 -7.7863059 -7.2172403 -6.7950468 -6.4186697 -6.354404 -6.1803041 -6.4436531 -6.8985887 -7.2517738 -7.1008253 -6.8119516 -6.7322736 -6.5393848]]...]
INFO - root - 2017-12-15 08:23:44.317282: step 36710, loss = 0.23, batch loss = 0.20 (34.8 examples/sec; 0.230 sec/batch; 18h:52m:18s remains)
INFO - root - 2017-12-15 08:23:46.612386: step 36720, loss = 0.18, batch loss = 0.15 (33.5 examples/sec; 0.239 sec/batch; 19h:38m:09s remains)
INFO - root - 2017-12-15 08:23:48.888374: step 36730, loss = 0.22, batch loss = 0.18 (35.9 examples/sec; 0.223 sec/batch; 18h:18m:33s remains)
INFO - root - 2017-12-15 08:23:51.135374: step 36740, loss = 0.19, batch loss = 0.16 (34.0 examples/sec; 0.235 sec/batch; 19h:18m:36s remains)
INFO - root - 2017-12-15 08:23:53.464565: step 36750, loss = 0.23, batch loss = 0.19 (34.8 examples/sec; 0.230 sec/batch; 18h:51m:40s remains)
INFO - root - 2017-12-15 08:23:55.731376: step 36760, loss = 0.23, batch loss = 0.20 (35.3 examples/sec; 0.227 sec/batch; 18h:37m:32s remains)
INFO - root - 2017-12-15 08:23:57.997813: step 36770, loss = 0.33, batch loss = 0.30 (34.4 examples/sec; 0.233 sec/batch; 19h:06m:09s remains)
INFO - root - 2017-12-15 08:24:00.255358: step 36780, loss = 0.22, batch loss = 0.19 (36.3 examples/sec; 0.221 sec/batch; 18h:07m:22s remains)
INFO - root - 2017-12-15 08:24:02.544971: step 36790, loss = 0.28, batch loss = 0.25 (33.4 examples/sec; 0.240 sec/batch; 19h:40m:49s remains)
INFO - root - 2017-12-15 08:24:04.848899: step 36800, loss = 0.21, batch loss = 0.18 (36.0 examples/sec; 0.222 sec/batch; 18h:14m:27s remains)
2017-12-15 08:24:05.116872: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8302672 -6.3413992 -6.951808 -7.8585339 -8.6688519 -9.194066 -9.6218472 -9.7576809 -9.6081915 -9.3269339 -8.3793936 -6.9697018 -5.87316 -5.1281414 -4.9236846][-2.7710218 -4.7518187 -5.6903582 -7.059844 -8.0273743 -8.4089928 -8.7508173 -8.909193 -8.7987642 -8.74206 -7.9269924 -6.4463148 -5.2113261 -4.3157492 -3.9585228][-2.1454849 -3.311595 -4.6391659 -6.456583 -7.3899803 -7.4807014 -7.5912209 -7.6228294 -7.4742775 -7.5977 -7.0734329 -5.7535276 -4.4635286 -3.5466821 -3.193392][-2.4341991 -2.5345829 -4.2964392 -6.4710875 -7.1962795 -6.8306618 -6.4445076 -6.115346 -5.87953 -6.3289757 -6.19665 -5.08794 -3.6901772 -2.705307 -2.55896][-3.0820024 -2.6710901 -4.7015409 -6.8882217 -7.1155281 -6.2103081 -5.2861242 -4.4615488 -4.242043 -5.1228552 -5.5164418 -5.0670929 -3.8098996 -2.7642345 -2.7627573][-4.4931526 -3.6419289 -5.4703307 -7.0396891 -6.33442 -4.6651697 -2.9520433 -1.5808916 -1.4705209 -3.0561018 -4.2163162 -4.7929983 -4.3286886 -3.7423615 -4.0690002][-6.03043 -4.9751048 -6.0800829 -6.4029169 -4.3845525 -1.6720582 0.82061315 2.6675889 2.3998191 -0.053248882 -1.9774286 -3.8360665 -4.6105185 -4.8860369 -5.7321224][-6.4945908 -5.3110766 -5.9189234 -5.422821 -2.59973 0.80918574 3.9922464 6.1797209 5.5768652 2.5018151 0.10513806 -2.4611738 -4.154922 -5.2569804 -6.6138906][-6.8468542 -5.8070545 -6.3804016 -5.6519432 -2.7609901 0.79867935 4.2107849 6.7398062 6.1433916 3.1114357 0.7652936 -2.0425673 -4.0386214 -5.2932258 -6.6828623][-7.0218973 -6.2411013 -6.9775057 -6.3788033 -3.9064093 -0.77728355 2.4039304 4.7708244 4.2437878 1.5903206 -0.599965 -3.219368 -4.8178396 -5.63038 -6.4541416][-6.4228888 -5.9078445 -7.0107079 -6.8468237 -5.1689997 -2.7098351 -0.19904804 1.7197852 1.2601693 -0.861922 -2.6567416 -4.62084 -5.3588324 -5.4928703 -5.72801][-5.0690126 -4.9851274 -6.6305962 -7.1823063 -6.4269724 -4.6980853 -2.8132455 -1.485563 -1.7705953 -3.1514034 -4.3185396 -5.2871881 -5.0512562 -4.7130003 -4.7525682][-4.6384892 -4.5516462 -6.311799 -7.3571024 -7.5012474 -6.4875383 -5.268322 -4.4619584 -4.5681982 -5.2274857 -5.5974741 -5.293891 -4.1622238 -3.6903868 -3.9514689][-4.9016876 -4.6582546 -6.0474658 -7.1702766 -7.8755112 -7.4782734 -6.71461 -6.2142687 -6.1569242 -6.2477536 -5.9882421 -4.946147 -3.5418715 -3.2769675 -3.8340201][-5.6723557 -5.281724 -6.1745129 -7.009449 -7.7160692 -7.6093121 -7.2130222 -6.9258547 -6.8400335 -6.7136059 -6.2638378 -5.1803312 -4.1405969 -4.2130756 -4.8770208]]...]
INFO - root - 2017-12-15 08:24:07.376733: step 36810, loss = 0.20, batch loss = 0.17 (35.2 examples/sec; 0.228 sec/batch; 18h:41m:15s remains)
INFO - root - 2017-12-15 08:24:09.662383: step 36820, loss = 0.37, batch loss = 0.34 (34.7 examples/sec; 0.230 sec/batch; 18h:55m:42s remains)
INFO - root - 2017-12-15 08:24:11.921634: step 36830, loss = 0.27, batch loss = 0.24 (35.4 examples/sec; 0.226 sec/batch; 18h:33m:12s remains)
INFO - root - 2017-12-15 08:24:14.219903: step 36840, loss = 0.22, batch loss = 0.19 (36.1 examples/sec; 0.222 sec/batch; 18h:12m:54s remains)
INFO - root - 2017-12-15 08:24:16.461453: step 36850, loss = 0.22, batch loss = 0.19 (36.0 examples/sec; 0.222 sec/batch; 18h:15m:30s remains)
INFO - root - 2017-12-15 08:24:18.750775: step 36860, loss = 0.29, batch loss = 0.26 (34.8 examples/sec; 0.230 sec/batch; 18h:51m:09s remains)
INFO - root - 2017-12-15 08:24:21.018260: step 36870, loss = 0.19, batch loss = 0.16 (35.2 examples/sec; 0.227 sec/batch; 18h:39m:13s remains)
INFO - root - 2017-12-15 08:24:23.266692: step 36880, loss = 0.19, batch loss = 0.15 (35.3 examples/sec; 0.227 sec/batch; 18h:36m:26s remains)
INFO - root - 2017-12-15 08:24:25.547818: step 36890, loss = 0.25, batch loss = 0.22 (35.3 examples/sec; 0.226 sec/batch; 18h:35m:33s remains)
INFO - root - 2017-12-15 08:24:27.829198: step 36900, loss = 0.26, batch loss = 0.23 (33.4 examples/sec; 0.240 sec/batch; 19h:40m:23s remains)
2017-12-15 08:24:28.134757: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.6419915 -0.5204432 -1.3373442 -3.0950463 -4.2142344 -4.7022943 -5.4602671 -5.8087511 -5.1817923 -4.8615541 -4.7001953 -4.406291 -3.7679145 -2.5187273 -1.7193856][-2.3335116 -1.7477467 -2.8081799 -4.736311 -5.8420782 -6.1876144 -6.5856466 -6.3037729 -5.1744537 -4.7494307 -4.5512772 -4.062016 -3.3716922 -2.2029266 -1.3832357][-4.2166247 -3.36594 -4.4718051 -6.264925 -7.0618896 -6.89141 -6.6811428 -5.7099395 -4.1995516 -3.7811882 -3.7888994 -3.2616382 -2.7234347 -1.84148 -1.2469391][-5.2758074 -4.2736645 -5.2390504 -6.7697182 -7.294095 -6.5242281 -5.5580997 -3.9730029 -2.1902313 -2.0347705 -2.6711271 -2.5572839 -2.3887033 -1.8128862 -1.4890038][-5.4001684 -4.1578627 -4.9580097 -6.1247463 -6.4760981 -5.1459732 -3.3744917 -1.1790231 0.85526919 0.54858446 -0.96529889 -1.7124624 -2.2106943 -1.9756191 -1.9480681][-4.265954 -2.6765337 -3.1441562 -4.0013409 -4.4654016 -2.966228 -0.6440177 1.9350674 3.9155037 2.8919356 0.30268312 -1.4954484 -2.8898523 -3.1122975 -3.2527065][-2.2477572 -0.51925576 -0.75231183 -1.5612741 -2.3497806 -0.8600862 1.7305257 4.4317846 6.2679815 4.7519007 1.4280684 -1.3131533 -3.6470671 -4.4944124 -4.7819953][-0.697436 1.5047297 1.474129 0.53993154 -0.81213844 0.18194127 2.4852831 4.8948965 6.4402552 4.8172407 1.4666841 -1.5511726 -4.2954931 -5.3507829 -5.5603719][0.25309229 2.7678778 2.7158778 1.4679248 -0.38536191 -0.0973084 1.4188154 3.0778658 4.0612354 2.5657895 -0.27095342 -2.9040012 -5.2166986 -5.8720379 -5.734376][0.3035543 2.7304447 2.5126383 0.92924666 -1.2672732 -1.6599076 -0.87443805 0.0920825 0.65913248 -0.43699098 -2.4754412 -4.30789 -5.8599191 -6.0925875 -5.6434412][-0.708158 1.3652318 0.87167764 -0.7499342 -2.7728162 -3.4045272 -3.1696739 -2.8762655 -2.6181059 -3.2687116 -4.5987554 -5.7116117 -6.5420923 -6.3471842 -5.6207414][-2.0925138 -0.6705389 -1.2503153 -2.5628226 -4.1748209 -5.0193634 -5.3535786 -5.5725946 -5.5256519 -5.9680357 -6.8153095 -7.2565002 -7.5070152 -6.9322004 -5.8529367][-4.1096253 -3.296452 -3.8413181 -4.8099937 -6.0279169 -6.8504219 -7.3849478 -7.7927485 -7.8092585 -7.9948206 -8.3086834 -8.2348537 -8.0588284 -7.1126385 -5.8108387][-5.9596853 -5.6788645 -6.1999116 -6.8513527 -7.6485138 -8.2167549 -8.6646261 -8.9432735 -8.8346853 -8.7664423 -8.7480774 -8.5311232 -8.2078619 -7.1145868 -5.7137537][-6.7677746 -6.8386822 -7.2734814 -7.6800861 -8.14823 -8.4365292 -8.7524538 -8.8568468 -8.5210943 -8.32519 -8.2838573 -8.309082 -8.0386362 -6.8365078 -5.5766306]]...]
INFO - root - 2017-12-15 08:24:30.429262: step 36910, loss = 0.28, batch loss = 0.25 (34.5 examples/sec; 0.232 sec/batch; 19h:00m:58s remains)
INFO - root - 2017-12-15 08:24:32.712239: step 36920, loss = 0.20, batch loss = 0.17 (34.6 examples/sec; 0.231 sec/batch; 18h:59m:32s remains)
INFO - root - 2017-12-15 08:24:35.020385: step 36930, loss = 0.21, batch loss = 0.18 (32.9 examples/sec; 0.244 sec/batch; 19h:59m:32s remains)
INFO - root - 2017-12-15 08:24:37.287809: step 36940, loss = 0.17, batch loss = 0.14 (36.0 examples/sec; 0.223 sec/batch; 18h:16m:09s remains)
INFO - root - 2017-12-15 08:24:39.580416: step 36950, loss = 0.21, batch loss = 0.17 (36.4 examples/sec; 0.220 sec/batch; 18h:03m:45s remains)
INFO - root - 2017-12-15 08:24:41.870856: step 36960, loss = 0.22, batch loss = 0.19 (35.7 examples/sec; 0.224 sec/batch; 18h:24m:03s remains)
INFO - root - 2017-12-15 08:24:44.120956: step 36970, loss = 0.19, batch loss = 0.16 (34.1 examples/sec; 0.235 sec/batch; 19h:16m:10s remains)
INFO - root - 2017-12-15 08:24:46.398120: step 36980, loss = 0.18, batch loss = 0.14 (36.2 examples/sec; 0.221 sec/batch; 18h:08m:38s remains)
INFO - root - 2017-12-15 08:24:48.687787: step 36990, loss = 0.29, batch loss = 0.26 (34.9 examples/sec; 0.229 sec/batch; 18h:49m:45s remains)
INFO - root - 2017-12-15 08:24:50.943738: step 37000, loss = 0.16, batch loss = 0.13 (33.0 examples/sec; 0.242 sec/batch; 19h:52m:56s remains)
2017-12-15 08:24:51.230354: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3357823 -0.82632482 -0.74035144 -1.0026287 -1.2241263 -2.0247092 -2.8932707 -3.0192719 -2.8491449 -2.5667806 -2.0369761 -1.883299 -2.1586843 -3.2406943 -3.9576986][-3.5845432 -1.5237741 -1.6810687 -1.9825585 -2.2200618 -3.070581 -3.713366 -3.4175658 -2.8404384 -2.2396014 -1.696744 -1.81196 -2.3355815 -3.6468396 -4.6393204][-4.3351941 -1.7268822 -2.311996 -2.9232078 -3.2715719 -4.0578318 -4.3872967 -3.7018137 -2.8384438 -1.97488 -1.3642509 -1.5636852 -2.0950997 -3.4318814 -4.6716604][-4.6129818 -1.4007654 -2.1813319 -3.0632758 -3.592823 -4.3686857 -4.5435114 -3.533582 -2.4260013 -1.3338404 -0.54268754 -0.63121414 -0.94761705 -2.1386623 -3.6592412][-4.6472759 -0.86398447 -1.4246182 -2.2837379 -2.7806458 -3.3934789 -3.6127722 -2.4405255 -1.2389088 -0.13514376 0.6036303 0.48928285 0.36697364 -0.60686576 -2.345089][-4.5352449 -0.679297 -0.73569679 -1.239293 -1.4109071 -1.672601 -1.9755924 -0.762102 0.50110865 1.528214 2.1498306 1.9179838 1.7798245 0.89471364 -0.9817884][-4.1959782 -0.50686443 0.012131691 -0.092364073 -0.015982389 -0.13391232 -0.81557989 0.10406756 1.2079172 2.056668 2.5678351 2.3610833 2.2124903 1.3448994 -0.52865803][-4.6336651 -0.84963775 0.26865935 0.63301444 0.88420248 0.6175344 -0.68863034 -0.34515619 0.38101172 0.91268635 1.2356071 1.1023557 1.0641897 0.40954757 -1.1267424][-5.5838032 -1.7578706 -0.27787638 0.53752184 0.97388649 0.46616316 -1.2575214 -1.4274182 -1.0994569 -0.88986361 -0.79078937 -0.80139875 -0.7280494 -1.0898936 -2.1153479][-6.424921 -2.9380069 -1.5433784 -0.54290235 -0.006809473 -0.744221 -2.6277885 -3.1307909 -3.1025248 -2.9888225 -2.8196881 -2.550205 -2.411418 -2.6319852 -3.2932062][-7.6069212 -4.5921354 -3.4687209 -2.3407669 -1.6128436 -2.3384473 -3.9663386 -4.3702583 -4.4385157 -4.3682585 -4.1135788 -3.6566467 -3.4589791 -3.6475396 -4.1251063][-8.4947824 -5.8915157 -4.8986864 -3.6343431 -2.77559 -3.386518 -4.4561195 -4.5554471 -4.6502309 -4.652688 -4.3649678 -3.9148316 -3.7056632 -3.8616657 -4.2659006][-8.4811859 -6.3268375 -5.5963268 -4.4091887 -3.5724483 -4.0171657 -4.5977497 -4.5194912 -4.6438484 -4.740303 -4.5400295 -4.2391634 -4.0861478 -4.2425084 -4.5985022][-7.5927391 -5.8416734 -5.3787813 -4.5453277 -4.04978 -4.4431362 -4.6938753 -4.518362 -4.5742874 -4.6187391 -4.4652605 -4.3366914 -4.2966094 -4.4509592 -4.7001963][-6.3741179 -5.045887 -4.8796315 -4.476223 -4.2864389 -4.5258913 -4.5166569 -4.3191414 -4.3283205 -4.3446178 -4.2849045 -4.2809391 -4.3294945 -4.4406757 -4.5003376]]...]
INFO - root - 2017-12-15 08:24:53.521122: step 37010, loss = 0.41, batch loss = 0.38 (34.6 examples/sec; 0.231 sec/batch; 18h:58m:27s remains)
INFO - root - 2017-12-15 08:24:55.803952: step 37020, loss = 0.28, batch loss = 0.24 (35.1 examples/sec; 0.228 sec/batch; 18h:43m:46s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 08:24:58.073700: step 37030, loss = 0.22, batch loss = 0.19 (34.3 examples/sec; 0.233 sec/batch; 19h:07m:20s remains)
INFO - root - 2017-12-15 08:25:00.379492: step 37040, loss = 0.20, batch loss = 0.17 (35.2 examples/sec; 0.227 sec/batch; 18h:40m:12s remains)
INFO - root - 2017-12-15 08:25:02.660899: step 37050, loss = 0.22, batch loss = 0.19 (35.4 examples/sec; 0.226 sec/batch; 18h:32m:31s remains)
INFO - root - 2017-12-15 08:25:04.927797: step 37060, loss = 0.34, batch loss = 0.31 (35.1 examples/sec; 0.228 sec/batch; 18h:41m:22s remains)
INFO - root - 2017-12-15 08:25:07.182182: step 37070, loss = 0.26, batch loss = 0.22 (35.9 examples/sec; 0.223 sec/batch; 18h:16m:11s remains)
INFO - root - 2017-12-15 08:25:09.513449: step 37080, loss = 0.20, batch loss = 0.17 (35.4 examples/sec; 0.226 sec/batch; 18h:32m:32s remains)
INFO - root - 2017-12-15 08:25:11.790887: step 37090, loss = 0.33, batch loss = 0.30 (36.1 examples/sec; 0.222 sec/batch; 18h:11m:49s remains)
INFO - root - 2017-12-15 08:25:14.098802: step 37100, loss = 0.34, batch loss = 0.31 (33.0 examples/sec; 0.243 sec/batch; 19h:54m:34s remains)
2017-12-15 08:25:14.399889: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1712685 -7.0809917 -6.1531672 -5.520215 -5.488265 -4.5500956 -3.6800525 -2.8565388 -1.937086 -1.2884964 -1.440304 -2.1181724 -3.2303505 -4.1762233 -4.9938211][-9.1077452 -8.7041616 -7.7973318 -7.2661209 -7.3105531 -6.8374028 -5.9414616 -4.9489927 -3.6173589 -2.422085 -2.3735375 -3.304039 -4.4851294 -5.4438944 -6.202713][-11.330118 -9.5503817 -8.3925285 -7.6800895 -7.6305447 -7.3917747 -6.7879524 -6.15164 -5.2083206 -4.2612987 -4.2967486 -5.2491236 -6.2197084 -6.9285307 -7.4635606][-11.815324 -9.0561485 -7.6110749 -6.6611156 -6.5000048 -6.4567633 -6.1802707 -6.1077547 -5.7746429 -5.4830227 -5.8325453 -6.7646585 -7.5276556 -8.0252781 -8.3186569][-10.93116 -7.7741 -6.1972876 -5.0827103 -4.6371346 -4.5429478 -4.4051495 -4.5841622 -4.771554 -5.2725534 -6.2512217 -7.4916549 -8.2904692 -8.8850012 -9.1575871][-9.7785244 -6.3363376 -4.5925388 -3.3365343 -2.8645439 -2.8960323 -3.1578956 -3.720674 -4.4887848 -5.4995861 -6.6123171 -7.7502313 -8.7007151 -9.4153442 -9.6055565][-6.84904 -3.675703 -1.5879054 -0.26426673 0.082706213 -0.24159789 -1.3437109 -2.7252955 -4.3888626 -5.9688587 -7.1497664 -7.7125645 -8.1053848 -8.6942883 -8.8420572][-5.5525308 -1.8451067 0.74716568 2.5101967 3.1322789 2.7164502 0.88561153 -1.4123913 -3.9218102 -5.7988868 -7.0659175 -7.4111886 -7.5549855 -7.95371 -7.9320893][-5.4370556 -1.7799016 0.89093089 2.8612537 3.846983 3.6315536 1.8060842 -0.49819756 -3.4750664 -5.7215385 -7.0497427 -7.0822597 -6.9681606 -7.2342024 -7.183321][-5.6652756 -2.367631 0.040297985 1.8288603 2.7660289 2.7920237 1.5010867 -0.43305039 -3.2072573 -5.3571944 -6.7522244 -6.8363538 -6.6323404 -6.6028323 -6.2896724][-6.3501592 -3.6377211 -1.874724 -0.56976771 0.31734586 0.44016123 -0.46412945 -1.9926071 -4.1062193 -5.5125494 -6.4126773 -6.3586354 -6.1405835 -6.0806589 -5.689312][-7.5598373 -5.4100914 -4.2581749 -3.4945955 -2.9877367 -2.9680352 -3.6121 -4.7177134 -5.9063683 -6.5051861 -6.70443 -6.5583296 -6.3723702 -6.1382046 -5.6324854][-7.8606262 -6.1837096 -5.5259409 -5.2037449 -4.9700613 -5.0452065 -5.522 -6.2648211 -6.9479675 -7.1545191 -7.098227 -6.8415103 -6.4462066 -5.8646431 -5.1815906][-7.2981005 -6.0599923 -5.7334847 -5.609261 -5.4942732 -5.5025058 -5.793838 -6.3317766 -6.6467409 -6.643652 -6.3776979 -6.0578947 -5.4978905 -4.8112392 -4.2282629][-6.4389381 -5.6106005 -5.6049995 -5.6760969 -5.7260251 -5.7543974 -5.7860284 -5.842844 -5.9022307 -5.9252472 -5.7721262 -5.4683623 -4.8959084 -4.1981864 -3.720871]]...]
INFO - root - 2017-12-15 08:25:16.703094: step 37110, loss = 0.19, batch loss = 0.16 (35.6 examples/sec; 0.225 sec/batch; 18h:25m:47s remains)
INFO - root - 2017-12-15 08:25:18.983751: step 37120, loss = 0.19, batch loss = 0.16 (36.6 examples/sec; 0.219 sec/batch; 17h:57m:18s remains)
INFO - root - 2017-12-15 08:25:21.264847: step 37130, loss = 0.21, batch loss = 0.18 (34.2 examples/sec; 0.234 sec/batch; 19h:10m:16s remains)
INFO - root - 2017-12-15 08:25:23.532540: step 37140, loss = 0.29, batch loss = 0.26 (36.7 examples/sec; 0.218 sec/batch; 17h:54m:27s remains)
INFO - root - 2017-12-15 08:25:25.849281: step 37150, loss = 0.24, batch loss = 0.21 (34.6 examples/sec; 0.231 sec/batch; 18h:59m:20s remains)
INFO - root - 2017-12-15 08:25:28.103611: step 37160, loss = 0.30, batch loss = 0.27 (35.9 examples/sec; 0.223 sec/batch; 18h:17m:05s remains)
INFO - root - 2017-12-15 08:25:30.331437: step 37170, loss = 0.22, batch loss = 0.19 (36.3 examples/sec; 0.220 sec/batch; 18h:04m:03s remains)
INFO - root - 2017-12-15 08:25:32.614984: step 37180, loss = 0.23, batch loss = 0.19 (34.2 examples/sec; 0.234 sec/batch; 19h:10m:00s remains)
INFO - root - 2017-12-15 08:25:34.858126: step 37190, loss = 0.40, batch loss = 0.37 (36.5 examples/sec; 0.219 sec/batch; 17h:57m:57s remains)
INFO - root - 2017-12-15 08:25:37.145854: step 37200, loss = 0.32, batch loss = 0.28 (34.4 examples/sec; 0.233 sec/batch; 19h:04m:59s remains)
2017-12-15 08:25:37.465866: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0231123 -4.3676109 -4.2622342 -4.5523396 -5.2072268 -6.008646 -6.22149 -5.7188067 -4.9959226 -4.4628668 -4.24488 -4.3462768 -4.5681486 -4.7162781 -4.77002][-4.4247532 -3.1733193 -3.0379574 -3.5609951 -4.6616673 -5.9134297 -6.2206745 -5.3961935 -4.3730721 -3.6929376 -3.4218016 -3.5974514 -3.9864511 -4.2620039 -4.4571013][-4.3089423 -1.5390611 -1.3752555 -2.1421428 -3.6835701 -5.2589016 -5.4985652 -4.3348823 -3.1099823 -2.4000461 -2.2367706 -2.6033258 -3.1668625 -3.5396855 -3.8524079][-3.8409953 -0.07799983 0.039438486 -0.97395706 -2.7942715 -4.3993835 -4.48394 -3.1787312 -1.9619973 -1.3396016 -1.3799475 -1.9873375 -2.6563389 -3.064043 -3.4162898][-3.0379889 1.2467542 1.3631551 0.18817163 -1.7131889 -3.1607912 -3.1127021 -1.933453 -1.0117074 -0.69351363 -1.0873314 -1.9151454 -2.5851257 -2.9385338 -3.2728906][-2.3312924 2.2922099 2.4409964 1.209142 -0.56701124 -1.6212804 -1.3931035 -0.40617704 0.14800262 0.059736729 -0.78928912 -1.90733 -2.6242151 -2.9843717 -3.3447976][-1.472508 2.69613 2.8801582 1.748697 0.34198213 -0.23949623 0.07251358 0.8728354 1.134973 0.70545745 -0.54890776 -1.936564 -2.7991195 -3.2340763 -3.5982108][-1.1955078 2.7912762 3.048424 2.0842874 1.0032821 0.72564292 0.92075157 1.5438433 1.6830063 1.1123624 -0.32866347 -1.8777604 -2.9056628 -3.5027251 -3.8836334][-1.4035349 2.5140002 2.918484 2.2633865 1.4362202 1.1669736 1.0217493 1.4770081 1.6050737 1.0489287 -0.36204362 -1.8992321 -3.0903561 -3.8927329 -4.2673841][-2.3251848 1.4551742 2.0548532 1.7509243 1.1010315 0.67193007 0.1489079 0.42941141 0.63505459 0.22917461 -0.93265533 -2.3002462 -3.5243175 -4.4447703 -4.765543][-3.6781285 -0.13318539 0.58499861 0.59331417 0.091167212 -0.504084 -1.2721953 -1.1823679 -0.90754223 -1.1041355 -1.8938618 -2.8914769 -3.9652305 -4.8898048 -5.108674][-4.8513269 -1.6853714 -1.010757 -0.91590011 -1.3734993 -2.077369 -2.891681 -2.9785767 -2.7369928 -2.8063562 -3.3058491 -3.9574168 -4.7001691 -5.3742571 -5.41311][-5.9494576 -3.3253083 -2.8863454 -2.8225451 -3.1486692 -3.7045918 -4.3188863 -4.4923725 -4.3791523 -4.4404516 -4.7467527 -5.0825405 -5.4497957 -5.7579207 -5.5628424][-6.5994997 -4.566422 -4.3393984 -4.31783 -4.4817142 -4.7814856 -5.1039124 -5.2767129 -5.2017074 -5.2123537 -5.3187218 -5.4248962 -5.5242863 -5.5935135 -5.3346162][-6.5288658 -5.0544667 -4.9877071 -4.9920149 -5.0409746 -5.1316352 -5.2233319 -5.3324547 -5.2899218 -5.2447653 -5.2350359 -5.2043476 -5.1818762 -5.1387529 -4.9062395]]...]
INFO - root - 2017-12-15 08:25:39.763214: step 37210, loss = 0.20, batch loss = 0.17 (34.4 examples/sec; 0.233 sec/batch; 19h:05m:15s remains)
INFO - root - 2017-12-15 08:25:42.055512: step 37220, loss = 0.24, batch loss = 0.20 (36.2 examples/sec; 0.221 sec/batch; 18h:08m:39s remains)
INFO - root - 2017-12-15 08:25:44.294127: step 37230, loss = 0.33, batch loss = 0.30 (36.6 examples/sec; 0.219 sec/batch; 17h:56m:55s remains)
INFO - root - 2017-12-15 08:25:46.549041: step 37240, loss = 0.36, batch loss = 0.33 (35.5 examples/sec; 0.225 sec/batch; 18h:27m:29s remains)
INFO - root - 2017-12-15 08:25:48.804231: step 37250, loss = 0.21, batch loss = 0.18 (36.4 examples/sec; 0.219 sec/batch; 18h:00m:02s remains)
INFO - root - 2017-12-15 08:25:51.101267: step 37260, loss = 0.23, batch loss = 0.20 (35.1 examples/sec; 0.228 sec/batch; 18h:41m:24s remains)
INFO - root - 2017-12-15 08:25:53.403218: step 37270, loss = 0.23, batch loss = 0.20 (33.5 examples/sec; 0.239 sec/batch; 19h:35m:24s remains)
INFO - root - 2017-12-15 08:25:55.660481: step 37280, loss = 0.15, batch loss = 0.12 (35.4 examples/sec; 0.226 sec/batch; 18h:32m:57s remains)
INFO - root - 2017-12-15 08:25:57.942991: step 37290, loss = 0.28, batch loss = 0.25 (35.5 examples/sec; 0.225 sec/batch; 18h:28m:40s remains)
INFO - root - 2017-12-15 08:26:00.199294: step 37300, loss = 0.21, batch loss = 0.18 (33.6 examples/sec; 0.238 sec/batch; 19h:30m:15s remains)
2017-12-15 08:26:00.482875: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7366338 -5.1716928 -5.123826 -5.1496239 -5.3329892 -5.6369963 -5.85876 -6.0760107 -6.1958418 -6.1128769 -5.9264517 -5.6680651 -5.3971558 -5.2302895 -5.2062874][-5.4712696 -6.0137653 -5.9535608 -5.9299221 -6.1923289 -6.6922903 -7.0974503 -7.542141 -7.7506647 -7.7497654 -7.7206163 -7.4589949 -7.0386696 -6.6684551 -6.4523764][-7.2498016 -6.9126692 -6.8406687 -6.7390194 -7.0787182 -7.7695589 -8.189702 -8.549675 -8.5151 -8.3479776 -8.4701986 -8.3265343 -7.9235506 -7.5428638 -7.167079][-8.2876806 -7.1172552 -6.8195114 -6.4621506 -6.7713556 -7.6763725 -8.1245852 -8.3384418 -8.1007223 -7.9681559 -8.5233412 -8.8546457 -8.6211014 -8.3061066 -7.7756414][-8.3026075 -6.7404661 -6.2368045 -5.5748634 -5.5348759 -6.2597256 -6.4193649 -6.2244768 -5.59404 -5.3965473 -6.5369577 -7.7569981 -8.12016 -8.1381493 -7.6928997][-7.2582064 -5.529882 -5.1775875 -4.5927591 -4.3721218 -4.7263632 -4.3113236 -3.5048347 -2.3446131 -1.7599223 -3.1139596 -5.1479897 -6.2436628 -6.7181158 -6.6369734][-5.1105909 -3.8818014 -3.6296711 -3.0493259 -2.5730965 -2.4009953 -1.4406586 -0.16762161 1.3869622 2.443311 1.0241807 -1.8687358 -3.9040565 -5.0169306 -5.4122729][-4.0128746 -3.011328 -2.8387036 -2.2562852 -1.5550305 -0.77358878 0.66002584 2.3136623 4.1657591 5.4741535 4.0437212 0.69840765 -1.9997399 -3.6852288 -4.7116261][-3.5428457 -2.976779 -2.8341002 -2.3544316 -1.8326757 -0.85346603 0.74839759 2.5733917 4.4206543 5.5523739 4.3559437 1.5515513 -0.98592925 -2.9456024 -4.3547754][-4.123517 -3.8475423 -3.5796692 -3.0576298 -2.7553625 -1.7325445 0.0067558289 2.0515573 3.813679 4.5389881 3.5242007 1.3324645 -0.97708941 -3.2034156 -4.8760729][-5.2915611 -5.1515307 -4.7868009 -4.22591 -4.0687523 -3.1550384 -1.6488744 0.1718533 1.6336894 2.053304 1.2173715 -0.26224041 -2.2462907 -4.3036618 -5.6622915][-6.212 -6.2011175 -5.9176106 -5.3756876 -5.278614 -4.8014603 -3.9553378 -2.7501826 -1.8445674 -1.7350299 -2.2297902 -2.9796691 -4.2556 -5.6113663 -6.3479548][-7.1558275 -7.3056107 -7.2928734 -6.6873713 -6.2575636 -5.7382078 -5.2374525 -4.6787491 -4.3122139 -4.4030933 -4.841053 -5.2925186 -6.1215954 -6.9488754 -7.2537355][-7.2435055 -7.3587132 -7.5825796 -7.0687819 -6.4550076 -6.0059686 -5.7268429 -5.5066037 -5.3385382 -5.466897 -5.9537029 -6.4135971 -7.0963097 -7.6830864 -7.8019829][-7.1372223 -6.9723415 -7.2062826 -6.7527943 -5.9408464 -5.4425898 -5.2014084 -5.1129704 -5.1505141 -5.50545 -6.270257 -6.8495226 -7.4133749 -7.824646 -7.766387]]...]
INFO - root - 2017-12-15 08:26:02.736913: step 37310, loss = 0.31, batch loss = 0.27 (35.4 examples/sec; 0.226 sec/batch; 18h:33m:06s remains)
INFO - root - 2017-12-15 08:26:05.003928: step 37320, loss = 0.26, batch loss = 0.23 (36.1 examples/sec; 0.222 sec/batch; 18h:10m:49s remains)
INFO - root - 2017-12-15 08:26:07.286203: step 37330, loss = 0.34, batch loss = 0.30 (36.6 examples/sec; 0.219 sec/batch; 17h:55m:57s remains)
INFO - root - 2017-12-15 08:26:09.552803: step 37340, loss = 0.23, batch loss = 0.20 (34.8 examples/sec; 0.230 sec/batch; 18h:50m:12s remains)
INFO - root - 2017-12-15 08:26:11.856860: step 37350, loss = 0.24, batch loss = 0.21 (35.8 examples/sec; 0.223 sec/batch; 18h:18m:10s remains)
INFO - root - 2017-12-15 08:26:14.100024: step 37360, loss = 0.28, batch loss = 0.25 (35.6 examples/sec; 0.225 sec/batch; 18h:26m:16s remains)
INFO - root - 2017-12-15 08:26:16.402794: step 37370, loss = 0.23, batch loss = 0.20 (34.6 examples/sec; 0.231 sec/batch; 18h:58m:21s remains)
INFO - root - 2017-12-15 08:26:18.659164: step 37380, loss = 0.28, batch loss = 0.25 (35.6 examples/sec; 0.225 sec/batch; 18h:25m:04s remains)
INFO - root - 2017-12-15 08:26:20.979503: step 37390, loss = 0.23, batch loss = 0.20 (32.7 examples/sec; 0.245 sec/batch; 20h:03m:21s remains)
INFO - root - 2017-12-15 08:26:23.295165: step 37400, loss = 0.28, batch loss = 0.25 (35.3 examples/sec; 0.227 sec/batch; 18h:36m:04s remains)
2017-12-15 08:26:23.584428: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8567581 -4.7557421 -5.3933554 -6.2500725 -6.8983936 -7.0264869 -6.9062281 -6.6515026 -6.2967582 -6.1744852 -5.901547 -5.3020039 -4.4805737 -3.5397191 -2.8859651][-5.5923538 -5.8114371 -6.2970963 -6.8508472 -7.1815014 -7.004024 -6.6162353 -6.3671861 -6.2700906 -6.5342765 -6.5120535 -6.2181044 -5.7051744 -4.9191012 -4.1455054][-7.0210147 -6.5715623 -6.8018417 -6.8676953 -6.5738173 -5.9131165 -5.1427722 -4.82932 -5.1128197 -5.782917 -5.97086 -5.8890753 -5.6109552 -5.0082674 -4.226027][-7.6623154 -6.6784377 -6.6997733 -6.2555494 -5.2437811 -4.0242577 -2.7578759 -2.4006824 -3.2300308 -4.39466 -4.8771157 -4.8383756 -4.6334205 -4.2273755 -3.6173537][-6.7391667 -5.3561215 -5.1631293 -4.3619909 -2.8408473 -1.1639585 0.53362465 0.75043964 -0.75539935 -2.44441 -3.1585104 -2.9861252 -2.73558 -2.4610732 -2.3031261][-4.851573 -3.1239238 -2.8467364 -2.0077872 -0.24623561 1.8521519 3.91749 3.8902197 1.7321758 -0.32305026 -1.0665809 -0.65861917 -0.35466886 -0.26638997 -0.7429055][-2.3561215 -0.92983532 -0.89918935 -0.33488762 1.4925358 4.047718 6.3968987 6.1181149 3.4219403 1.1447384 0.55944252 1.2844758 1.5828693 1.3661036 0.34612274][-0.76340878 0.45293188 0.16677904 0.54966879 2.4307952 5.244452 7.4794841 6.9195151 4.008831 1.816596 1.4531846 2.3547726 2.5921102 2.0429893 0.55813885][-0.369681 0.48361278 -0.070852518 0.29676676 2.17208 4.8570194 6.5840163 5.8791089 3.2968221 1.4481685 1.1778059 1.9452996 2.2159925 1.6478441 0.10015035][-0.80982757 -0.32600725 -1.0920563 -0.64435112 1.0855379 3.2749772 4.3475142 3.7193494 1.7654567 0.30259871 -0.2011528 0.11194944 0.47832322 0.31124806 -0.89303637][-1.9371233 -1.561654 -2.3487368 -1.9346218 -0.56278753 0.99949026 1.5879965 1.1336699 -0.30103743 -1.6604934 -2.4702544 -2.4742031 -1.9658506 -1.7793367 -2.7435462][-3.3961592 -3.0156925 -3.7289827 -3.4529963 -2.5258918 -1.4968667 -1.0178219 -1.2403467 -2.2902634 -3.4857793 -4.2770815 -4.2634873 -3.7626145 -3.5712724 -4.3826718][-4.6537027 -4.2610464 -4.8849525 -4.7792459 -4.2063761 -3.5348449 -3.0334544 -3.0375533 -3.7316661 -4.5708518 -5.0643349 -4.94518 -4.5636191 -4.5512404 -5.176651][-5.3613539 -4.9213352 -5.4642754 -5.4719009 -5.1346083 -4.7132835 -4.2209277 -4.0401044 -4.3570633 -4.8720407 -5.1258688 -4.9934473 -4.8069744 -4.9847136 -5.4001474][-5.656539 -5.0593491 -5.4443254 -5.4625816 -5.2324982 -5.0093074 -4.6246905 -4.4309216 -4.5793066 -4.8271241 -4.9310451 -4.8259892 -4.775012 -5.111227 -5.4539833]]...]
INFO - root - 2017-12-15 08:26:25.837951: step 37410, loss = 0.27, batch loss = 0.24 (34.8 examples/sec; 0.230 sec/batch; 18h:49m:08s remains)
INFO - root - 2017-12-15 08:26:28.146237: step 37420, loss = 0.35, batch loss = 0.32 (35.8 examples/sec; 0.223 sec/batch; 18h:18m:37s remains)
INFO - root - 2017-12-15 08:26:30.436883: step 37430, loss = 0.23, batch loss = 0.19 (35.3 examples/sec; 0.226 sec/batch; 18h:33m:13s remains)
INFO - root - 2017-12-15 08:26:32.710093: step 37440, loss = 0.21, batch loss = 0.17 (33.9 examples/sec; 0.236 sec/batch; 19h:20m:12s remains)
INFO - root - 2017-12-15 08:26:34.981626: step 37450, loss = 0.18, batch loss = 0.15 (35.5 examples/sec; 0.225 sec/batch; 18h:27m:49s remains)
INFO - root - 2017-12-15 08:26:37.246448: step 37460, loss = 0.19, batch loss = 0.16 (35.9 examples/sec; 0.223 sec/batch; 18h:15m:01s remains)
INFO - root - 2017-12-15 08:26:39.498330: step 37470, loss = 0.19, batch loss = 0.16 (35.0 examples/sec; 0.229 sec/batch; 18h:44m:20s remains)
INFO - root - 2017-12-15 08:26:41.772479: step 37480, loss = 0.18, batch loss = 0.15 (33.5 examples/sec; 0.239 sec/batch; 19h:34m:13s remains)
INFO - root - 2017-12-15 08:26:44.071096: step 37490, loss = 0.22, batch loss = 0.19 (34.3 examples/sec; 0.233 sec/batch; 19h:05m:06s remains)
INFO - root - 2017-12-15 08:26:46.350208: step 37500, loss = 0.29, batch loss = 0.26 (34.0 examples/sec; 0.235 sec/batch; 19h:15m:59s remains)
2017-12-15 08:26:46.678776: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.35525155 0.57567191 0.99733567 0.59343672 -0.33636546 -1.3414483 -1.562963 -1.200258 -1.3343604 -1.7445438 -2.4264369 -3.3224735 -4.080637 -4.2514634 -3.5214744][-0.090224028 0.75198388 0.858418 0.044699907 -1.2751667 -2.5046616 -2.6391311 -2.0450895 -1.7690759 -1.7992594 -2.3291941 -3.0129485 -3.6599605 -4.2024403 -3.6134455][-1.2476482 -0.19042587 -0.27327704 -1.0728134 -2.1589086 -3.2139971 -3.2515554 -2.5870028 -2.1741469 -2.1982243 -2.7546742 -3.3844137 -4.0604544 -4.9337606 -4.5337582][-2.7435684 -1.1546011 -1.2453438 -1.7536407 -2.3734286 -2.9867733 -2.7452705 -2.080198 -1.8889927 -2.3766909 -3.3729312 -4.3191376 -5.1060672 -6.1555653 -5.6861753][-3.71134 -1.8443962 -1.9171258 -2.2580435 -2.4142289 -2.1248677 -1.0926787 -0.28322792 -0.40926194 -1.3341335 -2.76504 -4.1950359 -5.3005037 -6.4422941 -6.0462484][-3.7519696 -2.482018 -2.6044712 -2.653923 -2.3608968 -1.2798905 0.4865973 1.6046956 1.460742 0.29038334 -1.5469465 -3.4840665 -4.9589529 -6.2246184 -5.9601746][-3.3739552 -2.5807936 -2.740706 -2.4162533 -1.5625432 0.16420341 2.4236717 3.708704 3.3127418 1.6357179 -0.87004983 -3.4613667 -5.220438 -6.3120127 -5.8803692][-3.3121262 -2.5503736 -2.7489111 -2.0547192 -0.587378 1.6324463 4.0548506 5.20948 4.3013206 2.0349469 -0.8624835 -3.6879916 -5.3753519 -6.0214128 -5.3372431][-3.7374034 -2.9969063 -3.1863549 -2.2841623 -0.59962785 1.7013137 3.8945975 4.7122188 3.619771 1.4345598 -1.1902592 -3.599262 -4.9593649 -5.1959958 -4.5301824][-4.3001676 -3.5218344 -3.7199326 -2.9588656 -1.5594234 0.15979695 1.7605481 2.4597673 1.7214706 0.2222023 -1.6871002 -3.5413852 -4.528532 -4.4772072 -3.9369533][-4.7361207 -4.0323544 -4.2474866 -3.6646748 -2.8245635 -2.000798 -1.0341735 -0.44048345 -0.88523364 -1.8031751 -2.9817634 -4.10092 -4.6124039 -4.2674923 -3.7803411][-5.0696793 -4.5073943 -4.7399597 -4.2745419 -3.8788939 -3.745909 -3.3643048 -3.0686729 -3.4301817 -3.9233351 -4.5172958 -4.972744 -4.971036 -4.4014368 -3.9632525][-4.9582958 -4.4869947 -4.6853828 -4.3275423 -4.2233453 -4.5903878 -4.5490456 -4.4045148 -4.5847425 -4.5895691 -4.563427 -4.4800119 -4.1779189 -3.6413021 -3.3841488][-4.2252483 -3.9133091 -4.2682343 -4.2352605 -4.5283947 -5.2045841 -5.2436314 -5.049315 -5.0442324 -4.7461314 -4.3201513 -4.0621433 -3.8364062 -3.4475112 -3.36413][-3.2172785 -2.7612638 -3.1357181 -3.3134274 -3.9591055 -4.9924107 -5.2428656 -5.1256657 -5.0478315 -4.63639 -4.2206078 -4.15426 -4.2255 -4.0876389 -4.1980453]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-37500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-37500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 08:26:49.282232: step 37510, loss = 0.30, batch loss = 0.26 (36.3 examples/sec; 0.221 sec/batch; 18h:04m:53s remains)
INFO - root - 2017-12-15 08:26:51.547089: step 37520, loss = 0.21, batch loss = 0.18 (36.0 examples/sec; 0.222 sec/batch; 18h:11m:47s remains)
INFO - root - 2017-12-15 08:26:53.856899: step 37530, loss = 0.21, batch loss = 0.18 (34.0 examples/sec; 0.236 sec/batch; 19h:18m:20s remains)
INFO - root - 2017-12-15 08:26:56.144152: step 37540, loss = 0.27, batch loss = 0.23 (35.4 examples/sec; 0.226 sec/batch; 18h:31m:38s remains)
INFO - root - 2017-12-15 08:26:58.396048: step 37550, loss = 0.17, batch loss = 0.14 (35.2 examples/sec; 0.227 sec/batch; 18h:35m:43s remains)
INFO - root - 2017-12-15 08:27:00.684848: step 37560, loss = 0.32, batch loss = 0.29 (35.5 examples/sec; 0.225 sec/batch; 18h:26m:26s remains)
INFO - root - 2017-12-15 08:27:02.985014: step 37570, loss = 0.51, batch loss = 0.48 (34.1 examples/sec; 0.235 sec/batch; 19h:14m:25s remains)
INFO - root - 2017-12-15 08:27:05.312931: step 37580, loss = 0.25, batch loss = 0.22 (32.7 examples/sec; 0.244 sec/batch; 20h:01m:21s remains)
INFO - root - 2017-12-15 08:27:07.626747: step 37590, loss = 0.22, batch loss = 0.19 (35.6 examples/sec; 0.225 sec/batch; 18h:25m:43s remains)
INFO - root - 2017-12-15 08:27:09.874564: step 37600, loss = 0.28, batch loss = 0.25 (36.1 examples/sec; 0.222 sec/batch; 18h:09m:46s remains)
2017-12-15 08:27:10.141983: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8926927 -3.9428353 -4.2508945 -3.3818979 -2.0156398 -0.73502088 -0.029365063 -0.37262893 -1.5499129 -2.8465948 -4.6597033 -5.737237 -5.763669 -5.0986896 -4.0753784][-1.5252323 -3.0615463 -3.2941518 -2.4848578 -1.4098322 -0.60406244 -0.2092576 -0.79050434 -2.1206868 -3.5916581 -5.4000807 -6.5104685 -6.533186 -5.7645617 -4.4348583][-3.0525622 -3.8942239 -3.9195926 -2.9250584 -1.7853889 -0.92435813 -0.58231926 -1.2738413 -2.521693 -3.6680765 -5.233489 -6.2427163 -6.3719406 -5.6873989 -4.3397493][-4.8652229 -5.1824541 -4.925714 -3.4991891 -2.0334561 -0.81323147 -0.28373671 -0.82428479 -1.8768134 -2.8024304 -4.3569841 -5.547215 -5.9924259 -5.4816761 -4.2390089][-5.7848091 -5.6622114 -5.1520338 -3.3139424 -1.5394959 -0.0080285072 0.53516364 0.026116848 -0.94590819 -1.8159888 -3.5370488 -5.0338287 -5.7539663 -5.3705006 -4.2499638][-6.0337915 -5.7338796 -5.0708408 -3.0540028 -1.2180365 0.32210326 0.75361872 0.27128792 -0.60365212 -1.4210048 -3.2412348 -4.8713269 -5.6713037 -5.4040833 -4.3797326][-6.0044117 -5.6892824 -4.967237 -2.9010086 -1.0533395 0.38610339 0.68346882 0.18222904 -0.7051388 -1.5630713 -3.37391 -5.0446787 -5.7812285 -5.5731449 -4.5421562][-6.2586803 -5.6039953 -4.8213859 -2.69348 -0.80713332 0.44673443 0.49364233 -0.249995 -1.5307416 -2.5887244 -4.1738577 -5.6305304 -6.1553869 -5.8318844 -4.6494961][-6.2401252 -5.2879324 -4.5932665 -2.4853659 -0.53495789 0.562299 0.33850265 -0.81490839 -2.4666448 -3.5187371 -4.8540635 -6.0672216 -6.3732929 -5.9109068 -4.5969977][-5.8951397 -4.9377627 -4.4389949 -2.3472703 -0.2866416 0.80292654 0.63182211 -0.61957908 -2.4235389 -3.5562782 -4.816968 -6.0168653 -6.3364744 -5.8052158 -4.4498987][-5.1203103 -4.0817513 -3.6188571 -1.5738001 0.51854992 1.6346059 1.4527848 0.10470366 -1.7981297 -3.0965168 -4.6155405 -5.9421406 -6.3742323 -5.7431412 -4.3885908][-3.2319584 -2.3056834 -2.0576811 -0.51801431 1.1866226 2.260638 2.1017368 0.70300937 -1.1769547 -2.7098806 -4.5850534 -5.9758291 -6.4689817 -5.7842722 -4.4302616][-2.1368878 -1.406743 -1.4948328 -0.58610988 0.77395797 1.9796789 1.9666464 0.735008 -0.80684221 -2.3571966 -4.4172268 -5.88801 -6.4602938 -5.7662177 -4.4439831][-2.4676573 -1.6601987 -1.6834891 -1.091436 -0.04148531 1.1220679 1.1118848 0.12182689 -1.034799 -2.3761616 -4.367136 -5.7611923 -6.3317118 -5.6126957 -4.3147264][-2.8415639 -1.9884206 -1.9481113 -1.5414104 -0.82517231 -0.040604353 -0.31061113 -1.1400266 -1.7783666 -2.6772907 -4.3160763 -5.5090351 -6.0502949 -5.3676014 -4.0949922]]...]
INFO - root - 2017-12-15 08:27:12.422491: step 37610, loss = 0.25, batch loss = 0.21 (33.5 examples/sec; 0.239 sec/batch; 19h:34m:49s remains)
INFO - root - 2017-12-15 08:27:14.713043: step 37620, loss = 0.40, batch loss = 0.37 (34.3 examples/sec; 0.233 sec/batch; 19h:05m:17s remains)
INFO - root - 2017-12-15 08:27:16.983633: step 37630, loss = 0.27, batch loss = 0.24 (34.9 examples/sec; 0.229 sec/batch; 18h:45m:13s remains)
INFO - root - 2017-12-15 08:27:19.296052: step 37640, loss = 0.18, batch loss = 0.15 (35.4 examples/sec; 0.226 sec/batch; 18h:31m:45s remains)
INFO - root - 2017-12-15 08:27:21.604544: step 37650, loss = 0.38, batch loss = 0.35 (34.6 examples/sec; 0.231 sec/batch; 18h:55m:21s remains)
INFO - root - 2017-12-15 08:27:23.859384: step 37660, loss = 0.21, batch loss = 0.18 (35.7 examples/sec; 0.224 sec/batch; 18h:21m:14s remains)
INFO - root - 2017-12-15 08:27:26.152517: step 37670, loss = 0.24, batch loss = 0.21 (33.9 examples/sec; 0.236 sec/batch; 19h:20m:34s remains)
INFO - root - 2017-12-15 08:27:28.463352: step 37680, loss = 0.18, batch loss = 0.14 (34.4 examples/sec; 0.233 sec/batch; 19h:03m:07s remains)
INFO - root - 2017-12-15 08:27:30.736879: step 37690, loss = 0.32, batch loss = 0.29 (34.0 examples/sec; 0.235 sec/batch; 19h:14m:28s remains)
INFO - root - 2017-12-15 08:27:33.042629: step 37700, loss = 0.24, batch loss = 0.21 (34.9 examples/sec; 0.229 sec/batch; 18h:47m:12s remains)
2017-12-15 08:27:33.364161: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3113589 -4.579812 -4.7911639 -4.9990487 -5.2576838 -6.0668535 -6.7420235 -7.0873308 -7.2321777 -7.0486879 -6.7691803 -6.3850651 -6.4773417 -7.013133 -7.4113207][-5.3851185 -5.9127011 -6.6090508 -7.0537167 -7.0165958 -7.2466688 -7.3780203 -7.4201641 -7.5612526 -7.6825104 -7.6004381 -7.2292805 -7.199501 -7.6995726 -8.0772886][-7.0274906 -6.5428934 -7.509346 -7.9887476 -7.6403437 -7.229394 -6.74207 -6.4700289 -6.7378359 -7.2589769 -7.4807405 -7.2328005 -7.1266041 -7.4918833 -7.5904417][-7.7255535 -6.5390596 -7.5503473 -7.8715496 -7.0396862 -5.8927631 -4.6100059 -3.8740504 -4.4071016 -5.4312987 -6.204567 -6.3899126 -6.3227882 -6.3480511 -5.8614082][-7.5686808 -6.0138988 -6.8340397 -6.7760696 -5.36741 -3.5617771 -1.5803851 -0.46313274 -1.2624493 -2.813405 -4.2746449 -5.1683044 -5.3756375 -4.9994774 -3.7886763][-7.3074684 -5.2260022 -5.6847568 -5.1712341 -3.3090539 -1.0177257 1.5351415 2.952069 1.9918177 -0.043905973 -2.2980676 -4.0021467 -4.4721661 -3.8780293 -2.1767185][-6.5153704 -4.6764717 -4.7651968 -3.9247615 -1.7791708 0.8645668 3.7162955 5.2943382 4.3124838 1.9643757 -0.87859631 -2.9767377 -3.4948769 -2.6873891 -0.81093037][-6.4231052 -4.3988481 -4.2646532 -3.2886689 -1.0350803 1.7431972 4.54492 5.9812269 5.0089788 2.6163366 -0.23516512 -2.2019887 -2.4251497 -1.3037615 0.53009224][-6.5435743 -4.4016132 -4.172924 -3.2501643 -1.1624632 1.4081888 3.7118127 4.7811031 3.8882749 1.6196456 -0.79307604 -2.1400774 -1.8870387 -0.35627723 1.3645644][-6.8646173 -4.6962767 -4.4874811 -3.7974782 -2.1333988 -0.13745475 1.3728862 2.09331 1.4038529 -0.41215861 -2.1498556 -2.8169889 -2.2317617 -0.40341842 1.2247941][-7.2947044 -5.1036963 -4.9054251 -4.4646482 -3.2540958 -1.9949147 -1.3265624 -0.80434835 -1.1529537 -2.3615191 -3.470082 -3.7738302 -3.0615427 -1.1417463 0.45779204][-7.9135323 -5.709094 -5.4323635 -5.0230322 -4.1402812 -3.5168364 -3.5005403 -3.0744305 -3.0426517 -3.6875107 -4.3110261 -4.4307756 -3.6828396 -1.8638607 -0.41980708][-8.5118628 -6.3096085 -5.8685312 -5.322063 -4.6955786 -4.5916734 -4.9960618 -4.6456165 -4.3834848 -4.5249949 -4.7030888 -4.6930304 -4.0475969 -2.5210094 -1.2198581][-8.7665768 -6.5883112 -5.9670458 -5.2580471 -4.8193331 -5.0186825 -5.5429354 -5.3429656 -5.1754026 -5.03601 -4.7916212 -4.6033735 -4.0937319 -2.939714 -1.8099287][-8.4264164 -6.3130665 -5.6394286 -4.9505925 -4.6459622 -4.9419327 -5.3758926 -5.3459711 -5.4228978 -5.2560239 -4.85204 -4.5470304 -4.1859646 -3.431716 -2.6672292]]...]
INFO - root - 2017-12-15 08:27:35.659673: step 37710, loss = 0.22, batch loss = 0.19 (34.9 examples/sec; 0.229 sec/batch; 18h:46m:12s remains)
INFO - root - 2017-12-15 08:27:37.936742: step 37720, loss = 0.26, batch loss = 0.23 (32.2 examples/sec; 0.249 sec/batch; 20h:20m:55s remains)
INFO - root - 2017-12-15 08:27:40.239082: step 37730, loss = 0.23, batch loss = 0.20 (35.5 examples/sec; 0.225 sec/batch; 18h:26m:43s remains)
INFO - root - 2017-12-15 08:27:42.517604: step 37740, loss = 0.32, batch loss = 0.29 (34.0 examples/sec; 0.235 sec/batch; 19h:16m:37s remains)
INFO - root - 2017-12-15 08:27:44.788793: step 37750, loss = 0.25, batch loss = 0.21 (35.8 examples/sec; 0.223 sec/batch; 18h:17m:19s remains)
INFO - root - 2017-12-15 08:27:47.080905: step 37760, loss = 0.25, batch loss = 0.22 (34.5 examples/sec; 0.232 sec/batch; 18h:58m:20s remains)
INFO - root - 2017-12-15 08:27:49.357935: step 37770, loss = 0.21, batch loss = 0.17 (35.5 examples/sec; 0.225 sec/batch; 18h:26m:40s remains)
INFO - root - 2017-12-15 08:27:51.635112: step 37780, loss = 0.20, batch loss = 0.17 (35.3 examples/sec; 0.226 sec/batch; 18h:32m:01s remains)
INFO - root - 2017-12-15 08:27:53.938579: step 37790, loss = 0.24, batch loss = 0.20 (34.7 examples/sec; 0.230 sec/batch; 18h:50m:54s remains)
INFO - root - 2017-12-15 08:27:56.215530: step 37800, loss = 0.21, batch loss = 0.18 (35.0 examples/sec; 0.228 sec/batch; 18h:42m:05s remains)
2017-12-15 08:27:56.527926: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5766888 -3.4780509 -3.7327693 -3.4694109 -2.5874321 -1.5142341 -1.3630391 -1.7504518 -2.052335 -2.4369481 -2.3382919 -2.3907051 -2.3038971 -2.2244494 -2.1891465][-2.6981864 -2.9935408 -3.6923914 -3.8638515 -3.2915251 -2.2090206 -1.8150257 -1.7012417 -1.7634301 -1.9431051 -1.8293087 -2.0413811 -2.2623003 -2.2752314 -2.1631758][-3.0432587 -2.5325923 -3.5004494 -3.9563742 -3.5770833 -2.616034 -2.1096497 -1.749686 -1.5657115 -1.6547157 -1.4337428 -1.758796 -2.1873305 -2.1056683 -1.8256845][-3.4744544 -2.3578572 -3.4592836 -4.1599612 -3.8793979 -2.8339272 -2.1489236 -1.5730474 -1.2622442 -1.2588795 -0.88733888 -1.1667193 -1.6804756 -1.691597 -1.5433865][-4.2032914 -2.6544096 -3.4879246 -4.0199327 -3.4572272 -2.062732 -1.22363 -0.64506006 -0.4290899 -0.57798409 -0.15623379 -0.32352853 -0.96263766 -1.0843258 -1.0461404][-4.715641 -2.6416225 -2.9160039 -3.2251859 -2.4247644 -0.71592188 0.29387569 0.83289456 0.8741672 0.39115667 0.57667184 0.38670063 -0.26733744 -0.47090507 -0.75392604][-4.4766741 -2.7377849 -2.4896014 -2.5104408 -1.5642812 0.44478011 1.7686958 2.4924526 2.363061 1.4122174 1.1380377 0.72310305 -0.033538818 -0.39213479 -0.95250928][-4.3251233 -2.3816552 -1.7121862 -1.6112593 -0.66653943 1.3281779 2.7517877 3.5015507 3.0999551 1.6348302 0.85621142 0.25008607 -0.42070174 -0.70555866 -1.2628359][-3.4730284 -1.4595859 -0.68162084 -0.6704185 -0.21093559 1.1541886 2.2987037 3.0145884 2.5771675 0.94624829 -0.084234953 -0.85378158 -1.405829 -1.4630814 -1.7433023][-2.304476 -0.569342 -0.11414719 -0.36538398 -0.44823337 0.17024803 0.91610694 1.4363029 1.013999 -0.33269596 -1.2269493 -1.8877764 -2.2608128 -2.1231766 -1.9338956][-1.4767849 -0.088121653 -0.098986626 -0.5002166 -0.93182027 -0.98187268 -0.67797661 -0.40087068 -0.74696457 -1.632201 -2.1252704 -2.5111654 -2.6970072 -2.3625855 -1.7089901][-1.3252203 -0.12154794 -0.50840962 -0.89228237 -1.5543993 -2.0090146 -2.0015001 -1.8997737 -2.0299404 -2.2465439 -2.3248811 -2.4609473 -2.6560283 -2.2629106 -1.3936726][-1.5627599 -0.31056011 -0.9062537 -1.3129976 -2.078717 -2.7357779 -2.8729024 -2.9183962 -2.7677991 -2.3282347 -2.0939932 -1.9538462 -1.9079885 -1.5364172 -0.8575691][-2.1148386 -0.68629158 -1.370971 -1.8830012 -2.7130854 -3.3811085 -3.5743504 -3.5476189 -3.0033576 -1.9841317 -1.5512462 -1.2521697 -1.2379408 -1.1709718 -0.932503][-3.0073786 -1.3646071 -2.0036161 -2.6996624 -3.5435481 -4.1069188 -4.3445673 -4.2494583 -3.324317 -1.882023 -1.3591859 -1.0723523 -1.0561789 -1.3280611 -1.59344]]...]
INFO - root - 2017-12-15 08:27:58.821681: step 37810, loss = 0.17, batch loss = 0.14 (34.4 examples/sec; 0.233 sec/batch; 19h:03m:39s remains)
INFO - root - 2017-12-15 08:28:01.101750: step 37820, loss = 0.21, batch loss = 0.18 (35.2 examples/sec; 0.227 sec/batch; 18h:34m:55s remains)
INFO - root - 2017-12-15 08:28:03.411024: step 37830, loss = 0.19, batch loss = 0.16 (35.1 examples/sec; 0.228 sec/batch; 18h:40m:43s remains)
INFO - root - 2017-12-15 08:28:05.684635: step 37840, loss = 0.19, batch loss = 0.15 (36.2 examples/sec; 0.221 sec/batch; 18h:06m:03s remains)
INFO - root - 2017-12-15 08:28:07.997746: step 37850, loss = 0.22, batch loss = 0.18 (33.8 examples/sec; 0.237 sec/batch; 19h:22m:10s remains)
INFO - root - 2017-12-15 08:28:10.281256: step 37860, loss = 0.24, batch loss = 0.21 (35.0 examples/sec; 0.228 sec/batch; 18h:41m:28s remains)
INFO - root - 2017-12-15 08:28:12.569425: step 37870, loss = 0.23, batch loss = 0.19 (36.3 examples/sec; 0.221 sec/batch; 18h:02m:49s remains)
INFO - root - 2017-12-15 08:28:14.871073: step 37880, loss = 0.30, batch loss = 0.27 (34.9 examples/sec; 0.229 sec/batch; 18h:45m:02s remains)
INFO - root - 2017-12-15 08:28:17.137414: step 37890, loss = 0.22, batch loss = 0.19 (35.3 examples/sec; 0.227 sec/batch; 18h:33m:06s remains)
INFO - root - 2017-12-15 08:28:19.419909: step 37900, loss = 0.20, batch loss = 0.17 (33.9 examples/sec; 0.236 sec/batch; 19h:17m:07s remains)
2017-12-15 08:28:19.737831: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1372957 -5.8161664 -5.8080034 -5.02851 -3.5141468 -2.107861 -1.010453 -0.77711582 -1.7283242 -2.606313 -2.6207049 -2.5363863 -2.1220834 -0.83251262 -0.9598974][-4.7469654 -5.8553333 -5.7940702 -4.69707 -2.7887018 -1.1847031 0.056064606 0.3768208 -0.67041969 -1.8167189 -2.2445722 -2.5734057 -2.360435 -1.3531095 -1.5111692][-5.3683567 -6.0697808 -6.0991936 -5.0449524 -3.1461923 -1.4975969 -0.05933857 0.39529276 -0.52484846 -1.6203425 -2.1885085 -2.7760658 -2.6542854 -1.8469919 -2.051573][-6.0442514 -6.5421128 -6.6723461 -5.7463341 -3.9534774 -2.2091157 -0.5647856 0.048258305 -0.75520849 -1.7624854 -2.3639529 -3.0322182 -2.9230347 -2.3409472 -2.5029621][-7.3005266 -7.0167036 -7.1375489 -6.20138 -4.3836126 -2.4763885 -0.65965486 0.0513978 -0.525494 -1.3859942 -2.0690451 -2.9624348 -3.0460081 -2.757405 -2.9299831][-7.528306 -7.131814 -7.0582981 -5.9793968 -4.0274153 -1.9467146 -0.0059602261 0.89042115 0.73864675 -0.0024652481 -0.84408796 -2.1145241 -2.4027631 -2.3765833 -2.7518053][-6.6111274 -6.5947013 -6.1815429 -4.84435 -2.7120366 -0.42194998 1.6782684 2.9597363 3.0809283 2.3230352 1.1623595 -0.67546141 -1.554117 -2.0397208 -2.7624574][-6.3151255 -5.9932775 -5.3974414 -3.9048724 -1.785748 0.59057522 2.8280978 4.5242023 4.9633985 4.4190488 3.1655302 1.0504739 -0.38291264 -1.5876238 -2.9765429][-5.921217 -5.5772448 -4.9997115 -3.6053538 -1.727838 0.44326282 2.4381042 3.8499846 4.4360795 4.4343586 3.4496531 1.4450989 -0.10829163 -1.2617371 -2.955596][-5.8709936 -5.7011042 -5.37339 -4.331255 -2.8627224 -0.9215982 0.80444884 1.8001842 2.507154 3.042748 2.2207561 0.42132306 -0.71521413 -1.3558688 -2.9430056][-6.1209593 -6.1788054 -6.1472664 -5.4550285 -4.324441 -2.4608858 -1.0013229 -0.44225645 0.19434762 1.0326962 0.41967893 -1.051218 -1.5985351 -1.6425173 -2.9981034][-6.2346144 -6.397531 -6.5617561 -6.1925468 -5.3960352 -3.7970128 -2.5043144 -2.2116156 -1.7901181 -1.1247528 -1.6866543 -2.8654082 -3.1366858 -2.7868848 -3.7344713][-6.1458778 -6.2759485 -6.5676737 -6.4996958 -6.0797429 -4.9894018 -3.9232373 -3.8432651 -3.6912713 -3.2008276 -3.6856279 -4.70345 -5.1236076 -4.6741724 -4.7865562][-6.0078716 -6.0643659 -6.4033489 -6.3683414 -6.132268 -5.5107241 -4.662118 -4.6833816 -4.7496033 -4.5397191 -4.8752594 -5.7218695 -6.1085043 -5.6454511 -5.2425833][-5.7946072 -5.8181534 -6.2223973 -6.1066623 -5.8247285 -5.4138813 -4.6566882 -4.6451759 -4.7088685 -4.7256 -5.1037312 -5.9999337 -6.5036974 -6.3209291 -5.9104195]]...]
INFO - root - 2017-12-15 08:28:21.993023: step 37910, loss = 0.23, batch loss = 0.19 (35.7 examples/sec; 0.224 sec/batch; 18h:18m:42s remains)
INFO - root - 2017-12-15 08:28:24.287438: step 37920, loss = 0.22, batch loss = 0.19 (36.1 examples/sec; 0.221 sec/batch; 18h:07m:21s remains)
INFO - root - 2017-12-15 08:28:26.560868: step 37930, loss = 0.31, batch loss = 0.27 (34.9 examples/sec; 0.229 sec/batch; 18h:46m:17s remains)
INFO - root - 2017-12-15 08:28:28.836524: step 37940, loss = 0.21, batch loss = 0.17 (35.3 examples/sec; 0.227 sec/batch; 18h:33m:43s remains)
INFO - root - 2017-12-15 08:28:31.087853: step 37950, loss = 0.47, batch loss = 0.44 (35.8 examples/sec; 0.224 sec/batch; 18h:18m:27s remains)
INFO - root - 2017-12-15 08:28:33.356107: step 37960, loss = 0.34, batch loss = 0.31 (35.2 examples/sec; 0.227 sec/batch; 18h:35m:23s remains)
INFO - root - 2017-12-15 08:28:35.635036: step 37970, loss = 0.27, batch loss = 0.24 (34.1 examples/sec; 0.235 sec/batch; 19h:12m:59s remains)
INFO - root - 2017-12-15 08:28:37.894942: step 37980, loss = 0.22, batch loss = 0.19 (35.8 examples/sec; 0.224 sec/batch; 18h:18m:21s remains)
INFO - root - 2017-12-15 08:28:40.246632: step 37990, loss = 0.19, batch loss = 0.16 (33.6 examples/sec; 0.238 sec/batch; 19h:27m:22s remains)
INFO - root - 2017-12-15 08:28:42.530966: step 38000, loss = 0.45, batch loss = 0.42 (35.6 examples/sec; 0.225 sec/batch; 18h:23m:15s remains)
2017-12-15 08:28:42.813621: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3528566 -7.4435768 -7.4995127 -7.3918848 -7.0036221 -6.4828267 -6.087163 -6.0048275 -6.17148 -6.3674736 -6.5832686 -6.8311787 -6.7700067 -6.6185155 -6.5169153][-6.9548678 -8.133172 -8.2363987 -8.196413 -7.9139218 -7.2635851 -6.5571284 -6.155735 -6.2901878 -6.5978022 -6.9169707 -7.253334 -7.2282004 -7.0683961 -6.9595356][-7.3504434 -7.9071574 -8.0807085 -8.193306 -8.0953407 -7.2809567 -6.1920271 -5.420877 -5.5331125 -6.0648775 -6.6653242 -7.3950753 -7.6788397 -7.7190471 -7.6312914][-5.8792858 -6.06322 -6.6121669 -7.1374893 -7.2864041 -6.2647214 -4.8317032 -3.7907915 -3.9796388 -4.7607069 -5.542161 -6.5135965 -7.0238409 -7.2840667 -7.2219162][-3.7953234 -3.5990934 -4.5645032 -5.3374453 -5.4370289 -4.0126157 -2.2430015 -0.96916628 -1.27057 -2.1453474 -3.0367148 -4.14526 -4.9252996 -5.3524084 -5.182579][-1.9911835 -1.6792883 -2.9837041 -3.8234692 -3.6421227 -1.7342746 0.2847836 1.6532867 1.342685 0.46803069 -0.60694826 -1.9181001 -2.8792274 -3.2668874 -2.8995204][-0.45706153 -0.091257572 -1.5088805 -2.287863 -1.928936 0.30802464 2.3180084 3.5566263 3.2321615 2.430563 1.4339013 0.28093767 -0.502488 -0.55610442 0.078166962][-0.68468332 0.11416435 -1.2983232 -1.983911 -1.4728596 0.867507 2.7581196 3.7307196 3.3999038 2.9325104 2.3787346 1.664541 1.239856 1.5706751 2.38835][-1.8801949 -0.9030813 -2.2074203 -2.7535474 -2.3058763 -0.3347578 1.1516342 1.7825241 1.6439216 1.7383764 1.7233608 1.3949142 1.2037673 1.8316388 2.7710238][-3.0675049 -2.1998456 -3.3196735 -3.8430817 -3.7312713 -2.3628612 -1.3175086 -0.90048969 -0.76727486 -0.17039537 0.25862098 0.11969519 0.024017334 0.81687284 1.8334446][-4.2983255 -3.7843981 -4.6691766 -5.1465745 -5.3255267 -4.5842357 -3.8992264 -3.5711825 -3.207803 -2.2757537 -1.6186047 -1.6564605 -1.6279744 -0.79617751 0.12959886][-5.0401297 -4.8373623 -5.4604936 -5.7670121 -6.0730996 -5.91239 -5.5697021 -5.3368483 -4.9798956 -4.1899161 -3.7118216 -3.76405 -3.6881502 -3.0614591 -2.4654562][-5.4359913 -5.4302726 -5.7605004 -5.93153 -6.2875605 -6.3940306 -6.233357 -6.0128593 -5.7783723 -5.395133 -5.3013973 -5.4737315 -5.5365505 -5.3483272 -5.187727][-6.3504314 -6.3605585 -6.4694924 -6.54422 -6.747117 -6.8352604 -6.7356358 -6.5665722 -6.5322385 -6.5521135 -6.7758703 -7.0295458 -7.1206393 -7.1888385 -7.2298355][-6.9963312 -6.7361445 -6.7533221 -6.7560697 -6.8120236 -6.8431072 -6.784379 -6.7020526 -6.7976456 -7.0026455 -7.2826366 -7.4769659 -7.5667281 -7.7661252 -7.9293194]]...]
INFO - root - 2017-12-15 08:28:45.103354: step 38010, loss = 0.25, batch loss = 0.22 (33.9 examples/sec; 0.236 sec/batch; 19h:18m:27s remains)
INFO - root - 2017-12-15 08:28:47.364528: step 38020, loss = 0.17, batch loss = 0.13 (36.3 examples/sec; 0.220 sec/batch; 18h:02m:11s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 08:28:49.646806: step 38030, loss = 0.24, batch loss = 0.21 (36.2 examples/sec; 0.221 sec/batch; 18h:03m:58s remains)
INFO - root - 2017-12-15 08:28:51.932925: step 38040, loss = 0.27, batch loss = 0.24 (35.7 examples/sec; 0.224 sec/batch; 18h:20m:55s remains)
INFO - root - 2017-12-15 08:28:54.229688: step 38050, loss = 0.23, batch loss = 0.20 (35.7 examples/sec; 0.224 sec/batch; 18h:20m:53s remains)
INFO - root - 2017-12-15 08:28:56.523354: step 38060, loss = 0.24, batch loss = 0.20 (34.6 examples/sec; 0.231 sec/batch; 18h:53m:14s remains)
INFO - root - 2017-12-15 08:28:58.787328: step 38070, loss = 0.24, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 18h:20m:12s remains)
INFO - root - 2017-12-15 08:29:01.036788: step 38080, loss = 0.18, batch loss = 0.15 (35.5 examples/sec; 0.225 sec/batch; 18h:25m:47s remains)
INFO - root - 2017-12-15 08:29:03.301377: step 38090, loss = 0.35, batch loss = 0.31 (35.6 examples/sec; 0.225 sec/batch; 18h:22m:37s remains)
INFO - root - 2017-12-15 08:29:05.606110: step 38100, loss = 0.20, batch loss = 0.17 (35.7 examples/sec; 0.224 sec/batch; 18h:18m:07s remains)
2017-12-15 08:29:05.896448: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4829278 -4.2602143 -4.1992307 -4.9167237 -6.1151772 -7.1929388 -7.113368 -6.334918 -6.1073103 -5.0228696 -4.4464588 -3.968967 -4.3635254 -5.5956044 -6.8891439][-4.5410957 -4.27835 -4.328517 -5.0857458 -6.1533337 -6.7040911 -5.7731085 -4.710001 -4.3137465 -3.2211788 -2.7177956 -2.3935206 -3.3978939 -5.3093452 -6.9133139][-5.957263 -4.4978986 -4.7250757 -5.3789864 -6.1262064 -6.1319261 -4.3984566 -3.0339079 -2.670963 -2.0198426 -1.8290842 -1.5635455 -2.7437739 -4.9186497 -6.6111374][-7.0551014 -4.9230576 -5.3525658 -5.5909796 -5.6346049 -5.0175052 -2.7131379 -1.1936684 -1.1706917 -1.2002753 -1.7064196 -1.8032037 -2.7931981 -4.7981148 -6.3051844][-7.778615 -5.2691355 -5.7459331 -5.3268876 -4.53306 -3.2152119 -0.47104442 1.2954824 0.73776364 -0.1987133 -1.7410085 -2.6231487 -3.4992285 -5.1659088 -6.3505487][-7.7040453 -5.6121817 -5.8417873 -4.5911994 -2.9325087 -1.0277023 2.041012 4.0198069 2.7685106 0.76518655 -2.0087082 -3.891799 -4.923388 -6.37928 -7.2527695][-7.1584959 -5.6894493 -5.5914955 -3.7578769 -1.4113255 0.99613523 4.3591852 6.3797045 4.4460278 1.5876129 -2.0378504 -4.6982107 -5.9333687 -7.4511528 -8.26982][-7.3062105 -5.8173494 -5.6100292 -3.5975742 -0.89367771 1.8816311 5.2446728 7.2276936 5.0466251 1.9411995 -1.9819986 -4.88021 -6.2506108 -7.9285278 -8.8233][-7.6080542 -6.34956 -6.2929497 -4.4962425 -1.6995525 1.103224 3.8568704 5.5255308 3.6191604 0.92179108 -2.7148361 -5.328371 -6.5616856 -8.2873764 -9.2143154][-7.8899927 -6.881588 -7.1608219 -6.0157385 -3.4994717 -1.0421804 0.72574282 1.8860128 0.40823483 -1.7620838 -4.5957756 -6.377243 -7.0950336 -8.5711622 -9.276186][-7.7537251 -6.8798866 -7.5138168 -7.0566006 -5.1322069 -3.4281616 -2.6796503 -1.9533426 -2.9122953 -4.5971251 -6.556324 -7.333909 -7.4543352 -8.4995861 -8.7758923][-7.2353764 -6.3349743 -7.0589056 -7.0609035 -5.8430147 -4.9516897 -4.9468131 -4.5793777 -5.1552906 -6.4585924 -7.5521383 -7.500452 -7.2479849 -7.9293041 -7.8252134][-6.4640017 -5.53776 -6.0495481 -6.2132959 -5.6266928 -5.3166833 -5.550518 -5.3824883 -5.8258715 -6.819253 -7.1840219 -6.8276711 -6.5982666 -6.9745235 -6.660635][-5.6096697 -4.7008705 -4.988204 -5.1569381 -4.9680462 -4.9282117 -5.193759 -5.1980548 -5.4912848 -5.9662385 -5.9408855 -5.6882458 -5.5841618 -5.6983109 -5.4072065][-4.9964819 -4.1585646 -4.297708 -4.4100041 -4.3796558 -4.3605852 -4.53353 -4.6086369 -4.7818041 -4.9226704 -4.8051486 -4.6967888 -4.6554236 -4.6141882 -4.4506454]]...]
INFO - root - 2017-12-15 08:29:08.181482: step 38110, loss = 0.23, batch loss = 0.20 (34.5 examples/sec; 0.232 sec/batch; 18h:56m:56s remains)
INFO - root - 2017-12-15 08:29:10.463607: step 38120, loss = 0.15, batch loss = 0.12 (33.4 examples/sec; 0.239 sec/batch; 19h:34m:56s remains)
INFO - root - 2017-12-15 08:29:12.748550: step 38130, loss = 0.22, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 18h:13m:40s remains)
INFO - root - 2017-12-15 08:29:15.022760: step 38140, loss = 0.21, batch loss = 0.18 (34.6 examples/sec; 0.231 sec/batch; 18h:52m:55s remains)
INFO - root - 2017-12-15 08:29:17.296837: step 38150, loss = 0.29, batch loss = 0.26 (35.0 examples/sec; 0.229 sec/batch; 18h:41m:17s remains)
INFO - root - 2017-12-15 08:29:19.553600: step 38160, loss = 0.30, batch loss = 0.27 (36.0 examples/sec; 0.222 sec/batch; 18h:09m:58s remains)
INFO - root - 2017-12-15 08:29:21.846537: step 38170, loss = 0.29, batch loss = 0.26 (34.8 examples/sec; 0.230 sec/batch; 18h:48m:12s remains)
INFO - root - 2017-12-15 08:29:24.129444: step 38180, loss = 0.17, batch loss = 0.14 (33.5 examples/sec; 0.239 sec/batch; 19h:30m:35s remains)
INFO - root - 2017-12-15 08:29:26.384168: step 38190, loss = 0.28, batch loss = 0.25 (35.4 examples/sec; 0.226 sec/batch; 18h:27m:48s remains)
INFO - root - 2017-12-15 08:29:28.630593: step 38200, loss = 0.23, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 18h:08m:34s remains)
2017-12-15 08:29:28.938130: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.905447 -5.5849056 -5.988327 -5.9294872 -5.5670128 -5.2014751 -5.13679 -5.2757998 -5.4174533 -5.3740053 -5.3596125 -5.339654 -5.6735096 -5.9598989 -5.9655905][-4.7018919 -6.7697353 -7.238163 -6.9454308 -6.1846738 -5.4861293 -5.3176956 -5.6112642 -6.1668477 -6.4595594 -6.5047903 -6.2801723 -6.504734 -6.8724136 -7.0090103][-6.4305763 -7.5361452 -7.9411993 -7.3873968 -6.2868013 -5.2125263 -4.8023896 -5.1834249 -6.2713547 -7.1877337 -7.4527435 -7.0203085 -7.0981178 -7.5203018 -7.6917315][-7.5337696 -7.70422 -7.8788147 -7.0358114 -5.6890254 -4.1941671 -3.411272 -3.7437067 -5.35747 -7.0171185 -7.7314816 -7.4012346 -7.6259537 -8.1736383 -8.3357067][-7.6235285 -7.1129951 -6.8487206 -5.5346761 -3.8910789 -2.0497267 -0.7182591 -0.68148208 -2.7354674 -5.0692482 -6.2298093 -6.1470575 -6.6649485 -7.3177633 -7.6407051][-7.5868015 -6.3770866 -5.7522717 -3.9603539 -1.9844306 0.19759035 2.3469613 3.1536291 0.78909039 -2.1494322 -3.8089843 -4.2010593 -5.0624003 -5.8334875 -6.3395948][-6.8037224 -5.5804539 -4.7995467 -2.7592986 -0.55877876 1.7916267 4.6839294 6.3811579 3.834306 0.32127023 -1.8192044 -2.7958958 -4.0469918 -4.99217 -5.5631151][-6.510148 -5.1832089 -4.3628235 -2.1815724 0.1962347 2.5863564 5.9335785 8.46697 6.1199942 2.4443252 0.0919373 -1.4855247 -3.2827325 -4.6139197 -5.4052134][-6.9611273 -5.8370452 -5.1427641 -2.972862 -0.64294076 1.458596 4.7694387 7.7456207 5.9837027 2.693083 0.59647727 -1.1570114 -3.213726 -4.7657642 -5.770503][-7.6142054 -6.8538733 -6.4565907 -4.5901809 -2.5626726 -0.87361777 1.90674 4.6291227 3.4334981 0.7664814 -1.0503969 -2.7416008 -4.7703009 -6.2921362 -7.1960382][-8.1212559 -7.6220026 -7.5185671 -6.1826277 -4.695612 -3.5362194 -1.5153358 0.57235527 -0.12892628 -2.0558496 -3.49722 -4.8662853 -6.6384974 -7.9418411 -8.6276226][-8.596508 -8.2476921 -8.3373785 -7.5568571 -6.6660833 -5.9702678 -4.7213387 -3.3946576 -3.7619119 -4.9207745 -5.8879385 -6.841917 -8.14188 -9.0753155 -9.5064735][-8.3433723 -8.00671 -8.1710224 -7.8096476 -7.3772697 -7.0461788 -6.4251614 -5.7999172 -6.0624285 -6.7671795 -7.4474106 -8.0880909 -8.8837757 -9.4273787 -9.5747395][-7.2474222 -6.813343 -6.9812584 -6.9042234 -6.807663 -6.7580957 -6.57932 -6.3946533 -6.5448227 -6.8806839 -7.2635508 -7.582016 -7.9343758 -8.112484 -8.0416784][-6.0165234 -5.4282708 -5.5224075 -5.5556426 -5.6127491 -5.7039948 -5.7644444 -5.7855263 -5.8670259 -5.9977508 -6.1314077 -6.2001019 -6.2264566 -6.2160549 -6.12236]]...]
INFO - root - 2017-12-15 08:29:31.212880: step 38210, loss = 0.30, batch loss = 0.26 (35.1 examples/sec; 0.228 sec/batch; 18h:38m:31s remains)
INFO - root - 2017-12-15 08:29:33.489488: step 38220, loss = 0.20, batch loss = 0.17 (35.8 examples/sec; 0.224 sec/batch; 18h:16m:20s remains)
INFO - root - 2017-12-15 08:29:35.736974: step 38230, loss = 0.20, batch loss = 0.17 (36.2 examples/sec; 0.221 sec/batch; 18h:02m:28s remains)
INFO - root - 2017-12-15 08:29:38.019166: step 38240, loss = 0.22, batch loss = 0.19 (35.0 examples/sec; 0.229 sec/batch; 18h:42m:26s remains)
INFO - root - 2017-12-15 08:29:40.300572: step 38250, loss = 0.25, batch loss = 0.22 (36.4 examples/sec; 0.220 sec/batch; 17h:57m:57s remains)
INFO - root - 2017-12-15 08:29:42.563645: step 38260, loss = 0.18, batch loss = 0.15 (36.2 examples/sec; 0.221 sec/batch; 18h:04m:55s remains)
INFO - root - 2017-12-15 08:29:44.860467: step 38270, loss = 0.26, batch loss = 0.23 (34.6 examples/sec; 0.231 sec/batch; 18h:53m:19s remains)
INFO - root - 2017-12-15 08:29:47.135112: step 38280, loss = 0.21, batch loss = 0.18 (35.3 examples/sec; 0.226 sec/batch; 18h:30m:36s remains)
INFO - root - 2017-12-15 08:29:49.387968: step 38290, loss = 0.22, batch loss = 0.18 (35.3 examples/sec; 0.226 sec/batch; 18h:30m:11s remains)
INFO - root - 2017-12-15 08:29:51.673883: step 38300, loss = 0.19, batch loss = 0.16 (34.2 examples/sec; 0.234 sec/batch; 19h:05m:19s remains)
2017-12-15 08:29:51.985056: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0228486 -5.001132 -5.0182552 -4.5983877 -4.0812559 -3.6896524 -3.3419404 -3.0263324 -2.5339696 -1.7354238 -1.1365635 -0.72564888 -0.55969167 -0.75535309 -1.7228959][-5.2395315 -6.1470838 -6.0992842 -5.6611457 -5.1689272 -4.7307019 -4.29672 -3.9274347 -3.5304465 -2.617137 -1.7317617 -1.2669894 -1.183535 -1.3312864 -2.1064067][-5.9670291 -6.4612522 -6.5339389 -6.0550675 -5.5563545 -5.1066942 -4.6285477 -4.3045053 -4.2555642 -3.5097232 -2.594193 -2.234261 -2.2784138 -2.32504 -2.718245][-6.0328951 -6.1011791 -6.184823 -5.5660229 -4.8882828 -4.35419 -3.7917948 -3.5456867 -4.0452061 -3.7953391 -3.2287092 -3.0975075 -3.2272363 -3.1640954 -3.2240782][-5.6178775 -4.9797707 -4.8899393 -4.1167459 -3.1671271 -2.53654 -1.8423845 -1.5457771 -2.4741807 -2.8472404 -2.7583723 -3.0910847 -3.4722071 -3.6196566 -3.5146964][-4.6524439 -3.3222589 -2.9348285 -2.0909581 -1.0844104 -0.55705976 0.2151885 0.68338776 -0.4691999 -1.3853949 -1.8647412 -2.8058667 -3.4659526 -3.9172823 -3.7442205][-3.0838594 -1.5378335 -0.95283568 -0.16538119 0.82142663 1.2958448 2.0781107 2.5690427 1.191375 -0.20406938 -1.0403923 -2.3254969 -3.1349037 -3.8718467 -3.8374176][-1.8125441 -0.15753317 0.39871335 1.0090764 1.8952708 2.3117871 3.0063343 3.4042969 1.9661126 0.38201022 -0.42233098 -1.6604985 -2.5483427 -3.5719528 -3.7277312][-1.3745193 0.1099515 0.33280206 0.66790605 1.3202779 1.5778763 2.0963559 2.4351563 1.2926807 0.041683435 -0.32765269 -1.1878154 -2.0416937 -3.1128683 -3.4721346][-2.1128392 -0.76848304 -0.85835576 -0.71063948 -0.3223325 -0.29754734 -0.061588049 0.21930599 -0.43595719 -1.0803244 -0.84917021 -1.0809653 -1.6312413 -2.4809513 -3.0974879][-3.3777673 -2.1263571 -2.3407321 -2.2990139 -2.0988531 -2.1859779 -2.1273649 -1.9162076 -2.1260438 -2.2357934 -1.5804689 -1.300669 -1.5723946 -2.1652837 -2.990804][-4.9029846 -3.6975265 -3.8341513 -3.781384 -3.6421809 -3.6605625 -3.5458689 -3.3051465 -3.2148628 -2.9454482 -2.1855326 -1.6959043 -1.742798 -2.0896447 -3.0243568][-6.0120325 -4.7808113 -4.7654514 -4.6484203 -4.4677105 -4.2839489 -4.0827842 -3.8728342 -3.6766884 -3.2050862 -2.5226622 -2.075366 -1.9906271 -2.1835427 -3.185102][-6.4736047 -5.3016467 -5.2059054 -5.0608368 -4.852407 -4.4893708 -4.1011229 -3.9432518 -3.756845 -3.2571082 -2.7828894 -2.3988135 -2.1698604 -2.222183 -2.9825485][-6.5174875 -5.4434233 -5.3172503 -5.1695189 -4.963397 -4.5170813 -4.002491 -3.8875687 -3.741962 -3.2500775 -2.8829913 -2.4074235 -1.9538481 -1.8080864 -2.1405196]]...]
INFO - root - 2017-12-15 08:29:54.255328: step 38310, loss = 0.33, batch loss = 0.29 (34.8 examples/sec; 0.230 sec/batch; 18h:45m:51s remains)
INFO - root - 2017-12-15 08:29:56.555508: step 38320, loss = 0.22, batch loss = 0.19 (35.2 examples/sec; 0.227 sec/batch; 18h:33m:56s remains)
INFO - root - 2017-12-15 08:29:58.815349: step 38330, loss = 0.21, batch loss = 0.18 (35.4 examples/sec; 0.226 sec/batch; 18h:27m:43s remains)
INFO - root - 2017-12-15 08:30:01.070485: step 38340, loss = 0.26, batch loss = 0.23 (35.3 examples/sec; 0.226 sec/batch; 18h:29m:54s remains)
INFO - root - 2017-12-15 08:30:03.376438: step 38350, loss = 0.19, batch loss = 0.16 (34.4 examples/sec; 0.233 sec/batch; 19h:00m:27s remains)
INFO - root - 2017-12-15 08:30:05.662658: step 38360, loss = 0.19, batch loss = 0.16 (35.8 examples/sec; 0.223 sec/batch; 18h:14m:19s remains)
INFO - root - 2017-12-15 08:30:07.934967: step 38370, loss = 0.32, batch loss = 0.29 (34.6 examples/sec; 0.231 sec/batch; 18h:54m:39s remains)
INFO - root - 2017-12-15 08:30:10.289277: step 38380, loss = 0.40, batch loss = 0.36 (35.4 examples/sec; 0.226 sec/batch; 18h:26m:39s remains)
INFO - root - 2017-12-15 08:30:12.556669: step 38390, loss = 0.20, batch loss = 0.17 (36.3 examples/sec; 0.220 sec/batch; 18h:00m:49s remains)
INFO - root - 2017-12-15 08:30:14.837377: step 38400, loss = 0.24, batch loss = 0.20 (33.2 examples/sec; 0.241 sec/batch; 19h:39m:42s remains)
2017-12-15 08:30:15.131456: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.81872773 -2.8255391 -3.9004889 -5.2488861 -6.2889671 -5.9365759 -5.1186228 -4.203074 -3.5279117 -2.8350847 -3.2152226 -4.3228483 -5.0551167 -5.7335529 -6.0089359][-2.8129387 -3.7742591 -4.5370131 -5.3726268 -5.8831511 -5.4026747 -4.9168196 -4.45055 -4.0661554 -3.6097271 -3.9338522 -4.7265587 -5.4598475 -6.1879797 -6.500999][-4.5736942 -4.6621323 -5.1046686 -5.1813869 -4.9754877 -4.1800766 -3.8528261 -3.7870731 -3.790648 -3.7658973 -4.3265219 -5.0329437 -5.8991222 -6.5853553 -6.6913505][-5.4960661 -5.3221278 -5.3866978 -4.6131496 -3.6189435 -2.45992 -2.1166837 -2.4269729 -2.9257607 -3.5129437 -4.545186 -5.3762269 -6.4468384 -7.1015625 -7.1012206][-5.3428488 -5.0336037 -4.8709183 -3.5289788 -2.0259004 -0.49018717 0.21447539 -0.13601828 -1.1632471 -2.4278195 -4.1338077 -5.3205767 -6.6095271 -7.2932286 -7.3201542][-4.8816347 -4.4006376 -3.9275661 -2.3494873 -0.71517408 1.3033373 2.7374144 2.6405683 1.1697166 -0.79774535 -3.3535204 -5.0426931 -6.5296946 -7.23292 -7.241869][-4.6176853 -4.1152434 -3.3274 -1.6392963 0.20007443 2.7804203 5.0223823 5.3099527 3.5267224 0.89564466 -2.4485154 -4.5865211 -6.269145 -7.10221 -7.1308322][-4.7036362 -4.140718 -3.4710257 -1.9073938 0.030142784 3.0034537 5.9686522 6.7986522 5.1074653 2.2650638 -1.5234369 -4.0324454 -5.8998814 -6.9783192 -7.0388074][-5.034306 -4.6486144 -4.4336052 -3.1049914 -0.98299336 2.1817532 5.3850384 6.5724082 5.2282577 2.6286058 -1.1389282 -3.7580404 -5.5966969 -6.8216481 -6.918211][-5.7270451 -5.4758282 -5.7753429 -4.8274012 -2.6172347 0.40126944 3.2934465 4.6270766 3.8657084 1.7426727 -1.583236 -4.01245 -5.5324121 -6.6811748 -6.7605734][-6.0213509 -5.8266988 -6.598629 -6.2800417 -4.4227734 -1.815196 0.55844855 1.8879018 1.5806131 -0.1287024 -2.8024187 -4.891777 -5.983717 -6.8060994 -6.7823043][-6.0569992 -6.026906 -6.8706708 -6.8933492 -5.6092324 -3.6404691 -1.8268297 -0.81476796 -0.98593891 -2.2873976 -4.3646584 -6.2032795 -6.9668927 -7.4882274 -7.2010255][-5.9267941 -6.076004 -6.8010378 -6.9055877 -6.14231 -4.8301196 -3.4907305 -2.8420634 -3.1813364 -4.3997669 -6.0984774 -7.7443018 -8.2503033 -8.4533405 -7.9056773][-5.873405 -6.0368166 -6.6453876 -6.8228025 -6.47498 -5.5906944 -4.5982828 -4.3089771 -4.8616405 -5.9953089 -7.3638377 -8.80152 -9.1454468 -9.0092144 -8.2368793][-6.29238 -6.2219324 -6.5930104 -6.7292304 -6.6758966 -6.2025614 -5.5329905 -5.4114027 -5.8860145 -6.8465681 -8.0605717 -9.3574047 -9.5647688 -9.0830555 -8.1386185]]...]
INFO - root - 2017-12-15 08:30:17.425034: step 38410, loss = 0.27, batch loss = 0.24 (36.5 examples/sec; 0.219 sec/batch; 17h:53m:51s remains)
INFO - root - 2017-12-15 08:30:19.695200: step 38420, loss = 0.35, batch loss = 0.32 (35.3 examples/sec; 0.227 sec/batch; 18h:31m:42s remains)
INFO - root - 2017-12-15 08:30:22.011106: step 38430, loss = 0.17, batch loss = 0.14 (35.8 examples/sec; 0.223 sec/batch; 18h:14m:31s remains)
INFO - root - 2017-12-15 08:30:24.310055: step 38440, loss = 0.32, batch loss = 0.29 (34.5 examples/sec; 0.232 sec/batch; 18h:54m:59s remains)
INFO - root - 2017-12-15 08:30:26.606397: step 38450, loss = 0.32, batch loss = 0.28 (34.4 examples/sec; 0.233 sec/batch; 18h:59m:58s remains)
INFO - root - 2017-12-15 08:30:28.909906: step 38460, loss = 0.35, batch loss = 0.31 (35.4 examples/sec; 0.226 sec/batch; 18h:26m:43s remains)
INFO - root - 2017-12-15 08:30:31.196103: step 38470, loss = 0.24, batch loss = 0.21 (34.7 examples/sec; 0.231 sec/batch; 18h:50m:00s remains)
INFO - root - 2017-12-15 08:30:33.477773: step 38480, loss = 0.22, batch loss = 0.19 (34.3 examples/sec; 0.234 sec/batch; 19h:04m:31s remains)
INFO - root - 2017-12-15 08:30:35.782277: step 38490, loss = 0.21, batch loss = 0.17 (34.8 examples/sec; 0.230 sec/batch; 18h:46m:13s remains)
INFO - root - 2017-12-15 08:30:38.051413: step 38500, loss = 0.31, batch loss = 0.28 (34.3 examples/sec; 0.233 sec/batch; 19h:02m:50s remains)
2017-12-15 08:30:38.365128: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0883 -6.8258171 -6.76188 -6.805995 -6.6937819 -6.3404694 -5.9252634 -5.7939944 -6.0963287 -6.2616415 -6.3152113 -6.3068972 -6.5077114 -6.5047541 -6.0547137][-4.91272 -6.9886 -6.9852295 -7.1532011 -7.0219879 -6.6204281 -6.1969585 -6.1885018 -6.7445469 -6.9779177 -7.1323757 -7.2386427 -7.5363073 -7.5644312 -7.1789722][-4.4106941 -5.8705869 -5.9761677 -6.3006372 -6.1207585 -5.6102896 -5.1040373 -5.1340814 -5.8929005 -6.2275248 -6.5727205 -6.9406996 -7.4504433 -7.5063982 -7.2322674][-4.2062311 -4.7516718 -4.71134 -4.9100838 -4.4046135 -3.6170328 -2.8859613 -2.9610095 -4.0792093 -4.6758695 -5.3871045 -6.2034349 -7.0495129 -7.2156315 -7.0766611][-4.1480355 -3.5061879 -3.086587 -2.9078572 -1.9199171 -0.78757286 0.23080564 0.18880129 -1.202001 -2.1703563 -3.2831726 -4.5568066 -5.7576456 -6.1940765 -6.3163881][-4.2391367 -2.6956725 -1.941466 -1.5206639 -0.15174794 1.2449849 2.3970625 2.4525917 0.92022467 -0.33866823 -1.6283834 -3.0945554 -4.516974 -5.2503643 -5.607285][-4.4388914 -2.5876505 -1.5436857 -0.82712841 1.1123495 2.9886377 4.3153954 4.4631538 2.8757393 1.2504439 -0.3581636 -2.1591146 -3.944283 -5.0814862 -5.6040115][-4.393528 -2.4620667 -1.181708 -0.32066083 2.0648305 4.2573595 5.5906715 5.7510681 4.0876684 2.132951 0.33627534 -1.5691053 -3.5200377 -5.016326 -5.6872568][-4.5270758 -2.7854698 -1.5782659 -0.92080021 1.490597 3.7464402 4.9197083 5.0015593 3.2989089 1.1986039 -0.60719395 -2.2927783 -4.0105577 -5.5092707 -6.0953236][-4.8330927 -3.4837627 -2.5094001 -2.2176466 -0.05482173 2.1915872 3.3353441 3.4972389 1.8442256 -0.23391938 -1.9460644 -3.336309 -4.706501 -6.0651884 -6.4468155][-4.8272533 -3.899437 -3.2129078 -3.3166261 -1.6553261 0.23816633 1.1824667 1.3264658 -0.23140693 -1.9304825 -3.3436022 -4.2880449 -5.2121592 -6.2710524 -6.4109392][-5.172327 -4.6495485 -4.2224803 -4.5443611 -3.3894143 -1.9811635 -1.2703751 -1.0891368 -2.3897016 -3.5822022 -4.4896941 -4.8962135 -5.3783503 -6.189106 -6.2504349][-5.6476288 -5.409193 -5.1374426 -5.4943552 -4.7471991 -3.7531955 -3.098567 -2.7311397 -3.6647887 -4.4263649 -4.9642024 -4.9713058 -5.1994686 -5.9290314 -6.01774][-6.2173347 -6.2001705 -6.1642876 -6.5171394 -6.0910482 -5.4715281 -4.9961181 -4.6593895 -5.3498144 -5.8935118 -6.0978069 -5.6745934 -5.562294 -5.9974537 -5.9117603][-6.8102713 -6.8586693 -6.9593563 -7.2067089 -6.9110851 -6.5800972 -6.3467226 -6.157547 -6.6038628 -6.9336891 -6.8851385 -6.2361221 -5.9091077 -6.0702562 -5.83193]]...]
INFO - root - 2017-12-15 08:30:40.671406: step 38510, loss = 0.26, batch loss = 0.23 (33.7 examples/sec; 0.238 sec/batch; 19h:24m:17s remains)
INFO - root - 2017-12-15 08:30:42.984077: step 38520, loss = 0.22, batch loss = 0.18 (34.8 examples/sec; 0.230 sec/batch; 18h:45m:27s remains)
INFO - root - 2017-12-15 08:30:45.259104: step 38530, loss = 0.52, batch loss = 0.49 (35.0 examples/sec; 0.228 sec/batch; 18h:39m:00s remains)
INFO - root - 2017-12-15 08:30:47.577975: step 38540, loss = 0.19, batch loss = 0.16 (35.2 examples/sec; 0.227 sec/batch; 18h:31m:55s remains)
INFO - root - 2017-12-15 08:30:49.867183: step 38550, loss = 0.19, batch loss = 0.16 (34.7 examples/sec; 0.231 sec/batch; 18h:49m:36s remains)
INFO - root - 2017-12-15 08:30:52.186260: step 38560, loss = 0.19, batch loss = 0.16 (35.2 examples/sec; 0.227 sec/batch; 18h:32m:45s remains)
INFO - root - 2017-12-15 08:30:54.458940: step 38570, loss = 0.21, batch loss = 0.18 (35.0 examples/sec; 0.229 sec/batch; 18h:39m:28s remains)
INFO - root - 2017-12-15 08:30:56.760633: step 38580, loss = 0.18, batch loss = 0.15 (34.5 examples/sec; 0.232 sec/batch; 18h:54m:18s remains)
INFO - root - 2017-12-15 08:30:59.071458: step 38590, loss = 0.18, batch loss = 0.15 (34.2 examples/sec; 0.234 sec/batch; 19h:06m:56s remains)
INFO - root - 2017-12-15 08:31:01.334799: step 38600, loss = 0.24, batch loss = 0.21 (35.6 examples/sec; 0.225 sec/batch; 18h:20m:25s remains)
2017-12-15 08:31:01.627396: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5196857 -6.1576509 -5.9856749 -4.8609009 -2.8512139 -1.5088493 -0.23927307 0.35871887 0.62102437 0.23033071 -0.50308144 -0.90521479 -1.9168077 -2.7470558 -3.1892071][-5.2072115 -6.0287447 -6.2523212 -5.1927404 -2.7344875 -0.90529764 0.35001493 0.62549233 0.40198112 -0.24091959 -0.94608808 -1.2709634 -1.7347298 -1.6445525 -1.5973895][-5.21694 -5.3118439 -5.9576654 -5.3219662 -3.1091413 -1.1553832 -0.026284218 0.020292282 -0.42103863 -1.1014217 -1.6770474 -1.9148419 -1.861201 -0.9634372 -0.39319468][-5.275485 -4.6077986 -5.2804246 -5.04912 -3.3748584 -1.6020737 -0.64351416 -0.48006225 -0.85780668 -1.5011971 -1.9854114 -2.1812572 -1.9254477 -0.77268696 0.14354491][-4.3771925 -3.2108459 -3.6999846 -3.7683747 -2.7384973 -1.1228982 -0.21472049 0.12900662 -0.05279541 -0.7240603 -1.2019147 -1.3950953 -1.2332531 -0.3242631 0.61368418][-3.487421 -1.5450703 -1.9879843 -2.3418379 -1.7424638 -0.1815455 0.82969093 1.4982328 1.5509093 0.742944 0.034430742 -0.37632167 -0.54457593 0.042627573 1.0930583][-2.2460032 -0.73397458 -1.2752545 -1.8106459 -1.3837509 0.42371011 1.9538369 3.054214 3.288465 2.4476032 1.5439997 0.76121569 -0.057046413 0.085072279 1.1887863][-1.8655524 -0.51705527 -1.4123502 -2.3231738 -1.9384876 0.12824512 2.2239776 3.7191963 4.199717 3.4240007 2.4020095 1.3529837 0.06425786 -0.23813295 0.593163][-2.0525446 -0.8677212 -2.1634822 -3.2984319 -3.0673947 -1.2541044 0.96840239 2.5965509 3.0605249 2.5135016 1.7410216 0.84726977 -0.35543644 -0.74400771 -0.30466223][-3.1332049 -1.8261014 -3.1765585 -4.1758285 -3.9678948 -2.7101793 -0.79526854 0.68212008 0.93924189 0.50950432 0.10463023 -0.3596853 -1.2537571 -1.6014504 -1.5610776][-4.3467669 -2.7327592 -3.8527865 -4.5767512 -4.5778308 -4.0559106 -2.5749516 -1.280663 -1.1477454 -1.4212923 -1.7513952 -2.17244 -2.7543688 -2.9650931 -3.2958655][-5.4769878 -3.6305332 -4.4060068 -4.9331226 -5.1312866 -5.1783676 -4.18114 -3.0561833 -2.9699428 -3.3946714 -4.1197052 -4.7663422 -5.1071262 -5.1161947 -5.4562178][-6.38903 -4.5305796 -4.9831409 -5.2984056 -5.7137189 -6.2503881 -5.7831421 -4.9931936 -5.0813742 -5.6999979 -6.7387056 -7.4045458 -7.3212957 -7.01198 -7.168541][-6.6082025 -4.8962331 -5.0164943 -5.2435188 -5.8611727 -6.6898727 -6.6551847 -6.185142 -6.2523484 -6.7272577 -7.6450148 -8.1556282 -7.7239337 -7.2263718 -7.1693678][-5.9294624 -4.4794455 -4.5422 -4.774477 -5.3564663 -6.0848279 -6.1907487 -5.9052153 -5.8662639 -6.0822897 -6.8980823 -7.3581572 -6.9172525 -6.4889431 -6.4137278]]...]
INFO - root - 2017-12-15 08:31:03.926614: step 38610, loss = 0.27, batch loss = 0.24 (34.8 examples/sec; 0.230 sec/batch; 18h:44m:59s remains)
INFO - root - 2017-12-15 08:31:06.195416: step 38620, loss = 0.19, batch loss = 0.16 (35.1 examples/sec; 0.228 sec/batch; 18h:37m:01s remains)
INFO - root - 2017-12-15 08:31:08.499099: step 38630, loss = 0.20, batch loss = 0.17 (34.5 examples/sec; 0.232 sec/batch; 18h:54m:07s remains)
INFO - root - 2017-12-15 08:31:10.783936: step 38640, loss = 0.25, batch loss = 0.22 (35.4 examples/sec; 0.226 sec/batch; 18h:28m:02s remains)
INFO - root - 2017-12-15 08:31:13.045513: step 38650, loss = 0.32, batch loss = 0.29 (36.8 examples/sec; 0.217 sec/batch; 17h:44m:12s remains)
INFO - root - 2017-12-15 08:31:15.293880: step 38660, loss = 0.28, batch loss = 0.25 (35.8 examples/sec; 0.223 sec/batch; 18h:13m:47s remains)
INFO - root - 2017-12-15 08:31:17.580509: step 38670, loss = 0.28, batch loss = 0.25 (35.6 examples/sec; 0.225 sec/batch; 18h:21m:43s remains)
INFO - root - 2017-12-15 08:31:19.836710: step 38680, loss = 0.22, batch loss = 0.19 (34.2 examples/sec; 0.234 sec/batch; 19h:06m:59s remains)
INFO - root - 2017-12-15 08:31:22.087819: step 38690, loss = 0.30, batch loss = 0.27 (36.1 examples/sec; 0.221 sec/batch; 18h:03m:48s remains)
INFO - root - 2017-12-15 08:31:24.390261: step 38700, loss = 0.21, batch loss = 0.18 (34.6 examples/sec; 0.231 sec/batch; 18h:53m:07s remains)
2017-12-15 08:31:24.663604: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.3965061 -0.81574035 -3.0233817 -5.3055391 -6.2670217 -6.919961 -6.8585286 -6.9502106 -7.1040196 -7.2385836 -7.4157839 -6.9668975 -6.4368443 -6.1405435 -5.266902][-1.5289294 -2.3336976 -4.2775335 -5.867609 -6.09446 -5.9822721 -5.6152449 -5.669385 -6.0702972 -6.6988897 -7.4629183 -7.2687192 -6.7770576 -6.2401524 -5.122014][-3.9686463 -3.556479 -5.146946 -5.9896326 -5.6923709 -5.0489979 -4.4854293 -4.4260206 -4.9554605 -5.772253 -7.0073233 -7.0880146 -6.5852675 -5.8521595 -4.5710754][-5.5884867 -4.5985742 -5.8720613 -6.1100521 -5.2224107 -3.7269521 -2.4880614 -1.9527071 -2.4500127 -3.4821463 -5.4210434 -6.0193272 -5.8901386 -5.3775234 -4.1870279][-7.0840163 -5.5211878 -6.2247033 -5.866776 -4.2969484 -1.790321 0.30419755 1.3113029 0.77857208 -0.575704 -3.2390625 -4.568696 -5.0012608 -4.9450336 -4.0286627][-7.2506981 -5.3510003 -5.9630551 -5.5753479 -3.5955682 -0.28306162 2.4697938 3.8142815 3.4645071 1.9634447 -1.2376075 -3.0020649 -3.7082362 -3.8740311 -3.0752566][-6.0417948 -4.1896434 -4.8512573 -4.7072821 -2.3578632 1.4625037 4.516345 5.9741955 5.61329 3.7562246 0.012853384 -2.2137671 -3.1312087 -3.2781253 -2.1736937][-4.65518 -2.460614 -3.3831067 -3.837678 -1.5531147 2.0102777 4.7450118 5.8927164 5.49023 3.5454888 -0.20897317 -2.5540147 -3.5481179 -3.5732713 -2.17513][-3.9423862 -1.7477603 -3.3107305 -4.6580377 -3.0040028 -0.16413188 1.9770141 2.9125071 2.7683272 1.2257035 -1.7972308 -3.7629762 -4.5152264 -4.43447 -2.9323273][-4.7275815 -2.6484733 -4.4882612 -6.2683172 -5.1477609 -2.9620507 -1.2750821 -0.56806028 -0.63235116 -1.8368882 -4.1735373 -5.7973137 -6.2880225 -6.0330458 -4.6570292][-5.9455152 -4.0031962 -5.7170534 -7.5276885 -7.0334339 -5.6198616 -4.4882178 -4.0904517 -4.1377711 -4.9705582 -6.5729775 -7.7238107 -7.923975 -7.7141557 -6.7584496][-6.6978316 -5.06944 -6.6585717 -8.4683838 -8.6309986 -7.9748654 -7.3767314 -7.1279788 -6.9944973 -7.3768682 -8.256361 -8.9131336 -8.845787 -8.5940628 -7.9411378][-6.9365139 -5.3449755 -6.4330664 -7.8282738 -8.2171621 -8.0819149 -7.9520841 -7.9522371 -7.8874278 -8.0575314 -8.5169353 -8.766428 -8.4477024 -8.0980415 -7.6587777][-6.4335904 -4.8491607 -5.33643 -6.0779915 -6.349771 -6.387238 -6.4638324 -6.5723925 -6.5795646 -6.7291927 -7.0463986 -7.2051845 -6.9848342 -6.7405791 -6.4727783][-5.3607273 -4.0157738 -4.2115107 -4.5588627 -4.6832509 -4.6888847 -4.7059164 -4.7532043 -4.8090725 -4.9742441 -5.239953 -5.4103708 -5.35021 -5.2164383 -5.0737553]]...]
INFO - root - 2017-12-15 08:31:26.939954: step 38710, loss = 0.23, batch loss = 0.20 (34.8 examples/sec; 0.230 sec/batch; 18h:46m:34s remains)
INFO - root - 2017-12-15 08:31:29.222680: step 38720, loss = 0.31, batch loss = 0.28 (36.1 examples/sec; 0.221 sec/batch; 18h:03m:38s remains)
INFO - root - 2017-12-15 08:31:31.506307: step 38730, loss = 0.26, batch loss = 0.23 (34.6 examples/sec; 0.231 sec/batch; 18h:52m:22s remains)
INFO - root - 2017-12-15 08:31:33.786823: step 38740, loss = 0.23, batch loss = 0.20 (34.5 examples/sec; 0.232 sec/batch; 18h:56m:19s remains)
INFO - root - 2017-12-15 08:31:36.044675: step 38750, loss = 0.24, batch loss = 0.21 (35.6 examples/sec; 0.225 sec/batch; 18h:20m:16s remains)
INFO - root - 2017-12-15 08:31:38.328560: step 38760, loss = 0.18, batch loss = 0.15 (34.6 examples/sec; 0.231 sec/batch; 18h:53m:11s remains)
INFO - root - 2017-12-15 08:31:40.592190: step 38770, loss = 0.32, batch loss = 0.29 (35.7 examples/sec; 0.224 sec/batch; 18h:17m:34s remains)
INFO - root - 2017-12-15 08:31:42.876286: step 38780, loss = 0.25, batch loss = 0.22 (34.4 examples/sec; 0.233 sec/batch; 18h:58m:11s remains)
INFO - root - 2017-12-15 08:31:45.165068: step 38790, loss = 0.23, batch loss = 0.20 (34.2 examples/sec; 0.234 sec/batch; 19h:04m:13s remains)
INFO - root - 2017-12-15 08:31:47.449836: step 38800, loss = 0.27, batch loss = 0.24 (35.7 examples/sec; 0.224 sec/batch; 18h:18m:18s remains)
2017-12-15 08:31:47.715144: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.660147 -5.5376406 -6.0769167 -5.8783855 -5.0991426 -4.2016153 -3.62402 -3.3252306 -3.6311595 -4.7494059 -5.2183437 -5.2128868 -5.0629148 -4.3661747 -3.3460517][-4.82819 -6.2953215 -6.9911809 -6.9617524 -6.2250471 -5.3338995 -4.7338057 -4.3078208 -4.5666485 -5.6828523 -6.3287969 -6.4521432 -6.3071833 -5.529459 -4.4650736][-5.37521 -6.3609595 -7.015121 -7.05139 -6.2902 -5.3795567 -4.6419516 -4.0327477 -4.229537 -5.3352528 -6.3048406 -6.6765404 -6.6432405 -5.970192 -5.0069427][-5.4567738 -5.8064003 -6.207427 -6.1762552 -5.3350797 -4.3472166 -3.3802359 -2.543432 -2.6534555 -3.7565267 -5.1452684 -5.8829479 -6.1651449 -5.88702 -5.2156076][-5.1207037 -4.6210136 -4.5661116 -4.2789383 -3.3304696 -2.2168176 -0.93578351 0.19967127 0.16457987 -1.0407007 -2.8492742 -3.9411931 -4.5656939 -4.7850661 -4.4804673][-4.7309732 -3.4791279 -2.9102776 -2.2700078 -1.132057 0.241153 1.9672022 3.4222798 3.3574839 1.9856811 -0.17117381 -1.5214192 -2.3778975 -2.9271789 -2.8912461][-4.1809793 -2.7615306 -1.8263963 -0.88520849 0.44602203 2.09298 4.1372876 5.7259517 5.582695 3.959784 1.6031535 0.2483747 -0.50542581 -1.0809026 -1.1780226][-4.0142441 -2.6080554 -1.6054965 -0.53975856 0.88013673 2.6415181 4.66948 6.2061429 6.0210977 4.192668 1.8928323 0.66510367 0.12376451 -0.24506879 -0.29360056][-4.2806635 -2.9433897 -2.0862908 -1.0822964 0.29242229 1.9956927 3.8086319 5.1735053 4.8337846 2.8296952 0.73326921 -0.32039249 -0.58691037 -0.580017 -0.43526769][-4.3654289 -3.1102102 -2.521255 -1.7455562 -0.56794322 0.88928914 2.2683172 3.2529397 2.6730938 0.51549029 -1.2701312 -2.0048056 -1.8513625 -1.22406 -0.62609196][-4.6566372 -3.5858865 -3.3313341 -2.8131363 -1.8805435 -0.72843862 0.15408421 0.684803 -0.027313232 -2.1303513 -3.4646108 -3.7689395 -3.19109 -2.0082276 -0.98790741][-5.4584141 -4.6672482 -4.668282 -4.3542457 -3.6186266 -2.7207811 -2.1904316 -1.9398353 -2.624408 -4.4067822 -5.2015429 -5.1414752 -4.3737049 -2.9371743 -1.6783447][-6.3162818 -5.8589268 -6.0568037 -5.9087224 -5.351243 -4.6105814 -4.1913681 -4.0102296 -4.5944042 -5.9527445 -6.30816 -6.0392342 -5.2784996 -3.889524 -2.6357586][-6.8227119 -6.5989833 -6.8522377 -6.8317051 -6.4666791 -5.8485794 -5.4306188 -5.2507267 -5.6612115 -6.5939975 -6.702888 -6.4311666 -5.87152 -4.7951384 -3.7816][-6.74344 -6.5850177 -6.7818036 -6.8106871 -6.5843568 -6.0866709 -5.6705704 -5.4738636 -5.7274475 -6.2874937 -6.2725778 -6.0686264 -5.7099776 -4.9958463 -4.3185673]]...]
INFO - root - 2017-12-15 08:31:49.988459: step 38810, loss = 0.23, batch loss = 0.20 (33.7 examples/sec; 0.237 sec/batch; 19h:21m:13s remains)
INFO - root - 2017-12-15 08:31:52.264091: step 38820, loss = 0.28, batch loss = 0.25 (34.5 examples/sec; 0.232 sec/batch; 18h:53m:24s remains)
INFO - root - 2017-12-15 08:31:54.562337: step 38830, loss = 0.21, batch loss = 0.17 (34.4 examples/sec; 0.233 sec/batch; 18h:58m:53s remains)
INFO - root - 2017-12-15 08:31:56.847429: step 38840, loss = 0.26, batch loss = 0.23 (34.5 examples/sec; 0.232 sec/batch; 18h:53m:29s remains)
INFO - root - 2017-12-15 08:31:59.125266: step 38850, loss = 0.24, batch loss = 0.21 (35.5 examples/sec; 0.225 sec/batch; 18h:21m:28s remains)
INFO - root - 2017-12-15 08:32:01.398876: step 38860, loss = 0.15, batch loss = 0.12 (34.4 examples/sec; 0.232 sec/batch; 18h:56m:49s remains)
INFO - root - 2017-12-15 08:32:03.675541: step 38870, loss = 0.21, batch loss = 0.18 (36.1 examples/sec; 0.221 sec/batch; 18h:03m:32s remains)
INFO - root - 2017-12-15 08:32:05.973689: step 38880, loss = 0.20, batch loss = 0.17 (32.7 examples/sec; 0.244 sec/batch; 19h:55m:42s remains)
INFO - root - 2017-12-15 08:32:08.256726: step 38890, loss = 0.19, batch loss = 0.16 (35.5 examples/sec; 0.225 sec/batch; 18h:22m:01s remains)
INFO - root - 2017-12-15 08:32:10.522357: step 38900, loss = 0.22, batch loss = 0.19 (35.3 examples/sec; 0.226 sec/batch; 18h:28m:19s remains)
2017-12-15 08:32:10.817785: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7604525 -4.5657792 -4.1101646 -2.7249148 -1.8717256 -1.3164159 -1.4580737 -1.5564711 -1.6080961 -2.2614315 -2.5890658 -2.0909731 -1.1169508 -0.91296077 -1.8663608][-3.2821054 -3.8799675 -3.0986171 -1.5792414 -0.80337429 -0.51588988 -0.83191848 -1.1354021 -1.4791616 -2.3171568 -2.9764173 -2.9475298 -2.3548107 -2.4234426 -3.4519596][-4.56036 -4.0174685 -3.0302582 -1.6198337 -0.99055052 -0.9569217 -1.2204571 -1.620383 -2.2251823 -3.1860156 -4.0472031 -4.2907572 -4.2312045 -4.7529 -5.7626262][-6.2661085 -4.5647893 -3.3874826 -2.0104659 -1.3550663 -1.3765562 -1.3929987 -1.6701617 -2.4836531 -3.6325159 -4.7316504 -5.202054 -5.6864443 -6.5444317 -7.4096346][-7.768198 -5.3054862 -3.9593334 -2.3290424 -1.3070207 -0.95006883 -0.37192035 -0.2536025 -1.1709379 -2.666898 -4.1138411 -4.9205027 -5.9718809 -7.0007553 -7.7270455][-8.0722818 -5.6590657 -4.0564384 -2.1955481 -0.67632008 0.27037072 1.6450467 2.3075905 1.3253582 -0.61946774 -2.5443642 -3.80689 -5.2863212 -6.2597136 -6.6935148][-7.2767687 -5.2571726 -3.5711191 -1.5497736 0.50557494 2.1288638 4.3016076 5.3368707 4.1332555 1.7057333 -0.66296852 -2.2100341 -3.6251571 -4.1505117 -4.3057113][-5.6426907 -4.1161022 -2.7188022 -0.80299258 1.409374 3.3839841 5.962635 7.0429106 5.5345316 2.8185515 0.40076208 -0.98199797 -1.7510639 -1.6640024 -1.7180932][-4.3038411 -3.2932868 -2.3431516 -0.86288011 1.1264422 2.8498883 5.1088786 5.7429352 4.2505121 1.8557143 -0.17860818 -1.006554 -0.91786695 -0.34563959 -0.36135709][-3.6875095 -3.2000551 -2.8870845 -2.0206645 -0.54448605 0.64475417 2.2413607 2.5329266 1.4695058 -0.082645178 -1.4603924 -1.6846001 -0.97393489 -0.34646654 -0.53888941][-3.343168 -3.4428916 -3.864089 -3.5139561 -2.5312037 -1.8423653 -0.95500755 -0.84281397 -1.4248729 -2.2925658 -2.9619942 -2.6761265 -1.6044705 -0.96681631 -1.3549619][-3.6226816 -4.1457043 -4.9738474 -4.9816675 -4.3401451 -4.0317626 -3.8377981 -3.8889961 -4.2142754 -4.6086693 -4.78447 -4.2419634 -3.0568159 -2.500457 -2.9692562][-4.0572171 -4.9744415 -6.0648813 -6.3000183 -5.8743048 -5.7010841 -5.9383936 -6.0658236 -6.284308 -6.4197326 -6.3019772 -5.7281785 -4.5749145 -4.0237045 -4.4047737][-4.6962919 -5.9833226 -7.2824955 -7.7017679 -7.4240937 -7.2375989 -7.5559034 -7.585258 -7.6528087 -7.6455827 -7.3999777 -6.8601332 -5.8813715 -5.3585052 -5.5324483][-5.9822664 -7.4207773 -8.666173 -9.1176872 -8.96402 -8.734642 -8.8348083 -8.5775757 -8.3503456 -8.1481848 -7.901341 -7.5495009 -6.9253864 -6.5828066 -6.6914492]]...]
INFO - root - 2017-12-15 08:32:13.098307: step 38910, loss = 0.20, batch loss = 0.17 (33.9 examples/sec; 0.236 sec/batch; 19h:14m:41s remains)
INFO - root - 2017-12-15 08:32:15.375940: step 38920, loss = 0.22, batch loss = 0.19 (36.0 examples/sec; 0.222 sec/batch; 18h:08m:01s remains)
INFO - root - 2017-12-15 08:32:17.616452: step 38930, loss = 0.26, batch loss = 0.23 (36.3 examples/sec; 0.220 sec/batch; 17h:57m:12s remains)
INFO - root - 2017-12-15 08:32:19.895261: step 38940, loss = 0.31, batch loss = 0.28 (35.1 examples/sec; 0.228 sec/batch; 18h:34m:44s remains)
INFO - root - 2017-12-15 08:32:22.159005: step 38950, loss = 0.26, batch loss = 0.23 (36.1 examples/sec; 0.221 sec/batch; 18h:03m:38s remains)
INFO - root - 2017-12-15 08:32:24.418818: step 38960, loss = 0.23, batch loss = 0.19 (36.0 examples/sec; 0.223 sec/batch; 18h:08m:34s remains)
INFO - root - 2017-12-15 08:32:26.661797: step 38970, loss = 0.32, batch loss = 0.29 (35.8 examples/sec; 0.223 sec/batch; 18h:12m:03s remains)
INFO - root - 2017-12-15 08:32:28.924075: step 38980, loss = 0.24, batch loss = 0.21 (33.2 examples/sec; 0.241 sec/batch; 19h:38m:03s remains)
INFO - root - 2017-12-15 08:32:31.183122: step 38990, loss = 0.29, batch loss = 0.26 (36.6 examples/sec; 0.219 sec/batch; 17h:50m:04s remains)
INFO - root - 2017-12-15 08:32:33.441473: step 39000, loss = 0.19, batch loss = 0.15 (35.8 examples/sec; 0.223 sec/batch; 18h:12m:40s remains)
2017-12-15 08:32:33.747829: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5333023 -6.1813822 -6.0849552 -5.9282784 -5.8456092 -5.8523607 -5.8891315 -5.9414845 -5.9928374 -5.8952384 -5.6964436 -5.456192 -5.1699591 -4.85262 -4.5930815][-6.2926741 -7.7233725 -7.60822 -7.2771168 -6.9058142 -6.7251339 -6.6769505 -6.6935434 -6.7451458 -6.797121 -6.8315659 -6.7004595 -6.3912382 -6.0062304 -5.5924387][-7.4053841 -8.4909172 -8.5150633 -8.1338291 -7.4327717 -6.9113693 -6.7335434 -6.7710285 -6.8920488 -7.0528011 -7.3311539 -7.5020752 -7.326004 -6.9712753 -6.4589806][-7.5107374 -8.31205 -8.5641232 -8.26785 -7.3446875 -6.4172115 -5.9110031 -5.8439941 -6.1064987 -6.5649815 -7.2900524 -7.9056177 -7.9862804 -7.7494917 -7.204215][-6.6704826 -7.2044153 -7.55649 -7.3352251 -6.3476849 -5.0933695 -4.1723719 -3.845422 -4.1243138 -4.9740133 -6.4802837 -7.747304 -8.0857477 -8.0049229 -7.5743055][-5.30966 -5.4384432 -5.8504052 -5.6685829 -4.76982 -3.3696709 -1.959252 -1.1065555 -1.1631958 -2.3007851 -4.6062579 -6.6331692 -7.4119921 -7.7491693 -7.6027756][-3.3903439 -3.3044758 -3.9333146 -3.8723869 -3.1884506 -1.7674236 0.023525953 1.4295065 1.6803956 0.41161585 -2.4111366 -4.9546666 -6.2156048 -7.1205187 -7.3847351][-1.9365442 -1.3870592 -1.9640762 -1.8506222 -1.329628 -0.027798176 1.7811353 3.1951573 3.3301027 1.9030015 -0.99501026 -3.5492928 -5.0283041 -6.2731152 -6.8291874][-1.4431677 -0.41123092 -0.63717091 -0.256768 0.29777551 1.4439077 2.996094 4.1133051 3.93913 2.3259194 -0.54603529 -3.0382547 -4.5400534 -5.7458334 -6.3308945][-1.5952358 -0.025955439 -0.02314353 0.39459348 0.83512068 1.703361 2.8523338 3.5966465 3.3593433 1.8613017 -0.83190751 -3.2469721 -4.7531281 -5.7826605 -6.1949372][-2.2582276 -0.03789258 0.24812841 0.65573 0.86588573 1.2268414 1.7786758 2.082093 1.7432125 0.41417432 -1.894343 -4.0417795 -5.3985348 -6.0949097 -6.2008839][-2.9550231 -0.01245141 0.46137309 0.72788882 0.65133953 0.52980137 0.48820114 0.27917933 -0.34152341 -1.6668141 -3.6517749 -5.4615989 -6.5022259 -6.7728291 -6.5838194][-3.6020348 -0.23827004 0.16747618 0.16033101 -0.22801113 -0.6078006 -0.90111005 -1.3516119 -2.1254008 -3.4423428 -5.2488108 -6.7889786 -7.5005817 -7.4758186 -7.0924244][-4.089097 -0.8954457 -0.70982325 -0.80903697 -1.1707981 -1.4919127 -1.8393841 -2.4239247 -3.22204 -4.4523153 -6.0376945 -7.3416719 -7.9210339 -7.8485374 -7.3434124][-4.3714 -1.6746402 -1.6896732 -1.7447412 -1.6943833 -1.5842255 -1.7099136 -2.2102835 -2.969763 -4.2113557 -5.7571716 -6.9645805 -7.5884228 -7.6585274 -7.1744156]]...]
INFO - root - 2017-12-15 08:32:36.029301: step 39010, loss = 0.27, batch loss = 0.24 (34.9 examples/sec; 0.229 sec/batch; 18h:41m:13s remains)
INFO - root - 2017-12-15 08:32:38.287008: step 39020, loss = 0.26, batch loss = 0.22 (33.9 examples/sec; 0.236 sec/batch; 19h:13m:27s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 08:32:40.576421: step 39030, loss = 0.23, batch loss = 0.20 (34.8 examples/sec; 0.230 sec/batch; 18h:45m:09s remains)
INFO - root - 2017-12-15 08:32:42.882029: step 39040, loss = 0.23, batch loss = 0.20 (33.9 examples/sec; 0.236 sec/batch; 19h:15m:23s remains)
INFO - root - 2017-12-15 08:32:45.152404: step 39050, loss = 0.16, batch loss = 0.12 (36.0 examples/sec; 0.222 sec/batch; 18h:06m:17s remains)
INFO - root - 2017-12-15 08:32:47.419186: step 39060, loss = 0.22, batch loss = 0.18 (35.2 examples/sec; 0.227 sec/batch; 18h:32m:37s remains)
INFO - root - 2017-12-15 08:32:49.741984: step 39070, loss = 0.29, batch loss = 0.26 (33.8 examples/sec; 0.237 sec/batch; 19h:18m:34s remains)
INFO - root - 2017-12-15 08:32:52.026205: step 39080, loss = 0.20, batch loss = 0.17 (35.6 examples/sec; 0.225 sec/batch; 18h:18m:42s remains)
INFO - root - 2017-12-15 08:32:54.333530: step 39090, loss = 0.23, batch loss = 0.20 (31.9 examples/sec; 0.251 sec/batch; 20h:25m:52s remains)
INFO - root - 2017-12-15 08:32:56.600370: step 39100, loss = 0.22, batch loss = 0.19 (36.1 examples/sec; 0.222 sec/batch; 18h:05m:06s remains)
2017-12-15 08:32:56.887989: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0333385 -8.8574858 -6.0539637 -5.4321842 -4.3603864 -1.910476 -1.1007965 -1.4565387 -1.4653556 -3.5087957 -3.8292344 -3.0128257 -2.8532393 -3.4795008 -3.624826][-7.3517323 -9.1840334 -6.5610008 -6.1053185 -5.3131962 -2.9194946 -1.8645027 -2.1187313 -1.9548628 -3.9925723 -4.6662116 -3.985291 -3.6807423 -4.0948477 -4.3912735][-8.763463 -9.1666851 -6.5459671 -6.0534596 -5.4040785 -3.2832391 -2.105336 -2.1306925 -1.9604836 -4.1280303 -5.3095875 -4.8376913 -4.5202761 -4.8665328 -5.4053769][-9.7006664 -9.0735712 -6.2859821 -5.3741188 -4.3885312 -2.3072689 -0.942721 -0.85788012 -0.87866139 -3.145546 -4.7986755 -4.6391683 -4.4308205 -4.7859674 -5.3774061][-9.7516155 -8.7115011 -5.6578484 -4.1194444 -2.4068103 -0.031713247 1.5768766 1.4754882 0.83009458 -1.9733115 -4.2430463 -4.5146723 -4.3828073 -4.5446272 -4.877986][-9.9086542 -8.3125448 -5.0243216 -2.839963 -0.33776927 2.5550611 4.5096216 4.2350378 2.9157808 -0.74960554 -3.8163781 -4.6461782 -4.5156546 -4.38886 -4.3808608][-9.7113724 -8.0262194 -4.5816393 -2.0575886 1.0225518 4.4831944 6.9663153 6.8619356 5.4373531 1.2433581 -2.4301627 -3.7558045 -3.8971043 -3.8700011 -3.7673283][-9.6896572 -8.1103706 -4.8021784 -2.3623364 0.89706039 4.622653 7.5934734 7.8837509 6.8669167 2.7417619 -1.125843 -2.8432302 -3.5400753 -3.7860107 -3.6259766][-9.8728638 -8.5822258 -5.8113 -3.8832459 -1.0222991 2.4499562 5.4028559 6.0698595 5.6962919 2.338928 -1.1152126 -2.9429541 -4.0533075 -4.492434 -4.2055159][-10.024927 -9.0419407 -6.8370314 -5.6549997 -3.5722382 -0.78334522 1.6427813 2.3802679 2.5360477 0.021206141 -2.6397581 -4.1379194 -5.1636443 -5.4955769 -5.1451912][-10.289581 -9.5541239 -7.7013969 -7.0578709 -5.5893631 -3.551815 -1.9379561 -1.4723094 -1.3628685 -3.3930326 -5.161561 -5.7704911 -6.2166233 -6.3327017 -5.931262][-10.211144 -9.649004 -8.0477924 -7.6370363 -6.4369183 -4.7715769 -3.809628 -3.839962 -4.1301017 -6.0343208 -7.1167622 -6.8339167 -6.54883 -6.1635137 -5.5021353][-9.3753872 -8.7242451 -7.1419163 -6.8258352 -5.7864876 -4.2842989 -3.7147651 -3.9676585 -4.538918 -6.3135633 -7.09966 -6.2757854 -5.413568 -4.4816656 -3.7131538][-8.4184637 -7.4655256 -5.6543121 -5.3067312 -4.473033 -3.3056982 -2.9767175 -3.3524938 -3.96211 -5.554821 -6.2893925 -5.1452484 -3.8306351 -2.6147029 -2.1428032][-7.9335775 -6.6863756 -4.635869 -4.1717663 -3.5487061 -2.62995 -2.406415 -2.8781469 -3.496017 -4.9284272 -5.7046356 -4.362505 -2.9156542 -1.7229217 -1.532198]]...]
INFO - root - 2017-12-15 08:32:59.204749: step 39110, loss = 0.26, batch loss = 0.23 (34.8 examples/sec; 0.230 sec/batch; 18h:44m:09s remains)
INFO - root - 2017-12-15 08:33:01.552033: step 39120, loss = 0.21, batch loss = 0.18 (35.4 examples/sec; 0.226 sec/batch; 18h:26m:11s remains)
INFO - root - 2017-12-15 08:33:03.849700: step 39130, loss = 0.23, batch loss = 0.20 (31.9 examples/sec; 0.251 sec/batch; 20h:27m:58s remains)
INFO - root - 2017-12-15 08:33:06.151836: step 39140, loss = 0.26, batch loss = 0.22 (34.8 examples/sec; 0.230 sec/batch; 18h:43m:59s remains)
INFO - root - 2017-12-15 08:33:08.451705: step 39150, loss = 0.19, batch loss = 0.16 (35.4 examples/sec; 0.226 sec/batch; 18h:24m:44s remains)
INFO - root - 2017-12-15 08:33:10.728021: step 39160, loss = 0.22, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 18h:08m:56s remains)
INFO - root - 2017-12-15 08:33:13.005693: step 39170, loss = 0.19, batch loss = 0.16 (36.4 examples/sec; 0.220 sec/batch; 17h:55m:22s remains)
INFO - root - 2017-12-15 08:33:15.270387: step 39180, loss = 0.24, batch loss = 0.21 (35.0 examples/sec; 0.229 sec/batch; 18h:38m:59s remains)
INFO - root - 2017-12-15 08:33:17.583968: step 39190, loss = 0.32, batch loss = 0.29 (34.9 examples/sec; 0.229 sec/batch; 18h:41m:32s remains)
INFO - root - 2017-12-15 08:33:19.857466: step 39200, loss = 0.38, batch loss = 0.35 (34.8 examples/sec; 0.230 sec/batch; 18h:44m:35s remains)
2017-12-15 08:33:20.157435: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.996809 -7.1378145 -7.36788 -7.7223387 -8.0168152 -7.8243284 -7.5011883 -7.4697723 -7.5360107 -7.58117 -7.4386091 -7.266098 -7.2537832 -7.3024731 -7.5616293][-7.9418516 -8.5122547 -8.7721157 -9.2939739 -9.6399 -9.2200661 -8.7453079 -8.8919716 -9.24671 -9.5210476 -9.5330009 -9.3268566 -9.2177429 -9.1278286 -9.3191528][-9.53204 -8.8716917 -8.9447241 -9.3421154 -9.624526 -8.920042 -8.0483141 -8.0821953 -8.6182175 -9.1784325 -9.443697 -9.2786255 -9.2412777 -9.2649469 -9.53863][-10.202686 -8.5161562 -8.2834835 -8.3104458 -8.2373924 -7.1124763 -5.6733737 -5.3545852 -6.0724487 -7.0631695 -7.8134108 -7.9412241 -8.1582813 -8.4064846 -8.6888227][-10.045261 -7.4349308 -6.818552 -6.3482685 -5.7814503 -4.1924825 -2.1250484 -1.3727443 -2.4140239 -4.1100645 -5.8101883 -6.6216183 -7.1373215 -7.4422503 -7.5243254][-9.4349566 -6.4632158 -5.5029945 -4.6479292 -3.6720016 -1.5348854 1.2311928 2.3888628 0.81105089 -1.6728541 -4.1002107 -5.3587532 -5.8218613 -5.9663868 -5.7010856][-8.1347685 -5.2403169 -4.0137305 -2.9214582 -1.5470273 1.2338784 4.7882776 6.3378458 4.2089653 1.0156143 -1.8351197 -3.23013 -3.4936917 -3.419868 -2.9098625][-7.7057686 -4.7350993 -3.3976586 -2.1298196 -0.29924512 3.0554645 7.0844574 8.7992287 6.1959362 2.5982306 -0.20812011 -1.4520619 -1.4343029 -1.3021119 -1.0047923][-7.904027 -5.2096767 -4.024065 -2.6911669 -0.68544555 2.6881406 6.4133539 7.7533865 4.892519 1.4793911 -0.75925756 -1.2997444 -0.57881868 -0.17734027 -0.14131927][-8.035943 -5.6783247 -4.8724675 -3.8382275 -2.1524119 0.5894115 3.2161238 3.8422134 1.2941475 -1.2321669 -2.2631788 -1.6614904 -0.32989788 0.0478487 -0.34564626][-8.2294674 -6.1158023 -5.6666474 -5.0391779 -3.8703895 -1.8711258 -0.34044123 -0.24390578 -2.0954649 -3.5656028 -3.6003418 -2.432493 -1.1222401 -1.0185766 -1.8394513][-8.9921885 -7.06618 -6.7547655 -6.2970743 -5.539681 -4.1448255 -3.4256504 -3.754602 -4.9261189 -5.5422039 -5.0988684 -4.0745525 -3.1556618 -3.2774553 -4.2075391][-10.079677 -8.2492886 -7.8163013 -7.2992439 -6.8310614 -5.9839239 -5.9976106 -6.7034988 -7.3654613 -7.4154625 -6.8136644 -6.0426531 -5.4820604 -5.7354016 -6.5506153][-10.403665 -8.5001135 -7.79535 -7.0966072 -6.7941732 -6.4525909 -7.0491695 -8.0584555 -8.553545 -8.4997988 -7.9633589 -7.4478312 -7.1280355 -7.2986593 -7.792305][-10.001118 -8.1244373 -7.3362274 -6.5313587 -6.2392225 -6.144021 -6.826519 -7.7011509 -8.0754013 -8.1401825 -8.0098772 -8.0236492 -8.0603571 -8.1439571 -8.2804108]]...]
INFO - root - 2017-12-15 08:33:22.432578: step 39210, loss = 0.24, batch loss = 0.20 (35.6 examples/sec; 0.225 sec/batch; 18h:19m:46s remains)
INFO - root - 2017-12-15 08:33:24.718930: step 39220, loss = 0.26, batch loss = 0.23 (36.3 examples/sec; 0.221 sec/batch; 17h:57m:51s remains)
INFO - root - 2017-12-15 08:33:26.983843: step 39230, loss = 0.22, batch loss = 0.19 (35.2 examples/sec; 0.228 sec/batch; 18h:32m:22s remains)
INFO - root - 2017-12-15 08:33:29.247943: step 39240, loss = 0.25, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 18h:15m:04s remains)
INFO - root - 2017-12-15 08:33:31.483630: step 39250, loss = 0.21, batch loss = 0.17 (35.1 examples/sec; 0.228 sec/batch; 18h:32m:39s remains)
INFO - root - 2017-12-15 08:33:33.756958: step 39260, loss = 0.28, batch loss = 0.24 (34.3 examples/sec; 0.234 sec/batch; 19h:01m:15s remains)
INFO - root - 2017-12-15 08:33:36.031146: step 39270, loss = 0.25, batch loss = 0.22 (35.4 examples/sec; 0.226 sec/batch; 18h:24m:00s remains)
INFO - root - 2017-12-15 08:33:38.320190: step 39280, loss = 0.19, batch loss = 0.16 (35.9 examples/sec; 0.223 sec/batch; 18h:10m:06s remains)
INFO - root - 2017-12-15 08:33:40.580259: step 39290, loss = 0.20, batch loss = 0.17 (35.2 examples/sec; 0.228 sec/batch; 18h:32m:11s remains)
INFO - root - 2017-12-15 08:33:42.836677: step 39300, loss = 0.24, batch loss = 0.21 (35.0 examples/sec; 0.228 sec/batch; 18h:36m:31s remains)
2017-12-15 08:33:43.106581: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2513824 -6.2921381 -6.696949 -6.4623656 -4.5426693 -2.5385156 -1.2082777 0.20524216 1.1946404 1.2259469 0.38539219 -1.1412944 -3.4595113 -4.5747118 -4.364851][-4.59575 -6.856041 -7.2796483 -6.7123652 -4.4132805 -1.9809842 -0.34353971 1.3167412 2.3457136 2.1983876 1.322144 -0.32245481 -2.9500396 -4.041728 -3.793596][-5.2929153 -7.0331793 -7.3569107 -6.4894509 -3.8088946 -1.1532621 0.64480972 2.1825495 2.965147 2.5792651 1.6015558 -0.11149907 -2.9140797 -3.9830396 -3.6351295][-6.508163 -7.064394 -7.2943764 -6.2444429 -3.3784981 -0.79772818 0.90713048 2.2422686 2.6975098 2.1514878 1.0476282 -0.4477824 -3.2130764 -4.2870531 -3.7770605][-7.30957 -7.0789752 -7.3413177 -6.3291373 -3.4827762 -1.1309519 0.38578773 1.5415816 1.8948026 1.3848467 0.27077055 -0.93280196 -3.5176415 -4.505497 -3.7215052][-7.3171015 -7.0745983 -7.3340878 -6.2654819 -3.2063532 -0.73043621 0.83529854 1.829545 2.0612526 1.4124842 0.14777875 -1.0744225 -3.4050107 -4.3295527 -3.2190862][-6.5943818 -7.0488806 -7.2695093 -6.0748415 -2.6292486 0.26057076 1.9273477 2.712348 2.725286 1.8168402 0.16087556 -1.3228822 -3.3762896 -4.0196056 -2.5157723][-6.6829686 -6.9392524 -7.1531315 -5.93691 -2.3064816 0.85754704 2.6519451 3.3932304 3.341507 2.1306949 0.10532808 -1.7145947 -3.4241633 -3.6881862 -1.8569925][-6.599196 -6.8153205 -7.075336 -5.9616275 -2.3909907 0.98640013 2.9777431 3.8576317 3.7084827 2.2809587 -0.013700008 -2.240603 -3.8663547 -3.8374348 -1.8372502][-6.5758462 -6.8445554 -7.2379 -6.3432031 -3.0594392 0.29257822 2.3969059 3.3215895 2.9010386 1.3615217 -0.82934141 -3.1050489 -4.6266646 -4.5988946 -2.6622827][-6.5983396 -6.9916253 -7.6380534 -7.1733112 -4.5082045 -1.5741558 0.36565733 1.2668018 0.92944312 -0.4063561 -2.0769196 -3.8655484 -5.168108 -5.2941184 -3.6960776][-6.6749296 -7.1956353 -8.0520668 -7.9461656 -5.8405676 -3.2806892 -1.4642015 -0.49847877 -0.59683847 -1.5912447 -2.8303738 -4.1554947 -5.3202152 -5.5135555 -4.48676][-6.7278681 -7.3022695 -8.248457 -8.31019 -6.6047149 -4.2718554 -2.6399024 -1.6608572 -1.6734357 -2.4586518 -3.4462194 -4.7097254 -5.8433924 -5.83607 -5.14931][-6.681448 -7.2776423 -8.3144894 -8.5729046 -7.2949295 -5.282012 -3.8172061 -2.8000352 -2.5489144 -3.0684981 -3.7566438 -4.917676 -6.1860619 -6.0322275 -5.5443544][-6.4218044 -6.9627995 -8.09237 -8.652956 -7.8622279 -6.1796541 -4.7390504 -3.6804175 -3.0293591 -3.164433 -3.6984787 -4.8574376 -6.4369783 -6.4654 -6.1856937]]...]
INFO - root - 2017-12-15 08:33:45.377208: step 39310, loss = 0.20, batch loss = 0.17 (35.9 examples/sec; 0.223 sec/batch; 18h:07m:25s remains)
INFO - root - 2017-12-15 08:33:47.643211: step 39320, loss = 0.27, batch loss = 0.24 (35.9 examples/sec; 0.223 sec/batch; 18h:09m:50s remains)
INFO - root - 2017-12-15 08:33:49.899727: step 39330, loss = 0.19, batch loss = 0.16 (35.4 examples/sec; 0.226 sec/batch; 18h:23m:54s remains)
INFO - root - 2017-12-15 08:33:52.195438: step 39340, loss = 0.25, batch loss = 0.22 (35.1 examples/sec; 0.228 sec/batch; 18h:34m:51s remains)
INFO - root - 2017-12-15 08:33:54.472984: step 39350, loss = 0.24, batch loss = 0.21 (35.8 examples/sec; 0.224 sec/batch; 18h:12m:23s remains)
INFO - root - 2017-12-15 08:33:56.752777: step 39360, loss = 0.22, batch loss = 0.19 (36.0 examples/sec; 0.222 sec/batch; 18h:05m:38s remains)
INFO - root - 2017-12-15 08:33:59.023911: step 39370, loss = 0.20, batch loss = 0.17 (35.2 examples/sec; 0.227 sec/batch; 18h:28m:52s remains)
INFO - root - 2017-12-15 08:34:01.287657: step 39380, loss = 0.23, batch loss = 0.20 (35.9 examples/sec; 0.223 sec/batch; 18h:09m:18s remains)
INFO - root - 2017-12-15 08:34:03.557469: step 39390, loss = 0.19, batch loss = 0.15 (32.3 examples/sec; 0.248 sec/batch; 20h:10m:19s remains)
INFO - root - 2017-12-15 08:34:05.816540: step 39400, loss = 0.22, batch loss = 0.19 (36.1 examples/sec; 0.222 sec/batch; 18h:03m:59s remains)
2017-12-15 08:34:06.122546: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9355352 -7.0839052 -7.5186634 -7.4493914 -7.1970997 -6.7148786 -6.0503016 -5.7372036 -5.8416767 -5.85382 -5.8672838 -5.8335772 -5.9899116 -6.3175631 -6.5059071][-5.4862242 -8.0230923 -8.6281834 -8.63502 -8.3669243 -7.6187673 -6.5439687 -6.0707979 -6.3982711 -6.6935091 -7.0145168 -7.1183 -7.3686342 -7.7336416 -7.9189353][-6.6332054 -8.4039974 -9.0741405 -9.0256014 -8.5753479 -7.3682313 -5.6522489 -4.8414373 -5.32005 -6.0704217 -6.9030848 -7.360045 -7.7989836 -8.236433 -8.4935684][-7.1215439 -8.1626453 -8.7408056 -8.43424 -7.6654072 -5.8814259 -3.3307114 -2.1055543 -2.8464108 -4.094224 -5.3416557 -6.1332245 -6.9040484 -7.7047243 -8.2521324][-7.2539167 -7.3586783 -7.4789915 -6.6024785 -5.4478035 -2.9594233 0.44604635 1.9331684 0.5676713 -1.685729 -3.4517515 -4.4766273 -5.5710669 -6.8370438 -7.6599522][-6.9494395 -6.5024595 -5.8803611 -4.189599 -2.6107633 0.37281585 4.3474226 6.0204539 4.1320496 0.88154387 -1.4288263 -2.5785241 -3.7306137 -5.3076687 -6.4041681][-6.3152795 -5.8800392 -4.6224108 -2.1496544 -0.15025139 2.9881363 6.8867278 8.3795013 6.084969 2.4508591 -0.13278651 -1.3459086 -2.4552064 -4.0808439 -5.2544279][-6.1330729 -5.3667879 -3.969697 -1.3577533 0.6241827 3.5940351 6.9637141 8.0730953 5.6866479 2.1361094 -0.494758 -1.6067019 -2.606158 -4.0054665 -5.0343857][-5.9608879 -5.1369185 -4.1789646 -2.3016121 -0.93432677 1.461817 4.2210321 5.217154 3.316431 0.37848544 -1.9165329 -2.7769797 -3.6074295 -4.7837515 -5.7539682][-6.2929068 -5.6169605 -5.3073864 -4.3682871 -3.5635078 -1.7749634 0.41860557 1.5139334 0.40724587 -1.5874901 -3.2956047 -3.9898338 -4.8409357 -5.7436123 -6.6400323][-6.8641906 -6.3805676 -6.529562 -6.2363544 -5.7399054 -4.3322105 -2.5867662 -1.5432985 -2.1221302 -3.3753779 -4.5768971 -5.2894926 -6.3688178 -7.1067944 -7.7682939][-7.2355881 -6.8982878 -7.2798386 -7.3267031 -6.9640131 -5.8225641 -4.4659338 -3.8271389 -4.4362111 -5.3040924 -6.0662174 -6.7025704 -7.6931777 -8.2146044 -8.542551][-7.3724103 -7.1212788 -7.5881815 -7.82725 -7.5975752 -6.7116551 -5.7230158 -5.4237432 -6.0246477 -6.5980611 -6.933785 -7.3214345 -7.9190331 -8.0952168 -8.2202406][-7.1162872 -6.8423395 -7.2714033 -7.5995593 -7.5702682 -6.97332 -6.268899 -6.0703082 -6.4417439 -6.7437811 -6.8833895 -7.1458769 -7.4389911 -7.3550506 -7.3344431][-6.6077104 -6.2484655 -6.5674853 -6.8468504 -6.9251513 -6.656775 -6.3012743 -6.2368312 -6.4365187 -6.5471344 -6.577703 -6.6596012 -6.6086817 -6.41504 -6.2916079]]...]
INFO - root - 2017-12-15 08:34:08.431485: step 39410, loss = 0.21, batch loss = 0.17 (33.6 examples/sec; 0.238 sec/batch; 19h:24m:18s remains)
INFO - root - 2017-12-15 08:34:10.688001: step 39420, loss = 0.25, batch loss = 0.22 (36.2 examples/sec; 0.221 sec/batch; 17h:58m:18s remains)
INFO - root - 2017-12-15 08:34:12.952644: step 39430, loss = 0.18, batch loss = 0.14 (35.7 examples/sec; 0.224 sec/batch; 18h:15m:38s remains)
INFO - root - 2017-12-15 08:34:15.250305: step 39440, loss = 0.20, batch loss = 0.17 (35.2 examples/sec; 0.227 sec/batch; 18h:29m:47s remains)
INFO - root - 2017-12-15 08:34:17.507076: step 39450, loss = 0.20, batch loss = 0.17 (35.6 examples/sec; 0.225 sec/batch; 18h:17m:03s remains)
INFO - root - 2017-12-15 08:34:19.785160: step 39460, loss = 0.15, batch loss = 0.12 (35.7 examples/sec; 0.224 sec/batch; 18h:13m:59s remains)
INFO - root - 2017-12-15 08:34:22.075796: step 39470, loss = 0.21, batch loss = 0.18 (34.6 examples/sec; 0.231 sec/batch; 18h:49m:51s remains)
INFO - root - 2017-12-15 08:34:24.365418: step 39480, loss = 0.24, batch loss = 0.20 (35.4 examples/sec; 0.226 sec/batch; 18h:24m:53s remains)
INFO - root - 2017-12-15 08:34:26.694204: step 39490, loss = 0.22, batch loss = 0.18 (34.0 examples/sec; 0.235 sec/batch; 19h:09m:29s remains)
INFO - root - 2017-12-15 08:34:28.970245: step 39500, loss = 0.21, batch loss = 0.18 (35.7 examples/sec; 0.224 sec/batch; 18h:15m:00s remains)
2017-12-15 08:34:29.261877: I tensorflow/core/kernels/logging_ops.cc:79] [[[2.0333383 1.191474 -0.093348742 -1.4741927 -3.2651339 -4.7993693 -5.761837 -5.7334785 -5.7557335 -6.5240679 -5.98573 -5.1943989 -5.322156 -4.7190919 -3.4351344][2.6681554 3.0708029 1.3522673 -0.25954187 -2.06035 -3.6356211 -4.6147118 -4.817749 -5.0836892 -5.9783249 -5.9228592 -5.6647635 -5.8677869 -5.2292509 -3.7903247][2.2576478 4.1438112 2.1469824 0.36529517 -1.1874593 -2.4585891 -3.2452264 -3.55997 -4.0257154 -5.1896715 -5.6550322 -6.1593871 -6.5642776 -5.9535007 -4.3206782][0.68028593 3.8930104 1.8369009 0.087533236 -1.0859582 -1.860755 -2.1077321 -2.2610958 -2.7593565 -4.1507864 -5.0705504 -6.222312 -6.9885845 -6.5569677 -4.8414021][-1.6183939 2.2870734 0.43002892 -0.96094871 -1.5041941 -1.6007248 -1.2033945 -1.0932778 -1.4386933 -2.8988609 -4.1504622 -5.7147264 -6.7986116 -6.6822987 -5.1037474][-3.2057848 0.50094032 -0.83662152 -1.7844752 -1.6227171 -0.98137259 -0.11429 0.20548534 -0.094590664 -1.6963099 -3.224791 -4.9810567 -6.2916632 -6.4814796 -5.1417952][-4.33834 -1.0901535 -1.7801321 -2.3028004 -1.5477769 -0.22669911 1.0895331 1.7091796 1.4244156 -0.43143129 -2.1878467 -3.9427443 -5.37904 -5.7854667 -4.7828722][-6.2064142 -2.9609156 -2.9881892 -2.9752676 -1.5843635 0.449651 2.2188661 3.05284 2.8769295 0.83390927 -1.154008 -2.782393 -4.16302 -4.6468124 -3.936595][-7.0549793 -4.2648282 -3.98884 -3.5979056 -1.7754726 0.77534389 2.8114049 3.9008133 3.8529851 1.6719759 -0.45370507 -2.0548618 -3.3750086 -3.8478382 -3.2837009][-6.8528337 -4.6945667 -4.3770204 -3.8669243 -2.0958748 0.36782193 2.3950365 3.6190498 3.5880501 1.2715154 -0.83014727 -2.2494674 -3.348608 -3.5314038 -2.9302063][-6.4836969 -4.8772812 -4.6651616 -4.1455879 -2.6496835 -0.5531702 1.2847466 2.3838732 2.3012969 0.058384418 -1.8450835 -3.002629 -3.7529197 -3.4974632 -2.7617159][-6.4705362 -5.121151 -4.9335513 -4.5030966 -3.3603568 -1.5856764 -0.035313368 0.96435165 0.95888877 -1.0033264 -2.7609239 -3.9262667 -4.4932356 -4.0401354 -3.3674984][-6.7276573 -5.532073 -5.51159 -5.3038988 -4.4266391 -2.9932725 -1.7707882 -0.92445314 -0.90583122 -2.5379667 -4.1583595 -5.30409 -5.7362671 -5.177021 -4.5321212][-6.6481762 -5.6823721 -5.9889245 -6.071485 -5.4867334 -4.421999 -3.4330389 -2.7279077 -2.7190225 -4.0512295 -5.4432192 -6.311347 -6.5096378 -5.9421697 -5.3764935][-6.349431 -5.5421753 -6.064919 -6.3482342 -5.9505048 -5.188632 -4.4588251 -3.9620352 -4.1417542 -5.3449459 -6.4911566 -7.0412035 -7.0401411 -6.535759 -6.0432329]]...]
INFO - root - 2017-12-15 08:34:31.554254: step 39510, loss = 0.18, batch loss = 0.14 (35.0 examples/sec; 0.228 sec/batch; 18h:35m:06s remains)
INFO - root - 2017-12-15 08:34:33.861559: step 39520, loss = 0.21, batch loss = 0.18 (34.7 examples/sec; 0.231 sec/batch; 18h:46m:17s remains)
INFO - root - 2017-12-15 08:34:36.136681: step 39530, loss = 0.28, batch loss = 0.25 (35.8 examples/sec; 0.223 sec/batch; 18h:11m:17s remains)
INFO - root - 2017-12-15 08:34:38.432693: step 39540, loss = 0.28, batch loss = 0.25 (32.7 examples/sec; 0.245 sec/batch; 19h:56m:20s remains)
INFO - root - 2017-12-15 08:34:40.727373: step 39550, loss = 0.26, batch loss = 0.23 (35.6 examples/sec; 0.225 sec/batch; 18h:17m:07s remains)
INFO - root - 2017-12-15 08:34:42.986602: step 39560, loss = 0.24, batch loss = 0.21 (36.2 examples/sec; 0.221 sec/batch; 18h:00m:27s remains)
INFO - root - 2017-12-15 08:34:45.265772: step 39570, loss = 0.24, batch loss = 0.21 (34.4 examples/sec; 0.232 sec/batch; 18h:54m:21s remains)
INFO - root - 2017-12-15 08:34:47.559361: step 39580, loss = 0.20, batch loss = 0.17 (34.5 examples/sec; 0.232 sec/batch; 18h:51m:39s remains)
INFO - root - 2017-12-15 08:34:49.843007: step 39590, loss = 0.16, batch loss = 0.13 (33.8 examples/sec; 0.236 sec/batch; 19h:14m:19s remains)
INFO - root - 2017-12-15 08:34:52.107672: step 39600, loss = 0.27, batch loss = 0.24 (34.7 examples/sec; 0.231 sec/batch; 18h:46m:25s remains)
2017-12-15 08:34:52.389650: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8815117 -4.4088163 -4.2997561 -4.5366282 -4.8151054 -4.7824011 -4.6279621 -4.8421078 -6.0519161 -6.6794519 -5.9138737 -4.5662284 -3.6771483 -3.3515766 -3.2488146][-3.6007357 -3.5485847 -3.4856589 -3.6725206 -3.9679832 -4.0225205 -4.027586 -4.1577077 -5.0380292 -5.3777027 -4.50043 -3.2638898 -2.5168979 -2.3529992 -2.3585327][-3.3990827 -2.9041529 -2.8085303 -2.9425697 -3.2348309 -3.2786827 -3.2271929 -3.2494836 -3.9886203 -4.3276281 -3.5507894 -2.4558842 -1.8249015 -1.694092 -1.7813425][-3.0729418 -2.2930324 -2.1001942 -2.2266302 -2.5201926 -2.4977856 -2.276998 -2.2023458 -3.0238333 -3.5215921 -2.9032936 -1.9855505 -1.4523621 -1.1846173 -1.0885167][-4.0288544 -2.6036148 -2.3035326 -2.3138824 -2.4585607 -2.0584598 -1.3547268 -1.0146074 -1.8917311 -2.7074265 -2.3893936 -1.6424487 -1.0581998 -0.51885331 -0.099097729][-4.9325094 -3.2909279 -2.8898244 -2.7073729 -2.4524579 -1.201498 0.44728947 1.2501543 0.28457165 -1.1160027 -1.3524994 -0.98084438 -0.58470345 0.046235323 0.58858728][-5.034225 -3.7202501 -3.2484729 -2.8842363 -2.1545563 -0.024109364 2.5768259 3.8253047 2.506109 0.30669975 -0.68598592 -0.91641319 -0.89240634 -0.29864061 0.33040071][-5.1620941 -3.991755 -3.5185826 -3.0406053 -2.0360012 0.55391788 3.6016929 5.0166435 3.4328506 0.73630118 -0.7990396 -1.5398996 -1.7797008 -1.2558401 -0.52013922][-4.9943733 -3.7069519 -3.2150793 -2.753449 -1.761228 0.76884532 3.6493976 4.9265757 3.3564646 0.64930463 -1.1352866 -2.21933 -2.6293063 -2.1790223 -1.4008353][-4.2589736 -2.7408724 -2.2480657 -1.9851941 -1.4136629 0.48076606 2.6131341 3.508569 1.9978092 -0.23262548 -1.7121418 -2.7994113 -3.1363127 -2.6840672 -1.8747787][-3.5850947 -1.8865373 -1.4378198 -1.4505799 -1.4287206 -0.39720094 0.86181259 1.3789608 0.10198116 -1.348752 -2.1971498 -2.9923711 -3.1713476 -2.6047659 -1.5732861][-3.1602318 -1.2709508 -0.83486319 -1.1318324 -1.59075 -1.2167714 -0.61611748 -0.45239365 -1.4851966 -2.1847887 -2.4006896 -2.8259392 -2.9192402 -2.3025496 -1.1519725][-3.3699903 -1.432205 -1.052968 -1.5981777 -2.3441279 -2.326149 -2.0829282 -2.1068754 -3.0250347 -3.16892 -2.8942845 -3.0952849 -3.2307839 -2.7411368 -1.7420728][-4.3964386 -2.6417644 -2.3769436 -2.9077151 -3.5647521 -3.5742488 -3.3924837 -3.559839 -4.5399103 -4.6488976 -4.3110857 -4.3320594 -4.3185816 -3.8013549 -2.9262629][-5.6763744 -4.209765 -4.0972652 -4.5686226 -4.9393578 -4.6370955 -4.2531104 -4.4861231 -5.5540271 -5.7998867 -5.5609488 -5.3585119 -5.1892939 -4.6451321 -3.9269762]]...]
INFO - root - 2017-12-15 08:34:54.685521: step 39610, loss = 0.26, batch loss = 0.23 (35.7 examples/sec; 0.224 sec/batch; 18h:12m:59s remains)
INFO - root - 2017-12-15 08:34:56.950028: step 39620, loss = 0.29, batch loss = 0.26 (35.9 examples/sec; 0.223 sec/batch; 18h:08m:57s remains)
INFO - root - 2017-12-15 08:34:59.202580: step 39630, loss = 0.21, batch loss = 0.18 (34.4 examples/sec; 0.233 sec/batch; 18h:55m:46s remains)
INFO - root - 2017-12-15 08:35:01.454749: step 39640, loss = 0.27, batch loss = 0.24 (35.8 examples/sec; 0.223 sec/batch; 18h:10m:32s remains)
INFO - root - 2017-12-15 08:35:03.721872: step 39650, loss = 0.22, batch loss = 0.18 (35.8 examples/sec; 0.224 sec/batch; 18h:11m:15s remains)
INFO - root - 2017-12-15 08:35:06.006236: step 39660, loss = 0.30, batch loss = 0.27 (35.8 examples/sec; 0.224 sec/batch; 18h:11m:45s remains)
INFO - root - 2017-12-15 08:35:08.267016: step 39670, loss = 0.32, batch loss = 0.29 (36.4 examples/sec; 0.220 sec/batch; 17h:51m:56s remains)
INFO - root - 2017-12-15 08:35:10.569700: step 39680, loss = 0.22, batch loss = 0.19 (34.9 examples/sec; 0.229 sec/batch; 18h:39m:54s remains)
INFO - root - 2017-12-15 08:35:12.835678: step 39690, loss = 0.33, batch loss = 0.30 (34.3 examples/sec; 0.233 sec/batch; 18h:58m:13s remains)
INFO - root - 2017-12-15 08:35:15.120752: step 39700, loss = 0.21, batch loss = 0.18 (36.6 examples/sec; 0.219 sec/batch; 17h:47m:04s remains)
2017-12-15 08:35:15.421713: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.7033968 -7.8596487 -7.25233 -6.6210823 -6.451169 -6.6383514 -6.7625227 -6.5968647 -6.4489555 -6.6797123 -7.1694736 -7.3857918 -7.3382177 -7.1508007 -7.1461935][-8.65509 -8.55448 -7.77481 -6.9262371 -6.6958971 -6.9469204 -7.1086988 -6.8456516 -6.59896 -6.8172159 -7.4095449 -7.6525564 -7.5340977 -7.3651171 -7.4262877][-9.1042032 -7.9785538 -7.0994635 -6.3161192 -6.1347256 -6.1573839 -5.9307818 -5.4747696 -5.24951 -5.6401596 -6.5245123 -7.0684118 -7.0684681 -7.0071182 -7.1784811][-8.8664484 -6.8996305 -5.955719 -5.2012959 -4.9499416 -4.7338967 -3.9597769 -3.0683889 -2.8467214 -3.6148438 -5.0919104 -6.1629596 -6.376627 -6.4953756 -6.9007883][-8.311079 -5.5443697 -4.7124777 -4.07005 -3.6964097 -3.1208014 -1.59831 0.09390831 0.6172576 -0.3383323 -2.4694703 -4.2705083 -4.8493867 -5.3513842 -6.2959604][-7.3746328 -4.1884513 -3.4074969 -2.6993651 -2.062654 -1.048216 1.1599422 3.4841192 4.2723923 3.1894305 0.64888525 -1.719847 -2.8217151 -4.1049843 -5.7940063][-6.5715609 -3.8380237 -2.8983359 -1.8416625 -0.74023068 0.70908737 3.2285826 5.7691984 6.5311947 5.2585869 2.5691435 0.014015913 -1.3710996 -3.13833 -5.3822479][-7.951334 -4.8291926 -3.4864984 -1.9484457 -0.54054725 0.86189556 3.0319645 5.3606987 5.9947395 4.5028534 1.7692568 -0.62931836 -1.9977896 -3.8331213 -6.1858997][-9.9928617 -6.59886 -4.7321572 -2.760488 -1.1221787 0.080718517 1.4534085 2.970104 3.1702178 1.7052684 -0.53069019 -2.5382719 -3.8820443 -5.5979042 -7.6434317][-11.72948 -8.27199 -6.0752773 -3.7020721 -1.8597753 -0.70685565 -0.0045084953 0.70777464 0.58665204 -0.74718785 -2.483453 -4.1379952 -5.6376715 -7.1475792 -8.670126][-12.910279 -9.72431 -7.6250906 -5.2334251 -3.1932218 -1.8123643 -1.46377 -1.1224643 -1.4105757 -2.6009686 -4.0118594 -5.4761581 -7.0928764 -8.2776556 -9.2326651][-13.300912 -10.430312 -8.6723661 -6.3601885 -4.1536837 -2.6427238 -2.4669087 -2.3623576 -2.6306572 -3.7462463 -5.010932 -6.3887873 -8.0295486 -8.840786 -9.2551775][-12.669343 -10.065581 -8.6733856 -6.6007929 -4.3915539 -2.8815451 -2.7934752 -2.6284575 -2.7562804 -3.6774595 -4.9392967 -6.4216251 -8.0903645 -8.6938591 -8.7448969][-11.892288 -9.5264463 -8.5132179 -6.8311338 -4.9479837 -3.5746284 -3.3526039 -2.9583893 -2.8334384 -3.5729651 -4.8510027 -6.3355503 -7.8903284 -8.4410343 -8.5031662][-10.344758 -8.313406 -7.612031 -6.3202085 -4.8353496 -3.87611 -3.7646382 -3.3588061 -3.1088634 -3.6103187 -4.7449846 -6.070653 -7.3498764 -7.9451962 -8.1173058]]...]
INFO - root - 2017-12-15 08:35:17.684350: step 39710, loss = 0.20, batch loss = 0.16 (35.5 examples/sec; 0.225 sec/batch; 18h:18m:24s remains)
INFO - root - 2017-12-15 08:35:19.952291: step 39720, loss = 0.28, batch loss = 0.25 (35.0 examples/sec; 0.228 sec/batch; 18h:34m:19s remains)
INFO - root - 2017-12-15 08:35:22.205008: step 39730, loss = 0.20, batch loss = 0.17 (35.5 examples/sec; 0.226 sec/batch; 18h:20m:23s remains)
INFO - root - 2017-12-15 08:35:24.473206: step 39740, loss = 0.24, batch loss = 0.21 (34.5 examples/sec; 0.232 sec/batch; 18h:50m:43s remains)
INFO - root - 2017-12-15 08:35:26.765932: step 39750, loss = 0.35, batch loss = 0.32 (34.5 examples/sec; 0.232 sec/batch; 18h:52m:48s remains)
INFO - root - 2017-12-15 08:35:29.071288: step 39760, loss = 0.19, batch loss = 0.16 (34.7 examples/sec; 0.231 sec/batch; 18h:46m:07s remains)
INFO - root - 2017-12-15 08:35:31.336331: step 39770, loss = 0.36, batch loss = 0.32 (35.6 examples/sec; 0.225 sec/batch; 18h:16m:38s remains)
INFO - root - 2017-12-15 08:35:33.630270: step 39780, loss = 0.35, batch loss = 0.32 (35.7 examples/sec; 0.224 sec/batch; 18h:13m:39s remains)
INFO - root - 2017-12-15 08:35:35.900015: step 39790, loss = 0.25, batch loss = 0.22 (35.0 examples/sec; 0.229 sec/batch; 18h:35m:52s remains)
INFO - root - 2017-12-15 08:35:38.169233: step 39800, loss = 0.25, batch loss = 0.22 (33.8 examples/sec; 0.237 sec/batch; 19h:14m:01s remains)
2017-12-15 08:35:38.454860: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6760416 -8.2336845 -8.9136162 -9.5192175 -9.906786 -9.5452385 -9.2039757 -9.043457 -8.6332607 -7.7502713 -7.3739905 -7.7375474 -7.7324195 -8.0087395 -8.3455858][-5.0592051 -7.8306618 -8.7663479 -9.5835247 -10.074396 -9.9459953 -9.9015141 -9.7773161 -9.1143188 -8.0245924 -7.6262584 -7.9490967 -7.8726692 -8.1474037 -8.43297][-4.9743443 -6.64853 -7.6154733 -8.5002346 -8.96764 -8.7629128 -8.5332584 -8.0281715 -7.0648479 -6.4510121 -6.7056704 -7.2331405 -7.2135925 -7.4347715 -7.5214729][-5.2150497 -5.9556255 -6.8821812 -7.7132235 -7.7528553 -6.9260492 -6.0592184 -4.8825455 -3.8113523 -4.0962291 -5.2158709 -6.2271676 -6.6250825 -7.0853319 -7.0559664][-6.2674541 -6.0477848 -6.8484926 -7.4349165 -6.5993347 -4.8151093 -3.2060065 -1.3031371 -0.1541357 -1.4189022 -3.5886881 -5.49963 -6.5461631 -7.2695079 -7.1438107][-6.9166822 -6.1509738 -6.756083 -6.986228 -5.2851124 -2.7394452 -0.50861514 2.1365504 3.45887 1.3035812 -1.914701 -4.9532137 -6.9565105 -7.9417958 -7.7183547][-6.9105225 -6.0962658 -6.539084 -6.449245 -4.119616 -1.1476332 1.5694849 4.9382277 6.4247851 3.7209344 -0.077284575 -4.0286484 -6.9382868 -8.2317066 -8.1567631][-6.8110805 -5.7842708 -6.1559715 -5.9267435 -3.3750992 -0.32421863 2.8086114 6.8342032 8.40373 5.3415475 1.1628635 -3.3897398 -6.8042817 -8.2897243 -8.2110329][-6.3222942 -5.2385721 -5.7030611 -5.6367474 -3.469779 -0.84120715 2.3287468 6.385838 7.8341079 4.7582831 0.62340069 -3.7457013 -7.0878644 -8.333293 -7.9331126][-5.9181347 -5.0705996 -5.8866267 -6.2776 -4.7244148 -2.6902151 0.042721987 3.4976454 4.7432723 2.1903229 -1.2532395 -4.9097853 -7.6953297 -8.3708534 -7.5373788][-5.8423643 -5.4159102 -6.722086 -7.596909 -6.6437988 -5.2894807 -3.3485076 -0.56130314 0.74090171 -0.96002948 -3.5702472 -6.4432721 -8.4004 -8.3779945 -7.0576925][-5.8734097 -5.7428122 -7.3499966 -8.5024958 -8.0854836 -7.448051 -6.3791986 -4.3615389 -3.0225415 -3.8466821 -5.6239185 -7.5872359 -8.515007 -7.7393188 -6.0929861][-6.2973123 -6.1401434 -7.4816103 -8.3548317 -8.2255669 -8.1953793 -7.8550887 -6.611659 -5.5308461 -5.7340384 -6.7531958 -7.7834406 -7.7510262 -6.5082731 -5.0325904][-7.22894 -6.5501041 -7.1078134 -7.3426023 -7.39182 -7.9798651 -8.3449659 -7.82828 -7.1341834 -7.0664716 -7.56459 -7.9697123 -7.3244047 -6.0241933 -4.7931023][-8.0260124 -6.5777125 -6.1933308 -5.8178725 -6.1369743 -7.1981697 -8.0226192 -8.0731888 -7.8782349 -7.9603658 -8.2537937 -8.2311115 -7.2864184 -6.0529642 -4.9204021]]...]
INFO - root - 2017-12-15 08:35:40.766537: step 39810, loss = 0.19, batch loss = 0.16 (34.1 examples/sec; 0.235 sec/batch; 19h:04m:31s remains)
INFO - root - 2017-12-15 08:35:43.040106: step 39820, loss = 0.18, batch loss = 0.15 (34.5 examples/sec; 0.232 sec/batch; 18h:52m:40s remains)
INFO - root - 2017-12-15 08:35:45.294568: step 39830, loss = 0.22, batch loss = 0.19 (35.5 examples/sec; 0.226 sec/batch; 18h:20m:04s remains)
INFO - root - 2017-12-15 08:35:47.595326: step 39840, loss = 0.27, batch loss = 0.24 (35.8 examples/sec; 0.224 sec/batch; 18h:10m:53s remains)
INFO - root - 2017-12-15 08:35:49.896939: step 39850, loss = 0.18, batch loss = 0.15 (34.3 examples/sec; 0.233 sec/batch; 18h:56m:16s remains)
INFO - root - 2017-12-15 08:35:52.160487: step 39860, loss = 0.17, batch loss = 0.14 (36.6 examples/sec; 0.219 sec/batch; 17h:47m:27s remains)
INFO - root - 2017-12-15 08:35:54.450734: step 39870, loss = 0.26, batch loss = 0.22 (34.5 examples/sec; 0.232 sec/batch; 18h:50m:43s remains)
INFO - root - 2017-12-15 08:35:56.759939: step 39880, loss = 0.21, batch loss = 0.18 (36.2 examples/sec; 0.221 sec/batch; 17h:57m:40s remains)
INFO - root - 2017-12-15 08:35:59.008338: step 39890, loss = 0.25, batch loss = 0.21 (35.5 examples/sec; 0.225 sec/batch; 18h:18m:32s remains)
INFO - root - 2017-12-15 08:36:01.269520: step 39900, loss = 0.25, batch loss = 0.22 (36.1 examples/sec; 0.222 sec/batch; 18h:01m:59s remains)
2017-12-15 08:36:01.591769: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4279461 -6.1026669 -7.1267838 -8.4684048 -8.1862135 -7.7237678 -6.9591889 -6.1901369 -5.814579 -5.8638973 -6.5448 -8.2769289 -8.1148 -7.2716203 -7.2529531][-4.8049631 -5.66003 -7.0336542 -8.5861816 -8.3952732 -8.1070614 -7.278934 -6.3480906 -5.970674 -6.2224245 -7.1012516 -8.9419594 -8.6289425 -7.5734076 -7.3616815][-4.826911 -4.3282423 -5.8774772 -7.4277425 -7.4654675 -7.3944335 -6.5832033 -5.5227041 -5.1741667 -5.7339005 -6.8666048 -8.6783352 -8.550046 -7.7838812 -7.615582][-4.4993448 -2.9694288 -4.4974804 -5.8322306 -5.9156265 -5.8617859 -5.1108694 -4.0523562 -3.9103241 -4.9438009 -6.3700972 -7.9805832 -8.1208172 -7.7681985 -7.7367926][-4.411272 -2.014287 -3.5356305 -4.6026039 -4.5757408 -4.18237 -3.1348431 -1.9876851 -2.1614165 -3.7522359 -5.4552307 -6.9840961 -7.4563236 -7.5406528 -7.6726642][-4.5666022 -2.1166756 -3.3055077 -3.9270051 -3.3542547 -1.9354894 -0.026169777 1.3573053 0.795295 -1.5862615 -3.8006229 -5.7258458 -6.7595763 -7.4744573 -7.8848615][-5.4015913 -2.7965856 -3.2284675 -3.2199175 -1.6970282 0.96220088 3.7978158 5.4938617 4.53726 1.2112648 -1.840981 -4.6063538 -6.4495487 -7.6438937 -8.19407][-6.73667 -3.39602 -3.3110394 -2.6680958 -0.19117641 3.3163528 6.6186562 8.3931723 7.0895329 3.0093951 -0.62778461 -4.0425754 -6.3079319 -7.6117258 -8.0429087][-7.5161972 -4.2195621 -4.0825491 -3.2798908 -0.2676506 3.5861535 6.7102661 8.3340349 7.0831432 2.8384318 -1.0595449 -4.6876431 -6.7766266 -7.8125043 -7.9084797][-7.98752 -5.2351751 -5.3782129 -4.926362 -2.0105135 1.4461291 3.8511558 5.3413253 4.5897131 0.944402 -2.7578731 -6.363245 -8.0866871 -8.6774178 -8.40036][-7.8872466 -5.5390844 -6.2105565 -6.682487 -4.6440668 -2.0624533 -0.36165345 1.0258853 0.87979841 -1.7368815 -4.7890635 -8.1587439 -9.4025345 -9.5380459 -9.0439224][-7.8247347 -5.1917152 -6.1257663 -7.6656771 -6.7458606 -5.1921368 -4.0663614 -2.8596025 -2.5407691 -4.1491709 -6.6236811 -9.7294922 -10.504594 -10.191416 -9.5323658][-8.3156738 -5.0821629 -6.0321217 -8.186265 -8.088666 -7.3266144 -6.5111046 -5.4858656 -5.1229944 -6.07578 -8.0793791 -10.870837 -11.359442 -10.550467 -9.7337265][-8.4167519 -4.7588363 -5.9637218 -8.3176613 -8.71195 -8.4448872 -7.6703548 -6.7515903 -6.5183158 -7.2745848 -8.9052124 -11.096761 -11.369259 -10.353168 -9.4826365][-8.0361176 -4.4302645 -5.5888343 -7.6411738 -8.2149715 -8.2459869 -7.6272707 -6.925046 -6.8726625 -7.5650783 -8.882803 -10.380527 -10.534531 -9.7484283 -8.9801407]]...]
INFO - root - 2017-12-15 08:36:03.866288: step 39910, loss = 0.22, batch loss = 0.18 (34.3 examples/sec; 0.233 sec/batch; 18h:57m:30s remains)
INFO - root - 2017-12-15 08:36:06.145909: step 39920, loss = 0.26, batch loss = 0.23 (35.9 examples/sec; 0.223 sec/batch; 18h:06m:33s remains)
INFO - root - 2017-12-15 08:36:08.416516: step 39930, loss = 0.21, batch loss = 0.17 (32.5 examples/sec; 0.246 sec/batch; 20h:00m:51s remains)
INFO - root - 2017-12-15 08:36:10.699777: step 39940, loss = 0.25, batch loss = 0.22 (35.5 examples/sec; 0.225 sec/batch; 18h:18m:11s remains)
INFO - root - 2017-12-15 08:36:13.002243: step 39950, loss = 0.31, batch loss = 0.28 (35.0 examples/sec; 0.229 sec/batch; 18h:34m:55s remains)
INFO - root - 2017-12-15 08:36:15.307180: step 39960, loss = 0.19, batch loss = 0.16 (32.2 examples/sec; 0.248 sec/batch; 20h:10m:25s remains)
INFO - root - 2017-12-15 08:36:17.565530: step 39970, loss = 0.24, batch loss = 0.20 (35.8 examples/sec; 0.224 sec/batch; 18h:10m:44s remains)
INFO - root - 2017-12-15 08:36:19.859484: step 39980, loss = 0.22, batch loss = 0.18 (33.7 examples/sec; 0.237 sec/batch; 19h:16m:30s remains)
INFO - root - 2017-12-15 08:36:22.172800: step 39990, loss = 0.25, batch loss = 0.22 (35.0 examples/sec; 0.229 sec/batch; 18h:34m:24s remains)
INFO - root - 2017-12-15 08:36:24.473577: step 40000, loss = 0.21, batch loss = 0.18 (33.2 examples/sec; 0.241 sec/batch; 19h:36m:03s remains)
2017-12-15 08:36:24.751053: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.89803863 -0.069126129 0.25873327 0.072479248 -0.0092144012 -0.20827341 -0.47069418 -0.96523583 -1.6720755 -1.9617239 -1.5235275 -0.97086942 -0.74672616 -0.56174564 -0.25025439][-0.96671271 0.29848242 0.55408716 0.46780038 0.40736175 0.19167018 -0.16436076 -0.70066035 -1.3073468 -1.6243193 -1.3178362 -0.86907709 -0.61967981 -0.3831023 -0.078923464][-1.055966 0.51778579 0.67907643 0.64043 0.59054947 0.4235599 0.10604429 -0.34893835 -0.78694844 -1.1062539 -1.0098987 -0.75868762 -0.56357777 -0.29477262 0.095103979][-1.5015132 0.36281204 0.53322029 0.66341686 0.75854135 0.73738456 0.53159642 0.18592024 -0.1670208 -0.56638241 -0.75120747 -0.78468752 -0.66257536 -0.31869292 0.23126149][-2.1959608 -0.080686331 0.12558484 0.51887631 0.94768643 1.1753318 1.1162474 0.82515025 0.43883038 -0.14796948 -0.70690286 -1.1360967 -1.1334194 -0.67504 0.069676876][-3.0551834 -0.89920628 -0.610152 0.16191483 1.0860806 1.6909637 1.8266556 1.5916936 1.0509837 0.18805933 -0.74936712 -1.636381 -1.7935388 -1.3173523 -0.4751718][-3.4519649 -1.6594813 -1.1883767 -0.061393738 1.2969739 2.1705496 2.3800542 2.1214435 1.4352901 0.36095405 -0.84508932 -2.0455532 -2.4375734 -2.1473331 -1.4110645][-3.599369 -2.0018649 -1.49125 -0.32428849 0.99741888 1.8173058 2.0116756 1.8949425 1.3882782 0.41939783 -0.87940407 -2.3280003 -2.9538865 -2.9361229 -2.3896377][-3.6173787 -2.0778813 -1.6374252 -0.76059544 0.16914368 0.87579489 1.179435 1.3956037 1.3597336 0.7185235 -0.53907 -2.1407168 -3.0106454 -3.2903626 -2.9202394][-3.5187001 -2.0151331 -1.7310562 -1.1823663 -0.67202854 -0.024688482 0.4136095 0.93684173 1.2430971 0.88759017 -0.22494197 -1.8451412 -2.826462 -3.3639667 -3.1551769][-3.4318411 -1.9473376 -1.8199352 -1.5633433 -1.3660774 -0.76041818 -0.24834657 0.46388865 1.0422542 0.96790552 0.060054541 -1.4223797 -2.3991346 -3.1134853 -3.0352345][-3.3932264 -1.7885969 -1.7436254 -1.6087103 -1.626956 -1.1291786 -0.66914511 0.18467903 1.0229754 1.2106411 0.54227638 -0.72431469 -1.6375427 -2.4773254 -2.5068421][-3.2236362 -1.5124606 -1.5102532 -1.5167394 -1.7496035 -1.4302216 -1.1065238 -0.19724727 0.88058949 1.285373 0.84043837 -0.19784927 -1.0499543 -1.8788471 -1.8895888][-3.007246 -1.3271174 -1.3771908 -1.4693861 -1.8215559 -1.633414 -1.4182779 -0.50151241 0.67931485 1.1883037 0.90016317 0.047697783 -0.77706158 -1.5525589 -1.5715594][-2.7686527 -1.2730372 -1.4972818 -1.7688677 -2.1843491 -2.0100586 -1.6728492 -0.76483643 0.47439671 0.99476552 0.80920434 0.20769954 -0.58537471 -1.3459466 -1.5064173]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 08:36:27.476432: step 40010, loss = 0.22, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 18h:07m:31s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 08:36:29.749927: step 40020, loss = 0.30, batch loss = 0.27 (35.2 examples/sec; 0.227 sec/batch; 18h:26m:23s remains)
INFO - root - 2017-12-15 08:36:32.003631: step 40030, loss = 0.36, batch loss = 0.33 (35.5 examples/sec; 0.225 sec/batch; 18h:18m:16s remains)
INFO - root - 2017-12-15 08:36:34.271174: step 40040, loss = 0.18, batch loss = 0.15 (36.0 examples/sec; 0.222 sec/batch; 18h:03m:29s remains)
INFO - root - 2017-12-15 08:36:36.551708: step 40050, loss = 0.25, batch loss = 0.22 (34.5 examples/sec; 0.232 sec/batch; 18h:50m:11s remains)
INFO - root - 2017-12-15 08:36:38.863405: step 40060, loss = 0.24, batch loss = 0.21 (34.9 examples/sec; 0.229 sec/batch; 18h:36m:28s remains)
INFO - root - 2017-12-15 08:36:41.156582: step 40070, loss = 0.17, batch loss = 0.14 (32.6 examples/sec; 0.246 sec/batch; 19h:57m:06s remains)
INFO - root - 2017-12-15 08:36:43.415722: step 40080, loss = 0.22, batch loss = 0.19 (36.1 examples/sec; 0.222 sec/batch; 18h:00m:45s remains)
INFO - root - 2017-12-15 08:36:45.672157: step 40090, loss = 0.22, batch loss = 0.19 (34.9 examples/sec; 0.229 sec/batch; 18h:37m:09s remains)
INFO - root - 2017-12-15 08:36:47.968526: step 40100, loss = 0.22, batch loss = 0.19 (32.9 examples/sec; 0.243 sec/batch; 19h:43m:23s remains)
2017-12-15 08:36:48.283538: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.7538569 -0.83915472 -1.3858094 -1.7447213 -1.9474969 -1.2901138 -0.051337004 0.3229847 -0.31312656 -1.8947397 -3.8929019 -4.8272486 -4.7852864 -3.6565104 -2.1369796][1.1235986 -0.14850664 -0.91132855 -1.5639621 -1.9693568 -1.487962 -0.3051337 -0.15612936 -0.825215 -2.2548754 -3.9734204 -4.7724752 -4.6948977 -3.5490277 -1.9547423][0.82213306 0.250849 -0.68485463 -1.710804 -2.3546495 -2.1962538 -1.3529289 -1.4158213 -2.1042194 -3.2841673 -4.561892 -4.9716024 -4.5403385 -3.1889608 -1.6706932][-0.032824755 -0.12947822 -1.1123618 -2.0632703 -2.4235289 -2.2969847 -1.7250372 -2.0789139 -2.8583694 -3.9435959 -4.9500074 -5.00887 -4.2297759 -2.804 -1.4534676][-1.6797923 -1.0578446 -1.6904061 -2.157069 -2.025959 -1.4888048 -0.78458464 -1.1677948 -2.2094364 -3.5773199 -4.68581 -4.7277241 -3.9526248 -2.6605847 -1.3700068][-2.5451326 -1.7570751 -1.9805011 -1.8375536 -0.89885271 0.48940039 1.7168086 1.440449 -0.22092247 -2.3294802 -4.1877527 -4.6619406 -3.9680266 -2.7615309 -1.5340581][-2.6901715 -2.1226254 -2.250937 -1.70655 -0.0049340725 2.3847363 4.4795666 4.5084648 2.2369621 -0.75237465 -3.4511716 -4.536056 -4.2108359 -3.2477245 -2.200624][-2.7945607 -2.3051791 -2.5977952 -2.1453109 -0.3456881 2.5703461 5.4955873 6.2624369 4.1649265 0.92716312 -2.0802982 -3.6243606 -3.7721863 -3.205487 -2.4540789][-2.7033768 -2.3796992 -3.0005708 -3.0371644 -1.7183331 0.98147511 4.1073065 5.4709091 4.079814 1.3576376 -1.3487551 -2.8510413 -3.1574996 -2.8204865 -2.3992651][-2.4963262 -2.1562185 -3.0418191 -3.6470542 -3.1020215 -1.0680258 1.5617099 3.0273435 2.2785437 0.48630023 -1.4080015 -2.3480096 -2.596427 -2.3807931 -2.224438][-2.1725812 -1.6996512 -2.5549381 -3.4831352 -3.5584064 -2.2146223 -0.36815441 0.71427536 0.30096221 -0.72776115 -1.7758932 -2.051441 -1.9035271 -1.5452609 -1.3285501][-2.0343702 -1.2807037 -1.8281045 -2.6711183 -3.043925 -2.3181467 -1.2975113 -0.82691717 -1.1382712 -1.7350799 -2.2877378 -2.0586519 -1.42795 -0.83575237 -0.56752777][-2.75154 -1.6884181 -1.7877164 -2.332037 -2.6705837 -2.2258554 -1.6698315 -1.6821399 -2.0768158 -2.6152842 -3.0454865 -2.6472051 -1.7017274 -1.0396702 -0.85807836][-3.398788 -2.2889261 -2.192359 -2.6310108 -2.7991648 -2.3271644 -1.8690829 -1.9221175 -2.1952951 -2.7218661 -3.2306507 -2.9256756 -1.8687781 -1.327637 -1.2532324][-3.8707347 -2.8534842 -2.6331539 -2.9304533 -2.9315443 -2.6533287 -2.4317641 -2.4664881 -2.411936 -2.8027203 -3.3391461 -3.1540756 -2.2684903 -2.0951357 -2.1528182]]...]
INFO - root - 2017-12-15 08:36:50.541780: step 40110, loss = 0.30, batch loss = 0.27 (35.4 examples/sec; 0.226 sec/batch; 18h:21m:47s remains)
INFO - root - 2017-12-15 08:36:52.778078: step 40120, loss = 0.22, batch loss = 0.19 (36.0 examples/sec; 0.222 sec/batch; 18h:03m:32s remains)
INFO - root - 2017-12-15 08:36:55.104720: step 40130, loss = 0.26, batch loss = 0.23 (33.0 examples/sec; 0.243 sec/batch; 19h:41m:41s remains)
INFO - root - 2017-12-15 08:36:57.367450: step 40140, loss = 0.18, batch loss = 0.15 (35.5 examples/sec; 0.225 sec/batch; 18h:18m:22s remains)
INFO - root - 2017-12-15 08:36:59.633964: step 40150, loss = 0.26, batch loss = 0.23 (35.7 examples/sec; 0.224 sec/batch; 18h:11m:54s remains)
INFO - root - 2017-12-15 08:37:01.895758: step 40160, loss = 0.22, batch loss = 0.19 (35.3 examples/sec; 0.227 sec/batch; 18h:24m:12s remains)
INFO - root - 2017-12-15 08:37:04.173848: step 40170, loss = 0.27, batch loss = 0.23 (36.0 examples/sec; 0.222 sec/batch; 18h:02m:30s remains)
INFO - root - 2017-12-15 08:37:06.429318: step 40180, loss = 0.18, batch loss = 0.15 (35.8 examples/sec; 0.223 sec/batch; 18h:08m:45s remains)
INFO - root - 2017-12-15 08:37:08.745909: step 40190, loss = 0.20, batch loss = 0.17 (34.1 examples/sec; 0.234 sec/batch; 19h:01m:51s remains)
INFO - root - 2017-12-15 08:37:11.021755: step 40200, loss = 0.26, batch loss = 0.23 (34.6 examples/sec; 0.231 sec/batch; 18h:44m:48s remains)
2017-12-15 08:37:11.323619: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8398885 -4.9009609 -5.8726273 -6.7337532 -6.9109392 -6.5761833 -6.5659714 -6.8665075 -6.9710379 -6.7354031 -6.685174 -6.2788191 -5.4694662 -4.9995842 -3.99766][-3.605211 -6.03092 -7.1903191 -7.7615471 -7.3402472 -6.8195467 -6.8254642 -6.9920063 -6.9249411 -6.9444818 -7.397687 -6.8275518 -5.63691 -5.078783 -4.1084247][-5.2723222 -6.9003849 -8.0942392 -8.11276 -7.19573 -6.54908 -6.3091259 -6.1851559 -6.1471882 -6.428297 -7.1062565 -6.6006756 -5.46842 -4.9690685 -4.0055451][-6.6751432 -7.5254173 -8.58466 -7.9388638 -6.4773808 -5.459157 -4.573596 -4.1068048 -4.33545 -5.0427771 -5.9519358 -5.9885921 -5.5633206 -5.2734823 -4.3299665][-7.5308695 -7.5179491 -8.4400072 -7.256938 -5.32454 -3.7931204 -2.1529002 -1.3824968 -1.9464343 -3.2489643 -4.5409031 -5.1904416 -5.5701904 -5.5419111 -4.6618915][-7.3259134 -7.025631 -7.8152056 -6.2083635 -3.8348403 -1.8382442 0.36788988 1.3549652 0.40815806 -1.6179831 -3.3140852 -4.3682747 -5.2059488 -5.3244209 -4.5497737][-6.5813208 -6.5292425 -7.1309633 -5.1336241 -2.2643011 0.21710086 2.7054474 3.6695859 2.2614291 -0.40514505 -2.5220656 -3.8943775 -5.0157337 -5.3545966 -4.7429333][-6.5667267 -6.2215142 -6.5581141 -4.20623 -0.89880037 1.9197428 4.4539318 5.2466211 3.4229515 0.35739923 -2.2244642 -4.0741506 -5.2964439 -5.8156004 -5.3669105][-6.6224842 -6.12897 -6.2221856 -3.7942576 -0.40733123 2.430218 4.6998043 5.1517105 3.1079028 0.055179119 -2.6869726 -4.6927786 -5.6527662 -6.1033592 -5.6402912][-6.7707052 -6.2095251 -6.1742115 -4.0193162 -1.0191362 1.3954389 3.1011279 3.2466185 1.3114164 -1.1986799 -3.4632246 -5.0145035 -5.5078554 -5.6797743 -5.2417507][-6.9960308 -6.4329071 -6.3054495 -4.5058432 -2.0755546 -0.27552605 0.76361728 0.61977506 -0.91830242 -2.3778329 -3.5624812 -4.36043 -4.3855658 -4.434411 -4.3197236][-7.1719151 -6.6303949 -6.3853607 -4.8650966 -2.9617839 -1.6841719 -1.1455519 -1.3847194 -2.1794374 -2.160815 -1.9837739 -2.0626853 -1.8701946 -2.0699391 -2.4564562][-7.2317247 -6.7399836 -6.415329 -5.2060585 -3.8351555 -2.9144976 -2.5477283 -2.6209788 -2.5440886 -1.0796394 0.29369116 0.83054447 1.0735848 0.83349276 -0.055933714][-7.3282585 -6.9061089 -6.5791054 -5.6629472 -4.6960278 -3.9749124 -3.5498133 -3.3267045 -2.4613056 0.049702168 2.1310532 2.8731182 3.1400616 3.0562932 2.0912454][-7.5342941 -7.2104797 -6.9032755 -6.0663023 -5.072041 -4.1732774 -3.5207887 -3.1860306 -2.1328402 0.66478848 2.754813 3.3653953 3.6338966 3.8548596 3.2649782]]...]
INFO - root - 2017-12-15 08:37:13.571601: step 40210, loss = 0.25, batch loss = 0.22 (35.3 examples/sec; 0.227 sec/batch; 18h:23m:49s remains)
INFO - root - 2017-12-15 08:37:15.872800: step 40220, loss = 0.29, batch loss = 0.25 (33.8 examples/sec; 0.237 sec/batch; 19h:14m:29s remains)
INFO - root - 2017-12-15 08:37:18.136447: step 40230, loss = 0.27, batch loss = 0.23 (35.9 examples/sec; 0.223 sec/batch; 18h:04m:05s remains)
INFO - root - 2017-12-15 08:37:20.431389: step 40240, loss = 0.19, batch loss = 0.16 (35.9 examples/sec; 0.223 sec/batch; 18h:06m:18s remains)
INFO - root - 2017-12-15 08:37:22.723697: step 40250, loss = 0.33, batch loss = 0.29 (36.1 examples/sec; 0.222 sec/batch; 18h:00m:53s remains)
INFO - root - 2017-12-15 08:37:25.053602: step 40260, loss = 0.22, batch loss = 0.18 (34.8 examples/sec; 0.230 sec/batch; 18h:41m:07s remains)
INFO - root - 2017-12-15 08:37:27.324351: step 40270, loss = 0.26, batch loss = 0.23 (34.2 examples/sec; 0.234 sec/batch; 19h:00m:20s remains)
INFO - root - 2017-12-15 08:37:29.597253: step 40280, loss = 0.28, batch loss = 0.25 (34.8 examples/sec; 0.230 sec/batch; 18h:39m:48s remains)
INFO - root - 2017-12-15 08:37:31.887089: step 40290, loss = 0.26, batch loss = 0.23 (35.2 examples/sec; 0.227 sec/batch; 18h:25m:24s remains)
INFO - root - 2017-12-15 08:37:34.201159: step 40300, loss = 0.27, batch loss = 0.24 (35.8 examples/sec; 0.223 sec/batch; 18h:07m:43s remains)
2017-12-15 08:37:34.501120: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9667358 -6.1486359 -6.1502743 -6.4466429 -6.9652061 -7.6198635 -7.9703979 -7.8990822 -7.7128544 -7.3517 -7.33348 -7.4114523 -7.3593292 -7.4066668 -7.2483435][-5.551651 -6.3299685 -6.5061293 -6.9834986 -7.6511831 -8.346344 -8.47726 -7.988884 -7.4141445 -6.8636131 -7.0600348 -7.3236971 -7.4317293 -7.68396 -7.5324326][-5.6996231 -5.987412 -6.3664751 -7.0429955 -7.7989531 -8.3825369 -8.1168041 -7.0765553 -6.0920005 -5.5322809 -6.0311289 -6.4817362 -6.6830387 -7.0527706 -6.9734955][-5.8956504 -5.7562318 -6.3742476 -7.2883034 -8.0653982 -8.3839884 -7.5123024 -5.7630768 -4.3963122 -3.97833 -4.7336426 -5.4612432 -5.8500938 -6.3069735 -6.3272018][-6.256731 -5.8397522 -6.6690683 -7.8039007 -8.4537926 -8.2001944 -6.3870335 -3.806051 -2.1610172 -2.0022175 -3.0477262 -4.2507811 -5.0664949 -5.6937027 -5.84969][-5.8237047 -5.4897642 -6.4394779 -7.6836157 -8.0335922 -7.0370874 -4.16778 -0.76085353 0.97781539 0.76686883 -0.65047193 -2.4634478 -3.912276 -4.74759 -5.0252571][-5.0858212 -5.0552659 -6.0275044 -7.1160965 -6.9079075 -5.0562067 -1.1667628 2.924998 4.7026148 3.9627697 1.8321435 -0.86577857 -3.0838881 -4.0823159 -4.398272][-4.9262757 -4.9269323 -5.8767109 -6.7405033 -5.9739428 -3.2822294 1.3580327 5.8375444 7.6281767 6.3563175 3.4040697 -0.054973125 -2.8766251 -4.0544424 -4.3596487][-5.1792607 -5.3283954 -6.3024836 -6.9628496 -5.7683611 -2.6126685 2.2321832 6.7534132 8.3823853 6.4885426 2.9785259 -0.73509645 -3.7023134 -4.9122381 -5.1814718][-5.82034 -6.1945257 -7.2950306 -7.8986425 -6.6454659 -3.6440306 0.74510384 4.8050661 5.9365425 3.6490452 0.30671191 -2.931288 -5.4263277 -6.4010916 -6.5407143][-6.2976427 -6.819047 -8.0455456 -8.7499523 -7.8125658 -5.4340882 -1.8448894 1.4030063 1.9027007 -0.21570587 -2.7209835 -5.1034441 -6.9145594 -7.5229931 -7.5041571][-6.5996923 -7.1128082 -8.3507261 -9.1385031 -8.5998926 -6.8708744 -4.0812564 -1.7458421 -1.7911413 -3.5201688 -5.1831932 -6.8538084 -7.9944477 -8.1713829 -7.9632521][-6.8568382 -7.1941071 -8.3051968 -9.1045685 -8.9171667 -7.8020773 -5.8540759 -4.4596 -4.7552156 -5.8840008 -6.9563637 -8.11769 -8.59416 -8.3462448 -8.1185226][-7.0480871 -7.102747 -7.9737778 -8.7534943 -8.9099731 -8.3918266 -7.30725 -6.6536694 -6.9604788 -7.5700655 -8.1239147 -8.6767359 -8.6032038 -8.1960363 -8.1687975][-7.0670166 -6.8356466 -7.3816328 -7.9793768 -8.2791481 -8.1449642 -7.7129927 -7.5666056 -7.837203 -8.1008558 -8.2127857 -8.2313881 -7.9408112 -7.6989212 -7.9364328]]...]
INFO - root - 2017-12-15 08:37:36.754135: step 40310, loss = 0.22, batch loss = 0.19 (35.8 examples/sec; 0.223 sec/batch; 18h:07m:02s remains)
INFO - root - 2017-12-15 08:37:39.034995: step 40320, loss = 0.26, batch loss = 0.23 (33.6 examples/sec; 0.238 sec/batch; 19h:18m:02s remains)
INFO - root - 2017-12-15 08:37:41.293358: step 40330, loss = 0.26, batch loss = 0.22 (35.7 examples/sec; 0.224 sec/batch; 18h:12m:26s remains)
INFO - root - 2017-12-15 08:37:43.569443: step 40340, loss = 0.25, batch loss = 0.22 (36.4 examples/sec; 0.219 sec/batch; 17h:48m:47s remains)
INFO - root - 2017-12-15 08:37:45.828716: step 40350, loss = 0.18, batch loss = 0.15 (34.1 examples/sec; 0.235 sec/batch; 19h:02m:08s remains)
INFO - root - 2017-12-15 08:37:48.102696: step 40360, loss = 0.18, batch loss = 0.14 (35.5 examples/sec; 0.225 sec/batch; 18h:16m:01s remains)
INFO - root - 2017-12-15 08:37:50.406193: step 40370, loss = 0.17, batch loss = 0.14 (35.2 examples/sec; 0.227 sec/batch; 18h:27m:29s remains)
INFO - root - 2017-12-15 08:37:52.686112: step 40380, loss = 0.25, batch loss = 0.22 (34.4 examples/sec; 0.233 sec/batch; 18h:53m:27s remains)
INFO - root - 2017-12-15 08:37:54.960447: step 40390, loss = 0.19, batch loss = 0.16 (36.4 examples/sec; 0.220 sec/batch; 17h:49m:07s remains)
INFO - root - 2017-12-15 08:37:57.252315: step 40400, loss = 0.30, batch loss = 0.27 (35.6 examples/sec; 0.225 sec/batch; 18h:13m:32s remains)
2017-12-15 08:37:57.528794: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.9551215 -7.3221192 -6.7309213 -5.2800837 -3.66584 -3.1222577 -2.8652415 -2.9788208 -3.6140692 -5.3192749 -6.599978 -6.4029446 -6.0371814 -5.6744204 -5.3601718][-9.24911 -8.9713287 -8.0023327 -6.4340343 -5.00428 -4.5597253 -4.5219994 -4.8798285 -5.6688585 -7.3558216 -8.2716217 -8.08498 -7.7095022 -7.1870737 -6.6935997][-10.608252 -9.8452244 -8.4213972 -6.6281691 -5.4296203 -5.0525951 -5.0674095 -5.6087713 -6.7529292 -8.7019176 -9.6320591 -9.6527233 -9.31296 -8.6743641 -7.9349861][-12.185599 -10.456617 -8.4061975 -5.8933816 -4.3255281 -3.5262311 -3.1206634 -3.7304277 -5.3930564 -7.7025347 -8.9701481 -9.3009768 -9.1878319 -8.8025093 -8.1848326][-12.345076 -9.7034378 -6.9700174 -3.6640506 -1.6117022 -0.38965344 0.39734483 -0.50175929 -2.8328488 -5.7374449 -7.7611251 -8.4038334 -8.5097179 -8.4583187 -7.9355216][-10.827898 -7.7497826 -4.7332458 -1.1539944 0.99116611 2.4378929 3.3749418 2.3181887 -0.32236326 -3.6672843 -6.4076958 -7.3366547 -7.6036258 -7.9569793 -7.6297913][-8.9011536 -6.1548858 -3.1360483 0.41762209 2.5986056 4.2860522 5.4440212 4.6395206 2.1975093 -1.3733821 -4.668581 -5.7996016 -6.1952162 -6.9699736 -6.9193711][-8.4411936 -5.3756609 -2.2839043 1.3706436 3.7909532 5.7871151 7.0901632 6.4055891 3.8585491 -0.28839326 -4.227838 -5.8016386 -6.3278232 -7.1781211 -6.9723268][-8.5802078 -5.3351431 -2.1528976 1.3387632 3.6493626 5.715394 7.0572205 6.5533581 3.982172 -0.60163426 -4.6685162 -6.5620995 -7.108418 -7.8313007 -7.5039916][-9.1809378 -6.1101103 -3.1806247 -0.23121905 1.6895027 3.5447659 4.8598819 4.6250062 2.230351 -2.1376212 -5.8185015 -7.612772 -7.8984408 -8.279809 -7.7635946][-10.017943 -7.5167351 -5.0104671 -2.5651867 -0.9860245 0.4514997 1.6568174 1.6532516 -0.35601568 -4.0783653 -7.1358323 -8.7259541 -8.99618 -9.0884447 -8.3323956][-10.430202 -8.5163813 -6.4349194 -4.3824911 -3.2198889 -2.4418106 -1.7622485 -1.9926032 -3.7552524 -6.6593466 -8.81954 -10.024454 -10.248579 -10.013073 -8.774807][-10.387676 -8.8522015 -7.1327395 -5.4343681 -4.772316 -4.6435781 -4.6247272 -5.096365 -6.5620894 -8.669693 -9.7877178 -10.332143 -10.275507 -9.6899891 -8.3204632][-10.507718 -9.2251453 -7.9098639 -6.5212064 -6.0708318 -6.0741944 -6.1513472 -6.5055637 -7.5606966 -8.9743414 -9.3679886 -9.4363022 -9.3195982 -8.8012276 -7.721694][-10.080324 -8.63714 -7.5007172 -6.2679248 -5.7025266 -5.6225357 -5.7154284 -6.0632892 -6.9616885 -8.0825 -8.1923723 -8.160511 -8.1673374 -7.7618289 -7.0933309]]...]
INFO - root - 2017-12-15 08:37:59.781278: step 40410, loss = 0.30, batch loss = 0.27 (34.5 examples/sec; 0.232 sec/batch; 18h:49m:56s remains)
INFO - root - 2017-12-15 08:38:02.017364: step 40420, loss = 0.21, batch loss = 0.18 (35.5 examples/sec; 0.225 sec/batch; 18h:17m:23s remains)
INFO - root - 2017-12-15 08:38:04.259732: step 40430, loss = 0.26, batch loss = 0.23 (35.7 examples/sec; 0.224 sec/batch; 18h:09m:19s remains)
INFO - root - 2017-12-15 08:38:06.522278: step 40440, loss = 0.30, batch loss = 0.27 (33.8 examples/sec; 0.236 sec/batch; 19h:11m:10s remains)
INFO - root - 2017-12-15 08:38:08.835136: step 40450, loss = 0.18, batch loss = 0.14 (35.1 examples/sec; 0.228 sec/batch; 18h:30m:05s remains)
INFO - root - 2017-12-15 08:38:11.114352: step 40460, loss = 0.31, batch loss = 0.28 (34.7 examples/sec; 0.231 sec/batch; 18h:43m:03s remains)
INFO - root - 2017-12-15 08:38:13.358400: step 40470, loss = 0.18, batch loss = 0.15 (36.4 examples/sec; 0.220 sec/batch; 17h:50m:20s remains)
INFO - root - 2017-12-15 08:38:15.649500: step 40480, loss = 0.19, batch loss = 0.16 (34.4 examples/sec; 0.233 sec/batch; 18h:52m:54s remains)
INFO - root - 2017-12-15 08:38:17.907730: step 40490, loss = 0.23, batch loss = 0.19 (35.8 examples/sec; 0.223 sec/batch; 18h:07m:36s remains)
INFO - root - 2017-12-15 08:38:20.166791: step 40500, loss = 0.28, batch loss = 0.25 (36.4 examples/sec; 0.220 sec/batch; 17h:50m:23s remains)
2017-12-15 08:38:20.463580: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1554902 -4.91208 -4.9519691 -4.9263477 -4.7920113 -4.7366848 -4.8086472 -4.9068279 -4.8264389 -4.694953 -4.6705637 -4.7496696 -4.8811073 -4.8828831 -4.7822003][-5.0466881 -5.5554843 -5.4982758 -5.1485863 -4.6649818 -4.5243464 -4.7090473 -4.8712482 -4.641037 -4.4543009 -4.5859976 -4.8893051 -5.1935768 -5.33114 -5.2837191][-6.1067166 -5.7539067 -5.6548986 -5.0162897 -4.3165431 -4.1100445 -4.5535975 -4.8861017 -4.4934082 -4.3226452 -4.574419 -4.9131923 -5.1606278 -5.3879766 -5.4086871][-6.5051546 -5.8149867 -5.748148 -5.0492477 -4.2396641 -3.8055358 -4.3839712 -4.8064737 -4.145359 -3.9662485 -4.4359331 -4.8183403 -5.0113907 -5.3431292 -5.559041][-7.2154589 -6.0037584 -5.800004 -4.9947443 -3.8161039 -2.9702885 -3.6727376 -4.0574837 -2.9898243 -2.8193336 -3.7909436 -4.6161861 -5.1623287 -5.6669388 -5.9719186][-6.5693121 -4.8772354 -4.3452797 -3.1928387 -1.5858607 -0.24945331 -1.0617492 -1.5548825 -0.48971403 -0.78111124 -2.5656097 -4.0714931 -4.9876318 -5.5921955 -5.8749828][-5.7559724 -4.30715 -3.2986894 -1.7456394 0.43804479 2.6958969 2.0838702 1.4286933 2.3727458 1.6429086 -0.90780413 -3.0798175 -4.3427649 -5.1785879 -5.6740918][-5.7060962 -3.5222042 -1.8198985 0.1629653 2.8861 5.983284 5.5582981 4.6518021 5.4088211 4.3857288 1.1457314 -1.6145886 -3.3049083 -4.5376911 -5.3723297][-5.43367 -2.8181305 -0.76774311 1.5429385 4.511878 7.7564888 7.0310087 5.5095968 5.6094322 4.1910648 0.74051619 -2.0618446 -3.5960975 -4.7015381 -5.3301206][-6.7432003 -4.2798338 -2.3998814 -0.12224126 2.6321843 5.5806742 4.68956 2.7664263 2.2665946 0.89861727 -1.8786548 -3.934813 -4.7601385 -5.4725714 -5.8652377][-7.3156528 -5.0616856 -3.4949267 -1.6225755 0.52508736 2.7349675 1.7477734 -0.1624825 -0.78256893 -1.6684895 -3.5303097 -4.8425407 -5.237112 -5.7232981 -6.0831079][-7.0772352 -5.2696705 -4.4033041 -3.2605262 -1.9024162 -0.35904574 -0.94466567 -2.3728373 -2.9766378 -3.5558231 -4.7132273 -5.4992523 -5.7153273 -6.0414734 -6.2651091][-6.865283 -5.6509542 -5.4407043 -4.9227962 -4.2428474 -3.3464854 -3.550262 -4.3162675 -4.6441612 -4.8384104 -5.31785 -5.695353 -5.7832222 -5.9514103 -6.0531721][-6.1601548 -5.302392 -5.3387895 -5.1950254 -4.9576159 -4.524663 -4.495585 -4.7662358 -4.8348761 -4.7965908 -4.9589972 -5.1707687 -5.2727265 -5.381052 -5.4819517][-5.4127316 -4.8268633 -5.0177765 -5.1201534 -5.1746187 -5.0088 -4.8503022 -4.8556519 -4.8275328 -4.7662086 -4.8360515 -4.9636564 -5.0344553 -5.0568318 -5.0508685]]...]
INFO - root - 2017-12-15 08:38:22.719698: step 40510, loss = 0.19, batch loss = 0.16 (36.2 examples/sec; 0.221 sec/batch; 17h:54m:21s remains)
INFO - root - 2017-12-15 08:38:25.012808: step 40520, loss = 0.22, batch loss = 0.18 (34.8 examples/sec; 0.230 sec/batch; 18h:37m:15s remains)
INFO - root - 2017-12-15 08:38:27.321321: step 40530, loss = 0.30, batch loss = 0.27 (35.2 examples/sec; 0.227 sec/batch; 18h:26m:57s remains)
INFO - root - 2017-12-15 08:38:29.581589: step 40540, loss = 0.21, batch loss = 0.17 (35.4 examples/sec; 0.226 sec/batch; 18h:19m:43s remains)
INFO - root - 2017-12-15 08:38:31.867563: step 40550, loss = 0.19, batch loss = 0.16 (35.6 examples/sec; 0.225 sec/batch; 18h:12m:57s remains)
INFO - root - 2017-12-15 08:38:34.095476: step 40560, loss = 0.18, batch loss = 0.15 (36.0 examples/sec; 0.222 sec/batch; 18h:00m:53s remains)
INFO - root - 2017-12-15 08:38:36.374954: step 40570, loss = 0.24, batch loss = 0.21 (35.2 examples/sec; 0.227 sec/batch; 18h:26m:37s remains)
INFO - root - 2017-12-15 08:38:38.637055: step 40580, loss = 0.21, batch loss = 0.18 (35.0 examples/sec; 0.228 sec/batch; 18h:30m:35s remains)
INFO - root - 2017-12-15 08:38:40.947497: step 40590, loss = 0.20, batch loss = 0.17 (35.0 examples/sec; 0.228 sec/batch; 18h:30m:36s remains)
INFO - root - 2017-12-15 08:38:43.215323: step 40600, loss = 0.19, batch loss = 0.16 (34.8 examples/sec; 0.230 sec/batch; 18h:37m:23s remains)
2017-12-15 08:38:43.486189: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5849977 -4.3005953 -4.4513454 -3.8356917 -2.6655591 -1.5879829 -1.2166699 -1.8564782 -2.9662273 -3.4004879 -3.40652 -3.0286391 -2.4424255 -2.1734283 -2.3723705][-2.854737 -4.0470486 -4.330862 -4.0599947 -3.2341785 -2.4125798 -2.0765553 -2.5402727 -3.2560351 -3.5529432 -3.6563642 -3.4065044 -2.8950887 -2.4743128 -2.4476874][-3.2572684 -4.1450338 -4.7172136 -4.8417788 -4.3901262 -3.8108606 -3.2813075 -3.3593819 -3.7402 -3.9011819 -4.1209917 -3.9246597 -3.3895798 -2.9026692 -2.7275484][-4.0387588 -4.7797012 -5.6135311 -5.9568796 -5.6235209 -4.9668012 -4.0357523 -3.752476 -3.8803005 -3.9676127 -4.2244015 -4.0572481 -3.5133607 -2.9197514 -2.5684166][-4.6376276 -5.103725 -6.0258665 -6.3892326 -5.983489 -5.0128412 -3.4718633 -2.6699319 -2.5522685 -2.8497822 -3.5546219 -3.7229819 -3.2692747 -2.6406212 -2.1392083][-4.810482 -4.9880581 -5.8906064 -6.1986127 -5.5623941 -4.0598593 -1.8089494 -0.51874232 -0.28840578 -0.94766378 -2.1506333 -2.7145166 -2.5252705 -2.0524814 -1.4495239][-4.1282554 -4.4208755 -5.2188673 -5.3752728 -4.3653536 -2.3393915 0.39726639 1.902396 2.0700829 1.0047972 -0.54549825 -1.4140326 -1.614406 -1.4081373 -0.86481726][-3.0915644 -3.2898817 -3.9281821 -3.976378 -2.7400808 -0.33115232 2.6526721 4.1939011 4.2499 2.8471901 1.0026767 -0.3147862 -0.91460669 -0.93849182 -0.51320183][-2.2472093 -2.2464712 -2.8781657 -3.0970631 -1.9725031 0.56068492 3.6174967 5.1299868 5.1246662 3.6677706 1.747844 0.29034305 -0.41040742 -0.501356 -0.13863254][-1.5465813 -1.4809176 -2.289206 -2.9656782 -2.2686112 0.078436375 2.9253957 4.2698221 4.2774572 2.9963973 1.1843057 -0.26242375 -0.92398214 -0.91740155 -0.56860197][-1.3462396 -1.1999965 -2.2141318 -3.4392779 -3.4632428 -1.6317086 0.87770271 2.222326 2.4648368 1.4561341 -0.18427849 -1.51612 -1.9152887 -1.7314976 -1.4722004][-1.8147832 -1.4861741 -2.5018094 -3.9777536 -4.6095743 -3.4096889 -1.3894384 -0.21926308 0.16400528 -0.42009127 -1.6749671 -2.7824962 -3.0289071 -2.7164402 -2.4354978][-2.7808683 -2.3034282 -3.1450276 -4.575707 -5.5579319 -5.0362163 -3.7131982 -2.8763936 -2.4249961 -2.6747231 -3.5368438 -4.318367 -4.3728809 -3.8474836 -3.4580979][-4.3905354 -4.0652041 -4.8893318 -6.1915188 -7.164917 -6.986311 -6.1752234 -5.601119 -5.0888195 -4.9956408 -5.3443165 -5.6566114 -5.4668684 -4.6475096 -4.1029558][-5.6000361 -5.2797384 -5.854414 -6.660759 -7.271678 -7.2142887 -6.887804 -6.8299541 -6.6185036 -6.4504614 -6.4348364 -6.3995571 -6.1200695 -5.196279 -4.4565215]]...]
INFO - root - 2017-12-15 08:38:45.758483: step 40610, loss = 0.20, batch loss = 0.17 (35.8 examples/sec; 0.224 sec/batch; 18h:07m:26s remains)
INFO - root - 2017-12-15 08:38:48.018395: step 40620, loss = 0.19, batch loss = 0.15 (34.3 examples/sec; 0.233 sec/batch; 18h:55m:22s remains)
INFO - root - 2017-12-15 08:38:50.321930: step 40630, loss = 0.35, batch loss = 0.31 (34.8 examples/sec; 0.230 sec/batch; 18h:38m:06s remains)
INFO - root - 2017-12-15 08:38:52.590069: step 40640, loss = 0.25, batch loss = 0.22 (34.7 examples/sec; 0.230 sec/batch; 18h:41m:04s remains)
INFO - root - 2017-12-15 08:38:54.890637: step 40650, loss = 0.26, batch loss = 0.22 (35.5 examples/sec; 0.226 sec/batch; 18h:17m:03s remains)
INFO - root - 2017-12-15 08:38:57.193162: step 40660, loss = 0.20, batch loss = 0.16 (35.5 examples/sec; 0.225 sec/batch; 18h:16m:11s remains)
INFO - root - 2017-12-15 08:38:59.453455: step 40670, loss = 0.34, batch loss = 0.31 (34.7 examples/sec; 0.230 sec/batch; 18h:40m:23s remains)
INFO - root - 2017-12-15 08:39:01.702732: step 40680, loss = 0.32, batch loss = 0.29 (34.2 examples/sec; 0.234 sec/batch; 18h:57m:20s remains)
INFO - root - 2017-12-15 08:39:03.989702: step 40690, loss = 0.26, batch loss = 0.22 (32.5 examples/sec; 0.246 sec/batch; 19h:57m:44s remains)
INFO - root - 2017-12-15 08:39:06.269371: step 40700, loss = 0.19, batch loss = 0.16 (35.2 examples/sec; 0.227 sec/batch; 18h:24m:46s remains)
2017-12-15 08:39:06.560310: I tensorflow/core/kernels/logging_ops.cc:79] [[[2.3600128 0.15403557 -1.725718 -3.2691751 -3.8365493 -3.7071853 -3.2610106 -3.0882201 -3.3108363 -3.4331951 -2.8250597 -2.1160047 -1.9845699 -2.6947517 -3.5367172][0.23387957 -0.673103 -2.2481692 -3.5946531 -3.8169317 -3.3626552 -2.7994783 -2.8240838 -3.0072446 -3.0194783 -2.4457576 -1.8851461 -1.7596401 -2.2839665 -3.0523672][-2.4749382 -2.1833868 -3.117269 -3.9059563 -3.724577 -2.9897838 -2.32663 -2.4971712 -2.8466215 -3.0439515 -2.6734052 -2.2093203 -1.942324 -2.3096778 -3.0410454][-4.4682913 -3.5617549 -3.7199917 -3.7291353 -3.0930753 -2.2145653 -1.5758097 -1.907613 -2.5759912 -3.196842 -3.0515943 -2.4894648 -1.9118851 -1.9726956 -2.642482][-5.9129667 -4.5717726 -4.1450877 -3.3134704 -1.9419641 -0.75406432 -0.096939325 -0.55636859 -1.6014984 -2.8682294 -3.3838785 -3.2419503 -2.7327447 -2.6250515 -3.07172][-7.0137024 -5.4544592 -4.5607758 -2.9614739 -0.66636574 1.047158 1.7816851 1.3594968 0.1931653 -1.5329493 -2.619431 -2.9500608 -2.6749165 -2.7935169 -3.3985686][-6.9055796 -5.8983884 -4.9336386 -2.8334715 0.28426409 2.7224143 3.8574851 3.7258489 2.7154963 0.72165847 -0.81635273 -1.4797795 -1.3625491 -1.7297199 -2.6744297][-6.1370869 -5.700449 -5.1456518 -3.0880556 0.40394998 3.4602411 5.1231928 5.3768892 4.6264057 2.3538573 0.1827054 -0.9548099 -1.1684678 -1.5194337 -2.3799887][-5.5122252 -5.3545256 -5.456141 -3.9801497 -0.92254734 2.1539242 4.088068 4.7297382 4.4909496 2.5154955 0.19653296 -1.0470703 -1.4682807 -1.6757796 -2.1660008][-5.3556662 -5.0841966 -5.4537272 -4.6588144 -2.4056637 0.271384 2.1260569 2.8548663 2.8847158 1.4684374 -0.26352024 -0.93885565 -1.2243203 -1.4142331 -1.7438425][-4.8709774 -4.3847804 -4.8413706 -4.5568109 -3.0674543 -1.0014939 0.61736774 1.0616374 0.92147779 -0.15277648 -1.2738681 -1.3978893 -1.6627259 -2.0488009 -2.3431988][-4.5770521 -3.8240862 -4.2008238 -4.3124919 -3.6377225 -2.2808545 -0.88681972 -0.49829292 -0.48165905 -0.8429569 -1.1110507 -1.0661101 -1.7494196 -2.8641057 -3.5513096][-4.7764592 -3.6958957 -3.5993762 -3.6037598 -3.5208392 -2.9390898 -2.0201094 -1.8130677 -1.6536849 -1.1617987 -0.47245002 -0.28355038 -1.3912301 -3.3657322 -4.5279465][-4.5318441 -3.4529638 -3.033536 -2.881407 -3.0803869 -3.1114311 -2.8321877 -3.0582995 -2.9974833 -2.1744628 -1.1054394 -1.0022453 -2.3247962 -4.5937433 -5.8855171][-4.1494169 -3.4073038 -3.1786253 -3.0812182 -3.3889108 -3.676827 -3.681715 -3.9326344 -3.7570262 -2.8590837 -2.0731466 -2.4336288 -3.9558845 -6.0854158 -7.1881657]]...]
INFO - root - 2017-12-15 08:39:08.815144: step 40710, loss = 0.27, batch loss = 0.23 (36.6 examples/sec; 0.218 sec/batch; 17h:41m:32s remains)
INFO - root - 2017-12-15 08:39:11.105223: step 40720, loss = 0.24, batch loss = 0.21 (35.3 examples/sec; 0.226 sec/batch; 18h:20m:39s remains)
INFO - root - 2017-12-15 08:39:13.353088: step 40730, loss = 0.16, batch loss = 0.13 (35.8 examples/sec; 0.224 sec/batch; 18h:08m:02s remains)
INFO - root - 2017-12-15 08:39:15.612415: step 40740, loss = 0.21, batch loss = 0.18 (34.2 examples/sec; 0.234 sec/batch; 18h:55m:58s remains)
INFO - root - 2017-12-15 08:39:17.876306: step 40750, loss = 0.18, batch loss = 0.14 (34.0 examples/sec; 0.235 sec/batch; 19h:04m:02s remains)
INFO - root - 2017-12-15 08:39:20.175293: step 40760, loss = 0.17, batch loss = 0.14 (35.0 examples/sec; 0.229 sec/batch; 18h:31m:55s remains)
INFO - root - 2017-12-15 08:39:22.478933: step 40770, loss = 0.23, batch loss = 0.20 (35.5 examples/sec; 0.225 sec/batch; 18h:15m:16s remains)
INFO - root - 2017-12-15 08:39:24.799329: step 40780, loss = 0.21, batch loss = 0.18 (35.7 examples/sec; 0.224 sec/batch; 18h:09m:57s remains)
INFO - root - 2017-12-15 08:39:27.063955: step 40790, loss = 0.27, batch loss = 0.23 (34.5 examples/sec; 0.232 sec/batch; 18h:46m:34s remains)
INFO - root - 2017-12-15 08:39:29.332027: step 40800, loss = 0.23, batch loss = 0.20 (35.3 examples/sec; 0.226 sec/batch; 18h:20m:19s remains)
2017-12-15 08:39:29.598640: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2746136 -2.650147 -1.3056264 -0.66094613 -0.87680876 -1.1002496 -1.0353991 -2.014703 -1.3678281 -1.3961658 -2.8583417 -2.7828207 -1.5308969 -1.1995717 -1.5031973][-3.489542 -2.8420119 -1.8693591 -1.5403209 -1.8498549 -1.8503313 -1.5587271 -2.6461143 -1.9556439 -1.8188031 -2.7971687 -2.7844596 -1.5533483 -1.1591811 -1.2055112][-4.958384 -3.5304804 -2.8292425 -2.6849542 -2.9347553 -2.7148015 -2.2319691 -3.3436913 -2.8881054 -2.6179814 -3.0546033 -3.0027065 -1.903545 -1.4823699 -1.2340626][-5.6710796 -3.7396131 -3.2394555 -3.1048985 -3.2434611 -3.0125132 -2.5527573 -3.6751981 -3.5170572 -3.2573156 -3.356298 -3.1251383 -2.0911231 -1.694463 -1.4845202][-5.0461345 -2.7455773 -2.5100162 -2.5573425 -2.7709625 -2.5288744 -2.0550139 -2.8557861 -2.7966218 -2.7086332 -2.881568 -2.6251161 -1.8764257 -1.8195909 -1.8022357][-4.0613952 -1.4206817 -1.1403618 -1.1933012 -1.2541349 -0.76277649 -0.28275895 -0.70666027 -0.72962403 -0.96778464 -1.5123484 -1.2156091 -0.79291439 -1.2406243 -1.5668826][-2.8921866 -0.084903955 0.55138755 0.622458 0.83700776 1.6715028 2.2619369 2.1281021 1.9455092 1.2489438 0.097096205 0.18441272 0.16365671 -0.76080823 -1.2781513][-2.4479613 0.27678609 1.1629908 1.5094764 2.1434491 3.2693894 3.8862922 3.7179248 3.3243058 2.4627283 0.85691881 0.58066082 0.17973042 -0.92402279 -1.426651][-3.3664031 -0.90141881 0.03632021 0.45257711 1.2526562 2.60273 3.4448049 3.334758 2.7792747 2.0153844 0.33857107 -0.17582369 -0.88021874 -1.7939223 -1.9586318][-4.5815353 -2.7616723 -2.1045024 -1.7330654 -0.84184277 0.54859066 1.6065671 1.624892 1.0687699 0.64527345 -0.60712183 -1.3499273 -2.2773364 -2.8952417 -2.8213844][-5.2578106 -4.2219543 -4.0488834 -3.7877367 -2.9824839 -1.7897977 -0.66598892 -0.38025296 -0.7322973 -0.67467988 -1.2549515 -2.1149969 -3.462379 -4.0850329 -4.0991993][-5.114789 -4.5699916 -4.7889557 -4.6388273 -4.0295019 -3.1818848 -2.3400242 -1.8227216 -1.6078969 -0.88860345 -0.93616581 -1.9166384 -3.6250858 -4.4615946 -4.9512806][-4.4014444 -4.0350704 -4.3284535 -4.46087 -4.2943344 -3.9091897 -3.3559299 -2.5419464 -1.7720639 -0.53115273 -0.027960539 -0.96092153 -2.8273482 -3.9264174 -5.0173564][-3.8214798 -3.6103382 -3.9070547 -4.3313293 -4.5213528 -4.5368395 -4.2286882 -3.1155822 -1.8793395 -0.22434282 0.6885736 -0.060636759 -1.659687 -2.8524048 -4.3912125][-3.6398668 -3.3581576 -3.6137962 -4.4103518 -4.9910479 -5.2320623 -4.8111811 -3.512661 -2.0712965 -0.18401122 0.94933772 0.51330662 -0.61528039 -1.6430364 -3.3260722]]...]
INFO - root - 2017-12-15 08:39:31.838365: step 40810, loss = 0.27, batch loss = 0.23 (35.3 examples/sec; 0.227 sec/batch; 18h:21m:20s remains)
INFO - root - 2017-12-15 08:39:34.144430: step 40820, loss = 0.19, batch loss = 0.16 (35.7 examples/sec; 0.224 sec/batch; 18h:08m:32s remains)
INFO - root - 2017-12-15 08:39:36.428108: step 40830, loss = 0.20, batch loss = 0.16 (35.7 examples/sec; 0.224 sec/batch; 18h:08m:05s remains)
INFO - root - 2017-12-15 08:39:38.702807: step 40840, loss = 0.22, batch loss = 0.18 (34.9 examples/sec; 0.229 sec/batch; 18h:35m:21s remains)
INFO - root - 2017-12-15 08:39:40.983415: step 40850, loss = 0.21, batch loss = 0.18 (34.8 examples/sec; 0.230 sec/batch; 18h:38m:35s remains)
INFO - root - 2017-12-15 08:39:43.231731: step 40860, loss = 0.38, batch loss = 0.35 (36.1 examples/sec; 0.221 sec/batch; 17h:55m:48s remains)
INFO - root - 2017-12-15 08:39:45.527880: step 40870, loss = 0.25, batch loss = 0.22 (33.7 examples/sec; 0.237 sec/batch; 19h:12m:43s remains)
INFO - root - 2017-12-15 08:39:47.784936: step 40880, loss = 0.26, batch loss = 0.23 (35.2 examples/sec; 0.227 sec/batch; 18h:23m:31s remains)
INFO - root - 2017-12-15 08:39:50.071607: step 40890, loss = 0.23, batch loss = 0.19 (34.5 examples/sec; 0.232 sec/batch; 18h:48m:35s remains)
INFO - root - 2017-12-15 08:39:52.369725: step 40900, loss = 0.19, batch loss = 0.16 (35.6 examples/sec; 0.225 sec/batch; 18h:13m:23s remains)
2017-12-15 08:39:52.671468: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9285514 -3.8844233 -4.1560264 -4.6996841 -4.9978 -5.3524361 -5.8970032 -6.505352 -6.7016249 -6.5977068 -5.8273983 -4.9173603 -4.5881743 -4.4816184 -3.7720141][-4.6602855 -4.7248 -4.8930154 -5.4291544 -5.80962 -6.1250448 -6.5593119 -7.0310268 -6.9714117 -6.6454611 -5.9974451 -5.2528043 -4.93093 -4.8818083 -4.4236484][-6.0845757 -5.5066509 -5.2615452 -5.4612455 -5.6458035 -5.7838178 -5.9966507 -6.236815 -5.9981689 -5.566586 -5.0490284 -4.4978509 -4.3121176 -4.5112429 -4.4363713][-6.7577486 -5.7680888 -5.2873845 -5.13951 -5.1260567 -5.1498303 -5.2320604 -5.2774611 -4.9734554 -4.5760789 -4.097919 -3.4948592 -3.3687415 -3.6804788 -3.8105044][-6.5307198 -5.0966959 -4.3750324 -3.9527268 -3.6692019 -3.43464 -3.2365098 -2.9905434 -2.6475351 -2.3965216 -2.0667515 -1.5419674 -1.6005443 -2.0909667 -2.4655597][-4.7428522 -3.2474809 -2.4583316 -2.1077347 -1.8032095 -1.3765694 -0.91387606 -0.33809531 0.012302637 -0.076137543 -0.2208817 0.11053586 -0.12179017 -0.66830337 -1.2094117][-2.3873651 -1.2014943 -0.5588398 -0.66613936 -0.53071523 0.11234212 0.88970613 1.7946055 2.1839039 1.8031867 1.2887993 1.4707949 1.1867242 0.63530946 -0.016603947][-0.20319033 1.0741055 1.517596 0.79036522 0.62030888 1.3805225 2.3353107 3.4161894 3.8287461 3.2698166 2.4224756 2.3538244 1.9568546 1.3188958 0.5036962][0.90488029 2.307234 2.7336385 1.3373227 0.93296432 1.7911069 2.8391387 4.0007524 4.4572439 3.8874104 2.7102411 2.1457112 1.5702522 0.8701787 0.036052704][-0.070002079 1.4711733 1.8599627 0.045497656 -0.51797712 0.21151328 1.0677636 2.0350926 2.4495275 2.0975831 0.89237094 -0.013286591 -0.67121983 -1.3588848 -2.1023304][-2.0096378 -0.30177844 0.036455631 -1.7980959 -2.3882437 -1.8822742 -1.3151619 -0.8375448 -0.64231944 -0.59309292 -1.4962299 -2.4724441 -2.9346595 -3.5235989 -4.1221995][-3.8943281 -2.1912935 -1.81842 -3.2512314 -3.6478031 -3.3111458 -2.9912696 -3.0157893 -3.1841605 -2.9987497 -3.6481738 -4.4272995 -4.5363941 -4.8005276 -5.1464968][-5.6742835 -4.1437593 -3.8361597 -4.8145452 -4.8975677 -4.58331 -4.4319849 -4.693264 -5.0401735 -4.7690363 -5.1076746 -5.6484795 -5.5143461 -5.443686 -5.5751953][-6.2561903 -5.0371714 -4.9207973 -5.5081 -5.42762 -5.1648951 -5.2358608 -5.7032595 -6.1945863 -5.9070377 -5.9932051 -6.1833034 -5.7652311 -5.3572912 -5.2773218][-5.911334 -4.9091382 -4.90065 -5.2334433 -5.0238361 -4.7149649 -4.8775024 -5.3349495 -5.8355522 -5.6279907 -5.509944 -5.4093709 -4.833487 -4.3123984 -4.1790004]]...]
INFO - root - 2017-12-15 08:39:55.037021: step 40910, loss = 0.20, batch loss = 0.17 (32.6 examples/sec; 0.245 sec/batch; 19h:52m:23s remains)
INFO - root - 2017-12-15 08:39:57.333873: step 40920, loss = 0.25, batch loss = 0.22 (35.7 examples/sec; 0.224 sec/batch; 18h:08m:51s remains)
INFO - root - 2017-12-15 08:39:59.600012: step 40930, loss = 0.24, batch loss = 0.20 (35.4 examples/sec; 0.226 sec/batch; 18h:19m:01s remains)
INFO - root - 2017-12-15 08:40:01.847499: step 40940, loss = 0.24, batch loss = 0.21 (36.5 examples/sec; 0.219 sec/batch; 17h:46m:01s remains)
INFO - root - 2017-12-15 08:40:04.157588: step 40950, loss = 0.33, batch loss = 0.30 (34.1 examples/sec; 0.234 sec/batch; 18h:59m:19s remains)
INFO - root - 2017-12-15 08:40:06.436619: step 40960, loss = 0.28, batch loss = 0.24 (36.4 examples/sec; 0.220 sec/batch; 17h:47m:24s remains)
INFO - root - 2017-12-15 08:40:08.718623: step 40970, loss = 0.25, batch loss = 0.22 (34.4 examples/sec; 0.233 sec/batch; 18h:51m:08s remains)
INFO - root - 2017-12-15 08:40:10.990061: step 40980, loss = 0.21, batch loss = 0.17 (35.0 examples/sec; 0.228 sec/batch; 18h:29m:13s remains)
INFO - root - 2017-12-15 08:40:13.245507: step 40990, loss = 0.24, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 18h:10m:09s remains)
INFO - root - 2017-12-15 08:40:15.536447: step 41000, loss = 0.18, batch loss = 0.15 (35.2 examples/sec; 0.228 sec/batch; 18h:25m:28s remains)
2017-12-15 08:40:15.837075: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7622327 -2.8826065 -2.7022417 -2.5622954 -2.5766275 -3.314846 -3.9838314 -3.8430004 -3.8014107 -5.0343933 -6.4995384 -6.9479313 -6.6230717 -6.2326031 -5.4486933][-2.9652781 -3.5438519 -3.1583161 -2.5141673 -2.0628893 -2.6856389 -3.3817854 -3.4636703 -4.0587177 -5.7172508 -7.5550756 -8.2230539 -7.7946148 -7.1219234 -6.2260542][-4.6068077 -4.481442 -3.9788613 -2.7772806 -1.5894281 -1.5075271 -1.56724 -1.4824854 -2.3584423 -4.2592125 -6.37712 -7.6630583 -7.4912052 -6.9094248 -6.2289963][-6.3604193 -5.4996853 -5.106421 -3.5472293 -1.5991895 -0.68324256 0.10250974 0.65317941 -0.3477751 -2.5975413 -5.0539 -6.885232 -7.108264 -6.6462812 -6.0194316][-8.0198145 -6.4362316 -6.0226564 -4.10599 -1.3990324 0.029980898 1.4941423 2.4499886 1.1123071 -1.6661839 -4.5090189 -6.7834997 -7.1135521 -6.6599636 -5.9302106][-8.8009834 -7.1959372 -6.8102694 -4.7272115 -1.5877879 0.18503261 2.2615621 3.5172408 2.0297825 -1.1623073 -4.2020092 -6.7047606 -7.1399336 -6.7851734 -6.0112081][-8.4011164 -7.2792969 -6.9208317 -4.8994622 -1.5774946 0.613292 3.349035 5.1909847 3.899709 0.521626 -2.7621765 -5.6780243 -6.63717 -6.7309179 -5.980094][-7.8900967 -6.8258715 -6.6677113 -4.8959055 -1.7452331 0.50646234 3.4297731 5.5161095 4.5851374 1.3069253 -2.0592237 -5.2358246 -6.5288277 -6.7462158 -5.8548837][-7.4407415 -6.4950619 -6.6008177 -5.152596 -2.4170747 -0.42179298 2.1679194 4.0794086 3.3261039 0.46767807 -2.6817088 -5.709167 -6.8685741 -6.8677511 -5.7123809][-7.5503359 -6.5629835 -6.8092585 -5.7022257 -3.4307249 -1.3901856 0.94317365 2.3680184 1.802233 -0.63751924 -3.6238105 -6.36633 -7.3294525 -7.0370741 -5.6945734][-6.9865675 -5.9979382 -6.3195658 -5.7027922 -3.9619324 -2.0920703 0.00927186 1.0398862 0.61148381 -1.3458619 -4.1817088 -6.6644459 -7.4011116 -6.8524456 -5.5210633][-5.61186 -4.6729059 -5.18392 -5.0846014 -4.0391016 -2.8660111 -1.3950357 -0.46287179 -0.61880803 -1.8909961 -4.1905117 -6.1965294 -6.71523 -6.2073703 -5.1891289][-5.2892828 -4.4573278 -4.9472122 -4.9328175 -4.257865 -3.6481309 -2.7265182 -1.9300878 -2.091599 -2.7356071 -4.45953 -6.0325537 -6.0629835 -5.6617737 -5.0309963][-5.9223895 -5.3527479 -5.77602 -5.5298824 -4.8699007 -4.6536646 -4.0125189 -3.2783213 -3.6580467 -4.3284712 -5.4646816 -6.3378878 -5.9710207 -5.5273514 -4.8779392][-5.8444653 -5.6032047 -6.2588911 -6.016098 -5.6826954 -6.0606885 -5.7460661 -4.9785681 -5.5572572 -6.0659294 -6.3246393 -6.6158314 -6.0553265 -5.4521427 -4.66273]]...]
INFO - root - 2017-12-15 08:40:18.146092: step 41010, loss = 0.19, batch loss = 0.16 (33.2 examples/sec; 0.241 sec/batch; 19h:31m:35s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 08:40:20.423467: step 41020, loss = 0.28, batch loss = 0.25 (35.4 examples/sec; 0.226 sec/batch; 18h:18m:11s remains)
INFO - root - 2017-12-15 08:40:22.721954: step 41030, loss = 0.23, batch loss = 0.20 (35.4 examples/sec; 0.226 sec/batch; 18h:19m:08s remains)
INFO - root - 2017-12-15 08:40:25.035699: step 41040, loss = 0.20, batch loss = 0.16 (34.6 examples/sec; 0.231 sec/batch; 18h:44m:11s remains)
INFO - root - 2017-12-15 08:40:27.335021: step 41050, loss = 0.19, batch loss = 0.15 (34.3 examples/sec; 0.233 sec/batch; 18h:51m:24s remains)
INFO - root - 2017-12-15 08:40:29.610817: step 41060, loss = 0.20, batch loss = 0.17 (34.4 examples/sec; 0.232 sec/batch; 18h:48m:48s remains)
INFO - root - 2017-12-15 08:40:31.910249: step 41070, loss = 0.23, batch loss = 0.20 (34.7 examples/sec; 0.231 sec/batch; 18h:40m:05s remains)
INFO - root - 2017-12-15 08:40:34.181561: step 41080, loss = 0.28, batch loss = 0.25 (35.8 examples/sec; 0.223 sec/batch; 18h:04m:27s remains)
INFO - root - 2017-12-15 08:40:36.423009: step 41090, loss = 0.17, batch loss = 0.14 (35.4 examples/sec; 0.226 sec/batch; 18h:18m:04s remains)
INFO - root - 2017-12-15 08:40:38.722436: step 41100, loss = 0.19, batch loss = 0.15 (33.3 examples/sec; 0.240 sec/batch; 19h:26m:13s remains)
2017-12-15 08:40:39.045276: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2918139 -2.5288036 -2.9973712 -3.7886496 -4.4988804 -4.6994486 -4.022965 -3.5275211 -3.6094851 -3.7144265 -3.7564774 -3.8082891 -3.5057561 -3.191812 -3.3863573][-2.2591829 -2.8671036 -3.56917 -4.3637185 -4.8272715 -4.6137705 -3.7997398 -3.0151873 -2.7674215 -2.7893848 -3.1007857 -3.4650536 -3.3814917 -3.3506992 -4.0214911][-3.7853348 -3.6900609 -4.3679748 -4.9758129 -4.9746418 -4.0980587 -2.9861212 -2.1388063 -1.8582001 -1.9351151 -2.6626015 -3.4662027 -3.7469416 -4.0112538 -4.9872365][-5.18386 -4.5148153 -4.7870045 -4.8689795 -4.09147 -2.45424 -1.1406789 -0.51387084 -0.58771658 -1.1771046 -2.5253725 -3.7768378 -4.4617696 -4.8691125 -5.9056158][-6.5419483 -5.1385422 -4.7562914 -3.9466705 -2.25656 -0.039348125 1.3536205 1.7787693 1.3141615 0.053929806 -1.8861344 -3.5455289 -4.7762794 -5.5110188 -6.5144072][-7.2043657 -5.4138131 -4.596343 -2.9349761 -0.3165611 2.4786627 3.9968245 4.3546858 3.4925625 1.6120658 -0.87405705 -2.9701467 -4.8474712 -6.1099844 -7.035614][-7.060853 -5.4677963 -4.465282 -2.272855 1.0428262 4.4034014 6.0729656 6.2972527 5.0715084 2.6846898 -0.24939656 -2.7462835 -5.0357838 -6.5812874 -7.2358332][-6.8196774 -5.2321625 -4.4943848 -2.5022233 0.60770726 3.9884393 5.6826229 5.886116 4.5663052 2.1227343 -0.8538909 -3.478488 -5.6457682 -7.0492764 -7.1338692][-6.6694088 -5.1933007 -4.7974105 -3.3804026 -0.93931794 2.0298312 3.6119645 3.6706374 2.3167655 0.15582013 -2.5022655 -4.9122906 -6.6474819 -7.526638 -6.9236212][-6.8539534 -5.7390046 -5.5756598 -4.6921606 -2.941287 -0.45826411 1.0320969 0.83571434 -0.60485637 -2.4854479 -4.7376785 -6.7061319 -7.7512112 -7.7945967 -6.4968357][-6.9378195 -6.3028917 -6.393734 -6.0022545 -5.0045133 -3.3812513 -2.234205 -2.6429367 -4.1085162 -5.5618849 -7.0676689 -8.1282721 -8.2675514 -7.5856113 -5.907362][-6.8982496 -6.6543016 -6.9401503 -6.89764 -6.554594 -5.9285145 -5.3176813 -5.6080222 -6.469903 -7.3054419 -8.1592932 -8.5371828 -8.178153 -7.2630558 -5.642313][-6.7594624 -6.7259207 -7.0982122 -7.2291336 -7.2366381 -7.2326994 -7.0152941 -7.0465746 -7.2684631 -7.7530355 -8.3685656 -8.5031557 -7.9625959 -7.1034451 -5.6790538][-6.7340021 -6.6323919 -6.9587679 -7.0656481 -7.0197592 -7.13972 -7.2253075 -7.2159748 -7.2977285 -7.7249517 -8.135787 -7.9509535 -7.3236961 -6.7588768 -5.7937155][-7.1780729 -6.9556723 -7.1628256 -7.1734686 -6.8903522 -6.8250513 -6.9818697 -6.8375483 -6.5212059 -6.6784058 -6.7721443 -6.5444508 -6.30546 -6.3872519 -6.0851917]]...]
INFO - root - 2017-12-15 08:40:41.303615: step 41110, loss = 0.25, batch loss = 0.22 (34.6 examples/sec; 0.231 sec/batch; 18h:43m:02s remains)
INFO - root - 2017-12-15 08:40:43.584827: step 41120, loss = 0.27, batch loss = 0.24 (35.7 examples/sec; 0.224 sec/batch; 18h:07m:57s remains)
INFO - root - 2017-12-15 08:40:45.843046: step 41130, loss = 0.20, batch loss = 0.17 (34.9 examples/sec; 0.229 sec/batch; 18h:32m:51s remains)
INFO - root - 2017-12-15 08:40:48.096815: step 41140, loss = 0.30, batch loss = 0.27 (34.6 examples/sec; 0.231 sec/batch; 18h:43m:03s remains)
INFO - root - 2017-12-15 08:40:50.343828: step 41150, loss = 0.23, batch loss = 0.20 (36.2 examples/sec; 0.221 sec/batch; 17h:54m:21s remains)
INFO - root - 2017-12-15 08:40:52.619892: step 41160, loss = 0.18, batch loss = 0.14 (34.6 examples/sec; 0.231 sec/batch; 18h:43m:49s remains)
INFO - root - 2017-12-15 08:40:54.903772: step 41170, loss = 0.25, batch loss = 0.21 (34.0 examples/sec; 0.235 sec/batch; 19h:02m:30s remains)
INFO - root - 2017-12-15 08:40:57.203989: step 41180, loss = 0.18, batch loss = 0.15 (35.6 examples/sec; 0.225 sec/batch; 18h:10m:34s remains)
INFO - root - 2017-12-15 08:40:59.474267: step 41190, loss = 0.19, batch loss = 0.16 (35.8 examples/sec; 0.223 sec/batch; 18h:05m:02s remains)
INFO - root - 2017-12-15 08:41:01.708138: step 41200, loss = 0.22, batch loss = 0.19 (35.0 examples/sec; 0.229 sec/batch; 18h:30m:18s remains)
2017-12-15 08:41:01.996086: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.53687656 -2.4962275 -2.7906275 -3.138041 -3.384325 -3.5599372 -3.8758307 -4.341794 -4.7467656 -4.7459397 -4.7751102 -4.9027967 -4.8478351 -4.7219172 -4.74675][-2.9238284 -3.8063297 -4.13301 -4.4089723 -4.4416785 -4.3612003 -4.5237985 -4.907609 -5.3373041 -5.4849162 -5.6578722 -5.9203482 -5.8562546 -5.6322656 -5.5479813][-4.8724232 -5.0226874 -5.2224197 -5.2901926 -5.0100508 -4.493783 -4.268959 -4.3979034 -4.7506886 -5.0433273 -5.4394584 -5.9821367 -6.2066288 -6.0302277 -5.9148006][-5.7329206 -5.2728539 -5.3563514 -5.330308 -4.8900137 -4.0459976 -3.487663 -3.4606216 -3.8428693 -4.2994061 -4.818696 -5.5192962 -6.1089544 -6.1057949 -6.07681][-5.7425041 -4.7095919 -4.5489807 -4.2924595 -3.6631367 -2.5987513 -1.697073 -1.4466523 -1.7840499 -2.3880365 -3.192693 -4.28647 -5.4026332 -5.6559954 -5.7683659][-4.923893 -3.594481 -3.2354627 -2.7293413 -1.8884913 -0.64310455 0.47199798 0.8744297 0.57604575 -0.086524963 -1.0393424 -2.4270356 -4.093863 -4.6251011 -4.903964][-3.058944 -1.8315871 -1.5256431 -1.0264271 -0.16574073 1.1127794 2.371309 2.8547523 2.5613334 1.9473412 0.97941065 -0.55203092 -2.6188908 -3.3853831 -3.8087325][-1.725667 -0.54732347 -0.538857 -0.18421769 0.63296294 1.8420131 3.058399 3.6373394 3.4307396 2.9418089 2.0801051 0.60472059 -1.4787446 -2.2505505 -2.693444][-1.2005421 -0.29709852 -0.62599349 -0.40488935 0.42119002 1.5611656 2.5168831 2.9591177 2.6952918 2.2656119 1.6311846 0.51230812 -1.1126479 -1.5743623 -1.834094][-1.7615885 -1.176828 -1.7339275 -1.6430547 -0.83025897 0.23267221 0.96338773 1.2520249 0.91911221 0.50051904 0.059527874 -0.60544789 -1.5470252 -1.5843112 -1.5531923][-2.6865134 -2.3403435 -3.0038834 -3.0495973 -2.3605068 -1.5450319 -1.1235362 -1.0089103 -1.3348446 -1.6541321 -1.9259939 -2.2105076 -2.54217 -2.2479186 -1.9554706][-3.6495421 -3.4403467 -4.077683 -4.2435765 -3.8003957 -3.3540378 -3.2904129 -3.3135762 -3.5466309 -3.707304 -3.7495456 -3.6996598 -3.607748 -3.1530771 -2.7418194][-4.725564 -4.5830417 -5.1561661 -5.4290876 -5.244143 -5.0931997 -5.1912851 -5.2394094 -5.3013439 -5.2444458 -5.060689 -4.7856846 -4.4810429 -4.02372 -3.61589][-5.7098789 -5.4671888 -5.9097552 -6.1798577 -6.1632004 -6.1858416 -6.2844296 -6.2422538 -6.106792 -5.8755155 -5.6124773 -5.3424296 -5.0760927 -4.744246 -4.4253721][-6.1642694 -5.719058 -6.0083866 -6.2201958 -6.2538557 -6.2733488 -6.28934 -6.1839442 -6.0056725 -5.8091936 -5.6451712 -5.5139065 -5.4046516 -5.2386694 -5.0496554]]...]
INFO - root - 2017-12-15 08:41:04.306517: step 41210, loss = 0.20, batch loss = 0.17 (35.9 examples/sec; 0.223 sec/batch; 18h:02m:28s remains)
INFO - root - 2017-12-15 08:41:06.597871: step 41220, loss = 0.27, batch loss = 0.23 (35.7 examples/sec; 0.224 sec/batch; 18h:09m:06s remains)
INFO - root - 2017-12-15 08:41:08.868586: step 41230, loss = 0.22, batch loss = 0.19 (35.1 examples/sec; 0.228 sec/batch; 18h:27m:00s remains)
INFO - root - 2017-12-15 08:41:11.207634: step 41240, loss = 0.22, batch loss = 0.19 (34.3 examples/sec; 0.233 sec/batch; 18h:52m:51s remains)
INFO - root - 2017-12-15 08:41:13.509456: step 41250, loss = 0.41, batch loss = 0.38 (36.7 examples/sec; 0.218 sec/batch; 17h:39m:08s remains)
INFO - root - 2017-12-15 08:41:15.773746: step 41260, loss = 0.15, batch loss = 0.12 (34.8 examples/sec; 0.230 sec/batch; 18h:36m:21s remains)
INFO - root - 2017-12-15 08:41:18.054446: step 41270, loss = 0.19, batch loss = 0.16 (34.8 examples/sec; 0.230 sec/batch; 18h:34m:50s remains)
INFO - root - 2017-12-15 08:41:20.320221: step 41280, loss = 0.32, batch loss = 0.29 (33.4 examples/sec; 0.239 sec/batch; 19h:22m:12s remains)
INFO - root - 2017-12-15 08:41:22.580507: step 41290, loss = 0.22, batch loss = 0.19 (35.7 examples/sec; 0.224 sec/batch; 18h:08m:30s remains)
INFO - root - 2017-12-15 08:41:24.904176: step 41300, loss = 0.18, batch loss = 0.15 (36.6 examples/sec; 0.219 sec/batch; 17h:41m:57s remains)
2017-12-15 08:41:25.209927: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.196116 -3.9086733 -3.9294496 -4.3376465 -4.8959532 -5.066577 -5.5284519 -5.8673468 -6.3126364 -5.8941469 -4.8764086 -2.9911377 -1.1461122 -0.29627693 0.45497274][-3.1452589 -4.0968881 -4.3664565 -4.8331285 -5.1993113 -5.1008615 -5.2323866 -5.4030676 -5.9982038 -5.9451013 -5.5369024 -4.2275004 -2.5717382 -1.658494 -0.96379411][-3.2087502 -4.2430639 -4.5705624 -4.8617058 -4.9693584 -4.514267 -4.0806308 -4.0275784 -4.9283423 -5.6635914 -6.0207205 -5.2271547 -3.6564164 -2.7014966 -2.1734638][-3.8927457 -5.0222244 -5.3760576 -5.3128667 -4.9548388 -3.8336039 -2.5923057 -2.2353182 -3.340713 -4.7680235 -5.7459364 -5.3035445 -3.7377877 -2.7121489 -2.2579675][-4.6066141 -5.4242573 -5.4147539 -4.77887 -3.8329687 -2.1038623 -0.17939115 0.44857979 -0.84226441 -2.9481862 -4.5166993 -4.4110622 -2.9977262 -2.0932465 -1.7275183][-4.480999 -4.9122744 -4.2835808 -2.9906831 -1.5422398 0.61228156 2.8110991 3.345892 1.7640214 -0.94281805 -2.9202752 -2.993825 -1.784603 -1.0796415 -0.86808109][-4.115571 -4.2711248 -2.9413629 -0.923761 1.0697091 3.4881649 5.4873939 5.5803714 3.6586046 0.58866286 -1.5658876 -1.7055473 -0.55430865 0.23581052 0.47157836][-4.3322921 -3.7598774 -1.6880035 0.87724924 3.1913619 5.461853 6.83943 6.405386 4.29977 1.158999 -1.0474323 -1.1881083 0.03618145 0.95904708 0.98710513][-4.7099495 -3.7209995 -1.5476363 0.92570829 3.114059 5.0387249 5.844697 5.084918 3.1293888 0.33186507 -1.6693397 -1.803964 -0.53124309 0.34190416 0.092503548][-5.6392326 -4.74183 -3.1892104 -1.1315022 0.8593626 2.383924 2.7626958 2.0264463 0.5885632 -1.4596546 -3.1036835 -3.2915218 -2.0792651 -1.3374168 -1.5746953][-6.9112997 -6.3157024 -5.5333161 -3.9338903 -2.0906856 -0.86991763 -0.6152246 -1.0422465 -1.8249642 -3.0423305 -4.2401915 -4.4559326 -3.4912219 -3.0410402 -3.2503176][-7.5684204 -7.3619242 -7.4280062 -6.4649158 -4.8881989 -3.8817363 -3.5401955 -3.6691728 -3.9725888 -4.4140415 -4.9754782 -5.1380157 -4.6944928 -4.7478437 -4.9142313][-6.983469 -7.09139 -7.9142613 -7.568882 -6.3269997 -5.5659165 -5.1603203 -5.0286579 -5.1370163 -5.216403 -5.4980097 -5.608367 -5.5274925 -5.8558125 -5.707139][-5.2244272 -5.4234524 -6.7799807 -7.0681467 -6.3921328 -6.0279713 -5.6742496 -5.3660107 -5.5112467 -5.6852093 -5.8258605 -5.4537573 -5.1271124 -5.2223749 -4.8185635][-2.4858627 -2.6952786 -4.5182018 -5.5034809 -5.5425954 -5.7511015 -5.6393242 -5.3675241 -5.6735539 -6.0388384 -5.9861178 -4.9096365 -3.9786882 -3.6887031 -3.3125658]]...]
INFO - root - 2017-12-15 08:41:27.499073: step 41310, loss = 0.32, batch loss = 0.28 (34.6 examples/sec; 0.231 sec/batch; 18h:41m:23s remains)
INFO - root - 2017-12-15 08:41:29.778808: step 41320, loss = 0.23, batch loss = 0.19 (34.2 examples/sec; 0.234 sec/batch; 18h:55m:44s remains)
INFO - root - 2017-12-15 08:41:32.068184: step 41330, loss = 0.27, batch loss = 0.23 (36.1 examples/sec; 0.222 sec/batch; 17h:56m:23s remains)
INFO - root - 2017-12-15 08:41:34.318285: step 41340, loss = 0.23, batch loss = 0.19 (35.8 examples/sec; 0.224 sec/batch; 18h:04m:58s remains)
INFO - root - 2017-12-15 08:41:36.609041: step 41350, loss = 0.36, batch loss = 0.33 (33.9 examples/sec; 0.236 sec/batch; 19h:04m:03s remains)
INFO - root - 2017-12-15 08:41:38.878532: step 41360, loss = 0.19, batch loss = 0.16 (35.9 examples/sec; 0.223 sec/batch; 18h:02m:44s remains)
INFO - root - 2017-12-15 08:41:41.158378: step 41370, loss = 0.22, batch loss = 0.18 (35.8 examples/sec; 0.224 sec/batch; 18h:05m:05s remains)
INFO - root - 2017-12-15 08:41:43.434288: step 41380, loss = 0.25, batch loss = 0.22 (32.7 examples/sec; 0.245 sec/batch; 19h:46m:43s remains)
INFO - root - 2017-12-15 08:41:45.699185: step 41390, loss = 0.17, batch loss = 0.14 (35.8 examples/sec; 0.224 sec/batch; 18h:05m:10s remains)
INFO - root - 2017-12-15 08:41:47.946327: step 41400, loss = 0.23, batch loss = 0.19 (34.9 examples/sec; 0.229 sec/batch; 18h:30m:46s remains)
2017-12-15 08:41:48.264357: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9047174 -6.209928 -6.7168655 -7.0690556 -6.4683838 -5.7255392 -5.4092121 -4.888104 -4.9946423 -5.108325 -4.5999861 -3.6853867 -3.2707877 -2.8346953 -2.2749968][-5.0146418 -6.5617838 -6.9824982 -7.0673971 -6.5390587 -6.1055241 -5.9198837 -5.5910926 -5.9489627 -6.1530151 -5.4570317 -4.1828766 -3.3681235 -2.7523265 -2.2849371][-5.6242924 -6.1547174 -6.3512955 -6.2224064 -5.7900848 -5.5617762 -5.3286695 -5.1034617 -5.7438593 -6.2493796 -5.7037778 -4.4701214 -3.5874941 -2.802496 -2.3270648][-6.0341558 -5.8418307 -5.5943222 -5.0051403 -4.346642 -4.02711 -3.6167731 -3.5338907 -4.6323795 -5.7190323 -5.6681471 -4.6749048 -3.8324268 -2.9563551 -2.4616239][-6.2876835 -5.7940745 -4.9911642 -3.676342 -2.5192125 -1.7235322 -0.88821495 -0.89888704 -2.5492206 -4.3530183 -4.9907045 -4.3403 -3.7286487 -3.0711403 -2.85848][-6.5364838 -5.6698265 -4.3686771 -2.3029451 -0.450441 1.1088281 2.4182808 2.4044015 0.25432658 -2.3122957 -3.7975459 -3.6743336 -3.51311 -3.1094732 -3.1882653][-5.9769545 -5.5782051 -3.91216 -1.2763169 1.1846156 3.4633725 5.3318787 5.4200029 2.8723176 -0.247087 -2.4441273 -2.9941833 -3.1338172 -2.8788679 -3.1319811][-5.8523321 -5.6376786 -3.75088 -0.91930377 1.7451475 4.7060728 7.1137772 7.390811 4.7792826 1.4244783 -1.4433548 -2.6077666 -2.7820916 -2.6112185 -3.0246634][-5.8708386 -5.9959459 -4.2053008 -1.5875828 0.82193255 4.1420536 6.9079895 7.3387146 4.8803129 1.4821138 -1.7876315 -3.3789229 -3.3068094 -2.9320283 -3.1638553][-6.030323 -6.4914064 -5.1919746 -3.256382 -1.4619381 1.7367439 4.4807415 5.1078663 3.2252734 0.43613434 -2.7856648 -4.322648 -3.8969359 -3.49491 -3.7162447][-6.3570385 -7.0037794 -6.2098045 -4.8925781 -3.5677252 -0.72256386 1.6478333 2.1997783 0.77440047 -1.3615342 -4.1991611 -5.4796576 -4.8964829 -4.6571555 -4.7762604][-7.1178017 -7.693264 -7.1771803 -6.2710247 -5.2690473 -2.9504018 -1.3360634 -1.0812978 -2.0250509 -3.5962324 -5.9874163 -6.8198719 -6.2178335 -6.0332909 -5.9760008][-7.4935951 -7.9066038 -7.7536917 -7.3411293 -6.6071444 -4.9668713 -3.9147897 -3.7614524 -4.2782593 -5.2148128 -6.8762054 -7.2403269 -6.7527742 -6.66117 -6.5369835][-7.5902243 -7.460454 -7.390707 -7.2033424 -6.5895405 -5.4015136 -4.7503352 -4.7458925 -5.0952463 -5.7475848 -6.8639374 -6.9293356 -6.4642782 -6.3463182 -6.25406][-7.217042 -6.434804 -6.363883 -6.3180304 -5.8736162 -5.2261124 -5.0645523 -5.2183337 -5.3582354 -5.6907845 -6.349329 -6.2598648 -5.8570051 -5.592659 -5.37617]]...]
INFO - root - 2017-12-15 08:41:50.591370: step 41410, loss = 0.24, batch loss = 0.21 (32.4 examples/sec; 0.247 sec/batch; 19h:57m:15s remains)
INFO - root - 2017-12-15 08:41:52.868276: step 41420, loss = 0.30, batch loss = 0.27 (35.7 examples/sec; 0.224 sec/batch; 18h:06m:48s remains)
INFO - root - 2017-12-15 08:41:55.165956: step 41430, loss = 0.23, batch loss = 0.19 (33.6 examples/sec; 0.238 sec/batch; 19h:14m:39s remains)
INFO - root - 2017-12-15 08:41:57.417322: step 41440, loss = 0.25, batch loss = 0.22 (35.5 examples/sec; 0.226 sec/batch; 18h:14m:12s remains)
INFO - root - 2017-12-15 08:41:59.657742: step 41450, loss = 0.20, batch loss = 0.17 (35.6 examples/sec; 0.225 sec/batch; 18h:09m:05s remains)
INFO - root - 2017-12-15 08:42:01.911812: step 41460, loss = 0.21, batch loss = 0.17 (34.9 examples/sec; 0.229 sec/batch; 18h:31m:28s remains)
INFO - root - 2017-12-15 08:42:04.197986: step 41470, loss = 0.18, batch loss = 0.14 (34.0 examples/sec; 0.235 sec/batch; 19h:02m:11s remains)
INFO - root - 2017-12-15 08:42:06.440482: step 41480, loss = 0.20, batch loss = 0.16 (36.6 examples/sec; 0.219 sec/batch; 17h:41m:17s remains)
INFO - root - 2017-12-15 08:42:08.711626: step 41490, loss = 0.19, batch loss = 0.16 (34.5 examples/sec; 0.232 sec/batch; 18h:44m:20s remains)
INFO - root - 2017-12-15 08:42:10.999247: step 41500, loss = 0.21, batch loss = 0.18 (35.1 examples/sec; 0.228 sec/batch; 18h:25m:27s remains)
2017-12-15 08:42:11.282826: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.8843677 -0.74165535 -1.4977729 -2.0527923 -2.6334317 -3.4953701 -3.8154068 -3.416172 -2.4694729 -1.7582525 -1.6223204 -1.8527089 -3.0280573 -3.7386847 -3.625165][0.48285508 -0.52876687 -1.4614372 -2.1999996 -3.1588528 -4.32658 -4.6887541 -4.0688748 -3.0196025 -2.3116255 -2.2007496 -2.6188395 -3.6971879 -4.285862 -4.2037678][-0.48349082 -0.74310291 -1.752228 -2.6974866 -3.8130622 -5.0010624 -5.1386786 -4.020834 -2.7832603 -2.1800883 -2.2240469 -2.8150456 -3.9071698 -4.6748247 -4.9184394][-1.8177669 -1.5887837 -2.6373005 -3.5278893 -4.4132948 -5.1108646 -4.6442966 -2.9765494 -1.555645 -1.1313767 -1.4555606 -2.4144056 -3.7610123 -4.8557253 -5.4619131][-3.5621419 -3.0425415 -3.7392535 -4.1278811 -4.1307774 -3.738553 -2.5362298 -0.51134 0.779263 0.67961311 -0.19297361 -1.6199613 -3.2840598 -4.6757631 -5.459054][-4.9487619 -4.3244543 -4.34752 -3.8598514 -2.8422194 -1.3602924 0.45227695 2.5245948 3.3874149 2.5291824 1.0375428 -0.88869905 -2.9018056 -4.3886929 -5.1259971][-5.51573 -4.7132916 -4.0625896 -2.9608524 -1.224411 1.0620544 3.3025322 5.1405497 5.451694 3.8965888 1.7216065 -0.59049451 -2.7473319 -4.0216603 -4.3766947][-5.26136 -3.7872214 -2.6415358 -1.4451327 0.3886199 2.8493462 5.0809326 6.4956517 6.2083869 4.0685139 1.4322209 -1.0396683 -3.1268511 -3.92802 -3.7534053][-3.9880712 -2.1256285 -1.0078448 -0.130903 1.4525983 3.5634627 5.3122296 6.0687194 5.2660561 2.884202 0.14519882 -2.1610076 -3.9556928 -4.2474127 -3.5802979][-2.5018277 -0.75030351 -0.1230638 0.33503389 1.4638729 3.0339942 4.1591725 4.1801229 3.0235491 0.73281717 -1.6450962 -3.4955153 -4.9285569 -4.9257593 -4.1398907][-1.6016011 -0.087439537 0.046532631 -0.042138577 0.47282338 1.4570432 1.9211187 1.3715379 0.10831499 -1.7402093 -3.5829229 -4.9805584 -5.9699244 -5.7835426 -5.0496941][-1.8079002 -0.51002836 -0.57928014 -0.98393834 -1.1485513 -0.74873936 -0.76409495 -1.6465361 -2.8784142 -4.23552 -5.535851 -6.4638286 -6.9303212 -6.5899849 -5.939959][-2.948612 -1.7402905 -1.7583661 -2.3356414 -2.8818028 -2.8178196 -3.154366 -4.2554169 -5.4240112 -6.3420997 -7.16995 -7.5924535 -7.501967 -7.0658064 -6.5354848][-4.1629047 -2.9959588 -3.044426 -3.7156036 -4.2107124 -4.1403503 -4.5185614 -5.5766554 -6.5373192 -7.0195866 -7.418128 -7.4375858 -6.9654183 -6.5118351 -6.1398964][-5.0406647 -4.007638 -4.1695795 -4.6009097 -4.6123571 -4.4688144 -4.8082323 -5.5889835 -6.1886582 -6.3259287 -6.416729 -6.163352 -5.545475 -5.1888847 -5.0189667]]...]
INFO - root - 2017-12-15 08:42:13.524711: step 41510, loss = 0.23, batch loss = 0.19 (34.9 examples/sec; 0.230 sec/batch; 18h:33m:13s remains)
INFO - root - 2017-12-15 08:42:15.766575: step 41520, loss = 0.20, batch loss = 0.16 (35.6 examples/sec; 0.225 sec/batch; 18h:09m:38s remains)
INFO - root - 2017-12-15 08:42:18.040423: step 41530, loss = 0.26, batch loss = 0.23 (35.3 examples/sec; 0.227 sec/batch; 18h:19m:44s remains)
INFO - root - 2017-12-15 08:42:20.315014: step 41540, loss = 0.22, batch loss = 0.19 (34.9 examples/sec; 0.229 sec/batch; 18h:31m:37s remains)
INFO - root - 2017-12-15 08:42:22.592591: step 41550, loss = 0.23, batch loss = 0.20 (36.2 examples/sec; 0.221 sec/batch; 17h:52m:46s remains)
INFO - root - 2017-12-15 08:42:24.870855: step 41560, loss = 0.16, batch loss = 0.13 (36.1 examples/sec; 0.222 sec/batch; 17h:55m:42s remains)
INFO - root - 2017-12-15 08:42:27.152494: step 41570, loss = 0.30, batch loss = 0.26 (34.3 examples/sec; 0.233 sec/batch; 18h:50m:55s remains)
INFO - root - 2017-12-15 08:42:29.438303: step 41580, loss = 0.18, batch loss = 0.15 (35.5 examples/sec; 0.225 sec/batch; 18h:11m:39s remains)
INFO - root - 2017-12-15 08:42:31.693821: step 41590, loss = 0.20, batch loss = 0.17 (36.0 examples/sec; 0.222 sec/batch; 17h:57m:48s remains)
INFO - root - 2017-12-15 08:42:33.962749: step 41600, loss = 0.23, batch loss = 0.19 (35.6 examples/sec; 0.225 sec/batch; 18h:09m:58s remains)
2017-12-15 08:42:34.249811: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1743298 -5.35124 -5.3965564 -5.3755875 -5.0530615 -4.7594781 -4.5851336 -4.6394773 -4.8092175 -4.9809008 -5.107234 -5.1280336 -5.0535555 -4.9153881 -4.5742455][-4.4863319 -6.2693081 -6.3473845 -6.3107319 -5.7492929 -5.1186934 -4.7217646 -4.7205515 -5.0150023 -5.3447189 -5.7163372 -5.9872766 -6.0636549 -6.0122643 -5.6226244][-4.6228175 -6.1209555 -6.31057 -6.2027917 -5.3648138 -4.4607677 -3.9502473 -3.9491181 -4.4384537 -4.9799204 -5.6121168 -6.1583643 -6.36112 -6.3564405 -5.9332581][-5.1665421 -6.1360588 -6.1715641 -5.57946 -4.368207 -3.2237697 -2.4580352 -2.3176563 -2.9682505 -3.7670991 -4.6646905 -5.555326 -6.051733 -6.1867037 -5.8627672][-5.7080574 -6.0247507 -5.6176143 -4.3365726 -2.6990683 -1.1284186 0.09610343 0.39549708 -0.53769815 -1.7128837 -2.8680704 -4.1246653 -4.9076643 -5.2247772 -5.1811476][-6.2035336 -6.0473185 -5.0403719 -2.9035275 -0.58586633 1.7359238 3.4932253 3.7432373 2.2206075 0.34854412 -1.3284793 -2.9355986 -3.9741554 -4.3853498 -4.4385548][-6.468255 -6.1543722 -4.6758876 -2.0569952 0.76702142 3.8408158 6.0097237 6.0968637 4.0374374 1.3985703 -0.93873394 -2.7727118 -3.9270549 -4.2719641 -4.3062811][-6.2729187 -5.7344284 -4.4803705 -2.092608 0.72846246 4.0467806 6.3481293 6.49354 4.422596 1.4891982 -1.1554828 -2.9745252 -4.2394009 -4.6989784 -4.7278509][-5.2971 -4.8116465 -4.1533003 -2.3698022 0.018675804 2.9932611 4.9783354 5.1683092 3.5017903 1.0216119 -1.3338838 -2.7691112 -3.9655528 -4.55767 -4.6894865][-3.9061646 -3.7261572 -3.809227 -2.6821575 -0.92586231 1.2610795 2.6465614 2.7012827 1.4544263 -0.48478138 -2.2611113 -2.9962535 -3.9235909 -4.5315437 -4.6620092][-3.2051802 -3.3206606 -3.8658006 -3.1486096 -1.9470699 -0.50316072 0.31067967 0.15386677 -0.7689513 -2.4387827 -3.6973629 -3.9963737 -4.6982222 -5.1376047 -5.0771718][-3.3464782 -3.3473296 -4.0658264 -3.6729999 -2.9448009 -2.19324 -1.8377309 -2.0208015 -2.6618071 -4.11316 -4.9873095 -5.2493248 -5.9299 -6.3360939 -5.9383392][-3.9272461 -3.743783 -4.4069691 -4.1273918 -3.6730559 -3.4006438 -3.4627824 -3.7558827 -4.3285561 -5.3279819 -5.920536 -6.3226318 -7.0855522 -7.4057722 -6.6637506][-5.0351362 -4.5856104 -4.8225021 -4.4083753 -3.9230947 -3.9573522 -4.4246988 -4.8473873 -5.327908 -5.9390965 -6.4556313 -6.9696026 -7.7405386 -8.0424767 -7.1730528][-6.3559084 -5.6306672 -5.4112606 -4.7801352 -4.2656064 -4.3663788 -4.8941579 -5.3424215 -5.7863064 -6.4198036 -7.10546 -7.6835928 -8.2951593 -8.3801451 -7.3519735]]...]
INFO - root - 2017-12-15 08:42:36.519460: step 41610, loss = 0.17, batch loss = 0.14 (34.6 examples/sec; 0.231 sec/batch; 18h:41m:05s remains)
INFO - root - 2017-12-15 08:42:38.789638: step 41620, loss = 0.43, batch loss = 0.40 (36.0 examples/sec; 0.222 sec/batch; 17h:56m:35s remains)
INFO - root - 2017-12-15 08:42:41.069194: step 41630, loss = 0.25, batch loss = 0.22 (35.7 examples/sec; 0.224 sec/batch; 18h:06m:03s remains)
INFO - root - 2017-12-15 08:42:43.391546: step 41640, loss = 0.24, batch loss = 0.21 (35.9 examples/sec; 0.223 sec/batch; 17h:58m:53s remains)
INFO - root - 2017-12-15 08:42:45.661374: step 41650, loss = 0.27, batch loss = 0.24 (36.0 examples/sec; 0.222 sec/batch; 17h:57m:17s remains)
INFO - root - 2017-12-15 08:42:47.938123: step 41660, loss = 0.20, batch loss = 0.17 (34.7 examples/sec; 0.231 sec/batch; 18h:38m:45s remains)
INFO - root - 2017-12-15 08:42:50.198291: step 41670, loss = 0.26, batch loss = 0.23 (35.5 examples/sec; 0.225 sec/batch; 18h:10m:56s remains)
INFO - root - 2017-12-15 08:42:52.475753: step 41680, loss = 0.36, batch loss = 0.33 (35.8 examples/sec; 0.224 sec/batch; 18h:04m:28s remains)
INFO - root - 2017-12-15 08:42:54.771735: step 41690, loss = 0.25, batch loss = 0.22 (34.6 examples/sec; 0.231 sec/batch; 18h:41m:09s remains)
INFO - root - 2017-12-15 08:42:57.065726: step 41700, loss = 0.36, batch loss = 0.33 (34.8 examples/sec; 0.230 sec/batch; 18h:35m:42s remains)
2017-12-15 08:42:57.361134: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.26783 -5.3075504 -6.0002022 -6.3935881 -6.4963903 -6.3491898 -5.9920111 -5.5239272 -5.1273265 -5.7497835 -6.0418959 -5.7437439 -5.9147568 -5.5710649 -4.5850105][-4.5308151 -6.0186548 -6.6710148 -6.9032507 -6.8276806 -6.5603447 -6.0729604 -5.4393859 -4.9825721 -5.5746441 -6.1662731 -6.4548092 -6.7827311 -6.309103 -5.0778146][-5.546361 -6.2284164 -6.7841177 -6.7815981 -6.4469395 -5.9653893 -5.2671375 -4.4357853 -3.984112 -4.7753038 -5.8734202 -6.6941366 -7.3020554 -6.8708172 -5.4590664][-5.9868474 -5.9680133 -6.4221725 -6.1603346 -5.5462174 -4.7335653 -3.7264285 -2.8795485 -2.6334667 -3.6536229 -5.1467819 -6.4797955 -7.339901 -7.0366583 -5.6243496][-5.6645174 -5.4308233 -5.8126431 -5.2794657 -4.33097 -3.0470104 -1.5122452 -0.5045352 -0.41665852 -1.8220313 -3.8168437 -5.5416164 -6.6586409 -6.7086296 -5.5427675][-5.2922263 -4.7184286 -4.939374 -4.0893416 -2.9087684 -1.1208061 1.2543528 2.736141 2.7223356 0.83414364 -1.9215387 -4.245008 -5.7561331 -6.1527214 -5.3004704][-4.4291658 -4.1015024 -4.1133547 -3.0205 -1.6939864 0.56377935 3.6867721 5.6913643 5.7133522 3.476311 -0.041646481 -3.0433288 -5.0019174 -5.7171221 -5.0965166][-4.5526848 -4.1576734 -4.0442834 -2.7664876 -1.2469023 1.1814053 4.5370169 6.7069035 6.7114134 4.2750092 0.41383004 -2.7664223 -4.8848381 -5.6360407 -5.0156474][-4.9162827 -4.4938245 -4.3558178 -3.0701714 -1.4876525 0.76505613 3.5494468 5.2480221 5.2430534 3.1496303 -0.46080792 -3.413713 -5.2278938 -5.7596507 -5.0627146][-5.2392106 -4.8550148 -4.9110389 -3.87375 -2.4064698 -0.58255279 1.36152 2.4732769 2.3994219 0.75632763 -2.113905 -4.4032884 -5.8577795 -6.1440792 -5.2266383][-5.5816813 -5.274909 -5.6002054 -5.0190058 -3.8905327 -2.5029197 -1.3005779 -0.7067337 -0.78657806 -1.9496541 -3.9166777 -5.4164844 -6.3241568 -6.2497139 -5.2466927][-5.541173 -5.3416796 -5.8993511 -5.729908 -5.0136623 -4.0581589 -3.3411441 -2.9700575 -2.9947639 -3.8572264 -5.1212692 -5.8464041 -6.3848419 -6.1414175 -5.1357279][-5.08329 -5.0384517 -5.7913518 -5.937181 -5.5624814 -4.9090376 -4.5402722 -4.3869267 -4.3303518 -5.0149488 -5.6500278 -5.7782536 -6.2231874 -5.90801 -4.9702978][-4.0997553 -4.1079884 -5.0256214 -5.4503288 -5.4394293 -5.1718931 -5.0098796 -4.83871 -4.617691 -5.2776184 -5.6336317 -5.3896589 -5.8880367 -5.6509056 -4.8093662][-2.9937332 -2.8990712 -3.8866844 -4.5361052 -4.8820095 -4.9068384 -4.7962132 -4.5839252 -4.3377962 -5.1884089 -5.4460983 -4.9186158 -5.5786695 -5.4830723 -4.6842241]]...]
INFO - root - 2017-12-15 08:42:59.656418: step 41710, loss = 0.20, batch loss = 0.17 (34.3 examples/sec; 0.233 sec/batch; 18h:50m:00s remains)
INFO - root - 2017-12-15 08:43:01.920500: step 41720, loss = 0.21, batch loss = 0.17 (35.2 examples/sec; 0.227 sec/batch; 18h:21m:08s remains)
INFO - root - 2017-12-15 08:43:04.193140: step 41730, loss = 0.19, batch loss = 0.16 (36.6 examples/sec; 0.219 sec/batch; 17h:39m:38s remains)
INFO - root - 2017-12-15 08:43:06.505326: step 41740, loss = 0.27, batch loss = 0.24 (33.4 examples/sec; 0.239 sec/batch; 19h:20m:07s remains)
INFO - root - 2017-12-15 08:43:08.770896: step 41750, loss = 0.22, batch loss = 0.18 (35.3 examples/sec; 0.226 sec/batch; 18h:17m:04s remains)
INFO - root - 2017-12-15 08:43:11.036273: step 41760, loss = 0.20, batch loss = 0.16 (35.4 examples/sec; 0.226 sec/batch; 18h:15m:13s remains)
INFO - root - 2017-12-15 08:43:13.313246: step 41770, loss = 0.34, batch loss = 0.31 (36.2 examples/sec; 0.221 sec/batch; 17h:49m:37s remains)
INFO - root - 2017-12-15 08:43:15.595621: step 41780, loss = 0.22, batch loss = 0.19 (33.9 examples/sec; 0.236 sec/batch; 19h:02m:14s remains)
INFO - root - 2017-12-15 08:43:17.874942: step 41790, loss = 0.22, batch loss = 0.19 (34.6 examples/sec; 0.231 sec/batch; 18h:40m:53s remains)
INFO - root - 2017-12-15 08:43:20.147040: step 41800, loss = 0.27, batch loss = 0.23 (36.1 examples/sec; 0.222 sec/batch; 17h:54m:38s remains)
2017-12-15 08:43:20.431866: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5050383 -4.1135864 -4.1819887 -4.4653025 -5.2263284 -6.0472379 -5.8361778 -5.5257716 -5.34402 -5.330297 -4.9863796 -5.0463772 -5.5767045 -5.5817862 -5.602222][-4.7693992 -4.3487959 -4.24188 -4.4552727 -5.0643258 -5.4283733 -4.8783731 -4.4496765 -4.2059317 -4.3870764 -4.5447955 -5.1061797 -5.9641838 -6.2049608 -6.3044076][-5.8193026 -4.35704 -4.1005335 -4.0664577 -4.1347408 -3.6387489 -2.5687959 -2.0803237 -1.9498358 -2.5516596 -3.5382314 -4.7502074 -5.92867 -6.4370947 -6.6624613][-6.5401363 -3.851068 -3.4312134 -3.0001168 -2.3892126 -1.1780902 0.18090391 0.53254151 0.4602375 -0.6792767 -2.4801736 -4.1647677 -5.5117884 -6.1186924 -6.3864875][-6.308506 -3.1654353 -2.8779483 -2.19672 -1.0146332 0.7704041 2.4176564 2.8303828 2.6761198 1.1595089 -1.1889348 -3.0961366 -4.7044296 -5.45739 -5.7802086][-5.1717167 -2.65441 -2.4302449 -1.399014 0.36442065 2.4468613 4.304255 4.8639121 4.6570835 2.7671313 0.23053765 -1.7094531 -3.5504627 -4.5851221 -4.9926891][-3.7429113 -2.0383635 -1.8970463 -0.72579336 1.1089058 2.7895966 4.3578668 4.9648623 4.7701907 2.7854118 0.59335995 -0.96172071 -2.7552426 -3.84239 -4.0614533][-3.6517062 -1.94156 -1.949914 -0.8902384 0.50817776 1.4400172 2.5870261 3.3319626 3.3123016 1.4299724 -0.21200562 -1.2283874 -2.7650108 -3.6167662 -3.3614249][-3.9900231 -2.406327 -2.4138298 -1.5999224 -0.74517429 -0.50121605 0.16489172 0.794477 0.51610541 -1.2713344 -2.2542624 -2.6859899 -3.7831459 -4.2743239 -3.5274351][-4.6265011 -2.9022439 -2.8314335 -2.2241061 -1.7987723 -2.1227548 -2.0072863 -1.8415941 -2.4181843 -4.0249076 -4.393363 -4.4427176 -5.0689526 -5.1048145 -4.0495782][-5.332356 -3.3337336 -3.176013 -2.7784634 -2.6401591 -3.2904463 -3.5372975 -3.6329055 -4.2347188 -5.33463 -5.2458816 -5.1725006 -5.6942005 -5.4791012 -4.5858603][-6.4410534 -4.3214922 -4.1368532 -3.7168217 -3.6369309 -4.4241667 -4.8174944 -5.0832567 -5.6032314 -6.1668749 -5.7566004 -5.6865768 -6.1709208 -5.8248434 -5.2994719][-7.5690756 -5.2453609 -4.8208656 -4.2806888 -4.1181021 -4.9399452 -5.4897723 -5.8685675 -6.2591319 -6.4141817 -5.8244314 -5.80385 -6.2774525 -6.0609045 -6.0847149][-8.01233 -5.5744767 -5.0123453 -4.4168005 -4.3573194 -5.2672567 -5.8156862 -6.0425124 -6.1244688 -6.051959 -5.4651542 -5.5010586 -6.0525627 -6.1048994 -6.5020833][-8.0695505 -5.7266464 -5.1859188 -4.78551 -5.0146551 -5.9536281 -6.3229027 -6.2466197 -6.1315975 -5.9479342 -5.36563 -5.3432751 -5.8942113 -6.1314926 -6.60136]]...]
INFO - root - 2017-12-15 08:43:22.721982: step 41810, loss = 0.20, batch loss = 0.16 (35.7 examples/sec; 0.224 sec/batch; 18h:05m:20s remains)
INFO - root - 2017-12-15 08:43:25.015366: step 41820, loss = 0.22, batch loss = 0.19 (33.7 examples/sec; 0.237 sec/batch; 19h:10m:09s remains)
INFO - root - 2017-12-15 08:43:27.282765: step 41830, loss = 0.30, batch loss = 0.26 (34.8 examples/sec; 0.230 sec/batch; 18h:34m:28s remains)
INFO - root - 2017-12-15 08:43:29.574238: step 41840, loss = 0.25, batch loss = 0.21 (36.1 examples/sec; 0.221 sec/batch; 17h:53m:00s remains)
INFO - root - 2017-12-15 08:43:31.875280: step 41850, loss = 0.39, batch loss = 0.36 (35.0 examples/sec; 0.229 sec/batch; 18h:28m:48s remains)
INFO - root - 2017-12-15 08:43:34.165089: step 41860, loss = 0.21, batch loss = 0.17 (35.8 examples/sec; 0.224 sec/batch; 18h:03m:42s remains)
INFO - root - 2017-12-15 08:43:36.428635: step 41870, loss = 0.21, batch loss = 0.18 (36.0 examples/sec; 0.222 sec/batch; 17h:55m:15s remains)
INFO - root - 2017-12-15 08:43:38.711738: step 41880, loss = 0.18, batch loss = 0.15 (35.4 examples/sec; 0.226 sec/batch; 18h:15m:24s remains)
INFO - root - 2017-12-15 08:43:41.040685: step 41890, loss = 0.17, batch loss = 0.14 (33.6 examples/sec; 0.238 sec/batch; 19h:14m:38s remains)
INFO - root - 2017-12-15 08:43:43.324885: step 41900, loss = 0.21, batch loss = 0.17 (35.7 examples/sec; 0.224 sec/batch; 18h:06m:03s remains)
2017-12-15 08:43:43.619923: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4755669 -6.0090914 -5.9744415 -5.4109974 -4.1843128 -3.1088567 -2.5780461 -2.7428913 -3.1118543 -3.2652369 -3.2109764 -3.7915025 -4.1959 -4.4958744 -4.2534113][-5.039094 -5.942008 -5.7326117 -5.1472216 -3.753448 -2.7484958 -2.4041185 -2.5826 -3.0760465 -3.6150827 -3.9090748 -4.4800653 -4.700417 -4.9959869 -4.7891092][-5.0433826 -5.1816635 -4.809103 -4.2408547 -2.736331 -1.7836007 -1.2936428 -1.3303072 -2.112571 -3.1148853 -3.9173927 -4.6055994 -4.7355127 -5.0363674 -4.8299313][-4.7847376 -4.2663231 -3.6070373 -2.9691038 -1.5131238 -0.64781523 0.087935209 0.35507107 -0.64561045 -2.2318146 -3.7539115 -4.6411204 -4.68505 -4.766458 -4.3849392][-5.0046015 -3.8998909 -2.8146069 -1.7343934 -0.20380068 0.56661367 1.2849197 1.5350516 0.20521116 -1.6563495 -3.5254612 -4.2730675 -4.0428257 -3.8778279 -3.4849162][-5.226264 -3.8211288 -2.4303291 -0.87151313 0.8552599 1.6927965 2.3239021 2.3481932 0.68155432 -1.4107599 -3.3772483 -3.8871841 -3.3912477 -2.8246069 -2.335952][-5.8645678 -4.2534075 -2.66436 -0.62376094 1.3033223 2.1659842 2.6492805 2.4552822 0.58719635 -1.5874338 -3.3279116 -3.4844933 -2.7144582 -1.8467085 -1.2565539][-6.0842891 -4.7005348 -3.3190143 -1.2535838 0.51817322 1.2752292 1.6566062 1.4032962 -0.20980453 -2.0920587 -3.4326444 -3.2346299 -2.2463412 -1.1658117 -0.44353914][-6.3930888 -4.9080868 -3.6746113 -1.9008262 -0.6457963 -0.24334693 -0.035115719 -0.26376081 -1.3643074 -2.7401235 -3.6879067 -3.1744094 -1.91801 -0.60659158 0.36206031][-6.9482889 -5.4846578 -4.2528706 -2.6297524 -1.7562206 -1.7094181 -1.719214 -1.9366828 -2.5786135 -3.5295238 -4.1215887 -3.2463706 -1.6877719 -0.050820351 1.2408166][-6.8998294 -5.9080353 -5.1956425 -4.0447 -3.3946352 -3.3267431 -3.2796135 -3.3087654 -3.5171716 -4.1971879 -4.665277 -3.7908678 -2.2122419 -0.29652941 1.231832][-6.3944216 -5.869482 -5.7095766 -5.0999069 -4.6908531 -4.6213121 -4.6048737 -4.4914169 -4.3042107 -4.6440153 -4.9880528 -4.3755708 -3.1301353 -1.2827055 0.14401126][-5.4982319 -5.3176908 -5.6025596 -5.45945 -5.3173757 -5.3610229 -5.4208221 -5.2926922 -4.9425616 -5.03839 -5.2494073 -4.7837796 -3.7493474 -2.0735304 -0.8949821][-4.3610916 -4.2863932 -4.8178797 -5.0930142 -5.3293748 -5.6330719 -5.8532276 -5.8154945 -5.4999018 -5.5140333 -5.693862 -5.3787551 -4.4697504 -2.9628198 -1.9256594][-3.5663877 -3.3877823 -3.850858 -4.2170825 -4.5570469 -4.9548683 -5.3526039 -5.6546249 -5.6986165 -5.8028488 -5.9640722 -5.7762752 -5.0425506 -3.8637033 -3.1469097]]...]
INFO - root - 2017-12-15 08:43:45.872777: step 41910, loss = 0.20, batch loss = 0.17 (35.9 examples/sec; 0.223 sec/batch; 18h:00m:12s remains)
INFO - root - 2017-12-15 08:43:48.128449: step 41920, loss = 0.25, batch loss = 0.22 (35.4 examples/sec; 0.226 sec/batch; 18h:14m:19s remains)
INFO - root - 2017-12-15 08:43:50.415005: step 41930, loss = 0.20, batch loss = 0.17 (35.8 examples/sec; 0.224 sec/batch; 18h:02m:39s remains)
INFO - root - 2017-12-15 08:43:52.677811: step 41940, loss = 0.22, batch loss = 0.19 (35.6 examples/sec; 0.225 sec/batch; 18h:08m:29s remains)
INFO - root - 2017-12-15 08:43:54.948516: step 41950, loss = 0.28, batch loss = 0.25 (35.4 examples/sec; 0.226 sec/batch; 18h:13m:27s remains)
INFO - root - 2017-12-15 08:43:57.214708: step 41960, loss = 0.17, batch loss = 0.14 (35.8 examples/sec; 0.223 sec/batch; 18h:00m:48s remains)
INFO - root - 2017-12-15 08:43:59.512034: step 41970, loss = 0.25, batch loss = 0.21 (35.6 examples/sec; 0.225 sec/batch; 18h:07m:39s remains)
INFO - root - 2017-12-15 08:44:01.785286: step 41980, loss = 0.21, batch loss = 0.17 (34.0 examples/sec; 0.235 sec/batch; 18h:58m:33s remains)
INFO - root - 2017-12-15 08:44:04.064625: step 41990, loss = 0.21, batch loss = 0.18 (31.3 examples/sec; 0.255 sec/batch; 20h:36m:55s remains)
INFO - root - 2017-12-15 08:44:06.329766: step 42000, loss = 0.27, batch loss = 0.24 (35.5 examples/sec; 0.225 sec/batch; 18h:10m:01s remains)
2017-12-15 08:44:06.656575: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.279243 -6.1438923 -6.6178827 -6.6528282 -6.1290164 -5.7298923 -5.2907171 -3.9251771 -2.1987526 -0.92310762 0.34617496 1.1964853 0.62568021 -1.0366119 -2.4156604][-7.43931 -7.02606 -6.942358 -6.5412989 -5.7583737 -5.42861 -5.3814898 -4.3991232 -2.9991145 -1.8149726 -0.4948051 0.26901174 -0.26047885 -1.5041683 -2.3761921][-8.70359 -7.0389795 -6.3482046 -5.6191788 -4.6811657 -4.4385576 -4.8357487 -4.4605231 -3.6646914 -2.8646352 -1.7226236 -1.0285991 -1.3510008 -2.1518717 -2.5530326][-9.0902967 -6.359087 -5.2606235 -4.4938669 -3.5893965 -3.2872789 -3.8067722 -3.8104382 -3.6052747 -3.4534359 -2.7887185 -2.2817938 -2.2003033 -2.3309875 -2.3290455][-8.7356691 -5.3602324 -4.2303958 -3.558733 -2.5923324 -1.7933058 -1.733449 -1.5848415 -1.9008694 -2.6580827 -2.88685 -2.95369 -2.5801187 -2.0977252 -1.8930908][-7.8716373 -4.3227034 -3.3210368 -2.6083465 -1.4417064 0.012696266 1.0667882 1.8157685 1.1804245 -0.64534831 -1.9351346 -2.6634483 -2.2490692 -1.528055 -1.4904783][-6.7949038 -3.7089748 -2.8583491 -1.8296303 -0.33708048 1.6204143 3.7194555 5.2992249 4.467597 1.6180134 -0.75694966 -2.2976859 -2.1856914 -1.439662 -1.4956036][-6.4228992 -3.6956182 -2.8565595 -1.3458661 0.39244533 2.5492136 5.4154768 7.5333891 6.5285988 3.1714532 0.26624203 -1.9094477 -2.2985532 -1.7634063 -2.0279591][-6.5063362 -4.3193769 -3.3577046 -1.5108923 0.21662807 2.19508 5.2069464 7.2825756 6.3030539 3.2748759 0.49934673 -1.8752 -2.57925 -2.20256 -2.4575925][-6.59087 -4.850204 -3.8687296 -2.1354973 -0.75056589 0.73523164 3.1389773 4.5629444 3.6800554 1.4874766 -0.73685133 -2.7222979 -3.203022 -2.7162025 -2.6718218][-6.7211857 -5.281086 -4.3237743 -2.9330575 -1.9494606 -0.93588781 0.66823554 1.4272652 0.72657061 -0.59560513 -2.1693842 -3.6202583 -3.8100722 -3.0776455 -2.5348189][-6.7909813 -5.5976477 -4.840282 -3.9450552 -3.3107114 -2.585016 -1.6533656 -1.4397769 -1.9835683 -2.6545053 -3.5464573 -4.3069649 -4.1404028 -3.1331017 -2.226954][-6.413187 -5.448802 -4.9522758 -4.4923964 -4.1580429 -3.7554502 -3.3982487 -3.4912281 -3.8277593 -4.0424819 -4.4323568 -4.6739516 -4.3074884 -3.3424673 -2.4527349][-5.980958 -5.2042217 -4.9220948 -4.7171373 -4.5445924 -4.4215546 -4.446538 -4.6748066 -4.888011 -4.9327903 -4.976552 -4.8523235 -4.49842 -3.8121014 -3.2176979][-5.6842709 -5.0186558 -4.8799486 -4.8055277 -4.78141 -4.8695803 -5.0604 -5.2550049 -5.3345242 -5.288703 -5.1578722 -5.0019846 -4.8543749 -4.5021067 -4.215662]]...]
INFO - root - 2017-12-15 08:44:08.947146: step 42010, loss = 0.28, batch loss = 0.25 (34.6 examples/sec; 0.231 sec/batch; 18h:39m:41s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 08:44:11.247445: step 42020, loss = 0.20, batch loss = 0.17 (35.2 examples/sec; 0.227 sec/batch; 18h:19m:26s remains)
INFO - root - 2017-12-15 08:44:13.528178: step 42030, loss = 0.26, batch loss = 0.23 (33.5 examples/sec; 0.238 sec/batch; 19h:14m:36s remains)
INFO - root - 2017-12-15 08:44:15.842733: step 42040, loss = 0.32, batch loss = 0.29 (34.8 examples/sec; 0.230 sec/batch; 18h:33m:18s remains)
INFO - root - 2017-12-15 08:44:18.100578: step 42050, loss = 0.22, batch loss = 0.19 (34.0 examples/sec; 0.236 sec/batch; 19h:00m:38s remains)
INFO - root - 2017-12-15 08:44:20.383274: step 42060, loss = 0.26, batch loss = 0.23 (35.5 examples/sec; 0.225 sec/batch; 18h:09m:57s remains)
INFO - root - 2017-12-15 08:44:22.635383: step 42070, loss = 0.17, batch loss = 0.14 (35.8 examples/sec; 0.223 sec/batch; 18h:01m:01s remains)
INFO - root - 2017-12-15 08:44:24.953549: step 42080, loss = 0.19, batch loss = 0.16 (35.2 examples/sec; 0.227 sec/batch; 18h:19m:50s remains)
INFO - root - 2017-12-15 08:44:27.232281: step 42090, loss = 0.18, batch loss = 0.15 (35.9 examples/sec; 0.223 sec/batch; 17h:58m:57s remains)
INFO - root - 2017-12-15 08:44:29.498582: step 42100, loss = 0.32, batch loss = 0.29 (35.4 examples/sec; 0.226 sec/batch; 18h:13m:03s remains)
2017-12-15 08:44:29.791107: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1064181 -4.018146 -4.459507 -4.1639566 -3.3258009 -2.194324 -0.993188 -0.48341811 -0.40804529 -1.2668567 -2.7212765 -3.4461703 -3.7786083 -3.3671417 -2.771399][-2.2372391 -3.5500693 -3.9047253 -3.4309311 -2.79713 -1.9262248 -0.609249 -0.10105467 -0.35775816 -1.4349372 -2.6840479 -3.0340209 -3.2147224 -2.9458234 -2.3976157][-2.0473225 -2.4308414 -2.4289875 -1.8822583 -1.6598508 -1.343298 -0.30841208 0.022384882 -0.61246359 -1.8743081 -2.9249935 -2.8567348 -3.0390749 -3.1245205 -2.5412183][-1.7367365 -1.2942674 -0.8867532 -0.39677727 -0.60277617 -0.92122447 -0.43478227 -0.33538437 -1.3531725 -2.6307588 -3.5109625 -3.1911867 -3.2421608 -3.3686733 -2.6363227][-1.7368841 -0.53225338 0.3012836 0.84878254 0.54865074 -0.16948771 -0.062978983 0.00638628 -1.2191236 -2.5355406 -3.355952 -2.9940298 -2.8253922 -2.8240328 -2.1614203][-1.9053671 -0.50814652 0.8245492 1.8254583 2.0284083 1.4712338 1.5772166 1.9519131 0.72341752 -0.75932145 -1.7137699 -1.5499055 -1.3177102 -1.4072073 -1.0398326][-1.901422 -0.78345394 0.9734776 2.5299342 3.3902395 3.4546983 3.9702218 4.7154007 3.3330567 1.4891646 0.19683838 -0.039131403 0.29921532 0.16316056 0.26633263][-2.2722502 -1.1858449 0.69705749 2.345804 3.4712465 3.9989297 4.8368711 5.6802397 4.2389421 2.3278058 1.1238663 0.84137082 1.3142457 1.0893757 0.88041425][-2.9183519 -2.0402904 -0.504037 0.78074908 1.841706 2.4565394 3.2105134 3.7650826 2.5018108 1.0673697 0.53199172 0.70383263 1.3436089 1.0606108 0.71311331][-3.5530763 -3.1572838 -2.1846247 -1.4809222 -0.69993651 0.036699772 0.74811244 1.1234002 0.13884258 -0.75327563 -0.73812163 -0.27077842 0.45470357 0.22640872 -0.065640211][-4.3121185 -4.3407259 -3.9257884 -3.7253513 -3.1904862 -2.3055842 -1.5977011 -1.2295313 -1.8038538 -2.141896 -1.7457819 -1.173908 -0.52315938 -0.69811177 -1.0156753][-4.4518623 -4.754612 -4.858613 -5.1018248 -4.8198433 -3.9873366 -3.3813081 -2.9659085 -3.2342587 -3.232517 -2.6427002 -2.01925 -1.4343715 -1.4473031 -1.7306137][-3.8715572 -4.2189088 -4.649826 -5.178689 -5.2177377 -4.700428 -4.3798218 -4.02596 -4.1854105 -4.1511006 -3.70021 -3.1828814 -2.6305568 -2.4178348 -2.5099673][-3.1749723 -3.3020594 -3.7951841 -4.3942928 -4.651412 -4.4996309 -4.498971 -4.2817016 -4.4428835 -4.5857086 -4.4455547 -4.0981846 -3.6601644 -3.4037228 -3.456563][-2.6130636 -2.4488015 -2.8053775 -3.3469369 -3.7071474 -3.8762472 -4.0694151 -3.9904222 -4.1513114 -4.4737167 -4.6933432 -4.6202297 -4.3636665 -4.1123638 -4.1696134]]...]
INFO - root - 2017-12-15 08:44:32.066757: step 42110, loss = 0.24, batch loss = 0.21 (34.8 examples/sec; 0.230 sec/batch; 18h:32m:03s remains)
INFO - root - 2017-12-15 08:44:34.331139: step 42120, loss = 0.22, batch loss = 0.19 (36.0 examples/sec; 0.222 sec/batch; 17h:55m:28s remains)
INFO - root - 2017-12-15 08:44:36.586225: step 42130, loss = 0.29, batch loss = 0.26 (34.7 examples/sec; 0.230 sec/batch; 18h:35m:21s remains)
INFO - root - 2017-12-15 08:44:38.889709: step 42140, loss = 0.37, batch loss = 0.34 (33.4 examples/sec; 0.240 sec/batch; 19h:19m:52s remains)
INFO - root - 2017-12-15 08:44:41.156431: step 42150, loss = 0.16, batch loss = 0.13 (34.8 examples/sec; 0.230 sec/batch; 18h:33m:51s remains)
INFO - root - 2017-12-15 08:44:43.483927: step 42160, loss = 0.25, batch loss = 0.21 (33.7 examples/sec; 0.237 sec/batch; 19h:08m:17s remains)
INFO - root - 2017-12-15 08:44:45.737374: step 42170, loss = 0.21, batch loss = 0.17 (35.3 examples/sec; 0.227 sec/batch; 18h:16m:12s remains)
INFO - root - 2017-12-15 08:44:48.016642: step 42180, loss = 0.21, batch loss = 0.18 (36.4 examples/sec; 0.220 sec/batch; 17h:43m:49s remains)
INFO - root - 2017-12-15 08:44:50.286960: step 42190, loss = 0.25, batch loss = 0.22 (34.4 examples/sec; 0.232 sec/batch; 18h:44m:11s remains)
INFO - root - 2017-12-15 08:44:52.531283: step 42200, loss = 0.31, batch loss = 0.27 (35.0 examples/sec; 0.228 sec/batch; 18h:25m:22s remains)
2017-12-15 08:44:52.808571: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5233698 -4.42323 -3.9601464 -3.0816808 -2.0478415 -1.1893976 -1.2662381 -1.6368884 -1.9019706 -2.6694794 -3.2308915 -3.2798746 -3.0501125 -2.53884 -2.9240041][-2.3891318 -3.6001735 -3.3095002 -2.5035627 -1.3638413 -0.41397655 -0.31343043 -0.42764902 -0.54906738 -1.326905 -1.9958048 -2.0174129 -1.7596327 -1.2415767 -1.4898415][-2.0080328 -2.7275612 -2.7100711 -2.1906936 -1.3131652 -0.47288942 -0.19640374 0.012875319 0.098365068 -0.56317437 -1.2187505 -1.1118032 -0.68484342 0.0058357716 0.088080406][-2.5411723 -2.6586678 -2.7386675 -2.3598738 -1.683542 -0.9087224 -0.3777647 0.13497734 0.40570545 -0.18961644 -0.91967213 -0.88035059 -0.48688614 0.25183916 0.71193695][-3.4091702 -2.9754744 -2.9715397 -2.5007794 -1.8411205 -0.957739 -0.18260622 0.44272876 0.74479914 0.090641975 -0.73167014 -0.98256087 -0.81584334 -0.18740249 0.46752429][-4.2377734 -3.3304396 -3.0898824 -2.4617531 -1.7328169 -0.51083291 0.7041893 1.5311852 1.791832 0.93747354 -0.17092609 -0.87504447 -1.0696597 -0.72343087 -0.048276424][-4.3790083 -3.5113869 -3.1169248 -2.397151 -1.5636173 0.052243948 1.7593141 2.814559 2.9745798 1.7912593 0.2113657 -1.028398 -1.5600088 -1.4483938 -0.78701544][-4.6209016 -3.4172184 -2.9023931 -2.1163692 -1.2410526 0.58135271 2.558742 3.7369442 3.7505136 2.2603116 0.30443931 -1.2253497 -1.9911865 -2.2087955 -1.6641555][-4.5135064 -3.00986 -2.3917444 -1.7131848 -1.0387644 0.68037319 2.6825838 3.8521352 3.7242403 1.9971237 -0.25110173 -1.9896472 -2.9471145 -3.4077632 -2.929275][-4.5460043 -3.0513246 -2.6078789 -2.22733 -2.0090208 -0.6027106 1.2960076 2.4160261 2.2452536 0.54514027 -1.6074339 -3.2271769 -4.1462679 -4.6163945 -4.2058563][-5.0569634 -3.7715945 -3.47898 -3.3614902 -3.6001215 -2.6702313 -1.1006597 -0.10262775 -0.22926688 -1.749923 -3.6242518 -4.9940195 -5.7315388 -5.9784102 -5.5513434][-5.6311231 -4.5590038 -4.4206486 -4.5094161 -5.0977044 -4.6654596 -3.5187621 -2.6481025 -2.7097383 -3.9831676 -5.4437456 -6.5192394 -7.0465069 -7.1072874 -6.7073832][-5.7314186 -4.84824 -4.7679071 -4.9960265 -5.9107046 -6.021347 -5.3514538 -4.5929208 -4.404007 -5.0931921 -5.9449978 -6.7331262 -7.1479926 -7.2227077 -7.0790124][-5.0374513 -4.3047113 -4.3764005 -4.7833772 -5.9136882 -6.5012903 -6.3759 -5.7864766 -5.3260269 -5.3943491 -5.6020083 -6.0554 -6.4201589 -6.52656 -6.6476488][-4.0093241 -3.5045218 -3.7895885 -4.3297558 -5.5155654 -6.4315238 -6.7307172 -6.3147078 -5.651906 -5.3192625 -5.0579157 -5.1668787 -5.3283067 -5.3863106 -5.7051377]]...]
INFO - root - 2017-12-15 08:44:55.110550: step 42210, loss = 0.25, batch loss = 0.22 (35.2 examples/sec; 0.227 sec/batch; 18h:18m:58s remains)
INFO - root - 2017-12-15 08:44:57.425061: step 42220, loss = 0.20, batch loss = 0.17 (34.7 examples/sec; 0.230 sec/batch; 18h:34m:23s remains)
INFO - root - 2017-12-15 08:44:59.695029: step 42230, loss = 0.27, batch loss = 0.23 (33.6 examples/sec; 0.238 sec/batch; 19h:12m:50s remains)
INFO - root - 2017-12-15 08:45:01.964246: step 42240, loss = 0.24, batch loss = 0.20 (35.8 examples/sec; 0.224 sec/batch; 18h:01m:40s remains)
INFO - root - 2017-12-15 08:45:04.255253: step 42250, loss = 0.26, batch loss = 0.23 (33.8 examples/sec; 0.237 sec/batch; 19h:04m:32s remains)
INFO - root - 2017-12-15 08:45:06.528925: step 42260, loss = 0.34, batch loss = 0.30 (34.3 examples/sec; 0.233 sec/batch; 18h:47m:38s remains)
INFO - root - 2017-12-15 08:45:08.765289: step 42270, loss = 0.21, batch loss = 0.18 (35.7 examples/sec; 0.224 sec/batch; 18h:03m:34s remains)
INFO - root - 2017-12-15 08:45:11.026589: step 42280, loss = 0.20, batch loss = 0.17 (34.9 examples/sec; 0.229 sec/batch; 18h:28m:09s remains)
INFO - root - 2017-12-15 08:45:13.318378: step 42290, loss = 0.21, batch loss = 0.17 (32.9 examples/sec; 0.243 sec/batch; 19h:36m:20s remains)
INFO - root - 2017-12-15 08:45:15.594109: step 42300, loss = 0.19, batch loss = 0.15 (35.2 examples/sec; 0.228 sec/batch; 18h:20m:44s remains)
2017-12-15 08:45:15.900056: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9975572 -6.952322 -6.7163105 -6.7113008 -7.085288 -6.649437 -6.4530268 -7.051424 -7.0353384 -6.7067175 -6.6008158 -6.599493 -6.9123335 -6.4393244 -5.5255327][-5.4056211 -6.229578 -6.1909142 -6.1054468 -6.1839695 -5.5358582 -5.3395524 -6.1453137 -6.236207 -6.1587238 -6.4450626 -6.7222538 -7.2252789 -6.7704659 -5.7086029][-5.3702965 -5.6117907 -5.6959248 -5.3791084 -4.9651675 -4.00846 -3.6123719 -4.4396172 -4.8402929 -5.2841997 -6.0693045 -6.5603838 -7.0511732 -6.5638123 -5.4250379][-5.1628394 -5.0800123 -5.3172474 -4.62977 -3.4534779 -2.1444936 -1.5537713 -2.2963803 -3.2288103 -4.4765882 -5.8914995 -6.7714634 -7.3047247 -6.6924696 -5.390317][-3.8419323 -3.4412708 -3.8315506 -3.0163865 -1.3683798 0.052572727 0.74442363 0.085560083 -1.3186963 -3.3580372 -5.4155984 -6.6948385 -7.31711 -6.7715187 -5.4404421][-2.6203341 -1.8060987 -1.9919682 -0.91600227 1.0321507 2.4083989 3.1464217 2.4785502 0.61590624 -1.9234864 -4.5094151 -6.2476735 -7.08374 -6.7090969 -5.4578376][-1.5027429 -0.60857773 -0.54585469 0.81459332 2.9156263 4.31505 5.2564812 4.6267147 2.4635322 -0.22750759 -3.0802493 -5.2089777 -6.3817596 -6.3467236 -5.3767934][-1.5624244 -0.41833794 -0.014179707 1.5589151 3.6412904 5.2246809 6.4563389 5.8796225 3.6671793 1.11742 -1.720062 -4.0262084 -5.4523163 -5.7903957 -4.9919391][-2.3640423 -0.98951852 -0.40777874 1.2331939 3.0881369 4.7990274 6.2186689 5.5431614 3.4835551 1.2686801 -1.4738195 -3.6878128 -5.0255537 -5.5092821 -4.7048349][-3.5821769 -1.8696599 -1.3211772 -0.044253349 1.289623 2.9339797 4.225317 3.3776491 1.5256734 -0.35416186 -2.8641386 -4.7637029 -5.7679224 -6.1319518 -5.0577221][-4.777288 -3.0350533 -2.6630995 -1.8482153 -1.1222988 0.25853133 1.3481677 0.42778659 -1.0734954 -2.5432038 -4.7872453 -6.3886633 -7.0591373 -7.3089905 -5.9097385][-4.9550791 -3.7203503 -3.8217094 -3.6690755 -3.6568055 -2.7779076 -1.9974153 -2.7599697 -3.7245302 -4.522862 -6.2119694 -7.4373732 -7.7848148 -7.9638233 -6.4823961][-5.0579195 -4.2158875 -4.6662359 -5.096096 -5.6128454 -5.2880139 -4.9105635 -5.4931879 -5.998517 -6.2998285 -7.4171057 -8.28268 -8.3813887 -8.2613869 -6.7369995][-5.9580445 -5.2370472 -5.6445904 -6.1124964 -6.6145754 -6.469449 -6.3339586 -6.805232 -7.1426964 -7.3635073 -8.2022 -8.9022932 -8.9224529 -8.5490808 -6.9990864][-7.446682 -6.6608191 -6.6131916 -6.5500937 -6.6052685 -6.3679743 -6.2376919 -6.5742397 -6.951005 -7.314991 -8.0414686 -8.6615849 -8.6913576 -8.2424841 -6.8245783]]...]
INFO - root - 2017-12-15 08:45:18.224383: step 42310, loss = 0.22, batch loss = 0.19 (32.2 examples/sec; 0.248 sec/batch; 20h:01m:35s remains)
INFO - root - 2017-12-15 08:45:20.502198: step 42320, loss = 0.17, batch loss = 0.14 (34.6 examples/sec; 0.231 sec/batch; 18h:38m:11s remains)
INFO - root - 2017-12-15 08:45:22.813476: step 42330, loss = 0.27, batch loss = 0.23 (35.1 examples/sec; 0.228 sec/batch; 18h:20m:51s remains)
INFO - root - 2017-12-15 08:45:25.139004: step 42340, loss = 0.29, batch loss = 0.26 (34.6 examples/sec; 0.231 sec/batch; 18h:37m:28s remains)
INFO - root - 2017-12-15 08:45:27.405935: step 42350, loss = 0.29, batch loss = 0.26 (34.9 examples/sec; 0.229 sec/batch; 18h:27m:53s remains)
INFO - root - 2017-12-15 08:45:29.675591: step 42360, loss = 0.20, batch loss = 0.17 (34.7 examples/sec; 0.231 sec/batch; 18h:35m:19s remains)
INFO - root - 2017-12-15 08:45:31.945499: step 42370, loss = 0.23, batch loss = 0.19 (36.2 examples/sec; 0.221 sec/batch; 17h:48m:50s remains)
INFO - root - 2017-12-15 08:45:34.197293: step 42380, loss = 0.28, batch loss = 0.25 (34.7 examples/sec; 0.231 sec/batch; 18h:35m:17s remains)
INFO - root - 2017-12-15 08:45:36.459672: step 42390, loss = 0.18, batch loss = 0.15 (36.1 examples/sec; 0.221 sec/batch; 17h:50m:03s remains)
INFO - root - 2017-12-15 08:45:38.774153: step 42400, loss = 0.32, batch loss = 0.28 (34.0 examples/sec; 0.235 sec/batch; 18h:57m:05s remains)
2017-12-15 08:45:39.077106: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5407653 -6.8464203 -7.2100716 -7.5976667 -7.7458811 -7.4467936 -6.9441481 -6.774581 -6.9128227 -7.1078491 -7.3528423 -7.5266171 -7.5524693 -7.33858 -6.853673][-6.4420681 -8.0316324 -8.4734154 -8.7002268 -8.5233555 -7.7505217 -6.7071705 -6.3600817 -6.7454519 -7.223815 -7.8685656 -8.3060369 -8.4367514 -8.250638 -7.6694164][-8.22788 -8.8350019 -9.2079191 -9.1814356 -8.7632446 -7.6415024 -6.1542397 -5.7998276 -6.588728 -7.4536915 -8.6605864 -9.4639568 -9.7840128 -9.6087 -8.7176285][-8.7349634 -8.6342869 -8.9390278 -8.6917305 -8.1082516 -6.7439957 -5.0042787 -4.7885804 -5.989572 -7.1375027 -8.8242989 -9.8357935 -10.204965 -10.17802 -9.1725826][-8.66828 -7.9855585 -7.9160881 -7.1021233 -6.0586252 -4.21257 -2.0109615 -1.6012337 -3.1085651 -4.5954313 -6.8626165 -8.2547073 -9.0641451 -9.62367 -8.8398466][-8.8360357 -7.4022236 -6.8140411 -5.30258 -3.6904953 -1.1821088 1.685446 2.468735 0.64486194 -1.267662 -4.3294773 -6.4129677 -7.9991589 -9.270813 -8.6987619][-7.9146624 -6.3533669 -5.4302549 -3.5905013 -1.8329852 0.92379022 4.1950703 5.2550621 3.0685456 0.80187845 -2.8296585 -5.4796715 -7.6892357 -9.4504519 -8.97067][-7.4342346 -5.5760155 -4.4803276 -2.5775819 -0.72208369 2.1866519 5.6979065 6.8448362 4.2247286 1.8068483 -2.0880864 -5.0659924 -7.7012606 -9.7664509 -9.4677668][-7.7369595 -6.0576782 -5.1730042 -3.5361161 -1.7265456 1.2295401 4.7228231 5.791358 3.0966384 0.97141647 -2.6950746 -5.643908 -8.4153156 -10.36258 -9.9611492][-8.4631529 -7.1608953 -6.6852579 -5.6353312 -4.2359285 -1.44115 1.7091744 2.5077798 0.0078101158 -1.7183564 -5.0099897 -7.6483173 -9.9649963 -11.238329 -10.389701][-9.0381508 -8.0928116 -8.001049 -7.4047623 -6.2482071 -3.5124793 -0.6813482 -0.18567252 -2.504112 -4.0284977 -6.88498 -9.1044388 -10.929717 -11.46008 -10.253381][-9.305603 -8.5903845 -8.6991825 -8.4102163 -7.5909767 -5.3851166 -3.298326 -3.122468 -4.8356643 -5.912693 -8.0885267 -9.7420826 -11.045252 -11.040987 -9.7906342][-9.3901882 -8.8003836 -8.9987669 -8.9770336 -8.6264114 -7.2908459 -6.0418015 -6.0212212 -6.9478235 -7.5628862 -8.99671 -10.03801 -10.784693 -10.564701 -9.4369621][-8.8478909 -8.3426485 -8.5894375 -8.6910448 -8.5387812 -7.7562981 -6.9599257 -6.8751831 -7.30779 -7.6859074 -8.5612059 -9.2154675 -9.6541376 -9.43792 -8.5817871][-7.7731676 -7.2579937 -7.4454842 -7.4909487 -7.3530893 -6.985671 -6.676466 -6.7138758 -6.9172421 -7.1583786 -7.618844 -7.9924116 -8.2273321 -8.0051165 -7.3749676]]...]
INFO - root - 2017-12-15 08:45:41.327457: step 42410, loss = 0.27, batch loss = 0.24 (35.5 examples/sec; 0.225 sec/batch; 18h:08m:32s remains)
INFO - root - 2017-12-15 08:45:43.564670: step 42420, loss = 0.27, batch loss = 0.24 (35.6 examples/sec; 0.225 sec/batch; 18h:06m:59s remains)
INFO - root - 2017-12-15 08:45:45.818617: step 42430, loss = 0.34, batch loss = 0.31 (37.0 examples/sec; 0.216 sec/batch; 17h:24m:50s remains)
INFO - root - 2017-12-15 08:45:48.121447: step 42440, loss = 0.20, batch loss = 0.17 (35.2 examples/sec; 0.227 sec/batch; 18h:19m:07s remains)
INFO - root - 2017-12-15 08:45:50.441889: step 42450, loss = 0.27, batch loss = 0.24 (34.6 examples/sec; 0.231 sec/batch; 18h:37m:07s remains)
INFO - root - 2017-12-15 08:45:52.677787: step 42460, loss = 0.26, batch loss = 0.22 (34.8 examples/sec; 0.230 sec/batch; 18h:29m:55s remains)
INFO - root - 2017-12-15 08:45:55.002154: step 42470, loss = 0.15, batch loss = 0.12 (33.7 examples/sec; 0.237 sec/batch; 19h:07m:30s remains)
INFO - root - 2017-12-15 08:45:57.306471: step 42480, loss = 0.22, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 17h:56m:35s remains)
INFO - root - 2017-12-15 08:45:59.605609: step 42490, loss = 0.22, batch loss = 0.19 (31.8 examples/sec; 0.251 sec/batch; 20h:15m:07s remains)
INFO - root - 2017-12-15 08:46:01.874535: step 42500, loss = 0.32, batch loss = 0.29 (35.5 examples/sec; 0.226 sec/batch; 18h:10m:02s remains)
2017-12-15 08:46:02.168106: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5733662 -3.5485568 -3.878417 -4.3136435 -4.8738947 -5.290329 -5.5478773 -5.6028748 -5.4610929 -5.2542372 -4.1643372 -3.4600966 -3.5651822 -3.9169636 -4.4315681][-2.6892719 -3.064827 -3.25777 -3.7296066 -4.2540665 -4.4142342 -4.223177 -4.1477728 -4.1660228 -4.2647772 -3.4122353 -2.5150533 -2.4597883 -2.878829 -3.5310979][-3.0262542 -2.8645813 -2.7734578 -3.0692503 -3.482286 -3.5304642 -3.04658 -2.7519662 -2.9495804 -3.4606586 -3.1153281 -2.3360248 -2.0839157 -2.2172682 -2.5303595][-3.2410769 -2.3381867 -1.8577161 -1.887924 -2.1377406 -2.1062257 -1.4900885 -1.1047496 -1.6487381 -2.7793808 -3.0244389 -2.5395141 -2.1266727 -1.7021031 -1.5047106][-3.5521142 -1.8156124 -1.1372463 -0.95357585 -0.86413062 -0.53052044 0.37555075 0.92587757 0.23309445 -1.2983263 -2.1712885 -2.197458 -1.7927753 -1.0089895 -0.23886561][-3.5864577 -1.5938115 -0.66434491 -0.077095985 0.49236107 1.3404408 2.5435576 3.1101871 2.1830349 0.37115717 -1.1148632 -1.7824357 -1.7440134 -1.0032957 0.17050838][-2.8673186 -1.1937541 -0.15871382 0.84735036 2.0244508 3.4466767 4.8694315 5.3040867 3.9604921 1.7734866 -0.22888947 -1.4076996 -1.6863046 -1.1826109 -0.056121826][-2.0290897 -0.69550788 0.068037271 0.94879317 2.2383432 4.0054345 5.7974048 6.2251611 4.7247353 2.3314114 -0.036243916 -1.5656108 -2.0961468 -1.8037398 -0.71357608][-1.1175938 -0.378474 -0.12668085 0.18670821 0.90255761 2.369586 4.0131612 4.4129753 3.1778841 1.2277086 -0.9924475 -2.5539114 -3.1135426 -2.8739734 -1.7455956][-1.0741454 -0.71750617 -0.89317465 -1.1108367 -0.94727159 0.025661945 1.166795 1.4932044 0.63699627 -0.69576347 -2.4425368 -3.7212391 -4.0412273 -3.6030138 -2.4248154][-1.7109125 -1.4779223 -1.7248521 -2.1099577 -2.1586583 -1.5817519 -0.94538045 -0.6561538 -1.0717102 -1.9120079 -3.3554118 -4.4578485 -4.5744748 -3.9813037 -2.7121489][-2.6069419 -2.3203521 -2.4582129 -2.6196921 -2.6615219 -2.4003825 -2.1885633 -1.944325 -1.9726963 -2.618495 -4.0906582 -5.164813 -5.1567087 -4.4068747 -2.8892751][-3.1938195 -2.7995219 -2.7643735 -2.633764 -2.5323958 -2.4324281 -2.4722974 -2.2343619 -2.2003906 -3.0092731 -4.5242233 -5.4372206 -5.0800304 -4.0553093 -2.4116864][-3.8152275 -3.3561773 -3.2944562 -2.8632531 -2.5530214 -2.5085855 -2.5942323 -2.3713751 -2.3788128 -3.2109246 -4.4754944 -5.0287366 -4.2368007 -2.9489124 -1.2209265][-4.705864 -4.2331352 -4.2789092 -3.671757 -3.2581251 -3.293817 -3.3456078 -3.2113919 -3.2825632 -3.8222148 -4.4978852 -4.5816975 -3.390759 -1.9415281 -0.33487606]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-42500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-42500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 08:46:04.777902: step 42510, loss = 0.49, batch loss = 0.46 (34.4 examples/sec; 0.233 sec/batch; 18h:45m:16s remains)
INFO - root - 2017-12-15 08:46:07.093807: step 42520, loss = 0.18, batch loss = 0.15 (33.6 examples/sec; 0.238 sec/batch; 19h:10m:59s remains)
INFO - root - 2017-12-15 08:46:09.390737: step 42530, loss = 0.28, batch loss = 0.25 (36.1 examples/sec; 0.222 sec/batch; 17h:50m:50s remains)
INFO - root - 2017-12-15 08:46:11.656690: step 42540, loss = 0.15, batch loss = 0.12 (36.1 examples/sec; 0.222 sec/batch; 17h:50m:39s remains)
INFO - root - 2017-12-15 08:46:13.924547: step 42550, loss = 0.25, batch loss = 0.22 (35.8 examples/sec; 0.224 sec/batch; 18h:01m:17s remains)
INFO - root - 2017-12-15 08:46:16.216230: step 42560, loss = 0.25, batch loss = 0.22 (34.8 examples/sec; 0.230 sec/batch; 18h:30m:03s remains)
INFO - root - 2017-12-15 08:46:18.468155: step 42570, loss = 0.18, batch loss = 0.15 (35.3 examples/sec; 0.227 sec/batch; 18h:16m:14s remains)
INFO - root - 2017-12-15 08:46:20.731909: step 42580, loss = 0.16, batch loss = 0.13 (35.8 examples/sec; 0.223 sec/batch; 17h:59m:04s remains)
INFO - root - 2017-12-15 08:46:22.986633: step 42590, loss = 0.21, batch loss = 0.18 (35.4 examples/sec; 0.226 sec/batch; 18h:10m:46s remains)
INFO - root - 2017-12-15 08:46:25.311523: step 42600, loss = 0.21, batch loss = 0.18 (34.7 examples/sec; 0.231 sec/batch; 18h:34m:03s remains)
2017-12-15 08:46:25.582627: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9871316 -8.1777287 -8.329443 -8.3039322 -7.8653116 -7.3031154 -7.1948 -7.1670861 -7.5575185 -8.2205629 -8.6667061 -8.5443993 -7.9162331 -7.2358251 -6.5214272][-7.798646 -9.0669117 -9.1799526 -9.09044 -8.4466305 -7.7131796 -7.6527 -7.4829712 -7.9060187 -8.7975426 -9.7172165 -9.925539 -9.3445892 -8.5036163 -7.6381822][-8.2666187 -8.404129 -8.4292984 -8.437561 -7.7422147 -6.84948 -6.8468814 -6.5941606 -7.0249696 -8.07139 -9.3440914 -9.930047 -9.507247 -8.6870232 -7.7123284][-7.9394298 -6.994453 -6.9627876 -7.0817766 -6.1911736 -5.0525017 -5.0068784 -4.6888514 -5.2429647 -6.5304208 -8.177599 -9.1094322 -9.0199165 -8.2377062 -7.1086292][-6.8365574 -5.3690929 -5.1457405 -5.2699528 -4.2444606 -2.5659118 -1.8121281 -1.2187951 -1.9203732 -3.5134821 -5.6954527 -7.3156171 -7.7657619 -7.1272931 -5.9358664][-6.521554 -4.4199543 -3.9463778 -3.9732578 -2.802763 -0.40240502 1.3372953 2.3800755 1.4166467 -0.68684578 -3.4968143 -5.7305064 -6.7170043 -6.1992345 -4.8740625][-6.0973549 -4.2777014 -3.66247 -3.4244025 -1.9845532 1.2037821 4.1646438 5.6883206 4.2259312 1.1751101 -2.4406016 -5.2158775 -6.4704885 -5.9746475 -4.4757228][-5.9083648 -4.0449471 -3.4191585 -2.732224 -1.037647 2.6660609 6.4205837 8.1250792 6.16185 2.1648617 -1.9868612 -5.0893469 -6.3896704 -5.9891276 -4.4738865][-5.5480156 -3.6291142 -3.0099106 -1.9921141 -0.34139013 3.0040598 6.3428974 7.5644712 5.1707845 0.87585974 -2.8937697 -5.3745112 -6.1727552 -5.8069506 -4.5737524][-5.5351005 -3.7688832 -3.4551642 -2.5854473 -1.2764226 1.2844305 3.8595519 4.6548948 2.4062381 -1.6172003 -4.6405554 -6.2622576 -6.5617638 -6.1981735 -5.1537495][-5.8911686 -4.3589859 -4.2958384 -3.7065299 -2.734576 -0.884737 1.0318265 1.6887836 -0.12332487 -3.5091147 -5.8616552 -6.9308095 -7.1136889 -6.8919621 -6.2098312][-6.6814952 -5.6624851 -5.76758 -5.2059731 -4.2662325 -2.8792803 -1.5863342 -1.3002597 -2.847126 -5.4945493 -7.2537365 -7.8465195 -7.8774562 -7.6101027 -7.0614052][-7.3486233 -6.8742323 -7.0593519 -6.5612383 -5.74918 -4.8928661 -4.2411375 -4.2934651 -5.4125175 -7.05391 -8.0256824 -8.239027 -8.1052475 -7.7262254 -7.2503066][-7.0628405 -7.0997066 -7.43455 -7.0933213 -6.5262294 -6.0661497 -5.8661537 -6.0504875 -6.6932678 -7.3146439 -7.5892668 -7.5579906 -7.3766823 -7.0148211 -6.579361][-6.1873636 -6.4440494 -6.8477011 -6.7862062 -6.5500417 -6.4617934 -6.5530391 -6.7986956 -7.0599489 -7.0790453 -6.9092684 -6.7115312 -6.4170461 -5.9929152 -5.5833492]]...]
INFO - root - 2017-12-15 08:46:27.854167: step 42610, loss = 0.30, batch loss = 0.27 (36.2 examples/sec; 0.221 sec/batch; 17h:47m:58s remains)
INFO - root - 2017-12-15 08:46:30.131220: step 42620, loss = 0.17, batch loss = 0.13 (34.8 examples/sec; 0.230 sec/batch; 18h:30m:16s remains)
INFO - root - 2017-12-15 08:46:32.397276: step 42630, loss = 0.21, batch loss = 0.18 (34.9 examples/sec; 0.229 sec/batch; 18h:26m:48s remains)
INFO - root - 2017-12-15 08:46:34.649357: step 42640, loss = 0.29, batch loss = 0.26 (35.6 examples/sec; 0.225 sec/batch; 18h:06m:05s remains)
INFO - root - 2017-12-15 08:46:36.922994: step 42650, loss = 0.17, batch loss = 0.14 (34.9 examples/sec; 0.229 sec/batch; 18h:27m:30s remains)
INFO - root - 2017-12-15 08:46:39.216849: step 42660, loss = 0.29, batch loss = 0.26 (35.1 examples/sec; 0.228 sec/batch; 18h:21m:13s remains)
INFO - root - 2017-12-15 08:46:41.504172: step 42670, loss = 0.21, batch loss = 0.18 (35.8 examples/sec; 0.224 sec/batch; 18h:00m:07s remains)
INFO - root - 2017-12-15 08:46:43.786144: step 42680, loss = 0.24, batch loss = 0.21 (33.7 examples/sec; 0.238 sec/batch; 19h:07m:29s remains)
INFO - root - 2017-12-15 08:46:46.083984: step 42690, loss = 0.27, batch loss = 0.24 (35.9 examples/sec; 0.223 sec/batch; 17h:55m:48s remains)
INFO - root - 2017-12-15 08:46:48.379378: step 42700, loss = 0.17, batch loss = 0.14 (35.1 examples/sec; 0.228 sec/batch; 18h:20m:04s remains)
2017-12-15 08:46:48.660346: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.85288894 -1.152022 -0.26790369 0.82851958 1.1874993 0.82105136 0.015739202 -0.72805166 -0.59555864 -0.11056352 -0.5392462 -0.9283371 -0.35469437 0.4039464 0.9820509][-1.5893369 -1.2337612 -0.11991 1.0549977 1.3683405 0.79873228 -0.16852403 -1.1366522 -1.1215585 -0.58667946 -0.7887181 -0.84651566 0.016382694 0.76588559 1.2266619][-1.9603674 -1.2783973 -0.26780379 0.83962393 1.0916941 0.46458507 -0.50169659 -1.4034142 -1.3871218 -0.75087285 -0.78092146 -0.68414521 0.19061565 0.75864792 1.1531239][-1.9228702 -0.94473469 -0.07747817 0.88519979 0.98382473 0.29015446 -0.57691121 -1.2937031 -1.1668894 -0.46117115 -0.4390471 -0.40556228 0.13420463 0.33157492 0.63399529][-1.66315 -0.49499917 0.13114119 0.95247459 0.99809766 0.30104089 -0.43506718 -0.81678259 -0.47265375 0.2028749 0.1157558 -0.12447524 -0.21231389 -0.46550238 -0.17789769][-1.55029 -0.32809544 0.028953075 0.77276325 0.79594088 0.30763531 -0.052130222 0.093586683 0.78755236 1.3834419 0.99789453 0.28563428 -0.42566192 -1.0064117 -0.66444004][-1.463675 -0.64511645 -0.49702621 0.32108927 0.58095551 0.54770255 0.66209793 1.3267286 2.3650682 2.7062533 1.926811 0.79187465 -0.37475514 -1.2046647 -1.0324723][-1.9004549 -1.1715786 -1.1076181 -0.24514294 0.22326374 0.63785982 1.0529141 1.9038022 2.8883364 2.8023112 1.7189763 0.38628674 -0.81727552 -1.6291997 -1.6229389][-2.4829566 -1.9468713 -1.9481285 -1.137053 -0.61555958 0.0019814968 0.41041279 1.1314905 1.7868383 1.4677174 0.47766256 -0.69380236 -1.5351074 -2.02058 -2.0895519][-3.3068497 -2.9558656 -2.8835359 -2.1568563 -1.7227312 -1.1600294 -0.847867 -0.31546628 -0.039004803 -0.34548724 -0.85714591 -1.67651 -1.9342797 -1.96302 -2.0349052][-3.9793115 -3.6818683 -3.4890924 -2.8947134 -2.7279816 -2.5124693 -2.4574621 -2.1780634 -2.1505947 -2.202775 -2.0838051 -2.3038518 -2.0130107 -1.720024 -1.8612263][-4.2176256 -3.9910188 -3.8571551 -3.5283697 -3.5856829 -3.8304152 -4.0299535 -3.9018972 -3.8191538 -3.4261804 -2.6783614 -2.377887 -1.75378 -1.2793067 -1.3229846][-4.1006589 -3.9038277 -3.7962904 -3.7010798 -4.0376115 -4.5753927 -4.8787069 -4.7089233 -4.3175745 -3.4348168 -2.2096007 -1.4910635 -0.72099149 -0.20510554 -0.18670893][-3.9577894 -3.6711693 -3.565784 -3.7114167 -4.2316957 -4.902607 -5.1397486 -4.7886209 -3.992898 -2.7513463 -1.2709295 -0.35429394 0.38466 0.83058977 0.89615607][-4.0566611 -3.6496024 -3.5648224 -3.8682928 -4.4126635 -4.9521375 -5.0195608 -4.4128704 -3.314157 -2.0501041 -0.73944485 0.13795209 0.79070544 1.2533956 1.42502]]...]
INFO - root - 2017-12-15 08:46:50.930570: step 42710, loss = 0.21, batch loss = 0.18 (35.2 examples/sec; 0.227 sec/batch; 18h:17m:40s remains)
INFO - root - 2017-12-15 08:46:53.211414: step 42720, loss = 0.22, batch loss = 0.19 (35.8 examples/sec; 0.224 sec/batch; 17h:59m:52s remains)
INFO - root - 2017-12-15 08:46:55.481401: step 42730, loss = 0.17, batch loss = 0.14 (36.2 examples/sec; 0.221 sec/batch; 17h:47m:03s remains)
INFO - root - 2017-12-15 08:46:57.803675: step 42740, loss = 0.33, batch loss = 0.30 (34.6 examples/sec; 0.231 sec/batch; 18h:36m:47s remains)
INFO - root - 2017-12-15 08:47:00.040981: step 42750, loss = 0.26, batch loss = 0.22 (35.8 examples/sec; 0.223 sec/batch; 17h:59m:10s remains)
INFO - root - 2017-12-15 08:47:02.324939: step 42760, loss = 0.26, batch loss = 0.23 (35.3 examples/sec; 0.227 sec/batch; 18h:14m:21s remains)
INFO - root - 2017-12-15 08:47:04.617465: step 42770, loss = 0.28, batch loss = 0.24 (34.5 examples/sec; 0.232 sec/batch; 18h:38m:23s remains)
INFO - root - 2017-12-15 08:47:06.917579: step 42780, loss = 0.22, batch loss = 0.19 (35.6 examples/sec; 0.225 sec/batch; 18h:06m:07s remains)
INFO - root - 2017-12-15 08:47:09.223765: step 42790, loss = 0.18, batch loss = 0.15 (34.8 examples/sec; 0.230 sec/batch; 18h:30m:24s remains)
INFO - root - 2017-12-15 08:47:11.510237: step 42800, loss = 0.19, batch loss = 0.16 (34.8 examples/sec; 0.230 sec/batch; 18h:30m:18s remains)
2017-12-15 08:47:11.805171: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.016974449 -0.42252505 -0.86328316 -1.3684311 -1.6753647 -1.2240995 -0.50678897 -0.57840407 -1.2063795 -2.013036 -2.7451961 -2.8659117 -2.8016787 -2.4785087 -1.7526898][-0.63541031 -0.33561075 -0.78121924 -1.3840911 -1.9954878 -1.9339601 -1.2411261 -0.8966614 -0.80798924 -1.0279603 -1.5376797 -1.7166346 -1.8338768 -1.8336256 -1.5676726][-1.744555 -1.0045074 -1.5029817 -2.0159016 -2.4643409 -2.4780271 -1.7622733 -1.0494379 -0.35565352 -0.1168139 -0.48481989 -0.77424681 -1.0611978 -1.2313074 -1.1499119][-2.2324917 -1.3781692 -1.9119912 -2.1879187 -2.3008864 -2.1909425 -1.4624872 -0.53404927 0.44125748 0.80827355 0.45424628 -0.0099685192 -0.52053738 -0.91502619 -0.84541678][-2.5785935 -1.5540043 -2.0596213 -2.166013 -2.0843542 -1.9055307 -1.1116 -0.073897839 1.0108848 1.6489582 1.5913677 1.0910602 0.2065959 -0.66416717 -0.84170794][-3.0341313 -2.0665972 -2.4889481 -2.320503 -1.8054318 -1.4113381 -0.51989722 0.4227829 1.1921887 1.8208964 2.1089747 1.8004463 0.72877932 -0.46792126 -0.70799196][-2.9997501 -2.3599317 -2.4668164 -2.1774807 -1.3345702 -0.52032614 0.32885313 0.91339445 1.2324409 1.6169307 1.9051878 1.7455628 0.73045635 -0.65022087 -0.95660126][-3.2100601 -2.2433665 -2.1583259 -1.8058989 -0.95098674 -0.13820052 0.62512159 0.93686247 0.86313009 0.99906993 1.270021 1.2109699 0.335191 -1.0283755 -1.4399288][-3.4787893 -2.3803835 -2.2500887 -1.9327943 -1.2870557 -0.60579228 -0.024861097 0.045636415 -0.23752236 -0.14695072 0.09945488 0.24277568 -0.22340226 -1.3596065 -1.9263052][-3.7910385 -2.791887 -2.7926767 -2.4665833 -2.0216982 -1.5234323 -1.1013142 -1.0655731 -1.3297423 -1.2898909 -1.0512795 -0.81485617 -0.94183969 -1.812952 -2.3802867][-3.2344239 -2.6561694 -2.9905388 -2.7660205 -2.5609145 -2.258059 -1.8961602 -1.783174 -1.9984931 -2.0995247 -2.0537682 -1.8918456 -1.8274724 -2.3900683 -2.8238544][-2.3531525 -2.2039344 -2.9654279 -2.8401866 -2.7782743 -2.6363802 -2.3483019 -2.1883631 -2.4531014 -2.6868632 -2.7187581 -2.6093984 -2.4893584 -2.8231766 -3.1644692][-1.7002155 -1.9899821 -3.0556996 -2.9626105 -3.0073109 -3.0663652 -2.8141317 -2.5378757 -2.6836507 -2.8272686 -2.7448509 -2.7024331 -2.7105412 -3.0220945 -3.3287995][-1.2190627 -1.8941823 -3.3160219 -3.3614905 -3.3646216 -3.3873215 -3.113508 -2.7622209 -2.8620286 -2.9387729 -2.7952094 -2.8185976 -2.9217513 -3.1311743 -3.3080912][-1.2556179 -2.0747819 -3.600085 -3.7729115 -3.7821507 -3.7559164 -3.5242896 -3.231638 -3.3644805 -3.3742557 -3.1348093 -3.1119404 -3.2039371 -3.2189653 -3.2397058]]...]
INFO - root - 2017-12-15 08:47:14.087643: step 42810, loss = 0.18, batch loss = 0.14 (35.5 examples/sec; 0.226 sec/batch; 18h:09m:23s remains)
INFO - root - 2017-12-15 08:47:16.358745: step 42820, loss = 0.25, batch loss = 0.22 (36.9 examples/sec; 0.217 sec/batch; 17h:27m:43s remains)
INFO - root - 2017-12-15 08:47:18.630228: step 42830, loss = 0.26, batch loss = 0.23 (36.0 examples/sec; 0.222 sec/batch; 17h:52m:55s remains)
INFO - root - 2017-12-15 08:47:20.936863: step 42840, loss = 0.33, batch loss = 0.29 (36.1 examples/sec; 0.222 sec/batch; 17h:51m:05s remains)
INFO - root - 2017-12-15 08:47:23.202666: step 42850, loss = 0.24, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 18h:02m:30s remains)
INFO - root - 2017-12-15 08:47:25.495918: step 42860, loss = 0.24, batch loss = 0.21 (33.9 examples/sec; 0.236 sec/batch; 18h:58m:17s remains)
INFO - root - 2017-12-15 08:47:27.771104: step 42870, loss = 0.18, batch loss = 0.14 (35.6 examples/sec; 0.225 sec/batch; 18h:04m:24s remains)
INFO - root - 2017-12-15 08:47:30.075043: step 42880, loss = 0.31, batch loss = 0.27 (34.4 examples/sec; 0.233 sec/batch; 18h:43m:41s remains)
INFO - root - 2017-12-15 08:47:32.390158: step 42890, loss = 0.18, batch loss = 0.14 (35.8 examples/sec; 0.224 sec/batch; 17h:59m:27s remains)
INFO - root - 2017-12-15 08:47:34.661446: step 42900, loss = 0.23, batch loss = 0.20 (34.6 examples/sec; 0.231 sec/batch; 18h:36m:53s remains)
2017-12-15 08:47:34.964788: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9343505 -5.6334515 -5.9131289 -6.2014251 -6.3512115 -6.2748213 -6.023665 -5.9084916 -5.8586969 -5.8822765 -5.9937468 -6.1069303 -6.0913644 -5.9814587 -5.9435444][-5.4677458 -5.7738981 -5.9888873 -6.3843431 -6.5915565 -6.3049097 -5.7277288 -5.3265142 -5.1220589 -5.0867319 -5.2299418 -5.4227142 -5.4606948 -5.4097805 -5.4324665][-4.5437279 -3.8755937 -4.0655107 -4.6020994 -4.9645915 -4.6728764 -3.8168311 -3.2443259 -3.1103809 -3.261764 -3.6930161 -4.07253 -4.1535783 -4.0362778 -3.9351244][-3.1585522 -1.8052356 -2.0011146 -2.5319729 -2.952451 -2.4903812 -1.1687008 -0.39572525 -0.39780509 -0.91691041 -1.8442526 -2.4316835 -2.5408337 -2.3527164 -2.1347542][-1.4111762 0.75713873 0.38534379 -0.16307855 -0.51504076 0.21935892 1.9208291 2.7055733 2.294512 1.2359786 -0.17374587 -0.92736256 -0.903455 -0.4557538 0.06242919][-0.11925387 2.3573778 1.7554538 1.2105095 0.93751669 1.7967169 3.7112925 4.3491745 3.5014417 2.0163634 0.3166213 -0.48052359 -0.33672357 0.38609195 1.2119164][0.18734765 2.5320508 1.7972476 1.1754777 0.92025566 1.7412202 3.6651332 4.1367979 3.1264355 1.6526418 0.043813705 -0.56678867 -0.40413082 0.340348 1.1773887][-0.72281778 1.7668507 0.86845994 0.019111872 -0.2257967 0.4231627 2.0565674 2.3601162 1.4232912 0.19387436 -0.9916718 -1.3242354 -1.2292889 -0.60753286 0.15540433][-2.3020535 0.12513399 -0.85584915 -1.8522731 -2.0920582 -1.7160041 -0.53375888 -0.37735116 -1.1822397 -2.0965726 -2.8216553 -2.9701478 -2.9302142 -2.4001536 -1.7582181][-3.5049517 -1.2470226 -2.2707644 -3.3685145 -3.7065921 -3.6422429 -2.9600933 -2.9493637 -3.5070894 -3.9845896 -4.2821865 -4.18436 -4.0861344 -3.7222319 -3.3686461][-4.3965449 -2.5131488 -3.53544 -4.5985274 -4.9414268 -5.1049318 -4.8795505 -4.9716282 -5.2639465 -5.368432 -5.2966208 -5.025938 -4.8710055 -4.7014937 -4.6595035][-5.2399845 -3.8262081 -4.6944122 -5.53191 -5.771934 -5.95319 -5.9221554 -5.954669 -6.0150118 -5.9761271 -5.802043 -5.5562963 -5.445066 -5.4336767 -5.5744152][-5.4436646 -4.2478371 -4.8611269 -5.4007573 -5.5891371 -5.7076373 -5.7474642 -5.7777662 -5.7571306 -5.6443191 -5.498735 -5.3394394 -5.2383518 -5.3039651 -5.5173116][-5.247582 -4.2396417 -4.6048107 -4.9005985 -4.9986992 -5.0718012 -5.1251893 -5.1546879 -5.1304064 -5.048315 -4.9711332 -4.8931513 -4.8337593 -4.9046345 -5.0713348][-4.9595194 -4.1518483 -4.3016462 -4.3884983 -4.4135008 -4.440279 -4.4695044 -4.4933152 -4.5063944 -4.5031519 -4.4960184 -4.4812059 -4.4702339 -4.5096941 -4.5768371]]...]
INFO - root - 2017-12-15 08:47:37.228429: step 42910, loss = 0.25, batch loss = 0.21 (34.7 examples/sec; 0.231 sec/batch; 18h:33m:30s remains)
INFO - root - 2017-12-15 08:47:39.520328: step 42920, loss = 0.24, batch loss = 0.20 (35.2 examples/sec; 0.227 sec/batch; 18h:17m:16s remains)
INFO - root - 2017-12-15 08:47:41.789441: step 42930, loss = 0.26, batch loss = 0.22 (35.0 examples/sec; 0.229 sec/batch; 18h:23m:51s remains)
INFO - root - 2017-12-15 08:47:44.048811: step 42940, loss = 0.24, batch loss = 0.21 (35.4 examples/sec; 0.226 sec/batch; 18h:10m:25s remains)
INFO - root - 2017-12-15 08:47:46.315331: step 42950, loss = 0.32, batch loss = 0.28 (34.3 examples/sec; 0.233 sec/batch; 18h:45m:57s remains)
INFO - root - 2017-12-15 08:47:48.571478: step 42960, loss = 0.30, batch loss = 0.27 (36.1 examples/sec; 0.222 sec/batch; 17h:49m:02s remains)
INFO - root - 2017-12-15 08:47:50.812098: step 42970, loss = 0.39, batch loss = 0.36 (35.6 examples/sec; 0.225 sec/batch; 18h:03m:54s remains)
INFO - root - 2017-12-15 08:47:53.074962: step 42980, loss = 0.18, batch loss = 0.15 (35.4 examples/sec; 0.226 sec/batch; 18h:10m:22s remains)
INFO - root - 2017-12-15 08:47:55.394437: step 42990, loss = 0.20, batch loss = 0.17 (35.6 examples/sec; 0.225 sec/batch; 18h:05m:01s remains)
INFO - root - 2017-12-15 08:47:57.649990: step 43000, loss = 0.29, batch loss = 0.26 (35.1 examples/sec; 0.228 sec/batch; 18h:21m:01s remains)
2017-12-15 08:47:57.926493: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.82941139 -2.4609032 -2.6572649 -2.9885442 -3.8269351 -5.1427064 -5.9440603 -6.6285272 -7.1963782 -7.2577457 -7.2932291 -7.053689 -6.06339 -4.91644 -3.9137955][-1.6030087 -3.1201968 -3.3387747 -3.7691555 -4.8053389 -6.117732 -6.8706536 -7.4963665 -8.0114174 -8.1020622 -8.2829952 -8.13916 -7.4209595 -6.5895638 -5.6539812][-3.9631886 -4.7697887 -4.9312124 -5.3928576 -6.130496 -6.6816149 -6.7453012 -6.8487144 -7.0265474 -7.1568508 -7.664196 -7.8439169 -7.6643867 -7.4203525 -6.8158722][-6.7651982 -6.8613977 -6.8472514 -6.9923511 -6.90732 -6.319622 -5.470891 -5.1543655 -5.3383369 -5.7834477 -6.6364822 -7.0542183 -7.1476526 -7.1637368 -6.7640982][-8.8359566 -8.1806126 -7.9461546 -7.6747074 -6.8155107 -5.3522768 -3.8148823 -3.2153707 -3.6350141 -4.5789566 -5.8504324 -6.5337086 -6.8930721 -7.0249157 -6.7057948][-9.0483761 -8.13602 -7.7826619 -7.0860195 -5.276166 -2.7884223 -0.46691346 0.47574091 -0.10480189 -1.5857699 -3.5121574 -4.8309674 -5.9056854 -6.4202967 -6.2322531][-7.8825836 -7.2149734 -6.8353205 -5.746068 -3.0710528 0.24573469 3.23392 4.4624944 3.8600988 2.1534939 -0.2231884 -2.3448517 -4.4282513 -5.6291056 -5.6929073][-6.8829579 -6.1149049 -5.926353 -4.8142891 -1.7298607 2.0719385 5.4856105 6.9116607 6.2790732 4.4930968 1.8388066 -0.87060881 -3.6316955 -5.2241788 -5.3374438][-6.0961218 -5.5013142 -5.9137788 -5.4131451 -2.7018552 1.0107698 4.420887 5.8070703 5.3025627 3.8121104 1.335247 -1.4130049 -4.2051826 -5.6470909 -5.591423][-5.4791193 -5.0182419 -6.0831709 -6.3657222 -4.551034 -1.6337531 1.236557 2.5479288 2.4378 1.4675629 -0.58276284 -3.0391934 -5.3642573 -6.2452488 -5.9695654][-4.9484015 -4.6857939 -6.1920357 -6.9502535 -6.0219278 -4.0703115 -1.9175699 -0.85779631 -0.69863 -1.1993525 -2.7984982 -4.678237 -6.1176672 -6.1717949 -5.5998287][-5.4172549 -5.2003708 -6.7143788 -7.6371655 -7.3458872 -6.2890043 -4.995553 -4.3088055 -3.883029 -3.980509 -4.9806557 -6.1501141 -6.7633181 -6.3196082 -5.6949978][-6.03288 -5.7729235 -7.1186352 -8.0643473 -8.2736921 -7.8694038 -7.156364 -6.71687 -6.1355691 -5.8951254 -6.3907228 -6.9461884 -6.9051204 -6.264277 -5.7995286][-6.0934954 -5.7647376 -6.9048424 -7.8634634 -8.3347263 -8.334671 -8.039712 -7.6915913 -7.094759 -6.7891245 -6.9269447 -7.0199509 -6.6530633 -6.0385284 -5.6474342][-6.2352037 -5.7441149 -6.4592619 -7.1457796 -7.5742803 -7.6988463 -7.6274657 -7.3963881 -6.9362435 -6.6622696 -6.6000113 -6.4827566 -6.1356173 -5.7630267 -5.5045047]]...]
INFO - root - 2017-12-15 08:48:00.263124: step 43010, loss = 0.21, batch loss = 0.17 (32.6 examples/sec; 0.246 sec/batch; 19h:45m:05s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 08:48:02.533483: step 43020, loss = 0.23, batch loss = 0.20 (36.2 examples/sec; 0.221 sec/batch; 17h:45m:34s remains)
INFO - root - 2017-12-15 08:48:04.835405: step 43030, loss = 0.23, batch loss = 0.20 (34.5 examples/sec; 0.232 sec/batch; 18h:39m:56s remains)
INFO - root - 2017-12-15 08:48:07.124171: step 43040, loss = 0.31, batch loss = 0.28 (34.6 examples/sec; 0.231 sec/batch; 18h:36m:25s remains)
INFO - root - 2017-12-15 08:48:09.425727: step 43050, loss = 0.18, batch loss = 0.15 (33.4 examples/sec; 0.239 sec/batch; 19h:15m:22s remains)
INFO - root - 2017-12-15 08:48:11.726760: step 43060, loss = 0.19, batch loss = 0.15 (35.2 examples/sec; 0.227 sec/batch; 18h:16m:33s remains)
INFO - root - 2017-12-15 08:48:14.025626: step 43070, loss = 0.17, batch loss = 0.14 (35.0 examples/sec; 0.228 sec/batch; 18h:21m:12s remains)
INFO - root - 2017-12-15 08:48:16.286762: step 43080, loss = 0.19, batch loss = 0.16 (35.9 examples/sec; 0.223 sec/batch; 17h:56m:17s remains)
INFO - root - 2017-12-15 08:48:18.605376: step 43090, loss = 0.21, batch loss = 0.18 (34.0 examples/sec; 0.235 sec/batch; 18h:53m:53s remains)
INFO - root - 2017-12-15 08:48:20.852122: step 43100, loss = 0.38, batch loss = 0.35 (35.6 examples/sec; 0.225 sec/batch; 18h:03m:34s remains)
2017-12-15 08:48:21.124008: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1637118 -3.5852373 -1.8308859 -0.0015597343 0.18130088 -0.015437126 -0.80385995 -2.537575 -3.6467369 -3.7474127 -3.0371776 -2.894135 -2.9381378 -2.7244477 -2.6708343][-3.2690334 -3.9292336 -2.030869 0.10634398 0.77208018 0.82793975 -0.002175808 -1.8031186 -2.9910514 -3.3623943 -3.0232544 -2.9772243 -2.9639821 -2.7670293 -2.7928925][-4.7710524 -4.6207542 -2.7240422 -0.55312896 0.47413754 0.77132392 -0.022665262 -1.5569332 -2.5160995 -3.0625927 -2.99746 -2.9123433 -2.8848705 -2.6987157 -2.7046185][-5.398417 -4.7026763 -2.9662502 -1.0455899 -0.0326488 0.28247118 -0.43594933 -1.5922111 -2.407733 -3.0208793 -3.0355651 -2.8442593 -2.7740278 -2.6944158 -2.6459939][-4.8805275 -3.9390125 -2.3633957 -0.71112025 0.12138391 0.4018321 0.030476332 -0.61229289 -1.2683704 -1.9784365 -2.1738784 -2.1254728 -2.3164973 -2.4829545 -2.5643172][-4.2210808 -3.1151612 -1.835134 -0.39658391 0.28035355 0.71182156 0.908875 0.811887 0.23799181 -0.5375104 -0.89567089 -1.1876125 -1.750772 -2.1580393 -2.5187082][-2.99772 -2.4938102 -1.5639486 -0.32535136 0.25642109 0.92297435 1.4237485 1.4476278 0.92807007 0.14959478 -0.33077693 -0.81938732 -1.4462961 -1.8681188 -2.2807441][-3.0778251 -2.6833243 -1.9658396 -0.76826668 -0.11265135 0.80924153 1.5268776 1.6697574 1.2778406 0.65070295 0.022021055 -0.73300576 -1.3253697 -1.723811 -2.3039927][-3.9294567 -3.3522096 -2.6916814 -1.6017079 -0.86193991 0.28780508 1.0447099 1.1606686 0.88939714 0.36249685 -0.28329647 -0.85791516 -1.1689178 -1.6628994 -2.4676747][-5.003377 -4.0161934 -3.2966394 -2.2691369 -1.3413481 -0.13716507 0.40229535 0.24501252 -0.21646523 -0.81619978 -1.3824602 -1.6031933 -1.5692434 -2.0306182 -2.7164595][-5.487689 -4.0117493 -3.2097526 -2.231426 -1.2319118 -0.16430855 0.062400103 -0.37138355 -1.1347362 -1.8133008 -2.2468641 -2.2509878 -2.1073749 -2.4604771 -2.9090362][-5.3076439 -3.6242638 -2.7423925 -1.7257247 -0.69027281 0.0056777 -0.13883662 -0.74967217 -1.7122529 -2.3963633 -2.6126776 -2.438813 -2.3137102 -2.645215 -2.9285977][-4.7977524 -3.0145133 -2.0342112 -0.95377684 -0.12501693 0.13230681 -0.30273759 -0.9888413 -2.0275259 -2.7341921 -3.0225971 -2.9815245 -2.9783223 -3.1430681 -3.1757438][-4.2935743 -2.5359442 -1.4870062 -0.39998817 0.15946364 0.09126687 -0.39334631 -1.0165745 -2.1120179 -2.9185073 -3.3477745 -3.580225 -3.5648985 -3.5793786 -3.4372907][-3.9835653 -2.2884731 -1.2786291 -0.30150557 -0.014579296 -0.17884994 -0.52572584 -1.0671974 -2.1153421 -2.978581 -3.5176349 -3.7977185 -3.6534846 -3.489748 -3.1971688]]...]
INFO - root - 2017-12-15 08:48:23.448683: step 43110, loss = 0.22, batch loss = 0.18 (34.6 examples/sec; 0.231 sec/batch; 18h:33m:39s remains)
INFO - root - 2017-12-15 08:48:25.768041: step 43120, loss = 0.24, batch loss = 0.21 (35.0 examples/sec; 0.229 sec/batch; 18h:23m:57s remains)
INFO - root - 2017-12-15 08:48:28.020206: step 43130, loss = 0.22, batch loss = 0.19 (36.3 examples/sec; 0.221 sec/batch; 17h:43m:39s remains)
INFO - root - 2017-12-15 08:48:30.284160: step 43140, loss = 0.21, batch loss = 0.17 (36.2 examples/sec; 0.221 sec/batch; 17h:46m:15s remains)
INFO - root - 2017-12-15 08:48:32.581536: step 43150, loss = 0.21, batch loss = 0.18 (32.8 examples/sec; 0.244 sec/batch; 19h:37m:19s remains)
INFO - root - 2017-12-15 08:48:34.879021: step 43160, loss = 0.23, batch loss = 0.20 (35.7 examples/sec; 0.224 sec/batch; 18h:01m:04s remains)
INFO - root - 2017-12-15 08:48:37.148778: step 43170, loss = 0.30, batch loss = 0.27 (34.5 examples/sec; 0.232 sec/batch; 18h:39m:40s remains)
INFO - root - 2017-12-15 08:48:39.444928: step 43180, loss = 0.19, batch loss = 0.16 (35.4 examples/sec; 0.226 sec/batch; 18h:09m:38s remains)
INFO - root - 2017-12-15 08:48:41.694354: step 43190, loss = 0.16, batch loss = 0.13 (35.0 examples/sec; 0.228 sec/batch; 18h:21m:13s remains)
INFO - root - 2017-12-15 08:48:43.947846: step 43200, loss = 0.25, batch loss = 0.22 (34.8 examples/sec; 0.230 sec/batch; 18h:27m:22s remains)
2017-12-15 08:48:44.234830: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.9652972 -0.046759605 -1.2625239 -2.0859642 -2.4773703 -2.584661 -2.5625556 -2.4122996 -1.8161471 -1.1284052 -0.20665073 -0.11334276 -0.9280026 -1.778156 -1.5926279][-0.78363621 -1.6630659 -2.8426173 -3.5860894 -3.7848225 -3.5329108 -3.1519547 -2.6966953 -1.7628326 -0.94593012 -0.040624619 0.080765963 -0.83925843 -1.8312658 -2.1188784][-3.0291059 -2.8309054 -3.7729344 -4.2642803 -4.2176976 -3.6530962 -3.0693345 -2.3389997 -1.2180359 -0.5682745 -0.03961587 -0.15937662 -1.1165118 -1.9723164 -2.4519589][-3.8102295 -2.8813648 -3.7528274 -4.1446037 -3.9905787 -3.245631 -2.4745343 -1.5803187 -0.47385693 -0.22767758 -0.292333 -0.75964689 -1.7273309 -2.4055617 -3.0820293][-3.1402578 -1.9333354 -2.9268148 -3.310225 -3.0088742 -2.0755265 -1.0622885 0.15436912 1.1152647 0.85192633 0.017223597 -1.0121183 -2.1546664 -2.7217395 -3.4982517][-1.585237 -0.75645733 -2.0711873 -2.6080809 -2.2745824 -1.0825063 0.41636133 2.0351825 2.8757362 2.1531606 0.56825519 -1.0174906 -2.3012419 -2.7781734 -3.5435629][-0.1522789 -0.097657919 -1.732861 -2.2737432 -1.6215719 0.11521983 2.2988663 4.2977495 4.9112015 3.6684961 1.3993857 -0.83830881 -2.3357778 -2.7469296 -3.4134479][-0.1894865 -0.51092517 -2.2317092 -2.623734 -1.5411927 0.78992677 3.5083203 5.7006335 6.0107265 4.3966155 1.6656811 -1.0581335 -2.7426209 -3.0491505 -3.503026][-1.4147301 -1.8169026 -3.2908292 -3.3924332 -1.9682791 0.77278566 3.6910396 5.6054015 5.4646664 3.5862451 0.69253993 -2.1536531 -3.7440734 -3.7759757 -3.9563956][-3.3370948 -3.627665 -4.733253 -4.6244574 -3.1830113 -0.45888686 2.2313585 3.7671905 3.4837189 1.7333176 -0.94411683 -3.5864983 -4.9024968 -4.6206083 -4.5234585][-5.0474062 -5.1086888 -5.8224468 -5.5899696 -4.4028544 -2.1223519 0.15128565 1.3785703 1.1181924 -0.24795055 -2.490416 -4.7145262 -5.7169456 -5.2800207 -5.1414595][-6.2101569 -6.1153207 -6.5373392 -6.2849593 -5.6103315 -4.1032019 -2.3449728 -1.2831595 -1.218536 -1.9652903 -3.6408443 -5.3953681 -6.0458407 -5.6269903 -5.629714][-6.7665439 -6.4385347 -6.5312142 -6.2171426 -5.93629 -5.077569 -3.8666515 -3.0832114 -2.8946178 -3.2693059 -4.4049635 -5.6170888 -5.8935008 -5.5506783 -5.8608909][-7.1356163 -6.4296107 -6.2891049 -5.9802628 -5.8558626 -5.3979812 -4.6632442 -4.1762381 -4.0201817 -4.3025341 -5.1113853 -5.8096628 -5.7730312 -5.5275106 -6.0740919][-6.9174614 -5.6859221 -5.4045367 -5.2204466 -5.0768657 -4.9378338 -4.7703419 -4.707242 -4.7088103 -5.0100632 -5.7143359 -5.9910707 -5.7234468 -5.5336027 -6.1543007]]...]
INFO - root - 2017-12-15 08:48:46.491912: step 43210, loss = 0.22, batch loss = 0.19 (35.5 examples/sec; 0.225 sec/batch; 18h:06m:30s remains)
INFO - root - 2017-12-15 08:48:48.753026: step 43220, loss = 0.16, batch loss = 0.13 (34.2 examples/sec; 0.234 sec/batch; 18h:48m:04s remains)
INFO - root - 2017-12-15 08:48:51.023456: step 43230, loss = 0.23, batch loss = 0.19 (35.7 examples/sec; 0.224 sec/batch; 18h:00m:24s remains)
INFO - root - 2017-12-15 08:48:53.254948: step 43240, loss = 0.22, batch loss = 0.19 (35.6 examples/sec; 0.225 sec/batch; 18h:03m:51s remains)
INFO - root - 2017-12-15 08:48:55.526713: step 43250, loss = 0.26, batch loss = 0.23 (36.0 examples/sec; 0.222 sec/batch; 17h:51m:44s remains)
INFO - root - 2017-12-15 08:48:57.828414: step 43260, loss = 0.20, batch loss = 0.17 (34.2 examples/sec; 0.234 sec/batch; 18h:47m:38s remains)
INFO - root - 2017-12-15 08:49:00.087328: step 43270, loss = 0.24, batch loss = 0.21 (35.4 examples/sec; 0.226 sec/batch; 18h:08m:52s remains)
INFO - root - 2017-12-15 08:49:02.397011: step 43280, loss = 0.26, batch loss = 0.23 (33.7 examples/sec; 0.237 sec/batch; 19h:03m:03s remains)
INFO - root - 2017-12-15 08:49:04.670225: step 43290, loss = 0.26, batch loss = 0.23 (35.0 examples/sec; 0.228 sec/batch; 18h:21m:17s remains)
INFO - root - 2017-12-15 08:49:06.929978: step 43300, loss = 0.29, batch loss = 0.26 (35.9 examples/sec; 0.223 sec/batch; 17h:53m:13s remains)
2017-12-15 08:49:07.241186: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.18007 -2.25585 -2.3205903 -2.5722558 -2.7933228 -3.1431403 -3.6011956 -3.9931326 -3.7600589 -3.2733579 -3.118937 -3.2250066 -2.7751832 -2.5148916 -3.1830955][-2.761004 -3.2107608 -3.2346983 -3.2871881 -3.327491 -3.5569782 -3.9356189 -4.261981 -4.1085482 -3.8291774 -3.8190858 -3.8985713 -3.417367 -3.1689694 -3.709554][-4.0490103 -3.9578695 -4.0299788 -3.8908896 -3.764956 -3.8570435 -4.1042423 -4.2812176 -4.1756649 -4.1177907 -4.2102718 -4.2328434 -3.7290077 -3.4159238 -3.6990891][-4.7208834 -4.28461 -4.3945208 -4.1172261 -3.756031 -3.6167881 -3.677989 -3.6304853 -3.5071959 -3.6483366 -3.9291902 -4.0661354 -3.5507507 -3.0836551 -3.0814583][-4.5963454 -3.8498161 -3.8873565 -3.57179 -3.0040321 -2.5849872 -2.4293697 -2.1530168 -2.0037127 -2.279053 -2.7998786 -3.2893174 -2.7875485 -2.0204802 -1.7042189][-3.0928292 -2.1021023 -2.021064 -1.6809762 -0.98268723 -0.33498156 0.056791544 0.45808911 0.50202751 0.1201396 -0.656855 -1.6633013 -1.1737701 -0.16119003 0.25254107][-1.1704484 -0.27921367 -0.083847284 0.33648634 1.1237562 1.962889 2.5873506 3.0470946 2.99005 2.6233203 1.6788211 0.14254022 0.54090595 1.6709294 2.007683][-0.52557278 0.48249173 0.76161194 1.1728771 1.8128707 2.636616 3.3932922 4.0188828 4.0213442 3.751797 2.72216 0.87804842 1.1832526 2.2267392 2.4436319][-1.2357832 -0.4771595 -0.43740904 -0.37096775 -0.067823172 0.71494055 1.6422777 2.4299295 2.6242449 2.5787413 1.759033 -0.015544653 0.1516273 0.96269417 1.1599622][-2.9039149 -2.7502859 -3.10587 -3.2296052 -3.0412898 -2.2946186 -1.4105926 -0.64683938 -0.28806496 -0.064068317 -0.585799 -1.9901993 -1.9535445 -1.464636 -1.2628813][-4.820363 -4.9908981 -5.4694653 -5.6350794 -5.5075235 -4.9006834 -4.2370625 -3.6553407 -3.240787 -2.9011545 -3.2097588 -4.1867957 -4.2386222 -3.9768009 -3.719018][-5.63197 -5.7594404 -6.0874615 -6.1574125 -6.078372 -5.5869732 -5.1384478 -4.7649727 -4.4706831 -4.3065443 -4.6468258 -5.3783236 -5.5448675 -5.4907188 -5.2823133][-5.5420361 -5.2968273 -5.41355 -5.3316607 -5.2099485 -4.7463818 -4.4391546 -4.2938089 -4.1804514 -4.2227926 -4.7286949 -5.4415894 -5.8197341 -5.9714336 -5.8978786][-5.1587849 -4.154932 -3.9010377 -3.6554985 -3.628433 -3.4450727 -3.4152737 -3.4784307 -3.542558 -3.7600222 -4.4147444 -5.0835276 -5.4331236 -5.6505036 -5.72741][-4.5282545 -2.8658719 -2.3332565 -2.1445568 -2.3176029 -2.3497255 -2.4582314 -2.7165816 -2.8885345 -3.2147846 -3.9449391 -4.5845184 -4.8149967 -4.9832144 -5.1778069]]...]
INFO - root - 2017-12-15 08:49:09.565306: step 43310, loss = 0.18, batch loss = 0.15 (35.4 examples/sec; 0.226 sec/batch; 18h:08m:04s remains)
INFO - root - 2017-12-15 08:49:11.832385: step 43320, loss = 0.21, batch loss = 0.18 (36.1 examples/sec; 0.222 sec/batch; 17h:49m:10s remains)
INFO - root - 2017-12-15 08:49:14.126084: step 43330, loss = 0.29, batch loss = 0.25 (34.6 examples/sec; 0.231 sec/batch; 18h:35m:25s remains)
INFO - root - 2017-12-15 08:49:16.386675: step 43340, loss = 0.18, batch loss = 0.15 (35.7 examples/sec; 0.224 sec/batch; 17h:59m:02s remains)
INFO - root - 2017-12-15 08:49:18.686829: step 43350, loss = 0.26, batch loss = 0.23 (35.3 examples/sec; 0.226 sec/batch; 18h:10m:46s remains)
INFO - root - 2017-12-15 08:49:20.948471: step 43360, loss = 0.21, batch loss = 0.18 (33.7 examples/sec; 0.237 sec/batch; 19h:02m:30s remains)
INFO - root - 2017-12-15 08:49:23.215937: step 43370, loss = 0.37, batch loss = 0.33 (34.5 examples/sec; 0.232 sec/batch; 18h:37m:16s remains)
INFO - root - 2017-12-15 08:49:25.526699: step 43380, loss = 0.21, batch loss = 0.17 (35.7 examples/sec; 0.224 sec/batch; 17h:59m:08s remains)
INFO - root - 2017-12-15 08:49:27.789045: step 43390, loss = 0.35, batch loss = 0.32 (35.9 examples/sec; 0.223 sec/batch; 17h:55m:07s remains)
INFO - root - 2017-12-15 08:49:30.067005: step 43400, loss = 0.21, batch loss = 0.18 (34.8 examples/sec; 0.230 sec/batch; 18h:27m:22s remains)
2017-12-15 08:49:30.369741: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9400802 -5.6085138 -6.650526 -6.9272995 -6.5947495 -6.6599274 -6.4632635 -5.4566426 -4.863483 -5.048913 -5.78216 -6.8792524 -7.6667738 -7.703454 -7.5746136][-3.8806007 -4.1443987 -5.5664263 -6.293396 -6.3244867 -6.4176626 -5.7596359 -4.224411 -3.4365318 -3.6271663 -4.5168095 -5.9425898 -7.2080479 -7.4884434 -7.45953][-3.9956322 -2.7756577 -4.3299189 -5.5346823 -6.1861887 -6.2162886 -4.8724213 -2.9833405 -2.2636354 -2.7939048 -4.0339694 -5.7022305 -7.2889204 -7.7080946 -7.643096][-3.6746669 -1.1263909 -2.6046121 -4.2057467 -5.3058429 -4.9292912 -2.5718155 -0.29664397 0.18107843 -1.0162426 -2.9087353 -4.8581972 -6.7842627 -7.6187449 -7.7098074][-3.9907398 -0.47265172 -1.5478222 -3.0015509 -3.9767237 -2.8930657 0.57590127 3.2474821 3.2940443 1.2683103 -1.3800577 -3.6148295 -5.8589854 -7.3895035 -7.9179916][-4.1569614 -0.42144513 -1.0052323 -2.0373104 -2.5367823 -0.727566 3.4441741 6.0800295 5.3909769 2.508486 -0.75211608 -3.1159103 -5.6789627 -7.8291054 -8.5904636][-4.0214229 -0.86510181 -0.89088738 -1.3514997 -1.0793349 1.4456451 5.8453875 8.16441 6.7068443 3.1883037 -0.34927964 -2.85713 -5.6193705 -8.1478386 -8.9854851][-5.3920951 -2.438617 -1.8361647 -1.5762461 -0.63508117 2.2436988 6.49658 8.6014376 6.8590403 3.2049105 -0.25478172 -2.8375325 -5.6617117 -8.3414688 -9.1351614][-6.6765852 -3.9558721 -2.93784 -2.288563 -1.3012925 1.1552873 4.58037 6.1340685 4.3086815 1.1105607 -1.6950854 -4.0751677 -6.682723 -8.94786 -9.3906374][-7.2213697 -4.9165249 -3.8764198 -3.2787888 -2.6459556 -0.96840537 1.4052789 2.2608969 0.53269219 -1.9640394 -3.9800324 -6.0163069 -8.0604916 -9.4953375 -9.4479809][-7.4244118 -5.4679108 -4.7572184 -4.4615822 -4.2807693 -3.4532368 -2.0095222 -1.6711099 -3.0558856 -4.6593847 -5.8453145 -7.5245543 -9.03434 -9.7466116 -9.322011][-7.0188141 -5.2800207 -5.0594316 -5.1714592 -5.5066347 -5.5272064 -4.8982897 -4.8509951 -5.82469 -6.5681572 -7.1517477 -8.4521666 -9.5519857 -9.67618 -8.8944006][-5.8083186 -4.4525232 -4.7804651 -5.297811 -5.9511738 -6.5110025 -6.425663 -6.4068422 -6.9259491 -7.0497885 -7.1857061 -8.075798 -8.8317623 -8.6816492 -7.8651609][-5.0598221 -4.1342173 -4.7513075 -5.2886505 -5.8472071 -6.4057245 -6.4889708 -6.3485861 -6.4096117 -6.2231855 -6.1798611 -6.6997118 -7.2017431 -7.0839176 -6.4317503][-4.8243275 -4.08892 -4.6824932 -5.107934 -5.4966021 -5.9754958 -6.133297 -5.8358574 -5.5025377 -5.1721611 -5.0895052 -5.3211079 -5.6279659 -5.6382551 -5.2182055]]...]
INFO - root - 2017-12-15 08:49:32.640723: step 43410, loss = 0.19, batch loss = 0.15 (33.0 examples/sec; 0.243 sec/batch; 19h:29m:07s remains)
INFO - root - 2017-12-15 08:49:34.926710: step 43420, loss = 0.20, batch loss = 0.17 (34.7 examples/sec; 0.230 sec/batch; 18h:29m:12s remains)
INFO - root - 2017-12-15 08:49:37.192464: step 43430, loss = 0.21, batch loss = 0.18 (35.6 examples/sec; 0.225 sec/batch; 18h:03m:43s remains)
INFO - root - 2017-12-15 08:49:39.497022: step 43440, loss = 0.26, batch loss = 0.23 (34.7 examples/sec; 0.231 sec/batch; 18h:32m:10s remains)
INFO - root - 2017-12-15 08:49:41.763194: step 43450, loss = 0.27, batch loss = 0.24 (36.3 examples/sec; 0.221 sec/batch; 17h:43m:06s remains)
INFO - root - 2017-12-15 08:49:44.035092: step 43460, loss = 0.27, batch loss = 0.24 (34.4 examples/sec; 0.233 sec/batch; 18h:41m:25s remains)
INFO - root - 2017-12-15 08:49:46.318639: step 43470, loss = 0.27, batch loss = 0.24 (35.7 examples/sec; 0.224 sec/batch; 17h:58m:38s remains)
INFO - root - 2017-12-15 08:49:48.623420: step 43480, loss = 0.25, batch loss = 0.22 (33.9 examples/sec; 0.236 sec/batch; 18h:55m:41s remains)
INFO - root - 2017-12-15 08:49:50.866260: step 43490, loss = 0.26, batch loss = 0.23 (35.5 examples/sec; 0.225 sec/batch; 18h:04m:07s remains)
INFO - root - 2017-12-15 08:49:53.152425: step 43500, loss = 0.25, batch loss = 0.22 (32.6 examples/sec; 0.246 sec/batch; 19h:42m:45s remains)
2017-12-15 08:49:53.446113: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6228971 -5.5482035 -6.1290956 -6.55186 -6.6950769 -6.7552042 -6.7933645 -6.6907845 -6.2612057 -5.5096636 -4.7001982 -3.9687438 -3.1165338 -1.93195 -0.63033807][-5.3732557 -6.0681195 -6.6540871 -7.0765676 -7.2326784 -7.2397308 -7.041688 -6.5292568 -5.5942698 -4.3476419 -3.07892 -2.08649 -1.3207815 -0.60106766 0.13403535][-5.4118857 -6.223381 -6.8952436 -7.2852783 -7.2704182 -6.8715668 -6.0797291 -5.0328054 -3.8513331 -2.6350751 -1.4825325 -0.63224208 -0.31123018 -0.2671771 -0.09160924][-6.1055374 -6.6444311 -7.1809845 -7.1890411 -6.5933008 -5.4704719 -4.0616951 -2.7670155 -1.8035134 -1.1370986 -0.63830411 -0.3604064 -0.43558598 -0.66050267 -0.68857396][-6.9506731 -7.00249 -7.0318251 -6.3295603 -5.0345917 -3.3600843 -1.616176 -0.3524853 0.099033356 -0.033158064 -0.20477223 -0.23863077 -0.24711108 -0.5334692 -0.80024827][-7.3766785 -6.9374371 -6.3462381 -5.0043774 -3.2256935 -1.260972 0.69493437 1.969476 2.0604408 1.3365383 0.59887028 0.0902009 -0.22465181 -0.60503685 -0.97123253][-6.9631996 -6.55762 -5.5846291 -3.8895159 -1.8614919 0.31595659 2.4393051 3.8793705 3.885977 2.6236994 1.1134005 -0.19312477 -0.791 -1.0987065 -1.3139461][-7.0049152 -6.3030539 -5.0083432 -3.0561888 -0.91143107 1.2295017 3.2419789 4.5931654 4.347105 2.7131312 0.84905505 -0.634956 -1.1165063 -1.2355391 -1.4059367][-7.1176491 -6.1422548 -4.381094 -2.2131641 -0.25833309 1.3770592 2.7670286 3.6581838 3.2159379 1.7377613 0.15203094 -0.96169329 -1.2924145 -1.320632 -1.6112297][-6.804678 -5.5640011 -3.5940957 -1.6366327 -0.23879981 0.79913497 1.5714879 2.0117633 1.5825226 0.47275949 -0.58347905 -1.1978735 -1.2107302 -0.99484861 -1.3493245][-5.6139264 -4.4209661 -2.7231495 -1.1998587 -0.24171305 0.45880318 0.77395129 0.63515043 -0.044061422 -0.8274833 -1.4492552 -1.6388772 -1.249374 -0.8758775 -1.5096691][-4.8094978 -3.8245249 -2.4703228 -1.1744257 -0.36310458 0.18806052 0.21994686 -0.32670331 -1.1837096 -1.7126663 -2.1357827 -2.0129044 -1.6237764 -1.3364196 -2.1054773][-4.6110678 -3.6960816 -2.7015128 -1.5712266 -0.76653528 -0.26725817 -0.29371655 -0.94174469 -1.8150511 -2.2805495 -2.6053677 -2.5933909 -2.3352067 -2.0148673 -2.607][-3.988898 -3.076987 -2.6527452 -1.9708349 -1.0478779 -0.35726118 -0.11621308 -0.77571249 -1.8138998 -2.4031327 -2.8194118 -2.9074419 -2.5997553 -2.2990978 -2.7093167][-3.9773979 -3.3258958 -3.5656767 -3.1569026 -1.8586485 -0.82604742 -0.3167367 -0.87995243 -2.0526547 -2.7123253 -3.1213744 -3.0765605 -2.7326894 -2.5821073 -3.0501537]]...]
INFO - root - 2017-12-15 08:49:55.701893: step 43510, loss = 0.35, batch loss = 0.32 (35.7 examples/sec; 0.224 sec/batch; 17h:58m:29s remains)
INFO - root - 2017-12-15 08:49:57.942844: step 43520, loss = 0.19, batch loss = 0.16 (35.3 examples/sec; 0.226 sec/batch; 18h:10m:46s remains)
INFO - root - 2017-12-15 08:50:00.250090: step 43530, loss = 0.20, batch loss = 0.17 (36.2 examples/sec; 0.221 sec/batch; 17h:43m:47s remains)
INFO - root - 2017-12-15 08:50:02.528294: step 43540, loss = 0.23, batch loss = 0.20 (36.3 examples/sec; 0.221 sec/batch; 17h:42m:46s remains)
INFO - root - 2017-12-15 08:50:04.814368: step 43550, loss = 0.23, batch loss = 0.20 (33.7 examples/sec; 0.237 sec/batch; 19h:03m:37s remains)
INFO - root - 2017-12-15 08:50:07.118132: step 43560, loss = 0.25, batch loss = 0.21 (35.2 examples/sec; 0.227 sec/batch; 18h:15m:09s remains)
INFO - root - 2017-12-15 08:50:09.394249: step 43570, loss = 0.19, batch loss = 0.16 (35.8 examples/sec; 0.224 sec/batch; 17h:56m:18s remains)
INFO - root - 2017-12-15 08:50:11.696552: step 43580, loss = 0.18, batch loss = 0.15 (34.6 examples/sec; 0.231 sec/batch; 18h:33m:42s remains)
INFO - root - 2017-12-15 08:50:13.947909: step 43590, loss = 0.26, batch loss = 0.22 (34.9 examples/sec; 0.229 sec/batch; 18h:23m:54s remains)
INFO - root - 2017-12-15 08:50:16.238717: step 43600, loss = 0.21, batch loss = 0.18 (32.6 examples/sec; 0.246 sec/batch; 19h:42m:58s remains)
2017-12-15 08:50:16.527830: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0963387 -4.5460973 -4.7457972 -4.5498834 -3.5137386 -2.8845811 -3.0318096 -4.2085428 -5.7287636 -6.67593 -7.2030411 -6.7368984 -5.0826044 -2.6620467 -0.116997][-5.7481441 -5.2700367 -5.4997077 -5.3133025 -4.4324522 -3.7971957 -3.6024866 -4.2737255 -5.5308504 -6.5875549 -7.04379 -6.7072811 -5.4872837 -3.5184994 -1.1912742][-7.4456406 -5.7240953 -5.868453 -5.6562738 -4.8527813 -4.1002111 -3.4624081 -3.4589953 -4.3019228 -5.4316244 -6.157814 -6.3226471 -5.7665963 -4.387249 -2.5359945][-8.4465685 -5.7833529 -5.9323654 -5.6746254 -4.6887207 -3.5856318 -2.4664018 -1.8020978 -2.1150227 -3.3320155 -4.5703573 -5.4419956 -5.7792053 -5.1923704 -4.0406876][-8.6981344 -5.3707104 -5.5256653 -5.2353621 -3.9344308 -2.2075369 -0.48398292 0.85521841 0.98827648 -0.55275536 -2.4639645 -4.0029345 -5.1309433 -5.3359375 -5.0452862][-8.0781937 -4.8608751 -5.0447206 -4.795351 -3.2338004 -0.87157881 1.2821472 3.0525949 3.6490963 1.8840072 -0.61735678 -2.7692561 -4.4030137 -5.0631809 -5.49588][-7.2229748 -4.5745878 -4.8026628 -4.5773106 -2.837522 -0.077580929 2.2102172 4.0333357 4.7776251 3.0168817 0.24089479 -2.2539976 -4.1197076 -4.8409014 -5.4752016][-6.7989349 -4.40246 -4.8033066 -4.7018661 -3.07802 -0.34267461 1.7654521 3.4282563 4.1086645 2.4458568 -0.32706773 -2.847 -4.520062 -4.94347 -5.48812][-6.7842951 -4.5168586 -5.0164871 -5.0779877 -3.7849922 -1.5138676 0.14218068 1.5192025 2.084718 0.71908712 -1.7154126 -4.0594883 -5.4707766 -5.5552073 -5.8548775][-7.2574835 -4.9224362 -5.3269148 -5.3913393 -4.3197122 -2.6830528 -1.6211399 -0.609581 -0.21802211 -1.1411345 -3.0619416 -5.0632439 -6.1747251 -6.00688 -6.1296678][-7.7710848 -5.0795736 -5.2754312 -5.3803296 -4.6663122 -3.7997973 -3.2414527 -2.4275668 -2.1129944 -2.7452583 -4.2029614 -5.7447805 -6.5263052 -6.1833134 -6.1856833][-8.9225769 -5.6535959 -5.3565779 -5.2120171 -4.695672 -4.5597677 -4.5091076 -3.8746839 -3.5675354 -3.9898665 -4.9604244 -5.9901943 -6.4198923 -6.0472822 -6.05595][-10.175779 -6.1283121 -4.9852724 -4.2706127 -3.8071337 -4.3879342 -4.8871479 -4.5074596 -4.2298427 -4.4065561 -4.9050479 -5.4912167 -5.6365318 -5.2986956 -5.3131371][-11.11271 -6.6289415 -4.8193359 -3.587801 -3.2616634 -4.3752308 -5.1443853 -4.990036 -4.7909632 -4.8741651 -5.1082344 -5.3605762 -5.2609453 -4.9253836 -4.9157696][-11.885306 -7.5240345 -5.502522 -4.1137238 -4.0167537 -5.2404447 -5.882462 -5.8063774 -5.7391787 -5.8208938 -5.8911686 -5.894845 -5.6940432 -5.4517403 -5.4436221]]...]
INFO - root - 2017-12-15 08:50:18.800520: step 43610, loss = 0.32, batch loss = 0.28 (33.3 examples/sec; 0.240 sec/batch; 19h:15m:11s remains)
INFO - root - 2017-12-15 08:50:21.060379: step 43620, loss = 0.21, batch loss = 0.17 (36.1 examples/sec; 0.222 sec/batch; 17h:48m:12s remains)
INFO - root - 2017-12-15 08:50:23.322017: step 43630, loss = 0.19, batch loss = 0.15 (34.4 examples/sec; 0.233 sec/batch; 18h:40m:20s remains)
INFO - root - 2017-12-15 08:50:25.606688: step 43640, loss = 0.24, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 17h:58m:24s remains)
INFO - root - 2017-12-15 08:50:27.879127: step 43650, loss = 0.28, batch loss = 0.24 (36.3 examples/sec; 0.221 sec/batch; 17h:41m:46s remains)
INFO - root - 2017-12-15 08:50:30.154814: step 43660, loss = 0.24, batch loss = 0.21 (36.8 examples/sec; 0.217 sec/batch; 17h:25m:20s remains)
INFO - root - 2017-12-15 08:50:32.403743: step 43670, loss = 0.23, batch loss = 0.20 (36.4 examples/sec; 0.220 sec/batch; 17h:36m:57s remains)
INFO - root - 2017-12-15 08:50:34.711298: step 43680, loss = 0.23, batch loss = 0.20 (34.9 examples/sec; 0.229 sec/batch; 18h:23m:19s remains)
INFO - root - 2017-12-15 08:50:37.007865: step 43690, loss = 0.25, batch loss = 0.22 (33.4 examples/sec; 0.239 sec/batch; 19h:12m:24s remains)
INFO - root - 2017-12-15 08:50:39.316371: step 43700, loss = 0.28, batch loss = 0.24 (36.2 examples/sec; 0.221 sec/batch; 17h:44m:04s remains)
2017-12-15 08:50:39.602585: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3181751 -6.5493608 -5.85213 -3.8510003 -3.2178423 -2.4152451 -2.0859213 -2.2734251 -2.9422328 -4.0089369 -4.5282369 -4.74244 -4.5148849 -3.7661095 -2.7159171][-4.4110661 -6.8260918 -5.8634915 -3.6914427 -2.9448636 -2.2341409 -2.118793 -2.3386827 -2.9890597 -4.1164312 -4.8480759 -5.2359142 -5.1998091 -4.5268927 -3.2450409][-5.0994272 -6.7950182 -5.7938185 -3.67775 -2.9661727 -2.2226617 -2.0230241 -1.8741055 -2.022898 -2.7758212 -3.6207135 -4.454422 -5.1607809 -5.0085 -3.7539442][-6.2256236 -6.8510594 -5.8996167 -3.8718352 -3.1088037 -2.2033162 -1.6474444 -0.90344143 -0.48825264 -1.0358629 -2.1908391 -3.6692431 -5.2242675 -5.5359292 -4.317667][-7.016458 -6.9366145 -6.18478 -4.3518968 -3.5870152 -2.4302688 -1.2748134 0.21105313 1.0922718 0.44940972 -1.1953305 -3.3204226 -5.4605446 -5.9271693 -4.6512127][-7.4819784 -6.9128761 -6.3894405 -4.7684622 -3.9150264 -2.3887033 -0.47158074 1.8662076 3.0233531 2.0637074 -0.095939159 -2.6565821 -5.062604 -5.5729618 -4.3342485][-7.0695372 -6.8898396 -6.4637432 -4.7900772 -3.5399561 -1.4199713 1.285394 4.193831 5.2959905 3.8624597 1.1164896 -1.8176132 -4.4180961 -5.0046067 -3.916163][-7.0045733 -6.9333286 -6.4853296 -4.5917692 -2.7447205 0.10620785 3.3257003 6.2379274 6.8660994 4.762836 1.4719398 -1.6098931 -4.21177 -4.9532642 -4.0290618][-6.9575958 -6.8259172 -6.3386078 -4.3228917 -2.1578727 0.94610739 3.9798617 6.2799158 6.2952995 3.8421717 0.53870487 -2.1986074 -4.470191 -5.2570462 -4.4131765][-6.85495 -6.69261 -6.1693716 -4.1571264 -2.166748 0.51138568 2.7639918 4.1910381 3.9101911 1.812665 -0.78362441 -2.7754197 -4.6143651 -5.3323727 -4.5528936][-7.2201571 -7.0556746 -6.40698 -4.3441992 -2.7024238 -0.59613729 1.0893314 2.0792489 2.0229945 0.78402686 -0.90993619 -2.2105849 -3.6828909 -4.5399284 -4.1137061][-7.7807221 -7.6188059 -6.8063655 -4.6279159 -3.2583425 -1.3650626 0.38532615 1.6226752 2.1301312 1.6421425 0.455415 -0.55437469 -2.0901926 -3.2520697 -3.3083856][-8.0370293 -7.8851485 -7.0065041 -4.8425426 -3.6719198 -1.7851698 0.37640452 2.1762466 3.0417709 2.7082386 1.3079824 0.11633539 -1.5112145 -2.7490759 -2.94599][-7.8631449 -7.6710324 -6.7961926 -4.7348347 -3.7251134 -1.9011656 0.45744133 2.5071602 3.2112856 2.4876332 0.76508641 -0.45883906 -1.8465171 -2.7384372 -2.7750854][-7.4039788 -7.1361308 -6.286356 -4.3695555 -3.5439076 -1.9568144 0.20147777 1.9691315 2.1718116 1.2273364 -0.52311754 -1.4654067 -2.3126881 -2.8329184 -2.6006925]]...]
INFO - root - 2017-12-15 08:50:41.891841: step 43710, loss = 0.24, batch loss = 0.21 (34.4 examples/sec; 0.232 sec/batch; 18h:38m:37s remains)
INFO - root - 2017-12-15 08:50:44.164643: step 43720, loss = 0.36, batch loss = 0.33 (35.5 examples/sec; 0.225 sec/batch; 18h:04m:08s remains)
INFO - root - 2017-12-15 08:50:46.438140: step 43730, loss = 0.22, batch loss = 0.19 (36.2 examples/sec; 0.221 sec/batch; 17h:44m:57s remains)
INFO - root - 2017-12-15 08:50:48.725996: step 43740, loss = 0.38, batch loss = 0.35 (35.2 examples/sec; 0.227 sec/batch; 18h:14m:26s remains)
INFO - root - 2017-12-15 08:50:51.024259: step 43750, loss = 0.23, batch loss = 0.20 (34.7 examples/sec; 0.231 sec/batch; 18h:30m:19s remains)
INFO - root - 2017-12-15 08:50:53.270327: step 43760, loss = 0.23, batch loss = 0.20 (36.6 examples/sec; 0.218 sec/batch; 17h:30m:42s remains)
INFO - root - 2017-12-15 08:50:55.568976: step 43770, loss = 0.33, batch loss = 0.29 (35.4 examples/sec; 0.226 sec/batch; 18h:07m:42s remains)
INFO - root - 2017-12-15 08:50:57.834243: step 43780, loss = 0.33, batch loss = 0.30 (34.8 examples/sec; 0.230 sec/batch; 18h:24m:40s remains)
INFO - root - 2017-12-15 08:51:00.111511: step 43790, loss = 0.20, batch loss = 0.16 (34.8 examples/sec; 0.230 sec/batch; 18h:25m:37s remains)
INFO - root - 2017-12-15 08:51:02.405113: step 43800, loss = 0.32, batch loss = 0.29 (34.8 examples/sec; 0.230 sec/batch; 18h:26m:28s remains)
2017-12-15 08:51:02.715207: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4631608 -4.7637391 -4.9085765 -5.2062354 -5.2018404 -5.0568905 -4.8196678 -4.5595551 -4.3760424 -4.2464385 -4.4231863 -5.1075249 -5.8616209 -6.5600753 -6.5705614][-4.7720695 -5.9718962 -5.9820175 -6.221211 -6.1515007 -5.8495131 -5.536643 -5.301693 -5.0101109 -4.765913 -4.9234171 -5.6244135 -6.3597236 -6.8724794 -6.4932852][-6.9345975 -7.2354784 -7.2286215 -7.4136143 -7.2275076 -6.8294969 -6.4177518 -6.0090537 -5.5172362 -5.2039838 -5.4161286 -6.1754675 -6.9554677 -7.2937393 -6.5274458][-8.5814419 -8.1873512 -8.21042 -8.2009106 -7.7383962 -7.1520319 -6.468421 -5.714705 -5.1279869 -5.0745683 -5.5947227 -6.5996003 -7.4664845 -7.6821222 -6.839663][-9.0333557 -8.2305813 -8.194171 -7.9283195 -7.0752883 -6.1510553 -5.0660839 -4.0805979 -3.6381445 -4.0178657 -4.9667339 -6.4646959 -7.5729055 -7.8307033 -7.1073456][-9.215497 -7.7724915 -7.50467 -6.96165 -5.7499895 -4.4083605 -2.733037 -1.3795319 -0.99286616 -1.7555546 -3.1123664 -5.0899549 -6.6155734 -7.2722058 -6.9183216][-8.2878437 -6.7609038 -6.3025541 -5.442668 -3.8372178 -2.0982325 0.046968222 1.64206 2.0372925 1.1420696 -0.51578844 -2.8977346 -4.817687 -5.7568803 -5.7510414][-7.3051581 -5.4557066 -4.7306356 -3.5596528 -1.6236733 0.53084683 3.026711 4.6911511 4.9605112 3.8434877 1.9525347 -0.80888033 -3.1340365 -4.3289976 -4.4273219][-6.7984381 -4.6611004 -3.6416228 -2.3243003 -0.31826067 1.9737568 4.5602751 6.21348 6.4088278 5.1881666 3.0900555 0.060057878 -2.5573487 -3.9412532 -4.1436095][-7.303998 -5.0960665 -4.004302 -2.8431401 -1.1381608 0.99702883 3.5265193 5.2728748 5.4247508 3.9988379 1.9228916 -0.6030575 -3.0270944 -4.66922 -5.2998438][-7.89175 -5.7319679 -4.7756295 -3.8100147 -2.4552872 -0.634536 1.4455674 2.7591028 2.4359493 0.85198283 -0.83556831 -2.583405 -4.6083956 -6.2310295 -6.9068618][-8.2071762 -6.2727189 -5.7610168 -5.0893426 -3.930378 -2.4374976 -0.94351113 -0.27033758 -1.0252086 -2.5521877 -3.7313631 -4.7692785 -6.2468271 -7.61002 -8.0722523][-7.9425759 -6.4123378 -6.4848514 -6.2674193 -5.4827528 -4.6100807 -3.831636 -3.6051309 -4.2642555 -5.3031578 -6.0345731 -6.6082649 -7.5067568 -8.2389717 -8.2729921][-6.9452591 -5.7043781 -6.1478672 -6.3887978 -6.1248717 -5.8396006 -5.539567 -5.4521418 -5.8127956 -6.4037876 -6.8041668 -7.1004691 -7.5400019 -7.8465667 -7.720396][-6.1549883 -4.8556776 -5.1152625 -5.2720828 -5.1154718 -5.0881472 -5.1152382 -5.2350588 -5.4512486 -5.7313914 -5.9880266 -6.3075395 -6.6520205 -6.7411637 -6.4535913]]...]
INFO - root - 2017-12-15 08:51:04.990662: step 43810, loss = 0.31, batch loss = 0.28 (35.9 examples/sec; 0.223 sec/batch; 17h:50m:44s remains)
INFO - root - 2017-12-15 08:51:07.308829: step 43820, loss = 0.23, batch loss = 0.20 (34.3 examples/sec; 0.233 sec/batch; 18h:42m:39s remains)
INFO - root - 2017-12-15 08:51:09.561268: step 43830, loss = 0.21, batch loss = 0.18 (36.0 examples/sec; 0.222 sec/batch; 17h:47m:42s remains)
INFO - root - 2017-12-15 08:51:11.847340: step 43840, loss = 0.25, batch loss = 0.22 (34.9 examples/sec; 0.230 sec/batch; 18h:24m:08s remains)
INFO - root - 2017-12-15 08:51:14.147780: step 43850, loss = 0.20, batch loss = 0.16 (34.8 examples/sec; 0.230 sec/batch; 18h:26m:43s remains)
INFO - root - 2017-12-15 08:51:16.437031: step 43860, loss = 0.21, batch loss = 0.18 (35.1 examples/sec; 0.228 sec/batch; 18h:17m:03s remains)
INFO - root - 2017-12-15 08:51:18.738235: step 43870, loss = 0.15, batch loss = 0.12 (34.0 examples/sec; 0.235 sec/batch; 18h:50m:56s remains)
INFO - root - 2017-12-15 08:51:21.013522: step 43880, loss = 0.20, batch loss = 0.17 (35.6 examples/sec; 0.225 sec/batch; 18h:01m:06s remains)
INFO - root - 2017-12-15 08:51:23.305420: step 43890, loss = 0.30, batch loss = 0.27 (35.1 examples/sec; 0.228 sec/batch; 18h:16m:06s remains)
INFO - root - 2017-12-15 08:51:25.587885: step 43900, loss = 0.34, batch loss = 0.31 (34.9 examples/sec; 0.229 sec/batch; 18h:21m:15s remains)
2017-12-15 08:51:25.897430: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2314714 -4.0425692 -5.1977758 -5.7682419 -5.493144 -5.2178297 -4.1200528 -2.6172197 -2.3549736 -2.7750819 -3.7749677 -5.4899273 -6.6620703 -7.0062189 -7.0428514][-1.9087172 -4.1559248 -5.4858561 -5.9496903 -5.4965196 -4.9702244 -3.5407579 -1.8321419 -1.3755822 -1.7623949 -3.003036 -5.0456963 -6.5943565 -7.2845478 -7.5399609][-1.9840889 -3.6737041 -4.934546 -5.1749582 -4.5668116 -3.8812189 -2.3529725 -0.76993918 -0.4120661 -0.9036299 -2.2718976 -4.4430523 -6.1673822 -7.0882587 -7.5843124][-1.9680489 -2.9575827 -3.9921885 -3.9899788 -3.3657486 -2.7192097 -1.3287573 -0.025067568 0.16189766 -0.327119 -1.7431095 -3.8245721 -5.4123063 -6.458518 -7.2267981][-2.2221911 -2.3165603 -3.0274155 -2.748081 -2.0346239 -1.3299387 0.0044043064 0.98151064 0.97549677 0.39983368 -1.1453441 -3.2402802 -4.7448854 -5.8939619 -6.7757053][-2.2994077 -1.7266949 -2.0966251 -1.7399243 -0.97616518 -0.11828017 1.1391551 1.9229634 1.7813399 1.0195475 -0.74245155 -2.9258008 -4.4237523 -5.6937342 -6.5661077][-2.3233781 -1.6295682 -1.7274171 -1.2822347 -0.3997736 0.7120769 2.1077769 3.0372102 2.9379299 1.9080088 -0.27792251 -2.6534061 -4.2954292 -5.7722716 -6.6642761][-2.1502502 -1.5601342 -1.7364824 -1.4240975 -0.42916811 1.0479579 2.8402879 4.1631289 4.2393732 3.067117 0.58972931 -1.9239033 -3.8494 -5.6271181 -6.5919466][-1.8257883 -1.0435115 -1.5750349 -1.8431805 -1.1790985 0.2310977 2.1015761 3.6204021 3.9294617 2.8133857 0.41323352 -1.9422431 -3.9773297 -5.7575541 -6.5541859][-2.6099432 -1.6180561 -2.4233751 -3.1005163 -2.6958022 -1.57813 -0.051143169 1.3281422 1.6931505 0.73907828 -1.3266573 -3.2563803 -5.0478616 -6.4361115 -6.8768396][-3.9073586 -2.6398997 -3.4231622 -4.14355 -3.949827 -3.3103476 -2.4159315 -1.4580029 -1.1489902 -1.7961491 -3.4387088 -4.8890839 -6.3013372 -7.3018131 -7.5138054][-5.1471186 -3.6511683 -4.3170404 -5.0521145 -5.1268606 -4.90934 -4.638567 -4.2134919 -4.006176 -4.2624483 -5.3861103 -6.38204 -7.4184675 -8.1456614 -8.2068815][-6.84501 -5.2209978 -5.6673641 -6.1951041 -6.3372517 -6.3886042 -6.620657 -6.7011757 -6.6177769 -6.5139713 -7.0469904 -7.5767584 -8.2326231 -8.663434 -8.573081][-8.1363325 -6.66838 -6.9043159 -7.0765734 -7.0185938 -7.0061283 -7.4122648 -7.8036489 -7.8614092 -7.6508026 -7.9028997 -8.2246561 -8.5614862 -8.6285191 -8.321846][-7.9887114 -6.8822737 -6.9402828 -6.7409525 -6.4670887 -6.4092741 -6.7813911 -7.1486673 -7.1927338 -7.0608339 -7.2927237 -7.5716982 -7.7173166 -7.6116381 -7.2534227]]...]
INFO - root - 2017-12-15 08:51:28.182889: step 43910, loss = 0.23, batch loss = 0.20 (35.0 examples/sec; 0.228 sec/batch; 18h:18m:21s remains)
INFO - root - 2017-12-15 08:51:30.457935: step 43920, loss = 0.33, batch loss = 0.30 (35.9 examples/sec; 0.223 sec/batch; 17h:52m:24s remains)
INFO - root - 2017-12-15 08:51:32.724143: step 43930, loss = 0.23, batch loss = 0.20 (35.5 examples/sec; 0.226 sec/batch; 18h:05m:01s remains)
INFO - root - 2017-12-15 08:51:35.002874: step 43940, loss = 0.17, batch loss = 0.14 (34.3 examples/sec; 0.233 sec/batch; 18h:42m:49s remains)
INFO - root - 2017-12-15 08:51:37.297760: step 43950, loss = 0.23, batch loss = 0.20 (34.4 examples/sec; 0.233 sec/batch; 18h:39m:09s remains)
INFO - root - 2017-12-15 08:51:39.583623: step 43960, loss = 0.33, batch loss = 0.30 (33.8 examples/sec; 0.237 sec/batch; 18h:58m:23s remains)
INFO - root - 2017-12-15 08:51:41.862414: step 43970, loss = 0.23, batch loss = 0.20 (35.7 examples/sec; 0.224 sec/batch; 17h:56m:13s remains)
INFO - root - 2017-12-15 08:51:44.109723: step 43980, loss = 0.27, batch loss = 0.23 (35.6 examples/sec; 0.225 sec/batch; 18h:00m:04s remains)
INFO - root - 2017-12-15 08:51:46.397602: step 43990, loss = 0.26, batch loss = 0.22 (35.3 examples/sec; 0.227 sec/batch; 18h:10m:53s remains)
INFO - root - 2017-12-15 08:51:48.711449: step 44000, loss = 0.16, batch loss = 0.12 (35.3 examples/sec; 0.227 sec/batch; 18h:11m:04s remains)
2017-12-15 08:51:49.019266: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8893852 -4.6369038 -4.6687393 -4.4498072 -4.4383163 -4.47289 -4.5137491 -4.6945705 -5.1337204 -5.5420256 -5.4122119 -5.1544852 -5.9453897 -6.7011757 -6.6783552][-4.4039478 -4.2501497 -4.4814491 -4.6443663 -5.0629716 -5.125308 -4.8861117 -4.7164416 -5.0494542 -5.8724213 -5.8326826 -5.2720394 -5.8600368 -6.1770773 -5.7487583][-4.1648588 -3.0090961 -3.3444681 -3.8264718 -4.6952839 -4.8982105 -4.3314877 -3.7472186 -3.91663 -5.090394 -5.181098 -4.7320404 -5.6363735 -5.8984547 -5.3032985][-2.9768696 -0.89005077 -1.3132737 -2.0419035 -3.3726561 -3.7160408 -2.8319125 -1.7105618 -1.6377014 -3.1555443 -3.5309877 -3.5474553 -4.9647269 -5.4265404 -4.8228927][-1.3955975 1.1439874 0.52368665 -0.48714638 -2.1255546 -2.44497 -1.1487653 0.52501893 0.74482107 -1.2927376 -2.1492043 -2.5929492 -4.2569251 -4.7100487 -4.0203338][0.044523478 2.6675451 1.7672698 0.5657692 -1.0830448 -1.1798419 0.54781795 2.8964818 3.3220513 0.71322322 -0.85564041 -1.7615664 -3.4188907 -3.7000875 -2.8986962][0.67070508 3.3340695 2.4753196 1.2313983 -0.2367487 -0.033191681 2.0240772 4.93828 5.3658285 2.2703197 0.096171379 -1.2478967 -2.7277839 -2.7538493 -1.7716466][0.16861606 2.6869438 1.9224989 0.87514639 -0.29455948 -0.030716419 1.8178647 4.7338057 5.0173769 1.8630617 -0.37454355 -1.7397656 -2.8838983 -2.61963 -1.4835205][-1.0877508 1.2933903 0.90330958 0.23124337 -0.67491078 -0.54307795 0.72091722 2.9936116 3.0688756 0.42911053 -1.3201559 -2.6174965 -3.4660358 -2.9603872 -1.662087][-2.2426353 -0.077220917 -0.10326505 -0.39063656 -1.062142 -1.0379323 -0.42326045 1.1599102 1.1531286 -0.78642786 -2.0250044 -3.33114 -3.9842887 -3.1763029 -1.7401794][-2.9728889 -1.3081179 -1.3951977 -1.5632515 -2.0373867 -2.1385448 -2.0891254 -1.089904 -1.0965348 -2.262954 -3.0602241 -4.2568684 -4.6615129 -3.5579638 -1.9443624][-3.4336135 -2.2765348 -2.4771729 -2.6295304 -3.0408177 -3.3149433 -3.5214252 -2.8396096 -2.7152586 -3.1896572 -3.5058656 -4.3662195 -4.6472912 -3.4392776 -1.7916031][-3.73285 -2.9018328 -3.1170278 -3.2843828 -3.6625881 -4.046803 -4.3297453 -3.7132463 -3.396441 -3.3299794 -3.1352158 -3.5111294 -3.6573343 -2.3862054 -0.8829118][-4.1245337 -3.5208714 -3.6524363 -3.7768586 -3.9970076 -4.349474 -4.633503 -4.2461066 -3.9581378 -3.6027756 -3.1498704 -3.1033401 -3.0264342 -1.6385114 -0.21705151][-4.3909683 -4.0178585 -4.0685534 -4.0432158 -3.9745867 -4.1572552 -4.4556284 -4.4502282 -4.3162432 -3.8216832 -3.2586045 -2.8867662 -2.6329257 -1.2865665 -0.10737586]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 08:51:51.316734: step 44010, loss = 0.28, batch loss = 0.25 (35.2 examples/sec; 0.227 sec/batch; 18h:13m:36s remains)
INFO - root - 2017-12-15 08:51:53.587893: step 44020, loss = 0.24, batch loss = 0.20 (34.6 examples/sec; 0.231 sec/batch; 18h:32m:25s remains)
INFO - root - 2017-12-15 08:51:55.874035: step 44030, loss = 0.25, batch loss = 0.21 (34.3 examples/sec; 0.233 sec/batch; 18h:40m:39s remains)
INFO - root - 2017-12-15 08:51:58.136222: step 44040, loss = 0.17, batch loss = 0.14 (35.2 examples/sec; 0.227 sec/batch; 18h:12m:01s remains)
INFO - root - 2017-12-15 08:52:00.408308: step 44050, loss = 0.33, batch loss = 0.29 (36.4 examples/sec; 0.220 sec/batch; 17h:37m:59s remains)
INFO - root - 2017-12-15 08:52:02.671449: step 44060, loss = 0.23, batch loss = 0.20 (34.9 examples/sec; 0.229 sec/batch; 18h:22m:44s remains)
INFO - root - 2017-12-15 08:52:04.972736: step 44070, loss = 0.24, batch loss = 0.21 (33.9 examples/sec; 0.236 sec/batch; 18h:53m:52s remains)
INFO - root - 2017-12-15 08:52:07.784556: step 44080, loss = 0.24, batch loss = 0.21 (18.9 examples/sec; 0.423 sec/batch; 33h:51m:52s remains)
INFO - root - 2017-12-15 08:52:11.230885: step 44090, loss = 0.23, batch loss = 0.20 (35.7 examples/sec; 0.224 sec/batch; 17h:55m:42s remains)
INFO - root - 2017-12-15 08:52:13.513697: step 44100, loss = 0.24, batch loss = 0.21 (35.9 examples/sec; 0.223 sec/batch; 17h:49m:53s remains)
2017-12-15 08:52:13.788733: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4817796 -6.5804787 -7.1028662 -6.71082 -6.0445395 -5.319438 -4.611927 -4.4214792 -4.7375755 -5.7169666 -6.6615505 -7.00366 -7.3088961 -7.38661 -6.8520985][-4.6740508 -7.4854536 -8.14817 -7.3381519 -6.4012146 -5.593915 -4.6446228 -4.3303123 -4.6901035 -5.80155 -6.8391294 -7.1236372 -7.3555984 -7.38457 -7.0738211][-5.9777336 -8.2205191 -8.9576864 -7.5592594 -6.4206667 -5.6400127 -4.4704752 -3.9568634 -4.4583945 -5.83002 -7.0874777 -7.344451 -7.5239372 -7.3565168 -6.9966612][-5.6126571 -7.3769941 -8.3878107 -6.7146893 -5.5840244 -4.6894808 -3.0605664 -2.2727354 -2.857142 -4.5261626 -6.21434 -6.8850613 -7.3778315 -7.3000784 -7.0680466][-4.1384916 -5.3022013 -6.5398064 -4.8538113 -3.7457285 -2.7383449 -0.96559322 -0.028262615 -0.64732456 -2.481091 -4.4725046 -5.617507 -6.7001839 -7.0345945 -7.1431112][-3.2894878 -4.0991025 -5.3942881 -3.8260026 -2.8900142 -1.7845993 0.087517738 1.2295935 0.89001846 -0.77368057 -2.7182686 -4.2247806 -5.8104448 -6.6767855 -7.2132292][-3.0388279 -3.5220935 -4.7469482 -3.1216733 -2.0599556 -0.77696753 1.0831892 2.3203413 2.159385 0.34642053 -1.8687174 -3.8575273 -5.682353 -6.7671766 -7.4403172][-3.0184665 -2.8494186 -3.6689909 -1.8975958 -0.520668 1.0746384 2.945277 4.0786896 3.73266 1.3680813 -1.5593987 -4.1127162 -6.0539312 -7.250947 -7.8506231][-3.8565063 -3.1912429 -3.7769198 -1.9524658 -0.34130239 1.3003874 3.1233971 4.1958427 3.905165 1.5278902 -1.693521 -4.5599155 -6.305872 -7.566905 -8.0907259][-6.1115522 -4.8282366 -5.0326872 -3.1141882 -1.5525897 -0.10821366 1.5694082 2.5671265 2.1412084 -0.16207314 -3.2370408 -5.7549224 -7.0475903 -8.1260538 -8.3802195][-8.2225227 -6.5003166 -6.3518925 -4.5901704 -3.3388674 -2.3274617 -1.03757 -0.34142411 -0.99737215 -2.869036 -5.3467388 -7.1673841 -7.9420824 -8.7623425 -8.6903658][-9.5040941 -7.7293377 -7.5269928 -6.0920362 -5.1522613 -4.507813 -3.6608856 -3.3338099 -4.1753197 -5.6381884 -7.3837862 -8.48266 -8.9493151 -9.4709768 -9.0652924][-9.6655273 -7.9456835 -7.7273149 -6.610343 -6.0107212 -5.7139153 -5.3432536 -5.4297323 -6.4821835 -7.7620354 -9.0094051 -9.5529213 -9.7638521 -9.8734837 -9.1099348][-8.683547 -7.1548185 -6.9384594 -6.1756263 -5.9828873 -6.0433817 -6.0050955 -6.3020544 -7.3676205 -8.4341545 -9.1720152 -9.3506956 -9.4591932 -9.3844357 -8.5469341][-7.7040176 -6.3690128 -6.2004323 -5.8070717 -5.8548384 -5.9753237 -5.8994904 -6.1231661 -6.9992027 -7.8793 -8.27969 -8.3850317 -8.5764246 -8.44671 -7.6397982]]...]
INFO - root - 2017-12-15 08:52:16.079202: step 44110, loss = 0.19, batch loss = 0.15 (33.4 examples/sec; 0.239 sec/batch; 19h:10m:07s remains)
INFO - root - 2017-12-15 08:52:18.336351: step 44120, loss = 0.19, batch loss = 0.16 (36.3 examples/sec; 0.221 sec/batch; 17h:39m:51s remains)
INFO - root - 2017-12-15 08:52:20.626685: step 44130, loss = 0.24, batch loss = 0.21 (33.7 examples/sec; 0.238 sec/batch; 19h:02m:32s remains)
INFO - root - 2017-12-15 08:52:22.903056: step 44140, loss = 0.23, batch loss = 0.20 (34.8 examples/sec; 0.230 sec/batch; 18h:24m:44s remains)
INFO - root - 2017-12-15 08:52:25.222976: step 44150, loss = 0.26, batch loss = 0.23 (32.7 examples/sec; 0.245 sec/batch; 19h:36m:57s remains)
INFO - root - 2017-12-15 08:52:27.542496: step 44160, loss = 0.20, batch loss = 0.16 (34.4 examples/sec; 0.233 sec/batch; 18h:38m:17s remains)
INFO - root - 2017-12-15 08:52:29.807566: step 44170, loss = 0.20, batch loss = 0.17 (34.5 examples/sec; 0.232 sec/batch; 18h:33m:28s remains)
INFO - root - 2017-12-15 08:52:32.050093: step 44180, loss = 0.25, batch loss = 0.22 (35.7 examples/sec; 0.224 sec/batch; 17h:56m:29s remains)
INFO - root - 2017-12-15 08:52:34.332361: step 44190, loss = 0.26, batch loss = 0.23 (35.8 examples/sec; 0.224 sec/batch; 17h:54m:15s remains)
INFO - root - 2017-12-15 08:52:36.626915: step 44200, loss = 0.20, batch loss = 0.17 (33.6 examples/sec; 0.238 sec/batch; 19h:02m:34s remains)
2017-12-15 08:52:36.938197: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8979077 -5.0020676 -3.628695 -3.1766655 -2.6712122 -2.1612778 -2.2150812 -2.5306957 -2.2981443 -2.170882 -1.2837183 -0.3998462 0.50756836 0.39128184 -0.59374404][-4.7945986 -5.3253717 -4.082747 -3.6554885 -3.0240202 -2.3605494 -2.2388976 -2.3426368 -2.0809658 -2.2280266 -1.7103655 -1.2106743 -0.66050303 -0.92397952 -1.7752686][-5.2357163 -5.2351327 -4.0628786 -3.6447086 -2.8482182 -2.0577328 -1.7615916 -1.7963984 -1.790807 -2.1581955 -1.9537418 -1.8762505 -1.8829887 -2.3227592 -2.9366148][-5.4378648 -4.802546 -3.6035624 -3.1981227 -2.3320146 -1.4959129 -1.1023349 -1.1901915 -1.5253806 -2.0620914 -2.087949 -2.3888807 -2.8879397 -3.4649141 -3.9538302][-5.32215 -4.0283384 -2.6696248 -2.1799803 -1.1977209 -0.40431583 -0.10025334 -0.49456048 -1.3445888 -2.0708902 -2.1939604 -2.7814288 -3.6855726 -4.4307337 -4.926651][-5.024436 -3.2906656 -1.7196522 -0.98202908 0.30333161 1.1570988 1.3476741 0.55403924 -0.80032837 -1.813256 -2.1131113 -2.874517 -4.0811653 -5.0477304 -5.69131][-4.8185692 -2.9467032 -1.2310519 -0.24509287 1.3660645 2.4218323 2.8125598 2.0269139 0.511112 -0.77290964 -1.4752905 -2.5914977 -4.1851244 -5.4527645 -6.2513332][-4.7980165 -3.0986009 -1.3909642 -0.1920085 1.6834085 2.9018614 3.5221741 2.8495471 1.3414235 -0.15889597 -1.3145043 -2.7225211 -4.5149212 -5.8986244 -6.5387545][-5.3520842 -3.9629633 -2.4303169 -1.1425682 0.78719163 2.0630014 2.824724 2.275389 0.96030879 -0.55762196 -2.011451 -3.4423811 -5.0625143 -6.221004 -6.4544039][-5.7439513 -4.8311753 -3.7105155 -2.6899743 -1.05714 0.16595411 1.019928 0.6313777 -0.40435183 -1.7950499 -3.3553081 -4.6343794 -5.7792835 -6.3481321 -5.9846263][-5.8196993 -5.3419785 -4.7645149 -4.337553 -3.2670319 -2.1552987 -1.281925 -1.4924446 -2.1902468 -3.4179649 -4.9645548 -5.9883242 -6.529067 -6.4064264 -5.5833111][-6.0475063 -5.9112082 -5.7567158 -5.8695173 -5.307333 -4.2869825 -3.4002371 -3.4073286 -3.7636614 -4.7232251 -6.0746732 -6.8184023 -6.8574047 -6.2656736 -5.3075943][-6.0826125 -6.1016073 -6.1741142 -6.5928955 -6.3939238 -5.4935503 -4.60453 -4.412611 -4.5374031 -5.3075981 -6.4158144 -6.888299 -6.5847168 -5.7806091 -4.9374104][-5.5939112 -5.7464361 -6.112998 -6.9054985 -7.1088095 -6.4130373 -5.5676508 -5.1960526 -5.1188993 -5.672327 -6.4480562 -6.6449547 -6.1904631 -5.4446588 -4.8611307][-5.15635 -5.38873 -5.9561124 -6.913476 -7.3225603 -6.8212013 -6.0857716 -5.6922565 -5.5756645 -5.9073896 -6.30727 -6.2988815 -5.9239063 -5.4557934 -5.1668215]]...]
INFO - root - 2017-12-15 08:52:39.215201: step 44210, loss = 0.18, batch loss = 0.15 (34.4 examples/sec; 0.232 sec/batch; 18h:35m:58s remains)
INFO - root - 2017-12-15 08:52:41.489473: step 44220, loss = 0.23, batch loss = 0.20 (35.9 examples/sec; 0.223 sec/batch; 17h:51m:23s remains)
INFO - root - 2017-12-15 08:52:43.801496: step 44230, loss = 0.17, batch loss = 0.14 (34.8 examples/sec; 0.230 sec/batch; 18h:24m:08s remains)
INFO - root - 2017-12-15 08:52:46.058640: step 44240, loss = 0.22, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 17h:51m:39s remains)
INFO - root - 2017-12-15 08:52:48.382558: step 44250, loss = 0.23, batch loss = 0.20 (33.4 examples/sec; 0.239 sec/batch; 19h:09m:06s remains)
INFO - root - 2017-12-15 08:52:50.656694: step 44260, loss = 0.22, batch loss = 0.19 (34.6 examples/sec; 0.232 sec/batch; 18h:32m:13s remains)
INFO - root - 2017-12-15 08:52:52.941420: step 44270, loss = 0.25, batch loss = 0.22 (34.7 examples/sec; 0.230 sec/batch; 18h:26m:16s remains)
INFO - root - 2017-12-15 08:52:55.174807: step 44280, loss = 0.25, batch loss = 0.22 (35.8 examples/sec; 0.223 sec/batch; 17h:52m:45s remains)
INFO - root - 2017-12-15 08:52:57.439760: step 44290, loss = 0.21, batch loss = 0.18 (34.4 examples/sec; 0.233 sec/batch; 18h:36m:58s remains)
INFO - root - 2017-12-15 08:52:59.705474: step 44300, loss = 0.44, batch loss = 0.40 (35.9 examples/sec; 0.223 sec/batch; 17h:48m:54s remains)
2017-12-15 08:53:00.029962: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5695086 -5.4861889 -5.5582657 -5.571002 -5.6327505 -5.6440635 -5.6858664 -5.8163924 -5.8693018 -5.8040447 -5.680479 -5.5693145 -5.4668164 -5.3104563 -5.1823568][-5.2127819 -6.4597321 -6.4633684 -6.3280354 -6.1696477 -5.8896494 -5.7912016 -5.9685006 -6.1365824 -6.2576389 -6.336319 -6.2988358 -6.0997572 -5.8540993 -5.7137971][-7.1204243 -7.3253441 -7.181818 -6.8126154 -6.30768 -5.6520472 -5.3737278 -5.51173 -5.7126503 -6.0463562 -6.405118 -6.5002136 -6.31806 -6.1675262 -6.1137581][-8.557127 -7.7850008 -7.2851286 -6.4885235 -5.46739 -4.3589888 -3.8345389 -3.9179306 -4.3175926 -5.1239138 -5.9345541 -6.2799149 -6.3976917 -6.4779634 -6.4258957][-8.5852137 -7.2205744 -6.3894339 -5.13482 -3.668633 -2.1542423 -1.2742563 -1.1383109 -1.757309 -3.2188926 -4.6557493 -5.3052325 -5.9010925 -6.2685413 -6.2747383][-8.0814676 -6.1651468 -5.0962281 -3.4760485 -1.5964592 0.24716949 1.4440384 1.9327462 1.2567434 -0.71753168 -2.76089 -3.7517138 -5.0342236 -5.7872992 -5.9294243][-6.7003193 -4.9245491 -3.6571627 -1.889675 0.2330327 2.2551324 3.5638011 4.2574577 3.6357224 1.5198841 -0.8517462 -2.2243409 -4.2913108 -5.4338646 -5.5926108][-5.0106778 -3.110296 -1.9348295 -0.47023284 1.3541348 3.2486169 4.5851 5.4250145 4.8692694 2.9584687 0.54086256 -1.1253039 -3.7246614 -5.0553446 -5.2724791][-3.7556155 -2.1584866 -1.3030457 -0.33735383 1.0395002 2.6621416 3.8488319 4.6449156 4.209877 2.6949637 0.55357146 -0.97528577 -3.470325 -4.6720867 -4.9065986][-3.8667634 -2.7196712 -2.193198 -1.6368183 -0.66417646 0.60991025 1.6742473 2.4809949 2.256386 1.2313831 -0.30248177 -1.3837475 -3.3984842 -4.3152609 -4.5166235][-4.8746691 -4.1343031 -3.8086755 -3.5001009 -2.7788103 -1.6939034 -0.54849041 0.44144154 0.51147628 -0.090258121 -1.1237482 -1.9594711 -3.6364586 -4.27491 -4.3229918][-5.8874531 -5.3238974 -5.02707 -4.7636914 -4.1351333 -3.1298196 -1.9519076 -0.86343908 -0.61585665 -1.149519 -1.962254 -2.6833522 -4.2431631 -4.6886368 -4.4433918][-6.1164069 -5.3903637 -4.9462318 -4.7070885 -4.2588663 -3.4817698 -2.3875122 -1.367295 -1.2425629 -1.9021356 -2.6375098 -3.3933854 -5.004149 -5.300632 -4.68194][-5.3091235 -4.2890911 -3.8597155 -3.8267775 -3.7535052 -3.3054638 -2.5261331 -1.7605827 -1.7130654 -2.30722 -2.9893053 -3.7910004 -5.4333935 -5.5883336 -4.70895][-4.4402018 -3.122839 -2.8258424 -2.9499354 -3.109333 -2.8709025 -2.3389554 -1.7970189 -1.8044968 -2.3340764 -3.0372353 -3.8726535 -5.4792356 -5.5098271 -4.4772911]]...]
INFO - root - 2017-12-15 08:53:02.348307: step 44310, loss = 0.18, batch loss = 0.15 (35.1 examples/sec; 0.228 sec/batch; 18h:13m:53s remains)
INFO - root - 2017-12-15 08:53:04.629047: step 44320, loss = 0.17, batch loss = 0.14 (34.5 examples/sec; 0.232 sec/batch; 18h:33m:28s remains)
INFO - root - 2017-12-15 08:53:06.918334: step 44330, loss = 0.20, batch loss = 0.17 (34.7 examples/sec; 0.231 sec/batch; 18h:28m:33s remains)
INFO - root - 2017-12-15 08:53:09.210869: step 44340, loss = 0.26, batch loss = 0.23 (34.9 examples/sec; 0.229 sec/batch; 18h:21m:18s remains)
INFO - root - 2017-12-15 08:53:11.492017: step 44350, loss = 0.20, batch loss = 0.17 (35.2 examples/sec; 0.227 sec/batch; 18h:12m:04s remains)
INFO - root - 2017-12-15 08:53:13.762129: step 44360, loss = 0.22, batch loss = 0.19 (36.7 examples/sec; 0.218 sec/batch; 17h:26m:08s remains)
INFO - root - 2017-12-15 08:53:16.014725: step 44370, loss = 0.40, batch loss = 0.37 (36.1 examples/sec; 0.222 sec/batch; 17h:43m:47s remains)
INFO - root - 2017-12-15 08:53:18.324533: step 44380, loss = 0.23, batch loss = 0.20 (35.4 examples/sec; 0.226 sec/batch; 18h:03m:52s remains)
INFO - root - 2017-12-15 08:53:20.571108: step 44390, loss = 0.25, batch loss = 0.22 (35.6 examples/sec; 0.224 sec/batch; 17h:57m:35s remains)
INFO - root - 2017-12-15 08:53:22.833244: step 44400, loss = 0.23, batch loss = 0.20 (35.8 examples/sec; 0.224 sec/batch; 17h:54m:07s remains)
2017-12-15 08:53:23.208128: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6880221 -3.8976183 -2.9760551 -2.483665 -2.2090557 -1.6631246 -1.5219328 -2.1064148 -2.0724475 -1.7847356 -1.5388105 -1.3731606 -1.3111703 -1.3554194 -1.2925148][-3.8815618 -3.4064116 -2.4839108 -1.9785826 -1.6988796 -1.2456712 -1.2710981 -2.0382173 -2.0896807 -1.8284291 -1.6052274 -1.4444267 -1.4427325 -1.5006838 -1.4120712][-3.5704598 -2.4004622 -1.3671769 -0.75422382 -0.50521231 -0.23279452 -0.53422236 -1.5904566 -1.8595231 -1.7478017 -1.5985825 -1.4739082 -1.5134609 -1.5994525 -1.5329361][-3.3675661 -1.7072757 -0.52265596 0.21070623 0.42811155 0.56825638 0.10766101 -1.1489196 -1.6124561 -1.6567183 -1.5916393 -1.508925 -1.5792249 -1.7029034 -1.7153078][-3.7804117 -1.8827758 -0.65629673 0.17225051 0.41837931 0.6162982 0.24868536 -0.94461918 -1.4152827 -1.5417489 -1.5353072 -1.513642 -1.6257575 -1.7972329 -1.89645][-4.4761057 -2.6330938 -1.4703848 -0.61901867 -0.31832111 0.015854597 -0.020921946 -0.88582289 -1.2625504 -1.4320188 -1.5048909 -1.5419376 -1.7196931 -1.9360454 -2.089256][-4.5302391 -3.1968083 -2.0895815 -1.192531 -0.7911427 -0.29920614 -0.030349731 -0.59239268 -0.86171913 -1.1141708 -1.3060405 -1.5303617 -1.8750069 -2.1993785 -2.5133111][-4.7927036 -3.4888368 -2.4509628 -1.5231031 -0.98067546 -0.39611781 0.035672665 -0.36691558 -0.57654881 -0.94918692 -1.3072088 -1.7866231 -2.2855403 -2.6935704 -3.2119641][-4.6768904 -3.25884 -2.2726119 -1.4044956 -0.86980379 -0.3883096 -0.10152674 -0.56998014 -0.75590026 -1.2076676 -1.6445341 -2.2439024 -2.7525787 -3.1572723 -3.7973666][-4.2534609 -2.5348768 -1.5444415 -0.84047449 -0.50149858 -0.25175071 -0.29003429 -1.0558703 -1.2512206 -1.6424123 -2.0043733 -2.6709332 -3.1696007 -3.5534902 -4.2352953][-3.4793825 -1.391167 -0.29569471 0.2639184 0.29388571 0.23732972 -0.3229388 -1.6719 -1.9761519 -2.2715669 -2.4905329 -3.1484857 -3.5832157 -3.8371587 -4.4199247][-2.8612421 -0.50867462 0.743309 1.2566478 1.0597489 0.77117968 -0.24921918 -2.106869 -2.5492685 -2.7452664 -2.8894756 -3.5312958 -3.8513927 -3.9144902 -4.2847958][-2.7845397 -0.41624939 0.813817 1.2595413 0.94987154 0.6895752 -0.43282759 -2.4037893 -2.8366265 -2.8156443 -2.7884588 -3.3700433 -3.5809054 -3.514009 -3.6577773][-2.893039 -0.72216821 0.30487728 0.60828114 0.26338029 0.15814137 -0.84868395 -2.7012694 -3.0677528 -2.7580655 -2.4314013 -2.8283794 -2.869921 -2.6356924 -2.5610752][-3.1761923 -1.3076653 -0.55697906 -0.39077461 -0.67898595 -0.57991445 -1.3562951 -2.8567882 -3.0256548 -2.4255738 -1.7941947 -1.9930837 -1.9680505 -1.6230252 -1.3975108]]...]
INFO - root - 2017-12-15 08:53:25.396561: step 44410, loss = 0.19, batch loss = 0.15 (35.6 examples/sec; 0.225 sec/batch; 17h:59m:57s remains)
INFO - root - 2017-12-15 08:53:27.651570: step 44420, loss = 0.30, batch loss = 0.27 (35.0 examples/sec; 0.229 sec/batch; 18h:18m:14s remains)
INFO - root - 2017-12-15 08:53:29.925660: step 44430, loss = 0.19, batch loss = 0.16 (34.6 examples/sec; 0.231 sec/batch; 18h:30m:59s remains)
INFO - root - 2017-12-15 08:53:32.258344: step 44440, loss = 0.33, batch loss = 0.30 (34.3 examples/sec; 0.233 sec/batch; 18h:39m:33s remains)
INFO - root - 2017-12-15 08:53:34.540002: step 44450, loss = 0.20, batch loss = 0.17 (35.9 examples/sec; 0.223 sec/batch; 17h:48m:21s remains)
INFO - root - 2017-12-15 08:53:36.826194: step 44460, loss = 0.29, batch loss = 0.26 (35.4 examples/sec; 0.226 sec/batch; 18h:03m:47s remains)
INFO - root - 2017-12-15 08:53:39.133626: step 44470, loss = 0.19, batch loss = 0.15 (31.8 examples/sec; 0.252 sec/batch; 20h:08m:35s remains)
INFO - root - 2017-12-15 08:53:41.392968: step 44480, loss = 0.17, batch loss = 0.14 (34.3 examples/sec; 0.233 sec/batch; 18h:40m:21s remains)
INFO - root - 2017-12-15 08:53:43.654316: step 44490, loss = 0.36, batch loss = 0.33 (35.2 examples/sec; 0.227 sec/batch; 18h:11m:33s remains)
INFO - root - 2017-12-15 08:53:45.915122: step 44500, loss = 0.30, batch loss = 0.27 (34.8 examples/sec; 0.230 sec/batch; 18h:24m:28s remains)
2017-12-15 08:53:46.199101: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7365475 -7.0940647 -7.4744849 -7.454185 -7.0868192 -6.5663619 -6.3866405 -6.4395247 -6.3431735 -6.7720361 -6.9507375 -5.8738232 -5.0994434 -5.1277628 -5.2805319][-6.252193 -7.972559 -8.3994522 -8.3631277 -7.8971858 -7.2620344 -7.1015286 -7.2237716 -7.2664127 -7.8548336 -8.0403652 -6.6325989 -5.599474 -5.477457 -5.2217903][-7.1503372 -8.1607046 -8.6561852 -8.53116 -7.8601465 -7.0562181 -6.8647757 -7.173645 -7.5723395 -8.4898052 -8.7294216 -7.0232425 -5.6080604 -5.0518665 -4.2455244][-7.6423488 -7.8863335 -8.4260845 -8.0185165 -6.8028679 -5.5477266 -5.2752209 -5.9882975 -7.1204886 -8.6986675 -9.1849766 -7.3752666 -5.6591959 -4.6005082 -3.3251448][-7.9131083 -7.4095297 -7.7221341 -6.6775131 -4.4947639 -2.430073 -2.0251622 -3.2540658 -5.3749046 -7.8351331 -8.74926 -7.2753363 -5.6910706 -4.3675261 -2.81702][-7.5593462 -7.0903597 -6.9296527 -5.0487242 -1.7916453 1.2190342 1.999023 0.4980619 -2.4466386 -5.803978 -7.4437041 -6.6054091 -5.318222 -3.8169203 -2.1035039][-6.996757 -6.8407249 -6.1861525 -3.4932356 0.71232605 4.5519271 5.6659255 4.1395755 0.70757532 -3.4111485 -5.989666 -6.0000172 -5.1413584 -3.4745877 -1.6086526][-6.6361861 -6.4685483 -5.6736279 -2.5985641 2.1139402 6.4088521 7.7303691 6.383451 2.6993322 -1.8342627 -4.972106 -5.6915188 -5.2900877 -3.7439141 -1.8329213][-6.0345111 -6.2469835 -5.7451434 -3.0859566 1.1596718 5.2323556 6.7159171 5.6110311 2.0813622 -2.1461065 -5.0462255 -5.8646412 -5.5768194 -4.1720724 -2.3541114][-5.4865913 -6.0361819 -6.1871071 -4.4994197 -1.3612211 2.0145674 3.5196304 2.7130508 -0.42686594 -3.8628917 -5.9830046 -6.34417 -5.9298735 -4.750495 -3.2538571][-5.0467191 -5.823153 -6.5390835 -5.81345 -3.8067856 -1.2461034 0.087762833 -0.4985224 -3.0747013 -5.6039109 -6.8818541 -6.9026866 -6.4227591 -5.5365648 -4.5512547][-5.541378 -6.3808708 -7.4264088 -7.305337 -6.2766623 -4.564168 -3.4334238 -3.8396168 -5.6586514 -7.1650219 -7.6806264 -7.4549761 -6.8547564 -6.2523613 -5.8624687][-6.3316841 -6.9233427 -8.0379963 -8.2913265 -7.9696364 -6.8506994 -5.8505325 -5.9342723 -7.061686 -7.7906771 -7.8704929 -7.6098714 -7.0803628 -6.7601323 -6.8957572][-7.0008054 -7.0897164 -7.9267516 -8.3353214 -8.314146 -7.5640821 -6.7693725 -6.7165356 -7.3863831 -7.7745523 -7.7721725 -7.625102 -7.2356005 -7.1208649 -7.5475836][-7.5830069 -7.1825957 -7.6265192 -7.9256024 -8.0945644 -7.6470337 -7.0678482 -7.0047092 -7.4140434 -7.665534 -7.7074184 -7.6768394 -7.4011331 -7.364212 -7.7211161]]...]
INFO - root - 2017-12-15 08:53:48.449697: step 44510, loss = 0.21, batch loss = 0.18 (36.2 examples/sec; 0.221 sec/batch; 17h:42m:03s remains)
INFO - root - 2017-12-15 08:53:50.714286: step 44520, loss = 0.25, batch loss = 0.22 (35.9 examples/sec; 0.223 sec/batch; 17h:50m:50s remains)
INFO - root - 2017-12-15 08:53:52.998452: step 44530, loss = 0.24, batch loss = 0.21 (36.2 examples/sec; 0.221 sec/batch; 17h:41m:42s remains)
INFO - root - 2017-12-15 08:53:55.269536: step 44540, loss = 0.22, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 17h:49m:56s remains)
INFO - root - 2017-12-15 08:53:57.543055: step 44550, loss = 0.16, batch loss = 0.13 (34.9 examples/sec; 0.229 sec/batch; 18h:20m:28s remains)
INFO - root - 2017-12-15 08:53:59.807532: step 44560, loss = 0.21, batch loss = 0.18 (35.0 examples/sec; 0.229 sec/batch; 18h:18m:09s remains)
INFO - root - 2017-12-15 08:54:02.087641: step 44570, loss = 0.24, batch loss = 0.20 (36.2 examples/sec; 0.221 sec/batch; 17h:41m:29s remains)
INFO - root - 2017-12-15 08:54:04.342588: step 44580, loss = 0.19, batch loss = 0.16 (35.9 examples/sec; 0.223 sec/batch; 17h:48m:42s remains)
INFO - root - 2017-12-15 08:54:06.635639: step 44590, loss = 0.27, batch loss = 0.24 (34.5 examples/sec; 0.232 sec/batch; 18h:32m:16s remains)
INFO - root - 2017-12-15 08:54:08.936821: step 44600, loss = 0.23, batch loss = 0.19 (34.7 examples/sec; 0.231 sec/batch; 18h:26m:30s remains)
2017-12-15 08:54:09.250708: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9133081 -4.9233809 -4.8213358 -4.6099424 -4.548605 -4.8903437 -5.0849028 -5.2924905 -5.9078712 -6.4514365 -5.924098 -5.1142178 -3.386431 -1.4573747 -0.10575461][-4.40226 -4.5986481 -4.5297794 -4.3055964 -4.1675148 -4.3932266 -4.520155 -4.7961216 -5.4799471 -5.9966097 -5.4621038 -4.575778 -2.5135899 -0.25212598 1.298332][-4.3716369 -4.3528271 -4.4021773 -4.2201214 -4.0004244 -4.0149674 -4.0099983 -4.3344736 -5.1303678 -5.6926317 -5.1528034 -4.0866656 -1.8987305 0.30123138 1.9446039][-4.7035522 -4.5436163 -4.7652912 -4.5782714 -4.1014462 -3.7550416 -3.3890018 -3.5022197 -4.3954105 -5.1761732 -4.8064308 -3.6897681 -1.8277912 -0.11371684 1.3581059][-4.57699 -4.070888 -4.3841667 -4.0782208 -3.1975899 -2.3805473 -1.5227133 -1.3502022 -2.5468597 -3.7672782 -3.9517097 -3.2939727 -2.065249 -1.0424343 -0.059522629][-4.3506994 -3.3896313 -3.508162 -2.7766087 -1.3113893 0.17687941 1.7014947 2.2501063 0.88832307 -0.88413715 -2.075422 -2.359051 -1.9778137 -1.7103764 -1.2986636][-3.4530187 -2.7482555 -2.6002762 -1.4598355 0.52992129 2.5051146 4.4700193 5.3281717 3.8862729 1.6187921 -0.45632291 -1.3963766 -1.5578914 -1.820731 -1.8862337][-3.4949312 -2.5112352 -2.2279441 -0.88508928 1.3291118 3.5101051 5.7360935 6.9397783 5.6642165 3.269311 0.6858952 -0.6564064 -1.1342182 -1.6002743 -1.9481654][-3.8109648 -2.7242823 -2.4374239 -1.2757425 0.67251086 2.7762885 4.9486022 6.1807756 5.0623484 2.8724666 0.35977769 -0.92682004 -1.256933 -1.5867699 -1.964983][-4.317451 -3.5451524 -3.5453126 -2.8888705 -1.4127165 0.36275005 2.1594009 3.2325287 2.4736409 0.83935189 -1.1176081 -1.9652745 -1.9858763 -2.110517 -2.3877745][-4.7786674 -4.3081818 -4.7023087 -4.5352373 -3.5090551 -2.2041965 -1.0283117 -0.22743845 -0.70984173 -1.8054209 -3.154916 -3.4492512 -3.1915083 -3.1677511 -3.3051584][-4.6709843 -4.2522511 -5.009428 -5.2600136 -4.7461381 -4.0533862 -3.4871411 -2.9661238 -3.1649756 -3.735399 -4.4613457 -4.4346304 -4.1835203 -4.2481389 -4.3825793][-4.3225527 -3.9946947 -5.0448 -5.473835 -5.2548361 -5.0618639 -5.0020266 -4.8026886 -4.9166965 -5.1836462 -5.5334644 -5.4018755 -5.2507153 -5.3351502 -5.410861][-4.3279858 -3.9835577 -5.0686913 -5.39729 -5.1690197 -5.2438803 -5.5214977 -5.6691351 -5.9399257 -6.0413313 -6.1049376 -5.9863296 -6.0705318 -6.33069 -6.455791][-4.3048325 -3.8832002 -4.80235 -4.9325161 -4.6830482 -4.8284645 -5.1712742 -5.6376343 -6.2276983 -6.3113208 -6.3550191 -6.3961945 -6.6516757 -7.054883 -7.2544169]]...]
INFO - root - 2017-12-15 08:54:11.481688: step 44610, loss = 0.33, batch loss = 0.30 (35.9 examples/sec; 0.223 sec/batch; 17h:48m:00s remains)
INFO - root - 2017-12-15 08:54:13.750007: step 44620, loss = 0.23, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 17h:45m:54s remains)
INFO - root - 2017-12-15 08:54:16.020450: step 44630, loss = 0.25, batch loss = 0.22 (35.3 examples/sec; 0.227 sec/batch; 18h:08m:43s remains)
INFO - root - 2017-12-15 08:54:18.325677: step 44640, loss = 0.24, batch loss = 0.20 (34.8 examples/sec; 0.230 sec/batch; 18h:23m:11s remains)
INFO - root - 2017-12-15 08:54:20.591802: step 44650, loss = 0.16, batch loss = 0.12 (35.9 examples/sec; 0.223 sec/batch; 17h:49m:34s remains)
INFO - root - 2017-12-15 08:54:22.886516: step 44660, loss = 0.16, batch loss = 0.13 (35.6 examples/sec; 0.225 sec/batch; 17h:58m:08s remains)
INFO - root - 2017-12-15 08:54:25.180995: step 44670, loss = 0.25, batch loss = 0.22 (34.8 examples/sec; 0.230 sec/batch; 18h:22m:49s remains)
INFO - root - 2017-12-15 08:54:27.441739: step 44680, loss = 0.19, batch loss = 0.16 (34.9 examples/sec; 0.229 sec/batch; 18h:18m:12s remains)
INFO - root - 2017-12-15 08:54:29.704976: step 44690, loss = 0.18, batch loss = 0.15 (35.2 examples/sec; 0.227 sec/batch; 18h:09m:38s remains)
INFO - root - 2017-12-15 08:54:31.984937: step 44700, loss = 0.34, batch loss = 0.31 (36.7 examples/sec; 0.218 sec/batch; 17h:26m:19s remains)
2017-12-15 08:54:32.262960: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2470026 -7.1741638 -6.9807558 -6.7952433 -6.5292339 -6.2786417 -6.2156968 -6.2867641 -6.3685446 -6.44277 -6.4845572 -6.5537205 -6.508153 -6.4408493 -6.1470108][-6.2232213 -8.134326 -7.83239 -7.6256962 -7.3531017 -7.0540876 -6.92708 -6.9585867 -6.9994841 -6.9905157 -6.9542084 -7.0737877 -7.1663256 -7.3236237 -7.21819][-6.6672335 -8.4994841 -8.2251606 -7.9487152 -7.5099297 -6.9426079 -6.5144157 -6.4379005 -6.5893507 -6.8410006 -6.9210587 -7.1689024 -7.3955593 -7.7104378 -7.8041868][-6.9691677 -8.18785 -8.1331968 -7.7076912 -6.8313875 -5.6870871 -4.7699442 -4.6106749 -5.0476861 -5.794498 -6.1348114 -6.6764727 -7.1472597 -7.6867008 -7.9938393][-6.7948923 -7.1386929 -7.2784214 -6.6746264 -5.2164574 -3.4511998 -2.1439836 -1.9727303 -2.7175014 -3.9202402 -4.6347857 -5.7121868 -6.46805 -7.205678 -7.7891779][-5.7212992 -5.4158621 -5.54391 -4.7075109 -2.6533282 -0.46379232 0.95866418 1.0227828 -0.11573172 -1.7518837 -2.8533664 -4.2990007 -5.1196718 -5.905529 -6.7691183][-4.4820042 -3.9598169 -3.9653873 -3.0546296 -0.64711475 1.8962843 3.595866 4.0283928 2.9746559 1.1233089 -0.50413525 -2.4808848 -3.6911449 -4.6885605 -5.6797795][-3.7797246 -3.1957054 -3.2034216 -2.5182717 -0.047026634 2.803829 4.9571962 6.0278778 5.2455034 3.2736294 1.1125412 -1.3512049 -2.8835812 -3.9970651 -4.930881][-3.2011421 -2.7416382 -2.9899554 -2.849684 -0.98200655 1.4805634 3.5933611 4.9658394 4.7030649 3.2335551 1.2099199 -1.2043036 -2.7848389 -4.1469946 -5.0849023][-3.0186224 -2.5845428 -3.2769976 -3.7936611 -2.8861241 -1.2696706 0.40449572 1.9421928 2.4093831 1.6000166 0.00070953369 -2.0623727 -3.5408754 -5.1333046 -5.8973165][-3.616611 -3.1396499 -4.249907 -5.0657282 -4.69274 -3.8256838 -2.8117416 -1.6098574 -0.84044349 -1.022697 -2.0022538 -3.4761896 -4.6890678 -6.2748718 -6.6661892][-4.6860533 -4.1702843 -5.4386387 -6.1441679 -5.9160075 -5.5891762 -5.0815396 -4.1996169 -3.3413625 -3.1508927 -3.8310192 -5.0665236 -6.0764251 -7.5749693 -7.648016][-5.8398743 -5.21248 -6.3094721 -6.7152357 -6.5657148 -6.5712709 -6.3360395 -5.7354069 -5.0616107 -4.7929368 -5.3338966 -6.3350477 -7.1520987 -8.4964466 -8.4281311][-6.6521978 -5.9223394 -6.6510391 -6.7271252 -6.6636858 -6.8015852 -6.721633 -6.3654041 -5.9843917 -5.8132248 -6.173245 -6.83951 -7.5490484 -8.6101694 -8.5076389][-6.8808661 -6.168786 -6.6511688 -6.5819807 -6.5482178 -6.4947872 -6.1388483 -5.8658447 -5.9292307 -6.0503321 -6.306489 -6.5986538 -7.0921736 -7.7498007 -7.6934042]]...]
INFO - root - 2017-12-15 08:54:34.504236: step 44710, loss = 0.23, batch loss = 0.20 (34.8 examples/sec; 0.230 sec/batch; 18h:22m:25s remains)
INFO - root - 2017-12-15 08:54:36.777006: step 44720, loss = 0.19, batch loss = 0.16 (35.1 examples/sec; 0.228 sec/batch; 18h:12m:42s remains)
INFO - root - 2017-12-15 08:54:39.037938: step 44730, loss = 0.17, batch loss = 0.14 (35.5 examples/sec; 0.225 sec/batch; 18h:00m:37s remains)
INFO - root - 2017-12-15 08:54:41.270602: step 44740, loss = 0.20, batch loss = 0.17 (35.4 examples/sec; 0.226 sec/batch; 18h:04m:26s remains)
INFO - root - 2017-12-15 08:54:43.539689: step 44750, loss = 0.20, batch loss = 0.17 (35.5 examples/sec; 0.225 sec/batch; 17h:59m:33s remains)
INFO - root - 2017-12-15 08:54:45.814984: step 44760, loss = 0.21, batch loss = 0.18 (35.1 examples/sec; 0.228 sec/batch; 18h:13m:53s remains)
INFO - root - 2017-12-15 08:54:48.077313: step 44770, loss = 0.17, batch loss = 0.14 (35.4 examples/sec; 0.226 sec/batch; 18h:03m:26s remains)
INFO - root - 2017-12-15 08:54:50.334974: step 44780, loss = 0.27, batch loss = 0.24 (35.9 examples/sec; 0.223 sec/batch; 17h:47m:18s remains)
INFO - root - 2017-12-15 08:54:52.629487: step 44790, loss = 0.17, batch loss = 0.14 (34.3 examples/sec; 0.233 sec/batch; 18h:39m:25s remains)
INFO - root - 2017-12-15 08:54:54.924454: step 44800, loss = 0.24, batch loss = 0.20 (35.7 examples/sec; 0.224 sec/batch; 17h:54m:05s remains)
2017-12-15 08:54:55.221086: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8845421 -3.4225545 -3.9628272 -4.6522055 -5.79112 -6.5357933 -6.4819021 -5.5659437 -5.0147057 -5.299603 -5.6758966 -5.8727274 -5.4188509 -5.20988 -5.5276995][-1.4034832 -2.5129488 -3.6901507 -4.7744465 -5.9352503 -6.3927193 -6.1392517 -5.1132803 -4.4879341 -4.813385 -5.2980127 -5.7449894 -5.4764929 -5.4688292 -5.7346954][-0.5606153 -1.5244465 -3.2202139 -4.5231171 -5.3278561 -5.2131023 -4.740212 -3.6346302 -2.9716651 -3.3762882 -4.1744938 -5.0033922 -5.2769322 -5.5204744 -5.7490959][-0.74869025 -1.0013099 -2.7890291 -3.8129044 -4.0709071 -3.5821228 -2.9093752 -1.8523246 -1.3442158 -2.0335505 -3.2008228 -4.2651892 -4.867557 -5.1977968 -5.2656145][-1.9145477 -1.0843661 -2.7024262 -3.3370028 -3.1215973 -2.2926717 -1.3542004 -0.38223839 -0.095164061 -1.0135436 -2.2615547 -3.3430266 -4.1305065 -4.68226 -4.8583536][-2.5657613 -1.5779338 -2.5151162 -2.5971913 -1.9817928 -0.85487926 0.23972726 1.217078 1.2722683 0.043246269 -1.342685 -2.4612703 -3.4626422 -4.1489153 -4.4642563][-3.6187544 -2.2419446 -2.2065289 -1.689482 -0.91010118 -0.03878355 0.95211744 1.8733084 1.7915962 0.28945518 -1.2055694 -2.3582113 -3.454906 -4.023839 -4.1865025][-4.43644 -2.6061244 -1.9961985 -1.1933717 -0.44949961 0.18382835 1.0569465 1.8869607 1.772568 0.17936158 -1.3390335 -2.5593867 -3.6603451 -4.2102051 -4.1902771][-4.9484015 -2.9499624 -1.9100565 -0.92307329 -0.12827182 0.48769307 1.2849436 2.0292146 1.7463725 0.16521549 -1.3107886 -2.6352348 -3.6625042 -4.0971651 -3.8687346][-5.1271954 -3.3525882 -2.4177377 -1.6057811 -0.77336609 -0.11484671 0.70601106 1.3828597 1.0760674 -0.31569183 -1.6755447 -2.9189849 -3.6136928 -3.6782985 -3.2108142][-4.7501736 -3.3842301 -2.9087427 -2.4672985 -1.6413915 -0.79474103 0.15562725 0.759686 0.4795115 -0.7147491 -1.9667096 -3.0586872 -3.709311 -3.7005227 -3.2284138][-3.6646621 -2.7643719 -2.7405305 -2.6432745 -1.9468395 -1.0411021 -0.10350418 0.20405245 -0.33326912 -1.4943867 -2.7811697 -3.8752117 -4.477448 -4.4094963 -3.8197708][-2.860688 -1.9655111 -1.9429384 -1.996719 -1.51479 -0.8382535 -0.27599919 -0.32005835 -1.0926757 -2.2551897 -3.4744444 -4.4934278 -4.936985 -4.8001232 -4.2355509][-2.0012665 -0.73743558 -0.60801804 -0.81260538 -0.76582897 -0.52019608 -0.37589669 -0.65071571 -1.4801582 -2.6272893 -3.8724718 -4.9516768 -5.4867454 -5.5205221 -5.2231979][-0.65380228 0.76944447 0.67576575 0.19704032 -0.28935087 -0.58239651 -0.89945245 -1.4651796 -2.3846779 -3.4997768 -4.7617655 -5.8018847 -6.2855444 -6.3108273 -6.0961542]]...]
INFO - root - 2017-12-15 08:54:57.530023: step 44810, loss = 0.29, batch loss = 0.25 (32.9 examples/sec; 0.243 sec/batch; 19h:27m:08s remains)
INFO - root - 2017-12-15 08:54:59.837681: step 44820, loss = 0.22, batch loss = 0.19 (34.6 examples/sec; 0.231 sec/batch; 18h:28m:00s remains)
INFO - root - 2017-12-15 08:55:02.158587: step 44830, loss = 0.24, batch loss = 0.21 (35.3 examples/sec; 0.227 sec/batch; 18h:06m:47s remains)
INFO - root - 2017-12-15 08:55:04.451930: step 44840, loss = 0.24, batch loss = 0.20 (35.7 examples/sec; 0.224 sec/batch; 17h:52m:57s remains)
INFO - root - 2017-12-15 08:55:06.735740: step 44850, loss = 0.30, batch loss = 0.27 (35.4 examples/sec; 0.226 sec/batch; 18h:04m:29s remains)
INFO - root - 2017-12-15 08:55:09.000659: step 44860, loss = 0.19, batch loss = 0.15 (36.2 examples/sec; 0.221 sec/batch; 17h:39m:55s remains)
INFO - root - 2017-12-15 08:55:11.292701: step 44870, loss = 0.29, batch loss = 0.25 (33.8 examples/sec; 0.237 sec/batch; 18h:54m:53s remains)
INFO - root - 2017-12-15 08:55:13.562285: step 44880, loss = 0.17, batch loss = 0.14 (34.9 examples/sec; 0.229 sec/batch; 18h:18m:00s remains)
INFO - root - 2017-12-15 08:55:15.814501: step 44890, loss = 0.26, batch loss = 0.23 (35.8 examples/sec; 0.224 sec/batch; 17h:52m:01s remains)
INFO - root - 2017-12-15 08:55:18.119294: step 44900, loss = 0.23, batch loss = 0.20 (32.2 examples/sec; 0.248 sec/batch; 19h:49m:21s remains)
2017-12-15 08:55:18.403409: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3693147 -5.4374409 -4.9733772 -4.7638054 -3.4025826 -2.6032615 -3.1971793 -4.2492628 -5.1892819 -5.7806325 -6.3621283 -6.9710741 -7.2418118 -6.5638113 -4.9626269][-6.9760294 -5.380784 -4.7994318 -4.54685 -2.9210422 -2.0470369 -2.865361 -4.1543059 -5.1779428 -5.7197466 -6.4874067 -7.3754396 -7.7423573 -6.9451456 -5.1440468][-7.3991547 -4.7241449 -4.2359829 -4.1834927 -2.424283 -1.3369005 -2.0491147 -3.2858315 -4.2597456 -4.8314142 -6.0471239 -7.3848648 -7.9769421 -7.2139091 -5.3387876][-7.0560093 -3.7301438 -3.6091104 -3.832088 -2.0279891 -0.47739017 -0.80697441 -1.9782367 -2.875113 -3.5069046 -4.997488 -6.884244 -7.8126698 -7.2358484 -5.4368792][-6.6395044 -3.0025558 -3.3182869 -3.6878238 -1.8185259 0.091584206 0.09261322 -0.8470062 -1.6758807 -2.34453 -3.9856167 -6.2462273 -7.5432367 -7.1180496 -5.3633318][-4.826951 -1.509805 -2.2258449 -2.6388407 -0.76942158 1.404335 1.7042661 0.94255781 0.030875683 -0.79111552 -2.4859912 -4.98425 -6.6280632 -6.6129236 -5.04422][-2.5630717 0.18524051 -0.67750645 -1.1403126 0.71313453 3.233989 3.9989402 3.4487507 2.1350448 0.79244161 -1.1840327 -3.8316865 -5.7569046 -6.1898403 -4.7498207][-1.5134666 1.3435826 0.49850726 0.18140626 2.086416 4.8550739 5.9343824 5.3557091 3.4307115 1.3570323 -1.0595695 -3.810266 -5.7825494 -6.2683182 -4.758791][-1.4571509 1.5435412 0.90948105 0.75251031 2.6075084 5.19629 6.10855 5.2920656 2.8886168 0.44758868 -2.1096516 -4.6090565 -6.23188 -6.5512047 -4.9742661][-2.4242675 0.54671073 -0.012131929 -0.25260234 1.1577342 3.2603314 4.0110874 3.3179815 1.1364493 -1.2506548 -3.585495 -5.6702256 -6.7783985 -6.7745934 -5.1719756][-3.8412292 -1.1189591 -1.6814147 -1.9655702 -0.84407485 0.87492561 1.7577531 1.6278567 -0.1658206 -2.5918841 -4.73796 -6.3161926 -7.0264044 -6.7016535 -5.0828362][-5.2275085 -2.8313088 -3.2577798 -3.3829322 -2.3738415 -1.0712599 -0.24796796 -0.1642127 -1.7429717 -3.999783 -5.706625 -6.7662811 -7.0996971 -6.45568 -4.8583565][-6.5145912 -4.367281 -4.5681543 -4.5075307 -3.6522052 -2.8029923 -2.1895149 -2.1018443 -3.4818888 -5.4566016 -6.6328411 -7.209753 -7.2535563 -6.4187593 -4.8272066][-7.6191921 -5.78941 -5.8263979 -5.5541334 -4.7680488 -4.1880631 -3.7718325 -3.8188634 -4.9714203 -6.5200005 -7.1750088 -7.3138885 -7.093379 -6.1119175 -4.6244259][-7.4849539 -6.0752521 -6.0176191 -5.6986389 -5.069665 -4.683794 -4.403862 -4.5568972 -5.4469819 -6.4178476 -6.6444445 -6.5191011 -6.1560407 -5.2716146 -4.1503496]]...]
INFO - root - 2017-12-15 08:55:20.657653: step 44910, loss = 0.25, batch loss = 0.22 (34.1 examples/sec; 0.235 sec/batch; 18h:44m:02s remains)
INFO - root - 2017-12-15 08:55:22.925356: step 44920, loss = 0.17, batch loss = 0.14 (35.5 examples/sec; 0.225 sec/batch; 17h:59m:03s remains)
INFO - root - 2017-12-15 08:55:25.194245: step 44930, loss = 0.25, batch loss = 0.22 (35.0 examples/sec; 0.229 sec/batch; 18h:15m:38s remains)
INFO - root - 2017-12-15 08:55:27.496967: step 44940, loss = 0.24, batch loss = 0.20 (34.2 examples/sec; 0.234 sec/batch; 18h:40m:18s remains)
INFO - root - 2017-12-15 08:55:29.784000: step 44950, loss = 0.39, batch loss = 0.36 (34.9 examples/sec; 0.229 sec/batch; 18h:17m:51s remains)
INFO - root - 2017-12-15 08:55:32.057116: step 44960, loss = 0.23, batch loss = 0.20 (34.7 examples/sec; 0.231 sec/batch; 18h:25m:12s remains)
INFO - root - 2017-12-15 08:55:34.327037: step 44970, loss = 0.25, batch loss = 0.22 (35.3 examples/sec; 0.227 sec/batch; 18h:07m:26s remains)
INFO - root - 2017-12-15 08:55:36.628321: step 44980, loss = 0.27, batch loss = 0.24 (33.5 examples/sec; 0.238 sec/batch; 19h:02m:40s remains)
INFO - root - 2017-12-15 08:55:38.894099: step 44990, loss = 0.28, batch loss = 0.25 (34.8 examples/sec; 0.230 sec/batch; 18h:22m:47s remains)
INFO - root - 2017-12-15 08:55:41.179903: step 45000, loss = 0.27, batch loss = 0.23 (36.2 examples/sec; 0.221 sec/batch; 17h:39m:48s remains)
2017-12-15 08:55:41.466680: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6954088 -4.6958656 -4.3360209 -4.7058616 -4.8842878 -5.0633578 -6.1327229 -6.6727552 -7.0702367 -7.6248155 -7.2532516 -6.5528369 -5.7095804 -5.5649595 -5.7377362][-4.5829935 -3.5083022 -3.1534042 -3.6687746 -3.8844752 -4.0469089 -5.1157169 -5.669414 -6.0411959 -6.6565032 -6.3558273 -5.7283516 -4.9744711 -4.8968687 -4.961875][-4.2321596 -2.1470587 -1.7721077 -2.4207227 -2.7754488 -2.8976781 -3.6947207 -4.0244923 -4.2476988 -4.8806572 -4.7127528 -4.1579156 -3.4808202 -3.3874636 -3.356158][-3.9952068 -1.1344922 -0.7084142 -1.2828128 -1.5932031 -1.4854524 -1.6669054 -1.5284426 -1.6983633 -2.3686104 -2.3732827 -2.1049597 -1.6868882 -1.6994905 -1.6281197][-4.0552821 -0.92259932 -0.51976848 -0.94974434 -1.1089396 -0.67939031 -0.13852 0.43004918 0.29089761 -0.32356918 -0.34786141 -0.22936273 -0.059517622 -0.29509246 -0.34915423][-4.2966413 -1.0525409 -0.52965009 -0.61125278 -0.426144 0.35654974 1.5171959 2.4660516 2.1629014 1.4173694 1.3249032 1.1530008 0.97432065 0.4133749 0.14788365][-4.1106033 -1.1453393 -0.23032212 0.16827774 0.61552882 1.4715366 2.7094164 3.6510282 3.1654978 2.0898051 1.5767109 1.0562978 0.67227459 0.063336611 -0.2356956][-4.7304897 -1.9720136 -1.0185359 -0.39501023 0.18697739 0.97406721 2.100709 3.0062089 2.6572065 1.6250298 0.81696677 -0.072724819 -0.64123774 -1.2493472 -1.5308219][-4.9933515 -2.7275798 -2.0252073 -1.5181859 -1.1098168 -0.49045181 0.55107594 1.3541484 1.0543399 0.034612656 -1.0483835 -2.0538011 -2.6022995 -3.0989177 -3.2742524][-5.3351803 -3.4852335 -2.9793015 -2.6323647 -2.4527595 -2.0448456 -1.2777296 -0.73060071 -1.1399357 -2.1074126 -3.2250047 -4.2015667 -4.5959244 -4.94213 -5.1113987][-5.7936573 -4.1237731 -3.5665517 -3.2715745 -3.3840413 -3.465688 -3.0859888 -2.7697711 -3.1830485 -3.9385707 -4.9187064 -5.6459813 -5.8634777 -6.1766024 -6.4595556][-6.3071246 -4.62516 -3.9984274 -3.8138981 -4.1994424 -4.6502066 -4.572084 -4.3477879 -4.6969137 -5.1962752 -5.8557034 -6.3592639 -6.459609 -6.8075018 -7.3564911][-7.2568245 -5.3991961 -4.6224842 -4.4061766 -4.8096027 -5.3884211 -5.4246941 -5.0506845 -5.2027431 -5.5750866 -6.0198956 -6.2912669 -6.3391709 -6.8771329 -7.6357193][-7.8865566 -5.8703556 -4.9628448 -4.5196085 -4.6446304 -5.140398 -5.3423443 -4.978888 -4.9742155 -5.2816324 -5.6215515 -5.7958012 -5.8607211 -6.4487929 -7.3245769][-7.8908658 -6.0824947 -5.1715384 -4.5481892 -4.46752 -4.9436865 -5.335659 -5.1459641 -5.1460991 -5.4762416 -5.6824808 -5.6527147 -5.6128216 -6.1398859 -7.0035081]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-45000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-45000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 08:55:44.229333: step 45010, loss = 0.24, batch loss = 0.21 (35.5 examples/sec; 0.225 sec/batch; 17h:59m:52s remains)
INFO - root - 2017-12-15 08:55:46.510770: step 45020, loss = 0.29, batch loss = 0.25 (34.1 examples/sec; 0.235 sec/batch; 18h:43m:51s remains)
INFO - root - 2017-12-15 08:55:48.790425: step 45030, loss = 0.18, batch loss = 0.15 (36.5 examples/sec; 0.219 sec/batch; 17h:30m:39s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 08:55:51.088246: step 45040, loss = 0.16, batch loss = 0.13 (35.7 examples/sec; 0.224 sec/batch; 17h:52m:48s remains)
INFO - root - 2017-12-15 08:55:53.401062: step 45050, loss = 0.19, batch loss = 0.16 (31.7 examples/sec; 0.252 sec/batch; 20h:07m:38s remains)
INFO - root - 2017-12-15 08:55:55.693495: step 45060, loss = 0.24, batch loss = 0.21 (35.9 examples/sec; 0.223 sec/batch; 17h:46m:58s remains)
INFO - root - 2017-12-15 08:55:57.998767: step 45070, loss = 0.18, batch loss = 0.15 (35.2 examples/sec; 0.227 sec/batch; 18h:08m:34s remains)
INFO - root - 2017-12-15 08:56:00.239864: step 45080, loss = 0.26, batch loss = 0.23 (35.7 examples/sec; 0.224 sec/batch; 17h:52m:45s remains)
INFO - root - 2017-12-15 08:56:02.507531: step 45090, loss = 0.32, batch loss = 0.28 (35.6 examples/sec; 0.225 sec/batch; 17h:56m:25s remains)
INFO - root - 2017-12-15 08:56:04.756290: step 45100, loss = 0.27, batch loss = 0.24 (35.0 examples/sec; 0.229 sec/batch; 18h:14m:53s remains)
2017-12-15 08:56:05.033861: I tensorflow/core/kernels/logging_ops.cc:79] [[[2.7652919 2.3699229 1.3604534 0.35791564 -0.83487821 -1.6861318 -1.7365471 -1.2536712 -0.70775735 -0.36405277 -0.42129779 -0.9198463 -1.1570916 -0.5675658 0.19897771][2.9880226 3.6607497 2.636708 1.081466 -0.888584 -2.2518265 -2.5629826 -2.0141151 -1.2928829 -0.506575 0.1143136 0.027265787 0.1120851 0.78185344 1.2097771][2.0181024 3.5507634 2.7581823 1.1016479 -1.0313412 -2.4168181 -2.8985052 -2.516789 -1.8842196 -0.90169811 0.25904179 0.75615716 1.2335212 1.8021305 1.5810909][-0.15647864 2.1644351 1.5915473 0.22611094 -1.3397632 -2.3064406 -2.6911278 -2.3235657 -1.7922344 -1.0120262 0.12828541 0.86056376 1.6488609 2.0731847 1.2382758][-2.2046678 0.56210709 0.40359879 -0.22008514 -0.94917488 -1.4084834 -1.6075211 -1.2290504 -0.97868979 -0.81621003 -0.2100153 0.37927294 1.0931413 1.1553781 -0.072834969][-4.0836668 -0.90893221 -0.31424439 0.0089027882 0.17985463 0.15074849 0.0089831352 0.19911242 0.020912647 -0.56189704 -0.56028259 -0.2656585 0.062599421 -0.42214024 -1.8634986][-5.324729 -2.1464005 -0.85312462 0.45484376 1.4021151 1.8138187 1.7617385 1.5788367 0.75037408 -0.53382874 -1.016263 -0.95537627 -1.0374017 -2.0627971 -3.5657697][-5.472374 -2.5670328 -1.065425 0.61621809 2.0646608 2.919245 3.0812213 2.570112 1.0292952 -0.86791861 -1.7830369 -2.1133454 -2.714103 -4.074439 -5.3361568][-5.4031997 -2.7657099 -1.4488282 0.10022283 1.5478222 2.6044543 3.029568 2.4090416 0.46197033 -1.7113817 -2.8918202 -3.5201023 -4.3994222 -5.58708 -6.2186584][-5.386745 -3.1740532 -2.2307491 -1.1362772 0.05915308 1.1040709 1.6042626 0.88896346 -1.1433926 -3.2162006 -4.3967381 -5.0352926 -5.70094 -6.307292 -6.3508253][-5.1871977 -3.6169233 -3.3823714 -2.8504088 -1.9207546 -0.92683995 -0.48110259 -1.2475302 -3.0138671 -4.6521339 -5.5813684 -6.0090666 -6.3100328 -6.3978157 -6.1158481][-4.9755774 -3.8105326 -4.1722851 -4.3402643 -3.8930635 -3.1867027 -2.9372025 -3.66904 -5.0032358 -6.0484343 -6.5116072 -6.4707975 -6.1823149 -5.6875014 -5.1041846][-4.7121058 -3.6689053 -4.4588237 -5.2509713 -5.4343777 -5.1909428 -5.1530218 -5.6574945 -6.3482866 -6.7353706 -6.6801233 -6.2118168 -5.50173 -4.6597309 -3.951684][-4.8887129 -3.8483584 -4.7120161 -5.6601763 -6.04008 -5.9585123 -5.8917637 -6.0561285 -6.2581472 -6.2786865 -6.0522509 -5.5648746 -4.7801681 -3.8744853 -3.2316732][-5.079113 -4.166604 -5.0657463 -5.98983 -6.358079 -6.287693 -6.1296949 -6.0379219 -5.971385 -5.8474703 -5.5562487 -5.0739865 -4.3605189 -3.6343236 -3.1469059]]...]
INFO - root - 2017-12-15 08:56:07.290808: step 45110, loss = 0.17, batch loss = 0.13 (34.3 examples/sec; 0.233 sec/batch; 18h:36m:49s remains)
INFO - root - 2017-12-15 08:56:09.585232: step 45120, loss = 0.24, batch loss = 0.21 (35.6 examples/sec; 0.224 sec/batch; 17h:55m:10s remains)
INFO - root - 2017-12-15 08:56:11.847343: step 45130, loss = 0.21, batch loss = 0.18 (36.1 examples/sec; 0.221 sec/batch; 17h:40m:12s remains)
INFO - root - 2017-12-15 08:56:14.118749: step 45140, loss = 0.33, batch loss = 0.30 (35.6 examples/sec; 0.225 sec/batch; 17h:56m:49s remains)
INFO - root - 2017-12-15 08:56:16.382369: step 45150, loss = 0.28, batch loss = 0.25 (34.6 examples/sec; 0.231 sec/batch; 18h:26m:22s remains)
INFO - root - 2017-12-15 08:56:18.664393: step 45160, loss = 0.34, batch loss = 0.31 (35.3 examples/sec; 0.227 sec/batch; 18h:06m:36s remains)
INFO - root - 2017-12-15 08:56:20.963556: step 45170, loss = 0.19, batch loss = 0.16 (33.5 examples/sec; 0.239 sec/batch; 19h:04m:35s remains)
INFO - root - 2017-12-15 08:56:23.240630: step 45180, loss = 0.18, batch loss = 0.15 (35.2 examples/sec; 0.227 sec/batch; 18h:07m:09s remains)
INFO - root - 2017-12-15 08:56:25.503994: step 45190, loss = 0.28, batch loss = 0.25 (36.1 examples/sec; 0.222 sec/batch; 17h:40m:56s remains)
INFO - root - 2017-12-15 08:56:27.784809: step 45200, loss = 0.28, batch loss = 0.25 (35.2 examples/sec; 0.227 sec/batch; 18h:08m:30s remains)
2017-12-15 08:56:28.073475: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5078182 -6.412406 -6.6380925 -6.7769823 -6.5470705 -6.4621568 -6.5947423 -6.6815629 -6.50576 -6.2092071 -5.9421687 -5.59202 -5.3335552 -5.226305 -5.2783461][-7.4967351 -7.6709056 -7.9778309 -8.1893711 -8.0194721 -8.0011215 -8.14843 -8.3415937 -8.3355579 -8.0182438 -7.5878172 -7.0712686 -6.7838464 -6.5807014 -6.5007863][-7.8057065 -7.62535 -7.9331141 -8.1554394 -7.9959607 -7.9802666 -8.0025339 -8.2545872 -8.6206141 -8.5847473 -8.2489319 -7.6999989 -7.4062762 -7.1295609 -6.8489885][-8.4910126 -7.4360442 -7.5801272 -7.5174441 -7.0110927 -6.5800486 -6.2147169 -6.3436131 -7.2878146 -8.0355091 -8.3082027 -8.0872688 -7.9535336 -7.723506 -7.2540984][-8.9588242 -7.480114 -7.5459504 -7.0475039 -5.7064438 -4.4156561 -3.4611044 -3.525136 -5.0733204 -6.8729706 -8.0713263 -8.49111 -8.6352329 -8.4538717 -7.7359428][-7.8870077 -6.6665926 -6.8311687 -6.0058222 -3.7641554 -1.1741492 0.65017366 0.66602206 -1.4404862 -4.0926595 -6.4083562 -7.7603455 -8.310236 -8.2479534 -7.4069715][-5.5235214 -5.103014 -5.1537647 -4.081811 -1.1339202 2.5869617 5.210433 5.4808016 3.0696607 -0.32730627 -3.7953687 -6.2668982 -7.3272309 -7.3885622 -6.532023][-4.8589239 -4.0153403 -3.9370937 -2.7979517 0.50915074 4.8565907 7.9793015 8.6588192 6.2848387 2.5507274 -1.5243373 -4.8311644 -6.3698807 -6.413362 -5.7069478][-5.1702747 -4.1304512 -4.0400052 -2.9920621 0.39229703 4.8835444 8.1807432 9.1672392 7.1038876 3.562561 -0.29399002 -3.6574006 -5.32797 -5.3391418 -4.890419][-6.7434845 -5.3849158 -5.0281978 -4.0277796 -0.88859117 3.2011971 6.1564322 7.0975428 5.4074664 2.6346521 -0.20391297 -2.9519691 -4.2218337 -4.1803465 -4.28262][-8.9517355 -7.3095675 -6.6664815 -5.6723695 -3.038065 -0.012629509 2.0213327 2.6665545 1.5112183 -0.10391164 -1.2880633 -2.882154 -3.7057338 -3.7212932 -4.3840723][-10.582418 -8.7780037 -7.9246883 -6.9910016 -5.1027946 -3.3109455 -2.3598044 -2.2137203 -2.9351931 -3.2237659 -2.6929131 -3.0769756 -3.3697743 -3.3991346 -4.6921415][-11.211898 -9.3590431 -8.5008736 -7.6954417 -6.4091082 -5.5328641 -5.3798757 -5.5888605 -6.02862 -5.6386318 -4.1967211 -3.9689162 -3.8908727 -3.9992981 -5.6646261][-11.03086 -9.4131956 -8.7823944 -8.2840748 -7.4706192 -7.064096 -7.1634617 -7.3290467 -7.5426126 -7.0512605 -5.7525544 -5.5801258 -5.4470024 -5.6100826 -7.2783637][-9.6345577 -8.3067112 -8.0128927 -7.8407569 -7.4313369 -7.3103757 -7.5580111 -7.7259512 -7.8507762 -7.4838591 -6.608026 -6.6102047 -6.5543838 -6.8592429 -8.25355]]...]
INFO - root - 2017-12-15 08:56:30.369099: step 45210, loss = 0.24, batch loss = 0.21 (35.4 examples/sec; 0.226 sec/batch; 18h:00m:38s remains)
INFO - root - 2017-12-15 08:56:32.625747: step 45220, loss = 0.20, batch loss = 0.16 (35.6 examples/sec; 0.225 sec/batch; 17h:57m:15s remains)
INFO - root - 2017-12-15 08:56:34.883497: step 45230, loss = 0.36, batch loss = 0.33 (36.5 examples/sec; 0.219 sec/batch; 17h:30m:17s remains)
INFO - root - 2017-12-15 08:56:37.126527: step 45240, loss = 0.31, batch loss = 0.28 (34.8 examples/sec; 0.230 sec/batch; 18h:21m:48s remains)
INFO - root - 2017-12-15 08:56:39.387462: step 45250, loss = 0.17, batch loss = 0.14 (35.2 examples/sec; 0.227 sec/batch; 18h:06m:40s remains)
INFO - root - 2017-12-15 08:56:41.640646: step 45260, loss = 0.24, batch loss = 0.21 (36.1 examples/sec; 0.221 sec/batch; 17h:39m:27s remains)
INFO - root - 2017-12-15 08:56:43.943832: step 45270, loss = 0.26, batch loss = 0.23 (34.6 examples/sec; 0.231 sec/batch; 18h:27m:54s remains)
INFO - root - 2017-12-15 08:56:46.199269: step 45280, loss = 0.37, batch loss = 0.33 (35.9 examples/sec; 0.223 sec/batch; 17h:47m:08s remains)
INFO - root - 2017-12-15 08:56:48.459703: step 45290, loss = 0.24, batch loss = 0.21 (35.2 examples/sec; 0.227 sec/batch; 18h:08m:48s remains)
INFO - root - 2017-12-15 08:56:50.695425: step 45300, loss = 0.18, batch loss = 0.15 (35.9 examples/sec; 0.223 sec/batch; 17h:46m:52s remains)
2017-12-15 08:56:51.018019: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7584605 -4.60182 -4.6065097 -4.7825031 -4.5269289 -4.3906994 -5.3459721 -6.7885237 -7.6827555 -8.0930014 -8.0803547 -7.6536388 -7.0820494 -6.2902 -5.5128489][-4.3392339 -4.2457027 -4.2311711 -4.3527937 -4.0335789 -4.1616774 -5.5643234 -7.1521597 -8.26448 -8.8649912 -8.9419794 -8.6973791 -8.2067337 -7.2758017 -6.2763977][-4.7242165 -3.6916161 -3.6815736 -3.6374388 -3.0803671 -3.1814713 -4.5196409 -5.8186703 -7.1437187 -8.1911707 -8.7689972 -9.1035442 -8.7879639 -7.7207832 -6.5368547][-5.1348591 -3.5628526 -3.656538 -3.4418972 -2.5179403 -2.3243322 -3.1775002 -3.9082198 -5.3699284 -6.853117 -7.9710827 -8.8471107 -8.8267193 -7.8522472 -6.6690474][-4.9078674 -2.952744 -3.2752776 -2.8862791 -1.6712971 -1.1811274 -1.3251691 -1.4520264 -2.8468738 -4.6200275 -6.2476788 -7.6085615 -8.0640783 -7.4949875 -6.582613][-4.7483721 -3.0743256 -3.4878569 -2.865941 -1.3584886 -0.25209475 0.62911868 1.0450549 -0.23301506 -2.2827022 -4.4493632 -6.2570591 -7.1073 -6.884201 -6.3018694][-3.4043417 -2.2552555 -2.6750422 -1.985714 -0.31421256 1.6310031 3.5046089 4.2911015 2.9781234 0.40749931 -2.4602962 -4.7377687 -6.0276489 -6.2274647 -6.0930018][-2.209332 -1.0791537 -1.3783454 -0.6584568 0.98719 3.5441196 6.0760317 7.020587 5.5903378 2.6189601 -0.70661509 -3.3418951 -5.0226564 -5.6177206 -5.9542894][-2.0130825 -0.77844882 -0.96977997 -0.53309655 0.57356572 2.8265808 4.8823071 5.3450623 3.8435104 1.2416673 -1.5906346 -3.7578166 -5.1320372 -5.5301456 -5.9074326][-1.8465451 -0.45073175 -0.88936973 -0.98550832 -0.48928428 0.9548192 1.7937281 1.5461731 0.02122283 -1.9950162 -3.978997 -5.3581257 -6.1195135 -5.9954605 -6.1130886][-3.4225125 -2.1023521 -2.8938956 -3.1351938 -2.6077564 -1.6354175 -1.3789678 -1.489387 -2.587873 -3.9127407 -5.1498017 -6.0961456 -6.5563335 -6.2555933 -6.2475691][-5.30647 -4.1264791 -5.0035963 -5.0755348 -4.3754964 -3.7847505 -3.65421 -3.3632808 -4.0604696 -4.8564019 -5.7242441 -6.5565662 -6.9514656 -6.6102047 -6.4115047][-6.4105644 -5.4277821 -6.2462063 -6.0678167 -5.2730532 -4.9795728 -4.767127 -4.2043824 -4.7617178 -5.4527054 -6.2254391 -7.051753 -7.4068661 -6.979928 -6.4519119][-7.8579683 -7.0551481 -7.6319828 -7.1729078 -6.3721991 -6.258935 -5.8795547 -5.1314278 -5.5403271 -6.1922584 -6.783679 -7.4001055 -7.5859489 -7.0564423 -6.2684317][-8.2957335 -7.485158 -7.7889729 -7.3784781 -6.9240732 -7.13912 -6.9260759 -6.4741364 -6.8480272 -7.3302736 -7.5760965 -7.7676916 -7.5511694 -6.8598604 -5.9287376]]...]
INFO - root - 2017-12-15 08:56:53.277391: step 45310, loss = 0.27, batch loss = 0.24 (35.4 examples/sec; 0.226 sec/batch; 18h:02m:55s remains)
INFO - root - 2017-12-15 08:56:55.551139: step 45320, loss = 0.28, batch loss = 0.25 (35.9 examples/sec; 0.223 sec/batch; 17h:47m:54s remains)
INFO - root - 2017-12-15 08:56:57.794124: step 45330, loss = 0.19, batch loss = 0.16 (35.6 examples/sec; 0.225 sec/batch; 17h:56m:48s remains)
INFO - root - 2017-12-15 08:57:00.047884: step 45340, loss = 0.27, batch loss = 0.24 (34.5 examples/sec; 0.232 sec/batch; 18h:29m:25s remains)
INFO - root - 2017-12-15 08:57:02.314236: step 45350, loss = 0.24, batch loss = 0.21 (36.8 examples/sec; 0.217 sec/batch; 17h:19m:15s remains)
INFO - root - 2017-12-15 08:57:04.592187: step 45360, loss = 0.21, batch loss = 0.17 (34.7 examples/sec; 0.230 sec/batch; 18h:23m:01s remains)
INFO - root - 2017-12-15 08:57:06.907199: step 45370, loss = 0.39, batch loss = 0.35 (34.5 examples/sec; 0.232 sec/batch; 18h:30m:28s remains)
INFO - root - 2017-12-15 08:57:09.224276: step 45380, loss = 0.52, batch loss = 0.49 (35.4 examples/sec; 0.226 sec/batch; 18h:02m:43s remains)
INFO - root - 2017-12-15 08:57:11.500088: step 45390, loss = 0.23, batch loss = 0.20 (33.6 examples/sec; 0.238 sec/batch; 19h:00m:49s remains)
INFO - root - 2017-12-15 08:57:13.748946: step 45400, loss = 0.24, batch loss = 0.21 (35.8 examples/sec; 0.223 sec/batch; 17h:48m:14s remains)
2017-12-15 08:57:14.048621: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8714471 -3.6167104 -3.632473 -3.9691563 -4.3648005 -4.4204369 -4.7686911 -5.5933647 -5.9122386 -4.9723797 -3.6459475 -3.0898194 -2.8230696 -2.2127235 -1.7204759][-4.3720951 -4.9154229 -4.9374409 -5.0376587 -5.2727327 -5.3369627 -5.7045193 -6.503912 -6.780549 -5.9283504 -4.7385812 -4.2996716 -4.1674585 -3.6557961 -3.1016407][-6.1764145 -6.0693741 -5.6380186 -5.2079635 -4.9808874 -4.8225913 -5.1860924 -6.0149474 -6.3350916 -5.6961818 -4.8067322 -4.4497471 -4.3240118 -3.9373662 -3.4503479][-7.0162287 -6.5649185 -5.7950974 -4.8932724 -4.12011 -3.7764344 -4.1921005 -5.14678 -5.5091438 -5.0108261 -4.3412762 -4.096036 -3.9560118 -3.633615 -3.1948144][-7.3530626 -6.3673592 -5.4547844 -4.1579838 -2.8269343 -2.1845982 -2.5181196 -3.5420814 -4.0663815 -3.7551506 -3.2871709 -3.0602794 -3.0521653 -2.8153827 -2.5188296][-6.2634449 -4.9535441 -3.9407907 -2.3100727 -0.48923922 0.50013852 0.33521128 -0.72619915 -1.5915432 -1.6198533 -1.3278674 -1.202647 -1.1423898 -0.93171728 -0.92847657][-3.7148051 -2.5608156 -1.6892231 -0.020101786 1.9991686 3.00618 2.8487518 1.6386323 0.54500842 0.32005739 0.38810039 0.20832133 0.19360876 0.33402848 0.21599817][-1.952112 -0.88471365 -0.34766531 1.1980929 3.1308653 3.9383309 3.6570299 2.4314802 1.4114232 1.1665637 0.91766691 0.4666431 0.21838307 0.20205688 0.027476311][-1.3385425 -0.37096119 0.0019812584 1.4038997 3.0412047 3.6059177 3.291158 2.3164294 1.4560928 1.0554466 0.501868 -0.25210667 -0.56520331 -0.68380117 -0.93821645][-1.3117379 -0.55507314 -0.39947331 0.66028166 1.8472126 2.048702 1.6175323 0.83817029 0.10180521 -0.43920231 -1.11361 -1.8378716 -2.0663257 -2.2550325 -2.6200056][-2.1307693 -1.43695 -1.5191487 -0.98746729 -0.34901512 -0.52456546 -1.1129925 -1.6764371 -2.1198204 -2.5483475 -3.1435266 -3.6393781 -3.8591719 -4.1971526 -4.654614][-3.1424718 -2.4014955 -2.6068358 -2.44502 -2.2058284 -2.5418191 -3.0523775 -3.3306704 -3.5373621 -3.8258934 -4.2502375 -4.6306553 -4.8546391 -5.28479 -5.7326741][-3.6306217 -2.855 -3.1820173 -3.3126726 -3.2608819 -3.5723119 -3.9315562 -4.0015497 -4.0687728 -4.2388754 -4.4789181 -4.6931305 -4.8582869 -5.2522907 -5.7226715][-3.9870505 -3.1660397 -3.4613791 -3.75313 -3.8335724 -4.1401014 -4.4255447 -4.4142675 -4.322134 -4.3504667 -4.5133476 -4.7310867 -5.0038328 -5.4424295 -5.8671465][-4.05439 -3.1589241 -3.4002028 -3.799578 -4.0405111 -4.354157 -4.5268383 -4.4248362 -4.2724695 -4.2816772 -4.5394831 -4.9101954 -5.2953939 -5.6831722 -5.9091663]]...]
INFO - root - 2017-12-15 08:57:16.321055: step 45410, loss = 0.22, batch loss = 0.19 (34.8 examples/sec; 0.230 sec/batch; 18h:20m:48s remains)
INFO - root - 2017-12-15 08:57:18.602879: step 45420, loss = 0.29, batch loss = 0.25 (35.1 examples/sec; 0.228 sec/batch; 18h:09m:39s remains)
INFO - root - 2017-12-15 08:57:20.891660: step 45430, loss = 0.20, batch loss = 0.17 (35.6 examples/sec; 0.225 sec/batch; 17h:55m:08s remains)
INFO - root - 2017-12-15 08:57:23.179647: step 45440, loss = 0.25, batch loss = 0.22 (34.4 examples/sec; 0.233 sec/batch; 18h:33m:42s remains)
INFO - root - 2017-12-15 08:57:25.459645: step 45450, loss = 0.33, batch loss = 0.29 (36.0 examples/sec; 0.222 sec/batch; 17h:44m:21s remains)
INFO - root - 2017-12-15 08:57:27.721811: step 45460, loss = 0.34, batch loss = 0.31 (36.4 examples/sec; 0.220 sec/batch; 17h:31m:32s remains)
INFO - root - 2017-12-15 08:57:30.010901: step 45470, loss = 0.27, batch loss = 0.24 (31.9 examples/sec; 0.250 sec/batch; 19h:57m:51s remains)
INFO - root - 2017-12-15 08:57:32.298731: step 45480, loss = 0.19, batch loss = 0.16 (35.7 examples/sec; 0.224 sec/batch; 17h:52m:42s remains)
INFO - root - 2017-12-15 08:57:34.562158: step 45490, loss = 0.17, batch loss = 0.14 (35.8 examples/sec; 0.223 sec/batch; 17h:48m:29s remains)
INFO - root - 2017-12-15 08:57:36.803209: step 45500, loss = 0.22, batch loss = 0.19 (34.8 examples/sec; 0.230 sec/batch; 18h:18m:02s remains)
2017-12-15 08:57:37.111447: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6357912 -3.4662552 -3.2647953 -3.4030218 -3.7179251 -3.3856306 -3.288785 -2.8421254 -2.4588771 -2.6258638 -2.9693708 -2.7768116 -1.6609907 -0.69518995 -0.54103196][-3.7211535 -4.5498123 -4.4156179 -4.4250708 -4.4306879 -3.9423194 -3.7984624 -3.4876959 -3.2701643 -3.5682805 -3.9083347 -3.7210631 -2.7389352 -1.9253223 -1.6559145][-4.7062397 -4.5789366 -4.5655556 -4.3837595 -4.0486875 -3.3877921 -3.1346307 -3.0316205 -3.27274 -3.9967608 -4.5927258 -4.5919991 -3.862143 -3.210784 -2.871212][-5.689868 -4.7612705 -4.8097734 -4.3089018 -3.4687388 -2.3867402 -1.8571588 -1.9288468 -2.6125517 -3.8494425 -4.8064194 -5.2719421 -4.9821415 -4.5037003 -4.0744209][-6.5892296 -4.5627136 -4.4159145 -3.454977 -1.9348172 -0.26627684 0.69713879 0.50545406 -0.68462718 -2.4804285 -3.862299 -4.9816008 -5.3478885 -4.9921694 -4.4191542][-6.7922931 -3.8944106 -3.5783751 -2.3228707 -0.44248164 1.574671 3.1330886 3.0019207 1.3941858 -0.91158724 -2.5285006 -4.0441408 -4.8128052 -4.4723 -3.6617718][-5.871305 -3.3092661 -2.8862934 -1.4756658 0.5198102 2.7190332 4.6837564 4.6338983 2.7531953 -0.018367052 -1.8340925 -3.4788585 -4.4754791 -4.2261643 -3.3227854][-5.25884 -2.8447249 -2.3424394 -0.90181768 1.0886321 3.525 5.7637744 5.8006158 3.7115302 0.46963811 -1.6440012 -3.4592357 -4.5400248 -4.3864436 -3.5562921][-5.7270389 -3.3725748 -2.704313 -1.2102695 0.75039792 3.2674417 5.4714737 5.5640497 3.4981389 0.23281884 -1.9515024 -3.82507 -4.9312873 -4.8983583 -4.3225875][-6.4688978 -4.2432833 -3.5140762 -2.2231455 -0.64424586 1.5853682 3.4930344 3.7850666 2.2489347 -0.57230389 -2.6227872 -4.5647964 -5.6298437 -5.7586288 -5.4443831][-6.8363762 -4.7500091 -4.0844631 -3.2881942 -2.380389 -0.77218688 0.685225 1.2454338 0.42081547 -1.6844068 -3.5904696 -5.3840561 -6.1189909 -6.1699286 -6.0972462][-7.2633 -5.1703157 -4.5512996 -4.104557 -3.761091 -2.8182607 -1.8840066 -1.1862816 -1.3047864 -2.4415302 -3.8608441 -5.3498726 -5.69646 -5.5881939 -5.6812263][-7.1342649 -4.88694 -4.352303 -4.1549234 -4.2651186 -3.9884176 -3.5391772 -2.7824817 -2.4542975 -2.8707166 -3.8986073 -5.0339642 -4.9938784 -4.75408 -4.9262967][-6.8821673 -4.3925538 -3.9169106 -3.7603517 -3.9890461 -4.0152206 -3.9528823 -3.2691612 -2.6896439 -2.7141051 -3.4466908 -4.1519833 -3.9115686 -3.8348556 -4.1796803][-6.2143011 -3.4376001 -3.0558167 -2.9085979 -3.2177024 -3.4555562 -3.6379008 -3.1211779 -2.5000081 -2.3612287 -2.839251 -3.2366557 -3.1088967 -3.38276 -3.8955932]]...]
INFO - root - 2017-12-15 08:57:39.390364: step 45510, loss = 0.37, batch loss = 0.34 (34.3 examples/sec; 0.233 sec/batch; 18h:35m:51s remains)
INFO - root - 2017-12-15 08:57:41.680275: step 45520, loss = 0.15, batch loss = 0.12 (35.6 examples/sec; 0.224 sec/batch; 17h:53m:21s remains)
INFO - root - 2017-12-15 08:57:43.928296: step 45530, loss = 0.22, batch loss = 0.19 (36.0 examples/sec; 0.222 sec/batch; 17h:41m:43s remains)
INFO - root - 2017-12-15 08:57:46.191790: step 45540, loss = 0.17, batch loss = 0.14 (33.2 examples/sec; 0.241 sec/batch; 19h:13m:36s remains)
INFO - root - 2017-12-15 08:57:48.464038: step 45550, loss = 0.20, batch loss = 0.17 (35.0 examples/sec; 0.228 sec/batch; 18h:11m:45s remains)
INFO - root - 2017-12-15 08:57:50.788960: step 45560, loss = 0.27, batch loss = 0.23 (34.4 examples/sec; 0.233 sec/batch; 18h:33m:38s remains)
INFO - root - 2017-12-15 08:57:53.025427: step 45570, loss = 0.28, batch loss = 0.25 (36.3 examples/sec; 0.220 sec/batch; 17h:32m:52s remains)
INFO - root - 2017-12-15 08:57:55.334890: step 45580, loss = 0.20, batch loss = 0.17 (34.7 examples/sec; 0.231 sec/batch; 18h:22m:25s remains)
INFO - root - 2017-12-15 08:57:57.659611: step 45590, loss = 0.23, batch loss = 0.20 (34.4 examples/sec; 0.232 sec/batch; 18h:30m:58s remains)
INFO - root - 2017-12-15 08:57:59.900830: step 45600, loss = 0.21, batch loss = 0.17 (35.9 examples/sec; 0.223 sec/batch; 17h:46m:46s remains)
2017-12-15 08:58:00.207796: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7983408 -5.401526 -4.9152584 -4.686316 -4.7171955 -5.1193209 -5.6198716 -5.6078939 -5.4537578 -5.2821655 -5.1492386 -5.0707932 -5.0050383 -4.6624613 -4.0078187][-5.2953053 -4.9268203 -4.264596 -4.1475172 -4.3029532 -4.8973913 -5.6710634 -5.7387052 -5.6551476 -5.4902868 -5.309351 -5.1801786 -5.0051603 -4.4166565 -3.4080167][-4.9539819 -3.6775217 -2.8206456 -2.9379432 -3.323926 -4.0444679 -4.9965229 -5.172967 -5.3189917 -5.3885193 -5.3224068 -5.24526 -5.0010571 -4.2592888 -3.003222][-5.2277994 -2.8914876 -1.969501 -2.2202454 -2.753551 -3.5486927 -4.4243655 -4.5681229 -4.8995104 -5.3456535 -5.6863461 -5.8093557 -5.5258741 -4.7770071 -3.495563][-5.8446269 -2.5133405 -1.7714944 -2.1451356 -2.5784345 -2.931087 -3.274344 -3.1589949 -3.6258011 -4.5703497 -5.4976311 -6.03808 -5.8162661 -5.2116032 -4.1684227][-5.8146353 -2.8019822 -2.1209748 -2.2049165 -2.0962124 -1.674934 -1.1435813 -0.55795455 -1.1970627 -2.7667563 -4.4462137 -5.5687504 -5.5790348 -5.2559624 -4.6811371][-6.2025023 -3.838068 -3.2094355 -2.6231856 -1.4950697 -0.0016527176 1.7031341 2.8433588 2.0162356 -0.18063688 -2.5436733 -4.1791639 -4.4370556 -4.4900393 -4.5429239][-5.7319455 -4.2811232 -4.0232472 -3.2610703 -1.5428777 1.2201838 4.3229761 6.0954838 5.1554184 2.5085933 -0.26466203 -2.1467564 -2.4324648 -2.6648648 -3.3252916][-4.6295209 -3.760478 -4.075871 -3.7625556 -2.2447989 0.83242512 4.6297531 6.9158144 6.3640919 3.8866069 1.202894 -0.52295971 -0.57724905 -0.56403196 -1.3810105][-3.3335614 -2.9206202 -3.8153167 -4.1638937 -3.3529482 -0.91097844 2.3572342 4.6195965 4.730895 3.1269987 1.2981098 0.21117663 0.57718825 0.89900923 0.16441822][-2.4621325 -2.0557132 -3.2379146 -4.256227 -4.2892776 -2.8192565 -0.43913352 1.3973737 1.8713539 1.171715 0.33778405 0.14753222 0.980268 1.6777914 1.3077283][-1.5694234 -0.90857613 -2.2367704 -3.8010898 -4.4488649 -3.9145606 -2.3979182 -0.91408122 -0.022868395 0.01915884 -0.20724392 0.039332628 0.92068648 1.8430808 1.8321326][-0.82364511 0.4239502 -0.89685774 -2.9119558 -3.9706335 -4.2651148 -3.5303984 -2.2306395 -0.941834 -0.32174742 -0.099491358 0.26051211 0.79804683 1.4863935 1.5850399][-1.2802548 0.56043339 -0.47439456 -2.5172269 -3.725421 -4.5458369 -4.4466057 -3.4256968 -2.1148827 -1.3232391 -0.89633667 -0.59340143 -0.44839132 -0.06908226 0.097012281][-2.8363802 -0.66754353 -1.4109354 -3.0877836 -4.1269422 -4.9982214 -5.1880751 -4.3634968 -3.2302282 -2.5070662 -2.1666608 -2.1481206 -2.4215524 -2.2638738 -1.8231773]]...]
INFO - root - 2017-12-15 08:58:02.434664: step 45610, loss = 0.30, batch loss = 0.26 (35.6 examples/sec; 0.225 sec/batch; 17h:54m:09s remains)
INFO - root - 2017-12-15 08:58:04.713083: step 45620, loss = 0.22, batch loss = 0.19 (34.8 examples/sec; 0.230 sec/batch; 18h:18m:55s remains)
INFO - root - 2017-12-15 08:58:06.993230: step 45630, loss = 0.24, batch loss = 0.21 (35.8 examples/sec; 0.224 sec/batch; 17h:48m:42s remains)
INFO - root - 2017-12-15 08:58:09.305795: step 45640, loss = 0.19, batch loss = 0.16 (34.6 examples/sec; 0.231 sec/batch; 18h:25m:49s remains)
INFO - root - 2017-12-15 08:58:11.590018: step 45650, loss = 0.27, batch loss = 0.24 (35.7 examples/sec; 0.224 sec/batch; 17h:50m:47s remains)
INFO - root - 2017-12-15 08:58:13.842132: step 45660, loss = 0.21, batch loss = 0.18 (35.6 examples/sec; 0.225 sec/batch; 17h:55m:05s remains)
INFO - root - 2017-12-15 08:58:16.130521: step 45670, loss = 0.21, batch loss = 0.18 (33.1 examples/sec; 0.242 sec/batch; 19h:16m:48s remains)
INFO - root - 2017-12-15 08:58:18.413131: step 45680, loss = 0.35, batch loss = 0.31 (35.9 examples/sec; 0.223 sec/batch; 17h:45m:34s remains)
INFO - root - 2017-12-15 08:58:20.698328: step 45690, loss = 0.23, batch loss = 0.20 (35.3 examples/sec; 0.227 sec/batch; 18h:02m:53s remains)
INFO - root - 2017-12-15 08:58:22.978951: step 45700, loss = 0.23, batch loss = 0.20 (34.9 examples/sec; 0.229 sec/batch; 18h:16m:35s remains)
2017-12-15 08:58:23.336221: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7634192 -7.1685648 -6.9386721 -6.5380192 -6.0329924 -5.4910636 -5.058218 -4.75278 -4.5418887 -4.4902644 -4.386332 -4.3468771 -4.3994827 -4.44141 -4.4580345][-6.1971245 -7.8826237 -7.571311 -7.1234236 -6.5631876 -6.0065355 -5.4764934 -5.0638618 -4.7898135 -4.8322535 -4.89399 -4.990725 -5.2068939 -5.3367805 -5.4390068][-7.0777035 -7.9656134 -7.5529909 -7.08626 -6.52917 -6.059103 -5.5621843 -5.1369066 -4.8681211 -5.131218 -5.4909182 -5.8146157 -6.3020039 -6.52796 -6.7420654][-7.6420755 -7.7222128 -7.2032423 -6.7755175 -6.2961063 -6.0128465 -5.5911751 -5.2134018 -4.9892564 -5.4823084 -6.1040754 -6.547389 -7.2025118 -7.3755226 -7.5992422][-7.97797 -7.1696939 -6.3788605 -5.8548555 -5.4795866 -5.3608131 -4.9477215 -4.6735973 -4.535974 -5.1615677 -5.9516244 -6.4666958 -7.2165861 -7.26999 -7.4182186][-7.6296539 -6.3919878 -5.1902833 -4.4596624 -4.0647955 -3.847091 -3.2518837 -3.0194542 -3.0485148 -3.8987527 -4.8474407 -5.4278679 -6.1470108 -5.9770193 -5.929987][-6.7265377 -5.636487 -3.942096 -2.8688798 -2.2016356 -1.6622469 -0.82321024 -0.45047486 -0.46143293 -1.4587593 -2.6337991 -3.3379102 -4.0575356 -3.7196217 -3.4483123][-6.4074764 -5.07638 -2.9459758 -1.468502 -0.39025414 0.57083225 1.5653048 2.0085151 2.0617979 1.0408731 -0.38284862 -1.2215836 -1.8577853 -1.305705 -0.77432656][-6.3796549 -5.1604128 -2.9249701 -1.2495756 0.14207768 1.5515954 2.7047837 3.26989 3.5199673 2.4842079 0.87077212 -0.014328241 -0.38906765 0.41736221 1.1567848][-6.796648 -5.9882736 -4.1359873 -2.6385145 -1.2218158 0.36645269 1.688916 2.5254114 3.0743568 1.984133 0.26451874 -0.47054219 -0.49186909 0.4809041 1.3975511][-7.3767586 -6.9932394 -5.6602397 -4.3715305 -3.0337272 -1.4740727 -0.051637888 0.87824988 1.5041428 0.40577507 -1.1887249 -1.5608306 -1.2760755 -0.25596714 0.75683594][-7.9516459 -7.8956261 -7.0487604 -6.048327 -4.9241538 -3.4958787 -1.9612329 -1.0525569 -0.46379662 -1.6120822 -2.978642 -3.1236744 -2.7906325 -1.9411385 -1.0597183][-8.2354908 -8.341857 -7.94009 -7.3891296 -6.6786118 -5.5361719 -4.1343784 -3.4271126 -3.0283902 -4.1222196 -5.1818814 -5.1868496 -4.8824911 -4.3562856 -3.7879953][-8.1669655 -8.2474165 -8.1533241 -7.9259958 -7.4930696 -6.5792303 -5.4006481 -4.9408145 -4.815403 -5.7859116 -6.6781397 -6.8126774 -6.7856979 -6.6439486 -6.3993607][-8.0088234 -8.0013456 -8.1148233 -8.0829449 -7.7620344 -7.0171833 -6.154119 -5.9331894 -6.002696 -6.7947807 -7.5517454 -7.8538074 -8.0339727 -8.1333828 -8.1325493]]...]
INFO - root - 2017-12-15 08:58:25.584994: step 45710, loss = 0.23, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 17h:42m:57s remains)
INFO - root - 2017-12-15 08:58:27.838662: step 45720, loss = 0.24, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 17h:50m:36s remains)
INFO - root - 2017-12-15 08:58:30.117811: step 45730, loss = 0.23, batch loss = 0.20 (36.2 examples/sec; 0.221 sec/batch; 17h:35m:46s remains)
INFO - root - 2017-12-15 08:58:32.371149: step 45740, loss = 0.23, batch loss = 0.20 (35.7 examples/sec; 0.224 sec/batch; 17h:50m:20s remains)
INFO - root - 2017-12-15 08:58:34.607086: step 45750, loss = 0.22, batch loss = 0.19 (35.7 examples/sec; 0.224 sec/batch; 17h:50m:07s remains)
INFO - root - 2017-12-15 08:58:36.890599: step 45760, loss = 0.21, batch loss = 0.18 (34.4 examples/sec; 0.233 sec/batch; 18h:32m:33s remains)
INFO - root - 2017-12-15 08:58:39.199545: step 45770, loss = 0.23, batch loss = 0.19 (34.5 examples/sec; 0.232 sec/batch; 18h:27m:42s remains)
INFO - root - 2017-12-15 08:58:41.505064: step 45780, loss = 0.17, batch loss = 0.14 (35.5 examples/sec; 0.225 sec/batch; 17h:57m:06s remains)
INFO - root - 2017-12-15 08:58:43.778426: step 45790, loss = 0.21, batch loss = 0.18 (35.6 examples/sec; 0.225 sec/batch; 17h:54m:06s remains)
INFO - root - 2017-12-15 08:58:46.023479: step 45800, loss = 0.20, batch loss = 0.17 (34.7 examples/sec; 0.231 sec/batch; 18h:21m:44s remains)
2017-12-15 08:58:46.334218: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6312313 -5.6075368 -5.0282068 -4.2245588 -2.7999258 -2.0351298 -2.2449737 -2.8229232 -4.4812584 -5.3235908 -5.3901539 -5.2103052 -4.9113078 -4.1219749 -3.5549471][-4.9018955 -5.4489174 -4.8116817 -3.958292 -2.5983665 -1.9150584 -2.3878639 -3.0763292 -4.5730157 -5.40368 -5.4748707 -5.2762647 -5.138463 -4.56923 -4.1762638][-5.341939 -5.66354 -4.9374857 -3.9496779 -2.6802316 -2.0779235 -2.5471683 -3.1109631 -4.196866 -4.9524021 -5.2158737 -5.284122 -5.482173 -5.3549194 -5.2439671][-6.2078972 -6.2776814 -5.5748129 -4.4874954 -3.1367252 -2.3278105 -2.5206542 -2.8521998 -3.4149981 -3.8596382 -4.1815925 -4.5123963 -4.9905224 -5.2696009 -5.5783234][-6.9141521 -6.2277184 -5.381918 -4.0036373 -2.1854262 -0.75524843 -0.43518794 -0.58641231 -1.0872438 -1.5850801 -2.2755442 -3.0129285 -3.8164291 -4.3255172 -4.862968][-7.39627 -5.8853469 -4.6178179 -2.60808 -0.041748285 2.1102607 3.0298254 2.9839232 1.9537308 0.74568725 -0.75379813 -2.1405361 -3.2845237 -3.8047233 -4.4234571][-7.371665 -5.592185 -3.7064118 -0.9083904 2.2402961 4.6939611 5.7965183 5.6393871 3.72597 1.6381195 -0.52333057 -2.3443069 -3.7380261 -4.2804809 -4.9870958][-6.7960815 -4.646534 -2.3268812 0.81578588 3.9468167 6.0935392 6.9578295 6.5725584 3.9319818 1.3082514 -1.0023478 -2.8059485 -4.2533278 -4.8879023 -5.814105][-5.7751312 -3.5380788 -1.4293811 1.2304416 3.659889 5.0761108 5.6861105 5.4244661 2.7296236 0.29093218 -1.6952791 -3.2358148 -4.6053619 -5.4538727 -6.6421089][-5.2450733 -3.2157714 -1.6828523 0.091428518 1.5627367 2.0358293 2.3399251 2.3505218 0.084886074 -1.7151935 -2.8687208 -3.8246069 -4.9355345 -5.9819765 -7.3398228][-5.9348731 -4.0839663 -2.8536544 -1.6607684 -0.8399905 -1.0552644 -1.041188 -0.7719245 -2.6012266 -3.7531853 -3.9745655 -4.2861366 -5.0428724 -6.109375 -7.4779415][-7.1671982 -5.405467 -4.306674 -3.4015739 -2.9815626 -3.5105615 -3.5885415 -2.8527024 -4.17295 -4.6692777 -4.0607147 -3.9204857 -4.539856 -5.8073335 -7.1351042][-7.9373646 -6.2364774 -5.2868786 -4.5428324 -4.1007037 -4.5567579 -4.6120033 -3.4272871 -4.2968521 -4.4174881 -3.40445 -3.2250466 -4.1404486 -5.7579889 -7.1303244][-8.4755373 -6.7051172 -5.671669 -4.7798004 -3.9681578 -4.1369762 -4.1672955 -2.6695313 -3.4203072 -3.8614783 -3.0592797 -3.1613636 -4.3174877 -6.0129547 -7.2978058][-8.650404 -6.52437 -5.086915 -3.8954487 -2.8494627 -2.9170353 -3.2044711 -1.7890918 -2.673188 -3.5460663 -3.1416037 -3.5256705 -4.7456393 -6.183188 -7.2199135]]...]
INFO - root - 2017-12-15 08:58:48.597947: step 45810, loss = 0.23, batch loss = 0.20 (35.4 examples/sec; 0.226 sec/batch; 17h:59m:43s remains)
INFO - root - 2017-12-15 08:58:50.897580: step 45820, loss = 0.37, batch loss = 0.34 (34.7 examples/sec; 0.230 sec/batch; 18h:20m:12s remains)
INFO - root - 2017-12-15 08:58:53.164170: step 45830, loss = 0.37, batch loss = 0.34 (36.3 examples/sec; 0.220 sec/batch; 17h:33m:00s remains)
INFO - root - 2017-12-15 08:58:55.472387: step 45840, loss = 0.25, batch loss = 0.22 (35.4 examples/sec; 0.226 sec/batch; 17h:58m:31s remains)
INFO - root - 2017-12-15 08:58:57.738260: step 45850, loss = 0.25, batch loss = 0.22 (35.9 examples/sec; 0.223 sec/batch; 17h:44m:14s remains)
INFO - root - 2017-12-15 08:59:00.002176: step 45860, loss = 0.20, batch loss = 0.17 (35.8 examples/sec; 0.223 sec/batch; 17h:46m:41s remains)
INFO - root - 2017-12-15 08:59:02.293419: step 45870, loss = 0.22, batch loss = 0.19 (34.3 examples/sec; 0.233 sec/batch; 18h:34m:05s remains)
INFO - root - 2017-12-15 08:59:04.595216: step 45880, loss = 0.35, batch loss = 0.31 (36.0 examples/sec; 0.222 sec/batch; 17h:40m:47s remains)
INFO - root - 2017-12-15 08:59:06.912608: step 45890, loss = 0.20, batch loss = 0.17 (31.6 examples/sec; 0.253 sec/batch; 20h:07m:50s remains)
INFO - root - 2017-12-15 08:59:09.211045: step 45900, loss = 0.20, batch loss = 0.16 (34.4 examples/sec; 0.233 sec/batch; 18h:32m:11s remains)
2017-12-15 08:59:09.504146: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3152089 -4.6536846 -5.0979147 -5.6499577 -5.8952465 -5.643651 -5.0189009 -4.6838717 -4.8392677 -5.1172924 -5.2960768 -5.5356779 -5.68312 -5.7276049 -5.3967428][-6.0076437 -6.2410126 -6.6207933 -7.3348875 -7.6630754 -7.1611719 -6.0285578 -5.4566736 -5.8016381 -6.3423967 -6.7330589 -7.2128468 -7.3203745 -7.3247643 -6.8910589][-7.6095362 -7.3564806 -7.70045 -8.503705 -8.6922951 -7.8571615 -6.2519631 -5.638998 -6.443224 -7.4753923 -8.3478022 -9.2759285 -9.30564 -9.0340977 -8.2439041][-8.2050037 -7.1785212 -7.3138266 -7.9527235 -7.7191973 -6.4516239 -4.36256 -3.753191 -4.9715252 -6.5652609 -8.1079884 -9.6863766 -9.8670959 -9.6445494 -8.8729382][-9.1147947 -7.1217194 -7.1242571 -7.3996172 -6.5363026 -4.8190641 -2.2863245 -1.3028057 -2.5858724 -4.4725819 -6.5279531 -8.6984415 -9.13442 -9.1068153 -8.6520729][-8.4386845 -5.9102664 -5.5884218 -5.2382 -3.4897294 -1.6123434 1.0804119 2.2801287 0.70139837 -1.5959918 -4.2539291 -7.2342215 -8.3736038 -8.9642181 -8.8564892][-7.3233566 -4.354722 -3.6235282 -2.6675298 -0.21283746 1.9622681 5.0332956 6.4332571 4.5947905 1.6923676 -1.541633 -5.5119324 -7.6710672 -8.7742109 -8.8821249][-6.892067 -3.6207371 -2.5792875 -1.1870059 1.5528865 3.8650143 7.0005226 8.2772884 5.9962044 2.5942738 -0.667449 -4.8974881 -7.4566793 -8.6842794 -8.8626642][-7.0346012 -3.6210403 -2.5605221 -1.1501658 1.6867359 4.0368223 6.8386354 7.6654243 5.1357794 1.5501404 -1.5161679 -5.6711273 -8.3121872 -9.4101286 -9.3916664][-8.342783 -5.051405 -4.0357981 -2.6927836 0.031217813 2.4644806 4.8293982 5.1430149 2.9830816 -0.20708561 -2.7596903 -6.5525084 -8.9975252 -9.786418 -9.3816][-9.5566874 -6.6501493 -5.7243257 -4.4561014 -2.1477611 -0.13741469 1.390203 1.1441839 -0.66022682 -3.4728107 -5.466342 -8.4250364 -10.246347 -10.448805 -9.4744291][-9.9003305 -7.4793444 -6.8388281 -5.9151077 -4.140255 -2.6021607 -1.8239737 -2.4727538 -4.0414076 -6.46729 -8.0552492 -9.9881649 -10.991172 -10.711007 -9.4196644][-9.823514 -7.9101791 -7.4885025 -7.0043421 -5.9859724 -4.984726 -4.7218924 -5.4540386 -6.5328531 -8.1267815 -9.029542 -10.050526 -10.49048 -9.9688673 -8.4740191][-8.6792269 -7.3083782 -7.057086 -6.85604 -6.4393568 -5.9814048 -6.0792727 -6.78893 -7.54519 -8.42495 -8.7022514 -8.9775286 -8.9578333 -8.2942638 -6.8493538][-6.8923836 -5.8293343 -5.60554 -5.4330211 -5.2648778 -5.1267428 -5.2877235 -5.6937609 -6.1265225 -6.5591068 -6.6459723 -6.7169361 -6.7051573 -6.2963896 -5.3193521]]...]
INFO - root - 2017-12-15 08:59:11.758062: step 45910, loss = 0.20, batch loss = 0.17 (35.8 examples/sec; 0.224 sec/batch; 17h:47m:43s remains)
INFO - root - 2017-12-15 08:59:14.015918: step 45920, loss = 0.23, batch loss = 0.20 (33.5 examples/sec; 0.239 sec/batch; 18h:59m:25s remains)
INFO - root - 2017-12-15 08:59:16.301371: step 45930, loss = 0.22, batch loss = 0.18 (35.7 examples/sec; 0.224 sec/batch; 17h:51m:03s remains)
INFO - root - 2017-12-15 08:59:18.554657: step 45940, loss = 0.21, batch loss = 0.17 (35.9 examples/sec; 0.223 sec/batch; 17h:42m:51s remains)
INFO - root - 2017-12-15 08:59:20.841807: step 45950, loss = 0.19, batch loss = 0.16 (35.0 examples/sec; 0.229 sec/batch; 18h:11m:49s remains)
INFO - root - 2017-12-15 08:59:23.069514: step 45960, loss = 0.31, batch loss = 0.28 (36.8 examples/sec; 0.217 sec/batch; 17h:18m:12s remains)
INFO - root - 2017-12-15 08:59:25.312509: step 45970, loss = 0.26, batch loss = 0.22 (35.4 examples/sec; 0.226 sec/batch; 18h:00m:11s remains)
INFO - root - 2017-12-15 08:59:27.639149: step 45980, loss = 0.25, batch loss = 0.22 (35.5 examples/sec; 0.225 sec/batch; 17h:55m:14s remains)
INFO - root - 2017-12-15 08:59:29.934118: step 45990, loss = 0.18, batch loss = 0.15 (34.7 examples/sec; 0.231 sec/batch; 18h:22m:00s remains)
INFO - root - 2017-12-15 08:59:32.220442: step 46000, loss = 0.16, batch loss = 0.13 (34.3 examples/sec; 0.233 sec/batch; 18h:32m:54s remains)
2017-12-15 08:59:32.529102: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.9486649 -1.9984509 -1.3409238 -1.2393748 -1.8532169 -2.5442591 -2.6833568 -1.9718535 -1.7126456 -1.536271 -1.8257997 -2.1954257 -1.783815 -0.88711762 -0.19689345][-1.6852078 -2.2054918 -1.9524542 -1.7953053 -1.9965633 -2.4870157 -2.398849 -1.8815651 -1.8495284 -1.9940699 -2.5414476 -3.073869 -2.850261 -2.2684474 -1.7223819][-2.6277723 -2.6217225 -2.6304526 -2.3132796 -2.1070406 -2.3194623 -1.9587235 -1.5356483 -1.5903884 -2.0431843 -2.8921661 -3.7178962 -3.8167295 -3.7769704 -3.5900645][-3.2775648 -3.2821231 -3.5654869 -2.9893212 -2.3253217 -1.986022 -1.2364912 -0.6521709 -0.80198145 -1.5830066 -2.8013077 -4.0054169 -4.6095953 -5.0677509 -5.4027481][-4.2444067 -4.1150942 -4.5386524 -3.5546756 -2.2913136 -1.289674 -0.24806356 0.42138243 0.04613924 -1.162866 -2.7894804 -4.3065925 -5.3004274 -6.0249043 -6.6950593][-5.0714722 -4.8993444 -5.19096 -3.8151996 -1.9641614 -0.26447439 1.2147951 2.0245013 1.4770911 -0.15438175 -2.2420771 -4.0327597 -5.3917627 -6.2400069 -6.980299][-5.8512912 -5.996489 -5.9502726 -4.1279869 -1.6228944 0.83122706 2.651958 3.4950027 2.7888875 0.82360792 -1.5709069 -3.6527476 -5.5272236 -6.5411181 -7.1626377][-7.4013262 -7.2230544 -6.6474414 -4.4233437 -1.3108051 1.6293974 3.4881845 4.0707545 3.0201688 0.63382626 -2.0412216 -4.3328204 -6.3686972 -7.2399859 -7.3860908][-8.4109306 -7.9942055 -7.13741 -4.7736444 -1.4319921 1.4170797 3.05223 3.4542389 2.2875543 -0.29090798 -2.9393239 -5.1451015 -6.9572334 -7.585568 -7.3064427][-8.8066778 -8.2161446 -7.3273406 -5.1258616 -2.0688565 0.17890906 1.2400794 1.5535309 0.55788517 -1.7280076 -3.8799114 -5.7579017 -7.193665 -7.5462937 -7.0294414][-8.7852573 -7.9571781 -7.2013421 -5.5135322 -3.0171566 -1.5088053 -1.0271425 -0.90378129 -1.6430093 -3.4799414 -5.1312037 -6.5915742 -7.6113105 -7.583858 -6.6559415][-8.3996134 -7.5059352 -7.0249429 -5.8597441 -3.9465034 -2.9733953 -2.9088259 -3.00819 -3.5197945 -4.9172049 -6.0576134 -6.9515047 -7.5095997 -7.1610126 -5.9938111][-7.8336148 -7.1814451 -7.1142335 -6.4664025 -5.0904827 -4.2796803 -4.2847595 -4.4195871 -4.7251935 -5.5813308 -6.1349087 -6.577106 -6.785625 -6.2318449 -5.0846004][-7.412931 -6.9909463 -7.2871222 -7.079658 -6.1470637 -5.286212 -5.0206881 -4.9443297 -4.9550748 -5.3801279 -5.6416965 -5.8225174 -5.7036858 -5.0181432 -4.0494127][-7.2018747 -6.7664609 -7.1579971 -7.1798563 -6.6549282 -5.8893337 -5.4230566 -5.1349835 -4.94024 -4.9909792 -4.9766507 -4.8686647 -4.4209995 -3.6708796 -3.0667012]]...]
INFO - root - 2017-12-15 08:59:34.826188: step 46010, loss = 0.23, batch loss = 0.20 (35.9 examples/sec; 0.223 sec/batch; 17h:44m:55s remains)
INFO - root - 2017-12-15 08:59:37.101428: step 46020, loss = 0.31, batch loss = 0.27 (36.3 examples/sec; 0.220 sec/batch; 17h:32m:01s remains)
INFO - root - 2017-12-15 08:59:39.344687: step 46030, loss = 0.22, batch loss = 0.19 (35.3 examples/sec; 0.227 sec/batch; 18h:02m:35s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 08:59:41.616906: step 46040, loss = 0.19, batch loss = 0.16 (35.8 examples/sec; 0.224 sec/batch; 17h:47m:36s remains)
INFO - root - 2017-12-15 08:59:43.901196: step 46050, loss = 0.21, batch loss = 0.18 (36.0 examples/sec; 0.222 sec/batch; 17h:39m:30s remains)
INFO - root - 2017-12-15 08:59:46.179772: step 46060, loss = 0.22, batch loss = 0.19 (34.6 examples/sec; 0.231 sec/batch; 18h:22m:23s remains)
INFO - root - 2017-12-15 08:59:48.425296: step 46070, loss = 0.21, batch loss = 0.18 (35.7 examples/sec; 0.224 sec/batch; 17h:51m:06s remains)
INFO - root - 2017-12-15 08:59:50.669393: step 46080, loss = 0.20, batch loss = 0.17 (35.3 examples/sec; 0.227 sec/batch; 18h:02m:04s remains)
INFO - root - 2017-12-15 08:59:52.949766: step 46090, loss = 0.20, batch loss = 0.17 (35.4 examples/sec; 0.226 sec/batch; 17h:59m:57s remains)
INFO - root - 2017-12-15 08:59:55.235360: step 46100, loss = 0.20, batch loss = 0.17 (35.5 examples/sec; 0.226 sec/batch; 17h:56m:44s remains)
2017-12-15 08:59:55.517925: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0006866 -8.6265955 -8.4208994 -7.6610518 -6.4350767 -5.4827614 -5.0931873 -5.0332117 -5.1478872 -6.4653063 -7.3930168 -7.5284224 -7.1087208 -6.2723694 -5.4134459][-7.3566275 -8.6175041 -8.2925816 -7.3775539 -6.0186043 -4.8058081 -4.4272175 -4.6548977 -5.1947575 -6.4957027 -7.6002722 -7.9899645 -7.7563553 -7.0322285 -6.239171][-8.0366039 -8.1325436 -7.8617973 -6.9818115 -5.4103785 -3.8636854 -3.3034439 -3.66007 -4.5405245 -6.0521584 -7.2662029 -7.861371 -7.8585186 -7.33768 -6.6639414][-8.19716 -7.5948191 -7.3848228 -6.5214815 -4.6926866 -2.6054161 -1.4918255 -1.7013903 -2.8767753 -4.6826797 -6.0119066 -6.8452635 -7.1318178 -6.9409447 -6.3740788][-8.0873966 -7.309907 -6.9192162 -5.8859558 -3.7137117 -0.86844587 1.1913543 1.4987068 -0.038965702 -2.5474002 -4.161027 -5.2183456 -5.8649521 -6.1868534 -5.7827263][-8.1665764 -6.7615747 -5.7728977 -4.2993584 -1.7710174 1.7460034 4.7033892 5.4210224 3.1367481 -0.4986968 -2.6342461 -3.8842206 -4.8366966 -5.67712 -5.3454638][-7.6285439 -6.0639248 -4.543745 -2.6855381 0.19197631 4.210721 7.9420147 8.94873 5.8616552 1.1187973 -1.642848 -2.9982293 -4.1012945 -5.4241648 -5.3322039][-7.4774561 -5.5842834 -3.8967445 -1.9261189 1.0821404 5.0897779 9.0792847 10.059064 6.4669094 1.2262142 -1.8623372 -3.0114627 -3.9114089 -5.3543367 -5.431078][-7.2002783 -5.1856642 -3.7362151 -2.0254955 0.68226886 4.1031294 7.5384388 8.1740417 4.591012 -0.48286986 -3.5097296 -4.1687169 -4.528605 -5.6347413 -5.8596792][-7.2252083 -5.3422508 -4.543992 -3.3866577 -1.2132113 1.4624388 4.1532087 4.6174278 1.5874217 -2.8027453 -5.436419 -5.8755989 -6.0251732 -6.6895847 -6.8516083][-7.3993311 -5.7517242 -5.6451645 -4.9515514 -3.2792845 -1.1104715 0.98033237 1.2793548 -1.1436087 -4.7305193 -6.9280128 -7.1596584 -7.146513 -7.3432331 -7.2707472][-7.6785631 -6.1969395 -6.53668 -6.227602 -4.9696503 -3.1214125 -1.5429525 -1.4432075 -3.278758 -6.1897182 -7.8293495 -7.7739639 -7.61096 -7.407217 -7.0342178][-8.3316813 -7.0808334 -7.6915359 -7.59089 -6.4967394 -4.6523042 -3.1556156 -2.90737 -4.1910677 -6.4834776 -7.6395016 -7.3530369 -6.9317617 -6.3873539 -5.8245587][-8.8823938 -7.8842888 -8.562808 -8.5807686 -7.6442442 -5.8353543 -4.3039603 -3.8011961 -4.5403795 -6.2426724 -6.90769 -6.4030437 -5.8678303 -5.3407879 -4.8708878][-9.4093485 -8.7134809 -9.3176489 -9.4293671 -8.7398262 -7.1390362 -5.6649957 -4.8910265 -5.1168385 -6.210063 -6.5171766 -6.0714431 -5.643208 -5.2728319 -4.9794436]]...]
INFO - root - 2017-12-15 08:59:57.795415: step 46110, loss = 0.20, batch loss = 0.16 (34.4 examples/sec; 0.232 sec/batch; 18h:29m:14s remains)
INFO - root - 2017-12-15 09:00:00.112864: step 46120, loss = 0.20, batch loss = 0.17 (33.7 examples/sec; 0.237 sec/batch; 18h:53m:18s remains)
INFO - root - 2017-12-15 09:00:02.389242: step 46130, loss = 0.35, batch loss = 0.32 (36.1 examples/sec; 0.222 sec/batch; 17h:38m:28s remains)
INFO - root - 2017-12-15 09:00:04.651256: step 46140, loss = 0.22, batch loss = 0.19 (35.0 examples/sec; 0.229 sec/batch; 18h:11m:00s remains)
INFO - root - 2017-12-15 09:00:06.920112: step 46150, loss = 0.19, batch loss = 0.16 (34.7 examples/sec; 0.230 sec/batch; 18h:19m:31s remains)
INFO - root - 2017-12-15 09:00:09.199195: step 46160, loss = 0.22, batch loss = 0.19 (34.6 examples/sec; 0.231 sec/batch; 18h:23m:21s remains)
INFO - root - 2017-12-15 09:00:11.510558: step 46170, loss = 0.19, batch loss = 0.16 (35.0 examples/sec; 0.229 sec/batch; 18h:10m:46s remains)
INFO - root - 2017-12-15 09:00:13.759819: step 46180, loss = 0.29, batch loss = 0.25 (35.0 examples/sec; 0.229 sec/batch; 18h:10m:48s remains)
INFO - root - 2017-12-15 09:00:16.035556: step 46190, loss = 0.20, batch loss = 0.16 (35.6 examples/sec; 0.225 sec/batch; 17h:52m:23s remains)
INFO - root - 2017-12-15 09:00:18.323017: step 46200, loss = 0.20, batch loss = 0.16 (35.0 examples/sec; 0.228 sec/batch; 18h:10m:09s remains)
2017-12-15 09:00:18.612936: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9287524 -6.6569777 -5.8152094 -5.2079411 -5.2881117 -5.6407833 -5.8165088 -5.718698 -5.5930429 -5.6974945 -5.6189032 -5.1681633 -4.8630438 -4.72378 -5.0898285][-4.911963 -7.0456934 -6.6447067 -5.8217587 -5.8401546 -6.2134027 -6.2031355 -6.1029172 -5.78197 -5.6392403 -5.5855103 -5.2994661 -5.0253716 -4.9814334 -5.6494336][-6.0494013 -6.9760246 -6.9140549 -6.0551457 -5.8627071 -5.8438692 -5.3521404 -5.3384271 -5.1783352 -5.1435633 -5.3797832 -5.3434744 -5.0258188 -5.0920444 -5.9043303][-7.1777544 -7.1661987 -7.4369316 -6.8178291 -6.2830954 -5.4187856 -4.2249804 -4.1202788 -3.9945536 -4.0306931 -4.6643586 -5.1648183 -5.2328892 -5.5966382 -6.4197845][-7.6930037 -7.2939005 -7.6771622 -7.0929604 -5.8120327 -3.8990867 -2.0077548 -1.6665854 -1.5141447 -1.6085451 -2.6430008 -3.8017402 -4.6475921 -5.6199789 -6.676465][-7.8133326 -7.72948 -7.6714506 -6.4974356 -4.1331725 -1.2890251 1.2221467 1.9769557 2.0402358 1.4872274 -0.25122547 -2.1143403 -3.7031157 -5.2146091 -6.5640583][-8.126483 -8.5830345 -8.074996 -6.0828066 -2.5171697 1.1930013 4.5668516 5.9970894 5.8926868 4.3958731 1.6369829 -0.92708492 -3.2304754 -5.2324905 -6.8073263][-8.8682528 -9.37163 -8.60419 -6.161726 -2.0866044 1.8300745 5.7187996 7.7733135 7.62988 5.5986462 2.3698418 -0.553967 -3.3514357 -5.6583815 -7.3800888][-9.86245 -10.291685 -9.41283 -6.9504652 -3.1411405 0.37636304 3.9265821 5.8265419 5.6384497 3.5876973 0.64201641 -2.2013965 -4.82668 -6.7423944 -8.0869894][-10.654534 -10.933436 -10.416329 -8.4815912 -5.299715 -2.1843512 0.8768158 2.4190428 2.0371034 0.067365885 -2.599685 -5.1847363 -7.1395445 -8.2466087 -8.8762484][-10.691655 -10.749967 -10.725587 -9.5784655 -7.37679 -4.882638 -2.5301788 -1.3923819 -1.7869804 -3.3364177 -5.39789 -7.4641171 -8.5790367 -8.8609838 -8.8737688][-9.9504929 -9.6889267 -10.116905 -9.78329 -8.4600105 -6.5696974 -4.9886479 -4.4329853 -4.8712478 -6.0138607 -7.3750763 -8.5658932 -8.7320108 -8.5131617 -8.1567307][-9.0946627 -8.5637789 -9.3325062 -9.6961651 -9.0273857 -7.520071 -6.4705391 -6.317811 -6.8117142 -7.5539951 -8.2450743 -8.4761219 -7.7976942 -7.3114347 -6.8714485][-8.0628862 -7.2718792 -8.2729959 -9.1452112 -9.0838118 -8.0243368 -7.4083843 -7.3283405 -7.5641985 -7.89349 -8.1400375 -7.697052 -6.5078659 -5.6787271 -5.0119686][-7.0268316 -5.9929476 -6.8465576 -7.6973667 -7.9286194 -7.371171 -7.3207903 -7.2908258 -7.2048512 -7.237318 -7.2945862 -6.4992676 -5.0930767 -4.1521139 -3.3249907]]...]
INFO - root - 2017-12-15 09:00:20.867718: step 46210, loss = 0.16, batch loss = 0.13 (35.1 examples/sec; 0.228 sec/batch; 18h:07m:37s remains)
INFO - root - 2017-12-15 09:00:23.148986: step 46220, loss = 0.30, batch loss = 0.27 (34.0 examples/sec; 0.235 sec/batch; 18h:42m:48s remains)
INFO - root - 2017-12-15 09:00:25.410289: step 46230, loss = 0.26, batch loss = 0.23 (35.1 examples/sec; 0.228 sec/batch; 18h:06m:41s remains)
INFO - root - 2017-12-15 09:00:27.689314: step 46240, loss = 0.25, batch loss = 0.22 (35.3 examples/sec; 0.226 sec/batch; 17h:59m:54s remains)
INFO - root - 2017-12-15 09:00:29.961246: step 46250, loss = 0.26, batch loss = 0.22 (36.6 examples/sec; 0.218 sec/batch; 17h:21m:52s remains)
INFO - root - 2017-12-15 09:00:32.232595: step 46260, loss = 0.27, batch loss = 0.24 (35.0 examples/sec; 0.229 sec/batch; 18h:11m:48s remains)
INFO - root - 2017-12-15 09:00:34.573639: step 46270, loss = 0.19, batch loss = 0.16 (34.8 examples/sec; 0.230 sec/batch; 18h:16m:50s remains)
INFO - root - 2017-12-15 09:00:36.815420: step 46280, loss = 0.40, batch loss = 0.37 (35.0 examples/sec; 0.228 sec/batch; 18h:08m:58s remains)
INFO - root - 2017-12-15 09:00:39.084299: step 46290, loss = 0.25, batch loss = 0.21 (34.7 examples/sec; 0.231 sec/batch; 18h:21m:12s remains)
INFO - root - 2017-12-15 09:00:41.366677: step 46300, loss = 0.28, batch loss = 0.25 (36.3 examples/sec; 0.220 sec/batch; 17h:29m:57s remains)
2017-12-15 09:00:41.650992: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3640325 -3.0202873 -3.4133127 -3.3378987 -3.8117671 -4.5195017 -3.3432088 -1.8434262 -1.6487595 -2.6743355 -3.8023086 -4.7730155 -5.9396172 -6.7297063 -7.0892816][-2.9770668 -3.4035945 -3.7571192 -3.7514935 -4.0702524 -4.44691 -3.0618608 -1.6892093 -1.5638664 -2.3177028 -3.2004547 -4.2923164 -5.7849884 -6.8382435 -7.3859234][-3.4805694 -3.1828809 -3.6428976 -3.8582025 -4.1243505 -4.2531185 -2.9101691 -1.959415 -2.0985744 -2.6525025 -3.3723502 -4.3813181 -5.9442916 -7.0572505 -7.6002789][-3.0761821 -2.1008482 -2.5198102 -2.7261081 -2.8969848 -2.9447188 -1.8350291 -1.3300092 -1.8174214 -2.4264495 -3.1903706 -4.1851807 -5.6939 -6.9196138 -7.5868812][-2.7224813 -1.1041992 -1.4127228 -1.5384846 -1.7958494 -1.8725594 -0.85605621 -0.41661406 -1.072813 -1.7156351 -2.4810789 -3.6186244 -5.2725096 -6.6160741 -7.4583297][-2.6515441 -0.50384784 -0.51225638 -0.51746213 -0.91169858 -0.93918371 0.32534766 0.9256978 0.25050378 -0.40786552 -1.2544241 -2.7695203 -4.7539659 -6.1964293 -7.2628784][-2.5103245 -0.25402021 0.12238383 0.29986143 -0.1855247 -0.10659337 1.5441871 2.5238547 1.8960958 1.1319625 0.1544857 -1.8174651 -4.1032548 -5.6833086 -7.0492878][-2.4546816 -0.13123178 0.5396533 0.88029742 0.22949886 0.18725348 2.0961337 3.55437 3.1363521 2.4412756 1.414288 -0.9450618 -3.4336991 -5.1826067 -6.9695063][-3.0578365 -0.80905306 0.092324257 0.63760495 0.082149029 0.16454268 2.2810731 3.9580169 3.6835647 3.0771761 1.9389553 -0.73760092 -3.24335 -4.9146252 -6.9349823][-4.3610363 -2.2334394 -1.3082879 -0.61954284 -0.86474419 -0.52378213 1.6598792 3.1901121 3.0061049 2.5714808 1.3805473 -1.3612468 -3.5679107 -4.9484577 -7.0020165][-5.8837028 -4.1938639 -3.6280012 -2.9262133 -2.8343077 -2.2712984 -0.25502181 1.0497599 0.92507839 0.54981446 -0.74085689 -3.2560549 -4.9216776 -5.88416 -7.6669927][-6.387846 -5.2732944 -5.2288265 -4.6888065 -4.46975 -3.8511014 -2.0422583 -0.91773355 -1.1452112 -1.7161489 -3.1665649 -5.3955021 -6.5132155 -7.0418682 -8.3627729][-5.8653049 -5.0988855 -5.4596539 -5.1116962 -4.8408585 -4.2288127 -2.7389228 -1.9059924 -2.2636795 -3.0245078 -4.5475388 -6.5142303 -7.3471656 -7.6552277 -8.59025][-5.7168703 -4.9030457 -5.3848777 -5.2114224 -4.8648014 -4.276999 -3.1403151 -2.6384377 -3.0720141 -3.9051368 -5.3216252 -6.92251 -7.5845313 -7.8778491 -8.61147][-5.1864986 -4.1974506 -4.6976538 -4.7991271 -4.6443591 -4.2382436 -3.4424686 -3.1430471 -3.6407292 -4.5185952 -5.7650962 -6.9940395 -7.5005221 -7.8168797 -8.4297905]]...]
INFO - root - 2017-12-15 09:00:43.916232: step 46310, loss = 0.21, batch loss = 0.18 (36.1 examples/sec; 0.222 sec/batch; 17h:38m:24s remains)
INFO - root - 2017-12-15 09:00:46.188495: step 46320, loss = 0.19, batch loss = 0.16 (34.6 examples/sec; 0.231 sec/batch; 18h:22m:16s remains)
INFO - root - 2017-12-15 09:00:48.451838: step 46330, loss = 0.29, batch loss = 0.25 (34.6 examples/sec; 0.231 sec/batch; 18h:23m:56s remains)
INFO - root - 2017-12-15 09:00:50.715327: step 46340, loss = 0.16, batch loss = 0.13 (36.4 examples/sec; 0.220 sec/batch; 17h:29m:00s remains)
INFO - root - 2017-12-15 09:00:53.016189: step 46350, loss = 0.25, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 17h:50m:13s remains)
INFO - root - 2017-12-15 09:00:55.325621: step 46360, loss = 0.19, batch loss = 0.16 (35.5 examples/sec; 0.226 sec/batch; 17h:55m:42s remains)
INFO - root - 2017-12-15 09:00:57.606025: step 46370, loss = 0.19, batch loss = 0.16 (35.6 examples/sec; 0.225 sec/batch; 17h:50m:57s remains)
INFO - root - 2017-12-15 09:00:59.852113: step 46380, loss = 0.24, batch loss = 0.21 (34.8 examples/sec; 0.230 sec/batch; 18h:17m:04s remains)
INFO - root - 2017-12-15 09:01:02.127700: step 46390, loss = 0.29, batch loss = 0.26 (36.4 examples/sec; 0.220 sec/batch; 17h:29m:00s remains)
INFO - root - 2017-12-15 09:01:04.394792: step 46400, loss = 0.27, batch loss = 0.24 (35.4 examples/sec; 0.226 sec/batch; 17h:57m:04s remains)
2017-12-15 09:01:04.686779: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9521508 -1.9218203 -1.3815466 -1.3824853 -1.7260613 -2.0216973 -2.5239074 -2.8760777 -2.7382526 -2.7190793 -2.4406497 -2.001508 -1.2123955 -0.42226946 -0.3605907][-1.9174938 -1.0514882 -0.35140133 -0.22198296 -0.66922724 -1.2087328 -1.948674 -2.3479037 -2.0855815 -1.7536976 -1.3312368 -1.062868 -0.54038596 -0.021090746 -0.17495537][-1.5255201 -0.24113488 0.37728405 0.42623997 -0.15159965 -0.79192424 -1.4925973 -1.6645801 -1.0293199 -0.53477228 -0.31650198 -0.37447762 -0.18868876 -0.032658577 -0.27070439][-2.1065438 -0.48476553 0.14044333 0.22691536 -0.21191621 -0.69415653 -1.2214745 -1.1374786 -0.3422823 0.15946436 0.309227 0.15081429 0.15517449 0.0084810257 -0.55901957][-3.1664631 -1.2480084 -0.53019488 -0.23633575 -0.42322314 -0.7409327 -1.0730907 -0.83671343 -0.1030674 0.13017845 0.15868711 0.23352361 0.45267773 0.22134614 -0.50233114][-4.1746731 -2.0308051 -1.3724787 -0.89718544 -0.7935276 -0.80094075 -0.93132889 -0.76804757 -0.31961477 -0.40986848 -0.4231329 -0.20224476 0.28103232 0.39571071 -0.12041545][-4.6209373 -2.778527 -2.20697 -1.7206764 -1.5851595 -1.3911706 -1.2707831 -1.0433638 -0.71456063 -1.1295228 -1.2321743 -0.91693532 -0.2621243 0.17912626 -0.15833521][-4.42928 -2.7946696 -2.309082 -2.0828152 -2.366231 -2.2565341 -2.0719843 -1.7009379 -1.3296244 -1.8810256 -2.1865993 -1.8818526 -1.0938638 -0.37204611 -0.61104023][-4.1951714 -2.4729941 -1.9826205 -1.8495548 -2.498184 -2.6638498 -2.5741241 -2.2425218 -1.9325061 -2.6504102 -3.0802574 -2.8420563 -2.1119361 -1.3372817 -1.3569044][-4.66224 -2.9401433 -2.2922206 -2.0048208 -2.5880251 -2.7513061 -2.8144679 -2.6353345 -2.3043962 -2.8618743 -3.21283 -3.0663805 -2.3833942 -1.5275061 -1.297868][-5.3122654 -3.5265858 -2.5763671 -2.0051718 -2.4016802 -2.6265495 -2.8334329 -2.673229 -2.2572162 -2.6107438 -2.8939242 -2.8203964 -1.9666407 -0.94876409 -0.64026964][-5.9602833 -4.1191325 -2.9596539 -2.061759 -2.0600615 -2.2476006 -2.4935315 -2.3700464 -2.0670362 -2.2893796 -2.3181903 -2.1287172 -1.274217 -0.47413731 -0.41276705][-6.4964037 -4.6713753 -3.3823361 -2.2981598 -1.8622842 -1.8197968 -2.0688937 -2.1102118 -1.8415586 -1.8264017 -1.5502131 -1.3138204 -0.768062 -0.63962018 -1.099744][-5.8386455 -4.1495409 -3.0443556 -1.9192268 -1.1033558 -0.936018 -1.3798156 -1.7076442 -1.5414908 -1.2013453 -0.75849736 -0.71854186 -0.63195312 -1.0112139 -1.7994342][-4.6376085 -2.8945892 -1.7768139 -0.55446804 0.19049573 -0.11479616 -0.90386641 -1.3907615 -1.2691156 -0.78971255 -0.48471761 -0.74364114 -0.90471935 -1.3008573 -1.9976425]]...]
INFO - root - 2017-12-15 09:01:06.947301: step 46410, loss = 0.18, batch loss = 0.14 (35.3 examples/sec; 0.227 sec/batch; 18h:01m:50s remains)
INFO - root - 2017-12-15 09:01:09.248338: step 46420, loss = 0.44, batch loss = 0.40 (35.8 examples/sec; 0.224 sec/batch; 17h:46m:34s remains)
INFO - root - 2017-12-15 09:01:11.511651: step 46430, loss = 0.31, batch loss = 0.27 (35.3 examples/sec; 0.227 sec/batch; 18h:01m:07s remains)
INFO - root - 2017-12-15 09:01:13.773011: step 46440, loss = 0.27, batch loss = 0.23 (34.8 examples/sec; 0.230 sec/batch; 18h:16m:42s remains)
INFO - root - 2017-12-15 09:01:16.022596: step 46450, loss = 0.27, batch loss = 0.24 (35.2 examples/sec; 0.227 sec/batch; 18h:02m:37s remains)
INFO - root - 2017-12-15 09:01:18.326832: step 46460, loss = 0.18, batch loss = 0.15 (34.9 examples/sec; 0.229 sec/batch; 18h:11m:53s remains)
INFO - root - 2017-12-15 09:01:20.644237: step 46470, loss = 0.15, batch loss = 0.12 (33.5 examples/sec; 0.239 sec/batch; 18h:59m:54s remains)
INFO - root - 2017-12-15 09:01:22.908305: step 46480, loss = 0.20, batch loss = 0.16 (34.5 examples/sec; 0.232 sec/batch; 18h:25m:52s remains)
INFO - root - 2017-12-15 09:01:25.197950: step 46490, loss = 0.20, batch loss = 0.17 (34.7 examples/sec; 0.231 sec/batch; 18h:18m:47s remains)
INFO - root - 2017-12-15 09:01:27.471421: step 46500, loss = 0.19, batch loss = 0.16 (34.8 examples/sec; 0.230 sec/batch; 18h:16m:05s remains)
2017-12-15 09:01:27.767516: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0225277 -6.0324192 -6.3467016 -6.5707006 -6.6314497 -6.2538891 -5.8005114 -5.5772486 -5.5383282 -5.7547903 -5.9276142 -5.955122 -5.8682241 -5.8212667 -5.951066][-4.8370171 -7.2691927 -7.8752851 -8.20156 -8.1158 -7.3872023 -6.6401663 -6.3440628 -6.4017973 -6.9595737 -7.3692646 -7.2660866 -6.8774662 -6.5044813 -6.5537519][-6.3656654 -7.9356976 -8.7575293 -9.0625181 -8.6607323 -7.4743805 -6.4000158 -6.0485477 -6.2311697 -7.2070866 -8.04588 -7.9584179 -7.3988085 -6.7441797 -6.575469][-7.2555056 -8.139698 -8.8646193 -8.8450041 -7.7994623 -5.9502983 -4.5248351 -4.2661581 -4.7811804 -6.3668747 -7.919239 -8.1361322 -7.7261848 -7.0516214 -6.7453175][-7.2691517 -7.7678127 -8.0027657 -7.2975855 -5.42443 -2.93322 -1.1862855 -1.1790924 -2.0810149 -4.2660151 -6.4950504 -7.2559805 -7.3594742 -7.07119 -6.8258047][-7.0537291 -6.8329821 -6.3334551 -4.7798138 -2.0891449 1.0214114 3.1625288 2.8990538 1.4253342 -1.3628141 -4.2145038 -5.7127533 -6.4769087 -6.6052713 -6.4779558][-6.0857763 -5.8166695 -4.6127434 -2.2935576 1.0204792 4.5386648 6.85297 6.4241238 4.3791304 0.93643188 -2.3383734 -4.5465817 -5.890964 -6.1888781 -5.9604177][-5.9947319 -5.3472252 -3.8890314 -1.2654257 2.2715447 5.9145422 8.4173231 8.0690775 5.6266994 1.8566296 -1.4420233 -4.1334 -5.8702612 -6.186934 -5.8354816][-6.1816764 -5.5903053 -4.5695395 -2.4077132 0.88868427 4.4111881 7.0018835 6.8370314 4.4600058 0.75999403 -2.0118306 -4.5505352 -6.3172483 -6.6551094 -6.1859169][-6.5965424 -6.3023968 -6.1149511 -4.8905506 -2.1359527 1.1381652 3.7065356 3.8863885 1.814913 -1.6517916 -3.8229666 -5.7690692 -7.1488352 -7.2788544 -6.5991468][-7.1044168 -7.1249352 -7.6553116 -7.2431774 -5.0879841 -2.2331984 0.04770875 0.26239872 -1.3336077 -4.2471833 -5.837038 -7.1288395 -7.8860693 -7.5213165 -6.6704245][-7.5329142 -7.6654129 -8.4780064 -8.5339041 -7.0631771 -4.8641157 -2.9752998 -2.7592051 -3.8780246 -6.1124611 -7.44265 -8.293457 -8.4847364 -7.7263422 -6.7480774][-7.7830896 -7.8465567 -8.5881758 -8.7870474 -7.9587393 -6.5425816 -5.2678003 -5.1135197 -5.9043818 -7.5118618 -8.4990435 -8.9060411 -8.5811958 -7.5314569 -6.6424189][-7.64036 -7.4946384 -7.9970207 -8.1429415 -7.7072296 -6.8996325 -6.1313171 -6.1526289 -6.7805958 -7.8049183 -8.4089918 -8.4505606 -7.9011965 -6.9663363 -6.352562][-7.2511187 -6.8470383 -7.0778103 -7.0981693 -6.8483295 -6.390276 -5.9720225 -6.0660725 -6.5871334 -7.2454176 -7.6043425 -7.4802332 -6.9536171 -6.28469 -5.9076152]]...]
INFO - root - 2017-12-15 09:01:30.048274: step 46510, loss = 0.29, batch loss = 0.26 (35.9 examples/sec; 0.223 sec/batch; 17h:41m:54s remains)
INFO - root - 2017-12-15 09:01:32.298930: step 46520, loss = 0.18, batch loss = 0.15 (36.1 examples/sec; 0.222 sec/batch; 17h:36m:59s remains)
INFO - root - 2017-12-15 09:01:34.589029: step 46530, loss = 0.16, batch loss = 0.12 (34.2 examples/sec; 0.234 sec/batch; 18h:33m:51s remains)
INFO - root - 2017-12-15 09:01:36.886201: step 46540, loss = 0.16, batch loss = 0.13 (35.8 examples/sec; 0.223 sec/batch; 17h:44m:04s remains)
INFO - root - 2017-12-15 09:01:39.155143: step 46550, loss = 0.29, batch loss = 0.26 (34.1 examples/sec; 0.235 sec/batch; 18h:39m:12s remains)
INFO - root - 2017-12-15 09:01:41.395464: step 46560, loss = 0.17, batch loss = 0.13 (35.2 examples/sec; 0.227 sec/batch; 18h:02m:19s remains)
INFO - root - 2017-12-15 09:01:43.646547: step 46570, loss = 0.24, batch loss = 0.21 (35.3 examples/sec; 0.226 sec/batch; 17h:59m:05s remains)
INFO - root - 2017-12-15 09:01:45.941192: step 46580, loss = 0.19, batch loss = 0.15 (35.4 examples/sec; 0.226 sec/batch; 17h:55m:49s remains)
INFO - root - 2017-12-15 09:01:48.190354: step 46590, loss = 0.25, batch loss = 0.22 (35.1 examples/sec; 0.228 sec/batch; 18h:06m:10s remains)
INFO - root - 2017-12-15 09:01:50.470956: step 46600, loss = 0.27, batch loss = 0.24 (34.5 examples/sec; 0.232 sec/batch; 18h:23m:38s remains)
2017-12-15 09:01:50.753172: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.2518911 -7.65635 -7.4196916 -7.0379505 -6.6600065 -6.3423367 -5.753571 -5.3788958 -5.8941689 -7.174984 -8.0670147 -7.8818555 -7.7601304 -7.3760023 -7.2744732][-7.0836496 -7.4424663 -7.5350685 -7.50451 -7.3820224 -7.1688976 -6.4506702 -5.8419456 -6.1857209 -7.246027 -7.8911567 -7.4831538 -6.9268832 -6.60539 -6.9206867][-7.1457143 -6.4689827 -6.7678189 -7.052175 -7.10156 -6.8926196 -5.9161758 -4.8983417 -5.04016 -6.0450482 -6.6259513 -6.1751518 -5.3443055 -5.2032089 -5.8941612][-7.0615983 -5.7504172 -6.0816703 -6.3209491 -6.0329609 -5.3641868 -3.9120672 -2.5424123 -2.6900549 -4.0062194 -4.940155 -4.748363 -3.8087268 -3.8841391 -4.9907637][-6.1558213 -4.7311497 -5.0147257 -5.1564789 -4.5266809 -3.2567849 -1.0246164 0.85590863 0.588706 -1.3837516 -2.8896654 -2.9085152 -1.9026911 -2.1745443 -3.5793505][-5.2565327 -3.7788107 -3.81973 -3.6943266 -2.7394669 -0.98824728 1.7932677 3.9072857 3.3360147 0.68918753 -1.4604515 -1.8141148 -0.8455534 -1.1228651 -2.4792867][-4.7226734 -3.5111504 -3.0415773 -2.4291897 -1.2434478 0.71669626 3.6776762 5.7690339 5.00793 1.8332567 -0.86326277 -1.4992316 -0.48317528 -0.51744509 -1.4395792][-5.3618693 -3.831157 -2.8462291 -1.84232 -0.71006262 1.1852458 4.0652342 5.8951678 5.0924792 1.8165827 -1.2113285 -1.9798207 -1.0609368 -0.93097878 -1.547204][-6.0753193 -4.2910342 -3.0736561 -1.9016327 -0.95182848 0.73078108 3.1964984 4.4006605 3.3551016 0.10767341 -2.9944701 -3.6559625 -2.7111356 -2.3841581 -2.809413][-6.5442877 -4.6587667 -3.6204321 -2.8144104 -2.3687108 -1.2320887 0.62105274 1.2049656 0.029613972 -2.9416776 -5.7646956 -6.0061035 -4.8197656 -4.2121115 -4.5133133][-7.0182614 -5.2566357 -4.6602468 -4.2782774 -4.188529 -3.5078683 -2.1703792 -1.8349171 -2.8659422 -5.2307253 -7.5198412 -7.4096251 -6.2027435 -5.6277514 -5.9095573][-7.3913474 -5.8788152 -5.5551 -5.4705048 -5.5343266 -5.3030066 -4.5122995 -4.3783946 -5.2960234 -7.0544105 -8.6979961 -8.4370337 -7.4321346 -7.0303273 -7.2669926][-7.51074 -6.4175949 -6.3090844 -6.4012809 -6.5294018 -6.600544 -6.2506886 -6.2582493 -7.0760846 -8.2602034 -9.2865181 -9.00531 -8.2774582 -8.0663252 -8.1555061][-7.3618684 -6.5796752 -6.5460343 -6.5819445 -6.549962 -6.6498704 -6.5452695 -6.6424904 -7.3847303 -8.1767044 -8.6653471 -8.3512526 -7.8413153 -7.733798 -7.7048025][-6.6778955 -5.9756632 -5.9080992 -5.9245315 -5.8436289 -5.7954626 -5.6167593 -5.6667585 -6.1685152 -6.6937866 -6.946363 -6.6894827 -6.3804817 -6.3100109 -6.2824712]]...]
INFO - root - 2017-12-15 09:01:53.014240: step 46610, loss = 0.26, batch loss = 0.23 (36.4 examples/sec; 0.220 sec/batch; 17h:26m:08s remains)
INFO - root - 2017-12-15 09:01:55.286733: step 46620, loss = 0.22, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 17h:40m:24s remains)
INFO - root - 2017-12-15 09:01:57.580922: step 46630, loss = 0.26, batch loss = 0.23 (33.3 examples/sec; 0.240 sec/batch; 19h:03m:32s remains)
INFO - root - 2017-12-15 09:01:59.865832: step 46640, loss = 0.17, batch loss = 0.13 (35.7 examples/sec; 0.224 sec/batch; 17h:48m:06s remains)
INFO - root - 2017-12-15 09:02:02.166171: step 46650, loss = 0.23, batch loss = 0.20 (33.2 examples/sec; 0.241 sec/batch; 19h:09m:06s remains)
INFO - root - 2017-12-15 09:02:04.451484: step 46660, loss = 0.22, batch loss = 0.19 (35.7 examples/sec; 0.224 sec/batch; 17h:46m:56s remains)
INFO - root - 2017-12-15 09:02:06.682485: step 46670, loss = 0.15, batch loss = 0.12 (35.8 examples/sec; 0.223 sec/batch; 17h:43m:17s remains)
INFO - root - 2017-12-15 09:02:08.994956: step 46680, loss = 0.19, batch loss = 0.16 (33.2 examples/sec; 0.241 sec/batch; 19h:07m:00s remains)
INFO - root - 2017-12-15 09:02:11.298368: step 46690, loss = 0.19, batch loss = 0.15 (35.0 examples/sec; 0.229 sec/batch; 18h:08m:27s remains)
INFO - root - 2017-12-15 09:02:13.584088: step 46700, loss = 0.19, batch loss = 0.16 (35.5 examples/sec; 0.225 sec/batch; 17h:53m:58s remains)
2017-12-15 09:02:13.886449: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1428246 -1.6886637 -0.76759565 -0.42322457 -0.31005049 -0.57372117 -1.0669972 -1.6289366 -2.44257 -2.6745362 -2.9587467 -3.1705825 -3.7436142 -4.5198593 -5.0318928][-2.4970574 -2.0873022 -0.98611546 -0.52717245 -0.28017139 -0.32656646 -0.541994 -0.96322775 -1.8796538 -2.6229336 -3.7494776 -4.7022486 -5.772912 -6.8354135 -7.4254055][-3.3238316 -2.5932472 -1.4849024 -0.70285511 0.28779697 1.078125 1.3455615 0.92924619 -0.2687422 -1.740876 -3.6955278 -5.3420544 -6.8887882 -8.26252 -8.97522][-4.2933769 -3.4342089 -2.3255849 -1.0746109 0.83388877 2.7205753 3.5434504 2.95716 1.2647431 -0.94395876 -3.5372446 -5.7390494 -7.5584092 -9.1136169 -9.868578][-5.2253027 -4.2687607 -3.1438425 -1.5974376 0.76264191 3.3223724 4.5523057 3.8331661 1.7088363 -0.95636213 -3.801043 -6.1836433 -7.9600363 -9.4175472 -9.92667][-5.752008 -4.5684414 -3.4213872 -1.9519531 0.54402661 3.4692435 4.9693732 4.29473 1.8772755 -1.0932525 -4.0908155 -6.4971 -8.05923 -9.1653595 -9.196805][-6.1441669 -4.6068439 -3.2549708 -1.9056194 0.59176707 3.5533118 4.9790235 4.2409825 1.5166485 -1.6531403 -4.7480822 -6.9851274 -8.1020479 -8.5756 -7.9908514][-6.6432467 -4.457583 -2.6890097 -1.3104426 1.015871 3.4242082 4.0067325 2.83501 -0.058060408 -3.0156388 -5.8708982 -7.5552211 -7.9478712 -7.5133891 -6.1360474][-6.73315 -4.1037817 -2.1247902 -0.73788524 1.3427198 3.0032463 2.8205438 1.5508478 -1.181181 -3.8573916 -6.3811293 -7.4317775 -7.2426939 -6.0778408 -4.056757][-6.0369978 -3.5361867 -1.8596352 -0.46591091 1.5764918 2.7806058 2.2769485 1.068223 -1.5813018 -4.300714 -6.7622643 -7.3197241 -6.6475344 -4.9479523 -2.5860808][-5.6855497 -3.404645 -2.1350229 -0.58294606 1.4980733 2.3939915 1.7471628 0.60003376 -1.924341 -4.7464261 -7.1795111 -7.356977 -6.3315468 -4.343925 -1.9241657][-5.3663549 -3.0565584 -2.2476056 -0.78986788 0.85740066 1.2656333 0.55147028 -0.48486221 -2.8198259 -5.549531 -7.7449708 -7.5931134 -6.3363795 -4.2621088 -1.9691838][-5.3544159 -3.0678854 -2.7819209 -1.6679299 -0.69234407 -0.68661237 -1.2836392 -2.21615 -4.304965 -6.7304192 -8.46852 -8.0269089 -6.5779133 -4.5139208 -2.5831761][-5.4444518 -3.1006937 -3.1163182 -2.3296645 -1.9090056 -2.1154826 -2.7091551 -3.6393075 -5.164609 -6.8525639 -7.8648553 -7.2298636 -5.9188724 -4.3807182 -3.1492782][-4.9498425 -2.6283293 -2.8542616 -2.5376921 -2.5057149 -2.6483049 -3.1017671 -3.8274398 -4.5704327 -5.3616905 -5.6908693 -5.0251036 -4.1119332 -3.4125488 -3.0365267]]...]
INFO - root - 2017-12-15 09:02:16.179759: step 46710, loss = 0.22, batch loss = 0.19 (35.0 examples/sec; 0.228 sec/batch; 18h:08m:19s remains)
INFO - root - 2017-12-15 09:02:18.475720: step 46720, loss = 0.23, batch loss = 0.20 (34.6 examples/sec; 0.231 sec/batch; 18h:20m:28s remains)
INFO - root - 2017-12-15 09:02:20.751124: step 46730, loss = 0.18, batch loss = 0.14 (36.0 examples/sec; 0.222 sec/batch; 17h:38m:21s remains)
INFO - root - 2017-12-15 09:02:23.041596: step 46740, loss = 0.24, batch loss = 0.21 (36.0 examples/sec; 0.222 sec/batch; 17h:39m:20s remains)
INFO - root - 2017-12-15 09:02:25.287448: step 46750, loss = 0.19, batch loss = 0.16 (35.1 examples/sec; 0.228 sec/batch; 18h:06m:11s remains)
INFO - root - 2017-12-15 09:02:27.562190: step 46760, loss = 0.29, batch loss = 0.26 (34.7 examples/sec; 0.230 sec/batch; 18h:17m:35s remains)
INFO - root - 2017-12-15 09:02:29.876112: step 46770, loss = 0.30, batch loss = 0.26 (33.4 examples/sec; 0.240 sec/batch; 19h:00m:48s remains)
INFO - root - 2017-12-15 09:02:32.153666: step 46780, loss = 0.33, batch loss = 0.30 (36.2 examples/sec; 0.221 sec/batch; 17h:32m:42s remains)
INFO - root - 2017-12-15 09:02:34.473554: step 46790, loss = 0.21, batch loss = 0.18 (34.8 examples/sec; 0.230 sec/batch; 18h:14m:42s remains)
INFO - root - 2017-12-15 09:02:36.764153: step 46800, loss = 0.43, batch loss = 0.39 (34.9 examples/sec; 0.230 sec/batch; 18h:12m:54s remains)
2017-12-15 09:02:37.053007: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1222434 -5.3572407 -5.6034117 -5.8903632 -5.7641883 -5.2578259 -5.1170468 -5.139307 -4.8726377 -4.4342556 -4.5647335 -4.9846544 -5.22939 -5.7299657 -6.1256595][-5.2546482 -4.2645464 -4.3831968 -4.5966349 -4.2821541 -3.7314334 -3.6816673 -3.7921057 -3.5537314 -3.2539937 -3.4105406 -3.7129798 -3.9681129 -4.5320835 -5.1065111][-4.8087025 -3.27659 -3.13766 -3.1693354 -2.7041843 -2.204134 -2.0854385 -2.1292725 -2.0119312 -2.0398545 -2.2653778 -2.5120332 -2.8570738 -3.26858 -3.6020074][-3.8601742 -2.0420129 -1.7153625 -1.6592185 -1.2005496 -0.71392334 -0.47101259 -0.28652811 -0.30461645 -0.72446585 -1.1701685 -1.7199717 -2.4541461 -2.7344451 -2.9505167][-3.4754119 -1.2825563 -0.88999176 -0.69132817 -0.024050951 0.62334085 1.133976 1.4879055 1.52317 0.993567 0.40074277 -0.6834929 -2.0435636 -2.496675 -2.8273337][-3.1048715 -0.89392543 -0.46803856 -0.080301762 0.73689508 1.5119162 2.0887222 2.4490647 2.7181764 2.5035048 2.1035967 0.88468742 -0.67898142 -1.3658385 -2.0416493][-2.4241812 -0.79166257 -0.50850916 -0.22672558 0.43632269 1.1746032 1.6986303 2.1545897 2.8593392 3.1467724 2.9884019 2.0295668 0.71373725 -0.096116781 -1.1871768][-2.4516096 -1.0681338 -0.9938823 -0.87497032 -0.36212671 0.39178014 0.84930229 1.3303344 2.200964 2.5985541 2.5871902 2.0645061 1.284025 0.53584695 -0.76239336][-2.9758079 -1.7714682 -1.7923706 -1.785578 -1.528841 -1.0926485 -0.80214262 -0.34542763 0.49415278 0.84411407 0.96087027 1.080328 1.0476003 0.52001214 -0.66812062][-3.5145783 -2.6136968 -2.6729779 -2.6902628 -2.8709586 -3.031096 -3.07327 -2.6128631 -1.7620945 -1.3868597 -1.1299348 -0.48725021 -0.023511171 -0.32102156 -1.1226263][-4.1988144 -3.6247892 -3.6150122 -3.4020357 -3.7228363 -4.3573546 -4.768136 -4.4888034 -3.8393049 -3.6055598 -3.2487469 -2.3003442 -1.5920918 -1.6081285 -1.8801003][-4.40055 -4.0799112 -4.073431 -3.7248697 -3.8777511 -4.4837704 -4.8878107 -4.813251 -4.5624809 -4.66368 -4.3211403 -3.3707111 -2.6199827 -2.3323441 -2.0196526][-4.183002 -3.9667277 -4.0806675 -3.768054 -3.9047976 -4.4434271 -4.8241959 -4.8953257 -4.7947154 -4.9510956 -4.6296449 -3.7593811 -2.9817986 -2.4294629 -1.6187253][-4.1162834 -3.8622837 -3.9035566 -3.5328526 -3.5777667 -3.9885159 -4.33434 -4.6163092 -4.6671181 -4.7944331 -4.5092573 -3.832931 -3.0719059 -2.385093 -1.3316095][-3.9329381 -3.4136343 -3.256978 -2.6982994 -2.4025936 -2.4091504 -2.5785227 -3.033607 -3.3016479 -3.5647621 -3.6348176 -3.471653 -2.9405789 -2.4448876 -1.6392126]]...]
INFO - root - 2017-12-15 09:02:39.386202: step 46810, loss = 0.35, batch loss = 0.32 (35.0 examples/sec; 0.229 sec/batch; 18h:08m:53s remains)
INFO - root - 2017-12-15 09:02:41.683095: step 46820, loss = 0.21, batch loss = 0.18 (35.5 examples/sec; 0.226 sec/batch; 17h:54m:28s remains)
INFO - root - 2017-12-15 09:02:44.027805: step 46830, loss = 0.21, batch loss = 0.18 (34.2 examples/sec; 0.234 sec/batch; 18h:33m:01s remains)
INFO - root - 2017-12-15 09:02:46.321283: step 46840, loss = 0.19, batch loss = 0.16 (34.4 examples/sec; 0.232 sec/batch; 18h:26m:06s remains)
INFO - root - 2017-12-15 09:02:48.636747: step 46850, loss = 0.28, batch loss = 0.25 (33.2 examples/sec; 0.241 sec/batch; 19h:08m:20s remains)
INFO - root - 2017-12-15 09:02:50.928573: step 46860, loss = 0.26, batch loss = 0.23 (34.7 examples/sec; 0.231 sec/batch; 18h:18m:12s remains)
INFO - root - 2017-12-15 09:02:53.265185: step 46870, loss = 0.20, batch loss = 0.17 (31.8 examples/sec; 0.251 sec/batch; 19h:56m:05s remains)
INFO - root - 2017-12-15 09:02:55.583131: step 46880, loss = 0.25, batch loss = 0.22 (34.4 examples/sec; 0.233 sec/batch; 18h:26m:49s remains)
INFO - root - 2017-12-15 09:02:57.858589: step 46890, loss = 0.19, batch loss = 0.16 (36.6 examples/sec; 0.219 sec/batch; 17h:20m:44s remains)
INFO - root - 2017-12-15 09:03:00.119822: step 46900, loss = 0.24, batch loss = 0.21 (35.0 examples/sec; 0.229 sec/batch; 18h:08m:21s remains)
2017-12-15 09:03:00.421725: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.98636 -4.7459326 -5.3491 -5.2902946 -4.6991339 -4.6775007 -5.5255804 -6.5395918 -7.0147204 -6.4710097 -6.1222587 -5.7018423 -5.511127 -6.1689453 -6.13326][-5.075984 -4.0592022 -4.8168011 -4.9777694 -4.4842887 -4.4527845 -5.2234268 -6.2951975 -6.7549539 -6.0467839 -5.8134718 -5.5500193 -5.3869982 -5.98448 -5.7402134][-4.4487591 -2.5701079 -3.4387937 -3.848773 -3.5606813 -3.6859555 -4.4195366 -5.5639505 -6.0272708 -5.3600626 -5.3734789 -5.2391577 -4.9838905 -5.3743334 -4.9375076][-3.70709 -0.87563121 -1.5609704 -2.1309884 -2.1467433 -2.506356 -3.299808 -4.5039697 -4.9112577 -4.4032097 -4.7040281 -4.67284 -4.276988 -4.5844097 -4.2768936][-2.7192791 0.18696404 -0.49391127 -1.1135331 -1.0706221 -1.3739126 -2.0094879 -3.0044613 -3.2895637 -3.0121722 -3.6603565 -3.8790245 -3.51894 -3.8361173 -3.7297933][-2.0972676 0.80425286 0.099577665 -0.43782032 -0.24655247 -0.17490292 -0.42653012 -1.0404251 -1.3016751 -1.3655593 -2.2259912 -2.5200326 -2.0489249 -2.3363464 -2.5251157][-1.1906202 1.0503674 0.2100594 -0.22119164 0.20610356 0.57344604 0.54370856 0.12301278 -0.26343751 -0.5854212 -1.4256344 -1.6300708 -1.0825807 -1.3350368 -1.6555889][-1.0022007 1.1204882 0.20279145 -0.14674377 0.16361046 0.59774709 0.50247478 -0.038477659 -0.7106204 -1.2794781 -2.0750017 -2.1778874 -1.4873221 -1.5061013 -1.4810159][-1.0979512 1.1964035 0.37823248 0.031427622 0.04626894 0.24636936 -0.16592622 -1.0393401 -1.8780154 -2.4957061 -3.0479984 -2.8317666 -1.8685901 -1.5174394 -1.0319362][-1.5651715 0.98245454 0.27808309 -0.067709208 -0.37703979 -0.5175947 -1.1685674 -2.1599214 -3.010956 -3.5854115 -3.9294991 -3.4644547 -2.2973638 -1.649529 -0.90978181][-1.8432612 0.74161768 0.16377091 -0.1244421 -0.52092016 -0.86361086 -1.6058798 -2.5632133 -3.2886138 -3.9122677 -4.3391161 -3.9627757 -2.9266038 -2.3187287 -1.5947101][-2.2629359 0.17720652 -0.21600795 -0.38101649 -0.58531511 -0.826215 -1.434078 -2.1458013 -2.667768 -3.153887 -3.5787554 -3.3109586 -2.5882006 -2.3387501 -2.052099][-2.8745532 -0.65181231 -0.93370974 -1.1078358 -1.3051957 -1.4399538 -1.7852068 -2.1250196 -2.2205558 -2.34716 -2.5426009 -2.3907549 -2.0336542 -2.246304 -2.4861026][-2.9774442 -0.96725154 -1.1776934 -1.6103873 -2.1020036 -2.3386433 -2.3813975 -2.2376471 -2.022403 -1.8080301 -1.7089274 -1.6501933 -1.6454439 -2.1421146 -2.6691682][-2.896522 -0.92298007 -1.1483661 -1.8927214 -2.8437872 -3.2518985 -2.9727378 -2.3550117 -1.8189117 -1.2269377 -0.76614404 -0.76374006 -0.95861232 -1.4752686 -2.0985684]]...]
INFO - root - 2017-12-15 09:03:02.723782: step 46910, loss = 0.23, batch loss = 0.19 (34.0 examples/sec; 0.235 sec/batch; 18h:39m:02s remains)
INFO - root - 2017-12-15 09:03:05.016384: step 46920, loss = 0.21, batch loss = 0.18 (34.0 examples/sec; 0.236 sec/batch; 18h:41m:18s remains)
INFO - root - 2017-12-15 09:03:07.347671: step 46930, loss = 0.20, batch loss = 0.17 (33.6 examples/sec; 0.238 sec/batch; 18h:54m:54s remains)
INFO - root - 2017-12-15 09:03:09.654236: step 46940, loss = 0.18, batch loss = 0.15 (34.7 examples/sec; 0.230 sec/batch; 18h:16m:24s remains)
INFO - root - 2017-12-15 09:03:11.995303: step 46950, loss = 0.26, batch loss = 0.22 (31.1 examples/sec; 0.257 sec/batch; 20h:23m:42s remains)
INFO - root - 2017-12-15 09:03:14.304312: step 46960, loss = 0.23, batch loss = 0.20 (35.0 examples/sec; 0.229 sec/batch; 18h:07m:27s remains)
INFO - root - 2017-12-15 09:03:16.577707: step 46970, loss = 0.36, batch loss = 0.33 (34.4 examples/sec; 0.233 sec/batch; 18h:27m:31s remains)
INFO - root - 2017-12-15 09:03:18.834982: step 46980, loss = 0.21, batch loss = 0.17 (36.0 examples/sec; 0.222 sec/batch; 17h:37m:42s remains)
INFO - root - 2017-12-15 09:03:21.127435: step 46990, loss = 0.19, batch loss = 0.15 (35.3 examples/sec; 0.227 sec/batch; 17h:59m:32s remains)
INFO - root - 2017-12-15 09:03:23.417369: step 47000, loss = 0.22, batch loss = 0.18 (34.7 examples/sec; 0.231 sec/batch; 18h:17m:11s remains)
2017-12-15 09:03:23.724974: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.1042919 -0.017730713 -1.6366365 -3.2910895 -4.3381028 -5.37264 -6.4696608 -6.366087 -4.8349638 -3.8034754 -3.9324746 -3.6787152 -3.8255689 -4.3876867 -5.3469129][-0.00044584274 -0.61311555 -2.533663 -4.404376 -5.2514391 -5.9633579 -6.9868922 -7.0782704 -5.7710233 -4.6632543 -4.4878607 -4.1861839 -4.2787294 -4.9544249 -6.01049][-2.1152287 -1.9969826 -3.6022141 -5.1503897 -5.6334724 -6.0444984 -6.7827883 -6.8613949 -5.8906183 -4.9314117 -4.6304455 -4.3994627 -4.4415593 -5.2164588 -6.3788991][-3.9273958 -3.0835097 -3.9450421 -4.9139128 -5.2825928 -5.6417317 -6.0158043 -5.9681873 -5.4279556 -4.8337851 -4.708384 -4.6659431 -4.5600805 -5.2663412 -6.5403166][-5.15703 -3.9130816 -4.2958064 -4.7586412 -5.0332131 -5.2584629 -5.0318332 -4.4803123 -3.9650578 -3.4069855 -3.402442 -3.5083129 -3.3065128 -4.113203 -5.6966105][-5.765707 -4.5590115 -4.5028381 -4.3928175 -4.4252138 -4.2962866 -3.300251 -2.0750625 -1.461078 -1.0148273 -1.4634955 -1.9277678 -1.8336353 -3.058619 -5.12323][-5.9900832 -4.9502287 -4.6190944 -4.04061 -3.5153971 -2.5329714 -0.66909993 1.1008623 1.7656479 1.9504433 0.86785746 -0.050291538 -0.21313477 -1.8439686 -4.2440877][-6.3439827 -5.33506 -4.9604683 -4.2064486 -3.1139638 -1.3212503 1.0789003 3.0880885 3.8012037 3.7715549 2.1309028 0.803545 0.46235204 -1.2226433 -3.5033972][-6.7244005 -5.8754187 -5.5992327 -4.9245977 -3.6980338 -1.6847496 0.84332156 3.0813785 4.003819 3.8417 1.8981433 0.41967177 -0.021806002 -1.7409531 -3.9270246][-7.1140857 -6.4556179 -6.3270431 -5.9047251 -4.8731632 -2.9239025 -0.49552488 1.7260778 2.6992359 2.2158828 0.19148064 -1.190751 -1.6827495 -3.2605999 -5.2327886][-7.4972196 -7.0905986 -7.1946125 -7.14608 -6.5466862 -5.0451875 -3.0073364 -1.0394534 -0.099157333 -0.724272 -2.3799608 -3.3493087 -3.7309728 -5.055202 -6.597506][-7.3293338 -7.1893477 -7.7003169 -8.11001 -7.963963 -7.0215106 -5.7018223 -4.3512239 -3.6636894 -4.3234491 -5.501688 -6.0046062 -6.1660738 -7.2275071 -8.3142481][-6.5187578 -6.5518212 -7.4633584 -8.1636925 -8.2750874 -7.737504 -6.9570222 -6.1000204 -5.5645285 -6.144763 -6.923316 -7.12571 -7.0830612 -7.7842584 -8.44899][-5.2211342 -5.1427331 -6.28049 -7.1540084 -7.3910155 -7.0145345 -6.5974431 -6.1586227 -5.7253 -6.0251651 -6.5148935 -6.7142482 -6.564219 -7.0180039 -7.4191971][-3.991847 -3.6123738 -4.8076634 -5.7984309 -6.1957703 -5.9550743 -5.7684116 -5.5133886 -5.0636206 -5.0428157 -5.3142982 -5.6741228 -5.714747 -6.2455435 -6.7412896]]...]
INFO - root - 2017-12-15 09:03:26.028642: step 47010, loss = 0.18, batch loss = 0.15 (35.3 examples/sec; 0.227 sec/batch; 17h:59m:50s remains)
INFO - root - 2017-12-15 09:03:28.325282: step 47020, loss = 0.22, batch loss = 0.19 (36.1 examples/sec; 0.222 sec/batch; 17h:34m:00s remains)
INFO - root - 2017-12-15 09:03:30.597514: step 47030, loss = 0.16, batch loss = 0.13 (36.1 examples/sec; 0.222 sec/batch; 17h:35m:16s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 09:03:32.897021: step 47040, loss = 0.22, batch loss = 0.19 (35.6 examples/sec; 0.225 sec/batch; 17h:48m:28s remains)
INFO - root - 2017-12-15 09:03:35.183383: step 47050, loss = 0.21, batch loss = 0.18 (34.6 examples/sec; 0.231 sec/batch; 18h:19m:44s remains)
INFO - root - 2017-12-15 09:03:37.536055: step 47060, loss = 0.23, batch loss = 0.20 (33.6 examples/sec; 0.238 sec/batch; 18h:51m:40s remains)
INFO - root - 2017-12-15 09:03:39.853978: step 47070, loss = 0.29, batch loss = 0.25 (33.5 examples/sec; 0.239 sec/batch; 18h:55m:44s remains)
INFO - root - 2017-12-15 09:03:42.158253: step 47080, loss = 0.26, batch loss = 0.22 (34.9 examples/sec; 0.229 sec/batch; 18h:09m:51s remains)
INFO - root - 2017-12-15 09:03:44.447934: step 47090, loss = 0.30, batch loss = 0.26 (34.1 examples/sec; 0.234 sec/batch; 18h:34m:30s remains)
INFO - root - 2017-12-15 09:03:46.738721: step 47100, loss = 0.24, batch loss = 0.21 (35.6 examples/sec; 0.225 sec/batch; 17h:48m:54s remains)
2017-12-15 09:03:47.029189: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0798442 -5.3183002 -5.6785669 -5.911901 -5.9998174 -6.0314784 -6.08185 -6.1493578 -6.1849804 -6.187623 -6.1443348 -6.04943 -5.9652305 -5.8322363 -5.5449591][-4.6985192 -6.2402639 -6.6101255 -6.7844105 -6.7204518 -6.5579481 -6.344399 -6.1759982 -6.1176643 -6.2534862 -6.508821 -6.7713609 -6.9294877 -6.9248524 -6.6653838][-6.3496804 -6.9778662 -7.1808538 -7.0992494 -6.6968122 -6.1127777 -5.425149 -4.8968716 -4.6927118 -4.98707 -5.7340589 -6.595408 -7.2337666 -7.5050492 -7.4040937][-7.1131573 -6.8803554 -6.8647442 -6.5731072 -5.9012051 -4.884346 -3.7005224 -2.7608566 -2.3719134 -2.7804449 -4.0785322 -5.6127748 -6.7977195 -7.4816051 -7.7015409][-6.5057464 -5.7727003 -5.6026878 -5.1889076 -4.316113 -2.9282765 -1.2401279 0.15008307 0.80396175 0.30567598 -1.6131923 -3.8142433 -5.5990887 -6.7737675 -7.3455591][-5.6655331 -4.5478759 -4.3911996 -3.97898 -2.9075239 -1.1207176 1.1367421 3.0852985 4.1249318 3.6278028 1.2019341 -1.5244234 -3.8486626 -5.5505838 -6.4952168][-4.165915 -3.4770088 -3.6542645 -3.4463317 -2.2240422 -0.11900496 2.581852 5.0270138 6.4795885 5.932786 3.0998983 0.14298439 -2.5228696 -4.5791249 -5.7905273][-3.0693688 -2.5000858 -3.139298 -3.21896 -2.01842 0.083634853 2.7469373 5.349649 6.9883156 6.3779316 3.4199643 0.40101075 -2.3139305 -4.41274 -5.5425367][-1.9389704 -1.6996346 -2.913491 -3.4930782 -2.6831856 -0.92309451 1.4361866 3.9609504 5.6730857 5.1460776 2.4810243 -0.25340295 -2.9229929 -4.9243574 -5.8901215][-1.9795697 -2.0494683 -3.5750685 -4.5159445 -4.1557865 -2.8555267 -0.9659425 1.1961582 2.7323718 2.3407993 0.1943388 -2.1681871 -4.4601059 -6.1057892 -6.7460136][-3.1952062 -3.3487926 -4.7395773 -5.7360082 -5.774231 -5.1195669 -3.909318 -2.4111266 -1.2630166 -1.5252423 -3.1092794 -4.9784455 -6.6662245 -7.6948004 -7.8189306][-4.5582314 -4.6702075 -5.75533 -6.6166887 -6.8679237 -6.59174 -5.9510283 -5.1225233 -4.4476056 -4.6929932 -5.8052263 -7.02174 -8.0007448 -8.4012642 -8.0449848][-5.5985069 -5.5658569 -6.2714338 -6.8468475 -7.0819187 -7.0388613 -6.7836003 -6.4268231 -6.0881991 -6.3010993 -7.0338097 -7.6774406 -7.9987454 -7.9393067 -7.4127989][-6.3927336 -6.1745787 -6.5951357 -6.9483819 -7.1219053 -7.1244698 -7.0262089 -6.7985172 -6.5402322 -6.6953435 -7.1366315 -7.4190784 -7.4043579 -7.0710878 -6.4460936][-7.0665617 -6.6386218 -6.8490353 -6.9639177 -6.9741325 -6.8930206 -6.8118691 -6.6298318 -6.4491196 -6.5781941 -6.7808571 -6.7368603 -6.4461379 -5.9177065 -5.2619781]]...]
INFO - root - 2017-12-15 09:03:49.302552: step 47110, loss = 0.15, batch loss = 0.12 (36.4 examples/sec; 0.219 sec/batch; 17h:23m:58s remains)
INFO - root - 2017-12-15 09:03:51.612599: step 47120, loss = 0.24, batch loss = 0.20 (35.2 examples/sec; 0.227 sec/batch; 18h:00m:57s remains)
INFO - root - 2017-12-15 09:03:53.943374: step 47130, loss = 0.28, batch loss = 0.24 (34.7 examples/sec; 0.231 sec/batch; 18h:17m:09s remains)
INFO - root - 2017-12-15 09:03:56.227307: step 47140, loss = 0.26, batch loss = 0.23 (35.6 examples/sec; 0.225 sec/batch; 17h:48m:10s remains)
INFO - root - 2017-12-15 09:03:58.516169: step 47150, loss = 0.33, batch loss = 0.30 (35.5 examples/sec; 0.225 sec/batch; 17h:51m:37s remains)
INFO - root - 2017-12-15 09:04:00.777820: step 47160, loss = 0.19, batch loss = 0.16 (34.9 examples/sec; 0.229 sec/batch; 18h:09m:39s remains)
INFO - root - 2017-12-15 09:04:03.066607: step 47170, loss = 0.22, batch loss = 0.18 (35.9 examples/sec; 0.223 sec/batch; 17h:41m:07s remains)
INFO - root - 2017-12-15 09:04:05.329913: step 47180, loss = 0.18, batch loss = 0.15 (35.9 examples/sec; 0.223 sec/batch; 17h:38m:48s remains)
INFO - root - 2017-12-15 09:04:07.611392: step 47190, loss = 0.34, batch loss = 0.31 (34.1 examples/sec; 0.235 sec/batch; 18h:35m:24s remains)
INFO - root - 2017-12-15 09:04:09.909157: step 47200, loss = 0.20, batch loss = 0.17 (35.2 examples/sec; 0.227 sec/batch; 17h:59m:23s remains)
2017-12-15 09:04:10.205764: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0998759 -6.1092515 -5.9975481 -6.1409421 -6.5611553 -6.8459606 -6.7032595 -6.4022007 -6.4454203 -6.7410469 -6.7799988 -6.5356178 -6.5156593 -6.3856764 -5.8591962][-7.9211206 -7.6127672 -7.3676062 -7.3059006 -7.4260912 -7.3984671 -7.1071053 -6.9818068 -7.3776646 -8.0590906 -8.4404755 -8.2651091 -8.1790628 -7.9187784 -7.0447683][-9.3970175 -7.6358309 -7.1246939 -6.7828989 -6.3989325 -5.879056 -5.3388076 -5.47114 -6.3392429 -7.561511 -8.4511108 -8.502059 -8.4984541 -8.3345394 -7.3493919][-9.5723228 -6.7149343 -5.9944057 -5.3807712 -4.5750771 -3.4486413 -2.4346714 -2.60499 -3.8071856 -5.6168003 -7.0240593 -7.3676195 -7.5886316 -7.7306538 -6.877903][-8.7674332 -5.187788 -4.2318397 -3.3010712 -2.2411349 -0.48083794 1.2397768 1.3401575 -0.21581912 -2.729073 -4.6528206 -5.3646479 -5.97069 -6.4724436 -5.8953915][-7.3776197 -3.5918839 -2.3634093 -1.0432764 0.38505793 2.6660349 4.9482975 5.4041109 3.585026 0.46483517 -1.9085447 -3.0860443 -4.1675124 -4.9904046 -4.795804][-6.3330355 -2.8175139 -1.3479831 0.27372885 1.9646881 4.5420294 7.0597963 7.7001982 5.6974812 2.2147067 -0.47834921 -2.0497427 -3.40169 -4.2636652 -4.25744][-6.7450848 -3.3327622 -1.8895645 -0.20473123 1.37391 3.7390091 6.0525684 6.6568546 4.7678995 1.4990489 -1.1444874 -2.7529404 -3.9752994 -4.5214243 -4.4798241][-7.9856005 -5.1151295 -3.9390502 -2.4895806 -1.1835079 0.68699813 2.5322726 3.1078274 1.7154045 -0.86240566 -3.1313729 -4.5199108 -5.4541397 -5.587173 -5.2900286][-9.7474613 -7.6629896 -6.8234386 -5.5777993 -4.4131794 -2.7970567 -1.2273313 -0.7033205 -1.6461203 -3.599122 -5.3183513 -6.335063 -6.9405594 -6.701438 -6.1029396][-10.546149 -9.2061157 -8.6781044 -7.6694145 -6.8452339 -5.6340294 -4.3232608 -3.7930493 -4.3978324 -5.8408551 -7.0076046 -7.6379538 -7.9521723 -7.47812 -6.6875877][-10.258915 -9.4009247 -9.1487217 -8.4890366 -7.9875 -7.0981236 -5.9508171 -5.4227619 -5.8922224 -6.9871082 -7.6544781 -8.0166607 -8.1873016 -7.6391039 -6.7936678][-9.3405037 -8.6522217 -8.5055084 -8.0472937 -7.7289085 -7.1303406 -6.2740221 -5.9019136 -6.3043022 -7.0955267 -7.4671259 -7.7224989 -7.7705975 -7.2153044 -6.4832163][-7.9265041 -7.1579113 -7.0140982 -6.6530914 -6.4448433 -6.133852 -5.635704 -5.4873219 -5.9127727 -6.5512371 -6.8072367 -6.9591942 -6.9051294 -6.467041 -5.9644222][-6.4725494 -5.6325021 -5.4829135 -5.259203 -5.1591721 -5.0573711 -4.9267645 -5.0277047 -5.4233541 -5.820466 -5.9626741 -6.0290318 -5.9868317 -5.7469692 -5.4743729]]...]
INFO - root - 2017-12-15 09:04:12.468690: step 47210, loss = 0.30, batch loss = 0.27 (35.5 examples/sec; 0.226 sec/batch; 17h:52m:55s remains)
INFO - root - 2017-12-15 09:04:14.752387: step 47220, loss = 0.19, batch loss = 0.16 (35.3 examples/sec; 0.227 sec/batch; 17h:57m:16s remains)
INFO - root - 2017-12-15 09:04:17.028568: step 47230, loss = 0.20, batch loss = 0.17 (35.0 examples/sec; 0.229 sec/batch; 18h:07m:47s remains)
INFO - root - 2017-12-15 09:04:19.333717: step 47240, loss = 0.18, batch loss = 0.15 (35.1 examples/sec; 0.228 sec/batch; 18h:02m:32s remains)
INFO - root - 2017-12-15 09:04:21.601206: step 47250, loss = 0.22, batch loss = 0.19 (36.0 examples/sec; 0.222 sec/batch; 17h:36m:50s remains)
INFO - root - 2017-12-15 09:04:23.888620: step 47260, loss = 0.31, batch loss = 0.28 (35.3 examples/sec; 0.226 sec/batch; 17h:56m:29s remains)
INFO - root - 2017-12-15 09:04:26.191967: step 47270, loss = 0.20, batch loss = 0.16 (36.5 examples/sec; 0.219 sec/batch; 17h:21m:56s remains)
INFO - root - 2017-12-15 09:04:28.476666: step 47280, loss = 0.32, batch loss = 0.29 (34.8 examples/sec; 0.230 sec/batch; 18h:12m:39s remains)
INFO - root - 2017-12-15 09:04:30.753455: step 47290, loss = 0.27, batch loss = 0.23 (33.5 examples/sec; 0.239 sec/batch; 18h:54m:25s remains)
INFO - root - 2017-12-15 09:04:33.039171: step 47300, loss = 0.32, batch loss = 0.29 (34.7 examples/sec; 0.231 sec/batch; 18h:17m:20s remains)
2017-12-15 09:04:33.332339: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1418648 -3.631799 -3.3289146 -3.2545819 -3.8175952 -4.4693346 -4.8056679 -4.9850473 -5.0094166 -4.5957856 -3.9334736 -3.2003081 -2.5223632 -1.9304497 -1.7513685][-2.6807139 -3.4660993 -3.1575024 -2.9012618 -3.5357747 -4.1991467 -4.5430689 -4.6376381 -4.5841932 -4.096611 -3.401351 -2.5058548 -1.7956518 -1.4206924 -1.3785975][-2.8786526 -3.0413837 -2.7219503 -2.337518 -3.0417838 -3.749 -4.0440769 -4.065834 -3.8013687 -3.1646292 -2.5854042 -1.9069729 -1.4399791 -1.3894525 -1.5924679][-2.9266 -2.5600572 -2.24361 -1.8341193 -2.4869869 -3.1407425 -3.353121 -3.3707147 -2.930162 -2.1732302 -1.7551408 -1.3434248 -1.1897013 -1.4268551 -1.8792734][-2.6379521 -2.0340149 -1.9621949 -1.6641905 -2.0586281 -2.3561063 -2.1683412 -1.9844061 -1.4614224 -0.87984288 -0.92872918 -0.93369234 -1.0903095 -1.5039082 -1.9977832][-2.5098302 -1.7937096 -1.8341117 -1.6537404 -1.7496705 -1.5698192 -0.7943778 -0.26135039 0.23436952 0.37059951 -0.25493193 -0.69874942 -1.0712494 -1.5741627 -1.9898124][-2.6002913 -2.0522685 -2.0133772 -1.6955793 -1.2855434 -0.391783 1.2172635 2.3232279 2.6627231 2.1294012 0.84534931 -0.099927425 -0.8082701 -1.4818649 -1.8605556][-3.3255422 -2.7257476 -2.4786916 -1.8528001 -0.87610221 0.80022311 3.1303391 4.6716037 4.7962618 3.5862207 1.6928403 0.27277923 -0.73898983 -1.366051 -1.5040615][-4.2697783 -3.5056839 -3.0579448 -2.1610904 -0.91652524 1.0332158 3.524281 5.0157928 4.9096036 3.4661188 1.4940362 -0.061859369 -1.030714 -1.3313891 -1.1928074][-5.1267548 -4.1563997 -3.6042266 -2.6116726 -1.3947804 0.44086814 2.6866632 3.8545136 3.5665736 2.0938182 0.30464721 -1.1032276 -1.8771963 -1.7968475 -1.3674911][-5.9285507 -4.7892289 -4.2682137 -3.3569674 -2.378129 -0.96419477 0.79231381 1.5983093 1.1313052 -0.27066541 -1.7919756 -2.8898067 -3.3285475 -2.977844 -2.4576406][-6.5926323 -5.389308 -4.9238753 -4.1952682 -3.5861704 -2.7565143 -1.6007729 -1.1969537 -1.8127737 -3.0946622 -4.1833134 -4.7359352 -4.7197185 -4.3271627 -3.9519072][-6.6175327 -5.5968556 -5.3265481 -4.9073257 -4.690804 -4.3753753 -3.7315896 -3.5087409 -4.1360445 -5.1896057 -5.7945428 -5.8137951 -5.5374441 -5.2991395 -5.1078205][-6.1899786 -5.4540887 -5.3877926 -5.2611847 -5.3754115 -5.4969773 -5.2586088 -5.1144476 -5.6669331 -6.4452958 -6.654737 -6.3960953 -6.0738811 -5.9768724 -5.8606734][-5.64118 -5.1449718 -5.2551594 -5.3542032 -5.68307 -6.0869637 -6.1775389 -6.1184549 -6.5239506 -7.04859 -7.0337605 -6.6632624 -6.318748 -6.1924343 -6.0079355]]...]
INFO - root - 2017-12-15 09:04:35.584625: step 47310, loss = 0.25, batch loss = 0.22 (35.3 examples/sec; 0.226 sec/batch; 17h:55m:45s remains)
INFO - root - 2017-12-15 09:04:37.889567: step 47320, loss = 0.26, batch loss = 0.23 (35.1 examples/sec; 0.228 sec/batch; 18h:04m:21s remains)
INFO - root - 2017-12-15 09:04:40.220509: step 47330, loss = 0.26, batch loss = 0.22 (34.4 examples/sec; 0.233 sec/batch; 18h:26m:44s remains)
INFO - root - 2017-12-15 09:04:42.534593: step 47340, loss = 0.17, batch loss = 0.14 (34.1 examples/sec; 0.235 sec/batch; 18h:36m:03s remains)
INFO - root - 2017-12-15 09:04:44.828835: step 47350, loss = 0.23, batch loss = 0.20 (35.6 examples/sec; 0.225 sec/batch; 17h:47m:11s remains)
INFO - root - 2017-12-15 09:04:47.135230: step 47360, loss = 0.27, batch loss = 0.24 (34.5 examples/sec; 0.232 sec/batch; 18h:20m:41s remains)
INFO - root - 2017-12-15 09:04:49.395042: step 47370, loss = 0.45, batch loss = 0.42 (35.6 examples/sec; 0.225 sec/batch; 17h:49m:08s remains)
INFO - root - 2017-12-15 09:04:51.704849: step 47380, loss = 0.21, batch loss = 0.18 (34.7 examples/sec; 0.231 sec/batch; 18h:15m:25s remains)
INFO - root - 2017-12-15 09:04:53.964631: step 47390, loss = 0.20, batch loss = 0.17 (34.9 examples/sec; 0.229 sec/batch; 18h:10m:19s remains)
INFO - root - 2017-12-15 09:04:56.238260: step 47400, loss = 0.20, batch loss = 0.17 (34.6 examples/sec; 0.231 sec/batch; 18h:17m:18s remains)
2017-12-15 09:04:56.524069: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.05492 -3.4845929 -3.7999661 -3.540391 -2.8896027 -2.3999996 -2.6378436 -3.4782214 -4.4180126 -4.5158558 -3.9486275 -3.3281131 -2.6924253 -1.911086 -1.0077801][-3.5448134 -3.1947184 -3.3499153 -2.7057381 -1.9981934 -1.689724 -1.7136757 -2.2587156 -2.956769 -2.7557104 -2.3956029 -2.3187637 -2.0509658 -1.2052767 -0.096904993][-3.5871387 -2.4247954 -2.4593933 -1.4874606 -0.91360772 -0.81163049 -0.6038878 -0.85153067 -1.3687376 -0.97103727 -0.82114732 -1.2891555 -1.4599991 -0.63664877 0.58742404][-2.821224 -1.6989956 -1.9453182 -0.74639165 -0.083658934 0.00031447411 0.49125719 0.36659741 -0.10181379 0.27468324 0.14859533 -0.76718116 -1.5463536 -0.989349 0.18230057][-2.9267588 -2.2294157 -2.52242 -0.92649472 0.25283957 0.89337325 1.9842041 1.9327333 1.1877928 0.98469257 0.22336793 -1.0130472 -2.3084414 -2.0973432 -0.98865378][-4.45676 -3.8494282 -3.7430127 -1.6441264 0.35442519 1.9564073 3.8899314 3.9514563 2.7228796 1.5467787 -0.19615459 -1.7396684 -3.3020468 -3.39228 -2.4372485][-5.4496946 -5.1687589 -4.6506515 -2.3446882 0.077664137 2.3726752 4.92017 5.1196413 3.5212834 1.3483262 -1.309036 -2.9501007 -4.4992852 -4.7152843 -3.9902675][-6.5909042 -5.9771309 -5.2705393 -3.2478409 -1.0131963 1.2382116 3.7886722 3.9422362 2.14562 -0.33050931 -3.187305 -4.4869852 -5.4511857 -5.4902124 -4.9637852][-7.6627941 -6.8286381 -6.1444168 -4.5635214 -2.6812546 -0.81406212 1.5487807 1.9399717 0.32232118 -1.9280146 -4.4167604 -5.3540974 -5.97847 -6.0787015 -5.6611657][-8.3242788 -7.043746 -6.3211174 -5.1058779 -3.6825829 -2.268837 -0.26273227 0.24462795 -1.1770877 -3.1039629 -5.1805158 -6.101768 -6.72101 -6.8534384 -6.238039][-7.9597816 -6.23718 -5.4677591 -4.4764948 -3.4782107 -2.7797177 -1.6818902 -1.5488122 -2.9286766 -4.5490637 -6.080667 -6.7207108 -7.077486 -7.1232004 -6.4426146][-7.4010248 -5.5460882 -4.8676615 -4.1269608 -3.4950428 -3.3103123 -2.8999677 -3.1348064 -4.3001781 -5.4218035 -6.2037373 -6.4096317 -6.4766293 -6.4802656 -6.0419073][-6.8149214 -5.0424194 -4.5177212 -3.8625937 -3.1422632 -2.8553867 -2.816978 -3.2614012 -4.1952353 -5.1257954 -5.6503611 -5.6293316 -5.4904537 -5.4233336 -5.258388][-5.676919 -4.0903549 -3.734056 -3.0356705 -2.1390402 -1.7027256 -1.8591082 -2.6343305 -3.7353842 -4.823318 -5.4103613 -5.33387 -4.9654517 -4.5663643 -4.1998267][-4.0580664 -2.7753322 -2.8682549 -2.4984896 -1.9014014 -1.451587 -1.6017493 -2.453582 -3.5858412 -4.6646538 -5.132319 -4.8683872 -4.3065233 -3.6508665 -3.1588871]]...]
INFO - root - 2017-12-15 09:04:58.819567: step 47410, loss = 0.35, batch loss = 0.32 (34.7 examples/sec; 0.231 sec/batch; 18h:15m:15s remains)
INFO - root - 2017-12-15 09:05:01.111193: step 47420, loss = 0.17, batch loss = 0.14 (34.4 examples/sec; 0.233 sec/batch; 18h:25m:48s remains)
INFO - root - 2017-12-15 09:05:03.373928: step 47430, loss = 0.23, batch loss = 0.20 (35.9 examples/sec; 0.223 sec/batch; 17h:38m:07s remains)
INFO - root - 2017-12-15 09:05:05.635401: step 47440, loss = 0.20, batch loss = 0.17 (32.9 examples/sec; 0.243 sec/batch; 19h:15m:16s remains)
INFO - root - 2017-12-15 09:05:07.911686: step 47450, loss = 0.23, batch loss = 0.20 (35.4 examples/sec; 0.226 sec/batch; 17h:54m:56s remains)
INFO - root - 2017-12-15 09:05:10.228171: step 47460, loss = 0.30, batch loss = 0.26 (35.1 examples/sec; 0.228 sec/batch; 18h:02m:49s remains)
INFO - root - 2017-12-15 09:05:12.520973: step 47470, loss = 0.24, batch loss = 0.21 (33.9 examples/sec; 0.236 sec/batch; 18h:40m:32s remains)
INFO - root - 2017-12-15 09:05:14.820332: step 47480, loss = 0.27, batch loss = 0.24 (34.3 examples/sec; 0.233 sec/batch; 18h:28m:06s remains)
INFO - root - 2017-12-15 09:05:17.104711: step 47490, loss = 0.22, batch loss = 0.18 (35.5 examples/sec; 0.225 sec/batch; 17h:50m:58s remains)
INFO - root - 2017-12-15 09:05:19.366098: step 47500, loss = 0.24, batch loss = 0.21 (36.3 examples/sec; 0.220 sec/batch; 17h:26m:01s remains)
2017-12-15 09:05:19.665574: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5380642 -6.4445772 -6.6047659 -5.7642922 -4.937192 -4.1163616 -4.18227 -4.4396582 -4.7083054 -5.0393472 -5.2766943 -5.5653038 -6.1727438 -6.7184076 -6.8429689][-4.1263113 -6.6852045 -6.9255886 -5.956697 -5.0523772 -4.1640911 -4.0831394 -4.2870321 -4.6675367 -5.2082634 -5.6347151 -6.1064987 -6.875905 -7.5105095 -7.6924276][-3.818723 -5.7767591 -6.182539 -5.2612295 -4.3256845 -3.3575134 -3.0981398 -3.2216685 -3.8098083 -4.7256527 -5.4310713 -6.1246109 -6.9383869 -7.5243912 -7.745759][-3.4119182 -4.6204453 -5.2299356 -4.3485317 -3.2339606 -2.0062551 -1.3987813 -1.434572 -2.431448 -3.8303285 -4.9377637 -5.8490734 -6.6618233 -7.1720581 -7.3998089][-3.2604518 -3.7068481 -4.4474535 -3.4001663 -1.8983192 -0.23794222 0.80212092 0.84802175 -0.61251843 -2.4968092 -4.0244355 -5.11747 -5.9078112 -6.384428 -6.6145744][-2.5799289 -2.7298973 -3.5301325 -2.2689261 -0.26829433 1.9069607 3.3777769 3.469775 1.6546063 -0.7999115 -2.8705368 -4.2584019 -5.1063 -5.6693945 -5.8619461][-2.1565192 -2.2765815 -2.9088235 -1.3778744 1.1772141 3.9062555 5.8147945 5.8732462 3.7432215 0.88016963 -1.6790277 -3.3473287 -4.3760519 -5.0642204 -5.2130303][-2.6407037 -2.4686303 -2.962492 -1.3782824 1.4123483 4.4016581 6.5109386 6.4868813 4.3355331 1.4534686 -1.2349461 -2.9425015 -3.9876471 -4.7504215 -4.9175053][-3.2427914 -2.7550528 -3.2611413 -2.0986655 0.30277276 3.0025995 4.836874 4.6197596 2.8349922 0.43833947 -1.8467371 -3.2368073 -4.0538759 -4.7797108 -4.9790335][-3.9208918 -3.2006719 -3.7428236 -3.2539029 -1.6993725 0.3106463 1.588563 1.2567282 0.12367153 -1.3101641 -2.7445917 -3.4857066 -3.9643359 -4.653204 -5.0581431][-4.6002164 -3.6378174 -4.2993402 -4.5288191 -3.9267111 -2.6082144 -1.8818774 -2.2523744 -2.7235653 -3.0968053 -3.4975367 -3.5679259 -3.7738483 -4.474453 -5.1451569][-4.8376904 -3.6631432 -4.3431063 -4.9951959 -5.0997267 -4.3834667 -4.2974577 -4.80593 -4.776875 -4.3701963 -4.0252595 -3.7298563 -3.8739908 -4.590065 -5.3661914][-4.6263108 -3.3967526 -3.8856411 -4.6026559 -5.0049334 -4.6754637 -5.06962 -5.5588579 -5.1081419 -4.210742 -3.5569253 -3.2516222 -3.6150665 -4.4204483 -5.2305183][-4.7004452 -3.4575605 -3.6923692 -4.2266479 -4.4941683 -4.2243347 -4.7304478 -4.9815831 -4.2583117 -3.3047528 -2.7358854 -2.7024262 -3.4651585 -4.3788519 -5.0705261][-5.4137774 -4.127717 -4.1213136 -4.3708744 -4.2519069 -3.8038425 -4.1827826 -4.2559633 -3.5661705 -2.8460724 -2.543819 -2.8787649 -4.0148411 -4.9671407 -5.4071255]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-47500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-47500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 09:05:22.277257: step 47510, loss = 0.28, batch loss = 0.25 (35.9 examples/sec; 0.223 sec/batch; 17h:39m:32s remains)
INFO - root - 2017-12-15 09:05:24.600419: step 47520, loss = 0.20, batch loss = 0.17 (34.3 examples/sec; 0.233 sec/batch; 18h:26m:23s remains)
INFO - root - 2017-12-15 09:05:26.870226: step 47530, loss = 0.22, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 17h:39m:48s remains)
INFO - root - 2017-12-15 09:05:29.152890: step 47540, loss = 0.35, batch loss = 0.32 (35.0 examples/sec; 0.229 sec/batch; 18h:05m:20s remains)
INFO - root - 2017-12-15 09:05:31.412733: step 47550, loss = 0.22, batch loss = 0.18 (35.9 examples/sec; 0.223 sec/batch; 17h:39m:10s remains)
INFO - root - 2017-12-15 09:05:33.685467: step 47560, loss = 0.22, batch loss = 0.19 (35.7 examples/sec; 0.224 sec/batch; 17h:43m:22s remains)
INFO - root - 2017-12-15 09:05:35.978324: step 47570, loss = 0.32, batch loss = 0.28 (34.7 examples/sec; 0.230 sec/batch; 18h:14m:09s remains)
INFO - root - 2017-12-15 09:05:38.245216: step 47580, loss = 0.37, batch loss = 0.34 (34.9 examples/sec; 0.229 sec/batch; 18h:08m:14s remains)
INFO - root - 2017-12-15 09:05:40.567273: step 47590, loss = 0.18, batch loss = 0.15 (35.3 examples/sec; 0.227 sec/batch; 17h:57m:14s remains)
INFO - root - 2017-12-15 09:05:42.885554: step 47600, loss = 0.19, batch loss = 0.16 (35.3 examples/sec; 0.227 sec/batch; 17h:57m:27s remains)
2017-12-15 09:05:43.190908: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9439857 -4.740438 -5.5579224 -6.1467018 -7.1299534 -7.5952044 -7.6508608 -7.3071146 -7.7427721 -7.5289612 -7.3386602 -7.6546736 -7.5374756 -7.6362352 -8.23668][-1.9282644 -4.5151424 -5.3511448 -5.981771 -7.0848761 -7.5872707 -7.45786 -6.8872252 -7.1604595 -7.0854683 -7.0496826 -7.275918 -7.0992985 -7.1755724 -7.6985025][-1.9252169 -3.8124995 -4.8170352 -5.5354 -6.5338993 -6.6335163 -5.9466763 -5.0185404 -5.1771321 -5.4827428 -5.99778 -6.2809381 -6.2154226 -6.3168306 -6.7559109][-2.2335689 -3.3634865 -4.4461536 -5.0041261 -5.49195 -4.820713 -3.3175406 -1.8852968 -1.9063632 -2.7474804 -4.1470542 -4.9049048 -5.2464657 -5.5175419 -5.8572874][-2.8757496 -3.2984707 -4.3433743 -4.3837194 -4.1011324 -2.42234 0.17450261 2.3025439 2.339987 0.70566654 -1.8192021 -3.4799051 -4.266221 -4.6693282 -4.8628073][-3.8046267 -4.0258913 -4.7868519 -3.9458556 -2.804112 -0.18643928 3.3793943 6.2593908 6.3723469 3.9025176 0.29990149 -2.5725071 -3.9459691 -4.524951 -4.489974][-4.742981 -5.367084 -5.7417545 -3.9840064 -2.0042973 1.2971735 5.5316772 9.0484562 9.2545891 6.154213 1.8106863 -2.1506467 -3.9324329 -4.4927006 -4.0953112][-5.840724 -6.96198 -7.1595154 -4.9391165 -2.397037 1.1706889 5.6345634 9.4425421 9.6240025 6.2187014 1.6511815 -2.621989 -4.2677755 -4.5951295 -3.8903484][-6.9704332 -8.4142151 -8.567358 -6.3036766 -3.5977583 -0.28741324 3.7750838 7.0858097 6.9725533 3.7497265 -0.42359293 -4.165894 -5.399332 -5.4664745 -4.5930781][-8.1891937 -9.5155172 -9.4295387 -7.3788795 -4.7789521 -2.1010401 0.96896553 3.2339461 2.8625538 0.29634953 -3.0466197 -5.8274474 -6.6994505 -6.5918837 -5.5431757][-9.2114811 -10.083426 -9.6966448 -8.0280056 -5.827323 -3.98382 -1.9609711 -0.56036627 -0.96672249 -2.7318199 -5.0532322 -6.7846842 -7.4396677 -7.315135 -6.2531476][-9.354352 -9.5914154 -8.9157276 -7.6985121 -6.1938763 -5.2697506 -4.1073704 -3.1890807 -3.4016361 -4.4773068 -5.8660536 -6.6859512 -7.3094244 -7.4816751 -6.7785387][-8.4298391 -8.0190573 -7.1561184 -6.2719917 -5.5151367 -5.431129 -4.9712048 -4.3099318 -4.1661363 -4.5686722 -5.0534835 -5.3546371 -6.0641279 -6.6253424 -6.4160976][-6.9472828 -6.2266965 -5.4267774 -4.8182087 -4.5790596 -4.9052963 -4.8723021 -4.3040085 -3.8648372 -3.84522 -3.6910124 -3.6515832 -4.3368335 -5.22573 -5.4176569][-5.7133427 -5.1487885 -4.6863551 -4.3132219 -4.2973518 -4.5835137 -4.574316 -4.0020137 -3.3449728 -3.119767 -2.6809154 -2.4049132 -2.8823094 -3.9153621 -4.4543362]]...]
INFO - root - 2017-12-15 09:05:45.467903: step 47610, loss = 0.30, batch loss = 0.26 (32.8 examples/sec; 0.244 sec/batch; 19h:18m:42s remains)
INFO - root - 2017-12-15 09:05:47.742848: step 47620, loss = 0.29, batch loss = 0.26 (35.4 examples/sec; 0.226 sec/batch; 17h:53m:50s remains)
INFO - root - 2017-12-15 09:05:50.025084: step 47630, loss = 0.26, batch loss = 0.23 (33.7 examples/sec; 0.238 sec/batch; 18h:47m:40s remains)
INFO - root - 2017-12-15 09:05:52.303431: step 47640, loss = 0.20, batch loss = 0.16 (35.3 examples/sec; 0.227 sec/batch; 17h:57m:26s remains)
INFO - root - 2017-12-15 09:05:54.579294: step 47650, loss = 0.23, batch loss = 0.20 (35.9 examples/sec; 0.223 sec/batch; 17h:37m:52s remains)
INFO - root - 2017-12-15 09:05:56.869287: step 47660, loss = 0.22, batch loss = 0.18 (34.9 examples/sec; 0.229 sec/batch; 18h:07m:22s remains)
INFO - root - 2017-12-15 09:05:59.172139: step 47670, loss = 0.27, batch loss = 0.24 (34.7 examples/sec; 0.230 sec/batch; 18h:13m:31s remains)
INFO - root - 2017-12-15 09:06:01.449130: step 47680, loss = 0.29, batch loss = 0.26 (33.7 examples/sec; 0.237 sec/batch; 18h:47m:16s remains)
INFO - root - 2017-12-15 09:06:03.706808: step 47690, loss = 0.29, batch loss = 0.26 (35.4 examples/sec; 0.226 sec/batch; 17h:54m:04s remains)
INFO - root - 2017-12-15 09:06:06.034384: step 47700, loss = 0.27, batch loss = 0.24 (34.3 examples/sec; 0.233 sec/batch; 18h:27m:53s remains)
2017-12-15 09:06:06.326283: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1760914 -2.6541476 -2.8029137 -2.6021738 -2.8136783 -3.1602888 -3.6488481 -4.4729357 -4.6935873 -3.9800031 -3.3947306 -2.9234297 -2.908185 -2.8465662 -2.466464][-1.6575732 -1.8980188 -2.0832045 -1.9143816 -2.0159504 -2.1721721 -2.5741975 -3.6373942 -4.4591985 -4.5443883 -4.7096863 -4.7281408 -4.7566919 -4.5454221 -3.969451][-1.8336098 -1.4097617 -1.7171383 -1.5697594 -1.39539 -1.1273866 -1.2265985 -2.2899644 -3.4781618 -4.2912397 -5.3935661 -6.0512609 -6.2475295 -6.0032225 -5.4801655][-1.2972118 -0.50285292 -1.0510335 -0.89677882 -0.29840541 0.49708843 0.92243814 0.13434243 -1.2151452 -2.6940515 -4.6602221 -5.99617 -6.469883 -6.3597107 -6.1029158][-1.1184719 -0.077337742 -0.925225 -0.73769951 0.461545 2.0549877 3.3219125 3.2501075 2.1856468 0.37117457 -2.2144177 -4.1486959 -4.9566183 -5.1298695 -5.2186575][-1.7959471 -0.93511987 -2.0713322 -1.7568861 -0.014155626 2.2399924 4.323863 5.1607571 4.6980085 2.9105952 0.0641942 -2.196166 -3.3764992 -3.8704402 -4.2725468][-2.3738363 -2.0676482 -3.3920243 -2.9449868 -0.96350527 1.3774049 3.6667483 5.1067829 5.2128267 3.600311 0.89765 -1.2836673 -2.6274362 -3.3088608 -3.8047948][-3.3775032 -3.3515525 -4.650918 -4.0482864 -2.0871961 -0.040096521 1.947973 3.6042926 4.0839958 2.6821887 0.32670498 -1.4525586 -2.6721847 -3.3692551 -3.7909694][-4.6846046 -4.9164133 -6.1806359 -5.6776772 -4.0594873 -2.4925272 -1.0066803 0.58442187 1.2508605 0.13969946 -1.7376907 -2.9222274 -3.8563519 -4.4669046 -4.7235012][-6.1697912 -6.5167618 -7.6071587 -7.3245182 -6.2030606 -5.20881 -4.2684755 -2.9506273 -2.2656314 -3.0149012 -4.2425904 -4.7814989 -5.3910494 -5.9247532 -6.0719509][-7.0176773 -7.1692171 -7.9172153 -7.8483505 -7.2345724 -6.7548113 -6.3950377 -5.6019278 -5.1131783 -5.5564423 -6.2100253 -6.3320675 -6.688796 -7.1581144 -7.2639713][-7.0794945 -6.9718943 -7.4246874 -7.5425386 -7.3890381 -7.3513079 -7.4831743 -7.2115488 -6.9100924 -7.07339 -7.328927 -7.1954346 -7.3121576 -7.6636343 -7.8012934][-6.6235552 -6.3484058 -6.6063251 -6.8149185 -6.9104462 -7.0869064 -7.3980312 -7.4263124 -7.2660551 -7.2629557 -7.2640238 -7.0228605 -7.0072069 -7.2524891 -7.4089737][-5.8323789 -5.3709574 -5.4470062 -5.5921888 -5.7162666 -5.9063921 -6.2360306 -6.4653683 -6.4967155 -6.4715118 -6.3818369 -6.214385 -6.1908092 -6.3335638 -6.4614248][-5.0029159 -4.4418821 -4.4497805 -4.526413 -4.6194696 -4.7550936 -5.035624 -5.3273134 -5.467175 -5.5092821 -5.4977255 -5.4643588 -5.4492807 -5.4913058 -5.5627]]...]
INFO - root - 2017-12-15 09:06:08.583319: step 47710, loss = 0.18, batch loss = 0.15 (35.1 examples/sec; 0.228 sec/batch; 18h:02m:32s remains)
INFO - root - 2017-12-15 09:06:10.854569: step 47720, loss = 0.29, batch loss = 0.26 (35.9 examples/sec; 0.223 sec/batch; 17h:36m:20s remains)
INFO - root - 2017-12-15 09:06:13.128945: step 47730, loss = 0.20, batch loss = 0.16 (35.1 examples/sec; 0.228 sec/batch; 18h:01m:29s remains)
INFO - root - 2017-12-15 09:06:15.403506: step 47740, loss = 0.32, batch loss = 0.28 (34.7 examples/sec; 0.231 sec/batch; 18h:15m:29s remains)
INFO - root - 2017-12-15 09:06:17.659464: step 47750, loss = 0.16, batch loss = 0.13 (36.3 examples/sec; 0.220 sec/batch; 17h:24m:44s remains)
INFO - root - 2017-12-15 09:06:19.933507: step 47760, loss = 0.27, batch loss = 0.24 (36.3 examples/sec; 0.221 sec/batch; 17h:27m:04s remains)
INFO - root - 2017-12-15 09:06:22.230247: step 47770, loss = 0.19, batch loss = 0.16 (32.9 examples/sec; 0.243 sec/batch; 19h:14m:26s remains)
INFO - root - 2017-12-15 09:06:24.509959: step 47780, loss = 0.21, batch loss = 0.18 (34.7 examples/sec; 0.230 sec/batch; 18h:12m:52s remains)
INFO - root - 2017-12-15 09:06:26.756884: step 47790, loss = 0.21, batch loss = 0.18 (34.8 examples/sec; 0.230 sec/batch; 18h:10m:47s remains)
INFO - root - 2017-12-15 09:06:29.023473: step 47800, loss = 0.29, batch loss = 0.26 (34.8 examples/sec; 0.230 sec/batch; 18h:12m:12s remains)
2017-12-15 09:06:29.295029: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9780865 -4.6284447 -3.613534 -3.3376687 -2.0966351 -0.8659364 -0.62081707 -1.5312281 -1.4416662 -0.43330634 -0.17040133 -0.81956947 -1.8628125 -2.4126987 -2.2216682][-3.5728555 -4.3007584 -3.3827925 -3.2803912 -2.3076148 -1.3272963 -1.1569617 -2.2298937 -2.5403342 -1.7797734 -1.579016 -1.7952662 -2.3978353 -2.8830774 -2.6532269][-4.1193342 -4.2283092 -3.4673762 -3.4742861 -2.656342 -1.8640604 -1.8079841 -3.043149 -3.6871672 -3.1889331 -3.0094049 -2.9897969 -3.2168725 -3.5138378 -3.1661158][-4.2004356 -4.0717244 -3.315434 -3.2644591 -2.4396887 -1.7364893 -1.6981891 -2.8971891 -3.8605895 -3.7681415 -3.7580421 -3.6317568 -3.5412092 -3.5445266 -3.0463653][-4.6247768 -3.5699425 -2.6609695 -2.386342 -1.3739145 -0.57575607 -0.45547616 -1.6407444 -2.8960383 -3.3657117 -3.6628778 -3.4714036 -3.349575 -3.4160967 -3.0264924][-4.8596568 -3.1028752 -2.0316072 -1.5355381 -0.31575871 0.68520617 0.94252563 -0.14874578 -1.5355544 -2.3293765 -2.7638059 -2.6015582 -2.5501418 -2.7550037 -2.5762861][-4.4608145 -2.8677261 -1.566309 -0.83749676 0.61470127 1.8344676 2.2453797 1.331383 -0.0084791183 -0.75201559 -1.0179223 -0.82765961 -0.76076555 -0.9982574 -1.0903842][-4.5361085 -2.8394067 -1.2985611 -0.44007659 1.0338614 2.3441408 2.8649957 2.1117609 0.9066329 0.42713094 0.51022696 0.88603878 1.0439274 0.83020067 0.56247592][-4.7174158 -3.00274 -1.3251903 -0.46296549 0.81393266 2.025573 2.6700265 2.0404398 0.99192929 0.80974507 1.2919989 1.8296826 2.0558226 1.9583175 1.8310039][-4.8851566 -3.2955506 -1.5871799 -0.80817842 0.18899226 1.1972485 1.8585069 1.281034 0.32729602 0.46099615 1.4192667 2.1260245 2.5088036 2.6265118 2.7743185][-5.0082316 -3.623451 -2.0231767 -1.4232404 -0.7085073 0.058106661 0.56363106 -0.17780209 -1.1170421 -0.70644522 0.56167531 1.3386843 1.7881281 2.1852639 2.6619527][-5.15858 -3.9670391 -2.5308535 -2.1002896 -1.5698199 -0.94622576 -0.62865019 -1.7410946 -2.824131 -2.3721833 -1.1747912 -0.54796743 -0.045898914 0.7986424 1.7982466][-5.2932539 -4.2693691 -2.9646685 -2.5964398 -2.0802343 -1.4585137 -1.2559186 -2.6238205 -3.8830357 -3.6860614 -2.765007 -2.3133996 -1.7573793 -0.63018334 0.62399578][-5.2523918 -4.3877058 -3.2047882 -2.8352716 -2.2556729 -1.5745296 -1.4306256 -2.8542776 -4.2468109 -4.3598976 -3.8167248 -3.63833 -3.210016 -2.1247981 -1.0044328][-5.0342536 -4.2859764 -3.2518134 -2.8991997 -2.3462534 -1.7130523 -1.6630749 -3.0727985 -4.4232774 -4.6671658 -4.3875141 -4.344851 -4.053782 -3.1340973 -2.232661]]...]
INFO - root - 2017-12-15 09:06:31.589687: step 47810, loss = 0.23, batch loss = 0.20 (35.5 examples/sec; 0.225 sec/batch; 17h:47m:46s remains)
INFO - root - 2017-12-15 09:06:33.850471: step 47820, loss = 0.22, batch loss = 0.19 (34.6 examples/sec; 0.231 sec/batch; 18h:16m:17s remains)
INFO - root - 2017-12-15 09:06:36.178793: step 47830, loss = 0.19, batch loss = 0.15 (32.7 examples/sec; 0.245 sec/batch; 19h:20m:40s remains)
INFO - root - 2017-12-15 09:06:38.442764: step 47840, loss = 0.23, batch loss = 0.20 (35.9 examples/sec; 0.223 sec/batch; 17h:38m:31s remains)
INFO - root - 2017-12-15 09:06:40.715986: step 47850, loss = 0.25, batch loss = 0.22 (35.7 examples/sec; 0.224 sec/batch; 17h:43m:59s remains)
INFO - root - 2017-12-15 09:06:42.973975: step 47860, loss = 0.17, batch loss = 0.13 (35.9 examples/sec; 0.223 sec/batch; 17h:37m:40s remains)
INFO - root - 2017-12-15 09:06:45.229482: step 47870, loss = 0.24, batch loss = 0.21 (35.9 examples/sec; 0.223 sec/batch; 17h:38m:22s remains)
INFO - root - 2017-12-15 09:06:47.530326: step 47880, loss = 0.20, batch loss = 0.17 (34.8 examples/sec; 0.230 sec/batch; 18h:10m:04s remains)
INFO - root - 2017-12-15 09:06:49.831380: step 47890, loss = 0.21, batch loss = 0.17 (36.2 examples/sec; 0.221 sec/batch; 17h:28m:46s remains)
INFO - root - 2017-12-15 09:06:52.108093: step 47900, loss = 0.27, batch loss = 0.23 (34.3 examples/sec; 0.233 sec/batch; 18h:26m:16s remains)
2017-12-15 09:06:52.396839: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7306528 -5.679966 -5.8501263 -6.3005157 -6.8215938 -7.1894364 -7.4904785 -7.72259 -7.7306728 -7.4931092 -7.2178068 -6.8427472 -6.278 -5.5462856 -5.003747][-6.6753578 -6.2839117 -6.263844 -6.6114736 -7.1913323 -7.5185575 -7.7375822 -7.9521523 -8.0204268 -7.7682796 -7.3455577 -6.6314754 -5.7095137 -4.68153 -3.9000978][-8.6466722 -6.7957492 -6.5147357 -6.58283 -7.0527058 -7.147119 -7.0906129 -7.16201 -7.3000593 -7.1560683 -6.6642404 -5.6113806 -4.4122596 -3.2527645 -2.32709][-9.8124142 -7.2740765 -6.97004 -6.7033372 -6.8540397 -6.459362 -5.8107896 -5.4553308 -5.510293 -5.5377588 -5.1289892 -3.9556208 -2.6940079 -1.592514 -0.667796][-10.107153 -7.6187987 -7.3156457 -6.6404362 -6.2746143 -5.3468428 -4.2199364 -3.5444198 -3.4788961 -3.589556 -3.3261423 -2.3818293 -1.4107578 -0.63314688 0.0781765][-9.79753 -7.4689445 -7.0641789 -5.8382549 -4.74754 -3.1950922 -1.6956129 -0.87521315 -0.8729744 -1.3181813 -1.5948713 -1.396069 -1.1177984 -0.95190191 -0.74112296][-8.7743874 -7.0678678 -6.5373659 -4.7663193 -2.9307761 -0.82170165 0.95566106 1.7707944 1.4672356 0.51142907 -0.44327533 -1.0809138 -1.4695773 -1.8627452 -2.2416937][-8.8987465 -6.8054423 -5.9979277 -3.740386 -1.4077997 0.99800277 2.8308015 3.4889984 2.7599974 1.3380947 -0.099679708 -1.3998971 -2.1720982 -2.8925374 -3.6547771][-9.0080509 -6.7493896 -5.778789 -3.3235245 -0.78334546 1.7066967 3.4805727 3.9204078 2.8530025 1.024682 -0.81721258 -2.460088 -3.3003926 -4.1065392 -4.9547777][-8.8477764 -6.4897728 -5.6401582 -3.3457391 -1.0509588 1.1294889 2.6399903 2.9903374 1.8592834 -0.076440096 -2.0080359 -3.695755 -4.5321474 -5.2581568 -5.9081855][-8.1786795 -5.8031139 -5.2217216 -3.4201331 -1.6871107 -0.10407233 1.0323861 1.1814752 0.13054085 -1.6178291 -3.3592706 -4.9298186 -5.6457253 -6.098599 -6.3867421][-7.0287275 -4.5920353 -4.4797773 -3.4552772 -2.4723148 -1.5956786 -0.87244606 -0.82723975 -1.7638941 -3.1854978 -4.5848665 -5.8017168 -6.3151207 -6.4619641 -6.4324708][-5.9466066 -3.3537455 -3.4872065 -3.0891039 -2.8502049 -2.6380093 -2.2375422 -2.3237522 -3.2110903 -4.3781409 -5.4273605 -6.2775836 -6.6961007 -6.7493496 -6.5732975][-5.5224247 -2.7539127 -2.9309826 -2.8216815 -3.0452714 -3.3272934 -3.1934183 -3.4120326 -4.3190985 -5.3596106 -6.0756073 -6.6834316 -7.0945888 -7.048171 -6.6195107][-5.5311742 -2.7510469 -2.8501055 -2.7724247 -3.0621564 -3.5953758 -3.6948967 -4.1795092 -5.1466026 -6.0847263 -6.6131191 -7.0466833 -7.3943152 -7.2240515 -6.6393819]]...]
INFO - root - 2017-12-15 09:06:54.649692: step 47910, loss = 0.19, batch loss = 0.16 (35.3 examples/sec; 0.226 sec/batch; 17h:53m:45s remains)
INFO - root - 2017-12-15 09:06:56.946892: step 47920, loss = 0.21, batch loss = 0.18 (34.9 examples/sec; 0.229 sec/batch; 18h:08m:01s remains)
INFO - root - 2017-12-15 09:06:59.238899: step 47930, loss = 0.22, batch loss = 0.19 (35.0 examples/sec; 0.228 sec/batch; 18h:03m:27s remains)
INFO - root - 2017-12-15 09:07:01.526393: step 47940, loss = 0.23, batch loss = 0.19 (33.7 examples/sec; 0.238 sec/batch; 18h:46m:56s remains)
INFO - root - 2017-12-15 09:07:03.821863: step 47950, loss = 0.24, batch loss = 0.21 (35.8 examples/sec; 0.224 sec/batch; 17h:40m:33s remains)
INFO - root - 2017-12-15 09:07:06.053176: step 47960, loss = 0.31, batch loss = 0.28 (36.0 examples/sec; 0.222 sec/batch; 17h:33m:06s remains)
INFO - root - 2017-12-15 09:07:08.338356: step 47970, loss = 0.18, batch loss = 0.15 (34.0 examples/sec; 0.236 sec/batch; 18h:36m:59s remains)
INFO - root - 2017-12-15 09:07:10.650794: step 47980, loss = 0.27, batch loss = 0.23 (34.6 examples/sec; 0.231 sec/batch; 18h:16m:40s remains)
INFO - root - 2017-12-15 09:07:12.939933: step 47990, loss = 0.21, batch loss = 0.18 (35.2 examples/sec; 0.227 sec/batch; 17h:57m:39s remains)
INFO - root - 2017-12-15 09:07:15.220711: step 48000, loss = 0.24, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 17h:32m:51s remains)
2017-12-15 09:07:15.513466: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.229933 -2.8987865 -2.6443846 -2.4952295 -2.4945073 -2.4419453 -2.0754337 -1.7323967 -1.6167397 -1.3661723 -0.617869 -0.375203 -0.65454364 -0.76880348 -1.1551319][-4.1762276 -3.5924134 -3.2018366 -2.8452427 -2.9452114 -2.9399652 -2.4847846 -1.90624 -1.5707518 -1.4542048 -0.72917223 -0.36084962 -0.602227 -0.7053597 -1.4259521][-5.2297297 -4.1961441 -3.8999436 -3.4029875 -3.59757 -3.5939045 -2.9982004 -2.1090808 -1.688887 -1.8443931 -1.3715718 -1.0014465 -1.1289501 -1.2471634 -2.1959927][-4.7840085 -3.8946671 -3.7947266 -3.3007965 -3.4771471 -3.3559966 -2.6550219 -1.8934003 -1.6594149 -2.3211458 -2.1091192 -1.6371428 -1.740398 -1.9373524 -2.6625144][-4.0258226 -2.96622 -2.8732164 -2.1639507 -2.0833015 -1.8382466 -1.2049685 -0.72308636 -1.0093282 -2.1846342 -2.2639186 -1.8170733 -1.8471019 -1.9296414 -2.3434577][-3.7682977 -2.2623317 -2.0014234 -0.89940345 -0.34686148 0.2647109 0.97615075 1.1714809 0.4280858 -1.0455588 -1.266974 -1.0358415 -1.1876935 -1.3298509 -1.6044238][-3.1080356 -1.986645 -1.4351628 0.040617228 1.1054282 2.0626917 2.7354684 2.7478704 2.0235281 0.7241323 0.47808218 0.388263 -0.073506594 -0.3594228 -0.51701796][-3.0229609 -1.7102023 -0.90599728 0.77196527 2.0215893 2.88663 3.2349167 3.0494537 2.5602226 1.7608385 1.6221957 1.3517623 0.6025486 0.11463118 0.15806985][-3.0007994 -1.5182209 -0.65141594 0.86170459 1.7205155 2.1323915 2.1306586 1.9458675 1.7926517 1.4436493 1.402771 1.0024908 0.36338353 -0.03813839 -0.23971677][-2.6882792 -1.1301125 -0.43751168 0.27184367 0.37380075 0.27273917 -0.0611012 -0.17421508 -0.12943625 -0.24148679 -0.19804907 -0.38811946 -0.93073082 -1.5256395 -2.1159267][-1.7604158 -0.12058353 0.20443892 -0.056567669 -0.69311297 -1.3211478 -1.9676157 -2.0023355 -1.7701018 -1.6390922 -1.4433099 -1.6937311 -2.4826596 -3.5202968 -4.2757354][-1.0829883 0.46966386 0.4556551 -0.60953462 -1.8614386 -2.8784046 -3.4580963 -3.1064124 -2.5367846 -2.1660903 -1.9256227 -2.3483098 -3.4027786 -4.5318956 -5.0406427][-1.448635 -0.086704016 -0.29682839 -1.7362167 -3.2870185 -4.1936231 -4.3219705 -3.4193094 -2.460912 -2.0565248 -1.9590518 -2.42785 -3.5002255 -4.44011 -4.6983585][-2.3429644 -1.1247729 -1.2785683 -2.5161498 -3.7570348 -4.3275719 -4.0075212 -2.7618029 -1.6884695 -1.5131567 -1.6692865 -2.0841353 -2.7709689 -3.5678282 -3.5594625][-3.0749123 -1.8375101 -1.7845546 -2.5073719 -3.2911515 -3.6205072 -3.0337739 -1.7900956 -0.89765882 -1.0680184 -1.3895228 -1.5737329 -1.726815 -2.1565802 -1.9578109]]...]
INFO - root - 2017-12-15 09:07:17.809534: step 48010, loss = 0.36, batch loss = 0.33 (34.8 examples/sec; 0.230 sec/batch; 18h:08m:39s remains)
INFO - root - 2017-12-15 09:07:20.098107: step 48020, loss = 0.43, batch loss = 0.40 (32.9 examples/sec; 0.243 sec/batch; 19h:14m:06s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 09:07:22.390283: step 48030, loss = 0.23, batch loss = 0.20 (35.7 examples/sec; 0.224 sec/batch; 17h:41m:57s remains)
INFO - root - 2017-12-15 09:07:24.676670: step 48040, loss = 0.16, batch loss = 0.13 (34.5 examples/sec; 0.232 sec/batch; 18h:19m:25s remains)
INFO - root - 2017-12-15 09:07:26.940645: step 48050, loss = 0.28, batch loss = 0.24 (34.9 examples/sec; 0.229 sec/batch; 18h:07m:28s remains)
INFO - root - 2017-12-15 09:07:29.214800: step 48060, loss = 0.27, batch loss = 0.24 (34.6 examples/sec; 0.231 sec/batch; 18h:16m:48s remains)
INFO - root - 2017-12-15 09:07:31.483472: step 48070, loss = 0.30, batch loss = 0.26 (36.0 examples/sec; 0.222 sec/batch; 17h:32m:43s remains)
INFO - root - 2017-12-15 09:07:33.742364: step 48080, loss = 0.24, batch loss = 0.20 (36.5 examples/sec; 0.219 sec/batch; 17h:20m:12s remains)
INFO - root - 2017-12-15 09:07:36.004710: step 48090, loss = 0.27, batch loss = 0.23 (35.7 examples/sec; 0.224 sec/batch; 17h:41m:58s remains)
INFO - root - 2017-12-15 09:07:38.257457: step 48100, loss = 0.23, batch loss = 0.20 (35.1 examples/sec; 0.228 sec/batch; 17h:59m:02s remains)
2017-12-15 09:07:38.547300: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6925364 -8.0799828 -9.3351 -9.1782312 -8.3243732 -7.7576537 -7.22022 -6.3432832 -6.0070553 -6.0371428 -6.1565733 -6.1582913 -5.9808097 -6.279778 -6.5850239][-6.7268958 -9.05724 -10.210928 -9.7761211 -8.8114185 -8.1308327 -7.3684225 -6.261065 -5.6592722 -5.5106354 -5.7881122 -5.8888221 -5.6781588 -5.8729105 -6.1862779][-7.7414484 -8.7525082 -9.45052 -8.5831566 -7.5026393 -6.6880064 -5.755352 -4.4921856 -3.7652049 -3.5692947 -4.1249733 -4.5537615 -4.5794783 -4.6338353 -4.8991661][-8.259716 -7.9718032 -8.1949987 -6.9004526 -5.6085234 -4.5978575 -3.5970478 -2.4228156 -1.7286577 -1.6029718 -2.3184359 -3.0193965 -3.3718052 -3.237498 -3.3521707][-8.6499462 -7.331677 -7.1782041 -5.6458082 -4.1351609 -2.852448 -1.7164825 -0.51376891 0.21316552 0.17449141 -0.53883886 -1.3784415 -2.0190718 -1.8732622 -1.9859688][-8.0081749 -6.2108469 -5.6879764 -4.088954 -2.5708489 -1.0860697 0.31199336 1.7545495 2.350975 1.8936877 1.0716434 0.083354 -0.84416509 -0.79709184 -1.0932865][-6.2750564 -4.247262 -3.3274946 -1.731117 -0.2341938 1.3397202 2.9390788 4.3129492 4.2815742 3.1248732 2.1250954 0.96941519 -0.12292385 -0.21132326 -0.67840219][-5.5478725 -3.0738146 -2.0364149 -0.39424729 1.2960763 3.0499554 4.6582122 5.6764417 4.9361286 3.1636419 1.9344969 0.80429626 -0.1567173 -0.35822189 -1.0229247][-5.4547949 -2.9083204 -1.9741665 -0.36242771 1.4070463 3.0322442 4.3290048 4.7815595 3.670907 1.7045071 0.62390351 -0.29143739 -1.1290547 -1.4117353 -2.2903857][-5.8288212 -3.4390583 -2.7197347 -1.3858988 0.22247958 1.6310987 2.5167871 2.5553007 1.3046298 -0.35691977 -1.07533 -1.6671222 -2.2309327 -2.4699953 -3.3032098][-6.5929203 -4.6478415 -4.2307873 -3.345614 -2.0517521 -0.78607893 -0.1489439 -0.37759781 -1.522934 -2.7704315 -3.1164722 -3.4756989 -3.7137327 -3.9550931 -4.6261806][-7.148809 -5.7780008 -5.7094269 -5.2926712 -4.418756 -3.3082938 -2.6736531 -2.8475182 -3.651654 -4.442482 -4.4828372 -4.5383782 -4.5307379 -4.8015618 -5.4364996][-7.7490396 -6.947854 -7.1423397 -7.0505853 -6.4883 -5.4035187 -4.6220808 -4.5842066 -5.1061215 -5.5605583 -5.3662987 -5.2426023 -4.9319038 -5.1220016 -5.70175][-8.0781775 -7.6625271 -7.9035625 -7.9068241 -7.5981402 -6.7116761 -6.062294 -5.9956369 -6.3304863 -6.3962235 -6.2105508 -6.179677 -5.8219543 -5.9733944 -6.3959837][-7.9765224 -7.7828612 -7.9817243 -7.9202409 -7.7322841 -7.1456957 -6.6382 -6.5389662 -6.6530852 -6.4830832 -6.3920307 -6.441061 -6.1951637 -6.3613248 -6.598722]]...]
INFO - root - 2017-12-15 09:07:40.822084: step 48110, loss = 0.20, batch loss = 0.16 (35.4 examples/sec; 0.226 sec/batch; 17h:51m:12s remains)
INFO - root - 2017-12-15 09:07:43.092478: step 48120, loss = 0.20, batch loss = 0.16 (35.0 examples/sec; 0.229 sec/batch; 18h:03m:36s remains)
INFO - root - 2017-12-15 09:07:45.373619: step 48130, loss = 0.25, batch loss = 0.22 (35.6 examples/sec; 0.224 sec/batch; 17h:43m:41s remains)
INFO - root - 2017-12-15 09:07:47.638191: step 48140, loss = 0.25, batch loss = 0.22 (35.5 examples/sec; 0.225 sec/batch; 17h:48m:11s remains)
INFO - root - 2017-12-15 09:07:49.917351: step 48150, loss = 0.35, batch loss = 0.31 (35.6 examples/sec; 0.225 sec/batch; 17h:45m:52s remains)
INFO - root - 2017-12-15 09:07:52.207894: step 48160, loss = 0.30, batch loss = 0.27 (36.1 examples/sec; 0.222 sec/batch; 17h:31m:11s remains)
INFO - root - 2017-12-15 09:07:54.499162: step 48170, loss = 0.24, batch loss = 0.21 (33.4 examples/sec; 0.239 sec/batch; 18h:53m:49s remains)
INFO - root - 2017-12-15 09:07:56.786840: step 48180, loss = 0.26, batch loss = 0.23 (34.2 examples/sec; 0.234 sec/batch; 18h:26m:56s remains)
INFO - root - 2017-12-15 09:07:59.093014: step 48190, loss = 0.27, batch loss = 0.24 (34.2 examples/sec; 0.234 sec/batch; 18h:28m:13s remains)
INFO - root - 2017-12-15 09:08:01.384063: step 48200, loss = 0.24, batch loss = 0.20 (35.1 examples/sec; 0.228 sec/batch; 17h:59m:00s remains)
2017-12-15 09:08:01.712212: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6234632 -5.8306909 -5.6322937 -5.327877 -5.0935144 -4.8420715 -4.3180552 -4.5525389 -5.2454395 -6.0238867 -6.9090877 -7.7983618 -7.5388284 -6.564559 -5.5641651][-4.8494153 -5.1508222 -4.924571 -4.3615494 -3.9668121 -3.580503 -2.7572327 -2.9645867 -3.7971554 -4.8143654 -6.0799923 -7.5602317 -7.5493841 -6.6259451 -5.747591][-4.7017555 -4.5208054 -4.226306 -3.4965572 -2.9324374 -2.4069579 -1.2859035 -1.3956611 -2.371809 -3.6332254 -5.2523203 -7.279624 -7.5979719 -6.8710127 -6.1403694][-4.2031546 -3.3941975 -3.0208688 -2.2548275 -1.6629462 -1.101077 0.18878961 0.063061476 -1.148151 -2.7373011 -4.6500463 -7.0558529 -7.7051058 -7.2042241 -6.6193652][-3.3772116 -2.2439046 -1.855459 -1.1224653 -0.48055911 0.12206888 1.6192534 1.5298269 0.091313839 -1.7310791 -3.800961 -6.4421339 -7.3801227 -7.16356 -6.7350931][-2.6764259 -1.3841352 -1.2165377 -0.61265552 0.026851416 0.73760009 2.5775709 2.726531 1.2929182 -0.53018856 -2.6186085 -5.4134283 -6.6625028 -6.7194166 -6.5165052][-1.9775214 -1.0037335 -0.90643513 -0.3137809 0.44118834 1.3810213 3.6287832 4.074719 2.6282382 0.77074409 -1.488446 -4.533823 -6.0409527 -6.4231644 -6.4588504][-2.1699719 -1.0238082 -0.86845243 -0.254488 0.56808734 1.6628494 4.2685924 5.1535773 3.8445725 2.0137615 -0.43581414 -3.6817639 -5.3821898 -5.9611635 -6.1842084][-2.8324165 -1.4508016 -1.1953613 -0.67698419 -0.1697824 0.73557019 3.352675 4.6407356 3.7311072 2.26233 -0.049030781 -3.2554033 -4.9146895 -5.485754 -5.7830305][-3.3608787 -1.8820235 -1.6409762 -1.4606731 -1.4529965 -0.83654642 1.6308155 3.1383557 2.619494 1.5475953 -0.53766024 -3.5589018 -5.1843176 -5.681798 -5.858562][-4.2946939 -2.5823255 -2.2309964 -2.2137992 -2.3654008 -1.8897957 0.22239256 1.645519 1.2546074 0.29891276 -1.7493176 -4.6576986 -6.2057748 -6.6586275 -6.598217][-5.1702633 -3.305476 -2.8171091 -2.7644513 -3.0043073 -2.8427382 -1.3580514 -0.29365289 -0.58761895 -1.3480937 -3.080188 -5.4701738 -6.6990628 -6.9985166 -6.7749152][-5.3233128 -3.6041393 -3.1564145 -3.1045966 -3.3394952 -3.4239182 -2.5815315 -2.0312991 -2.3481555 -3.0192885 -4.374053 -6.0246925 -6.7190032 -6.76931 -6.3195581][-5.2388468 -3.8228824 -3.4771385 -3.3668306 -3.4320164 -3.5322237 -3.1177938 -2.9488845 -3.23379 -3.7382169 -4.6586828 -5.6633739 -5.93134 -5.8424873 -5.3687582][-4.7639713 -3.5825162 -3.3214507 -3.1633823 -3.10186 -3.1647975 -3.1152496 -3.2234411 -3.4816332 -3.8626332 -4.4071426 -4.8900537 -4.9398346 -4.8783875 -4.5103569]]...]
INFO - root - 2017-12-15 09:08:03.986551: step 48210, loss = 0.16, batch loss = 0.13 (34.4 examples/sec; 0.232 sec/batch; 18h:20m:27s remains)
INFO - root - 2017-12-15 09:08:06.284574: step 48220, loss = 0.19, batch loss = 0.16 (34.8 examples/sec; 0.230 sec/batch; 18h:08m:46s remains)
INFO - root - 2017-12-15 09:08:08.557166: step 48230, loss = 0.17, batch loss = 0.14 (35.7 examples/sec; 0.224 sec/batch; 17h:41m:21s remains)
INFO - root - 2017-12-15 09:08:10.817281: step 48240, loss = 0.19, batch loss = 0.16 (32.9 examples/sec; 0.243 sec/batch; 19h:13m:13s remains)
INFO - root - 2017-12-15 09:08:13.093703: step 48250, loss = 0.24, batch loss = 0.21 (34.6 examples/sec; 0.231 sec/batch; 18h:14m:07s remains)
INFO - root - 2017-12-15 09:08:15.382956: step 48260, loss = 0.20, batch loss = 0.17 (35.3 examples/sec; 0.227 sec/batch; 17h:54m:44s remains)
INFO - root - 2017-12-15 09:08:17.662823: step 48270, loss = 0.24, batch loss = 0.20 (35.3 examples/sec; 0.227 sec/batch; 17h:54m:06s remains)
INFO - root - 2017-12-15 09:08:19.934816: step 48280, loss = 0.21, batch loss = 0.18 (34.9 examples/sec; 0.229 sec/batch; 18h:04m:36s remains)
INFO - root - 2017-12-15 09:08:22.202122: step 48290, loss = 0.32, batch loss = 0.28 (35.6 examples/sec; 0.225 sec/batch; 17h:44m:30s remains)
INFO - root - 2017-12-15 09:08:24.483225: step 48300, loss = 0.23, batch loss = 0.20 (34.1 examples/sec; 0.235 sec/batch; 18h:32m:09s remains)
2017-12-15 09:08:24.775399: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0081141 -3.7764974 -3.8906908 -3.1262896 -2.0407286 -1.019976 -0.51616609 -0.41270471 -1.0507033 -2.0784423 -2.7248902 -2.8401375 -3.7648501 -5.2604666 -6.4201717][-3.4798462 -3.5265121 -3.8521051 -3.5430853 -2.8330541 -1.93962 -1.1219051 -0.46033347 -0.74534786 -1.5537286 -1.8393493 -1.8078785 -2.9427104 -4.6035309 -6.141386][-4.3720675 -3.9019914 -4.2261114 -4.0280008 -3.4282327 -2.5173092 -1.3671746 -0.19370818 -0.32216728 -1.0967804 -1.3173578 -1.4286597 -2.7578228 -4.4714479 -6.1062288][-4.8695478 -3.8355384 -4.1176338 -3.8615994 -3.2321048 -2.3476236 -0.925508 0.50248122 0.34973788 -0.61311209 -1.0580126 -1.6351922 -3.3574886 -5.1390858 -6.7739296][-4.6778493 -2.8909748 -2.8854177 -2.537735 -1.9131954 -1.1550224 0.42564535 2.1011031 2.0117743 0.86901474 0.13932204 -1.0744357 -3.3672528 -5.3930531 -6.9558096][-4.2273517 -1.8354809 -1.4759557 -1.1048477 -0.3928839 0.29190826 2.0017345 3.8218706 3.7206714 2.3083651 1.2263477 -0.56371987 -3.2021618 -5.3193207 -6.85007][-3.1218631 -0.86652732 -0.27950835 0.059829712 0.83819413 1.3750246 3.1139958 5.0498419 5.0232859 3.3936565 2.0203226 -0.17108536 -2.98436 -5.173275 -6.7308044][-2.3183644 -0.10084939 0.44074917 0.65761352 1.4947014 1.9010856 3.5339105 5.4584532 5.4749031 3.777386 2.3631222 0.1035192 -2.6960716 -4.8905249 -6.5494585][-2.511791 -0.66105843 -0.30794835 -0.095824718 0.78582788 0.91860032 2.0685656 3.6869819 3.670718 2.2984536 1.3094451 -0.53853452 -2.8958821 -4.7736411 -6.3462648][-3.1678827 -1.8991914 -1.8403022 -1.579586 -0.61529505 -0.58977842 0.15750504 1.4688885 1.3950386 0.38399267 -0.20516276 -1.5258636 -3.3880336 -4.844522 -6.34095][-3.0002553 -2.147707 -2.2692721 -2.0307891 -1.2982643 -1.5999352 -1.1965314 -0.064159632 -0.036444426 -0.59087133 -0.91280997 -1.9354303 -3.493731 -4.8014727 -6.3598585][-2.4770019 -1.6435556 -1.8061296 -1.7858169 -1.5763445 -2.2647233 -2.139514 -1.1066909 -0.93941844 -1.1416303 -1.2055968 -1.9533191 -3.3101416 -4.6127834 -6.3444843][-2.3185444 -1.2424127 -1.2600842 -1.3676767 -1.5487211 -2.3993089 -2.2454255 -1.1170663 -0.72821355 -0.68523514 -0.65426993 -1.5087731 -2.9661171 -4.4611263 -6.4408951][-2.8007667 -1.4968259 -1.2614709 -1.2308232 -1.3781292 -1.9865446 -1.4992247 -0.057096481 0.44208264 0.38438964 0.10817909 -1.2086197 -2.9556634 -4.63818 -6.7642984][-3.6479859 -2.109977 -1.6263137 -1.4424913 -1.4877295 -1.849708 -1.0764215 0.519006 0.96311831 0.80912495 0.2311914 -1.5074358 -3.437851 -5.1440029 -7.1503716]]...]
INFO - root - 2017-12-15 09:08:27.070088: step 48310, loss = 0.25, batch loss = 0.22 (35.9 examples/sec; 0.223 sec/batch; 17h:36m:42s remains)
INFO - root - 2017-12-15 09:08:29.376711: step 48320, loss = 0.26, batch loss = 0.22 (34.8 examples/sec; 0.230 sec/batch; 18h:09m:36s remains)
INFO - root - 2017-12-15 09:08:31.677174: step 48330, loss = 0.22, batch loss = 0.18 (33.9 examples/sec; 0.236 sec/batch; 18h:37m:57s remains)
INFO - root - 2017-12-15 09:08:33.970220: step 48340, loss = 0.30, batch loss = 0.27 (34.5 examples/sec; 0.232 sec/batch; 18h:19m:03s remains)
INFO - root - 2017-12-15 09:08:36.253245: step 48350, loss = 0.25, batch loss = 0.22 (35.2 examples/sec; 0.227 sec/batch; 17h:55m:11s remains)
INFO - root - 2017-12-15 09:08:38.552471: step 48360, loss = 0.18, batch loss = 0.15 (35.7 examples/sec; 0.224 sec/batch; 17h:42m:16s remains)
INFO - root - 2017-12-15 09:08:40.820647: step 48370, loss = 0.19, batch loss = 0.15 (35.9 examples/sec; 0.223 sec/batch; 17h:34m:56s remains)
INFO - root - 2017-12-15 09:08:43.086704: step 48380, loss = 0.24, batch loss = 0.20 (34.5 examples/sec; 0.232 sec/batch; 18h:16m:48s remains)
INFO - root - 2017-12-15 09:08:45.356955: step 48390, loss = 0.21, batch loss = 0.18 (34.4 examples/sec; 0.233 sec/batch; 18h:21m:13s remains)
INFO - root - 2017-12-15 09:08:47.645398: step 48400, loss = 0.21, batch loss = 0.18 (34.4 examples/sec; 0.233 sec/batch; 18h:22m:36s remains)
2017-12-15 09:08:47.953113: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.9438411 -1.3321691 -1.8770542 -3.0663643 -3.716531 -3.5540092 -2.9366424 -2.4765863 -2.3846462 -3.0014691 -3.4367604 -3.8809369 -4.4874725 -5.1009192 -5.4291735][-1.995339 -1.4912244 -2.1219995 -3.3662002 -3.8826561 -3.5280547 -2.8923509 -2.2791898 -2.0825794 -2.7067664 -3.1088092 -3.4435196 -3.9755969 -4.5308123 -4.6072245][-3.2596745 -1.9753489 -2.6839333 -3.77699 -4.0324249 -3.4394004 -2.7793064 -1.9998944 -1.73822 -2.3904305 -2.80181 -3.1542535 -3.5477996 -3.7823019 -3.4080768][-3.8381648 -2.0898669 -2.8955121 -3.8236504 -3.7864242 -2.9327538 -2.152885 -1.252041 -1.0584909 -1.9038517 -2.6236935 -3.2718923 -3.6195374 -3.4139235 -2.4996223][-3.9142625 -1.8863459 -2.7170687 -3.3056347 -2.7633495 -1.5306926 -0.49219406 0.50350094 0.58164334 -0.56093574 -1.8423369 -3.1009667 -3.6176562 -3.239068 -1.9161993][-3.588794 -1.5605907 -2.236021 -2.3403544 -1.2223808 0.37754583 1.6805003 2.7938237 2.806057 1.3798714 -0.32549536 -1.9630589 -2.6151075 -2.2197332 -0.8146801][-3.3577523 -1.5568614 -2.04327 -1.7087927 -0.15169597 1.688514 3.1621609 4.3117275 4.3182406 2.7491593 0.97736955 -0.58057511 -1.1051295 -0.82389104 0.31986547][-3.9539123 -2.208035 -2.5852485 -2.0012555 -0.28895426 1.5060821 2.9859886 4.1303453 4.1293178 2.5095968 0.96202826 -0.079445839 -0.35859191 -0.250813 0.68282032][-5.4920034 -3.7770443 -3.9697943 -3.1462004 -1.4883943 -0.050280809 1.0572896 1.9332681 1.8082829 0.31881928 -0.6135602 -0.83231723 -0.72309208 -0.55867851 0.36907172][-7.2197919 -5.4301882 -5.379457 -4.5025997 -3.0607104 -1.9794166 -1.1955025 -0.65297043 -0.97358477 -2.1473818 -2.3228443 -1.8667772 -1.5605805 -1.3073645 -0.53495514][-8.0127306 -6.2406597 -6.2408714 -5.6335039 -4.4611712 -3.5618196 -2.8000445 -2.2197232 -2.318428 -2.9558911 -2.7714205 -2.3877954 -2.4814737 -2.4622488 -2.0092235][-8.143898 -6.3557291 -6.4104323 -6.1432409 -5.2818766 -4.6003475 -4.0597277 -3.4783168 -3.3516951 -3.584116 -3.360564 -3.329546 -3.7717843 -3.8938665 -3.5233161][-7.5850515 -5.6795244 -5.7447209 -5.7884989 -5.1644974 -4.68145 -4.4617319 -4.1398172 -4.1406684 -4.3977814 -4.4563446 -4.6740036 -4.9822092 -4.9443831 -4.6510324][-6.262 -4.1979446 -4.3932104 -4.8168783 -4.4520984 -3.975605 -3.7606578 -3.5714526 -3.7823644 -4.3641157 -5.0705791 -5.6595812 -5.8221865 -5.8037152 -5.8581877][-4.9021072 -2.5228581 -2.761219 -3.4189124 -3.3857927 -2.9833961 -2.7206042 -2.6028612 -3.0700111 -4.0733685 -5.4542618 -6.4078016 -6.3998528 -6.337738 -6.5026727]]...]
INFO - root - 2017-12-15 09:08:50.228670: step 48410, loss = 0.21, batch loss = 0.18 (36.3 examples/sec; 0.220 sec/batch; 17h:22m:11s remains)
INFO - root - 2017-12-15 09:08:52.512611: step 48420, loss = 0.18, batch loss = 0.15 (35.6 examples/sec; 0.225 sec/batch; 17h:43m:06s remains)
INFO - root - 2017-12-15 09:08:54.838472: step 48430, loss = 0.15, batch loss = 0.12 (34.5 examples/sec; 0.232 sec/batch; 18h:16m:16s remains)
INFO - root - 2017-12-15 09:08:57.126708: step 48440, loss = 0.22, batch loss = 0.19 (34.7 examples/sec; 0.231 sec/batch; 18h:11m:31s remains)
INFO - root - 2017-12-15 09:08:59.448315: step 48450, loss = 0.23, batch loss = 0.20 (33.6 examples/sec; 0.238 sec/batch; 18h:47m:35s remains)
INFO - root - 2017-12-15 09:09:01.745413: step 48460, loss = 0.19, batch loss = 0.16 (35.6 examples/sec; 0.225 sec/batch; 17h:44m:36s remains)
INFO - root - 2017-12-15 09:09:03.991993: step 48470, loss = 0.21, batch loss = 0.18 (35.8 examples/sec; 0.224 sec/batch; 17h:39m:02s remains)
INFO - root - 2017-12-15 09:09:06.270012: step 48480, loss = 0.31, batch loss = 0.28 (34.8 examples/sec; 0.230 sec/batch; 18h:08m:25s remains)
INFO - root - 2017-12-15 09:09:08.558213: step 48490, loss = 0.17, batch loss = 0.14 (35.8 examples/sec; 0.223 sec/batch; 17h:37m:40s remains)
INFO - root - 2017-12-15 09:09:10.828023: step 48500, loss = 0.25, batch loss = 0.22 (34.2 examples/sec; 0.234 sec/batch; 18h:28m:35s remains)
2017-12-15 09:09:11.117751: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.46180403 -1.6820142 -2.6886902 -4.0586205 -4.932785 -4.9118366 -4.1386895 -3.6782451 -3.4498796 -3.8476315 -4.2528267 -4.9941645 -5.3736115 -4.4936552 -3.3723454][-2.1990473 -2.7710576 -3.8896127 -5.36766 -6.3364983 -6.3182745 -5.5627108 -5.0906315 -4.94736 -5.2886219 -5.6946092 -6.2336245 -6.1404943 -5.0538664 -3.9727163][-4.0513678 -3.9161763 -5.2377691 -6.5000305 -7.3424282 -7.3099146 -6.4659281 -5.76699 -5.5433393 -6.0593395 -6.6918716 -7.0732355 -6.5486431 -5.2686 -4.2031531][-6.1900358 -5.4528685 -6.9008026 -7.8836079 -8.3731995 -8.2096586 -7.0738173 -5.8427291 -5.4186497 -5.9467554 -6.7188358 -7.0230608 -6.3411045 -5.0230517 -4.0459518][-7.1812649 -6.2897587 -7.4346256 -7.6715336 -7.2811289 -6.4239902 -4.7133074 -3.4648178 -3.52171 -4.8663969 -6.3361559 -6.6632404 -5.9323316 -4.7160587 -3.7438354][-6.9905291 -6.1588111 -6.9562416 -6.6081276 -5.4826813 -3.754046 -1.5627414 -0.42132831 -0.98395514 -3.0753713 -5.1133914 -5.5212278 -4.6956906 -3.7169366 -3.0174708][-5.719842 -5.4138985 -6.0936127 -5.2944226 -3.4200268 -0.74517107 2.0081947 3.3884704 2.7102039 -0.0094013214 -2.9183819 -3.8191407 -3.2204337 -2.54608 -2.2049994][-5.3486485 -4.6304836 -4.9228573 -3.3789902 -0.63126028 2.6877105 5.5817366 6.752121 5.6404934 2.4837573 -0.98202229 -2.4312673 -2.2780089 -1.9600657 -1.7935675][-5.6501694 -4.4575195 -4.4248705 -2.6372445 -0.015914917 3.0023324 5.5936089 6.4276924 5.5766363 2.9552958 -0.098236084 -1.5883818 -1.6496515 -1.814688 -2.0025611][-5.8389864 -4.3123989 -4.3256559 -2.9393091 -0.92089057 1.8951433 4.5173903 5.4727535 4.8549337 2.5959117 -0.021567345 -1.5252552 -1.7205951 -2.2812779 -2.7323897][-6.2417469 -4.4657116 -4.6540284 -3.74491 -2.1610365 0.44773936 2.8519804 3.6672356 2.8643029 0.80104804 -1.2661926 -2.4113908 -2.6268208 -3.2313232 -3.5949974][-7.15123 -5.4740248 -5.8793793 -5.4098854 -4.2325816 -2.2965047 -0.60889661 -0.066468716 -0.7986151 -2.2211905 -3.2731667 -3.619087 -3.5300283 -3.795176 -3.7850566][-7.7101259 -6.0474353 -6.5170383 -6.3646889 -5.43962 -4.0909228 -3.0421538 -2.7242246 -3.4696898 -4.4848237 -4.9669876 -4.8367186 -4.5047951 -4.4686041 -4.0381923][-8.3293114 -6.3836622 -6.5898352 -6.4442654 -5.64423 -4.6029654 -3.8676689 -3.7329998 -4.6217079 -5.7316256 -6.1284447 -5.6775022 -5.0264649 -4.6378622 -4.0155029][-7.6387215 -5.6401758 -5.8645325 -6.04247 -5.469861 -4.5491343 -4.0688567 -4.2787213 -5.4128132 -6.6884165 -6.9302874 -5.9965119 -4.96789 -4.3553867 -3.7323055]]...]
INFO - root - 2017-12-15 09:09:13.368941: step 48510, loss = 0.16, batch loss = 0.13 (35.9 examples/sec; 0.223 sec/batch; 17h:35m:02s remains)
INFO - root - 2017-12-15 09:09:15.640550: step 48520, loss = 0.35, batch loss = 0.32 (34.0 examples/sec; 0.235 sec/batch; 18h:33m:51s remains)
INFO - root - 2017-12-15 09:09:18.082151: step 48530, loss = 0.27, batch loss = 0.24 (20.2 examples/sec; 0.395 sec/batch; 31h:11m:05s remains)
INFO - root - 2017-12-15 09:09:20.348675: step 48540, loss = 0.23, batch loss = 0.20 (35.3 examples/sec; 0.226 sec/batch; 17h:51m:46s remains)
INFO - root - 2017-12-15 09:09:22.603860: step 48550, loss = 0.22, batch loss = 0.19 (35.2 examples/sec; 0.227 sec/batch; 17h:54m:42s remains)
INFO - root - 2017-12-15 09:09:24.878605: step 48560, loss = 0.26, batch loss = 0.23 (34.6 examples/sec; 0.231 sec/batch; 18h:15m:07s remains)
INFO - root - 2017-12-15 09:09:27.143734: step 48570, loss = 0.33, batch loss = 0.30 (34.8 examples/sec; 0.230 sec/batch; 18h:06m:19s remains)
INFO - root - 2017-12-15 09:09:29.429448: step 48580, loss = 0.18, batch loss = 0.15 (34.7 examples/sec; 0.231 sec/batch; 18h:11m:40s remains)
INFO - root - 2017-12-15 09:09:31.716732: step 48590, loss = 0.19, batch loss = 0.15 (35.7 examples/sec; 0.224 sec/batch; 17h:40m:06s remains)
INFO - root - 2017-12-15 09:09:34.006897: step 48600, loss = 0.22, batch loss = 0.19 (35.1 examples/sec; 0.228 sec/batch; 17h:56m:58s remains)
2017-12-15 09:09:34.320134: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9054399 -4.0100889 -3.9195664 -3.8988521 -3.8614841 -3.8603354 -3.7861805 -4.2154469 -4.2681665 -3.8888528 -3.9874451 -4.0188141 -3.4011426 -3.2253766 -3.6961436][-2.8969569 -4.27056 -4.2082291 -4.2141304 -4.1512127 -4.0579643 -3.9036436 -4.1924238 -4.1347809 -3.6963532 -3.7195742 -3.5932481 -3.0387969 -3.0975966 -3.7510614][-3.7064252 -4.068347 -4.1067939 -4.1881275 -4.1111684 -3.8252888 -3.4648581 -3.4895897 -3.3137124 -2.927773 -3.0047953 -2.8422322 -2.5034366 -2.7740083 -3.5216503][-4.2316694 -3.91777 -3.9415712 -3.9236429 -3.6929493 -3.0199568 -2.3669953 -2.1171389 -2.0081325 -1.8096166 -2.0700531 -2.0338542 -2.0752838 -2.4713001 -3.2467396][-4.5533218 -3.8955874 -3.6535826 -3.2158027 -2.5554838 -1.2289523 -0.099238157 0.466928 0.35968709 0.068433762 -0.60634661 -1.047093 -1.7302926 -2.2291172 -3.0279112][-4.4093552 -3.5012772 -2.8039749 -1.824055 -0.70145595 1.2916453 2.8258822 3.4709942 2.8190434 1.718832 0.54664946 -0.5477457 -1.8836359 -2.3980064 -3.2174613][-3.551538 -2.9952478 -1.9383192 -0.68749118 0.65428042 3.1210654 4.9670658 5.6382942 4.4442654 2.5017216 0.94618869 -0.7059921 -2.6075585 -3.1044507 -3.898411][-3.8854046 -3.1021161 -1.8631852 -0.49428511 0.84759974 3.3804772 5.3136244 6.0644226 4.5985823 2.0140073 0.36162114 -1.4161314 -3.4005694 -3.8019192 -4.4607973][-4.5596886 -3.6637855 -2.4202235 -1.2645627 -0.29403543 1.8011076 3.5412209 4.3800993 2.9556553 0.23245668 -1.2135495 -2.7454691 -4.5271935 -4.9490252 -5.3035645][-4.7300758 -3.8880105 -2.8707504 -2.2407968 -1.9386683 -0.62751269 0.67097497 1.4421375 0.13776183 -2.3853054 -3.6873765 -4.8865871 -6.21344 -6.4344683 -6.3310127][-4.7120419 -3.9472985 -3.2244701 -2.925837 -3.2884388 -2.966531 -2.315587 -1.755726 -2.699733 -4.6451521 -5.7603459 -6.6198387 -7.3749285 -7.29844 -6.7897606][-4.3333817 -3.4561028 -3.0444081 -2.9784484 -3.7827094 -4.1567388 -4.1242046 -3.9071727 -4.5524807 -5.8772488 -6.894824 -7.6283855 -7.9463844 -7.544054 -6.6976414][-2.9264982 -1.9303604 -1.8724705 -2.1501129 -3.5393777 -4.5208969 -5.0308213 -5.1156139 -5.3853936 -5.9909468 -6.9004345 -7.6526756 -7.7568188 -7.02367 -6.0741415][-2.2657433 -1.3253531 -1.7586602 -2.2215512 -3.8064022 -4.8456469 -5.3143373 -5.1817989 -4.8326864 -4.8499107 -5.670785 -6.5744858 -6.8076611 -6.0939126 -5.2042408][-1.7899214 -0.81563139 -1.6085756 -2.0796843 -3.4776726 -4.4131832 -4.7885828 -4.5051975 -3.9130106 -3.7809706 -4.5644841 -5.5172706 -5.8914824 -5.2840562 -4.4524045]]...]
INFO - root - 2017-12-15 09:09:36.564395: step 48610, loss = 0.26, batch loss = 0.23 (35.0 examples/sec; 0.229 sec/batch; 18h:01m:27s remains)
INFO - root - 2017-12-15 09:09:38.841309: step 48620, loss = 0.20, batch loss = 0.17 (34.7 examples/sec; 0.231 sec/batch; 18h:11m:42s remains)
INFO - root - 2017-12-15 09:09:41.105618: step 48630, loss = 0.20, batch loss = 0.17 (36.5 examples/sec; 0.219 sec/batch; 17h:16m:26s remains)
INFO - root - 2017-12-15 09:09:43.400543: step 48640, loss = 0.29, batch loss = 0.26 (35.4 examples/sec; 0.226 sec/batch; 17h:48m:26s remains)
INFO - root - 2017-12-15 09:09:45.654565: step 48650, loss = 0.27, batch loss = 0.24 (36.3 examples/sec; 0.221 sec/batch; 17h:23m:47s remains)
INFO - root - 2017-12-15 09:09:47.919849: step 48660, loss = 0.26, batch loss = 0.23 (36.1 examples/sec; 0.221 sec/batch; 17h:27m:16s remains)
INFO - root - 2017-12-15 09:09:50.208066: step 48670, loss = 0.17, batch loss = 0.14 (35.1 examples/sec; 0.228 sec/batch; 17h:57m:59s remains)
INFO - root - 2017-12-15 09:09:52.467059: step 48680, loss = 0.35, batch loss = 0.32 (34.7 examples/sec; 0.230 sec/batch; 18h:10m:15s remains)
INFO - root - 2017-12-15 09:09:54.755118: step 48690, loss = 0.15, batch loss = 0.12 (35.9 examples/sec; 0.223 sec/batch; 17h:35m:31s remains)
INFO - root - 2017-12-15 09:09:57.022874: step 48700, loss = 0.22, batch loss = 0.19 (35.0 examples/sec; 0.229 sec/batch; 18h:01m:07s remains)
2017-12-15 09:09:57.325020: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7089095 -3.1964631 -2.8223786 -2.4960618 -2.0732186 -1.9803218 -1.8039618 -2.4475148 -3.5282731 -4.1357284 -4.1988511 -3.4409485 -2.3132224 -1.1090127 -0.020737886][-3.6238391 -3.2293222 -3.2913568 -3.1314585 -2.5809789 -2.3471441 -1.982515 -2.3987474 -3.255532 -3.7366824 -3.6277878 -2.7076797 -1.7734866 -1.1832793 -0.72953165][-2.9585583 -2.5614321 -3.0089631 -3.0250757 -2.5083115 -2.3156257 -1.9587039 -2.2196374 -2.8982015 -3.3245816 -3.1127744 -2.2634337 -1.6937294 -1.7478753 -1.8878951][-1.9330879 -1.5992072 -2.2378247 -2.4139907 -2.084475 -1.9684864 -1.8571304 -2.0980082 -2.6334786 -2.9274061 -2.6995802 -2.0055418 -1.7048032 -2.1631508 -2.6456621][-1.1155823 -0.7789191 -1.5128138 -1.536376 -1.0285012 -0.64258218 -0.65444839 -0.93578672 -1.4402388 -1.8720355 -2.0171366 -1.7803315 -1.7990057 -2.3952422 -2.8274503][-0.52052355 -0.36375475 -1.1380357 -0.79923522 0.17168975 1.1882863 1.4725742 1.3145459 0.76670647 -0.091170073 -1.0144044 -1.5307052 -1.8759904 -2.2774706 -2.2655668][-0.04033041 -0.28080928 -1.1517521 -0.57661784 0.7238462 2.1943984 2.761107 2.6588683 2.0998874 0.96092319 -0.51992834 -1.3818753 -1.7003807 -1.5535403 -0.91337609][-0.36134517 -0.56628156 -1.5397835 -0.93736267 0.34356523 1.9327445 2.7098451 2.7450023 2.3649445 1.2876065 -0.33737123 -1.3024042 -1.5803814 -1.1213218 -0.17737222][-0.96833038 -1.0340626 -2.1294875 -1.6764411 -0.6206516 0.83982873 1.8329258 2.1834421 2.092876 1.1838794 -0.38764989 -1.4472675 -1.8535041 -1.3680055 -0.43844426][-1.8075181 -1.7222295 -2.9003921 -2.7509971 -2.0466726 -0.81867421 0.31335735 0.99694729 1.2263892 0.57354331 -0.770445 -1.8475323 -2.3840539 -1.9498036 -1.1642983][-3.1787009 -2.8015451 -3.8655491 -3.9219768 -3.4812069 -2.4686198 -1.3059404 -0.32896197 0.11795163 -0.34970915 -1.4889936 -2.532686 -3.146152 -2.9319174 -2.3490942][-4.7819066 -3.9372854 -4.6162586 -4.6375 -4.22145 -3.2569437 -2.1486888 -1.217461 -0.9083848 -1.4876941 -2.5940082 -3.6312723 -4.3247862 -4.3178163 -3.8370218][-6.133893 -4.9593182 -5.3247433 -5.3144279 -4.9189353 -4.0941544 -3.199203 -2.473906 -2.3143532 -3.0014653 -4.0123773 -4.75515 -5.11703 -4.8933172 -4.2838063][-6.6381183 -5.3354478 -5.526825 -5.5526986 -5.2575703 -4.62885 -3.9601936 -3.4345698 -3.3516645 -4.0319529 -4.8440552 -5.1921387 -5.1091843 -4.6479549 -3.9889123][-6.4027977 -5.01379 -5.0007038 -4.9374714 -4.7036896 -4.2699065 -3.8350754 -3.545825 -3.5832503 -4.2360382 -4.9408321 -5.1160059 -4.8054771 -4.2811575 -3.6868954]]...]
INFO - root - 2017-12-15 09:09:59.601113: step 48710, loss = 0.24, batch loss = 0.21 (35.6 examples/sec; 0.224 sec/batch; 17h:41m:28s remains)
INFO - root - 2017-12-15 09:10:01.862720: step 48720, loss = 0.23, batch loss = 0.19 (36.3 examples/sec; 0.220 sec/batch; 17h:22m:19s remains)
INFO - root - 2017-12-15 09:10:04.146715: step 48730, loss = 0.32, batch loss = 0.29 (36.0 examples/sec; 0.222 sec/batch; 17h:31m:08s remains)
INFO - root - 2017-12-15 09:10:06.404693: step 48740, loss = 0.21, batch loss = 0.18 (35.1 examples/sec; 0.228 sec/batch; 17h:58m:48s remains)
INFO - root - 2017-12-15 09:10:08.703658: step 48750, loss = 0.18, batch loss = 0.15 (34.4 examples/sec; 0.232 sec/batch; 18h:18m:12s remains)
INFO - root - 2017-12-15 09:10:10.995494: step 48760, loss = 0.30, batch loss = 0.26 (32.4 examples/sec; 0.247 sec/batch; 19h:28m:27s remains)
INFO - root - 2017-12-15 09:10:13.266499: step 48770, loss = 0.24, batch loss = 0.21 (35.6 examples/sec; 0.225 sec/batch; 17h:42m:03s remains)
INFO - root - 2017-12-15 09:10:15.534445: step 48780, loss = 0.19, batch loss = 0.16 (35.1 examples/sec; 0.228 sec/batch; 17h:58m:39s remains)
INFO - root - 2017-12-15 09:10:17.792251: step 48790, loss = 0.30, batch loss = 0.27 (34.1 examples/sec; 0.235 sec/batch; 18h:30m:53s remains)
INFO - root - 2017-12-15 09:10:20.084220: step 48800, loss = 0.20, batch loss = 0.17 (35.1 examples/sec; 0.228 sec/batch; 17h:57m:02s remains)
2017-12-15 09:10:20.377126: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5674012 -6.5197573 -7.4674006 -8.19383 -7.9391251 -7.2508907 -6.406641 -4.7234659 -3.1494746 -1.7664149 -1.5014076 -2.0092845 -3.0692222 -4.3280468 -6.0077715][-3.6269879 -6.5670252 -7.802639 -8.5698061 -8.1116428 -7.4241676 -6.9027033 -5.7754726 -4.7198362 -4.1831856 -4.2557368 -4.8630686 -5.7313461 -6.7828379 -7.7838984][-5.0389524 -6.6346245 -7.7457771 -8.2368927 -7.4263239 -6.8001127 -6.3937378 -5.5873852 -5.1087008 -5.5112257 -6.1216702 -6.7702 -7.415946 -8.21889 -8.7110806][-6.1996222 -6.5619574 -7.2701855 -7.2253389 -5.8484907 -4.9132562 -4.1860666 -3.3940852 -3.4679642 -4.6612005 -5.8713655 -6.8752127 -7.7646503 -8.4444427 -8.6419249][-7.203084 -6.4185772 -6.6422834 -5.9974413 -3.943964 -2.43152 -1.1107501 0.070128679 -0.26657212 -2.099503 -3.9822111 -5.5672369 -6.7938166 -7.5948925 -7.7416525][-7.3763304 -6.0605359 -5.8451805 -4.7219992 -2.1276443 -0.011692524 1.8896105 3.4403193 3.0204933 1.0176189 -1.3068315 -3.876853 -5.4896469 -6.4396887 -6.609086][-6.7577085 -5.6179647 -5.198247 -3.7574496 -0.76812613 1.8256915 4.2166052 5.9118233 5.3702993 3.3228424 0.640491 -2.6980908 -4.6814041 -5.6944895 -5.7317419][-6.6888266 -5.5076666 -5.1622534 -3.6327434 -0.43034804 2.2442081 4.7814846 6.3401861 5.6454449 3.4902866 0.61848712 -2.9403338 -4.7630911 -5.6230087 -5.3850183][-6.4831772 -5.5192862 -5.489697 -4.2617273 -1.272964 1.2385259 3.4986312 4.6723757 3.7978013 1.6289358 -1.0751117 -4.2049279 -5.5809207 -6.0628605 -5.4719505][-6.120997 -5.5253148 -5.9123383 -5.2468977 -2.9123511 -0.73135507 0.99863744 1.7932222 0.86055541 -1.1379819 -3.4571609 -5.7893195 -6.6269341 -6.596118 -5.7298779][-5.8716407 -5.7234135 -6.3610373 -6.2891245 -4.8077688 -3.1898859 -2.130682 -1.6500092 -2.4314337 -4.1242518 -5.7980118 -7.1604576 -7.4400263 -6.9828968 -6.0104227][-5.8251257 -6.0466375 -6.8143129 -7.2051983 -6.5446272 -5.4912558 -5.0591574 -4.8056469 -5.2965736 -6.5880575 -7.6372194 -8.1639748 -7.8657627 -7.0137587 -5.9082203][-5.9896879 -6.337944 -7.0951729 -7.70166 -7.6420107 -7.1230803 -7.1228924 -7.0582113 -7.2174358 -7.9034691 -8.3338013 -8.2083855 -7.45473 -6.3117752 -5.1463552][-6.2202158 -6.3942356 -6.9686527 -7.5262728 -7.7398891 -7.6578646 -7.8405581 -7.905057 -7.8393497 -8.0538244 -7.8840332 -7.31157 -6.3874445 -5.278532 -4.3322535][-6.2129488 -6.0254316 -6.3529854 -6.7013159 -6.9701662 -7.1486263 -7.3872404 -7.5143614 -7.4010324 -7.3373823 -6.9537878 -6.2749624 -5.4787688 -4.6652837 -4.0091558]]...]
INFO - root - 2017-12-15 09:10:22.657720: step 48810, loss = 0.24, batch loss = 0.21 (36.2 examples/sec; 0.221 sec/batch; 17h:25m:49s remains)
INFO - root - 2017-12-15 09:10:24.947573: step 48820, loss = 0.19, batch loss = 0.16 (35.6 examples/sec; 0.225 sec/batch; 17h:43m:20s remains)
INFO - root - 2017-12-15 09:10:27.202924: step 48830, loss = 0.24, batch loss = 0.20 (35.5 examples/sec; 0.226 sec/batch; 17h:46m:18s remains)
INFO - root - 2017-12-15 09:10:29.460489: step 48840, loss = 0.19, batch loss = 0.16 (34.7 examples/sec; 0.230 sec/batch; 18h:08m:25s remains)
INFO - root - 2017-12-15 09:10:31.746614: step 48850, loss = 0.20, batch loss = 0.17 (35.1 examples/sec; 0.228 sec/batch; 17h:56m:32s remains)
INFO - root - 2017-12-15 09:10:34.011155: step 48860, loss = 0.20, batch loss = 0.16 (35.7 examples/sec; 0.224 sec/batch; 17h:38m:02s remains)
INFO - root - 2017-12-15 09:10:36.251233: step 48870, loss = 0.18, batch loss = 0.15 (34.9 examples/sec; 0.229 sec/batch; 18h:03m:57s remains)
INFO - root - 2017-12-15 09:10:38.540091: step 48880, loss = 0.22, batch loss = 0.19 (35.4 examples/sec; 0.226 sec/batch; 17h:49m:01s remains)
INFO - root - 2017-12-15 09:10:40.792839: step 48890, loss = 0.20, batch loss = 0.17 (36.2 examples/sec; 0.221 sec/batch; 17h:23m:10s remains)
INFO - root - 2017-12-15 09:10:43.061583: step 48900, loss = 0.37, batch loss = 0.34 (35.3 examples/sec; 0.227 sec/batch; 17h:52m:27s remains)
2017-12-15 09:10:43.357837: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0031703 -4.2841167 -4.4420176 -4.4500747 -4.4103169 -4.0261817 -3.7297161 -3.7558784 -3.7921982 -4.3899193 -4.2739172 -3.7342408 -4.0426 -3.7603726 -3.6952863][-3.8536365 -4.3208656 -4.3311176 -4.2315836 -4.1030889 -3.653975 -3.295228 -3.3861196 -3.5642824 -4.3330641 -4.4746885 -4.1924915 -4.5006933 -4.144701 -3.9550076][-4.242486 -4.217267 -4.1063166 -3.8740716 -3.5289736 -2.8094878 -2.2835488 -2.4050224 -2.8317876 -3.9298429 -4.4701214 -4.5131173 -4.9039125 -4.4952641 -4.1792207][-4.3802929 -4.0325203 -3.7703669 -3.2723074 -2.5425425 -1.4685433 -0.83236074 -1.051931 -1.7668887 -3.2057037 -4.1702843 -4.5419297 -5.085887 -4.7011333 -4.3075962][-4.688436 -3.5457292 -3.0264344 -2.1904604 -1.0772058 0.30814362 1.0384591 0.7525866 -0.2679466 -2.0755396 -3.529218 -4.2950144 -5.1019239 -4.8189373 -4.391202][-4.3630486 -2.8986483 -2.20706 -1.1668434 0.25365305 2.0229781 3.0085022 2.7911003 1.5041137 -0.748083 -2.7676184 -3.9669051 -5.0570831 -4.8834963 -4.4484854][-3.5414667 -2.4700458 -1.7926829 -0.65683293 1.0870886 3.3027637 4.5699615 4.4073172 2.8905361 0.25709677 -2.1596076 -3.6783481 -4.9429169 -4.8303547 -4.3736925][-3.6252432 -2.5841434 -1.9216602 -0.62040484 1.4149542 3.9876606 5.5278759 5.4338436 3.7349145 0.93076348 -1.6816 -3.4110179 -4.8170624 -4.736011 -4.2973695][-4.0207214 -3.0031905 -2.3661041 -0.98751283 1.1580923 3.8188632 5.4060965 5.3549337 3.7551224 1.0929036 -1.4624442 -3.2352791 -4.7125568 -4.6835213 -4.3003454][-4.4322567 -3.4305751 -2.9152622 -1.6896727 0.19584632 2.5691226 3.9886625 4.0326538 2.8339612 0.55937576 -1.6204274 -3.1803756 -4.5876408 -4.5857668 -4.2845192][-4.7526503 -3.7843387 -3.4604511 -2.5946195 -1.2232616 0.6808722 1.9239237 2.2233326 1.4760144 -0.2615968 -1.8514773 -3.0528624 -4.3465443 -4.35915 -4.1820283][-4.9310522 -4.0333862 -3.9317317 -3.4992447 -2.6816652 -1.2648535 -0.17280579 0.28493524 -0.0635438 -1.1892978 -2.1858242 -2.9722328 -4.0650663 -4.1015267 -4.0431576][-5.0713148 -4.2859812 -4.4068012 -4.4280014 -4.1172318 -3.1113732 -2.1336813 -1.6036654 -1.5422134 -2.1631198 -2.6400852 -2.9612837 -3.8749342 -3.9119828 -3.9061332][-5.0376229 -4.3178616 -4.5709696 -4.8783512 -4.9230342 -4.1965361 -3.3049011 -2.7352355 -2.4715438 -2.8687193 -3.0267437 -3.0130358 -3.7955647 -3.7774777 -3.7989559][-4.8081951 -4.07709 -4.326848 -4.6790819 -4.8710327 -4.3630538 -3.6138368 -3.1544583 -2.9264011 -3.3289919 -3.324178 -3.1120181 -3.7311971 -3.6697645 -3.7149477]]...]
INFO - root - 2017-12-15 09:10:45.614234: step 48910, loss = 0.23, batch loss = 0.20 (35.1 examples/sec; 0.228 sec/batch; 17h:57m:11s remains)
INFO - root - 2017-12-15 09:10:47.906297: step 48920, loss = 0.34, batch loss = 0.30 (35.5 examples/sec; 0.225 sec/batch; 17h:45m:06s remains)
INFO - root - 2017-12-15 09:10:50.154845: step 48930, loss = 0.22, batch loss = 0.19 (37.1 examples/sec; 0.216 sec/batch; 17h:00m:18s remains)
INFO - root - 2017-12-15 09:10:52.407061: step 48940, loss = 0.16, batch loss = 0.12 (36.4 examples/sec; 0.220 sec/batch; 17h:18m:29s remains)
INFO - root - 2017-12-15 09:10:54.701372: step 48950, loss = 0.20, batch loss = 0.16 (34.5 examples/sec; 0.232 sec/batch; 18h:15m:52s remains)
INFO - root - 2017-12-15 09:10:56.959467: step 48960, loss = 0.38, batch loss = 0.35 (34.4 examples/sec; 0.232 sec/batch; 18h:18m:33s remains)
INFO - root - 2017-12-15 09:10:59.231049: step 48970, loss = 0.31, batch loss = 0.28 (36.0 examples/sec; 0.222 sec/batch; 17h:29m:22s remains)
INFO - root - 2017-12-15 09:11:01.503244: step 48980, loss = 0.20, batch loss = 0.17 (34.5 examples/sec; 0.232 sec/batch; 18h:16m:29s remains)
INFO - root - 2017-12-15 09:11:03.783756: step 48990, loss = 0.22, batch loss = 0.19 (36.1 examples/sec; 0.221 sec/batch; 17h:26m:06s remains)
INFO - root - 2017-12-15 09:11:06.070618: step 49000, loss = 0.24, batch loss = 0.20 (35.9 examples/sec; 0.223 sec/batch; 17h:32m:13s remains)
2017-12-15 09:11:06.353051: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4523036 -3.1178946 -2.8860266 -2.2594407 -1.8392437 -1.6913363 -1.693496 -1.421488 -1.2830162 -1.8388691 -2.2134504 -2.4635282 -3.2642395 -4.0821285 -5.0426216][-1.9304677 -2.9431298 -2.6487448 -2.0912585 -1.6574609 -1.3718181 -1.3213298 -1.1256046 -1.1517282 -1.8840477 -2.5467737 -3.1470318 -4.02855 -5.0205784 -5.9868135][-1.9879456 -2.5004547 -2.3505876 -2.0396311 -1.768808 -1.5273743 -1.3789716 -1.1267718 -1.2872117 -2.114511 -2.9764123 -3.868896 -4.8555012 -5.7577372 -6.3027744][-2.2423108 -2.3123751 -2.2475448 -2.0520277 -1.8366889 -1.6138868 -1.3493751 -1.0105399 -1.287039 -2.1838992 -3.2020342 -4.34776 -5.4176722 -6.1895561 -6.3813219][-2.6019349 -2.1912975 -2.1389155 -1.9927827 -1.7394631 -1.3227954 -0.74782431 -0.22433877 -0.57671869 -1.5727406 -2.7823312 -4.2752852 -5.5780926 -6.2484093 -6.2301807][-2.912797 -2.1245577 -1.916119 -1.6522334 -1.2192037 -0.52209485 0.45674467 1.3980608 0.98992062 -0.36716342 -1.899646 -3.6737227 -5.1648846 -5.8047638 -5.7455816][-2.414752 -1.758353 -1.4611399 -1.1193968 -0.54925537 0.44962049 1.931031 3.4010193 2.9608347 1.1299636 -0.79955208 -2.7128232 -4.269999 -4.9234281 -4.871419][-2.0459902 -1.6492043 -1.2748088 -0.73078334 0.044706106 1.402441 3.2312834 5.0344229 4.5671015 2.3265464 0.16178131 -1.6641626 -3.2034504 -3.9654288 -3.9411907][-2.2010629 -1.8671641 -1.3503757 -0.6693058 -0.0038597584 1.2449586 2.8380663 4.5207109 4.0636911 1.9095037 0.1195147 -1.2290641 -2.5345478 -3.2930591 -3.2863612][-2.5155535 -1.9788356 -1.273916 -0.55348623 -0.16318154 0.64863467 1.7638009 3.0899694 2.6340654 0.73910022 -0.68460631 -1.5982213 -2.536895 -3.1586685 -3.1009734][-2.9807677 -2.2264991 -1.4388369 -0.71945262 -0.45160115 0.10063243 0.74238253 1.5126052 0.95076323 -0.73671556 -1.9349068 -2.6453474 -3.1768486 -3.5168405 -3.2251012][-4.2134304 -3.2390323 -2.4206054 -1.7384232 -1.5030103 -1.0852213 -0.78856456 -0.44749105 -0.98904896 -2.4645474 -3.4732962 -3.9799352 -3.9885759 -3.9888935 -3.5746536][-5.5901504 -4.5037065 -3.7765849 -3.219923 -2.9794402 -2.5931675 -2.4079916 -2.2573538 -2.6697259 -3.7623553 -4.5003676 -4.9108891 -4.711061 -4.6176887 -4.3946261][-6.3098383 -5.2722683 -4.8239522 -4.5342922 -4.3577347 -4.0026183 -3.8313627 -3.8119509 -4.1830149 -5.0060005 -5.7025928 -6.1851072 -5.8643379 -5.703434 -5.6565218][-6.473773 -5.6391096 -5.6250458 -5.6955538 -5.6436558 -5.281188 -5.0805268 -5.1693983 -5.5006671 -6.0435181 -6.5825644 -7.0038996 -6.5336885 -6.1962485 -6.1148548]]...]
INFO - root - 2017-12-15 09:11:08.645457: step 49010, loss = 0.18, batch loss = 0.15 (34.9 examples/sec; 0.229 sec/batch; 18h:04m:17s remains)
INFO - root - 2017-12-15 09:11:10.907721: step 49020, loss = 0.21, batch loss = 0.18 (37.1 examples/sec; 0.216 sec/batch; 16h:59m:31s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 09:11:13.200295: step 49030, loss = 0.17, batch loss = 0.14 (34.6 examples/sec; 0.231 sec/batch; 18h:12m:44s remains)
INFO - root - 2017-12-15 09:11:15.482715: step 49040, loss = 0.20, batch loss = 0.17 (32.7 examples/sec; 0.244 sec/batch; 19h:14m:48s remains)
INFO - root - 2017-12-15 09:11:17.763440: step 49050, loss = 0.21, batch loss = 0.18 (35.5 examples/sec; 0.226 sec/batch; 17h:45m:31s remains)
INFO - root - 2017-12-15 09:11:20.030739: step 49060, loss = 0.23, batch loss = 0.20 (34.1 examples/sec; 0.235 sec/batch; 18h:27m:49s remains)
INFO - root - 2017-12-15 09:11:22.335094: step 49070, loss = 0.23, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 17h:29m:55s remains)
INFO - root - 2017-12-15 09:11:24.587108: step 49080, loss = 0.22, batch loss = 0.19 (35.8 examples/sec; 0.224 sec/batch; 17h:35m:46s remains)
INFO - root - 2017-12-15 09:11:26.815066: step 49090, loss = 0.22, batch loss = 0.19 (35.7 examples/sec; 0.224 sec/batch; 17h:39m:55s remains)
INFO - root - 2017-12-15 09:11:29.123577: step 49100, loss = 0.19, batch loss = 0.16 (36.1 examples/sec; 0.221 sec/batch; 17h:25m:54s remains)
2017-12-15 09:11:29.398473: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8215251 -5.5283117 -5.7711258 -5.4131775 -4.9336958 -4.6119671 -4.852355 -5.0042048 -4.8690653 -4.9503765 -5.45255 -5.9202824 -6.2765031 -6.4374442 -6.4931841][-4.9847326 -5.5914803 -5.6376343 -5.0706272 -4.38564 -3.7018723 -3.8191543 -4.0143471 -3.9748468 -4.110858 -4.6194811 -5.1960659 -5.5372915 -5.7421665 -5.7859716][-5.5742645 -5.8322186 -5.957242 -5.267025 -4.3225136 -3.3331418 -3.17237 -3.0994294 -3.0507605 -3.2751584 -3.9548192 -4.5267849 -4.5881643 -4.7236586 -4.6742506][-6.3766069 -6.0189123 -6.1451139 -5.2069764 -3.8220768 -2.4091773 -2.0452991 -1.8597572 -1.8588474 -2.1239755 -3.024009 -3.4426689 -3.0797718 -2.8976247 -2.6730366][-6.4229217 -5.9768209 -5.8136349 -4.4686084 -2.6716242 -0.99242592 -0.487131 -0.31722152 -0.54652572 -0.96256709 -1.9694548 -2.1604066 -1.4226459 -0.96378326 -0.68118596][-6.7256918 -5.8032846 -5.3068247 -3.5806293 -1.4460378 0.52946663 1.2506676 1.2974024 0.80421972 0.21928 -0.83170509 -0.78735197 0.17878318 0.8403368 1.078794][-6.2323313 -5.6702147 -4.931787 -2.8653159 -0.51554728 1.6691167 2.7275057 2.8051229 2.2263594 1.3825428 0.039156675 -0.042180777 1.0177283 1.7329152 1.8195138][-6.0726519 -5.4210639 -4.4981985 -2.1138847 0.39830613 2.7106051 3.9719315 4.0525684 3.5011396 2.3294473 0.40606785 -0.27440417 0.50069356 1.1102161 1.0163281][-6.091835 -5.5969934 -4.765296 -2.2676237 0.3103652 2.529788 3.8505211 3.9515781 3.6005011 2.3732505 0.1279583 -1.0149928 -0.50621152 -0.16894555 -0.57149649][-6.4738832 -6.1824479 -5.5920334 -3.2491102 -0.78891468 1.2493217 2.5511203 2.6753516 2.4293075 1.2870634 -0.81638777 -2.2367635 -2.134017 -2.1906223 -2.7082095][-6.6165729 -6.5260649 -6.3202572 -4.5228586 -2.6255255 -0.9930892 0.13607764 0.19816542 -0.077111483 -1.1614052 -2.9601445 -4.3400826 -4.4475603 -4.5665045 -4.8248496][-6.6577668 -6.707839 -6.7872295 -5.5318022 -4.2102642 -2.8496881 -1.7986022 -1.7236743 -2.0772846 -3.0992947 -4.4576082 -5.7584286 -6.0271029 -6.2641292 -6.3430343][-6.7180338 -6.80913 -6.8950429 -5.8463626 -4.9683075 -3.9061499 -3.2337632 -3.3053865 -3.8400865 -4.7489357 -5.8702993 -7.2561688 -7.6871576 -7.8268795 -7.5221996][-7.1145854 -7.2605662 -7.2982368 -6.2670131 -5.649662 -4.73568 -4.2458439 -4.450326 -5.065064 -5.7096171 -6.6092739 -7.9898944 -8.201189 -8.0603409 -7.566431][-7.2581744 -7.4051366 -7.3885932 -6.3580532 -5.896204 -4.922369 -4.419837 -4.5428076 -5.1285143 -5.538825 -6.4199681 -8.0036545 -8.0919933 -7.8080273 -7.3865991]]...]
INFO - root - 2017-12-15 09:11:31.695332: step 49110, loss = 0.22, batch loss = 0.18 (35.9 examples/sec; 0.223 sec/batch; 17h:31m:25s remains)
INFO - root - 2017-12-15 09:11:33.958509: step 49120, loss = 0.17, batch loss = 0.14 (35.0 examples/sec; 0.228 sec/batch; 17h:58m:26s remains)
INFO - root - 2017-12-15 09:11:36.222398: step 49130, loss = 0.29, batch loss = 0.26 (35.1 examples/sec; 0.228 sec/batch; 17h:57m:45s remains)
INFO - root - 2017-12-15 09:11:38.468772: step 49140, loss = 0.31, batch loss = 0.27 (35.5 examples/sec; 0.225 sec/batch; 17h:44m:49s remains)
INFO - root - 2017-12-15 09:11:40.747903: step 49150, loss = 0.25, batch loss = 0.22 (36.2 examples/sec; 0.221 sec/batch; 17h:23m:57s remains)
INFO - root - 2017-12-15 09:11:43.040545: step 49160, loss = 0.30, batch loss = 0.27 (34.7 examples/sec; 0.231 sec/batch; 18h:09m:06s remains)
INFO - root - 2017-12-15 09:11:45.278281: step 49170, loss = 0.20, batch loss = 0.17 (36.3 examples/sec; 0.220 sec/batch; 17h:20m:15s remains)
INFO - root - 2017-12-15 09:11:47.515582: step 49180, loss = 0.20, batch loss = 0.16 (36.7 examples/sec; 0.218 sec/batch; 17h:08m:56s remains)
INFO - root - 2017-12-15 09:11:49.776143: step 49190, loss = 0.27, batch loss = 0.23 (35.7 examples/sec; 0.224 sec/batch; 17h:37m:45s remains)
INFO - root - 2017-12-15 09:11:52.028425: step 49200, loss = 0.38, batch loss = 0.34 (35.6 examples/sec; 0.225 sec/batch; 17h:41m:37s remains)
2017-12-15 09:11:52.325824: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5549719 -5.1297588 -5.4281464 -5.6718664 -5.8797245 -5.6374269 -5.0097513 -4.7860065 -4.997716 -5.1537857 -5.3043652 -5.5792675 -5.4811392 -5.0729074 -4.5340147][-4.5445023 -6.1172571 -6.3890944 -6.5208435 -6.4489746 -5.7843695 -4.8453846 -4.5088654 -4.87193 -5.17558 -5.5265532 -6.04613 -6.2030582 -6.0936427 -5.5401211][-6.3219881 -6.7894707 -6.8731489 -6.7333479 -6.1701689 -4.9632535 -3.7542725 -3.2832599 -3.866276 -4.5183411 -5.234293 -6.0529737 -6.5922632 -6.9011431 -6.3579073][-7.3948326 -6.9469595 -6.7555075 -6.3033667 -5.2389107 -3.4732018 -1.9225298 -1.1459595 -1.8612106 -2.888257 -4.074924 -5.2626982 -6.253253 -7.0948324 -6.7338905][-7.9515133 -6.7175274 -6.22624 -5.4802647 -4.0012884 -1.6792786 0.33000422 1.5216382 0.82131267 -0.59904826 -2.3764946 -4.0542507 -5.6130075 -6.9293566 -6.7970982][-7.5614595 -6.2300081 -5.4357347 -4.4301853 -2.6786404 0.064386845 2.5016606 4.1343527 3.6998308 1.8658078 -0.46737874 -2.6918797 -4.9167137 -6.6783714 -6.7314138][-6.6856241 -5.7997255 -4.8423319 -3.7553868 -1.9467289 1.0142474 3.7804072 5.6732969 5.4805546 3.4600346 0.79178643 -1.9416904 -4.5740557 -6.5070372 -6.5772676][-6.6984396 -5.6708865 -4.7068205 -3.6480474 -1.898106 0.99851489 3.7348835 5.5454845 5.5037174 3.4897735 0.67802954 -2.2655547 -4.9351397 -6.6392527 -6.5720606][-6.9623613 -5.9989676 -5.2163477 -4.2656355 -2.5863714 0.022684097 2.4218957 3.9467647 3.9031265 2.1506655 -0.46509504 -3.4328132 -5.8775954 -7.1478872 -6.9108992][-7.5124922 -6.7685604 -6.3450127 -5.6005492 -4.1101952 -1.9638844 0.031217337 1.2881217 1.0372667 -0.45223892 -2.6996317 -5.3155122 -7.234252 -7.9064951 -7.4467373][-8.1610289 -7.630867 -7.5156317 -6.9164915 -5.5569534 -3.8520281 -2.3108165 -1.3198502 -1.8241783 -3.3233128 -5.2600842 -7.2225275 -8.4247532 -8.5661926 -8.0022764][-8.6586113 -8.1993923 -8.1910172 -7.5991192 -6.2766037 -4.889596 -3.8493996 -3.2989082 -3.9266124 -5.427722 -7.1240377 -8.5690041 -9.21361 -9.0238323 -8.3762817][-8.9003525 -8.3562641 -8.2033224 -7.4432583 -6.1189413 -4.9559994 -4.2876506 -4.1135178 -4.8088489 -6.3018885 -7.8566475 -9.0231476 -9.34606 -8.89847 -7.9901409][-8.5904894 -7.731885 -7.2430716 -6.1650591 -4.7073622 -3.5848396 -3.3003678 -3.592124 -4.4927235 -6.0957203 -7.6963911 -8.6405668 -8.74311 -8.068697 -6.9702988][-7.7348843 -6.4652452 -5.5980959 -4.1623573 -2.5549951 -1.4445711 -1.5824449 -2.496706 -3.7417316 -5.1331959 -6.7438517 -7.781661 -8.05403 -7.3205004 -6.1099539]]...]
INFO - root - 2017-12-15 09:11:54.617447: step 49210, loss = 0.27, batch loss = 0.24 (36.5 examples/sec; 0.219 sec/batch; 17h:15m:25s remains)
INFO - root - 2017-12-15 09:11:56.883935: step 49220, loss = 0.25, batch loss = 0.22 (35.7 examples/sec; 0.224 sec/batch; 17h:36m:47s remains)
INFO - root - 2017-12-15 09:11:59.142544: step 49230, loss = 0.20, batch loss = 0.16 (35.8 examples/sec; 0.223 sec/batch; 17h:34m:20s remains)
INFO - root - 2017-12-15 09:12:01.388226: step 49240, loss = 0.21, batch loss = 0.18 (35.9 examples/sec; 0.223 sec/batch; 17h:32m:17s remains)
INFO - root - 2017-12-15 09:12:03.663861: step 49250, loss = 0.35, batch loss = 0.31 (34.5 examples/sec; 0.232 sec/batch; 18h:15m:35s remains)
INFO - root - 2017-12-15 09:12:05.962695: step 49260, loss = 0.30, batch loss = 0.27 (34.4 examples/sec; 0.233 sec/batch; 18h:17m:59s remains)
INFO - root - 2017-12-15 09:12:08.249800: step 49270, loss = 0.29, batch loss = 0.26 (35.4 examples/sec; 0.226 sec/batch; 17h:46m:50s remains)
INFO - root - 2017-12-15 09:12:10.571777: step 49280, loss = 0.22, batch loss = 0.18 (35.0 examples/sec; 0.228 sec/batch; 17h:57m:55s remains)
INFO - root - 2017-12-15 09:12:12.848795: step 49290, loss = 0.25, batch loss = 0.22 (33.5 examples/sec; 0.239 sec/batch; 18h:47m:53s remains)
INFO - root - 2017-12-15 09:12:15.130635: step 49300, loss = 0.24, batch loss = 0.21 (35.2 examples/sec; 0.227 sec/batch; 17h:53m:08s remains)
2017-12-15 09:12:15.418491: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.040477753 -1.0425777 -1.1131614 -1.9816329 -3.1832814 -4.5932083 -5.5374031 -5.7638779 -6.3802028 -6.6218467 -6.8040938 -6.3194895 -4.4581165 -1.7762333 1.9214196][-1.969322 -1.8088146 -1.9637868 -2.846725 -3.7474058 -4.9147 -5.6123571 -5.7253666 -6.41377 -6.8617873 -7.0569353 -6.2385416 -3.6840844 0.082339287 4.4156733][-4.0047159 -2.4645336 -2.8018858 -3.6560726 -4.2099638 -4.9235525 -5.211916 -5.2151423 -6.0218372 -6.7274718 -7.0851641 -6.2719359 -3.7171092 0.31965518 4.5526843][-5.9768105 -3.071414 -3.1316044 -3.525311 -3.5025802 -3.8063374 -3.7206178 -3.7631392 -4.7787609 -5.8696232 -6.7804103 -6.4496365 -4.4368491 -0.9504385 2.5864105][-7.5649881 -3.6976156 -3.3352442 -3.073175 -2.320035 -1.9982204 -1.4004157 -1.4399649 -2.7219472 -4.3718138 -6.0001917 -6.3342991 -5.2576227 -2.6622791 0.03172183][-7.6195078 -4.0343404 -3.3802338 -2.5548379 -1.0282317 0.22457409 1.4143341 1.4889004 0.030441046 -2.13341 -4.4969578 -5.6281681 -5.6535578 -4.2507725 -2.4581141][-7.0746551 -4.2820225 -3.3784046 -2.028404 0.27747202 2.4555526 4.0210547 4.0791478 2.4232965 -0.099636078 -2.951771 -4.6420293 -5.5007305 -5.2159491 -4.3831663][-7.6880469 -4.7929482 -4.0060658 -2.5989823 0.088666916 2.9397964 4.7843461 4.7887282 3.0045033 0.27889967 -2.5471401 -4.139679 -5.2251635 -5.4990325 -5.2955647][-7.7498674 -4.9303589 -4.4560843 -3.5336204 -1.3135531 1.4107425 3.2220759 3.3023396 1.7064788 -0.55716515 -2.9516878 -4.3343883 -5.310163 -5.6592903 -5.61412][-7.4263496 -4.56047 -4.25469 -3.8384748 -2.4281669 -0.48392808 0.73486328 0.57834458 -0.69658434 -2.1684062 -3.9247406 -5.0107455 -5.8243647 -6.0191088 -5.8231454][-7.0296335 -4.2202659 -4.0342059 -4.0009208 -3.3436046 -2.3433168 -1.8454577 -2.3204939 -3.3373892 -4.1289291 -5.1804237 -5.7612658 -6.21358 -6.108232 -5.7086735][-6.2112179 -3.5059252 -3.5166645 -3.8671122 -3.8555479 -3.6697345 -3.6705036 -4.2335343 -5.0088344 -5.3493519 -5.9594889 -6.410821 -6.6399074 -6.2888174 -5.7361517][-5.1292815 -2.4998586 -2.7585883 -3.4609451 -3.8304496 -4.1100221 -4.4069958 -4.9510841 -5.5632687 -5.88778 -6.5033326 -7.14113 -7.3608184 -6.9062529 -6.2142143][-4.730885 -2.128057 -2.4554689 -3.2814188 -3.7401063 -4.2276764 -4.8147516 -5.4693646 -6.0154681 -6.3645339 -6.9868994 -7.5115232 -7.6308904 -7.1410203 -6.4215][-4.812129 -2.3304169 -2.7069414 -3.4609804 -3.7848244 -4.3461761 -5.1917191 -5.99663 -6.5591688 -6.8682289 -7.2876625 -7.5515251 -7.4490542 -6.91155 -6.1032968]]...]
INFO - root - 2017-12-15 09:12:17.688427: step 49310, loss = 0.30, batch loss = 0.27 (36.0 examples/sec; 0.223 sec/batch; 17h:30m:14s remains)
INFO - root - 2017-12-15 09:12:19.933338: step 49320, loss = 0.17, batch loss = 0.14 (36.5 examples/sec; 0.219 sec/batch; 17h:14m:55s remains)
INFO - root - 2017-12-15 09:12:22.232519: step 49330, loss = 0.30, batch loss = 0.27 (36.2 examples/sec; 0.221 sec/batch; 17h:22m:41s remains)
INFO - root - 2017-12-15 09:12:24.525858: step 49340, loss = 0.19, batch loss = 0.16 (36.5 examples/sec; 0.219 sec/batch; 17h:14m:30s remains)
INFO - root - 2017-12-15 09:12:26.795309: step 49350, loss = 0.22, batch loss = 0.18 (35.5 examples/sec; 0.226 sec/batch; 17h:44m:44s remains)
INFO - root - 2017-12-15 09:12:29.070160: step 49360, loss = 0.23, batch loss = 0.19 (35.0 examples/sec; 0.229 sec/batch; 17h:58m:48s remains)
INFO - root - 2017-12-15 09:12:31.356118: step 49370, loss = 0.22, batch loss = 0.19 (35.5 examples/sec; 0.225 sec/batch; 17h:43m:08s remains)
INFO - root - 2017-12-15 09:12:33.625590: step 49380, loss = 0.19, batch loss = 0.16 (35.0 examples/sec; 0.229 sec/batch; 17h:58m:33s remains)
INFO - root - 2017-12-15 09:12:35.887340: step 49390, loss = 0.19, batch loss = 0.16 (36.4 examples/sec; 0.220 sec/batch; 17h:17m:29s remains)
INFO - root - 2017-12-15 09:12:38.196259: step 49400, loss = 0.27, batch loss = 0.24 (33.9 examples/sec; 0.236 sec/batch; 18h:33m:41s remains)
2017-12-15 09:12:38.469592: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.13435 -8.4557076 -8.06661 -7.8745651 -7.7187119 -7.6729965 -7.0016708 -6.2131519 -6.4315071 -7.8995113 -9.8564939 -11.663054 -11.554303 -10.38905 -9.6713438][-7.87628 -9.418273 -8.8914528 -8.5963831 -8.2360954 -7.9872046 -7.2114506 -6.5874367 -7.1254253 -8.7507944 -10.478646 -12.191009 -12.275663 -11.275589 -10.691924][-9.2535954 -9.8060617 -9.19934 -8.673069 -7.8518915 -7.0664091 -6.0357571 -5.733604 -6.759304 -8.6280146 -10.113362 -11.492446 -11.783811 -11.300899 -10.983975][-9.9491787 -9.580615 -8.7612181 -7.8330221 -6.4572153 -4.927568 -3.5512879 -3.6249583 -5.2896824 -7.6226425 -9.0438595 -10.111733 -10.676489 -10.793125 -10.833109][-9.9933949 -8.8489971 -7.7594318 -6.3466797 -4.3626652 -1.8757682 0.12223268 -0.12836385 -2.508357 -5.4939661 -7.0309734 -7.8859882 -8.8014975 -9.7073278 -10.348403][-9.1764393 -7.7815962 -6.4249153 -4.5424347 -2.112752 1.1636066 3.7681863 3.381922 0.34458947 -3.2944055 -5.1847696 -5.8809104 -6.9113722 -8.432929 -9.54535][-8.07289 -6.8578205 -5.0545559 -2.5849864 0.3214817 4.3774462 7.5734196 7.2341585 3.6321242 -0.62648034 -3.2118952 -4.2289853 -5.5640597 -7.5150137 -8.99389][-7.2365923 -6.1583891 -4.3062239 -1.6364136 1.4450221 5.8721514 9.4915571 9.4327955 5.7004881 1.2167447 -1.7609779 -3.1819074 -5.0376067 -7.4154243 -9.2132454][-7.6414518 -6.9290895 -5.4947138 -3.2154784 -0.52343583 3.6213615 7.2127333 7.4846649 4.4548483 0.63545537 -2.0986278 -3.6477075 -5.7261677 -8.1420908 -9.949337][-8.2804079 -7.6746397 -6.5633888 -4.7512841 -2.7058039 0.73586941 3.8795812 4.3073864 2.1085441 -0.69902837 -2.7922919 -4.3739929 -6.452302 -8.6226826 -10.264712][-9.4010859 -8.6083651 -7.7706842 -6.4350653 -5.0449724 -2.5599616 -0.2264843 0.12394309 -1.1857681 -2.8547628 -4.0770311 -5.4494171 -7.2167282 -8.9003353 -10.208777][-10.471541 -9.6168022 -9.1310072 -8.3636055 -7.4742327 -5.776926 -4.0402174 -3.6937437 -4.0680876 -4.7019644 -5.361547 -6.4354658 -7.7282953 -8.9419088 -9.9728165][-10.439576 -9.4413242 -9.2423725 -9.00257 -8.4816933 -7.2536793 -5.8764558 -5.5035686 -5.4448614 -5.7799215 -6.5466833 -7.4272566 -8.0735188 -8.6658945 -9.3765173][-10.132538 -8.7129173 -8.6097069 -8.8454838 -8.5872 -7.6384926 -6.479126 -6.06756 -5.743248 -6.1389308 -7.2674618 -8.0208082 -8.1640549 -8.3182793 -8.798461][-9.6736307 -8.0241327 -8.0051317 -8.5475569 -8.4162941 -7.6271152 -6.5994606 -6.0406733 -5.4550438 -5.8526 -7.118269 -7.8050718 -7.9196672 -8.0522518 -8.5025158]]...]
INFO - root - 2017-12-15 09:12:40.721151: step 49410, loss = 0.37, batch loss = 0.34 (35.2 examples/sec; 0.227 sec/batch; 17h:51m:14s remains)
INFO - root - 2017-12-15 09:12:43.024710: step 49420, loss = 0.25, batch loss = 0.22 (35.0 examples/sec; 0.228 sec/batch; 17h:57m:07s remains)
INFO - root - 2017-12-15 09:12:45.262849: step 49430, loss = 0.18, batch loss = 0.15 (36.5 examples/sec; 0.219 sec/batch; 17h:14m:34s remains)
INFO - root - 2017-12-15 09:12:47.516172: step 49440, loss = 0.22, batch loss = 0.18 (36.0 examples/sec; 0.222 sec/batch; 17h:28m:17s remains)
INFO - root - 2017-12-15 09:12:49.762347: step 49450, loss = 0.20, batch loss = 0.17 (34.0 examples/sec; 0.235 sec/batch; 18h:28m:56s remains)
INFO - root - 2017-12-15 09:12:52.062467: step 49460, loss = 0.27, batch loss = 0.24 (35.2 examples/sec; 0.227 sec/batch; 17h:52m:41s remains)
INFO - root - 2017-12-15 09:12:54.318335: step 49470, loss = 0.27, batch loss = 0.24 (36.0 examples/sec; 0.222 sec/batch; 17h:29m:09s remains)
INFO - root - 2017-12-15 09:12:56.580343: step 49480, loss = 0.33, batch loss = 0.30 (35.8 examples/sec; 0.224 sec/batch; 17h:34m:57s remains)
INFO - root - 2017-12-15 09:12:58.837577: step 49490, loss = 0.36, batch loss = 0.32 (34.8 examples/sec; 0.230 sec/batch; 18h:02m:59s remains)
INFO - root - 2017-12-15 09:13:01.102051: step 49500, loss = 0.26, batch loss = 0.23 (35.6 examples/sec; 0.225 sec/batch; 17h:40m:43s remains)
2017-12-15 09:13:01.384773: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1716118 -5.4436312 -4.8912115 -4.4000034 -3.7414994 -3.0107536 -2.9754362 -3.0583644 -2.4383519 -1.5176187 -0.6577493 -1.1527267 -3.179625 -5.3111892 -6.0156198][-5.0692477 -4.3621774 -4.0137062 -3.4442124 -2.8080089 -2.3267896 -2.6319897 -3.1730311 -3.0040829 -2.3586793 -1.8469107 -2.4224281 -4.2402515 -6.26641 -6.8098392][-4.9578943 -3.4214942 -3.4270768 -2.7768214 -2.196197 -1.976054 -2.6006238 -3.592279 -3.974268 -3.8074749 -3.64696 -4.2090931 -5.7063994 -7.3941555 -7.523613][-4.3912597 -2.567764 -2.846751 -2.1727893 -1.7434974 -1.7068384 -2.3729613 -3.5187039 -4.442934 -4.9589219 -5.2684484 -5.8931808 -7.2621145 -8.64232 -8.3975153][-4.1124153 -2.1063428 -2.2552533 -1.4989462 -1.1271205 -1.056391 -1.3383611 -2.1658988 -3.2115037 -4.2603846 -5.2031822 -6.1979713 -7.7344723 -9.1236935 -8.8526316][-4.50364 -2.0056798 -1.6484013 -0.71448863 -0.34414005 0.11657381 0.73762608 0.63537669 -0.35447478 -1.9454627 -3.5346286 -5.0640326 -7.0010529 -8.6971388 -8.6451349][-5.0696559 -2.6771703 -1.6554842 -0.37192845 0.2891345 1.5140867 3.3248503 4.1126966 3.1404798 0.87808084 -1.5844021 -3.8651972 -6.2807903 -8.3819246 -8.5607224][-6.4387894 -3.9107928 -2.4138079 -0.74096358 0.40474629 2.6425722 5.5681553 7.0548735 5.97628 3.000345 -0.53126287 -3.6908662 -6.5700207 -8.9939041 -9.2867956][-7.561017 -5.202672 -3.6185765 -1.795595 -0.3701036 2.4633329 5.8503647 7.5416555 6.2917547 3.1217582 -0.77931309 -4.2026291 -7.100913 -9.5209351 -9.8188677][-8.1513672 -6.3259144 -5.0698953 -3.52934 -2.1763206 0.72134686 3.9865863 5.48717 4.0731506 1.0588179 -2.5936084 -5.5180383 -8.0082827 -10.037727 -10.118276][-8.5136929 -7.5065041 -6.8940382 -5.8281994 -4.7258968 -2.0458844 0.86113596 2.023 0.62136984 -1.9750483 -5.2242169 -7.5394049 -9.296114 -10.615162 -10.299028][-8.5691385 -8.2078829 -8.2307339 -7.774827 -7.0920677 -4.6814709 -2.0403132 -0.95664775 -2.0803032 -4.2374096 -7.0826874 -8.8235722 -9.91073 -10.772932 -10.277371][-8.5738516 -8.4320812 -8.883811 -8.9402895 -8.6977568 -6.6333303 -4.1559515 -2.918586 -3.6075706 -5.3190184 -7.7777119 -8.97888 -9.423562 -10.149253 -9.7284641][-8.1419992 -7.913867 -8.4669256 -8.8273926 -8.9748755 -7.3272448 -5.2049007 -3.9749389 -4.4302983 -5.9392586 -8.30791 -9.1200562 -8.9632435 -9.61227 -9.2434235][-7.2607527 -6.8041496 -7.2421608 -7.6075 -7.9023552 -6.6824055 -5.148519 -4.3602009 -5.0791168 -6.7765055 -9.3515472 -10.0731 -9.486124 -9.8806581 -9.3497467]]...]
INFO - root - 2017-12-15 09:13:03.649560: step 49510, loss = 0.23, batch loss = 0.20 (35.9 examples/sec; 0.223 sec/batch; 17h:30m:15s remains)
INFO - root - 2017-12-15 09:13:05.919892: step 49520, loss = 0.24, batch loss = 0.21 (35.3 examples/sec; 0.227 sec/batch; 17h:49m:58s remains)
INFO - root - 2017-12-15 09:13:08.210911: step 49530, loss = 0.26, batch loss = 0.23 (35.7 examples/sec; 0.224 sec/batch; 17h:38m:18s remains)
INFO - root - 2017-12-15 09:13:10.509474: step 49540, loss = 0.25, batch loss = 0.21 (34.0 examples/sec; 0.235 sec/batch; 18h:28m:54s remains)
INFO - root - 2017-12-15 09:13:12.772641: step 49550, loss = 0.18, batch loss = 0.15 (35.9 examples/sec; 0.223 sec/batch; 17h:32m:15s remains)
INFO - root - 2017-12-15 09:13:15.059855: step 49560, loss = 0.31, batch loss = 0.28 (36.2 examples/sec; 0.221 sec/batch; 17h:21m:24s remains)
INFO - root - 2017-12-15 09:13:17.378649: step 49570, loss = 0.22, batch loss = 0.18 (34.0 examples/sec; 0.235 sec/batch; 18h:28m:12s remains)
INFO - root - 2017-12-15 09:13:19.657500: step 49580, loss = 0.15, batch loss = 0.12 (34.9 examples/sec; 0.229 sec/batch; 18h:01m:29s remains)
INFO - root - 2017-12-15 09:13:21.962128: step 49590, loss = 0.26, batch loss = 0.22 (35.8 examples/sec; 0.224 sec/batch; 17h:34m:09s remains)
INFO - root - 2017-12-15 09:13:24.243589: step 49600, loss = 0.34, batch loss = 0.31 (35.2 examples/sec; 0.227 sec/batch; 17h:51m:05s remains)
2017-12-15 09:13:24.549397: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8326266 -3.4611211 -2.9804265 -2.7436044 -2.8305197 -3.0453324 -2.7671607 -2.1414461 -1.2934579 -0.64456511 -0.7011373 -1.4069967 -2.2254033 -2.9626164 -3.9358766][-4.2628226 -2.6823182 -2.12254 -2.1528311 -2.7796741 -3.4128215 -3.224647 -2.7319882 -2.0065782 -1.2954876 -1.12861 -1.6647031 -2.5999913 -3.4048278 -4.3174706][-3.367723 -0.70157993 -0.15911722 -0.39735842 -1.2896664 -2.2527003 -2.4766126 -2.5855026 -2.4707465 -2.1292822 -1.975931 -2.3648965 -3.1076407 -3.7728465 -4.497982][-3.4739158 0.38809681 1.0196002 0.9087131 0.21541238 -0.847443 -1.3138344 -1.8119593 -2.3201251 -2.4769363 -2.5592732 -3.0142055 -3.6687393 -4.1647062 -4.6443276][-4.0314965 1.1571889 1.6563089 1.4923987 0.95788407 0.014866114 -0.52494013 -1.3413943 -2.27534 -2.8883812 -3.1492174 -3.5900762 -4.0966392 -4.3098707 -4.3230448][-3.5430222 1.6564486 2.3038781 2.3463905 2.3109014 1.7584345 1.1208727 -0.094572306 -1.4971364 -2.5909185 -3.12567 -3.6745119 -4.0033932 -3.9547267 -3.5781131][-2.9690194 1.5383167 2.4962795 2.8798959 3.5083711 3.5183132 2.8632686 1.4725707 -0.0011677742 -1.2556138 -1.9679257 -2.6979237 -3.0746462 -3.1161475 -2.6670861][-4.5621567 -0.0017619133 1.1730344 1.8942287 3.1485522 3.8503978 3.4450581 2.1212676 0.89626074 -0.25595033 -0.97039986 -1.5180373 -1.9454315 -2.1267078 -1.704102][-6.63186 -2.082561 -0.84453452 0.060457706 1.706286 2.7595217 2.4713714 1.274157 0.39011908 -0.58462358 -1.206581 -1.4150773 -1.6237996 -1.8326125 -1.3773035][-8.5275393 -4.2077885 -3.1095755 -2.2910559 -0.80118608 0.061985493 -0.28391683 -1.2378974 -1.7092233 -2.2507083 -2.578289 -2.4390571 -2.3712254 -2.5392265 -2.0952952][-9.5996714 -5.6937075 -4.8571987 -4.2424083 -3.1773961 -2.8291543 -3.2737906 -4.0192451 -4.161067 -4.3182344 -4.31896 -3.8603792 -3.4766994 -3.3672154 -2.809669][-9.429575 -5.8124523 -5.1538968 -4.7798238 -4.3691692 -4.6292963 -5.0014682 -5.5203094 -5.5001154 -5.421041 -5.3441286 -4.8032503 -4.1941695 -3.8711848 -3.3172665][-9.0838156 -5.8054152 -5.3679328 -5.2671785 -5.3404846 -5.8664379 -6.0071983 -6.3141508 -6.2532568 -6.020452 -5.8445354 -5.2792544 -4.5104108 -4.0775294 -3.7110078][-8.9146128 -6.11494 -5.8913631 -6.0149474 -6.339757 -6.9618273 -7.0379725 -7.2617846 -7.2541933 -6.8459826 -6.411706 -5.8538761 -5.0552692 -4.5325909 -4.3372617][-8.2956266 -5.9820137 -5.9118319 -6.1743021 -6.6522903 -7.2857952 -7.3813581 -7.5282154 -7.4047556 -6.8579435 -6.3564596 -5.9252148 -5.2861037 -4.8849454 -4.8478312]]...]
INFO - root - 2017-12-15 09:13:26.798929: step 49610, loss = 0.22, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 17h:31m:48s remains)
INFO - root - 2017-12-15 09:13:29.092929: step 49620, loss = 0.17, batch loss = 0.14 (34.6 examples/sec; 0.231 sec/batch; 18h:08m:53s remains)
INFO - root - 2017-12-15 09:13:31.368234: step 49630, loss = 0.24, batch loss = 0.21 (36.3 examples/sec; 0.220 sec/batch; 17h:19m:06s remains)
INFO - root - 2017-12-15 09:13:33.681630: step 49640, loss = 0.24, batch loss = 0.21 (35.0 examples/sec; 0.229 sec/batch; 17h:58m:27s remains)
INFO - root - 2017-12-15 09:13:35.938566: step 49650, loss = 0.22, batch loss = 0.19 (35.3 examples/sec; 0.227 sec/batch; 17h:47m:48s remains)
INFO - root - 2017-12-15 09:13:38.192309: step 49660, loss = 0.19, batch loss = 0.15 (35.0 examples/sec; 0.228 sec/batch; 17h:56m:34s remains)
INFO - root - 2017-12-15 09:13:40.450825: step 49670, loss = 0.22, batch loss = 0.19 (35.8 examples/sec; 0.224 sec/batch; 17h:34m:03s remains)
INFO - root - 2017-12-15 09:13:42.705090: step 49680, loss = 0.20, batch loss = 0.17 (34.4 examples/sec; 0.233 sec/batch; 18h:16m:02s remains)
INFO - root - 2017-12-15 09:13:44.957833: step 49690, loss = 0.20, batch loss = 0.16 (35.9 examples/sec; 0.223 sec/batch; 17h:31m:43s remains)
INFO - root - 2017-12-15 09:13:47.219914: step 49700, loss = 0.27, batch loss = 0.24 (35.1 examples/sec; 0.228 sec/batch; 17h:53m:37s remains)
2017-12-15 09:13:47.521882: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4044275 -4.4172263 -4.0673981 -4.444006 -4.8765264 -5.4809065 -5.4044313 -5.05166 -5.5883913 -6.1523967 -6.3064485 -5.8168197 -5.372014 -5.0813274 -4.3424587][-2.8297973 -2.77778 -2.3525698 -2.873826 -3.54952 -4.5416 -4.7448983 -4.5218182 -5.3439083 -5.9845014 -6.2848282 -6.0234256 -5.721055 -5.456883 -4.5481033][-2.2923877 -1.3767715 -0.91205895 -1.3497645 -1.9762868 -3.1264572 -3.4594383 -3.4108741 -4.5908546 -5.43112 -6.0967431 -6.20161 -6.0314169 -5.7490144 -4.7423973][-2.4035795 -0.26872325 0.30574918 0.095396757 -0.26994228 -1.3315233 -1.5907663 -1.492591 -2.9220257 -4.1899395 -5.5780721 -6.2745719 -6.3671823 -6.1823378 -5.1437483][-3.5645757 -0.81891882 -0.1712997 -0.048904419 -0.092436552 -0.92498922 -0.78072262 -0.18945885 -1.46527 -3.1064692 -5.0025396 -6.1026583 -6.4332118 -6.4475732 -5.5147328][-4.6718178 -2.10742 -1.2804568 -0.72679973 -0.304047 -0.57718813 0.22130632 1.3622217 0.21571279 -1.7423983 -3.9576011 -5.30988 -5.9285173 -6.2757912 -5.6109524][-5.0643024 -3.2470334 -2.1784577 -1.1441718 -0.29876971 -0.089879036 1.1587729 2.5776553 1.5574887 -0.52800429 -2.8801994 -4.2993627 -5.1439505 -5.8944654 -5.5254931][-5.4901996 -3.8159459 -2.5950451 -1.4350963 -0.60974288 -0.19112229 1.1831577 2.6129875 1.754312 -0.2497673 -2.5078666 -3.7627501 -4.7175679 -5.7039485 -5.4251642][-5.8884373 -4.5105391 -3.24437 -1.9677341 -1.1239983 -0.39810526 1.0805416 2.4032607 1.6796103 -0.21414232 -2.3698256 -3.4724355 -4.4925575 -5.5537248 -5.2631397][-5.7049108 -4.7679911 -3.5144105 -2.2050505 -1.2829823 -0.19242644 1.1791797 2.1119103 1.3522174 -0.55034471 -2.5716696 -3.4245486 -4.332407 -5.2928009 -4.9827147][-5.8476839 -5.3875437 -4.2774363 -3.1509356 -2.3189015 -1.1288249 -0.034417152 0.47883558 -0.37303078 -2.1673634 -3.7452731 -4.0938082 -4.5595436 -5.1827574 -4.7791653][-5.89067 -5.7182417 -4.8193612 -4.0721846 -3.4415643 -2.223892 -1.3413765 -1.0864292 -1.9645703 -3.6069255 -4.7315927 -4.7180958 -4.8560658 -5.2074461 -4.7674818][-6.0156784 -6.0441713 -5.3470912 -4.7755337 -4.138361 -2.9497886 -2.1579311 -2.0411651 -2.8599725 -4.3163633 -5.3201227 -5.2656174 -5.2657728 -5.4184675 -4.9132013][-6.66057 -6.80881 -6.2726035 -5.7003222 -5.0068541 -4.0093861 -3.3613889 -3.3079593 -3.9990497 -5.1713066 -5.9423075 -5.7601576 -5.5826912 -5.532464 -4.9979544][-6.8178678 -7.0337286 -6.75983 -6.255681 -5.6274633 -4.8892226 -4.3738232 -4.3665752 -4.9687548 -5.7704887 -6.2270284 -5.9611855 -5.715095 -5.5645571 -5.0230193]]...]
INFO - root - 2017-12-15 09:13:49.785687: step 49710, loss = 0.18, batch loss = 0.14 (33.3 examples/sec; 0.240 sec/batch; 18h:51m:25s remains)
INFO - root - 2017-12-15 09:13:52.041035: step 49720, loss = 0.24, batch loss = 0.21 (34.5 examples/sec; 0.232 sec/batch; 18h:14m:00s remains)
INFO - root - 2017-12-15 09:13:54.307292: step 49730, loss = 0.22, batch loss = 0.19 (35.0 examples/sec; 0.228 sec/batch; 17h:56m:35s remains)
INFO - root - 2017-12-15 09:13:56.584520: step 49740, loss = 0.25, batch loss = 0.22 (35.2 examples/sec; 0.227 sec/batch; 17h:50m:04s remains)
INFO - root - 2017-12-15 09:13:58.897932: step 49750, loss = 0.19, batch loss = 0.16 (32.9 examples/sec; 0.243 sec/batch; 19h:05m:02s remains)
INFO - root - 2017-12-15 09:14:01.197215: step 49760, loss = 0.21, batch loss = 0.18 (34.8 examples/sec; 0.230 sec/batch; 18h:03m:58s remains)
INFO - root - 2017-12-15 09:14:03.491939: step 49770, loss = 0.23, batch loss = 0.19 (33.6 examples/sec; 0.238 sec/batch; 18h:40m:28s remains)
INFO - root - 2017-12-15 09:14:05.789859: step 49780, loss = 0.24, batch loss = 0.21 (35.3 examples/sec; 0.227 sec/batch; 17h:48m:58s remains)
INFO - root - 2017-12-15 09:14:08.066921: step 49790, loss = 0.29, batch loss = 0.26 (33.2 examples/sec; 0.241 sec/batch; 18h:56m:40s remains)
INFO - root - 2017-12-15 09:14:10.362524: step 49800, loss = 0.17, batch loss = 0.14 (34.9 examples/sec; 0.229 sec/batch; 18h:00m:58s remains)
2017-12-15 09:14:10.647569: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6970167 -8.1280594 -8.3993282 -8.3845539 -8.3326149 -7.8966823 -7.0737262 -6.082634 -5.513319 -5.4363604 -6.0858068 -7.0590768 -7.8734136 -8.433939 -8.7322378][-6.3755717 -8.303606 -8.56479 -8.4584637 -8.06813 -7.1009569 -5.900877 -4.8133478 -4.1141663 -3.993196 -4.7388697 -5.9902821 -7.0429726 -7.877068 -8.5880327][-6.6060381 -7.621645 -7.6634393 -7.2974677 -6.5042729 -4.9413357 -3.3405037 -2.2717819 -1.6982765 -1.7315197 -2.7458091 -4.3282738 -5.6345658 -6.7192144 -7.7525539][-5.9698086 -6.2816229 -6.0970869 -5.45131 -4.2884235 -2.3109531 -0.47183967 0.66127396 1.2437005 1.1107554 -0.22945929 -2.1418302 -3.8079314 -5.1648583 -6.5153403][-5.1522875 -4.8777122 -4.5362396 -3.62473 -2.1586144 -0.0030040741 2.0111156 3.4356155 4.2495227 4.1468525 2.7478046 0.60011911 -1.5440433 -3.2972982 -4.8693371][-4.0987711 -3.51401 -3.2418439 -2.3779137 -1.0579276 0.95670748 3.0000987 4.5503349 5.4264855 5.4409504 4.1615481 2.1771569 0.037075043 -1.7673376 -3.381299][-3.3307464 -2.8702538 -2.7556536 -2.0847917 -0.98921096 0.80387044 2.7928143 4.3344707 5.0588355 4.8492713 3.6287394 1.8907413 -0.049275637 -1.6364405 -2.7583492][-3.7662356 -3.3353021 -3.5064964 -3.135175 -2.316431 -0.7813741 1.0781391 2.5329027 3.1145635 2.7337132 1.6804936 0.30053377 -1.2332512 -2.2973182 -2.6694906][-4.983779 -4.53835 -4.9693165 -4.8798485 -4.249114 -2.9547496 -1.3100646 0.041858912 0.51335311 -0.048350334 -0.91828489 -1.9252023 -2.970928 -3.5667934 -3.3798265][-6.210824 -5.6565356 -6.2592645 -6.3630271 -5.8644881 -4.9386206 -3.7842827 -2.7995777 -2.4724798 -3.1657836 -3.8518462 -4.4826493 -5.0550151 -5.1595249 -4.5668983][-6.880898 -6.1993036 -6.9016395 -7.1467061 -6.7771654 -6.2421107 -5.6678467 -5.0858583 -4.8720932 -5.5358143 -6.0050011 -6.2710457 -6.3846588 -6.1954355 -5.5567284][-7.0038128 -6.2808938 -6.9451742 -7.1511731 -6.813025 -6.571054 -6.4145651 -6.10448 -5.9290104 -6.4236555 -6.6873369 -6.6933193 -6.5898528 -6.3481345 -5.9367414][-6.8502207 -6.1804304 -6.7980924 -6.9694424 -6.7394996 -6.6714039 -6.6808763 -6.4898944 -6.3113093 -6.5673227 -6.6692648 -6.5171056 -6.32803 -6.1737952 -6.0075254][-6.2082672 -5.5736408 -6.090559 -6.2822785 -6.2410469 -6.3331981 -6.4149623 -6.2817283 -6.1003809 -6.13043 -6.1248236 -5.9622507 -5.8536139 -5.8849974 -5.9582405][-5.7367 -5.0638003 -5.3846169 -5.5469742 -5.6645594 -5.7774763 -5.7876444 -5.6601229 -5.4800072 -5.3468666 -5.3175845 -5.2488117 -5.2446957 -5.4053235 -5.6504755]]...]
INFO - root - 2017-12-15 09:14:12.933624: step 49810, loss = 0.18, batch loss = 0.15 (34.7 examples/sec; 0.230 sec/batch; 18h:04m:42s remains)
INFO - root - 2017-12-15 09:14:15.228904: step 49820, loss = 0.29, batch loss = 0.26 (34.6 examples/sec; 0.231 sec/batch; 18h:08m:00s remains)
INFO - root - 2017-12-15 09:14:17.516016: step 49830, loss = 0.22, batch loss = 0.19 (35.8 examples/sec; 0.223 sec/batch; 17h:32m:37s remains)
INFO - root - 2017-12-15 09:14:19.803864: step 49840, loss = 0.22, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 17h:29m:16s remains)
INFO - root - 2017-12-15 09:14:22.064720: step 49850, loss = 0.32, batch loss = 0.28 (34.6 examples/sec; 0.231 sec/batch; 18h:08m:49s remains)
INFO - root - 2017-12-15 09:14:24.363023: step 49860, loss = 0.20, batch loss = 0.17 (34.7 examples/sec; 0.230 sec/batch; 18h:05m:44s remains)
INFO - root - 2017-12-15 09:14:26.633705: step 49870, loss = 0.30, batch loss = 0.27 (34.4 examples/sec; 0.233 sec/batch; 18h:15m:46s remains)
INFO - root - 2017-12-15 09:14:28.892722: step 49880, loss = 0.21, batch loss = 0.17 (36.0 examples/sec; 0.223 sec/batch; 17h:28m:04s remains)
INFO - root - 2017-12-15 09:14:31.134831: step 49890, loss = 0.25, batch loss = 0.22 (35.0 examples/sec; 0.228 sec/batch; 17h:55m:43s remains)
INFO - root - 2017-12-15 09:14:33.435019: step 49900, loss = 0.21, batch loss = 0.18 (33.6 examples/sec; 0.238 sec/batch; 18h:41m:46s remains)
2017-12-15 09:14:33.725959: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1843877 -2.9711366 -3.3171725 -4.0601673 -5.59544 -6.6787891 -6.5841208 -5.8321075 -4.787416 -3.1491795 -1.7339933 -0.96427155 -0.79864466 -1.3614858 -2.2441666][-4.5606012 -4.2213225 -4.54123 -5.1924 -6.1557226 -6.4004507 -5.5611982 -4.46661 -3.4381602 -2.3419747 -1.5619522 -1.219296 -1.144389 -1.5661743 -2.7068985][-5.9845266 -4.9347372 -5.3334637 -5.7486067 -5.9250412 -5.2615318 -3.9491115 -2.677258 -1.9530452 -1.730323 -1.9317808 -2.0650594 -2.0091977 -2.313235 -3.4844685][-6.6636839 -5.4185047 -5.5349526 -5.0507259 -4.1530037 -2.6460147 -0.99939883 -0.038759947 -0.17599082 -1.1942594 -2.5077109 -3.2351258 -3.1898599 -3.3133163 -4.2097635][-7.1243334 -5.4140034 -4.99221 -3.5052702 -1.496673 0.79854751 2.7371981 3.4316437 2.4249685 0.14410067 -2.3100886 -3.9021959 -4.1830878 -4.2648125 -4.7659273][-6.3398094 -4.8918362 -4.1439509 -2.1297045 0.57775784 3.417681 5.7905121 6.5371103 4.9730434 1.7631481 -1.7085397 -4.3308382 -5.1783886 -5.28028 -5.3637652][-5.5569782 -4.7848644 -3.9245975 -1.7310321 1.2957892 4.3101406 6.9126987 7.7270126 6.0626917 2.6697261 -1.2347407 -4.4063077 -5.4131722 -5.3684206 -5.1849422][-5.860106 -5.3063331 -4.6194639 -2.8176746 -0.15377879 2.4796994 4.8707056 5.6604614 4.4199648 1.6212337 -1.8164688 -4.6210113 -5.4729614 -5.3730836 -5.2228632][-6.2006326 -5.9531965 -5.5916572 -4.4926233 -2.6524405 -0.66385078 1.2877522 2.0364726 1.3894935 -0.57538879 -3.2404275 -5.3302765 -5.9430079 -5.8364506 -5.8265696][-6.8190193 -6.5474443 -6.2893867 -5.6287689 -4.3251767 -2.7558727 -1.2349969 -0.68303239 -1.0948813 -2.5298235 -4.5307622 -5.9766531 -6.2201982 -5.8851671 -5.7510433][-7.2880688 -6.8723745 -6.6904249 -6.3657389 -5.5022464 -4.3762155 -3.3842783 -3.1443727 -3.572258 -4.6724825 -6.0034571 -6.6745253 -6.3667774 -5.6686249 -5.3231144][-7.2794294 -6.84235 -6.9451904 -7.1855516 -7.0650854 -6.5174866 -5.8451891 -5.6232176 -5.8262997 -6.3548 -6.8005781 -6.6630917 -6.02707 -5.3046064 -5.0095663][-6.5833035 -6.0134225 -6.2175632 -6.8139858 -7.2984743 -7.2947083 -6.9116793 -6.5995235 -6.5372143 -6.4423523 -6.2279568 -5.7469492 -5.1517706 -4.726099 -4.6294441][-5.8385296 -5.0486937 -5.0991535 -5.6777153 -6.4569063 -6.9150257 -6.9459743 -6.772191 -6.5386457 -5.9850044 -5.4621344 -4.9329033 -4.5396414 -4.387455 -4.4645867][-5.0659943 -4.0768681 -3.9575925 -4.3551421 -5.2150326 -5.9073734 -6.2199326 -6.1283298 -5.6396918 -4.7657886 -4.0719228 -3.6517105 -3.5463154 -3.6481774 -3.9896612]]...]
INFO - root - 2017-12-15 09:14:35.992861: step 49910, loss = 0.20, batch loss = 0.17 (35.3 examples/sec; 0.226 sec/batch; 17h:45m:56s remains)
INFO - root - 2017-12-15 09:14:38.246244: step 49920, loss = 0.28, batch loss = 0.25 (34.2 examples/sec; 0.234 sec/batch; 18h:21m:04s remains)
INFO - root - 2017-12-15 09:14:40.517320: step 49930, loss = 0.18, batch loss = 0.15 (35.7 examples/sec; 0.224 sec/batch; 17h:34m:20s remains)
INFO - root - 2017-12-15 09:14:42.844238: step 49940, loss = 0.25, batch loss = 0.22 (35.6 examples/sec; 0.225 sec/batch; 17h:39m:17s remains)
INFO - root - 2017-12-15 09:14:45.102586: step 49950, loss = 0.21, batch loss = 0.18 (34.8 examples/sec; 0.230 sec/batch; 18h:01m:17s remains)
INFO - root - 2017-12-15 09:14:47.367039: step 49960, loss = 0.32, batch loss = 0.29 (34.8 examples/sec; 0.230 sec/batch; 18h:03m:58s remains)
INFO - root - 2017-12-15 09:14:49.639631: step 49970, loss = 0.19, batch loss = 0.15 (34.6 examples/sec; 0.231 sec/batch; 18h:07m:52s remains)
INFO - root - 2017-12-15 09:14:51.955256: step 49980, loss = 0.21, batch loss = 0.18 (34.5 examples/sec; 0.232 sec/batch; 18h:10m:27s remains)
INFO - root - 2017-12-15 09:14:54.220186: step 49990, loss = 0.24, batch loss = 0.21 (34.9 examples/sec; 0.229 sec/batch; 17h:58m:47s remains)
INFO - root - 2017-12-15 09:14:56.476393: step 50000, loss = 0.18, batch loss = 0.15 (35.9 examples/sec; 0.223 sec/batch; 17h:30m:04s remains)
2017-12-15 09:14:56.781401: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5042448 -5.0957375 -5.9771838 -6.5352955 -6.73201 -6.6259336 -6.5135479 -6.7220287 -6.9632626 -7.0573616 -7.0440922 -7.2148514 -7.4063454 -7.3279877 -7.2023859][-3.7916813 -6.0424495 -7.1619654 -7.72069 -7.8422995 -7.5884266 -7.4186687 -7.7131462 -8.0407677 -8.2153349 -8.2775278 -8.4826393 -8.56536 -8.3137827 -7.9583445][-4.8946252 -6.565691 -7.7470903 -8.1775017 -8.1046762 -7.5085726 -6.9699736 -7.0371723 -7.396244 -7.7969885 -8.2147751 -8.6933374 -8.784502 -8.5753527 -7.9856157][-5.8051586 -7.0886126 -8.26319 -8.4724979 -8.1100454 -6.9701672 -5.7928281 -5.427227 -5.8752766 -6.7182722 -7.6903639 -8.4815617 -8.5993528 -8.5458612 -7.83731][-6.3634691 -7.7142878 -8.84933 -8.6954784 -7.7085075 -5.6030993 -3.4992666 -2.6184204 -3.2718208 -4.826458 -6.5537453 -7.8206968 -8.0758333 -8.1819878 -7.6267033][-6.7117262 -7.8408766 -8.65227 -7.7710629 -5.7671866 -2.460417 0.686254 2.079582 1.0216706 -1.5089679 -3.9495573 -5.7334843 -6.2095661 -6.5231895 -6.3886061][-6.8049726 -8.1034422 -8.6066208 -7.1501551 -4.4871645 -0.42549062 3.5710661 5.419714 4.1070642 0.96942163 -1.8736796 -3.7350678 -4.2691956 -4.7618055 -5.0568533][-7.0179996 -8.1695547 -8.2799711 -6.3215237 -3.2570586 1.311471 5.9915943 8.27786 6.6725731 2.6311238 -0.81754065 -2.8106568 -3.5980322 -4.191721 -4.6688576][-7.0878763 -8.3054667 -8.4198713 -6.5821676 -3.8138661 0.54738545 5.3215332 7.87477 6.4441118 2.3792155 -1.1842468 -3.2985682 -4.2133312 -4.6976724 -5.0835276][-7.0561123 -8.4295988 -8.9044914 -7.6155357 -5.5523653 -1.9361255 2.2052567 4.7169924 3.9303377 0.644434 -2.5824418 -4.6959467 -5.6334858 -6.1383934 -6.3612928][-6.6884613 -8.0383949 -8.8467865 -8.1021385 -6.6872444 -3.8784723 -0.76076972 1.2688239 0.98825741 -1.3187286 -3.8333237 -5.8065758 -6.7731266 -7.3238344 -7.4018383][-6.4762669 -7.8065062 -9.0343685 -8.9885759 -8.1503983 -6.1363587 -3.9833608 -2.3228145 -2.1285872 -3.4768205 -5.2719069 -6.87883 -7.546979 -7.9070053 -7.7944489][-6.7031412 -7.9707718 -9.4185686 -9.8416138 -9.4044828 -8.1237211 -6.8319359 -5.7123361 -5.4029708 -6.1211739 -7.2721205 -8.4178457 -8.865015 -9.2055035 -9.1109791][-6.6690674 -7.6662731 -9.0791531 -9.7323227 -9.6115971 -9.0449638 -8.552969 -7.98516 -7.6436863 -7.8890123 -8.4638147 -9.0173206 -9.2529869 -9.4559422 -9.3652649][-6.1888385 -6.6000223 -7.6407871 -8.3123207 -8.4920721 -8.48088 -8.4674072 -8.2914095 -8.0553389 -7.9620562 -7.9085169 -7.9003668 -8.0444374 -8.3105087 -8.3129606]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 09:14:59.551455: step 50010, loss = 0.19, batch loss = 0.16 (36.1 examples/sec; 0.222 sec/batch; 17h:23m:43s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 09:15:01.780742: step 50020, loss = 0.17, batch loss = 0.14 (36.2 examples/sec; 0.221 sec/batch; 17h:20m:20s remains)
INFO - root - 2017-12-15 09:15:04.070264: step 50030, loss = 0.16, batch loss = 0.13 (33.5 examples/sec; 0.238 sec/batch; 18h:42m:35s remains)
INFO - root - 2017-12-15 09:15:06.344568: step 50040, loss = 0.27, batch loss = 0.24 (35.8 examples/sec; 0.223 sec/batch; 17h:31m:14s remains)
INFO - root - 2017-12-15 09:15:08.633779: step 50050, loss = 0.19, batch loss = 0.15 (34.7 examples/sec; 0.231 sec/batch; 18h:06m:43s remains)
INFO - root - 2017-12-15 09:15:10.910661: step 50060, loss = 0.29, batch loss = 0.25 (34.2 examples/sec; 0.234 sec/batch; 18h:20m:29s remains)
INFO - root - 2017-12-15 09:15:13.223426: step 50070, loss = 0.23, batch loss = 0.19 (35.4 examples/sec; 0.226 sec/batch; 17h:43m:40s remains)
INFO - root - 2017-12-15 09:15:15.494536: step 50080, loss = 0.18, batch loss = 0.15 (32.5 examples/sec; 0.246 sec/batch; 19h:17m:06s remains)
INFO - root - 2017-12-15 09:15:17.773106: step 50090, loss = 0.25, batch loss = 0.22 (35.6 examples/sec; 0.225 sec/batch; 17h:37m:42s remains)
INFO - root - 2017-12-15 09:15:20.050208: step 50100, loss = 0.19, batch loss = 0.16 (35.1 examples/sec; 0.228 sec/batch; 17h:52m:32s remains)
2017-12-15 09:15:20.367289: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.56837213 -1.4636083 -2.6128814 -3.0599027 -3.6091411 -3.3706734 -2.1914124 -0.65342283 0.52362657 0.66016531 0.12407637 -0.89008546 -2.3955078 -3.2107711 -3.1716633][-1.9622325 -2.5727053 -3.3850875 -3.4907296 -3.5794482 -2.8137667 -1.5964706 -0.059318304 1.0441422 1.2301424 0.85804129 -0.25344324 -2.1388383 -3.2223678 -3.2820435][-2.9073062 -3.3472903 -3.8790886 -3.5707157 -3.0361023 -1.8508381 -0.60879827 0.8274982 1.6827848 1.6446121 1.1799045 -0.13993001 -2.4733319 -3.736136 -3.7653461][-3.2126281 -3.114382 -3.5266271 -3.0466065 -2.2968092 -0.95355 0.13728189 1.4866257 2.1245334 1.8735511 1.345741 -0.13361478 -2.8213069 -4.3304586 -4.4280043][-2.3965952 -1.8460999 -2.4997621 -2.4507108 -2.0269377 -0.96882045 0.13805246 1.4948454 2.051039 1.7555749 1.30756 -0.16726637 -2.9903598 -4.5791874 -4.5319281][-1.0299898 -0.56607044 -1.7143209 -2.12449 -2.0510137 -1.2149242 0.011828661 1.5082808 1.9343297 1.4088223 0.91950989 -0.46617448 -3.1803098 -4.6179733 -4.4240971][0.34795713 0.04490447 -1.5507019 -2.3523054 -2.16465 -1.1782396 0.2008419 1.7799785 2.036082 1.3741281 0.76482916 -0.63620305 -3.0865626 -4.42558 -4.2775259][0.57498264 0.039122105 -1.8592014 -2.817091 -2.3997138 -1.1719978 0.50229406 2.0807874 2.1858242 1.4999406 0.713048 -0.67478609 -2.742394 -3.9841528 -3.879813][0.19243526 -0.61198068 -2.5503478 -3.3581982 -2.63722 -1.2514511 0.65441132 1.9914429 1.8406842 1.1995401 0.24808168 -1.095892 -2.6985803 -3.9490619 -3.8951111][-0.47824824 -1.3699996 -3.2386746 -3.8068228 -3.1328392 -1.8322381 -0.16798377 0.89396334 0.691612 0.16238284 -0.9393065 -2.2549155 -3.6720266 -4.9525833 -4.9101038][-0.96137726 -1.6769338 -3.1300302 -3.6187544 -3.4086366 -2.6887262 -1.5042522 -0.94500232 -1.1140182 -1.3583822 -2.5167656 -3.7691588 -4.9385147 -6.0406609 -5.9637504][-1.2259064 -1.5166305 -2.73703 -3.3610716 -3.7615519 -3.67058 -2.8400688 -2.5896747 -2.6004419 -2.6628127 -3.9726686 -5.0645847 -6.0107765 -6.7104692 -6.4747162][-2.4025836 -1.99684 -2.8547316 -3.511764 -4.1769342 -4.3856273 -3.7857146 -3.7956381 -3.7603631 -4.1019158 -5.5797825 -6.7274523 -7.3491926 -7.395483 -6.849452][-3.7864308 -2.6058984 -3.1703463 -3.9461327 -4.9067726 -5.3577738 -4.9707565 -4.9628196 -4.7549357 -5.2435341 -6.6788673 -7.6429577 -7.9313602 -7.4949265 -6.984724][-5.0824718 -3.3542886 -3.5708036 -4.4125872 -5.5147448 -6.054492 -5.7260208 -5.5924768 -5.3398104 -5.870492 -7.1549349 -7.9450207 -7.927103 -7.1559181 -6.6572065]]...]
INFO - root - 2017-12-15 09:15:22.636184: step 50110, loss = 0.23, batch loss = 0.20 (34.4 examples/sec; 0.232 sec/batch; 18h:13m:04s remains)
INFO - root - 2017-12-15 09:15:24.894396: step 50120, loss = 0.23, batch loss = 0.20 (36.6 examples/sec; 0.218 sec/batch; 17h:07m:32s remains)
INFO - root - 2017-12-15 09:15:27.186317: step 50130, loss = 0.19, batch loss = 0.16 (35.5 examples/sec; 0.225 sec/batch; 17h:41m:07s remains)
INFO - root - 2017-12-15 09:15:29.468081: step 50140, loss = 0.28, batch loss = 0.25 (36.2 examples/sec; 0.221 sec/batch; 17h:19m:23s remains)
INFO - root - 2017-12-15 09:15:31.774283: step 50150, loss = 0.24, batch loss = 0.21 (35.1 examples/sec; 0.228 sec/batch; 17h:52m:37s remains)
INFO - root - 2017-12-15 09:15:34.077092: step 50160, loss = 0.18, batch loss = 0.14 (35.8 examples/sec; 0.223 sec/batch; 17h:30m:22s remains)
INFO - root - 2017-12-15 09:15:36.333470: step 50170, loss = 0.27, batch loss = 0.23 (35.6 examples/sec; 0.225 sec/batch; 17h:37m:22s remains)
INFO - root - 2017-12-15 09:15:38.597939: step 50180, loss = 0.19, batch loss = 0.15 (36.6 examples/sec; 0.218 sec/batch; 17h:07m:33s remains)
INFO - root - 2017-12-15 09:15:40.917075: step 50190, loss = 0.22, batch loss = 0.19 (35.8 examples/sec; 0.223 sec/batch; 17h:30m:56s remains)
INFO - root - 2017-12-15 09:15:43.222624: step 50200, loss = 0.22, batch loss = 0.19 (35.0 examples/sec; 0.229 sec/batch; 17h:56m:41s remains)
2017-12-15 09:15:43.586795: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3422441 -3.5324717 -3.9104285 -4.4414067 -4.8419266 -4.969523 -5.003768 -5.0264072 -4.8685637 -4.740097 -4.8351712 -5.2105 -5.4975414 -5.6345863 -5.7267561][-2.6649611 -4.5981283 -5.1694465 -5.8626847 -6.26182 -6.320962 -6.2637711 -5.9865761 -5.587513 -5.572402 -5.9100657 -6.6295023 -7.0992393 -7.2415581 -7.1908131][-4.0055971 -5.2389555 -5.9400091 -6.6284828 -6.8210974 -6.7213831 -6.5038433 -5.8480163 -5.2377315 -5.4519672 -6.0922575 -7.0988922 -7.769825 -7.9872856 -7.7507744][-5.4153013 -5.9035168 -6.5450239 -7.0029659 -6.8636665 -6.3905191 -5.7316794 -4.7248383 -4.1608591 -4.8636203 -5.8014307 -6.9734688 -7.9210634 -8.3089838 -7.9269829][-6.1941986 -5.7679873 -6.1801915 -6.4039259 -6.0549712 -5.1754265 -3.9677558 -2.583317 -2.1065848 -3.2235603 -4.5683813 -5.9698954 -7.0749607 -7.5858164 -7.1698189][-6.0704374 -5.2999697 -5.3315315 -5.2728267 -4.7508616 -3.4296846 -1.532007 0.39216566 1.1413724 -0.12781835 -1.98876 -3.8975036 -5.2900939 -5.9286261 -5.6610122][-5.7837152 -5.2200432 -4.8865356 -4.4225612 -3.5512629 -1.6911702 0.82381296 3.2682898 4.4372988 3.193536 0.97973466 -1.4904042 -3.2942564 -4.0789781 -4.1558309][-6.0848155 -5.4454632 -4.9464779 -4.2087846 -2.8741107 -0.44367337 2.4131334 5.1401024 6.5846462 5.4383116 3.0888064 0.27417421 -1.705904 -2.4678485 -2.6958916][-6.0998526 -5.5312176 -5.2395544 -4.6238751 -3.1903777 -0.69735563 2.1426332 4.734005 6.2584763 5.3891058 3.19396 0.66559482 -1.0294516 -1.4532633 -1.5582235][-6.3759174 -5.57169 -5.5539947 -5.3956223 -4.5078325 -2.6711786 -0.39418972 1.8629496 3.3908951 2.9056356 1.3491578 -0.22895956 -1.2111042 -1.1487403 -0.95775652][-6.6625118 -5.393364 -5.6422205 -5.9539566 -5.7719245 -4.831851 -3.5060933 -1.7845056 -0.19731402 -0.22576785 -0.99111879 -1.4417322 -1.5983052 -1.1714847 -0.61710548][-6.6021442 -5.0031834 -5.6359406 -6.4067764 -6.7288017 -6.4454021 -5.7865553 -4.7555785 -3.5311532 -3.3882463 -3.4820154 -2.9962041 -2.2963424 -1.500788 -0.62922871][-5.8518987 -4.1584511 -5.3327131 -6.50521 -7.2898178 -7.5136681 -7.4234142 -6.6694603 -5.7797518 -5.8005276 -5.6625471 -4.8231611 -3.8349483 -2.7806559 -1.543559][-4.3917141 -2.3792048 -3.8260927 -5.3257761 -6.3908157 -6.9908752 -7.36837 -7.0045729 -6.325983 -6.4767551 -6.5411425 -6.0675 -5.4658747 -4.6687226 -3.3727384][-2.1237178 0.58440256 -0.83689725 -2.707777 -4.2941751 -5.3868232 -6.0284948 -5.9569435 -5.6381721 -5.879324 -6.1835256 -6.3217182 -6.2721357 -5.7604113 -4.7766705]]...]
INFO - root - 2017-12-15 09:15:45.835837: step 50210, loss = 0.27, batch loss = 0.24 (35.5 examples/sec; 0.225 sec/batch; 17h:40m:21s remains)
INFO - root - 2017-12-15 09:15:48.143035: step 50220, loss = 0.24, batch loss = 0.21 (34.1 examples/sec; 0.235 sec/batch; 18h:23m:37s remains)
INFO - root - 2017-12-15 09:15:50.436823: step 50230, loss = 0.17, batch loss = 0.14 (34.9 examples/sec; 0.229 sec/batch; 17h:57m:59s remains)
INFO - root - 2017-12-15 09:15:52.730208: step 50240, loss = 0.30, batch loss = 0.27 (35.3 examples/sec; 0.227 sec/batch; 17h:47m:18s remains)
INFO - root - 2017-12-15 09:15:55.084738: step 50250, loss = 0.29, batch loss = 0.26 (34.6 examples/sec; 0.231 sec/batch; 18h:07m:30s remains)
INFO - root - 2017-12-15 09:15:57.390809: step 50260, loss = 0.18, batch loss = 0.15 (35.0 examples/sec; 0.228 sec/batch; 17h:54m:25s remains)
INFO - root - 2017-12-15 09:15:59.684687: step 50270, loss = 0.27, batch loss = 0.24 (34.1 examples/sec; 0.234 sec/batch; 18h:22m:51s remains)
INFO - root - 2017-12-15 09:16:02.000132: step 50280, loss = 0.16, batch loss = 0.13 (34.2 examples/sec; 0.234 sec/batch; 18h:19m:10s remains)
INFO - root - 2017-12-15 09:16:04.366220: step 50290, loss = 0.20, batch loss = 0.17 (32.3 examples/sec; 0.248 sec/batch; 19h:26m:24s remains)
INFO - root - 2017-12-15 09:16:06.701594: step 50300, loss = 0.30, batch loss = 0.27 (34.4 examples/sec; 0.233 sec/batch; 18h:14m:23s remains)
2017-12-15 09:16:07.090795: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8880525 -7.0651426 -8.0651045 -8.5660753 -9.16318 -9.5598364 -8.9760742 -8.7829523 -8.8306866 -9.0896587 -10.212071 -11.396472 -11.730442 -11.045906 -10.018692][-5.5921006 -6.65989 -8.1115732 -8.70548 -9.2269516 -9.496768 -8.7787313 -8.7332182 -8.8914642 -9.1364479 -10.446707 -11.943037 -12.393725 -11.697199 -10.769498][-5.5909281 -5.3500452 -6.97591 -7.5372248 -7.7968626 -7.8348722 -7.0800953 -7.0614567 -7.2006006 -7.4992867 -9.0963678 -10.860011 -11.460229 -11.031778 -10.465912][-5.9544468 -4.5426569 -6.0624528 -6.4179459 -6.0929317 -5.6846981 -4.7444963 -4.4508972 -4.6149139 -5.1776829 -6.9809542 -8.8478928 -9.5998535 -9.6504679 -9.533309][-6.2551079 -3.8898647 -5.324048 -5.6348381 -4.7034407 -3.6028507 -2.0217752 -1.1845018 -1.6872294 -2.8497677 -4.8579268 -6.5979586 -7.3073463 -7.7978525 -8.1265974][-5.5435643 -2.8585134 -4.0671535 -4.3167696 -2.8874879 -1.0771291 1.4442732 2.8732646 1.8309758 -0.26698446 -2.7363188 -4.3355045 -4.86531 -5.8442478 -6.7046671][-4.1968374 -1.5472599 -2.5138741 -2.760118 -1.047507 1.3692813 4.743371 6.5934792 4.8568573 1.7860715 -1.0502726 -2.6359518 -3.2203922 -4.65753 -5.90868][-3.2050617 -0.098238707 -0.82200253 -1.0160116 0.77930236 3.6839449 7.6493597 9.5880728 7.0744514 3.2583573 0.11001921 -1.5724747 -2.3386157 -4.0210495 -5.518796][-2.3834436 1.0506229 0.48641038 0.3026495 1.9439943 4.8303328 8.7773371 10.543717 7.5832758 3.5183375 0.4516809 -1.0994047 -1.8468839 -3.4050751 -5.0248666][-2.781811 0.64927912 0.0837872 -0.16375422 0.98106742 3.1800449 6.5202045 8.1599255 5.6059494 2.0254519 -0.55288756 -1.8462722 -2.5324535 -3.6736174 -5.071425][-3.8390598 -0.504277 -1.1679816 -1.5690365 -0.905547 0.5931046 3.0891459 4.6218815 2.7101963 -0.1859684 -2.3011289 -3.3781786 -4.035234 -4.7338572 -5.7680864][-4.8004389 -1.4949416 -2.4045267 -3.1321151 -2.9845147 -2.2914762 -0.74471343 0.70131469 -0.46759164 -2.7500012 -4.4606414 -5.2591648 -5.7130957 -6.0003452 -6.554635][-5.78888 -2.5787282 -3.747045 -4.8213396 -5.0740538 -4.9148817 -3.9890482 -2.5934839 -3.1164331 -4.8669939 -6.1359577 -6.6183081 -6.9269648 -7.069314 -7.3797083][-6.3642778 -3.4149137 -4.8327456 -6.0157404 -6.3000269 -6.1648417 -5.6167755 -4.4111185 -4.4541273 -5.6423283 -6.6128454 -7.068985 -7.54366 -7.9440784 -8.2047491][-6.8877487 -4.3599591 -5.7846966 -6.7510529 -6.8581629 -6.7157135 -6.4802494 -5.5756278 -5.4231563 -6.2627153 -7.0734873 -7.6241922 -8.1851406 -8.6077166 -8.63994]]...]
INFO - root - 2017-12-15 09:16:09.362537: step 50310, loss = 0.23, batch loss = 0.20 (35.3 examples/sec; 0.227 sec/batch; 17h:46m:48s remains)
INFO - root - 2017-12-15 09:16:11.692518: step 50320, loss = 0.20, batch loss = 0.16 (34.7 examples/sec; 0.231 sec/batch; 18h:05m:23s remains)
INFO - root - 2017-12-15 09:16:13.976968: step 50330, loss = 0.23, batch loss = 0.19 (35.4 examples/sec; 0.226 sec/batch; 17h:43m:48s remains)
INFO - root - 2017-12-15 09:16:16.252603: step 50340, loss = 0.18, batch loss = 0.15 (33.7 examples/sec; 0.238 sec/batch; 18h:37m:16s remains)
INFO - root - 2017-12-15 09:16:18.516878: step 50350, loss = 0.17, batch loss = 0.14 (35.8 examples/sec; 0.223 sec/batch; 17h:30m:33s remains)
INFO - root - 2017-12-15 09:16:20.899309: step 50360, loss = 0.18, batch loss = 0.15 (32.2 examples/sec; 0.248 sec/batch; 19h:26m:33s remains)
INFO - root - 2017-12-15 09:16:23.223365: step 50370, loss = 0.24, batch loss = 0.21 (34.3 examples/sec; 0.233 sec/batch; 18h:15m:50s remains)
INFO - root - 2017-12-15 09:16:25.558263: step 50380, loss = 0.24, batch loss = 0.21 (35.0 examples/sec; 0.229 sec/batch; 17h:56m:04s remains)
INFO - root - 2017-12-15 09:16:27.870035: step 50390, loss = 0.26, batch loss = 0.23 (34.4 examples/sec; 0.233 sec/batch; 18h:15m:00s remains)
INFO - root - 2017-12-15 09:16:30.156717: step 50400, loss = 0.21, batch loss = 0.18 (35.5 examples/sec; 0.225 sec/batch; 17h:38m:49s remains)
2017-12-15 09:16:30.470188: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1227179 -3.8110085 -3.5624201 -3.2363567 -2.1583257 -1.3779562 -1.3588125 -2.2167089 -3.4059548 -4.1228733 -4.1485586 -4.2671041 -4.4470558 -4.1139426 -3.48318][-2.7275889 -3.5692625 -3.4146433 -3.2419522 -2.4192123 -1.8818743 -1.963641 -2.7258537 -3.8232594 -4.5823746 -4.7571788 -4.9185891 -4.9446955 -4.3391128 -3.4780869][-3.8143711 -3.8222623 -3.8644228 -3.8411908 -3.1884708 -2.678539 -2.6082323 -3.0923684 -4.0138021 -4.942194 -5.4593396 -5.8733988 -5.8919954 -5.0931082 -3.9990385][-4.6832285 -4.0747042 -4.4524584 -4.6010246 -3.9674554 -3.1397836 -2.4723518 -2.3585374 -3.0706046 -4.4659729 -5.6568861 -6.5592108 -6.7428818 -5.8663912 -4.6483526][-5.1933451 -4.1809239 -4.7845249 -4.9586535 -4.2081728 -2.8187108 -1.2032541 -0.28844607 -0.92250216 -3.1057866 -5.3346615 -6.9194479 -7.3356895 -6.5039282 -5.2342381][-4.9853072 -4.1080093 -4.8574762 -4.9339685 -3.8852291 -1.6992433 1.0074964 2.760412 2.1892645 -0.7578783 -4.007926 -6.26011 -6.9044113 -6.26161 -5.1125345][-4.0727825 -3.6611509 -4.5174227 -4.4596434 -3.1025836 -0.10476494 3.6058767 6.1389112 5.6340971 2.1149809 -1.9733094 -4.8287983 -5.732769 -5.44456 -4.6894026][-3.7051272 -3.4370584 -4.1291137 -3.8170633 -2.1888156 1.3308499 5.6920385 8.7048445 8.2353363 4.4675369 -0.20051455 -3.5983293 -4.82024 -4.9810152 -4.633379][-3.984781 -3.750545 -4.2354813 -3.7525849 -2.0780261 1.5179656 5.8031321 8.6949 8.2368326 4.8173056 0.33159184 -3.0869203 -4.3255668 -4.7067156 -4.585619][-4.5343847 -4.2250919 -4.594615 -4.2838068 -3.1133058 -0.12386465 3.4198005 5.7897367 5.5381031 3.0382636 -0.46826136 -3.2348247 -4.1273708 -4.4480629 -4.41728][-5.6356597 -5.089757 -5.3241463 -5.2082787 -4.604043 -2.6509571 -0.080058336 1.8025744 1.959327 0.6186769 -1.6723309 -3.6145072 -4.1127133 -4.1030178 -3.7204304][-6.5635395 -5.8696275 -6.104393 -6.12325 -5.9290738 -4.9433708 -3.463294 -2.064466 -1.3843466 -1.6233585 -2.8095069 -3.9594564 -4.1059322 -3.7005448 -2.7945926][-6.6841259 -5.8861117 -6.3669291 -6.5840521 -6.6270905 -6.1834273 -5.5004778 -4.6520948 -3.8707361 -3.5027983 -3.7682509 -4.0984764 -3.9702766 -3.4419925 -2.2930264][-6.1335011 -5.2558432 -5.949142 -6.2737484 -6.3807364 -6.3241606 -6.1309533 -5.7051563 -4.9512906 -4.2183 -3.8530245 -3.6508207 -3.462966 -3.0673633 -1.9717705][-5.3428431 -4.4946203 -5.3733006 -5.6530018 -5.575633 -5.6248708 -5.8251381 -5.86333 -5.3258176 -4.5784836 -4.0688233 -3.7825775 -3.6618595 -3.3280182 -2.3735361]]...]
INFO - root - 2017-12-15 09:16:32.766850: step 50410, loss = 0.37, batch loss = 0.34 (34.2 examples/sec; 0.234 sec/batch; 18h:19m:41s remains)
INFO - root - 2017-12-15 09:16:35.059056: step 50420, loss = 0.27, batch loss = 0.24 (34.7 examples/sec; 0.231 sec/batch; 18h:04m:31s remains)
INFO - root - 2017-12-15 09:16:37.355071: step 50430, loss = 0.23, batch loss = 0.20 (35.3 examples/sec; 0.227 sec/batch; 17h:45m:59s remains)
INFO - root - 2017-12-15 09:16:39.646968: step 50440, loss = 0.24, batch loss = 0.21 (36.4 examples/sec; 0.220 sec/batch; 17h:14m:30s remains)
INFO - root - 2017-12-15 09:16:41.946474: step 50450, loss = 0.26, batch loss = 0.23 (33.8 examples/sec; 0.237 sec/batch; 18h:33m:46s remains)
INFO - root - 2017-12-15 09:16:44.213926: step 50460, loss = 0.24, batch loss = 0.21 (34.8 examples/sec; 0.230 sec/batch; 17h:59m:37s remains)
INFO - root - 2017-12-15 09:16:46.495954: step 50470, loss = 0.22, batch loss = 0.19 (34.6 examples/sec; 0.231 sec/batch; 18h:05m:37s remains)
INFO - root - 2017-12-15 09:16:48.781682: step 50480, loss = 0.19, batch loss = 0.16 (35.2 examples/sec; 0.228 sec/batch; 17h:49m:26s remains)
INFO - root - 2017-12-15 09:16:51.057275: step 50490, loss = 0.28, batch loss = 0.24 (35.3 examples/sec; 0.227 sec/batch; 17h:45m:26s remains)
INFO - root - 2017-12-15 09:16:53.400590: step 50500, loss = 0.29, batch loss = 0.26 (33.4 examples/sec; 0.240 sec/batch; 18h:46m:29s remains)
2017-12-15 09:16:53.738114: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7129915 -5.3995743 -5.3066854 -5.1237078 -5.0375676 -5.016212 -5.0205803 -5.0406489 -5.0927234 -5.2132368 -5.2615166 -5.2421761 -5.1992884 -5.1535435 -5.1033974][-5.2289476 -7.1115026 -7.0521832 -6.8627825 -6.7038031 -6.5502806 -6.4347534 -6.4222126 -6.5246353 -6.7429752 -6.9185095 -6.9928579 -6.898859 -6.6543827 -6.321023][-6.8725758 -7.8796349 -7.7402229 -7.4662428 -7.254015 -6.98707 -6.8085537 -6.8320971 -7.0814619 -7.4898767 -7.7827673 -8.0713463 -8.1269 -7.8816042 -7.3670664][-8.4636469 -7.930192 -7.7179909 -7.1970816 -6.6114912 -6.1093478 -5.85071 -5.9142265 -6.2867384 -6.8168068 -7.1384716 -7.7271566 -8.1972294 -8.2253885 -7.7688665][-9.2177153 -7.6258802 -7.3509073 -6.402648 -5.3397083 -4.6330223 -4.3342047 -4.5245509 -5.0192556 -5.492898 -5.7720084 -6.7616234 -7.5898333 -7.8048172 -7.4607286][-9.0273838 -7.0399675 -6.5817614 -5.2244282 -3.6136632 -2.7079082 -2.5519681 -2.9810796 -3.5464063 -3.9367964 -4.3875637 -5.9242439 -7.1265793 -7.4984369 -7.1804485][-8.2836857 -6.4819098 -5.69547 -4.0136895 -1.9845302 -0.87836277 -0.76210296 -1.3158416 -1.7987534 -2.005897 -2.5535643 -4.5587683 -6.1395044 -6.83799 -6.6484966][-8.1754208 -5.9396963 -4.8720217 -3.0700073 -0.86421192 0.54259539 0.87890792 0.64658213 0.54963255 0.6731503 -0.011513472 -2.4671929 -4.4787512 -5.6509967 -5.7549973][-8.3078527 -5.9948273 -4.5983553 -2.680516 -0.66965353 0.80342293 1.4356761 1.9213965 2.3035553 2.7354295 1.8095524 -1.0738291 -3.4216197 -4.8611927 -5.1981659][-8.3742924 -6.2868204 -4.6987982 -2.749027 -0.98052454 0.37513161 1.1096613 2.1805036 3.063257 3.7831285 2.7156394 -0.24640465 -2.6377792 -4.2676759 -4.8682604][-7.9595375 -6.2720909 -4.8020763 -3.0413976 -1.5558699 -0.37441623 0.55876231 2.0522401 3.2184069 4.0318422 2.9880478 0.39819312 -1.7441137 -3.442131 -4.2154694][-7.2432394 -5.9213114 -4.8383656 -3.4924574 -2.4803391 -1.63136 -0.77564025 0.91596651 2.4822996 3.33449 2.377033 0.32116365 -1.5503542 -3.146631 -3.8202839][-6.6493483 -5.4553509 -4.691947 -3.8162198 -3.2319837 -2.8878427 -2.4838603 -0.99355316 0.80140543 1.789459 1.2104709 -0.092151642 -1.5489572 -2.7494855 -3.2757592][-6.5679398 -5.4899769 -4.9524903 -4.3150253 -3.8068128 -3.7724352 -3.9479835 -3.0812702 -1.42616 -0.259892 -0.24259114 -0.8563596 -1.8016944 -2.4688528 -2.9659078][-7.0600376 -6.3364725 -6.1178188 -5.61868 -5.0841932 -5.1259089 -5.5649071 -5.198452 -3.9154167 -2.7726531 -2.4997084 -2.7256467 -3.1615062 -3.3731732 -3.5929351]]...]
INFO - root - 2017-12-15 09:16:56.007330: step 50510, loss = 0.19, batch loss = 0.16 (35.2 examples/sec; 0.227 sec/batch; 17h:48m:40s remains)
INFO - root - 2017-12-15 09:16:58.341824: step 50520, loss = 0.22, batch loss = 0.19 (35.1 examples/sec; 0.228 sec/batch; 17h:52m:22s remains)
INFO - root - 2017-12-15 09:17:00.639562: step 50530, loss = 0.17, batch loss = 0.14 (35.4 examples/sec; 0.226 sec/batch; 17h:42m:38s remains)
INFO - root - 2017-12-15 09:17:02.872408: step 50540, loss = 0.19, batch loss = 0.16 (36.9 examples/sec; 0.217 sec/batch; 16h:58m:31s remains)
INFO - root - 2017-12-15 09:17:05.134777: step 50550, loss = 0.31, batch loss = 0.27 (36.7 examples/sec; 0.218 sec/batch; 17h:04m:51s remains)
INFO - root - 2017-12-15 09:17:07.404014: step 50560, loss = 0.22, batch loss = 0.18 (35.4 examples/sec; 0.226 sec/batch; 17h:42m:21s remains)
INFO - root - 2017-12-15 09:17:09.676967: step 50570, loss = 0.19, batch loss = 0.16 (35.2 examples/sec; 0.227 sec/batch; 17h:48m:11s remains)
INFO - root - 2017-12-15 09:17:11.915340: step 50580, loss = 0.20, batch loss = 0.17 (35.8 examples/sec; 0.223 sec/batch; 17h:29m:39s remains)
INFO - root - 2017-12-15 09:17:14.142639: step 50590, loss = 0.26, batch loss = 0.22 (35.6 examples/sec; 0.225 sec/batch; 17h:36m:23s remains)
INFO - root - 2017-12-15 09:17:16.382662: step 50600, loss = 0.26, batch loss = 0.23 (35.4 examples/sec; 0.226 sec/batch; 17h:42m:33s remains)
2017-12-15 09:17:16.680731: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2892609 -4.8416252 -4.5947442 -4.4200592 -4.4703255 -4.5095963 -4.5489125 -4.8152533 -5.0825167 -5.3203297 -5.4161873 -5.473042 -5.1867914 -4.9524026 -4.6400843][-3.166007 -4.5286431 -4.4835348 -4.6344414 -4.9283381 -5.0168905 -4.9112978 -4.928968 -5.0629439 -5.368185 -5.5243807 -5.6554832 -5.4026566 -5.1868668 -4.8341932][-2.2571056 -3.2059591 -3.2316883 -3.6899638 -4.3019514 -4.5676794 -4.4988947 -4.3633604 -4.2993164 -4.3722029 -4.4179864 -4.4594507 -4.117547 -3.9055209 -3.704102][-1.9026282 -2.3963969 -2.4105968 -3.0074935 -3.7479777 -4.0274925 -3.9190526 -3.6726136 -3.4441671 -3.3396802 -3.3900545 -3.4137161 -2.9658825 -2.6790044 -2.4881268][-1.534146 -1.5359104 -1.4400995 -2.048136 -2.8142807 -2.9818707 -2.8717356 -2.5157478 -2.0886974 -1.8247874 -1.997963 -2.0408852 -1.5225415 -1.1816893 -1.0631133][-0.98022604 -0.61930716 -0.37453139 -0.93787122 -1.6771728 -1.68121 -1.5017698 -0.97683775 -0.2548871 0.26081061 0.11705494 0.16785216 0.71092439 0.89259338 0.7651813][-0.15615463 0.46125889 0.91826105 0.48346162 -0.12837982 0.11451077 0.40652943 1.0467596 1.945504 2.5815251 2.4874218 2.5961072 2.9725292 2.7296307 2.2187474][-0.37410724 0.59794259 1.3110447 1.0896897 0.68651962 1.0175579 1.3247795 1.9207828 2.7639363 3.4053519 3.4266346 3.5714676 3.7725475 3.1390507 2.3907497][-1.8414494 -0.69029117 0.065333128 0.041898966 -0.12574172 0.26309013 0.65360308 1.3379138 2.0960429 2.7255352 2.8628457 2.923033 2.8510516 1.9018934 1.0704482][-3.5214086 -2.4158263 -1.9456104 -2.1179492 -2.2536135 -1.9302206 -1.5368272 -0.82837486 -0.054381609 0.69670272 1.0671511 1.0614586 0.78226638 -0.38002038 -1.2515917][-4.6878939 -3.8115888 -3.7086029 -4.2361674 -4.6828885 -4.5931773 -4.3349695 -3.7862277 -2.9912417 -2.0244808 -1.3949113 -1.3390588 -1.6729395 -2.8476765 -3.7683854][-4.5879383 -3.9938362 -4.2152138 -5.02114 -5.82755 -6.0891876 -6.090744 -5.7243137 -5.0425158 -3.9208255 -3.0796459 -3.0242758 -3.3371103 -4.2837143 -5.1047621][-3.4517596 -3.0372157 -3.6349754 -4.6868682 -5.8060541 -6.4941645 -6.8751087 -6.716157 -6.0904627 -4.9647093 -3.8881044 -3.6020513 -3.6345391 -4.307498 -5.0354958][-2.0839472 -1.7039566 -2.6016228 -3.8097091 -5.0106187 -5.83762 -6.4212904 -6.4343233 -5.873807 -4.947649 -3.9828072 -3.5106235 -3.2259111 -3.5378575 -4.0141687][-0.93224144 -0.30617678 -1.4416379 -2.7358284 -3.9242306 -4.7999053 -5.5505981 -5.7078681 -5.2364731 -4.463522 -3.597718 -2.9536827 -2.4058948 -2.3619783 -2.5750222]]...]
INFO - root - 2017-12-15 09:17:18.920675: step 50610, loss = 0.22, batch loss = 0.19 (36.1 examples/sec; 0.222 sec/batch; 17h:22m:25s remains)
INFO - root - 2017-12-15 09:17:21.158904: step 50620, loss = 0.20, batch loss = 0.16 (35.1 examples/sec; 0.228 sec/batch; 17h:49m:15s remains)
INFO - root - 2017-12-15 09:17:23.421032: step 50630, loss = 0.16, batch loss = 0.13 (34.4 examples/sec; 0.233 sec/batch; 18h:12m:55s remains)
INFO - root - 2017-12-15 09:17:25.702481: step 50640, loss = 0.36, batch loss = 0.33 (35.6 examples/sec; 0.225 sec/batch; 17h:35m:40s remains)
INFO - root - 2017-12-15 09:17:27.972049: step 50650, loss = 0.18, batch loss = 0.15 (35.8 examples/sec; 0.224 sec/batch; 17h:30m:57s remains)
INFO - root - 2017-12-15 09:17:30.231258: step 50660, loss = 0.26, batch loss = 0.23 (36.2 examples/sec; 0.221 sec/batch; 17h:17m:40s remains)
INFO - root - 2017-12-15 09:17:32.498278: step 50670, loss = 0.19, batch loss = 0.16 (35.7 examples/sec; 0.224 sec/batch; 17h:32m:17s remains)
INFO - root - 2017-12-15 09:17:34.788904: step 50680, loss = 0.28, batch loss = 0.25 (36.3 examples/sec; 0.220 sec/batch; 17h:13m:57s remains)
INFO - root - 2017-12-15 09:17:37.048977: step 50690, loss = 0.19, batch loss = 0.16 (36.6 examples/sec; 0.219 sec/batch; 17h:06m:38s remains)
INFO - root - 2017-12-15 09:17:39.333494: step 50700, loss = 0.17, batch loss = 0.14 (35.2 examples/sec; 0.227 sec/batch; 17h:46m:20s remains)
2017-12-15 09:17:39.626246: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1120799 -3.5849483 -4.6989155 -5.3475418 -5.4076476 -5.1535282 -4.7813725 -4.7653236 -5.0627866 -5.3782296 -5.259737 -4.0811749 -2.6355076 -1.6508373 -1.3007684][-2.3976073 -3.6810279 -4.7489967 -5.2349358 -5.1555748 -4.7603645 -4.3705568 -4.5264554 -5.0265555 -5.476387 -5.2723866 -3.8348165 -2.1737883 -0.95533705 -0.52874839][-2.528748 -3.5140557 -4.4097033 -4.573761 -4.2779932 -3.7689381 -3.4823802 -3.8586984 -4.6162405 -5.3058367 -5.1019192 -3.5434861 -1.6927269 -0.18531561 0.31612086][-2.7151675 -3.5375795 -4.2063103 -4.02049 -3.5271711 -2.979346 -2.732584 -3.2643964 -4.3175826 -5.2733083 -5.2264881 -3.7008164 -1.7694745 -0.0032567978 0.61827278][-3.1147995 -3.6523438 -3.9167376 -3.3100829 -2.46074 -1.7155335 -1.3788744 -2.0757291 -3.4569788 -4.8352647 -5.2326889 -4.0723934 -2.3191953 -0.55593836 0.069987059][-3.6388192 -3.6500902 -3.336061 -2.0704339 -0.59595811 0.5279789 0.9679234 0.0029571056 -1.8820872 -3.8540711 -4.8915372 -4.3221788 -2.8955395 -1.2537262 -0.66309166][-3.7882707 -3.8611979 -3.0519471 -1.2808487 0.80946803 2.4169595 3.1470249 2.23715 0.14333272 -2.2063336 -3.7282476 -3.6410465 -2.5339599 -1.2058822 -0.7635963][-3.6260819 -3.4990993 -2.5492542 -0.67428434 1.6426198 3.4484727 4.409586 3.8288448 1.8709719 -0.41491318 -2.0609441 -2.3836544 -1.6442078 -0.68852496 -0.52113307][-2.8982148 -2.4988656 -1.6221787 -0.052308083 2.020261 3.5669649 4.4272346 4.0537453 2.2857792 0.2290113 -1.201766 -1.8046278 -1.3096033 -0.54343808 -0.46326065][-2.6949475 -1.9450355 -1.212497 -0.052178621 1.6564362 2.9074161 3.5202916 3.2153866 1.5173957 -0.31189752 -1.3681815 -2.0092661 -1.5830517 -0.77984953 -0.52141988][-3.4311924 -2.4439347 -1.7740333 -0.88209832 0.5563004 1.6312449 2.0809042 1.8053291 0.32603097 -1.1307985 -1.7080686 -2.1615775 -1.7374454 -0.9324379 -0.46754634][-4.5242414 -3.5863698 -3.0713415 -2.4080548 -1.3286188 -0.36717749 0.09329319 0.037565947 -1.0774586 -2.1019144 -2.2427249 -2.494082 -2.0879073 -1.24549 -0.6139766][-5.0331388 -4.5724926 -4.5486908 -4.3181314 -3.6862831 -2.9575982 -2.469775 -2.2842121 -2.9197884 -3.4368744 -3.170403 -3.1038151 -2.5767446 -1.6120598 -0.70933449][-4.5843697 -4.6189919 -5.1425157 -5.4104285 -5.3248825 -4.9881325 -4.5951929 -4.2762656 -4.6464291 -4.8406429 -4.3081188 -3.8432477 -3.0343549 -1.8502963 -0.67339814][-3.5739741 -3.7053857 -4.4801598 -5.1314144 -5.5817 -5.779809 -5.7028933 -5.6108222 -5.9817224 -6.0395374 -5.3860683 -4.5936456 -3.3660445 -1.9377029 -0.581442]]...]
INFO - root - 2017-12-15 09:17:41.853277: step 50710, loss = 0.24, batch loss = 0.21 (37.1 examples/sec; 0.216 sec/batch; 16h:54m:04s remains)
INFO - root - 2017-12-15 09:17:44.110331: step 50720, loss = 0.25, batch loss = 0.22 (35.9 examples/sec; 0.223 sec/batch; 17h:26m:53s remains)
INFO - root - 2017-12-15 09:17:46.372440: step 50730, loss = 0.24, batch loss = 0.21 (35.0 examples/sec; 0.229 sec/batch; 17h:54m:21s remains)
INFO - root - 2017-12-15 09:17:48.644259: step 50740, loss = 0.22, batch loss = 0.19 (36.0 examples/sec; 0.222 sec/batch; 17h:24m:17s remains)
INFO - root - 2017-12-15 09:17:50.920614: step 50750, loss = 0.29, batch loss = 0.26 (36.5 examples/sec; 0.219 sec/batch; 17h:09m:47s remains)
INFO - root - 2017-12-15 09:17:53.189334: step 50760, loss = 0.24, batch loss = 0.21 (36.1 examples/sec; 0.222 sec/batch; 17h:20m:06s remains)
INFO - root - 2017-12-15 09:17:55.472071: step 50770, loss = 0.25, batch loss = 0.22 (34.1 examples/sec; 0.234 sec/batch; 18h:20m:39s remains)
INFO - root - 2017-12-15 09:17:57.720215: step 50780, loss = 0.18, batch loss = 0.15 (35.9 examples/sec; 0.223 sec/batch; 17h:26m:55s remains)
INFO - root - 2017-12-15 09:17:59.952624: step 50790, loss = 0.22, batch loss = 0.18 (35.9 examples/sec; 0.223 sec/batch; 17h:25m:59s remains)
INFO - root - 2017-12-15 09:18:02.211157: step 50800, loss = 0.25, batch loss = 0.22 (35.4 examples/sec; 0.226 sec/batch; 17h:40m:59s remains)
2017-12-15 09:18:02.487664: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5322907 -3.8649104 -4.4046512 -4.9678383 -4.5479851 -3.3986368 -3.2218623 -4.7742596 -5.8473244 -6.9307652 -8.8151541 -9.5937548 -9.5395308 -8.8353567 -7.4080935][-5.2620125 -3.8771772 -4.3260794 -4.7521114 -4.2396793 -2.7700477 -2.4124012 -4.23694 -5.3664436 -6.3391924 -8.3551168 -9.2513237 -9.36792 -8.8247461 -7.468667][-5.9777017 -3.3434577 -3.635644 -3.6870513 -3.0415068 -1.3334446 -0.72730803 -2.6197028 -3.8376327 -4.7760181 -7.0173817 -8.3449287 -8.8923149 -8.5962381 -7.4436722][-5.7053881 -2.6672301 -3.0473061 -3.0092726 -2.5224288 -0.7794441 0.047109842 -1.6083206 -2.7170217 -3.4980016 -5.6857252 -7.3504915 -8.3340626 -8.3136473 -7.3876143][-4.501091 -1.5020328 -2.145575 -2.1768503 -1.863233 -0.17067695 0.84890866 -0.40143621 -1.2896671 -1.977478 -4.2552676 -6.334877 -7.5655918 -7.8304625 -7.164053][-3.6054604 -0.64881527 -1.5632038 -1.7197497 -1.3584268 0.59440517 1.95596 1.1471605 0.4020474 -0.44426024 -3.0917735 -5.58354 -7.1031857 -7.6805696 -7.1410561][-2.1621382 0.055795908 -1.5399015 -2.2624807 -1.9564098 0.36097169 2.4057553 2.4834635 1.9116256 0.6527133 -2.4659564 -5.2698674 -7.1294279 -7.9605579 -7.3303175][-0.72908545 1.3211989 -0.67859614 -1.963748 -1.5977688 0.90969038 3.1670182 3.6141255 2.8843539 1.1405251 -2.2835045 -5.1654091 -7.3821926 -8.4828939 -7.7040024][0.11906266 2.4007943 0.12289977 -1.7830372 -1.4607754 0.807261 2.8515236 3.5492108 2.8633072 0.88332582 -2.5410819 -5.3230681 -7.7555532 -9.0190964 -8.0298119][-0.014755964 2.8329089 0.53415346 -1.8557072 -1.7138619 -0.094719648 1.3085678 2.0043604 1.4943597 -0.27521467 -3.2227292 -5.5582266 -7.9780607 -9.2774591 -8.2295456][-1.4212971 2.4753506 0.62371731 -2.0139673 -2.2388258 -1.4067833 -0.7096 -0.24192715 -0.68581045 -2.2547855 -4.589159 -6.4781857 -8.52965 -9.5078249 -8.3375216][-3.8571005 0.45750642 -0.96333539 -3.4185128 -3.8115849 -3.4691763 -3.1651738 -2.9245727 -3.3681421 -4.6246867 -6.2818584 -7.5852032 -8.8786955 -9.3052244 -8.0512838][-6.1640263 -2.0152059 -3.0243351 -4.9774213 -5.4090962 -5.3404465 -5.2259684 -5.183846 -5.6263514 -6.5977993 -7.72686 -8.57255 -9.2148228 -9.124465 -7.8178644][-7.8068218 -4.2986794 -4.8937168 -6.12325 -6.4042969 -6.3861046 -6.2845306 -6.324192 -6.7601089 -7.4953938 -8.2455 -8.7502184 -8.9308424 -8.49577 -7.2816343][-8.0257978 -5.4683323 -5.8628836 -6.5775633 -6.778348 -6.7593918 -6.588172 -6.5497036 -6.7941751 -7.2049694 -7.6320486 -7.9043331 -7.8997831 -7.4634361 -6.5350504]]...]
INFO - root - 2017-12-15 09:18:04.789919: step 50810, loss = 0.24, batch loss = 0.20 (33.1 examples/sec; 0.241 sec/batch; 18h:53m:33s remains)
INFO - root - 2017-12-15 09:18:07.072936: step 50820, loss = 0.27, batch loss = 0.24 (35.2 examples/sec; 0.227 sec/batch; 17h:45m:51s remains)
INFO - root - 2017-12-15 09:18:09.374139: step 50830, loss = 0.21, batch loss = 0.18 (35.0 examples/sec; 0.229 sec/batch; 17h:53m:28s remains)
INFO - root - 2017-12-15 09:18:11.672567: step 50840, loss = 0.21, batch loss = 0.17 (34.6 examples/sec; 0.231 sec/batch; 18h:05m:06s remains)
INFO - root - 2017-12-15 09:18:13.919676: step 50850, loss = 0.28, batch loss = 0.25 (36.1 examples/sec; 0.221 sec/batch; 17h:19m:15s remains)
INFO - root - 2017-12-15 09:18:16.235493: step 50860, loss = 0.31, batch loss = 0.27 (34.9 examples/sec; 0.229 sec/batch; 17h:55m:02s remains)
INFO - root - 2017-12-15 09:18:18.506181: step 50870, loss = 0.23, batch loss = 0.20 (35.0 examples/sec; 0.228 sec/batch; 17h:51m:33s remains)
INFO - root - 2017-12-15 09:18:20.778568: step 50880, loss = 0.25, batch loss = 0.22 (36.3 examples/sec; 0.220 sec/batch; 17h:13m:48s remains)
INFO - root - 2017-12-15 09:18:23.070898: step 50890, loss = 0.24, batch loss = 0.21 (35.1 examples/sec; 0.228 sec/batch; 17h:50m:14s remains)
INFO - root - 2017-12-15 09:18:25.339392: step 50900, loss = 0.18, batch loss = 0.14 (34.3 examples/sec; 0.233 sec/batch; 18h:15m:23s remains)
2017-12-15 09:18:25.638086: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9729354 -3.3765178 -3.5478823 -3.8174384 -4.5354404 -5.5524311 -6.5631809 -6.7472134 -6.7539206 -6.0736837 -4.5404158 -3.1846471 -2.6245587 -2.2453647 -1.8654153][-4.7397308 -3.5360527 -3.6428161 -3.8747427 -4.3957529 -5.1042838 -6.033185 -6.2979221 -6.3449469 -5.7625475 -4.4157081 -3.1425953 -2.608458 -2.4190714 -2.1399274][-5.7264662 -4.1587911 -4.1882114 -4.2407665 -4.23336 -4.2313442 -4.6507149 -4.9397168 -5.2232752 -5.1464558 -4.3160429 -3.2843666 -2.7632475 -2.6810031 -2.4030964][-6.534586 -4.3574944 -4.2937632 -4.1705713 -3.4376011 -2.498184 -2.385627 -2.6407976 -3.2491817 -3.9079976 -3.9867306 -3.5759597 -3.1732366 -3.0148559 -2.6149266][-6.0507717 -3.7599955 -3.6432147 -3.4101353 -1.9335178 -0.043929338 0.81479406 0.61007738 -0.38936675 -2.0483551 -3.2586322 -3.6953397 -3.4952457 -3.2252414 -2.5680926][-5.5363617 -3.0981483 -3.0945098 -2.838191 -0.78651583 2.0811055 3.6876681 3.7797701 2.5142157 -0.07561326 -2.3506749 -3.5709405 -3.5053306 -2.9546022 -2.087393][-4.6151657 -2.7619133 -2.8674593 -2.5397286 -0.070158482 3.3933318 5.737936 6.1982994 4.8861609 1.778456 -0.99942327 -2.6229222 -2.7490742 -2.342809 -1.6858556][-4.8344994 -2.8819914 -2.9262459 -2.5660257 -0.1698513 3.4085915 6.2379246 7.1483679 5.98606 2.8173964 -0.10047889 -1.8520272 -2.2190716 -2.1098664 -1.7702909][-5.8710556 -3.7486935 -3.826328 -3.6617284 -1.7759581 1.2699842 4.2114153 5.4165363 4.5790024 1.9798682 -0.31417954 -1.7199759 -2.2359333 -2.5554683 -2.5077567][-7.4445095 -5.1007462 -5.0767322 -5.0047913 -3.7581315 -1.4205647 1.2438364 2.5245578 2.0946743 0.45740271 -1.0753655 -2.0734572 -2.7623034 -3.3794768 -3.5485322][-8.3680191 -5.7204442 -5.5681534 -5.57222 -4.870656 -3.3690355 -1.2428025 -0.037186861 -0.038980722 -0.79766178 -1.6800302 -2.4861619 -3.3380232 -4.1503487 -4.4977236][-8.47552 -5.6202044 -5.2897882 -5.2581978 -5.0565891 -4.3579016 -2.9446135 -1.8574491 -1.4774837 -1.5806041 -2.103152 -2.7778654 -3.6097836 -4.4671607 -4.9641504][-7.8918848 -4.8330994 -4.244586 -4.0827994 -4.1719732 -4.0161595 -3.1696444 -2.3353307 -1.9048533 -1.7968435 -2.1638267 -2.7511449 -3.5528357 -4.48965 -5.1280737][-7.2520695 -4.0257344 -3.1148648 -2.7512288 -2.8634119 -2.9250486 -2.6023102 -2.0812478 -1.7031558 -1.6959131 -2.1499124 -2.7189105 -3.4455719 -4.3981805 -4.990736][-6.7864952 -3.3999009 -2.2616343 -1.7778504 -1.8892289 -2.1453571 -2.2135932 -1.9249315 -1.7496281 -1.9038506 -2.3276536 -2.7168825 -3.2793105 -4.0465302 -4.5541716]]...]
INFO - root - 2017-12-15 09:18:27.981455: step 50910, loss = 0.19, batch loss = 0.16 (34.3 examples/sec; 0.234 sec/batch; 18h:15m:55s remains)
INFO - root - 2017-12-15 09:18:30.250129: step 50920, loss = 0.18, batch loss = 0.15 (35.0 examples/sec; 0.228 sec/batch; 17h:52m:06s remains)
INFO - root - 2017-12-15 09:18:32.500752: step 50930, loss = 0.17, batch loss = 0.14 (35.6 examples/sec; 0.225 sec/batch; 17h:34m:39s remains)
INFO - root - 2017-12-15 09:18:34.738973: step 50940, loss = 0.24, batch loss = 0.21 (36.1 examples/sec; 0.222 sec/batch; 17h:20m:00s remains)
INFO - root - 2017-12-15 09:18:37.013900: step 50950, loss = 0.29, batch loss = 0.25 (35.7 examples/sec; 0.224 sec/batch; 17h:30m:37s remains)
INFO - root - 2017-12-15 09:18:39.302683: step 50960, loss = 0.25, batch loss = 0.22 (33.8 examples/sec; 0.237 sec/batch; 18h:31m:16s remains)
INFO - root - 2017-12-15 09:18:41.534116: step 50970, loss = 0.27, batch loss = 0.24 (35.6 examples/sec; 0.224 sec/batch; 17h:33m:13s remains)
INFO - root - 2017-12-15 09:18:43.805562: step 50980, loss = 0.22, batch loss = 0.19 (35.0 examples/sec; 0.229 sec/batch; 17h:52m:35s remains)
INFO - root - 2017-12-15 09:18:46.066899: step 50990, loss = 0.24, batch loss = 0.21 (36.3 examples/sec; 0.221 sec/batch; 17h:14m:48s remains)
INFO - root - 2017-12-15 09:18:48.315462: step 51000, loss = 0.16, batch loss = 0.13 (35.9 examples/sec; 0.223 sec/batch; 17h:26m:17s remains)
2017-12-15 09:18:48.607075: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9332111 -4.1884117 -4.5354619 -4.8532391 -5.1777134 -5.5004225 -5.7685204 -6.10454 -6.2529144 -6.3977547 -6.8811941 -7.3066559 -7.4315653 -6.958642 -6.04469][-4.70549 -4.9339437 -5.4247117 -5.7251387 -5.8838644 -6.055779 -6.0994654 -6.3520803 -6.5165653 -6.6853333 -7.4338074 -8.1514635 -8.535676 -8.2842054 -7.2330914][-5.764308 -5.0674152 -5.6702518 -5.9471374 -5.8407855 -5.84951 -5.6714048 -5.7756376 -6.0415831 -6.33127 -7.3327532 -8.2635078 -8.8544874 -8.9174957 -7.9392796][-6.6477222 -5.3545885 -6.0470352 -6.2539587 -5.6491308 -5.1486044 -4.4313588 -3.9996481 -4.1807489 -4.7033515 -6.118186 -7.4114904 -8.2946863 -8.6936111 -8.08447][-6.9822969 -5.4208889 -6.1634822 -6.2506332 -4.9994793 -3.6793115 -2.042552 -0.689124 -0.59294939 -1.4345572 -3.388679 -5.2654381 -6.7824068 -7.8207369 -7.7806234][-6.3581057 -4.876935 -5.6158638 -5.6427317 -3.84624 -1.8298378 0.6142838 2.967087 3.4007685 2.1110718 -0.56464446 -3.2025692 -5.4069443 -7.0014906 -7.4558334][-5.1009808 -4.0728941 -4.7083645 -4.6921482 -2.5552821 -0.047552586 2.8622553 5.9211607 6.5939207 4.6017647 1.1622677 -2.1619453 -4.9493971 -6.7667475 -7.4303904][-4.3786664 -3.2620602 -3.7436361 -3.6753592 -1.6557578 0.74385095 3.5247047 6.6469393 7.355051 4.8497763 0.95902967 -2.5556028 -5.5554142 -7.2579718 -7.7589235][-4.0945945 -2.9154723 -3.3339252 -3.2905879 -1.845201 -0.28550136 1.5472386 3.8833349 4.2531862 1.7330034 -1.8629885 -4.786665 -7.2189722 -8.2390623 -8.1578493][-4.405364 -3.1152289 -3.5391643 -3.7139153 -3.1193981 -2.4753461 -1.6526799 -0.31530631 -0.22158694 -2.3655562 -5.1390977 -7.0863714 -8.5930624 -8.6948223 -8.0487251][-5.0596647 -3.6961102 -3.9513512 -4.2434459 -4.3054457 -4.2285171 -3.9940622 -3.4940538 -3.6009831 -5.0409584 -6.6533966 -7.5470581 -8.3956976 -8.1460447 -7.380681][-6.0371113 -4.4995131 -4.4146514 -4.7256517 -5.2357473 -5.5436363 -5.5865984 -5.475688 -5.68567 -6.424757 -7.1289721 -7.3727808 -7.7458277 -7.4121504 -6.7272415][-6.7377367 -5.1252189 -4.7428603 -5.0467467 -5.7935944 -6.3496418 -6.6165838 -6.7347603 -6.9431963 -7.2228661 -7.3329535 -7.2632008 -7.2444038 -6.8654156 -6.2786713][-6.9792461 -5.5325422 -5.045207 -5.1789532 -5.7163057 -6.1761227 -6.5014286 -6.7396822 -6.9008417 -6.9764404 -6.9077244 -6.7844944 -6.6430016 -6.3423824 -5.8815546][-6.92091 -5.8013582 -5.3989506 -5.3017826 -5.465651 -5.7302284 -6.0449581 -6.3116312 -6.45654 -6.4728565 -6.4610386 -6.4106703 -6.2628632 -5.9895363 -5.5704517]]...]
INFO - root - 2017-12-15 09:18:50.920884: step 51010, loss = 0.21, batch loss = 0.18 (33.1 examples/sec; 0.241 sec/batch; 18h:52m:28s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 09:18:53.194110: step 51020, loss = 0.30, batch loss = 0.27 (34.1 examples/sec; 0.235 sec/batch; 18h:20m:11s remains)
INFO - root - 2017-12-15 09:18:55.473373: step 51030, loss = 0.17, batch loss = 0.14 (34.3 examples/sec; 0.233 sec/batch; 18h:13m:19s remains)
INFO - root - 2017-12-15 09:18:57.749761: step 51040, loss = 0.22, batch loss = 0.19 (35.7 examples/sec; 0.224 sec/batch; 17h:30m:15s remains)
INFO - root - 2017-12-15 09:19:00.039975: step 51050, loss = 0.25, batch loss = 0.21 (34.7 examples/sec; 0.230 sec/batch; 18h:00m:38s remains)
INFO - root - 2017-12-15 09:19:02.332344: step 51060, loss = 0.16, batch loss = 0.13 (35.2 examples/sec; 0.227 sec/batch; 17h:46m:15s remains)
INFO - root - 2017-12-15 09:19:04.626003: step 51070, loss = 0.25, batch loss = 0.22 (34.8 examples/sec; 0.230 sec/batch; 17h:56m:55s remains)
INFO - root - 2017-12-15 09:19:06.896037: step 51080, loss = 0.21, batch loss = 0.17 (34.3 examples/sec; 0.233 sec/batch; 18h:14m:35s remains)
INFO - root - 2017-12-15 09:19:09.221092: step 51090, loss = 0.20, batch loss = 0.17 (34.5 examples/sec; 0.232 sec/batch; 18h:06m:06s remains)
INFO - root - 2017-12-15 09:19:11.496293: step 51100, loss = 0.23, batch loss = 0.19 (34.2 examples/sec; 0.234 sec/batch; 18h:16m:22s remains)
2017-12-15 09:19:11.778125: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5725963 -1.2513572 0.37235165 1.2627718 1.1528373 0.17303443 -1.5352095 -3.22117 -4.2365923 -5.0093842 -5.208919 -4.8312144 -4.2774577 -3.8920548 -3.1661251][-2.7459054 -1.9928288 -0.66909075 0.023080349 -0.034704447 -0.98150957 -2.4891617 -3.8474519 -4.696301 -5.2736845 -5.4671593 -5.2031446 -4.6773243 -4.1382885 -3.3016975][-3.9327142 -2.6471441 -1.6430727 -1.1791571 -1.1671593 -1.9884357 -3.0813346 -3.8866267 -4.4721208 -4.9062934 -5.2395868 -5.3096409 -5.0875392 -4.4794817 -3.4955535][-4.5887523 -2.7363663 -2.02943 -1.8235183 -1.7914103 -2.5843587 -3.227541 -3.4637828 -3.7368073 -3.9211221 -4.3687563 -4.72116 -4.7877674 -4.2263546 -3.2367425][-4.3435664 -2.0740824 -1.5285567 -1.527076 -1.6392162 -2.3759513 -2.5077918 -2.0939462 -2.0721884 -2.1608732 -2.8281035 -3.412401 -3.7384527 -3.4886012 -2.8874447][-3.4710157 -1.1672646 -0.77084947 -0.79523361 -0.94192255 -1.3853095 -0.8944149 -0.20451617 -0.35731065 -0.84739959 -1.9387232 -2.7569013 -3.1657817 -3.1021795 -2.8893161][-2.3203928 -0.54321885 -0.24602365 9.8705292e-05 0.3410151 0.65959144 1.8435252 2.5712206 1.8136084 0.55777025 -1.0309879 -2.1090584 -2.5299628 -2.6020467 -2.6484859][-2.6811998 -1.3234036 -0.93939912 -0.075236082 1.0272608 2.2310789 3.9696815 4.689806 3.4095194 1.5782814 -0.3272357 -1.6023803 -2.002002 -2.1511846 -2.1974747][-3.8290086 -2.8951838 -2.4192088 -1.0646344 0.61610913 2.4122975 4.3275766 5.0077343 3.4892333 1.4988203 -0.31571352 -1.5655081 -1.9134289 -2.0710752 -2.0799057][-5.0764809 -4.4610491 -3.8557148 -2.2383611 -0.38607264 1.5691426 3.4533303 4.0765963 2.5131419 0.72238922 -0.52584064 -1.3809955 -1.6979291 -2.057369 -2.3331652][-5.7898345 -5.5521026 -4.9655313 -3.5191917 -1.9799359 -0.068511009 1.7315154 2.2706106 0.87143087 -0.46652162 -1.02924 -1.4069676 -1.8824964 -2.6668217 -3.3108983][-6.272769 -6.2971191 -5.7486868 -4.5863571 -3.5163054 -1.7983547 -0.10449266 0.24201608 -0.98298824 -1.9280523 -1.8863337 -1.940328 -2.762527 -3.9721713 -4.7518115][-6.3897629 -6.4909658 -5.98616 -5.0867558 -4.4497108 -3.1155553 -1.7520521 -1.6733006 -2.7898955 -3.381279 -2.7578611 -2.531606 -3.7222753 -5.2284412 -5.9302187][-6.3326082 -6.4186869 -6.0664525 -5.516737 -5.2434998 -4.3502245 -3.4208946 -3.4620397 -4.2667727 -4.473527 -3.5805211 -3.2483644 -4.5544629 -6.0468264 -6.5488043][-6.3884859 -6.3481569 -6.1077857 -5.8520374 -5.7841825 -5.2928514 -4.697752 -4.6263819 -4.9967308 -5.0005188 -4.2839584 -4.1133528 -5.2442422 -6.4243536 -6.6983757]]...]
INFO - root - 2017-12-15 09:19:14.076089: step 51110, loss = 0.27, batch loss = 0.24 (35.5 examples/sec; 0.225 sec/batch; 17h:37m:16s remains)
INFO - root - 2017-12-15 09:19:16.311248: step 51120, loss = 0.18, batch loss = 0.15 (36.9 examples/sec; 0.217 sec/batch; 16h:56m:22s remains)
INFO - root - 2017-12-15 09:19:18.584494: step 51130, loss = 0.23, batch loss = 0.19 (35.0 examples/sec; 0.229 sec/batch; 17h:52m:27s remains)
INFO - root - 2017-12-15 09:19:20.855229: step 51140, loss = 0.17, batch loss = 0.13 (34.4 examples/sec; 0.232 sec/batch; 18h:09m:54s remains)
INFO - root - 2017-12-15 09:19:23.126841: step 51150, loss = 0.20, batch loss = 0.17 (35.4 examples/sec; 0.226 sec/batch; 17h:39m:40s remains)
INFO - root - 2017-12-15 09:19:25.416626: step 51160, loss = 0.25, batch loss = 0.22 (36.0 examples/sec; 0.222 sec/batch; 17h:21m:56s remains)
INFO - root - 2017-12-15 09:19:27.707079: step 51170, loss = 0.23, batch loss = 0.19 (35.3 examples/sec; 0.226 sec/batch; 17h:41m:15s remains)
INFO - root - 2017-12-15 09:19:29.976235: step 51180, loss = 0.27, batch loss = 0.24 (36.8 examples/sec; 0.218 sec/batch; 17h:00m:34s remains)
INFO - root - 2017-12-15 09:19:32.228199: step 51190, loss = 0.21, batch loss = 0.18 (35.6 examples/sec; 0.225 sec/batch; 17h:33m:50s remains)
INFO - root - 2017-12-15 09:19:34.543518: step 51200, loss = 0.32, batch loss = 0.29 (35.2 examples/sec; 0.227 sec/batch; 17h:44m:58s remains)
2017-12-15 09:19:34.852639: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5534778 -2.640291 -3.7176332 -4.3249331 -4.4219103 -4.010067 -3.7318404 -3.6588473 -4.3599644 -5.3021641 -5.3168278 -4.3102922 -3.0156469 -2.0434706 -1.6832192][-3.5060217 -3.2898188 -4.0114107 -4.3287091 -4.4763412 -4.4880352 -4.7259173 -4.7845435 -5.1904755 -5.8565016 -5.8492842 -4.9849644 -3.520577 -2.0919189 -1.7283504][-4.8938513 -3.7796144 -4.2031169 -4.1295462 -4.1498375 -4.3138728 -4.8867164 -5.062181 -5.3945341 -6.1046329 -6.3361521 -5.5089436 -4.0136528 -2.3457491 -1.9713781][-5.777977 -4.3090544 -4.0956593 -3.3221965 -3.1176977 -3.3315372 -4.120523 -4.4850149 -4.9548841 -5.8189497 -6.3165445 -5.8306489 -4.8252687 -3.2785149 -2.7393174][-5.7963428 -3.652317 -2.8718123 -1.600657 -1.3934375 -1.7618836 -2.6785946 -3.1440067 -3.6237612 -4.27384 -4.9339147 -5.18696 -5.1283135 -4.0828891 -3.4197016][-4.3794327 -2.3023961 -1.4095502 -0.090097427 0.18437076 -0.030555725 -0.67324018 -0.98999786 -1.3930366 -1.973569 -2.9068778 -3.8566642 -4.5180545 -4.0542111 -3.4811859][-2.8430891 -1.1884233 -0.25654054 0.98692155 1.5735312 1.7777903 1.4983177 1.3362525 0.78788185 -0.045142412 -1.2498698 -2.5974619 -3.5807614 -3.4684625 -3.1184821][-1.8302556 0.21845126 1.0432096 1.9797895 2.85859 3.3375523 2.9936049 2.6392148 1.8980558 0.86856747 -0.34940803 -1.7008016 -2.695363 -2.8887014 -2.7999921][-0.83724082 1.2688251 1.5675592 1.8716609 3.0702364 3.8697383 3.44362 2.9953287 2.0175173 0.47552013 -0.80139124 -1.8876035 -2.557492 -2.8081055 -2.908396][-1.4160013 0.4803381 0.24187851 0.2299788 1.6869695 2.6452639 2.2439187 1.7662723 0.39696741 -1.6390002 -2.6942635 -3.220504 -3.4220188 -3.5245633 -3.7535906][-3.3269227 -1.6297412 -1.7875051 -1.5381432 -0.0017123222 0.61498094 -0.061133146 -0.78465295 -2.3307927 -4.1776018 -4.5525961 -4.5643482 -4.6189051 -4.8407831 -5.0978584][-4.8942671 -3.1855273 -2.9570115 -2.6119065 -1.6433606 -1.7101254 -2.6471496 -3.4479394 -4.8007107 -6.0762873 -5.980093 -6.0611572 -6.4420624 -6.6712513 -6.5462][-6.3477545 -4.6494017 -4.3355718 -4.3210778 -4.0738878 -4.4395456 -5.213172 -5.88214 -6.8399572 -7.4686146 -7.3193884 -7.6712627 -8.0266695 -7.8068361 -7.22462][-6.580883 -5.1140718 -4.9861193 -5.2568293 -5.4128056 -5.7530746 -6.244348 -6.7752171 -7.437747 -7.724021 -7.5847559 -7.7501135 -7.717638 -7.2553473 -6.6152067][-6.0820923 -5.0125265 -5.0187936 -5.2200642 -5.362175 -5.5293522 -5.845511 -6.2761717 -6.8548326 -6.9835596 -6.6897087 -6.48917 -6.3115396 -6.0129695 -5.5768762]]...]
INFO - root - 2017-12-15 09:19:37.108311: step 51210, loss = 0.23, batch loss = 0.20 (35.4 examples/sec; 0.226 sec/batch; 17h:38m:38s remains)
INFO - root - 2017-12-15 09:19:39.428020: step 51220, loss = 0.20, batch loss = 0.16 (35.4 examples/sec; 0.226 sec/batch; 17h:39m:40s remains)
INFO - root - 2017-12-15 09:19:41.750639: step 51230, loss = 0.21, batch loss = 0.18 (34.2 examples/sec; 0.234 sec/batch; 18h:16m:05s remains)
INFO - root - 2017-12-15 09:19:44.039798: step 51240, loss = 0.21, batch loss = 0.17 (36.5 examples/sec; 0.219 sec/batch; 17h:06m:39s remains)
INFO - root - 2017-12-15 09:19:46.329181: step 51250, loss = 0.28, batch loss = 0.25 (34.2 examples/sec; 0.234 sec/batch; 18h:17m:20s remains)
INFO - root - 2017-12-15 09:19:48.596714: step 51260, loss = 0.23, batch loss = 0.20 (34.1 examples/sec; 0.234 sec/batch; 18h:18m:50s remains)
INFO - root - 2017-12-15 09:19:50.893719: step 51270, loss = 0.22, batch loss = 0.19 (34.5 examples/sec; 0.232 sec/batch; 18h:06m:21s remains)
INFO - root - 2017-12-15 09:19:53.187765: step 51280, loss = 0.23, batch loss = 0.20 (34.5 examples/sec; 0.232 sec/batch; 18h:05m:45s remains)
INFO - root - 2017-12-15 09:19:55.484700: step 51290, loss = 0.27, batch loss = 0.23 (34.9 examples/sec; 0.229 sec/batch; 17h:55m:36s remains)
INFO - root - 2017-12-15 09:19:57.793426: step 51300, loss = 0.25, batch loss = 0.22 (35.3 examples/sec; 0.227 sec/batch; 17h:42m:32s remains)
2017-12-15 09:19:58.113796: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8684587 -4.5728531 -3.5143259 -2.2957947 -1.3305562 -0.93549263 -1.0251633 -1.2545111 -1.2324153 -0.37640786 0.37539363 0.12224817 -0.46467078 -0.68858707 -1.4294735][-3.874445 -3.8380287 -2.7065051 -1.4610941 -0.57042789 -0.61445463 -1.2676287 -1.7368219 -1.7305493 -0.80063248 -0.34376967 -0.82863188 -1.4967363 -1.7260758 -2.3411582][-3.9057195 -3.262584 -2.1969249 -1.0453247 -0.157408 -0.34044588 -1.1821218 -1.6000268 -1.6323884 -0.99857485 -1.0375369 -1.825182 -2.5974317 -2.7495074 -3.01694][-4.1791077 -2.886359 -2.0889537 -1.0660925 0.058640242 0.29069996 -0.18875027 -0.35575891 -0.435279 -0.39426768 -1.1929076 -2.5002575 -3.5528853 -3.7710309 -3.5097606][-4.4689956 -2.9108734 -2.4477444 -1.4404204 0.0501132 1.003387 1.2303796 1.6217301 1.6569221 1.1121802 -0.47411585 -2.4659679 -4.0485778 -4.4250464 -3.8005013][-4.4024763 -3.0095558 -2.7741733 -1.7835877 -0.2181592 1.2708309 2.3539245 3.5764215 4.1243114 3.1631343 0.80510283 -1.9445877 -4.0023103 -4.696969 -4.1374507][-4.1593404 -2.9338741 -2.6764331 -1.7155643 -0.37712955 1.2762973 3.010258 4.8787823 5.8466263 4.6457691 1.9343426 -1.0754591 -3.2036629 -4.0848408 -3.8200057][-3.8759866 -2.8215849 -2.3169665 -1.2779899 -0.24107289 1.2322295 3.0747411 4.8768864 5.7074957 4.3678827 1.7987783 -0.6547296 -2.1476774 -2.8707156 -2.8691285][-3.8227103 -2.6735258 -1.8438644 -0.82863533 -0.23496675 0.74639821 2.1914189 3.0819128 3.2492454 1.9956067 0.35733318 -0.70383465 -1.2952993 -1.7980609 -2.0224833][-4.0097284 -2.6456349 -1.5839899 -0.64709711 -0.32098746 0.166188 0.76354051 0.47791362 0.011716604 -0.80870581 -1.0661671 -0.75717568 -0.7012465 -0.89535856 -1.0204028][-4.4942231 -2.9470193 -1.7171241 -0.69536459 -0.50856543 -0.63844252 -0.86037028 -1.8084338 -2.3749089 -2.5788047 -1.8346337 -0.79756117 -0.37130809 -0.17678738 -0.05790472][-5.0870981 -3.446435 -2.2326503 -1.3569639 -1.409369 -1.9941664 -2.5850916 -3.4732075 -3.6965485 -3.4467411 -2.2297914 -1.0477412 -0.51476121 -0.08056426 0.24297142][-5.369051 -3.8675842 -2.9486732 -2.3139439 -2.5431714 -3.3057046 -3.9620836 -4.5329561 -4.2503452 -3.7426066 -2.6215553 -1.7579561 -1.2812839 -0.766124 -0.44098258][-5.2597065 -4.03048 -3.4543281 -3.0342288 -3.2902038 -3.8875749 -4.3657236 -4.608799 -4.1056013 -3.7136483 -2.9347296 -2.402235 -2.02847 -1.6874419 -1.6549304][-4.9663696 -3.9424896 -3.6131189 -3.3394485 -3.4640989 -3.7132545 -3.9911065 -4.1692352 -3.8734303 -3.6793523 -3.1233337 -2.7398267 -2.410604 -2.31292 -2.6184509]]...]
INFO - root - 2017-12-15 09:20:00.409668: step 51310, loss = 0.20, batch loss = 0.17 (32.6 examples/sec; 0.246 sec/batch; 19h:11m:48s remains)
INFO - root - 2017-12-15 09:20:02.687944: step 51320, loss = 0.23, batch loss = 0.20 (36.5 examples/sec; 0.219 sec/batch; 17h:08m:10s remains)
INFO - root - 2017-12-15 09:20:04.966296: step 51330, loss = 0.21, batch loss = 0.17 (34.9 examples/sec; 0.229 sec/batch; 17h:53m:43s remains)
INFO - root - 2017-12-15 09:20:07.266262: step 51340, loss = 0.19, batch loss = 0.16 (33.6 examples/sec; 0.238 sec/batch; 18h:36m:14s remains)
INFO - root - 2017-12-15 09:20:09.579023: step 51350, loss = 0.24, batch loss = 0.21 (34.2 examples/sec; 0.234 sec/batch; 18h:15m:42s remains)
INFO - root - 2017-12-15 09:20:11.874178: step 51360, loss = 0.19, batch loss = 0.16 (35.2 examples/sec; 0.227 sec/batch; 17h:44m:21s remains)
INFO - root - 2017-12-15 09:20:14.125438: step 51370, loss = 0.25, batch loss = 0.22 (34.7 examples/sec; 0.231 sec/batch; 18h:00m:25s remains)
INFO - root - 2017-12-15 09:20:16.416560: step 51380, loss = 0.22, batch loss = 0.19 (34.8 examples/sec; 0.230 sec/batch; 17h:58m:01s remains)
INFO - root - 2017-12-15 09:20:18.671878: step 51390, loss = 0.26, batch loss = 0.22 (35.7 examples/sec; 0.224 sec/batch; 17h:29m:25s remains)
INFO - root - 2017-12-15 09:20:20.942170: step 51400, loss = 0.25, batch loss = 0.21 (33.4 examples/sec; 0.239 sec/batch; 18h:41m:33s remains)
2017-12-15 09:20:21.283054: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4547523 -2.8385456 -3.275507 -3.4503651 -3.5051405 -3.5232854 -3.3036819 -2.9641623 -2.5280449 -2.2650893 -2.4972279 -3.0237322 -3.4141829 -3.3762 -2.9389269][-1.5961369 -2.6392848 -3.0304713 -3.2033613 -3.3367653 -3.5568185 -3.5084202 -3.2518086 -2.8900795 -2.7151759 -3.1808515 -3.9331033 -4.3981562 -4.3489165 -3.86536][-1.77796 -2.3584843 -2.6776917 -2.8464189 -3.0181267 -3.2961135 -3.2250938 -2.9656212 -2.6144688 -2.4509175 -3.0138206 -3.9334638 -4.5469 -4.64675 -4.2223496][-2.5881588 -2.7010009 -2.8158624 -2.8084722 -2.7816939 -2.7968268 -2.425101 -2.0532715 -1.6835585 -1.5174277 -2.1138935 -3.1873102 -4.0776482 -4.5411329 -4.4133019][-3.7181447 -3.4087992 -3.3285258 -3.09345 -2.8523364 -2.5820673 -1.88736 -1.3461366 -0.9463222 -0.75701451 -1.2843785 -2.4087887 -3.5017312 -4.2615991 -4.4054108][-4.4728527 -3.9817944 -3.839849 -3.5638604 -3.2415142 -2.6879044 -1.6069425 -0.762908 -0.24468684 0.0010826588 -0.47752059 -1.6265023 -2.867311 -3.7883708 -4.1201534][-4.5308671 -4.2042823 -4.1269631 -3.9066691 -3.4509957 -2.4842427 -0.93358016 0.36608744 1.1308692 1.487962 1.0009484 -0.28150082 -1.8083974 -3.0038376 -3.5587449][-4.5123472 -4.1426153 -4.1228819 -4.0060911 -3.5954168 -2.5188298 -0.77007115 0.91955447 2.0647571 2.7103145 2.2848151 0.85278153 -0.94556153 -2.2908337 -2.9867518][-4.2209339 -3.7244372 -3.7470207 -3.8109589 -3.6279659 -2.7467103 -1.1199127 0.78999305 2.2538502 3.1279371 2.7518156 1.258014 -0.57836688 -1.8781645 -2.5879555][-3.891192 -3.2669413 -3.2989814 -3.4913955 -3.4903035 -2.878962 -1.5202786 0.376482 1.9709833 2.9336641 2.6262653 1.2259283 -0.52789116 -1.7947088 -2.5240242][-3.777267 -3.056673 -3.0910044 -3.3681703 -3.5329161 -3.2688973 -2.3686779 -0.76668608 0.74685669 1.760535 1.6709564 0.526052 -0.98423922 -2.0971768 -2.7061591][-3.7977815 -3.0278 -3.0397129 -3.3290229 -3.5805938 -3.6473355 -3.2774181 -2.1602106 -0.95033729 -0.020875216 0.075367212 -0.70162213 -1.7815739 -2.5737636 -2.9668827][-3.9247394 -3.1538861 -3.0993562 -3.263206 -3.4148433 -3.6182137 -3.622232 -3.003612 -2.2345614 -1.5198653 -1.326931 -1.7455418 -2.4122684 -2.989264 -3.2391133][-3.9343309 -3.1804228 -3.0945129 -3.215611 -3.3379917 -3.6298728 -3.87042 -3.694778 -3.309845 -2.7442179 -2.4365478 -2.5418346 -2.9016368 -3.2820234 -3.3889399][-3.7996974 -3.0211651 -2.87988 -3.0064821 -3.1838465 -3.5361085 -3.903307 -4.0446949 -3.9773171 -3.5939608 -3.2908008 -3.2230434 -3.3174973 -3.4686153 -3.4537284]]...]
INFO - root - 2017-12-15 09:20:23.589909: step 51410, loss = 0.24, batch loss = 0.20 (34.7 examples/sec; 0.231 sec/batch; 18h:00m:23s remains)
INFO - root - 2017-12-15 09:20:25.885929: step 51420, loss = 0.28, batch loss = 0.24 (35.2 examples/sec; 0.227 sec/batch; 17h:45m:28s remains)
INFO - root - 2017-12-15 09:20:28.211471: step 51430, loss = 0.19, batch loss = 0.16 (32.9 examples/sec; 0.243 sec/batch; 18h:57m:28s remains)
INFO - root - 2017-12-15 09:20:30.545767: step 51440, loss = 0.29, batch loss = 0.26 (34.8 examples/sec; 0.230 sec/batch; 17h:56m:03s remains)
INFO - root - 2017-12-15 09:20:32.864749: step 51450, loss = 0.23, batch loss = 0.20 (35.3 examples/sec; 0.226 sec/batch; 17h:40m:33s remains)
INFO - root - 2017-12-15 09:20:35.138445: step 51460, loss = 0.24, batch loss = 0.21 (34.0 examples/sec; 0.236 sec/batch; 18h:23m:10s remains)
INFO - root - 2017-12-15 09:20:37.430607: step 51470, loss = 0.30, batch loss = 0.26 (35.1 examples/sec; 0.228 sec/batch; 17h:47m:00s remains)
INFO - root - 2017-12-15 09:20:39.719580: step 51480, loss = 0.21, batch loss = 0.17 (35.9 examples/sec; 0.223 sec/batch; 17h:23m:46s remains)
INFO - root - 2017-12-15 09:20:41.975250: step 51490, loss = 0.21, batch loss = 0.17 (36.2 examples/sec; 0.221 sec/batch; 17h:14m:44s remains)
INFO - root - 2017-12-15 09:20:44.231019: step 51500, loss = 0.23, batch loss = 0.20 (34.6 examples/sec; 0.232 sec/batch; 18h:04m:21s remains)
2017-12-15 09:20:44.526567: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.431983 -2.2269084 -2.7837617 -3.1310716 -2.5455463 -1.3380854 -0.55452812 -1.2811236 -3.0064557 -3.9797051 -4.196197 -3.8189979 -3.0113997 -1.4837102 -0.58539343][-2.5110869 -1.3002987 -1.9488274 -2.4132555 -1.8617368 -0.76014543 -0.091621876 -0.99096036 -2.7949605 -3.844101 -4.3572993 -4.3551216 -3.9485235 -2.7164321 -1.9116461][-1.4823368 0.24724913 0.10069513 -0.059633732 0.57339525 1.3532789 1.5089626 0.26485562 -1.8899242 -3.4384303 -4.6103239 -5.1455774 -5.1250329 -4.3045177 -3.6761985][-1.4172645 1.1816304 1.5932698 1.6451981 2.2405455 2.6394074 2.3572395 1.0446105 -0.92756343 -2.6559491 -4.1324749 -4.9439878 -5.159966 -4.8793774 -4.79916][-2.6339574 0.9536283 1.7083256 1.8301771 2.2710311 2.4388816 2.2111256 1.3774636 -0.015711784 -1.274568 -2.7182577 -3.6710718 -4.1830649 -4.3854008 -4.7703896][-3.3764925 -0.25607073 0.77124715 1.1201794 1.613714 2.0294187 2.3763893 2.2001789 1.2535572 0.21764898 -1.2981632 -2.3166506 -3.0129406 -3.5295386 -4.0403862][-4.3269334 -2.2610517 -1.1392661 -0.3925128 0.48420954 1.3674397 2.4067786 2.8785207 2.4747479 1.6592789 0.14678288 -0.79999816 -1.3429958 -1.9861164 -2.7513239][-5.8785057 -4.2985811 -3.2588277 -2.1882157 -0.87090635 0.4040103 1.8008206 2.7497122 2.8337824 2.2747681 0.95263767 0.24201536 -0.1829927 -0.95646966 -1.8349264][-6.9210482 -5.8835297 -5.1319456 -4.1333642 -2.8229027 -1.357693 0.21059799 1.4694259 1.8016455 1.4329743 0.4531548 -0.014845133 -0.39799893 -1.250607 -2.0314696][-7.5410175 -6.83543 -6.4778004 -5.8546963 -4.8412752 -3.5506411 -2.0951362 -0.84358239 -0.4582684 -0.74118304 -1.2929583 -1.2944221 -1.338544 -1.979794 -2.6135051][-7.7251816 -7.1744728 -7.11835 -6.7455168 -5.8888941 -4.9271145 -3.9313297 -2.9806011 -2.8540459 -3.2676399 -3.4203124 -2.9735858 -2.695209 -3.0017667 -3.3474524][-7.1736078 -6.5076361 -6.3508806 -5.8930259 -5.2720919 -4.7981339 -4.3361926 -3.9398146 -4.361495 -4.9143205 -4.8975282 -4.3848038 -4.035481 -4.2020664 -4.372283][-6.1771374 -5.1365523 -4.7127786 -4.1696053 -3.8419516 -3.7442756 -3.7176886 -4.0934138 -5.1170654 -5.8920827 -5.9042912 -5.4339132 -4.9611073 -4.9073606 -4.934948][-5.0266461 -3.8137994 -3.3020053 -2.7170079 -2.4461598 -2.4529436 -2.7084765 -3.495018 -4.796958 -5.7967062 -6.0444026 -5.7037153 -5.2648315 -5.0513906 -4.8882332][-4.2078819 -2.9209857 -2.18378 -1.4597764 -1.2567637 -1.3441589 -1.725611 -2.7916636 -4.3173714 -5.3439522 -5.7218895 -5.4843054 -5.057445 -4.7168026 -4.3887682]]...]
INFO - root - 2017-12-15 09:20:46.786303: step 51510, loss = 0.20, batch loss = 0.17 (35.5 examples/sec; 0.225 sec/batch; 17h:34m:14s remains)
INFO - root - 2017-12-15 09:20:49.056403: step 51520, loss = 0.34, batch loss = 0.31 (35.7 examples/sec; 0.224 sec/batch; 17h:30m:13s remains)
INFO - root - 2017-12-15 09:20:51.309462: step 51530, loss = 0.22, batch loss = 0.19 (35.9 examples/sec; 0.223 sec/batch; 17h:23m:48s remains)
INFO - root - 2017-12-15 09:20:53.624307: step 51540, loss = 0.18, batch loss = 0.15 (35.2 examples/sec; 0.227 sec/batch; 17h:44m:13s remains)
INFO - root - 2017-12-15 09:20:55.908955: step 51550, loss = 0.20, batch loss = 0.16 (35.7 examples/sec; 0.224 sec/batch; 17h:28m:54s remains)
INFO - root - 2017-12-15 09:20:58.210704: step 51560, loss = 0.25, batch loss = 0.22 (36.5 examples/sec; 0.219 sec/batch; 17h:06m:17s remains)
INFO - root - 2017-12-15 09:21:00.449422: step 51570, loss = 0.24, batch loss = 0.21 (36.2 examples/sec; 0.221 sec/batch; 17h:16m:09s remains)
INFO - root - 2017-12-15 09:21:02.736051: step 51580, loss = 0.18, batch loss = 0.15 (36.4 examples/sec; 0.220 sec/batch; 17h:09m:39s remains)
INFO - root - 2017-12-15 09:21:04.997996: step 51590, loss = 0.19, batch loss = 0.15 (36.5 examples/sec; 0.219 sec/batch; 17h:05m:04s remains)
INFO - root - 2017-12-15 09:21:07.292168: step 51600, loss = 0.15, batch loss = 0.12 (35.3 examples/sec; 0.227 sec/batch; 17h:42m:15s remains)
2017-12-15 09:21:07.609767: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8534029 -4.342905 -4.8055725 -5.2320461 -5.1795053 -4.78117 -4.7051411 -4.7333975 -4.8157406 -5.0820332 -5.2424364 -5.0234175 -4.4064407 -4.115726 -4.246511][-4.8831215 -5.015996 -5.8291254 -6.469574 -6.3451796 -5.7240887 -5.2695284 -5.1810341 -5.4001102 -5.7692862 -5.934864 -5.5918322 -4.6963239 -4.0751667 -3.7796969][-5.9730787 -4.9382744 -5.7880306 -6.5587349 -6.402936 -5.6259232 -4.6292305 -4.2349339 -4.6962485 -5.3387027 -5.806139 -5.7252994 -4.8772774 -4.1344576 -3.4151073][-6.2702579 -4.2210007 -4.8550725 -5.614964 -5.2481408 -4.1562481 -2.5735321 -2.0148132 -2.9322221 -4.1452112 -5.147006 -5.50815 -4.9042416 -4.1695333 -3.2206981][-6.4054794 -3.6085272 -3.85724 -4.2687798 -3.2376838 -1.537102 0.54683614 1.126884 -0.24830747 -2.2532167 -4.0040951 -4.9693851 -4.6730137 -4.1007919 -3.18542][-5.9757261 -3.1846058 -3.0136604 -2.8497419 -0.80609953 1.7997777 4.3831062 4.9740133 3.1193278 0.178823 -2.4099076 -4.0484962 -4.307435 -3.9250579 -3.2165451][-5.3581696 -3.2041357 -2.6661119 -1.8764596 1.0713017 4.5903234 7.5176449 8.1299791 5.9456396 2.2236903 -1.1868827 -3.355916 -4.1330009 -3.7725711 -3.36202][-6.1823974 -4.317873 -3.6085253 -2.303679 1.2108791 5.4053335 8.4866762 9.2518234 7.0832233 3.0675204 -0.81688941 -3.19425 -4.1712675 -3.8371658 -3.6776583][-7.5800266 -6.3875952 -5.8525419 -4.4405456 -0.93684244 3.4060137 6.559063 7.8308268 6.2449207 2.6662242 -1.2809327 -3.5647063 -4.6728 -4.3823624 -4.3111248][-8.76567 -7.9106941 -7.6890392 -6.6050367 -3.8633268 -0.14353085 2.8636873 4.6275454 4.0909529 1.577714 -1.912982 -3.9942336 -5.1649961 -5.0216722 -4.9126129][-9.1583357 -8.2496185 -8.4543743 -8.0345993 -6.3356743 -3.614758 -0.96813869 1.0310247 1.3721316 0.058570147 -2.5050142 -4.3268394 -5.5017042 -5.4846506 -5.196425][-8.8051147 -7.6287909 -8.3715115 -8.7531824 -7.9903479 -6.2808647 -4.1978126 -2.3204756 -1.4990034 -1.8607628 -3.3964109 -4.8124003 -6.015995 -6.0720711 -5.56509][-7.6110706 -6.2839775 -7.7131529 -8.8551617 -8.6258278 -7.7010689 -6.50224 -5.2377377 -4.273016 -3.9394665 -4.6938424 -5.8943968 -6.988986 -7.1304293 -6.5258522][-6.0742836 -4.4723473 -6.4389925 -8.1930943 -8.4846373 -8.3439684 -8.1493492 -7.5105848 -6.60176 -5.9304457 -6.2281847 -7.1949482 -7.9575338 -8.004199 -7.4350529][-4.8852148 -2.7560468 -4.74218 -6.7792521 -7.5271864 -8.1190987 -8.7461443 -8.6011372 -7.98228 -7.4362698 -7.6079874 -8.1721239 -8.5315313 -8.31163 -7.6749344]]...]
INFO - root - 2017-12-15 09:21:09.871527: step 51610, loss = 0.27, batch loss = 0.24 (35.9 examples/sec; 0.223 sec/batch; 17h:23m:27s remains)
INFO - root - 2017-12-15 09:21:12.134863: step 51620, loss = 0.21, batch loss = 0.18 (35.3 examples/sec; 0.227 sec/batch; 17h:40m:35s remains)
INFO - root - 2017-12-15 09:21:14.381169: step 51630, loss = 0.40, batch loss = 0.37 (35.4 examples/sec; 0.226 sec/batch; 17h:39m:09s remains)
INFO - root - 2017-12-15 09:21:16.616726: step 51640, loss = 0.25, batch loss = 0.22 (36.4 examples/sec; 0.220 sec/batch; 17h:10m:00s remains)
INFO - root - 2017-12-15 09:21:18.919808: step 51650, loss = 0.36, batch loss = 0.32 (34.3 examples/sec; 0.233 sec/batch; 18h:10m:55s remains)
INFO - root - 2017-12-15 09:21:21.186997: step 51660, loss = 0.23, batch loss = 0.19 (35.3 examples/sec; 0.227 sec/batch; 17h:41m:49s remains)
INFO - root - 2017-12-15 09:21:23.475051: step 51670, loss = 0.32, batch loss = 0.28 (35.6 examples/sec; 0.225 sec/batch; 17h:31m:40s remains)
INFO - root - 2017-12-15 09:21:25.761417: step 51680, loss = 0.28, batch loss = 0.25 (34.5 examples/sec; 0.232 sec/batch; 18h:06m:32s remains)
INFO - root - 2017-12-15 09:21:28.107524: step 51690, loss = 0.21, batch loss = 0.18 (32.7 examples/sec; 0.244 sec/batch; 19h:03m:37s remains)
INFO - root - 2017-12-15 09:21:30.446721: step 51700, loss = 0.19, batch loss = 0.16 (33.3 examples/sec; 0.240 sec/batch; 18h:43m:34s remains)
2017-12-15 09:21:30.844959: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9919558 -3.6612525 -3.3178608 -3.1348395 -3.0642433 -2.7689829 -2.291132 -2.6547577 -3.4200616 -3.7956221 -3.5530958 -2.6450019 -2.2520688 -2.3164244 -2.9580052][-4.721375 -4.8845763 -4.474885 -4.1334858 -3.8969264 -3.2839246 -2.3711421 -2.5396638 -3.3918467 -4.0181794 -4.0996761 -3.5731835 -3.6231213 -3.7555137 -4.2368445][-6.0907307 -5.5895767 -5.1904545 -4.811183 -4.4865294 -3.6765616 -2.4136996 -2.2722366 -2.9311156 -3.6099887 -3.9835553 -4.0389071 -4.5759192 -4.8194962 -5.1305113][-6.5112152 -5.451273 -5.0431852 -4.6591067 -4.3081012 -3.3269255 -1.736981 -1.1868024 -1.4522359 -2.119112 -2.8462534 -3.5575426 -4.7174225 -5.2728834 -5.5821657][-6.132659 -4.746911 -4.4210372 -4.01304 -3.5791519 -2.4573193 -0.4890691 0.64906454 0.95007157 0.32628131 -0.75844753 -2.0605302 -3.8115473 -4.7859764 -5.3354869][-5.1266127 -3.8336382 -3.7166429 -3.4150543 -2.9001698 -1.5730181 0.80343676 2.5932252 3.4459417 2.8360374 1.3612623 -0.50025249 -2.7092323 -4.0208654 -4.7899666][-4.3496695 -3.4695754 -3.5528903 -3.2889771 -2.4408941 -0.47592914 2.7394035 5.4660225 6.8582563 5.9653025 3.8095376 1.1216242 -1.5807109 -3.2751369 -4.2185335][-4.4550281 -3.5951338 -3.8815567 -3.7365997 -2.7557447 -0.48889732 3.1105335 6.3717375 8.0399714 6.9694738 4.4018793 1.3231013 -1.362437 -3.2166977 -4.1703205][-4.9728823 -4.0616903 -4.5430565 -4.5677719 -3.7190719 -1.6395988 1.659801 4.8046665 6.3774242 5.3778982 2.9256709 0.071885586 -2.2004561 -3.9217958 -4.6690578][-5.0559034 -4.0403728 -4.6196008 -4.8107634 -4.2235136 -2.579726 0.087939024 2.8299391 4.2177229 3.421653 1.3526075 -0.984063 -2.896111 -4.473021 -5.0698872][-4.947772 -3.9426165 -4.5495367 -4.8782907 -4.6571989 -3.6285465 -1.8398494 0.2849648 1.4016395 0.87486744 -0.65986073 -2.2616527 -3.7455111 -4.9743042 -5.3904305][-4.9825964 -4.114399 -4.6873379 -5.0863342 -5.2292275 -4.7826066 -3.8310924 -2.3302467 -1.4205536 -1.5947928 -2.5596607 -3.5375876 -4.7531962 -5.6679382 -5.9595604][-5.5899696 -4.8732777 -5.3535328 -5.7141056 -6.0164285 -5.8651891 -5.4115877 -4.3660364 -3.6536741 -3.5845952 -4.1226158 -4.6985884 -5.9410949 -6.7460423 -7.0211439][-6.4907379 -5.8762274 -6.2462873 -6.55313 -6.9084263 -6.8862591 -6.6274281 -5.82227 -5.18073 -4.9414949 -5.2215195 -5.693387 -7.0802317 -7.89338 -8.1924562][-7.2886438 -6.6785374 -6.8940392 -7.110117 -7.462595 -7.5709467 -7.4429941 -6.83358 -6.262332 -6.0010834 -6.2483225 -6.8065586 -8.2788849 -9.0419025 -9.2210312]]...]
INFO - root - 2017-12-15 09:21:33.134776: step 51710, loss = 0.28, batch loss = 0.25 (35.9 examples/sec; 0.223 sec/batch; 17h:22m:08s remains)
INFO - root - 2017-12-15 09:21:35.474219: step 51720, loss = 0.19, batch loss = 0.16 (35.0 examples/sec; 0.228 sec/batch; 17h:48m:51s remains)
INFO - root - 2017-12-15 09:21:37.727442: step 51730, loss = 0.27, batch loss = 0.24 (34.3 examples/sec; 0.233 sec/batch; 18h:10m:05s remains)
INFO - root - 2017-12-15 09:21:40.005321: step 51740, loss = 0.26, batch loss = 0.23 (34.9 examples/sec; 0.229 sec/batch; 17h:52m:04s remains)
INFO - root - 2017-12-15 09:21:42.329754: step 51750, loss = 0.39, batch loss = 0.35 (35.3 examples/sec; 0.227 sec/batch; 17h:41m:23s remains)
INFO - root - 2017-12-15 09:21:44.668035: step 51760, loss = 0.23, batch loss = 0.20 (34.0 examples/sec; 0.235 sec/batch; 18h:20m:01s remains)
INFO - root - 2017-12-15 09:21:47.012646: step 51770, loss = 0.21, batch loss = 0.18 (31.0 examples/sec; 0.258 sec/batch; 20h:08m:26s remains)
INFO - root - 2017-12-15 09:21:49.328724: step 51780, loss = 0.26, batch loss = 0.23 (35.3 examples/sec; 0.227 sec/batch; 17h:40m:39s remains)
INFO - root - 2017-12-15 09:21:51.611557: step 51790, loss = 0.23, batch loss = 0.20 (34.5 examples/sec; 0.232 sec/batch; 18h:05m:58s remains)
INFO - root - 2017-12-15 09:21:53.955055: step 51800, loss = 0.29, batch loss = 0.26 (32.8 examples/sec; 0.244 sec/batch; 19h:00m:47s remains)
2017-12-15 09:21:54.356251: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9066582 -4.6103845 -5.2512794 -5.592309 -5.6903849 -5.3597727 -4.8301649 -4.4319615 -4.4356432 -4.3305187 -4.2411613 -4.2102237 -4.0598884 -3.6707547 -3.3428288][-4.3932266 -5.4227333 -5.9250989 -6.0470982 -5.9341545 -5.492187 -5.0064836 -4.8412008 -5.1070628 -5.1989183 -5.0592227 -4.7632971 -4.34525 -3.7825556 -3.4734774][-5.6774969 -5.7004557 -5.80159 -5.6696029 -5.3736973 -4.8123255 -4.2976708 -4.2500191 -4.630312 -4.8831749 -4.8943329 -4.6578021 -4.2463942 -3.7797568 -3.6502867][-6.0588455 -5.2519269 -4.9722381 -4.5640125 -4.0993271 -3.4814434 -3.0504308 -3.230823 -3.7471402 -4.1834211 -4.3255134 -4.1051421 -3.7257779 -3.444387 -3.5772674][-5.4398828 -4.102066 -3.6202121 -3.1091385 -2.5104942 -1.6480515 -1.137308 -1.4584049 -2.2073147 -3.0594194 -3.6041925 -3.6896429 -3.5305164 -3.3860219 -3.4788113][-4.148478 -2.3666904 -1.9420989 -1.6114382 -0.90290773 0.45335817 1.4139152 1.0881994 0.0042443275 -1.3610735 -2.3642831 -2.84268 -3.0552368 -3.1383703 -3.0774863][-2.5358992 -0.96101427 -0.65118563 -0.44735026 0.52591848 2.3900163 3.7889531 3.4713047 1.9563229 0.014033794 -1.4924061 -2.3604159 -2.8671255 -3.1420214 -2.8845668][-1.6240971 0.21896243 0.23432446 0.14874625 1.0035942 2.9309461 4.6299047 4.5892124 2.9618227 0.59600735 -1.3545628 -2.4878795 -3.05311 -3.344615 -2.8655694][-0.66833949 1.3038552 0.85726047 0.27343726 0.57270336 1.8577478 3.3227103 3.5465057 2.2666562 0.13861585 -1.6540529 -2.6659622 -3.2158875 -3.6343143 -3.0777605][-0.0683198 1.9792058 1.2323234 0.28190064 -0.026837349 0.506974 1.5001383 1.9727461 1.1659489 -0.5471276 -2.14899 -3.0749516 -3.5386043 -3.8730838 -3.2872343][0.050412655 2.0865915 1.3007741 0.26544046 -0.51438618 -0.615119 -0.0023510456 0.53422 0.17479467 -1.0949365 -2.4966648 -3.4097247 -3.815356 -4.0137868 -3.4558215][-1.1027129 0.8011713 0.36671066 -0.27144122 -0.99848938 -1.4501563 -1.3470283 -1.1317222 -1.4358749 -2.2674274 -3.2380266 -4.0399051 -4.3467312 -4.2643089 -3.6889629][-2.7894394 -1.1163857 -1.0980773 -1.181174 -1.6240569 -2.1715102 -2.4344134 -2.6410465 -3.1084268 -3.6909537 -4.3876553 -5.0119877 -5.1654692 -4.7798481 -4.076673][-4.5446992 -2.8708603 -2.4915709 -2.1900821 -2.4165099 -2.9826779 -3.5344357 -4.0515637 -4.55164 -4.9921961 -5.3541317 -5.7170954 -5.6547303 -5.0882158 -4.3598642][-6.0385189 -4.1459246 -3.4064145 -2.8223352 -2.9585066 -3.6811967 -4.4568505 -5.1206732 -5.5939302 -5.8803573 -5.9777193 -5.9182978 -5.5739479 -4.9316649 -4.230659]]...]
INFO - root - 2017-12-15 09:21:56.655671: step 51810, loss = 0.20, batch loss = 0.17 (33.9 examples/sec; 0.236 sec/batch; 18h:25m:29s remains)
INFO - root - 2017-12-15 09:21:58.953959: step 51820, loss = 0.21, batch loss = 0.17 (32.5 examples/sec; 0.246 sec/batch; 19h:10m:20s remains)
INFO - root - 2017-12-15 09:22:01.268465: step 51830, loss = 0.20, batch loss = 0.17 (34.6 examples/sec; 0.231 sec/batch; 18h:02m:18s remains)
INFO - root - 2017-12-15 09:22:03.578917: step 51840, loss = 0.24, batch loss = 0.20 (34.9 examples/sec; 0.229 sec/batch; 17h:53m:03s remains)
INFO - root - 2017-12-15 09:22:05.882474: step 51850, loss = 0.37, batch loss = 0.34 (34.6 examples/sec; 0.231 sec/batch; 18h:01m:53s remains)
INFO - root - 2017-12-15 09:22:08.189197: step 51860, loss = 0.21, batch loss = 0.18 (34.7 examples/sec; 0.230 sec/batch; 17h:57m:05s remains)
INFO - root - 2017-12-15 09:22:10.449435: step 51870, loss = 0.28, batch loss = 0.25 (35.1 examples/sec; 0.228 sec/batch; 17h:44m:44s remains)
INFO - root - 2017-12-15 09:22:12.726910: step 51880, loss = 0.21, batch loss = 0.17 (34.5 examples/sec; 0.232 sec/batch; 18h:03m:37s remains)
INFO - root - 2017-12-15 09:22:14.996018: step 51890, loss = 0.31, batch loss = 0.27 (35.2 examples/sec; 0.227 sec/batch; 17h:42m:45s remains)
INFO - root - 2017-12-15 09:22:17.320383: step 51900, loss = 0.31, batch loss = 0.28 (34.2 examples/sec; 0.234 sec/batch; 18h:13m:45s remains)
2017-12-15 09:22:17.725437: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.227376 -6.9458227 -7.0524521 -7.2882423 -7.4663076 -6.923171 -6.25638 -6.4160929 -6.9686003 -7.5442076 -8.6341114 -9.1774416 -9.30508 -8.4785433 -7.0092096][-5.0270338 -6.08172 -6.3436556 -6.7916851 -7.0106163 -6.4998789 -5.9029665 -6.2691298 -7.1402111 -8.012825 -9.2836094 -10.014461 -10.183087 -9.2738 -7.7365961][-4.8993921 -5.2174759 -5.3677797 -5.9114418 -6.0461421 -5.2865896 -4.6261816 -5.1786079 -6.4096527 -7.7039375 -9.30259 -10.312195 -10.628703 -9.763382 -8.2395267][-4.0354419 -3.8283784 -3.9941607 -4.6435056 -4.8176355 -3.9578955 -3.258239 -3.8800344 -5.40889 -7.1717796 -9.2084808 -10.483064 -10.943707 -10.224026 -8.7486591][-3.0889833 -2.3481591 -2.5142703 -3.0624802 -3.2016816 -2.0654182 -1.0636225 -1.4767423 -3.2258768 -5.3657827 -7.7821331 -9.432622 -10.289822 -10.081136 -8.9796352][-2.7194905 -1.6159701 -1.6540151 -1.9130828 -1.7882253 -0.11226082 1.5793533 1.7286062 0.0075726509 -2.3286679 -4.9730005 -7.0156097 -8.4601574 -9.0549831 -8.5177774][-1.5308282 -0.255404 -0.17209363 -0.15174079 0.11360717 1.9244988 3.8863347 4.397892 2.6454923 0.061897516 -2.8276207 -5.1234527 -7.0372477 -8.1521387 -7.8679972][-1.5189772 0.21837187 0.59193039 0.91473269 1.3917751 3.3301847 5.392231 6.131856 4.47225 1.8161738 -1.247906 -3.8324513 -6.1004238 -7.498148 -7.3423624][-3.7897487 -1.7447851 -1.1583117 -0.76876104 -0.2086699 1.8911979 4.0552778 5.126811 3.9873693 1.7715943 -0.9726218 -3.517417 -5.8800826 -7.3354721 -7.1865034][-5.4162841 -3.3739438 -2.8617072 -2.650394 -2.2771113 -0.46316552 1.2325997 2.0457075 1.1193819 -0.74777412 -2.9328127 -4.94318 -7.0437536 -8.1176167 -7.5323906][-6.7953424 -4.6641521 -4.0984674 -3.8847637 -3.4575548 -1.7773762 -0.48571575 -0.13020062 -1.2006711 -3.0491171 -5.0576 -6.6920023 -8.4289742 -8.9694614 -7.8993044][-8.5817986 -6.5856905 -6.2027731 -6.2304945 -5.8813 -4.3552017 -3.259069 -3.0537796 -3.9816771 -5.5422111 -7.1816292 -8.3500357 -9.5574121 -9.6530647 -8.2869711][-9.0007563 -7.2463369 -7.1871777 -7.5820341 -7.5374193 -6.4706888 -5.7110538 -5.7125559 -6.5843945 -7.9159031 -9.1522284 -9.777936 -10.39015 -10.060848 -8.445879][-8.6008539 -7.2173462 -7.3158159 -7.8019028 -7.8445835 -7.0923634 -6.5033751 -6.5563612 -7.34076 -8.5080719 -9.5766087 -9.9687042 -10.212072 -9.7020092 -8.1265068][-8.1581984 -7.0934753 -7.249053 -7.7129602 -7.8380623 -7.4386692 -7.05278 -7.1647081 -7.8693314 -8.882103 -9.7766533 -10.003654 -9.8997374 -9.2599583 -7.8216429]]...]
INFO - root - 2017-12-15 09:22:20.082237: step 51910, loss = 0.19, batch loss = 0.16 (33.2 examples/sec; 0.241 sec/batch; 18h:47m:43s remains)
INFO - root - 2017-12-15 09:22:22.355172: step 51920, loss = 0.21, batch loss = 0.18 (35.7 examples/sec; 0.224 sec/batch; 17h:26m:29s remains)
INFO - root - 2017-12-15 09:22:24.627322: step 51930, loss = 0.20, batch loss = 0.17 (35.3 examples/sec; 0.226 sec/batch; 17h:39m:02s remains)
INFO - root - 2017-12-15 09:22:26.948007: step 51940, loss = 0.33, batch loss = 0.30 (35.4 examples/sec; 0.226 sec/batch; 17h:35m:27s remains)
INFO - root - 2017-12-15 09:22:29.235627: step 51950, loss = 0.19, batch loss = 0.16 (35.0 examples/sec; 0.229 sec/batch; 17h:50m:12s remains)
INFO - root - 2017-12-15 09:22:31.535499: step 51960, loss = 0.24, batch loss = 0.21 (35.2 examples/sec; 0.227 sec/batch; 17h:42m:34s remains)
INFO - root - 2017-12-15 09:22:33.824915: step 51970, loss = 0.23, batch loss = 0.19 (35.1 examples/sec; 0.228 sec/batch; 17h:44m:47s remains)
INFO - root - 2017-12-15 09:22:36.074690: step 51980, loss = 0.26, batch loss = 0.23 (35.8 examples/sec; 0.223 sec/batch; 17h:24m:53s remains)
INFO - root - 2017-12-15 09:22:38.363123: step 51990, loss = 0.20, batch loss = 0.17 (34.4 examples/sec; 0.232 sec/batch; 18h:05m:50s remains)
INFO - root - 2017-12-15 09:22:40.645958: step 52000, loss = 0.23, batch loss = 0.19 (35.3 examples/sec; 0.227 sec/batch; 17h:40m:00s remains)
2017-12-15 09:22:40.967683: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21204853 -1.0636008 -1.660025 -2.1038811 -2.2726765 -2.1263947 -1.793648 -1.6297331 -1.4025735 -1.0308508 -0.87002504 -0.99372065 -1.287629 -1.6455079 -1.6408257][-0.63955188 -1.1346664 -1.9322845 -2.4153655 -2.6800179 -2.7311966 -2.5555859 -2.5024049 -2.390801 -2.0643995 -1.7565767 -1.7946277 -2.0583136 -2.3176386 -2.1186][-1.6774402 -1.3808171 -2.1983502 -2.6314502 -2.9449666 -3.1393633 -3.0172379 -2.9436402 -2.7537417 -2.3193891 -1.8576792 -1.8871374 -2.1949508 -2.4395208 -2.2291963][-1.91708 -1.2625158 -2.1730676 -2.6820476 -3.04411 -3.3013794 -3.2461579 -3.1386926 -2.7779083 -2.2196364 -1.7918601 -1.9589744 -2.4187877 -2.7420242 -2.6254125][-2.3834703 -1.4495462 -2.37799 -2.9749663 -3.3038468 -3.4952502 -3.4520998 -3.2100463 -2.4732671 -1.6257515 -1.1388737 -1.3246027 -1.9081442 -2.430994 -2.5725324][-3.0990987 -1.9615028 -2.6832271 -3.1183331 -3.2185476 -3.1143312 -2.9370253 -2.484853 -1.3988018 -0.303589 0.23273659 0.028360367 -0.67867744 -1.4205537 -1.9118153][-3.1867981 -2.2127976 -2.6226835 -2.835223 -2.6663597 -2.225322 -1.9313971 -1.3273368 -0.11554384 0.99325204 1.4520266 1.1974745 0.41193151 -0.5085746 -1.2685452][-3.3678892 -2.1924479 -2.3026752 -2.2915223 -1.823909 -1.1369354 -0.83419633 -0.234411 0.82984972 1.7796161 2.1641147 1.9236376 1.2068162 0.34622955 -0.34806967][-3.1044633 -1.7852957 -1.64423 -1.5111492 -1.023041 -0.34464121 -0.12893438 0.31188393 1.0162921 1.6346769 1.8973687 1.6941013 1.1542759 0.5281 0.085758686][-2.9907434 -1.5989428 -1.3655853 -1.2565958 -0.88976014 -0.28076768 -0.12138319 0.17029524 0.6036067 1.0487325 1.3219893 1.1070926 0.61898065 0.093751907 -0.24793649][-3.2099409 -1.9329765 -1.687481 -1.6545287 -1.4256423 -0.8509028 -0.73424971 -0.65908682 -0.44216907 -0.13510823 0.079636335 -0.14578128 -0.58380353 -1.0959373 -1.4253035][-3.8433161 -2.618032 -2.3442659 -2.2430463 -2.1141994 -1.5643625 -1.4494838 -1.5656925 -1.5210103 -1.3052174 -1.123009 -1.2629205 -1.5365541 -2.0267985 -2.4256034][-4.19813 -2.9110308 -2.6751175 -2.61391 -2.7616191 -2.3873079 -2.330523 -2.5066202 -2.4848819 -2.3252738 -2.2308109 -2.3391359 -2.4822526 -2.9950333 -3.4475319][-4.6526308 -3.293231 -3.0619636 -3.0383306 -3.3546786 -3.0615828 -3.023978 -3.1389844 -3.0166025 -2.7484102 -2.6957505 -2.8206458 -2.9583569 -3.5410113 -4.035388][-5.496069 -4.1764364 -3.9715531 -3.8884885 -4.156168 -3.72677 -3.5275135 -3.3871279 -3.0076849 -2.5096991 -2.3905573 -2.4432719 -2.5033891 -2.9912729 -3.3448975]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 09:22:43.303770: step 52010, loss = 0.19, batch loss = 0.16 (35.8 examples/sec; 0.223 sec/batch; 17h:24m:34s remains)
INFO - root - 2017-12-15 09:22:45.606056: step 52020, loss = 0.15, batch loss = 0.12 (34.0 examples/sec; 0.235 sec/batch; 18h:19m:10s remains)
INFO - root - 2017-12-15 09:22:47.900317: step 52030, loss = 0.21, batch loss = 0.18 (35.4 examples/sec; 0.226 sec/batch; 17h:36m:59s remains)
INFO - root - 2017-12-15 09:22:50.189950: step 52040, loss = 0.21, batch loss = 0.18 (32.3 examples/sec; 0.248 sec/batch; 19h:18m:29s remains)
INFO - root - 2017-12-15 09:22:52.469737: step 52050, loss = 0.29, batch loss = 0.26 (35.5 examples/sec; 0.225 sec/batch; 17h:32m:39s remains)
INFO - root - 2017-12-15 09:22:54.782422: step 52060, loss = 0.31, batch loss = 0.28 (35.4 examples/sec; 0.226 sec/batch; 17h:35m:27s remains)
INFO - root - 2017-12-15 09:22:57.030528: step 52070, loss = 0.20, batch loss = 0.17 (35.6 examples/sec; 0.224 sec/batch; 17h:28m:54s remains)
INFO - root - 2017-12-15 09:22:59.301099: step 52080, loss = 0.21, batch loss = 0.18 (35.5 examples/sec; 0.225 sec/batch; 17h:32m:07s remains)
INFO - root - 2017-12-15 09:23:01.587649: step 52090, loss = 0.18, batch loss = 0.15 (35.1 examples/sec; 0.228 sec/batch; 17h:45m:21s remains)
INFO - root - 2017-12-15 09:23:03.876426: step 52100, loss = 0.24, batch loss = 0.20 (35.8 examples/sec; 0.223 sec/batch; 17h:23m:09s remains)
2017-12-15 09:23:04.208295: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1751423 -5.9436674 -7.0888777 -8.271821 -8.3610535 -8.3649893 -8.702817 -8.8201408 -8.572298 -8.28795 -8.3287973 -8.7410727 -8.6866217 -8.5992565 -8.9369831][-5.1758747 -5.0862837 -6.3166552 -7.9125214 -8.5883341 -8.594347 -8.8088741 -9.0214834 -8.7657986 -8.1997271 -8.1106539 -8.5970736 -8.461339 -8.1913624 -8.6646261][-5.5068245 -3.8742747 -5.1717439 -6.9075031 -7.7491636 -7.3702059 -7.0152788 -7.1910343 -7.0145321 -6.4838505 -6.5508194 -7.3070374 -7.4516134 -7.3259935 -7.9193811][-5.3786764 -2.8417737 -4.1800942 -5.9997797 -6.7608786 -5.84538 -4.7823057 -4.7168045 -4.6496677 -4.3279057 -4.615777 -5.548696 -6.0417709 -6.2867203 -6.8668261][-5.2075806 -2.4553726 -3.6134176 -5.0555687 -5.3283472 -3.8487673 -2.0530496 -1.5572873 -1.8322794 -2.0692236 -2.631053 -3.5868268 -4.3040247 -5.0713272 -5.6577325][-4.3845429 -2.27694 -2.857233 -3.3736682 -2.71391 -0.56435752 2.0560997 3.1056135 2.2550418 0.78021 -0.66529608 -1.9815695 -2.9781928 -4.2198162 -4.8189044][-3.5766256 -2.6379333 -2.8024585 -2.4123735 -0.86394596 1.9436386 5.288291 7.0587521 5.8852482 3.5038855 1.151232 -0.85441148 -2.3078501 -3.9765496 -4.5900316][-4.5618663 -3.6726546 -3.6422553 -2.7862358 -0.72167027 2.5311787 6.3364687 8.5476084 7.4098873 4.7524529 2.0092762 -0.62083995 -2.6434071 -4.5562992 -5.1220245][-5.8011184 -4.818553 -4.6788864 -3.8986559 -2.0586889 0.95018435 4.5377369 6.6731834 5.9193439 3.6085832 1.0629234 -1.5665109 -3.6887603 -5.5448608 -6.0413113][-7.1424837 -5.8660293 -5.7817793 -5.5218568 -4.2886953 -1.9810189 0.88104081 2.7521155 2.3851025 0.61128736 -1.5332444 -3.7130291 -5.3438129 -6.841836 -7.3320761][-7.9905796 -6.3245974 -6.4770374 -7.018631 -6.49872 -4.8547039 -2.7791493 -1.1705626 -1.2479242 -2.7308679 -4.6848359 -6.4028177 -7.3758068 -8.3876791 -8.8696508][-8.33783 -6.2953815 -6.6748638 -7.8193779 -7.807291 -6.8143282 -5.6468906 -4.4823093 -4.2784529 -5.3079109 -6.9752741 -8.3448353 -8.9931517 -9.7821693 -10.195898][-8.3336277 -6.1280317 -6.6604042 -8.1238441 -8.3304749 -7.8524542 -7.4707317 -6.77569 -6.3639994 -6.9941177 -8.2775745 -9.2207966 -9.5875206 -10.198252 -10.446919][-7.6107035 -5.5595779 -6.1794405 -7.6113119 -7.9604721 -7.8000879 -7.8429642 -7.4480104 -7.0436449 -7.4045305 -8.2005568 -8.6029215 -8.6300364 -8.9265757 -8.95926][-6.3142004 -4.62879 -5.2459221 -6.2874527 -6.6409388 -6.7595844 -6.9199886 -6.7486973 -6.5102692 -6.6993637 -7.0749445 -7.0578251 -6.846756 -6.8574495 -6.7604628]]...]
INFO - root - 2017-12-15 09:23:06.521917: step 52110, loss = 0.24, batch loss = 0.21 (35.2 examples/sec; 0.227 sec/batch; 17h:40m:55s remains)
INFO - root - 2017-12-15 09:23:08.791261: step 52120, loss = 0.18, batch loss = 0.14 (35.1 examples/sec; 0.228 sec/batch; 17h:44m:09s remains)
INFO - root - 2017-12-15 09:23:11.051205: step 52130, loss = 0.21, batch loss = 0.18 (35.9 examples/sec; 0.223 sec/batch; 17h:21m:49s remains)
INFO - root - 2017-12-15 09:23:13.358466: step 52140, loss = 0.21, batch loss = 0.18 (36.0 examples/sec; 0.222 sec/batch; 17h:17m:56s remains)
INFO - root - 2017-12-15 09:23:15.629094: step 52150, loss = 0.24, batch loss = 0.21 (36.5 examples/sec; 0.219 sec/batch; 17h:03m:12s remains)
INFO - root - 2017-12-15 09:23:17.925349: step 52160, loss = 0.28, batch loss = 0.25 (35.2 examples/sec; 0.227 sec/batch; 17h:42m:18s remains)
INFO - root - 2017-12-15 09:23:20.183145: step 52170, loss = 0.22, batch loss = 0.19 (34.7 examples/sec; 0.230 sec/batch; 17h:56m:00s remains)
INFO - root - 2017-12-15 09:23:22.449840: step 52180, loss = 0.22, batch loss = 0.18 (36.0 examples/sec; 0.222 sec/batch; 17h:19m:16s remains)
INFO - root - 2017-12-15 09:23:24.709317: step 52190, loss = 0.26, batch loss = 0.23 (33.9 examples/sec; 0.236 sec/batch; 18h:21m:52s remains)
INFO - root - 2017-12-15 09:23:26.946892: step 52200, loss = 0.19, batch loss = 0.15 (35.6 examples/sec; 0.225 sec/batch; 17h:29m:01s remains)
2017-12-15 09:23:27.293003: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4267051 -5.4753466 -5.8490829 -6.1175671 -6.2540789 -6.2515497 -6.1701651 -6.0757885 -6.0993576 -6.2329483 -6.2924948 -6.2699728 -6.2172918 -6.089325 -5.9078283][-5.2984486 -6.609911 -7.0118179 -7.3152227 -7.4874926 -7.4624681 -7.3269892 -7.2143764 -7.2892809 -7.4767885 -7.538559 -7.4238224 -7.198597 -6.9027014 -6.6529875][-6.9431391 -7.7513533 -8.3219643 -8.7296591 -8.7513447 -8.3220549 -7.8204432 -7.6423578 -7.8646307 -8.3258848 -8.7399626 -8.7470951 -8.3682041 -7.8898172 -7.4253931][-7.8282433 -8.2660255 -9.0351782 -9.4035521 -8.8902893 -7.7599688 -6.8492632 -6.5377264 -6.9736996 -7.9881105 -9.0214291 -9.2936287 -9.0891638 -8.6223192 -7.9032421][-7.834981 -7.8115063 -8.452879 -8.4876432 -7.3378143 -5.5466428 -4.1342812 -3.5292988 -4.0292497 -5.6220441 -7.3664875 -8.1745605 -8.42762 -8.2831659 -7.5827713][-6.9878912 -6.7300272 -7.2040796 -7.0832224 -5.4750271 -3.1616728 -1.023963 0.33934736 -0.066575766 -2.4770222 -5.1622353 -6.6773472 -7.6226254 -7.7922173 -7.1403465][-5.3087091 -5.5167894 -6.0444255 -5.8803577 -3.9069362 -1.0157615 2.1967661 4.4589138 4.2017365 1.0578649 -2.4527304 -4.8974667 -6.4157724 -6.7222738 -5.9695072][-4.2924008 -4.4694252 -5.3338289 -5.1972523 -2.6872466 0.81429553 4.9384394 8.0813875 8.0312843 4.522975 0.40203524 -2.8573518 -4.9745336 -5.4851866 -4.8073444][-4.1567364 -4.4494219 -5.8739977 -5.7875109 -3.069416 0.45871687 4.9184084 8.44913 8.6102724 5.3366919 1.2336314 -2.5654302 -4.7950807 -5.2313871 -4.56624][-5.0201445 -5.246717 -7.016777 -7.0730877 -4.4763021 -1.4015195 2.479943 5.619525 5.8226061 3.2665088 -0.33525407 -3.8033285 -5.6615343 -5.8819876 -5.1530061][-5.994153 -6.1819754 -7.9578447 -7.9297781 -5.583703 -3.3065968 -0.43924916 1.8696921 1.9832232 0.17503214 -2.5253887 -5.3112273 -6.6074762 -6.4956379 -5.57561][-6.5252008 -6.6784954 -8.2748222 -8.26544 -6.4301262 -4.8780603 -2.9302742 -1.436115 -1.4035833 -2.4666674 -4.2498288 -5.9829741 -6.6501827 -6.44602 -5.6420851][-6.5284491 -6.5616713 -7.862812 -7.9010382 -6.5819111 -5.53154 -4.1676388 -3.1993175 -3.2096155 -3.7901692 -4.7363482 -5.6453543 -5.8907547 -5.6940436 -5.0689774][-6.3173733 -6.0982113 -6.882618 -6.8388138 -5.8725843 -5.0246325 -4.0703406 -3.4863291 -3.5208359 -3.8266845 -4.3302312 -4.7752042 -4.9655628 -4.930325 -4.6280136][-5.8610029 -5.4326639 -5.8204069 -5.663372 -4.9011817 -4.189682 -3.5001378 -3.1420944 -3.2321749 -3.4115753 -3.5860455 -3.9020383 -4.2594805 -4.3409252 -4.2434559]]...]
INFO - root - 2017-12-15 09:23:29.538485: step 52210, loss = 0.30, batch loss = 0.26 (35.1 examples/sec; 0.228 sec/batch; 17h:45m:08s remains)
INFO - root - 2017-12-15 09:23:31.826075: step 52220, loss = 0.19, batch loss = 0.16 (33.6 examples/sec; 0.238 sec/batch; 18h:33m:44s remains)
INFO - root - 2017-12-15 09:23:34.095965: step 52230, loss = 0.21, batch loss = 0.17 (37.1 examples/sec; 0.216 sec/batch; 16h:48m:16s remains)
INFO - root - 2017-12-15 09:23:36.378872: step 52240, loss = 0.25, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 17h:26m:39s remains)
INFO - root - 2017-12-15 09:23:38.633195: step 52250, loss = 0.28, batch loss = 0.25 (36.1 examples/sec; 0.221 sec/batch; 17h:14m:05s remains)
INFO - root - 2017-12-15 09:23:40.903980: step 52260, loss = 0.22, batch loss = 0.19 (34.6 examples/sec; 0.231 sec/batch; 18h:00m:51s remains)
INFO - root - 2017-12-15 09:23:43.219705: step 52270, loss = 0.35, batch loss = 0.32 (35.1 examples/sec; 0.228 sec/batch; 17h:45m:49s remains)
INFO - root - 2017-12-15 09:23:45.470217: step 52280, loss = 0.25, batch loss = 0.22 (35.0 examples/sec; 0.229 sec/batch; 17h:48m:19s remains)
INFO - root - 2017-12-15 09:23:47.736700: step 52290, loss = 0.23, batch loss = 0.20 (34.7 examples/sec; 0.231 sec/batch; 17h:56m:46s remains)
INFO - root - 2017-12-15 09:23:50.044936: step 52300, loss = 0.26, batch loss = 0.23 (35.8 examples/sec; 0.223 sec/batch; 17h:23m:34s remains)
2017-12-15 09:23:50.357917: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8264322 -6.8359718 -8.0949659 -9.07362 -9.2994022 -9.1271305 -8.6443844 -8.1440105 -7.7462626 -7.4950151 -7.7570105 -7.8485184 -7.6570158 -7.1966553 -6.6248188][-7.6410103 -8.0925674 -9.2978621 -10.09557 -10.039702 -9.7468462 -8.9201651 -8.0654917 -7.7804365 -7.7241335 -7.995801 -7.8121157 -7.922802 -8.0166883 -7.5466805][-9.5215416 -8.2691746 -9.3342838 -9.9128923 -9.4611092 -9.049942 -8.0568027 -7.0450668 -6.8791313 -7.0327606 -7.4343948 -7.0442266 -7.4545212 -8.0757494 -7.85336][-9.1505651 -6.8416729 -7.8431149 -8.4033146 -7.5696259 -7.0187187 -6.1548195 -5.2718239 -5.2540135 -5.505733 -6.040617 -5.43997 -5.9582539 -6.871829 -6.9673367][-8.03562 -5.114172 -5.9424758 -6.2582378 -4.6807079 -3.4932675 -2.3529539 -1.474885 -1.5405711 -1.8929305 -2.6697931 -2.2048013 -2.9638448 -4.2611985 -4.9741974][-6.5283937 -3.936141 -4.8528562 -5.1628566 -2.9835377 -0.96761227 0.70054317 1.6627114 1.5192285 0.79268432 -0.5045836 -0.69000793 -1.7505598 -3.2925782 -4.4401445][-4.2167521 -2.4744811 -3.5754173 -4.1091061 -1.6791921 0.88093543 3.0063837 4.0503092 3.9242647 2.6471097 0.79143333 -0.033075571 -1.461688 -3.2299659 -4.493525][-3.3782706 -1.7092916 -2.886466 -3.3937335 -0.74266446 2.3970635 5.1565924 6.5660238 6.6196117 4.9348841 2.4760544 0.79805493 -1.3389957 -3.60712 -4.9649916][-4.04764 -2.0637367 -3.2350461 -3.8872972 -1.4647832 1.8387883 4.947154 6.4941511 6.587265 4.8009958 2.0776832 -0.22508287 -2.9156971 -5.43542 -6.5921688][-5.1051831 -2.5826504 -3.6417294 -4.6891937 -3.0286081 -0.20359564 2.4559386 3.6362903 3.7003086 2.1587698 -0.39640355 -2.6450744 -5.13893 -7.4198895 -8.1942692][-6.9438515 -4.2426343 -5.3599291 -6.5321407 -5.4166932 -3.1275234 -0.94944239 -0.0080809593 0.21642399 -0.679677 -2.8776152 -4.9392323 -6.9916277 -8.9311666 -9.4102144][-9.4057922 -7.0489388 -8.3296614 -9.5239954 -8.8662786 -7.1284981 -5.4485693 -4.5295968 -4.1605883 -4.4682865 -5.9815178 -7.5971832 -9.005353 -10.410618 -10.482235][-10.573743 -8.79367 -10.19523 -11.475478 -11.353787 -10.428377 -9.4379358 -8.7430439 -8.2240982 -8.2528753 -9.2088728 -10.096756 -10.746794 -11.503314 -11.192444][-10.016169 -8.9002132 -10.326707 -11.438411 -11.678065 -11.402456 -10.980029 -10.429591 -9.8890285 -9.7310677 -10.28673 -10.79486 -11.038021 -11.351258 -10.817004][-8.5154667 -7.7698164 -8.871192 -9.6302261 -9.9525118 -10.060118 -9.9977121 -9.5856352 -9.1807146 -9.102541 -9.3331547 -9.4518251 -9.517951 -9.5259018 -8.9803934]]...]
INFO - root - 2017-12-15 09:23:52.642331: step 52310, loss = 0.21, batch loss = 0.18 (37.1 examples/sec; 0.215 sec/batch; 16h:45m:51s remains)
INFO - root - 2017-12-15 09:23:54.927334: step 52320, loss = 0.21, batch loss = 0.18 (35.4 examples/sec; 0.226 sec/batch; 17h:36m:42s remains)
INFO - root - 2017-12-15 09:23:57.199681: step 52330, loss = 0.16, batch loss = 0.13 (36.4 examples/sec; 0.220 sec/batch; 17h:05m:40s remains)
INFO - root - 2017-12-15 09:23:59.464509: step 52340, loss = 0.29, batch loss = 0.26 (35.3 examples/sec; 0.227 sec/batch; 17h:38m:23s remains)
INFO - root - 2017-12-15 09:24:01.715476: step 52350, loss = 0.19, batch loss = 0.16 (35.2 examples/sec; 0.227 sec/batch; 17h:40m:50s remains)
INFO - root - 2017-12-15 09:24:03.976505: step 52360, loss = 0.27, batch loss = 0.24 (32.7 examples/sec; 0.244 sec/batch; 19h:00m:48s remains)
INFO - root - 2017-12-15 09:24:06.268121: step 52370, loss = 0.19, batch loss = 0.16 (35.8 examples/sec; 0.223 sec/batch; 17h:21m:56s remains)
INFO - root - 2017-12-15 09:24:08.530545: step 52380, loss = 0.22, batch loss = 0.18 (35.9 examples/sec; 0.223 sec/batch; 17h:19m:20s remains)
INFO - root - 2017-12-15 09:24:10.863697: step 52390, loss = 0.19, batch loss = 0.15 (34.3 examples/sec; 0.233 sec/batch; 18h:09m:36s remains)
INFO - root - 2017-12-15 09:24:13.130788: step 52400, loss = 0.25, batch loss = 0.22 (32.0 examples/sec; 0.250 sec/batch; 19h:26m:42s remains)
2017-12-15 09:24:13.556960: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7490611 -4.0215521 -3.5329759 -2.563453 -1.942144 -2.6366923 -3.8074839 -4.9557171 -5.626801 -6.070075 -5.9654808 -5.5225148 -4.5272174 -3.7432885 -3.6618955][-2.9332652 -4.0102315 -4.2476768 -3.7610741 -3.0705147 -3.2459092 -3.7862887 -4.3820076 -4.8604803 -5.3844385 -5.6709127 -5.6363258 -4.9312921 -4.4037733 -4.5162277][-2.7742605 -3.8846018 -4.8574648 -4.8581409 -3.9599082 -3.2808256 -2.8835707 -2.8083065 -3.1454744 -3.9325848 -4.7885895 -5.31489 -5.0569038 -4.7807579 -5.0030203][-2.301825 -3.5219989 -5.2045145 -5.6746135 -4.4758415 -2.8871355 -1.5821838 -0.72516787 -0.83302188 -1.8984767 -3.3104677 -4.3256969 -4.3420563 -4.1119576 -4.2649374][-2.1296892 -3.1773424 -5.1339078 -5.6453471 -3.8713923 -1.4288988 0.64895582 2.0548861 1.9702456 0.36706352 -1.7492818 -3.3491821 -3.6593566 -3.4519186 -3.3864694][-2.5045938 -3.1627743 -4.799274 -4.9244137 -2.4629242 0.65425658 3.3221657 5.3189087 5.2515116 3.0533164 0.21565461 -2.0031948 -2.6777329 -2.6786866 -2.6178086][-3.2713702 -3.513382 -4.5550013 -4.0492659 -0.98918223 2.4571903 5.39917 7.7568865 7.6757488 5.0249081 1.6998248 -0.97446132 -2.0679083 -2.3734531 -2.3496015][-4.1455927 -3.9094706 -4.2512197 -3.179384 0.18203473 3.6460917 6.5644426 8.8720827 8.5330219 5.50482 1.9532316 -0.95391619 -2.469394 -3.1329162 -3.1449192][-5.2658958 -4.9934464 -5.0308876 -3.6835985 -0.43910682 2.6860902 5.260807 7.2146597 6.76194 3.9130337 0.629235 -2.1236153 -3.7348118 -4.5593309 -4.5001011][-6.2322626 -6.3189964 -6.407856 -5.1358595 -2.3375566 0.28829789 2.3702185 3.8361614 3.3407295 0.94421363 -1.8242996 -4.1416759 -5.6671171 -6.4919271 -6.1930022][-6.7280855 -7.27145 -7.4882374 -6.4753704 -4.2629914 -2.1072755 -0.44108582 0.59274483 0.12512326 -1.7389618 -3.9545708 -5.7234936 -7.0037794 -7.7562094 -7.33045][-7.1009455 -8.0420971 -8.5072489 -7.9295731 -6.4331818 -4.7927032 -3.4785857 -2.80291 -3.1753778 -4.4739046 -6.1313834 -7.4793129 -8.4914246 -9.0438185 -8.5235186][-7.7442236 -8.9236965 -9.5719147 -9.339983 -8.3881044 -7.2117796 -6.2395134 -5.8445759 -6.2074156 -7.2017632 -8.5083933 -9.5055809 -10.11778 -10.244792 -9.5278625][-7.6983461 -8.7863111 -9.4187851 -9.3970785 -8.8658743 -8.1130619 -7.4570017 -7.239666 -7.5522127 -8.2485619 -9.158824 -9.7674437 -9.9627047 -9.7376156 -8.9598522][-7.0459843 -7.6275682 -8.0040026 -8.0322895 -7.809206 -7.4744215 -7.1621237 -7.1147108 -7.3909473 -7.8357 -8.3369675 -8.6211023 -8.5545645 -8.1611881 -7.4797974]]...]
INFO - root - 2017-12-15 09:24:15.888194: step 52410, loss = 0.33, batch loss = 0.29 (34.9 examples/sec; 0.229 sec/batch; 17h:48m:43s remains)
INFO - root - 2017-12-15 09:24:18.160384: step 52420, loss = 0.23, batch loss = 0.20 (34.1 examples/sec; 0.235 sec/batch; 18h:15m:52s remains)
INFO - root - 2017-12-15 09:24:20.456354: step 52430, loss = 0.18, batch loss = 0.15 (34.6 examples/sec; 0.232 sec/batch; 18h:00m:44s remains)
INFO - root - 2017-12-15 09:24:22.780726: step 52440, loss = 0.23, batch loss = 0.20 (33.8 examples/sec; 0.237 sec/batch; 18h:24m:24s remains)
INFO - root - 2017-12-15 09:24:25.092253: step 52450, loss = 0.16, batch loss = 0.13 (35.5 examples/sec; 0.225 sec/batch; 17h:32m:01s remains)
INFO - root - 2017-12-15 09:24:27.336870: step 52460, loss = 0.20, batch loss = 0.16 (35.7 examples/sec; 0.224 sec/batch; 17h:25m:51s remains)
INFO - root - 2017-12-15 09:24:29.569800: step 52470, loss = 0.21, batch loss = 0.17 (35.3 examples/sec; 0.227 sec/batch; 17h:38m:48s remains)
INFO - root - 2017-12-15 09:24:31.853998: step 52480, loss = 0.24, batch loss = 0.21 (35.8 examples/sec; 0.224 sec/batch; 17h:23m:10s remains)
INFO - root - 2017-12-15 09:24:34.154965: step 52490, loss = 0.19, batch loss = 0.16 (34.8 examples/sec; 0.230 sec/batch; 17h:51m:33s remains)
INFO - root - 2017-12-15 09:24:36.419692: step 52500, loss = 0.23, batch loss = 0.19 (36.1 examples/sec; 0.221 sec/batch; 17h:13m:05s remains)
2017-12-15 09:24:36.783070: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0134997 -4.9990282 -5.3671961 -6.1399984 -7.1116533 -7.5673523 -7.3255215 -6.7309914 -5.6756568 -4.776989 -4.0307331 -3.1782541 -2.6273897 -2.8575332 -4.1386061][-4.5294409 -5.4678545 -5.8894091 -6.5978994 -7.3160295 -7.2381048 -6.4146414 -5.4845657 -4.3501787 -3.4987679 -2.9387102 -2.3114905 -1.8633387 -2.1297913 -3.525151][-4.912075 -5.6037507 -6.053257 -6.6952696 -7.0790424 -6.4525375 -5.3257828 -4.3341064 -3.2097592 -2.4567995 -2.0755095 -1.6668997 -1.4273092 -1.781163 -3.1910033][-5.1699667 -5.6221914 -6.1830025 -6.7439175 -6.7873173 -5.6784844 -4.3918877 -3.3804159 -2.3215089 -1.7264268 -1.578546 -1.5417314 -1.6220903 -2.133836 -3.4133906][-5.3019152 -5.4988165 -6.0431786 -6.3233576 -5.8124671 -4.035121 -2.4209039 -1.3187076 -0.33427429 0.028436661 -0.16169524 -0.51632726 -0.90140915 -1.525847 -2.6028042][-5.2336187 -5.1113262 -5.5356579 -5.4983969 -4.5587521 -2.2558596 -0.40836525 0.72173619 1.5622225 1.6560569 1.2323923 0.59132385 -0.074859142 -0.79343545 -1.6041982][-4.7529483 -4.8132124 -5.2171 -5.0214291 -3.9162917 -1.4115992 0.37018251 1.3566847 1.9461572 1.8252203 1.2513523 0.42311049 -0.39073014 -1.0320735 -1.4854672][-4.618926 -4.6091294 -4.9632969 -4.6857891 -3.530436 -1.0247629 0.655426 1.6042132 2.083143 1.9255393 1.374378 0.4860568 -0.4629389 -1.1570197 -1.5183376][-4.5084915 -4.414619 -4.7463007 -4.4324856 -3.2523079 -0.864941 0.76352 1.9055603 2.5679204 2.6069648 2.1537998 1.2029219 0.083862543 -0.73463249 -1.2008401][-4.87892 -4.6671782 -4.8927383 -4.5081282 -3.3605304 -1.3176162 0.10755658 1.3997848 2.2176921 2.4794319 2.2142451 1.45334 0.33383322 -0.58152044 -1.2287881][-5.6473279 -5.3020277 -5.51546 -5.2764769 -4.3128796 -2.676646 -1.4667848 -0.079987764 0.80097032 1.2105246 1.0899997 0.50000358 -0.55811012 -1.4319174 -2.1359091][-6.1662307 -5.9249525 -6.36425 -6.41739 -5.774127 -4.45585 -3.2522621 -1.7414998 -0.72823966 -0.32101858 -0.56016982 -1.3122609 -2.39519 -3.1290953 -3.7528868][-6.0196652 -5.90734 -6.6310368 -7.0503321 -6.8843517 -5.9743972 -4.7846026 -3.2638416 -2.305516 -2.0654583 -2.5675836 -3.4485469 -4.3984413 -4.8351412 -5.1308961][-5.4011326 -5.0741491 -5.910327 -6.6859322 -7.1435328 -6.8305407 -5.865005 -4.4960289 -3.632936 -3.4369235 -3.9489288 -4.65368 -5.3077641 -5.5509262 -5.6519423][-4.3695889 -3.6861663 -4.5849161 -5.6676407 -6.6915913 -6.9414549 -6.3263025 -5.21603 -4.4251566 -4.220294 -4.658071 -5.2036419 -5.5514436 -5.6207781 -5.5948963]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-52500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue/model.ckpt-52500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 09:24:39.522406: step 52510, loss = 0.31, batch loss = 0.28 (36.6 examples/sec; 0.219 sec/batch; 17h:00m:40s remains)
INFO - root - 2017-12-15 09:24:41.786927: step 52520, loss = 0.22, batch loss = 0.18 (34.4 examples/sec; 0.232 sec/batch; 18h:03m:44s remains)
INFO - root - 2017-12-15 09:24:44.076755: step 52530, loss = 0.18, batch loss = 0.15 (34.8 examples/sec; 0.230 sec/batch; 17h:53m:16s remains)
INFO - root - 2017-12-15 09:24:46.387060: step 52540, loss = 0.23, batch loss = 0.20 (32.1 examples/sec; 0.249 sec/batch; 19h:22m:13s remains)
INFO - root - 2017-12-15 09:24:48.644136: step 52550, loss = 0.20, batch loss = 0.16 (35.7 examples/sec; 0.224 sec/batch; 17h:26m:55s remains)
INFO - root - 2017-12-15 09:24:50.936466: step 52560, loss = 0.20, batch loss = 0.17 (34.8 examples/sec; 0.230 sec/batch; 17h:52m:19s remains)
INFO - root - 2017-12-15 09:24:53.237933: step 52570, loss = 0.22, batch loss = 0.19 (33.8 examples/sec; 0.237 sec/batch; 18h:23m:43s remains)
INFO - root - 2017-12-15 09:24:55.521279: step 52580, loss = 0.26, batch loss = 0.23 (36.6 examples/sec; 0.219 sec/batch; 17h:00m:48s remains)
INFO - root - 2017-12-15 09:24:57.820049: step 52590, loss = 0.48, batch loss = 0.45 (34.6 examples/sec; 0.231 sec/batch; 17h:59m:18s remains)
INFO - root - 2017-12-15 09:25:00.135289: step 52600, loss = 0.20, batch loss = 0.17 (33.1 examples/sec; 0.242 sec/batch; 18h:48m:00s remains)
2017-12-15 09:25:00.490587: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2109988 -4.522851 -6.0809689 -7.1438379 -7.1148252 -7.2174644 -7.4096727 -7.3762178 -7.268343 -7.527133 -7.5555077 -7.3996143 -7.2333546 -7.0503111 -6.9839649][-4.3183074 -5.6232758 -7.1243124 -8.08318 -8.1766672 -8.2698984 -8.2210388 -7.8752756 -7.4765759 -7.5104561 -7.5565615 -7.57732 -7.5698881 -7.6240273 -7.6685648][-5.6661687 -6.0136929 -7.2393293 -8.1141176 -8.2353582 -8.1041479 -7.661911 -6.9927721 -6.4586248 -6.5818911 -6.8841915 -7.0166206 -7.0082045 -7.1199074 -7.3378019][-6.3614645 -5.8655024 -6.7467976 -7.3491879 -7.0883255 -6.2748909 -5.2632313 -4.3384314 -3.8637533 -4.44842 -5.2592773 -5.7482386 -5.93827 -6.1549244 -6.5539713][-6.5530853 -5.2997141 -5.6527839 -5.7210288 -4.8970728 -3.43261 -2.0396142 -1.0266784 -0.6637429 -1.7413602 -3.2455478 -4.3372726 -4.853219 -5.1618156 -5.5482483][-5.8753443 -4.38501 -4.1958051 -3.8276031 -2.5234323 -0.55418277 1.0918214 2.1633828 2.3708866 0.69285941 -1.638082 -3.3599744 -4.0982256 -4.3606415 -4.6220036][-4.9283347 -3.6535048 -3.1630526 -2.4557431 -0.65192616 1.8472688 3.7839267 4.94983 4.8659267 2.5511091 -0.49173939 -2.8206475 -3.9067135 -4.2672458 -4.4604292][-4.95493 -3.4575758 -2.7172534 -1.5544426 0.73137617 3.544996 5.53267 6.4231215 5.8599682 3.2086298 -0.179075 -2.8931952 -4.1736441 -4.6249676 -4.7507496][-5.3062124 -3.8011878 -2.8196959 -1.2475421 1.3238053 4.1070852 5.8272905 6.2052841 5.17235 2.506274 -0.84396327 -3.5687065 -4.8605251 -5.2484741 -5.3013725][-5.6467414 -4.2803788 -3.2956824 -1.654058 0.7778151 3.1716478 4.4343987 4.3933668 3.2224929 0.83026147 -2.140877 -4.5426741 -5.645071 -5.8704004 -5.8485112][-6.0721846 -5.0061784 -4.2197332 -2.7814727 -0.77394283 0.99200726 1.7737157 1.5458214 0.46488237 -1.4755106 -3.8000469 -5.4912119 -6.0859194 -6.0373693 -5.8785572][-6.3805857 -5.5831327 -5.0885539 -4.0535359 -2.6628561 -1.5663817 -1.1944035 -1.5439593 -2.6099982 -4.0686808 -5.4565516 -6.1323757 -6.1068254 -5.8032675 -5.5515194][-6.1606193 -5.6208344 -5.4253817 -4.9010677 -4.2311888 -3.7745271 -3.7756341 -4.1437712 -4.9184284 -5.6778059 -6.1627173 -6.1126757 -5.795279 -5.4215212 -5.0655308][-5.5729866 -5.2287474 -5.2969275 -5.20201 -5.0571618 -5.0138316 -5.1632776 -5.4340796 -5.8154678 -5.9942837 -5.9524975 -5.6312633 -5.2047138 -4.7300682 -4.2720075][-4.9220333 -4.6629324 -4.8603873 -4.9579644 -5.0220633 -5.0814328 -5.1844816 -5.3807116 -5.6139493 -5.6601372 -5.4688525 -5.036602 -4.49538 -3.9545183 -3.5504618]]...]
INFO - root - 2017-12-15 09:25:02.787462: step 52610, loss = 0.21, batch loss = 0.18 (33.6 examples/sec; 0.238 sec/batch; 18h:30m:41s remains)
INFO - root - 2017-12-15 09:25:05.046845: step 52620, loss = 0.20, batch loss = 0.17 (35.0 examples/sec; 0.228 sec/batch; 17h:45m:02s remains)
INFO - root - 2017-12-15 09:25:07.326525: step 52630, loss = 0.19, batch loss = 0.16 (35.1 examples/sec; 0.228 sec/batch; 17h:42m:59s remains)
INFO - root - 2017-12-15 09:25:09.604727: step 52640, loss = 0.24, batch loss = 0.21 (35.4 examples/sec; 0.226 sec/batch; 17h:35m:25s remains)
INFO - root - 2017-12-15 09:25:11.871474: step 52650, loss = 0.22, batch loss = 0.19 (35.6 examples/sec; 0.225 sec/batch; 17h:29m:28s remains)
INFO - root - 2017-12-15 09:25:14.147087: step 52660, loss = 0.30, batch loss = 0.26 (35.8 examples/sec; 0.223 sec/batch; 17h:20m:54s remains)
INFO - root - 2017-12-15 09:25:16.394853: step 52670, loss = 0.25, batch loss = 0.22 (35.6 examples/sec; 0.225 sec/batch; 17h:27m:28s remains)
INFO - root - 2017-12-15 09:25:18.679767: step 52680, loss = 0.26, batch loss = 0.23 (34.2 examples/sec; 0.234 sec/batch; 18h:09m:45s remains)
INFO - root - 2017-12-15 09:25:20.946203: step 52690, loss = 0.25, batch loss = 0.22 (34.1 examples/sec; 0.235 sec/batch; 18h:14m:08s remains)
INFO - root - 2017-12-15 09:25:23.277805: step 52700, loss = 0.19, batch loss = 0.16 (34.4 examples/sec; 0.232 sec/batch; 18h:04m:02s remains)
2017-12-15 09:25:23.622028: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8855715 -4.9295349 -4.8478308 -5.021534 -5.7904835 -6.5112991 -6.2833595 -6.0579877 -6.4863396 -6.6342306 -6.8822327 -7.1684809 -7.2873707 -7.3343315 -7.9027472][-7.0267324 -5.6425228 -5.4883752 -5.5671892 -6.0135818 -6.6252794 -6.3133593 -6.2449389 -7.0247641 -7.4978504 -7.9374065 -8.126833 -8.0617361 -7.8793325 -8.3898582][-8.0965919 -6.3079929 -6.1471186 -6.0243921 -6.1654482 -6.606616 -6.2234631 -6.166935 -7.2032423 -8.0538616 -8.6228666 -8.7976885 -8.6193838 -8.0532665 -8.1875143][-8.8626413 -6.8094912 -6.7149458 -6.2583704 -5.9127283 -5.7580814 -4.8832154 -4.3780432 -5.3055387 -6.3988848 -7.1283426 -7.3896303 -7.1900368 -6.6158876 -6.6516685][-9.5710754 -6.9722986 -6.801033 -5.91377 -4.8884277 -3.9450555 -2.465898 -1.5890478 -2.4797347 -3.9277697 -4.9368029 -5.4620771 -5.5647039 -5.2367244 -5.291976][-8.8043184 -5.9820018 -5.6907644 -4.3366489 -2.655091 -1.0588266 0.69875693 1.5191278 0.36512709 -1.557739 -3.0533671 -4.1030884 -4.5989566 -4.4633646 -4.4105492][-6.8331814 -4.2171187 -3.7419388 -1.9752743 0.098476648 2.0402372 3.6590064 4.3328 2.9419296 0.5187726 -1.4879119 -3.1868882 -4.0247293 -3.9585798 -3.8357162][-5.1872129 -2.528805 -1.8209875 0.041162729 2.1187966 4.1071396 5.7333975 6.3495207 4.786726 2.0780156 -0.50019979 -2.8525066 -4.1789484 -4.3155222 -4.227706][-4.8413577 -2.0069683 -1.2389568 0.30367327 1.8540132 3.658458 5.2233267 5.7407103 4.3678532 1.9260862 -0.73690128 -3.2982726 -4.72923 -4.8588457 -4.7529783][-5.5282297 -2.346509 -1.4404585 -0.30244243 0.43074965 1.7982938 3.3235447 3.8851564 2.7059295 0.53854322 -1.9062719 -4.2364864 -5.3446188 -5.2821121 -5.171772][-6.6006222 -3.2378783 -2.2342076 -1.6078278 -1.7490444 -0.85036635 0.5461545 1.0695276 0.19401646 -1.3162153 -3.20401 -5.2062426 -6.2403283 -6.2506022 -6.2780123][-8.2309685 -5.1480589 -4.3310041 -4.1336594 -4.7458429 -4.1841216 -2.8873794 -2.261198 -2.7239738 -3.5436831 -4.7754841 -6.2678108 -7.0576296 -7.0775642 -7.2007036][-9.4540358 -6.7651634 -6.1539984 -6.2612495 -7.2029266 -7.0896072 -6.0092707 -5.2144918 -5.1748095 -5.4330416 -6.1182446 -7.1279316 -7.5765581 -7.479929 -7.6424122][-9.1136 -6.7286882 -6.1793351 -6.3605623 -7.3504314 -7.5609188 -6.940382 -6.4035912 -6.3020649 -6.3644848 -6.6797409 -7.2800264 -7.5048046 -7.3198776 -7.469511][-7.7251873 -5.8278713 -5.3783693 -5.4622478 -6.1732035 -6.4453545 -6.2556829 -6.1476345 -6.2296844 -6.3029308 -6.4802485 -6.8386955 -6.9018755 -6.6958666 -6.7583895]]...]
INFO - root - 2017-12-15 09:25:25.886484: step 52710, loss = 0.23, batch loss = 0.20 (34.3 examples/sec; 0.233 sec/batch; 18h:08m:40s remains)
INFO - root - 2017-12-15 09:25:28.164309: step 52720, loss = 0.25, batch loss = 0.22 (34.8 examples/sec; 0.230 sec/batch; 17h:52m:19s remains)
INFO - root - 2017-12-15 09:25:30.473425: step 52730, loss = 0.17, batch loss = 0.14 (33.9 examples/sec; 0.236 sec/batch; 18h:20m:57s remains)
INFO - root - 2017-12-15 09:25:32.772842: step 52740, loss = 0.27, batch loss = 0.24 (33.7 examples/sec; 0.237 sec/batch; 18h:26m:44s remains)
INFO - root - 2017-12-15 09:25:35.057433: step 52750, loss = 0.31, batch loss = 0.28 (34.9 examples/sec; 0.229 sec/batch; 17h:48m:29s remains)
INFO - root - 2017-12-15 09:25:37.358623: step 52760, loss = 0.45, batch loss = 0.42 (34.6 examples/sec; 0.231 sec/batch; 17h:58m:25s remains)
INFO - root - 2017-12-15 09:25:39.640886: step 52770, loss = 0.20, batch loss = 0.16 (35.5 examples/sec; 0.226 sec/batch; 17h:31m:35s remains)
INFO - root - 2017-12-15 09:25:41.898828: step 52780, loss = 0.24, batch loss = 0.21 (35.7 examples/sec; 0.224 sec/batch; 17h:24m:39s remains)
INFO - root - 2017-12-15 09:25:44.189925: step 52790, loss = 0.22, batch loss = 0.18 (35.0 examples/sec; 0.228 sec/batch; 17h:44m:46s remains)
INFO - root - 2017-12-15 09:25:46.478142: step 52800, loss = 0.20, batch loss = 0.17 (33.6 examples/sec; 0.238 sec/batch; 18h:28m:51s remains)
2017-12-15 09:25:46.830754: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5295124 -8.4205685 -8.7799587 -8.6580667 -7.7762289 -6.7207432 -6.0036469 -6.246532 -6.6859303 -7.0639935 -7.1377954 -7.4432621 -7.7893038 -7.6805935 -7.9793634][-7.2128992 -9.3592644 -9.8122454 -9.6633081 -8.7943945 -7.6659031 -6.7103415 -6.789608 -7.0800009 -7.3276711 -7.0582628 -7.1257772 -7.4480019 -7.0227079 -7.4758387][-7.5142679 -9.0755939 -9.4806376 -9.3361731 -8.5535679 -7.4235296 -6.2495008 -6.1449442 -6.3661747 -6.3614807 -5.6040277 -5.3162012 -5.4289751 -4.778223 -5.6083922][-8.3337021 -8.471715 -8.4377708 -7.9683456 -7.134841 -6.1392837 -4.92659 -4.7732506 -5.1714635 -5.2365742 -4.4077826 -4.0751019 -3.8694861 -2.8450017 -3.7950411][-8.7546082 -7.5786676 -6.8236752 -5.7051811 -4.4940481 -3.2316151 -1.8950977 -1.8938789 -2.8123832 -3.4291754 -3.3069656 -3.3291049 -3.0165114 -1.6552861 -2.3661704][-8.4438457 -6.8875151 -5.3685064 -3.3847175 -1.4592606 0.43448257 2.0818169 2.00932 0.46006227 -1.0461957 -1.9190654 -2.3982463 -2.0553279 -0.79082751 -1.2907765][-7.492672 -6.266366 -4.0928082 -1.1669614 1.7523592 4.5543385 6.4744749 6.10968 3.8228028 1.5736413 -0.1050384 -1.1721295 -1.0373118 -0.0033218861 -0.082489014][-7.5241117 -6.2039652 -3.9565268 -0.74325955 2.7876174 6.3177729 8.5960817 8.2134857 5.6212664 2.9508221 0.81440187 -0.48965454 -0.66700709 0.036381483 0.4892745][-7.8320589 -6.8440933 -5.1777678 -2.5914252 0.56188059 3.9487321 6.3810539 6.4130592 4.4188032 2.1268251 0.27907181 -0.77046573 -1.0535971 -0.48919988 0.46340537][-8.350955 -7.8157253 -6.85701 -5.0986423 -2.7179635 0.054889917 2.0092747 2.256655 0.9436965 -0.71800923 -1.7614211 -2.2533185 -2.4349289 -1.7829685 -0.45578623][-8.9859228 -8.8993225 -8.5152912 -7.365922 -5.5625687 -3.2970819 -1.783855 -1.5011823 -2.334512 -3.5587013 -3.9713805 -3.9828854 -4.1667042 -3.4978585 -2.0840373][-9.3241291 -9.5145578 -9.5257559 -8.8631191 -7.5661364 -5.8156805 -4.6669264 -4.2844772 -4.6454654 -5.3847785 -5.4145536 -5.2790546 -5.4360733 -4.8573723 -3.5928979][-9.1592512 -9.4702263 -9.8143806 -9.6182079 -8.8595428 -7.6928835 -6.84284 -6.446147 -6.542098 -6.9899578 -6.8893509 -6.81079 -6.9151783 -6.277606 -5.1872859][-8.6967163 -9.0615473 -9.71777 -9.9118719 -9.5347614 -8.7427053 -8.061039 -7.79535 -8.0132618 -8.53384 -8.6518164 -8.6525908 -8.5435114 -7.8232336 -6.8890743][-7.9908419 -8.1982517 -8.8687105 -9.1896086 -9.0364771 -8.5651836 -8.1300735 -8.1596823 -8.6217365 -9.2040272 -9.4535761 -9.4279623 -9.09226 -8.4277287 -7.8890572]]...]
INFO - root - 2017-12-15 09:25:49.110569: step 52810, loss = 0.21, batch loss = 0.18 (35.4 examples/sec; 0.226 sec/batch; 17h:34m:08s remains)
INFO - root - 2017-12-15 09:25:51.405002: step 52820, loss = 0.26, batch loss = 0.23 (34.0 examples/sec; 0.235 sec/batch; 18h:17m:07s remains)
INFO - root - 2017-12-15 09:25:53.663033: step 52830, loss = 0.29, batch loss = 0.26 (34.8 examples/sec; 0.230 sec/batch; 17h:50m:39s remains)
INFO - root - 2017-12-15 09:25:55.971327: step 52840, loss = 0.24, batch loss = 0.20 (34.3 examples/sec; 0.233 sec/batch; 18h:06m:25s remains)
INFO - root - 2017-12-15 09:25:58.238124: step 52850, loss = 0.24, batch loss = 0.21 (35.8 examples/sec; 0.223 sec/batch; 17h:20m:57s remains)
INFO - root - 2017-12-15 09:26:00.511767: step 52860, loss = 0.15, batch loss = 0.12 (35.8 examples/sec; 0.223 sec/batch; 17h:20m:26s remains)
INFO - root - 2017-12-15 09:26:02.817098: step 52870, loss = 0.17, batch loss = 0.13 (32.4 examples/sec; 0.247 sec/batch; 19h:12m:29s remains)
INFO - root - 2017-12-15 09:26:05.074660: step 52880, loss = 0.19, batch loss = 0.16 (34.5 examples/sec; 0.232 sec/batch; 18h:01m:30s remains)
INFO - root - 2017-12-15 09:26:07.341149: step 52890, loss = 0.21, batch loss = 0.18 (35.5 examples/sec; 0.226 sec/batch; 17h:31m:14s remains)
INFO - root - 2017-12-15 09:26:09.601805: step 52900, loss = 0.21, batch loss = 0.18 (35.8 examples/sec; 0.223 sec/batch; 17h:20m:33s remains)
2017-12-15 09:26:09.918482: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1858344 -6.07872 -6.192317 -5.8991237 -5.1839676 -4.7096786 -4.4366822 -4.2355862 -4.6393528 -5.5008287 -6.6723051 -6.9806519 -6.2977524 -5.7663 -5.1816363][-4.7633133 -6.8699284 -7.0482664 -6.6590776 -5.7567892 -5.0913844 -4.5135803 -4.011322 -4.2786818 -5.1842546 -6.9090824 -7.7447577 -7.1519442 -6.6884775 -6.0253167][-5.3903913 -6.9190683 -7.1429558 -6.6097975 -5.5086594 -4.6060796 -3.5558367 -2.6535816 -2.7275155 -3.6485329 -5.9307556 -7.4051256 -7.2266092 -6.9846363 -6.3074532][-6.239686 -6.8574924 -7.1393013 -6.4212532 -5.1605186 -4.1647782 -3.0055852 -2.0233502 -1.9505723 -2.7509294 -5.3541784 -7.3206282 -7.7037344 -7.7734566 -7.1090317][-6.5110312 -6.1880331 -6.2729826 -5.1354856 -3.611299 -2.3842905 -0.97809255 0.10857725 0.35834241 -0.3179791 -3.0620027 -5.3343582 -6.37982 -6.8290186 -6.2915373][-6.6511755 -5.428216 -5.3324132 -3.7035453 -1.7955382 -0.079832077 1.7902982 3.1171806 3.6841338 3.1020133 0.22475076 -2.4254618 -4.3468385 -5.4473076 -5.2774916][-5.9955597 -4.9656963 -4.7014356 -2.6289694 -0.37713265 1.7704546 3.9506891 5.3678923 6.0859346 5.387722 2.2139714 -0.66573048 -3.2636175 -4.6721859 -4.8139672][-5.6987014 -4.7150764 -4.3551493 -2.1701646 0.052175045 2.3022382 4.3739891 5.4783926 6.1747952 5.363287 2.3133228 -0.28031898 -2.8794575 -4.1923404 -4.4689608][-5.8402853 -5.09099 -4.9742818 -3.0493152 -1.089257 1.0336099 2.8902214 3.8157418 4.6452808 3.7824852 1.0237901 -1.090381 -3.3756118 -4.4091673 -4.7905722][-5.9250927 -5.4766216 -5.6190991 -3.994128 -2.3039622 -0.31139243 1.4519567 2.26662 3.0186827 1.8134921 -0.85439515 -2.6883619 -4.6476231 -5.3610606 -5.600502][-5.8513279 -5.6572838 -5.9884434 -4.6762638 -3.3628943 -1.7414416 -0.20295787 0.52454567 1.0550568 -0.44294989 -2.925138 -4.4583626 -5.950036 -6.196023 -6.2697048][-6.1186581 -6.268631 -6.8830471 -5.99072 -5.052844 -3.7976823 -2.3633027 -1.4745319 -0.75001872 -2.0355926 -4.0204167 -5.438077 -6.5009947 -6.6167059 -6.5900793][-6.5306187 -6.9503984 -7.8060703 -7.2965212 -6.5128937 -5.5276923 -4.3526115 -3.4323747 -2.7168727 -3.6386228 -5.2253942 -6.4653273 -7.0231695 -7.107173 -7.070425][-6.5020447 -6.9006138 -7.7760849 -7.5832195 -7.0303917 -6.5512466 -5.9209661 -5.2207289 -4.7672715 -5.4521832 -6.5886917 -7.5141659 -7.5699415 -7.5523739 -7.3222914][-6.1193972 -6.2723751 -6.9836092 -7.0443926 -6.7330084 -6.6714964 -6.4909315 -6.0663919 -5.9378052 -6.3369408 -7.0660782 -7.7227955 -7.6007252 -7.4701729 -7.0821075]]...]
INFO - root - 2017-12-15 09:26:12.205817: step 52910, loss = 0.26, batch loss = 0.23 (34.7 examples/sec; 0.230 sec/batch; 17h:53m:26s remains)
INFO - root - 2017-12-15 09:26:14.504106: step 52920, loss = 0.22, batch loss = 0.19 (36.1 examples/sec; 0.222 sec/batch; 17h:13m:21s remains)
INFO - root - 2017-12-15 09:26:16.731141: step 52930, loss = 0.23, batch loss = 0.19 (36.1 examples/sec; 0.221 sec/batch; 17h:11m:21s remains)
INFO - root - 2017-12-15 09:26:18.980913: step 52940, loss = 0.41, batch loss = 0.37 (35.1 examples/sec; 0.228 sec/batch; 17h:40m:41s remains)
INFO - root - 2017-12-15 09:26:21.234618: step 52950, loss = 0.24, batch loss = 0.21 (35.4 examples/sec; 0.226 sec/batch; 17h:31m:43s remains)
INFO - root - 2017-12-15 09:26:23.468495: step 52960, loss = 0.19, batch loss = 0.16 (36.6 examples/sec; 0.218 sec/batch; 16h:57m:01s remains)
INFO - root - 2017-12-15 09:26:25.770021: step 52970, loss = 0.19, batch loss = 0.16 (35.1 examples/sec; 0.228 sec/batch; 17h:43m:14s remains)
INFO - root - 2017-12-15 09:26:28.058510: step 52980, loss = 0.24, batch loss = 0.21 (35.1 examples/sec; 0.228 sec/batch; 17h:40m:26s remains)
INFO - root - 2017-12-15 09:26:30.317506: step 52990, loss = 0.18, batch loss = 0.15 (36.0 examples/sec; 0.222 sec/batch; 17h:14m:05s remains)
INFO - root - 2017-12-15 09:26:32.578366: step 53000, loss = 0.31, batch loss = 0.27 (36.6 examples/sec; 0.219 sec/batch; 16h:58m:21s remains)
2017-12-15 09:26:32.933272: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7221932 -3.3242722 -3.5969853 -3.8567059 -4.0357656 -4.1907129 -4.2940578 -4.2162466 -4.0481892 -4.0172234 -4.0071411 -4.1514411 -4.3782558 -4.5879765 -4.8010044][-2.770633 -3.6451302 -4.049726 -4.5521011 -4.9092226 -5.1645231 -5.3789749 -5.3331604 -5.1039495 -5.0552053 -5.0984659 -5.2670956 -5.4305449 -5.5751352 -5.7872243][-3.8535371 -3.9235377 -4.3081422 -4.9161987 -5.3598347 -5.66236 -5.910943 -5.8141651 -5.4649811 -5.3420115 -5.3912468 -5.5936956 -5.7345924 -5.8622007 -6.1331358][-4.7508707 -4.1893363 -4.4579048 -5.0593805 -5.5234413 -5.75886 -5.7931852 -5.42418 -4.95232 -4.8109894 -4.8983321 -5.2115641 -5.46402 -5.7169361 -6.0872555][-4.7324619 -3.24474 -3.2881632 -3.9148674 -4.5397453 -4.7857862 -4.4781132 -3.6289921 -2.9726756 -2.8069658 -2.9508159 -3.4782455 -4.1337466 -4.8725214 -5.585413][-4.0452738 -1.791998 -1.6373216 -2.319828 -3.1928639 -3.6621618 -3.2839007 -2.067378 -1.2506953 -1.0179265 -1.139197 -1.7384183 -2.5986736 -3.6428194 -4.6718664][-3.7013907 -1.4254181 -1.0095632 -1.4800941 -2.3275344 -2.896697 -2.4883757 -1.0267067 -0.21746635 -0.028908491 -0.20264864 -0.8409704 -1.6636257 -2.6627665 -3.7626634][-4.4206715 -2.0914598 -1.2789835 -1.2927585 -1.8910754 -2.4272742 -2.0371068 -0.5433712 0.16875315 0.34582686 0.13808012 -0.48843729 -1.2197218 -2.0538716 -3.1122649][-5.4079947 -3.0767353 -1.9535773 -1.607718 -1.9524013 -2.386205 -2.0552847 -0.62991619 0.081535816 0.29403996 0.073169947 -0.57857811 -1.2505513 -1.8761961 -2.82828][-6.266963 -4.2120352 -3.1671329 -2.6803951 -2.7925022 -3.0291216 -2.7079878 -1.4902928 -0.80612767 -0.63050187 -0.87329388 -1.5907829 -2.1689968 -2.5056305 -3.2007542][-6.7624617 -5.1664615 -4.4553471 -4.0412483 -4.0696168 -4.1706238 -3.9070492 -3.0156212 -2.3529086 -2.1741843 -2.3579161 -3.0446236 -3.523366 -3.5746117 -4.0237312][-7.0414314 -5.9075603 -5.5695882 -5.3223953 -5.406487 -5.4196596 -5.3045893 -4.8772049 -4.2933702 -4.11834 -4.0714879 -4.4751935 -4.7621965 -4.6198282 -4.8500834][-7.1061773 -6.2727585 -6.1287961 -5.895772 -5.9400554 -5.8227472 -5.7474341 -5.6643114 -5.2807045 -5.1477966 -4.8608036 -4.9519787 -5.0998449 -4.9830084 -5.0946474][-5.819623 -5.1783509 -5.1675787 -4.9868155 -5.0029354 -4.842742 -4.7989244 -4.9171185 -4.7728291 -4.6953421 -4.3203726 -4.2668414 -4.5017247 -4.5884805 -4.7309008][-4.0350251 -3.5158992 -3.6790605 -3.6360049 -3.6533537 -3.559099 -3.6301522 -3.8928847 -3.9382567 -3.9066346 -3.617409 -3.6130118 -4.0158892 -4.3011885 -4.4765072]]...]
INFO - root - 2017-12-15 09:26:35.208426: step 53010, loss = 0.22, batch loss = 0.19 (35.0 examples/sec; 0.229 sec/batch; 17h:45m:59s remains)
INFO - root - 2017-12-15 09:26:37.452043: step 53020, loss = 0.30, batch loss = 0.27 (36.2 examples/sec; 0.221 sec/batch; 17h:08m:02s remains)
INFO - root - 2017-12-15 09:26:39.801855: step 53030, loss = 0.21, batch loss = 0.18 (34.2 examples/sec; 0.234 sec/batch; 18h:09m:24s remains)
INFO - root - 2017-12-15 09:26:42.071430: step 53040, loss = 0.25, batch loss = 0.21 (35.4 examples/sec; 0.226 sec/batch; 17h:31m:35s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-0.01-0.01-continue
INFO - root - 2017-12-15 09:26:44.364983: step 53050, loss = 0.28, batch loss = 0.25 (34.3 examples/sec; 0.233 sec/batch; 18h:07m:03s remains)
INFO - root - 2017-12-15 09:26:46.705471: step 53060, loss = 0.22, batch loss = 0.19 (35.3 examples/sec; 0.226 sec/batch; 17h:34m:08s remains)
INFO - root - 2017-12-15 09:26:48.975770: step 53070, loss = 0.29, batch loss = 0.26 (35.9 examples/sec; 0.223 sec/batch; 17h:18m:39s remains)
INFO - root - 2017-12-15 09:26:51.221557: step 53080, loss = 0.22, batch loss = 0.19 (36.4 examples/sec; 0.220 sec/batch; 17h:03m:19s remains)
INFO - root - 2017-12-15 09:26:53.474793: step 53090, loss = 0.21, batch loss = 0.18 (34.2 examples/sec; 0.234 sec/batch; 18h:10m:23s remains)
INFO - root - 2017-12-15 09:26:55.786661: step 53100, loss = 0.20, batch loss = 0.17 (35.3 examples/sec; 0.227 sec/batch; 17h:35m:15s remains)
2017-12-15 09:26:56.149873: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4887092 -4.2143822 -4.171237 -4.1221209 -4.09148 -4.0709772 -4.0554276 -4.011095 -3.9200454 -3.8505578 -3.8397481 -3.9000335 -4.00097 -4.0718489 -4.0505514][-3.7333903 -5.1096663 -5.099081 -5.0560441 -5.0194683 -4.9728794 -4.9459352 -4.8881745 -4.7966294 -4.7566147 -4.7501984 -4.8071918 -4.9251952 -4.9978876 -4.9219489][-4.8507624 -5.8129473 -5.7977314 -5.74377 -5.705636 -5.6783051 -5.6767769 -5.6187611 -5.5398283 -5.586113 -5.65248 -5.7639437 -5.8947792 -5.8969975 -5.68282][-6.2110062 -6.2980976 -6.2770987 -6.1773791 -6.1347389 -6.1583958 -6.22935 -6.1845365 -6.0689564 -6.1729336 -6.3451166 -6.5690718 -6.6958 -6.5548763 -6.1832142][-6.4072714 -5.6422033 -5.5746937 -5.3856115 -5.2913537 -5.3503156 -5.484632 -5.4571242 -5.2833347 -5.4468884 -5.7232723 -6.1664505 -6.3808184 -6.0902977 -5.6851273][-4.9348917 -3.7741802 -3.6319838 -3.2778587 -3.1500716 -3.1504059 -3.2188511 -3.2211003 -2.9864869 -3.2504077 -3.7096317 -4.4613876 -4.8470964 -4.5399561 -4.3004503][-3.1136615 -1.859807 -1.4559519 -0.89648831 -0.77313828 -0.76741004 -0.78554142 -0.80990744 -0.45238268 -0.78108788 -1.4325349 -2.5288141 -3.1565673 -3.00742 -3.07197][-1.8951898 -0.29556561 0.37601829 1.1554434 1.1768446 1.1398087 1.1072814 1.061661 1.5461524 1.1874504 0.29936051 -1.164283 -2.1495421 -2.3626618 -2.8015919][-1.3209749 0.35664821 1.2048635 2.1825879 2.2257526 2.194854 2.1852167 2.1712391 2.6820972 2.2465436 1.1625705 -0.60592353 -1.9791224 -2.5847197 -3.203752][-1.4797106 0.063740015 1.0080926 2.0832856 2.1520956 2.0419614 1.9378269 1.8533633 2.2112763 1.7575438 0.59902596 -1.123237 -2.6070895 -3.4597497 -3.9343076][-2.311034 -0.95114803 -0.056442261 0.860075 0.88941956 0.51498151 0.13206911 -0.00093960762 0.1438179 -0.29686487 -1.2575169 -2.5592351 -3.9208889 -4.7263522 -4.8487916][-3.2450998 -2.1139512 -1.4309102 -0.71873546 -0.78568864 -1.3790107 -2.1011703 -2.3838382 -2.4872694 -2.9498055 -3.7009382 -4.4819708 -5.4613485 -6.043355 -5.91117][-4.0498004 -3.07385 -2.6192615 -2.1512268 -2.1889315 -2.8077977 -3.6138184 -3.8237457 -4.0195913 -4.6101651 -5.3013172 -5.7200403 -6.3008041 -6.6297684 -6.3411989][-4.5011964 -3.493639 -3.0748773 -2.7550962 -2.7556095 -3.2725408 -4.0134583 -4.115458 -4.3559995 -5.0050163 -5.7062283 -5.9190044 -6.1480703 -6.305994 -6.0328236][-4.5344391 -3.4894927 -3.1280074 -3.0005617 -3.123296 -3.6938066 -4.389565 -4.4466734 -4.696743 -5.269835 -5.8969107 -6.0003839 -6.0716505 -6.1402445 -5.900209]]...]
INFO - root - 2017-12-15 09:26:58.442126: step 53110, loss = 0.27, batch loss = 0.24 (34.7 examples/sec; 0.230 sec/batch; 17h:52m:41s remains)
INFO - root - 2017-12-15 09:27:00.711996: step 53120, loss = 0.17, batch loss = 0.14 (33.2 examples/sec; 0.241 sec/batch; 18h:40m:26s remains)
INFO - root - 2017-12-15 09:27:03.040674: step 53130, loss = 0.19, batch loss = 0.16 (33.4 examples/sec; 0.239 sec/batch; 18h:34m:04s remains)
INFO - root - 2017-12-15 09:27:05.350670: step 53140, loss = 0.20, batch loss = 0.17 (34.4 examples/sec; 0.232 sec/batch; 18h:01m:30s remains)
INFO - root - 2017-12-15 09:27:07.653383: step 53150, loss = 0.22, batch loss = 0.19 (33.4 examples/sec; 0.240 sec/batch; 18h:36m:08s remains)
INFO - root - 2017-12-15 09:27:09.954560: step 53160, loss = 0.20, batch loss = 0.17 (35.8 examples/sec; 0.223 sec/batch; 17h:19m:26s remains)
INFO - root - 2017-12-15 09:27:12.239114: step 53170, loss = 0.18, batch loss = 0.15 (32.3 examples/sec; 0.247 sec/batch; 19h:12m:08s remains)
INFO - root - 2017-12-15 09:27:14.533378: step 53180, loss = 0.31, batch loss = 0.28 (33.8 examples/sec; 0.237 sec/batch; 18h:22m:28s remains)
INFO - root - 2017-12-15 09:27:16.825462: step 53190, loss = 0.21, batch loss = 0.18 (33.1 examples/sec; 0.241 sec/batch; 18h:44m:09s remains)
INFO - root - 2017-12-15 09:27:19.111841: step 53200, loss = 0.17, batch loss = 0.14 (36.6 examples/sec; 0.219 sec/batch; 16h:58m:07s remains)
2017-12-15 09:27:19.451686: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.81629848 0.48025465 -0.94085121 -2.5007925 -3.3845377 -4.1462407 -5.4460406 -7.0880413 -7.9279518 -8.81761 -9.56376 -9.8717241 -9.58166 -9.2385483 -8.8519287][1.4662409 2.3959148 1.0245883 -0.49699521 -1.4294769 -2.3018353 -3.8200855 -5.6738567 -6.5195513 -7.6132879 -8.7833 -9.6057215 -9.6220245 -9.2597408 -8.9348679][0.65615988 2.8290265 1.752866 0.68421865 -0.033733845 -0.8178792 -2.121958 -3.6573062 -4.335639 -5.531589 -6.8727274 -8.0095177 -8.4823923 -8.0913973 -7.4312658][-0.73623455 2.0355246 1.2680411 0.62659073 0.21052837 -0.3041234 -0.93574417 -1.716869 -2.2712498 -3.7323177 -5.1414771 -6.2758064 -7.1366625 -6.8947372 -5.7962093][-2.5108843 0.26086998 -0.256137 -0.47868788 -0.34995663 -0.25161862 0.23072147 0.48313498 -0.031520844 -2.0260816 -3.7658119 -4.8721418 -6.1186657 -6.2893333 -4.8500953][-4.3524489 -1.5763018 -1.8685957 -1.7342504 -0.98916316 0.067891359 1.8670642 2.9737742 2.2577899 -0.29956973 -2.5972829 -3.9370005 -5.5715637 -6.2814121 -4.8716288][-5.150979 -2.9057267 -2.9659548 -2.4464405 -1.0354958 0.90362597 3.6372969 5.31592 4.621911 1.8876841 -0.88696766 -2.7399564 -4.9151988 -6.2799873 -5.2483792][-5.8258643 -3.8473601 -3.7498612 -2.8395672 -0.90199769 1.589196 4.6134739 6.4191647 5.7802153 3.2144578 0.39770603 -1.9255344 -4.6060514 -6.4328823 -5.7299728][-5.7460585 -4.3102446 -4.2293553 -3.0025501 -0.88842988 1.6739221 4.516902 6.0173464 5.2340155 3.0384176 0.60651231 -1.6838803 -4.5892572 -6.6728516 -6.3452854][-5.9898214 -4.6488252 -4.5233526 -3.2357917 -1.1792482 1.1072199 3.5213554 4.5540905 3.4868691 1.6581452 -0.041601419 -1.7944093 -4.4094181 -6.504652 -6.6068668][-6.93736 -5.5276737 -5.3065991 -4.13395 -2.3380163 -0.52557123 1.2898128 1.9226344 0.68050027 -0.62955177 -1.3464403 -2.2758472 -4.0960665 -5.8406949 -6.2528563][-7.333952 -5.8137884 -5.5694218 -4.6427464 -3.263166 -1.9490796 -0.76169884 -0.66211319 -1.9712083 -2.9721351 -3.0623832 -3.168576 -4.1134768 -5.3109283 -5.786149][-7.0456944 -5.4046965 -5.2709227 -4.8583932 -4.1392956 -3.502759 -2.9473786 -3.1168921 -4.3392363 -5.0260973 -4.8063526 -4.3162045 -4.53707 -5.2287579 -5.56252][-5.7823896 -4.09153 -4.1929789 -4.5327559 -4.6710453 -4.69999 -4.7192888 -5.2211308 -6.3679295 -6.8254805 -6.4730864 -5.6239204 -5.2641392 -5.5369458 -5.7118635][-4.0531039 -2.2386754 -2.6432006 -3.6014345 -4.5789843 -5.326993 -5.8912153 -6.6241541 -7.6218758 -8.0176239 -7.6687384 -6.8186331 -6.1893167 -6.1847038 -6.217773]]...]
INFO - root - 2017-12-15 09:27:21.746714: step 53210, loss = 0.20, batch loss = 0.17 (34.9 examples/sec; 0.229 sec/batch; 17h:46m:08s remains)
INFO - root - 2017-12-15 09:27:24.003133: step 53220, loss = 0.43, batch loss = 0.40 (36.0 examples/sec; 0.222 sec/batch; 17h:15m:39s remains)
INFO - root - 2017-12-15 09:27:26.273959: step 53230, loss = 0.37, batch loss = 0.34 (35.9 examples/sec; 0.223 sec/batch; 17h:16m:29s remains)
INFO - root - 2017-12-15 09:27:28.534166: step 53240, loss = 0.19, batch loss = 0.16 (34.5 examples/sec; 0.232 sec/batch; 17h:59m:12s remains)
INFO - root - 2017-12-15 09:27:30.801143: step 53250, loss = 0.27, batch loss = 0.24 (35.4 examples/sec; 0.226 sec/batch; 17h:32m:34s remains)
INFO - root - 2017-12-15 09:27:33.134805: step 53260, loss = 0.30, batch loss = 0.27 (33.2 examples/sec; 0.241 sec/batch; 18h:41m:57s remains)
INFO - root - 2017-12-15 09:27:35.423440: step 53270, loss = 0.24, batch loss = 0.20 (35.2 examples/sec; 0.227 sec/batch; 17h:37m:03s remains)
INFO - root - 2017-12-15 09:27:37.706310: step 53280, loss = 0.27, batch loss = 0.24 (35.6 examples/sec; 0.225 sec/batch; 17h:27m:07s remains)
INFO - root - 2017-12-15 09:27:40.023011: step 53290, loss = 0.24, batch loss = 0.21 (35.5 examples/sec; 0.225 sec/batch; 17h:28m:55s remains)
INFO - root - 2017-12-15 09:27:42.352234: step 53300, loss = 0.18, batch loss = 0.14 (34.7 examples/sec; 0.231 sec/batch; 17h:53m:02s remains)
2017-12-15 09:27:42.695891: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.885788 -8.281003 -8.6391907 -8.599412 -8.4212742 -8.2910223 -8.2160282 -8.0875082 -7.9282866 -8.1369686 -8.3961163 -8.439292 -8.4657106 -8.3740377 -8.4862][-6.981988 -9.1220379 -9.8122282 -9.82755 -9.6880617 -9.5081787 -9.4367809 -9.3115253 -8.8737316 -9.1049871 -9.455471 -9.3628721 -9.1882534 -8.7496881 -8.649538][-6.2721033 -7.711195 -8.673192 -8.6664934 -8.5020647 -8.1237812 -8.06091 -8.0200405 -7.2430038 -7.3962946 -7.9178238 -7.7876778 -7.4833984 -6.6790714 -6.3059549][-5.2560635 -5.9691353 -7.099062 -6.9825697 -6.6531849 -5.8312826 -5.5118308 -5.429472 -4.3069153 -4.3216295 -5.1379337 -5.1887484 -4.9130278 -3.9737597 -3.3920426][-4.6559772 -4.4358325 -5.5113659 -5.1824427 -4.7208567 -3.6200418 -3.1669703 -3.1284575 -1.9836354 -1.9482356 -2.9892097 -3.2199903 -3.1145492 -2.2553713 -1.5278711][-3.2478824 -2.1009963 -2.8325779 -2.2530742 -1.8256645 -0.77515054 -0.33784807 -0.29843521 0.56797767 0.4470799 -0.71905041 -0.96579373 -0.9641149 -0.29349256 0.42225266][-1.8375171 -0.39202034 -0.60453904 0.25342894 0.67889333 1.5556419 1.9383304 1.9920485 2.3220146 1.8217909 0.62149572 0.53486061 0.53074431 0.92639613 1.4037576][-1.4113603 0.16027117 0.28182864 1.2648487 1.784085 2.6127946 2.9160478 2.9066379 2.8504274 2.1677158 1.1055782 1.1785076 1.2241704 1.2298443 1.2386632][-1.3667443 0.22927642 0.49111581 1.334717 1.7859857 2.3714979 2.3865521 2.2711093 2.1819689 1.6716897 0.97903967 1.2837214 1.4921393 1.3030002 0.82257104][-1.801666 -0.046257257 0.2544651 0.743073 0.89131117 1.0851076 0.79350615 0.52850842 0.42498899 0.036194563 -0.27365398 0.24310708 0.79471493 0.63158131 -0.2277987][-2.3893952 -0.7935369 -0.805038 -0.71609128 -0.80188096 -0.979287 -1.546772 -2.0245664 -2.1510761 -2.2805064 -2.2251902 -1.5301551 -0.69666839 -0.57132995 -1.5455675][-3.5185776 -2.1620238 -2.2779515 -2.2117124 -2.3384655 -2.7925596 -3.5318506 -4.0336084 -4.0219035 -3.9980912 -3.8836546 -3.2139332 -2.1391714 -1.5123663 -2.2646387][-4.6269703 -3.4230914 -3.516243 -3.1791704 -3.0729458 -3.5210469 -4.2122269 -4.6619253 -4.8088017 -4.9943104 -4.9725571 -4.43469 -3.2932231 -2.1094282 -2.4848554][-5.2181292 -4.2476492 -4.3492908 -3.7577014 -3.39574 -3.6897907 -4.3720088 -4.8971634 -5.2379518 -5.4513702 -5.3461986 -4.883215 -3.7552762 -2.14486 -2.2013183][-5.4899454 -4.8208923 -5.0838723 -4.5288744 -3.9139934 -3.7889643 -4.2328176 -4.5783281 -4.7192326 -4.6926184 -4.4877667 -4.1053658 -3.1213989 -1.3704576 -1.2894073]]...]
INFO - root - 2017-12-15 09:27:45.007067: step 53310, loss = 0.24, batch loss = 0.21 (34.4 examples/sec; 0.233 sec/batch; 18h:03m:21s remains)
INFO - root - 2017-12-15 09:27:47.267882: step 53320, loss = 0.36, batch loss = 0.33 (36.6 examples/sec; 0.219 sec/batch; 16h:58m:03s remains)
INFO - root - 2017-12-15 09:27:49.570737: step 53330, loss = 0.18, batch loss = 0.15 (33.9 examples/sec; 0.236 sec/batch; 18h:18m:55s remains)
INFO - root - 2017-12-15 09:27:51.849322: step 53340, loss = 0.18, batch loss = 0.15 (35.1 examples/sec; 0.228 sec/batch; 17h:39m:48s remains)
INFO - root - 2017-12-15 09:27:54.126699: step 53350, loss = 0.24, batch loss = 0.21 (34.5 examples/sec; 0.232 sec/batch; 18h:00m:09s remains)
INFO - root - 2017-12-15 09:27:56.438588: step 53360, loss = 0.27, batch loss = 0.24 (35.6 examples/sec; 0.224 sec/batch; 17h:24m:21s remains)
INFO - root - 2017-12-15 09:27:58.731740: step 53370, loss = 0.27, batch loss = 0.24 (35.7 examples/sec; 0.224 sec/batch; 17h:23m:20s remains)
INFO - root - 2017-12-15 09:28:01.029765: step 53380, loss = 0.26, batch loss = 0.23 (35.0 examples/sec; 0.229 sec/batch; 17h:44m:20s remains)
INFO - root - 2017-12-15 09:28:03.337341: step 53390, loss = 0.19, batch loss = 0.16 (33.4 examples/sec; 0.239 sec/batch; 18h:33m:57s remains)
INFO - root - 2017-12-15 09:28:05.583213: step 53400, loss = 0.24, batch loss = 0.21 (34.9 examples/sec; 0.229 sec/batch; 17h:47m:16s remains)
2017-12-15 09:28:05.912225: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1336012 -5.3188586 -4.4992447 -4.1145144 -4.9125938 -5.6945934 -5.14814 -4.7889442 -4.2991972 -3.6306393 -3.3836107 -2.9462445 -2.2952342 -1.9634303 -1.5310121][-4.7302732 -4.3946729 -3.8344495 -3.7407355 -4.5642037 -5.1901164 -4.5533619 -4.3933563 -4.1370964 -3.4359341 -2.9403639 -2.6210775 -2.4794276 -2.2578089 -1.8033073][-4.4288468 -3.6316152 -3.3980517 -3.5120974 -4.10616 -4.33399 -3.6451669 -3.8006022 -3.7172844 -3.0770483 -2.5540321 -2.3840547 -2.695606 -2.5613785 -2.1810853][-4.4254284 -3.2207646 -3.2975452 -3.5125713 -3.6572037 -3.1898479 -2.3288717 -2.7110827 -2.882952 -2.6874225 -2.3710239 -2.3159623 -2.77313 -2.6847062 -2.4630084][-4.4997587 -3.0493081 -3.170264 -3.1966012 -2.7195871 -1.4866599 -0.32137239 -0.76308537 -1.3034638 -1.7861696 -1.6811639 -1.465529 -1.8210596 -1.7937478 -1.7974786][-4.6702976 -3.162879 -2.974746 -2.4335525 -1.1278576 0.75997782 2.1469572 1.6031847 0.45666766 -0.82925272 -0.89664352 -0.41434681 -0.43720961 -0.44029915 -0.65374219][-4.9202375 -3.5517569 -2.9403481 -1.7359231 0.37129092 2.8729503 4.5179758 3.7938125 1.8201892 -0.31149185 -0.50908649 0.2651279 0.64631081 0.71875715 0.50181866][-5.0948486 -3.430558 -2.44713 -0.86586511 1.4957089 4.1444893 5.7982454 4.86096 2.3549039 -0.22120619 -0.38977373 0.53990579 1.2201419 1.5139296 1.4254334][-5.3397121 -3.2754717 -2.2279568 -0.80240691 1.1946263 3.4041541 4.7198 3.6153615 1.3054457 -0.983268 -0.89570618 0.13556314 1.0437713 1.511421 1.3732479][-5.3857713 -2.9800439 -2.1647663 -1.273631 0.068736076 1.5989211 2.3521044 1.300833 -0.22830033 -1.5207038 -0.99885035 -0.15581107 0.67083216 0.99250269 0.65419054][-5.6464605 -3.025424 -2.3923647 -1.8501942 -1.0199029 -0.12325311 0.050557852 -0.84537125 -1.5181427 -1.8543162 -1.1969755 -0.73715031 -0.0824306 0.12935352 -0.43718719][-6.4635897 -3.8871732 -3.1475992 -2.5275788 -1.7907549 -1.0829409 -1.0508622 -1.6763617 -1.8975639 -1.8424759 -1.4482273 -1.4492204 -0.93523479 -0.6613977 -1.2309706][-6.8449917 -4.4741449 -3.6557305 -2.9001269 -2.0581534 -1.208557 -1.052248 -1.5416173 -1.8387294 -1.8126684 -1.782647 -2.1271777 -1.7249825 -1.2481242 -1.4805508][-7.1433067 -5.0085363 -4.0284634 -3.1503456 -2.3279986 -1.5315135 -1.2814997 -1.64465 -2.0757115 -2.2634945 -2.6277039 -3.1410542 -2.8167129 -2.1718228 -2.1036413][-7.4878826 -5.5697832 -4.5981073 -3.7599163 -3.0325892 -2.2799075 -1.9418769 -2.2903707 -2.8847628 -3.312355 -3.9022312 -4.4136386 -4.0944223 -3.3353717 -3.0162616]]...]
INFO - root - 2017-12-15 09:28:08.181830: step 53410, loss = 0.26, batch loss = 0.23 (35.0 examples/sec; 0.229 sec/batch; 17h:42m:52s remains)
INFO - root - 2017-12-15 09:28:10.465318: step 53420, loss = 0.18, batch loss = 0.15 (33.3 examples/sec; 0.241 sec/batch; 18h:38m:40s remains)
INFO - root - 2017-12-15 09:28:12.762775: step 53430, loss = 0.20, batch loss = 0.17 (35.6 examples/sec; 0.224 sec/batch; 17h:24m:02s remains)
INFO - root - 2017-12-15 09:28:15.019490: step 53440, loss = 0.19, batch loss = 0.16 (35.6 examples/sec; 0.225 sec/batch; 17h:24m:34s remains)
INFO - root - 2017-12-15 09:28:17.352420: step 53450, loss = 0.32, batch loss = 0.28 (34.5 examples/sec; 0.232 sec/batch; 17h:59m:06s remains)
INFO - root - 2017-12-15 09:28:19.633940: step 53460, loss = 0.21, batch loss = 0.18 (34.5 examples/sec; 0.232 sec/batch; 17h:58m:28s remains)
INFO - root - 2017-12-15 09:28:21.902680: step 53470, loss = 0.21, batch loss = 0.18 (35.3 examples/sec; 0.227 sec/batch; 17h:34m:01s remains)
INFO - root - 2017-12-15 09:28:24.188976: step 53480, loss = 0.29, batch loss = 0.26 (35.0 examples/sec; 0.229 sec/batch; 17h:42m:51s remains)
INFO - root - 2017-12-15 09:28:26.531832: step 53490, loss = 0.24, batch loss = 0.20 (32.1 examples/sec; 0.249 sec/batch; 19h:19m:36s remains)
INFO - root - 2017-12-15 09:28:28.811214: step 53500, loss = 0.21, batch loss = 0.18 (35.1 examples/sec; 0.228 sec/batch; 17h:39m:00s remains)
2017-12-15 09:28:29.132002: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1380429 -3.1885448 -2.3702731 -2.7637699 -3.2009754 -4.4288216 -5.3892393 -5.4656467 -5.1038084 -4.3062544 -3.9593835 -3.6115909 -2.9173512 -3.3696656 -4.4502764][-4.0674562 -3.5187883 -2.6090817 -2.89708 -3.4147213 -4.5460052 -5.3501472 -5.3489738 -4.8945265 -4.1560659 -3.9539483 -3.7578115 -3.1387248 -3.4347398 -4.2402086][-5.4876652 -4.1406741 -3.1615007 -3.1165991 -3.5682957 -4.4541769 -5.083766 -5.0902157 -4.62366 -4.166254 -4.3068042 -4.44141 -3.9948068 -4.0327644 -4.3652425][-6.1506991 -4.1612916 -3.1469843 -2.8722861 -3.3015518 -4.1002131 -4.6559467 -4.6469212 -4.1770067 -4.0521088 -4.6456633 -5.23093 -4.9969845 -4.7818589 -4.6207991][-6.2743216 -4.0741673 -3.2607474 -2.9986706 -3.4262061 -4.0485373 -4.3140774 -4.0610218 -3.4037204 -3.4453177 -4.4794922 -5.6312551 -5.7526164 -5.4586964 -4.9286075][-5.7478204 -3.5068367 -3.0848505 -3.1037269 -3.4600322 -3.6122544 -3.3138084 -2.4753397 -1.4259903 -1.5106723 -3.0685949 -4.9983368 -5.7751904 -5.8129549 -5.1905918][-4.5808516 -2.471282 -2.3086123 -2.5061827 -2.6613324 -2.3180315 -1.4079342 0.087496281 1.4625564 1.2309477 -0.88897371 -3.5161209 -4.97015 -5.5195966 -5.0464706][-3.730804 -1.4477488 -1.3455043 -1.5357413 -1.5035956 -0.727713 0.75060797 2.8384774 4.4861012 4.0728617 1.4414604 -1.750191 -3.8702812 -4.8900785 -4.5798097][-3.4979706 -1.0097703 -0.70076728 -0.68307841 -0.4209069 0.79003024 2.7370651 5.1295729 6.8379831 6.2993584 3.370672 -0.15986323 -2.7464948 -4.0782461 -3.9403741][-4.2231207 -1.690419 -1.0982797 -0.91649687 -0.58783996 0.77462864 2.8348386 5.1136818 6.5943604 6.044857 3.3190916 0.080065012 -2.3269396 -3.514545 -3.4042826][-5.466146 -3.2471535 -2.479248 -2.2008679 -2.0267026 -0.89403343 0.81154752 2.5715253 3.6982791 3.31057 1.2636213 -1.1455075 -2.824523 -3.4579902 -3.2058794][-6.8217897 -5.0131416 -4.1808829 -3.8660948 -3.7717159 -2.9147284 -1.702185 -0.53269982 0.22054315 0.070127964 -1.1563579 -2.6071656 -3.4114141 -3.3767049 -2.96351][-7.7988653 -6.32143 -5.6773214 -5.4614506 -5.4197531 -4.9007339 -4.1988559 -3.4831309 -3.0054216 -2.9642956 -3.4655845 -4.0822043 -4.1339064 -3.5692997 -3.0995696][-7.7112274 -6.3947573 -6.1433816 -6.1472607 -6.1732378 -6.006074 -5.7563467 -5.3809352 -5.1288362 -5.0605583 -5.2122812 -5.3772831 -5.0731759 -4.3730068 -4.0416455][-7.1745768 -5.8825817 -6.0182505 -6.2042027 -6.2551279 -6.2837009 -6.21552 -5.994976 -5.8886728 -5.8470621 -5.9480753 -6.0117264 -5.6374435 -5.0522709 -4.9716606]]...]
INFO - root - 2017-12-15 09:28:31.451132: step 53510, loss = 0.28, batch loss = 0.25 (34.6 examples/sec; 0.231 sec/batch; 17h:56m:22s remains)
