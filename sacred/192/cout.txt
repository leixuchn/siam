INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "192"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01-clip50-initconv1-4-baias-relu
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/Relu:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/Relu:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-10 07:19:23.112077: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:19:23.112147: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:19:23.112174: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:19:23.112195: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:19:23.112217: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:19:23.450764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-10 07:19:23.450802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-10 07:19:23.450809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-10 07:19:23.450956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>]
kkkkkkkkkkkkkkkkkkkkkkk [<tf.Variable 'siamese_fc/conv5/def/offset1/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/offset2/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b1/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b2/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv1/weights/Momentum:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv1/BatchNorm/beta/Momentum:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv1/BatchNorm/gamma/Momentum:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b1/weights/Momentum:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b1/BatchNorm/beta/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b1/BatchNorm/gamma/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b2/weights/Momentum:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'OptimiINFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-10 07:19:26.431161: step 0, loss = 0.75, batch loss = 0.69 (3.6 examples/sec; 2.229 sec/batch; 205h:50m:15s remains)
2017-12-10 07:19:26.820184: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00019512407 0.00020677768 0.00021505477 0.00022543539 0.00023837376 0.00025144202 0.00026278209 0.0002738529 0.00028260436 0.0002856768 0.00027847028 0.00026099294 0.00023733969 0.00021543536 0.00019533241][0.00020904164 0.00022234906 0.00023320178 0.00025045543 0.00027569558 0.00030133463 0.00032287539 0.00033938474 0.00035042537 0.00034991989 0.00033624942 0.00030786515 0.00027256249 0.00024117026 0.00021253666][0.00021848801 0.00023237232 0.00024654536 0.0002738508 0.00031447868 0.00035609346 0.00039057995 0.00041582918 0.00042917766 0.00042358713 0.00040006934 0.00035725784 0.00030848157 0.00026661312 0.00022959107][0.00022695938 0.00024045247 0.00025796128 0.00029506214 0.00034889919 0.00040552943 0.00045589876 0.00049776159 0.00051472231 0.00049838651 0.00045982547 0.00040235705 0.00033952974 0.00028757475 0.00024344075][0.00023442532 0.00024780686 0.00026933296 0.00031652994 0.00038313589 0.00045688779 0.00053218973 0.00060098793 0.00061842328 0.00058307388 0.00052105618 0.00044550857 0.00036886867 0.00030668269 0.00025473151][0.00024529445 0.00026092835 0.00029027511 0.00034780765 0.00042369761 0.0005125723 0.00061807048 0.00071830448 0.00072372414 0.00065392768 0.00056455517 0.00047340788 0.00038647532 0.00031800862 0.0002617182][0.00026306909 0.00028330125 0.00032072826 0.00038217855 0.00045951415 0.00055786059 0.00068953086 0.00080157578 0.00077223196 0.00066643045 0.0005627576 0.00047139954 0.00038631874 0.0003181131 0.0002611755][0.00028128855 0.0003063292 0.00034626431 0.00040369717 0.00047484468 0.00056568027 0.00068127108 0.00075116445 0.00069101894 0.00059041096 0.00050393521 0.00043383215 0.00036509053 0.00030570955 0.00025425261][0.00029487486 0.00032083518 0.00035608633 0.00040263266 0.00045894671 0.00052039063 0.00059098832 0.00061355944 0.0005592372 0.00049394806 0.0004384483 0.0003919357 0.00033884204 0.00028983186 0.00024606389][0.00029754138 0.00032440684 0.0003551821 0.00039282424 0.00043333918 0.00046635419 0.00049951009 0.00049737951 0.00045884555 0.00042132309 0.00038983268 0.00036313027 0.00032138126 0.00027881973 0.00024067404][0.00029487503 0.00032289297 0.00034978605 0.00037837945 0.00040443207 0.00041863017 0.00043002251 0.00041697361 0.0003885373 0.00036677052 0.00034987307 0.00033583265 0.00030238956 0.00026708198 0.00023469885][0.00028898544 0.00031316455 0.00033257573 0.0003490274 0.0003629819 0.00036695151 0.00036873287 0.00035287123 0.00033558632 0.00032562352 0.00031608439 0.00030721232 0.00028083666 0.00025322751 0.00022688185][0.000274451 0.00028971059 0.00029903595 0.00030491466 0.00031285067 0.000316742 0.00031857044 0.00030781399 0.0003028241 0.0003032024 0.00029763932 0.00029152035 0.00026753865 0.00024397402 0.00022143841][0.00025861865 0.00026663431 0.00027040657 0.00027353418 0.00028172458 0.00028843078 0.000292638 0.00028951745 0.00029542894 0.00030349547 0.00029978922 0.00029219154 0.00026482163 0.0002410803 0.00021989018][0.00025081515 0.00025827711 0.00026204562 0.00026583832 0.0002736398 0.00028099949 0.0002883675 0.00029229606 0.00030558778 0.00031783682 0.00031340294 0.00030171787 0.00026982656 0.00024331351 0.00022094774]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01-clip50-initconv1-4-baias-relu/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01-clip50-initconv1-4-baias-relu/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-10 07:19:29.352120: step 10, loss = 0.76, batch loss = 0.69 (39.0 examples/sec; 0.205 sec/batch; 18h:57m:34s remains)
INFO - root - 2017-12-10 07:19:31.388233: step 20, loss = 0.79, batch loss = 0.69 (38.8 examples/sec; 0.206 sec/batch; 19h:03m:55s remains)
zeLoss/siamese_fc/conv2/b2/BatchNorm/beta/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b2/BatchNorm/gamma/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv3/weights/Momentum:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv3/BatchNorm/beta/Momentum:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv3/BatchNorm/gamma/Momentum:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b1/weights/Momentum:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b1/BatchNorm/beta/Momentum:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b1/BatchNorm/gamma/Momentum:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b2/weights/Momentum:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b2/BatchNorm/beta/Momentum:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b2/BatchNorm/gamma/Momentum:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/offset1/weights/Momentum:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/offset2/weights/Momentum:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/b1/weights/Momentum:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/b1/biases/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/b2/weights/Momentum:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/b2/biases/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/detection/biases/Momentum:0' shape=(1,) dtype=float32_ref>]
INFO - root - 2017-12-10 07:19:33.431211: step 30, loss = 0.85, batch loss = 0.69 (38.4 examples/sec; 0.208 sec/batch; 19h:13m:04s remains)
INFO - root - 2017-12-10 07:19:35.470508: step 40, loss = 0.90, batch loss = 0.69 (38.7 examples/sec; 0.207 sec/batch; 19h:04m:56s remains)
INFO - root - 2017-12-10 07:19:37.531334: step 50, loss = 0.94, batch loss = 0.69 (38.4 examples/sec; 0.208 sec/batch; 19h:13m:06s remains)
INFO - root - 2017-12-10 07:19:39.571809: step 60, loss = 0.99, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 19h:06m:50s remains)
INFO - root - 2017-12-10 07:19:41.607885: step 70, loss = 1.03, batch loss = 0.69 (38.9 examples/sec; 0.206 sec/batch; 18h:58m:39s remains)
INFO - root - 2017-12-10 07:19:43.657098: step 80, loss = 1.06, batch loss = 0.69 (38.5 examples/sec; 0.208 sec/batch; 19h:10m:49s remains)
INFO - root - 2017-12-10 07:19:45.688357: step 90, loss = 1.08, batch loss = 0.69 (38.8 examples/sec; 0.206 sec/batch; 19h:02m:24s remains)
INFO - root - 2017-12-10 07:19:47.754744: step 100, loss = 1.10, batch loss = 0.69 (38.2 examples/sec; 0.210 sec/batch; 19h:20m:45s remains)
2017-12-10 07:19:48.031286: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00051632803 -0.00049313926 -0.00052787177 -0.00060732383 -0.0006549 -0.000637159 -0.00057771872 -0.00041141594 -0.00020138547 7.1511604e-06 0.00014059292 0.00020222133 0.00019038375 3.478257e-05 -0.00020240666][0.00060212007 0.00063083833 0.00057612546 0.00046590669 0.00037070364 0.00033569057 0.00035971962 0.00052321423 0.0007696636 0.0010550204 0.0013012714 0.0014991066 0.0016008401 0.0015153093 0.0012948881][0.0019399351 0.0019312445 0.0018360666 0.0016783192 0.001529214 0.0014417297 0.0014415323 0.0015978189 0.0018782271 0.0022483608 0.0026246323 0.0029401183 0.0031234887 0.0031139911 0.0029139789][0.0030916017 0.0030282307 0.0028681797 0.0026416006 0.0024052765 0.0022393642 0.0022199377 0.0023853243 0.0027428702 0.0032390961 0.0037898226 0.0042718342 0.0045845304 0.004667012 0.004495752][0.0039726384 0.0038545076 0.0036254134 0.0033277865 0.0030220007 0.0028102053 0.0027877511 0.0029948507 0.0034340736 0.0040288363 0.004693252 0.0052901665 0.0056895176 0.0058247307 0.0056726784][0.0046124933 0.0044281911 0.004101879 0.0037220176 0.003361301 0.0031269458 0.0031281011 0.0034038736 0.0039341729 0.004628811 0.0053735226 0.0060064578 0.0064156484 0.0065589705 0.0063967314][0.0049856687 0.0047356822 0.00433675 0.0038950248 0.003500245 0.0032809107 0.0033286544 0.0036660861 0.0042493911 0.0049758963 0.0057151448 0.0063115032 0.0066736005 0.0067806868 0.0065893969][0.0051203184 0.004799529 0.0043601282 0.0038965726 0.003507 0.0033154408 0.0034010033 0.0037758118 0.0043662172 0.0050565274 0.0057215691 0.0062260618 0.0065047452 0.0065438319 0.006313066][0.0049846815 0.0046459269 0.0041982019 0.0037531918 0.0034049987 0.0032557729 0.0033700489 0.0037389984 0.0042798091 0.0048786197 0.0054225503 0.0058135213 0.0060038855 0.005989139 0.0057469476][0.0046821823 0.0043388437 0.0039097825 0.0035162903 0.0032289247 0.0031282245 0.0032574 0.0035908138 0.0040489072 0.0045265574 0.0049363188 0.0052085752 0.0053177318 0.0052646995 0.0050233258][0.0042279856 0.0039290031 0.0035631051 0.003250889 0.0030357181 0.0029776832 0.0030988771 0.003366481 0.0037173852 0.0040691309 0.0043590153 0.0045379065 0.0045898864 0.0045107761 0.0042731185][0.0036146948 0.0034072637 0.0031385529 0.0029210169 0.0027776933 0.0027527381 0.0028510103 0.0030460623 0.0032920698 0.0035346029 0.0037313011 0.0038466603 0.0038666893 0.0037769303 0.0035394141][0.0028446396 0.0027208747 0.0025513452 0.0024190168 0.0023382558 0.0023356955 0.0024078246 0.0025430564 0.0027096653 0.0028750622 0.0030102329 0.0030906773 0.0030953716 0.0029974487 0.0027635815][0.0018508141 0.0018085209 0.0017172834 0.0016517644 0.0016144589 0.001625211 0.0016757301 0.0017693588 0.001882019 0.00199976 0.0020976425 0.0021557035 0.0021471991 0.0020452035 0.0018110406][0.00064627547 0.00066372287 0.000622652 0.00060143648 0.00059506437 0.00061541237 0.00065222662 0.00071762362 0.0007916363 0.00087679876 0.00094819348 0.00098634046 0.00096474122 0.00085936114 0.0006326451]]...]
INFO - root - 2017-12-10 07:19:50.067247: step 110, loss = 1.11, batch loss = 0.69 (40.1 examples/sec; 0.199 sec/batch; 18h:23m:59s remains)
INFO - root - 2017-12-10 07:19:52.110211: step 120, loss = 1.14, batch loss = 0.69 (39.4 examples/sec; 0.203 sec/batch; 18h:43m:30s remains)
INFO - root - 2017-12-10 07:19:54.169597: step 130, loss = 1.17, batch loss = 0.69 (39.2 examples/sec; 0.204 sec/batch; 18h:49m:54s remains)
INFO - root - 2017-12-10 07:19:56.208954: step 140, loss = 1.20, batch loss = 0.69 (39.6 examples/sec; 0.202 sec/batch; 18h:38m:53s remains)
INFO - root - 2017-12-10 07:19:58.249944: step 150, loss = 1.21, batch loss = 0.69 (38.5 examples/sec; 0.208 sec/batch; 19h:11m:22s remains)
INFO - root - 2017-12-10 07:20:00.290045: step 160, loss = 25.80, batch loss = 25.26 (38.5 examples/sec; 0.208 sec/batch; 19h:12m:06s remains)
INFO - root - 2017-12-10 07:20:02.325327: step 170, loss = 79.50, batch loss = 78.91 (40.1 examples/sec; 0.199 sec/batch; 18h:24m:42s remains)
INFO - root - 2017-12-10 07:20:04.373863: step 180, loss = 103.30, batch loss = 102.65 (37.2 examples/sec; 0.215 sec/batch; 19h:51m:58s remains)
INFO - root - 2017-12-10 07:20:06.400501: step 190, loss = 1.41, batch loss = 0.69 (41.0 examples/sec; 0.195 sec/batch; 18h:01m:04s remains)
INFO - root - 2017-12-10 07:20:08.431886: step 200, loss = 1.47, batch loss = 0.69 (38.7 examples/sec; 0.207 sec/batch; 19h:05m:59s remains)
2017-12-10 07:20:08.743262: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0083832461 -0.0083804969 -0.0083721383 -0.008371599 -0.0083756242 -0.0083839968 -0.0083853826 -0.0083852252 -0.0083843227 -0.0083819348 -0.0083796578 -0.0083833281 -0.0083842315 -0.0083833234 -0.0083773918][-0.0083729476 -0.0083742216 -0.0083748959 -0.0083763627 -0.00837887 -0.0083792564 -0.0083837667 -0.008390083 -0.0083907489 -0.0083900616 -0.0083884932 -0.0083845574 -0.008381865 -0.008383234 -0.008375111][-0.0083650528 -0.0083663883 -0.0083703641 -0.0083762491 -0.0083814 -0.0083807 -0.00837808 -0.0083832443 -0.0083915247 -0.0083915517 -0.0083902935 -0.0083868764 -0.00838089 -0.0083782794 -0.0083747292][-0.0083646132 -0.0083678206 -0.0083758282 -0.008381567 -0.008382557 -0.0083872164 -0.0083877146 -0.0083850846 -0.0083834613 -0.0083898911 -0.0083886907 -0.0083889188 -0.008389676 -0.0083851721 -0.0083755711][-0.00834313 -0.0083528021 -0.008365985 -0.0083793988 -0.0083840685 -0.0083857207 -0.0083884448 -0.0083887037 -0.0083889039 -0.0083858781 -0.0083823474 -0.0083874874 -0.008388361 -0.0083823362 -0.0083708139][-0.0083580865 -0.0083566923 -0.0083573526 -0.0083710244 -0.0083748735 -0.0083746044 -0.0083716931 -0.0083777718 -0.0083868941 -0.0083863037 -0.0083849058 -0.0083840974 -0.0083836811 -0.00838422 -0.0083829574][-0.008357401 -0.0083613442 -0.00837113 -0.0083726579 -0.0083755925 -0.0083792815 -0.00838203 -0.0083822189 -0.0083850939 -0.0083858259 -0.008384414 -0.0083848583 -0.0083853155 -0.0083836224 -0.0083803842][-0.0083571859 -0.0083677275 -0.00838194 -0.0083892988 -0.0083924932 -0.0083928443 -0.0083910646 -0.0083882455 -0.0083859675 -0.0083862217 -0.0083864527 -0.0083859526 -0.0083851572 -0.0083858147 -0.0083864927][-0.008354634 -0.0083668465 -0.0083791753 -0.0083878934 -0.0083912788 -0.0083916411 -0.0083897654 -0.0083878925 -0.0083872108 -0.0083877174 -0.0083872089 -0.0083868457 -0.00838627 -0.0083861556 -0.0083870422][-0.00835417 -0.00836567 -0.0083757406 -0.0083823157 -0.0083865151 -0.0083871046 -0.0083857607 -0.0083857737 -0.0083885314 -0.0083922977 -0.0083923778 -0.00838926 -0.0083858063 -0.0083875358 -0.0083893454][-0.0083655128 -0.0083719809 -0.0083772792 -0.0083846254 -0.00838851 -0.008389242 -0.0083902143 -0.0083926488 -0.0083927633 -0.008394 -0.0083942972 -0.008392497 -0.0083895568 -0.0083877612 -0.0083879884][-0.0083698211 -0.0083789956 -0.0083884466 -0.0083927531 -0.0083951578 -0.0083951317 -0.0083919084 -0.0083912378 -0.0083920248 -0.00839354 -0.0083945151 -0.0083923694 -0.0083892308 -0.0083873859 -0.0083871577][-0.0083712041 -0.008378746 -0.0083889561 -0.0083946688 -0.00839665 -0.0083962325 -0.00838904 -0.0083821807 -0.0083885994 -0.0083936593 -0.0083941054 -0.0083928136 -0.0083902469 -0.0083881561 -0.0083869966][-0.008370175 -0.008370054 -0.0083811544 -0.0083907 -0.0083934357 -0.00839253 -0.0083904695 -0.0083882743 -0.0083867647 -0.0083903512 -0.0083915368 -0.0083913859 -0.0083904853 -0.0083909761 -0.0083910059][-0.0083824415 -0.0083820727 -0.0083864313 -0.0083888322 -0.008387533 -0.0083875954 -0.0083863493 -0.0083835572 -0.0083840806 -0.0083888648 -0.0083890948 -0.0083897468 -0.0083909389 -0.0083936946 -0.0083937515]]...]
INFO - root - 2017-12-10 07:20:10.752989: step 210, loss = 1.51, batch loss = 0.69 (40.7 examples/sec; 0.196 sec/batch; 18h:07m:46s remains)
INFO - root - 2017-12-10 07:20:12.744389: step 220, loss = 1.54, batch loss = 0.69 (38.8 examples/sec; 0.206 sec/batch; 19h:02m:37s remains)
INFO - root - 2017-12-10 07:20:14.748870: step 230, loss = 1.57, batch loss = 0.69 (39.7 examples/sec; 0.201 sec/batch; 18h:35m:35s remains)
INFO - root - 2017-12-10 07:20:16.754822: step 240, loss = 1.63, batch loss = 0.69 (39.7 examples/sec; 0.202 sec/batch; 18h:36m:46s remains)
INFO - root - 2017-12-10 07:20:18.757502: step 250, loss = 1.70, batch loss = 0.69 (41.0 examples/sec; 0.195 sec/batch; 17h:59m:39s remains)
INFO - root - 2017-12-10 07:20:20.800142: step 260, loss = 1.77, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:38m:43s remains)
INFO - root - 2017-12-10 07:20:22.796827: step 270, loss = 1.82, batch loss = 0.69 (40.8 examples/sec; 0.196 sec/batch; 18h:04m:51s remains)
INFO - root - 2017-12-10 07:20:24.789501: step 280, loss = 1.85, batch loss = 0.69 (38.9 examples/sec; 0.205 sec/batch; 18h:57m:22s remains)
INFO - root - 2017-12-10 07:20:26.796962: step 290, loss = 1.87, batch loss = 0.69 (40.6 examples/sec; 0.197 sec/batch; 18h:09m:58s remains)
INFO - root - 2017-12-10 07:20:28.802918: step 300, loss = 1.90, batch loss = 0.70 (38.4 examples/sec; 0.209 sec/batch; 19h:14m:49s remains)
2017-12-10 07:20:29.109135: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.4116669 0.40686798 0.39993757 0.40256429 0.41324514 0.42562106 0.43223265 0.42797396 0.41182777 0.38697463 0.35965866 0.33652067 0.32172889 0.31539357 0.31423151][0.41657975 0.41665736 0.41194853 0.41314012 0.41922686 0.42572573 0.42745373 0.42081195 0.40509608 0.38273776 0.3586871 0.33834872 0.325126 0.31907794 0.31745002][0.4135302 0.41872361 0.41554642 0.41385704 0.41346335 0.41228861 0.40795669 0.39872891 0.38426024 0.36600396 0.3471666 0.33139127 0.320936 0.31573543 0.31378976][0.41564694 0.42445993 0.42115709 0.41554338 0.40849459 0.40005887 0.390233 0.37881833 0.36560488 0.35107777 0.33697066 0.32551217 0.31796652 0.31405148 0.31233954][0.42396751 0.43352646 0.42766273 0.41747132 0.40516251 0.3920854 0.379445 0.36766019 0.35601962 0.3439827 0.33251414 0.32353061 0.31805256 0.31560105 0.31485921][0.43827137 0.44539073 0.434769 0.41993248 0.40453273 0.39022616 0.378144 0.36827004 0.35867691 0.34767431 0.33616954 0.32708481 0.32219827 0.32099271 0.32175064][0.45728073 0.46122015 0.44631004 0.42850852 0.41242287 0.39922896 0.38924846 0.38160279 0.37308958 0.36112338 0.34710214 0.33566841 0.32989824 0.32932714 0.33154282][0.47435185 0.4772602 0.4611575 0.4437035 0.42938596 0.41835225 0.41004631 0.40306035 0.39361367 0.37890691 0.36110002 0.34642208 0.33903745 0.3384735 0.34163746][0.48156163 0.48466983 0.47016332 0.45579821 0.44490525 0.43643272 0.42920244 0.42182353 0.41075078 0.39355516 0.3729732 0.3559998 0.34727263 0.34629688 0.34964627][0.47488314 0.47793815 0.46571487 0.45510277 0.44782439 0.44185403 0.43572953 0.42840755 0.41704056 0.39973137 0.37920803 0.36217034 0.35308552 0.3515566 0.35434204][0.45707756 0.45985085 0.44998705 0.44282803 0.43847629 0.43436131 0.42906964 0.42210826 0.411658 0.39647543 0.37877348 0.36396688 0.35571089 0.35374635 0.35546258][0.43262118 0.43560588 0.42846379 0.42462218 0.42273691 0.42000383 0.41521123 0.40846902 0.39918372 0.38697538 0.3734431 0.362182 0.35555321 0.35332516 0.35372081][0.40676981 0.41030523 0.405711 0.40472379 0.40509933 0.40404466 0.40045109 0.39471921 0.38719913 0.37827995 0.36902127 0.36126083 0.35605788 0.35319424 0.3517592][0.38750029 0.39103112 0.3869409 0.38619131 0.38678032 0.38653049 0.38452563 0.38111898 0.37692523 0.37248445 0.36811447 0.36410153 0.36043206 0.35697275 0.3535831][0.37942812 0.3811346 0.373474 0.36783263 0.3637521 0.360439 0.3577792 0.35633925 0.35654771 0.35839257 0.3610822 0.36346734 0.36439309 0.36342973 0.36070022]]...]
INFO - root - 2017-12-10 07:20:31.098919: step 310, loss = 1.94, batch loss = 0.72 (40.6 examples/sec; 0.197 sec/batch; 18h:10m:20s remains)
INFO - root - 2017-12-10 07:20:33.084443: step 320, loss = 1.97, batch loss = 0.72 (40.8 examples/sec; 0.196 sec/batch; 18h:04m:43s remains)
INFO - root - 2017-12-10 07:20:35.087385: step 330, loss = 2.00, batch loss = 0.72 (40.3 examples/sec; 0.199 sec/batch; 18h:20m:20s remains)
INFO - root - 2017-12-10 07:20:37.086007: step 340, loss = 2.03, batch loss = 0.73 (40.1 examples/sec; 0.199 sec/batch; 18h:23m:34s remains)
INFO - root - 2017-12-10 07:20:39.098183: step 350, loss = 2.05, batch loss = 0.72 (40.1 examples/sec; 0.200 sec/batch; 18h:24m:48s remains)
INFO - root - 2017-12-10 07:20:41.107216: step 360, loss = 2.11, batch loss = 0.72 (40.4 examples/sec; 0.198 sec/batch; 18h:16m:59s remains)
INFO - root - 2017-12-10 07:20:43.095396: step 370, loss = 2.15, batch loss = 0.72 (40.9 examples/sec; 0.196 sec/batch; 18h:03m:06s remains)
INFO - root - 2017-12-10 07:20:45.068085: step 380, loss = 2.20, batch loss = 0.72 (40.5 examples/sec; 0.198 sec/batch; 18h:13m:28s remains)
INFO - root - 2017-12-10 07:20:47.043936: step 390, loss = 2.23, batch loss = 0.73 (41.2 examples/sec; 0.194 sec/batch; 17h:54m:30s remains)
INFO - root - 2017-12-10 07:20:49.052061: step 400, loss = 2.26, batch loss = 0.73 (41.6 examples/sec; 0.193 sec/batch; 17h:45m:41s remains)
2017-12-10 07:20:49.348198: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.74695957 0.75831932 0.75789768 0.75793791 0.75831473 0.75879312 0.75918275 0.75948048 0.7598837 0.76058328 0.76161039 0.76283288 0.76404583 0.7650252 0.76558042][0.76043564 0.77113795 0.76983583 0.76937479 0.76964527 0.77033067 0.77114213 0.77205563 0.77332336 0.77515018 0.77747488 0.779997 0.7823323 0.78411114 0.78506947][0.75546038 0.76621693 0.76420575 0.7634213 0.76374769 0.76477611 0.76609892 0.76764923 0.76971751 0.7725265 0.77594912 0.77954853 0.78278786 0.7851699 0.78637415][0.75204116 0.76329142 0.76089829 0.75999904 0.760498 0.7619198 0.76375073 0.76586366 0.76854849 0.77203536 0.77616018 0.7804032 0.78412759 0.78675634 0.78794873][0.75083345 0.76290655 0.76034284 0.75938207 0.75998223 0.76168036 0.76391721 0.76650441 0.76967311 0.773609 0.77810216 0.78258312 0.78636783 0.78886038 0.78975469][0.75270754 0.76611209 0.76380718 0.76299113 0.76368725 0.76550841 0.76792145 0.7707082 0.773996 0.77789026 0.78217053 0.78628016 0.78955638 0.79144591 0.79172492][0.75807995 0.77301431 0.7715199 0.77116728 0.77203888 0.77385229 0.77613974 0.778665 0.78148341 0.78467673 0.78804755 0.79112184 0.79333657 0.79422349 0.7936523][0.76462841 0.78174931 0.7812497 0.7814706 0.78249681 0.78412527 0.78602904 0.78800756 0.79004967 0.79220349 0.79434657 0.796129 0.79708862 0.7968573 0.79539025][0.77167213 0.7900275 0.79030031 0.79081392 0.791724 0.79298329 0.79438686 0.79577893 0.79712147 0.79842448 0.79958594 0.8003186 0.8002395 0.79906738 0.79683608][0.77721059 0.79636782 0.79701138 0.79749584 0.79806554 0.79881805 0.79969531 0.80060893 0.80149734 0.80231291 0.80291831 0.80302662 0.80228943 0.80048966 0.7977283][0.78060389 0.80010569 0.80077273 0.801011 0.80115384 0.801419 0.8018629 0.80244863 0.80309445 0.80369174 0.80405384 0.80387193 0.80281436 0.80071163 0.7977131][0.78151661 0.80097467 0.80143666 0.80134869 0.80110747 0.8010065 0.80116653 0.80157238 0.80211914 0.80265236 0.80294889 0.80269223 0.80157614 0.79947346 0.79655814][0.78022748 0.799428 0.7996332 0.79928547 0.79879761 0.79848868 0.79850078 0.79881907 0.79932189 0.79983312 0.80012059 0.79988796 0.79887295 0.79699135 0.79442036][0.77746814 0.79633313 0.79632461 0.79583669 0.79524916 0.79486388 0.79481649 0.79508615 0.79554725 0.79602605 0.7963081 0.79614049 0.79531741 0.79379308 0.79173011][0.774013 0.79251772 0.79233742 0.7917943 0.79120642 0.79082906 0.79077512 0.79101574 0.79143143 0.79186612 0.79214251 0.79206622 0.79149157 0.79039955 0.788923]]...]
INFO - root - 2017-12-10 07:20:51.323898: step 410, loss = 2.30, batch loss = 0.74 (40.8 examples/sec; 0.196 sec/batch; 18h:05m:05s remains)
INFO - root - 2017-12-10 07:20:53.315166: step 420, loss = 2.30, batch loss = 0.72 (40.7 examples/sec; 0.196 sec/batch; 18h:06m:58s remains)
INFO - root - 2017-12-10 07:20:55.301569: step 430, loss = 2.31, batch loss = 0.71 (39.5 examples/sec; 0.202 sec/batch; 18h:40m:22s remains)
INFO - root - 2017-12-10 07:20:57.312597: step 440, loss = 2.32, batch loss = 0.70 (40.0 examples/sec; 0.200 sec/batch; 18h:27m:23s remains)
INFO - root - 2017-12-10 07:20:59.329577: step 450, loss = 2.34, batch loss = 0.69 (40.8 examples/sec; 0.196 sec/batch; 18h:03m:58s remains)
INFO - root - 2017-12-10 07:21:01.328566: step 460, loss = 2.37, batch loss = 0.69 (40.7 examples/sec; 0.197 sec/batch; 18h:07m:48s remains)
INFO - root - 2017-12-10 07:21:03.327333: step 470, loss = 2.40, batch loss = 0.69 (40.3 examples/sec; 0.198 sec/batch; 18h:18m:16s remains)
INFO - root - 2017-12-10 07:21:05.361557: step 480, loss = 2.44, batch loss = 0.69 (39.6 examples/sec; 0.202 sec/batch; 18h:38m:03s remains)
INFO - root - 2017-12-10 07:21:07.349705: step 490, loss = 2.49, batch loss = 0.69 (40.1 examples/sec; 0.199 sec/batch; 18h:23m:01s remains)
INFO - root - 2017-12-10 07:21:09.336286: step 500, loss = 2.52, batch loss = 0.69 (40.5 examples/sec; 0.198 sec/batch; 18h:13m:48s remains)
2017-12-10 07:21:09.617701: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.017084323 -0.017110663 -0.017113769 -0.017112166 -0.017120684 -0.017121641 -0.017124444 -0.017123185 -0.017116308 -0.01711964 -0.017120572 -0.017119115 -0.017111724 -0.017097617 -0.017077783][-0.017137207 -0.017150626 -0.017150626 -0.017150626 -0.017150626 -0.017150626 -0.017150626 -0.017150626 -0.017150626 -0.017150626 -0.017150613 -0.01714986 -0.017141892 -0.017124876 -0.017102383][-0.017139859 -0.017150616 -0.017150626 -0.017150626 -0.017150626 -0.017150626 -0.017150626 -0.017150626 -0.017150626 -0.017150626 -0.017150626 -0.017149968 -0.017143188 -0.017127166 -0.017106298][-0.017139956 -0.017149869 -0.017150626 -0.017150626 -0.017150626 -0.017150626 -0.017150626 -0.017150626 -0.017150626 -0.017150626 -0.017150626 -0.017150057 -0.017143304 -0.017128102 -0.0171074][-0.0171337 -0.017143633 -0.017147236 -0.017148655 -0.017148348 -0.01714783 -0.017147489 -0.017146878 -0.017145984 -0.017145738 -0.017143942 -0.017141037 -0.01713194 -0.017113872 -0.017088512][-0.017099259 -0.017106211 -0.017112371 -0.017118534 -0.017120991 -0.017124766 -0.017129049 -0.017131992 -0.017132675 -0.017131079 -0.017125875 -0.0171176 -0.017103719 -0.017082423 -0.017055025][-0.017042896 -0.017046493 -0.017047521 -0.017055815 -0.01706085 -0.017072966 -0.017089006 -0.017101491 -0.017106727 -0.017103668 -0.01709274 -0.017077023 -0.017055886 -0.017030716 -0.017002122][-0.016992383 -0.016986215 -0.016977109 -0.016978322 -0.016977796 -0.016987965 -0.017006826 -0.017028239 -0.01704294 -0.017046262 -0.017036801 -0.017019298 -0.016996607 -0.016973948 -0.01694987][-0.016947819 -0.016933888 -0.016913548 -0.016909851 -0.016906051 -0.016910717 -0.016923031 -0.016939482 -0.016954266 -0.016960852 -0.016958348 -0.016947249 -0.016930524 -0.016913915 -0.016895495][-0.016915098 -0.01690284 -0.016878955 -0.016871814 -0.016867958 -0.016869517 -0.016875291 -0.01688339 -0.016891664 -0.0168969 -0.016897243 -0.016892273 -0.016883323 -0.016871382 -0.016857753][-0.016901059 -0.016890299 -0.016869331 -0.016862601 -0.016860418 -0.016860174 -0.016861863 -0.016864697 -0.016867943 -0.01687002 -0.016870057 -0.016868049 -0.016864333 -0.016856354 -0.016847217][-0.01691599 -0.016905755 -0.016888322 -0.016884286 -0.016885029 -0.01688258 -0.016882479 -0.016884418 -0.016886963 -0.016886935 -0.016886285 -0.016885268 -0.016882235 -0.016876509 -0.016870389][-0.016953966 -0.016949032 -0.016931213 -0.016925404 -0.016926844 -0.016929172 -0.01693215 -0.016934656 -0.016935656 -0.016935444 -0.016933275 -0.016930413 -0.016925411 -0.016919592 -0.016914427][-0.01700018 -0.017010132 -0.016990138 -0.016987814 -0.016991945 -0.016995093 -0.016997635 -0.016998416 -0.01699743 -0.016995389 -0.01699248 -0.01699003 -0.01698691 -0.016983744 -0.016981252][-0.017031379 -0.017056786 -0.01704447 -0.017042352 -0.017045818 -0.017048677 -0.017051192 -0.017051727 -0.017050819 -0.017048528 -0.017045578 -0.017043404 -0.017041659 -0.017039912 -0.017038133]]...]
INFO - root - 2017-12-10 07:21:11.623576: step 510, loss = 2.55, batch loss = 0.69 (40.1 examples/sec; 0.200 sec/batch; 18h:24m:15s remains)
INFO - root - 2017-12-10 07:21:13.622024: step 520, loss = 2.56, batch loss = 0.69 (39.8 examples/sec; 0.201 sec/batch; 18h:32m:02s remains)
INFO - root - 2017-12-10 07:21:15.632805: step 530, loss = 2.58, batch loss = 0.69 (40.9 examples/sec; 0.195 sec/batch; 18h:01m:22s remains)
INFO - root - 2017-12-10 07:21:17.632189: step 540, loss = 2.60, batch loss = 0.69 (40.5 examples/sec; 0.197 sec/batch; 18h:12m:04s remains)
INFO - root - 2017-12-10 07:21:19.634931: step 550, loss = 2.62, batch loss = 0.69 (38.4 examples/sec; 0.208 sec/batch; 19h:12m:20s remains)
INFO - root - 2017-12-10 07:21:21.675356: step 560, loss = 2.65, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 19h:15m:37s remains)
INFO - root - 2017-12-10 07:21:23.673021: step 570, loss = 2.68, batch loss = 0.69 (40.5 examples/sec; 0.197 sec/batch; 18h:12m:03s remains)
INFO - root - 2017-12-10 07:21:25.647112: step 580, loss = 2.72, batch loss = 0.69 (41.2 examples/sec; 0.194 sec/batch; 17h:53m:53s remains)
INFO - root - 2017-12-10 07:21:27.633667: step 590, loss = 10.85, batch loss = 8.79 (41.0 examples/sec; 0.195 sec/batch; 17h:58m:47s remains)
INFO - root - 2017-12-10 07:21:29.648406: step 600, loss = 20.45, batch loss = 18.37 (39.6 examples/sec; 0.202 sec/batch; 18h:38m:38s remains)
2017-12-10 07:21:29.978447: I tensorflow/core/kernels/logging_ops.cc:79] [[[23.44496 23.694628 23.606787 23.381863 23.04496 22.60261 22.133688 21.47291 20.510515 19.097614 17.152718 14.793227 12.252028 9.8823509 7.9043522][25.224939 25.516914 25.437746 25.226267 24.927658 24.555609 24.168634 23.628691 22.736677 21.31212 19.27552 16.76111 14.004002 11.358783 9.0769835][24.955297 25.246407 25.178566 25.02619 24.808681 24.567841 24.290537 23.851286 23.03838 21.652567 19.638956 17.134731 14.419806 11.808424 9.5313654][24.326441 24.641344 24.618664 24.49959 24.309519 24.083548 23.798145 23.329672 22.485395 21.102289 19.150331 16.778004 14.24813 11.842589 9.7524681][23.149727 23.540352 23.632187 23.608213 23.486454 23.28441 22.974949 22.446918 21.540289 20.139688 18.248173 16.023022 13.706985 11.551973 9.7172594][21.272114 21.808296 22.109102 22.273075 22.301424 22.182945 21.888178 21.322607 20.372019 18.970871 17.150316 15.070515 12.952453 11.023992 9.422987][18.644442 19.410473 19.997503 20.436012 20.703539 20.745829 20.529144 19.977118 19.024988 17.652061 15.916663 13.979753 12.038836 10.297997 8.8813353][15.486474 16.495878 17.406824 18.166273 18.726349 18.985407 18.903687 18.415873 17.504833 16.190836 14.557176 12.766521 10.993619 9.4166241 8.146781][12.242974 13.424211 14.609937 15.646015 16.462055 16.927608 16.993086 16.59866 15.765554 14.545974 13.045873 11.428314 9.84187 8.43896 7.3038397][9.4387083 10.695156 11.982987 13.145612 14.074568 14.634099 14.780326 14.467691 13.736839 12.656006 11.339293 9.9407063 8.5826559 7.3917837 6.4249821][7.3994265 8.5795288 9.78239 10.857431 11.697289 12.200363 12.33479 12.069867 11.458342 10.567384 9.4951458 8.3697548 7.2888541 6.3568969 5.6052551][6.1263442 7.1264381 8.1138468 8.9449492 9.5551729 9.88398 9.9246 9.6805716 9.1979866 8.5244446 7.7332821 6.9142566 6.1377087 5.4785404 4.9575028][5.4129448 6.2289839 6.9925857 7.5794969 7.9508004 8.0888653 8.0100975 7.749507 7.3529744 6.8629894 6.3206334 5.7764587 5.2740664 4.8593287 4.547153][5.0540767 5.7196717 6.3040257 6.7212958 6.9346962 6.9540443 6.8083034 6.5463414 6.2132382 5.8459721 5.4702764 5.114789 4.8005428 4.5524321 4.381948][4.9003229 5.4584289 5.9185886 6.2342787 6.3677125 6.3433075 6.1916475 5.9556851 5.6771278 5.3885775 5.110146 4.8557234 4.6372361 4.4724369 4.3720131]]...]
INFO - root - 2017-12-10 07:21:31.995259: step 610, loss = 24.70, batch loss = 22.60 (36.6 examples/sec; 0.218 sec/batch; 20h:07m:26s remains)
INFO - root - 2017-12-10 07:21:34.043185: step 620, loss = 25.65, batch loss = 23.53 (38.7 examples/sec; 0.207 sec/batch; 19h:02m:40s remains)
INFO - root - 2017-12-10 07:21:36.083028: step 630, loss = 27.95, batch loss = 25.81 (39.0 examples/sec; 0.205 sec/batch; 18h:54m:25s remains)
INFO - root - 2017-12-10 07:21:38.183250: step 640, loss = 2.85, batch loss = 0.69 (38.8 examples/sec; 0.206 sec/batch; 19h:01m:49s remains)
INFO - root - 2017-12-10 07:21:40.217365: step 650, loss = 2.86, batch loss = 0.69 (39.4 examples/sec; 0.203 sec/batch; 18h:42m:56s remains)
INFO - root - 2017-12-10 07:21:42.249068: step 660, loss = 2.88, batch loss = 0.69 (39.6 examples/sec; 0.202 sec/batch; 18h:37m:36s remains)
INFO - root - 2017-12-10 07:21:44.251265: step 670, loss = 2.91, batch loss = 0.69 (39.1 examples/sec; 0.205 sec/batch; 18h:52m:37s remains)
INFO - root - 2017-12-10 07:21:46.297118: step 680, loss = 2.93, batch loss = 0.69 (38.9 examples/sec; 0.206 sec/batch; 18h:57m:04s remains)
INFO - root - 2017-12-10 07:21:48.320308: step 690, loss = 3.01, batch loss = 0.76 (39.9 examples/sec; 0.200 sec/batch; 18h:28m:14s remains)
INFO - root - 2017-12-10 07:21:50.376896: step 700, loss = 3.69, batch loss = 1.40 (38.4 examples/sec; 0.209 sec/batch; 19h:13m:31s remains)
2017-12-10 07:21:50.709428: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20205756 0.22005063 0.23533359 0.24651125 0.25162864 0.25130212 0.24764174 0.24095836 0.23658016 0.23827615 0.24363938 0.24760726 0.2509684 0.24429199 0.22937436][0.26995304 0.2966654 0.31823358 0.33207121 0.33671531 0.3328681 0.32329571 0.30974162 0.29987365 0.29746008 0.2983413 0.29925293 0.29891321 0.2906796 0.27386668][0.34552878 0.38265398 0.41381741 0.43351832 0.43887311 0.43100902 0.41312763 0.39072523 0.37115005 0.35895032 0.3514002 0.34638324 0.3402088 0.32786113 0.30950865][0.433916 0.48667338 0.53144318 0.56180876 0.57106006 0.56033945 0.53219205 0.49521211 0.4585602 0.42909905 0.40646508 0.38973358 0.37563661 0.35897553 0.34024554][0.53356451 0.60871476 0.67389071 0.717489 0.73306185 0.72068036 0.68353868 0.63116747 0.57473791 0.52323884 0.47917268 0.44501883 0.41743553 0.39212281 0.36709875][0.61268485 0.71085447 0.79898691 0.85967445 0.8880952 0.884916 0.85058224 0.794919 0.72671115 0.65777743 0.59153247 0.53294271 0.48266712 0.43955469 0.4005017][0.65302587 0.76733506 0.87410229 0.95152533 0.9967683 1.0094414 0.98771363 0.93891269 0.86893034 0.79027838 0.7081759 0.62978679 0.5593996 0.49717829 0.44224784][0.65230805 0.77327794 0.89015526 0.98282737 1.0452119 1.0763826 1.072346 1.0369471 0.97442955 0.89649016 0.80957037 0.72114688 0.63785338 0.56038213 0.48979411][0.6202448 0.7377038 0.85509014 0.951468 1.0222528 1.0664012 1.0785559 1.0597448 1.0122428 0.94549286 0.86403656 0.77595663 0.68935484 0.60629284 0.52725744][0.56564665 0.67050225 0.77791274 0.87078732 0.94522709 0.9991042 1.0278124 1.0286314 1.0005287 0.95015818 0.8807562 0.80025512 0.71508616 0.63168675 0.55032951][0.50252891 0.59130871 0.68343562 0.76610696 0.83692896 0.89243811 0.93039936 0.94685251 0.93907177 0.90873754 0.8570506 0.7895183 0.71200365 0.63329732 0.55577791][0.44665173 0.51808542 0.5930832 0.66205138 0.72312111 0.77516294 0.81427974 0.83847392 0.8440488 0.83009851 0.79704541 0.74779731 0.68519419 0.61772543 0.54958463][0.39976311 0.45850718 0.51968932 0.57745141 0.62890524 0.67286968 0.70611459 0.73196363 0.74510419 0.74308634 0.72591019 0.69330686 0.64738548 0.59237754 0.53483725][0.37234843 0.4199017 0.46848607 0.51329541 0.55385923 0.5866605 0.6118927 0.63328159 0.6468454 0.65123373 0.64393842 0.62371469 0.59160161 0.54938138 0.50351024][0.34214815 0.38380364 0.42451462 0.45937732 0.48994663 0.51340038 0.52993637 0.5449304 0.55532533 0.55930674 0.55512738 0.54110986 0.51788741 0.48610988 0.45215833]]...]
INFO - root - 2017-12-10 07:21:52.748691: step 710, loss = 3.95, batch loss = 1.62 (38.5 examples/sec; 0.208 sec/batch; 19h:08m:19s remains)
INFO - root - 2017-12-10 07:21:54.758360: step 720, loss = 3.09, batch loss = 0.69 (39.7 examples/sec; 0.201 sec/batch; 18h:33m:50s remains)
INFO - root - 2017-12-10 07:21:56.779719: step 730, loss = 3.16, batch loss = 0.69 (39.6 examples/sec; 0.202 sec/batch; 18h:37m:19s remains)
INFO - root - 2017-12-10 07:21:58.831666: step 740, loss = 3.21, batch loss = 0.69 (40.0 examples/sec; 0.200 sec/batch; 18h:25m:37s remains)
INFO - root - 2017-12-10 07:22:00.873334: step 750, loss = 3.25, batch loss = 0.69 (39.3 examples/sec; 0.204 sec/batch; 18h:46m:18s remains)
INFO - root - 2017-12-10 07:22:02.902752: step 760, loss = 3.30, batch loss = 0.69 (40.8 examples/sec; 0.196 sec/batch; 18h:05m:24s remains)
INFO - root - 2017-12-10 07:22:04.915180: step 770, loss = 3.35, batch loss = 0.69 (39.8 examples/sec; 0.201 sec/batch; 18h:31m:43s remains)
INFO - root - 2017-12-10 07:22:06.967860: step 780, loss = 3.39, batch loss = 0.69 (39.1 examples/sec; 0.205 sec/batch; 18h:52m:15s remains)
INFO - root - 2017-12-10 07:22:08.975391: step 790, loss = 3.43, batch loss = 0.69 (41.2 examples/sec; 0.194 sec/batch; 17h:53m:19s remains)
INFO - root - 2017-12-10 07:22:11.020808: step 800, loss = 3.46, batch loss = 0.69 (40.2 examples/sec; 0.199 sec/batch; 18h:19m:08s remains)
2017-12-10 07:22:11.343603: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.022363937 -0.022362495 -0.02236264 -0.022366432 -0.022370536 -0.022376498 -0.022381021 -0.02238225 -0.022379514 -0.022375805 -0.022378713 -0.022365926 -0.022360047 -0.022361843 -0.022362603][-0.022352371 -0.022352654 -0.022352515 -0.022355037 -0.022360709 -0.022365043 -0.022372514 -0.022380091 -0.022370897 -0.022361092 -0.022357417 -0.022360319 -0.022373343 -0.022373719 -0.022370437][-0.022312399 -0.022322195 -0.022322586 -0.022334695 -0.022346837 -0.022352327 -0.022360846 -0.022364028 -0.02236394 -0.022361923 -0.022365665 -0.022353107 -0.022359449 -0.022371186 -0.022381933][-0.022213047 -0.022215735 -0.022228105 -0.022269608 -0.022305794 -0.022322625 -0.022336666 -0.02234979 -0.022361195 -0.022363259 -0.0223661 -0.022367263 -0.022366975 -0.022365643 -0.022365281][-0.022037609 -0.022031633 -0.022018449 -0.022105122 -0.022185776 -0.022204328 -0.022217071 -0.022220649 -0.022246316 -0.022278825 -0.022303132 -0.022320909 -0.02233183 -0.022342563 -0.022362981][-0.02200464 -0.021913787 -0.021797003 -0.021845922 -0.021881739 -0.021917587 -0.021953296 -0.021995008 -0.022037769 -0.022082914 -0.022129109 -0.022141488 -0.022153543 -0.022153039 -0.022180216][-0.021969492 -0.02192067 -0.021825539 -0.021821897 -0.021769714 -0.021796044 -0.021730242 -0.021645321 -0.021635378 -0.02164256 -0.021703992 -0.021742862 -0.02176401 -0.021770108 -0.02178859][-0.021979898 -0.021914987 -0.021888545 -0.021859193 -0.021704208 -0.021624941 -0.021499598 -0.021404477 -0.021347076 -0.021303583 -0.02132472 -0.021375123 -0.021432329 -0.021441828 -0.021460436][-0.022058509 -0.021991692 -0.021868283 -0.021793999 -0.021596517 -0.021414779 -0.021244679 -0.021169081 -0.021223204 -0.021249661 -0.021270527 -0.021299452 -0.021330114 -0.021338332 -0.021353522][-0.022123009 -0.02206756 -0.021901859 -0.021819588 -0.0216298 -0.021467507 -0.021436689 -0.021408068 -0.021412477 -0.021541268 -0.021656225 -0.021710416 -0.021761227 -0.021743566 -0.021723673][-0.022198483 -0.022167562 -0.022035539 -0.022034662 -0.021873428 -0.021772306 -0.021744777 -0.021773037 -0.021792928 -0.02191128 -0.021992117 -0.022073073 -0.022147298 -0.02209238 -0.02205503][-0.022286277 -0.02229628 -0.022177789 -0.022128828 -0.02209625 -0.022044102 -0.022116853 -0.02212856 -0.022119448 -0.022181964 -0.022191364 -0.022211045 -0.022223921 -0.022193916 -0.02217493][-0.022370251 -0.022370094 -0.022337519 -0.022308227 -0.022274848 -0.022231454 -0.022232831 -0.022240497 -0.02224496 -0.022250909 -0.022259317 -0.022300953 -0.022335449 -0.02230297 -0.022268113][-0.022379451 -0.022405488 -0.02237362 -0.022399593 -0.022402916 -0.022424066 -0.022395441 -0.022348087 -0.022324346 -0.022347271 -0.022353465 -0.022375811 -0.022381775 -0.022354223 -0.022324929][-0.022385845 -0.022392355 -0.022411939 -0.022432292 -0.022442419 -0.022439035 -0.02244834 -0.022442196 -0.022432491 -0.022417031 -0.02241363 -0.022424379 -0.022423383 -0.022417303 -0.022400934]]...]
INFO - root - 2017-12-10 07:22:13.379415: step 810, loss = 3.48, batch loss = 0.69 (39.3 examples/sec; 0.204 sec/batch; 18h:46m:05s remains)
INFO - root - 2017-12-10 07:22:15.429454: step 820, loss = 3.50, batch loss = 0.69 (39.4 examples/sec; 0.203 sec/batch; 18h:41m:40s remains)
INFO - root - 2017-12-10 07:22:17.438749: step 830, loss = 3.52, batch loss = 0.68 (38.1 examples/sec; 0.210 sec/batch; 19h:19m:35s remains)
INFO - root - 2017-12-10 07:22:19.475319: step 840, loss = 3.53, batch loss = 0.68 (38.2 examples/sec; 0.209 sec/batch; 19h:16m:56s remains)
INFO - root - 2017-12-10 07:22:21.510884: step 850, loss = 3.56, batch loss = 0.68 (39.7 examples/sec; 0.202 sec/batch; 18h:33m:53s remains)
INFO - root - 2017-12-10 07:22:23.549501: step 860, loss = 3.58, batch loss = 0.68 (39.4 examples/sec; 0.203 sec/batch; 18h:42m:28s remains)
INFO - root - 2017-12-10 07:22:25.553558: step 870, loss = 3.60, batch loss = 0.68 (38.4 examples/sec; 0.208 sec/batch; 19h:10m:19s remains)
INFO - root - 2017-12-10 07:22:27.593548: step 880, loss = 3.65, batch loss = 0.68 (40.6 examples/sec; 0.197 sec/batch; 18h:08m:43s remains)
INFO - root - 2017-12-10 07:22:29.630554: step 890, loss = 3.68, batch loss = 0.68 (39.5 examples/sec; 0.202 sec/batch; 18h:38m:27s remains)
INFO - root - 2017-12-10 07:22:31.655132: step 900, loss = 3.71, batch loss = 0.67 (40.9 examples/sec; 0.196 sec/batch; 18h:01m:27s remains)
2017-12-10 07:22:32.057579: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.010628307 -0.011571287 -0.011528587 -0.010520911 -0.0090811886 -0.0075810673 -0.0064269854 -0.00458131 -0.0024594571 -0.004942935 -0.0078371828 -0.012945393 -0.019207498 -0.01921295 -0.019231491][-0.0111809 -0.011749901 -0.011523184 -0.010679612 -0.0095338039 -0.008071268 -0.0070554 -0.0050272532 -0.0049856938 -0.0069923857 -0.009829293 -0.012690234 -0.015560887 -0.019114362 -0.0190829][-0.0069827177 -0.0075863274 -0.0081900675 -0.0081941513 -0.0081981579 -0.010502337 -0.012806374 -0.01565383 -0.019096393 -0.019097643 -0.019100683 -0.019103775 -0.019109139 -0.019145168 -0.019174442][-0.0064142365 -0.0071732849 -0.0079322644 -0.0079351347 -0.007937951 -0.010241042 -0.012543983 -0.015525904 -0.019103002 -0.019105282 -0.019109316 -0.019113176 -0.019118816 -0.019090135 -0.019181725][0.0030836798 0.0020539481 0.0010240749 -0.0021155365 -0.0052550044 -0.0052580461 -0.0052611232 -0.0072456384 -0.0092314892 -0.0092352061 -0.0092406524 -0.00927968 -0.012220196 -0.012224927 -0.015186189][0.027770277 0.028803427 0.023543667 0.029487696 0.041718286 0.038559936 0.035400409 0.032280777 0.029164094 0.025278624 0.021393355 0.018525746 0.014846429 0.012268331 0.0097400844][0.064872041 0.0627235 0.060567159 0.054276031 0.048123408 0.053873386 0.059450302 0.059355948 0.059221249 0.049961258 0.036863271 0.030165449 0.023869526 0.020912573 0.021475241][0.045398068 0.055678125 0.052682046 0.05213486 0.051727448 0.050638866 0.049229939 0.049224075 0.06402728 0.076058626 0.073291034 0.0705308 0.067054734 0.067084178 0.067074373][0.059537943 0.0589501 0.053843793 0.052067582 0.049466129 0.0524447 0.055101383 0.05102383 0.06176642 0.072242856 0.0679155 0.065111563 0.061571676 0.063121945 0.064650446][0.060137954 0.058789719 0.0721374 0.080241531 0.072678745 0.070660129 0.068315953 0.067406386 0.066224143 0.050081167 0.033939697 0.029541556 0.024388775 0.040558111 0.056693632][0.021240942 0.019728716 0.016905203 0.028695464 0.038198497 0.034842726 0.03178291 0.042338837 0.054446321 0.051111866 0.046436753 0.034427676 0.022839192 0.038270392 0.054582458][-0.017445987 -0.016386114 -0.015767742 -0.012316164 0.0047323611 0.018510327 0.017472878 0.0019139946 0.00047914125 0.01322199 0.011847518 -0.00087162852 -0.0099035362 -0.0071751755 0.0083386917][-0.016207658 -0.016909355 -0.01651617 -0.016586173 -0.01706763 -0.015540097 -0.015817685 -0.0027807076 0.01027447 0.01030872 0.011820946 0.00028931536 -0.011247269 0.001150541 0.026373662][-0.017982559 -0.014964836 -0.011111913 -0.011683416 -0.013516875 -0.014212692 -0.015890181 -0.015527889 -0.015146196 -0.0039842334 0.0087726843 -0.0013489537 -0.011586078 -0.00028964318 0.0090377461][-0.020419706 -0.018241558 -0.016046245 -0.014017429 -0.011993773 -0.011320828 -0.01082235 -0.00912948 -0.0094730109 -0.0094579533 -0.0094502131 -0.010797806 -0.01214465 -0.013423453 -0.0030232202]]...]
INFO - root - 2017-12-10 07:22:34.104447: step 910, loss = 3.73, batch loss = 0.67 (39.4 examples/sec; 0.203 sec/batch; 18h:41m:13s remains)
INFO - root - 2017-12-10 07:22:36.129766: step 920, loss = 3.75, batch loss = 0.66 (40.7 examples/sec; 0.197 sec/batch; 18h:05m:58s remains)
INFO - root - 2017-12-10 07:22:38.167401: step 930, loss = 3.77, batch loss = 0.66 (39.3 examples/sec; 0.204 sec/batch; 18h:45m:09s remains)
INFO - root - 2017-12-10 07:22:40.198486: step 940, loss = 3.79, batch loss = 0.66 (38.9 examples/sec; 0.206 sec/batch; 18h:57m:09s remains)
INFO - root - 2017-12-10 07:22:42.236274: step 950, loss = 3.82, batch loss = 0.66 (39.2 examples/sec; 0.204 sec/batch; 18h:46m:23s remains)
INFO - root - 2017-12-10 07:22:44.281479: step 960, loss = 3.84, batch loss = 0.66 (39.6 examples/sec; 0.202 sec/batch; 18h:36m:12s remains)
INFO - root - 2017-12-10 07:22:46.330407: step 970, loss = 3.88, batch loss = 0.68 (41.7 examples/sec; 0.192 sec/batch; 17h:39m:24s remains)
INFO - root - 2017-12-10 07:22:48.344835: step 980, loss = 3.89, batch loss = 0.66 (39.1 examples/sec; 0.205 sec/batch; 18h:50m:22s remains)
INFO - root - 2017-12-10 07:22:50.386482: step 990, loss = 3.90, batch loss = 0.65 (38.0 examples/sec; 0.211 sec/batch; 19h:24m:27s remains)
INFO - root - 2017-12-10 07:22:52.433086: step 1000, loss = 3.94, batch loss = 0.66 (39.0 examples/sec; 0.205 sec/batch; 18h:53m:31s remains)
2017-12-10 07:22:52.788154: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.020560399 0.02188433 0.022669755 0.022324864 0.020934854 0.022018474 0.023053177 0.025351495 0.029011894 0.01742401 0.0064141136 -0.0053448118 -0.020146422 -0.020068295 -0.019972198][0.022969075 0.024276607 0.025720552 0.026647858 0.026361756 0.026088502 0.024761382 0.024825417 0.025194503 0.025104363 0.025577143 0.014119811 0.0037116073 -0.0073523074 -0.020134915][0.030333783 0.030479848 0.030588243 0.030293174 0.029107071 0.028061993 0.026571892 0.015334383 0.0052656177 0.0050397255 -0.0074255355 0.0025850441 0.0053516533 -0.0073920079 -0.0046984684][0.030256405 0.030369714 0.030481361 0.030473214 0.031378157 0.031306535 0.030023236 0.019072626 0.0066977069 -0.0046807323 -0.017315032 -0.017266326 -0.017253816 -0.0071646906 -0.0071950126][0.030189607 0.030166831 0.030115195 0.029987436 0.029819213 0.029503722 0.02909822 0.028788604 0.028623402 0.0282678 0.027137142 0.024621621 0.021759637 0.019065246 0.007486131][0.0985924 0.096889675 0.094605923 0.092172027 0.089057758 0.08758162 0.086530924 0.085366935 0.083177924 0.079155341 0.0741732 0.0573244 0.033003081 0.010894179 -0.013304682][0.18528444 0.19039044 0.19443864 0.19100265 0.18458538 0.17959033 0.17388876 0.172009 0.16961443 0.16626988 0.15390322 0.13174239 0.10888508 0.085598379 0.068319663][0.17964084 0.19063115 0.2016011 0.20335308 0.2035163 0.19169317 0.17796251 0.17011309 0.16050987 0.16019307 0.15181202 0.14191665 0.13063402 0.11480913 0.10869075][0.13119209 0.13699928 0.1434163 0.15734282 0.16887462 0.17631483 0.18284577 0.17221688 0.1644344 0.14938009 0.13760389 0.12973864 0.11924878 0.11771998 0.11507481][0.10423647 0.10473175 0.10511866 0.10401666 0.10240936 0.10930577 0.11095132 0.11442374 0.11981957 0.10511048 0.097350255 0.088138789 0.07672669 0.075469762 0.072201058][0.095746472 0.07575357 0.055631045 0.034472328 0.0071080588 0.0023760423 -0.0025268905 0.00014865585 0.02351892 0.040752079 0.059526544 0.076637387 0.064334437 0.0650233 0.065697819][-0.015700106 -0.019257735 -0.023691613 -0.023625808 -0.023556832 -0.023520434 -0.023505012 -0.023027565 -0.022560675 -0.0027214531 0.01760212 0.035089675 0.057950359 0.056792695 0.054946605][-0.012779289 -0.015985534 -0.019489367 -0.023653904 -0.023666514 -0.023659524 -0.02365692 -0.023641199 -0.023621045 -0.023602549 -0.023570837 -0.023530247 -0.023484539 -0.023472669 -0.023455178][-0.022660604 -0.02261951 -0.022366174 -0.023114186 -0.023884399 -0.023879798 -0.023877421 -0.02385837 -0.023847753 -0.023808904 -0.023762418 -0.023703977 -0.023639893 -0.023610933 -0.0235779][-0.016774684 -0.019254554 -0.022181718 -0.023000609 -0.024086887 -0.024088342 -0.024093837 -0.024085294 -0.024064178 -0.024017151 -0.023967961 -0.023885258 -0.023808183 -0.023768188 -0.023727113]]...]
INFO - root - 2017-12-10 07:22:54.847764: step 1010, loss = 3.95, batch loss = 0.66 (40.7 examples/sec; 0.197 sec/batch; 18h:06m:17s remains)
INFO - root - 2017-12-10 07:22:56.885952: step 1020, loss = 3.99, batch loss = 0.68 (37.9 examples/sec; 0.211 sec/batch; 19h:26m:40s remains)
INFO - root - 2017-12-10 07:22:58.913374: step 1030, loss = 4.03, batch loss = 0.67 (39.1 examples/sec; 0.205 sec/batch; 18h:50m:01s remains)
INFO - root - 2017-12-10 07:23:00.954985: step 1040, loss = 4.04, batch loss = 0.64 (39.7 examples/sec; 0.201 sec/batch; 18h:32m:52s remains)
INFO - root - 2017-12-10 07:23:02.997643: step 1050, loss = 4.05, batch loss = 0.63 (38.7 examples/sec; 0.207 sec/batch; 19h:01m:02s remains)
INFO - root - 2017-12-10 07:23:05.033562: step 1060, loss = 4.07, batch loss = 0.63 (40.5 examples/sec; 0.198 sec/batch; 18h:12m:13s remains)
INFO - root - 2017-12-10 07:23:07.075716: step 1070, loss = 4.13, batch loss = 0.67 (40.0 examples/sec; 0.200 sec/batch; 18h:25m:50s remains)
INFO - root - 2017-12-10 07:23:09.098269: step 1080, loss = 4.17, batch loss = 0.67 (39.4 examples/sec; 0.203 sec/batch; 18h:42m:06s remains)
INFO - root - 2017-12-10 07:23:11.165960: step 1090, loss = 4.21, batch loss = 0.67 (38.3 examples/sec; 0.209 sec/batch; 19h:14m:26s remains)
INFO - root - 2017-12-10 07:23:13.192709: step 1100, loss = 4.24, batch loss = 0.65 (39.5 examples/sec; 0.203 sec/batch; 18h:38m:36s remains)
2017-12-10 07:23:13.533259: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14671299 0.14656833 0.14687401 0.14752933 0.11158026 0.10880093 0.0999163 0.0960497 0.12219093 0.087321 0.053778164 0.01510508 -0.022834543 -0.034145474 -0.034152724][0.1413947 0.14081585 0.14052534 0.14059278 0.14108276 0.14162788 0.096821524 0.085113168 0.042319797 0.038869955 0.038676649 0.0052234605 0.0048372336 -0.034067187 -0.034068823][0.093917467 0.093668021 0.093952633 0.094440468 0.068977639 0.043196082 0.0045250542 -0.034087997 -0.034105152 -0.034109458 -0.034088328 -0.034074876 -0.034074068 -0.034078959 -0.034084946][0.10041153 0.096181549 0.092239641 0.09216433 0.066927835 0.041559629 0.011995725 -0.017269669 -0.017286696 -0.017298317 -0.017284319 -0.017271051 -0.017270364 -0.0035847872 0.010084722][0.11860128 0.10909521 0.099826522 0.095976405 0.092084713 0.083716348 0.075206704 0.075193889 0.0751858 0.075187109 0.099145807 0.09998443 0.077177361 0.048970886 0.020238426][0.15718129 0.16269186 0.16961426 0.13488948 0.10012303 0.066789582 0.025437173 -0.0082992539 0.038348444 0.13899258 0.25656328 0.35318252 0.36567092 0.37043086 0.3705593][0.45342958 0.46852714 0.48520315 0.49034274 0.473593 0.36177468 0.24647298 0.18675935 0.13622373 0.20118207 0.29446989 0.30860218 0.31738105 0.32091114 0.320701][0.41516495 0.42567503 0.43608889 0.44451338 0.44171613 0.39519218 0.29844508 0.19466975 0.075624973 -0.023728449 -0.034033414 -0.034039743 -0.034045864 -0.034074545 -0.034103207][0.21814159 0.30839789 0.32063615 0.22127512 0.11214035 0.029783271 0.033698045 0.037178151 0.03973379 0.011066075 -0.014958018 -0.024430865 -0.033885 -0.033913285 -0.033964448][0.012475397 0.093332954 0.17442346 0.1798563 0.18396565 0.084222749 -0.014516523 0.006646473 0.027442329 0.030257851 0.032403655 0.0078854971 -0.015415266 -0.024541602 -0.033882506][-0.0017339587 -0.0038490165 -0.0060403496 -0.0060114004 -0.0059849657 -0.0059855934 -0.0059844423 -0.012351343 -0.018752649 0.0012748614 0.021509688 0.026241876 0.030497685 0.0068762042 -0.024946842][-0.014017146 -0.021524243 -0.031326436 -0.033581972 -0.033548143 -0.033547323 -0.033532403 -0.033546977 -0.033564411 -0.033574443 -0.033615649 -0.0074516758 -0.0073385686 -0.0030004494 -0.0033736695][-0.011412583 -0.017873557 -0.025587015 -0.033598058 -0.033586923 -0.033620339 -0.0336163 -0.0336523 -0.033697788 -0.033714451 -0.033756379 -0.033793621 -0.033830337 -0.033884831 -0.033952113][-0.033829968 -0.033833381 -0.033837717 -0.033832222 -0.033823706 -0.033820506 -0.033817057 -0.033840492 -0.033873528 -0.033890922 -0.033933785 -0.034000967 -0.034016598 -0.034010693 -0.03405543][-0.034012057 -0.034008395 -0.034004886 -0.033992715 -0.033984311 -0.03398421 -0.033984527 -0.034005627 -0.034036536 -0.034064002 -0.034081113 -0.034105595 -0.034127947 -0.034129269 -0.034148265]]...]
INFO - root - 2017-12-10 07:23:15.588714: step 1110, loss = 4.31, batch loss = 0.66 (39.1 examples/sec; 0.204 sec/batch; 18h:49m:22s remains)
INFO - root - 2017-12-10 07:23:17.625789: step 1120, loss = 4.35, batch loss = 0.66 (38.8 examples/sec; 0.206 sec/batch; 18h:59m:49s remains)
INFO - root - 2017-12-10 07:23:19.681607: step 1130, loss = 4.33, batch loss = 0.61 (38.9 examples/sec; 0.206 sec/batch; 18h:56m:31s remains)
INFO - root - 2017-12-10 07:23:21.750642: step 1140, loss = 4.38, batch loss = 0.64 (33.6 examples/sec; 0.238 sec/batch; 21h:54m:15s remains)
INFO - root - 2017-12-10 07:23:23.810353: step 1150, loss = 4.46, batch loss = 0.69 (39.3 examples/sec; 0.204 sec/batch; 18h:45m:22s remains)
INFO - root - 2017-12-10 07:23:25.867085: step 1160, loss = 4.49, batch loss = 0.69 (39.7 examples/sec; 0.202 sec/batch; 18h:33m:10s remains)
INFO - root - 2017-12-10 07:23:27.874501: step 1170, loss = 4.52, batch loss = 0.69 (39.5 examples/sec; 0.203 sec/batch; 18h:38m:25s remains)
INFO - root - 2017-12-10 07:23:29.888425: step 1180, loss = 4.56, batch loss = 0.68 (37.6 examples/sec; 0.213 sec/batch; 19h:34m:06s remains)
INFO - root - 2017-12-10 07:23:31.927294: step 1190, loss = 4.60, batch loss = 0.69 (39.9 examples/sec; 0.201 sec/batch; 18h:28m:17s remains)
INFO - root - 2017-12-10 07:23:33.977744: step 1200, loss = 4.64, batch loss = 0.69 (39.0 examples/sec; 0.205 sec/batch; 18h:51m:12s remains)
2017-12-10 07:23:34.306288: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.053284146 -0.053284146 -0.053284146 -0.053284146 -0.05328415 -0.053284153 -0.053284161 -0.053284168 -0.053284176 -0.053284179 -0.053284187 -0.053284187 -0.053284187 -0.053284187 -0.053284187][-0.053284146 -0.053284146 -0.053284146 -0.053284146 -0.053284146 -0.05328415 -0.053284153 -0.053284161 -0.053284165 -0.053284168 -0.053284176 -0.053284176 -0.053284176 -0.053284176 -0.053284176][-0.053272843 -0.053272843 -0.053272847 -0.053278513 -0.053284157 -0.053284157 -0.053284157 -0.053284161 -0.053284161 -0.053284165 -0.053284165 -0.053284165 -0.053284165 -0.053284165 -0.053284165][-0.053272843 -0.053272843 -0.053272847 -0.053274889 -0.053276997 -0.053273626 -0.053270273 -0.053266909 -0.053263526 -0.053267155 -0.053270686 -0.053274062 -0.053277411 -0.053280775 -0.053284157][-0.053238783 -0.053238586 -0.053238463 -0.053243276 -0.053248342 -0.053247847 -0.053247489 -0.053245522 -0.053243577 -0.053250726 -0.053257637 -0.05326438 -0.053271055 -0.053277582 -0.053284146][-0.053162523 -0.053160336 -0.053158578 -0.053171676 -0.053185347 -0.053194545 -0.053203609 -0.053213444 -0.05322301 -0.053234708 -0.053245828 -0.053255662 -0.053265393 -0.053274691 -0.05327801][-0.053139228 -0.053134665 -0.053130779 -0.053143948 -0.053157907 -0.053158987 -0.053160027 -0.053162698 -0.0531646 -0.053180922 -0.053196508 -0.053219218 -0.053241659 -0.053262666 -0.05327801][-0.053123042 -0.053115204 -0.053108212 -0.053107388 -0.053106818 -0.053094968 -0.053096153 -0.053099092 -0.053101193 -0.05313329 -0.053165261 -0.053205691 -0.053232752 -0.053258203 -0.05327801][-0.05305618 -0.053048339 -0.053041313 -0.053032394 -0.053023603 -0.053011946 -0.05302728 -0.053044122 -0.053060416 -0.053101722 -0.053143043 -0.053189825 -0.053209592 -0.053235043 -0.05325485][-0.052971166 -0.052963011 -0.052955978 -0.05295109 -0.052945886 -0.052937847 -0.052956831 -0.052977268 -0.052997135 -0.053048678 -0.053096384 -0.053153735 -0.053184025 -0.053220142 -0.053250607][-0.053005226 -0.052997269 -0.052990362 -0.0529827 -0.052974544 -0.052963626 -0.052979611 -0.052984431 -0.052988667 -0.053022604 -0.053052872 -0.053106852 -0.053133812 -0.053180993 -0.053222187][-0.053081434 -0.053075474 -0.053070202 -0.053054255 -0.053037491 -0.053016879 -0.053023446 -0.053016488 -0.053009238 -0.053038623 -0.053064682 -0.053115569 -0.053139478 -0.053183883 -0.053228322][-0.05310455 -0.053100962 -0.053097822 -0.053081803 -0.053064786 -0.053052321 -0.053066943 -0.053067174 -0.053067617 -0.053092413 -0.053114008 -0.053152021 -0.053163219 -0.053195916 -0.053228326][-0.053120662 -0.053120367 -0.05312036 -0.053118337 -0.053115845 -0.05311631 -0.053130787 -0.053130753 -0.053131 -0.053140048 -0.05314526 -0.053165555 -0.053172134 -0.053200386 -0.053228334][-0.05319874 -0.053198479 -0.053198531 -0.053198963 -0.053199053 -0.053199325 -0.053199656 -0.05318572 -0.053171769 -0.053171616 -0.053167477 -0.053181417 -0.053195298 -0.05322355 -0.0532515]]...]
INFO - root - 2017-12-10 07:23:36.332680: step 1210, loss = 4.67, batch loss = 0.69 (40.7 examples/sec; 0.197 sec/batch; 18h:05m:10s remains)
INFO - root - 2017-12-10 07:23:38.372857: step 1220, loss = 4.71, batch loss = 0.69 (40.5 examples/sec; 0.197 sec/batch; 18h:10m:25s remains)
INFO - root - 2017-12-10 07:23:40.383409: step 1230, loss = 4.74, batch loss = 0.69 (40.1 examples/sec; 0.200 sec/batch; 18h:22m:27s remains)
INFO - root - 2017-12-10 07:23:42.398518: step 1240, loss = 4.77, batch loss = 0.69 (40.7 examples/sec; 0.197 sec/batch; 18h:05m:46s remains)
INFO - root - 2017-12-10 07:23:44.451855: step 1250, loss = 4.80, batch loss = 0.69 (39.3 examples/sec; 0.204 sec/batch; 18h:45m:10s remains)
INFO - root - 2017-12-10 07:23:46.506761: step 1260, loss = 4.82, batch loss = 0.69 (38.1 examples/sec; 0.210 sec/batch; 19h:20m:04s remains)
INFO - root - 2017-12-10 07:23:48.573272: step 1270, loss = 4.85, batch loss = 0.69 (40.2 examples/sec; 0.199 sec/batch; 18h:18m:03s remains)
INFO - root - 2017-12-10 07:23:50.617112: step 1280, loss = 4.88, batch loss = 0.69 (39.1 examples/sec; 0.205 sec/batch; 18h:50m:06s remains)
INFO - root - 2017-12-10 07:23:52.658993: step 1290, loss = 4.92, batch loss = 0.69 (39.6 examples/sec; 0.202 sec/batch; 18h:34m:49s remains)
INFO - root - 2017-12-10 07:23:54.702397: step 1300, loss = 4.95, batch loss = 0.69 (38.7 examples/sec; 0.207 sec/batch; 19h:01m:07s remains)
2017-12-10 07:23:55.010553: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.054881234 -0.054881234 -0.054881234 -0.054881237 -0.054881245 -0.054881256 -0.054881264 -0.054881275 -0.05487939 -0.054877415 -0.054877471 -0.054877479 -0.054877628 -0.054874528 -0.054870658][-0.054881237 -0.054881237 -0.054881237 -0.054881241 -0.054881245 -0.054881252 -0.054881256 -0.054881264 -0.054881271 -0.054877397 -0.054877508 -0.054877572 -0.054877609 -0.054877628 -0.054874156][-0.05487733 -0.0548793 -0.054881245 -0.054881249 -0.054881252 -0.054881252 -0.054881252 -0.054881256 -0.054879256 -0.054877415 -0.05487752 -0.054877616 -0.054877661 -0.054877676 -0.054877684][-0.05487347 -0.054877367 -0.054881245 -0.054875907 -0.054870475 -0.054864991 -0.054859482 -0.054853987 -0.054846607 -0.05484657 -0.054846648 -0.054845564 -0.054844379 -0.054844182 -0.054843739][-0.054839153 -0.054843038 -0.05484695 -0.054843482 -0.054838054 -0.054832514 -0.054826908 -0.054821264 -0.054815665 -0.054815423 -0.054815382 -0.054813042 -0.054810561 -0.054810144 -0.054809351][-0.054806717 -0.054810517 -0.054814387 -0.054810882 -0.054803416 -0.054797698 -0.054793876 -0.054787893 -0.054781958 -0.054781526 -0.054781377 -0.054784663 -0.054787796 -0.054793146 -0.054798137][-0.054748606 -0.054749276 -0.054786962 -0.054783572 -0.054774273 -0.054760814 -0.054749113 -0.054734487 -0.054721683 -0.054720052 -0.054718819 -0.054734457 -0.054749981 -0.05476851 -0.054786805][-0.054701056 -0.054698016 -0.054767013 -0.054746911 -0.054722976 -0.05469802 -0.05468449 -0.05467438 -0.054658283 -0.054666311 -0.054682508 -0.054714207 -0.054734245 -0.054752834 -0.054771189][-0.05466472 -0.054659247 -0.054688267 -0.054664724 -0.054638479 -0.054613959 -0.054613713 -0.054616917 -0.054613121 -0.054633804 -0.054662555 -0.0547071 -0.05472707 -0.054745685 -0.054764077][-0.054696288 -0.054688934 -0.054681193 -0.054663029 -0.054642245 -0.054610681 -0.054603416 -0.054599334 -0.054595105 -0.054615956 -0.054644525 -0.054702803 -0.054736227 -0.054767709 -0.054799415][-0.054731656 -0.054724142 -0.054716945 -0.0546963 -0.054675598 -0.054644167 -0.054637115 -0.054633182 -0.054629035 -0.054637123 -0.054660946 -0.0547205 -0.054755233 -0.054786872 -0.054809295][-0.05476461 -0.0547573 -0.05475029 -0.054729626 -0.054708958 -0.05467784 -0.054671094 -0.054667644 -0.054684907 -0.054693073 -0.054715216 -0.054749958 -0.054785971 -0.054811861 -0.054828066][-0.054804306 -0.054800827 -0.054788958 -0.054767579 -0.054755956 -0.054732531 -0.054733552 -0.054738779 -0.054763667 -0.054773 -0.05478872 -0.054798972 -0.054821938 -0.054834675 -0.054847177][-0.054811973 -0.054812249 -0.054804321 -0.054799709 -0.054804452 -0.054803241 -0.054804113 -0.05480475 -0.054804806 -0.05480453 -0.054816883 -0.054829597 -0.054842338 -0.054855108 -0.054855246][-0.054858807 -0.054858685 -0.05485896 -0.054859385 -0.054859284 -0.054857429 -0.054855824 -0.054843158 -0.054830473 -0.054817565 -0.054817028 -0.0548296 -0.054842342 -0.054855108 -0.054868113]]...]
INFO - root - 2017-12-10 07:23:57.057743: step 1310, loss = 4.98, batch loss = 0.69 (39.7 examples/sec; 0.202 sec/batch; 18h:33m:25s remains)
INFO - root - 2017-12-10 07:23:59.098995: step 1320, loss = 5.02, batch loss = 0.69 (36.9 examples/sec; 0.217 sec/batch; 19h:58m:00s remains)
INFO - root - 2017-12-10 07:24:01.112445: step 1330, loss = 5.05, batch loss = 0.69 (40.2 examples/sec; 0.199 sec/batch; 18h:18m:37s remains)
INFO - root - 2017-12-10 07:24:03.144268: step 1340, loss = 5.07, batch loss = 0.69 (37.5 examples/sec; 0.213 sec/batch; 19h:36m:06s remains)
INFO - root - 2017-12-10 07:24:05.217450: step 1350, loss = 5.11, batch loss = 0.69 (38.8 examples/sec; 0.206 sec/batch; 18h:59m:21s remains)
INFO - root - 2017-12-10 07:24:07.253213: step 1360, loss = 5.13, batch loss = 0.69 (40.7 examples/sec; 0.196 sec/batch; 18h:04m:23s remains)
INFO - root - 2017-12-10 07:24:09.319470: step 1370, loss = 5.15, batch loss = 0.69 (40.0 examples/sec; 0.200 sec/batch; 18h:24m:07s remains)
INFO - root - 2017-12-10 07:24:11.368053: step 1380, loss = 5.17, batch loss = 0.69 (38.8 examples/sec; 0.206 sec/batch; 18h:57m:37s remains)
INFO - root - 2017-12-10 07:24:13.413121: step 1390, loss = 5.19, batch loss = 0.69 (40.4 examples/sec; 0.198 sec/batch; 18h:13m:26s remains)
INFO - root - 2017-12-10 07:24:15.472516: step 1400, loss = 5.21, batch loss = 0.69 (40.3 examples/sec; 0.198 sec/batch; 18h:14m:18s remains)
2017-12-10 07:24:15.850445: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.054466914 -0.054466914 -0.054466922 -0.054464698 -0.054464888 -0.054462709 -0.054460745 -0.054456431 -0.054453053 -0.054451846 -0.054453362 -0.054450657 -0.054451048 -0.054446012 -0.054446835][-0.054466914 -0.054466914 -0.054466914 -0.054466933 -0.0544647 -0.054464757 -0.054462712 -0.054458417 -0.054454371 -0.054454513 -0.054451417 -0.054450087 -0.054450937 -0.05444666 -0.054447524][-0.054443609 -0.054449584 -0.054455623 -0.054455634 -0.054453447 -0.054460891 -0.054462723 -0.054458428 -0.05445414 -0.054454289 -0.054454219 -0.054453887 -0.054453615 -0.054453496 -0.054453455][-0.054441374 -0.054440286 -0.054441523 -0.0544336 -0.054423261 -0.05442242 -0.0544162 -0.054413348 -0.054410569 -0.0544109 -0.054411046 -0.05441137 -0.054411452 -0.054410003 -0.054408059][-0.054390933 -0.054377329 -0.054366048 -0.054352041 -0.054345895 -0.054344509 -0.054340124 -0.054351248 -0.054362845 -0.054371685 -0.054372668 -0.05437367 -0.054374322 -0.054373682 -0.054371566][-0.054331556 -0.054310881 -0.054298975 -0.054284588 -0.054278348 -0.054285858 -0.054290544 -0.054304089 -0.054318089 -0.054329846 -0.054333813 -0.054336637 -0.0543446 -0.054350268 -0.054354515][-0.054237586 -0.054222416 -0.054216262 -0.054191362 -0.05418323 -0.054203063 -0.054218713 -0.054228339 -0.054238677 -0.054252893 -0.054259267 -0.054273877 -0.054298189 -0.05431845 -0.054337047][-0.0541909 -0.054130647 -0.054090932 -0.054066423 -0.054057974 -0.05406794 -0.054068893 -0.054106064 -0.054143969 -0.054180935 -0.054209489 -0.054245092 -0.054290589 -0.054310963 -0.054334268][-0.054144453 -0.054059036 -0.054015089 -0.0539869 -0.053970043 -0.053981144 -0.053992856 -0.054043289 -0.054085851 -0.0541273 -0.054169092 -0.054222979 -0.0542821 -0.054301858 -0.054323852][-0.054101985 -0.054036621 -0.053985815 -0.053953078 -0.053943872 -0.053940911 -0.053945754 -0.053991873 -0.05403972 -0.05409953 -0.054159578 -0.054223575 -0.054301191 -0.054337744 -0.054372489][-0.054157685 -0.054104518 -0.054057408 -0.054059219 -0.05405388 -0.054023542 -0.053995375 -0.054040864 -0.05408835 -0.0541421 -0.054195922 -0.054259058 -0.054331783 -0.054362182 -0.054389283][-0.05422572 -0.054179769 -0.05413378 -0.054137066 -0.054107785 -0.05409674 -0.054107271 -0.054144308 -0.054191321 -0.054238755 -0.054278936 -0.054325107 -0.054367244 -0.054391164 -0.054412212][-0.054306563 -0.054254539 -0.054252535 -0.05422492 -0.054227639 -0.054240152 -0.054247189 -0.054285258 -0.054307487 -0.054344542 -0.054366935 -0.054387979 -0.054407895 -0.054422643 -0.054436889][-0.054375384 -0.054370876 -0.054356586 -0.054358348 -0.054355331 -0.054368567 -0.054375276 -0.054391541 -0.054395389 -0.054408737 -0.054407738 -0.054422528 -0.05443681 -0.054436866 -0.054436915][-0.054416712 -0.054409366 -0.054411303 -0.054405347 -0.054408289 -0.054415297 -0.054419875 -0.054429486 -0.0544384 -0.054436944 -0.054422531 -0.054436788 -0.054436829 -0.054451849 -0.054466859]]...]
INFO - root - 2017-12-10 07:24:17.851930: step 1410, loss = 5.24, batch loss = 0.69 (40.1 examples/sec; 0.200 sec/batch; 18h:22m:14s remains)
INFO - root - 2017-12-10 07:24:19.859867: step 1420, loss = 5.28, batch loss = 0.69 (39.9 examples/sec; 0.200 sec/batch; 18h:25m:58s remains)
INFO - root - 2017-12-10 07:24:21.903954: step 1430, loss = 5.33, batch loss = 0.69 (39.5 examples/sec; 0.203 sec/batch; 18h:38m:52s remains)
INFO - root - 2017-12-10 07:24:23.953774: step 1440, loss = 5.38, batch loss = 0.69 (40.5 examples/sec; 0.198 sec/batch; 18h:10m:18s remains)
INFO - root - 2017-12-10 07:24:25.961195: step 1450, loss = 5.41, batch loss = 0.69 (40.4 examples/sec; 0.198 sec/batch; 18h:11m:25s remains)
INFO - root - 2017-12-10 07:24:27.983853: step 1460, loss = 5.44, batch loss = 0.69 (40.6 examples/sec; 0.197 sec/batch; 18h:06m:25s remains)
INFO - root - 2017-12-10 07:24:29.996724: step 1470, loss = 5.46, batch loss = 0.69 (40.0 examples/sec; 0.200 sec/batch; 18h:24m:17s remains)
INFO - root - 2017-12-10 07:24:32.037109: step 1480, loss = 5.50, batch loss = 0.69 (39.3 examples/sec; 0.203 sec/batch; 18h:42m:14s remains)
INFO - root - 2017-12-10 07:24:34.091819: step 1490, loss = 5.53, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 19h:12m:53s remains)
INFO - root - 2017-12-10 07:24:36.113220: step 1500, loss = 5.55, batch loss = 0.68 (40.2 examples/sec; 0.199 sec/batch; 18h:17m:56s remains)
2017-12-10 07:24:36.434949: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.053387739 -0.053367827 -0.053388961 -0.053405955 -0.0534018 -0.053397689 -0.053397711 -0.053393602 -0.05338772 -0.053385779 -0.053384166 -0.053384345 -0.053380642 -0.053375714 -0.053370941][-0.053367846 -0.053367846 -0.053367864 -0.05336792 -0.053363748 -0.053359624 -0.053355422 -0.053351209 -0.053346969 -0.053346943 -0.053346541 -0.053346008 -0.053345673 -0.053345621 -0.053342409][-0.053345665 -0.053354204 -0.053362876 -0.05336073 -0.053356837 -0.053359605 -0.053351287 -0.053347107 -0.053347018 -0.053346831 -0.053346697 -0.05334653 -0.053346209 -0.053345662 -0.053345166][-0.05333285 -0.053329073 -0.053330231 -0.053317171 -0.053293858 -0.053281523 -0.053275112 -0.053272035 -0.053268686 -0.053273395 -0.053278483 -0.053273436 -0.053268019 -0.053272374 -0.05327687][-0.053274121 -0.053239979 -0.053217813 -0.053184152 -0.053168066 -0.053141136 -0.053116996 -0.05311786 -0.053119358 -0.053148046 -0.053176068 -0.053192884 -0.053192142 -0.053200353 -0.053209458][-0.053048976 -0.039709069 -0.026065374 -0.016777717 -0.0090036429 -0.024191841 -0.040219553 -0.030965012 -0.019128695 -0.018335097 -0.030853242 -0.045309842 -0.048792794 -0.051389314 -0.052347921][-0.013365615 -0.012249809 -0.0110409 0.013641279 0.036268953 0.032834549 0.0067044795 -0.029723946 -0.042397678 -0.041550566 -0.029481288 -0.019411698 -0.03464165 -0.049795371 -0.051625703][0.032338817 0.0334833 0.034205209 0.035705831 0.036690775 0.033553112 0.0088190958 -0.012629937 -0.012828313 -0.014591862 -0.01805827 -0.022346355 -0.02606779 -0.028130416 -0.0408786][0.022871714 0.02105948 0.019182902 0.00459921 -0.010659911 0.010394577 0.030992854 0.027921248 0.025556196 0.0054166131 -0.016535755 -0.021702874 -0.026229605 -0.02825786 -0.040957108][0.022329602 0.0614884 0.083946481 0.070517465 0.071124524 0.0630137 0.056827191 0.022936743 -0.0097761974 -0.0311651 -0.052538171 -0.052690759 -0.052831925 -0.052945971 -0.053081907][0.0078936964 0.038076323 0.038258646 0.023633022 0.025988761 0.03865033 0.051402833 0.035297018 -0.0009322688 -0.037186965 -0.052511774 -0.052683283 -0.05283764 -0.052957218 -0.053083524][-0.00453474 0.0076720752 0.019579951 0.019647006 0.019719977 0.021164376 0.022608008 0.021158371 0.01968601 -0.016538586 -0.037101671 -0.037206471 -0.052941483 -0.053067174 -0.053187072][-0.0531 -0.05300393 -0.052977115 -0.052926369 -0.052895788 -0.05285963 -0.052820776 -0.016693093 0.01945379 0.019376177 0.019326206 -0.016914092 -0.05311146 -0.053188324 -0.053294353][-0.053262524 -0.053230081 -0.05322285 -0.053194739 -0.0531802 -0.053168733 -0.053169217 -0.053158596 -0.053138085 -0.053118102 -0.053098753 -0.053138476 -0.053221263 -0.053282134 -0.053303294][-0.053328987 -0.053320527 -0.053323794 -0.053317789 -0.053311974 -0.053313743 -0.053317606 -0.053290009 -0.053260338 -0.053254087 -0.053203788 -0.053219255 -0.053249855 -0.05330985 -0.053376086]]...]
INFO - root - 2017-12-10 07:24:38.451297: step 1510, loss = 5.58, batch loss = 0.67 (38.7 examples/sec; 0.207 sec/batch; 18h:59m:31s remains)
INFO - root - 2017-12-10 07:24:40.473610: step 1520, loss = 5.61, batch loss = 0.66 (40.1 examples/sec; 0.200 sec/batch; 18h:21m:17s remains)
INFO - root - 2017-12-10 07:24:42.474208: step 1530, loss = 5.63, batch loss = 0.65 (40.0 examples/sec; 0.200 sec/batch; 18h:24m:32s remains)
INFO - root - 2017-12-10 07:24:44.505240: step 1540, loss = 5.65, batch loss = 0.64 (38.7 examples/sec; 0.207 sec/batch; 18h:59m:21s remains)
INFO - root - 2017-12-10 07:24:46.553922: step 1550, loss = 5.67, batch loss = 0.64 (38.2 examples/sec; 0.210 sec/batch; 19h:16m:36s remains)
INFO - root - 2017-12-10 07:24:48.531165: step 1560, loss = 5.69, batch loss = 0.63 (40.2 examples/sec; 0.199 sec/batch; 18h:18m:58s remains)
INFO - root - 2017-12-10 07:24:50.543355: step 1570, loss = 5.73, batch loss = 0.64 (39.3 examples/sec; 0.204 sec/batch; 18h:43m:03s remains)
INFO - root - 2017-12-10 07:24:52.588907: step 1580, loss = 5.76, batch loss = 0.64 (36.8 examples/sec; 0.217 sec/batch; 19h:58m:40s remains)
INFO - root - 2017-12-10 07:24:54.619577: step 1590, loss = 5.78, batch loss = 0.63 (38.9 examples/sec; 0.206 sec/batch; 18h:53m:29s remains)
INFO - root - 2017-12-10 07:24:56.624582: step 1600, loss = 5.87, batch loss = 0.69 (40.3 examples/sec; 0.198 sec/batch; 18h:14m:05s remains)
2017-12-10 07:24:56.954111: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.053856924 -0.053857446 -0.053858694 -0.053858582 -0.053858332 -0.053860705 -0.053864073 -0.053863868 -0.053865571 -0.053864136 -0.053861592 -0.053858295 -0.053870071 -0.053867564 -0.053857975][-0.05380705 -0.053808685 -0.0538119 -0.053813282 -0.053813647 -0.053815953 -0.053818773 -0.053819053 -0.053819455 -0.0538228 -0.053821083 -0.053819332 -0.053812947 -0.053809781 -0.053824037][-0.053755525 -0.053754829 -0.053756546 -0.053759795 -0.053762782 -0.053766642 -0.053770997 -0.053772561 -0.053775605 -0.053775813 -0.053725243 -0.05367453 -0.0536775 -0.053673197 -0.053676441][-0.05372183 -0.053706765 -0.053693533 -0.0536582 -0.053630617 -0.053620744 -0.053615127 -0.053611834 -0.053614784 -0.053630725 -0.053650357 -0.05367351 -0.053687006 -0.053688504 -0.053690944][-0.053523231 -0.053452376 -0.053389117 -0.053299736 -0.053232629 -0.053202461 -0.053197309 -0.053226613 -0.053265449 -0.053332914 -0.05340331 -0.053479075 -0.053538285 -0.053575672 -0.053602345][-0.053149562 -0.053026229 -0.052914865 -0.052095819 -0.051320046 -0.051353749 -0.051290844 -0.052001558 -0.052856967 -0.052959513 -0.05308602 -0.053212646 -0.053325556 -0.053421937 -0.053501237][-0.052789114 -0.05259591 -0.052409891 -0.024571745 0.0042732619 0.0044719167 0.0042043589 0.046351716 0.07847371 0.048486985 0.021533191 0.0024042316 -0.017874718 -0.015724819 -0.0098068789][0.013268702 0.047670811 0.079205148 0.11388864 0.16375217 0.20825174 0.22896433 0.25717407 0.24903527 0.19551274 0.16212943 0.12285102 0.078488432 0.051036596 0.01366958][0.067146443 0.099035285 0.13280356 0.26633725 0.39490166 0.34765539 0.29521427 0.27638122 0.24936685 0.24247244 0.23082748 0.21014708 0.18062243 0.14437163 0.060558826][0.049790069 0.0607571 0.077947669 0.19204956 0.32695287 0.37713963 0.41016266 0.36358008 0.26801708 0.23217979 0.21596992 0.19171438 0.17888236 0.15045413 0.093950592][0.19782186 0.19665074 0.19917643 0.21279192 0.22707054 0.30112913 0.37448215 0.39914811 0.4202871 0.36375096 0.28285697 0.2412253 0.22731432 0.1339137 0.072909363][0.23853612 0.23599365 0.23363206 0.23313013 0.23357305 0.24362311 0.24553135 0.25889578 0.28650016 0.2470693 0.19688243 0.21313104 0.220449 0.14455041 0.082442738][0.021330632 0.020921849 0.020327851 0.018697575 0.017140992 0.016666591 0.00026030838 0.023238242 0.083337732 0.13225979 0.10285293 0.04139556 0.040526189 0.01024273 0.018830113][-0.05352113 -0.053518765 -0.053530145 -0.053504147 -0.053477149 -0.05345257 -0.053434651 -0.053423669 -0.053434525 -0.053507783 -0.053493936 -0.031999175 -0.010199111 -0.029632887 -0.051357754][-0.053798631 -0.053799555 -0.053808227 -0.053790685 -0.05378823 -0.053755749 -0.053755328 -0.053689357 -0.053655948 -0.053676285 -0.053689029 -0.053745206 -0.053727452 -0.053673834 -0.053630367]]...]
INFO - root - 2017-12-10 07:24:58.990861: step 1610, loss = 5.91, batch loss = 0.69 (40.2 examples/sec; 0.199 sec/batch; 18h:16m:50s remains)
INFO - root - 2017-12-10 07:25:00.994957: step 1620, loss = 5.94, batch loss = 0.69 (38.9 examples/sec; 0.206 sec/batch; 18h:53m:52s remains)
INFO - root - 2017-12-10 07:25:03.021440: step 1630, loss = 5.97, batch loss = 0.69 (36.6 examples/sec; 0.219 sec/batch; 20h:05m:59s remains)
INFO - root - 2017-12-10 07:25:05.015136: step 1640, loss = 6.00, batch loss = 0.69 (39.8 examples/sec; 0.201 sec/batch; 18h:29m:02s remains)
INFO - root - 2017-12-10 07:25:07.061645: step 1650, loss = 6.03, batch loss = 0.69 (38.4 examples/sec; 0.208 sec/batch; 19h:07m:36s remains)
INFO - root - 2017-12-10 07:25:09.069263: step 1660, loss = 6.06, batch loss = 0.69 (39.8 examples/sec; 0.201 sec/batch; 18h:29m:19s remains)
INFO - root - 2017-12-10 07:25:11.082043: step 1670, loss = 6.10, batch loss = 0.69 (39.2 examples/sec; 0.204 sec/batch; 18h:45m:07s remains)
INFO - root - 2017-12-10 07:25:13.115411: step 1680, loss = 6.12, batch loss = 0.67 (40.8 examples/sec; 0.196 sec/batch; 18h:02m:24s remains)
INFO - root - 2017-12-10 07:25:15.119949: step 1690, loss = 6.12, batch loss = 0.64 (40.7 examples/sec; 0.197 sec/batch; 18h:04m:28s remains)
INFO - root - 2017-12-10 07:25:17.139872: step 1700, loss = 6.19, batch loss = 0.69 (40.9 examples/sec; 0.196 sec/batch; 17h:58m:56s remains)
2017-12-10 07:25:17.447974: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.054310951 -0.054310694 -0.0543121 -0.054311387 -0.054311838 -0.054323416 -0.054319862 -0.054346435 -0.054342531 -0.054338675 -0.05433872 -0.05433869 -0.054336958 -0.054335427 -0.054335419][-0.054312084 -0.054312017 -0.054324556 -0.054323737 -0.054324329 -0.054335769 -0.054335833 -0.05434595 -0.054342318 -0.054338165 -0.054338373 -0.054338347 -0.054337434 -0.05433581 -0.054332476][-0.054304924 -0.054315604 -0.054337952 -0.054337978 -0.054348871 -0.054349057 -0.054345448 -0.0543418 -0.054337706 -0.054337814 -0.054337736 -0.054337703 -0.054337565 -0.054337554 -0.054337531][-0.054274846 -0.054168172 -0.054050412 -0.053933825 -0.053829946 -0.053701118 -0.053587496 -0.053586215 -0.053598609 -0.053609941 -0.053620826 -0.053631227 -0.053641766 -0.053745966 -0.053839672][-0.053519916 -0.053415552 -0.053283818 -0.0531011 -0.052914355 -0.052740328 -0.052579727 -0.052544627 -0.0525287 -0.052566163 -0.052660573 -0.052732475 -0.052803986 -0.052959211 -0.053199291][-0.052501593 -0.052363276 -0.052230168 -0.052040149 -0.051831204 -0.051638421 -0.051449172 -0.051439106 -0.051430862 -0.051509954 -0.051693287 -0.05186256 -0.052028716 -0.052346013 -0.052747987][-0.051633105 -0.051420737 -0.051327728 -0.051144987 -0.050940841 -0.050876565 -0.050671697 -0.050683353 -0.050622158 -0.050715234 -0.050904665 -0.051107135 -0.051308122 -0.051781125 -0.052345525][-0.051018868 -0.0507766 -0.050642453 -0.050401319 -0.050172769 -0.050221965 -0.050186295 -0.050198309 -0.050165918 -0.050323743 -0.050588865 -0.05086036 -0.051133066 -0.051677782 -0.052292943][-0.051078279 -0.050713465 -0.05056446 -0.050265603 -0.050024495 -0.050073728 -0.050022453 -0.050148528 -0.050186966 -0.05040773 -0.05071013 -0.051001225 -0.051290877 -0.051840678 -0.052456882][-0.051229518 -0.051003244 -0.05098768 -0.050805468 -0.050612286 -0.050729178 -0.05079079 -0.050919384 -0.050933775 -0.051148038 -0.051462889 -0.051781144 -0.052095681 -0.052464768 -0.052983232][-0.052068625 -0.051860593 -0.051846921 -0.051729783 -0.051694807 -0.05177214 -0.051880095 -0.051983792 -0.051994115 -0.052176617 -0.052390337 -0.052648086 -0.05289337 -0.053210832 -0.05352179][-0.053010926 -0.052824017 -0.052915096 -0.052715141 -0.052839726 -0.052875098 -0.052905768 -0.052986387 -0.053048391 -0.053195413 -0.053269975 -0.053435784 -0.053570844 -0.0537825 -0.053928107][-0.053751469 -0.053617712 -0.053698264 -0.053578049 -0.053465731 -0.053527214 -0.053648375 -0.053712778 -0.053793665 -0.053927831 -0.053934969 -0.054061942 -0.054219097 -0.054268595 -0.054279909][-0.054341577 -0.054227367 -0.054226235 -0.054168459 -0.054183155 -0.054061584 -0.054125991 -0.054176897 -0.054176886 -0.0542351 -0.054292519 -0.054349862 -0.05434991 -0.0543499 -0.054349888][-0.054348778 -0.054348778 -0.054341651 -0.054348778 -0.054340616 -0.054291688 -0.054234531 -0.054234792 -0.054292362 -0.0542926 -0.054292805 -0.054350216 -0.054350324 -0.054350313 -0.0543503]]...]
INFO - root - 2017-12-10 07:25:19.464276: step 1710, loss = 6.22, batch loss = 0.69 (38.4 examples/sec; 0.209 sec/batch; 19h:09m:46s remains)
INFO - root - 2017-12-10 07:25:21.514956: step 1720, loss = 6.24, batch loss = 0.69 (40.3 examples/sec; 0.198 sec/batch; 18h:13m:15s remains)
INFO - root - 2017-12-10 07:25:23.550623: step 1730, loss = 6.26, batch loss = 0.69 (39.6 examples/sec; 0.202 sec/batch; 18h:32m:36s remains)
INFO - root - 2017-12-10 07:25:25.608566: step 1740, loss = 6.29, batch loss = 0.69 (39.3 examples/sec; 0.203 sec/batch; 18h:41m:08s remains)
INFO - root - 2017-12-10 07:25:27.619770: step 1750, loss = 6.31, batch loss = 0.69 (40.2 examples/sec; 0.199 sec/batch; 18h:17m:07s remains)
INFO - root - 2017-12-10 07:25:29.641442: step 1760, loss = 6.34, batch loss = 0.69 (39.0 examples/sec; 0.205 sec/batch; 18h:52m:05s remains)
INFO - root - 2017-12-10 07:25:31.623815: step 1770, loss = 6.37, batch loss = 0.69 (39.8 examples/sec; 0.201 sec/batch; 18h:27m:35s remains)
INFO - root - 2017-12-10 07:25:33.620658: step 1780, loss = 6.40, batch loss = 0.69 (40.0 examples/sec; 0.200 sec/batch; 18h:23m:19s remains)
INFO - root - 2017-12-10 07:25:35.645205: step 1790, loss = 6.42, batch loss = 0.69 (39.3 examples/sec; 0.203 sec/batch; 18h:41m:11s remains)
INFO - root - 2017-12-10 07:25:37.674370: step 1800, loss = 6.45, batch loss = 0.69 (38.7 examples/sec; 0.207 sec/batch; 18h:58m:16s remains)
2017-12-10 07:25:38.070363: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.054203294 -0.054203462 -0.054203622 -0.054203849 -0.054204233 -0.054204624 -0.054205097 -0.054205518 -0.054206189 -0.054206796 -0.054207221 -0.054207634 -0.054207884 -0.054208126 -0.054208126][-0.054203629 -0.054203793 -0.054203954 -0.054204181 -0.054204393 -0.054204777 -0.054205082 -0.054205447 -0.054205935 -0.054206364 -0.054206789 -0.054207031 -0.054207277 -0.054207522 -0.054207642][-0.0542042 -0.0542045 -0.0542048 -0.054205026 -0.054205239 -0.054205325 -0.054205328 -0.054205395 -0.054205582 -0.054205827 -0.05420607 -0.054206312 -0.054206554 -0.054206919 -0.054207161][-0.054204565 -0.054204803 -0.054205038 -0.054070342 -0.053944122 -0.053819105 -0.053819109 -0.053819053 -0.05381912 -0.053835124 -0.053846736 -0.053861074 -0.053753283 -0.053648103 -0.053544328][-0.053723317 -0.053596377 -0.053472858 -0.053090058 -0.052842308 -0.052592356 -0.052480377 -0.052511491 -0.052540459 -0.052717932 -0.0528887 -0.053056605 -0.053099789 -0.053007539 -0.052917022][-0.052335996 -0.05206522 -0.051806003 -0.051428221 -0.051200464 -0.050985143 -0.050905347 -0.05092191 -0.051067587 -0.051348682 -0.051631939 -0.051923953 -0.052107874 -0.052197475 -0.052264955][-0.050522491 -0.050230358 -0.04994294 -0.049534641 -0.049287312 -0.049068045 -0.048994027 -0.049013197 -0.049159169 -0.049475439 -0.049810629 -0.0501702 -0.050430611 -0.050840057 -0.051228441][-0.0487971 -0.048448086 -0.048108075 -0.047658518 -0.047387026 -0.047159072 -0.047087293 -0.047287762 -0.047614019 -0.0481087 -0.048619773 -0.049197726 -0.04968036 -0.050229844 -0.050758291][-0.047921013 -0.047471408 -0.047025178 -0.046657842 -0.046367951 -0.046222765 -0.046238117 -0.046640083 -0.047061864 -0.04766712 -0.048289459 -0.048977591 -0.04956878 -0.050118141 -0.050758049][-0.04792089 -0.047471285 -0.047025058 -0.0467926 -0.046629116 -0.046608988 -0.046624336 -0.046933174 -0.047262456 -0.047759309 -0.048105426 -0.048779346 -0.049478441 -0.05022648 -0.051062904][-0.048402417 -0.04807993 -0.047757395 -0.047772981 -0.04773096 -0.047835737 -0.047963068 -0.048240736 -0.048540995 -0.048876259 -0.0490631 -0.049583331 -0.050131332 -0.050866317 -0.051689483][-0.049790096 -0.049611382 -0.049424488 -0.049434997 -0.049372926 -0.049443007 -0.0495381 -0.049830314 -0.050013866 -0.050245509 -0.050319862 -0.050715983 -0.051123247 -0.051676381 -0.052341551][-0.051603969 -0.051446617 -0.051287919 -0.051328879 -0.051286444 -0.051360533 -0.051449902 -0.05173957 -0.05192288 -0.052119467 -0.05214189 -0.05247033 -0.052800991 -0.05303416 -0.053378306][-0.053329375 -0.053228907 -0.053122811 -0.053205032 -0.053186748 -0.05326964 -0.053356886 -0.053465251 -0.05346828 -0.053486448 -0.053332984 -0.053443022 -0.053551428 -0.05364453 -0.053848591][-0.054205224 -0.054205224 -0.054205224 -0.054205224 -0.054205343 -0.054205582 -0.054205824 -0.054112807 -0.054020539 -0.053928245 -0.053663623 -0.053663578 -0.053663429 -0.053756639 -0.053849116]]...]
INFO - root - 2017-12-10 07:25:40.110090: step 1810, loss = 6.47, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 19h:02m:59s remains)
INFO - root - 2017-12-10 07:25:42.107292: step 1820, loss = 6.49, batch loss = 0.69 (39.2 examples/sec; 0.204 sec/batch; 18h:43m:34s remains)
