INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "46"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-03 09:50:38.192747: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 09:50:38.192786: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 09:50:38.192792: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 09:50:38.192797: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 09:50:38.192801: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 09:50:38.779208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2017-12-03 09:50:38.779246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-03 09:50:38.779254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-03 09:50:38.779262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-03 09:50:41.560929: step 0, loss = 1.29, batch loss = 1.05 (3.5 examples/sec; 2.292 sec/batch; 211h:41m:31s remains)
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-03 09:50:43.818918: step 10, loss = 1.24, batch loss = 0.99 (44.7 examples/sec; 0.179 sec/batch; 16h:32m:45s remains)
INFO - root - 2017-12-03 09:50:45.608114: step 20, loss = 1.24, batch loss = 0.99 (45.5 examples/sec; 0.176 sec/batch; 16h:15m:06s remains)
INFO - root - 2017-12-03 09:50:47.390633: step 30, loss = 1.22, batch loss = 0.97 (44.2 examples/sec; 0.181 sec/batch; 16h:42m:57s remains)
INFO - root - 2017-12-03 09:50:49.175594: step 40, loss = 1.30, batch loss = 1.06 (43.8 examples/sec; 0.182 sec/batch; 16h:51m:11s remains)
INFO - root - 2017-12-03 09:50:50.944194: step 50, loss = 1.31, batch loss = 1.07 (45.1 examples/sec; 0.177 sec/batch; 16h:22m:02s remains)
INFO - root - 2017-12-03 09:50:52.749883: step 60, loss = 1.19, batch loss = 0.94 (44.8 examples/sec; 0.179 sec/batch; 16h:29m:54s remains)
INFO - root - 2017-12-03 09:50:54.538136: step 70, loss = 1.26, batch loss = 1.01 (44.7 examples/sec; 0.179 sec/batch; 16h:31m:09s remains)
INFO - root - 2017-12-03 09:50:56.337091: step 80, loss = 1.20, batch loss = 0.96 (44.6 examples/sec; 0.179 sec/batch; 16h:34m:19s remains)
INFO - root - 2017-12-03 09:50:58.133777: step 90, loss = 1.27, batch loss = 1.02 (45.7 examples/sec; 0.175 sec/batch; 16h:08m:54s remains)
INFO - root - 2017-12-03 09:50:59.900164: step 100, loss = 1.32, batch loss = 1.07 (45.3 examples/sec; 0.177 sec/batch; 16h:19m:10s remains)
INFO - root - 2017-12-03 09:51:01.766174: step 110, loss = 1.31, batch loss = 1.07 (45.2 examples/sec; 0.177 sec/batch; 16h:20m:50s remains)
INFO - root - 2017-12-03 09:51:03.559505: step 120, loss = 1.23, batch loss = 0.98 (45.6 examples/sec; 0.175 sec/batch; 16h:11m:02s remains)
INFO - root - 2017-12-03 09:51:05.337937: step 130, loss = 1.23, batch loss = 0.98 (45.1 examples/sec; 0.177 sec/batch; 16h:22m:32s remains)
INFO - root - 2017-12-03 09:51:07.153834: step 140, loss = 1.28, batch loss = 1.03 (44.6 examples/sec; 0.179 sec/batch; 16h:32m:53s remains)
INFO - root - 2017-12-03 09:51:08.951830: step 150, loss = 1.28, batch loss = 1.03 (43.6 examples/sec; 0.184 sec/batch; 16h:56m:56s remains)
INFO - root - 2017-12-03 09:51:10.786154: step 160, loss = 1.31, batch loss = 1.07 (44.6 examples/sec; 0.180 sec/batch; 16h:34m:30s remains)
INFO - root - 2017-12-03 09:51:12.591976: step 170, loss = 1.30, batch loss = 1.05 (44.9 examples/sec; 0.178 sec/batch; 16h:26m:20s remains)
INFO - root - 2017-12-03 09:51:14.394354: step 180, loss = 1.21, batch loss = 0.96 (43.5 examples/sec; 0.184 sec/batch; 16h:58m:19s remains)
INFO - root - 2017-12-03 09:51:16.199275: step 190, loss = 1.39, batch loss = 1.14 (43.9 examples/sec; 0.182 sec/batch; 16h:50m:04s remains)
INFO - root - 2017-12-03 09:51:18.022446: step 200, loss = 1.31, batch loss = 1.06 (41.2 examples/sec; 0.194 sec/batch; 17h:56m:17s remains)
INFO - root - 2017-12-03 09:51:19.912683: step 210, loss = 1.32, batch loss = 1.08 (44.8 examples/sec; 0.179 sec/batch; 16h:28m:57s remains)
INFO - root - 2017-12-03 09:51:21.729286: step 220, loss = 1.27, batch loss = 1.02 (42.4 examples/sec; 0.189 sec/batch; 17h:25m:32s remains)
INFO - root - 2017-12-03 09:51:23.555800: step 230, loss = 1.19, batch loss = 0.95 (43.9 examples/sec; 0.182 sec/batch; 16h:49m:14s remains)
INFO - root - 2017-12-03 09:51:25.393473: step 240, loss = 1.23, batch loss = 0.98 (45.0 examples/sec; 0.178 sec/batch; 16h:24m:42s remains)
INFO - root - 2017-12-03 09:51:27.223979: step 250, loss = 1.13, batch loss = 0.88 (44.6 examples/sec; 0.180 sec/batch; 16h:34m:06s remains)
INFO - root - 2017-12-03 09:51:29.039164: step 260, loss = 1.31, batch loss = 1.06 (42.4 examples/sec; 0.189 sec/batch; 17h:23m:54s remains)
INFO - root - 2017-12-03 09:51:30.869006: step 270, loss = 1.29, batch loss = 1.05 (45.0 examples/sec; 0.178 sec/batch; 16h:24m:07s remains)
INFO - root - 2017-12-03 09:51:32.679154: step 280, loss = 1.29, batch loss = 1.04 (43.9 examples/sec; 0.182 sec/batch; 16h:47m:53s remains)
INFO - root - 2017-12-03 09:51:34.500648: step 290, loss = 1.25, batch loss = 1.00 (44.4 examples/sec; 0.180 sec/batch; 16h:38m:24s remains)
INFO - root - 2017-12-03 09:51:36.336405: step 300, loss = 1.37, batch loss = 1.12 (42.4 examples/sec; 0.189 sec/batch; 17h:23m:41s remains)
INFO - root - 2017-12-03 09:51:38.249830: step 310, loss = 1.30, batch loss = 1.05 (44.4 examples/sec; 0.180 sec/batch; 16h:37m:36s remains)
INFO - root - 2017-12-03 09:51:40.059799: step 320, loss = 1.27, batch loss = 1.02 (42.8 examples/sec; 0.187 sec/batch; 17h:14m:37s remains)
INFO - root - 2017-12-03 09:51:41.902908: step 330, loss = 1.31, batch loss = 1.07 (42.8 examples/sec; 0.187 sec/batch; 17h:15m:50s remains)
INFO - root - 2017-12-03 09:51:43.710248: step 340, loss = 1.25, batch loss = 1.00 (45.7 examples/sec; 0.175 sec/batch; 16h:08m:32s remains)
INFO - root - 2017-12-03 09:51:45.527429: step 350, loss = 1.24, batch loss = 1.00 (43.7 examples/sec; 0.183 sec/batch; 16h:52m:51s remains)
INFO - root - 2017-12-03 09:51:47.334190: step 360, loss = 1.23, batch loss = 0.99 (43.7 examples/sec; 0.183 sec/batch; 16h:52m:35s remains)
INFO - root - 2017-12-03 09:51:49.171659: step 370, loss = 1.22, batch loss = 0.97 (41.0 examples/sec; 0.195 sec/batch; 17h:59m:31s remains)
INFO - root - 2017-12-03 09:51:50.995837: step 380, loss = 1.22, batch loss = 0.98 (43.8 examples/sec; 0.183 sec/batch; 16h:52m:00s remains)
INFO - root - 2017-12-03 09:51:52.839998: step 390, loss = 1.30, batch loss = 1.05 (42.3 examples/sec; 0.189 sec/batch; 17h:25m:43s remains)
INFO - root - 2017-12-03 09:51:54.664298: step 400, loss = 1.28, batch loss = 1.04 (44.4 examples/sec; 0.180 sec/batch; 16h:37m:30s remains)
INFO - root - 2017-12-03 09:51:56.556486: step 410, loss = 1.21, batch loss = 0.96 (43.0 examples/sec; 0.186 sec/batch; 17h:09m:07s remains)
INFO - root - 2017-12-03 09:51:58.418755: step 420, loss = 1.24, batch loss = 0.99 (44.4 examples/sec; 0.180 sec/batch; 16h:37m:42s remains)
INFO - root - 2017-12-03 09:52:00.247024: step 430, loss = 1.25, batch loss = 1.00 (43.4 examples/sec; 0.184 sec/batch; 17h:00m:48s remains)
INFO - root - 2017-12-03 09:52:02.082865: step 440, loss = 1.33, batch loss = 1.08 (42.7 examples/sec; 0.187 sec/batch; 17h:16m:53s remains)
INFO - root - 2017-12-03 09:52:03.914053: step 450, loss = 1.23, batch loss = 0.98 (46.0 examples/sec; 0.174 sec/batch; 16h:02m:52s remains)
INFO - root - 2017-12-03 09:52:05.757427: step 460, loss = 1.17, batch loss = 0.92 (43.9 examples/sec; 0.182 sec/batch; 16h:48m:20s remains)
INFO - root - 2017-12-03 09:52:07.585557: step 470, loss = 1.25, batch loss = 1.00 (44.7 examples/sec; 0.179 sec/batch; 16h:30m:39s remains)
INFO - root - 2017-12-03 09:52:09.418799: step 480, loss = 1.28, batch loss = 1.03 (43.0 examples/sec; 0.186 sec/batch; 17h:10m:28s remains)
INFO - root - 2017-12-03 09:52:11.242642: step 490, loss = 1.21, batch loss = 0.96 (44.2 examples/sec; 0.181 sec/batch; 16h:41m:51s remains)
INFO - root - 2017-12-03 09:52:13.071303: step 500, loss = 1.30, batch loss = 1.05 (42.8 examples/sec; 0.187 sec/batch; 17h:13m:31s remains)
INFO - root - 2017-12-03 09:52:14.964242: step 510, loss = 1.30, batch loss = 1.05 (43.9 examples/sec; 0.182 sec/batch; 16h:49m:13s remains)
INFO - root - 2017-12-03 09:52:16.805802: step 520, loss = 1.30, batch loss = 1.05 (43.7 examples/sec; 0.183 sec/batch; 16h:53m:26s remains)
INFO - root - 2017-12-03 09:52:18.647920: step 530, loss = 1.25, batch loss = 1.00 (43.5 examples/sec; 0.184 sec/batch; 16h:57m:02s remains)
INFO - root - 2017-12-03 09:52:20.480522: step 540, loss = 1.33, batch loss = 1.08 (43.8 examples/sec; 0.183 sec/batch; 16h:51m:09s remains)
INFO - root - 2017-12-03 09:52:22.321458: step 550, loss = 1.34, batch loss = 1.09 (41.8 examples/sec; 0.191 sec/batch; 17h:38m:10s remains)
INFO - root - 2017-12-03 09:52:24.143261: step 560, loss = 1.24, batch loss = 0.99 (45.0 examples/sec; 0.178 sec/batch; 16h:23m:43s remains)
INFO - root - 2017-12-03 09:52:25.981279: step 570, loss = 1.25, batch loss = 1.01 (43.6 examples/sec; 0.183 sec/batch; 16h:55m:04s remains)
INFO - root - 2017-12-03 09:52:27.810891: step 580, loss = 1.28, batch loss = 1.04 (44.3 examples/sec; 0.180 sec/batch; 16h:37m:56s remains)
INFO - root - 2017-12-03 09:52:29.641439: step 590, loss = 1.20, batch loss = 0.95 (43.4 examples/sec; 0.184 sec/batch; 16h:58m:52s remains)
INFO - root - 2017-12-03 09:52:31.482903: step 600, loss = 1.36, batch loss = 1.11 (41.5 examples/sec; 0.193 sec/batch; 17h:46m:17s remains)
INFO - root - 2017-12-03 09:52:33.418220: step 610, loss = 1.30, batch loss = 1.06 (44.2 examples/sec; 0.181 sec/batch; 16h:41m:16s remains)
INFO - root - 2017-12-03 09:52:35.268261: step 620, loss = 1.24, batch loss = 0.99 (43.4 examples/sec; 0.184 sec/batch; 16h:58m:58s remains)
INFO - root - 2017-12-03 09:52:37.098439: step 630, loss = 1.34, batch loss = 1.09 (44.1 examples/sec; 0.181 sec/batch; 16h:42m:47s remains)
INFO - root - 2017-12-03 09:52:38.937093: step 640, loss = 1.26, batch loss = 1.01 (44.3 examples/sec; 0.181 sec/batch; 16h:39m:26s remains)
INFO - root - 2017-12-03 09:52:40.768482: step 650, loss = 1.21, batch loss = 0.96 (43.5 examples/sec; 0.184 sec/batch; 16h:56m:33s remains)
INFO - root - 2017-12-03 09:52:42.597650: step 660, loss = 1.23, batch loss = 0.98 (44.7 examples/sec; 0.179 sec/batch; 16h:30m:49s remains)
INFO - root - 2017-12-03 09:52:44.434808: step 670, loss = 1.25, batch loss = 1.00 (42.8 examples/sec; 0.187 sec/batch; 17h:14m:12s remains)
INFO - root - 2017-12-03 09:52:46.280954: step 680, loss = 1.20, batch loss = 0.95 (44.1 examples/sec; 0.181 sec/batch; 16h:43m:32s remains)
INFO - root - 2017-12-03 09:52:48.114554: step 690, loss = 1.25, batch loss = 1.00 (43.6 examples/sec; 0.184 sec/batch; 16h:54m:54s remains)
INFO - root - 2017-12-03 09:52:49.951158: step 700, loss = 1.23, batch loss = 0.99 (43.8 examples/sec; 0.182 sec/batch; 16h:48m:54s remains)
INFO - root - 2017-12-03 09:52:51.837187: step 710, loss = 1.36, batch loss = 1.11 (45.1 examples/sec; 0.178 sec/batch; 16h:21m:39s remains)
INFO - root - 2017-12-03 09:52:53.682635: step 720, loss = 1.24, batch loss = 1.00 (42.1 examples/sec; 0.190 sec/batch; 17h:31m:44s remains)
INFO - root - 2017-12-03 09:52:55.517773: step 730, loss = 1.20, batch loss = 0.96 (42.5 examples/sec; 0.188 sec/batch; 17h:21m:12s remains)
INFO - root - 2017-12-03 09:52:57.355515: step 740, loss = 1.22, batch loss = 0.97 (43.9 examples/sec; 0.182 sec/batch; 16h:47m:18s remains)
INFO - root - 2017-12-03 09:52:59.174131: step 750, loss = 1.42, batch loss = 1.18 (43.8 examples/sec; 0.183 sec/batch; 16h:50m:19s remains)
INFO - root - 2017-12-03 09:53:00.995351: step 760, loss = 1.34, batch loss = 1.09 (44.3 examples/sec; 0.181 sec/batch; 16h:39m:33s remains)
INFO - root - 2017-12-03 09:53:02.881663: step 770, loss = 1.28, batch loss = 1.03 (43.1 examples/sec; 0.186 sec/batch; 17h:06m:37s remains)
INFO - root - 2017-12-03 09:53:04.697982: step 780, loss = 1.22, batch loss = 0.97 (43.9 examples/sec; 0.182 sec/batch; 16h:48m:17s remains)
INFO - root - 2017-12-03 09:53:06.545386: step 790, loss = 1.20, batch loss = 0.95 (42.4 examples/sec; 0.189 sec/batch; 17h:23m:24s remains)
INFO - root - 2017-12-03 09:53:08.369654: step 800, loss = 1.26, batch loss = 1.01 (44.1 examples/sec; 0.181 sec/batch; 16h:42m:19s remains)
INFO - root - 2017-12-03 09:53:10.262913: step 810, loss = 1.20, batch loss = 0.95 (43.5 examples/sec; 0.184 sec/batch; 16h:55m:54s remains)
INFO - root - 2017-12-03 09:53:12.099307: step 820, loss = 1.27, batch loss = 1.02 (41.9 examples/sec; 0.191 sec/batch; 17h:35m:30s remains)
INFO - root - 2017-12-03 09:53:13.929313: step 830, loss = 1.27, batch loss = 1.02 (44.5 examples/sec; 0.180 sec/batch; 16h:34m:22s remains)
INFO - root - 2017-12-03 09:53:15.764781: step 840, loss = 1.24, batch loss = 1.00 (43.5 examples/sec; 0.184 sec/batch; 16h:56m:44s remains)
INFO - root - 2017-12-03 09:53:17.595177: step 850, loss = 1.29, batch loss = 1.05 (44.3 examples/sec; 0.181 sec/batch; 16h:37m:55s remains)
INFO - root - 2017-12-03 09:53:19.430166: step 860, loss = 1.32, batch loss = 1.07 (44.1 examples/sec; 0.181 sec/batch; 16h:42m:51s remains)
INFO - root - 2017-12-03 09:53:21.267352: step 870, loss = 1.26, batch loss = 1.02 (43.2 examples/sec; 0.185 sec/batch; 17h:03m:25s remains)
INFO - root - 2017-12-03 09:53:23.123326: step 880, loss = 1.27, batch loss = 1.02 (45.0 examples/sec; 0.178 sec/batch; 16h:21m:39s remains)
INFO - root - 2017-12-03 09:53:24.967896: step 890, loss = 1.32, batch loss = 1.07 (41.9 examples/sec; 0.191 sec/batch; 17h:35m:09s remains)
INFO - root - 2017-12-03 09:53:26.818576: step 900, loss = 1.24, batch loss = 0.99 (40.4 examples/sec; 0.198 sec/batch; 18h:13m:41s remains)
INFO - root - 2017-12-03 09:53:28.717153: step 910, loss = 1.38, batch loss = 1.13 (45.1 examples/sec; 0.177 sec/batch; 16h:19m:36s remains)
INFO - root - 2017-12-03 09:53:30.560004: step 920, loss = 1.26, batch loss = 1.02 (42.8 examples/sec; 0.187 sec/batch; 17h:13m:11s remains)
INFO - root - 2017-12-03 09:53:32.386321: step 930, loss = 1.29, batch loss = 1.04 (44.4 examples/sec; 0.180 sec/batch; 16h:36m:27s remains)
INFO - root - 2017-12-03 09:53:34.217579: step 940, loss = 1.18, batch loss = 0.93 (44.3 examples/sec; 0.181 sec/batch; 16h:37m:32s remains)
INFO - root - 2017-12-03 09:53:36.049731: step 950, loss = 1.24, batch loss = 0.99 (43.4 examples/sec; 0.184 sec/batch; 16h:58m:00s remains)
INFO - root - 2017-12-03 09:53:37.887991: step 960, loss = 1.29, batch loss = 1.05 (44.0 examples/sec; 0.182 sec/batch; 16h:43m:35s remains)
INFO - root - 2017-12-03 09:53:39.722162: step 970, loss = 1.27, batch loss = 1.02 (43.1 examples/sec; 0.186 sec/batch; 17h:05m:49s remains)
INFO - root - 2017-12-03 09:53:41.571924: step 980, loss = 1.35, batch loss = 1.10 (43.1 examples/sec; 0.186 sec/batch; 17h:06m:17s remains)
INFO - root - 2017-12-03 09:53:43.435329: step 990, loss = 1.19, batch loss = 0.94 (44.5 examples/sec; 0.180 sec/batch; 16h:32m:43s remains)
INFO - root - 2017-12-03 09:53:45.266101: step 1000, loss = 1.30, batch loss = 1.05 (44.0 examples/sec; 0.182 sec/batch; 16h:44m:19s remains)
INFO - root - 2017-12-03 09:53:47.165560: step 1010, loss = 1.33, batch loss = 1.08 (43.5 examples/sec; 0.184 sec/batch; 16h:55m:17s remains)
INFO - root - 2017-12-03 09:53:48.998940: step 1020, loss = 1.26, batch loss = 1.02 (42.6 examples/sec; 0.188 sec/batch; 17h:16m:44s remains)
INFO - root - 2017-12-03 09:53:50.822862: step 1030, loss = 1.23, batch loss = 0.99 (45.3 examples/sec; 0.176 sec/batch; 16h:14m:42s remains)
INFO - root - 2017-12-03 09:53:52.664744: step 1040, loss = 1.19, batch loss = 0.94 (44.7 examples/sec; 0.179 sec/batch; 16h:27m:49s remains)
INFO - root - 2017-12-03 09:53:54.517469: step 1050, loss = 1.30, batch loss = 1.05 (42.4 examples/sec; 0.189 sec/batch; 17h:22m:55s remains)
INFO - root - 2017-12-03 09:53:56.396258: step 1060, loss = 1.30, batch loss = 1.05 (43.2 examples/sec; 0.185 sec/batch; 17h:02m:41s remains)
INFO - root - 2017-12-03 09:53:58.246385: step 1070, loss = 1.39, batch loss = 1.14 (42.8 examples/sec; 0.187 sec/batch; 17h:12m:02s remains)
INFO - root - 2017-12-03 09:54:00.088723: step 1080, loss = 1.29, batch loss = 1.04 (44.7 examples/sec; 0.179 sec/batch; 16h:29m:16s remains)
INFO - root - 2017-12-03 09:54:01.926187: step 1090, loss = 1.26, batch loss = 1.01 (43.9 examples/sec; 0.182 sec/batch; 16h:47m:26s remains)
INFO - root - 2017-12-03 09:54:03.777557: step 1100, loss = 1.33, batch loss = 1.09 (42.5 examples/sec; 0.188 sec/batch; 17h:20m:33s remains)
INFO - root - 2017-12-03 09:54:05.704726: step 1110, loss = 1.21, batch loss = 0.96 (43.3 examples/sec; 0.185 sec/batch; 17h:01m:15s remains)
INFO - root - 2017-12-03 09:54:07.547919: step 1120, loss = 1.30, batch loss = 1.05 (44.9 examples/sec; 0.178 sec/batch; 16h:24m:48s remains)
INFO - root - 2017-12-03 09:54:09.426364: step 1130, loss = 1.24, batch loss = 0.99 (42.8 examples/sec; 0.187 sec/batch; 17h:11m:40s remains)
INFO - root - 2017-12-03 09:54:11.276078: step 1140, loss = 1.21, batch loss = 0.96 (42.7 examples/sec; 0.187 sec/batch; 17h:14m:34s remains)
INFO - root - 2017-12-03 09:54:13.103633: step 1150, loss = 1.33, batch loss = 1.08 (43.5 examples/sec; 0.184 sec/batch; 16h:55m:34s remains)
INFO - root - 2017-12-03 09:54:14.966756: step 1160, loss = 1.27, batch loss = 1.02 (43.9 examples/sec; 0.182 sec/batch; 16h:46m:27s remains)
INFO - root - 2017-12-03 09:54:16.851291: step 1170, loss = 1.26, batch loss = 1.01 (43.7 examples/sec; 0.183 sec/batch; 16h:51m:57s remains)
INFO - root - 2017-12-03 09:54:18.687010: step 1180, loss = 1.24, batch loss = 0.99 (45.0 examples/sec; 0.178 sec/batch; 16h:21m:19s remains)
INFO - root - 2017-12-03 09:54:20.545661: step 1190, loss = 1.31, batch loss = 1.07 (41.0 examples/sec; 0.195 sec/batch; 17h:57m:41s remains)
INFO - root - 2017-12-03 09:54:22.383079: step 1200, loss = 1.26, batch loss = 1.01 (43.0 examples/sec; 0.186 sec/batch; 17h:06m:17s remains)
INFO - root - 2017-12-03 09:54:24.311395: step 1210, loss = 1.27, batch loss = 1.03 (43.6 examples/sec; 0.184 sec/batch; 16h:53m:17s remains)
INFO - root - 2017-12-03 09:54:26.153907: step 1220, loss = 1.34, batch loss = 1.09 (43.6 examples/sec; 0.183 sec/batch; 16h:51m:58s remains)
INFO - root - 2017-12-03 09:54:28.016556: step 1230, loss = 1.29, batch loss = 1.04 (44.5 examples/sec; 0.180 sec/batch; 16h:32m:36s remains)
INFO - root - 2017-12-03 09:54:29.856512: step 1240, loss = 1.36, batch loss = 1.12 (42.1 examples/sec; 0.190 sec/batch; 17h:30m:18s remains)
INFO - root - 2017-12-03 09:54:31.707183: step 1250, loss = 1.25, batch loss = 1.00 (43.2 examples/sec; 0.185 sec/batch; 17h:02m:50s remains)
INFO - root - 2017-12-03 09:54:33.555530: step 1260, loss = 1.22, batch loss = 0.97 (44.0 examples/sec; 0.182 sec/batch; 16h:44m:14s remains)
INFO - root - 2017-12-03 09:54:35.412899: step 1270, loss = 1.28, batch loss = 1.04 (42.4 examples/sec; 0.189 sec/batch; 17h:22m:02s remains)
INFO - root - 2017-12-03 09:54:37.265582: step 1280, loss = 1.27, batch loss = 1.02 (44.8 examples/sec; 0.178 sec/batch; 16h:25m:03s remains)
INFO - root - 2017-12-03 09:54:39.100491: step 1290, loss = 1.30, batch loss = 1.05 (43.3 examples/sec; 0.185 sec/batch; 16h:59m:52s remains)
INFO - root - 2017-12-03 09:54:40.934631: step 1300, loss = 1.24, batch loss = 1.00 (43.0 examples/sec; 0.186 sec/batch; 17h:05m:52s remains)
INFO - root - 2017-12-03 09:54:42.865465: step 1310, loss = 1.21, batch loss = 0.96 (44.2 examples/sec; 0.181 sec/batch; 16h:40m:00s remains)
INFO - root - 2017-12-03 09:54:44.715019: step 1320, loss = 1.17, batch loss = 0.92 (43.8 examples/sec; 0.183 sec/batch; 16h:48m:28s remains)
INFO - root - 2017-12-03 09:54:46.555252: step 1330, loss = 1.28, batch loss = 1.03 (44.8 examples/sec; 0.179 sec/batch; 16h:25m:35s remains)
INFO - root - 2017-12-03 09:54:48.400123: step 1340, loss = 1.23, batch loss = 0.99 (42.9 examples/sec; 0.186 sec/batch; 17h:09m:09s remains)
INFO - root - 2017-12-03 09:54:50.277606: step 1350, loss = 1.24, batch loss = 0.99 (42.8 examples/sec; 0.187 sec/batch; 17h:10m:26s remains)
INFO - root - 2017-12-03 09:54:52.127871: step 1360, loss = 1.25, batch loss = 1.00 (42.5 examples/sec; 0.188 sec/batch; 17h:18m:54s remains)
INFO - root - 2017-12-03 09:54:53.971364: step 1370, loss = 1.25, batch loss = 1.00 (43.4 examples/sec; 0.184 sec/batch; 16h:57m:33s remains)
INFO - root - 2017-12-03 09:54:55.864362: step 1380, loss = 1.20, batch loss = 0.95 (41.6 examples/sec; 0.192 sec/batch; 17h:40m:16s remains)
INFO - root - 2017-12-03 09:54:57.699497: step 1390, loss = 1.22, batch loss = 0.97 (42.8 examples/sec; 0.187 sec/batch; 17h:12m:12s remains)
INFO - root - 2017-12-03 09:54:59.519134: step 1400, loss = 1.34, batch loss = 1.10 (44.2 examples/sec; 0.181 sec/batch; 16h:39m:24s remains)
INFO - root - 2017-12-03 09:55:01.440679: step 1410, loss = 1.19, batch loss = 0.95 (43.8 examples/sec; 0.182 sec/batch; 16h:46m:58s remains)
INFO - root - 2017-12-03 09:55:03.292275: step 1420, loss = 1.29, batch loss = 1.05 (42.9 examples/sec; 0.187 sec/batch; 17h:09m:32s remains)
INFO - root - 2017-12-03 09:55:05.143097: step 1430, loss = 1.28, batch loss = 1.03 (44.0 examples/sec; 0.182 sec/batch; 16h:43m:01s remains)
INFO - root - 2017-12-03 09:55:07.001060: step 1440, loss = 1.20, batch loss = 0.95 (42.7 examples/sec; 0.187 sec/batch; 17h:14m:28s remains)
