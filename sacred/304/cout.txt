INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "304"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-fixqian-clip5
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
inputs Tensor("batch:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
net Tensor("siamese_fc/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
inputs Tensor("batch:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_1/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
net Tensor("siamese_fc_1/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - preproces -- None
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-16 12:34:53.765078: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 12:34:53.765114: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 12:34:53.765121: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 12:34:53.765125: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 12:34:53.765130: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 12:34:54.624570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-16 12:34:54.624609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-16 12:34:54.624616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-16 12:34:54.624628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
inputs Tensor("batch_1:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_2/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
net Tensor("siamese_fc_2/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("siamese_fc_2/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_2/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
inputs Tensor("batch_1:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_3/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
net Tensor("siamese_fc_3/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_3/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_3/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
Tensor("detection_1/add:0", shape=(8, 15, 15), dtype=float32)
[u'siamese_fc/conv1/weights:0',
 u'siamese_fc/conv1/BatchNorm/beta:0',
 u'siamese_fc/conv1/BatchNorm/gamma:0',
 u'siamese_fc/conv1/BatchNorm/moving_mean:0',
 u'siamese_fc/conv1/BatchNorm/moving_variance:0',
 u'siamese_fc/conv2/b1/weights:0',
 u'siamese_fc/conv2/b1/BatchNorm/beta:0',
 u'siamese_fc/conv2/b1/BatchNorm/gamma:0',
 u'siamese_fc/conv2/b1/BatchNorm/moving_mean:0',
 u'siamese_fc/conv2/b1/BatchNorm/moving_variance:0',
 u'siamese_fc/conv2/b2/weights:0',
 u'siamese_fc/conv2/b2/BatchNorm/beta:0',
 u'siamese_fc/conv2/b2/BatchNorm/gamma:0',
 u'siamese_fc/conv2/b2/BatchNorm/moving_mean:0',
 u'siamese_fc/conv2/b2/BatchNorm/moving_variance:0',
 u'siamese_fc/conv3/weights:0',
 u'siamese_fc/conv3/BatchNorm/beta:0',
 u'siamese_fc/conv3/BatchNorm/gamma:0',
 u'siamese_fc/conv3/BatchNorm/moving_mean:0',
 u'siamese_fc/conv3/BatchNorm/moving_variance:0',
 u'siamese_fc/conv4/b1/weights:0',
 u'siamese_fc/conv4/b1/BatchNorm/beta:0',
 u'siamese_fc/conv4/b1/BatchNorm/gamma:0',
 u'siamese_fc/conv4/b1/BatchNorm/moving_mean:0',
 u'siamese_fc/conv4/b1/BatchNorm/moving_variance:0',
 u'siamese_fc/conv4/b2/weights:0',
 u'siamese_fc/conv4/b2/BatchNorm/beta:0',
 u'siamese_fc/conv4/b2/BatchNorm/gamma:0',
 u'siamese_fc/conv4/b2/BatchNorm/moving_mean:0',
 u'siamese_fc/conv4/b2/BatchNorm/moving_variance:0',
 u'siamese_fc/conv5/def/offset1/weights:0',
 u'siamese_fc/conv5/def/b1/weights:0',
 u'detection/biases:0',
 u'global_step:0',
 u'OptimizeLoss/siamese_fc/conv1/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv1/BatchNorm/beta/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv1/BatchNorm/gamma/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv2/b1/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv2/b1/BatchNorm/beta/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv2/b1/BatchNorm/gamma/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv2/b2/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv2/b2/BatchNorm/beta/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv2/b2/BatchNorm/gamma/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv3/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv3/BatchNorm/beta/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv3/BatchNorm/gamma/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv4/b1/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv4/b1/BatchNorm/beta/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv4/b1/BatchNorm/gamma/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv4/b2/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv4/b2/BatchNorm/beta/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv4/b2/BatchNorm/gamma/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv5/def/offset1/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv5/def/b1/weights/Momentum:0',
 u'OptimizeLoss/detection/biases/Momentum:0']
INFO - root - Starting threads 0 ...

INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-16 12:35:00.570988: step 0, loss = 2.28, batch loss = 2.23 (2.2 examples/sec; 3.640 sec/batch; 336h:11m:48s remains)
2017-12-16 12:35:01.262111: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289265 -4.4288926 -4.4288549 -4.4288211 -4.4287915 -4.4287705 -4.42876 -4.4287677 -4.4287934 -4.4288082 -4.4288154 -4.4288259 -4.4288373 -4.4288473 -4.4288521][-4.4289165 -4.4288712 -4.4288135 -4.4287562 -4.42871 -4.4286819 -4.4286704 -4.4286833 -4.4287224 -4.4287457 -4.4287553 -4.4287734 -4.4287977 -4.4288187 -4.4288263][-4.4289141 -4.4288564 -4.4287786 -4.4287 -4.4286385 -4.4286032 -4.4285913 -4.4286036 -4.4286494 -4.4286776 -4.4286942 -4.4287295 -4.4287744 -4.4288049 -4.4288149][-4.4289155 -4.4288473 -4.4287539 -4.428658 -4.4285812 -4.4285331 -4.4285121 -4.428524 -4.4285789 -4.4286246 -4.42866 -4.4287095 -4.4287667 -4.4288049 -4.4288173][-4.4289203 -4.4288497 -4.4287491 -4.4286427 -4.428545 -4.4284725 -4.4284272 -4.4284353 -4.428515 -4.4285936 -4.4286485 -4.4287019 -4.428762 -4.4288068 -4.4288211][-4.428926 -4.4288597 -4.4287581 -4.42864 -4.4285173 -4.4284067 -4.4283214 -4.4283175 -4.428431 -4.4285574 -4.4286461 -4.4287028 -4.4287558 -4.4288006 -4.4288177][-4.4289317 -4.4288721 -4.4287715 -4.4286442 -4.4284964 -4.4283361 -4.4281864 -4.4281464 -4.428298 -4.4284897 -4.4286203 -4.428688 -4.4287329 -4.4287724 -4.428792][-4.4289351 -4.4288821 -4.4287896 -4.4286623 -4.4285 -4.4283075 -4.4281111 -4.4280405 -4.4282246 -4.4284511 -4.4285946 -4.4286661 -4.4287081 -4.428741 -4.4287639][-4.4289341 -4.4288869 -4.4288082 -4.4286976 -4.4285517 -4.4283853 -4.4282346 -4.4281969 -4.4283314 -4.4285035 -4.4286141 -4.4286733 -4.42871 -4.4287381 -4.428762][-4.4289312 -4.4288888 -4.4288206 -4.4287286 -4.4286141 -4.4284964 -4.4284039 -4.4283838 -4.4284568 -4.4285607 -4.4286304 -4.4286728 -4.4287081 -4.4287386 -4.4287634][-4.4289336 -4.4288936 -4.4288311 -4.4287519 -4.4286656 -4.4285851 -4.4285269 -4.4285107 -4.4285398 -4.4285879 -4.4286227 -4.4286575 -4.4287009 -4.4287386 -4.428761][-4.4289403 -4.4289045 -4.4288497 -4.4287872 -4.4287276 -4.4286714 -4.4286304 -4.4286127 -4.4286127 -4.42862 -4.4286232 -4.4286489 -4.4287019 -4.4287481 -4.4287734][-4.4289474 -4.4289165 -4.42887 -4.428822 -4.428781 -4.4287438 -4.4287138 -4.4286942 -4.42868 -4.4286675 -4.4286585 -4.4286819 -4.4287343 -4.4287815 -4.4288087][-4.4289536 -4.4289265 -4.4288878 -4.4288478 -4.4288168 -4.4287925 -4.4287715 -4.4287553 -4.4287448 -4.4287376 -4.4287329 -4.4287515 -4.42879 -4.4288282 -4.4288521][-4.4289622 -4.4289403 -4.42891 -4.4288783 -4.4288559 -4.4288416 -4.42883 -4.4288197 -4.4288144 -4.4288125 -4.4288125 -4.4288235 -4.4288459 -4.4288707 -4.4288859]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-fixqian-clip5/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-fixqian-clip5/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 12:35:05.101912: step 10, loss = 2.11, batch loss = 2.05 (29.0 examples/sec; 0.276 sec/batch; 25h:26m:45s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-fixqian-clip5
INFO - root - 2017-12-16 12:35:07.886444: step 20, loss = 0.51, batch loss = 0.45 (29.1 examples/sec; 0.275 sec/batch; 25h:24m:04s remains)
INFO - root - 2017-12-16 12:35:10.651434: step 30, loss = 0.42, batch loss = 0.37 (28.7 examples/sec; 0.279 sec/batch; 25h:43m:56s remains)
INFO - root - 2017-12-16 12:35:13.460737: step 40, loss = 0.43, batch loss = 0.37 (27.2 examples/sec; 0.295 sec/batch; 27h:12m:36s remains)
INFO - root - 2017-12-16 12:35:16.236909: step 50, loss = 0.46, batch loss = 0.40 (28.6 examples/sec; 0.280 sec/batch; 25h:49m:20s remains)
INFO - root - 2017-12-16 12:35:18.972941: step 60, loss = 0.57, batch loss = 0.51 (29.7 examples/sec; 0.269 sec/batch; 24h:50m:28s remains)
INFO - root - 2017-12-16 12:35:21.722074: step 70, loss = 0.33, batch loss = 0.27 (28.4 examples/sec; 0.281 sec/batch; 25h:59m:01s remains)
INFO - root - 2017-12-16 12:35:24.521877: step 80, loss = 0.43, batch loss = 0.37 (27.7 examples/sec; 0.289 sec/batch; 26h:38m:26s remains)
INFO - root - 2017-12-16 12:35:27.293640: step 90, loss = 0.33, batch loss = 0.27 (30.1 examples/sec; 0.266 sec/batch; 24h:33m:54s remains)
INFO - root - 2017-12-16 12:35:30.026018: step 100, loss = 0.32, batch loss = 0.26 (29.4 examples/sec; 0.272 sec/batch; 25h:07m:14s remains)
2017-12-16 12:35:30.511053: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8967764 -2.8497438 -2.8174458 -2.8175216 -2.8253803 -2.8200076 -2.7718773 -2.665621 -2.5043631 -2.3615425 -2.4448466 -2.7254148 -3.1723258 -3.6333358 -4.0975323][-2.8665771 -2.7102857 -2.5993097 -2.5602729 -2.5475035 -2.5290518 -2.4912376 -2.4259019 -2.2961581 -2.1701703 -2.2853479 -2.6367273 -3.1415372 -3.6506052 -4.14363][-2.5680134 -2.3048866 -2.1083083 -2.0203302 -1.9785931 -1.9462235 -1.9205437 -1.8858542 -1.8113465 -1.7413521 -1.924587 -2.3531437 -2.9319472 -3.5090244 -4.0665159][-1.9832296 -1.6267562 -1.3376169 -1.1730919 -1.0747638 -1.0301545 -1.0405364 -1.0747693 -1.0846035 -1.1242723 -1.4126482 -1.9165456 -2.5852208 -3.2550006 -3.9114468][-1.2687998 -0.76885009 -0.3460784 -0.08692503 0.070958614 0.11240149 0.041004658 -0.10135651 -0.22665071 -0.37971497 -0.77663255 -1.3853412 -2.1792395 -2.9690754 -3.7262564][-0.75951052 -0.1156354 0.45908594 0.8189559 0.99936628 0.9601078 0.80502796 0.55860806 0.34809065 0.12407398 -0.3427496 -1.0202615 -1.9085081 -2.7895317 -3.6202569][-0.70942473 0.0077605247 0.64402342 1.0532899 1.2233458 1.1242256 0.904747 0.63197041 0.4347868 0.21857691 -0.25031805 -0.94441319 -1.8726571 -2.7941468 -3.65941][-1.1329968 -0.43896151 0.1832819 0.59306288 0.76055479 0.64847708 0.4532876 0.25886631 0.13346767 -0.048816681 -0.489357 -1.16063 -2.0825555 -3.0002584 -3.8378847][-1.8242328 -1.2441902 -0.70323992 -0.32175398 -0.12558794 -0.17832327 -0.27541208 -0.3797946 -0.42183208 -0.55995274 -0.97327757 -1.6079152 -2.4974174 -3.3570929 -4.1098256][-2.6421778 -2.224447 -1.8274689 -1.5347195 -1.3463757 -1.3221204 -1.3104105 -1.3075831 -1.2440054 -1.2720447 -1.5981033 -2.1591232 -2.9616261 -3.7097507 -4.3493662][-3.3374743 -3.1055012 -2.8751006 -2.6858182 -2.5392938 -2.4780326 -2.3903868 -2.3029795 -2.1922166 -2.1727722 -2.423327 -2.8715854 -3.5137231 -4.0927453 -4.5825019][-3.840095 -3.7348373 -3.6186123 -3.503274 -3.3950987 -3.3079925 -3.1827693 -3.059761 -2.94727 -2.9245968 -3.1056466 -3.4355249 -3.9103141 -4.3201323 -4.6506925][-4.1405563 -4.1150889 -4.0735869 -4.0238657 -3.9647844 -3.9007196 -3.7953174 -3.6821117 -3.5925968 -3.5643158 -3.6682079 -3.8621438 -4.152348 -4.400569 -4.5886045][-4.183445 -4.166296 -4.1572442 -4.1497769 -4.1409249 -4.13391 -4.0935435 -4.0482378 -4.004591 -3.9836783 -4.0361476 -4.1303048 -4.2762361 -4.3984151 -4.4879751][-4.2122397 -4.194901 -4.185638 -4.18533 -4.1891828 -4.1978769 -4.2004318 -4.1925778 -4.1822805 -4.1746516 -4.1961246 -4.23577 -4.2919345 -4.3371325 -4.3645639]]...]
INFO - root - 2017-12-16 12:35:33.277822: step 110, loss = 0.41, batch loss = 0.35 (29.7 examples/sec; 0.269 sec/batch; 24h:51m:03s remains)
INFO - root - 2017-12-16 12:35:36.060704: step 120, loss = 0.42, batch loss = 0.36 (29.3 examples/sec; 0.273 sec/batch; 25h:10m:39s remains)
INFO - root - 2017-12-16 12:35:38.831964: step 130, loss = 0.40, batch loss = 0.35 (29.7 examples/sec; 0.270 sec/batch; 24h:53m:44s remains)
INFO - root - 2017-12-16 12:35:41.614824: step 140, loss = 0.37, batch loss = 0.31 (29.8 examples/sec; 0.268 sec/batch; 24h:44m:48s remains)
INFO - root - 2017-12-16 12:35:44.423450: step 150, loss = 0.32, batch loss = 0.26 (27.0 examples/sec; 0.297 sec/batch; 27h:23m:10s remains)
INFO - root - 2017-12-16 12:35:47.214263: step 160, loss = 0.35, batch loss = 0.29 (29.0 examples/sec; 0.276 sec/batch; 25h:28m:55s remains)
INFO - root - 2017-12-16 12:35:49.979239: step 170, loss = 0.34, batch loss = 0.28 (28.2 examples/sec; 0.284 sec/batch; 26h:10m:30s remains)
INFO - root - 2017-12-16 12:35:52.821085: step 180, loss = 0.44, batch loss = 0.38 (27.9 examples/sec; 0.287 sec/batch; 26h:28m:21s remains)
INFO - root - 2017-12-16 12:35:55.648867: step 190, loss = 0.35, batch loss = 0.29 (27.7 examples/sec; 0.288 sec/batch; 26h:37m:16s remains)
INFO - root - 2017-12-16 12:35:58.480038: step 200, loss = 0.44, batch loss = 0.38 (28.2 examples/sec; 0.284 sec/batch; 26h:10m:54s remains)
2017-12-16 12:35:59.012220: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7385828 -2.0414207 -2.1633868 -2.1736159 -2.1078753 -2.0184772 -1.9188235 -1.777925 -1.626411 -1.5354695 -1.7652833 -2.2359309 -2.7839794 -3.3547623 -3.9517093][-2.3022766 -2.5700541 -2.6183395 -2.5622118 -2.4380522 -2.3204954 -2.2218435 -2.1066315 -1.97699 -1.877681 -2.1065853 -2.5706258 -3.137219 -3.7098196 -4.2927361][-2.6448379 -2.858521 -2.8184161 -2.6628909 -2.4386199 -2.2781103 -2.1468379 -2.0267689 -1.907949 -1.8380988 -2.1322079 -2.6375093 -3.248507 -3.8979404 -4.5416279][-2.6751761 -2.7758408 -2.6200795 -2.3084671 -1.9114108 -1.6279609 -1.4163551 -1.2751057 -1.2028081 -1.2630935 -1.7003982 -2.3489082 -3.1036139 -3.9019222 -4.6638856][-2.4887714 -2.4745903 -2.1599991 -1.6757591 -1.0949805 -0.60462022 -0.28950119 -0.14017773 -0.1587863 -0.37843418 -0.98908353 -1.8512151 -2.8193102 -3.7999556 -4.6981268][-2.1638334 -2.0703681 -1.6034548 -0.93689394 -0.20265627 0.45595694 0.88180256 1.0421534 0.89436436 0.49542093 -0.31183004 -1.3563769 -2.515213 -3.6711144 -4.6791205][-1.8311601 -1.6706944 -1.1068375 -0.34560823 0.48387384 1.2286439 1.7438631 1.9213839 1.613698 1.0654116 0.12307453 -1.0317695 -2.3367968 -3.6132107 -4.6659245][-1.732944 -1.5312011 -0.90176129 -0.11296606 0.77535295 1.5338807 2.0503078 2.1921778 1.8341475 1.1820412 0.16550207 -1.0259035 -2.3631816 -3.6602435 -4.7054033][-1.9775326 -1.8260548 -1.2545974 -0.52697635 0.28917646 1.0054922 1.51195 1.6409531 1.3335238 0.70576763 -0.22640419 -1.3609567 -2.6420603 -3.8544209 -4.8044696][-2.4699683 -2.3680971 -1.9059994 -1.2994816 -0.63148284 -0.01821804 0.43382597 0.58525467 0.36026525 -0.15090036 -0.92107987 -1.900851 -3.03643 -4.1030507 -4.9264364][-3.3567176 -3.3162663 -2.9631021 -2.468802 -1.9288876 -1.4178662 -1.0089307 -0.8265121 -1.0016975 -1.3718631 -1.9591618 -2.7356949 -3.6660109 -4.5545588 -5.2240157][-4.4400916 -4.4537787 -4.1746554 -3.7235992 -3.2487326 -2.8576703 -2.5288005 -2.3469687 -2.4730611 -2.7351456 -3.1597095 -3.7571318 -4.4417238 -5.0500479 -5.4963975][-5.3018856 -5.3462391 -5.1471815 -4.7806277 -4.3644485 -4.0565009 -3.8198032 -3.6910439 -3.7457998 -3.9194736 -4.1862974 -4.5659895 -5.0010648 -5.3741446 -5.6168537][-5.7309246 -5.7581954 -5.5830393 -5.3034444 -4.9866338 -4.7458763 -4.5980849 -4.5360074 -4.5502863 -4.6455445 -4.7908721 -4.9661551 -5.1745434 -5.3761315 -5.4858913][-5.6172023 -5.6276484 -5.4851503 -5.272666 -5.0421991 -4.869503 -4.7890916 -4.7820034 -4.8044004 -4.8436656 -4.915791 -4.993566 -5.0815077 -5.1741619 -5.2147245]]...]
INFO - root - 2017-12-16 12:36:01.839778: step 210, loss = 0.36, batch loss = 0.30 (27.5 examples/sec; 0.291 sec/batch; 26h:52m:12s remains)
INFO - root - 2017-12-16 12:36:04.678709: step 220, loss = 0.33, batch loss = 0.27 (27.6 examples/sec; 0.290 sec/batch; 26h:44m:19s remains)
INFO - root - 2017-12-16 12:36:07.526977: step 230, loss = 0.28, batch loss = 0.22 (27.6 examples/sec; 0.290 sec/batch; 26h:48m:03s remains)
INFO - root - 2017-12-16 12:36:10.372694: step 240, loss = 0.51, batch loss = 0.45 (27.9 examples/sec; 0.287 sec/batch; 26h:27m:27s remains)
INFO - root - 2017-12-16 12:36:13.215527: step 250, loss = 0.35, batch loss = 0.29 (28.8 examples/sec; 0.277 sec/batch; 25h:35m:57s remains)
INFO - root - 2017-12-16 12:36:16.050127: step 260, loss = 0.41, batch loss = 0.35 (28.3 examples/sec; 0.282 sec/batch; 26h:03m:37s remains)
INFO - root - 2017-12-16 12:36:18.916235: step 270, loss = 0.31, batch loss = 0.25 (27.9 examples/sec; 0.287 sec/batch; 26h:27m:31s remains)
INFO - root - 2017-12-16 12:36:21.705124: step 280, loss = 0.26, batch loss = 0.20 (29.2 examples/sec; 0.274 sec/batch; 25h:19m:18s remains)
INFO - root - 2017-12-16 12:36:24.617066: step 290, loss = 0.31, batch loss = 0.25 (27.5 examples/sec; 0.291 sec/batch; 26h:53m:15s remains)
INFO - root - 2017-12-16 12:36:27.451052: step 300, loss = 0.27, batch loss = 0.21 (28.6 examples/sec; 0.280 sec/batch; 25h:48m:25s remains)
2017-12-16 12:36:27.931980: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3904481 -2.6439519 -2.7218041 -2.5615597 -2.1305625 -1.4308763 -0.62266254 -0.13826275 -0.19277048 -0.58738685 -1.3392637 -2.3245032 -3.2900164 -3.9257641 -4.2189112][-2.0595758 -2.3502741 -2.4851713 -2.4105809 -2.0283215 -1.4061453 -0.64627695 -0.096646309 -0.033491611 -0.40827179 -1.1926455 -2.2552147 -3.26358 -3.9342079 -4.2148395][-1.8028879 -2.0582347 -2.2030115 -2.038806 -1.6336603 -1.0331473 -0.3073287 0.16141367 0.19515085 -0.1930027 -1.0147521 -2.1269763 -3.2476315 -4.004118 -4.3632789][-1.3819494 -1.6379943 -1.7671003 -1.6977212 -1.3350835 -0.754575 -0.1118741 0.3197732 0.2928133 -0.11407232 -0.91575742 -2.0469491 -3.1995168 -4.047843 -4.5075216][-0.93796682 -1.2452478 -1.3594475 -1.3596008 -1.0883923 -0.48861933 0.24155045 0.60827351 0.45476246 -0.0871048 -0.97808623 -2.1027651 -3.2325077 -4.0933847 -4.5976839][-0.5376339 -0.92682838 -1.1126232 -1.1304564 -0.82725549 -0.2120142 0.56002617 1.0071611 0.88682842 0.281703 -0.7664845 -2.0496433 -3.2443776 -4.1361933 -4.6871033][-0.47076726 -0.91045928 -1.1616359 -1.1210711 -0.695333 0.042203426 0.8544817 1.3606811 1.3403807 0.7538538 -0.35636044 -1.7956955 -3.2019713 -4.2050676 -4.7865338][-0.6156497 -1.0415385 -1.2984765 -1.2471278 -0.71999025 0.1653614 1.0230546 1.5358796 1.5269122 1.0243235 -0.13560486 -1.6869607 -3.2151608 -4.338367 -4.9734182][-0.80144763 -1.2137928 -1.390204 -1.3221612 -0.79331946 0.1661787 1.102149 1.6477489 1.5822029 1.0249362 -0.10638571 -1.6693475 -3.2709208 -4.5279293 -5.2186203][-1.0277348 -1.4113073 -1.633678 -1.5865383 -1.1412778 -0.19346905 0.8743124 1.5679092 1.5913415 0.9742198 -0.21145821 -1.7683933 -3.3914545 -4.7283592 -5.4753275][-1.710875 -2.217304 -2.5352261 -2.4949217 -2.0385962 -1.1451604 -0.1221714 0.658905 0.864408 0.32782364 -0.81109309 -2.3477466 -3.8501537 -5.0622268 -5.7405281][-2.3957086 -2.9682467 -3.3337021 -3.3273451 -2.8954124 -2.1000443 -1.1795194 -0.44965792 -0.18321848 -0.59783077 -1.677953 -3.1722527 -4.5993052 -5.5517106 -6.0266857][-3.0671921 -3.5917678 -3.9358375 -3.9246306 -3.4769006 -2.7550669 -2.0056388 -1.4234686 -1.2353914 -1.5349898 -2.4587255 -3.8310714 -5.1645737 -5.983048 -6.267972][-3.5723479 -4.0832248 -4.4206805 -4.4495778 -4.0823159 -3.3927307 -2.6928308 -2.2544603 -2.2167013 -2.544086 -3.3310809 -4.4694986 -5.5940933 -6.3054118 -6.4205704][-4.1545315 -4.6567645 -4.9494462 -4.9188657 -4.5805845 -3.9876196 -3.373323 -2.9977345 -3.0197711 -3.4093986 -4.1345124 -5.0310974 -5.8970971 -6.4027638 -6.4183216]]...]
INFO - root - 2017-12-16 12:36:30.762039: step 310, loss = 0.28, batch loss = 0.22 (28.1 examples/sec; 0.284 sec/batch; 26h:14m:16s remains)
INFO - root - 2017-12-16 12:36:33.604122: step 320, loss = 0.30, batch loss = 0.24 (28.9 examples/sec; 0.277 sec/batch; 25h:34m:12s remains)
INFO - root - 2017-12-16 12:36:36.448788: step 330, loss = 0.42, batch loss = 0.36 (28.2 examples/sec; 0.283 sec/batch; 26h:08m:57s remains)
INFO - root - 2017-12-16 12:36:39.278866: step 340, loss = 0.36, batch loss = 0.30 (27.1 examples/sec; 0.295 sec/batch; 27h:15m:42s remains)
INFO - root - 2017-12-16 12:36:42.132896: step 350, loss = 0.25, batch loss = 0.20 (27.8 examples/sec; 0.288 sec/batch; 26h:35m:44s remains)
INFO - root - 2017-12-16 12:36:44.967890: step 360, loss = 0.41, batch loss = 0.36 (27.9 examples/sec; 0.287 sec/batch; 26h:28m:20s remains)
INFO - root - 2017-12-16 12:36:47.816074: step 370, loss = 0.27, batch loss = 0.21 (27.0 examples/sec; 0.296 sec/batch; 27h:18m:02s remains)
INFO - root - 2017-12-16 12:36:50.672150: step 380, loss = 0.34, batch loss = 0.28 (29.6 examples/sec; 0.270 sec/batch; 24h:55m:19s remains)
INFO - root - 2017-12-16 12:36:53.520557: step 390, loss = 0.34, batch loss = 0.28 (28.0 examples/sec; 0.286 sec/batch; 26h:24m:06s remains)
INFO - root - 2017-12-16 12:36:56.358920: step 400, loss = 0.33, batch loss = 0.27 (28.4 examples/sec; 0.281 sec/batch; 25h:56m:36s remains)
2017-12-16 12:36:56.819297: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0169997 -2.3169675 -2.4675159 -2.4879079 -2.4281003 -2.2820623 -2.0426633 -1.7585936 -1.5255446 -1.4019568 -1.8030486 -2.7332416 -3.8656969 -5.0400987 -5.8737688][-2.7181916 -3.0548825 -3.1833787 -3.124496 -2.9281054 -2.7399395 -2.4989252 -2.2025242 -2.0226529 -2.028007 -2.5339718 -3.4154525 -4.4316 -5.4661694 -6.2128954][-3.2369876 -3.6331363 -3.7319484 -3.6348872 -3.3820677 -3.0088029 -2.5529978 -2.244312 -2.1578374 -2.2628098 -2.8531833 -3.8319864 -4.8811522 -5.8252935 -6.4371333][-3.3844905 -3.7554429 -3.748512 -3.48327 -2.9991763 -2.4332571 -1.903594 -1.5286398 -1.4363692 -1.6848345 -2.4589233 -3.4957352 -4.5932493 -5.6390958 -6.3159103][-3.0864849 -3.4911003 -3.4500251 -2.9235864 -2.035068 -1.0633087 -0.20685816 0.2759161 0.29660034 -0.085283756 -1.0058968 -2.3743284 -3.789922 -4.9482265 -5.6957741][-2.6551621 -2.9844828 -2.6703677 -1.7605164 -0.46255827 0.882627 1.9183078 2.5295959 2.4698672 1.711885 0.3586297 -1.1936214 -2.6493669 -3.9990082 -4.93069][-2.0867522 -2.288187 -1.7484837 -0.45909834 1.1332278 2.5605197 3.578373 3.7827597 3.2752533 2.387733 1.1336789 -0.50477576 -2.1648581 -3.4385884 -4.1968937][-1.8091519 -1.9551713 -1.471175 -0.2059288 1.3764381 2.73492 3.5874014 3.6044588 3.0722947 2.0926704 0.76891279 -0.73357344 -2.2608397 -3.4352818 -4.1174316][-2.0707185 -2.0919821 -1.3676934 -0.27097464 0.86073589 1.8012481 2.3276806 2.2031493 1.7729321 0.94556904 -0.24983406 -1.6405361 -3.0802588 -4.0754685 -4.4525709][-2.2253563 -2.3370123 -1.9165716 -0.96152663 0.045697212 0.53747272 0.63613367 0.35206842 -0.0161767 -0.56761122 -1.4954398 -2.6259387 -3.8381772 -4.7258353 -5.0858841][-3.358417 -3.2808442 -2.806716 -2.1708102 -1.7050452 -1.430156 -1.1843057 -1.3733356 -1.7891419 -2.2398603 -2.9498711 -3.8571463 -4.8036642 -5.5377855 -5.7653208][-4.4686446 -4.2954283 -3.8872602 -3.4180231 -3.0346646 -2.9488614 -3.0302892 -3.179261 -3.3329587 -3.5789385 -4.0740328 -4.9106159 -5.7557192 -6.226882 -6.2803326][-5.5167027 -5.4185295 -5.2496939 -4.9125586 -4.6328149 -4.4876957 -4.4143076 -4.5131607 -4.7039423 -4.8843107 -5.2324262 -5.7535348 -6.1934972 -6.4438114 -6.3449736][-5.8057613 -5.74043 -5.54154 -5.3337502 -5.1958928 -5.1205564 -5.0771184 -5.0391617 -5.0505819 -5.2056313 -5.479032 -5.929626 -6.3074923 -6.3740225 -6.1959748][-5.8804054 -5.8771954 -5.7933512 -5.636508 -5.4897442 -5.3837595 -5.3496146 -5.4032412 -5.4434094 -5.5269027 -5.6783876 -5.7963629 -5.8193502 -5.9187932 -5.9118371]]...]
INFO - root - 2017-12-16 12:36:59.709939: step 410, loss = 0.27, batch loss = 0.21 (28.8 examples/sec; 0.278 sec/batch; 25h:37m:08s remains)
INFO - root - 2017-12-16 12:37:02.553860: step 420, loss = 0.29, batch loss = 0.23 (29.1 examples/sec; 0.275 sec/batch; 25h:19m:36s remains)
INFO - root - 2017-12-16 12:37:05.419272: step 430, loss = 0.34, batch loss = 0.28 (27.0 examples/sec; 0.296 sec/batch; 27h:20m:05s remains)
INFO - root - 2017-12-16 12:37:08.337024: step 440, loss = 0.26, batch loss = 0.20 (27.5 examples/sec; 0.291 sec/batch; 26h:51m:25s remains)
INFO - root - 2017-12-16 12:37:11.224269: step 450, loss = 0.29, batch loss = 0.23 (26.6 examples/sec; 0.301 sec/batch; 27h:44m:31s remains)
INFO - root - 2017-12-16 12:37:14.044050: step 460, loss = 0.29, batch loss = 0.23 (28.0 examples/sec; 0.286 sec/batch; 26h:20m:50s remains)
INFO - root - 2017-12-16 12:37:16.923387: step 470, loss = 0.31, batch loss = 0.25 (27.4 examples/sec; 0.292 sec/batch; 26h:57m:03s remains)
INFO - root - 2017-12-16 12:37:19.782179: step 480, loss = 0.28, batch loss = 0.22 (28.0 examples/sec; 0.285 sec/batch; 26h:18m:18s remains)
INFO - root - 2017-12-16 12:37:22.584371: step 490, loss = 0.32, batch loss = 0.26 (29.2 examples/sec; 0.274 sec/batch; 25h:14m:54s remains)
INFO - root - 2017-12-16 12:37:25.407833: step 500, loss = 0.33, batch loss = 0.27 (29.0 examples/sec; 0.276 sec/batch; 25h:26m:36s remains)
2017-12-16 12:37:25.896624: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.3307519 -0.47377658 -0.50121355 -0.35879183 -0.25911236 -0.204391 -0.20395422 -0.18052387 -0.2523489 -0.42136049 -0.97417307 -1.8380687 -2.6704857 -3.4875512 -4.1249118][-0.43449473 -0.5247438 -0.54449439 -0.38124609 -0.24346495 -0.28264952 -0.3430295 -0.41204739 -0.66192913 -0.9469192 -1.5728509 -2.5223632 -3.3865466 -4.145319 -4.6266413][-0.47171259 -0.49734354 -0.40743542 -0.28769732 -0.28456354 -0.30935287 -0.39679384 -0.605654 -0.94719982 -1.3231046 -1.9573376 -2.9413812 -3.8468647 -4.4587326 -4.7590184][-0.47075939 -0.35512257 -0.12539244 0.086905 0.13321352 0.011932373 -0.20083332 -0.47213626 -0.77908564 -1.1698811 -1.9729061 -2.9224782 -3.7344136 -4.3494773 -4.6575942][-0.27528238 -0.11044502 0.1589179 0.39281464 0.4548192 0.38939333 0.23165989 0.00086450577 -0.34528494 -0.70461392 -1.4091673 -2.3827937 -3.2154059 -3.850564 -4.1001649][-0.04970026 0.20811415 0.52688265 0.81543446 0.99744511 0.98087692 0.95412874 0.82804441 0.50468206 0.1512537 -0.53246307 -1.4370604 -2.2006941 -2.9519172 -3.3688593][0.12500763 0.46193171 0.76788473 1.1405907 1.3861575 1.4965706 1.5953369 1.5987353 1.3356647 1.0355034 0.36759567 -0.47462487 -1.26913 -2.0346906 -2.5573313][0.13202429 0.34870958 0.62815046 1.0686855 1.4350204 1.7006249 1.9588923 2.0709929 1.9559841 1.7525029 1.1831512 0.39316845 -0.34827232 -1.1131091 -1.6229167][0.012686253 0.19952536 0.49885225 0.96489668 1.2803574 1.5488768 1.7931552 1.9120779 1.8140321 1.761538 1.415493 0.84289837 0.26521158 -0.31416512 -0.75177646][-0.32600069 -0.17876863 0.082747936 0.42109156 0.75520658 0.90288067 0.94548512 1.0809975 1.0908523 1.1742177 1.035728 0.67734623 0.35995007 -0.062434673 -0.500653][-1.0655458 -0.966367 -0.76467705 -0.51102996 -0.42147732 -0.33345985 -0.37074471 -0.3819952 -0.36249208 -0.14047289 -0.033410549 -0.12431908 -0.19864082 -0.43309093 -0.84821463][-1.9188724 -1.8536491 -1.697161 -1.4964166 -1.5747254 -1.8169465 -1.9964817 -2.169874 -2.2286918 -1.8901358 -1.6828415 -1.5758529 -1.4721305 -1.58532 -1.8753316][-2.722815 -2.6025038 -2.5393486 -2.6065922 -2.8486402 -3.1929402 -3.5880704 -3.8871696 -4.028626 -3.8308647 -3.5261002 -3.224777 -3.0123122 -3.025301 -3.0978293][-3.2911973 -3.1750369 -3.0866976 -3.1862097 -3.6104009 -4.0597181 -4.6824059 -5.2471085 -5.5686922 -5.5782003 -5.3222141 -4.8973432 -4.4471207 -4.2980218 -4.2524128][-3.774991 -3.6795535 -3.6013293 -3.7882676 -4.2106013 -4.8238335 -5.6223612 -6.292417 -6.7164478 -6.9407973 -6.8075161 -6.3966651 -5.8284016 -5.3896422 -5.1551695]]...]
INFO - root - 2017-12-16 12:37:28.720298: step 510, loss = 0.26, batch loss = 0.20 (28.8 examples/sec; 0.278 sec/batch; 25h:38m:26s remains)
INFO - root - 2017-12-16 12:37:31.579381: step 520, loss = 0.30, batch loss = 0.25 (28.3 examples/sec; 0.283 sec/batch; 26h:06m:24s remains)
INFO - root - 2017-12-16 12:37:34.437092: step 530, loss = 0.28, batch loss = 0.22 (28.7 examples/sec; 0.278 sec/batch; 25h:40m:09s remains)
INFO - root - 2017-12-16 12:37:37.292535: step 540, loss = 0.29, batch loss = 0.23 (29.1 examples/sec; 0.275 sec/batch; 25h:20m:32s remains)
INFO - root - 2017-12-16 12:37:40.165624: step 550, loss = 0.29, batch loss = 0.23 (27.7 examples/sec; 0.289 sec/batch; 26h:37m:49s remains)
INFO - root - 2017-12-16 12:37:43.000304: step 560, loss = 0.32, batch loss = 0.26 (28.5 examples/sec; 0.281 sec/batch; 25h:55m:16s remains)
2017-12-16 12:37:44.160072: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 152152 get requests, put_count=152150 evicted_count=1000 eviction_rate=0.00657246 and unsatisfied allocation rate=0.00724276
2017-12-16 12:37:44.160184: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
INFO - root - 2017-12-16 12:37:45.862947: step 570, loss = 0.26, batch loss = 0.20 (28.9 examples/sec; 0.277 sec/batch; 25h:29m:50s remains)
INFO - root - 2017-12-16 12:37:48.757776: step 580, loss = 0.32, batch loss = 0.26 (28.5 examples/sec; 0.280 sec/batch; 25h:50m:08s remains)
INFO - root - 2017-12-16 12:37:51.624175: step 590, loss = 0.26, batch loss = 0.20 (27.9 examples/sec; 0.287 sec/batch; 26h:27m:08s remains)
INFO - root - 2017-12-16 12:37:54.458721: step 600, loss = 0.24, batch loss = 0.18 (27.7 examples/sec; 0.289 sec/batch; 26h:39m:53s remains)
2017-12-16 12:37:54.963070: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7907999 -2.9610295 -3.3165212 -3.6706305 -4.0406337 -4.081502 -3.9110975 -3.6789312 -3.462043 -3.25818 -3.2734814 -3.7353783 -4.1849914 -4.6141787 -5.0167][-3.8769507 -4.0860257 -4.5569673 -4.9436817 -5.2892609 -5.395669 -5.1622014 -4.861083 -4.5603719 -4.1154833 -4.0295744 -4.368166 -4.8629708 -5.2875228 -5.6352348][-4.638762 -5.1578727 -5.6163139 -5.9712257 -6.1325865 -6.0543652 -5.6768622 -5.3559403 -4.9928861 -4.5799804 -4.5476775 -4.8449492 -5.3918858 -5.803616 -6.2234936][-4.6200657 -5.3396058 -5.9836078 -6.2778521 -6.3366809 -6.0914769 -5.6017141 -5.0054579 -4.4160957 -3.9607377 -3.9725728 -4.448586 -5.2091093 -5.831131 -6.473671][-4.3111567 -5.1193681 -5.82236 -5.965517 -5.9507012 -5.5137782 -4.6987896 -3.7338572 -2.7172775 -2.3170638 -2.4721878 -3.3288374 -4.2628436 -5.2284937 -6.3480163][-3.3855314 -4.1524816 -5.0222592 -5.2851877 -5.1097975 -4.3364277 -3.158092 -2.0915504 -1.0140388 -0.42799902 -0.66905069 -1.8089755 -3.1246295 -4.6418161 -6.1399493][-2.1985278 -3.1297264 -3.8976555 -4.2224727 -4.016788 -3.3037658 -1.9313641 -0.72459984 0.25517702 0.88684559 0.5964694 -0.44130588 -1.992835 -3.7281513 -5.3545728][-1.0794909 -2.077822 -2.873076 -3.0777464 -2.8223724 -2.2126317 -1.2045128 0.1907568 1.4865732 2.0877762 1.7064762 0.54577112 -1.143909 -3.0519164 -4.8587346][-0.037824154 -0.99767756 -1.8814087 -2.1625552 -2.0671542 -1.5204508 -0.64192462 0.63212776 1.8866014 2.4312658 2.20919 0.99584579 -0.77242494 -2.7332926 -4.4947481][0.28544092 -0.3435483 -1.0672379 -1.3567522 -1.3580894 -1.1344261 -0.5093298 0.52845 1.6453652 2.0909982 1.7888565 0.61467552 -0.77716041 -2.5778723 -4.4663391][-0.10408592 -0.64100623 -1.2999723 -1.6589618 -1.7390535 -1.668586 -1.2865088 -0.6999445 -0.048129082 0.47415638 0.39448214 -0.49100375 -1.6500177 -3.0806413 -4.4568877][-1.0391257 -1.4905388 -1.9248984 -2.2634628 -2.5612788 -2.5791259 -2.4957833 -2.3671243 -1.982985 -1.5140905 -1.6017926 -2.1320426 -2.7540765 -3.6585588 -4.5417519][-2.2513375 -2.7600541 -3.155288 -3.4018972 -3.6714792 -3.7730043 -3.8166556 -3.7643604 -3.6268709 -3.3831973 -3.283824 -3.5154209 -3.8625002 -4.4143596 -4.9416909][-3.6519141 -4.0220408 -4.31807 -4.5797586 -4.8449345 -4.9852252 -5.17223 -5.1119289 -4.9455214 -4.7258997 -4.5653772 -4.5668311 -4.6530643 -4.8626194 -5.1038923][-5.0391741 -5.2013044 -5.4439673 -5.7087007 -5.9641523 -6.1118422 -6.2192683 -6.2071934 -6.0387154 -5.7421017 -5.4495664 -5.3338451 -5.1632886 -5.076086 -5.1195712]]...]
INFO - root - 2017-12-16 12:37:57.791192: step 610, loss = 0.25, batch loss = 0.19 (27.5 examples/sec; 0.291 sec/batch; 26h:51m:52s remains)
INFO - root - 2017-12-16 12:38:00.646234: step 620, loss = 0.26, batch loss = 0.21 (28.1 examples/sec; 0.284 sec/batch; 26h:12m:52s remains)
INFO - root - 2017-12-16 12:38:03.540833: step 630, loss = 0.32, batch loss = 0.26 (27.6 examples/sec; 0.290 sec/batch; 26h:42m:40s remains)
INFO - root - 2017-12-16 12:38:06.430794: step 640, loss = 0.31, batch loss = 0.25 (28.8 examples/sec; 0.278 sec/batch; 25h:37m:30s remains)
INFO - root - 2017-12-16 12:38:09.246010: step 650, loss = 0.38, batch loss = 0.32 (28.8 examples/sec; 0.278 sec/batch; 25h:37m:04s remains)
INFO - root - 2017-12-16 12:38:12.100708: step 660, loss = 0.22, batch loss = 0.16 (28.3 examples/sec; 0.283 sec/batch; 26h:04m:09s remains)
INFO - root - 2017-12-16 12:38:14.994166: step 670, loss = 0.28, batch loss = 0.22 (27.8 examples/sec; 0.288 sec/batch; 26h:32m:55s remains)
INFO - root - 2017-12-16 12:38:17.853672: step 680, loss = 0.31, batch loss = 0.25 (27.9 examples/sec; 0.287 sec/batch; 26h:24m:43s remains)
INFO - root - 2017-12-16 12:38:20.712106: step 690, loss = 0.26, batch loss = 0.21 (28.9 examples/sec; 0.277 sec/batch; 25h:33m:16s remains)
INFO - root - 2017-12-16 12:38:23.564310: step 700, loss = 0.24, batch loss = 0.18 (27.3 examples/sec; 0.293 sec/batch; 27h:00m:38s remains)
2017-12-16 12:38:24.077187: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5725546 -3.5397322 -3.383884 -3.1636171 -2.9857807 -2.8463738 -2.7244024 -2.5896516 -2.5985622 -2.8687382 -3.2289889 -3.8258138 -4.3184209 -4.6785216 -4.7914453][-4.5171394 -4.4090524 -4.2257032 -3.9792209 -3.74146 -3.5056403 -3.2263117 -3.0127196 -2.9042091 -2.9599776 -3.3338766 -3.8928821 -4.4703574 -4.7650814 -4.784112][-4.510963 -4.3973236 -4.336175 -4.2155457 -4.0671177 -3.8554485 -3.521898 -3.1421127 -2.9422836 -2.9844289 -3.3534069 -3.9540825 -4.6021962 -4.9366441 -4.9253464][-4.1714568 -4.1077385 -4.1550837 -4.1711168 -4.1057816 -3.8471122 -3.5129561 -3.1129055 -2.9715624 -2.9653428 -3.2544651 -3.9496875 -4.6402187 -5.1261215 -5.1874676][-3.3508382 -3.4179425 -3.6246545 -3.8138952 -3.8042545 -3.5421329 -3.1413684 -2.7087688 -2.5056996 -2.5376368 -2.9173083 -3.7652991 -4.5993614 -5.2123642 -5.3722649][-2.2135015 -2.2485769 -2.496695 -2.7922564 -2.9235983 -2.7765822 -2.3616307 -1.9735456 -1.7881675 -1.8762689 -2.3902609 -3.4195118 -4.4920177 -5.2043495 -5.528842][-1.0288424 -1.0380306 -1.3296959 -1.6829565 -1.8289816 -1.6851008 -1.2308185 -0.88809943 -0.87547493 -1.0777431 -1.7658339 -2.9688745 -4.19808 -4.9993672 -5.4300232][0.51817894 0.63008213 0.19561815 -0.34350252 -0.71101117 -0.74273419 -0.4363296 -0.15856457 -0.17546511 -0.35999632 -1.187676 -2.5600319 -3.8741598 -4.8537397 -5.3869166][0.60870361 0.48240471 0.28208303 -0.2769351 -0.693028 -0.6545167 -0.3137207 0.0025496483 0.12656355 -0.00924778 -0.79618073 -2.1756439 -3.5668461 -4.6392875 -5.2281685][-0.39939165 -0.78071856 -1.0702107 -1.5011826 -1.6621222 -1.2984579 -0.85526443 -0.40994787 -0.11608553 -0.16768551 -0.79508948 -2.0602305 -3.4445012 -4.5217528 -5.1775322][-1.8116686 -2.1320839 -2.2828689 -2.4157476 -2.4286797 -2.0786619 -1.5708005 -1.1231956 -0.92225027 -0.96494794 -1.4570374 -2.5641465 -3.7169344 -4.6374211 -5.3054342][-3.1028161 -3.4789956 -3.7384238 -3.6216753 -3.4334216 -3.10646 -2.6418796 -2.2640224 -2.0319221 -2.0749528 -2.4816573 -3.3556018 -4.2962723 -5.092536 -5.7120738][-5.3826089 -5.6705813 -5.8111224 -5.7246571 -5.5498815 -5.1138411 -4.5772786 -4.1218581 -3.8335547 -3.8674448 -4.2168522 -4.7951508 -5.353488 -5.6671572 -5.8491845][-5.9805026 -6.4024954 -6.5543451 -6.5900126 -6.4259486 -6.0973244 -5.6747646 -5.311954 -5.0877004 -5.0461659 -5.2308025 -5.4264026 -5.6847363 -5.8617535 -5.8798246][-6.4180384 -6.9549084 -7.2787857 -7.2622509 -7.0011511 -6.6511564 -6.1453733 -5.7767773 -5.6396294 -5.7003355 -5.8951545 -5.9631796 -5.9918489 -6.0948896 -6.0613747]]...]
INFO - root - 2017-12-16 12:38:26.965355: step 710, loss = 0.27, batch loss = 0.22 (28.8 examples/sec; 0.277 sec/batch; 25h:34m:13s remains)
INFO - root - 2017-12-16 12:38:29.843761: step 720, loss = 0.32, batch loss = 0.27 (29.5 examples/sec; 0.271 sec/batch; 24h:59m:46s remains)
INFO - root - 2017-12-16 12:38:32.669639: step 730, loss = 0.33, batch loss = 0.28 (28.2 examples/sec; 0.283 sec/batch; 26h:06m:30s remains)
INFO - root - 2017-12-16 12:38:35.485802: step 740, loss = 0.34, batch loss = 0.28 (28.7 examples/sec; 0.279 sec/batch; 25h:41m:22s remains)
INFO - root - 2017-12-16 12:38:38.357573: step 750, loss = 0.28, batch loss = 0.22 (26.8 examples/sec; 0.299 sec/batch; 27h:31m:01s remains)
INFO - root - 2017-12-16 12:38:41.235107: step 760, loss = 0.27, batch loss = 0.21 (29.0 examples/sec; 0.276 sec/batch; 25h:26m:22s remains)
INFO - root - 2017-12-16 12:38:44.088330: step 770, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.281 sec/batch; 25h:54m:21s remains)
INFO - root - 2017-12-16 12:38:46.926799: step 780, loss = 0.25, batch loss = 0.19 (29.4 examples/sec; 0.273 sec/batch; 25h:06m:52s remains)
INFO - root - 2017-12-16 12:38:49.823925: step 790, loss = 0.32, batch loss = 0.26 (25.8 examples/sec; 0.310 sec/batch; 28h:32m:55s remains)
INFO - root - 2017-12-16 12:38:52.723852: step 800, loss = 0.24, batch loss = 0.19 (28.8 examples/sec; 0.277 sec/batch; 25h:33m:43s remains)
2017-12-16 12:38:53.263001: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.250313 -1.9892659 -2.0798855 -2.2037556 -2.3320906 -2.4481103 -2.3367836 -2.1628928 -2.0791438 -2.0126467 -2.4643469 -3.3641706 -3.9714313 -4.508019 -4.7557559][-1.6801856 -1.7508864 -1.7410276 -1.8143055 -1.9869862 -2.0533636 -1.9966249 -1.9506288 -1.9926925 -2.0822191 -2.5701032 -3.3663378 -3.9931464 -4.3586931 -4.46847][-0.94205737 -1.4017692 -1.6013312 -1.6236403 -1.5380423 -1.5291491 -1.3926044 -1.3997576 -1.5898707 -1.770035 -2.435627 -3.2166231 -3.8501952 -4.170918 -4.2731261][-0.63952518 -0.98837638 -1.2250829 -1.2572672 -1.2225268 -1.0275655 -1.0678675 -1.1398709 -1.2641199 -1.6958332 -2.3265989 -3.0824616 -3.7329409 -4.0078731 -4.0737224][-0.5597837 -0.80812669 -1.0238907 -1.00243 -0.76682329 -0.83046269 -0.94910383 -1.0049376 -1.1302729 -1.4912615 -2.2045438 -3.0197606 -3.7323976 -4.0193238 -3.9876051][-0.54512525 -0.59599829 -0.77271342 -0.79922915 -0.62217236 -0.59786916 -0.73098755 -0.853261 -1.0057838 -1.2840705 -1.8506472 -2.6685221 -3.2272737 -3.5256803 -3.6816447][-0.55894852 -0.72199774 -0.90611124 -0.83845973 -0.62753057 -0.67427754 -0.71146226 -0.86491275 -1.1050544 -1.2574856 -1.7786312 -2.5688715 -3.0617802 -3.3305624 -3.4472003][-0.57497334 -0.70752764 -0.9526372 -0.90652561 -0.69171834 -0.57994771 -0.49209714 -0.63213253 -0.97872877 -1.0881519 -1.4204109 -2.1782451 -2.7284393 -2.9934349 -3.1872921][-0.74106145 -0.88129091 -0.89383459 -0.66459107 -0.59444928 -0.49634361 -0.31060839 -0.30983829 -0.52815819 -0.54366422 -0.86476064 -1.5133421 -2.1400018 -2.6926284 -3.028038][-0.76981473 -1.0946116 -1.2010579 -1.0153134 -0.67052031 -0.34857607 -0.16747189 -0.14258337 -0.25068283 -0.20643759 -0.52282 -1.1293569 -1.8081417 -2.5052762 -3.0441837][-1.8839338 -2.0762141 -2.1428504 -2.1430967 -1.9600275 -1.5570221 -1.2305989 -1.1232023 -1.0733268 -1.0140481 -1.3588498 -1.9655106 -2.592319 -3.2350211 -3.6040165][-2.9627175 -3.2095075 -3.2469378 -3.0746737 -2.7778282 -2.4138305 -2.1735928 -2.1079519 -2.0421004 -2.0183158 -2.3971632 -3.1298928 -3.857913 -4.4065948 -4.6564245][-4.3549895 -4.6340551 -4.6209297 -4.4639053 -4.1695342 -3.7799344 -3.5595539 -3.5018508 -3.4870815 -3.4577632 -3.7264364 -4.3775616 -5.0024476 -5.465456 -5.6389461][-5.4786215 -5.6389675 -5.6041374 -5.4715085 -5.1814804 -4.9450407 -4.8340712 -4.6652355 -4.5369005 -4.5301557 -4.7518592 -5.2410583 -5.5980105 -5.8225851 -5.8565598][-6.3299875 -6.4273028 -6.2050452 -6.0744014 -5.9356728 -5.8112631 -5.7741838 -5.5977268 -5.4230294 -5.3215942 -5.37408 -5.728807 -6.0493722 -6.1552935 -6.0420365]]...]
INFO - root - 2017-12-16 12:38:56.108764: step 810, loss = 0.31, batch loss = 0.25 (28.2 examples/sec; 0.283 sec/batch; 26h:06m:31s remains)
INFO - root - 2017-12-16 12:38:58.977344: step 820, loss = 0.24, batch loss = 0.18 (26.8 examples/sec; 0.298 sec/batch; 27h:27m:22s remains)
INFO - root - 2017-12-16 12:39:01.793085: step 830, loss = 0.28, batch loss = 0.23 (28.6 examples/sec; 0.279 sec/batch; 25h:44m:45s remains)
INFO - root - 2017-12-16 12:39:04.720948: step 840, loss = 0.26, batch loss = 0.20 (26.7 examples/sec; 0.300 sec/batch; 27h:38m:23s remains)
INFO - root - 2017-12-16 12:39:07.600629: step 850, loss = 0.25, batch loss = 0.19 (28.1 examples/sec; 0.285 sec/batch; 26h:13m:53s remains)
INFO - root - 2017-12-16 12:39:10.506898: step 860, loss = 0.27, batch loss = 0.21 (29.0 examples/sec; 0.276 sec/batch; 25h:23m:51s remains)
INFO - root - 2017-12-16 12:39:13.377729: step 870, loss = 0.21, batch loss = 0.15 (28.0 examples/sec; 0.285 sec/batch; 26h:16m:35s remains)
INFO - root - 2017-12-16 12:39:16.226171: step 880, loss = 0.25, batch loss = 0.19 (27.9 examples/sec; 0.287 sec/batch; 26h:26m:59s remains)
INFO - root - 2017-12-16 12:39:19.054302: step 890, loss = 0.26, batch loss = 0.21 (29.1 examples/sec; 0.275 sec/batch; 25h:18m:37s remains)
INFO - root - 2017-12-16 12:39:21.905811: step 900, loss = 0.23, batch loss = 0.17 (28.5 examples/sec; 0.280 sec/batch; 25h:48m:44s remains)
2017-12-16 12:39:22.353470: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8637753 -2.5952415 -3.2584236 -3.1649289 -3.1704841 -3.1023335 -3.2878478 -3.4038565 -3.389715 -3.2222509 -3.6271248 -4.660955 -5.1278462 -5.5337009 -5.8264294][-1.8300781 -2.7434795 -3.6464548 -3.9668884 -4.515964 -4.7679067 -4.6530433 -4.712275 -4.67886 -4.3974462 -4.6234126 -5.4441113 -6.0597467 -6.1903992 -6.1324215][-1.850095 -3.0459366 -4.0337172 -4.6215487 -5.3779364 -5.8298326 -6.2297616 -5.8419542 -4.9882288 -4.7074223 -4.9329815 -5.9825897 -6.4770889 -6.6195412 -6.3873119][-1.3722615 -2.7099226 -4.298121 -5.2622161 -5.6980124 -5.5662246 -5.0297685 -4.3844423 -4.1599393 -4.2511945 -5.0026298 -6.1802988 -6.9351826 -7.25478 -6.8104434][-0.92594934 -2.4480369 -3.6067443 -4.374208 -4.7055492 -3.8518505 -2.6057949 -1.5435374 -0.71473026 -1.1422634 -2.8239636 -5.0393591 -6.493968 -7.081933 -6.8435187][-1.0966954 -2.5032091 -3.5016026 -3.3515544 -2.2122982 -0.27523375 1.7162881 2.7022529 2.6262331 1.7644181 -0.24231672 -2.8694973 -4.821733 -6.1339779 -6.6894197][-1.6921775 -2.8617387 -3.3912435 -2.3926537 -0.27105761 2.5247746 5.1151657 6.2343864 6.0048389 4.35927 1.4051104 -1.5256579 -3.5226505 -5.0941811 -5.9584856][-2.0981989 -2.7864566 -2.9457035 -1.5829556 1.2029762 4.1333151 6.4736309 7.1129961 6.3835707 4.9439983 2.3163495 -0.95422935 -3.5735741 -5.1502051 -5.7620764][-2.8009624 -3.4842381 -3.4733799 -1.9049997 0.38047075 2.9727206 5.115654 5.3974638 4.3053551 2.6379609 0.20461893 -2.2113957 -4.0640478 -5.4704347 -6.1221752][-3.6123252 -4.3646355 -4.5398574 -3.4452612 -1.5987928 -0.06144762 0.92968273 0.97007895 0.26277924 -0.89995289 -2.8451316 -4.8019719 -6.1464958 -6.7525797 -6.9245353][-4.6667061 -5.5826645 -6.16852 -5.6611137 -4.8697705 -3.6568573 -2.9776726 -3.0687294 -3.4730921 -4.1800566 -5.5581088 -6.9034257 -7.5877447 -7.6364479 -7.5148172][-6.0801387 -6.5715542 -7.0321178 -6.9819245 -6.5411091 -5.7364941 -5.3416247 -5.4089646 -5.6953182 -5.843606 -6.3591828 -7.5015049 -8.1507483 -7.9381995 -7.5364656][-6.4713926 -7.1648636 -7.8050947 -7.8236642 -7.9045639 -7.320857 -6.8589244 -6.4452581 -6.5626235 -6.7229452 -7.1125917 -7.4980907 -7.6570039 -7.5724068 -7.3220024][-7.6838317 -7.5525341 -7.6970191 -7.9684906 -8.0385666 -7.5194726 -7.0449152 -6.5055933 -6.1339846 -6.0321765 -6.3186822 -6.9713058 -7.3531423 -7.187623 -6.9041052][-7.4112973 -7.6073875 -7.4616003 -7.3389807 -7.2725124 -7.1183233 -6.9045935 -6.0915651 -5.4515676 -5.5173082 -5.7641187 -5.6908369 -5.5725293 -5.6973581 -5.63861]]...]
INFO - root - 2017-12-16 12:39:25.188168: step 910, loss = 0.27, batch loss = 0.22 (27.2 examples/sec; 0.294 sec/batch; 27h:05m:41s remains)
INFO - root - 2017-12-16 12:39:28.027031: step 920, loss = 0.30, batch loss = 0.25 (28.1 examples/sec; 0.285 sec/batch; 26h:14m:01s remains)
INFO - root - 2017-12-16 12:39:30.874239: step 930, loss = 0.28, batch loss = 0.22 (28.8 examples/sec; 0.277 sec/batch; 25h:33m:15s remains)
INFO - root - 2017-12-16 12:39:33.748789: step 940, loss = 0.25, batch loss = 0.19 (28.8 examples/sec; 0.278 sec/batch; 25h:37m:02s remains)
INFO - root - 2017-12-16 12:39:36.607679: step 950, loss = 0.25, batch loss = 0.19 (27.7 examples/sec; 0.289 sec/batch; 26h:35m:41s remains)
INFO - root - 2017-12-16 12:39:39.514022: step 960, loss = 0.33, batch loss = 0.27 (28.7 examples/sec; 0.278 sec/batch; 25h:37m:41s remains)
INFO - root - 2017-12-16 12:39:42.351365: step 970, loss = 0.27, batch loss = 0.21 (28.0 examples/sec; 0.285 sec/batch; 26h:15m:54s remains)
INFO - root - 2017-12-16 12:39:45.151707: step 980, loss = 0.37, batch loss = 0.32 (28.2 examples/sec; 0.284 sec/batch; 26h:10m:01s remains)
INFO - root - 2017-12-16 12:39:48.020479: step 990, loss = 0.27, batch loss = 0.22 (28.0 examples/sec; 0.285 sec/batch; 26h:16m:05s remains)
INFO - root - 2017-12-16 12:39:50.881288: step 1000, loss = 0.24, batch loss = 0.18 (29.0 examples/sec; 0.276 sec/batch; 25h:24m:15s remains)
2017-12-16 12:39:51.365874: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.80200982 -1.6123459 -2.2539318 -2.8535094 -3.1194901 -3.1707134 -3.2344458 -3.0861337 -2.5864062 -1.8112268 -1.4894881 -1.9600387 -2.6992357 -3.4381933 -3.7645023][-1.02319 -2.2654028 -3.5029917 -4.4709396 -5.0534787 -5.1537123 -4.8288159 -4.44977 -4.017458 -3.298574 -2.9510818 -3.2178428 -3.6552558 -4.1514316 -4.2348809][-1.4281046 -3.0251908 -4.4025674 -5.4858961 -6.1610417 -6.1869698 -5.872798 -5.2909155 -4.5612631 -4.0510144 -4.0791655 -4.544332 -5.029211 -5.3138204 -5.0426941][-1.6793246 -3.4710326 -5.07986 -5.9397526 -6.0314908 -5.5882096 -5.0169711 -4.4426565 -4.0876765 -3.9380221 -4.3152132 -5.2618227 -6.0445185 -6.3710279 -5.9769325][-1.7961292 -3.714967 -5.3157196 -5.8878384 -5.1410933 -3.5495489 -2.0980613 -1.366035 -1.416965 -2.0612535 -3.4032786 -5.1602287 -6.4216418 -7.0153646 -6.6644697][-1.7909269 -3.3681808 -4.6401758 -4.7517123 -3.7045143 -1.6759109 0.52967358 1.7304883 1.6036997 0.44612169 -1.6152558 -4.1364055 -6.1379948 -7.1808338 -6.958662][-1.6607866 -3.0154555 -3.8627362 -3.532969 -1.95298 0.42537785 2.7959313 4.1718082 4.0018206 2.607255 -0.0076375008 -3.062546 -5.3752794 -6.6159868 -6.6357193][-1.51057 -2.5867558 -3.3845968 -3.0707486 -1.2792339 1.3279958 3.9019117 5.2796431 4.936058 3.3916535 0.83245277 -2.2421062 -4.7739592 -6.2184048 -6.3843446][-1.1862693 -2.1404915 -2.955965 -2.9179513 -1.9226937 0.12858009 2.5261445 3.9166059 3.9204111 2.8805308 0.62282705 -2.0493538 -4.1514626 -5.5187664 -5.8306708][-1.0489841 -1.9322448 -2.8742456 -3.4020841 -2.983573 -1.8435376 -0.46104789 0.82925177 1.296793 0.64514256 -0.61101079 -2.2915785 -3.9439847 -5.0030341 -5.2481852][-1.6059647 -2.363677 -3.2667818 -4.1012182 -4.3946085 -4.1037488 -3.6064296 -3.2427707 -2.890574 -2.5135915 -2.7659311 -3.4280562 -3.7431462 -4.1283875 -4.4396191][-2.4632347 -3.2306857 -4.2682438 -5.3048959 -6.0892019 -6.3060169 -6.1216755 -5.9739356 -5.6741533 -5.1031995 -4.5777049 -4.2997384 -4.1487513 -4.0983677 -3.9322665][-3.5156424 -4.053381 -5.0199504 -6.272613 -7.4695435 -8.2142754 -8.4565353 -8.2935648 -7.5398932 -6.4477115 -5.6176429 -4.9476027 -4.4443622 -4.181457 -4.0029373][-4.1737776 -4.5420012 -5.263134 -6.3690562 -7.7475295 -8.7824726 -9.1340837 -8.9080353 -8.1705523 -7.0508194 -5.8074861 -4.969039 -4.5365586 -4.1681709 -3.8292055][-4.1444554 -4.3732228 -4.9556046 -5.785635 -6.8609786 -8.0336761 -8.6127262 -8.48155 -7.8268204 -6.7910357 -5.695302 -4.8705349 -4.2390022 -4.0074267 -3.9176073]]...]
INFO - root - 2017-12-16 12:39:54.379509: step 1010, loss = 0.28, batch loss = 0.22 (27.5 examples/sec; 0.291 sec/batch; 26h:46m:15s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-fixqian-clip5
INFO - root - 2017-12-16 12:39:57.243978: step 1020, loss = 0.34, batch loss = 0.28 (29.1 examples/sec; 0.275 sec/batch; 25h:16m:42s remains)
INFO - root - 2017-12-16 12:40:00.053466: step 1030, loss = 0.25, batch loss = 0.20 (28.6 examples/sec; 0.280 sec/batch; 25h:47m:26s remains)
INFO - root - 2017-12-16 12:40:02.960459: step 1040, loss = 0.26, batch loss = 0.20 (28.1 examples/sec; 0.284 sec/batch; 26h:10m:47s remains)
INFO - root - 2017-12-16 12:40:05.827943: step 1050, loss = 0.25, batch loss = 0.19 (28.7 examples/sec; 0.279 sec/batch; 25h:40m:08s remains)
INFO - root - 2017-12-16 12:40:08.744520: step 1060, loss = 0.41, batch loss = 0.35 (27.8 examples/sec; 0.288 sec/batch; 26h:29m:51s remains)
INFO - root - 2017-12-16 12:40:11.609506: step 1070, loss = 0.26, batch loss = 0.20 (25.6 examples/sec; 0.312 sec/batch; 28h:44m:30s remains)
INFO - root - 2017-12-16 12:40:14.469563: step 1080, loss = 0.30, batch loss = 0.24 (27.3 examples/sec; 0.293 sec/batch; 27h:00m:26s remains)
