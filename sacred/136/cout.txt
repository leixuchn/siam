INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "136"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lastaddlayer-adm-0.001
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-07 05:13:45.433155: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 05:13:45.433190: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 05:13:45.433197: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 05:13:45.433201: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 05:13:45.433205: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 256, 6, 6), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 256, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 6, 6), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 256, 20, 20), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 256, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 20, 20), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-07 05:13:52.110702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 6.38GiB
2017-12-07 05:13:52.110773: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0x55cb8fb54bd0
2017-12-07 05:13:57.573694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 1 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 4.02GiB
2017-12-07 05:13:57.573752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 0 and 1
2017-12-07 05:13:57.573767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 1 and 0
2017-12-07 05:13:57.573783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 1 
2017-12-07 05:13:57.573796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y N 
2017-12-07 05:13:57.573800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 1:   N Y 
2017-12-07 05:13:57.573809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
2017-12-07 05:13:57.573813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b1/weights:0' shape=(3, 3, 192, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b1/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b2/weights:0' shape=(3, 3, 192, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b2/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>]
INFO - root - 2017-12-07 05:14:21.529711: step 0, loss = 1.95, batch loss = 1.87 (0.5 examples/sec; 17.304 sec/batch; 1598h:10m:31s remains)
2017-12-07 05:14:22.681040: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3696446 -4.4018412 -4.4718418 -4.5645418 -4.6436071 -4.6737618 -4.6453857 -4.5893283 -4.5468364 -4.5413165 -4.5655832 -4.5960116 -4.5991216 -4.5586262 -4.4861684][-4.4172945 -4.4809742 -4.5831046 -4.6983981 -4.7796812 -4.78954 -4.7356691 -4.6723671 -4.6461568 -4.6569219 -4.6838236 -4.7097025 -4.7044516 -4.6486578 -4.5518408][-4.4594264 -4.5467649 -4.6676445 -4.7845154 -4.8395462 -4.7975526 -4.6976867 -4.6304684 -4.6349287 -4.6732392 -4.7114434 -4.743835 -4.7450376 -4.6935267 -4.5910063][-4.4813023 -4.5763321 -4.7010937 -4.8011484 -4.8010373 -4.6722784 -4.495573 -4.406065 -4.434402 -4.509201 -4.5918794 -4.670157 -4.710844 -4.6868896 -4.5955863][-4.4737382 -4.5592022 -4.6772819 -4.7489896 -4.6778674 -4.4472208 -4.177979 -4.0433025 -4.071341 -4.1865578 -4.3505816 -4.5157466 -4.6261034 -4.644383 -4.573688][-4.438869 -4.5089245 -4.6180258 -4.6620293 -4.5290875 -4.2151237 -3.8647351 -3.6701317 -3.6717069 -3.8243091 -4.0849462 -4.3525934 -4.5392408 -4.596951 -4.5471029][-4.3970761 -4.4543643 -4.556448 -4.5798769 -4.4127984 -4.061233 -3.6689649 -3.4250512 -3.399487 -3.5911341 -3.9318969 -4.2702379 -4.500536 -4.5749364 -4.5344968][-4.3642793 -4.4173455 -4.5121288 -4.5294795 -4.3698945 -4.0398865 -3.6620607 -3.4128759 -3.3960721 -3.6263323 -3.99228 -4.3227215 -4.5267653 -4.5807223 -4.5373244][-4.371398 -4.4211845 -4.5011773 -4.5246458 -4.4101644 -4.1545806 -3.8410211 -3.626317 -3.636322 -3.8702848 -4.1878963 -4.4353051 -4.5620451 -4.5808668 -4.537488][-4.4327245 -4.4747286 -4.5299044 -4.5546112 -4.4920597 -4.3218288 -4.0837588 -3.9124036 -3.9375422 -4.13989 -4.3754067 -4.5218554 -4.572473 -4.5680447 -4.5336151][-4.4992547 -4.5331225 -4.5646806 -4.5867429 -4.5604177 -4.4514713 -4.2709303 -4.1348772 -4.16412 -4.321497 -4.4781804 -4.5498724 -4.5607343 -4.5566959 -4.5361943][-4.5112476 -4.5419803 -4.5701876 -4.6013684 -4.6000962 -4.5265594 -4.3835716 -4.2768211 -4.3029437 -4.4127717 -4.5077219 -4.5377707 -4.5404911 -4.5501084 -4.541841][-4.448204 -4.4790478 -4.524159 -4.5831137 -4.6082377 -4.5616345 -4.4475017 -4.3600864 -4.3708291 -4.4334974 -4.4848924 -4.4990954 -4.5121422 -4.5400276 -4.5420475][-4.34149 -4.3707151 -4.4364419 -4.5219216 -4.56387 -4.5332928 -4.4354944 -4.3496308 -4.3352766 -4.3616819 -4.3941364 -4.4211912 -4.4642129 -4.5180111 -4.5329437][-4.2498803 -4.2723331 -4.3485694 -4.4437242 -4.4817872 -4.4428244 -4.3351541 -4.2352023 -4.2062531 -4.2271738 -4.2744808 -4.33566 -4.4139657 -4.4914784 -4.5170979]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lastaddlayer-adm-0.001/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lastaddlayer-adm-0.001/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 05:14:31.412524: step 10, loss = 1.99, batch loss = 1.90 (10.9 examples/sec; 0.736 sec/batch; 67h:59m:17s remains)
INFO - root - 2017-12-07 05:14:38.283162: step 20, loss = 2.01, batch loss = 1.93 (10.5 examples/sec; 0.762 sec/batch; 70h:22m:26s remains)
INFO - root - 2017-12-07 05:14:45.311441: step 30, loss = 2.00, batch loss = 1.92 (11.8 examples/sec; 0.677 sec/batch; 62h:30m:30s remains)
INFO - root - 2017-12-07 05:14:52.383320: step 40, loss = 2.07, batch loss = 1.99 (11.4 examples/sec; 0.704 sec/batch; 64h:58m:44s remains)
INFO - root - 2017-12-07 05:14:59.493270: step 50, loss = 2.05, batch loss = 1.97 (12.0 examples/sec; 0.669 sec/batch; 61h:49m:01s remains)
INFO - root - 2017-12-07 05:15:06.528560: step 60, loss = 2.04, batch loss = 1.96 (11.4 examples/sec; 0.700 sec/batch; 64h:39m:31s remains)
INFO - root - 2017-12-07 05:15:13.613617: step 70, loss = 1.97, batch loss = 1.88 (11.3 examples/sec; 0.705 sec/batch; 65h:05m:59s remains)
INFO - root - 2017-12-07 05:15:20.600493: step 80, loss = 2.02, batch loss = 1.94 (11.0 examples/sec; 0.726 sec/batch; 67h:01m:30s remains)
INFO - root - 2017-12-07 05:15:27.706904: step 90, loss = 2.05, batch loss = 1.96 (11.2 examples/sec; 0.712 sec/batch; 65h:46m:31s remains)
INFO - root - 2017-12-07 05:15:34.642909: step 100, loss = 2.01, batch loss = 1.93 (12.1 examples/sec; 0.659 sec/batch; 60h:50m:14s remains)
2017-12-07 05:15:35.412150: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4237728 -4.3625221 -4.2678504 -4.1827602 -4.1291423 -4.1167092 -4.1435223 -4.2182078 -4.32577 -4.4127903 -4.4715886 -4.5127735 -4.4997854 -4.4342465 -4.3668408][-4.389401 -4.322298 -4.215199 -4.1165848 -4.05276 -4.0394034 -4.0775084 -4.1617155 -4.2675347 -4.3547091 -4.4188714 -4.46718 -4.471417 -4.4278007 -4.3772283][-4.3810511 -4.3237762 -4.2220812 -4.1195087 -4.0407209 -4.0087614 -4.030766 -4.1016111 -4.1972218 -4.2816792 -4.347229 -4.3990426 -4.4186959 -4.3942542 -4.3540816][-4.3872266 -4.3487453 -4.2664232 -4.1732054 -4.085968 -4.0272069 -4.01734 -4.0643125 -4.1476903 -4.2255225 -4.2831464 -4.3279514 -4.3511963 -4.3344216 -4.2928209][-4.3965855 -4.3733788 -4.3086286 -4.2246141 -4.1336517 -4.0549507 -4.02118 -4.05232 -4.1240816 -4.1874256 -4.2282939 -4.260716 -4.2846079 -4.2726669 -4.226295][-4.4036579 -4.3861418 -4.3266225 -4.2392297 -4.1376023 -4.04456 -3.9982386 -4.0232639 -4.0893254 -4.1456709 -4.1785259 -4.20724 -4.2390313 -4.2360954 -4.1877666][-4.4061217 -4.3844891 -4.3162193 -4.2122788 -4.0946264 -3.9940925 -3.9449725 -3.9680734 -4.0330048 -4.0949755 -4.1328483 -4.1636987 -4.2046728 -4.2154107 -4.1783996][-4.4010758 -4.3718534 -4.2889981 -4.1682878 -4.0409122 -3.9404602 -3.8910317 -3.9069731 -3.9684436 -4.0446692 -4.102262 -4.1436648 -4.1886382 -4.2048068 -4.1818309][-4.3953419 -4.3626304 -4.2735906 -4.1489048 -4.0234761 -3.9258869 -3.8701484 -3.8663557 -3.9155614 -4.0052047 -4.0913157 -4.1532307 -4.1964498 -4.2009392 -4.1767564][-4.3931675 -4.3650937 -4.2840838 -4.1731534 -4.064002 -3.9745936 -3.9119151 -3.8832951 -3.909529 -3.9978392 -4.103549 -4.1879086 -4.2310643 -4.2187457 -4.182848][-4.4017849 -4.3887658 -4.3345971 -4.2595325 -4.1846485 -4.1103621 -4.0377526 -3.9784677 -3.9722383 -4.0424333 -4.1542525 -4.2584419 -4.3099189 -4.2903972 -4.2434673][-4.4112825 -4.414206 -4.3901153 -4.3537216 -4.3161263 -4.2623973 -4.1902032 -4.1189952 -4.0946622 -4.1459837 -4.2483745 -4.3516836 -4.4033422 -4.385128 -4.3380303][-4.4097033 -4.4217424 -4.4179721 -4.40837 -4.4010754 -4.3769612 -4.3286734 -4.2737761 -4.2522507 -4.28851 -4.3649154 -4.4400125 -4.4746776 -4.45901 -4.4213805][-4.3993111 -4.4166279 -4.4248567 -4.4294634 -4.4371896 -4.4352374 -4.4162092 -4.3900595 -4.382247 -4.4075861 -4.4531403 -4.4921989 -4.5034184 -4.4868946 -4.4599652][-4.3822789 -4.4005442 -4.4140315 -4.4235377 -4.4327288 -4.4387441 -4.4389296 -4.4356256 -4.4395781 -4.4552431 -4.4750848 -4.4863281 -4.4813771 -4.4650083 -4.4464779]]...]
INFO - root - 2017-12-07 05:15:42.471573: step 110, loss = 2.06, batch loss = 1.98 (11.0 examples/sec; 0.730 sec/batch; 67h:25m:23s remains)
INFO - root - 2017-12-07 05:15:49.427657: step 120, loss = 1.99, batch loss = 1.91 (11.7 examples/sec; 0.684 sec/batch; 63h:10m:30s remains)
INFO - root - 2017-12-07 05:15:56.424682: step 130, loss = 2.01, batch loss = 1.92 (12.1 examples/sec; 0.664 sec/batch; 61h:15m:33s remains)
INFO - root - 2017-12-07 05:16:03.446747: step 140, loss = 2.05, batch loss = 1.97 (11.9 examples/sec; 0.672 sec/batch; 62h:03m:44s remains)
INFO - root - 2017-12-07 05:16:10.510081: step 150, loss = 2.06, batch loss = 1.98 (11.2 examples/sec; 0.716 sec/batch; 66h:06m:26s remains)
INFO - root - 2017-12-07 05:16:17.469312: step 160, loss = 2.01, batch loss = 1.93 (11.2 examples/sec; 0.712 sec/batch; 65h:43m:35s remains)
INFO - root - 2017-12-07 05:16:24.401181: step 170, loss = 2.07, batch loss = 1.99 (11.9 examples/sec; 0.674 sec/batch; 62h:15m:54s remains)
INFO - root - 2017-12-07 05:16:31.206604: step 180, loss = 1.99, batch loss = 1.91 (12.4 examples/sec; 0.645 sec/batch; 59h:32m:32s remains)
INFO - root - 2017-12-07 05:16:38.329979: step 190, loss = 2.03, batch loss = 1.95 (11.8 examples/sec; 0.679 sec/batch; 62h:41m:16s remains)
INFO - root - 2017-12-07 05:16:45.432633: step 200, loss = 2.00, batch loss = 1.92 (11.5 examples/sec; 0.694 sec/batch; 64h:04m:41s remains)
2017-12-07 05:16:46.271949: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6461892 -4.6716642 -4.6311717 -4.5541463 -4.4994698 -4.4751229 -4.455874 -4.4420948 -4.4326653 -4.4122062 -4.3851347 -4.3711548 -4.3795352 -4.4054976 -4.4215903][-4.6511278 -4.6761241 -4.6315556 -4.5336213 -4.4464412 -4.3925371 -4.348021 -4.3289561 -4.33872 -4.3469982 -4.33218 -4.3078127 -4.30208 -4.3285813 -4.3585334][-4.528306 -4.549552 -4.527926 -4.4504137 -4.359601 -4.2821817 -4.2144327 -4.1954322 -4.2410684 -4.2994804 -4.3126092 -4.27737 -4.2353759 -4.2303357 -4.2523251][-4.3558717 -4.3697639 -4.3767943 -4.3426981 -4.2724833 -4.1853147 -4.1023469 -4.0832791 -4.1616826 -4.2680812 -4.3136086 -4.2745175 -4.1950984 -4.1427608 -4.1416922][-4.2637515 -4.2642369 -4.2760777 -4.265667 -4.2088809 -4.1138458 -4.0168567 -3.9908524 -4.0888934 -4.2278242 -4.2999077 -4.2662992 -4.1715956 -4.0908537 -4.0795226][-4.2657776 -4.2402115 -4.2277155 -4.206985 -4.1423106 -4.0385418 -3.9276752 -3.8890922 -3.9940786 -4.1533213 -4.247396 -4.2331438 -4.1513157 -4.0724463 -4.0703731][-4.3177524 -4.256547 -4.2038012 -4.1518269 -4.0671587 -3.9547293 -3.8283622 -3.7700934 -3.8778224 -4.0517335 -4.1652918 -4.1788568 -4.1242661 -4.0652084 -4.0758238][-4.3600273 -4.2774868 -4.1912341 -4.1140814 -4.022985 -3.9254336 -3.8094244 -3.7407458 -3.8349204 -4.00119 -4.1193504 -4.1483126 -4.1112423 -4.0653863 -4.0790033][-4.3760614 -4.2914438 -4.194263 -4.1143169 -4.0430422 -3.9929812 -3.9236519 -3.8740289 -3.950119 -4.0859008 -4.1838293 -4.2004 -4.1540265 -4.1046348 -4.1058545][-4.4384775 -4.360498 -4.2626343 -4.1885524 -4.1409802 -4.1365995 -4.10849 -4.0772085 -4.1428924 -4.251904 -4.3245554 -4.3148508 -4.2443304 -4.1858249 -4.1788282][-4.5496588 -4.4810877 -4.3864141 -4.3136511 -4.283545 -4.310626 -4.3023009 -4.2758265 -4.3224931 -4.4032726 -4.4575982 -4.432272 -4.3495593 -4.2943435 -4.2900195][-4.6461163 -4.5844407 -4.4924684 -4.4175525 -4.4029841 -4.4547133 -4.4620466 -4.4404306 -4.4608216 -4.50515 -4.5386567 -4.5114455 -4.4402795 -4.3991265 -4.3975296][-4.6843319 -4.6437149 -4.5718126 -4.50208 -4.4918766 -4.5452933 -4.5596328 -4.5506635 -4.5569663 -4.5698843 -4.5769906 -4.5486116 -4.5013127 -4.4831095 -4.4863367][-4.6712852 -4.6644239 -4.6259527 -4.5734839 -4.5645227 -4.6024218 -4.6154065 -4.6166921 -4.6176791 -4.615591 -4.6077685 -4.5847173 -4.5611081 -4.5571432 -4.558641][-4.6165857 -4.631319 -4.6219273 -4.5956364 -4.5958776 -4.6219459 -4.6348429 -4.6422839 -4.6421132 -4.6346936 -4.6237817 -4.6095757 -4.59969 -4.5968919 -4.5877147]]...]
INFO - root - 2017-12-07 05:16:53.384315: step 210, loss = 1.99, batch loss = 1.91 (11.5 examples/sec; 0.693 sec/batch; 63h:59m:10s remains)
INFO - root - 2017-12-07 05:17:00.448295: step 220, loss = 2.04, batch loss = 1.95 (11.9 examples/sec; 0.671 sec/batch; 61h:58m:31s remains)
INFO - root - 2017-12-07 05:17:07.544192: step 230, loss = 2.02, batch loss = 1.94 (11.2 examples/sec; 0.716 sec/batch; 66h:03m:15s remains)
INFO - root - 2017-12-07 05:17:14.529884: step 240, loss = 2.03, batch loss = 1.95 (11.7 examples/sec; 0.683 sec/batch; 62h:59m:52s remains)
INFO - root - 2017-12-07 05:17:21.512378: step 250, loss = 2.03, batch loss = 1.95 (11.0 examples/sec; 0.724 sec/batch; 66h:49m:15s remains)
INFO - root - 2017-12-07 05:17:28.533168: step 260, loss = 2.00, batch loss = 1.92 (11.0 examples/sec; 0.726 sec/batch; 66h:59m:22s remains)
INFO - root - 2017-12-07 05:17:35.627751: step 270, loss = 2.05, batch loss = 1.96 (10.7 examples/sec; 0.746 sec/batch; 68h:52m:59s remains)
INFO - root - 2017-12-07 05:17:42.652812: step 280, loss = 2.02, batch loss = 1.94 (11.2 examples/sec; 0.713 sec/batch; 65h:50m:35s remains)
INFO - root - 2017-12-07 05:17:49.751372: step 290, loss = 1.99, batch loss = 1.91 (11.4 examples/sec; 0.701 sec/batch; 64h:40m:16s remains)
INFO - root - 2017-12-07 05:17:56.846767: step 300, loss = 2.05, batch loss = 1.97 (11.8 examples/sec; 0.678 sec/batch; 62h:36m:04s remains)
2017-12-07 05:17:57.704173: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0852885 -4.0873141 -4.10043 -4.1388011 -4.1908774 -4.2165813 -4.1979351 -4.1466312 -4.0801992 -4.0341582 -4.0431862 -4.1188774 -4.2101345 -4.2540827 -4.2529783][-4.2017765 -4.2105327 -4.2031 -4.2075796 -4.2355013 -4.2540679 -4.2424641 -4.1993456 -4.1386337 -4.102694 -4.11979 -4.1902375 -4.2659497 -4.2914195 -4.271687][-4.3568993 -4.3511324 -4.3046279 -4.2576056 -4.2486386 -4.2621732 -4.2705736 -4.2550635 -4.2189207 -4.1961203 -4.2134233 -4.266717 -4.3193946 -4.324501 -4.2860394][-4.4736137 -4.43639 -4.3401527 -4.2344084 -4.1875877 -4.2017803 -4.2358341 -4.253202 -4.2464871 -4.2378469 -4.254221 -4.299439 -4.3427119 -4.3408689 -4.2928777][-4.5025921 -4.428865 -4.28964 -4.1376948 -4.0595388 -4.0713263 -4.1174927 -4.1538682 -4.168633 -4.1693845 -4.1937375 -4.25571 -4.3184767 -4.3295732 -4.2810693][-4.4676933 -4.3627973 -4.1920452 -4.0211248 -3.9308383 -3.937794 -3.9777837 -4.0109425 -4.0255113 -4.0176339 -4.0539174 -4.1547222 -4.260601 -4.2979856 -4.2553325][-4.4144769 -4.282649 -4.0928741 -3.931617 -3.8568563 -3.8689556 -3.89858 -3.9152741 -3.8987436 -3.8527906 -3.8891287 -4.0239725 -4.16513 -4.2286882 -4.2063808][-4.3653536 -4.2164092 -4.0229859 -3.8765249 -3.8199272 -3.8400972 -3.8679898 -3.8729811 -3.821054 -3.7349031 -3.7588649 -3.9036312 -4.05704 -4.1382952 -4.1450071][-4.3308725 -4.1978374 -4.0336661 -3.9112623 -3.8689926 -3.8920102 -3.9240937 -3.9263198 -3.8568621 -3.7523558 -3.7546022 -3.8800294 -4.0229697 -4.1063981 -4.131453][-4.29541 -4.203836 -4.1008749 -4.0296831 -4.0176373 -4.0520272 -4.0908179 -4.0913477 -4.0176997 -3.907779 -3.8805625 -3.9673634 -4.0838084 -4.1567283 -4.183538][-4.2576046 -4.2087126 -4.1742849 -4.1668472 -4.1933966 -4.24001 -4.2780409 -4.2726593 -4.201159 -4.0903096 -4.0327106 -4.0831056 -4.180223 -4.2515621 -4.2794194][-4.2465539 -4.2281203 -4.2408528 -4.2773609 -4.3254595 -4.37585 -4.4093456 -4.4007277 -4.3335257 -4.2214837 -4.1406641 -4.1643853 -4.2503967 -4.3309445 -4.3687882][-4.2606525 -4.2600913 -4.2904286 -4.3407969 -4.3876748 -4.4278069 -4.4551654 -4.4485459 -4.3905878 -4.2871013 -4.2008762 -4.2076 -4.2801638 -4.3634782 -4.411334][-4.2947564 -4.300086 -4.3260083 -4.3683314 -4.4004178 -4.425128 -4.4487844 -4.4469676 -4.40219 -4.317584 -4.2444949 -4.2449732 -4.29981 -4.3684154 -4.4124918][-4.3239379 -4.3204203 -4.3327069 -4.3579078 -4.3768473 -4.3968544 -4.4248581 -4.433619 -4.4038634 -4.3389254 -4.2837 -4.2835722 -4.3181715 -4.35643 -4.3801904]]...]
INFO - root - 2017-12-07 05:18:04.743182: step 310, loss = 2.04, batch loss = 1.95 (11.2 examples/sec; 0.715 sec/batch; 65h:56m:13s remains)
INFO - root - 2017-12-07 05:18:11.806636: step 320, loss = 1.99, batch loss = 1.91 (11.2 examples/sec; 0.714 sec/batch; 65h:52m:20s remains)
INFO - root - 2017-12-07 05:18:18.941048: step 330, loss = 2.00, batch loss = 1.92 (11.3 examples/sec; 0.709 sec/batch; 65h:26m:58s remains)
INFO - root - 2017-12-07 05:18:26.138072: step 340, loss = 2.01, batch loss = 1.92 (11.3 examples/sec; 0.707 sec/batch; 65h:15m:36s remains)
INFO - root - 2017-12-07 05:18:33.028958: step 350, loss = 2.03, batch loss = 1.95 (10.8 examples/sec; 0.741 sec/batch; 68h:19m:18s remains)
INFO - root - 2017-12-07 05:18:40.078167: step 360, loss = 2.06, batch loss = 1.98 (11.1 examples/sec; 0.720 sec/batch; 66h:25m:57s remains)
INFO - root - 2017-12-07 05:18:47.172073: step 370, loss = 2.01, batch loss = 1.93 (10.9 examples/sec; 0.736 sec/batch; 67h:52m:33s remains)
INFO - root - 2017-12-07 05:18:54.265490: step 380, loss = 2.05, batch loss = 1.97 (11.3 examples/sec; 0.706 sec/batch; 65h:10m:04s remains)
INFO - root - 2017-12-07 05:19:01.435590: step 390, loss = 2.01, batch loss = 1.93 (12.2 examples/sec; 0.658 sec/batch; 60h:43m:05s remains)
INFO - root - 2017-12-07 05:19:08.468561: step 400, loss = 1.99, batch loss = 1.90 (10.6 examples/sec; 0.754 sec/batch; 69h:33m:53s remains)
2017-12-07 05:19:09.221093: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5337048 -4.5259347 -4.5047326 -4.5058041 -4.5438557 -4.5576434 -4.5273576 -4.4963632 -4.5186563 -4.57594 -4.6251383 -4.642138 -4.6785603 -4.7485576 -4.7932763][-4.59997 -4.5980258 -4.57891 -4.5799985 -4.6121416 -4.6048765 -4.5428357 -4.4856472 -4.5064478 -4.582283 -4.6475878 -4.665998 -4.6855445 -4.7309871 -4.7662845][-4.6422195 -4.6503687 -4.6317458 -4.6228395 -4.6297979 -4.5872951 -4.49343 -4.4169688 -4.4381986 -4.5331912 -4.6167636 -4.6439695 -4.6548657 -4.6755433 -4.7011776][-4.6157975 -4.628922 -4.6038184 -4.5806808 -4.5617781 -4.491271 -4.3777966 -4.2928686 -4.3141479 -4.4194965 -4.5097189 -4.5386167 -4.5480332 -4.5674872 -4.6044512][-4.5263944 -4.5243616 -4.4757147 -4.4320049 -4.3994117 -4.3243656 -4.2132273 -4.1356587 -4.1634145 -4.2762756 -4.3649964 -4.387114 -4.3992763 -4.4339051 -4.4990482][-4.4513321 -4.4226842 -4.3441558 -4.2782531 -4.236589 -4.1628227 -4.05329 -3.9801891 -4.0126872 -4.1326346 -4.2263627 -4.2502618 -4.2715683 -4.3286338 -4.4235339][-4.3882532 -4.3447013 -4.2708673 -4.2172365 -4.1899343 -4.1230478 -4.0038462 -3.9160464 -3.9287586 -4.0321789 -4.1296387 -4.1747866 -4.2202525 -4.30063 -4.410162][-4.2953024 -4.2428637 -4.2044024 -4.2111626 -4.2445 -4.2172551 -4.1002827 -3.9856277 -3.9462006 -3.9943337 -4.0784125 -4.1527128 -4.2342138 -4.3365259 -4.4473968][-4.1792188 -4.10522 -4.1016583 -4.1863875 -4.3108158 -4.3513784 -4.2557855 -4.1206279 -4.0255041 -4.0118518 -4.0747414 -4.1736312 -4.2883029 -4.4050813 -4.5073614][-4.0686536 -3.9691412 -3.9845319 -4.1330476 -4.3461 -4.4604144 -4.4004331 -4.2662911 -4.1422467 -4.0915484 -4.1396112 -4.2503815 -4.3797493 -4.4963932 -4.582583][-4.0218325 -3.8966978 -3.897064 -4.065012 -4.3311367 -4.5008526 -4.4807143 -4.3691554 -4.2498083 -4.1966863 -4.247406 -4.3643827 -4.4937682 -4.6007805 -4.6667523][-4.06762 -3.9242253 -3.8937421 -4.0431471 -4.3087077 -4.4900732 -4.4974709 -4.4174008 -4.3250942 -4.2934527 -4.3585806 -4.478703 -4.5991516 -4.691514 -4.7345915][-4.1970129 -4.0592594 -4.0067759 -4.115243 -4.3307548 -4.4792833 -4.4963393 -4.4470758 -4.3859959 -4.3740683 -4.4417257 -4.5533366 -4.6602325 -4.7363677 -4.7584572][-4.381268 -4.2650714 -4.2010374 -4.2592816 -4.3965483 -4.4850235 -4.4943581 -4.4664793 -4.4311466 -4.4289908 -4.4834466 -4.5726075 -4.6592021 -4.7176032 -4.7248483][-4.5328374 -4.4576979 -4.4054985 -4.4272 -4.4894242 -4.5182328 -4.5093513 -4.4877896 -4.464612 -4.4594927 -4.4884095 -4.5437202 -4.6023993 -4.6407938 -4.63771]]...]
INFO - root - 2017-12-07 05:19:16.297779: step 410, loss = 2.02, batch loss = 1.93 (11.7 examples/sec; 0.681 sec/batch; 62h:49m:50s remains)
INFO - root - 2017-12-07 05:19:23.415925: step 420, loss = 2.02, batch loss = 1.94 (11.2 examples/sec; 0.716 sec/batch; 66h:00m:48s remains)
INFO - root - 2017-12-07 05:19:30.495666: step 430, loss = 2.04, batch loss = 1.96 (10.9 examples/sec; 0.734 sec/batch; 67h:40m:54s remains)
INFO - root - 2017-12-07 05:19:37.613269: step 440, loss = 2.02, batch loss = 1.93 (11.1 examples/sec; 0.720 sec/batch; 66h:23m:00s remains)
INFO - root - 2017-12-07 05:19:44.666644: step 450, loss = 2.02, batch loss = 1.94 (11.1 examples/sec; 0.724 sec/batch; 66h:44m:01s remains)
INFO - root - 2017-12-07 05:19:51.800466: step 460, loss = 2.04, batch loss = 1.96 (11.1 examples/sec; 0.720 sec/batch; 66h:22m:38s remains)
INFO - root - 2017-12-07 05:19:58.940025: step 470, loss = 2.02, batch loss = 1.93 (10.5 examples/sec; 0.759 sec/batch; 70h:00m:12s remains)
INFO - root - 2017-12-07 05:20:05.959553: step 480, loss = 2.00, batch loss = 1.92 (11.5 examples/sec; 0.698 sec/batch; 64h:22m:25s remains)
INFO - root - 2017-12-07 05:20:12.823653: step 490, loss = 2.00, batch loss = 1.92 (11.0 examples/sec; 0.726 sec/batch; 66h:58m:54s remains)
INFO - root - 2017-12-07 05:20:19.847888: step 500, loss = 2.06, batch loss = 1.98 (11.2 examples/sec; 0.715 sec/batch; 65h:58m:20s remains)
2017-12-07 05:20:20.641360: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6170588 -4.6213441 -4.6244988 -4.6217618 -4.59756 -4.5623393 -4.529213 -4.5015531 -4.4736047 -4.4385657 -4.3855205 -4.3145852 -4.2447457 -4.199862 -4.1760936][-4.5725908 -4.5679908 -4.563076 -4.5496321 -4.5153613 -4.4764848 -4.451529 -4.4427323 -4.4361472 -4.41186 -4.3578582 -4.2810841 -4.2084904 -4.1735106 -4.1680231][-4.4944878 -4.47395 -4.4601254 -4.4464903 -4.4178295 -4.3937006 -4.3965154 -4.4283504 -4.4606752 -4.4556432 -4.3972144 -4.3061924 -4.2246957 -4.1931663 -4.2027316][-4.3996553 -4.3614316 -4.3437247 -4.332489 -4.3107352 -4.3023887 -4.3424816 -4.4261861 -4.4998074 -4.5022516 -4.4200687 -4.3001552 -4.2056236 -4.1828794 -4.22785][-4.3057132 -4.2673016 -4.26017 -4.2559247 -4.2349858 -4.2202091 -4.265059 -4.3641644 -4.4469733 -4.4334583 -4.3164754 -4.175663 -4.0925293 -4.1105533 -4.2187524][-4.2283974 -4.2155547 -4.2313371 -4.2361526 -4.205267 -4.1585989 -4.1594043 -4.2166314 -4.2717381 -4.2406139 -4.1098533 -3.9753821 -3.9331024 -4.00888 -4.1786084][-4.1946383 -4.2140164 -4.2456264 -4.2488575 -4.1978884 -4.1065488 -4.0340648 -4.0212145 -4.0513563 -4.0404005 -3.9455519 -3.8527546 -3.8551471 -3.9610815 -4.1386476][-4.2023268 -4.2235584 -4.2420993 -4.2375021 -4.1863875 -4.0785265 -3.9547336 -3.8916831 -3.9215646 -3.9620764 -3.9385295 -3.9073119 -3.9376614 -4.0282497 -4.1581345][-4.2318339 -4.2151465 -4.1974139 -4.1925921 -4.1767426 -4.107398 -3.9995031 -3.9419396 -3.9849656 -4.0534096 -4.07974 -4.0961051 -4.1378794 -4.2004671 -4.2705221][-4.2831955 -4.2190681 -4.16847 -4.171083 -4.2005095 -4.1897697 -4.1355782 -4.1161404 -4.1701593 -4.2284288 -4.2544489 -4.2789121 -4.3172579 -4.3645663 -4.4008446][-4.3592396 -4.2775626 -4.2135692 -4.2139878 -4.2566609 -4.275053 -4.2605824 -4.2745342 -4.3283114 -4.3600616 -4.3628974 -4.3710346 -4.3989773 -4.4403586 -4.4628916][-4.4330373 -4.3617306 -4.3014894 -4.2908335 -4.3154521 -4.3302789 -4.3295331 -4.349144 -4.3834724 -4.3875275 -4.3730345 -4.3725829 -4.3969259 -4.4309688 -4.4397554][-4.4752221 -4.4226489 -4.3802152 -4.3662109 -4.3693719 -4.3720922 -4.3741231 -4.3852725 -4.39458 -4.3801618 -4.3606505 -4.3555803 -4.3717952 -4.3901873 -4.386229][-4.4941607 -4.4575891 -4.4408207 -4.4382186 -4.4355555 -4.4347506 -4.4410009 -4.4488378 -4.4455433 -4.4285593 -4.4092622 -4.3923192 -4.38307 -4.3717141 -4.3505516][-4.4824324 -4.4565754 -4.4605451 -4.4726439 -4.4742918 -4.4751978 -4.4851875 -4.4941258 -4.4903674 -4.4802666 -4.4652963 -4.4366212 -4.3979559 -4.3540349 -4.31667]]...]
INFO - root - 2017-12-07 05:20:27.556760: step 510, loss = 2.04, batch loss = 1.95 (11.1 examples/sec; 0.723 sec/batch; 66h:38m:04s remains)
INFO - root - 2017-12-07 05:20:34.618238: step 520, loss = 2.02, batch loss = 1.93 (11.3 examples/sec; 0.706 sec/batch; 65h:03m:55s remains)
INFO - root - 2017-12-07 05:20:41.658703: step 530, loss = 2.04, batch loss = 1.95 (11.1 examples/sec; 0.718 sec/batch; 66h:11m:12s remains)
INFO - root - 2017-12-07 05:20:48.606522: step 540, loss = 1.98, batch loss = 1.89 (11.0 examples/sec; 0.729 sec/batch; 67h:13m:35s remains)
INFO - root - 2017-12-07 05:20:55.716523: step 550, loss = 2.01, batch loss = 1.93 (11.3 examples/sec; 0.709 sec/batch; 65h:23m:58s remains)
INFO - root - 2017-12-07 05:21:02.814157: step 560, loss = 2.00, batch loss = 1.91 (11.6 examples/sec; 0.687 sec/batch; 63h:19m:33s remains)
INFO - root - 2017-12-07 05:21:09.959144: step 570, loss = 2.01, batch loss = 1.92 (11.7 examples/sec; 0.683 sec/batch; 62h:57m:56s remains)
INFO - root - 2017-12-07 05:21:16.961014: step 580, loss = 2.04, batch loss = 1.95 (11.3 examples/sec; 0.706 sec/batch; 65h:04m:15s remains)
INFO - root - 2017-12-07 05:21:23.991578: step 590, loss = 2.05, batch loss = 1.97 (11.4 examples/sec; 0.700 sec/batch; 64h:33m:40s remains)
INFO - root - 2017-12-07 05:21:31.075487: step 600, loss = 2.00, batch loss = 1.91 (11.3 examples/sec; 0.711 sec/batch; 65h:31m:35s remains)
2017-12-07 05:21:31.795951: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1099119 -4.0640507 -4.0708919 -4.12952 -4.2078414 -4.2794013 -4.3065348 -4.3084359 -4.2968588 -4.2760844 -4.291811 -4.3458862 -4.4297633 -4.5192723 -4.5599432][-4.0899882 -4.0373635 -4.05701 -4.1322966 -4.2118344 -4.2723069 -4.2824788 -4.2708983 -4.2453189 -4.2097096 -4.2273216 -4.3043447 -4.4208655 -4.5340776 -4.5839543][-4.1808424 -4.1367059 -4.1720347 -4.2532907 -4.3170428 -4.3449373 -4.3261023 -4.3043509 -4.280014 -4.241889 -4.2486429 -4.3153839 -4.4248424 -4.5258355 -4.5613065][-4.307477 -4.2821827 -4.3205137 -4.387948 -4.4265957 -4.4274087 -4.3978257 -4.3838944 -4.3720031 -4.33765 -4.3226414 -4.3521976 -4.4184704 -4.4789453 -4.49104][-4.40745 -4.3912 -4.4088387 -4.43921 -4.4490952 -4.4385824 -4.4192367 -4.4233451 -4.4302292 -4.4130116 -4.3964753 -4.3980484 -4.4143629 -4.4278941 -4.42225][-4.4508314 -4.42209 -4.3982854 -4.3711953 -4.3415656 -4.3226981 -4.3219724 -4.3555684 -4.393415 -4.4142547 -4.4302039 -4.43751 -4.4295778 -4.4121542 -4.3945813][-4.4666514 -4.4017453 -4.3212171 -4.2326193 -4.1667566 -4.1452203 -4.1655097 -4.2303376 -4.3003697 -4.3584809 -4.4136682 -4.4469547 -4.4449377 -4.421567 -4.3984942][-4.4433727 -4.3532305 -4.2383513 -4.1179972 -4.0401382 -4.0215764 -4.048492 -4.1227078 -4.207859 -4.2884135 -4.3722177 -4.4309273 -4.4481592 -4.4375706 -4.4200287][-4.39911 -4.3213754 -4.214685 -4.1022649 -4.0365615 -4.0268106 -4.0518074 -4.118197 -4.2009439 -4.2862034 -4.3762894 -4.4421816 -4.4692965 -4.468472 -4.4560089][-4.3877883 -4.34655 -4.2734561 -4.1921754 -4.1521969 -4.1551113 -4.17661 -4.2271204 -4.2945752 -4.3665061 -4.4369984 -4.4829149 -4.4987864 -4.4946637 -4.4821696][-4.4061079 -4.3890281 -4.34139 -4.2927237 -4.2858539 -4.3087254 -4.3336563 -4.372766 -4.4218512 -4.4701333 -4.5079241 -4.5213704 -4.513638 -4.4961996 -4.4776373][-4.4194098 -4.4074249 -4.373405 -4.357914 -4.3899918 -4.4375825 -4.4663253 -4.491178 -4.5178008 -4.5410457 -4.5505319 -4.5378923 -4.5108194 -4.481854 -4.4585662][-4.4001431 -4.3907952 -4.3693781 -4.3815551 -4.4385309 -4.4934964 -4.5121222 -4.517447 -4.5269718 -4.5386143 -4.5387893 -4.5196571 -4.4889441 -4.4592285 -4.4374185][-4.3688755 -4.3607221 -4.3460846 -4.3669896 -4.4227481 -4.4643183 -4.4627304 -4.45044 -4.4537849 -4.4698 -4.4778094 -4.4686441 -4.448967 -4.4300356 -4.4172544][-4.3629851 -4.353137 -4.3395715 -4.3596268 -4.4038482 -4.4264569 -4.4059954 -4.3804979 -4.3799505 -4.3979249 -4.4105325 -4.4109416 -4.4048572 -4.3994746 -4.3978319]]...]
INFO - root - 2017-12-07 05:21:38.864951: step 610, loss = 2.05, batch loss = 1.97 (11.8 examples/sec; 0.679 sec/batch; 62h:38m:25s remains)
INFO - root - 2017-12-07 05:21:45.887327: step 620, loss = 1.99, batch loss = 1.90 (11.6 examples/sec; 0.691 sec/batch; 63h:40m:54s remains)
INFO - root - 2017-12-07 05:21:52.930342: step 630, loss = 1.99, batch loss = 1.91 (11.0 examples/sec; 0.725 sec/batch; 66h:48m:31s remains)
INFO - root - 2017-12-07 05:21:59.931538: step 640, loss = 2.04, batch loss = 1.96 (11.3 examples/sec; 0.707 sec/batch; 65h:08m:20s remains)
INFO - root - 2017-12-07 05:22:06.953640: step 650, loss = 2.04, batch loss = 1.96 (11.0 examples/sec; 0.728 sec/batch; 67h:07m:44s remains)
INFO - root - 2017-12-07 05:22:13.988903: step 660, loss = 2.03, batch loss = 1.95 (12.5 examples/sec; 0.642 sec/batch; 59h:12m:04s remains)
INFO - root - 2017-12-07 05:22:20.973813: step 670, loss = 2.06, batch loss = 1.97 (11.2 examples/sec; 0.715 sec/batch; 65h:54m:04s remains)
INFO - root - 2017-12-07 05:22:27.975410: step 680, loss = 2.06, batch loss = 1.98 (11.8 examples/sec; 0.676 sec/batch; 62h:19m:22s remains)
INFO - root - 2017-12-07 05:22:35.053163: step 690, loss = 2.02, batch loss = 1.93 (11.0 examples/sec; 0.729 sec/batch; 67h:11m:21s remains)
INFO - root - 2017-12-07 05:22:42.037070: step 700, loss = 2.00, batch loss = 1.92 (11.6 examples/sec; 0.690 sec/batch; 63h:37m:56s remains)
2017-12-07 05:22:42.774763: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2999816 -4.3007751 -4.2951927 -4.2952428 -4.3011274 -4.3027196 -4.2932625 -4.2783146 -4.2660151 -4.2618046 -4.2657571 -4.2673259 -4.2412338 -4.1731882 -4.096519][-4.4070086 -4.4127007 -4.4044151 -4.3994379 -4.3989711 -4.3923545 -4.3679481 -4.3300095 -4.289959 -4.262547 -4.2506218 -4.2450352 -4.222055 -4.166131 -4.1045966][-4.4659905 -4.4735322 -4.4646225 -4.4608312 -4.4622612 -4.4553022 -4.4228482 -4.3673487 -4.30279 -4.2511926 -4.2204275 -4.2052259 -4.1884003 -4.1556025 -4.1217093][-4.4372897 -4.4325895 -4.4193006 -4.4246278 -4.4445615 -4.4548573 -4.4286137 -4.36805 -4.2945576 -4.2382488 -4.2083111 -4.1972861 -4.1900353 -4.1754327 -4.1626887][-4.3404884 -4.30979 -4.2807503 -4.2908273 -4.3348861 -4.3714976 -4.3591065 -4.3011351 -4.2326264 -4.1968236 -4.2022719 -4.2278209 -4.2436395 -4.2374659 -4.2275949][-4.230031 -4.176239 -4.1286798 -4.1331205 -4.1879539 -4.2357492 -4.2221723 -4.1565385 -4.0934424 -4.0928335 -4.1628532 -4.2576342 -4.3181858 -4.3229895 -4.3056192][-4.1445456 -4.08406 -4.0372353 -4.0454869 -4.10131 -4.1364603 -4.0948005 -3.9984024 -3.9266443 -3.9563754 -4.091598 -4.2594194 -4.3689513 -4.3897619 -4.3679652][-4.106287 -4.0583544 -4.0348244 -4.063056 -4.1207876 -4.1367068 -4.0606842 -3.9273295 -3.8358984 -3.8764057 -4.0469065 -4.2533326 -4.3862915 -4.4152508 -4.3934336][-4.1265831 -4.1048245 -4.1089749 -4.1509027 -4.2030692 -4.2030005 -4.11127 -3.9626312 -3.860666 -3.8972402 -4.0655365 -4.2641292 -4.3854675 -4.4076219 -4.3856049][-4.1835642 -4.1863518 -4.1999426 -4.2305403 -4.2622352 -4.2536116 -4.1774673 -4.0546165 -3.9681346 -3.9977591 -4.1361294 -4.2924614 -4.3784733 -4.3826437 -4.3552475][-4.2352319 -4.242599 -4.2367506 -4.2331681 -4.2372766 -4.2298193 -4.1898141 -4.1222839 -4.0775862 -4.1124978 -4.2194705 -4.3279486 -4.3740511 -4.3567238 -4.31729][-4.2471833 -4.2383933 -4.1982527 -4.15819 -4.140974 -4.1393442 -4.1341743 -4.1209126 -4.1227846 -4.1725631 -4.2610955 -4.3358078 -4.3527288 -4.3157749 -4.2608042][-4.239378 -4.2081852 -4.1447306 -4.0877914 -4.0677924 -4.0758171 -4.089365 -4.1014113 -4.1217785 -4.1706572 -4.2412548 -4.2965255 -4.302474 -4.2579851 -4.1951876][-4.2302046 -4.1886921 -4.1286621 -4.0825005 -4.0762458 -4.0943446 -4.1104364 -4.1201572 -4.1312175 -4.1598406 -4.2049985 -4.2424836 -4.2442861 -4.200346 -4.1410408][-4.2219038 -4.1889296 -4.1494584 -4.1271496 -4.1369638 -4.1607828 -4.1746306 -4.1744761 -4.1690674 -4.17526 -4.1948571 -4.2134061 -4.2093086 -4.1682787 -4.1204467]]...]
INFO - root - 2017-12-07 05:22:49.808529: step 710, loss = 2.06, batch loss = 1.98 (11.4 examples/sec; 0.704 sec/batch; 64h:51m:47s remains)
INFO - root - 2017-12-07 05:22:56.740144: step 720, loss = 2.03, batch loss = 1.95 (12.3 examples/sec; 0.648 sec/batch; 59h:43m:58s remains)
INFO - root - 2017-12-07 05:23:03.844055: step 730, loss = 1.99, batch loss = 1.91 (10.9 examples/sec; 0.732 sec/batch; 67h:30m:20s remains)
INFO - root - 2017-12-07 05:23:10.873659: step 740, loss = 2.03, batch loss = 1.94 (11.2 examples/sec; 0.714 sec/batch; 65h:46m:06s remains)
INFO - root - 2017-12-07 05:23:17.889216: step 750, loss = 2.00, batch loss = 1.92 (11.8 examples/sec; 0.679 sec/batch; 62h:31m:56s remains)
INFO - root - 2017-12-07 05:23:24.914323: step 760, loss = 2.01, batch loss = 1.93 (11.9 examples/sec; 0.670 sec/batch; 61h:42m:16s remains)
INFO - root - 2017-12-07 05:23:31.983590: step 770, loss = 1.98, batch loss = 1.90 (11.2 examples/sec; 0.713 sec/batch; 65h:44m:07s remains)
INFO - root - 2017-12-07 05:23:39.061350: step 780, loss = 2.05, batch loss = 1.96 (11.0 examples/sec; 0.725 sec/batch; 66h:49m:15s remains)
INFO - root - 2017-12-07 05:23:46.151885: step 790, loss = 2.05, batch loss = 1.97 (10.9 examples/sec; 0.735 sec/batch; 67h:43m:09s remains)
INFO - root - 2017-12-07 05:23:53.226727: step 800, loss = 2.05, batch loss = 1.96 (11.3 examples/sec; 0.707 sec/batch; 65h:10m:34s remains)
2017-12-07 05:23:54.067901: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4685268 -4.4472489 -4.43398 -4.4005179 -4.3177738 -4.2106352 -4.12169 -4.0697713 -4.0719028 -4.1344371 -4.2388906 -4.3417563 -4.3987489 -4.3883462 -4.3500719][-4.5135059 -4.4987726 -4.4871674 -4.4563556 -4.3797894 -4.2775984 -4.1867409 -4.1332784 -4.1269078 -4.1676779 -4.2468042 -4.3220525 -4.3581724 -4.3374634 -4.2863564][-4.5269713 -4.5097294 -4.4902077 -4.4518428 -4.3761725 -4.2877474 -4.2140136 -4.1761832 -4.1747403 -4.2105474 -4.286727 -4.3586421 -4.3907294 -4.3631358 -4.2891][-4.4787922 -4.4431109 -4.4056673 -4.35349 -4.275486 -4.2044611 -4.1647038 -4.1664877 -4.1958952 -4.2513356 -4.3443575 -4.4258294 -4.460453 -4.4257631 -4.3297048][-4.3789725 -4.3149362 -4.2556543 -4.1890793 -4.1023459 -4.0356922 -4.0210376 -4.0659175 -4.1443787 -4.24359 -4.3696423 -4.4657545 -4.5029817 -4.4645619 -4.3608818][-4.2840648 -4.2001667 -4.1207247 -4.0372186 -3.9296665 -3.8449268 -3.8274655 -3.894197 -4.0163755 -4.1690187 -4.3385954 -4.4563465 -4.5001149 -4.4652581 -4.3672533][-4.26855 -4.1857834 -4.0946379 -3.9913034 -3.8515997 -3.7260349 -3.6725223 -3.7233472 -3.8585393 -4.0448866 -4.2441235 -4.37289 -4.4145451 -4.378942 -4.2902217][-4.3165789 -4.2514973 -4.1606865 -4.0463042 -3.8844223 -3.7256479 -3.6352918 -3.6575181 -3.7812846 -3.9672754 -4.1613808 -4.2770247 -4.3066149 -4.2680588 -4.1839781][-4.3789477 -4.3370943 -4.2618575 -4.1619387 -4.0168195 -3.8692112 -3.775269 -3.7767284 -3.8712513 -4.0207868 -4.1733551 -4.2548161 -4.2688341 -4.2267814 -4.1379075][-4.4386339 -4.4211211 -4.3688989 -4.2951932 -4.1851821 -4.0715857 -3.989778 -3.9690094 -4.0205145 -4.1186309 -4.2203226 -4.26925 -4.2770038 -4.2386851 -4.1473622][-4.5222316 -4.5167289 -4.4803591 -4.4291515 -4.3558373 -4.2822409 -4.2209182 -4.184989 -4.1985831 -4.2542529 -4.3193088 -4.35193 -4.3582244 -4.322546 -4.2286067][-4.6166115 -4.6182685 -4.5929465 -4.5596361 -4.5166731 -4.4741125 -4.430696 -4.3911037 -4.3857765 -4.4199719 -4.4683771 -4.4970593 -4.5023656 -4.4649644 -4.3702493][-4.659687 -4.6613231 -4.6456671 -4.6287961 -4.6100292 -4.5907712 -4.5651455 -4.5347972 -4.5283084 -4.5546875 -4.596036 -4.6252017 -4.6300731 -4.5942893 -4.5083094][-4.6058755 -4.5989332 -4.5891542 -4.5863895 -4.588912 -4.5917063 -4.5869012 -4.5747175 -4.5764651 -4.5994959 -4.6324868 -4.656827 -4.6627235 -4.6407838 -4.5831451][-4.4747853 -4.4612603 -4.4525452 -4.4532886 -4.462533 -4.4756207 -4.4840021 -4.4847045 -4.4910121 -4.5090094 -4.5326567 -4.5526285 -4.5669646 -4.5733857 -4.5616536]]...]
INFO - root - 2017-12-07 05:24:01.153505: step 810, loss = 2.00, batch loss = 1.92 (11.1 examples/sec; 0.722 sec/batch; 66h:32m:04s remains)
INFO - root - 2017-12-07 05:24:08.243050: step 820, loss = 1.96, batch loss = 1.88 (11.1 examples/sec; 0.720 sec/batch; 66h:20m:00s remains)
INFO - root - 2017-12-07 05:24:15.254985: step 830, loss = 2.08, batch loss = 1.99 (11.1 examples/sec; 0.722 sec/batch; 66h:33m:36s remains)
INFO - root - 2017-12-07 05:24:22.185463: step 840, loss = 1.97, batch loss = 1.89 (11.7 examples/sec; 0.686 sec/batch; 63h:11m:15s remains)
INFO - root - 2017-12-07 05:24:29.333638: step 850, loss = 2.01, batch loss = 1.93 (11.8 examples/sec; 0.678 sec/batch; 62h:26m:06s remains)
INFO - root - 2017-12-07 05:24:36.440234: step 860, loss = 2.07, batch loss = 1.99 (10.9 examples/sec; 0.736 sec/batch; 67h:48m:30s remains)
INFO - root - 2017-12-07 05:24:43.482509: step 870, loss = 2.01, batch loss = 1.93 (10.6 examples/sec; 0.752 sec/batch; 69h:15m:00s remains)
INFO - root - 2017-12-07 05:24:50.564214: step 880, loss = 2.07, batch loss = 1.99 (10.9 examples/sec; 0.737 sec/batch; 67h:53m:39s remains)
INFO - root - 2017-12-07 05:24:57.684800: step 890, loss = 2.04, batch loss = 1.96 (10.4 examples/sec; 0.769 sec/batch; 70h:50m:16s remains)
INFO - root - 2017-12-07 05:25:04.596210: step 900, loss = 2.01, batch loss = 1.92 (11.4 examples/sec; 0.703 sec/batch; 64h:46m:36s remains)
2017-12-07 05:25:05.374913: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3805032 -4.2977438 -4.2597117 -4.2632618 -4.2958593 -4.3227572 -4.3030567 -4.2448373 -4.2015982 -4.2275252 -4.3021703 -4.3692145 -4.4055018 -4.4146705 -4.4142513][-4.3352475 -4.2471666 -4.2184606 -4.2453961 -4.2992668 -4.327601 -4.3038468 -4.257771 -4.2468615 -4.3077173 -4.3933935 -4.4350333 -4.4420476 -4.4446988 -4.4553385][-4.2629061 -4.1873875 -4.1745787 -4.2178879 -4.2721481 -4.2952094 -4.2844152 -4.2802286 -4.3195767 -4.4131527 -4.4967265 -4.4989753 -4.454699 -4.4193091 -4.4065533][-4.1751051 -4.122304 -4.1245165 -4.1719222 -4.210989 -4.2267642 -4.23947 -4.2838058 -4.369215 -4.4824266 -4.5540142 -4.5182056 -4.4209886 -4.3386827 -4.2887526][-4.117558 -4.0815368 -4.086668 -4.1166062 -4.1250486 -4.1191416 -4.1331868 -4.2026162 -4.3197927 -4.4445224 -4.5059547 -4.4518323 -4.3374615 -4.2463155 -4.1954031][-4.1105275 -4.0771646 -4.0773406 -4.0876012 -4.0681539 -4.0263696 -4.0029855 -4.0534263 -4.1752787 -4.31166 -4.3833404 -4.3430109 -4.2441077 -4.1671348 -4.1324496][-4.1431966 -4.1026745 -4.0953646 -4.0948558 -4.0564985 -3.9736848 -3.8878736 -3.8769214 -3.9743354 -4.1255131 -4.2351289 -4.2408714 -4.1759567 -4.1137609 -4.0869617][-4.2143083 -4.1647768 -4.1475706 -4.1386514 -4.0915837 -3.9862387 -3.8560519 -3.7800763 -3.8297718 -3.9774041 -4.120038 -4.1746902 -4.1487756 -4.1097293 -4.1042762][-4.2990918 -4.2444749 -4.212852 -4.1910086 -4.1422319 -4.0472112 -3.9247041 -3.8281002 -3.8380456 -3.9618213 -4.1092825 -4.1844897 -4.173224 -4.1407862 -4.1514945][-4.382338 -4.332716 -4.2866859 -4.2449837 -4.1906328 -4.1144943 -4.0246215 -3.9496846 -3.9538968 -4.061954 -4.2070746 -4.292511 -4.280417 -4.2291336 -4.2188983][-4.4401565 -4.4093122 -4.3657656 -4.3218284 -4.2781491 -4.2308011 -4.1744447 -4.12324 -4.1260557 -4.214395 -4.3471227 -4.4346843 -4.4204025 -4.3447981 -4.295651][-4.4598484 -4.452425 -4.423563 -4.3944173 -4.37987 -4.3742561 -4.353765 -4.316102 -4.3000073 -4.3473425 -4.44502 -4.51927 -4.5061669 -4.4254737 -4.3562713][-4.4789004 -4.4915404 -4.4768996 -4.4584227 -4.4645038 -4.4936476 -4.5085578 -4.4847431 -4.4481649 -4.4524813 -4.5097771 -4.5664673 -4.5606923 -4.4941239 -4.428463][-4.484817 -4.5108767 -4.5108204 -4.4999075 -4.5099239 -4.5511875 -4.5867858 -4.5765662 -4.5318427 -4.5074897 -4.5335016 -4.576735 -4.58103 -4.5369291 -4.4867797][-4.4632006 -4.4913936 -4.5041914 -4.5029984 -4.5093141 -4.5398378 -4.574368 -4.5742126 -4.5382767 -4.5063181 -4.5096478 -4.5337858 -4.5384097 -4.512331 -4.480536]]...]
INFO - root - 2017-12-07 05:25:12.471659: step 910, loss = 2.08, batch loss = 1.99 (10.7 examples/sec; 0.748 sec/batch; 68h:56m:11s remains)
INFO - root - 2017-12-07 05:25:19.571896: step 920, loss = 2.07, batch loss = 1.99 (10.7 examples/sec; 0.747 sec/batch; 68h:48m:10s remains)
INFO - root - 2017-12-07 05:25:26.773513: step 930, loss = 2.05, batch loss = 1.96 (11.0 examples/sec; 0.726 sec/batch; 66h:50m:11s remains)
INFO - root - 2017-12-07 05:25:33.774873: step 940, loss = 2.00, batch loss = 1.92 (11.9 examples/sec; 0.670 sec/batch; 61h:44m:13s remains)
INFO - root - 2017-12-07 05:25:40.749693: step 950, loss = 2.05, batch loss = 1.97 (11.9 examples/sec; 0.671 sec/batch; 61h:46m:45s remains)
INFO - root - 2017-12-07 05:25:47.900489: step 960, loss = 2.01, batch loss = 1.92 (11.1 examples/sec; 0.722 sec/batch; 66h:32m:02s remains)
INFO - root - 2017-12-07 05:25:54.938955: step 970, loss = 2.00, batch loss = 1.91 (10.9 examples/sec; 0.736 sec/batch; 67h:47m:47s remains)
INFO - root - 2017-12-07 05:26:02.001404: step 980, loss = 2.03, batch loss = 1.95 (10.8 examples/sec; 0.739 sec/batch; 68h:00m:57s remains)
INFO - root - 2017-12-07 05:26:09.216754: step 990, loss = 1.97, batch loss = 1.89 (11.3 examples/sec; 0.707 sec/batch; 65h:04m:03s remains)
INFO - root - 2017-12-07 05:26:16.211619: step 1000, loss = 1.98, batch loss = 1.90 (12.3 examples/sec; 0.648 sec/batch; 59h:39m:46s remains)
2017-12-07 05:26:16.948320: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2421927 -4.3235869 -4.4303994 -4.4985876 -4.5170908 -4.4872894 -4.4097457 -4.3737459 -4.4395304 -4.5535078 -4.6701703 -4.7437062 -4.7397046 -4.643681 -4.4868822][-4.1152687 -4.2312937 -4.3634024 -4.4451766 -4.4706068 -4.4294925 -4.3357244 -4.299376 -4.3987207 -4.5511675 -4.6851611 -4.7537622 -4.7500114 -4.6584 -4.4994245][-4.0667353 -4.2041931 -4.3462348 -4.4333334 -4.4551892 -4.3817129 -4.2446675 -4.183157 -4.2899075 -4.4714928 -4.6292138 -4.7081923 -4.709816 -4.6274409 -4.4820013][-4.1011577 -4.2326512 -4.3660917 -4.4513288 -4.4531975 -4.3232274 -4.122427 -4.0153112 -4.1085353 -4.3076353 -4.4939156 -4.6036787 -4.6316872 -4.5733633 -4.4539318][-4.2372341 -4.3328824 -4.4168906 -4.4473863 -4.38222 -4.1747775 -3.9166112 -3.7772841 -3.8680682 -4.0909777 -4.3119597 -4.4711361 -4.5509148 -4.53307 -4.4454665][-4.45199 -4.4956479 -4.4916706 -4.4168444 -4.2472563 -3.9621634 -3.6710427 -3.5329163 -3.6496508 -3.9130239 -4.1755505 -4.3877053 -4.5217128 -4.5383611 -4.4684753][-4.6090174 -4.6118355 -4.5369143 -4.3722315 -4.1225052 -3.7967119 -3.50528 -3.3903482 -3.5425539 -3.8417487 -4.1348672 -4.3787551 -4.5388775 -4.5668149 -4.4984684][-4.62953 -4.614748 -4.518538 -4.3288412 -4.0697784 -3.7676373 -3.514101 -3.4242854 -3.5804689 -3.8752141 -4.1670351 -4.4099116 -4.5617385 -4.581748 -4.5113568][-4.5868959 -4.5644312 -4.4823337 -4.323648 -4.1195674 -3.8981631 -3.7139828 -3.6431689 -3.7588675 -3.992929 -4.2370167 -4.4409904 -4.5605965 -4.5675635 -4.5029745][-4.4963751 -4.4716916 -4.4191709 -4.3228431 -4.2089128 -4.0917835 -3.9800072 -3.9146109 -3.9700003 -4.1266184 -4.3084126 -4.4614682 -4.5466356 -4.5441966 -4.4884329][-4.3635721 -4.3416944 -4.3253717 -4.3036628 -4.2861452 -4.2610583 -4.2006516 -4.1290293 -4.1350718 -4.2396121 -4.3825183 -4.5047216 -4.569663 -4.557261 -4.4953117][-4.2433815 -4.2327261 -4.2510934 -4.293025 -4.3498759 -4.3859987 -4.3571644 -4.2867541 -4.2767353 -4.3668985 -4.4979339 -4.6049314 -4.653523 -4.6212239 -4.5330062][-4.1777568 -4.1811948 -4.2205505 -4.2941275 -4.3824835 -4.4421554 -4.4304442 -4.3745422 -4.3779645 -4.4802942 -4.6142044 -4.7104259 -4.7415371 -4.6869931 -4.572907][-4.2211375 -4.2347836 -4.2767515 -4.3445387 -4.4199586 -4.4678392 -4.4570932 -4.4176383 -4.4407029 -4.5559206 -4.6905842 -4.7749319 -4.7884097 -4.7187634 -4.5940819][-4.3660536 -4.3857574 -4.419281 -4.462563 -4.5038824 -4.5219092 -4.5011082 -4.4712338 -4.5026212 -4.6121383 -4.732656 -4.7983994 -4.7924771 -4.7129931 -4.5895596]]...]
INFO - root - 2017-12-07 05:26:23.723951: step 1010, loss = 2.02, batch loss = 1.94 (11.3 examples/sec; 0.710 sec/batch; 65h:22m:57s remains)
INFO - root - 2017-12-07 05:26:30.866445: step 1020, loss = 2.00, batch loss = 1.91 (11.6 examples/sec; 0.689 sec/batch; 63h:26m:35s remains)
INFO - root - 2017-12-07 05:26:38.101014: step 1030, loss = 2.03, batch loss = 1.94 (11.9 examples/sec; 0.672 sec/batch; 61h:54m:43s remains)
INFO - root - 2017-12-07 05:26:45.210811: step 1040, loss = 2.00, batch loss = 1.92 (11.5 examples/sec; 0.698 sec/batch; 64h:17m:20s remains)
INFO - root - 2017-12-07 05:26:52.251435: step 1050, loss = 2.02, batch loss = 1.94 (11.3 examples/sec; 0.708 sec/batch; 65h:12m:15s remains)
INFO - root - 2017-12-07 05:26:59.290702: step 1060, loss = 2.02, batch loss = 1.94 (11.5 examples/sec; 0.698 sec/batch; 64h:14m:43s remains)
INFO - root - 2017-12-07 05:27:06.391678: step 1070, loss = 1.98, batch loss = 1.89 (11.9 examples/sec; 0.674 sec/batch; 62h:02m:14s remains)
INFO - root - 2017-12-07 05:27:13.424130: step 1080, loss = 2.04, batch loss = 1.96 (11.9 examples/sec; 0.672 sec/batch; 61h:51m:48s remains)
INFO - root - 2017-12-07 05:27:20.549966: step 1090, loss = 1.98, batch loss = 1.90 (11.7 examples/sec; 0.686 sec/batch; 63h:10m:03s remains)
INFO - root - 2017-12-07 05:27:27.683392: step 1100, loss = 1.98, batch loss = 1.90 (11.8 examples/sec; 0.675 sec/batch; 62h:09m:20s remains)
2017-12-07 05:27:28.452334: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4508181 -4.2924733 -4.1399584 -4.0739493 -4.0598836 -4.0793753 -4.1876564 -4.3149805 -4.40732 -4.4726009 -4.5115652 -4.5517964 -4.5847917 -4.5658774 -4.483952][-4.5072165 -4.3563285 -4.1788712 -4.0443296 -3.9489079 -3.9087312 -4.0105386 -4.1913877 -4.352788 -4.4620872 -4.5118423 -4.5447335 -4.57448 -4.5713997 -4.5047226][-4.5858569 -4.4449835 -4.24537 -4.0420976 -3.8715491 -3.787364 -3.8871732 -4.1028762 -4.3069468 -4.4402533 -4.4940076 -4.5204663 -4.5558705 -4.57522 -4.5297561][-4.6434207 -4.5314994 -4.34112 -4.1140652 -3.9009676 -3.7841344 -3.8619137 -4.0718679 -4.2815638 -4.4150891 -4.4622617 -4.4810629 -4.5211134 -4.5585957 -4.5340161][-4.638207 -4.5695939 -4.4122152 -4.2013626 -3.9729033 -3.822751 -3.8503067 -4.0103083 -4.2022381 -4.3405228 -4.3992295 -4.4324765 -4.4842582 -4.5309834 -4.518836][-4.5941539 -4.5823784 -4.4684038 -4.2853613 -4.053122 -3.861213 -3.8082592 -3.8837094 -4.0430946 -4.1994295 -4.2991605 -4.3787165 -4.4593625 -4.5132709 -4.5036349][-4.537981 -4.5903568 -4.5313072 -4.3850832 -4.1596189 -3.9248374 -3.7794909 -3.7569871 -3.8769052 -4.054389 -4.20426 -4.3387465 -4.453001 -4.5195527 -4.5108743][-4.5025434 -4.5982814 -4.5916238 -4.48823 -4.2856679 -4.0361161 -3.8296671 -3.7278998 -3.7993524 -3.9789927 -4.1616116 -4.3292718 -4.4664631 -4.5499287 -4.5465651][-4.4941621 -4.5988808 -4.6276479 -4.5633993 -4.3962173 -4.1670027 -3.9521153 -3.8132915 -3.835057 -3.9868665 -4.1663136 -4.3376617 -4.4835782 -4.5831552 -4.5892076][-4.488482 -4.5833025 -4.6346526 -4.6070471 -4.4834595 -4.2951026 -4.1065841 -3.9650612 -3.9393055 -4.0328989 -4.1742315 -4.3230867 -4.4645696 -4.5760741 -4.5955172][-4.4779582 -4.5480967 -4.6063995 -4.6079617 -4.5308223 -4.3936963 -4.2506781 -4.1273594 -4.0610571 -4.0807724 -4.1635261 -4.2795649 -4.4113245 -4.5258217 -4.5562806][-4.4688272 -4.5094781 -4.5583138 -4.5745654 -4.5384774 -4.459487 -4.3724985 -4.2800136 -4.1916122 -4.146935 -4.1678696 -4.2445149 -4.3575015 -4.4629211 -4.4973955][-4.4461203 -4.4710827 -4.5048971 -4.5210695 -4.5108213 -4.4846716 -4.4548354 -4.4015241 -4.3144832 -4.2312369 -4.203867 -4.2417812 -4.3287783 -4.4145794 -4.4427547][-4.4133677 -4.4264684 -4.4443192 -4.4521828 -4.4527979 -4.46321 -4.4766665 -4.4537187 -4.3770366 -4.27699 -4.2233453 -4.2401056 -4.312222 -4.3835282 -4.4036093][-4.3745203 -4.370873 -4.3753519 -4.3758144 -4.3781462 -4.4069495 -4.4429922 -4.4341931 -4.3656268 -4.2639446 -4.2050929 -4.2214208 -4.2965813 -4.3692293 -4.3892722]]...]
INFO - root - 2017-12-07 05:27:35.485178: step 1110, loss = 2.03, batch loss = 1.94 (12.3 examples/sec; 0.650 sec/batch; 59h:50m:27s remains)
INFO - root - 2017-12-07 05:27:42.471140: step 1120, loss = 2.09, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 64h:56m:52s remains)
INFO - root - 2017-12-07 05:27:49.503468: step 1130, loss = 2.04, batch loss = 1.96 (11.8 examples/sec; 0.679 sec/batch; 62h:27m:51s remains)
INFO - root - 2017-12-07 05:27:56.530921: step 1140, loss = 2.06, batch loss = 1.97 (12.2 examples/sec; 0.653 sec/batch; 60h:07m:58s remains)
INFO - root - 2017-12-07 05:28:03.596272: step 1150, loss = 2.05, batch loss = 1.97 (10.9 examples/sec; 0.736 sec/batch; 67h:44m:10s remains)
INFO - root - 2017-12-07 05:28:10.640033: step 1160, loss = 2.00, batch loss = 1.92 (11.1 examples/sec; 0.723 sec/batch; 66h:35m:16s remains)
INFO - root - 2017-12-07 05:28:17.497917: step 1170, loss = 2.02, batch loss = 1.94 (12.2 examples/sec; 0.655 sec/batch; 60h:19m:15s remains)
INFO - root - 2017-12-07 05:28:24.558599: step 1180, loss = 2.01, batch loss = 1.93 (11.6 examples/sec; 0.690 sec/batch; 63h:29m:19s remains)
INFO - root - 2017-12-07 05:28:31.690136: step 1190, loss = 1.99, batch loss = 1.91 (10.8 examples/sec; 0.741 sec/batch; 68h:08m:56s remains)
INFO - root - 2017-12-07 05:28:38.699754: step 1200, loss = 1.95, batch loss = 1.87 (11.1 examples/sec; 0.720 sec/batch; 66h:13m:04s remains)
2017-12-07 05:28:39.507126: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4190307 -4.4273386 -4.4621172 -4.5301781 -4.5826173 -4.5942888 -4.5734897 -4.5238886 -4.5022182 -4.5187082 -4.5535893 -4.5758424 -4.5688071 -4.5345325 -4.4760046][-4.30194 -4.3108783 -4.34736 -4.4460082 -4.5462747 -4.5863056 -4.5591264 -4.4931679 -4.4719772 -4.5063987 -4.5576925 -4.5881653 -4.5712662 -4.5115309 -4.4308925][-4.236948 -4.2376657 -4.2721524 -4.3984742 -4.5337629 -4.5752015 -4.5091977 -4.4110236 -4.395401 -4.4551091 -4.5233383 -4.5603781 -4.5380483 -4.465425 -4.3733344][-4.2137747 -4.2084088 -4.2530251 -4.3956332 -4.5263968 -4.5229526 -4.3887682 -4.2487674 -4.2443252 -4.3395395 -4.4389076 -4.4912853 -4.4756727 -4.4105115 -4.3208447][-4.1902647 -4.1818061 -4.2472491 -4.38786 -4.4762149 -4.4057007 -4.2067928 -4.0325522 -4.0417891 -4.1759949 -4.3175049 -4.40042 -4.4049039 -4.3615804 -4.28658][-4.1596951 -4.1658478 -4.2450266 -4.3583045 -4.3819408 -4.2490382 -4.013576 -3.8202424 -3.8361032 -4.0035205 -4.1851621 -4.3029366 -4.332736 -4.3145905 -4.2614989][-4.159327 -4.1929479 -4.27218 -4.3381486 -4.2921553 -4.108871 -3.8604639 -3.6655619 -3.6856313 -3.8728087 -4.0801005 -4.2245235 -4.2776833 -4.2777066 -4.2461638][-4.1887755 -4.2527981 -4.3207068 -4.3403296 -4.2471347 -4.040844 -3.8041611 -3.6310387 -3.6626334 -3.8605227 -4.0776281 -4.2288685 -4.2857113 -4.2828217 -4.2618442][-4.2219782 -4.3166628 -4.3833327 -4.3820367 -4.2794633 -4.0906973 -3.8979754 -3.7696872 -3.8091574 -3.9832482 -4.1709051 -4.2984681 -4.3404255 -4.3250761 -4.3095808][-4.2805333 -4.4022331 -4.4770074 -4.4740639 -4.3858452 -4.241221 -4.1124535 -4.0333066 -4.0658813 -4.1865096 -4.3111744 -4.3926411 -4.4142756 -4.3943357 -4.38784][-4.3609262 -4.4931579 -4.5737286 -4.5754828 -4.5056596 -4.4082913 -4.3437982 -4.3159714 -4.3461313 -4.411046 -4.4622612 -4.4869685 -4.4824147 -4.464251 -4.4687538][-4.4564652 -4.5754547 -4.6497788 -4.651701 -4.5956607 -4.5295806 -4.5083189 -4.52112 -4.5490284 -4.5602789 -4.5394616 -4.5114241 -4.4876604 -4.4811769 -4.4997988][-4.5411048 -4.6215715 -4.6721478 -4.6657543 -4.6141315 -4.5610514 -4.5515423 -4.5776186 -4.59778 -4.5736442 -4.5123405 -4.4619074 -4.43739 -4.4460254 -4.4721413][-4.5469866 -4.5744214 -4.5968485 -4.58856 -4.5498734 -4.5078559 -4.4930887 -4.5121365 -4.5258479 -4.4966674 -4.437201 -4.394381 -4.3770814 -4.3937731 -4.41974][-4.4609466 -4.4526258 -4.4619746 -4.4691629 -4.4578519 -4.4351006 -4.4152884 -4.4178414 -4.4235034 -4.4017653 -4.3602142 -4.3332405 -4.3275814 -4.3533592 -4.3835535]]...]
INFO - root - 2017-12-07 05:28:46.595224: step 1210, loss = 2.03, batch loss = 1.95 (11.5 examples/sec; 0.695 sec/batch; 63h:57m:42s remains)
INFO - root - 2017-12-07 05:28:53.660816: step 1220, loss = 2.00, batch loss = 1.91 (10.8 examples/sec; 0.737 sec/batch; 67h:51m:33s remains)
INFO - root - 2017-12-07 05:29:00.728430: step 1230, loss = 1.97, batch loss = 1.89 (10.8 examples/sec; 0.744 sec/batch; 68h:27m:08s remains)
INFO - root - 2017-12-07 05:29:07.727143: step 1240, loss = 2.04, batch loss = 1.95 (11.2 examples/sec; 0.713 sec/batch; 65h:37m:08s remains)
INFO - root - 2017-12-07 05:29:14.791892: step 1250, loss = 2.08, batch loss = 1.99 (11.3 examples/sec; 0.711 sec/batch; 65h:23m:48s remains)
INFO - root - 2017-12-07 05:29:21.853012: step 1260, loss = 2.01, batch loss = 1.92 (11.8 examples/sec; 0.676 sec/batch; 62h:11m:21s remains)
INFO - root - 2017-12-07 05:29:28.875690: step 1270, loss = 2.06, batch loss = 1.98 (10.9 examples/sec; 0.737 sec/batch; 67h:50m:17s remains)
INFO - root - 2017-12-07 05:29:36.019507: step 1280, loss = 2.07, batch loss = 1.99 (11.3 examples/sec; 0.709 sec/batch; 65h:12m:29s remains)
INFO - root - 2017-12-07 05:29:43.037517: step 1290, loss = 2.03, batch loss = 1.95 (10.9 examples/sec; 0.733 sec/batch; 67h:24m:57s remains)
INFO - root - 2017-12-07 05:29:50.045139: step 1300, loss = 2.03, batch loss = 1.95 (11.5 examples/sec; 0.693 sec/batch; 63h:43m:45s remains)
2017-12-07 05:29:50.802106: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4587612 -4.4476333 -4.3895431 -4.2937016 -4.1883707 -4.0920844 -4.0227585 -4.0102415 -4.0353107 -4.1236258 -4.2592053 -4.368886 -4.4819689 -4.5474186 -4.4897943][-4.4937968 -4.4797354 -4.4346194 -4.3717804 -4.3128662 -4.2558608 -4.1959281 -4.176939 -4.1973858 -4.2702837 -4.3910775 -4.5015531 -4.6031666 -4.6611509 -4.6104169][-4.4944725 -4.4671121 -4.433404 -4.406558 -4.3999653 -4.3878913 -4.3472137 -4.3273683 -4.3432574 -4.4019251 -4.502737 -4.6074758 -4.69113 -4.7268267 -4.6831446][-4.4508176 -4.4068718 -4.3838305 -4.3831 -4.4068618 -4.4099007 -4.3652797 -4.3298507 -4.3397207 -4.4047093 -4.5033274 -4.6090989 -4.6774716 -4.686739 -4.6465993][-4.3487444 -4.292531 -4.2871995 -4.3059855 -4.3338947 -4.3173194 -4.2377057 -4.1672568 -4.173923 -4.2764611 -4.4088879 -4.5339351 -4.6006713 -4.593133 -4.5551543][-4.2129965 -4.1470718 -4.1523347 -4.1711273 -4.1821761 -4.1393571 -4.0306358 -3.9353287 -3.9494822 -4.098886 -4.2725153 -4.4143381 -4.4841933 -4.4721365 -4.4444671][-4.1183834 -4.0426211 -4.0322723 -4.0177107 -3.9922264 -3.9223094 -3.7993684 -3.7009013 -3.73432 -3.9214282 -4.115243 -4.25503 -4.3217053 -4.3166509 -4.3097491][-4.1197171 -4.0427423 -4.0027223 -3.9417167 -3.8741636 -3.7827117 -3.6610165 -3.5776093 -3.6339467 -3.830225 -4.0100918 -4.1215682 -4.1694651 -4.1695781 -4.183239][-4.1717458 -4.104547 -4.0429788 -3.9540377 -3.8631356 -3.7680323 -3.6676674 -3.6122918 -3.6845505 -3.8577824 -3.9984438 -4.0671291 -4.0826917 -4.0770164 -4.0985641][-4.22454 -4.1755757 -4.1160283 -4.0318508 -3.9449868 -3.8649774 -3.7964354 -3.767117 -3.8348625 -3.9613183 -4.0591793 -4.0990958 -4.09667 -4.0868278 -4.1006188][-4.2646689 -4.2422881 -4.2057037 -4.1563845 -4.1077132 -4.071372 -4.0442863 -4.0302181 -4.0673141 -4.1293044 -4.1812034 -4.2015648 -4.1909723 -4.17808 -4.1774921][-4.2999353 -4.2988567 -4.2857847 -4.2778339 -4.284224 -4.3074393 -4.3204327 -4.3148432 -4.3192306 -4.3294897 -4.3436069 -4.3463931 -4.3273382 -4.307476 -4.2905498][-4.3208237 -4.3202219 -4.3091969 -4.3158479 -4.356164 -4.4212985 -4.4627886 -4.4694467 -4.4655976 -4.4613585 -4.4670372 -4.47197 -4.4607906 -4.4436245 -4.4214745][-4.3182406 -4.3027725 -4.2721176 -4.2622705 -4.2987123 -4.3713489 -4.42429 -4.4454556 -4.4525452 -4.4604926 -4.4813991 -4.5072241 -4.5180573 -4.5180249 -4.5128684][-4.3122878 -4.28099 -4.227632 -4.1910057 -4.2026186 -4.2558136 -4.3022919 -4.32869 -4.3454366 -4.3645563 -4.3961415 -4.4339371 -4.4599152 -4.4775639 -4.4963212]]...]
INFO - root - 2017-12-07 05:29:57.908547: step 1310, loss = 2.05, batch loss = 1.96 (11.4 examples/sec; 0.699 sec/batch; 64h:21m:02s remains)
INFO - root - 2017-12-07 05:30:05.024184: step 1320, loss = 2.00, batch loss = 1.92 (11.9 examples/sec; 0.671 sec/batch; 61h:44m:27s remains)
INFO - root - 2017-12-07 05:30:11.987991: step 1330, loss = 2.06, batch loss = 1.98 (11.2 examples/sec; 0.712 sec/batch; 65h:30m:35s remains)
INFO - root - 2017-12-07 05:30:18.953577: step 1340, loss = 2.00, batch loss = 1.92 (10.7 examples/sec; 0.747 sec/batch; 68h:45m:08s remains)
INFO - root - 2017-12-07 05:30:26.060961: step 1350, loss = 2.03, batch loss = 1.95 (10.9 examples/sec; 0.732 sec/batch; 67h:22m:17s remains)
INFO - root - 2017-12-07 05:30:33.170160: step 1360, loss = 2.01, batch loss = 1.93 (11.4 examples/sec; 0.704 sec/batch; 64h:46m:57s remains)
INFO - root - 2017-12-07 05:30:40.254351: step 1370, loss = 1.97, batch loss = 1.89 (12.2 examples/sec; 0.655 sec/batch; 60h:14m:07s remains)
INFO - root - 2017-12-07 05:30:47.364071: step 1380, loss = 2.00, batch loss = 1.92 (11.4 examples/sec; 0.700 sec/batch; 64h:23m:59s remains)
INFO - root - 2017-12-07 05:30:54.423228: step 1390, loss = 1.96, batch loss = 1.88 (11.0 examples/sec; 0.730 sec/batch; 67h:07m:55s remains)
INFO - root - 2017-12-07 05:31:01.479927: step 1400, loss = 1.98, batch loss = 1.89 (11.2 examples/sec; 0.712 sec/batch; 65h:27m:18s remains)
2017-12-07 05:31:02.187100: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3339763 -4.3558588 -4.4038472 -4.4387589 -4.4401722 -4.4099035 -4.3602643 -4.2825861 -4.2249765 -4.2113395 -4.231916 -4.2411828 -4.217526 -4.2020426 -4.2155495][-4.3068676 -4.3493314 -4.4153962 -4.4687591 -4.4803252 -4.4485435 -4.3927231 -4.3004651 -4.2312808 -4.2185655 -4.2528992 -4.2765961 -4.2518353 -4.2116747 -4.191958][-4.3239012 -4.3841071 -4.4558148 -4.5108633 -4.5195208 -4.4842172 -4.4289227 -4.3396821 -4.2796879 -4.2772646 -4.3213181 -4.3487978 -4.3195052 -4.2601452 -4.2087488][-4.361721 -4.429553 -4.4895635 -4.5216403 -4.502409 -4.4489822 -4.3934631 -4.3263373 -4.3013053 -4.3271 -4.3809967 -4.4068494 -4.3742032 -4.3067522 -4.2373176][-4.3744311 -4.4376974 -4.4754119 -4.4659677 -4.3940077 -4.2947559 -4.2185583 -4.1785512 -4.2067957 -4.2828026 -4.3634391 -4.4047532 -4.386488 -4.3330269 -4.2668304][-4.3396339 -4.396915 -4.4091578 -4.3463364 -4.2026463 -4.0307317 -3.9122205 -3.8959906 -3.992631 -4.1433916 -4.2744942 -4.3524561 -4.3657422 -4.3389349 -4.289362][-4.3118405 -4.3673897 -4.3535614 -4.2293749 -4.0019197 -3.7432134 -3.5741191 -3.5787611 -3.7522826 -3.9999211 -4.2035236 -4.32745 -4.3683972 -4.357 -4.3167663][-4.3434381 -4.4028258 -4.3732743 -4.1991696 -3.9005384 -3.5633287 -3.3502274 -3.37402 -3.6148806 -3.9486952 -4.2122941 -4.3657708 -4.4162569 -4.4030252 -4.3614721][-4.4390488 -4.4987073 -4.4689989 -4.2824674 -3.9610014 -3.5879683 -3.3578744 -3.4051898 -3.6903582 -4.0669117 -4.3427095 -4.4819417 -4.5087113 -4.4724874 -4.423152][-4.569788 -4.6216545 -4.597301 -4.434248 -4.1460075 -3.7973416 -3.5907431 -3.6593869 -3.9530535 -4.3147993 -4.5478406 -4.6300197 -4.6023817 -4.525351 -4.4593425][-4.6773872 -4.7188592 -4.6975989 -4.5712371 -4.3488784 -4.0677185 -3.9009824 -3.9631505 -4.2067809 -4.4956665 -4.6625471 -4.6938915 -4.6256919 -4.5179696 -4.4386187][-4.7284822 -4.771523 -4.7643032 -4.685019 -4.5375309 -4.3361444 -4.2013683 -4.2201524 -4.3604474 -4.5310197 -4.6183767 -4.6172662 -4.5484538 -4.4499516 -4.3818159][-4.7191772 -4.7693338 -4.7812366 -4.7485404 -4.6707416 -4.5447397 -4.4358587 -4.3971753 -4.4166551 -4.4536762 -4.4574814 -4.4393764 -4.4081469 -4.3640647 -4.3412542][-4.66534 -4.7171907 -4.7419395 -4.7381239 -4.7052479 -4.6344986 -4.5451369 -4.4615378 -4.3803682 -4.3076434 -4.2491508 -4.2345614 -4.264945 -4.3018026 -4.3479142][-4.5772824 -4.6300831 -4.668 -4.6815834 -4.6704993 -4.6304307 -4.556181 -4.4532266 -4.31876 -4.1899195 -4.1075211 -4.109303 -4.1875224 -4.2865572 -4.386086]]...]
INFO - root - 2017-12-07 05:31:09.275162: step 1410, loss = 2.04, batch loss = 1.96 (10.7 examples/sec; 0.749 sec/batch; 68h:54m:58s remains)
INFO - root - 2017-12-07 05:31:16.231445: step 1420, loss = 2.03, batch loss = 1.94 (10.8 examples/sec; 0.740 sec/batch; 68h:02m:31s remains)
INFO - root - 2017-12-07 05:31:23.321816: step 1430, loss = 2.05, batch loss = 1.97 (10.6 examples/sec; 0.752 sec/batch; 69h:07m:59s remains)
INFO - root - 2017-12-07 05:31:30.343526: step 1440, loss = 2.00, batch loss = 1.92 (11.5 examples/sec; 0.694 sec/batch; 63h:51m:11s remains)
INFO - root - 2017-12-07 05:31:37.466683: step 1450, loss = 2.02, batch loss = 1.94 (12.1 examples/sec; 0.660 sec/batch; 60h:38m:58s remains)
INFO - root - 2017-12-07 05:31:44.478513: step 1460, loss = 2.05, batch loss = 1.97 (12.4 examples/sec; 0.646 sec/batch; 59h:23m:17s remains)
INFO - root - 2017-12-07 05:31:51.505496: step 1470, loss = 2.03, batch loss = 1.95 (11.5 examples/sec; 0.699 sec/batch; 64h:14m:06s remains)
INFO - root - 2017-12-07 05:31:58.587233: step 1480, loss = 2.03, batch loss = 1.94 (12.3 examples/sec; 0.652 sec/batch; 59h:55m:10s remains)
INFO - root - 2017-12-07 05:32:05.652050: step 1490, loss = 2.05, batch loss = 1.97 (11.6 examples/sec; 0.690 sec/batch; 63h:24m:13s remains)
INFO - root - 2017-12-07 05:32:12.487695: step 1500, loss = 2.02, batch loss = 1.94 (11.5 examples/sec; 0.694 sec/batch; 63h:50m:51s remains)
2017-12-07 05:32:13.301291: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4445171 -4.4460268 -4.4972711 -4.5483465 -4.5717711 -4.5929379 -4.6363764 -4.6665187 -4.6248941 -4.5452213 -4.5182848 -4.566525 -4.6225824 -4.6336713 -4.5629272][-4.2412624 -4.2874351 -4.3769031 -4.4358878 -4.4450321 -4.4547844 -4.5018325 -4.5571251 -4.5493293 -4.4890246 -4.4558406 -4.48776 -4.538322 -4.561101 -4.5120673][-4.0442452 -4.143137 -4.2762403 -4.3389316 -4.3227181 -4.3059168 -4.3295422 -4.377522 -4.3927007 -4.3762231 -4.370121 -4.3973718 -4.4406295 -4.473485 -4.4489164][-4.020689 -4.1299238 -4.2653222 -4.3107634 -4.2610116 -4.2119713 -4.2025352 -4.2245641 -4.2466435 -4.2744803 -4.3156681 -4.363647 -4.4093666 -4.4499731 -4.4410129][-4.1771059 -4.2534304 -4.3468046 -4.3424854 -4.2452712 -4.159636 -4.1219687 -4.1226158 -4.1467266 -4.2127137 -4.3152485 -4.4124455 -4.482512 -4.5304775 -4.5231862][-4.3297997 -4.3728576 -4.4213443 -4.3673162 -4.2225595 -4.1014991 -4.0407338 -4.0232167 -4.0395284 -4.1206827 -4.2693133 -4.43052 -4.55211 -4.622375 -4.6191192][-4.3844986 -4.4026861 -4.4167571 -4.3302526 -4.1502862 -4.0007739 -3.9232881 -3.9008074 -3.9135866 -3.9856164 -4.1398425 -4.3370233 -4.5130119 -4.6246691 -4.6417003][-4.3302264 -4.3469229 -4.3463178 -4.2453108 -4.0467935 -3.8784976 -3.7941041 -3.7805812 -3.793607 -3.840137 -3.9605706 -4.1512551 -4.3598886 -4.5124421 -4.5583029][-4.2505822 -4.2869892 -4.300561 -4.2120266 -4.02089 -3.854167 -3.7797186 -3.7802534 -3.7860842 -3.7911267 -3.8548727 -4.0089602 -4.2168531 -4.3875241 -4.4493632][-4.2220297 -4.271944 -4.3136921 -4.2629857 -4.1084938 -3.9683189 -3.9254894 -3.9475622 -3.9443552 -3.9129171 -3.9291422 -4.0455585 -4.22972 -4.3924537 -4.4473605][-4.2189059 -4.2630115 -4.3364635 -4.3460255 -4.2510519 -4.1503263 -4.1431956 -4.1929531 -4.1930189 -4.1436243 -4.1321707 -4.2176657 -4.3662577 -4.4982343 -4.5316262][-4.2507396 -4.2767882 -4.3669381 -4.4275517 -4.3948846 -4.3351369 -4.3492837 -4.4136319 -4.4223957 -4.3675556 -4.3350935 -4.3892722 -4.499454 -4.5927935 -4.6006255][-4.3319449 -4.3355341 -4.4116926 -4.4891582 -4.5011978 -4.48 -4.5026956 -4.5655069 -4.5826921 -4.5321765 -4.4827819 -4.5056052 -4.5821629 -4.6444941 -4.6310678][-4.4259639 -4.4144273 -4.4589195 -4.5195045 -4.5485396 -4.5555725 -4.5829463 -4.6358957 -4.6569295 -4.6184216 -4.5669131 -4.5675955 -4.6187286 -4.6612582 -4.6367617][-4.4795785 -4.4672017 -4.489418 -4.5280643 -4.5585361 -4.5790591 -4.6052704 -4.6430717 -4.6620154 -4.6414294 -4.6053386 -4.5976686 -4.6239161 -4.643446 -4.6125431]]...]
INFO - root - 2017-12-07 05:32:20.327760: step 1510, loss = 2.02, batch loss = 1.94 (12.1 examples/sec; 0.663 sec/batch; 60h:56m:48s remains)
INFO - root - 2017-12-07 05:32:27.448550: step 1520, loss = 2.02, batch loss = 1.94 (11.8 examples/sec; 0.680 sec/batch; 62h:33m:50s remains)
INFO - root - 2017-12-07 05:32:34.270699: step 1530, loss = 2.01, batch loss = 1.93 (12.0 examples/sec; 0.667 sec/batch; 61h:17m:33s remains)
INFO - root - 2017-12-07 05:32:41.347231: step 1540, loss = 2.01, batch loss = 1.92 (11.7 examples/sec; 0.686 sec/batch; 63h:01m:16s remains)
INFO - root - 2017-12-07 05:32:48.477300: step 1550, loss = 2.02, batch loss = 1.93 (11.6 examples/sec; 0.690 sec/batch; 63h:28m:22s remains)
INFO - root - 2017-12-07 05:32:55.555629: step 1560, loss = 1.96, batch loss = 1.87 (10.6 examples/sec; 0.751 sec/batch; 69h:03m:32s remains)
INFO - root - 2017-12-07 05:33:02.614588: step 1570, loss = 2.04, batch loss = 1.96 (11.3 examples/sec; 0.708 sec/batch; 65h:04m:26s remains)
INFO - root - 2017-12-07 05:33:09.635761: step 1580, loss = 2.06, batch loss = 1.98 (11.0 examples/sec; 0.728 sec/batch; 66h:57m:10s remains)
INFO - root - 2017-12-07 05:33:16.678977: step 1590, loss = 2.03, batch loss = 1.95 (11.4 examples/sec; 0.704 sec/batch; 64h:39m:59s remains)
INFO - root - 2017-12-07 05:33:23.681371: step 1600, loss = 2.04, batch loss = 1.96 (11.6 examples/sec; 0.690 sec/batch; 63h:26m:54s remains)
2017-12-07 05:33:24.421178: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3973694 -4.3890586 -4.3954611 -4.4009891 -4.390029 -4.3782296 -4.3792405 -4.4043431 -4.4424076 -4.4800377 -4.4997921 -4.4980941 -4.4555135 -4.3842916 -4.339849][-4.3911467 -4.3879137 -4.406405 -4.4381266 -4.4569273 -4.4562526 -4.4586077 -4.4816337 -4.5125718 -4.5247207 -4.4967175 -4.451292 -4.38249 -4.3045049 -4.260108][-4.3893681 -4.3915453 -4.4179015 -4.4676695 -4.502841 -4.5033612 -4.5001588 -4.5146208 -4.53252 -4.5233541 -4.46641 -4.3967848 -4.3150158 -4.23412 -4.1863637][-4.3717642 -4.3943033 -4.43268 -4.4825287 -4.4993753 -4.4660435 -4.4307609 -4.429832 -4.4520655 -4.4538417 -4.4038019 -4.3309784 -4.2396579 -4.150229 -4.0971084][-4.31424 -4.3580718 -4.4067035 -4.4405193 -4.4131527 -4.3189015 -4.2330413 -4.2230144 -4.2787442 -4.3252897 -4.3037858 -4.2333426 -4.1319838 -4.035357 -3.9833472][-4.2256589 -4.2756696 -4.3244076 -4.3406048 -4.2790318 -4.1364365 -4.0079155 -3.9978855 -4.0982723 -4.1990728 -4.2080307 -4.1355309 -4.0240378 -3.9322305 -3.8922758][-4.1731558 -4.2052732 -4.2349405 -4.2334261 -4.158865 -3.9966009 -3.840302 -3.8260875 -3.95897 -4.1078992 -4.1510868 -4.0888124 -3.9795773 -3.8994386 -3.8717115][-4.1791806 -4.1785889 -4.1737876 -4.15058 -4.0797911 -3.9312015 -3.7741094 -3.7520797 -3.8905668 -4.0640073 -4.1409125 -4.1098814 -4.0227909 -3.9585323 -3.9356096][-4.2286863 -4.2012281 -4.1660376 -4.126976 -4.06856 -3.9569719 -3.8351066 -3.819078 -3.9404531 -4.105937 -4.198019 -4.1972613 -4.1364303 -4.0789909 -4.04543][-4.2970915 -4.2548494 -4.1992188 -4.1497564 -4.100687 -4.0249562 -3.9501712 -3.9550462 -4.0613503 -4.20261 -4.2901731 -4.3043184 -4.2593765 -4.1942077 -4.139739][-4.3512464 -4.3065434 -4.2460361 -4.2013845 -4.1686468 -4.124527 -4.0876594 -4.1081405 -4.1952653 -4.3024406 -4.37104 -4.3839903 -4.3410883 -4.2609544 -4.187542][-4.3820434 -4.3456264 -4.2962775 -4.2717214 -4.2632132 -4.2463503 -4.2369881 -4.2666841 -4.3360581 -4.407486 -4.4475803 -4.4459209 -4.3915915 -4.2948866 -4.2086511][-4.3701162 -4.3470974 -4.3180156 -4.3159704 -4.3298478 -4.335053 -4.347486 -4.3891125 -4.4503779 -4.4932594 -4.5038033 -4.4822073 -4.4180312 -4.3173842 -4.22868][-4.326323 -4.313848 -4.3036466 -4.3214397 -4.3533816 -4.37387 -4.4005222 -4.4508328 -4.5061922 -4.5283237 -4.5147681 -4.4743943 -4.4085064 -4.321928 -4.2448907][-4.3008037 -4.2798672 -4.2666764 -4.2913828 -4.337719 -4.3703628 -4.4029675 -4.4525867 -4.5009284 -4.5123 -4.4857368 -4.4327745 -4.3686118 -4.3054428 -4.2549443]]...]
INFO - root - 2017-12-07 05:33:31.494614: step 1610, loss = 2.04, batch loss = 1.96 (10.9 examples/sec; 0.735 sec/batch; 67h:32m:21s remains)
INFO - root - 2017-12-07 05:33:38.605082: step 1620, loss = 2.02, batch loss = 1.94 (10.7 examples/sec; 0.749 sec/batch; 68h:48m:10s remains)
INFO - root - 2017-12-07 05:33:45.529233: step 1630, loss = 2.01, batch loss = 1.93 (11.6 examples/sec; 0.692 sec/batch; 63h:37m:32s remains)
INFO - root - 2017-12-07 05:33:52.553942: step 1640, loss = 2.01, batch loss = 1.93 (12.0 examples/sec; 0.667 sec/batch; 61h:18m:13s remains)
INFO - root - 2017-12-07 05:33:59.544207: step 1650, loss = 2.00, batch loss = 1.92 (10.9 examples/sec; 0.731 sec/batch; 67h:09m:12s remains)
INFO - root - 2017-12-07 05:34:06.669608: step 1660, loss = 2.01, batch loss = 1.92 (11.0 examples/sec; 0.725 sec/batch; 66h:34m:59s remains)
INFO - root - 2017-12-07 05:34:13.464589: step 1670, loss = 2.00, batch loss = 1.91 (11.4 examples/sec; 0.702 sec/batch; 64h:30m:24s remains)
INFO - root - 2017-12-07 05:34:20.522901: step 1680, loss = 2.04, batch loss = 1.96 (12.0 examples/sec; 0.668 sec/batch; 61h:24m:25s remains)
INFO - root - 2017-12-07 05:34:27.557920: step 1690, loss = 2.02, batch loss = 1.94 (11.5 examples/sec; 0.697 sec/batch; 64h:02m:45s remains)
INFO - root - 2017-12-07 05:34:34.740094: step 1700, loss = 2.03, batch loss = 1.95 (11.0 examples/sec; 0.726 sec/batch; 66h:43m:35s remains)
2017-12-07 05:34:35.464179: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9275579 -3.8052161 -3.8711033 -4.0822124 -4.3498487 -4.5556645 -4.6049223 -4.5346646 -4.4548163 -4.4522533 -4.5032487 -4.5193486 -4.4766006 -4.4078407 -4.3702984][-3.9366312 -3.8379476 -3.8874445 -4.0783768 -4.338531 -4.5175514 -4.5290985 -4.4545279 -4.4211173 -4.4816523 -4.5750732 -4.603385 -4.5490484 -4.4651294 -4.4241667][-4.0084805 -3.9640737 -4.01819 -4.1858292 -4.3908858 -4.4696875 -4.3853817 -4.2875443 -4.3098097 -4.4535766 -4.6103539 -4.6670089 -4.614131 -4.523757 -4.4847317][-4.08474 -4.103754 -4.1859012 -4.3369346 -4.4510508 -4.3811517 -4.1712551 -4.044168 -4.1233273 -4.354147 -4.5832448 -4.6750731 -4.6311026 -4.5391164 -4.5074968][-4.1400313 -4.2101717 -4.3121514 -4.4172115 -4.4087229 -4.1955428 -3.8941998 -3.7653728 -3.9029531 -4.2099295 -4.4976149 -4.6169338 -4.5767765 -4.4812021 -4.4626217][-4.1949072 -4.2824483 -4.3588915 -4.3711138 -4.2349658 -3.9296632 -3.6102571 -3.5265987 -3.7306223 -4.0917645 -4.407372 -4.5283766 -4.4729061 -4.369535 -4.3699689][-4.2685647 -4.3332515 -4.3417439 -4.2490129 -4.0266056 -3.7043197 -3.4324844 -3.4246213 -3.6889057 -4.0726075 -4.3710284 -4.4532657 -4.3597331 -4.2517233 -4.2785754][-4.3380966 -4.3641877 -4.3118448 -4.157207 -3.9161611 -3.6390276 -3.4560478 -3.521107 -3.8090625 -4.1656241 -4.3999796 -4.4089651 -4.2680483 -4.16296 -4.2168984][-4.3631921 -4.3676 -4.2999392 -4.1492496 -3.9463398 -3.7458651 -3.6537566 -3.7605913 -4.0252318 -4.3149524 -4.4668412 -4.4107351 -4.2435465 -4.1470613 -4.2134552][-4.3638139 -4.3687654 -4.3218279 -4.2159157 -4.0772219 -3.9551744 -3.9266455 -4.0363369 -4.2443829 -4.452724 -4.5406203 -4.4625621 -4.3054538 -4.2158928 -4.2643814][-4.3974519 -4.4077458 -4.3854432 -4.3285437 -4.2552581 -4.2019873 -4.2111387 -4.2986569 -4.4366426 -4.5665116 -4.6123891 -4.539825 -4.408421 -4.3179803 -4.3293009][-4.4881077 -4.4953079 -4.4853935 -4.4634247 -4.4380989 -4.4294124 -4.4529719 -4.5095921 -4.5839648 -4.649776 -4.6666803 -4.6007 -4.4854956 -4.3877363 -4.3662953][-4.578155 -4.5797458 -4.5809402 -4.5823941 -4.5797386 -4.5809937 -4.5916805 -4.6130881 -4.6383681 -4.6573625 -4.6512074 -4.5902591 -4.4935 -4.405386 -4.376451][-4.5709171 -4.5740771 -4.5852871 -4.5974679 -4.6002626 -4.59741 -4.5924983 -4.5872278 -4.5782957 -4.5648775 -4.5428414 -4.4916048 -4.421978 -4.3629971 -4.3487167][-4.4711533 -4.4759994 -4.4892139 -4.5003138 -4.5011082 -4.4953885 -4.4855156 -4.4705253 -4.4489388 -4.4259939 -4.4041934 -4.371531 -4.3339205 -4.306273 -4.3062949]]...]
INFO - root - 2017-12-07 05:34:42.367822: step 1710, loss = 2.04, batch loss = 1.96 (12.5 examples/sec; 0.638 sec/batch; 58h:36m:35s remains)
INFO - root - 2017-12-07 05:34:49.440000: step 1720, loss = 2.04, batch loss = 1.95 (12.3 examples/sec; 0.649 sec/batch; 59h:39m:47s remains)
INFO - root - 2017-12-07 05:34:56.452205: step 1730, loss = 2.00, batch loss = 1.92 (11.6 examples/sec; 0.691 sec/batch; 63h:30m:42s remains)
INFO - root - 2017-12-07 05:35:03.495379: step 1740, loss = 2.05, batch loss = 1.96 (11.3 examples/sec; 0.706 sec/batch; 64h:54m:17s remains)
INFO - root - 2017-12-07 05:35:10.554761: step 1750, loss = 2.06, batch loss = 1.97 (11.3 examples/sec; 0.709 sec/batch; 65h:07m:48s remains)
INFO - root - 2017-12-07 05:35:17.579591: step 1760, loss = 2.03, batch loss = 1.94 (10.2 examples/sec; 0.784 sec/batch; 72h:00m:01s remains)
INFO - root - 2017-12-07 05:35:24.597572: step 1770, loss = 2.01, batch loss = 1.92 (10.9 examples/sec; 0.735 sec/batch; 67h:33m:01s remains)
INFO - root - 2017-12-07 05:35:31.573391: step 1780, loss = 2.05, batch loss = 1.97 (11.7 examples/sec; 0.686 sec/batch; 63h:03m:18s remains)
INFO - root - 2017-12-07 05:35:38.583994: step 1790, loss = 2.01, batch loss = 1.93 (11.5 examples/sec; 0.693 sec/batch; 63h:41m:22s remains)
INFO - root - 2017-12-07 05:35:45.655402: step 1800, loss = 2.01, batch loss = 1.93 (11.4 examples/sec; 0.703 sec/batch; 64h:34m:38s remains)
2017-12-07 05:35:46.418935: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5482635 -4.5900784 -4.5719032 -4.5430026 -4.525033 -4.5273228 -4.5639009 -4.6098981 -4.6392379 -4.6349111 -4.6191745 -4.6108279 -4.6007991 -4.5835161 -4.5528655][-4.4024949 -4.4219284 -4.4028535 -4.3941965 -4.4124885 -4.4418974 -4.4809136 -4.519825 -4.5422168 -4.5352697 -4.522769 -4.5183382 -4.5091844 -4.4914093 -4.4594479][-4.3103008 -4.3053117 -4.2812572 -4.2856274 -4.3327465 -4.3884015 -4.4277849 -4.4506445 -4.4503846 -4.4241252 -4.406158 -4.4171882 -4.432199 -4.4281979 -4.4063926][-4.28157 -4.2638621 -4.2313323 -4.2360129 -4.2897243 -4.3520393 -4.3851423 -4.3989606 -4.3867316 -4.3381362 -4.3049932 -4.3303208 -4.3791556 -4.3981843 -4.3893414][-4.3445153 -4.3385005 -4.3061609 -4.2982249 -4.3278446 -4.3607178 -4.3700514 -4.3788047 -4.3755531 -4.325366 -4.2726173 -4.2872934 -4.3422718 -4.3726897 -4.3752251][-4.4476891 -4.4629078 -4.4429832 -4.4289274 -4.4278154 -4.4161234 -4.3902445 -4.3867683 -4.4024048 -4.3816118 -4.3280129 -4.3064504 -4.3156924 -4.3145852 -4.3120613][-4.4462423 -4.487277 -4.4978704 -4.4905777 -4.4579091 -4.3983188 -4.3323326 -4.2995462 -4.3216166 -4.3544164 -4.345356 -4.2989526 -4.2353115 -4.1683912 -4.1441092][-4.3149281 -4.3941035 -4.4487586 -4.4511719 -4.3802795 -4.2672176 -4.1559887 -4.0864611 -4.0989928 -4.1856813 -4.2569551 -4.2303257 -4.1104813 -3.9719653 -3.9112659][-4.1627979 -4.2824721 -4.3765 -4.3833914 -4.2773476 -4.1116571 -3.9492722 -3.8487091 -3.8599517 -3.9907429 -4.1392317 -4.168541 -4.0471635 -3.8708196 -3.7712989][-4.0573778 -4.2003789 -4.3139281 -4.3216791 -4.203968 -4.0073252 -3.8042958 -3.6918237 -3.7268775 -3.891767 -4.0835986 -4.1725092 -4.1001663 -3.9373791 -3.8170609][-4.0154724 -4.1554322 -4.2644939 -4.2777553 -4.1857991 -4.0089808 -3.8034697 -3.6942835 -3.7455332 -3.9124033 -4.1041126 -4.226078 -4.2137389 -4.1003 -3.9934564][-4.0361834 -4.1494246 -4.236156 -4.2530212 -4.2072115 -4.1001611 -3.9519773 -3.8653312 -3.8977892 -4.0136323 -4.1626582 -4.2880607 -4.3179145 -4.2520547 -4.1672068][-4.1199951 -4.1962352 -4.2494216 -4.2615218 -4.2518492 -4.2193875 -4.1606636 -4.124404 -4.1353145 -4.1789865 -4.2594628 -4.3594971 -4.4083209 -4.3791981 -4.3172064][-4.2927 -4.3325262 -4.3565793 -4.362299 -4.3672423 -4.3791885 -4.3927927 -4.4106803 -4.4179363 -4.4116335 -4.429563 -4.4823318 -4.5208173 -4.5154576 -4.4786873][-4.5154481 -4.5334191 -4.5448823 -4.5497012 -4.5595837 -4.58494 -4.623045 -4.6585288 -4.6641531 -4.6419296 -4.62902 -4.638864 -4.6470294 -4.6397247 -4.613656]]...]
INFO - root - 2017-12-07 05:35:53.445184: step 1810, loss = 1.99, batch loss = 1.91 (11.2 examples/sec; 0.715 sec/batch; 65h:39m:55s remains)
INFO - root - 2017-12-07 05:36:00.477503: step 1820, loss = 2.00, batch loss = 1.92 (12.2 examples/sec; 0.657 sec/batch; 60h:18m:29s remains)
INFO - root - 2017-12-07 05:36:07.344266: step 1830, loss = 2.01, batch loss = 1.93 (10.9 examples/sec; 0.736 sec/batch; 67h:34m:57s remains)
INFO - root - 2017-12-07 05:36:14.315397: step 1840, loss = 2.00, batch loss = 1.92 (11.0 examples/sec; 0.724 sec/batch; 66h:31m:12s remains)
INFO - root - 2017-12-07 05:36:21.243385: step 1850, loss = 2.01, batch loss = 1.92 (11.1 examples/sec; 0.722 sec/batch; 66h:20m:16s remains)
INFO - root - 2017-12-07 05:36:28.238135: step 1860, loss = 1.92, batch loss = 1.83 (11.5 examples/sec; 0.698 sec/batch; 64h:06m:34s remains)
INFO - root - 2017-12-07 05:36:35.179547: step 1870, loss = 2.04, batch loss = 1.95 (11.7 examples/sec; 0.683 sec/batch; 62h:43m:12s remains)
INFO - root - 2017-12-07 05:36:42.206974: step 1880, loss = 2.05, batch loss = 1.97 (11.5 examples/sec; 0.698 sec/batch; 64h:08m:25s remains)
INFO - root - 2017-12-07 05:36:49.245438: step 1890, loss = 2.05, batch loss = 1.97 (11.1 examples/sec; 0.718 sec/batch; 65h:55m:17s remains)
INFO - root - 2017-12-07 05:36:56.361617: step 1900, loss = 2.00, batch loss = 1.92 (10.9 examples/sec; 0.734 sec/batch; 67h:23m:39s remains)
2017-12-07 05:36:57.158786: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6286597 -4.6543431 -4.6578116 -4.6458669 -4.6326466 -4.63559 -4.6474695 -4.6554732 -4.6564331 -4.6579366 -4.6671104 -4.6844554 -4.6956015 -4.6935172 -4.6892781][-4.7079368 -4.7167048 -4.7014065 -4.6681981 -4.6338859 -4.6189981 -4.6227603 -4.6348286 -4.6445522 -4.6593852 -4.6846318 -4.7154136 -4.7291017 -4.7149229 -4.6952639][-4.7284923 -4.7055821 -4.6673465 -4.6162677 -4.5642557 -4.5291734 -4.5249538 -4.5414305 -4.5578671 -4.5783105 -4.6078038 -4.64136 -4.6515021 -4.6238976 -4.5869384][-4.6440496 -4.5866318 -4.5312462 -4.4758182 -4.4185624 -4.3770928 -4.3793769 -4.4134393 -4.4427552 -4.4583559 -4.4691758 -4.4835525 -4.4787316 -4.4386234 -4.3921685][-4.4623041 -4.3952012 -4.3419833 -4.3001509 -4.2527647 -4.2177253 -4.2378817 -4.3079796 -4.366611 -4.3761044 -4.3545594 -4.3334312 -4.3018055 -4.2477541 -4.1976962][-4.2760944 -4.23927 -4.2048397 -4.17723 -4.1356106 -4.1010828 -4.1326275 -4.2397909 -4.3340697 -4.3388414 -4.2872586 -4.2335577 -4.1784668 -4.1139736 -4.0606575][-4.173501 -4.1848516 -4.166204 -4.1368227 -4.0821366 -4.035563 -4.0650344 -4.1931243 -4.3176794 -4.328784 -4.2677021 -4.2016077 -4.1380105 -4.0664539 -4.0082154][-4.147522 -4.2008986 -4.1881824 -4.1509194 -4.0845337 -4.0206609 -4.0275264 -4.1390429 -4.268146 -4.3022346 -4.2712736 -4.23381 -4.1933508 -4.1308675 -4.0740538][-4.1858044 -4.2437205 -4.2233987 -4.1805592 -4.1118569 -4.0397673 -4.0315914 -4.1130209 -4.223804 -4.2772856 -4.2926 -4.3077078 -4.315938 -4.2885365 -4.2544332][-4.2909951 -4.3243985 -4.2839079 -4.2256289 -4.1453915 -4.0628314 -4.049222 -4.1161752 -4.2143574 -4.2821136 -4.3297029 -4.3879189 -4.4476271 -4.46813 -4.4684577][-4.4446721 -4.4470067 -4.3775926 -4.2895026 -4.18146 -4.0784049 -4.058815 -4.123795 -4.2196207 -4.299243 -4.3577957 -4.4327316 -4.5228209 -4.5739818 -4.5946093][-4.5683827 -4.5624304 -4.475666 -4.3594551 -4.226089 -4.109828 -4.0868974 -4.1505036 -4.2467413 -4.32873 -4.3686271 -4.4273782 -4.5214925 -4.5819635 -4.6066804][-4.60868 -4.6257377 -4.552711 -4.4366684 -4.3087468 -4.205615 -4.1888194 -4.2449789 -4.3280921 -4.3879595 -4.3866439 -4.40996 -4.4938498 -4.5549893 -4.5775433][-4.548924 -4.6017585 -4.5700788 -4.4945879 -4.4126277 -4.3539362 -4.354044 -4.3976326 -4.4486995 -4.4640117 -4.4208856 -4.4125023 -4.4814043 -4.5382504 -4.5523086][-4.43481 -4.5098839 -4.5255504 -4.5083141 -4.4857569 -4.4776597 -4.4981141 -4.53161 -4.5531316 -4.5343213 -4.4765444 -4.4563284 -4.5059323 -4.5488763 -4.5464392]]...]
INFO - root - 2017-12-07 05:37:04.218412: step 1910, loss = 1.97, batch loss = 1.89 (11.5 examples/sec; 0.694 sec/batch; 63h:43m:24s remains)
INFO - root - 2017-12-07 05:37:11.242091: step 1920, loss = 2.04, batch loss = 1.95 (11.7 examples/sec; 0.684 sec/batch; 62h:49m:07s remains)
INFO - root - 2017-12-07 05:37:18.266338: step 1930, loss = 2.02, batch loss = 1.94 (11.0 examples/sec; 0.726 sec/batch; 66h:38m:10s remains)
INFO - root - 2017-12-07 05:37:25.290915: step 1940, loss = 2.04, batch loss = 1.96 (10.7 examples/sec; 0.746 sec/batch; 68h:27m:27s remains)
INFO - root - 2017-12-07 05:37:32.352240: step 1950, loss = 2.01, batch loss = 1.93 (10.7 examples/sec; 0.745 sec/batch; 68h:26m:54s remains)
INFO - root - 2017-12-07 05:37:39.310514: step 1960, loss = 2.03, batch loss = 1.95 (11.3 examples/sec; 0.710 sec/batch; 65h:14m:00s remains)
INFO - root - 2017-12-07 05:37:46.361666: step 1970, loss = 2.05, batch loss = 1.96 (11.6 examples/sec; 0.690 sec/batch; 63h:18m:57s remains)
INFO - root - 2017-12-07 05:37:53.552968: step 1980, loss = 2.02, batch loss = 1.94 (10.3 examples/sec; 0.780 sec/batch; 71h:35m:04s remains)
INFO - root - 2017-12-07 05:38:00.509547: step 1990, loss = 2.02, batch loss = 1.94 (11.4 examples/sec; 0.704 sec/batch; 64h:39m:46s remains)
INFO - root - 2017-12-07 05:38:07.500924: step 2000, loss = 2.03, batch loss = 1.94 (10.5 examples/sec; 0.762 sec/batch; 69h:58m:59s remains)
2017-12-07 05:38:08.269255: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3932924 -4.4361115 -4.4869151 -4.5512958 -4.6009984 -4.6354461 -4.6673541 -4.6699386 -4.6301689 -4.5946937 -4.5973125 -4.6304007 -4.6434546 -4.6195879 -4.59269][-4.5361838 -4.5660057 -4.59516 -4.6579814 -4.7074089 -4.723527 -4.7253933 -4.6985016 -4.637517 -4.5855217 -4.5640535 -4.5676446 -4.5546732 -4.5111146 -4.4795451][-4.64042 -4.6652169 -4.6692934 -4.7066388 -4.7119021 -4.6631103 -4.6157632 -4.5832744 -4.5491962 -4.5187912 -4.4903107 -4.4641328 -4.4168262 -4.3445497 -4.2964864][-4.6712008 -4.7036433 -4.6869383 -4.678669 -4.6183949 -4.4987583 -4.4096375 -4.3981218 -4.4270954 -4.4478073 -4.4288216 -4.3733935 -4.2815289 -4.172987 -4.1092558][-4.6328554 -4.6751847 -4.6321349 -4.5555453 -4.4268465 -4.2606974 -4.1497784 -4.1601462 -4.2521863 -4.3375411 -4.3484406 -4.2806163 -4.1616521 -4.0324788 -3.9732745][-4.5470905 -4.5877151 -4.5130091 -4.3613477 -4.1756124 -3.9916182 -3.8711882 -3.8769231 -3.9997182 -4.1499572 -4.2160091 -4.1767321 -4.0791111 -3.9658372 -3.9277821][-4.471261 -4.4927092 -4.3822794 -4.1723886 -3.9573462 -3.7780221 -3.6428776 -3.6055095 -3.7161651 -3.9180911 -4.058177 -4.086194 -4.0555305 -3.9881432 -3.9799161][-4.4187551 -4.408092 -4.2730393 -4.05211 -3.8575916 -3.7115111 -3.5687625 -3.4707103 -3.5295358 -3.7432127 -3.9401326 -4.0381055 -4.0756283 -4.0509105 -4.0632167][-4.368691 -4.3394909 -4.2234054 -4.0590272 -3.9427321 -3.8674335 -3.7448685 -3.597281 -3.5767603 -3.7342072 -3.926054 -4.0520797 -4.11939 -4.1126337 -4.1299219][-4.3165836 -4.2966256 -4.2398715 -4.1688581 -4.1452303 -4.1388988 -4.0470915 -3.8769064 -3.788285 -3.8547688 -3.9862843 -4.0947347 -4.1620135 -4.171257 -4.2041006][-4.2663064 -4.2608776 -4.261086 -4.2683797 -4.3046684 -4.3344736 -4.267642 -4.1057892 -3.9969652 -4.0123281 -4.0930018 -4.1708722 -4.2242093 -4.2479792 -4.2938709][-4.2760797 -4.2713604 -4.3012357 -4.3494086 -4.4059167 -4.4414749 -4.3914328 -4.2592154 -4.1706848 -4.1890879 -4.260426 -4.3225331 -4.3558397 -4.3667159 -4.3893828][-4.3314891 -4.313333 -4.3448806 -4.4058795 -4.4655995 -4.5007372 -4.4720688 -4.3785567 -4.3155146 -4.3435755 -4.4150977 -4.4739771 -4.5011067 -4.498971 -4.4910502][-4.3598042 -4.3351254 -4.3649344 -4.4313483 -4.4934182 -4.5335517 -4.5319033 -4.4797974 -4.4323168 -4.4454327 -4.494123 -4.5367618 -4.5610967 -4.5591912 -4.5429773][-4.3732057 -4.3564558 -4.387743 -4.4534364 -4.5135584 -4.5557551 -4.5741954 -4.557168 -4.5260458 -4.5191293 -4.5328665 -4.5460529 -4.5539956 -4.5475903 -4.5302253]]...]
INFO - root - 2017-12-07 05:38:15.290806: step 2010, loss = 2.03, batch loss = 1.94 (11.6 examples/sec; 0.691 sec/batch; 63h:27m:58s remains)
INFO - root - 2017-12-07 05:38:22.417545: step 2020, loss = 2.03, batch loss = 1.95 (10.9 examples/sec; 0.736 sec/batch; 67h:34m:36s remains)
INFO - root - 2017-12-07 05:38:29.583409: step 2030, loss = 1.98, batch loss = 1.90 (10.8 examples/sec; 0.739 sec/batch; 67h:50m:28s remains)
INFO - root - 2017-12-07 05:38:36.545068: step 2040, loss = 2.01, batch loss = 1.93 (11.3 examples/sec; 0.711 sec/batch; 65h:16m:12s remains)
INFO - root - 2017-12-07 05:38:43.531130: step 2050, loss = 2.05, batch loss = 1.97 (11.9 examples/sec; 0.672 sec/batch; 61h:41m:38s remains)
INFO - root - 2017-12-07 05:38:50.653977: step 2060, loss = 2.05, batch loss = 1.97 (11.2 examples/sec; 0.713 sec/batch; 65h:26m:45s remains)
INFO - root - 2017-12-07 05:38:57.771343: step 2070, loss = 2.05, batch loss = 1.97 (11.4 examples/sec; 0.699 sec/batch; 64h:09m:28s remains)
INFO - root - 2017-12-07 05:39:04.808080: step 2080, loss = 2.07, batch loss = 1.99 (11.2 examples/sec; 0.717 sec/batch; 65h:49m:14s remains)
INFO - root - 2017-12-07 05:39:11.873203: step 2090, loss = 2.02, batch loss = 1.94 (11.2 examples/sec; 0.714 sec/batch; 65h:31m:33s remains)
INFO - root - 2017-12-07 05:39:18.867678: step 2100, loss = 2.00, batch loss = 1.92 (12.0 examples/sec; 0.664 sec/batch; 60h:58m:08s remains)
2017-12-07 05:39:19.663026: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.232574 -4.28148 -4.3978252 -4.4662213 -4.4111447 -4.3168917 -4.1820483 -4.058 -4.0573845 -4.1424084 -4.2271533 -4.2692862 -4.2466331 -4.1980271 -4.1766171][-4.2963758 -4.3399525 -4.4198217 -4.4398608 -4.3699017 -4.2872992 -4.1651368 -4.0512362 -4.0677834 -4.1720819 -4.2899485 -4.3753333 -4.3629231 -4.3038135 -4.254899][-4.3767719 -4.4289131 -4.4818039 -4.458499 -4.3723254 -4.2845745 -4.1573715 -4.0447321 -4.0665789 -4.1787677 -4.3167667 -4.4381843 -4.455164 -4.4143348 -4.3493795][-4.4635353 -4.5335307 -4.5708017 -4.5089746 -4.3978477 -4.2868185 -4.1431265 -4.0368977 -4.0729957 -4.1934161 -4.3416214 -4.4790993 -4.5230374 -4.5094781 -4.4368854][-4.534708 -4.6130147 -4.6374664 -4.5346746 -4.3850436 -4.2384825 -4.0750628 -3.9841051 -4.0444684 -4.1774707 -4.3344717 -4.4814782 -4.5584793 -4.5870028 -4.5205832][-4.5444307 -4.619175 -4.6338105 -4.5054531 -4.32673 -4.1496367 -3.9665365 -3.8848195 -3.9624233 -4.1049981 -4.270967 -4.429831 -4.5435882 -4.6203461 -4.5754437][-4.5267973 -4.5884528 -4.5899148 -4.4452453 -4.2410088 -4.0356441 -3.838145 -3.7659397 -3.8620291 -4.0232072 -4.2129636 -4.39201 -4.53259 -4.6339469 -4.5952735][-4.5210342 -4.564013 -4.5483537 -4.3946037 -4.1696367 -3.9415436 -3.7398493 -3.6787524 -3.7891395 -3.9698718 -4.1899319 -4.3997588 -4.5607834 -4.6606936 -4.6028671][-4.5240631 -4.5460615 -4.5257368 -4.3942676 -4.1858811 -3.95928 -3.7557867 -3.6815581 -3.7696738 -3.9387527 -4.1687379 -4.4063954 -4.588294 -4.6840367 -4.6111054][-4.5347004 -4.5446153 -4.5315108 -4.4428349 -4.2826209 -4.0803051 -3.8749943 -3.7754176 -3.825218 -3.9705007 -4.1896143 -4.4281764 -4.6128078 -4.7042904 -4.6343718][-4.5492177 -4.555366 -4.5514512 -4.5028939 -4.3995271 -4.2423773 -4.0600023 -3.9634404 -3.9981589 -4.123744 -4.3067555 -4.496604 -4.6400361 -4.709332 -4.6546397][-4.5563512 -4.5635023 -4.5678463 -4.5500631 -4.496747 -4.3947248 -4.2598648 -4.1878419 -4.2168665 -4.3141003 -4.4414368 -4.5601053 -4.6470895 -4.6908765 -4.6605916][-4.5857391 -4.5937347 -4.6016583 -4.598588 -4.5701256 -4.5077305 -4.4216385 -4.3795123 -4.4060855 -4.4730663 -4.54583 -4.6010075 -4.6397734 -4.663208 -4.6561317][-4.620985 -4.6340656 -4.6468382 -4.6536059 -4.6396036 -4.6047821 -4.5568171 -4.53605 -4.5571346 -4.5946641 -4.6192875 -4.6232772 -4.6265087 -4.6350231 -4.6421218][-4.6091986 -4.6257038 -4.6454344 -4.6640396 -4.66536 -4.651413 -4.625731 -4.6104517 -4.6176844 -4.6297421 -4.6228213 -4.6011362 -4.5910053 -4.5932217 -4.602695]]...]
INFO - root - 2017-12-07 05:39:26.727874: step 2110, loss = 2.00, batch loss = 1.92 (10.8 examples/sec; 0.740 sec/batch; 67h:53m:10s remains)
INFO - root - 2017-12-07 05:39:33.829999: step 2120, loss = 2.01, batch loss = 1.93 (11.6 examples/sec; 0.691 sec/batch; 63h:25m:25s remains)
INFO - root - 2017-12-07 05:39:40.865971: step 2130, loss = 2.00, batch loss = 1.91 (12.7 examples/sec; 0.630 sec/batch; 57h:47m:15s remains)
INFO - root - 2017-12-07 05:39:47.924480: step 2140, loss = 2.02, batch loss = 1.94 (11.9 examples/sec; 0.670 sec/batch; 61h:29m:39s remains)
INFO - root - 2017-12-07 05:39:55.037012: step 2150, loss = 2.06, batch loss = 1.97 (11.1 examples/sec; 0.720 sec/batch; 66h:03m:02s remains)
INFO - root - 2017-12-07 05:40:02.051274: step 2160, loss = 2.02, batch loss = 1.94 (11.1 examples/sec; 0.720 sec/batch; 66h:01m:45s remains)
INFO - root - 2017-12-07 05:40:09.183974: step 2170, loss = 2.08, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 66h:20m:28s remains)
INFO - root - 2017-12-07 05:40:16.273042: step 2180, loss = 2.01, batch loss = 1.92 (10.4 examples/sec; 0.768 sec/batch; 70h:29m:09s remains)
INFO - root - 2017-12-07 05:40:23.372966: step 2190, loss = 2.04, batch loss = 1.96 (10.8 examples/sec; 0.741 sec/batch; 67h:57m:09s remains)
INFO - root - 2017-12-07 05:40:30.407234: step 2200, loss = 2.02, batch loss = 1.94 (11.9 examples/sec; 0.671 sec/batch; 61h:35m:40s remains)
2017-12-07 05:40:31.186431: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7477713 -4.7785406 -4.7758856 -4.6821313 -4.5484872 -4.464572 -4.4692569 -4.5546017 -4.6546946 -4.68048 -4.6495848 -4.619709 -4.5979776 -4.5833955 -4.58154][-4.7298436 -4.7496929 -4.7423773 -4.6606526 -4.5498104 -4.488821 -4.507668 -4.5919166 -4.6801167 -4.6980333 -4.6702633 -4.65977 -4.6647458 -4.6762519 -4.6926885][-4.7019076 -4.7233424 -4.7189074 -4.6418457 -4.5404186 -4.4818153 -4.49453 -4.5635905 -4.6355882 -4.6644373 -4.6770363 -4.7220287 -4.7803736 -4.8238525 -4.8423204][-4.6976271 -4.7306905 -4.735086 -4.6462655 -4.5197077 -4.4257417 -4.4029121 -4.4470639 -4.5167103 -4.5827842 -4.6650176 -4.782711 -4.8984728 -4.9649086 -4.9654322][-4.7138081 -4.7652783 -4.7703323 -4.6502242 -4.4736843 -4.3230715 -4.24671 -4.2558217 -4.3286233 -4.447485 -4.6073961 -4.7954259 -4.9560056 -5.0282841 -4.9951324][-4.7337427 -4.7981868 -4.7899866 -4.6253843 -4.3842115 -4.1644998 -4.0226865 -3.9930773 -4.076314 -4.2596769 -4.5047121 -4.7608094 -4.9544153 -5.0145535 -4.9272838][-4.7437348 -4.8078 -4.7732744 -4.5636554 -4.2633529 -3.9829586 -3.7816484 -3.7198725 -3.8199863 -4.0696836 -4.3981237 -4.7149229 -4.9271016 -4.9562984 -4.7984896][-4.7449656 -4.7998118 -4.7396889 -4.5049505 -4.1768303 -3.8683791 -3.6337657 -3.5487049 -3.6581857 -3.9458997 -4.3232102 -4.6738024 -4.8884788 -4.8900652 -4.681982][-4.7537441 -4.8013291 -4.7337651 -4.5069485 -4.1911869 -3.8944311 -3.6638839 -3.5729456 -3.6784167 -3.9522409 -4.3123059 -4.6453233 -4.842701 -4.8417692 -4.6408534][-4.7638631 -4.814671 -4.7686186 -4.5858951 -4.3218703 -4.0714664 -3.8771822 -3.8006396 -3.8923998 -4.1072416 -4.3837976 -4.6387453 -4.7928858 -4.8125443 -4.6783257][-4.7547846 -4.8111706 -4.8022552 -4.6840572 -4.4955831 -4.3132329 -4.1740165 -4.125267 -4.1990709 -4.3391447 -4.5057974 -4.6569414 -4.7607732 -4.8092294 -4.758358][-4.6908669 -4.753294 -4.782701 -4.7299352 -4.6161056 -4.4964547 -4.4066892 -4.3869419 -4.4464707 -4.5282803 -4.6121488 -4.685317 -4.7536612 -4.8226147 -4.8300061][-4.5884118 -4.6499834 -4.7067528 -4.7067037 -4.6516137 -4.5773325 -4.5224819 -4.5246029 -4.5761814 -4.6268716 -4.6652451 -4.697125 -4.7456908 -4.8134527 -4.8415976][-4.4847574 -4.5366273 -4.5998125 -4.6289635 -4.6145806 -4.5769444 -4.5521007 -4.5711808 -4.6167288 -4.6519418 -4.667552 -4.6805987 -4.7151937 -4.7639766 -4.7840462][-4.4112115 -4.4476976 -4.498446 -4.5333 -4.539278 -4.5276027 -4.52483 -4.5507545 -4.5873556 -4.6120195 -4.6190462 -4.6278586 -4.6593986 -4.6972289 -4.713243]]...]
INFO - root - 2017-12-07 05:40:38.324918: step 2210, loss = 2.03, batch loss = 1.94 (10.9 examples/sec; 0.732 sec/batch; 67h:07m:54s remains)
INFO - root - 2017-12-07 05:40:45.345643: step 2220, loss = 2.00, batch loss = 1.92 (11.0 examples/sec; 0.727 sec/batch; 66h:41m:47s remains)
INFO - root - 2017-12-07 05:40:52.437527: step 2230, loss = 2.01, batch loss = 1.92 (11.5 examples/sec; 0.698 sec/batch; 64h:02m:41s remains)
INFO - root - 2017-12-07 05:40:59.543245: step 2240, loss = 2.00, batch loss = 1.92 (11.6 examples/sec; 0.687 sec/batch; 63h:01m:56s remains)
INFO - root - 2017-12-07 05:41:06.631683: step 2250, loss = 2.03, batch loss = 1.94 (10.9 examples/sec; 0.731 sec/batch; 67h:03m:32s remains)
INFO - root - 2017-12-07 05:41:13.724822: step 2260, loss = 2.01, batch loss = 1.92 (11.1 examples/sec; 0.724 sec/batch; 66h:23m:26s remains)
INFO - root - 2017-12-07 05:41:20.876794: step 2270, loss = 2.04, batch loss = 1.96 (10.6 examples/sec; 0.754 sec/batch; 69h:09m:34s remains)
INFO - root - 2017-12-07 05:41:27.995515: step 2280, loss = 2.03, batch loss = 1.95 (10.7 examples/sec; 0.750 sec/batch; 68h:50m:12s remains)
INFO - root - 2017-12-07 05:41:35.065892: step 2290, loss = 2.03, batch loss = 1.95 (11.2 examples/sec; 0.713 sec/batch; 65h:25m:45s remains)
INFO - root - 2017-12-07 05:41:42.157835: step 2300, loss = 2.03, batch loss = 1.94 (11.2 examples/sec; 0.714 sec/batch; 65h:26m:41s remains)
2017-12-07 05:41:42.966531: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5171061 -4.5695367 -4.6366496 -4.6645 -4.6247706 -4.5578575 -4.4893336 -4.4494295 -4.4022565 -4.3279586 -4.2491388 -4.163065 -4.0954447 -4.1147738 -4.2171435][-4.5595074 -4.61455 -4.6701846 -4.6772308 -4.6289682 -4.5661697 -4.4892788 -4.413116 -4.3101573 -4.2044821 -4.1572871 -4.1292109 -4.1086469 -4.14235 -4.2230668][-4.5775175 -4.6241012 -4.6648016 -4.6513476 -4.595325 -4.533289 -4.4426227 -4.3247418 -4.1822505 -4.0851526 -4.0970635 -4.1327305 -4.1638036 -4.2159586 -4.2742867][-4.5398579 -4.5662508 -4.5892205 -4.5625434 -4.511065 -4.4627953 -4.3761992 -4.2357063 -4.0822 -4.0187845 -4.07265 -4.1409574 -4.2088609 -4.27935 -4.3187747][-4.47469 -4.4860053 -4.5021248 -4.48009 -4.4473186 -4.4138565 -4.3282404 -4.1724744 -4.0312653 -4.0101395 -4.0778251 -4.1462178 -4.2301817 -4.3092208 -4.3324451][-4.4211159 -4.4366903 -4.463603 -4.4528179 -4.41812 -4.3607683 -4.2434797 -4.0676861 -3.9607062 -4.0010252 -4.0904469 -4.163075 -4.25779 -4.3394537 -4.3510666][-4.3774872 -4.4169788 -4.4646864 -4.4558048 -4.3839459 -4.2630358 -4.0940914 -3.9149511 -3.8672161 -3.9750459 -4.0962291 -4.18532 -4.2846427 -4.3615236 -4.37053][-4.3312287 -4.4032216 -4.4659047 -4.4403739 -4.3147798 -4.1361065 -3.9485312 -3.8118014 -3.8388 -3.9892116 -4.1203828 -4.2153549 -4.3030281 -4.3652139 -4.3767076][-4.2668071 -4.3596787 -4.427732 -4.3944449 -4.2566652 -4.0792842 -3.9266279 -3.8601947 -3.9375932 -4.0835786 -4.1919694 -4.2668867 -4.3209009 -4.3533664 -4.3652539][-4.19251 -4.2903976 -4.3763885 -4.3825045 -4.29411 -4.1632366 -4.060389 -4.0428195 -4.1220512 -4.2285409 -4.2972612 -4.3355088 -4.3431649 -4.3413453 -4.3518658][-4.1796613 -4.2755766 -4.3740788 -4.4190159 -4.3817167 -4.2952681 -4.23328 -4.2392392 -4.2965922 -4.357173 -4.3898439 -4.3988705 -4.3758988 -4.3539262 -4.3604321][-4.2437 -4.3314872 -4.4147038 -4.4634075 -4.4510288 -4.3998723 -4.3653016 -4.3679881 -4.3873663 -4.4063635 -4.418942 -4.4246106 -4.4035797 -4.3823709 -4.3818178][-4.3488641 -4.4113994 -4.4556675 -4.4783478 -4.4711237 -4.4441314 -4.42066 -4.4091177 -4.3999896 -4.395998 -4.4021931 -4.4165883 -4.4119511 -4.3988042 -4.3892193][-4.452177 -4.4849324 -4.4778848 -4.4516749 -4.4300718 -4.4204659 -4.4120059 -4.3977289 -4.3832092 -4.3771334 -4.3836474 -4.4015903 -4.4054079 -4.3949327 -4.3784266][-4.5095096 -4.5208569 -4.4726562 -4.4084711 -4.3771625 -4.3858747 -4.3966084 -4.3882937 -4.3778772 -4.3769784 -4.3846493 -4.3984084 -4.3992009 -4.3855543 -4.3667026]]...]
INFO - root - 2017-12-07 05:41:49.954732: step 2310, loss = 2.02, batch loss = 1.94 (11.0 examples/sec; 0.726 sec/batch; 66h:35m:14s remains)
INFO - root - 2017-12-07 05:41:57.046490: step 2320, loss = 2.03, batch loss = 1.94 (11.9 examples/sec; 0.673 sec/batch; 61h:41m:10s remains)
INFO - root - 2017-12-07 05:42:04.019726: step 2330, loss = 2.04, batch loss = 1.96 (12.1 examples/sec; 0.662 sec/batch; 60h:44m:33s remains)
INFO - root - 2017-12-07 05:42:11.127861: step 2340, loss = 2.00, batch loss = 1.91 (11.4 examples/sec; 0.704 sec/batch; 64h:34m:14s remains)
INFO - root - 2017-12-07 05:42:18.229966: step 2350, loss = 2.04, batch loss = 1.96 (11.5 examples/sec; 0.699 sec/batch; 64h:04m:00s remains)
INFO - root - 2017-12-07 05:42:25.268529: step 2360, loss = 2.02, batch loss = 1.94 (11.4 examples/sec; 0.700 sec/batch; 64h:13m:29s remains)
INFO - root - 2017-12-07 05:42:32.307535: step 2370, loss = 2.03, batch loss = 1.95 (11.3 examples/sec; 0.709 sec/batch; 65h:02m:52s remains)
INFO - root - 2017-12-07 05:42:39.423722: step 2380, loss = 1.98, batch loss = 1.89 (10.6 examples/sec; 0.756 sec/batch; 69h:19m:27s remains)
INFO - root - 2017-12-07 05:42:46.532970: step 2390, loss = 2.06, batch loss = 1.97 (10.7 examples/sec; 0.750 sec/batch; 68h:47m:51s remains)
INFO - root - 2017-12-07 05:42:53.599802: step 2400, loss = 2.01, batch loss = 1.93 (11.5 examples/sec; 0.694 sec/batch; 63h:39m:29s remains)
2017-12-07 05:42:54.374290: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5125051 -4.4648509 -4.3349576 -4.1854734 -4.0798335 -4.0712647 -4.132628 -4.187809 -4.2533259 -4.3394766 -4.4062395 -4.4367967 -4.42062 -4.3863029 -4.3623776][-4.4767718 -4.3874974 -4.2172847 -4.0393262 -3.9257686 -3.9539578 -4.0849977 -4.2084684 -4.3087192 -4.395421 -4.4448876 -4.4648161 -4.4596472 -4.4440246 -4.4276567][-4.4920297 -4.3999753 -4.2206454 -4.0340137 -3.9202271 -3.9626579 -4.1162238 -4.2613463 -4.3599644 -4.4276528 -4.4590974 -4.4744477 -4.4871693 -4.49169 -4.4704351][-4.5280252 -4.4688616 -4.3108606 -4.1365461 -4.0343232 -4.0726743 -4.1988578 -4.3065982 -4.3656273 -4.4044027 -4.4265747 -4.4466939 -4.4765038 -4.4935737 -4.4574952][-4.5161471 -4.5007863 -4.3884192 -4.2496076 -4.1680212 -4.187129 -4.2493572 -4.2781324 -4.2780828 -4.2961588 -4.33703 -4.3917689 -4.4516182 -4.475718 -4.4178734][-4.4578195 -4.4793754 -4.4241152 -4.3279529 -4.25578 -4.2308531 -4.2058439 -4.1480613 -4.0997071 -4.1160755 -4.1931009 -4.299468 -4.403296 -4.4475355 -4.3900552][-4.3975506 -4.4371839 -4.4268851 -4.3618121 -4.2786436 -4.1902452 -4.0786567 -3.9568977 -3.8934374 -3.9305182 -4.0440335 -4.1939793 -4.338336 -4.4140921 -4.3858018][-4.3738313 -4.4057837 -4.41331 -4.3576384 -4.2528725 -4.1064882 -3.9354343 -3.7883551 -3.7380931 -3.8006012 -3.9308667 -4.0967269 -4.2626138 -4.3677444 -4.3828044][-4.3990021 -4.4047647 -4.4025149 -4.3406205 -4.2228694 -4.060277 -3.8896697 -3.7669203 -3.7457595 -3.8112714 -3.9165609 -4.0530176 -4.2027678 -4.315794 -4.3659525][-4.4247241 -4.4074163 -4.3892508 -4.3244028 -4.2194653 -4.0945091 -3.9859805 -3.9255648 -3.9314909 -3.9716165 -4.0182667 -4.0937934 -4.199338 -4.3006639 -4.371726][-4.4385986 -4.4148235 -4.38662 -4.3235044 -4.2437339 -4.1756797 -4.1449718 -4.147367 -4.1673455 -4.1755466 -4.1714411 -4.1942115 -4.2563324 -4.3390026 -4.4187202][-4.4700561 -4.45363 -4.4210978 -4.3619585 -4.3047013 -4.2789507 -4.2955575 -4.3266144 -4.3401017 -4.3206596 -4.2910395 -4.2858 -4.3177471 -4.3846822 -4.4691067][-4.4892445 -4.490633 -4.465867 -4.4179692 -4.3814707 -4.375771 -4.4039454 -4.4348431 -4.4340091 -4.3960562 -4.3476682 -4.3179064 -4.3228745 -4.3749895 -4.4659996][-4.4671154 -4.4947944 -4.4903426 -4.4625549 -4.4458418 -4.4453158 -4.4599524 -4.4741068 -4.4590878 -4.40832 -4.3376794 -4.2760458 -4.2523222 -4.2895594 -4.3826356][-4.4152908 -4.4713521 -4.4935284 -4.4883485 -4.4860845 -4.484139 -4.477951 -4.4679036 -4.4369612 -4.3760042 -4.2875338 -4.1998668 -4.1561389 -4.1830015 -4.2718]]...]
INFO - root - 2017-12-07 05:43:01.451213: step 2410, loss = 2.05, batch loss = 1.97 (10.6 examples/sec; 0.754 sec/batch; 69h:09m:26s remains)
INFO - root - 2017-12-07 05:43:08.534589: step 2420, loss = 2.01, batch loss = 1.92 (10.8 examples/sec; 0.740 sec/batch; 67h:48m:36s remains)
INFO - root - 2017-12-07 05:43:15.600380: step 2430, loss = 2.02, batch loss = 1.94 (11.2 examples/sec; 0.711 sec/batch; 65h:13m:31s remains)
INFO - root - 2017-12-07 05:43:22.562149: step 2440, loss = 1.98, batch loss = 1.90 (12.1 examples/sec; 0.662 sec/batch; 60h:38m:55s remains)
INFO - root - 2017-12-07 05:43:29.604654: step 2450, loss = 2.00, batch loss = 1.91 (11.5 examples/sec; 0.697 sec/batch; 63h:54m:42s remains)
INFO - root - 2017-12-07 05:43:36.662241: step 2460, loss = 2.00, batch loss = 1.91 (11.3 examples/sec; 0.706 sec/batch; 64h:43m:16s remains)
INFO - root - 2017-12-07 05:43:43.717208: step 2470, loss = 2.04, batch loss = 1.96 (11.2 examples/sec; 0.712 sec/batch; 65h:14m:28s remains)
INFO - root - 2017-12-07 05:43:50.768820: step 2480, loss = 2.03, batch loss = 1.94 (11.3 examples/sec; 0.709 sec/batch; 64h:58m:17s remains)
INFO - root - 2017-12-07 05:43:57.730298: step 2490, loss = 2.03, batch loss = 1.94 (11.6 examples/sec; 0.691 sec/batch; 63h:18m:46s remains)
INFO - root - 2017-12-07 05:44:04.880656: step 2500, loss = 1.97, batch loss = 1.89 (11.8 examples/sec; 0.677 sec/batch; 62h:02m:44s remains)
2017-12-07 05:44:05.672639: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5314989 -4.5589724 -4.5679512 -4.5668364 -4.5449748 -4.5160031 -4.504961 -4.5257206 -4.5610027 -4.5798593 -4.578351 -4.5593228 -4.5215616 -4.4715114 -4.4268632][-4.676405 -4.6919146 -4.6802626 -4.6629934 -4.6183453 -4.5608492 -4.5342207 -4.5664072 -4.6273708 -4.6603713 -4.6671667 -4.6616368 -4.634089 -4.5851164 -4.5340881][-4.7876372 -4.7822938 -4.74107 -4.6993442 -4.6214333 -4.5176759 -4.4515543 -4.4798508 -4.568543 -4.6310797 -4.662456 -4.6848545 -4.6875367 -4.660573 -4.6220055][-4.8235583 -4.8006029 -4.7355771 -4.66586 -4.5522685 -4.4049687 -4.299263 -4.3211722 -4.4393969 -4.542326 -4.6029148 -4.6470275 -4.6748915 -4.6755033 -4.659359][-4.7593379 -4.7150631 -4.637794 -4.5531678 -4.4240708 -4.2639523 -4.1425772 -4.1604662 -4.3005347 -4.4409814 -4.5311995 -4.5908041 -4.6330538 -4.6545362 -4.6565528][-4.6101904 -4.5335631 -4.4470897 -4.3598995 -4.2298045 -4.0742211 -3.9459131 -3.9483895 -4.0929651 -4.2692909 -4.4022622 -4.4888592 -4.5430517 -4.5731459 -4.5867043][-4.4725742 -4.370688 -4.2782712 -4.1881523 -4.0539908 -3.896502 -3.7551768 -3.7268996 -3.8555069 -4.0543165 -4.2327232 -4.3472109 -4.4023552 -4.4239073 -4.447556][-4.3947163 -4.2958131 -4.2131462 -4.1281786 -4.0014777 -3.8508081 -3.7070055 -3.6527905 -3.743721 -3.9232879 -4.1096945 -4.2381606 -4.2978177 -4.319994 -4.3584313][-4.4047551 -4.3306165 -4.2688417 -4.1915965 -4.0763578 -3.9401503 -3.8055892 -3.7458591 -3.807194 -3.9348078 -4.0789018 -4.1914768 -4.2542548 -4.2917786 -4.3491292][-4.437191 -4.3843217 -4.3434734 -4.2805471 -4.1854253 -4.0663514 -3.9413502 -3.8786938 -3.910907 -3.9788871 -4.0710998 -4.1736317 -4.2571616 -4.3243351 -4.4045711][-4.4524093 -4.4045205 -4.3730888 -4.3304353 -4.2638946 -4.1686635 -4.0695581 -4.0179892 -4.0281296 -4.0401688 -4.0788364 -4.1716995 -4.2760353 -4.3731117 -4.4705315][-4.5100465 -4.4560237 -4.414844 -4.3789558 -4.3336353 -4.2667184 -4.2097864 -4.1911626 -4.1978345 -4.1710968 -4.1594586 -4.2299461 -4.3379221 -4.4501843 -4.5538349][-4.5852246 -4.5307212 -4.4797225 -4.44357 -4.4150882 -4.3824039 -4.3680439 -4.3820014 -4.3950391 -4.349668 -4.3014355 -4.3364596 -4.4243298 -4.5291634 -4.624259][-4.641767 -4.60285 -4.5613656 -4.5334563 -4.5224094 -4.519896 -4.5352359 -4.5671163 -4.5868511 -4.5469041 -4.4918475 -4.4981179 -4.5509715 -4.6190648 -4.6767378][-4.6604238 -4.6487827 -4.6290445 -4.6184087 -4.6227841 -4.6349077 -4.6559215 -4.6846857 -4.7039862 -4.6818695 -4.6418276 -4.6338177 -4.6521955 -4.6756444 -4.6886082]]...]
INFO - root - 2017-12-07 05:44:12.681825: step 2510, loss = 2.03, batch loss = 1.95 (11.1 examples/sec; 0.720 sec/batch; 65h:59m:00s remains)
INFO - root - 2017-12-07 05:44:19.784649: step 2520, loss = 2.01, batch loss = 1.93 (12.0 examples/sec; 0.666 sec/batch; 61h:04m:30s remains)
INFO - root - 2017-12-07 05:44:26.946710: step 2530, loss = 2.03, batch loss = 1.95 (11.1 examples/sec; 0.723 sec/batch; 66h:17m:14s remains)
INFO - root - 2017-12-07 05:44:34.041445: step 2540, loss = 1.97, batch loss = 1.88 (10.6 examples/sec; 0.757 sec/batch; 69h:22m:03s remains)
INFO - root - 2017-12-07 05:44:41.062273: step 2550, loss = 2.01, batch loss = 1.93 (11.3 examples/sec; 0.710 sec/batch; 65h:03m:54s remains)
INFO - root - 2017-12-07 05:44:47.856236: step 2560, loss = 2.04, batch loss = 1.96 (11.5 examples/sec; 0.697 sec/batch; 63h:53m:15s remains)
INFO - root - 2017-12-07 05:44:54.852832: step 2570, loss = 2.03, batch loss = 1.94 (11.9 examples/sec; 0.670 sec/batch; 61h:23m:30s remains)
INFO - root - 2017-12-07 05:45:01.916656: step 2580, loss = 2.02, batch loss = 1.94 (11.7 examples/sec; 0.684 sec/batch; 62h:42m:54s remains)
INFO - root - 2017-12-07 05:45:08.967112: step 2590, loss = 1.99, batch loss = 1.91 (11.6 examples/sec; 0.687 sec/batch; 62h:57m:49s remains)
INFO - root - 2017-12-07 05:45:16.124568: step 2600, loss = 2.04, batch loss = 1.95 (11.4 examples/sec; 0.701 sec/batch; 64h:16m:19s remains)
2017-12-07 05:45:16.877576: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3915696 -4.363174 -4.3827715 -4.4482517 -4.5075727 -4.5108695 -4.4885955 -4.483696 -4.5032907 -4.523181 -4.5328689 -4.5277939 -4.52385 -4.5391903 -4.5615153][-4.2970138 -4.2676353 -4.3031087 -4.3844137 -4.4418106 -4.4243259 -4.3828917 -4.3830509 -4.419014 -4.449379 -4.4638052 -4.4562926 -4.4537797 -4.4704561 -4.4860706][-4.1764421 -4.1583953 -4.2187409 -4.3128881 -4.3632526 -4.325047 -4.2671332 -4.2706032 -4.3197365 -4.3582025 -4.3730464 -4.3594842 -4.350338 -4.3525982 -4.3406672][-4.067522 -4.0711789 -4.1585822 -4.2572536 -4.2928953 -4.2369456 -4.1638885 -4.168447 -4.2334518 -4.2884884 -4.3087573 -4.2873249 -4.2594852 -4.2358851 -4.1923585][-3.9868703 -4.0103827 -4.1160936 -4.2065125 -4.2134018 -4.1299286 -4.0322123 -4.0355873 -4.1284075 -4.2159691 -4.2527666 -4.226048 -4.1715808 -4.1149483 -4.0463123][-3.9643915 -3.9887152 -4.0916038 -4.1583347 -4.1227884 -4.0010796 -3.8734183 -3.8771758 -4.0084014 -4.1415925 -4.2078285 -4.190402 -4.1231942 -4.0502572 -3.9740193][-4.0128145 -4.0326648 -4.1185884 -4.1482353 -4.0614085 -3.8984642 -3.7470434 -3.7540991 -3.9198086 -4.0965896 -4.1974688 -4.2050219 -4.1462655 -4.080862 -4.0159025][-4.0934634 -4.118619 -4.1912212 -4.1938734 -4.0759363 -3.8977299 -3.7535889 -3.7649162 -3.9347732 -4.120739 -4.2305722 -4.2534504 -4.2053566 -4.1528029 -4.1092563][-4.2014532 -4.2386479 -4.3025222 -4.2943449 -4.1736255 -4.0114245 -3.8979061 -3.9073658 -4.0457592 -4.1999288 -4.2870522 -4.3083944 -4.2703261 -4.2303872 -4.2067966][-4.3280621 -4.3712254 -4.4142332 -4.387887 -4.2630463 -4.1124635 -4.0185466 -4.0202875 -4.1272674 -4.2543426 -4.3272276 -4.3514247 -4.3274765 -4.2941828 -4.2706704][-4.4185858 -4.4620242 -4.4860873 -4.444983 -4.314702 -4.1631207 -4.0690436 -4.0575266 -4.1417971 -4.2548509 -4.327703 -4.3635387 -4.3557339 -4.3201337 -4.2784772][-4.4287825 -4.4602265 -4.4746604 -4.4350595 -4.3137078 -4.1703625 -4.0779691 -4.0638762 -4.1409936 -4.2504611 -4.3303318 -4.3802495 -4.3874755 -4.35334 -4.2994242][-4.3597193 -4.3590717 -4.355823 -4.3181548 -4.2199373 -4.1086273 -4.0429478 -4.05171 -4.1427851 -4.2647214 -4.363759 -4.432045 -4.4569387 -4.4331245 -4.382205][-4.242794 -4.2105284 -4.183979 -4.1413903 -4.0730233 -4.0112553 -3.9933212 -4.0413361 -4.1527138 -4.2864618 -4.4011464 -4.4833941 -4.5209413 -4.5109773 -4.4746265][-4.1321497 -4.0826373 -4.0430632 -4.004405 -3.9728768 -3.9618895 -3.9873672 -4.0654793 -4.1859412 -4.3177624 -4.4324207 -4.5136766 -4.552001 -4.5494118 -4.5276842]]...]
INFO - root - 2017-12-07 05:45:23.866220: step 2610, loss = 2.02, batch loss = 1.94 (12.3 examples/sec; 0.650 sec/batch; 59h:34m:07s remains)
INFO - root - 2017-12-07 05:45:30.921431: step 2620, loss = 2.01, batch loss = 1.93 (10.9 examples/sec; 0.736 sec/batch; 67h:26m:30s remains)
INFO - root - 2017-12-07 05:45:37.951942: step 2630, loss = 2.00, batch loss = 1.92 (11.5 examples/sec; 0.695 sec/batch; 63h:41m:59s remains)
INFO - root - 2017-12-07 05:45:45.053087: step 2640, loss = 2.01, batch loss = 1.93 (11.5 examples/sec; 0.697 sec/batch; 63h:53m:51s remains)
INFO - root - 2017-12-07 05:45:52.086198: step 2650, loss = 2.02, batch loss = 1.94 (11.0 examples/sec; 0.729 sec/batch; 66h:47m:12s remains)
INFO - root - 2017-12-07 05:45:58.974276: step 2660, loss = 2.03, batch loss = 1.95 (11.3 examples/sec; 0.707 sec/batch; 64h:44m:43s remains)
INFO - root - 2017-12-07 05:46:06.035650: step 2670, loss = 2.06, batch loss = 1.98 (11.5 examples/sec; 0.694 sec/batch; 63h:37m:38s remains)
INFO - root - 2017-12-07 05:46:13.000118: step 2680, loss = 2.03, batch loss = 1.95 (12.3 examples/sec; 0.651 sec/batch; 59h:36m:39s remains)
INFO - root - 2017-12-07 05:46:20.087029: step 2690, loss = 2.04, batch loss = 1.96 (11.6 examples/sec; 0.689 sec/batch; 63h:04m:38s remains)
INFO - root - 2017-12-07 05:46:27.136335: step 2700, loss = 2.00, batch loss = 1.92 (11.1 examples/sec; 0.719 sec/batch; 65h:52m:13s remains)
2017-12-07 05:46:27.939813: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2542481 -4.2698617 -4.3190327 -4.3724914 -4.3965807 -4.4112272 -4.436552 -4.4573913 -4.4616189 -4.4571176 -4.4530172 -4.4481649 -4.4407868 -4.4294138 -4.4138985][-4.2219729 -4.2652674 -4.3369107 -4.4076576 -4.4450331 -4.468905 -4.4993029 -4.5185561 -4.5199752 -4.5114183 -4.50124 -4.4891653 -4.4746971 -4.4558039 -4.4338288][-4.2839284 -4.3500051 -4.4164929 -4.4709854 -4.4964795 -4.5120893 -4.5369124 -4.5551739 -4.5601525 -4.5567141 -4.5507636 -4.5412254 -4.5260158 -4.5055313 -4.485323][-4.4025192 -4.4648108 -4.495234 -4.507997 -4.5028877 -4.493824 -4.5045815 -4.5242858 -4.5397773 -4.5509462 -4.5628619 -4.5694323 -4.5576372 -4.535387 -4.52162][-4.4894485 -4.52852 -4.5203285 -4.4959097 -4.4612727 -4.4251909 -4.420341 -4.445941 -4.4780078 -4.5081105 -4.5414815 -4.5676107 -4.5559912 -4.5220714 -4.5118952][-4.4576826 -4.4751077 -4.4488192 -4.4045606 -4.3411369 -4.2706532 -4.2453465 -4.284821 -4.3494382 -4.4117255 -4.4766574 -4.5311232 -4.5261765 -4.480515 -4.4717531][-4.3140464 -4.3249707 -4.3091788 -4.2680459 -4.1801124 -4.0655279 -4.0070472 -4.0543861 -4.15584 -4.2563052 -4.3554354 -4.4404855 -4.451992 -4.4081912 -4.4060736][-4.16022 -4.1760855 -4.1804624 -4.1540651 -4.0488949 -3.888489 -3.7888269 -3.8281972 -3.9510634 -4.0784512 -4.199358 -4.3000717 -4.3279548 -4.3006573 -4.3108683][-4.0878453 -4.1066103 -4.1276045 -4.11847 -4.0111122 -3.8274236 -3.6961708 -3.7160532 -3.8373547 -3.9740644 -4.1003227 -4.2015924 -4.2415953 -4.2289009 -4.2342319][-4.1558647 -4.1717181 -4.2122264 -4.2277346 -4.1422763 -3.9749074 -3.8414552 -3.8356156 -3.9249802 -4.0390954 -4.1425071 -4.2210131 -4.25669 -4.2414875 -4.21687][-4.3379469 -4.3418288 -4.3890638 -4.4167919 -4.3594375 -4.2372136 -4.1282697 -4.1041718 -4.1522894 -4.2230954 -4.2870841 -4.3342528 -4.3530769 -4.318078 -4.2466979][-4.4839053 -4.4703627 -4.5012164 -4.521018 -4.48727 -4.4176083 -4.3430624 -4.311058 -4.3285141 -4.3589125 -4.3863268 -4.4054971 -4.4014425 -4.3460422 -4.2497439][-4.4762907 -4.4486175 -4.4478393 -4.4456806 -4.4240513 -4.3936815 -4.3465338 -4.3155837 -4.317822 -4.3200994 -4.3270383 -4.3436003 -4.3436384 -4.2996359 -4.2254858][-4.4077854 -4.3701591 -4.3317995 -4.3018241 -4.2838106 -4.2789249 -4.2535529 -4.2337022 -4.24133 -4.2395048 -4.246696 -4.2782955 -4.3025546 -4.2930584 -4.2582169][-4.3582258 -4.3154268 -4.2513733 -4.2039123 -4.195291 -4.2169185 -4.2184849 -4.2206464 -4.2443438 -4.2524123 -4.2634192 -4.3007531 -4.3425159 -4.3627577 -4.3512959]]...]
INFO - root - 2017-12-07 05:46:35.002325: step 2710, loss = 2.05, batch loss = 1.97 (11.8 examples/sec; 0.680 sec/batch; 62h:18m:10s remains)
INFO - root - 2017-12-07 05:46:42.016137: step 2720, loss = 2.04, batch loss = 1.96 (12.1 examples/sec; 0.661 sec/batch; 60h:30m:42s remains)
INFO - root - 2017-12-07 05:46:49.059643: step 2730, loss = 1.97, batch loss = 1.89 (11.0 examples/sec; 0.727 sec/batch; 66h:37m:18s remains)
INFO - root - 2017-12-07 05:46:56.075321: step 2740, loss = 2.05, batch loss = 1.97 (11.0 examples/sec; 0.729 sec/batch; 66h:46m:49s remains)
INFO - root - 2017-12-07 05:47:03.154323: step 2750, loss = 1.99, batch loss = 1.91 (10.9 examples/sec; 0.735 sec/batch; 67h:19m:32s remains)
INFO - root - 2017-12-07 05:47:10.200488: step 2760, loss = 2.01, batch loss = 1.93 (11.8 examples/sec; 0.677 sec/batch; 61h:58m:07s remains)
INFO - root - 2017-12-07 05:47:17.299923: step 2770, loss = 2.01, batch loss = 1.92 (12.1 examples/sec; 0.659 sec/batch; 60h:23m:57s remains)
INFO - root - 2017-12-07 05:47:24.336968: step 2780, loss = 1.98, batch loss = 1.90 (11.8 examples/sec; 0.680 sec/batch; 62h:14m:31s remains)
INFO - root - 2017-12-07 05:47:31.450020: step 2790, loss = 2.05, batch loss = 1.97 (10.9 examples/sec; 0.731 sec/batch; 66h:58m:09s remains)
INFO - root - 2017-12-07 05:47:38.458930: step 2800, loss = 2.06, batch loss = 1.97 (11.1 examples/sec; 0.723 sec/batch; 66h:14m:32s remains)
2017-12-07 05:47:39.177955: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4510689 -4.4974022 -4.4932246 -4.4868841 -4.4860296 -4.4657383 -4.408124 -4.3260355 -4.29669 -4.3721819 -4.4641781 -4.5344057 -4.6008968 -4.6209617 -4.5547991][-4.3726387 -4.3974886 -4.397 -4.3964763 -4.3985839 -4.3897085 -4.3627329 -4.3098917 -4.290431 -4.3595734 -4.45117 -4.5281978 -4.5932012 -4.6076136 -4.54684][-4.3302989 -4.3285313 -4.3155851 -4.3016572 -4.2901058 -4.27969 -4.268877 -4.2434912 -4.2422805 -4.3263826 -4.4394517 -4.5359025 -4.608077 -4.6214151 -4.56346][-4.3309107 -4.3223281 -4.2910171 -4.246254 -4.2040825 -4.1766944 -4.164474 -4.1461954 -4.1510081 -4.2530541 -4.3987122 -4.5302587 -4.6303077 -4.6562505 -4.5997505][-4.3550191 -4.35961 -4.3274374 -4.2622123 -4.19385 -4.1411958 -4.1062074 -4.0618224 -4.0475206 -4.1545973 -4.3278842 -4.4990764 -4.6414733 -4.6935124 -4.6389651][-4.3630781 -4.3736229 -4.3482671 -4.276927 -4.1913762 -4.1148653 -4.0509157 -3.967011 -3.9287617 -4.0414677 -4.2411413 -4.4510984 -4.6334348 -4.7140255 -4.6623344][-4.3330569 -4.3347082 -4.3058133 -4.22869 -4.1265807 -4.0258665 -3.936069 -3.8247128 -3.7854137 -3.9216666 -4.1536832 -4.3983469 -4.6089029 -4.705585 -4.656136][-4.3029137 -4.2736 -4.2234941 -4.1460457 -4.0410647 -3.9255075 -3.8265996 -3.712846 -3.6833811 -3.8426707 -4.0964255 -4.3590364 -4.57592 -4.6707506 -4.622952][-4.2914066 -4.2119722 -4.12842 -4.0703645 -4.0002 -3.90263 -3.8184617 -3.7340019 -3.7246437 -3.8843055 -4.1241226 -4.3673673 -4.5595765 -4.6330647 -4.5848894][-4.2847137 -4.1527219 -4.0443139 -4.0112391 -3.991065 -3.9388287 -3.8913684 -3.8510106 -3.8722856 -4.0326281 -4.2412791 -4.4349012 -4.5758543 -4.6127896 -4.55938][-4.2379284 -4.0677648 -3.9574773 -3.9467115 -3.9766459 -3.9812207 -3.9738247 -3.9694774 -4.0188603 -4.184516 -4.3665905 -4.5116429 -4.6042056 -4.61156 -4.553143][-4.1680422 -3.9801342 -3.8859971 -3.8947 -3.963043 -4.0222516 -4.0469441 -4.05694 -4.117198 -4.2742949 -4.42957 -4.5407748 -4.6071734 -4.6091018 -4.5586462][-4.1472578 -3.9635527 -3.8971839 -3.9289956 -4.0219126 -4.1128707 -4.1461411 -4.1451783 -4.1964192 -4.328855 -4.4544377 -4.5353642 -4.58232 -4.5909204 -4.5576296][-4.2494278 -4.0682368 -4.0249553 -4.0830836 -4.1891651 -4.27982 -4.2963958 -4.2717328 -4.2992954 -4.3905859 -4.4737415 -4.5244184 -4.5516887 -4.56286 -4.54217][-4.4610868 -4.2993064 -4.2612233 -4.3202271 -4.410419 -4.4726615 -4.4659705 -4.4261951 -4.4325185 -4.4829969 -4.5170918 -4.5319633 -4.5341048 -4.5334735 -4.5135779]]...]
INFO - root - 2017-12-07 05:47:46.208570: step 2810, loss = 2.02, batch loss = 1.94 (12.4 examples/sec; 0.647 sec/batch; 59h:14m:44s remains)
INFO - root - 2017-12-07 05:47:53.182104: step 2820, loss = 2.02, batch loss = 1.93 (11.5 examples/sec; 0.693 sec/batch; 63h:29m:05s remains)
INFO - root - 2017-12-07 05:48:00.218915: step 2830, loss = 2.02, batch loss = 1.94 (11.2 examples/sec; 0.713 sec/batch; 65h:15m:09s remains)
INFO - root - 2017-12-07 05:48:07.274099: step 2840, loss = 1.96, batch loss = 1.87 (10.9 examples/sec; 0.734 sec/batch; 67h:13m:08s remains)
INFO - root - 2017-12-07 05:48:14.283571: step 2850, loss = 2.02, batch loss = 1.93 (11.1 examples/sec; 0.721 sec/batch; 66h:00m:00s remains)
INFO - root - 2017-12-07 05:48:21.287804: step 2860, loss = 2.01, batch loss = 1.93 (10.8 examples/sec; 0.743 sec/batch; 68h:03m:00s remains)
INFO - root - 2017-12-07 05:48:28.314623: step 2870, loss = 2.03, batch loss = 1.95 (11.3 examples/sec; 0.707 sec/batch; 64h:43m:00s remains)
INFO - root - 2017-12-07 05:48:35.249463: step 2880, loss = 2.02, batch loss = 1.94 (12.1 examples/sec; 0.663 sec/batch; 60h:41m:23s remains)
INFO - root - 2017-12-07 05:48:42.323326: step 2890, loss = 2.00, batch loss = 1.91 (11.9 examples/sec; 0.670 sec/batch; 61h:23m:03s remains)
INFO - root - 2017-12-07 05:48:49.301289: step 2900, loss = 2.00, batch loss = 1.92 (11.6 examples/sec; 0.689 sec/batch; 63h:05m:39s remains)
2017-12-07 05:48:50.110028: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3842635 -4.3732729 -4.3663392 -4.35619 -4.3263631 -4.2860126 -4.2341866 -4.1841092 -4.1876116 -4.2578945 -4.3583589 -4.4152269 -4.3887444 -4.3166046 -4.2736869][-4.3565488 -4.3184247 -4.3027453 -4.3085613 -4.3129945 -4.3085976 -4.2720461 -4.2108717 -4.1961322 -4.266017 -4.37791 -4.44802 -4.43162 -4.3617477 -4.3123646][-4.3422236 -4.2875142 -4.2640843 -4.2771254 -4.3013697 -4.3206754 -4.299159 -4.236239 -4.210135 -4.272018 -4.3803158 -4.4578176 -4.4577127 -4.3991485 -4.3514924][-4.3526444 -4.299922 -4.272573 -4.2795744 -4.2991638 -4.3153691 -4.2941442 -4.2353354 -4.2097907 -4.264781 -4.3637905 -4.4403477 -4.4503169 -4.4053588 -4.3650622][-4.3780184 -4.3438869 -4.3191304 -4.3106961 -4.3054938 -4.2955108 -4.2611141 -4.2099385 -4.1964631 -4.2529178 -4.3457732 -4.4145327 -4.4207683 -4.37408 -4.3337975][-4.4001751 -4.3842578 -4.3644023 -4.3422155 -4.3130178 -4.28082 -4.2434621 -4.2130823 -4.2246838 -4.2894683 -4.3757873 -4.4311295 -4.4248843 -4.3633852 -4.3087568][-4.4129415 -4.4001918 -4.3819237 -4.3565021 -4.3236518 -4.294394 -4.27833 -4.2810421 -4.318141 -4.3854818 -4.4540071 -4.4886785 -4.4715991 -4.3984437 -4.3255444][-4.4268584 -4.4114137 -4.38867 -4.3631577 -4.3387189 -4.3255215 -4.3370085 -4.3668957 -4.4165549 -4.4762573 -4.520895 -4.5357904 -4.5168624 -4.4483089 -4.3696437][-4.444694 -4.430603 -4.3988643 -4.3688564 -4.3510466 -4.34965 -4.3699365 -4.3993983 -4.4385858 -4.481729 -4.5089035 -4.5161543 -4.5075207 -4.4614749 -4.3999939][-4.448329 -4.4440241 -4.4059958 -4.3659182 -4.3450751 -4.3460274 -4.3566232 -4.3645868 -4.3780804 -4.4013281 -4.4184241 -4.4290366 -4.43754 -4.4239631 -4.3970971][-4.438179 -4.4532652 -4.4146266 -4.3593054 -4.323266 -4.317533 -4.3170581 -4.3050981 -4.2927256 -4.29169 -4.2930641 -4.3020654 -4.3244066 -4.3448472 -4.3609886][-4.433475 -4.4580355 -4.4193273 -4.348968 -4.2920237 -4.2774181 -4.277667 -4.2642245 -4.2411942 -4.2164016 -4.1890845 -4.1817226 -4.20924 -4.2540145 -4.300909][-4.426837 -4.4462986 -4.4115672 -4.3416538 -4.2741265 -4.2514653 -4.2540479 -4.2462735 -4.2234011 -4.1800718 -4.1204906 -4.0915728 -4.1225982 -4.1776805 -4.2271881][-4.4022985 -4.4036989 -4.3773704 -4.3274355 -4.2707314 -4.2487922 -4.2565022 -4.2588673 -4.2452559 -4.1928816 -4.1059594 -4.0577121 -4.0901327 -4.1398239 -4.164238][-4.3464165 -4.3309274 -4.3207932 -4.3068547 -4.2831869 -4.2770209 -4.2957149 -4.3130012 -4.3135386 -4.2587495 -4.1513247 -4.0852427 -4.1088495 -4.143795 -4.1373863]]...]
INFO - root - 2017-12-07 05:48:57.120681: step 2910, loss = 2.02, batch loss = 1.94 (11.7 examples/sec; 0.681 sec/batch; 62h:22m:40s remains)
INFO - root - 2017-12-07 05:49:04.193010: step 2920, loss = 2.07, batch loss = 1.98 (11.5 examples/sec; 0.697 sec/batch; 63h:49m:20s remains)
INFO - root - 2017-12-07 05:49:11.210805: step 2930, loss = 2.09, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 63h:32m:36s remains)
INFO - root - 2017-12-07 05:49:18.277876: step 2940, loss = 1.99, batch loss = 1.91 (11.2 examples/sec; 0.712 sec/batch; 65h:08m:43s remains)
INFO - root - 2017-12-07 05:49:25.357054: step 2950, loss = 2.04, batch loss = 1.96 (11.4 examples/sec; 0.701 sec/batch; 64h:09m:39s remains)
INFO - root - 2017-12-07 05:49:32.346286: step 2960, loss = 2.06, batch loss = 1.97 (12.0 examples/sec; 0.667 sec/batch; 61h:03m:44s remains)
INFO - root - 2017-12-07 05:49:39.317104: step 2970, loss = 2.03, batch loss = 1.95 (12.4 examples/sec; 0.645 sec/batch; 59h:04m:23s remains)
INFO - root - 2017-12-07 05:49:46.394585: step 2980, loss = 2.00, batch loss = 1.92 (11.2 examples/sec; 0.715 sec/batch; 65h:28m:11s remains)
INFO - root - 2017-12-07 05:49:53.279915: step 2990, loss = 2.01, batch loss = 1.93 (12.0 examples/sec; 0.668 sec/batch; 61h:06m:00s remains)
INFO - root - 2017-12-07 05:50:00.374037: step 3000, loss = 2.06, batch loss = 1.97 (11.1 examples/sec; 0.722 sec/batch; 66h:06m:44s remains)
2017-12-07 05:50:01.092285: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3780975 -4.37117 -4.3642297 -4.3537478 -4.3396297 -4.3239231 -4.3125982 -4.3085866 -4.3090973 -4.3093281 -4.3068638 -4.3016539 -4.2961707 -4.2921386 -4.2879372][-4.4787164 -4.4818 -4.4752636 -4.4550352 -4.4272523 -4.3999019 -4.38148 -4.3744154 -4.3729534 -4.3700476 -4.3636417 -4.3541822 -4.3434629 -4.332603 -4.3221531][-4.5599122 -4.5988688 -4.6081605 -4.581615 -4.5341325 -4.484283 -4.4489479 -4.4347959 -4.4375653 -4.4454188 -4.4494038 -4.4446678 -4.4294686 -4.406486 -4.3825574][-4.5510516 -4.6490893 -4.6970158 -4.6757827 -4.6059551 -4.5199251 -4.4495888 -4.4174333 -4.4312792 -4.475419 -4.5220532 -4.5476136 -4.5407348 -4.5068445 -4.4646392][-4.4186049 -4.576467 -4.6754861 -4.6730819 -4.585907 -4.4581008 -4.3391862 -4.27492 -4.2968321 -4.392755 -4.5102868 -4.5975165 -4.6245351 -4.5954089 -4.5390244][-4.2174892 -4.4202137 -4.5636392 -4.58519 -4.4896293 -4.3223867 -4.1495767 -4.0422411 -4.0583291 -4.195447 -4.3847971 -4.5469465 -4.6296787 -4.6237831 -4.5576754][-4.0484056 -4.2680788 -4.4273653 -4.4574509 -4.3492212 -4.1412082 -3.9177914 -3.7769089 -3.7891817 -3.9545033 -4.196291 -4.4182982 -4.5521841 -4.5740342 -4.5013814][-3.9718022 -4.1922 -4.3345423 -4.3511205 -4.2238464 -3.9779801 -3.7136021 -3.5545359 -3.57549 -3.7684941 -4.0421524 -4.2882485 -4.4388118 -4.468154 -4.3816013][-4.0055881 -4.2128735 -4.3302255 -4.3300452 -4.1915073 -3.9292386 -3.6455381 -3.4779036 -3.5047426 -3.7136497 -3.9967675 -4.23025 -4.3547106 -4.3595653 -4.2447295][-4.1051812 -4.2812405 -4.3757687 -4.370296 -4.2414641 -3.9994538 -3.7304716 -3.5681548 -3.5960627 -3.7954552 -4.05856 -4.2589345 -4.3427405 -4.3124881 -4.1660457][-4.2382827 -4.3636985 -4.43207 -4.4302387 -4.3346534 -4.1509972 -3.9369779 -3.798104 -3.8127077 -3.9693537 -4.1782813 -4.3313141 -4.3850455 -4.3396721 -4.1841316][-4.3670835 -4.4382024 -4.48824 -4.5028133 -4.4545522 -4.3439336 -4.199883 -4.0944872 -4.0914879 -4.1885891 -4.3249769 -4.4232912 -4.453196 -4.4069047 -4.266654][-4.4483023 -4.4792333 -4.5210924 -4.559391 -4.5631618 -4.5207205 -4.4392385 -4.3603673 -4.3360124 -4.3764858 -4.4489021 -4.5016036 -4.5164742 -4.4791679 -4.3728137][-4.4692359 -4.4708142 -4.5021825 -4.5530572 -4.5942445 -4.6049442 -4.5767264 -4.5237756 -4.4834118 -4.480011 -4.5093079 -4.5392866 -4.5578318 -4.5464272 -4.4860706][-4.4264851 -4.4097476 -4.427598 -4.4740129 -4.5293612 -4.5722947 -4.5833411 -4.5553112 -4.5126524 -4.49031 -4.5037847 -4.5321841 -4.565433 -4.5857921 -4.5742197]]...]
INFO - root - 2017-12-07 05:50:08.183782: step 3010, loss = 2.02, batch loss = 1.94 (11.3 examples/sec; 0.709 sec/batch; 64h:53m:22s remains)
INFO - root - 2017-12-07 05:50:15.215918: step 3020, loss = 2.02, batch loss = 1.93 (11.4 examples/sec; 0.701 sec/batch; 64h:08m:42s remains)
INFO - root - 2017-12-07 05:50:22.202324: step 3030, loss = 2.01, batch loss = 1.93 (11.3 examples/sec; 0.706 sec/batch; 64h:38m:01s remains)
INFO - root - 2017-12-07 05:50:29.215471: step 3040, loss = 2.03, batch loss = 1.95 (10.7 examples/sec; 0.747 sec/batch; 68h:20m:04s remains)
INFO - root - 2017-12-07 05:50:36.261125: step 3050, loss = 1.98, batch loss = 1.90 (11.0 examples/sec; 0.730 sec/batch; 66h:49m:04s remains)
INFO - root - 2017-12-07 05:50:43.255318: step 3060, loss = 2.01, batch loss = 1.93 (11.4 examples/sec; 0.703 sec/batch; 64h:20m:59s remains)
INFO - root - 2017-12-07 05:50:50.343927: step 3070, loss = 2.04, batch loss = 1.96 (11.7 examples/sec; 0.685 sec/batch; 62h:39m:50s remains)
INFO - root - 2017-12-07 05:50:57.148206: step 3080, loss = 2.10, batch loss = 2.02 (11.2 examples/sec; 0.716 sec/batch; 65h:29m:10s remains)
INFO - root - 2017-12-07 05:51:04.233287: step 3090, loss = 2.02, batch loss = 1.94 (10.6 examples/sec; 0.753 sec/batch; 68h:55m:19s remains)
INFO - root - 2017-12-07 05:51:11.298148: step 3100, loss = 2.06, batch loss = 1.97 (11.1 examples/sec; 0.718 sec/batch; 65h:43m:49s remains)
2017-12-07 05:51:12.081538: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5593219 -4.5719662 -4.5922337 -4.6140618 -4.6310015 -4.6420116 -4.6544447 -4.6722097 -4.6899843 -4.7056923 -4.7241912 -4.7409649 -4.7387009 -4.7028518 -4.6502872][-4.6247096 -4.6589265 -4.6956916 -4.7265072 -4.7435431 -4.747591 -4.7491541 -4.7605314 -4.7793159 -4.8003087 -4.826683 -4.84763 -4.8374467 -4.7719488 -4.6713991][-4.6161933 -4.67732 -4.7303853 -4.7643428 -4.7713361 -4.7542543 -4.7296548 -4.7227325 -4.7390404 -4.7709265 -4.8151808 -4.8503904 -4.842968 -4.7625594 -4.62529][-4.5579481 -4.6488085 -4.7128086 -4.7371469 -4.717742 -4.6607189 -4.5916758 -4.5542808 -4.5724874 -4.6322613 -4.7115164 -4.7699256 -4.7740645 -4.7029686 -4.559556][-4.4778876 -4.5938187 -4.6609364 -4.6648073 -4.6070886 -4.4981055 -4.3731275 -4.2991147 -4.3255672 -4.4337611 -4.5652103 -4.6460423 -4.6571388 -4.6133375 -4.5055146][-4.4260364 -4.5478153 -4.6079016 -4.58724 -4.492003 -4.3357725 -4.1589484 -4.0463595 -4.0772848 -4.2360086 -4.4169555 -4.5089917 -4.5151567 -4.50169 -4.4547577][-4.4060349 -4.5130754 -4.5543003 -4.506537 -4.382091 -4.1969852 -3.9862351 -3.8413279 -3.869823 -4.0625048 -4.2741146 -4.3658233 -4.3642988 -4.3748016 -4.3913493][-4.4140468 -4.4940286 -4.5085158 -4.4376788 -4.3009305 -4.1094651 -3.8864713 -3.7217519 -3.7446249 -3.9535837 -4.1786113 -4.2729564 -4.2777858 -4.3057494 -4.3658867][-4.4513869 -4.5065684 -4.4991884 -4.418745 -4.2887163 -4.1117625 -3.8985381 -3.73079 -3.7486312 -3.951407 -4.1661444 -4.2651992 -4.2848487 -4.3220754 -4.3992553][-4.4979239 -4.5409627 -4.5288959 -4.458138 -4.3514185 -4.2048373 -4.0232892 -3.874517 -3.8888159 -4.06355 -4.2477117 -4.3431349 -4.3693271 -4.4013228 -4.4712224][-4.5321064 -4.5765581 -4.5770116 -4.5310216 -4.4565558 -4.3480024 -4.2113533 -4.0965447 -4.1082487 -4.243175 -4.3864775 -4.4674435 -4.4861436 -4.4982395 -4.5437617][-4.5212035 -4.5690403 -4.587471 -4.5739727 -4.5380764 -4.4731364 -4.3896718 -4.3182411 -4.3293467 -4.4195819 -4.5176029 -4.5762234 -4.5785556 -4.5633454 -4.577116][-4.4578257 -4.5012841 -4.5316877 -4.5460382 -4.5489745 -4.5301294 -4.498683 -4.4709578 -4.4840617 -4.5356584 -4.5908532 -4.6233544 -4.6106672 -4.5780025 -4.5703721][-4.3807955 -4.4099083 -4.4398417 -4.4699841 -4.5009308 -4.5178208 -4.5242996 -4.5278411 -4.5432048 -4.568181 -4.59008 -4.5982304 -4.5783925 -4.5486841 -4.5439086][-4.3204489 -4.337132 -4.3620334 -4.3956928 -4.4360991 -4.465703 -4.4860506 -4.5023394 -4.5185862 -4.5286403 -4.527626 -4.5180387 -4.4990358 -4.486237 -4.5007324]]...]
INFO - root - 2017-12-07 05:51:19.105155: step 3110, loss = 2.00, batch loss = 1.91 (11.3 examples/sec; 0.709 sec/batch; 64h:52m:51s remains)
INFO - root - 2017-12-07 05:51:26.196483: step 3120, loss = 2.02, batch loss = 1.93 (10.3 examples/sec; 0.774 sec/batch; 70h:46m:18s remains)
INFO - root - 2017-12-07 05:51:33.332958: step 3130, loss = 2.07, batch loss = 1.99 (11.1 examples/sec; 0.722 sec/batch; 66h:02m:10s remains)
INFO - root - 2017-12-07 05:51:40.366912: step 3140, loss = 2.02, batch loss = 1.93 (11.7 examples/sec; 0.682 sec/batch; 62h:22m:33s remains)
INFO - root - 2017-12-07 05:51:47.315830: step 3150, loss = 1.99, batch loss = 1.91 (11.0 examples/sec; 0.727 sec/batch; 66h:29m:49s remains)
INFO - root - 2017-12-07 05:51:54.416309: step 3160, loss = 2.00, batch loss = 1.92 (11.8 examples/sec; 0.677 sec/batch; 61h:56m:19s remains)
INFO - root - 2017-12-07 05:52:01.500505: step 3170, loss = 2.04, batch loss = 1.96 (11.5 examples/sec; 0.694 sec/batch; 63h:26m:50s remains)
INFO - root - 2017-12-07 05:52:08.449341: step 3180, loss = 2.08, batch loss = 2.00 (10.7 examples/sec; 0.745 sec/batch; 68h:08m:40s remains)
INFO - root - 2017-12-07 05:52:15.424712: step 3190, loss = 2.04, batch loss = 1.96 (11.5 examples/sec; 0.696 sec/batch; 63h:37m:52s remains)
INFO - root - 2017-12-07 05:52:22.406977: step 3200, loss = 2.00, batch loss = 1.92 (11.6 examples/sec; 0.692 sec/batch; 63h:16m:40s remains)
2017-12-07 05:52:23.240659: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.550879 -4.569838 -4.5835314 -4.6056256 -4.6488395 -4.6921053 -4.6983004 -4.6764007 -4.6472335 -4.6392393 -4.6627159 -4.6914587 -4.6888466 -4.6320491 -4.5274477][-4.6893659 -4.7154908 -4.7104535 -4.7110624 -4.7569857 -4.8114085 -4.801755 -4.7511044 -4.7071767 -4.7169552 -4.7894197 -4.8652186 -4.8825788 -4.8096843 -4.6572022][-4.806057 -4.827251 -4.7843652 -4.7391906 -4.7678714 -4.8230557 -4.790966 -4.7083273 -4.6484828 -4.6825943 -4.818315 -4.9622068 -5.0184445 -4.9460111 -4.759407][-4.8663168 -4.8533506 -4.7556596 -4.6563497 -4.66079 -4.70881 -4.6604867 -4.5589066 -4.4860554 -4.535109 -4.7231317 -4.9315233 -5.0382957 -4.9928169 -4.8044972][-4.8436 -4.7698631 -4.619081 -4.4856114 -4.4770184 -4.5106859 -4.4447751 -4.3222637 -4.2170196 -4.2442007 -4.4503975 -4.7102709 -4.8890371 -4.9157181 -4.7728825][-4.7256231 -4.5929976 -4.4187388 -4.2836428 -4.2741122 -4.2896948 -4.201993 -4.050395 -3.9019723 -3.8909957 -4.0958991 -4.4012384 -4.6643991 -4.7821865 -4.702281][-4.5714893 -4.4152727 -4.2550573 -4.1387072 -4.123816 -4.1137404 -4.0074596 -3.8358698 -3.6607285 -3.6256042 -3.8248675 -4.1591296 -4.4873185 -4.6787028 -4.6481204][-4.4843063 -4.3432369 -4.2119713 -4.1000261 -4.0555773 -4.0105686 -3.8960865 -3.7359071 -3.5703292 -3.5372596 -3.7377596 -4.0879574 -4.4491029 -4.6702733 -4.6518888][-4.5334759 -4.4316425 -4.3273616 -4.2040405 -4.1202273 -4.0474467 -3.9419289 -3.8241544 -3.7013347 -3.6945987 -3.8991601 -4.2407026 -4.5806723 -4.7712398 -4.7236972][-4.6964474 -4.6403532 -4.5517726 -4.4175448 -4.3098946 -4.2336774 -4.1587877 -4.097156 -4.0240073 -4.0358291 -4.2196655 -4.5080943 -4.7739315 -4.8931885 -4.8014922][-4.8008261 -4.7807269 -4.7078981 -4.5902538 -4.5040178 -4.4660645 -4.4417248 -4.4295349 -4.3897505 -4.3999434 -4.5384264 -4.7446485 -4.9108672 -4.9500189 -4.8262377][-4.7822223 -4.79577 -4.7538004 -4.6788173 -4.6416745 -4.6598763 -4.688302 -4.709991 -4.6881342 -4.686914 -4.7689371 -4.8862591 -4.9575405 -4.9302063 -4.7900476][-4.6879683 -4.7182384 -4.7011194 -4.66527 -4.6673179 -4.7199903 -4.774857 -4.8062296 -4.7927775 -4.7817631 -4.8160567 -4.8647041 -4.8741965 -4.81434 -4.6831708][-4.5424819 -4.5682526 -4.56568 -4.5563831 -4.574316 -4.6225586 -4.6645012 -4.6846237 -4.678504 -4.6720662 -4.6846452 -4.6995592 -4.686121 -4.6300468 -4.5327625][-4.4105697 -4.4252667 -4.4276185 -4.4276242 -4.4398746 -4.4632821 -4.4806294 -4.4875193 -4.4860468 -4.4862347 -4.4921594 -4.4952412 -4.4818292 -4.4459271 -4.3892932]]...]
INFO - root - 2017-12-07 05:52:30.295883: step 3210, loss = 2.02, batch loss = 1.94 (10.9 examples/sec; 0.731 sec/batch; 66h:50m:08s remains)
INFO - root - 2017-12-07 05:52:37.300331: step 3220, loss = 2.02, batch loss = 1.94 (11.4 examples/sec; 0.700 sec/batch; 64h:00m:03s remains)
INFO - root - 2017-12-07 05:52:44.410515: step 3230, loss = 2.02, batch loss = 1.94 (11.9 examples/sec; 0.670 sec/batch; 61h:14m:35s remains)
INFO - root - 2017-12-07 05:52:51.390179: step 3240, loss = 2.00, batch loss = 1.92 (12.7 examples/sec; 0.631 sec/batch; 57h:44m:27s remains)
INFO - root - 2017-12-07 05:52:58.399963: step 3250, loss = 2.01, batch loss = 1.93 (11.8 examples/sec; 0.680 sec/batch; 62h:13m:14s remains)
INFO - root - 2017-12-07 05:53:05.472271: step 3260, loss = 2.02, batch loss = 1.94 (11.6 examples/sec; 0.688 sec/batch; 62h:55m:47s remains)
INFO - root - 2017-12-07 05:53:12.553805: step 3270, loss = 2.08, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 64h:32m:33s remains)
INFO - root - 2017-12-07 05:53:19.968052: step 3280, loss = 1.95, batch loss = 1.87 (9.6 examples/sec; 0.834 sec/batch; 76h:17m:28s remains)
INFO - root - 2017-12-07 05:53:28.467879: step 3290, loss = 2.01, batch loss = 1.92 (10.6 examples/sec; 0.755 sec/batch; 69h:04m:43s remains)
INFO - root - 2017-12-07 05:53:35.480941: step 3300, loss = 1.99, batch loss = 1.91 (10.6 examples/sec; 0.754 sec/batch; 68h:57m:11s remains)
2017-12-07 05:53:36.285049: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3764429 -4.4705935 -4.5306792 -4.5245433 -4.4743819 -4.4701953 -4.5651836 -4.7029948 -4.7926593 -4.8216786 -4.8046341 -4.7340407 -4.6336112 -4.5572395 -4.552319][-4.5252976 -4.5819135 -4.6141372 -4.6038456 -4.5695682 -4.5725846 -4.6558318 -4.7656574 -4.8258066 -4.8258505 -4.7695479 -4.6473379 -4.5004411 -4.3931255 -4.3860326][-4.597003 -4.6099238 -4.6207933 -4.6188951 -4.6116734 -4.6331935 -4.7053757 -4.7785335 -4.7980919 -4.7630696 -4.663938 -4.4909911 -4.2983289 -4.1590343 -4.1478491][-4.5874763 -4.5677776 -4.5645056 -4.5717626 -4.5874104 -4.6236634 -4.6821585 -4.7263789 -4.7235465 -4.6709189 -4.5436358 -4.3267403 -4.0822544 -3.9030385 -3.8821177][-4.5063467 -4.479104 -4.480617 -4.5008507 -4.5276761 -4.5597715 -4.5951772 -4.6238847 -4.6326871 -4.5999656 -4.4747868 -4.2289662 -3.9329491 -3.7116182 -3.6828742][-4.4321165 -4.4052944 -4.4159007 -4.4399166 -4.4541435 -4.4551435 -4.4544568 -4.4792924 -4.5272679 -4.5483513 -4.4603014 -4.22097 -3.907795 -3.6720743 -3.6410193][-4.3755231 -4.3454294 -4.3542075 -4.3671079 -4.3556581 -4.3160954 -4.2759547 -4.2923312 -4.3718982 -4.447166 -4.4191408 -4.2402525 -3.981698 -3.7928524 -3.7849355][-4.2856221 -4.2446003 -4.2379236 -4.2285557 -4.1949472 -4.1319571 -4.0739121 -4.0843878 -4.1804681 -4.2980251 -4.3398862 -4.2590241 -4.1113114 -4.01296 -4.0414767][-4.1669154 -4.1049948 -4.0705166 -4.0310445 -3.9848883 -3.9274018 -3.8882141 -3.9155023 -4.0283828 -4.1797891 -4.2883649 -4.3086491 -4.2795348 -4.268815 -4.3134809][-4.0748973 -3.9950283 -3.9377811 -3.8761516 -3.8241968 -3.7781813 -3.7611346 -3.8039427 -3.9318581 -4.1123672 -4.2751703 -4.372087 -4.4219046 -4.4564557 -4.4880457][-4.0818486 -3.9913824 -3.918798 -3.8469162 -3.7892275 -3.742485 -3.7276585 -3.7723358 -3.90969 -4.1119485 -4.3055367 -4.4275765 -4.4840894 -4.5018878 -4.498065][-4.1982975 -4.0996256 -4.0180449 -3.9530702 -3.9048314 -3.8669567 -3.8525193 -3.8887079 -4.0179281 -4.2160153 -4.403141 -4.4989681 -4.5024972 -4.4592147 -4.4085641][-4.3767385 -4.2704821 -4.1845489 -4.1388106 -4.1172209 -4.0977764 -4.0830212 -4.1040359 -4.2088118 -4.3773041 -4.5287433 -4.5749607 -4.51343 -4.4106121 -4.3267212][-4.5436788 -4.4491854 -4.3700194 -4.3437152 -4.3469858 -4.34072 -4.3189716 -4.3196335 -4.3905921 -4.5145855 -4.6159368 -4.61356 -4.5095062 -4.3796091 -4.2895937][-4.6288824 -4.5711918 -4.5193353 -4.5149822 -4.5359759 -4.53528 -4.5027618 -4.4846849 -4.5229445 -4.5961571 -4.6403489 -4.5960689 -4.4761896 -4.3510523 -4.274611]]...]
INFO - root - 2017-12-07 05:53:43.201943: step 3310, loss = 2.05, batch loss = 1.97 (14.0 examples/sec; 0.571 sec/batch; 52h:12m:31s remains)
INFO - root - 2017-12-07 05:53:50.206445: step 3320, loss = 2.07, batch loss = 1.98 (10.5 examples/sec; 0.762 sec/batch; 69h:43m:10s remains)
INFO - root - 2017-12-07 05:53:57.201780: step 3330, loss = 2.03, batch loss = 1.95 (11.1 examples/sec; 0.718 sec/batch; 65h:39m:19s remains)
INFO - root - 2017-12-07 05:54:04.170167: step 3340, loss = 2.01, batch loss = 1.92 (11.7 examples/sec; 0.682 sec/batch; 62h:18m:59s remains)
INFO - root - 2017-12-07 05:54:11.163858: step 3350, loss = 2.01, batch loss = 1.93 (11.5 examples/sec; 0.694 sec/batch; 63h:26m:07s remains)
INFO - root - 2017-12-07 05:54:18.150592: step 3360, loss = 2.00, batch loss = 1.91 (11.1 examples/sec; 0.718 sec/batch; 65h:38m:15s remains)
INFO - root - 2017-12-07 05:54:25.249224: step 3370, loss = 2.00, batch loss = 1.92 (11.1 examples/sec; 0.721 sec/batch; 65h:56m:35s remains)
INFO - root - 2017-12-07 05:54:32.261658: step 3380, loss = 1.93, batch loss = 1.85 (11.0 examples/sec; 0.725 sec/batch; 66h:16m:37s remains)
INFO - root - 2017-12-07 05:54:39.273593: step 3390, loss = 2.01, batch loss = 1.93 (11.6 examples/sec; 0.689 sec/batch; 63h:00m:32s remains)
INFO - root - 2017-12-07 05:54:46.232251: step 3400, loss = 2.00, batch loss = 1.92 (11.3 examples/sec; 0.706 sec/batch; 64h:31m:41s remains)
2017-12-07 05:54:46.953784: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2117496 -4.3200989 -4.3910656 -4.4082971 -4.4125905 -4.4431763 -4.4852343 -4.5162959 -4.5207319 -4.4770746 -4.4221015 -4.39353 -4.3594189 -4.3253894 -4.3075495][-4.14042 -4.2826366 -4.3613296 -4.3739438 -4.3857174 -4.4393821 -4.5032048 -4.5341311 -4.51528 -4.4493752 -4.3884168 -4.3634138 -4.3220811 -4.2535467 -4.2009196][-4.1764026 -4.3209829 -4.3997869 -4.4137177 -4.4365292 -4.4970527 -4.5500197 -4.5474453 -4.489665 -4.4120378 -4.3686748 -4.3611608 -4.3165646 -4.222445 -4.14781][-4.2726111 -4.3756461 -4.440465 -4.468152 -4.5098667 -4.5606213 -4.5684562 -4.5087233 -4.4153566 -4.3492546 -4.3471227 -4.3650303 -4.315876 -4.1988096 -4.1078844][-4.3472743 -4.3749728 -4.404182 -4.4449148 -4.499476 -4.5205097 -4.4653935 -4.3511934 -4.249702 -4.2308822 -4.2963119 -4.3552547 -4.3130283 -4.1812768 -4.0737982][-4.3389082 -4.3014 -4.3003616 -4.3476996 -4.3971028 -4.3747497 -4.2576923 -4.1078181 -4.02685 -4.0771613 -4.2149959 -4.3224883 -4.3067508 -4.1911626 -4.0953989][-4.2648797 -4.1937776 -4.1791797 -4.22716 -4.2661562 -4.2116146 -4.058847 -3.8997564 -3.8555455 -3.97423 -4.1734481 -4.3255014 -4.3484788 -4.2740316 -4.2131782][-4.1959271 -4.130435 -4.1132259 -4.1417775 -4.156126 -4.0809131 -3.9264987 -3.790266 -3.7901483 -3.9595232 -4.199975 -4.3816781 -4.4370985 -4.4076586 -4.3874989][-4.1988258 -4.1616654 -4.1544166 -4.1610045 -4.1476216 -4.0701265 -3.9446032 -3.853174 -3.8874106 -4.0671792 -4.2989621 -4.4613843 -4.5110273 -4.4998245 -4.503252][-4.264627 -4.2459111 -4.2590318 -4.2689314 -4.2557569 -4.1980219 -4.1150551 -4.0686541 -4.1130571 -4.2591724 -4.4276624 -4.5190258 -4.5144486 -4.4780407 -4.4745979][-4.3247275 -4.2979984 -4.3295994 -4.369658 -4.38185 -4.3519697 -4.3019361 -4.2808475 -4.3225746 -4.4213171 -4.508687 -4.5140204 -4.4405227 -4.3608985 -4.3403068][-4.3367023 -4.2831326 -4.3182487 -4.3894792 -4.4399061 -4.442832 -4.4110503 -4.3966675 -4.4297 -4.4865975 -4.5066652 -4.44375 -4.32285 -4.2193513 -4.1961493][-4.2695475 -4.1961708 -4.2296519 -4.3204651 -4.3959088 -4.4168577 -4.3898716 -4.3811874 -4.4264092 -4.4785089 -4.468565 -4.3640561 -4.215107 -4.1059494 -4.0859451][-4.1687441 -4.0878382 -4.1117997 -4.1983643 -4.27264 -4.2928567 -4.2692518 -4.2803 -4.3617496 -4.4426789 -4.4343295 -4.3105774 -4.1439528 -4.0318913 -4.0100646][-4.1389966 -4.0583096 -4.0703611 -4.1391907 -4.20177 -4.2120504 -4.1850481 -4.2060318 -4.3066549 -4.409709 -4.4117146 -4.2896929 -4.1251612 -4.0165181 -3.9909198]]...]
INFO - root - 2017-12-07 05:54:53.938534: step 3410, loss = 1.97, batch loss = 1.89 (11.7 examples/sec; 0.683 sec/batch; 62h:27m:30s remains)
INFO - root - 2017-12-07 05:55:01.010459: step 3420, loss = 2.02, batch loss = 1.94 (11.6 examples/sec; 0.691 sec/batch; 63h:12m:30s remains)
INFO - root - 2017-12-07 05:55:08.019688: step 3430, loss = 2.05, batch loss = 1.96 (11.9 examples/sec; 0.673 sec/batch; 61h:30m:39s remains)
INFO - root - 2017-12-07 05:55:15.108986: step 3440, loss = 2.00, batch loss = 1.92 (11.2 examples/sec; 0.711 sec/batch; 65h:00m:23s remains)
INFO - root - 2017-12-07 05:55:22.184481: step 3450, loss = 2.07, batch loss = 1.98 (11.4 examples/sec; 0.702 sec/batch; 64h:10m:26s remains)
INFO - root - 2017-12-07 05:55:29.201800: step 3460, loss = 2.01, batch loss = 1.93 (11.3 examples/sec; 0.711 sec/batch; 64h:59m:16s remains)
INFO - root - 2017-12-07 05:55:36.259180: step 3470, loss = 2.03, batch loss = 1.95 (10.9 examples/sec; 0.736 sec/batch; 67h:18m:40s remains)
INFO - root - 2017-12-07 05:55:43.049677: step 3480, loss = 1.99, batch loss = 1.91 (11.1 examples/sec; 0.720 sec/batch; 65h:49m:12s remains)
INFO - root - 2017-12-07 05:55:50.058317: step 3490, loss = 2.04, batch loss = 1.96 (12.1 examples/sec; 0.663 sec/batch; 60h:35m:43s remains)
INFO - root - 2017-12-07 05:55:57.021742: step 3500, loss = 1.97, batch loss = 1.89 (12.0 examples/sec; 0.667 sec/batch; 60h:57m:38s remains)
2017-12-07 05:55:57.834873: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3767576 -4.3877511 -4.4039931 -4.4173322 -4.424634 -4.4279108 -4.4279528 -4.4291124 -4.4316907 -4.4337831 -4.4389381 -4.4482913 -4.4581132 -4.4620137 -4.4494677][-4.4711862 -4.4936714 -4.5168247 -4.5303521 -4.5271053 -4.5053906 -4.470737 -4.4462619 -4.4457541 -4.465354 -4.4936428 -4.5226579 -4.5415092 -4.5433927 -4.5215259][-4.5457726 -4.579761 -4.6030293 -4.6008024 -4.5656395 -4.4966664 -4.408833 -4.3518491 -4.3567934 -4.4113669 -4.4841237 -4.55287 -4.5926075 -4.5957046 -4.5638237][-4.5799279 -4.6188765 -4.6223025 -4.5769329 -4.4896173 -4.3683376 -4.2338772 -4.1531878 -4.1693354 -4.2637377 -4.3938332 -4.522037 -4.6016884 -4.6207533 -4.584332][-4.5693765 -4.5987844 -4.5618281 -4.4595237 -4.3226538 -4.1715436 -4.02188 -3.938067 -3.9635463 -4.084105 -4.259758 -4.4435468 -4.5706825 -4.618412 -4.5886312][-4.5253053 -4.5387278 -4.4583559 -4.3036852 -4.1310081 -3.9759519 -3.8458273 -3.7822568 -3.8138659 -3.9353943 -4.1225786 -4.3261123 -4.4806476 -4.55647 -4.547286][-4.4766879 -4.4896569 -4.3902154 -4.2070823 -4.0184636 -3.8744512 -3.777977 -3.7415693 -3.775203 -3.8791552 -4.0446796 -4.2289367 -4.3759246 -4.4601316 -4.4708319][-4.4469924 -4.4767494 -4.3906107 -4.2107191 -4.0237989 -3.8918059 -3.8187163 -3.8042517 -3.8423662 -3.9275434 -4.0541468 -4.1939325 -4.3052664 -4.3707328 -4.3858128][-4.4415469 -4.4946885 -4.43754 -4.2808423 -4.1102762 -3.9916887 -3.9343176 -3.9362774 -3.9807024 -4.0471325 -4.1280165 -4.2147641 -4.2794719 -4.3117323 -4.3203568][-4.4513106 -4.5188961 -4.4869509 -4.361114 -4.2210746 -4.1302137 -4.095809 -4.1111445 -4.1559482 -4.2014461 -4.2434034 -4.2873812 -4.3103952 -4.3073306 -4.3003254][-4.4555926 -4.5267119 -4.5133405 -4.4197588 -4.3196359 -4.2689185 -4.2668509 -4.2991362 -4.3433666 -4.372026 -4.388658 -4.398531 -4.3823719 -4.3445086 -4.3135104][-4.439343 -4.5050435 -4.5048022 -4.4424248 -4.3780632 -4.3579283 -4.3812628 -4.429286 -4.4768524 -4.5004148 -4.5040359 -4.4873428 -4.4383912 -4.3761048 -4.32857][-4.407218 -4.4642224 -4.4740205 -4.4367461 -4.3939948 -4.3820653 -4.4072485 -4.45422 -4.499342 -4.5227861 -4.5222278 -4.4934511 -4.4350953 -4.3741913 -4.3302][-4.3662858 -4.4097857 -4.4210548 -4.4006262 -4.3738933 -4.3621426 -4.3744431 -4.4047222 -4.4379473 -4.4568138 -4.4560328 -4.4349551 -4.3953085 -4.3571472 -4.32978][-4.3243003 -4.345377 -4.3401985 -4.3198495 -4.3042979 -4.2997942 -4.3068509 -4.3197584 -4.3353982 -4.3464813 -4.3547812 -4.3639383 -4.3644104 -4.3583331 -4.3470531]]...]
INFO - root - 2017-12-07 05:56:04.813095: step 3510, loss = 2.02, batch loss = 1.94 (11.3 examples/sec; 0.707 sec/batch; 64h:35m:50s remains)
INFO - root - 2017-12-07 05:56:11.825963: step 3520, loss = 2.01, batch loss = 1.93 (11.7 examples/sec; 0.682 sec/batch; 62h:19m:32s remains)
INFO - root - 2017-12-07 05:56:18.913192: step 3530, loss = 2.06, batch loss = 1.97 (11.7 examples/sec; 0.684 sec/batch; 62h:32m:21s remains)
INFO - root - 2017-12-07 05:56:25.939118: step 3540, loss = 2.06, batch loss = 1.98 (11.4 examples/sec; 0.704 sec/batch; 64h:18m:52s remains)
INFO - root - 2017-12-07 05:56:33.000387: step 3550, loss = 1.97, batch loss = 1.89 (11.4 examples/sec; 0.705 sec/batch; 64h:23m:52s remains)
INFO - root - 2017-12-07 05:56:40.080347: step 3560, loss = 1.99, batch loss = 1.91 (11.0 examples/sec; 0.726 sec/batch; 66h:22m:13s remains)
INFO - root - 2017-12-07 05:56:47.052398: step 3570, loss = 1.99, batch loss = 1.91 (12.1 examples/sec; 0.663 sec/batch; 60h:32m:24s remains)
INFO - root - 2017-12-07 05:56:54.063294: step 3580, loss = 2.01, batch loss = 1.92 (12.4 examples/sec; 0.648 sec/batch; 59h:10m:31s remains)
INFO - root - 2017-12-07 05:57:01.064561: step 3590, loss = 2.02, batch loss = 1.94 (11.7 examples/sec; 0.683 sec/batch; 62h:24m:38s remains)
INFO - root - 2017-12-07 05:57:07.859409: step 3600, loss = 1.99, batch loss = 1.91 (11.5 examples/sec; 0.693 sec/batch; 63h:20m:49s remains)
2017-12-07 05:57:08.654867: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5194192 -4.5408192 -4.5132279 -4.4091759 -4.2616863 -4.1454334 -4.0986671 -4.0880585 -4.0783658 -4.1025324 -4.1711135 -4.236165 -4.2616754 -4.2564321 -4.2584505][-4.4349918 -4.4939189 -4.5175838 -4.4568858 -4.3118687 -4.1649156 -4.0903273 -4.0803437 -4.0831366 -4.1088195 -4.1682768 -4.2215962 -4.2426987 -4.2438951 -4.2444935][-4.3472075 -4.4411731 -4.5133324 -4.5020008 -4.3747683 -4.2059331 -4.0954981 -4.0690274 -4.0786562 -4.1134744 -4.1780653 -4.2377157 -4.2714133 -4.2857833 -4.2843642][-4.2723513 -4.3809466 -4.480382 -4.5058017 -4.4015808 -4.2223 -4.0825534 -4.0330715 -4.0440421 -4.0942087 -4.1803846 -4.2651649 -4.3205156 -4.3450503 -4.3372879][-4.2198982 -4.3178854 -4.4177642 -4.4551172 -4.3691349 -4.191041 -4.0393605 -3.9807491 -3.9992383 -4.0693774 -4.1810856 -4.2932572 -4.3651562 -4.389792 -4.3709869][-4.2018318 -4.27796 -4.3624496 -4.3971972 -4.3243876 -4.1564951 -4.0073757 -3.9516933 -3.9841824 -4.0715666 -4.1956787 -4.3169017 -4.3890963 -4.4068418 -4.3770094][-4.2072635 -4.2648311 -4.3355966 -4.3664942 -4.3020191 -4.145668 -4.0013089 -3.9457145 -3.9887404 -4.0899434 -4.2159872 -4.3319764 -4.3922758 -4.4001164 -4.3609624][-4.2320156 -4.2779489 -4.3431478 -4.376164 -4.3234415 -4.1861477 -4.0481339 -3.9829164 -4.0183258 -4.119791 -4.2454205 -4.3582344 -4.4097915 -4.4091382 -4.3649745][-4.2915497 -4.3284245 -4.3896022 -4.4272 -4.3925633 -4.28594 -4.1650939 -4.0892291 -4.0990696 -4.1787257 -4.2947478 -4.4046445 -4.45262 -4.4477429 -4.4052968][-4.3622384 -4.3823528 -4.4303651 -4.4681163 -4.4518285 -4.3785458 -4.2831249 -4.2085605 -4.1950817 -4.2459383 -4.3441768 -4.44942 -4.4984412 -4.4935613 -4.4578848][-4.4106894 -4.4143729 -4.4462719 -4.4836016 -4.4866004 -4.4455576 -4.3803043 -4.3199806 -4.292727 -4.3164477 -4.3913279 -4.4854321 -4.5358663 -4.5345907 -4.5079632][-4.4313364 -4.4239631 -4.4449143 -4.4850235 -4.5076704 -4.4946761 -4.4564028 -4.4111209 -4.3785486 -4.3803005 -4.4274721 -4.4998617 -4.5445914 -4.5456734 -4.5287266][-4.4187193 -4.4085793 -4.4241624 -4.46644 -4.5008769 -4.5042028 -4.4812965 -4.4447093 -4.4109764 -4.40028 -4.4281383 -4.48128 -4.5145693 -4.5143695 -4.5064049][-4.3919096 -4.3875694 -4.4054537 -4.4476705 -4.4852858 -4.4971871 -4.4852886 -4.4569063 -4.4251704 -4.4089727 -4.4240055 -4.4600434 -4.4788632 -4.473949 -4.4704061][-4.3588662 -4.3623304 -4.385211 -4.4264255 -4.4649796 -4.4853592 -4.4871 -4.4709668 -4.4451652 -4.4257278 -4.42855 -4.4469147 -4.4517326 -4.4412875 -4.4350429]]...]
INFO - root - 2017-12-07 05:57:15.666897: step 3610, loss = 2.04, batch loss = 1.96 (12.2 examples/sec; 0.655 sec/batch; 59h:53m:07s remains)
INFO - root - 2017-12-07 05:57:22.664483: step 3620, loss = 1.96, batch loss = 1.88 (11.6 examples/sec; 0.688 sec/batch; 62h:49m:09s remains)
INFO - root - 2017-12-07 05:57:29.725946: step 3630, loss = 2.04, batch loss = 1.96 (11.1 examples/sec; 0.723 sec/batch; 66h:02m:19s remains)
INFO - root - 2017-12-07 05:57:36.713749: step 3640, loss = 2.04, batch loss = 1.96 (11.8 examples/sec; 0.679 sec/batch; 62h:00m:45s remains)
INFO - root - 2017-12-07 05:57:43.606887: step 3650, loss = 2.07, batch loss = 1.98 (12.3 examples/sec; 0.650 sec/batch; 59h:24m:43s remains)
INFO - root - 2017-12-07 05:57:50.578588: step 3660, loss = 2.05, batch loss = 1.97 (11.8 examples/sec; 0.679 sec/batch; 62h:01m:46s remains)
INFO - root - 2017-12-07 05:57:57.768492: step 3670, loss = 1.99, batch loss = 1.90 (11.2 examples/sec; 0.713 sec/batch; 65h:07m:41s remains)
INFO - root - 2017-12-07 05:58:04.854679: step 3680, loss = 2.00, batch loss = 1.92 (11.2 examples/sec; 0.717 sec/batch; 65h:31m:45s remains)
INFO - root - 2017-12-07 05:58:11.761723: step 3690, loss = 2.01, batch loss = 1.93 (12.3 examples/sec; 0.651 sec/batch; 59h:28m:36s remains)
INFO - root - 2017-12-07 05:58:18.813931: step 3700, loss = 2.00, batch loss = 1.91 (10.4 examples/sec; 0.767 sec/batch; 70h:05m:08s remains)
2017-12-07 05:58:19.634575: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4200153 -4.4280472 -4.4150763 -4.3788657 -4.3729649 -4.3794341 -4.40286 -4.44043 -4.4165325 -4.3670526 -4.3497214 -4.3260918 -4.3307209 -4.3925476 -4.441][-4.3651347 -4.3714876 -4.3705688 -4.3524165 -4.3589578 -4.3660026 -4.3881989 -4.4240079 -4.3972988 -4.3438344 -4.3172827 -4.278337 -4.2828283 -4.3701839 -4.4480371][-4.359621 -4.3675852 -4.3670282 -4.3513532 -4.3618112 -4.3749695 -4.3982635 -4.4244537 -4.3999028 -4.353303 -4.3188696 -4.2715325 -4.2777977 -4.3750048 -4.4685993][-4.4080348 -4.4128137 -4.3967047 -4.3639874 -4.3642817 -4.3798618 -4.3959284 -4.4041371 -4.38388 -4.3544269 -4.3318982 -4.3015437 -4.31789 -4.4071722 -4.4944682][-4.4819441 -4.4754395 -4.4214048 -4.3450603 -4.3151464 -4.3151665 -4.3117833 -4.3005157 -4.2873635 -4.2868948 -4.2941446 -4.2969923 -4.3309546 -4.4046812 -4.4761186][-4.533587 -4.5106506 -4.40958 -4.276031 -4.1981478 -4.1673841 -4.1426139 -4.1201353 -4.1223192 -4.1602612 -4.2075486 -4.2445073 -4.293695 -4.3547335 -4.4124188][-4.5100775 -4.4658828 -4.3315754 -4.1552677 -4.0326352 -3.9668617 -3.9251163 -3.9077082 -3.9431374 -4.0364871 -4.1332035 -4.20322 -4.2700911 -4.3243957 -4.3655028][-4.4331927 -4.3741822 -4.2351627 -4.0581665 -3.9259753 -3.8455868 -3.796227 -3.7856174 -3.8474524 -3.9843459 -4.1246877 -4.2229667 -4.3035259 -4.3523464 -4.370573][-4.4037824 -4.3508129 -4.2399011 -4.1013117 -3.9934058 -3.9211416 -3.8728008 -3.857995 -3.9136224 -4.0477242 -4.1928015 -4.2974143 -4.3746057 -4.4094496 -4.4012246][-4.422955 -4.3896084 -4.3206034 -4.2313132 -4.1573625 -4.1024723 -4.0606756 -4.0394526 -4.0705481 -4.1662145 -4.2780538 -4.3665013 -4.4301615 -4.4513645 -4.4276257][-4.4613256 -4.4510031 -4.4211226 -4.3765755 -4.3389421 -4.3101788 -4.2834997 -4.2620287 -4.265595 -4.3053308 -4.3610396 -4.413094 -4.4539433 -4.466001 -4.4411931][-4.4793539 -4.4903946 -4.4904604 -4.4811187 -4.472939 -4.4666681 -4.4576592 -4.4454188 -4.438592 -4.4404044 -4.4487143 -4.4616752 -4.4744258 -4.4752812 -4.4526529][-4.4567657 -4.4789324 -4.4910922 -4.49747 -4.5046635 -4.5129428 -4.5165129 -4.5125628 -4.5047617 -4.4920473 -4.4808488 -4.4755788 -4.4722223 -4.4656038 -4.4460673][-4.4224453 -4.4391212 -4.4464726 -4.4508734 -4.4579573 -4.4670644 -4.4725943 -4.4707012 -4.4634514 -4.4500823 -4.4371877 -4.4302144 -4.425384 -4.4194226 -4.4079485][-4.4198303 -4.4227786 -4.4200048 -4.4173174 -4.4194722 -4.4237361 -4.4264235 -4.4234867 -4.4169164 -4.4073415 -4.3991227 -4.3959312 -4.3936543 -4.3909283 -4.3879151]]...]
INFO - root - 2017-12-07 05:58:26.615413: step 3710, loss = 2.00, batch loss = 1.91 (11.6 examples/sec; 0.690 sec/batch; 62h:58m:40s remains)
INFO - root - 2017-12-07 05:58:33.583021: step 3720, loss = 2.01, batch loss = 1.93 (11.2 examples/sec; 0.714 sec/batch; 65h:10m:42s remains)
INFO - root - 2017-12-07 05:58:40.583254: step 3730, loss = 2.03, batch loss = 1.95 (10.9 examples/sec; 0.736 sec/batch; 67h:14m:51s remains)
INFO - root - 2017-12-07 05:58:47.685531: step 3740, loss = 2.06, batch loss = 1.98 (10.9 examples/sec; 0.732 sec/batch; 66h:48m:43s remains)
INFO - root - 2017-12-07 05:58:54.623898: step 3750, loss = 2.03, batch loss = 1.95 (11.4 examples/sec; 0.703 sec/batch; 64h:13m:45s remains)
INFO - root - 2017-12-07 05:59:01.682650: step 3760, loss = 1.99, batch loss = 1.91 (11.6 examples/sec; 0.693 sec/batch; 63h:14m:51s remains)
INFO - root - 2017-12-07 05:59:08.773602: step 3770, loss = 2.04, batch loss = 1.95 (12.3 examples/sec; 0.652 sec/batch; 59h:34m:36s remains)
INFO - root - 2017-12-07 05:59:15.811017: step 3780, loss = 1.98, batch loss = 1.90 (11.2 examples/sec; 0.713 sec/batch; 65h:08m:36s remains)
INFO - root - 2017-12-07 05:59:22.806752: step 3790, loss = 2.02, batch loss = 1.94 (11.0 examples/sec; 0.729 sec/batch; 66h:35m:10s remains)
INFO - root - 2017-12-07 05:59:29.863212: step 3800, loss = 2.02, batch loss = 1.94 (10.9 examples/sec; 0.731 sec/batch; 66h:47m:06s remains)
2017-12-07 05:59:30.609575: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3840775 -4.3154106 -4.2210989 -4.1739902 -4.2518306 -4.3905091 -4.5251079 -4.5718031 -4.4972281 -4.3892822 -4.3042502 -4.27818 -4.3225741 -4.4237647 -4.5207787][-4.4759355 -4.4051857 -4.3085604 -4.2601261 -4.3224525 -4.4310894 -4.5347185 -4.5607824 -4.48462 -4.3942719 -4.34214 -4.3417816 -4.3996181 -4.4941692 -4.5600576][-4.6148758 -4.5398068 -4.440989 -4.39018 -4.4181361 -4.4638495 -4.5027733 -4.4902859 -4.4214783 -4.370194 -4.3671021 -4.3979421 -4.4516926 -4.5115066 -4.5435314][-4.6911936 -4.6176853 -4.5255647 -4.4739375 -4.4619637 -4.4338837 -4.3918896 -4.3238192 -4.2607508 -4.2629814 -4.3242936 -4.399281 -4.4616303 -4.5015287 -4.5189753][-4.6678786 -4.6034851 -4.5228767 -4.4627509 -4.4047132 -4.3073339 -4.1863418 -4.0644603 -4.0103817 -4.0828419 -4.225791 -4.3599033 -4.4517655 -4.4963918 -4.5209827][-4.5983448 -4.5519795 -4.4800496 -4.3948655 -4.2799954 -4.1170034 -3.92775 -3.7623661 -3.7317953 -3.8904018 -4.1190124 -4.3167381 -4.4487348 -4.5115714 -4.5493388][-4.5185595 -4.5026374 -4.4455266 -4.3433309 -4.18768 -3.9817178 -3.7446485 -3.5409598 -3.532692 -3.7594666 -4.0533395 -4.30712 -4.4750867 -4.550766 -4.5870624][-4.4517174 -4.4766617 -4.4486494 -4.3513613 -4.1902404 -3.9846816 -3.7332649 -3.5143371 -3.5261483 -3.7768462 -4.0971422 -4.37934 -4.5510254 -4.6054149 -4.6100521][-4.4231853 -4.4968309 -4.5080152 -4.4360261 -4.2997918 -4.1231027 -3.88456 -3.6805565 -3.7076402 -3.9456992 -4.2583103 -4.5305643 -4.6619186 -4.6581182 -4.604434][-4.4487853 -4.5694361 -4.6227612 -4.5851583 -4.4811921 -4.3306489 -4.1093283 -3.9405563 -3.9830155 -4.1932931 -4.4731693 -4.6995964 -4.7626076 -4.694417 -4.586668][-4.5152822 -4.6604414 -4.7381582 -4.7235851 -4.6463189 -4.5151715 -4.3229103 -4.2058935 -4.266583 -4.4486175 -4.6789937 -4.8337188 -4.824605 -4.707098 -4.5666895][-4.5688233 -4.7209983 -4.8146081 -4.8219857 -4.7693233 -4.6566749 -4.5024877 -4.4300141 -4.495615 -4.6453214 -4.8103042 -4.8835545 -4.815906 -4.67329 -4.524549][-4.5707684 -4.7181654 -4.8244457 -4.8591638 -4.8392587 -4.7605848 -4.6518674 -4.6043735 -4.6483226 -4.7411418 -4.8216877 -4.8177786 -4.717391 -4.5778103 -4.447125][-4.5220528 -4.6476145 -4.7524014 -4.806601 -4.8179059 -4.7828708 -4.7213755 -4.6864266 -4.6942596 -4.71841 -4.7186213 -4.6627383 -4.559988 -4.4463816 -4.35092][-4.4388814 -4.5318546 -4.6153297 -4.6662283 -4.6877494 -4.6791258 -4.6508222 -4.6260409 -4.6086111 -4.5861154 -4.543211 -4.4755526 -4.3963647 -4.3221455 -4.2656341]]...]
INFO - root - 2017-12-07 05:59:37.434314: step 3810, loss = 1.98, batch loss = 1.89 (10.8 examples/sec; 0.741 sec/batch; 67h:39m:33s remains)
INFO - root - 2017-12-07 05:59:44.537404: step 3820, loss = 2.02, batch loss = 1.94 (10.6 examples/sec; 0.753 sec/batch; 68h:45m:26s remains)
INFO - root - 2017-12-07 05:59:51.491981: step 3830, loss = 1.99, batch loss = 1.91 (11.9 examples/sec; 0.675 sec/batch; 61h:35m:26s remains)
INFO - root - 2017-12-07 05:59:58.651831: step 3840, loss = 1.98, batch loss = 1.90 (11.6 examples/sec; 0.689 sec/batch; 62h:54m:58s remains)
INFO - root - 2017-12-07 06:00:05.697755: step 3850, loss = 2.01, batch loss = 1.93 (11.3 examples/sec; 0.711 sec/batch; 64h:54m:33s remains)
INFO - root - 2017-12-07 06:00:12.784679: step 3860, loss = 2.05, batch loss = 1.97 (11.5 examples/sec; 0.697 sec/batch; 63h:35m:05s remains)
INFO - root - 2017-12-07 06:00:19.868720: step 3870, loss = 2.03, batch loss = 1.94 (11.2 examples/sec; 0.715 sec/batch; 65h:16m:53s remains)
INFO - root - 2017-12-07 06:00:26.970412: step 3880, loss = 2.02, batch loss = 1.94 (10.8 examples/sec; 0.738 sec/batch; 67h:21m:38s remains)
INFO - root - 2017-12-07 06:00:33.986960: step 3890, loss = 2.02, batch loss = 1.94 (10.9 examples/sec; 0.735 sec/batch; 67h:03m:58s remains)
INFO - root - 2017-12-07 06:00:41.021470: step 3900, loss = 2.01, batch loss = 1.92 (11.3 examples/sec; 0.707 sec/batch; 64h:34m:02s remains)
2017-12-07 06:00:41.780861: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6724305 -4.6903157 -4.6312432 -4.5404911 -4.44545 -4.3676248 -4.3608189 -4.4238806 -4.5092235 -4.5554361 -4.5354414 -4.4682455 -4.3694525 -4.2893467 -4.23825][-4.6497679 -4.6351733 -4.54863 -4.4385247 -4.3423862 -4.2800012 -4.2842913 -4.3557596 -4.4551992 -4.5286655 -4.5298491 -4.4629812 -4.3525281 -4.2666121 -4.2125516][-4.6220031 -4.5837731 -4.4742131 -4.3449697 -4.2429972 -4.1833286 -4.1869326 -4.2605295 -4.3699932 -4.4643092 -4.4775953 -4.407711 -4.2887421 -4.1945405 -4.1367607][-4.6032867 -4.544179 -4.4046912 -4.2429957 -4.1192851 -4.0466208 -4.0416174 -4.1167045 -4.2411041 -4.3567519 -4.3817129 -4.3202853 -4.2143059 -4.1260972 -4.07279][-4.5909615 -4.5112066 -4.3427463 -4.1501193 -4.0034633 -3.9168594 -3.9040997 -3.9814332 -4.1212373 -4.2563663 -4.2946916 -4.2544122 -4.1782703 -4.1110096 -4.0659785][-4.5774493 -4.4819636 -4.2955394 -4.0865545 -3.9250402 -3.8261662 -3.808882 -3.8908811 -4.0408659 -4.1871719 -4.2387443 -4.2248254 -4.1823139 -4.1407461 -4.1036997][-4.5674109 -4.4598784 -4.2666178 -4.0557966 -3.8863578 -3.7774796 -3.76208 -3.8537438 -4.0080743 -4.146369 -4.195416 -4.1955619 -4.1777477 -4.1654849 -4.1491938][-4.5674944 -4.4647532 -4.2821555 -4.0880656 -3.9270821 -3.819252 -3.8092215 -3.9050403 -4.0472546 -4.153615 -4.1774907 -4.1750078 -4.1702619 -4.1849318 -4.1952438][-4.58242 -4.5030417 -4.347476 -4.181901 -4.0379367 -3.9356191 -3.9253376 -4.0100031 -4.1310482 -4.2047849 -4.2049594 -4.1927934 -4.1876626 -4.2100854 -4.2348447][-4.6023874 -4.5519834 -4.427505 -4.2901211 -4.1585689 -4.0583725 -4.0443282 -4.1156912 -4.2201805 -4.2765942 -4.2669487 -4.247797 -4.2332735 -4.2431564 -4.2671642][-4.6122756 -4.595665 -4.5051851 -4.3954759 -4.2764912 -4.1801805 -4.1652637 -4.22632 -4.3160172 -4.3611073 -4.3501749 -4.3313146 -4.3077364 -4.2942133 -4.3021765][-4.6099987 -4.62583 -4.5663881 -4.4772611 -4.3678188 -4.2770777 -4.2637105 -4.3178391 -4.3972259 -4.4411821 -4.4407721 -4.4301677 -4.3984423 -4.3570747 -4.3368011][-4.582211 -4.6236262 -4.5907488 -4.5180268 -4.4183731 -4.3365741 -4.3255649 -4.3742723 -4.4467273 -4.4963126 -4.5126891 -4.5150442 -4.4819837 -4.4216666 -4.3742714][-4.5274792 -4.5864859 -4.5809932 -4.5313954 -4.4507113 -4.3839612 -4.3744183 -4.4146681 -4.4772611 -4.5272036 -4.5548368 -4.5660048 -4.5376821 -4.4735451 -4.4129376][-4.4551587 -4.5154691 -4.5300636 -4.5046043 -4.4484358 -4.398798 -4.3896575 -4.4201961 -4.4718885 -4.5187731 -4.5499873 -4.5627937 -4.54012 -4.4856863 -4.4291496]]...]
INFO - root - 2017-12-07 06:00:48.838807: step 3910, loss = 2.09, batch loss = 2.01 (10.5 examples/sec; 0.759 sec/batch; 69h:15m:10s remains)
INFO - root - 2017-12-07 06:00:55.832887: step 3920, loss = 2.03, batch loss = 1.95 (11.0 examples/sec; 0.725 sec/batch; 66h:08m:11s remains)
INFO - root - 2017-12-07 06:01:02.850539: step 3930, loss = 2.07, batch loss = 1.99 (11.9 examples/sec; 0.674 sec/batch; 61h:29m:42s remains)
INFO - root - 2017-12-07 06:01:09.935436: step 3940, loss = 2.00, batch loss = 1.92 (11.6 examples/sec; 0.687 sec/batch; 62h:44m:05s remains)
INFO - root - 2017-12-07 06:01:16.949202: step 3950, loss = 2.04, batch loss = 1.96 (10.8 examples/sec; 0.742 sec/batch; 67h:40m:58s remains)
INFO - root - 2017-12-07 06:01:24.078482: step 3960, loss = 2.00, batch loss = 1.91 (10.5 examples/sec; 0.765 sec/batch; 69h:47m:01s remains)
INFO - root - 2017-12-07 06:01:31.157839: step 3970, loss = 2.03, batch loss = 1.95 (10.4 examples/sec; 0.768 sec/batch; 70h:06m:54s remains)
INFO - root - 2017-12-07 06:01:38.050947: step 3980, loss = 2.08, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 61h:21m:58s remains)
INFO - root - 2017-12-07 06:01:45.093810: step 3990, loss = 2.00, batch loss = 1.92 (12.3 examples/sec; 0.650 sec/batch; 59h:17m:45s remains)
INFO - root - 2017-12-07 06:01:52.162584: step 4000, loss = 2.02, batch loss = 1.94 (11.6 examples/sec; 0.691 sec/batch; 63h:01m:54s remains)
2017-12-07 06:01:52.916535: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5003614 -4.5265665 -4.5746393 -4.6584935 -4.7488523 -4.7996521 -4.7937679 -4.7542872 -4.7356515 -4.7508378 -4.7648387 -4.7488708 -4.7201447 -4.7054372 -4.7009473][-4.573493 -4.6198926 -4.692306 -4.7971768 -4.8908663 -4.9137335 -4.8569264 -4.7669687 -4.7186532 -4.7301764 -4.7525597 -4.7438016 -4.718761 -4.7157474 -4.7339888][-4.59246 -4.6558752 -4.7540603 -4.8765798 -4.9533758 -4.91713 -4.7885318 -4.6490192 -4.5838203 -4.5984559 -4.6330452 -4.64533 -4.6463151 -4.6694584 -4.7121811][-4.5779767 -4.6368947 -4.7489357 -4.8804674 -4.9226751 -4.8066773 -4.5960631 -4.419486 -4.3664517 -4.4126477 -4.4706993 -4.5078692 -4.5438237 -4.5974603 -4.6531348][-4.54474 -4.5781822 -4.6877503 -4.8126559 -4.8060222 -4.6016545 -4.3095951 -4.10307 -4.0902147 -4.2123346 -4.3259983 -4.392015 -4.4581218 -4.5342455 -4.5906825][-4.4776278 -4.48067 -4.5819907 -4.6956477 -4.6425953 -4.358613 -3.9851873 -3.7326124 -3.7528405 -3.973913 -4.1836805 -4.3096352 -4.4142904 -4.5086155 -4.5619044][-4.365231 -4.3427687 -4.4299669 -4.5311332 -4.4522696 -4.1211014 -3.6858287 -3.3844488 -3.4271779 -3.7360158 -4.0455637 -4.2527561 -4.4055991 -4.5134315 -4.5617523][-4.2427645 -4.1967897 -4.2655845 -4.3654056 -4.3102703 -4.00814 -3.5836415 -3.273509 -3.3195553 -3.6484091 -3.9907808 -4.2390041 -4.4191833 -4.5269179 -4.5585952][-4.1871262 -4.1256742 -4.1715555 -4.2725244 -4.2810631 -4.0958247 -3.7791185 -3.5244265 -3.5466855 -3.8118153 -4.095984 -4.3044453 -4.4567513 -4.5276918 -4.51188][-4.2228718 -4.1643553 -4.1866961 -4.2716875 -4.3321443 -4.2728553 -4.07916 -3.8906989 -3.885633 -4.0683312 -4.26957 -4.40566 -4.5004492 -4.5146379 -4.4348855][-4.3317809 -4.2768469 -4.2731061 -4.3250055 -4.3978639 -4.4114966 -4.2956395 -4.1375847 -4.0918465 -4.1979113 -4.3400283 -4.4367819 -4.5114641 -4.512733 -4.4031334][-4.447834 -4.3736191 -4.3193989 -4.3183994 -4.3730392 -4.4146686 -4.35168 -4.2178507 -4.1439643 -4.1917982 -4.2961307 -4.3839054 -4.4784265 -4.5278373 -4.4551759][-4.501224 -4.4024544 -4.3033776 -4.2694454 -4.3157945 -4.3752732 -4.3542428 -4.2567816 -4.1815681 -4.2033753 -4.2892289 -4.3764539 -4.4882011 -4.5801148 -4.5565629][-4.505424 -4.4179368 -4.3267937 -4.3083267 -4.36813 -4.4391232 -4.4406309 -4.3671846 -4.2870297 -4.2817769 -4.3532443 -4.4412842 -4.5527272 -4.648119 -4.645329][-4.5004177 -4.4655814 -4.4220657 -4.4387693 -4.504612 -4.5613751 -4.5594792 -4.4947977 -4.4051766 -4.3713675 -4.4185696 -4.4975371 -4.58985 -4.6587396 -4.6626673]]...]
INFO - root - 2017-12-07 06:01:59.928292: step 4010, loss = 2.03, batch loss = 1.94 (11.5 examples/sec; 0.697 sec/batch; 63h:33m:32s remains)
INFO - root - 2017-12-07 06:02:07.135654: step 4020, loss = 1.96, batch loss = 1.88 (11.8 examples/sec; 0.676 sec/batch; 61h:42m:56s remains)
INFO - root - 2017-12-07 06:02:14.197685: step 4030, loss = 1.99, batch loss = 1.91 (11.6 examples/sec; 0.691 sec/batch; 63h:00m:54s remains)
INFO - root - 2017-12-07 06:02:21.276407: step 4040, loss = 2.00, batch loss = 1.92 (11.6 examples/sec; 0.689 sec/batch; 62h:51m:07s remains)
INFO - root - 2017-12-07 06:02:28.339181: step 4050, loss = 2.00, batch loss = 1.92 (11.4 examples/sec; 0.702 sec/batch; 64h:02m:11s remains)
INFO - root - 2017-12-07 06:02:35.459809: step 4060, loss = 2.01, batch loss = 1.93 (11.5 examples/sec; 0.697 sec/batch; 63h:37m:01s remains)
INFO - root - 2017-12-07 06:02:42.583000: step 4070, loss = 1.98, batch loss = 1.90 (11.2 examples/sec; 0.714 sec/batch; 65h:07m:15s remains)
INFO - root - 2017-12-07 06:02:49.691169: step 4080, loss = 2.04, batch loss = 1.95 (11.5 examples/sec; 0.697 sec/batch; 63h:37m:32s remains)
INFO - root - 2017-12-07 06:02:56.709953: step 4090, loss = 2.01, batch loss = 1.93 (12.4 examples/sec; 0.646 sec/batch; 58h:54m:45s remains)
INFO - root - 2017-12-07 06:03:03.794664: step 4100, loss = 2.04, batch loss = 1.96 (11.1 examples/sec; 0.719 sec/batch; 65h:35m:58s remains)
2017-12-07 06:03:04.571274: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2462049 -4.2649508 -4.2984619 -4.3267317 -4.3446341 -4.3506179 -4.3410883 -4.3166475 -4.2820206 -4.2480121 -4.2245979 -4.2135344 -4.2097235 -4.2139344 -4.2292027][-4.2529082 -4.300467 -4.3610544 -4.4018331 -4.4231353 -4.4311171 -4.4226832 -4.3931146 -4.3435674 -4.2891307 -4.2493968 -4.2302947 -4.225749 -4.2351074 -4.2632322][-4.2525587 -4.3402219 -4.4364204 -4.4846148 -4.4959645 -4.4970136 -4.4891577 -4.4628386 -4.4116726 -4.3490992 -4.301434 -4.2803364 -4.2782416 -4.2936206 -4.33278][-4.2570024 -4.3797612 -4.5009694 -4.5384693 -4.5145149 -4.48422 -4.4648881 -4.4489555 -4.4252229 -4.3969321 -4.3790913 -4.3811588 -4.3946724 -4.4188104 -4.4592309][-4.2704182 -4.4077196 -4.5320988 -4.5422611 -4.4643645 -4.3717031 -4.306829 -4.282598 -4.3028383 -4.3583388 -4.4250126 -4.4916973 -4.547399 -4.589015 -4.6157241][-4.2682128 -4.3872428 -4.4874067 -4.4643569 -4.3320427 -4.1598282 -4.011198 -3.9391184 -3.9983883 -4.17145 -4.3677516 -4.5357552 -4.6587186 -4.7256851 -4.7277246][-4.2630243 -4.342113 -4.3996015 -4.3454862 -4.1717248 -3.9231348 -3.6702743 -3.517844 -3.5955977 -3.8826418 -4.212719 -4.4860196 -4.6779461 -4.7681618 -4.7409525][-4.2756982 -4.3233647 -4.3517818 -4.2889366 -4.1023293 -3.8061168 -3.4704757 -3.2369804 -3.3032436 -3.6516809 -4.0621943 -4.4013352 -4.6351924 -4.7287722 -4.6658316][-4.3028035 -4.3449945 -4.3761091 -4.3331366 -4.1674786 -3.8726022 -3.5190289 -3.260874 -3.3059402 -3.6421695 -4.0478787 -4.3790369 -4.59416 -4.6490545 -4.5414009][-4.3368449 -4.3890138 -4.4371543 -4.4246883 -4.3005705 -4.0509739 -3.7476804 -3.5332143 -3.5751574 -3.8530893 -4.1809974 -4.4319935 -4.5683107 -4.5549383 -4.4102879][-4.370966 -4.4373093 -4.5057788 -4.5310259 -4.4696569 -4.3070235 -4.1000156 -3.9601204 -3.9979773 -4.1847396 -4.3813438 -4.501543 -4.5276742 -4.4514818 -4.3069081][-4.3872209 -4.4618344 -4.5403004 -4.5901852 -4.58807 -4.5244756 -4.4353085 -4.3852663 -4.4229321 -4.5102749 -4.55838 -4.5331693 -4.4530954 -4.3498974 -4.2534804][-4.37182 -4.4475975 -4.5286269 -4.5895734 -4.6256275 -4.6369047 -4.6435061 -4.6673112 -4.7032 -4.70442 -4.6259308 -4.4825797 -4.3399386 -4.2579689 -4.2415652][-4.3160691 -4.3825655 -4.4625163 -4.53239 -4.5922832 -4.6455393 -4.7024884 -4.7548013 -4.7677884 -4.7004266 -4.5403633 -4.3365955 -4.1864953 -4.1539726 -4.2131314][-4.220264 -4.2665815 -4.3361006 -4.4056826 -4.471323 -4.5345306 -4.5997128 -4.6416059 -4.6223254 -4.5156031 -4.3261461 -4.1176162 -3.9946728 -4.0089231 -4.1152043]]...]
INFO - root - 2017-12-07 06:03:11.574242: step 4110, loss = 2.02, batch loss = 1.94 (11.7 examples/sec; 0.687 sec/batch; 62h:38m:07s remains)
INFO - root - 2017-12-07 06:03:18.676278: step 4120, loss = 2.01, batch loss = 1.92 (11.6 examples/sec; 0.692 sec/batch; 63h:09m:40s remains)
INFO - root - 2017-12-07 06:03:25.766774: step 4130, loss = 2.03, batch loss = 1.95 (11.7 examples/sec; 0.687 sec/batch; 62h:37m:36s remains)
INFO - root - 2017-12-07 06:03:32.681035: step 4140, loss = 2.01, batch loss = 1.93 (11.5 examples/sec; 0.696 sec/batch; 63h:28m:24s remains)
INFO - root - 2017-12-07 06:03:39.720769: step 4150, loss = 2.00, batch loss = 1.92 (10.5 examples/sec; 0.761 sec/batch; 69h:24m:09s remains)
INFO - root - 2017-12-07 06:03:46.795871: step 4160, loss = 2.07, batch loss = 1.99 (10.8 examples/sec; 0.741 sec/batch; 67h:36m:11s remains)
INFO - root - 2017-12-07 06:03:53.754972: step 4170, loss = 2.05, batch loss = 1.96 (11.7 examples/sec; 0.685 sec/batch; 62h:29m:05s remains)
INFO - root - 2017-12-07 06:04:00.851255: step 4180, loss = 2.03, batch loss = 1.95 (11.4 examples/sec; 0.701 sec/batch; 63h:56m:02s remains)
INFO - root - 2017-12-07 06:04:07.904381: step 4190, loss = 2.03, batch loss = 1.95 (11.2 examples/sec; 0.713 sec/batch; 65h:01m:08s remains)
INFO - root - 2017-12-07 06:04:14.979079: step 4200, loss = 2.03, batch loss = 1.94 (10.8 examples/sec; 0.742 sec/batch; 67h:37m:28s remains)
2017-12-07 06:04:15.858415: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2388196 -4.2795782 -4.385601 -4.4802718 -4.4900994 -4.3890972 -4.2503004 -4.1755652 -4.1981325 -4.2679443 -4.3438921 -4.398313 -4.4406285 -4.4604006 -4.4466224][-4.2787914 -4.3430276 -4.4588161 -4.5432396 -4.5274749 -4.3979445 -4.250731 -4.1903257 -4.2242413 -4.29172 -4.3554769 -4.3846111 -4.3991909 -4.4028306 -4.385993][-4.3765993 -4.4583812 -4.5555 -4.5930843 -4.526947 -4.3673978 -4.2190785 -4.1728692 -4.2174282 -4.2860503 -4.3420916 -4.3606343 -4.3639436 -4.3626738 -4.348114][-4.4803677 -4.5555887 -4.6024837 -4.5657496 -4.4375248 -4.2531185 -4.1046972 -4.0658956 -4.1247959 -4.2229176 -4.3127789 -4.3606315 -4.3822732 -4.3896728 -4.3743224][-4.5612311 -4.61059 -4.5970745 -4.4871149 -4.3010902 -4.0890627 -3.9303951 -3.8912945 -3.9675062 -4.1134377 -4.2734456 -4.3891878 -4.4572721 -4.4830055 -4.458848][-4.6238365 -4.6440434 -4.5767155 -4.410069 -4.1823506 -3.9436486 -3.7641702 -3.7117174 -3.7891965 -3.9654369 -4.1876864 -4.3764625 -4.5008507 -4.55543 -4.5300074][-4.6509118 -4.65116 -4.5592737 -4.3792 -4.1460814 -3.8983979 -3.6946239 -3.6083219 -3.654058 -3.8195629 -4.0644665 -4.3015394 -4.4723206 -4.5623822 -4.5593486][-4.6463227 -4.6457071 -4.5699716 -4.4234014 -4.2229996 -3.9934382 -3.7839675 -3.6601167 -3.6592789 -3.7905693 -4.0227695 -4.2671051 -4.4480114 -4.5540309 -4.5786757][-4.6476789 -4.660933 -4.6254988 -4.5334582 -4.3853335 -4.1906323 -3.9953678 -3.8524709 -3.8180268 -3.9193985 -4.122261 -4.3362093 -4.481812 -4.5636487 -4.5938835][-4.6486187 -4.679913 -4.6857815 -4.64884 -4.556324 -4.406064 -4.2379718 -4.1010189 -4.0617132 -4.1480947 -4.3179364 -4.4798927 -4.5645413 -4.5936046 -4.60314][-4.6379261 -4.6864176 -4.7236133 -4.7292862 -4.6831694 -4.572361 -4.432219 -4.316112 -4.2909656 -4.3735909 -4.5102839 -4.6174159 -4.6462502 -4.6279321 -4.6095247][-4.6088457 -4.6670685 -4.7216091 -4.7543011 -4.7429438 -4.6655941 -4.5526948 -4.4612703 -4.4520316 -4.52758 -4.62629 -4.6838689 -4.6764011 -4.63798 -4.6083617][-4.5541363 -4.6110959 -4.6720572 -4.7231526 -4.7426319 -4.7053232 -4.6301918 -4.5679646 -4.5664144 -4.6205206 -4.6763153 -4.6959972 -4.6740031 -4.6382642 -4.6119642][-4.4837346 -4.5289845 -4.5856609 -4.643445 -4.6826935 -4.6808796 -4.6453953 -4.6116881 -4.6097822 -4.6356263 -4.6578593 -4.6592312 -4.639389 -4.6126666 -4.5896258][-4.4027686 -4.429543 -4.4696112 -4.5159326 -4.5563564 -4.5753841 -4.5705843 -4.5572391 -4.5518408 -4.5567055 -4.5618944 -4.5597868 -4.5467768 -4.526741 -4.5064821]]...]
INFO - root - 2017-12-07 06:04:22.905908: step 4210, loss = 2.07, batch loss = 1.98 (11.9 examples/sec; 0.672 sec/batch; 61h:18m:27s remains)
INFO - root - 2017-12-07 06:04:30.033395: step 4220, loss = 2.06, batch loss = 1.98 (10.6 examples/sec; 0.754 sec/batch; 68h:43m:08s remains)
INFO - root - 2017-12-07 06:04:37.134593: step 4230, loss = 1.99, batch loss = 1.91 (10.8 examples/sec; 0.738 sec/batch; 67h:18m:55s remains)
INFO - root - 2017-12-07 06:04:44.248776: step 4240, loss = 1.98, batch loss = 1.89 (11.0 examples/sec; 0.726 sec/batch; 66h:09m:53s remains)
INFO - root - 2017-12-07 06:04:51.207950: step 4250, loss = 1.97, batch loss = 1.88 (11.2 examples/sec; 0.715 sec/batch; 65h:13m:11s remains)
INFO - root - 2017-12-07 06:04:58.251121: step 4260, loss = 2.06, batch loss = 1.98 (11.0 examples/sec; 0.728 sec/batch; 66h:24m:29s remains)
INFO - root - 2017-12-07 06:05:05.370065: step 4270, loss = 2.02, batch loss = 1.93 (11.7 examples/sec; 0.686 sec/batch; 62h:33m:01s remains)
INFO - root - 2017-12-07 06:05:12.424447: step 4280, loss = 1.98, batch loss = 1.89 (11.7 examples/sec; 0.683 sec/batch; 62h:15m:36s remains)
INFO - root - 2017-12-07 06:05:19.521722: step 4290, loss = 2.04, batch loss = 1.96 (11.6 examples/sec; 0.689 sec/batch; 62h:49m:07s remains)
INFO - root - 2017-12-07 06:05:26.501121: step 4300, loss = 1.97, batch loss = 1.89 (12.1 examples/sec; 0.660 sec/batch; 60h:09m:46s remains)
2017-12-07 06:05:27.297819: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3949814 -4.417294 -4.428268 -4.4285336 -4.426271 -4.4238248 -4.4191842 -4.4110031 -4.4045 -4.4021344 -4.3983331 -4.3911414 -4.3795629 -4.3649569 -4.3477659][-4.4718909 -4.517488 -4.5373926 -4.5380778 -4.5289793 -4.5125561 -4.4889774 -4.4685168 -4.4675741 -4.4852614 -4.5024662 -4.5053082 -4.49036 -4.4609756 -4.4214411][-4.5239391 -4.5862107 -4.611855 -4.610816 -4.5897117 -4.5452161 -4.4807391 -4.4312754 -4.4386311 -4.5012159 -4.574297 -4.6171556 -4.6157494 -4.5769782 -4.5155749][-4.5334353 -4.5980887 -4.6180053 -4.6054754 -4.5610595 -4.4694872 -4.3380618 -4.2337508 -4.2356248 -4.3537683 -4.5164433 -4.6362891 -4.67564 -4.64805 -4.5823336][-4.5226731 -4.5810347 -4.5852942 -4.5452695 -4.4596133 -4.3019652 -4.0835705 -3.9043424 -3.8806162 -4.0445814 -4.3065991 -4.5263958 -4.6287856 -4.6342039 -4.5898647][-4.5140886 -4.5649595 -4.5491514 -4.4691949 -4.3223662 -4.07767 -3.7613385 -3.5056982 -3.4548483 -3.65953 -4.0188484 -4.3443313 -4.5173149 -4.5641665 -4.5524721][-4.5167532 -4.5664067 -4.5442061 -4.4417892 -4.2480054 -3.9269626 -3.5177588 -3.185102 -3.100162 -3.3286209 -3.7602048 -4.1669445 -4.4010272 -4.4923058 -4.5168285][-4.5352812 -4.5944309 -4.5864649 -4.4965258 -4.306736 -3.9711452 -3.5252197 -3.1439729 -3.0221272 -3.2422867 -3.6892715 -4.1172996 -4.37461 -4.4916434 -4.5372434][-4.5594625 -4.6357517 -4.653204 -4.5975742 -4.4579134 -4.1893167 -3.8005967 -3.4335568 -3.2829387 -3.455755 -3.8520377 -4.2400932 -4.4801354 -4.5920763 -4.626009][-4.5620136 -4.6632829 -4.7184963 -4.7085342 -4.6346264 -4.4694052 -4.20205 -3.9152036 -3.7649477 -3.864707 -4.1529551 -4.4540668 -4.6494508 -4.7326131 -4.7264671][-4.51539 -4.6292553 -4.722599 -4.7665625 -4.7598124 -4.689651 -4.5456562 -4.3683338 -4.2552814 -4.29089 -4.4544005 -4.6484327 -4.7809181 -4.8178148 -4.7599692][-4.4367909 -4.531064 -4.6316481 -4.7091818 -4.752255 -4.7515464 -4.7011342 -4.6224184 -4.5634351 -4.5659833 -4.6304116 -4.7213273 -4.7820559 -4.7708292 -4.6804733][-4.3689857 -4.4215145 -4.4893785 -4.5560536 -4.6112537 -4.6441817 -4.6459465 -4.6277084 -4.608695 -4.6025543 -4.6142387 -4.63678 -4.6441479 -4.6081958 -4.525063][-4.3309879 -4.3491211 -4.3756752 -4.406435 -4.4409804 -4.4736547 -4.494163 -4.502399 -4.50212 -4.4960322 -4.4879088 -4.4792252 -4.4634633 -4.4298563 -4.379818][-4.3150678 -4.3174171 -4.3223014 -4.3287015 -4.340867 -4.3569965 -4.371139 -4.381392 -4.3854237 -4.3818455 -4.3722458 -4.3592873 -4.3442497 -4.3262391 -4.3078828]]...]
INFO - root - 2017-12-07 06:05:34.459611: step 4310, loss = 2.07, batch loss = 1.99 (11.9 examples/sec; 0.671 sec/batch; 61h:09m:43s remains)
INFO - root - 2017-12-07 06:05:41.438396: step 4320, loss = 2.07, batch loss = 1.98 (11.2 examples/sec; 0.715 sec/batch; 65h:11m:48s remains)
INFO - root - 2017-12-07 06:05:48.498704: step 4330, loss = 2.02, batch loss = 1.93 (10.9 examples/sec; 0.736 sec/batch; 67h:02m:50s remains)
INFO - root - 2017-12-07 06:05:55.567287: step 4340, loss = 2.02, batch loss = 1.94 (11.4 examples/sec; 0.702 sec/batch; 64h:01m:24s remains)
INFO - root - 2017-12-07 06:06:02.676569: step 4350, loss = 2.01, batch loss = 1.93 (10.8 examples/sec; 0.743 sec/batch; 67h:43m:41s remains)
INFO - root - 2017-12-07 06:06:09.739364: step 4360, loss = 2.04, batch loss = 1.96 (11.2 examples/sec; 0.715 sec/batch; 65h:10m:25s remains)
INFO - root - 2017-12-07 06:06:16.704160: step 4370, loss = 1.99, batch loss = 1.91 (11.8 examples/sec; 0.678 sec/batch; 61h:48m:59s remains)
INFO - root - 2017-12-07 06:06:23.683510: step 4380, loss = 2.01, batch loss = 1.92 (11.4 examples/sec; 0.700 sec/batch; 63h:47m:12s remains)
INFO - root - 2017-12-07 06:06:30.728051: step 4390, loss = 2.01, batch loss = 1.92 (10.7 examples/sec; 0.745 sec/batch; 67h:51m:21s remains)
INFO - root - 2017-12-07 06:06:37.788076: step 4400, loss = 2.04, batch loss = 1.96 (11.0 examples/sec; 0.728 sec/batch; 66h:23m:38s remains)
2017-12-07 06:06:38.576614: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5600872 -4.552527 -4.5321393 -4.5132885 -4.5083189 -4.5172253 -4.5262852 -4.4805727 -4.4005985 -4.3481011 -4.3775125 -4.4684148 -4.5215473 -4.4929795 -4.4402108][-4.5409546 -4.5546007 -4.5367804 -4.5118036 -4.5063305 -4.5045066 -4.4904046 -4.4410644 -4.38356 -4.3600531 -4.405787 -4.4916615 -4.5296068 -4.4935522 -4.4399619][-4.474009 -4.519083 -4.50766 -4.4667687 -4.4526167 -4.450109 -4.431787 -4.3935113 -4.365603 -4.3641696 -4.40813 -4.4679546 -4.4843926 -4.4543862 -4.4114285][-4.34856 -4.4188151 -4.4224477 -4.3748236 -4.352294 -4.3516541 -4.3343844 -4.31095 -4.3184242 -4.3416386 -4.3763742 -4.3950148 -4.3820472 -4.3576837 -4.3271871][-4.2367363 -4.3115458 -4.3228059 -4.2707686 -4.2250395 -4.1917567 -4.1458831 -4.1231136 -4.165617 -4.2307758 -4.2806606 -4.2849383 -4.2600112 -4.242898 -4.2141089][-4.2013669 -4.2545085 -4.245935 -4.1701841 -4.0696592 -3.9594564 -3.8564782 -3.8329439 -3.9194231 -4.0429063 -4.1406422 -4.1735597 -4.164216 -4.1573305 -4.1215363][-4.2233734 -4.2442594 -4.2000613 -4.0788326 -3.9012496 -3.6937141 -3.5247302 -3.5071266 -3.6513939 -3.8482041 -4.021421 -4.1164193 -4.1447787 -4.1554775 -4.1179953][-4.3371315 -4.3372135 -4.2652979 -4.1048675 -3.8686988 -3.5967526 -3.3899479 -3.3831682 -3.5692723 -3.819876 -4.0527267 -4.2029858 -4.2746167 -4.3099246 -4.27865][-4.5247169 -4.5216112 -4.4447217 -4.28173 -4.0500145 -3.7932298 -3.6086488 -3.6123972 -3.7854671 -4.019083 -4.2514229 -4.4204335 -4.5215387 -4.5762773 -4.5543203][-4.6527953 -4.664607 -4.6072059 -4.478097 -4.3002224 -4.1134405 -3.9883633 -3.9998789 -4.124064 -4.2923913 -4.4781151 -4.63418 -4.7388039 -4.7911162 -4.7676754][-4.6748347 -4.7087855 -4.6841712 -4.6031642 -4.4925146 -4.3867731 -4.323442 -4.3351479 -4.4030414 -4.4983659 -4.6190224 -4.7301054 -4.7984648 -4.8241758 -4.7977366][-4.6349263 -4.6816573 -4.6852922 -4.6520233 -4.6049967 -4.5649047 -4.540092 -4.5404935 -4.5605984 -4.5947528 -4.647284 -4.6906586 -4.6999192 -4.6955118 -4.6789665][-4.5487232 -4.5948772 -4.6146517 -4.6153121 -4.611918 -4.6105938 -4.60596 -4.5990205 -4.59363 -4.5919108 -4.5932026 -4.5777955 -4.5422592 -4.5263071 -4.5282359][-4.4321761 -4.4692268 -4.4967022 -4.5179615 -4.5374842 -4.5539684 -4.5611482 -4.5586209 -4.5505114 -4.5380168 -4.5179753 -4.4830184 -4.4470572 -4.4514575 -4.4761829][-4.3125286 -4.3363929 -4.3615918 -4.3875852 -4.4121637 -4.4321923 -4.4439106 -4.4472847 -4.4452066 -4.4387612 -4.4282079 -4.4176788 -4.4249916 -4.4748054 -4.5257788]]...]
INFO - root - 2017-12-07 06:06:45.628115: step 4410, loss = 2.03, batch loss = 1.95 (11.4 examples/sec; 0.703 sec/batch; 64h:03m:41s remains)
INFO - root - 2017-12-07 06:06:52.685464: step 4420, loss = 2.03, batch loss = 1.95 (10.9 examples/sec; 0.734 sec/batch; 66h:55m:51s remains)
INFO - root - 2017-12-07 06:06:59.677895: step 4430, loss = 2.04, batch loss = 1.95 (11.1 examples/sec; 0.724 sec/batch; 65h:58m:23s remains)
INFO - root - 2017-12-07 06:07:06.815764: step 4440, loss = 2.04, batch loss = 1.96 (9.9 examples/sec; 0.809 sec/batch; 73h:42m:05s remains)
INFO - root - 2017-12-07 06:07:13.816032: step 4450, loss = 2.04, batch loss = 1.95 (10.7 examples/sec; 0.750 sec/batch; 68h:23m:05s remains)
INFO - root - 2017-12-07 06:07:20.870090: step 4460, loss = 2.05, batch loss = 1.97 (11.7 examples/sec; 0.682 sec/batch; 62h:07m:49s remains)
INFO - root - 2017-12-07 06:07:27.909430: step 4470, loss = 2.03, batch loss = 1.95 (12.0 examples/sec; 0.665 sec/batch; 60h:35m:51s remains)
INFO - root - 2017-12-07 06:07:35.007805: step 4480, loss = 1.99, batch loss = 1.90 (11.3 examples/sec; 0.711 sec/batch; 64h:46m:23s remains)
INFO - root - 2017-12-07 06:07:42.040935: step 4490, loss = 2.01, batch loss = 1.92 (11.9 examples/sec; 0.674 sec/batch; 61h:22m:21s remains)
INFO - root - 2017-12-07 06:07:49.049740: step 4500, loss = 2.01, batch loss = 1.93 (11.5 examples/sec; 0.697 sec/batch; 63h:30m:15s remains)
2017-12-07 06:07:49.807497: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6246157 -4.6892638 -4.7169857 -4.7225208 -4.690176 -4.610631 -4.5071545 -4.458744 -4.5300794 -4.6524296 -4.7389231 -4.7477775 -4.6730456 -4.5699258 -4.4686666][-4.6247158 -4.7082663 -4.75018 -4.7601585 -4.708478 -4.5910687 -4.4579864 -4.4153423 -4.53597 -4.7076735 -4.8262691 -4.8421702 -4.7339363 -4.5822573 -4.4437056][-4.6354542 -4.7122216 -4.7428322 -4.7357039 -4.6621885 -4.5273952 -4.3902388 -4.3644514 -4.5239487 -4.72918 -4.8749175 -4.9074688 -4.7814889 -4.5944519 -4.4280019][-4.6392574 -4.67705 -4.6656332 -4.6206846 -4.5231519 -4.3881555 -4.2608404 -4.2518435 -4.4335909 -4.65568 -4.8244305 -4.8808436 -4.7653255 -4.5800166 -4.4139462][-4.6280327 -4.6227117 -4.5684066 -4.4856811 -4.3677707 -4.2329965 -4.1054564 -4.0964413 -4.2798533 -4.5128274 -4.7026639 -4.7857242 -4.70177 -4.550086 -4.4054837][-4.6112018 -4.5794821 -4.5001845 -4.3851018 -4.2387629 -4.08338 -3.9311948 -3.8970625 -4.0638661 -4.3070946 -4.5286875 -4.6537237 -4.623076 -4.5247011 -4.4114184][-4.5967617 -4.5710354 -4.4937396 -4.3598433 -4.1743946 -3.9726479 -3.7766523 -3.7056637 -3.8477366 -4.0982571 -4.3585968 -4.5325518 -4.5626836 -4.514884 -4.4286957][-4.5787625 -4.5887656 -4.5422606 -4.4128718 -4.204473 -3.9651573 -3.7390504 -3.6490958 -3.7727 -4.0224781 -4.2980537 -4.490787 -4.5484991 -4.5210538 -4.447144][-4.5602679 -4.607028 -4.5998378 -4.49596 -4.2981591 -4.0624828 -3.854073 -3.7815139 -3.8946817 -4.1199722 -4.3628879 -4.5228324 -4.5631084 -4.5251737 -4.4543118][-4.5349388 -4.6029959 -4.6273088 -4.5558057 -4.3947587 -4.19399 -4.0320106 -3.9975274 -4.1098309 -4.2990041 -4.4807425 -4.580617 -4.5779495 -4.5161395 -4.446981][-4.4890146 -4.5597763 -4.6015882 -4.5602894 -4.4430857 -4.2917433 -4.1812153 -4.1791043 -4.2851887 -4.4366021 -4.5597486 -4.6043887 -4.5607891 -4.474555 -4.4027333][-4.41663 -4.48267 -4.5307155 -4.5131955 -4.4409366 -4.3430295 -4.2765379 -4.2909346 -4.3768783 -4.4850578 -4.5593815 -4.5675006 -4.4998178 -4.3978138 -4.3223147][-4.3435111 -4.4020343 -4.4506207 -4.449894 -4.413754 -4.3627763 -4.3323412 -4.355917 -4.4176025 -4.4818277 -4.5129676 -4.4887233 -4.4038539 -4.2956486 -4.2224545][-4.3049006 -4.3551769 -4.3979445 -4.4082742 -4.3968105 -4.3745351 -4.36206 -4.3813486 -4.4158554 -4.4451971 -4.4472661 -4.4020143 -4.308238 -4.2002292 -4.1345587][-4.3089228 -4.3435163 -4.3736024 -4.3909774 -4.3990731 -4.39617 -4.3869834 -4.3836803 -4.3817873 -4.3845563 -4.3801031 -4.3381705 -4.25259 -4.1579676 -4.1087384]]...]
INFO - root - 2017-12-07 06:07:56.916443: step 4510, loss = 1.97, batch loss = 1.89 (11.2 examples/sec; 0.716 sec/batch; 65h:11m:55s remains)
INFO - root - 2017-12-07 06:08:03.949581: step 4520, loss = 2.02, batch loss = 1.94 (11.5 examples/sec; 0.699 sec/batch; 63h:39m:05s remains)
INFO - root - 2017-12-07 06:08:10.962647: step 4530, loss = 1.99, batch loss = 1.91 (11.5 examples/sec; 0.695 sec/batch; 63h:18m:20s remains)
INFO - root - 2017-12-07 06:08:17.958880: step 4540, loss = 1.96, batch loss = 1.88 (11.2 examples/sec; 0.713 sec/batch; 64h:55m:51s remains)
INFO - root - 2017-12-07 06:08:25.097587: step 4550, loss = 2.01, batch loss = 1.92 (10.6 examples/sec; 0.756 sec/batch; 68h:51m:46s remains)
INFO - root - 2017-12-07 06:08:32.124757: step 4560, loss = 2.06, batch loss = 1.98 (11.3 examples/sec; 0.707 sec/batch; 64h:23m:56s remains)
INFO - root - 2017-12-07 06:08:39.132963: step 4570, loss = 1.97, batch loss = 1.89 (11.3 examples/sec; 0.710 sec/batch; 64h:42m:19s remains)
INFO - root - 2017-12-07 06:08:46.212402: step 4580, loss = 2.04, batch loss = 1.95 (11.0 examples/sec; 0.728 sec/batch; 66h:18m:42s remains)
INFO - root - 2017-12-07 06:08:53.286686: step 4590, loss = 2.05, batch loss = 1.97 (11.1 examples/sec; 0.723 sec/batch; 65h:52m:56s remains)
INFO - root - 2017-12-07 06:09:00.302936: step 4600, loss = 2.03, batch loss = 1.95 (10.8 examples/sec; 0.740 sec/batch; 67h:21m:49s remains)
2017-12-07 06:09:01.020073: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4931889 -4.5011797 -4.5000992 -4.500968 -4.5058527 -4.500442 -4.4784732 -4.4595375 -4.4571481 -4.4490471 -4.4207306 -4.4068909 -4.4486589 -4.5437956 -4.6373563][-4.5714903 -4.5819464 -4.5782795 -4.5711656 -4.56488 -4.547009 -4.513875 -4.4905314 -4.4907608 -4.4871578 -4.4647431 -4.4562469 -4.5042272 -4.6079965 -4.7003269][-4.6188984 -4.6268125 -4.622375 -4.6110859 -4.5922008 -4.5581064 -4.51408 -4.4914742 -4.5003343 -4.514174 -4.5156307 -4.5247107 -4.5795007 -4.6821795 -4.7617555][-4.5883269 -4.5789833 -4.5679984 -4.5554662 -4.523416 -4.4682813 -4.4144411 -4.4006495 -4.4318333 -4.4823246 -4.5290413 -4.5740142 -4.647193 -4.7488742 -4.8137355][-4.4504948 -4.4186053 -4.3999639 -4.3893857 -4.3447986 -4.2597919 -4.1854439 -4.1775279 -4.2413931 -4.3479347 -4.4592133 -4.5566573 -4.6593947 -4.7602797 -4.8112187][-4.2697282 -4.2274151 -4.2052541 -4.1886706 -4.119422 -3.9876418 -3.8734536 -3.8566532 -3.9489052 -4.1148634 -4.294292 -4.444963 -4.5764 -4.6793227 -4.7282333][-4.1400437 -4.0980849 -4.0725231 -4.0358658 -3.9302812 -3.744442 -3.5781312 -3.5297947 -3.6257918 -3.8319426 -4.066205 -4.266933 -4.4289756 -4.543973 -4.6042929][-4.0890007 -4.0580091 -4.0362787 -3.9901798 -3.8700128 -3.6577735 -3.4536982 -3.3608117 -3.4270477 -3.6366172 -3.8965924 -4.1301928 -4.3146691 -4.4435086 -4.5192513][-4.1322055 -4.1195626 -4.1064773 -4.0691528 -3.9655132 -3.7738955 -3.5754285 -3.4576201 -3.4832675 -3.6582057 -3.9012904 -4.1356807 -4.3216891 -4.4511089 -4.5286031][-4.2609344 -4.2614985 -4.2516103 -4.2282109 -4.1587667 -4.0205045 -3.8670678 -3.7574089 -3.7556911 -3.8818641 -4.0779138 -4.2823668 -4.4502439 -4.5630989 -4.6228676][-4.3991995 -4.40694 -4.403048 -4.3987117 -4.3676829 -4.289012 -4.1940417 -4.1193633 -4.1144996 -4.1952758 -4.32658 -4.4761572 -4.60892 -4.6939259 -4.725389][-4.48429 -4.4926963 -4.5026588 -4.5185776 -4.5175095 -4.4841056 -4.4395533 -4.4100022 -4.4214287 -4.4679089 -4.5250268 -4.5974855 -4.6821141 -4.7423625 -4.7574277][-4.4763503 -4.4897637 -4.5138044 -4.5382924 -4.5459375 -4.5338473 -4.5196342 -4.5243082 -4.5512109 -4.5663047 -4.5490236 -4.5431 -4.5883403 -4.654459 -4.6956182][-4.3853397 -4.4041829 -4.4372654 -4.4589367 -4.4604807 -4.4561615 -4.4599023 -4.4822421 -4.5084434 -4.4870291 -4.4051757 -4.338098 -4.3639517 -4.4696426 -4.5771451][-4.276185 -4.2981548 -4.3350134 -4.3511977 -4.3439531 -4.3416171 -4.3551035 -4.3806415 -4.3889546 -4.3267817 -4.1955867 -4.0915627 -4.1165638 -4.2793722 -4.467865]]...]
INFO - root - 2017-12-07 06:09:08.087461: step 4610, loss = 1.98, batch loss = 1.90 (11.3 examples/sec; 0.706 sec/batch; 64h:18m:18s remains)
INFO - root - 2017-12-07 06:09:15.161991: step 4620, loss = 2.05, batch loss = 1.97 (10.9 examples/sec; 0.735 sec/batch; 66h:56m:25s remains)
INFO - root - 2017-12-07 06:09:21.894464: step 4630, loss = 2.06, batch loss = 1.97 (12.7 examples/sec; 0.630 sec/batch; 57h:22m:08s remains)
INFO - root - 2017-12-07 06:09:28.908221: step 4640, loss = 2.01, batch loss = 1.92 (11.6 examples/sec; 0.691 sec/batch; 62h:58m:23s remains)
INFO - root - 2017-12-07 06:09:35.987865: step 4650, loss = 2.03, batch loss = 1.95 (12.0 examples/sec; 0.666 sec/batch; 60h:41m:38s remains)
INFO - root - 2017-12-07 06:09:42.938349: step 4660, loss = 2.04, batch loss = 1.96 (11.5 examples/sec; 0.696 sec/batch; 63h:22m:22s remains)
INFO - root - 2017-12-07 06:09:49.960591: step 4670, loss = 2.01, batch loss = 1.93 (11.2 examples/sec; 0.716 sec/batch; 65h:09m:42s remains)
INFO - root - 2017-12-07 06:09:57.015550: step 4680, loss = 2.01, batch loss = 1.92 (11.2 examples/sec; 0.711 sec/batch; 64h:45m:59s remains)
INFO - root - 2017-12-07 06:10:03.970174: step 4690, loss = 2.01, batch loss = 1.93 (11.5 examples/sec; 0.699 sec/batch; 63h:36m:15s remains)
INFO - root - 2017-12-07 06:10:10.971768: step 4700, loss = 2.02, batch loss = 1.94 (11.9 examples/sec; 0.672 sec/batch; 61h:10m:30s remains)
2017-12-07 06:10:11.818568: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3404641 -4.3435874 -4.38668 -4.4318881 -4.4440317 -4.4160895 -4.3867712 -4.3892612 -4.4251208 -4.467401 -4.4851208 -4.4717245 -4.439292 -4.4359188 -4.4622235][-4.2926717 -4.2971416 -4.3504658 -4.4157887 -4.4622369 -4.4610171 -4.4257827 -4.3940868 -4.3970065 -4.4238329 -4.4515452 -4.4529424 -4.4242554 -4.4138436 -4.4351864][-4.1782031 -4.1985741 -4.2695255 -4.359889 -4.4431376 -4.4720974 -4.4371262 -4.3819895 -4.3643045 -4.3906479 -4.434392 -4.4458532 -4.4129472 -4.3870063 -4.3944654][-4.1211057 -4.1658487 -4.249342 -4.3443584 -4.4206057 -4.432302 -4.373848 -4.3072057 -4.3025703 -4.3561954 -4.4209929 -4.4379454 -4.3976474 -4.3604646 -4.359983][-4.1507964 -4.20731 -4.2870312 -4.3555088 -4.3760695 -4.3138533 -4.2010512 -4.1361866 -4.1765838 -4.2767768 -4.3621483 -4.3798122 -4.3318067 -4.2883873 -4.292757][-4.2317486 -4.2775187 -4.3283086 -4.3460994 -4.297256 -4.1557126 -3.9885921 -3.9294243 -4.0145283 -4.1542516 -4.2493582 -4.2624097 -4.2099466 -4.1664963 -4.1827326][-4.3431473 -4.37208 -4.3837953 -4.34431 -4.2357321 -4.0419106 -3.8440847 -3.7798882 -3.8725896 -4.0165257 -4.102777 -4.1021743 -4.0485811 -4.0191374 -4.0568852][-4.4203906 -4.4495063 -4.4551082 -4.3956738 -4.2664995 -4.0681796 -3.872833 -3.7930658 -3.8536325 -3.9673724 -4.03351 -4.0198421 -3.9687307 -3.9540412 -4.0022264][-4.4726515 -4.5188866 -4.544322 -4.4998307 -4.3825207 -4.2093291 -4.0340152 -3.9385357 -3.9571748 -4.0323062 -4.082737 -4.0677137 -4.024436 -4.0130997 -4.0514851][-4.5030146 -4.5612664 -4.6038375 -4.5819836 -4.4929509 -4.3611126 -4.2184019 -4.1198807 -4.1088376 -4.1531892 -4.1915 -4.181407 -4.1456118 -4.1290855 -4.1536107][-4.4773769 -4.542706 -4.5986261 -4.6020117 -4.5511961 -4.4703045 -4.3660035 -4.2792783 -4.2599053 -4.2926307 -4.3277092 -4.3260465 -4.2942195 -4.2683086 -4.280313][-4.4536681 -4.5243793 -4.5863729 -4.6027751 -4.5743947 -4.5276871 -4.4572763 -4.3959622 -4.3912334 -4.426641 -4.4598794 -4.4607162 -4.4311275 -4.3981953 -4.4000635][-4.452692 -4.5241561 -4.5829062 -4.6004858 -4.5816607 -4.5532284 -4.5129104 -4.4862671 -4.5059261 -4.5488033 -4.5760593 -4.5714321 -4.5409985 -4.502779 -4.4927225][-4.4412913 -4.4998951 -4.5486383 -4.5700388 -4.5675359 -4.5585623 -4.545836 -4.5498233 -4.5878634 -4.6328678 -4.6510377 -4.6377778 -4.6054039 -4.5657606 -4.5453086][-4.4147248 -4.4493909 -4.4813061 -4.5046582 -4.5188961 -4.5283203 -4.5357933 -4.55663 -4.5978751 -4.6354895 -4.64456 -4.6270385 -4.596868 -4.563436 -4.5424786]]...]
INFO - root - 2017-12-07 06:10:18.821741: step 4710, loss = 2.02, batch loss = 1.94 (11.3 examples/sec; 0.708 sec/batch; 64h:26m:36s remains)
INFO - root - 2017-12-07 06:10:25.924755: step 4720, loss = 1.98, batch loss = 1.90 (11.0 examples/sec; 0.725 sec/batch; 66h:01m:15s remains)
INFO - root - 2017-12-07 06:10:33.108407: step 4730, loss = 1.98, batch loss = 1.90 (11.6 examples/sec; 0.689 sec/batch; 62h:44m:08s remains)
INFO - root - 2017-12-07 06:10:40.066232: step 4740, loss = 2.05, batch loss = 1.96 (12.5 examples/sec; 0.638 sec/batch; 58h:03m:16s remains)
INFO - root - 2017-12-07 06:10:47.059548: step 4750, loss = 1.98, batch loss = 1.90 (11.7 examples/sec; 0.683 sec/batch; 62h:08m:44s remains)
INFO - root - 2017-12-07 06:10:54.052666: step 4760, loss = 2.00, batch loss = 1.91 (11.9 examples/sec; 0.673 sec/batch; 61h:14m:33s remains)
INFO - root - 2017-12-07 06:11:01.045584: step 4770, loss = 1.99, batch loss = 1.91 (12.2 examples/sec; 0.657 sec/batch; 59h:46m:19s remains)
INFO - root - 2017-12-07 06:11:08.043755: step 4780, loss = 2.04, batch loss = 1.96 (11.0 examples/sec; 0.726 sec/batch; 66h:06m:33s remains)
INFO - root - 2017-12-07 06:11:15.120789: step 4790, loss = 2.01, batch loss = 1.93 (10.6 examples/sec; 0.755 sec/batch; 68h:44m:33s remains)
INFO - root - 2017-12-07 06:11:22.074036: step 4800, loss = 1.96, batch loss = 1.87 (11.8 examples/sec; 0.676 sec/batch; 61h:33m:36s remains)
2017-12-07 06:11:22.916634: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6536012 -4.6003866 -4.5451789 -4.5678177 -4.6315174 -4.6420307 -4.6155539 -4.5587149 -4.5126076 -4.5098457 -4.5217519 -4.5356722 -4.5323486 -4.4886637 -4.4171677][-4.59638 -4.5566444 -4.5258741 -4.5792437 -4.6588 -4.6649337 -4.6166453 -4.5372739 -4.4835072 -4.4957223 -4.5295491 -4.5518289 -4.5435281 -4.4826903 -4.3871059][-4.4844747 -4.4617715 -4.4684234 -4.56039 -4.6555982 -4.652967 -4.5789676 -4.4846272 -4.4337897 -4.4726281 -4.54096 -4.583858 -4.581018 -4.5151124 -4.4064412][-4.3843341 -4.3681655 -4.3957834 -4.4966726 -4.5854712 -4.5677996 -4.476768 -4.3821354 -4.3432131 -4.4127464 -4.5165 -4.5818791 -4.5835342 -4.5085831 -4.3895245][-4.3397417 -4.3134832 -4.32752 -4.3999524 -4.462822 -4.433773 -4.3442416 -4.2576466 -4.2227154 -4.2995028 -4.4176 -4.4971952 -4.5023232 -4.4204807 -4.2977395][-4.3576841 -4.3110504 -4.2865086 -4.3028765 -4.3246722 -4.2846889 -4.2007346 -4.1189146 -4.0739322 -4.1348381 -4.2516294 -4.345715 -4.367311 -4.2951021 -4.1838422][-4.3729448 -4.316946 -4.2562413 -4.2192445 -4.1993017 -4.1412663 -4.0506649 -3.9626162 -3.9055476 -3.9520731 -4.0731993 -4.1976614 -4.25459 -4.2069035 -4.1137362][-4.3267412 -4.2798228 -4.21068 -4.1545129 -4.1119547 -4.03071 -3.9226801 -3.8269362 -3.7731628 -3.8220761 -3.9542754 -4.1094007 -4.2014446 -4.1824503 -4.1063623][-4.2441568 -4.2304773 -4.1921034 -4.1615233 -4.13039 -4.0429831 -3.9284027 -3.83603 -3.7921319 -3.8379188 -3.9591503 -4.1092324 -4.209043 -4.2096257 -4.1485844][-4.1808138 -4.2193565 -4.2342572 -4.2534962 -4.2543321 -4.1803937 -4.0807147 -4.0061245 -3.9732246 -4.001204 -4.0823107 -4.1902251 -4.2690678 -4.2804327 -4.2352438][-4.1782961 -4.2420859 -4.2893338 -4.3460183 -4.375916 -4.3227339 -4.2422633 -4.1865778 -4.1675706 -4.1879115 -4.238225 -4.3052335 -4.3548579 -4.3679042 -4.3417225][-4.2131371 -4.2676578 -4.3160048 -4.3826842 -4.4309196 -4.4022818 -4.3446655 -4.3079848 -4.3072028 -4.3360152 -4.3811088 -4.4285331 -4.4542193 -4.4605494 -4.4500637][-4.2686224 -4.2948718 -4.3252831 -4.3832622 -4.4414921 -4.4385729 -4.4015164 -4.379199 -4.3902955 -4.4297972 -4.4849133 -4.5333228 -4.5513411 -4.5494704 -4.537806][-4.3398056 -4.336853 -4.3424625 -4.38463 -4.446907 -4.4670253 -4.4464812 -4.4274917 -4.4352756 -4.4737163 -4.5313606 -4.581048 -4.5999374 -4.5961881 -4.5757189][-4.3849354 -4.3582487 -4.3414316 -4.36903 -4.4371567 -4.4808831 -4.4795527 -4.4640021 -4.4627614 -4.4903245 -4.5368462 -4.5778542 -4.5995 -4.6054769 -4.591732]]...]
INFO - root - 2017-12-07 06:11:29.938511: step 4810, loss = 2.00, batch loss = 1.91 (11.3 examples/sec; 0.710 sec/batch; 64h:38m:03s remains)
INFO - root - 2017-12-07 06:11:36.956893: step 4820, loss = 2.05, batch loss = 1.97 (11.2 examples/sec; 0.714 sec/batch; 65h:01m:43s remains)
INFO - root - 2017-12-07 06:11:43.944353: step 4830, loss = 2.03, batch loss = 1.94 (11.8 examples/sec; 0.681 sec/batch; 61h:56m:43s remains)
INFO - root - 2017-12-07 06:11:50.952047: step 4840, loss = 2.06, batch loss = 1.98 (11.5 examples/sec; 0.698 sec/batch; 63h:32m:01s remains)
INFO - root - 2017-12-07 06:11:58.044476: step 4850, loss = 1.99, batch loss = 1.91 (11.0 examples/sec; 0.727 sec/batch; 66h:10m:05s remains)
INFO - root - 2017-12-07 06:12:04.977240: step 4860, loss = 2.03, batch loss = 1.94 (11.1 examples/sec; 0.718 sec/batch; 65h:21m:09s remains)
INFO - root - 2017-12-07 06:12:12.042540: step 4870, loss = 2.05, batch loss = 1.97 (11.6 examples/sec; 0.688 sec/batch; 62h:39m:32s remains)
INFO - root - 2017-12-07 06:12:19.046005: step 4880, loss = 1.99, batch loss = 1.90 (11.8 examples/sec; 0.676 sec/batch; 61h:29m:38s remains)
INFO - root - 2017-12-07 06:12:26.104899: step 4890, loss = 2.06, batch loss = 1.98 (11.3 examples/sec; 0.709 sec/batch; 64h:29m:57s remains)
INFO - root - 2017-12-07 06:12:33.139362: step 4900, loss = 2.00, batch loss = 1.92 (10.9 examples/sec; 0.736 sec/batch; 66h:59m:54s remains)
2017-12-07 06:12:33.916007: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.109489 -4.2265253 -4.3106294 -4.34567 -4.2849393 -4.1862411 -4.1555934 -4.1691175 -4.1829944 -4.205883 -4.2633696 -4.3454237 -4.4343691 -4.44252 -4.3794365][-4.1191087 -4.2578049 -4.3573756 -4.3897166 -4.3157578 -4.2008052 -4.1550159 -4.1791086 -4.222846 -4.271575 -4.334682 -4.413681 -4.5159526 -4.5588078 -4.5209618][-4.0579176 -4.2090282 -4.3323493 -4.3677711 -4.2736721 -4.1320844 -4.0643544 -4.0937448 -4.172791 -4.2692494 -4.3721242 -4.4739823 -4.5891771 -4.6492643 -4.6234241][-3.9427955 -4.094202 -4.24292 -4.2929058 -4.1892562 -4.0230947 -3.92379 -3.9361353 -4.0293303 -4.1673365 -4.3181214 -4.452404 -4.5708461 -4.62006 -4.5822463][-3.873605 -4.0112538 -4.1680384 -4.2295136 -4.1225266 -3.9304931 -3.7891369 -3.7651267 -3.8588068 -4.0316873 -4.227303 -4.3920264 -4.504158 -4.517828 -4.4423642][-3.8968732 -4.0202284 -4.1684704 -4.2291608 -4.1181331 -3.9014158 -3.7194111 -3.6574965 -3.7445223 -3.9412127 -4.1672893 -4.3532495 -4.4515963 -4.4179559 -4.2962894][-4.0106993 -4.1100554 -4.22294 -4.2560625 -4.1310134 -3.9017153 -3.7039578 -3.6262176 -3.7084188 -3.9143677 -4.1539512 -4.35103 -4.4344754 -4.3600559 -4.2010851][-4.146318 -4.1983013 -4.24655 -4.2253647 -4.0823236 -3.8655553 -3.693285 -3.637871 -3.721689 -3.9160352 -4.1450348 -4.3396454 -4.4209919 -4.34298 -4.1836677][-4.2321386 -4.2254415 -4.195704 -4.1157427 -3.9680655 -3.7993178 -3.697562 -3.6938024 -3.78077 -3.9430835 -4.1332378 -4.3041582 -4.390099 -4.3410068 -4.216712][-4.2840166 -4.2172484 -4.11107 -3.9823062 -3.8486106 -3.7543216 -3.7472959 -3.803942 -3.8974109 -4.0286684 -4.169239 -4.2993731 -4.3802223 -4.3602877 -4.2773409][-4.3465471 -4.2455573 -4.0901723 -3.9337282 -3.8188977 -3.78621 -3.8534362 -3.9498971 -4.0505557 -4.1632996 -4.2586784 -4.3450465 -4.4153 -4.414607 -4.3676419][-4.4438004 -4.3548222 -4.1925216 -4.0267162 -3.914402 -3.9060411 -4.0041304 -4.116004 -4.2245922 -4.3288417 -4.384974 -4.4273834 -4.4755411 -4.4746284 -4.4505644][-4.5495358 -4.4952006 -4.3580179 -4.2032061 -4.0953169 -4.091588 -4.190074 -4.3009624 -4.4075341 -4.4909635 -4.5031872 -4.4994183 -4.5117831 -4.4944658 -4.4789815][-4.6055746 -4.5963206 -4.5141115 -4.4029093 -4.3201261 -4.3175879 -4.3932714 -4.4801888 -4.5594854 -4.5992017 -4.564074 -4.5191903 -4.4975085 -4.4651775 -4.4536371][-4.5855412 -4.6200786 -4.598712 -4.544426 -4.4983072 -4.4980388 -4.541832 -4.5883737 -4.6222982 -4.6124415 -4.5427971 -4.4755549 -4.4356742 -4.3993564 -4.3887753]]...]
INFO - root - 2017-12-07 06:12:40.907202: step 4910, loss = 2.06, batch loss = 1.98 (12.4 examples/sec; 0.644 sec/batch; 58h:37m:19s remains)
INFO - root - 2017-12-07 06:12:48.002112: step 4920, loss = 2.00, batch loss = 1.91 (11.7 examples/sec; 0.683 sec/batch; 62h:09m:01s remains)
INFO - root - 2017-12-07 06:12:55.164342: step 4930, loss = 2.03, batch loss = 1.94 (11.3 examples/sec; 0.710 sec/batch; 64h:33m:46s remains)
INFO - root - 2017-12-07 06:13:02.205056: step 4940, loss = 2.01, batch loss = 1.92 (11.3 examples/sec; 0.706 sec/batch; 64h:13m:35s remains)
INFO - root - 2017-12-07 06:13:09.259669: step 4950, loss = 2.05, batch loss = 1.97 (11.2 examples/sec; 0.714 sec/batch; 64h:58m:41s remains)
INFO - root - 2017-12-07 06:13:16.274052: step 4960, loss = 2.04, batch loss = 1.96 (11.5 examples/sec; 0.699 sec/batch; 63h:33m:30s remains)
INFO - root - 2017-12-07 06:13:23.253756: step 4970, loss = 2.04, batch loss = 1.96 (11.0 examples/sec; 0.729 sec/batch; 66h:22m:10s remains)
INFO - root - 2017-12-07 06:13:30.245140: step 4980, loss = 2.04, batch loss = 1.95 (11.8 examples/sec; 0.679 sec/batch; 61h:48m:21s remains)
INFO - root - 2017-12-07 06:13:37.449436: step 4990, loss = 1.98, batch loss = 1.90 (11.9 examples/sec; 0.672 sec/batch; 61h:10m:36s remains)
INFO - root - 2017-12-07 06:13:44.621404: step 5000, loss = 2.02, batch loss = 1.94 (11.5 examples/sec; 0.693 sec/batch; 63h:04m:06s remains)
2017-12-07 06:13:45.409999: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5720916 -4.5930991 -4.5977235 -4.600965 -4.6323528 -4.6933494 -4.7472448 -4.7458858 -4.6796365 -4.5876951 -4.5467229 -4.5521607 -4.5779214 -4.6000648 -4.5764127][-4.5300751 -4.5495787 -4.5364614 -4.490026 -4.4676633 -4.5025673 -4.5656281 -4.5971355 -4.5628839 -4.4803658 -4.4239993 -4.417284 -4.45112 -4.498179 -4.5248117][-4.4689827 -4.4715133 -4.4380522 -4.3622494 -4.3063717 -4.3143497 -4.3692942 -4.4162049 -4.4136267 -4.36036 -4.3164296 -4.3123169 -4.3410077 -4.3831878 -4.44127][-4.3834152 -4.3636179 -4.3315835 -4.2640872 -4.2016444 -4.1783347 -4.1970997 -4.2215056 -4.222146 -4.1914849 -4.1850395 -4.2144046 -4.24893 -4.2778082 -4.3483748][-4.2634387 -4.2276082 -4.2269659 -4.1995125 -4.1522751 -4.0963311 -4.0590229 -4.0353651 -4.0154109 -4.0024252 -4.0409212 -4.1132426 -4.1634541 -4.1810117 -4.249485][-4.1323581 -4.0855179 -4.1092486 -4.1255207 -4.1083779 -4.0470281 -3.971379 -3.9019032 -3.8602903 -3.8635573 -3.9429512 -4.0577383 -4.1362739 -4.1579127 -4.2177763][-4.0624676 -3.9756217 -3.9695785 -3.9873705 -3.9983327 -3.9718082 -3.9086397 -3.8321829 -3.7956028 -3.8279929 -3.9484456 -4.1008635 -4.211761 -4.2398171 -4.2665915][-4.1118374 -3.9751635 -3.8979015 -3.8717499 -3.8895907 -3.9092271 -3.889997 -3.8430831 -3.8401773 -3.9140811 -4.0679789 -4.235497 -4.350872 -4.3603959 -4.3300805][-4.2440748 -4.0745525 -3.9351969 -3.867352 -3.8836355 -3.9327061 -3.9501669 -3.9379954 -3.98432 -4.1158633 -4.2947254 -4.4397507 -4.496521 -4.4430528 -4.3486209][-4.3204575 -4.147655 -3.9919741 -3.9164574 -3.9441257 -4.0112028 -4.0522962 -4.074976 -4.1713753 -4.3474641 -4.5296278 -4.6150789 -4.5660677 -4.4214854 -4.2714815][-4.3183761 -4.1653004 -4.028657 -3.9731834 -4.0212584 -4.1019063 -4.1547527 -4.1953325 -4.304132 -4.4732056 -4.6259856 -4.6539621 -4.5271349 -4.321486 -4.1327286][-4.2917762 -4.1526203 -4.037477 -4.0039978 -4.07176 -4.1649776 -4.2236414 -4.2721062 -4.3787174 -4.5202918 -4.6355777 -4.6341658 -4.4937811 -4.2853575 -4.0870357][-4.3014154 -4.1637149 -4.048492 -4.0150466 -4.0924273 -4.207006 -4.2845092 -4.34605 -4.4527 -4.5731211 -4.6611881 -4.6568437 -4.5468197 -4.377522 -4.2019176][-4.4346313 -4.3026671 -4.1677518 -4.1036162 -4.1627994 -4.2794366 -4.3612051 -4.4195967 -4.5108547 -4.6123281 -4.6872644 -4.69802 -4.6321778 -4.5177751 -4.3901][-4.6121588 -4.4877958 -4.3250866 -4.2202082 -4.2419777 -4.3325119 -4.394927 -4.433177 -4.504066 -4.6021714 -4.6865582 -4.7221527 -4.6996536 -4.6337872 -4.5555706]]...]
INFO - root - 2017-12-07 06:13:52.496603: step 5010, loss = 1.98, batch loss = 1.90 (11.1 examples/sec; 0.720 sec/batch; 65h:31m:36s remains)
INFO - root - 2017-12-07 06:13:59.410676: step 5020, loss = 2.03, batch loss = 1.95 (11.3 examples/sec; 0.706 sec/batch; 64h:13m:39s remains)
INFO - root - 2017-12-07 06:14:06.544382: step 5030, loss = 2.05, batch loss = 1.97 (11.4 examples/sec; 0.701 sec/batch; 63h:47m:44s remains)
INFO - root - 2017-12-07 06:14:13.568963: step 5040, loss = 2.01, batch loss = 1.93 (11.3 examples/sec; 0.705 sec/batch; 64h:09m:32s remains)
INFO - root - 2017-12-07 06:14:20.585044: step 5050, loss = 1.99, batch loss = 1.91 (10.7 examples/sec; 0.751 sec/batch; 68h:16m:08s remains)
INFO - root - 2017-12-07 06:14:27.642502: step 5060, loss = 2.00, batch loss = 1.92 (10.9 examples/sec; 0.731 sec/batch; 66h:31m:51s remains)
INFO - root - 2017-12-07 06:14:34.721143: step 5070, loss = 2.08, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 64h:11m:40s remains)
INFO - root - 2017-12-07 06:14:41.865183: step 5080, loss = 2.02, batch loss = 1.94 (11.4 examples/sec; 0.701 sec/batch; 63h:43m:06s remains)
INFO - root - 2017-12-07 06:14:48.979152: step 5090, loss = 2.04, batch loss = 1.95 (11.4 examples/sec; 0.700 sec/batch; 63h:38m:55s remains)
INFO - root - 2017-12-07 06:14:56.067855: step 5100, loss = 2.06, batch loss = 1.98 (12.1 examples/sec; 0.661 sec/batch; 60h:06m:05s remains)
2017-12-07 06:14:56.848469: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6785564 -4.7004557 -4.6839824 -4.6394749 -4.5752707 -4.5208478 -4.48509 -4.4525642 -4.4274569 -4.4153843 -4.4244962 -4.4513674 -4.4779124 -4.5018058 -4.5090127][-4.6519055 -4.6635613 -4.6562991 -4.6284423 -4.5687184 -4.5008931 -4.438385 -4.3780689 -4.3455386 -4.3413119 -4.3642073 -4.407608 -4.4486713 -4.4821668 -4.4923239][-4.5920677 -4.5874381 -4.5946474 -4.5969224 -4.5504007 -4.4650269 -4.3651762 -4.2742877 -4.2417717 -4.2561979 -4.2992768 -4.3541102 -4.3989925 -4.4329829 -4.4436011][-4.5400853 -4.5228596 -4.5426941 -4.5559068 -4.4938431 -4.375813 -4.2454247 -4.1499915 -4.1415339 -4.1858459 -4.2515249 -4.3092275 -4.3404188 -4.3546486 -4.351182][-4.479362 -4.4621468 -4.4807897 -4.4649472 -4.3574147 -4.2113853 -4.0791264 -4.0174603 -4.0566044 -4.1449137 -4.2371902 -4.2936764 -4.3001423 -4.28207 -4.2588153][-4.4293485 -4.4296017 -4.4289041 -4.3643827 -4.2223477 -4.0728221 -3.9541025 -3.9248002 -4.0050788 -4.1393828 -4.2672939 -4.3300128 -4.3183188 -4.2731857 -4.2350264][-4.4191608 -4.4459739 -4.4195523 -4.3146634 -4.1611991 -4.0114741 -3.887347 -3.8649809 -3.969209 -4.1480513 -4.3196697 -4.4035459 -4.3878908 -4.3257275 -4.2752657][-4.4173789 -4.4672546 -4.4308906 -4.3145862 -4.1559181 -3.9896338 -3.8481708 -3.8299742 -3.9609413 -4.1735353 -4.3694825 -4.4642129 -4.4466329 -4.3748264 -4.3149152][-4.3940215 -4.4430623 -4.4122262 -4.3161569 -4.1644163 -3.9868441 -3.8591557 -3.8764641 -4.0372171 -4.2512636 -4.4286284 -4.5071874 -4.4803729 -4.40663 -4.3442421][-4.3415456 -4.3581967 -4.3306813 -4.2713628 -4.1584287 -4.0243154 -3.9643166 -4.0409107 -4.2119732 -4.3852425 -4.5072765 -4.5500288 -4.5159106 -4.4497428 -4.3935537][-4.2950878 -4.25203 -4.2101612 -4.1874132 -4.1458683 -4.1025796 -4.1328168 -4.2601047 -4.4119229 -4.520062 -4.5756955 -4.5842471 -4.5506625 -4.5009861 -4.46057][-4.2938 -4.2032337 -4.1456132 -4.1469584 -4.1634769 -4.1912065 -4.2761893 -4.4178057 -4.5368319 -4.5893126 -4.5968571 -4.5863175 -4.56154 -4.5319843 -4.5094271][-4.3204937 -4.2208714 -4.164711 -4.1798382 -4.2238674 -4.2749047 -4.3577175 -4.4662309 -4.5433807 -4.5630193 -4.5527616 -4.5425396 -4.5307178 -4.5185204 -4.5097156][-4.3151379 -4.222 -4.1726923 -4.1987967 -4.2614708 -4.3244438 -4.3912406 -4.4548416 -4.4883823 -4.4850116 -4.4721069 -4.469924 -4.4693441 -4.4685426 -4.4684668][-4.2366476 -4.1455278 -4.1061435 -4.1424379 -4.2225313 -4.3098564 -4.3884292 -4.4362864 -4.4422483 -4.4193687 -4.4048853 -4.407608 -4.4123983 -4.4163423 -4.4197578]]...]
INFO - root - 2017-12-07 06:15:03.988166: step 5110, loss = 2.03, batch loss = 1.94 (11.8 examples/sec; 0.675 sec/batch; 61h:25m:03s remains)
INFO - root - 2017-12-07 06:15:11.028850: step 5120, loss = 1.99, batch loss = 1.91 (12.3 examples/sec; 0.648 sec/batch; 58h:56m:37s remains)
INFO - root - 2017-12-07 06:15:17.895798: step 5130, loss = 2.06, batch loss = 1.97 (11.6 examples/sec; 0.691 sec/batch; 62h:49m:21s remains)
INFO - root - 2017-12-07 06:15:24.997793: step 5140, loss = 2.00, batch loss = 1.92 (11.8 examples/sec; 0.676 sec/batch; 61h:27m:42s remains)
INFO - root - 2017-12-07 06:15:31.830956: step 5150, loss = 2.01, batch loss = 1.93 (11.9 examples/sec; 0.670 sec/batch; 60h:55m:56s remains)
INFO - root - 2017-12-07 06:15:38.954724: step 5160, loss = 2.08, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 59h:05m:38s remains)
INFO - root - 2017-12-07 06:15:46.063512: step 5170, loss = 2.01, batch loss = 1.93 (12.4 examples/sec; 0.644 sec/batch; 58h:33m:45s remains)
INFO - root - 2017-12-07 06:15:53.147612: step 5180, loss = 1.98, batch loss = 1.90 (11.4 examples/sec; 0.700 sec/batch; 63h:39m:48s remains)
INFO - root - 2017-12-07 06:16:00.357622: step 5190, loss = 2.00, batch loss = 1.92 (11.3 examples/sec; 0.705 sec/batch; 64h:06m:24s remains)
INFO - root - 2017-12-07 06:16:07.430477: step 5200, loss = 2.02, batch loss = 1.93 (11.6 examples/sec; 0.689 sec/batch; 62h:37m:52s remains)
2017-12-07 06:16:08.200570: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4946465 -4.4966445 -4.4860134 -4.4626479 -4.4340692 -4.4087915 -4.3916144 -4.3806729 -4.3683372 -4.3485756 -4.322588 -4.2965903 -4.2751794 -4.2586479 -4.2463584][-4.5139818 -4.5472965 -4.5599761 -4.5428324 -4.5060716 -4.467628 -4.4433422 -4.4362178 -4.433702 -4.4197717 -4.3901296 -4.3509812 -4.3119431 -4.2801328 -4.2575464][-4.4235573 -4.4974489 -4.5502963 -4.5519514 -4.5094204 -4.4531336 -4.41912 -4.4216766 -4.4450111 -4.4612246 -4.4521494 -4.4169135 -4.3670325 -4.3179736 -4.2806273][-4.2417088 -4.3515239 -4.4491081 -4.4777641 -4.4323368 -4.34936 -4.2923956 -4.2977586 -4.3542414 -4.4241948 -4.4680777 -4.4644704 -4.4201503 -4.3586264 -4.3058758][-4.0504284 -4.1895919 -4.3180327 -4.3629589 -4.3064752 -4.1861734 -4.0925326 -4.0865245 -4.1689663 -4.298202 -4.4119368 -4.4635024 -4.4461813 -4.3850274 -4.3232603][-3.9132886 -4.0799422 -4.2216291 -4.2621393 -4.17885 -4.0151081 -3.8857815 -3.8706949 -3.9728734 -4.1451669 -4.3138485 -4.4179153 -4.4376955 -4.390388 -4.3292656][-3.8717887 -4.05715 -4.2000637 -4.2231617 -4.1032362 -3.8947601 -3.7350593 -3.7212591 -3.8467281 -4.0458016 -4.24188 -4.3772812 -4.4257755 -4.3950558 -4.3374362][-3.9495738 -4.1229386 -4.2542753 -4.2635889 -4.1260471 -3.9008112 -3.7235115 -3.7060015 -3.83987 -4.0430112 -4.2409186 -4.3850765 -4.4479561 -4.4282055 -4.3693094][-4.1235056 -4.2549372 -4.359302 -4.3616467 -4.2380095 -4.0412807 -3.8780951 -3.8478265 -3.953249 -4.1193957 -4.2889175 -4.4243693 -4.4943447 -4.4864264 -4.428647][-4.3081841 -4.3976936 -4.4677382 -4.4603405 -4.3670092 -4.2301893 -4.1119456 -4.0769682 -4.1280379 -4.2178454 -4.3268242 -4.4357486 -4.5130658 -4.5314126 -4.4927211][-4.4216757 -4.4805484 -4.51648 -4.4970121 -4.4356828 -4.3587832 -4.2924852 -4.2643008 -4.2608991 -4.26526 -4.2999468 -4.3722606 -4.4590654 -4.5204434 -4.5240235][-4.4390612 -4.4727058 -4.4754333 -4.4461012 -4.4113393 -4.3725529 -4.336555 -4.3065376 -4.2564063 -4.1960058 -4.1778121 -4.2230062 -4.3227887 -4.4328752 -4.49277][-4.3982377 -4.4059649 -4.3807783 -4.3461471 -4.3270826 -4.3007708 -4.260509 -4.2055469 -4.1146536 -4.021853 -3.9913423 -4.0397496 -4.1577125 -4.3046222 -4.4119005][-4.349319 -4.3361998 -4.2949109 -4.2612062 -4.2510715 -4.2276764 -4.1716466 -4.0824089 -3.9573951 -3.8477094 -3.8254726 -3.8968365 -4.037076 -4.2031593 -4.3347864][-4.3164148 -4.2870317 -4.2420826 -4.2159586 -4.217895 -4.2093649 -4.1564522 -4.0562296 -3.9183202 -3.7926152 -3.7613082 -3.8437493 -3.9991491 -4.1746178 -4.3166542]]...]
INFO - root - 2017-12-07 06:16:15.251804: step 5210, loss = 2.02, batch loss = 1.93 (11.4 examples/sec; 0.704 sec/batch; 63h:59m:26s remains)
INFO - root - 2017-12-07 06:16:22.340067: step 5220, loss = 2.01, batch loss = 1.93 (11.7 examples/sec; 0.685 sec/batch; 62h:17m:04s remains)
INFO - root - 2017-12-07 06:16:29.416949: step 5230, loss = 2.01, batch loss = 1.92 (11.0 examples/sec; 0.726 sec/batch; 65h:59m:01s remains)
INFO - root - 2017-12-07 06:16:36.457425: step 5240, loss = 2.05, batch loss = 1.97 (11.6 examples/sec; 0.691 sec/batch; 62h:51m:02s remains)
INFO - root - 2017-12-07 06:16:43.518212: step 5250, loss = 2.01, batch loss = 1.93 (10.8 examples/sec; 0.740 sec/batch; 67h:14m:27s remains)
INFO - root - 2017-12-07 06:16:50.549535: step 5260, loss = 2.05, batch loss = 1.96 (11.5 examples/sec; 0.696 sec/batch; 63h:14m:15s remains)
INFO - root - 2017-12-07 06:16:57.631965: step 5270, loss = 2.03, batch loss = 1.95 (11.2 examples/sec; 0.713 sec/batch; 64h:50m:44s remains)
INFO - root - 2017-12-07 06:17:04.773127: step 5280, loss = 2.04, batch loss = 1.96 (11.3 examples/sec; 0.708 sec/batch; 64h:18m:59s remains)
INFO - root - 2017-12-07 06:17:11.792303: step 5290, loss = 1.99, batch loss = 1.90 (13.4 examples/sec; 0.597 sec/batch; 54h:15m:43s remains)
INFO - root - 2017-12-07 06:17:18.784246: step 5300, loss = 2.01, batch loss = 1.93 (10.8 examples/sec; 0.742 sec/batch; 67h:26m:15s remains)
2017-12-07 06:17:19.563120: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5823827 -4.6013618 -4.6336641 -4.6481609 -4.6303163 -4.6000891 -4.5861764 -4.5571117 -4.5211787 -4.519702 -4.5684295 -4.6373777 -4.67293 -4.6515551 -4.5739112][-4.6609058 -4.6582584 -4.6833758 -4.7053308 -4.6932707 -4.6729093 -4.6816654 -4.6640606 -4.6239696 -4.6215739 -4.6869984 -4.7775087 -4.8196874 -4.7868066 -4.6817551][-4.666254 -4.6444755 -4.6616416 -4.6897454 -4.6687665 -4.6355152 -4.6538696 -4.6618352 -4.6520596 -4.6757278 -4.7600312 -4.8621521 -4.9081984 -4.8765893 -4.7656188][-4.5376081 -4.5306668 -4.5614772 -4.5998564 -4.5543118 -4.4703393 -4.4526405 -4.4784317 -4.5275469 -4.6074467 -4.7222137 -4.83645 -4.8929834 -4.883162 -4.7985168][-4.3263459 -4.3630648 -4.4236298 -4.4666953 -4.3835697 -4.2196717 -4.1232471 -4.1398449 -4.2499056 -4.4048615 -4.5718756 -4.7129312 -4.7899532 -4.8128076 -4.7712326][-4.1346903 -4.2060137 -4.2897844 -4.3257537 -4.2070556 -3.9694185 -3.7825217 -3.7578173 -3.8940928 -4.1075754 -4.33148 -4.5178957 -4.6354985 -4.6939011 -4.6950431][-4.0764942 -4.1520166 -4.2382183 -4.2591677 -4.117712 -3.8367715 -3.5796671 -3.4986582 -3.6163826 -3.8439641 -4.0992537 -4.3321128 -4.50359 -4.5995793 -4.6310263][-4.196569 -4.2445841 -4.322649 -4.3386993 -4.1988282 -3.9146349 -3.6281209 -3.5052226 -3.5873439 -3.7978392 -4.0513315 -4.2979808 -4.4995761 -4.613822 -4.6446948][-4.3573942 -4.3647375 -4.4320827 -4.462193 -4.3565526 -4.1186571 -3.8613839 -3.7274837 -3.7719564 -3.952028 -4.1888113 -4.4234829 -4.6177988 -4.718555 -4.7168474][-4.4738822 -4.4456329 -4.4979219 -4.5395908 -4.4765635 -4.3120213 -4.1244764 -4.0080838 -4.022059 -4.1663275 -4.382318 -4.5994072 -4.7660484 -4.8317256 -4.7841568][-4.5734024 -4.5392475 -4.5759468 -4.614346 -4.5803804 -4.4852509 -4.3738079 -4.2891817 -4.2782955 -4.3738995 -4.5508032 -4.7368255 -4.8626819 -4.8814859 -4.7913432][-4.6425986 -4.6197634 -4.64328 -4.6764927 -4.6664128 -4.6287055 -4.5807958 -4.5287161 -4.4998407 -4.5435324 -4.6652727 -4.801425 -4.8748446 -4.8462825 -4.7272515][-4.6390867 -4.6250257 -4.6453424 -4.6776867 -4.6843219 -4.6794372 -4.6699519 -4.6467376 -4.6214495 -4.6351886 -4.7041078 -4.7812424 -4.8024759 -4.7429838 -4.6208572][-4.5756679 -4.5679393 -4.5864358 -4.6134982 -4.62175 -4.6228185 -4.6240153 -4.6163716 -4.6031551 -4.6070371 -4.6415524 -4.6761041 -4.667438 -4.6045547 -4.5042644][-4.444983 -4.4382367 -4.4506369 -4.468842 -4.4743528 -4.4754105 -4.4783964 -4.4781632 -4.4746165 -4.4770184 -4.4923124 -4.5033226 -4.4876637 -4.4428363 -4.3807817]]...]
INFO - root - 2017-12-07 06:17:26.621155: step 5310, loss = 2.10, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 63h:20m:30s remains)
INFO - root - 2017-12-07 06:17:33.582299: step 5320, loss = 1.98, batch loss = 1.90 (11.2 examples/sec; 0.716 sec/batch; 65h:05m:14s remains)
INFO - root - 2017-12-07 06:17:40.662096: step 5330, loss = 2.02, batch loss = 1.94 (10.7 examples/sec; 0.749 sec/batch; 68h:06m:34s remains)
INFO - root - 2017-12-07 06:17:47.807656: step 5340, loss = 2.00, batch loss = 1.92 (11.3 examples/sec; 0.705 sec/batch; 64h:04m:54s remains)
INFO - root - 2017-12-07 06:17:54.868774: step 5350, loss = 2.04, batch loss = 1.96 (12.0 examples/sec; 0.664 sec/batch; 60h:19m:55s remains)
INFO - root - 2017-12-07 06:18:01.920323: step 5360, loss = 2.05, batch loss = 1.97 (11.6 examples/sec; 0.689 sec/batch; 62h:38m:19s remains)
INFO - root - 2017-12-07 06:18:08.977582: step 5370, loss = 2.00, batch loss = 1.92 (10.9 examples/sec; 0.735 sec/batch; 66h:45m:24s remains)
INFO - root - 2017-12-07 06:18:15.976112: step 5380, loss = 2.04, batch loss = 1.95 (11.1 examples/sec; 0.722 sec/batch; 65h:37m:15s remains)
INFO - root - 2017-12-07 06:18:22.921227: step 5390, loss = 2.04, batch loss = 1.96 (11.8 examples/sec; 0.679 sec/batch; 61h:40m:36s remains)
INFO - root - 2017-12-07 06:18:30.037318: step 5400, loss = 2.01, batch loss = 1.93 (10.6 examples/sec; 0.752 sec/batch; 68h:20m:43s remains)
2017-12-07 06:18:30.838596: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3654895 -4.4475226 -4.4953952 -4.49253 -4.4722061 -4.457366 -4.4654479 -4.5098948 -4.5754805 -4.6260867 -4.6285119 -4.5940018 -4.5520439 -4.530015 -4.5264897][-4.3905072 -4.4938536 -4.5768576 -4.6104937 -4.604846 -4.5797691 -4.5639944 -4.58669 -4.6545177 -4.7243114 -4.7306228 -4.6776295 -4.6128912 -4.5799756 -4.5747619][-4.4260268 -4.5521646 -4.6569905 -4.7048678 -4.6921892 -4.6432881 -4.5996585 -4.5933757 -4.6477151 -4.7202911 -4.7275043 -4.6758165 -4.6174297 -4.58563 -4.5646057][-4.474102 -4.6014991 -4.6903896 -4.7084908 -4.6582174 -4.5722809 -4.49731 -4.4583 -4.4988546 -4.5882015 -4.6256213 -4.6100526 -4.5822325 -4.55459 -4.5047507][-4.5183935 -4.6171455 -4.6495571 -4.5994544 -4.4908094 -4.3616271 -4.2544932 -4.1883063 -4.2262688 -4.3555684 -4.4521961 -4.4998417 -4.5277605 -4.5251288 -4.4652414][-4.5293822 -4.5673275 -4.5044818 -4.3597946 -4.185626 -4.0208917 -3.8997207 -3.8324199 -3.8904297 -4.0729852 -4.2327056 -4.34804 -4.4549832 -4.5130396 -4.4722595][-4.490447 -4.4547081 -4.2976394 -4.0752444 -3.8629286 -3.68978 -3.5738378 -3.5230763 -3.6121647 -3.8426483 -4.0478077 -4.2086515 -4.3814621 -4.5011835 -4.4822578][-4.4559689 -4.3800373 -4.1768355 -3.9242308 -3.7017343 -3.5294538 -3.4239984 -3.3998508 -3.5288224 -3.7943883 -4.0044627 -4.1522145 -4.3319077 -4.476532 -4.4753909][-4.480546 -4.4171758 -4.2321033 -4.0075369 -3.8066251 -3.6433623 -3.5426812 -3.5286865 -3.6762748 -3.9427454 -4.1213303 -4.22108 -4.3590856 -4.4782014 -4.4680204][-4.5584035 -4.5470967 -4.4313951 -4.27668 -4.1134238 -3.9510581 -3.8325806 -3.8054323 -3.9543142 -4.2051473 -4.3497863 -4.4008594 -4.4718733 -4.5226207 -4.4704919][-4.6385541 -4.6800294 -4.6251349 -4.5253506 -4.3922005 -4.2406249 -4.1289291 -4.1140776 -4.2611327 -4.4721317 -4.5722337 -4.5827971 -4.5953832 -4.5790067 -4.4895391][-4.674355 -4.7406335 -4.7195549 -4.6604838 -4.5651755 -4.4485278 -4.3648758 -4.36816 -4.4950681 -4.6464038 -4.7013788 -4.6857719 -4.6526527 -4.5718031 -4.4471][-4.6557636 -4.73549 -4.7372046 -4.7016659 -4.6311049 -4.5366473 -4.4740973 -4.4928918 -4.6004019 -4.7012987 -4.7182503 -4.6746573 -4.5909791 -4.4478745 -4.2941332][-4.5732427 -4.6550655 -4.6637735 -4.6282291 -4.5622115 -4.4805312 -4.4361691 -4.474299 -4.5755663 -4.6457415 -4.6352148 -4.5653176 -4.4403892 -4.2534184 -4.0871191][-4.4502187 -4.5227342 -4.5351276 -4.4967217 -4.4280229 -4.3500671 -4.3151093 -4.3682218 -4.4726233 -4.5381832 -4.524168 -4.44532 -4.2948227 -4.0761833 -3.8945475]]...]
INFO - root - 2017-12-07 06:18:37.915517: step 5410, loss = 2.05, batch loss = 1.96 (11.3 examples/sec; 0.706 sec/batch; 64h:09m:27s remains)
INFO - root - 2017-12-07 06:18:45.020121: step 5420, loss = 2.01, batch loss = 1.92 (10.8 examples/sec; 0.743 sec/batch; 67h:31m:42s remains)
INFO - root - 2017-12-07 06:18:52.050308: step 5430, loss = 2.01, batch loss = 1.93 (10.5 examples/sec; 0.759 sec/batch; 68h:57m:15s remains)
INFO - root - 2017-12-07 06:18:59.101304: step 5440, loss = 2.06, batch loss = 1.97 (11.0 examples/sec; 0.731 sec/batch; 66h:22m:18s remains)
INFO - root - 2017-12-07 06:19:06.145837: step 5450, loss = 2.00, batch loss = 1.92 (12.0 examples/sec; 0.669 sec/batch; 60h:48m:19s remains)
INFO - root - 2017-12-07 06:19:13.065236: step 5460, loss = 2.02, batch loss = 1.93 (11.9 examples/sec; 0.670 sec/batch; 60h:53m:20s remains)
INFO - root - 2017-12-07 06:19:20.196845: step 5470, loss = 2.05, batch loss = 1.97 (11.7 examples/sec; 0.684 sec/batch; 62h:08m:09s remains)
INFO - root - 2017-12-07 06:19:27.221042: step 5480, loss = 2.05, batch loss = 1.97 (11.4 examples/sec; 0.700 sec/batch; 63h:35m:09s remains)
INFO - root - 2017-12-07 06:19:34.232859: step 5490, loss = 2.03, batch loss = 1.95 (11.2 examples/sec; 0.714 sec/batch; 64h:50m:48s remains)
INFO - root - 2017-12-07 06:19:41.337722: step 5500, loss = 2.03, batch loss = 1.95 (11.5 examples/sec; 0.695 sec/batch; 63h:08m:15s remains)
2017-12-07 06:19:42.109587: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5077877 -4.5436382 -4.5500946 -4.5607629 -4.5982623 -4.6486111 -4.6568451 -4.6508479 -4.6521516 -4.6375475 -4.6156154 -4.5973692 -4.5851502 -4.5688148 -4.5387192][-4.5686059 -4.6066818 -4.6093688 -4.6073108 -4.6240988 -4.661305 -4.6500649 -4.6309605 -4.6506944 -4.6668515 -4.6737647 -4.6714725 -4.663794 -4.6437364 -4.6010871][-4.6399913 -4.6864738 -4.6897879 -4.6672077 -4.6437469 -4.64605 -4.5954704 -4.539906 -4.5753207 -4.6420741 -4.6971545 -4.7233486 -4.7296391 -4.7147303 -4.6667132][-4.6872706 -4.7193727 -4.6979733 -4.6359582 -4.5650516 -4.5364962 -4.451622 -4.3588743 -4.4221506 -4.56885 -4.6889668 -4.7515054 -4.7761855 -4.7680321 -4.7141533][-4.7013416 -4.6846786 -4.6014643 -4.4740267 -4.3479834 -4.2947736 -4.1817336 -4.046371 -4.1356587 -4.3776717 -4.5773983 -4.69743 -4.7619629 -4.7751493 -4.7217865][-4.6883163 -4.6061754 -4.4435539 -4.2422071 -4.06263 -3.9875538 -3.8429642 -3.6449435 -3.7322814 -4.0538235 -4.3370452 -4.5418973 -4.6840959 -4.7467771 -4.7081156][-4.664793 -4.5287542 -4.3061123 -4.0455813 -3.8177354 -3.7185781 -3.5411186 -3.2799578 -3.3523736 -3.726675 -4.0746913 -4.3632069 -4.5879154 -4.704401 -4.6873474][-4.6708784 -4.5338373 -4.3181872 -4.0527225 -3.8025012 -3.6748435 -3.463212 -3.1661086 -3.2480273 -3.648982 -4.0230002 -4.3450117 -4.590807 -4.7091532 -4.6859818][-4.7285686 -4.65646 -4.523612 -4.3077431 -4.0605183 -3.9051185 -3.6601017 -3.3605285 -3.4646888 -3.8523645 -4.2075372 -4.5089159 -4.7118921 -4.7759233 -4.7094107][-4.786232 -4.8023548 -4.7829738 -4.6538787 -4.4421506 -4.2752032 -4.0112157 -3.7341366 -3.8466575 -4.1843066 -4.4885378 -4.7363415 -4.8666973 -4.8579173 -4.7410989][-4.7896781 -4.8758645 -4.9597931 -4.9292083 -4.7784615 -4.6226621 -4.3728013 -4.1540127 -4.2736187 -4.53928 -4.7618442 -4.9249654 -4.9714828 -4.899447 -4.7529364][-4.7203779 -4.8245764 -4.956028 -4.9969997 -4.9187479 -4.8058529 -4.6116567 -4.473103 -4.5899787 -4.7696877 -4.8998241 -4.97636 -4.9603605 -4.8608985 -4.7265615][-4.5973744 -4.6812906 -4.8055105 -4.8775735 -4.8632474 -4.8110495 -4.6944256 -4.6312037 -4.7259941 -4.8222103 -4.8745856 -4.8922539 -4.8554463 -4.7726889 -4.6846275][-4.4807262 -4.5354161 -4.6267281 -4.6991138 -4.7246122 -4.7219563 -4.6763415 -4.6631494 -4.7220821 -4.7555904 -4.7612004 -4.7561383 -4.7309604 -4.6898489 -4.6545596][-4.400301 -4.4325571 -4.4896755 -4.5447125 -4.5790133 -4.5950584 -4.5817847 -4.5814466 -4.60842 -4.6187544 -4.6226177 -4.6314917 -4.63509 -4.6338391 -4.6352348]]...]
INFO - root - 2017-12-07 06:19:49.184098: step 5510, loss = 2.04, batch loss = 1.95 (11.4 examples/sec; 0.703 sec/batch; 63h:50m:02s remains)
INFO - root - 2017-12-07 06:19:56.153619: step 5520, loss = 2.05, batch loss = 1.97 (10.9 examples/sec; 0.731 sec/batch; 66h:21m:48s remains)
INFO - root - 2017-12-07 06:20:03.246219: step 5530, loss = 2.01, batch loss = 1.93 (11.1 examples/sec; 0.718 sec/batch; 65h:10m:31s remains)
INFO - root - 2017-12-07 06:20:10.268788: step 5540, loss = 2.03, batch loss = 1.95 (11.9 examples/sec; 0.670 sec/batch; 60h:51m:15s remains)
INFO - root - 2017-12-07 06:20:17.387087: step 5550, loss = 2.01, batch loss = 1.92 (11.9 examples/sec; 0.675 sec/batch; 61h:16m:38s remains)
INFO - root - 2017-12-07 06:20:24.518124: step 5560, loss = 2.05, batch loss = 1.97 (11.9 examples/sec; 0.673 sec/batch; 61h:06m:18s remains)
INFO - root - 2017-12-07 06:20:31.580589: step 5570, loss = 2.01, batch loss = 1.93 (11.3 examples/sec; 0.707 sec/batch; 64h:11m:13s remains)
INFO - root - 2017-12-07 06:20:38.561839: step 5580, loss = 2.05, batch loss = 1.97 (11.6 examples/sec; 0.692 sec/batch; 62h:51m:21s remains)
INFO - root - 2017-12-07 06:20:45.663096: step 5590, loss = 2.03, batch loss = 1.94 (10.9 examples/sec; 0.737 sec/batch; 66h:53m:31s remains)
INFO - root - 2017-12-07 06:20:52.680310: step 5600, loss = 2.04, batch loss = 1.96 (10.6 examples/sec; 0.758 sec/batch; 68h:48m:16s remains)
2017-12-07 06:20:53.456816: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3832021 -4.4126773 -4.4471412 -4.4721613 -4.47609 -4.4630184 -4.4538584 -4.4593973 -4.4739137 -4.4786139 -4.462111 -4.426878 -4.3887758 -4.3624597 -4.3482594][-4.4220843 -4.4641533 -4.5189915 -4.5634694 -4.5723372 -4.5490084 -4.5282311 -4.5287066 -4.5433488 -4.5517983 -4.5369334 -4.4961882 -4.447196 -4.4103279 -4.3897295][-4.4486256 -4.4994974 -4.5740256 -4.6356125 -4.6444044 -4.6063032 -4.5722542 -4.566525 -4.5823712 -4.6017723 -4.5994992 -4.5624652 -4.5097318 -4.462111 -4.4304256][-4.4525824 -4.5087972 -4.59114 -4.650682 -4.6395736 -4.5731759 -4.5153589 -4.4976292 -4.5215068 -4.5697308 -4.5977807 -4.5795374 -4.5336223 -4.4785237 -4.4349103][-4.4393044 -4.4939046 -4.5681663 -4.6034288 -4.5446634 -4.4240284 -4.3219228 -4.2922478 -4.3498387 -4.4585576 -4.5398908 -4.5518832 -4.5184889 -4.4585981 -4.4047065][-4.4047561 -4.4452705 -4.5008855 -4.5024519 -4.38264 -4.1915154 -4.0402908 -4.0045753 -4.1064925 -4.2838106 -4.4267197 -4.4806824 -4.4711666 -4.4201694 -4.3692269][-4.3467965 -4.3706636 -4.4140129 -4.3894968 -4.2156725 -3.9584739 -3.7584858 -3.7030005 -3.8218408 -4.0411396 -4.2359829 -4.3426013 -4.3774104 -4.3627419 -4.3397527][-4.3023853 -4.3306975 -4.3828397 -4.352725 -4.1506419 -3.8526886 -3.6153023 -3.5336587 -3.6453853 -3.8785386 -4.0979786 -4.2342587 -4.3026919 -4.3270183 -4.3386669][-4.2958441 -4.3509841 -4.4378266 -4.4341888 -4.2460146 -3.9481454 -3.6938169 -3.5904405 -3.6818733 -3.8967149 -4.0910487 -4.2054219 -4.2709889 -4.31527 -4.3561964][-4.3185368 -4.402616 -4.5273962 -4.56611 -4.42376 -4.1587996 -3.9037871 -3.7822204 -3.8431826 -4.0184622 -4.1659374 -4.2377591 -4.2771444 -4.3190584 -4.3760252][-4.3716764 -4.4642076 -4.6036935 -4.6794376 -4.5986028 -4.3945007 -4.1693339 -4.0493321 -4.08622 -4.219265 -4.3271365 -4.3590717 -4.3628197 -4.3833213 -4.4330168][-4.4344397 -4.5142412 -4.6410203 -4.7339067 -4.7081919 -4.5722132 -4.3954315 -4.2897835 -4.3100286 -4.4092627 -4.4894686 -4.4982386 -4.482089 -4.4860969 -4.51876][-4.4803433 -4.539885 -4.6398058 -4.7316761 -4.7448883 -4.6675553 -4.5382872 -4.4516377 -4.4624043 -4.5398197 -4.6049147 -4.6088529 -4.5921974 -4.5880055 -4.595367][-4.4931364 -4.5369005 -4.6129265 -4.6969256 -4.7373323 -4.7088661 -4.6264429 -4.5608974 -4.5643878 -4.6184759 -4.6632752 -4.6625843 -4.6471882 -4.6332855 -4.6138959][-4.4581709 -4.4901466 -4.5409579 -4.6042762 -4.6496735 -4.6516557 -4.6109042 -4.5681033 -4.5631866 -4.5890946 -4.6119132 -4.6113925 -4.6026373 -4.5848269 -4.55096]]...]
INFO - root - 2017-12-07 06:21:00.627092: step 5610, loss = 2.04, batch loss = 1.95 (11.7 examples/sec; 0.686 sec/batch; 62h:15m:29s remains)
INFO - root - 2017-12-07 06:21:07.498817: step 5620, loss = 2.06, batch loss = 1.97 (11.3 examples/sec; 0.710 sec/batch; 64h:26m:27s remains)
INFO - root - 2017-12-07 06:21:14.621217: step 5630, loss = 2.06, batch loss = 1.97 (10.4 examples/sec; 0.767 sec/batch; 69h:37m:32s remains)
INFO - root - 2017-12-07 06:21:21.602839: step 5640, loss = 1.99, batch loss = 1.91 (11.1 examples/sec; 0.721 sec/batch; 65h:26m:57s remains)
INFO - root - 2017-12-07 06:21:28.699345: step 5650, loss = 2.00, batch loss = 1.91 (11.8 examples/sec; 0.676 sec/batch; 61h:21m:45s remains)
INFO - root - 2017-12-07 06:21:35.562831: step 5660, loss = 2.03, batch loss = 1.95 (10.9 examples/sec; 0.733 sec/batch; 66h:30m:34s remains)
INFO - root - 2017-12-07 06:21:42.596184: step 5670, loss = 2.04, batch loss = 1.95 (10.9 examples/sec; 0.737 sec/batch; 66h:52m:27s remains)
INFO - root - 2017-12-07 06:21:49.727489: step 5680, loss = 2.00, batch loss = 1.91 (10.2 examples/sec; 0.786 sec/batch; 71h:22m:37s remains)
INFO - root - 2017-12-07 06:21:56.742741: step 5690, loss = 2.03, batch loss = 1.94 (11.6 examples/sec; 0.689 sec/batch; 62h:31m:33s remains)
INFO - root - 2017-12-07 06:22:03.725935: step 5700, loss = 2.00, batch loss = 1.92 (11.8 examples/sec; 0.680 sec/batch; 61h:44m:41s remains)
2017-12-07 06:22:04.496057: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.256391 -4.2134781 -4.1485968 -4.0878143 -4.0652056 -4.0780492 -4.1007566 -4.1281466 -4.1454215 -4.1356049 -4.1155896 -4.1056247 -4.1030426 -4.0928197 -4.0899563][-4.2673759 -4.231976 -4.1790142 -4.1286168 -4.1059709 -4.1079578 -4.1126413 -4.1258469 -4.1432457 -4.1479273 -4.1393476 -4.1305418 -4.1243544 -4.1099887 -4.0971174][-4.2721343 -4.2507167 -4.2248883 -4.2046652 -4.1992259 -4.1985946 -4.1839004 -4.1693807 -4.1670022 -4.1711674 -4.1691003 -4.1645808 -4.158751 -4.1423931 -4.1209521][-4.277997 -4.2738194 -4.2753735 -4.2828259 -4.2931662 -4.2928867 -4.2662249 -4.2332497 -4.2139125 -4.2127843 -4.2151594 -4.2166142 -4.2130256 -4.199152 -4.1801372][-4.282443 -4.2953124 -4.3200197 -4.3454466 -4.3611217 -4.3549128 -4.3183227 -4.2786312 -4.2545934 -4.2520556 -4.2615776 -4.2751803 -4.2816005 -4.2770634 -4.2656322][-4.2826486 -4.30305 -4.3384409 -4.3695455 -4.38251 -4.3667555 -4.3204088 -4.274435 -4.2461977 -4.2401171 -4.2532673 -4.2774653 -4.299552 -4.3144569 -4.319][-4.2822838 -4.2949376 -4.3221445 -4.3417187 -4.3399124 -4.3110642 -4.2565217 -4.20656 -4.1768541 -4.1703835 -4.1889467 -4.224082 -4.2627225 -4.2991767 -4.32179][-4.2899275 -4.2859974 -4.2919292 -4.28763 -4.2619138 -4.2179985 -4.1595321 -4.1126218 -4.0890985 -4.0888271 -4.115386 -4.1610703 -4.2152181 -4.2690549 -4.30463][-4.2996192 -4.2799044 -4.2664323 -4.24339 -4.2011089 -4.1475811 -4.0888853 -4.0470276 -4.03249 -4.0397325 -4.071003 -4.1219349 -4.1850128 -4.2481561 -4.2901144][-4.2993 -4.2733879 -4.25325 -4.22822 -4.1884604 -4.1406846 -4.0898328 -4.0537162 -4.0422592 -4.0480871 -4.0747347 -4.1240034 -4.1866574 -4.2488718 -4.2901783][-4.3009229 -4.27676 -4.261167 -4.2447791 -4.21879 -4.1876254 -4.1542206 -4.129724 -4.1197214 -4.1177526 -4.1329842 -4.1721015 -4.2232623 -4.2729912 -4.3054338][-4.3008084 -4.2845335 -4.2811532 -4.2779484 -4.2690554 -4.2582049 -4.2440209 -4.2297 -4.218667 -4.2090311 -4.2130761 -4.2373123 -4.2713351 -4.304822 -4.3245673][-4.29171 -4.2859445 -4.2930603 -4.3003511 -4.3047957 -4.310389 -4.30985 -4.3006425 -4.2871237 -4.2715044 -4.2663822 -4.27823 -4.3001571 -4.3227582 -4.3327456][-4.293447 -4.2901387 -4.2959023 -4.3032241 -4.3126678 -4.3274808 -4.3347788 -4.3296509 -4.3172059 -4.2991848 -4.2874 -4.2907085 -4.3058548 -4.3220119 -4.32585][-4.3111677 -4.3037362 -4.30129 -4.30396 -4.31459 -4.3323665 -4.3395905 -4.3336415 -4.320353 -4.30205 -4.2893796 -4.2912765 -4.3047171 -4.3168163 -4.31604]]...]
INFO - root - 2017-12-07 06:22:11.546689: step 5710, loss = 1.99, batch loss = 1.91 (10.4 examples/sec; 0.769 sec/batch; 69h:48m:17s remains)
INFO - root - 2017-12-07 06:22:18.532458: step 5720, loss = 2.01, batch loss = 1.93 (11.5 examples/sec; 0.695 sec/batch; 63h:07m:42s remains)
INFO - root - 2017-12-07 06:22:25.602724: step 5730, loss = 2.00, batch loss = 1.92 (11.1 examples/sec; 0.719 sec/batch; 65h:18m:23s remains)
INFO - root - 2017-12-07 06:22:32.649245: step 5740, loss = 2.02, batch loss = 1.94 (11.1 examples/sec; 0.722 sec/batch; 65h:33m:11s remains)
INFO - root - 2017-12-07 06:22:39.699493: step 5750, loss = 2.04, batch loss = 1.96 (11.0 examples/sec; 0.730 sec/batch; 66h:14m:28s remains)
INFO - root - 2017-12-07 06:22:46.692092: step 5760, loss = 2.05, batch loss = 1.97 (10.8 examples/sec; 0.742 sec/batch; 67h:18m:32s remains)
INFO - root - 2017-12-07 06:22:53.689777: step 5770, loss = 2.02, batch loss = 1.93 (11.3 examples/sec; 0.707 sec/batch; 64h:08m:07s remains)
INFO - root - 2017-12-07 06:23:00.738811: step 5780, loss = 2.04, batch loss = 1.95 (11.6 examples/sec; 0.693 sec/batch; 62h:51m:31s remains)
INFO - root - 2017-12-07 06:23:07.748044: step 5790, loss = 2.01, batch loss = 1.93 (11.8 examples/sec; 0.678 sec/batch; 61h:31m:21s remains)
INFO - root - 2017-12-07 06:23:14.820096: step 5800, loss = 2.00, batch loss = 1.92 (12.3 examples/sec; 0.649 sec/batch; 58h:55m:24s remains)
2017-12-07 06:23:15.543254: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4439654 -4.3096004 -4.1842532 -4.15964 -4.2595677 -4.4134669 -4.5513768 -4.62675 -4.6254992 -4.5719509 -4.4823222 -4.4179182 -4.4133606 -4.4317236 -4.438448][-4.42207 -4.3337731 -4.2473388 -4.2347178 -4.3292093 -4.4744282 -4.6010313 -4.6756835 -4.6831732 -4.6432958 -4.5629816 -4.4799495 -4.4374022 -4.4278951 -4.4249625][-4.4437094 -4.4134483 -4.385952 -4.3901267 -4.4549069 -4.5537958 -4.6415644 -4.7010679 -4.7195187 -4.7115479 -4.6682873 -4.5880694 -4.5149765 -4.474659 -4.4557385][-4.4875932 -4.4927449 -4.5083623 -4.5073342 -4.5038342 -4.520236 -4.5549817 -4.5998397 -4.6372976 -4.6759963 -4.6919537 -4.6452966 -4.567071 -4.5052481 -4.462471][-4.4830222 -4.50222 -4.537734 -4.5119419 -4.4151607 -4.3247123 -4.2972536 -4.322783 -4.3684492 -4.4443393 -4.5278392 -4.5494909 -4.5042024 -4.4389791 -4.372293][-4.4399033 -4.4484315 -4.4675961 -4.4028959 -4.2180071 -4.0256906 -3.9443343 -3.9664738 -4.030014 -4.137475 -4.2871518 -4.39805 -4.4135218 -4.3567948 -4.2695465][-4.4134512 -4.3928657 -4.3606529 -4.2399035 -3.9849985 -3.7038898 -3.5663853 -3.6035242 -3.7221637 -3.8854632 -4.1082125 -4.3146539 -4.4069939 -4.3690414 -4.2677879][-4.4047794 -4.3702259 -4.3053617 -4.1627378 -3.8984411 -3.5821183 -3.394681 -3.422132 -3.581212 -3.7930598 -4.0564117 -4.3117518 -4.4530253 -4.4281135 -4.330442][-4.4838004 -4.450335 -4.3818817 -4.2545624 -4.0341878 -3.7571318 -3.5748594 -3.5858936 -3.7497721 -3.97366 -4.2141156 -4.429635 -4.5409412 -4.4908795 -4.3967419][-4.6270809 -4.5970788 -4.52687 -4.4019608 -4.2068191 -3.9818704 -3.8477793 -3.8675885 -4.03604 -4.2669034 -4.4735441 -4.6204028 -4.6637359 -4.5701947 -4.4716711][-4.7760248 -4.759387 -4.6993918 -4.5920954 -4.4297218 -4.2569423 -4.1628089 -4.1748147 -4.3108068 -4.5088692 -4.6695676 -4.7592854 -4.750658 -4.635725 -4.5428925][-4.8643508 -4.8676271 -4.8345461 -4.7722917 -4.672904 -4.563271 -4.4979787 -4.4914141 -4.5706849 -4.6986465 -4.794313 -4.8289633 -4.78843 -4.6804061 -4.6029878][-4.7879887 -4.81821 -4.8200569 -4.8069496 -4.7724881 -4.7263355 -4.6967654 -4.6933918 -4.7351284 -4.7992334 -4.837316 -4.831233 -4.7784266 -4.6918993 -4.6269321][-4.5932341 -4.6552224 -4.7009687 -4.7359648 -4.7574606 -4.7642317 -4.7681432 -4.7734909 -4.7840471 -4.7917323 -4.7841411 -4.7546387 -4.703537 -4.6388941 -4.5825176][-4.4042578 -4.4677753 -4.527894 -4.5818772 -4.6291175 -4.665966 -4.6934991 -4.7093754 -4.7083259 -4.6900492 -4.6615839 -4.6252642 -4.5804753 -4.5287933 -4.4792318]]...]
INFO - root - 2017-12-07 06:23:22.620689: step 5810, loss = 2.01, batch loss = 1.93 (11.4 examples/sec; 0.702 sec/batch; 63h:42m:16s remains)
INFO - root - 2017-12-07 06:23:29.654194: step 5820, loss = 2.05, batch loss = 1.97 (12.3 examples/sec; 0.651 sec/batch; 59h:04m:41s remains)
INFO - root - 2017-12-07 06:23:36.670018: step 5830, loss = 1.98, batch loss = 1.90 (12.2 examples/sec; 0.656 sec/batch; 59h:31m:08s remains)
INFO - root - 2017-12-07 06:23:43.657425: step 5840, loss = 2.06, batch loss = 1.98 (11.6 examples/sec; 0.689 sec/batch; 62h:30m:13s remains)
INFO - root - 2017-12-07 06:23:50.661331: step 5850, loss = 2.01, batch loss = 1.93 (11.9 examples/sec; 0.673 sec/batch; 61h:02m:08s remains)
INFO - root - 2017-12-07 06:23:57.755098: step 5860, loss = 2.08, batch loss = 1.99 (11.0 examples/sec; 0.729 sec/batch; 66h:08m:55s remains)
INFO - root - 2017-12-07 06:24:04.775004: step 5870, loss = 2.02, batch loss = 1.93 (10.8 examples/sec; 0.740 sec/batch; 67h:10m:49s remains)
INFO - root - 2017-12-07 06:24:11.784542: step 5880, loss = 2.05, batch loss = 1.97 (11.4 examples/sec; 0.704 sec/batch; 63h:52m:56s remains)
INFO - root - 2017-12-07 06:24:18.771013: step 5890, loss = 1.99, batch loss = 1.91 (11.6 examples/sec; 0.691 sec/batch; 62h:40m:12s remains)
INFO - root - 2017-12-07 06:24:25.893308: step 5900, loss = 2.09, batch loss = 2.01 (10.9 examples/sec; 0.735 sec/batch; 66h:42m:28s remains)
2017-12-07 06:24:26.634077: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6289515 -4.63642 -4.6349 -4.5986881 -4.4813666 -4.3265781 -4.1343646 -3.9386303 -3.8197126 -3.774044 -3.8247087 -4.012496 -4.242218 -4.4576325 -4.5998173][-4.5540724 -4.5911641 -4.6188321 -4.5936356 -4.48017 -4.3229537 -4.1290789 -3.9283919 -3.8052871 -3.7470083 -3.7589607 -3.8965321 -4.1108675 -4.3548 -4.5344868][-4.449728 -4.5304208 -4.5836291 -4.55907 -4.4502749 -4.307003 -4.15083 -3.9910469 -3.8918147 -3.8320184 -3.8070476 -3.8912494 -4.0777593 -4.3192873 -4.507596][-4.3601284 -4.4661007 -4.5232267 -4.4826126 -4.3762717 -4.2550683 -4.1556783 -4.0663214 -4.0173078 -3.9736283 -3.9301987 -3.9805474 -4.1333652 -4.3398919 -4.5085058][-4.3005342 -4.4175525 -4.4708967 -4.4164729 -4.3070173 -4.1967554 -4.1328473 -4.1010747 -4.098772 -4.0803013 -4.0503759 -4.0999436 -4.2279963 -4.3856063 -4.5180469][-4.280354 -4.4042978 -4.4476104 -4.3767748 -4.2498255 -4.1273308 -4.0658083 -4.0582471 -4.0760975 -4.0801 -4.087707 -4.1624112 -4.2817969 -4.4043493 -4.5089884][-4.3076906 -4.428309 -4.4597907 -4.3794565 -4.2298164 -4.0730276 -3.9844568 -3.9702687 -3.9910157 -4.0233488 -4.0823712 -4.1897206 -4.3065004 -4.4095764 -4.4941521][-4.3565865 -4.4780388 -4.5114331 -4.4379139 -4.2742405 -4.079761 -3.954248 -3.9192169 -3.9431591 -4.0163827 -4.1317625 -4.2645936 -4.3715916 -4.45297 -4.5089426][-4.4332619 -4.5541115 -4.592658 -4.5311666 -4.3669405 -4.1537609 -4.0053816 -3.9522266 -3.9782536 -4.0801158 -4.2232513 -4.3548808 -4.4410095 -4.5015373 -4.5318627][-4.5213952 -4.6323819 -4.6699896 -4.6183324 -4.4642038 -4.2612038 -4.1132545 -4.0508065 -4.0717936 -4.1719623 -4.3004446 -4.3999338 -4.4581022 -4.5052748 -4.5266242][-4.5875778 -4.671824 -4.7080011 -4.6619592 -4.5202155 -4.3491955 -4.2269464 -4.1713843 -4.1865869 -4.2622604 -4.3475757 -4.4004045 -4.4297738 -4.4667764 -4.4901342][-4.588346 -4.6541986 -4.704874 -4.6609087 -4.5276423 -4.4038672 -4.3289223 -4.2963033 -4.307271 -4.3455253 -4.3742228 -4.3783259 -4.3807473 -4.4068089 -4.4339318][-4.5240569 -4.5944934 -4.6683264 -4.6254263 -4.5010486 -4.4248462 -4.3959842 -4.386529 -4.3950052 -4.397799 -4.3738828 -4.3369093 -4.3195686 -4.3388944 -4.372189][-4.3996325 -4.4949756 -4.5993805 -4.5691595 -4.4646821 -4.420897 -4.4169493 -4.4158835 -4.4149318 -4.3850589 -4.3186946 -4.2566509 -4.2352896 -4.2608333 -4.30797][-4.2936645 -4.4300871 -4.5595746 -4.5448651 -4.45553 -4.4173179 -4.4131021 -4.404839 -4.3866019 -4.3265152 -4.2323289 -4.1635079 -4.151917 -4.1935244 -4.25761]]...]
INFO - root - 2017-12-07 06:24:33.704978: step 5910, loss = 2.02, batch loss = 1.93 (11.8 examples/sec; 0.680 sec/batch; 61h:40m:54s remains)
INFO - root - 2017-12-07 06:24:40.728133: step 5920, loss = 1.99, batch loss = 1.91 (11.7 examples/sec; 0.682 sec/batch; 61h:51m:25s remains)
INFO - root - 2017-12-07 06:24:47.747522: step 5930, loss = 2.05, batch loss = 1.97 (11.5 examples/sec; 0.695 sec/batch; 63h:00m:45s remains)
INFO - root - 2017-12-07 06:24:54.895125: step 5940, loss = 1.97, batch loss = 1.88 (10.9 examples/sec; 0.733 sec/batch; 66h:30m:23s remains)
INFO - root - 2017-12-07 06:25:01.807891: step 5950, loss = 2.01, batch loss = 1.93 (10.8 examples/sec; 0.741 sec/batch; 67h:15m:00s remains)
INFO - root - 2017-12-07 06:25:08.807775: step 5960, loss = 2.03, batch loss = 1.95 (11.5 examples/sec; 0.699 sec/batch; 63h:21m:51s remains)
INFO - root - 2017-12-07 06:25:15.772695: step 5970, loss = 2.04, batch loss = 1.96 (11.7 examples/sec; 0.685 sec/batch; 62h:08m:51s remains)
INFO - root - 2017-12-07 06:25:22.782572: step 5980, loss = 2.01, batch loss = 1.93 (11.7 examples/sec; 0.683 sec/batch; 61h:54m:15s remains)
INFO - root - 2017-12-07 06:25:29.831458: step 5990, loss = 2.02, batch loss = 1.94 (11.0 examples/sec; 0.730 sec/batch; 66h:13m:13s remains)
INFO - root - 2017-12-07 06:25:36.946563: step 6000, loss = 2.03, batch loss = 1.95 (10.7 examples/sec; 0.748 sec/batch; 67h:48m:59s remains)
2017-12-07 06:25:37.691786: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3323269 -4.3925052 -4.5006332 -4.6005912 -4.6580715 -4.6676836 -4.663054 -4.6826487 -4.7004008 -4.6872211 -4.62747 -4.5174251 -4.418786 -4.3922205 -4.4408774][-4.2415609 -4.2992239 -4.4149141 -4.5413365 -4.6365981 -4.6717377 -4.6681042 -4.6708145 -4.671174 -4.6443896 -4.5710568 -4.4471803 -4.3412056 -4.3241873 -4.4045353][-4.2093863 -4.2604103 -4.3558259 -4.4774323 -4.5892019 -4.6534042 -4.6678109 -4.6651192 -4.6499877 -4.6062646 -4.5232019 -4.3961887 -4.285296 -4.2666235 -4.3579741][-4.236074 -4.2786174 -4.3361864 -4.416925 -4.5014443 -4.5713348 -4.6120114 -4.6309505 -4.6271119 -4.5924659 -4.5248942 -4.4148097 -4.3036222 -4.2652769 -4.3375897][-4.3100929 -4.346477 -4.3651795 -4.3908939 -4.4274249 -4.4789314 -4.524641 -4.552691 -4.5585809 -4.5504684 -4.5338507 -4.4792018 -4.3965158 -4.3495927 -4.392889][-4.3989692 -4.4153061 -4.3917093 -4.3664527 -4.3588037 -4.3812881 -4.4043074 -4.4099374 -4.4060812 -4.4295287 -4.4859977 -4.511436 -4.4869661 -4.4610167 -4.4935422][-4.4543567 -4.449759 -4.4035816 -4.3520231 -4.3149319 -4.2996073 -4.26847 -4.2193565 -4.1832647 -4.2216921 -4.3366752 -4.4332428 -4.4748344 -4.5003476 -4.5556684][-4.454277 -4.454668 -4.4224234 -4.3809094 -4.3370471 -4.2840509 -4.1823306 -4.0566459 -3.9658511 -3.9949026 -4.141057 -4.2896214 -4.3902168 -4.4687696 -4.5554485][-4.4150152 -4.4375811 -4.4380355 -4.4290781 -4.3997369 -4.3223391 -4.1605744 -3.9658303 -3.8249693 -3.8351705 -3.9892778 -4.1650434 -4.3048291 -4.4174366 -4.5217109][-4.3785887 -4.4299717 -4.4623194 -4.4845672 -4.4759073 -4.3868079 -4.1891007 -3.9570131 -3.789798 -3.7830467 -3.9265928 -4.1075525 -4.267252 -4.3919454 -4.4887681][-4.3858628 -4.4575753 -4.5038838 -4.5452042 -4.557117 -4.4742212 -4.2796254 -4.0517311 -3.8853438 -3.8590348 -3.9711676 -4.1363587 -4.3014259 -4.4254375 -4.494091][-4.3497052 -4.4279003 -4.4828978 -4.5438151 -4.5812931 -4.5329766 -4.3873811 -4.2024822 -4.0520682 -4.0016952 -4.0676208 -4.1991067 -4.3526578 -4.4635434 -4.4930272][-4.2262926 -4.303472 -4.3776708 -4.4712877 -4.5415268 -4.54203 -4.4668832 -4.343298 -4.21645 -4.137897 -4.1484976 -4.2354479 -4.363595 -4.4527206 -4.4483118][-4.0678973 -4.1447206 -4.2409687 -4.366961 -4.4606314 -4.4965978 -4.483263 -4.4302769 -4.3468909 -4.2590523 -4.228889 -4.2770219 -4.37696 -4.4428048 -4.4166141][-3.9818993 -4.0474072 -4.1506147 -4.2890081 -4.3862977 -4.4329448 -4.4566913 -4.4666977 -4.4415627 -4.3734212 -4.3323903 -4.3611312 -4.4368134 -4.4770112 -4.4342813]]...]
INFO - root - 2017-12-07 06:25:44.648546: step 6010, loss = 2.03, batch loss = 1.95 (12.0 examples/sec; 0.666 sec/batch; 60h:23m:37s remains)
INFO - root - 2017-12-07 06:25:51.701713: step 6020, loss = 2.04, batch loss = 1.96 (11.1 examples/sec; 0.723 sec/batch; 65h:32m:01s remains)
INFO - root - 2017-12-07 06:25:58.738643: step 6030, loss = 2.01, batch loss = 1.92 (11.4 examples/sec; 0.705 sec/batch; 63h:54m:49s remains)
INFO - root - 2017-12-07 06:26:05.704609: step 6040, loss = 2.01, batch loss = 1.93 (11.5 examples/sec; 0.694 sec/batch; 62h:54m:43s remains)
INFO - root - 2017-12-07 06:26:12.734489: step 6050, loss = 2.01, batch loss = 1.93 (10.8 examples/sec; 0.743 sec/batch; 67h:20m:44s remains)
INFO - root - 2017-12-07 06:26:19.647188: step 6060, loss = 2.02, batch loss = 1.94 (11.4 examples/sec; 0.701 sec/batch; 63h:32m:20s remains)
INFO - root - 2017-12-07 06:26:26.644050: step 6070, loss = 2.02, batch loss = 1.94 (12.0 examples/sec; 0.666 sec/batch; 60h:25m:48s remains)
INFO - root - 2017-12-07 06:26:33.714614: step 6080, loss = 2.05, batch loss = 1.96 (11.8 examples/sec; 0.676 sec/batch; 61h:15m:18s remains)
INFO - root - 2017-12-07 06:26:40.861634: step 6090, loss = 1.99, batch loss = 1.91 (10.7 examples/sec; 0.745 sec/batch; 67h:34m:58s remains)
INFO - root - 2017-12-07 06:26:47.911339: step 6100, loss = 2.02, batch loss = 1.94 (10.5 examples/sec; 0.763 sec/batch; 69h:09m:52s remains)
2017-12-07 06:26:48.736226: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2965875 -4.4131947 -4.5619106 -4.675005 -4.7266417 -4.7099543 -4.6373978 -4.5588212 -4.524981 -4.56891 -4.6582265 -4.7192254 -4.7142057 -4.6790318 -4.6643476][-4.2976341 -4.4181871 -4.5698357 -4.6823983 -4.7325363 -4.7091122 -4.6191688 -4.512146 -4.4501557 -4.4939895 -4.6266727 -4.7331748 -4.7401657 -4.6898093 -4.6659012][-4.297143 -4.3979583 -4.5216408 -4.6119423 -4.6485682 -4.6134944 -4.5163126 -4.4097071 -4.3421392 -4.3755441 -4.5202212 -4.6622071 -4.697154 -4.6401296 -4.6007595][-4.307734 -4.3797069 -4.4546123 -4.505899 -4.5156374 -4.4590635 -4.3531809 -4.2657156 -4.2254791 -4.2620683 -4.3987155 -4.5585928 -4.626894 -4.5715523 -4.5091996][-4.3330479 -4.3799672 -4.404355 -4.4079151 -4.3817673 -4.3027978 -4.1928172 -4.1323905 -4.14 -4.2053814 -4.3416715 -4.5095396 -4.5962038 -4.5360336 -4.446104][-4.3494015 -4.375689 -4.356606 -4.3119769 -4.2487803 -4.1610093 -4.0654435 -4.0338483 -4.0872107 -4.1986856 -4.3532753 -4.5191054 -4.5956655 -4.5169458 -4.3987694][-4.3599167 -4.3734488 -4.3291082 -4.250577 -4.15266 -4.0476346 -3.9516406 -3.9226885 -3.9975276 -4.1641889 -4.3732514 -4.5555234 -4.6201062 -4.5297971 -4.3984442][-4.37945 -4.39658 -4.3525667 -4.2635703 -4.1474762 -4.0154586 -3.8906336 -3.8276882 -3.8846903 -4.0804005 -4.3441973 -4.5547962 -4.6242008 -4.5533943 -4.440937][-4.4177532 -4.4533257 -4.4308186 -4.3542724 -4.2375636 -4.0877604 -3.927748 -3.8188066 -3.8396134 -4.0329185 -4.3195753 -4.5389895 -4.6127434 -4.5697064 -4.4887333][-4.4709816 -4.5381246 -4.5509338 -4.5004144 -4.3971133 -4.2415633 -4.0478892 -3.8932953 -3.8804474 -4.0613227 -4.340694 -4.5425272 -4.6061716 -4.5814114 -4.531126][-4.5257797 -4.63013 -4.6784267 -4.6490922 -4.5551057 -4.3970766 -4.1818151 -4.002862 -3.9758892 -4.1429081 -4.3938856 -4.5603013 -4.6087213 -4.5934134 -4.5646887][-4.571146 -4.7092772 -4.7897367 -4.7797976 -4.6946845 -4.5413847 -4.3271623 -4.1528153 -4.1255426 -4.2672524 -4.4657297 -4.5849276 -4.6140018 -4.5988832 -4.5801339][-4.5910316 -4.7549129 -4.8655181 -4.8791838 -4.8101616 -4.6695285 -4.4744358 -4.3216248 -4.2979431 -4.4033833 -4.5393734 -4.6133623 -4.62774 -4.612546 -4.597549][-4.5714688 -4.74138 -4.8689284 -4.9045897 -4.8616586 -4.7507405 -4.5969691 -4.4794507 -4.4612112 -4.5290623 -4.610321 -4.6514988 -4.6584983 -4.6471481 -4.6340413][-4.5111408 -4.6646214 -4.7921677 -4.8455286 -4.8333278 -4.7645473 -4.6631155 -4.5847354 -4.5686479 -4.6007519 -4.6381836 -4.6565113 -4.6586618 -4.6500139 -4.6370306]]...]
INFO - root - 2017-12-07 06:26:55.689666: step 6110, loss = 2.02, batch loss = 1.94 (11.5 examples/sec; 0.695 sec/batch; 63h:01m:03s remains)
INFO - root - 2017-12-07 06:27:02.667974: step 6120, loss = 2.07, batch loss = 1.99 (10.9 examples/sec; 0.736 sec/batch; 66h:41m:28s remains)
INFO - root - 2017-12-07 06:27:09.779609: step 6130, loss = 2.00, batch loss = 1.91 (11.1 examples/sec; 0.718 sec/batch; 65h:07m:20s remains)
INFO - root - 2017-12-07 06:27:16.773661: step 6140, loss = 2.02, batch loss = 1.93 (11.2 examples/sec; 0.712 sec/batch; 64h:31m:13s remains)
INFO - root - 2017-12-07 06:27:23.751098: step 6150, loss = 1.98, batch loss = 1.90 (12.1 examples/sec; 0.660 sec/batch; 59h:50m:03s remains)
INFO - root - 2017-12-07 06:27:30.678898: step 6160, loss = 2.04, batch loss = 1.96 (12.6 examples/sec; 0.633 sec/batch; 57h:23m:49s remains)
INFO - root - 2017-12-07 06:27:37.733285: step 6170, loss = 2.04, batch loss = 1.95 (11.0 examples/sec; 0.728 sec/batch; 66h:01m:34s remains)
INFO - root - 2017-12-07 06:27:44.617826: step 6180, loss = 2.02, batch loss = 1.94 (11.4 examples/sec; 0.702 sec/batch; 63h:39m:29s remains)
INFO - root - 2017-12-07 06:27:51.628079: step 6190, loss = 2.04, batch loss = 1.96 (11.6 examples/sec; 0.692 sec/batch; 62h:42m:15s remains)
INFO - root - 2017-12-07 06:27:58.652850: step 6200, loss = 2.04, batch loss = 1.96 (12.0 examples/sec; 0.667 sec/batch; 60h:28m:54s remains)
2017-12-07 06:27:59.368883: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3912907 -4.4205036 -4.3993316 -4.3469462 -4.3330841 -4.3569465 -4.3187652 -4.1842237 -4.0739255 -4.10221 -4.2415323 -4.3909435 -4.4608083 -4.4643936 -4.4700422][-4.3589044 -4.4434595 -4.469645 -4.4402361 -4.427906 -4.4334564 -4.3690844 -4.2077408 -4.0730381 -4.0965042 -4.2356462 -4.3855391 -4.4779658 -4.5220194 -4.555789][-4.3723512 -4.4880786 -4.5453215 -4.5334525 -4.5136356 -4.4906945 -4.4033575 -4.2356825 -4.098628 -4.1112275 -4.2319641 -4.3703394 -4.4785833 -4.5537267 -4.6004987][-4.414155 -4.5392008 -4.6006756 -4.58624 -4.5394115 -4.4711294 -4.3543005 -4.1928897 -4.074533 -4.089488 -4.1977029 -4.3292112 -4.4530416 -4.5441713 -4.5816545][-4.4800434 -4.5943375 -4.6394706 -4.5994215 -4.5019264 -4.3713517 -4.2253513 -4.0868039 -4.0073061 -4.0394945 -4.1461711 -4.2803173 -4.4247632 -4.5251837 -4.5393348][-4.55459 -4.6415877 -4.6516337 -4.5642209 -4.3979487 -4.1991911 -4.0305591 -3.9326169 -3.914614 -3.9859107 -4.1104608 -4.2604213 -4.423243 -4.5210176 -4.50306][-4.5669303 -4.617404 -4.5933313 -4.4663105 -4.2473869 -4.0033159 -3.8349071 -3.7929821 -3.845077 -3.9601049 -4.1066027 -4.2739725 -4.4424319 -4.5228329 -4.4752278][-4.5179076 -4.5418 -4.5042515 -4.3697062 -4.1395802 -3.8918355 -3.7543035 -3.7657566 -3.8613544 -3.9999292 -4.1596756 -4.331099 -4.4819536 -4.5319252 -4.4664822][-4.4294453 -4.4489093 -4.4260278 -4.3136311 -4.098628 -3.8701746 -3.7715921 -3.8185837 -3.93286 -4.0812693 -4.2480741 -4.4077787 -4.523808 -4.5410271 -4.4701762][-4.3337927 -4.3632307 -4.3712077 -4.2840619 -4.0820317 -3.8781855 -3.8199892 -3.8977592 -4.0309191 -4.1900311 -4.350184 -4.4724045 -4.5377841 -4.5234694 -4.4487019][-4.2629418 -4.3102107 -4.3458495 -4.277864 -4.0929046 -3.9250429 -3.9065089 -4.0095325 -4.1656528 -4.3358297 -4.4670296 -4.5251808 -4.5260978 -4.4778867 -4.3959637][-4.2319479 -4.2912474 -4.3414335 -4.2901912 -4.1317372 -4.0000811 -4.0055423 -4.123476 -4.3022985 -4.4747343 -4.5620322 -4.55493 -4.4966969 -4.4186082 -4.3313136][-4.2293429 -4.280529 -4.3279338 -4.2911205 -4.1648703 -4.069921 -4.0988479 -4.2298665 -4.4120383 -4.5628219 -4.6063461 -4.5564713 -4.4602885 -4.3583665 -4.26756][-4.2483025 -4.29084 -4.3315115 -4.3079824 -4.2109418 -4.1408186 -4.1770296 -4.299602 -4.4561553 -4.5696292 -4.5833859 -4.514955 -4.397058 -4.2766104 -4.1928797][-4.3115869 -4.3540998 -4.3868694 -4.3631883 -4.2803736 -4.2170124 -4.2375069 -4.3256779 -4.4350104 -4.5061884 -4.5028896 -4.429935 -4.305696 -4.1847768 -4.1225114]]...]
INFO - root - 2017-12-07 06:28:06.383342: step 6210, loss = 2.01, batch loss = 1.93 (10.9 examples/sec; 0.731 sec/batch; 66h:13m:45s remains)
INFO - root - 2017-12-07 06:28:13.534300: step 6220, loss = 2.02, batch loss = 1.93 (11.2 examples/sec; 0.717 sec/batch; 64h:58m:44s remains)
INFO - root - 2017-12-07 06:28:20.553610: step 6230, loss = 2.04, batch loss = 1.96 (11.1 examples/sec; 0.719 sec/batch; 65h:09m:24s remains)
INFO - root - 2017-12-07 06:28:27.642569: step 6240, loss = 2.04, batch loss = 1.95 (11.1 examples/sec; 0.718 sec/batch; 65h:04m:04s remains)
INFO - root - 2017-12-07 06:28:34.714392: step 6250, loss = 2.00, batch loss = 1.91 (11.6 examples/sec; 0.688 sec/batch; 62h:18m:42s remains)
INFO - root - 2017-12-07 06:28:41.810064: step 6260, loss = 2.08, batch loss = 1.99 (11.2 examples/sec; 0.713 sec/batch; 64h:35m:59s remains)
INFO - root - 2017-12-07 06:28:48.902726: step 6270, loss = 2.00, batch loss = 1.92 (11.7 examples/sec; 0.687 sec/batch; 62h:13m:24s remains)
INFO - root - 2017-12-07 06:28:55.783909: step 6280, loss = 2.03, batch loss = 1.95 (11.3 examples/sec; 0.706 sec/batch; 63h:58m:43s remains)
INFO - root - 2017-12-07 06:29:02.850613: step 6290, loss = 2.04, batch loss = 1.96 (11.2 examples/sec; 0.712 sec/batch; 64h:32m:20s remains)
INFO - root - 2017-12-07 06:29:09.933015: step 6300, loss = 2.04, batch loss = 1.95 (11.1 examples/sec; 0.723 sec/batch; 65h:29m:01s remains)
2017-12-07 06:29:10.659073: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3591318 -4.37181 -4.391818 -4.40261 -4.3987279 -4.3991332 -4.4163327 -4.44057 -4.4333019 -4.3955221 -4.4034071 -4.4608755 -4.4904909 -4.4709554 -4.4294405][-4.3676152 -4.3891892 -4.413588 -4.4157524 -4.38541 -4.3416615 -4.312386 -4.3160162 -4.3285933 -4.3313293 -4.3761129 -4.4608288 -4.5143857 -4.5102849 -4.47439][-4.3773384 -4.4084787 -4.4351807 -4.4224796 -4.3606634 -4.2774348 -4.2142138 -4.2056484 -4.2359238 -4.2681284 -4.3353415 -4.4406476 -4.5254207 -4.5513678 -4.5372553][-4.387876 -4.4258914 -4.4526229 -4.4195213 -4.3211842 -4.2034431 -4.1190186 -4.1066055 -4.1497421 -4.1965122 -4.2689543 -4.3828006 -4.493784 -4.5522084 -4.566443][-4.3972812 -4.4414949 -4.4697971 -4.4195728 -4.2886705 -4.1364241 -4.029985 -4.0084887 -4.0529885 -4.1060591 -4.1815567 -4.2996459 -4.4242835 -4.5014653 -4.5330782][-4.4042096 -4.4577684 -4.4934039 -4.4358463 -4.2804308 -4.0864639 -3.9368987 -3.8827584 -3.9149733 -3.9818063 -4.085638 -4.2270246 -4.35899 -4.4304585 -4.4491386][-4.4093904 -4.4718175 -4.5152912 -4.4555387 -4.281754 -4.0425968 -3.8368177 -3.7395329 -3.7598586 -3.8560185 -4.0110965 -4.1938734 -4.3287573 -4.3695769 -4.3413911][-4.4117165 -4.4793911 -4.5280204 -4.4724493 -4.2959824 -4.0329614 -3.787519 -3.6585522 -3.6733854 -3.7946806 -3.9906428 -4.1992726 -4.3202958 -4.3150125 -4.2299366][-4.4126897 -4.4813128 -4.53976 -4.5070529 -4.3556089 -4.1045127 -3.8546367 -3.7164736 -3.7225506 -3.8420606 -4.0382609 -4.2328863 -4.3163533 -4.2634993 -4.1407351][-4.4148927 -4.48739 -4.5656343 -4.573657 -4.4674582 -4.249372 -4.01622 -3.8817205 -3.8762505 -3.9777515 -4.1515303 -4.3118033 -4.3460011 -4.2512712 -4.1128669][-4.421608 -4.4975095 -4.5932407 -4.6381497 -4.5767059 -4.3973017 -4.1914816 -4.0738249 -4.0664511 -4.157506 -4.3135295 -4.4393868 -4.4296522 -4.3060541 -4.1683073][-4.4276943 -4.499918 -4.5962048 -4.6587262 -4.6356893 -4.505959 -4.3431854 -4.2522259 -4.250608 -4.3349519 -4.4777627 -4.5748262 -4.5357418 -4.4062557 -4.28517][-4.4301538 -4.4939551 -4.5720773 -4.6304784 -4.635149 -4.5626378 -4.4520149 -4.3906693 -4.3979239 -4.47514 -4.6039548 -4.67929 -4.62841 -4.5089369 -4.4080858][-4.4259048 -4.4793935 -4.5325642 -4.5732379 -4.5931678 -4.5737338 -4.5201774 -4.4907765 -4.5055842 -4.5705462 -4.6746831 -4.7285767 -4.6815386 -4.5816879 -4.4964213][-4.4107008 -4.4585867 -4.4957061 -4.5189195 -4.5376782 -4.54463 -4.5290508 -4.5201864 -4.5351014 -4.5830274 -4.6564493 -4.6909361 -4.6572862 -4.5859575 -4.5202928]]...]
INFO - root - 2017-12-07 06:29:17.759385: step 6310, loss = 2.03, batch loss = 1.95 (10.7 examples/sec; 0.745 sec/batch; 67h:29m:32s remains)
INFO - root - 2017-12-07 06:29:24.902538: step 6320, loss = 2.00, batch loss = 1.92 (11.2 examples/sec; 0.713 sec/batch; 64h:34m:39s remains)
INFO - root - 2017-12-07 06:29:31.883939: step 6330, loss = 1.96, batch loss = 1.88 (11.6 examples/sec; 0.690 sec/batch; 62h:29m:56s remains)
INFO - root - 2017-12-07 06:29:39.020803: step 6340, loss = 2.05, batch loss = 1.97 (11.2 examples/sec; 0.716 sec/batch; 64h:52m:53s remains)
INFO - root - 2017-12-07 06:29:46.024716: step 6350, loss = 2.03, batch loss = 1.95 (11.3 examples/sec; 0.707 sec/batch; 64h:02m:30s remains)
INFO - root - 2017-12-07 06:29:53.166629: step 6360, loss = 2.04, batch loss = 1.96 (11.1 examples/sec; 0.718 sec/batch; 65h:03m:05s remains)
INFO - root - 2017-12-07 06:30:00.207962: step 6370, loss = 2.00, batch loss = 1.92 (12.1 examples/sec; 0.660 sec/batch; 59h:47m:27s remains)
INFO - root - 2017-12-07 06:30:07.200364: step 6380, loss = 2.02, batch loss = 1.94 (11.4 examples/sec; 0.701 sec/batch; 63h:32m:22s remains)
INFO - root - 2017-12-07 06:30:14.293482: step 6390, loss = 2.00, batch loss = 1.92 (11.3 examples/sec; 0.705 sec/batch; 63h:53m:54s remains)
INFO - root - 2017-12-07 06:30:21.432260: step 6400, loss = 1.98, batch loss = 1.90 (11.6 examples/sec; 0.688 sec/batch; 62h:17m:24s remains)
2017-12-07 06:30:22.166297: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3951812 -4.4678788 -4.5709748 -4.6882949 -4.7408428 -4.6871924 -4.5815306 -4.50896 -4.498652 -4.5133476 -4.4848566 -4.4219494 -4.3775949 -4.3572788 -4.3925014][-4.443047 -4.5402145 -4.6636233 -4.7809525 -4.7966475 -4.6733418 -4.4949789 -4.3825216 -4.3713183 -4.4171519 -4.4311004 -4.4019537 -4.3785148 -4.3658543 -4.3942175][-4.5035577 -4.60947 -4.7272696 -4.8112097 -4.765274 -4.5659971 -4.3260283 -4.1943793 -4.2022886 -4.2830114 -4.3286705 -4.3279586 -4.3251357 -4.3192768 -4.3360481][-4.5564756 -4.6431394 -4.7251425 -4.7462072 -4.6254158 -4.3667731 -4.0995731 -3.98301 -4.0313168 -4.1472216 -4.2125449 -4.2279778 -4.2326832 -4.2225966 -4.2262087][-4.5561762 -4.6076145 -4.6423917 -4.6126161 -4.451076 -4.1759386 -3.9190705 -3.8360996 -3.924809 -4.0660148 -4.1380754 -4.1533747 -4.1522474 -4.1409855 -4.1461797][-4.4861684 -4.5028358 -4.5060959 -4.4648271 -4.3134828 -4.060379 -3.827745 -3.7702832 -3.8791623 -4.034359 -4.1107883 -4.1257463 -4.1228881 -4.1293039 -4.1554689][-4.380249 -4.3737555 -4.3748927 -4.3574052 -4.2424369 -4.0166736 -3.7993295 -3.7445505 -3.8467383 -3.9986155 -4.0851679 -4.1179771 -4.1381159 -4.1864524 -4.2555537][-4.2875953 -4.2717991 -4.2937551 -4.316359 -4.2449503 -4.051332 -3.850553 -3.7819433 -3.8504376 -3.9728801 -4.06586 -4.1332331 -4.1977429 -4.3009324 -4.4168987][-4.2282181 -4.2105803 -4.2536473 -4.3155022 -4.2985368 -4.1609206 -3.9920201 -3.9092605 -3.9371021 -4.0213404 -4.1130157 -4.2120452 -4.3167067 -4.4477549 -4.5715647][-4.2150393 -4.1996641 -4.2513065 -4.3383875 -4.3791275 -4.3147159 -4.1895332 -4.0973358 -4.0911641 -4.1447067 -4.23251 -4.3459687 -4.4604373 -4.5736928 -4.6529756][-4.2830262 -4.26811 -4.3068953 -4.3880134 -4.4632196 -4.4623618 -4.3817706 -4.2899361 -4.260778 -4.2908206 -4.367681 -4.4726119 -4.5643158 -4.6230364 -4.6296186][-4.3915467 -4.3776994 -4.3904805 -4.4439607 -4.5233502 -4.5648284 -4.5271349 -4.4489756 -4.4079885 -4.4152918 -4.465816 -4.5390663 -4.5879784 -4.5790758 -4.5192547][-4.4754195 -4.4626513 -4.4550705 -4.484705 -4.5574951 -4.6243587 -4.6254711 -4.571866 -4.5273485 -4.5084586 -4.5190969 -4.5491252 -4.5576148 -4.5055017 -4.4174323][-4.5207739 -4.5013356 -4.48463 -4.5047178 -4.5712628 -4.6491022 -4.6771274 -4.6471195 -4.6029286 -4.5610647 -4.5355153 -4.5316315 -4.5179729 -4.45504 -4.3701129][-4.536952 -4.5113373 -4.4978004 -4.522512 -4.5846052 -4.6551018 -4.6855583 -4.6630421 -4.6180596 -4.5637732 -4.5184503 -4.4958277 -4.4711971 -4.41701 -4.355598]]...]
INFO - root - 2017-12-07 06:30:29.118229: step 6410, loss = 1.99, batch loss = 1.91 (12.5 examples/sec; 0.640 sec/batch; 57h:55m:40s remains)
INFO - root - 2017-12-07 06:30:36.081301: step 6420, loss = 2.05, batch loss = 1.97 (11.8 examples/sec; 0.679 sec/batch; 61h:29m:58s remains)
INFO - root - 2017-12-07 06:30:43.181880: step 6430, loss = 2.02, batch loss = 1.94 (11.2 examples/sec; 0.713 sec/batch; 64h:32m:58s remains)
INFO - root - 2017-12-07 06:30:50.151608: step 6440, loss = 2.01, batch loss = 1.93 (12.0 examples/sec; 0.669 sec/batch; 60h:33m:51s remains)
INFO - root - 2017-12-07 06:30:57.164444: step 6450, loss = 1.99, batch loss = 1.91 (10.6 examples/sec; 0.757 sec/batch; 68h:32m:05s remains)
INFO - root - 2017-12-07 06:31:04.289966: step 6460, loss = 2.02, batch loss = 1.94 (10.6 examples/sec; 0.755 sec/batch; 68h:20m:10s remains)
INFO - root - 2017-12-07 06:31:11.445828: step 6470, loss = 2.03, batch loss = 1.95 (10.8 examples/sec; 0.738 sec/batch; 66h:49m:24s remains)
INFO - root - 2017-12-07 06:31:18.482929: step 6480, loss = 1.97, batch loss = 1.89 (11.7 examples/sec; 0.681 sec/batch; 61h:42m:19s remains)
INFO - root - 2017-12-07 06:31:25.580546: step 6490, loss = 2.01, batch loss = 1.93 (11.5 examples/sec; 0.695 sec/batch; 62h:55m:16s remains)
INFO - root - 2017-12-07 06:31:32.607883: step 6500, loss = 2.04, batch loss = 1.96 (11.1 examples/sec; 0.723 sec/batch; 65h:27m:01s remains)
2017-12-07 06:31:33.366516: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.569828 -4.6951613 -4.7474623 -4.6994672 -4.5951509 -4.4755287 -4.3718519 -4.3272476 -4.3854432 -4.5026684 -4.5898652 -4.6265392 -4.6466475 -4.6608629 -4.6769018][-4.6143241 -4.7542086 -4.7893643 -4.7028575 -4.5614724 -4.41413 -4.2980328 -4.253376 -4.3248076 -4.4743991 -4.5936818 -4.6434832 -4.6491585 -4.6293969 -4.60933][-4.6418529 -4.7857485 -4.8014383 -4.6861134 -4.5206842 -4.3509636 -4.224277 -4.1895733 -4.2864637 -4.4686761 -4.6175113 -4.6759644 -4.6609745 -4.5931411 -4.518189][-4.6517377 -4.7884045 -4.7850766 -4.6520276 -4.4744086 -4.2892952 -4.1507158 -4.1307144 -4.262423 -4.4743557 -4.6402364 -4.7000203 -4.6621389 -4.5469656 -4.4190083][-4.6484551 -4.7613988 -4.7316151 -4.577878 -4.38115 -4.1703372 -4.0066218 -3.9886117 -4.1497393 -4.3853869 -4.5638828 -4.6315122 -4.5895615 -4.4532704 -4.2931404][-4.6399755 -4.7275295 -4.6804981 -4.5217867 -4.317852 -4.0886722 -3.9001369 -3.8664551 -4.0262513 -4.259429 -4.4398265 -4.5182061 -4.4884186 -4.3572326 -4.1904831][-4.6201224 -4.6816378 -4.6277866 -4.4866886 -4.303853 -4.086206 -3.8874211 -3.822149 -3.9443846 -4.1426854 -4.3052616 -4.3874006 -4.378509 -4.2804947 -4.1438909][-4.5832095 -4.614007 -4.5527978 -4.4259353 -4.2607784 -4.0603275 -3.8657813 -3.7799997 -3.8652973 -4.032444 -4.1777406 -4.2641544 -4.2858362 -4.241879 -4.1564355][-4.5374618 -4.54669 -4.4939966 -4.394309 -4.2556195 -4.0804644 -3.909863 -3.8345561 -3.9081614 -4.0565195 -4.178092 -4.2480383 -4.2796974 -4.274673 -4.23504][-4.4860525 -4.4835372 -4.452939 -4.394455 -4.2939692 -4.1504436 -4.0106611 -3.9596977 -4.0357323 -4.16924 -4.2626066 -4.308466 -4.3391747 -4.3571796 -4.3471208][-4.4407568 -4.4352431 -4.4238486 -4.3993373 -4.3288574 -4.2071 -4.0964584 -4.0770845 -4.1658974 -4.2878876 -4.3595996 -4.3961687 -4.4363503 -4.4698744 -4.4671893][-4.4170403 -4.4223862 -4.4347782 -4.445972 -4.4112577 -4.31852 -4.2374744 -4.2452097 -4.3446994 -4.4544578 -4.5023212 -4.5179958 -4.5450525 -4.5696511 -4.5585842][-4.4022136 -4.4202676 -4.45085 -4.48511 -4.4813 -4.4228411 -4.3693266 -4.387012 -4.4767084 -4.5654626 -4.5929365 -4.5884194 -4.5921206 -4.60002 -4.5856481][-4.3851104 -4.4071922 -4.4405341 -4.4749827 -4.4837418 -4.4531407 -4.4222155 -4.4364519 -4.5000873 -4.5649776 -4.584208 -4.5751672 -4.5683408 -4.5677085 -4.5568938][-4.3726683 -4.3948183 -4.4263406 -4.455678 -4.4679232 -4.4561229 -4.4422374 -4.451097 -4.4878392 -4.5265617 -4.5350337 -4.5213928 -4.5075488 -4.501164 -4.4948487]]...]
INFO - root - 2017-12-07 06:31:40.396578: step 6510, loss = 1.98, batch loss = 1.89 (11.3 examples/sec; 0.706 sec/batch; 63h:57m:16s remains)
INFO - root - 2017-12-07 06:31:47.413931: step 6520, loss = 2.03, batch loss = 1.94 (11.7 examples/sec; 0.684 sec/batch; 61h:57m:42s remains)
INFO - root - 2017-12-07 06:31:54.498079: step 6530, loss = 2.00, batch loss = 1.92 (11.2 examples/sec; 0.713 sec/batch; 64h:32m:52s remains)
INFO - root - 2017-12-07 06:32:01.423872: step 6540, loss = 1.99, batch loss = 1.91 (11.0 examples/sec; 0.726 sec/batch; 65h:41m:45s remains)
INFO - root - 2017-12-07 06:32:08.498870: step 6550, loss = 2.08, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 64h:53m:08s remains)
INFO - root - 2017-12-07 06:32:15.489345: step 6560, loss = 2.04, batch loss = 1.96 (11.9 examples/sec; 0.673 sec/batch; 60h:56m:56s remains)
INFO - root - 2017-12-07 06:32:22.487024: step 6570, loss = 2.03, batch loss = 1.95 (12.0 examples/sec; 0.669 sec/batch; 60h:33m:31s remains)
INFO - root - 2017-12-07 06:32:29.633289: step 6580, loss = 2.03, batch loss = 1.95 (10.8 examples/sec; 0.737 sec/batch; 66h:45m:47s remains)
INFO - root - 2017-12-07 06:32:36.796842: step 6590, loss = 2.02, batch loss = 1.93 (10.9 examples/sec; 0.736 sec/batch; 66h:36m:29s remains)
INFO - root - 2017-12-07 06:32:43.854872: step 6600, loss = 2.04, batch loss = 1.95 (11.5 examples/sec; 0.697 sec/batch; 63h:07m:21s remains)
2017-12-07 06:32:44.591903: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4555283 -4.5168328 -4.52577 -4.4962845 -4.4397297 -4.3689337 -4.3105726 -4.2750878 -4.2721448 -4.2783027 -4.3057284 -4.3389735 -4.354784 -4.3434882 -4.2785459][-4.4663758 -4.5115333 -4.490212 -4.4346967 -4.3676915 -4.3036184 -4.2622952 -4.246944 -4.2482629 -4.2473407 -4.2616892 -4.29415 -4.3224325 -4.3220353 -4.2705541][-4.472508 -4.500988 -4.4534693 -4.3737283 -4.2995629 -4.24838 -4.2323294 -4.2463331 -4.2627006 -4.2660265 -4.2715464 -4.2848191 -4.3050933 -4.3078403 -4.2756352][-4.4757414 -4.4852519 -4.4074316 -4.3027391 -4.2278104 -4.1984062 -4.2111635 -4.2516527 -4.2859507 -4.3101873 -4.3246264 -4.3244119 -4.3293524 -4.3350096 -4.3243585][-4.471735 -4.4659662 -4.3657503 -4.2417507 -4.1635866 -4.1453619 -4.17149 -4.2233868 -4.2693281 -4.3123488 -4.34172 -4.3361392 -4.3343482 -4.3566704 -4.3849583][-4.4635177 -4.4542832 -4.3496442 -4.213623 -4.11864 -4.0817575 -4.0908656 -4.1362333 -4.1868482 -4.24727 -4.2877278 -4.2692089 -4.2556939 -4.293582 -4.3623443][-4.4666882 -4.46203 -4.3600149 -4.208529 -4.0776324 -3.9936914 -3.9643543 -3.9998839 -4.062397 -4.147963 -4.1975622 -4.1658344 -4.1426425 -4.1814713 -4.266911][-4.48295 -4.4870882 -4.3920045 -4.2297707 -4.0649 -3.9329982 -3.8691738 -3.8998322 -3.9731522 -4.0746541 -4.1274519 -4.0900784 -4.0670929 -4.1013651 -4.181015][-4.5045543 -4.5233321 -4.4491663 -4.3041053 -4.1360321 -3.9817104 -3.9013236 -3.927891 -4.0036907 -4.107687 -4.1541777 -4.1089492 -4.0767336 -4.0893793 -4.1395316][-4.5229659 -4.5630155 -4.5234594 -4.4207387 -4.281271 -4.13056 -4.04274 -4.0510979 -4.107079 -4.1962976 -4.2335052 -4.1857576 -4.1389875 -4.1243162 -4.1435652][-4.5281615 -4.5944347 -4.591382 -4.53245 -4.427135 -4.2892251 -4.196208 -4.1743231 -4.1913233 -4.2446146 -4.2651272 -4.224575 -4.1809163 -4.1631613 -4.1786847][-4.515162 -4.598228 -4.6232705 -4.6016169 -4.530148 -4.411314 -4.3210263 -4.2788057 -4.2662168 -4.27991 -4.2747526 -4.23979 -4.2118917 -4.2094421 -4.2384477][-4.4873309 -4.5747604 -4.6176372 -4.6262965 -4.5898933 -4.501152 -4.4288893 -4.3908839 -4.3744674 -4.367311 -4.3401113 -4.3027277 -4.28527 -4.2962637 -4.3345771][-4.4506984 -4.5305662 -4.5796375 -4.6069646 -4.6040082 -4.5573692 -4.5156946 -4.4925508 -4.483645 -4.4738789 -4.4424677 -4.4066653 -4.3943038 -4.4118624 -4.451736][-4.4133048 -4.4785128 -4.5257664 -4.5620718 -4.5845284 -4.5730538 -4.5516925 -4.5350766 -4.5286193 -4.5240889 -4.5039673 -4.4785891 -4.4731131 -4.494318 -4.5338883]]...]
INFO - root - 2017-12-07 06:32:51.638411: step 6610, loss = 2.07, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 59h:42m:55s remains)
INFO - root - 2017-12-07 06:32:58.667043: step 6620, loss = 1.99, batch loss = 1.91 (11.2 examples/sec; 0.717 sec/batch; 64h:56m:10s remains)
INFO - root - 2017-12-07 06:33:05.594609: step 6630, loss = 2.02, batch loss = 1.94 (11.9 examples/sec; 0.674 sec/batch; 60h:58m:37s remains)
INFO - root - 2017-12-07 06:33:12.607805: step 6640, loss = 2.04, batch loss = 1.95 (11.2 examples/sec; 0.713 sec/batch; 64h:34m:50s remains)
INFO - root - 2017-12-07 06:33:19.750976: step 6650, loss = 1.99, batch loss = 1.91 (10.6 examples/sec; 0.754 sec/batch; 68h:14m:25s remains)
INFO - root - 2017-12-07 06:33:26.788352: step 6660, loss = 2.02, batch loss = 1.94 (10.9 examples/sec; 0.734 sec/batch; 66h:27m:59s remains)
INFO - root - 2017-12-07 06:33:33.823264: step 6670, loss = 2.02, batch loss = 1.94 (11.3 examples/sec; 0.709 sec/batch; 64h:11m:26s remains)
INFO - root - 2017-12-07 06:33:40.936787: step 6680, loss = 2.03, batch loss = 1.95 (11.2 examples/sec; 0.716 sec/batch; 64h:45m:48s remains)
INFO - root - 2017-12-07 06:33:47.999511: step 6690, loss = 2.03, batch loss = 1.95 (11.3 examples/sec; 0.706 sec/batch; 63h:54m:42s remains)
INFO - root - 2017-12-07 06:33:54.822618: step 6700, loss = 2.04, batch loss = 1.96 (11.0 examples/sec; 0.729 sec/batch; 65h:59m:05s remains)
2017-12-07 06:33:55.635152: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3121495 -4.337307 -4.3497424 -4.3543992 -4.3416338 -4.3124142 -4.3124442 -4.3265815 -4.34371 -4.3820577 -4.4382977 -4.5157404 -4.5889935 -4.618526 -4.5847721][-4.3484044 -4.3942394 -4.4215131 -4.4277287 -4.4099965 -4.3841748 -4.3852277 -4.3834291 -4.3852491 -4.4150486 -4.4743032 -4.5770845 -4.6811705 -4.7344465 -4.7049751][-4.3888507 -4.4500017 -4.478826 -4.4735651 -4.44487 -4.4155412 -4.3975244 -4.363524 -4.3542557 -4.387136 -4.4556966 -4.5832648 -4.712894 -4.7948632 -4.7875628][-4.4261451 -4.4941344 -4.5127492 -4.4872336 -4.4419389 -4.3928885 -4.3288913 -4.2533116 -4.2437797 -4.2918639 -4.3764482 -4.526154 -4.672163 -4.7771587 -4.8030081][-4.4538918 -4.5216732 -4.5279074 -4.4838476 -4.4148722 -4.3201942 -4.1805491 -4.0587864 -4.0587149 -4.1365023 -4.2494278 -4.4168882 -4.5661855 -4.6848297 -4.7470655][-4.4661536 -4.5285897 -4.5186977 -4.4487395 -4.3385415 -4.17267 -3.9546623 -3.8109398 -3.8501072 -3.9874291 -4.1455383 -4.3225145 -4.4612222 -4.5779276 -4.6625576][-4.4744325 -4.5286903 -4.496655 -4.3875856 -4.2167106 -3.9724894 -3.6994493 -3.57277 -3.6821752 -3.9058154 -4.1228285 -4.303597 -4.4217324 -4.5139031 -4.5864229][-4.4898515 -4.5412006 -4.486773 -4.3358011 -4.1068316 -3.8072395 -3.5179813 -3.4226058 -3.5887136 -3.8876858 -4.1611223 -4.3517051 -4.4518857 -4.5017648 -4.5268011][-4.5135117 -4.5760932 -4.5202913 -4.3531179 -4.1028209 -3.7922773 -3.5169117 -3.4378972 -3.6137941 -3.9419546 -4.2412577 -4.4366903 -4.5288038 -4.5320373 -4.4949412][-4.5449476 -4.641408 -4.6262093 -4.4925823 -4.2687583 -3.9820926 -3.7286868 -3.6380289 -3.7804031 -4.084106 -4.3560987 -4.5295482 -4.6096649 -4.5769782 -4.4918666][-4.5677862 -4.7002082 -4.7391 -4.6681981 -4.5008588 -4.2604957 -4.0382051 -3.9374516 -4.0414062 -4.28929 -4.49498 -4.6179008 -4.6747947 -4.6210356 -4.5121794][-4.5725846 -4.7157049 -4.784626 -4.7709737 -4.6742644 -4.5028334 -4.3331509 -4.2485027 -4.330369 -4.5172043 -4.6453929 -4.7054377 -4.7254758 -4.6556549 -4.53991][-4.5619459 -4.6911721 -4.7582407 -4.7753482 -4.7393842 -4.6427441 -4.5384579 -4.490097 -4.5596247 -4.6898785 -4.7557893 -4.764111 -4.7441325 -4.6620283 -4.5505066][-4.5313644 -4.629777 -4.672863 -4.6910028 -4.6891155 -4.6486068 -4.6014023 -4.5863662 -4.6438355 -4.7309232 -4.7615047 -4.7415848 -4.6961269 -4.6142492 -4.5205507][-4.4796233 -4.5436625 -4.5563459 -4.5582328 -4.5629072 -4.5488071 -4.5381784 -4.5485311 -4.5968308 -4.656951 -4.6705408 -4.6401644 -4.5888715 -4.5211754 -4.456212]]...]
INFO - root - 2017-12-07 06:34:02.628998: step 6710, loss = 2.02, batch loss = 1.93 (11.0 examples/sec; 0.729 sec/batch; 65h:58m:48s remains)
INFO - root - 2017-12-07 06:34:09.692530: step 6720, loss = 1.99, batch loss = 1.91 (10.3 examples/sec; 0.777 sec/batch; 70h:17m:48s remains)
INFO - root - 2017-12-07 06:34:16.717428: step 6730, loss = 2.03, batch loss = 1.94 (10.9 examples/sec; 0.732 sec/batch; 66h:12m:48s remains)
INFO - root - 2017-12-07 06:34:23.747331: step 6740, loss = 1.99, batch loss = 1.91 (11.4 examples/sec; 0.703 sec/batch; 63h:35m:57s remains)
INFO - root - 2017-12-07 06:34:30.807315: step 6750, loss = 2.03, batch loss = 1.95 (10.6 examples/sec; 0.756 sec/batch; 68h:24m:36s remains)
INFO - root - 2017-12-07 06:34:37.809040: step 6760, loss = 2.03, batch loss = 1.95 (11.0 examples/sec; 0.727 sec/batch; 65h:44m:38s remains)
INFO - root - 2017-12-07 06:34:44.922414: step 6770, loss = 1.99, batch loss = 1.91 (10.6 examples/sec; 0.752 sec/batch; 67h:59m:56s remains)
INFO - root - 2017-12-07 06:34:51.813176: step 6780, loss = 2.03, batch loss = 1.95 (11.7 examples/sec; 0.685 sec/batch; 61h:56m:32s remains)
INFO - root - 2017-12-07 06:34:58.805712: step 6790, loss = 2.05, batch loss = 1.97 (12.4 examples/sec; 0.643 sec/batch; 58h:08m:30s remains)
INFO - root - 2017-12-07 06:35:05.869563: step 6800, loss = 2.03, batch loss = 1.94 (11.8 examples/sec; 0.679 sec/batch; 61h:23m:09s remains)
2017-12-07 06:35:06.615095: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4453597 -4.5204315 -4.5472641 -4.5252848 -4.5044575 -4.47647 -4.4340625 -4.4300141 -4.4891458 -4.5684891 -4.6117449 -4.6315475 -4.6725588 -4.70453 -4.6629767][-4.4700842 -4.5238671 -4.5090737 -4.44324 -4.3908305 -4.3422756 -4.2830076 -4.2667336 -4.3368115 -4.454072 -4.5339832 -4.5658984 -4.5955095 -4.6087656 -4.5451684][-4.4963121 -4.5523725 -4.52253 -4.4349771 -4.3577604 -4.2767777 -4.1811228 -4.1312981 -4.1918521 -4.3383393 -4.4511418 -4.4931755 -4.5020833 -4.4912019 -4.4055285][-4.5120192 -4.5852146 -4.5595684 -4.4669104 -4.3713202 -4.259325 -4.1287789 -4.0437641 -4.0890303 -4.25524 -4.3956842 -4.4466729 -4.4417777 -4.4160533 -4.3282413][-4.509459 -4.5997272 -4.5809956 -4.4856205 -4.3773642 -4.2453237 -4.0950913 -3.9916682 -4.0306964 -4.2106247 -4.3682384 -4.43116 -4.4350576 -4.4227295 -4.36376][-4.484314 -4.5773354 -4.5560174 -4.4563928 -4.3405342 -4.1978135 -4.0465155 -3.9483013 -3.9992833 -4.1932182 -4.3597026 -4.4318151 -4.4497709 -4.4581065 -4.4344926][-4.4477525 -4.5320492 -4.5057054 -4.3985963 -4.2719841 -4.1117005 -3.9564981 -3.8748174 -3.9537106 -4.1678991 -4.3419075 -4.4202633 -4.4486909 -4.4683566 -4.4622245][-4.4273725 -4.5032096 -4.4779892 -4.3685427 -4.2244987 -4.0238461 -3.8350072 -3.7525561 -3.8469067 -4.076221 -4.2606845 -4.3511119 -4.4008012 -4.4360266 -4.4421883][-4.4277854 -4.4957457 -4.472404 -4.3636618 -4.2017918 -3.9535055 -3.7155256 -3.6145518 -3.7010672 -3.9303083 -4.1268487 -4.2407618 -4.3264885 -4.3868036 -4.4057927][-4.4502048 -4.5200272 -4.5062633 -4.4083147 -4.2452536 -3.9841802 -3.7302651 -3.6183114 -3.6774931 -3.8721104 -4.0551324 -4.1787643 -4.2929449 -4.37803 -4.4123387][-4.4850287 -4.5751405 -4.5905037 -4.5177951 -4.37155 -4.1364603 -3.9052534 -3.7931242 -3.817158 -3.9542623 -4.1017747 -4.2175126 -4.3394613 -4.4376078 -4.4797359][-4.5140591 -4.6229248 -4.6677904 -4.623538 -4.5071659 -4.3264656 -4.1479187 -4.0504084 -4.0492611 -4.1310363 -4.238584 -4.342032 -4.4578853 -4.5518293 -4.5874319][-4.5278139 -4.6456814 -4.7147622 -4.7021227 -4.6253567 -4.5086527 -4.3942637 -4.3245111 -4.3122206 -4.3494267 -4.4106894 -4.4820824 -4.5655203 -4.6302004 -4.646029][-4.509778 -4.620657 -4.6959682 -4.7067623 -4.666853 -4.603807 -4.5440764 -4.5043936 -4.4914603 -4.4987106 -4.5184007 -4.5477815 -4.5834055 -4.6059008 -4.5976362][-4.4365149 -4.5161653 -4.57395 -4.592196 -4.5798521 -4.5561123 -4.5347829 -4.5199652 -4.512259 -4.5066423 -4.502162 -4.5014863 -4.5018706 -4.4946718 -4.4721651]]...]
INFO - root - 2017-12-07 06:35:13.615415: step 6810, loss = 2.00, batch loss = 1.92 (11.7 examples/sec; 0.682 sec/batch; 61h:41m:40s remains)
INFO - root - 2017-12-07 06:35:20.605275: step 6820, loss = 2.03, batch loss = 1.95 (12.0 examples/sec; 0.668 sec/batch; 60h:27m:33s remains)
INFO - root - 2017-12-07 06:35:27.697450: step 6830, loss = 1.99, batch loss = 1.91 (11.3 examples/sec; 0.710 sec/batch; 64h:13m:39s remains)
INFO - root - 2017-12-07 06:35:34.819171: step 6840, loss = 2.03, batch loss = 1.95 (12.1 examples/sec; 0.663 sec/batch; 59h:57m:20s remains)
INFO - root - 2017-12-07 06:35:41.884381: step 6850, loss = 2.01, batch loss = 1.93 (11.4 examples/sec; 0.703 sec/batch; 63h:38m:05s remains)
INFO - root - 2017-12-07 06:35:48.847536: step 6860, loss = 2.01, batch loss = 1.93 (11.7 examples/sec; 0.685 sec/batch; 61h:57m:58s remains)
INFO - root - 2017-12-07 06:35:55.925212: step 6870, loss = 2.03, batch loss = 1.95 (10.9 examples/sec; 0.732 sec/batch; 66h:10m:20s remains)
INFO - root - 2017-12-07 06:36:02.990497: step 6880, loss = 2.01, batch loss = 1.93 (11.4 examples/sec; 0.704 sec/batch; 63h:40m:06s remains)
INFO - root - 2017-12-07 06:36:10.113162: step 6890, loss = 2.00, batch loss = 1.92 (11.7 examples/sec; 0.683 sec/batch; 61h:44m:29s remains)
INFO - root - 2017-12-07 06:36:17.162576: step 6900, loss = 1.99, batch loss = 1.91 (11.7 examples/sec; 0.686 sec/batch; 62h:03m:34s remains)
2017-12-07 06:36:18.055643: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1639342 -4.1593118 -4.2652516 -4.4584761 -4.67907 -4.8426504 -4.8458576 -4.7092414 -4.5644116 -4.485445 -4.4828281 -4.4959722 -4.5136681 -4.56491 -4.6009216][-4.1380811 -4.1154451 -4.2092929 -4.4132204 -4.6490936 -4.8178282 -4.8259788 -4.6939774 -4.5502973 -4.4754958 -4.479835 -4.5101891 -4.5518017 -4.6167245 -4.6468968][-4.113658 -4.0878768 -4.1941323 -4.4129348 -4.6402807 -4.7692652 -4.7496343 -4.6282187 -4.5233135 -4.4895544 -4.5136418 -4.5515766 -4.5903616 -4.6390867 -4.6456141][-4.102016 -4.0849028 -4.2156372 -4.4525952 -4.6652164 -4.7357826 -4.6631584 -4.5424142 -4.4906368 -4.5193582 -4.5724244 -4.6121287 -4.6287947 -4.6401095 -4.621315][-4.1194625 -4.1161156 -4.2525468 -4.4723945 -4.6458731 -4.654851 -4.52823 -4.4016466 -4.4045129 -4.5003924 -4.5931578 -4.6498318 -4.6576424 -4.6376925 -4.6011171][-4.1624756 -4.1902814 -4.3098783 -4.4553285 -4.5284262 -4.4488573 -4.26192 -4.122344 -4.1710215 -4.3307662 -4.4817185 -4.5938988 -4.6372104 -4.6159849 -4.569313][-4.2415948 -4.2967677 -4.3831267 -4.4246411 -4.3637872 -4.1864243 -3.9507728 -3.8012152 -3.8782673 -4.0809765 -4.2830935 -4.4654188 -4.5724287 -4.5804458 -4.5363479][-4.3390226 -4.4114518 -4.4698792 -4.4250879 -4.2576146 -4.0110254 -3.751169 -3.5983579 -3.6752477 -3.8779011 -4.0952349 -4.3261175 -4.4993434 -4.5565448 -4.5259628][-4.4236455 -4.51592 -4.5713067 -4.4877639 -4.26734 -3.9900398 -3.7338352 -3.5870297 -3.6392179 -3.8048577 -4.0091448 -4.2565022 -4.4627123 -4.5527563 -4.5308771][-4.4746971 -4.5845819 -4.6552949 -4.5781074 -4.3629932 -4.1077633 -3.8902357 -3.7659128 -3.7856526 -3.8905709 -4.051826 -4.2688675 -4.4559255 -4.5445 -4.5250731][-4.4897289 -4.6033916 -4.6968307 -4.6620865 -4.503531 -4.3138185 -4.1604996 -4.0718417 -4.0640411 -4.1005063 -4.1966991 -4.3503671 -4.4864984 -4.5458155 -4.5165377][-4.5090351 -4.6085606 -4.706141 -4.7122607 -4.6223507 -4.5057178 -4.4135985 -4.3603778 -4.34686 -4.3431158 -4.3866129 -4.4782872 -4.5568886 -4.5738339 -4.525044][-4.5238981 -4.5988851 -4.683785 -4.7150273 -4.6805782 -4.6227231 -4.5740886 -4.5431356 -4.5290828 -4.503727 -4.508574 -4.5512714 -4.5900297 -4.58204 -4.526968][-4.494472 -4.547915 -4.6187363 -4.6631017 -4.6643596 -4.6403909 -4.6120119 -4.5857391 -4.5621285 -4.5211515 -4.501441 -4.5192184 -4.5463724 -4.5395827 -4.4959455][-4.4468431 -4.4798756 -4.5285435 -4.5653534 -4.5744534 -4.5626984 -4.5424833 -4.5187907 -4.4926987 -4.4505987 -4.4261851 -4.438159 -4.4652572 -4.4666028 -4.4381518]]...]
INFO - root - 2017-12-07 06:36:25.131951: step 6910, loss = 2.01, batch loss = 1.93 (11.6 examples/sec; 0.691 sec/batch; 62h:30m:49s remains)
INFO - root - 2017-12-07 06:36:32.261332: step 6920, loss = 2.03, batch loss = 1.94 (11.4 examples/sec; 0.700 sec/batch; 63h:18m:13s remains)
INFO - root - 2017-12-07 06:36:39.432638: step 6930, loss = 2.00, batch loss = 1.92 (11.4 examples/sec; 0.703 sec/batch; 63h:34m:33s remains)
INFO - root - 2017-12-07 06:36:46.196672: step 6940, loss = 2.00, batch loss = 1.92 (12.0 examples/sec; 0.666 sec/batch; 60h:13m:20s remains)
INFO - root - 2017-12-07 06:36:53.252838: step 6950, loss = 2.08, batch loss = 1.99 (10.8 examples/sec; 0.739 sec/batch; 66h:51m:37s remains)
INFO - root - 2017-12-07 06:37:00.316063: step 6960, loss = 2.03, batch loss = 1.95 (11.2 examples/sec; 0.717 sec/batch; 64h:48m:33s remains)
INFO - root - 2017-12-07 06:37:07.310173: step 6970, loss = 2.02, batch loss = 1.93 (11.5 examples/sec; 0.698 sec/batch; 63h:09m:11s remains)
INFO - root - 2017-12-07 06:37:14.308293: step 6980, loss = 2.03, batch loss = 1.94 (11.6 examples/sec; 0.687 sec/batch; 62h:05m:46s remains)
INFO - root - 2017-12-07 06:37:21.385233: step 6990, loss = 1.99, batch loss = 1.91 (11.0 examples/sec; 0.725 sec/batch; 65h:31m:54s remains)
INFO - root - 2017-12-07 06:37:28.440508: step 7000, loss = 2.01, batch loss = 1.93 (11.5 examples/sec; 0.697 sec/batch; 63h:00m:51s remains)
2017-12-07 06:37:29.222908: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42495 -4.398849 -4.3303857 -4.2562857 -4.2000704 -4.1859465 -4.1971345 -4.2219806 -4.2454252 -4.2231212 -4.1838846 -4.2070012 -4.2624574 -4.2859006 -4.2751842][-4.4231195 -4.3906031 -4.3354154 -4.2721491 -4.2196007 -4.2052317 -4.2130637 -4.2307315 -4.2573318 -4.2542105 -4.2346778 -4.2419739 -4.2488141 -4.2371144 -4.2176676][-4.4373074 -4.3959694 -4.3473396 -4.2910428 -4.2406874 -4.2150683 -4.1983032 -4.1974573 -4.2353373 -4.2718492 -4.2882004 -4.2773342 -4.2283211 -4.1846137 -4.166296][-4.4183049 -4.3753386 -4.3344488 -4.2913318 -4.2542672 -4.2243409 -4.1778412 -4.1557255 -4.2068357 -4.2887335 -4.3474979 -4.3333769 -4.2493458 -4.1833148 -4.1637273][-4.3591738 -4.3273687 -4.3011026 -4.2707515 -4.2447109 -4.2105293 -4.1370196 -4.0948563 -4.1597652 -4.2856731 -4.3846879 -4.3831573 -4.2908859 -4.213769 -4.1911469][-4.2602134 -4.2431688 -4.2347608 -4.2052078 -4.1722016 -4.1328268 -4.0572238 -4.0180044 -4.102756 -4.264936 -4.3924494 -4.409586 -4.331069 -4.2532487 -4.2273579][-4.1327162 -4.1346207 -4.146543 -4.1215782 -4.0850949 -4.0535922 -4.0056973 -3.9916465 -4.0927916 -4.2658873 -4.3925757 -4.4121466 -4.34448 -4.271328 -4.2496858][-4.029882 -4.0616074 -4.1026311 -4.100378 -4.074357 -4.059442 -4.0511079 -4.0672359 -4.1608434 -4.2982645 -4.3800569 -4.371459 -4.3014412 -4.2364964 -4.2240286][-3.9917622 -4.042141 -4.1012421 -4.1219187 -4.1115465 -4.1177521 -4.1481867 -4.1875105 -4.25569 -4.3317041 -4.3573551 -4.3209972 -4.2498374 -4.192029 -4.1790438][-4.0134182 -4.0667095 -4.1313028 -4.1643753 -4.1657348 -4.1888437 -4.2407689 -4.2890687 -4.3261495 -4.3431172 -4.3255277 -4.2787132 -4.2193928 -4.1686487 -4.1413693][-4.083775 -4.14207 -4.2110052 -4.2443943 -4.2481785 -4.2664285 -4.3015676 -4.3285112 -4.3352346 -4.3194318 -4.2909145 -4.258904 -4.2230558 -4.1784792 -4.1326561][-4.1606059 -4.225492 -4.2892933 -4.3137908 -4.313045 -4.3151417 -4.3191767 -4.3207259 -4.3099313 -4.2871857 -4.2695456 -4.259304 -4.2432232 -4.2029376 -4.142858][-4.19133 -4.2596612 -4.3183441 -4.3416071 -4.3473859 -4.3444567 -4.328104 -4.309176 -4.2843571 -4.2587433 -4.2462988 -4.2446804 -4.2378006 -4.2055736 -4.1423006][-4.1767583 -4.2363472 -4.2927051 -4.3240533 -4.3482323 -4.3526506 -4.3284144 -4.2939234 -4.2589989 -4.23233 -4.22284 -4.2279291 -4.2325 -4.2139392 -4.1527076][-4.0923743 -4.1431837 -4.1993675 -4.2433319 -4.2888083 -4.3094954 -4.2927585 -4.2574921 -4.2270031 -4.2129617 -4.2135711 -4.230176 -4.2422376 -4.2237453 -4.1608095]]...]
INFO - root - 2017-12-07 06:37:36.206173: step 7010, loss = 2.02, batch loss = 1.93 (11.9 examples/sec; 0.670 sec/batch; 60h:36m:48s remains)
INFO - root - 2017-12-07 06:37:43.241596: step 7020, loss = 2.03, batch loss = 1.94 (12.5 examples/sec; 0.638 sec/batch; 57h:41m:01s remains)
INFO - root - 2017-12-07 06:37:50.288065: step 7030, loss = 2.01, batch loss = 1.93 (11.2 examples/sec; 0.716 sec/batch; 64h:45m:12s remains)
INFO - root - 2017-12-07 06:37:57.322040: step 7040, loss = 2.04, batch loss = 1.96 (10.8 examples/sec; 0.741 sec/batch; 67h:01m:02s remains)
INFO - root - 2017-12-07 06:38:04.342055: step 7050, loss = 2.03, batch loss = 1.95 (11.5 examples/sec; 0.694 sec/batch; 62h:45m:34s remains)
INFO - root - 2017-12-07 06:38:11.379175: step 7060, loss = 1.99, batch loss = 1.91 (10.7 examples/sec; 0.747 sec/batch; 67h:33m:26s remains)
INFO - root - 2017-12-07 06:38:18.449045: step 7070, loss = 2.03, batch loss = 1.95 (10.3 examples/sec; 0.776 sec/batch; 70h:10m:34s remains)
INFO - root - 2017-12-07 06:38:25.531719: step 7080, loss = 2.03, batch loss = 1.95 (10.9 examples/sec; 0.733 sec/batch; 66h:14m:00s remains)
INFO - root - 2017-12-07 06:38:32.541689: step 7090, loss = 2.01, batch loss = 1.92 (11.7 examples/sec; 0.686 sec/batch; 62h:02m:15s remains)
INFO - root - 2017-12-07 06:38:39.543312: step 7100, loss = 2.06, batch loss = 1.98 (11.5 examples/sec; 0.698 sec/batch; 63h:05m:58s remains)
2017-12-07 06:38:40.354656: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4153652 -4.3633881 -4.3057675 -4.2730608 -4.2862244 -4.3150706 -4.322577 -4.2930284 -4.2661061 -4.2825513 -4.3077321 -4.2831163 -4.2084603 -4.176218 -4.2072935][-4.3514876 -4.3416963 -4.3002977 -4.253315 -4.2481437 -4.289927 -4.3311186 -4.3278518 -4.31172 -4.3278189 -4.3426394 -4.3042812 -4.22361 -4.1920753 -4.2222104][-4.2754135 -4.3243852 -4.3173027 -4.2599168 -4.2185612 -4.242332 -4.2939529 -4.3199792 -4.3420572 -4.3838997 -4.3948393 -4.334867 -4.2411737 -4.2027831 -4.2292032][-4.2137194 -4.3036461 -4.3274465 -4.2670512 -4.1858826 -4.1592808 -4.1790547 -4.215066 -4.2978659 -4.4032845 -4.4346089 -4.3583794 -4.2490263 -4.1946931 -4.2103462][-4.1732106 -4.2625594 -4.3043218 -4.2581577 -4.1549778 -4.0586939 -4.0018964 -4.0169086 -4.15957 -4.3580756 -4.4475346 -4.377811 -4.25518 -4.1798148 -4.178][-4.1799617 -4.2374449 -4.27541 -4.2433929 -4.1266351 -3.9571376 -3.8086393 -3.78621 -3.9735646 -4.2669158 -4.4363213 -4.3937211 -4.2681508 -4.1775818 -4.1548467][-4.2460132 -4.2779193 -4.3041744 -4.2727504 -4.132576 -3.8918805 -3.6589315 -3.5939798 -3.7982595 -4.148241 -4.3777437 -4.3662033 -4.2502556 -4.166492 -4.1428938][-4.3436475 -4.3776345 -4.4020824 -4.3631573 -4.1957693 -3.9073963 -3.6221402 -3.5204072 -3.7024925 -4.0430832 -4.2845573 -4.2960095 -4.2049942 -4.1524849 -4.1566925][-4.3859887 -4.4444547 -4.4951611 -4.4726744 -4.3151035 -4.0411448 -3.7707124 -3.6550756 -3.7745261 -4.0234776 -4.20249 -4.2060156 -4.1393189 -4.1244631 -4.1645503][-4.3220782 -4.4176092 -4.5154433 -4.5376363 -4.4311256 -4.2269721 -4.0316157 -3.9430394 -3.9962778 -4.1104045 -4.1706514 -4.130569 -4.075243 -4.0864606 -4.1502876][-4.2043505 -4.332077 -4.4653535 -4.5250711 -4.470232 -4.3390579 -4.2308512 -4.1966276 -4.2207656 -4.2318587 -4.1856604 -4.1003928 -4.0538969 -4.0844321 -4.1606665][-4.1163688 -4.2445922 -4.381856 -4.4589348 -4.4389081 -4.3526912 -4.2910538 -4.2951622 -4.3257122 -4.3087673 -4.2304969 -4.1413112 -4.1085925 -4.1429605 -4.2081537][-4.0960345 -4.20033 -4.3304019 -4.4092078 -4.39884 -4.3222461 -4.2595563 -4.2624459 -4.3027239 -4.3108778 -4.2733421 -4.2270236 -4.2142038 -4.2344913 -4.2633085][-4.1455054 -4.223762 -4.340991 -4.4030828 -4.3733315 -4.2858119 -4.207026 -4.19321 -4.2272267 -4.2545848 -4.2597537 -4.2585764 -4.2669296 -4.2744846 -4.2684031][-4.2169704 -4.2644696 -4.3573523 -4.4026442 -4.3628674 -4.2732644 -4.1917238 -4.1679745 -4.1832123 -4.1945729 -4.2008772 -4.2130337 -4.23054 -4.2318873 -4.2081656]]...]
INFO - root - 2017-12-07 06:38:47.248195: step 7110, loss = 2.08, batch loss = 1.99 (11.1 examples/sec; 0.722 sec/batch; 65h:16m:46s remains)
INFO - root - 2017-12-07 06:38:54.373581: step 7120, loss = 2.00, batch loss = 1.92 (11.5 examples/sec; 0.695 sec/batch; 62h:49m:30s remains)
INFO - root - 2017-12-07 06:39:01.399726: step 7130, loss = 2.03, batch loss = 1.95 (11.4 examples/sec; 0.704 sec/batch; 63h:38m:47s remains)
INFO - root - 2017-12-07 06:39:08.472644: step 7140, loss = 2.07, batch loss = 1.99 (10.7 examples/sec; 0.751 sec/batch; 67h:50m:51s remains)
INFO - root - 2017-12-07 06:39:15.510586: step 7150, loss = 1.99, batch loss = 1.91 (10.6 examples/sec; 0.755 sec/batch; 68h:12m:37s remains)
INFO - root - 2017-12-07 06:39:22.515498: step 7160, loss = 2.04, batch loss = 1.95 (10.9 examples/sec; 0.733 sec/batch; 66h:14m:47s remains)
INFO - root - 2017-12-07 06:39:29.502725: step 7170, loss = 2.00, batch loss = 1.92 (12.1 examples/sec; 0.662 sec/batch; 59h:51m:47s remains)
INFO - root - 2017-12-07 06:39:36.525866: step 7180, loss = 2.04, batch loss = 1.96 (12.3 examples/sec; 0.651 sec/batch; 58h:49m:11s remains)
INFO - root - 2017-12-07 06:39:43.694636: step 7190, loss = 2.03, batch loss = 1.95 (11.6 examples/sec; 0.692 sec/batch; 62h:32m:11s remains)
INFO - root - 2017-12-07 06:39:50.681108: step 7200, loss = 2.01, batch loss = 1.93 (11.5 examples/sec; 0.696 sec/batch; 62h:55m:08s remains)
2017-12-07 06:39:51.493187: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3296204 -4.380868 -4.4142284 -4.43904 -4.4579458 -4.4546032 -4.4179025 -4.3781219 -4.366982 -4.3820262 -4.37653 -4.34115 -4.3045673 -4.2952352 -4.3083696][-4.4894161 -4.5673571 -4.6034245 -4.615056 -4.6172476 -4.5877318 -4.5193934 -4.4523025 -4.426724 -4.4431686 -4.4543085 -4.4373355 -4.3999548 -4.3686461 -4.3474784][-4.6023154 -4.68358 -4.7186065 -4.7253017 -4.7120929 -4.6557503 -4.5589156 -4.4642372 -4.4260497 -4.4580383 -4.5111203 -4.5347266 -4.4990473 -4.4337215 -4.365624][-4.5842 -4.650949 -4.69522 -4.7164474 -4.7011728 -4.6271896 -4.5056825 -4.3780127 -4.3249936 -4.3900061 -4.5161347 -4.6063766 -4.588819 -4.4996824 -4.3943696][-4.4720025 -4.5229092 -4.5832839 -4.6123495 -4.576787 -4.4682503 -4.3086219 -4.1386595 -4.0707674 -4.1845651 -4.4100161 -4.5975804 -4.63537 -4.5583797 -4.4426537][-4.35445 -4.394773 -4.4675884 -4.4828262 -4.4042268 -4.2452912 -4.0377755 -3.8176193 -3.724292 -3.8759782 -4.1933327 -4.4831743 -4.6090126 -4.593914 -4.5151372][-4.2717457 -4.2990913 -4.3637152 -4.3415937 -4.2113309 -4.0155616 -3.7808013 -3.533808 -3.4249992 -3.5943365 -3.9615324 -4.3187237 -4.5246525 -4.5878458 -4.5675707][-4.231894 -4.2552919 -4.3062515 -4.2508297 -4.0933881 -3.8972192 -3.6756124 -3.4394784 -3.3306212 -3.4922152 -3.8523967 -4.215909 -4.4533477 -4.5544863 -4.5560703][-4.2084007 -4.2522893 -4.3158374 -4.2785683 -4.1576371 -4.0118375 -3.834275 -3.6243448 -3.5093427 -3.627506 -3.9245114 -4.2347927 -4.4515271 -4.5448332 -4.527667][-4.2043638 -4.2668438 -4.3491569 -4.3600893 -4.3130641 -4.2396064 -4.117075 -3.9424126 -3.8246291 -3.8851604 -4.0932488 -4.32312 -4.4932876 -4.5571694 -4.5084133][-4.2471457 -4.3115511 -4.3973141 -4.4437656 -4.4561 -4.4409351 -4.3714595 -4.2479715 -4.1493063 -4.166224 -4.2894521 -4.4345503 -4.5468903 -4.5702834 -4.4859257][-4.3024549 -4.3633347 -4.4446492 -4.5025616 -4.5365229 -4.5446415 -4.5131187 -4.4474063 -4.3879871 -4.38557 -4.4417582 -4.5167069 -4.5824375 -4.5771837 -4.4751921][-4.3239288 -4.3870831 -4.4639945 -4.5174956 -4.5436163 -4.5469313 -4.5359483 -4.5220165 -4.5081158 -4.5045285 -4.5161557 -4.5375371 -4.568121 -4.5510206 -4.459115][-4.3160877 -4.3832955 -4.4543495 -4.50045 -4.5193887 -4.5205765 -4.5223131 -4.540153 -4.5568004 -4.5572028 -4.5443563 -4.5299816 -4.5287428 -4.4995203 -4.4195843][-4.3073411 -4.3615556 -4.4136176 -4.4473882 -4.4657373 -4.4747996 -4.4869728 -4.5131378 -4.535234 -4.5377297 -4.5270123 -4.5135961 -4.5055418 -4.4709668 -4.3970809]]...]
INFO - root - 2017-12-07 06:39:58.346557: step 7210, loss = 2.05, batch loss = 1.97 (15.2 examples/sec; 0.525 sec/batch; 47h:25m:30s remains)
INFO - root - 2017-12-07 06:40:05.230609: step 7220, loss = 2.04, batch loss = 1.96 (11.5 examples/sec; 0.695 sec/batch; 62h:46m:04s remains)
INFO - root - 2017-12-07 06:40:12.259846: step 7230, loss = 2.05, batch loss = 1.96 (11.5 examples/sec; 0.693 sec/batch; 62h:37m:12s remains)
INFO - root - 2017-12-07 06:40:19.266048: step 7240, loss = 2.00, batch loss = 1.92 (11.7 examples/sec; 0.684 sec/batch; 61h:47m:05s remains)
INFO - root - 2017-12-07 06:40:26.276163: step 7250, loss = 2.05, batch loss = 1.96 (12.1 examples/sec; 0.659 sec/batch; 59h:32m:01s remains)
INFO - root - 2017-12-07 06:40:33.324184: step 7260, loss = 1.98, batch loss = 1.89 (12.0 examples/sec; 0.668 sec/batch; 60h:20m:45s remains)
INFO - root - 2017-12-07 06:40:40.268935: step 7270, loss = 2.02, batch loss = 1.94 (11.5 examples/sec; 0.697 sec/batch; 62h:58m:16s remains)
INFO - root - 2017-12-07 06:40:47.378750: step 7280, loss = 2.03, batch loss = 1.95 (10.9 examples/sec; 0.733 sec/batch; 66h:14m:58s remains)
INFO - root - 2017-12-07 06:40:54.377118: step 7290, loss = 1.99, batch loss = 1.91 (11.7 examples/sec; 0.684 sec/batch; 61h:49m:53s remains)
INFO - root - 2017-12-07 06:41:01.499972: step 7300, loss = 2.01, batch loss = 1.93 (10.9 examples/sec; 0.731 sec/batch; 66h:02m:51s remains)
2017-12-07 06:41:02.308693: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4790707 -4.4603243 -4.4542871 -4.4914107 -4.525063 -4.449008 -4.2842011 -4.2058024 -4.258019 -4.3522692 -4.4581609 -4.5479965 -4.6109438 -4.6075191 -4.5291262][-4.4637332 -4.4496121 -4.440465 -4.4614697 -4.4846768 -4.4091716 -4.238502 -4.1472464 -4.1955051 -4.2818408 -4.3613853 -4.4230022 -4.4744544 -4.4695678 -4.3869057][-4.4441061 -4.4335918 -4.4193473 -4.4266276 -4.4440212 -4.3750696 -4.2081757 -4.1127524 -4.1649652 -4.2522364 -4.30223 -4.3267274 -4.3606572 -4.3569746 -4.280951][-4.4185066 -4.4118185 -4.391293 -4.3849545 -4.3923144 -4.3258157 -4.1659446 -4.0696297 -4.131475 -4.2362919 -4.2835503 -4.2921267 -4.3205991 -4.3272338 -4.2650175][-4.3952236 -4.4018173 -4.3817749 -4.3563442 -4.3342295 -4.2469368 -4.0777407 -3.9759631 -4.0542264 -4.1955919 -4.2776446 -4.3081365 -4.3464193 -4.3544989 -4.2894759][-4.3978791 -4.4223595 -4.4061728 -4.3475862 -4.2661448 -4.1272974 -3.9290156 -3.8173409 -3.9195824 -4.1100235 -4.2459412 -4.3175926 -4.3715076 -4.3706083 -4.2881508][-4.4225855 -4.4611316 -4.4480577 -4.3569884 -4.2103319 -4.0092998 -3.7697377 -3.6443095 -3.7745566 -4.0142455 -4.1963139 -4.2983575 -4.3602819 -4.3523545 -4.2624683][-4.450541 -4.4996643 -4.4973722 -4.3949938 -4.2108431 -3.9713926 -3.7045505 -3.5638964 -3.7034726 -3.9660683 -4.1650138 -4.2721558 -4.337615 -4.3373046 -4.2592869][-4.4690752 -4.5216403 -4.534348 -4.4461765 -4.26452 -4.0310326 -3.7815371 -3.6439669 -3.7618546 -4.008503 -4.1946712 -4.2903619 -4.3587413 -4.3787203 -4.3194065][-4.4694262 -4.5220733 -4.55645 -4.5071578 -4.36143 -4.1610975 -3.9503605 -3.8288679 -3.9076152 -4.111845 -4.2745867 -4.3541818 -4.4177337 -4.45459 -4.414711][-4.4469008 -4.5034361 -4.5678558 -4.5739994 -4.484396 -4.3272719 -4.1590242 -4.0561161 -4.0925789 -4.2382426 -4.3668933 -4.4257364 -4.4697561 -4.5058551 -4.4808531][-4.4200931 -4.475657 -4.5630174 -4.6189628 -4.5858593 -4.4754462 -4.34697 -4.2627645 -4.2680607 -4.3597069 -4.4495378 -4.4826179 -4.4945469 -4.5074968 -4.4845643][-4.4077048 -4.4519143 -4.5436649 -4.6256504 -4.63365 -4.5651808 -4.4704123 -4.4033771 -4.3935909 -4.4467955 -4.4993558 -4.5055771 -4.488152 -4.4730291 -4.4428058][-4.4199739 -4.4445958 -4.517684 -4.5935512 -4.6165943 -4.5766096 -4.5087376 -4.4558268 -4.4390745 -4.4644785 -4.4841676 -4.4683595 -4.4394503 -4.41678 -4.388062][-4.4510756 -4.4541693 -4.4932013 -4.5387545 -4.5531774 -4.5254 -4.4746618 -4.42842 -4.4047594 -4.4138947 -4.4196734 -4.39787 -4.3738503 -4.3630681 -4.3487983]]...]
INFO - root - 2017-12-07 06:41:09.359935: step 7310, loss = 2.03, batch loss = 1.95 (11.0 examples/sec; 0.727 sec/batch; 65h:40m:38s remains)
INFO - root - 2017-12-07 06:41:16.342798: step 7320, loss = 2.02, batch loss = 1.94 (11.6 examples/sec; 0.688 sec/batch; 62h:07m:31s remains)
INFO - root - 2017-12-07 06:41:23.544193: step 7330, loss = 2.01, batch loss = 1.92 (10.8 examples/sec; 0.739 sec/batch; 66h:43m:23s remains)
INFO - root - 2017-12-07 06:41:30.707765: step 7340, loss = 2.03, batch loss = 1.95 (10.8 examples/sec; 0.739 sec/batch; 66h:43m:31s remains)
INFO - root - 2017-12-07 06:41:37.672029: step 7350, loss = 2.02, batch loss = 1.94 (11.4 examples/sec; 0.700 sec/batch; 63h:12m:00s remains)
INFO - root - 2017-12-07 06:41:44.726573: step 7360, loss = 2.02, batch loss = 1.93 (11.6 examples/sec; 0.688 sec/batch; 62h:05m:56s remains)
INFO - root - 2017-12-07 06:41:51.824114: step 7370, loss = 2.00, batch loss = 1.92 (11.1 examples/sec; 0.722 sec/batch; 65h:11m:53s remains)
INFO - root - 2017-12-07 06:41:58.875778: step 7380, loss = 1.99, batch loss = 1.91 (11.1 examples/sec; 0.722 sec/batch; 65h:13m:50s remains)
INFO - root - 2017-12-07 06:42:05.952039: step 7390, loss = 2.03, batch loss = 1.94 (11.2 examples/sec; 0.714 sec/batch; 64h:28m:43s remains)
INFO - root - 2017-12-07 06:42:12.994960: step 7400, loss = 1.99, batch loss = 1.91 (11.9 examples/sec; 0.672 sec/batch; 60h:41m:12s remains)
2017-12-07 06:42:13.831766: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3236432 -4.4455876 -4.5588231 -4.5768118 -4.5369072 -4.4944673 -4.4560494 -4.4048109 -4.3585706 -4.372426 -4.4510512 -4.5123749 -4.5285568 -4.5355039 -4.5614266][-4.3314004 -4.4702506 -4.5929923 -4.6038108 -4.5500555 -4.5094728 -4.4926062 -4.4557562 -4.4004307 -4.3924 -4.4583926 -4.5189466 -4.5335569 -4.5346656 -4.5470948][-4.3365078 -4.4827948 -4.6003466 -4.5863409 -4.5039697 -4.4607668 -4.468049 -4.4539275 -4.4002743 -4.3724918 -4.40826 -4.4523511 -4.4719748 -4.4832783 -4.4915452][-4.339612 -4.4849043 -4.5851974 -4.53415 -4.4137282 -4.3563657 -4.3783545 -4.3876815 -4.3451848 -4.2998281 -4.2891016 -4.3085871 -4.3517346 -4.4118395 -4.4607878][-4.3418465 -4.481843 -4.5650454 -4.4830465 -4.3258324 -4.2399898 -4.2522469 -4.2671957 -4.2322164 -4.1661263 -4.1028342 -4.0996246 -4.1784987 -4.312355 -4.4487858][-4.342732 -4.4786162 -4.55382 -4.4551148 -4.2669339 -4.1337314 -4.0996265 -4.092144 -4.0555439 -3.9778326 -3.8876104 -3.8803282 -3.9942019 -4.1970129 -4.4311604][-4.3375545 -4.4736037 -4.5495129 -4.4485712 -4.2430997 -4.0617728 -3.9638126 -3.9198165 -3.8771505 -3.8060215 -3.7339282 -3.7542377 -3.8977 -4.1429954 -4.449307][-4.3277392 -4.4677634 -4.5560913 -4.4716549 -4.2728434 -4.0654621 -3.9157419 -3.8390865 -3.789609 -3.7420926 -3.73174 -3.8092213 -3.9747736 -4.2209468 -4.5312667][-4.3190727 -4.4653254 -4.5749125 -4.5214152 -4.3514476 -4.1446624 -3.9650707 -3.8651493 -3.8043818 -3.7852726 -3.853642 -3.9872446 -4.1524706 -4.3653893 -4.6272764][-4.3175325 -4.4740405 -4.6123281 -4.6034145 -4.4837718 -4.3088942 -4.1282468 -4.0189843 -3.9420724 -3.9346085 -4.0556622 -4.20615 -4.3345661 -4.4859443 -4.6736484][-4.3243318 -4.4904866 -4.6570163 -4.6965165 -4.6367531 -4.5124 -4.3502679 -4.2363405 -4.1440468 -4.1359563 -4.2757177 -4.417129 -4.4987221 -4.5768981 -4.6697741][-4.3310237 -4.4981227 -4.6774516 -4.747581 -4.7313747 -4.6506143 -4.5151696 -4.4085622 -4.3211942 -4.3203773 -4.4529195 -4.562891 -4.5980964 -4.6139197 -4.6238556][-4.33155 -4.4875636 -4.6604128 -4.7403054 -4.7460413 -4.69744 -4.5976491 -4.5123096 -4.4461575 -4.455471 -4.5603881 -4.6250043 -4.6209888 -4.5973263 -4.5622597][-4.3166561 -4.4493775 -4.6042829 -4.6859775 -4.7031302 -4.6757097 -4.6042404 -4.531579 -4.4774008 -4.4934211 -4.5747919 -4.6099272 -4.5905485 -4.5582075 -4.5174942][-4.2829151 -4.3831992 -4.5148668 -4.5949764 -4.6150708 -4.5896082 -4.521666 -4.4419236 -4.3860221 -4.409586 -4.48709 -4.521389 -4.5125904 -4.4982 -4.4851408]]...]
INFO - root - 2017-12-07 06:42:20.972116: step 7410, loss = 2.05, batch loss = 1.97 (10.8 examples/sec; 0.740 sec/batch; 66h:47m:09s remains)
INFO - root - 2017-12-07 06:42:28.058665: step 7420, loss = 2.01, batch loss = 1.93 (11.1 examples/sec; 0.721 sec/batch; 65h:07m:00s remains)
INFO - root - 2017-12-07 06:42:35.125423: step 7430, loss = 2.05, batch loss = 1.97 (11.5 examples/sec; 0.696 sec/batch; 62h:51m:24s remains)
INFO - root - 2017-12-07 06:42:42.181702: step 7440, loss = 1.99, batch loss = 1.91 (12.0 examples/sec; 0.665 sec/batch; 60h:01m:51s remains)
INFO - root - 2017-12-07 06:42:49.260658: step 7450, loss = 2.04, batch loss = 1.96 (11.9 examples/sec; 0.671 sec/batch; 60h:37m:18s remains)
INFO - root - 2017-12-07 06:42:56.331820: step 7460, loss = 2.02, batch loss = 1.94 (11.1 examples/sec; 0.722 sec/batch; 65h:13m:14s remains)
INFO - root - 2017-12-07 06:43:03.356084: step 7470, loss = 2.00, batch loss = 1.92 (11.7 examples/sec; 0.681 sec/batch; 61h:31m:37s remains)
INFO - root - 2017-12-07 06:43:10.424125: step 7480, loss = 2.03, batch loss = 1.95 (11.6 examples/sec; 0.692 sec/batch; 62h:26m:46s remains)
INFO - root - 2017-12-07 06:43:17.554587: step 7490, loss = 2.01, batch loss = 1.93 (11.3 examples/sec; 0.706 sec/batch; 63h:43m:13s remains)
INFO - root - 2017-12-07 06:43:24.534064: step 7500, loss = 2.01, batch loss = 1.93 (11.4 examples/sec; 0.702 sec/batch; 63h:24m:09s remains)
2017-12-07 06:43:25.338650: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5752153 -4.6687589 -4.7449884 -4.7891684 -4.8081355 -4.8108373 -4.7962594 -4.767571 -4.7460685 -4.7443075 -4.7415919 -4.7115579 -4.6708217 -4.6324382 -4.5973806][-4.5350628 -4.6669769 -4.7787485 -4.8473964 -4.8710327 -4.8550153 -4.8022437 -4.7356687 -4.7075825 -4.7299356 -4.7481894 -4.7177057 -4.6661739 -4.6152868 -4.5623713][-4.4848452 -4.6357665 -4.7582521 -4.8279977 -4.8380833 -4.7863989 -4.6808691 -4.5673289 -4.5326815 -4.5975256 -4.6648717 -4.6594434 -4.6185627 -4.5738049 -4.5156302][-4.4429665 -4.5907826 -4.7017517 -4.7479873 -4.7199697 -4.6180711 -4.4600463 -4.3104873 -4.277441 -4.3949728 -4.5286179 -4.5698743 -4.5569582 -4.5316591 -4.4813042][-4.4159651 -4.5403385 -4.6330676 -4.654685 -4.5814271 -4.4268222 -4.2344742 -4.079484 -4.0643692 -4.2233043 -4.4096251 -4.492682 -4.5038366 -4.493474 -4.4564061][-4.4112139 -4.4869509 -4.5574131 -4.56582 -4.4634295 -4.2713766 -4.0663714 -3.9269779 -3.942625 -4.1247253 -4.3347349 -4.443295 -4.4669375 -4.4585652 -4.4313679][-4.4432249 -4.4613323 -4.5004139 -4.4921203 -4.367013 -4.1439366 -3.9160631 -3.7747045 -3.8155723 -4.0218749 -4.2664475 -4.4167571 -4.4621849 -4.4504418 -4.4191751][-4.4584408 -4.4548659 -4.4790015 -4.4512215 -4.300499 -4.0472054 -3.7872572 -3.6239185 -3.6746392 -3.9038889 -4.1909609 -4.3942647 -4.4692564 -4.4546561 -4.4089003][-4.4204354 -4.44908 -4.50182 -4.4793 -4.3272071 -4.087956 -3.8451264 -3.6828048 -3.712934 -3.9048924 -4.1657815 -4.3675103 -4.4502726 -4.4362035 -4.3778753][-4.3590889 -4.4424858 -4.5399265 -4.5447116 -4.4255352 -4.2515821 -4.0823874 -3.954174 -3.9407103 -4.0407209 -4.2039971 -4.3442926 -4.402976 -4.3833604 -4.31414][-4.3209395 -4.4320259 -4.5539284 -4.5894513 -4.5202522 -4.4239917 -4.34134 -4.2701859 -4.2397213 -4.2602544 -4.3172321 -4.3687315 -4.3789721 -4.3412371 -4.2600846][-4.3219109 -4.4171267 -4.5238714 -4.5759392 -4.5478292 -4.5074081 -4.4941664 -4.4945459 -4.4996347 -4.4978848 -4.4819188 -4.4513416 -4.4047017 -4.3432355 -4.25382][-4.3428345 -4.389255 -4.4492068 -4.4971619 -4.4897861 -4.4652181 -4.4716077 -4.5113897 -4.5650344 -4.5959496 -4.5776491 -4.5198112 -4.4495268 -4.3780551 -4.2848096][-4.3799882 -4.375668 -4.3771763 -4.4042678 -4.4084258 -4.3789778 -4.3595438 -4.3816695 -4.4457455 -4.5208759 -4.5626445 -4.5487618 -4.5013638 -4.4292397 -4.3277655][-4.4583874 -4.4136 -4.3561563 -4.3599176 -4.3861904 -4.3666391 -4.3242178 -4.3130403 -4.3538923 -4.4348283 -4.5166259 -4.5493922 -4.532279 -4.4680047 -4.3701444]]...]
INFO - root - 2017-12-07 06:43:32.356725: step 7510, loss = 2.02, batch loss = 1.94 (11.4 examples/sec; 0.702 sec/batch; 63h:23m:11s remains)
INFO - root - 2017-12-07 06:43:39.466388: step 7520, loss = 2.03, batch loss = 1.95 (11.0 examples/sec; 0.729 sec/batch; 65h:48m:17s remains)
INFO - root - 2017-12-07 06:43:46.524492: step 7530, loss = 2.03, batch loss = 1.95 (10.9 examples/sec; 0.736 sec/batch; 66h:24m:35s remains)
INFO - root - 2017-12-07 06:43:53.618561: step 7540, loss = 2.05, batch loss = 1.96 (11.1 examples/sec; 0.722 sec/batch; 65h:09m:53s remains)
INFO - root - 2017-12-07 06:44:00.667116: step 7550, loss = 2.03, batch loss = 1.95 (12.1 examples/sec; 0.663 sec/batch; 59h:48m:24s remains)
INFO - root - 2017-12-07 06:44:07.687797: step 7560, loss = 2.06, batch loss = 1.98 (11.7 examples/sec; 0.685 sec/batch; 61h:51m:51s remains)
INFO - root - 2017-12-07 06:44:14.693666: step 7570, loss = 2.09, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 63h:22m:52s remains)
INFO - root - 2017-12-07 06:44:21.793358: step 7580, loss = 2.03, batch loss = 1.95 (11.2 examples/sec; 0.711 sec/batch; 64h:11m:36s remains)
INFO - root - 2017-12-07 06:44:28.881472: step 7590, loss = 2.02, batch loss = 1.94 (11.5 examples/sec; 0.698 sec/batch; 62h:57m:15s remains)
INFO - root - 2017-12-07 06:44:35.878988: step 7600, loss = 2.00, batch loss = 1.92 (11.6 examples/sec; 0.690 sec/batch; 62h:16m:26s remains)
2017-12-07 06:44:36.627583: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3277454 -4.3656836 -4.4031525 -4.4378347 -4.4779377 -4.5177617 -4.5559869 -4.5869904 -4.6103339 -4.6262159 -4.6181321 -4.5918679 -4.5713906 -4.5592084 -4.5459185][-4.335187 -4.3850126 -4.4264011 -4.4618368 -4.5056725 -4.5455809 -4.5829906 -4.6175971 -4.6512437 -4.6853795 -4.6853371 -4.6552782 -4.6399665 -4.6399393 -4.6324339][-4.3360057 -4.3919163 -4.430479 -4.4575419 -4.4988427 -4.5343361 -4.5619569 -4.5923057 -4.6324615 -4.6817336 -4.6888304 -4.652463 -4.6365819 -4.6454096 -4.6456532][-4.3386488 -4.3983631 -4.4295878 -4.4389677 -4.4663863 -4.4855924 -4.4905434 -4.5121279 -4.5586772 -4.6181984 -4.6297421 -4.5814309 -4.5463295 -4.5498462 -4.562336][-4.3492913 -4.410696 -4.430419 -4.4111729 -4.4039197 -4.3783956 -4.3384051 -4.3474851 -4.4088435 -4.4891238 -4.5146685 -4.4547224 -4.3796268 -4.3564811 -4.3807945][-4.3677721 -4.431118 -4.4387074 -4.3812981 -4.3135891 -4.2126803 -4.1082473 -4.105371 -4.1980882 -4.3194919 -4.3732586 -4.3069663 -4.1860948 -4.1249127 -4.15905][-4.3879 -4.4520307 -4.4477067 -4.3482251 -4.2037711 -4.0111046 -3.8313637 -3.8126268 -3.9502048 -4.1354604 -4.2380323 -4.17778 -4.023798 -3.9266806 -3.9621165][-4.4047618 -4.4715495 -4.4634948 -4.3330865 -4.1238112 -3.8557236 -3.6107068 -3.5625665 -3.7303307 -3.9765811 -4.1332264 -4.0964317 -3.9345508 -3.8081708 -3.8265052][-4.4139843 -4.4906039 -4.4978051 -4.3698478 -4.1437478 -3.8527341 -3.5798063 -3.4974334 -3.6551952 -3.9209249 -4.10776 -4.0989146 -3.9498672 -3.8047295 -3.7878571][-4.4095159 -4.4957032 -4.5311246 -4.4402604 -4.2518344 -3.9982152 -3.7463703 -3.6459715 -3.7594683 -3.9892573 -4.1694808 -4.1848855 -4.0585327 -3.9024916 -3.8398523][-4.389442 -4.4756136 -4.5340014 -4.4938903 -4.3755445 -4.1977577 -4.0022187 -3.9060025 -3.9700537 -4.1395631 -4.2906632 -4.3190432 -4.2158189 -4.0579791 -3.9548914][-4.3566289 -4.4314241 -4.4952669 -4.4912848 -4.4379005 -4.3387213 -4.2177577 -4.157989 -4.2000756 -4.3171988 -4.4298658 -4.4545164 -4.3730187 -4.2297 -4.1063447][-4.3172164 -4.3764734 -4.4336061 -4.4422569 -4.4243641 -4.3790784 -4.3257756 -4.3207927 -4.3739734 -4.4649367 -4.5421 -4.5507131 -4.4839535 -4.3642864 -4.2416711][-4.2875142 -4.3451328 -4.405179 -4.4173303 -4.4027357 -4.3655519 -4.3387375 -4.3710332 -4.4444861 -4.5271316 -4.5787578 -4.567163 -4.51119 -4.4259472 -4.3294897][-4.27647 -4.349288 -4.4284959 -4.4474692 -4.4170961 -4.3502979 -4.3016582 -4.3335915 -4.4085541 -4.4869537 -4.5273519 -4.5043497 -4.4624481 -4.419487 -4.3642693]]...]
INFO - root - 2017-12-07 06:44:43.656286: step 7610, loss = 2.02, batch loss = 1.94 (10.9 examples/sec; 0.736 sec/batch; 66h:24m:27s remains)
INFO - root - 2017-12-07 06:44:50.599936: step 7620, loss = 2.01, batch loss = 1.93 (11.7 examples/sec; 0.683 sec/batch; 61h:38m:53s remains)
INFO - root - 2017-12-07 06:44:57.662291: step 7630, loss = 1.99, batch loss = 1.91 (12.5 examples/sec; 0.640 sec/batch; 57h:47m:00s remains)
INFO - root - 2017-12-07 06:45:04.747624: step 7640, loss = 2.03, batch loss = 1.95 (11.9 examples/sec; 0.672 sec/batch; 60h:37m:18s remains)
INFO - root - 2017-12-07 06:45:11.800124: step 7650, loss = 2.02, batch loss = 1.94 (11.6 examples/sec; 0.691 sec/batch; 62h:20m:10s remains)
INFO - root - 2017-12-07 06:45:18.865093: step 7660, loss = 1.99, batch loss = 1.90 (11.3 examples/sec; 0.707 sec/batch; 63h:49m:19s remains)
INFO - root - 2017-12-07 06:45:25.948189: step 7670, loss = 2.01, batch loss = 1.92 (12.0 examples/sec; 0.668 sec/batch; 60h:14m:42s remains)
INFO - root - 2017-12-07 06:45:32.956393: step 7680, loss = 2.03, batch loss = 1.95 (11.3 examples/sec; 0.709 sec/batch; 63h:56m:36s remains)
INFO - root - 2017-12-07 06:45:40.037233: step 7690, loss = 2.04, batch loss = 1.95 (10.8 examples/sec; 0.741 sec/batch; 66h:50m:37s remains)
INFO - root - 2017-12-07 06:45:47.006192: step 7700, loss = 2.08, batch loss = 1.99 (11.4 examples/sec; 0.702 sec/batch; 63h:21m:12s remains)
2017-12-07 06:45:47.751812: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5904417 -4.6583161 -4.6880217 -4.7164421 -4.7419224 -4.7390928 -4.7376375 -4.7477026 -4.7521996 -4.7376819 -4.7041035 -4.6449766 -4.554111 -4.44351 -4.3420529][-4.4563842 -4.5670347 -4.6192369 -4.6638618 -4.7033596 -4.7055845 -4.706521 -4.7167616 -4.7124686 -4.6907768 -4.674757 -4.6524825 -4.5868206 -4.4702668 -4.3514194][-4.3335266 -4.4718518 -4.5271831 -4.5561213 -4.5842681 -4.5870256 -4.5897932 -4.5949926 -4.5810642 -4.561614 -4.5777035 -4.608674 -4.5852108 -4.4814472 -4.3608551][-4.3023605 -4.4328208 -4.463131 -4.4552264 -4.4542727 -4.4520779 -4.4457145 -4.4330783 -4.4121323 -4.4074817 -4.4659367 -4.5539155 -4.5801606 -4.511107 -4.4126635][-4.3469663 -4.4265075 -4.401361 -4.3377018 -4.2933345 -4.2769856 -4.2621531 -4.2483678 -4.251678 -4.2838211 -4.3791232 -4.5003676 -4.5509672 -4.5058184 -4.4369793][-4.3718805 -4.3760872 -4.2737656 -4.1459484 -4.0488811 -4.0117731 -3.9998806 -4.0113783 -4.0709105 -4.1547842 -4.2794085 -4.4109335 -4.4580193 -4.40842 -4.3507056][-4.3822603 -4.3305993 -4.1820788 -4.018414 -3.8774905 -3.806165 -3.7861652 -3.8214777 -3.9364219 -4.0734076 -4.2290435 -4.3687429 -4.4127717 -4.351 -4.2806187][-4.406734 -4.3441954 -4.2045255 -4.0464616 -3.8772292 -3.7619238 -3.7140541 -3.749867 -3.8933761 -4.0616703 -4.2286792 -4.3628736 -4.4038177 -4.3412471 -4.2603726][-4.4230609 -4.3883567 -4.2914891 -4.1680574 -3.996474 -3.8465428 -3.7626216 -3.7783797 -3.9243407 -4.1068544 -4.2717566 -4.39223 -4.42808 -4.3761053 -4.2980013][-4.4005537 -4.4041729 -4.36353 -4.29131 -4.1534262 -4.0067129 -3.9145026 -3.9212391 -4.061213 -4.2428517 -4.3941178 -4.4910221 -4.5133786 -4.4651985 -4.3905988][-4.334919 -4.3750963 -4.3914604 -4.3762946 -4.2973638 -4.1952457 -4.1300097 -4.1409955 -4.2535725 -4.3900905 -4.4924903 -4.553875 -4.563323 -4.5219622 -4.4562283][-4.2633085 -4.3328567 -4.3995934 -4.4472528 -4.4424567 -4.4025774 -4.3695755 -4.3737664 -4.4261541 -4.4784255 -4.50754 -4.5279312 -4.5313535 -4.5052843 -4.454905][-4.2036762 -4.2758937 -4.3675137 -4.4571967 -4.50967 -4.5215654 -4.5131412 -4.5080633 -4.510396 -4.4988079 -4.4755349 -4.4639425 -4.4567146 -4.4370337 -4.3986053][-4.1640844 -4.2145782 -4.2904205 -4.3795042 -4.4501524 -4.486393 -4.493556 -4.4860525 -4.4650507 -4.4270988 -4.3853636 -4.359055 -4.34356 -4.3268695 -4.302176][-4.1837158 -4.1997223 -4.2379074 -4.3033667 -4.3638477 -4.3955607 -4.3996162 -4.3829069 -4.3504858 -4.3114219 -4.2775478 -4.257884 -4.248189 -4.2406955 -4.2305455]]...]
INFO - root - 2017-12-07 06:45:54.773421: step 7710, loss = 2.00, batch loss = 1.92 (11.0 examples/sec; 0.729 sec/batch; 65h:48m:32s remains)
INFO - root - 2017-12-07 06:46:01.796774: step 7720, loss = 2.02, batch loss = 1.94 (11.3 examples/sec; 0.710 sec/batch; 64h:01m:46s remains)
INFO - root - 2017-12-07 06:46:08.771503: step 7730, loss = 2.01, batch loss = 1.93 (11.3 examples/sec; 0.706 sec/batch; 63h:40m:17s remains)
INFO - root - 2017-12-07 06:46:15.909438: step 7740, loss = 2.03, batch loss = 1.94 (11.2 examples/sec; 0.714 sec/batch; 64h:24m:12s remains)
INFO - root - 2017-12-07 06:46:22.940570: step 7750, loss = 2.05, batch loss = 1.97 (11.7 examples/sec; 0.686 sec/batch; 61h:54m:00s remains)
INFO - root - 2017-12-07 06:46:30.003468: step 7760, loss = 2.06, batch loss = 1.97 (11.1 examples/sec; 0.720 sec/batch; 64h:55m:10s remains)
INFO - root - 2017-12-07 06:46:36.845700: step 7770, loss = 1.98, batch loss = 1.90 (11.5 examples/sec; 0.695 sec/batch; 62h:42m:32s remains)
INFO - root - 2017-12-07 06:46:43.955444: step 7780, loss = 2.03, batch loss = 1.95 (11.8 examples/sec; 0.679 sec/batch; 61h:13m:47s remains)
INFO - root - 2017-12-07 06:46:50.961786: step 7790, loss = 2.04, batch loss = 1.96 (11.3 examples/sec; 0.711 sec/batch; 64h:06m:57s remains)
INFO - root - 2017-12-07 06:46:58.001819: step 7800, loss = 2.02, batch loss = 1.94 (10.9 examples/sec; 0.733 sec/batch; 66h:07m:09s remains)
2017-12-07 06:46:58.770092: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5336404 -4.5180941 -4.5210953 -4.5379248 -4.5673494 -4.5965281 -4.6111326 -4.5910196 -4.55874 -4.5499821 -4.5550122 -4.564518 -4.5840178 -4.61042 -4.6302447][-4.4496994 -4.44761 -4.4800858 -4.5198913 -4.5685606 -4.610847 -4.6325254 -4.6132841 -4.5739269 -4.5637474 -4.5665326 -4.5619664 -4.5662189 -4.5804296 -4.5846109][-4.3626752 -4.3732872 -4.4363251 -4.5020571 -4.56588 -4.6097622 -4.6286788 -4.6089015 -4.5676231 -4.559226 -4.5594177 -4.5444837 -4.53873 -4.5547276 -4.5641155][-4.3142457 -4.3344154 -4.419332 -4.49681 -4.5536714 -4.57033 -4.5619965 -4.5330486 -4.5012774 -4.508533 -4.5227804 -4.518528 -4.5189209 -4.5492468 -4.5810452][-4.3060412 -4.3400345 -4.431273 -4.5010438 -4.5300369 -4.4973841 -4.4356785 -4.3712831 -4.3470626 -4.3924904 -4.4538336 -4.4919319 -4.5146117 -4.5551553 -4.6043363][-4.3222775 -4.3710566 -4.4548464 -4.5038853 -4.4977627 -4.4053288 -4.2652097 -4.1318173 -4.0996847 -4.1981006 -4.331563 -4.4397154 -4.5060859 -4.5560765 -4.6057653][-4.3374534 -4.3893132 -4.4513826 -4.4665933 -4.4167752 -4.2573428 -4.023315 -3.8040366 -3.7506819 -3.9087214 -4.131835 -4.3290772 -4.4634452 -4.537251 -4.5824866][-4.3810515 -4.4247241 -4.4581494 -4.4390564 -4.3480296 -4.1246939 -3.7987037 -3.4901233 -3.4028482 -3.6123633 -3.9268832 -4.2080894 -4.3970189 -4.4868965 -4.5235319][-4.4437008 -4.4839973 -4.5006762 -4.4669943 -4.3601565 -4.10863 -3.7322979 -3.3701177 -3.2504966 -3.4726868 -3.8358691 -4.15567 -4.356081 -4.4373107 -4.4566121][-4.5188432 -4.5615811 -4.5756946 -4.5491633 -4.4617043 -4.2416496 -3.899405 -3.5640669 -3.4387169 -3.6208975 -3.9556327 -4.2496367 -4.411912 -4.4525943 -4.4356418][-4.5867615 -4.6413851 -4.6557441 -4.63297 -4.5661035 -4.4005327 -4.1363835 -3.8744395 -3.7640569 -3.8890429 -4.1494064 -4.3763127 -4.4815135 -4.4835916 -4.4416232][-4.5929542 -4.6761312 -4.7085686 -4.6949697 -4.6402121 -4.5179772 -4.3296022 -4.1482711 -4.0673933 -4.1447077 -4.3227081 -4.4775777 -4.541429 -4.5291491 -4.4846287][-4.5526648 -4.6732073 -4.7453971 -4.7545352 -4.7082553 -4.6125145 -4.485826 -4.3749852 -4.3319135 -4.380722 -4.4855151 -4.5736904 -4.6059966 -4.590486 -4.5532832][-4.4878426 -4.6240659 -4.7229061 -4.75512 -4.7210636 -4.6456265 -4.5632706 -4.4991961 -4.4762774 -4.4988141 -4.5455513 -4.5827608 -4.5931678 -4.5824928 -4.5637012][-4.4198737 -4.5553079 -4.6596413 -4.701705 -4.6749969 -4.608139 -4.5442467 -4.4979634 -4.4756541 -4.4743743 -4.4835186 -4.4912972 -4.4949374 -4.4966578 -4.4957352]]...]
INFO - root - 2017-12-07 06:47:05.914492: step 7810, loss = 2.00, batch loss = 1.91 (11.7 examples/sec; 0.682 sec/batch; 61h:31m:23s remains)
INFO - root - 2017-12-07 06:47:12.988834: step 7820, loss = 1.98, batch loss = 1.89 (11.7 examples/sec; 0.686 sec/batch; 61h:51m:59s remains)
INFO - root - 2017-12-07 06:47:19.979078: step 7830, loss = 1.99, batch loss = 1.91 (11.5 examples/sec; 0.696 sec/batch; 62h:48m:27s remains)
INFO - root - 2017-12-07 06:47:27.012309: step 7840, loss = 2.00, batch loss = 1.92 (10.6 examples/sec; 0.752 sec/batch; 67h:50m:15s remains)
INFO - root - 2017-12-07 06:47:34.108905: step 7850, loss = 2.01, batch loss = 1.93 (11.2 examples/sec; 0.713 sec/batch; 64h:18m:19s remains)
INFO - root - 2017-12-07 06:47:41.213422: step 7860, loss = 1.99, batch loss = 1.91 (12.4 examples/sec; 0.645 sec/batch; 58h:10m:52s remains)
INFO - root - 2017-12-07 06:47:48.265813: step 7870, loss = 2.00, batch loss = 1.92 (11.8 examples/sec; 0.677 sec/batch; 61h:02m:37s remains)
INFO - root - 2017-12-07 06:47:55.378930: step 7880, loss = 1.98, batch loss = 1.90 (11.0 examples/sec; 0.727 sec/batch; 65h:34m:02s remains)
INFO - root - 2017-12-07 06:48:02.440947: step 7890, loss = 2.06, batch loss = 1.97 (11.5 examples/sec; 0.697 sec/batch; 62h:51m:05s remains)
INFO - root - 2017-12-07 06:48:09.369734: step 7900, loss = 2.03, batch loss = 1.95 (11.1 examples/sec; 0.718 sec/batch; 64h:46m:28s remains)
2017-12-07 06:48:10.068183: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5076265 -4.5333815 -4.5594897 -4.5838957 -4.601222 -4.6010075 -4.58699 -4.560751 -4.5284677 -4.508492 -4.5039964 -4.4997306 -4.4932108 -4.4804864 -4.45356][-4.6813283 -4.71119 -4.7303982 -4.7416544 -4.7506466 -4.7546253 -4.7477479 -4.7163773 -4.6656051 -4.6270313 -4.6043053 -4.5844884 -4.5732055 -4.5643854 -4.5403094][-4.8184967 -4.8403411 -4.8368015 -4.8120937 -4.7937317 -4.8004408 -4.804234 -4.7721472 -4.7164397 -4.6794505 -4.6557822 -4.6331077 -4.6275063 -4.6302676 -4.6162729][-4.8453088 -4.8355975 -4.7959256 -4.7239833 -4.6741672 -4.6911469 -4.7113309 -4.6848369 -4.64406 -4.6364856 -4.6424966 -4.6525311 -4.680059 -4.7036643 -4.6964979][-4.75987 -4.6922393 -4.6029758 -4.48153 -4.4061551 -4.4417648 -4.4830952 -4.4723406 -4.4616079 -4.4897718 -4.5359697 -4.6049018 -4.6907992 -4.7504191 -4.7528205][-4.6477151 -4.51058 -4.3668962 -4.1979351 -4.0943217 -4.1360283 -4.1798191 -4.1828265 -4.2016153 -4.2482085 -4.3193417 -4.4354377 -4.5723271 -4.6759424 -4.7068982][-4.5740376 -4.3853846 -4.2 -3.9916139 -3.8508484 -3.8648012 -3.8716965 -3.879657 -3.9290917 -3.9869938 -4.068861 -4.207068 -4.3745208 -4.5183635 -4.5862479][-4.558938 -4.3682175 -4.1774144 -3.9565663 -3.7803934 -3.7334585 -3.6724536 -3.6688848 -3.7383044 -3.80798 -3.9067652 -4.059576 -4.2467189 -4.415771 -4.5030022][-4.5849218 -4.4487739 -4.3000879 -4.1096473 -3.9305277 -3.8383813 -3.7329855 -3.7118702 -3.7754207 -3.8441446 -3.9542298 -4.1153111 -4.3104095 -4.4750013 -4.5441074][-4.6089978 -4.5472879 -4.4658828 -4.3346519 -4.1891923 -4.0916319 -3.9895315 -3.961534 -4.0083418 -4.0683861 -4.1727133 -4.3206296 -4.49533 -4.6259165 -4.6550746][-4.6345367 -4.6224651 -4.5886965 -4.5109558 -4.4142528 -4.3388038 -4.2620354 -4.2348938 -4.2643409 -4.30981 -4.3890266 -4.5058532 -4.643672 -4.7350345 -4.7311478][-4.6670384 -4.6774931 -4.6596255 -4.6089358 -4.5497832 -4.5010982 -4.4532576 -4.4310403 -4.4419684 -4.4647236 -4.5097489 -4.5897331 -4.686749 -4.744626 -4.7269292][-4.6564507 -4.6793771 -4.6703935 -4.63791 -4.6044884 -4.5788274 -4.5550418 -4.5384212 -4.5337 -4.5356827 -4.5540328 -4.601634 -4.6620889 -4.6959677 -4.6779752][-4.5688806 -4.5965509 -4.5966024 -4.58206 -4.5705032 -4.5648069 -4.5593333 -4.5514636 -4.5450935 -4.5443292 -4.5570879 -4.5885673 -4.6247311 -4.6419582 -4.6233673][-4.4416294 -4.4631562 -4.4679031 -4.4663506 -4.4677119 -4.4709086 -4.472703 -4.4717274 -4.4708624 -4.475728 -4.4912443 -4.5177646 -4.5439968 -4.556354 -4.543469]]...]
INFO - root - 2017-12-07 06:48:17.160631: step 7910, loss = 2.00, batch loss = 1.92 (11.0 examples/sec; 0.730 sec/batch; 65h:49m:13s remains)
INFO - root - 2017-12-07 06:48:24.195469: step 7920, loss = 2.04, batch loss = 1.96 (10.7 examples/sec; 0.748 sec/batch; 67h:26m:32s remains)
INFO - root - 2017-12-07 06:48:31.057837: step 7930, loss = 1.99, batch loss = 1.91 (11.2 examples/sec; 0.712 sec/batch; 64h:11m:41s remains)
INFO - root - 2017-12-07 06:48:38.071242: step 7940, loss = 2.05, batch loss = 1.97 (11.4 examples/sec; 0.699 sec/batch; 63h:02m:02s remains)
INFO - root - 2017-12-07 06:48:45.106940: step 7950, loss = 2.07, batch loss = 1.99 (11.5 examples/sec; 0.696 sec/batch; 62h:44m:26s remains)
INFO - root - 2017-12-07 06:48:52.030384: step 7960, loss = 2.09, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 59h:18m:17s remains)
INFO - root - 2017-12-07 06:48:59.112334: step 7970, loss = 2.00, batch loss = 1.92 (11.4 examples/sec; 0.700 sec/batch; 63h:04m:03s remains)
INFO - root - 2017-12-07 06:49:06.213548: step 7980, loss = 2.07, batch loss = 1.98 (11.2 examples/sec; 0.715 sec/batch; 64h:25m:11s remains)
INFO - root - 2017-12-07 06:49:13.306817: step 7990, loss = 2.06, batch loss = 1.97 (11.3 examples/sec; 0.707 sec/batch; 63h:44m:36s remains)
INFO - root - 2017-12-07 06:49:20.361355: step 8000, loss = 1.99, batch loss = 1.91 (12.0 examples/sec; 0.666 sec/batch; 60h:03m:19s remains)
2017-12-07 06:49:21.148128: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1324739 -4.1216526 -4.1710563 -4.2597694 -4.3309746 -4.3691907 -4.4095612 -4.4463029 -4.4508438 -4.4327283 -4.4394603 -4.4928913 -4.5729876 -4.6269708 -4.6101494][-4.1984773 -4.1955781 -4.2439308 -4.3292966 -4.4077744 -4.4550071 -4.4991007 -4.5349913 -4.526917 -4.4878044 -4.4726658 -4.5084381 -4.5637274 -4.5738873 -4.5044188][-4.3125792 -4.3089418 -4.3449907 -4.4143496 -4.4855008 -4.5249691 -4.5439887 -4.5465426 -4.5078521 -4.4511747 -4.4324465 -4.4658556 -4.4949975 -4.4457645 -4.3089991][-4.3843021 -4.3667574 -4.3789134 -4.4174104 -4.4610124 -4.4649734 -4.4288554 -4.3771687 -4.3106112 -4.26446 -4.2879996 -4.3673239 -4.4073515 -4.32233 -4.1277413][-4.3543696 -4.3101478 -4.2883015 -4.2872157 -4.2941613 -4.2543454 -4.1654706 -4.0853696 -4.0328197 -4.0402727 -4.13697 -4.276401 -4.33821 -4.2434697 -4.0265265][-4.2140427 -4.1747642 -4.1387038 -4.1104164 -4.0909152 -4.021657 -3.9019811 -3.8291404 -3.8267887 -3.8921309 -4.02782 -4.1747761 -4.2276196 -4.1437831 -3.9613187][-4.0549884 -4.0652781 -4.0493474 -4.0179749 -3.990345 -3.9163837 -3.791429 -3.7422945 -3.7862778 -3.8699977 -3.9810102 -4.0783134 -4.0962276 -4.0345135 -3.920845][-3.9578781 -4.009995 -4.0211177 -3.9957724 -3.9758568 -3.9308498 -3.8430471 -3.8333969 -3.8998754 -3.9545562 -3.9943476 -4.0147429 -3.9892511 -3.9479961 -3.9066126][-3.9436879 -4.0182972 -4.052135 -4.0386729 -4.0296407 -4.0203476 -3.9819286 -4.0029597 -4.060576 -4.0594816 -4.0228977 -3.9749827 -3.9177868 -3.8970189 -3.9131095][-4.01181 -4.0919838 -4.1460762 -4.1586189 -4.1669884 -4.1777377 -4.1623092 -4.1818118 -4.208446 -4.1620293 -4.0770397 -3.9851544 -3.9078012 -3.896369 -3.9378202][-4.1163325 -4.1715388 -4.2258267 -4.263741 -4.2991767 -4.3282862 -4.3239055 -4.3243189 -4.3100286 -4.2320251 -4.1215835 -4.0038443 -3.9141269 -3.9035072 -3.9533677][-4.262064 -4.2719617 -4.2984476 -4.3359776 -4.3830867 -4.4230156 -4.4284768 -4.4169369 -4.3783703 -4.2923908 -4.1793647 -4.0510774 -3.9480739 -3.9213715 -3.9621119][-4.4145212 -4.3765054 -4.3577681 -4.3650646 -4.3947105 -4.4302254 -4.4496164 -4.4533005 -4.431334 -4.3773737 -4.2960238 -4.1796227 -4.06344 -3.9994617 -4.0056872][-4.5054617 -4.4229517 -4.3565183 -4.3184919 -4.3107929 -4.3342295 -4.3771248 -4.4224687 -4.4469161 -4.4472227 -4.4187384 -4.3356838 -4.2205358 -4.1236434 -4.0839052][-4.5267491 -4.4366536 -4.3539939 -4.285718 -4.2424822 -4.2470546 -4.30366 -4.3830256 -4.4452734 -4.4860406 -4.4989424 -4.452229 -4.3543954 -4.24927 -4.1759057]]...]
INFO - root - 2017-12-07 06:49:28.173363: step 8010, loss = 2.03, batch loss = 1.94 (11.2 examples/sec; 0.714 sec/batch; 64h:21m:38s remains)
INFO - root - 2017-12-07 06:49:35.158413: step 8020, loss = 2.00, batch loss = 1.91 (11.4 examples/sec; 0.699 sec/batch; 63h:02m:09s remains)
INFO - root - 2017-12-07 06:49:42.114754: step 8030, loss = 2.00, batch loss = 1.91 (12.4 examples/sec; 0.646 sec/batch; 58h:11m:14s remains)
INFO - root - 2017-12-07 06:49:49.112239: step 8040, loss = 2.00, batch loss = 1.91 (12.5 examples/sec; 0.639 sec/batch; 57h:35m:08s remains)
INFO - root - 2017-12-07 06:49:56.182259: step 8050, loss = 2.03, batch loss = 1.95 (11.5 examples/sec; 0.698 sec/batch; 62h:55m:40s remains)
INFO - root - 2017-12-07 06:50:03.215020: step 8060, loss = 2.03, batch loss = 1.95 (11.6 examples/sec; 0.689 sec/batch; 62h:04m:14s remains)
INFO - root - 2017-12-07 06:50:10.363853: step 8070, loss = 2.05, batch loss = 1.97 (10.9 examples/sec; 0.733 sec/batch; 66h:04m:55s remains)
INFO - root - 2017-12-07 06:50:17.279899: step 8080, loss = 2.07, batch loss = 1.98 (11.2 examples/sec; 0.714 sec/batch; 64h:20m:35s remains)
INFO - root - 2017-12-07 06:50:24.377310: step 8090, loss = 2.03, batch loss = 1.95 (10.9 examples/sec; 0.736 sec/batch; 66h:17m:38s remains)
INFO - root - 2017-12-07 06:50:31.415468: step 8100, loss = 2.02, batch loss = 1.94 (11.0 examples/sec; 0.728 sec/batch; 65h:33m:33s remains)
2017-12-07 06:50:32.190135: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6174793 -4.6974249 -4.7569308 -4.7739086 -4.7410622 -4.6874256 -4.648962 -4.6298428 -4.6215758 -4.635859 -4.6607814 -4.6732974 -4.65729 -4.6154294 -4.5514574][-4.7006235 -4.812211 -4.8834043 -4.8847847 -4.8102245 -4.7006907 -4.6231685 -4.5949278 -4.5972071 -4.6344185 -4.6900077 -4.7336011 -4.7349582 -4.6961613 -4.6257663][-4.7222714 -4.855001 -4.9226069 -4.8955188 -4.7733693 -4.5995421 -4.4697266 -4.4307537 -4.4571428 -4.5360885 -4.6354413 -4.7208338 -4.7479982 -4.7211561 -4.6579742][-4.6878843 -4.8128748 -4.8509693 -4.7833614 -4.6239538 -4.4038038 -4.2288122 -4.1795049 -4.2347016 -4.3577762 -4.4957209 -4.6197324 -4.6825309 -4.6833992 -4.6390805][-4.6274195 -4.7191019 -4.7049365 -4.5896339 -4.4054923 -4.1692276 -3.9759676 -3.9245734 -4.0048409 -4.155488 -4.3073077 -4.4475455 -4.549057 -4.5951562 -4.5782986][-4.5612192 -4.6102152 -4.5448184 -4.3906317 -4.1958451 -3.9686635 -3.7721229 -3.7122412 -3.8051622 -3.9673519 -4.1137123 -4.2529306 -4.3870678 -4.484354 -4.4990191][-4.5183024 -4.54407 -4.4577012 -4.28941 -4.0907764 -3.8676596 -3.6552699 -3.5680239 -3.6584313 -3.8261278 -3.9702451 -4.1123486 -4.2697892 -4.3987017 -4.4357543][-4.53168 -4.5700889 -4.5025773 -4.3472576 -4.1478348 -3.9083076 -3.6626601 -3.5347922 -3.6187859 -3.8024516 -3.9659326 -4.1211176 -4.2786889 -4.3966784 -4.4278893][-4.5844674 -4.66042 -4.6390953 -4.5213161 -4.3362989 -4.0828762 -3.805706 -3.643245 -3.7223997 -3.9330454 -4.1288185 -4.2944746 -4.4242783 -4.4928217 -4.4887319][-4.6307726 -4.7364869 -4.7653871 -4.7005944 -4.5495186 -4.3090982 -4.0350261 -3.8668046 -3.9374552 -4.1515174 -4.3583732 -4.5184503 -4.6136417 -4.6337023 -4.5888748][-4.635354 -4.7513709 -4.816668 -4.803987 -4.7040243 -4.5146832 -4.29184 -4.1547804 -4.2122383 -4.3987923 -4.580205 -4.7042017 -4.7583704 -4.7463017 -4.6746721][-4.5941553 -4.7006035 -4.7817106 -4.8098116 -4.7667279 -4.6510305 -4.5065141 -4.4172611 -4.4598212 -4.6022377 -4.7327218 -4.801929 -4.8155503 -4.784986 -4.703155][-4.5308876 -4.6147585 -4.6908979 -4.7352304 -4.7306395 -4.6764703 -4.6029978 -4.5578809 -4.585948 -4.6759739 -4.7471962 -4.770978 -4.765882 -4.7357416 -4.6631832][-4.4809184 -4.5393896 -4.59594 -4.631073 -4.6366568 -4.6178632 -4.5927916 -4.5796852 -4.5981016 -4.643415 -4.6687417 -4.6675925 -4.6594181 -4.6383276 -4.5843096][-4.4399724 -4.4733944 -4.5072818 -4.5259461 -4.5286527 -4.5236044 -4.52101 -4.5223613 -4.532424 -4.5492077 -4.5552082 -4.5514021 -4.5463338 -4.5317116 -4.495244]]...]
INFO - root - 2017-12-07 06:50:39.158901: step 8110, loss = 1.97, batch loss = 1.89 (11.5 examples/sec; 0.693 sec/batch; 62h:29m:09s remains)
INFO - root - 2017-12-07 06:50:46.209082: step 8120, loss = 1.97, batch loss = 1.89 (11.1 examples/sec; 0.721 sec/batch; 64h:57m:42s remains)
INFO - root - 2017-12-07 06:50:53.197919: step 8130, loss = 2.04, batch loss = 1.96 (11.5 examples/sec; 0.693 sec/batch; 62h:27m:18s remains)
INFO - root - 2017-12-07 06:51:00.227113: step 8140, loss = 1.95, batch loss = 1.86 (11.2 examples/sec; 0.713 sec/batch; 64h:17m:10s remains)
INFO - root - 2017-12-07 06:51:07.240698: step 8150, loss = 1.97, batch loss = 1.89 (11.8 examples/sec; 0.677 sec/batch; 61h:01m:11s remains)
INFO - root - 2017-12-07 06:51:14.260285: step 8160, loss = 2.04, batch loss = 1.95 (11.6 examples/sec; 0.688 sec/batch; 62h:00m:14s remains)
INFO - root - 2017-12-07 06:51:21.274903: step 8170, loss = 2.01, batch loss = 1.93 (11.1 examples/sec; 0.722 sec/batch; 65h:01m:35s remains)
INFO - root - 2017-12-07 06:51:28.311798: step 8180, loss = 2.00, batch loss = 1.92 (11.3 examples/sec; 0.711 sec/batch; 64h:03m:26s remains)
INFO - root - 2017-12-07 06:51:35.355452: step 8190, loss = 1.96, batch loss = 1.88 (11.6 examples/sec; 0.687 sec/batch; 61h:53m:49s remains)
INFO - root - 2017-12-07 06:51:42.369998: step 8200, loss = 2.04, batch loss = 1.95 (12.0 examples/sec; 0.668 sec/batch; 60h:13m:08s remains)
2017-12-07 06:51:43.268593: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4401546 -4.4621515 -4.4674754 -4.486475 -4.520298 -4.5465465 -4.5434909 -4.5179887 -4.5141215 -4.5314908 -4.5353651 -4.5081964 -4.4771538 -4.4773517 -4.5028024][-4.493135 -4.4931264 -4.4789038 -4.4959579 -4.5317788 -4.5567236 -4.5495067 -4.5117483 -4.4999309 -4.523386 -4.533814 -4.4967275 -4.4518943 -4.4501567 -4.483645][-4.5487394 -4.5256386 -4.483922 -4.476079 -4.4817882 -4.4785171 -4.4560776 -4.4163065 -4.4138694 -4.4547663 -4.4759755 -4.4294987 -4.364573 -4.3472857 -4.3797078][-4.5907669 -4.5472121 -4.48617 -4.4539838 -4.4275408 -4.3945017 -4.3605132 -4.3265705 -4.3360286 -4.3900752 -4.4121833 -4.3541451 -4.27073 -4.2384558 -4.2693586][-4.573689 -4.5135283 -4.4459543 -4.4057126 -4.3687878 -4.3194966 -4.2720709 -4.2337203 -4.2449651 -4.3004236 -4.3181019 -4.2634063 -4.1872749 -4.157279 -4.1912069][-4.4593139 -4.3913531 -4.3286347 -4.2996321 -4.2696776 -4.2100754 -4.1373534 -4.0735168 -4.06936 -4.1230264 -4.1540127 -4.1381292 -4.1099606 -4.1095824 -4.1559243][-4.2696753 -4.2025771 -4.1552334 -4.149313 -4.138751 -4.0812626 -3.9836719 -3.8810818 -3.8515458 -3.9043553 -3.9648521 -4.0097461 -4.0509114 -4.0946441 -4.1608419][-4.0912862 -4.0301027 -4.0068774 -4.0368156 -4.0640984 -4.035285 -3.9368267 -3.8086064 -3.7565603 -3.8033965 -3.8888996 -3.9812944 -4.0648923 -4.125371 -4.1905603][-4.0498686 -3.997642 -3.9897797 -4.0464358 -4.1087885 -4.1163578 -4.0280008 -3.882755 -3.8057208 -3.843086 -3.9461243 -4.0637956 -4.1557779 -4.2013721 -4.2380953][-4.2136755 -4.1609354 -4.1409221 -4.1886744 -4.2588143 -4.2832932 -4.1959062 -4.0386324 -3.9414468 -3.9712124 -4.080091 -4.1971006 -4.2787724 -4.30421 -4.3079758][-4.4236937 -4.3720026 -4.3349438 -4.36539 -4.4346118 -4.4687462 -4.3948216 -4.2482634 -4.1446047 -4.160615 -4.2464037 -4.3301034 -4.3833089 -4.3915319 -4.3797522][-4.5441194 -4.4963942 -4.4603324 -4.4867706 -4.5540385 -4.5924411 -4.541697 -4.4303679 -4.3454123 -4.354784 -4.412137 -4.4565053 -4.4742537 -4.4647903 -4.4479227][-4.5815387 -4.5461221 -4.5296912 -4.5630007 -4.62199 -4.6516309 -4.6149526 -4.537425 -4.4804811 -4.488358 -4.5232077 -4.5452304 -4.5455017 -4.528409 -4.5103068][-4.5666389 -4.5555253 -4.5665817 -4.6096711 -4.6582227 -4.6769738 -4.6474271 -4.5935106 -4.5556617 -4.5567508 -4.5717535 -4.5806723 -4.5761614 -4.5580111 -4.538496][-4.5135446 -4.5252714 -4.5533214 -4.5936861 -4.6275296 -4.6376467 -4.6159878 -4.5814323 -4.5590968 -4.556067 -4.55744 -4.5562172 -4.5482645 -4.5312924 -4.5103426]]...]
INFO - root - 2017-12-07 06:51:50.270184: step 8210, loss = 2.04, batch loss = 1.95 (11.3 examples/sec; 0.709 sec/batch; 63h:53m:52s remains)
INFO - root - 2017-12-07 06:51:57.332649: step 8220, loss = 2.03, batch loss = 1.95 (11.5 examples/sec; 0.696 sec/batch; 62h:41m:03s remains)
INFO - root - 2017-12-07 06:52:04.358901: step 8230, loss = 2.04, batch loss = 1.96 (12.4 examples/sec; 0.644 sec/batch; 58h:01m:50s remains)
INFO - root - 2017-12-07 06:52:11.347780: step 8240, loss = 2.01, batch loss = 1.93 (11.6 examples/sec; 0.692 sec/batch; 62h:17m:55s remains)
INFO - root - 2017-12-07 06:52:18.196135: step 8250, loss = 2.01, batch loss = 1.93 (11.2 examples/sec; 0.716 sec/batch; 64h:31m:09s remains)
INFO - root - 2017-12-07 06:52:25.083530: step 8260, loss = 2.02, batch loss = 1.94 (10.7 examples/sec; 0.748 sec/batch; 67h:22m:07s remains)
INFO - root - 2017-12-07 06:52:32.156905: step 8270, loss = 2.04, batch loss = 1.95 (11.4 examples/sec; 0.702 sec/batch; 63h:11m:40s remains)
INFO - root - 2017-12-07 06:52:39.202979: step 8280, loss = 2.04, batch loss = 1.96 (11.7 examples/sec; 0.685 sec/batch; 61h:39m:20s remains)
INFO - root - 2017-12-07 06:52:46.292406: step 8290, loss = 2.02, batch loss = 1.94 (11.5 examples/sec; 0.694 sec/batch; 62h:29m:50s remains)
INFO - root - 2017-12-07 06:52:53.409168: step 8300, loss = 2.01, batch loss = 1.93 (11.1 examples/sec; 0.720 sec/batch; 64h:49m:09s remains)
2017-12-07 06:52:54.156449: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3288569 -4.3354034 -4.34111 -4.3515763 -4.3853 -4.412643 -4.4201117 -4.4281049 -4.4414439 -4.4616165 -4.492166 -4.5105362 -4.4995275 -4.4593811 -4.385139][-4.2759409 -4.2843094 -4.3076925 -4.3399992 -4.3931217 -4.4324536 -4.4429178 -4.4538641 -4.4679904 -4.4818506 -4.5001407 -4.4974523 -4.4588571 -4.3934407 -4.3078852][-4.2245221 -4.2358203 -4.26657 -4.3052216 -4.3627834 -4.4034061 -4.4109259 -4.4199104 -4.4294076 -4.4356031 -4.4459453 -4.4340191 -4.3832784 -4.3070111 -4.2242246][-4.2240176 -4.243927 -4.268343 -4.2908731 -4.333508 -4.3636332 -4.3645477 -4.3709412 -4.3735209 -4.3716941 -4.382988 -4.3812652 -4.3380761 -4.2602096 -4.1812243][-4.28275 -4.3133421 -4.324585 -4.3203659 -4.33804 -4.3520761 -4.3473897 -4.356607 -4.3581629 -4.3541222 -4.3729129 -4.3921952 -4.3637929 -4.2816138 -4.1951694][-4.3125806 -4.3554964 -4.3602071 -4.3341188 -4.3218136 -4.3062739 -4.2818279 -4.2835593 -4.2839036 -4.2878666 -4.3299656 -4.3872271 -4.3911443 -4.3159776 -4.2235966][-4.2642665 -4.3253212 -4.3437834 -4.3206306 -4.2849631 -4.2223725 -4.15109 -4.1238804 -4.1173019 -4.140161 -4.2213206 -4.3244915 -4.3736768 -4.3199329 -4.2279191][-4.1751904 -4.25152 -4.2909265 -4.2891455 -4.243001 -4.1296129 -3.9991693 -3.933733 -3.9196889 -3.9632618 -4.0817795 -4.2238846 -4.3186874 -4.2978644 -4.2122378][-4.0991979 -4.1821351 -4.2379603 -4.2628965 -4.2209616 -4.0690155 -3.882669 -3.777422 -3.7574484 -3.8184254 -3.968204 -4.1426234 -4.274714 -4.2885032 -4.2125144][-4.076283 -4.1576138 -4.2194452 -4.2671275 -4.2493863 -4.0958519 -3.8887486 -3.7646925 -3.7488351 -3.8250713 -3.9842598 -4.1587973 -4.2846322 -4.2999649 -4.2208066][-4.100903 -4.170486 -4.2292972 -4.2848754 -4.2954011 -4.1742592 -3.9915283 -3.8787456 -3.8756642 -3.9578252 -4.1008005 -4.2426257 -4.3265214 -4.3183489 -4.2306337][-4.1378965 -4.1993475 -4.2621756 -4.323741 -4.3572812 -4.2813964 -4.1453438 -4.0581369 -4.0631332 -4.1368184 -4.2469478 -4.3402047 -4.37314 -4.3333645 -4.2384176][-4.2139087 -4.2727046 -4.3463092 -4.4155078 -4.4609509 -4.4285617 -4.3482223 -4.2949677 -4.2989764 -4.3391428 -4.3885479 -4.4150958 -4.3980923 -4.3338795 -4.2408581][-4.2793975 -4.3370824 -4.4188113 -4.4957047 -4.54742 -4.5503755 -4.5226469 -4.5022469 -4.5028524 -4.5030437 -4.4893374 -4.4579792 -4.4113913 -4.3464127 -4.2748313][-4.2802978 -4.3360853 -4.4236803 -4.5135388 -4.57746 -4.60565 -4.6133194 -4.6093955 -4.6000309 -4.5780258 -4.5395427 -4.4901872 -4.4387784 -4.3810945 -4.3260684]]...]
INFO - root - 2017-12-07 06:53:01.235088: step 8310, loss = 2.01, batch loss = 1.93 (11.6 examples/sec; 0.689 sec/batch; 62h:00m:56s remains)
INFO - root - 2017-12-07 06:53:08.419844: step 8320, loss = 1.99, batch loss = 1.91 (10.6 examples/sec; 0.755 sec/batch; 67h:57m:18s remains)
INFO - root - 2017-12-07 06:53:15.605124: step 8330, loss = 2.03, batch loss = 1.95 (10.7 examples/sec; 0.748 sec/batch; 67h:21m:22s remains)
INFO - root - 2017-12-07 06:53:22.781929: step 8340, loss = 2.01, batch loss = 1.93 (10.9 examples/sec; 0.734 sec/batch; 66h:04m:46s remains)
INFO - root - 2017-12-07 06:53:29.871828: step 8350, loss = 2.09, batch loss = 2.00 (10.7 examples/sec; 0.748 sec/batch; 67h:22m:24s remains)
INFO - root - 2017-12-07 06:53:36.902630: step 8360, loss = 2.02, batch loss = 1.94 (10.8 examples/sec; 0.744 sec/batch; 66h:59m:08s remains)
INFO - root - 2017-12-07 06:53:43.923749: step 8370, loss = 2.02, batch loss = 1.93 (11.1 examples/sec; 0.723 sec/batch; 65h:03m:50s remains)
INFO - root - 2017-12-07 06:53:50.942309: step 8380, loss = 2.06, batch loss = 1.98 (11.1 examples/sec; 0.720 sec/batch; 64h:49m:05s remains)
INFO - root - 2017-12-07 06:53:58.031529: step 8390, loss = 1.99, batch loss = 1.90 (11.9 examples/sec; 0.673 sec/batch; 60h:35m:52s remains)
INFO - root - 2017-12-07 06:54:05.093014: step 8400, loss = 2.03, batch loss = 1.95 (11.0 examples/sec; 0.730 sec/batch; 65h:45m:05s remains)
2017-12-07 06:54:05.881853: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4850354 -4.5414224 -4.5371847 -4.4644818 -4.3597336 -4.2818317 -4.248847 -4.2343903 -4.2538123 -4.3364682 -4.4686704 -4.567883 -4.5686975 -4.4835582 -4.3717179][-4.47751 -4.5548849 -4.5921717 -4.5584884 -4.470993 -4.3782425 -4.3103085 -4.2597618 -4.2400241 -4.2864294 -4.40367 -4.525631 -4.574193 -4.5315056 -4.4355669][-4.4579535 -4.54021 -4.5912189 -4.5682883 -4.4832587 -4.3865242 -4.3191729 -4.274797 -4.2458167 -4.2631321 -4.3517017 -4.472939 -4.5593257 -4.5677862 -4.5021138][-4.4344611 -4.5084095 -4.5544395 -4.526464 -4.4377823 -4.342721 -4.2891121 -4.2677264 -4.2451749 -4.24171 -4.3011174 -4.4084706 -4.5216794 -4.5817666 -4.5586104][-4.4134746 -4.4773445 -4.5143938 -4.4776073 -4.3798261 -4.2764974 -4.2121258 -4.1837878 -4.1511784 -4.1338205 -4.1849952 -4.2942796 -4.4391332 -4.554152 -4.5848947][-4.3971772 -4.4501152 -4.4706373 -4.4143414 -4.2916255 -4.157937 -4.0512075 -3.9829631 -3.9271536 -3.9172745 -4.0035968 -4.1509395 -4.336483 -4.4986959 -4.5802741][-4.3839483 -4.4231644 -4.4239092 -4.3461418 -4.1996365 -4.0351005 -3.8855033 -3.773797 -3.698698 -3.7205343 -3.8728924 -4.0775695 -4.2918258 -4.4722137 -4.5801711][-4.37899 -4.408339 -4.4011 -4.3238683 -4.1840277 -4.0194345 -3.8570871 -3.7227559 -3.6416965 -3.6976638 -3.901396 -4.1394067 -4.3515711 -4.5146728 -4.6174307][-4.3933778 -4.427073 -4.4336567 -4.3838162 -4.2799211 -4.143414 -3.9969375 -3.8628578 -3.7864165 -3.8577783 -4.0659623 -4.2919955 -4.4717031 -4.5974941 -4.6747494][-4.4250054 -4.4741092 -4.5060487 -4.4922881 -4.4292407 -4.3283195 -4.20927 -4.0885215 -4.0213604 -4.0850253 -4.2582 -4.4436793 -4.5821509 -4.6698985 -4.7136555][-4.451004 -4.5140023 -4.5673184 -4.58375 -4.5532236 -4.4818788 -4.3864751 -4.2830496 -4.2305632 -4.2825279 -4.415411 -4.5591459 -4.6596851 -4.7089491 -4.7090859][-4.4494629 -4.5166287 -4.5819921 -4.6217046 -4.6180754 -4.5728712 -4.5003896 -4.4219022 -4.3929706 -4.442616 -4.5481892 -4.6563783 -4.7166944 -4.718729 -4.6636982][-4.4211855 -4.4823484 -4.5514317 -4.6067324 -4.6265926 -4.6080551 -4.5654159 -4.52386 -4.5257235 -4.5801873 -4.663651 -4.7325792 -4.7434878 -4.6905179 -4.5859752][-4.38175 -4.4324965 -4.4985881 -4.5597234 -4.594306 -4.59809 -4.5877814 -4.5866213 -4.6151128 -4.6661911 -4.7173738 -4.7354383 -4.6945252 -4.60292 -4.484468][-4.34364 -4.3804469 -4.4342322 -4.4884892 -4.5275545 -4.549005 -4.5671415 -4.5944529 -4.6304545 -4.6599255 -4.6656265 -4.6314483 -4.556942 -4.4661222 -4.380506]]...]
INFO - root - 2017-12-07 06:54:12.813621: step 8410, loss = 2.01, batch loss = 1.92 (11.7 examples/sec; 0.686 sec/batch; 61h:43m:17s remains)
INFO - root - 2017-12-07 06:54:19.811776: step 8420, loss = 1.98, batch loss = 1.90 (12.0 examples/sec; 0.667 sec/batch; 60h:01m:24s remains)
INFO - root - 2017-12-07 06:54:26.764865: step 8430, loss = 2.00, batch loss = 1.92 (11.4 examples/sec; 0.703 sec/batch; 63h:14m:36s remains)
INFO - root - 2017-12-07 06:54:33.740226: step 8440, loss = 2.04, batch loss = 1.96 (10.9 examples/sec; 0.732 sec/batch; 65h:53m:47s remains)
INFO - root - 2017-12-07 06:54:40.723099: step 8450, loss = 1.94, batch loss = 1.86 (11.2 examples/sec; 0.717 sec/batch; 64h:33m:26s remains)
INFO - root - 2017-12-07 06:54:47.777723: step 8460, loss = 2.04, batch loss = 1.96 (10.7 examples/sec; 0.745 sec/batch; 67h:04m:27s remains)
INFO - root - 2017-12-07 06:54:54.833158: step 8470, loss = 2.02, batch loss = 1.94 (11.6 examples/sec; 0.691 sec/batch; 62h:13m:25s remains)
INFO - root - 2017-12-07 06:55:01.761735: step 8480, loss = 2.05, batch loss = 1.97 (11.2 examples/sec; 0.715 sec/batch; 64h:20m:52s remains)
INFO - root - 2017-12-07 06:55:08.752815: step 8490, loss = 2.03, batch loss = 1.95 (11.7 examples/sec; 0.684 sec/batch; 61h:32m:04s remains)
INFO - root - 2017-12-07 06:55:15.943410: step 8500, loss = 2.02, batch loss = 1.94 (11.5 examples/sec; 0.694 sec/batch; 62h:29m:57s remains)
2017-12-07 06:55:16.659309: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4777079 -4.4731231 -4.459836 -4.4420619 -4.4263906 -4.4186621 -4.4182644 -4.4198871 -4.4180822 -4.4147921 -4.4157362 -4.4237604 -4.438302 -4.4553204 -4.4694109][-4.4654627 -4.4600091 -4.443563 -4.4233742 -4.4072509 -4.3986063 -4.3965254 -4.3945289 -4.386198 -4.3745046 -4.3655663 -4.3654504 -4.3767109 -4.3951902 -4.4131689][-4.4780364 -4.4680839 -4.4455733 -4.4222245 -4.40934 -4.4085979 -4.4151382 -4.416656 -4.4059925 -4.3891196 -4.3731852 -4.3669424 -4.3752785 -4.3935957 -4.4135008][-4.4540334 -4.4311461 -4.3925171 -4.3561621 -4.3429527 -4.3566213 -4.3857222 -4.4049454 -4.4033442 -4.3905048 -4.3753152 -4.369391 -4.377738 -4.396595 -4.420011][-4.3965588 -4.3505874 -4.2821317 -4.2155604 -4.1880302 -4.2096157 -4.2633162 -4.3051023 -4.3156452 -4.3071179 -4.2937222 -4.2915912 -4.3031011 -4.3254361 -4.3564472][-4.3282585 -4.257411 -4.1538072 -4.0524492 -4.0043745 -4.0251632 -4.0933676 -4.1480618 -4.1617556 -4.14753 -4.1305962 -4.1338639 -4.153213 -4.1837296 -4.2252469][-4.2654967 -4.1766405 -4.0523415 -3.9370832 -3.8807783 -3.9011209 -3.9716885 -4.02186 -4.0245996 -3.9925578 -3.9649494 -3.973731 -4.0032573 -4.0422683 -4.0915923][-4.2087717 -4.1131058 -3.9947877 -3.9003251 -3.8623569 -3.8957851 -3.9661684 -4.0042052 -3.9929991 -3.9441366 -3.9056306 -3.9164197 -3.9506714 -3.9918513 -4.0412531][-4.2096691 -4.1223726 -4.0247893 -3.9636147 -3.9526298 -4.0024157 -4.070056 -4.0970736 -4.08138 -4.0302954 -3.9880874 -3.9949985 -4.0231614 -4.0563345 -4.09847][-4.2976975 -4.2311239 -4.1576138 -4.1238041 -4.131196 -4.1857471 -4.2420759 -4.2583103 -4.2446837 -4.2043934 -4.1683903 -4.1683016 -4.1825676 -4.2022738 -4.2321429][-4.424161 -4.3838329 -4.3358393 -4.3194079 -4.3327541 -4.3796792 -4.4191232 -4.4255309 -4.4148531 -4.3899336 -4.3674326 -4.3648067 -4.368639 -4.3764019 -4.3911037][-4.5310826 -4.5107369 -4.4842892 -4.4777403 -4.4876537 -4.514956 -4.5306807 -4.5234365 -4.5101147 -4.494843 -4.4840207 -4.4827862 -4.4832754 -4.4848061 -4.4883914][-4.579442 -4.5687413 -4.5568457 -4.5533791 -4.55342 -4.5570326 -4.5492277 -4.5288806 -4.5095115 -4.4962025 -4.4896789 -4.4889412 -4.4886227 -4.4882097 -4.4879646][-4.5477266 -4.5415521 -4.5360918 -4.531796 -4.5232348 -4.5110378 -4.4909358 -4.4663243 -4.4456544 -4.4325042 -4.4262853 -4.4248295 -4.423439 -4.4218588 -4.4219017][-4.4809813 -4.4735122 -4.4664145 -4.4580188 -4.4471412 -4.4348059 -4.41967 -4.4033489 -4.3884826 -4.3771586 -4.3700418 -4.3665218 -4.3624849 -4.3586411 -4.3589015]]...]
INFO - root - 2017-12-07 06:55:23.746792: step 8510, loss = 2.07, batch loss = 1.99 (11.1 examples/sec; 0.724 sec/batch; 65h:08m:42s remains)
INFO - root - 2017-12-07 06:55:30.881533: step 8520, loss = 2.01, batch loss = 1.93 (11.1 examples/sec; 0.720 sec/batch; 64h:45m:09s remains)
INFO - root - 2017-12-07 06:55:37.868574: step 8530, loss = 2.03, batch loss = 1.95 (10.9 examples/sec; 0.734 sec/batch; 66h:05m:53s remains)
INFO - root - 2017-12-07 06:55:44.905330: step 8540, loss = 1.97, batch loss = 1.88 (11.4 examples/sec; 0.700 sec/batch; 63h:02m:05s remains)
INFO - root - 2017-12-07 06:55:52.040876: step 8550, loss = 1.97, batch loss = 1.89 (10.9 examples/sec; 0.734 sec/batch; 66h:01m:17s remains)
INFO - root - 2017-12-07 06:55:59.156854: step 8560, loss = 2.00, batch loss = 1.91 (11.2 examples/sec; 0.717 sec/batch; 64h:28m:57s remains)
INFO - root - 2017-12-07 06:56:06.346073: step 8570, loss = 2.00, batch loss = 1.92 (11.4 examples/sec; 0.701 sec/batch; 63h:04m:11s remains)
INFO - root - 2017-12-07 06:56:13.381460: step 8580, loss = 2.05, batch loss = 1.96 (12.1 examples/sec; 0.659 sec/batch; 59h:19m:28s remains)
INFO - root - 2017-12-07 06:56:20.475144: step 8590, loss = 2.01, batch loss = 1.92 (11.3 examples/sec; 0.706 sec/batch; 63h:29m:25s remains)
INFO - root - 2017-12-07 06:56:27.539292: step 8600, loss = 2.00, batch loss = 1.92 (11.0 examples/sec; 0.730 sec/batch; 65h:42m:58s remains)
2017-12-07 06:56:28.325271: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5147586 -4.5239782 -4.5337338 -4.5472689 -4.5528927 -4.5477123 -4.535388 -4.5239787 -4.5165367 -4.5103154 -4.502954 -4.489162 -4.4693832 -4.4272923 -4.35956][-4.5580034 -4.5594158 -4.5745993 -4.6034951 -4.6219449 -4.6195707 -4.6028504 -4.5903559 -4.5871644 -4.5845594 -4.5766873 -4.5563645 -4.5258389 -4.4744258 -4.3993368][-4.5561657 -4.5467343 -4.5735979 -4.6284323 -4.6660213 -4.6620808 -4.6346025 -4.623436 -4.637013 -4.6510015 -4.6476359 -4.6202064 -4.5768867 -4.520082 -4.448349][-4.4953308 -4.4760303 -4.5103292 -4.5812588 -4.6209383 -4.5916033 -4.5350432 -4.527678 -4.5769629 -4.6273079 -4.6387267 -4.6094422 -4.5597782 -4.5079985 -4.4550848][-4.39643 -4.37324 -4.4114289 -4.4827766 -4.4995003 -4.4152966 -4.29984 -4.2807436 -4.3757281 -4.4865594 -4.5387654 -4.5290337 -4.4875631 -4.4469132 -4.4154229][-4.2920976 -4.2792416 -4.3266778 -4.3951139 -4.3813734 -4.2308578 -4.0385575 -3.9870267 -4.1188993 -4.2963076 -4.4079757 -4.4381738 -4.4151425 -4.3822746 -4.3625531][-4.2370772 -4.2270632 -4.2672334 -4.3195114 -4.276536 -4.0756598 -3.8206108 -3.7311749 -3.8810034 -4.109787 -4.2776566 -4.3547659 -4.35675 -4.3288507 -4.3088694][-4.2076421 -4.1829185 -4.1999168 -4.2387829 -4.1984992 -4.0017414 -3.7345257 -3.619576 -3.7630446 -4.015965 -4.2291613 -4.3498926 -4.3747025 -4.343503 -4.3037205][-4.1700435 -4.1362057 -4.1378355 -4.1801748 -4.1785254 -4.0399919 -3.820071 -3.7041154 -3.8155596 -4.051774 -4.2756414 -4.4142 -4.4446392 -4.4019432 -4.3347616][-4.1346426 -4.097136 -4.0945468 -4.146543 -4.1885738 -4.1252327 -3.98007 -3.8840399 -3.951925 -4.14032 -4.3435769 -4.4759073 -4.4992914 -4.44485 -4.3582096][-4.1039991 -4.0765805 -4.0892558 -4.1639152 -4.2482305 -4.2566524 -4.1930428 -4.1318526 -4.1603012 -4.2910194 -4.4571743 -4.565424 -4.5707893 -4.5039644 -4.402657][-4.1022539 -4.0991511 -4.1366763 -4.2312422 -4.3443131 -4.4056206 -4.4088788 -4.3805614 -4.3796248 -4.4567385 -4.57507 -4.6470451 -4.6330824 -4.5631557 -4.4610682][-4.1711092 -4.1790338 -4.216145 -4.3012705 -4.4077878 -4.4863682 -4.5244308 -4.5198946 -4.5078349 -4.5469551 -4.6183844 -4.6560187 -4.6381907 -4.5850945 -4.49935][-4.3023915 -4.315711 -4.3386703 -4.3946457 -4.4670868 -4.525569 -4.5607576 -4.5614414 -4.5465989 -4.5589628 -4.590055 -4.6034465 -4.5944834 -4.5694437 -4.5104642][-4.4216275 -4.4405718 -4.4567623 -4.4861369 -4.5146227 -4.5272479 -4.5249181 -4.5046797 -4.4807172 -4.4730506 -4.4768157 -4.4812574 -4.4912214 -4.4990621 -4.4736724]]...]
INFO - root - 2017-12-07 06:56:35.348252: step 8610, loss = 2.01, batch loss = 1.93 (11.9 examples/sec; 0.673 sec/batch; 60h:32m:20s remains)
INFO - root - 2017-12-07 06:56:42.412364: step 8620, loss = 2.05, batch loss = 1.97 (12.4 examples/sec; 0.643 sec/batch; 57h:52m:15s remains)
INFO - root - 2017-12-07 06:56:49.484117: step 8630, loss = 2.06, batch loss = 1.97 (11.8 examples/sec; 0.677 sec/batch; 60h:56m:45s remains)
INFO - root - 2017-12-07 06:56:56.537765: step 8640, loss = 2.01, batch loss = 1.93 (11.1 examples/sec; 0.721 sec/batch; 64h:51m:23s remains)
INFO - root - 2017-12-07 06:57:03.690383: step 8650, loss = 1.99, batch loss = 1.91 (11.4 examples/sec; 0.702 sec/batch; 63h:08m:00s remains)
INFO - root - 2017-12-07 06:57:10.654409: step 8660, loss = 2.01, batch loss = 1.93 (11.2 examples/sec; 0.713 sec/batch; 64h:05m:40s remains)
INFO - root - 2017-12-07 06:57:17.765162: step 8670, loss = 2.03, batch loss = 1.95 (12.0 examples/sec; 0.668 sec/batch; 60h:07m:33s remains)
INFO - root - 2017-12-07 06:57:24.782039: step 8680, loss = 2.01, batch loss = 1.93 (11.6 examples/sec; 0.692 sec/batch; 62h:16m:34s remains)
INFO - root - 2017-12-07 06:57:31.953551: step 8690, loss = 2.02, batch loss = 1.94 (11.1 examples/sec; 0.723 sec/batch; 65h:02m:15s remains)
INFO - root - 2017-12-07 06:57:39.006717: step 8700, loss = 1.95, batch loss = 1.86 (11.1 examples/sec; 0.720 sec/batch; 64h:47m:43s remains)
2017-12-07 06:57:39.705303: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5771518 -4.6534357 -4.70127 -4.7189255 -4.7352467 -4.7543349 -4.7594604 -4.7644358 -4.78991 -4.8251472 -4.8407645 -4.843236 -4.8284492 -4.7852139 -4.7174044][-4.6899934 -4.7774835 -4.8165178 -4.8143778 -4.8137703 -4.8221722 -4.8174925 -4.8132343 -4.854352 -4.9290919 -4.9769869 -5.0079908 -5.0021596 -4.9381437 -4.8448153][-4.7389383 -4.828104 -4.8596497 -4.8489895 -4.8286996 -4.795702 -4.7398124 -4.6974335 -4.7527242 -4.8829441 -4.9872618 -5.0718837 -5.0922308 -5.0179625 -4.912497][-4.730968 -4.8142362 -4.8402624 -4.8306427 -4.7896929 -4.6907234 -4.5490513 -4.4414515 -4.5009542 -4.6911182 -4.8682194 -5.0171514 -5.0730772 -5.0047183 -4.9079976][-4.6942806 -4.7602015 -4.77152 -4.7545581 -4.6887407 -4.5213957 -4.2870135 -4.1044383 -4.15267 -4.3855505 -4.61997 -4.8110819 -4.8945274 -4.8540087 -4.8010907][-4.6452756 -4.6905627 -4.677177 -4.641449 -4.5507836 -4.3302488 -4.0226312 -3.7795134 -3.8063021 -4.0521431 -4.30791 -4.5088348 -4.60724 -4.6147242 -4.6341052][-4.6265135 -4.650301 -4.6072912 -4.543973 -4.4314036 -4.1867423 -3.84934 -3.5868707 -3.601912 -3.8403618 -4.085597 -4.27154 -4.375896 -4.4272823 -4.5067654][-4.6419878 -4.6522851 -4.589283 -4.5103049 -4.4002743 -4.1843004 -3.8889093 -3.6663866 -3.6861291 -3.8991613 -4.1172638 -4.270843 -4.3584719 -4.417697 -4.5058913][-4.6811781 -4.7018461 -4.6434684 -4.5603089 -4.4447536 -4.2479811 -4.0055933 -3.8469102 -3.8872826 -4.0889044 -4.3019991 -4.4365106 -4.5017562 -4.5431857 -4.5979657][-4.7612534 -4.8125587 -4.7765341 -4.6863804 -4.5374 -4.318222 -4.1024933 -4.0014954 -4.0735474 -4.2778659 -4.4974236 -4.6233435 -4.6661048 -4.6813478 -4.6921062][-4.8404522 -4.9125161 -4.9011664 -4.8164425 -4.6444783 -4.4127622 -4.2312794 -4.18513 -4.2822781 -4.4604888 -4.649456 -4.7547574 -4.779386 -4.77775 -4.7601137][-4.8533711 -4.9279571 -4.933279 -4.8651028 -4.7047348 -4.5037365 -4.3775916 -4.371551 -4.4614139 -4.580514 -4.7116714 -4.7896523 -4.8053842 -4.8038754 -4.7825041][-4.7949905 -4.8655996 -4.8797836 -4.8336406 -4.7162995 -4.5890479 -4.5398946 -4.5603266 -4.6101942 -4.6522317 -4.7122989 -4.7605071 -4.7713141 -4.7730122 -4.756423][-4.6562667 -4.7227273 -4.7517872 -4.7381973 -4.677742 -4.626843 -4.6356077 -4.6621013 -4.6672912 -4.6513524 -4.6619349 -4.6875839 -4.6929502 -4.69231 -4.6744804][-4.5036674 -4.5599675 -4.5948629 -4.5984907 -4.5716395 -4.5598888 -4.589119 -4.6115956 -4.5936079 -4.5558553 -4.5444841 -4.5537162 -4.5516543 -4.5456023 -4.5278144]]...]
INFO - root - 2017-12-07 06:57:46.702742: step 8710, loss = 2.01, batch loss = 1.93 (11.6 examples/sec; 0.689 sec/batch; 61h:56m:37s remains)
INFO - root - 2017-12-07 06:57:53.788246: step 8720, loss = 2.06, batch loss = 1.98 (11.4 examples/sec; 0.705 sec/batch; 63h:22m:40s remains)
INFO - root - 2017-12-07 06:58:00.908607: step 8730, loss = 1.98, batch loss = 1.89 (11.5 examples/sec; 0.693 sec/batch; 62h:19m:23s remains)
INFO - root - 2017-12-07 06:58:08.038258: step 8740, loss = 2.02, batch loss = 1.94 (11.0 examples/sec; 0.729 sec/batch; 65h:32m:11s remains)
INFO - root - 2017-12-07 06:58:15.039184: step 8750, loss = 2.03, batch loss = 1.94 (11.4 examples/sec; 0.699 sec/batch; 62h:51m:48s remains)
INFO - root - 2017-12-07 06:58:21.892422: step 8760, loss = 2.02, batch loss = 1.94 (11.0 examples/sec; 0.726 sec/batch; 65h:16m:07s remains)
INFO - root - 2017-12-07 06:58:28.805860: step 8770, loss = 2.03, batch loss = 1.95 (11.7 examples/sec; 0.682 sec/batch; 61h:18m:05s remains)
INFO - root - 2017-12-07 06:58:35.946657: step 8780, loss = 2.06, batch loss = 1.98 (11.1 examples/sec; 0.718 sec/batch; 64h:32m:23s remains)
INFO - root - 2017-12-07 06:58:42.965834: step 8790, loss = 1.97, batch loss = 1.89 (10.5 examples/sec; 0.760 sec/batch; 68h:20m:04s remains)
INFO - root - 2017-12-07 06:58:49.948421: step 8800, loss = 2.04, batch loss = 1.96 (10.9 examples/sec; 0.734 sec/batch; 65h:58m:16s remains)
2017-12-07 06:58:50.666740: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2346072 -4.2138166 -4.2134571 -4.2301025 -4.2673688 -4.297462 -4.2695007 -4.1966553 -4.1714878 -4.2495761 -4.3951335 -4.5353446 -4.6157761 -4.627099 -4.5808558][-4.2451854 -4.2623305 -4.2732735 -4.2774816 -4.3031878 -4.3378134 -4.3274908 -4.2665181 -4.2295671 -4.2838669 -4.40942 -4.5389266 -4.6145511 -4.6107383 -4.534585][-4.26435 -4.3339782 -4.3644848 -4.3544173 -4.3500652 -4.360486 -4.3463945 -4.2959886 -4.2645245 -4.3142443 -4.42506 -4.5327978 -4.58255 -4.5472603 -4.4447007][-4.2719083 -4.3868876 -4.4401622 -4.4221182 -4.3856707 -4.3567009 -4.3158035 -4.2617588 -4.2444863 -4.3100586 -4.4215794 -4.5064859 -4.5210447 -4.458725 -4.359745][-4.2894187 -4.43116 -4.4987688 -4.4744325 -4.4061942 -4.333734 -4.2483344 -4.1633172 -4.1447268 -4.2357211 -4.3744607 -4.4597082 -4.4543366 -4.3858814 -4.3179674][-4.3438349 -4.4846563 -4.5418715 -4.5000963 -4.399344 -4.2799277 -4.1368456 -4.0007524 -3.9653029 -4.090673 -4.2884755 -4.4136829 -4.4194894 -4.3619089 -4.3311253][-4.4044061 -4.5205507 -4.5447764 -4.4727907 -4.3358212 -4.1663146 -3.9667225 -3.7892687 -3.7475052 -3.9182158 -4.1895385 -4.3765769 -4.4140186 -4.3719087 -4.3630805][-4.4379406 -4.5138073 -4.5025873 -4.4051428 -4.241734 -4.035851 -3.8096015 -3.6351433 -3.6192613 -3.8297384 -4.1443706 -4.3688092 -4.4308343 -4.3937125 -4.3797288][-4.4237008 -4.4654174 -4.4474745 -4.3588529 -4.1959867 -3.9839756 -3.7766426 -3.654264 -3.6850438 -3.8968539 -4.1822443 -4.3852797 -4.4400434 -4.3935142 -4.3580689][-4.3654141 -4.3985252 -4.4084563 -4.3621387 -4.23078 -4.0488133 -3.8941052 -3.8389297 -3.9036822 -4.073864 -4.2770648 -4.4149575 -4.4396205 -4.3823943 -4.3338032][-4.2917228 -4.3298469 -4.3811116 -4.3923826 -4.3127966 -4.17618 -4.0742111 -4.0674558 -4.14138 -4.2575359 -4.3803291 -4.4624329 -4.4686618 -4.4145555 -4.3646369][-4.2106638 -4.2608633 -4.3512173 -4.4200716 -4.39852 -4.3040996 -4.2341752 -4.2431293 -4.3047724 -4.381546 -4.4611578 -4.5217118 -4.5289559 -4.4819031 -4.4254584][-4.164638 -4.2193851 -4.3305235 -4.4371762 -4.462008 -4.4048548 -4.3570127 -4.3657303 -4.4042525 -4.4492464 -4.5025964 -4.5524073 -4.5634356 -4.5229492 -4.4613981][-4.1833138 -4.2254009 -4.3221416 -4.4305625 -4.48074 -4.4607177 -4.4407854 -4.450542 -4.4615941 -4.4692392 -4.4910741 -4.5253458 -4.5438285 -4.5203924 -4.4651608][-4.2316442 -4.2521539 -4.313025 -4.4016042 -4.4592376 -4.4630833 -4.4658155 -4.4796023 -4.4703679 -4.4400663 -4.4205971 -4.4304271 -4.4594784 -4.4674716 -4.4303951]]...]
INFO - root - 2017-12-07 06:58:57.771245: step 8810, loss = 2.06, batch loss = 1.98 (11.4 examples/sec; 0.704 sec/batch; 63h:16m:48s remains)
INFO - root - 2017-12-07 06:59:04.836376: step 8820, loss = 2.01, batch loss = 1.93 (11.3 examples/sec; 0.711 sec/batch; 63h:54m:41s remains)
INFO - root - 2017-12-07 06:59:11.813783: step 8830, loss = 2.00, batch loss = 1.91 (11.2 examples/sec; 0.715 sec/batch; 64h:18m:07s remains)
INFO - root - 2017-12-07 06:59:18.858055: step 8840, loss = 2.02, batch loss = 1.94 (11.1 examples/sec; 0.719 sec/batch; 64h:37m:54s remains)
INFO - root - 2017-12-07 06:59:25.900760: step 8850, loss = 2.05, batch loss = 1.97 (11.4 examples/sec; 0.700 sec/batch; 62h:58m:00s remains)
INFO - root - 2017-12-07 06:59:32.933835: step 8860, loss = 1.99, batch loss = 1.91 (11.9 examples/sec; 0.671 sec/batch; 60h:18m:07s remains)
INFO - root - 2017-12-07 06:59:39.930092: step 8870, loss = 2.01, batch loss = 1.93 (11.6 examples/sec; 0.689 sec/batch; 61h:56m:48s remains)
INFO - root - 2017-12-07 06:59:46.990412: step 8880, loss = 2.05, batch loss = 1.97 (11.5 examples/sec; 0.696 sec/batch; 62h:33m:12s remains)
INFO - root - 2017-12-07 06:59:54.044275: step 8890, loss = 2.01, batch loss = 1.93 (11.5 examples/sec; 0.698 sec/batch; 62h:47m:19s remains)
INFO - root - 2017-12-07 07:00:01.124503: step 8900, loss = 2.02, batch loss = 1.93 (11.5 examples/sec; 0.693 sec/batch; 62h:17m:38s remains)
2017-12-07 07:00:01.911328: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2764373 -4.3353386 -4.4574938 -4.5787921 -4.6420875 -4.6469207 -4.6126451 -4.6107483 -4.6771812 -4.7515125 -4.7605152 -4.6681862 -4.5092406 -4.3639259 -4.3016825][-4.1191859 -4.2409568 -4.4097 -4.5581756 -4.6388612 -4.6403513 -4.5893617 -4.5710287 -4.631537 -4.7096739 -4.71497 -4.6086063 -4.4368954 -4.290874 -4.2307024][-4.05149 -4.2356896 -4.4360557 -4.5809565 -4.6429372 -4.6163592 -4.5356884 -4.494997 -4.5416217 -4.6102581 -4.6094246 -4.5051537 -4.3471408 -4.2205043 -4.1813149][-4.0457816 -4.2671189 -4.476182 -4.5853105 -4.5891232 -4.502243 -4.3820848 -4.330586 -4.3855367 -4.4659524 -4.4846554 -4.4146013 -4.2965045 -4.2020183 -4.1892819][-4.1718054 -4.3806119 -4.5428162 -4.5672178 -4.4730086 -4.300571 -4.1275373 -4.0685577 -4.1581459 -4.2823706 -4.3489547 -4.3415575 -4.2878513 -4.2412014 -4.2577939][-4.3507695 -4.5107927 -4.5955067 -4.529953 -4.3527856 -4.1189666 -3.9015131 -3.8285842 -3.9557087 -4.126472 -4.2415929 -4.2962041 -4.3121967 -4.3182583 -4.349474][-4.4590044 -4.5662594 -4.5828738 -4.4558182 -4.2315421 -3.976263 -3.7344174 -3.642777 -3.8007321 -4.0153785 -4.1697946 -4.2727227 -4.3396544 -4.3714042 -4.3867583][-4.5150666 -4.5871058 -4.5553389 -4.3923039 -4.1460214 -3.8930898 -3.6525161 -3.5526736 -3.730772 -3.9763689 -4.1491051 -4.2699895 -4.352736 -4.3760681 -4.35697][-4.5495329 -4.6214108 -4.5864291 -4.4332004 -4.2017407 -3.9752793 -3.7672668 -3.6777697 -3.8441868 -4.0796394 -4.2350187 -4.3388686 -4.40605 -4.4071631 -4.3637018][-4.5779696 -4.6627741 -4.6477017 -4.5258493 -4.3281159 -4.1373625 -3.9774458 -3.9143543 -4.052844 -4.2533126 -4.37907 -4.4581757 -4.5060477 -4.4915047 -4.4377456][-4.6272817 -4.7062721 -4.6990185 -4.6034389 -4.4444809 -4.2957916 -4.1932373 -4.1711655 -4.2834911 -4.4335256 -4.5188766 -4.56563 -4.59321 -4.5735946 -4.523818][-4.6933007 -4.7586432 -4.7610283 -4.7012882 -4.59095 -4.4879684 -4.4348125 -4.4410472 -4.515276 -4.6001048 -4.6368484 -4.648849 -4.6582489 -4.6415443 -4.6065068][-4.7417393 -4.79239 -4.80803 -4.785089 -4.7236915 -4.6603389 -4.6350656 -4.6459265 -4.680469 -4.7109175 -4.7137594 -4.704464 -4.7009029 -4.688097 -4.6654873][-4.7406282 -4.7672806 -4.7827992 -4.7814651 -4.7580543 -4.728672 -4.717618 -4.7222042 -4.7293158 -4.729125 -4.717164 -4.7004738 -4.6919074 -4.6844282 -4.6718693][-4.6645579 -4.6718111 -4.6799393 -4.6858311 -4.6835361 -4.6746969 -4.6679296 -4.6629052 -4.6538172 -4.6417789 -4.6288161 -4.6169 -4.6125507 -4.612226 -4.6096692]]...]
INFO - root - 2017-12-07 07:00:08.953638: step 8910, loss = 2.01, batch loss = 1.93 (11.5 examples/sec; 0.696 sec/batch; 62h:31m:37s remains)
INFO - root - 2017-12-07 07:00:15.845714: step 8920, loss = 2.00, batch loss = 1.92 (11.2 examples/sec; 0.714 sec/batch; 64h:11m:21s remains)
INFO - root - 2017-12-07 07:00:22.894716: step 8930, loss = 2.03, batch loss = 1.95 (11.1 examples/sec; 0.722 sec/batch; 64h:54m:03s remains)
INFO - root - 2017-12-07 07:00:29.860681: step 8940, loss = 2.02, batch loss = 1.94 (12.2 examples/sec; 0.656 sec/batch; 58h:55m:35s remains)
INFO - root - 2017-12-07 07:00:36.899773: step 8950, loss = 2.02, batch loss = 1.93 (11.7 examples/sec; 0.681 sec/batch; 61h:12m:16s remains)
INFO - root - 2017-12-07 07:00:44.062883: step 8960, loss = 2.01, batch loss = 1.92 (11.2 examples/sec; 0.713 sec/batch; 64h:02m:50s remains)
INFO - root - 2017-12-07 07:00:51.063240: step 8970, loss = 2.02, batch loss = 1.94 (10.6 examples/sec; 0.753 sec/batch; 67h:41m:02s remains)
INFO - root - 2017-12-07 07:00:58.037090: step 8980, loss = 2.01, batch loss = 1.92 (10.9 examples/sec; 0.733 sec/batch; 65h:54m:44s remains)
INFO - root - 2017-12-07 07:01:05.171586: step 8990, loss = 2.03, batch loss = 1.95 (11.1 examples/sec; 0.719 sec/batch; 64h:35m:29s remains)
INFO - root - 2017-12-07 07:01:12.190715: step 9000, loss = 2.04, batch loss = 1.96 (10.9 examples/sec; 0.735 sec/batch; 66h:03m:04s remains)
2017-12-07 07:01:12.997075: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5593743 -4.5322776 -4.4940104 -4.4637971 -4.4200187 -4.3806868 -4.377255 -4.41159 -4.4669852 -4.5182943 -4.5444555 -4.533072 -4.4527154 -4.3027925 -4.1406517][-4.5961618 -4.5758457 -4.5481029 -4.5217724 -4.4715037 -4.4202628 -4.4101377 -4.44622 -4.5099382 -4.5692906 -4.59709 -4.5725632 -4.4705782 -4.30489 -4.1387043][-4.6314158 -4.617209 -4.6009216 -4.5783615 -4.5175257 -4.4496131 -4.4327178 -4.4807339 -4.5649428 -4.6379371 -4.6630883 -4.6198864 -4.4996362 -4.3265543 -4.1625786][-4.628993 -4.6304111 -4.6205778 -4.5881586 -4.503387 -4.4054866 -4.3730254 -4.4396348 -4.5613565 -4.6652136 -4.6998248 -4.6477528 -4.5260429 -4.3665581 -4.2182093][-4.5835905 -4.5888429 -4.5662432 -4.50568 -4.3848333 -4.24722 -4.1918712 -4.2790294 -4.4517193 -4.6040759 -4.6659522 -4.6267967 -4.5304961 -4.4094539 -4.2921467][-4.5196848 -4.5052109 -4.4437008 -4.3360958 -4.1660814 -3.9803596 -3.895457 -4.0012703 -4.2320275 -4.4454279 -4.5530429 -4.5500221 -4.4959111 -4.4175124 -4.3325653][-4.4359751 -4.3976374 -4.2986622 -4.1475549 -3.9326491 -3.7007236 -3.5828023 -3.7001789 -3.980726 -4.2501059 -4.40855 -4.4526396 -4.4373322 -4.3806067 -4.3105383][-4.3631206 -4.3268304 -4.2266779 -4.0629187 -3.8278377 -3.563868 -3.408711 -3.5153887 -3.8118231 -4.1051345 -4.2927961 -4.3733773 -4.3836327 -4.32882 -4.2568927][-4.33753 -4.3339491 -4.2609892 -4.1111736 -3.8883438 -3.6324382 -3.4621108 -3.5395598 -3.8079538 -4.0785475 -4.2618 -4.3581734 -4.3773828 -4.3099265 -4.2199397][-4.343936 -4.3709068 -4.3282533 -4.207397 -4.0266171 -3.8219664 -3.6752396 -3.7262905 -3.9351206 -4.1446662 -4.2965889 -4.3884892 -4.4042625 -4.3215818 -4.2087579][-4.37057 -4.412148 -4.3994923 -4.3252606 -4.2092085 -4.0743489 -3.9699142 -3.9983389 -4.1308155 -4.2585425 -4.3588762 -4.4256086 -4.4280076 -4.3424249 -4.2300692][-4.4282284 -4.4624767 -4.4668179 -4.4365029 -4.3814335 -4.3084631 -4.2461343 -4.2618251 -4.3302569 -4.3883023 -4.4380255 -4.4662356 -4.4458418 -4.3690133 -4.2826138][-4.473947 -4.4906125 -4.4967546 -4.491724 -4.4780154 -4.449429 -4.4206057 -4.4331117 -4.4638329 -4.4835644 -4.502161 -4.5001588 -4.4599285 -4.39527 -4.3392777][-4.4711795 -4.47553 -4.4790435 -4.4834347 -4.4895411 -4.4864559 -4.4781742 -4.4873676 -4.498733 -4.5031815 -4.5108714 -4.5019765 -4.4647059 -4.4204574 -4.3883953][-4.419404 -4.4269171 -4.4340186 -4.4393229 -4.4492197 -4.4561372 -4.4547954 -4.454452 -4.4518209 -4.4504805 -4.4587183 -4.4630575 -4.4541125 -4.4441018 -4.440639]]...]
INFO - root - 2017-12-07 07:01:19.924117: step 9010, loss = 2.01, batch loss = 1.93 (12.2 examples/sec; 0.655 sec/batch; 58h:53m:48s remains)
INFO - root - 2017-12-07 07:01:27.004961: step 9020, loss = 2.03, batch loss = 1.95 (10.9 examples/sec; 0.735 sec/batch; 66h:03m:04s remains)
INFO - root - 2017-12-07 07:01:34.017179: step 9030, loss = 2.05, batch loss = 1.97 (11.3 examples/sec; 0.710 sec/batch; 63h:48m:08s remains)
INFO - root - 2017-12-07 07:01:41.152969: step 9040, loss = 2.03, batch loss = 1.95 (11.3 examples/sec; 0.705 sec/batch; 63h:22m:59s remains)
INFO - root - 2017-12-07 07:01:48.143940: step 9050, loss = 2.00, batch loss = 1.92 (11.9 examples/sec; 0.671 sec/batch; 60h:18m:32s remains)
INFO - root - 2017-12-07 07:01:55.172089: step 9060, loss = 2.08, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 61h:52m:34s remains)
INFO - root - 2017-12-07 07:02:02.208585: step 9070, loss = 2.00, batch loss = 1.92 (11.5 examples/sec; 0.697 sec/batch; 62h:38m:00s remains)
INFO - root - 2017-12-07 07:02:09.277263: step 9080, loss = 2.02, batch loss = 1.94 (10.6 examples/sec; 0.755 sec/batch; 67h:47m:50s remains)
INFO - root - 2017-12-07 07:02:16.154596: step 9090, loss = 2.02, batch loss = 1.94 (11.1 examples/sec; 0.719 sec/batch; 64h:33m:04s remains)
INFO - root - 2017-12-07 07:02:23.219261: step 9100, loss = 2.04, batch loss = 1.96 (11.6 examples/sec; 0.691 sec/batch; 62h:06m:54s remains)
2017-12-07 07:02:24.065029: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7103629 -4.710875 -4.6935325 -4.660604 -4.6336284 -4.6187525 -4.6135058 -4.6174951 -4.61501 -4.5873909 -4.556469 -4.5505943 -4.5671568 -4.5746689 -4.564404][-4.7024879 -4.7263379 -4.7314076 -4.724854 -4.714128 -4.6951394 -4.6640019 -4.6342335 -4.6010327 -4.5444922 -4.4915957 -4.4807472 -4.5170932 -4.5456533 -4.548243][-4.6189795 -4.65932 -4.6915107 -4.7289433 -4.7522659 -4.7394943 -4.6865449 -4.6201105 -4.560185 -4.4856591 -4.4126058 -4.37947 -4.4106536 -4.454206 -4.4803205][-4.5573816 -4.5928397 -4.6262841 -4.679081 -4.7085223 -4.6904516 -4.6219025 -4.5395737 -4.4909945 -4.4431968 -4.3750215 -4.3157563 -4.3127789 -4.3453789 -4.3876324][-4.5553765 -4.56843 -4.5745692 -4.5991893 -4.5891027 -4.532939 -4.4361677 -4.3481178 -4.343749 -4.3682413 -4.3509197 -4.2976041 -4.2648969 -4.2738867 -4.3146777][-4.5427804 -4.5266666 -4.4981956 -4.4811449 -4.41469 -4.302629 -4.1567364 -4.0517097 -4.0984325 -4.2223334 -4.3056917 -4.3207326 -4.3021541 -4.2954011 -4.3125496][-4.4576135 -4.4129782 -4.3675079 -4.3313584 -4.2296429 -4.0738535 -3.8740523 -3.7355318 -3.8106928 -4.0242214 -4.2298594 -4.3586922 -4.3979831 -4.3877397 -4.3582373][-4.3132024 -4.2475243 -4.2175164 -4.205142 -4.118947 -3.958879 -3.727226 -3.5521071 -3.6194696 -3.8686888 -4.1454167 -4.3537655 -4.44502 -4.4380946 -4.3664584][-4.2023797 -4.1271648 -4.1275563 -4.1583939 -4.1269097 -4.0133891 -3.8090861 -3.641073 -3.6855447 -3.9059353 -4.1667371 -4.3712239 -4.4631944 -4.4395552 -4.3289938][-4.2013607 -4.1154823 -4.1241369 -4.1700873 -4.1793714 -4.1264277 -3.996356 -3.8910286 -3.9429393 -4.1249 -4.3299704 -4.4749479 -4.5201039 -4.4466891 -4.2846589][-4.3006339 -4.2108912 -4.20119 -4.2237215 -4.2326617 -4.2176852 -4.1665163 -4.1442065 -4.2265058 -4.3825407 -4.5255904 -4.6003895 -4.5905738 -4.4641776 -4.2560019][-4.4196925 -4.3463807 -4.3265758 -4.3267841 -4.32198 -4.3211522 -4.3219638 -4.3579311 -4.4595032 -4.5862637 -4.6700811 -4.6859937 -4.635272 -4.4804993 -4.2569656][-4.4716282 -4.4259849 -4.4206495 -4.4281483 -4.4316425 -4.4468284 -4.4732909 -4.5276728 -4.6209412 -4.7133951 -4.7525706 -4.7325563 -4.6595283 -4.5039597 -4.3001809][-4.4271793 -4.3972235 -4.4167752 -4.4576216 -4.4989605 -4.5461292 -4.5860209 -4.6277609 -4.6910696 -4.7521844 -4.7737474 -4.7507386 -4.6861644 -4.5630951 -4.4104238][-4.3303294 -4.2967777 -4.3291769 -4.4032021 -4.4905849 -4.5780182 -4.631166 -4.6508112 -4.6719179 -4.700459 -4.7182407 -4.7140384 -4.6815095 -4.6122522 -4.5271668]]...]
INFO - root - 2017-12-07 07:02:31.062573: step 9110, loss = 2.00, batch loss = 1.91 (10.9 examples/sec; 0.732 sec/batch; 65h:43m:20s remains)
INFO - root - 2017-12-07 07:02:38.144159: step 9120, loss = 2.00, batch loss = 1.92 (10.6 examples/sec; 0.756 sec/batch; 67h:53m:26s remains)
INFO - root - 2017-12-07 07:02:45.165345: step 9130, loss = 2.02, batch loss = 1.94 (11.8 examples/sec; 0.680 sec/batch; 61h:05m:28s remains)
INFO - root - 2017-12-07 07:02:52.168861: step 9140, loss = 2.03, batch loss = 1.94 (11.7 examples/sec; 0.686 sec/batch; 61h:35m:22s remains)
INFO - root - 2017-12-07 07:02:59.235348: step 9150, loss = 2.03, batch loss = 1.95 (11.1 examples/sec; 0.722 sec/batch; 64h:51m:32s remains)
INFO - root - 2017-12-07 07:03:06.299017: step 9160, loss = 2.01, batch loss = 1.92 (11.8 examples/sec; 0.677 sec/batch; 60h:45m:56s remains)
INFO - root - 2017-12-07 07:03:13.379609: step 9170, loss = 2.05, batch loss = 1.96 (11.7 examples/sec; 0.683 sec/batch; 61h:19m:32s remains)
INFO - root - 2017-12-07 07:03:20.345383: step 9180, loss = 2.04, batch loss = 1.96 (11.4 examples/sec; 0.704 sec/batch; 63h:15m:59s remains)
INFO - root - 2017-12-07 07:03:27.503749: step 9190, loss = 2.01, batch loss = 1.92 (11.0 examples/sec; 0.727 sec/batch; 65h:18m:20s remains)
INFO - root - 2017-12-07 07:03:34.521843: step 9200, loss = 1.96, batch loss = 1.88 (11.6 examples/sec; 0.692 sec/batch; 62h:10m:49s remains)
2017-12-07 07:03:35.244805: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4906907 -4.58003 -4.6523046 -4.6842084 -4.6672325 -4.6328473 -4.6121168 -4.6260657 -4.6509509 -4.6602492 -4.6727419 -4.6915507 -4.6935859 -4.6598797 -4.59345][-4.559958 -4.6662173 -4.7346315 -4.7358732 -4.6682439 -4.583765 -4.5448012 -4.5831943 -4.6508327 -4.6935277 -4.7185855 -4.7420764 -4.747005 -4.7092848 -4.6324134][-4.6186972 -4.7244358 -4.76455 -4.716877 -4.5904665 -4.4522982 -4.3912735 -4.4536171 -4.5741134 -4.6700435 -4.7183609 -4.7406521 -4.73975 -4.7007866 -4.6292958][-4.6712036 -4.7656956 -4.7596822 -4.6510086 -4.4670324 -4.28444 -4.2034221 -4.282733 -4.4527917 -4.6043181 -4.6807156 -4.7027993 -4.6929245 -4.6512713 -4.5917549][-4.7250366 -4.8056526 -4.7552609 -4.5876746 -4.3539619 -4.1340733 -4.0272574 -4.1083846 -4.3111763 -4.5040693 -4.6065264 -4.6354995 -4.6231766 -4.5799 -4.5328751][-4.7745171 -4.8446927 -4.7602358 -4.5487242 -4.2772727 -4.0174885 -3.860743 -3.9068618 -4.1131763 -4.3334489 -4.4698319 -4.5306096 -4.5389557 -4.506948 -4.4759974][-4.8142095 -4.8753719 -4.7655306 -4.5246825 -4.2221875 -3.9177337 -3.6928267 -3.6794631 -3.884999 -4.145853 -4.3383765 -4.4467959 -4.4831457 -4.4632878 -4.442142][-4.8256831 -4.8836646 -4.7622538 -4.5113997 -4.196394 -3.8596563 -3.5823112 -3.5269148 -3.7434652 -4.0508122 -4.2917895 -4.4330311 -4.4864092 -4.4695969 -4.4415026][-4.8034229 -4.86901 -4.7621655 -4.5354161 -4.2434134 -3.9176552 -3.6451864 -3.579025 -3.7798305 -4.0874147 -4.3315597 -4.4763288 -4.5399146 -4.5242958 -4.4801741][-4.7783642 -4.8615723 -4.795785 -4.6285391 -4.383594 -4.0869293 -3.83505 -3.7601547 -3.9160013 -4.1845913 -4.4066048 -4.5486479 -4.6241422 -4.6133356 -4.5530581][-4.7648187 -4.8646188 -4.8422766 -4.7355494 -4.5358362 -4.271318 -4.0489097 -3.9796443 -4.0897217 -4.2981706 -4.4889522 -4.6341 -4.7213054 -4.7123942 -4.6322765][-4.749115 -4.85794 -4.8683825 -4.8042517 -4.6475229 -4.4311118 -4.2613893 -4.2211 -4.3006248 -4.4441876 -4.5889792 -4.7162223 -4.7876105 -4.7651281 -4.6729937][-4.712357 -4.8238177 -4.85563 -4.8185592 -4.7028961 -4.54471 -4.4338961 -4.4241309 -4.4821463 -4.571003 -4.6654463 -4.7565522 -4.7971745 -4.7626085 -4.6793089][-4.6460042 -4.7471132 -4.780499 -4.7517567 -4.6675067 -4.5671215 -4.51186 -4.5267777 -4.5739484 -4.6273255 -4.6796074 -4.730895 -4.7428508 -4.7073832 -4.6475158][-4.5490832 -4.6208053 -4.6397395 -4.6120634 -4.5582876 -4.5113997 -4.5003848 -4.5253854 -4.5588794 -4.5879259 -4.6130252 -4.6358438 -4.633153 -4.6060815 -4.5717416]]...]
INFO - root - 2017-12-07 07:03:42.370596: step 9210, loss = 1.97, batch loss = 1.89 (11.0 examples/sec; 0.726 sec/batch; 65h:12m:42s remains)
INFO - root - 2017-12-07 07:03:49.467210: step 9220, loss = 2.01, batch loss = 1.93 (11.1 examples/sec; 0.724 sec/batch; 64h:59m:23s remains)
INFO - root - 2017-12-07 07:03:56.489229: step 9230, loss = 2.07, batch loss = 1.99 (11.5 examples/sec; 0.693 sec/batch; 62h:14m:58s remains)
INFO - root - 2017-12-07 07:04:03.591895: step 9240, loss = 2.04, batch loss = 1.96 (11.8 examples/sec; 0.679 sec/batch; 60h:57m:18s remains)
INFO - root - 2017-12-07 07:04:10.540825: step 9250, loss = 2.05, batch loss = 1.97 (11.7 examples/sec; 0.685 sec/batch; 61h:32m:08s remains)
INFO - root - 2017-12-07 07:04:17.721150: step 9260, loss = 1.99, batch loss = 1.91 (11.1 examples/sec; 0.723 sec/batch; 64h:54m:55s remains)
INFO - root - 2017-12-07 07:04:24.806954: step 9270, loss = 2.01, batch loss = 1.92 (10.9 examples/sec; 0.736 sec/batch; 66h:03m:00s remains)
INFO - root - 2017-12-07 07:04:31.711104: step 9280, loss = 2.05, batch loss = 1.96 (12.0 examples/sec; 0.669 sec/batch; 60h:04m:45s remains)
INFO - root - 2017-12-07 07:04:38.732753: step 9290, loss = 2.02, batch loss = 1.94 (12.0 examples/sec; 0.668 sec/batch; 59h:58m:02s remains)
INFO - root - 2017-12-07 07:04:45.806487: step 9300, loss = 2.01, batch loss = 1.92 (11.6 examples/sec; 0.693 sec/batch; 62h:10m:36s remains)
2017-12-07 07:04:46.612316: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3243484 -4.271503 -4.2254653 -4.1820793 -4.1573715 -4.1824226 -4.2388892 -4.2627974 -4.2484293 -4.2507524 -4.2793345 -4.3361335 -4.4012971 -4.4356103 -4.4284735][-4.3301477 -4.2850661 -4.2375693 -4.1821542 -4.14109 -4.1564755 -4.2078733 -4.2210345 -4.19767 -4.1998563 -4.2427373 -4.3227482 -4.4067793 -4.4536266 -4.4491429][-4.3554358 -4.3351645 -4.3056121 -4.2567158 -4.2121215 -4.2143469 -4.2440629 -4.2275805 -4.1816945 -4.1807046 -4.2362442 -4.332334 -4.4313374 -4.4859133 -4.4759173][-4.3305278 -4.3385296 -4.3341093 -4.301609 -4.2585883 -4.244626 -4.247736 -4.20089 -4.1366591 -4.1389217 -4.2119684 -4.3239574 -4.4394379 -4.5030651 -4.4897809][-4.2340336 -4.2598414 -4.2805567 -4.264215 -4.2155938 -4.1838865 -4.165206 -4.1028962 -4.0350981 -4.0486593 -4.1434507 -4.275311 -4.409781 -4.4857316 -4.4758687][-4.106288 -4.1308002 -4.1686964 -4.1637025 -4.1115327 -4.071537 -4.0435891 -3.9847016 -3.9292588 -3.9549885 -4.067646 -4.21599 -4.3628063 -4.4467368 -4.44316][-3.9915712 -4.0021453 -4.0403152 -4.0329714 -3.9809337 -3.9486744 -3.9281464 -3.8939066 -3.8699596 -3.9078507 -4.0282159 -4.1838608 -4.3313403 -4.4143682 -4.4136653][-3.9569662 -3.9669046 -4.0079741 -3.9980912 -3.948837 -3.9234207 -3.9090664 -3.9053032 -3.9183173 -3.9660244 -4.080483 -4.226522 -4.3575621 -4.4273739 -4.4198508][-3.99065 -4.013206 -4.0669618 -4.061409 -4.0187588 -3.9975784 -3.9881375 -4.0090017 -4.0518227 -4.1046104 -4.2045465 -4.328578 -4.4335265 -4.4841261 -4.4625683][-4.0414653 -4.0825186 -4.1426134 -4.140451 -4.1066341 -4.0962834 -4.1036892 -4.1480312 -4.2083769 -4.2626724 -4.3452311 -4.441144 -4.518259 -4.5505161 -4.5164695][-4.1150103 -4.1754918 -4.2383633 -4.2400517 -4.2112026 -4.2057834 -4.2218366 -4.2678432 -4.3204241 -4.3654056 -4.4310341 -4.5029559 -4.5596228 -4.5815253 -4.5462475][-4.2086668 -4.2682056 -4.3182654 -4.3168311 -4.287734 -4.27482 -4.2785363 -4.2971663 -4.3168488 -4.3485451 -4.4117093 -4.4818287 -4.5385389 -4.5643239 -4.5390635][-4.2921071 -4.3220325 -4.3392968 -4.3259683 -4.2953162 -4.2725959 -4.2554746 -4.2302403 -4.2024589 -4.2181764 -4.2910171 -4.3836231 -4.4671688 -4.5166011 -4.5090251][-4.3545332 -4.3527422 -4.3387418 -4.3119073 -4.277256 -4.2487836 -4.2177348 -4.1534028 -4.0800471 -4.0767841 -4.1573839 -4.2749734 -4.3935633 -4.4709239 -4.4748006][-4.3809633 -4.3633714 -4.33682 -4.3051548 -4.2716022 -4.2467375 -4.2173386 -4.1382041 -4.0450835 -4.0360117 -4.12167 -4.2487903 -4.38152 -4.4627428 -4.4547167]]...]
INFO - root - 2017-12-07 07:04:53.702675: step 9310, loss = 2.03, batch loss = 1.95 (11.0 examples/sec; 0.727 sec/batch; 65h:15m:36s remains)
INFO - root - 2017-12-07 07:05:00.753639: step 9320, loss = 1.98, batch loss = 1.90 (11.7 examples/sec; 0.686 sec/batch; 61h:37m:32s remains)
INFO - root - 2017-12-07 07:05:07.788539: step 9330, loss = 2.04, batch loss = 1.96 (12.0 examples/sec; 0.669 sec/batch; 60h:03m:36s remains)
INFO - root - 2017-12-07 07:05:14.945240: step 9340, loss = 2.07, batch loss = 1.99 (11.1 examples/sec; 0.723 sec/batch; 64h:53m:36s remains)
INFO - root - 2017-12-07 07:05:22.026704: step 9350, loss = 1.99, batch loss = 1.91 (11.0 examples/sec; 0.727 sec/batch; 65h:13m:56s remains)
INFO - root - 2017-12-07 07:05:29.011858: step 9360, loss = 2.02, batch loss = 1.94 (10.8 examples/sec; 0.744 sec/batch; 66h:45m:51s remains)
INFO - root - 2017-12-07 07:05:36.039117: step 9370, loss = 2.04, batch loss = 1.96 (11.4 examples/sec; 0.699 sec/batch; 62h:45m:36s remains)
INFO - root - 2017-12-07 07:05:43.072204: step 9380, loss = 2.02, batch loss = 1.94 (11.6 examples/sec; 0.692 sec/batch; 62h:04m:35s remains)
INFO - root - 2017-12-07 07:05:50.134223: step 9390, loss = 2.02, batch loss = 1.94 (10.7 examples/sec; 0.745 sec/batch; 66h:52m:39s remains)
INFO - root - 2017-12-07 07:05:57.195692: step 9400, loss = 2.05, batch loss = 1.97 (11.0 examples/sec; 0.725 sec/batch; 65h:05m:06s remains)
2017-12-07 07:05:57.969836: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4547548 -4.4828863 -4.5434165 -4.5723891 -4.5671515 -4.59185 -4.6486235 -4.6742282 -4.610085 -4.4539108 -4.3635168 -4.4169111 -4.52318 -4.61987 -4.6727467][-4.4452763 -4.5270481 -4.6119008 -4.6456404 -4.6353254 -4.6261744 -4.6600208 -4.6949692 -4.6566887 -4.5260224 -4.4212346 -4.4359508 -4.5121565 -4.5931382 -4.6388435][-4.4911771 -4.6002293 -4.6871648 -4.7151451 -4.7046041 -4.6721263 -4.6736808 -4.69487 -4.6678452 -4.5614824 -4.4583735 -4.4517488 -4.4931788 -4.5452523 -4.588449][-4.56629 -4.6626463 -4.7161622 -4.7177815 -4.7100911 -4.6849985 -4.6719928 -4.6749043 -4.6443405 -4.5513964 -4.4615536 -4.4543076 -4.466435 -4.4811978 -4.5259676][-4.592721 -4.6422663 -4.6254463 -4.5784345 -4.5760269 -4.5930285 -4.5980134 -4.5848403 -4.5345554 -4.4354911 -4.3665247 -4.3978214 -4.4243793 -4.4277525 -4.4792585][-4.5817528 -4.5736895 -4.4634333 -4.3387475 -4.3204188 -4.3811226 -4.4196897 -4.3932815 -4.3180981 -4.2202425 -4.1944351 -4.3063869 -4.3992677 -4.4166813 -4.47081][-4.5524449 -4.4929624 -4.2864709 -4.072248 -4.0164456 -4.09781 -4.1584945 -4.1171131 -4.0321865 -3.9574606 -3.9873013 -4.1920552 -4.3768458 -4.4358239 -4.5019975][-4.5470243 -4.4532342 -4.17993 -3.9033265 -3.8096347 -3.8880734 -3.9453235 -3.8842726 -3.8081779 -3.7771626 -3.8611271 -4.1332989 -4.3873029 -4.4794364 -4.5484915][-4.6172051 -4.509511 -4.2124887 -3.9144251 -3.8043714 -3.8696866 -3.9066451 -3.8366013 -3.7884541 -3.8052988 -3.9240746 -4.2020164 -4.4505939 -4.5316095 -4.58373][-4.7038856 -4.6141963 -4.3345857 -4.048173 -3.9330981 -3.9780018 -3.9939284 -3.9354141 -3.9287407 -3.9838991 -4.1144915 -4.348156 -4.5392852 -4.5861554 -4.6131563][-4.7736425 -4.7227788 -4.5021286 -4.25245 -4.1384044 -4.1632586 -4.1694484 -4.1347547 -4.1573877 -4.2284245 -4.3453903 -4.5083022 -4.626411 -4.6388764 -4.6423879][-4.8348923 -4.8253832 -4.6922808 -4.5089173 -4.4059458 -4.4090829 -4.41893 -4.41683 -4.4575243 -4.5229321 -4.5996056 -4.6759672 -4.7137604 -4.6921635 -4.67442][-4.8509541 -4.864079 -4.8091173 -4.7037392 -4.6277251 -4.6157579 -4.6256742 -4.6405215 -4.6808586 -4.7264509 -4.7616472 -4.7766914 -4.7627897 -4.7230687 -4.6905251][-4.82312 -4.8290792 -4.8175392 -4.7789016 -4.7412453 -4.7291217 -4.733633 -4.7433925 -4.757515 -4.7599764 -4.7517686 -4.7385726 -4.7120624 -4.6788783 -4.6499362][-4.78084 -4.7720313 -4.7708411 -4.7644773 -4.751369 -4.743052 -4.739913 -4.7338982 -4.709497 -4.6566391 -4.6111732 -4.6028848 -4.6028113 -4.6025419 -4.5964465]]...]
INFO - root - 2017-12-07 07:06:05.088226: step 9410, loss = 2.03, batch loss = 1.95 (11.5 examples/sec; 0.695 sec/batch; 62h:24m:33s remains)
INFO - root - 2017-12-07 07:06:12.108589: step 9420, loss = 2.01, batch loss = 1.93 (12.3 examples/sec; 0.652 sec/batch; 58h:30m:57s remains)
INFO - root - 2017-12-07 07:06:19.142804: step 9430, loss = 2.07, batch loss = 1.99 (11.2 examples/sec; 0.711 sec/batch; 63h:49m:09s remains)
INFO - root - 2017-12-07 07:06:26.204304: step 9440, loss = 2.02, batch loss = 1.94 (11.7 examples/sec; 0.684 sec/batch; 61h:23m:36s remains)
INFO - root - 2017-12-07 07:06:33.212247: step 9450, loss = 1.99, batch loss = 1.91 (11.4 examples/sec; 0.704 sec/batch; 63h:12m:42s remains)
INFO - root - 2017-12-07 07:06:40.290592: step 9460, loss = 2.04, batch loss = 1.96 (11.3 examples/sec; 0.705 sec/batch; 63h:17m:04s remains)
INFO - root - 2017-12-07 07:06:47.354081: step 9470, loss = 1.99, batch loss = 1.91 (11.6 examples/sec; 0.687 sec/batch; 61h:39m:02s remains)
INFO - root - 2017-12-07 07:06:54.403150: step 9480, loss = 2.07, batch loss = 1.99 (11.8 examples/sec; 0.675 sec/batch; 60h:35m:31s remains)
INFO - root - 2017-12-07 07:07:01.500522: step 9490, loss = 2.05, batch loss = 1.97 (11.2 examples/sec; 0.711 sec/batch; 63h:50m:02s remains)
INFO - root - 2017-12-07 07:07:08.496574: step 9500, loss = 2.03, batch loss = 1.95 (10.8 examples/sec; 0.742 sec/batch; 66h:33m:21s remains)
2017-12-07 07:07:09.337428: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1530614 -4.2386646 -4.3109293 -4.3169889 -4.22226 -4.0734453 -3.9648526 -3.9189677 -3.9320416 -3.9809022 -4.0779462 -4.1952596 -4.279511 -4.3426504 -4.3969779][-4.1790609 -4.2722983 -4.352551 -4.3650637 -4.2771935 -4.1321578 -4.011559 -3.9448409 -3.9435806 -3.9970095 -4.1100836 -4.2380085 -4.3212852 -4.3750539 -4.4208064][-4.2447505 -4.3329053 -4.3959417 -4.3922758 -4.3089833 -4.1851258 -4.07266 -4.0008364 -3.9953167 -4.0586848 -4.1843042 -4.3121729 -4.38524 -4.4220252 -4.4470797][-4.3042531 -4.3846884 -4.4182615 -4.3861389 -4.2980585 -4.18967 -4.0876269 -4.0210466 -4.0249476 -4.1074367 -4.2499671 -4.3802629 -4.4439158 -4.4612088 -4.4589362][-4.3262324 -4.3998909 -4.4121327 -4.3582983 -4.25703 -4.1423917 -4.033145 -3.9665222 -3.9888754 -4.1021738 -4.2728925 -4.4183145 -4.4786482 -4.4756064 -4.4435439][-4.3293495 -4.4009838 -4.39954 -4.3239403 -4.1952229 -4.0552487 -3.9251726 -3.8536513 -3.8968663 -4.0440469 -4.2423286 -4.4001193 -4.4543314 -4.4309 -4.3762794][-4.3333616 -4.4085212 -4.404695 -4.312604 -4.1580257 -3.9977994 -3.8570352 -3.7836537 -3.8392453 -3.9985797 -4.1960258 -4.342628 -4.3805652 -4.3446245 -4.2885184][-4.34202 -4.423533 -4.4220963 -4.3191805 -4.1536407 -3.9949837 -3.8683934 -3.8046093 -3.8598578 -4.0021267 -4.1647315 -4.2760277 -4.2945094 -4.262805 -4.2287569][-4.3310337 -4.4097624 -4.40914 -4.3014131 -4.1382284 -3.9985316 -3.9070969 -3.8679731 -3.9161208 -4.0275245 -4.1436424 -4.2131124 -4.2168536 -4.1993871 -4.1980104][-4.283586 -4.3599834 -4.3679552 -4.2706466 -4.1191053 -3.9986084 -3.9419093 -3.9314821 -3.9778349 -4.0682106 -4.1508484 -4.1892757 -4.1906557 -4.19267 -4.2164822][-4.24362 -4.3225751 -4.3431191 -4.264338 -4.1251907 -4.0063348 -3.9533575 -3.9547138 -4.0056653 -4.0974689 -4.17214 -4.2031708 -4.2182961 -4.2411656 -4.2736473][-4.2296472 -4.3170509 -4.352777 -4.2948437 -4.1670313 -4.0346828 -3.9619894 -3.9607482 -4.0210018 -4.1259232 -4.2050142 -4.2398925 -4.2729545 -4.3084011 -4.3297658][-4.2217197 -4.3123941 -4.3530011 -4.3049207 -4.1859517 -4.0489931 -3.9708967 -3.9822624 -4.0628242 -4.1786861 -4.2580395 -4.2962122 -4.3407645 -4.376935 -4.3764668][-4.2130923 -4.288373 -4.3173642 -4.2665048 -4.1550965 -4.0315084 -3.9710252 -4.0070734 -4.1063795 -4.2236962 -4.2969942 -4.3299284 -4.3640056 -4.3787813 -4.3535347][-4.177331 -4.2284493 -4.2460341 -4.1977239 -4.1026063 -4.0124612 -3.9932933 -4.0593896 -4.1683989 -4.2732177 -4.331111 -4.346293 -4.3456674 -4.3177886 -4.2665215]]...]
INFO - root - 2017-12-07 07:07:16.431375: step 9510, loss = 2.00, batch loss = 1.92 (11.5 examples/sec; 0.698 sec/batch; 62h:38m:41s remains)
INFO - root - 2017-12-07 07:07:23.522879: step 9520, loss = 2.00, batch loss = 1.91 (11.9 examples/sec; 0.670 sec/batch; 60h:05m:08s remains)
INFO - root - 2017-12-07 07:07:30.534995: step 9530, loss = 2.09, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 62h:35m:53s remains)
INFO - root - 2017-12-07 07:07:37.630442: step 9540, loss = 2.02, batch loss = 1.94 (11.3 examples/sec; 0.707 sec/batch; 63h:28m:13s remains)
INFO - root - 2017-12-07 07:07:44.636961: step 9550, loss = 1.99, batch loss = 1.91 (11.3 examples/sec; 0.709 sec/batch; 63h:36m:10s remains)
INFO - root - 2017-12-07 07:07:51.624044: step 9560, loss = 2.08, batch loss = 2.00 (11.8 examples/sec; 0.675 sec/batch; 60h:35m:28s remains)
INFO - root - 2017-12-07 07:07:58.719116: step 9570, loss = 1.99, batch loss = 1.91 (11.9 examples/sec; 0.672 sec/batch; 60h:17m:20s remains)
INFO - root - 2017-12-07 07:08:05.780331: step 9580, loss = 2.01, batch loss = 1.92 (11.3 examples/sec; 0.709 sec/batch; 63h:37m:13s remains)
INFO - root - 2017-12-07 07:08:12.759806: step 9590, loss = 2.05, batch loss = 1.96 (11.3 examples/sec; 0.710 sec/batch; 63h:42m:30s remains)
INFO - root - 2017-12-07 07:08:19.769075: step 9600, loss = 2.08, batch loss = 1.99 (10.9 examples/sec; 0.732 sec/batch; 65h:40m:49s remains)
2017-12-07 07:08:20.513925: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4715223 -4.504251 -4.5063982 -4.4482017 -4.3781853 -4.3443274 -4.3401814 -4.325613 -4.2925234 -4.282743 -4.2920117 -4.2923222 -4.2900324 -4.2903466 -4.3077416][-4.4824605 -4.5187774 -4.5297046 -4.4725132 -4.3847232 -4.3256769 -4.3040123 -4.2826495 -4.2458148 -4.2292447 -4.2408566 -4.2673349 -4.3061075 -4.3211136 -4.32218][-4.475544 -4.5084519 -4.5249615 -4.479497 -4.3937621 -4.3295374 -4.3055243 -4.2858248 -4.2466965 -4.22155 -4.2254686 -4.267211 -4.3383703 -4.3680553 -4.3565297][-4.4434209 -4.4694195 -4.4899416 -4.4640021 -4.3947597 -4.3362269 -4.3146181 -4.3009858 -4.2700176 -4.250361 -4.2556591 -4.310153 -4.3999114 -4.4362936 -4.4164529][-4.3909292 -4.4114389 -4.4328485 -4.423645 -4.3716917 -4.3215146 -4.3036394 -4.3021693 -4.2942138 -4.2943616 -4.3069792 -4.3616447 -4.4431806 -4.4673972 -4.4339418][-4.3459406 -4.3624406 -4.3729024 -4.3612738 -4.3115492 -4.2612519 -4.2404938 -4.2441249 -4.2602544 -4.2893152 -4.3226194 -4.3827548 -4.45881 -4.4776325 -4.4412956][-4.2899408 -4.311873 -4.3139153 -4.2882166 -4.228055 -4.1795778 -4.15942 -4.1556196 -4.1762505 -4.2218447 -4.2754345 -4.3510771 -4.4386954 -4.4699273 -4.4459295][-4.2482977 -4.2841539 -4.2850957 -4.2457337 -4.1787634 -4.1461983 -4.1398268 -4.1214232 -4.1217103 -4.1612206 -4.2192583 -4.3004227 -4.4018235 -4.4558544 -4.4574585][-4.2059469 -4.2543135 -4.2679911 -4.2335534 -4.1661749 -4.1342468 -4.1199794 -4.0769315 -4.0633192 -4.1161704 -4.19637 -4.288764 -4.3991055 -4.4687533 -4.4923735][-4.1480217 -4.2047148 -4.2415147 -4.2238283 -4.1586838 -4.1141076 -4.07435 -4.0051761 -3.9892476 -4.0728111 -4.18765 -4.295939 -4.4060855 -4.4729996 -4.4994135][-4.1327324 -4.19287 -4.2468433 -4.2479935 -4.1965289 -4.1543074 -4.1057038 -4.0244946 -4.0091209 -4.1047468 -4.2299676 -4.3379416 -4.4295678 -4.4756222 -4.4853649][-4.1989112 -4.2526889 -4.3071518 -4.3136182 -4.2704282 -4.2332077 -4.1857457 -4.1091037 -4.106277 -4.2071743 -4.32693 -4.4228611 -4.4902067 -4.5087495 -4.490263][-4.3351784 -4.3605523 -4.3856277 -4.374176 -4.3314533 -4.3124485 -4.2858744 -4.2265649 -4.2269287 -4.3071613 -4.4020557 -4.4878035 -4.5487032 -4.5573268 -4.5162024][-4.4740005 -4.4647574 -4.4655185 -4.4526258 -4.43203 -4.4434257 -4.4428525 -4.3981309 -4.3871651 -4.430037 -4.4900303 -4.5573578 -4.6087122 -4.6051111 -4.5402179][-4.5838094 -4.5544462 -4.5514278 -4.5580492 -4.5680838 -4.5965066 -4.6019716 -4.5632257 -4.542881 -4.5618882 -4.5984707 -4.6453161 -4.678441 -4.6563373 -4.5682564]]...]
INFO - root - 2017-12-07 07:08:27.599929: step 9610, loss = 2.04, batch loss = 1.96 (11.3 examples/sec; 0.706 sec/batch; 63h:19m:40s remains)
INFO - root - 2017-12-07 07:08:34.631066: step 9620, loss = 2.02, batch loss = 1.93 (11.6 examples/sec; 0.692 sec/batch; 62h:05m:21s remains)
INFO - root - 2017-12-07 07:08:41.700001: step 9630, loss = 2.04, batch loss = 1.95 (11.1 examples/sec; 0.718 sec/batch; 64h:25m:40s remains)
INFO - root - 2017-12-07 07:08:48.788942: step 9640, loss = 2.06, batch loss = 1.97 (10.8 examples/sec; 0.739 sec/batch; 66h:18m:38s remains)
INFO - root - 2017-12-07 07:08:55.740722: step 9650, loss = 2.07, batch loss = 1.98 (11.4 examples/sec; 0.701 sec/batch; 62h:52m:54s remains)
INFO - root - 2017-12-07 07:09:02.806977: step 9660, loss = 2.01, batch loss = 1.93 (11.8 examples/sec; 0.680 sec/batch; 61h:01m:18s remains)
INFO - root - 2017-12-07 07:09:09.823642: step 9670, loss = 2.02, batch loss = 1.94 (11.5 examples/sec; 0.694 sec/batch; 62h:14m:15s remains)
INFO - root - 2017-12-07 07:09:16.878257: step 9680, loss = 2.00, batch loss = 1.92 (11.4 examples/sec; 0.701 sec/batch; 62h:52m:19s remains)
INFO - root - 2017-12-07 07:09:23.945657: step 9690, loss = 2.07, batch loss = 1.99 (11.5 examples/sec; 0.697 sec/batch; 62h:29m:46s remains)
INFO - root - 2017-12-07 07:09:30.986304: step 9700, loss = 2.04, batch loss = 1.96 (11.1 examples/sec; 0.724 sec/batch; 64h:53m:43s remains)
2017-12-07 07:09:31.801961: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6276913 -4.6655359 -4.6729517 -4.6765203 -4.6760678 -4.6656127 -4.64424 -4.6189818 -4.5935979 -4.5745983 -4.5651894 -4.5472488 -4.5042815 -4.4294457 -4.3426771][-4.7164812 -4.7659488 -4.7721319 -4.7775087 -4.7826114 -4.7708821 -4.739964 -4.7080936 -4.6925845 -4.6909733 -4.6955681 -4.6848211 -4.6307869 -4.5256467 -4.3963242][-4.76904 -4.8282781 -4.8176031 -4.8021855 -4.7950182 -4.7739611 -4.7274451 -4.6956949 -4.71543 -4.7577896 -4.7953405 -4.8011084 -4.7401013 -4.6061935 -4.4368134][-4.7450948 -4.8251872 -4.795752 -4.7466383 -4.7112513 -4.6627259 -4.580523 -4.5396762 -4.6070919 -4.7165022 -4.8118706 -4.8547444 -4.7996707 -4.6457677 -4.4499431][-4.6315269 -4.7444248 -4.7175417 -4.6490831 -4.5746074 -4.469017 -4.3210959 -4.2558188 -4.3774061 -4.5736012 -4.7484646 -4.847795 -4.8135371 -4.652307 -4.4448886][-4.5350819 -4.670702 -4.6586585 -4.5779824 -4.4490628 -4.2548437 -4.0122466 -3.9074597 -4.074379 -4.355866 -4.6164484 -4.7846928 -4.7898531 -4.6432323 -4.4400449][-4.5187902 -4.6371384 -4.6310077 -4.5312104 -4.3407636 -4.0509315 -3.7100775 -3.5632153 -3.760004 -4.1033173 -4.4300828 -4.6648283 -4.7294245 -4.6252375 -4.4424429][-4.5120931 -4.5859866 -4.582942 -4.4749422 -4.2378306 -3.8737483 -3.4588189 -3.2949691 -3.5220506 -3.9009414 -4.2654581 -4.5443439 -4.6616559 -4.6067357 -4.4489212][-4.4851265 -4.5138235 -4.52654 -4.4328918 -4.1763639 -3.7779961 -3.3525262 -3.2230239 -3.486094 -3.8737288 -4.2319469 -4.5059385 -4.6329675 -4.6015096 -4.4561682][-4.4281683 -4.4450183 -4.500793 -4.4532213 -4.223165 -3.8519988 -3.4784932 -3.4115968 -3.6943979 -4.0575547 -4.3625312 -4.5736518 -4.6569428 -4.61034 -4.4600782][-4.3579969 -4.3958893 -4.5150075 -4.531672 -4.3595686 -4.0563755 -3.7612793 -3.753139 -4.0384407 -4.3549614 -4.5788622 -4.6969962 -4.7090025 -4.6220226 -4.4566383][-4.3022838 -4.3780828 -4.5559697 -4.63069 -4.5248981 -4.3049836 -4.0903058 -4.1111159 -4.3561845 -4.6009426 -4.7426195 -4.7833719 -4.7390809 -4.6189523 -4.4445167][-4.2715569 -4.3739839 -4.5800104 -4.6921167 -4.6503792 -4.5136046 -4.3709111 -4.3904662 -4.556613 -4.7144742 -4.789834 -4.7899618 -4.7249188 -4.5940218 -4.4229393][-4.2795181 -4.37571 -4.5679445 -4.6958728 -4.71218 -4.647501 -4.5689945 -4.5813727 -4.669066 -4.7499022 -4.7782617 -4.7583652 -4.6875844 -4.5546632 -4.392591][-4.3680182 -4.4428344 -4.5905857 -4.7055082 -4.7533293 -4.7393465 -4.7059879 -4.7073436 -4.737812 -4.7628613 -4.7588897 -4.7252727 -4.6462979 -4.5087585 -4.3567467]]...]
INFO - root - 2017-12-07 07:09:38.745683: step 9710, loss = 1.98, batch loss = 1.90 (11.7 examples/sec; 0.685 sec/batch; 61h:26m:04s remains)
INFO - root - 2017-12-07 07:09:45.857279: step 9720, loss = 2.01, batch loss = 1.93 (11.2 examples/sec; 0.717 sec/batch; 64h:18m:53s remains)
INFO - root - 2017-12-07 07:09:52.991695: step 9730, loss = 2.04, batch loss = 1.95 (10.9 examples/sec; 0.734 sec/batch; 65h:49m:51s remains)
INFO - root - 2017-12-07 07:09:59.978349: step 9740, loss = 2.06, batch loss = 1.97 (11.4 examples/sec; 0.704 sec/batch; 63h:07m:45s remains)
INFO - root - 2017-12-07 07:10:06.956670: step 9750, loss = 2.03, batch loss = 1.94 (10.9 examples/sec; 0.736 sec/batch; 65h:57m:03s remains)
INFO - root - 2017-12-07 07:10:14.009381: step 9760, loss = 2.01, batch loss = 1.92 (12.2 examples/sec; 0.654 sec/batch; 58h:39m:15s remains)
INFO - root - 2017-12-07 07:10:21.106118: step 9770, loss = 2.05, batch loss = 1.97 (11.8 examples/sec; 0.676 sec/batch; 60h:36m:59s remains)
INFO - root - 2017-12-07 07:10:28.145150: step 9780, loss = 1.97, batch loss = 1.88 (11.2 examples/sec; 0.713 sec/batch; 63h:54m:27s remains)
INFO - root - 2017-12-07 07:10:35.180090: step 9790, loss = 2.00, batch loss = 1.91 (10.7 examples/sec; 0.747 sec/batch; 66h:59m:06s remains)
INFO - root - 2017-12-07 07:10:41.912598: step 9800, loss = 2.04, batch loss = 1.95 (11.9 examples/sec; 0.675 sec/batch; 60h:30m:20s remains)
2017-12-07 07:10:42.641549: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4472375 -4.4839764 -4.5264397 -4.5575948 -4.5677433 -4.5565829 -4.5370107 -4.5141311 -4.5013661 -4.5051889 -4.4940615 -4.4661245 -4.4402027 -4.4307761 -4.4290318][-4.4905529 -4.5385303 -4.5881052 -4.6184134 -4.6125884 -4.5774775 -4.5410872 -4.5076523 -4.4888878 -4.4984989 -4.4987669 -4.48528 -4.4769034 -4.4757462 -4.4688816][-4.5501194 -4.6034031 -4.6489725 -4.6675377 -4.6382713 -4.5739641 -4.5142522 -4.4612613 -4.4292836 -4.441257 -4.4640646 -4.4834604 -4.5095387 -4.5238695 -4.5112824][-4.5967979 -4.6462479 -4.6755557 -4.6692066 -4.6081195 -4.5117064 -4.4255438 -4.3515763 -4.3101192 -4.3282928 -4.3778658 -4.4437642 -4.5200834 -4.5594997 -4.54268][-4.6005635 -4.6338882 -4.6389093 -4.5997324 -4.5012083 -4.3731194 -4.2624521 -4.1702313 -4.129705 -4.1657643 -4.2463775 -4.3686266 -4.5073972 -4.5771465 -4.558475][-4.5477567 -4.561254 -4.5433583 -4.4737391 -4.3370352 -4.1775317 -4.043848 -3.9368861 -3.9085488 -3.9756243 -4.0930848 -4.284162 -4.4980941 -4.6032333 -4.5841937][-4.4633803 -4.4662027 -4.4451838 -4.363595 -4.2040219 -4.0223751 -3.8698766 -3.7563992 -3.7472754 -3.8409982 -3.985105 -4.2315993 -4.5060968 -4.6373796 -4.6209507][-4.4187851 -4.4228415 -4.4213719 -4.3483491 -4.1813869 -3.9879014 -3.8213682 -3.7077551 -3.7161384 -3.8185775 -3.9654968 -4.2249041 -4.5107484 -4.6452947 -4.6403122][-4.4499664 -4.4578533 -4.4754553 -4.41941 -4.2648096 -4.082377 -3.9217048 -3.8216686 -3.8432605 -3.9352624 -4.0585284 -4.2795973 -4.5176773 -4.63064 -4.6409521][-4.5273414 -4.5346913 -4.5526614 -4.5114045 -4.3917618 -4.2518611 -4.1300387 -4.0654535 -4.09699 -4.1658659 -4.2453666 -4.3832092 -4.5269184 -4.59772 -4.6167374][-4.5929785 -4.5910468 -4.5920596 -4.5597305 -4.483036 -4.3978162 -4.3287034 -4.3052535 -4.342485 -4.3876681 -4.4221272 -4.4725976 -4.5225244 -4.5494957 -4.5661168][-4.5943336 -4.5798612 -4.564815 -4.5432549 -4.5090842 -4.475605 -4.4540167 -4.4581237 -4.4863176 -4.5048428 -4.5034752 -4.4965472 -4.4922938 -4.49337 -4.5029097][-4.5289431 -4.5067763 -4.4890342 -4.4815941 -4.4806857 -4.4845138 -4.4927626 -4.5043807 -4.5123086 -4.5035834 -4.4808226 -4.453475 -4.4369359 -4.43361 -4.4393206][-4.4287953 -4.4063191 -4.3934565 -4.3945141 -4.4054437 -4.4196782 -4.4322028 -4.4379191 -4.4321079 -4.414001 -4.3908844 -4.3686428 -4.362875 -4.3718534 -4.3865619][-4.3382249 -4.3214283 -4.3132415 -4.3144984 -4.3220925 -4.3305559 -4.3359365 -4.3363318 -4.3298798 -4.3168964 -4.3042717 -4.2958612 -4.3060312 -4.3310781 -4.3594623]]...]
INFO - root - 2017-12-07 07:10:49.731655: step 9810, loss = 2.04, batch loss = 1.96 (11.0 examples/sec; 0.730 sec/batch; 65h:28m:17s remains)
INFO - root - 2017-12-07 07:10:56.855584: step 9820, loss = 2.00, batch loss = 1.91 (11.2 examples/sec; 0.716 sec/batch; 64h:12m:07s remains)
INFO - root - 2017-12-07 07:11:03.883712: step 9830, loss = 2.02, batch loss = 1.94 (11.6 examples/sec; 0.687 sec/batch; 61h:33m:31s remains)
INFO - root - 2017-12-07 07:11:10.968524: step 9840, loss = 1.99, batch loss = 1.90 (11.4 examples/sec; 0.704 sec/batch; 63h:05m:19s remains)
INFO - root - 2017-12-07 07:11:18.097218: step 9850, loss = 2.03, batch loss = 1.95 (12.0 examples/sec; 0.664 sec/batch; 59h:32m:53s remains)
INFO - root - 2017-12-07 07:11:25.096042: step 9860, loss = 2.01, batch loss = 1.93 (10.9 examples/sec; 0.732 sec/batch; 65h:35m:30s remains)
INFO - root - 2017-12-07 07:11:32.135688: step 9870, loss = 2.08, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 63h:14m:06s remains)
INFO - root - 2017-12-07 07:11:39.126823: step 9880, loss = 2.02, batch loss = 1.93 (11.5 examples/sec; 0.694 sec/batch; 62h:13m:02s remains)
INFO - root - 2017-12-07 07:11:46.104030: step 9890, loss = 1.99, batch loss = 1.91 (11.3 examples/sec; 0.709 sec/batch; 63h:30m:23s remains)
INFO - root - 2017-12-07 07:11:53.159808: step 9900, loss = 2.02, batch loss = 1.94 (11.0 examples/sec; 0.729 sec/batch; 65h:21m:34s remains)
2017-12-07 07:11:53.951809: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1450372 -4.0352521 -4.0102324 -4.08812 -4.2454066 -4.3902912 -4.4505925 -4.3861833 -4.304852 -4.2544389 -4.2371435 -4.2694449 -4.3394456 -4.4064336 -4.4349422][-4.1937504 -4.1303043 -4.1240163 -4.1900854 -4.3169713 -4.4329014 -4.4844685 -4.4230247 -4.3423715 -4.28466 -4.2581887 -4.2861753 -4.35366 -4.419498 -4.4515376][-4.3228607 -4.2922125 -4.2837324 -4.3143034 -4.3925343 -4.4719367 -4.5081987 -4.4536457 -4.3832884 -4.3318186 -4.3141341 -4.3471389 -4.4028482 -4.4484076 -4.4640951][-4.4779325 -4.478476 -4.4615974 -4.4473844 -4.4676437 -4.49959 -4.512567 -4.4673495 -4.4170237 -4.3896585 -4.39794 -4.4404421 -4.4760542 -4.4895897 -4.4812703][-4.5604453 -4.5875273 -4.5755897 -4.53703 -4.5083904 -4.4891481 -4.471241 -4.4344859 -4.4126434 -4.4184251 -4.4520426 -4.4943242 -4.4973817 -4.4760985 -4.4531093][-4.5835752 -4.6064382 -4.5869656 -4.522645 -4.4374366 -4.3527031 -4.2963152 -4.2626457 -4.2625723 -4.2988491 -4.3651371 -4.4305034 -4.4329643 -4.397635 -4.3652048][-4.5947003 -4.5746722 -4.5006571 -4.3727045 -4.2040491 -4.047698 -3.9624431 -3.9366512 -3.9595177 -4.0336947 -4.153842 -4.2796383 -4.3267894 -4.3024535 -4.2538118][-4.6030984 -4.5374022 -4.3966389 -4.193769 -3.953517 -3.7669277 -3.699872 -3.6979496 -3.7446492 -3.8545921 -4.0193796 -4.1961474 -4.2854776 -4.2692223 -4.1948223][-4.5731144 -4.4982953 -4.3521986 -4.1480379 -3.9134548 -3.762706 -3.7482114 -3.7696178 -3.8180976 -3.9250565 -4.0780334 -4.2431607 -4.3318648 -4.3079772 -4.20291][-4.5133729 -4.4683652 -4.3849263 -4.258986 -4.098033 -4.0094419 -4.0357876 -4.06695 -4.1014118 -4.1722708 -4.26469 -4.363791 -4.4203553 -4.3943753 -4.28609][-4.4711652 -4.4534473 -4.4352126 -4.3975024 -4.3292489 -4.3020635 -4.3503156 -4.3853135 -4.3998766 -4.4248996 -4.4437761 -4.4670143 -4.4989429 -4.4955649 -4.4232597][-4.5001631 -4.49402 -4.5026054 -4.500658 -4.4813886 -4.4882746 -4.5433497 -4.5844736 -4.5857358 -4.5740628 -4.5342383 -4.4954605 -4.5072751 -4.5356641 -4.5206537][-4.570231 -4.5690827 -4.5748219 -4.5599608 -4.5364442 -4.5455885 -4.5959606 -4.639257 -4.629468 -4.5906663 -4.512054 -4.4338179 -4.4366255 -4.500906 -4.5492311][-4.6066303 -4.6127262 -4.6093 -4.5692987 -4.5253358 -4.5255461 -4.5699515 -4.6130905 -4.598299 -4.5390162 -4.4305453 -4.3322024 -4.3351645 -4.4294753 -4.5276523][-4.5977621 -4.6173959 -4.611733 -4.5551558 -4.4969149 -4.4949813 -4.5429287 -4.5922556 -4.5865269 -4.5201883 -4.3868093 -4.2708573 -4.264441 -4.3572092 -4.4622092]]...]
INFO - root - 2017-12-07 07:12:00.887383: step 9910, loss = 2.01, batch loss = 1.93 (11.3 examples/sec; 0.710 sec/batch; 63h:36m:25s remains)
INFO - root - 2017-12-07 07:12:07.924652: step 9920, loss = 2.07, batch loss = 1.98 (11.3 examples/sec; 0.711 sec/batch; 63h:40m:38s remains)
INFO - root - 2017-12-07 07:12:15.010178: step 9930, loss = 2.07, batch loss = 1.99 (11.3 examples/sec; 0.707 sec/batch; 63h:22m:13s remains)
INFO - root - 2017-12-07 07:12:22.106898: step 9940, loss = 1.96, batch loss = 1.88 (11.8 examples/sec; 0.677 sec/batch; 60h:38m:06s remains)
INFO - root - 2017-12-07 07:12:29.030764: step 9950, loss = 2.04, batch loss = 1.95 (12.0 examples/sec; 0.666 sec/batch; 59h:39m:21s remains)
INFO - root - 2017-12-07 07:12:36.128372: step 9960, loss = 2.06, batch loss = 1.98 (11.3 examples/sec; 0.707 sec/batch; 63h:20m:53s remains)
INFO - root - 2017-12-07 07:12:43.202151: step 9970, loss = 2.04, batch loss = 1.95 (11.4 examples/sec; 0.702 sec/batch; 62h:54m:50s remains)
INFO - root - 2017-12-07 07:12:50.272177: step 9980, loss = 1.98, batch loss = 1.89 (10.9 examples/sec; 0.735 sec/batch; 65h:52m:39s remains)
INFO - root - 2017-12-07 07:12:57.289693: step 9990, loss = 2.03, batch loss = 1.95 (11.7 examples/sec; 0.681 sec/batch; 61h:02m:20s remains)
INFO - root - 2017-12-07 07:13:04.268441: step 10000, loss = 2.07, batch loss = 1.98 (11.8 examples/sec; 0.676 sec/batch; 60h:32m:09s remains)
2017-12-07 07:13:05.030395: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4574342 -4.4881711 -4.5521331 -4.6453185 -4.7308168 -4.77095 -4.7487497 -4.693285 -4.634716 -4.5698419 -4.5039239 -4.4877257 -4.5270343 -4.575789 -4.6165109][-4.5277247 -4.5709481 -4.6488352 -4.755415 -4.8437443 -4.8710041 -4.8275685 -4.7538338 -4.6993041 -4.6660233 -4.6378131 -4.629715 -4.6493945 -4.6699882 -4.6813903][-4.6126 -4.6540771 -4.7197566 -4.8027964 -4.8530912 -4.8322039 -4.7495394 -4.6657171 -4.6414886 -4.6693215 -4.7008896 -4.7202129 -4.73779 -4.7484851 -4.7443][-4.6831079 -4.7019725 -4.7311497 -4.7658796 -4.7539668 -4.6622996 -4.5301623 -4.4448872 -4.474854 -4.5784116 -4.6654992 -4.7085915 -4.73545 -4.7547922 -4.7584796][-4.6973963 -4.685183 -4.6699519 -4.6512694 -4.56981 -4.4014325 -4.2141423 -4.1258445 -4.2157087 -4.395349 -4.5243998 -4.5847831 -4.6378045 -4.6928186 -4.729692][-4.6468182 -4.6170049 -4.5658922 -4.4926925 -4.342514 -4.1044006 -3.8644371 -3.7703648 -3.9151459 -4.1624241 -4.3230424 -4.3967414 -4.484242 -4.5912924 -4.674993][-4.5799046 -4.5581207 -4.4826274 -4.3574429 -4.1513739 -3.8606741 -3.5817828 -3.4834435 -3.6665311 -3.9512897 -4.1229849 -4.1995568 -4.3114862 -4.4613276 -4.5879755][-4.5364108 -4.5520916 -4.4822774 -4.3385224 -4.1164727 -3.8120573 -3.5321336 -3.4498389 -3.6399291 -3.9003406 -4.0288243 -4.0733204 -4.1782684 -4.3397446 -4.4861588][-4.5254354 -4.589694 -4.5563674 -4.4394298 -4.25015 -3.9853373 -3.7516568 -3.7026525 -3.8654921 -4.0490761 -4.0999217 -4.1005192 -4.1846557 -4.3254404 -4.4524741][-4.5040207 -4.6098051 -4.62156 -4.5520515 -4.4169159 -4.22064 -4.0503016 -4.0233626 -4.1347876 -4.2371073 -4.2384534 -4.2342377 -4.323236 -4.4442978 -4.5334134][-4.4347143 -4.5786486 -4.6404157 -4.6245322 -4.5472155 -4.4212112 -4.311511 -4.2888336 -4.3357582 -4.3646536 -4.3387504 -4.3521657 -4.4650235 -4.5843086 -4.6536527][-4.3716192 -4.5349445 -4.6296368 -4.6513548 -4.6162138 -4.5431118 -4.4820151 -4.4589157 -4.4603214 -4.440444 -4.3972549 -4.4140229 -4.5260925 -4.639502 -4.70919][-4.3470216 -4.49434 -4.5902724 -4.6244855 -4.6069975 -4.5598879 -4.5288424 -4.513227 -4.5049963 -4.4774671 -4.4380031 -4.4498858 -4.535779 -4.6296549 -4.7020321][-4.349546 -4.46181 -4.5357957 -4.5612211 -4.54065 -4.5032196 -4.496191 -4.50535 -4.5079169 -4.4800982 -4.4343257 -4.4248672 -4.4789381 -4.5595446 -4.6422191][-4.3581843 -4.4388251 -4.4819832 -4.4894209 -4.4526629 -4.408442 -4.41113 -4.4416971 -4.4501014 -4.4099464 -4.3462434 -4.3172841 -4.3554511 -4.4357767 -4.5327277]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lastaddlayer-adm-0.001/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lastaddlayer-adm-0.001/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 07:13:12.742068: step 10010, loss = 2.02, batch loss = 1.94 (12.1 examples/sec; 0.660 sec/batch; 59h:09m:36s remains)
INFO - root - 2017-12-07 07:13:19.797170: step 10020, loss = 2.04, batch loss = 1.96 (12.0 examples/sec; 0.668 sec/batch; 59h:52m:23s remains)
INFO - root - 2017-12-07 07:13:26.815926: step 10030, loss = 2.04, batch loss = 1.96 (11.5 examples/sec; 0.698 sec/batch; 62h:31m:59s remains)
INFO - root - 2017-12-07 07:13:33.874806: step 10040, loss = 2.05, batch loss = 1.97 (11.0 examples/sec; 0.728 sec/batch; 65h:11m:04s remains)
INFO - root - 2017-12-07 07:13:40.849763: step 10050, loss = 2.02, batch loss = 1.94 (11.4 examples/sec; 0.702 sec/batch; 62h:49m:59s remains)
INFO - root - 2017-12-07 07:13:47.890098: step 10060, loss = 2.06, batch loss = 1.98 (11.9 examples/sec; 0.670 sec/batch; 60h:00m:19s remains)
INFO - root - 2017-12-07 07:13:54.940101: step 10070, loss = 2.05, batch loss = 1.96 (12.4 examples/sec; 0.643 sec/batch; 57h:34m:04s remains)
INFO - root - 2017-12-07 07:14:01.920894: step 10080, loss = 2.03, batch loss = 1.95 (10.9 examples/sec; 0.733 sec/batch; 65h:36m:24s remains)
INFO - root - 2017-12-07 07:14:08.904097: step 10090, loss = 2.05, batch loss = 1.97 (12.1 examples/sec; 0.660 sec/batch; 59h:06m:02s remains)
INFO - root - 2017-12-07 07:14:15.976358: step 10100, loss = 2.03, batch loss = 1.95 (11.9 examples/sec; 0.670 sec/batch; 60h:00m:15s remains)
2017-12-07 07:14:16.729055: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2515273 -4.280849 -4.3196149 -4.3509183 -4.3848109 -4.4185452 -4.4218006 -4.3907938 -4.363966 -4.3520956 -4.3505449 -4.3509512 -4.36644 -4.4010158 -4.4415579][-4.2732396 -4.309814 -4.348897 -4.3784256 -4.415606 -4.4570308 -4.4681959 -4.4400911 -4.4080639 -4.3864713 -4.3753514 -4.3690958 -4.3746357 -4.3945594 -4.4253082][-4.2995057 -4.3345947 -4.3685026 -4.391211 -4.4186492 -4.4472308 -4.4539752 -4.4365568 -4.4240093 -4.4239335 -4.4304371 -4.432776 -4.4308505 -4.4291921 -4.4357491][-4.3149614 -4.3371716 -4.3606377 -4.3734674 -4.3772058 -4.3668122 -4.34593 -4.3370857 -4.3634338 -4.4181762 -4.4743838 -4.5048771 -4.5040469 -4.4827142 -4.4584069][-4.3240638 -4.3197265 -4.3252554 -4.3242531 -4.2960653 -4.2324152 -4.1683555 -4.1632967 -4.2372518 -4.3632865 -4.4873066 -4.5593829 -4.5689521 -4.5379891 -4.4873347][-4.3294005 -4.2965574 -4.2796984 -4.2530642 -4.1762409 -4.0461149 -3.9359365 -3.9346962 -4.0561552 -4.2518592 -4.4449544 -4.56643 -4.5989261 -4.5739779 -4.5149913][-4.3415728 -4.287704 -4.246942 -4.1889029 -4.0629892 -3.8764393 -3.7284563 -3.7258272 -3.8745017 -4.1120338 -4.3518782 -4.5100484 -4.5639191 -4.5524974 -4.5049787][-4.3550019 -4.2924795 -4.2386675 -4.1646204 -4.0151739 -3.7972312 -3.6167748 -3.5941267 -3.7430444 -3.9966977 -4.25941 -4.4353213 -4.4961104 -4.4862647 -4.4511809][-4.3449721 -4.2972984 -4.2569032 -4.1947012 -4.0559659 -3.8372357 -3.6283898 -3.5559936 -3.6695037 -3.9146481 -4.1838794 -4.3686671 -4.43144 -4.4177427 -4.3922186][-4.3215661 -4.3062906 -4.3002896 -4.274292 -4.1826682 -4.0115886 -3.8087149 -3.6869609 -3.7355263 -3.9315343 -4.1688147 -4.3342819 -4.3834338 -4.3585167 -4.3346109][-4.2795658 -4.2961164 -4.334527 -4.3620028 -4.3388486 -4.2409396 -4.0750971 -3.9315956 -3.9205172 -4.0467348 -4.2248516 -4.3449364 -4.3600593 -4.3105702 -4.2734261][-4.2161536 -4.2480483 -4.3261232 -4.41325 -4.4615817 -4.4325662 -4.311615 -4.1726842 -4.1225471 -4.1776137 -4.2823153 -4.3475871 -4.3295221 -4.2630014 -4.2161093][-4.1957531 -4.2105184 -4.2951956 -4.4163427 -4.5153093 -4.5371408 -4.4589963 -4.3452125 -4.2802844 -4.2773838 -4.3070517 -4.3161263 -4.2730522 -4.2027869 -4.1568193][-4.2365441 -4.2209525 -4.2780561 -4.3909445 -4.50034 -4.5452962 -4.501164 -4.4203486 -4.3572674 -4.3136492 -4.281456 -4.2484074 -4.2019644 -4.151504 -4.12595][-4.3155403 -4.2858462 -4.31455 -4.3936806 -4.4782186 -4.5205054 -4.4976506 -4.4383745 -4.367898 -4.2889957 -4.2173743 -4.1734395 -4.1546235 -4.1486259 -4.1575556]]...]
INFO - root - 2017-12-07 07:14:23.844926: step 10110, loss = 2.03, batch loss = 1.94 (10.7 examples/sec; 0.744 sec/batch; 66h:39m:23s remains)
INFO - root - 2017-12-07 07:14:30.967784: step 10120, loss = 2.07, batch loss = 1.98 (11.0 examples/sec; 0.730 sec/batch; 65h:21m:33s remains)
INFO - root - 2017-12-07 07:14:37.951785: step 10130, loss = 2.00, batch loss = 1.91 (11.8 examples/sec; 0.681 sec/batch; 60h:56m:46s remains)
INFO - root - 2017-12-07 07:14:45.079583: step 10140, loss = 1.99, batch loss = 1.91 (10.6 examples/sec; 0.757 sec/batch; 67h:48m:17s remains)
INFO - root - 2017-12-07 07:14:52.138114: step 10150, loss = 2.01, batch loss = 1.93 (11.2 examples/sec; 0.712 sec/batch; 63h:45m:51s remains)
INFO - root - 2017-12-07 07:14:59.086523: step 10160, loss = 2.01, batch loss = 1.93 (11.6 examples/sec; 0.688 sec/batch; 61h:37m:37s remains)
INFO - root - 2017-12-07 07:15:06.201106: step 10170, loss = 2.02, batch loss = 1.94 (11.4 examples/sec; 0.699 sec/batch; 62h:37m:22s remains)
INFO - root - 2017-12-07 07:15:13.299406: step 10180, loss = 2.03, batch loss = 1.94 (11.1 examples/sec; 0.718 sec/batch; 64h:18m:43s remains)
INFO - root - 2017-12-07 07:15:20.351519: step 10190, loss = 2.02, batch loss = 1.94 (11.2 examples/sec; 0.717 sec/batch; 64h:09m:05s remains)
INFO - root - 2017-12-07 07:15:27.432187: step 10200, loss = 2.05, batch loss = 1.96 (10.2 examples/sec; 0.783 sec/batch; 70h:03m:37s remains)
2017-12-07 07:15:28.203253: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.438746 -4.4591665 -4.4772086 -4.5019221 -4.5274677 -4.5378108 -4.5291657 -4.5103359 -4.4973722 -4.5012608 -4.526391 -4.5663371 -4.606154 -4.6271858 -4.60953][-4.5875721 -4.6140032 -4.6249042 -4.6329765 -4.639503 -4.6238627 -4.5807514 -4.5357332 -4.519125 -4.5385962 -4.5808668 -4.6274338 -4.6625071 -4.6693816 -4.6356926][-4.6955137 -4.7220235 -4.7187767 -4.7018681 -4.6752191 -4.6223192 -4.5444021 -4.4859262 -4.4867926 -4.542345 -4.6132989 -4.6662483 -4.6873016 -4.6704288 -4.6229558][-4.7304811 -4.7583065 -4.7413406 -4.6954274 -4.6244688 -4.5222445 -4.4066048 -4.3438945 -4.3809462 -4.4936752 -4.6065507 -4.6670828 -4.6645727 -4.6126919 -4.5520878][-4.7060003 -4.738801 -4.7133923 -4.6398869 -4.5191011 -4.3578668 -4.1959949 -4.1218257 -4.1949134 -4.3739662 -4.5486145 -4.6395082 -4.62692 -4.5410542 -4.4617381][-4.670156 -4.6979508 -4.6564827 -4.553515 -4.3839426 -4.1630545 -3.947648 -3.8479724 -3.9422865 -4.1816282 -4.4297543 -4.5783715 -4.5872841 -4.4906006 -4.3980126][-4.6269979 -4.6378722 -4.5720687 -4.4415584 -4.2423024 -3.9872818 -3.7350965 -3.6061552 -3.7000408 -3.9752595 -4.2824507 -4.4926152 -4.5414486 -4.4574533 -4.3683887][-4.5767984 -4.5822706 -4.503746 -4.3602796 -4.15914 -3.9081531 -3.6524858 -3.5059009 -3.5818431 -3.8552885 -4.1840668 -4.4257278 -4.5006585 -4.435214 -4.359467][-4.5446167 -4.5701456 -4.5020351 -4.3671088 -4.1877918 -3.969579 -3.741446 -3.5969436 -3.64433 -3.877157 -4.1739268 -4.3983541 -4.4714842 -4.4217467 -4.362679][-4.5413017 -4.5966725 -4.5526533 -4.4399824 -4.294127 -4.1202979 -3.9381461 -3.8167932 -3.844008 -4.0189161 -4.2460408 -4.4156914 -4.46799 -4.4315877 -4.3879185][-4.5843916 -4.6444325 -4.6128063 -4.5224524 -4.4123931 -4.2862921 -4.1589279 -4.0771036 -4.09911 -4.2192283 -4.368031 -4.471662 -4.4982605 -4.4735737 -4.4403467][-4.6665421 -4.7021956 -4.6621909 -4.5861225 -4.5087452 -4.428885 -4.3547239 -4.3128037 -4.3355827 -4.4110951 -4.4934549 -4.5436263 -4.55216 -4.5396876 -4.5192447][-4.7381 -4.7342119 -4.6727929 -4.5972166 -4.5394726 -4.4951029 -4.4613295 -4.4489617 -4.4752975 -4.5257478 -4.5724664 -4.599319 -4.60323 -4.6015663 -4.6010327][-4.7493334 -4.7045135 -4.6094503 -4.5149484 -4.4565406 -4.4266539 -4.41361 -4.4236903 -4.4663095 -4.52195 -4.5705791 -4.6067486 -4.6225023 -4.6293011 -4.6402493][-4.6869745 -4.60505 -4.4719954 -4.3430586 -4.2595692 -4.2172251 -4.2078967 -4.2410975 -4.3154206 -4.40042 -4.475049 -4.5432568 -4.5924006 -4.6174641 -4.6299868]]...]
INFO - root - 2017-12-07 07:15:35.347146: step 10210, loss = 2.00, batch loss = 1.91 (11.1 examples/sec; 0.719 sec/batch; 64h:24m:14s remains)
INFO - root - 2017-12-07 07:15:42.430045: step 10220, loss = 2.00, batch loss = 1.91 (11.0 examples/sec; 0.727 sec/batch; 65h:05m:13s remains)
INFO - root - 2017-12-07 07:15:49.380207: step 10230, loss = 2.04, batch loss = 1.96 (11.1 examples/sec; 0.723 sec/batch; 64h:42m:36s remains)
INFO - root - 2017-12-07 07:15:56.366640: step 10240, loss = 2.03, batch loss = 1.94 (10.7 examples/sec; 0.749 sec/batch; 67h:05m:01s remains)
INFO - root - 2017-12-07 07:16:03.341318: step 10250, loss = 2.01, batch loss = 1.93 (11.3 examples/sec; 0.710 sec/batch; 63h:34m:29s remains)
INFO - root - 2017-12-07 07:16:10.334391: step 10260, loss = 2.03, batch loss = 1.94 (12.5 examples/sec; 0.638 sec/batch; 57h:04m:28s remains)
INFO - root - 2017-12-07 07:16:17.328517: step 10270, loss = 2.01, batch loss = 1.92 (11.4 examples/sec; 0.701 sec/batch; 62h:46m:41s remains)
