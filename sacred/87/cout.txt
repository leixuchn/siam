INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "87"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-1
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-06 06:22:23.083783: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 06:22:23.083826: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 06:22:23.083832: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 06:22:23.083836: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 06:22:23.083840: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-06 06:22:30.515321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 6.39GiB
2017-12-06 06:22:30.515380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-06 06:22:30.515387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-06 06:22:30.515400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-06 06:22:53.983783: step 0, loss = 0.65, batch loss = 0.57 (0.5 examples/sec; 17.135 sec/batch; 1582h:36m:19s remains)
2017-12-06 06:22:54.922942: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2533183 -4.347796 -4.5343456 -4.7766347 -4.9624615 -5.011601 -4.942575 -4.8439136 -4.7646632 -4.7106209 -4.668191 -4.608366 -4.4866018 -4.3528643 -4.2927861][-4.36799 -4.495347 -4.7179332 -4.9873152 -5.1669574 -5.1888304 -5.106617 -5.0064268 -4.9127994 -4.8515019 -4.8187103 -4.7453046 -4.5763221 -4.4127336 -4.3425875][-4.4958663 -4.6522751 -4.8891368 -5.112288 -5.1773443 -5.0698833 -4.901123 -4.7347779 -4.5939746 -4.5761266 -4.6639981 -4.6889744 -4.5641437 -4.4300556 -4.3689404][-4.5873723 -4.7536435 -4.9763794 -5.088212 -4.9463043 -4.6077423 -4.2312851 -3.8539338 -3.5826638 -3.6658843 -4.0109692 -4.2847342 -4.35575 -4.3549967 -4.3398962][-4.588254 -4.7518649 -4.9501324 -4.934711 -4.552999 -3.9380662 -3.2802722 -2.6084723 -2.2053719 -2.4625039 -3.1228242 -3.7014205 -4.0534711 -4.2489166 -4.2994118][-4.491003 -4.6505342 -4.819417 -4.658751 -4.0269384 -3.143414 -2.2211993 -1.2871218 -0.84804749 -1.3493078 -2.3238893 -3.180346 -3.7921951 -4.1770563 -4.2824945][-4.2735162 -4.4352026 -4.5736227 -4.3038988 -3.5307415 -2.5421348 -1.5260255 -0.51872373 -0.21644258 -0.97797084 -2.1472888 -3.10609 -3.8046732 -4.2510386 -4.3361006][-3.9956057 -4.1628661 -4.3033924 -4.0546718 -3.3574591 -2.5177379 -1.6792121 -0.88349032 -0.80043912 -1.6418557 -2.7614899 -3.5730884 -4.12941 -4.4597716 -4.4400635][-3.7572181 -3.9094369 -4.0837655 -3.9733195 -3.5134749 -2.9680896 -2.4453278 -1.9849017 -2.0445154 -2.7454586 -3.6122577 -4.1508474 -4.4837046 -4.6338248 -4.5024009][-3.626792 -3.7467866 -3.9617074 -4.02041 -3.8044298 -3.5127854 -3.2570155 -3.0854502 -3.188798 -3.6334896 -4.1929255 -4.51245 -4.7000327 -4.7169356 -4.5089984][-3.6682837 -3.7687039 -4.0048289 -4.1825924 -4.1219826 -3.9538589 -3.8340535 -3.8208182 -3.904387 -4.1026797 -4.4133773 -4.6392732 -4.790195 -4.7422409 -4.4903245][-3.8599491 -3.9942653 -4.2441912 -4.4757133 -4.4698896 -4.314868 -4.2123871 -4.2460318 -4.2918167 -4.3042765 -4.4407668 -4.6383033 -4.7953649 -4.7207842 -4.4563217][-4.1091809 -4.3090625 -4.5776 -4.8264275 -4.8393412 -4.6753073 -4.5536666 -4.56996 -4.5529313 -4.4143372 -4.4308758 -4.61486 -4.7796993 -4.6949368 -4.4309278][-4.2735667 -4.4901733 -4.7440825 -4.99821 -5.0407248 -4.8934631 -4.7767549 -4.781395 -4.7044263 -4.4531431 -4.4001637 -4.5873227 -4.766047 -4.6765981 -4.4060364][-4.1415195 -4.3203216 -4.5341945 -4.7877841 -4.8458481 -4.7090473 -4.6469765 -4.7141242 -4.6480193 -4.3721156 -4.3255272 -4.5440178 -4.7315564 -4.6316109 -4.351305]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-1/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-1/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 06:23:02.030317: step 10, loss = 1.07, batch loss = 1.00 (13.5 examples/sec; 0.592 sec/batch; 54h:38m:50s remains)
INFO - root - 2017-12-06 06:23:07.927883: step 20, loss = 0.91, batch loss = 0.83 (13.9 examples/sec; 0.575 sec/batch; 53h:04m:25s remains)
INFO - root - 2017-12-06 06:23:13.588075: step 30, loss = 0.79, batch loss = 0.72 (13.1 examples/sec; 0.612 sec/batch; 56h:29m:43s remains)
INFO - root - 2017-12-06 06:23:19.449262: step 40, loss = 1.06, batch loss = 0.99 (13.6 examples/sec; 0.589 sec/batch; 54h:25m:08s remains)
INFO - root - 2017-12-06 06:23:25.028576: step 50, loss = 0.97, batch loss = 0.90 (19.0 examples/sec; 0.421 sec/batch; 38h:50m:02s remains)
INFO - root - 2017-12-06 06:23:30.950925: step 60, loss = 1.11, batch loss = 1.04 (13.2 examples/sec; 0.607 sec/batch; 56h:03m:16s remains)
INFO - root - 2017-12-06 06:23:36.776118: step 70, loss = 0.96, batch loss = 0.89 (14.0 examples/sec; 0.570 sec/batch; 52h:38m:47s remains)
INFO - root - 2017-12-06 06:23:42.709614: step 80, loss = 0.92, batch loss = 0.85 (12.9 examples/sec; 0.621 sec/batch; 57h:18m:45s remains)
INFO - root - 2017-12-06 06:23:48.672298: step 90, loss = 0.85, batch loss = 0.78 (13.9 examples/sec; 0.577 sec/batch; 53h:14m:33s remains)
INFO - root - 2017-12-06 06:23:54.489799: step 100, loss = 1.07, batch loss = 1.00 (13.6 examples/sec; 0.588 sec/batch; 54h:16m:04s remains)
2017-12-06 06:23:55.122635: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.69641 -4.8010077 -4.5939136 -4.0748091 -3.5527053 -3.1526482 -2.7871225 -2.53365 -2.6498327 -3.0669661 -3.66125 -4.2631793 -4.6670475 -4.6677279 -4.2566586][-4.6911316 -4.7137561 -4.4488006 -3.8655627 -3.2774162 -2.8826761 -2.5925961 -2.4236953 -2.5579357 -2.9369974 -3.4778891 -4.0654392 -4.4963512 -4.515594 -4.1327686][-4.652535 -4.5763659 -4.2454896 -3.6239855 -3.0053091 -2.587944 -2.2940736 -2.1265359 -2.2762244 -2.6930342 -3.2555952 -3.8346667 -4.2478137 -4.2497015 -3.8673058][-4.6215467 -4.4724178 -4.1081948 -3.4930832 -2.8711376 -2.4054525 -2.0314832 -1.8147874 -1.9997687 -2.5158024 -3.1502745 -3.6954365 -4.01047 -3.9240127 -3.4906652][-4.6266308 -4.4601045 -4.1020617 -3.5213892 -2.8972452 -2.3503156 -1.8388174 -1.513649 -1.6921439 -2.2904351 -3.0042603 -3.5204637 -3.743753 -3.6086802 -3.1837578][-4.6543159 -4.5152979 -4.188355 -3.6317084 -2.9661593 -2.2825723 -1.5822647 -1.08353 -1.1699083 -1.8013544 -2.6123691 -3.1681938 -3.3920856 -3.3120885 -2.987443][-4.6568732 -4.5583229 -4.2583737 -3.6902869 -2.9452105 -2.1070404 -1.2366798 -0.5707581 -0.52414966 -1.1541448 -2.0690036 -2.7271533 -3.0501924 -3.1220107 -2.9402814][-4.6210647 -4.5512533 -4.272779 -3.7015877 -2.9079256 -1.9777346 -1.0274792 -0.26275826 -0.073956013 -0.63991952 -1.5960176 -2.3620305 -2.8391538 -3.1105011 -3.0871921][-4.5678892 -4.5307527 -4.3005972 -3.7799034 -3.017381 -2.0909193 -1.1412466 -0.3290205 -0.0042572021 -0.44640732 -1.3546429 -2.1603031 -2.7387528 -3.1516345 -3.2467213][-4.5011334 -4.4985371 -4.3408327 -3.9077089 -3.2321014 -2.3886402 -1.5026491 -0.69610786 -0.27750492 -0.56724834 -1.3428268 -2.1170766 -2.734386 -3.2109282 -3.3735914][-4.4426112 -4.459024 -4.3723855 -4.0516853 -3.5059979 -2.7867775 -1.9992146 -1.2496347 -0.79790163 -0.93928885 -1.5387385 -2.2339642 -2.8436744 -3.319809 -3.5105329][-4.4237404 -4.4394064 -4.4034038 -4.1899934 -3.7883143 -3.21383 -2.5482278 -1.9122097 -1.5022402 -1.5280468 -1.942091 -2.5138741 -3.0641372 -3.4802065 -3.6510167][-4.4327459 -4.4368391 -4.4226246 -4.284339 -3.9911315 -3.5477765 -3.0228844 -2.5536268 -2.2736237 -2.2831178 -2.5610857 -2.9751484 -3.3847122 -3.6758659 -3.7902243][-4.4625731 -4.4681282 -4.4784956 -4.4020896 -4.1951637 -3.8694689 -3.4965236 -3.2033372 -3.0687137 -3.1123137 -3.3042781 -3.5600815 -3.7941275 -3.943574 -4.0109944][-4.5019846 -4.5168405 -4.5507278 -4.535491 -4.4197907 -4.2152014 -3.98091 -3.8152142 -3.7634835 -3.8158836 -3.946239 -4.0881553 -4.1892481 -4.2393937 -4.2767935]]...]
INFO - root - 2017-12-06 06:24:00.927773: step 110, loss = 1.11, batch loss = 1.04 (13.6 examples/sec; 0.587 sec/batch; 54h:11m:58s remains)
INFO - root - 2017-12-06 06:24:06.644668: step 120, loss = 1.04, batch loss = 0.97 (14.1 examples/sec; 0.566 sec/batch; 52h:14m:54s remains)
INFO - root - 2017-12-06 06:24:12.074798: step 130, loss = 1.14, batch loss = 1.07 (17.9 examples/sec; 0.447 sec/batch; 41h:18m:18s remains)
INFO - root - 2017-12-06 06:24:17.285514: step 140, loss = 0.87, batch loss = 0.80 (15.4 examples/sec; 0.518 sec/batch; 47h:51m:50s remains)
INFO - root - 2017-12-06 06:24:22.351676: step 150, loss = 1.28, batch loss = 1.21 (15.2 examples/sec; 0.527 sec/batch; 48h:40m:49s remains)
INFO - root - 2017-12-06 06:24:27.876107: step 160, loss = 0.94, batch loss = 0.87 (13.4 examples/sec; 0.597 sec/batch; 55h:04m:10s remains)
INFO - root - 2017-12-06 06:24:33.397553: step 170, loss = 0.90, batch loss = 0.83 (15.1 examples/sec; 0.531 sec/batch; 49h:01m:39s remains)
INFO - root - 2017-12-06 06:24:39.026460: step 180, loss = 0.72, batch loss = 0.65 (14.1 examples/sec; 0.568 sec/batch; 52h:25m:43s remains)
INFO - root - 2017-12-06 06:24:44.660904: step 190, loss = 1.01, batch loss = 0.94 (14.7 examples/sec; 0.546 sec/batch; 50h:21m:38s remains)
INFO - root - 2017-12-06 06:24:50.357346: step 200, loss = 0.80, batch loss = 0.73 (13.4 examples/sec; 0.597 sec/batch; 55h:07m:49s remains)
2017-12-06 06:24:50.906947: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5738566 -3.5738416 -3.7394748 -4.0155826 -4.2786207 -4.4199038 -4.4240131 -4.3855143 -4.4206796 -4.5565877 -4.6821032 -4.6814537 -4.528419 -4.2814689 -4.0460262][-3.7763596 -3.7813878 -4.0179195 -4.397758 -4.7115245 -4.7765837 -4.6176696 -4.4829783 -4.5817542 -4.8989644 -5.1940441 -5.27174 -5.0873733 -4.7156038 -4.3249812][-4.2606063 -4.2726278 -4.5365586 -4.9278774 -5.1388197 -4.9278703 -4.4472933 -4.1741161 -4.4078217 -5.0171533 -5.5692635 -5.7903004 -5.6376944 -5.1776419 -4.6370354][-4.8543172 -4.8956633 -5.0989332 -5.3107138 -5.1784387 -4.4749308 -3.575573 -3.1967046 -3.6630976 -4.6740656 -5.5546656 -5.9742432 -5.9307079 -5.46841 -4.8506312][-5.2925868 -5.3294063 -5.33184 -5.1542764 -4.4916959 -3.1873786 -1.8800192 -1.4930501 -2.301826 -3.7902324 -5.0349746 -5.6792936 -5.8111358 -5.4649334 -4.8911467][-5.4096489 -5.3528242 -5.0247035 -4.360558 -3.1584687 -1.3033149 0.31866312 0.59720707 -0.64198089 -2.6069324 -4.181385 -5.0406547 -5.3771791 -5.2171431 -4.7714109][-5.1531706 -4.9555779 -4.3368778 -3.3226657 -1.7719126 0.45800495 2.2833924 2.3933578 0.75954962 -1.542064 -3.3181071 -4.3510404 -4.8855057 -4.9060335 -4.6002259][-4.6760612 -4.3826866 -3.6483979 -2.5868149 -1.0703979 1.1505651 2.9297962 2.8823514 1.0936151 -1.2277491 -2.9401631 -3.9834828 -4.6081362 -4.7252455 -4.500917][-4.07826 -3.8076048 -3.2236042 -2.5080185 -1.4757385 0.29966879 1.7944746 1.6974516 0.086835861 -1.8789465 -3.2211275 -4.0385442 -4.573535 -4.6712322 -4.4630613][-3.4085946 -3.281744 -3.1137404 -3.0198655 -2.6918974 -1.5512414 -0.42310429 -0.49231172 -1.7151349 -3.106751 -3.9232845 -4.36671 -4.67751 -4.6856279 -4.4635997][-2.7689924 -2.8445461 -3.1736836 -3.6773109 -3.942271 -3.3872242 -2.6600709 -2.7301812 -3.5443511 -4.3541031 -4.6818676 -4.7708669 -4.8515382 -4.76558 -4.5235295][-2.4231691 -2.6323905 -3.2761834 -4.0683455 -4.59497 -4.4285493 -4.076623 -4.2069807 -4.719677 -5.1098623 -5.1238346 -5.0030336 -4.9503016 -4.8195081 -4.5802464][-2.6619282 -2.865916 -3.5193188 -4.24682 -4.7188334 -4.698144 -4.5750623 -4.7474995 -5.0680623 -5.227664 -5.1198025 -4.9506435 -4.8729925 -4.753387 -4.5504436][-3.3074486 -3.43473 -3.9077151 -4.3889556 -4.6592588 -4.6258922 -4.57678 -4.7294054 -4.9315834 -4.9904661 -4.8672047 -4.72455 -4.6610141 -4.5721154 -4.422471][-3.9717894 -4.0536666 -4.3116994 -4.5202293 -4.5783129 -4.4835391 -4.4363251 -4.53689 -4.64954 -4.6485596 -4.5322933 -4.4209504 -4.3726854 -4.3188105 -4.230267]]...]
INFO - root - 2017-12-06 06:24:56.620641: step 210, loss = 0.93, batch loss = 0.86 (13.9 examples/sec; 0.576 sec/batch; 53h:10m:00s remains)
INFO - root - 2017-12-06 06:25:02.356982: step 220, loss = 0.82, batch loss = 0.75 (13.5 examples/sec; 0.592 sec/batch; 54h:41m:06s remains)
INFO - root - 2017-12-06 06:25:08.187086: step 230, loss = 0.94, batch loss = 0.87 (13.6 examples/sec; 0.586 sec/batch; 54h:06m:50s remains)
INFO - root - 2017-12-06 06:25:13.746645: step 240, loss = 0.82, batch loss = 0.75 (13.1 examples/sec; 0.611 sec/batch; 56h:23m:58s remains)
INFO - root - 2017-12-06 06:25:19.426587: step 250, loss = 0.96, batch loss = 0.89 (14.4 examples/sec; 0.556 sec/batch; 51h:18m:40s remains)
INFO - root - 2017-12-06 06:25:25.351993: step 260, loss = 0.98, batch loss = 0.91 (13.9 examples/sec; 0.577 sec/batch; 53h:12m:32s remains)
INFO - root - 2017-12-06 06:25:31.256099: step 270, loss = 1.19, batch loss = 1.12 (13.7 examples/sec; 0.582 sec/batch; 53h:42m:08s remains)
INFO - root - 2017-12-06 06:25:37.025987: step 280, loss = 0.80, batch loss = 0.73 (13.6 examples/sec; 0.586 sec/batch; 54h:06m:57s remains)
INFO - root - 2017-12-06 06:25:42.815995: step 290, loss = 0.96, batch loss = 0.89 (14.1 examples/sec; 0.567 sec/batch; 52h:19m:20s remains)
INFO - root - 2017-12-06 06:25:48.619565: step 300, loss = 0.93, batch loss = 0.86 (14.3 examples/sec; 0.561 sec/batch; 51h:43m:40s remains)
2017-12-06 06:25:49.244834: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8731842 -4.1945863 -4.5635896 -4.8007956 -4.7012134 -4.537816 -4.6326427 -4.9522042 -5.3024449 -5.3498678 -5.1677437 -4.9148474 -4.6649008 -4.500946 -4.4076023][-3.8738217 -4.247653 -4.7141719 -5.0380712 -4.9862652 -4.802012 -4.7636123 -4.9731579 -5.3196831 -5.4835563 -5.4875121 -5.3975368 -5.1561151 -4.8151851 -4.4683547][-4.1167917 -4.4479175 -4.8739033 -5.1012397 -4.9239244 -4.6041427 -4.3800845 -4.4337282 -4.7742605 -5.0889859 -5.3099413 -5.4309444 -5.2852154 -4.9004645 -4.455708][-4.4500227 -4.6022716 -4.8165569 -4.7893395 -4.3901019 -3.8905044 -3.4668803 -3.3740089 -3.7860646 -4.326921 -4.8140182 -5.2037463 -5.2351332 -4.9513359 -4.54752][-4.68954 -4.587081 -4.4702339 -4.1035872 -3.4600897 -2.7783432 -2.1546812 -1.9461901 -2.4880555 -3.2793005 -4.0631194 -4.749929 -5.0364537 -5.0212221 -4.8013434][-4.5807104 -4.2646708 -3.8541603 -3.2343178 -2.4734912 -1.6925707 -0.93916535 -0.65436983 -1.2811756 -2.2213011 -3.2381725 -4.1464758 -4.7180109 -5.0804434 -5.1175966][-4.1768227 -3.8139298 -3.281456 -2.5959373 -1.8584621 -1.0866697 -0.32759 -0.023589134 -0.59785175 -1.5106559 -2.5946441 -3.5831676 -4.3894048 -5.1090012 -5.3820868][-3.7758331 -3.5700693 -3.1133618 -2.5362208 -1.9168992 -1.2423425 -0.6201961 -0.37861824 -0.786643 -1.5169001 -2.4560513 -3.2873595 -4.1296806 -5.0191483 -5.42822][-3.59942 -3.6363292 -3.376277 -3.0029874 -2.5516896 -2.0353489 -1.6397116 -1.5138645 -1.7214324 -2.2006655 -2.8272452 -3.2663589 -3.8790586 -4.7012048 -5.1378279][-3.6187565 -3.8482087 -3.806066 -3.6573102 -3.406558 -3.0683641 -2.8722472 -2.8294604 -2.8283362 -3.0322433 -3.2987766 -3.3002334 -3.5589271 -4.1920238 -4.6139741][-3.7843804 -4.0422187 -4.1303988 -4.170773 -4.1536732 -4.0629239 -4.0569849 -4.0456424 -3.8340034 -3.7642136 -3.7417412 -3.4456081 -3.3956995 -3.8338904 -4.2341223][-3.9840965 -4.1197023 -4.2207885 -4.394722 -4.6366081 -4.8676252 -5.0802279 -5.0806608 -4.6981969 -4.3910165 -4.2053523 -3.8284035 -3.6086047 -3.8719125 -4.1988478][-4.2277007 -4.2225971 -4.27543 -4.5094523 -4.9241681 -5.3667183 -5.7011242 -5.694006 -5.2350955 -4.8089247 -4.6045008 -4.33023 -4.0810065 -4.1796732 -4.3648229][-4.6464014 -4.5044317 -4.451911 -4.6661372 -5.131875 -5.6188006 -5.9449272 -5.9184122 -5.4866209 -5.0668411 -4.9258051 -4.8215842 -4.6468506 -4.6285052 -4.6344256][-4.9748173 -4.6945086 -4.5089822 -4.6762261 -5.1524329 -5.6199245 -5.8978577 -5.867959 -5.5060139 -5.1345091 -5.0709395 -5.1510596 -5.1227527 -5.0616612 -4.9054174]]...]
INFO - root - 2017-12-06 06:25:55.038263: step 310, loss = 0.91, batch loss = 0.84 (14.6 examples/sec; 0.549 sec/batch; 50h:37m:09s remains)
INFO - root - 2017-12-06 06:26:00.939557: step 320, loss = 0.76, batch loss = 0.69 (13.4 examples/sec; 0.597 sec/batch; 55h:06m:26s remains)
INFO - root - 2017-12-06 06:26:06.783411: step 330, loss = 0.88, batch loss = 0.81 (13.9 examples/sec; 0.577 sec/batch; 53h:12m:57s remains)
INFO - root - 2017-12-06 06:26:12.562326: step 340, loss = 0.74, batch loss = 0.67 (15.2 examples/sec; 0.526 sec/batch; 48h:30m:12s remains)
INFO - root - 2017-12-06 06:26:18.265865: step 350, loss = 1.02, batch loss = 0.95 (13.6 examples/sec; 0.587 sec/batch; 54h:09m:52s remains)
INFO - root - 2017-12-06 06:26:24.072334: step 360, loss = 1.01, batch loss = 0.94 (14.4 examples/sec; 0.554 sec/batch; 51h:06m:28s remains)
INFO - root - 2017-12-06 06:26:29.869480: step 370, loss = 0.89, batch loss = 0.82 (14.1 examples/sec; 0.566 sec/batch; 52h:15m:33s remains)
INFO - root - 2017-12-06 06:26:35.431068: step 380, loss = 0.81, batch loss = 0.74 (15.0 examples/sec; 0.533 sec/batch; 49h:09m:58s remains)
INFO - root - 2017-12-06 06:26:40.961611: step 390, loss = 0.78, batch loss = 0.71 (14.1 examples/sec; 0.567 sec/batch; 52h:19m:02s remains)
INFO - root - 2017-12-06 06:26:46.362757: step 400, loss = 0.78, batch loss = 0.71 (15.6 examples/sec; 0.512 sec/batch; 47h:16m:10s remains)
2017-12-06 06:26:46.903531: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.22916 -4.2464819 -4.0974541 -4.0057607 -4.2477627 -4.6788497 -4.89042 -4.9558725 -5.1672444 -5.4867272 -5.4851818 -5.0462971 -4.6479626 -4.6413693 -4.7110949][-4.4647841 -4.5046124 -4.3454871 -4.2631578 -4.5508065 -4.9921489 -5.1161408 -5.0686674 -5.2431965 -5.5591674 -5.4810452 -4.9063821 -4.3985119 -4.3782125 -4.4699435][-4.5314198 -4.5312419 -4.3412423 -4.269712 -4.5716882 -4.9455585 -4.9131985 -4.7101169 -4.8554749 -5.2331676 -5.18926 -4.620595 -4.0930457 -4.0602846 -4.1592379][-4.3296647 -4.2048416 -3.909626 -3.8090534 -4.1205993 -4.4633245 -4.3375139 -3.9999769 -4.0877705 -4.5207849 -4.5555906 -4.0721083 -3.6000252 -3.6115968 -3.7753406][-4.1186643 -3.8527589 -3.3770432 -3.13649 -3.3956575 -3.7522013 -3.6674881 -3.3176291 -3.37391 -3.8318985 -3.8998089 -3.4436371 -2.9755335 -3.0309412 -3.3040242][-4.0215197 -3.7225657 -3.1403756 -2.7253585 -2.8374367 -3.1412842 -3.0927281 -2.7798738 -2.8501148 -3.378336 -3.52976 -3.110554 -2.604846 -2.6446228 -2.9735956][-3.9215624 -3.6817658 -3.1001539 -2.5710192 -2.5803428 -2.8828635 -2.87518 -2.5579104 -2.5657921 -3.106811 -3.3761911 -3.0941162 -2.6388516 -2.6584063 -2.9747496][-3.7778516 -3.5602939 -2.9626808 -2.3332419 -2.30549 -2.7582436 -2.9679046 -2.7618008 -2.6998689 -3.1311297 -3.4178834 -3.2707887 -2.9382651 -2.9815016 -3.2674351][-3.6335788 -3.3853421 -2.729311 -1.9827397 -1.9189887 -2.5879326 -3.1376688 -3.198122 -3.1570482 -3.4364872 -3.6445105 -3.5590594 -3.32979 -3.3961654 -3.6443477][-3.5765018 -3.2747536 -2.5455592 -1.6935105 -1.585933 -2.4199855 -3.2460556 -3.5519555 -3.5644281 -3.739974 -3.9113984 -3.9058588 -3.7783279 -3.84683 -4.0226007][-3.8412967 -3.5015247 -2.6696987 -1.6794486 -1.4706261 -2.3348289 -3.2858796 -3.7333341 -3.7920442 -3.9380031 -4.1577539 -4.2712374 -4.2474613 -4.3004522 -4.3645372][-4.3788676 -4.0478377 -3.1801462 -2.0982981 -1.7591803 -2.4887943 -3.3784842 -3.8621995 -3.956306 -4.11551 -4.3932691 -4.604033 -4.6645341 -4.6990805 -4.6594][-4.8806639 -4.5837736 -3.7704105 -2.7033169 -2.2537262 -2.7192378 -3.396651 -3.8296995 -3.9679487 -4.1635847 -4.4648323 -4.7162623 -4.8277869 -4.8532023 -4.7620015][-5.1117611 -4.859705 -4.1976371 -3.3117158 -2.8798823 -3.1126523 -3.5393078 -3.8487921 -3.9848778 -4.1820855 -4.4419465 -4.6648188 -4.7757936 -4.7825537 -4.673625][-5.0963392 -4.91013 -4.4447579 -3.8411379 -3.5410192 -3.6469409 -3.8736176 -4.032371 -4.1114006 -4.2612529 -4.4502945 -4.6161766 -4.6969962 -4.6692772 -4.5433426]]...]
INFO - root - 2017-12-06 06:26:52.014972: step 410, loss = 1.04, batch loss = 0.97 (17.4 examples/sec; 0.461 sec/batch; 42h:31m:27s remains)
INFO - root - 2017-12-06 06:26:57.820600: step 420, loss = 1.05, batch loss = 0.98 (14.2 examples/sec; 0.562 sec/batch; 51h:51m:08s remains)
INFO - root - 2017-12-06 06:27:03.421533: step 430, loss = 1.13, batch loss = 1.06 (14.0 examples/sec; 0.571 sec/batch; 52h:41m:25s remains)
INFO - root - 2017-12-06 06:27:09.197569: step 440, loss = 1.00, batch loss = 0.93 (13.8 examples/sec; 0.582 sec/batch; 53h:39m:44s remains)
INFO - root - 2017-12-06 06:27:14.730708: step 450, loss = 0.86, batch loss = 0.79 (14.1 examples/sec; 0.566 sec/batch; 52h:09m:47s remains)
INFO - root - 2017-12-06 06:27:20.460807: step 460, loss = 0.91, batch loss = 0.84 (14.1 examples/sec; 0.566 sec/batch; 52h:14m:00s remains)
INFO - root - 2017-12-06 06:27:26.292138: step 470, loss = 1.04, batch loss = 0.97 (14.3 examples/sec; 0.560 sec/batch; 51h:37m:39s remains)
INFO - root - 2017-12-06 06:27:32.067895: step 480, loss = 0.74, batch loss = 0.67 (13.6 examples/sec; 0.589 sec/batch; 54h:19m:27s remains)
INFO - root - 2017-12-06 06:27:37.898233: step 490, loss = 0.75, batch loss = 0.68 (14.0 examples/sec; 0.571 sec/batch; 52h:40m:48s remains)
INFO - root - 2017-12-06 06:27:43.689160: step 500, loss = 0.70, batch loss = 0.63 (13.5 examples/sec; 0.592 sec/batch; 54h:38m:25s remains)
2017-12-06 06:27:44.281494: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.118875 -5.7956791 -6.3592849 -6.4828405 -6.2889633 -6.0031586 -5.2633276 -4.3071337 -3.6470535 -3.6063166 -4.0574117 -4.6517286 -5.1649265 -5.3377867 -5.0623441][-4.0940218 -5.053823 -5.979475 -6.3571792 -6.2929468 -6.1041689 -5.2479382 -4.0172935 -3.1630068 -3.1146588 -3.6665387 -4.3363853 -4.8983011 -5.1045671 -4.8821507][-2.9924915 -4.2698178 -5.5358386 -6.1461358 -6.1653371 -6.0096712 -5.0422688 -3.6357088 -2.730525 -2.7680058 -3.4250693 -4.1343279 -4.6968603 -4.9512734 -4.8605366][-2.5936377 -4.0643063 -5.4537678 -6.1015668 -6.0223742 -5.7128849 -4.5829587 -3.0907693 -2.2828422 -2.4930513 -3.2323124 -3.9492738 -4.49708 -4.8477507 -4.9877682][-3.0679841 -4.52766 -5.7708697 -6.2578092 -5.88838 -5.2038479 -3.8045993 -2.2424998 -1.6179075 -2.0884733 -2.9628096 -3.7466474 -4.3235145 -4.8086619 -5.1902747][-4.0340014 -5.2542849 -6.1707582 -6.3674507 -5.5793314 -4.3614807 -2.5750432 -0.90339613 -0.50273132 -1.3327732 -2.4770041 -3.4723823 -4.1942434 -4.8271823 -5.3346157][-5.096746 -5.8597007 -6.3262534 -6.1393785 -4.906404 -3.1647544 -1.0128758 0.70979166 0.76765585 -0.51388979 -2.0314252 -3.3479276 -4.3088202 -5.0660744 -5.53829][-5.754766 -6.016057 -6.0655384 -5.55 -4.026628 -1.9957082 0.30186224 1.9314327 1.5851436 -0.10520124 -1.9279728 -3.5138404 -4.7271557 -5.5794239 -5.9199634][-5.6403089 -5.6209612 -5.5157685 -4.9321036 -3.3917394 -1.3874097 0.76437712 2.1505194 1.4991045 -0.36781406 -2.2736955 -3.9535725 -5.3015776 -6.1453428 -6.304122][-4.9588957 -4.9603119 -5.0055838 -4.6575971 -3.368751 -1.659621 0.041342735 1.0119829 0.27720213 -1.460824 -3.1675503 -4.6536417 -5.8464556 -6.4790621 -6.437345][-4.2962136 -4.4319644 -4.7091265 -4.7039304 -3.83991 -2.5949776 -1.5179279 -1.022243 -1.7001238 -3.1158395 -4.4114466 -5.4625721 -6.2520633 -6.5443339 -6.3471131][-3.8687339 -4.134088 -4.5825329 -4.8739567 -4.4782815 -3.7567565 -3.3131249 -3.2325683 -3.7929487 -4.829978 -5.6484947 -6.1968436 -6.517417 -6.4783821 -6.2122355][-3.697638 -4.1034422 -4.6229033 -5.0255775 -4.9663906 -4.6764269 -4.7309313 -4.9463406 -5.3684874 -6.0567312 -6.463881 -6.6142187 -6.5658283 -6.3084726 -6.0353169][-3.5621626 -4.0170617 -4.500258 -4.9040594 -5.082221 -5.0959225 -5.4309368 -5.7746077 -6.0632482 -6.4629717 -6.5864015 -6.5045204 -6.2594824 -5.9135995 -5.6522331][-3.5139551 -3.8798726 -4.2420731 -4.6105309 -4.9150333 -5.085443 -5.4448662 -5.7175174 -5.8460684 -6.0002809 -5.9567213 -5.7887468 -5.4992933 -5.1784291 -4.955914]]...]
INFO - root - 2017-12-06 06:27:50.087433: step 510, loss = 0.91, batch loss = 0.84 (14.5 examples/sec; 0.552 sec/batch; 50h:56m:00s remains)
INFO - root - 2017-12-06 06:27:55.881127: step 520, loss = 0.92, batch loss = 0.85 (13.4 examples/sec; 0.597 sec/batch; 55h:04m:30s remains)
INFO - root - 2017-12-06 06:28:01.644292: step 530, loss = 0.85, batch loss = 0.78 (13.8 examples/sec; 0.581 sec/batch; 53h:36m:57s remains)
INFO - root - 2017-12-06 06:28:07.398164: step 540, loss = 1.09, batch loss = 1.02 (13.2 examples/sec; 0.608 sec/batch; 56h:05m:45s remains)
INFO - root - 2017-12-06 06:28:13.281154: step 550, loss = 0.84, batch loss = 0.77 (13.0 examples/sec; 0.615 sec/batch; 56h:44m:38s remains)
INFO - root - 2017-12-06 06:28:18.941691: step 560, loss = 0.73, batch loss = 0.66 (13.3 examples/sec; 0.600 sec/batch; 55h:18m:27s remains)
INFO - root - 2017-12-06 06:28:24.795975: step 570, loss = 0.83, batch loss = 0.76 (13.7 examples/sec; 0.583 sec/batch; 53h:46m:25s remains)
INFO - root - 2017-12-06 06:28:30.608401: step 580, loss = 0.97, batch loss = 0.90 (14.2 examples/sec; 0.562 sec/batch; 51h:48m:21s remains)
INFO - root - 2017-12-06 06:28:36.551395: step 590, loss = 0.94, batch loss = 0.87 (14.3 examples/sec; 0.558 sec/batch; 51h:29m:27s remains)
INFO - root - 2017-12-06 06:28:41.950785: step 600, loss = 0.87, batch loss = 0.80 (16.1 examples/sec; 0.497 sec/batch; 45h:48m:43s remains)
2017-12-06 06:28:42.568891: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9865451 -3.8520396 -4.160305 -4.5530462 -4.6712637 -5.0301723 -5.2409143 -4.8857217 -4.54478 -4.1979356 -4.0601773 -4.5981169 -5.0849991 -5.2771029 -5.1971054][-4.1214333 -3.9703603 -4.2348833 -4.693738 -4.9181743 -5.2826643 -5.5320907 -5.2453189 -4.9187694 -4.6322494 -4.5178518 -4.979753 -5.3879642 -5.4566364 -5.2378821][-4.4721079 -4.2974796 -4.5272813 -5.0562983 -5.3905959 -5.7278605 -5.9322443 -5.6760607 -5.3776245 -5.1465254 -5.0058761 -5.258666 -5.4794016 -5.4039388 -5.0498505][-4.6728554 -4.445879 -4.5669365 -5.0440793 -5.4076052 -5.6796823 -5.7804461 -5.5708046 -5.38265 -5.2947478 -5.1991744 -5.2678967 -5.3151708 -5.1568484 -4.75404][-4.386765 -4.0480833 -3.9822497 -4.2578197 -4.5563331 -4.7789855 -4.831181 -4.7468872 -4.7568922 -4.864594 -4.9000449 -4.9376268 -4.9214697 -4.75077 -4.3837633][-3.7120943 -3.1486754 -2.8250418 -2.8388 -3.0436494 -3.2821746 -3.3932464 -3.5265064 -3.8140445 -4.1269026 -4.3048773 -4.4031515 -4.3800521 -4.2079945 -3.903492][-3.1257563 -2.2915554 -1.6478484 -1.337384 -1.3829381 -1.6129646 -1.8116965 -2.2010584 -2.8273723 -3.3714638 -3.6846635 -3.8712459 -3.8748264 -3.7127891 -3.4685493][-3.0134139 -2.1305163 -1.2884984 -0.67546487 -0.4758687 -0.57196069 -0.76034737 -1.2818773 -2.1468546 -2.8750196 -3.2963426 -3.5635393 -3.5989041 -3.4487765 -3.2535424][-3.3245459 -2.694515 -1.9373963 -1.2355525 -0.86928391 -0.80874515 -0.90614438 -1.3690321 -2.227638 -2.9385166 -3.3309522 -3.5804772 -3.6057699 -3.4592104 -3.2970695][-3.780103 -3.4806461 -2.9613802 -2.3687396 -2.0140178 -1.9078557 -1.9443557 -2.2607937 -2.8963232 -3.3950987 -3.625782 -3.7584949 -3.7337432 -3.5821776 -3.4396114][-4.0200825 -3.9630487 -3.688879 -3.302557 -3.1006536 -3.0724802 -3.1129773 -3.2919402 -3.6488709 -3.8869572 -3.9368656 -3.9340308 -3.854892 -3.7078149 -3.5800562][-3.9379425 -3.9826527 -3.8726106 -3.6743572 -3.6604192 -3.7651005 -3.8472939 -3.9399388 -4.0845494 -4.1276975 -4.0600872 -3.9759622 -3.8882041 -3.7709415 -3.6634622][-3.668052 -3.6975074 -3.6439161 -3.5448072 -3.6425903 -3.839231 -3.9697273 -4.0481558 -4.1020808 -4.0610709 -3.9569004 -3.8709733 -3.8312035 -3.7880869 -3.7354147][-3.4749062 -3.4659536 -3.4253106 -3.3610983 -3.4789538 -3.701051 -3.842917 -3.9055548 -3.9282506 -3.8681812 -3.7705135 -3.7162957 -3.7413883 -3.7821856 -3.8009448][-3.5297389 -3.4982891 -3.4556539 -3.392873 -3.4735785 -3.6628346 -3.7705307 -3.8073308 -3.8259411 -3.7821529 -3.697006 -3.6544018 -3.7089324 -3.7930584 -3.8562794]]...]
INFO - root - 2017-12-06 06:28:48.393619: step 610, loss = 0.82, batch loss = 0.75 (13.8 examples/sec; 0.579 sec/batch; 53h:21m:12s remains)
INFO - root - 2017-12-06 06:28:54.162887: step 620, loss = 0.86, batch loss = 0.79 (13.7 examples/sec; 0.582 sec/batch; 53h:40m:36s remains)
INFO - root - 2017-12-06 06:29:00.066587: step 630, loss = 0.83, batch loss = 0.76 (13.3 examples/sec; 0.603 sec/batch; 55h:34m:20s remains)
INFO - root - 2017-12-06 06:29:05.842271: step 640, loss = 1.06, batch loss = 0.99 (13.8 examples/sec; 0.581 sec/batch; 53h:31m:59s remains)
INFO - root - 2017-12-06 06:29:11.182104: step 650, loss = 1.28, batch loss = 1.21 (14.4 examples/sec; 0.555 sec/batch; 51h:10m:12s remains)
INFO - root - 2017-12-06 06:29:16.265229: step 660, loss = 0.78, batch loss = 0.71 (18.0 examples/sec; 0.444 sec/batch; 40h:54m:10s remains)
INFO - root - 2017-12-06 06:29:21.923580: step 670, loss = 1.15, batch loss = 1.08 (14.0 examples/sec; 0.571 sec/batch; 52h:38m:19s remains)
INFO - root - 2017-12-06 06:29:27.666210: step 680, loss = 0.92, batch loss = 0.85 (14.3 examples/sec; 0.560 sec/batch; 51h:39m:31s remains)
INFO - root - 2017-12-06 06:29:33.411543: step 690, loss = 0.97, batch loss = 0.90 (14.4 examples/sec; 0.557 sec/batch; 51h:22m:31s remains)
INFO - root - 2017-12-06 06:29:39.087550: step 700, loss = 0.95, batch loss = 0.88 (17.3 examples/sec; 0.462 sec/batch; 42h:35m:25s remains)
2017-12-06 06:29:39.601193: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6239505 -3.6404719 -3.6116152 -3.552166 -3.4924116 -3.4180896 -3.3301458 -3.2446971 -3.1863675 -3.171103 -3.1682057 -3.0950832 -2.7762079 -2.3553009 -2.1068473][-4.1731405 -4.2125816 -4.1716852 -4.0815859 -3.9852762 -3.8608942 -3.698864 -3.5132146 -3.3439846 -3.2265086 -3.1402926 -2.9877937 -2.5946093 -2.1212356 -1.8354959][-4.5425277 -4.5998816 -4.5795116 -4.5356 -4.5080013 -4.43934 -4.2765689 -4.0213313 -3.7474546 -3.5319791 -3.377655 -3.1705983 -2.7380707 -2.2282281 -1.8842742][-4.4859376 -4.4998851 -4.4827118 -4.5193963 -4.6418505 -4.7308674 -4.6587868 -4.3970809 -4.08164 -3.8537519 -3.7375093 -3.5901246 -3.214582 -2.7108107 -2.2899578][-3.983084 -3.8840539 -3.8294365 -3.9362187 -4.2231269 -4.4956894 -4.5571241 -4.351223 -4.0762768 -3.9479334 -4.0126925 -4.0860796 -3.8951089 -3.4510956 -2.9458561][-3.3577418 -3.0899022 -2.9557519 -3.099381 -3.4992111 -3.8767991 -3.9962988 -3.8080602 -3.586313 -3.6122668 -3.9538858 -4.3794017 -4.5011487 -4.2099042 -3.6744795][-2.7756948 -2.3778007 -2.2020779 -2.3913696 -2.8439679 -3.189456 -3.1959858 -2.887888 -2.6253552 -2.7488322 -3.3416839 -4.1418724 -4.6432037 -4.6001034 -4.1460657][-2.5261691 -2.1023567 -1.9573581 -2.2121751 -2.6753559 -2.899374 -2.69002 -2.1638837 -1.7466705 -1.8342013 -2.5307465 -3.5671496 -4.3556743 -4.5527244 -4.2551208][-2.7553024 -2.4067147 -2.325568 -2.6322095 -3.0939322 -3.218483 -2.8426561 -2.148241 -1.5662732 -1.5166981 -2.1403072 -3.1854396 -4.0406666 -4.3367782 -4.1578197][-3.2056904 -2.9980137 -2.9843369 -3.2964921 -3.7372427 -3.8308332 -3.4353347 -2.7454062 -2.1112099 -1.9325526 -2.3885794 -3.2596762 -3.9603484 -4.1640377 -3.9902244][-3.7138 -3.635133 -3.6613693 -3.9073405 -4.2595949 -4.3301959 -4.0014663 -3.4534991 -2.9120717 -2.7035425 -3.0173132 -3.6733849 -4.1378694 -4.1642971 -3.9312954][-4.0945597 -4.0903244 -4.1297712 -4.2711592 -4.4790716 -4.4846535 -4.2164159 -3.8287628 -3.429801 -3.2534235 -3.4726341 -3.9556544 -4.2324567 -4.1300192 -3.8738334][-4.2242684 -4.229228 -4.2749925 -4.3609667 -4.4817863 -4.4550848 -4.2446084 -3.9685948 -3.6550729 -3.4724774 -3.5863757 -3.9224789 -4.0744224 -3.9249187 -3.7160463][-4.1398921 -4.1275387 -4.1843348 -4.2769365 -4.3953094 -4.3980169 -4.2520123 -4.0512519 -3.774961 -3.5492105 -3.5418699 -3.7403207 -3.8049903 -3.6535938 -3.5208771][-3.8806067 -3.8536689 -3.9158483 -4.0287218 -4.1751766 -4.235621 -4.1639242 -4.0250278 -3.7723987 -3.5118513 -3.4068377 -3.4855633 -3.482935 -3.3396978 -3.2662973]]...]
INFO - root - 2017-12-06 06:29:45.140285: step 710, loss = 0.89, batch loss = 0.82 (14.9 examples/sec; 0.537 sec/batch; 49h:27m:44s remains)
INFO - root - 2017-12-06 06:29:50.765613: step 720, loss = 0.80, batch loss = 0.73 (13.1 examples/sec; 0.609 sec/batch; 56h:09m:11s remains)
INFO - root - 2017-12-06 06:29:56.473897: step 730, loss = 0.80, batch loss = 0.73 (14.4 examples/sec; 0.557 sec/batch; 51h:22m:05s remains)
INFO - root - 2017-12-06 06:30:02.176554: step 740, loss = 1.10, batch loss = 1.03 (13.6 examples/sec; 0.587 sec/batch; 54h:03m:34s remains)
INFO - root - 2017-12-06 06:30:07.779927: step 750, loss = 0.92, batch loss = 0.85 (14.4 examples/sec; 0.556 sec/batch; 51h:13m:30s remains)
INFO - root - 2017-12-06 06:30:13.257757: step 760, loss = 1.03, batch loss = 0.96 (14.1 examples/sec; 0.568 sec/batch; 52h:22m:03s remains)
INFO - root - 2017-12-06 06:30:18.752441: step 770, loss = 0.89, batch loss = 0.82 (13.8 examples/sec; 0.581 sec/batch; 53h:31m:23s remains)
INFO - root - 2017-12-06 06:30:24.503272: step 780, loss = 1.21, batch loss = 1.14 (13.7 examples/sec; 0.585 sec/batch; 53h:52m:32s remains)
INFO - root - 2017-12-06 06:30:30.210623: step 790, loss = 1.05, batch loss = 0.98 (13.8 examples/sec; 0.579 sec/batch; 53h:18m:34s remains)
INFO - root - 2017-12-06 06:30:35.961080: step 800, loss = 1.14, batch loss = 1.07 (17.3 examples/sec; 0.462 sec/batch; 42h:36m:05s remains)
2017-12-06 06:30:36.468087: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9451742 -3.6437542 -3.524616 -3.4557264 -3.2005835 -2.8224835 -2.5787649 -2.5202355 -2.6386876 -2.8825767 -3.2297254 -3.7733486 -4.4162145 -4.8455992 -4.8960781][-4.3878222 -4.0726724 -3.8789771 -3.7333508 -3.4190347 -2.996588 -2.6634994 -2.5093048 -2.5627179 -2.7662711 -3.1238413 -3.6659203 -4.3059363 -4.7420168 -4.7884011][-4.5631723 -4.23372 -3.9872229 -3.7807622 -3.4060185 -2.9244137 -2.4978838 -2.2607465 -2.3130841 -2.5980434 -3.0979438 -3.7209408 -4.3610907 -4.7558613 -4.7136807][-4.3172059 -3.9300766 -3.618453 -3.3469758 -2.9222741 -2.4168408 -1.9768512 -1.7600062 -1.9530728 -2.4933 -3.2541261 -3.9841864 -4.5808606 -4.8586955 -4.6661773][-3.748811 -3.2665496 -2.8673534 -2.5162616 -2.0432067 -1.5100427 -1.060257 -0.90744424 -1.334903 -2.2509403 -3.3531528 -4.2137861 -4.7528062 -4.8738327 -4.4966812][-3.2188091 -2.6432219 -2.1419904 -1.6999283 -1.1790373 -0.58687377 -0.068058968 0.070396423 -0.57013011 -1.8621376 -3.3070273 -4.3322277 -4.8517942 -4.8095045 -4.2008328][-3.0404277 -2.3888664 -1.7800586 -1.2554257 -0.71239853 -0.072984695 0.54507303 0.74974394 0.038575172 -1.4688182 -3.1169386 -4.2409558 -4.7128534 -4.4635253 -3.5768361][-3.3024859 -2.6170435 -1.9281559 -1.3559349 -0.84256935 -0.23878908 0.37845278 0.61368895 -0.035615444 -1.4924159 -3.0792439 -4.1149158 -4.4257112 -3.9276352 -2.8053493][-3.8921566 -3.2419162 -2.529083 -1.9567971 -1.5199652 -1.0402496 -0.55678511 -0.37530804 -0.8915906 -2.0913918 -3.3976378 -4.1963439 -4.2945242 -3.603199 -2.384007][-4.6340113 -4.0953331 -3.4454381 -2.9257989 -2.560605 -2.1766243 -1.7901576 -1.6070876 -1.9225702 -2.7833805 -3.7601979 -4.3458023 -4.3097377 -3.5590594 -2.4054849][-5.3713608 -4.9752417 -4.4492793 -4.0229096 -3.7188346 -3.38462 -3.027482 -2.7797351 -2.8892269 -3.4488285 -4.1638141 -4.6236939 -4.5544019 -3.8581462 -2.851831][-5.8761716 -5.6098881 -5.2338653 -4.9257689 -4.6862764 -4.395772 -4.06754 -3.7917066 -3.7801151 -4.1367164 -4.6612883 -5.022573 -4.949091 -4.3525591 -3.5202863][-5.9448757 -5.7989526 -5.5730305 -5.3784237 -5.1959739 -4.9494839 -4.6776495 -4.4499946 -4.4172435 -4.6523132 -5.0156674 -5.2691326 -5.2032437 -4.7481785 -4.1204858][-5.6164885 -5.5655918 -5.4605823 -5.3533525 -5.221384 -5.0327954 -4.8439341 -4.7012477 -4.6853223 -4.8299313 -5.0431995 -5.1891751 -5.14862 -4.8644705 -4.4432583][-5.0892034 -5.0943909 -5.065341 -5.018445 -4.9392595 -4.8191643 -4.7063322 -4.6225371 -4.60023 -4.6532712 -4.7353854 -4.7898769 -4.773469 -4.6362505 -4.3783679]]...]
INFO - root - 2017-12-06 06:30:41.848715: step 810, loss = 0.91, batch loss = 0.84 (17.4 examples/sec; 0.459 sec/batch; 42h:14m:49s remains)
INFO - root - 2017-12-06 06:30:47.151509: step 820, loss = 0.75, batch loss = 0.68 (14.2 examples/sec; 0.565 sec/batch; 52h:04m:51s remains)
INFO - root - 2017-12-06 06:30:52.863557: step 830, loss = 0.94, batch loss = 0.87 (14.2 examples/sec; 0.565 sec/batch; 52h:01m:11s remains)
INFO - root - 2017-12-06 06:30:58.632793: step 840, loss = 0.80, batch loss = 0.73 (13.4 examples/sec; 0.598 sec/batch; 55h:05m:51s remains)
INFO - root - 2017-12-06 06:31:04.298864: step 850, loss = 0.95, batch loss = 0.88 (14.8 examples/sec; 0.542 sec/batch; 49h:57m:38s remains)
INFO - root - 2017-12-06 06:31:09.847850: step 860, loss = 1.07, batch loss = 1.00 (14.5 examples/sec; 0.552 sec/batch; 50h:52m:10s remains)
INFO - root - 2017-12-06 06:31:15.506142: step 870, loss = 1.00, batch loss = 0.93 (15.0 examples/sec; 0.533 sec/batch; 49h:05m:14s remains)
INFO - root - 2017-12-06 06:31:20.662235: step 880, loss = 0.94, batch loss = 0.87 (14.9 examples/sec; 0.539 sec/batch; 49h:36m:46s remains)
INFO - root - 2017-12-06 06:31:26.240586: step 890, loss = 1.21, batch loss = 1.14 (13.8 examples/sec; 0.578 sec/batch; 53h:13m:37s remains)
INFO - root - 2017-12-06 06:31:31.929731: step 900, loss = 0.70, batch loss = 0.63 (14.0 examples/sec; 0.573 sec/batch; 52h:48m:19s remains)
2017-12-06 06:31:32.507068: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6383791 -3.2943275 -3.1144352 -3.0327992 -3.1985002 -3.7161798 -4.222187 -4.7319503 -5.1738825 -5.1346779 -4.7688251 -4.1292329 -3.8058314 -4.174964 -4.423636][-3.4594598 -3.84176 -4.09288 -4.1199336 -4.0936813 -4.3175926 -4.6385617 -5.0788956 -5.5094376 -5.5181193 -5.0315766 -4.1453686 -3.5974112 -3.7326839 -3.9653866][-3.2688046 -4.4490662 -5.1612549 -5.2002258 -4.7900009 -4.6246128 -4.7749038 -5.178484 -5.7172546 -5.9360619 -5.53613 -4.5689445 -3.88559 -3.8134942 -3.9803882][-3.4322631 -4.963974 -5.8116322 -5.635694 -4.6729341 -4.0119281 -3.9328904 -4.3137307 -5.1204529 -5.7934766 -5.8448853 -5.1795883 -4.5774288 -4.4187989 -4.5407825][-4.2344408 -5.5968008 -6.2107468 -5.5260515 -3.8090787 -2.4980614 -2.041568 -2.3488305 -3.4868829 -4.6992483 -5.3566632 -5.2085505 -4.7478609 -4.4954443 -4.5265169][-5.1581469 -5.9961939 -6.1334782 -4.876967 -2.480351 -0.60693192 0.20217562 -0.084500313 -1.576345 -3.2427812 -4.3936896 -4.644536 -4.13981 -3.6681633 -3.5328987][-5.7200561 -5.8326035 -5.4442544 -3.8592422 -1.1775599 0.94257784 1.9270272 1.5383024 -0.29542637 -2.2500055 -3.6254756 -4.0100713 -3.2744784 -2.4215283 -1.9416091][-5.9177418 -5.52178 -4.8208332 -3.2969499 -0.85568357 1.1305451 2.0950499 1.6064496 -0.37411451 -2.3808031 -3.72221 -4.0136395 -2.9675198 -1.6550457 -0.7055614][-5.7336006 -5.5112295 -5.0318117 -4.0375471 -2.3226068 -0.76507711 0.12281227 -0.20893431 -1.8748615 -3.5910072 -4.6719866 -4.8425093 -3.7352872 -2.2575941 -1.0427611][-5.2399416 -5.6457191 -5.7928324 -5.5605874 -4.6756034 -3.5128498 -2.5739303 -2.4958467 -3.529027 -4.7341681 -5.4669561 -5.6047859 -4.7894959 -3.5259659 -2.3336172][-4.7978883 -5.6840267 -6.28961 -6.556005 -6.2552147 -5.4361119 -4.5556169 -4.1765542 -4.5968547 -5.2461061 -5.5968 -5.6780047 -5.2145858 -4.3354497 -3.4058514][-4.6477327 -5.5377483 -6.1511035 -6.4935751 -6.49459 -6.0561204 -5.4635353 -5.0635529 -5.0885563 -5.2772589 -5.3072333 -5.2835417 -5.038949 -4.511723 -3.9641178][-4.671011 -5.1486964 -5.4473681 -5.6529713 -5.7669973 -5.6295857 -5.3298025 -5.0733271 -4.9633126 -4.884902 -4.7406979 -4.6358643 -4.4912133 -4.1814985 -3.9268773][-5.187645 -5.2095537 -5.1511078 -5.1651435 -5.2306952 -5.1861062 -5.007473 -4.8683548 -4.7513232 -4.5253925 -4.2655258 -4.0821505 -3.9353104 -3.7257133 -3.6734056][-5.7653518 -5.6212316 -5.4001312 -5.2988558 -5.2942972 -5.2398458 -5.0455141 -4.92839 -4.8231678 -4.5077357 -4.1534691 -3.8818347 -3.6981006 -3.5629566 -3.6486442]]...]
INFO - root - 2017-12-06 06:31:38.276712: step 910, loss = 1.08, batch loss = 1.01 (13.1 examples/sec; 0.612 sec/batch; 56h:19m:40s remains)
INFO - root - 2017-12-06 06:31:44.028281: step 920, loss = 1.21, batch loss = 1.14 (14.4 examples/sec; 0.557 sec/batch; 51h:17m:40s remains)
INFO - root - 2017-12-06 06:31:49.613630: step 930, loss = 1.01, batch loss = 0.94 (14.7 examples/sec; 0.544 sec/batch; 50h:04m:10s remains)
INFO - root - 2017-12-06 06:31:55.410023: step 940, loss = 0.86, batch loss = 0.79 (13.6 examples/sec; 0.590 sec/batch; 54h:21m:47s remains)
INFO - root - 2017-12-06 06:32:01.159851: step 950, loss = 0.86, batch loss = 0.79 (15.6 examples/sec; 0.514 sec/batch; 47h:19m:31s remains)
INFO - root - 2017-12-06 06:32:06.942244: step 960, loss = 0.91, batch loss = 0.84 (13.6 examples/sec; 0.589 sec/batch; 54h:12m:10s remains)
INFO - root - 2017-12-06 06:32:12.641106: step 970, loss = 0.99, batch loss = 0.92 (14.3 examples/sec; 0.559 sec/batch; 51h:26m:07s remains)
INFO - root - 2017-12-06 06:32:18.178517: step 980, loss = 0.84, batch loss = 0.77 (13.5 examples/sec; 0.593 sec/batch; 54h:37m:17s remains)
INFO - root - 2017-12-06 06:32:23.891345: step 990, loss = 0.69, batch loss = 0.62 (13.2 examples/sec; 0.604 sec/batch; 55h:39m:36s remains)
INFO - root - 2017-12-06 06:32:29.736573: step 1000, loss = 0.79, batch loss = 0.72 (14.0 examples/sec; 0.573 sec/batch; 52h:43m:13s remains)
2017-12-06 06:32:30.315608: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5829167 -3.6572039 -3.8367643 -4.0512934 -4.2414131 -4.3439655 -4.2834358 -4.0842876 -3.92822 -4.0088859 -4.4057136 -4.8746343 -4.98662 -4.5786676 -4.0952549][-3.7893252 -3.7943859 -3.9411609 -4.1319008 -4.2466054 -4.2294774 -4.0544734 -3.8441179 -3.8362494 -4.1529484 -4.75114 -5.2953496 -5.4161439 -4.9809422 -4.4625039][-3.7309146 -3.7235298 -3.8894217 -4.0542612 -4.0699348 -3.9263422 -3.6676466 -3.4536991 -3.5585723 -4.0457993 -4.8129025 -5.4233971 -5.5209813 -5.0525541 -4.5440717][-3.441889 -3.4885705 -3.735888 -3.91228 -3.8233471 -3.5101109 -3.1462913 -2.9373407 -3.1420331 -3.7424402 -4.6145782 -5.2501011 -5.2859244 -4.7694306 -4.2688003][-2.99464 -3.1427388 -3.4959972 -3.7263205 -3.5542092 -3.0293639 -2.4772522 -2.2152586 -2.4956002 -3.2128935 -4.1833792 -4.8404703 -4.8397522 -4.3315473 -3.9014471][-2.6023507 -2.9163585 -3.3618774 -3.6184053 -3.3419778 -2.5793073 -1.7421975 -1.3194263 -1.6320682 -2.5087488 -3.6555469 -4.3993125 -4.4289303 -3.9976435 -3.6899996][-2.6335423 -3.0688426 -3.5010719 -3.6570148 -3.2046633 -2.2328753 -1.1440778 -0.56047392 -0.89097524 -1.9348743 -3.2762501 -4.1246328 -4.176115 -3.7758436 -3.5321622][-3.0035963 -3.3815632 -3.6719244 -3.6267862 -2.9619684 -1.8624451 -0.71013236 -0.13709307 -0.54103255 -1.7093289 -3.1622241 -4.091867 -4.1816268 -3.8049393 -3.6048865][-3.5129013 -3.6863513 -3.7533436 -3.4733667 -2.6197133 -1.4854813 -0.4947288 -0.15854549 -0.74176693 -1.9766834 -3.3410954 -4.1911192 -4.26947 -3.8969715 -3.7094116][-4.0796008 -4.0329251 -3.881182 -3.4029317 -2.4497595 -1.3686461 -0.64936471 -0.642262 -1.4245539 -2.635035 -3.7379005 -4.3312454 -4.2541356 -3.7572761 -3.4715526][-4.618 -4.4908013 -4.1951404 -3.5524154 -2.5146894 -1.4786994 -0.9911468 -1.3027356 -2.3065012 -3.4950876 -4.3416009 -4.624701 -4.2870445 -3.5796139 -3.1533527][-4.9664316 -4.9083176 -4.5321918 -3.7430623 -2.6184621 -1.6149836 -1.3050778 -1.9044876 -3.1520286 -4.3552051 -4.9924073 -4.9549727 -4.2981811 -3.3664031 -2.8451781][-5.1923647 -5.2942691 -4.86801 -3.9681191 -2.8031726 -1.8355198 -1.5988464 -2.3206251 -3.6493335 -4.7810388 -5.2188654 -4.945713 -4.1177597 -3.1487708 -2.6729326][-5.2774782 -5.4804115 -4.96587 -3.9597096 -2.8121853 -1.9348466 -1.7058122 -2.3492627 -3.5354154 -4.5226841 -4.8404269 -4.5277233 -3.8036642 -3.0404217 -2.734901][-5.1713982 -5.3833542 -4.8016233 -3.7665064 -2.7408948 -2.0274982 -1.7925124 -2.2300472 -3.0993958 -3.8660369 -4.1273994 -3.9173281 -3.443203 -2.9336462 -2.7371297]]...]
INFO - root - 2017-12-06 06:32:35.875983: step 1010, loss = 0.90, batch loss = 0.83 (13.6 examples/sec; 0.587 sec/batch; 54h:05m:15s remains)
INFO - root - 2017-12-06 06:32:41.723149: step 1020, loss = 0.93, batch loss = 0.86 (13.9 examples/sec; 0.574 sec/batch; 52h:52m:32s remains)
INFO - root - 2017-12-06 06:32:47.519996: step 1030, loss = 0.89, batch loss = 0.82 (14.1 examples/sec; 0.566 sec/batch; 52h:04m:41s remains)
INFO - root - 2017-12-06 06:32:53.349037: step 1040, loss = 0.72, batch loss = 0.65 (13.8 examples/sec; 0.579 sec/batch; 53h:20m:40s remains)
INFO - root - 2017-12-06 06:32:59.158227: step 1050, loss = 1.06, batch loss = 0.99 (13.7 examples/sec; 0.583 sec/batch; 53h:40m:57s remains)
INFO - root - 2017-12-06 06:33:04.979603: step 1060, loss = 0.83, batch loss = 0.76 (13.5 examples/sec; 0.595 sec/batch; 54h:45m:33s remains)
INFO - root - 2017-12-06 06:33:10.805076: step 1070, loss = 0.75, batch loss = 0.68 (14.5 examples/sec; 0.553 sec/batch; 50h:53m:34s remains)
INFO - root - 2017-12-06 06:33:16.277424: step 1080, loss = 0.79, batch loss = 0.72 (15.0 examples/sec; 0.532 sec/batch; 49h:01m:06s remains)
INFO - root - 2017-12-06 06:33:21.432369: step 1090, loss = 0.93, batch loss = 0.86 (13.7 examples/sec; 0.585 sec/batch; 53h:53m:07s remains)
INFO - root - 2017-12-06 06:33:27.237405: step 1100, loss = 0.77, batch loss = 0.70 (13.0 examples/sec; 0.615 sec/batch; 56h:34m:27s remains)
2017-12-06 06:33:27.871168: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5167871 -2.6186748 -2.0902004 -1.885289 -2.204828 -2.885211 -3.7987492 -4.5728059 -5.0296941 -5.2460055 -5.2450171 -5.0751271 -4.7249722 -4.1860104 -3.5776918][-4.1353559 -3.2450347 -2.4707179 -1.8893802 -1.8338933 -2.2903681 -3.1453724 -3.989212 -4.61633 -5.0035114 -5.1168795 -5.0508156 -4.7731128 -4.2566185 -3.611768][-4.2139435 -3.295651 -2.2553375 -1.3449428 -1.0743589 -1.4728341 -2.3599255 -3.3396349 -4.2037458 -4.7965946 -5.0074172 -4.9890008 -4.754859 -4.2741013 -3.6369748][-4.3963556 -3.4126158 -2.1208975 -0.9368341 -0.50036287 -0.87259436 -1.7869165 -2.8880498 -3.9901094 -4.7825694 -5.0927763 -5.0985832 -4.8675418 -4.3849711 -3.7393017][-4.9310265 -4.0326662 -2.7066903 -1.4299212 -0.82794666 -1.0299904 -1.7713053 -2.8043423 -3.9673464 -4.8179064 -5.1810794 -5.2196441 -4.9915681 -4.5034595 -3.8593321][-5.2035618 -4.5513444 -3.4108922 -2.2263849 -1.4738088 -1.4159544 -1.9025009 -2.8080497 -3.9649589 -4.7976069 -5.1776991 -5.23948 -5.0024624 -4.5058379 -3.8857467][-5.1812029 -4.7336354 -3.7731066 -2.6948304 -1.7670944 -1.3858242 -1.58553 -2.3812771 -3.5980537 -4.487926 -4.9524179 -5.090023 -4.8946977 -4.4074526 -3.8183432][-5.3368077 -5.1053028 -4.3465834 -3.3508918 -2.1908708 -1.3647478 -1.1263282 -1.6943884 -2.92454 -3.9000816 -4.4928174 -4.7637191 -4.6816835 -4.2622294 -3.7225072][-5.5945606 -5.6290054 -5.1497121 -4.3203278 -3.0569468 -1.897229 -1.2423599 -1.4822187 -2.5579181 -3.4937625 -4.1158032 -4.4656382 -4.4950881 -4.1780005 -3.7007585][-5.618722 -5.8475013 -5.6577616 -5.0845904 -3.9402287 -2.723217 -1.86767 -1.8170285 -2.6145134 -3.3840168 -3.9528253 -4.3176274 -4.4139624 -4.1893363 -3.7710919][-5.2539687 -5.6311417 -5.7079339 -5.4188795 -4.5035224 -3.3966925 -2.4954019 -2.2215643 -2.7057524 -3.2855136 -3.8167124 -4.2003117 -4.3396578 -4.1864538 -3.8214209][-4.6718764 -5.118103 -5.3954473 -5.3729849 -4.7578778 -3.9088378 -3.1095443 -2.6958423 -2.8487823 -3.1751542 -3.6202221 -4.0048566 -4.18205 -4.0883937 -3.7827079][-4.37461 -4.7499123 -5.0650387 -5.1970921 -4.8935871 -4.4010086 -3.8240733 -3.3526237 -3.1943698 -3.2057395 -3.4600234 -3.7732496 -3.9613588 -3.920294 -3.690387][-4.2742987 -4.5113106 -4.7394943 -4.9065204 -4.8746414 -4.7649307 -4.4867969 -4.0746469 -3.7099342 -3.4308989 -3.4273229 -3.5872006 -3.7333419 -3.7205153 -3.5579326][-3.9832077 -4.1431341 -4.2938867 -4.4421463 -4.5880575 -4.7785587 -4.7877326 -4.5128069 -4.0706806 -3.6224096 -3.4004316 -3.3818407 -3.4475989 -3.4471929 -3.3461287]]...]
INFO - root - 2017-12-06 06:33:33.645169: step 1110, loss = 0.88, batch loss = 0.81 (14.7 examples/sec; 0.545 sec/batch; 50h:08m:44s remains)
INFO - root - 2017-12-06 06:33:39.553644: step 1120, loss = 1.16, batch loss = 1.09 (13.4 examples/sec; 0.595 sec/batch; 54h:46m:19s remains)
INFO - root - 2017-12-06 06:33:45.466933: step 1130, loss = 0.88, batch loss = 0.81 (13.3 examples/sec; 0.603 sec/batch; 55h:29m:33s remains)
INFO - root - 2017-12-06 06:33:51.179697: step 1140, loss = 1.03, batch loss = 0.96 (13.5 examples/sec; 0.593 sec/batch; 54h:32m:24s remains)
INFO - root - 2017-12-06 06:33:56.970086: step 1150, loss = 1.03, batch loss = 0.96 (13.8 examples/sec; 0.579 sec/batch; 53h:18m:07s remains)
INFO - root - 2017-12-06 06:34:02.759844: step 1160, loss = 0.84, batch loss = 0.77 (13.8 examples/sec; 0.578 sec/batch; 53h:14m:26s remains)
INFO - root - 2017-12-06 06:34:08.591567: step 1170, loss = 0.84, batch loss = 0.77 (13.9 examples/sec; 0.577 sec/batch; 53h:05m:52s remains)
INFO - root - 2017-12-06 06:34:14.380352: step 1180, loss = 0.98, batch loss = 0.91 (13.5 examples/sec; 0.593 sec/batch; 54h:36m:35s remains)
INFO - root - 2017-12-06 06:34:20.151319: step 1190, loss = 0.88, batch loss = 0.81 (13.9 examples/sec; 0.577 sec/batch; 53h:08m:31s remains)
INFO - root - 2017-12-06 06:34:25.764820: step 1200, loss = 0.76, batch loss = 0.69 (13.9 examples/sec; 0.575 sec/batch; 52h:57m:32s remains)
2017-12-06 06:34:26.378035: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5614767 -4.6145067 -4.8289518 -5.0753107 -5.168324 -5.3376164 -5.4618282 -5.3371181 -5.1151128 -4.8578124 -4.7196321 -4.6552334 -4.7916613 -4.91918 -4.7726207][-4.2899623 -4.3575773 -4.5534739 -4.7952638 -4.8680739 -5.0406666 -5.2068172 -5.1727867 -5.1185026 -5.033875 -4.9822903 -4.8755541 -4.8952508 -4.9054708 -4.7300668][-4.0102773 -4.1508446 -4.3508239 -4.5840559 -4.5939589 -4.6640611 -4.7225194 -4.7253208 -4.8921542 -5.0669432 -5.2107425 -5.1639571 -5.0929823 -4.9285607 -4.6262655][-3.8234224 -4.0609035 -4.3465509 -4.62929 -4.5619955 -4.4061742 -4.1783 -4.1075187 -4.4855909 -4.9718814 -5.4066033 -5.5383348 -5.4594612 -5.1467018 -4.6674232][-3.7507031 -3.9742846 -4.2806215 -4.5944738 -4.4284067 -3.9720473 -3.3446589 -3.1139212 -3.6825886 -4.5366693 -5.3568506 -5.7719407 -5.8028255 -5.4584756 -4.8550406][-3.7357535 -3.8612993 -4.082479 -4.3114738 -3.9456835 -3.145189 -2.0745378 -1.5910265 -2.3203621 -3.5866518 -4.84387 -5.587718 -5.8334374 -5.6308508 -5.0417557][-3.8067479 -3.8782735 -3.9754174 -4.0084057 -3.3614855 -2.2312074 -0.76824093 0.033938408 -0.76665306 -2.3771086 -3.9921324 -5.0362139 -5.5360513 -5.6033115 -5.1909242][-3.9427476 -4.0351515 -4.0549269 -3.9026337 -3.0487452 -1.7470305 -0.11592293 0.96656227 0.24842167 -1.4519632 -3.2031519 -4.425519 -5.1254377 -5.4637828 -5.3245044][-4.0833039 -4.2552886 -4.2724986 -4.063189 -3.2137136 -2.0176053 -0.58227754 0.52593517 0.11190224 -1.2678785 -2.8143489 -4.0175648 -4.7784948 -5.2481623 -5.3136506][-4.2761574 -4.5761118 -4.6653728 -4.5203805 -3.8354023 -2.9011779 -1.8702838 -0.9506886 -0.99293756 -1.752286 -2.7897391 -3.7355573 -4.4102116 -4.8826194 -5.0208898][-4.5397387 -4.9437175 -5.1033382 -5.0120153 -4.4953904 -3.8253114 -3.1985536 -2.5618925 -2.3462698 -2.5023112 -2.9403973 -3.4772716 -3.9573421 -4.3859572 -4.5490022][-4.6831307 -5.1106453 -5.2921023 -5.2286124 -4.8616514 -4.436379 -4.1483326 -3.7926998 -3.4641604 -3.20168 -3.157048 -3.3118024 -3.5926061 -4.00738 -4.2299228][-4.6708651 -4.9999228 -5.1060371 -5.0253568 -4.74962 -4.5034194 -4.4588046 -4.3511415 -4.0445724 -3.620368 -3.3630505 -3.3123517 -3.4660716 -3.9061496 -4.2138844][-4.37623 -4.5458007 -4.5768857 -4.4958191 -4.2886329 -4.1555228 -4.2433314 -4.3226447 -4.1370125 -3.7843084 -3.5696507 -3.4827919 -3.570821 -4.0241971 -4.3849359][-3.9374037 -3.9758227 -4.01582 -4.0174079 -3.9115987 -3.8465257 -3.9371538 -4.09434 -4.0473175 -3.8728442 -3.8329194 -3.8031521 -3.8364925 -4.2367415 -4.5684233]]...]
INFO - root - 2017-12-06 06:34:32.343017: step 1210, loss = 1.05, batch loss = 0.98 (13.5 examples/sec; 0.594 sec/batch; 54h:42m:13s remains)
INFO - root - 2017-12-06 06:34:38.198080: step 1220, loss = 0.76, batch loss = 0.69 (14.6 examples/sec; 0.548 sec/batch; 50h:25m:04s remains)
INFO - root - 2017-12-06 06:34:44.016228: step 1230, loss = 0.72, batch loss = 0.65 (13.7 examples/sec; 0.584 sec/batch; 53h:42m:38s remains)
INFO - root - 2017-12-06 06:34:49.916123: step 1240, loss = 1.01, batch loss = 0.94 (14.1 examples/sec; 0.569 sec/batch; 52h:22m:03s remains)
INFO - root - 2017-12-06 06:34:55.648500: step 1250, loss = 1.12, batch loss = 1.05 (13.8 examples/sec; 0.580 sec/batch; 53h:20m:24s remains)
INFO - root - 2017-12-06 06:35:00.835331: step 1260, loss = 0.90, batch loss = 0.83 (14.6 examples/sec; 0.549 sec/batch; 50h:32m:12s remains)
INFO - root - 2017-12-06 06:35:06.165008: step 1270, loss = 1.01, batch loss = 0.94 (14.0 examples/sec; 0.573 sec/batch; 52h:43m:49s remains)
INFO - root - 2017-12-06 06:35:11.792224: step 1280, loss = 1.17, batch loss = 1.10 (13.7 examples/sec; 0.584 sec/batch; 53h:46m:29s remains)
INFO - root - 2017-12-06 06:35:17.585395: step 1290, loss = 0.93, batch loss = 0.86 (13.9 examples/sec; 0.577 sec/batch; 53h:05m:57s remains)
INFO - root - 2017-12-06 06:35:23.111259: step 1300, loss = 0.87, batch loss = 0.80 (13.7 examples/sec; 0.585 sec/batch; 53h:49m:31s remains)
2017-12-06 06:35:23.667603: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2696748 -4.0240383 -3.7617581 -3.4998615 -3.1410232 -2.6809323 -2.2322049 -1.8602605 -1.9519372 -2.6690347 -3.4373834 -4.1854982 -4.72645 -4.798779 -4.586143][-4.5140653 -4.0987329 -3.7230024 -3.4544177 -3.2184987 -2.9386797 -2.6236856 -2.2857239 -2.3421564 -2.9899535 -3.6251431 -4.2500091 -4.7390356 -4.8115373 -4.6136746][-4.6440177 -4.05609 -3.6007104 -3.3884554 -3.3572001 -3.2932577 -3.0599031 -2.6568556 -2.6009531 -3.1662369 -3.7076302 -4.2513413 -4.713625 -4.8005428 -4.6301546][-4.6045613 -3.8713989 -3.391356 -3.2963629 -3.5056827 -3.6169991 -3.3326025 -2.7221038 -2.4872434 -3.0105853 -3.5677817 -4.1335835 -4.63031 -4.7656188 -4.6356745][-4.4180632 -3.6069074 -3.1529276 -3.1769695 -3.5634322 -3.7160614 -3.2068913 -2.2727771 -1.8552654 -2.4212065 -3.1389561 -3.8643646 -4.4828472 -4.7169857 -4.6432981][-4.2726283 -3.4489994 -3.0180931 -3.0754294 -3.4857519 -3.5185513 -2.6789777 -1.4196715 -0.9021759 -1.5997317 -2.5835395 -3.5625734 -4.34969 -4.6965318 -4.6687913][-4.3337893 -3.5466332 -3.1091733 -3.0889904 -3.3491182 -3.1223526 -1.9394939 -0.47350287 0.000228405 -0.88346648 -2.1393764 -3.363627 -4.2952418 -4.7205243 -4.7075009][-4.4499025 -3.7303457 -3.2872043 -3.1599598 -3.2044883 -2.6886125 -1.2698312 0.20495272 0.49105358 -0.56100154 -1.9721036 -3.320482 -4.3126359 -4.7607441 -4.7357087][-4.5167685 -3.9024839 -3.481277 -3.2956297 -3.1676607 -2.4527931 -1.004761 0.26620197 0.2783761 -0.85826683 -2.2304387 -3.5055227 -4.4237404 -4.8178244 -4.749455][-4.4446917 -3.9656897 -3.6101389 -3.4258831 -3.1993797 -2.4100718 -1.1143057 -0.17807913 -0.44814014 -1.6102748 -2.8229299 -3.8810375 -4.6188025 -4.8981714 -4.7588086][-4.2247958 -3.8705192 -3.6019032 -3.4593484 -3.1898561 -2.4424405 -1.4268312 -0.88072205 -1.4043856 -2.5752554 -3.605104 -4.3961334 -4.8917131 -5.0029655 -4.7665668][-3.9921532 -3.7586136 -3.5945733 -3.5046175 -3.2162523 -2.562458 -1.8463268 -1.6401067 -2.3562624 -3.5343497 -4.4054313 -4.9333739 -5.1656218 -5.0806818 -4.7435141][-3.9186738 -3.7791147 -3.7034068 -3.6477842 -3.3533938 -2.8179252 -2.3556209 -2.357888 -3.1514597 -4.285182 -5.003499 -5.3161273 -5.3322258 -5.0841632 -4.6782956][-4.0836968 -3.9909055 -3.9594889 -3.9190769 -3.6461036 -3.2536888 -2.9919457 -3.0807943 -3.8290224 -4.828063 -5.3690777 -5.5078177 -5.3751783 -5.0203805 -4.5865283][-4.30613 -4.2377639 -4.2310486 -4.2069826 -3.9940419 -3.7367692 -3.6014524 -3.7016153 -4.31623 -5.1164274 -5.4838452 -5.4967284 -5.283587 -4.8907356 -4.4768605]]...]
INFO - root - 2017-12-06 06:35:29.041031: step 1310, loss = 1.27, batch loss = 1.20 (17.0 examples/sec; 0.469 sec/batch; 43h:11m:28s remains)
INFO - root - 2017-12-06 06:35:34.248903: step 1320, loss = 0.83, batch loss = 0.76 (15.1 examples/sec; 0.530 sec/batch; 48h:44m:25s remains)
INFO - root - 2017-12-06 06:35:39.783517: step 1330, loss = 1.22, batch loss = 1.15 (14.2 examples/sec; 0.563 sec/batch; 51h:45m:26s remains)
INFO - root - 2017-12-06 06:35:45.603234: step 1340, loss = 0.90, batch loss = 0.83 (14.0 examples/sec; 0.572 sec/batch; 52h:38m:52s remains)
INFO - root - 2017-12-06 06:35:51.346395: step 1350, loss = 0.88, batch loss = 0.81 (13.8 examples/sec; 0.580 sec/batch; 53h:19m:52s remains)
INFO - root - 2017-12-06 06:35:57.185403: step 1360, loss = 0.99, batch loss = 0.92 (13.3 examples/sec; 0.601 sec/batch; 55h:19m:12s remains)
INFO - root - 2017-12-06 06:36:02.277120: step 1370, loss = 0.73, batch loss = 0.66 (15.2 examples/sec; 0.527 sec/batch; 48h:27m:19s remains)
INFO - root - 2017-12-06 06:36:07.753292: step 1380, loss = 0.90, batch loss = 0.83 (13.9 examples/sec; 0.577 sec/batch; 53h:04m:49s remains)
INFO - root - 2017-12-06 06:36:13.417103: step 1390, loss = 0.80, batch loss = 0.73 (15.4 examples/sec; 0.518 sec/batch; 47h:40m:51s remains)
INFO - root - 2017-12-06 06:36:19.104675: step 1400, loss = 0.87, batch loss = 0.80 (14.8 examples/sec; 0.540 sec/batch; 49h:39m:26s remains)
2017-12-06 06:36:19.804544: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9053488 -5.7980509 -5.8825817 -6.037343 -6.0734906 -6.1173091 -6.3333707 -6.5617638 -6.7872057 -6.9682803 -6.8862433 -6.5196166 -6.0713873 -5.5495381 -4.8425527][-5.6871 -5.3656425 -5.4132276 -5.6889868 -5.8224764 -5.9578967 -6.3358383 -6.7300339 -7.1344738 -7.459713 -7.3766794 -6.9275846 -6.4519825 -5.8684816 -4.9885178][-4.9081182 -4.2226582 -4.1592121 -4.5832925 -4.9198132 -5.2226033 -5.7579756 -6.2704554 -6.8460026 -7.3669949 -7.3664074 -6.94783 -6.55826 -6.009738 -5.0618486][-3.5865421 -2.5414543 -2.4128916 -3.046587 -3.6136909 -4.0055017 -4.52152 -5.01602 -5.7408295 -6.5275345 -6.7257085 -6.4830513 -6.2851744 -5.8780375 -5.0121689][-2.3394954 -1.0646796 -0.92732239 -1.7338209 -2.4014184 -2.6475062 -2.88524 -3.2143478 -4.0648179 -5.1742849 -5.6728749 -5.730052 -5.7837105 -5.5672417 -4.8734775][-1.6670756 -0.27826595 -0.11615658 -0.98480058 -1.5879281 -1.4803731 -1.210701 -1.2386813 -2.1743848 -3.5935009 -4.416225 -4.8334341 -5.1702967 -5.1649594 -4.6766481][-1.6794486 -0.28340673 -0.045117378 -0.81815863 -1.2666771 -0.84597421 -0.1306839 0.13315678 -0.81786323 -2.4075346 -3.4364886 -4.1034031 -4.6449857 -4.81805 -4.5012527][-2.2435417 -1.0426912 -0.74872088 -1.2852476 -1.5171595 -0.95964885 -0.0722518 0.31395388 -0.62750196 -2.2116022 -3.231097 -3.8883224 -4.4314265 -4.657146 -4.414609][-3.0987914 -2.2267418 -1.9602158 -2.2162893 -2.1979079 -1.6488388 -0.86892509 -0.56507063 -1.5298865 -2.9873571 -3.780633 -4.1559973 -4.5000043 -4.6595631 -4.4062161][-4.0714188 -3.5284011 -3.3221793 -3.3032594 -3.0326655 -2.563978 -2.0300426 -1.9090822 -2.8658941 -4.1254611 -4.5996037 -4.6063676 -4.7012486 -4.7476168 -4.4427328][-4.8974495 -4.6147308 -4.4824238 -4.241909 -3.7394805 -3.3114944 -2.9581227 -2.9717917 -3.8456008 -4.9081235 -5.1522894 -4.92319 -4.8591609 -4.8127937 -4.4621425][-5.4429035 -5.3241439 -5.2392063 -4.8541722 -4.1932335 -3.7369306 -3.3902602 -3.408582 -4.1536193 -5.0930915 -5.311594 -5.0536814 -4.9362168 -4.8118939 -4.4194908][-5.8329024 -5.7958984 -5.7107105 -5.2472591 -4.508832 -3.9736679 -3.4904242 -3.403614 -4.0340986 -4.9345059 -5.2518806 -5.0613132 -4.9305959 -4.7471237 -4.3321919][-5.9300823 -5.915514 -5.7961392 -5.3418303 -4.642993 -4.1049867 -3.531852 -3.3219419 -3.844825 -4.7327385 -5.1762705 -5.0698876 -4.9402323 -4.7234564 -4.2914772][-5.6212254 -5.657167 -5.56675 -5.220439 -4.6405268 -4.191865 -3.6745949 -3.4094067 -3.8332109 -4.681685 -5.1948328 -5.1339488 -4.9893126 -4.7577467 -4.3085742]]...]
INFO - root - 2017-12-06 06:36:25.295227: step 1410, loss = 1.06, batch loss = 0.99 (13.1 examples/sec; 0.610 sec/batch; 56h:04m:16s remains)
INFO - root - 2017-12-06 06:36:31.024120: step 1420, loss = 1.02, batch loss = 0.95 (14.1 examples/sec; 0.568 sec/batch; 52h:13m:28s remains)
INFO - root - 2017-12-06 06:36:36.816284: step 1430, loss = 1.03, batch loss = 0.96 (14.0 examples/sec; 0.570 sec/batch; 52h:24m:25s remains)
INFO - root - 2017-12-06 06:36:42.527654: step 1440, loss = 0.83, batch loss = 0.76 (13.3 examples/sec; 0.603 sec/batch; 55h:26m:31s remains)
INFO - root - 2017-12-06 06:36:47.861138: step 1450, loss = 0.85, batch loss = 0.78 (15.6 examples/sec; 0.514 sec/batch; 47h:14m:42s remains)
INFO - root - 2017-12-06 06:36:53.387019: step 1460, loss = 1.08, batch loss = 1.01 (14.1 examples/sec; 0.567 sec/batch; 52h:09m:30s remains)
INFO - root - 2017-12-06 06:36:59.167968: step 1470, loss = 1.06, batch loss = 0.99 (13.8 examples/sec; 0.580 sec/batch; 53h:22m:09s remains)
INFO - root - 2017-12-06 06:37:04.957092: step 1480, loss = 0.90, batch loss = 0.83 (13.9 examples/sec; 0.574 sec/batch; 52h:45m:04s remains)
INFO - root - 2017-12-06 06:37:10.749269: step 1490, loss = 1.06, batch loss = 0.99 (13.2 examples/sec; 0.605 sec/batch; 55h:35m:23s remains)
INFO - root - 2017-12-06 06:37:16.587717: step 1500, loss = 0.99, batch loss = 0.92 (13.5 examples/sec; 0.595 sec/batch; 54h:39m:41s remains)
2017-12-06 06:37:17.133407: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8956413 -3.8605664 -3.7907925 -3.582541 -2.9876962 -2.7428308 -3.0160253 -3.3662612 -4.2648988 -4.8553147 -4.2461123 -4.0091724 -4.3886352 -4.175838 -3.8477356][-4.55628 -4.5034728 -4.3541312 -4.0299606 -3.2789237 -2.9529262 -3.1656172 -3.4444418 -4.4993033 -5.1891136 -4.3047476 -3.9057958 -4.3818536 -4.273756 -4.0309291][-4.9037247 -4.8625836 -4.6719046 -4.2386012 -3.3699646 -3.0919614 -3.3465061 -3.56539 -4.7297244 -5.5301256 -4.46391 -3.9388139 -4.4743724 -4.4960971 -4.3837409][-4.8314867 -4.7916789 -4.561759 -4.02754 -3.0801792 -2.9488192 -3.3480463 -3.5168767 -4.6779547 -5.532238 -4.4048758 -3.8008909 -4.3307939 -4.4972644 -4.5762167][-4.484292 -4.4261742 -4.1300154 -3.4819622 -2.4766736 -2.4631345 -2.9968081 -3.1311173 -4.212059 -5.0782213 -4.0514231 -3.4400189 -3.9107416 -4.2274857 -4.5434742][-4.0971465 -3.97389 -3.564405 -2.7741184 -1.6792586 -1.6897748 -2.3195848 -2.508502 -3.5249014 -4.3874931 -3.58265 -3.0585756 -3.4587343 -3.8887153 -4.4112706][-3.8542609 -3.6169291 -3.0575519 -2.1038487 -0.94359183 -0.99202681 -1.70541 -2.0152953 -3.0283306 -3.8913045 -3.3225822 -2.8735785 -3.1375942 -3.5697048 -4.1975818][-3.7941854 -3.4609566 -2.8583331 -1.9032407 -0.84070635 -0.97738791 -1.6894441 -1.9995031 -2.8877723 -3.6426363 -3.2048903 -2.7481217 -2.8421979 -3.2445273 -3.9393797][-3.9288402 -3.56292 -3.0382814 -2.2190583 -1.3406892 -1.5234637 -2.0969508 -2.2548709 -2.9092636 -3.4794452 -3.05258 -2.5324881 -2.5131598 -2.9575455 -3.7551372][-4.1711583 -3.813941 -3.4038699 -2.7421522 -2.0388415 -2.2217157 -2.5965197 -2.5622749 -2.9716768 -3.3532739 -2.8851557 -2.3191888 -2.2863903 -2.818316 -3.6971464][-4.4262676 -4.0917211 -3.7874274 -3.2704167 -2.7365446 -2.8993468 -3.1146975 -3.0034084 -3.2964261 -3.5479507 -3.0389552 -2.4425697 -2.4293871 -3.0031097 -3.8465085][-4.6174135 -4.3303146 -4.1273837 -3.7768779 -3.4451435 -3.5973675 -3.7388849 -3.6738453 -3.922339 -4.084115 -3.5761404 -2.9694266 -2.9278989 -3.4233475 -4.1226459][-4.6533756 -4.4580069 -4.3350086 -4.1493473 -4.0140939 -4.1473417 -4.2521877 -4.26886 -4.4602609 -4.5431933 -4.1114287 -3.568784 -3.4910583 -3.8510058 -4.3584118][-4.6217241 -4.5101509 -4.4128747 -4.3095574 -4.2833362 -4.3780937 -4.456852 -4.5196834 -4.6503472 -4.6911678 -4.3940749 -3.9868808 -3.9036031 -4.1373572 -4.4655361][-4.6257915 -4.5763397 -4.4855881 -4.39132 -4.37605 -4.42673 -4.469655 -4.5195394 -4.5966115 -4.6316671 -4.4781718 -4.2317872 -4.1793089 -4.3250532 -4.516068]]...]
INFO - root - 2017-12-06 06:37:22.189106: step 1510, loss = 1.03, batch loss = 0.96 (14.6 examples/sec; 0.548 sec/batch; 50h:23m:51s remains)
INFO - root - 2017-12-06 06:37:28.008757: step 1520, loss = 0.86, batch loss = 0.79 (13.1 examples/sec; 0.613 sec/batch; 56h:21m:31s remains)
INFO - root - 2017-12-06 06:37:33.816756: step 1530, loss = 0.83, batch loss = 0.76 (13.5 examples/sec; 0.593 sec/batch; 54h:29m:03s remains)
INFO - root - 2017-12-06 06:37:39.616302: step 1540, loss = 1.11, batch loss = 1.04 (14.6 examples/sec; 0.547 sec/batch; 50h:16m:03s remains)
INFO - root - 2017-12-06 06:37:45.521449: step 1550, loss = 0.90, batch loss = 0.83 (13.3 examples/sec; 0.602 sec/batch; 55h:22m:40s remains)
INFO - root - 2017-12-06 06:37:51.314434: step 1560, loss = 0.80, batch loss = 0.73 (14.3 examples/sec; 0.559 sec/batch; 51h:25m:58s remains)
INFO - root - 2017-12-06 06:37:57.135784: step 1570, loss = 0.75, batch loss = 0.68 (14.5 examples/sec; 0.553 sec/batch; 50h:48m:46s remains)
INFO - root - 2017-12-06 06:38:02.674402: step 1580, loss = 0.85, batch loss = 0.78 (14.1 examples/sec; 0.566 sec/batch; 52h:00m:32s remains)
INFO - root - 2017-12-06 06:38:08.175781: step 1590, loss = 0.92, batch loss = 0.85 (14.3 examples/sec; 0.561 sec/batch; 51h:33m:08s remains)
INFO - root - 2017-12-06 06:38:13.951209: step 1600, loss = 0.79, batch loss = 0.72 (13.5 examples/sec; 0.593 sec/batch; 54h:31m:55s remains)
2017-12-06 06:38:14.526920: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9980536 -5.161027 -5.2299156 -5.110981 -4.8486609 -4.4492273 -4.0869923 -3.953558 -4.1637316 -4.3949366 -4.48647 -4.6376171 -4.7415638 -4.7198029 -4.5318418][-5.1147118 -5.37814 -5.5688853 -5.4748216 -5.1002774 -4.505125 -3.9134674 -3.6408267 -3.9684179 -4.3890867 -4.5842447 -4.8678493 -5.0446973 -4.9801369 -4.6386032][-5.362041 -5.7570362 -6.13205 -6.1337347 -5.6875839 -4.9171462 -4.08906 -3.65471 -4.0849829 -4.7133942 -5.0246029 -5.4014907 -5.6042509 -5.481214 -5.00583][-5.4798951 -5.8684464 -6.3335934 -6.4302592 -5.963232 -5.0451035 -3.9832537 -3.4168913 -3.9496946 -4.7745261 -5.1859446 -5.603085 -5.8314366 -5.7459083 -5.2908516][-5.2811656 -5.477746 -5.81713 -5.8533707 -5.3300776 -4.2858734 -3.0379567 -2.4921827 -3.2889895 -4.4388595 -5.063849 -5.5626507 -5.8255739 -5.8168 -5.4623632][-5.1582575 -5.1108584 -5.1595383 -4.9353905 -4.2245979 -2.9863 -1.5546632 -1.138798 -2.3753207 -4.0309391 -5.0138497 -5.6706448 -5.9590473 -5.9821768 -5.6795115][-5.1792488 -4.944849 -4.6816854 -4.0938864 -3.0402706 -1.4213271 0.34624624 0.7623682 -0.856164 -3.0354674 -4.4583044 -5.399447 -5.8150387 -5.9439211 -5.7235088][-5.1971307 -4.8960638 -4.4906621 -3.6924722 -2.3699772 -0.36794806 1.7850552 2.3720183 0.60574865 -1.9055953 -3.6879015 -4.9282756 -5.5427332 -5.8518677 -5.7321968][-5.4145069 -5.2083483 -4.8741493 -4.0982461 -2.7072165 -0.51471758 1.893096 2.7635217 1.1811042 -1.2991014 -3.1393447 -4.5016994 -5.2734346 -5.767458 -5.7469082][-5.8134947 -5.8194075 -5.6925483 -5.0637312 -3.6985483 -1.4735682 1.0278955 2.1796865 0.96153021 -1.230618 -2.8657484 -4.1537461 -5.0062413 -5.6644292 -5.7264175][-6.1368847 -6.3922448 -6.5638714 -6.1773882 -4.9214387 -2.8013735 -0.39461946 0.8328619 -0.14527273 -2.0422764 -3.3829045 -4.4527555 -5.2521858 -5.9346962 -5.9663448][-6.1648264 -6.5807619 -6.9946585 -6.8458266 -5.7634068 -3.8826351 -1.7545598 -0.69363832 -1.5794678 -3.1863992 -4.2241645 -5.0300412 -5.714859 -6.3415785 -6.2991858][-5.7976389 -6.1800852 -6.6288614 -6.5918 -5.7114573 -4.1864386 -2.4518788 -1.6724274 -2.5107923 -3.8214788 -4.59824 -5.2024846 -5.7962465 -6.373693 -6.3271503][-5.2701058 -5.5448823 -5.91004 -5.8983722 -5.2315569 -4.1128664 -2.8093526 -2.3235183 -3.1119075 -4.1584249 -4.748486 -5.2254853 -5.7323608 -6.2169552 -6.1671352][-4.683311 -4.8538871 -5.1229315 -5.1185293 -4.6361022 -3.8616624 -2.943166 -2.6774647 -3.3469214 -4.1415067 -4.5924621 -4.9920459 -5.4344835 -5.8311443 -5.7995582]]...]
INFO - root - 2017-12-06 06:38:20.367583: step 1610, loss = 0.96, batch loss = 0.89 (13.7 examples/sec; 0.585 sec/batch; 53h:44m:50s remains)
INFO - root - 2017-12-06 06:38:26.030545: step 1620, loss = 0.90, batch loss = 0.82 (13.9 examples/sec; 0.576 sec/batch; 52h:57m:46s remains)
INFO - root - 2017-12-06 06:38:31.883713: step 1630, loss = 1.11, batch loss = 1.03 (13.7 examples/sec; 0.585 sec/batch; 53h:44m:28s remains)
INFO - root - 2017-12-06 06:38:37.733286: step 1640, loss = 0.88, batch loss = 0.81 (13.9 examples/sec; 0.576 sec/batch; 52h:57m:22s remains)
INFO - root - 2017-12-06 06:38:43.492391: step 1650, loss = 0.87, batch loss = 0.80 (13.7 examples/sec; 0.585 sec/batch; 53h:48m:31s remains)
INFO - root - 2017-12-06 06:38:47.973877: step 1660, loss = 0.90, batch loss = 0.83 (19.5 examples/sec; 0.411 sec/batch; 37h:47m:34s remains)
INFO - root - 2017-12-06 06:38:52.129493: step 1670, loss = 0.88, batch loss = 0.81 (19.6 examples/sec; 0.408 sec/batch; 37h:31m:11s remains)
INFO - root - 2017-12-06 06:38:56.364176: step 1680, loss = 1.03, batch loss = 0.96 (19.9 examples/sec; 0.403 sec/batch; 37h:01m:29s remains)
INFO - root - 2017-12-06 06:39:00.530004: step 1690, loss = 0.85, batch loss = 0.78 (19.3 examples/sec; 0.415 sec/batch; 38h:08m:04s remains)
INFO - root - 2017-12-06 06:39:04.718025: step 1700, loss = 0.82, batch loss = 0.75 (19.3 examples/sec; 0.415 sec/batch; 38h:06m:54s remains)
2017-12-06 06:39:05.260957: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9477658 -2.6084976 -2.9796224 -3.9908631 -5.0795379 -5.8913713 -6.1921511 -5.7879581 -5.0862374 -4.578299 -4.418716 -4.3988214 -4.2884378 -4.0172782 -3.7639689][-2.2541311 -1.7606688 -2.083529 -3.2569466 -4.5910459 -5.5378036 -5.8701954 -5.5514631 -5.0850773 -4.8883619 -4.9921532 -5.119688 -5.0031638 -4.6505213 -4.3210888][-1.801815 -1.3541002 -1.7495995 -3.0523591 -4.4904876 -5.3230147 -5.3982105 -5.0136938 -4.7921729 -4.9679551 -5.394238 -5.7354078 -5.7009096 -5.3645029 -5.06247][-1.6998401 -1.5199022 -2.0965014 -3.4270868 -4.7258816 -5.1314468 -4.638669 -4.005794 -3.9852691 -4.5575604 -5.3704987 -6.03047 -6.2095079 -5.9902153 -5.7499347][-1.9602473 -2.1097364 -2.8239503 -3.9495649 -4.7443404 -4.4246097 -3.2366056 -2.346699 -2.5629478 -3.5978441 -4.8675251 -5.94052 -6.421669 -6.3381567 -6.1002769][-2.6539245 -3.0140224 -3.6200457 -4.1970615 -4.1750293 -3.0641532 -1.3268504 -0.29801559 -0.81316423 -2.3448238 -4.0791407 -5.5504212 -6.2859735 -6.2405691 -5.9082026][-3.6394649 -3.9755824 -4.2315216 -4.1176448 -3.3229115 -1.7145035 0.19153261 1.1616173 0.34696245 -1.552042 -3.5454652 -5.1487608 -5.8843203 -5.7250185 -5.2818403][-4.4937596 -4.6433368 -4.5310941 -3.9355025 -2.8045914 -1.2508857 0.31569147 0.97374535 0.039183617 -1.8335202 -3.6716995 -4.9758058 -5.3754787 -5.0070877 -4.5388765][-4.854764 -4.845912 -4.5646758 -3.8784475 -2.8879259 -1.7952402 -0.84408903 -0.57191777 -1.4408088 -2.9849296 -4.3839808 -5.1121492 -4.9770317 -4.3628693 -3.9063513][-4.7370253 -4.7441983 -4.5335574 -4.03722 -3.4006481 -2.8344774 -2.449522 -2.4986651 -3.2335565 -4.3551955 -5.2430344 -5.3947587 -4.7931738 -3.9771173 -3.4902353][-4.6470151 -4.7639084 -4.7257857 -4.4881368 -4.181848 -4.0032291 -4.0062466 -4.24079 -4.789259 -5.4772167 -5.8925128 -5.6362729 -4.7840734 -3.863297 -3.3131933][-4.906404 -5.0791936 -5.1631513 -5.1016374 -4.9990807 -5.0307603 -5.2033696 -5.4402218 -5.741055 -6.0262346 -6.0564642 -5.578032 -4.6915174 -3.8116894 -3.2740362][-5.093596 -5.2324934 -5.3555684 -5.3884926 -5.3839111 -5.4579782 -5.5944805 -5.6907978 -5.7348552 -5.7188067 -5.535182 -5.046144 -4.3181949 -3.6269975 -3.2277136][-4.8068829 -4.8780255 -4.9927573 -5.0724015 -5.1043444 -5.1385255 -5.1598291 -5.1085358 -4.9868741 -4.8258123 -4.6033816 -4.2559924 -3.79577 -3.3757319 -3.1721153][-4.20434 -4.2296886 -4.3215766 -4.4152732 -4.4555488 -4.4426074 -4.3796577 -4.2652988 -4.1119952 -3.9476483 -3.7949476 -3.6237788 -3.4114211 -3.2301946 -3.18351]]...]
INFO - root - 2017-12-06 06:39:09.445694: step 1710, loss = 0.98, batch loss = 0.91 (19.5 examples/sec; 0.409 sec/batch; 37h:36m:18s remains)
INFO - root - 2017-12-06 06:39:13.635963: step 1720, loss = 1.08, batch loss = 1.01 (19.3 examples/sec; 0.414 sec/batch; 38h:04m:18s remains)
INFO - root - 2017-12-06 06:39:17.515012: step 1730, loss = 0.94, batch loss = 0.87 (19.2 examples/sec; 0.417 sec/batch; 38h:20m:32s remains)
INFO - root - 2017-12-06 06:39:21.730208: step 1740, loss = 0.94, batch loss = 0.87 (18.5 examples/sec; 0.433 sec/batch; 39h:44m:54s remains)
INFO - root - 2017-12-06 06:39:25.902469: step 1750, loss = 1.08, batch loss = 1.01 (19.5 examples/sec; 0.411 sec/batch; 37h:45m:45s remains)
INFO - root - 2017-12-06 06:39:30.014623: step 1760, loss = 1.15, batch loss = 1.08 (19.8 examples/sec; 0.405 sec/batch; 37h:10m:55s remains)
INFO - root - 2017-12-06 06:39:34.209855: step 1770, loss = 0.86, batch loss = 0.79 (18.9 examples/sec; 0.423 sec/batch; 38h:54m:03s remains)
INFO - root - 2017-12-06 06:39:38.404682: step 1780, loss = 0.96, batch loss = 0.89 (19.7 examples/sec; 0.406 sec/batch; 37h:16m:53s remains)
INFO - root - 2017-12-06 06:39:42.683967: step 1790, loss = 0.83, batch loss = 0.76 (18.3 examples/sec; 0.437 sec/batch; 40h:10m:15s remains)
INFO - root - 2017-12-06 06:39:46.891121: step 1800, loss = 0.81, batch loss = 0.74 (18.2 examples/sec; 0.440 sec/batch; 40h:24m:02s remains)
2017-12-06 06:39:47.473281: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3577738 -4.7150726 -4.9028506 -4.9653044 -4.8412476 -4.6388664 -4.50649 -4.5067096 -4.5003967 -4.3027725 -4.1028166 -4.1075544 -4.2376819 -4.3011093 -4.2234435][-3.8421197 -4.16652 -4.3807836 -4.4786153 -4.4750948 -4.4336505 -4.4085879 -4.3963 -4.3402305 -4.0930796 -3.7931991 -3.6977365 -3.7556851 -3.7819333 -3.7117405][-3.3619509 -3.6643524 -3.9163976 -4.0731883 -4.1961818 -4.362433 -4.5214953 -4.5566216 -4.4799647 -4.2272139 -3.8962641 -3.7398119 -3.7206066 -3.6967146 -3.64718][-2.9481924 -3.2696011 -3.5786719 -3.8320434 -4.0681934 -4.4149356 -4.7588711 -4.8931384 -4.8801618 -4.6834579 -4.3949351 -4.2121258 -4.0952859 -3.9876788 -3.9470274][-2.7385006 -3.0566661 -3.3978155 -3.7196355 -4.0146551 -4.4543204 -4.9035625 -5.1534228 -5.29147 -5.2053275 -4.9582462 -4.7232533 -4.4567871 -4.2224741 -4.1670079][-2.8189712 -3.0562556 -3.3384421 -3.598263 -3.7884731 -4.1351724 -4.5628324 -4.9245014 -5.2691693 -5.3222551 -5.1089416 -4.7982759 -4.3684278 -3.9803634 -3.9022634][-3.1346998 -3.2580562 -3.378397 -3.4152193 -3.3189278 -3.363657 -3.6320925 -4.0753965 -4.6410561 -4.8754711 -4.7381325 -4.3852048 -3.8227901 -3.2650058 -3.1183898][-3.3166924 -3.3497772 -3.2877305 -3.0691662 -2.6534121 -2.3002181 -2.2989233 -2.7335815 -3.4669559 -3.9306908 -3.9687979 -3.6838918 -3.0708413 -2.3578787 -2.0735273][-3.1168633 -3.1440396 -3.0100961 -2.6537905 -2.0340509 -1.3232565 -0.937135 -1.2057502 -2.0315588 -2.7037878 -2.9748342 -2.9062948 -2.4256074 -1.677017 -1.241816][-2.5966451 -2.7213378 -2.6581545 -2.320416 -1.6887691 -0.85399151 -0.18442869 -0.24909019 -1.0737777 -1.8277245 -2.1987557 -2.3095138 -2.0466051 -1.4090154 -0.92047358][-2.1754932 -2.381134 -2.4044061 -2.1697991 -1.7172284 -1.0926449 -0.46297026 -0.41826296 -1.1160142 -1.7726295 -2.0347219 -2.1214457 -2.0221424 -1.6194015 -1.2492979][-2.1185012 -2.2933059 -2.3505414 -2.2436862 -2.0581994 -1.8332622 -1.5403118 -1.5406749 -2.0195971 -2.432642 -2.486623 -2.4333417 -2.3959522 -2.2199528 -2.0498519][-2.5654287 -2.6331096 -2.6580734 -2.6432753 -2.6298494 -2.6878839 -2.7275529 -2.8605418 -3.1497436 -3.3143218 -3.2259855 -3.092617 -3.047524 -2.9832017 -2.94362][-3.5887818 -3.547673 -3.4917691 -3.4973655 -3.5053585 -3.59751 -3.7678325 -3.993609 -4.1821194 -4.2009263 -4.0900321 -3.981781 -3.9259419 -3.8709745 -3.8399386][-4.7671914 -4.6959209 -4.5937319 -4.5871005 -4.5665617 -4.5798035 -4.7114086 -4.9380393 -5.0938606 -5.0891271 -5.0373311 -4.9895592 -4.9195542 -4.824964 -4.7373714]]...]
INFO - root - 2017-12-06 06:39:51.594484: step 1810, loss = 0.86, batch loss = 0.79 (19.1 examples/sec; 0.418 sec/batch; 38h:22m:46s remains)
INFO - root - 2017-12-06 06:39:55.852127: step 1820, loss = 0.84, batch loss = 0.77 (19.2 examples/sec; 0.417 sec/batch; 38h:18m:02s remains)
INFO - root - 2017-12-06 06:39:59.842321: step 1830, loss = 0.85, batch loss = 0.78 (19.5 examples/sec; 0.410 sec/batch; 37h:41m:38s remains)
INFO - root - 2017-12-06 06:40:04.000321: step 1840, loss = 1.00, batch loss = 0.93 (18.9 examples/sec; 0.423 sec/batch; 38h:50m:12s remains)
INFO - root - 2017-12-06 06:40:08.193059: step 1850, loss = 0.90, batch loss = 0.83 (19.1 examples/sec; 0.420 sec/batch; 38h:33m:51s remains)
INFO - root - 2017-12-06 06:40:12.405090: step 1860, loss = 0.80, batch loss = 0.73 (20.2 examples/sec; 0.396 sec/batch; 36h:22m:22s remains)
INFO - root - 2017-12-06 06:40:16.608284: step 1870, loss = 0.87, batch loss = 0.80 (18.6 examples/sec; 0.431 sec/batch; 39h:32m:49s remains)
INFO - root - 2017-12-06 06:40:20.795646: step 1880, loss = 0.94, batch loss = 0.87 (17.7 examples/sec; 0.453 sec/batch; 41h:36m:32s remains)
INFO - root - 2017-12-06 06:40:24.908907: step 1890, loss = 0.91, batch loss = 0.84 (19.3 examples/sec; 0.414 sec/batch; 38h:02m:14s remains)
INFO - root - 2017-12-06 06:40:29.165431: step 1900, loss = 0.74, batch loss = 0.67 (19.1 examples/sec; 0.419 sec/batch; 38h:28m:48s remains)
2017-12-06 06:40:29.751362: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.3272104 -6.3778157 -5.9832253 -5.6999211 -5.48742 -5.1984529 -5.0402446 -5.3829451 -6.0894008 -6.5637503 -6.322031 -5.8996439 -5.4948382 -5.0130105 -4.7265644][-6.3329525 -6.4681611 -5.9283352 -5.4679794 -5.2829294 -5.1569076 -5.0778565 -5.408217 -6.1405621 -6.6658287 -6.5516944 -6.2849436 -5.84633 -5.193727 -4.7515416][-6.1925092 -6.3811283 -5.6080017 -4.8458896 -4.6285229 -4.7346077 -4.8029132 -5.0951109 -5.7543283 -6.2836723 -6.3562489 -6.3183279 -5.9058909 -5.1394143 -4.6045718][-5.94538 -6.163744 -5.1697984 -4.078959 -3.746655 -3.9940326 -4.1671262 -4.4235673 -5.0432568 -5.6505394 -5.9487963 -6.1356468 -5.8045559 -5.0165286 -4.4380693][-5.4910517 -5.7993221 -4.7958169 -3.5320952 -2.9948404 -3.1110048 -3.2030947 -3.4606004 -4.2223525 -5.0849047 -5.6188445 -5.951026 -5.7035193 -4.9639616 -4.4035716][-4.750351 -5.20692 -4.4394445 -3.2459326 -2.5246592 -2.2873468 -2.094424 -2.3026028 -3.2743888 -4.4759655 -5.2489967 -5.697958 -5.55974 -4.9404683 -4.4868059][-3.8564811 -4.4137425 -3.9787049 -3.0299816 -2.2617652 -1.692533 -1.1853898 -1.2562952 -2.2699611 -3.641798 -4.5851784 -5.2316084 -5.33757 -4.9570136 -4.694603][-3.1079993 -3.6494167 -3.469898 -2.8081985 -2.1357996 -1.4106262 -0.69539428 -0.55055547 -1.3524396 -2.6082425 -3.5955451 -4.518096 -4.9942074 -4.9314117 -4.8940449][-2.7134585 -3.133337 -3.0769882 -2.6475458 -2.1538367 -1.4497347 -0.64490557 -0.25267839 -0.68761086 -1.6455033 -2.5627961 -3.7322929 -4.5603232 -4.775836 -4.9087877][-2.7923656 -3.0543432 -3.0287604 -2.7633052 -2.441473 -1.8570297 -1.0713367 -0.5068996 -0.61201215 -1.2207823 -1.9441693 -3.1963415 -4.2347813 -4.6125736 -4.8337507][-3.3260481 -3.4757485 -3.4523153 -3.2922046 -3.0970192 -2.6721797 -2.010612 -1.4210677 -1.3221004 -1.600219 -2.0135026 -3.0984147 -4.146018 -4.5573688 -4.7711034][-4.085 -4.1737785 -4.1695647 -4.0787463 -3.9501822 -3.6633394 -3.1898236 -2.7161648 -2.5461864 -2.5720003 -2.6498427 -3.4007573 -4.309876 -4.6873989 -4.8359795][-4.7966065 -4.8547692 -4.877687 -4.8480844 -4.7761564 -4.6107187 -4.3299756 -4.0198855 -3.8516853 -3.727159 -3.5592322 -3.941396 -4.611814 -4.913343 -4.9831738][-5.2143412 -5.247375 -5.2951374 -5.3235393 -5.3145814 -5.2532268 -5.1316752 -4.9627457 -4.8205509 -4.6296945 -4.3434477 -4.4321961 -4.8297811 -5.0145679 -4.9997177][-5.1732364 -5.1876321 -5.243206 -5.3031559 -5.3420644 -5.3550782 -5.3356586 -5.2700996 -5.1779766 -5.0048146 -4.7347069 -4.6885662 -4.8831015 -4.9535942 -4.8657784]]...]
INFO - root - 2017-12-06 06:40:33.883731: step 1910, loss = 0.93, batch loss = 0.86 (19.1 examples/sec; 0.419 sec/batch; 38h:30m:33s remains)
INFO - root - 2017-12-06 06:40:38.063621: step 1920, loss = 0.75, batch loss = 0.68 (19.7 examples/sec; 0.406 sec/batch; 37h:14m:42s remains)
INFO - root - 2017-12-06 06:40:42.250355: step 1930, loss = 1.09, batch loss = 1.02 (19.0 examples/sec; 0.420 sec/batch; 38h:33m:59s remains)
INFO - root - 2017-12-06 06:40:46.191468: step 1940, loss = 0.81, batch loss = 0.74 (18.5 examples/sec; 0.434 sec/batch; 39h:48m:23s remains)
INFO - root - 2017-12-06 06:40:50.373132: step 1950, loss = 0.87, batch loss = 0.80 (19.5 examples/sec; 0.410 sec/batch; 37h:39m:22s remains)
INFO - root - 2017-12-06 06:40:54.612422: step 1960, loss = 0.76, batch loss = 0.69 (17.6 examples/sec; 0.456 sec/batch; 41h:51m:08s remains)
INFO - root - 2017-12-06 06:40:58.718597: step 1970, loss = 0.97, batch loss = 0.90 (19.5 examples/sec; 0.410 sec/batch; 37h:35m:54s remains)
INFO - root - 2017-12-06 06:41:02.854455: step 1980, loss = 0.84, batch loss = 0.77 (18.9 examples/sec; 0.423 sec/batch; 38h:49m:47s remains)
INFO - root - 2017-12-06 06:41:07.083384: step 1990, loss = 1.00, batch loss = 0.93 (18.1 examples/sec; 0.442 sec/batch; 40h:36m:55s remains)
INFO - root - 2017-12-06 06:41:11.332861: step 2000, loss = 0.94, batch loss = 0.87 (19.7 examples/sec; 0.406 sec/batch; 37h:16m:00s remains)
2017-12-06 06:41:11.841616: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3203521 -4.3756394 -4.3954911 -4.3912125 -4.4040766 -4.4450026 -4.5090017 -4.5560164 -4.66311 -4.8259244 -4.9531546 -5.0199966 -4.971899 -4.8322029 -4.6575675][-4.2416949 -4.2744331 -4.2407737 -4.1763487 -4.1630707 -4.2150469 -4.2955451 -4.3111515 -4.4405961 -4.6748929 -4.8546381 -4.9822063 -4.98422 -4.8594766 -4.6839004][-4.1469741 -4.1334243 -3.990242 -3.8094912 -3.7277198 -3.7678189 -3.8386488 -3.7913396 -3.9438357 -4.2477612 -4.4727521 -4.6760335 -4.7652092 -4.6996422 -4.5703864][-4.2120323 -4.1760807 -3.9109969 -3.5816965 -3.3830986 -3.337424 -3.3023059 -3.1183462 -3.2674394 -3.618536 -3.8773017 -4.1737442 -4.3763161 -4.4115148 -4.3733683][-4.3613377 -4.2784548 -3.8775117 -3.4058027 -3.0567985 -2.8615408 -2.6544957 -2.2833161 -2.4489167 -2.8637176 -3.1686478 -3.6067045 -3.9743547 -4.1544843 -4.2354422][-4.4328279 -4.2395024 -3.6895013 -3.1126969 -2.6113546 -2.2515554 -1.876312 -1.3455925 -1.5957313 -2.1386635 -2.5246177 -3.1598425 -3.7551489 -4.0915856 -4.28156][-4.3427653 -3.9824398 -3.2583897 -2.6248293 -2.0307791 -1.5673602 -1.110661 -0.50838733 -0.9160769 -1.6506433 -2.1312129 -2.9629223 -3.7969584 -4.2467723 -4.490921][-4.0229869 -3.4964914 -2.6153417 -1.9901135 -1.4344392 -0.99893332 -0.61380792 -0.088122368 -0.66590977 -1.5507889 -2.0890384 -3.06181 -4.0697227 -4.5602145 -4.7765903][-3.6564136 -3.0584831 -2.1287792 -1.5813918 -1.1662939 -0.87725616 -0.679703 -0.351933 -1.0400553 -1.9472077 -2.4660461 -3.4642806 -4.5058255 -4.9332261 -5.0359068][-3.4247303 -2.8838639 -2.0763607 -1.6763406 -1.4352868 -1.3161118 -1.3225412 -1.2272594 -1.9312425 -2.7257676 -3.1477137 -4.0309286 -4.9323292 -5.2063227 -5.1599793][-3.4425004 -3.0429816 -2.4837179 -2.252059 -2.1600494 -2.1652796 -2.3125637 -2.3997707 -3.0170851 -3.6098435 -3.888588 -4.55846 -5.2174234 -5.304841 -5.1290207][-3.7200055 -3.48088 -3.1647418 -3.0744877 -3.0863276 -3.1701849 -3.363986 -3.5147951 -3.9607866 -4.3215671 -4.4508376 -4.8666668 -5.2564583 -5.2009 -4.963799][-3.9893475 -3.8816311 -3.7403696 -3.7362273 -3.8025391 -3.9184468 -4.1030254 -4.2483625 -4.5127678 -4.6776395 -4.7026081 -4.9025154 -5.0730057 -4.9632978 -4.7456431][-4.16031 -4.1246328 -4.0782089 -4.1036081 -4.1762543 -4.2869072 -4.4374967 -4.55373 -4.6817818 -4.7288609 -4.7049422 -4.7657394 -4.8053651 -4.7038884 -4.5448585][-4.2489338 -4.2452574 -4.2464919 -4.2799244 -4.3380342 -4.410233 -4.4982452 -4.5622978 -4.6081228 -4.6086087 -4.5751028 -4.566186 -4.5448537 -4.4761467 -4.3807912]]...]
INFO - root - 2017-12-06 06:41:15.997779: step 2010, loss = 1.03, batch loss = 0.96 (19.6 examples/sec; 0.408 sec/batch; 37h:25m:14s remains)
INFO - root - 2017-12-06 06:41:20.191509: step 2020, loss = 0.87, batch loss = 0.80 (19.0 examples/sec; 0.420 sec/batch; 38h:34m:32s remains)
INFO - root - 2017-12-06 06:41:24.296466: step 2030, loss = 0.93, batch loss = 0.86 (19.6 examples/sec; 0.408 sec/batch; 37h:26m:27s remains)
INFO - root - 2017-12-06 06:41:28.515672: step 2040, loss = 0.83, batch loss = 0.76 (19.0 examples/sec; 0.421 sec/batch; 38h:37m:57s remains)
INFO - root - 2017-12-06 06:41:32.398240: step 2050, loss = 1.08, batch loss = 1.01 (20.1 examples/sec; 0.397 sec/batch; 36h:26m:49s remains)
INFO - root - 2017-12-06 06:41:36.601894: step 2060, loss = 1.02, batch loss = 0.95 (19.1 examples/sec; 0.418 sec/batch; 38h:23m:15s remains)
INFO - root - 2017-12-06 06:41:40.761109: step 2070, loss = 0.95, batch loss = 0.88 (19.5 examples/sec; 0.411 sec/batch; 37h:41m:30s remains)
INFO - root - 2017-12-06 06:41:44.981288: step 2080, loss = 0.98, batch loss = 0.91 (20.8 examples/sec; 0.385 sec/batch; 35h:20m:51s remains)
INFO - root - 2017-12-06 06:41:49.179723: step 2090, loss = 0.96, batch loss = 0.89 (18.1 examples/sec; 0.442 sec/batch; 40h:32m:47s remains)
INFO - root - 2017-12-06 06:41:53.378218: step 2100, loss = 0.94, batch loss = 0.87 (18.7 examples/sec; 0.427 sec/batch; 39h:10m:36s remains)
2017-12-06 06:41:53.906964: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0361238 -4.9123726 -4.6687474 -4.4026232 -4.1751504 -3.9998684 -3.9516606 -4.1003733 -4.3223925 -4.5176606 -4.620153 -4.6057363 -4.6112962 -4.7037573 -4.80052][-5.0202208 -4.8400211 -4.5509381 -4.1824245 -3.8261137 -3.5788889 -3.5268188 -3.7420807 -4.04765 -4.3079767 -4.4801426 -4.5205135 -4.5468469 -4.6595283 -4.782814][-4.9633255 -4.7177095 -4.3855019 -3.8891635 -3.3694108 -3.0413423 -3.00278 -3.3121097 -3.730334 -4.0574422 -4.3002534 -4.4088173 -4.4638138 -4.5989184 -4.7577634][-4.8810124 -4.5919676 -4.2369652 -3.6324909 -2.9563022 -2.5460794 -2.5125771 -2.8959966 -3.4238467 -3.8232167 -4.1421947 -4.3313584 -4.4076638 -4.5454893 -4.7330847][-4.7740917 -4.4845085 -4.1277928 -3.4474218 -2.63034 -2.1308835 -2.105113 -2.5267797 -3.1242306 -3.6095672 -4.0283289 -4.32462 -4.4281011 -4.5396256 -4.7207794][-4.68562 -4.432138 -4.08959 -3.3520298 -2.3820412 -1.7713289 -1.7846854 -2.245307 -2.8607941 -3.409215 -3.9421821 -4.3715787 -4.5169039 -4.5810332 -4.7130833][-4.6876416 -4.4856033 -4.18244 -3.4274855 -2.3149965 -1.5669773 -1.6189382 -2.1255822 -2.7119498 -3.2660708 -3.8818922 -4.4355931 -4.6301 -4.6383581 -4.6957231][-4.7913375 -4.6448507 -4.4099264 -3.7232883 -2.5664587 -1.6921682 -1.7043953 -2.1972253 -2.7126966 -3.2209263 -3.8553486 -4.4691715 -4.6969337 -4.6559849 -4.6456423][-4.9117751 -4.8261275 -4.6693654 -4.1193748 -3.0416582 -2.1048708 -1.9835217 -2.3628392 -2.776258 -3.2145348 -3.8222177 -4.4354897 -4.6680222 -4.5921226 -4.5475698][-4.9641595 -4.9393725 -4.8365526 -4.4180131 -3.4885244 -2.5632246 -2.2953932 -2.5057621 -2.8016734 -3.1692576 -3.7445238 -4.3378906 -4.5591063 -4.4631867 -4.422019][-4.9500079 -4.9972072 -4.931644 -4.6037054 -3.8245475 -2.9719918 -2.6075015 -2.6548271 -2.8155198 -3.0996234 -3.6287203 -4.2023087 -4.4227738 -4.3358393 -4.3371549][-4.8990846 -5.0327287 -5.0281482 -4.7775106 -4.1514406 -3.4201031 -3.0113964 -2.9114728 -2.9227731 -3.102901 -3.5538092 -4.0831532 -4.2954226 -4.2391634 -4.3021235][-4.7773852 -4.9858422 -5.0555639 -4.8872213 -4.4210734 -3.8423719 -3.4509354 -3.2568293 -3.1462407 -3.2161732 -3.5558844 -3.9994388 -4.188138 -4.1684504 -4.2861676][-4.5925927 -4.8313646 -4.9575458 -4.8598957 -4.5323915 -4.1090941 -3.7886317 -3.5787725 -3.4124534 -3.4048634 -3.6364069 -3.9735217 -4.1270123 -4.1347332 -4.2776456][-4.4055138 -4.6106954 -4.7470231 -4.6908355 -4.4626837 -4.1779571 -3.9647012 -3.8082166 -3.6602364 -3.6313705 -3.7836554 -4.0171056 -4.1290345 -4.142179 -4.2658033]]...]
INFO - root - 2017-12-06 06:41:58.123904: step 2110, loss = 1.01, batch loss = 0.94 (19.8 examples/sec; 0.404 sec/batch; 37h:02m:22s remains)
INFO - root - 2017-12-06 06:42:02.273221: step 2120, loss = 0.96, batch loss = 0.89 (18.9 examples/sec; 0.424 sec/batch; 38h:55m:21s remains)
INFO - root - 2017-12-06 06:42:06.489905: step 2130, loss = 0.77, batch loss = 0.70 (19.0 examples/sec; 0.421 sec/batch; 38h:39m:38s remains)
INFO - root - 2017-12-06 06:42:10.635046: step 2140, loss = 0.93, batch loss = 0.86 (19.0 examples/sec; 0.420 sec/batch; 38h:32m:45s remains)
INFO - root - 2017-12-06 06:42:14.563800: step 2150, loss = 0.82, batch loss = 0.75 (20.2 examples/sec; 0.396 sec/batch; 36h:19m:27s remains)
INFO - root - 2017-12-06 06:42:18.677153: step 2160, loss = 1.05, batch loss = 0.98 (20.6 examples/sec; 0.388 sec/batch; 35h:36m:28s remains)
INFO - root - 2017-12-06 06:42:22.779930: step 2170, loss = 0.97, batch loss = 0.90 (20.9 examples/sec; 0.383 sec/batch; 35h:06m:28s remains)
INFO - root - 2017-12-06 06:42:26.954176: step 2180, loss = 0.88, batch loss = 0.81 (19.0 examples/sec; 0.421 sec/batch; 38h:37m:21s remains)
INFO - root - 2017-12-06 06:42:31.115097: step 2190, loss = 1.07, batch loss = 1.00 (19.8 examples/sec; 0.403 sec/batch; 37h:00m:56s remains)
INFO - root - 2017-12-06 06:42:35.222031: step 2200, loss = 0.89, batch loss = 0.82 (19.6 examples/sec; 0.408 sec/batch; 37h:24m:14s remains)
2017-12-06 06:42:35.776927: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0625944 -5.987402 -5.9856377 -5.9369469 -5.4747467 -4.7858868 -4.2648311 -4.0403786 -4.1425295 -4.5571756 -4.8736362 -4.9799643 -4.936903 -4.7792549 -4.4342475][-5.7956 -5.6865096 -5.6838174 -5.6218624 -5.0940752 -4.347652 -3.8082216 -3.6000254 -3.7723405 -4.2609916 -4.63398 -4.7793145 -4.8236661 -4.7803769 -4.5525184][-5.4946604 -5.3231225 -5.3033571 -5.2386293 -4.6666594 -3.9007609 -3.3544917 -3.1546493 -3.3660436 -3.8681548 -4.2972465 -4.5523229 -4.7732553 -4.9254727 -4.8806505][-5.1978216 -4.9632878 -4.9029312 -4.8085222 -4.1943426 -3.408709 -2.8257127 -2.6121109 -2.8554058 -3.3706739 -3.9055264 -4.3712854 -4.8387351 -5.2187791 -5.3524275][-4.9975142 -4.7175493 -4.581459 -4.4111247 -3.7380037 -2.918129 -2.2743561 -2.0267498 -2.3091683 -2.896821 -3.6055717 -4.3297963 -5.0383844 -5.5714893 -5.7857547][-4.9187202 -4.6447811 -4.4207444 -4.143127 -3.3995895 -2.548707 -1.8671439 -1.5890987 -1.9235241 -2.6254883 -3.492146 -4.3932214 -5.2289515 -5.7826037 -5.9438][-5.0219831 -4.7476816 -4.4123292 -4.0086441 -3.18533 -2.3115711 -1.6310813 -1.3389251 -1.7102804 -2.5268259 -3.494734 -4.4753132 -5.3402133 -5.837296 -5.8615017][-5.3313026 -5.001493 -4.5176668 -3.9732316 -3.065958 -2.1747854 -1.5351019 -1.25688 -1.6460309 -2.5439136 -3.5888867 -4.6295524 -5.4879847 -5.9127364 -5.7935834][-5.7196403 -5.3039832 -4.6933155 -4.0338793 -3.0580738 -2.1578414 -1.5797429 -1.355449 -1.751822 -2.6673217 -3.7471597 -4.8311381 -5.6601167 -6.0322094 -5.8467517][-6.0778432 -5.6017971 -4.9401741 -4.2480392 -3.2714493 -2.3961535 -1.8838086 -1.7231491 -2.0947394 -2.9453669 -3.975136 -5.0319138 -5.7847891 -6.1154504 -5.9521613][-6.2242851 -5.7616315 -5.1761827 -4.5859671 -3.7237215 -2.9302351 -2.4867735 -2.363034 -2.6566496 -3.3724909 -4.2958217 -5.2636018 -5.9183812 -6.2112 -6.0989289][-6.1084447 -5.7095757 -5.2747765 -4.877758 -4.2182703 -3.5571485 -3.1900933 -3.0941062 -3.2919235 -3.8632915 -4.6732697 -5.535255 -6.0869169 -6.3384538 -6.2746878][-5.7960114 -5.4869475 -5.2009354 -4.9819174 -4.5189714 -3.9737062 -3.6640882 -3.6027524 -3.7465949 -4.21706 -4.9470673 -5.7249384 -6.1928992 -6.4083 -6.3928361][-5.3432655 -5.1324081 -4.966424 -4.8724074 -4.5622482 -4.1113138 -3.8396673 -3.8113961 -3.9373138 -4.3320103 -4.9827814 -5.6767797 -6.074945 -6.2608886 -6.2944937][-4.8694806 -4.7449036 -4.659811 -4.6356626 -4.4484921 -4.1081843 -3.8911908 -3.886903 -4.0038314 -4.31783 -4.8551712 -5.4324389 -5.762362 -5.9171581 -5.9683356]]...]
INFO - root - 2017-12-06 06:42:39.985588: step 2210, loss = 0.95, batch loss = 0.88 (18.3 examples/sec; 0.438 sec/batch; 40h:13m:02s remains)
INFO - root - 2017-12-06 06:42:44.115288: step 2220, loss = 0.77, batch loss = 0.70 (19.7 examples/sec; 0.407 sec/batch; 37h:19m:12s remains)
INFO - root - 2017-12-06 06:42:48.254973: step 2230, loss = 0.89, batch loss = 0.82 (19.5 examples/sec; 0.410 sec/batch; 37h:34m:15s remains)
INFO - root - 2017-12-06 06:42:52.381244: step 2240, loss = 0.76, batch loss = 0.68 (18.5 examples/sec; 0.432 sec/batch; 39h:37m:37s remains)
INFO - root - 2017-12-06 06:42:56.616901: step 2250, loss = 0.82, batch loss = 0.75 (18.6 examples/sec; 0.431 sec/batch; 39h:30m:16s remains)
INFO - root - 2017-12-06 06:43:00.578661: step 2260, loss = 0.94, batch loss = 0.87 (18.9 examples/sec; 0.423 sec/batch; 38h:48m:37s remains)
INFO - root - 2017-12-06 06:43:04.692656: step 2270, loss = 1.20, batch loss = 1.13 (19.1 examples/sec; 0.418 sec/batch; 38h:20m:41s remains)
INFO - root - 2017-12-06 06:43:08.931160: step 2280, loss = 1.02, batch loss = 0.95 (19.8 examples/sec; 0.404 sec/batch; 37h:04m:39s remains)
INFO - root - 2017-12-06 06:43:13.106341: step 2290, loss = 0.74, batch loss = 0.67 (19.7 examples/sec; 0.406 sec/batch; 37h:13m:00s remains)
INFO - root - 2017-12-06 06:43:17.253318: step 2300, loss = 1.17, batch loss = 1.10 (18.8 examples/sec; 0.425 sec/batch; 39h:01m:01s remains)
2017-12-06 06:43:17.719578: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2155433 -5.3672709 -5.3079982 -4.9263124 -4.5948524 -4.5413766 -4.4933343 -4.3472824 -4.1250634 -3.6655188 -3.0597715 -2.7150471 -2.7156353 -3.0647206 -3.7369654][-5.0871954 -5.1681957 -5.0100927 -4.5795736 -4.2303758 -4.0883489 -3.9289398 -3.6216173 -3.152823 -2.7346926 -2.5476952 -2.5568817 -2.793509 -3.3679233 -4.1196871][-4.6631069 -4.7011886 -4.5009561 -4.1240869 -3.8247459 -3.5705383 -3.2725172 -2.8317199 -2.1715038 -1.8547971 -2.0984914 -2.4230549 -2.8714967 -3.6869097 -4.5434942][-4.1849942 -4.3143315 -4.1553292 -3.85979 -3.5843594 -3.1946092 -2.7999849 -2.3788967 -1.808387 -1.7232201 -2.2672129 -2.8028142 -3.379225 -4.2669916 -5.0399847][-4.0356321 -4.2572284 -4.0519156 -3.6860278 -3.2834978 -2.6857178 -2.203804 -1.9152782 -1.720722 -2.0257385 -2.7835059 -3.4706109 -4.1257534 -4.8952 -5.3909945][-4.2976789 -4.54167 -4.1491504 -3.5021906 -2.7638783 -1.8806081 -1.2724071 -1.1102064 -1.4057233 -2.1683893 -3.1091404 -3.9615419 -4.7292967 -5.3490782 -5.5507221][-4.97146 -5.2130256 -4.5679755 -3.4960406 -2.2878938 -1.1176951 -0.38999462 -0.32834148 -1.0390544 -2.1556334 -3.241353 -4.2199068 -5.0506525 -5.5178385 -5.4835291][-5.6840911 -5.8797951 -5.0397277 -3.6475737 -2.1716642 -1.0110152 -0.421175 -0.50553012 -1.3534617 -2.4933219 -3.5255766 -4.4415865 -5.1423731 -5.4059663 -5.2097712][-5.8339138 -5.9379373 -5.114048 -3.7808595 -2.5242085 -1.7808485 -1.5692296 -1.8207207 -2.5642087 -3.448957 -4.2121105 -4.8323727 -5.1814609 -5.1708012 -4.879818][-5.4551082 -5.4576349 -4.80735 -3.79385 -3.0051284 -2.7555208 -2.8968625 -3.2869933 -3.8965821 -4.4943519 -4.946454 -5.2141814 -5.2044911 -4.9686747 -4.6558938][-4.957746 -4.8697715 -4.4208221 -3.7749376 -3.4019961 -3.4967413 -3.8456781 -4.282836 -4.7335606 -5.1135 -5.33317 -5.3396521 -5.1329784 -4.8203878 -4.5612721][-4.4065781 -4.3386192 -4.1592264 -3.8911762 -3.8099358 -4.055964 -4.4698415 -4.8462243 -5.113564 -5.3128076 -5.3558989 -5.2056751 -4.9439445 -4.6668172 -4.5033126][-4.0604124 -4.0575271 -4.0508504 -4.0457215 -4.1316104 -4.4151163 -4.81143 -5.0885277 -5.17778 -5.204464 -5.1079454 -4.9119749 -4.6980615 -4.5177245 -4.4526763][-4.07703 -4.0731196 -4.0650182 -4.1778197 -4.3685532 -4.6511884 -4.9571795 -5.0839152 -5.0256872 -4.9402976 -4.8064404 -4.6717253 -4.5622468 -4.4791675 -4.469749][-4.1896205 -4.2202692 -4.199966 -4.3637419 -4.6002645 -4.8337374 -4.9859281 -4.9596825 -4.8290319 -4.7316775 -4.6399503 -4.5852728 -4.5529823 -4.5143194 -4.5082121]]...]
INFO - root - 2017-12-06 06:43:21.879137: step 2310, loss = 0.96, batch loss = 0.89 (19.1 examples/sec; 0.419 sec/batch; 38h:23m:24s remains)
INFO - root - 2017-12-06 06:43:25.957617: step 2320, loss = 0.81, batch loss = 0.74 (19.5 examples/sec; 0.410 sec/batch; 37h:33m:59s remains)
INFO - root - 2017-12-06 06:43:30.135671: step 2330, loss = 1.13, batch loss = 1.06 (19.6 examples/sec; 0.408 sec/batch; 37h:26m:05s remains)
INFO - root - 2017-12-06 06:43:34.283592: step 2340, loss = 1.09, batch loss = 1.02 (19.2 examples/sec; 0.418 sec/batch; 38h:18m:26s remains)
INFO - root - 2017-12-06 06:43:38.444220: step 2350, loss = 1.13, batch loss = 1.06 (19.8 examples/sec; 0.404 sec/batch; 37h:03m:07s remains)
INFO - root - 2017-12-06 06:43:42.549176: step 2360, loss = 0.90, batch loss = 0.83 (18.8 examples/sec; 0.424 sec/batch; 38h:55m:28s remains)
INFO - root - 2017-12-06 06:43:46.448318: step 2370, loss = 0.96, batch loss = 0.89 (18.5 examples/sec; 0.433 sec/batch; 39h:42m:24s remains)
INFO - root - 2017-12-06 06:43:50.573950: step 2380, loss = 0.86, batch loss = 0.79 (20.0 examples/sec; 0.400 sec/batch; 36h:38m:03s remains)
INFO - root - 2017-12-06 06:43:54.739077: step 2390, loss = 0.91, batch loss = 0.84 (19.0 examples/sec; 0.420 sec/batch; 38h:33m:24s remains)
INFO - root - 2017-12-06 06:43:58.894263: step 2400, loss = 1.06, batch loss = 0.99 (18.7 examples/sec; 0.428 sec/batch; 39h:14m:46s remains)
2017-12-06 06:43:59.373788: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5797234 -4.5978932 -4.5598116 -4.5274143 -4.7412252 -5.0255294 -5.1947546 -5.2862606 -5.2327008 -5.0959487 -4.9406147 -4.8079386 -4.6801467 -4.745636 -4.8169775][-4.2291446 -4.1116996 -3.8840089 -3.6074345 -3.6879985 -3.9981966 -4.1956334 -4.3555608 -4.43745 -4.45244 -4.4647841 -4.4693789 -4.3792276 -4.5094795 -4.69259][-3.9211767 -3.6475453 -3.194912 -2.6109457 -2.4960427 -2.8052912 -3.0307736 -3.2493827 -3.4844327 -3.6809983 -3.9101 -4.0719042 -4.059165 -4.2462354 -4.5023875][-3.6692312 -3.3014166 -2.7072351 -1.8536673 -1.5079875 -1.7712345 -1.9989922 -2.2436194 -2.5989892 -2.997139 -3.4527984 -3.7499225 -3.8382802 -4.0819397 -4.3574486][-3.5891016 -3.182754 -2.5582743 -1.5667937 -0.97631 -1.0909774 -1.250196 -1.4524982 -1.8470182 -2.4234123 -3.1196089 -3.568891 -3.7915432 -4.0879669 -4.3299465][-3.6002669 -3.1972909 -2.6541758 -1.7221711 -0.988991 -0.89146471 -0.89202046 -0.95503592 -1.2835858 -1.9633298 -2.8846855 -3.5248933 -3.9044335 -4.2463341 -4.4253488][-3.7067714 -3.3767505 -3.0025764 -2.3243849 -1.5938029 -1.2699499 -1.0112751 -0.82139039 -0.97365022 -1.6261091 -2.6886334 -3.5411217 -4.1104536 -4.5257611 -4.6585131][-3.8767567 -3.6749077 -3.4952106 -3.1640491 -2.5569236 -2.0587862 -1.5336833 -1.0834403 -1.0075386 -1.4849563 -2.5332975 -3.5612617 -4.3580933 -4.9041109 -5.0269971][-3.974694 -3.9347448 -3.9336119 -3.9154873 -3.4504609 -2.8444238 -2.0862296 -1.4423122 -1.1941414 -1.4413087 -2.336282 -3.442137 -4.472424 -5.2230783 -5.4028397][-3.9747188 -4.0674653 -4.2006383 -4.3995552 -4.0673661 -3.4469478 -2.5505445 -1.8130617 -1.488698 -1.5242014 -2.1726809 -3.2156136 -4.4056525 -5.3699908 -5.6389694][-3.8945045 -4.0660152 -4.2855649 -4.5798244 -4.3646507 -3.8137074 -2.91458 -2.1901636 -1.8548157 -1.7369945 -2.1327474 -3.0016665 -4.2080131 -5.2984638 -5.6406369][-3.894866 -4.1106658 -4.3838406 -4.6694469 -4.5112028 -4.0348234 -3.2111034 -2.5597298 -2.231076 -2.0158236 -2.2183406 -2.8922093 -3.9918847 -5.0696039 -5.4384165][-3.9343195 -4.1719522 -4.4848828 -4.7523651 -4.6386461 -4.2258596 -3.509105 -2.9473636 -2.6290755 -2.3768742 -2.4756637 -2.9802575 -3.8905132 -4.82517 -5.1583223][-3.9647322 -4.1756053 -4.4807367 -4.7405748 -4.6971083 -4.3693171 -3.7866712 -3.3125498 -3.0198629 -2.8015528 -2.8757205 -3.2505062 -3.9319685 -4.6368179 -4.8909879][-4.0281062 -4.1901326 -4.4314408 -4.652195 -4.6659751 -4.4343123 -4.0082631 -3.62927 -3.3735571 -3.226542 -3.3190212 -3.6003344 -4.0556731 -4.5098028 -4.6698914]]...]
INFO - root - 2017-12-06 06:44:03.516618: step 2410, loss = 0.85, batch loss = 0.78 (19.3 examples/sec; 0.415 sec/batch; 38h:02m:13s remains)
INFO - root - 2017-12-06 06:44:07.657628: step 2420, loss = 0.92, batch loss = 0.85 (18.2 examples/sec; 0.440 sec/batch; 40h:18m:16s remains)
INFO - root - 2017-12-06 06:44:11.860971: step 2430, loss = 0.91, batch loss = 0.84 (19.0 examples/sec; 0.421 sec/batch; 38h:35m:58s remains)
INFO - root - 2017-12-06 06:44:15.961866: step 2440, loss = 0.88, batch loss = 0.81 (19.6 examples/sec; 0.409 sec/batch; 37h:29m:04s remains)
INFO - root - 2017-12-06 06:44:20.168549: step 2450, loss = 0.87, batch loss = 0.80 (20.0 examples/sec; 0.400 sec/batch; 36h:39m:11s remains)
INFO - root - 2017-12-06 06:44:24.326092: step 2460, loss = 0.95, batch loss = 0.88 (18.8 examples/sec; 0.426 sec/batch; 39h:02m:12s remains)
INFO - root - 2017-12-06 06:44:28.336291: step 2470, loss = 0.88, batch loss = 0.81 (22.6 examples/sec; 0.354 sec/batch; 32h:29m:38s remains)
INFO - root - 2017-12-06 06:44:32.190671: step 2480, loss = 0.91, batch loss = 0.84 (20.4 examples/sec; 0.392 sec/batch; 35h:56m:08s remains)
INFO - root - 2017-12-06 06:44:36.261616: step 2490, loss = 0.92, batch loss = 0.85 (22.0 examples/sec; 0.364 sec/batch; 33h:21m:04s remains)
INFO - root - 2017-12-06 06:44:40.441999: step 2500, loss = 0.73, batch loss = 0.66 (19.1 examples/sec; 0.418 sec/batch; 38h:18m:09s remains)
2017-12-06 06:44:40.937300: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9002624 -5.1013203 -5.0325632 -5.0240564 -5.0661659 -4.9884624 -4.8783946 -4.9206367 -5.1858125 -5.3813539 -5.4141903 -5.2985229 -5.105329 -4.923173 -4.7929726][-5.1197066 -5.2828569 -5.1080356 -5.07064 -5.1401014 -5.0424838 -4.8583956 -4.8292928 -5.234 -5.65227 -5.8378515 -5.7986693 -5.6087818 -5.3673444 -5.1450095][-5.2677612 -5.34412 -5.060813 -4.9951892 -5.0566144 -4.8727202 -4.5511541 -4.3728647 -4.8556252 -5.5258508 -5.9794726 -6.1267338 -6.0306215 -5.8110294 -5.5122819][-5.3303113 -5.2772059 -4.8822622 -4.7534142 -4.7295642 -4.394352 -3.879529 -3.5035558 -4.03487 -4.9894013 -5.8251681 -6.2647204 -6.3152618 -6.1573725 -5.8437366][-5.3338532 -5.1765819 -4.7281084 -4.4970489 -4.2706804 -3.7040358 -2.9490767 -2.3186185 -2.8293991 -4.0812955 -5.3708577 -6.1638575 -6.393599 -6.3505697 -6.1568327][-5.2660475 -5.0122824 -4.6041517 -4.2777739 -3.7354033 -2.840781 -1.8301516 -0.920064 -1.35882 -2.8870349 -4.6268368 -5.7788024 -6.1815581 -6.2718191 -6.3414268][-5.1097145 -4.7977138 -4.5119295 -4.168376 -3.3158052 -2.0903862 -0.85381222 0.32079554 0.0067720413 -1.7155671 -3.7476773 -5.1887426 -5.7398663 -5.9193964 -6.230288][-4.931509 -4.6313534 -4.5132594 -4.2627687 -3.2580841 -1.8015361 -0.47110128 0.80211973 0.58567333 -1.0969112 -3.0337906 -4.5447183 -5.1778421 -5.3934627 -5.8376718][-4.7442951 -4.5218258 -4.5719309 -4.4828629 -3.5262933 -2.0361369 -0.77154994 0.42255545 0.23645592 -1.1824989 -2.6763206 -4.0004845 -4.6104522 -4.8164077 -5.3160534][-4.5884552 -4.5034795 -4.6904311 -4.7773867 -4.020287 -2.6448312 -1.5176258 -0.46665263 -0.62297106 -1.6861637 -2.6740186 -3.7625198 -4.3636541 -4.5287919 -4.9579358][-4.5103626 -4.5631561 -4.8160071 -4.988821 -4.4451046 -3.3099544 -2.389348 -1.5330422 -1.6884978 -2.4398475 -3.0229111 -3.9355378 -4.5855165 -4.7106857 -4.9360843][-4.5595317 -4.7130256 -4.9453917 -5.059772 -4.6061645 -3.7683187 -3.1446829 -2.554847 -2.7899146 -3.3761916 -3.7015352 -4.4727225 -5.1622849 -5.2376432 -5.2016816][-4.7420464 -4.9253755 -5.080852 -5.0834479 -4.6255317 -4.051208 -3.7165444 -3.4231925 -3.7611027 -4.249434 -4.4013572 -5.0101066 -5.67088 -5.6935358 -5.4659538][-4.940547 -5.092248 -5.13097 -5.0065084 -4.5423961 -4.1765032 -4.0745254 -4.0470157 -4.44684 -4.8231864 -4.8354907 -5.2276359 -5.7562523 -5.7548761 -5.4802289][-5.0548978 -5.16475 -5.0884013 -4.8627024 -4.4409823 -4.21076 -4.2312026 -4.360918 -4.7335172 -4.9649754 -4.8749146 -5.0442033 -5.3796644 -5.3906846 -5.1958251]]...]
INFO - root - 2017-12-06 06:44:45.113584: step 2510, loss = 0.98, batch loss = 0.91 (20.1 examples/sec; 0.399 sec/batch; 36h:34m:06s remains)
INFO - root - 2017-12-06 06:44:49.307194: step 2520, loss = 0.77, batch loss = 0.70 (19.1 examples/sec; 0.419 sec/batch; 38h:25m:25s remains)
INFO - root - 2017-12-06 06:44:53.507800: step 2530, loss = 0.88, batch loss = 0.81 (18.8 examples/sec; 0.426 sec/batch; 39h:04m:02s remains)
INFO - root - 2017-12-06 06:44:57.663879: step 2540, loss = 0.95, batch loss = 0.88 (19.3 examples/sec; 0.414 sec/batch; 37h:55m:12s remains)
INFO - root - 2017-12-06 06:45:01.812132: step 2550, loss = 0.90, batch loss = 0.83 (19.0 examples/sec; 0.422 sec/batch; 38h:38m:32s remains)
INFO - root - 2017-12-06 06:45:05.897771: step 2560, loss = 0.96, batch loss = 0.89 (19.3 examples/sec; 0.415 sec/batch; 38h:00m:21s remains)
INFO - root - 2017-12-06 06:45:10.032563: step 2570, loss = 1.00, batch loss = 0.93 (20.9 examples/sec; 0.382 sec/batch; 35h:01m:51s remains)
INFO - root - 2017-12-06 06:45:14.014118: step 2580, loss = 0.86, batch loss = 0.79 (19.3 examples/sec; 0.415 sec/batch; 38h:03m:56s remains)
INFO - root - 2017-12-06 06:45:18.097468: step 2590, loss = 0.97, batch loss = 0.90 (19.3 examples/sec; 0.414 sec/batch; 37h:57m:41s remains)
INFO - root - 2017-12-06 06:45:22.280283: step 2600, loss = 1.05, batch loss = 0.98 (19.2 examples/sec; 0.417 sec/batch; 38h:13m:58s remains)
2017-12-06 06:45:22.772930: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8861609 -4.0978818 -4.2091694 -4.2304521 -4.1735282 -4.0892992 -3.9269824 -3.7374735 -3.6407905 -3.7442367 -3.9625986 -3.9003 -3.8429484 -4.0202246 -4.3977609][-4.019793 -4.1560078 -4.278707 -4.2945857 -4.2467456 -4.1986008 -4.0149217 -3.8177378 -3.6562316 -3.5343654 -3.5324945 -3.4086323 -3.3895946 -3.5927265 -3.9477711][-4.2295666 -4.3367319 -4.5042605 -4.502471 -4.3880005 -4.2985315 -4.0444813 -3.8277168 -3.6443329 -3.3911457 -3.2296228 -3.0142255 -2.9295104 -3.0068502 -3.2249112][-4.4257669 -4.57651 -4.8002334 -4.7758718 -4.5692978 -4.383112 -4.0536785 -3.847363 -3.7012014 -3.4425733 -3.2049596 -2.849762 -2.5878596 -2.4334195 -2.4442143][-4.4285221 -4.6289659 -4.9091492 -4.9131556 -4.6644096 -4.3692226 -3.9810457 -3.7813907 -3.6870346 -3.517894 -3.308104 -2.8691821 -2.4412332 -2.0568254 -1.8474817][-4.2497129 -4.4094396 -4.7063069 -4.812645 -4.6333513 -4.2657671 -3.7950726 -3.5237696 -3.4159613 -3.3369479 -3.2195139 -2.8180821 -2.3529723 -1.8888147 -1.5767386][-3.9665239 -4.0467219 -4.3139982 -4.5594172 -4.5189366 -4.1181092 -3.5713589 -3.189867 -3.0283768 -2.9782627 -2.9393888 -2.6987193 -2.3841839 -2.0354655 -1.7889476][-3.7016401 -3.7751894 -4.0099125 -4.3650479 -4.4383764 -4.0508246 -3.4782639 -3.0021405 -2.7785838 -2.7036617 -2.7282207 -2.7269125 -2.6352494 -2.4448929 -2.2664177][-3.6472321 -3.7483985 -3.9178808 -4.2692575 -4.3793464 -4.0501404 -3.5171242 -2.9979541 -2.7647388 -2.7027068 -2.8192606 -3.0427926 -3.1295238 -3.0365491 -2.8753834][-3.8297937 -3.8863282 -3.920995 -4.1523623 -4.2328849 -4.0051303 -3.5817337 -3.1139369 -2.9719582 -3.0158153 -3.2465982 -3.5892162 -3.7174625 -3.635895 -3.4678268][-4.0704865 -4.0213914 -3.9138925 -4.0057821 -4.0453057 -3.9363117 -3.6608205 -3.3245318 -3.315598 -3.4647558 -3.7225063 -3.9915223 -4.010035 -3.8922839 -3.7549798][-4.29444 -4.1645813 -3.9934778 -4.0197425 -4.0442605 -4.0169177 -3.8625102 -3.6660111 -3.7473516 -3.9181995 -4.0936728 -4.1923409 -4.0924897 -3.9708405 -3.9113083][-4.4923291 -4.3446651 -4.1824689 -4.188746 -4.2087574 -4.2072549 -4.121726 -4.0321851 -4.14709 -4.2833619 -4.3631778 -4.3354216 -4.1981907 -4.1164718 -4.1246881][-4.707181 -4.5952172 -4.4606314 -4.4366741 -4.4270039 -4.4102035 -4.3519011 -4.3199787 -4.4215765 -4.5147195 -4.5372519 -4.4745865 -4.3780904 -4.3469524 -4.3738327][-4.82095 -4.7771564 -4.6812072 -4.631424 -4.5991812 -4.5730834 -4.5369849 -4.5374751 -4.6151218 -4.6890497 -4.6973591 -4.6457014 -4.5989418 -4.5942879 -4.6093826]]...]
INFO - root - 2017-12-06 06:45:26.955540: step 2610, loss = 0.94, batch loss = 0.87 (19.6 examples/sec; 0.407 sec/batch; 37h:20m:17s remains)
INFO - root - 2017-12-06 06:45:31.115407: step 2620, loss = 0.79, batch loss = 0.72 (18.5 examples/sec; 0.432 sec/batch; 39h:34m:49s remains)
INFO - root - 2017-12-06 06:45:35.218240: step 2630, loss = 0.76, batch loss = 0.69 (18.2 examples/sec; 0.439 sec/batch; 40h:11m:00s remains)
INFO - root - 2017-12-06 06:45:39.307047: step 2640, loss = 0.91, batch loss = 0.84 (19.2 examples/sec; 0.416 sec/batch; 38h:09m:45s remains)
INFO - root - 2017-12-06 06:45:43.397500: step 2650, loss = 1.02, batch loss = 0.95 (19.3 examples/sec; 0.414 sec/batch; 37h:55m:28s remains)
INFO - root - 2017-12-06 06:45:47.621152: step 2660, loss = 1.03, batch loss = 0.96 (19.5 examples/sec; 0.411 sec/batch; 37h:37m:27s remains)
INFO - root - 2017-12-06 06:45:51.707799: step 2670, loss = 1.18, batch loss = 1.11 (20.1 examples/sec; 0.399 sec/batch; 36h:31m:56s remains)
INFO - root - 2017-12-06 06:45:55.844205: step 2680, loss = 1.13, batch loss = 1.06 (19.5 examples/sec; 0.411 sec/batch; 37h:37m:45s remains)
INFO - root - 2017-12-06 06:45:59.739294: step 2690, loss = 0.76, batch loss = 0.69 (18.8 examples/sec; 0.425 sec/batch; 38h:57m:14s remains)
INFO - root - 2017-12-06 06:46:03.916730: step 2700, loss = 0.90, batch loss = 0.83 (20.4 examples/sec; 0.393 sec/batch; 35h:57m:29s remains)
2017-12-06 06:46:04.377625: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1906981 -2.026329 -2.4060459 -3.0841277 -3.8491211 -4.4833088 -4.9993 -5.4193316 -5.3819909 -4.9385691 -4.0791893 -3.307498 -3.153934 -3.3821959 -3.6093512][-1.7762134 -1.7061658 -2.3014405 -3.1351821 -3.9069719 -4.4261866 -4.8381066 -5.332437 -5.4364238 -4.9816709 -3.8850632 -2.7782431 -2.3964586 -2.4994726 -2.6580329][-2.0473561 -2.1477396 -2.9608383 -3.8443236 -4.377934 -4.469027 -4.5074024 -4.9458675 -5.28737 -5.1431065 -4.246995 -3.118382 -2.5692668 -2.4561253 -2.449245][-2.9765182 -3.2744493 -4.1822815 -4.8972383 -4.9351459 -4.3785257 -3.8683372 -4.1218534 -4.7148881 -5.0962462 -4.79568 -3.9821393 -3.3823497 -3.0591493 -2.8890033][-4.2072015 -4.5569315 -5.2717242 -5.5208621 -4.8707619 -3.5944917 -2.5125093 -2.5490808 -3.4191661 -4.4608531 -4.9935737 -4.7491918 -4.2518978 -3.7817783 -3.4602902][-5.1525416 -5.3941464 -5.6703076 -5.2528906 -3.8577337 -1.911618 -0.33372831 -0.18687534 -1.3420217 -3.0306091 -4.4257812 -4.9019785 -4.7097964 -4.23935 -3.7976677][-5.5515814 -5.7134156 -5.588017 -4.6183071 -2.6720676 -0.24619436 1.7482276 2.1118259 0.77348185 -1.3519204 -3.4137425 -4.564899 -4.77893 -4.472014 -4.0299087][-5.6362095 -5.8778763 -5.62601 -4.4564619 -2.310173 0.28039312 2.4815564 3.0292511 1.6560645 -0.55938077 -2.8299131 -4.3150887 -4.757699 -4.5843472 -4.1778708][-5.4866381 -5.9582338 -5.9130573 -4.9519205 -3.0076275 -0.63573313 1.4462452 2.0806336 0.82462406 -1.1804068 -3.21657 -4.5981274 -4.9447536 -4.7065411 -4.2065034][-5.0396447 -5.7875957 -6.1858363 -5.7379313 -4.2656527 -2.3061879 -0.48405981 0.14431715 -0.99053097 -2.7422576 -4.3869967 -5.395267 -5.3893509 -4.8517962 -4.07764][-4.3928528 -5.4137678 -6.28245 -6.3848686 -5.4266028 -3.9140177 -2.3664694 -1.7275355 -2.6545238 -4.128509 -5.3965054 -6.0321541 -5.6987963 -4.8658528 -3.8122392][-3.8462894 -5.076839 -6.2926517 -6.7985992 -6.2618561 -5.1536446 -3.8828192 -3.2285471 -3.8820567 -5.0430593 -5.9789724 -6.337667 -5.8249006 -4.8225193 -3.5948029][-3.4130223 -4.73063 -6.1258764 -6.8712626 -6.6515222 -5.9083166 -4.9675245 -4.3818803 -4.7664704 -5.5653019 -6.1424847 -6.2478137 -5.6512041 -4.6294928 -3.4290984][-3.0271742 -4.2606549 -5.6283426 -6.4511356 -6.4311333 -5.9906869 -5.4016948 -4.9923511 -5.1830397 -5.609139 -5.8483434 -5.7632647 -5.1994467 -4.3223958 -3.3390603][-2.5696802 -3.6560471 -4.8974538 -5.6784973 -5.7558451 -5.5061979 -5.1708527 -4.940475 -5.0374455 -5.2277517 -5.2841077 -5.1415086 -4.7114277 -4.0737214 -3.3818512]]...]
INFO - root - 2017-12-06 06:46:08.486191: step 2710, loss = 1.04, batch loss = 0.97 (19.8 examples/sec; 0.405 sec/batch; 37h:03m:40s remains)
INFO - root - 2017-12-06 06:46:12.722161: step 2720, loss = 0.91, batch loss = 0.84 (19.4 examples/sec; 0.412 sec/batch; 37h:46m:32s remains)
INFO - root - 2017-12-06 06:46:16.961225: step 2730, loss = 0.76, batch loss = 0.69 (19.6 examples/sec; 0.409 sec/batch; 37h:28m:08s remains)
INFO - root - 2017-12-06 06:46:21.201449: step 2740, loss = 1.20, batch loss = 1.13 (18.9 examples/sec; 0.422 sec/batch; 38h:40m:31s remains)
INFO - root - 2017-12-06 06:46:25.342793: step 2750, loss = 0.81, batch loss = 0.74 (18.7 examples/sec; 0.428 sec/batch; 39h:11m:06s remains)
INFO - root - 2017-12-06 06:46:29.514966: step 2760, loss = 0.93, batch loss = 0.86 (19.7 examples/sec; 0.407 sec/batch; 37h:15m:23s remains)
INFO - root - 2017-12-06 06:46:33.860273: step 2770, loss = 0.93, batch loss = 0.86 (19.6 examples/sec; 0.408 sec/batch; 37h:23m:42s remains)
INFO - root - 2017-12-06 06:46:38.214917: step 2780, loss = 0.82, batch loss = 0.75 (14.3 examples/sec; 0.558 sec/batch; 51h:05m:23s remains)
INFO - root - 2017-12-06 06:46:44.650646: step 2790, loss = 1.02, batch loss = 0.95 (12.6 examples/sec; 0.637 sec/batch; 58h:19m:27s remains)
INFO - root - 2017-12-06 06:46:50.701750: step 2800, loss = 0.81, batch loss = 0.74 (12.8 examples/sec; 0.627 sec/batch; 57h:25m:39s remains)
2017-12-06 06:46:51.263070: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8045526 -3.9602268 -5.4311304 -6.3007865 -6.5366955 -6.2554736 -5.7846594 -5.21305 -4.7792325 -4.9377675 -5.4267411 -5.90481 -6.1147633 -6.1420088 -5.8938794][-1.9872205 -3.0078154 -4.5261025 -5.6332226 -6.1575456 -6.0678844 -5.6030164 -4.9395728 -4.4510059 -4.6199617 -5.1817708 -5.7943568 -6.0911751 -6.1303911 -5.8982778][-1.6050954 -2.3216193 -3.5672288 -4.609807 -5.294713 -5.4291897 -5.0605721 -4.369256 -3.8266935 -4.0155745 -4.6663647 -5.4352846 -5.8990355 -6.0243692 -5.8455486][-1.8072653 -2.2213826 -2.9942255 -3.6488488 -4.2148037 -4.4185157 -4.1259031 -3.4988108 -3.0610175 -3.455241 -4.2987456 -5.2148962 -5.8344474 -6.0493 -5.9133387][-2.4034355 -2.6653023 -3.0607214 -3.2444253 -3.4461908 -3.4479222 -3.0858424 -2.5291991 -2.3127878 -3.06212 -4.1787148 -5.2299719 -5.9715 -6.2496915 -6.1372161][-3.04566 -3.2885168 -3.4524415 -3.2243903 -2.9123697 -2.4456425 -1.8117881 -1.2378049 -1.2466609 -2.3742635 -3.8028617 -5.0775337 -6.0079327 -6.4136882 -6.3750658][-3.3828177 -3.6688018 -3.7219024 -3.2065823 -2.4062076 -1.3964758 -0.41523647 0.25720644 0.1269722 -1.2515984 -2.9350204 -4.5230441 -5.7634735 -6.3780751 -6.4547844][-3.5634289 -3.8767223 -3.8898752 -3.2490854 -2.1646571 -0.79295778 0.45419788 1.3140135 1.2652879 -0.12455702 -1.8915482 -3.7634916 -5.3329468 -6.1759968 -6.3736658][-3.7758253 -3.9516358 -3.8710232 -3.3001213 -2.325563 -0.99219751 0.31743193 1.4128075 1.6140585 0.46428776 -1.1569118 -3.1018472 -4.8445549 -5.8469634 -6.1560984][-4.1553659 -4.110261 -3.9087458 -3.5616341 -3.0125029 -2.0751605 -0.9122262 0.33506393 0.78762007 0.026403904 -1.192596 -2.8558524 -4.4623089 -5.4627156 -5.8371234][-4.6375923 -4.3236523 -3.9114473 -3.711623 -3.5799422 -3.1459584 -2.268508 -1.0115845 -0.41385412 -0.82244706 -1.6114891 -2.8518043 -4.13815 -5.0338268 -5.4590411][-4.9656782 -4.3354487 -3.6424119 -3.4434972 -3.5637789 -3.5783815 -3.05482 -1.9730735 -1.3971453 -1.6274929 -2.1646242 -3.0887957 -4.0663228 -4.8067522 -5.223176][-5.3557482 -4.5013785 -3.6312394 -3.3719921 -3.5853295 -3.8955436 -3.677207 -2.8503251 -2.3645244 -2.5021052 -2.8945761 -3.6014206 -4.301486 -4.8294787 -5.1597495][-5.9078941 -4.9554534 -4.0060949 -3.6899061 -3.9281063 -4.3806672 -4.3517685 -3.76309 -3.382791 -3.4313936 -3.67562 -4.1828833 -4.6354194 -4.938014 -5.140729][-6.2220368 -5.3522415 -4.4688425 -4.151813 -4.380559 -4.8295574 -4.859282 -4.4359694 -4.1696053 -4.1636367 -4.2665133 -4.5992851 -4.88216 -5.0042391 -5.0645614]]...]
INFO - root - 2017-12-06 06:46:57.495933: step 2810, loss = 0.96, batch loss = 0.89 (13.0 examples/sec; 0.618 sec/batch; 56h:33m:55s remains)
INFO - root - 2017-12-06 06:47:03.500477: step 2820, loss = 0.69, batch loss = 0.62 (12.8 examples/sec; 0.623 sec/batch; 57h:03m:19s remains)
INFO - root - 2017-12-06 06:47:09.780290: step 2830, loss = 0.90, batch loss = 0.83 (12.9 examples/sec; 0.622 sec/batch; 56h:57m:08s remains)
INFO - root - 2017-12-06 06:47:16.020957: step 2840, loss = 0.74, batch loss = 0.67 (12.9 examples/sec; 0.619 sec/batch; 56h:38m:25s remains)
INFO - root - 2017-12-06 06:47:22.276286: step 2850, loss = 1.06, batch loss = 0.99 (13.0 examples/sec; 0.617 sec/batch; 56h:32m:06s remains)
INFO - root - 2017-12-06 06:47:28.536759: step 2860, loss = 0.91, batch loss = 0.84 (13.3 examples/sec; 0.601 sec/batch; 55h:03m:16s remains)
INFO - root - 2017-12-06 06:47:34.736631: step 2870, loss = 0.96, batch loss = 0.88 (13.3 examples/sec; 0.600 sec/batch; 54h:56m:53s remains)
INFO - root - 2017-12-06 06:47:40.876451: step 2880, loss = 0.95, batch loss = 0.88 (12.6 examples/sec; 0.637 sec/batch; 58h:20m:30s remains)
INFO - root - 2017-12-06 06:47:47.210324: step 2890, loss = 0.69, batch loss = 0.62 (12.5 examples/sec; 0.642 sec/batch; 58h:44m:29s remains)
INFO - root - 2017-12-06 06:47:53.332991: step 2900, loss = 0.95, batch loss = 0.88 (14.4 examples/sec; 0.554 sec/batch; 50h:43m:34s remains)
2017-12-06 06:47:53.766031: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8172259 -4.9113011 -4.9220285 -4.9217367 -4.7889261 -4.6067386 -4.5348697 -4.5836768 -4.791924 -5.0471048 -5.1048832 -5.2577128 -5.2051463 -4.6773667 -4.315938][-4.4202852 -4.5909696 -4.5630779 -4.5446424 -4.4047794 -4.1951828 -4.2345371 -4.4530516 -4.7879066 -5.085238 -5.1071458 -5.2497282 -5.1063919 -4.3785286 -3.9711909][-3.885664 -4.1922188 -4.0958204 -4.0095205 -3.9204042 -3.697876 -3.8582525 -4.347261 -4.84994 -5.10105 -4.9450126 -4.996841 -4.8093433 -3.9270868 -3.4512539][-3.1783247 -3.6632118 -3.5414572 -3.3396475 -3.2862115 -3.0543168 -3.3105683 -4.1041722 -4.7881203 -4.90866 -4.4187722 -4.2932305 -4.2580471 -3.5217113 -3.0373859][-2.6023164 -3.202852 -3.162591 -2.8710096 -2.7436137 -2.455817 -2.7104957 -3.6696224 -4.431778 -4.3786564 -3.5417418 -3.2140541 -3.4888263 -3.2169142 -2.8268747][-2.491416 -3.125401 -3.2287605 -2.9021783 -2.5846672 -2.1876755 -2.34959 -3.2771494 -4.0110369 -3.849122 -2.8385334 -2.3443332 -2.8097782 -3.0397425 -2.8772161][-2.8068497 -3.4411261 -3.6645312 -3.3213148 -2.8122261 -2.3558133 -2.4753513 -3.2598681 -3.8225045 -3.5524213 -2.5836558 -2.0698626 -2.5610418 -3.116657 -3.2186272][-3.3482885 -3.9892745 -4.2418151 -3.8646464 -3.2668643 -2.8590765 -2.9936738 -3.5812178 -3.8740187 -3.4392557 -2.5780296 -2.1930702 -2.7011962 -3.3622606 -3.5614161][-3.7700074 -4.4048514 -4.64889 -4.2904267 -3.7641649 -3.4664383 -3.6296551 -4.0499215 -4.1099911 -3.5578063 -2.7768555 -2.5387449 -3.068902 -3.6695235 -3.7829025][-3.9819138 -4.5156927 -4.6875443 -4.3819923 -4.0092053 -3.7922044 -3.9775953 -4.3583722 -4.3601131 -3.8597677 -3.1462302 -2.9669423 -3.4787664 -3.9081621 -3.8612471][-4.0494037 -4.4090896 -4.4455047 -4.2040753 -3.9582295 -3.7238398 -3.8985968 -4.3416839 -4.3968077 -4.0627489 -3.4284062 -3.2694993 -3.7671943 -4.0235786 -3.8779502][-3.9990287 -4.1489677 -4.0606933 -3.9237132 -3.7550905 -3.4341855 -3.5468011 -4.025641 -4.12987 -3.986006 -3.5645506 -3.496829 -3.9639068 -4.1062403 -3.9778838][-3.9784694 -3.9381888 -3.7908938 -3.7796907 -3.6324406 -3.2327094 -3.2793126 -3.7374208 -3.8245883 -3.7882457 -3.6508157 -3.7472897 -4.1437259 -4.2224665 -4.185111][-4.0884571 -3.9552972 -3.8483703 -3.9563248 -3.7928934 -3.3746555 -3.3919578 -3.7936616 -3.7942483 -3.7153838 -3.8205633 -4.1211772 -4.4023652 -4.4045854 -4.449018][-4.2157335 -4.1137567 -4.141489 -4.3564777 -4.1932917 -3.8247705 -3.8388071 -4.1512408 -4.0615749 -3.844321 -4.0369015 -4.4702592 -4.6212578 -4.5263658 -4.6002874]]...]
INFO - root - 2017-12-06 06:47:59.836304: step 2910, loss = 0.75, batch loss = 0.68 (12.7 examples/sec; 0.628 sec/batch; 57h:28m:53s remains)
INFO - root - 2017-12-06 06:48:05.881781: step 2920, loss = 1.24, batch loss = 1.16 (13.0 examples/sec; 0.616 sec/batch; 56h:24m:13s remains)
INFO - root - 2017-12-06 06:48:12.125340: step 2930, loss = 1.23, batch loss = 1.16 (13.1 examples/sec; 0.612 sec/batch; 55h:59m:03s remains)
INFO - root - 2017-12-06 06:48:18.308126: step 2940, loss = 0.87, batch loss = 0.80 (12.7 examples/sec; 0.630 sec/batch; 57h:38m:05s remains)
INFO - root - 2017-12-06 06:48:24.460251: step 2950, loss = 1.13, batch loss = 1.06 (13.0 examples/sec; 0.615 sec/batch; 56h:18m:42s remains)
INFO - root - 2017-12-06 06:48:30.750310: step 2960, loss = 0.94, batch loss = 0.87 (12.5 examples/sec; 0.638 sec/batch; 58h:22m:52s remains)
INFO - root - 2017-12-06 06:48:36.952418: step 2970, loss = 0.94, batch loss = 0.87 (13.1 examples/sec; 0.610 sec/batch; 55h:50m:58s remains)
INFO - root - 2017-12-06 06:48:43.186793: step 2980, loss = 0.86, batch loss = 0.79 (13.1 examples/sec; 0.611 sec/batch; 55h:55m:22s remains)
INFO - root - 2017-12-06 06:48:49.423653: step 2990, loss = 0.86, batch loss = 0.79 (12.8 examples/sec; 0.627 sec/batch; 57h:22m:51s remains)
INFO - root - 2017-12-06 06:48:55.735043: step 3000, loss = 0.95, batch loss = 0.88 (12.5 examples/sec; 0.640 sec/batch; 58h:35m:37s remains)
2017-12-06 06:48:56.350305: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6896858 -4.6522479 -4.5175152 -4.2723212 -4.0538969 -4.1038308 -4.446115 -4.6548519 -4.5688281 -4.4619408 -4.3843336 -4.3587413 -4.2721868 -4.1391268 -4.0913148][-4.6139121 -4.5282426 -4.3468709 -4.0183239 -3.7257767 -3.8076062 -4.3173652 -4.6221428 -4.4637766 -4.3100843 -4.2251687 -4.2199316 -4.1468754 -4.0313435 -4.0264311][-4.6530657 -4.5514235 -4.302443 -3.8509326 -3.4913003 -3.5977919 -4.1814857 -4.5000658 -4.3098178 -4.2184434 -4.2292647 -4.2919116 -4.288444 -4.2578621 -4.3062253][-4.9446125 -4.8704729 -4.556314 -3.9575176 -3.4972982 -3.5396137 -3.9950228 -4.1818886 -4.0318065 -4.1686277 -4.4524078 -4.6818576 -4.785594 -4.8527102 -4.9279728][-5.36486 -5.2977138 -4.9111824 -4.1644268 -3.5383358 -3.3728743 -3.4971757 -3.4690542 -3.461792 -3.9366755 -4.6311736 -5.1545262 -5.3850656 -5.5091925 -5.5775657][-5.6128879 -5.4806242 -4.9960704 -4.1248 -3.3194785 -2.8657124 -2.5475197 -2.2769341 -2.4303327 -3.2132478 -4.3519764 -5.2989478 -5.727334 -5.8903069 -5.9638796][-5.6180363 -5.3458052 -4.74464 -3.8472724 -3.0042517 -2.3348708 -1.5920954 -1.0410874 -1.2223401 -2.1366162 -3.5861864 -4.9348516 -5.5889387 -5.8204479 -5.9603434][-5.3796644 -4.9683647 -4.3171277 -3.5485547 -2.8388257 -2.0950058 -1.0747645 -0.26505852 -0.309371 -1.1617239 -2.7173998 -4.3012667 -5.1249065 -5.4550457 -5.719614][-4.9719129 -4.5203114 -3.9632905 -3.4652631 -2.9862363 -2.297884 -1.2298381 -0.27723837 -0.092729092 -0.78577805 -2.3078232 -3.9419699 -4.821084 -5.2117777 -5.5694284][-4.8038712 -4.4036536 -3.9937458 -3.7310219 -3.463526 -2.9427476 -2.0677302 -1.1222048 -0.68167496 -1.1610558 -2.5611148 -4.1162314 -4.9517078 -5.3099828 -5.6457343][-5.1163707 -4.8216829 -4.4992595 -4.267252 -4.0542355 -3.7069256 -3.1170545 -2.2521677 -1.6033223 -1.8926663 -3.1515393 -4.583818 -5.3423929 -5.6219015 -5.8661022][-5.5795035 -5.3868718 -5.0642772 -4.6913342 -4.3753047 -4.1364412 -3.7853734 -3.029439 -2.3196056 -2.5468369 -3.7160656 -5.0313058 -5.7284164 -5.9609928 -6.1146078][-5.8395276 -5.6824794 -5.2947259 -4.7292695 -4.2720766 -4.0996184 -3.8950603 -3.2613583 -2.6647322 -2.9469628 -4.06035 -5.2553329 -5.8987932 -6.1245084 -6.2155504][-5.7185392 -5.5589967 -5.1236281 -4.4579353 -3.9350886 -3.7915313 -3.6199784 -3.0885744 -2.6764462 -3.0258689 -4.054594 -5.1032424 -5.6997771 -5.962647 -6.0506468][-5.2847366 -5.1237082 -4.7002897 -4.0721331 -3.6081369 -3.5066957 -3.3299866 -2.8862383 -2.6275418 -2.9916193 -3.9040627 -4.8262758 -5.3866358 -5.6672335 -5.7414474]]...]
INFO - root - 2017-12-06 06:49:02.568619: step 3010, loss = 0.99, batch loss = 0.92 (12.3 examples/sec; 0.649 sec/batch; 59h:23m:27s remains)
INFO - root - 2017-12-06 06:49:08.843535: step 3020, loss = 1.03, batch loss = 0.96 (12.6 examples/sec; 0.633 sec/batch; 57h:53m:55s remains)
INFO - root - 2017-12-06 06:49:15.010357: step 3030, loss = 1.01, batch loss = 0.94 (13.2 examples/sec; 0.606 sec/batch; 55h:27m:56s remains)
INFO - root - 2017-12-06 06:49:21.194368: step 3040, loss = 0.97, batch loss = 0.90 (12.8 examples/sec; 0.624 sec/batch; 57h:09m:07s remains)
INFO - root - 2017-12-06 06:49:27.417912: step 3050, loss = 0.84, batch loss = 0.77 (14.5 examples/sec; 0.552 sec/batch; 50h:31m:10s remains)
INFO - root - 2017-12-06 06:49:33.635799: step 3060, loss = 0.91, batch loss = 0.84 (12.4 examples/sec; 0.644 sec/batch; 58h:55m:52s remains)
INFO - root - 2017-12-06 06:49:39.928489: step 3070, loss = 0.95, batch loss = 0.88 (12.5 examples/sec; 0.640 sec/batch; 58h:35m:11s remains)
INFO - root - 2017-12-06 06:49:46.122502: step 3080, loss = 1.03, batch loss = 0.96 (13.0 examples/sec; 0.614 sec/batch; 56h:13m:29s remains)
INFO - root - 2017-12-06 06:49:52.390219: step 3090, loss = 1.08, batch loss = 1.01 (12.7 examples/sec; 0.632 sec/batch; 57h:49m:15s remains)
INFO - root - 2017-12-06 06:49:58.617969: step 3100, loss = 1.00, batch loss = 0.93 (12.9 examples/sec; 0.622 sec/batch; 56h:53m:07s remains)
2017-12-06 06:49:59.221860: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3385706 -5.4303727 -5.4508061 -5.4652853 -5.4829111 -5.4846 -5.4836092 -5.5415573 -5.6562409 -5.725327 -5.6404991 -5.52313 -5.44741 -5.359271 -5.3275456][-5.0703726 -5.2276635 -5.2588663 -5.2643156 -5.2583785 -5.2101336 -5.1181517 -5.085525 -5.2061882 -5.3967161 -5.3954439 -5.2514529 -5.1351523 -5.0075006 -4.9536686][-4.6125259 -4.8455024 -4.8999929 -4.8839769 -4.8339572 -4.6968856 -4.462822 -4.2782869 -4.3990388 -4.78218 -4.9455681 -4.822422 -4.7094631 -4.5966454 -4.5508947][-4.0531244 -4.3435297 -4.4177456 -4.3690009 -4.2560573 -4.0023446 -3.5985684 -3.2322712 -3.3504357 -3.9529972 -4.3100209 -4.2519169 -4.2008328 -4.1413479 -4.11911][-3.4817953 -3.7697048 -3.8240929 -3.7363033 -3.5641313 -3.1927595 -2.6015525 -2.0418816 -2.1423662 -2.9228735 -3.447217 -3.477772 -3.5227263 -3.5331864 -3.5512218][-2.9240906 -3.1613195 -3.1650205 -3.032793 -2.83463 -2.4186139 -1.6898437 -0.95119047 -1.0145252 -1.9054797 -2.56996 -2.6705923 -2.7881393 -2.8896594 -2.9992113][-2.6540179 -2.8350725 -2.7928085 -2.6244664 -2.4381511 -2.0956657 -1.3559172 -0.51869559 -0.53832054 -1.4493752 -2.2027674 -2.3179553 -2.4208691 -2.584758 -2.7994864][-2.7902246 -2.9585335 -2.9222302 -2.7437811 -2.5944397 -2.4176743 -1.8202353 -1.0342698 -0.96402454 -1.7520363 -2.5037565 -2.6037445 -2.6211677 -2.7805529 -3.0362744][-3.2253594 -3.3866186 -3.3908458 -3.2470169 -3.1402783 -3.0768704 -2.6609151 -2.0257454 -1.8664391 -2.4380479 -3.0982647 -3.2178047 -3.1842866 -3.2933855 -3.4986551][-3.7186127 -3.8469853 -3.8792238 -3.8044932 -3.7667329 -3.7654943 -3.5022407 -3.0480113 -2.8649998 -3.2357352 -3.7520227 -3.8771908 -3.8149576 -3.8451498 -3.9576006][-4.1346235 -4.2319422 -4.2895341 -4.2927742 -4.3275671 -4.36781 -4.2361784 -3.9564047 -3.8044577 -4.0254292 -4.3815002 -4.4671884 -4.3495927 -4.2605252 -4.2429361][-4.3445587 -4.4376054 -4.5339422 -4.6114163 -4.6895132 -4.743659 -4.7004194 -4.5549092 -4.4511323 -4.5882096 -4.8180122 -4.85232 -4.6967731 -4.5271425 -4.3829126][-4.3653884 -4.4545383 -4.5772185 -4.7005239 -4.789371 -4.8417554 -4.8537688 -4.8063717 -4.7535367 -4.8456678 -5.0015612 -5.0244441 -4.8997025 -4.73179 -4.5171423][-4.3220639 -4.3858094 -4.4977927 -4.6360192 -4.7349238 -4.783895 -4.8145552 -4.8187814 -4.8104529 -4.8793278 -4.9969063 -5.0334115 -4.9678779 -4.8500113 -4.6273265][-4.2716551 -4.3113275 -4.3961649 -4.519125 -4.6179671 -4.6722903 -4.7097754 -4.7425656 -4.778203 -4.850276 -4.9485979 -4.9997988 -4.98071 -4.9082527 -4.6923547]]...]
INFO - root - 2017-12-06 06:50:05.507217: step 3110, loss = 0.86, batch loss = 0.79 (12.1 examples/sec; 0.659 sec/batch; 60h:16m:02s remains)
INFO - root - 2017-12-06 06:50:11.675609: step 3120, loss = 0.93, batch loss = 0.86 (12.6 examples/sec; 0.633 sec/batch; 57h:52m:38s remains)
INFO - root - 2017-12-06 06:50:17.932192: step 3130, loss = 1.06, batch loss = 0.99 (13.3 examples/sec; 0.603 sec/batch; 55h:12m:20s remains)
INFO - root - 2017-12-06 06:50:24.211098: step 3140, loss = 0.89, batch loss = 0.82 (12.6 examples/sec; 0.633 sec/batch; 57h:56m:24s remains)
INFO - root - 2017-12-06 06:50:29.022404: step 3150, loss = 0.93, batch loss = 0.86 (18.1 examples/sec; 0.441 sec/batch; 40h:22m:36s remains)
INFO - root - 2017-12-06 06:50:33.106847: step 3160, loss = 0.94, batch loss = 0.87 (18.8 examples/sec; 0.424 sec/batch; 38h:49m:53s remains)
INFO - root - 2017-12-06 06:50:37.352152: step 3170, loss = 0.74, batch loss = 0.67 (19.0 examples/sec; 0.422 sec/batch; 38h:35m:08s remains)
INFO - root - 2017-12-06 06:50:41.550889: step 3180, loss = 1.08, batch loss = 1.01 (19.1 examples/sec; 0.418 sec/batch; 38h:14m:53s remains)
INFO - root - 2017-12-06 06:50:45.732201: step 3190, loss = 1.02, batch loss = 0.95 (19.2 examples/sec; 0.417 sec/batch; 38h:10m:04s remains)
INFO - root - 2017-12-06 06:50:49.898951: step 3200, loss = 0.98, batch loss = 0.91 (19.7 examples/sec; 0.405 sec/batch; 37h:04m:12s remains)
2017-12-06 06:50:50.374143: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0168128 -5.0463948 -5.0800276 -5.0290184 -4.9311118 -5.0092826 -5.2357516 -5.1955376 -4.9935036 -4.9681849 -5.0779061 -5.2000947 -5.2896872 -5.165617 -4.7327571][-5.2309465 -5.2814379 -5.3500266 -5.2900224 -5.1490278 -5.2364779 -5.5052319 -5.3884668 -5.0548358 -5.0383444 -5.2278676 -5.4315691 -5.6264362 -5.5996113 -5.1400733][-5.2823548 -5.2719612 -5.3108182 -5.22733 -5.0397754 -5.0988307 -5.3187404 -5.0694175 -4.6203995 -4.6807652 -4.9824533 -5.2571912 -5.575181 -5.7408028 -5.3985357][-5.2795429 -5.1547565 -5.0975027 -4.96823 -4.7327485 -4.7168665 -4.8063016 -4.3996973 -3.8957648 -4.0833445 -4.4812779 -4.7708616 -5.1722836 -5.549736 -5.4279485][-5.339159 -5.0929432 -4.9020495 -4.7120118 -4.4339581 -4.30429 -4.218399 -3.7005603 -3.2200322 -3.4934897 -3.8667533 -4.0648437 -4.4857178 -5.051177 -5.2199197][-5.4456978 -5.0938673 -4.7707376 -4.5156722 -4.1763935 -3.8675804 -3.5484245 -2.9370947 -2.505424 -2.7840531 -3.0420184 -3.1172013 -3.5539682 -4.3091431 -4.8340845][-5.3927951 -4.9548182 -4.5241785 -4.2206268 -3.8343425 -3.3782921 -2.8669696 -2.2202752 -1.8924749 -2.1784503 -2.3280945 -2.3088326 -2.7850215 -3.72313 -4.5646052][-5.1018009 -4.61764 -4.1716757 -3.8837087 -3.5093083 -3.0035548 -2.4293795 -1.8426354 -1.6907847 -2.0463798 -2.1686752 -2.1175797 -2.6367316 -3.6750402 -4.6519818][-4.77282 -4.3606472 -4.0384393 -3.854109 -3.5480013 -3.0579102 -2.5070798 -2.0170622 -2.0422654 -2.4768078 -2.6206799 -2.6066093 -3.1537271 -4.1632619 -5.053103][-4.6374564 -4.3687253 -4.209374 -4.1574326 -3.9406781 -3.5253141 -3.0956316 -2.7649961 -2.91918 -3.3918324 -3.5656338 -3.5945323 -4.0926619 -4.9289651 -5.5487838][-4.7508469 -4.647325 -4.6191993 -4.6669497 -4.5382481 -4.2240987 -3.9542739 -3.789794 -3.9933054 -4.4252295 -4.6051431 -4.6509552 -5.0284195 -5.6138458 -5.9025087][-4.9296026 -4.98538 -5.0613327 -5.1694202 -5.1012015 -4.8745995 -4.7379742 -4.6969619 -4.8828578 -5.1999741 -5.3318172 -5.3585625 -5.5983419 -5.9377513 -5.9453411][-4.8640432 -5.0360322 -5.183423 -5.3150649 -5.2860403 -5.141459 -5.0957165 -5.1310382 -5.2745152 -5.4603767 -5.5171213 -5.5056643 -5.62759 -5.7771206 -5.6233435][-4.5619745 -4.7456121 -4.8922181 -5.0004478 -4.9858947 -4.9048219 -4.9091043 -4.9755883 -5.0625648 -5.13402 -5.1281404 -5.1070871 -5.1795006 -5.2391739 -5.067184][-4.2494965 -4.3608322 -4.4494438 -4.5089054 -4.5021591 -4.4761395 -4.5161834 -4.58933 -4.6372781 -4.6477246 -4.6225615 -4.6144609 -4.663136 -4.6810155 -4.5569973]]...]
INFO - root - 2017-12-06 06:50:54.632410: step 3210, loss = 0.99, batch loss = 0.92 (18.5 examples/sec; 0.432 sec/batch; 39h:29m:26s remains)
INFO - root - 2017-12-06 06:50:58.605109: step 3220, loss = 0.76, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 18h:56m:10s remains)
INFO - root - 2017-12-06 06:51:02.787366: step 3230, loss = 0.93, batch loss = 0.86 (18.9 examples/sec; 0.422 sec/batch; 38h:37m:37s remains)
INFO - root - 2017-12-06 06:51:06.922566: step 3240, loss = 0.90, batch loss = 0.83 (18.8 examples/sec; 0.425 sec/batch; 38h:50m:57s remains)
INFO - root - 2017-12-06 06:51:11.159637: step 3250, loss = 0.85, batch loss = 0.78 (18.8 examples/sec; 0.426 sec/batch; 38h:57m:02s remains)
INFO - root - 2017-12-06 06:51:15.329537: step 3260, loss = 0.99, batch loss = 0.92 (18.7 examples/sec; 0.427 sec/batch; 39h:02m:11s remains)
INFO - root - 2017-12-06 06:51:19.588096: step 3270, loss = 1.13, batch loss = 1.06 (19.6 examples/sec; 0.409 sec/batch; 37h:25m:21s remains)
INFO - root - 2017-12-06 06:51:23.719028: step 3280, loss = 0.80, batch loss = 0.73 (20.0 examples/sec; 0.399 sec/batch; 36h:31m:06s remains)
INFO - root - 2017-12-06 06:51:27.939299: step 3290, loss = 0.86, batch loss = 0.79 (19.5 examples/sec; 0.409 sec/batch; 37h:25m:51s remains)
INFO - root - 2017-12-06 06:51:32.132530: step 3300, loss = 0.69, batch loss = 0.62 (18.5 examples/sec; 0.433 sec/batch; 39h:34m:57s remains)
2017-12-06 06:51:32.625438: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0668507 -4.18398 -4.2142553 -4.1358218 -4.0123014 -3.9882915 -4.0677137 -4.1827984 -4.4479237 -4.530643 -4.3621879 -4.176331 -4.0191741 -3.9294479 -3.921947][-4.4367852 -4.6824861 -4.7863774 -4.6587896 -4.3585787 -4.145658 -4.0924606 -4.2326269 -4.6755729 -4.8631015 -4.7151403 -4.4895158 -4.2444029 -4.062942 -3.9912889][-4.9892139 -5.3165016 -5.4169512 -5.1762342 -4.6202717 -4.0718451 -3.7270491 -3.8305233 -4.5352383 -4.95799 -4.9672356 -4.7636662 -4.4161444 -4.1112385 -3.9692965][-5.4637122 -5.7997136 -5.83816 -5.474925 -4.6642966 -3.707274 -2.9420197 -2.9242628 -3.9004991 -4.6466537 -4.9685006 -4.9091392 -4.5027008 -4.0466771 -3.8369112][-5.6875095 -5.9960842 -5.9739876 -5.5136561 -4.5299053 -3.2217107 -1.9794588 -1.7227185 -2.870894 -3.9399049 -4.6796851 -4.9110093 -4.575912 -4.039712 -3.7974935][-5.5928593 -5.8401737 -5.7137184 -5.1469173 -4.0538454 -2.4790766 -0.79851437 -0.24171925 -1.480284 -2.8374062 -3.9902484 -4.6040163 -4.521523 -4.1079454 -3.9353848][-5.2573881 -5.4211278 -5.1603374 -4.4864693 -3.3212552 -1.5819528 0.37090302 1.1568136 -0.16744184 -1.763706 -3.239027 -4.1958308 -4.3987126 -4.2195578 -4.1810112][-4.92865 -5.0490603 -4.7119093 -4.0109453 -2.889575 -1.2293139 0.59775925 1.3397183 0.029702663 -1.5577157 -3.0564816 -4.0917134 -4.4205475 -4.4118056 -4.4586234][-4.8045139 -4.9506454 -4.6073146 -3.9227872 -2.9048555 -1.5231164 -0.21261072 0.14598989 -1.1146479 -2.4815178 -3.7262373 -4.5601096 -4.8062181 -4.82058 -4.8468485][-4.8046741 -4.9991436 -4.6910796 -4.0338778 -3.1261108 -2.0526094 -1.2934017 -1.373507 -2.5864077 -3.6852419 -4.5664778 -5.0627403 -5.1248927 -5.0912719 -5.0757222][-4.8617711 -5.0864277 -4.861721 -4.3049579 -3.5461218 -2.7428551 -2.3579187 -2.694531 -3.7508841 -4.5465045 -5.0466537 -5.1863403 -5.0495152 -4.9537883 -4.9443226][-4.9320602 -5.1786633 -5.1008673 -4.7430058 -4.1969805 -3.6229773 -3.4088969 -3.7353323 -4.4816222 -4.9497609 -5.1543779 -5.0744133 -4.836514 -4.735209 -4.7912631][-4.905448 -5.1343417 -5.1808758 -5.0331507 -4.7200131 -4.3518672 -4.20747 -4.3793116 -4.7599745 -4.9328213 -4.9440513 -4.794878 -4.5838232 -4.5318465 -4.65408][-4.7922463 -4.9665 -5.0491538 -5.0107722 -4.8480711 -4.6225457 -4.5082703 -4.5444841 -4.6724706 -4.6792336 -4.6211624 -4.5103412 -4.3922858 -4.3957086 -4.5321722][-4.6150341 -4.7345276 -4.796371 -4.7838573 -4.6955848 -4.5758677 -4.5057192 -4.4922132 -4.5027857 -4.456049 -4.395679 -4.3357377 -4.2874947 -4.3104048 -4.4093347]]...]
INFO - root - 2017-12-06 06:51:36.774751: step 3310, loss = 1.00, batch loss = 0.93 (19.6 examples/sec; 0.407 sec/batch; 37h:14m:13s remains)
INFO - root - 2017-12-06 06:51:40.962507: step 3320, loss = 1.20, batch loss = 1.13 (18.7 examples/sec; 0.428 sec/batch; 39h:05m:50s remains)
INFO - root - 2017-12-06 06:51:44.772991: step 3330, loss = 0.85, batch loss = 0.78 (19.1 examples/sec; 0.419 sec/batch; 38h:17m:00s remains)
INFO - root - 2017-12-06 06:51:48.939113: step 3340, loss = 1.08, batch loss = 1.01 (19.4 examples/sec; 0.413 sec/batch; 37h:45m:23s remains)
INFO - root - 2017-12-06 06:51:53.302699: step 3350, loss = 0.96, batch loss = 0.89 (20.2 examples/sec; 0.395 sec/batch; 36h:07m:59s remains)
INFO - root - 2017-12-06 06:51:57.782823: step 3360, loss = 0.92, batch loss = 0.85 (13.8 examples/sec; 0.579 sec/batch; 52h:57m:17s remains)
INFO - root - 2017-12-06 06:52:03.892214: step 3370, loss = 0.91, batch loss = 0.84 (13.4 examples/sec; 0.595 sec/batch; 54h:25m:57s remains)
INFO - root - 2017-12-06 06:52:09.864597: step 3380, loss = 0.68, batch loss = 0.61 (13.3 examples/sec; 0.601 sec/batch; 54h:57m:46s remains)
INFO - root - 2017-12-06 06:52:15.876184: step 3390, loss = 0.71, batch loss = 0.64 (13.4 examples/sec; 0.598 sec/batch; 54h:41m:10s remains)
INFO - root - 2017-12-06 06:52:21.946267: step 3400, loss = 0.86, batch loss = 0.79 (13.0 examples/sec; 0.613 sec/batch; 56h:03m:53s remains)
2017-12-06 06:52:22.444352: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2886333 -3.6910248 -3.8059089 -3.6464632 -3.6171646 -4.1458564 -4.642787 -4.6154618 -4.3703928 -4.2550488 -4.3515224 -4.1681976 -3.9439657 -3.9686105 -4.1277819][-3.5917821 -4.1541204 -4.4030714 -4.4043455 -4.5556746 -5.1076207 -5.4067717 -5.0509787 -4.4415169 -4.0802126 -4.1102338 -3.8765302 -3.4908156 -3.3466048 -3.4656308][-4.02376 -4.6775389 -5.0656376 -5.2875605 -5.5705514 -5.9689884 -5.9115224 -5.1987581 -4.3330503 -3.8736818 -3.9719863 -3.7912083 -3.24797 -2.8475194 -2.8733597][-4.4286141 -5.0677967 -5.5369539 -5.9044929 -6.1531043 -6.1940813 -5.6781349 -4.6832838 -3.7966542 -3.4680142 -3.759789 -3.7154281 -3.1236572 -2.5834661 -2.5764503][-4.5587993 -5.1553793 -5.6291752 -5.9932179 -6.0264845 -5.62016 -4.6833448 -3.5741074 -2.9032493 -2.8941336 -3.4261632 -3.505794 -2.9499249 -2.4405327 -2.542974][-4.3371015 -4.9253516 -5.3665695 -5.6015644 -5.3406167 -4.5046844 -3.2244833 -2.0933025 -1.7134485 -2.0996966 -2.9261284 -3.204119 -2.8459225 -2.5955482 -2.9374714][-3.9754167 -4.5099854 -4.8660135 -4.8921108 -4.3189077 -3.1259503 -1.619262 -0.55030513 -0.53894496 -1.400696 -2.5801716 -3.1464293 -3.1159151 -3.2164907 -3.7974749][-3.7001016 -4.1208639 -4.3624477 -4.20037 -3.4057465 -2.0389578 -0.50984859 0.37247372 -0.028080463 -1.3199134 -2.7589 -3.5541701 -3.7820611 -4.0869417 -4.7289896][-3.7666287 -4.0210166 -4.1805406 -3.9977102 -3.2342069 -2.0038095 -0.71346307 -0.091499329 -0.70892358 -2.0953925 -3.484668 -4.2713823 -4.5362878 -4.7722306 -5.2322044][-3.7946448 -3.9557557 -4.1950307 -4.2474427 -3.7791297 -2.9016328 -1.9479554 -1.5231209 -2.1831064 -3.4440925 -4.5405197 -5.0289612 -5.0125532 -4.8993697 -4.9900961][-3.5154769 -3.6230688 -4.0295458 -4.4266114 -4.3396115 -3.8397889 -3.1743503 -2.9222722 -3.6303763 -4.7479677 -5.5071187 -5.5312562 -5.0381603 -4.4661856 -4.1749287][-2.991096 -3.0502748 -3.6011043 -4.2749033 -4.5039659 -4.3080335 -3.8710377 -3.7822285 -4.5168033 -5.4514551 -5.8422585 -5.3693371 -4.4332032 -3.5781174 -3.1333804][-2.4118679 -2.4346042 -3.0716403 -3.8820741 -4.2693672 -4.2241812 -3.924123 -3.9692309 -4.6737661 -5.3703747 -5.4386125 -4.6530328 -3.5001535 -2.5818095 -2.2158706][-2.0424685 -2.0150154 -2.6747823 -3.520381 -3.9470553 -3.913137 -3.6587472 -3.8065481 -4.4486709 -4.9035783 -4.7388525 -3.8729584 -2.7298617 -1.8692069 -1.6406825][-2.0943844 -2.0576398 -2.6912208 -3.5168254 -3.9304476 -3.8389559 -3.5757561 -3.7606895 -4.3189249 -4.562952 -4.2399125 -3.4193144 -2.4099479 -1.6665118 -1.5378566]]...]
INFO - root - 2017-12-06 06:52:28.461653: step 3410, loss = 0.95, batch loss = 0.88 (13.5 examples/sec; 0.593 sec/batch; 54h:12m:04s remains)
INFO - root - 2017-12-06 06:52:34.479591: step 3420, loss = 0.88, batch loss = 0.81 (12.9 examples/sec; 0.622 sec/batch; 56h:53m:01s remains)
INFO - root - 2017-12-06 06:52:40.486377: step 3430, loss = 1.05, batch loss = 0.98 (13.6 examples/sec; 0.589 sec/batch; 53h:51m:58s remains)
INFO - root - 2017-12-06 06:52:46.338465: step 3440, loss = 0.81, batch loss = 0.74 (13.7 examples/sec; 0.586 sec/batch; 53h:33m:56s remains)
INFO - root - 2017-12-06 06:52:52.450267: step 3450, loss = 1.11, batch loss = 1.04 (13.1 examples/sec; 0.610 sec/batch; 55h:45m:13s remains)
INFO - root - 2017-12-06 06:52:58.532108: step 3460, loss = 0.94, batch loss = 0.87 (13.1 examples/sec; 0.611 sec/batch; 55h:48m:12s remains)
INFO - root - 2017-12-06 06:53:04.616250: step 3470, loss = 0.86, batch loss = 0.79 (12.9 examples/sec; 0.619 sec/batch; 56h:36m:14s remains)
INFO - root - 2017-12-06 06:53:10.623324: step 3480, loss = 1.01, batch loss = 0.94 (13.1 examples/sec; 0.609 sec/batch; 55h:38m:13s remains)
INFO - root - 2017-12-06 06:53:16.645069: step 3490, loss = 0.90, batch loss = 0.83 (13.1 examples/sec; 0.613 sec/batch; 56h:00m:18s remains)
INFO - root - 2017-12-06 06:53:22.678039: step 3500, loss = 0.64, batch loss = 0.57 (13.5 examples/sec; 0.595 sec/batch; 54h:20m:09s remains)
2017-12-06 06:53:23.391588: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6310349 -4.1212826 -4.4451809 -4.4384036 -4.0000353 -3.6208217 -3.5124681 -3.5348744 -3.4370594 -3.4203458 -3.3967566 -3.4661779 -3.9316552 -4.3496432 -4.2273984][-3.9474137 -4.3487725 -4.5600147 -4.6279106 -4.3507571 -3.8451266 -3.5186536 -3.3936591 -3.2563071 -3.3382773 -3.4693394 -3.7028871 -4.2826786 -4.6735005 -4.3810244][-4.026547 -4.5129924 -4.7580442 -4.9310713 -4.7443914 -4.0287318 -3.4407492 -3.1616802 -3.0486608 -3.2564654 -3.5400982 -3.9663289 -4.6186075 -4.9383869 -4.548563][-3.7789192 -4.4740357 -4.8794308 -5.1439252 -4.9443154 -4.0158582 -3.1995683 -2.8008471 -2.7597876 -3.1035621 -3.5437968 -4.1680551 -4.91558 -5.2556853 -4.9436865][-3.2847166 -4.2116404 -4.8165107 -5.1070762 -4.8010259 -3.7199938 -2.7823963 -2.3163996 -2.3399906 -2.75825 -3.328702 -4.11983 -4.9567471 -5.3950334 -5.2775683][-2.8503976 -3.8959179 -4.6273994 -4.9068928 -4.4740143 -3.3089204 -2.3061059 -1.7953777 -1.8306928 -2.2251625 -2.8730721 -3.7483275 -4.6109905 -5.1460614 -5.202383][-2.8695064 -3.8570442 -4.574574 -4.8004704 -4.2618833 -3.0860658 -2.0550842 -1.501883 -1.4658689 -1.716217 -2.338876 -3.1911354 -4.003746 -4.6179185 -4.8202333][-3.4089637 -4.2427821 -4.845067 -4.9992671 -4.4347382 -3.3772283 -2.4214625 -1.8960617 -1.7736893 -1.7962294 -2.2411578 -2.9222991 -3.5833955 -4.2408566 -4.5606976][-4.088172 -4.7145057 -5.161521 -5.3027844 -4.8519492 -4.0712 -3.342535 -2.9557612 -2.796617 -2.5941432 -2.7547107 -3.1588411 -3.6003604 -4.2394114 -4.627274][-4.5028648 -4.9147635 -5.2084117 -5.365695 -5.0901341 -4.6168427 -4.161767 -3.9588668 -3.8282948 -3.500701 -3.3929219 -3.5052953 -3.725646 -4.3092456 -4.7235813][-4.5171804 -4.757091 -4.9296818 -5.0909181 -4.9543653 -4.6997023 -4.4743342 -4.4376197 -4.3551021 -3.9971805 -3.7258291 -3.61746 -3.6505549 -4.1483049 -4.5603456][-4.1835866 -4.3441477 -4.4349337 -4.574913 -4.523066 -4.3996735 -4.3562241 -4.4597058 -4.4210243 -4.0443726 -3.6926126 -3.4664817 -3.3738794 -3.7803471 -4.1986609][-3.691184 -3.8477731 -3.8997481 -4.0164208 -4.0125113 -3.9756317 -4.0827456 -4.29892 -4.2887177 -3.8650978 -3.4563177 -3.1828642 -3.0339499 -3.3760304 -3.8160915][-3.343159 -3.4925745 -3.4938462 -3.5537539 -3.5519695 -3.5656791 -3.7672534 -4.04409 -4.0487461 -3.5913746 -3.1546645 -2.8896484 -2.7475379 -3.0456095 -3.4931519][-3.2115767 -3.2998967 -3.2135103 -3.1624432 -3.1153793 -3.1513653 -3.4004335 -3.7022967 -3.7236004 -3.2735114 -2.8374126 -2.5980124 -2.5023112 -2.7835445 -3.2199423]]...]
INFO - root - 2017-12-06 06:53:29.495803: step 3510, loss = 0.94, batch loss = 0.87 (13.5 examples/sec; 0.592 sec/batch; 54h:04m:22s remains)
INFO - root - 2017-12-06 06:53:35.492647: step 3520, loss = 1.11, batch loss = 1.04 (13.3 examples/sec; 0.600 sec/batch; 54h:49m:43s remains)
INFO - root - 2017-12-06 06:53:41.572367: step 3530, loss = 0.90, batch loss = 0.83 (13.1 examples/sec; 0.611 sec/batch; 55h:52m:39s remains)
INFO - root - 2017-12-06 06:53:47.707159: step 3540, loss = 0.95, batch loss = 0.88 (12.9 examples/sec; 0.622 sec/batch; 56h:49m:21s remains)
INFO - root - 2017-12-06 06:53:53.598403: step 3550, loss = 0.61, batch loss = 0.54 (13.2 examples/sec; 0.604 sec/batch; 55h:11m:18s remains)
INFO - root - 2017-12-06 06:53:59.668350: step 3560, loss = 0.82, batch loss = 0.75 (13.0 examples/sec; 0.613 sec/batch; 56h:03m:17s remains)
INFO - root - 2017-12-06 06:54:05.707514: step 3570, loss = 0.87, batch loss = 0.80 (13.3 examples/sec; 0.601 sec/batch; 54h:54m:28s remains)
INFO - root - 2017-12-06 06:54:11.805044: step 3580, loss = 0.67, batch loss = 0.60 (13.3 examples/sec; 0.604 sec/batch; 55h:08m:53s remains)
INFO - root - 2017-12-06 06:54:17.810310: step 3590, loss = 0.92, batch loss = 0.85 (13.2 examples/sec; 0.608 sec/batch; 55h:31m:21s remains)
INFO - root - 2017-12-06 06:54:23.818390: step 3600, loss = 0.93, batch loss = 0.86 (13.6 examples/sec; 0.588 sec/batch; 53h:44m:13s remains)
2017-12-06 06:54:24.407550: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9853368 -5.1678624 -5.1011357 -4.6624112 -3.9121246 -3.2982407 -3.1767745 -3.2995682 -3.3546596 -3.4720833 -3.6458247 -3.8212564 -3.9211676 -3.9153986 -3.845655][-4.8335981 -5.0475345 -4.9946556 -4.5210929 -3.6906877 -2.9718485 -2.7289085 -2.7403536 -2.7489705 -2.93587 -3.312201 -3.6359968 -3.7762909 -3.7871461 -3.7477105][-4.4550252 -4.7318978 -4.7470217 -4.3466539 -3.5836015 -2.8388071 -2.4656563 -2.3555059 -2.2776721 -2.4256561 -2.8849821 -3.3410764 -3.5960457 -3.6756592 -3.6666133][-3.8875828 -4.3076043 -4.4947996 -4.296989 -3.7303658 -3.0403507 -2.5652199 -2.3275321 -2.1437397 -2.1889658 -2.6306195 -3.1704898 -3.5421686 -3.6738284 -3.6375372][-3.5426006 -4.1166749 -4.4905477 -4.4895277 -4.090126 -3.4238892 -2.830296 -2.4772544 -2.2414517 -2.2379377 -2.6620698 -3.2418973 -3.6698167 -3.7857008 -3.6616712][-3.4435627 -4.0613627 -4.5375934 -4.6633396 -4.3680925 -3.6958213 -3.0044544 -2.5837307 -2.3677585 -2.4029071 -2.8336463 -3.4205246 -3.8645291 -3.9525108 -3.7507873][-3.3098605 -3.8997803 -4.4511414 -4.6829405 -4.4649529 -3.7944736 -3.0515542 -2.6007004 -2.4219766 -2.4931502 -2.890162 -3.432339 -3.87203 -3.9827538 -3.7805912][-3.0777836 -3.6243153 -4.261014 -4.6030726 -4.4549365 -3.8191862 -3.0865233 -2.6354179 -2.4590955 -2.5019078 -2.7965486 -3.2480679 -3.6801748 -3.8627002 -3.7389345][-2.9070077 -3.3903308 -4.0896468 -4.5233741 -4.4578586 -3.9119864 -3.2622724 -2.832936 -2.6154199 -2.571166 -2.7529674 -3.1469254 -3.6048856 -3.8685894 -3.8185642][-2.873857 -3.3054667 -4.0280285 -4.5196271 -4.5263786 -4.0776048 -3.5398831 -3.1523781 -2.8956392 -2.7711573 -2.8845901 -3.2698109 -3.7690129 -4.0824208 -4.0592823][-2.971216 -3.3418446 -4.01644 -4.5119543 -4.5835447 -4.2513304 -3.8334844 -3.499455 -3.2257864 -3.0480952 -3.1313024 -3.5313172 -4.0406704 -4.3268509 -4.2745867][-3.0913661 -3.3627613 -3.9263597 -4.4058671 -4.5536265 -4.3508854 -4.0178633 -3.6982574 -3.3994536 -3.1946781 -3.2825906 -3.7195644 -4.2189903 -4.4341006 -4.3179173][-3.1791577 -3.3211606 -3.7357595 -4.1729755 -4.3787704 -4.2846932 -3.9988217 -3.6796587 -3.3672705 -3.1623755 -3.2742696 -3.7446954 -4.2207203 -4.3583612 -4.1991549][-3.2331753 -3.2827439 -3.5693841 -3.9458272 -4.1728692 -4.1540179 -3.9123068 -3.6147721 -3.3161654 -3.1269956 -3.2434411 -3.6834264 -4.08893 -4.1668143 -4.0138555][-3.2973332 -3.3082032 -3.503799 -3.8027239 -4.0091815 -4.0345325 -3.858541 -3.6204922 -3.370851 -3.2083974 -3.2997835 -3.6450548 -3.9340858 -3.9455366 -3.813447]]...]
INFO - root - 2017-12-06 06:54:30.537568: step 3610, loss = 1.09, batch loss = 1.02 (13.3 examples/sec; 0.600 sec/batch; 54h:50m:19s remains)
INFO - root - 2017-12-06 06:54:36.614867: step 3620, loss = 0.72, batch loss = 0.65 (13.1 examples/sec; 0.609 sec/batch; 55h:37m:52s remains)
INFO - root - 2017-12-06 06:54:42.708470: step 3630, loss = 0.77, batch loss = 0.70 (12.8 examples/sec; 0.626 sec/batch; 57h:11m:04s remains)
INFO - root - 2017-12-06 06:54:48.738009: step 3640, loss = 0.91, batch loss = 0.84 (13.3 examples/sec; 0.601 sec/batch; 54h:56m:10s remains)
INFO - root - 2017-12-06 06:54:54.600824: step 3650, loss = 0.91, batch loss = 0.84 (15.3 examples/sec; 0.522 sec/batch; 47h:40m:03s remains)
INFO - root - 2017-12-06 06:55:00.633255: step 3660, loss = 1.05, batch loss = 0.98 (13.0 examples/sec; 0.613 sec/batch; 56h:01m:13s remains)
INFO - root - 2017-12-06 06:55:06.493691: step 3670, loss = 0.88, batch loss = 0.81 (16.0 examples/sec; 0.501 sec/batch; 45h:47m:24s remains)
INFO - root - 2017-12-06 06:55:12.600491: step 3680, loss = 0.91, batch loss = 0.84 (12.8 examples/sec; 0.626 sec/batch; 57h:09m:13s remains)
INFO - root - 2017-12-06 06:55:18.601913: step 3690, loss = 0.99, batch loss = 0.92 (13.8 examples/sec; 0.579 sec/batch; 52h:51m:04s remains)
INFO - root - 2017-12-06 06:55:24.690371: step 3700, loss = 0.68, batch loss = 0.61 (13.2 examples/sec; 0.605 sec/batch; 55h:13m:27s remains)
2017-12-06 06:55:25.330793: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.224014 -2.816215 -3.3363941 -3.7212293 -3.8602016 -3.8537178 -3.7652164 -3.6830842 -3.7542932 -3.9233761 -4.0724773 -4.1495652 -4.13979 -4.13518 -4.1464643][-2.5662069 -3.1109347 -3.6294236 -4.0421329 -4.2259274 -4.2840295 -4.2567344 -4.2174382 -4.2864704 -4.3950806 -4.4630222 -4.4599705 -4.3882594 -4.3275108 -4.3054142][-3.0849543 -3.5793536 -4.0148892 -4.3356233 -4.449564 -4.496798 -4.4942794 -4.4854355 -4.5423059 -4.5658441 -4.5439229 -4.4802837 -4.3986549 -4.3406205 -4.3066087][-3.5877302 -4.0706754 -4.3769054 -4.4933376 -4.4163361 -4.3675542 -4.3367848 -4.3289194 -4.3835125 -4.3372655 -4.2486587 -4.159574 -4.1332035 -4.1804461 -4.1951942][-3.795001 -4.291369 -4.4770641 -4.3567047 -4.0391231 -3.8595982 -3.7848136 -3.7680621 -3.8451219 -3.7543662 -3.626194 -3.5393076 -3.6156831 -3.8621674 -4.0248475][-3.6885109 -4.2016931 -4.3312178 -4.0374761 -3.5119729 -3.2073936 -3.0842052 -3.0474432 -3.1448526 -3.03228 -2.8963137 -2.8405857 -3.0469763 -3.5339327 -3.9128876][-3.4789071 -3.9474225 -4.0549655 -3.687737 -3.0687993 -2.7226815 -2.6112449 -2.5839787 -2.6827903 -2.5517936 -2.4033785 -2.373064 -2.6931524 -3.3792975 -3.9725122][-3.3012662 -3.6660433 -3.7559462 -3.4042003 -2.8215597 -2.5263395 -2.4824853 -2.4941363 -2.572968 -2.4272571 -2.2538145 -2.2279146 -2.6256928 -3.4216187 -4.1490345][-3.239892 -3.5014777 -3.5896482 -3.311769 -2.8340197 -2.6184044 -2.6466084 -2.7083709 -2.7737846 -2.6348767 -2.4473615 -2.4154043 -2.8135819 -3.5727751 -4.294498][-3.4186363 -3.6303391 -3.7343404 -3.5703533 -3.2579327 -3.1352935 -3.2066922 -3.3030596 -3.3506846 -3.2224121 -3.0368738 -2.9770985 -3.26544 -3.8269939 -4.3860378][-3.7499774 -3.9866533 -4.1262617 -4.0830975 -3.945272 -3.9176707 -4.019115 -4.1339989 -4.1648965 -4.04749 -3.8737311 -3.7717118 -3.8827741 -4.1731691 -4.5020566][-4.0033298 -4.2875171 -4.4583497 -4.4992094 -4.4875507 -4.5294375 -4.6427393 -4.7646751 -4.7964864 -4.6953416 -4.5334048 -4.3954258 -4.3684268 -4.4542456 -4.6024375][-4.0379333 -4.3380365 -4.5125546 -4.5804043 -4.613904 -4.6694784 -4.7605753 -4.8611474 -4.8986363 -4.8362041 -4.7161703 -4.5883918 -4.5072961 -4.493607 -4.5425515][-3.9334478 -4.2133541 -4.3623271 -4.4101958 -4.4287119 -4.4548087 -4.499979 -4.5610209 -4.5977063 -4.5781727 -4.5167608 -4.4361305 -4.3685632 -4.335556 -4.3457279][-3.8626628 -4.1148252 -4.23214 -4.2439313 -4.2231469 -4.2045894 -4.2010612 -4.2198553 -4.2441998 -4.2457471 -4.2256551 -4.1877971 -4.1498351 -4.1249208 -4.1217928]]...]
INFO - root - 2017-12-06 06:55:31.362054: step 3710, loss = 0.92, batch loss = 0.85 (12.8 examples/sec; 0.623 sec/batch; 56h:52m:40s remains)
INFO - root - 2017-12-06 06:55:37.435612: step 3720, loss = 0.86, batch loss = 0.79 (13.5 examples/sec; 0.592 sec/batch; 54h:06m:21s remains)
INFO - root - 2017-12-06 06:55:43.476277: step 3730, loss = 0.80, batch loss = 0.73 (12.8 examples/sec; 0.626 sec/batch; 57h:10m:13s remains)
INFO - root - 2017-12-06 06:55:49.609880: step 3740, loss = 0.96, batch loss = 0.89 (12.8 examples/sec; 0.623 sec/batch; 56h:56m:07s remains)
INFO - root - 2017-12-06 06:55:55.642367: step 3750, loss = 0.99, batch loss = 0.92 (13.9 examples/sec; 0.577 sec/batch; 52h:42m:23s remains)
INFO - root - 2017-12-06 06:56:01.617726: step 3760, loss = 0.83, batch loss = 0.76 (12.9 examples/sec; 0.620 sec/batch; 56h:36m:35s remains)
INFO - root - 2017-12-06 06:56:07.734362: step 3770, loss = 0.98, batch loss = 0.91 (13.5 examples/sec; 0.592 sec/batch; 54h:01m:55s remains)
INFO - root - 2017-12-06 06:56:13.839715: step 3780, loss = 0.82, batch loss = 0.75 (13.8 examples/sec; 0.581 sec/batch; 53h:01m:46s remains)
INFO - root - 2017-12-06 06:56:20.000613: step 3790, loss = 0.73, batch loss = 0.66 (13.3 examples/sec; 0.602 sec/batch; 54h:57m:27s remains)
INFO - root - 2017-12-06 06:56:26.009802: step 3800, loss = 0.82, batch loss = 0.75 (13.3 examples/sec; 0.604 sec/batch; 55h:07m:13s remains)
2017-12-06 06:56:26.693415: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2780008 -5.6199861 -5.803998 -5.7888784 -5.6192312 -5.4035854 -5.2272453 -5.0854855 -5.0629106 -5.1379514 -5.1480021 -5.0519071 -4.9199934 -4.8402596 -4.7080264][-5.7464943 -6.2645669 -6.5748596 -6.6067266 -6.3965774 -6.073349 -5.782197 -5.5922856 -5.6799235 -5.9892044 -6.1179814 -5.9459257 -5.6427317 -5.41673 -5.1352706][-5.8739991 -6.5043077 -6.8966475 -6.9707527 -6.7554669 -6.3492188 -5.9276686 -5.6848307 -5.9690485 -6.6821423 -7.0840988 -6.9313588 -6.4842091 -6.0954542 -5.6369586][-5.5933533 -6.1973643 -6.5653534 -6.6408248 -6.4166193 -5.9214435 -5.310174 -4.9394064 -5.4596214 -6.7244959 -7.5730071 -7.6190162 -7.1910024 -6.7330246 -6.1271033][-5.1978679 -5.6477265 -5.8683133 -5.83662 -5.4644995 -4.7547045 -3.8584132 -3.2522111 -3.9651234 -5.81082 -7.196003 -7.6210604 -7.4528832 -7.1337771 -6.5148277][-5.0614119 -5.2908592 -5.2674971 -4.9892111 -4.2812605 -3.1803975 -1.8791978 -0.92842054 -1.7051702 -4.0046935 -5.8811393 -6.7735348 -7.0652952 -7.0890408 -6.6236115][-5.2742677 -5.3621778 -5.128531 -4.5563464 -3.4252663 -1.8279469 -0.063718319 1.27807 0.60280085 -1.9102976 -4.1236596 -5.4452481 -6.2236481 -6.6368165 -6.4059162][-5.5926695 -5.7262821 -5.4560928 -4.7484751 -3.387382 -1.4899178 0.55777025 2.1672058 1.7395039 -0.654901 -2.9146106 -4.4601364 -5.511445 -6.1280689 -6.0395646][-5.7215452 -6.0311856 -5.9068923 -5.3280835 -4.1213565 -2.379945 -0.51443434 0.94897318 0.752491 -1.1645555 -3.0880399 -4.4989872 -5.4305973 -5.9128757 -5.7574739][-5.4662228 -5.95321 -6.0614176 -5.7759862 -4.9847074 -3.7237096 -2.3691623 -1.3475149 -1.5075984 -2.901135 -4.3287525 -5.3849297 -5.9391456 -6.0538225 -5.6757708][-4.9227967 -5.4890356 -5.80432 -5.8226585 -5.4785938 -4.7457042 -3.9429934 -3.4121695 -3.6451168 -4.6156192 -5.6073084 -6.3169556 -6.512454 -6.2783365 -5.6958942][-4.3569837 -4.868865 -5.2724648 -5.484025 -5.4720526 -5.1599522 -4.788826 -4.6177692 -4.8844271 -5.5000768 -6.11241 -6.5432177 -6.545404 -6.1672373 -5.5491266][-3.9633029 -4.3123531 -4.6660919 -4.917088 -5.0585413 -5.0111957 -4.9053559 -4.9108391 -5.1239929 -5.4529762 -5.771883 -5.9977608 -5.9460053 -5.6211715 -5.1345563][-3.8076138 -3.9636519 -4.1883521 -4.3537507 -4.4577346 -4.4798965 -4.4822869 -4.5346189 -4.6666403 -4.8236904 -4.9833975 -5.1051822 -5.0877967 -4.9110861 -4.6368628][-3.8782308 -3.8783009 -3.9543667 -4.0035439 -4.0178685 -4.0110464 -4.0135841 -4.0418792 -4.107058 -4.1920347 -4.297997 -4.3897071 -4.4263768 -4.3884311 -4.2998657]]...]
INFO - root - 2017-12-06 06:56:32.767713: step 3810, loss = 0.94, batch loss = 0.87 (13.0 examples/sec; 0.614 sec/batch; 56h:05m:31s remains)
INFO - root - 2017-12-06 06:56:38.609098: step 3820, loss = 0.88, batch loss = 0.81 (18.0 examples/sec; 0.445 sec/batch; 40h:36m:38s remains)
INFO - root - 2017-12-06 06:56:44.646241: step 3830, loss = 0.96, batch loss = 0.89 (13.0 examples/sec; 0.617 sec/batch; 56h:22m:11s remains)
INFO - root - 2017-12-06 06:56:50.775308: step 3840, loss = 0.86, batch loss = 0.79 (12.7 examples/sec; 0.630 sec/batch; 57h:31m:32s remains)
INFO - root - 2017-12-06 06:56:56.794273: step 3850, loss = 0.87, batch loss = 0.80 (13.1 examples/sec; 0.609 sec/batch; 55h:36m:40s remains)
INFO - root - 2017-12-06 06:57:02.902778: step 3860, loss = 0.90, batch loss = 0.83 (13.3 examples/sec; 0.603 sec/batch; 55h:04m:22s remains)
INFO - root - 2017-12-06 06:57:08.862134: step 3870, loss = 0.78, batch loss = 0.71 (12.7 examples/sec; 0.628 sec/batch; 57h:21m:03s remains)
INFO - root - 2017-12-06 06:57:14.960980: step 3880, loss = 0.77, batch loss = 0.70 (13.2 examples/sec; 0.605 sec/batch; 55h:12m:38s remains)
INFO - root - 2017-12-06 06:57:21.041151: step 3890, loss = 0.92, batch loss = 0.85 (13.0 examples/sec; 0.617 sec/batch; 56h:20m:34s remains)
INFO - root - 2017-12-06 06:57:27.214764: step 3900, loss = 0.81, batch loss = 0.74 (12.8 examples/sec; 0.625 sec/batch; 57h:03m:20s remains)
2017-12-06 06:57:27.828417: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4900293 -5.440958 -5.0337706 -4.454433 -4.0755172 -3.9951634 -4.4412537 -4.94532 -5.1959128 -5.2498465 -5.0210152 -4.7278347 -4.2654939 -3.7255335 -3.3842511][-5.422379 -5.1749835 -4.4708519 -3.6976643 -3.3323493 -3.4214969 -4.0418372 -4.6471038 -5.0581851 -5.2697687 -5.1787906 -4.9803376 -4.5497012 -3.9709063 -3.4330981][-5.2512755 -4.8011131 -3.8136241 -2.843003 -2.4785507 -2.7280192 -3.5611463 -4.3900375 -5.0617828 -5.4649267 -5.5041785 -5.3580036 -4.9322629 -4.3019328 -3.5810175][-5.0516124 -4.4095497 -3.1884274 -2.0479007 -1.6576052 -1.972785 -2.9346809 -3.9928582 -4.8897653 -5.4290504 -5.522768 -5.3889518 -4.994782 -4.3811111 -3.6094069][-4.9556284 -4.1694512 -2.7583189 -1.4269001 -0.90698886 -1.1305618 -2.0747015 -3.2767124 -4.3082433 -4.9623361 -5.13212 -5.0486469 -4.7655478 -4.2883368 -3.5957344][-4.9991369 -4.1730971 -2.6969442 -1.2308211 -0.521255 -0.5072391 -1.2733977 -2.4798913 -3.5348341 -4.2608824 -4.497499 -4.4548359 -4.3053994 -4.0589876 -3.60379][-5.0850706 -4.3190241 -2.9250827 -1.4802229 -0.64668369 -0.36871433 -0.86825204 -1.9484303 -2.927984 -3.646415 -3.9015677 -3.8533535 -3.7825038 -3.7783499 -3.6198568][-5.1623635 -4.4826746 -3.2285435 -1.9074183 -1.0722423 -0.6518631 -0.96281624 -1.8710105 -2.7076216 -3.3514123 -3.5567915 -3.4290786 -3.3010333 -3.3939209 -3.4600425][-5.2240434 -4.636827 -3.5155816 -2.3381429 -1.5751984 -1.1548071 -1.4042494 -2.1542494 -2.8282089 -3.3789978 -3.5776207 -3.4297583 -3.1940982 -3.1821189 -3.2375665][-5.2613897 -4.7724285 -3.7855692 -2.7507319 -2.0771735 -1.7277322 -2.0022752 -2.6299772 -3.1601548 -3.618329 -3.8363068 -3.7416553 -3.4655013 -3.3293462 -3.2626529][-5.2707977 -4.90815 -4.0880303 -3.2150397 -2.63237 -2.3459349 -2.6441033 -3.156179 -3.5594807 -3.9377637 -4.1663327 -4.1119609 -3.8367629 -3.6433294 -3.4729521][-5.2354283 -5.0393691 -4.4437943 -3.7722936 -3.2905383 -3.0262609 -3.2651892 -3.6058316 -3.8641152 -4.1864939 -4.4528174 -4.4491582 -4.1995058 -3.9878018 -3.7511439][-5.0886712 -5.0505438 -4.6844568 -4.21194 -3.83053 -3.5806744 -3.7369967 -3.9374561 -4.0904245 -4.3765631 -4.6707935 -4.7049961 -4.5045896 -4.3104897 -4.0637231][-4.8878937 -4.9389162 -4.7331152 -4.4065857 -4.1068163 -3.8907409 -3.9873424 -4.1184754 -4.2402787 -4.5102377 -4.7957268 -4.8330112 -4.6908522 -4.55014 -4.3480349][-4.7330742 -4.7928305 -4.6529231 -4.4081984 -4.1760592 -4.0206571 -4.097198 -4.2143068 -4.3433914 -4.5803065 -4.79356 -4.7783728 -4.658361 -4.5590062 -4.4115114]]...]
INFO - root - 2017-12-06 06:57:33.941102: step 3910, loss = 1.31, batch loss = 1.24 (13.0 examples/sec; 0.618 sec/batch; 56h:22m:15s remains)
INFO - root - 2017-12-06 06:57:40.085541: step 3920, loss = 0.94, batch loss = 0.87 (12.7 examples/sec; 0.630 sec/batch; 57h:32m:29s remains)
INFO - root - 2017-12-06 06:57:46.129727: step 3930, loss = 1.17, batch loss = 1.10 (13.2 examples/sec; 0.605 sec/batch; 55h:12m:48s remains)
INFO - root - 2017-12-06 06:57:52.253660: step 3940, loss = 0.86, batch loss = 0.79 (13.2 examples/sec; 0.607 sec/batch; 55h:21m:56s remains)
INFO - root - 2017-12-06 06:57:58.373158: step 3950, loss = 1.21, batch loss = 1.14 (12.8 examples/sec; 0.623 sec/batch; 56h:49m:34s remains)
INFO - root - 2017-12-06 06:58:04.529451: step 3960, loss = 0.77, batch loss = 0.70 (13.0 examples/sec; 0.614 sec/batch; 56h:04m:11s remains)
INFO - root - 2017-12-06 06:58:10.341910: step 3970, loss = 0.92, batch loss = 0.85 (19.8 examples/sec; 0.404 sec/batch; 36h:50m:55s remains)
INFO - root - 2017-12-06 06:58:16.473940: step 3980, loss = 1.10, batch loss = 1.03 (12.7 examples/sec; 0.629 sec/batch; 57h:25m:21s remains)
INFO - root - 2017-12-06 06:58:22.577830: step 3990, loss = 0.79, batch loss = 0.72 (12.7 examples/sec; 0.631 sec/batch; 57h:35m:34s remains)
INFO - root - 2017-12-06 06:58:28.565178: step 4000, loss = 0.98, batch loss = 0.91 (13.1 examples/sec; 0.612 sec/batch; 55h:49m:41s remains)
2017-12-06 06:58:29.101354: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5135221 -4.7380462 -5.0647459 -5.2606282 -5.2210412 -5.0402875 -4.8895221 -4.844871 -4.8366027 -4.8447227 -4.9217834 -4.9750314 -4.9687505 -4.9708915 -5.0139132][-4.2718487 -4.6075082 -5.0184054 -5.1539483 -4.9711218 -4.683002 -4.5254216 -4.5169005 -4.5416784 -4.59036 -4.7612066 -4.8715625 -4.8715396 -4.9049096 -5.0107059][-3.9524293 -4.4141879 -4.8890142 -4.8912053 -4.4747148 -4.0069752 -3.8111606 -3.8700812 -3.9889026 -4.1329756 -4.4307523 -4.6308184 -4.6367736 -4.6634889 -4.772192][-3.8130426 -4.3821769 -4.8734322 -4.6864657 -3.946454 -3.1278663 -2.7428212 -2.9066846 -3.2851458 -3.6569808 -4.1073437 -4.398901 -4.4003325 -4.3483019 -4.3656311][-3.7643132 -4.3990517 -4.8784137 -4.517211 -3.4568143 -2.1384466 -1.414741 -1.7502775 -2.5974264 -3.3771677 -4.0315008 -4.4171791 -4.4024973 -4.187726 -4.0050268][-3.5274479 -4.1353254 -4.6055651 -4.2100358 -2.9753757 -1.1583447 -0.005384922 -0.52081776 -1.9223464 -3.2035284 -4.1365819 -4.6699328 -4.6664472 -4.2551718 -3.7973838][-3.0558581 -3.51197 -3.9775825 -3.7560394 -2.6313725 -0.58148813 0.91504574 0.32721472 -1.4164672 -3.0597806 -4.2447186 -4.9553661 -5.0204263 -4.4492445 -3.7258208][-2.6413164 -2.9300003 -3.4463253 -3.5774727 -2.8506632 -0.98663139 0.56395006 0.19431257 -1.4389198 -3.1599026 -4.4761968 -5.2969017 -5.4039245 -4.6986237 -3.7497456][-2.6378932 -2.81468 -3.3796933 -3.8396571 -3.556267 -2.1002004 -0.72366285 -0.75142145 -1.9426124 -3.480258 -4.7657137 -5.566164 -5.6637049 -4.8845177 -3.7864711][-2.9857812 -3.0802372 -3.5988674 -4.1749153 -4.1652279 -3.0923347 -1.9791167 -1.7628784 -2.4860711 -3.7223825 -4.8488789 -5.5379729 -5.6231956 -4.87731 -3.7758121][-3.2512481 -3.2785215 -3.6856937 -4.2129259 -4.3359747 -3.6103773 -2.8037734 -2.5471985 -2.9723444 -3.9122393 -4.7914448 -5.3387766 -5.4459505 -4.8338928 -3.8683956][-3.3421392 -3.3383284 -3.6490564 -4.100862 -4.31313 -3.8845809 -3.349323 -3.1532407 -3.4407361 -4.1069579 -4.7072897 -5.1375136 -5.2790565 -4.8167758 -4.0064116][-3.4806378 -3.4453073 -3.6671753 -4.0650434 -4.3685541 -4.193356 -3.8216226 -3.6402829 -3.8164043 -4.2388163 -4.60997 -4.9668427 -5.1331758 -4.7959776 -4.1380491][-3.7721775 -3.6580706 -3.7731605 -4.1278038 -4.5249577 -4.5874085 -4.3262277 -4.0886817 -4.1366725 -4.35408 -4.5422764 -4.8357162 -5.00082 -4.7656026 -4.2753148][-4.1282258 -3.9707136 -4.0070248 -4.3076096 -4.7393203 -4.9790692 -4.8123617 -4.5094557 -4.4220619 -4.4494653 -4.4432106 -4.6217828 -4.7661371 -4.6451449 -4.3557329]]...]
INFO - root - 2017-12-06 06:58:35.152850: step 4010, loss = 0.91, batch loss = 0.84 (13.2 examples/sec; 0.605 sec/batch; 55h:10m:49s remains)
INFO - root - 2017-12-06 06:58:41.270399: step 4020, loss = 0.77, batch loss = 0.70 (12.8 examples/sec; 0.624 sec/batch; 56h:55m:13s remains)
INFO - root - 2017-12-06 06:58:47.305479: step 4030, loss = 0.86, batch loss = 0.79 (13.2 examples/sec; 0.605 sec/batch; 55h:13m:07s remains)
INFO - root - 2017-12-06 06:58:53.332383: step 4040, loss = 0.77, batch loss = 0.70 (13.6 examples/sec; 0.589 sec/batch; 53h:42m:27s remains)
INFO - root - 2017-12-06 06:58:59.439192: step 4050, loss = 0.90, batch loss = 0.83 (13.1 examples/sec; 0.612 sec/batch; 55h:48m:46s remains)
INFO - root - 2017-12-06 06:59:05.466593: step 4060, loss = 0.87, batch loss = 0.80 (13.1 examples/sec; 0.613 sec/batch; 55h:54m:53s remains)
INFO - root - 2017-12-06 06:59:11.481626: step 4070, loss = 0.93, batch loss = 0.86 (13.2 examples/sec; 0.605 sec/batch; 55h:13m:14s remains)
INFO - root - 2017-12-06 06:59:17.542125: step 4080, loss = 0.95, batch loss = 0.88 (12.8 examples/sec; 0.626 sec/batch; 57h:07m:42s remains)
INFO - root - 2017-12-06 06:59:23.614223: step 4090, loss = 0.81, batch loss = 0.74 (13.3 examples/sec; 0.603 sec/batch; 55h:02m:12s remains)
INFO - root - 2017-12-06 06:59:29.688882: step 4100, loss = 0.91, batch loss = 0.84 (13.0 examples/sec; 0.613 sec/batch; 55h:57m:14s remains)
2017-12-06 06:59:30.233945: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0539289 -4.081728 -4.1346221 -4.1830263 -4.274271 -4.3557816 -4.3501639 -4.3037348 -4.3726587 -4.4766397 -4.3990569 -4.1939616 -4.0525208 -4.0392957 -4.1421137][-4.3256345 -4.3062944 -4.3157167 -4.376205 -4.5334506 -4.6591687 -4.670763 -4.6351976 -4.7463307 -4.8842125 -4.7665243 -4.5051122 -4.360795 -4.424499 -4.599196][-4.5603929 -4.4409957 -4.4014726 -4.5251045 -4.778327 -4.9311323 -4.9213328 -4.8682461 -4.9612546 -5.0603986 -4.8758621 -4.6306963 -4.6025066 -4.8455763 -5.12052][-4.5330396 -4.3359251 -4.3288531 -4.6254497 -5.014708 -5.1171904 -4.9208488 -4.7035384 -4.6751261 -4.7208753 -4.5614214 -4.500865 -4.7440176 -5.2274332 -5.5612345][-4.3401475 -4.1508994 -4.2753839 -4.7977862 -5.3082862 -5.2363148 -4.6538939 -4.0988526 -3.9077837 -4.02562 -4.1407642 -4.4823523 -5.0631151 -5.6767282 -5.8771791][-4.3471794 -4.260788 -4.5140882 -5.12497 -5.5540819 -5.1091037 -3.96029 -2.9658403 -2.7165306 -3.2118664 -3.9301281 -4.8216729 -5.6783962 -6.1973548 -6.0438056][-4.5584946 -4.5815144 -4.8696909 -5.3428526 -5.4408913 -4.464448 -2.70403 -1.3078167 -1.1983166 -2.3360994 -3.8251209 -5.2002931 -6.1119318 -6.3098173 -5.7348862][-4.8783388 -4.9036374 -5.1236916 -5.3710303 -5.0943556 -3.67583 -1.5111072 0.079657078 -0.087703228 -1.8276982 -3.8564274 -5.3720417 -6.0642653 -5.8751588 -5.031693][-5.1358147 -5.1031785 -5.2598681 -5.3575373 -4.8543663 -3.3194332 -1.2301559 0.16758585 -0.23528528 -2.1230593 -4.1103787 -5.3167953 -5.6231732 -5.1688824 -4.3394551][-5.2237463 -5.1766086 -5.3171082 -5.3637462 -4.8208332 -3.4831667 -1.8574767 -0.873862 -1.3277152 -2.8722854 -4.2840495 -4.8944311 -4.8432665 -4.3716273 -3.8335395][-5.0876417 -5.0604739 -5.18972 -5.227735 -4.8042035 -3.8907609 -2.8965764 -2.3008547 -2.5728135 -3.4735382 -4.0979805 -4.1249113 -3.897362 -3.6560121 -3.5523129][-4.7643442 -4.7470732 -4.81534 -4.8404479 -4.6200151 -4.2292457 -3.8774331 -3.5921438 -3.5872064 -3.8099906 -3.7506351 -3.4113498 -3.1980181 -3.290916 -3.5533681][-4.346458 -4.3240776 -4.3029919 -4.2956223 -4.269691 -4.3649497 -4.5258164 -4.4424748 -4.2081542 -3.959017 -3.504441 -3.0724654 -3.0003819 -3.375381 -3.8531091][-4.1000075 -4.0542488 -3.9633389 -3.96561 -4.1060061 -4.503427 -4.8993607 -4.8524437 -4.4693279 -3.998687 -3.4337122 -3.0884008 -3.1709042 -3.6860051 -4.2352529][-4.1266751 -4.023921 -3.8566232 -3.8583179 -4.0987959 -4.5985937 -5.0112877 -4.9113288 -4.4632025 -3.9653311 -3.4883704 -3.2886643 -3.4619234 -3.985589 -4.4921379]]...]
INFO - root - 2017-12-06 06:59:36.352705: step 4110, loss = 0.98, batch loss = 0.91 (13.3 examples/sec; 0.603 sec/batch; 55h:01m:35s remains)
INFO - root - 2017-12-06 06:59:42.330850: step 4120, loss = 0.91, batch loss = 0.84 (14.6 examples/sec; 0.547 sec/batch; 49h:56m:13s remains)
INFO - root - 2017-12-06 06:59:48.437879: step 4130, loss = 1.00, batch loss = 0.93 (12.8 examples/sec; 0.624 sec/batch; 56h:53m:21s remains)
INFO - root - 2017-12-06 06:59:54.473592: step 4140, loss = 1.02, batch loss = 0.95 (13.2 examples/sec; 0.604 sec/batch; 55h:05m:49s remains)
INFO - root - 2017-12-06 07:00:00.551487: step 4150, loss = 0.77, batch loss = 0.70 (12.6 examples/sec; 0.635 sec/batch; 57h:55m:12s remains)
INFO - root - 2017-12-06 07:00:06.577347: step 4160, loss = 0.99, batch loss = 0.92 (13.0 examples/sec; 0.615 sec/batch; 56h:07m:58s remains)
INFO - root - 2017-12-06 07:00:12.636988: step 4170, loss = 1.21, batch loss = 1.14 (12.9 examples/sec; 0.618 sec/batch; 56h:21m:28s remains)
INFO - root - 2017-12-06 07:00:18.661703: step 4180, loss = 1.11, batch loss = 1.04 (13.4 examples/sec; 0.598 sec/batch; 54h:33m:10s remains)
INFO - root - 2017-12-06 07:00:24.654698: step 4190, loss = 0.97, batch loss = 0.90 (14.0 examples/sec; 0.570 sec/batch; 51h:58m:10s remains)
INFO - root - 2017-12-06 07:00:30.725226: step 4200, loss = 0.96, batch loss = 0.89 (13.1 examples/sec; 0.610 sec/batch; 55h:35m:58s remains)
2017-12-06 07:00:31.289953: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5282271 -3.5762734 -3.7377644 -4.0222864 -4.2311387 -4.2061663 -4.090857 -3.812325 -3.5961237 -3.3515415 -3.0689406 -3.0336311 -3.1908114 -3.4461784 -3.7760277][-3.3644562 -3.4611597 -3.674175 -4.0186319 -4.2708106 -4.2046337 -4.0005546 -3.7062256 -3.4942989 -3.2033651 -2.8486605 -2.7230895 -2.8697941 -3.1526723 -3.4728632][-3.4955053 -3.5809109 -3.7714498 -4.0838051 -4.2811718 -4.1027141 -3.7630024 -3.4450264 -3.3322625 -3.1461356 -2.801837 -2.5926719 -2.6809363 -2.9160945 -3.1930485][-3.8396857 -3.8623216 -3.9457078 -4.1394758 -4.2011065 -3.872561 -3.3872776 -3.0327108 -3.0370741 -3.0657926 -2.8492889 -2.6398413 -2.6835265 -2.8547547 -3.0999331][-4.2007341 -4.1659575 -4.1047711 -4.1281743 -4.0236535 -3.5548282 -2.9496398 -2.541219 -2.6001244 -2.8404369 -2.8505802 -2.7643781 -2.8560872 -3.0338364 -3.2911019][-4.4597545 -4.4301281 -4.2676959 -4.12993 -3.867162 -3.2822404 -2.5778608 -2.1049201 -2.1466091 -2.5032792 -2.7447524 -2.8556936 -3.0541062 -3.2954056 -3.5935357][-4.5839486 -4.6138568 -4.4350648 -4.2112112 -3.8630452 -3.2399788 -2.4987202 -1.9752376 -1.9461622 -2.2637427 -2.6040463 -2.869123 -3.1576619 -3.4517245 -3.80225][-4.6387286 -4.7200813 -4.5815387 -4.3516464 -4.0077734 -3.4481769 -2.7796001 -2.274 -2.159663 -2.3432252 -2.6466002 -2.9630208 -3.2778883 -3.5843985 -3.9524546][-4.7185082 -4.8146253 -4.7210279 -4.5149121 -4.2037749 -3.738517 -3.1841221 -2.7467971 -2.6029396 -2.701201 -2.965611 -3.2699623 -3.5669203 -3.8510103 -4.1715422][-4.8621674 -4.9357424 -4.8749371 -4.7030072 -4.4135771 -4.0024948 -3.5171638 -3.1459527 -3.0449567 -3.1596768 -3.43686 -3.7261653 -4.0090666 -4.2770033 -4.5208273][-5.0134511 -5.0745783 -5.0606766 -4.9410868 -4.6792903 -4.2875776 -3.8194098 -3.4652777 -3.4078579 -3.569144 -3.88585 -4.1911926 -4.5042167 -4.7880464 -4.9734263][-5.0351877 -5.11082 -5.1736579 -5.1475363 -4.9629951 -4.6403732 -4.2182078 -3.8804874 -3.8149326 -3.9396784 -4.2170959 -4.4969096 -4.8183088 -5.1215215 -5.2891269][-4.8480163 -4.9325967 -5.0626473 -5.1413078 -5.0742049 -4.8825817 -4.5723319 -4.2938852 -4.1856146 -4.1964021 -4.3464136 -4.5292335 -4.7988939 -5.0855255 -5.2571321][-4.49795 -4.5726137 -4.7287869 -4.8683009 -4.897017 -4.83155 -4.6585932 -4.478766 -4.3522792 -4.2593703 -4.2637477 -4.3233542 -4.4936461 -4.715641 -4.8768959][-4.1874466 -4.2468333 -4.3849874 -4.5297484 -4.5977836 -4.6042657 -4.5356994 -4.4329543 -4.3092432 -4.1720433 -4.0997682 -4.0886645 -4.1707487 -4.310328 -4.433651]]...]
INFO - root - 2017-12-06 07:00:37.258843: step 4210, loss = 0.83, batch loss = 0.76 (13.8 examples/sec; 0.579 sec/batch; 52h:50m:09s remains)
INFO - root - 2017-12-06 07:00:43.315608: step 4220, loss = 1.14, batch loss = 1.07 (13.0 examples/sec; 0.616 sec/batch; 56h:08m:09s remains)
INFO - root - 2017-12-06 07:00:49.334953: step 4230, loss = 0.86, batch loss = 0.79 (13.4 examples/sec; 0.599 sec/batch; 54h:37m:10s remains)
INFO - root - 2017-12-06 07:00:55.411795: step 4240, loss = 0.87, batch loss = 0.80 (13.3 examples/sec; 0.601 sec/batch; 54h:49m:37s remains)
INFO - root - 2017-12-06 07:01:01.485539: step 4250, loss = 0.84, batch loss = 0.77 (12.7 examples/sec; 0.632 sec/batch; 57h:38m:21s remains)
INFO - root - 2017-12-06 07:01:07.650387: step 4260, loss = 1.20, batch loss = 1.13 (12.7 examples/sec; 0.631 sec/batch; 57h:34m:42s remains)
INFO - root - 2017-12-06 07:01:13.677194: step 4270, loss = 0.91, batch loss = 0.84 (13.8 examples/sec; 0.579 sec/batch; 52h:48m:48s remains)
INFO - root - 2017-12-06 07:01:19.719177: step 4280, loss = 0.80, batch loss = 0.73 (12.3 examples/sec; 0.650 sec/batch; 59h:14m:12s remains)
INFO - root - 2017-12-06 07:01:25.772814: step 4290, loss = 0.88, batch loss = 0.81 (15.0 examples/sec; 0.535 sec/batch; 48h:46m:21s remains)
INFO - root - 2017-12-06 07:01:31.764854: step 4300, loss = 0.79, batch loss = 0.72 (13.2 examples/sec; 0.605 sec/batch; 55h:08m:42s remains)
2017-12-06 07:01:32.410642: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8781052 -4.9648113 -5.0222044 -5.0675125 -5.0723205 -5.0092411 -4.9081211 -4.829896 -4.8016286 -4.7736349 -4.7254848 -4.67267 -4.6172671 -4.5342765 -4.4090667][-5.1604505 -5.2529421 -5.3142104 -5.3748255 -5.3715596 -5.2415009 -5.0420747 -4.9137759 -4.9291506 -5.0131803 -5.0678864 -5.0555987 -4.9890962 -4.8690424 -4.689589][-5.245132 -5.325304 -5.376091 -5.4088054 -5.3224306 -5.0341721 -4.6555357 -4.4373031 -4.5181808 -4.8302832 -5.1632242 -5.3230209 -5.3020334 -5.1681376 -4.9570189][-5.1028342 -5.1548076 -5.1827888 -5.1463804 -4.8904943 -4.3420811 -3.7068079 -3.3452847 -3.4612832 -4.0459828 -4.778101 -5.2375126 -5.3549848 -5.2772574 -5.094696][-4.8315773 -4.8107972 -4.763236 -4.6063709 -4.1443653 -3.313663 -2.4168007 -1.9000404 -2.0180383 -2.8404279 -3.9692307 -4.7711015 -5.1033587 -5.1622796 -5.0769815][-4.6318069 -4.5482688 -4.3726444 -4.03417 -3.368463 -2.3259838 -1.2382371 -0.56441236 -0.6275928 -1.5925016 -3.0126057 -4.1148753 -4.6869216 -4.9279056 -4.9655972][-4.6135583 -4.560132 -4.3151455 -3.8359818 -3.0184283 -1.8435216 -0.65033364 0.14205217 0.16691017 -0.80535865 -2.3316765 -3.6226132 -4.40039 -4.7942467 -4.906786][-4.7091508 -4.7568893 -4.5720987 -4.1128025 -3.2915394 -2.1106246 -0.93419933 -0.14040804 -0.075222492 -0.95785069 -2.3858838 -3.6642356 -4.4987116 -4.9373541 -5.0330257][-4.8226366 -4.9693842 -4.9153347 -4.6062694 -3.9438827 -2.9229724 -1.9122732 -1.2521493 -1.2271712 -1.9818044 -3.169806 -4.256958 -4.979321 -5.3287015 -5.3117208][-4.9097881 -5.119647 -5.172771 -5.0331645 -4.6083064 -3.8690097 -3.113297 -2.6199775 -2.6262736 -3.2262053 -4.1361632 -4.96997 -5.4986906 -5.6860466 -5.5235391][-4.898047 -5.1518922 -5.2768946 -5.2623181 -5.0652022 -4.637373 -4.1417646 -3.764874 -3.7274377 -4.1301022 -4.7642794 -5.3431282 -5.667943 -5.6986508 -5.4344883][-4.7284474 -4.9951177 -5.1792178 -5.2685962 -5.2502093 -5.0713329 -4.8020244 -4.5441322 -4.4567447 -4.6270304 -4.9521732 -5.260201 -5.3959937 -5.3180518 -5.0327883][-4.4596963 -4.6602392 -4.8393044 -4.9830103 -5.0702286 -5.0451632 -4.9305043 -4.7954183 -4.7276034 -4.7625461 -4.8572111 -4.9366536 -4.9163766 -4.7732363 -4.5205994][-4.2410345 -4.3326693 -4.4323111 -4.5475454 -4.6536322 -4.6880965 -4.6498737 -4.5915985 -4.5567341 -4.5531178 -4.5599246 -4.5435071 -4.4592004 -4.3063464 -4.1152253][-4.1296592 -4.1344891 -4.1447849 -4.1941285 -4.2727804 -4.3259969 -4.333075 -4.3191805 -4.3094196 -4.3039441 -4.2887082 -4.2456145 -4.1552868 -4.0337968 -3.9227271]]...]
INFO - root - 2017-12-06 07:01:38.573757: step 4310, loss = 1.09, batch loss = 1.02 (13.2 examples/sec; 0.608 sec/batch; 55h:23m:36s remains)
INFO - root - 2017-12-06 07:01:44.676060: step 4320, loss = 1.02, batch loss = 0.95 (13.1 examples/sec; 0.611 sec/batch; 55h:43m:17s remains)
INFO - root - 2017-12-06 07:01:50.769892: step 4330, loss = 0.94, batch loss = 0.87 (13.4 examples/sec; 0.596 sec/batch; 54h:17m:42s remains)
INFO - root - 2017-12-06 07:01:56.832389: step 4340, loss = 1.03, batch loss = 0.96 (13.3 examples/sec; 0.602 sec/batch; 54h:54m:26s remains)
INFO - root - 2017-12-06 07:02:02.940406: step 4350, loss = 0.93, batch loss = 0.86 (13.5 examples/sec; 0.591 sec/batch; 53h:53m:07s remains)
INFO - root - 2017-12-06 07:02:09.084818: step 4360, loss = 1.11, batch loss = 1.04 (12.6 examples/sec; 0.634 sec/batch; 57h:45m:34s remains)
INFO - root - 2017-12-06 07:02:15.120858: step 4370, loss = 0.69, batch loss = 0.62 (13.6 examples/sec; 0.589 sec/batch; 53h:38m:37s remains)
INFO - root - 2017-12-06 07:02:21.180835: step 4380, loss = 0.93, batch loss = 0.86 (12.8 examples/sec; 0.625 sec/batch; 56h:58m:56s remains)
INFO - root - 2017-12-06 07:02:27.262277: step 4390, loss = 0.92, batch loss = 0.85 (12.2 examples/sec; 0.658 sec/batch; 59h:57m:32s remains)
INFO - root - 2017-12-06 07:02:33.171859: step 4400, loss = 1.02, batch loss = 0.95 (13.2 examples/sec; 0.607 sec/batch; 55h:21m:00s remains)
2017-12-06 07:02:33.753037: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8748631 -3.5028539 -3.1032119 -3.1426153 -3.4486256 -3.6946697 -3.7131932 -3.6837244 -3.6503913 -3.6139126 -3.6741872 -3.833715 -3.9868698 -3.9111507 -3.7594688][-3.7839785 -3.2861304 -2.7638397 -2.7181861 -3.0563407 -3.3750081 -3.3910112 -3.3330052 -3.3304482 -3.323947 -3.3840432 -3.5646698 -3.7748189 -3.7373652 -3.5954058][-3.7651937 -3.2098043 -2.6647358 -2.5759876 -2.9176683 -3.2261128 -3.1603518 -2.9994373 -2.9746792 -2.9513419 -2.9514084 -3.0976262 -3.3365707 -3.3833842 -3.359395][-3.7547698 -3.2313797 -2.764277 -2.6926179 -3.0043182 -3.210319 -3.0024703 -2.6755838 -2.5789776 -2.5591269 -2.5538013 -2.7092271 -3.00032 -3.1803272 -3.33107][-3.8048332 -3.3799872 -3.0239387 -2.9629216 -3.1977048 -3.2696056 -2.9268236 -2.3963361 -2.1432366 -2.1352284 -2.2054505 -2.4339943 -2.7896795 -3.0900469 -3.3783178][-3.99162 -3.6681306 -3.3726172 -3.2559867 -3.3811417 -3.3475537 -2.9276843 -2.2223732 -1.7624221 -1.7536023 -1.9630744 -2.3230581 -2.7379029 -3.1084127 -3.4525251][-4.0515046 -3.8063862 -3.5208995 -3.3362582 -3.3883343 -3.3353915 -2.9138951 -2.1160569 -1.4587674 -1.3914795 -1.7390831 -2.2811961 -2.7979894 -3.2272773 -3.5859923][-3.9281039 -3.7356393 -3.45225 -3.2362928 -3.2875566 -3.3145423 -2.9996681 -2.254472 -1.4886737 -1.3008101 -1.6904175 -2.3404388 -2.9256074 -3.3658843 -3.7275786][-4.0800738 -3.9371715 -3.6481249 -3.4126983 -3.4671211 -3.6213648 -3.5025182 -2.9836705 -2.3182304 -2.0702939 -2.3943696 -2.9861159 -3.5116425 -3.858326 -4.1495996][-4.6392655 -4.5270991 -4.1928639 -3.8993788 -3.8983622 -4.15007 -4.2345715 -3.9854062 -3.5339103 -3.3015094 -3.4911466 -3.9135206 -4.3213091 -4.5674791 -4.7801275][-5.2319293 -5.07021 -4.6143017 -4.2120891 -4.1062117 -4.3987355 -4.6529589 -4.6325436 -4.3848104 -4.1990461 -4.2469578 -4.4857097 -4.8037181 -5.0350814 -5.2383213][-5.4321861 -5.14927 -4.5246944 -3.983634 -3.7687764 -4.0687056 -4.4516349 -4.5944996 -4.5006347 -4.3861513 -4.3578672 -4.4651289 -4.7342234 -5.0191765 -5.2937093][-5.2079883 -4.7881393 -4.0477548 -3.4369311 -3.1845067 -3.4727223 -3.9281673 -4.169 -4.1672711 -4.1129417 -4.0624204 -4.0842342 -4.3064165 -4.6226268 -4.9681163][-5.1273179 -4.655189 -3.9143996 -3.3355122 -3.1096272 -3.3520613 -3.7689142 -4.003068 -4.0164061 -3.9930458 -3.9464769 -3.9264402 -4.098536 -4.3979568 -4.7667446][-5.191453 -4.7888589 -4.1697388 -3.7019563 -3.5318742 -3.6982584 -3.9883757 -4.1364641 -4.1182036 -4.0913124 -4.033349 -3.9763579 -4.0864277 -4.3326774 -4.689044]]...]
INFO - root - 2017-12-06 07:02:39.794077: step 4410, loss = 0.81, batch loss = 0.74 (13.0 examples/sec; 0.617 sec/batch; 56h:12m:32s remains)
INFO - root - 2017-12-06 07:02:45.895121: step 4420, loss = 0.78, batch loss = 0.71 (13.2 examples/sec; 0.604 sec/batch; 55h:02m:33s remains)
INFO - root - 2017-12-06 07:02:51.860155: step 4430, loss = 0.91, batch loss = 0.84 (12.4 examples/sec; 0.644 sec/batch; 58h:40m:00s remains)
INFO - root - 2017-12-06 07:02:57.849564: step 4440, loss = 0.76, batch loss = 0.69 (13.1 examples/sec; 0.609 sec/batch; 55h:30m:33s remains)
INFO - root - 2017-12-06 07:03:03.980649: step 4450, loss = 1.00, batch loss = 0.93 (12.7 examples/sec; 0.629 sec/batch; 57h:20m:47s remains)
INFO - root - 2017-12-06 07:03:10.128146: step 4460, loss = 0.95, batch loss = 0.88 (13.1 examples/sec; 0.612 sec/batch; 55h:44m:21s remains)
INFO - root - 2017-12-06 07:03:16.165077: step 4470, loss = 0.82, batch loss = 0.75 (13.5 examples/sec; 0.591 sec/batch; 53h:48m:42s remains)
INFO - root - 2017-12-06 07:03:22.227872: step 4480, loss = 0.85, batch loss = 0.78 (13.7 examples/sec; 0.582 sec/batch; 53h:03m:21s remains)
INFO - root - 2017-12-06 07:03:28.310862: step 4490, loss = 0.87, batch loss = 0.80 (13.1 examples/sec; 0.612 sec/batch; 55h:43m:35s remains)
INFO - root - 2017-12-06 07:03:34.491431: step 4500, loss = 0.89, batch loss = 0.82 (13.1 examples/sec; 0.612 sec/batch; 55h:44m:14s remains)
2017-12-06 07:03:35.119791: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3458424 -4.4452896 -4.9686542 -5.5885024 -5.8463392 -5.3670654 -4.2387724 -3.3335428 -3.2769709 -3.6506793 -4.1728415 -4.5941639 -4.8033028 -4.9277072 -4.9047484][-4.0363288 -3.8991714 -4.2577896 -4.9205513 -5.4474292 -5.3688231 -4.5736518 -3.7250779 -3.64614 -4.0117445 -4.5038648 -4.8632822 -4.8621197 -4.7209926 -4.5474882][-3.866293 -3.3538175 -3.219892 -3.5112379 -4.023571 -4.3856091 -4.2249155 -3.74535 -3.8424139 -4.3142262 -4.8413725 -5.1428003 -4.8620009 -4.3302503 -3.892982][-3.9937382 -3.1121521 -2.35884 -1.9720695 -2.0888047 -2.6183965 -3.0228181 -2.9763255 -3.3862283 -4.1524358 -4.8823347 -5.23137 -4.8011422 -4.0135593 -3.4383385][-4.1409893 -3.0544014 -1.8541172 -0.84288168 -0.42301798 -0.8061192 -1.3976843 -1.4858522 -2.1028371 -3.289115 -4.4385834 -5.0486927 -4.7510309 -3.993391 -3.4408765][-4.1652937 -3.1236739 -1.764014 -0.35234928 0.56625271 0.5089612 0.065882683 0.16240311 -0.47852278 -2.0598519 -3.6862154 -4.6676445 -4.6955462 -4.1587176 -3.7008352][-4.188355 -3.5176558 -2.3261247 -0.74447489 0.59067154 0.9543581 0.89613676 1.3614068 0.853704 -0.95030403 -2.8762522 -4.1085577 -4.4517083 -4.2156878 -3.9402323][-4.2217431 -4.087359 -3.3041015 -1.8590822 -0.38171053 0.31150484 0.69756651 1.5222311 1.2554092 -0.45450187 -2.3261609 -3.5327749 -4.0172133 -4.0700822 -4.0786223][-4.0899825 -4.4697189 -4.2494698 -3.2766087 -2.0666571 -1.3225369 -0.62585306 0.43572664 0.49104452 -0.83404994 -2.3679924 -3.3330266 -3.7975111 -4.0844822 -4.3823342][-3.8905139 -4.6444364 -4.98612 -4.6284156 -3.8974979 -3.2861834 -2.4368534 -1.2262635 -0.80409455 -1.6097527 -2.7483497 -3.4826198 -3.8969109 -4.3285308 -4.791502][-3.8150861 -4.733099 -5.4498515 -5.5727887 -5.2404838 -4.7719116 -3.9025545 -2.6758065 -1.982866 -2.3113964 -3.0840249 -3.6454728 -3.9979799 -4.4598842 -4.9410172][-3.8172023 -4.6333694 -5.4328847 -5.8171053 -5.7495589 -5.4286175 -4.7110562 -3.6647344 -2.9000177 -2.9012232 -3.3677342 -3.7722518 -4.03024 -4.4155946 -4.8208561][-3.82193 -4.3009 -4.9268703 -5.3691983 -5.4551544 -5.3024368 -4.8693795 -4.1514459 -3.5020657 -3.3447728 -3.5814209 -3.8404086 -3.9960632 -4.2605929 -4.5695395][-3.8260503 -3.9456334 -4.3159113 -4.6875081 -4.8253231 -4.786324 -4.60682 -4.2200832 -3.7872796 -3.6066766 -3.6994913 -3.8392887 -3.90178 -4.0564728 -4.291646][-3.9272187 -3.8224375 -3.962347 -4.185153 -4.2946358 -4.3071728 -4.2825122 -4.1487069 -3.9440696 -3.8084109 -3.7998731 -3.8040733 -3.7372551 -3.7480886 -3.8694303]]...]
INFO - root - 2017-12-06 07:03:41.038675: step 4510, loss = 0.93, batch loss = 0.86 (13.2 examples/sec; 0.607 sec/batch; 55h:18m:19s remains)
INFO - root - 2017-12-06 07:03:47.079764: step 4520, loss = 1.02, batch loss = 0.95 (13.1 examples/sec; 0.612 sec/batch; 55h:47m:28s remains)
INFO - root - 2017-12-06 07:03:53.072086: step 4530, loss = 1.09, batch loss = 1.02 (13.3 examples/sec; 0.602 sec/batch; 54h:51m:53s remains)
INFO - root - 2017-12-06 07:03:59.140115: step 4540, loss = 0.63, batch loss = 0.56 (13.4 examples/sec; 0.599 sec/batch; 54h:35m:15s remains)
INFO - root - 2017-12-06 07:04:05.212789: step 4550, loss = 1.00, batch loss = 0.93 (13.2 examples/sec; 0.605 sec/batch; 55h:09m:02s remains)
INFO - root - 2017-12-06 07:04:11.244397: step 4560, loss = 1.08, batch loss = 1.01 (13.4 examples/sec; 0.598 sec/batch; 54h:30m:21s remains)
INFO - root - 2017-12-06 07:04:17.298754: step 4570, loss = 0.78, batch loss = 0.71 (13.4 examples/sec; 0.596 sec/batch; 54h:15m:54s remains)
INFO - root - 2017-12-06 07:04:23.299643: step 4580, loss = 1.17, batch loss = 1.10 (14.0 examples/sec; 0.571 sec/batch; 51h:59m:53s remains)
INFO - root - 2017-12-06 07:04:29.360011: step 4590, loss = 1.02, batch loss = 0.95 (13.3 examples/sec; 0.601 sec/batch; 54h:42m:08s remains)
INFO - root - 2017-12-06 07:04:35.486352: step 4600, loss = 0.92, batch loss = 0.85 (13.3 examples/sec; 0.601 sec/batch; 54h:43m:47s remains)
2017-12-06 07:04:36.076604: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.41477 -4.4402008 -4.5045214 -4.5756178 -4.6175418 -4.6320257 -4.61349 -4.5469704 -4.365972 -4.2778611 -4.4461474 -4.6755757 -4.8469305 -4.9341645 -4.9086313][-4.7996607 -4.8427916 -4.9222116 -4.9933305 -5.0118551 -5.0059729 -4.985682 -4.9172845 -4.710649 -4.6163263 -4.8056588 -5.0497046 -5.2133756 -5.263329 -5.1422114][-5.1384406 -5.1566892 -5.2122364 -5.2604933 -5.2420917 -5.20485 -5.189013 -5.1708603 -5.0475693 -5.0423679 -5.2803044 -5.5493822 -5.7156777 -5.7286491 -5.490025][-5.2533636 -5.1619177 -5.103034 -5.0596495 -4.9525337 -4.8278937 -4.7953596 -4.9011636 -5.0255442 -5.2651892 -5.6406264 -5.9873176 -6.1888409 -6.1922555 -5.8735309][-5.0030012 -4.7081532 -4.4063978 -4.1519203 -3.8516352 -3.5258605 -3.3917971 -3.6545823 -4.17686 -4.8441291 -5.5253124 -6.0788584 -6.389883 -6.4330893 -6.1071734][-4.567729 -4.0446405 -3.4406013 -2.8961122 -2.3034124 -1.6321723 -1.2397306 -1.5615339 -2.4773221 -3.6402173 -4.7453103 -5.5987382 -6.0615773 -6.1782951 -5.9221292][-4.3548932 -3.653311 -2.7729964 -1.939395 -1.0377316 0.01834631 0.75766754 0.51680279 -0.66476727 -2.2483263 -3.7589271 -4.8738174 -5.4040732 -5.5178614 -5.3173418][-4.4701824 -3.7470081 -2.7343278 -1.7357152 -0.65183115 0.6665616 1.7144356 1.6930423 0.49542427 -1.2826819 -3.0718398 -4.3795686 -4.9340181 -4.9993277 -4.79434][-4.7846179 -4.2174468 -3.2895441 -2.3465135 -1.3300028 -0.044748306 1.0961227 1.320333 0.37243176 -1.2902708 -3.1330376 -4.5262232 -5.104949 -5.1232948 -4.8485923][-5.0315757 -4.6998968 -3.9909554 -3.2545686 -2.4964352 -1.5205126 -0.57413149 -0.2446413 -0.85339212 -2.1821356 -3.8469586 -5.1932406 -5.79609 -5.8018236 -5.4399996][-5.0978894 -4.9724693 -4.5056529 -4.0219545 -3.5880024 -3.0455189 -2.4802649 -2.2244408 -2.524472 -3.3913569 -4.675488 -5.8209281 -6.4083939 -6.4528642 -6.0798669][-4.9439325 -4.9230781 -4.649416 -4.401897 -4.26411 -4.1176581 -3.9484339 -3.8647623 -3.9295878 -4.3112569 -5.1148019 -5.9449921 -6.4606638 -6.5965762 -6.3460164][-4.4307919 -4.4721308 -4.3552318 -4.2900848 -4.3467035 -4.441782 -4.5454 -4.6233521 -4.510149 -4.4603047 -4.8097653 -5.3105464 -5.7496152 -6.0359025 -6.0278358][-3.6128774 -3.7304389 -3.7455511 -3.7764635 -3.8839092 -4.0243645 -4.22194 -4.3745413 -4.1164293 -3.7529423 -3.7840075 -4.0623069 -4.5040565 -4.9966192 -5.2842979][-2.8010626 -3.0054553 -3.1236348 -3.1804414 -3.2349167 -3.2836618 -3.4233584 -3.5436611 -3.1681371 -2.6283298 -2.4978485 -2.6994343 -3.2465129 -3.9770689 -4.5432448]]...]
INFO - root - 2017-12-06 07:04:41.985007: step 4610, loss = 0.71, batch loss = 0.64 (14.0 examples/sec; 0.570 sec/batch; 51h:53m:20s remains)
INFO - root - 2017-12-06 07:04:48.036460: step 4620, loss = 1.17, batch loss = 1.10 (13.3 examples/sec; 0.600 sec/batch; 54h:37m:32s remains)
INFO - root - 2017-12-06 07:04:54.078786: step 4630, loss = 0.99, batch loss = 0.92 (13.3 examples/sec; 0.601 sec/batch; 54h:44m:31s remains)
INFO - root - 2017-12-06 07:05:00.132876: step 4640, loss = 0.96, batch loss = 0.89 (13.0 examples/sec; 0.616 sec/batch; 56h:05m:24s remains)
INFO - root - 2017-12-06 07:05:06.157289: step 4650, loss = 0.79, batch loss = 0.72 (13.3 examples/sec; 0.601 sec/batch; 54h:46m:29s remains)
INFO - root - 2017-12-06 07:05:12.146310: step 4660, loss = 0.82, batch loss = 0.75 (13.0 examples/sec; 0.614 sec/batch; 55h:53m:12s remains)
INFO - root - 2017-12-06 07:05:18.190280: step 4670, loss = 0.74, batch loss = 0.67 (13.2 examples/sec; 0.606 sec/batch; 55h:09m:40s remains)
INFO - root - 2017-12-06 07:05:24.245167: step 4680, loss = 0.84, batch loss = 0.77 (13.4 examples/sec; 0.599 sec/batch; 54h:30m:49s remains)
INFO - root - 2017-12-06 07:05:30.276559: step 4690, loss = 0.77, batch loss = 0.70 (13.0 examples/sec; 0.614 sec/batch; 55h:55m:05s remains)
INFO - root - 2017-12-06 07:05:36.283311: step 4700, loss = 1.05, batch loss = 0.98 (13.3 examples/sec; 0.602 sec/batch; 54h:51m:22s remains)
2017-12-06 07:05:36.891200: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3684916 -4.6000495 -4.9098964 -5.0639572 -5.0410976 -4.880044 -4.7187805 -4.6561565 -4.6325006 -4.7175345 -4.8642063 -4.9607272 -4.9896774 -4.9387083 -4.8783193][-4.6030011 -4.9400606 -5.3023338 -5.4410529 -5.3232088 -5.0202694 -4.70587 -4.5579772 -4.5404816 -4.6594276 -4.7827077 -4.7591715 -4.6517186 -4.5410647 -4.5106406][-4.5796976 -4.9889889 -5.37531 -5.4541731 -5.2068429 -4.7608747 -4.3450947 -4.1846938 -4.2353292 -4.4195848 -4.4934206 -4.2920961 -3.9906301 -3.7826898 -3.7659261][-4.3101707 -4.7408695 -5.0921803 -5.0507646 -4.6579738 -4.1180034 -3.722311 -3.6796107 -3.8643975 -4.0986571 -4.0549903 -3.62711 -3.1050193 -2.785697 -2.760762][-3.8586297 -4.2449217 -4.4888377 -4.2615886 -3.7032123 -3.1090012 -2.8307686 -3.0215309 -3.4181557 -3.7103691 -3.5393627 -2.9280562 -2.2576892 -1.8724346 -1.8435771][-3.390327 -3.6591086 -3.7166374 -3.2391927 -2.4816065 -1.8309894 -1.6989985 -2.176857 -2.8280873 -3.2105706 -2.9687676 -2.2904596 -1.60482 -1.2518904 -1.2790768][-3.0232117 -3.1343389 -2.9734981 -2.2445574 -1.297576 -0.5886519 -0.58182454 -1.3250976 -2.2214468 -2.7222333 -2.4853373 -1.8445809 -1.2491741 -1.0210171 -1.1769638][-2.9296484 -2.919982 -2.596756 -1.6972687 -0.62517953 0.14511919 0.12203455 -0.73270178 -1.7477112 -2.3480527 -2.2030158 -1.7053449 -1.3084295 -1.2707815 -1.5766077][-3.1885204 -3.17623 -2.8291869 -1.9065199 -0.82546973 -0.037796021 0.0044736862 -0.74659419 -1.6973445 -2.3379593 -2.3140457 -2.0057573 -1.8277199 -1.9413469 -2.301038][-3.5750122 -3.6568968 -3.4058158 -2.6097205 -1.6601737 -0.94198442 -0.84307647 -1.4145658 -2.204963 -2.8251226 -2.9115219 -2.7602262 -2.7346168 -2.8940601 -3.1546583][-3.8732793 -4.1263533 -4.0714221 -3.5134726 -2.7698684 -2.1496961 -2.0128214 -2.445555 -3.0968933 -3.6846392 -3.8395095 -3.7391977 -3.7258124 -3.8039272 -3.8711095][-3.9801228 -4.4224343 -4.6033883 -4.3285971 -3.7999306 -3.2603588 -3.0873241 -3.4272819 -3.9904795 -4.5407367 -4.7304349 -4.6087909 -4.4891691 -4.3898396 -4.2145395][-3.9995861 -4.51781 -4.8427062 -4.784626 -4.4232211 -3.9421248 -3.7460933 -4.0340042 -4.558846 -5.0841932 -5.2981176 -5.1411023 -4.8780694 -4.5936575 -4.2056842][-4.0135274 -4.4592352 -4.75912 -4.7559013 -4.4686966 -4.0324349 -3.8543844 -4.1372018 -4.6786308 -5.2424507 -5.5285797 -5.4081244 -5.0826979 -4.6888452 -4.1766815][-3.8253508 -4.127048 -4.3172779 -4.2722588 -4.00928 -3.6181049 -3.4740567 -3.7697878 -4.3413653 -5.0084944 -5.4655852 -5.5291772 -5.3112764 -4.9273186 -4.391921]]...]
INFO - root - 2017-12-06 07:05:42.977978: step 4710, loss = 0.89, batch loss = 0.82 (13.3 examples/sec; 0.603 sec/batch; 54h:56m:12s remains)
INFO - root - 2017-12-06 07:05:48.936399: step 4720, loss = 0.73, batch loss = 0.66 (13.3 examples/sec; 0.601 sec/batch; 54h:45m:47s remains)
INFO - root - 2017-12-06 07:05:54.939998: step 4730, loss = 0.86, batch loss = 0.79 (13.1 examples/sec; 0.613 sec/batch; 55h:48m:25s remains)
INFO - root - 2017-12-06 07:06:00.995429: step 4740, loss = 0.75, batch loss = 0.68 (13.0 examples/sec; 0.617 sec/batch; 56h:12m:26s remains)
INFO - root - 2017-12-06 07:06:07.110972: step 4750, loss = 0.71, batch loss = 0.64 (13.7 examples/sec; 0.585 sec/batch; 53h:15m:59s remains)
INFO - root - 2017-12-06 07:06:13.183677: step 4760, loss = 1.06, batch loss = 0.99 (12.9 examples/sec; 0.619 sec/batch; 56h:20m:31s remains)
INFO - root - 2017-12-06 07:06:19.271840: step 4770, loss = 0.77, batch loss = 0.70 (13.8 examples/sec; 0.580 sec/batch; 52h:47m:57s remains)
INFO - root - 2017-12-06 07:06:25.339694: step 4780, loss = 1.04, batch loss = 0.97 (13.2 examples/sec; 0.608 sec/batch; 55h:18m:40s remains)
INFO - root - 2017-12-06 07:06:31.406448: step 4790, loss = 0.81, batch loss = 0.74 (12.9 examples/sec; 0.622 sec/batch; 56h:36m:02s remains)
INFO - root - 2017-12-06 07:06:37.395483: step 4800, loss = 0.70, batch loss = 0.63 (13.3 examples/sec; 0.599 sec/batch; 54h:33m:33s remains)
2017-12-06 07:06:38.049945: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3526707 -5.5025587 -5.5776839 -5.590425 -5.5969052 -5.61743 -5.6700544 -5.754199 -5.8733525 -5.975718 -5.9230413 -5.687211 -5.2527528 -4.8017063 -4.5440879][-4.5452309 -4.8967433 -5.1839561 -5.3559856 -5.461771 -5.530962 -5.5816565 -5.6227131 -5.7122431 -5.846467 -5.8905153 -5.8086615 -5.5703416 -5.2903423 -5.0945563][-3.657984 -4.2176743 -4.727694 -5.0498786 -5.2488656 -5.3460178 -5.3436174 -5.2753739 -5.2731018 -5.4007263 -5.5255003 -5.6003475 -5.6100521 -5.5728655 -5.5127959][-2.9274631 -3.5807719 -4.2086248 -4.5946403 -4.8170114 -4.877492 -4.7695336 -4.5712757 -4.5012569 -4.6763906 -4.9090085 -5.10736 -5.3125696 -5.4925432 -5.6050415][-2.5621228 -3.1436417 -3.726311 -4.031333 -4.1542349 -4.0721607 -3.7894518 -3.4557731 -3.3799238 -3.7155161 -4.1381831 -4.4537477 -4.7616963 -5.0649033 -5.323566][-2.6263981 -3.0535512 -3.4667089 -3.5805814 -3.5087776 -3.1879668 -2.6705019 -2.1896122 -2.1254783 -2.6611705 -3.312993 -3.7562132 -4.0933976 -4.4102483 -4.7461004][-3.0509391 -3.3475275 -3.5744731 -3.4848223 -3.18851 -2.6086574 -1.8572726 -1.2266021 -1.1295719 -1.7764218 -2.5860322 -3.1452575 -3.4889486 -3.7627797 -4.1171384][-3.7553575 -3.9644175 -4.0340204 -3.7746272 -3.2926137 -2.5317702 -1.6407852 -0.90667057 -0.7129128 -1.2864225 -2.0702839 -2.66242 -3.0174415 -3.2734056 -3.6462061][-4.5930614 -4.7474003 -4.7191243 -4.3549318 -3.74972 -2.9128656 -1.9996252 -1.2405591 -0.9235661 -1.2614367 -1.8159657 -2.3020227 -2.6480675 -2.9226117 -3.3344376][-5.3472128 -5.4554834 -5.397171 -5.0219812 -4.3637967 -3.5203619 -2.6630239 -1.9420617 -1.5310669 -1.601567 -1.8565602 -2.1500285 -2.4265747 -2.6912403 -3.1044207][-5.7815976 -5.8750372 -5.8865089 -5.633122 -5.011075 -4.1962414 -3.4137838 -2.7571409 -2.3247612 -2.2165837 -2.2288988 -2.3335862 -2.5019212 -2.7048268 -3.0393124][-5.9010396 -6.0080934 -6.1338668 -6.0484638 -5.5387192 -4.8300595 -4.1870656 -3.6575801 -3.2941818 -3.113116 -2.9655771 -2.9099224 -2.956171 -3.0665083 -3.2685761][-5.77642 -5.9105749 -6.1228571 -6.1481647 -5.7648659 -5.2213259 -4.7812653 -4.4526677 -4.2622271 -4.1316166 -3.9356611 -3.7847276 -3.7371602 -3.7521122 -3.7984478][-5.5515156 -5.707715 -5.937387 -5.9719305 -5.6804371 -5.3016753 -5.0481057 -4.9238205 -4.930573 -4.934967 -4.8139362 -4.6713209 -4.5937533 -4.5440888 -4.4651542][-5.3963437 -5.5399394 -5.7140989 -5.6908665 -5.4646473 -5.2191453 -5.0719643 -5.0615678 -5.1830406 -5.2986603 -5.2866731 -5.1968465 -5.1323061 -5.0697718 -4.966176]]...]
INFO - root - 2017-12-06 07:06:44.096779: step 4810, loss = 0.86, batch loss = 0.79 (13.0 examples/sec; 0.613 sec/batch; 55h:48m:35s remains)
INFO - root - 2017-12-06 07:06:50.152188: step 4820, loss = 0.88, batch loss = 0.81 (13.3 examples/sec; 0.602 sec/batch; 54h:49m:56s remains)
INFO - root - 2017-12-06 07:06:56.054528: step 4830, loss = 0.80, batch loss = 0.73 (13.0 examples/sec; 0.613 sec/batch; 55h:47m:55s remains)
INFO - root - 2017-12-06 07:07:02.109829: step 4840, loss = 1.16, batch loss = 1.09 (13.8 examples/sec; 0.580 sec/batch; 52h:46m:09s remains)
INFO - root - 2017-12-06 07:07:08.171220: step 4850, loss = 0.87, batch loss = 0.80 (13.3 examples/sec; 0.600 sec/batch; 54h:34m:11s remains)
INFO - root - 2017-12-06 07:07:14.318429: step 4860, loss = 0.88, batch loss = 0.81 (12.3 examples/sec; 0.649 sec/batch; 59h:04m:52s remains)
INFO - root - 2017-12-06 07:07:20.420409: step 4870, loss = 1.04, batch loss = 0.97 (13.3 examples/sec; 0.603 sec/batch; 54h:52m:49s remains)
INFO - root - 2017-12-06 07:07:26.401321: step 4880, loss = 0.73, batch loss = 0.66 (13.5 examples/sec; 0.592 sec/batch; 53h:54m:00s remains)
INFO - root - 2017-12-06 07:07:32.481046: step 4890, loss = 0.91, batch loss = 0.84 (13.2 examples/sec; 0.607 sec/batch; 55h:16m:26s remains)
INFO - root - 2017-12-06 07:07:38.603346: step 4900, loss = 0.96, batch loss = 0.89 (12.8 examples/sec; 0.626 sec/batch; 56h:59m:06s remains)
2017-12-06 07:07:39.270639: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1305561 -3.0923598 -2.7287321 -2.1951849 -1.6411948 -1.1978152 -1.4783795 -2.234338 -3.2259445 -4.0752983 -4.6801686 -5.1950679 -5.8917408 -6.3672633 -6.2460442][-3.2014499 -3.5791087 -3.4946609 -3.0035186 -2.3250794 -1.6448133 -1.5974703 -2.0533836 -2.8846703 -3.753962 -4.4619832 -5.100564 -5.9368534 -6.6224141 -6.6278353][-3.0000176 -3.6683033 -3.8540754 -3.4567509 -2.6575105 -1.6930716 -1.2986538 -1.5352004 -2.3487411 -3.32027 -4.156496 -4.9071774 -5.8364763 -6.6541233 -6.7568054][-2.4809825 -3.2849636 -3.6763504 -3.3635526 -2.4086559 -1.1417034 -0.44247437 -0.58215404 -1.5270622 -2.704133 -3.730691 -4.6184406 -5.5769749 -6.3911734 -6.5118942][-1.9098177 -2.7241724 -3.2481039 -3.0010142 -1.896076 -0.33231926 0.67063379 0.62146759 -0.4993875 -1.9393551 -3.2262974 -4.3054228 -5.2527061 -5.91879 -5.9394836][-1.7975721 -2.5404482 -3.1028686 -2.8863535 -1.659786 0.17579222 1.5305414 1.6940203 0.50810194 -1.1333499 -2.6728745 -3.9518106 -4.8508215 -5.2813282 -5.1316209][-2.2300491 -2.8146138 -3.288837 -3.0494635 -1.7756164 0.20061111 1.8263702 2.2540741 1.1440463 -0.55970311 -2.2358959 -3.6271877 -4.4226952 -4.5844216 -4.2431159][-2.9959066 -3.2869873 -3.4856822 -3.1377449 -1.9355266 -0.10082245 1.4870138 2.0146828 1.0479383 -0.5667665 -2.2052624 -3.5655074 -4.2332335 -4.2027888 -3.699218][-3.5163665 -3.3904207 -3.1730306 -2.6586106 -1.6519215 -0.26725292 0.89309311 1.2292638 0.38114071 -1.0155954 -2.4316962 -3.6224942 -4.1778316 -4.0695887 -3.4735746][-3.7453985 -3.2120304 -2.5599053 -1.8602796 -1.1025233 -0.34143305 0.16262388 0.107409 -0.66525626 -1.7637956 -2.8522539 -3.8001912 -4.3017907 -4.2515612 -3.6773562][-3.9328732 -3.1503956 -2.1718633 -1.2902479 -0.71414208 -0.51830578 -0.66155481 -1.1363049 -1.8700597 -2.6848741 -3.4395375 -4.1392345 -4.6215892 -4.6832089 -4.2303224][-4.1743693 -3.327831 -2.1770339 -1.1232321 -0.59976077 -0.7817471 -1.4163127 -2.2644436 -3.0400894 -3.6739264 -4.133812 -4.5641565 -4.956121 -5.0581431 -4.7405119][-4.5758944 -3.8379033 -2.6803489 -1.5388815 -0.99622345 -1.3162472 -2.1813197 -3.244328 -4.0847235 -4.6185141 -4.8250241 -4.9704666 -5.2035713 -5.2670388 -5.0532832][-4.9127927 -4.4195151 -3.4549353 -2.3965161 -1.8424516 -2.1038084 -2.912879 -3.9214194 -4.6987929 -5.1174326 -5.1026039 -4.968431 -4.9935894 -4.9678116 -4.829011][-5.036798 -4.8498898 -4.2252226 -3.4196625 -2.9298234 -3.0477071 -3.5926838 -4.3200932 -4.8656545 -5.0760021 -4.8628306 -4.5143762 -4.3567276 -4.2382164 -4.1627665]]...]
INFO - root - 2017-12-06 07:07:45.314453: step 4910, loss = 1.02, batch loss = 0.95 (13.3 examples/sec; 0.602 sec/batch; 54h:47m:49s remains)
INFO - root - 2017-12-06 07:07:51.332613: step 4920, loss = 0.81, batch loss = 0.74 (13.5 examples/sec; 0.591 sec/batch; 53h:47m:15s remains)
INFO - root - 2017-12-06 07:07:57.410799: step 4930, loss = 0.83, batch loss = 0.76 (13.0 examples/sec; 0.613 sec/batch; 55h:49m:00s remains)
INFO - root - 2017-12-06 07:08:03.342608: step 4940, loss = 1.00, batch loss = 0.93 (13.8 examples/sec; 0.581 sec/batch; 52h:51m:51s remains)
INFO - root - 2017-12-06 07:08:09.473999: step 4950, loss = 1.01, batch loss = 0.94 (13.1 examples/sec; 0.610 sec/batch; 55h:27m:28s remains)
INFO - root - 2017-12-06 07:08:15.546553: step 4960, loss = 1.01, batch loss = 0.94 (13.5 examples/sec; 0.595 sec/batch; 54h:06m:39s remains)
INFO - root - 2017-12-06 07:08:21.658975: step 4970, loss = 0.91, batch loss = 0.84 (13.0 examples/sec; 0.617 sec/batch; 56h:09m:36s remains)
INFO - root - 2017-12-06 07:08:27.623193: step 4980, loss = 1.03, batch loss = 0.96 (13.5 examples/sec; 0.591 sec/batch; 53h:44m:55s remains)
INFO - root - 2017-12-06 07:08:33.711033: step 4990, loss = 0.78, batch loss = 0.71 (13.1 examples/sec; 0.609 sec/batch; 55h:24m:45s remains)
INFO - root - 2017-12-06 07:08:39.739983: step 5000, loss = 0.93, batch loss = 0.86 (13.0 examples/sec; 0.613 sec/batch; 55h:48m:02s remains)
2017-12-06 07:08:40.328989: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.97891 -5.2331309 -4.6313248 -3.5890002 -2.9002793 -2.7570624 -2.9802775 -3.544194 -4.1900558 -4.841424 -5.3479166 -5.3480072 -5.0876946 -4.9161911 -4.8763456][-5.1709146 -5.7322226 -5.3422318 -4.3744116 -3.5867002 -3.2936113 -3.3566017 -3.6689196 -3.9846427 -4.3916273 -4.906352 -4.998466 -4.7610812 -4.6063719 -4.6204071][-5.2190714 -5.9904647 -5.7106533 -4.7709045 -3.9000487 -3.5203066 -3.5648046 -3.8799715 -4.0250587 -4.1803946 -4.5742307 -4.6295919 -4.3939819 -4.328886 -4.4551558][-5.1116443 -5.9619675 -5.7149286 -4.8210506 -3.8931956 -3.4146619 -3.4920585 -3.9752038 -4.133616 -4.0862222 -4.2463622 -4.1914816 -3.9971249 -4.0655437 -4.3056607][-5.0927992 -5.9272985 -5.6189995 -4.6840467 -3.573297 -2.7373405 -2.5807195 -3.1959529 -3.6470726 -3.7478328 -3.9084413 -3.9603956 -3.9822493 -4.177186 -4.4083147][-5.22642 -5.9286094 -5.44184 -4.3153462 -2.832731 -1.3408289 -0.56826138 -1.1682875 -2.2194476 -2.9758129 -3.5305979 -3.9350426 -4.295795 -4.5587659 -4.6557617][-5.3193803 -5.7839985 -5.0436687 -3.670609 -1.8679733 0.22684669 1.6713181 1.0629606 -0.7736311 -2.349227 -3.3477328 -3.99638 -4.519464 -4.7188964 -4.640152][-5.2992129 -5.4991894 -4.5401196 -3.0268564 -1.2467752 0.95400333 2.6709094 1.9496589 -0.46498442 -2.4905658 -3.5667775 -4.0944853 -4.4887323 -4.5420856 -4.346705][-5.291214 -5.2794042 -4.2663317 -2.8533268 -1.5049047 0.20149803 1.667665 0.89776039 -1.483443 -3.2944698 -3.9857986 -4.0941167 -4.2322016 -4.2027235 -4.028686][-5.5178218 -5.4136992 -4.544765 -3.398169 -2.5939434 -1.5609217 -0.47236323 -1.0969095 -2.9516573 -4.1225424 -4.2171149 -3.8740449 -3.8507547 -3.9207478 -3.9402933][-5.8355932 -5.7749405 -5.2041316 -4.3823042 -3.9722807 -3.4079452 -2.5418034 -2.8373754 -3.9993331 -4.5171638 -4.1516771 -3.542217 -3.5087254 -3.8029354 -4.0725093][-5.8954163 -6.0029674 -5.7894297 -5.3016858 -5.1229043 -4.7807455 -3.9894438 -3.9012303 -4.4529843 -4.553781 -4.009398 -3.3541462 -3.3734388 -3.8380857 -4.235466][-5.5564461 -5.8438873 -5.9428825 -5.7814255 -5.7971668 -5.619318 -4.9053707 -4.5444007 -4.6851263 -4.6227064 -4.1295843 -3.5595586 -3.586838 -4.0408287 -4.3651662][-4.9534645 -5.3607125 -5.6438508 -5.7033854 -5.8745537 -5.8801346 -5.3749595 -4.976265 -4.9570704 -4.9450126 -4.6757593 -4.2890553 -4.2960687 -4.598978 -4.7415495][-4.3798 -4.7850542 -5.1095905 -5.2519007 -5.4763217 -5.6294637 -5.4179478 -5.183702 -5.2013006 -5.3239508 -5.305223 -5.1542983 -5.1858697 -5.3335967 -5.34551]]...]
INFO - root - 2017-12-06 07:08:46.474590: step 5010, loss = 0.82, batch loss = 0.75 (12.8 examples/sec; 0.627 sec/batch; 57h:01m:54s remains)
INFO - root - 2017-12-06 07:08:52.616099: step 5020, loss = 1.12, batch loss = 1.05 (13.2 examples/sec; 0.605 sec/batch; 55h:03m:48s remains)
INFO - root - 2017-12-06 07:08:58.616513: step 5030, loss = 1.21, batch loss = 1.14 (12.8 examples/sec; 0.625 sec/batch; 56h:50m:42s remains)
INFO - root - 2017-12-06 07:09:04.521285: step 5040, loss = 1.01, batch loss = 0.94 (16.6 examples/sec; 0.483 sec/batch; 43h:53m:22s remains)
INFO - root - 2017-12-06 07:09:10.558309: step 5050, loss = 0.82, batch loss = 0.74 (13.1 examples/sec; 0.611 sec/batch; 55h:35m:53s remains)
INFO - root - 2017-12-06 07:09:16.639013: step 5060, loss = 0.85, batch loss = 0.78 (13.1 examples/sec; 0.612 sec/batch; 55h:40m:43s remains)
INFO - root - 2017-12-06 07:09:22.736499: step 5070, loss = 1.03, batch loss = 0.96 (13.0 examples/sec; 0.616 sec/batch; 56h:00m:42s remains)
INFO - root - 2017-12-06 07:09:28.857835: step 5080, loss = 0.87, batch loss = 0.80 (13.0 examples/sec; 0.617 sec/batch; 56h:07m:50s remains)
INFO - root - 2017-12-06 07:09:34.972210: step 5090, loss = 0.98, batch loss = 0.91 (13.0 examples/sec; 0.615 sec/batch; 55h:56m:09s remains)
INFO - root - 2017-12-06 07:09:41.075126: step 5100, loss = 0.92, batch loss = 0.84 (13.5 examples/sec; 0.593 sec/batch; 53h:58m:17s remains)
2017-12-06 07:09:41.635830: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3011127 -4.1372213 -4.0500803 -4.0092387 -4.020824 -4.1566172 -4.2374039 -4.2682543 -4.2740064 -4.2652459 -4.2726684 -4.2065325 -4.1560965 -4.0344396 -3.8316634][-4.7714133 -4.5867977 -4.429915 -4.2433252 -4.0540195 -3.9925766 -3.9381139 -3.9798996 -4.1250052 -4.2956557 -4.425705 -4.3990602 -4.3147674 -4.1033382 -3.8022063][-5.065588 -4.9479303 -4.8097959 -4.50639 -4.0554876 -3.7211428 -3.5110211 -3.5376902 -3.7897127 -4.15877 -4.5016985 -4.645844 -4.6357017 -4.3950596 -3.9743478][-5.0305614 -5.017971 -4.9509382 -4.5741181 -3.8867443 -3.2945771 -2.9364772 -2.9245694 -3.2368894 -3.7727933 -4.3489532 -4.7359233 -4.9015288 -4.73166 -4.2347503][-4.7495608 -4.8317828 -4.8280559 -4.40176 -3.5701153 -2.8213818 -2.3693147 -2.3147848 -2.6312439 -3.23374 -3.9737453 -4.5698605 -4.9321566 -4.8898015 -4.3733039][-4.3999453 -4.5678468 -4.6129136 -4.18489 -3.3289623 -2.5279408 -2.0130036 -1.9098468 -2.1790881 -2.7382498 -3.5345268 -4.2468157 -4.7581711 -4.8209252 -4.3077703][-4.0574007 -4.2718387 -4.3330779 -3.9501672 -3.1822112 -2.41686 -1.8656123 -1.724597 -1.9343092 -2.3986721 -3.1739607 -3.9275346 -4.52636 -4.6304855 -4.120605][-3.8008549 -3.9876692 -4.0058894 -3.6761198 -3.0373669 -2.3305881 -1.7415988 -1.5754313 -1.7541492 -2.1481528 -2.8886144 -3.6666641 -4.302568 -4.39292 -3.8974161][-3.6696539 -3.7919061 -3.7369797 -3.4345748 -2.9067955 -2.2532537 -1.62005 -1.4237618 -1.6055315 -1.9937263 -2.7358689 -3.5416708 -4.1727076 -4.2385035 -3.7862163][-3.6482821 -3.7474012 -3.65565 -3.376756 -2.9398208 -2.360297 -1.7385168 -1.5347648 -1.73068 -2.1257663 -2.844579 -3.6316547 -4.214992 -4.2579832 -3.8640704][-3.739351 -3.8939176 -3.8302846 -3.5950556 -3.2440615 -2.7693 -2.2238626 -2.0258875 -2.1893992 -2.5150335 -3.1187057 -3.7991605 -4.2944856 -4.31504 -3.9802742][-3.76717 -4.0390935 -4.0710144 -3.9390292 -3.7113538 -3.3869281 -2.9813337 -2.814986 -2.884254 -3.0367665 -3.4149237 -3.8882315 -4.2451687 -4.2328391 -3.951869][-3.5712557 -3.9720445 -4.1514525 -4.1942854 -4.1461678 -4.021893 -3.8278365 -3.7440147 -3.7135563 -3.6541064 -3.7463231 -3.9442525 -4.1122494 -4.0393519 -3.7949879][-3.2908454 -3.7575541 -4.0860796 -4.3261471 -4.467392 -4.5297909 -4.5383792 -4.5472045 -4.4504824 -4.2370272 -4.0994554 -4.0494719 -4.0370545 -3.8917837 -3.65532][-3.2530951 -3.6895347 -4.0903254 -4.4422913 -4.6864672 -4.8434424 -4.9712858 -5.0414972 -4.9349141 -4.6774483 -4.4291019 -4.2256947 -4.0850744 -3.871264 -3.6124811]]...]
INFO - root - 2017-12-06 07:09:47.687157: step 5110, loss = 0.93, batch loss = 0.86 (13.1 examples/sec; 0.609 sec/batch; 55h:25m:08s remains)
INFO - root - 2017-12-06 07:09:53.716184: step 5120, loss = 0.79, batch loss = 0.72 (13.6 examples/sec; 0.586 sec/batch; 53h:18m:01s remains)
INFO - root - 2017-12-06 07:09:59.849825: step 5130, loss = 0.87, batch loss = 0.80 (12.3 examples/sec; 0.649 sec/batch; 59h:02m:16s remains)
INFO - root - 2017-12-06 07:10:05.981285: step 5140, loss = 0.68, batch loss = 0.61 (14.0 examples/sec; 0.570 sec/batch; 51h:49m:08s remains)
INFO - root - 2017-12-06 07:10:11.967192: step 5150, loss = 0.81, batch loss = 0.74 (12.9 examples/sec; 0.619 sec/batch; 56h:16m:00s remains)
INFO - root - 2017-12-06 07:10:18.056307: step 5160, loss = 1.03, batch loss = 0.96 (13.0 examples/sec; 0.616 sec/batch; 55h:58m:35s remains)
INFO - root - 2017-12-06 07:10:24.159006: step 5170, loss = 0.92, batch loss = 0.85 (12.8 examples/sec; 0.627 sec/batch; 56h:59m:38s remains)
INFO - root - 2017-12-06 07:10:30.122485: step 5180, loss = 0.91, batch loss = 0.84 (13.2 examples/sec; 0.606 sec/batch; 55h:04m:25s remains)
INFO - root - 2017-12-06 07:10:36.239363: step 5190, loss = 0.91, batch loss = 0.84 (13.5 examples/sec; 0.592 sec/batch; 53h:50m:04s remains)
INFO - root - 2017-12-06 07:10:42.274809: step 5200, loss = 0.90, batch loss = 0.83 (12.9 examples/sec; 0.619 sec/batch; 56h:18m:50s remains)
2017-12-06 07:10:42.851466: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5075059 -5.5350194 -5.44913 -5.3035793 -5.1472874 -5.0263686 -4.9587522 -4.9381161 -4.9329948 -4.8838768 -4.7868137 -4.6749358 -4.5552292 -4.4307 -4.3210912][-5.1733747 -5.4027953 -5.424654 -5.2932181 -5.0940084 -4.9254708 -4.8576527 -4.9112067 -5.0262122 -5.0719218 -5.0178614 -4.9073286 -4.7614045 -4.5997515 -4.4594493][-4.3903 -4.8844609 -5.0483 -4.9311028 -4.6506371 -4.3874803 -4.2969265 -4.4483757 -4.7629557 -5.003191 -5.0935092 -5.0733109 -4.9684 -4.8042917 -4.635931][-3.2376571 -4.069416 -4.4116321 -4.2971854 -3.8576043 -3.3928087 -3.2050467 -3.4492195 -4.0319467 -4.5581422 -4.8873506 -5.037282 -5.0424242 -4.9288058 -4.7619185][-2.020633 -3.1656036 -3.6954718 -3.5881863 -2.9233136 -2.1249411 -1.7029383 -1.9855132 -2.8736551 -3.7613912 -4.4034715 -4.7729549 -4.9303389 -4.9106164 -4.7857089][-1.2528932 -2.5116067 -3.151629 -3.0709977 -2.2264519 -1.0697103 -0.29174232 -0.47036362 -1.5684419 -2.7962246 -3.7686651 -4.3813043 -4.7044892 -4.7917991 -4.7246957][-1.3287747 -2.4351687 -3.0608206 -3.0143375 -2.1343298 -0.82691717 0.22320604 0.30848694 -0.72861886 -2.073801 -3.2453685 -4.0503759 -4.5282731 -4.7162843 -4.6860094][-2.2063122 -3.0352893 -3.5424933 -3.5104268 -2.7632594 -1.6260545 -0.5947206 -0.25329494 -0.9316287 -2.0349827 -3.111908 -3.9478693 -4.5106244 -4.7770448 -4.771667][-3.3521106 -3.9484091 -4.3202553 -4.2689681 -3.7302873 -2.9744847 -2.2278566 -1.828974 -2.1312263 -2.7807577 -3.4836159 -4.1259065 -4.6365032 -4.9324164 -4.9654212][-4.2360759 -4.6883883 -4.9494648 -4.8584614 -4.5235577 -4.1660185 -3.7675338 -3.45571 -3.5050468 -3.7141223 -3.9915049 -4.3813157 -4.7927346 -5.0877185 -5.1712551][-4.6921721 -5.0413179 -5.182632 -5.037703 -4.8495331 -4.7713618 -4.6105051 -4.373436 -4.214572 -4.0427532 -3.9849057 -4.2367897 -4.6575432 -5.0157061 -5.1960464][-4.8281126 -5.0618396 -5.0421042 -4.805223 -4.6696658 -4.692111 -4.6028266 -4.3609223 -3.9755864 -3.4657197 -3.1854432 -3.4643517 -4.0629563 -4.6021705 -4.9485564][-4.7758975 -4.89546 -4.7099733 -4.356883 -4.1888723 -4.1543155 -3.9838359 -3.635622 -3.0037262 -2.2058215 -1.7770178 -2.1701412 -3.0536509 -3.8784163 -4.4614429][-4.5976095 -4.6257033 -4.347517 -3.9469972 -3.754848 -3.6164713 -3.3062658 -2.8124726 -2.0009212 -1.0167024 -0.44286394 -0.86829495 -1.9548671 -3.0443718 -3.8759317][-4.3801675 -4.3303471 -4.0397129 -3.6866388 -3.5589519 -3.42036 -3.069494 -2.5157533 -1.682056 -0.70170712 -0.021924019 -0.28137016 -1.2959249 -2.4321399 -3.3909111]]...]
INFO - root - 2017-12-06 07:10:48.965313: step 5210, loss = 0.68, batch loss = 0.61 (13.1 examples/sec; 0.609 sec/batch; 55h:22m:17s remains)
INFO - root - 2017-12-06 07:10:54.996384: step 5220, loss = 1.11, batch loss = 1.04 (13.5 examples/sec; 0.594 sec/batch; 53h:58m:54s remains)
INFO - root - 2017-12-06 07:11:01.008534: step 5230, loss = 0.94, batch loss = 0.87 (13.3 examples/sec; 0.603 sec/batch; 54h:51m:15s remains)
INFO - root - 2017-12-06 07:11:07.022585: step 5240, loss = 0.91, batch loss = 0.84 (13.1 examples/sec; 0.609 sec/batch; 55h:21m:44s remains)
INFO - root - 2017-12-06 07:11:13.062101: step 5250, loss = 0.88, batch loss = 0.81 (13.3 examples/sec; 0.602 sec/batch; 54h:44m:34s remains)
INFO - root - 2017-12-06 07:11:19.009397: step 5260, loss = 1.10, batch loss = 1.03 (12.9 examples/sec; 0.620 sec/batch; 56h:22m:31s remains)
INFO - root - 2017-12-06 07:11:25.103186: step 5270, loss = 0.99, batch loss = 0.92 (13.1 examples/sec; 0.611 sec/batch; 55h:30m:54s remains)
INFO - root - 2017-12-06 07:11:31.215090: step 5280, loss = 1.12, batch loss = 1.05 (13.7 examples/sec; 0.585 sec/batch; 53h:08m:16s remains)
INFO - root - 2017-12-06 07:11:37.254775: step 5290, loss = 1.00, batch loss = 0.93 (13.0 examples/sec; 0.613 sec/batch; 55h:45m:34s remains)
INFO - root - 2017-12-06 07:11:43.345216: step 5300, loss = 0.92, batch loss = 0.85 (13.4 examples/sec; 0.595 sec/batch; 54h:06m:28s remains)
2017-12-06 07:11:43.932641: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0954609 -5.2129726 -5.34154 -5.2837758 -5.0666995 -4.9039955 -4.8701968 -4.8350029 -4.7171555 -4.6668267 -4.7116895 -4.8028669 -4.8498816 -4.7816515 -4.5633192][-5.6137514 -5.7540679 -5.954288 -5.9392595 -5.7094359 -5.5291009 -5.5089054 -5.4594421 -5.2623248 -5.1602168 -5.204114 -5.331183 -5.4248972 -5.3770528 -5.08133][-5.8740149 -5.9511766 -6.1167397 -6.1188216 -5.9219937 -5.7726569 -5.8364468 -5.8954263 -5.7574081 -5.658421 -5.6702795 -5.7684956 -5.8772049 -5.8647356 -5.5220714][-5.8857379 -5.892561 -5.9130454 -5.8222179 -5.5796523 -5.3944631 -5.5462027 -5.8595705 -6.0330162 -6.1236043 -6.142787 -6.1653819 -6.2216287 -6.2072392 -5.8463526][-5.5334387 -5.5472116 -5.4226279 -5.1539183 -4.7316685 -4.3609524 -4.436697 -4.9852643 -5.6618733 -6.21159 -6.4270439 -6.449419 -6.4263325 -6.373836 -6.0540156][-4.6307478 -4.7674956 -4.6422381 -4.2348208 -3.551719 -2.8386412 -2.5519722 -3.0922513 -4.2469468 -5.4458876 -6.132565 -6.348855 -6.31415 -6.2381115 -6.0225019][-3.5376096 -3.8251345 -3.856144 -3.4261985 -2.4754195 -1.2981858 -0.40167713 -0.62413144 -2.0494571 -3.8944597 -5.2240357 -5.8195114 -5.9047837 -5.8717632 -5.7628536][-3.0161574 -3.4117222 -3.6529474 -3.2850866 -2.1434081 -0.52623415 1.0134821 1.282248 -0.11551476 -2.298902 -4.1038203 -5.0651693 -5.3787284 -5.4714041 -5.4443979][-3.4473281 -3.798948 -4.1578927 -3.9153059 -2.7929897 -1.0025275 0.90445995 1.573349 0.41536665 -1.6876066 -3.5691113 -4.6491022 -5.0947113 -5.2693281 -5.2572756][-4.4541545 -4.5944042 -4.8515491 -4.6887302 -3.809552 -2.2525501 -0.44725204 0.31140709 -0.6035192 -2.4023592 -4.0553131 -4.9904995 -5.3665509 -5.464797 -5.346168][-5.1107116 -4.9898372 -5.0600481 -4.9633284 -4.4618139 -3.3827689 -1.9866185 -1.3730505 -2.0874016 -3.5299065 -4.8798089 -5.6228437 -5.8662715 -5.8032331 -5.5134912][-5.2734594 -5.0026832 -4.9580646 -4.9217963 -4.7590795 -4.1624746 -3.2538261 -2.8449574 -3.3562098 -4.3907185 -5.3968096 -5.9703269 -6.1229897 -5.9364414 -5.5179348][-5.2006245 -4.9437122 -4.931849 -4.9937129 -5.0551777 -4.8077893 -4.3192616 -4.1052728 -4.4286962 -5.0474362 -5.6513715 -6.0003648 -6.0604472 -5.7995491 -5.3211846][-4.9669023 -4.7651715 -4.8264666 -4.9959135 -5.1980062 -5.1730766 -4.9839725 -4.9268513 -5.1347175 -5.444231 -5.7036009 -5.8300977 -5.7929363 -5.4878359 -4.9916315][-4.4279656 -4.2065382 -4.26733 -4.4793811 -4.7383447 -4.8210268 -4.8016686 -4.853662 -5.0205708 -5.1820807 -5.2663746 -5.285666 -5.2293081 -4.9741168 -4.5546584]]...]
INFO - root - 2017-12-06 07:11:50.028931: step 5310, loss = 1.27, batch loss = 1.20 (13.3 examples/sec; 0.603 sec/batch; 54h:48m:23s remains)
INFO - root - 2017-12-06 07:11:56.085403: step 5320, loss = 0.73, batch loss = 0.66 (14.3 examples/sec; 0.560 sec/batch; 50h:52m:11s remains)
INFO - root - 2017-12-06 07:12:01.969105: step 5330, loss = 0.98, batch loss = 0.91 (13.7 examples/sec; 0.586 sec/batch; 53h:14m:02s remains)
INFO - root - 2017-12-06 07:12:07.971809: step 5340, loss = 0.74, batch loss = 0.67 (13.5 examples/sec; 0.592 sec/batch; 53h:50m:11s remains)
INFO - root - 2017-12-06 07:12:14.028753: step 5350, loss = 0.98, batch loss = 0.91 (13.5 examples/sec; 0.592 sec/batch; 53h:49m:50s remains)
INFO - root - 2017-12-06 07:12:20.032684: step 5360, loss = 0.82, batch loss = 0.75 (13.2 examples/sec; 0.608 sec/batch; 55h:15m:42s remains)
INFO - root - 2017-12-06 07:12:25.950364: step 5370, loss = 0.79, batch loss = 0.72 (13.9 examples/sec; 0.577 sec/batch; 52h:25m:49s remains)
INFO - root - 2017-12-06 07:12:32.013508: step 5380, loss = 0.76, batch loss = 0.69 (13.1 examples/sec; 0.610 sec/batch; 55h:28m:20s remains)
INFO - root - 2017-12-06 07:12:37.992877: step 5390, loss = 0.92, batch loss = 0.85 (13.5 examples/sec; 0.591 sec/batch; 53h:41m:50s remains)
INFO - root - 2017-12-06 07:12:44.040694: step 5400, loss = 0.74, batch loss = 0.67 (13.4 examples/sec; 0.597 sec/batch; 54h:16m:21s remains)
2017-12-06 07:12:44.584549: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1052566 -4.1118841 -4.1396613 -4.1685143 -4.1211343 -4.1153088 -4.0368505 -3.9379625 -4.0101509 -4.1358433 -4.3156672 -4.4231296 -4.4909587 -4.4791207 -4.2965407][-4.3263192 -4.4537826 -4.6245551 -4.748795 -4.6983395 -4.6799941 -4.6114769 -4.550601 -4.69233 -4.8928881 -5.130362 -5.1789665 -5.0926576 -4.9029622 -4.5032711][-4.5710196 -4.7322121 -4.9741268 -5.1669078 -5.0868549 -5.00965 -4.9672322 -5.0306563 -5.3305883 -5.6736851 -5.9769812 -5.92548 -5.6326103 -5.2649026 -4.7009487][-4.7274737 -4.8427243 -5.0533705 -5.2289057 -5.0649896 -4.8904729 -4.860178 -5.0641575 -5.5987778 -6.1421747 -6.5159669 -6.33325 -5.8306665 -5.3284535 -4.7132287][-4.8332081 -4.9287777 -5.070436 -5.1377096 -4.8085594 -4.479846 -4.3712769 -4.6387815 -5.3969955 -6.1328211 -6.5659366 -6.2820177 -5.6527777 -5.0911803 -4.5142474][-4.8621807 -4.9782767 -5.0600667 -4.9862809 -4.4363766 -3.8526556 -3.4905853 -3.6454208 -4.5052433 -5.3788757 -5.9017715 -5.708725 -5.1789451 -4.7435365 -4.3006163][-4.6164093 -4.6993656 -4.7092028 -4.5384364 -3.8480341 -2.995131 -2.2846241 -2.2025609 -3.0354309 -3.999109 -4.68525 -4.8100686 -4.6093259 -4.4683743 -4.2217121][-4.1504359 -4.1449604 -4.0858083 -3.9112053 -3.2503886 -2.2744687 -1.2763388 -0.8994689 -1.6002343 -2.6222286 -3.5192637 -4.0402269 -4.224894 -4.3590941 -4.2578368][-3.8860044 -3.864069 -3.8286266 -3.7947209 -3.3726983 -2.5404916 -1.4946444 -0.86039758 -1.3125246 -2.2736664 -3.2741082 -4.0156193 -4.3764648 -4.5585446 -4.4342837][-4.0418358 -4.1252794 -4.2316203 -4.4285469 -4.3294792 -3.7997577 -2.9135041 -2.128161 -2.2509422 -2.985188 -3.8820283 -4.587142 -4.8800344 -4.9139776 -4.6459622][-4.37111 -4.5782504 -4.8524861 -5.2351832 -5.3390465 -5.0570326 -4.4062076 -3.6470861 -3.5311594 -3.9967587 -4.6736679 -5.1703138 -5.2383885 -5.0561838 -4.6572509][-4.6933074 -4.9587135 -5.3146181 -5.69394 -5.7637892 -5.5842052 -5.1695328 -4.6232409 -4.4846287 -4.7602506 -5.1947055 -5.4290891 -5.2716103 -4.9284515 -4.4849691][-4.9044504 -5.1407833 -5.4465609 -5.6672487 -5.5636454 -5.39762 -5.1861138 -4.9104028 -4.915029 -5.0918159 -5.2985029 -5.3151712 -5.0431147 -4.6650767 -4.2698135][-4.8912482 -5.0721745 -5.2808404 -5.3266311 -5.0484886 -4.8510189 -4.7509179 -4.6773272 -4.8551168 -4.9995437 -5.0354257 -4.9516144 -4.7275109 -4.4704709 -4.2077127][-4.6114025 -4.7411752 -4.9052024 -4.8995891 -4.5731788 -4.3218274 -4.2086139 -4.2088003 -4.4683442 -4.5953774 -4.531992 -4.4349303 -4.353848 -4.3212061 -4.2501845]]...]
INFO - root - 2017-12-06 07:12:50.588620: step 5410, loss = 1.15, batch loss = 1.08 (14.0 examples/sec; 0.570 sec/batch; 51h:47m:02s remains)
INFO - root - 2017-12-06 07:12:56.649003: step 5420, loss = 0.90, batch loss = 0.83 (13.2 examples/sec; 0.604 sec/batch; 54h:53m:26s remains)
INFO - root - 2017-12-06 07:13:02.722013: step 5430, loss = 0.83, batch loss = 0.76 (13.2 examples/sec; 0.606 sec/batch; 55h:01m:15s remains)
INFO - root - 2017-12-06 07:13:08.742830: step 5440, loss = 0.75, batch loss = 0.68 (13.5 examples/sec; 0.594 sec/batch; 53h:55m:29s remains)
INFO - root - 2017-12-06 07:13:14.825504: step 5450, loss = 1.02, batch loss = 0.95 (13.2 examples/sec; 0.608 sec/batch; 55h:16m:01s remains)
INFO - root - 2017-12-06 07:13:20.908688: step 5460, loss = 0.94, batch loss = 0.87 (12.7 examples/sec; 0.631 sec/batch; 57h:17m:07s remains)
INFO - root - 2017-12-06 07:13:26.778104: step 5470, loss = 0.95, batch loss = 0.88 (15.2 examples/sec; 0.526 sec/batch; 47h:46m:22s remains)
INFO - root - 2017-12-06 07:13:32.673952: step 5480, loss = 1.04, batch loss = 0.97 (13.2 examples/sec; 0.607 sec/batch; 55h:10m:13s remains)
INFO - root - 2017-12-06 07:13:38.701980: step 5490, loss = 0.75, batch loss = 0.68 (13.8 examples/sec; 0.579 sec/batch; 52h:36m:44s remains)
INFO - root - 2017-12-06 07:13:44.792663: step 5500, loss = 1.00, batch loss = 0.93 (12.8 examples/sec; 0.624 sec/batch; 56h:40m:02s remains)
2017-12-06 07:13:45.422499: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5815344 -5.4521933 -5.4192605 -5.2874808 -4.6042876 -3.8140016 -3.5105209 -3.4499831 -3.3822505 -3.4708898 -3.4090476 -3.0845127 -2.7353969 -2.4164548 -2.3128285][-5.2450876 -5.0129323 -4.931776 -4.7590089 -3.9968805 -3.121495 -2.7747321 -2.7458005 -2.7002723 -2.7669256 -2.6193857 -2.2405374 -2.0963869 -2.3076503 -2.73195][-4.520329 -4.2640491 -4.2293763 -4.1553807 -3.5253053 -2.7271609 -2.3775091 -2.3586781 -2.3321149 -2.358556 -2.11543 -1.6687624 -1.6474652 -2.26296 -3.1580555][-3.873085 -3.6962776 -3.7433846 -3.7982926 -3.3631258 -2.6774406 -2.2719486 -2.2064476 -2.2987635 -2.472105 -2.3044748 -1.8599966 -1.8465383 -2.5921414 -3.692858][-3.582871 -3.5359991 -3.6151154 -3.668458 -3.2716744 -2.5699775 -2.0002375 -1.8285983 -2.1317291 -2.6557846 -2.8005984 -2.5873818 -2.6567421 -3.3727188 -4.3966489][-3.7115932 -3.874115 -3.943357 -3.7545943 -3.081677 -2.1082911 -1.227603 -0.82877612 -1.2479088 -2.1630394 -2.8154192 -3.1197848 -3.4795909 -4.1825075 -4.9930973][-4.2898045 -4.684226 -4.7074518 -4.1088305 -2.9331679 -1.5408986 -0.30318356 0.38080359 -0.011522293 -1.1837721 -2.2894211 -3.1185772 -3.8583398 -4.6670876 -5.3226032][-4.8616471 -5.3808007 -5.348628 -4.4493914 -2.9579725 -1.4276311 -0.20096922 0.46622324 0.16012669 -0.90294337 -2.0302267 -3.0386395 -3.9777119 -4.8584781 -5.3934975][-5.0494146 -5.4643669 -5.3626556 -4.454113 -3.1109595 -1.9568248 -1.21611 -0.92013645 -1.1994216 -1.8914025 -2.6388879 -3.439605 -4.2801743 -5.0517125 -5.4205022][-5.0717573 -5.2515154 -5.0291543 -4.2411938 -3.3036337 -2.7425866 -2.6099069 -2.7485328 -3.0668855 -3.4394593 -3.7721071 -4.2224488 -4.7792687 -5.2874742 -5.4493237][-4.9688425 -4.9411244 -4.6341743 -3.9901533 -3.4128098 -3.3065143 -3.6034968 -4.0307937 -4.4199128 -4.6484165 -4.7369089 -4.9048638 -5.15919 -5.3821454 -5.3753271][-4.5858703 -4.5384617 -4.2877131 -3.7975585 -3.4345837 -3.5600898 -4.0383 -4.5486407 -4.9661379 -5.173039 -5.1918311 -5.1986709 -5.2367883 -5.2681117 -5.1918306][-4.2184343 -4.2628121 -4.1383352 -3.8077755 -3.5869784 -3.8020825 -4.29081 -4.7428637 -5.0964079 -5.2694016 -5.2486591 -5.1527953 -5.0640712 -5.0024538 -4.93521][-4.1092629 -4.2184591 -4.1911416 -4.0234666 -3.9453502 -4.1852517 -4.5906649 -4.9120765 -5.1524186 -5.2555394 -5.1731486 -5.0079079 -4.857275 -4.7719884 -4.7382021][-4.1229749 -4.2877846 -4.355423 -4.3402348 -4.3809776 -4.5982838 -4.8726692 -5.0358906 -5.1421676 -5.1588516 -5.0353861 -4.8600588 -4.7191734 -4.6548529 -4.6500449]]...]
INFO - root - 2017-12-06 07:13:51.460003: step 5510, loss = 1.00, batch loss = 0.93 (13.8 examples/sec; 0.582 sec/batch; 52h:49m:53s remains)
INFO - root - 2017-12-06 07:13:57.649858: step 5520, loss = 0.83, batch loss = 0.76 (13.1 examples/sec; 0.609 sec/batch; 55h:19m:39s remains)
INFO - root - 2017-12-06 07:14:03.671427: step 5530, loss = 0.80, batch loss = 0.73 (13.2 examples/sec; 0.607 sec/batch; 55h:05m:17s remains)
INFO - root - 2017-12-06 07:14:09.794591: step 5540, loss = 0.83, batch loss = 0.76 (13.0 examples/sec; 0.616 sec/batch; 55h:56m:34s remains)
INFO - root - 2017-12-06 07:14:15.893249: step 5550, loss = 1.03, batch loss = 0.96 (13.2 examples/sec; 0.606 sec/batch; 55h:00m:02s remains)
INFO - root - 2017-12-06 07:14:22.085723: step 5560, loss = 1.15, batch loss = 1.08 (12.6 examples/sec; 0.633 sec/batch; 57h:27m:35s remains)
INFO - root - 2017-12-06 07:14:28.214210: step 5570, loss = 0.65, batch loss = 0.58 (13.3 examples/sec; 0.599 sec/batch; 54h:25m:25s remains)
INFO - root - 2017-12-06 07:14:34.138904: step 5580, loss = 1.10, batch loss = 1.03 (12.7 examples/sec; 0.630 sec/batch; 57h:13m:46s remains)
INFO - root - 2017-12-06 07:14:40.275020: step 5590, loss = 0.90, batch loss = 0.83 (13.1 examples/sec; 0.611 sec/batch; 55h:27m:11s remains)
INFO - root - 2017-12-06 07:14:46.392139: step 5600, loss = 0.85, batch loss = 0.78 (13.9 examples/sec; 0.577 sec/batch; 52h:24m:12s remains)
2017-12-06 07:14:46.992185: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8074834 -3.7556272 -3.6181846 -3.75916 -4.0404282 -4.2028856 -4.4029222 -4.6099682 -4.7207336 -4.6244988 -4.4352221 -4.1651249 -4.074203 -4.248764 -4.4921832][-4.1538591 -4.1052032 -3.9880819 -4.0666647 -4.2878509 -4.4718094 -4.7019286 -4.9465432 -5.0279479 -4.8253551 -4.5512948 -4.2125287 -4.0766006 -4.2771678 -4.5103264][-4.4291592 -4.3602448 -4.2619572 -4.2808723 -4.4286518 -4.6196918 -4.8503051 -5.1045842 -5.1790905 -4.9282126 -4.6377525 -4.3665724 -4.2915416 -4.4926581 -4.6592073][-4.4908319 -4.4003291 -4.3271141 -4.31247 -4.3702335 -4.5242391 -4.7108335 -4.9285722 -5.0186443 -4.7998137 -4.5907049 -4.5103264 -4.5623264 -4.6876993 -4.6772604][-4.6015224 -4.52099 -4.5202284 -4.5169096 -4.4544997 -4.4554358 -4.4500504 -4.5031953 -4.6171846 -4.5591598 -4.5677943 -4.767745 -4.9643464 -4.9441919 -4.6673217][-4.676713 -4.6414165 -4.7758484 -4.8353724 -4.6113029 -4.309052 -3.9062636 -3.6043169 -3.7159832 -3.9618061 -4.3528018 -4.9108243 -5.2809138 -5.1835661 -4.7236991][-4.4020648 -4.420433 -4.689527 -4.8165112 -4.4176588 -3.7525735 -2.8693352 -2.1409168 -2.2699008 -2.9126072 -3.7124591 -4.56827 -5.0892282 -5.0498991 -4.6242442][-4.047998 -4.0877781 -4.3860612 -4.5303855 -4.0111108 -3.0676069 -1.8041449 -0.71438694 -0.86568928 -1.8664591 -2.9723682 -3.9625793 -4.5360317 -4.6024795 -4.3783426][-4.0369749 -4.0669603 -4.275723 -4.3978271 -3.9097807 -2.9543066 -1.684659 -0.54925489 -0.67941427 -1.7384861 -2.8420424 -3.7250633 -4.2199955 -4.3451591 -4.3451619][-4.2496834 -4.233717 -4.2979546 -4.4153438 -4.125237 -3.4166322 -2.4867365 -1.6463938 -1.7290971 -2.5255916 -3.329735 -3.9407916 -4.289876 -4.4310932 -4.5978656][-4.3777461 -4.2899961 -4.1913753 -4.3103762 -4.265049 -3.875742 -3.3505809 -2.858397 -2.8997707 -3.372015 -3.82277 -4.1643562 -4.4002585 -4.5804577 -4.8687015][-4.3138204 -4.200407 -4.0125122 -4.1337814 -4.271574 -4.124536 -3.9036639 -3.675838 -3.6903009 -3.8800871 -4.0350642 -4.1702929 -4.3305755 -4.5679131 -4.9458303][-4.1386242 -4.0735927 -3.88747 -3.99992 -4.2026267 -4.1717315 -4.10285 -4.0157294 -4.003602 -4.0057621 -3.9748425 -3.9726102 -4.0813293 -4.3587894 -4.7891097][-4.1049709 -4.07828 -3.9387314 -4.0138111 -4.1894183 -4.1954322 -4.1811929 -4.1555777 -4.117167 -4.0248713 -3.9285579 -3.8783138 -3.9666719 -4.2532754 -4.66937][-4.3007178 -4.2697754 -4.1715531 -4.2044649 -4.320612 -4.3352532 -4.338707 -4.3346677 -4.2914686 -4.1951694 -4.1278005 -4.1050305 -4.20579 -4.4685493 -4.7993903]]...]
INFO - root - 2017-12-06 07:14:52.985691: step 5610, loss = 0.95, batch loss = 0.88 (12.8 examples/sec; 0.626 sec/batch; 56h:52m:04s remains)
INFO - root - 2017-12-06 07:14:59.019231: step 5620, loss = 0.86, batch loss = 0.78 (13.9 examples/sec; 0.576 sec/batch; 52h:15m:58s remains)
INFO - root - 2017-12-06 07:15:04.922499: step 5630, loss = 0.86, batch loss = 0.79 (13.3 examples/sec; 0.603 sec/batch; 54h:46m:46s remains)
INFO - root - 2017-12-06 07:15:11.037775: step 5640, loss = 0.82, batch loss = 0.75 (13.1 examples/sec; 0.611 sec/batch; 55h:27m:38s remains)
INFO - root - 2017-12-06 07:15:17.081977: step 5650, loss = 0.86, batch loss = 0.79 (13.6 examples/sec; 0.587 sec/batch; 53h:18m:22s remains)
INFO - root - 2017-12-06 07:15:23.220331: step 5660, loss = 1.01, batch loss = 0.94 (13.3 examples/sec; 0.602 sec/batch; 54h:40m:14s remains)
INFO - root - 2017-12-06 07:15:29.274178: step 5670, loss = 0.96, batch loss = 0.89 (13.4 examples/sec; 0.596 sec/batch; 54h:07m:19s remains)
INFO - root - 2017-12-06 07:15:35.349114: step 5680, loss = 0.96, batch loss = 0.89 (12.9 examples/sec; 0.619 sec/batch; 56h:12m:06s remains)
INFO - root - 2017-12-06 07:15:41.282809: step 5690, loss = 0.68, batch loss = 0.61 (13.3 examples/sec; 0.604 sec/batch; 54h:48m:33s remains)
INFO - root - 2017-12-06 07:15:47.330377: step 5700, loss = 0.77, batch loss = 0.70 (13.2 examples/sec; 0.607 sec/batch; 55h:07m:27s remains)
2017-12-06 07:15:47.923454: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3130341 -2.9678907 -2.6380424 -2.2674887 -2.0592918 -2.3153226 -2.6828623 -3.0642865 -3.4381907 -3.5863104 -3.6546423 -3.5174811 -2.957078 -2.14958 -1.8916731][-3.4196804 -3.0478392 -2.6929293 -2.2001505 -1.7884333 -1.77476 -1.9058654 -2.1056149 -2.3466365 -2.515645 -2.6638093 -2.5910487 -2.1162629 -1.4506135 -1.2763536][-3.5905292 -3.2877934 -3.0217576 -2.5539489 -2.049149 -1.811693 -1.7175386 -1.6876945 -1.7073083 -1.796742 -1.963012 -1.9562967 -1.6189113 -1.1658137 -1.1058152][-3.7533047 -3.5951374 -3.5187633 -3.2389054 -2.8175151 -2.5026269 -2.2755544 -2.0317013 -1.8086472 -1.7490611 -1.8356819 -1.8267131 -1.5847232 -1.3078082 -1.3396125][-3.8273907 -3.811832 -3.9447429 -3.9219596 -3.6756897 -3.4142046 -3.1898956 -2.8342261 -2.4479978 -2.2371209 -2.20104 -2.1492069 -1.9766388 -1.8376594 -1.948416][-3.8168957 -3.8567007 -4.0904503 -4.2316508 -4.1003003 -3.9228277 -3.8028431 -3.4574893 -3.0279808 -2.7449861 -2.6582561 -2.6124263 -2.5370328 -2.5455074 -2.7378068][-3.7725015 -3.7341447 -3.8965781 -4.0469766 -3.9343746 -3.8155253 -3.8014653 -3.4948423 -3.0670023 -2.7666092 -2.7034779 -2.7332692 -2.8140512 -3.0091419 -3.307622][-3.7382448 -3.5355372 -3.53509 -3.6111712 -3.4568973 -3.330667 -3.3135824 -2.9737551 -2.5160036 -2.2118545 -2.21005 -2.3653989 -2.654902 -3.0601015 -3.482857][-3.7634807 -3.4269166 -3.30882 -3.3237422 -3.1222353 -2.936038 -2.8184609 -2.3791242 -1.8674223 -1.5773578 -1.6426516 -1.9168665 -2.3856616 -2.935195 -3.4380679][-3.8077202 -3.4454117 -3.3194313 -3.3317986 -3.1136291 -2.8628886 -2.6345074 -2.1180921 -1.5929973 -1.3521364 -1.4551373 -1.7801292 -2.3251414 -2.8898408 -3.3825383][-3.8147061 -3.5172741 -3.4687314 -3.5509598 -3.3842812 -3.1177437 -2.829083 -2.2990263 -1.8222072 -1.6431987 -1.7381961 -2.045989 -2.5732331 -3.0629127 -3.4699373][-3.7924089 -3.5865436 -3.6376774 -3.8391988 -3.780807 -3.5661786 -3.3001459 -2.8308544 -2.4374585 -2.3021061 -2.3581083 -2.6107311 -3.0488956 -3.4056931 -3.6712248][-3.7578804 -3.6136756 -3.729156 -4.0278254 -4.0734143 -3.9433496 -3.7463658 -3.3593638 -3.0501618 -2.9494803 -2.9779375 -3.1700873 -3.490736 -3.7069829 -3.828135][-3.7578816 -3.6240561 -3.7261906 -4.040575 -4.1394997 -4.085772 -3.9539237 -3.6327934 -3.4022374 -3.3481932 -3.3729048 -3.5083897 -3.7252619 -3.8455205 -3.8686459][-3.8240747 -3.6691473 -3.7139442 -3.9879956 -4.0886483 -4.0700755 -3.9681113 -3.6878576 -3.5193295 -3.5084271 -3.5391145 -3.6333895 -3.7903988 -3.8702013 -3.8475568]]...]
INFO - root - 2017-12-06 07:15:53.975633: step 5710, loss = 0.83, batch loss = 0.76 (13.2 examples/sec; 0.607 sec/batch; 55h:05m:21s remains)
INFO - root - 2017-12-06 07:15:59.934762: step 5720, loss = 0.78, batch loss = 0.71 (13.5 examples/sec; 0.591 sec/batch; 53h:38m:42s remains)
INFO - root - 2017-12-06 07:16:06.083568: step 5730, loss = 0.88, batch loss = 0.81 (12.6 examples/sec; 0.635 sec/batch; 57h:35m:42s remains)
INFO - root - 2017-12-06 07:16:12.187298: step 5740, loss = 1.03, batch loss = 0.96 (13.0 examples/sec; 0.617 sec/batch; 55h:59m:01s remains)
INFO - root - 2017-12-06 07:16:18.326109: step 5750, loss = 1.02, batch loss = 0.95 (13.1 examples/sec; 0.609 sec/batch; 55h:18m:54s remains)
INFO - root - 2017-12-06 07:16:24.412010: step 5760, loss = 1.22, batch loss = 1.15 (13.3 examples/sec; 0.600 sec/batch; 54h:28m:11s remains)
INFO - root - 2017-12-06 07:16:30.540697: step 5770, loss = 1.03, batch loss = 0.96 (12.9 examples/sec; 0.621 sec/batch; 56h:19m:56s remains)
INFO - root - 2017-12-06 07:16:36.495405: step 5780, loss = 0.99, batch loss = 0.92 (13.6 examples/sec; 0.590 sec/batch; 53h:31m:23s remains)
INFO - root - 2017-12-06 07:16:42.462391: step 5790, loss = 1.09, batch loss = 1.02 (14.7 examples/sec; 0.544 sec/batch; 49h:24m:36s remains)
INFO - root - 2017-12-06 07:16:48.500690: step 5800, loss = 0.79, batch loss = 0.72 (13.2 examples/sec; 0.605 sec/batch; 54h:53m:38s remains)
2017-12-06 07:16:49.086588: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4880016 -3.0173349 -2.7523346 -2.9827037 -3.6136303 -4.5417795 -5.404767 -5.7576971 -5.4351597 -4.5784407 -3.5890117 -2.8945069 -2.6681504 -2.6081851 -2.7269616][-3.2043862 -3.1245298 -3.2026372 -3.4585059 -3.8914177 -4.6476378 -5.4354367 -5.7381935 -5.4169741 -4.7464633 -4.0444326 -3.5595279 -3.4172931 -3.3709664 -3.491188][-3.3627164 -3.5204141 -3.8496151 -4.0127239 -4.0470147 -4.4212251 -5.0260139 -5.3107524 -5.0377483 -4.5851984 -4.2396812 -4.0330892 -4.0185323 -3.9811065 -4.0462327][-3.6893132 -3.8439415 -4.1977992 -4.1102481 -3.6177702 -3.5393143 -3.9873731 -4.363874 -4.1902413 -3.8764896 -3.819922 -3.9248085 -4.0794082 -4.0371466 -3.9418855][-3.9576621 -3.9472501 -4.1655636 -3.8292177 -2.8694572 -2.3604312 -2.655066 -3.1908245 -3.1951346 -2.956852 -3.1138306 -3.538312 -3.9083521 -3.8809755 -3.5656514][-4.197753 -3.9392872 -3.9221306 -3.3878448 -2.0894568 -1.1730392 -1.2528861 -1.9601142 -2.2558057 -2.147985 -2.4829268 -3.2092605 -3.8672304 -3.9892035 -3.5691452][-4.355113 -3.9289451 -3.7217929 -3.1290936 -1.7259362 -0.45629549 -0.12686539 -0.82369709 -1.448494 -1.581214 -2.0997064 -3.0327718 -3.94549 -4.299746 -3.952796][-4.4739594 -3.9714141 -3.6817126 -3.2181678 -2.026381 -0.70932794 -0.0018577576 -0.45321655 -1.2384784 -1.6306715 -2.2760427 -3.2107863 -4.1502838 -4.6156983 -4.4138365][-4.7619643 -4.2522006 -3.9885468 -3.7063212 -2.8416257 -1.7369814 -0.88278246 -1.0476739 -1.8311269 -2.4299691 -3.1540494 -3.9159725 -4.6134062 -4.9644594 -4.813705][-5.171802 -4.7399573 -4.5703087 -4.4198575 -3.7884183 -2.9481668 -2.1369789 -2.0920193 -2.7531598 -3.44003 -4.19213 -4.7527866 -5.1108723 -5.2225223 -5.0239239][-5.494215 -5.2165613 -5.1750221 -5.1485534 -4.7180357 -4.1316929 -3.4695539 -3.2848644 -3.7016654 -4.2800078 -4.9379997 -5.3223462 -5.4381413 -5.37822 -5.1486111][-5.5467124 -5.4249406 -5.464591 -5.5445919 -5.3466606 -5.0356355 -4.6105433 -4.4049306 -4.5789127 -4.9278822 -5.3881292 -5.61905 -5.5939531 -5.4368811 -5.17361][-5.1997852 -5.1831212 -5.2435455 -5.3677955 -5.3546906 -5.2740455 -5.1041527 -4.994318 -5.0340881 -5.1700811 -5.4175792 -5.5354276 -5.4844794 -5.3304873 -5.0828991][-4.7477641 -4.7752738 -4.8299723 -4.9609241 -5.0576 -5.1251931 -5.1431794 -5.1429644 -5.1434326 -5.1482806 -5.2404981 -5.2933741 -5.25937 -5.1478543 -4.9525547][-4.368413 -4.4013329 -4.4453936 -4.543448 -4.6524844 -4.7596006 -4.8540049 -4.91263 -4.9169207 -4.8845444 -4.905 -4.920866 -4.891849 -4.812191 -4.682189]]...]
INFO - root - 2017-12-06 07:16:55.158938: step 5810, loss = 0.91, batch loss = 0.84 (13.4 examples/sec; 0.596 sec/batch; 54h:04m:34s remains)
INFO - root - 2017-12-06 07:17:01.281782: step 5820, loss = 1.16, batch loss = 1.09 (13.8 examples/sec; 0.578 sec/batch; 52h:25m:19s remains)
INFO - root - 2017-12-06 07:17:07.370409: step 5830, loss = 0.82, batch loss = 0.75 (13.1 examples/sec; 0.610 sec/batch; 55h:18m:48s remains)
INFO - root - 2017-12-06 07:17:13.444372: step 5840, loss = 1.06, batch loss = 0.99 (13.1 examples/sec; 0.609 sec/batch; 55h:14m:57s remains)
INFO - root - 2017-12-06 07:17:19.462343: step 5850, loss = 0.87, batch loss = 0.80 (13.5 examples/sec; 0.591 sec/batch; 53h:38m:09s remains)
INFO - root - 2017-12-06 07:17:25.542409: step 5860, loss = 1.06, batch loss = 0.99 (13.0 examples/sec; 0.614 sec/batch; 55h:42m:55s remains)
INFO - root - 2017-12-06 07:17:31.592508: step 5870, loss = 0.83, batch loss = 0.76 (13.4 examples/sec; 0.597 sec/batch; 54h:10m:06s remains)
INFO - root - 2017-12-06 07:17:37.661160: step 5880, loss = 0.88, batch loss = 0.81 (12.8 examples/sec; 0.627 sec/batch; 56h:52m:02s remains)
INFO - root - 2017-12-06 07:17:43.794771: step 5890, loss = 0.73, batch loss = 0.66 (12.7 examples/sec; 0.630 sec/batch; 57h:11m:53s remains)
INFO - root - 2017-12-06 07:17:49.824631: step 5900, loss = 0.90, batch loss = 0.83 (13.2 examples/sec; 0.607 sec/batch; 55h:02m:02s remains)
2017-12-06 07:17:50.464652: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6821241 -4.8295579 -5.0686545 -5.2212481 -5.3939114 -5.3072023 -4.8886557 -4.843215 -5.0980716 -5.3792033 -5.4954076 -5.4530735 -5.2857218 -5.0408812 -4.78168][-4.4412546 -4.6379 -4.9707618 -5.2372737 -5.5515585 -5.5765934 -5.1447768 -5.1141081 -5.446938 -5.7994814 -5.8786278 -5.6516452 -5.2520108 -4.8092384 -4.4673147][-4.0404887 -4.2970834 -4.7214479 -5.14109 -5.5712829 -5.6484013 -5.163938 -5.0773687 -5.456089 -5.89525 -5.9776373 -5.6292634 -5.0219674 -4.3606734 -3.8983834][-3.4439211 -3.855135 -4.4415722 -5.0461097 -5.4796023 -5.4308891 -4.8101907 -4.629498 -5.03414 -5.5584154 -5.7132297 -5.3737965 -4.6569977 -3.7754374 -3.0855582][-2.7035947 -3.3305891 -4.0969481 -4.8236628 -5.1136446 -4.7334251 -3.867579 -3.5829883 -4.0343542 -4.6895304 -5.015717 -4.8371124 -4.1616306 -3.1379557 -2.1730893][-2.2117512 -2.957319 -3.7654214 -4.440805 -4.4547167 -3.5923748 -2.3819785 -1.9387238 -2.439363 -3.2902954 -3.9157023 -4.0757575 -3.6366823 -2.6826208 -1.5785096][-2.2792747 -2.9870534 -3.6832473 -4.1494818 -3.8434913 -2.5305066 -0.9775939 -0.34578466 -0.85820508 -1.8841949 -2.8261986 -3.3723969 -3.2607474 -2.5543938 -1.5293097][-2.7828794 -3.3526497 -3.8896453 -4.1391149 -3.6768491 -2.2211916 -0.5705905 0.1499424 -0.31009007 -1.3422225 -2.4231343 -3.1856222 -3.3047037 -2.8240416 -1.9601955][-3.2888932 -3.6912551 -4.136652 -4.31131 -3.9801896 -2.7908416 -1.3698871 -0.74013829 -1.1290996 -1.9939508 -2.9419684 -3.6367702 -3.7678401 -3.3229814 -2.5377803][-3.6095693 -3.8739858 -4.3023763 -4.5125833 -4.4391022 -3.7008162 -2.6712329 -2.2232525 -2.5532584 -3.2208014 -3.8923626 -4.3285894 -4.3050222 -3.7467694 -2.9803178][-3.9093151 -4.0644579 -4.4783192 -4.7273178 -4.802474 -4.4156179 -3.7276442 -3.4756565 -3.7925315 -4.3365736 -4.7272987 -4.838191 -4.6190171 -3.977845 -3.3266757][-4.3339467 -4.4141545 -4.7346649 -4.9270129 -4.9617543 -4.7135429 -4.2460513 -4.1751461 -4.5498781 -5.0344529 -5.1833954 -5.04162 -4.7149591 -4.1319666 -3.7174251][-4.6598496 -4.7541447 -4.9493661 -5.0011559 -4.8814611 -4.603961 -4.2580652 -4.3355885 -4.7936831 -5.2391171 -5.2195983 -4.9591384 -4.6208391 -4.1654568 -3.9787984][-4.7849059 -4.9647384 -5.0701594 -4.9683986 -4.7027555 -4.3652515 -4.0869336 -4.239459 -4.7296858 -5.1292224 -5.0318184 -4.7518907 -4.4532604 -4.1277184 -4.08344][-4.8344836 -5.0326262 -5.0536361 -4.8432226 -4.4855356 -4.1194792 -3.9028823 -4.072834 -4.5232005 -4.8581934 -4.7706738 -4.5555038 -4.3394008 -4.1528926 -4.1951227]]...]
INFO - root - 2017-12-06 07:17:56.530753: step 5910, loss = 0.85, batch loss = 0.78 (12.8 examples/sec; 0.624 sec/batch; 56h:35m:50s remains)
INFO - root - 2017-12-06 07:18:02.576596: step 5920, loss = 0.86, batch loss = 0.79 (13.7 examples/sec; 0.584 sec/batch; 52h:57m:29s remains)
INFO - root - 2017-12-06 07:18:08.499946: step 5930, loss = 0.95, batch loss = 0.88 (13.2 examples/sec; 0.607 sec/batch; 55h:06m:24s remains)
INFO - root - 2017-12-06 07:18:14.660818: step 5940, loss = 0.90, batch loss = 0.83 (13.2 examples/sec; 0.608 sec/batch; 55h:09m:42s remains)
INFO - root - 2017-12-06 07:18:20.690326: step 5950, loss = 0.96, batch loss = 0.89 (13.6 examples/sec; 0.587 sec/batch; 53h:14m:42s remains)
INFO - root - 2017-12-06 07:18:26.708515: step 5960, loss = 0.91, batch loss = 0.84 (12.8 examples/sec; 0.627 sec/batch; 56h:50m:42s remains)
INFO - root - 2017-12-06 07:18:32.767851: step 5970, loss = 1.01, batch loss = 0.94 (13.4 examples/sec; 0.596 sec/batch; 54h:01m:10s remains)
INFO - root - 2017-12-06 07:18:38.850919: step 5980, loss = 0.90, batch loss = 0.83 (13.2 examples/sec; 0.608 sec/batch; 55h:06m:57s remains)
INFO - root - 2017-12-06 07:18:44.888563: step 5990, loss = 1.03, batch loss = 0.96 (12.9 examples/sec; 0.618 sec/batch; 56h:02m:37s remains)
INFO - root - 2017-12-06 07:18:50.975869: step 6000, loss = 0.86, batch loss = 0.79 (13.7 examples/sec; 0.583 sec/batch; 52h:51m:00s remains)
2017-12-06 07:18:51.595009: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8413439 -4.9176536 -5.0503736 -5.2354159 -5.3966541 -5.4645128 -5.4337444 -5.365468 -5.3435206 -5.3843193 -5.4616809 -5.5235491 -5.5040879 -5.3693023 -5.1339374][-4.930881 -5.1315422 -5.3951521 -5.63587 -5.7372937 -5.6492944 -5.4488592 -5.2922325 -5.2834682 -5.4127178 -5.5950394 -5.7141466 -5.6896806 -5.519598 -5.2339621][-5.1080127 -5.4295688 -5.752039 -5.9313869 -5.8734741 -5.5857525 -5.2056179 -4.9670882 -4.9925752 -5.2533665 -5.5857248 -5.7858658 -5.7718973 -5.5776334 -5.2612586][-5.3404908 -5.7294564 -6.0136738 -6.022222 -5.703702 -5.143096 -4.5611348 -4.2672715 -4.3895063 -4.8354387 -5.3492284 -5.6536679 -5.6932888 -5.5189557 -5.2059903][-5.6107435 -5.9813337 -6.1191173 -5.8350205 -5.1311502 -4.2078447 -3.4205837 -3.144206 -3.4482727 -4.1205215 -4.8439989 -5.3061423 -5.4793191 -5.3911133 -5.119576][-5.8639097 -6.1338453 -6.045433 -5.4082541 -4.2874708 -2.993963 -2.0268967 -1.8365574 -2.3722143 -3.2791858 -4.2411771 -4.9295568 -5.2889466 -5.3015318 -5.0727749][-6.0583372 -6.1821547 -5.8289804 -4.8432508 -3.3799744 -1.8066871 -0.71545506 -0.62359 -1.3448477 -2.4394283 -3.6345203 -4.5924811 -5.1583347 -5.2521129 -5.0559921][-6.1216669 -6.1419082 -5.6000514 -4.386519 -2.7480671 -1.0498478 0.091468334 0.1027956 -0.7247386 -1.9281347 -3.2811036 -4.4403834 -5.1412926 -5.2455711 -5.0460005][-6.097672 -6.1256523 -5.5543427 -4.3044033 -2.6840668 -1.018141 0.086891651 0.031263351 -0.84222245 -2.0548825 -3.4015563 -4.589479 -5.2737951 -5.2891059 -5.03465][-5.908534 -6.0520344 -5.6038537 -4.4885092 -3.0604129 -1.6110241 -0.66593289 -0.75504732 -1.6082478 -2.763361 -3.992861 -5.0513115 -5.5633755 -5.3991389 -5.0369697][-5.4555798 -5.80634 -5.6344004 -4.8274894 -3.7514153 -2.6748364 -1.9825287 -2.0652747 -2.8279667 -3.881753 -4.9358015 -5.7436681 -5.9632416 -5.5743856 -5.0865917][-4.8944774 -5.543004 -5.7474394 -5.3350286 -4.6363592 -3.9219978 -3.4890232 -3.5849435 -4.23504 -5.1328611 -5.9340787 -6.4042163 -6.3140812 -5.7576218 -5.1931496][-4.4557285 -5.4828858 -6.0486097 -5.9559669 -5.5198579 -5.037488 -4.7896686 -4.9317284 -5.4976635 -6.2027626 -6.6994162 -6.8223019 -6.5009727 -5.8900638 -5.3245687][-4.4134016 -5.780055 -6.523427 -6.527564 -6.1720085 -5.7720842 -5.6159372 -5.8012576 -6.2996888 -6.8182087 -7.0499287 -6.922471 -6.4921656 -5.9144621 -5.4058433][-4.8069291 -6.2965689 -6.955121 -6.8132696 -6.3482175 -5.8953085 -5.749958 -5.978591 -6.4668341 -6.9007196 -7.0101714 -6.7923193 -6.3603668 -5.8416014 -5.4010344]]...]
INFO - root - 2017-12-06 07:18:57.507262: step 6010, loss = 1.08, batch loss = 1.01 (13.3 examples/sec; 0.604 sec/batch; 54h:45m:00s remains)
INFO - root - 2017-12-06 07:19:03.625895: step 6020, loss = 1.15, batch loss = 1.08 (13.8 examples/sec; 0.579 sec/batch; 52h:31m:20s remains)
INFO - root - 2017-12-06 07:19:09.784413: step 6030, loss = 0.80, batch loss = 0.73 (12.7 examples/sec; 0.631 sec/batch; 57h:12m:02s remains)
INFO - root - 2017-12-06 07:19:15.899842: step 6040, loss = 1.06, batch loss = 0.99 (13.2 examples/sec; 0.604 sec/batch; 54h:45m:31s remains)
INFO - root - 2017-12-06 07:19:21.943758: step 6050, loss = 1.00, batch loss = 0.93 (14.0 examples/sec; 0.571 sec/batch; 51h:46m:22s remains)
INFO - root - 2017-12-06 07:19:28.002079: step 6060, loss = 1.00, batch loss = 0.93 (13.3 examples/sec; 0.601 sec/batch; 54h:27m:57s remains)
INFO - root - 2017-12-06 07:19:34.039533: step 6070, loss = 0.80, batch loss = 0.73 (13.1 examples/sec; 0.613 sec/batch; 55h:32m:23s remains)
INFO - root - 2017-12-06 07:19:40.093169: step 6080, loss = 1.14, batch loss = 1.07 (14.0 examples/sec; 0.570 sec/batch; 51h:39m:27s remains)
INFO - root - 2017-12-06 07:19:46.030239: step 6090, loss = 0.88, batch loss = 0.81 (13.7 examples/sec; 0.583 sec/batch; 52h:50m:55s remains)
INFO - root - 2017-12-06 07:19:52.119776: step 6100, loss = 1.00, batch loss = 0.93 (12.9 examples/sec; 0.619 sec/batch; 56h:06m:27s remains)
2017-12-06 07:19:52.736758: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.862515 -4.4577026 -5.1576123 -5.6669407 -5.8263893 -5.69777 -5.4282856 -5.1377492 -4.9202666 -4.8981194 -5.0732756 -5.2888379 -5.4011941 -5.3449745 -5.2795529][-3.9249523 -4.6294065 -5.4103489 -5.9250994 -6.0213675 -5.8218107 -5.4920945 -5.1406364 -4.8626051 -4.8123736 -5.0600591 -5.4620161 -5.7643905 -5.7650728 -5.7320442][-3.9397802 -4.6229048 -5.3145323 -5.68337 -5.6710196 -5.4691496 -5.1972466 -4.9174156 -4.7052007 -4.6876268 -4.951756 -5.4184628 -5.8294363 -5.8720865 -5.8839474][-3.9422669 -4.5588784 -5.0974007 -5.2625132 -5.1362476 -4.9800329 -4.8329592 -4.6973448 -4.6457162 -4.7273874 -4.9521518 -5.3063192 -5.6520662 -5.6673255 -5.6852927][-3.9655817 -4.528276 -4.931613 -4.9133086 -4.6524267 -4.4855423 -4.4294834 -4.4283547 -4.5556068 -4.7782321 -4.9827847 -5.1985989 -5.4162574 -5.4000134 -5.4271727][-4.0176406 -4.482976 -4.6905093 -4.4450274 -3.9935367 -3.7415156 -3.7552876 -3.904459 -4.2212672 -4.6010995 -4.8289413 -4.9449697 -4.9751821 -4.8178997 -4.7289705][-4.1074896 -4.4450722 -4.4350557 -3.9615564 -3.2856393 -2.8551593 -2.8227913 -3.0401006 -3.5179079 -4.1054864 -4.5067649 -4.6939297 -4.5760145 -4.1750588 -3.8211088][-4.2448964 -4.4902349 -4.3280659 -3.7020116 -2.8179209 -2.1012878 -1.8099055 -1.8679705 -2.365886 -3.1637149 -3.9481757 -4.5207806 -4.5583215 -4.1390343 -3.6517112][-4.5406837 -4.8246636 -4.6560779 -3.9959512 -2.9673581 -1.9548395 -1.2827528 -1.0020766 -1.3273733 -2.1621571 -3.2636375 -4.2566304 -4.58819 -4.3556695 -3.9599204][-4.9866419 -5.4344025 -5.3915691 -4.8263059 -3.802866 -2.6560018 -1.6847346 -1.0304523 -1.0323894 -1.6549382 -2.7708886 -3.9156184 -4.4093628 -4.3670883 -4.1403193][-5.4108224 -6.0198355 -6.1403737 -5.7302618 -4.8424425 -3.7394955 -2.6184888 -1.6860108 -1.3928635 -1.766037 -2.7419722 -3.8035419 -4.2859535 -4.3472486 -4.2687879][-5.6686411 -6.3679571 -6.638371 -6.4092927 -5.7441082 -4.8069282 -3.672128 -2.6059399 -2.140974 -2.3603997 -3.1802955 -4.0491486 -4.4036489 -4.4495072 -4.4167109][-5.6781387 -6.3745475 -6.7482939 -6.7039013 -6.3052807 -5.6161346 -4.6353507 -3.6541591 -3.18424 -3.3334463 -3.9900484 -4.6234961 -4.81315 -4.7777939 -4.7118373][-5.4297924 -6.0629654 -6.4859123 -6.5871096 -6.4211893 -6.003767 -5.3087859 -4.5893364 -4.236207 -4.3396997 -4.7892323 -5.1670089 -5.2095642 -5.1093745 -5.0107303][-4.9156489 -5.4400959 -5.86362 -6.0630279 -6.0542707 -5.8419356 -5.4157286 -4.9723406 -4.7583895 -4.8343935 -5.1052217 -5.2895393 -5.2557316 -5.1383953 -5.026947]]...]
INFO - root - 2017-12-06 07:19:58.665049: step 6110, loss = 1.05, batch loss = 0.98 (16.0 examples/sec; 0.500 sec/batch; 45h:21m:57s remains)
INFO - root - 2017-12-06 07:20:04.699824: step 6120, loss = 1.18, batch loss = 1.11 (13.6 examples/sec; 0.586 sec/batch; 53h:09m:38s remains)
INFO - root - 2017-12-06 07:20:10.797440: step 6130, loss = 0.75, batch loss = 0.68 (13.0 examples/sec; 0.614 sec/batch; 55h:41m:16s remains)
INFO - root - 2017-12-06 07:20:16.868336: step 6140, loss = 1.14, batch loss = 1.07 (13.4 examples/sec; 0.598 sec/batch; 54h:12m:56s remains)
INFO - root - 2017-12-06 07:20:22.847336: step 6150, loss = 0.86, batch loss = 0.78 (13.1 examples/sec; 0.610 sec/batch; 55h:20m:04s remains)
INFO - root - 2017-12-06 07:20:28.989872: step 6160, loss = 0.88, batch loss = 0.81 (12.6 examples/sec; 0.635 sec/batch; 57h:31m:30s remains)
INFO - root - 2017-12-06 07:20:35.033600: step 6170, loss = 0.87, batch loss = 0.80 (13.7 examples/sec; 0.585 sec/batch; 53h:01m:50s remains)
INFO - root - 2017-12-06 07:20:41.136876: step 6180, loss = 1.14, batch loss = 1.07 (13.1 examples/sec; 0.609 sec/batch; 55h:12m:37s remains)
INFO - root - 2017-12-06 07:20:47.286942: step 6190, loss = 1.08, batch loss = 1.01 (12.4 examples/sec; 0.648 sec/batch; 58h:41m:29s remains)
INFO - root - 2017-12-06 07:20:53.340625: step 6200, loss = 0.73, batch loss = 0.66 (13.4 examples/sec; 0.598 sec/batch; 54h:12m:24s remains)
2017-12-06 07:20:54.030897: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0135159 -4.0384035 -4.1776414 -4.2559004 -4.3062038 -4.31744 -4.1765542 -3.8451455 -3.4373839 -3.3236635 -3.6399767 -4.1339221 -4.434999 -4.522315 -4.570189][-3.9858272 -4.0971351 -4.3374 -4.4481926 -4.4056177 -4.286016 -4.0849214 -3.7350352 -3.3746693 -3.3452241 -3.7308555 -4.2802992 -4.562057 -4.5880961 -4.6708574][-4.191412 -4.3257136 -4.5359936 -4.6192927 -4.5101476 -4.2684064 -4.0056267 -3.6808805 -3.4204965 -3.4583077 -3.8299239 -4.3588214 -4.6423006 -4.6608248 -4.7670617][-4.5323882 -4.6915407 -4.8292246 -4.8405504 -4.6200905 -4.1994081 -3.8214955 -3.5437388 -3.4338315 -3.5548203 -3.8747969 -4.3173714 -4.5873156 -4.5966554 -4.6634355][-4.7843142 -5.0067468 -5.1377549 -5.0933175 -4.7182412 -4.0388303 -3.4225287 -3.0993214 -3.1142511 -3.3696222 -3.7128448 -4.1176109 -4.4280725 -4.4732661 -4.4630208][-4.7648258 -5.0623507 -5.2462244 -5.1811142 -4.67431 -3.7378464 -2.8193803 -2.3775268 -2.4967165 -2.9749961 -3.5139585 -4.0319161 -4.4607425 -4.5674953 -4.4570074][-4.5668893 -4.9160137 -5.1378713 -5.0492287 -4.4574952 -3.3455915 -2.2055702 -1.7036588 -1.9849529 -2.7178245 -3.5137758 -4.1970129 -4.7235084 -4.8226666 -4.5502167][-4.3819323 -4.7198915 -4.9499207 -4.8328595 -4.1771688 -2.9672084 -1.7387481 -1.2909043 -1.7320895 -2.6256728 -3.6072595 -4.4383392 -5.0167289 -5.0444765 -4.5889244][-4.2460923 -4.5324697 -4.7532339 -4.6267385 -3.9389579 -2.7313495 -1.5874126 -1.300652 -1.8130672 -2.7159662 -3.7630296 -4.6558919 -5.2081404 -5.1318493 -4.5683146][-4.0542879 -4.2833023 -4.4925938 -4.3765287 -3.7161543 -2.6489458 -1.7618155 -1.6906056 -2.2137287 -3.0596404 -4.0536785 -4.8514094 -5.23619 -5.0110097 -4.4140744][-3.7021754 -3.8747616 -4.0460348 -3.9442801 -3.3821902 -2.5870159 -2.04616 -2.2079954 -2.7738442 -3.5994396 -4.4905519 -5.080565 -5.204155 -4.8311648 -4.2893724][-3.3475621 -3.4660749 -3.5703926 -3.4808483 -3.0950751 -2.6143935 -2.3818271 -2.7261076 -3.3717098 -4.2019844 -4.9475946 -5.2594814 -5.0810504 -4.5869622 -4.1623721][-3.198792 -3.245626 -3.2555089 -3.1879027 -3.0105171 -2.777776 -2.7362566 -3.2030468 -3.9389722 -4.7385836 -5.2630649 -5.2638092 -4.8147593 -4.2291951 -3.9197831][-3.266959 -3.2594876 -3.2240467 -3.2211623 -3.207572 -3.0790939 -3.1079698 -3.6172528 -4.3612208 -5.0374861 -5.3026323 -5.057003 -4.4622312 -3.8519247 -3.6365271][-3.487536 -3.4722931 -3.4726622 -3.5595167 -3.6196909 -3.4902225 -3.5067143 -3.9395816 -4.5496545 -5.0140491 -5.0526848 -4.6850719 -4.0912609 -3.5329857 -3.3803968]]...]
INFO - root - 2017-12-06 07:21:00.083679: step 6210, loss = 0.93, batch loss = 0.86 (13.0 examples/sec; 0.614 sec/batch; 55h:41m:35s remains)
INFO - root - 2017-12-06 07:21:06.052969: step 6220, loss = 0.84, batch loss = 0.77 (13.5 examples/sec; 0.593 sec/batch; 53h:44m:29s remains)
INFO - root - 2017-12-06 07:21:12.107592: step 6230, loss = 1.04, batch loss = 0.97 (15.2 examples/sec; 0.527 sec/batch; 47h:44m:17s remains)
INFO - root - 2017-12-06 07:21:18.158235: step 6240, loss = 1.05, batch loss = 0.98 (12.9 examples/sec; 0.621 sec/batch; 56h:19m:04s remains)
INFO - root - 2017-12-06 07:21:24.183138: step 6250, loss = 0.89, batch loss = 0.82 (13.8 examples/sec; 0.581 sec/batch; 52h:38m:35s remains)
INFO - root - 2017-12-06 07:21:30.329892: step 6260, loss = 1.15, batch loss = 1.08 (13.5 examples/sec; 0.593 sec/batch; 53h:46m:44s remains)
INFO - root - 2017-12-06 07:21:36.380618: step 6270, loss = 0.86, batch loss = 0.79 (13.5 examples/sec; 0.594 sec/batch; 53h:47m:23s remains)
INFO - root - 2017-12-06 07:21:42.470248: step 6280, loss = 0.87, batch loss = 0.80 (12.6 examples/sec; 0.634 sec/batch; 57h:28m:38s remains)
INFO - root - 2017-12-06 07:21:48.463483: step 6290, loss = 0.94, batch loss = 0.87 (13.6 examples/sec; 0.587 sec/batch; 53h:09m:08s remains)
INFO - root - 2017-12-06 07:21:54.509459: step 6300, loss = 0.86, batch loss = 0.79 (13.4 examples/sec; 0.598 sec/batch; 54h:11m:06s remains)
2017-12-06 07:21:55.133750: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6214519 -4.5548391 -4.5356741 -4.2740612 -3.754297 -3.2286391 -2.9463794 -3.0544906 -3.201685 -3.1994004 -3.3365569 -3.6816511 -3.8781433 -3.8390458 -3.767339][-4.6322584 -4.536653 -4.4788733 -4.1614361 -3.5447495 -2.8287058 -2.3426807 -2.4625587 -2.8819985 -3.1424789 -3.3732748 -3.7158971 -3.9206824 -3.920001 -3.9014423][-4.6732211 -4.54245 -4.4151444 -4.0089116 -3.2825472 -2.4050879 -1.7446282 -1.8757243 -2.5783687 -3.142906 -3.4991012 -3.8292546 -4.0216408 -4.0489078 -4.0788231][-4.7404966 -4.5872459 -4.3775148 -3.8562326 -3.0193005 -2.0403926 -1.2594914 -1.3653059 -2.2910295 -3.1422844 -3.6539397 -4.0012145 -4.1927776 -4.2302833 -4.2655334][-4.8243546 -4.6737027 -4.4103665 -3.7918954 -2.8558908 -1.8143332 -0.95136642 -0.97583222 -1.9903996 -3.0381637 -3.7128048 -4.1319132 -4.3591619 -4.4117889 -4.4223776][-4.9095125 -4.7925119 -4.5292072 -3.8674107 -2.8647 -1.7723875 -0.832314 -0.71363115 -1.6479075 -2.7442083 -3.550736 -4.1004424 -4.4062 -4.4740295 -4.4373965][-4.9701552 -4.9016438 -4.6857028 -4.0407243 -3.0225754 -1.9011955 -0.9198873 -0.67208862 -1.4250994 -2.4358058 -3.3214302 -4.0169725 -4.3791413 -4.389677 -4.2505684][-5.0054097 -4.9870052 -4.8372688 -4.25578 -3.2985356 -2.2257049 -1.2649581 -0.9249835 -1.4359736 -2.2477224 -3.1230481 -3.9222364 -4.3063412 -4.2056246 -3.9325922][-5.0282135 -5.0605254 -4.9876885 -4.5006385 -3.656652 -2.7158647 -1.8420222 -1.4253659 -1.6643193 -2.2006419 -2.9843388 -3.8214746 -4.2082114 -4.0173187 -3.6600888][-5.0246739 -5.0906019 -5.0931749 -4.7130208 -4.0212049 -3.2908986 -2.5833807 -2.1344573 -2.1377702 -2.4061236 -3.0533581 -3.8523233 -4.1927662 -3.9295034 -3.5465999][-5.01766 -5.0952234 -5.1695113 -4.9111772 -4.3898196 -3.9003072 -3.4044545 -2.9623964 -2.78864 -2.8507247 -3.3564663 -4.0652814 -4.30423 -3.9499156 -3.5382051][-5.0386934 -5.1048474 -5.2011309 -5.0235667 -4.6256971 -4.3059807 -3.988678 -3.5958071 -3.3464475 -3.312897 -3.7309706 -4.3668919 -4.5248532 -4.0953627 -3.6390541][-5.087986 -5.142271 -5.2085586 -5.0499783 -4.707602 -4.4630175 -4.2530279 -3.9329071 -3.7015336 -3.6747534 -4.0580573 -4.6182151 -4.7129431 -4.2650371 -3.7951272][-5.089365 -5.1586838 -5.2067952 -5.0763235 -4.8039994 -4.622273 -4.4797807 -4.2344203 -4.0547762 -4.0532866 -4.377244 -4.8126073 -4.838623 -4.4157515 -3.9721327][-4.9952173 -5.0887365 -5.1274314 -5.0216055 -4.831635 -4.7273107 -4.6452775 -4.4602757 -4.3243937 -4.3422208 -4.5982146 -4.8955936 -4.8655224 -4.5054393 -4.1230335]]...]
INFO - root - 2017-12-06 07:22:01.263932: step 6310, loss = 1.05, batch loss = 0.97 (13.4 examples/sec; 0.598 sec/batch; 54h:12m:10s remains)
INFO - root - 2017-12-06 07:22:07.297287: step 6320, loss = 0.90, batch loss = 0.83 (13.1 examples/sec; 0.610 sec/batch; 55h:16m:16s remains)
INFO - root - 2017-12-06 07:22:13.214207: step 6330, loss = 0.94, batch loss = 0.87 (12.9 examples/sec; 0.619 sec/batch; 56h:06m:56s remains)
INFO - root - 2017-12-06 07:22:19.353634: step 6340, loss = 0.99, batch loss = 0.92 (12.9 examples/sec; 0.621 sec/batch; 56h:15m:06s remains)
INFO - root - 2017-12-06 07:22:25.336871: step 6350, loss = 1.02, batch loss = 0.95 (13.0 examples/sec; 0.617 sec/batch; 55h:51m:20s remains)
INFO - root - 2017-12-06 07:22:31.431368: step 6360, loss = 0.97, batch loss = 0.90 (13.3 examples/sec; 0.603 sec/batch; 54h:39m:56s remains)
INFO - root - 2017-12-06 07:22:37.508006: step 6370, loss = 0.94, batch loss = 0.87 (12.9 examples/sec; 0.622 sec/batch; 56h:21m:09s remains)
INFO - root - 2017-12-06 07:22:43.630513: step 6380, loss = 0.85, batch loss = 0.78 (13.3 examples/sec; 0.602 sec/batch; 54h:31m:25s remains)
INFO - root - 2017-12-06 07:22:49.614759: step 6390, loss = 0.85, batch loss = 0.78 (13.3 examples/sec; 0.604 sec/batch; 54h:40m:48s remains)
INFO - root - 2017-12-06 07:22:55.648357: step 6400, loss = 1.06, batch loss = 0.99 (13.0 examples/sec; 0.617 sec/batch; 55h:53m:03s remains)
2017-12-06 07:22:56.251985: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2325721 -5.75943 -6.3919654 -6.7620668 -6.7873926 -6.4892273 -5.7533484 -5.1943541 -5.2240186 -5.5904913 -5.9641395 -6.0359416 -5.8336143 -5.2670112 -4.6552539][-5.4445772 -5.9073691 -6.4611473 -6.6829939 -6.5193815 -5.9913063 -5.0087881 -4.241333 -4.2201796 -4.6658726 -5.0840917 -5.2270303 -5.1471686 -4.7405405 -4.3012786][-5.7136283 -6.1333714 -6.5458627 -6.5201931 -6.0570507 -5.1930146 -3.9245367 -2.9789917 -2.9073634 -3.4334848 -3.92863 -4.1555157 -4.2020092 -4.0253773 -3.8778293][-6.0516009 -6.4570417 -6.7227907 -6.4310284 -5.6211557 -4.3782854 -2.8319564 -1.7799559 -1.706588 -2.3299685 -2.9278328 -3.2330441 -3.3458343 -3.3006275 -3.398068][-6.219274 -6.5802226 -6.7044291 -6.2022166 -5.1317372 -3.6408412 -1.9477682 -0.88739252 -0.84143233 -1.5147064 -2.1801653 -2.5397944 -2.6754985 -2.6685333 -2.90946][-6.0390468 -6.2646303 -6.2064381 -5.5579128 -4.3861041 -2.9019876 -1.32342 -0.41800261 -0.43741035 -1.0819838 -1.7341833 -2.1073387 -2.2626629 -2.286135 -2.627651][-5.6192765 -5.6360421 -5.33615 -4.5863833 -3.4734395 -2.2045527 -0.93453145 -0.30605793 -0.45714641 -1.0871768 -1.6960223 -2.0688474 -2.2811062 -2.3988159 -2.8746324][-5.125515 -4.9266787 -4.412425 -3.6377907 -2.7371514 -1.8495708 -0.98134637 -0.59294271 -0.82173848 -1.4297683 -2.0034673 -2.4339602 -2.796226 -3.1004469 -3.7650323][-4.6930008 -4.3241777 -3.6667604 -2.9545748 -2.3721292 -1.9729364 -1.5479565 -1.3214288 -1.5181813 -2.0348747 -2.5703568 -3.1131229 -3.6618857 -4.1461792 -4.9454904][-4.3647146 -3.9048777 -3.2118874 -2.6052744 -2.3152838 -2.3701613 -2.3720417 -2.291652 -2.4039342 -2.7761679 -3.2527704 -3.9024694 -4.5963006 -5.1715727 -5.9461575][-4.0833497 -3.6532574 -3.0811887 -2.6528392 -2.6047902 -2.9693282 -3.2730193 -3.3197274 -3.3669434 -3.6111481 -4.022223 -4.6678782 -5.3440704 -5.8515921 -6.4057522][-4.0590024 -3.7190533 -3.3261027 -3.0888023 -3.194639 -3.6628983 -4.0841446 -4.2183275 -4.2737513 -4.4667888 -4.7977276 -5.2746925 -5.7183833 -5.9692717 -6.1646185][-4.2481604 -3.9838877 -3.7070518 -3.5951729 -3.7739565 -4.236124 -4.6931043 -4.9100642 -5.0111175 -5.1794643 -5.3637567 -5.50748 -5.5560231 -5.4328136 -5.2464752][-4.5191493 -4.267653 -4.0102935 -3.9463675 -4.1419086 -4.5632234 -5.0170631 -5.2562151 -5.3181581 -5.3660145 -5.2904396 -5.0487804 -4.7410417 -4.3498125 -3.9746795][-4.7965817 -4.5496068 -4.2761645 -4.2024879 -4.3636193 -4.71841 -5.1194148 -5.2724009 -5.17248 -4.9893117 -4.623723 -4.0891252 -3.6133254 -3.1842871 -2.8963976]]...]
INFO - root - 2017-12-06 07:23:02.360255: step 6410, loss = 0.74, batch loss = 0.67 (12.9 examples/sec; 0.618 sec/batch; 55h:59m:42s remains)
INFO - root - 2017-12-06 07:23:08.406195: step 6420, loss = 0.95, batch loss = 0.88 (12.6 examples/sec; 0.637 sec/batch; 57h:42m:31s remains)
INFO - root - 2017-12-06 07:23:14.503219: step 6430, loss = 0.93, batch loss = 0.86 (13.2 examples/sec; 0.605 sec/batch; 54h:45m:32s remains)
INFO - root - 2017-12-06 07:23:20.421497: step 6440, loss = 1.37, batch loss = 1.30 (13.4 examples/sec; 0.596 sec/batch; 53h:59m:41s remains)
INFO - root - 2017-12-06 07:23:26.622172: step 6450, loss = 0.69, batch loss = 0.62 (12.7 examples/sec; 0.628 sec/batch; 56h:52m:03s remains)
INFO - root - 2017-12-06 07:23:32.704020: step 6460, loss = 0.95, batch loss = 0.88 (13.4 examples/sec; 0.598 sec/batch; 54h:11m:00s remains)
INFO - root - 2017-12-06 07:23:38.789023: step 6470, loss = 1.00, batch loss = 0.93 (13.1 examples/sec; 0.610 sec/batch; 55h:14m:34s remains)
INFO - root - 2017-12-06 07:23:44.898859: step 6480, loss = 0.98, batch loss = 0.91 (12.9 examples/sec; 0.622 sec/batch; 56h:21m:50s remains)
INFO - root - 2017-12-06 07:23:51.070499: step 6490, loss = 0.98, batch loss = 0.91 (13.1 examples/sec; 0.611 sec/batch; 55h:20m:42s remains)
INFO - root - 2017-12-06 07:23:57.200236: step 6500, loss = 1.06, batch loss = 0.99 (13.1 examples/sec; 0.609 sec/batch; 55h:09m:24s remains)
2017-12-06 07:23:57.751301: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1712947 -4.5235977 -4.8021765 -4.901927 -4.9083395 -4.9302478 -4.9379125 -5.0553179 -5.3779726 -5.7893553 -6.1877856 -6.3996959 -6.4021149 -6.2439704 -5.981184][-4.355999 -4.7296233 -5.0162516 -5.0880837 -5.0351539 -4.9725637 -4.9096556 -4.9877229 -5.2543592 -5.6400905 -6.1049571 -6.4004517 -6.4399595 -6.3349247 -6.1382079][-4.5520067 -4.9459128 -5.2362709 -5.2587342 -5.1162987 -4.9339304 -4.8262773 -4.9051046 -5.0877051 -5.3630147 -5.804914 -6.148519 -6.2467194 -6.2443686 -6.1822591][-4.7816534 -5.1963177 -5.4432211 -5.3576851 -5.1071892 -4.8018889 -4.6045361 -4.6316233 -4.7366 -4.9209819 -5.2999554 -5.6505208 -5.8449516 -5.9964361 -6.0898466][-5.0324354 -5.3894448 -5.539957 -5.3199592 -4.9151621 -4.4365463 -4.0676436 -3.9544704 -4.0353084 -4.2679009 -4.6507421 -5.0383162 -5.4012394 -5.7617669 -5.9924049][-5.2970886 -5.5400319 -5.6214409 -5.301857 -4.6843729 -3.9374535 -3.2943444 -2.9558115 -3.0505276 -3.4907649 -4.03242 -4.5667658 -5.1553783 -5.7181506 -6.0290375][-5.5428362 -5.675559 -5.7578859 -5.4121227 -4.5727096 -3.5205767 -2.5683696 -1.9709496 -2.0920565 -2.7998481 -3.610316 -4.3744588 -5.1586633 -5.8259592 -6.1247616][-5.655354 -5.7059245 -5.8196568 -5.4904661 -4.4979897 -3.2136374 -2.0078406 -1.2385187 -1.3989849 -2.2714124 -3.3083119 -4.3077064 -5.21261 -5.8534584 -6.0872097][-5.6177397 -5.6583638 -5.7862215 -5.4686041 -4.459816 -3.0916946 -1.7460611 -0.94290948 -1.1406555 -2.0047746 -3.1037741 -4.2455463 -5.19388 -5.733882 -5.8867931][-5.479835 -5.5502224 -5.6519408 -5.331161 -4.4591007 -3.2294703 -1.974519 -1.2956619 -1.5119841 -2.2207675 -3.2100065 -4.3611426 -5.2781296 -5.6806979 -5.739862][-5.3065772 -5.3893228 -5.4270654 -5.0820932 -4.3885474 -3.4647553 -2.5664644 -2.1805296 -2.4307432 -2.9617257 -3.7817373 -4.8091068 -5.5666575 -5.780365 -5.7436204][-5.18891 -5.2771311 -5.2278919 -4.8204265 -4.2433996 -3.656214 -3.222487 -3.1926093 -3.5024178 -3.9156265 -4.558773 -5.3350906 -5.7943025 -5.7949328 -5.6920424][-5.1302066 -5.204268 -5.0553222 -4.586132 -4.0865059 -3.7759333 -3.7433388 -3.96834 -4.2956634 -4.6326408 -5.1230764 -5.6341038 -5.82172 -5.7037745 -5.6006708][-5.0578752 -5.132453 -4.9483371 -4.4823294 -4.0588951 -3.9479623 -4.1593685 -4.4866738 -4.8035469 -5.1119552 -5.4905968 -5.7891254 -5.8013487 -5.6706319 -5.6243291][-4.9736795 -5.0778632 -4.9271951 -4.5300565 -4.1937714 -4.1946836 -4.4728861 -4.7696362 -5.0631227 -5.3855939 -5.6886258 -5.8316407 -5.7453966 -5.6284671 -5.6310034]]...]
INFO - root - 2017-12-06 07:24:03.818110: step 6510, loss = 0.86, batch loss = 0.79 (13.5 examples/sec; 0.594 sec/batch; 53h:49m:26s remains)
INFO - root - 2017-12-06 07:24:09.929941: step 6520, loss = 1.20, batch loss = 1.13 (12.7 examples/sec; 0.629 sec/batch; 56h:58m:26s remains)
INFO - root - 2017-12-06 07:24:16.055481: step 6530, loss = 0.82, batch loss = 0.75 (12.5 examples/sec; 0.639 sec/batch; 57h:53m:48s remains)
INFO - root - 2017-12-06 07:24:21.783631: step 6540, loss = 0.77, batch loss = 0.70 (13.8 examples/sec; 0.580 sec/batch; 52h:30m:29s remains)
INFO - root - 2017-12-06 07:24:27.838718: step 6550, loss = 1.26, batch loss = 1.19 (13.4 examples/sec; 0.596 sec/batch; 53h:59m:09s remains)
INFO - root - 2017-12-06 07:24:33.913719: step 6560, loss = 1.23, batch loss = 1.16 (12.9 examples/sec; 0.619 sec/batch; 56h:01m:28s remains)
INFO - root - 2017-12-06 07:24:39.949879: step 6570, loss = 0.97, batch loss = 0.89 (13.2 examples/sec; 0.606 sec/batch; 54h:51m:03s remains)
INFO - root - 2017-12-06 07:24:46.045936: step 6580, loss = 1.09, batch loss = 1.02 (13.1 examples/sec; 0.612 sec/batch; 55h:23m:42s remains)
INFO - root - 2017-12-06 07:24:52.181279: step 6590, loss = 0.77, batch loss = 0.70 (13.3 examples/sec; 0.602 sec/batch; 54h:31m:19s remains)
INFO - root - 2017-12-06 07:24:58.262996: step 6600, loss = 0.91, batch loss = 0.84 (13.1 examples/sec; 0.612 sec/batch; 55h:26m:37s remains)
2017-12-06 07:24:58.880870: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.117722 -3.9545794 -3.8559895 -4.0073323 -4.26397 -4.327497 -4.2035785 -4.0509062 -3.8525829 -3.7065258 -3.5674286 -3.6639915 -3.7951922 -3.770957 -3.9706016][-4.0966539 -3.923687 -3.80787 -3.9719198 -4.2367277 -4.3164692 -4.2335987 -4.19002 -4.0958576 -3.9397779 -3.7469368 -3.8237917 -3.9583147 -3.9039373 -4.0437074][-4.1167493 -4.0009866 -3.9082785 -4.0152807 -4.1843181 -4.2375436 -4.215323 -4.3062825 -4.3720613 -4.2191854 -4.0086184 -4.0639048 -4.1777673 -4.0589943 -4.0523534][-4.1166372 -4.0865335 -4.0225196 -4.0131245 -4.0231519 -4.040029 -4.1171937 -4.3620744 -4.6078372 -4.492137 -4.2999392 -4.3005662 -4.280057 -4.0747643 -3.9701309][-4.0931468 -4.1502008 -4.1067328 -3.951472 -3.7626619 -3.716013 -3.8782349 -4.1992612 -4.5650439 -4.5008278 -4.3853736 -4.3635669 -4.1800089 -3.9402626 -3.8475819][-4.099627 -4.2197237 -4.1793594 -3.8942041 -3.5043159 -3.3327451 -3.44943 -3.677824 -4.0634289 -4.0737267 -4.1198592 -4.163816 -3.8504899 -3.6286364 -3.6296492][-4.1626129 -4.3088255 -4.2518024 -3.8562751 -3.2835727 -2.9363058 -2.8661997 -2.8704414 -3.2101214 -3.3156118 -3.6101077 -3.801187 -3.3876777 -3.1989574 -3.2912276][-4.3042388 -4.4375062 -4.3441067 -3.8743939 -3.190629 -2.7011237 -2.4128804 -2.2124891 -2.5404038 -2.7451169 -3.2670448 -3.5867803 -3.0563345 -2.7871733 -2.815618][-4.51115 -4.64073 -4.5505853 -4.091485 -3.4008317 -2.8436933 -2.3733962 -2.0384963 -2.3535497 -2.614316 -3.2695556 -3.6473365 -3.0062263 -2.5533147 -2.3565371][-4.6816449 -4.8330812 -4.8171873 -4.4766326 -3.8818712 -3.3353631 -2.7464042 -2.3417962 -2.5894389 -2.8080852 -3.4573865 -3.8291376 -3.1557791 -2.5742509 -2.2134445][-4.7366719 -4.9070811 -4.9889579 -4.8143115 -4.3685713 -3.8811727 -3.2568178 -2.8791466 -3.0586169 -3.164516 -3.6758318 -3.9779947 -3.3517232 -2.770102 -2.4409394][-4.718214 -4.8968587 -5.043951 -5.00832 -4.7522483 -4.3974366 -3.8319888 -3.5334918 -3.6473224 -3.6054275 -3.9060245 -4.1166539 -3.6056507 -3.1181228 -2.9420068][-4.6445503 -4.8271241 -5.0018725 -5.0478582 -4.9701538 -4.802906 -4.3992429 -4.2089782 -4.265398 -4.0968819 -4.1845131 -4.2934461 -3.9337897 -3.5611482 -3.5093069][-4.5255723 -4.6727633 -4.8111577 -4.8611474 -4.8906603 -4.9054236 -4.7183156 -4.6561246 -4.7043648 -4.5087614 -4.4617691 -4.4817171 -4.2616243 -3.9960721 -3.9905186][-4.3770971 -4.4648271 -4.5353565 -4.5551171 -4.6333413 -4.7731957 -4.7571096 -4.7776051 -4.8398914 -4.7128592 -4.6243076 -4.5779505 -4.446979 -4.2691183 -4.2638311]]...]
INFO - root - 2017-12-06 07:25:04.924671: step 6610, loss = 1.04, batch loss = 0.97 (13.5 examples/sec; 0.591 sec/batch; 53h:32m:26s remains)
INFO - root - 2017-12-06 07:25:11.132913: step 6620, loss = 0.95, batch loss = 0.88 (12.9 examples/sec; 0.620 sec/batch; 56h:10m:01s remains)
INFO - root - 2017-12-06 07:25:17.229406: step 6630, loss = 1.03, batch loss = 0.96 (13.0 examples/sec; 0.613 sec/batch; 55h:31m:29s remains)
INFO - root - 2017-12-06 07:25:23.329119: step 6640, loss = 1.08, batch loss = 1.01 (13.1 examples/sec; 0.611 sec/batch; 55h:18m:14s remains)
INFO - root - 2017-12-06 07:25:29.344438: step 6650, loss = 0.97, batch loss = 0.90 (12.8 examples/sec; 0.624 sec/batch; 56h:26m:10s remains)
INFO - root - 2017-12-06 07:25:35.393117: step 6660, loss = 0.87, batch loss = 0.80 (12.7 examples/sec; 0.632 sec/batch; 57h:11m:26s remains)
INFO - root - 2017-12-06 07:25:41.488266: step 6670, loss = 0.77, batch loss = 0.70 (13.3 examples/sec; 0.602 sec/batch; 54h:31m:26s remains)
INFO - root - 2017-12-06 07:25:47.536867: step 6680, loss = 1.00, batch loss = 0.93 (12.7 examples/sec; 0.630 sec/batch; 56h:58m:55s remains)
INFO - root - 2017-12-06 07:25:53.500442: step 6690, loss = 0.87, batch loss = 0.80 (13.0 examples/sec; 0.614 sec/batch; 55h:32m:47s remains)
INFO - root - 2017-12-06 07:25:59.551335: step 6700, loss = 1.23, batch loss = 1.16 (13.6 examples/sec; 0.589 sec/batch; 53h:17m:39s remains)
2017-12-06 07:26:00.139532: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0477033 -4.3642926 -4.6629391 -4.881711 -4.8816051 -4.3108883 -3.1244345 -1.6895976 -0.89438033 -1.4972148 -2.6992881 -3.9513233 -4.7738791 -5.0024948 -5.09061][-4.07931 -4.2928252 -4.5395193 -4.8552642 -5.1204934 -4.7713327 -3.6264024 -1.9767199 -0.81528068 -1.0366549 -1.9504957 -3.0199895 -3.7960303 -4.1742578 -4.6193261][-3.8099601 -4.0231409 -4.4159055 -4.9214177 -5.4119134 -5.2641373 -4.2623935 -2.6408236 -1.3716087 -1.2382219 -1.6946492 -2.3597429 -2.9303977 -3.3626106 -4.0603213][-3.0147676 -3.2417631 -3.8749285 -4.6206884 -5.2458773 -5.2504435 -4.4982014 -3.1432731 -2.06368 -1.8394549 -2.0243864 -2.3999858 -2.7894042 -3.1716771 -3.8596625][-2.1131866 -2.2458618 -3.0148203 -3.9417839 -4.6246567 -4.7124538 -4.2328854 -3.2467742 -2.49423 -2.4371169 -2.7021828 -3.076875 -3.3963723 -3.6802692 -4.1612878][-1.5925848 -1.5957913 -2.3750751 -3.3510942 -3.9841666 -4.0404806 -3.7506809 -3.1145542 -2.6454167 -2.7849889 -3.2740817 -3.8484306 -4.2541609 -4.4905238 -4.738842][-1.742723 -1.6991696 -2.3771634 -3.19916 -3.6565301 -3.5964098 -3.417196 -3.0389624 -2.6686151 -2.8038313 -3.3857317 -4.1871033 -4.8042693 -5.1076293 -5.2291427][-2.381969 -2.3937426 -2.9421701 -3.4748352 -3.6997952 -3.5509541 -3.4813383 -3.2941039 -2.8906903 -2.8256512 -3.2630656 -4.1436329 -4.9287 -5.3144183 -5.3877273][-2.9812236 -3.1139913 -3.6322262 -3.95314 -3.9940348 -3.7728739 -3.784668 -3.7590559 -3.376246 -3.1524332 -3.3567939 -4.1201706 -4.9012341 -5.2918348 -5.3270669][-3.0733116 -3.2905645 -3.866138 -4.1615543 -4.1846228 -3.9843326 -4.1065736 -4.2520103 -4.0193892 -3.7910526 -3.8210733 -4.3641849 -4.9783244 -5.2564359 -5.1831861][-2.9246573 -3.1202443 -3.6991661 -4.0526104 -4.2083006 -4.1535735 -4.3852158 -4.6532993 -4.60827 -4.4970388 -4.4965839 -4.8408642 -5.2049336 -5.2731857 -5.0236764][-3.1211629 -3.1972642 -3.6310952 -3.987009 -4.2815719 -4.4031644 -4.6649923 -4.8997908 -4.9385204 -4.9427252 -4.9868031 -5.20229 -5.3361244 -5.216527 -4.8481212][-3.5623212 -3.5140748 -3.774163 -4.1210332 -4.5115724 -4.7377472 -4.9243903 -5.0110183 -5.0359364 -5.1039352 -5.1798725 -5.2867417 -5.2402024 -5.0109882 -4.634285][-3.8794205 -3.8113828 -3.9846346 -4.30326 -4.6779823 -4.8962107 -4.9817772 -4.9570255 -4.9641275 -5.0543365 -5.1161332 -5.1141319 -4.9480515 -4.6827426 -4.3729062][-4.0048971 -3.9857123 -4.1163979 -4.3510885 -4.59978 -4.7254362 -4.7322087 -4.6609597 -4.6405225 -4.6818304 -4.6856675 -4.6129575 -4.4203396 -4.1929336 -3.981015]]...]
INFO - root - 2017-12-06 07:26:06.252318: step 6710, loss = 0.93, batch loss = 0.86 (13.1 examples/sec; 0.609 sec/batch; 55h:05m:23s remains)
INFO - root - 2017-12-06 07:26:12.263320: step 6720, loss = 0.82, batch loss = 0.75 (13.3 examples/sec; 0.601 sec/batch; 54h:23m:04s remains)
INFO - root - 2017-12-06 07:26:18.331572: step 6730, loss = 0.79, batch loss = 0.72 (13.3 examples/sec; 0.600 sec/batch; 54h:17m:41s remains)
INFO - root - 2017-12-06 07:26:24.458494: step 6740, loss = 0.79, batch loss = 0.72 (12.7 examples/sec; 0.631 sec/batch; 57h:07m:47s remains)
INFO - root - 2017-12-06 07:26:30.589840: step 6750, loss = 1.00, batch loss = 0.93 (13.3 examples/sec; 0.600 sec/batch; 54h:15m:57s remains)
INFO - root - 2017-12-06 07:26:36.512555: step 6760, loss = 0.88, batch loss = 0.81 (13.3 examples/sec; 0.604 sec/batch; 54h:36m:55s remains)
INFO - root - 2017-12-06 07:26:42.625077: step 6770, loss = 0.82, batch loss = 0.75 (12.3 examples/sec; 0.651 sec/batch; 58h:56m:08s remains)
INFO - root - 2017-12-06 07:26:48.721344: step 6780, loss = 0.91, batch loss = 0.84 (13.6 examples/sec; 0.587 sec/batch; 53h:06m:48s remains)
INFO - root - 2017-12-06 07:26:54.868717: step 6790, loss = 0.94, batch loss = 0.87 (12.7 examples/sec; 0.630 sec/batch; 57h:00m:50s remains)
INFO - root - 2017-12-06 07:27:00.984832: step 6800, loss = 0.84, batch loss = 0.77 (12.6 examples/sec; 0.635 sec/batch; 57h:27m:15s remains)
2017-12-06 07:27:01.622427: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5297928 -2.7926483 -2.5658441 -3.1764121 -4.2751589 -4.8279409 -4.5480108 -4.0562544 -3.8175495 -3.9735394 -3.8237762 -3.2693202 -2.9213221 -3.0784295 -3.8017404][-3.4543273 -2.6553431 -2.3760393 -3.0519948 -4.3167629 -5.0345616 -4.8904982 -4.4991169 -4.2770171 -4.2494626 -3.8223922 -3.0846922 -2.7117715 -2.9840298 -3.8050222][-3.4062834 -2.5087876 -2.1026266 -2.6989117 -3.9575257 -4.7620735 -4.7510343 -4.4712706 -4.31098 -4.189568 -3.6235123 -2.8155389 -2.4607873 -2.8249598 -3.7178776][-3.4561555 -2.4678183 -1.9258966 -2.3412633 -3.4139102 -4.1661491 -4.2149239 -4.0059967 -3.925678 -3.8460786 -3.3703551 -2.6941776 -2.4300618 -2.838696 -3.7411473][-3.6172624 -2.6492467 -2.0940785 -2.3542304 -3.1283817 -3.6559832 -3.5810604 -3.2984977 -3.2320082 -3.2634165 -3.0501757 -2.6923361 -2.6020832 -3.018455 -3.8843803][-3.75596 -2.9584751 -2.545033 -2.7511811 -3.2501481 -3.4418342 -3.0540705 -2.501061 -2.340637 -2.5031192 -2.6555729 -2.7232189 -2.8728926 -3.2954283 -4.0956492][-3.9386106 -3.3895984 -3.1596298 -3.3443074 -3.6088228 -3.4436803 -2.6636176 -1.7481418 -1.4310539 -1.7329535 -2.2814169 -2.794739 -3.2395954 -3.7263794 -4.4443712][-4.1948276 -3.8824446 -3.7858615 -3.9371908 -4.0219769 -3.5945075 -2.5336432 -1.3403769 -0.87376881 -1.256995 -2.0687253 -2.901679 -3.5907896 -4.1589284 -4.7776423][-4.3954921 -4.2937174 -4.3081112 -4.4633312 -4.4953556 -4.0001078 -2.8711243 -1.5734618 -0.95983863 -1.272907 -2.110199 -3.0568664 -3.8568246 -4.4286108 -4.8787336][-4.4686809 -4.5226254 -4.6206331 -4.8005843 -4.8658133 -4.4682636 -3.4936824 -2.2947688 -1.6169498 -1.7739508 -2.475266 -3.3704586 -4.1512527 -4.6411438 -4.8878884][-4.4382334 -4.5639596 -4.701396 -4.8945708 -4.9978042 -4.7585297 -4.0688934 -3.1141191 -2.4727507 -2.4805231 -2.9902537 -3.7436092 -4.4281707 -4.8131862 -4.9155717][-4.3727388 -4.4755907 -4.5907311 -4.7407184 -4.8112497 -4.6881523 -4.318327 -3.7275743 -3.2653823 -3.2003498 -3.5169482 -4.0772238 -4.60929 -4.8685269 -4.8786764][-4.4136324 -4.4671421 -4.545794 -4.6276731 -4.6051106 -4.5145526 -4.405941 -4.1807604 -3.9553902 -3.8857055 -4.0547519 -4.4311385 -4.7876081 -4.8890543 -4.7973619][-4.5006313 -4.5403843 -4.62439 -4.6885428 -4.59906 -4.4950261 -4.5416765 -4.5765996 -4.5240083 -4.4350867 -4.4992471 -4.7536826 -5.0006313 -4.9850993 -4.7762589][-4.4137192 -4.4654684 -4.6013141 -4.7227383 -4.6623263 -4.5508657 -4.6533794 -4.8021493 -4.8005896 -4.6498308 -4.6408134 -4.853 -5.0850811 -5.0557289 -4.7662854]]...]
INFO - root - 2017-12-06 07:27:07.722817: step 6810, loss = 0.81, batch loss = 0.74 (13.1 examples/sec; 0.610 sec/batch; 55h:10m:22s remains)
INFO - root - 2017-12-06 07:27:13.760782: step 6820, loss = 1.10, batch loss = 1.03 (13.8 examples/sec; 0.579 sec/batch; 52h:21m:11s remains)
INFO - root - 2017-12-06 07:27:19.842792: step 6830, loss = 1.02, batch loss = 0.95 (13.2 examples/sec; 0.607 sec/batch; 54h:53m:44s remains)
INFO - root - 2017-12-06 07:27:25.791507: step 6840, loss = 0.92, batch loss = 0.85 (13.3 examples/sec; 0.602 sec/batch; 54h:29m:23s remains)
INFO - root - 2017-12-06 07:27:31.864963: step 6850, loss = 1.02, batch loss = 0.95 (13.0 examples/sec; 0.617 sec/batch; 55h:48m:06s remains)
INFO - root - 2017-12-06 07:27:37.797483: step 6860, loss = 0.87, batch loss = 0.80 (13.2 examples/sec; 0.608 sec/batch; 55h:01m:09s remains)
INFO - root - 2017-12-06 07:27:43.881278: step 6870, loss = 0.88, batch loss = 0.81 (12.7 examples/sec; 0.630 sec/batch; 56h:58m:46s remains)
INFO - root - 2017-12-06 07:27:50.030930: step 6880, loss = 0.89, batch loss = 0.82 (13.3 examples/sec; 0.599 sec/batch; 54h:13m:06s remains)
INFO - root - 2017-12-06 07:27:56.069740: step 6890, loss = 0.74, batch loss = 0.67 (13.3 examples/sec; 0.602 sec/batch; 54h:24m:54s remains)
INFO - root - 2017-12-06 07:28:02.211403: step 6900, loss = 0.81, batch loss = 0.74 (13.2 examples/sec; 0.606 sec/batch; 54h:47m:45s remains)
2017-12-06 07:28:02.861983: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.76682 -4.3839412 -4.1225967 -3.9473724 -4.1782422 -4.5803876 -4.6150341 -4.2202139 -3.7006333 -3.3848071 -3.4949253 -3.9805934 -4.2878 -4.2259407 -4.1373463][-4.9463983 -4.5791163 -4.3015652 -4.1753969 -4.4845114 -4.8961625 -4.9111519 -4.6336842 -4.2647781 -4.0051365 -4.1310406 -4.6665583 -5.0075397 -4.8816452 -4.6732693][-4.8435473 -4.5332522 -4.283772 -4.2446938 -4.6346383 -5.0148897 -5.0311556 -4.8912177 -4.7252464 -4.6634669 -4.9769764 -5.6564755 -6.0075412 -5.6569586 -5.1361227][-4.6755476 -4.422246 -4.19789 -4.16645 -4.4629326 -4.6668453 -4.6072578 -4.5699887 -4.695035 -5.0256753 -5.7145538 -6.6186714 -6.9927425 -6.3798828 -5.4499][-4.4682827 -4.2361073 -4.0448232 -3.9930003 -4.06728 -3.9139619 -3.5563519 -3.4666207 -3.9009209 -4.7420874 -5.8855 -7.0271964 -7.4431086 -6.6525497 -5.403646][-4.0051875 -3.8393862 -3.7190955 -3.6371207 -3.465678 -2.8899183 -2.0484381 -1.7454913 -2.4763618 -3.8505917 -5.43092 -6.817296 -7.3119431 -6.458333 -5.058702][-3.6362987 -3.5321727 -3.3884096 -3.2072172 -2.7797146 -1.781554 -0.43267751 0.0569849 -0.96563578 -2.7782543 -4.641017 -6.2018528 -6.8148775 -6.0317802 -4.654][-3.6274946 -3.4788613 -3.2344112 -2.9555335 -2.3262556 -0.9923687 0.71448517 1.2255821 -0.038494587 -2.0467017 -3.889164 -5.3876162 -6.0622287 -5.4733458 -4.3021021][-4.00378 -3.7861586 -3.4526834 -3.1911082 -2.5621557 -1.1574798 0.5053153 0.85958529 -0.35065222 -2.1583157 -3.6547782 -4.8394837 -5.4509029 -5.0679016 -4.1737266][-4.5195708 -4.2913241 -3.9489577 -3.786972 -3.3123524 -2.1413658 -0.86566424 -0.67063 -1.5622506 -2.8863959 -3.8969183 -4.6919312 -5.206347 -5.0348191 -4.4521418][-4.809206 -4.6112351 -4.3349876 -4.3005581 -4.0551405 -3.3202724 -2.6093831 -2.5407512 -2.9896104 -3.7273006 -4.23231 -4.6505718 -5.0932531 -5.1483 -4.9323907][-4.8408537 -4.6045046 -4.368227 -4.439249 -4.451654 -4.2376742 -4.1022215 -4.1265974 -4.1766167 -4.3702826 -4.375977 -4.4283361 -4.8090525 -5.0881371 -5.2489958][-4.7030087 -4.3461161 -4.0330567 -4.1250477 -4.3428712 -4.5827336 -4.9224405 -5.0744166 -4.9250627 -4.7113576 -4.3012562 -4.0561619 -4.3268204 -4.740057 -5.2271018][-4.6060834 -4.0593071 -3.510649 -3.5093255 -3.8306806 -4.4006066 -5.0718474 -5.3956633 -5.2673287 -4.8314667 -4.1783257 -3.7539861 -3.8697014 -4.3263373 -5.0504041][-4.6534472 -3.8762064 -3.0181363 -2.868448 -3.2261868 -3.9762425 -4.811429 -5.2933588 -5.3253212 -4.8582616 -4.1524639 -3.6649013 -3.5500593 -3.8948686 -4.6671605]]...]
INFO - root - 2017-12-06 07:28:08.995140: step 6910, loss = 0.99, batch loss = 0.92 (12.9 examples/sec; 0.622 sec/batch; 56h:15m:26s remains)
INFO - root - 2017-12-06 07:28:15.090459: step 6920, loss = 0.82, batch loss = 0.75 (12.7 examples/sec; 0.629 sec/batch; 56h:52m:42s remains)
INFO - root - 2017-12-06 07:28:21.151012: step 6930, loss = 0.85, batch loss = 0.78 (13.4 examples/sec; 0.596 sec/batch; 53h:51m:17s remains)
INFO - root - 2017-12-06 07:28:27.241055: step 6940, loss = 0.91, batch loss = 0.84 (12.9 examples/sec; 0.621 sec/batch; 56h:09m:17s remains)
INFO - root - 2017-12-06 07:28:33.292640: step 6950, loss = 1.10, batch loss = 1.03 (13.9 examples/sec; 0.576 sec/batch; 52h:06m:07s remains)
INFO - root - 2017-12-06 07:28:39.355266: step 6960, loss = 0.93, batch loss = 0.86 (12.8 examples/sec; 0.623 sec/batch; 56h:18m:30s remains)
INFO - root - 2017-12-06 07:28:45.313315: step 6970, loss = 0.80, batch loss = 0.73 (13.1 examples/sec; 0.611 sec/batch; 55h:15m:49s remains)
INFO - root - 2017-12-06 07:28:51.412135: step 6980, loss = 0.99, batch loss = 0.92 (13.2 examples/sec; 0.607 sec/batch; 54h:53m:27s remains)
INFO - root - 2017-12-06 07:28:57.382763: step 6990, loss = 1.13, batch loss = 1.06 (13.3 examples/sec; 0.603 sec/batch; 54h:32m:53s remains)
INFO - root - 2017-12-06 07:29:03.449801: step 7000, loss = 0.68, batch loss = 0.61 (13.1 examples/sec; 0.609 sec/batch; 55h:06m:07s remains)
2017-12-06 07:29:04.096624: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6761832 -4.8258557 -4.5310984 -4.1301169 -3.8680992 -3.8771129 -3.8516159 -3.6566041 -3.5201814 -3.3800941 -3.4818029 -3.7359161 -4.0148778 -4.0207791 -3.8590002][-4.5571055 -4.7591205 -4.5322752 -4.1366549 -3.8295834 -3.8662622 -3.9460723 -3.8307452 -3.7772059 -3.6411681 -3.659492 -3.5924883 -3.6100638 -3.6016095 -3.4876149][-4.541553 -4.7287 -4.5322208 -4.13757 -3.7915688 -3.7884691 -3.8445883 -3.7293558 -3.7349143 -3.6771922 -3.7181015 -3.3603802 -3.0130885 -2.9412675 -2.9451287][-4.5516748 -4.6467562 -4.3883204 -3.9644165 -3.5801761 -3.5227487 -3.4824743 -3.3207407 -3.3972352 -3.4989243 -3.6947384 -3.2306037 -2.5843954 -2.4032173 -2.4913433][-4.4672418 -4.6071272 -4.3419847 -3.8805256 -3.4080114 -3.2560425 -3.0467415 -2.8289828 -3.0748711 -3.4438238 -3.8865807 -3.530108 -2.766372 -2.4884181 -2.5692468][-4.2837195 -4.6249347 -4.4529467 -3.9961121 -3.4034638 -3.1025252 -2.6663063 -2.382024 -2.9057488 -3.6383879 -4.3750443 -4.296237 -3.5896416 -3.2159829 -3.1621501][-3.8109565 -4.3775492 -4.3821158 -4.0280237 -3.372889 -2.9247727 -2.2712388 -1.9004612 -2.6644721 -3.7016885 -4.6551261 -4.9152012 -4.4107256 -3.9552119 -3.7366877][-3.4163299 -4.062417 -4.2242928 -4.0274558 -3.3674006 -2.7505789 -1.8896468 -1.3887031 -2.2427828 -3.4296353 -4.4402928 -4.8993959 -4.600163 -4.1397119 -3.8736882][-3.3405585 -3.9424529 -4.1389041 -4.0464063 -3.4537015 -2.7898452 -1.9264295 -1.3869154 -2.0734973 -3.1446648 -4.0317097 -4.4899244 -4.2759938 -3.8077512 -3.574476][-3.2934303 -3.8315215 -4.0090528 -3.975899 -3.5394242 -2.950887 -2.2726889 -1.8157635 -2.2218938 -3.0175905 -3.6864173 -4.0213361 -3.8048072 -3.2884581 -3.0232563][-3.29641 -3.7662992 -3.8838608 -3.8284969 -3.5132473 -3.0022972 -2.5194712 -2.205909 -2.4386687 -3.0240488 -3.5345695 -3.7806435 -3.5552325 -2.9604762 -2.5643382][-3.5054905 -3.8756008 -3.9025869 -3.8050189 -3.5583606 -3.0706983 -2.6906753 -2.5376358 -2.7613263 -3.255888 -3.66493 -3.8286712 -3.5769315 -2.9180026 -2.3674397][-3.7979345 -4.0641522 -4.0442171 -3.9993179 -3.8627634 -3.4254539 -3.0709882 -2.9771943 -3.2459335 -3.6947436 -4.0218782 -4.1050205 -3.7854912 -3.0812101 -2.4152524][-4.0035076 -4.2065496 -4.2076726 -4.2734246 -4.2558632 -3.96133 -3.6611168 -3.5400491 -3.7820473 -4.0989871 -4.3047619 -4.3345823 -3.9955301 -3.3612232 -2.718343][-3.9661255 -4.1787043 -4.2585139 -4.4291105 -4.4584756 -4.29527 -4.0408154 -3.8443589 -4.0245485 -4.1986074 -4.2870364 -4.297678 -4.0022697 -3.5331466 -3.0695639]]...]
INFO - root - 2017-12-06 07:29:10.208715: step 7010, loss = 0.89, batch loss = 0.82 (12.8 examples/sec; 0.627 sec/batch; 56h:41m:18s remains)
INFO - root - 2017-12-06 07:29:16.298692: step 7020, loss = 0.79, batch loss = 0.72 (12.7 examples/sec; 0.628 sec/batch; 56h:47m:56s remains)
INFO - root - 2017-12-06 07:29:22.263400: step 7030, loss = 0.92, batch loss = 0.85 (13.2 examples/sec; 0.606 sec/batch; 54h:45m:21s remains)
INFO - root - 2017-12-06 07:29:28.252090: step 7040, loss = 1.11, batch loss = 1.04 (12.8 examples/sec; 0.627 sec/batch; 56h:38m:31s remains)
INFO - root - 2017-12-06 07:29:34.352343: step 7050, loss = 0.89, batch loss = 0.82 (13.1 examples/sec; 0.609 sec/batch; 55h:01m:31s remains)
INFO - root - 2017-12-06 07:29:40.384709: step 7060, loss = 1.06, batch loss = 0.99 (13.3 examples/sec; 0.601 sec/batch; 54h:21m:26s remains)
INFO - root - 2017-12-06 07:29:46.406683: step 7070, loss = 0.92, batch loss = 0.85 (13.1 examples/sec; 0.608 sec/batch; 54h:59m:52s remains)
INFO - root - 2017-12-06 07:29:52.355843: step 7080, loss = 1.01, batch loss = 0.94 (12.6 examples/sec; 0.633 sec/batch; 57h:14m:22s remains)
INFO - root - 2017-12-06 07:29:58.497644: step 7090, loss = 1.02, batch loss = 0.95 (13.1 examples/sec; 0.610 sec/batch; 55h:09m:42s remains)
INFO - root - 2017-12-06 07:30:04.588449: step 7100, loss = 1.05, batch loss = 0.98 (13.0 examples/sec; 0.613 sec/batch; 55h:25m:40s remains)
2017-12-06 07:30:05.231996: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9770637 -3.7453332 -3.2443514 -2.8461556 -2.7764914 -3.0320029 -3.4282522 -3.6565802 -3.6961129 -3.7317123 -3.6211095 -3.2495027 -2.7595558 -2.4127688 -2.5368237][-4.0148335 -4.0185437 -3.6510472 -3.2170844 -2.9990382 -3.1818736 -3.6314113 -3.9811311 -4.1635003 -4.3144288 -4.2088718 -3.7271852 -3.106678 -2.5995359 -2.5362997][-4.0186296 -4.3367934 -4.14869 -3.6479354 -3.1602402 -3.0379782 -3.3257234 -3.75703 -4.234869 -4.7421207 -4.8352323 -4.3734632 -3.70723 -3.0787544 -2.8025036][-3.8484097 -4.4026351 -4.3469081 -3.7413776 -2.8978372 -2.2896066 -2.2096937 -2.640521 -3.547246 -4.67967 -5.2356768 -4.997262 -4.3852787 -3.6794214 -3.2015827][-3.4449205 -4.0409193 -4.0392137 -3.3585134 -2.2081561 -1.0798366 -0.4937408 -0.758137 -2.0341618 -3.8780706 -5.1011043 -5.3129253 -4.8924055 -4.2008271 -3.6069381][-3.1897471 -3.6564989 -3.6020291 -2.8648984 -1.5039036 0.076736927 1.2393651 1.3426113 -0.051419735 -2.4261692 -4.3151412 -5.0966778 -5.014626 -4.5040927 -3.9693928][-3.3276024 -3.6189876 -3.492749 -2.7408266 -1.275907 0.6560173 2.38838 3.0080571 1.7917938 -0.78286934 -3.1151106 -4.3808918 -4.6808395 -4.4795704 -4.1842222][-3.6878042 -3.9026153 -3.8140545 -3.1599655 -1.734303 0.35054302 2.4597487 3.5406518 2.7302003 0.35434675 -2.0391922 -3.5397282 -4.1228027 -4.247242 -4.3017182][-3.9723 -4.3105931 -4.4091368 -3.9936767 -2.808485 -0.89580655 1.1745849 2.4265747 2.1235533 0.42159653 -1.4723654 -2.7496576 -3.3556981 -3.6907029 -4.0684571][-3.8869314 -4.4731951 -4.8508396 -4.7919674 -4.0303016 -2.5756869 -0.89923191 0.24068832 0.35780859 -0.47817421 -1.4684174 -2.1090581 -2.4496653 -2.792398 -3.3344038][-3.3416398 -4.0919681 -4.6885843 -4.972888 -4.6945715 -3.8236542 -2.7159739 -1.8471296 -1.4992516 -1.6394088 -1.779933 -1.7242477 -1.6669786 -1.8498018 -2.385776][-2.5739903 -3.2591088 -3.9161317 -4.4193034 -4.5365553 -4.1942177 -3.6083224 -3.0417228 -2.6750884 -2.4952269 -2.1960573 -1.7640934 -1.4391711 -1.4080849 -1.7895036][-1.9972711 -2.4597337 -3.0950096 -3.706697 -4.0430937 -4.0248137 -3.7446365 -3.3660874 -3.0552831 -2.8667393 -2.5878224 -2.2146351 -1.8792191 -1.7060146 -1.8685675][-1.970273 -2.2010553 -2.8046985 -3.4856377 -3.9233906 -4.0038333 -3.7712085 -3.386548 -3.0422449 -2.8966603 -2.8175993 -2.7089033 -2.549031 -2.3942335 -2.4125929][-2.5396481 -2.6201153 -3.1927509 -3.8906736 -4.3331771 -4.3648772 -4.0466137 -3.5351706 -3.0381348 -2.8159239 -2.8220139 -2.902549 -2.9219284 -2.8649607 -2.8301251]]...]
INFO - root - 2017-12-06 07:30:11.309554: step 7110, loss = 1.29, batch loss = 1.22 (13.3 examples/sec; 0.600 sec/batch; 54h:16m:01s remains)
INFO - root - 2017-12-06 07:30:17.326910: step 7120, loss = 0.97, batch loss = 0.90 (13.4 examples/sec; 0.596 sec/batch; 53h:54m:18s remains)
INFO - root - 2017-12-06 07:30:23.434002: step 7130, loss = 0.94, batch loss = 0.87 (12.7 examples/sec; 0.631 sec/batch; 57h:02m:41s remains)
INFO - root - 2017-12-06 07:30:29.483613: step 7140, loss = 0.89, batch loss = 0.82 (12.9 examples/sec; 0.618 sec/batch; 55h:51m:12s remains)
INFO - root - 2017-12-06 07:30:35.607534: step 7150, loss = 0.90, batch loss = 0.83 (13.2 examples/sec; 0.608 sec/batch; 54h:54m:53s remains)
INFO - root - 2017-12-06 07:30:41.767186: step 7160, loss = 1.12, batch loss = 1.05 (12.5 examples/sec; 0.641 sec/batch; 57h:54m:22s remains)
INFO - root - 2017-12-06 07:30:47.894989: step 7170, loss = 0.83, batch loss = 0.76 (13.5 examples/sec; 0.594 sec/batch; 53h:42m:41s remains)
INFO - root - 2017-12-06 07:30:53.900585: step 7180, loss = 0.98, batch loss = 0.91 (12.9 examples/sec; 0.620 sec/batch; 55h:59m:26s remains)
INFO - root - 2017-12-06 07:31:00.027535: step 7190, loss = 0.99, batch loss = 0.92 (13.5 examples/sec; 0.595 sec/batch; 53h:43m:36s remains)
INFO - root - 2017-12-06 07:31:06.124472: step 7200, loss = 1.04, batch loss = 0.97 (13.3 examples/sec; 0.602 sec/batch; 54h:23m:38s remains)
2017-12-06 07:31:06.702931: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1333551 -5.2315655 -5.2492862 -5.2298632 -5.2352304 -5.2677088 -5.2545052 -5.1851983 -5.1352038 -5.1468792 -5.1755705 -5.1828117 -5.1740203 -5.1184435 -5.0393982][-5.2174907 -5.3063126 -5.2911143 -5.2455316 -5.2468853 -5.2918019 -5.2813635 -5.195178 -5.1348205 -5.1615524 -5.2159033 -5.250493 -5.2695642 -5.1966081 -5.0782928][-5.27546 -5.3661647 -5.3222113 -5.2262859 -5.1558943 -5.1270485 -5.0824142 -5.0153894 -4.9953876 -5.0597205 -5.1375275 -5.1842637 -5.2308321 -5.1522093 -5.0185833][-5.3027143 -5.3831177 -5.2951107 -5.1118321 -4.9011612 -4.6851926 -4.5217066 -4.468236 -4.5355058 -4.7152462 -4.8788476 -4.9596019 -5.0301914 -4.9420896 -4.813477][-5.2963543 -5.3197479 -5.1414366 -4.8085632 -4.4047332 -3.9538388 -3.6853807 -3.7061934 -3.9064639 -4.24312 -4.5224609 -4.6492829 -4.7458482 -4.6397491 -4.496314][-5.2958107 -5.2290087 -4.921751 -4.3922696 -3.7466316 -3.0818145 -2.8341832 -3.0766735 -3.4405243 -3.8434772 -4.1479406 -4.2949891 -4.4462376 -4.3690248 -4.2161484][-5.206996 -4.9958048 -4.5436687 -3.8167257 -2.9224072 -2.0674648 -1.9194627 -2.4722576 -3.0057435 -3.3649135 -3.5892746 -3.7195272 -3.9619186 -4.0051694 -3.9095678][-5.0169258 -4.6689944 -4.1459489 -3.3532915 -2.327322 -1.3386045 -1.2572253 -2.0296009 -2.6685815 -2.9174395 -3.0414565 -3.1441329 -3.4393678 -3.6095037 -3.6111064][-4.8865671 -4.555191 -4.1424131 -3.44949 -2.4216726 -1.3851297 -1.3251214 -2.152951 -2.7652757 -2.8763041 -2.9273796 -3.0161057 -3.2872043 -3.4967051 -3.5525577][-4.8784814 -4.7074614 -4.5274439 -3.9998796 -3.01866 -2.0021188 -1.9574764 -2.7339516 -3.248538 -3.2522626 -3.2864208 -3.3741229 -3.5549796 -3.7185125 -3.7696013][-4.9669189 -5.0094104 -5.0498276 -4.6660781 -3.7545106 -2.8392231 -2.8384426 -3.5310874 -3.9610543 -3.9125144 -3.9460711 -3.999187 -4.0440388 -4.098279 -4.1121912][-5.0830507 -5.2732086 -5.4135151 -5.0906892 -4.2665825 -3.5293472 -3.6033344 -4.2136307 -4.5768433 -4.5104775 -4.5273175 -4.5235195 -4.461988 -4.4236579 -4.4035759][-5.1064191 -5.3390636 -5.4580116 -5.1484995 -4.4522977 -3.9280179 -4.0497875 -4.5244403 -4.7702618 -4.6732922 -4.6745777 -4.6766682 -4.6234083 -4.564774 -4.5195308][-5.0651989 -5.2914171 -5.3636608 -5.09548 -4.5883141 -4.2735858 -4.394228 -4.6774187 -4.7482977 -4.6094251 -4.6065955 -4.661756 -4.6793203 -4.6306539 -4.5412989][-4.961513 -5.136735 -5.1684203 -4.9653416 -4.6364732 -4.4557371 -4.52503 -4.6373725 -4.5969872 -4.4794326 -4.5029969 -4.5936561 -4.6441994 -4.5932364 -4.471004]]...]
INFO - root - 2017-12-06 07:31:12.744001: step 7210, loss = 0.94, batch loss = 0.87 (13.3 examples/sec; 0.601 sec/batch; 54h:16m:45s remains)
INFO - root - 2017-12-06 07:31:18.776267: step 7220, loss = 1.03, batch loss = 0.96 (13.7 examples/sec; 0.585 sec/batch; 52h:54m:06s remains)
INFO - root - 2017-12-06 07:31:24.896692: step 7230, loss = 0.97, batch loss = 0.90 (12.9 examples/sec; 0.618 sec/batch; 55h:51m:50s remains)
INFO - root - 2017-12-06 07:31:30.942909: step 7240, loss = 0.97, batch loss = 0.90 (13.4 examples/sec; 0.598 sec/batch; 53h:59m:40s remains)
INFO - root - 2017-12-06 07:31:37.051469: step 7250, loss = 0.94, batch loss = 0.87 (13.1 examples/sec; 0.612 sec/batch; 55h:19m:47s remains)
INFO - root - 2017-12-06 07:31:43.116734: step 7260, loss = 0.80, batch loss = 0.73 (13.6 examples/sec; 0.586 sec/batch; 52h:58m:35s remains)
INFO - root - 2017-12-06 07:31:49.244946: step 7270, loss = 0.91, batch loss = 0.84 (13.0 examples/sec; 0.618 sec/batch; 55h:48m:20s remains)
INFO - root - 2017-12-06 07:31:55.345915: step 7280, loss = 1.00, batch loss = 0.93 (13.4 examples/sec; 0.597 sec/batch; 53h:56m:49s remains)
INFO - root - 2017-12-06 07:32:01.111294: step 7290, loss = 0.77, batch loss = 0.70 (13.1 examples/sec; 0.611 sec/batch; 55h:13m:21s remains)
INFO - root - 2017-12-06 07:32:07.274162: step 7300, loss = 0.78, batch loss = 0.71 (13.2 examples/sec; 0.608 sec/batch; 54h:56m:10s remains)
2017-12-06 07:32:07.857153: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5614214 -4.5133548 -4.6705732 -5.0931644 -5.4269223 -5.6105576 -5.7645049 -5.7641721 -5.4372745 -4.9272041 -4.4749565 -4.3118925 -4.2041922 -4.1397605 -4.4820614][-3.8819218 -3.7349503 -3.8950853 -4.3940926 -4.859633 -5.196785 -5.4745307 -5.4382839 -4.9990377 -4.4704132 -4.0721555 -3.9837186 -3.8657024 -3.7447765 -4.0885081][-3.1519661 -2.9509237 -3.1268525 -3.6578393 -4.2070289 -4.6320324 -4.986661 -4.9834356 -4.6332035 -4.3273511 -4.0859618 -3.9781814 -3.7421162 -3.51385 -3.8242016][-2.777492 -2.5929358 -2.797389 -3.2695189 -3.7057424 -3.9505398 -4.2326045 -4.3958883 -4.4446292 -4.6158137 -4.5653114 -4.2986112 -3.8511083 -3.4800286 -3.7266474][-2.962904 -2.851933 -3.0298109 -3.2563202 -3.2531276 -2.957159 -2.9448042 -3.363404 -4.0540943 -4.8709283 -5.0945315 -4.7273989 -4.178721 -3.721962 -3.8557749][-3.5027564 -3.4335389 -3.4729719 -3.3178983 -2.6968143 -1.6617661 -1.1595526 -1.7610619 -3.1160903 -4.5762887 -5.17699 -4.9309845 -4.5113888 -4.1204548 -4.166286][-4.0361972 -4.0359297 -3.9449425 -3.4205358 -2.1957049 -0.48414135 0.52016306 -0.11338282 -1.9045067 -3.7739105 -4.7141461 -4.7656264 -4.6397862 -4.4354343 -4.4578519][-4.367362 -4.5351796 -4.4831467 -3.8389187 -2.3664696 -0.37658787 0.96474504 0.57734966 -1.1482146 -2.9938374 -4.0694876 -4.4166932 -4.5682058 -4.5337596 -4.5631385][-4.4133239 -4.7939029 -4.9511952 -4.5662413 -3.3754573 -1.5823381 -0.18521833 -0.13100719 -1.2967057 -2.7071145 -3.6299546 -4.0635471 -4.2679706 -4.2672639 -4.3677578][-4.1368666 -4.6787968 -5.1137638 -5.186214 -4.5506921 -3.1973572 -1.9636431 -1.6041105 -2.1741815 -3.0399494 -3.6176028 -3.9150913 -3.9796579 -3.8967001 -4.0833335][-3.65872 -4.1980329 -4.7923155 -5.2522492 -5.1437855 -4.2858005 -3.4144044 -3.0360775 -3.2149091 -3.6411767 -3.9429574 -4.1247892 -4.0867872 -3.8949983 -4.0406017][-3.2288766 -3.5840712 -4.0972486 -4.6759386 -4.9367027 -4.5654049 -4.1514344 -3.9378006 -3.9690182 -4.1824112 -4.3842058 -4.569088 -4.5511537 -4.2945533 -4.2944536][-3.0888028 -3.258132 -3.5349779 -3.929456 -4.28384 -4.28786 -4.2958064 -4.3284316 -4.3847704 -4.5321045 -4.6980577 -4.8825927 -4.9249692 -4.6872854 -4.5715208][-3.2116809 -3.2842457 -3.308248 -3.4030733 -3.6742089 -3.9003446 -4.1711993 -4.3545117 -4.4647989 -4.5845714 -4.686574 -4.7785587 -4.8425608 -4.7213778 -4.6223083][-3.3909073 -3.3948274 -3.2353005 -3.1130831 -3.2930117 -3.6359687 -4.007925 -4.1950784 -4.3127661 -4.4114342 -4.4237413 -4.39538 -4.4561381 -4.4944634 -4.5330968]]...]
INFO - root - 2017-12-06 07:32:13.971921: step 7310, loss = 1.09, batch loss = 1.02 (12.9 examples/sec; 0.619 sec/batch; 55h:57m:25s remains)
INFO - root - 2017-12-06 07:32:20.024598: step 7320, loss = 0.86, batch loss = 0.79 (12.9 examples/sec; 0.619 sec/batch; 55h:53m:25s remains)
INFO - root - 2017-12-06 07:32:26.122551: step 7330, loss = 0.79, batch loss = 0.72 (13.2 examples/sec; 0.608 sec/batch; 54h:54m:41s remains)
INFO - root - 2017-12-06 07:32:32.233639: step 7340, loss = 1.07, batch loss = 1.00 (12.9 examples/sec; 0.619 sec/batch; 55h:52m:59s remains)
INFO - root - 2017-12-06 07:32:38.339015: step 7350, loss = 1.06, batch loss = 0.99 (13.1 examples/sec; 0.609 sec/batch; 54h:57m:55s remains)
INFO - root - 2017-12-06 07:32:44.475120: step 7360, loss = 1.06, batch loss = 0.99 (13.1 examples/sec; 0.609 sec/batch; 55h:00m:06s remains)
INFO - root - 2017-12-06 07:32:50.564200: step 7370, loss = 1.01, batch loss = 0.94 (13.3 examples/sec; 0.601 sec/batch; 54h:15m:19s remains)
INFO - root - 2017-12-06 07:32:56.619156: step 7380, loss = 0.70, batch loss = 0.62 (13.2 examples/sec; 0.607 sec/batch; 54h:49m:58s remains)
INFO - root - 2017-12-06 07:33:02.718269: step 7390, loss = 1.03, batch loss = 0.95 (13.2 examples/sec; 0.605 sec/batch; 54h:40m:08s remains)
INFO - root - 2017-12-06 07:33:08.651225: step 7400, loss = 0.93, batch loss = 0.86 (13.4 examples/sec; 0.599 sec/batch; 54h:06m:51s remains)
2017-12-06 07:33:09.274525: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8831034 -4.3044972 -4.6120143 -4.6180873 -4.3484349 -3.8656347 -3.1156693 -2.4576569 -2.1654797 -2.2684753 -2.7201908 -3.3336186 -3.9431882 -4.4926753 -4.7512169][-4.3283892 -4.6400762 -4.9257503 -5.038383 -4.9759178 -4.5898042 -3.7675953 -2.9594846 -2.5586224 -2.6146312 -2.9587007 -3.4788208 -4.1144905 -4.6617565 -4.8617611][-4.3516059 -4.563406 -4.8777132 -5.2191544 -5.5034137 -5.3429871 -4.5851903 -3.7086535 -3.2520111 -3.2967348 -3.5535893 -3.9324412 -4.4703631 -4.9225283 -5.0259957][-4.1836286 -4.2338028 -4.4675369 -4.9387145 -5.4818525 -5.5262151 -4.9157672 -4.0878325 -3.687165 -3.8086073 -4.0463624 -4.2967281 -4.6884942 -5.0295625 -5.0555038][-4.0320468 -3.8731687 -3.903168 -4.3170586 -4.8803754 -4.9608593 -4.4352365 -3.687242 -3.4053454 -3.7206788 -4.1021953 -4.3634558 -4.6681204 -4.9089923 -4.8592906][-3.9483483 -3.6646004 -3.5105948 -3.7194076 -3.9824824 -3.7476885 -3.009356 -2.1636271 -2.0096819 -2.7233746 -3.5602918 -4.11191 -4.5155988 -4.7440124 -4.6529522][-3.9337823 -3.6655812 -3.4346125 -3.3870823 -3.1524012 -2.3495562 -1.12115 -0.00944376 0.0075850487 -1.2408934 -2.7235656 -3.7585311 -4.4273953 -4.7574019 -4.6944323][-4.0227547 -3.8669748 -3.6455371 -3.3690624 -2.7182698 -1.4693871 0.16192579 1.5088243 1.4364266 -0.19044828 -2.1520524 -3.5660539 -4.4601841 -4.9085131 -4.896225][-4.1485858 -4.1378846 -4.0147476 -3.6836505 -2.9602416 -1.7067702 -0.11038065 1.2183928 1.2493234 -0.24399996 -2.1834714 -3.6569531 -4.6099825 -5.0962515 -5.0978742][-4.2685938 -4.4144483 -4.4824581 -4.3362632 -3.8910096 -2.9796991 -1.7106624 -0.54545927 -0.27869558 -1.2651207 -2.7807822 -4.034647 -4.8842621 -5.3059683 -5.2443571][-4.3908691 -4.6704464 -4.9441543 -5.0567183 -4.9692373 -4.4481316 -3.5459008 -2.5839963 -2.1369958 -2.6151381 -3.6158061 -4.5441656 -5.2073536 -5.4966478 -5.335165][-4.4979115 -4.8399162 -5.2074556 -5.4574709 -5.5883923 -5.3509979 -4.7606006 -4.030221 -3.5547044 -3.7308209 -4.3421354 -4.9814758 -5.4405608 -5.5679436 -5.3159885][-4.5240569 -4.8170924 -5.15299 -5.4189744 -5.6335211 -5.5643654 -5.208488 -4.7117076 -4.3329573 -4.397923 -4.7627678 -5.1699986 -5.4364371 -5.4209881 -5.132442][-4.3718991 -4.5313187 -4.7699032 -5.0084109 -5.244422 -5.2790046 -5.086266 -4.7759614 -4.5224638 -4.5549774 -4.7718577 -5.0175381 -5.1572738 -5.0783124 -4.8226013][-4.1595945 -4.1665277 -4.2939992 -4.4821558 -4.6990252 -4.7876554 -4.7113566 -4.5406909 -4.4042907 -4.4365139 -4.5668797 -4.7071166 -4.7762852 -4.6963549 -4.5131235]]...]
INFO - root - 2017-12-06 07:33:15.370357: step 7410, loss = 0.83, batch loss = 0.76 (13.3 examples/sec; 0.601 sec/batch; 54h:16m:46s remains)
INFO - root - 2017-12-06 07:33:21.436551: step 7420, loss = 0.83, batch loss = 0.76 (13.0 examples/sec; 0.617 sec/batch; 55h:42m:01s remains)
INFO - root - 2017-12-06 07:33:27.609296: step 7430, loss = 1.15, batch loss = 1.08 (13.3 examples/sec; 0.602 sec/batch; 54h:19m:14s remains)
INFO - root - 2017-12-06 07:33:33.687538: step 7440, loss = 1.01, batch loss = 0.94 (12.8 examples/sec; 0.624 sec/batch; 56h:19m:57s remains)
INFO - root - 2017-12-06 07:33:39.821128: step 7450, loss = 0.94, batch loss = 0.87 (12.9 examples/sec; 0.621 sec/batch; 56h:05m:47s remains)
INFO - root - 2017-12-06 07:33:45.803356: step 7460, loss = 1.06, batch loss = 0.99 (13.5 examples/sec; 0.594 sec/batch; 53h:37m:26s remains)
INFO - root - 2017-12-06 07:33:51.900895: step 7470, loss = 0.92, batch loss = 0.85 (13.3 examples/sec; 0.600 sec/batch; 54h:10m:30s remains)
INFO - root - 2017-12-06 07:33:57.988685: step 7480, loss = 1.05, batch loss = 0.98 (13.1 examples/sec; 0.611 sec/batch; 55h:10m:00s remains)
INFO - root - 2017-12-06 07:34:04.124097: step 7490, loss = 1.06, batch loss = 0.99 (13.7 examples/sec; 0.586 sec/batch; 52h:52m:04s remains)
INFO - root - 2017-12-06 07:34:10.044284: step 7500, loss = 0.81, batch loss = 0.74 (17.1 examples/sec; 0.468 sec/batch; 42h:16m:01s remains)
2017-12-06 07:34:10.633629: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3859429 -4.4916406 -4.5431795 -4.4004993 -4.0428209 -3.585542 -3.3864684 -3.6671891 -4.164381 -4.75485 -5.5323048 -6.0391445 -5.7941289 -4.9996977 -4.1894393][-3.7416451 -3.8582351 -4.0478258 -4.1618948 -3.9732077 -3.5149455 -3.1982393 -3.2987175 -3.6842618 -4.2738276 -5.190546 -5.8932705 -5.8194103 -5.126708 -4.2665029][-3.1427231 -3.3592408 -3.8145595 -4.2620316 -4.2886157 -3.8697579 -3.4547796 -3.3037696 -3.5030069 -4.0250225 -4.9188347 -5.6550922 -5.7271061 -5.1961279 -4.3602009][-2.9325294 -3.2930627 -4.0045195 -4.7009611 -4.8432903 -4.4267979 -3.9521623 -3.6150053 -3.6706214 -4.086658 -4.738822 -5.2677865 -5.398736 -5.0667667 -4.3600883][-3.1055491 -3.6117876 -4.4923615 -5.2724581 -5.3721848 -4.8508129 -4.2489309 -3.7542675 -3.6989849 -4.0121489 -4.4009676 -4.7112384 -4.9215374 -4.8476038 -4.3376479][-3.4205716 -4.0296583 -4.9389782 -5.6150942 -5.4895496 -4.72328 -3.8863778 -3.2402227 -3.1526771 -3.4569712 -3.7826543 -4.1099439 -4.5332036 -4.720613 -4.3353477][-3.4979579 -4.1100211 -4.9561725 -5.4475183 -5.0733752 -4.0255308 -2.890409 -2.1278181 -2.0982575 -2.5756688 -3.0943894 -3.6827655 -4.3920097 -4.7199473 -4.2817764][-3.1268458 -3.7503686 -4.5929728 -4.969862 -4.4340367 -3.1943803 -1.8517137 -1.0787287 -1.17155 -1.9110661 -2.7356477 -3.6165843 -4.5073953 -4.7963042 -4.1581225][-2.7843919 -3.4535117 -4.3306017 -4.7185693 -4.1858287 -2.932647 -1.5783727 -0.8561306 -0.98190451 -1.8061078 -2.8019395 -3.8272102 -4.7620063 -4.9599524 -4.1428313][-3.0165632 -3.6360178 -4.4451985 -4.8402839 -4.3991141 -3.3054228 -2.2059286 -1.6858449 -1.7572873 -2.4061477 -3.2567191 -4.1600504 -5.0000653 -5.1906033 -4.4316912][-3.7431641 -4.1727047 -4.7607203 -5.1006823 -4.7597623 -3.87833 -3.1063628 -2.8382535 -2.8971291 -3.3335905 -3.8848844 -4.5027032 -5.1589737 -5.4125481 -4.9169149][-4.4739194 -4.6743279 -5.0206394 -5.2883744 -5.0289721 -4.3269053 -3.7491477 -3.6048508 -3.661222 -3.9779763 -4.3574524 -4.7757549 -5.297369 -5.6030669 -5.3713927][-4.8902049 -4.9755249 -5.2051759 -5.4450111 -5.2732682 -4.7327948 -4.2139845 -4.0132823 -3.9982839 -4.2453709 -4.583333 -4.9291477 -5.3825922 -5.7019286 -5.6440477][-4.9037623 -4.9359064 -5.1012797 -5.3182974 -5.2470264 -4.9020109 -4.4600997 -4.2132258 -4.1177778 -4.2639756 -4.5401096 -4.8183131 -5.1990747 -5.4982777 -5.5309372][-4.5487385 -4.4848366 -4.5516257 -4.7008066 -4.7085471 -4.5536633 -4.2494941 -4.0492287 -3.9749496 -4.0840039 -4.2971969 -4.4867482 -4.7697282 -5.030839 -5.1192675]]...]
INFO - root - 2017-12-06 07:34:16.713546: step 7510, loss = 0.99, batch loss = 0.92 (13.6 examples/sec; 0.588 sec/batch; 53h:06m:07s remains)
INFO - root - 2017-12-06 07:34:22.833717: step 7520, loss = 1.03, batch loss = 0.96 (13.0 examples/sec; 0.616 sec/batch; 55h:35m:04s remains)
INFO - root - 2017-12-06 07:34:28.912148: step 7530, loss = 1.01, batch loss = 0.94 (13.2 examples/sec; 0.605 sec/batch; 54h:37m:54s remains)
INFO - root - 2017-12-06 07:34:34.974245: step 7540, loss = 1.00, batch loss = 0.93 (13.1 examples/sec; 0.612 sec/batch; 55h:16m:12s remains)
INFO - root - 2017-12-06 07:34:41.047380: step 7550, loss = 1.23, batch loss = 1.16 (13.1 examples/sec; 0.610 sec/batch; 55h:01m:09s remains)
INFO - root - 2017-12-06 07:34:47.147610: step 7560, loss = 0.92, batch loss = 0.85 (13.4 examples/sec; 0.595 sec/batch; 53h:42m:59s remains)
INFO - root - 2017-12-06 07:34:53.198594: step 7570, loss = 1.23, batch loss = 1.16 (13.0 examples/sec; 0.614 sec/batch; 55h:24m:03s remains)
INFO - root - 2017-12-06 07:34:59.314463: step 7580, loss = 0.98, batch loss = 0.91 (12.9 examples/sec; 0.622 sec/batch; 56h:05m:44s remains)
INFO - root - 2017-12-06 07:35:05.309737: step 7590, loss = 0.89, batch loss = 0.82 (13.6 examples/sec; 0.588 sec/batch; 53h:03m:14s remains)
INFO - root - 2017-12-06 07:35:11.457563: step 7600, loss = 0.95, batch loss = 0.88 (13.2 examples/sec; 0.605 sec/batch; 54h:34m:54s remains)
2017-12-06 07:35:12.015500: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.6532903 -5.359025 -5.0218906 -4.44872 -3.9183133 -3.7800794 -4.161684 -4.313427 -4.0056329 -3.9503269 -4.3714352 -4.6844468 -4.8529148 -5.1932793 -5.3399][-5.6662779 -5.6082587 -5.4537926 -5.1320939 -4.8146944 -4.7574997 -5.158278 -5.2590618 -4.8797755 -4.791842 -5.1777844 -5.3344555 -5.3140712 -5.5743833 -5.6732435][-5.5950365 -5.6953135 -5.71803 -5.6737552 -5.543201 -5.4575357 -5.6617155 -5.5614715 -5.1447783 -5.1621757 -5.6699586 -5.8362722 -5.7811513 -6.0122828 -6.0165191][-5.336031 -5.5113125 -5.738349 -5.9646029 -5.9297147 -5.6718454 -5.4334774 -4.9408054 -4.4714489 -4.6568885 -5.3655119 -5.6942196 -5.8150158 -6.1562581 -6.1404309][-4.9372659 -5.0681143 -5.4483271 -5.8889213 -5.8674517 -5.3325472 -4.4663057 -3.4223752 -2.8732429 -3.2941661 -4.301352 -4.9459033 -5.3985825 -5.9477592 -6.0086541][-4.8266745 -4.6903658 -4.9802914 -5.4529977 -5.358717 -4.5211444 -3.0353997 -1.4586368 -0.85459018 -1.573137 -2.9201212 -3.9282222 -4.7212505 -5.4789233 -5.6965303][-5.0184431 -4.5716357 -4.6344252 -5.0176868 -4.8335538 -3.7720351 -1.898488 -0.022414207 0.57863665 -0.37884235 -1.9136252 -3.0951073 -4.0229378 -4.8568177 -5.2329774][-5.0085163 -4.4292207 -4.3854113 -4.7372909 -4.5851378 -3.5275993 -1.679307 0.10499907 0.61794424 -0.41281033 -1.8791602 -2.9868884 -3.8213 -4.5378304 -4.9118915][-4.9035254 -4.3653746 -4.3739839 -4.7513919 -4.6940532 -3.7944741 -2.2677121 -0.87732387 -0.55792403 -1.5624299 -2.8375132 -3.7467926 -4.3639822 -4.8082151 -4.96708][-5.0586562 -4.6139212 -4.655931 -4.9518509 -4.8953071 -4.1777306 -3.0704234 -2.1493213 -2.0380096 -2.9426537 -4.0002394 -4.7323675 -5.1812353 -5.3598676 -5.2169256][-5.4132566 -5.0261636 -4.9916553 -5.1109953 -5.0106316 -4.4964414 -3.7927027 -3.2593081 -3.2690394 -3.9559152 -4.720046 -5.266108 -5.5955772 -5.6137638 -5.2742567][-5.7689033 -5.451736 -5.3549576 -5.3712969 -5.3174391 -5.0533481 -4.6769176 -4.3720427 -4.3423672 -4.6848884 -5.0872169 -5.4169092 -5.6274843 -5.5565782 -5.1369824][-5.9099579 -5.7184582 -5.6623569 -5.69098 -5.722734 -5.6434417 -5.4654703 -5.2707791 -5.1795273 -5.2629375 -5.3894253 -5.5125074 -5.5609035 -5.3861132 -4.9282379][-5.5761218 -5.493506 -5.5080166 -5.5741649 -5.6421022 -5.6368895 -5.5560365 -5.446856 -5.3821392 -5.3959718 -5.4166293 -5.4100361 -5.3160634 -5.0581269 -4.6180868][-4.79934 -4.7627816 -4.7993469 -4.8636293 -4.912406 -4.9082928 -4.86594 -4.8251424 -4.8277464 -4.8741417 -4.8974833 -4.85956 -4.729075 -4.4939532 -4.1754556]]...]
INFO - root - 2017-12-06 07:35:17.973280: step 7610, loss = 0.73, batch loss = 0.66 (13.3 examples/sec; 0.601 sec/batch; 54h:15m:40s remains)
INFO - root - 2017-12-06 07:35:24.007647: step 7620, loss = 0.88, batch loss = 0.81 (13.4 examples/sec; 0.598 sec/batch; 53h:58m:53s remains)
INFO - root - 2017-12-06 07:35:29.973051: step 7630, loss = 1.00, batch loss = 0.93 (13.2 examples/sec; 0.605 sec/batch; 54h:33m:48s remains)
INFO - root - 2017-12-06 07:35:36.035973: step 7640, loss = 0.76, batch loss = 0.69 (13.3 examples/sec; 0.600 sec/batch; 54h:06m:18s remains)
INFO - root - 2017-12-06 07:35:41.981844: step 7650, loss = 0.87, batch loss = 0.80 (13.2 examples/sec; 0.608 sec/batch; 54h:49m:44s remains)
INFO - root - 2017-12-06 07:35:48.152967: step 7660, loss = 0.69, batch loss = 0.62 (13.2 examples/sec; 0.604 sec/batch; 54h:29m:47s remains)
INFO - root - 2017-12-06 07:35:54.259708: step 7670, loss = 0.83, batch loss = 0.76 (13.0 examples/sec; 0.616 sec/batch; 55h:33m:35s remains)
INFO - root - 2017-12-06 07:36:00.364633: step 7680, loss = 1.00, batch loss = 0.93 (13.4 examples/sec; 0.595 sec/batch; 53h:40m:06s remains)
INFO - root - 2017-12-06 07:36:06.415657: step 7690, loss = 0.98, batch loss = 0.91 (13.0 examples/sec; 0.614 sec/batch; 55h:26m:08s remains)
INFO - root - 2017-12-06 07:36:12.439429: step 7700, loss = 1.15, batch loss = 1.08 (13.1 examples/sec; 0.610 sec/batch; 55h:02m:58s remains)
2017-12-06 07:36:13.053063: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5245602 -3.539911 -3.9568973 -4.685822 -5.2828565 -5.4877658 -5.5898886 -5.518085 -5.4193406 -5.3986025 -5.492003 -5.6268268 -5.59513 -5.2766266 -4.7177978][-2.4787574 -2.6393476 -3.33142 -4.3615217 -5.1557283 -5.4137287 -5.5416775 -5.5030842 -5.4332137 -5.3595324 -5.3986287 -5.5353708 -5.4910827 -5.0908 -4.3682175][-1.6287744 -2.0160491 -2.9557405 -4.1558461 -4.9969888 -5.2029314 -5.2222967 -5.135664 -5.1020436 -5.0374231 -5.0889478 -5.2413635 -5.1654282 -4.7006516 -3.8672895][-1.1954958 -1.8114924 -2.8381925 -3.975399 -4.6760206 -4.7377253 -4.5382557 -4.357697 -4.4261155 -4.5070972 -4.69822 -4.9361124 -4.8783917 -4.4326763 -3.600508][-1.5128798 -2.2037854 -3.0473096 -3.8439391 -4.2273192 -4.0892844 -3.6460457 -3.3904161 -3.6430881 -3.9678352 -4.3581142 -4.7244182 -4.7251954 -4.351954 -3.6106162][-2.3092422 -2.9182329 -3.4119473 -3.7219443 -3.7067969 -3.3947372 -2.8026748 -2.5006938 -2.9038296 -3.4387808 -3.9967084 -4.487843 -4.5665174 -4.3098273 -3.72277][-2.8966889 -3.3599105 -3.5257249 -3.4110699 -3.0326085 -2.5916622 -1.9907084 -1.6748762 -2.1071777 -2.7641976 -3.4634759 -4.0612941 -4.2147522 -4.1201792 -3.7218475][-3.1913233 -3.5538032 -3.5553026 -3.2675309 -2.6886744 -2.1581242 -1.6106365 -1.2631052 -1.5775895 -2.2552373 -3.0668478 -3.6942272 -3.868273 -3.948287 -3.7278674][-3.3087997 -3.6818864 -3.7086949 -3.5035067 -2.9827223 -2.4776626 -2.0085065 -1.6069746 -1.7185457 -2.3345425 -3.1738834 -3.7108622 -3.8224509 -4.0282483 -3.9520257][-3.1915364 -3.6565194 -3.7892723 -3.7679086 -3.4868331 -3.1484098 -2.8269854 -2.4869215 -2.5160351 -3.05347 -3.7928555 -4.1729488 -4.1889095 -4.3717265 -4.3024459][-3.0928354 -3.5950394 -3.7693889 -3.8721652 -3.8506708 -3.7543218 -3.6641128 -3.5060437 -3.560698 -3.9759037 -4.4969416 -4.7081804 -4.646615 -4.7186627 -4.588788][-3.2642229 -3.735435 -3.8812597 -4.020596 -4.190064 -4.3372259 -4.4766493 -4.4660287 -4.4676356 -4.6159778 -4.7987485 -4.8171482 -4.7109513 -4.7132092 -4.5984287][-3.5134561 -3.9262447 -4.0109577 -4.130693 -4.4051528 -4.7439241 -5.0628357 -5.1406131 -5.04527 -4.9398971 -4.82905 -4.6708755 -4.5247173 -4.492424 -4.4309139][-3.6990066 -4.0226283 -4.0308056 -4.0712452 -4.3447223 -4.7717872 -5.1980667 -5.3521457 -5.2424259 -5.0486298 -4.8196731 -4.5816646 -4.402832 -4.3276105 -4.26823][-3.8775051 -4.1008229 -4.0799923 -4.0803223 -4.3107061 -4.7108784 -5.1111956 -5.2573147 -5.1457152 -4.9378986 -4.6868572 -4.4400578 -4.2625194 -4.170867 -4.1046686]]...]
INFO - root - 2017-12-06 07:36:19.091869: step 7710, loss = 1.02, batch loss = 0.95 (13.2 examples/sec; 0.608 sec/batch; 54h:51m:13s remains)
INFO - root - 2017-12-06 07:36:25.040446: step 7720, loss = 0.93, batch loss = 0.86 (13.3 examples/sec; 0.603 sec/batch; 54h:22m:39s remains)
INFO - root - 2017-12-06 07:36:31.076693: step 7730, loss = 0.84, batch loss = 0.77 (13.3 examples/sec; 0.603 sec/batch; 54h:21m:32s remains)
INFO - root - 2017-12-06 07:36:37.041485: step 7740, loss = 1.03, batch loss = 0.96 (12.9 examples/sec; 0.618 sec/batch; 55h:44m:15s remains)
INFO - root - 2017-12-06 07:36:43.134610: step 7750, loss = 1.10, batch loss = 1.03 (13.3 examples/sec; 0.600 sec/batch; 54h:07m:03s remains)
INFO - root - 2017-12-06 07:36:49.135214: step 7760, loss = 1.30, batch loss = 1.22 (13.5 examples/sec; 0.591 sec/batch; 53h:19m:48s remains)
INFO - root - 2017-12-06 07:36:55.286612: step 7770, loss = 0.75, batch loss = 0.68 (12.8 examples/sec; 0.626 sec/batch; 56h:29m:53s remains)
INFO - root - 2017-12-06 07:37:01.341806: step 7780, loss = 0.96, batch loss = 0.89 (13.9 examples/sec; 0.575 sec/batch; 51h:50m:39s remains)
INFO - root - 2017-12-06 07:37:07.439545: step 7790, loss = 1.05, batch loss = 0.98 (13.4 examples/sec; 0.598 sec/batch; 53h:57m:45s remains)
INFO - root - 2017-12-06 07:37:13.464700: step 7800, loss = 0.90, batch loss = 0.83 (13.6 examples/sec; 0.590 sec/batch; 53h:13m:20s remains)
2017-12-06 07:37:14.024594: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0966916 -3.907573 -3.5064163 -3.2993479 -3.6439731 -4.2957363 -4.7836375 -5.0944028 -5.1815186 -5.1361952 -4.9663935 -4.6425853 -4.646637 -4.9367437 -5.1382794][-3.8740449 -3.5122252 -3.0208576 -2.7325473 -2.9649086 -3.6074657 -4.0970874 -4.3614545 -4.4366097 -4.4548354 -4.398313 -4.1651263 -4.1455221 -4.399374 -4.6378026][-3.4979947 -3.0169179 -2.5073247 -2.1776168 -2.2971315 -2.8866196 -3.3575823 -3.5373483 -3.5227702 -3.5544689 -3.6048074 -3.5113053 -3.5396438 -3.7950997 -4.0911465][-3.2044978 -2.7163923 -2.2267048 -1.8947933 -1.9438503 -2.4035721 -2.763433 -2.8017621 -2.6328204 -2.6324117 -2.7644134 -2.8163948 -2.9430671 -3.2407923 -3.6279049][-3.2966614 -2.9039397 -2.4259758 -2.0454538 -1.9773469 -2.177192 -2.3275368 -2.2261834 -1.9508679 -1.947638 -2.157038 -2.3419197 -2.5811925 -2.9164047 -3.3685009][-3.7424762 -3.470588 -2.9849129 -2.5083637 -2.2471926 -2.0910237 -2.0067356 -1.8541915 -1.5955341 -1.6404073 -1.9155233 -2.1700847 -2.4603148 -2.7472272 -3.1470835][-4.2812233 -4.1261649 -3.6451585 -3.0907636 -2.6547146 -2.1936927 -1.9325213 -1.8392608 -1.7412589 -1.896342 -2.191925 -2.3937566 -2.5958741 -2.713419 -2.9458394][-4.7468972 -4.7134619 -4.279871 -3.7089462 -3.2001719 -2.6168804 -2.2952962 -2.3175623 -2.4415677 -2.7300825 -3.012918 -3.072454 -3.0589309 -2.9169493 -2.9074626][-4.9569893 -5.0668344 -4.7465472 -4.2441025 -3.7873356 -3.2835255 -3.0213199 -3.1338964 -3.41211 -3.7659824 -3.9851918 -3.8602657 -3.6050053 -3.2432446 -3.0407827][-4.834446 -5.0916071 -4.9387221 -4.58639 -4.2747908 -3.9663484 -3.8254936 -3.970118 -4.2643414 -4.5803571 -4.69451 -4.4113269 -3.9933596 -3.5571833 -3.3098617][-4.63742 -4.9903159 -4.9754038 -4.7680812 -4.6003656 -4.4734149 -4.44323 -4.5687275 -4.78293 -5.0074854 -5.0302553 -4.6803122 -4.2129188 -3.8333576 -3.65415][-4.5725751 -4.9185863 -4.9605937 -4.8484697 -4.77876 -4.7490478 -4.7695303 -4.8502822 -4.9655423 -5.0936022 -5.0758624 -4.7642627 -4.37523 -4.1358123 -4.0668397][-4.559649 -4.8265896 -4.8690634 -4.8147049 -4.8023353 -4.8027811 -4.8207374 -4.8561993 -4.8915758 -4.9421167 -4.9220362 -4.7165833 -4.4861808 -4.4182291 -4.4561529][-4.5845323 -4.7550206 -4.7824187 -4.7625275 -4.7726936 -4.7675743 -4.7609553 -4.7604227 -4.7522907 -4.7644849 -4.7591786 -4.6673913 -4.5897655 -4.6417289 -4.7414985][-4.6252103 -4.7089815 -4.7180223 -4.710248 -4.7209606 -4.7144704 -4.6989727 -4.684545 -4.6651855 -4.6654382 -4.6738873 -4.6604 -4.6709542 -4.7542386 -4.8520918]]...]
INFO - root - 2017-12-06 07:37:20.125015: step 7810, loss = 0.77, batch loss = 0.70 (13.3 examples/sec; 0.602 sec/batch; 54h:19m:58s remains)
INFO - root - 2017-12-06 07:37:26.119959: step 7820, loss = 0.85, batch loss = 0.78 (15.0 examples/sec; 0.532 sec/batch; 48h:01m:14s remains)
INFO - root - 2017-12-06 07:37:32.210317: step 7830, loss = 0.81, batch loss = 0.74 (13.3 examples/sec; 0.601 sec/batch; 54h:13m:10s remains)
INFO - root - 2017-12-06 07:37:38.220371: step 7840, loss = 0.86, batch loss = 0.79 (13.3 examples/sec; 0.600 sec/batch; 54h:07m:55s remains)
INFO - root - 2017-12-06 07:37:44.307188: step 7850, loss = 1.04, batch loss = 0.97 (13.1 examples/sec; 0.613 sec/batch; 55h:16m:35s remains)
INFO - root - 2017-12-06 07:37:50.347619: step 7860, loss = 0.78, batch loss = 0.71 (13.7 examples/sec; 0.585 sec/batch; 52h:47m:08s remains)
INFO - root - 2017-12-06 07:37:56.383991: step 7870, loss = 0.84, batch loss = 0.77 (13.0 examples/sec; 0.616 sec/batch; 55h:30m:36s remains)
INFO - root - 2017-12-06 07:38:02.479556: step 7880, loss = 0.86, batch loss = 0.79 (13.1 examples/sec; 0.612 sec/batch; 55h:08m:29s remains)
INFO - root - 2017-12-06 07:38:08.461711: step 7890, loss = 1.24, batch loss = 1.17 (13.2 examples/sec; 0.605 sec/batch; 54h:33m:54s remains)
INFO - root - 2017-12-06 07:38:14.457345: step 7900, loss = 0.95, batch loss = 0.88 (12.8 examples/sec; 0.624 sec/batch; 56h:13m:16s remains)
2017-12-06 07:38:15.001291: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7567096 -4.6247659 -4.562758 -4.6929016 -4.9311051 -5.0223708 -4.9843326 -4.9844141 -5.056427 -5.1361532 -5.1740546 -5.1659055 -5.1255851 -4.9967294 -4.8735533][-4.4303279 -4.1852822 -4.1481862 -4.4175577 -4.8360434 -5.0487432 -5.0537071 -5.0935516 -5.2193031 -5.3925772 -5.5510387 -5.626123 -5.5783687 -5.3760686 -5.1411424][-3.9087636 -3.5665534 -3.5968456 -4.016932 -4.5915632 -4.8847651 -4.8692226 -4.898262 -5.062695 -5.3430262 -5.6621242 -5.8619242 -5.8294215 -5.5845637 -5.2679992][-3.1804991 -2.8266776 -2.961946 -3.5044422 -4.1760359 -4.500422 -4.394001 -4.3437238 -4.5090218 -4.86983 -5.3327217 -5.6679907 -5.686852 -5.4711037 -5.1714153][-2.4217715 -2.1634843 -2.4482424 -3.0888834 -3.7943194 -4.104928 -3.8723693 -3.6748297 -3.7677462 -4.1363268 -4.6560044 -5.0642157 -5.1356354 -4.9970651 -4.7812428][-1.7959163 -1.6785266 -2.1260643 -2.8554811 -3.5549784 -3.8454232 -3.5556874 -3.2277789 -3.186007 -3.4548316 -3.9224944 -4.3188238 -4.4081492 -4.3575563 -4.2615619][-1.5187511 -1.5012023 -2.0465066 -2.8093166 -3.4633763 -3.7600791 -3.5307255 -3.1444592 -2.9521279 -3.0609612 -3.429961 -3.7664697 -3.8342154 -3.8472977 -3.8624945][-1.5730836 -1.5863571 -2.1234558 -2.8268757 -3.3864627 -3.7000647 -3.6028817 -3.2551215 -2.9899812 -2.9552493 -3.2358155 -3.5004196 -3.5032568 -3.5179267 -3.6009488][-1.6619081 -1.652952 -2.1429942 -2.7801056 -3.2692642 -3.626781 -3.6969404 -3.4652903 -3.2020783 -3.0547051 -3.2304628 -3.3749533 -3.2296968 -3.1461327 -3.2310786][-1.627058 -1.5434573 -1.9877396 -2.6050363 -3.0881219 -3.5274911 -3.7514074 -3.6448288 -3.3924437 -3.1412768 -3.1753321 -3.1375127 -2.7952766 -2.5694573 -2.6357579][-1.5076268 -1.2952337 -1.7054079 -2.3551478 -2.8914473 -3.4181194 -3.7249074 -3.6510425 -3.341238 -2.9889188 -2.8697779 -2.6531425 -2.1895511 -1.9075394 -2.0134714][-1.5881751 -1.2275448 -1.5896213 -2.2725286 -2.883399 -3.46631 -3.7600646 -3.6112719 -3.1803863 -2.7294891 -2.4735885 -2.1610618 -1.7443416 -1.5499349 -1.7675462][-2.0243659 -1.5384064 -1.8013923 -2.4610016 -3.118062 -3.7159648 -3.9410696 -3.6636183 -3.1078734 -2.5929523 -2.2887812 -2.0291166 -1.8137414 -1.7958283 -2.1399856][-2.6345289 -2.0889649 -2.2335484 -2.8017764 -3.4525807 -4.04651 -4.2278829 -3.8597889 -3.2419806 -2.743042 -2.4925051 -2.3825533 -2.3786449 -2.4987659 -2.9096918][-3.3638024 -2.8531711 -2.8983493 -3.3167653 -3.8656874 -4.3949022 -4.5448966 -4.1511774 -3.5379803 -3.1058879 -2.9489934 -2.9863148 -3.1273739 -3.3138328 -3.7141881]]...]
INFO - root - 2017-12-06 07:38:21.010357: step 7910, loss = 0.94, batch loss = 0.87 (13.5 examples/sec; 0.592 sec/batch; 53h:22m:04s remains)
INFO - root - 2017-12-06 07:38:27.061399: step 7920, loss = 0.99, batch loss = 0.92 (13.2 examples/sec; 0.607 sec/batch; 54h:41m:38s remains)
INFO - root - 2017-12-06 07:38:32.951176: step 7930, loss = 0.94, batch loss = 0.87 (13.2 examples/sec; 0.606 sec/batch; 54h:38m:50s remains)
INFO - root - 2017-12-06 07:38:39.059953: step 7940, loss = 0.90, batch loss = 0.83 (13.2 examples/sec; 0.608 sec/batch; 54h:46m:35s remains)
INFO - root - 2017-12-06 07:38:45.141720: step 7950, loss = 1.10, batch loss = 1.03 (12.8 examples/sec; 0.625 sec/batch; 56h:20m:39s remains)
INFO - root - 2017-12-06 07:38:51.113291: step 7960, loss = 1.21, batch loss = 1.14 (13.7 examples/sec; 0.583 sec/batch; 52h:31m:29s remains)
INFO - root - 2017-12-06 07:38:57.168067: step 7970, loss = 0.88, batch loss = 0.81 (13.2 examples/sec; 0.607 sec/batch; 54h:45m:17s remains)
INFO - root - 2017-12-06 07:39:03.274649: step 7980, loss = 1.04, batch loss = 0.97 (12.7 examples/sec; 0.632 sec/batch; 56h:55m:56s remains)
INFO - root - 2017-12-06 07:39:09.482360: step 7990, loss = 0.83, batch loss = 0.76 (12.5 examples/sec; 0.638 sec/batch; 57h:28m:13s remains)
INFO - root - 2017-12-06 07:39:15.590708: step 8000, loss = 0.65, batch loss = 0.58 (13.1 examples/sec; 0.609 sec/batch; 54h:53m:08s remains)
2017-12-06 07:39:16.163682: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2737551 -3.4004753 -3.6423261 -3.9642873 -4.1681767 -4.2404423 -4.2501168 -4.092752 -3.7228227 -3.4650891 -3.6100755 -4.0340376 -4.4491553 -4.6733675 -4.8358269][-3.732899 -3.8291728 -4.0116162 -4.2635226 -4.4459233 -4.4872904 -4.4475632 -4.2387314 -3.8180974 -3.5623002 -3.7803032 -4.2812276 -4.7051039 -4.8108168 -4.7592297][-4.2998896 -4.3154244 -4.3830514 -4.5371618 -4.7282586 -4.7310395 -4.6111946 -4.3510189 -3.8898797 -3.6302862 -3.911608 -4.4439521 -4.8116894 -4.7265048 -4.3628216][-4.6146812 -4.5134244 -4.4604397 -4.5104012 -4.6950803 -4.5928617 -4.3152609 -3.9756463 -3.4689713 -3.22722 -3.6154478 -4.20896 -4.5452213 -4.3251176 -3.7341676][-4.4018345 -4.1650305 -3.9959664 -3.9309449 -4.0611606 -3.794693 -3.3493786 -2.9912758 -2.5379484 -2.425565 -3.0029812 -3.6963696 -4.0351949 -3.7864428 -3.1565123][-3.5889158 -3.2585778 -2.9884808 -2.7987728 -2.8554521 -2.481951 -1.9831536 -1.758409 -1.5261779 -1.6940286 -2.5112443 -3.2437413 -3.5328493 -3.3164134 -2.8062539][-2.3803992 -2.1350324 -1.9169319 -1.7084618 -1.769732 -1.429492 -1.0407565 -1.0678864 -1.1006823 -1.4766257 -2.3508558 -2.9346986 -3.0649014 -2.8886907 -2.5750866][-1.4010754 -1.3836696 -1.3687508 -1.2706668 -1.4198992 -1.2318351 -1.0573924 -1.330075 -1.5178001 -1.8687887 -2.5313559 -2.8037302 -2.7318764 -2.5960908 -2.4572895][-1.1471558 -1.3012638 -1.4723489 -1.5453982 -1.833313 -1.8697164 -1.9033844 -2.2762265 -2.4470441 -2.5914614 -2.8670616 -2.7435274 -2.48918 -2.4016163 -2.3925884][-1.5767555 -1.8071644 -2.0659728 -2.3045847 -2.7449055 -2.9921403 -3.1393418 -3.4172878 -3.4161875 -3.3073218 -3.2102656 -2.7567823 -2.3884714 -2.3368974 -2.3828361][-2.4148076 -2.6246312 -2.8410368 -3.1328573 -3.6256175 -3.9748333 -4.1292405 -4.2153373 -4.0027165 -3.6963949 -3.3847585 -2.8292446 -2.4815145 -2.4461379 -2.4711657][-3.3117421 -3.4379015 -3.536885 -3.7715032 -4.1971273 -4.5246119 -4.6234746 -4.5526152 -4.21688 -3.8222423 -3.4457388 -2.9727187 -2.6958823 -2.5953479 -2.5272515][-4.1692324 -4.1702046 -4.1266308 -4.2291164 -4.4967785 -4.7132578 -4.7511034 -4.6342344 -4.3266368 -3.9739816 -3.6309996 -3.2552304 -2.9589958 -2.6817956 -2.4513731][-4.8784914 -4.7651539 -4.5833721 -4.50871 -4.5662565 -4.6422925 -4.6712952 -4.6490226 -4.5164466 -4.3276439 -4.0727243 -3.7207534 -3.3090768 -2.8038993 -2.3847826][-5.1959462 -5.0176721 -4.7529006 -4.5393734 -4.4095831 -4.3571382 -4.426466 -4.5916963 -4.7179894 -4.7630491 -4.633852 -4.27611 -3.7545881 -3.0899286 -2.5201869]]...]
INFO - root - 2017-12-06 07:39:22.198788: step 8010, loss = 1.10, batch loss = 1.03 (13.1 examples/sec; 0.609 sec/batch; 54h:55m:03s remains)
INFO - root - 2017-12-06 07:39:28.221945: step 8020, loss = 0.91, batch loss = 0.84 (13.4 examples/sec; 0.596 sec/batch; 53h:44m:06s remains)
INFO - root - 2017-12-06 07:39:34.357901: step 8030, loss = 0.97, batch loss = 0.90 (13.1 examples/sec; 0.611 sec/batch; 55h:05m:18s remains)
INFO - root - 2017-12-06 07:39:40.134241: step 8040, loss = 0.84, batch loss = 0.77 (13.0 examples/sec; 0.618 sec/batch; 55h:40m:28s remains)
INFO - root - 2017-12-06 07:39:46.250273: step 8050, loss = 0.76, batch loss = 0.69 (12.7 examples/sec; 0.630 sec/batch; 56h:47m:30s remains)
INFO - root - 2017-12-06 07:39:52.229396: step 8060, loss = 0.77, batch loss = 0.70 (12.6 examples/sec; 0.636 sec/batch; 57h:17m:33s remains)
INFO - root - 2017-12-06 07:39:58.189514: step 8070, loss = 1.10, batch loss = 1.03 (14.4 examples/sec; 0.556 sec/batch; 50h:04m:05s remains)
INFO - root - 2017-12-06 07:40:04.231513: step 8080, loss = 1.02, batch loss = 0.95 (13.5 examples/sec; 0.593 sec/batch; 53h:26m:48s remains)
INFO - root - 2017-12-06 07:40:10.373112: step 8090, loss = 0.84, batch loss = 0.77 (13.5 examples/sec; 0.591 sec/batch; 53h:16m:23s remains)
INFO - root - 2017-12-06 07:40:16.502804: step 8100, loss = 0.95, batch loss = 0.88 (13.1 examples/sec; 0.610 sec/batch; 54h:56m:49s remains)
2017-12-06 07:40:17.055352: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0133786 -3.991169 -3.9540441 -3.9281864 -4.0162983 -4.2670622 -4.4531441 -4.3922763 -4.2064195 -4.0777216 -3.8294585 -3.409466 -3.2130065 -3.3696346 -3.6739397][-4.1849694 -4.1163969 -3.9753194 -3.7624617 -3.7558713 -3.9642329 -4.1358871 -4.0833864 -3.949719 -3.8809314 -3.6349378 -3.199419 -2.9835587 -3.1164169 -3.4420228][-4.3667154 -4.3946395 -4.22975 -3.8140664 -3.624135 -3.7128224 -3.8590117 -3.852468 -3.83006 -3.8558402 -3.6254206 -3.2256966 -3.0427823 -3.1541719 -3.4632366][-4.3502607 -4.6685357 -4.6475606 -4.1425247 -3.7686934 -3.6545243 -3.6856732 -3.6498885 -3.6849849 -3.7791748 -3.602782 -3.3455276 -3.2973237 -3.4230781 -3.69244][-3.9908679 -4.7335567 -5.0045366 -4.5793424 -4.0881009 -3.7149055 -3.4773374 -3.2647555 -3.25497 -3.3846474 -3.3304207 -3.3244138 -3.5150993 -3.7126961 -3.9417624][-3.3337188 -4.4746909 -5.0667825 -4.8335056 -4.2960253 -3.6278791 -2.9796469 -2.458154 -2.3272181 -2.490854 -2.6344595 -2.9428205 -3.4302557 -3.7698762 -4.0156269][-2.7292354 -4.0679703 -4.8332682 -4.7528477 -4.1514282 -3.1654592 -2.0578239 -1.1977994 -0.9448359 -1.1795833 -1.6120663 -2.2594614 -3.0398076 -3.5769582 -3.9189298][-2.5546589 -3.80189 -4.44502 -4.3298526 -3.6151445 -2.4002044 -0.98605156 0.065265179 0.34011126 -0.00671196 -0.70572329 -1.5786166 -2.5312033 -3.2533298 -3.7441602][-2.9417479 -3.84649 -4.1123514 -3.7880576 -2.9938383 -1.7988412 -0.42588949 0.5450573 0.74726343 0.35218668 -0.4410193 -1.3307731 -2.2535284 -3.0459592 -3.6445231][-3.6622548 -4.1397624 -3.9893191 -3.4830267 -2.7824895 -1.9197268 -0.929651 -0.24615335 -0.1162734 -0.43914366 -1.1314521 -1.8536611 -2.5675468 -3.2522821 -3.7951856][-4.3379264 -4.4695826 -4.0605078 -3.5181923 -3.0581269 -2.6920366 -2.2465942 -1.9314253 -1.8459837 -2.0082674 -2.46907 -2.9200749 -3.3358011 -3.7732797 -4.1144719][-4.7238483 -4.6405506 -4.1685028 -3.7236919 -3.5374374 -3.6266143 -3.6826847 -3.6683702 -3.5716505 -3.5434551 -3.7443819 -3.9203818 -4.0572438 -4.2322955 -4.3473558][-4.7488041 -4.5768723 -4.1841292 -3.9140306 -3.9532704 -4.3190255 -4.6706753 -4.8015542 -4.6622334 -4.4873042 -4.4922972 -4.48639 -4.45599 -4.4447927 -4.3895879][-4.621449 -4.4680805 -4.2498474 -4.1687346 -4.3418627 -4.765882 -5.1639953 -5.2806835 -5.0970039 -4.86644 -4.792079 -4.7176733 -4.6098957 -4.4778175 -4.2928352][-4.5499439 -4.455503 -4.4395776 -4.5336986 -4.7679381 -5.1068621 -5.3660278 -5.3491821 -5.1000943 -4.8746176 -4.813026 -4.7703152 -4.6673617 -4.4622517 -4.1654592]]...]
INFO - root - 2017-12-06 07:40:23.175403: step 8110, loss = 0.82, batch loss = 0.75 (12.8 examples/sec; 0.625 sec/batch; 56h:16m:49s remains)
INFO - root - 2017-12-06 07:40:29.192248: step 8120, loss = 0.82, batch loss = 0.75 (13.3 examples/sec; 0.603 sec/batch; 54h:20m:11s remains)
INFO - root - 2017-12-06 07:40:35.333810: step 8130, loss = 1.16, batch loss = 1.09 (13.1 examples/sec; 0.612 sec/batch; 55h:06m:16s remains)
INFO - root - 2017-12-06 07:40:41.325981: step 8140, loss = 0.80, batch loss = 0.73 (13.6 examples/sec; 0.589 sec/batch; 53h:04m:20s remains)
INFO - root - 2017-12-06 07:40:47.268301: step 8150, loss = 0.89, batch loss = 0.82 (13.2 examples/sec; 0.605 sec/batch; 54h:28m:22s remains)
INFO - root - 2017-12-06 07:40:53.377456: step 8160, loss = 0.75, batch loss = 0.68 (12.9 examples/sec; 0.621 sec/batch; 55h:58m:13s remains)
INFO - root - 2017-12-06 07:40:59.424446: step 8170, loss = 0.96, batch loss = 0.89 (13.4 examples/sec; 0.598 sec/batch; 53h:54m:52s remains)
INFO - root - 2017-12-06 07:41:05.559200: step 8180, loss = 0.99, batch loss = 0.92 (13.3 examples/sec; 0.601 sec/batch; 54h:08m:55s remains)
INFO - root - 2017-12-06 07:41:11.511831: step 8190, loss = 0.86, batch loss = 0.79 (13.0 examples/sec; 0.613 sec/batch; 55h:13m:57s remains)
INFO - root - 2017-12-06 07:41:17.682142: step 8200, loss = 0.96, batch loss = 0.89 (12.6 examples/sec; 0.633 sec/batch; 57h:02m:21s remains)
2017-12-06 07:41:18.247115: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1160645 -4.3013215 -4.6687503 -5.0018396 -5.0777345 -5.029305 -4.5679159 -4.0030971 -3.9088092 -3.8565674 -3.7511516 -3.9413161 -4.1832371 -4.365406 -4.6335235][-4.0828924 -4.2335792 -4.5334415 -4.8045564 -4.8368149 -4.7141294 -4.1655011 -3.5995409 -3.5260751 -3.5003526 -3.459403 -3.7115283 -3.9949527 -4.1963096 -4.4327388][-4.0563273 -4.1877522 -4.4407682 -4.6627216 -4.6396089 -4.4257894 -3.7993217 -3.2846904 -3.3318896 -3.4491897 -3.552573 -3.8428781 -4.0598927 -4.1492782 -4.2231107][-4.0963607 -4.2419815 -4.4621968 -4.6085911 -4.4545054 -4.0579457 -3.3052077 -2.8411095 -3.0625105 -3.4219763 -3.7597764 -4.129508 -4.2891078 -4.2617 -4.1575193][-4.2202773 -4.4368749 -4.6756616 -4.7691507 -4.4630003 -3.8382335 -2.9174109 -2.4498794 -2.830617 -3.4703929 -4.0975223 -4.6075387 -4.7538738 -4.6218143 -4.3602409][-4.3412395 -4.6295161 -4.9049935 -4.9433522 -4.4456844 -3.5480959 -2.39491 -1.8139427 -2.301589 -3.229203 -4.1755366 -4.9035554 -5.1335316 -4.9853978 -4.6682291][-4.3585358 -4.6403823 -4.8856697 -4.7889829 -4.0228419 -2.8247509 -1.4088304 -0.67320395 -1.2998748 -2.5748634 -3.8937616 -4.918119 -5.3055291 -5.2095895 -4.9248343][-4.3267088 -4.5799241 -4.7787142 -4.5315404 -3.4906387 -1.9911599 -0.25225782 0.73055744 0.048145294 -1.4979272 -3.1597004 -4.5148864 -5.1185436 -5.1279912 -4.9179091][-4.3152752 -4.6038337 -4.8590708 -4.5954633 -3.4277315 -1.739043 0.27525187 1.5156875 0.92108107 -0.65236688 -2.4460251 -3.9819713 -4.7044425 -4.762166 -4.5863075][-4.3071036 -4.6971703 -5.1523976 -5.1195369 -4.1413536 -2.5627749 -0.54492712 0.78194094 0.3619895 -0.96695185 -2.5542693 -3.935699 -4.5020065 -4.4087234 -4.1354332][-4.2651739 -4.7249908 -5.3761473 -5.662693 -5.0803146 -3.8708549 -2.1557674 -0.95520616 -1.18922 -2.1295979 -3.2940068 -4.2844734 -4.5283666 -4.1896038 -3.7725654][-4.2122049 -4.6496964 -5.3319182 -5.7624493 -5.4968376 -4.6962123 -3.4264092 -2.4753788 -2.5799835 -3.1815965 -3.9275579 -4.5287275 -4.5135455 -4.01528 -3.5037894][-4.2081265 -4.591413 -5.188334 -5.5969009 -5.4885778 -5.0051675 -4.1688519 -3.5306973 -3.6485558 -4.0877781 -4.549695 -4.8530393 -4.648932 -4.0417423 -3.4516089][-4.2931628 -4.6662049 -5.1902885 -5.528513 -5.4447618 -5.1057062 -4.5231624 -4.1083322 -4.286016 -4.6674323 -4.9729462 -5.1269288 -4.86388 -4.2600837 -3.670929][-4.3573656 -4.7126808 -5.1684313 -5.412405 -5.2499418 -4.8751416 -4.2909722 -3.9018497 -4.1181564 -4.5016861 -4.80514 -5.020299 -4.88383 -4.4320583 -3.9646449]]...]
INFO - root - 2017-12-06 07:41:24.341062: step 8210, loss = 1.00, batch loss = 0.93 (13.2 examples/sec; 0.606 sec/batch; 54h:35m:31s remains)
INFO - root - 2017-12-06 07:41:30.406130: step 8220, loss = 1.06, batch loss = 0.98 (12.9 examples/sec; 0.620 sec/batch; 55h:51m:47s remains)
INFO - root - 2017-12-06 07:41:36.417143: step 8230, loss = 0.94, batch loss = 0.87 (13.6 examples/sec; 0.587 sec/batch; 52h:54m:48s remains)
INFO - root - 2017-12-06 07:41:42.521785: step 8240, loss = 0.94, batch loss = 0.87 (12.7 examples/sec; 0.630 sec/batch; 56h:42m:06s remains)
INFO - root - 2017-12-06 07:41:48.465368: step 8250, loss = 0.98, batch loss = 0.91 (13.1 examples/sec; 0.612 sec/batch; 55h:09m:27s remains)
INFO - root - 2017-12-06 07:41:54.573924: step 8260, loss = 0.73, batch loss = 0.66 (13.5 examples/sec; 0.591 sec/batch; 53h:13m:49s remains)
INFO - root - 2017-12-06 07:42:00.675886: step 8270, loss = 0.80, batch loss = 0.73 (13.2 examples/sec; 0.604 sec/batch; 54h:25m:18s remains)
INFO - root - 2017-12-06 07:42:06.740239: step 8280, loss = 1.05, batch loss = 0.98 (12.9 examples/sec; 0.620 sec/batch; 55h:51m:56s remains)
INFO - root - 2017-12-06 07:42:12.803967: step 8290, loss = 0.92, batch loss = 0.85 (12.9 examples/sec; 0.620 sec/batch; 55h:49m:32s remains)
INFO - root - 2017-12-06 07:42:18.927966: step 8300, loss = 1.05, batch loss = 0.98 (12.7 examples/sec; 0.628 sec/batch; 56h:33m:46s remains)
2017-12-06 07:42:19.609488: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9479532 -5.0208879 -5.0766468 -5.1119461 -5.1283259 -5.127789 -5.1358862 -5.1990733 -5.318552 -5.4406543 -5.5130286 -5.4889154 -5.3988705 -5.2761889 -5.1451511][-5.1843925 -5.3855586 -5.4999051 -5.5165195 -5.4618254 -5.3704834 -5.3293972 -5.4244723 -5.6309476 -5.8296342 -5.9412389 -5.9041252 -5.752862 -5.5334315 -5.2842212][-5.4930787 -5.7933636 -5.9044652 -5.8091545 -5.5754108 -5.2969108 -5.1695065 -5.3028193 -5.6135635 -5.9034433 -6.0844803 -6.10424 -5.9633036 -5.6735044 -5.2988777][-5.5132179 -5.8407173 -5.9432578 -5.7520638 -5.3027372 -4.7261186 -4.4189568 -4.5464773 -4.9818745 -5.4232249 -5.714489 -5.8498573 -5.7831373 -5.476099 -5.0285292][-5.160377 -5.4731464 -5.647274 -5.4831877 -4.8583965 -3.8962665 -3.2760344 -3.3118391 -3.9092004 -4.6221709 -5.0835428 -5.34903 -5.3499064 -5.0382681 -4.5575495][-4.6618443 -4.9127064 -5.176343 -5.0726771 -4.2556911 -2.8444843 -1.8075631 -1.7002184 -2.5550165 -3.7311113 -4.5189309 -4.959106 -5.0304456 -4.7303739 -4.2404556][-4.277308 -4.4001613 -4.6545925 -4.5218983 -3.4867349 -1.7136657 -0.33229303 -0.12460279 -1.25197 -2.9165888 -4.0782466 -4.6944685 -4.8395696 -4.574789 -4.0866532][-4.3815103 -4.3877077 -4.5100555 -4.2059555 -2.9514704 -0.99029994 0.55709743 0.82775784 -0.43177891 -2.4010656 -3.8613925 -4.6309052 -4.8521438 -4.6202955 -4.1123481][-4.8684707 -4.7775316 -4.6830988 -4.15176 -2.7963753 -0.89690351 0.57746983 0.90595961 -0.22849941 -2.190659 -3.7912605 -4.6819339 -4.9741955 -4.7560773 -4.2009816][-5.2032781 -5.0582285 -4.8173194 -4.2178159 -3.0186582 -1.440254 -0.19527435 0.25559759 -0.49749088 -2.1470077 -3.7050452 -4.6538367 -4.9771013 -4.7490058 -4.1553679][-5.3364925 -5.2543573 -5.0345054 -4.5286674 -3.5916111 -2.3703432 -1.3287659 -0.70942974 -1.0220633 -2.2871368 -3.7573593 -4.7597966 -5.0999861 -4.8448238 -4.1840038][-5.3720937 -5.399097 -5.2554617 -4.861486 -4.1502132 -3.24236 -2.3983216 -1.7060068 -1.6949167 -2.5924358 -3.8815594 -4.8395147 -5.1727877 -4.9222751 -4.2338543][-5.3369093 -5.4322267 -5.3526239 -5.0723119 -4.5800776 -3.9890721 -3.3981218 -2.7602997 -2.5434742 -3.0483217 -3.9616878 -4.6643848 -4.8982854 -4.6974034 -4.1127234][-5.2723408 -5.3738818 -5.3289261 -5.1528754 -4.863893 -4.5568776 -4.2211742 -3.7299798 -3.4439683 -3.6340408 -4.1300163 -4.5041094 -4.5976439 -4.4603162 -4.0542436][-5.2131119 -5.2668662 -5.2171121 -5.1096048 -4.9666038 -4.8536868 -4.7132158 -4.4122992 -4.1741228 -4.1999764 -4.4076176 -4.5492816 -4.5449986 -4.4490108 -4.1988397]]...]
INFO - root - 2017-12-06 07:42:25.660839: step 8310, loss = 0.71, batch loss = 0.64 (13.2 examples/sec; 0.606 sec/batch; 54h:36m:57s remains)
INFO - root - 2017-12-06 07:42:31.796411: step 8320, loss = 0.73, batch loss = 0.66 (12.9 examples/sec; 0.618 sec/batch; 55h:38m:03s remains)
INFO - root - 2017-12-06 07:42:37.838552: step 8330, loss = 1.02, batch loss = 0.95 (12.9 examples/sec; 0.618 sec/batch; 55h:40m:34s remains)
INFO - root - 2017-12-06 07:42:43.690797: step 8340, loss = 0.72, batch loss = 0.65 (13.6 examples/sec; 0.589 sec/batch; 53h:03m:45s remains)
INFO - root - 2017-12-06 07:42:49.784499: step 8350, loss = 1.11, batch loss = 1.04 (13.1 examples/sec; 0.608 sec/batch; 54h:46m:56s remains)
INFO - root - 2017-12-06 07:42:55.719285: step 8360, loss = 0.98, batch loss = 0.91 (12.8 examples/sec; 0.627 sec/batch; 56h:29m:28s remains)
INFO - root - 2017-12-06 07:43:01.786899: step 8370, loss = 0.87, batch loss = 0.80 (13.3 examples/sec; 0.600 sec/batch; 53h:58m:50s remains)
INFO - root - 2017-12-06 07:43:07.865892: step 8380, loss = 1.04, batch loss = 0.97 (12.6 examples/sec; 0.636 sec/batch; 57h:16m:46s remains)
INFO - root - 2017-12-06 07:43:13.926883: step 8390, loss = 0.80, batch loss = 0.73 (12.7 examples/sec; 0.629 sec/batch; 56h:35m:10s remains)
INFO - root - 2017-12-06 07:43:20.034698: step 8400, loss = 1.10, batch loss = 1.03 (12.9 examples/sec; 0.621 sec/batch; 55h:55m:26s remains)
2017-12-06 07:43:20.620999: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0222416 -4.3425364 -4.7669058 -5.2113252 -5.3809915 -5.2137532 -4.8274117 -4.3924522 -4.241993 -4.4640808 -4.8817658 -5.3066998 -5.5645008 -5.6322985 -5.4587617][-4.2577386 -4.6978073 -5.2312613 -5.707778 -5.7537527 -5.3863025 -4.76809 -4.1384149 -3.9844532 -4.3794518 -5.0511107 -5.7398381 -6.1616344 -6.3129888 -6.2181377][-4.5139837 -5.021781 -5.5643997 -5.9189124 -5.6856494 -5.025538 -4.1443534 -3.3330612 -3.2173438 -3.8476915 -4.819211 -5.7971873 -6.3707991 -6.5783844 -6.5751796][-4.7071753 -5.2065792 -5.6480885 -5.7687955 -5.1946397 -4.2472897 -3.1628728 -2.2323759 -2.1748068 -3.04005 -4.3083639 -5.5690508 -6.2697687 -6.4846945 -6.5550203][-4.8161635 -5.2418861 -5.5170107 -5.35765 -4.4485912 -3.2454252 -2.0214906 -1.0192611 -1.0201156 -2.0933981 -3.6062467 -5.07669 -5.8439417 -6.0357585 -6.1783867][-4.8770485 -5.2055826 -5.3041697 -4.8926568 -3.7396979 -2.3919559 -1.1496017 -0.15361643 -0.21284723 -1.3731782 -2.9214265 -4.3997526 -5.1387768 -5.2966456 -5.505599][-4.92468 -5.1826077 -5.1593428 -4.597455 -3.343801 -1.9800754 -0.825681 0.096241951 -0.028086662 -1.1640086 -2.5256772 -3.7861423 -4.4187551 -4.5802522 -4.8462353][-4.9602213 -5.1975222 -5.13286 -4.5371704 -3.3055921 -2.0220475 -0.98447275 -0.1445179 -0.31315851 -1.3045864 -2.3174324 -3.2020311 -3.6559551 -3.8282077 -4.1398053][-4.96743 -5.232429 -5.2169194 -4.7003593 -3.5926552 -2.447638 -1.5107644 -0.70948553 -0.83219504 -1.5922039 -2.2397511 -2.7652698 -3.0270231 -3.1513457 -3.4646795][-4.9315972 -5.2273388 -5.2909503 -4.8741264 -3.8963292 -2.8511953 -1.9322448 -1.0938525 -1.1447082 -1.7328732 -2.1779816 -2.533093 -2.73235 -2.8145108 -3.1409867][-4.874156 -5.1661725 -5.2619023 -4.8804026 -3.9601591 -2.9600463 -2.0250177 -1.161006 -1.2090914 -1.7997637 -2.275527 -2.6972146 -2.9662466 -2.9908953 -3.2855339][-4.820189 -5.0947757 -5.1961894 -4.828486 -3.9134951 -2.9222269 -2.0122342 -1.1933191 -1.315047 -2.044121 -2.6718428 -3.2093089 -3.5302958 -3.4719181 -3.6797624][-4.7520137 -5.0086651 -5.1142888 -4.7806587 -3.8842175 -2.9076688 -2.0729349 -1.3803477 -1.6060052 -2.4899812 -3.3128705 -3.9860389 -4.352725 -4.2320437 -4.3279161][-4.6437721 -4.889061 -5.029593 -4.7888894 -3.9881239 -3.0602112 -2.3016002 -1.7225792 -1.9630206 -2.8729029 -3.8497243 -4.67766 -5.0982985 -4.9619088 -4.9707155][-4.4939294 -4.7301927 -4.9311538 -4.82526 -4.1965966 -3.376991 -2.6765966 -2.1283097 -2.2633162 -3.0442147 -4.0447359 -4.9752693 -5.4518862 -5.3778281 -5.4156809]]...]
INFO - root - 2017-12-06 07:43:26.722039: step 8410, loss = 0.84, batch loss = 0.77 (13.6 examples/sec; 0.590 sec/batch; 53h:04m:29s remains)
INFO - root - 2017-12-06 07:43:32.925531: step 8420, loss = 0.94, batch loss = 0.87 (12.9 examples/sec; 0.622 sec/batch; 55h:59m:04s remains)
INFO - root - 2017-12-06 07:43:39.059232: step 8430, loss = 0.94, batch loss = 0.87 (12.6 examples/sec; 0.634 sec/batch; 57h:02m:39s remains)
INFO - root - 2017-12-06 07:43:45.143794: step 8440, loss = 1.04, batch loss = 0.97 (13.3 examples/sec; 0.600 sec/batch; 53h:58m:29s remains)
INFO - root - 2017-12-06 07:43:51.211253: step 8450, loss = 0.80, batch loss = 0.73 (13.4 examples/sec; 0.598 sec/batch; 53h:49m:27s remains)
INFO - root - 2017-12-06 07:43:57.352326: step 8460, loss = 1.09, batch loss = 1.02 (13.4 examples/sec; 0.597 sec/batch; 53h:42m:50s remains)
INFO - root - 2017-12-06 07:44:03.242001: step 8470, loss = 0.77, batch loss = 0.70 (13.3 examples/sec; 0.602 sec/batch; 54h:10m:51s remains)
INFO - root - 2017-12-06 07:44:09.372673: step 8480, loss = 1.12, batch loss = 1.05 (13.0 examples/sec; 0.616 sec/batch; 55h:26m:22s remains)
INFO - root - 2017-12-06 07:44:15.308927: step 8490, loss = 0.99, batch loss = 0.92 (13.4 examples/sec; 0.598 sec/batch; 53h:49m:24s remains)
INFO - root - 2017-12-06 07:44:21.431325: step 8500, loss = 0.71, batch loss = 0.64 (13.0 examples/sec; 0.616 sec/batch; 55h:25m:56s remains)
2017-12-06 07:44:21.975307: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0457988 -5.0452304 -4.9175978 -4.7630358 -4.6273918 -4.5271187 -4.4960284 -4.5264878 -4.6200624 -4.7744384 -4.9226828 -5.0239234 -5.0750957 -5.0722356 -5.012588][-5.0388021 -5.054337 -4.9571118 -4.8324165 -4.7192068 -4.6419764 -4.612289 -4.6146903 -4.6598597 -4.7572918 -4.867466 -4.9634018 -5.0389471 -5.0791411 -5.0534363][-4.98575 -4.9188056 -4.7714472 -4.6302257 -4.51896 -4.4647121 -4.4455113 -4.4564786 -4.4999862 -4.5618682 -4.6435103 -4.7336707 -4.825808 -4.8958435 -4.8961039][-4.952312 -4.741087 -4.4689403 -4.2646289 -4.12581 -4.0792713 -4.0556593 -4.1037784 -4.2106972 -4.2826586 -4.37363 -4.47979 -4.5915914 -4.6789503 -4.6992488][-4.8996058 -4.5411611 -4.0961652 -3.764091 -3.5247197 -3.4105065 -3.3058209 -3.3793619 -3.6094255 -3.7546797 -3.9236755 -4.0927529 -4.2423868 -4.3479228 -4.3912878][-4.7492347 -4.2716565 -3.6543598 -3.1627035 -2.7779822 -2.5326104 -2.2593517 -2.308264 -2.6569684 -2.886447 -3.1668959 -3.4263825 -3.6221404 -3.7460539 -3.8157749][-4.380043 -3.8701706 -3.1758697 -2.581964 -2.0960569 -1.7254834 -1.2604916 -1.2478542 -1.6580832 -1.922363 -2.2899423 -2.6326852 -2.8658032 -3.0013812 -3.0957937][-3.9744794 -3.547142 -2.9164007 -2.3349195 -1.8555021 -1.4233396 -0.81505013 -0.72646689 -1.0933039 -1.2840033 -1.632472 -1.9831145 -2.2074814 -2.3354709 -2.4498029][-3.5782526 -3.288435 -2.8195605 -2.3729684 -2.0444775 -1.705394 -1.1625574 -1.1184883 -1.4116795 -1.4650033 -1.6861165 -1.9444633 -2.0948248 -2.1859753 -2.3077161][-3.3162587 -3.1704135 -2.9004707 -2.6452198 -2.5175741 -2.3412585 -2.0072732 -2.0882213 -2.3154559 -2.2600622 -2.3420608 -2.4780753 -2.5320597 -2.5715971 -2.6811666][-3.4843774 -3.4523206 -3.3465166 -3.2435741 -3.2392626 -3.1729565 -3.0455093 -3.2219167 -3.3761756 -3.2643242 -3.2355092 -3.2525971 -3.2167435 -3.2045979 -3.2831097][-3.8840442 -3.9098163 -3.8983655 -3.8801634 -3.9204075 -3.9073079 -3.9159031 -4.1178656 -4.2155943 -4.1114116 -4.0310607 -3.9751701 -3.8932941 -3.8491406 -3.8851764][-4.3371844 -4.3626189 -4.3639927 -4.352777 -4.3551106 -4.3170509 -4.3404856 -4.4814334 -4.5310297 -4.4639697 -4.3821673 -4.3078384 -4.2288961 -4.1795011 -4.1777792][-4.620235 -4.6110597 -4.5659451 -4.514781 -4.459549 -4.3816242 -4.3682847 -4.4306016 -4.452601 -4.4190631 -4.3528943 -4.2869596 -4.2327414 -4.1928153 -4.168798][-4.5840025 -4.5362592 -4.4403057 -4.3587017 -4.2895746 -4.2128482 -4.1841383 -4.2058258 -4.2266665 -4.2137384 -4.1580048 -4.1052146 -4.0805316 -4.0611877 -4.0338144]]...]
INFO - root - 2017-12-06 07:44:28.084173: step 8510, loss = 1.03, batch loss = 0.96 (13.1 examples/sec; 0.613 sec/batch; 55h:08m:16s remains)
INFO - root - 2017-12-06 07:44:34.185818: step 8520, loss = 1.00, batch loss = 0.93 (13.1 examples/sec; 0.609 sec/batch; 54h:49m:51s remains)
INFO - root - 2017-12-06 07:44:40.322864: step 8530, loss = 0.93, batch loss = 0.86 (12.2 examples/sec; 0.656 sec/batch; 59h:01m:59s remains)
INFO - root - 2017-12-06 07:44:47.165163: step 8540, loss = 0.72, batch loss = 0.65 (10.5 examples/sec; 0.760 sec/batch; 68h:22m:14s remains)
INFO - root - 2017-12-06 07:44:54.961942: step 8550, loss = 0.90, batch loss = 0.83 (10.1 examples/sec; 0.794 sec/batch; 71h:26m:58s remains)
INFO - root - 2017-12-06 07:45:02.757815: step 8560, loss = 0.80, batch loss = 0.73 (10.2 examples/sec; 0.783 sec/batch; 70h:24m:51s remains)
INFO - root - 2017-12-06 07:45:10.489801: step 8570, loss = 0.80, batch loss = 0.73 (10.8 examples/sec; 0.741 sec/batch; 66h:42m:04s remains)
INFO - root - 2017-12-06 07:45:18.191550: step 8580, loss = 1.05, batch loss = 0.98 (10.3 examples/sec; 0.776 sec/batch; 69h:49m:06s remains)
INFO - root - 2017-12-06 07:45:26.149114: step 8590, loss = 0.91, batch loss = 0.84 (10.1 examples/sec; 0.795 sec/batch; 71h:34m:25s remains)
INFO - root - 2017-12-06 07:45:33.943709: step 8600, loss = 0.95, batch loss = 0.88 (9.9 examples/sec; 0.804 sec/batch; 72h:22m:31s remains)
2017-12-06 07:45:34.635873: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1222839 -5.22421 -5.2589903 -5.2286177 -5.2127233 -5.2020216 -5.2353544 -5.311223 -5.3333559 -5.4077115 -5.4721088 -5.4496732 -5.44657 -5.4225111 -5.4395065][-5.076911 -5.1660409 -5.1956444 -5.1464295 -5.1440282 -5.1326275 -5.1964049 -5.3132977 -5.352778 -5.5727215 -5.7473979 -5.7150526 -5.7075911 -5.6354752 -5.677093][-5.0428271 -5.0527754 -5.0170135 -4.9044342 -4.8993521 -4.8559704 -4.9177585 -5.0118122 -5.016715 -5.4700084 -5.8370733 -5.8570833 -5.9067264 -5.7671671 -5.8149471][-4.9618039 -4.7996511 -4.61195 -4.3642459 -4.2727046 -4.1069527 -4.1076074 -4.1296549 -4.0859432 -4.8704739 -5.520885 -5.6944151 -5.8952956 -5.6591167 -5.635426][-4.6472859 -4.2363009 -3.8290176 -3.4021912 -3.1404157 -2.7788107 -2.6859004 -2.610189 -2.5443728 -3.7121935 -4.7127309 -5.1808519 -5.6669188 -5.3232207 -5.1274276][-4.1835532 -3.5512719 -2.9151187 -2.2833531 -1.7595096 -1.1607585 -0.97617006 -0.78735447 -0.70036006 -2.198091 -3.5537376 -4.3961735 -5.2634263 -4.9148679 -4.5707846][-4.0258832 -3.3258028 -2.5606747 -1.7576885 -0.9386065 -0.087903023 0.19288397 0.51895475 0.67543459 -0.95194411 -2.5159593 -3.6486626 -4.8630233 -4.6397371 -4.2634654][-4.332 -3.8014841 -3.1281977 -2.3296044 -1.3410597 -0.3485961 -0.056425095 0.31689167 0.54770184 -0.92138004 -2.3922989 -3.530159 -4.7851453 -4.6190763 -4.2501225][-4.9472613 -4.6601505 -4.1483922 -3.4312747 -2.4111047 -1.4203963 -1.223932 -0.97804427 -0.78298068 -1.9738374 -3.1471925 -4.0360694 -5.0331764 -4.8392305 -4.4689169][-5.6599932 -5.5420885 -5.1021962 -4.4093432 -3.4220943 -2.5481915 -2.4888206 -2.4321802 -2.3182714 -3.204545 -4.0305614 -4.5960989 -5.232429 -5.0121951 -4.6713662][-6.1983089 -6.1508684 -5.7199459 -5.0217137 -4.1276951 -3.4463713 -3.5114756 -3.6155329 -3.5747585 -4.1612968 -4.6557393 -4.9223633 -5.2224593 -5.0004134 -4.7225366][-6.2559958 -6.1802483 -5.7376709 -5.0464969 -4.2965641 -3.8220468 -3.9583912 -4.1528616 -4.1921167 -4.5612426 -4.8119907 -4.8745508 -4.9550157 -4.7945809 -4.6477451][-5.9424324 -5.7722545 -5.3210869 -4.669807 -4.0821686 -3.766293 -3.8963768 -4.104466 -4.2064095 -4.4456067 -4.5583954 -4.5107484 -4.5120983 -4.5061269 -4.5839791][-5.5213032 -5.2722559 -4.8789845 -4.3515964 -3.9433877 -3.7317641 -3.8002155 -3.9630458 -4.0810513 -4.2250152 -4.2289858 -4.1188674 -4.1223311 -4.2916059 -4.6110024][-5.1127315 -4.8026023 -4.4901481 -4.1309643 -3.899966 -3.7717834 -3.7988229 -3.9108062 -4.0142756 -4.0950265 -4.0292339 -3.8988364 -3.9518361 -4.2753968 -4.7768307]]...]
INFO - root - 2017-12-06 07:45:42.547166: step 8610, loss = 0.93, batch loss = 0.86 (10.9 examples/sec; 0.737 sec/batch; 66h:18m:12s remains)
INFO - root - 2017-12-06 07:45:50.359902: step 8620, loss = 0.99, batch loss = 0.92 (10.3 examples/sec; 0.778 sec/batch; 70h:00m:48s remains)
INFO - root - 2017-12-06 07:45:58.115583: step 8630, loss = 0.89, batch loss = 0.82 (10.2 examples/sec; 0.783 sec/batch; 70h:24m:17s remains)
INFO - root - 2017-12-06 07:46:05.792019: step 8640, loss = 0.67, batch loss = 0.60 (10.1 examples/sec; 0.792 sec/batch; 71h:16m:14s remains)
INFO - root - 2017-12-06 07:46:13.579923: step 8650, loss = 0.77, batch loss = 0.70 (10.4 examples/sec; 0.769 sec/batch; 69h:10m:31s remains)
INFO - root - 2017-12-06 07:46:21.478406: step 8660, loss = 1.02, batch loss = 0.95 (10.3 examples/sec; 0.779 sec/batch; 70h:06m:47s remains)
INFO - root - 2017-12-06 07:46:29.177510: step 8670, loss = 1.03, batch loss = 0.96 (10.4 examples/sec; 0.769 sec/batch; 69h:10m:30s remains)
INFO - root - 2017-12-06 07:46:36.907082: step 8680, loss = 1.01, batch loss = 0.94 (10.2 examples/sec; 0.783 sec/batch; 70h:24m:15s remains)
INFO - root - 2017-12-06 07:46:44.692194: step 8690, loss = 1.08, batch loss = 1.01 (10.1 examples/sec; 0.790 sec/batch; 71h:03m:30s remains)
INFO - root - 2017-12-06 07:46:52.456809: step 8700, loss = 0.80, batch loss = 0.73 (10.1 examples/sec; 0.790 sec/batch; 71h:02m:06s remains)
2017-12-06 07:46:53.172751: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6315255 -5.0170312 -5.3223128 -5.6693487 -5.9263277 -5.9691992 -5.8232913 -5.5499911 -5.2679672 -5.2408943 -5.4409733 -5.6301565 -5.7063961 -5.5235829 -5.1651707][-5.3016453 -5.8210692 -6.14174 -6.4293861 -6.5429478 -6.414845 -6.0783792 -5.6913948 -5.4108462 -5.5971689 -6.0281763 -6.3516364 -6.5317354 -6.3042 -5.7740989][-5.7816448 -6.3485303 -6.5850964 -6.696723 -6.5798645 -6.2363248 -5.6723089 -5.1789656 -5.0099888 -5.5543804 -6.2509542 -6.6694589 -6.9417839 -6.7380314 -6.1267581][-5.9170089 -6.47863 -6.6280823 -6.5826621 -6.2447815 -5.6263466 -4.7196436 -4.0347719 -4.0798044 -5.1307521 -6.1233788 -6.5890875 -6.9047675 -6.7398438 -6.129406][-5.8251457 -6.3358574 -6.3735819 -6.0914235 -5.3802547 -4.2670531 -2.8378243 -1.9552279 -2.4038582 -4.1367993 -5.5098906 -6.0886645 -6.5004759 -6.4189205 -5.8911018][-5.4646444 -5.8173761 -5.7029486 -5.1587076 -4.0241609 -2.3491666 -0.41434765 0.54376459 -0.42626238 -2.8149695 -4.5510693 -5.3055129 -5.8545322 -5.9121222 -5.5576372][-5.0632987 -5.2898965 -5.1398468 -4.4912853 -3.1097248 -1.0681126 1.1900868 2.1598449 0.80931139 -1.8977551 -3.7629137 -4.6017623 -5.2220144 -5.42537 -5.30004][-4.9820709 -5.2490282 -5.2319655 -4.6359687 -3.2104149 -1.0955694 1.1742978 2.0391641 0.54622984 -2.0654316 -3.7203231 -4.4023685 -4.8904438 -5.1369286 -5.1819053][-5.1457853 -5.5525036 -5.735249 -5.3002625 -4.0426416 -2.1835523 -0.24662113 0.38211727 -1.053432 -3.2662358 -4.4705844 -4.7950644 -4.9732008 -5.0970278 -5.1597371][-5.3626361 -5.94452 -6.3274169 -6.0947514 -5.1270781 -3.6674271 -2.1551366 -1.7779889 -3.0767007 -4.8373322 -5.6259165 -5.635047 -5.5140862 -5.4190083 -5.3219991][-5.52084 -6.2234173 -6.7341471 -6.629426 -5.8793955 -4.7370234 -3.5462329 -3.3408492 -4.4816923 -5.8999748 -6.4801054 -6.3859138 -6.1119485 -5.8360958 -5.518568][-5.5586843 -6.2812004 -6.8173904 -6.7668648 -6.1864214 -5.3055334 -4.3725305 -4.2555 -5.1632295 -6.2507191 -6.7122536 -6.6287403 -6.3430672 -5.9993386 -5.5511713][-5.4532695 -6.1469469 -6.6656179 -6.6643538 -6.25773 -5.6462536 -4.9912958 -4.9014177 -5.4647474 -6.1223373 -6.4106874 -6.3381572 -6.1012387 -5.7860212 -5.3339357][-5.1421223 -5.7651892 -6.2299786 -6.2810855 -6.056622 -5.7132406 -5.3297954 -5.220417 -5.3842039 -5.5949869 -5.6818671 -5.6043797 -5.4269061 -5.184617 -4.8192725][-4.6008034 -5.0826454 -5.4277306 -5.4911852 -5.3851819 -5.2205782 -5.0266695 -4.8804746 -4.7561746 -4.681622 -4.6643567 -4.6199684 -4.5283079 -4.3981977 -4.1898975]]...]
INFO - root - 2017-12-06 07:47:00.928730: step 8710, loss = 0.99, batch loss = 0.92 (10.3 examples/sec; 0.773 sec/batch; 69h:31m:23s remains)
INFO - root - 2017-12-06 07:47:08.664275: step 8720, loss = 1.19, batch loss = 1.12 (10.0 examples/sec; 0.797 sec/batch; 71h:42m:43s remains)
INFO - root - 2017-12-06 07:47:16.466828: step 8730, loss = 0.74, batch loss = 0.67 (10.4 examples/sec; 0.771 sec/batch; 69h:21m:28s remains)
INFO - root - 2017-12-06 07:47:24.266112: step 8740, loss = 0.86, batch loss = 0.79 (10.6 examples/sec; 0.756 sec/batch; 68h:00m:14s remains)
INFO - root - 2017-12-06 07:47:32.041622: step 8750, loss = 0.94, batch loss = 0.87 (10.1 examples/sec; 0.792 sec/batch; 71h:11m:27s remains)
INFO - root - 2017-12-06 07:47:39.810415: step 8760, loss = 1.01, batch loss = 0.94 (10.4 examples/sec; 0.767 sec/batch; 68h:56m:57s remains)
INFO - root - 2017-12-06 07:47:47.641391: step 8770, loss = 1.04, batch loss = 0.97 (10.5 examples/sec; 0.764 sec/batch; 68h:40m:12s remains)
INFO - root - 2017-12-06 07:47:55.535850: step 8780, loss = 1.10, batch loss = 1.03 (10.2 examples/sec; 0.786 sec/batch; 70h:39m:04s remains)
INFO - root - 2017-12-06 07:48:03.226752: step 8790, loss = 0.85, batch loss = 0.78 (11.5 examples/sec; 0.698 sec/batch; 62h:43m:10s remains)
INFO - root - 2017-12-06 07:48:11.091751: step 8800, loss = 0.87, batch loss = 0.80 (10.1 examples/sec; 0.794 sec/batch; 71h:24m:05s remains)
2017-12-06 07:48:11.781601: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2203035 -5.2028546 -5.1956758 -5.1743631 -5.1167731 -5.0534329 -5.0314403 -5.07236 -5.1760049 -5.2679324 -5.2695289 -5.2025418 -5.1240993 -5.0530987 -4.9804568][-5.4511571 -5.3683243 -5.3587809 -5.3658452 -5.3167934 -5.2484078 -5.2709179 -5.4114966 -5.6324096 -5.8001118 -5.7872849 -5.6517687 -5.5193176 -5.4306536 -5.342803][-5.4861689 -5.3115249 -5.2593112 -5.2652121 -5.2179031 -5.1397872 -5.2007513 -5.4772615 -5.8705807 -6.1646881 -6.1776509 -5.990387 -5.8039932 -5.7264991 -5.6900463][-5.2005463 -4.998229 -4.9459066 -4.9258275 -4.8117318 -4.6297679 -4.6122665 -4.9689674 -5.5576382 -6.042747 -6.1663804 -5.9781013 -5.7287388 -5.65045 -5.7341037][-4.6664972 -4.4693747 -4.4068742 -4.3003764 -4.041965 -3.6882765 -3.507072 -3.8958821 -4.7032547 -5.4379582 -5.7435279 -5.6380014 -5.3628531 -5.2531252 -5.4340477][-3.927475 -3.7286437 -3.6102622 -3.39647 -2.9691072 -2.4002635 -2.0053895 -2.382736 -3.3962114 -4.4261789 -5.0018554 -5.1155725 -4.9242773 -4.7731986 -4.944109][-3.2272394 -3.0118918 -2.7863798 -2.4365582 -1.8335426 -1.0395231 -0.42827606 -0.73126006 -1.8598628 -3.1998825 -4.1386638 -4.5392327 -4.5103788 -4.360548 -4.4424505][-2.7720346 -2.6480646 -2.3580523 -1.8819904 -1.147253 -0.19700193 0.57170868 0.45987368 -0.52040505 -1.9980199 -3.3182306 -4.0312028 -4.163475 -4.0391707 -4.016233][-2.7826304 -2.9484339 -2.7854023 -2.2737217 -1.4291275 -0.33497238 0.59135914 0.79021025 0.21381044 -1.0988503 -2.6533074 -3.6649358 -3.9682395 -3.8930135 -3.7971542][-3.1108947 -3.6800437 -3.8352478 -3.4427521 -2.5662131 -1.3948839 -0.37948084 0.073737144 -0.043973923 -0.89074969 -2.2903373 -3.4254036 -3.89133 -3.9129896 -3.7811773][-3.3266869 -4.2549791 -4.8499317 -4.8023686 -4.1068358 -3.021925 -2.0246069 -1.4531496 -1.255852 -1.562464 -2.4494443 -3.3643494 -3.8577309 -3.9680674 -3.8612454][-3.2398286 -4.4063792 -5.4257083 -5.8201056 -5.4741406 -4.6486473 -3.7465534 -3.1319165 -2.7908812 -2.7381525 -3.06771 -3.5253997 -3.8295765 -3.965024 -3.9982181][-2.8220835 -4.0540857 -5.3560934 -6.119617 -6.1416454 -5.6654863 -4.9764013 -4.4211445 -4.0750585 -3.8608212 -3.7865736 -3.7834015 -3.7569785 -3.8030257 -3.9946547][-2.41328 -3.4944167 -4.8669868 -5.86407 -6.1773944 -5.9993587 -5.5536709 -5.1458359 -4.91089 -4.6800394 -4.3645625 -4.00343 -3.6527233 -3.5360091 -3.8248851][-2.4739723 -3.270175 -4.5019903 -5.554574 -6.0074778 -5.9644465 -5.6511326 -5.3658352 -5.2885833 -5.1764073 -4.8294883 -4.3172259 -3.7909918 -3.5437431 -3.8348327]]...]
INFO - root - 2017-12-06 07:48:19.377536: step 8810, loss = 0.94, batch loss = 0.87 (10.8 examples/sec; 0.740 sec/batch; 66h:29m:51s remains)
INFO - root - 2017-12-06 07:48:27.098541: step 8820, loss = 0.94, batch loss = 0.87 (10.0 examples/sec; 0.796 sec/batch; 71h:34m:18s remains)
INFO - root - 2017-12-06 07:48:34.816191: step 8830, loss = 0.70, batch loss = 0.63 (10.5 examples/sec; 0.764 sec/batch; 68h:42m:00s remains)
INFO - root - 2017-12-06 07:48:42.638695: step 8840, loss = 1.17, batch loss = 1.10 (10.1 examples/sec; 0.789 sec/batch; 70h:55m:24s remains)
INFO - root - 2017-12-06 07:48:50.405248: step 8850, loss = 1.15, batch loss = 1.08 (10.2 examples/sec; 0.788 sec/batch; 70h:48m:19s remains)
INFO - root - 2017-12-06 07:48:58.177824: step 8860, loss = 0.84, batch loss = 0.76 (10.2 examples/sec; 0.785 sec/batch; 70h:32m:28s remains)
INFO - root - 2017-12-06 07:49:06.063625: step 8870, loss = 0.87, batch loss = 0.80 (10.0 examples/sec; 0.800 sec/batch; 71h:54m:32s remains)
INFO - root - 2017-12-06 07:49:13.768216: step 8880, loss = 0.99, batch loss = 0.92 (10.2 examples/sec; 0.785 sec/batch; 70h:34m:02s remains)
INFO - root - 2017-12-06 07:49:21.377291: step 8890, loss = 0.77, batch loss = 0.70 (10.6 examples/sec; 0.757 sec/batch; 68h:04m:18s remains)
INFO - root - 2017-12-06 07:49:29.196943: step 8900, loss = 0.89, batch loss = 0.82 (10.6 examples/sec; 0.755 sec/batch; 67h:53m:57s remains)
2017-12-06 07:49:29.882199: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.715044 -3.9444964 -4.2510376 -4.4997449 -4.5062418 -4.1882005 -3.9582746 -4.0311065 -4.4452243 -4.8847218 -5.0241008 -4.8816891 -4.5638556 -4.2120075 -3.8479981][-2.8906426 -3.4461737 -4.0960732 -4.542707 -4.7195134 -4.514133 -4.2535868 -4.214642 -4.4215736 -4.5987811 -4.5392361 -4.2462435 -3.7998655 -3.4198542 -3.1007571][-2.1960018 -3.0040064 -3.9769235 -4.5986905 -4.8895636 -4.706811 -4.3178988 -4.080266 -4.0432348 -4.0040808 -3.8489456 -3.5092626 -3.0553081 -2.7681093 -2.6094213][-2.2035913 -3.1188359 -4.1955996 -4.7778449 -4.9461265 -4.534791 -3.7946472 -3.2652836 -3.0753744 -3.0302515 -2.9771643 -2.7570105 -2.4917734 -2.4281063 -2.4966574][-2.666122 -3.572649 -4.5169921 -4.8572316 -4.704947 -3.8919685 -2.7107167 -1.9074333 -1.7715938 -2.0059261 -2.2533488 -2.3093083 -2.3441024 -2.5167801 -2.7408783][-2.9209328 -3.7520723 -4.49598 -4.589797 -4.1122704 -2.9635506 -1.4571946 -0.48438692 -0.59475207 -1.3164456 -2.0033679 -2.407429 -2.6935828 -2.965945 -3.1719787][-3.1661444 -3.8091269 -4.3143721 -4.2308016 -3.5411243 -2.2493956 -0.65227747 0.35635805 -0.033177853 -1.1939485 -2.2110169 -2.8308587 -3.2031851 -3.4116573 -3.4721007][-3.542706 -3.9755173 -4.2763138 -4.1447177 -3.4716671 -2.325053 -0.96112919 -0.099260807 -0.55845547 -1.8135588 -2.8447781 -3.4096119 -3.6915069 -3.7509761 -3.6840901][-3.8895814 -4.1434078 -4.2735934 -4.1205177 -3.5667045 -2.6915662 -1.7339802 -1.1434078 -1.5285351 -2.6493549 -3.5431359 -3.9467142 -4.0986042 -4.0409908 -3.9236615][-4.1763997 -4.2569842 -4.2106724 -3.9721873 -3.5006058 -2.8610682 -2.3138762 -2.069602 -2.4048576 -3.3330631 -4.0701632 -4.3084106 -4.3408275 -4.2096186 -4.089118][-4.4973812 -4.4542265 -4.3241296 -4.0640783 -3.704824 -3.2810287 -3.0523119 -3.0675123 -3.3239574 -3.9995193 -4.5402732 -4.6118975 -4.537221 -4.3837824 -4.29874][-4.7354975 -4.6493359 -4.5371246 -4.3149476 -4.0651026 -3.8146038 -3.777118 -3.9090354 -4.0694966 -4.4964218 -4.839632 -4.797997 -4.6583333 -4.5137024 -4.475141][-4.8508539 -4.7533994 -4.6719818 -4.4945159 -4.3266191 -4.2178679 -4.3019705 -4.4722619 -4.5467439 -4.7544603 -4.9204178 -4.8307524 -4.67559 -4.5491629 -4.5422015][-4.9829149 -4.89327 -4.8452463 -4.7398548 -4.6462159 -4.6250749 -4.7491684 -4.8924651 -4.9058118 -4.9449687 -4.9746337 -4.88012 -4.7569957 -4.67801 -4.6982994][-5.0034976 -4.9371057 -4.9221482 -4.896637 -4.8764572 -4.8928137 -4.9769392 -5.0404129 -5.0052633 -4.9574223 -4.9215479 -4.851913 -4.7853818 -4.7716584 -4.8320737]]...]
INFO - root - 2017-12-06 07:49:37.636932: step 8910, loss = 0.88, batch loss = 0.81 (10.5 examples/sec; 0.761 sec/batch; 68h:26m:37s remains)
INFO - root - 2017-12-06 07:49:45.389851: step 8920, loss = 0.85, batch loss = 0.78 (10.5 examples/sec; 0.764 sec/batch; 68h:38m:08s remains)
INFO - root - 2017-12-06 07:49:53.120561: step 8930, loss = 0.93, batch loss = 0.86 (10.2 examples/sec; 0.784 sec/batch; 70h:28m:39s remains)
INFO - root - 2017-12-06 07:50:00.854341: step 8940, loss = 0.76, batch loss = 0.69 (10.0 examples/sec; 0.796 sec/batch; 71h:33m:32s remains)
INFO - root - 2017-12-06 07:50:08.531654: step 8950, loss = 0.82, batch loss = 0.75 (10.8 examples/sec; 0.740 sec/batch; 66h:31m:35s remains)
INFO - root - 2017-12-06 07:50:16.348197: step 8960, loss = 0.91, batch loss = 0.84 (10.2 examples/sec; 0.786 sec/batch; 70h:35m:58s remains)
INFO - root - 2017-12-06 07:50:24.160710: step 8970, loss = 0.97, batch loss = 0.90 (10.5 examples/sec; 0.765 sec/batch; 68h:45m:25s remains)
INFO - root - 2017-12-06 07:50:32.002951: step 8980, loss = 0.95, batch loss = 0.88 (10.2 examples/sec; 0.781 sec/batch; 70h:10m:35s remains)
INFO - root - 2017-12-06 07:50:39.732187: step 8990, loss = 1.00, batch loss = 0.93 (10.4 examples/sec; 0.771 sec/batch; 69h:14m:54s remains)
INFO - root - 2017-12-06 07:50:47.499342: step 9000, loss = 0.81, batch loss = 0.74 (9.6 examples/sec; 0.830 sec/batch; 74h:37m:36s remains)
2017-12-06 07:50:48.173918: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5307198 -5.5182209 -5.4234681 -5.1675072 -4.8010373 -4.536 -4.3404241 -4.1862974 -4.2001224 -4.431788 -4.8151159 -5.0571308 -4.9364052 -4.4719462 -3.7432125][-5.524128 -5.5679488 -5.5407443 -5.3120379 -4.914916 -4.5535865 -4.2600012 -4.0769711 -4.2118835 -4.686605 -5.2053404 -5.4764428 -5.344183 -4.784452 -3.9220605][-5.3408189 -5.5296011 -5.5762687 -5.3347178 -4.8474269 -4.3145466 -3.8724043 -3.6549773 -3.9541242 -4.7645774 -5.5088482 -5.8232918 -5.6706314 -5.0726776 -4.1619534][-5.152173 -5.4882956 -5.5582919 -5.2244415 -4.5627851 -3.7763808 -3.094398 -2.7766812 -3.2438221 -4.4654112 -5.53988 -5.9305215 -5.7363734 -5.1256733 -4.2542772][-4.91202 -5.3356428 -5.4158049 -5.0011282 -4.1559591 -3.1008461 -2.1170404 -1.5979977 -2.1553652 -3.7816281 -5.2627416 -5.7941709 -5.5481367 -4.9105349 -4.1160588][-4.8540454 -5.246954 -5.3141565 -4.8354216 -3.8281815 -2.5550871 -1.3221755 -0.57098079 -1.0643327 -2.894968 -4.6914964 -5.3822427 -5.1390343 -4.521915 -3.8536096][-5.2036762 -5.49753 -5.5678496 -5.0930328 -4.0137663 -2.6670797 -1.3759451 -0.4837451 -0.73577142 -2.3937407 -4.190362 -4.9250851 -4.7195148 -4.1758971 -3.6240697][-5.5065079 -5.6710334 -5.7653685 -5.3858709 -4.37815 -3.1557326 -2.1020842 -1.3623054 -1.4392102 -2.7070596 -4.2182422 -4.8717794 -4.7316036 -4.328629 -3.8872402][-5.4559145 -5.4632325 -5.5795889 -5.3841066 -4.58896 -3.5897923 -2.8575675 -2.4069118 -2.4519472 -3.3504574 -4.4911509 -5.0298586 -4.9953804 -4.8142281 -4.5594325][-4.9944715 -4.9115114 -5.0547438 -5.0650425 -4.5727272 -3.8508272 -3.3893027 -3.1892176 -3.2757173 -3.9134221 -4.7255974 -5.1541705 -5.2026381 -5.1963086 -5.1243749][-4.3158484 -4.1867304 -4.3049517 -4.4369774 -4.2388883 -3.8169525 -3.570224 -3.5419087 -3.6820471 -4.1574049 -4.7033253 -5.0174317 -5.1054406 -5.1892543 -5.2508245][-3.6751554 -3.4955435 -3.5280085 -3.6550972 -3.622592 -3.4456618 -3.3692472 -3.427177 -3.5819438 -3.9323595 -4.2620325 -4.4711695 -4.5862761 -4.7109566 -4.8506088][-3.2698407 -3.0235288 -2.9405322 -2.987174 -2.9963944 -2.9676945 -3.0048544 -3.0838335 -3.2011979 -3.4316692 -3.5925493 -3.7080145 -3.8320851 -3.9788792 -4.150692][-3.1870008 -2.9048638 -2.7667775 -2.7688446 -2.7778909 -2.7944174 -2.849051 -2.8709867 -2.8907142 -2.9843831 -3.0144048 -3.045434 -3.1551514 -3.3184152 -3.5090716][-3.3108578 -3.04288 -2.9329166 -2.9801805 -3.0395513 -3.0848141 -3.1301804 -3.0963078 -3.014924 -2.9719844 -2.895575 -2.8352203 -2.875035 -3.0066204 -3.1809244]]...]
INFO - root - 2017-12-06 07:50:55.982127: step 9010, loss = 1.07, batch loss = 1.00 (10.5 examples/sec; 0.765 sec/batch; 68h:46m:42s remains)
INFO - root - 2017-12-06 07:51:03.762408: step 9020, loss = 0.86, batch loss = 0.79 (10.2 examples/sec; 0.788 sec/batch; 70h:45m:48s remains)
INFO - root - 2017-12-06 07:51:11.594959: step 9030, loss = 0.99, batch loss = 0.92 (10.6 examples/sec; 0.755 sec/batch; 67h:47m:45s remains)
INFO - root - 2017-12-06 07:51:19.432098: step 9040, loss = 0.84, batch loss = 0.77 (9.8 examples/sec; 0.815 sec/batch; 73h:12m:53s remains)
INFO - root - 2017-12-06 07:51:27.226575: step 9050, loss = 0.82, batch loss = 0.75 (10.9 examples/sec; 0.734 sec/batch; 65h:58m:24s remains)
INFO - root - 2017-12-06 07:51:34.991881: step 9060, loss = 0.92, batch loss = 0.85 (10.2 examples/sec; 0.788 sec/batch; 70h:45m:33s remains)
INFO - root - 2017-12-06 07:51:42.706288: step 9070, loss = 0.93, batch loss = 0.86 (10.6 examples/sec; 0.754 sec/batch; 67h:41m:51s remains)
INFO - root - 2017-12-06 07:51:50.441032: step 9080, loss = 0.86, batch loss = 0.79 (10.2 examples/sec; 0.785 sec/batch; 70h:30m:22s remains)
INFO - root - 2017-12-06 07:51:58.184053: step 9090, loss = 0.90, batch loss = 0.83 (11.9 examples/sec; 0.669 sec/batch; 60h:08m:42s remains)
INFO - root - 2017-12-06 07:52:05.816656: step 9100, loss = 1.08, batch loss = 1.01 (10.3 examples/sec; 0.775 sec/batch; 69h:35m:10s remains)
2017-12-06 07:52:06.536741: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6674876 -4.8110666 -5.0794396 -5.1736755 -5.2531576 -5.3185577 -5.275219 -5.2532887 -5.2590041 -5.0966067 -4.8037939 -4.6656117 -4.6608844 -4.5870638 -4.5278144][-4.6297684 -4.8794408 -5.1865582 -5.2686524 -5.3020248 -5.3071046 -5.2451448 -5.2484717 -5.2140055 -4.9066129 -4.4084535 -4.1851139 -4.2437296 -4.2308712 -4.1898661][-4.554491 -4.94716 -5.2614708 -5.2957354 -5.2470746 -5.1729655 -5.1002688 -5.1681156 -5.1665893 -4.8011179 -4.1282182 -3.7088461 -3.6668973 -3.6459665 -3.6252851][-4.5528765 -5.0917716 -5.38484 -5.3134737 -5.1336484 -4.940608 -4.8344326 -4.96255 -5.1067538 -4.8408484 -4.1218462 -3.4815779 -3.2069135 -3.1030455 -3.089627][-4.6681938 -5.2379932 -5.4105444 -5.1518912 -4.7937346 -4.4215088 -4.1715984 -4.2322879 -4.5804381 -4.6382217 -4.1418443 -3.468503 -3.0255647 -2.8543105 -2.8839605][-4.6550074 -5.1015511 -5.0874534 -4.6185989 -4.0512905 -3.4367318 -2.9070084 -2.7477365 -3.3055358 -3.8712533 -3.8858645 -3.5084884 -3.1500611 -3.0194995 -3.1276922][-4.3345385 -4.5446548 -4.3843627 -3.861824 -3.1935225 -2.3296983 -1.4136245 -0.96258235 -1.705698 -2.7895157 -3.4303226 -3.5981464 -3.599189 -3.6388845 -3.7874079][-3.7314472 -3.7010088 -3.5184503 -3.1855698 -2.6599236 -1.7343328 -0.54420829 0.13203526 -0.68859434 -2.0432868 -3.0876012 -3.7111177 -4.0970688 -4.3270621 -4.4656682][-3.173924 -2.9820213 -2.8555946 -2.8519092 -2.6993709 -2.0174289 -0.91040659 -0.25040054 -0.90677714 -2.0885868 -3.1045804 -3.8407991 -4.3684373 -4.6551833 -4.7120924][-3.0183492 -2.7292466 -2.6608639 -2.9232936 -3.1474524 -2.8637075 -2.1282346 -1.6874158 -2.1167128 -2.9068851 -3.6198139 -4.1692257 -4.5720615 -4.7400517 -4.6384883][-3.4187169 -3.0763931 -3.0148673 -3.3402886 -3.7214003 -3.7391098 -3.4041407 -3.2107477 -3.483634 -3.9577181 -4.4015956 -4.7589808 -5.0079017 -5.015646 -4.72565][-4.1301122 -3.7978003 -3.7162776 -3.9121902 -4.1854639 -4.2937517 -4.2301583 -4.2113376 -4.411963 -4.7252374 -5.0196071 -5.2650733 -5.4391251 -5.3348169 -4.88614][-4.7279119 -4.4557247 -4.37381 -4.4293818 -4.538733 -4.6457634 -4.7216015 -4.78296 -4.9210987 -5.12537 -5.3127508 -5.4773946 -5.6315107 -5.5136423 -5.0060539][-4.9209232 -4.7303553 -4.70016 -4.7161245 -4.7322326 -4.8216381 -4.9513984 -5.0293045 -5.0992861 -5.1956034 -5.2778292 -5.3745518 -5.52881 -5.4883146 -5.0695391][-4.7441549 -4.6314139 -4.6628428 -4.7127423 -4.7174339 -4.780962 -4.8943963 -4.9484234 -4.9612989 -4.9685717 -4.9783311 -5.0317683 -5.176333 -5.2400537 -5.0291271]]...]
INFO - root - 2017-12-06 07:52:14.152492: step 9110, loss = 1.00, batch loss = 0.93 (10.5 examples/sec; 0.763 sec/batch; 68h:32m:33s remains)
INFO - root - 2017-12-06 07:52:21.858434: step 9120, loss = 0.58, batch loss = 0.51 (10.4 examples/sec; 0.766 sec/batch; 68h:50m:44s remains)
INFO - root - 2017-12-06 07:52:29.619168: step 9130, loss = 0.72, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 70h:24m:08s remains)
INFO - root - 2017-12-06 07:52:37.359950: step 9140, loss = 1.11, batch loss = 1.04 (10.4 examples/sec; 0.766 sec/batch; 68h:48m:09s remains)
INFO - root - 2017-12-06 07:52:45.091043: step 9150, loss = 0.89, batch loss = 0.82 (10.1 examples/sec; 0.791 sec/batch; 71h:04m:21s remains)
INFO - root - 2017-12-06 07:52:52.847751: step 9160, loss = 0.81, batch loss = 0.74 (10.7 examples/sec; 0.748 sec/batch; 67h:09m:10s remains)
INFO - root - 2017-12-06 07:53:00.731743: step 9170, loss = 1.07, batch loss = 1.00 (10.2 examples/sec; 0.786 sec/batch; 70h:34m:58s remains)
INFO - root - 2017-12-06 07:53:08.552500: step 9180, loss = 1.03, batch loss = 0.96 (10.6 examples/sec; 0.753 sec/batch; 67h:39m:09s remains)
INFO - root - 2017-12-06 07:53:16.308557: step 9190, loss = 0.97, batch loss = 0.90 (10.3 examples/sec; 0.773 sec/batch; 69h:27m:45s remains)
INFO - root - 2017-12-06 07:53:24.174648: step 9200, loss = 0.74, batch loss = 0.66 (10.2 examples/sec; 0.782 sec/batch; 70h:11m:34s remains)
2017-12-06 07:53:24.833267: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0225449 -5.2447791 -5.4237347 -5.5579829 -5.5317807 -5.1700854 -4.5615516 -4.0173693 -3.8478203 -4.0404043 -4.2176003 -4.458147 -4.8165975 -4.8166451 -4.2653079][-4.8977747 -5.029521 -5.2018003 -5.4308591 -5.5285168 -5.2479448 -4.6315894 -3.9838147 -3.6733074 -3.7562537 -3.9255345 -4.1970406 -4.5645857 -4.6092882 -4.2080846][-4.7640243 -4.819437 -4.9224133 -5.1582685 -5.2933097 -5.05061 -4.4800549 -3.8999274 -3.670608 -3.7820077 -3.9482782 -4.1948452 -4.5220075 -4.569809 -4.2442932][-4.623867 -4.6790042 -4.7205009 -4.86434 -4.8922853 -4.5496535 -3.9435811 -3.4582047 -3.4609151 -3.7943022 -4.0956426 -4.3931489 -4.7277093 -4.7706242 -4.4203115][-4.41308 -4.5826044 -4.6065316 -4.5859571 -4.4143934 -3.8849063 -3.1179159 -2.589824 -2.7983022 -3.4934745 -4.1241422 -4.620275 -5.0460935 -5.103035 -4.6760745][-4.1155343 -4.5015736 -4.5799809 -4.4036193 -4.0203671 -3.2805552 -2.2576072 -1.5046284 -1.7232492 -2.7226934 -3.7696691 -4.5949626 -5.2218781 -5.3602643 -4.8749456][-3.8289242 -4.4684553 -4.6506958 -4.3761821 -3.8340645 -2.9301858 -1.6480174 -0.57617593 -0.60118866 -1.7156296 -3.0938313 -4.2748194 -5.1660295 -5.4451456 -4.9427319][-3.5363069 -4.40205 -4.7272925 -4.4620242 -3.8864121 -2.9272981 -1.4655933 -0.1124177 0.15221548 -0.87249541 -2.4191835 -3.8929925 -5.0531592 -5.4913788 -4.9988456][-3.2235584 -4.2351356 -4.7313519 -4.6021743 -4.1642303 -3.3249385 -1.8754792 -0.44301414 0.027573586 -0.7379632 -2.205548 -3.7818317 -5.0824003 -5.6247163 -5.1506653][-3.1291833 -4.131247 -4.7322636 -4.7586331 -4.5247588 -3.929553 -2.7115824 -1.4495735 -0.94488859 -1.433738 -2.6273618 -4.0445623 -5.2594275 -5.8016787 -5.3551426][-3.3746855 -4.1981459 -4.7750182 -4.8883505 -4.7993097 -4.4488134 -3.581156 -2.6367993 -2.2091191 -2.4812012 -3.3505266 -4.4670815 -5.4451666 -5.8967772 -5.4975309][-3.7731235 -4.3286462 -4.7748494 -4.8992314 -4.8801212 -4.721199 -4.2164588 -3.6362381 -3.343699 -3.4726784 -4.0453558 -4.828743 -5.5121274 -5.8250432 -5.4869385][-4.1016359 -4.4092374 -4.68926 -4.7906179 -4.8025732 -4.7559013 -4.5212259 -4.2258391 -4.0587296 -4.1041713 -4.451457 -4.9544435 -5.3820786 -5.5601592 -5.3018503][-4.2852807 -4.426115 -4.5786567 -4.6588426 -4.698348 -4.7186007 -4.6476417 -4.5252128 -4.4392834 -4.4360495 -4.6133585 -4.9063725 -5.151535 -5.2266064 -5.0396276][-4.370657 -4.4131761 -4.4778442 -4.5350547 -4.5888605 -4.6399369 -4.6477618 -4.6128488 -4.5731926 -4.5504646 -4.6182938 -4.760932 -4.8815207 -4.8994861 -4.7820439]]...]
INFO - root - 2017-12-06 07:53:32.580366: step 9210, loss = 0.89, batch loss = 0.82 (10.3 examples/sec; 0.780 sec/batch; 70h:00m:17s remains)
INFO - root - 2017-12-06 07:53:40.328094: step 9220, loss = 0.76, batch loss = 0.69 (10.7 examples/sec; 0.747 sec/batch; 67h:05m:10s remains)
INFO - root - 2017-12-06 07:53:48.172094: step 9230, loss = 1.06, batch loss = 0.99 (10.2 examples/sec; 0.784 sec/batch; 70h:24m:50s remains)
INFO - root - 2017-12-06 07:53:56.021358: step 9240, loss = 1.01, batch loss = 0.94 (10.9 examples/sec; 0.733 sec/batch; 65h:46m:39s remains)
INFO - root - 2017-12-06 07:54:03.815220: step 9250, loss = 0.84, batch loss = 0.77 (10.2 examples/sec; 0.782 sec/batch; 70h:12m:09s remains)
INFO - root - 2017-12-06 07:54:11.656090: step 9260, loss = 0.86, batch loss = 0.79 (10.2 examples/sec; 0.784 sec/batch; 70h:23m:12s remains)
INFO - root - 2017-12-06 07:54:19.345695: step 9270, loss = 0.88, batch loss = 0.81 (10.1 examples/sec; 0.796 sec/batch; 71h:26m:21s remains)
INFO - root - 2017-12-06 07:54:27.188163: step 9280, loss = 0.80, batch loss = 0.73 (10.7 examples/sec; 0.749 sec/batch; 67h:13m:39s remains)
INFO - root - 2017-12-06 07:54:34.988789: step 9290, loss = 0.82, batch loss = 0.75 (10.5 examples/sec; 0.765 sec/batch; 68h:40m:23s remains)
INFO - root - 2017-12-06 07:54:42.729942: step 9300, loss = 0.94, batch loss = 0.87 (10.4 examples/sec; 0.772 sec/batch; 69h:17m:33s remains)
2017-12-06 07:54:43.399880: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8240662 -4.8988237 -5.0121303 -5.1450109 -5.2318578 -5.20286 -5.1179228 -5.0665965 -5.0423789 -5.002851 -4.905777 -4.7540679 -4.563674 -4.3440018 -4.1338506][-5.3501592 -5.3305559 -5.39534 -5.5802078 -5.7635784 -5.7846918 -5.7123375 -5.7162914 -5.7875075 -5.8395839 -5.7957277 -5.6512437 -5.4324551 -5.1386485 -4.818121][-5.6902189 -5.5119219 -5.4703512 -5.67058 -5.9221163 -5.9592557 -5.8517046 -5.9129405 -6.1625662 -6.4421334 -6.5896072 -6.5569029 -6.3690071 -6.0263186 -5.60835][-5.6849208 -5.3557639 -5.2007957 -5.3586545 -5.5651808 -5.4784164 -5.1803293 -5.1725492 -5.5944557 -6.2087207 -6.7102132 -6.9517283 -6.9325418 -6.6254115 -6.1462984][-5.3522234 -4.9462423 -4.6972861 -4.7421551 -4.7271447 -4.3311558 -3.6935444 -3.5181248 -4.0747137 -5.0107713 -5.8843079 -6.4614987 -6.7361984 -6.634234 -6.2254128][-4.7239318 -4.3402486 -3.9978464 -3.8376186 -3.4360313 -2.5791297 -1.5304217 -1.1765256 -1.9014122 -3.1493554 -4.3696775 -5.2742243 -5.8841681 -6.1009312 -5.8865767][-4.0311618 -3.7534206 -3.3342679 -2.9358749 -2.1183009 -0.79057789 0.64152241 1.1616602 0.27940798 -1.2257013 -2.7497668 -3.9739511 -4.8911867 -5.4245043 -5.4261613][-3.731765 -3.6153393 -3.1929779 -2.6648998 -1.6519535 -0.12965202 1.4539261 2.1027136 1.2409463 -0.26064396 -1.8460016 -3.2249906 -4.2751341 -4.9589767 -5.0802169][-4.0013938 -4.0435824 -3.7029932 -3.1965885 -2.2749021 -0.94365263 0.41289234 1.0197468 0.31997538 -0.91849971 -2.2495706 -3.445153 -4.2848816 -4.8070421 -4.8647623][-4.6786766 -4.7956972 -4.560286 -4.1905828 -3.5530188 -2.6486042 -1.731801 -1.2843263 -1.7917962 -2.7056317 -3.6614356 -4.4429088 -4.8051362 -4.8906689 -4.7131619][-5.5163522 -5.6213646 -5.4643793 -5.2649136 -4.956799 -4.5117807 -4.02078 -3.7097867 -3.9976635 -4.5842857 -5.1764727 -5.5005445 -5.3630104 -5.005312 -4.5801344][-6.1338553 -6.1686249 -6.0204763 -5.9207659 -5.845787 -5.7275863 -5.5367093 -5.3175745 -5.4204693 -5.7109642 -5.9980087 -5.9539113 -5.4796619 -4.8988371 -4.3915114][-6.3389444 -6.3002462 -6.1177082 -6.0238495 -6.0260344 -6.0348392 -5.9584312 -5.7783957 -5.7954693 -5.8865514 -5.9392385 -5.6408291 -4.9997573 -4.41537 -4.0019674][-6.1620154 -6.1261787 -5.951189 -5.8338413 -5.80947 -5.7902451 -5.6774397 -5.4600835 -5.414855 -5.3728061 -5.2641611 -4.8464775 -4.1782231 -3.6996417 -3.4473398][-5.6923566 -5.7042208 -5.5899754 -5.4845037 -5.4293718 -5.3474026 -5.1567106 -4.8729448 -4.7507019 -4.5999489 -4.37257 -3.9388571 -3.3499942 -3.0126681 -2.9102914]]...]
INFO - root - 2017-12-06 07:54:51.119686: step 9310, loss = 1.00, batch loss = 0.93 (10.0 examples/sec; 0.798 sec/batch; 71h:41m:01s remains)
INFO - root - 2017-12-06 07:54:58.681263: step 9320, loss = 0.96, batch loss = 0.89 (11.1 examples/sec; 0.722 sec/batch; 64h:51m:13s remains)
INFO - root - 2017-12-06 07:55:06.462951: step 9330, loss = 0.82, batch loss = 0.75 (9.9 examples/sec; 0.807 sec/batch; 72h:27m:16s remains)
INFO - root - 2017-12-06 07:55:14.213739: step 9340, loss = 1.15, batch loss = 1.08 (10.1 examples/sec; 0.795 sec/batch; 71h:21m:37s remains)
INFO - root - 2017-12-06 07:55:21.957254: step 9350, loss = 0.84, batch loss = 0.77 (10.9 examples/sec; 0.736 sec/batch; 66h:03m:27s remains)
INFO - root - 2017-12-06 07:55:29.841909: step 9360, loss = 0.96, batch loss = 0.89 (10.0 examples/sec; 0.804 sec/batch; 72h:08m:57s remains)
INFO - root - 2017-12-06 07:55:37.646442: step 9370, loss = 1.11, batch loss = 1.04 (10.1 examples/sec; 0.791 sec/batch; 71h:01m:43s remains)
INFO - root - 2017-12-06 07:55:45.359401: step 9380, loss = 1.03, batch loss = 0.96 (10.0 examples/sec; 0.801 sec/batch; 71h:53m:21s remains)
INFO - root - 2017-12-06 07:55:53.240418: step 9390, loss = 0.79, batch loss = 0.72 (10.1 examples/sec; 0.795 sec/batch; 71h:21m:50s remains)
INFO - root - 2017-12-06 07:56:01.008933: step 9400, loss = 0.83, batch loss = 0.76 (10.6 examples/sec; 0.755 sec/batch; 67h:44m:03s remains)
2017-12-06 07:56:01.655308: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.143651 -4.9546709 -4.8064585 -4.9321165 -5.296339 -5.3967071 -5.1854515 -5.2810483 -5.5633135 -5.2354875 -4.3876128 -3.9345198 -4.0550847 -4.195786 -4.390388][-4.6779714 -4.4999547 -4.5281272 -4.8160558 -5.2390566 -5.4120293 -5.2383466 -5.2248917 -5.3871017 -5.041451 -4.2268047 -3.8300483 -4.0616341 -4.2750864 -4.5034909][-4.33537 -4.127955 -4.2986755 -4.7033715 -5.1139951 -5.3320036 -5.2271438 -5.118463 -5.1390486 -4.7931476 -4.0768876 -3.8285303 -4.2026277 -4.4362459 -4.6157937][-4.4005947 -4.0960135 -4.2652926 -4.6745296 -5.0519271 -5.302917 -5.2665663 -5.0739098 -4.9177418 -4.5282421 -3.9101787 -3.8544235 -4.3949242 -4.6426086 -4.7336483][-4.9071412 -4.4752164 -4.4540386 -4.700285 -4.9878583 -5.2039413 -5.1894975 -4.962266 -4.6296468 -4.1398044 -3.6186013 -3.794533 -4.5599823 -4.9096789 -4.9581113][-5.4247155 -4.9171691 -4.5764871 -4.4966235 -4.5827308 -4.647419 -4.5855942 -4.3433108 -3.8288174 -3.1970904 -2.8277078 -3.2981095 -4.3360753 -4.8995256 -5.0247579][-5.7858887 -5.2776728 -4.6354413 -4.1571441 -3.9568162 -3.8106239 -3.6699808 -3.4142411 -2.8115277 -2.1605842 -2.0523498 -2.8161018 -4.0077348 -4.7241035 -4.9144449][-5.8641338 -5.4829645 -4.7455969 -4.0124893 -3.5933361 -3.3286602 -3.1883035 -2.9787817 -2.45536 -1.9650261 -2.0997331 -3.0091758 -4.148767 -4.8429632 -4.9621572][-5.7931151 -5.6393991 -5.0203691 -4.1943684 -3.6046267 -3.2812009 -3.1615443 -2.9765816 -2.6019621 -2.3205004 -2.5925946 -3.5265791 -4.5547638 -5.17897 -5.1996884][-5.7332211 -5.7549868 -5.3483615 -4.5987711 -3.9122858 -3.5250568 -3.381006 -3.1734867 -2.882699 -2.7001092 -2.9522367 -3.8031776 -4.7366276 -5.3345466 -5.3423834][-5.7741647 -5.8751092 -5.696516 -5.1831493 -4.526855 -4.0586877 -3.8196244 -3.5703061 -3.2844768 -3.0648766 -3.1834035 -3.8550305 -4.6459141 -5.1828356 -5.2258039][-5.7279143 -5.7876859 -5.7323866 -5.4718742 -4.9569254 -4.4699516 -4.1733823 -3.9480925 -3.6931231 -3.4153214 -3.403259 -3.8949413 -4.5134907 -4.9313946 -4.9932876][-5.5327506 -5.4805822 -5.4024239 -5.2805171 -4.9703088 -4.5927796 -4.3175535 -4.1326122 -3.8824365 -3.5393734 -3.4506376 -3.8702035 -4.4259982 -4.7646966 -4.8202834][-5.2301712 -5.0838075 -4.9331031 -4.8533 -4.7236228 -4.5102291 -4.3062773 -4.1306696 -3.8147922 -3.3550432 -3.17444 -3.5759192 -4.1679444 -4.5245576 -4.6239052][-4.9059153 -4.7337909 -4.5700812 -4.5211291 -4.5127964 -4.4352188 -4.3009858 -4.1202703 -3.7228885 -3.1199658 -2.7971411 -3.1359468 -3.7703657 -4.2044015 -4.4015775]]...]
INFO - root - 2017-12-06 07:56:09.527028: step 9410, loss = 1.16, batch loss = 1.09 (10.4 examples/sec; 0.772 sec/batch; 69h:14m:55s remains)
INFO - root - 2017-12-06 07:56:17.284853: step 9420, loss = 0.84, batch loss = 0.77 (10.4 examples/sec; 0.770 sec/batch; 69h:07m:14s remains)
INFO - root - 2017-12-06 07:56:25.089116: step 9430, loss = 1.21, batch loss = 1.14 (9.8 examples/sec; 0.812 sec/batch; 72h:53m:21s remains)
INFO - root - 2017-12-06 07:56:33.016757: step 9440, loss = 1.09, batch loss = 1.02 (10.6 examples/sec; 0.755 sec/batch; 67h:43m:27s remains)
INFO - root - 2017-12-06 07:56:40.920803: step 9450, loss = 0.91, batch loss = 0.84 (10.0 examples/sec; 0.798 sec/batch; 71h:37m:41s remains)
INFO - root - 2017-12-06 07:56:48.706023: step 9460, loss = 0.94, batch loss = 0.87 (10.4 examples/sec; 0.766 sec/batch; 68h:41m:52s remains)
INFO - root - 2017-12-06 07:56:56.577634: step 9470, loss = 0.95, batch loss = 0.88 (10.0 examples/sec; 0.801 sec/batch; 71h:51m:17s remains)
INFO - root - 2017-12-06 07:57:04.474984: step 9480, loss = 0.92, batch loss = 0.84 (10.2 examples/sec; 0.786 sec/batch; 70h:30m:08s remains)
INFO - root - 2017-12-06 07:57:12.367198: step 9490, loss = 0.96, batch loss = 0.89 (10.5 examples/sec; 0.760 sec/batch; 68h:11m:28s remains)
INFO - root - 2017-12-06 07:57:20.341549: step 9500, loss = 0.94, batch loss = 0.87 (10.3 examples/sec; 0.780 sec/batch; 69h:58m:34s remains)
2017-12-06 07:57:20.988672: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3617253 -3.5222955 -3.6748719 -3.8946116 -4.0664806 -3.8587596 -3.3532944 -3.2396431 -3.461822 -3.9620106 -4.5493 -4.8270884 -4.8662281 -4.7226639 -4.524375][-3.7156348 -3.8599098 -3.97531 -4.1608787 -4.2587018 -3.9188159 -3.3449035 -3.1740546 -3.2147956 -3.6131957 -4.20326 -4.4924731 -4.6455865 -4.6831937 -4.5478764][-4.213829 -4.2705412 -4.2720656 -4.4005313 -4.4665189 -4.0891051 -3.4897342 -3.15236 -2.9083886 -3.1169083 -3.6638174 -3.956403 -4.1842294 -4.3992195 -4.3788848][-4.6821108 -4.5512524 -4.3528571 -4.4095745 -4.4988661 -4.1945877 -3.6560707 -3.1555867 -2.679399 -2.7613602 -3.2847714 -3.5685475 -3.7866652 -4.0660462 -4.1317792][-4.903306 -4.55458 -4.1537857 -4.1263084 -4.2382236 -4.0966353 -3.741889 -3.23035 -2.7033632 -2.7163718 -3.1975746 -3.4377706 -3.5940266 -3.8070652 -3.8612275][-4.76762 -4.3030863 -3.8326192 -3.7432821 -3.8248444 -3.81302 -3.6450748 -3.2570796 -2.8483653 -2.8757091 -3.3410888 -3.5757055 -3.6746039 -3.729125 -3.6713648][-4.2838111 -3.8442714 -3.455811 -3.3381207 -3.3754878 -3.4270906 -3.3606791 -3.1221786 -2.8761678 -2.9679122 -3.4850588 -3.8260534 -3.9409642 -3.8361542 -3.6037867][-3.6773241 -3.35451 -3.1394658 -3.0537403 -3.0714116 -3.1377633 -3.1258759 -2.9821017 -2.7904072 -2.8821948 -3.4171171 -3.9204388 -4.158535 -3.9897275 -3.6190562][-3.2947464 -3.0980315 -3.0519929 -3.0566015 -3.105938 -3.164058 -3.184639 -3.0644417 -2.8172238 -2.7842121 -3.2079451 -3.8298912 -4.2115889 -4.0383987 -3.623538][-3.1833029 -3.0569522 -3.1300573 -3.2788486 -3.4166577 -3.4881749 -3.5332 -3.4023643 -3.0704255 -2.8239894 -3.0270405 -3.6673167 -4.1591945 -4.0548406 -3.7245374][-3.2837427 -3.2202795 -3.3903091 -3.6750607 -3.9009242 -4.0086875 -4.102046 -4.0128756 -3.6324193 -3.1775677 -3.1287694 -3.6510754 -4.153976 -4.1458855 -3.9670179][-3.3722405 -3.3913245 -3.6554666 -4.0508051 -4.3469663 -4.5051131 -4.6477218 -4.6368647 -4.2977133 -3.759409 -3.5321312 -3.8302417 -4.1704679 -4.1652374 -4.0868745][-3.3560126 -3.4111028 -3.7183325 -4.1453419 -4.4504938 -4.6276369 -4.8065348 -4.8748283 -4.6385837 -4.1820612 -3.9195867 -4.0175338 -4.144536 -4.0879588 -4.04922][-3.4677863 -3.4836154 -3.7230051 -4.0658092 -4.3105478 -4.4620357 -4.6225128 -4.7217522 -4.6018 -4.3146152 -4.1202579 -4.1087012 -4.0981903 -4.0210156 -4.0062141][-3.6810534 -3.6537206 -3.7800951 -3.9969018 -4.165298 -4.2758045 -4.4046011 -4.5162396 -4.4989662 -4.3694177 -4.2522182 -4.1866803 -4.1145215 -4.0397749 -4.0229082]]...]
INFO - root - 2017-12-06 07:57:28.874941: step 9510, loss = 0.77, batch loss = 0.70 (10.2 examples/sec; 0.785 sec/batch; 70h:27m:06s remains)
INFO - root - 2017-12-06 07:57:36.676760: step 9520, loss = 0.89, batch loss = 0.82 (10.2 examples/sec; 0.788 sec/batch; 70h:41m:18s remains)
INFO - root - 2017-12-06 07:57:44.403554: step 9530, loss = 1.07, batch loss = 1.00 (10.3 examples/sec; 0.780 sec/batch; 70h:01m:06s remains)
INFO - root - 2017-12-06 07:57:52.214795: step 9540, loss = 1.13, batch loss = 1.06 (10.0 examples/sec; 0.801 sec/batch; 71h:53m:56s remains)
INFO - root - 2017-12-06 07:58:00.100644: step 9550, loss = 0.90, batch loss = 0.83 (10.6 examples/sec; 0.754 sec/batch; 67h:40m:30s remains)
INFO - root - 2017-12-06 07:58:07.932919: step 9560, loss = 0.91, batch loss = 0.84 (10.6 examples/sec; 0.755 sec/batch; 67h:45m:12s remains)
INFO - root - 2017-12-06 07:58:15.708768: step 9570, loss = 0.81, batch loss = 0.74 (10.3 examples/sec; 0.775 sec/batch; 69h:31m:03s remains)
INFO - root - 2017-12-06 07:58:23.507296: step 9580, loss = 0.87, batch loss = 0.80 (10.6 examples/sec; 0.753 sec/batch; 67h:32m:37s remains)
INFO - root - 2017-12-06 07:58:31.461269: step 9590, loss = 1.17, batch loss = 1.10 (10.2 examples/sec; 0.786 sec/batch; 70h:31m:54s remains)
INFO - root - 2017-12-06 07:58:39.387429: step 9600, loss = 0.93, batch loss = 0.86 (10.1 examples/sec; 0.794 sec/batch; 71h:14m:31s remains)
2017-12-06 07:58:39.961539: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0672622 -3.4130416 -3.5104754 -3.219974 -2.6797078 -2.324856 -2.2538891 -2.3683324 -2.7682009 -3.3334229 -3.8163671 -4.0514035 -4.0414824 -3.81039 -3.5904508][-3.1432834 -3.4656184 -3.5572317 -3.3210282 -2.8154111 -2.4483628 -2.3973265 -2.5427508 -2.9399064 -3.4396818 -3.817302 -4.0548511 -4.1854196 -4.1018038 -3.9606996][-3.1811376 -3.5157285 -3.6374354 -3.4570792 -2.9538357 -2.5570087 -2.5312042 -2.7198431 -3.1330042 -3.5577111 -3.8208928 -4.1060266 -4.4266868 -4.514636 -4.4509463][-3.0915327 -3.4587483 -3.6354737 -3.4950116 -2.9654293 -2.5191984 -2.5157113 -2.7726521 -3.263865 -3.6873353 -3.9073379 -4.2833648 -4.7587676 -4.9487138 -4.888216][-2.9484477 -3.2918086 -3.4792638 -3.3325567 -2.7399046 -2.21291 -2.1942658 -2.506999 -3.0954952 -3.59889 -3.8902924 -4.384779 -4.9347534 -5.133976 -5.0068197][-2.9404387 -3.1797421 -3.3010895 -3.0832233 -2.3910797 -1.7407384 -1.6343746 -1.9257207 -2.5581627 -3.1783752 -3.6316538 -4.2654114 -4.8433566 -5.0041933 -4.8053465][-3.1429682 -3.2257843 -3.2161293 -2.9140573 -2.1802752 -1.463748 -1.2474749 -1.4156623 -1.9552145 -2.6443346 -3.2537594 -3.9911857 -4.5649352 -4.6848788 -4.4481368][-3.4134183 -3.3744464 -3.2547178 -2.9163034 -2.2342298 -1.5503445 -1.2727737 -1.3037629 -1.7010078 -2.39445 -3.0693464 -3.7963181 -4.3094425 -4.3872356 -4.1504254][-3.6159618 -3.5085216 -3.3381953 -3.0312767 -2.4529233 -1.8452077 -1.5451803 -1.4950595 -1.798337 -2.4838095 -3.183635 -3.8330913 -4.2540369 -4.2854409 -4.0479479][-3.688097 -3.5166974 -3.3235946 -3.110146 -2.6552062 -2.1422532 -1.8722215 -1.8385437 -2.123785 -2.81793 -3.5358334 -4.08139 -4.37983 -4.3401537 -4.0744543][-3.6411304 -3.4261265 -3.2418661 -3.1431108 -2.8313882 -2.4606993 -2.2972648 -2.3480682 -2.6251416 -3.274931 -3.9566991 -4.3494277 -4.4752312 -4.3261814 -4.01693][-3.5741861 -3.3480484 -3.1843648 -3.1748948 -3.0103121 -2.8218641 -2.8063407 -2.9525895 -3.1971724 -3.7276187 -4.2713737 -4.4705095 -4.4176717 -4.1911974 -3.8877759][-3.6845093 -3.4160278 -3.21175 -3.1957235 -3.1348133 -3.1299672 -3.2838354 -3.5313616 -3.7621222 -4.136641 -4.4828572 -4.5136366 -4.3482132 -4.1229863 -3.8857138][-4.0781565 -3.7045193 -3.4089854 -3.3397174 -3.3436599 -3.4847245 -3.7618537 -4.0811577 -4.3255591 -4.5711641 -4.7514772 -4.7027473 -4.5131383 -4.338491 -4.1711369][-4.6257215 -4.1600251 -3.8155184 -3.7255273 -3.7925913 -4.0352592 -4.3661976 -4.689714 -4.9038262 -5.0169053 -5.0736966 -5.0298891 -4.8858848 -4.7695751 -4.6160259]]...]
INFO - root - 2017-12-06 07:58:47.847045: step 9610, loss = 1.00, batch loss = 0.93 (10.1 examples/sec; 0.792 sec/batch; 71h:03m:00s remains)
INFO - root - 2017-12-06 07:58:55.704483: step 9620, loss = 0.92, batch loss = 0.85 (10.1 examples/sec; 0.792 sec/batch; 71h:04m:11s remains)
INFO - root - 2017-12-06 07:59:03.645101: step 9630, loss = 0.92, batch loss = 0.85 (10.1 examples/sec; 0.788 sec/batch; 70h:41m:54s remains)
INFO - root - 2017-12-06 07:59:11.330807: step 9640, loss = 1.05, batch loss = 0.98 (10.4 examples/sec; 0.772 sec/batch; 69h:13m:54s remains)
INFO - root - 2017-12-06 07:59:19.233837: step 9650, loss = 0.99, batch loss = 0.92 (9.9 examples/sec; 0.811 sec/batch; 72h:45m:25s remains)
INFO - root - 2017-12-06 07:59:27.239720: step 9660, loss = 0.89, batch loss = 0.82 (10.0 examples/sec; 0.800 sec/batch; 71h:44m:40s remains)
INFO - root - 2017-12-06 07:59:35.178419: step 9670, loss = 0.96, batch loss = 0.89 (10.1 examples/sec; 0.792 sec/batch; 71h:02m:48s remains)
INFO - root - 2017-12-06 07:59:43.121355: step 9680, loss = 1.03, batch loss = 0.96 (10.1 examples/sec; 0.795 sec/batch; 71h:17m:59s remains)
INFO - root - 2017-12-06 07:59:50.991261: step 9690, loss = 1.18, batch loss = 1.11 (10.1 examples/sec; 0.791 sec/batch; 70h:55m:14s remains)
INFO - root - 2017-12-06 07:59:58.733931: step 9700, loss = 1.08, batch loss = 1.00 (12.0 examples/sec; 0.668 sec/batch; 59h:52m:54s remains)
2017-12-06 07:59:59.403575: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.4738979 -5.7295113 -5.6254754 -5.7274952 -5.8141479 -5.656024 -5.42976 -5.1651621 -4.9844828 -4.8991461 -4.9121447 -5.0383978 -5.0649242 -4.9655275 -4.8374047][-5.2853432 -5.6434984 -5.5748482 -5.6615481 -5.6720667 -5.4159236 -5.0519724 -4.6135554 -4.4181743 -4.5081434 -4.7319 -5.0517974 -5.1896648 -5.1041503 -4.9483805][-4.7615843 -5.1880274 -5.2091923 -5.2907815 -5.2222576 -4.8672333 -4.3292661 -3.6846292 -3.4978867 -3.8492928 -4.3416009 -4.8694992 -5.1726155 -5.157198 -4.9990354][-4.161953 -4.6607146 -4.8183579 -4.8950777 -4.7295518 -4.2863784 -3.5698018 -2.6985059 -2.4931169 -3.1066046 -3.8230081 -4.4893069 -4.9517212 -5.055995 -4.9675212][-3.8160219 -4.3726883 -4.6480265 -4.7073555 -4.4173717 -3.8263466 -2.8621926 -1.6956823 -1.4187093 -2.2864876 -3.2283785 -3.9896936 -4.5594954 -4.8123317 -4.8699574][-3.7240794 -4.2875237 -4.651001 -4.7018027 -4.2677913 -3.4802134 -2.2135334 -0.67583632 -0.28152657 -1.4196093 -2.6358542 -3.5168715 -4.1719756 -4.5641 -4.7779422][-3.6145642 -4.1120019 -4.535737 -4.6114216 -4.06388 -3.1254206 -1.6533296 0.18214846 0.71238756 -0.59165072 -2.0134916 -3.0360937 -3.8027859 -4.3377237 -4.6823478][-3.4795742 -3.8936656 -4.3799491 -4.5237222 -3.9402313 -2.9737477 -1.5289083 0.32739162 0.8977828 -0.35237408 -1.7607849 -2.8253155 -3.6614296 -4.2723432 -4.6457748][-3.3587728 -3.7693012 -4.3912106 -4.7105045 -4.2292843 -3.3439317 -2.0581627 -0.3419385 0.22138071 -0.81087089 -2.0385923 -3.0329351 -3.8437016 -4.4198318 -4.7089019][-2.9900484 -3.4638407 -4.3094444 -4.9512448 -4.7654648 -4.0846052 -2.9912784 -1.3854132 -0.78410769 -1.5812492 -2.596952 -3.4787724 -4.1960831 -4.6651373 -4.81457][-2.3889563 -2.8855276 -3.9203045 -4.8919015 -5.086586 -4.70338 -3.8555908 -2.4434607 -1.8665802 -2.451714 -3.2598715 -4.0108647 -4.5918813 -4.9105554 -4.9002628][-1.9500744 -2.4373436 -3.563832 -4.7271838 -5.19917 -5.0674191 -4.4658146 -3.3427329 -2.850091 -3.2549698 -3.8744085 -4.4861774 -4.9240465 -5.0832267 -4.906611][-1.7828393 -2.2675931 -3.4038761 -4.6090655 -5.2144079 -5.239583 -4.8253727 -3.9868932 -3.5935216 -3.8639183 -4.3284936 -4.8049726 -5.111721 -5.1295171 -4.8089743][-1.7845755 -2.2008264 -3.2184765 -4.3389006 -4.9882793 -5.1362967 -4.8851147 -4.2953405 -3.9949284 -4.1796317 -4.535583 -4.91343 -5.1349955 -5.0502586 -4.6362634][-2.2986152 -2.5722685 -3.3384457 -4.2262349 -4.8140726 -5.0523639 -4.9571013 -4.55656 -4.2907958 -4.357676 -4.5712943 -4.8371663 -5.0111623 -4.8941741 -4.4600873]]...]
INFO - root - 2017-12-06 08:00:07.362188: step 9710, loss = 0.81, batch loss = 0.74 (10.5 examples/sec; 0.759 sec/batch; 68h:05m:02s remains)
INFO - root - 2017-12-06 08:00:15.198669: step 9720, loss = 1.03, batch loss = 0.96 (10.0 examples/sec; 0.801 sec/batch; 71h:50m:13s remains)
INFO - root - 2017-12-06 08:00:23.170869: step 9730, loss = 0.87, batch loss = 0.80 (10.2 examples/sec; 0.783 sec/batch; 70h:12m:28s remains)
INFO - root - 2017-12-06 08:00:31.024983: step 9740, loss = 1.11, batch loss = 1.04 (11.3 examples/sec; 0.705 sec/batch; 63h:14m:38s remains)
INFO - root - 2017-12-06 08:00:38.923599: step 9750, loss = 0.85, batch loss = 0.78 (9.8 examples/sec; 0.814 sec/batch; 72h:57m:37s remains)
INFO - root - 2017-12-06 08:00:46.934171: step 9760, loss = 0.77, batch loss = 0.70 (9.9 examples/sec; 0.810 sec/batch; 72h:37m:30s remains)
INFO - root - 2017-12-06 08:00:54.773716: step 9770, loss = 0.93, batch loss = 0.86 (10.4 examples/sec; 0.769 sec/batch; 68h:56m:06s remains)
INFO - root - 2017-12-06 08:01:02.648981: step 9780, loss = 0.89, batch loss = 0.82 (9.7 examples/sec; 0.824 sec/batch; 73h:51m:30s remains)
INFO - root - 2017-12-06 08:01:10.585443: step 9790, loss = 0.97, batch loss = 0.90 (10.2 examples/sec; 0.787 sec/batch; 70h:31m:21s remains)
INFO - root - 2017-12-06 08:01:18.482785: step 9800, loss = 0.84, batch loss = 0.77 (9.9 examples/sec; 0.812 sec/batch; 72h:44m:33s remains)
2017-12-06 08:01:19.155697: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6942058 -4.66605 -4.5901256 -4.4642811 -4.3447547 -4.2968664 -4.3229737 -4.4042 -4.5158782 -4.5651393 -4.4733458 -4.5302205 -4.7845511 -4.9356484 -5.0957088][-4.1339645 -4.0950069 -3.9459343 -3.6866581 -3.4346278 -3.3360205 -3.4053183 -3.5829513 -3.8064735 -3.935945 -3.9141545 -4.0668516 -4.4337153 -4.6944652 -4.98387][-3.9540124 -3.891537 -3.6331553 -3.1800661 -2.7386403 -2.5590048 -2.6785092 -2.9858885 -3.3690538 -3.6301012 -3.7353935 -4.008522 -4.4723859 -4.8257866 -5.2282147][-3.7039604 -3.6425474 -3.2778325 -2.6023197 -1.9114735 -1.5750017 -1.6671638 -2.0338392 -2.5358772 -2.93054 -3.2033062 -3.6455505 -4.2161827 -4.6679468 -5.1967554][-3.5533953 -3.5107226 -3.0542722 -2.1605823 -1.1942477 -0.64033818 -0.61129951 -0.933969 -1.4685032 -1.9675384 -2.4322519 -3.087146 -3.7965508 -4.385891 -5.0789175][-3.626713 -3.6133893 -3.0958681 -2.0368795 -0.84343696 -0.063848972 0.15785742 -0.014576435 -0.47238469 -1.0292897 -1.699759 -2.5758576 -3.4316959 -4.1676216 -5.0326219][-3.647491 -3.6811984 -3.1703448 -2.0756352 -0.80741644 0.12760162 0.59603977 0.69484472 0.44027233 -0.08273983 -0.90070653 -1.9511096 -2.9345744 -3.7961881 -4.8062396][-4.1066685 -4.2165494 -3.8241227 -2.8946242 -1.7941916 -0.87573886 -0.21494484 0.1898818 0.24439526 -0.07060194 -0.81344986 -1.8455617 -2.8140388 -3.6646097 -4.6831217][-4.6705141 -4.9006405 -4.7691245 -4.213635 -3.486913 -2.7601194 -2.0428057 -1.4068029 -1.0096262 -0.93869328 -1.3349948 -2.1100726 -2.8953047 -3.5925865 -4.499321][-4.6169252 -4.9593668 -5.1141844 -4.9529352 -4.5981107 -4.1074548 -3.471993 -2.7896283 -2.1925738 -1.776098 -1.7874198 -2.2447908 -2.7882705 -3.2906313 -4.0661721][-4.5282216 -4.916851 -5.2435918 -5.3088708 -5.1392307 -4.77159 -4.2497087 -3.6733615 -3.0794063 -2.5222702 -2.2937367 -2.4779739 -2.7650547 -3.0515704 -3.6882994][-4.6635571 -5.0324268 -5.4079351 -5.5377035 -5.3760629 -4.9776244 -4.4778337 -4.0261207 -3.5583937 -3.0469246 -2.7552052 -2.7712312 -2.831774 -2.9049568 -3.3988631][-4.863153 -5.2093191 -5.5895896 -5.7200794 -5.4798369 -4.94007 -4.3560739 -3.9663312 -3.6372945 -3.2658079 -3.0225654 -2.9589543 -2.8358219 -2.721693 -3.1027594][-5.1360092 -5.515698 -5.9383783 -6.1114721 -5.8317485 -5.1585259 -4.4468508 -4.0184689 -3.7228427 -3.4316053 -3.2218623 -3.0990067 -2.8319578 -2.5810561 -2.8970957][-5.3707151 -5.8517056 -6.3842716 -6.67791 -6.4498587 -5.7380042 -4.9496417 -4.4218621 -4.0364041 -3.7033734 -3.4724674 -3.2952905 -2.9509506 -2.6537943 -2.957515]]...]
INFO - root - 2017-12-06 08:01:27.012500: step 9810, loss = 1.09, batch loss = 1.02 (10.3 examples/sec; 0.779 sec/batch; 69h:49m:35s remains)
INFO - root - 2017-12-06 08:01:34.854140: step 9820, loss = 0.96, batch loss = 0.89 (9.7 examples/sec; 0.825 sec/batch; 73h:59m:02s remains)
INFO - root - 2017-12-06 08:01:42.670609: step 9830, loss = 0.83, batch loss = 0.76 (10.4 examples/sec; 0.771 sec/batch; 69h:07m:37s remains)
INFO - root - 2017-12-06 08:01:50.567930: step 9840, loss = 0.90, batch loss = 0.83 (10.1 examples/sec; 0.795 sec/batch; 71h:13m:34s remains)
INFO - root - 2017-12-06 08:01:58.250742: step 9850, loss = 0.93, batch loss = 0.86 (10.0 examples/sec; 0.804 sec/batch; 72h:01m:14s remains)
INFO - root - 2017-12-06 08:02:06.037944: step 9860, loss = 0.86, batch loss = 0.79 (10.6 examples/sec; 0.754 sec/batch; 67h:34m:35s remains)
INFO - root - 2017-12-06 08:02:13.904363: step 9870, loss = 1.18, batch loss = 1.11 (10.1 examples/sec; 0.794 sec/batch; 71h:07m:02s remains)
INFO - root - 2017-12-06 08:02:21.840936: step 9880, loss = 1.06, batch loss = 0.99 (10.2 examples/sec; 0.787 sec/batch; 70h:31m:22s remains)
INFO - root - 2017-12-06 08:02:29.677189: step 9890, loss = 0.84, batch loss = 0.77 (9.9 examples/sec; 0.806 sec/batch; 72h:14m:19s remains)
INFO - root - 2017-12-06 08:02:37.578171: step 9900, loss = 0.90, batch loss = 0.83 (9.9 examples/sec; 0.804 sec/batch; 72h:03m:29s remains)
2017-12-06 08:02:38.210630: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4416018 -4.6970959 -5.1459141 -5.5947733 -5.5451493 -5.1993194 -4.8121595 -4.4389052 -4.2133269 -4.2619524 -4.4298887 -4.6113925 -4.8358593 -4.9493957 -4.8726969][-4.4510775 -4.7183843 -5.1899881 -5.677628 -5.5977454 -5.1082006 -4.5825858 -4.16535 -4.0255527 -4.2250037 -4.5832105 -4.8938017 -5.0765171 -4.9985256 -4.6856833][-4.2767386 -4.502665 -4.9464331 -5.4000072 -5.2733822 -4.6887918 -4.0708504 -3.6710584 -3.7078259 -4.1200371 -4.7205429 -5.1941776 -5.3257985 -5.0445046 -4.4656634][-4.1403494 -4.3021631 -4.6824737 -5.0012407 -4.7304626 -4.0268455 -3.3101392 -2.9288733 -3.1565418 -3.8156037 -4.69099 -5.390573 -5.5845313 -5.22355 -4.43578][-4.1831894 -4.2809668 -4.5384226 -4.6292791 -4.1437988 -3.3251982 -2.527106 -2.1418097 -2.4957433 -3.3520582 -4.46807 -5.4141922 -5.7750883 -5.4801121 -4.6115994][-4.2314711 -4.27931 -4.3660727 -4.2168503 -3.5702724 -2.7347727 -1.9527371 -1.6109111 -2.0573554 -3.0432146 -4.298316 -5.4155984 -5.9393606 -5.7776065 -4.9486904][-4.173059 -4.1691422 -4.0454888 -3.6894987 -3.0014522 -2.2614348 -1.6168621 -1.435885 -1.9971089 -3.0541985 -4.3141408 -5.438427 -5.9852142 -5.8896561 -5.1778493][-3.9754915 -3.9451845 -3.6538775 -3.1716392 -2.5472715 -1.9814036 -1.5599661 -1.637115 -2.3766656 -3.4598615 -4.5906224 -5.5603309 -5.9730191 -5.8395114 -5.2425604][-3.76471 -3.6958256 -3.268661 -2.7100096 -2.1921337 -1.8578804 -1.7176969 -2.0630682 -2.9765692 -4.0391135 -4.9636846 -5.7110581 -5.9585838 -5.7794933 -5.2720432][-3.7109332 -3.5438418 -2.9815528 -2.3896167 -2.0226667 -1.9725525 -2.1084495 -2.6152678 -3.5459864 -4.4606295 -5.1494322 -5.7021093 -5.8653235 -5.6992612 -5.2979145][-3.7616453 -3.463316 -2.8026781 -2.2484646 -2.0841112 -2.3142402 -2.685854 -3.270963 -4.0839939 -4.738462 -5.1782026 -5.5869827 -5.7611246 -5.680501 -5.4073329][-3.8781118 -3.5082767 -2.8477049 -2.3919294 -2.387177 -2.7725744 -3.2584839 -3.8486347 -4.503828 -4.9303284 -5.1935034 -5.5211611 -5.7595663 -5.7799134 -5.6125607][-4.0241914 -3.6988006 -3.1608396 -2.8447776 -2.92763 -3.3218894 -3.7865565 -4.3110924 -4.8181305 -5.0879264 -5.2318392 -5.4953346 -5.7896261 -5.8855486 -5.7785025][-4.1599884 -3.9528995 -3.6171947 -3.4665763 -3.5952878 -3.9307425 -4.315208 -4.7422261 -5.0925236 -5.2067361 -5.214849 -5.3740134 -5.6683383 -5.806838 -5.7328558][-4.2226839 -4.1567268 -4.0306854 -4.0178337 -4.1596432 -4.4340467 -4.7614241 -5.0894265 -5.2727528 -5.2213058 -5.073122 -5.1126757 -5.3537488 -5.5002975 -5.4423008]]...]
INFO - root - 2017-12-06 08:02:46.094948: step 9910, loss = 1.09, batch loss = 1.02 (9.8 examples/sec; 0.817 sec/batch; 73h:13m:53s remains)
INFO - root - 2017-12-06 08:02:54.042824: step 9920, loss = 1.03, batch loss = 0.96 (10.4 examples/sec; 0.770 sec/batch; 69h:00m:02s remains)
INFO - root - 2017-12-06 08:03:01.877655: step 9930, loss = 0.91, batch loss = 0.84 (10.0 examples/sec; 0.802 sec/batch; 71h:52m:10s remains)
INFO - root - 2017-12-06 08:03:09.695960: step 9940, loss = 0.93, batch loss = 0.86 (10.2 examples/sec; 0.786 sec/batch; 70h:28m:04s remains)
INFO - root - 2017-12-06 08:03:17.555883: step 9950, loss = 1.15, batch loss = 1.08 (10.3 examples/sec; 0.778 sec/batch; 69h:41m:50s remains)
INFO - root - 2017-12-06 08:03:25.360521: step 9960, loss = 0.93, batch loss = 0.86 (10.1 examples/sec; 0.795 sec/batch; 71h:13m:11s remains)
INFO - root - 2017-12-06 08:03:33.201598: step 9970, loss = 1.04, batch loss = 0.97 (10.2 examples/sec; 0.785 sec/batch; 70h:21m:44s remains)
INFO - root - 2017-12-06 08:03:40.944136: step 9980, loss = 0.71, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 69h:38m:01s remains)
INFO - root - 2017-12-06 08:03:48.816018: step 9990, loss = 1.07, batch loss = 1.00 (10.1 examples/sec; 0.792 sec/batch; 70h:57m:53s remains)
INFO - root - 2017-12-06 08:03:56.596100: step 10000, loss = 0.89, batch loss = 0.82 (10.5 examples/sec; 0.760 sec/batch; 68h:02m:42s remains)
2017-12-06 08:03:57.218315: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4128971 -4.7749844 -5.1609817 -5.5424762 -5.7284842 -5.6556644 -5.4111123 -5.1668391 -4.8112235 -4.4815516 -4.3658957 -4.386416 -4.407928 -4.4801559 -4.884892][-4.9192095 -5.3322792 -5.7187881 -6.0672011 -6.165854 -5.9384813 -5.5091434 -5.1198044 -4.7515068 -4.5221868 -4.5421028 -4.709094 -4.8468184 -4.9539552 -5.2950616][-5.7161341 -6.059617 -6.3052478 -6.4733748 -6.3592906 -5.8816314 -5.2127671 -4.6960306 -4.4318295 -4.4322019 -4.6727262 -5.0398574 -5.3188028 -5.4326491 -5.640358][-6.3020916 -6.48411 -6.493926 -6.3784084 -5.9626179 -5.1626368 -4.2384768 -3.6580582 -3.6437156 -4.0061789 -4.5443 -5.1315932 -5.5259562 -5.6465111 -5.7345209][-6.3174486 -6.2826619 -5.9976063 -5.5654492 -4.82922 -3.7332194 -2.638227 -2.1453202 -2.5193629 -3.3141239 -4.1636052 -4.9391589 -5.4207091 -5.568181 -5.5990586][-5.8431325 -5.6061239 -5.0576239 -4.3447571 -3.3100753 -1.9527993 -0.79389024 -0.56765056 -1.4245026 -2.609796 -3.6571748 -4.5531077 -5.1424503 -5.3719993 -5.4200463][-5.2440844 -4.8588653 -4.1120796 -3.1953683 -1.9183698 -0.32375431 0.85416508 0.72487688 -0.59843183 -2.0320907 -3.1051562 -4.0217037 -4.7002134 -5.0346494 -5.1550832][-4.8934364 -4.4412837 -3.586513 -2.5682545 -1.1802397 0.44373035 1.4278107 0.89494705 -0.67126346 -2.031836 -2.8885956 -3.6373253 -4.272532 -4.6327033 -4.8305855][-4.9179296 -4.54015 -3.7838631 -2.8679614 -1.6087327 -0.26541042 0.278893 -0.52911067 -1.9465809 -2.862113 -3.2765861 -3.7243795 -4.2128296 -4.5187778 -4.7349839][-5.0735836 -4.8894668 -4.43803 -3.8004107 -2.8008447 -1.8126454 -1.6521134 -2.5213189 -3.5376911 -3.884377 -3.8841739 -4.1189651 -4.5018497 -4.7462506 -4.9479489][-5.0996661 -5.1355171 -5.0176816 -4.6688018 -3.9351106 -3.2471187 -3.3018932 -4.0628815 -4.6433096 -4.554626 -4.3631005 -4.5636873 -4.921237 -5.1111345 -5.2816954][-4.9639907 -5.1321044 -5.2049055 -5.0276575 -4.5024772 -4.01688 -4.1297665 -4.7206278 -4.9860072 -4.6978788 -4.516777 -4.7956452 -5.2023559 -5.4041553 -5.5940633][-4.8923516 -5.0258431 -5.0588689 -4.8236103 -4.3335338 -3.9053824 -3.9596269 -4.3812284 -4.501183 -4.2523251 -4.2238865 -4.6198249 -5.1087909 -5.380928 -5.63803][-5.003561 -5.0152678 -4.8762174 -4.4406919 -3.8233426 -3.3418183 -3.3122082 -3.6229262 -3.7337027 -3.5982933 -3.6648893 -4.0701365 -4.5694785 -4.9182463 -5.2858748][-5.0000067 -4.9019833 -4.6334009 -4.067153 -3.364058 -2.8274555 -2.7203894 -2.9263558 -3.0082552 -2.8948088 -2.9218354 -3.2265744 -3.686913 -4.1336007 -4.6579137]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-1/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-1/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 08:04:05.675066: step 10010, loss = 1.02, batch loss = 0.95 (10.1 examples/sec; 0.793 sec/batch; 71h:03m:26s remains)
INFO - root - 2017-12-06 08:04:13.581490: step 10020, loss = 0.99, batch loss = 0.92 (10.0 examples/sec; 0.801 sec/batch; 71h:46m:05s remains)
INFO - root - 2017-12-06 08:04:21.418103: step 10030, loss = 0.93, batch loss = 0.86 (9.7 examples/sec; 0.822 sec/batch; 73h:36m:37s remains)
INFO - root - 2017-12-06 08:04:29.307207: step 10040, loss = 0.94, batch loss = 0.87 (9.8 examples/sec; 0.812 sec/batch; 72h:46m:15s remains)
INFO - root - 2017-12-06 08:04:37.188082: step 10050, loss = 0.77, batch loss = 0.70 (10.3 examples/sec; 0.780 sec/batch; 69h:52m:48s remains)
INFO - root - 2017-12-06 08:04:44.932572: step 10060, loss = 1.19, batch loss = 1.12 (10.4 examples/sec; 0.772 sec/batch; 69h:10m:51s remains)
INFO - root - 2017-12-06 08:04:52.810484: step 10070, loss = 1.18, batch loss = 1.11 (10.2 examples/sec; 0.785 sec/batch; 70h:16m:31s remains)
INFO - root - 2017-12-06 08:05:00.701531: step 10080, loss = 1.03, batch loss = 0.96 (10.3 examples/sec; 0.774 sec/batch; 69h:18m:51s remains)
INFO - root - 2017-12-06 08:05:08.674463: step 10090, loss = 0.84, batch loss = 0.77 (9.8 examples/sec; 0.817 sec/batch; 73h:10m:16s remains)
INFO - root - 2017-12-06 08:05:16.504844: step 10100, loss = 0.87, batch loss = 0.80 (10.3 examples/sec; 0.777 sec/batch; 69h:37m:44s remains)
2017-12-06 08:05:17.155512: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4591596 -3.165709 -3.1268234 -3.3734341 -3.6315901 -3.8883963 -4.1622863 -4.294241 -4.2476888 -4.1048465 -3.9716597 -3.9048383 -3.8791285 -3.856163 -3.8550041][-3.4644163 -3.1443787 -3.0872571 -3.3392372 -3.6286352 -3.9211142 -4.2366939 -4.3913693 -4.3279448 -4.1471696 -3.9597797 -3.8317981 -3.7550786 -3.7143593 -3.7595379][-3.4533441 -3.1104085 -3.061357 -3.3435025 -3.6550975 -3.932564 -4.2075739 -4.328752 -4.2709355 -4.1362619 -3.9752345 -3.8254974 -3.7044449 -3.6422398 -3.7188][-3.4187636 -3.0792789 -3.0501823 -3.3266816 -3.577153 -3.7315054 -3.8613372 -3.9143891 -3.9296963 -3.9627764 -3.9423366 -3.8433509 -3.7072921 -3.6100035 -3.6640143][-3.3877528 -3.0750587 -3.047791 -3.2407713 -3.3144789 -3.2259526 -3.1177545 -3.1001616 -3.264401 -3.573945 -3.8186624 -3.8823032 -3.783982 -3.616153 -3.5570872][-3.3908534 -3.1130326 -3.0551348 -3.0956473 -2.9067268 -2.49097 -2.0875921 -2.0201213 -2.412209 -3.0744529 -3.6609 -3.9649088 -3.9371815 -3.666301 -3.4401035][-3.4137197 -3.174521 -3.0637786 -2.9205289 -2.435997 -1.6706266 -0.98762512 -0.92406535 -1.6016376 -2.6219988 -3.5232341 -4.0343075 -4.0544715 -3.6824625 -3.3155327][-3.3727121 -3.158215 -3.0128088 -2.7620006 -2.1147683 -1.1987987 -0.43922782 -0.44057631 -1.3008218 -2.4779539 -3.4641337 -4.0191722 -4.0508709 -3.649909 -3.2657366][-3.2262363 -3.02934 -2.9229896 -2.7505312 -2.2174671 -1.4886887 -0.92359304 -0.98166966 -1.7836037 -2.8187881 -3.6077967 -4.0013437 -3.9536986 -3.5536366 -3.2275052][-2.9858813 -2.8105726 -2.8146076 -2.868535 -2.6318617 -2.26442 -2.006597 -2.1105042 -2.737045 -3.4868903 -3.9481342 -4.0712557 -3.8798635 -3.4752855 -3.2114701][-2.8310385 -2.6367693 -2.7369156 -3.0390549 -3.1409163 -3.1432588 -3.1720021 -3.2928905 -3.6914129 -4.1110644 -4.251987 -4.1287704 -3.8137417 -3.4115613 -3.1685538][-2.8618443 -2.6164415 -2.7651443 -3.2414846 -3.5780427 -3.7837405 -3.9308605 -3.9746912 -4.1106315 -4.2360082 -4.1731648 -3.9328413 -3.5950708 -3.2501438 -3.0170469][-3.0686135 -2.7874892 -2.9283414 -3.4327807 -3.8130178 -4.0200815 -4.1240897 -4.0363364 -3.9363632 -3.8492379 -3.7054312 -3.4765835 -3.2267752 -3.032388 -2.8756456][-3.4667263 -3.1566083 -3.2298107 -3.6315789 -3.9170563 -4.0372529 -4.0601859 -3.8566794 -3.5578842 -3.283726 -3.0695693 -2.8650734 -2.7375565 -2.7487066 -2.7693915][-3.897119 -3.5855341 -3.5834923 -3.8398314 -3.9868686 -4.01933 -3.9853747 -3.7010269 -3.2539592 -2.8404808 -2.570919 -2.4136739 -2.4235885 -2.6391442 -2.8616548]]...]
INFO - root - 2017-12-06 08:05:24.931608: step 10110, loss = 0.87, batch loss = 0.80 (10.5 examples/sec; 0.764 sec/batch; 68h:26m:11s remains)
INFO - root - 2017-12-06 08:05:32.686829: step 10120, loss = 1.01, batch loss = 0.94 (10.1 examples/sec; 0.788 sec/batch; 70h:36m:15s remains)
INFO - root - 2017-12-06 08:05:40.538011: step 10130, loss = 0.92, batch loss = 0.85 (10.3 examples/sec; 0.774 sec/batch; 69h:17m:02s remains)
INFO - root - 2017-12-06 08:05:48.471007: step 10140, loss = 0.96, batch loss = 0.89 (10.4 examples/sec; 0.768 sec/batch; 68h:44m:01s remains)
INFO - root - 2017-12-06 08:05:56.328565: step 10150, loss = 0.76, batch loss = 0.69 (10.5 examples/sec; 0.762 sec/batch; 68h:12m:23s remains)
INFO - root - 2017-12-06 08:06:04.017754: step 10160, loss = 0.98, batch loss = 0.91 (10.6 examples/sec; 0.753 sec/batch; 67h:27m:08s remains)
INFO - root - 2017-12-06 08:06:11.755593: step 10170, loss = 1.19, batch loss = 1.12 (10.0 examples/sec; 0.797 sec/batch; 71h:22m:13s remains)
INFO - root - 2017-12-06 08:06:19.590847: step 10180, loss = 0.83, batch loss = 0.76 (10.5 examples/sec; 0.761 sec/batch; 68h:06m:38s remains)
INFO - root - 2017-12-06 08:06:27.459685: step 10190, loss = 1.00, batch loss = 0.93 (10.0 examples/sec; 0.802 sec/batch; 71h:46m:59s remains)
INFO - root - 2017-12-06 08:06:35.389859: step 10200, loss = 1.21, batch loss = 1.14 (9.7 examples/sec; 0.822 sec/batch; 73h:36m:25s remains)
2017-12-06 08:06:36.031917: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5236793 -4.9130445 -5.1523452 -5.2785735 -5.3398285 -5.3635831 -5.2785892 -5.16267 -5.1692066 -5.2821789 -5.4766264 -5.8115678 -6.1203027 -6.2987227 -6.3147531][-5.2052011 -5.6298823 -5.8910122 -6.0273085 -6.0859146 -6.0441813 -5.8485003 -5.5729094 -5.4094057 -5.4012036 -5.5472174 -5.8751969 -6.1628428 -6.3109426 -6.28994][-5.9316611 -6.3779564 -6.6295028 -6.7194686 -6.6739569 -6.4446812 -6.0324578 -5.5463953 -5.2347488 -5.16951 -5.3393989 -5.7144346 -6.0142283 -6.150279 -6.0795121][-6.4765549 -6.9396243 -7.1680341 -7.170064 -6.924181 -6.4289041 -5.7689509 -5.1076665 -4.74305 -4.7352772 -5.0274634 -5.5091662 -5.8429141 -5.9705429 -5.8376837][-6.6883755 -7.1401525 -7.3008418 -7.1428876 -6.6157141 -5.8320045 -4.9616232 -4.2027149 -3.8813562 -4.0291605 -4.5259686 -5.1647706 -5.5500727 -5.6842518 -5.5214229][-6.5931091 -6.9910574 -7.0083885 -6.5942354 -5.7227888 -4.6570959 -3.6311035 -2.835201 -2.6280642 -3.0090816 -3.7935677 -4.6415939 -5.0968413 -5.2660036 -5.1284723][-6.2581019 -6.5853057 -6.4158964 -5.7133708 -4.5231438 -3.2317848 -2.0942483 -1.2834544 -1.2257261 -1.8816159 -3.0037508 -4.0928769 -4.6245704 -4.8434086 -4.7580013][-5.7737246 -5.9843187 -5.627202 -4.7045479 -3.3172512 -1.9079676 -0.70358181 0.12214231 0.040362358 -0.86143136 -2.2777274 -3.5625918 -4.1471305 -4.4080796 -4.3886957][-5.2316003 -5.3354845 -4.8864074 -3.9320774 -2.5861635 -1.2629802 -0.12765265 0.635581 0.43885231 -0.579653 -2.0916202 -3.387219 -3.9093397 -4.1272907 -4.1318908][-4.8429265 -4.9230738 -4.5313 -3.7470958 -2.6577163 -1.6054168 -0.69977593 -0.11477757 -0.3738184 -1.3295608 -2.6730928 -3.733552 -4.0245485 -4.0648212 -4.0173297][-4.80502 -4.9404321 -4.6875939 -4.1379256 -3.3495522 -2.5969257 -1.9752553 -1.6033788 -1.89047 -2.6915936 -3.72014 -4.4297328 -4.4357948 -4.2411075 -4.0844417][-5.1239805 -5.2804856 -5.1349874 -4.7780056 -4.2389665 -3.7341311 -3.3607326 -3.1556468 -3.4366574 -4.0610647 -4.7660289 -5.1683512 -4.9976907 -4.6623807 -4.4422951][-5.6846447 -5.7480574 -5.5736566 -5.2622657 -4.8393111 -4.4911337 -4.2961216 -4.2164879 -4.4998689 -5.0109596 -5.5065284 -5.748044 -5.5792785 -5.2685356 -5.0812006][-6.1836562 -6.0557504 -5.7160053 -5.2862573 -4.8351393 -4.55626 -4.4627271 -4.4809823 -4.8074822 -5.3177 -5.7710319 -6.01679 -6.0039549 -5.8563213 -5.7514219][-6.390492 -6.0551071 -5.5018148 -4.8618135 -4.2996364 -4.0354347 -3.9932315 -4.1003189 -4.506753 -5.1094313 -5.6434164 -5.9756722 -6.1723895 -6.2396374 -6.2436237]]...]
INFO - root - 2017-12-06 08:06:43.921253: step 10210, loss = 0.82, batch loss = 0.75 (10.0 examples/sec; 0.803 sec/batch; 71h:55m:08s remains)
INFO - root - 2017-12-06 08:06:51.774810: step 10220, loss = 0.87, batch loss = 0.80 (10.4 examples/sec; 0.767 sec/batch; 68h:38m:22s remains)
INFO - root - 2017-12-06 08:06:59.560646: step 10230, loss = 1.11, batch loss = 1.04 (10.9 examples/sec; 0.736 sec/batch; 65h:55m:01s remains)
INFO - root - 2017-12-06 08:07:07.401869: step 10240, loss = 0.83, batch loss = 0.76 (10.2 examples/sec; 0.784 sec/batch; 70h:08m:32s remains)
INFO - root - 2017-12-06 08:07:15.211374: step 10250, loss = 0.86, batch loss = 0.79 (10.3 examples/sec; 0.776 sec/batch; 69h:26m:01s remains)
INFO - root - 2017-12-06 08:07:23.073650: step 10260, loss = 1.31, batch loss = 1.24 (10.1 examples/sec; 0.789 sec/batch; 70h:39m:27s remains)
INFO - root - 2017-12-06 08:07:30.919456: step 10270, loss = 0.83, batch loss = 0.76 (10.4 examples/sec; 0.767 sec/batch; 68h:39m:17s remains)
INFO - root - 2017-12-06 08:07:38.823613: step 10280, loss = 0.93, batch loss = 0.86 (10.2 examples/sec; 0.785 sec/batch; 70h:16m:50s remains)
INFO - root - 2017-12-06 08:07:46.750050: step 10290, loss = 0.78, batch loss = 0.71 (10.0 examples/sec; 0.799 sec/batch; 71h:29m:49s remains)
INFO - root - 2017-12-06 08:07:54.635962: step 10300, loss = 0.82, batch loss = 0.75 (10.3 examples/sec; 0.777 sec/batch; 69h:32m:29s remains)
2017-12-06 08:07:55.339547: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.642067 -4.6824322 -4.7186947 -4.7495823 -4.7131639 -4.607429 -4.4718084 -4.2495871 -3.9998882 -3.9202776 -4.1555653 -4.6288 -5.1564212 -5.5835543 -5.7484961][-4.0929594 -4.1296358 -4.190877 -4.2543821 -4.211658 -4.0590477 -3.8946939 -3.6792769 -3.4589088 -3.4397163 -3.7487278 -4.2522364 -4.76061 -5.1841483 -5.3949695][-4.179316 -4.2193289 -4.3337016 -4.4423704 -4.3570995 -4.0649481 -3.7972217 -3.6182616 -3.5370121 -3.6591072 -4.0133433 -4.4228792 -4.7254996 -4.9466977 -5.0775318][-4.59475 -4.6606088 -4.8632765 -5.0182481 -4.8202338 -4.249444 -3.7395828 -3.57111 -3.742784 -4.1550307 -4.6168203 -4.8991141 -4.9110732 -4.8436871 -4.8384867][-4.7155266 -4.85911 -5.1898665 -5.3829083 -4.9950261 -4.0271621 -3.1495624 -2.934443 -3.4438367 -4.3077955 -4.9992657 -5.2191586 -4.9999456 -4.7112079 -4.6144471][-4.2161946 -4.4891577 -4.9831743 -5.1948996 -4.5463839 -3.1276581 -1.8460612 -1.5660198 -2.4503958 -3.8307636 -4.8156881 -5.055645 -4.755321 -4.4284678 -4.3658409][-3.2204175 -3.642163 -4.3268456 -4.5685635 -3.6637576 -1.8222845 -0.18397188 0.15378904 -1.0494914 -2.8651648 -4.1232805 -4.4531341 -4.2248611 -4.0413866 -4.1367598][-2.1448982 -2.7127688 -3.6304893 -4.0333505 -3.1109676 -1.1079504 0.72868824 1.1547294 -0.14007521 -2.1172054 -3.5039198 -3.9237297 -3.8318348 -3.8499334 -4.12189][-1.499383 -2.15957 -3.3062973 -4.0134506 -3.4122024 -1.6624632 0.067987919 0.5652833 -0.50527859 -2.220947 -3.453516 -3.8632834 -3.8661788 -4.0035629 -4.3657012][-1.4821136 -2.1436412 -3.3993514 -4.3726912 -4.2181988 -2.9434659 -1.5065808 -0.986567 -1.6816316 -2.8809192 -3.7598975 -4.0733767 -4.1159735 -4.2852988 -4.6659732][-1.9243493 -2.5767136 -3.8615382 -4.9962955 -5.2128229 -4.3689451 -3.1837473 -2.5994532 -2.8860192 -3.5404589 -4.0351877 -4.229907 -4.2974434 -4.4741745 -4.8512454][-2.300941 -2.9866767 -4.3037882 -5.5582247 -6.0570407 -5.5474625 -4.545279 -3.8761351 -3.8101394 -4.0208144 -4.1872067 -4.2585421 -4.3320489 -4.5211754 -4.887958][-2.2879791 -2.9852 -4.3331976 -5.6645727 -6.3524084 -6.1126442 -5.3194966 -4.63571 -4.3230157 -4.2445254 -4.2148309 -4.2217393 -4.29704 -4.492866 -4.830544][-2.0038586 -2.6284165 -3.9534287 -5.3106327 -6.127398 -6.1374092 -5.628531 -5.0428047 -4.6010675 -4.3774929 -4.2953644 -4.3124394 -4.4164286 -4.6252851 -4.900517][-1.7993894 -2.1738799 -3.3025784 -4.5826874 -5.4965711 -5.8044195 -5.6800942 -5.3069344 -4.8410997 -4.5484648 -4.4664521 -4.5122547 -4.6613488 -4.8962121 -5.1071219]]...]
INFO - root - 2017-12-06 08:08:03.261203: step 10310, loss = 0.79, batch loss = 0.72 (9.5 examples/sec; 0.842 sec/batch; 75h:20m:04s remains)
INFO - root - 2017-12-06 08:08:11.134174: step 10320, loss = 1.04, batch loss = 0.97 (10.3 examples/sec; 0.780 sec/batch; 69h:49m:33s remains)
INFO - root - 2017-12-06 08:08:19.003136: step 10330, loss = 0.85, batch loss = 0.78 (9.9 examples/sec; 0.805 sec/batch; 72h:02m:20s remains)
INFO - root - 2017-12-06 08:08:26.920040: step 10340, loss = 0.91, batch loss = 0.84 (10.1 examples/sec; 0.796 sec/batch; 71h:12m:03s remains)
INFO - root - 2017-12-06 08:08:34.721214: step 10350, loss = 0.99, batch loss = 0.92 (10.0 examples/sec; 0.799 sec/batch; 71h:32m:21s remains)
INFO - root - 2017-12-06 08:08:42.553885: step 10360, loss = 1.01, batch loss = 0.94 (9.9 examples/sec; 0.807 sec/batch; 72h:15m:24s remains)
INFO - root - 2017-12-06 08:08:50.454101: step 10370, loss = 0.87, batch loss = 0.80 (10.0 examples/sec; 0.798 sec/batch; 71h:22m:47s remains)
INFO - root - 2017-12-06 08:08:58.357266: step 10380, loss = 0.63, batch loss = 0.56 (10.2 examples/sec; 0.782 sec/batch; 69h:56m:21s remains)
INFO - root - 2017-12-06 08:09:06.069266: step 10390, loss = 0.93, batch loss = 0.86 (10.6 examples/sec; 0.756 sec/batch; 67h:39m:49s remains)
INFO - root - 2017-12-06 08:09:13.964767: step 10400, loss = 0.76, batch loss = 0.69 (10.1 examples/sec; 0.792 sec/batch; 70h:52m:42s remains)
2017-12-06 08:09:14.627869: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1831179 -4.246717 -4.33315 -4.3317571 -4.2266631 -4.0926237 -3.9559824 -3.9108171 -4.050982 -4.2464228 -4.39718 -4.4761529 -4.4441929 -4.3168597 -4.1892009][-4.3131585 -4.4274583 -4.5178471 -4.465971 -4.284791 -4.0948987 -3.9649642 -3.9989767 -4.234901 -4.4844928 -4.653079 -4.7435427 -4.7030959 -4.5164289 -4.3105645][-4.5665188 -4.7429261 -4.8120174 -4.6701474 -4.3932133 -4.1604357 -4.063889 -4.18849 -4.5036988 -4.7805152 -4.9433489 -5.04183 -5.0097857 -4.7847013 -4.5062652][-4.8662934 -5.0562453 -5.0186052 -4.7113996 -4.30951 -4.0539732 -4.0295887 -4.2669516 -4.6623526 -4.9804649 -5.1726284 -5.3044519 -5.297092 -5.0671368 -4.7432985][-5.0273042 -5.0936565 -4.81278 -4.2596588 -3.7285008 -3.4764843 -3.5478728 -3.9141839 -4.413763 -4.8461485 -5.1842146 -5.4277196 -5.4783678 -5.2892637 -4.9613948][-4.9911509 -4.819932 -4.2339969 -3.4440112 -2.8252096 -2.5843062 -2.7358918 -3.1935859 -3.76437 -4.3513374 -4.9406919 -5.377799 -5.5230236 -5.4059329 -5.1053433][-4.7691369 -4.3388791 -3.489166 -2.5447965 -1.8853776 -1.631887 -1.8142316 -2.3159802 -2.9132235 -3.643172 -4.5048208 -5.1778054 -5.4731073 -5.4617825 -5.2064552][-4.477387 -3.8502631 -2.8584914 -1.8848855 -1.2232473 -0.91536641 -1.0748434 -1.5767834 -2.1694086 -3.0050292 -4.0628862 -4.9217358 -5.3857613 -5.4934578 -5.2836504][-4.2660942 -3.6070609 -2.6405411 -1.7347121 -1.0613737 -0.6328361 -0.6974802 -1.1478837 -1.7238362 -2.6304724 -3.75505 -4.675355 -5.2574115 -5.4613767 -5.2905121][-4.1307716 -3.5853443 -2.7613745 -1.9667678 -1.270905 -0.70874095 -0.66622949 -1.051244 -1.6119587 -2.5437284 -3.6065912 -4.4362464 -5.0342131 -5.2857509 -5.1643858][-4.1517148 -3.7883604 -3.164166 -2.5219941 -1.8675363 -1.2648051 -1.1767309 -1.5144181 -2.0360482 -2.8881223 -3.7515333 -4.354712 -4.8466907 -5.0757656 -4.9883776][-4.277235 -4.0945735 -3.68294 -3.2383008 -2.7409585 -2.2505429 -2.2002277 -2.5102377 -2.9598269 -3.62357 -4.21367 -4.5469775 -4.84535 -4.9753923 -4.8799744][-4.4172897 -4.3391733 -4.096312 -3.8524632 -3.5895939 -3.3228638 -3.3516607 -3.6247978 -3.9611831 -4.3698883 -4.6739 -4.7875853 -4.8976407 -4.9137454 -4.7989588][-4.5651917 -4.5249572 -4.3819647 -4.2740812 -4.1998363 -4.1268797 -4.1934419 -4.3936458 -4.5973344 -4.7751465 -4.8768873 -4.8884468 -4.8929915 -4.8304567 -4.6979275][-4.6054344 -4.5853634 -4.5099459 -4.47807 -4.4998646 -4.5231524 -4.5794373 -4.6823044 -4.7617779 -4.7917771 -4.7945004 -4.7878819 -4.7632675 -4.6704125 -4.53371]]...]
INFO - root - 2017-12-06 08:09:22.578499: step 10410, loss = 0.63, batch loss = 0.56 (10.1 examples/sec; 0.794 sec/batch; 70h:59m:54s remains)
INFO - root - 2017-12-06 08:09:30.561341: step 10420, loss = 0.96, batch loss = 0.89 (9.9 examples/sec; 0.807 sec/batch; 72h:11m:53s remains)
INFO - root - 2017-12-06 08:09:38.423009: step 10430, loss = 0.95, batch loss = 0.88 (10.6 examples/sec; 0.758 sec/batch; 67h:47m:59s remains)
INFO - root - 2017-12-06 08:09:46.310465: step 10440, loss = 0.92, batch loss = 0.85 (10.1 examples/sec; 0.791 sec/batch; 70h:47m:55s remains)
INFO - root - 2017-12-06 08:09:54.051536: step 10450, loss = 1.03, batch loss = 0.96 (10.8 examples/sec; 0.742 sec/batch; 66h:23m:37s remains)
INFO - root - 2017-12-06 08:10:01.776547: step 10460, loss = 0.94, batch loss = 0.87 (10.7 examples/sec; 0.750 sec/batch; 67h:05m:54s remains)
INFO - root - 2017-12-06 08:10:09.697390: step 10470, loss = 0.92, batch loss = 0.85 (10.2 examples/sec; 0.786 sec/batch; 70h:17m:58s remains)
INFO - root - 2017-12-06 08:10:17.524466: step 10480, loss = 0.92, batch loss = 0.85 (10.1 examples/sec; 0.793 sec/batch; 70h:54m:22s remains)
INFO - root - 2017-12-06 08:10:25.293851: step 10490, loss = 0.98, batch loss = 0.91 (10.3 examples/sec; 0.774 sec/batch; 69h:12m:07s remains)
INFO - root - 2017-12-06 08:10:33.207182: step 10500, loss = 0.80, batch loss = 0.73 (9.9 examples/sec; 0.808 sec/batch; 72h:18m:24s remains)
2017-12-06 08:10:33.815720: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8021789 -5.8812795 -6.0617957 -6.2040224 -6.229557 -6.2212553 -6.2195473 -6.120398 -5.761157 -5.2166257 -4.6407738 -4.1163797 -3.7228568 -3.4566143 -3.1749024][-5.7081046 -5.8330388 -6.1235309 -6.3469114 -6.4300222 -6.445365 -6.4131756 -6.2074671 -5.7280183 -5.1027522 -4.5223312 -4.059731 -3.7487833 -3.522644 -3.2054641][-5.1667414 -5.4007077 -5.8321271 -6.1394219 -6.2989545 -6.3540449 -6.3017225 -6.0051441 -5.4915051 -4.9317942 -4.4760919 -4.1345968 -3.8971393 -3.6579921 -3.2813234][-4.1978312 -4.6214261 -5.2501121 -5.6611586 -5.8912086 -5.9730296 -5.8868933 -5.5203285 -5.0447507 -4.67895 -4.4405479 -4.2247915 -4.0203052 -3.735585 -3.3249922][-2.9159405 -3.6030383 -4.4920454 -5.052639 -5.3510413 -5.4253693 -5.2521696 -4.8075294 -4.405314 -4.2860694 -4.2374287 -4.0630093 -3.836015 -3.5309682 -3.1829977][-1.7926912 -2.6986978 -3.7848446 -4.4527445 -4.77104 -4.7866921 -4.4609923 -3.8875511 -3.5310745 -3.5987487 -3.6397247 -3.4528639 -3.2538543 -3.0626016 -2.9264474][-1.4019105 -2.3041508 -3.3467283 -3.885216 -4.0207567 -3.8471324 -3.2972317 -2.5733745 -2.2597854 -2.4804184 -2.6163449 -2.5524287 -2.5771317 -2.6537547 -2.8189816][-1.794322 -2.4540935 -3.2013078 -3.37368 -3.1406322 -2.6738474 -1.9020481 -1.114269 -0.94116759 -1.372741 -1.7017491 -1.9157009 -2.276247 -2.6264772 -3.0322948][-2.5849786 -2.8765354 -3.2349038 -3.029201 -2.4711547 -1.7810349 -0.91136503 -0.20550489 -0.28265047 -0.95264125 -1.4953468 -2.0038342 -2.6372938 -3.129709 -3.5942914][-3.2222538 -3.2082314 -3.3134532 -2.9513254 -2.3322668 -1.6286764 -0.82924318 -0.31670332 -0.634022 -1.43068 -2.0824447 -2.7463372 -3.4471292 -3.8784938 -4.2213931][-3.3764687 -3.2586632 -3.3662963 -3.1481218 -2.7463489 -2.2359324 -1.6681511 -1.4026394 -1.8256249 -2.5350749 -3.0824134 -3.6370828 -4.1286964 -4.3288913 -4.4811044][-3.32126 -3.2232771 -3.4382236 -3.4934897 -3.4119351 -3.1579995 -2.8667853 -2.8240354 -3.2004757 -3.6358807 -3.9010162 -4.1585531 -4.32104 -4.2983308 -4.3240361][-3.5289016 -3.4385557 -3.6727118 -3.9023421 -4.0386791 -3.9643519 -3.8790712 -3.9657888 -4.1906166 -4.3278031 -4.3227539 -4.3188839 -4.2352157 -4.1007223 -4.0699353][-4.1226134 -4.041214 -4.1895738 -4.4103646 -4.5787344 -4.539093 -4.5193758 -4.6188397 -4.707829 -4.6883454 -4.5703897 -4.4333806 -4.2304993 -4.06398 -4.0172091][-4.8223443 -4.8165703 -4.8638682 -4.9560618 -5.0444069 -4.964015 -4.9094753 -4.940742 -4.9436541 -4.8951368 -4.7769141 -4.6047 -4.3679624 -4.1770563 -4.12039]]...]
INFO - root - 2017-12-06 08:10:41.593883: step 10510, loss = 0.94, batch loss = 0.87 (10.2 examples/sec; 0.786 sec/batch; 70h:15m:53s remains)
INFO - root - 2017-12-06 08:10:49.460295: step 10520, loss = 0.83, batch loss = 0.76 (9.9 examples/sec; 0.806 sec/batch; 72h:04m:36s remains)
INFO - root - 2017-12-06 08:10:57.288679: step 10530, loss = 0.86, batch loss = 0.79 (10.7 examples/sec; 0.747 sec/batch; 66h:45m:53s remains)
INFO - root - 2017-12-06 08:11:05.172507: step 10540, loss = 0.97, batch loss = 0.90 (9.8 examples/sec; 0.818 sec/batch; 73h:11m:56s remains)
INFO - root - 2017-12-06 08:11:12.977898: step 10550, loss = 0.82, batch loss = 0.75 (10.7 examples/sec; 0.749 sec/batch; 66h:56m:34s remains)
INFO - root - 2017-12-06 08:11:20.883747: step 10560, loss = 1.09, batch loss = 1.02 (9.9 examples/sec; 0.811 sec/batch; 72h:29m:19s remains)
INFO - root - 2017-12-06 08:11:28.646471: step 10570, loss = 1.13, batch loss = 1.06 (10.6 examples/sec; 0.757 sec/batch; 67h:44m:04s remains)
INFO - root - 2017-12-06 08:11:36.493051: step 10580, loss = 0.78, batch loss = 0.71 (10.5 examples/sec; 0.765 sec/batch; 68h:22m:00s remains)
INFO - root - 2017-12-06 08:11:44.301445: step 10590, loss = 0.77, batch loss = 0.70 (11.0 examples/sec; 0.727 sec/batch; 65h:03m:03s remains)
INFO - root - 2017-12-06 08:11:52.045889: step 10600, loss = 1.10, batch loss = 1.03 (10.2 examples/sec; 0.788 sec/batch; 70h:25m:37s remains)
2017-12-06 08:11:52.660718: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9873028 -3.2177184 -2.4269004 -2.0054686 -2.0923469 -2.3702738 -2.7335479 -3.131587 -3.5755358 -4.0689478 -4.2682495 -4.02948 -3.6813476 -3.4352729 -3.2274265][-3.5031979 -2.7935843 -2.1331537 -1.8224411 -1.8644795 -2.0789685 -2.3990908 -2.6831644 -3.1495717 -3.7022126 -3.910851 -3.7566314 -3.5124619 -3.434058 -3.4510806][-3.2411273 -2.7282677 -2.391515 -2.332629 -2.3578079 -2.4132698 -2.5301361 -2.5738769 -2.9544556 -3.497813 -3.72437 -3.701736 -3.624259 -3.7400861 -3.9646704][-3.2793059 -3.0187793 -3.0880418 -3.3407416 -3.4053521 -3.3003106 -3.1531749 -2.956975 -3.2001941 -3.6517706 -3.8782949 -3.9447427 -3.9760818 -4.1700249 -4.4541469][-3.6587546 -3.6401751 -4.0261674 -4.4497042 -4.4810677 -4.2118616 -3.8469038 -3.5606508 -3.7549558 -4.1470761 -4.3905473 -4.4668241 -4.4817662 -4.6053548 -4.8619342][-4.253859 -4.4259887 -4.9199667 -5.2512813 -5.046494 -4.4812551 -3.8628559 -3.5659013 -3.8329928 -4.3161731 -4.6892767 -4.8120785 -4.8014607 -4.8215246 -5.0232263][-4.721005 -4.9688897 -5.3340111 -5.3487039 -4.771389 -3.8142753 -2.9503806 -2.7038512 -3.1230254 -3.841011 -4.4922652 -4.8065886 -4.8785315 -4.8624949 -5.036869][-4.7429748 -4.928102 -5.0352631 -4.6633325 -3.6784251 -2.2925346 -1.2163386 -1.0753088 -1.6905701 -2.7184696 -3.7584465 -4.443327 -4.792448 -4.9085088 -5.1368647][-4.512753 -4.6542597 -4.6266952 -4.1114283 -2.9896612 -1.3911498 -0.21263409 -0.11683607 -0.72739625 -1.789134 -3.0034761 -3.9743743 -4.591423 -4.8936367 -5.1971254][-4.3985047 -4.5732484 -4.5638566 -4.1348324 -3.1545973 -1.6582696 -0.58636546 -0.54020333 -1.0648229 -1.9906087 -3.1333106 -4.1124821 -4.7254663 -5.0033717 -5.1899719][-4.3672767 -4.5649781 -4.5975022 -4.2852349 -3.5265658 -2.357353 -1.5750034 -1.6473475 -2.1865189 -3.0464442 -4.0474811 -4.8373938 -5.2254343 -5.265728 -5.1318221][-4.2982717 -4.5080104 -4.6198444 -4.4635668 -3.9759579 -3.2138467 -2.719512 -2.8306646 -3.3284273 -4.052824 -4.7978296 -5.3139157 -5.4882483 -5.3391027 -4.9562182][-4.1293569 -4.3329878 -4.5324507 -4.5381827 -4.3167243 -3.9165902 -3.6325247 -3.7026141 -4.0587721 -4.5388927 -4.9656868 -5.2273316 -5.2859726 -5.1048164 -4.6891785][-3.9498937 -4.1124554 -4.3509269 -4.482729 -4.4766612 -4.3553319 -4.2498856 -4.307023 -4.5408168 -4.812294 -4.9776092 -5.0218673 -4.9832821 -4.8078384 -4.4512196][-3.9074626 -4.0712256 -4.3555593 -4.6196442 -4.7966695 -4.8836946 -4.9263039 -4.9700308 -5.0874906 -5.1834097 -5.1559134 -5.044488 -4.9212265 -4.7322702 -4.4448104]]...]
INFO - root - 2017-12-06 08:12:00.546257: step 10610, loss = 0.80, batch loss = 0.73 (9.9 examples/sec; 0.805 sec/batch; 71h:59m:20s remains)
INFO - root - 2017-12-06 08:12:08.284302: step 10620, loss = 1.04, batch loss = 0.97 (10.5 examples/sec; 0.759 sec/batch; 67h:53m:14s remains)
INFO - root - 2017-12-06 08:12:16.176093: step 10630, loss = 0.96, batch loss = 0.89 (9.9 examples/sec; 0.806 sec/batch; 72h:05m:49s remains)
INFO - root - 2017-12-06 08:12:24.036182: step 10640, loss = 0.75, batch loss = 0.68 (10.1 examples/sec; 0.791 sec/batch; 70h:41m:31s remains)
INFO - root - 2017-12-06 08:12:31.982528: step 10650, loss = 1.05, batch loss = 0.98 (10.0 examples/sec; 0.796 sec/batch; 71h:11m:29s remains)
INFO - root - 2017-12-06 08:12:39.806536: step 10660, loss = 1.03, batch loss = 0.96 (10.2 examples/sec; 0.787 sec/batch; 70h:22m:05s remains)
INFO - root - 2017-12-06 08:12:47.580852: step 10670, loss = 0.81, batch loss = 0.74 (10.4 examples/sec; 0.772 sec/batch; 69h:02m:11s remains)
INFO - root - 2017-12-06 08:12:55.478623: step 10680, loss = 0.76, batch loss = 0.69 (9.8 examples/sec; 0.817 sec/batch; 72h:59m:34s remains)
INFO - root - 2017-12-06 08:13:03.327630: step 10690, loss = 0.96, batch loss = 0.89 (10.2 examples/sec; 0.782 sec/batch; 69h:53m:23s remains)
INFO - root - 2017-12-06 08:13:11.010950: step 10700, loss = 0.90, batch loss = 0.83 (10.2 examples/sec; 0.782 sec/batch; 69h:53m:09s remains)
2017-12-06 08:13:11.680344: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0976317 -2.6268885 -3.2751608 -3.7866294 -4.0731297 -4.1368523 -4.1076145 -3.9825814 -3.8482985 -3.7102695 -3.557394 -3.4718158 -3.4183002 -3.3157225 -3.1161933][-2.4166806 -2.9658234 -3.5587473 -3.9857273 -4.2207041 -4.2611547 -4.2003727 -4.0451145 -3.8888528 -3.7349672 -3.568238 -3.4645572 -3.4065354 -3.3421881 -3.2242944][-2.8620477 -3.3986335 -3.8956146 -4.1889887 -4.2990155 -4.2345457 -4.0810838 -3.8705347 -3.6882696 -3.5249593 -3.3614607 -3.2749779 -3.2716508 -3.3160343 -3.350668][-3.2851729 -3.7245643 -4.0514736 -4.1483641 -4.0702634 -3.8596284 -3.6133051 -3.38162 -3.2234371 -3.1044347 -2.9921379 -2.9604659 -3.0404234 -3.2134418 -3.4114971][-3.5832634 -3.842854 -3.9408872 -3.8073108 -3.5295448 -3.1935592 -2.9061308 -2.7198958 -2.6561403 -2.6485703 -2.6424553 -2.6890733 -2.8330388 -3.0786097 -3.3703475][-3.7105842 -3.7675993 -3.6470404 -3.3303843 -2.9208636 -2.5276818 -2.2597358 -2.1596274 -2.2152016 -2.3354752 -2.4451401 -2.5631757 -2.7300127 -2.9709349 -3.2641039][-3.7057171 -3.6170073 -3.37498 -2.9883027 -2.5507116 -2.171093 -1.9555902 -1.9407029 -2.088516 -2.2939074 -2.4733224 -2.6265249 -2.7774901 -2.9606612 -3.1881924][-3.6809592 -3.5533712 -3.3063812 -2.9530325 -2.5585523 -2.2238939 -2.059932 -2.1004305 -2.2876844 -2.514627 -2.7053137 -2.85842 -2.9773908 -3.0912542 -3.2326329][-3.7361169 -3.6368132 -3.4421046 -3.1582685 -2.8299689 -2.5512936 -2.4299178 -2.4904795 -2.6641679 -2.8601694 -3.0262318 -3.1681056 -3.2651138 -3.3320961 -3.4065287][-3.8762612 -3.8081453 -3.6646955 -3.4529102 -3.2016726 -2.9877272 -2.9015651 -2.9545174 -3.0820947 -3.2240164 -3.3585234 -3.4944644 -3.5869648 -3.6334743 -3.6686676][-4.0077362 -3.9564478 -3.856056 -3.7145338 -3.5439241 -3.3939977 -3.330951 -3.3593409 -3.4321432 -3.5198138 -3.6238513 -3.7524676 -3.8475475 -3.8916981 -3.9126983][-4.0483465 -4.0103865 -3.9482665 -3.8686342 -3.7694867 -3.6737621 -3.625432 -3.6286168 -3.6567011 -3.7002931 -3.7723486 -3.879082 -3.9630795 -4.0046039 -4.0246644][-3.981508 -3.9518342 -3.9170272 -3.8836193 -3.8392978 -3.7866614 -3.7511368 -3.7385383 -3.7374086 -3.7482681 -3.7873318 -3.8587651 -3.9168499 -3.9479349 -3.9691763][-3.8500447 -3.8157043 -3.7896121 -3.779283 -3.7671003 -3.7433691 -3.7187457 -3.6980464 -3.6769774 -3.660136 -3.6631839 -3.6890671 -3.7089496 -3.7208626 -3.7376823][-3.70364 -3.6530948 -3.6199214 -3.61405 -3.6162767 -3.6122792 -3.6020362 -3.5838249 -3.5525837 -3.5145607 -3.4829664 -3.4598629 -3.4339163 -3.4148555 -3.4111037]]...]
INFO - root - 2017-12-06 08:13:19.565674: step 10710, loss = 0.80, batch loss = 0.73 (10.5 examples/sec; 0.759 sec/batch; 67h:52m:41s remains)
INFO - root - 2017-12-06 08:13:27.460612: step 10720, loss = 1.01, batch loss = 0.93 (9.9 examples/sec; 0.809 sec/batch; 72h:16m:10s remains)
INFO - root - 2017-12-06 08:13:35.383665: step 10730, loss = 0.96, batch loss = 0.89 (10.2 examples/sec; 0.786 sec/batch; 70h:17m:42s remains)
INFO - root - 2017-12-06 08:13:43.230892: step 10740, loss = 1.13, batch loss = 1.06 (10.0 examples/sec; 0.799 sec/batch; 71h:26m:58s remains)
INFO - root - 2017-12-06 08:13:51.105965: step 10750, loss = 0.93, batch loss = 0.86 (10.2 examples/sec; 0.785 sec/batch; 70h:12m:06s remains)
INFO - root - 2017-12-06 08:13:58.888329: step 10760, loss = 0.81, batch loss = 0.74 (10.5 examples/sec; 0.759 sec/batch; 67h:50m:10s remains)
INFO - root - 2017-12-06 08:14:06.766454: step 10770, loss = 0.91, batch loss = 0.84 (9.9 examples/sec; 0.805 sec/batch; 71h:56m:33s remains)
INFO - root - 2017-12-06 08:14:14.640726: step 10780, loss = 0.92, batch loss = 0.85 (10.0 examples/sec; 0.803 sec/batch; 71h:45m:17s remains)
INFO - root - 2017-12-06 08:14:22.564824: step 10790, loss = 0.82, batch loss = 0.75 (10.0 examples/sec; 0.801 sec/batch; 71h:33m:30s remains)
INFO - root - 2017-12-06 08:14:30.425129: step 10800, loss = 0.87, batch loss = 0.80 (10.1 examples/sec; 0.789 sec/batch; 70h:30m:01s remains)
2017-12-06 08:14:31.054903: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1012816 -5.0664611 -5.0384789 -5.0174417 -4.899579 -4.7529097 -4.5977025 -4.404696 -4.2677522 -4.2912512 -4.5164266 -4.733882 -4.7188783 -4.41422 -3.6629314][-4.895689 -4.8312273 -4.7730274 -4.7451472 -4.6430421 -4.48143 -4.2901053 -4.0932627 -3.9978812 -4.0925741 -4.3649111 -4.5806708 -4.5207586 -4.0916624 -3.2104852][-4.6057715 -4.5309262 -4.4523568 -4.3962703 -4.2947288 -4.1039109 -3.8716204 -3.6941249 -3.6568456 -3.8409414 -4.1857181 -4.415998 -4.3825707 -3.9591248 -3.1648214][-4.3239894 -4.2920947 -4.2083783 -4.0594759 -3.8258076 -3.480623 -3.1406932 -3.0293639 -3.1107035 -3.4093955 -3.8414941 -4.1298156 -4.2278337 -3.998966 -3.4813828][-4.2434626 -4.24818 -4.0977979 -3.7544653 -3.2477708 -2.6023021 -2.0312014 -1.9934103 -2.2807865 -2.718307 -3.2468345 -3.6384702 -3.9257438 -3.971159 -3.7832971][-4.4983459 -4.4626861 -4.1081061 -3.4528191 -2.5978885 -1.6215286 -0.77618408 -0.81154895 -1.3726676 -1.9576285 -2.5686865 -3.0750575 -3.5547161 -3.887723 -4.004179][-4.9112377 -4.8118968 -4.234354 -3.3061662 -2.2151494 -1.1011019 -0.15566397 -0.29213381 -1.0969105 -1.7538278 -2.3317719 -2.8630419 -3.4083714 -3.8595543 -4.1269169][-5.165225 -5.0995631 -4.488678 -3.5151925 -2.4281325 -1.4665549 -0.68983579 -0.88272524 -1.7008297 -2.2199328 -2.5986629 -3.0380292 -3.5317032 -3.9309859 -4.1987615][-5.1782966 -5.2350321 -4.7532067 -3.9289317 -3.0190108 -2.3439281 -1.8541496 -2.0335846 -2.6644335 -2.9358954 -3.0883214 -3.4365354 -3.8748071 -4.18031 -4.39098][-4.9707465 -5.1160836 -4.7704697 -4.1512642 -3.4934328 -3.1127203 -2.941215 -3.1826768 -3.6266475 -3.6698587 -3.642242 -3.9222891 -4.3308411 -4.5640712 -4.6934714][-4.4904294 -4.6912327 -4.4907961 -4.0983028 -3.7036061 -3.5692735 -3.6643076 -4.0198 -4.3378739 -4.2232032 -4.0813537 -4.2840662 -4.6494594 -4.826025 -4.8714137][-4.008359 -4.2367988 -4.1492109 -3.9231987 -3.7090936 -3.7186892 -3.9838321 -4.4182777 -4.6446953 -4.4536815 -4.2737 -4.4016871 -4.6960845 -4.8361382 -4.8343859][-3.7084184 -3.9055505 -3.8857863 -3.7640972 -3.6289041 -3.6733985 -4.0175495 -4.471642 -4.6397538 -4.4526067 -4.3032246 -4.3985834 -4.6277652 -4.7592726 -4.7718573][-3.4559495 -3.6016104 -3.680141 -3.7363014 -3.6922688 -3.7193949 -4.0764332 -4.5024433 -4.6429 -4.5268679 -4.4613276 -4.5682898 -4.7402678 -4.8462319 -4.874577][-3.1610277 -3.3133316 -3.5816422 -3.9360852 -4.0676808 -4.07522 -4.368423 -4.7104058 -4.8349309 -4.8075657 -4.8098707 -4.9061871 -4.99476 -5.0334935 -5.0565176]]...]
INFO - root - 2017-12-06 08:14:38.870129: step 10810, loss = 0.93, batch loss = 0.86 (9.8 examples/sec; 0.813 sec/batch; 72h:36m:47s remains)
INFO - root - 2017-12-06 08:14:46.787875: step 10820, loss = 0.88, batch loss = 0.81 (10.0 examples/sec; 0.797 sec/batch; 71h:11m:30s remains)
INFO - root - 2017-12-06 08:14:54.686821: step 10830, loss = 0.70, batch loss = 0.63 (9.9 examples/sec; 0.809 sec/batch; 72h:17m:02s remains)
INFO - root - 2017-12-06 08:15:02.587191: step 10840, loss = 0.87, batch loss = 0.80 (10.5 examples/sec; 0.765 sec/batch; 68h:22m:48s remains)
INFO - root - 2017-12-06 08:15:10.534775: step 10850, loss = 1.19, batch loss = 1.12 (9.7 examples/sec; 0.824 sec/batch; 73h:36m:30s remains)
INFO - root - 2017-12-06 08:15:18.378741: step 10860, loss = 0.95, batch loss = 0.88 (10.3 examples/sec; 0.773 sec/batch; 69h:05m:17s remains)
INFO - root - 2017-12-06 08:15:26.209371: step 10870, loss = 0.90, batch loss = 0.83 (10.3 examples/sec; 0.778 sec/batch; 69h:28m:35s remains)
INFO - root - 2017-12-06 08:15:34.131567: step 10880, loss = 1.01, batch loss = 0.94 (9.9 examples/sec; 0.810 sec/batch; 72h:22m:48s remains)
INFO - root - 2017-12-06 08:15:42.106737: step 10890, loss = 0.89, batch loss = 0.82 (9.9 examples/sec; 0.807 sec/batch; 72h:04m:05s remains)
INFO - root - 2017-12-06 08:15:49.938428: step 10900, loss = 0.86, batch loss = 0.79 (9.9 examples/sec; 0.805 sec/batch; 71h:56m:06s remains)
2017-12-06 08:15:50.592249: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5115638 -4.6154122 -4.7862906 -4.9274092 -4.9889154 -4.9989228 -5.0426917 -5.1578455 -5.18115 -5.140945 -5.0359488 -4.73075 -4.2288809 -3.7689536 -3.791389][-4.469419 -4.604794 -4.7494426 -4.7820177 -4.7362738 -4.7259626 -4.7784767 -4.8690209 -4.8439221 -4.7781239 -4.6091075 -4.1431494 -3.5619326 -3.1675508 -3.2903967][-4.535675 -4.7207112 -4.8335228 -4.7399716 -4.6108389 -4.6668849 -4.8045197 -4.8895559 -4.8678 -4.8746247 -4.70615 -4.0807447 -3.368119 -2.9617324 -3.1240535][-4.5305476 -4.8272524 -4.9986682 -4.8713641 -4.72516 -4.8810611 -5.0980082 -5.1577621 -5.1246548 -5.1775107 -4.9857268 -4.2159777 -3.3593872 -2.9085317 -3.1130695][-4.1282282 -4.5665026 -4.8542671 -4.75764 -4.5996494 -4.7925558 -5.0130377 -4.9981503 -4.9269328 -5.0022678 -4.8380861 -4.0731921 -3.1973665 -2.7931693 -3.1252527][-3.0554142 -3.5320115 -3.883924 -3.8138249 -3.6435478 -3.8754635 -4.1344833 -4.1588173 -4.1901617 -4.3864813 -4.3825283 -3.7774978 -2.9792919 -2.6646757 -3.1499918][-1.5887256 -2.0478883 -2.4793298 -2.51572 -2.4055693 -2.7074184 -3.0143375 -3.1430075 -3.3851805 -3.7437277 -3.8918238 -3.4243584 -2.6971331 -2.4781346 -3.1012692][-0.64049745 -1.0687661 -1.5639665 -1.7279856 -1.7024064 -2.0543215 -2.3745823 -2.585207 -3.005259 -3.460484 -3.6598947 -3.2360108 -2.5372458 -2.3785026 -3.0751231][-0.78605986 -1.1669939 -1.6751013 -1.9324667 -2.0001431 -2.376533 -2.6968069 -2.9431376 -3.4208553 -3.8523841 -3.9776678 -3.4834347 -2.7412033 -2.5399864 -3.1504586][-1.7331722 -2.0661926 -2.5397623 -2.8667569 -3.0540433 -3.4561851 -3.75324 -3.9721124 -4.3794956 -4.6899624 -4.6495738 -4.0141397 -3.1830797 -2.8447285 -3.2061338][-2.8282595 -3.1009724 -3.4568877 -3.7627149 -4.0127482 -4.4074087 -4.646647 -4.7884712 -5.070838 -5.2544208 -5.075839 -4.3810806 -3.5650592 -3.1625414 -3.2952316][-3.6394446 -3.8354306 -4.0470977 -4.2712407 -4.5312943 -4.8903642 -5.0784864 -5.1505032 -5.3114119 -5.3847618 -5.1477184 -4.5630765 -3.9310672 -3.5805888 -3.5719628][-4.0539446 -4.1924548 -4.322998 -4.485393 -4.7155809 -5.0020542 -5.1453352 -5.1810031 -5.2500525 -5.2474122 -5.0178676 -4.6333737 -4.2707181 -4.0434752 -3.9718096][-4.2300892 -4.37451 -4.5007157 -4.6191564 -4.783545 -4.9853768 -5.09004 -5.1127987 -5.137888 -5.1190553 -4.9538245 -4.7648778 -4.6242256 -4.4955182 -4.3934216][-4.4392204 -4.5982223 -4.7287583 -4.8133516 -4.900867 -5.0077577 -5.062778 -5.0691028 -5.0785847 -5.0754514 -4.9908333 -4.9422846 -4.92987 -4.8630934 -4.7646689]]...]
INFO - root - 2017-12-06 08:15:58.292356: step 10910, loss = 0.83, batch loss = 0.76 (10.3 examples/sec; 0.776 sec/batch; 69h:21m:54s remains)
INFO - root - 2017-12-06 08:16:06.148284: step 10920, loss = 0.76, batch loss = 0.69 (9.8 examples/sec; 0.817 sec/batch; 72h:56m:48s remains)
INFO - root - 2017-12-06 08:16:14.098385: step 10930, loss = 0.83, batch loss = 0.76 (9.6 examples/sec; 0.830 sec/batch; 74h:06m:11s remains)
INFO - root - 2017-12-06 08:16:21.921882: step 10940, loss = 0.97, batch loss = 0.90 (10.4 examples/sec; 0.768 sec/batch; 68h:36m:17s remains)
INFO - root - 2017-12-06 08:16:29.944177: step 10950, loss = 0.85, batch loss = 0.78 (9.8 examples/sec; 0.817 sec/batch; 72h:58m:27s remains)
INFO - root - 2017-12-06 08:16:37.808674: step 10960, loss = 1.04, batch loss = 0.97 (10.5 examples/sec; 0.760 sec/batch; 67h:51m:13s remains)
INFO - root - 2017-12-06 08:16:45.665208: step 10970, loss = 0.75, batch loss = 0.68 (9.9 examples/sec; 0.808 sec/batch; 72h:09m:59s remains)
INFO - root - 2017-12-06 08:16:53.535723: step 10980, loss = 0.78, batch loss = 0.71 (10.3 examples/sec; 0.779 sec/batch; 69h:34m:57s remains)
INFO - root - 2017-12-06 08:17:01.524658: step 10990, loss = 0.93, batch loss = 0.86 (10.1 examples/sec; 0.795 sec/batch; 70h:57m:19s remains)
INFO - root - 2017-12-06 08:17:09.412300: step 11000, loss = 0.86, batch loss = 0.79 (10.0 examples/sec; 0.799 sec/batch; 71h:20m:10s remains)
2017-12-06 08:17:10.039185: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4940276 -4.5857186 -4.5697918 -4.4982381 -4.4832206 -4.5503559 -4.6460495 -4.7355313 -4.7756248 -4.7722268 -4.7716794 -4.7408323 -4.6869831 -4.6689696 -4.6532016][-4.6355886 -4.7728987 -4.7441478 -4.6303782 -4.5935545 -4.6711698 -4.790956 -4.9151144 -4.966846 -4.9744992 -5.0562282 -5.117918 -5.1088409 -5.1397014 -5.1575003][-4.7698412 -4.9578719 -4.9123569 -4.7470946 -4.6543612 -4.66464 -4.7257557 -4.8441486 -4.9154425 -4.9547834 -5.1491246 -5.345674 -5.4323478 -5.5775189 -5.6932983][-4.8482318 -5.03253 -4.9041729 -4.6398311 -4.4636254 -4.3344836 -4.2379203 -4.3249922 -4.4886794 -4.6320853 -4.9666576 -5.3253884 -5.554328 -5.8570094 -6.1165094][-4.8876743 -4.9860706 -4.6761088 -4.2333035 -3.9302363 -3.5840628 -3.2368073 -3.275362 -3.633477 -4.0087047 -4.5384755 -5.0873475 -5.4952979 -5.9436607 -6.3245912][-4.9969349 -4.96914 -4.4319654 -3.7460017 -3.2367749 -2.5789967 -1.8868899 -1.8421233 -2.496438 -3.2554359 -4.0328035 -4.766871 -5.3316188 -5.8440375 -6.2536993][-5.2874613 -5.1240568 -4.3677187 -3.4055872 -2.622273 -1.6060438 -0.55492949 -0.41016388 -1.3464596 -2.5470476 -3.5928957 -4.4632921 -5.1072979 -5.588181 -5.917273][-5.6553698 -5.3596649 -4.44431 -3.2861001 -2.329638 -1.1445436 0.044718742 0.2220602 -0.84666562 -2.3025944 -3.4823666 -4.3360457 -4.9299822 -5.3023586 -5.4967489][-5.9258289 -5.5132256 -4.5352545 -3.3501542 -2.4357111 -1.3941185 -0.40045023 -0.31940556 -1.3411839 -2.7583623 -3.8307261 -4.476119 -4.8771095 -5.1000986 -5.1651111][-5.9800177 -5.5114164 -4.5884686 -3.5511558 -2.8538775 -2.1703176 -1.5597255 -1.6205661 -2.5064552 -3.6742327 -4.4339542 -4.7445593 -4.8890586 -4.9619861 -4.9456148][-5.8346157 -5.4204884 -4.6846209 -3.9397202 -3.5548062 -3.2686157 -3.0057926 -3.1335707 -3.8099554 -4.6090603 -4.97572 -4.9657917 -4.8990397 -4.8677335 -4.8362432][-5.4691181 -5.174119 -4.6835508 -4.2690682 -4.1817036 -4.2135019 -4.1821132 -4.3054557 -4.762496 -5.2053556 -5.245811 -5.0124321 -4.8239446 -4.7478509 -4.7323928][-4.9410944 -4.7479453 -4.4588871 -4.2867217 -4.3842649 -4.5947204 -4.7035193 -4.8385944 -5.1598153 -5.3680205 -5.2163982 -4.88573 -4.6538353 -4.5665994 -4.5551233][-4.4446049 -4.3156366 -4.1433854 -4.0888143 -4.2383528 -4.4821858 -4.651967 -4.816731 -5.0695848 -5.1561947 -4.9391122 -4.6145325 -4.409873 -4.3436227 -4.3303885][-4.1382284 -4.0460587 -3.9225812 -3.8799682 -3.98316 -4.1634231 -4.3169532 -4.4856558 -4.6845717 -4.727931 -4.5417957 -4.3047137 -4.1783633 -4.1545296 -4.1502504]]...]
INFO - root - 2017-12-06 08:17:17.916890: step 11010, loss = 0.91, batch loss = 0.84 (9.9 examples/sec; 0.809 sec/batch; 72h:16m:39s remains)
INFO - root - 2017-12-06 08:17:25.627557: step 11020, loss = 1.03, batch loss = 0.96 (10.5 examples/sec; 0.763 sec/batch; 68h:10m:32s remains)
INFO - root - 2017-12-06 08:17:33.465039: step 11030, loss = 0.80, batch loss = 0.73 (9.9 examples/sec; 0.808 sec/batch; 72h:10m:54s remains)
INFO - root - 2017-12-06 08:17:41.429403: step 11040, loss = 1.10, batch loss = 1.03 (10.0 examples/sec; 0.796 sec/batch; 71h:05m:11s remains)
INFO - root - 2017-12-06 08:17:49.292436: step 11050, loss = 1.05, batch loss = 0.98 (10.2 examples/sec; 0.786 sec/batch; 70h:11m:37s remains)
INFO - root - 2017-12-06 08:17:57.140808: step 11060, loss = 0.84, batch loss = 0.77 (10.6 examples/sec; 0.755 sec/batch; 67h:26m:15s remains)
INFO - root - 2017-12-06 08:18:04.948056: step 11070, loss = 0.86, batch loss = 0.79 (10.2 examples/sec; 0.784 sec/batch; 70h:00m:10s remains)
INFO - root - 2017-12-06 08:18:12.865344: step 11080, loss = 1.25, batch loss = 1.18 (10.2 examples/sec; 0.781 sec/batch; 69h:44m:04s remains)
INFO - root - 2017-12-06 08:18:20.773467: step 11090, loss = 1.14, batch loss = 1.07 (9.8 examples/sec; 0.813 sec/batch; 72h:37m:22s remains)
INFO - root - 2017-12-06 08:18:28.549561: step 11100, loss = 0.98, batch loss = 0.91 (10.1 examples/sec; 0.789 sec/batch; 70h:27m:17s remains)
2017-12-06 08:18:29.160124: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8876905 -4.2348919 -4.4512534 -4.6215763 -4.839787 -5.0020781 -4.8755517 -4.5035882 -4.1148148 -3.8653913 -3.9210305 -4.3279819 -4.7039766 -4.8035951 -4.6007681][-3.6792958 -3.9331729 -4.1503453 -4.4077282 -4.7180552 -4.9852319 -4.9386334 -4.5650783 -4.1311541 -3.8866422 -3.9690518 -4.376246 -4.7323055 -4.8328853 -4.6372523][-3.7301905 -3.9130819 -4.1045852 -4.3847613 -4.656354 -4.8469448 -4.7795744 -4.4471893 -4.0999885 -3.9682677 -4.0830736 -4.4374537 -4.79474 -4.9945507 -4.89215][-4.0237675 -4.1746492 -4.2637606 -4.4158688 -4.4956284 -4.4435716 -4.2547388 -3.9883773 -3.8531995 -3.9653578 -4.1948647 -4.5044222 -4.8605914 -5.2044439 -5.2450376][-4.3091478 -4.423316 -4.3333797 -4.2389903 -4.0246339 -3.665441 -3.360126 -3.2254729 -3.4249084 -3.8892546 -4.3210125 -4.6025786 -4.874033 -5.2595954 -5.40389][-4.265388 -4.3844767 -4.2067652 -3.9324896 -3.4306984 -2.7083988 -2.2191029 -2.2281091 -2.8147502 -3.6218255 -4.2426372 -4.5259681 -4.7102871 -5.0607948 -5.2599797][-3.9578409 -4.0761766 -3.9265585 -3.6313033 -2.9857912 -1.9318435 -1.1228878 -1.140739 -2.0300062 -3.0908253 -3.8533082 -4.2325583 -4.49238 -4.861681 -5.1078711][-3.6667166 -3.6704569 -3.5080135 -3.2913537 -2.743484 -1.6107664 -0.54363084 -0.46504474 -1.4854763 -2.6893024 -3.5886602 -4.1448636 -4.5524812 -4.9287434 -5.1806245][-3.4296799 -3.3139992 -3.1412694 -3.0449576 -2.7634482 -1.9071589 -0.89909458 -0.75192833 -1.6407692 -2.7852893 -3.7402043 -4.3969903 -4.7943664 -5.0303288 -5.1932163][-3.3129053 -3.2348869 -3.1652822 -3.1964271 -3.1713545 -2.7373362 -2.0546415 -1.8832772 -2.4168413 -3.217376 -4.0141535 -4.5824943 -4.7757583 -4.7629981 -4.8043823][-3.4566431 -3.4753454 -3.5485389 -3.6704266 -3.7762299 -3.6561313 -3.3181472 -3.1681314 -3.329895 -3.6606865 -4.1393681 -4.5157676 -4.5145125 -4.3245311 -4.27351][-3.7514215 -3.7873614 -3.9204495 -4.0797248 -4.2028255 -4.2130232 -4.1146817 -4.0579371 -4.0390496 -4.05833 -4.2384081 -4.4151187 -4.3366079 -4.1275234 -4.0374136][-4.0698085 -4.066925 -4.1908145 -4.3445525 -4.4488564 -4.497551 -4.5324969 -4.5781741 -4.5427766 -4.4685197 -4.4818954 -4.508347 -4.4262571 -4.2934136 -4.2006145][-4.4565353 -4.4527483 -4.5564361 -4.68954 -4.7719636 -4.8170652 -4.882947 -4.9536057 -4.9239759 -4.8453865 -4.7834072 -4.699595 -4.6013923 -4.5034795 -4.4161935][-4.7640858 -4.800632 -4.8988357 -4.998426 -5.0452685 -5.0593696 -5.0980182 -5.1417475 -5.1100378 -5.0487752 -4.9726267 -4.8498244 -4.7204247 -4.5940542 -4.4954734]]...]
INFO - root - 2017-12-06 08:18:37.068828: step 11110, loss = 1.08, batch loss = 1.01 (10.1 examples/sec; 0.792 sec/batch; 70h:41m:09s remains)
INFO - root - 2017-12-06 08:18:44.875405: step 11120, loss = 0.73, batch loss = 0.66 (10.8 examples/sec; 0.743 sec/batch; 66h:21m:22s remains)
INFO - root - 2017-12-06 08:18:52.724961: step 11130, loss = 0.83, batch loss = 0.76 (10.0 examples/sec; 0.800 sec/batch; 71h:25m:30s remains)
INFO - root - 2017-12-06 08:19:00.631758: step 11140, loss = 1.18, batch loss = 1.11 (10.4 examples/sec; 0.773 sec/batch; 68h:58m:18s remains)
INFO - root - 2017-12-06 08:19:08.356245: step 11150, loss = 1.03, batch loss = 0.96 (10.4 examples/sec; 0.770 sec/batch; 68h:42m:43s remains)
INFO - root - 2017-12-06 08:19:16.347753: step 11160, loss = 1.02, batch loss = 0.95 (9.7 examples/sec; 0.822 sec/batch; 73h:23m:40s remains)
INFO - root - 2017-12-06 08:19:24.195820: step 11170, loss = 1.06, batch loss = 0.99 (10.2 examples/sec; 0.781 sec/batch; 69h:41m:09s remains)
INFO - root - 2017-12-06 08:19:32.049284: step 11180, loss = 1.14, batch loss = 1.07 (10.8 examples/sec; 0.741 sec/batch; 66h:06m:05s remains)
INFO - root - 2017-12-06 08:19:39.922712: step 11190, loss = 0.70, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 69h:23m:33s remains)
INFO - root - 2017-12-06 08:19:47.858670: step 11200, loss = 0.96, batch loss = 0.89 (10.4 examples/sec; 0.768 sec/batch; 68h:34m:08s remains)
2017-12-06 08:19:48.523695: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2068167 -4.2972465 -4.265883 -4.1941485 -4.1237736 -4.0274825 -4.0077477 -4.099966 -4.3070278 -4.5841479 -4.7429352 -4.6081252 -4.2679114 -3.9099855 -3.6117074][-3.8595934 -4.0893741 -4.1500721 -4.1115761 -4.0469441 -3.9168169 -3.8674784 -3.9722211 -4.2611184 -4.661911 -4.9259524 -4.8047848 -4.3696084 -3.8782816 -3.4999228][-3.7462718 -4.1086698 -4.25304 -4.2009072 -4.0525007 -3.7824812 -3.6365387 -3.7157266 -4.046216 -4.5295529 -4.88702 -4.8433328 -4.4430065 -3.9719796 -3.6483312][-3.905237 -4.3160014 -4.4590082 -4.3191013 -3.9923716 -3.4955451 -3.1841326 -3.1941457 -3.5573063 -4.133193 -4.6032782 -4.7161446 -4.49745 -4.2146387 -4.074491][-4.0950775 -4.4631963 -4.5300136 -4.2465911 -3.6962733 -2.9429202 -2.4163303 -2.2893405 -2.6691694 -3.3611121 -3.9790726 -4.32896 -4.4077182 -4.4145455 -4.5201526][-4.1901331 -4.4357038 -4.374547 -3.9314306 -3.1710038 -2.1893425 -1.420532 -1.1032686 -1.5066385 -2.347451 -3.1429982 -3.7439075 -4.1353412 -4.4115806 -4.7241964][-4.2002783 -4.3264465 -4.1516318 -3.584971 -2.6785727 -1.548913 -0.57196689 -0.071887016 -0.52783751 -1.5474429 -2.5152597 -3.2850196 -3.8550878 -4.2339187 -4.6059508][-4.2462397 -4.3692694 -4.2025709 -3.644515 -2.7391005 -1.6174953 -0.58877993 -0.0017385483 -0.45905256 -1.5431135 -2.5469384 -3.2705476 -3.7546434 -3.9906175 -4.2468338][-4.3476281 -4.5904789 -4.5728135 -4.1797314 -3.4323058 -2.4849935 -1.609127 -1.0981619 -1.444057 -2.3188858 -3.1024327 -3.5138702 -3.6308303 -3.5020044 -3.5460293][-4.3993506 -4.7912121 -4.9906526 -4.8464465 -4.3321815 -3.6145928 -2.9630885 -2.5684078 -2.7127628 -3.1687708 -3.526607 -3.4784303 -3.0838008 -2.5022917 -2.4007919][-4.3599663 -4.8264527 -5.1868606 -5.2455015 -4.9353619 -4.3907819 -3.9110427 -3.6107569 -3.5410569 -3.556253 -3.4661002 -2.9967518 -2.1423979 -1.1987205 -1.1335166][-4.2632709 -4.7012019 -5.1034775 -5.2736025 -5.1184416 -4.7109303 -4.3709536 -4.1629057 -3.96252 -3.7001216 -3.3282895 -2.6614223 -1.6331983 -0.60396314 -0.6639235][-4.1685634 -4.53489 -4.9067731 -5.1267548 -5.106082 -4.8610711 -4.682055 -4.60422 -4.426641 -4.1086159 -3.6683061 -3.054471 -2.1901078 -1.3912907 -1.5199571][-4.0951219 -4.4002538 -4.7332683 -4.97243 -5.0646935 -4.995893 -4.9956656 -5.0646558 -5.0260267 -4.7973275 -4.3975763 -3.92466 -3.3476925 -2.8659139 -2.9438736][-4.0223627 -4.2791438 -4.5800204 -4.8184595 -4.9843321 -5.056345 -5.1911063 -5.3481855 -5.4297895 -5.3039274 -4.9630466 -4.5936136 -4.2324648 -3.9629333 -3.8837914]]...]
INFO - root - 2017-12-06 08:19:56.359874: step 11210, loss = 0.87, batch loss = 0.80 (10.2 examples/sec; 0.783 sec/batch; 69h:53m:08s remains)
INFO - root - 2017-12-06 08:20:04.272776: step 11220, loss = 1.01, batch loss = 0.94 (9.9 examples/sec; 0.807 sec/batch; 72h:00m:14s remains)
INFO - root - 2017-12-06 08:20:11.968325: step 11230, loss = 0.85, batch loss = 0.78 (10.4 examples/sec; 0.767 sec/batch; 68h:27m:55s remains)
INFO - root - 2017-12-06 08:20:19.885712: step 11240, loss = 0.69, batch loss = 0.62 (10.2 examples/sec; 0.785 sec/batch; 70h:05m:37s remains)
INFO - root - 2017-12-06 08:20:27.586461: step 11250, loss = 1.01, batch loss = 0.94 (10.2 examples/sec; 0.781 sec/batch; 69h:40m:07s remains)
INFO - root - 2017-12-06 08:20:35.350574: step 11260, loss = 0.83, batch loss = 0.76 (10.2 examples/sec; 0.788 sec/batch; 70h:18m:48s remains)
INFO - root - 2017-12-06 08:20:43.314107: step 11270, loss = 0.99, batch loss = 0.92 (10.1 examples/sec; 0.793 sec/batch; 70h:43m:34s remains)
INFO - root - 2017-12-06 08:20:51.219380: step 11280, loss = 1.09, batch loss = 1.02 (9.9 examples/sec; 0.807 sec/batch; 71h:58m:25s remains)
INFO - root - 2017-12-06 08:20:59.011816: step 11290, loss = 0.98, batch loss = 0.91 (10.5 examples/sec; 0.763 sec/batch; 68h:02m:36s remains)
INFO - root - 2017-12-06 08:21:06.877307: step 11300, loss = 0.87, batch loss = 0.80 (10.0 examples/sec; 0.804 sec/batch; 71h:42m:13s remains)
2017-12-06 08:21:07.519781: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7491679 -4.829453 -4.7099066 -4.2949042 -3.9685171 -3.940805 -4.1375751 -4.5700288 -4.812427 -4.6158314 -3.9489119 -3.1403358 -2.85425 -3.0324907 -3.2932205][-4.4614983 -4.6239519 -4.6560364 -4.3805161 -4.0280309 -3.7597389 -3.6723204 -4.0241432 -4.4035506 -4.3935537 -3.7687387 -2.919641 -2.6667566 -2.9445148 -3.2444992][-3.7962191 -4.0776839 -4.3203073 -4.3195987 -4.0692339 -3.589689 -3.2172875 -3.4365749 -3.866384 -4.0270557 -3.4783492 -2.6002011 -2.3601062 -2.6871812 -2.996335][-3.1764476 -3.5323291 -3.9434977 -4.21735 -4.0471678 -3.3199093 -2.6438441 -2.7141891 -3.1982489 -3.5709541 -3.1962333 -2.3687809 -2.1121297 -2.397608 -2.6750448][-2.9813817 -3.3378716 -3.7922246 -4.1575565 -3.8642564 -2.7750354 -1.7681696 -1.705828 -2.3235888 -2.9949508 -2.9347289 -2.3236175 -2.0884194 -2.2887168 -2.5211582][-3.1627979 -3.4621053 -3.8075576 -3.9919779 -3.3440142 -1.8428216 -0.55791092 -0.42040062 -1.2502024 -2.2661684 -2.6220937 -2.3889122 -2.272748 -2.4086084 -2.6216357][-3.4475772 -3.6943007 -3.8623123 -3.7237828 -2.6734085 -0.8688221 0.57445765 0.74940252 -0.22787237 -1.465786 -2.1641071 -2.3481495 -2.4649792 -2.6413541 -2.9212129][-3.7049096 -3.9262204 -3.9772036 -3.6170805 -2.4069157 -0.59261584 0.861207 1.1122508 0.23274088 -0.92218924 -1.7307966 -2.2200506 -2.6007304 -2.9201436 -3.3271046][-3.9047804 -4.0955939 -4.1441064 -3.8226385 -2.8494906 -1.3837278 -0.11290741 0.28009462 -0.20961237 -0.96836424 -1.62268 -2.1809511 -2.7088966 -3.1859899 -3.6949158][-3.9337738 -4.0503817 -4.1684203 -4.0875974 -3.5997469 -2.6747146 -1.6823816 -1.1624205 -1.2199085 -1.5099778 -1.869242 -2.2690034 -2.7463923 -3.2883368 -3.8399522][-3.6309628 -3.6615062 -3.8548551 -4.0622349 -4.086483 -3.7102461 -3.0276084 -2.4575663 -2.2104211 -2.1615322 -2.2568944 -2.4275744 -2.7338171 -3.2282248 -3.7629554][-3.1439593 -3.1032929 -3.3109398 -3.6863365 -4.044313 -4.0977354 -3.7474289 -3.2499566 -2.8764205 -2.6534917 -2.5635452 -2.5280595 -2.6455193 -3.0509677 -3.5515227][-2.7672448 -2.6781039 -2.8398266 -3.2365355 -3.7001863 -3.9901881 -3.9164314 -3.5711904 -3.2267685 -2.9920597 -2.8333244 -2.66336 -2.6309085 -2.9266233 -3.357692][-2.7507353 -2.6429968 -2.7224755 -3.0231135 -3.413486 -3.7453341 -3.7982538 -3.5720561 -3.3520052 -3.2636805 -3.1826487 -2.9945841 -2.8919091 -3.0733461 -3.3851032][-3.0834517 -3.0675664 -3.1048579 -3.2604098 -3.4671378 -3.6822236 -3.7087922 -3.5151491 -3.4264126 -3.5483997 -3.6335685 -3.5199909 -3.4207609 -3.5057511 -3.6736608]]...]
INFO - root - 2017-12-06 08:21:15.362684: step 11310, loss = 0.89, batch loss = 0.82 (10.7 examples/sec; 0.750 sec/batch; 66h:56m:17s remains)
INFO - root - 2017-12-06 08:21:23.288701: step 11320, loss = 0.62, batch loss = 0.55 (10.7 examples/sec; 0.744 sec/batch; 66h:24m:54s remains)
INFO - root - 2017-12-06 08:21:31.234385: step 11330, loss = 0.83, batch loss = 0.76 (9.6 examples/sec; 0.832 sec/batch; 74h:11m:45s remains)
INFO - root - 2017-12-06 08:21:39.102424: step 11340, loss = 0.72, batch loss = 0.65 (10.1 examples/sec; 0.788 sec/batch; 70h:19m:02s remains)
INFO - root - 2017-12-06 08:21:46.937963: step 11350, loss = 1.01, batch loss = 0.93 (10.2 examples/sec; 0.781 sec/batch; 69h:39m:28s remains)
INFO - root - 2017-12-06 08:21:54.834051: step 11360, loss = 1.07, batch loss = 1.00 (9.9 examples/sec; 0.804 sec/batch; 71h:44m:39s remains)
INFO - root - 2017-12-06 08:22:02.629789: step 11370, loss = 0.98, batch loss = 0.91 (10.2 examples/sec; 0.787 sec/batch; 70h:11m:23s remains)
INFO - root - 2017-12-06 08:22:10.467277: step 11380, loss = 1.04, batch loss = 0.97 (10.3 examples/sec; 0.780 sec/batch; 69h:33m:19s remains)
INFO - root - 2017-12-06 08:22:18.283747: step 11390, loss = 0.83, batch loss = 0.76 (10.2 examples/sec; 0.783 sec/batch; 69h:48m:46s remains)
INFO - root - 2017-12-06 08:22:26.123947: step 11400, loss = 0.96, batch loss = 0.89 (9.8 examples/sec; 0.814 sec/batch; 72h:37m:09s remains)
2017-12-06 08:22:26.746227: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0755758 -5.1495485 -5.2324529 -5.2812948 -5.30778 -5.3548098 -5.3651628 -5.3301783 -5.27857 -5.2395906 -5.2238436 -5.2258592 -5.2273545 -5.1669288 -5.0065975][-5.340086 -5.4473829 -5.5232716 -5.5391769 -5.5705996 -5.6853828 -5.7528119 -5.7112951 -5.627223 -5.5623174 -5.5205832 -5.4821978 -5.4601469 -5.3804073 -5.1606688][-5.3824644 -5.488893 -5.5189872 -5.4704857 -5.4963775 -5.6883049 -5.8078728 -5.7577305 -5.687283 -5.6771069 -5.6603346 -5.5858421 -5.5365314 -5.4645185 -5.2353811][-5.1425667 -5.2534113 -5.2667322 -5.1828251 -5.1658611 -5.3194928 -5.3852754 -5.2997026 -5.3014865 -5.4761262 -5.5963736 -5.5330915 -5.454092 -5.3816643 -5.1598549][-4.8206539 -4.90933 -4.9357586 -4.8315768 -4.7009048 -4.6541653 -4.5036592 -4.2833738 -4.35474 -4.8090019 -5.2168474 -5.2863808 -5.2194328 -5.1357193 -4.9106765][-4.53637 -4.5390325 -4.5622454 -4.4325066 -4.138936 -3.7962251 -3.3006873 -2.8101447 -2.8607688 -3.642241 -4.4869785 -4.8698936 -4.9090371 -4.827445 -4.5841017][-4.4656425 -4.3441792 -4.3040614 -4.1312919 -3.697686 -3.0716486 -2.1678355 -1.2453332 -1.1479714 -2.2546208 -3.5853961 -4.3899245 -4.6424232 -4.5981679 -4.3350425][-4.49979 -4.2223325 -4.0742049 -3.8986759 -3.4422553 -2.7043 -1.5532186 -0.21325779 0.12407017 -1.1895497 -2.8856454 -4.0540667 -4.5355544 -4.5389519 -4.2494164][-4.4192362 -3.9939814 -3.7869205 -3.7385218 -3.4550307 -2.8462393 -1.7809134 -0.38324738 0.098845005 -1.1295199 -2.8593214 -4.1398039 -4.7012472 -4.6564178 -4.2798939][-4.3297982 -3.7678468 -3.5488465 -3.7019014 -3.7148304 -3.3930678 -2.6596916 -1.5363944 -1.0492532 -2.0017591 -3.483434 -4.5887132 -5.0250421 -4.8308592 -4.3279624][-4.2029572 -3.4658158 -3.163008 -3.4616213 -3.8313413 -3.87522 -3.5437081 -2.8378012 -2.461834 -3.1268706 -4.2782378 -5.0874228 -5.2957125 -4.9529548 -4.3538113][-4.1543856 -3.2605147 -2.8086638 -3.1389563 -3.806056 -4.1970654 -4.190249 -3.8345718 -3.6053622 -4.0746183 -4.9532747 -5.47199 -5.4547162 -5.0131555 -4.3737063][-4.2132244 -3.2828283 -2.71497 -2.9749005 -3.7476511 -4.2913942 -4.4190788 -4.2367911 -4.1025319 -4.4535065 -5.1369071 -5.4764552 -5.3496342 -4.9170532 -4.3456407][-4.1655736 -3.3655808 -2.8226018 -2.9952986 -3.6682138 -4.1381545 -4.2416544 -4.1260743 -4.0423751 -4.3145432 -4.8490787 -5.1084766 -5.0019546 -4.6997104 -4.3121786][-4.1558037 -3.6041832 -3.2163839 -3.3308449 -3.7886786 -4.0750866 -4.1245956 -4.0636768 -4.0197763 -4.1962113 -4.5502172 -4.746613 -4.7178259 -4.5755491 -4.3769479]]...]
INFO - root - 2017-12-06 08:22:34.605327: step 11410, loss = 1.13, batch loss = 1.06 (10.1 examples/sec; 0.790 sec/batch; 70h:26m:15s remains)
INFO - root - 2017-12-06 08:22:42.500456: step 11420, loss = 0.83, batch loss = 0.76 (10.1 examples/sec; 0.789 sec/batch; 70h:24m:37s remains)
INFO - root - 2017-12-06 08:22:50.274664: step 11430, loss = 1.06, batch loss = 0.99 (9.8 examples/sec; 0.813 sec/batch; 72h:32m:26s remains)
INFO - root - 2017-12-06 08:22:58.043160: step 11440, loss = 0.85, batch loss = 0.78 (11.3 examples/sec; 0.707 sec/batch; 63h:05m:14s remains)
INFO - root - 2017-12-06 08:23:05.942392: step 11450, loss = 0.70, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 69h:57m:44s remains)
INFO - root - 2017-12-06 08:23:13.894994: step 11460, loss = 0.76, batch loss = 0.69 (9.9 examples/sec; 0.809 sec/batch; 72h:06m:27s remains)
INFO - root - 2017-12-06 08:23:21.783238: step 11470, loss = 0.71, batch loss = 0.64 (10.0 examples/sec; 0.801 sec/batch; 71h:25m:14s remains)
INFO - root - 2017-12-06 08:23:29.594248: step 11480, loss = 0.72, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 69h:28m:53s remains)
INFO - root - 2017-12-06 08:23:37.541869: step 11490, loss = 0.98, batch loss = 0.91 (10.0 examples/sec; 0.801 sec/batch; 71h:23m:50s remains)
INFO - root - 2017-12-06 08:23:45.389280: step 11500, loss = 1.07, batch loss = 1.00 (10.5 examples/sec; 0.761 sec/batch; 67h:51m:18s remains)
2017-12-06 08:23:46.031385: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0668812 -4.1977158 -4.3370972 -4.5022416 -4.614512 -4.5765781 -4.4306183 -4.3040767 -4.2763281 -4.3061204 -4.2920122 -4.230967 -4.1286006 -4.0773649 -4.0453629][-4.2414804 -4.4077272 -4.5597715 -4.7144923 -4.7318225 -4.5318112 -4.2244635 -3.9965997 -3.9874351 -4.0909128 -4.142067 -4.1347985 -3.9901533 -3.8665912 -3.7751956][-4.4371881 -4.6445923 -4.8031273 -4.9052649 -4.7671056 -4.3849998 -3.929152 -3.5913742 -3.6175575 -3.8647661 -4.0621433 -4.1610537 -3.9937987 -3.82412 -3.6881402][-4.6057148 -4.8582053 -4.9981022 -4.9680157 -4.5952091 -3.9852784 -3.3888602 -2.9930649 -3.1496229 -3.6478517 -4.0873041 -4.3416018 -4.1729422 -3.9594398 -3.7523732][-4.6457806 -4.891469 -4.9385562 -4.7109518 -4.0792627 -3.2165432 -2.4414868 -2.0484231 -2.4792018 -3.3362134 -4.0741076 -4.4902544 -4.335268 -4.065207 -3.762876][-4.4903083 -4.6310282 -4.5012927 -4.0639887 -3.2178459 -2.0946481 -1.0581949 -0.67508268 -1.4969256 -2.8271141 -3.9080405 -4.4709892 -4.3395023 -4.0056143 -3.6300383][-4.2178817 -4.2007861 -3.8949766 -3.33384 -2.3798556 -1.0376184 0.28526545 0.65746069 -0.55445814 -2.3419244 -3.7229686 -4.3648 -4.2517385 -3.8817976 -3.5030541][-4.0054235 -3.88953 -3.5275254 -2.9761326 -2.0649898 -0.68181658 0.71815825 0.98580551 -0.39419222 -2.3642728 -3.8154101 -4.3698392 -4.2153635 -3.8457549 -3.5431414][-4.0253038 -3.9374545 -3.6874833 -3.3043678 -2.60842 -1.4319124 -0.24884415 -0.1330843 -1.3430789 -3.0735898 -4.2770958 -4.5892053 -4.3537269 -4.0238333 -3.8242266][-4.2525082 -4.269577 -4.210937 -4.0467281 -3.6237633 -2.7524338 -1.8559911 -1.8137555 -2.69621 -3.9936218 -4.8094478 -4.8643937 -4.5876193 -4.3313408 -4.2104549][-4.4587622 -4.571784 -4.6625 -4.6401649 -4.4183702 -3.8101475 -3.1602583 -3.1525412 -3.7568386 -4.6358781 -5.0804796 -4.9720273 -4.7336979 -4.5746579 -4.496438][-4.471714 -4.6267633 -4.7836237 -4.8253603 -4.7384467 -4.3682914 -3.9637587 -3.9935446 -4.3938951 -4.92447 -5.0958648 -4.9386158 -4.7848587 -4.6926808 -4.6122952][-4.3235803 -4.4751997 -4.6325412 -4.6905909 -4.6769476 -4.4959793 -4.3075466 -4.370502 -4.6125836 -4.8781362 -4.908164 -4.7905221 -4.7217703 -4.6664691 -4.577014][-4.1735797 -4.3037148 -4.4340248 -4.4913321 -4.5061927 -4.43497 -4.370728 -4.4257946 -4.5394144 -4.6343365 -4.6240735 -4.5782862 -4.571353 -4.5473561 -4.4691725][-4.1204181 -4.2230306 -4.3174024 -4.367805 -4.390614 -4.370091 -4.3517914 -4.3708854 -4.3967624 -4.4086022 -4.3994522 -4.3960581 -4.4110203 -4.4019828 -4.3490343]]...]
INFO - root - 2017-12-06 08:23:53.846262: step 11510, loss = 0.92, batch loss = 0.85 (10.1 examples/sec; 0.791 sec/batch; 70h:31m:47s remains)
INFO - root - 2017-12-06 08:24:01.722166: step 11520, loss = 0.84, batch loss = 0.77 (10.8 examples/sec; 0.741 sec/batch; 66h:06m:21s remains)
INFO - root - 2017-12-06 08:24:09.700128: step 11530, loss = 0.85, batch loss = 0.78 (9.9 examples/sec; 0.809 sec/batch; 72h:06m:02s remains)
INFO - root - 2017-12-06 08:24:17.697310: step 11540, loss = 0.98, batch loss = 0.91 (10.0 examples/sec; 0.796 sec/batch; 70h:59m:30s remains)
INFO - root - 2017-12-06 08:24:25.444968: step 11550, loss = 0.91, batch loss = 0.84 (10.3 examples/sec; 0.780 sec/batch; 69h:30m:14s remains)
INFO - root - 2017-12-06 08:24:33.400413: step 11560, loss = 1.05, batch loss = 0.98 (10.0 examples/sec; 0.798 sec/batch; 71h:08m:42s remains)
INFO - root - 2017-12-06 08:24:41.335728: step 11570, loss = 0.75, batch loss = 0.68 (10.5 examples/sec; 0.760 sec/batch; 67h:42m:34s remains)
INFO - root - 2017-12-06 08:24:49.128805: step 11580, loss = 1.18, batch loss = 1.11 (9.9 examples/sec; 0.807 sec/batch; 71h:54m:42s remains)
INFO - root - 2017-12-06 08:24:56.991729: step 11590, loss = 0.84, batch loss = 0.77 (10.6 examples/sec; 0.754 sec/batch; 67h:12m:02s remains)
INFO - root - 2017-12-06 08:25:04.918732: step 11600, loss = 0.87, batch loss = 0.80 (9.8 examples/sec; 0.813 sec/batch; 72h:28m:04s remains)
2017-12-06 08:25:05.674581: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5246596 -4.5671687 -4.6156111 -4.6566572 -4.6948967 -4.7295127 -4.761354 -4.7943015 -4.8267741 -4.8549018 -4.8721051 -4.8759575 -4.8685932 -4.8508811 -4.8226418][-4.6493363 -4.7044621 -4.7571974 -4.79228 -4.8116527 -4.8179607 -4.8227453 -4.8418822 -4.8781438 -4.9266782 -4.978034 -5.0273948 -5.0710211 -5.1007729 -5.110198][-4.789125 -4.8572 -4.9022708 -4.906671 -4.8711805 -4.808887 -4.7525244 -4.7360125 -4.7668986 -4.8357258 -4.9267378 -5.0279455 -5.1265979 -5.2127848 -5.2819462][-5.0040951 -5.0519266 -5.043715 -4.9620814 -4.8144021 -4.63306 -4.4803452 -4.4090838 -4.4298935 -4.523788 -4.6617751 -4.8140845 -4.9523811 -5.081789 -5.2186027][-5.3019767 -5.297657 -5.1925416 -4.9719229 -4.6584539 -4.3143644 -4.0372539 -3.893858 -3.8939781 -4.0046515 -4.1747847 -4.3421645 -4.4632859 -4.596694 -4.805337][-5.5152817 -5.4620085 -5.2593451 -4.8946686 -4.4108973 -3.9065442 -3.5149317 -3.3116517 -3.301806 -3.4246068 -3.5939536 -3.705255 -3.7273593 -3.8203354 -4.1082129][-5.5238242 -5.4466763 -5.1794119 -4.7049723 -4.084991 -3.4565661 -2.9878135 -2.761162 -2.7672777 -2.8977604 -3.0141106 -2.9810729 -2.8302405 -2.8648574 -3.2468925][-5.2778158 -5.2475147 -4.9970503 -4.4883966 -3.8024535 -3.1155896 -2.6256661 -2.4136093 -2.4505033 -2.572268 -2.5823503 -2.3392508 -1.9820981 -1.9606247 -2.4336982][-4.74428 -4.85804 -4.7365532 -4.3086071 -3.6713965 -3.0245116 -2.5718825 -2.3882658 -2.4424872 -2.5385807 -2.4292893 -1.9860232 -1.4663856 -1.4159031 -1.945322][-4.0020037 -4.3257327 -4.411736 -4.1567698 -3.660821 -3.1203303 -2.7230587 -2.5425763 -2.5784833 -2.6404133 -2.453124 -1.9040046 -1.3308289 -1.2829893 -1.8056605][-3.2859416 -3.827919 -4.1269774 -4.0595851 -3.7242067 -3.3000681 -2.9392412 -2.7321725 -2.7294796 -2.7740595 -2.5960314 -2.0959842 -1.612973 -1.6051712 -2.0643818][-2.823796 -3.5023196 -3.9442019 -4.0193262 -3.8222356 -3.4998784 -3.1551807 -2.9078312 -2.8668442 -2.9235237 -2.8398018 -2.5273697 -2.2484841 -2.312372 -2.6795][-2.7546163 -3.4316602 -3.9054043 -4.0531282 -3.9561868 -3.7163126 -3.3945808 -3.1302447 -3.0781026 -3.1686807 -3.2025332 -3.1010389 -3.0272076 -3.1459265 -3.4026399][-3.1102612 -3.6476238 -4.0316281 -4.1600409 -4.10588 -3.9302273 -3.6631365 -3.4360816 -3.413336 -3.5453234 -3.66737 -3.7162061 -3.7776222 -3.9139185 -4.0687342][-3.706223 -4.0517254 -4.2770739 -4.3164496 -4.2481418 -4.1155987 -3.9257336 -3.7785873 -3.8067074 -3.9643631 -4.1231928 -4.24387 -4.3667974 -4.5002766 -4.5956707]]...]
INFO - root - 2017-12-06 08:25:13.414723: step 11610, loss = 0.78, batch loss = 0.71 (10.7 examples/sec; 0.749 sec/batch; 66h:43m:51s remains)
INFO - root - 2017-12-06 08:25:21.318656: step 11620, loss = 0.85, batch loss = 0.78 (10.0 examples/sec; 0.803 sec/batch; 71h:33m:12s remains)
INFO - root - 2017-12-06 08:25:29.573499: step 11630, loss = 0.76, batch loss = 0.69 (10.6 examples/sec; 0.754 sec/batch; 67h:14m:18s remains)
INFO - root - 2017-12-06 08:25:37.463929: step 11640, loss = 0.86, batch loss = 0.79 (10.2 examples/sec; 0.784 sec/batch; 69h:53m:02s remains)
INFO - root - 2017-12-06 08:25:45.189069: step 11650, loss = 0.92, batch loss = 0.85 (10.2 examples/sec; 0.783 sec/batch; 69h:45m:11s remains)
INFO - root - 2017-12-06 08:25:53.006695: step 11660, loss = 0.83, batch loss = 0.76 (10.3 examples/sec; 0.779 sec/batch; 69h:23m:42s remains)
INFO - root - 2017-12-06 08:26:00.872822: step 11670, loss = 1.04, batch loss = 0.97 (10.0 examples/sec; 0.799 sec/batch; 71h:10m:40s remains)
INFO - root - 2017-12-06 08:26:08.647528: step 11680, loss = 0.93, batch loss = 0.86 (9.9 examples/sec; 0.806 sec/batch; 71h:47m:20s remains)
INFO - root - 2017-12-06 08:26:16.666097: step 11690, loss = 1.24, batch loss = 1.17 (10.1 examples/sec; 0.793 sec/batch; 70h:40m:52s remains)
INFO - root - 2017-12-06 08:26:24.473102: step 11700, loss = 0.73, batch loss = 0.66 (10.3 examples/sec; 0.773 sec/batch; 68h:55m:29s remains)
2017-12-06 08:26:25.156094: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5453434 -2.1262598 -2.4687314 -3.37355 -3.9997668 -4.3774376 -4.5972219 -4.3549418 -4.0478568 -4.0863867 -4.3567433 -4.4457903 -4.1887636 -4.2237687 -4.6408815][-2.3312464 -1.9376452 -2.280508 -3.2238123 -3.87962 -4.2626381 -4.4284658 -4.0768852 -3.7939711 -3.9799173 -4.4279671 -4.7293625 -4.6492605 -4.7894626 -5.1293988][-2.1993809 -1.8047714 -2.0531516 -2.8452034 -3.4034095 -3.7342787 -3.8628979 -3.4722497 -3.2551532 -3.6242995 -4.3027587 -4.8720169 -5.0311117 -5.2997208 -5.5498414][-2.0985148 -1.6974919 -1.758153 -2.2552748 -2.6230786 -2.8760266 -2.9817586 -2.6070042 -2.4670467 -3.0089231 -3.9191129 -4.7401094 -5.1470308 -5.5789127 -5.7985][-2.0483379 -1.6409934 -1.4771349 -1.6413333 -1.7852185 -1.8654919 -1.8227615 -1.4363408 -1.3888328 -2.0976923 -3.1718423 -4.1684842 -4.8053365 -5.4072876 -5.6443977][-2.0282586 -1.5963933 -1.2199416 -1.077589 -0.99480438 -0.83009386 -0.51505303 -0.079155445 -0.18311024 -1.0720885 -2.218508 -3.2965117 -4.1161861 -4.819283 -5.0279813][-2.2623396 -1.806509 -1.3086193 -0.92688012 -0.58801174 -0.1034627 0.58227444 1.1147542 0.83910036 -0.25344849 -1.4484138 -2.5875747 -3.5689743 -4.2447977 -4.2847638][-2.8127003 -2.3510594 -1.8710556 -1.3827024 -0.83332324 -0.078389645 0.89018011 1.4531488 1.046699 -0.15841627 -1.3237414 -2.4878564 -3.5766606 -4.0832596 -3.8400302][-3.6175742 -3.1882405 -2.826138 -2.3718369 -1.7612329 -0.93681812 0.064801216 0.570415 0.19932461 -0.886281 -1.8765652 -2.9453676 -4.0387821 -4.3522716 -3.9062045][-4.5280547 -4.1451888 -3.9064889 -3.5608366 -2.9920218 -2.2120373 -1.3120944 -0.85362625 -1.0424473 -1.8312316 -2.580832 -3.482326 -4.5084581 -4.71881 -4.29909][-5.3394785 -4.9896917 -4.7587442 -4.4616289 -3.9412463 -3.2231929 -2.4411411 -2.0149589 -2.0159433 -2.5085204 -3.0492709 -3.7867069 -4.7229795 -4.9430809 -4.7071161][-6.0280595 -5.6922035 -5.3760576 -5.0711923 -4.5969038 -3.8974454 -3.1539745 -2.7114792 -2.5529037 -2.7775054 -3.1296177 -3.7235537 -4.5884686 -4.9007506 -4.9136376][-6.4154882 -6.0804572 -5.6582456 -5.3616476 -5.0083628 -4.3666096 -3.6434267 -3.2180192 -3.0054278 -3.0198112 -3.1791921 -3.6394508 -4.4666877 -4.9035273 -5.1285157][-6.5308847 -6.2104931 -5.7296119 -5.4705162 -5.2695274 -4.7359056 -4.0683465 -3.7123697 -3.5338109 -3.405673 -3.3921876 -3.7527905 -4.5522318 -5.1108236 -5.5044336][-6.3716927 -6.0475225 -5.5700469 -5.375473 -5.3056469 -4.8900814 -4.3127041 -4.03233 -3.8796256 -3.628397 -3.4905152 -3.7870462 -4.5370975 -5.1994629 -5.7678957]]...]
INFO - root - 2017-12-06 08:26:33.110579: step 11710, loss = 0.96, batch loss = 0.89 (10.2 examples/sec; 0.781 sec/batch; 69h:33m:51s remains)
INFO - root - 2017-12-06 08:26:40.953746: step 11720, loss = 1.13, batch loss = 1.06 (10.2 examples/sec; 0.783 sec/batch; 69h:47m:36s remains)
INFO - root - 2017-12-06 08:26:48.740683: step 11730, loss = 1.00, batch loss = 0.93 (10.4 examples/sec; 0.770 sec/batch; 68h:34m:04s remains)
INFO - root - 2017-12-06 08:26:56.585704: step 11740, loss = 0.59, batch loss = 0.52 (10.1 examples/sec; 0.788 sec/batch; 70h:14m:00s remains)
INFO - root - 2017-12-06 08:27:04.358944: step 11750, loss = 0.88, batch loss = 0.81 (10.5 examples/sec; 0.761 sec/batch; 67h:46m:47s remains)
INFO - root - 2017-12-06 08:27:12.091446: step 11760, loss = 0.73, batch loss = 0.66 (10.1 examples/sec; 0.791 sec/batch; 70h:27m:28s remains)
INFO - root - 2017-12-06 08:27:20.116105: step 11770, loss = 1.08, batch loss = 1.01 (9.6 examples/sec; 0.829 sec/batch; 73h:51m:55s remains)
INFO - root - 2017-12-06 08:27:27.877114: step 11780, loss = 1.05, batch loss = 0.98 (10.3 examples/sec; 0.778 sec/batch; 69h:19m:41s remains)
INFO - root - 2017-12-06 08:27:35.700239: step 11790, loss = 0.91, batch loss = 0.84 (9.9 examples/sec; 0.806 sec/batch; 71h:49m:04s remains)
INFO - root - 2017-12-06 08:27:43.629654: step 11800, loss = 1.15, batch loss = 1.08 (9.9 examples/sec; 0.807 sec/batch; 71h:51m:49s remains)
2017-12-06 08:27:44.242871: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1275382 -5.5010786 -4.70686 -4.7813964 -5.5050144 -5.5051594 -5.1716585 -4.96609 -5.0436258 -5.1950345 -5.4141159 -5.86201 -6.3687162 -6.3006649 -5.7416854][-5.924787 -5.4246416 -4.7715154 -4.9103107 -5.6590357 -5.7066464 -5.426722 -5.2890544 -5.4581928 -5.7120905 -5.8885241 -6.2326722 -6.7230482 -6.614212 -5.9629679][-5.4233413 -4.9538188 -4.3750734 -4.4933443 -5.1428361 -5.2063551 -5.0505767 -5.0832362 -5.3690128 -5.7843161 -6.0157838 -6.2973781 -6.7715578 -6.6877613 -6.0646739][-5.0224314 -4.5240641 -3.9274452 -3.8326325 -4.152185 -4.0889516 -4.0078349 -4.2251892 -4.639246 -5.3182349 -5.7760615 -6.0795555 -6.5557632 -6.5483046 -6.0634136][-4.4516559 -3.9613473 -3.4178948 -3.165247 -3.2137566 -2.9992089 -2.8682885 -3.1062145 -3.5102808 -4.375442 -5.0544748 -5.3962812 -5.9162307 -6.0654106 -5.8296561][-3.5250669 -3.08013 -2.6760607 -2.4801826 -2.5457892 -2.3302624 -2.1265428 -2.2489989 -2.4957352 -3.3225088 -4.0263195 -4.3129683 -4.8473673 -5.1761451 -5.2592373][-2.5205631 -2.137558 -1.8902893 -1.8859916 -2.2045276 -2.1234207 -1.8686087 -1.8406873 -1.9035079 -2.5399683 -3.1173298 -3.2885885 -3.7817407 -4.2338309 -4.5570498][-1.8316541 -1.456131 -1.3057117 -1.5479822 -2.2246051 -2.3133252 -1.9864793 -1.8245647 -1.7569582 -2.1976547 -2.6562514 -2.7749984 -3.23065 -3.7055445 -4.100265][-2.0325735 -1.5392482 -1.3375521 -1.7356207 -2.6862276 -2.9306774 -2.587086 -2.3347459 -2.1892512 -2.48342 -2.8590584 -2.9745879 -3.3991141 -3.8032098 -4.101439][-2.7327228 -2.11138 -1.7979801 -2.2287278 -3.2825041 -3.6486554 -3.4230866 -3.2128334 -3.0991914 -3.3089781 -3.5999374 -3.698853 -4.0468888 -4.3018761 -4.4083796][-3.5671668 -2.9661789 -2.6112988 -2.9707186 -3.9029775 -4.2691226 -4.1911125 -4.1123834 -4.1304502 -4.3422766 -4.5935678 -4.6859736 -4.8956423 -4.929121 -4.7947922][-4.5481858 -4.0631518 -3.7303069 -3.9435618 -4.6128054 -4.89381 -4.9214272 -4.9585032 -5.0904808 -5.3222642 -5.5300355 -5.5722704 -5.5899124 -5.3952255 -5.0449085][-5.0538816 -4.7259 -4.4605632 -4.5425663 -4.9416342 -5.1208992 -5.1908717 -5.285017 -5.4601555 -5.68326 -5.8431382 -5.8405647 -5.7202606 -5.4012146 -4.97496][-4.8411217 -4.6703224 -4.5096545 -4.5344872 -4.7368536 -4.8251758 -4.8792782 -4.9634762 -5.108851 -5.2877631 -5.4199123 -5.4205103 -5.2865534 -5.0023737 -4.6669579][-4.514957 -4.4299922 -4.3476229 -4.3487859 -4.427217 -4.450047 -4.466702 -4.5102119 -4.599524 -4.7174139 -4.8186865 -4.8408566 -4.756763 -4.5731111 -4.3767371]]...]
INFO - root - 2017-12-06 08:27:52.167040: step 11810, loss = 0.79, batch loss = 0.72 (10.3 examples/sec; 0.779 sec/batch; 69h:21m:47s remains)
INFO - root - 2017-12-06 08:28:00.007482: step 11820, loss = 0.87, batch loss = 0.80 (10.3 examples/sec; 0.778 sec/batch; 69h:18m:10s remains)
INFO - root - 2017-12-06 08:28:07.751042: step 11830, loss = 0.87, batch loss = 0.80 (10.6 examples/sec; 0.756 sec/batch; 67h:22m:06s remains)
INFO - root - 2017-12-06 08:28:15.735895: step 11840, loss = 0.90, batch loss = 0.83 (10.0 examples/sec; 0.803 sec/batch; 71h:29m:36s remains)
INFO - root - 2017-12-06 08:28:23.685234: step 11850, loss = 0.89, batch loss = 0.82 (9.9 examples/sec; 0.806 sec/batch; 71h:45m:25s remains)
INFO - root - 2017-12-06 08:28:31.567173: step 11860, loss = 0.73, batch loss = 0.66 (10.1 examples/sec; 0.793 sec/batch; 70h:40m:05s remains)
INFO - root - 2017-12-06 08:28:39.294642: step 11870, loss = 0.80, batch loss = 0.73 (10.3 examples/sec; 0.774 sec/batch; 68h:54m:25s remains)
INFO - root - 2017-12-06 08:28:47.189587: step 11880, loss = 0.77, batch loss = 0.70 (10.4 examples/sec; 0.767 sec/batch; 68h:21m:09s remains)
INFO - root - 2017-12-06 08:28:54.997883: step 11890, loss = 0.99, batch loss = 0.92 (10.3 examples/sec; 0.776 sec/batch; 69h:06m:32s remains)
INFO - root - 2017-12-06 08:29:02.219962: step 11900, loss = 0.98, batch loss = 0.91 (13.2 examples/sec; 0.604 sec/batch; 53h:46m:22s remains)
2017-12-06 08:29:02.760152: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4279737 -3.3107162 -3.2459555 -3.3564277 -3.5934243 -3.7276487 -3.6286497 -3.4199772 -3.3371062 -3.4332147 -3.6870358 -4.0433936 -4.3829713 -4.6680441 -4.8098865][-4.0336614 -4.0041804 -4.0032721 -4.1021352 -4.2677989 -4.3365073 -4.175396 -3.8964188 -3.7892413 -3.9137995 -4.1826024 -4.5152416 -4.8117828 -5.0425673 -5.12268][-4.7776403 -4.8575873 -4.908052 -4.9745727 -5.0482969 -5.0400376 -4.82356 -4.4866223 -4.3504419 -4.5175467 -4.8479166 -5.1679287 -5.3597169 -5.4665413 -5.4340014][-5.1890321 -5.3538547 -5.4001765 -5.3536263 -5.2413445 -5.0774937 -4.7585087 -4.3455729 -4.220912 -4.550952 -5.0964003 -5.5354214 -5.6937723 -5.6997995 -5.573287][-5.0755968 -5.2805533 -5.2781487 -5.0292025 -4.6081619 -4.1720772 -3.6638408 -3.1295509 -3.043189 -3.65461 -4.6005778 -5.3636689 -5.6786947 -5.7162437 -5.5642662][-4.9709406 -5.202487 -5.1506863 -4.6778 -3.8744037 -3.038197 -2.2031794 -1.4236188 -1.2578707 -2.0913064 -3.4788485 -4.6834083 -5.3287911 -5.554882 -5.4660411][-5.1225328 -5.35016 -5.258728 -4.6177096 -3.490963 -2.2818928 -1.1337104 -0.11802959 0.19300747 -0.73591948 -2.4162936 -3.9454861 -4.8971114 -5.3290434 -5.3022642][-5.3021007 -5.5063047 -5.4563909 -4.8524647 -3.6728759 -2.33335 -1.0777414 0.0379858 0.50158167 -0.35411692 -2.0491104 -3.603086 -4.6378536 -5.1274686 -5.08827][-5.268342 -5.4291267 -5.4915309 -5.1249204 -4.1736455 -2.96942 -1.8372214 -0.82006359 -0.2806778 -0.9138639 -2.3326216 -3.5969162 -4.4338932 -4.8163581 -4.7530708][-4.9670668 -5.0990872 -5.2897186 -5.2105179 -4.6073194 -3.7024257 -2.8524494 -2.0834153 -1.5488536 -1.9204869 -2.9334574 -3.7754302 -4.271965 -4.4709721 -4.4105234][-4.5553279 -4.7136412 -4.9573503 -5.0222259 -4.68038 -4.102047 -3.605746 -3.1649075 -2.7525887 -2.9606383 -3.6374989 -4.12577 -4.3203082 -4.3406739 -4.2649889][-4.1509724 -4.3239961 -4.5130367 -4.5587816 -4.3494806 -4.0595245 -3.9357812 -3.8392148 -3.6340036 -3.8152843 -4.2945147 -4.5637627 -4.5622687 -4.4363856 -4.2834873][-3.8523722 -4.0324473 -4.1550636 -4.1592655 -4.0310535 -3.9462354 -4.0927315 -4.2613492 -4.2402334 -4.4086223 -4.7414937 -4.8821979 -4.7869191 -4.574748 -4.34628][-3.7713494 -3.9271936 -4.0097008 -4.0305209 -3.9879901 -4.00335 -4.2323551 -4.5068765 -4.5937209 -4.7236681 -4.9213943 -4.9659381 -4.8251472 -4.5845051 -4.3446445][-3.8260021 -3.9275398 -3.9926846 -4.0577602 -4.1112475 -4.1810842 -4.3785486 -4.6206894 -4.7377329 -4.8314967 -4.9166622 -4.8750587 -4.7042851 -4.4850664 -4.2942071]]...]
INFO - root - 2017-12-06 08:29:08.923026: step 11910, loss = 0.88, batch loss = 0.81 (12.8 examples/sec; 0.624 sec/batch; 55h:36m:48s remains)
INFO - root - 2017-12-06 08:29:15.030297: step 11920, loss = 1.05, batch loss = 0.98 (13.0 examples/sec; 0.617 sec/batch; 54h:54m:39s remains)
INFO - root - 2017-12-06 08:29:21.252163: step 11930, loss = 1.12, batch loss = 1.05 (13.1 examples/sec; 0.610 sec/batch; 54h:20m:42s remains)
INFO - root - 2017-12-06 08:29:27.355600: step 11940, loss = 0.83, batch loss = 0.76 (12.7 examples/sec; 0.629 sec/batch; 56h:02m:30s remains)
INFO - root - 2017-12-06 08:29:33.417040: step 11950, loss = 0.87, batch loss = 0.80 (12.9 examples/sec; 0.621 sec/batch; 55h:16m:07s remains)
INFO - root - 2017-12-06 08:29:39.508407: step 11960, loss = 1.19, batch loss = 1.12 (13.5 examples/sec; 0.593 sec/batch; 52h:48m:33s remains)
INFO - root - 2017-12-06 08:29:45.504608: step 11970, loss = 0.92, batch loss = 0.85 (14.7 examples/sec; 0.543 sec/batch; 48h:18m:36s remains)
INFO - root - 2017-12-06 08:29:51.360204: step 11980, loss = 0.96, batch loss = 0.89 (13.6 examples/sec; 0.587 sec/batch; 52h:14m:56s remains)
INFO - root - 2017-12-06 08:29:57.481942: step 11990, loss = 1.09, batch loss = 1.02 (13.0 examples/sec; 0.616 sec/batch; 54h:49m:11s remains)
INFO - root - 2017-12-06 08:30:03.679688: step 12000, loss = 1.07, batch loss = 1.00 (13.4 examples/sec; 0.598 sec/batch; 53h:11m:40s remains)
2017-12-06 08:30:04.245104: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2211719 -4.2824531 -4.4879012 -4.6984067 -4.9617534 -5.187582 -5.2224131 -4.8841004 -4.557899 -4.545495 -4.7416768 -4.895607 -5.03304 -5.0640526 -4.9568334][-4.0670376 -4.1484661 -4.4265695 -4.7212539 -5.0647335 -5.3018947 -5.2937088 -4.8511271 -4.4376426 -4.4178791 -4.6635818 -4.8604927 -5.0347891 -5.0745463 -4.9608893][-3.7037396 -3.8395925 -4.1947985 -4.6001363 -5.0207686 -5.2603688 -5.207181 -4.6821747 -4.1875429 -4.0964336 -4.3136368 -4.5411992 -4.7713485 -4.8714371 -4.8130927][-3.162569 -3.3142905 -3.6839721 -4.178103 -4.67631 -4.9692974 -4.9278641 -4.396503 -3.8385394 -3.5864096 -3.6577141 -3.8854628 -4.198802 -4.4224806 -4.4954262][-2.5912361 -2.6538811 -2.9280281 -3.4331264 -3.986347 -4.3714662 -4.40066 -3.9247513 -3.3131163 -2.8468378 -2.7279053 -2.9361134 -3.3310876 -3.7130146 -3.9673345][-2.2518437 -2.2295296 -2.4063442 -2.8584452 -3.3821545 -3.777575 -3.7950759 -3.3383598 -2.6892843 -2.0651727 -1.8250325 -2.0343761 -2.5189962 -3.0470252 -3.4637394][-2.3141844 -2.3178682 -2.4697189 -2.8095832 -3.1812649 -3.4484963 -3.3580585 -2.8951044 -2.2690818 -1.6073942 -1.3346291 -1.5478435 -2.0536408 -2.6455569 -3.1660385][-2.7345634 -2.8269768 -2.9640791 -3.1159019 -3.2526677 -3.3534145 -3.1936421 -2.7694135 -2.2189507 -1.6093884 -1.3510234 -1.52208 -1.9203763 -2.4560328 -3.0334911][-3.2528677 -3.3858075 -3.4469669 -3.3856964 -3.3471045 -3.3728762 -3.2245955 -2.8583341 -2.3729296 -1.8370605 -1.5948772 -1.7031145 -1.9546561 -2.3800819 -2.9535418][-3.7165575 -3.8493967 -3.7816885 -3.5171685 -3.3657703 -3.3637929 -3.2572584 -2.9593177 -2.5735316 -2.1637151 -1.9401312 -1.9704192 -2.0830212 -2.365195 -2.8277407][-4.2336912 -4.3504672 -4.1287231 -3.6551213 -3.3822713 -3.3269691 -3.2415133 -3.0445905 -2.8170247 -2.5761476 -2.3818235 -2.3204372 -2.3047087 -2.4264364 -2.7260771][-4.705915 -4.79175 -4.4420176 -3.7980142 -3.409647 -3.3136387 -3.2592707 -3.1929383 -3.152914 -3.0892477 -2.9579864 -2.8385539 -2.7057014 -2.6527443 -2.7912183][-4.8603821 -4.9373584 -4.5694923 -3.9061916 -3.5041912 -3.4191308 -3.3763433 -3.3695474 -3.45662 -3.5582843 -3.5496895 -3.4745667 -3.2885504 -3.0870085 -3.0829291][-4.6476679 -4.702425 -4.3796234 -3.8436069 -3.5675786 -3.5717814 -3.5363533 -3.5083041 -3.6727571 -3.9578836 -4.1461124 -4.19187 -3.9813061 -3.6137831 -3.427386][-4.3739543 -4.396112 -4.1186037 -3.7356272 -3.6151204 -3.721174 -3.7447124 -3.7337995 -3.9798851 -4.420691 -4.76139 -4.8667555 -4.5749125 -4.041347 -3.6831684]]...]
INFO - root - 2017-12-06 08:30:10.222246: step 12010, loss = 0.82, batch loss = 0.75 (13.6 examples/sec; 0.590 sec/batch; 52h:30m:37s remains)
INFO - root - 2017-12-06 08:30:16.368306: step 12020, loss = 0.76, batch loss = 0.69 (13.0 examples/sec; 0.616 sec/batch; 54h:50m:17s remains)
INFO - root - 2017-12-06 08:30:22.485847: step 12030, loss = 0.68, batch loss = 0.61 (12.9 examples/sec; 0.619 sec/batch; 55h:06m:43s remains)
INFO - root - 2017-12-06 08:30:28.618585: step 12040, loss = 0.91, batch loss = 0.84 (13.3 examples/sec; 0.602 sec/batch; 53h:37m:19s remains)
INFO - root - 2017-12-06 08:30:34.725971: step 12050, loss = 1.13, batch loss = 1.06 (13.2 examples/sec; 0.608 sec/batch; 54h:05m:53s remains)
INFO - root - 2017-12-06 08:30:40.874985: step 12060, loss = 1.01, batch loss = 0.94 (12.9 examples/sec; 0.620 sec/batch; 55h:10m:18s remains)
INFO - root - 2017-12-06 08:30:46.992049: step 12070, loss = 0.90, batch loss = 0.83 (12.9 examples/sec; 0.620 sec/batch; 55h:12m:06s remains)
INFO - root - 2017-12-06 08:30:52.955195: step 12080, loss = 0.94, batch loss = 0.87 (13.2 examples/sec; 0.605 sec/batch; 53h:50m:08s remains)
INFO - root - 2017-12-06 08:30:59.090713: step 12090, loss = 1.00, batch loss = 0.93 (12.8 examples/sec; 0.627 sec/batch; 55h:47m:27s remains)
INFO - root - 2017-12-06 08:31:05.201324: step 12100, loss = 0.95, batch loss = 0.88 (13.3 examples/sec; 0.603 sec/batch; 53h:39m:53s remains)
2017-12-06 08:31:05.743626: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3799143 -4.4825883 -4.417942 -4.1858435 -3.934617 -3.8610868 -3.9321003 -4.2803516 -4.5073862 -4.5347013 -4.5871172 -4.5278273 -4.30357 -4.143362 -4.1456666][-3.9473593 -4.1088762 -4.0816832 -3.8421466 -3.638747 -3.6469805 -3.7801192 -4.2309442 -4.5977874 -4.7119761 -4.7355309 -4.5128679 -4.0583839 -3.7147079 -3.6487942][-3.6453714 -3.8692641 -3.90137 -3.6549404 -3.4412887 -3.4244165 -3.5181828 -3.9835532 -4.498529 -4.8181152 -4.9345016 -4.6460018 -4.0134587 -3.4743729 -3.2653751][-3.664598 -3.9259164 -3.9654887 -3.6439767 -3.3002269 -3.1065269 -3.0305171 -3.3739429 -4.0017405 -4.6172628 -4.9768782 -4.7864528 -4.1445436 -3.5439978 -3.2268653][-4.054842 -4.28701 -4.2419744 -3.7694685 -3.1984913 -2.7077482 -2.3164628 -2.3846796 -3.056756 -4.0227551 -4.7638588 -4.8453026 -4.3784175 -3.8824525 -3.5801859][-4.6223483 -4.7924442 -4.6239657 -3.9793284 -3.1493487 -2.3537526 -1.597507 -1.2689307 -1.8296294 -3.0536973 -4.2060542 -4.7128758 -4.6151919 -4.3774729 -4.1800122][-4.9715128 -5.1775351 -5.0230074 -4.3449106 -3.3613787 -2.3729398 -1.3426406 -0.59639359 -0.83591771 -2.0209093 -3.3698759 -4.274 -4.6409721 -4.7683811 -4.7577553][-4.8840075 -5.23254 -5.2690616 -4.7573066 -3.8085079 -2.8256474 -1.7497425 -0.75719833 -0.59761739 -1.4183757 -2.5893495 -3.5967863 -4.2889013 -4.7420959 -4.9579577][-4.4832215 -4.9493237 -5.2460313 -5.0472431 -4.3190937 -3.5162783 -2.6048627 -1.6255422 -1.185102 -1.5258269 -2.2729657 -3.0635641 -3.7774742 -4.3453155 -4.7085838][-4.013175 -4.51567 -5.0280108 -5.166707 -4.7496552 -4.1826787 -3.4953456 -2.6872544 -2.1809113 -2.1806414 -2.5160131 -2.9417174 -3.4419904 -3.8909192 -4.215085][-3.6263525 -4.06656 -4.6569595 -5.0222034 -4.8823504 -4.5373144 -4.0788231 -3.5219376 -3.1154981 -3.0020211 -3.1260252 -3.2981355 -3.5637479 -3.7563856 -3.8458767][-3.4697361 -3.7494326 -4.2077327 -4.575491 -4.5644045 -4.3650894 -4.1039176 -3.8198659 -3.6044245 -3.5581315 -3.6819859 -3.8387241 -4.0425968 -4.0529089 -3.8686242][-3.5967546 -3.7233691 -3.9024625 -4.0309525 -3.9339938 -3.7561507 -3.6251976 -3.5570478 -3.5251169 -3.616241 -3.8713632 -4.1930075 -4.50755 -4.4970226 -4.189487][-3.8333282 -3.9089396 -3.8419547 -3.6502018 -3.33299 -3.0609245 -2.9703057 -3.0336006 -3.1144485 -3.2948768 -3.6706569 -4.1859231 -4.6626492 -4.74629 -4.5049877][-4.0228062 -4.1631927 -4.0035439 -3.6000507 -3.0812678 -2.6745892 -2.5457168 -2.6590457 -2.7919326 -2.9858162 -3.3605938 -3.9201212 -4.4533458 -4.62401 -4.5433707]]...]
INFO - root - 2017-12-06 08:31:11.831142: step 12110, loss = 1.16, batch loss = 1.09 (13.4 examples/sec; 0.595 sec/batch; 52h:58m:11s remains)
INFO - root - 2017-12-06 08:31:17.992420: step 12120, loss = 0.81, batch loss = 0.74 (13.4 examples/sec; 0.599 sec/batch; 53h:16m:18s remains)
INFO - root - 2017-12-06 08:31:23.978006: step 12130, loss = 0.86, batch loss = 0.79 (12.9 examples/sec; 0.621 sec/batch; 55h:16m:44s remains)
INFO - root - 2017-12-06 08:31:30.097034: step 12140, loss = 0.85, batch loss = 0.78 (12.7 examples/sec; 0.630 sec/batch; 56h:03m:32s remains)
INFO - root - 2017-12-06 08:31:36.232770: step 12150, loss = 1.03, batch loss = 0.96 (13.1 examples/sec; 0.613 sec/batch; 54h:30m:25s remains)
INFO - root - 2017-12-06 08:31:42.294804: step 12160, loss = 0.79, batch loss = 0.72 (13.4 examples/sec; 0.596 sec/batch; 53h:01m:39s remains)
INFO - root - 2017-12-06 08:31:48.439185: step 12170, loss = 0.74, batch loss = 0.67 (13.4 examples/sec; 0.597 sec/batch; 53h:06m:02s remains)
INFO - root - 2017-12-06 08:31:54.472562: step 12180, loss = 0.97, batch loss = 0.90 (13.6 examples/sec; 0.587 sec/batch; 52h:13m:17s remains)
INFO - root - 2017-12-06 08:32:00.481604: step 12190, loss = 0.65, batch loss = 0.58 (12.9 examples/sec; 0.622 sec/batch; 55h:19m:50s remains)
INFO - root - 2017-12-06 08:32:06.560424: step 12200, loss = 0.79, batch loss = 0.72 (12.3 examples/sec; 0.650 sec/batch; 57h:48m:48s remains)
2017-12-06 08:32:07.092776: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6019664 -3.5628228 -3.7984631 -4.2720981 -4.7006626 -4.89693 -5.0270028 -5.123023 -5.1286926 -5.0317421 -4.9317923 -4.8658729 -4.8291607 -4.8162842 -4.818656][-3.0901265 -3.2483561 -3.7270141 -4.4614949 -5.1103663 -5.3546972 -5.4392209 -5.4599218 -5.3847346 -5.2141633 -5.03362 -4.9028287 -4.8025451 -4.728477 -4.7107663][-2.7500465 -3.0637965 -3.677104 -4.5097914 -5.22476 -5.441174 -5.4502916 -5.4140787 -5.2859116 -5.0591283 -4.7990794 -4.6296873 -4.5364227 -4.4506617 -4.4462848][-2.7575941 -3.09566 -3.657444 -4.3640957 -4.9368324 -5.0389614 -4.9718885 -4.9104071 -4.7429142 -4.4780784 -4.1508708 -3.9747896 -3.9865606 -3.9811869 -4.0495853][-3.0418653 -3.2475176 -3.5749319 -3.974823 -4.2464647 -4.1606989 -4.0187159 -3.9502945 -3.7657943 -3.5300906 -3.2305703 -3.1183474 -3.3184218 -3.4664693 -3.6303365][-3.4137044 -3.3718808 -3.3581026 -3.39502 -3.3581586 -3.1145089 -2.9389248 -2.8906322 -2.7151408 -2.5766134 -2.410769 -2.4171937 -2.8018286 -3.0861695 -3.3090973][-3.6575019 -3.3576241 -3.0075333 -2.7585742 -2.5539141 -2.2935767 -2.1647105 -2.1829634 -2.0777216 -2.0800207 -2.0842788 -2.1741862 -2.5993142 -2.895395 -3.1010847][-3.8313487 -3.3569489 -2.7697785 -2.3633816 -2.1432712 -1.9687741 -1.9167738 -2.0270247 -2.0653081 -2.2430437 -2.4221122 -2.5515847 -2.8609533 -3.0351696 -3.1708307][-4.1056871 -3.5947046 -2.9296207 -2.4971826 -2.3645248 -2.327486 -2.3532224 -2.5437918 -2.7353268 -3.0497947 -3.33178 -3.4456964 -3.5594323 -3.5603838 -3.6206269][-4.3902245 -3.964314 -3.3794527 -3.043715 -3.0505009 -3.1527858 -3.2383628 -3.4389873 -3.678894 -4.005507 -4.2671971 -4.3119826 -4.2438717 -4.1235194 -4.1605735][-4.4816713 -4.2178936 -3.8241119 -3.6542621 -3.8093286 -4.0285535 -4.1522827 -4.2894921 -4.438549 -4.6395812 -4.7800617 -4.7393155 -4.5837054 -4.4565334 -4.5257387][-4.2343574 -4.1271453 -3.9157238 -3.8849268 -4.1608348 -4.4933381 -4.6751628 -4.7243271 -4.6821575 -4.6559706 -4.6267695 -4.5264082 -4.3906546 -4.3485513 -4.4696531][-3.4876819 -3.4919722 -3.4306927 -3.5200253 -3.9091995 -4.3480072 -4.5840006 -4.5485096 -4.2938066 -4.0224442 -3.836426 -3.7130554 -3.6651564 -3.7552481 -3.9393501][-2.495121 -2.5602875 -2.6245127 -2.83989 -3.3377633 -3.8376112 -4.0733418 -3.9508829 -3.5358195 -3.0968618 -2.8244653 -2.7220976 -2.7763274 -2.9795918 -3.2161891][-1.8080137 -1.8739319 -2.0161984 -2.3244534 -2.8863485 -3.3934731 -3.5998378 -3.4345679 -2.9753537 -2.4981642 -2.2177713 -2.147052 -2.2525117 -2.481694 -2.7056196]]...]
INFO - root - 2017-12-06 08:32:13.211027: step 12210, loss = 0.80, batch loss = 0.73 (12.8 examples/sec; 0.625 sec/batch; 55h:38m:56s remains)
INFO - root - 2017-12-06 08:32:19.350590: step 12220, loss = 1.09, batch loss = 1.02 (12.7 examples/sec; 0.629 sec/batch; 55h:55m:35s remains)
INFO - root - 2017-12-06 08:32:25.496390: step 12230, loss = 1.09, batch loss = 1.02 (13.3 examples/sec; 0.603 sec/batch; 53h:37m:07s remains)
INFO - root - 2017-12-06 08:32:31.592187: step 12240, loss = 0.65, batch loss = 0.58 (13.0 examples/sec; 0.614 sec/batch; 54h:39m:08s remains)
INFO - root - 2017-12-06 08:32:37.684075: step 12250, loss = 1.00, batch loss = 0.93 (12.9 examples/sec; 0.620 sec/batch; 55h:11m:40s remains)
INFO - root - 2017-12-06 08:32:43.732666: step 12260, loss = 0.82, batch loss = 0.74 (13.1 examples/sec; 0.612 sec/batch; 54h:25m:40s remains)
INFO - root - 2017-12-06 08:32:49.831812: step 12270, loss = 0.84, batch loss = 0.77 (12.7 examples/sec; 0.630 sec/batch; 56h:00m:24s remains)
INFO - root - 2017-12-06 08:32:55.805876: step 12280, loss = 0.73, batch loss = 0.66 (13.3 examples/sec; 0.602 sec/batch; 53h:34m:09s remains)
INFO - root - 2017-12-06 08:33:01.963276: step 12290, loss = 0.95, batch loss = 0.88 (13.2 examples/sec; 0.608 sec/batch; 54h:06m:28s remains)
INFO - root - 2017-12-06 08:33:07.998442: step 12300, loss = 0.78, batch loss = 0.71 (13.0 examples/sec; 0.617 sec/batch; 54h:54m:59s remains)
2017-12-06 08:33:08.538544: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7460432 -4.8048315 -4.9462461 -5.1447549 -5.4669576 -5.8724356 -6.0927472 -5.97164 -5.6923943 -5.469018 -5.0968719 -4.753036 -4.6650352 -4.706924 -4.6919012][-4.4001012 -4.3471041 -4.5023432 -4.7429943 -5.1451697 -5.6369629 -5.925703 -5.9387579 -5.859036 -5.7863936 -5.4707165 -5.0939803 -4.9335203 -4.8967953 -4.8028355][-3.963855 -3.7190037 -3.8326912 -4.0643983 -4.4593406 -4.9991174 -5.3990498 -5.6355877 -5.81403 -5.8982067 -5.67077 -5.3384461 -5.1784906 -5.0924859 -4.911376][-3.6099918 -3.171051 -3.1783171 -3.3332398 -3.6634741 -4.2097559 -4.6996436 -5.1262493 -5.5165792 -5.7051144 -5.5580835 -5.33481 -5.269794 -5.2049007 -4.9695725][-3.5950136 -2.9732947 -2.7823582 -2.7714081 -2.9486184 -3.3538566 -3.8011093 -4.3408561 -4.9356165 -5.2840042 -5.3053832 -5.2755036 -5.3413539 -5.3100705 -5.0292368][-3.9846089 -3.201478 -2.717689 -2.3972893 -2.2602415 -2.3450449 -2.6172643 -3.2137871 -4.0008554 -4.5920768 -4.9162264 -5.1768942 -5.3984723 -5.4097438 -5.1230216][-4.6796432 -3.7977138 -2.9670849 -2.2160988 -1.6368558 -1.3115005 -1.3977449 -2.1030073 -3.1238158 -3.9895561 -4.6353588 -5.1648874 -5.4994721 -5.53746 -5.2662153][-5.3810735 -4.5064044 -3.4114804 -2.2603152 -1.2809725 -0.62128043 -0.62025905 -1.514221 -2.7887285 -3.8991308 -4.7748275 -5.4640036 -5.8312483 -5.8454056 -5.5513773][-5.9782753 -5.1939759 -3.9870262 -2.6048293 -1.3961191 -0.55981684 -0.51794481 -1.4953864 -2.8932676 -4.13415 -5.1507993 -5.9260554 -6.2718773 -6.2217422 -5.8820014][-6.4423947 -5.85657 -4.7158637 -3.3126857 -2.0985405 -1.2638776 -1.1721156 -2.0206428 -3.2917328 -4.4733171 -5.50063 -6.2722826 -6.540432 -6.4188151 -6.0706282][-6.6576896 -6.4004693 -5.5088515 -4.2749605 -3.2022569 -2.4715278 -2.3491292 -3.002656 -4.0234995 -4.9861059 -5.8504944 -6.4671726 -6.5898018 -6.4088831 -6.0853634][-6.3922038 -6.4578533 -5.9053564 -4.9651504 -4.0991888 -3.5165033 -3.4132934 -3.9106474 -4.6736822 -5.3654666 -5.9853821 -6.3872442 -6.35228 -6.0993929 -5.7940512][-5.7043486 -5.8987274 -5.6082897 -4.998395 -4.3971376 -3.9954152 -3.9571266 -4.3178034 -4.8173866 -5.267489 -5.7078323 -5.9641762 -5.8408542 -5.5530849 -5.2841191][-5.0270863 -5.1716228 -4.99295 -4.6349654 -4.2829623 -4.062798 -4.0814028 -4.3090205 -4.5912437 -4.8933773 -5.237958 -5.428874 -5.31457 -5.0851345 -4.8941946][-4.6240163 -4.649775 -4.4649858 -4.231328 -4.0260067 -3.9039648 -3.9484997 -4.1016331 -4.2652006 -4.4912152 -4.7642293 -4.918643 -4.8636971 -4.7344 -4.5990372]]...]
INFO - root - 2017-12-06 08:33:14.701303: step 12310, loss = 0.99, batch loss = 0.92 (13.5 examples/sec; 0.592 sec/batch; 52h:41m:31s remains)
INFO - root - 2017-12-06 08:33:20.808148: step 12320, loss = 0.84, batch loss = 0.77 (12.8 examples/sec; 0.625 sec/batch; 55h:34m:27s remains)
INFO - root - 2017-12-06 08:33:26.881653: step 12330, loss = 0.78, batch loss = 0.71 (13.2 examples/sec; 0.607 sec/batch; 53h:59m:17s remains)
INFO - root - 2017-12-06 08:33:32.973412: step 12340, loss = 1.15, batch loss = 1.08 (13.1 examples/sec; 0.612 sec/batch; 54h:27m:07s remains)
INFO - root - 2017-12-06 08:33:39.065284: step 12350, loss = 0.93, batch loss = 0.86 (13.2 examples/sec; 0.607 sec/batch; 54h:00m:27s remains)
INFO - root - 2017-12-06 08:33:45.136800: step 12360, loss = 0.86, batch loss = 0.79 (13.4 examples/sec; 0.598 sec/batch; 53h:13m:10s remains)
INFO - root - 2017-12-06 08:33:51.290470: step 12370, loss = 0.77, batch loss = 0.70 (12.7 examples/sec; 0.628 sec/batch; 55h:49m:56s remains)
INFO - root - 2017-12-06 08:33:57.381240: step 12380, loss = 0.80, batch loss = 0.73 (13.6 examples/sec; 0.588 sec/batch; 52h:16m:21s remains)
INFO - root - 2017-12-06 08:34:03.466708: step 12390, loss = 1.07, batch loss = 1.00 (13.0 examples/sec; 0.616 sec/batch; 54h:43m:54s remains)
INFO - root - 2017-12-06 08:34:09.406904: step 12400, loss = 0.82, batch loss = 0.75 (13.7 examples/sec; 0.586 sec/batch; 52h:04m:02s remains)
2017-12-06 08:34:09.941655: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0902357 -3.1011052 -3.362277 -3.5358019 -3.6229007 -3.6747499 -3.6811159 -3.6046746 -3.3880711 -3.3164601 -3.5167274 -3.7641037 -3.9066205 -3.8453612 -3.7188246][-3.2592115 -3.2501488 -3.4967277 -3.6145103 -3.6224856 -3.648324 -3.6139388 -3.4511144 -3.180841 -3.1708827 -3.4891241 -3.8124628 -3.9694085 -3.9296565 -3.8030286][-3.8429852 -3.733916 -3.8942976 -3.9448307 -3.9081769 -3.9376028 -3.8634815 -3.581532 -3.2121396 -3.2509854 -3.7346423 -4.1984859 -4.396666 -4.421711 -4.3490124][-4.3028021 -4.0015941 -4.0271273 -4.05766 -4.0782757 -4.1581759 -4.05128 -3.625052 -3.1074896 -3.1665902 -3.8841107 -4.6183949 -4.9629774 -5.1097784 -5.0657825][-4.1617808 -3.6608684 -3.5501733 -3.629885 -3.8107479 -3.9499393 -3.7425125 -3.1007 -2.388716 -2.4063849 -3.2896204 -4.2939353 -4.8540378 -5.1963015 -5.1941242][-3.6573396 -3.02431 -2.7937164 -2.9051309 -3.2333636 -3.3959055 -3.077662 -2.2625446 -1.4395499 -1.4070096 -2.3323503 -3.4614167 -4.1678267 -4.7079673 -4.7694249][-3.3663573 -2.7798624 -2.5121708 -2.6072888 -2.9533815 -3.0306189 -2.5721025 -1.6732297 -0.90819383 -0.90147781 -1.770262 -2.826591 -3.5013633 -4.0846119 -4.1228094][-3.5463295 -3.1730709 -2.9537873 -3.0258248 -3.3285689 -3.2959418 -2.7477145 -1.8632014 -1.2544851 -1.3127356 -2.0610342 -2.8910987 -3.3252034 -3.70246 -3.5778971][-4.069767 -3.8874059 -3.6977782 -3.7425785 -4.0558848 -4.06528 -3.6189759 -2.9068232 -2.5164635 -2.6180003 -3.16749 -3.7162642 -3.9067748 -4.0425749 -3.7451699][-4.4647241 -4.4224768 -4.2379947 -4.2425809 -4.5798035 -4.7000961 -4.4422703 -3.9742227 -3.8001232 -3.9105828 -4.1815681 -4.4100947 -4.46412 -4.5618777 -4.3180342][-4.555779 -4.6168346 -4.4685655 -4.4449606 -4.7523856 -4.9088683 -4.7727442 -4.5041795 -4.5159812 -4.6467309 -4.6883183 -4.6426759 -4.6097379 -4.7609639 -4.7164855][-4.6969352 -4.7714057 -4.6412778 -4.5706897 -4.7739172 -4.88884 -4.7713385 -4.5637116 -4.6497774 -4.8178163 -4.7756853 -4.5811644 -4.4756069 -4.6342826 -4.7536964][-4.9264226 -5.0000567 -4.9010711 -4.7831111 -4.8226538 -4.8439465 -4.7288351 -4.5275674 -4.5615296 -4.6741686 -4.6072831 -4.3651638 -4.2133112 -4.33578 -4.5390015][-4.9534531 -5.0376792 -4.9751096 -4.8234878 -4.6974983 -4.6298251 -4.5802069 -4.4922071 -4.5275311 -4.5460491 -4.42435 -4.13897 -3.9656358 -4.0589895 -4.265677][-4.8084106 -4.9270315 -4.9152856 -4.7264261 -4.4426513 -4.2972422 -4.3334837 -4.4145164 -4.4935374 -4.4263864 -4.246088 -3.961669 -3.844456 -3.9635026 -4.1282187]]...]
INFO - root - 2017-12-06 08:34:15.996093: step 12410, loss = 1.17, batch loss = 1.10 (13.6 examples/sec; 0.589 sec/batch; 52h:23m:59s remains)
INFO - root - 2017-12-06 08:34:22.076760: step 12420, loss = 0.71, batch loss = 0.64 (12.8 examples/sec; 0.623 sec/batch; 55h:23m:37s remains)
INFO - root - 2017-12-06 08:34:28.072521: step 12430, loss = 0.95, batch loss = 0.88 (13.5 examples/sec; 0.591 sec/batch; 52h:35m:15s remains)
INFO - root - 2017-12-06 08:34:34.220683: step 12440, loss = 0.87, batch loss = 0.80 (12.9 examples/sec; 0.622 sec/batch; 55h:15m:43s remains)
INFO - root - 2017-12-06 08:34:40.295503: step 12450, loss = 1.11, batch loss = 1.04 (13.3 examples/sec; 0.602 sec/batch; 53h:32m:09s remains)
INFO - root - 2017-12-06 08:34:46.414712: step 12460, loss = 0.90, batch loss = 0.83 (13.6 examples/sec; 0.586 sec/batch; 52h:07m:23s remains)
INFO - root - 2017-12-06 08:34:52.510250: step 12470, loss = 0.78, batch loss = 0.71 (13.2 examples/sec; 0.605 sec/batch; 53h:45m:30s remains)
INFO - root - 2017-12-06 08:34:58.620217: step 12480, loss = 0.94, batch loss = 0.87 (13.1 examples/sec; 0.611 sec/batch; 54h:18m:43s remains)
INFO - root - 2017-12-06 08:35:04.675015: step 12490, loss = 0.87, batch loss = 0.80 (13.2 examples/sec; 0.607 sec/batch; 53h:58m:40s remains)
INFO - root - 2017-12-06 08:35:10.812571: step 12500, loss = 0.96, batch loss = 0.89 (13.4 examples/sec; 0.596 sec/batch; 52h:59m:42s remains)
2017-12-06 08:35:11.374319: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.902225 -4.6281037 -4.3140078 -4.0107489 -3.4862533 -3.046875 -3.0418813 -3.3526716 -3.6387231 -3.79433 -3.8812604 -3.8833628 -3.8347323 -3.7591879 -3.7347274][-4.5156713 -4.2182412 -3.9345582 -3.645936 -3.1593838 -2.7841644 -2.8184667 -3.1802239 -3.5229459 -3.7047744 -3.7849648 -3.7922964 -3.8228583 -3.8518026 -3.8929439][-3.9394391 -3.7213802 -3.5783086 -3.4047406 -3.0262742 -2.6973925 -2.6600137 -2.9435196 -3.2487681 -3.4309444 -3.5582573 -3.6701531 -3.8453319 -3.9954755 -4.0732074][-3.4207151 -3.3484054 -3.382072 -3.3580608 -3.0937195 -2.7443047 -2.5358369 -2.6309581 -2.8292561 -3.013308 -3.239646 -3.5060327 -3.81573 -4.0461726 -4.1330795][-3.0038834 -3.0535452 -3.2151356 -3.321255 -3.162241 -2.7859254 -2.4407673 -2.3797777 -2.4711876 -2.6426642 -2.9400916 -3.3119745 -3.6799073 -3.9340191 -4.0368481][-3.0524836 -3.0763583 -3.2100906 -3.3400636 -3.2332826 -2.8474746 -2.4704008 -2.3649437 -2.4010041 -2.5545311 -2.8610263 -3.2418425 -3.5882683 -3.8211532 -3.9424059][-3.7728362 -3.6784291 -3.6456397 -3.6396747 -3.4459136 -2.9936068 -2.625721 -2.5593085 -2.597605 -2.7486341 -3.0502329 -3.4214525 -3.7276258 -3.926955 -4.0318289][-4.6283326 -4.4592257 -4.2784324 -4.0955272 -3.7546434 -3.2331514 -2.8925905 -2.8815925 -2.932797 -3.0920472 -3.4061038 -3.7882776 -4.083477 -4.2678061 -4.33212][-5.1500349 -4.9754505 -4.7328849 -4.4292731 -3.9817531 -3.4460149 -3.1452975 -3.1527884 -3.193047 -3.3670509 -3.7189376 -4.1295023 -4.4261594 -4.5788159 -4.5792546][-5.2535505 -5.1124625 -4.8878083 -4.5708704 -4.1090169 -3.5994632 -3.3131251 -3.3056035 -3.3531454 -3.5498703 -3.9142549 -4.3042107 -4.5418348 -4.5979729 -4.5137129][-4.9466739 -4.8857269 -4.7675438 -4.5591159 -4.20028 -3.7821879 -3.5322289 -3.5251331 -3.6058397 -3.8027694 -4.0957217 -4.3508191 -4.4437127 -4.3825073 -4.2392669][-4.547195 -4.5422382 -4.509913 -4.4158583 -4.212893 -3.9575241 -3.8070378 -3.8437698 -4.0010614 -4.207922 -4.3868918 -4.4489813 -4.3882213 -4.2698083 -4.1289997][-4.3980126 -4.3666224 -4.31084 -4.2362061 -4.1314659 -4.0170593 -3.9687076 -4.06963 -4.3393264 -4.6161318 -4.7532959 -4.6921959 -4.5498371 -4.4351773 -4.3074932][-4.3994479 -4.3252616 -4.2207727 -4.141037 -4.0833335 -4.0427737 -4.0500507 -4.1989145 -4.5561008 -4.9071126 -5.0648422 -4.9919486 -4.8748951 -4.8152151 -4.7068367][-4.4511724 -4.3580356 -4.232398 -4.171237 -4.1342468 -4.0999956 -4.0947671 -4.2313528 -4.6096754 -5.0161924 -5.2352791 -5.22849 -5.2016115 -5.2308011 -5.1707983]]...]
INFO - root - 2017-12-06 08:35:17.289778: step 12510, loss = 0.94, batch loss = 0.87 (13.1 examples/sec; 0.609 sec/batch; 54h:07m:31s remains)
INFO - root - 2017-12-06 08:35:23.473527: step 12520, loss = 1.06, batch loss = 0.99 (12.8 examples/sec; 0.627 sec/batch; 55h:45m:44s remains)
INFO - root - 2017-12-06 08:35:29.540658: step 12530, loss = 0.86, batch loss = 0.79 (13.3 examples/sec; 0.603 sec/batch; 53h:37m:22s remains)
INFO - root - 2017-12-06 08:35:35.665929: step 12540, loss = 0.98, batch loss = 0.91 (12.9 examples/sec; 0.619 sec/batch; 55h:00m:28s remains)
INFO - root - 2017-12-06 08:35:41.693455: step 12550, loss = 1.11, batch loss = 1.04 (13.2 examples/sec; 0.605 sec/batch; 53h:43m:30s remains)
INFO - root - 2017-12-06 08:35:47.810886: step 12560, loss = 1.03, batch loss = 0.96 (13.5 examples/sec; 0.595 sec/batch; 52h:50m:25s remains)
INFO - root - 2017-12-06 08:35:53.915542: step 12570, loss = 0.76, batch loss = 0.69 (12.9 examples/sec; 0.622 sec/batch; 55h:19m:03s remains)
INFO - root - 2017-12-06 08:35:59.956958: step 12580, loss = 0.99, batch loss = 0.92 (16.3 examples/sec; 0.490 sec/batch; 43h:32m:43s remains)
INFO - root - 2017-12-06 08:36:06.029677: step 12590, loss = 0.85, batch loss = 0.78 (12.9 examples/sec; 0.621 sec/batch; 55h:09m:49s remains)
INFO - root - 2017-12-06 08:36:12.061585: step 12600, loss = 0.85, batch loss = 0.78 (13.8 examples/sec; 0.580 sec/batch; 51h:34m:56s remains)
2017-12-06 08:36:12.606479: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.21073 -3.9561911 -3.8437457 -3.7884336 -3.9397521 -4.3330789 -4.7136731 -5.129015 -5.1172953 -4.6879387 -4.3383393 -4.2746482 -4.3321071 -4.6010923 -5.1617517][-4.024971 -3.6958451 -3.490036 -3.3468323 -3.4237115 -3.7069409 -3.9961946 -4.4034157 -4.4923663 -4.2462249 -4.0523973 -4.104763 -4.3058972 -4.7167492 -5.3027859][-3.974602 -3.6639013 -3.5127249 -3.3292396 -3.165185 -3.1231246 -3.2175317 -3.6557539 -3.9756212 -3.9981019 -3.9520314 -4.002624 -4.1898046 -4.6062775 -5.16438][-3.8613343 -3.67451 -3.7385347 -3.619473 -3.2029457 -2.7597623 -2.6177335 -3.0809259 -3.6899838 -4.058126 -4.1913228 -4.1979604 -4.2462878 -4.5123959 -4.9331141][-3.6329596 -3.5964031 -3.8737686 -3.8294878 -3.1954212 -2.39891 -2.0312679 -2.4364433 -3.2409022 -3.9386199 -4.3707194 -4.5128145 -4.5581245 -4.6990771 -4.9234872][-3.2747312 -3.4327867 -3.8758719 -3.8368089 -2.9897971 -1.879844 -1.3124604 -1.6164074 -2.4892821 -3.4474404 -4.2477937 -4.7534227 -5.0113564 -5.146554 -5.2092757][-2.8562646 -3.2465067 -3.8732841 -3.8644247 -2.9094334 -1.6092694 -0.88820481 -1.0124295 -1.7356715 -2.7227049 -3.7638178 -4.6775274 -5.2404866 -5.4935079 -5.5461717][-2.4938526 -3.1168947 -3.9739747 -4.0989356 -3.2410185 -1.9627767 -1.1702538 -1.0562098 -1.4418688 -2.2064672 -3.2660518 -4.4504128 -5.2448907 -5.6231837 -5.7837791][-2.5023499 -3.2465901 -4.2402043 -4.5314069 -3.9046125 -2.8321872 -2.0854113 -1.8136466 -1.8867805 -2.3162615 -3.1792221 -4.3536277 -5.1685934 -5.6009359 -5.9133663][-2.9312901 -3.6767697 -4.659544 -5.0719118 -4.6973023 -3.899385 -3.2790391 -2.9694667 -2.8614383 -2.9578295 -3.4741392 -4.349237 -4.9654202 -5.3797927 -5.8388][-3.6083953 -4.2875657 -5.1904984 -5.67334 -5.4906569 -4.9390907 -4.4629731 -4.1970539 -4.02034 -3.8426251 -3.9708724 -4.4130964 -4.7366471 -5.0903773 -5.6234155][-4.332242 -4.8698287 -5.6293488 -6.1134033 -6.0648408 -5.7280464 -5.3943253 -5.1953864 -4.9951029 -4.6550527 -4.4907651 -4.5674419 -4.66891 -4.9540315 -5.4513235][-4.8220072 -5.1511083 -5.7024765 -6.0966492 -6.1178427 -5.9548321 -5.7469234 -5.6166039 -5.4311213 -5.0749011 -4.7983513 -4.6888242 -4.7130246 -4.955287 -5.356462][-5.0885725 -5.2372508 -5.5610867 -5.8132105 -5.8496265 -5.7909474 -5.6698027 -5.5811887 -5.43834 -5.1666837 -4.9171262 -4.7713814 -4.7956462 -5.0017519 -5.2986155][-5.0708623 -5.0990868 -5.2356772 -5.3504019 -5.3742118 -5.3631945 -5.3104324 -5.26033 -5.1724725 -5.0109415 -4.8504028 -4.7538037 -4.7957072 -4.9603262 -5.1714163]]...]
INFO - root - 2017-12-06 08:36:18.694473: step 12610, loss = 0.84, batch loss = 0.77 (13.2 examples/sec; 0.605 sec/batch; 53h:48m:09s remains)
INFO - root - 2017-12-06 08:36:24.646578: step 12620, loss = 0.84, batch loss = 0.77 (13.2 examples/sec; 0.606 sec/batch; 53h:52m:03s remains)
INFO - root - 2017-12-06 08:36:30.795871: step 12630, loss = 0.89, batch loss = 0.82 (12.8 examples/sec; 0.623 sec/batch; 55h:20m:06s remains)
INFO - root - 2017-12-06 08:36:36.845961: step 12640, loss = 0.86, batch loss = 0.79 (13.3 examples/sec; 0.602 sec/batch; 53h:27m:16s remains)
INFO - root - 2017-12-06 08:36:42.919762: step 12650, loss = 0.89, batch loss = 0.82 (13.3 examples/sec; 0.603 sec/batch; 53h:35m:54s remains)
INFO - root - 2017-12-06 08:36:48.984389: step 12660, loss = 1.02, batch loss = 0.95 (13.3 examples/sec; 0.603 sec/batch; 53h:36m:41s remains)
INFO - root - 2017-12-06 08:36:55.069537: step 12670, loss = 0.96, batch loss = 0.89 (13.2 examples/sec; 0.608 sec/batch; 53h:59m:28s remains)
INFO - root - 2017-12-06 08:37:01.081564: step 12680, loss = 1.09, batch loss = 1.02 (13.7 examples/sec; 0.584 sec/batch; 51h:52m:26s remains)
INFO - root - 2017-12-06 08:37:07.218382: step 12690, loss = 1.04, batch loss = 0.97 (13.0 examples/sec; 0.616 sec/batch; 54h:44m:45s remains)
INFO - root - 2017-12-06 08:37:13.336180: step 12700, loss = 1.14, batch loss = 1.07 (13.0 examples/sec; 0.615 sec/batch; 54h:38m:33s remains)
2017-12-06 08:37:13.904434: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6806707 -4.6658421 -4.8562918 -5.1421618 -5.4261494 -5.4803519 -5.3590117 -5.2200527 -5.1998158 -5.2572961 -5.3310866 -5.3645797 -5.3049593 -5.158567 -4.9009762][-4.5964656 -4.6447916 -4.9482942 -5.272037 -5.4639444 -5.4680004 -5.350657 -5.1719303 -5.0749316 -5.0617495 -5.1245656 -5.181241 -5.0731034 -4.88672 -4.6625295][-4.5395641 -4.7007689 -5.111361 -5.4149833 -5.4168196 -5.2172041 -4.9585686 -4.7131157 -4.6476245 -4.6730747 -4.8098617 -4.9853091 -4.8698153 -4.6401114 -4.5135937][-4.680665 -4.9200239 -5.3480134 -5.5625987 -5.3431072 -4.8167782 -4.2169585 -3.8386774 -3.9237118 -4.1101055 -4.379015 -4.74099 -4.6630726 -4.4088941 -4.4139457][-4.8061042 -4.9926968 -5.2731495 -5.3461251 -4.9943967 -4.1773105 -3.1663017 -2.6327562 -2.9536858 -3.3883586 -3.8210549 -4.4239616 -4.4674144 -4.2226381 -4.311357][-4.7055912 -4.7131162 -4.6968565 -4.6188226 -4.2855225 -3.3242562 -1.973737 -1.3297682 -1.9398675 -2.6875229 -3.3417652 -4.2037497 -4.4065242 -4.2214451 -4.3452706][-4.2288523 -4.0404844 -3.7557273 -3.6241648 -3.4511485 -2.4962735 -0.96376634 -0.30935431 -1.2481394 -2.3297071 -3.212141 -4.2416925 -4.5743103 -4.5071306 -4.6395168][-3.5470643 -3.2510352 -2.882282 -2.8361559 -2.8934021 -2.0905476 -0.654819 -0.18412113 -1.3888681 -2.68178 -3.6594348 -4.6700773 -5.0278125 -5.0297747 -5.0830193][-3.0796041 -2.7969003 -2.5567956 -2.6360078 -2.880527 -2.3846028 -1.3674533 -1.2010343 -2.4123278 -3.6818526 -4.5936809 -5.4516859 -5.7567062 -5.7291532 -5.613946][-3.1178904 -2.9530478 -2.9174068 -3.0072002 -3.2927423 -3.1310043 -2.6080666 -2.6962237 -3.7518466 -4.8176222 -5.5265784 -6.1519117 -6.3481021 -6.2584367 -6.0044436][-3.491581 -3.4897318 -3.6271729 -3.6396897 -3.8962693 -4.044558 -3.9282255 -4.1140981 -4.8645163 -5.5769992 -5.9579506 -6.2951555 -6.4205084 -6.3643336 -6.0978808][-3.8713989 -4.0091705 -4.2849374 -4.2422223 -4.4865856 -4.8620176 -4.9568996 -5.0763025 -5.4608812 -5.7679076 -5.8147392 -5.9050541 -6.0205445 -6.0815868 -5.9026551][-3.895992 -4.15789 -4.542788 -4.4924607 -4.7340832 -5.2026615 -5.3549972 -5.35732 -5.4315319 -5.4141326 -5.236506 -5.1689558 -5.2775297 -5.4366045 -5.3710437][-3.380394 -3.7195463 -4.1831665 -4.1731787 -4.411624 -4.8496561 -4.9615374 -4.87049 -4.7472067 -4.5393729 -4.2799754 -4.1701245 -4.2760272 -4.4768376 -4.5005708][-2.6841407 -3.0123553 -3.4507918 -3.4382746 -3.6232076 -3.9180346 -3.9235861 -3.778856 -3.5916457 -3.3663936 -3.1747179 -3.1505942 -3.3024397 -3.5072985 -3.5506601]]...]
INFO - root - 2017-12-06 08:37:20.023425: step 12710, loss = 0.85, batch loss = 0.78 (13.5 examples/sec; 0.592 sec/batch; 52h:37m:42s remains)
INFO - root - 2017-12-06 08:37:25.994455: step 12720, loss = 0.78, batch loss = 0.71 (13.0 examples/sec; 0.615 sec/batch; 54h:36m:33s remains)
INFO - root - 2017-12-06 08:37:32.051124: step 12730, loss = 0.86, batch loss = 0.79 (13.7 examples/sec; 0.584 sec/batch; 51h:53m:08s remains)
INFO - root - 2017-12-06 08:37:38.129525: step 12740, loss = 1.02, batch loss = 0.95 (13.4 examples/sec; 0.597 sec/batch; 53h:02m:10s remains)
INFO - root - 2017-12-06 08:37:44.333442: step 12750, loss = 0.96, batch loss = 0.89 (13.0 examples/sec; 0.615 sec/batch; 54h:35m:12s remains)
INFO - root - 2017-12-06 08:37:50.378533: step 12760, loss = 1.49, batch loss = 1.42 (13.0 examples/sec; 0.618 sec/batch; 54h:51m:05s remains)
INFO - root - 2017-12-06 08:37:56.435259: step 12770, loss = 0.76, batch loss = 0.69 (13.4 examples/sec; 0.598 sec/batch; 53h:07m:10s remains)
INFO - root - 2017-12-06 08:38:02.508764: step 12780, loss = 0.93, batch loss = 0.86 (13.3 examples/sec; 0.604 sec/batch; 53h:36m:30s remains)
INFO - root - 2017-12-06 08:38:08.567458: step 12790, loss = 0.81, batch loss = 0.74 (13.2 examples/sec; 0.605 sec/batch; 53h:44m:44s remains)
INFO - root - 2017-12-06 08:38:14.757929: step 12800, loss = 0.96, batch loss = 0.89 (12.8 examples/sec; 0.624 sec/batch; 55h:23m:33s remains)
2017-12-06 08:38:15.324247: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6747537 -4.5031581 -4.9402876 -5.145154 -5.2120318 -5.0727134 -4.8892465 -4.6803517 -4.4338055 -4.3397827 -4.3504872 -4.3962522 -4.4274611 -4.3122878 -4.1728806][-3.2683821 -4.4642429 -5.1193204 -5.3888559 -5.4411039 -5.17944 -4.8317585 -4.5232549 -4.3448114 -4.4427276 -4.5964594 -4.7061605 -4.6933222 -4.45416 -4.1864619][-3.0193586 -4.480823 -5.2699256 -5.5292187 -5.4884739 -5.0491576 -4.4915905 -4.0304966 -3.9304731 -4.2799673 -4.677032 -4.9936552 -5.0703039 -4.8155665 -4.5076747][-3.0265691 -4.5432243 -5.3079848 -5.4536724 -5.2318978 -4.5902963 -3.8418021 -3.2229705 -3.1942048 -3.8130202 -4.5155549 -5.1231751 -5.3632951 -5.1560378 -4.8857431][-3.316088 -4.6260982 -5.1919208 -5.128345 -4.6774225 -3.842916 -2.9377789 -2.1620412 -2.1659279 -3.0415287 -4.0912356 -5.0294781 -5.4436808 -5.3105416 -5.1327119][-3.8237028 -4.7639089 -5.0504017 -4.7660565 -4.0877461 -3.0635514 -1.9680276 -0.97499609 -0.92720628 -1.9868317 -3.3648605 -4.6226215 -5.1946692 -5.1347904 -5.0393376][-4.4447346 -5.0265636 -5.0807204 -4.6439729 -3.79746 -2.61651 -1.3242798 -0.076102257 0.10889196 -1.0157261 -2.6321735 -4.1449642 -4.8539 -4.854589 -4.7907419][-4.8943906 -5.2878747 -5.2507567 -4.7995353 -3.9332037 -2.7217436 -1.3491547 0.044993877 0.37050247 -0.65322423 -2.2825129 -3.8472261 -4.5826488 -4.6062694 -4.5377607][-4.9518023 -5.3536134 -5.3776603 -5.0625358 -4.3628197 -3.3159108 -2.0682483 -0.73242283 -0.32425785 -1.0812647 -2.4535093 -3.7962952 -4.3743415 -4.3251176 -4.2082758][-4.5346503 -5.0169792 -5.1693668 -5.07781 -4.6850042 -3.9744346 -3.028317 -1.9277952 -1.5256 -1.9878259 -2.967948 -3.9246936 -4.2288904 -4.0487628 -3.8752415][-3.8247683 -4.3418026 -4.57236 -4.6770229 -4.6027074 -4.2664409 -3.6730349 -2.8759103 -2.5502152 -2.7834115 -3.4006362 -4.0011086 -4.0652294 -3.7663221 -3.5173531][-3.1523495 -3.6293659 -3.847712 -4.0469341 -4.2018957 -4.1655536 -3.8708916 -3.3492661 -3.1372864 -3.2460437 -3.6218696 -3.9841056 -3.8982155 -3.5285857 -3.210947][-2.8187928 -3.2246377 -3.4027071 -3.6327434 -3.9180031 -4.0774407 -3.9940646 -3.6842046 -3.5787392 -3.6402643 -3.8667407 -4.0657659 -3.8902216 -3.4999843 -3.1423354][-2.9670486 -3.2987275 -3.4437735 -3.6718581 -3.9998031 -4.260273 -4.3076625 -4.138505 -4.0895853 -4.1171937 -4.2294106 -4.2794995 -4.0347495 -3.6533277 -3.2921853][-3.3670754 -3.6182191 -3.7230709 -3.9091897 -4.2062359 -4.4919887 -4.61836 -4.5618496 -4.563972 -4.5838542 -4.6152329 -4.5478625 -4.2713418 -3.9382732 -3.6234775]]...]
INFO - root - 2017-12-06 08:38:21.445152: step 12810, loss = 0.89, batch loss = 0.82 (13.2 examples/sec; 0.608 sec/batch; 54h:00m:09s remains)
INFO - root - 2017-12-06 08:38:27.563399: step 12820, loss = 0.97, batch loss = 0.90 (12.9 examples/sec; 0.619 sec/batch; 54h:58m:10s remains)
INFO - root - 2017-12-06 08:38:33.572481: step 12830, loss = 1.03, batch loss = 0.96 (13.7 examples/sec; 0.586 sec/batch; 52h:02m:19s remains)
INFO - root - 2017-12-06 08:38:39.654774: step 12840, loss = 1.00, batch loss = 0.93 (13.2 examples/sec; 0.607 sec/batch; 53h:53m:20s remains)
INFO - root - 2017-12-06 08:38:45.798217: step 12850, loss = 0.99, batch loss = 0.92 (12.5 examples/sec; 0.639 sec/batch; 56h:42m:35s remains)
INFO - root - 2017-12-06 08:38:51.891696: step 12860, loss = 0.70, batch loss = 0.63 (13.6 examples/sec; 0.588 sec/batch; 52h:12m:36s remains)
INFO - root - 2017-12-06 08:38:58.010699: step 12870, loss = 1.02, batch loss = 0.95 (12.7 examples/sec; 0.629 sec/batch; 55h:53m:19s remains)
INFO - root - 2017-12-06 08:39:04.092619: step 12880, loss = 0.91, batch loss = 0.84 (13.0 examples/sec; 0.617 sec/batch; 54h:47m:45s remains)
INFO - root - 2017-12-06 08:39:10.129835: step 12890, loss = 0.84, batch loss = 0.77 (13.1 examples/sec; 0.611 sec/batch; 54h:15m:40s remains)
INFO - root - 2017-12-06 08:39:16.213888: step 12900, loss = 0.88, batch loss = 0.81 (13.6 examples/sec; 0.589 sec/batch; 52h:17m:48s remains)
2017-12-06 08:39:16.794901: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5354247 -4.8772888 -5.1398277 -5.3167896 -5.422595 -5.4537439 -5.4589252 -5.4831834 -5.5595417 -5.70909 -5.8101463 -5.7159085 -5.3767223 -5.0300531 -5.0010948][-4.5127549 -5.0234904 -5.4373269 -5.69122 -5.7982388 -5.7719269 -5.6870866 -5.5881186 -5.5869694 -5.8109045 -5.9991121 -5.8555169 -5.3445973 -4.8420849 -4.8135133][-4.1312952 -4.827703 -5.4615412 -5.8488817 -5.9578371 -5.8413916 -5.6250358 -5.3174891 -5.1594105 -5.4718332 -5.8007021 -5.6610188 -4.9923673 -4.30522 -4.2367878][-3.5577302 -4.4377031 -5.3182468 -5.8469224 -5.9315395 -5.6692815 -5.2533636 -4.6721396 -4.3284006 -4.7535205 -5.2698665 -5.2685547 -4.5981455 -3.786108 -3.633781][-3.0856256 -4.1091986 -5.14915 -5.6895871 -5.6622038 -5.1776285 -4.4745755 -3.5829415 -3.1058679 -3.7113941 -4.4784384 -4.7528677 -4.2902708 -3.4780746 -3.2221773][-2.7746654 -3.8393335 -4.8957925 -5.2890916 -5.0408173 -4.2807407 -3.2423658 -2.0552092 -1.5699544 -2.4576254 -3.529388 -4.1526175 -4.04032 -3.3693845 -3.0342293][-2.587914 -3.557476 -4.4965377 -4.6537929 -4.1380186 -3.0899205 -1.7413192 -0.30199432 0.12610722 -1.0355685 -2.3863194 -3.3628042 -3.6944702 -3.3529882 -3.0726047][-2.5938859 -3.3887284 -4.1595612 -4.1462607 -3.4801273 -2.2743146 -0.75127745 0.83194065 1.1885877 -0.13041449 -1.651737 -2.8897579 -3.6294415 -3.6614747 -3.495816][-2.6700218 -3.2854104 -3.94653 -3.9648032 -3.3983436 -2.2702529 -0.76359415 0.77378893 1.0282311 -0.28608942 -1.8209081 -3.1483707 -4.1261816 -4.4240217 -4.3104243][-2.6810679 -3.1086173 -3.7110815 -3.9159865 -3.6590867 -2.8365722 -1.5639021 -0.27194548 -0.110466 -1.2199631 -2.5397243 -3.7168152 -4.7159085 -5.1980414 -5.1608839][-2.6809781 -2.8748362 -3.39556 -3.8087459 -3.940681 -3.5787654 -2.709275 -1.7487752 -1.6554122 -2.4860463 -3.447577 -4.3018875 -5.1314206 -5.6713052 -5.6874347][-2.8944211 -2.8223772 -3.1580393 -3.6584039 -4.0826607 -4.1404762 -3.6684978 -2.9891882 -2.9428144 -3.567296 -4.2374239 -4.7828412 -5.3753743 -5.8479071 -5.8318219][-3.3878219 -3.1134238 -3.1968369 -3.6405175 -4.1709728 -4.4856658 -4.2839575 -3.7831318 -3.7560251 -4.2136378 -4.6551161 -4.9342895 -5.2609668 -5.5747595 -5.4923663][-4.0109534 -3.6730945 -3.5297232 -3.8559022 -4.4012775 -4.8596478 -4.8152165 -4.3744698 -4.2629418 -4.4706221 -4.6578574 -4.7135491 -4.8120775 -4.9608574 -4.8549008][-4.2727637 -4.046793 -3.858753 -4.1406107 -4.6972585 -5.22144 -5.2253084 -4.7161274 -4.4212961 -4.3668466 -4.331109 -4.2543383 -4.23166 -4.2878418 -4.2227]]...]
INFO - root - 2017-12-06 08:39:22.887747: step 12910, loss = 1.16, batch loss = 1.09 (13.1 examples/sec; 0.612 sec/batch; 54h:21m:51s remains)
INFO - root - 2017-12-06 08:39:28.971730: step 12920, loss = 0.84, batch loss = 0.77 (13.0 examples/sec; 0.614 sec/batch; 54h:30m:51s remains)
INFO - root - 2017-12-06 08:39:35.026926: step 12930, loss = 0.85, batch loss = 0.78 (13.0 examples/sec; 0.618 sec/batch; 54h:49m:06s remains)
INFO - root - 2017-12-06 08:39:40.975460: step 12940, loss = 0.64, batch loss = 0.57 (13.5 examples/sec; 0.591 sec/batch; 52h:28m:42s remains)
INFO - root - 2017-12-06 08:39:47.143634: step 12950, loss = 1.02, batch loss = 0.95 (13.0 examples/sec; 0.617 sec/batch; 54h:44m:48s remains)
INFO - root - 2017-12-06 08:39:53.272257: step 12960, loss = 1.04, batch loss = 0.97 (13.1 examples/sec; 0.612 sec/batch; 54h:18m:59s remains)
INFO - root - 2017-12-06 08:39:59.404434: step 12970, loss = 0.77, batch loss = 0.70 (13.3 examples/sec; 0.601 sec/batch; 53h:21m:43s remains)
INFO - root - 2017-12-06 08:40:05.497811: step 12980, loss = 0.98, batch loss = 0.91 (12.8 examples/sec; 0.626 sec/batch; 55h:35m:48s remains)
INFO - root - 2017-12-06 08:40:11.569531: step 12990, loss = 0.97, batch loss = 0.90 (13.1 examples/sec; 0.612 sec/batch; 54h:17m:55s remains)
INFO - root - 2017-12-06 08:40:17.693794: step 13000, loss = 1.00, batch loss = 0.93 (12.8 examples/sec; 0.626 sec/batch; 55h:33m:14s remains)
2017-12-06 08:40:18.241207: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0826974 -4.1562734 -4.2260723 -4.2716789 -4.2955384 -4.3088655 -4.3180366 -4.3467879 -4.4190149 -4.496119 -4.5243006 -4.4967141 -4.4337649 -4.3530884 -4.2701406][-4.2246647 -4.2803259 -4.3623247 -4.41503 -4.4088564 -4.35757 -4.2920928 -4.27305 -4.3536367 -4.491189 -4.5938535 -4.6199179 -4.590384 -4.52007 -4.4171782][-4.5599356 -4.5724983 -4.6054859 -4.5557103 -4.3648977 -4.0755377 -3.7869515 -3.6349444 -3.7191861 -3.9919629 -4.2872081 -4.4829254 -4.5766635 -4.5807128 -4.4972982][-5.0333624 -5.027844 -4.9723444 -4.70536 -4.1579871 -3.439296 -2.7685304 -2.4025471 -2.4870336 -2.9615793 -3.5644615 -4.0478592 -4.3481359 -4.4793062 -4.4545507][-5.2781754 -5.3388505 -5.2658048 -4.8206353 -3.92468 -2.7544427 -1.6654963 -1.0591416 -1.1283884 -1.789866 -2.706383 -3.5003924 -4.0169158 -4.2714868 -4.3117652][-5.0326605 -5.2397041 -5.3012733 -4.8904352 -3.8826985 -2.4733124 -1.1212509 -0.34555721 -0.37220621 -1.107784 -2.1901546 -3.1670346 -3.8032992 -4.1088157 -4.1733737][-4.4341846 -4.8256874 -5.1386728 -4.9868517 -4.1823082 -2.8764315 -1.5436759 -0.72942257 -0.67289972 -1.3016677 -2.2914326 -3.2141886 -3.8142076 -4.0834579 -4.127399][-3.7021008 -4.263268 -4.8553758 -5.0701189 -4.6672878 -3.7234666 -2.6604967 -1.9440086 -1.8038177 -2.2159905 -2.9287996 -3.6069629 -4.037745 -4.2064128 -4.2027297][-3.0182767 -3.6602955 -4.4335961 -4.9532285 -4.9675426 -4.4893317 -3.8516874 -3.3638647 -3.2087913 -3.4182916 -3.8340876 -4.2303171 -4.4603171 -4.5136967 -4.4509535][-2.5187373 -3.1460357 -3.9378002 -4.5800295 -4.8745089 -4.8219786 -4.6515975 -4.4811845 -4.4004459 -4.4840922 -4.6757097 -4.8532796 -4.9410129 -4.9219656 -4.8186679][-2.2408137 -2.8089819 -3.4933627 -4.0834751 -4.4839053 -4.7165194 -4.9071846 -5.0089197 -5.0093603 -5.0028834 -5.0319958 -5.0698428 -5.1023808 -5.0869513 -4.9881721][-2.1106169 -2.664021 -3.25694 -3.7507508 -4.1367445 -4.4688749 -4.7873149 -4.9680996 -4.9400997 -4.810926 -4.6979246 -4.6506829 -4.7050853 -4.774509 -4.7345967][-2.230119 -2.8355691 -3.4010396 -3.8221934 -4.1293216 -4.3883491 -4.5879431 -4.6227236 -4.4186692 -4.0969419 -3.8229187 -3.7118068 -3.8392088 -4.0559716 -4.1180062][-2.5539658 -3.2626143 -3.8582067 -4.2438784 -4.4493327 -4.5231614 -4.4546442 -4.2214937 -3.7794895 -3.2442102 -2.8012815 -2.6231847 -2.8331637 -3.217175 -3.4024658][-2.9481163 -3.7173979 -4.3284135 -4.6906219 -4.8095431 -4.7023954 -4.3826985 -3.924401 -3.3182287 -2.6508818 -2.0967972 -1.8686965 -2.1293828 -2.6226058 -2.8980699]]...]
INFO - root - 2017-12-06 08:40:24.402870: step 13010, loss = 0.65, batch loss = 0.58 (12.6 examples/sec; 0.634 sec/batch; 56h:17m:34s remains)
INFO - root - 2017-12-06 08:40:30.471524: step 13020, loss = 0.96, batch loss = 0.89 (13.0 examples/sec; 0.614 sec/batch; 54h:28m:20s remains)
INFO - root - 2017-12-06 08:40:36.585922: step 13030, loss = 0.87, batch loss = 0.80 (13.2 examples/sec; 0.607 sec/batch; 53h:52m:42s remains)
INFO - root - 2017-12-06 08:40:42.497045: step 13040, loss = 1.07, batch loss = 1.00 (12.7 examples/sec; 0.631 sec/batch; 55h:59m:12s remains)
INFO - root - 2017-12-06 08:40:48.616652: step 13050, loss = 0.92, batch loss = 0.85 (13.0 examples/sec; 0.616 sec/batch; 54h:37m:34s remains)
INFO - root - 2017-12-06 08:40:54.741669: step 13060, loss = 0.73, batch loss = 0.66 (12.5 examples/sec; 0.638 sec/batch; 56h:34m:41s remains)
INFO - root - 2017-12-06 08:41:00.765423: step 13070, loss = 0.93, batch loss = 0.86 (13.5 examples/sec; 0.593 sec/batch; 52h:34m:49s remains)
INFO - root - 2017-12-06 08:41:06.859078: step 13080, loss = 0.86, batch loss = 0.79 (12.9 examples/sec; 0.620 sec/batch; 55h:00m:27s remains)
INFO - root - 2017-12-06 08:41:13.003640: step 13090, loss = 0.71, batch loss = 0.64 (13.2 examples/sec; 0.604 sec/batch; 53h:35m:42s remains)
INFO - root - 2017-12-06 08:41:19.073179: step 13100, loss = 1.07, batch loss = 1.00 (13.2 examples/sec; 0.608 sec/batch; 53h:56m:26s remains)
2017-12-06 08:41:19.578167: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3598843 -5.4181042 -5.5198221 -5.59883 -5.6074934 -5.5469427 -5.4353123 -5.331584 -5.275928 -5.2930479 -5.3619347 -5.432538 -5.4650598 -5.4001589 -5.1802468][-5.6987581 -5.9114347 -6.119678 -6.2052965 -6.1013684 -5.8747263 -5.6337967 -5.50541 -5.533885 -5.6987042 -5.873044 -5.9497862 -5.9213486 -5.7430305 -5.4072909][-5.86959 -6.297369 -6.6446 -6.7091761 -6.3965788 -5.8891759 -5.4276667 -5.2457743 -5.3989706 -5.81318 -6.2047615 -6.3438644 -6.2920952 -6.0284233 -5.6036739][-5.7783008 -6.4046717 -6.8779325 -6.8988004 -6.3309531 -5.4841876 -4.740561 -4.4627738 -4.7253361 -5.4470282 -6.1724772 -6.4604011 -6.4550643 -6.1651182 -5.6847968][-5.4369521 -6.1548758 -6.67391 -6.6113772 -5.7898626 -4.6318188 -3.6277118 -3.2490363 -3.5839019 -4.602911 -5.7075739 -6.2000055 -6.2940855 -6.0438929 -5.5536528][-5.0222964 -5.6607056 -6.0672026 -5.8404932 -4.8271217 -3.5035055 -2.3975463 -2.0188503 -2.4243279 -3.6595695 -5.0506983 -5.7175303 -5.9060688 -5.7187238 -5.2499075][-4.7172422 -5.12614 -5.2464104 -4.7659903 -3.6386442 -2.3283646 -1.3276243 -1.0763195 -1.5606544 -2.8801062 -4.4052687 -5.2230511 -5.4926677 -5.35643 -4.9212017][-4.6854529 -4.8413982 -4.6170845 -3.8597188 -2.6762109 -1.4772086 -0.69932723 -0.62566352 -1.1890731 -2.470705 -3.9781294 -4.9128566 -5.2189436 -5.0670834 -4.6437078][-4.9370456 -4.9419708 -4.4708338 -3.5425735 -2.3910677 -1.3526208 -0.82395911 -0.90655375 -1.5192127 -2.660213 -3.9804618 -4.8802171 -5.0705266 -4.7874556 -4.3500328][-5.3472438 -5.3330855 -4.767024 -3.8099935 -2.7786677 -1.9375355 -1.6309159 -1.7821071 -2.3567195 -3.2710323 -4.2503138 -4.8978333 -4.8265076 -4.366662 -3.9502151][-5.745913 -5.8293209 -5.3313007 -4.4696045 -3.6041136 -2.9503202 -2.7787826 -2.8955584 -3.3389831 -3.9981387 -4.5819979 -4.8521762 -4.4830093 -3.8809209 -3.5068312][-6.0650892 -6.35313 -6.0803905 -5.4259224 -4.7306852 -4.2097778 -4.0643368 -4.08226 -4.3579683 -4.7877159 -5.0297174 -4.9263797 -4.304667 -3.5839942 -3.1788578][-6.2757959 -6.7954454 -6.8047442 -6.389256 -5.8288116 -5.3741879 -5.2078867 -5.1829805 -5.3799133 -5.6651998 -5.6482096 -5.2285047 -4.4075885 -3.5554311 -3.0354874][-6.3549261 -7.0557003 -7.2867737 -7.0210986 -6.4830647 -5.9852657 -5.756536 -5.751946 -5.9845638 -6.2540874 -6.1064463 -5.490159 -4.5502949 -3.6064129 -2.9875197][-6.2493858 -7.0437269 -7.384326 -7.141902 -6.5176005 -5.885129 -5.5480766 -5.5679941 -5.9075117 -6.3105221 -6.1973505 -5.5250645 -4.5472069 -3.5687976 -2.918324]]...]
INFO - root - 2017-12-06 08:41:25.602674: step 13110, loss = 0.86, batch loss = 0.79 (13.3 examples/sec; 0.602 sec/batch; 53h:25m:00s remains)
INFO - root - 2017-12-06 08:41:31.707065: step 13120, loss = 1.17, batch loss = 1.10 (12.7 examples/sec; 0.630 sec/batch; 55h:55m:03s remains)
INFO - root - 2017-12-06 08:41:37.851080: step 13130, loss = 0.74, batch loss = 0.67 (13.3 examples/sec; 0.602 sec/batch; 53h:25m:31s remains)
INFO - root - 2017-12-06 08:41:44.017521: step 13140, loss = 1.26, batch loss = 1.19 (13.3 examples/sec; 0.599 sec/batch; 53h:10m:42s remains)
INFO - root - 2017-12-06 08:41:49.979425: step 13150, loss = 1.01, batch loss = 0.94 (13.4 examples/sec; 0.598 sec/batch; 53h:00m:13s remains)
INFO - root - 2017-12-06 08:41:56.123425: step 13160, loss = 0.97, batch loss = 0.90 (13.4 examples/sec; 0.599 sec/batch; 53h:07m:45s remains)
INFO - root - 2017-12-06 08:42:02.253720: step 13170, loss = 1.13, batch loss = 1.06 (13.0 examples/sec; 0.617 sec/batch; 54h:41m:51s remains)
INFO - root - 2017-12-06 08:42:08.361207: step 13180, loss = 1.16, batch loss = 1.09 (13.1 examples/sec; 0.609 sec/batch; 54h:01m:36s remains)
INFO - root - 2017-12-06 08:42:14.326791: step 13190, loss = 0.80, batch loss = 0.73 (13.7 examples/sec; 0.584 sec/batch; 51h:47m:26s remains)
INFO - root - 2017-12-06 08:42:20.418344: step 13200, loss = 1.08, batch loss = 1.01 (12.9 examples/sec; 0.620 sec/batch; 55h:01m:19s remains)
2017-12-06 08:42:20.954525: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4569845 -4.4892087 -4.4997239 -4.5793514 -4.7329435 -4.8824649 -4.9839635 -5.0294228 -5.1510124 -5.2781982 -5.3246965 -5.4199948 -5.4147458 -5.1752272 -4.8906918][-4.6695423 -4.6548872 -4.6070132 -4.6443038 -4.7299757 -4.7921038 -4.8743057 -4.9444747 -5.1438003 -5.3746624 -5.4913387 -5.6476135 -5.6445127 -5.3100462 -4.916791][-4.6788616 -4.5364184 -4.3642578 -4.3000875 -4.24621 -4.1906962 -4.303896 -4.4975109 -4.8024044 -5.109005 -5.2617254 -5.38652 -5.3613 -5.0182104 -4.6066308][-4.5673447 -4.2408667 -3.8766479 -3.6465189 -3.4173872 -3.2890916 -3.5080395 -3.91976 -4.3460207 -4.6632833 -4.8226409 -4.8607841 -4.7843428 -4.4858131 -4.1150479][-4.3566961 -3.80293 -3.2037485 -2.7489748 -2.3308191 -2.18947 -2.5487075 -3.2015314 -3.7902899 -4.1052976 -4.2381024 -4.1651988 -3.9736145 -3.6918354 -3.4364505][-4.1869316 -3.4475389 -2.6300402 -1.9485393 -1.3319073 -1.177072 -1.6385896 -2.4647336 -3.2233236 -3.6035647 -3.7513986 -3.6561453 -3.3541312 -3.006074 -2.8674791][-4.1293111 -3.3422551 -2.4433093 -1.6648567 -0.93799925 -0.75558424 -1.2348845 -2.0685582 -2.865253 -3.3042634 -3.4667227 -3.4475703 -3.1024837 -2.5829806 -2.4749227][-4.1735768 -3.5126519 -2.729784 -2.1055591 -1.5190787 -1.3464513 -1.7259963 -2.3498678 -2.9348471 -3.3183763 -3.4321928 -3.4858902 -3.1786113 -2.5318823 -2.4285688][-4.3984118 -3.9196362 -3.3328753 -2.9602485 -2.6458344 -2.5136957 -2.7255635 -3.0351315 -3.2974675 -3.6110806 -3.7006996 -3.8160095 -3.6404166 -3.0359502 -2.9317417][-4.7234416 -4.3110957 -3.7889619 -3.5244803 -3.4080622 -3.3467426 -3.421093 -3.5203304 -3.5925894 -3.9209602 -4.0646806 -4.2359357 -4.2444754 -3.8230097 -3.6980009][-5.0126557 -4.5649242 -3.9792325 -3.6191459 -3.518641 -3.55407 -3.5955431 -3.6927438 -3.7921295 -4.15579 -4.3942475 -4.6057558 -4.7669606 -4.4936428 -4.3348527][-5.150651 -4.6790185 -4.0294795 -3.5004113 -3.2527742 -3.3625462 -3.4819355 -3.7814145 -4.0875406 -4.473949 -4.8112941 -5.0669713 -5.2732806 -4.9778728 -4.7057924][-5.0479145 -4.6295948 -4.051744 -3.4484615 -3.0012457 -3.0972691 -3.3066053 -3.86067 -4.4221754 -4.7964759 -5.1639733 -5.431592 -5.5983381 -5.197372 -4.7840476][-4.7780423 -4.483737 -4.1259727 -3.636647 -3.1038046 -3.0917211 -3.2854385 -3.92882 -4.5951829 -4.900672 -5.2012897 -5.4215627 -5.4985938 -5.0612097 -4.59855][-4.413198 -4.2244782 -4.0958753 -3.828213 -3.3817847 -3.2756424 -3.3946719 -3.9378133 -4.5247531 -4.7312803 -4.9206448 -5.0642157 -5.0412636 -4.634912 -4.2121077]]...]
INFO - root - 2017-12-06 08:42:27.127946: step 13210, loss = 0.92, batch loss = 0.85 (12.5 examples/sec; 0.639 sec/batch; 56h:41m:22s remains)
INFO - root - 2017-12-06 08:42:33.204233: step 13220, loss = 1.20, batch loss = 1.13 (13.8 examples/sec; 0.581 sec/batch; 51h:33m:38s remains)
INFO - root - 2017-12-06 08:42:39.326931: step 13230, loss = 0.81, batch loss = 0.74 (13.1 examples/sec; 0.609 sec/batch; 54h:00m:29s remains)
INFO - root - 2017-12-06 08:42:45.424217: step 13240, loss = 0.85, batch loss = 0.78 (13.4 examples/sec; 0.597 sec/batch; 52h:54m:47s remains)
INFO - root - 2017-12-06 08:42:51.544536: step 13250, loss = 1.02, batch loss = 0.95 (13.3 examples/sec; 0.602 sec/batch; 53h:22m:28s remains)
INFO - root - 2017-12-06 08:42:57.487384: step 13260, loss = 0.92, batch loss = 0.85 (13.6 examples/sec; 0.588 sec/batch; 52h:09m:32s remains)
INFO - root - 2017-12-06 08:43:03.599367: step 13270, loss = 0.87, batch loss = 0.80 (12.7 examples/sec; 0.632 sec/batch; 56h:02m:22s remains)
INFO - root - 2017-12-06 08:43:09.723780: step 13280, loss = 0.96, batch loss = 0.89 (13.9 examples/sec; 0.575 sec/batch; 50h:57m:50s remains)
INFO - root - 2017-12-06 08:43:15.849549: step 13290, loss = 1.00, batch loss = 0.92 (12.3 examples/sec; 0.649 sec/batch; 57h:33m:09s remains)
INFO - root - 2017-12-06 08:43:21.940812: step 13300, loss = 0.93, batch loss = 0.86 (13.3 examples/sec; 0.603 sec/batch; 53h:28m:04s remains)
2017-12-06 08:43:22.486712: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8017738 -2.7910776 -3.117485 -3.6403089 -4.0481014 -4.1014776 -3.9845381 -3.8209181 -3.3826563 -2.5779884 -1.911962 -2.0264792 -2.8960991 -3.9619818 -4.8610082][-2.9010959 -3.03883 -3.485101 -4.0886769 -4.5251546 -4.4529405 -4.1164603 -3.7451134 -3.2299719 -2.5567808 -2.074842 -2.3074191 -3.1444461 -4.0077257 -4.7306137][-3.2524772 -3.5131979 -4.0115838 -4.5899816 -4.9595094 -4.6789551 -4.0827441 -3.5076563 -2.9374008 -2.4740191 -2.2723787 -2.6869023 -3.4918554 -4.0662451 -4.4954858][-3.5686245 -3.9311864 -4.4169316 -4.8678694 -5.0729079 -4.5655375 -3.7708712 -3.1149552 -2.597368 -2.4168139 -2.5397205 -3.1424336 -3.8978565 -4.1820803 -4.2977705][-3.7051725 -4.1176462 -4.5914307 -4.9061131 -4.9122972 -4.2085266 -3.3207536 -2.7061806 -2.3319585 -2.4591961 -2.9005492 -3.6409447 -4.3091307 -4.3496785 -4.24323][-3.9696867 -4.311398 -4.7057624 -4.85796 -4.6372094 -3.7687876 -2.8449011 -2.2994637 -2.0807719 -2.4819527 -3.1827617 -3.9710195 -4.5105796 -4.392808 -4.1901164][-4.1752496 -4.3483214 -4.5694785 -4.5286369 -4.1015015 -3.1363745 -2.2439637 -1.8148415 -1.7998469 -2.4695487 -3.3654428 -4.1245842 -4.5217443 -4.3182945 -4.0754709][-4.2154579 -4.2301178 -4.2347031 -3.9456632 -3.2844841 -2.2700381 -1.4939485 -1.3100612 -1.638921 -2.6472921 -3.7494285 -4.4750948 -4.7494335 -4.4943967 -4.2033668][-4.2791781 -4.1913791 -3.9925554 -3.4453177 -2.5997882 -1.6337559 -1.0701463 -1.2370431 -1.9780085 -3.2857885 -4.5458107 -5.2385254 -5.4226804 -5.1521029 -4.8090696][-4.5264254 -4.3799367 -4.03824 -3.3142419 -2.3761957 -1.5431771 -1.2236378 -1.7048514 -2.7590151 -4.1880522 -5.44745 -6.0413456 -6.1087542 -5.7775154 -5.3364][-4.9321737 -4.7384033 -4.3200922 -3.5269952 -2.5800359 -1.8729832 -1.6851852 -2.2979631 -3.4559488 -4.8279686 -5.9605103 -6.4098539 -6.3227606 -5.8721948 -5.3250022][-5.1266174 -4.875289 -4.4292154 -3.665138 -2.8166492 -2.2560432 -2.1349137 -2.7381701 -3.8522158 -5.0909419 -6.0719051 -6.427969 -6.2532687 -5.7241583 -5.1196203][-4.9864354 -4.7056522 -4.3053727 -3.6760528 -3.0370212 -2.6804068 -2.6683083 -3.2439725 -4.2384882 -5.2802382 -6.0618296 -6.3053284 -6.0396976 -5.4239311 -4.7656245][-4.8210993 -4.602942 -4.3297296 -3.8981817 -3.4817281 -3.288363 -3.3311861 -3.7837529 -4.5311542 -5.2872667 -5.8168168 -5.9067283 -5.5333033 -4.8473587 -4.1858397][-4.8130965 -4.668963 -4.5078831 -4.2252388 -3.9369459 -3.7944682 -3.7972515 -4.050241 -4.4929018 -4.9616895 -5.2735925 -5.2490072 -4.8412461 -4.1795793 -3.5969038]]...]
INFO - root - 2017-12-06 08:43:28.612422: step 13310, loss = 0.96, batch loss = 0.89 (13.0 examples/sec; 0.617 sec/batch; 54h:41m:34s remains)
INFO - root - 2017-12-06 08:43:34.660652: step 13320, loss = 1.04, batch loss = 0.97 (13.1 examples/sec; 0.613 sec/batch; 54h:20m:44s remains)
INFO - root - 2017-12-06 08:43:40.699851: step 13330, loss = 0.85, batch loss = 0.78 (13.3 examples/sec; 0.603 sec/batch; 53h:29m:21s remains)
INFO - root - 2017-12-06 08:43:46.690172: step 13340, loss = 0.88, batch loss = 0.81 (13.1 examples/sec; 0.612 sec/batch; 54h:17m:50s remains)
INFO - root - 2017-12-06 08:43:52.745037: step 13350, loss = 0.89, batch loss = 0.82 (13.1 examples/sec; 0.609 sec/batch; 53h:59m:31s remains)
INFO - root - 2017-12-06 08:43:58.727995: step 13360, loss = 1.19, batch loss = 1.12 (16.6 examples/sec; 0.482 sec/batch; 42h:42m:39s remains)
INFO - root - 2017-12-06 08:44:04.889359: step 13370, loss = 0.95, batch loss = 0.88 (12.7 examples/sec; 0.629 sec/batch; 55h:46m:49s remains)
INFO - root - 2017-12-06 08:44:11.050992: step 13380, loss = 1.22, batch loss = 1.15 (12.9 examples/sec; 0.620 sec/batch; 54h:59m:28s remains)
INFO - root - 2017-12-06 08:44:17.127224: step 13390, loss = 0.95, batch loss = 0.88 (13.1 examples/sec; 0.610 sec/batch; 54h:05m:04s remains)
INFO - root - 2017-12-06 08:44:23.275061: step 13400, loss = 0.66, batch loss = 0.59 (12.8 examples/sec; 0.623 sec/batch; 55h:14m:59s remains)
2017-12-06 08:44:23.834755: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287896 -4.9395423 -5.27612 -5.4404907 -5.4774532 -5.4475918 -5.43213 -5.4820285 -5.5709667 -5.6029105 -5.5195594 -5.3421159 -5.124548 -4.9250278 -4.7636275][-4.4953842 -5.1381617 -5.5521588 -5.7029705 -5.651711 -5.5204377 -5.4610248 -5.5690975 -5.7912636 -5.9339161 -5.8602219 -5.5780573 -5.1876431 -4.8436642 -4.6023765][-4.2881541 -5.0564075 -5.5016031 -5.5379891 -5.2841272 -4.9695082 -4.8499713 -5.0902753 -5.580183 -5.9731402 -6.0109296 -5.6681452 -5.0987091 -4.5896 -4.2664533][-3.8759708 -4.7126875 -5.07639 -4.83607 -4.2229147 -3.6143186 -3.4173415 -3.8897693 -4.8170156 -5.6352205 -5.9252405 -5.6165276 -4.9177909 -4.2631755 -3.8758526][-3.3611319 -4.17999 -4.3605103 -3.727448 -2.6552887 -1.6952605 -1.4481838 -2.2555437 -3.7328742 -5.0777111 -5.6967573 -5.4868612 -4.7165041 -3.9413741 -3.4959784][-3.0157809 -3.7625175 -3.7201178 -2.6755247 -1.1204205 0.21447134 0.50868654 -0.62744331 -2.6000552 -4.4249511 -5.3799477 -5.3193712 -4.5533662 -3.709779 -3.2192531][-3.0395665 -3.7089159 -3.4625959 -2.0736337 -0.096389771 1.5864768 1.9511657 0.60626411 -1.6654508 -3.8046949 -5.0342832 -5.1589193 -4.4977508 -3.6686192 -3.1486921][-3.4226718 -4.0619721 -3.7488945 -2.2902853 -0.21438694 1.5585618 1.9496851 0.63182592 -1.5640008 -3.6503162 -4.9094219 -5.136013 -4.6212578 -3.8842807 -3.3596182][-3.9625525 -4.6481757 -4.4868546 -3.3432093 -1.6284695 -0.12013626 0.25799513 -0.74795794 -2.4530156 -4.079041 -5.0662751 -5.2494216 -4.8762989 -4.3065476 -3.8449488][-4.276237 -5.0313835 -5.1350274 -4.5010533 -3.3893204 -2.3395004 -2.0193069 -2.6373763 -3.7348447 -4.7570868 -5.3389783 -5.3925304 -5.1116314 -4.7134662 -4.366931][-4.14404 -4.9396524 -5.2944875 -5.1444764 -4.6336188 -4.0715075 -3.8738708 -4.2058005 -4.8095541 -5.3065548 -5.4869347 -5.3579292 -5.086256 -4.8195968 -4.6246238][-3.6423545 -4.415163 -4.9441805 -5.1576467 -5.1174021 -4.956039 -4.8863258 -5.034935 -5.2980762 -5.4214869 -5.3050737 -5.0249963 -4.7391357 -4.5758147 -4.5433969][-3.0381923 -3.7457166 -4.3429236 -4.7366652 -4.9342413 -4.9812365 -4.9821053 -5.0443864 -5.1455164 -5.112483 -4.8920259 -4.5674396 -4.2856317 -4.1902194 -4.2760587][-2.7239332 -3.384325 -3.9787705 -4.3981404 -4.6208329 -4.6810603 -4.6593494 -4.6724181 -4.7328587 -4.7124906 -4.55082 -4.2863526 -4.0479708 -3.9928756 -4.120533][-2.9101462 -3.5319257 -4.0852613 -4.4525781 -4.6056404 -4.59352 -4.5089736 -4.4821815 -4.5364618 -4.5780768 -4.5316534 -4.3777738 -4.2096958 -4.1716008 -4.2636142]]...]
INFO - root - 2017-12-06 08:44:29.944927: step 13410, loss = 1.08, batch loss = 1.01 (13.8 examples/sec; 0.582 sec/batch; 51h:33m:07s remains)
INFO - root - 2017-12-06 08:44:36.108153: step 13420, loss = 1.05, batch loss = 0.98 (13.2 examples/sec; 0.608 sec/batch; 53h:51m:31s remains)
INFO - root - 2017-12-06 08:44:42.214721: step 13430, loss = 1.24, batch loss = 1.17 (13.2 examples/sec; 0.604 sec/batch; 53h:33m:48s remains)
INFO - root - 2017-12-06 08:44:48.247061: step 13440, loss = 0.80, batch loss = 0.73 (13.6 examples/sec; 0.587 sec/batch; 52h:03m:46s remains)
INFO - root - 2017-12-06 08:44:54.398825: step 13450, loss = 0.84, batch loss = 0.77 (13.1 examples/sec; 0.611 sec/batch; 54h:07m:59s remains)
INFO - root - 2017-12-06 08:45:00.529969: step 13460, loss = 0.64, batch loss = 0.57 (13.0 examples/sec; 0.614 sec/batch; 54h:23m:05s remains)
INFO - root - 2017-12-06 08:45:06.388727: step 13470, loss = 1.02, batch loss = 0.95 (13.0 examples/sec; 0.616 sec/batch; 54h:37m:31s remains)
INFO - root - 2017-12-06 08:45:12.497497: step 13480, loss = 1.04, batch loss = 0.97 (12.6 examples/sec; 0.635 sec/batch; 56h:16m:04s remains)
INFO - root - 2017-12-06 08:45:18.419760: step 13490, loss = 0.89, batch loss = 0.82 (13.2 examples/sec; 0.607 sec/batch; 53h:45m:57s remains)
INFO - root - 2017-12-06 08:45:24.537396: step 13500, loss = 1.00, batch loss = 0.93 (12.4 examples/sec; 0.643 sec/batch; 56h:58m:35s remains)
2017-12-06 08:45:25.046052: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7101207 -3.8658326 -3.96035 -4.0319052 -4.1166277 -4.1082845 -4.0035233 -3.8089631 -3.612438 -3.4523616 -3.4016676 -3.4184752 -3.504837 -3.7690535 -4.1098928][-3.7604158 -3.8935685 -3.9313097 -3.9209895 -3.9399164 -3.8830678 -3.7808752 -3.6089892 -3.4346848 -3.3015122 -3.2950046 -3.3702433 -3.52589 -3.8529723 -4.2318864][-3.8569503 -3.9389172 -3.9033115 -3.7999434 -3.7411413 -3.6308298 -3.5369186 -3.4185829 -3.3017278 -3.2416115 -3.3306484 -3.5122039 -3.7826211 -4.1841149 -4.5723958][-3.9903727 -4.0001097 -3.8868716 -3.6897216 -3.5288291 -3.34584 -3.2303872 -3.1647735 -3.1258807 -3.1783721 -3.4167328 -3.7563891 -4.1827903 -4.6770573 -5.0709362][-4.128829 -4.051548 -3.8560433 -3.5626745 -3.2768307 -2.9876242 -2.8067489 -2.7698588 -2.8156612 -2.9807901 -3.3674703 -3.8644538 -4.43257 -5.0134449 -5.426125][-4.2107816 -4.0681443 -3.8067508 -3.4350705 -3.0135167 -2.5719132 -2.2663918 -2.214067 -2.3595877 -2.6540802 -3.1687303 -3.7783411 -4.4303966 -5.0592837 -5.49252][-4.1998034 -4.0451517 -3.7658224 -3.3558798 -2.8157742 -2.1940627 -1.7124751 -1.5971568 -1.847332 -2.2801785 -2.87713 -3.5322638 -4.2060585 -4.8490911 -5.3037906][-4.0886459 -3.9621739 -3.7171731 -3.3168466 -2.7229242 -2.0045888 -1.4126704 -1.2358196 -1.554945 -2.0748055 -2.6660945 -3.280654 -3.9217563 -4.5443892 -5.0068889][-4.0569363 -3.9507024 -3.7340989 -3.3363881 -2.7400322 -2.0459282 -1.4763427 -1.3024206 -1.6416368 -2.1856024 -2.7314348 -3.2884696 -3.8935421 -4.4840889 -4.9280992][-4.2798862 -4.1447768 -3.8976135 -3.4618134 -2.8563385 -2.2272458 -1.7625134 -1.6532485 -2.00509 -2.5781069 -3.1211796 -3.6490231 -4.2316446 -4.79616 -5.1895642][-4.7078557 -4.5276623 -4.2379684 -3.76997 -3.1486859 -2.5720954 -2.2274256 -2.214036 -2.5845151 -3.1923764 -3.7651134 -4.26958 -4.8137536 -5.3260026 -5.630187][-5.1558781 -4.940546 -4.6366324 -4.192977 -3.6015997 -3.0825138 -2.8374481 -2.8931184 -3.2264457 -3.8155665 -4.3987665 -4.8531156 -5.317028 -5.7445707 -5.9475594][-5.3983488 -5.1736965 -4.8886991 -4.5277805 -4.031559 -3.5893056 -3.4075718 -3.4699175 -3.7022657 -4.2093515 -4.7555065 -5.1274724 -5.4822721 -5.807651 -5.9183626][-5.3520308 -5.1568413 -4.9183288 -4.6649904 -4.2877564 -3.9137506 -3.7576029 -3.7983434 -3.9291067 -4.3345385 -4.8158765 -5.0991735 -5.340086 -5.5548534 -5.5804558][-5.0058193 -4.8655524 -4.68866 -4.5396113 -4.278017 -3.9760458 -3.8457727 -3.8731718 -3.9373417 -4.2410827 -4.6396685 -4.8433275 -4.990027 -5.1045742 -5.0611825]]...]
INFO - root - 2017-12-06 08:45:31.117678: step 13510, loss = 0.83, batch loss = 0.76 (13.2 examples/sec; 0.607 sec/batch; 53h:47m:12s remains)
INFO - root - 2017-12-06 08:45:37.225709: step 13520, loss = 0.90, batch loss = 0.83 (13.1 examples/sec; 0.611 sec/batch; 54h:09m:05s remains)
INFO - root - 2017-12-06 08:45:43.321277: step 13530, loss = 0.81, batch loss = 0.73 (13.5 examples/sec; 0.594 sec/batch; 52h:36m:28s remains)
INFO - root - 2017-12-06 08:45:49.417818: step 13540, loss = 1.09, batch loss = 1.02 (13.0 examples/sec; 0.615 sec/batch; 54h:30m:49s remains)
INFO - root - 2017-12-06 08:45:55.555200: step 13550, loss = 0.91, batch loss = 0.84 (13.0 examples/sec; 0.616 sec/batch; 54h:37m:02s remains)
INFO - root - 2017-12-06 08:46:01.651148: step 13560, loss = 0.77, batch loss = 0.70 (13.1 examples/sec; 0.611 sec/batch; 54h:07m:12s remains)
INFO - root - 2017-12-06 08:46:07.745188: step 13570, loss = 0.91, batch loss = 0.84 (13.0 examples/sec; 0.618 sec/batch; 54h:43m:36s remains)
INFO - root - 2017-12-06 08:46:13.306803: step 13580, loss = 0.72, batch loss = 0.65 (12.8 examples/sec; 0.626 sec/batch; 55h:27m:59s remains)
INFO - root - 2017-12-06 08:46:19.420780: step 13590, loss = 0.86, batch loss = 0.79 (12.6 examples/sec; 0.633 sec/batch; 56h:03m:12s remains)
INFO - root - 2017-12-06 08:46:25.578433: step 13600, loss = 0.80, batch loss = 0.73 (13.2 examples/sec; 0.606 sec/batch; 53h:43m:08s remains)
2017-12-06 08:46:26.114333: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.693918 -2.2406175 -3.1375918 -4.1819658 -4.9582872 -5.3996444 -5.6498809 -5.474081 -4.85927 -4.0838051 -3.3818674 -2.5698183 -1.9236784 -1.5354462 -1.5603647][-1.5965607 -2.1414967 -2.9552903 -3.9598186 -4.7926745 -5.299705 -5.5905108 -5.4660721 -4.9228573 -4.2326508 -3.6028461 -2.8110285 -2.1153212 -1.7166185 -1.7926145][-2.3638244 -2.9320257 -3.5764048 -4.3099561 -4.9153004 -5.2792177 -5.5036654 -5.4594183 -5.1133671 -4.6403003 -4.1761408 -3.5202265 -2.9228888 -2.6364198 -2.8295083][-3.5246482 -4.0815687 -4.5153985 -4.8902678 -5.1208911 -5.2065282 -5.2696857 -5.2525854 -5.1259451 -4.9400043 -4.7118793 -4.2676191 -3.8822758 -3.789654 -4.0716238][-4.4682312 -4.8573623 -5.0061746 -5.0273428 -4.8939371 -4.7043929 -4.6102281 -4.5833936 -4.652616 -4.7893491 -4.9050541 -4.7580423 -4.6117687 -4.6956158 -4.93518][-4.9387941 -4.996851 -4.7849545 -4.4885459 -4.064517 -3.637568 -3.429512 -3.3871632 -3.6291406 -4.0951052 -4.6550903 -4.9278 -5.0313253 -5.2335048 -5.3218541][-5.0962548 -4.8110609 -4.2676306 -3.7329612 -3.1134462 -2.4858866 -2.1547756 -2.0678709 -2.4068391 -3.0836005 -4.006835 -4.7344131 -5.1184626 -5.43815 -5.407764][-5.093473 -4.6588011 -3.9836259 -3.3572469 -2.6732016 -1.9325669 -1.5035987 -1.3771155 -1.7373371 -2.4251037 -3.4489841 -4.4283252 -5.0115242 -5.4452271 -5.4381952][-5.01152 -4.6677485 -4.0880532 -3.509798 -2.8511546 -2.1172113 -1.7086785 -1.6642473 -2.0829892 -2.6783237 -3.5558453 -4.4710422 -5.0536389 -5.5135155 -5.6075826][-4.9209204 -4.7975335 -4.4414196 -3.9981303 -3.4276605 -2.8037989 -2.5362954 -2.6747456 -3.1774321 -3.661634 -4.2712746 -4.8859167 -5.2579741 -5.5964723 -5.7548475][-4.8749804 -4.9268 -4.7774682 -4.5178638 -4.1101332 -3.6697936 -3.5586767 -3.8221307 -4.3045006 -4.6290879 -4.9465957 -5.2042656 -5.3099442 -5.4659114 -5.64491][-4.8226714 -4.9546542 -4.9412718 -4.88652 -4.7022243 -4.4477067 -4.3984394 -4.6027441 -4.8689046 -4.9515228 -4.9831414 -4.9389987 -4.8504319 -4.8555822 -5.0674624][-4.68254 -4.8470416 -4.9425855 -5.0871491 -5.1290364 -5.0255661 -4.9340425 -4.8933311 -4.7767811 -4.5565376 -4.3175883 -4.0490651 -3.8565683 -3.7566819 -4.0092926][-4.394485 -4.575892 -4.7732759 -5.0780349 -5.2924695 -5.3105454 -5.1739879 -4.854537 -4.3307481 -3.813648 -3.3214922 -2.8477376 -2.5871377 -2.4237003 -2.6791506][-4.1015697 -4.24838 -4.4855623 -4.8631239 -5.151186 -5.2455244 -5.1196208 -4.6529918 -3.8912222 -3.1988206 -2.554852 -1.9364359 -1.6324537 -1.4503751 -1.6775672]]...]
INFO - root - 2017-12-06 08:46:32.148123: step 13610, loss = 0.78, batch loss = 0.71 (12.9 examples/sec; 0.621 sec/batch; 55h:01m:43s remains)
INFO - root - 2017-12-06 08:46:38.226947: step 13620, loss = 0.87, batch loss = 0.80 (13.5 examples/sec; 0.591 sec/batch; 52h:19m:30s remains)
INFO - root - 2017-12-06 08:46:44.268134: step 13630, loss = 0.80, batch loss = 0.73 (12.9 examples/sec; 0.618 sec/batch; 54h:44m:34s remains)
INFO - root - 2017-12-06 08:46:50.263194: step 13640, loss = 1.04, batch loss = 0.97 (13.0 examples/sec; 0.616 sec/batch; 54h:35m:45s remains)
INFO - root - 2017-12-06 08:46:56.381905: step 13650, loss = 0.79, batch loss = 0.72 (13.4 examples/sec; 0.597 sec/batch; 52h:50m:11s remains)
INFO - root - 2017-12-06 08:47:02.469712: step 13660, loss = 0.77, batch loss = 0.70 (12.8 examples/sec; 0.623 sec/batch; 55h:12m:32s remains)
INFO - root - 2017-12-06 08:47:08.540299: step 13670, loss = 1.03, batch loss = 0.96 (13.1 examples/sec; 0.609 sec/batch; 53h:54m:18s remains)
INFO - root - 2017-12-06 08:47:14.617836: step 13680, loss = 1.01, batch loss = 0.94 (12.7 examples/sec; 0.628 sec/batch; 55h:39m:06s remains)
INFO - root - 2017-12-06 08:47:20.560177: step 13690, loss = 1.10, batch loss = 1.03 (12.9 examples/sec; 0.622 sec/batch; 55h:04m:55s remains)
INFO - root - 2017-12-06 08:47:26.616959: step 13700, loss = 0.95, batch loss = 0.88 (13.2 examples/sec; 0.604 sec/batch; 53h:29m:45s remains)
2017-12-06 08:47:27.136628: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8985574 -1.7817042 -1.9983156 -2.3339653 -2.592063 -2.7227242 -2.8423738 -2.9560642 -3.1540275 -3.5618429 -4.2028146 -4.8135529 -4.9992867 -4.7446175 -4.3911715][-2.2795134 -2.3790181 -2.7479 -3.0885758 -3.1478529 -2.9535427 -2.758461 -2.6607175 -2.7795758 -3.1181345 -3.658083 -4.2463641 -4.5482597 -4.5279074 -4.4282556][-2.9109097 -3.1540265 -3.544364 -3.7616396 -3.5717154 -3.04987 -2.4912465 -2.1331761 -2.1623957 -2.447979 -2.8654828 -3.3740489 -3.7749574 -4.0549812 -4.2776694][-3.4062319 -3.6868229 -3.9971297 -4.0213966 -3.5817811 -2.7707825 -1.8822248 -1.3085456 -1.3515491 -1.7131352 -2.1193156 -2.5814908 -3.0489011 -3.5873184 -4.0718336][-3.526953 -3.7951484 -4.0234566 -3.8625767 -3.1522565 -2.0027077 -0.8004806 -0.14334011 -0.42757988 -1.0984802 -1.6733665 -2.1788864 -2.6856666 -3.3625758 -3.9513187][-3.4967117 -3.7491472 -3.8929081 -3.5342121 -2.530817 -1.0059388 0.48239088 1.0911884 0.34987164 -0.8310802 -1.6824796 -2.2347746 -2.6861467 -3.3239574 -3.8588884][-3.4770496 -3.7138004 -3.7281299 -3.152792 -1.9233844 -0.16101599 1.5241117 2.0428848 0.82213354 -0.8844552 -2.0340326 -2.5758214 -2.8405828 -3.2817268 -3.6515369][-3.5931137 -3.8096864 -3.6759367 -2.937366 -1.6767232 0.0011873245 1.6207485 2.0584207 0.6370821 -1.28075 -2.5479269 -2.9932945 -3.0298095 -3.2478461 -3.470737][-3.8617997 -3.9816422 -3.7475553 -3.0243306 -1.9768319 -0.7065475 0.54266691 0.89699221 -0.26246548 -1.8699827 -2.9296474 -3.2087481 -3.1025577 -3.1980205 -3.4062567][-3.990562 -3.9825554 -3.751539 -3.2055016 -2.4674926 -1.6498234 -0.84522176 -0.60097766 -1.3641477 -2.4294229 -3.1009607 -3.1803107 -2.9958935 -3.0794618 -3.4162769][-3.9154537 -3.816407 -3.646138 -3.3138366 -2.838789 -2.3426905 -1.8970475 -1.7897933 -2.2602136 -2.877373 -3.2285018 -3.1846161 -2.9714718 -3.0339646 -3.4107292][-3.7209625 -3.5591593 -3.415678 -3.2298584 -2.9499121 -2.6880982 -2.504775 -2.5151672 -2.8030334 -3.1656768 -3.4204552 -3.4290638 -3.3013284 -3.3491702 -3.5976274][-3.4731236 -3.2841361 -3.0949559 -2.9184003 -2.7376943 -2.6508729 -2.7073565 -2.8621955 -3.1095891 -3.4218326 -3.7416928 -3.8790987 -3.8667254 -3.909236 -3.9889035][-3.2862139 -3.0700622 -2.787128 -2.5535955 -2.4362531 -2.4955983 -2.7680016 -3.0835342 -3.3544838 -3.6611326 -4.017756 -4.2158394 -4.2661152 -4.2875628 -4.226665][-3.1183233 -2.9078174 -2.6464224 -2.4936042 -2.5122771 -2.6619341 -2.9972124 -3.3585653 -3.6028466 -3.8365064 -4.1243834 -4.2703061 -4.3210306 -4.3088527 -4.1419134]]...]
INFO - root - 2017-12-06 08:47:33.249633: step 13710, loss = 0.79, batch loss = 0.72 (13.9 examples/sec; 0.577 sec/batch; 51h:06m:16s remains)
INFO - root - 2017-12-06 08:47:39.334377: step 13720, loss = 0.83, batch loss = 0.76 (13.6 examples/sec; 0.589 sec/batch; 52h:11m:42s remains)
INFO - root - 2017-12-06 08:47:45.396570: step 13730, loss = 1.05, batch loss = 0.98 (13.3 examples/sec; 0.602 sec/batch; 53h:17m:21s remains)
INFO - root - 2017-12-06 08:47:51.448582: step 13740, loss = 0.89, batch loss = 0.82 (13.5 examples/sec; 0.592 sec/batch; 52h:25m:00s remains)
INFO - root - 2017-12-06 08:47:57.580651: step 13750, loss = 0.74, batch loss = 0.67 (12.8 examples/sec; 0.624 sec/batch; 55h:12m:31s remains)
INFO - root - 2017-12-06 08:48:03.678951: step 13760, loss = 0.81, batch loss = 0.74 (13.4 examples/sec; 0.597 sec/batch; 52h:50m:49s remains)
INFO - root - 2017-12-06 08:48:09.807807: step 13770, loss = 0.74, batch loss = 0.67 (13.3 examples/sec; 0.603 sec/batch; 53h:23m:03s remains)
INFO - root - 2017-12-06 08:48:15.889674: step 13780, loss = 0.82, batch loss = 0.75 (13.1 examples/sec; 0.613 sec/batch; 54h:14m:04s remains)
INFO - root - 2017-12-06 08:48:21.736499: step 13790, loss = 0.85, batch loss = 0.78 (16.8 examples/sec; 0.476 sec/batch; 42h:08m:46s remains)
INFO - root - 2017-12-06 08:48:27.862112: step 13800, loss = 0.92, batch loss = 0.85 (13.2 examples/sec; 0.606 sec/batch; 53h:41m:05s remains)
2017-12-06 08:48:28.399366: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3929486 -4.0959988 -3.7251692 -3.4706526 -3.4838343 -3.6104777 -4.0108547 -4.5279231 -4.8409524 -4.8622074 -4.69949 -4.528192 -4.2929082 -3.902317 -3.6122134][-4.4276671 -4.0945125 -3.7690215 -3.5996304 -3.6331117 -3.6538498 -3.8837569 -4.3007312 -4.5618768 -4.5724568 -4.4300442 -4.3292909 -4.2228723 -3.9883332 -3.7739394][-4.4393392 -4.1011848 -3.8821514 -3.811837 -3.7986836 -3.68635 -3.7532825 -4.0287681 -4.1919923 -4.213171 -4.1711178 -4.2216859 -4.2844877 -4.2063885 -4.01609][-4.4123487 -4.0774169 -3.9682531 -3.9907367 -3.8935308 -3.6405253 -3.5797315 -3.7293315 -3.8292389 -3.9277661 -4.0340223 -4.229115 -4.4146404 -4.4599915 -4.2657681][-4.2952194 -3.9649944 -3.9371629 -4.0425014 -3.8814888 -3.5018811 -3.32997 -3.3831992 -3.5008082 -3.7118931 -3.9394395 -4.1928725 -4.4063983 -4.5315013 -4.3885365][-4.0925255 -3.8040702 -3.8510537 -4.021451 -3.8141713 -3.3239379 -3.03752 -3.0108519 -3.1795387 -3.4873054 -3.7759285 -4.0353971 -4.2419639 -4.4178495 -4.3773746][-3.917908 -3.705915 -3.826808 -4.0345325 -3.8077879 -3.2577691 -2.8669214 -2.7744098 -2.9820771 -3.3360643 -3.633369 -3.8784566 -4.0685172 -4.2777824 -4.34584][-3.8710248 -3.7271514 -3.9029322 -4.1334004 -3.9291956 -3.3662732 -2.9045382 -2.7688046 -3.0057349 -3.3778129 -3.658174 -3.851052 -3.9945805 -4.1969838 -4.329556][-3.9512475 -3.836607 -4.0439425 -4.293551 -4.1303325 -3.5681486 -3.0591612 -2.8952012 -3.1372681 -3.5108829 -3.7726645 -3.9109364 -4.0109472 -4.1830773 -4.3337445][-4.1027985 -4.0018659 -4.2182136 -4.4683332 -4.3280282 -3.7754705 -3.274909 -3.1128049 -3.3246946 -3.674221 -3.9155395 -4.0201182 -4.1002336 -4.2263565 -4.3328094][-4.2561722 -4.2301183 -4.4420962 -4.6376624 -4.4722543 -3.9591229 -3.5468438 -3.4185269 -3.5725827 -3.8678608 -4.0745945 -4.1603017 -4.2207007 -4.2696328 -4.2859445][-4.2905908 -4.4176068 -4.6465478 -4.7552633 -4.5181704 -4.0623484 -3.7842746 -3.7249103 -3.8377867 -4.0600319 -4.2021222 -4.2632217 -4.3017492 -4.2738566 -4.2013736][-4.1957507 -4.4814892 -4.7329087 -4.7601428 -4.44741 -4.0723782 -3.9562671 -4.0006986 -4.1113911 -4.2377615 -4.2730823 -4.2828217 -4.29007 -4.2110319 -4.0840659][-4.0399108 -4.4195762 -4.6842818 -4.6589203 -4.3151088 -4.0534949 -4.0886855 -4.235445 -4.3500967 -4.3508129 -4.25018 -4.1977911 -4.180572 -4.0933447 -3.964045][-3.9600828 -4.3486056 -4.5890427 -4.5040278 -4.1580729 -4.0087442 -4.1487966 -4.3559341 -4.4586692 -4.3594227 -4.1704364 -4.0918765 -4.0613375 -3.9810719 -3.8699141]]...]
INFO - root - 2017-12-06 08:48:34.467348: step 13810, loss = 0.96, batch loss = 0.89 (13.5 examples/sec; 0.595 sec/batch; 52h:39m:03s remains)
INFO - root - 2017-12-06 08:48:40.595827: step 13820, loss = 0.82, batch loss = 0.75 (12.8 examples/sec; 0.624 sec/batch; 55h:12m:58s remains)
INFO - root - 2017-12-06 08:48:46.631809: step 13830, loss = 0.83, batch loss = 0.76 (13.5 examples/sec; 0.592 sec/batch; 52h:25m:54s remains)
INFO - root - 2017-12-06 08:48:52.743919: step 13840, loss = 0.80, batch loss = 0.73 (13.6 examples/sec; 0.590 sec/batch; 52h:12m:54s remains)
INFO - root - 2017-12-06 08:48:58.835920: step 13850, loss = 0.87, batch loss = 0.80 (12.8 examples/sec; 0.627 sec/batch; 55h:29m:12s remains)
INFO - root - 2017-12-06 08:49:04.914038: step 13860, loss = 1.02, batch loss = 0.95 (13.5 examples/sec; 0.595 sec/batch; 52h:37m:15s remains)
INFO - root - 2017-12-06 08:49:11.014424: step 13870, loss = 1.02, batch loss = 0.95 (12.9 examples/sec; 0.619 sec/batch; 54h:48m:52s remains)
INFO - root - 2017-12-06 08:49:17.086888: step 13880, loss = 0.93, batch loss = 0.86 (13.9 examples/sec; 0.576 sec/batch; 50h:57m:14s remains)
INFO - root - 2017-12-06 08:49:23.132845: step 13890, loss = 0.91, batch loss = 0.84 (13.3 examples/sec; 0.603 sec/batch; 53h:20m:28s remains)
INFO - root - 2017-12-06 08:49:29.132470: step 13900, loss = 0.72, batch loss = 0.65 (12.7 examples/sec; 0.632 sec/batch; 55h:54m:47s remains)
2017-12-06 08:49:29.663225: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2090311 -5.2452669 -5.156641 -5.1543722 -5.290946 -5.5089817 -5.7514582 -5.8701372 -5.8244343 -5.6458387 -5.3750582 -5.1662655 -5.0775089 -5.0547075 -4.9655485][-5.0889974 -5.1220236 -5.0747852 -5.133975 -5.3390708 -5.6204267 -5.9647069 -6.21058 -6.2581382 -6.0988197 -5.7946324 -5.5505819 -5.3889823 -5.2635627 -5.0761328][-5.0706582 -5.0655971 -4.9292879 -4.8684812 -4.9957581 -5.29473 -5.7729383 -6.2320132 -6.5188327 -6.5457706 -6.3317137 -6.0834484 -5.8249049 -5.5712805 -5.2800894][-5.2495522 -5.1519041 -4.7892823 -4.3965693 -4.2027078 -4.315402 -4.7802815 -5.4383464 -6.1048265 -6.5668573 -6.6920438 -6.5883493 -6.2991428 -5.943141 -5.5651922][-5.4597135 -5.1973 -4.5189934 -3.6616607 -2.9295583 -2.5557237 -2.6933477 -3.3566496 -4.3779058 -5.4191117 -6.1513748 -6.4629455 -6.3702745 -6.0541039 -5.6751256][-5.7229505 -5.3165922 -4.379056 -3.1256816 -1.8463554 -0.81794214 -0.36481524 -0.74644041 -1.8887513 -3.3889856 -4.7619462 -5.6539063 -5.9791574 -5.911747 -5.6814108][-5.9314442 -5.4999828 -4.5202894 -3.1767311 -1.63047 -0.092219353 0.96391296 1.0483122 0.084701538 -1.5237634 -3.2362623 -4.5318956 -5.2424335 -5.5155358 -5.5640316][-5.9105597 -5.5470996 -4.76298 -3.6985736 -2.3555994 -0.77120447 0.57443333 1.0812516 0.5274291 -0.77134061 -2.3504694 -3.6446385 -4.4569879 -4.9198966 -5.1925535][-5.6125851 -5.3347206 -4.8432493 -4.2508574 -3.48728 -2.3554387 -1.1861427 -0.52571893 -0.65392995 -1.4072461 -2.476727 -3.4041729 -4.0160007 -4.4095149 -4.6934724][-5.1124 -4.8280644 -4.510819 -4.2946329 -4.1511655 -3.6835248 -3.0320225 -2.5596533 -2.4796357 -2.7647538 -3.25569 -3.7039373 -3.9971497 -4.1616244 -4.2830749][-4.8580136 -4.386282 -3.9303949 -3.7633774 -3.9741189 -4.056622 -4.0007253 -3.9159372 -3.8932946 -4.0182843 -4.172853 -4.2939472 -4.3382788 -4.2745194 -4.2008705][-5.0013309 -4.2636905 -3.4154558 -2.9848161 -3.2068205 -3.582999 -3.989167 -4.2524619 -4.37992 -4.5580034 -4.6443567 -4.6632032 -4.590271 -4.3717322 -4.1499949][-5.2877021 -4.3901873 -3.1758242 -2.4096568 -2.4369977 -2.8736932 -3.4951229 -3.9177279 -4.1324863 -4.3601031 -4.4506688 -4.4603448 -4.3589849 -4.0877576 -3.8163257][-5.5404162 -4.7000437 -3.3768156 -2.417438 -2.208909 -2.5339406 -3.1088483 -3.4815273 -3.6821969 -3.8897331 -3.9712937 -3.9875841 -3.9050982 -3.6640983 -3.4306438][-5.6207762 -5.04866 -3.9485307 -3.0661709 -2.7494259 -2.9353695 -3.3305187 -3.5468261 -3.6699982 -3.8029306 -3.8534346 -3.8652995 -3.8083472 -3.6405482 -3.4890909]]...]
INFO - root - 2017-12-06 08:49:35.781444: step 13910, loss = 0.96, batch loss = 0.89 (13.5 examples/sec; 0.594 sec/batch; 52h:35m:23s remains)
INFO - root - 2017-12-06 08:49:41.835016: step 13920, loss = 1.05, batch loss = 0.98 (12.9 examples/sec; 0.620 sec/batch; 54h:52m:47s remains)
INFO - root - 2017-12-06 08:49:47.994270: step 13930, loss = 0.80, batch loss = 0.73 (12.9 examples/sec; 0.618 sec/batch; 54h:40m:11s remains)
INFO - root - 2017-12-06 08:49:53.987337: step 13940, loss = 0.98, batch loss = 0.91 (13.2 examples/sec; 0.605 sec/batch; 53h:31m:12s remains)
INFO - root - 2017-12-06 08:50:00.149903: step 13950, loss = 0.74, batch loss = 0.67 (12.8 examples/sec; 0.627 sec/batch; 55h:28m:49s remains)
INFO - root - 2017-12-06 08:50:06.302677: step 13960, loss = 0.85, batch loss = 0.78 (12.8 examples/sec; 0.623 sec/batch; 55h:07m:45s remains)
INFO - root - 2017-12-06 08:50:12.363176: step 13970, loss = 0.96, batch loss = 0.89 (12.8 examples/sec; 0.626 sec/batch; 55h:22m:13s remains)
INFO - root - 2017-12-06 08:50:18.504139: step 13980, loss = 0.82, batch loss = 0.75 (12.5 examples/sec; 0.638 sec/batch; 56h:26m:32s remains)
INFO - root - 2017-12-06 08:50:24.552017: step 13990, loss = 1.04, batch loss = 0.97 (13.1 examples/sec; 0.610 sec/batch; 53h:56m:41s remains)
INFO - root - 2017-12-06 08:50:30.662410: step 14000, loss = 0.72, batch loss = 0.65 (12.9 examples/sec; 0.618 sec/batch; 54h:41m:03s remains)
2017-12-06 08:50:31.217750: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4835224 -4.2097592 -4.1700454 -4.2813959 -4.3326254 -4.5200782 -4.9405627 -5.2318349 -5.2409015 -4.7147708 -3.7555618 -2.9547191 -2.9112341 -3.5167227 -4.230648][-4.41064 -4.2496147 -4.394361 -4.6742549 -4.7649484 -4.8795929 -5.1778841 -5.3233738 -5.0767522 -4.1463923 -2.8110209 -1.9016225 -2.0523148 -2.9913363 -3.9766846][-4.3126082 -4.2802782 -4.5638547 -4.9164538 -4.9545302 -4.9314785 -5.0718207 -5.0536661 -4.5361977 -3.2619567 -1.7272141 -0.86851907 -1.2442741 -2.4333632 -3.6182749][-4.2631969 -4.3376904 -4.6837983 -5.0216 -4.9667044 -4.80037 -4.7592597 -4.554019 -3.81745 -2.3770628 -0.9114356 -0.30471611 -0.92231083 -2.2147141 -3.4573631][-4.4378304 -4.5675783 -4.8957796 -5.1418247 -4.9432831 -4.5837364 -4.2733178 -3.8010969 -2.9242983 -1.6260815 -0.60687041 -0.4817214 -1.3332434 -2.5720985 -3.6808186][-4.836266 -4.9395733 -5.1458664 -5.2079306 -4.8042436 -4.1789775 -3.4975533 -2.7284541 -1.9133713 -1.1484299 -0.90537214 -1.3481457 -2.3159585 -3.3385675 -4.1466575][-5.2617216 -5.2660575 -5.3003273 -5.1720934 -4.5635147 -3.6186686 -2.522553 -1.5300574 -0.978297 -0.95273304 -1.5128937 -2.3896208 -3.3251266 -4.0601807 -4.5294452][-5.4881496 -5.3750362 -5.2519169 -4.9941111 -4.2467351 -3.0241427 -1.5823967 -0.52955246 -0.39715528 -1.053256 -2.129555 -3.1619234 -3.9142857 -4.3559527 -4.5421915][-5.4957795 -5.3020926 -5.0643778 -4.7445297 -3.9500315 -2.6077352 -1.0880578 -0.23899031 -0.56189966 -1.5893567 -2.7605309 -3.6541183 -4.1259928 -4.352634 -4.3897052][-5.4915953 -5.3000808 -4.9932575 -4.6230531 -3.8393135 -2.5820193 -1.2900143 -0.79290652 -1.3895667 -2.4290547 -3.3903713 -4.01793 -4.229341 -4.3269296 -4.3120451][-5.5817537 -5.4359717 -5.0809817 -4.6556392 -3.9113846 -2.8879828 -1.9971819 -1.8490026 -2.5457163 -3.397562 -4.0402055 -4.3847208 -4.4198756 -4.4744897 -4.4517455][-5.6886129 -5.5796533 -5.2173672 -4.7731266 -4.1105 -3.3650694 -2.8856044 -3.012413 -3.6971443 -4.3073492 -4.6643391 -4.8262715 -4.813499 -4.8564243 -4.8064361][-5.625783 -5.5238376 -5.2090135 -4.8264551 -4.3089266 -3.8240476 -3.6583169 -3.9509661 -4.5548882 -4.9498448 -5.0957222 -5.1630688 -5.1589065 -5.1913977 -5.1195555][-5.3468075 -5.2241421 -4.973124 -4.6935611 -4.361074 -4.1118083 -4.1539159 -4.4940872 -4.9551563 -5.1814923 -5.2149825 -5.2483349 -5.2640018 -5.273931 -5.1875105][-5.0299797 -4.9100585 -4.7259312 -4.5509381 -4.388783 -4.3175244 -4.4451847 -4.71544 -5.0025067 -5.122611 -5.1343288 -5.1676488 -5.1893077 -5.1754785 -5.0805531]]...]
INFO - root - 2017-12-06 08:50:37.187706: step 14010, loss = 0.95, batch loss = 0.88 (13.3 examples/sec; 0.602 sec/batch; 53h:13m:49s remains)
INFO - root - 2017-12-06 08:50:43.247306: step 14020, loss = 1.29, batch loss = 1.22 (13.7 examples/sec; 0.585 sec/batch; 51h:46m:17s remains)
INFO - root - 2017-12-06 08:50:49.335011: step 14030, loss = 1.26, batch loss = 1.19 (13.1 examples/sec; 0.611 sec/batch; 54h:04m:39s remains)
INFO - root - 2017-12-06 08:50:55.442509: step 14040, loss = 0.85, batch loss = 0.78 (13.5 examples/sec; 0.594 sec/batch; 52h:31m:18s remains)
INFO - root - 2017-12-06 08:51:01.564561: step 14050, loss = 0.99, batch loss = 0.92 (13.1 examples/sec; 0.612 sec/batch; 54h:07m:47s remains)
INFO - root - 2017-12-06 08:51:07.671219: step 14060, loss = 0.81, batch loss = 0.74 (12.7 examples/sec; 0.632 sec/batch; 55h:53m:41s remains)
INFO - root - 2017-12-06 08:51:13.781275: step 14070, loss = 0.89, batch loss = 0.82 (13.6 examples/sec; 0.589 sec/batch; 52h:05m:23s remains)
INFO - root - 2017-12-06 08:51:19.896333: step 14080, loss = 0.97, batch loss = 0.90 (13.5 examples/sec; 0.592 sec/batch; 52h:23m:15s remains)
INFO - root - 2017-12-06 08:51:25.941310: step 14090, loss = 0.95, batch loss = 0.88 (13.9 examples/sec; 0.575 sec/batch; 50h:53m:14s remains)
INFO - root - 2017-12-06 08:51:32.049305: step 14100, loss = 0.74, batch loss = 0.67 (13.4 examples/sec; 0.599 sec/batch; 52h:57m:04s remains)
2017-12-06 08:51:32.591823: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0404425 -4.9908137 -4.8319054 -4.6819558 -4.5761671 -4.524178 -4.5628009 -4.6498857 -4.7095213 -4.7290835 -4.730906 -4.7093306 -4.6110229 -4.5240154 -4.4659843][-5.2767439 -5.2251983 -5.0359793 -4.8433976 -4.6772079 -4.544816 -4.5330439 -4.6586185 -4.7832327 -4.8542237 -4.868784 -4.8710613 -4.7463489 -4.6105647 -4.5028586][-5.3836584 -5.3356323 -5.1461115 -4.9334197 -4.707108 -4.4826021 -4.4145942 -4.594605 -4.7855005 -4.917593 -4.9401422 -4.982955 -4.894815 -4.7687902 -4.6453161][-5.306663 -5.2505417 -5.0628271 -4.8449259 -4.5802736 -4.2659225 -4.1249967 -4.3478928 -4.6084337 -4.816081 -4.8685403 -4.9749436 -4.9819632 -4.9055982 -4.7950473][-5.1687803 -5.0739708 -4.8708463 -4.6374111 -4.3180442 -3.8547394 -3.5190015 -3.6842885 -4.0065269 -4.3488712 -4.5349212 -4.7909961 -4.9331412 -4.9352093 -4.8716984][-5.0659704 -4.89097 -4.614491 -4.2943869 -3.8093574 -3.0492518 -2.3469081 -2.3990028 -2.9101334 -3.5132177 -3.9593534 -4.4329638 -4.7133007 -4.7971673 -4.7656736][-5.0570168 -4.7811766 -4.36693 -3.8704007 -3.1037292 -1.9307179 -0.7698791 -0.75510025 -1.6428933 -2.6510944 -3.4431171 -4.1297035 -4.5050287 -4.6290817 -4.5391369][-5.0896192 -4.7222714 -4.17293 -3.5576675 -2.6157427 -1.1889668 0.28444529 0.32152176 -0.92855692 -2.2582874 -3.2674901 -4.0646114 -4.4796615 -4.6002254 -4.3396444][-5.1388264 -4.65432 -4.0173397 -3.4695911 -2.7044687 -1.4834404 -0.094146729 -0.011825562 -1.2284186 -2.4995241 -3.4633195 -4.2189312 -4.6273608 -4.7198062 -4.2952452][-5.1182566 -4.4534593 -3.7405295 -3.3994946 -3.0663676 -2.3449523 -1.3117547 -1.1887209 -2.1087286 -3.0367346 -3.7490129 -4.3536925 -4.729228 -4.8374639 -4.3764071][-4.9969192 -4.1573515 -3.3715777 -3.2013144 -3.2885985 -3.1027226 -2.5220215 -2.4469957 -3.0758739 -3.621664 -3.9614787 -4.3048949 -4.6322784 -4.8229313 -4.4604244][-4.9053693 -3.9829216 -3.1472142 -3.0460868 -3.4033437 -3.6109793 -3.3915145 -3.3718312 -3.7834115 -4.0586834 -4.1007733 -4.1942549 -4.4394193 -4.69217 -4.4849868][-4.9055047 -4.0309229 -3.2541375 -3.2014394 -3.6719389 -4.0433555 -3.9989579 -3.9915948 -4.2745943 -4.4288483 -4.3180814 -4.247057 -4.3790636 -4.5893192 -4.4648795][-4.8304033 -4.0986071 -3.4714606 -3.4750657 -3.9328706 -4.28709 -4.2823167 -4.2641711 -4.474318 -4.609396 -4.5140381 -4.4098754 -4.435967 -4.5161977 -4.3844056][-4.6381869 -4.0849471 -3.6485577 -3.7014146 -4.0577307 -4.2933321 -4.2656279 -4.2298326 -4.3678741 -4.4827127 -4.4609542 -4.4197536 -4.428874 -4.4392667 -4.3158927]]...]
INFO - root - 2017-12-06 08:51:38.512056: step 14110, loss = 0.95, batch loss = 0.88 (15.3 examples/sec; 0.524 sec/batch; 46h:19m:19s remains)
INFO - root - 2017-12-06 08:51:44.508221: step 14120, loss = 1.01, batch loss = 0.94 (13.5 examples/sec; 0.591 sec/batch; 52h:15m:46s remains)
INFO - root - 2017-12-06 08:51:50.628606: step 14130, loss = 0.91, batch loss = 0.84 (13.3 examples/sec; 0.602 sec/batch; 53h:12m:05s remains)
INFO - root - 2017-12-06 08:51:56.753430: step 14140, loss = 0.79, batch loss = 0.72 (12.9 examples/sec; 0.618 sec/batch; 54h:38m:24s remains)
INFO - root - 2017-12-06 08:52:02.860273: step 14150, loss = 0.74, batch loss = 0.67 (13.5 examples/sec; 0.594 sec/batch; 52h:29m:38s remains)
INFO - root - 2017-12-06 08:52:08.890538: step 14160, loss = 0.84, batch loss = 0.77 (13.5 examples/sec; 0.594 sec/batch; 52h:28m:58s remains)
INFO - root - 2017-12-06 08:52:14.976844: step 14170, loss = 1.05, batch loss = 0.98 (13.0 examples/sec; 0.615 sec/batch; 54h:22m:36s remains)
INFO - root - 2017-12-06 08:52:20.987997: step 14180, loss = 0.98, batch loss = 0.91 (13.2 examples/sec; 0.604 sec/batch; 53h:24m:57s remains)
INFO - root - 2017-12-06 08:52:27.101482: step 14190, loss = 1.02, batch loss = 0.95 (13.4 examples/sec; 0.596 sec/batch; 52h:43m:12s remains)
INFO - root - 2017-12-06 08:52:33.186719: step 14200, loss = 1.19, batch loss = 1.12 (13.7 examples/sec; 0.584 sec/batch; 51h:39m:20s remains)
2017-12-06 08:52:33.713024: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0009661 -4.883738 -4.82151 -4.7490296 -4.72985 -4.8648686 -5.0851278 -5.2717056 -5.3341141 -5.3243046 -5.2970505 -5.1777687 -5.0245581 -4.8105245 -4.6573668][-4.8213296 -4.6524954 -4.6426663 -4.6576867 -4.7119102 -4.8996549 -5.1519065 -5.3079286 -5.3127089 -5.2620463 -5.1970811 -4.9661169 -4.6845207 -4.3846116 -4.2786345][-4.4980721 -4.3306341 -4.4324017 -4.5845141 -4.6873579 -4.8294086 -5.0024137 -5.0623221 -5.0233178 -5.0058169 -4.9760323 -4.6690888 -4.2256784 -3.7843642 -3.7073843][-4.1843123 -4.0424442 -4.2623773 -4.5349607 -4.6138673 -4.6047139 -4.5804749 -4.4830961 -4.453917 -4.5705657 -4.6653738 -4.3566966 -3.7600458 -3.1281419 -3.0064161][-4.0351272 -3.9261563 -4.22517 -4.53444 -4.4973731 -4.2743406 -3.9681487 -3.6823385 -3.7522244 -4.1080303 -4.41884 -4.2001395 -3.5094845 -2.7107022 -2.479501][-4.0700464 -3.998307 -4.3080697 -4.5160232 -4.2470703 -3.7484782 -3.1229119 -2.6846733 -2.988615 -3.6936319 -4.3211327 -4.2963815 -3.6074996 -2.7375679 -2.3762252][-4.1392241 -4.0870132 -4.3423448 -4.3672986 -3.7906816 -2.9553685 -1.9680412 -1.419812 -2.0733609 -3.2119858 -4.2139139 -4.4572282 -3.8683949 -3.0772891 -2.6725533][-4.1626043 -4.11437 -4.302177 -4.1665974 -3.353601 -2.2672238 -0.99394011 -0.38527822 -1.341548 -2.8110652 -4.0816784 -4.54846 -4.1101017 -3.5148375 -3.2037513][-4.1961026 -4.1397719 -4.2852545 -4.1156793 -3.3022888 -2.22951 -0.94181418 -0.38862371 -1.4375594 -2.9349484 -4.223938 -4.7679873 -4.432652 -3.992203 -3.7925568][-4.2656488 -4.1912422 -4.3180532 -4.2227669 -3.6166835 -2.7987361 -1.7771263 -1.4298196 -2.3593085 -3.5667911 -4.5934529 -5.0164151 -4.7090626 -4.3529129 -4.2325854][-4.4100752 -4.3134294 -4.3980083 -4.3435926 -3.9382012 -3.3962965 -2.7462654 -2.6896157 -3.4625463 -4.2929525 -4.9546208 -5.153698 -4.8356962 -4.5046821 -4.3979297][-4.5612717 -4.4191227 -4.4197664 -4.3168173 -3.9806037 -3.5902753 -3.2498794 -3.4753881 -4.1639819 -4.7270379 -5.1086407 -5.0964808 -4.7440481 -4.3927054 -4.2261329][-4.54395 -4.327549 -4.2450252 -4.0754256 -3.7261133 -3.3714752 -3.1957612 -3.5984325 -4.2840528 -4.7334533 -4.9666095 -4.828722 -4.45413 -4.0597429 -3.8011327][-4.4012723 -4.1095457 -3.9522018 -3.7380478 -3.4041185 -3.1066108 -3.0186138 -3.4619071 -4.13757 -4.5256948 -4.6938791 -4.5380888 -4.1670775 -3.7710347 -3.4866655][-4.2904649 -3.9637995 -3.72591 -3.4698732 -3.2055693 -3.052058 -3.0752635 -3.475539 -4.0569272 -4.3402424 -4.4287686 -4.2899694 -3.9456837 -3.5927548 -3.36881]]...]
INFO - root - 2017-12-06 08:52:39.810823: step 14210, loss = 1.07, batch loss = 1.00 (13.2 examples/sec; 0.605 sec/batch; 53h:32m:03s remains)
INFO - root - 2017-12-06 08:52:45.681593: step 14220, loss = 0.97, batch loss = 0.90 (13.5 examples/sec; 0.592 sec/batch; 52h:18m:09s remains)
INFO - root - 2017-12-06 08:52:51.762229: step 14230, loss = 1.35, batch loss = 1.28 (12.8 examples/sec; 0.625 sec/batch; 55h:13m:48s remains)
INFO - root - 2017-12-06 08:52:57.780822: step 14240, loss = 0.79, batch loss = 0.72 (14.4 examples/sec; 0.557 sec/batch; 49h:13m:34s remains)
INFO - root - 2017-12-06 08:53:03.810070: step 14250, loss = 0.87, batch loss = 0.80 (13.3 examples/sec; 0.600 sec/batch; 53h:02m:22s remains)
INFO - root - 2017-12-06 08:53:09.961956: step 14260, loss = 0.87, batch loss = 0.80 (13.2 examples/sec; 0.606 sec/batch; 53h:35m:29s remains)
INFO - root - 2017-12-06 08:53:16.086552: step 14270, loss = 0.91, batch loss = 0.84 (13.1 examples/sec; 0.612 sec/batch; 54h:03m:49s remains)
INFO - root - 2017-12-06 08:53:22.183741: step 14280, loss = 0.92, batch loss = 0.85 (13.3 examples/sec; 0.602 sec/batch; 53h:10m:51s remains)
INFO - root - 2017-12-06 08:53:28.313847: step 14290, loss = 1.29, batch loss = 1.22 (12.9 examples/sec; 0.618 sec/batch; 54h:38m:09s remains)
INFO - root - 2017-12-06 08:53:34.378941: step 14300, loss = 0.88, batch loss = 0.81 (13.1 examples/sec; 0.612 sec/batch; 54h:05m:30s remains)
2017-12-06 08:53:34.922714: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7619681 -4.7480068 -4.6961694 -4.69431 -4.6390548 -4.3580904 -4.042871 -4.0000029 -4.2951951 -4.55487 -4.5685625 -4.5855689 -4.5814853 -4.3516235 -3.9856732][-4.8696003 -4.9636407 -4.9440837 -4.9325037 -4.8145742 -4.4121633 -3.909246 -3.7565255 -4.1094394 -4.5608578 -4.7480106 -4.8743443 -4.9667559 -4.7331696 -4.2571239][-4.9236922 -5.0724649 -5.00888 -4.9171286 -4.7082863 -4.2062483 -3.5611203 -3.3251171 -3.7331197 -4.3621955 -4.7216587 -4.9767237 -5.2178774 -5.038969 -4.4894996][-5.0520978 -5.1654167 -4.9966016 -4.8142376 -4.5268917 -3.9475498 -3.1966524 -2.9167256 -3.40847 -4.2156596 -4.7131162 -5.0474706 -5.4258537 -5.3321066 -4.7287235][-4.9426847 -4.9736967 -4.7304587 -4.5146813 -4.203135 -3.5857615 -2.7598534 -2.457418 -3.063693 -4.0757942 -4.7453775 -5.1611338 -5.6451406 -5.6277256 -4.9626684][-4.5955763 -4.5149803 -4.2039185 -3.988121 -3.6754718 -3.0360594 -2.14477 -1.8230379 -2.5579352 -3.799968 -4.7207956 -5.2605138 -5.7694244 -5.7684054 -5.0693493][-4.2947283 -4.0687556 -3.6592689 -3.4151964 -3.0890346 -2.3918622 -1.4242649 -1.0743742 -1.9166727 -3.3596873 -4.5631266 -5.2998419 -5.8183937 -5.8114409 -5.1007524][-4.2367549 -3.8691065 -3.3746676 -3.1233673 -2.8066182 -2.0766869 -1.0841358 -0.72424293 -1.5763137 -3.060586 -4.4102941 -5.317832 -5.8473577 -5.84998 -5.1483707][-4.2756443 -3.8346334 -3.3380053 -3.1737666 -2.9896932 -2.3251677 -1.4405105 -1.1299572 -1.8720691 -3.2110944 -4.5187626 -5.468009 -5.9349008 -5.8811955 -5.1736126][-4.2129178 -3.7342925 -3.2941575 -3.2989156 -3.3377645 -2.778903 -2.0621328 -1.8726437 -2.5317569 -3.7262483 -4.9491019 -5.8463197 -6.1638694 -5.9556828 -5.1764984][-4.1996646 -3.7219989 -3.3441935 -3.4788303 -3.6805608 -3.2144313 -2.6229148 -2.5113869 -3.0549927 -4.0998554 -5.2473011 -6.0873117 -6.2800212 -5.9394412 -5.0900407][-4.3011003 -3.8576138 -3.5585423 -3.7784426 -4.0357566 -3.6199508 -3.0765367 -2.9297233 -3.2777138 -4.1010637 -5.1085591 -5.8379693 -5.9575281 -5.6066279 -4.8050919][-4.4858837 -4.0884008 -3.8815017 -4.1569705 -4.424552 -4.0751367 -3.5875232 -3.399734 -3.5792935 -4.1475668 -4.9195905 -5.45278 -5.4759688 -5.1419377 -4.4525857][-4.7346816 -4.405755 -4.2894058 -4.5933156 -4.8675656 -4.628828 -4.2232113 -4.0483646 -4.176156 -4.5658631 -5.0771656 -5.3677573 -5.2490568 -4.8693571 -4.2509508][-4.874177 -4.6247711 -4.5562296 -4.8219194 -5.0684352 -4.931633 -4.616487 -4.4857841 -4.6266637 -4.9174213 -5.2389131 -5.3686104 -5.1830444 -4.7656322 -4.1862836]]...]
INFO - root - 2017-12-06 08:53:41.078464: step 14310, loss = 1.16, batch loss = 1.09 (12.9 examples/sec; 0.620 sec/batch; 54h:46m:14s remains)
INFO - root - 2017-12-06 08:53:47.147704: step 14320, loss = 0.90, batch loss = 0.83 (13.2 examples/sec; 0.607 sec/batch; 53h:37m:01s remains)
INFO - root - 2017-12-06 08:53:53.082204: step 14330, loss = 1.08, batch loss = 1.01 (13.2 examples/sec; 0.605 sec/batch; 53h:30m:07s remains)
INFO - root - 2017-12-06 08:53:59.179044: step 14340, loss = 0.91, batch loss = 0.84 (13.3 examples/sec; 0.603 sec/batch; 53h:18m:16s remains)
INFO - root - 2017-12-06 08:54:05.235183: step 14350, loss = 0.95, batch loss = 0.88 (13.2 examples/sec; 0.606 sec/batch; 53h:32m:52s remains)
INFO - root - 2017-12-06 08:54:11.235125: step 14360, loss = 0.99, batch loss = 0.92 (13.3 examples/sec; 0.603 sec/batch; 53h:19m:00s remains)
INFO - root - 2017-12-06 08:54:17.380646: step 14370, loss = 0.73, batch loss = 0.66 (12.9 examples/sec; 0.619 sec/batch; 54h:42m:14s remains)
INFO - root - 2017-12-06 08:54:23.526095: step 14380, loss = 0.97, batch loss = 0.90 (13.3 examples/sec; 0.601 sec/batch; 53h:04m:08s remains)
INFO - root - 2017-12-06 08:54:29.609086: step 14390, loss = 0.91, batch loss = 0.84 (13.2 examples/sec; 0.607 sec/batch; 53h:39m:28s remains)
INFO - root - 2017-12-06 08:54:35.623344: step 14400, loss = 1.11, batch loss = 1.04 (12.8 examples/sec; 0.623 sec/batch; 55h:01m:13s remains)
2017-12-06 08:54:36.109611: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.253582 -5.7642708 -6.076252 -6.1839175 -5.9910603 -5.5068474 -5.0419984 -4.9301567 -5.0684509 -5.340672 -5.6284285 -5.8432617 -6.0097437 -6.0792389 -6.0186863][-5.4430981 -5.9442706 -6.2609587 -6.3785105 -6.11347 -5.4954424 -4.861608 -4.6687975 -4.8984804 -5.4026146 -5.9318571 -6.23485 -6.3856735 -6.4369607 -6.3497248][-5.5032244 -5.9376826 -6.2245746 -6.3451686 -6.0299854 -5.280118 -4.4608765 -4.1832924 -4.5771208 -5.4318318 -6.3083949 -6.7525368 -6.8745794 -6.8570547 -6.729888][-5.5181231 -5.824975 -6.0430784 -6.1632318 -5.8410535 -4.9951963 -3.9842505 -3.544486 -4.0364456 -5.24215 -6.5055919 -7.1478205 -7.2272005 -7.1098604 -6.94942][-5.5462337 -5.7532759 -5.9464674 -6.0809603 -5.7321072 -4.738306 -3.4531333 -2.7431455 -3.2487812 -4.7569833 -6.3865628 -7.2342911 -7.2544727 -7.0517645 -6.8898611][-5.5478196 -5.6790686 -5.8479643 -5.9330144 -5.4169793 -4.0840755 -2.3356493 -1.217386 -1.7033272 -3.5096066 -5.49683 -6.5912504 -6.5892081 -6.2782278 -6.0267591][-5.5013266 -5.500103 -5.5080667 -5.3699503 -4.5320048 -2.7590251 -0.47245145 1.1069131 0.65015507 -1.4138634 -3.7719021 -5.2616796 -5.4763618 -5.1903172 -4.82843][-5.550251 -5.4065495 -5.2224669 -4.8876715 -3.8787682 -1.9073231 0.70189095 2.6214852 2.2511396 0.1027379 -2.4841747 -4.3584247 -4.9687223 -4.8611326 -4.4582043][-5.7958159 -5.6789289 -5.5225358 -5.2663541 -4.4427805 -2.6883094 -0.2102685 1.6930704 1.441 -0.5091691 -2.9176691 -4.7619457 -5.5143805 -5.4772124 -5.0120907][-6.0733652 -6.1256485 -6.1881571 -6.2158866 -5.7689033 -4.4507389 -2.3643708 -0.69023871 -0.80222225 -2.390027 -4.3636584 -5.8887372 -6.5730457 -6.5108228 -5.9757609][-6.2057605 -6.3829622 -6.5956779 -6.837801 -6.7023344 -5.7567806 -4.0711293 -2.6649129 -2.6452525 -3.8819585 -5.4411869 -6.6617646 -7.2496724 -7.1053658 -6.4746227][-6.2020049 -6.3921218 -6.5474339 -6.7469082 -6.6746292 -5.9172335 -4.5719838 -3.457263 -3.3932467 -4.3447323 -5.5412474 -6.4903407 -6.9936008 -6.8544087 -6.2926521][-5.9901681 -6.096786 -6.0830879 -6.1085382 -5.9669938 -5.2949142 -4.2493639 -3.4600842 -3.4788654 -4.2724571 -5.1892681 -5.8821526 -6.2822986 -6.23081 -5.9008441][-5.6700044 -5.66607 -5.4788222 -5.314466 -5.0799665 -4.5057921 -3.7699718 -3.3161323 -3.4459183 -4.0875864 -4.7423096 -5.1904187 -5.4813681 -5.5470138 -5.4834375][-5.455276 -5.3929024 -5.0658622 -4.6915746 -4.32511 -3.8625472 -3.4365487 -3.2768836 -3.4659662 -3.9316671 -4.3884826 -4.7208982 -4.9948115 -5.1803017 -5.2903733]]...]
INFO - root - 2017-12-06 08:54:42.240522: step 14410, loss = 0.96, batch loss = 0.89 (12.7 examples/sec; 0.629 sec/batch; 55h:33m:04s remains)
INFO - root - 2017-12-06 08:54:48.368572: step 14420, loss = 1.10, batch loss = 1.03 (13.1 examples/sec; 0.608 sec/batch; 53h:45m:29s remains)
INFO - root - 2017-12-06 08:54:54.471479: step 14430, loss = 0.79, batch loss = 0.72 (13.1 examples/sec; 0.610 sec/batch; 53h:55m:06s remains)
INFO - root - 2017-12-06 08:55:00.520015: step 14440, loss = 0.96, batch loss = 0.89 (12.7 examples/sec; 0.628 sec/batch; 55h:31m:23s remains)
INFO - root - 2017-12-06 08:55:06.588033: step 14450, loss = 0.94, batch loss = 0.87 (13.5 examples/sec; 0.593 sec/batch; 52h:23m:40s remains)
INFO - root - 2017-12-06 08:55:12.762838: step 14460, loss = 0.90, batch loss = 0.83 (12.4 examples/sec; 0.648 sec/batch; 57h:12m:35s remains)
INFO - root - 2017-12-06 08:55:18.895643: step 14470, loss = 0.90, batch loss = 0.83 (12.8 examples/sec; 0.626 sec/batch; 55h:19m:28s remains)
INFO - root - 2017-12-06 08:55:24.944431: step 14480, loss = 0.88, batch loss = 0.81 (13.7 examples/sec; 0.583 sec/batch; 51h:32m:37s remains)
INFO - root - 2017-12-06 08:55:31.100578: step 14490, loss = 1.05, batch loss = 0.98 (12.9 examples/sec; 0.622 sec/batch; 54h:57m:34s remains)
INFO - root - 2017-12-06 08:55:37.156244: step 14500, loss = 1.23, batch loss = 1.16 (13.2 examples/sec; 0.608 sec/batch; 53h:40m:50s remains)
2017-12-06 08:55:37.695145: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8042288 -5.17929 -5.6383114 -6.0879984 -6.4078121 -6.424684 -6.4077258 -6.4692097 -6.5387688 -6.5378432 -6.4262671 -6.4534864 -6.6716528 -6.6814289 -6.4798079][-4.7654519 -5.1927576 -5.6781406 -6.0904484 -6.3426075 -6.2975707 -6.1952782 -6.2132626 -6.318913 -6.3869362 -6.3034935 -6.2446041 -6.3650851 -6.3462524 -6.19122][-4.9414935 -5.3382754 -5.7450562 -6.02445 -6.0945559 -5.87197 -5.5633941 -5.4547949 -5.575871 -5.7472396 -5.8009377 -5.8130794 -5.9464808 -5.9688005 -5.8936481][-5.2723589 -5.5396152 -5.7498536 -5.7925062 -5.5683413 -5.0506077 -4.4392905 -4.18594 -4.42462 -4.8412871 -5.1715841 -5.4037075 -5.6367841 -5.7115722 -5.666894][-5.4262629 -5.4543848 -5.3367324 -5.006608 -4.365387 -3.4815536 -2.5979166 -2.3274729 -2.9024658 -3.7931287 -4.558846 -5.0910168 -5.4226513 -5.4728932 -5.3821464][-5.1992073 -4.9635081 -4.5050116 -3.7770274 -2.6899276 -1.4468484 -0.36905527 -0.18554497 -1.2177215 -2.6894531 -3.9388976 -4.7773685 -5.2152219 -5.2580776 -5.1619072][-4.770133 -4.38158 -3.6876073 -2.634742 -1.1457779 0.41892862 1.591958 1.6332579 0.22953606 -1.6709602 -3.235353 -4.2644854 -4.7914143 -4.890789 -4.88507][-4.2620015 -3.8793559 -3.1070447 -1.8707025 -0.12388754 1.6113591 2.7119904 2.5594521 0.96258688 -1.1052706 -2.7518868 -3.7828727 -4.2729292 -4.3608127 -4.3779254][-3.7797856 -3.5435576 -2.8637538 -1.6542685 0.07247448 1.7050467 2.566134 2.2302079 0.67071104 -1.2738059 -2.7750735 -3.6342409 -3.9626155 -3.9243011 -3.8039761][-3.5124516 -3.486974 -3.0313404 -2.0754943 -0.68288159 0.58365536 1.131362 0.70928192 -0.59374642 -2.1623213 -3.3411548 -3.9227686 -3.9980021 -3.7347724 -3.3574598][-3.4875071 -3.6449409 -3.4597898 -2.8830321 -1.9989066 -1.2132623 -0.95406318 -1.3636231 -2.3058767 -3.3787565 -4.1380234 -4.4074807 -4.2298236 -3.7389846 -3.1363206][-3.6514275 -3.8895094 -3.9134951 -3.6813653 -3.2691376 -2.9100695 -2.8662958 -3.22107 -3.8273172 -4.4321895 -4.788259 -4.7962675 -4.466413 -3.8859198 -3.2324283][-3.9511473 -4.1666255 -4.284318 -4.2659273 -4.1485872 -4.044539 -4.0974154 -4.3508854 -4.6770444 -4.9217339 -4.98118 -4.8367782 -4.4984636 -4.0152421 -3.5164604][-4.1754413 -4.3115754 -4.4211965 -4.475378 -4.47403 -4.4631481 -4.5179954 -4.6564417 -4.7967582 -4.8542023 -4.7957563 -4.6395154 -4.4056292 -4.1217594 -3.8517923][-4.2573256 -4.3000817 -4.3435855 -4.3712964 -4.3753366 -4.3711166 -4.3972869 -4.4595189 -4.5164185 -4.5286708 -4.4866929 -4.409503 -4.3157663 -4.2151184 -4.1253433]]...]
INFO - root - 2017-12-06 08:55:43.773341: step 14510, loss = 0.77, batch loss = 0.70 (13.1 examples/sec; 0.612 sec/batch; 54h:01m:10s remains)
INFO - root - 2017-12-06 08:55:49.883348: step 14520, loss = 0.94, batch loss = 0.87 (13.1 examples/sec; 0.610 sec/batch; 53h:52m:18s remains)
INFO - root - 2017-12-06 08:55:56.008104: step 14530, loss = 0.95, batch loss = 0.88 (12.8 examples/sec; 0.625 sec/batch; 55h:12m:00s remains)
INFO - root - 2017-12-06 08:56:02.022433: step 14540, loss = 0.67, batch loss = 0.60 (13.1 examples/sec; 0.609 sec/batch; 53h:47m:11s remains)
INFO - root - 2017-12-06 08:56:08.001120: step 14550, loss = 1.09, batch loss = 1.02 (12.5 examples/sec; 0.638 sec/batch; 56h:19m:24s remains)
INFO - root - 2017-12-06 08:56:14.154837: step 14560, loss = 1.06, batch loss = 0.99 (13.0 examples/sec; 0.618 sec/batch; 54h:33m:26s remains)
INFO - root - 2017-12-06 08:56:20.300307: step 14570, loss = 1.01, batch loss = 0.94 (13.1 examples/sec; 0.609 sec/batch; 53h:49m:22s remains)
INFO - root - 2017-12-06 08:56:26.347369: step 14580, loss = 0.99, batch loss = 0.92 (13.4 examples/sec; 0.595 sec/batch; 52h:31m:41s remains)
INFO - root - 2017-12-06 08:56:32.553839: step 14590, loss = 0.77, batch loss = 0.70 (12.7 examples/sec; 0.632 sec/batch; 55h:46m:26s remains)
INFO - root - 2017-12-06 08:56:38.593596: step 14600, loss = 0.78, batch loss = 0.71 (13.4 examples/sec; 0.597 sec/batch; 52h:43m:02s remains)
2017-12-06 08:56:39.161404: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8180828 -4.9357986 -5.1453948 -4.9998846 -4.23491 -3.21005 -2.2757535 -1.9477601 -2.5969853 -3.5603991 -4.4348927 -5.048522 -5.3560786 -5.6753225 -6.2537384][-5.0689707 -5.2266436 -5.3905916 -5.1307349 -4.2929373 -3.2996533 -2.410737 -1.9952185 -2.4501014 -3.1493297 -3.665359 -4.0533733 -4.4013743 -4.9675589 -5.7923956][-5.0608788 -5.2533555 -5.3699503 -5.0373158 -4.1901031 -3.2408991 -2.3899705 -1.9575808 -2.2998192 -2.7566819 -2.8880415 -2.9793599 -3.2703242 -4.01175 -5.0404215][-4.8247056 -5.033062 -5.0941038 -4.6833005 -3.8068337 -2.8363838 -1.956553 -1.5941594 -2.0664055 -2.5204172 -2.4429674 -2.277786 -2.4159286 -3.1687651 -4.2716155][-4.4987307 -4.7061815 -4.7185941 -4.2165351 -3.2395995 -2.1311605 -1.1244042 -0.85308313 -1.6804841 -2.4420252 -2.44246 -2.1964574 -2.1772714 -2.7694206 -3.7471397][-4.3005877 -4.4716825 -4.418406 -3.8100724 -2.6711531 -1.3275456 -0.10376072 0.12864542 -1.1355948 -2.4094532 -2.7575827 -2.656033 -2.5612221 -2.9291649 -3.6610503][-4.3465095 -4.4573717 -4.3217688 -3.6206951 -2.3665226 -0.83443928 0.59989548 0.87654591 -0.6917522 -2.4426141 -3.2321928 -3.3974442 -3.3106303 -3.4890988 -3.9582496][-4.4509439 -4.5531392 -4.41066 -3.7005661 -2.4879243 -1.0090079 0.4003582 0.71394634 -0.84776878 -2.7644682 -3.844121 -4.2425246 -4.1768708 -4.1969252 -4.4122829][-4.2902164 -4.49565 -4.5087013 -3.9696341 -3.0132275 -1.84114 -0.68265057 -0.38650322 -1.6640813 -3.3605361 -4.4331322 -4.9058733 -4.8614478 -4.7939973 -4.82203][-3.701124 -4.1708264 -4.5450735 -4.3786335 -3.8379161 -3.0764418 -2.2266326 -1.9556561 -2.8224559 -4.0458894 -4.8625088 -5.2400255 -5.2119732 -5.139432 -5.0794125][-2.8750584 -3.7357068 -4.58802 -4.873138 -4.7704163 -4.3718729 -3.7739892 -3.5107532 -3.9909422 -4.7286086 -5.2343292 -5.4396143 -5.3810124 -5.3151917 -5.2292614][-2.1536069 -3.3657203 -4.6246624 -5.2632194 -5.4638653 -5.2879019 -4.8632121 -4.6212373 -4.8401365 -5.2220287 -5.4993424 -5.5888653 -5.5096664 -5.4336262 -5.3320513][-1.8309307 -3.1834493 -4.621562 -5.425797 -5.7302837 -5.6187663 -5.2870045 -5.0853014 -5.1805162 -5.372189 -5.521318 -5.5543222 -5.479147 -5.393857 -5.2858839][-2.1428747 -3.35738 -4.6520085 -5.3879042 -5.626822 -5.4777346 -5.17518 -5.00179 -5.0471759 -5.165185 -5.2644935 -5.2915115 -5.2430973 -5.1728673 -5.08311][-2.951694 -3.84328 -4.7627597 -5.265491 -5.3689094 -5.1821222 -4.9122276 -4.7615747 -4.7711058 -4.8415947 -4.9053884 -4.9332752 -4.9172935 -4.8914623 -4.8565145]]...]
INFO - root - 2017-12-06 08:56:45.313414: step 14610, loss = 0.76, batch loss = 0.69 (12.8 examples/sec; 0.623 sec/batch; 55h:00m:26s remains)
INFO - root - 2017-12-06 08:56:51.412862: step 14620, loss = 0.88, batch loss = 0.81 (13.1 examples/sec; 0.612 sec/batch; 54h:01m:11s remains)
INFO - root - 2017-12-06 08:56:57.513774: step 14630, loss = 1.20, batch loss = 1.13 (13.2 examples/sec; 0.604 sec/batch; 53h:18m:59s remains)
INFO - root - 2017-12-06 08:57:03.605004: step 14640, loss = 1.18, batch loss = 1.11 (13.2 examples/sec; 0.606 sec/batch; 53h:31m:48s remains)
INFO - root - 2017-12-06 08:57:09.537920: step 14650, loss = 0.93, batch loss = 0.86 (13.1 examples/sec; 0.609 sec/batch; 53h:46m:34s remains)
INFO - root - 2017-12-06 08:57:15.720012: step 14660, loss = 0.96, batch loss = 0.89 (12.6 examples/sec; 0.633 sec/batch; 55h:54m:37s remains)
INFO - root - 2017-12-06 08:57:21.735477: step 14670, loss = 0.82, batch loss = 0.75 (14.3 examples/sec; 0.561 sec/batch; 49h:30m:45s remains)
INFO - root - 2017-12-06 08:57:27.830131: step 14680, loss = 0.93, batch loss = 0.86 (13.0 examples/sec; 0.617 sec/batch; 54h:27m:58s remains)
INFO - root - 2017-12-06 08:57:33.959967: step 14690, loss = 0.96, batch loss = 0.89 (12.9 examples/sec; 0.621 sec/batch; 54h:49m:40s remains)
INFO - root - 2017-12-06 08:57:40.013927: step 14700, loss = 0.91, batch loss = 0.84 (13.7 examples/sec; 0.584 sec/batch; 51h:33m:32s remains)
2017-12-06 08:57:40.528290: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7439361 -3.9724624 -3.4665968 -3.210216 -3.215714 -3.7512121 -4.7303996 -5.5642996 -5.916502 -5.8170485 -5.423686 -4.8834386 -4.6592312 -4.6517754 -4.6826587][-4.1846595 -3.3197074 -2.7565379 -2.6120019 -2.7545462 -3.3779731 -4.4516687 -5.4063373 -5.7903862 -5.6893353 -5.3992662 -4.8983936 -4.568934 -4.493742 -4.5524316][-3.9158692 -3.0676575 -2.4931121 -2.4344883 -2.631685 -3.1949034 -4.2026715 -5.1441135 -5.4949331 -5.41655 -5.2857294 -4.8621931 -4.4152503 -4.244277 -4.3057237][-4.3685741 -3.6788683 -3.1481676 -3.0522466 -3.1056323 -3.3643506 -4.055397 -4.7740889 -5.0202174 -5.0079913 -5.1063519 -4.8249259 -4.3315868 -4.0454273 -3.9946561][-5.305325 -4.8443689 -4.3494821 -4.0903015 -3.8420048 -3.6107121 -3.7526751 -4.0648623 -4.2056446 -4.368257 -4.780292 -4.7376256 -4.3410583 -3.9719636 -3.7159081][-6.1858711 -5.8006778 -5.2070074 -4.7133961 -4.1077824 -3.333868 -2.8809633 -2.8280959 -2.9800825 -3.384737 -4.1049194 -4.3534064 -4.2084312 -3.9148307 -3.5518868][-6.6780849 -6.1704726 -5.38867 -4.6497765 -3.6577592 -2.3832107 -1.4780653 -1.2538507 -1.5476165 -2.2483411 -3.2638617 -3.8153799 -4.0139489 -3.9136143 -3.5887511][-6.7468109 -6.098846 -5.2240009 -4.3535795 -3.0716825 -1.4713528 -0.32993746 -0.061813354 -0.48541355 -1.4256766 -2.6988237 -3.5349011 -4.0503721 -4.1487594 -3.9620681][-6.5673418 -5.923317 -5.1454415 -4.3055439 -2.9695048 -1.3357389 -0.16969395 0.14661407 -0.24957228 -1.2744367 -2.6543598 -3.6726646 -4.4177237 -4.6721406 -4.6531258][-6.3656435 -5.9291339 -5.4093618 -4.7673254 -3.6478295 -2.2531059 -1.2064085 -0.81612372 -1.0546515 -1.9626317 -3.2285495 -4.2952724 -5.1763625 -5.5364485 -5.6311502][-6.1368246 -5.9807196 -5.7265739 -5.3242893 -4.545948 -3.5199118 -2.6794567 -2.254812 -2.3237753 -2.9917555 -3.9552431 -4.8905272 -5.7589221 -6.1640348 -6.3409748][-5.7564793 -5.78626 -5.6817684 -5.4682193 -5.04616 -4.4283671 -3.8781438 -3.5184851 -3.4900517 -3.9088089 -4.4923429 -5.1382151 -5.8196659 -6.1771078 -6.3999748][-5.1509695 -5.2505083 -5.2320418 -5.1827636 -5.072516 -4.8254895 -4.5457067 -4.2595081 -4.1669183 -4.3897085 -4.663949 -5.0111561 -5.4459925 -5.6918173 -5.888763][-4.4420419 -4.53251 -4.5638127 -4.6222439 -4.7269893 -4.762001 -4.6959782 -4.4850173 -4.3408089 -4.4121714 -4.4788222 -4.6006408 -4.8547254 -5.0230579 -5.1841545][-3.9385996 -4.0022664 -4.0491548 -4.1213722 -4.295258 -4.4784389 -4.5518575 -4.4333205 -4.2770171 -4.2429214 -4.1926618 -4.1859431 -4.3212371 -4.4302664 -4.5532069]]...]
INFO - root - 2017-12-06 08:57:46.598312: step 14710, loss = 1.09, batch loss = 1.02 (13.5 examples/sec; 0.594 sec/batch; 52h:26m:04s remains)
INFO - root - 2017-12-06 08:57:52.685741: step 14720, loss = 0.80, batch loss = 0.73 (13.5 examples/sec; 0.591 sec/batch; 52h:11m:36s remains)
INFO - root - 2017-12-06 08:57:58.865978: step 14730, loss = 1.05, batch loss = 0.98 (13.0 examples/sec; 0.615 sec/batch; 54h:14m:33s remains)
INFO - root - 2017-12-06 08:58:04.953164: step 14740, loss = 0.77, batch loss = 0.70 (13.3 examples/sec; 0.602 sec/batch; 53h:06m:27s remains)
INFO - root - 2017-12-06 08:58:11.047177: step 14750, loss = 1.02, batch loss = 0.95 (13.2 examples/sec; 0.606 sec/batch; 53h:29m:11s remains)
INFO - root - 2017-12-06 08:58:17.052007: step 14760, loss = 0.75, batch loss = 0.68 (13.3 examples/sec; 0.603 sec/batch; 53h:13m:42s remains)
INFO - root - 2017-12-06 08:58:23.183934: step 14770, loss = 0.78, batch loss = 0.71 (12.8 examples/sec; 0.627 sec/batch; 55h:17m:51s remains)
INFO - root - 2017-12-06 08:58:29.315222: step 14780, loss = 0.95, batch loss = 0.88 (13.0 examples/sec; 0.613 sec/batch; 54h:07m:28s remains)
INFO - root - 2017-12-06 08:58:35.464493: step 14790, loss = 0.85, batch loss = 0.78 (13.1 examples/sec; 0.613 sec/batch; 54h:05m:56s remains)
INFO - root - 2017-12-06 08:58:41.515176: step 14800, loss = 1.17, batch loss = 1.10 (13.6 examples/sec; 0.590 sec/batch; 52h:05m:20s remains)
2017-12-06 08:58:42.045837: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4397178 -3.4232106 -4.2797241 -4.7007475 -4.7313766 -4.6941066 -4.751543 -4.7915344 -4.7088571 -4.5905728 -4.5231309 -4.447701 -4.3486409 -4.2042909 -4.0200748][-2.1434445 -3.2577748 -4.3216796 -4.9357872 -5.0735092 -5.0395675 -5.0815172 -5.1862764 -5.2439842 -5.2752385 -5.2849941 -5.1982546 -5.0430913 -4.8617454 -4.6712031][-2.2721219 -3.3951321 -4.4648829 -5.0557694 -5.168715 -5.0990386 -5.1258512 -5.3087769 -5.535079 -5.7327576 -5.8361588 -5.7705812 -5.5858479 -5.3583236 -5.1298437][-2.6010842 -3.6650226 -4.5975661 -4.9864316 -4.9526215 -4.7960281 -4.7458887 -4.8999352 -5.2033863 -5.5485506 -5.7998738 -5.8677363 -5.7759819 -5.576571 -5.3258834][-2.8948092 -3.8135438 -4.5605884 -4.7113938 -4.4959383 -4.2089968 -3.9613774 -3.9006565 -4.1427364 -4.6346579 -5.1172323 -5.4269 -5.5506272 -5.4751925 -5.2686958][-3.1133347 -3.8205888 -4.3944378 -4.3871937 -4.0295916 -3.548111 -2.928196 -2.4296088 -2.4815068 -3.1728494 -4.0242643 -4.6843219 -5.0970492 -5.19665 -5.0886555][-3.184742 -3.6887245 -4.12797 -4.0567765 -3.5941644 -2.8916614 -1.8528621 -0.85016227 -0.67346406 -1.6281242 -2.9572313 -4.0049133 -4.6737218 -4.9111094 -4.8903875][-3.2492242 -3.5734248 -3.9070554 -3.8158696 -3.2662022 -2.36074 -1.0303712 0.30660534 0.62490416 -0.58911037 -2.3326726 -3.6685395 -4.4573903 -4.7298064 -4.7476425][-3.4629536 -3.6218894 -3.8584526 -3.7720866 -3.2001419 -2.2275407 -0.88885069 0.39114141 0.62379074 -0.64492249 -2.4431388 -3.757565 -4.4344249 -4.6214852 -4.6468077][-3.94309 -3.9523945 -4.0490246 -3.9272664 -3.3911116 -2.5266013 -1.4264393 -0.48724389 -0.479779 -1.6279514 -3.1322227 -4.1452818 -4.5384274 -4.5556817 -4.5675569][-4.56849 -4.4692359 -4.3854809 -4.1546183 -3.6636791 -3.0159822 -2.2892845 -1.7897961 -2.0213742 -2.9719372 -4.0371952 -4.6591649 -4.7385635 -4.5597548 -4.5080695][-5.0571146 -4.8774552 -4.6601653 -4.3460584 -3.9362867 -3.5507827 -3.2045994 -3.0877569 -3.4504943 -4.1485968 -4.7803288 -5.0642943 -4.9345164 -4.6254187 -4.4722786][-5.3450127 -5.120358 -4.8648996 -4.5624418 -4.270278 -4.1151881 -4.0596375 -4.1903152 -4.5657582 -4.9991422 -5.2519979 -5.2547207 -5.0005918 -4.6453123 -4.4328318][-5.45191 -5.228519 -4.9956226 -4.7317934 -4.5311465 -4.5251765 -4.6362872 -4.8795953 -5.211154 -5.4608865 -5.4768209 -5.2960577 -4.9846821 -4.6428919 -4.434948][-5.49439 -5.3299541 -5.1419296 -4.8801713 -4.6719618 -4.6827855 -4.846746 -5.119523 -5.4087791 -5.5873265 -5.5179977 -5.2611918 -4.9443727 -4.6609683 -4.518393]]...]
INFO - root - 2017-12-06 08:58:48.148850: step 14810, loss = 1.02, batch loss = 0.95 (13.5 examples/sec; 0.594 sec/batch; 52h:26m:08s remains)
INFO - root - 2017-12-06 08:58:54.317201: step 14820, loss = 0.89, batch loss = 0.82 (12.4 examples/sec; 0.646 sec/batch; 57h:01m:13s remains)
INFO - root - 2017-12-06 08:59:00.375979: step 14830, loss = 1.02, batch loss = 0.95 (13.0 examples/sec; 0.616 sec/batch; 54h:20m:04s remains)
INFO - root - 2017-12-06 08:59:06.403987: step 14840, loss = 0.80, batch loss = 0.73 (13.1 examples/sec; 0.610 sec/batch; 53h:47m:10s remains)
INFO - root - 2017-12-06 08:59:12.514308: step 14850, loss = 0.89, batch loss = 0.82 (12.6 examples/sec; 0.635 sec/batch; 56h:00m:45s remains)
INFO - root - 2017-12-06 08:59:18.552687: step 14860, loss = 1.02, batch loss = 0.95 (14.8 examples/sec; 0.539 sec/batch; 47h:34m:47s remains)
INFO - root - 2017-12-06 08:59:24.647395: step 14870, loss = 1.08, batch loss = 1.01 (12.8 examples/sec; 0.627 sec/batch; 55h:20m:20s remains)
INFO - root - 2017-12-06 08:59:30.689292: step 14880, loss = 0.86, batch loss = 0.79 (13.0 examples/sec; 0.614 sec/batch; 54h:09m:27s remains)
INFO - root - 2017-12-06 08:59:36.832862: step 14890, loss = 0.94, batch loss = 0.87 (13.1 examples/sec; 0.612 sec/batch; 53h:57m:02s remains)
INFO - root - 2017-12-06 08:59:42.913185: step 14900, loss = 0.79, batch loss = 0.72 (13.2 examples/sec; 0.605 sec/batch; 53h:20m:09s remains)
2017-12-06 08:59:43.497426: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.233367 -4.074131 -3.8844807 -3.8892388 -4.04537 -4.139132 -4.1273246 -4.1021142 -4.15472 -4.2543144 -4.3151717 -4.3296566 -4.266582 -4.237215 -4.2096987][-3.9814091 -3.7880173 -3.6607649 -3.7741134 -3.9913049 -4.0766106 -4.0095143 -3.9673061 -4.0021048 -4.07197 -4.1880517 -4.379868 -4.52058 -4.6073117 -4.6295691][-3.5767717 -3.3292909 -3.2893982 -3.5353663 -3.7670691 -3.7991676 -3.718833 -3.760793 -3.879416 -3.9402881 -4.0448174 -4.32846 -4.6090474 -4.7649741 -4.8160343][-3.0912404 -2.7893147 -2.8188605 -3.1661825 -3.371594 -3.3488028 -3.2847819 -3.4361372 -3.7028656 -3.8345385 -3.9278383 -4.2063622 -4.5174441 -4.6672792 -4.7121363][-2.8152919 -2.5055823 -2.5442662 -2.8313522 -2.872982 -2.7109184 -2.6410975 -2.9070227 -3.3734546 -3.6871154 -3.8601422 -4.1178632 -4.3784237 -4.4780712 -4.5014811][-2.9127064 -2.6102703 -2.5551271 -2.588696 -2.2743979 -1.8480232 -1.7635801 -2.1844838 -2.8724017 -3.44312 -3.8331842 -4.1700583 -4.4093852 -4.4584651 -4.4515162][-3.2873185 -2.9331398 -2.6832027 -2.40298 -1.7331166 -1.0486462 -0.94622278 -1.506139 -2.3669138 -3.1601939 -3.8129261 -4.33703 -4.6309471 -4.6493239 -4.5882821][-3.7449136 -3.3075624 -2.8643494 -2.3750322 -1.5800302 -0.80284 -0.64145756 -1.1908395 -2.0603352 -2.9207129 -3.7173276 -4.411726 -4.7874327 -4.787879 -4.6717057][-4.291575 -3.8174961 -3.2725492 -2.7199585 -2.0220187 -1.3243723 -1.0411141 -1.3893626 -2.102632 -2.861022 -3.5920367 -4.2642155 -4.6213279 -4.565279 -4.4037814][-4.7480407 -4.3636031 -3.8452795 -3.3177266 -2.7928395 -2.2466023 -1.820276 -1.8710496 -2.3599164 -2.9678507 -3.5242624 -4.0071087 -4.1812959 -3.9705241 -3.7297044][-4.8044357 -4.6447563 -4.3003721 -3.8780921 -3.5269361 -3.1370113 -2.6268477 -2.4296563 -2.7207284 -3.2074335 -3.601162 -3.8543174 -3.7710142 -3.3169136 -2.9066982][-4.412662 -4.4861274 -4.3924222 -4.1654568 -4.0086713 -3.7938967 -3.3329868 -3.0364585 -3.1894526 -3.5621836 -3.8238513 -3.9201195 -3.6517224 -2.9810078 -2.3512282][-3.7460406 -3.9435942 -4.0198593 -3.9609289 -3.999804 -4.0119271 -3.7410183 -3.51701 -3.648967 -3.9280381 -4.0715823 -4.0947008 -3.778152 -3.0533836 -2.3126285][-3.2615297 -3.4005566 -3.4186997 -3.370687 -3.53364 -3.771558 -3.7791829 -3.7509723 -3.9477954 -4.1407709 -4.1520624 -4.1089773 -3.8344429 -3.2635581 -2.635253][-3.464406 -3.4312441 -3.1933193 -2.9624977 -3.0991988 -3.4496567 -3.6977091 -3.8638003 -4.1113868 -4.1773009 -4.0299659 -3.8932512 -3.6708798 -3.3495393 -2.9740245]]...]
INFO - root - 2017-12-06 08:59:49.630964: step 14910, loss = 0.89, batch loss = 0.82 (12.8 examples/sec; 0.623 sec/batch; 54h:56m:32s remains)
INFO - root - 2017-12-06 08:59:55.716136: step 14920, loss = 0.97, batch loss = 0.90 (13.3 examples/sec; 0.603 sec/batch; 53h:10m:50s remains)
INFO - root - 2017-12-06 09:00:01.737344: step 14930, loss = 0.98, batch loss = 0.91 (13.9 examples/sec; 0.574 sec/batch; 50h:39m:00s remains)
INFO - root - 2017-12-06 09:00:07.856836: step 14940, loss = 0.65, batch loss = 0.58 (13.0 examples/sec; 0.618 sec/batch; 54h:28m:33s remains)
INFO - root - 2017-12-06 09:00:13.934893: step 14950, loss = 0.94, batch loss = 0.87 (13.0 examples/sec; 0.617 sec/batch; 54h:26m:15s remains)
INFO - root - 2017-12-06 09:00:20.056272: step 14960, loss = 0.86, batch loss = 0.79 (13.1 examples/sec; 0.611 sec/batch; 53h:55m:27s remains)
INFO - root - 2017-12-06 09:00:26.038452: step 14970, loss = 0.93, batch loss = 0.86 (13.0 examples/sec; 0.618 sec/batch; 54h:28m:08s remains)
INFO - root - 2017-12-06 09:00:32.088535: step 14980, loss = 0.95, batch loss = 0.88 (13.3 examples/sec; 0.601 sec/batch; 52h:58m:19s remains)
INFO - root - 2017-12-06 09:00:38.177308: step 14990, loss = 0.76, batch loss = 0.69 (12.9 examples/sec; 0.619 sec/batch; 54h:33m:59s remains)
INFO - root - 2017-12-06 09:00:44.151220: step 15000, loss = 0.95, batch loss = 0.88 (13.1 examples/sec; 0.609 sec/batch; 53h:43m:24s remains)
2017-12-06 09:00:44.672227: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8544242 -3.8384595 -3.8660312 -3.9690886 -4.164206 -4.3374534 -4.4684057 -4.5558071 -4.5002694 -4.1903348 -3.8584533 -3.6900797 -3.7077427 -3.7659853 -3.7860224][-3.8456726 -3.8796148 -4.0087986 -4.2293525 -4.4817572 -4.6160431 -4.6734266 -4.7161894 -4.6125164 -4.2600722 -3.9399714 -3.8164945 -3.8416879 -3.854521 -3.7660694][-3.9117229 -4.0367384 -4.279942 -4.5920067 -4.8267055 -4.8400269 -4.726634 -4.64063 -4.4773784 -4.1758986 -3.9884872 -4.0261121 -4.1246905 -4.1032281 -3.8574429][-4.0624943 -4.28438 -4.6015611 -4.9413314 -5.0865178 -4.9043603 -4.5146961 -4.1982784 -3.9801922 -3.8209872 -3.874023 -4.1825938 -4.4627748 -4.4728703 -4.0645432][-4.2715678 -4.5406914 -4.83917 -5.11068 -5.0916882 -4.6463623 -3.889986 -3.2810259 -3.0812426 -3.1922412 -3.5845122 -4.2170224 -4.7332578 -4.837656 -4.3125196][-4.4400067 -4.6812162 -4.8742948 -5.0008888 -4.7587233 -3.9923723 -2.8357368 -1.9361186 -1.8536916 -2.3723536 -3.2035103 -4.1554613 -4.8689089 -5.0568829 -4.474534][-4.4654622 -4.611136 -4.6422229 -4.5679631 -4.0747089 -3.0280836 -1.6031804 -0.56547427 -0.71704221 -1.7340791 -3.0330157 -4.1897931 -4.9286394 -5.0849447 -4.4790049][-4.4080405 -4.4522963 -4.3200579 -4.0415058 -3.3471541 -2.1623762 -0.73506522 0.16189098 -0.27582932 -1.6637666 -3.2165337 -4.3612208 -4.9649925 -5.00921 -4.4277][-4.3800907 -4.3760056 -4.1490421 -3.7518034 -2.9908698 -1.8711779 -0.69786882 -0.1481576 -0.75635386 -2.1648788 -3.6130013 -4.5289574 -4.9268961 -4.8716803 -4.3988957][-4.41601 -4.4156523 -4.1650314 -3.7374206 -3.0579388 -2.1805613 -1.3935583 -1.2000451 -1.812603 -2.9384542 -4.0041847 -4.5725765 -4.7771964 -4.7053137 -4.4223585][-4.5149665 -4.5486674 -4.3381767 -3.9716337 -3.4723237 -2.9197581 -2.5020175 -2.5391865 -3.0132442 -3.7200346 -4.3185329 -4.5631862 -4.6517205 -4.6339974 -4.5376849][-4.5906172 -4.6595678 -4.5215549 -4.2593875 -3.9524684 -3.6947212 -3.552958 -3.676059 -3.9689791 -4.3034139 -4.5286689 -4.57224 -4.6165514 -4.6608362 -4.6701379][-4.583549 -4.6840076 -4.6293116 -4.4801364 -4.3227234 -4.2352495 -4.2336626 -4.346765 -4.4884915 -4.5915661 -4.6137304 -4.5777683 -4.6034031 -4.6646733 -4.6899295][-4.4660225 -4.56851 -4.5710993 -4.508256 -4.4447279 -4.43315 -4.4689746 -4.531055 -4.5702314 -4.5644021 -4.5126066 -4.4572535 -4.457562 -4.4952497 -4.5041137][-4.2996464 -4.3696785 -4.3805 -4.3614154 -4.3511019 -4.3652778 -4.3947272 -4.4185047 -4.4166675 -4.3850236 -4.327569 -4.2723446 -4.2508287 -4.2619896 -4.2609234]]...]
INFO - root - 2017-12-06 09:00:50.834466: step 15010, loss = 0.90, batch loss = 0.83 (13.3 examples/sec; 0.603 sec/batch; 53h:12m:36s remains)
INFO - root - 2017-12-06 09:00:56.947190: step 15020, loss = 0.76, batch loss = 0.69 (14.0 examples/sec; 0.572 sec/batch; 50h:27m:55s remains)
INFO - root - 2017-12-06 09:01:03.057412: step 15030, loss = 0.92, batch loss = 0.85 (13.3 examples/sec; 0.602 sec/batch; 53h:03m:20s remains)
INFO - root - 2017-12-06 09:01:09.082501: step 15040, loss = 0.93, batch loss = 0.86 (13.5 examples/sec; 0.594 sec/batch; 52h:25m:07s remains)
INFO - root - 2017-12-06 09:01:15.178421: step 15050, loss = 0.74, batch loss = 0.67 (12.4 examples/sec; 0.645 sec/batch; 56h:52m:21s remains)
INFO - root - 2017-12-06 09:01:21.314081: step 15060, loss = 1.04, batch loss = 0.97 (13.1 examples/sec; 0.610 sec/batch; 53h:49m:47s remains)
INFO - root - 2017-12-06 09:01:27.437303: step 15070, loss = 1.19, batch loss = 1.12 (13.5 examples/sec; 0.591 sec/batch; 52h:07m:34s remains)
INFO - root - 2017-12-06 09:01:33.435763: step 15080, loss = 0.94, batch loss = 0.87 (12.8 examples/sec; 0.627 sec/batch; 55h:16m:32s remains)
INFO - root - 2017-12-06 09:01:39.507862: step 15090, loss = 1.19, batch loss = 1.12 (13.4 examples/sec; 0.598 sec/batch; 52h:44m:37s remains)
INFO - root - 2017-12-06 09:01:45.611555: step 15100, loss = 0.84, batch loss = 0.77 (12.7 examples/sec; 0.628 sec/batch; 55h:19m:56s remains)
2017-12-06 09:01:46.143375: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.849761 -4.9637251 -5.0191731 -5.00849 -5.0401731 -5.0936632 -5.0082326 -4.8245039 -4.6436563 -4.6096168 -4.614553 -4.5276718 -4.32338 -4.0747228 -3.9189551][-5.0190163 -5.1510367 -5.122128 -4.9906044 -4.8805237 -4.7761235 -4.5608325 -4.2772889 -4.0431352 -4.0961409 -4.3018389 -4.4576621 -4.4086041 -4.1306977 -3.8506632][-5.1290207 -5.2554755 -5.1010046 -4.7678604 -4.4153533 -4.0852518 -3.7290065 -3.4333713 -3.2731767 -3.4629676 -3.8953989 -4.3572373 -4.584868 -4.3909526 -4.0355959][-5.1293187 -5.2517719 -5.0118141 -4.477561 -3.8385437 -3.2113276 -2.6391687 -2.3227315 -2.3501225 -2.7435391 -3.3703747 -4.0783834 -4.5927019 -4.615871 -4.3365321][-5.051126 -5.1849442 -4.9460568 -4.30739 -3.4260225 -2.43971 -1.5167298 -1.0401652 -1.2618489 -1.9461575 -2.7730396 -3.643364 -4.3479939 -4.6253271 -4.5600028][-4.9843936 -5.155086 -4.9851389 -4.3543024 -3.3416758 -2.0307469 -0.71433496 0.030358315 -0.27594852 -1.2533252 -2.2656527 -3.1953177 -3.9610307 -4.4279785 -4.6313343][-4.976212 -5.1841507 -5.0768971 -4.4972835 -3.4909604 -2.0421276 -0.46386695 0.50633621 0.2530818 -0.85073376 -1.9516001 -2.8238933 -3.5514936 -4.1228008 -4.5724607][-4.996047 -5.2303 -5.1501312 -4.6390529 -3.7684865 -2.4587917 -0.90596938 0.11475754 -0.015959263 -0.99404454 -2.0067072 -2.7222688 -3.3442278 -3.9293649 -4.5157428][-5.0251193 -5.279561 -5.1844535 -4.7150021 -4.0181022 -3.0296054 -1.8102787 -0.96914816 -1.0179534 -1.7221622 -2.5089908 -3.0598736 -3.5561459 -4.067574 -4.6371207][-5.0680714 -5.339148 -5.2259054 -4.7550063 -4.1609936 -3.4684641 -2.6907053 -2.1629665 -2.2082558 -2.6471381 -3.2165136 -3.692189 -4.1193266 -4.53385 -4.9886308][-5.0737276 -5.3675466 -5.2865829 -4.8444228 -4.3113408 -3.7957814 -3.3621387 -3.1175251 -3.1594496 -3.403791 -3.8320148 -4.3101482 -4.7452111 -5.0914564 -5.4011378][-4.9499774 -5.2370033 -5.2338266 -4.8980289 -4.4586635 -4.0377607 -3.7685437 -3.7003472 -3.7340412 -3.8637705 -4.2117047 -4.7154508 -5.1958427 -5.499023 -5.6673279][-4.6947336 -4.9607396 -5.0716667 -4.9255171 -4.6579461 -4.3262105 -4.0957527 -4.0691519 -4.0889773 -4.1672239 -4.4485292 -4.9198766 -5.3806262 -5.6258526 -5.6648679][-4.430006 -4.6305285 -4.8076119 -4.8455772 -4.79343 -4.6069326 -4.4185891 -4.3698072 -4.3720369 -4.442657 -4.6455817 -4.9912605 -5.3183508 -5.4515553 -5.3824453][-4.3137646 -4.405098 -4.5335941 -4.6351666 -4.7124743 -4.654686 -4.542748 -4.5016251 -4.5270128 -4.6169739 -4.7556252 -4.9579387 -5.1191416 -5.1328473 -5.0068107]]...]
INFO - root - 2017-12-06 09:01:52.197352: step 15110, loss = 0.82, batch loss = 0.75 (13.5 examples/sec; 0.594 sec/batch; 52h:20m:51s remains)
INFO - root - 2017-12-06 09:01:58.297425: step 15120, loss = 1.22, batch loss = 1.15 (13.0 examples/sec; 0.613 sec/batch; 54h:03m:42s remains)
INFO - root - 2017-12-06 09:02:04.304147: step 15130, loss = 0.91, batch loss = 0.84 (13.4 examples/sec; 0.597 sec/batch; 52h:35m:35s remains)
INFO - root - 2017-12-06 09:02:10.389733: step 15140, loss = 0.87, batch loss = 0.80 (13.4 examples/sec; 0.598 sec/batch; 52h:40m:32s remains)
INFO - root - 2017-12-06 09:02:16.460517: step 15150, loss = 0.95, batch loss = 0.88 (12.6 examples/sec; 0.636 sec/batch; 56h:05m:18s remains)
INFO - root - 2017-12-06 09:02:22.511885: step 15160, loss = 0.96, batch loss = 0.89 (13.2 examples/sec; 0.606 sec/batch; 53h:22m:56s remains)
INFO - root - 2017-12-06 09:02:28.621128: step 15170, loss = 0.96, batch loss = 0.89 (13.4 examples/sec; 0.599 sec/batch; 52h:45m:28s remains)
INFO - root - 2017-12-06 09:02:34.664936: step 15180, loss = 0.80, batch loss = 0.73 (13.7 examples/sec; 0.584 sec/batch; 51h:29m:22s remains)
INFO - root - 2017-12-06 09:02:40.590347: step 15190, loss = 0.82, batch loss = 0.75 (13.3 examples/sec; 0.602 sec/batch; 53h:05m:09s remains)
INFO - root - 2017-12-06 09:02:46.664166: step 15200, loss = 0.74, batch loss = 0.67 (13.6 examples/sec; 0.589 sec/batch; 51h:55m:14s remains)
2017-12-06 09:02:47.165908: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9160233 -4.3779621 -4.4611044 -4.1890712 -3.9746 -3.9176965 -3.8005145 -3.6455717 -3.5963054 -3.8579543 -4.4549279 -4.9596672 -4.9520288 -4.6322007 -4.4383759][-4.1702561 -4.5766449 -4.6256094 -4.2964115 -3.9502447 -3.7588844 -3.5878918 -3.4711759 -3.5111649 -3.8628247 -4.5206728 -5.0865784 -5.1370893 -4.8228965 -4.6004524][-4.0251131 -4.4056497 -4.461143 -4.1119518 -3.653739 -3.3488283 -3.1421475 -3.0892625 -3.2166848 -3.6240928 -4.3141966 -4.93815 -5.0919194 -4.8703842 -4.677155][-3.6441205 -4.0823469 -4.2237086 -3.926903 -3.4221144 -3.0129778 -2.7237132 -2.6959305 -2.8640347 -3.2816436 -3.9787374 -4.6269383 -4.8632193 -4.7638721 -4.6467605][-3.1570439 -3.6843326 -3.9502444 -3.7587266 -3.2819357 -2.7983067 -2.4146118 -2.3635037 -2.5258875 -2.9201593 -3.6056066 -4.2365112 -4.5061946 -4.5230141 -4.5233564][-2.5968726 -3.147794 -3.5239778 -3.4910135 -3.1569393 -2.7058234 -2.277643 -2.1561449 -2.2376697 -2.542356 -3.163846 -3.7666149 -4.073441 -4.2095628 -4.357749][-2.1342905 -2.6594057 -3.1326723 -3.2848928 -3.1739831 -2.8224039 -2.3421032 -2.0582874 -1.9508455 -2.0836234 -2.62534 -3.2671933 -3.6724489 -3.9278684 -4.207684][-1.9391212 -2.3984258 -2.9556932 -3.3025651 -3.4379086 -3.225816 -2.6910186 -2.1711838 -1.7860672 -1.7181566 -2.2012293 -2.9246645 -3.4365673 -3.7753291 -4.1338592][-2.0192263 -2.3815172 -3.0252366 -3.5721323 -3.9544072 -3.9042292 -3.3137558 -2.5195251 -1.8357801 -1.615052 -2.094449 -2.9002595 -3.4722331 -3.8246875 -4.1904674][-2.2650921 -2.4915314 -3.1706438 -3.8591871 -4.4325066 -4.5090694 -3.8696668 -2.873513 -2.0192766 -1.7729294 -2.3112056 -3.1655087 -3.7419989 -4.051877 -4.3620992][-2.6149817 -2.69912 -3.3137369 -4.0108943 -4.6362953 -4.7704406 -4.1372471 -3.1245627 -2.3069539 -2.1456387 -2.7439299 -3.5943627 -4.1380768 -4.3736663 -4.5721068][-2.9541886 -2.9781928 -3.4970458 -4.1025815 -4.644608 -4.7592797 -4.193615 -3.3264353 -2.689817 -2.66894 -3.3056159 -4.1213493 -4.609086 -4.7293267 -4.7623315][-3.2566342 -3.3239274 -3.7765188 -4.2491198 -4.6247048 -4.6698956 -4.2181373 -3.5802476 -3.1869392 -3.3070252 -3.9472117 -4.6905107 -5.0797968 -5.0451345 -4.892622][-3.5345397 -3.6627305 -4.0462365 -4.3784809 -4.5661564 -4.5128779 -4.1736407 -3.7839363 -3.650578 -3.9079895 -4.5112286 -5.1215029 -5.3595824 -5.1818042 -4.9040947][-3.8265846 -3.9667559 -4.2435412 -4.435792 -4.4705548 -4.3359761 -4.0890021 -3.9161191 -4.0060515 -4.3615208 -4.8764262 -5.3007684 -5.3819551 -5.1335392 -4.8387489]]...]
INFO - root - 2017-12-06 09:02:53.293004: step 15210, loss = 0.71, batch loss = 0.64 (13.6 examples/sec; 0.587 sec/batch; 51h:44m:14s remains)
INFO - root - 2017-12-06 09:02:59.371443: step 15220, loss = 0.72, batch loss = 0.65 (13.2 examples/sec; 0.608 sec/batch; 53h:32m:53s remains)
INFO - root - 2017-12-06 09:03:05.475283: step 15230, loss = 0.72, batch loss = 0.65 (13.1 examples/sec; 0.609 sec/batch; 53h:39m:01s remains)
INFO - root - 2017-12-06 09:03:11.544743: step 15240, loss = 0.78, batch loss = 0.71 (13.8 examples/sec; 0.581 sec/batch; 51h:13m:20s remains)
INFO - root - 2017-12-06 09:03:17.569319: step 15250, loss = 0.97, batch loss = 0.90 (12.9 examples/sec; 0.621 sec/batch; 54h:43m:37s remains)
INFO - root - 2017-12-06 09:03:23.671096: step 15260, loss = 1.01, batch loss = 0.94 (13.3 examples/sec; 0.603 sec/batch; 53h:06m:51s remains)
INFO - root - 2017-12-06 09:03:29.819135: step 15270, loss = 1.23, batch loss = 1.16 (12.8 examples/sec; 0.627 sec/batch; 55h:14m:26s remains)
INFO - root - 2017-12-06 09:03:35.959057: step 15280, loss = 0.98, batch loss = 0.91 (13.2 examples/sec; 0.607 sec/batch; 53h:28m:40s remains)
INFO - root - 2017-12-06 09:03:41.835522: step 15290, loss = 0.97, batch loss = 0.90 (13.1 examples/sec; 0.610 sec/batch; 53h:43m:37s remains)
INFO - root - 2017-12-06 09:03:47.869206: step 15300, loss = 1.01, batch loss = 0.94 (13.3 examples/sec; 0.600 sec/batch; 52h:51m:43s remains)
2017-12-06 09:03:48.415165: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2043715 -4.032577 -3.9626608 -4.1053762 -4.2289033 -4.1867323 -3.9040146 -3.6358063 -3.6807108 -3.7729197 -4.0941219 -4.736711 -5.1532631 -4.987361 -4.2640257][-4.1692405 -4.0019174 -3.9347079 -4.1381741 -4.3405643 -4.3339505 -3.9563706 -3.4956985 -3.3296204 -3.333961 -3.6565311 -4.344666 -4.893239 -4.8714643 -4.3224897][-4.0655332 -3.9017363 -3.9138303 -4.2381229 -4.518085 -4.4964252 -3.9990172 -3.3265569 -2.9520264 -2.889538 -3.1810062 -3.8221645 -4.3983755 -4.483552 -4.10325][-3.8659358 -3.7266738 -3.8342929 -4.2521105 -4.5719113 -4.5033979 -3.9310198 -3.1504588 -2.6816115 -2.6134298 -2.858202 -3.3768706 -3.9022756 -4.0701175 -3.8630877][-3.5681112 -3.4935372 -3.6303051 -4.0423775 -4.3364038 -4.2234983 -3.6870384 -2.9984546 -2.6104817 -2.5916338 -2.7712979 -3.1159198 -3.5614953 -3.8302112 -3.8110182][-3.2296295 -3.246645 -3.3206177 -3.6227112 -3.8436525 -3.7093897 -3.2957401 -2.8223786 -2.6412573 -2.7634923 -2.9159632 -3.065558 -3.3918436 -3.7166414 -3.8505685][-3.0265648 -3.0828772 -2.9953215 -3.1239483 -3.2615919 -3.1248093 -2.8527291 -2.6059666 -2.6693449 -3.0052223 -3.2342591 -3.2888849 -3.4941075 -3.7735729 -3.9384871][-3.1063313 -3.0918067 -2.7650716 -2.7053723 -2.7833405 -2.6923153 -2.5816689 -2.5035405 -2.7416387 -3.2810884 -3.6749735 -3.7881963 -3.9401641 -4.0990191 -4.1411691][-3.4386206 -3.2784109 -2.7481508 -2.5266814 -2.5360882 -2.51002 -2.5624814 -2.610955 -2.9308608 -3.5861356 -4.11624 -4.333292 -4.4896488 -4.52607 -4.3828607][-3.9206331 -3.60331 -2.9645402 -2.6344738 -2.5578036 -2.5619786 -2.7275162 -2.877492 -3.2244587 -3.867857 -4.3980684 -4.6450839 -4.8190117 -4.798995 -4.5444283][-4.3493943 -3.9785259 -3.3749909 -3.0180783 -2.8769853 -2.8794122 -3.0803967 -3.2772291 -3.5737729 -4.0771375 -4.4930844 -4.7068772 -4.8677812 -4.8382106 -4.5806746][-4.553349 -4.2660804 -3.8422341 -3.5730567 -3.4267538 -3.3898292 -3.5413194 -3.7285848 -3.9297533 -4.2398 -4.5064678 -4.672462 -4.7764421 -4.7333789 -4.5353842][-4.3708963 -4.2435436 -4.077733 -3.9879804 -3.9037681 -3.8459764 -3.9425027 -4.107533 -4.2389817 -4.40362 -4.5675435 -4.69788 -4.7339554 -4.6716661 -4.5488477][-3.860456 -3.8652737 -3.9292431 -4.0471368 -4.0889068 -4.080287 -4.1647162 -4.3173308 -4.43628 -4.5381489 -4.6519184 -4.7567587 -4.7376657 -4.638566 -4.5497122][-3.3160188 -3.3586831 -3.5211329 -3.7650037 -3.9380889 -4.040391 -4.1695876 -4.3373775 -4.4728036 -4.542964 -4.6001759 -4.6638346 -4.622633 -4.5015273 -4.4150929]]...]
INFO - root - 2017-12-06 09:03:54.562144: step 15310, loss = 0.95, batch loss = 0.88 (13.3 examples/sec; 0.602 sec/batch; 53h:02m:35s remains)
INFO - root - 2017-12-06 09:04:00.650018: step 15320, loss = 1.14, batch loss = 1.07 (13.4 examples/sec; 0.596 sec/batch; 52h:28m:40s remains)
INFO - root - 2017-12-06 09:04:06.730132: step 15330, loss = 0.81, batch loss = 0.74 (13.4 examples/sec; 0.595 sec/batch; 52h:25m:29s remains)
INFO - root - 2017-12-06 09:04:12.831535: step 15340, loss = 0.95, batch loss = 0.88 (13.0 examples/sec; 0.615 sec/batch; 54h:13m:12s remains)
INFO - root - 2017-12-06 09:04:18.788376: step 15350, loss = 1.15, batch loss = 1.08 (12.9 examples/sec; 0.620 sec/batch; 54h:34m:53s remains)
INFO - root - 2017-12-06 09:04:24.884209: step 15360, loss = 0.92, batch loss = 0.85 (13.1 examples/sec; 0.610 sec/batch; 53h:45m:21s remains)
INFO - root - 2017-12-06 09:04:30.991413: step 15370, loss = 0.88, batch loss = 0.81 (13.0 examples/sec; 0.615 sec/batch; 54h:11m:06s remains)
INFO - root - 2017-12-06 09:04:37.141763: step 15380, loss = 0.84, batch loss = 0.77 (13.0 examples/sec; 0.617 sec/batch; 54h:20m:44s remains)
INFO - root - 2017-12-06 09:04:43.246098: step 15390, loss = 1.08, batch loss = 1.01 (13.4 examples/sec; 0.596 sec/batch; 52h:32m:31s remains)
INFO - root - 2017-12-06 09:04:49.235859: step 15400, loss = 1.04, batch loss = 0.96 (12.9 examples/sec; 0.622 sec/batch; 54h:45m:24s remains)
2017-12-06 09:04:49.757378: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6517587 -4.6799259 -4.7176991 -4.7589602 -4.7908244 -4.797358 -4.7712855 -4.7303934 -4.6964984 -4.6892743 -4.7070971 -4.723506 -4.7243824 -4.7183104 -4.709228][-4.6278191 -4.6888227 -4.7677808 -4.8444576 -4.9017773 -4.9211159 -4.8814745 -4.8113613 -4.74165 -4.6937284 -4.6743555 -4.65875 -4.6453519 -4.6499395 -4.6681738][-4.3873482 -4.5025148 -4.647893 -4.7766671 -4.8656254 -4.8758426 -4.7656531 -4.6096349 -4.4752941 -4.3746986 -4.3097 -4.2697515 -4.2649078 -4.29607 -4.3439865][-4.0089593 -4.1765938 -4.3875742 -4.5603566 -4.6537442 -4.5923944 -4.3253946 -4.0280046 -3.8490429 -3.7744427 -3.7491255 -3.7482169 -3.7850165 -3.8405395 -3.8878021][-3.6215138 -3.8160284 -4.0553322 -4.2340846 -4.2872944 -4.0891643 -3.6012142 -3.1543939 -3.0016289 -3.0572166 -3.1538711 -3.2402496 -3.3565316 -3.4609425 -3.4800334][-3.2648077 -3.4289067 -3.622227 -3.7549195 -3.7465239 -3.3981419 -2.6850557 -2.1336184 -2.0903931 -2.3370013 -2.5693154 -2.7225451 -2.9061623 -3.076529 -3.0722737][-3.1338706 -3.1987641 -3.264564 -3.3136592 -3.2691493 -2.8485878 -2.0098431 -1.4437869 -1.5555696 -1.973249 -2.2869909 -2.4402847 -2.6439309 -2.8800073 -2.8761606][-3.303443 -3.2648973 -3.188622 -3.1661339 -3.1456628 -2.7831888 -1.9835327 -1.4926441 -1.7107635 -2.1977229 -2.4986606 -2.5687003 -2.70501 -2.9546542 -2.9433646][-3.4877539 -3.4071167 -3.2518404 -3.1959009 -3.2296715 -2.9970446 -2.3486745 -1.9932878 -2.2844291 -2.7828355 -3.0123885 -2.9321737 -2.9202394 -3.1220727 -3.1018889][-3.6568308 -3.5825312 -3.438231 -3.4109564 -3.5065587 -3.4006562 -2.9143057 -2.6567061 -2.9495838 -3.4181449 -3.5862372 -3.3936663 -3.2526755 -3.3947353 -3.3624582][-3.8497791 -3.787467 -3.7092643 -3.7542865 -3.8929975 -3.8514869 -3.470808 -3.2632947 -3.5143762 -3.9362357 -4.0789361 -3.8556881 -3.6432438 -3.7069757 -3.6291552][-3.8784056 -3.7940302 -3.7665734 -3.8960295 -4.0963612 -4.1194324 -3.8364825 -3.6598022 -3.8449738 -4.1907649 -4.2982373 -4.0990992 -3.8892961 -3.9008095 -3.782269][-3.9172337 -3.7273495 -3.6521778 -3.8213122 -4.12185 -4.2481852 -4.0502625 -3.8503339 -3.8933859 -4.0835505 -4.1221466 -3.9902856 -3.8775377 -3.9390128 -3.8653238][-3.7956429 -3.4764671 -3.2957582 -3.4673064 -3.8933432 -4.1758075 -4.1040115 -3.9267888 -3.8783026 -3.944736 -3.9285698 -3.8606989 -3.8618557 -4.0027823 -4.0154252][-3.486572 -3.1459808 -2.9205317 -3.0853047 -3.5794756 -3.9743419 -4.017128 -3.884222 -3.8200486 -3.8572164 -3.8409789 -3.8273747 -3.9204869 -4.1308193 -4.2222776]]...]
INFO - root - 2017-12-06 09:04:55.912398: step 15410, loss = 0.70, batch loss = 0.63 (12.9 examples/sec; 0.622 sec/batch; 54h:49m:47s remains)
INFO - root - 2017-12-06 09:05:02.046022: step 15420, loss = 0.72, batch loss = 0.65 (12.3 examples/sec; 0.652 sec/batch; 57h:24m:12s remains)
INFO - root - 2017-12-06 09:05:08.151597: step 15430, loss = 0.85, batch loss = 0.77 (13.0 examples/sec; 0.616 sec/batch; 54h:16m:43s remains)
INFO - root - 2017-12-06 09:05:14.293889: step 15440, loss = 1.02, batch loss = 0.95 (13.2 examples/sec; 0.608 sec/batch; 53h:34m:33s remains)
INFO - root - 2017-12-06 09:05:20.233877: step 15450, loss = 0.93, batch loss = 0.86 (12.9 examples/sec; 0.620 sec/batch; 54h:36m:15s remains)
INFO - root - 2017-12-06 09:05:26.323087: step 15460, loss = 0.98, batch loss = 0.91 (13.0 examples/sec; 0.618 sec/batch; 54h:23m:30s remains)
INFO - root - 2017-12-06 09:05:32.391469: step 15470, loss = 0.96, batch loss = 0.89 (12.9 examples/sec; 0.619 sec/batch; 54h:31m:45s remains)
INFO - root - 2017-12-06 09:05:38.476668: step 15480, loss = 0.93, batch loss = 0.86 (13.2 examples/sec; 0.604 sec/batch; 53h:11m:18s remains)
INFO - root - 2017-12-06 09:05:44.501636: step 15490, loss = 1.12, batch loss = 1.05 (13.5 examples/sec; 0.592 sec/batch; 52h:07m:15s remains)
INFO - root - 2017-12-06 09:05:50.551326: step 15500, loss = 0.77, batch loss = 0.70 (13.0 examples/sec; 0.614 sec/batch; 54h:03m:34s remains)
2017-12-06 09:05:51.106095: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.99567 -3.99969 -3.9148512 -3.9960287 -4.4194307 -4.7165122 -4.7174978 -4.7124958 -4.4491634 -4.1952496 -4.2191086 -4.3673515 -4.4672503 -4.5613942 -4.5285306][-4.182312 -4.1592784 -4.06052 -4.1210556 -4.5723238 -4.9401865 -4.9654784 -4.9728422 -4.6998644 -4.3739572 -4.3996749 -4.6270304 -4.7914114 -4.8905177 -4.7918711][-4.4960728 -4.5040359 -4.4593062 -4.5745506 -4.9794803 -5.2986612 -5.2693691 -5.2013807 -4.9044 -4.5586114 -4.6541677 -5.0109377 -5.2762327 -5.4243879 -5.2854395][-4.7093759 -4.8336477 -4.9125528 -5.1053891 -5.3636627 -5.4742174 -5.2803187 -5.0591755 -4.7359676 -4.4288931 -4.6453862 -5.1330633 -5.4728589 -5.6776981 -5.5597391][-4.6726756 -4.9105077 -5.0733 -5.2400694 -5.215661 -4.9684362 -4.48553 -4.0427241 -3.7708623 -3.6541181 -4.0526357 -4.6686482 -5.060226 -5.2924623 -5.2478652][-4.56589 -4.8139682 -4.9092684 -4.8755422 -4.4507189 -3.7752585 -2.9109485 -2.2377889 -2.1895976 -2.4601564 -3.1036386 -3.8348298 -4.2589507 -4.4749947 -4.5111842][-4.5595164 -4.7336745 -4.6665368 -4.3391466 -3.5146177 -2.4947834 -1.2814608 -0.37096119 -0.60672903 -1.4029315 -2.3559363 -3.1959968 -3.6721795 -3.8713782 -3.946655][-4.67315 -4.7680564 -4.5504003 -3.9638348 -2.8946719 -1.7614179 -0.40595865 0.67629957 0.27344418 -0.9293561 -2.1383212 -3.0455265 -3.5822678 -3.7910972 -3.8590789][-4.7896032 -4.8571415 -4.5565634 -3.862618 -2.7945905 -1.8170671 -0.6539464 0.371871 0.032500744 -1.2162209 -2.4927015 -3.4098411 -3.9894569 -4.1998782 -4.2193274][-4.8323145 -4.9437642 -4.6451139 -4.0121775 -3.1731877 -2.5419431 -1.7801018 -1.010818 -1.2029231 -2.2106311 -3.3249569 -4.13524 -4.6507111 -4.771771 -4.6942186][-4.6638203 -4.8171163 -4.5537758 -4.0573516 -3.5696011 -3.336 -2.9776261 -2.5636821 -2.7147245 -3.4323716 -4.246892 -4.8680053 -5.2043562 -5.1484089 -4.9467874][-4.29512 -4.3817568 -4.113061 -3.7350395 -3.5857632 -3.7193365 -3.7162666 -3.6767993 -3.9363656 -4.491437 -5.0308809 -5.4556046 -5.5359688 -5.2412434 -4.8839288][-3.8661356 -3.7907965 -3.4840686 -3.1919351 -3.3108697 -3.7502985 -4.0358338 -4.2916336 -4.6950178 -5.1856437 -5.5344486 -5.7868948 -5.6056089 -5.1061459 -4.5922484][-3.5818682 -3.3799133 -3.0468454 -2.8246498 -3.0821958 -3.6873586 -4.1683884 -4.5691495 -5.0097132 -5.3921165 -5.5641041 -5.6579924 -5.2670426 -4.6556511 -4.0487857][-3.7012935 -3.5015411 -3.2087126 -3.0254819 -3.2220097 -3.7440314 -4.229167 -4.5997553 -4.9369364 -5.1382976 -5.1155114 -5.06945 -4.5849075 -3.9900093 -3.4157214]]...]
INFO - root - 2017-12-06 09:05:57.048886: step 15510, loss = 0.84, batch loss = 0.77 (13.8 examples/sec; 0.580 sec/batch; 51h:02m:53s remains)
INFO - root - 2017-12-06 09:06:03.177103: step 15520, loss = 0.85, batch loss = 0.77 (13.1 examples/sec; 0.611 sec/batch; 53h:49m:41s remains)
INFO - root - 2017-12-06 09:06:09.229932: step 15530, loss = 0.80, batch loss = 0.73 (12.9 examples/sec; 0.619 sec/batch; 54h:32m:02s remains)
INFO - root - 2017-12-06 09:06:15.331566: step 15540, loss = 0.99, batch loss = 0.92 (13.4 examples/sec; 0.599 sec/batch; 52h:42m:36s remains)
INFO - root - 2017-12-06 09:06:21.387947: step 15550, loss = 0.79, batch loss = 0.72 (13.0 examples/sec; 0.616 sec/batch; 54h:13m:30s remains)
INFO - root - 2017-12-06 09:06:27.479821: step 15560, loss = 0.89, batch loss = 0.82 (13.3 examples/sec; 0.601 sec/batch; 52h:55m:57s remains)
INFO - root - 2017-12-06 09:06:33.508077: step 15570, loss = 1.05, batch loss = 0.98 (13.2 examples/sec; 0.607 sec/batch; 53h:23m:42s remains)
INFO - root - 2017-12-06 09:06:39.545701: step 15580, loss = 0.67, batch loss = 0.60 (12.9 examples/sec; 0.621 sec/batch; 54h:42m:22s remains)
INFO - root - 2017-12-06 09:06:45.622710: step 15590, loss = 0.99, batch loss = 0.92 (13.9 examples/sec; 0.577 sec/batch; 50h:45m:52s remains)
INFO - root - 2017-12-06 09:06:51.594080: step 15600, loss = 0.77, batch loss = 0.70 (12.7 examples/sec; 0.628 sec/batch; 55h:16m:35s remains)
2017-12-06 09:06:52.132158: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7088356 -4.6185856 -4.9315205 -5.2695518 -5.4552956 -5.6130137 -5.7106242 -5.643959 -5.5321217 -5.3021111 -4.8226285 -4.3237777 -3.9669859 -3.594599 -3.340502][-4.581233 -4.5936828 -5.0694532 -5.5521731 -5.6753311 -5.6427627 -5.6546288 -5.5579262 -5.4442949 -5.2804875 -4.9756513 -4.7396345 -4.5795283 -4.2066445 -3.8651121][-4.4770408 -4.5597034 -5.1045876 -5.6506777 -5.7144847 -5.5200849 -5.4489231 -5.2788658 -5.1385546 -5.1024175 -5.0200868 -5.0459561 -5.0810246 -4.75699 -4.3560553][-4.2414737 -4.3756828 -4.8688412 -5.3580551 -5.3793216 -5.1049747 -4.970994 -4.7301722 -4.5825996 -4.7040086 -4.789011 -4.9726086 -5.1707554 -4.9551806 -4.5367589][-4.0157061 -4.1746254 -4.458499 -4.69231 -4.5744615 -4.2062888 -4.0014529 -3.760973 -3.744765 -4.1143265 -4.3573332 -4.6616969 -5.0460339 -5.02225 -4.6459789][-4.0291524 -4.1375203 -4.0679445 -3.8761992 -3.4841578 -2.938179 -2.5917835 -2.3552055 -2.5211868 -3.1160238 -3.5241997 -3.9621615 -4.5439816 -4.7365108 -4.4476423][-4.2702236 -4.2555466 -3.8330119 -3.235394 -2.5595431 -1.7909694 -1.2375524 -0.95267296 -1.2165916 -1.8997166 -2.4521732 -3.0708168 -3.8578088 -4.2620854 -4.0856242][-4.6453619 -4.4820724 -3.860714 -3.0544538 -2.2080142 -1.2825081 -0.556046 -0.21028137 -0.52207875 -1.2566981 -1.9725749 -2.8065095 -3.7347083 -4.2546306 -4.1569605][-4.9560661 -4.6983786 -4.1019764 -3.3399158 -2.5080035 -1.5777087 -0.78873944 -0.41585922 -0.75679326 -1.5584528 -2.4343343 -3.395957 -4.3111444 -4.7720022 -4.6776218][-5.0382175 -4.8177938 -4.422471 -3.8565271 -3.1588185 -2.3568203 -1.661613 -1.3598502 -1.733305 -2.5612006 -3.4505811 -4.3204417 -5.0264292 -5.2776847 -5.11326][-4.9464936 -4.8244863 -4.64601 -4.2908359 -3.758954 -3.1756206 -2.72612 -2.600338 -2.9901121 -3.7388709 -4.4779978 -5.0489316 -5.3583441 -5.3373308 -5.1582685][-4.9055462 -4.8879118 -4.8496308 -4.6277871 -4.2098675 -3.8407586 -3.6779585 -3.7480881 -4.0931525 -4.6541276 -5.169086 -5.3722453 -5.2121768 -4.8676991 -4.6636763][-4.9007831 -4.9992385 -5.0795021 -4.9946928 -4.7332406 -4.5685778 -4.6027565 -4.72361 -4.8664551 -5.0983591 -5.3051128 -5.1572189 -4.592556 -3.9801486 -3.7701421][-4.8711886 -5.0536108 -5.2327747 -5.2876472 -5.2131271 -5.2164469 -5.3266683 -5.3624659 -5.223959 -5.0842972 -4.977932 -4.574688 -3.7572293 -2.9595797 -2.7329316][-4.9280782 -5.080544 -5.2440915 -5.3372183 -5.3696413 -5.4640117 -5.5832539 -5.5418053 -5.2397175 -4.8884077 -4.5841832 -4.0581636 -3.1550407 -2.308928 -2.1123161]]...]
INFO - root - 2017-12-06 09:06:58.097500: step 15610, loss = 0.95, batch loss = 0.88 (13.7 examples/sec; 0.583 sec/batch; 51h:20m:27s remains)
INFO - root - 2017-12-06 09:07:04.168083: step 15620, loss = 0.83, batch loss = 0.76 (13.1 examples/sec; 0.611 sec/batch; 53h:47m:56s remains)
INFO - root - 2017-12-06 09:07:10.222221: step 15630, loss = 1.11, batch loss = 1.04 (13.0 examples/sec; 0.617 sec/batch; 54h:17m:52s remains)
INFO - root - 2017-12-06 09:07:16.307061: step 15640, loss = 0.98, batch loss = 0.91 (13.5 examples/sec; 0.594 sec/batch; 52h:15m:55s remains)
INFO - root - 2017-12-06 09:07:22.367050: step 15650, loss = 1.25, batch loss = 1.18 (13.1 examples/sec; 0.610 sec/batch; 53h:43m:45s remains)
INFO - root - 2017-12-06 09:07:28.508922: step 15660, loss = 0.97, batch loss = 0.90 (12.7 examples/sec; 0.629 sec/batch; 55h:20m:24s remains)
INFO - root - 2017-12-06 09:07:34.569051: step 15670, loss = 0.91, batch loss = 0.84 (13.6 examples/sec; 0.587 sec/batch; 51h:40m:26s remains)
INFO - root - 2017-12-06 09:07:40.658378: step 15680, loss = 0.96, batch loss = 0.89 (12.9 examples/sec; 0.619 sec/batch; 54h:28m:53s remains)
INFO - root - 2017-12-06 09:07:46.713653: step 15690, loss = 0.93, batch loss = 0.86 (13.4 examples/sec; 0.598 sec/batch; 52h:37m:26s remains)
INFO - root - 2017-12-06 09:07:52.861243: step 15700, loss = 0.94, batch loss = 0.87 (12.8 examples/sec; 0.624 sec/batch; 54h:56m:34s remains)
2017-12-06 09:07:53.381611: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.75615 -4.7534075 -4.7517853 -4.7458615 -4.7933073 -4.8967967 -4.9925294 -5.0616121 -5.0808182 -4.997107 -4.8824043 -4.7740736 -4.6997805 -4.7594247 -4.9162259][-4.7049575 -4.765522 -4.7958083 -4.7756357 -4.779047 -4.8390985 -4.9189906 -5.0208964 -5.1122518 -5.0871081 -5.0249567 -4.8750768 -4.6577249 -4.6423931 -4.7978163][-4.6923385 -4.8593292 -4.9501095 -4.9193144 -4.8114834 -4.6987157 -4.6599388 -4.7478223 -4.9454494 -5.0730324 -5.16304 -5.0456681 -4.7156734 -4.5967607 -4.7053609][-4.69846 -4.9672971 -5.1315913 -5.107254 -4.8283343 -4.4138632 -4.1204052 -4.0908909 -4.4217925 -4.8431616 -5.2145233 -5.22269 -4.84536 -4.611433 -4.6180563][-4.700386 -5.0037303 -5.208076 -5.1913733 -4.7099452 -3.9433074 -3.3129587 -3.0489583 -3.49392 -4.3645124 -5.1486716 -5.3851748 -5.0492053 -4.7060485 -4.5610385][-4.763854 -5.0237684 -5.2257271 -5.2129989 -4.5444174 -3.4344585 -2.4223752 -1.7820432 -2.2486837 -3.6265171 -4.9025044 -5.4835148 -5.317832 -4.9266219 -4.6019874][-4.9106817 -5.11187 -5.3234105 -5.3574867 -4.5643291 -3.1284199 -1.6878643 -0.50664473 -0.82127213 -2.6195881 -4.3502688 -5.3505287 -5.4885588 -5.2025709 -4.7688837][-4.9227505 -5.0967436 -5.3839583 -5.5958424 -4.9113097 -3.3644161 -1.6249928 0.068243027 0.067951679 -1.8450143 -3.750391 -5.0205965 -5.451364 -5.3764625 -4.9695921][-4.6649961 -4.7817245 -5.1252651 -5.5923767 -5.2978816 -4.0386147 -2.4107764 -0.58187246 -0.26842833 -1.8642874 -3.5338528 -4.7852988 -5.33856 -5.4311714 -5.10578][-4.2465615 -4.2507687 -4.521769 -5.1184444 -5.2950311 -4.5916958 -3.4373846 -1.923547 -1.4952614 -2.5846524 -3.7637839 -4.7328396 -5.203402 -5.3327746 -5.0826793][-4.0215058 -3.8932445 -3.9112751 -4.3519678 -4.808825 -4.63923 -4.1000714 -3.1748576 -2.8625517 -3.5242834 -4.233695 -4.8285232 -5.0782709 -5.0941234 -4.8597889][-4.1673708 -3.9345899 -3.620811 -3.6988831 -4.1689439 -4.3376484 -4.3516879 -4.1026092 -4.0775228 -4.5091047 -4.8965616 -5.1516266 -5.13968 -4.9385505 -4.6277037][-4.512928 -4.2305245 -3.6986463 -3.4444511 -3.7333798 -3.962095 -4.2639761 -4.5034695 -4.7632813 -5.142354 -5.403749 -5.4730163 -5.2982821 -4.9409142 -4.5529957][-4.8645225 -4.5647216 -3.9848621 -3.5738642 -3.6516831 -3.7924252 -4.1345029 -4.5739994 -4.9702473 -5.3160443 -5.5287776 -5.5342522 -5.3290377 -4.9634261 -4.5942097][-5.1255646 -4.8306446 -4.3325067 -3.9409142 -3.8849032 -3.9287114 -4.1804509 -4.5729585 -4.9256978 -5.2003145 -5.3795786 -5.397768 -5.2581482 -4.9955177 -4.7263923]]...]
INFO - root - 2017-12-06 09:07:59.461147: step 15710, loss = 0.83, batch loss = 0.76 (13.9 examples/sec; 0.577 sec/batch; 50h:45m:06s remains)
INFO - root - 2017-12-06 09:08:05.512934: step 15720, loss = 0.96, batch loss = 0.89 (12.7 examples/sec; 0.627 sec/batch; 55h:12m:46s remains)
INFO - root - 2017-12-06 09:08:11.641538: step 15730, loss = 0.98, batch loss = 0.91 (12.7 examples/sec; 0.629 sec/batch; 55h:20m:50s remains)
INFO - root - 2017-12-06 09:08:17.761737: step 15740, loss = 1.02, batch loss = 0.95 (13.9 examples/sec; 0.575 sec/batch; 50h:38m:10s remains)
INFO - root - 2017-12-06 09:08:23.787992: step 15750, loss = 0.87, batch loss = 0.80 (13.0 examples/sec; 0.615 sec/batch; 54h:04m:13s remains)
INFO - root - 2017-12-06 09:08:29.906954: step 15760, loss = 1.01, batch loss = 0.94 (12.6 examples/sec; 0.634 sec/batch; 55h:47m:48s remains)
INFO - root - 2017-12-06 09:08:35.990584: step 15770, loss = 0.65, batch loss = 0.57 (13.3 examples/sec; 0.601 sec/batch; 52h:52m:28s remains)
INFO - root - 2017-12-06 09:08:42.179772: step 15780, loss = 1.06, batch loss = 0.99 (12.6 examples/sec; 0.636 sec/batch; 55h:56m:33s remains)
INFO - root - 2017-12-06 09:08:48.305736: step 15790, loss = 1.12, batch loss = 1.05 (13.0 examples/sec; 0.614 sec/batch; 54h:02m:28s remains)
INFO - root - 2017-12-06 09:08:54.403385: step 15800, loss = 0.91, batch loss = 0.84 (13.2 examples/sec; 0.605 sec/batch; 53h:12m:54s remains)
2017-12-06 09:08:54.950549: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8709707 -4.8797922 -4.9821825 -5.1885729 -5.3012357 -5.2170277 -5.0351443 -4.93954 -5.0656509 -5.2513924 -5.2216911 -5.01928 -4.8151441 -4.7182193 -4.6954846][-4.8301611 -4.835207 -5.0147753 -5.3153248 -5.3638592 -5.0627203 -4.7202406 -4.6241231 -4.9521918 -5.3575044 -5.3016868 -4.9250979 -4.6063581 -4.5085964 -4.4826369][-4.7097387 -4.6747813 -4.9125957 -5.286108 -5.1940269 -4.5715485 -4.039701 -3.9775617 -4.5766726 -5.2998056 -5.2890792 -4.7801065 -4.427352 -4.4270196 -4.4423447][-4.5252862 -4.4129906 -4.6789656 -5.1102276 -4.8660398 -3.9139271 -3.22089 -3.2331939 -4.1006575 -5.1979494 -5.3581972 -4.8200974 -4.5140328 -4.666049 -4.7209215][-4.3991494 -4.2153978 -4.4615955 -4.8857622 -4.4823627 -3.22046 -2.3432493 -2.386353 -3.418072 -4.8666186 -5.350224 -4.9542265 -4.7724624 -5.0662255 -5.1440134][-4.4536562 -4.2295365 -4.3905044 -4.67607 -4.065021 -2.462739 -1.2495847 -1.2112889 -2.3074076 -4.0314717 -4.9229879 -4.811357 -4.7970076 -5.1928878 -5.3230758][-4.6555138 -4.449265 -4.4698133 -4.4649553 -3.5656452 -1.5975871 0.055819035 0.18226719 -0.99871063 -2.8667023 -4.0498419 -4.2271843 -4.3923635 -4.872911 -5.1281047][-4.8145447 -4.6420584 -4.4817224 -4.0874338 -2.8786175 -0.64981055 1.3244343 1.3862605 -0.015163898 -1.9195743 -3.180388 -3.5564907 -3.8389318 -4.3391657 -4.7338381][-4.8745952 -4.7656364 -4.4768143 -3.7541251 -2.3436449 -0.13995504 1.77104 1.5985746 0.00052928925 -1.7695055 -2.8889945 -3.3419263 -3.6306181 -4.0195909 -4.4355702][-4.9432874 -4.9540758 -4.7089677 -3.8980992 -2.5037041 -0.66751242 0.7275157 0.27112579 -1.2711565 -2.665503 -3.4440227 -3.8075349 -3.9545133 -4.0945511 -4.3797364][-4.952919 -5.0839639 -5.00327 -4.3608408 -3.1948025 -1.8772821 -1.1065304 -1.7324443 -3.0368207 -3.9790044 -4.3793225 -4.5463614 -4.4778905 -4.3451114 -4.4392962][-4.8737712 -5.0616074 -5.1021185 -4.71418 -3.9244077 -3.1707749 -2.8930001 -3.4764588 -4.4215822 -4.969985 -5.085465 -5.0481505 -4.7910295 -4.4707727 -4.4166989][-4.7230988 -4.9065242 -4.9781127 -4.7868772 -4.3685851 -4.0694132 -4.0713768 -4.4863973 -5.0748448 -5.3574142 -5.3151393 -5.1297894 -4.7815866 -4.420382 -4.3089194][-4.572309 -4.729495 -4.8197303 -4.751986 -4.58075 -4.5350275 -4.6348414 -4.8768888 -5.2055116 -5.3337536 -5.2277536 -4.983674 -4.6511226 -4.3594675 -4.2639422][-4.4909225 -4.5971403 -4.69353 -4.7098489 -4.6782718 -4.7238135 -4.824379 -4.9566154 -5.1303325 -5.1773124 -5.058743 -4.8258328 -4.5663166 -4.3841758 -4.3363895]]...]
INFO - root - 2017-12-06 09:09:01.059474: step 15810, loss = 1.12, batch loss = 1.05 (13.1 examples/sec; 0.610 sec/batch; 53h:38m:36s remains)
INFO - root - 2017-12-06 09:09:07.143816: step 15820, loss = 0.94, batch loss = 0.87 (13.1 examples/sec; 0.609 sec/batch; 53h:34m:48s remains)
INFO - root - 2017-12-06 09:09:13.083984: step 15830, loss = 0.92, batch loss = 0.85 (12.9 examples/sec; 0.622 sec/batch; 54h:41m:41s remains)
INFO - root - 2017-12-06 09:09:19.118738: step 15840, loss = 0.90, batch loss = 0.83 (13.3 examples/sec; 0.604 sec/batch; 53h:06m:10s remains)
INFO - root - 2017-12-06 09:09:25.226098: step 15850, loss = 0.77, batch loss = 0.70 (12.8 examples/sec; 0.623 sec/batch; 54h:50m:10s remains)
INFO - root - 2017-12-06 09:09:31.285985: step 15860, loss = 0.88, batch loss = 0.81 (13.1 examples/sec; 0.612 sec/batch; 53h:49m:07s remains)
INFO - root - 2017-12-06 09:09:37.331120: step 15870, loss = 1.09, batch loss = 1.02 (13.6 examples/sec; 0.589 sec/batch; 51h:48m:49s remains)
INFO - root - 2017-12-06 09:09:43.342823: step 15880, loss = 0.87, batch loss = 0.80 (13.5 examples/sec; 0.592 sec/batch; 52h:03m:01s remains)
INFO - root - 2017-12-06 09:09:49.391604: step 15890, loss = 1.23, batch loss = 1.16 (13.7 examples/sec; 0.584 sec/batch; 51h:20m:48s remains)
INFO - root - 2017-12-06 09:09:55.455655: step 15900, loss = 0.99, batch loss = 0.92 (12.9 examples/sec; 0.621 sec/batch; 54h:35m:36s remains)
2017-12-06 09:09:56.007406: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8777165 -4.7672009 -4.7239208 -4.7534361 -4.7770224 -4.7818961 -4.7942529 -4.8069787 -4.8056617 -4.8411222 -4.9367781 -5.064496 -5.1562638 -5.1482906 -5.0213137][-4.9725676 -4.950099 -4.9763284 -5.0792465 -5.1209888 -5.0930638 -5.0993447 -5.132061 -5.16393 -5.239459 -5.3721161 -5.5135341 -5.5864315 -5.5340781 -5.3223152][-4.8870683 -5.0142546 -5.13439 -5.30669 -5.3378854 -5.2225251 -5.1725941 -5.2092528 -5.3104868 -5.5076847 -5.7718873 -5.9968619 -6.0949888 -6.0304461 -5.7590227][-4.518528 -4.7675605 -4.956275 -5.1552114 -5.1270156 -4.8663411 -4.6960268 -4.7180729 -4.9388394 -5.3647189 -5.9047394 -6.3561893 -6.5921435 -6.5747876 -6.2612619][-3.9760504 -4.237792 -4.4268641 -4.5899539 -4.4543304 -3.9986608 -3.6265013 -3.5900028 -3.9514327 -4.7058659 -5.6637259 -6.4879384 -6.9784365 -7.072526 -6.7328796][-3.5047755 -3.7083278 -3.8493519 -3.9073787 -3.6148672 -2.9300647 -2.2749939 -2.0811059 -2.5180125 -3.6036055 -5.0068526 -6.224874 -7.0059328 -7.2623267 -6.9616928][-3.2674425 -3.3673153 -3.4219284 -3.33451 -2.867557 -1.948323 -0.96346545 -0.49623752 -0.87850356 -2.2037463 -3.9547913 -5.4550838 -6.4655113 -6.9200945 -6.7735929][-3.2416425 -3.2459142 -3.2154779 -3.0192571 -2.4321432 -1.3486443 -0.090503216 0.65721512 0.38226366 -1.0579665 -2.9434013 -4.48893 -5.5518112 -6.1423011 -6.1924152][-3.3414516 -3.3548994 -3.3473577 -3.1836891 -2.6142836 -1.4931378 -0.1396575 0.72770023 0.55128193 -0.77145338 -2.4530973 -3.7511981 -4.6675348 -5.2700958 -5.4500017][-3.4323821 -3.6205692 -3.8108268 -3.869096 -3.4707546 -2.435905 -1.1764553 -0.3628149 -0.43245602 -1.3984795 -2.6088436 -3.4940145 -4.1734371 -4.7039695 -4.9273276][-3.4983854 -3.9128819 -4.3862114 -4.7634692 -4.6178823 -3.7417719 -2.6742744 -1.9754748 -1.9420629 -2.5263193 -3.2732775 -3.8016641 -4.2675219 -4.6877346 -4.8632312][-3.6133099 -4.1233263 -4.7766886 -5.4107552 -5.5284314 -4.8920321 -4.0948443 -3.5456972 -3.438303 -3.7392843 -4.1882873 -4.5385828 -4.8715863 -5.1580238 -5.1983738][-3.7671351 -4.1898351 -4.8484259 -5.5754342 -5.8794317 -5.5123358 -5.02402 -4.6657228 -4.5382628 -4.6660848 -4.9627523 -5.262815 -5.5319734 -5.7095971 -5.6159868][-3.9031339 -4.1346054 -4.6617532 -5.3215408 -5.6999469 -5.5716949 -5.3463326 -5.1685247 -5.0703831 -5.1192932 -5.3403893 -5.6190114 -5.8665543 -5.9939871 -5.8374004][-4.0092587 -4.0677514 -4.4155159 -4.9011941 -5.2235646 -5.229775 -5.1602097 -5.1031985 -5.06342 -5.1171489 -5.3151822 -5.5816355 -5.8278332 -5.9457512 -5.7968073]]...]
INFO - root - 2017-12-06 09:10:02.075924: step 15910, loss = 1.05, batch loss = 0.98 (13.3 examples/sec; 0.600 sec/batch; 52h:45m:23s remains)
INFO - root - 2017-12-06 09:10:08.253949: step 15920, loss = 1.14, batch loss = 1.07 (13.2 examples/sec; 0.605 sec/batch; 53h:11m:02s remains)
INFO - root - 2017-12-06 09:10:14.287352: step 15930, loss = 0.92, batch loss = 0.84 (14.1 examples/sec; 0.568 sec/batch; 49h:58m:19s remains)
INFO - root - 2017-12-06 09:10:20.248442: step 15940, loss = 1.04, batch loss = 0.97 (13.1 examples/sec; 0.611 sec/batch; 53h:45m:09s remains)
INFO - root - 2017-12-06 09:10:26.312907: step 15950, loss = 0.66, batch loss = 0.59 (13.2 examples/sec; 0.607 sec/batch; 53h:19m:47s remains)
INFO - root - 2017-12-06 09:10:32.478110: step 15960, loss = 0.68, batch loss = 0.61 (13.2 examples/sec; 0.608 sec/batch; 53h:27m:52s remains)
INFO - root - 2017-12-06 09:10:38.593655: step 15970, loss = 0.87, batch loss = 0.80 (13.2 examples/sec; 0.608 sec/batch; 53h:28m:29s remains)
INFO - root - 2017-12-06 09:10:44.715899: step 15980, loss = 0.87, batch loss = 0.80 (12.5 examples/sec; 0.639 sec/batch; 56h:09m:58s remains)
INFO - root - 2017-12-06 09:10:50.881098: step 15990, loss = 1.15, batch loss = 1.08 (12.9 examples/sec; 0.621 sec/batch; 54h:34m:38s remains)
INFO - root - 2017-12-06 09:10:56.985169: step 16000, loss = 0.86, batch loss = 0.79 (13.0 examples/sec; 0.616 sec/batch; 54h:08m:24s remains)
2017-12-06 09:10:57.504134: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2986183 -3.7389517 -3.5752859 -3.7830441 -4.0299296 -4.1763391 -4.3195629 -4.6292195 -5.201282 -5.8269076 -6.2807894 -6.30365 -5.9762273 -5.6559043 -5.3262448][-4.5446973 -4.133882 -4.0235386 -4.2518744 -4.5642786 -4.7485118 -4.7873707 -4.9080329 -5.2691789 -5.7614985 -6.1728754 -6.191813 -5.9163342 -5.669982 -5.471674][-4.8416595 -4.6443334 -4.6490641 -4.8810892 -5.1365571 -5.2103176 -5.0215616 -4.8580151 -4.9361873 -5.3049526 -5.7380452 -5.7734232 -5.4806809 -5.2119608 -5.0287104][-5.00655 -4.9480557 -5.0424013 -5.275116 -5.4552975 -5.3755965 -4.9103212 -4.4187131 -4.2100463 -4.5091071 -5.0706406 -5.1794391 -4.83928 -4.4590611 -4.1450534][-4.8682017 -4.8256073 -4.9867225 -5.2782421 -5.4635849 -5.2940912 -4.5784359 -3.7262855 -3.2222359 -3.4802647 -4.269928 -4.6101928 -4.361361 -3.9274609 -3.4137521][-4.2696471 -4.1245794 -4.3584523 -4.8037863 -5.0902119 -4.8866444 -3.9451344 -2.7306056 -1.915652 -2.150027 -3.2456346 -3.9670949 -3.9923348 -3.6671793 -3.0386705][-3.3561268 -3.0696974 -3.3652167 -4.00908 -4.4290195 -4.192749 -3.05492 -1.5110946 -0.43675184 -0.68298841 -2.1115499 -3.2456908 -3.5785036 -3.4347653 -2.8220916][-2.3684204 -1.8912714 -2.217072 -3.1089816 -3.7676344 -3.595206 -2.3778369 -0.67514777 0.53451347 0.27003193 -1.3733644 -2.798018 -3.3459878 -3.3307891 -2.7919521][-1.7024848 -0.99200368 -1.2458608 -2.3117197 -3.269063 -3.3548317 -2.3682609 -0.84889531 0.23016453 -0.041687489 -1.6063538 -3.0272346 -3.5941153 -3.5383716 -2.98796][-1.7463534 -0.85679817 -0.91258025 -1.9335968 -3.0739303 -3.5008678 -2.97008 -1.8933702 -1.1058202 -1.3435545 -2.5985813 -3.7893281 -4.2080297 -3.9739866 -3.3257194][-2.5754614 -1.7212946 -1.5661051 -2.266593 -3.2509069 -3.8329115 -3.6970067 -3.0537794 -2.5551176 -2.7480588 -3.646975 -4.5129824 -4.7155738 -4.3162289 -3.6084909][-3.5310202 -2.903513 -2.6325312 -2.8942208 -3.4599147 -3.9270034 -3.9803259 -3.6400123 -3.382086 -3.5933368 -4.2203488 -4.77288 -4.7966528 -4.3466864 -3.69044][-3.9433117 -3.588485 -3.3478785 -3.2810733 -3.4068065 -3.6471035 -3.759326 -3.6159785 -3.5410032 -3.8343182 -4.3233552 -4.6595392 -4.6016192 -4.235497 -3.7526383][-3.6737161 -3.5203962 -3.3722525 -3.1875615 -3.102376 -3.2487588 -3.4412665 -3.4501987 -3.4932559 -3.8428774 -4.2534127 -4.4668384 -4.4262261 -4.2411981 -4.00528][-3.2134776 -3.1492338 -3.0929689 -2.9920745 -2.9780293 -3.2311454 -3.563802 -3.6861446 -3.7475588 -4.0224242 -4.2990294 -4.3947825 -4.3745518 -4.3363714 -4.3121142]]...]
INFO - root - 2017-12-06 09:11:03.613281: step 16010, loss = 0.97, batch loss = 0.90 (12.9 examples/sec; 0.621 sec/batch; 54h:34m:31s remains)
INFO - root - 2017-12-06 09:11:09.664702: step 16020, loss = 0.93, batch loss = 0.86 (13.2 examples/sec; 0.606 sec/batch; 53h:17m:38s remains)
INFO - root - 2017-12-06 09:11:15.707105: step 16030, loss = 0.97, batch loss = 0.90 (13.4 examples/sec; 0.596 sec/batch; 52h:22m:06s remains)
INFO - root - 2017-12-06 09:11:21.689838: step 16040, loss = 1.18, batch loss = 1.11 (12.7 examples/sec; 0.629 sec/batch; 55h:16m:41s remains)
INFO - root - 2017-12-06 09:11:27.661491: step 16050, loss = 0.94, batch loss = 0.87 (15.5 examples/sec; 0.518 sec/batch; 45h:29m:50s remains)
INFO - root - 2017-12-06 09:11:33.783614: step 16060, loss = 0.97, batch loss = 0.89 (12.8 examples/sec; 0.626 sec/batch; 55h:03m:04s remains)
INFO - root - 2017-12-06 09:11:39.927175: step 16070, loss = 0.71, batch loss = 0.64 (13.1 examples/sec; 0.611 sec/batch; 53h:42m:31s remains)
INFO - root - 2017-12-06 09:11:46.024212: step 16080, loss = 1.03, batch loss = 0.96 (12.8 examples/sec; 0.624 sec/batch; 54h:48m:29s remains)
INFO - root - 2017-12-06 09:11:52.109356: step 16090, loss = 0.96, batch loss = 0.89 (13.5 examples/sec; 0.592 sec/batch; 52h:02m:48s remains)
INFO - root - 2017-12-06 09:11:58.238495: step 16100, loss = 0.78, batch loss = 0.71 (12.9 examples/sec; 0.620 sec/batch; 54h:31m:27s remains)
2017-12-06 09:11:58.759070: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4747014 -4.6193075 -4.6881194 -4.6951852 -4.6590433 -4.6041794 -4.5785394 -4.5714979 -4.5452404 -4.5126104 -4.5062122 -4.5243168 -4.5806894 -4.6784415 -4.7652731][-4.4691567 -4.6508331 -4.7575254 -4.8005195 -4.7999506 -4.7855248 -4.7845654 -4.78128 -4.7332826 -4.6657186 -4.6325908 -4.6153812 -4.643991 -4.7493668 -4.8739843][-4.3706579 -4.5541658 -4.6779118 -4.7596831 -4.8232961 -4.9005661 -4.9478931 -4.9484639 -4.8739057 -4.7735958 -4.7291269 -4.6614203 -4.6133118 -4.6814146 -4.830977][-4.1240144 -4.2453203 -4.3392987 -4.4477406 -4.5883522 -4.7792106 -4.8534164 -4.8425541 -4.7481265 -4.6242247 -4.5980091 -4.4953923 -4.3563643 -4.36691 -4.5383806][-3.6823273 -3.6532187 -3.63121 -3.6934924 -3.8547876 -4.1059871 -4.1379313 -4.10858 -4.0669942 -3.9924393 -4.0626354 -3.9969656 -3.8223739 -3.807879 -4.0113339][-3.1852617 -2.985641 -2.812304 -2.7524242 -2.8498759 -3.0695593 -2.960535 -2.9010987 -3.0367236 -3.1440511 -3.3795819 -3.3806915 -3.2059989 -3.2052939 -3.4481995][-2.819911 -2.4704845 -2.1394219 -1.8833225 -1.8254857 -1.9246442 -1.6225681 -1.5625947 -2.0201287 -2.4449506 -2.8572049 -2.8866768 -2.701683 -2.7199306 -2.9892063][-2.7912388 -2.364341 -1.9150436 -1.4238234 -1.1194263 -0.9988327 -0.47959948 -0.41076422 -1.2013268 -1.9664655 -2.5327406 -2.5962312 -2.4571605 -2.5445514 -2.835134][-3.11642 -2.7495856 -2.312923 -1.6695712 -1.1323678 -0.77856541 -0.10931158 -0.0059833527 -0.9096663 -1.793833 -2.36748 -2.4374964 -2.3997326 -2.613142 -2.9503117][-3.5269465 -3.3318994 -3.0381188 -2.4000258 -1.7888973 -1.328464 -0.6999557 -0.61522079 -1.3582344 -2.0616968 -2.4346409 -2.4191661 -2.4416687 -2.75102 -3.1331463][-3.7398133 -3.7527788 -3.6686015 -3.1727233 -2.6650705 -2.2536335 -1.8085763 -1.7994881 -2.2699435 -2.661386 -2.7671597 -2.6493163 -2.6826835 -3.0034204 -3.365119][-3.6581783 -3.8395791 -3.9428864 -3.6241376 -3.3082867 -3.0471931 -2.8186617 -2.8899684 -3.1048539 -3.2111375 -3.131305 -2.9794464 -3.0109749 -3.2750077 -3.5525289][-3.3522677 -3.5863276 -3.778403 -3.595592 -3.4870906 -3.437691 -3.410934 -3.5484459 -3.600728 -3.5340066 -3.3844326 -3.2493379 -3.2534354 -3.4134686 -3.5832992][-3.0801229 -3.2421863 -3.4045854 -3.2914667 -3.3517325 -3.5123024 -3.6506846 -3.8486853 -3.8542552 -3.7451906 -3.6015534 -3.4769068 -3.4201283 -3.4598517 -3.5304933][-3.1019812 -3.1356502 -3.1938086 -3.0940723 -3.2283955 -3.4976842 -3.7139654 -3.926301 -3.94247 -3.867419 -3.7749844 -3.6739151 -3.5835552 -3.5468006 -3.5551703]]...]
INFO - root - 2017-12-06 09:12:04.872945: step 16110, loss = 1.03, batch loss = 0.96 (13.3 examples/sec; 0.603 sec/batch; 53h:01m:30s remains)
INFO - root - 2017-12-06 09:12:10.949282: step 16120, loss = 0.84, batch loss = 0.77 (13.5 examples/sec; 0.593 sec/batch; 52h:06m:39s remains)
INFO - root - 2017-12-06 09:12:17.059848: step 16130, loss = 1.07, batch loss = 1.00 (13.2 examples/sec; 0.604 sec/batch; 53h:04m:17s remains)
INFO - root - 2017-12-06 09:12:23.125764: step 16140, loss = 1.28, batch loss = 1.21 (13.0 examples/sec; 0.613 sec/batch; 53h:54m:07s remains)
INFO - root - 2017-12-06 09:12:29.160678: step 16150, loss = 0.80, batch loss = 0.73 (13.0 examples/sec; 0.614 sec/batch; 53h:54m:54s remains)
INFO - root - 2017-12-06 09:12:35.235541: step 16160, loss = 0.71, batch loss = 0.64 (13.0 examples/sec; 0.617 sec/batch; 54h:13m:03s remains)
INFO - root - 2017-12-06 09:12:41.396744: step 16170, loss = 0.85, batch loss = 0.78 (12.9 examples/sec; 0.621 sec/batch; 54h:36m:23s remains)
INFO - root - 2017-12-06 09:12:47.489799: step 16180, loss = 0.91, batch loss = 0.84 (13.3 examples/sec; 0.602 sec/batch; 52h:53m:28s remains)
INFO - root - 2017-12-06 09:12:53.564930: step 16190, loss = 1.07, batch loss = 1.00 (13.2 examples/sec; 0.608 sec/batch; 53h:24m:18s remains)
INFO - root - 2017-12-06 09:12:59.560695: step 16200, loss = 0.82, batch loss = 0.75 (16.1 examples/sec; 0.497 sec/batch; 43h:39m:36s remains)
2017-12-06 09:13:00.058077: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4692106 -6.3626933 -6.238409 -6.1542673 -5.9692917 -5.6939487 -5.4312639 -5.328536 -5.4063468 -5.6002278 -5.7659159 -5.9531941 -5.9893522 -5.782505 -5.6067691][-6.23991 -6.2850003 -6.3063679 -6.23309 -5.9241605 -5.4347448 -4.9845386 -4.9216175 -5.0994654 -5.3218203 -5.6018548 -6.078763 -6.4139471 -6.1857929 -5.8742504][-5.7295933 -5.7933555 -5.7694426 -5.5876842 -5.1021957 -4.3971081 -3.8201694 -3.8563709 -4.2118058 -4.5412092 -4.9223762 -5.6182494 -6.2240114 -6.070406 -5.7595444][-5.1308603 -5.1662917 -4.9804573 -4.5753574 -3.8528664 -2.9365721 -2.2531321 -2.3181074 -2.8208995 -3.3122342 -3.82774 -4.6946754 -5.5516863 -5.5759516 -5.3847866][-4.7715955 -4.7705193 -4.3583136 -3.6775856 -2.7141504 -1.6198077 -0.84215689 -0.8360436 -1.4440386 -2.1648011 -2.8461902 -3.7701721 -4.7211962 -4.9233856 -4.8972931][-4.67206 -4.6574588 -4.0696654 -3.2052422 -2.1428432 -1.0116577 -0.14162683 0.07793045 -0.52400875 -1.4852469 -2.37114 -3.2512972 -4.0773015 -4.3561058 -4.4375248][-4.7436838 -4.7718344 -4.1465311 -3.2424126 -2.2652595 -1.2984314 -0.41359234 0.060406208 -0.39807749 -1.4567182 -2.4835517 -3.2443004 -3.7864089 -4.0080671 -4.0854788][-4.9010062 -5.0099053 -4.4754663 -3.6232193 -2.8304808 -2.1600583 -1.4222806 -0.88172174 -1.2141912 -2.1961267 -3.1889544 -3.7351267 -3.906374 -3.9630744 -3.934896][-4.9974413 -5.1980419 -4.8361707 -4.1148562 -3.5430207 -3.1562238 -2.6419587 -2.2435219 -2.5454705 -3.3822291 -4.2262549 -4.5241795 -4.3279047 -4.1447473 -3.9564536][-5.0565062 -5.3032026 -5.0900455 -4.5147939 -4.1435046 -3.9850249 -3.7171462 -3.5490911 -3.8781102 -4.5809588 -5.215301 -5.2758641 -4.7928953 -4.3205223 -3.9179835][-5.1021576 -5.3894739 -5.2791052 -4.8236728 -4.5767407 -4.5535731 -4.4747334 -4.4743223 -4.797627 -5.367197 -5.7898111 -5.7208772 -5.14771 -4.4354091 -3.8197451][-5.2253451 -5.5988574 -5.5876088 -5.2321014 -5.0289311 -5.0048561 -4.965631 -4.9744959 -5.1935453 -5.6199069 -5.8999715 -5.8315754 -5.3802466 -4.6205959 -3.9165049][-5.3707738 -5.817802 -5.9188232 -5.6887803 -5.5185671 -5.4604158 -5.3955712 -5.3270659 -5.3975787 -5.6960831 -5.8973255 -5.8651266 -5.59608 -4.9765759 -4.3528795][-5.3501215 -5.8034711 -6.0089192 -5.9130607 -5.8001561 -5.7514153 -5.6987638 -5.5995564 -5.5825095 -5.7825928 -5.8883228 -5.8233614 -5.6675472 -5.3061132 -4.92941][-5.1209908 -5.540493 -5.8688731 -5.9098382 -5.8881235 -5.8895125 -5.8948755 -5.8315315 -5.7741332 -5.85887 -5.83687 -5.6759295 -5.5526733 -5.4683781 -5.411006]]...]
INFO - root - 2017-12-06 09:13:06.149077: step 16210, loss = 0.82, batch loss = 0.75 (12.7 examples/sec; 0.632 sec/batch; 55h:30m:47s remains)
INFO - root - 2017-12-06 09:13:12.246617: step 16220, loss = 1.02, batch loss = 0.95 (13.6 examples/sec; 0.587 sec/batch; 51h:31m:49s remains)
INFO - root - 2017-12-06 09:13:18.328032: step 16230, loss = 1.05, batch loss = 0.98 (13.0 examples/sec; 0.614 sec/batch; 53h:57m:13s remains)
INFO - root - 2017-12-06 09:13:24.413043: step 16240, loss = 0.75, batch loss = 0.68 (13.1 examples/sec; 0.610 sec/batch; 53h:34m:34s remains)
INFO - root - 2017-12-06 09:13:30.542124: step 16250, loss = 0.94, batch loss = 0.87 (13.6 examples/sec; 0.587 sec/batch; 51h:33m:02s remains)
INFO - root - 2017-12-06 09:13:36.488396: step 16260, loss = 0.76, batch loss = 0.69 (12.9 examples/sec; 0.618 sec/batch; 54h:16m:51s remains)
INFO - root - 2017-12-06 09:13:42.610949: step 16270, loss = 0.96, batch loss = 0.89 (13.3 examples/sec; 0.600 sec/batch; 52h:42m:37s remains)
INFO - root - 2017-12-06 09:13:48.743799: step 16280, loss = 0.85, batch loss = 0.78 (12.9 examples/sec; 0.621 sec/batch; 54h:30m:37s remains)
INFO - root - 2017-12-06 09:13:54.878142: step 16290, loss = 0.99, batch loss = 0.92 (12.7 examples/sec; 0.631 sec/batch; 55h:23m:47s remains)
INFO - root - 2017-12-06 09:14:00.976134: step 16300, loss = 0.77, batch loss = 0.70 (13.3 examples/sec; 0.602 sec/batch; 52h:55m:01s remains)
2017-12-06 09:14:01.468150: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0714569 -5.0701771 -5.0195508 -5.0522366 -5.1342769 -5.1281919 -5.0426922 -5.023406 -5.0975728 -5.1733036 -5.2015896 -5.1834335 -5.123425 -5.0156145 -4.8751531][-5.313971 -5.2629495 -5.1081295 -5.0770955 -5.1627297 -5.1499481 -5.0254407 -5.0256443 -5.1790681 -5.325892 -5.3837614 -5.3807869 -5.3543286 -5.2932739 -5.1820669][-5.4789519 -5.3714285 -5.0729313 -4.8858886 -4.8624215 -4.7649326 -4.5905452 -4.6791234 -4.9967003 -5.264317 -5.3593411 -5.3576317 -5.3478565 -5.3352871 -5.2850046][-5.6269059 -5.5038123 -5.0644469 -4.6399164 -4.3404441 -3.9898431 -3.7038016 -3.9605722 -4.5702953 -5.0458064 -5.2270751 -5.2523813 -5.23635 -5.2111449 -5.1807823][-5.7476296 -5.6753635 -5.14768 -4.4765072 -3.7949862 -3.0461278 -2.5757124 -3.0531445 -4.0610223 -4.8188457 -5.138587 -5.2470517 -5.2303052 -5.1166348 -5.0141287][-5.7488074 -5.7242017 -5.1286526 -4.2240057 -3.1231759 -1.9083216 -1.2186575 -1.9283216 -3.3798447 -4.461802 -4.9760842 -5.2418685 -5.2875013 -5.0894413 -4.8649082][-5.6314774 -5.6032395 -4.9615884 -3.8769841 -2.3901896 -0.72078538 0.20738745 -0.647444 -2.4782338 -3.9101899 -4.7021532 -5.2134337 -5.4103141 -5.1960273 -4.8553643][-5.5187349 -5.4297929 -4.7928157 -3.6501582 -1.9516933 0.0070447922 1.1162882 0.26783848 -1.7206428 -3.4126866 -4.4831734 -5.2449446 -5.6216688 -5.449892 -5.01519][-5.4421177 -5.2923579 -4.7684174 -3.7849083 -2.2088933 -0.30970812 0.82645035 0.17513227 -1.6282318 -3.3333359 -4.5223427 -5.3979664 -5.8706369 -5.7350864 -5.2127228][-5.2015848 -5.075551 -4.8169994 -4.2508497 -3.1722155 -1.716321 -0.74750662 -1.0983617 -2.4230914 -3.8393483 -4.8702831 -5.6345024 -6.0605412 -5.9223318 -5.3287759][-4.6575146 -4.642221 -4.7328711 -4.7019906 -4.3079486 -3.5072284 -2.8325028 -2.9119251 -3.65653 -4.5746474 -5.2381344 -5.7494168 -6.0484581 -5.8875628 -5.2684417][-3.9597116 -4.0626965 -4.4383259 -4.8353767 -5.0185475 -4.8154826 -4.4420342 -4.3487659 -4.619472 -5.0521636 -5.3488417 -5.6254878 -5.810523 -5.6424041 -5.0721607][-3.3643813 -3.5218163 -4.0071788 -4.5947585 -5.0937624 -5.2815723 -5.1581917 -5.0219851 -5.0208669 -5.1074581 -5.1477718 -5.2673497 -5.3732586 -5.2337275 -4.797184][-3.0235019 -3.1304619 -3.5329247 -4.0802441 -4.6354585 -4.9759617 -5.0115461 -4.922297 -4.8346357 -4.7553325 -4.6606975 -4.6793332 -4.7307267 -4.6510873 -4.4035506][-2.9808614 -2.9634261 -3.1602449 -3.512795 -3.9440169 -4.2637796 -4.3680558 -4.3427157 -4.2672544 -4.1582541 -4.0310969 -4.0013318 -4.0290613 -4.0222492 -3.9682264]]...]
INFO - root - 2017-12-06 09:14:07.653256: step 16310, loss = 0.86, batch loss = 0.79 (12.9 examples/sec; 0.618 sec/batch; 54h:17m:39s remains)
INFO - root - 2017-12-06 09:14:13.697629: step 16320, loss = 0.71, batch loss = 0.64 (12.8 examples/sec; 0.624 sec/batch; 54h:47m:12s remains)
INFO - root - 2017-12-06 09:14:19.816897: step 16330, loss = 0.96, batch loss = 0.89 (13.0 examples/sec; 0.617 sec/batch; 54h:13m:10s remains)
INFO - root - 2017-12-06 09:14:25.878967: step 16340, loss = 1.20, batch loss = 1.13 (13.6 examples/sec; 0.587 sec/batch; 51h:32m:24s remains)
INFO - root - 2017-12-06 09:14:31.900536: step 16350, loss = 0.84, batch loss = 0.77 (14.6 examples/sec; 0.548 sec/batch; 48h:09m:51s remains)
INFO - root - 2017-12-06 09:14:37.833850: step 16360, loss = 1.15, batch loss = 1.08 (13.4 examples/sec; 0.597 sec/batch; 52h:27m:17s remains)
INFO - root - 2017-12-06 09:14:43.926362: step 16370, loss = 0.79, batch loss = 0.72 (12.7 examples/sec; 0.632 sec/batch; 55h:27m:22s remains)
INFO - root - 2017-12-06 09:14:50.084876: step 16380, loss = 1.17, batch loss = 1.10 (12.7 examples/sec; 0.629 sec/batch; 55h:15m:32s remains)
INFO - root - 2017-12-06 09:14:56.174066: step 16390, loss = 0.90, batch loss = 0.83 (13.1 examples/sec; 0.609 sec/batch; 53h:28m:55s remains)
INFO - root - 2017-12-06 09:15:02.253816: step 16400, loss = 0.79, batch loss = 0.72 (13.4 examples/sec; 0.597 sec/batch; 52h:27m:49s remains)
2017-12-06 09:15:02.807889: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3041463 -4.4106717 -4.1867476 -3.7703459 -3.436085 -3.0736508 -2.8816233 -3.1635442 -3.8125017 -4.3525629 -4.3391356 -4.0153265 -3.5956359 -3.2493734 -3.3499718][-4.59906 -4.6001568 -4.2535763 -3.6295795 -3.1078844 -2.7206669 -2.7020278 -3.152544 -3.8528237 -4.4109445 -4.4521766 -4.19673 -3.7985981 -3.3994961 -3.3846495][-4.8537989 -4.8014765 -4.4720335 -3.8135648 -3.1521344 -2.6952155 -2.7837021 -3.3673534 -4.0298529 -4.5099006 -4.5537162 -4.3670754 -4.0215273 -3.5896215 -3.4498045][-4.8388877 -4.801507 -4.5975809 -4.0415311 -3.2288604 -2.5313742 -2.5464544 -3.2206106 -3.9240303 -4.4532609 -4.57326 -4.4745913 -4.2070704 -3.7901638 -3.5583887][-4.5338979 -4.5211539 -4.4494047 -4.0203152 -3.0263071 -1.8780847 -1.5643225 -2.2965105 -3.2863836 -4.1554174 -4.5139222 -4.5539379 -4.3839774 -4.010066 -3.7200096][-4.0890102 -4.0391617 -4.0279546 -3.663197 -2.4706383 -0.80241704 -0.055019855 -0.91968679 -2.4222116 -3.8371246 -4.5355115 -4.7223058 -4.6008363 -4.2119446 -3.8729706][-3.8844697 -3.7521973 -3.7283289 -3.3788185 -2.0804176 -0.098433495 0.93385077 -0.099030018 -2.0186379 -3.8375211 -4.772861 -5.0017972 -4.7996354 -4.3071771 -3.9392295][-4.1084881 -3.9445202 -3.878263 -3.543679 -2.3660569 -0.53740072 0.44336224 -0.53089571 -2.4346685 -4.2736397 -5.207984 -5.3260908 -4.92072 -4.281919 -3.9083147][-4.5303979 -4.3769889 -4.2631221 -3.9566939 -3.0759034 -1.7151768 -0.96419549 -1.6901584 -3.2166367 -4.7835169 -5.5637913 -5.5252838 -4.9293423 -4.177577 -3.8121834][-4.7403364 -4.6073365 -4.4793644 -4.2363486 -3.6989088 -2.8274775 -2.2830286 -2.6640429 -3.68248 -4.8892312 -5.5186672 -5.4343328 -4.8076162 -4.0465269 -3.6962831][-4.5287604 -4.4380364 -4.3460197 -4.1886916 -3.911221 -3.3744049 -2.9257541 -2.9702058 -3.5160015 -4.4015365 -4.9889932 -5.0429087 -4.5850887 -3.9257498 -3.6115375][-4.1226254 -4.0824089 -4.0153828 -3.8673306 -3.6482172 -3.2147548 -2.7633121 -2.5604277 -2.7825494 -3.4821434 -4.1726332 -4.5292964 -4.3690882 -3.8795 -3.6130643][-3.7825928 -3.7996979 -3.7381811 -3.5361037 -3.24687 -2.79767 -2.384728 -2.1315873 -2.2258596 -2.8465886 -3.6714666 -4.3070531 -4.3879323 -4.0188093 -3.7601426][-3.5120363 -3.5806286 -3.5630853 -3.365541 -3.0474916 -2.6234825 -2.3501701 -2.2392466 -2.3599157 -2.9190612 -3.7443202 -4.4607191 -4.6206813 -4.2820649 -3.9955883][-3.3275576 -3.4289751 -3.4850128 -3.3719463 -3.1325202 -2.7945971 -2.6640739 -2.7203388 -2.9033008 -3.368875 -4.0535307 -4.679677 -4.8184743 -4.4970689 -4.2141671]]...]
INFO - root - 2017-12-06 09:15:08.866946: step 16410, loss = 0.88, batch loss = 0.81 (12.9 examples/sec; 0.619 sec/batch; 54h:20m:39s remains)
INFO - root - 2017-12-06 09:15:14.941102: step 16420, loss = 0.86, batch loss = 0.79 (13.7 examples/sec; 0.586 sec/batch; 51h:27m:01s remains)
INFO - root - 2017-12-06 09:15:21.084680: step 16430, loss = 0.76, batch loss = 0.69 (13.0 examples/sec; 0.615 sec/batch; 53h:59m:15s remains)
INFO - root - 2017-12-06 09:15:27.261755: step 16440, loss = 0.88, batch loss = 0.81 (12.6 examples/sec; 0.636 sec/batch; 55h:50m:29s remains)
INFO - root - 2017-12-06 09:15:33.346354: step 16450, loss = 0.79, batch loss = 0.72 (13.8 examples/sec; 0.581 sec/batch; 50h:59m:21s remains)
INFO - root - 2017-12-06 09:15:39.554448: step 16460, loss = 0.94, batch loss = 0.87 (12.9 examples/sec; 0.618 sec/batch; 54h:14m:53s remains)
INFO - root - 2017-12-06 09:15:45.526818: step 16470, loss = 0.95, batch loss = 0.88 (13.4 examples/sec; 0.596 sec/batch; 52h:21m:48s remains)
INFO - root - 2017-12-06 09:15:51.632999: step 16480, loss = 0.92, batch loss = 0.85 (12.8 examples/sec; 0.623 sec/batch; 54h:43m:51s remains)
INFO - root - 2017-12-06 09:15:57.764355: step 16490, loss = 0.91, batch loss = 0.84 (12.8 examples/sec; 0.625 sec/batch; 54h:49m:15s remains)
INFO - root - 2017-12-06 09:16:03.925482: step 16500, loss = 1.20, batch loss = 1.13 (13.4 examples/sec; 0.595 sec/batch; 52h:14m:42s remains)
2017-12-06 09:16:04.407133: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7980323 -3.8332675 -3.9225352 -4.017312 -4.070672 -4.1230979 -4.2182221 -4.3182006 -4.3509636 -4.2310061 -4.077178 -3.9552193 -3.8650918 -3.7671647 -3.679285][-4.2991667 -4.3908668 -4.434773 -4.4239793 -4.35238 -4.2806211 -4.2827559 -4.3765144 -4.4829235 -4.4977274 -4.5281277 -4.6137156 -4.6913562 -4.6520567 -4.5292664][-4.7101288 -4.8610859 -4.8733273 -4.7692246 -4.5603557 -4.3053479 -4.1284595 -4.1467295 -4.2837381 -4.4078712 -4.6135168 -4.9251966 -5.2461743 -5.3450985 -5.2030573][-4.6946497 -4.873014 -4.8916826 -4.7939591 -4.5592356 -4.194181 -3.8728209 -3.7921267 -3.8667741 -3.9613585 -4.182447 -4.5590024 -5.0277567 -5.2688551 -5.1672487][-4.1063714 -4.2787428 -4.3513746 -4.3622131 -4.2054286 -3.8314917 -3.4613359 -3.3211617 -3.2771852 -3.2200642 -3.3208568 -3.6164594 -4.0998354 -4.4503117 -4.4863267][-3.3319991 -3.4141808 -3.5086322 -3.630352 -3.5803201 -3.2765002 -2.963901 -2.8485904 -2.7302957 -2.5193739 -2.4659007 -2.5985982 -2.9773288 -3.3610468 -3.5476811][-2.7961745 -2.7693305 -2.8678598 -3.0655456 -3.1052108 -2.8960943 -2.6921487 -2.6495974 -2.518527 -2.211904 -2.0010183 -1.8915057 -2.0317829 -2.3661039 -2.6948075][-2.6063604 -2.5325334 -2.6412554 -2.8623295 -2.9543633 -2.8426538 -2.7612069 -2.7948213 -2.7141891 -2.4292612 -2.1185691 -1.739886 -1.5740538 -1.794946 -2.1873217][-2.6792734 -2.6034369 -2.6906486 -2.8802388 -2.9974611 -2.9895623 -3.0126197 -3.1128354 -3.1425071 -3.0094664 -2.7123082 -2.1486292 -1.7049892 -1.7447274 -2.047368][-2.8944511 -2.810035 -2.841188 -2.9825497 -3.105093 -3.1428654 -3.1964693 -3.3333058 -3.4805183 -3.506402 -3.2400427 -2.5778875 -1.9840617 -1.8854237 -2.0612044][-3.1095772 -2.9667048 -2.9225311 -3.0522521 -3.2129626 -3.2758384 -3.305809 -3.4309273 -3.6225157 -3.6920266 -3.3878689 -2.7081094 -2.1455472 -2.0199125 -2.1062031][-3.2589874 -3.09191 -3.0279412 -3.2121544 -3.4669056 -3.5755103 -3.5388799 -3.5701241 -3.6830854 -3.6571977 -3.248754 -2.5941434 -2.1861691 -2.16541 -2.284174][-3.4323103 -3.3561289 -3.3462319 -3.5711091 -3.8930416 -4.0235572 -3.8889208 -3.7638426 -3.7144041 -3.4983191 -2.9761496 -2.4142368 -2.2561996 -2.4448409 -2.7025921][-3.6079504 -3.6250546 -3.653023 -3.8633513 -4.232461 -4.4067259 -4.2419186 -4.0354433 -3.8759694 -3.5115848 -2.9481549 -2.5273445 -2.5876842 -2.9654188 -3.3745975][-3.6275778 -3.7035141 -3.7627158 -3.982527 -4.4303656 -4.6869135 -4.5707417 -4.3932981 -4.2374573 -3.8265686 -3.3072171 -3.0166149 -3.1663456 -3.6184814 -4.1301641]]...]
INFO - root - 2017-12-06 09:16:10.486844: step 16510, loss = 0.96, batch loss = 0.89 (12.9 examples/sec; 0.619 sec/batch; 54h:17m:54s remains)
INFO - root - 2017-12-06 09:16:16.522647: step 16520, loss = 1.11, batch loss = 1.04 (13.4 examples/sec; 0.598 sec/batch; 52h:28m:55s remains)
INFO - root - 2017-12-06 09:16:22.618526: step 16530, loss = 0.90, batch loss = 0.83 (13.2 examples/sec; 0.608 sec/batch; 53h:22m:03s remains)
INFO - root - 2017-12-06 09:16:28.663692: step 16540, loss = 1.01, batch loss = 0.94 (13.0 examples/sec; 0.616 sec/batch; 54h:02m:37s remains)
INFO - root - 2017-12-06 09:16:34.774529: step 16550, loss = 0.82, batch loss = 0.75 (13.7 examples/sec; 0.584 sec/batch; 51h:17m:32s remains)
INFO - root - 2017-12-06 09:16:40.868752: step 16560, loss = 0.99, batch loss = 0.92 (13.3 examples/sec; 0.603 sec/batch; 52h:57m:16s remains)
INFO - root - 2017-12-06 09:16:46.966800: step 16570, loss = 0.85, batch loss = 0.78 (13.5 examples/sec; 0.591 sec/batch; 51h:54m:22s remains)
INFO - root - 2017-12-06 09:16:52.925743: step 16580, loss = 0.89, batch loss = 0.82 (12.6 examples/sec; 0.634 sec/batch; 55h:39m:38s remains)
INFO - root - 2017-12-06 09:16:59.063470: step 16590, loss = 0.88, batch loss = 0.81 (12.4 examples/sec; 0.648 sec/batch; 56h:49m:57s remains)
INFO - root - 2017-12-06 09:17:05.168591: step 16600, loss = 0.84, batch loss = 0.77 (13.7 examples/sec; 0.585 sec/batch; 51h:21m:27s remains)
2017-12-06 09:17:05.704340: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8343816 -5.0079241 -4.5250425 -4.1863084 -3.1901917 -2.2296507 -2.607338 -3.8688846 -4.4035273 -4.2013216 -3.6054366 -2.7837734 -2.3698332 -2.5220726 -2.7456679][-6.0564108 -5.4661841 -5.2418981 -5.0517774 -3.9840162 -2.7902284 -2.9220304 -3.9987266 -4.525249 -4.5113773 -3.9851868 -3.1463423 -2.7728035 -2.904465 -2.8941097][-5.9443407 -5.6376452 -5.7380934 -5.7841897 -4.7901258 -3.5837111 -3.5928686 -4.3943753 -4.7858677 -4.8997817 -4.5138545 -3.7860141 -3.4852343 -3.527415 -3.2717555][-5.5937719 -5.5452361 -5.9537916 -6.2332268 -5.3695059 -4.2869143 -4.2218041 -4.6449962 -4.8352714 -5.0806279 -4.9522972 -4.4714546 -4.315609 -4.3224564 -3.9188154][-5.0526495 -5.146009 -5.7215872 -6.0805511 -5.2719164 -4.3176789 -4.1505408 -4.1089211 -4.0772762 -4.5581737 -4.8113971 -4.6670322 -4.7616863 -4.8640871 -4.4383135][-4.4106479 -4.4366503 -4.9202871 -5.1177979 -4.2349892 -3.3347383 -3.0358658 -2.5214663 -2.2743495 -3.0827527 -3.8299007 -4.0678692 -4.491159 -4.8759251 -4.6018162][-3.82571 -3.6320856 -3.88033 -3.8575377 -2.899528 -2.0235684 -1.569102 -0.61279416 -0.15850878 -1.3816378 -2.693903 -3.2635241 -3.9179802 -4.5692778 -4.5229516][-3.5451975 -3.158443 -3.2100892 -3.0527787 -2.1145098 -1.2863226 -0.70713568 0.57721472 1.216269 -0.35200882 -2.1917284 -2.9805508 -3.6373215 -4.3685532 -4.4980807][-3.5262895 -3.0579343 -3.0728374 -2.9729266 -2.249094 -1.6428585 -1.1580026 0.094069004 0.77970886 -0.70969605 -2.5923264 -3.2969379 -3.7469521 -4.4252086 -4.6783333][-3.4609013 -2.937788 -3.0008533 -3.1313953 -2.8282645 -2.6757941 -2.5632184 -1.6670799 -1.0617616 -2.1436875 -3.591794 -3.966002 -4.12097 -4.690011 -5.0346036][-3.2358828 -2.584527 -2.6208973 -2.978569 -3.1228871 -3.4695342 -3.9009256 -3.5618234 -3.1154983 -3.7358966 -4.5333567 -4.4717369 -4.3084373 -4.6600666 -5.0115457][-3.1375351 -2.2606363 -2.0747979 -2.4455156 -2.8068998 -3.4753313 -4.4814358 -4.7941957 -4.5726914 -4.82974 -5.0297437 -4.5321951 -4.0536556 -4.1235495 -4.3896275][-3.4972925 -2.3978267 -1.8738587 -2.0215456 -2.3012137 -3.0049107 -4.3922114 -5.2641573 -5.2830639 -5.3126745 -5.0983057 -4.2907267 -3.6101446 -3.4923909 -3.6594415][-4.2362947 -3.0766759 -2.3084202 -2.1629879 -2.19291 -2.6779227 -4.0306158 -5.1238613 -5.2708569 -5.15748 -4.7565017 -3.9008088 -3.2639945 -3.1709952 -3.3181338][-4.9573426 -3.9370596 -3.1433437 -2.8090954 -2.5923302 -2.7458646 -3.7250328 -4.6722021 -4.7944388 -4.575809 -4.1594567 -3.4799869 -3.0451288 -3.0498457 -3.2345128]]...]
INFO - root - 2017-12-06 09:17:11.804421: step 16610, loss = 0.85, batch loss = 0.78 (13.0 examples/sec; 0.616 sec/batch; 54h:04m:09s remains)
INFO - root - 2017-12-06 09:17:17.898604: step 16620, loss = 1.03, batch loss = 0.96 (12.9 examples/sec; 0.619 sec/batch; 54h:17m:55s remains)
INFO - root - 2017-12-06 09:17:23.907393: step 16630, loss = 0.97, batch loss = 0.90 (13.2 examples/sec; 0.607 sec/batch; 53h:15m:22s remains)
INFO - root - 2017-12-06 09:17:30.122194: step 16640, loss = 0.81, batch loss = 0.74 (13.3 examples/sec; 0.602 sec/batch; 52h:50m:32s remains)
INFO - root - 2017-12-06 09:17:36.155412: step 16650, loss = 1.02, batch loss = 0.94 (13.0 examples/sec; 0.616 sec/batch; 54h:01m:41s remains)
INFO - root - 2017-12-06 09:17:42.138891: step 16660, loss = 1.06, batch loss = 0.99 (13.4 examples/sec; 0.599 sec/batch; 52h:33m:24s remains)
INFO - root - 2017-12-06 09:17:48.231210: step 16670, loss = 0.94, batch loss = 0.87 (13.4 examples/sec; 0.598 sec/batch; 52h:28m:48s remains)
INFO - root - 2017-12-06 09:17:54.206366: step 16680, loss = 0.71, batch loss = 0.64 (13.9 examples/sec; 0.577 sec/batch; 50h:34m:49s remains)
INFO - root - 2017-12-06 09:18:00.304765: step 16690, loss = 0.78, batch loss = 0.71 (12.8 examples/sec; 0.623 sec/batch; 54h:38m:56s remains)
INFO - root - 2017-12-06 09:18:06.463037: step 16700, loss = 0.76, batch loss = 0.69 (12.9 examples/sec; 0.622 sec/batch; 54h:35m:36s remains)
2017-12-06 09:18:07.005053: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9638042 -4.1093578 -4.3244438 -4.3856096 -4.2331924 -3.9878447 -3.83504 -3.8737268 -3.9886208 -4.0631056 -4.0312853 -3.8846016 -3.6636381 -3.526999 -3.6330996][-3.9794538 -4.1357708 -4.3638363 -4.4433527 -4.3227949 -4.1080556 -3.9872286 -4.0643754 -4.2158728 -4.3156004 -4.2830505 -4.1110277 -3.8660667 -3.7102833 -3.7810764][-4.200119 -4.3908648 -4.6083345 -4.6731954 -4.5730195 -4.3925581 -4.3027759 -4.4059339 -4.5722914 -4.6796031 -4.6440744 -4.4801297 -4.2825456 -4.1616187 -4.1962409][-4.5097857 -4.7192326 -4.85993 -4.8203788 -4.63456 -4.3724661 -4.198338 -4.2663188 -4.4707189 -4.6620593 -4.6927204 -4.5688429 -4.447566 -4.395256 -4.4266996][-4.7659836 -4.9739628 -5.0223708 -4.8559027 -4.5272121 -4.0887747 -3.6718943 -3.6115165 -3.9665949 -4.482779 -4.778986 -4.7404017 -4.6408229 -4.6128764 -4.6307325][-4.8286014 -4.9572082 -4.8795948 -4.5702071 -4.0882196 -3.4503884 -2.6478631 -2.3442092 -2.9490218 -3.9944961 -4.7155995 -4.7741828 -4.6176448 -4.5760326 -4.60251][-4.7337027 -4.7078466 -4.45421 -3.9597373 -3.2988291 -2.4196188 -1.109622 -0.4667232 -1.3530996 -2.985652 -4.2096891 -4.430738 -4.2433066 -4.1944771 -4.2088389][-4.73924 -4.6559319 -4.35546 -3.7821403 -3.0203376 -1.8921807 -0.035473824 1.0277987 0.019037724 -2.0759556 -3.7733178 -4.1996455 -4.0391483 -3.979178 -3.9069512][-4.9571791 -4.9580784 -4.7864466 -4.350738 -3.7126527 -2.5112643 -0.32636356 1.0898504 0.30893421 -1.848202 -3.7562535 -4.2964921 -4.1584234 -4.0335622 -3.7999666][-5.2426605 -5.3369236 -5.280704 -5.0429864 -4.6499462 -3.5538201 -1.3551424 0.24619913 -0.13647366 -2.0278411 -3.8569856 -4.4189191 -4.3381763 -4.1940923 -3.9025259][-5.5141335 -5.66998 -5.6508074 -5.541317 -5.3505192 -4.4592462 -2.5227795 -0.96192694 -0.99267507 -2.454318 -4.002532 -4.5410323 -4.5513434 -4.4280539 -4.1804318][-5.6918259 -5.9219646 -5.9327011 -5.8870482 -5.8175611 -5.213974 -3.7883511 -2.5386477 -2.4053066 -3.3970127 -4.4998336 -4.9007921 -4.9148726 -4.7761779 -4.564126][-5.6169252 -5.9343767 -6.0245037 -6.0464473 -6.06981 -5.7806997 -4.9561286 -4.1587625 -4.0298815 -4.6217818 -5.263133 -5.4510355 -5.3881536 -5.2024422 -4.9882545][-5.227643 -5.5667982 -5.7323866 -5.8232589 -5.9145012 -5.8582273 -5.4972067 -5.0799165 -4.9923739 -5.2863994 -5.5815415 -5.6186771 -5.5201025 -5.3354774 -5.1305194][-4.7303205 -5.0090971 -5.196435 -5.3245645 -5.4424338 -5.4988041 -5.3919578 -5.21463 -5.1701775 -5.2915144 -5.3922644 -5.3576875 -5.2545438 -5.0966187 -4.9285936]]...]
INFO - root - 2017-12-06 09:18:13.143387: step 16710, loss = 1.03, batch loss = 0.96 (12.9 examples/sec; 0.620 sec/batch; 54h:23m:22s remains)
INFO - root - 2017-12-06 09:18:19.280405: step 16720, loss = 0.79, batch loss = 0.72 (12.8 examples/sec; 0.626 sec/batch; 54h:55m:37s remains)
INFO - root - 2017-12-06 09:18:25.296693: step 16730, loss = 0.98, batch loss = 0.91 (13.2 examples/sec; 0.606 sec/batch; 53h:10m:34s remains)
INFO - root - 2017-12-06 09:18:31.424177: step 16740, loss = 1.08, batch loss = 1.01 (13.1 examples/sec; 0.609 sec/batch; 53h:24m:44s remains)
INFO - root - 2017-12-06 09:18:37.474053: step 16750, loss = 0.81, batch loss = 0.74 (13.0 examples/sec; 0.616 sec/batch; 54h:02m:16s remains)
INFO - root - 2017-12-06 09:18:43.546286: step 16760, loss = 0.90, batch loss = 0.83 (13.0 examples/sec; 0.618 sec/batch; 54h:10m:48s remains)
INFO - root - 2017-12-06 09:18:49.616147: step 16770, loss = 0.87, batch loss = 0.80 (12.9 examples/sec; 0.621 sec/batch; 54h:29m:27s remains)
INFO - root - 2017-12-06 09:18:55.698542: step 16780, loss = 0.97, batch loss = 0.90 (13.2 examples/sec; 0.605 sec/batch; 53h:02m:05s remains)
INFO - root - 2017-12-06 09:19:01.689293: step 16790, loss = 1.15, batch loss = 1.08 (13.4 examples/sec; 0.596 sec/batch; 52h:14m:37s remains)
INFO - root - 2017-12-06 09:19:07.823778: step 16800, loss = 0.71, batch loss = 0.64 (13.7 examples/sec; 0.586 sec/batch; 51h:22m:09s remains)
2017-12-06 09:19:08.407122: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6792569 -4.7528644 -4.67913 -4.69792 -4.91087 -5.0376606 -4.8777819 -4.5774655 -4.4241605 -4.4801712 -4.7412896 -4.6085076 -4.3260694 -4.3340297 -4.1573982][-4.7998004 -4.9153504 -4.8389406 -4.8246017 -5.0283718 -5.1432476 -5.000597 -4.7677331 -4.671402 -4.7360721 -5.0964456 -5.0097413 -4.6558952 -4.5674934 -4.3014846][-4.7117376 -4.8619184 -4.8324261 -4.8169394 -4.9699183 -5.0140224 -4.8719959 -4.7670789 -4.79335 -4.8499451 -5.2641764 -5.2926631 -4.9830351 -4.7915053 -4.3818483][-4.5626073 -4.7340703 -4.7883334 -4.7734022 -4.8085432 -4.6755176 -4.4322171 -4.4262009 -4.6158977 -4.7158132 -5.1404157 -5.3113604 -5.16001 -4.8844953 -4.3081088][-4.5042639 -4.6623726 -4.7806025 -4.7024045 -4.5071459 -4.0421844 -3.5573874 -3.5938826 -4.0163994 -4.2906041 -4.7135282 -5.0033283 -5.0851502 -4.7825875 -4.0443449][-4.5220575 -4.6065826 -4.7083163 -4.4848523 -3.9827611 -3.0709717 -2.2562108 -2.3207171 -3.0630116 -3.6542485 -4.08764 -4.4572444 -4.7984481 -4.5545692 -3.7143962][-4.5268736 -4.5036035 -4.5400643 -4.1658463 -3.4204197 -2.0909543 -0.96739721 -1.0906725 -2.1625187 -3.1004207 -3.5392606 -3.931093 -4.5097132 -4.3967671 -3.551862][-4.483779 -4.34576 -4.3396387 -3.9297194 -3.1756349 -1.6797235 -0.43630648 -0.65734196 -1.9033303 -3.0109262 -3.3750319 -3.6662679 -4.3629856 -4.3903208 -3.6427302][-4.4165998 -4.2022386 -4.2158232 -3.9197924 -3.3914595 -2.0531249 -0.96737838 -1.2913938 -2.4472709 -3.4662614 -3.6489072 -3.7516341 -4.4273324 -4.5468 -3.9394813][-4.4236665 -4.1650558 -4.1965756 -4.0635109 -3.8677635 -2.8896213 -2.1109505 -2.4855931 -3.3783913 -4.1218529 -4.0789828 -4.0085192 -4.6011882 -4.7640581 -4.2811704][-4.5464864 -4.2693129 -4.2819858 -4.2460785 -4.2954912 -3.6508274 -3.1443617 -3.4940472 -4.0565014 -4.4945407 -4.3206787 -4.2092748 -4.7443066 -4.9059215 -4.5201383][-4.7005334 -4.4533024 -4.4375648 -4.4303923 -4.5682712 -4.1219273 -3.7633057 -3.9874732 -4.2495871 -4.469995 -4.3104744 -4.2754254 -4.7617564 -4.8683553 -4.530561][-4.6246195 -4.4411926 -4.4394808 -4.4751668 -4.6076174 -4.2621555 -3.9416609 -3.9856184 -4.03946 -4.1637645 -4.1208978 -4.2186217 -4.6227865 -4.6194496 -4.2854371][-4.2672973 -4.1336565 -4.1784182 -4.2884521 -4.4089842 -4.1441979 -3.8527405 -3.7524052 -3.7070239 -3.8060083 -3.8704853 -4.0653658 -4.3570762 -4.2558203 -3.9211531][-3.9513433 -3.8384755 -3.8911624 -4.0293531 -4.131021 -3.9585826 -3.7290671 -3.5932615 -3.5402308 -3.6276257 -3.7334423 -3.925971 -4.0930691 -3.9591749 -3.6803913]]...]
INFO - root - 2017-12-06 09:19:14.366156: step 16810, loss = 0.87, batch loss = 0.80 (13.1 examples/sec; 0.612 sec/batch; 53h:40m:50s remains)
INFO - root - 2017-12-06 09:19:20.508128: step 16820, loss = 0.91, batch loss = 0.84 (12.8 examples/sec; 0.623 sec/batch; 54h:37m:39s remains)
INFO - root - 2017-12-06 09:19:26.628977: step 16830, loss = 1.05, batch loss = 0.98 (13.0 examples/sec; 0.618 sec/batch; 54h:09m:49s remains)
INFO - root - 2017-12-06 09:19:32.760662: step 16840, loss = 1.04, batch loss = 0.97 (12.7 examples/sec; 0.628 sec/batch; 55h:04m:33s remains)
INFO - root - 2017-12-06 09:19:38.780636: step 16850, loss = 0.98, batch loss = 0.91 (12.9 examples/sec; 0.621 sec/batch; 54h:28m:43s remains)
INFO - root - 2017-12-06 09:19:44.825393: step 16860, loss = 1.13, batch loss = 1.06 (13.5 examples/sec; 0.594 sec/batch; 52h:03m:06s remains)
INFO - root - 2017-12-06 09:19:50.888858: step 16870, loss = 0.76, batch loss = 0.69 (13.3 examples/sec; 0.601 sec/batch; 52h:40m:29s remains)
INFO - root - 2017-12-06 09:19:57.004201: step 16880, loss = 1.12, batch loss = 1.05 (12.4 examples/sec; 0.644 sec/batch; 56h:25m:45s remains)
INFO - root - 2017-12-06 09:20:03.075173: step 16890, loss = 0.95, batch loss = 0.88 (13.6 examples/sec; 0.586 sec/batch; 51h:24m:40s remains)
INFO - root - 2017-12-06 09:20:09.094712: step 16900, loss = 0.83, batch loss = 0.76 (12.9 examples/sec; 0.622 sec/batch; 54h:32m:55s remains)
2017-12-06 09:20:09.691898: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9504604 -4.7019625 -4.5778427 -4.5681458 -4.3934479 -4.0270414 -3.8757393 -4.1542115 -4.5118918 -4.6523647 -4.5744495 -4.303658 -3.9811957 -3.8326828 -3.8332624][-4.541903 -4.2899251 -4.1797 -4.207068 -4.1489453 -3.9687958 -3.9650335 -4.2844028 -4.5923929 -4.6686516 -4.5868659 -4.3739567 -4.1313 -3.9841349 -3.855232][-3.8128352 -3.5864408 -3.5617864 -3.6856246 -3.811327 -3.8819621 -4.0823154 -4.4895782 -4.78408 -4.8137865 -4.7050657 -4.5072303 -4.3074727 -4.1640954 -3.9252234][-2.9648225 -2.6919084 -2.7243509 -2.976557 -3.3370419 -3.6894357 -4.0944109 -4.5792079 -4.8558106 -4.8589826 -4.727694 -4.5326777 -4.3851948 -4.3162446 -4.0776863][-2.3199208 -1.8953729 -1.8834753 -2.2260292 -2.7756011 -3.3293777 -3.8610795 -4.3803287 -4.6542292 -4.6736073 -4.5700145 -4.435945 -4.4121952 -4.5124421 -4.4094496][-2.1741436 -1.5894921 -1.4731772 -1.836199 -2.4482913 -3.0242329 -3.5074339 -3.9337578 -4.1874161 -4.2794285 -4.2726555 -4.2413 -4.3611822 -4.6380272 -4.7155867][-2.7162137 -2.0198588 -1.7448606 -2.0310915 -2.6206625 -3.1343522 -3.456099 -3.6750638 -3.8479998 -4.028142 -4.1795635 -4.2849402 -4.4458165 -4.7005677 -4.8151512][-3.6741049 -2.8394127 -2.2879806 -2.3725345 -2.8980951 -3.4018435 -3.6400015 -3.6884365 -3.7511554 -3.9808185 -4.2762308 -4.5406327 -4.7206879 -4.8232813 -4.8074589][-4.3902884 -3.4558179 -2.6029766 -2.4020555 -2.8386192 -3.4431906 -3.7996116 -3.8343949 -3.8105173 -3.99265 -4.3433123 -4.7184353 -4.9347272 -4.9145889 -4.7264571][-4.6545668 -3.7469997 -2.6828213 -2.1823924 -2.5330758 -3.3466671 -4.0154047 -4.1784592 -4.0640607 -4.0794635 -4.3331614 -4.6898556 -4.9248886 -4.89824 -4.6600761][-4.685472 -3.9449244 -2.8165698 -2.0271356 -2.2099876 -3.1867075 -4.2037807 -4.5989447 -4.4441628 -4.2500014 -4.3118362 -4.5699883 -4.81246 -4.8904443 -4.80208][-4.6508589 -4.1856995 -3.1510868 -2.0964077 -1.9197609 -2.7674785 -3.9335861 -4.593 -4.5624332 -4.2980785 -4.2130051 -4.3900881 -4.6731458 -4.9099751 -5.0795436][-4.6478167 -4.4279017 -3.5930009 -2.39476 -1.7920749 -2.2418051 -3.2694285 -4.0511708 -4.1947422 -4.0233812 -3.9190769 -4.1013689 -4.470449 -4.8611317 -5.2470322][-4.710165 -4.6222649 -4.0295229 -2.9020529 -2.0434833 -2.0735817 -2.7786741 -3.4471016 -3.6330538 -3.5537536 -3.5049777 -3.7452571 -4.1616392 -4.6266441 -5.1474481][-4.763566 -4.7090693 -4.3601165 -3.4982929 -2.6342478 -2.3926055 -2.793808 -3.241293 -3.3275437 -3.2050104 -3.1485996 -3.4101167 -3.7810252 -4.1801033 -4.6946435]]...]
INFO - root - 2017-12-06 09:20:15.761107: step 16910, loss = 0.82, batch loss = 0.75 (13.1 examples/sec; 0.612 sec/batch; 53h:40m:06s remains)
INFO - root - 2017-12-06 09:20:21.962583: step 16920, loss = 1.05, batch loss = 0.98 (12.6 examples/sec; 0.635 sec/batch; 55h:42m:30s remains)
INFO - root - 2017-12-06 09:20:28.049964: step 16930, loss = 0.81, batch loss = 0.74 (13.4 examples/sec; 0.597 sec/batch; 52h:21m:05s remains)
INFO - root - 2017-12-06 09:20:34.131830: step 16940, loss = 0.99, batch loss = 0.92 (13.1 examples/sec; 0.611 sec/batch; 53h:33m:16s remains)
INFO - root - 2017-12-06 09:20:40.317232: step 16950, loss = 1.01, batch loss = 0.94 (12.9 examples/sec; 0.622 sec/batch; 54h:29m:38s remains)
INFO - root - 2017-12-06 09:20:46.196545: step 16960, loss = 0.91, batch loss = 0.84 (13.7 examples/sec; 0.584 sec/batch; 51h:10m:29s remains)
INFO - root - 2017-12-06 09:20:52.325338: step 16970, loss = 0.88, batch loss = 0.81 (12.6 examples/sec; 0.634 sec/batch; 55h:33m:21s remains)
INFO - root - 2017-12-06 09:20:58.410292: step 16980, loss = 0.71, batch loss = 0.64 (13.4 examples/sec; 0.598 sec/batch; 52h:22m:33s remains)
INFO - root - 2017-12-06 09:21:04.469174: step 16990, loss = 0.83, batch loss = 0.76 (13.8 examples/sec; 0.578 sec/batch; 50h:37m:38s remains)
INFO - root - 2017-12-06 09:21:10.564382: step 17000, loss = 0.90, batch loss = 0.83 (13.5 examples/sec; 0.591 sec/batch; 51h:46m:42s remains)
2017-12-06 09:21:11.072827: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.340776 -4.6656451 -5.2395964 -5.3944554 -5.0514545 -4.5113482 -4.0901828 -4.0256996 -4.0581264 -4.102644 -4.2794075 -4.5990648 -4.9003162 -4.9160171 -4.8562036][-4.7650666 -5.3084517 -6.0460887 -6.2247152 -5.7421422 -5.0584068 -4.5601549 -4.372345 -4.2390394 -4.1883769 -4.3410978 -4.7035995 -5.0926948 -5.1555367 -5.0726194][-4.7410555 -5.4319735 -6.2015791 -6.3661723 -5.8120561 -5.084723 -4.6561022 -4.5210953 -4.3739924 -4.3179755 -4.4639068 -4.8009853 -5.191926 -5.3089051 -5.2433233][-4.1148677 -4.8975511 -5.6216035 -5.7767754 -5.2685328 -4.6278949 -4.3795838 -4.4300776 -4.3930278 -4.4179239 -4.625155 -4.938796 -5.2556748 -5.3950424 -5.369525][-3.0890799 -3.9339108 -4.54807 -4.6489773 -4.238966 -3.7393675 -3.6440778 -3.8376327 -3.923104 -4.0974827 -4.4896078 -4.8967166 -5.1840196 -5.355113 -5.3892112][-1.9738352 -2.8877559 -3.3808331 -3.3582945 -3.0207033 -2.6116376 -2.4846234 -2.5863638 -2.6670091 -2.9965088 -3.6621065 -4.3063235 -4.708992 -5.0109353 -5.1789055][-1.3211131 -2.3272474 -2.7036941 -2.4853601 -2.1379066 -1.7278786 -1.3627396 -1.0864034 -0.98071504 -1.428648 -2.3715277 -3.2952662 -3.9099627 -4.4306345 -4.8027086][-1.3844953 -2.4386756 -2.6974649 -2.2490633 -1.8263452 -1.3883257 -0.78035808 -0.0863781 0.27630711 -0.21935749 -1.3463671 -2.4640024 -3.2394986 -3.932884 -4.4831824][-2.0997717 -3.0676355 -3.1950963 -2.5906947 -2.0977385 -1.6500974 -0.96029377 -0.091547966 0.43209791 -0.011544228 -1.1354365 -2.2576845 -3.0366216 -3.7401752 -4.3383255][-3.079493 -3.8221288 -3.8235168 -3.2094355 -2.7132702 -2.2931802 -1.7035193 -0.97417951 -0.47496057 -0.8003974 -1.7534938 -2.7119045 -3.3445139 -3.880049 -4.3579459][-4.0308061 -4.4933558 -4.403492 -3.9043033 -3.4821284 -3.1280003 -2.7176802 -2.2576833 -1.8763306 -2.0536988 -2.7488382 -3.463757 -3.8910024 -4.1864004 -4.4430132][-4.7218132 -4.9646745 -4.8673406 -4.560853 -4.2679925 -3.9931822 -3.743567 -3.5197959 -3.2674329 -3.3219566 -3.7582746 -4.2186351 -4.4473729 -4.519887 -4.5372896][-5.0229473 -5.150351 -5.1102705 -5.0060024 -4.868072 -4.6854253 -4.548315 -4.4609227 -4.3008571 -4.2728248 -4.4951506 -4.7457619 -4.833745 -4.7639389 -4.6036682][-4.8988204 -4.9950686 -5.0279112 -5.0725226 -5.0779948 -5.011796 -4.9677067 -4.9481874 -4.8508077 -4.7846 -4.846981 -4.9299321 -4.9328365 -4.8194146 -4.5890374][-4.5344286 -4.6320043 -4.7092648 -4.8147135 -4.8855205 -4.899106 -4.9182463 -4.927289 -4.8710871 -4.816452 -4.801652 -4.7893629 -4.7724371 -4.7027736 -4.5038733]]...]
INFO - root - 2017-12-06 09:21:17.199657: step 17010, loss = 1.01, batch loss = 0.94 (12.9 examples/sec; 0.618 sec/batch; 54h:11m:11s remains)
INFO - root - 2017-12-06 09:21:23.280976: step 17020, loss = 1.10, batch loss = 1.03 (12.9 examples/sec; 0.619 sec/batch; 54h:13m:12s remains)
INFO - root - 2017-12-06 09:21:29.371696: step 17030, loss = 1.01, batch loss = 0.94 (12.6 examples/sec; 0.637 sec/batch; 55h:51m:11s remains)
INFO - root - 2017-12-06 09:21:35.378454: step 17040, loss = 0.99, batch loss = 0.92 (13.2 examples/sec; 0.607 sec/batch; 53h:11m:50s remains)
INFO - root - 2017-12-06 09:21:41.486043: step 17050, loss = 1.09, batch loss = 1.02 (13.3 examples/sec; 0.603 sec/batch; 52h:52m:22s remains)
INFO - root - 2017-12-06 09:21:47.562431: step 17060, loss = 0.81, batch loss = 0.74 (13.4 examples/sec; 0.596 sec/batch; 52h:12m:36s remains)
INFO - root - 2017-12-06 09:21:53.661295: step 17070, loss = 0.89, batch loss = 0.82 (12.9 examples/sec; 0.620 sec/batch; 54h:17m:47s remains)
INFO - root - 2017-12-06 09:21:59.781370: step 17080, loss = 0.86, batch loss = 0.79 (12.9 examples/sec; 0.620 sec/batch; 54h:20m:49s remains)
INFO - root - 2017-12-06 09:22:05.909755: step 17090, loss = 1.16, batch loss = 1.09 (13.6 examples/sec; 0.587 sec/batch; 51h:24m:20s remains)
INFO - root - 2017-12-06 09:22:12.017285: step 17100, loss = 0.85, batch loss = 0.78 (12.7 examples/sec; 0.629 sec/batch; 55h:05m:48s remains)
2017-12-06 09:22:12.547183: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.837971 -3.7965574 -3.7879453 -3.8079064 -3.7718959 -3.6985581 -3.6193159 -3.5369623 -3.547616 -3.5552952 -3.527622 -3.4869189 -3.4373903 -3.4247108 -3.4594357][-4.2781196 -4.267271 -4.2637548 -4.28667 -4.2405972 -4.1572819 -4.0947533 -4.0140963 -4.0175328 -4.0231223 -3.9670177 -3.8572624 -3.7015398 -3.6017997 -3.5959721][-4.7600536 -4.762969 -4.6952028 -4.625576 -4.532464 -4.4475079 -4.4495444 -4.4245839 -4.4281631 -4.4712586 -4.4783578 -4.3801379 -4.1521316 -3.9615114 -3.9105256][-4.9858918 -4.9564619 -4.7739882 -4.5497994 -4.3742695 -4.2694106 -4.3560696 -4.4753695 -4.5716348 -4.7461133 -4.9205542 -4.9190583 -4.6332493 -4.32416 -4.2031837][-5.1698966 -5.1289277 -4.8120656 -4.3724494 -4.0315075 -3.8242488 -3.9680338 -4.2961764 -4.5347247 -4.8684335 -5.2055845 -5.2861309 -4.9279432 -4.4977608 -4.3086429][-5.5340781 -5.5198827 -5.0401411 -4.2772555 -3.615139 -3.1663218 -3.2789011 -3.8284163 -4.2899308 -4.8258491 -5.3147378 -5.5010619 -5.1644325 -4.6933265 -4.4014869][-5.711113 -5.7101049 -5.126965 -4.0598097 -2.983798 -2.1859095 -2.1683133 -2.9391279 -3.7842445 -4.6473045 -5.3464084 -5.6821237 -5.4830623 -5.0890927 -4.7248259][-5.5193324 -5.5281162 -5.041482 -3.9126232 -2.5410786 -1.411515 -1.190166 -2.08905 -3.3100431 -4.4957142 -5.3306422 -5.7034259 -5.5706978 -5.245038 -4.8913555][-5.446002 -5.4770823 -5.2155657 -4.2843843 -2.8888354 -1.5702808 -1.1248336 -1.9143889 -3.2267156 -4.4818816 -5.2361646 -5.4008145 -5.0892897 -4.6853743 -4.3735213][-5.5754409 -5.6244159 -5.5700026 -4.9810085 -3.798666 -2.4707904 -1.8227575 -2.3067899 -3.3973386 -4.4683237 -5.0183549 -4.9159851 -4.360682 -3.8005102 -3.5168447][-5.5714245 -5.6337562 -5.71774 -5.4859028 -4.6588163 -3.4817181 -2.7323852 -2.9277186 -3.7105575 -4.5579329 -5.0203104 -4.846179 -4.179965 -3.4698811 -3.1129904][-5.3563547 -5.427916 -5.5503769 -5.5392332 -5.0656219 -4.1550198 -3.46435 -3.5487461 -4.1218076 -4.7811737 -5.2023525 -5.077168 -4.464025 -3.7536132 -3.3655372][-5.1425762 -5.2077236 -5.3309035 -5.4378209 -5.289567 -4.7301111 -4.202868 -4.2244105 -4.5634861 -4.963954 -5.2575693 -5.1606207 -4.7035079 -4.1808262 -3.9409952][-4.9645648 -5.035181 -5.211484 -5.4242463 -5.5357237 -5.2820287 -4.8839507 -4.8053808 -4.9087396 -5.059236 -5.1986876 -5.1163521 -4.8534412 -4.5995469 -4.5416913][-4.8224521 -4.9608135 -5.2278109 -5.5108695 -5.7641392 -5.6939154 -5.3788161 -5.2343655 -5.2033048 -5.201664 -5.2415962 -5.2018385 -5.1201973 -5.0780048 -5.1085825]]...]
INFO - root - 2017-12-06 09:22:18.505241: step 17110, loss = 0.94, batch loss = 0.87 (13.0 examples/sec; 0.615 sec/batch; 53h:54m:29s remains)
INFO - root - 2017-12-06 09:22:24.564881: step 17120, loss = 0.66, batch loss = 0.59 (13.3 examples/sec; 0.604 sec/batch; 52h:52m:57s remains)
INFO - root - 2017-12-06 09:22:30.625280: step 17130, loss = 0.85, batch loss = 0.78 (13.3 examples/sec; 0.600 sec/batch; 52h:34m:17s remains)
INFO - root - 2017-12-06 09:22:36.696174: step 17140, loss = 1.09, batch loss = 1.02 (13.1 examples/sec; 0.612 sec/batch; 53h:37m:15s remains)
INFO - root - 2017-12-06 09:22:42.820004: step 17150, loss = 0.86, batch loss = 0.79 (12.9 examples/sec; 0.619 sec/batch; 54h:13m:27s remains)
INFO - root - 2017-12-06 09:22:48.906173: step 17160, loss = 0.91, batch loss = 0.84 (13.2 examples/sec; 0.605 sec/batch; 53h:00m:22s remains)
INFO - root - 2017-12-06 09:22:54.994704: step 17170, loss = 1.10, batch loss = 1.03 (13.0 examples/sec; 0.615 sec/batch; 53h:53m:30s remains)
INFO - root - 2017-12-06 09:23:01.111816: step 17180, loss = 0.75, batch loss = 0.68 (13.0 examples/sec; 0.618 sec/batch; 54h:05m:33s remains)
INFO - root - 2017-12-06 09:23:07.292062: step 17190, loss = 0.85, batch loss = 0.78 (12.9 examples/sec; 0.620 sec/batch; 54h:20m:39s remains)
INFO - root - 2017-12-06 09:23:13.397260: step 17200, loss = 1.03, batch loss = 0.96 (13.3 examples/sec; 0.603 sec/batch; 52h:48m:22s remains)
2017-12-06 09:23:13.932174: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5086579 -4.7118888 -4.8835912 -4.9088631 -4.7293425 -4.4610014 -4.3128238 -4.4083195 -4.6518369 -4.8240318 -4.7530003 -4.4923458 -4.2539768 -4.2049479 -4.2980895][-4.538661 -4.8064537 -5.0415998 -5.0512571 -4.7283611 -4.2280383 -3.8606772 -3.8775561 -4.2367053 -4.6798606 -4.8958364 -4.7998395 -4.5693021 -4.4346542 -4.4239469][-4.520546 -4.8360219 -5.128056 -5.1232738 -4.6487393 -3.8827431 -3.2326534 -3.0925298 -3.5169137 -4.2319493 -4.7886457 -4.9353518 -4.7864528 -4.6209908 -4.544013][-4.4706616 -4.8043466 -5.137291 -5.1286736 -4.5297279 -3.518486 -2.5696239 -2.205889 -2.6156492 -3.5430186 -4.4350557 -4.86815 -4.8608632 -4.7214103 -4.6255069][-4.4022164 -4.718677 -5.0629692 -5.0566807 -4.3872962 -3.2110248 -2.0227132 -1.4283566 -1.7505939 -2.7955587 -3.9617271 -4.6834483 -4.8527637 -4.7743688 -4.6806145][-4.3105679 -4.5785494 -4.8979139 -4.8967848 -4.2219381 -2.9881194 -1.6673031 -0.8896637 -1.076762 -2.1307487 -3.4753983 -4.4576092 -4.82862 -4.8329515 -4.7399917][-4.2707787 -4.4460645 -4.6887932 -4.6799707 -4.0517316 -2.8591213 -1.5217171 -0.6353054 -0.66511369 -1.6230564 -3.0275228 -4.2063746 -4.7861557 -4.8990955 -4.8129377][-4.3570175 -4.4012 -4.5179439 -4.476325 -3.9232194 -2.8450892 -1.5863268 -0.67174006 -0.54369354 -1.3104424 -2.6346602 -3.904566 -4.6716967 -4.9263062 -4.8835025][-4.5546789 -4.4785933 -4.4645209 -4.3853803 -3.9361749 -3.0329568 -1.9182518 -1.0216675 -0.73062086 -1.222331 -2.3199782 -3.5424902 -4.4334817 -4.8557625 -4.9176445][-4.824945 -4.664156 -4.5400443 -4.430325 -4.1142907 -3.4420373 -2.5205102 -1.6614153 -1.1969521 -1.3530006 -2.103771 -3.142498 -4.0687742 -4.6555595 -4.8761306][-5.0043759 -4.8268614 -4.6461711 -4.5259304 -4.3536553 -3.9431894 -3.2509174 -2.4504025 -1.8276379 -1.6504276 -2.0151906 -2.7834663 -3.64854 -4.3507318 -4.7424827][-4.9370441 -4.8176756 -4.6404648 -4.522254 -4.4676185 -4.3051095 -3.8714519 -3.1947942 -2.5033963 -2.0822971 -2.1118462 -2.5869679 -3.3020954 -4.0241017 -4.5399537][-4.6811008 -4.6486349 -4.5022879 -4.37241 -4.3568425 -4.3577628 -4.16961 -3.6988912 -3.0842953 -2.5778823 -2.395817 -2.6185498 -3.1350403 -3.7811763 -4.3364329][-4.4082432 -4.4526916 -4.3364224 -4.1764588 -4.1242409 -4.1836276 -4.1899576 -3.9752316 -3.5659845 -3.1253736 -2.8559382 -2.8867354 -3.1915333 -3.6925781 -4.2010946][-4.2918839 -4.3546023 -4.2389288 -4.0355735 -3.9072938 -3.941921 -4.0584059 -4.0809178 -3.9369712 -3.6689589 -3.4216912 -3.3407435 -3.465575 -3.7906232 -4.1796527]]...]
INFO - root - 2017-12-06 09:23:20.039910: step 17210, loss = 0.74, batch loss = 0.67 (13.1 examples/sec; 0.608 sec/batch; 53h:17m:01s remains)
INFO - root - 2017-12-06 09:23:25.988220: step 17220, loss = 1.14, batch loss = 1.06 (13.4 examples/sec; 0.597 sec/batch; 52h:14m:52s remains)
INFO - root - 2017-12-06 09:23:32.016840: step 17230, loss = 1.01, batch loss = 0.94 (13.5 examples/sec; 0.593 sec/batch; 51h:54m:03s remains)
INFO - root - 2017-12-06 09:23:38.133043: step 17240, loss = 0.73, batch loss = 0.66 (13.1 examples/sec; 0.609 sec/batch; 53h:21m:01s remains)
INFO - root - 2017-12-06 09:23:44.288906: step 17250, loss = 1.17, batch loss = 1.10 (12.4 examples/sec; 0.647 sec/batch; 56h:40m:09s remains)
INFO - root - 2017-12-06 09:23:50.267499: step 17260, loss = 0.86, batch loss = 0.79 (12.9 examples/sec; 0.619 sec/batch; 54h:13m:38s remains)
INFO - root - 2017-12-06 09:23:56.356878: step 17270, loss = 0.67, batch loss = 0.60 (13.5 examples/sec; 0.592 sec/batch; 51h:50m:16s remains)
INFO - root - 2017-12-06 09:24:02.424901: step 17280, loss = 0.81, batch loss = 0.74 (12.9 examples/sec; 0.618 sec/batch; 54h:09m:21s remains)
INFO - root - 2017-12-06 09:24:08.474033: step 17290, loss = 1.04, batch loss = 0.97 (13.8 examples/sec; 0.578 sec/batch; 50h:37m:40s remains)
INFO - root - 2017-12-06 09:24:14.500098: step 17300, loss = 0.90, batch loss = 0.83 (12.9 examples/sec; 0.619 sec/batch; 54h:10m:50s remains)
2017-12-06 09:24:15.055555: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.163331 -4.2707157 -4.4808621 -4.6027813 -4.6043859 -4.6562576 -4.8066621 -4.9948411 -5.1218677 -5.17889 -5.1924496 -5.1830635 -5.0922246 -4.8100805 -4.5471396][-4.2045331 -4.210103 -4.3050418 -4.3249106 -4.2574935 -4.273417 -4.4528847 -4.7059765 -4.8834152 -4.9569817 -4.9066105 -4.81283 -4.6761465 -4.3586578 -4.07698][-4.0803523 -3.9821336 -3.9574723 -3.9050968 -3.8377004 -3.8606672 -4.0568695 -4.3385868 -4.5613508 -4.6650314 -4.5701008 -4.4112782 -4.2480888 -3.92514 -3.6464059][-3.8925552 -3.7457519 -3.65968 -3.5960071 -3.582231 -3.6286597 -3.7880304 -4.0187731 -4.2457304 -4.394659 -4.2962503 -4.1028876 -3.9308016 -3.6272514 -3.366107][-3.7987614 -3.6228194 -3.4843948 -3.4210286 -3.4626856 -3.5264115 -3.6039069 -3.7082057 -3.8819716 -4.0848856 -4.0501509 -3.8848009 -3.7333813 -3.480608 -3.257741][-3.8537159 -3.6031885 -3.3393273 -3.2179525 -3.263607 -3.3500061 -3.3786922 -3.3625028 -3.4514434 -3.7015254 -3.7698264 -3.6802866 -3.5955219 -3.4492817 -3.3104472][-4.0530114 -3.7034769 -3.2595649 -3.0214634 -3.0144405 -3.1404042 -3.2025 -3.1538706 -3.1923647 -3.4723384 -3.6335092 -3.6252272 -3.6232724 -3.6142406 -3.5808082][-4.3260775 -3.9193687 -3.3315363 -2.9808686 -2.9132032 -3.0723228 -3.2137501 -3.2086284 -3.2501764 -3.5532539 -3.778234 -3.8345451 -3.89223 -3.9881525 -4.032619][-4.5610094 -4.1689816 -3.5197477 -3.1030087 -3.008368 -3.1903114 -3.4067335 -3.4766145 -3.5571651 -3.8772154 -4.1398764 -4.2577186 -4.3513603 -4.4764891 -4.5342879][-4.6101818 -4.2946334 -3.6894894 -3.2626452 -3.1646357 -3.3574033 -3.6385181 -3.7794838 -3.8843515 -4.2169776 -4.5279 -4.7282882 -4.8623252 -4.9648085 -4.9830523][-4.478601 -4.3078966 -3.8483043 -3.4451318 -3.31158 -3.438345 -3.7185719 -3.8888309 -4.0000019 -4.3524876 -4.711246 -4.9875183 -5.1561966 -5.2307096 -5.2076583][-4.2454128 -4.2959146 -4.0635591 -3.7347434 -3.5466197 -3.5362089 -3.7304544 -3.8750682 -3.9852533 -4.3480654 -4.7159591 -5.0069151 -5.1791573 -5.2314157 -5.1969681][-3.9608278 -4.2510839 -4.263433 -4.0550609 -3.8380957 -3.6981936 -3.7769516 -3.8732836 -3.9564552 -4.2612796 -4.5489769 -4.7592483 -4.889957 -4.9359632 -4.9365797][-3.6802666 -4.1735258 -4.3719549 -4.2736778 -4.0558686 -3.8367677 -3.8498876 -3.9461508 -4.01645 -4.215229 -4.365428 -4.4524312 -4.5220852 -4.5540833 -4.574894][-3.3871131 -4.0292253 -4.3282037 -4.2738738 -4.0463934 -3.8068874 -3.8270807 -3.996192 -4.120338 -4.250145 -4.2836308 -4.2659464 -4.2777481 -4.2880206 -4.2973633]]...]
INFO - root - 2017-12-06 09:24:21.101440: step 17310, loss = 1.00, batch loss = 0.93 (13.1 examples/sec; 0.611 sec/batch; 53h:27m:58s remains)
INFO - root - 2017-12-06 09:24:27.213731: step 17320, loss = 0.86, batch loss = 0.79 (13.0 examples/sec; 0.618 sec/batch; 54h:04m:26s remains)
INFO - root - 2017-12-06 09:24:33.257094: step 17330, loss = 0.88, batch loss = 0.81 (12.9 examples/sec; 0.620 sec/batch; 54h:18m:56s remains)
INFO - root - 2017-12-06 09:24:39.413207: step 17340, loss = 0.75, batch loss = 0.68 (13.2 examples/sec; 0.604 sec/batch; 52h:53m:37s remains)
INFO - root - 2017-12-06 09:24:45.549887: step 17350, loss = 0.99, batch loss = 0.92 (12.8 examples/sec; 0.624 sec/batch; 54h:35m:01s remains)
INFO - root - 2017-12-06 09:24:51.566226: step 17360, loss = 0.96, batch loss = 0.89 (12.7 examples/sec; 0.629 sec/batch; 55h:03m:46s remains)
INFO - root - 2017-12-06 09:24:57.685884: step 17370, loss = 0.92, batch loss = 0.85 (13.0 examples/sec; 0.618 sec/batch; 54h:03m:16s remains)
INFO - root - 2017-12-06 09:25:03.798753: step 17380, loss = 1.24, batch loss = 1.17 (13.4 examples/sec; 0.598 sec/batch; 52h:20m:37s remains)
INFO - root - 2017-12-06 09:25:09.826968: step 17390, loss = 1.02, batch loss = 0.95 (12.7 examples/sec; 0.631 sec/batch; 55h:15m:03s remains)
INFO - root - 2017-12-06 09:25:15.887021: step 17400, loss = 1.05, batch loss = 0.98 (13.3 examples/sec; 0.600 sec/batch; 52h:30m:54s remains)
2017-12-06 09:25:16.417373: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2645688 -4.3120461 -4.32769 -4.3815718 -4.4052148 -4.3072667 -4.1720252 -4.2840552 -4.6881766 -5.0424576 -5.1376047 -5.000061 -4.8293486 -4.6631718 -4.476923][-4.3437858 -4.4409823 -4.490149 -4.5733867 -4.5689983 -4.3648772 -4.0830812 -4.1533556 -4.6971254 -5.1991687 -5.3337932 -5.1182947 -4.8676777 -4.6789284 -4.5154648][-4.291934 -4.4309735 -4.486445 -4.5558596 -4.4763317 -4.1447296 -3.6882496 -3.6495323 -4.2762423 -4.9349427 -5.1844521 -4.9672155 -4.6740947 -4.4629 -4.335814][-4.1080422 -4.2771978 -4.3184381 -4.3496561 -4.1868677 -3.7453947 -3.1363037 -2.942044 -3.575644 -4.3600545 -4.7675734 -4.6474843 -4.3991766 -4.2232089 -4.1425176][-3.9743977 -4.1427808 -4.1180558 -4.033988 -3.7259414 -3.1490641 -2.3818302 -2.0090883 -2.5918357 -3.48169 -4.0934219 -4.1973796 -4.137691 -4.1079621 -4.0993381][-3.9506662 -4.070694 -3.9198122 -3.6303906 -3.106292 -2.318073 -1.3452263 -0.81138134 -1.3872797 -2.4331613 -3.2779942 -3.6605916 -3.8389158 -3.9891407 -4.0739875][-4.0624895 -4.0796242 -3.7586527 -3.2444439 -2.5171633 -1.4934433 -0.27697372 0.3548522 -0.34618425 -1.6402502 -2.7136824 -3.3365517 -3.7000332 -3.9635589 -4.1239948][-4.291862 -4.2224021 -3.7784984 -3.1068337 -2.2241507 -0.98333168 0.44798613 1.1261864 0.25198412 -1.2297843 -2.4028285 -3.1457283 -3.620096 -3.9322567 -4.131506][-4.5591683 -4.4936752 -4.0727248 -3.4375007 -2.5619121 -1.27947 0.16204739 0.79285526 -0.09266758 -1.4920874 -2.4966824 -3.1025398 -3.4731152 -3.6866024 -3.8532469][-4.8275533 -4.8647218 -4.5967913 -4.1487255 -3.4339147 -2.303436 -1.0494826 -0.49749732 -1.2189789 -2.3057728 -2.9444213 -3.1879208 -3.2204356 -3.2244446 -3.3458812][-4.9653354 -5.1300478 -5.0354886 -4.7812715 -4.2427969 -3.3098569 -2.2895949 -1.8231828 -2.3543983 -3.1074831 -3.4063673 -3.2885382 -2.9507742 -2.7375407 -2.858005][-4.8467 -5.0839677 -5.1097465 -5.0047274 -4.6419516 -3.9428329 -3.1782136 -2.8499534 -3.2592149 -3.7866547 -3.8998854 -3.5795507 -3.0028338 -2.6047547 -2.677372][-4.5942583 -4.8504009 -4.9293308 -4.9106297 -4.7011743 -4.2417688 -3.743335 -3.603054 -3.9657249 -4.36966 -4.4331117 -4.0881648 -3.4909916 -3.0183053 -2.995177][-4.3507705 -4.5785079 -4.6710234 -4.7007442 -4.6188326 -4.380466 -4.1269827 -4.14987 -4.4716549 -4.7734394 -4.8250356 -4.5744424 -4.1349578 -3.728425 -3.6310124][-4.1966434 -4.3724532 -4.4599009 -4.5120592 -4.5146241 -4.449904 -4.3848147 -4.4951196 -4.7305603 -4.8933482 -4.8985443 -4.7430663 -4.5026426 -4.2566319 -4.17774]]...]
INFO - root - 2017-12-06 09:25:22.417741: step 17410, loss = 0.97, batch loss = 0.90 (13.5 examples/sec; 0.592 sec/batch; 51h:50m:06s remains)
INFO - root - 2017-12-06 09:25:28.532193: step 17420, loss = 1.19, batch loss = 1.12 (13.0 examples/sec; 0.616 sec/batch; 53h:53m:48s remains)
INFO - root - 2017-12-06 09:25:34.539976: step 17430, loss = 1.17, batch loss = 1.10 (13.2 examples/sec; 0.606 sec/batch; 53h:02m:48s remains)
INFO - root - 2017-12-06 09:25:40.707050: step 17440, loss = 0.92, batch loss = 0.85 (12.5 examples/sec; 0.638 sec/batch; 55h:52m:11s remains)
INFO - root - 2017-12-06 09:25:46.801777: step 17450, loss = 0.86, batch loss = 0.79 (12.6 examples/sec; 0.634 sec/batch; 55h:30m:15s remains)
INFO - root - 2017-12-06 09:25:52.870084: step 17460, loss = 0.88, batch loss = 0.81 (13.1 examples/sec; 0.611 sec/batch; 53h:30m:39s remains)
INFO - root - 2017-12-06 09:25:58.995982: step 17470, loss = 0.79, batch loss = 0.72 (13.0 examples/sec; 0.615 sec/batch; 53h:46m:36s remains)
INFO - root - 2017-12-06 09:26:05.067408: step 17480, loss = 0.94, batch loss = 0.87 (13.5 examples/sec; 0.594 sec/batch; 51h:58m:29s remains)
INFO - root - 2017-12-06 09:26:11.174999: step 17490, loss = 0.82, batch loss = 0.75 (13.2 examples/sec; 0.605 sec/batch; 52h:57m:22s remains)
INFO - root - 2017-12-06 09:26:17.279660: step 17500, loss = 0.97, batch loss = 0.90 (12.8 examples/sec; 0.624 sec/batch; 54h:38m:05s remains)
2017-12-06 09:26:17.800648: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5132651 -4.3887024 -4.1927543 -4.0845418 -4.0611076 -4.0235167 -3.9614527 -3.9744267 -3.9465606 -3.9199922 -4.0117893 -4.1520591 -4.2445135 -4.3562331 -4.4810281][-4.0200987 -4.0109196 -3.9177635 -3.8352838 -3.751323 -3.5887072 -3.4426906 -3.4758766 -3.5326183 -3.6119249 -3.7517517 -3.8601906 -3.9034905 -4.011611 -4.1617985][-3.6151705 -3.7682142 -3.8163052 -3.7507796 -3.519732 -3.097487 -2.7628243 -2.832695 -3.1123457 -3.4364722 -3.6465845 -3.6388979 -3.5292563 -3.549274 -3.7256625][-3.5472677 -3.8429656 -3.9867656 -3.8875299 -3.4209104 -2.612591 -2.0120711 -2.1532347 -2.7513061 -3.3710232 -3.628196 -3.4687009 -3.1990871 -3.1399114 -3.3723502][-3.6271574 -4.00974 -4.1883211 -4.0219197 -3.2784476 -2.0357873 -1.1738539 -1.4845757 -2.4840467 -3.3849554 -3.6544576 -3.3460484 -2.9204264 -2.7917142 -3.0936432][-3.6561956 -4.0758686 -4.2174454 -3.8857789 -2.7690542 -1.1052954 -0.15920925 -0.87826777 -2.3490052 -3.4743428 -3.73923 -3.3118253 -2.7253602 -2.4738965 -2.8147926][-3.8473334 -4.213995 -4.2005043 -3.5311015 -1.9164705 0.13114166 0.98092365 -0.30133724 -2.2125027 -3.5043836 -3.829704 -3.3948455 -2.684406 -2.2771151 -2.5936384][-4.1886759 -4.4057941 -4.1647253 -3.1575811 -1.1918516 0.9646244 1.5630264 -0.11259174 -2.1773009 -3.480226 -3.8377006 -3.422437 -2.6442056 -2.1467063 -2.4218309][-4.3033733 -4.3385477 -3.9493213 -2.886137 -1.1382761 0.50823069 0.67132139 -0.9021349 -2.6126375 -3.6280067 -3.8295693 -3.328805 -2.5088854 -2.0430167 -2.3229246][-3.9760668 -3.8257942 -3.3881803 -2.5909894 -1.5614412 -0.84676051 -1.1101878 -2.3036282 -3.416131 -3.999115 -3.903599 -3.2238715 -2.3800659 -2.0004435 -2.2909534][-3.4230039 -3.1228492 -2.7309632 -2.3066316 -2.004571 -2.069999 -2.6128914 -3.4543519 -4.0604506 -4.2504783 -3.8728089 -3.0660839 -2.2807729 -2.0254121 -2.354038][-2.9992437 -2.6348734 -2.3797116 -2.282805 -2.3791115 -2.7623968 -3.3725388 -3.9981499 -4.2807789 -4.15952 -3.5871513 -2.7840066 -2.176744 -2.0918391 -2.4831629][-2.878243 -2.6269212 -2.6425505 -2.8149118 -2.9977591 -3.2885532 -3.7710583 -4.2357469 -4.2774811 -3.870482 -3.1556873 -2.4512708 -2.0761189 -2.1682808 -2.6301377][-3.0688815 -3.0829644 -3.4477432 -3.837086 -3.9809437 -4.020184 -4.2475944 -4.469871 -4.2244883 -3.590436 -2.8430095 -2.3167224 -2.1756644 -2.4089839 -2.8844385][-3.3524213 -3.6315823 -4.2471967 -4.7468882 -4.8291211 -4.68826 -4.7034245 -4.6915984 -4.2383604 -3.5460532 -2.9088778 -2.5635834 -2.5571032 -2.817713 -3.230237]]...]
INFO - root - 2017-12-06 09:26:23.923524: step 17510, loss = 1.08, batch loss = 1.00 (13.3 examples/sec; 0.603 sec/batch; 52h:45m:26s remains)
INFO - root - 2017-12-06 09:26:29.992425: step 17520, loss = 0.78, batch loss = 0.71 (13.0 examples/sec; 0.613 sec/batch; 53h:40m:13s remains)
INFO - root - 2017-12-06 09:26:36.068752: step 17530, loss = 0.82, batch loss = 0.75 (12.6 examples/sec; 0.634 sec/batch; 55h:30m:40s remains)
INFO - root - 2017-12-06 09:26:42.042979: step 17540, loss = 1.04, batch loss = 0.97 (13.1 examples/sec; 0.609 sec/batch; 53h:17m:11s remains)
INFO - root - 2017-12-06 09:26:48.134018: step 17550, loss = 0.80, batch loss = 0.73 (13.2 examples/sec; 0.605 sec/batch; 52h:58m:05s remains)
INFO - root - 2017-12-06 09:26:54.203058: step 17560, loss = 0.89, batch loss = 0.82 (13.1 examples/sec; 0.613 sec/batch; 53h:37m:20s remains)
INFO - root - 2017-12-06 09:27:00.320677: step 17570, loss = 1.23, batch loss = 1.16 (13.1 examples/sec; 0.609 sec/batch; 53h:15m:20s remains)
INFO - root - 2017-12-06 09:27:06.465884: step 17580, loss = 0.86, batch loss = 0.79 (13.5 examples/sec; 0.592 sec/batch; 51h:45m:03s remains)
INFO - root - 2017-12-06 09:27:12.490125: step 17590, loss = 1.17, batch loss = 1.10 (13.2 examples/sec; 0.604 sec/batch; 52h:52m:10s remains)
INFO - root - 2017-12-06 09:27:18.565079: step 17600, loss = 1.08, batch loss = 1.01 (13.4 examples/sec; 0.597 sec/batch; 52h:15m:07s remains)
2017-12-06 09:27:19.116632: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3894861 -2.7821374 -3.2620251 -3.6624713 -3.9238346 -3.9504778 -3.8163869 -3.699307 -3.6467173 -3.5957048 -3.4625375 -3.3067689 -3.2496886 -3.3782558 -3.6232507][-2.48213 -2.984807 -3.508029 -3.8830955 -4.0721297 -4.0064025 -3.7886691 -3.6662569 -3.6756175 -3.6980548 -3.6384261 -3.4805205 -3.3209753 -3.3260002 -3.4791384][-3.0976195 -3.6253285 -4.0994782 -4.3788028 -4.4579649 -4.3011756 -4.0227394 -3.9430366 -4.1008344 -4.2958903 -4.4149537 -4.364316 -4.144486 -3.9676538 -3.9188595][-4.0577903 -4.4827471 -4.7831087 -4.8372526 -4.6793051 -4.3240547 -3.8897455 -3.8280787 -4.221036 -4.7161789 -5.177804 -5.4247508 -5.2868547 -4.9857049 -4.7264166][-5.0573931 -5.276197 -5.3138504 -5.0171938 -4.4574547 -3.7533739 -3.0203285 -2.9108615 -3.6243131 -4.5410128 -5.4561343 -6.1330786 -6.207335 -5.9117312 -5.5414658][-5.6064148 -5.6240478 -5.4014354 -4.7085977 -3.6280189 -2.45401 -1.3030241 -1.0656393 -2.1462767 -3.5659347 -4.9695892 -6.1088896 -6.4451675 -6.2467895 -5.8694162][-5.5773363 -5.4858274 -5.1022835 -4.0650659 -2.4355328 -0.74040556 0.87106228 1.2996449 -0.1159029 -2.0352056 -3.8821793 -5.4252524 -6.014739 -5.9543719 -5.6531925][-5.3882303 -5.3283324 -4.9299564 -3.7109411 -1.6878703 0.38970327 2.3004651 2.8751135 1.2930732 -0.89987612 -2.9554014 -4.7001319 -5.4437141 -5.4819908 -5.2852688][-5.4792852 -5.5862541 -5.3426633 -4.213676 -2.1838033 -0.12758541 1.6861725 2.2488079 0.80378103 -1.2103479 -3.0692315 -4.6824889 -5.3829274 -5.390439 -5.198494][-5.7861943 -6.1199594 -6.1215677 -5.2906418 -3.5967319 -1.898242 -0.45385933 0.01410532 -1.0814941 -2.6195283 -4.0358734 -5.3131518 -5.8709526 -5.8120971 -5.5940065][-5.9463859 -6.4702244 -6.707386 -6.2427144 -5.0614471 -3.8681359 -2.8757548 -2.5300798 -3.2153206 -4.1946535 -5.1113882 -5.9839206 -6.3676043 -6.2662497 -6.03487][-5.8969932 -6.4777365 -6.8473926 -6.6954927 -6.0164337 -5.3061147 -4.7252975 -4.5137305 -4.8682542 -5.3760757 -5.8692508 -6.37522 -6.5958548 -6.4780273 -6.2355084][-5.5486059 -6.0113354 -6.3409305 -6.3416739 -6.0141759 -5.6560488 -5.3630252 -5.2508125 -5.4047961 -5.6262856 -5.8598137 -6.1342993 -6.2758813 -6.2138729 -6.0308385][-4.8197355 -5.079874 -5.27966 -5.3105226 -5.1735559 -5.0223732 -4.8878932 -4.8239207 -4.8788381 -4.9659209 -5.0744743 -5.2315493 -5.3509994 -5.3750257 -5.3085985][-3.8173885 -3.9225519 -4.0222349 -4.0561161 -4.0138659 -3.9662926 -3.9084792 -3.8617609 -3.875082 -3.9165471 -3.9762366 -4.0718164 -4.1632428 -4.2111988 -4.2080512]]...]
INFO - root - 2017-12-06 09:27:25.251959: step 17610, loss = 0.78, batch loss = 0.71 (13.0 examples/sec; 0.616 sec/batch; 53h:54m:19s remains)
INFO - root - 2017-12-06 09:27:31.347855: step 17620, loss = 0.99, batch loss = 0.92 (12.9 examples/sec; 0.618 sec/batch; 54h:04m:16s remains)
INFO - root - 2017-12-06 09:27:37.486290: step 17630, loss = 1.27, batch loss = 1.20 (12.9 examples/sec; 0.622 sec/batch; 54h:25m:07s remains)
INFO - root - 2017-12-06 09:27:43.636843: step 17640, loss = 0.95, batch loss = 0.88 (12.8 examples/sec; 0.624 sec/batch; 54h:32m:11s remains)
INFO - root - 2017-12-06 09:27:49.581963: step 17650, loss = 0.83, batch loss = 0.76 (13.3 examples/sec; 0.603 sec/batch; 52h:46m:17s remains)
INFO - root - 2017-12-06 09:27:55.669570: step 17660, loss = 0.95, batch loss = 0.88 (12.8 examples/sec; 0.627 sec/batch; 54h:50m:33s remains)
INFO - root - 2017-12-06 09:28:01.769078: step 17670, loss = 0.84, batch loss = 0.77 (12.8 examples/sec; 0.624 sec/batch; 54h:31m:43s remains)
INFO - root - 2017-12-06 09:28:07.850034: step 17680, loss = 1.22, batch loss = 1.15 (13.1 examples/sec; 0.612 sec/batch; 53h:31m:22s remains)
INFO - root - 2017-12-06 09:28:13.954460: step 17690, loss = 0.80, batch loss = 0.73 (12.7 examples/sec; 0.632 sec/batch; 55h:15m:48s remains)
INFO - root - 2017-12-06 09:28:20.057678: step 17700, loss = 1.20, batch loss = 1.13 (13.5 examples/sec; 0.594 sec/batch; 51h:54m:45s remains)
2017-12-06 09:28:20.615575: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2418728 -4.36146 -4.3054714 -4.0750818 -3.7058752 -3.5252771 -3.6252837 -3.7953854 -3.7959807 -3.8126061 -3.8655815 -4.0315924 -4.4589667 -5.0299864 -5.1767821][-3.8646076 -3.993696 -3.9921696 -3.8075802 -3.4785488 -3.2665844 -3.2686527 -3.3706155 -3.3215182 -3.2661676 -3.3173022 -3.6009886 -4.2582669 -5.0917206 -5.4111242][-3.5574632 -3.5985699 -3.6181476 -3.5382278 -3.299469 -3.0485609 -2.93856 -3.0081568 -2.9878774 -2.9305897 -3.0428591 -3.4657059 -4.2730312 -5.2396264 -5.6594124][-3.3077011 -3.2715192 -3.2607756 -3.2439728 -3.0417178 -2.7110667 -2.5001125 -2.5509996 -2.6015403 -2.6223373 -2.8533294 -3.4054837 -4.2383 -5.1278157 -5.5190091][-3.0436494 -2.9796093 -2.9445581 -2.9449883 -2.7378988 -2.35074 -2.1310639 -2.2111139 -2.3476975 -2.464627 -2.7780545 -3.3779147 -4.1312237 -4.8337522 -5.1389713][-2.9532371 -2.9145823 -2.8356547 -2.7366488 -2.4116304 -1.9362712 -1.7404397 -1.8896737 -2.1451907 -2.3913097 -2.7490029 -3.2663662 -3.804893 -4.303854 -4.6014733][-2.9180846 -2.9474137 -2.8308406 -2.6008015 -2.1213622 -1.5209796 -1.2844119 -1.468219 -1.8337624 -2.2171497 -2.6220322 -3.0113192 -3.2669334 -3.5560687 -3.9028969][-2.8510504 -2.911484 -2.7440486 -2.3978007 -1.8216429 -1.18026 -0.88808227 -1.0470424 -1.4914997 -2.0072103 -2.5233643 -2.8754086 -2.9280124 -3.0412192 -3.4302027][-3.0554938 -3.0656457 -2.7979279 -2.3766818 -1.813411 -1.2331653 -0.91063213 -0.9841485 -1.4414914 -2.0029628 -2.581615 -2.9208074 -2.8366313 -2.7828612 -3.1388698][-3.3871965 -3.340486 -3.0685894 -2.720098 -2.302907 -1.8477786 -1.5298147 -1.5184979 -1.913096 -2.3819749 -2.836067 -3.0362139 -2.7634256 -2.4936421 -2.727416][-3.5228763 -3.4649236 -3.3320603 -3.1817312 -2.9199107 -2.5069342 -2.1630828 -2.1125798 -2.4510357 -2.7945557 -3.03432 -3.0123143 -2.5400944 -2.0734575 -2.178236][-3.323802 -3.2830212 -3.3340211 -3.3974581 -3.2738771 -2.9127278 -2.5944743 -2.5436506 -2.821624 -3.0483735 -3.0698359 -2.7988224 -2.1350613 -1.5418601 -1.5717764][-3.0418227 -3.0312674 -3.2210579 -3.3998742 -3.3519042 -3.1044765 -2.8922088 -2.879864 -3.1141753 -3.2591119 -3.1308022 -2.6751103 -1.9168708 -1.2933309 -1.2721543][-2.7771902 -2.7890995 -3.0903125 -3.336987 -3.288784 -3.1032162 -2.9424577 -2.9272046 -3.1299667 -3.239531 -3.0504608 -2.5808692 -1.9601758 -1.4793489 -1.4338136][-2.5765748 -2.5894525 -2.9641132 -3.239867 -3.1368287 -2.9220357 -2.6797204 -2.5385771 -2.7000036 -2.8655684 -2.7694812 -2.4546585 -2.1152427 -1.8859084 -1.8845627]]...]
INFO - root - 2017-12-06 09:28:26.694205: step 17710, loss = 0.92, batch loss = 0.85 (12.8 examples/sec; 0.623 sec/batch; 54h:29m:04s remains)
INFO - root - 2017-12-06 09:28:32.857095: step 17720, loss = 0.91, batch loss = 0.84 (12.9 examples/sec; 0.622 sec/batch; 54h:22m:42s remains)
INFO - root - 2017-12-06 09:28:38.996357: step 17730, loss = 0.92, batch loss = 0.85 (14.1 examples/sec; 0.567 sec/batch; 49h:33m:30s remains)
INFO - root - 2017-12-06 09:28:45.094984: step 17740, loss = 0.76, batch loss = 0.69 (13.0 examples/sec; 0.613 sec/batch; 53h:36m:45s remains)
INFO - root - 2017-12-06 09:28:51.071719: step 17750, loss = 1.01, batch loss = 0.94 (13.6 examples/sec; 0.587 sec/batch; 51h:17m:04s remains)
INFO - root - 2017-12-06 09:28:57.239276: step 17760, loss = 1.14, batch loss = 1.07 (13.0 examples/sec; 0.613 sec/batch; 53h:37m:28s remains)
INFO - root - 2017-12-06 09:29:03.345373: step 17770, loss = 0.77, batch loss = 0.70 (13.0 examples/sec; 0.614 sec/batch; 53h:40m:25s remains)
INFO - root - 2017-12-06 09:29:09.460566: step 17780, loss = 0.87, batch loss = 0.80 (12.9 examples/sec; 0.619 sec/batch; 54h:04m:26s remains)
INFO - root - 2017-12-06 09:29:15.536558: step 17790, loss = 0.89, batch loss = 0.82 (13.3 examples/sec; 0.603 sec/batch; 52h:43m:27s remains)
INFO - root - 2017-12-06 09:29:21.584266: step 17800, loss = 0.70, batch loss = 0.63 (13.2 examples/sec; 0.604 sec/batch; 52h:47m:29s remains)
2017-12-06 09:29:22.114076: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8786426 -5.3149538 -5.7467546 -5.8368735 -5.5758128 -5.3127761 -5.2141752 -5.2000132 -5.109868 -4.8161721 -4.4546757 -4.2678304 -4.2517672 -4.2062216 -4.0807977][-4.9159093 -5.3077126 -5.7702069 -5.9080191 -5.6167707 -5.2129006 -4.9514012 -4.8849216 -4.8677864 -4.637516 -4.2564011 -3.9874172 -3.9392259 -3.9662442 -3.9179776][-5.0207157 -5.3847785 -5.8526778 -6.0220571 -5.7184439 -5.1992216 -4.789712 -4.6798043 -4.7390523 -4.6012082 -4.2485371 -3.9444761 -3.8706315 -3.9126074 -3.8715429][-5.0830693 -5.4211922 -5.8879747 -6.1104755 -5.8026447 -5.1573215 -4.5727134 -4.3929844 -4.5380745 -4.570437 -4.3539038 -4.1047196 -4.0370626 -4.0610032 -3.9755249][-5.0013871 -5.3212452 -5.7805696 -6.0777473 -5.7958994 -5.0338926 -4.2262549 -3.8985372 -4.102191 -4.367063 -4.4005418 -4.3098226 -4.3089595 -4.3292217 -4.2106943][-4.8456182 -5.1843524 -5.6474643 -5.9825525 -5.7039523 -4.8043418 -3.7014141 -3.1328802 -3.3501682 -3.8970819 -4.2690434 -4.4322062 -4.565897 -4.615284 -4.4810863][-4.505909 -4.8551869 -5.3328691 -5.6705256 -5.3481264 -4.2799807 -2.8659081 -2.0543523 -2.3361259 -3.232883 -3.9858885 -4.423912 -4.7046309 -4.7667246 -4.5839262][-3.8840318 -4.2210283 -4.736238 -5.1086488 -4.7491541 -3.5216107 -1.84853 -0.85450292 -1.268883 -2.5350089 -3.6441588 -4.3138876 -4.7224236 -4.7891912 -4.5451283][-3.3213625 -3.5899553 -4.1383004 -4.6260219 -4.3583684 -3.1084576 -1.2968826 -0.16983032 -0.66901684 -2.1378009 -3.4246926 -4.1878481 -4.6506944 -4.7533145 -4.5101829][-3.1695766 -3.3166113 -3.8136058 -4.3999124 -4.3555565 -3.3322234 -1.6597204 -0.57471013 -1.0012522 -2.3380353 -3.5150552 -4.1545115 -4.528944 -4.657536 -4.4706459][-3.3987355 -3.4037595 -3.7457669 -4.3301191 -4.4954772 -3.8070691 -2.4627662 -1.5466878 -1.8653088 -2.9211502 -3.8288336 -4.2209382 -4.4030747 -4.5159445 -4.4066839][-3.8379855 -3.7258561 -3.8587127 -4.3067946 -4.5801816 -4.2253504 -3.2973137 -2.5853343 -2.7907164 -3.5378647 -4.154799 -4.3363047 -4.34273 -4.4072266 -4.3537855][-4.3674388 -4.1521006 -4.0959349 -4.3644934 -4.6608658 -4.5791378 -4.0568285 -3.56996 -3.688767 -4.1405191 -4.4776278 -4.483799 -4.3536916 -4.350328 -4.3222623][-4.6816263 -4.4438667 -4.3051391 -4.4603028 -4.7652435 -4.8753161 -4.6656528 -4.3847594 -4.4588385 -4.7139568 -4.8670316 -4.753746 -4.5112085 -4.3977761 -4.3452072][-4.6040487 -4.4316125 -4.3582463 -4.5141444 -4.8303909 -5.0459332 -5.0269866 -4.8738322 -4.8918848 -5.0306377 -5.0884814 -4.915781 -4.5928221 -4.3432541 -4.2259812]]...]
INFO - root - 2017-12-06 09:29:28.193281: step 17810, loss = 1.10, batch loss = 1.03 (12.8 examples/sec; 0.623 sec/batch; 54h:25m:41s remains)
INFO - root - 2017-12-06 09:29:34.207576: step 17820, loss = 0.90, batch loss = 0.83 (13.7 examples/sec; 0.586 sec/batch; 51h:12m:29s remains)
INFO - root - 2017-12-06 09:29:40.270548: step 17830, loss = 0.95, batch loss = 0.88 (13.1 examples/sec; 0.613 sec/batch; 53h:33m:59s remains)
INFO - root - 2017-12-06 09:29:46.312725: step 17840, loss = 0.93, batch loss = 0.86 (13.3 examples/sec; 0.601 sec/batch; 52h:31m:32s remains)
INFO - root - 2017-12-06 09:29:52.437300: step 17850, loss = 0.97, batch loss = 0.90 (13.3 examples/sec; 0.601 sec/batch; 52h:29m:53s remains)
INFO - root - 2017-12-06 09:29:58.305828: step 17860, loss = 0.82, batch loss = 0.75 (13.1 examples/sec; 0.610 sec/batch; 53h:18m:48s remains)
INFO - root - 2017-12-06 09:30:04.444812: step 17870, loss = 0.89, batch loss = 0.82 (12.7 examples/sec; 0.628 sec/batch; 54h:55m:15s remains)
INFO - root - 2017-12-06 09:30:10.511863: step 17880, loss = 0.97, batch loss = 0.90 (12.9 examples/sec; 0.618 sec/batch; 54h:00m:22s remains)
INFO - root - 2017-12-06 09:30:16.599195: step 17890, loss = 0.97, batch loss = 0.90 (13.2 examples/sec; 0.608 sec/batch; 53h:05m:35s remains)
INFO - root - 2017-12-06 09:30:22.686971: step 17900, loss = 1.10, batch loss = 1.03 (13.1 examples/sec; 0.613 sec/batch; 53h:33m:11s remains)
2017-12-06 09:30:23.266776: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9693635 -2.2116706 -3.0484965 -3.8654337 -4.2082062 -4.0439658 -3.57505 -3.1542368 -2.9501972 -2.774142 -2.8165653 -3.2459867 -3.8386753 -4.4909706 -4.7751384][-2.5383816 -2.8082638 -3.6178162 -4.3386621 -4.5556917 -4.259861 -3.6810579 -3.2259278 -3.1076424 -3.0973134 -3.2051351 -3.5234935 -3.9406776 -4.3868394 -4.5244527][-3.5693438 -3.8297729 -4.4733758 -4.9588132 -5.013855 -4.6208558 -4.0019231 -3.6004987 -3.6096306 -3.7910779 -3.9698532 -4.1379275 -4.3053122 -4.4598227 -4.3849373][-4.3717384 -4.5549259 -4.9415669 -5.1636162 -5.0980082 -4.7268734 -4.2466044 -4.0593839 -4.235661 -4.5549593 -4.7548461 -4.7981277 -4.7618904 -4.6668439 -4.4120579][-4.4267993 -4.4761043 -4.6014395 -4.6340833 -4.5613103 -4.3389392 -4.131526 -4.2241292 -4.5434074 -4.9209914 -5.1213508 -5.1170464 -4.9932432 -4.7953129 -4.5070958][-3.8110082 -3.752476 -3.7281537 -3.7237763 -3.8041067 -3.8572721 -3.9366839 -4.2119594 -4.5408654 -4.8507667 -5.0202589 -5.0328884 -4.9351459 -4.7758632 -4.5772009][-3.0830729 -2.9701846 -2.8739257 -2.8819306 -3.1747255 -3.5567768 -3.8676405 -4.1977367 -4.3920364 -4.4911575 -4.5588779 -4.6258607 -4.6340642 -4.6040912 -4.5348492][-2.7933555 -2.6748867 -2.5170481 -2.4950037 -2.9343734 -3.5791798 -4.0158863 -4.3099513 -4.3472242 -4.1966257 -4.0649633 -4.104948 -4.2187157 -4.3532248 -4.4303532][-2.7822893 -2.6451392 -2.4071217 -2.2960649 -2.761167 -3.5371747 -4.0014005 -4.2305484 -4.17119 -3.8122525 -3.433702 -3.3723383 -3.5916138 -3.9471502 -4.2289047][-2.9366379 -2.788795 -2.4950283 -2.2952306 -2.6794538 -3.414145 -3.8190174 -3.9719789 -3.8592193 -3.3430331 -2.7152114 -2.5048597 -2.7993951 -3.3774977 -3.8866954][-3.2263336 -3.0900083 -2.7631612 -2.5061679 -2.7646351 -3.3615751 -3.7016408 -3.8395512 -3.7286272 -3.1265345 -2.3485048 -2.0152018 -2.310324 -2.99397 -3.6195812][-3.3749845 -3.2476923 -2.8919036 -2.5963817 -2.7029266 -3.0890827 -3.3686171 -3.5919409 -3.5769849 -3.0073047 -2.2500584 -1.897131 -2.1667635 -2.8453207 -3.474494][-3.5497026 -3.4338229 -3.0865879 -2.7998428 -2.7691593 -2.9098752 -3.1020072 -3.4352758 -3.5462725 -3.0903044 -2.5026307 -2.300904 -2.6494875 -3.3237967 -3.8790693][-4.0127387 -3.8736429 -3.5585649 -3.3048282 -3.1823959 -3.1270404 -3.223218 -3.6074886 -3.810446 -3.5048735 -3.1496351 -3.1775227 -3.6959052 -4.4065094 -4.8346677][-4.3012295 -4.1402283 -3.8961296 -3.6980352 -3.542732 -3.3821878 -3.4036038 -3.7665029 -4.0059948 -3.8110304 -3.6069922 -3.8026574 -4.458343 -5.2155466 -5.557622]]...]
INFO - root - 2017-12-06 09:30:29.407669: step 17910, loss = 1.16, batch loss = 1.09 (13.1 examples/sec; 0.610 sec/batch; 53h:19m:13s remains)
INFO - root - 2017-12-06 09:30:35.518700: step 17920, loss = 1.03, batch loss = 0.96 (13.2 examples/sec; 0.608 sec/batch; 53h:05m:15s remains)
INFO - root - 2017-12-06 09:30:41.654294: step 17930, loss = 0.78, batch loss = 0.71 (13.6 examples/sec; 0.589 sec/batch; 51h:26m:45s remains)
INFO - root - 2017-12-06 09:30:47.760107: step 17940, loss = 0.88, batch loss = 0.81 (13.1 examples/sec; 0.611 sec/batch; 53h:21m:31s remains)
INFO - root - 2017-12-06 09:30:53.878045: step 17950, loss = 0.69, batch loss = 0.62 (13.2 examples/sec; 0.608 sec/batch; 53h:06m:51s remains)
INFO - root - 2017-12-06 09:30:59.922583: step 17960, loss = 0.94, batch loss = 0.87 (13.2 examples/sec; 0.604 sec/batch; 52h:46m:07s remains)
INFO - root - 2017-12-06 09:31:05.826125: step 17970, loss = 1.07, batch loss = 1.00 (13.3 examples/sec; 0.603 sec/batch; 52h:42m:11s remains)
INFO - root - 2017-12-06 09:31:11.834899: step 17980, loss = 0.82, batch loss = 0.75 (13.1 examples/sec; 0.612 sec/batch; 53h:29m:09s remains)
INFO - root - 2017-12-06 09:31:17.928334: step 17990, loss = 0.77, batch loss = 0.70 (13.3 examples/sec; 0.603 sec/batch; 52h:38m:16s remains)
INFO - root - 2017-12-06 09:31:23.993902: step 18000, loss = 0.94, batch loss = 0.87 (13.4 examples/sec; 0.597 sec/batch; 52h:10m:39s remains)
2017-12-06 09:31:24.528037: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.32029 -4.19553 -4.057128 -3.9757106 -3.9543452 -3.8903379 -3.8591468 -4.0827994 -4.4184442 -4.7399435 -4.9656157 -5.0189834 -4.7235432 -4.03675 -3.3025842][-4.2839141 -4.12152 -3.9588389 -3.9167554 -3.9729664 -3.9507918 -3.9036379 -4.1200814 -4.5352607 -4.9698558 -5.2550278 -5.2587805 -4.8143735 -3.8824158 -2.9416156][-4.2718663 -4.115756 -3.9691021 -3.9912834 -4.1204138 -4.1314092 -4.0474949 -4.1942391 -4.6123257 -5.1033306 -5.4558835 -5.485425 -5.0489807 -4.0544605 -3.0250187][-4.3018012 -4.2060766 -4.1145358 -4.1929531 -4.3551674 -4.3787 -4.2599111 -4.2904716 -4.5912724 -5.04127 -5.4705372 -5.6574111 -5.4583144 -4.643414 -3.6488576][-4.3614593 -4.35423 -4.3190055 -4.4034095 -4.5296211 -4.5492487 -4.4234748 -4.3299689 -4.4187083 -4.7499948 -5.2517529 -5.689054 -5.9021969 -5.4976749 -4.6927242][-4.4025507 -4.4625783 -4.4464183 -4.4791021 -4.5033793 -4.4791675 -4.3656321 -4.1838427 -4.0518222 -4.2489457 -4.8021894 -5.4904304 -6.1567054 -6.2735057 -5.7810411][-4.3982606 -4.4808125 -4.4296417 -4.3512936 -4.1958575 -4.0327539 -3.884584 -3.6393535 -3.3457952 -3.4637065 -4.101109 -5.0430408 -6.10672 -6.6891718 -6.5221434][-4.3716664 -4.4405284 -4.2929721 -4.0289979 -3.6035929 -3.1443086 -2.8407485 -2.5414081 -2.2122846 -2.3899021 -3.2183504 -4.4431558 -5.8040948 -6.6568422 -6.6627083][-4.3670368 -4.4296074 -4.1761513 -3.6991343 -2.96174 -2.1312253 -1.5842695 -1.2283313 -0.97509885 -1.333046 -2.4085634 -3.8904457 -5.4209104 -6.3367825 -6.3304148][-4.3942976 -4.4908977 -4.1917439 -3.5715814 -2.6089716 -1.5043128 -0.74776173 -0.34180641 -0.194592 -0.73835135 -2.0104749 -3.6381183 -5.2058969 -6.0535173 -5.9461451][-4.4269533 -4.5873122 -4.3262024 -3.7169383 -2.7598004 -1.640502 -0.83319545 -0.38475323 -0.26118088 -0.87195706 -2.1812909 -3.7593241 -5.2077584 -5.8878574 -5.6880689][-4.4270167 -4.6346779 -4.4757967 -4.0378985 -3.3327055 -2.4505277 -1.7876561 -1.344985 -1.1679902 -1.6921875 -2.8233314 -4.1356869 -5.3040304 -5.7584009 -5.5351191][-4.3755536 -4.5832772 -4.5271645 -4.3362274 -3.9991407 -3.4738317 -3.0706797 -2.71601 -2.4855843 -2.8421209 -3.6571927 -4.5760155 -5.3710337 -5.6060019 -5.4328251][-4.2865257 -4.4425282 -4.43429 -4.4436073 -4.4283843 -4.2318473 -4.0798554 -3.8644969 -3.65282 -3.8661957 -4.3604388 -4.8670959 -5.2774696 -5.3252134 -5.2173576][-4.2109709 -4.2908044 -4.272296 -4.3830562 -4.5521874 -4.5716953 -4.5825839 -4.500083 -4.3885145 -4.5380368 -4.7835412 -4.9379449 -5.0067325 -4.9007134 -4.8392286]]...]
INFO - root - 2017-12-06 09:31:30.522774: step 18010, loss = 0.82, batch loss = 0.75 (15.5 examples/sec; 0.516 sec/batch; 45h:02m:55s remains)
INFO - root - 2017-12-06 09:31:36.690560: step 18020, loss = 0.98, batch loss = 0.91 (12.4 examples/sec; 0.644 sec/batch; 56h:17m:46s remains)
INFO - root - 2017-12-06 09:31:42.898584: step 18030, loss = 0.88, batch loss = 0.81 (13.2 examples/sec; 0.608 sec/batch; 53h:06m:37s remains)
INFO - root - 2017-12-06 09:31:48.946773: step 18040, loss = 1.10, batch loss = 1.03 (13.3 examples/sec; 0.603 sec/batch; 52h:38m:52s remains)
INFO - root - 2017-12-06 09:31:55.061241: step 18050, loss = 0.93, batch loss = 0.86 (13.2 examples/sec; 0.607 sec/batch; 53h:03m:33s remains)
INFO - root - 2017-12-06 09:32:01.112825: step 18060, loss = 1.15, batch loss = 1.08 (13.6 examples/sec; 0.587 sec/batch; 51h:18m:22s remains)
INFO - root - 2017-12-06 09:32:07.173891: step 18070, loss = 0.76, batch loss = 0.69 (16.0 examples/sec; 0.499 sec/batch; 43h:36m:45s remains)
INFO - root - 2017-12-06 09:32:13.252316: step 18080, loss = 1.11, batch loss = 1.04 (13.0 examples/sec; 0.614 sec/batch; 53h:36m:55s remains)
INFO - root - 2017-12-06 09:32:19.361310: step 18090, loss = 0.94, batch loss = 0.87 (13.2 examples/sec; 0.605 sec/batch; 52h:49m:48s remains)
INFO - root - 2017-12-06 09:32:25.408016: step 18100, loss = 0.79, batch loss = 0.72 (13.3 examples/sec; 0.603 sec/batch; 52h:38m:57s remains)
2017-12-06 09:32:26.001506: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.04342 -3.7287145 -3.6321254 -3.8330042 -3.8848107 -3.5663512 -3.4254327 -3.906775 -4.5472059 -4.6591897 -4.4150205 -4.2992654 -4.3602085 -4.4691329 -4.4206424][-3.8137803 -3.4673388 -3.4127574 -3.6783557 -3.7286952 -3.2895145 -3.0372281 -3.5476725 -4.2526093 -4.3003311 -3.9657466 -3.885215 -4.0609188 -4.278955 -4.2935605][-3.8774652 -3.5532012 -3.5498891 -3.8407588 -3.8711491 -3.3592043 -3.0084922 -3.4524655 -4.1487355 -4.1710367 -3.85504 -3.8598747 -4.1199956 -4.3888478 -4.418848][-4.2031317 -3.9287305 -3.9481885 -4.199872 -4.1745191 -3.6456306 -3.256 -3.5854471 -4.2334175 -4.3107448 -4.1007953 -4.1481724 -4.372191 -4.6279168 -4.6537318][-4.7028074 -4.4600883 -4.4324841 -4.5602293 -4.4260507 -3.9106216 -3.5569825 -3.8063896 -4.4237571 -4.6112924 -4.5161285 -4.4872832 -4.547616 -4.737608 -4.74114][-5.0689564 -4.8243213 -4.6825628 -4.6274056 -4.3823175 -3.9037628 -3.6078854 -3.7820218 -4.3498297 -4.6805592 -4.7196274 -4.5363822 -4.3186469 -4.3643351 -4.3170366][-4.9718528 -4.724401 -4.4714279 -4.2479072 -3.9244194 -3.468513 -3.1811728 -3.2490888 -3.7149343 -4.1525235 -4.3349524 -4.0655332 -3.6411147 -3.5883117 -3.5307426][-4.658977 -4.4123721 -4.0892253 -3.7486904 -3.3547564 -2.8720832 -2.5228579 -2.4755368 -2.8123064 -3.2767439 -3.5920405 -3.3865085 -2.9637508 -2.9292054 -2.9228749][-4.3036528 -4.1149197 -3.7760887 -3.3733733 -2.9388447 -2.4450831 -2.0714841 -1.9864788 -2.2504628 -2.7100821 -3.1253014 -3.0879915 -2.8159895 -2.8018928 -2.7583628][-4.0848231 -4.0055342 -3.7374041 -3.3772466 -2.9573817 -2.4720123 -2.1024904 -2.0017254 -2.1899822 -2.5951333 -3.052192 -3.2038975 -3.0907464 -3.0124235 -2.8380659][-4.1376562 -4.131598 -3.9533436 -3.7175577 -3.3933468 -2.9848237 -2.6784902 -2.5633769 -2.6401186 -2.9136248 -3.318325 -3.5742252 -3.5238054 -3.2768033 -2.927618][-4.1782517 -4.1786628 -4.0756984 -4.0158315 -3.885426 -3.6854348 -3.5607166 -3.5004404 -3.469094 -3.5446129 -3.8014443 -4.0343833 -3.942873 -3.503938 -3.0206523][-4.0397944 -4.0266156 -3.9418466 -3.9869678 -4.0343065 -4.0633397 -4.1481385 -4.1727114 -4.0608816 -3.9774828 -4.1045718 -4.2588787 -4.0817919 -3.5052528 -2.980262][-3.9025488 -3.836838 -3.6835315 -3.6944356 -3.7792907 -3.8758218 -4.0338712 -4.1181345 -4.0075259 -3.8914089 -3.994247 -4.1302323 -3.9521611 -3.4007239 -2.9456096][-3.9722815 -3.8208933 -3.5793738 -3.4918294 -3.502871 -3.4932806 -3.5610685 -3.6578598 -3.6189415 -3.5755694 -3.7237558 -3.88212 -3.7450747 -3.2868605 -2.938663]]...]
INFO - root - 2017-12-06 09:32:32.168509: step 18110, loss = 0.95, batch loss = 0.88 (13.0 examples/sec; 0.617 sec/batch; 53h:53m:06s remains)
INFO - root - 2017-12-06 09:32:38.232645: step 18120, loss = 0.88, batch loss = 0.81 (13.0 examples/sec; 0.613 sec/batch; 53h:33m:34s remains)
INFO - root - 2017-12-06 09:32:44.398749: step 18130, loss = 0.76, batch loss = 0.69 (13.2 examples/sec; 0.605 sec/batch; 52h:48m:32s remains)
INFO - root - 2017-12-06 09:32:50.522494: step 18140, loss = 1.10, batch loss = 1.03 (12.9 examples/sec; 0.622 sec/batch; 54h:19m:07s remains)
INFO - root - 2017-12-06 09:32:56.546403: step 18150, loss = 0.87, batch loss = 0.80 (13.0 examples/sec; 0.617 sec/batch; 53h:51m:58s remains)
INFO - root - 2017-12-06 09:33:02.732883: step 18160, loss = 0.78, batch loss = 0.71 (12.5 examples/sec; 0.638 sec/batch; 55h:42m:32s remains)
INFO - root - 2017-12-06 09:33:08.693400: step 18170, loss = 0.99, batch loss = 0.92 (13.6 examples/sec; 0.587 sec/batch; 51h:16m:14s remains)
INFO - root - 2017-12-06 09:33:14.619720: step 18180, loss = 0.83, batch loss = 0.76 (13.6 examples/sec; 0.590 sec/batch; 51h:31m:39s remains)
INFO - root - 2017-12-06 09:33:20.783111: step 18190, loss = 0.95, batch loss = 0.88 (12.8 examples/sec; 0.623 sec/batch; 54h:22m:54s remains)
INFO - root - 2017-12-06 09:33:26.969713: step 18200, loss = 1.14, batch loss = 1.07 (13.2 examples/sec; 0.605 sec/batch; 52h:48m:58s remains)
2017-12-06 09:33:27.558638: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0050144 -4.9647241 -4.8042517 -4.5810971 -4.469233 -4.4170837 -4.16237 -3.8263223 -3.7446213 -3.7431748 -3.7828774 -4.0947113 -4.4809713 -4.5820513 -4.4712629][-5.02435 -5.0188074 -4.8827853 -4.6480021 -4.4436288 -4.307817 -4.0192652 -3.659019 -3.6538143 -3.8117363 -3.9381561 -4.2250576 -4.63338 -4.83117 -4.7386622][-5.0345078 -5.0661178 -4.9771061 -4.7462492 -4.4456005 -4.1702657 -3.8153598 -3.4818251 -3.5955918 -3.9524038 -4.2449675 -4.5049186 -4.7902884 -4.9362893 -4.8032465][-5.0404048 -5.080761 -4.99895 -4.7500925 -4.3776917 -3.9773977 -3.5616329 -3.307528 -3.5812163 -4.1248693 -4.5873685 -4.79009 -4.8024411 -4.7268791 -4.5565329][-5.04965 -5.05568 -4.9087729 -4.5787311 -4.123879 -3.5993633 -3.0949516 -2.8850968 -3.302218 -4.0159731 -4.6581774 -4.858026 -4.6120276 -4.2711091 -4.0715432][-5.0545831 -4.9978075 -4.7332945 -4.2620878 -3.6900451 -3.0222392 -2.3377485 -2.0227723 -2.4685328 -3.3262568 -4.2165723 -4.5887675 -4.3306532 -3.875788 -3.6647794][-5.0775571 -4.9854894 -4.6388407 -4.0563836 -3.3709202 -2.5592246 -1.6125829 -1.0205123 -1.3292172 -2.2438316 -3.3922577 -4.0776644 -4.0495324 -3.66342 -3.4767146][-5.1199365 -5.0566688 -4.7440825 -4.1748958 -3.4710877 -2.6214292 -1.5138676 -0.64621305 -0.66491628 -1.41534 -2.6073842 -3.5076091 -3.7444663 -3.5255728 -3.4148688][-5.1636896 -5.1878805 -5.0064354 -4.5662403 -3.9468319 -3.211102 -2.177017 -1.2194281 -0.9302218 -1.2878039 -2.1856523 -3.0514462 -3.4115334 -3.3111506 -3.2613685][-5.1895914 -5.2959743 -5.2675633 -5.0103087 -4.5426059 -4.0019436 -3.2115676 -2.3599846 -1.9012246 -1.8619986 -2.3042505 -2.9160149 -3.1898565 -3.0879302 -3.0478573][-5.178628 -5.3281198 -5.4120526 -5.3245645 -5.0023022 -4.5946484 -3.9995096 -3.2976136 -2.8084447 -2.5477433 -2.6441112 -2.9867463 -3.1398044 -3.0787454 -3.1021576][-5.1235638 -5.2748942 -5.4070406 -5.437242 -5.2208514 -4.8592725 -4.3386192 -3.742655 -3.2869513 -2.9825263 -2.934706 -3.1192889 -3.2300091 -3.3141384 -3.4867399][-5.0406232 -5.1763654 -5.3142581 -5.4260707 -5.31793 -5.0022211 -4.5312185 -4.0475044 -3.6836629 -3.4389458 -3.3736997 -3.450207 -3.4801745 -3.6268535 -3.8850789][-4.9575348 -5.071754 -5.1708555 -5.2893267 -5.2485652 -4.9811034 -4.5763645 -4.22673 -4.0078883 -3.8685379 -3.8413849 -3.8452685 -3.754492 -3.8254795 -4.0638175][-4.9065967 -5.0047622 -5.0464396 -5.0991526 -5.059617 -4.8286772 -4.4926395 -4.2549567 -4.152266 -4.0875077 -4.094367 -4.0726404 -3.9085095 -3.8938165 -4.073173]]...]
INFO - root - 2017-12-06 09:33:33.714161: step 18210, loss = 0.94, batch loss = 0.87 (13.4 examples/sec; 0.595 sec/batch; 51h:58m:22s remains)
INFO - root - 2017-12-06 09:33:39.832511: step 18220, loss = 0.86, batch loss = 0.79 (13.0 examples/sec; 0.613 sec/batch; 53h:32m:43s remains)
INFO - root - 2017-12-06 09:33:45.885128: step 18230, loss = 0.88, batch loss = 0.81 (13.0 examples/sec; 0.614 sec/batch; 53h:34m:13s remains)
INFO - root - 2017-12-06 09:33:52.033710: step 18240, loss = 0.91, batch loss = 0.84 (12.9 examples/sec; 0.622 sec/batch; 54h:16m:57s remains)
INFO - root - 2017-12-06 09:33:58.112629: step 18250, loss = 1.12, batch loss = 1.05 (13.4 examples/sec; 0.596 sec/batch; 51h:58m:58s remains)
INFO - root - 2017-12-06 09:34:04.158600: step 18260, loss = 0.85, batch loss = 0.78 (13.4 examples/sec; 0.598 sec/batch; 52h:10m:31s remains)
INFO - root - 2017-12-06 09:34:10.207633: step 18270, loss = 0.91, batch loss = 0.84 (13.4 examples/sec; 0.596 sec/batch; 52h:00m:40s remains)
INFO - root - 2017-12-06 09:34:16.214758: step 18280, loss = 0.79, batch loss = 0.72 (13.2 examples/sec; 0.607 sec/batch; 53h:00m:32s remains)
INFO - root - 2017-12-06 09:34:22.167004: step 18290, loss = 1.02, batch loss = 0.94 (12.8 examples/sec; 0.626 sec/batch; 54h:36m:12s remains)
INFO - root - 2017-12-06 09:34:28.339047: step 18300, loss = 0.88, batch loss = 0.81 (12.9 examples/sec; 0.618 sec/batch; 53h:56m:59s remains)
2017-12-06 09:34:28.925941: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9998984 -3.4151793 -4.0631008 -4.6134691 -4.9204149 -5.0612063 -5.1217842 -5.1133285 -5.1598897 -5.2572632 -5.2752419 -5.1599889 -4.97969 -4.9688072 -5.3213673][-3.3214746 -3.8631341 -4.5840015 -5.0653219 -5.1679683 -5.0423546 -4.8348775 -4.6376805 -4.6835418 -4.9698181 -5.2352314 -5.3244095 -5.2413392 -5.2375131 -5.5446072][-3.4680066 -4.1722693 -5.0090227 -5.455904 -5.3688946 -4.9459753 -4.3866806 -3.9155598 -3.9228556 -4.4021459 -4.9474287 -5.2811427 -5.3594637 -5.4328465 -5.74094][-3.3984663 -4.2335877 -5.18127 -5.6302195 -5.3997903 -4.6896729 -3.7429152 -2.940275 -2.878006 -3.5648489 -4.4236064 -5.0232606 -5.2740784 -5.460979 -5.8308868][-3.2708385 -4.1613083 -5.1648188 -5.6172786 -5.2812262 -4.3236408 -3.0040717 -1.8439422 -1.6683245 -2.5586386 -3.7635248 -4.6472511 -5.0544629 -5.3574672 -5.8424339][-3.2228663 -4.0909824 -5.0722828 -5.5017138 -5.0832858 -3.9480605 -2.3341005 -0.83891249 -0.49308944 -1.5281858 -3.074523 -4.26414 -4.831965 -5.2307706 -5.8281379][-3.30404 -4.0962806 -4.9923568 -5.4040112 -4.9773893 -3.7928662 -2.0388019 -0.31247282 0.25203133 -0.76227665 -2.4967761 -3.9412966 -4.692245 -5.174408 -5.8228579][-3.5325127 -4.1954622 -4.9527373 -5.3786731 -5.0759583 -4.0292773 -2.3557644 -0.5900929 0.18431997 -0.5803287 -2.2419765 -3.7967815 -4.715744 -5.2594604 -5.8773379][-3.767344 -4.2426915 -4.814507 -5.2677517 -5.2036037 -4.481041 -3.1151943 -1.544163 -0.67882252 -1.0884757 -2.4295533 -3.8864615 -4.8858309 -5.4464631 -5.9522662][-3.799495 -4.0663357 -4.4430633 -4.908967 -5.1315293 -4.8515759 -3.9471407 -2.7197719 -1.8920939 -1.9934392 -2.9344168 -4.1382661 -5.0758185 -5.5687056 -5.9221773][-3.4892046 -3.5710454 -3.7783399 -4.2079687 -4.6596842 -4.8238182 -4.4403887 -3.6192515 -2.9212914 -2.8379025 -3.4287655 -4.3343034 -5.1200047 -5.4932718 -5.6946096][-2.9648089 -2.8999522 -2.9792295 -3.3102028 -3.8622084 -4.3879309 -4.5172176 -4.1138496 -3.5835428 -3.4189477 -3.719677 -4.3222294 -4.9332571 -5.1948419 -5.3035884][-2.5497096 -2.3721313 -2.3551288 -2.5378957 -3.0360126 -3.7561674 -4.288115 -4.2360535 -3.84724 -3.646477 -3.7382939 -4.1148963 -4.611794 -4.8161 -4.8630776][-2.4993451 -2.2498958 -2.1612532 -2.18769 -2.5182424 -3.2281144 -3.9515049 -4.1181293 -3.8430743 -3.6606922 -3.6628823 -3.9465938 -4.4275284 -4.6376095 -4.5981708][-2.8053675 -2.5498791 -2.4512048 -2.3955946 -2.5510826 -3.0993371 -3.7994504 -4.0564265 -3.892488 -3.773196 -3.7882116 -4.0929832 -4.6319461 -4.8937097 -4.7408686]]...]
INFO - root - 2017-12-06 09:34:34.959334: step 18310, loss = 1.05, batch loss = 0.98 (13.1 examples/sec; 0.612 sec/batch; 53h:25m:51s remains)
INFO - root - 2017-12-06 09:34:40.893324: step 18320, loss = 0.90, batch loss = 0.83 (12.6 examples/sec; 0.636 sec/batch; 55h:32m:55s remains)
INFO - root - 2017-12-06 09:34:47.014375: step 18330, loss = 0.90, batch loss = 0.83 (13.3 examples/sec; 0.602 sec/batch; 52h:34m:29s remains)
INFO - root - 2017-12-06 09:34:53.070390: step 18340, loss = 0.69, batch loss = 0.62 (13.1 examples/sec; 0.610 sec/batch; 53h:11m:47s remains)
INFO - root - 2017-12-06 09:34:59.161742: step 18350, loss = 0.89, batch loss = 0.82 (12.9 examples/sec; 0.618 sec/batch; 53h:56m:16s remains)
INFO - root - 2017-12-06 09:35:05.311136: step 18360, loss = 0.69, batch loss = 0.62 (13.6 examples/sec; 0.589 sec/batch; 51h:24m:21s remains)
INFO - root - 2017-12-06 09:35:11.422017: step 18370, loss = 0.92, batch loss = 0.85 (12.8 examples/sec; 0.623 sec/batch; 54h:20m:21s remains)
INFO - root - 2017-12-06 09:35:17.480670: step 18380, loss = 0.71, batch loss = 0.64 (13.3 examples/sec; 0.603 sec/batch; 52h:38m:44s remains)
INFO - root - 2017-12-06 09:35:23.565649: step 18390, loss = 0.89, batch loss = 0.82 (13.5 examples/sec; 0.595 sec/batch; 51h:53m:28s remains)
INFO - root - 2017-12-06 09:35:29.450633: step 18400, loss = 1.23, batch loss = 1.16 (13.5 examples/sec; 0.595 sec/batch; 51h:52m:17s remains)
2017-12-06 09:35:30.025653: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.108407 -5.1865063 -5.2206106 -5.2322607 -5.2770119 -5.3661532 -5.4722347 -5.5423117 -5.5034318 -5.3828998 -5.279954 -5.2966509 -5.4256024 -5.6090527 -5.8183823][-5.3180671 -5.44848 -5.4862123 -5.4752369 -5.5186124 -5.6608939 -5.8302855 -5.88388 -5.7654352 -5.5340586 -5.3140893 -5.2507057 -5.3707891 -5.6176395 -5.9233675][-5.4172916 -5.5330982 -5.5056849 -5.4195237 -5.4139471 -5.5579462 -5.7281427 -5.74358 -5.6318073 -5.4371042 -5.228971 -5.137084 -5.2190771 -5.4914894 -5.8586378][-5.5170231 -5.548471 -5.3793592 -5.1183176 -4.9427786 -4.948278 -5.0308075 -5.0601993 -5.0987587 -5.1001306 -5.0128212 -4.8945012 -4.8887281 -5.1344452 -5.5330172][-5.6064591 -5.4984341 -5.1385593 -4.6445055 -4.2340503 -4.0398364 -4.0522451 -4.17764 -4.4319077 -4.6172638 -4.5492954 -4.2539043 -4.0675116 -4.2617674 -4.7330494][-5.6517386 -5.3704333 -4.8052492 -4.0849757 -3.4609284 -3.1129842 -3.1188202 -3.3715169 -3.7512562 -3.9518924 -3.7220726 -3.1424813 -2.7836733 -2.9798827 -3.612215][-5.6470613 -5.176393 -4.4415169 -3.5312738 -2.7074137 -2.2557533 -2.3395593 -2.7528119 -3.1512032 -3.234489 -2.8017707 -2.0155017 -1.5932631 -1.818877 -2.6060538][-5.5075641 -4.8622851 -4.0293202 -3.0423462 -2.1753109 -1.7920353 -2.073827 -2.6888671 -3.0444775 -2.9485066 -2.3488343 -1.5247512 -1.1760578 -1.4223285 -2.2146821][-5.2303348 -4.5190392 -3.7157078 -2.8248339 -2.1357517 -1.9659204 -2.4547906 -3.1870515 -3.3965435 -3.133677 -2.4949384 -1.7966678 -1.6097934 -1.8833048 -2.5776396][-4.8549285 -4.2237015 -3.551394 -2.8639355 -2.4635329 -2.5079966 -3.0860267 -3.7753558 -3.7860298 -3.4377952 -2.9134674 -2.4458559 -2.4429016 -2.7778583 -3.4080544][-4.4992146 -4.0677724 -3.5934267 -3.1340151 -2.9799316 -3.1548493 -3.7148054 -4.2164774 -3.9817166 -3.5392032 -3.1395617 -2.9494557 -3.1706591 -3.6355705 -4.2853212][-4.2625341 -4.1159654 -3.8728521 -3.59969 -3.5611033 -3.7502499 -4.1943488 -4.4498682 -4.0792403 -3.617157 -3.34649 -3.4095407 -3.8161662 -4.3624854 -4.9761343][-4.190341 -4.3195791 -4.2705016 -4.1209159 -4.1341791 -4.335042 -4.6807971 -4.7695961 -4.4414091 -4.0859623 -3.9220824 -4.0838223 -4.5011754 -4.9998312 -5.5058856][-4.1529565 -4.4635124 -4.5193624 -4.4180803 -4.4307632 -4.6404281 -4.9507656 -4.9996257 -4.8229074 -4.6662273 -4.6261849 -4.7930179 -5.1039534 -5.45248 -5.7824626][-4.0625896 -4.3849063 -4.4409647 -4.3276558 -4.2877407 -4.4585862 -4.7624159 -4.8990426 -4.9321766 -4.9871378 -5.0513868 -5.1502547 -5.282289 -5.4393468 -5.5875192]]...]
INFO - root - 2017-12-06 09:35:36.131518: step 18410, loss = 0.81, batch loss = 0.74 (13.1 examples/sec; 0.608 sec/batch; 53h:04m:41s remains)
INFO - root - 2017-12-06 09:35:42.309997: step 18420, loss = 0.77, batch loss = 0.70 (12.9 examples/sec; 0.619 sec/batch; 53h:58m:53s remains)
INFO - root - 2017-12-06 09:35:48.429908: step 18430, loss = 1.03, batch loss = 0.96 (12.8 examples/sec; 0.623 sec/batch; 54h:19m:00s remains)
INFO - root - 2017-12-06 09:35:54.575331: step 18440, loss = 1.01, batch loss = 0.94 (13.2 examples/sec; 0.607 sec/batch; 52h:59m:16s remains)
INFO - root - 2017-12-06 09:36:00.726166: step 18450, loss = 0.86, batch loss = 0.79 (12.4 examples/sec; 0.644 sec/batch; 56h:08m:51s remains)
INFO - root - 2017-12-06 09:36:06.936073: step 18460, loss = 0.92, batch loss = 0.85 (13.1 examples/sec; 0.609 sec/batch; 53h:08m:35s remains)
INFO - root - 2017-12-06 09:36:12.574541: step 18470, loss = 1.13, batch loss = 1.06 (13.1 examples/sec; 0.609 sec/batch; 53h:08m:31s remains)
INFO - root - 2017-12-06 09:36:18.702654: step 18480, loss = 1.11, batch loss = 1.04 (13.0 examples/sec; 0.614 sec/batch; 53h:34m:43s remains)
INFO - root - 2017-12-06 09:36:24.751827: step 18490, loss = 0.79, batch loss = 0.72 (13.4 examples/sec; 0.596 sec/batch; 51h:59m:29s remains)
INFO - root - 2017-12-06 09:36:30.685571: step 18500, loss = 0.83, batch loss = 0.76 (12.8 examples/sec; 0.626 sec/batch; 54h:34m:31s remains)
2017-12-06 09:36:31.246252: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9214234 -4.0397987 -4.1425843 -4.2130232 -4.2572207 -4.3098979 -4.3829718 -4.441124 -4.4564357 -4.427309 -4.3734951 -4.3744488 -4.4559436 -4.5686131 -4.6546941][-4.0805044 -4.2277946 -4.3245816 -4.3574338 -4.3535748 -4.3947325 -4.5163765 -4.651566 -4.7286024 -4.7212205 -4.64963 -4.6070213 -4.64045 -4.7098746 -4.7548866][-4.3413339 -4.4747057 -4.506247 -4.4301982 -4.3002529 -4.2586846 -4.3886609 -4.612452 -4.8062277 -4.8838997 -4.8396568 -4.7771692 -4.7763233 -4.8147755 -4.8299785][-4.5820951 -4.6214533 -4.4955921 -4.2160525 -3.8692756 -3.6570482 -3.7230046 -4.0264516 -4.403028 -4.6710868 -4.7505512 -4.7390928 -4.7564244 -4.8070374 -4.83263][-4.7247362 -4.6120815 -4.2600689 -3.7015877 -3.0574124 -2.5825925 -2.4782276 -2.79916 -3.3831077 -3.9391339 -4.262825 -4.4037132 -4.5188313 -4.6444697 -4.7369447][-4.7517943 -4.5029473 -3.9411981 -3.1128142 -2.1570823 -1.3637106 -0.98900795 -1.2164989 -1.9472299 -2.8034737 -3.4433866 -3.8268521 -4.1151729 -4.3714695 -4.5771255][-4.6546593 -4.3461943 -3.6822143 -2.6985016 -1.5300779 -0.47419214 0.18008471 0.1349988 -0.612443 -1.6615443 -2.5798173 -3.2292745 -3.7202084 -4.1183205 -4.4402137][-4.4660482 -4.1963286 -3.5878434 -2.6361995 -1.4432175 -0.27749634 0.5592165 0.70888758 0.085869789 -0.96800184 -2.0208714 -2.8736975 -3.5286627 -4.0148411 -4.3970647][-4.2218075 -4.0317626 -3.5894544 -2.8401973 -1.8228595 -0.7464323 0.10552311 0.37572336 -0.041888237 -0.93622136 -1.9622264 -2.9010019 -3.6236484 -4.0933475 -4.4345188][-4.0127316 -3.8779163 -3.6278014 -3.1739697 -2.4693317 -1.6390576 -0.91460347 -0.61203051 -0.81754756 -1.4640198 -2.343528 -3.2435236 -3.9190097 -4.2772884 -4.4922919][-3.9838836 -3.8670695 -3.7563424 -3.5700221 -3.1888552 -2.6490483 -2.1156912 -1.8352728 -1.8742414 -2.2675788 -2.9449677 -3.719085 -4.2738466 -4.4951429 -4.5690227][-4.2370558 -4.132194 -4.073091 -4.0299582 -3.8715224 -3.5720882 -3.2211275 -2.9865518 -2.9177513 -3.0955014 -3.5620322 -4.1775703 -4.6069469 -4.7220039 -4.678699][-4.6570706 -4.6265111 -4.59196 -4.57801 -4.5056925 -4.3368464 -4.105473 -3.9024262 -3.7591205 -3.7722211 -4.0549231 -4.5232091 -4.8559604 -4.89951 -4.749712][-4.860971 -5.0194077 -5.0877852 -5.0973783 -5.0429964 -4.9161191 -4.7283692 -4.5228839 -4.3199267 -4.2213435 -4.3675528 -4.719924 -4.9807115 -4.9637847 -4.7006235][-4.4820867 -4.8859229 -5.1609254 -5.2907705 -5.3090982 -5.229125 -5.0601549 -4.83958 -4.5968757 -4.4482932 -4.5241928 -4.8024893 -5.0005336 -4.9169044 -4.5437927]]...]
INFO - root - 2017-12-06 09:36:37.328563: step 18510, loss = 0.93, batch loss = 0.86 (13.4 examples/sec; 0.596 sec/batch; 51h:57m:22s remains)
INFO - root - 2017-12-06 09:36:43.498429: step 18520, loss = 0.93, batch loss = 0.86 (12.9 examples/sec; 0.618 sec/batch; 53h:55m:33s remains)
INFO - root - 2017-12-06 09:36:49.500096: step 18530, loss = 0.91, batch loss = 0.84 (13.2 examples/sec; 0.607 sec/batch; 52h:57m:05s remains)
INFO - root - 2017-12-06 09:36:55.623379: step 18540, loss = 0.80, batch loss = 0.73 (13.4 examples/sec; 0.599 sec/batch; 52h:14m:45s remains)
INFO - root - 2017-12-06 09:37:01.705733: step 18550, loss = 0.80, batch loss = 0.73 (13.5 examples/sec; 0.592 sec/batch; 51h:36m:51s remains)
INFO - root - 2017-12-06 09:37:07.745642: step 18560, loss = 0.79, batch loss = 0.72 (13.3 examples/sec; 0.604 sec/batch; 52h:38m:06s remains)
INFO - root - 2017-12-06 09:37:13.789094: step 18570, loss = 0.77, batch loss = 0.70 (12.8 examples/sec; 0.624 sec/batch; 54h:27m:00s remains)
INFO - root - 2017-12-06 09:37:19.866277: step 18580, loss = 0.75, batch loss = 0.68 (13.0 examples/sec; 0.614 sec/batch; 53h:33m:43s remains)
INFO - root - 2017-12-06 09:37:25.952211: step 18590, loss = 0.83, batch loss = 0.76 (13.1 examples/sec; 0.610 sec/batch; 53h:13m:15s remains)
INFO - root - 2017-12-06 09:37:31.965780: step 18600, loss = 0.82, batch loss = 0.75 (13.3 examples/sec; 0.603 sec/batch; 52h:36m:28s remains)
2017-12-06 09:37:32.536034: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3862073 -3.1530485 -3.0168662 -3.1175151 -3.4454012 -3.8436973 -4.0058293 -3.9805782 -3.7135756 -3.3922341 -3.3410506 -3.4408979 -3.7711625 -4.279551 -4.6033821][-3.272418 -3.1211648 -3.0137644 -3.1302524 -3.42033 -3.8226008 -4.0368385 -4.0172224 -3.6749251 -3.2291918 -3.0886421 -3.20311 -3.5486732 -3.9738259 -4.193553][-3.2932372 -3.3423624 -3.3079376 -3.4147081 -3.6117363 -3.9090743 -4.100461 -4.1033154 -3.8189306 -3.3550425 -3.0887091 -3.099143 -3.315392 -3.5836062 -3.7097003][-3.3500576 -3.6601386 -3.7014103 -3.7473693 -3.8263414 -3.9917595 -4.1187897 -4.0886736 -3.9098675 -3.5462251 -3.2613964 -3.1835718 -3.2029841 -3.312114 -3.3942449][-3.3947527 -3.9108646 -4.0000896 -3.9387584 -3.9185939 -3.9456089 -3.8945751 -3.6088624 -3.4108167 -3.2964349 -3.2914641 -3.3847826 -3.3491349 -3.380501 -3.467731][-3.5458426 -4.1376977 -4.2177873 -4.0200915 -3.888186 -3.7439239 -3.3833122 -2.62036 -2.2317152 -2.4956293 -3.0438511 -3.55658 -3.6715808 -3.7173138 -3.8350639][-3.8286974 -4.3487873 -4.3561263 -4.0311728 -3.7732239 -3.4379959 -2.7349277 -1.4359729 -0.717448 -1.2890089 -2.3699095 -3.3348277 -3.7602739 -3.9592397 -4.1839523][-4.1481361 -4.5171442 -4.4458642 -4.0986972 -3.7970271 -3.3344312 -2.4197326 -0.80681348 0.22359371 -0.42793417 -1.6954365 -2.8333561 -3.4823427 -3.8423414 -4.2054119][-4.5123372 -4.7379723 -4.6278238 -4.3707142 -4.1378732 -3.6550994 -2.7474296 -1.2526188 -0.19047689 -0.69074583 -1.6909494 -2.5529733 -3.1253309 -3.4685254 -3.911258][-4.8194857 -4.9819679 -4.8835897 -4.7461171 -4.6492925 -4.2519832 -3.4918849 -2.3583763 -1.4609578 -1.7585597 -2.3143551 -2.6949511 -2.9911649 -3.181596 -3.5858312][-4.9653325 -5.1207848 -5.0808954 -5.0864062 -5.1671634 -4.8979354 -4.2715082 -3.4592004 -2.7793593 -2.9218109 -3.1284907 -3.1090832 -3.1207738 -3.1442516 -3.4775085][-4.9857225 -5.1391635 -5.1617537 -5.26015 -5.4531336 -5.2876639 -4.7629609 -4.201088 -3.7532647 -3.7900927 -3.7881453 -3.5467985 -3.3866014 -3.3541331 -3.6849284][-4.8818889 -5.0122962 -5.0646868 -5.1792083 -5.3701482 -5.2675676 -4.8627262 -4.5297384 -4.3070221 -4.2669311 -4.1839914 -3.9208469 -3.7294471 -3.7436023 -4.0899405][-4.7176428 -4.80737 -4.8453546 -4.9099336 -5.0257726 -4.9393182 -4.6420336 -4.4632988 -4.3938813 -4.3313117 -4.2859125 -4.1446915 -4.0260205 -4.10127 -4.4203396][-4.5990639 -4.6011114 -4.5636134 -4.5380144 -4.5339179 -4.4010663 -4.1722522 -4.0989995 -4.1446376 -4.1451931 -4.2116361 -4.2264271 -4.1886926 -4.2691293 -4.5111809]]...]
INFO - root - 2017-12-06 09:37:38.523172: step 18610, loss = 0.96, batch loss = 0.89 (12.8 examples/sec; 0.627 sec/batch; 54h:40m:01s remains)
INFO - root - 2017-12-06 09:37:44.504036: step 18620, loss = 0.67, batch loss = 0.60 (13.4 examples/sec; 0.599 sec/batch; 52h:12m:45s remains)
INFO - root - 2017-12-06 09:37:50.582402: step 18630, loss = 0.85, batch loss = 0.78 (12.7 examples/sec; 0.631 sec/batch; 55h:02m:54s remains)
INFO - root - 2017-12-06 09:37:56.609098: step 18640, loss = 1.00, batch loss = 0.93 (12.7 examples/sec; 0.630 sec/batch; 54h:53m:50s remains)
INFO - root - 2017-12-06 09:38:02.687901: step 18650, loss = 0.85, batch loss = 0.78 (12.9 examples/sec; 0.619 sec/batch; 53h:59m:39s remains)
INFO - root - 2017-12-06 09:38:08.753092: step 18660, loss = 0.99, batch loss = 0.92 (13.1 examples/sec; 0.609 sec/batch; 53h:05m:40s remains)
INFO - root - 2017-12-06 09:38:14.838308: step 18670, loss = 0.88, batch loss = 0.81 (13.4 examples/sec; 0.597 sec/batch; 52h:01m:55s remains)
INFO - root - 2017-12-06 09:38:20.921635: step 18680, loss = 0.75, batch loss = 0.68 (13.3 examples/sec; 0.603 sec/batch; 52h:31m:47s remains)
INFO - root - 2017-12-06 09:38:26.968295: step 18690, loss = 0.90, batch loss = 0.83 (13.2 examples/sec; 0.608 sec/batch; 52h:58m:52s remains)
INFO - root - 2017-12-06 09:38:33.086142: step 18700, loss = 0.89, batch loss = 0.82 (12.9 examples/sec; 0.621 sec/batch; 54h:09m:39s remains)
2017-12-06 09:38:33.728984: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5710669 -5.1889186 -4.9292765 -4.53358 -4.5362234 -4.9833488 -5.6174126 -6.2024927 -6.444396 -5.7752633 -4.6579618 -3.5115008 -3.1934695 -3.6387806 -3.8952088][-5.377449 -4.9715981 -4.8466082 -4.4704461 -4.3964524 -4.742485 -5.3747425 -6.14631 -6.6194878 -6.0443125 -4.9517345 -3.7747252 -3.4469438 -3.9116762 -4.1221495][-5.0549107 -4.605669 -4.5119238 -4.0525084 -3.789299 -3.9899554 -4.5976152 -5.5154481 -6.2587109 -5.9971 -5.2498488 -4.3233476 -4.0318456 -4.3793874 -4.4525666][-4.6379433 -4.0787148 -3.8554716 -3.2249205 -2.7345414 -2.820333 -3.4076283 -4.4090581 -5.3949542 -5.5851192 -5.4182868 -5.0035973 -4.8496995 -5.0167289 -4.8999529][-4.136127 -3.4064569 -2.9533675 -2.1420732 -1.4718986 -1.5130324 -2.1254604 -3.1939964 -4.4090886 -5.1108842 -5.5986109 -5.7847538 -5.8103957 -5.7872944 -5.4511471][-3.9093676 -2.9374037 -2.1915646 -1.226428 -0.45683932 -0.49271321 -1.1447272 -2.2243316 -3.5598755 -4.6359854 -5.6454468 -6.3492851 -6.5770035 -6.4561481 -5.9677382][-4.1830688 -2.9815345 -1.9599016 -0.90254569 -0.12492943 -0.18056345 -0.82901478 -1.7697601 -2.94361 -4.0278807 -5.2450948 -6.3037367 -6.7837687 -6.7762494 -6.3381424][-4.8430643 -3.5188959 -2.3001029 -1.2349458 -0.55137372 -0.68162489 -1.2832158 -1.9117751 -2.5803204 -3.2349916 -4.3023524 -5.5362244 -6.3089371 -6.6070147 -6.4414191][-5.5648489 -4.3437948 -3.136477 -2.2158377 -1.7039924 -1.86012 -2.2405877 -2.3242645 -2.2200754 -2.197181 -2.9172351 -4.2333851 -5.347774 -6.0574903 -6.2636209][-5.9416595 -5.0112991 -4.0867953 -3.5287762 -3.2826662 -3.3665028 -3.2941041 -2.6587462 -1.7513151 -1.120492 -1.5289962 -2.9089746 -4.3761535 -5.4836912 -6.0353775][-5.9415712 -5.3494372 -4.8271656 -4.721755 -4.7991295 -4.8193154 -4.2935281 -3.0672822 -1.6511474 -0.76864004 -1.031702 -2.3459427 -3.9301462 -5.1872292 -5.8948007][-5.9224372 -5.5925007 -5.3539305 -5.5174522 -5.809958 -5.8409142 -5.1216464 -3.6832116 -2.1800554 -1.4057689 -1.6746359 -2.7655432 -4.1103106 -5.1445045 -5.7496104][-5.9105253 -5.7630439 -5.5860739 -5.714448 -6.0160046 -6.099844 -5.4871569 -4.2269888 -2.9611769 -2.4791584 -2.8218412 -3.6301312 -4.5377855 -5.1325684 -5.4621239][-5.6107516 -5.5846906 -5.3534255 -5.2758861 -5.4581709 -5.6292911 -5.3495545 -4.5521607 -3.7065523 -3.4874871 -3.8182564 -4.3212123 -4.7665615 -4.9381175 -4.992321][-5.0181675 -5.059866 -4.8083868 -4.5722475 -4.64805 -4.9136844 -5.0091343 -4.7435775 -4.3320022 -4.2351151 -4.4117093 -4.6056237 -4.6905041 -4.5966845 -4.4848132]]...]
INFO - root - 2017-12-06 09:38:39.737865: step 18710, loss = 1.06, batch loss = 0.99 (13.9 examples/sec; 0.575 sec/batch; 50h:04m:51s remains)
INFO - root - 2017-12-06 09:38:45.717812: step 18720, loss = 1.00, batch loss = 0.93 (12.5 examples/sec; 0.642 sec/batch; 55h:57m:44s remains)
INFO - root - 2017-12-06 09:38:51.841147: step 18730, loss = 0.62, batch loss = 0.55 (12.7 examples/sec; 0.632 sec/batch; 55h:03m:09s remains)
INFO - root - 2017-12-06 09:38:58.011061: step 18740, loss = 0.88, batch loss = 0.81 (12.5 examples/sec; 0.639 sec/batch; 55h:42m:29s remains)
INFO - root - 2017-12-06 09:39:04.057347: step 18750, loss = 1.07, batch loss = 1.00 (13.1 examples/sec; 0.612 sec/batch; 53h:18m:22s remains)
INFO - root - 2017-12-06 09:39:10.158008: step 18760, loss = 0.98, batch loss = 0.91 (13.4 examples/sec; 0.599 sec/batch; 52h:12m:01s remains)
INFO - root - 2017-12-06 09:39:16.096613: step 18770, loss = 0.88, batch loss = 0.81 (13.5 examples/sec; 0.594 sec/batch; 51h:48m:26s remains)
INFO - root - 2017-12-06 09:39:22.183417: step 18780, loss = 0.82, batch loss = 0.75 (12.9 examples/sec; 0.618 sec/batch; 53h:53m:46s remains)
INFO - root - 2017-12-06 09:39:28.243443: step 18790, loss = 0.85, batch loss = 0.78 (13.4 examples/sec; 0.597 sec/batch; 52h:01m:45s remains)
INFO - root - 2017-12-06 09:39:34.361555: step 18800, loss = 0.91, batch loss = 0.84 (13.2 examples/sec; 0.607 sec/batch; 52h:51m:50s remains)
2017-12-06 09:39:34.909982: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.850059 -4.4449325 -4.7280006 -4.6704211 -4.5184617 -4.4040737 -4.293715 -4.05445 -3.6414313 -3.191196 -2.9210744 -2.8627512 -2.973104 -3.3062427 -3.9059563][-3.701252 -4.2125111 -4.4997945 -4.5318642 -4.5102487 -4.4931207 -4.3714929 -3.9971201 -3.4339693 -2.9407778 -2.6896727 -2.6044731 -2.5972104 -2.73973 -3.1569042][-3.5168793 -3.9045744 -4.2158461 -4.3822908 -4.4800258 -4.4760184 -4.2343235 -3.6612682 -2.9798274 -2.5434258 -2.423975 -2.4293914 -2.4157741 -2.4288809 -2.6447837][-3.2305162 -3.5309539 -3.9173186 -4.2081928 -4.3387008 -4.2105665 -3.7489944 -3.0040731 -2.3545671 -2.1517267 -2.3102725 -2.5335054 -2.6225576 -2.6153073 -2.6939571][-3.0374818 -3.2884893 -3.728838 -4.0376735 -4.0590272 -3.6722186 -2.9187889 -2.046011 -1.5176404 -1.6092229 -2.0879943 -2.5823245 -2.8720436 -2.9757442 -3.0633264][-3.047401 -3.2781386 -3.6763091 -3.8339634 -3.6012955 -2.8857098 -1.8483095 -0.9153707 -0.5937705 -1.0238605 -1.8278747 -2.5677748 -3.0468757 -3.2633874 -3.4074264][-3.2576551 -3.4556074 -3.6995096 -3.6089702 -3.079457 -2.0448272 -0.77026677 0.17550468 0.22163343 -0.62744308 -1.7779109 -2.7244701 -3.313798 -3.5441957 -3.6638546][-3.7619746 -3.8228381 -3.8047063 -3.4819102 -2.7809286 -1.579181 -0.16880274 0.80280304 0.66700554 -0.49527645 -1.8915501 -2.9906659 -3.6818423 -3.9374681 -4.0329666][-4.3541446 -4.2131162 -3.9069281 -3.4543889 -2.8141665 -1.745502 -0.47051096 0.37913084 0.20306158 -0.93939614 -2.2469461 -3.2737498 -3.9739137 -4.2474027 -4.3460283][-4.7411475 -4.5015888 -4.0185 -3.5407073 -3.0992897 -2.4085839 -1.5694959 -1.0551579 -1.233954 -2.0705059 -2.9276023 -3.5883052 -4.098403 -4.3146791 -4.4270926][-4.9048114 -4.6756382 -4.1343346 -3.6831851 -3.4424579 -3.1545756 -2.8088231 -2.6705041 -2.8390927 -3.2589703 -3.5400929 -3.7607899 -4.0720353 -4.2854886 -4.4584785][-4.9552884 -4.8334332 -4.354218 -3.9566314 -3.8091104 -3.7346218 -3.6590686 -3.6908603 -3.7426021 -3.7765265 -3.6442 -3.6320226 -3.921335 -4.2746406 -4.5784569][-5.05412 -5.1795793 -4.899734 -4.6005592 -4.4396367 -4.3319855 -4.219604 -4.167213 -4.017477 -3.7758372 -3.4504361 -3.3936081 -3.7650046 -4.293447 -4.7264442][-5.2395344 -5.6211796 -5.5458069 -5.3314819 -5.0995083 -4.8452625 -4.5718813 -4.3460875 -4.0459108 -3.702599 -3.3643742 -3.3522735 -3.78756 -4.4050474 -4.8619652][-5.496244 -5.9805865 -5.9773917 -5.7722626 -5.4700794 -5.105938 -4.7328725 -4.4154005 -4.093524 -3.8131306 -3.5791125 -3.6363857 -4.0684214 -4.630363 -4.9891586]]...]
INFO - root - 2017-12-06 09:39:41.050760: step 18810, loss = 0.78, batch loss = 0.71 (13.4 examples/sec; 0.598 sec/batch; 52h:07m:53s remains)
INFO - root - 2017-12-06 09:39:46.945320: step 18820, loss = 0.96, batch loss = 0.89 (13.4 examples/sec; 0.597 sec/batch; 52h:02m:04s remains)
INFO - root - 2017-12-06 09:39:53.086840: step 18830, loss = 1.01, batch loss = 0.94 (13.2 examples/sec; 0.605 sec/batch; 52h:41m:43s remains)
INFO - root - 2017-12-06 09:39:59.164374: step 18840, loss = 0.94, batch loss = 0.87 (13.9 examples/sec; 0.577 sec/batch; 50h:14m:04s remains)
INFO - root - 2017-12-06 09:40:05.243292: step 18850, loss = 1.06, batch loss = 0.99 (13.1 examples/sec; 0.612 sec/batch; 53h:20m:40s remains)
INFO - root - 2017-12-06 09:40:11.250349: step 18860, loss = 0.79, batch loss = 0.72 (13.5 examples/sec; 0.591 sec/batch; 51h:30m:23s remains)
INFO - root - 2017-12-06 09:40:17.279776: step 18870, loss = 0.99, batch loss = 0.92 (13.1 examples/sec; 0.612 sec/batch; 53h:19m:12s remains)
INFO - root - 2017-12-06 09:40:23.343819: step 18880, loss = 0.84, batch loss = 0.77 (12.7 examples/sec; 0.632 sec/batch; 55h:02m:11s remains)
INFO - root - 2017-12-06 09:40:29.424286: step 18890, loss = 0.97, batch loss = 0.89 (13.0 examples/sec; 0.613 sec/batch; 53h:26m:10s remains)
INFO - root - 2017-12-06 09:40:35.547133: step 18900, loss = 0.85, batch loss = 0.78 (13.3 examples/sec; 0.602 sec/batch; 52h:26m:28s remains)
2017-12-06 09:40:36.167794: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8149652 -3.9186828 -3.9978445 -4.0524263 -4.0848346 -4.106431 -4.1378107 -4.1719527 -4.1771021 -4.1584387 -4.1395907 -4.09374 -4.0285277 -3.9828153 -3.920352][-4.2500615 -4.4383345 -4.5607023 -4.6338992 -4.6600327 -4.6638603 -4.6939135 -4.7614145 -4.7917995 -4.7570682 -4.7224684 -4.6472373 -4.5634894 -4.5234575 -4.4560204][-4.8137846 -5.0873904 -5.260766 -5.3672476 -5.37837 -5.3120861 -5.2573743 -5.3007832 -5.357976 -5.3091421 -5.2643294 -5.1657858 -5.1089454 -5.1555629 -5.14106][-5.2929392 -5.6350684 -5.86505 -5.9953938 -5.9407105 -5.7011857 -5.4143467 -5.3430481 -5.47208 -5.48145 -5.481782 -5.3876324 -5.39063 -5.593348 -5.6868763][-5.4970751 -5.8602057 -6.12101 -6.2183218 -5.9815283 -5.4141874 -4.71006 -4.4339557 -4.7581315 -4.9983387 -5.1480751 -5.11224 -5.2047529 -5.5883217 -5.8213205][-5.4628525 -5.7992959 -6.0397763 -6.0166025 -5.447361 -4.3747244 -3.0716236 -2.5491581 -3.2607603 -3.964694 -4.3733516 -4.458087 -4.6460767 -5.1504946 -5.5013609][-5.3371735 -5.5865097 -5.7393923 -5.5553174 -4.5771413 -2.9181392 -0.96642351 -0.26263475 -1.5426776 -2.883867 -3.621763 -3.8737175 -4.1167593 -4.5899153 -4.9721894][-5.2284117 -5.3267889 -5.3346457 -5.0180545 -3.6786928 -1.4833269 0.99130869 1.7510719 -0.1050005 -2.0448918 -3.0695424 -3.4830682 -3.7348144 -4.0501118 -4.3837681][-5.233077 -5.1804752 -5.0567594 -4.7271051 -3.2508011 -0.80662918 1.8257656 2.4784374 0.27965689 -1.9311981 -3.0163708 -3.4719636 -3.6506369 -3.7353432 -3.9504616][-5.3876786 -5.2951455 -5.171958 -5.0196657 -3.7590725 -1.5482264 0.73659945 1.1886845 -0.90368152 -2.8988609 -3.7253554 -4.004806 -3.9900632 -3.8688314 -3.9579158][-5.5195684 -5.524591 -5.5300727 -5.6420546 -4.8093557 -3.128185 -1.4329486 -1.1918797 -2.9319954 -4.4521351 -4.8534455 -4.824152 -4.5750151 -4.33011 -4.3292689][-5.4540539 -5.6379805 -5.8229365 -6.1612277 -5.7702422 -4.6685658 -3.514823 -3.3814538 -4.6685147 -5.6715384 -5.6793246 -5.3447261 -4.9157381 -4.6559258 -4.6438513][-5.1029925 -5.4259386 -5.7235026 -6.1413908 -6.0530133 -5.4127131 -4.6156197 -4.4662166 -5.3067222 -5.8867745 -5.6481252 -5.1317267 -4.6672893 -4.4867187 -4.5413613][-4.5179553 -4.8520174 -5.1241183 -5.4401765 -5.4403954 -5.061944 -4.4750781 -4.2756 -4.7607126 -5.0561352 -4.7268181 -4.1820507 -3.8048964 -3.7499909 -3.9112253][-3.8938816 -4.1350937 -4.2971258 -4.4253745 -4.3661165 -4.0860977 -3.635891 -3.4136174 -3.6591821 -3.7941031 -3.4917352 -3.0507035 -2.8188581 -2.8672755 -3.0899358]]...]
INFO - root - 2017-12-06 09:40:42.271232: step 18910, loss = 0.70, batch loss = 0.63 (13.2 examples/sec; 0.607 sec/batch; 52h:51m:53s remains)
INFO - root - 2017-12-06 09:40:48.250073: step 18920, loss = 0.69, batch loss = 0.62 (13.0 examples/sec; 0.614 sec/batch; 53h:30m:30s remains)
INFO - root - 2017-12-06 09:40:54.266537: step 18930, loss = 1.08, batch loss = 1.01 (12.9 examples/sec; 0.618 sec/batch; 53h:50m:43s remains)
INFO - root - 2017-12-06 09:41:00.421705: step 18940, loss = 1.07, batch loss = 1.00 (12.7 examples/sec; 0.631 sec/batch; 54h:56m:09s remains)
INFO - root - 2017-12-06 09:41:06.521026: step 18950, loss = 0.94, batch loss = 0.87 (12.9 examples/sec; 0.618 sec/batch; 53h:50m:08s remains)
INFO - root - 2017-12-06 09:41:12.563002: step 18960, loss = 0.95, batch loss = 0.88 (13.0 examples/sec; 0.616 sec/batch; 53h:37m:30s remains)
INFO - root - 2017-12-06 09:41:18.620647: step 18970, loss = 0.75, batch loss = 0.68 (13.7 examples/sec; 0.583 sec/batch; 50h:44m:10s remains)
INFO - root - 2017-12-06 09:41:24.740552: step 18980, loss = 0.96, batch loss = 0.89 (13.1 examples/sec; 0.612 sec/batch; 53h:17m:22s remains)
INFO - root - 2017-12-06 09:41:30.863006: step 18990, loss = 1.04, batch loss = 0.97 (12.5 examples/sec; 0.639 sec/batch; 55h:38m:43s remains)
INFO - root - 2017-12-06 09:41:36.920059: step 19000, loss = 0.87, batch loss = 0.80 (13.2 examples/sec; 0.604 sec/batch; 52h:37m:49s remains)
2017-12-06 09:41:37.615946: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3036151 -5.7025933 -5.6522493 -5.3408332 -4.8480449 -3.9538193 -2.9271719 -2.4878354 -3.0029869 -3.8823562 -4.4622884 -4.5821738 -4.5244589 -4.3205748 -4.0256014][-4.9496608 -5.28416 -5.1877561 -4.8608775 -4.3824396 -3.5009801 -2.48831 -2.0794013 -2.684948 -3.7238584 -4.4385586 -4.6712155 -4.7151914 -4.5558605 -4.2137036][-4.9497967 -5.3139467 -5.2043324 -4.8544726 -4.3488712 -3.4279177 -2.37635 -1.9683123 -2.6046576 -3.70127 -4.4715886 -4.8173461 -4.9921317 -4.8878946 -4.4876041][-5.3564467 -5.7895031 -5.6748352 -5.2598977 -4.6506171 -3.6028762 -2.490469 -2.0888278 -2.7082362 -3.7380106 -4.4727435 -4.920517 -5.2423053 -5.1816726 -4.7129064][-5.8693018 -6.3217258 -6.1639175 -5.639689 -4.8724561 -3.6456985 -2.4359448 -2.0042484 -2.5841908 -3.5443354 -4.2802944 -4.8748264 -5.3422203 -5.3003979 -4.7575526][-6.1939144 -6.561677 -6.265255 -5.5713682 -4.6148024 -3.1936069 -1.8398626 -1.3861268 -2.0608332 -3.1299715 -3.982291 -4.734551 -5.304327 -5.253325 -4.6362529][-6.2146254 -6.394927 -5.8502035 -4.9166846 -3.753726 -2.1301467 -0.639972 -0.27509165 -1.2706912 -2.6607847 -3.7123022 -4.5596657 -5.1498294 -5.072875 -4.4247746][-5.954237 -5.8723841 -5.0615573 -3.948355 -2.6402607 -0.86709642 0.67082977 0.80667353 -0.61208797 -2.3511498 -3.5502515 -4.356461 -4.8521042 -4.7497444 -4.1663723][-5.5228839 -5.2404604 -4.339467 -3.2394838 -1.9539165 -0.23119497 1.1514177 1.0185318 -0.60852075 -2.4438674 -3.6065371 -4.2300997 -4.5444236 -4.4242406 -3.9696522][-5.1296272 -4.7673044 -3.97498 -3.0843139 -2.0229938 -0.60775161 0.42648172 0.14444828 -1.3410628 -2.9598556 -3.919466 -4.3311558 -4.4787884 -4.3350244 -3.9715328][-4.9312329 -4.6392093 -4.1037245 -3.5443947 -2.8132944 -1.7639954 -1.0130231 -1.2629406 -2.4221091 -3.68519 -4.4065914 -4.6526709 -4.6889577 -4.5102916 -4.155704][-4.9536734 -4.8408737 -4.6497431 -4.4493041 -4.006597 -3.1866627 -2.5205727 -2.5719662 -3.364769 -4.3118577 -4.865653 -5.0281944 -5.0100889 -4.779233 -4.3755031][-5.1231341 -5.228941 -5.3491526 -5.3892989 -5.0638747 -4.2921791 -3.5629926 -3.3854434 -3.9120545 -4.6909862 -5.1902075 -5.3342772 -5.2753305 -4.9782043 -4.5254011][-5.292635 -5.5422182 -5.8293295 -5.9427066 -5.6098065 -4.8291759 -4.0697031 -3.7836039 -4.1696897 -4.8629489 -5.3365731 -5.4619484 -5.3556008 -5.011076 -4.5624428][-5.3554115 -5.6109114 -5.9078388 -5.9988966 -5.653707 -4.9335394 -4.2630482 -3.9914479 -4.2913475 -4.8819823 -5.300921 -5.4003091 -5.27323 -4.9406319 -4.5611181]]...]
INFO - root - 2017-12-06 09:41:43.756941: step 19010, loss = 0.99, batch loss = 0.92 (13.3 examples/sec; 0.603 sec/batch; 52h:31m:50s remains)
INFO - root - 2017-12-06 09:41:49.788713: step 19020, loss = 0.91, batch loss = 0.84 (13.1 examples/sec; 0.612 sec/batch; 53h:17m:55s remains)
INFO - root - 2017-12-06 09:41:55.910019: step 19030, loss = 0.97, batch loss = 0.90 (13.1 examples/sec; 0.612 sec/batch; 53h:15m:37s remains)
INFO - root - 2017-12-06 09:42:01.803131: step 19040, loss = 0.90, batch loss = 0.83 (13.1 examples/sec; 0.609 sec/batch; 53h:03m:10s remains)
INFO - root - 2017-12-06 09:42:07.968274: step 19050, loss = 1.10, batch loss = 1.03 (12.6 examples/sec; 0.633 sec/batch; 55h:04m:56s remains)
INFO - root - 2017-12-06 09:42:14.064596: step 19060, loss = 1.03, batch loss = 0.96 (12.7 examples/sec; 0.630 sec/batch; 54h:52m:37s remains)
INFO - root - 2017-12-06 09:42:20.041538: step 19070, loss = 0.93, batch loss = 0.86 (13.2 examples/sec; 0.605 sec/batch; 52h:39m:56s remains)
INFO - root - 2017-12-06 09:42:26.070201: step 19080, loss = 0.84, batch loss = 0.77 (13.5 examples/sec; 0.594 sec/batch; 51h:43m:33s remains)
INFO - root - 2017-12-06 09:42:32.131354: step 19090, loss = 0.93, batch loss = 0.86 (13.1 examples/sec; 0.611 sec/batch; 53h:12m:13s remains)
INFO - root - 2017-12-06 09:42:38.200850: step 19100, loss = 0.82, batch loss = 0.75 (13.2 examples/sec; 0.606 sec/batch; 52h:46m:15s remains)
2017-12-06 09:42:38.790825: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5995655 -4.8391619 -5.0296545 -5.1533284 -5.183804 -5.1555886 -5.1492829 -5.1427193 -5.1387119 -5.1697922 -5.1914506 -5.1794329 -5.1819725 -5.1227732 -4.9564452][-4.9436917 -5.20331 -5.350234 -5.4041886 -5.3324952 -5.20812 -5.1467962 -5.1195989 -5.1327739 -5.2005615 -5.2691984 -5.3212304 -5.4045091 -5.3882184 -5.2101378][-5.2169328 -5.4120569 -5.4259529 -5.3400826 -5.1245813 -4.9065022 -4.8389721 -4.8655787 -4.9705973 -5.0989842 -5.2135787 -5.3439846 -5.5070739 -5.5197024 -5.3647432][-5.2377419 -5.313611 -5.1466351 -4.8780112 -4.4910293 -4.1893063 -4.1843066 -4.3797569 -4.67564 -4.8952017 -5.0289092 -5.2253423 -5.4373364 -5.4485044 -5.3531771][-5.0012722 -4.9328337 -4.5802612 -4.1144047 -3.527688 -3.1121266 -3.1887648 -3.645483 -4.2353563 -4.584887 -4.7128296 -4.9523573 -5.1857657 -5.1810179 -5.1733422][-4.6119032 -4.3491821 -3.8174343 -3.1781483 -2.411325 -1.8479674 -1.9636419 -2.7092113 -3.617465 -4.1110954 -4.2302732 -4.5045152 -4.7722974 -4.7802076 -4.884294][-4.2129288 -3.7555943 -3.0885286 -2.3224056 -1.4582894 -0.80566144 -0.95044827 -1.945951 -3.0635786 -3.6597831 -3.7913933 -4.0892649 -4.3971653 -4.4319859 -4.6332021][-4.000237 -3.4829109 -2.7942681 -1.9716408 -1.1128857 -0.44420314 -0.5863502 -1.7040002 -2.8671288 -3.5292306 -3.7040443 -3.9853396 -4.2761478 -4.2994576 -4.5309381][-4.0188479 -3.60083 -3.0346451 -2.2355306 -1.4251609 -0.76554441 -0.87982059 -2.0232878 -3.1349204 -3.847399 -4.0763221 -4.2714005 -4.4498839 -4.3914628 -4.5673728][-4.183784 -3.9612238 -3.6284614 -2.9569545 -2.2505608 -1.6381609 -1.7347941 -2.8281322 -3.8173695 -4.5036254 -4.7186632 -4.7529426 -4.7401748 -4.553813 -4.6177754][-4.37617 -4.3431587 -4.2636166 -3.8129585 -3.2971177 -2.8146269 -2.9126515 -3.850374 -4.630228 -5.1664152 -5.2610455 -5.0956392 -4.8855047 -4.5971942 -4.5632482][-4.5426645 -4.6070952 -4.6762214 -4.4428935 -4.1465077 -3.8454792 -3.94388 -4.6187935 -5.1293731 -5.4488668 -5.3961611 -5.0972624 -4.7827582 -4.4874072 -4.421946][-4.6004057 -4.6734118 -4.7814484 -4.7165384 -4.6057334 -4.4674525 -4.5485091 -4.9350214 -5.1790609 -5.2931857 -5.1573262 -4.8551855 -4.5707855 -4.3543882 -4.3037486][-4.552835 -4.6071892 -4.6997247 -4.7260957 -4.7275882 -4.6955504 -4.7486262 -4.9053535 -4.9621611 -4.9563384 -4.8265157 -4.6145091 -4.4318881 -4.312809 -4.2800612][-4.5248818 -4.5486088 -4.5949306 -4.6293921 -4.660378 -4.6702385 -4.6940947 -4.7296085 -4.7151003 -4.6830759 -4.6052318 -4.4934416 -4.4004049 -4.3453336 -4.3229551]]...]
INFO - root - 2017-12-06 09:42:44.893082: step 19110, loss = 0.83, batch loss = 0.76 (13.4 examples/sec; 0.599 sec/batch; 52h:07m:15s remains)
INFO - root - 2017-12-06 09:42:51.066862: step 19120, loss = 1.17, batch loss = 1.10 (13.2 examples/sec; 0.605 sec/batch; 52h:39m:56s remains)
INFO - root - 2017-12-06 09:42:57.127598: step 19130, loss = 0.68, batch loss = 0.61 (13.4 examples/sec; 0.597 sec/batch; 51h:58m:51s remains)
INFO - root - 2017-12-06 09:43:03.215551: step 19140, loss = 0.70, batch loss = 0.63 (13.2 examples/sec; 0.607 sec/batch; 52h:51m:54s remains)
INFO - root - 2017-12-06 09:43:09.170255: step 19150, loss = 1.05, batch loss = 0.98 (13.1 examples/sec; 0.609 sec/batch; 53h:00m:03s remains)
INFO - root - 2017-12-06 09:43:15.292718: step 19160, loss = 0.85, batch loss = 0.78 (13.5 examples/sec; 0.591 sec/batch; 51h:24m:17s remains)
INFO - root - 2017-12-06 09:43:21.329823: step 19170, loss = 0.59, batch loss = 0.52 (13.4 examples/sec; 0.597 sec/batch; 51h:58m:44s remains)
INFO - root - 2017-12-06 09:43:27.420744: step 19180, loss = 1.11, batch loss = 1.04 (13.3 examples/sec; 0.601 sec/batch; 52h:17m:19s remains)
INFO - root - 2017-12-06 09:43:33.590939: step 19190, loss = 1.03, batch loss = 0.96 (12.8 examples/sec; 0.624 sec/batch; 54h:16m:05s remains)
INFO - root - 2017-12-06 09:43:39.674751: step 19200, loss = 0.77, batch loss = 0.70 (13.1 examples/sec; 0.610 sec/batch; 53h:03m:25s remains)
2017-12-06 09:43:40.226907: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3820958 -4.4449911 -4.5396929 -4.5862007 -4.4983096 -4.3220935 -4.2533512 -4.3019843 -4.3246307 -4.3452773 -4.495688 -4.6467304 -4.6837387 -4.5857143 -4.4341397][-4.5624623 -4.658648 -4.8233533 -4.8936205 -4.6613312 -4.2277212 -4.0675316 -4.2808886 -4.4610739 -4.4863582 -4.6268024 -4.8016844 -4.8748922 -4.8040762 -4.65437][-4.870862 -5.0050821 -5.2398005 -5.3599858 -4.9859943 -4.2165008 -3.894244 -4.3496585 -4.8487959 -4.9415808 -5.0326753 -5.1793704 -5.2331414 -5.1403012 -4.9738297][-5.1224833 -5.2494607 -5.5031357 -5.6884432 -5.260181 -4.1893811 -3.6415994 -4.3399248 -5.2947836 -5.563067 -5.6073055 -5.689744 -5.6615672 -5.463191 -5.219841][-5.1563191 -5.1790328 -5.3291416 -5.51552 -5.1154122 -3.8695188 -3.0738192 -3.921561 -5.3475075 -5.8608932 -5.8841572 -5.8847361 -5.7554216 -5.4656949 -5.1696849][-4.954875 -4.78546 -4.7211947 -4.7849026 -4.4231162 -3.1339879 -2.1362967 -3.0367341 -4.8377185 -5.5917492 -5.6212921 -5.53992 -5.314754 -5.0273447 -4.79759][-4.7214279 -4.3570189 -4.0143886 -3.8039007 -3.3695374 -2.0888264 -0.95585513 -1.7980843 -3.7902298 -4.7578177 -4.8847747 -4.803153 -4.5684566 -4.4057889 -4.3528261][-4.6796012 -4.2144847 -3.643146 -3.1072388 -2.4632645 -1.1435168 0.0354352 -0.71898603 -2.6902854 -3.7651834 -4.0718017 -4.1326437 -4.0082121 -4.0289145 -4.1618652][-4.8518081 -4.4426036 -3.815151 -3.0811152 -2.2240679 -0.85109067 0.28799391 -0.3937726 -2.2230031 -3.3155251 -3.7958429 -4.0451455 -4.0614824 -4.1825953 -4.3713503][-5.1486111 -4.8718939 -4.3451619 -3.6399877 -2.7334032 -1.3926218 -0.37392998 -0.98040152 -2.5873263 -3.6024816 -4.134747 -4.4643774 -4.530694 -4.624795 -4.7459774][-5.4553647 -5.2917504 -4.9271121 -4.441853 -3.7092445 -2.5507777 -1.6626141 -2.0852723 -3.3277259 -4.1365948 -4.5850606 -4.8745265 -4.9399056 -4.9866047 -5.00522][-5.6815176 -5.6142673 -5.4147468 -5.2021632 -4.7588863 -3.903156 -3.1756268 -3.3678682 -4.1731515 -4.7018275 -4.972928 -5.1306968 -5.1558309 -5.1501374 -5.053483][-5.6178651 -5.5992222 -5.4991932 -5.4737325 -5.2904634 -4.7584853 -4.2297745 -4.2674327 -4.7155375 -5.0030966 -5.1193104 -5.1572723 -5.145371 -5.100399 -4.9245234][-5.2669387 -5.2533484 -5.19215 -5.2250557 -5.1761222 -4.8928132 -4.5673714 -4.545856 -4.7516766 -4.87941 -4.9213238 -4.9217014 -4.9104 -4.8585587 -4.6837606][-4.8568144 -4.8430758 -4.7936807 -4.8110723 -4.8065419 -4.6806531 -4.5131607 -4.4717364 -4.5240622 -4.5538969 -4.5699673 -4.5764618 -4.5725446 -4.5341878 -4.427886]]...]
INFO - root - 2017-12-06 09:43:46.290994: step 19210, loss = 1.05, batch loss = 0.98 (13.0 examples/sec; 0.617 sec/batch; 53h:39m:10s remains)
INFO - root - 2017-12-06 09:43:52.250660: step 19220, loss = 0.83, batch loss = 0.76 (13.2 examples/sec; 0.606 sec/batch; 52h:44m:11s remains)
INFO - root - 2017-12-06 09:43:58.231411: step 19230, loss = 0.76, batch loss = 0.69 (13.0 examples/sec; 0.614 sec/batch; 53h:26m:42s remains)
INFO - root - 2017-12-06 09:44:04.358626: step 19240, loss = 1.20, batch loss = 1.13 (13.1 examples/sec; 0.610 sec/batch; 53h:06m:59s remains)
INFO - root - 2017-12-06 09:44:10.243754: step 19250, loss = 0.91, batch loss = 0.84 (13.2 examples/sec; 0.606 sec/batch; 52h:43m:47s remains)
INFO - root - 2017-12-06 09:44:16.359692: step 19260, loss = 0.82, batch loss = 0.75 (13.1 examples/sec; 0.613 sec/batch; 53h:17m:58s remains)
INFO - root - 2017-12-06 09:44:22.473905: step 19270, loss = 0.84, batch loss = 0.77 (12.9 examples/sec; 0.619 sec/batch; 53h:50m:44s remains)
INFO - root - 2017-12-06 09:44:28.607723: step 19280, loss = 0.80, batch loss = 0.73 (13.4 examples/sec; 0.597 sec/batch; 51h:54m:56s remains)
INFO - root - 2017-12-06 09:44:34.655695: step 19290, loss = 1.15, batch loss = 1.08 (13.0 examples/sec; 0.613 sec/batch; 53h:21m:56s remains)
INFO - root - 2017-12-06 09:44:40.724479: step 19300, loss = 0.87, batch loss = 0.80 (13.5 examples/sec; 0.593 sec/batch; 51h:34m:26s remains)
2017-12-06 09:44:41.390231: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7549827 -4.3307648 -5.1095481 -5.6390157 -5.7650518 -5.4034634 -4.8158693 -4.6616545 -4.9954224 -5.4235253 -5.4864817 -5.1360917 -4.66422 -4.036272 -3.479012][-3.4344139 -4.1414809 -5.0517 -5.5846348 -5.6158309 -5.137867 -4.4701781 -4.29828 -4.7153811 -5.2670674 -5.3779488 -4.9717488 -4.4312763 -3.7316358 -3.095511][-3.0215049 -3.7672186 -4.7374091 -5.1948433 -5.0452933 -4.4089894 -3.694479 -3.6002605 -4.1978521 -4.9330096 -5.1449423 -4.7510595 -4.2182426 -3.5590997 -2.9302642][-2.6669483 -3.3375778 -4.2812128 -4.6141829 -4.2535877 -3.4300225 -2.6525164 -2.7580667 -3.6546063 -4.612659 -4.9511132 -4.6189785 -4.1450739 -3.6005063 -3.0649891][-2.516535 -3.0597866 -3.9224596 -4.0877538 -3.4799666 -2.408319 -1.4727652 -1.750488 -2.9683957 -4.1577148 -4.6940637 -4.5491648 -4.2105231 -3.7942395 -3.3880048][-2.5769634 -3.019408 -3.7640715 -3.7195621 -2.800168 -1.384197 -0.19735765 -0.5686388 -2.0559778 -3.4415817 -4.1878343 -4.2926459 -4.1396847 -3.8901265 -3.6682262][-2.674679 -3.0869946 -3.7472 -3.5468647 -2.3579192 -0.59478521 0.86516094 0.49204779 -1.1276541 -2.6423116 -3.5697904 -3.8782175 -3.8902645 -3.7995143 -3.8010716][-2.8029823 -3.253505 -3.9255698 -3.7863948 -2.6149828 -0.75649095 0.85743237 0.64190531 -0.8229661 -2.2856824 -3.2676575 -3.6541181 -3.7381928 -3.7363906 -3.9254551][-2.9822741 -3.4893134 -4.260365 -4.3821588 -3.5476573 -1.9362085 -0.33400869 -0.25069237 -1.3186014 -2.5352349 -3.3709908 -3.644671 -3.6751161 -3.6893713 -4.0106273][-3.2946126 -3.7196951 -4.4770222 -4.8417864 -4.4683056 -3.2755804 -1.7739832 -1.4410799 -2.1466727 -3.0741985 -3.6693649 -3.7317996 -3.6389863 -3.6276526 -4.032032][-3.9292588 -4.1045241 -4.626802 -5.0267053 -4.970829 -4.1345425 -2.7953157 -2.3932514 -2.9452009 -3.7323728 -4.1768179 -4.08832 -3.8972635 -3.8427289 -4.2447295][-4.7136841 -4.5761194 -4.7380834 -4.9909091 -5.0377679 -4.4173737 -3.2810087 -2.9251409 -3.4313126 -4.1755056 -4.5893989 -4.4945846 -4.313067 -4.2590261 -4.6061931][-5.3602319 -5.0154061 -4.87714 -4.9414911 -4.9304862 -4.4052873 -3.4822149 -3.2067285 -3.6840639 -4.391861 -4.8326831 -4.8344421 -4.7408233 -4.724268 -4.9967117][-5.6014628 -5.2126927 -4.9406366 -4.8718462 -4.7571459 -4.2953053 -3.6137321 -3.4435678 -3.8661664 -4.4873958 -4.9153795 -5.0068216 -4.9915786 -5.0023413 -5.1901684][-5.3778968 -5.0220933 -4.7260127 -4.5859194 -4.4101267 -4.0285978 -3.5607316 -3.4658937 -3.7884946 -4.268847 -4.6334929 -4.7783394 -4.824131 -4.8577118 -4.9841523]]...]
INFO - root - 2017-12-06 09:44:47.572887: step 19310, loss = 0.84, batch loss = 0.77 (12.9 examples/sec; 0.621 sec/batch; 54h:03m:50s remains)
INFO - root - 2017-12-06 09:44:53.669563: step 19320, loss = 0.88, batch loss = 0.81 (13.2 examples/sec; 0.608 sec/batch; 52h:55m:05s remains)
INFO - root - 2017-12-06 09:44:59.712007: step 19330, loss = 1.01, batch loss = 0.94 (13.3 examples/sec; 0.603 sec/batch; 52h:29m:49s remains)
INFO - root - 2017-12-06 09:45:05.738471: step 19340, loss = 1.13, batch loss = 1.06 (13.1 examples/sec; 0.611 sec/batch; 53h:08m:37s remains)
INFO - root - 2017-12-06 09:45:11.818283: step 19350, loss = 1.06, batch loss = 0.99 (12.8 examples/sec; 0.626 sec/batch; 54h:28m:40s remains)
INFO - root - 2017-12-06 09:45:17.768405: step 19360, loss = 0.82, batch loss = 0.75 (12.9 examples/sec; 0.621 sec/batch; 54h:00m:08s remains)
INFO - root - 2017-12-06 09:45:23.774546: step 19370, loss = 0.87, batch loss = 0.80 (14.3 examples/sec; 0.559 sec/batch; 48h:39m:08s remains)
INFO - root - 2017-12-06 09:45:29.901015: step 19380, loss = 1.14, batch loss = 1.07 (13.0 examples/sec; 0.615 sec/batch; 53h:31m:24s remains)
INFO - root - 2017-12-06 09:45:36.041940: step 19390, loss = 1.00, batch loss = 0.93 (12.7 examples/sec; 0.631 sec/batch; 54h:51m:01s remains)
INFO - root - 2017-12-06 09:45:42.003040: step 19400, loss = 0.69, batch loss = 0.62 (13.2 examples/sec; 0.607 sec/batch; 52h:45m:41s remains)
2017-12-06 09:45:42.648703: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.57898 -3.4965546 -4.2850552 -4.7321444 -4.5440288 -3.9022288 -3.2756381 -2.8514142 -2.7860994 -3.0485554 -3.3035502 -3.5944712 -4.02956 -4.4996266 -5.0180149][-2.3729651 -3.4324052 -4.2646489 -4.696476 -4.4982934 -3.8393114 -3.2845078 -3.0465627 -3.0762548 -3.32864 -3.621839 -3.9823303 -4.3918014 -4.7017589 -4.9316692][-2.4294252 -3.6020722 -4.4221783 -4.7425289 -4.42985 -3.6645727 -3.1325002 -3.0775335 -3.1933346 -3.4604979 -3.861877 -4.363585 -4.8295732 -5.0016441 -4.8611341][-2.8131559 -3.9586616 -4.64062 -4.7586846 -4.2824473 -3.3868856 -2.8240533 -2.8807685 -3.0367472 -3.3120399 -3.8183267 -4.4570618 -5.05873 -5.2532687 -4.9022346][-3.4673698 -4.3462229 -4.711884 -4.5372744 -3.8656411 -2.8546467 -2.2377782 -2.3711805 -2.6128128 -2.957036 -3.5291884 -4.2111588 -4.9361072 -5.2878432 -4.9898176][-4.0664487 -4.5852146 -4.6060882 -4.1280804 -3.2610888 -2.1445668 -1.4222932 -1.5446765 -1.9245851 -2.4522381 -3.1170654 -3.7723074 -4.5032625 -4.9955978 -4.9317746][-4.3595347 -4.6177578 -4.4574819 -3.8028338 -2.7970657 -1.5642278 -0.618696 -0.56075263 -1.0363796 -1.8211887 -2.6705997 -3.3498368 -4.0128307 -4.5391941 -4.7319741][-4.3744869 -4.5524449 -4.4382906 -3.8032584 -2.7636383 -1.4102957 -0.14169264 0.23757982 -0.26242256 -1.3049765 -2.4104121 -3.2283711 -3.8398678 -4.275331 -4.5748425][-4.272099 -4.4751391 -4.5442133 -4.0935969 -3.1769843 -1.8388865 -0.36738157 0.29412508 -0.16404104 -1.3509617 -2.6381593 -3.6075139 -4.186058 -4.4503717 -4.6532121][-4.20973 -4.3988733 -4.6365356 -4.4374709 -3.7675395 -2.6525085 -1.3043122 -0.63101053 -1.0686088 -2.2377141 -3.5100222 -4.483417 -4.9662442 -5.022902 -5.0011625][-4.2994652 -4.4170065 -4.7218323 -4.7444592 -4.3684382 -3.6034279 -2.5972216 -2.1058547 -2.5471025 -3.5729337 -4.6516781 -5.4561481 -5.772666 -5.6500525 -5.4245987][-4.4726892 -4.5293832 -4.8302016 -5.0058064 -4.8873768 -4.4552608 -3.8197925 -3.53173 -3.9160786 -4.6983314 -5.4800596 -6.0175867 -6.1443043 -5.9100866 -5.5845909][-4.5793557 -4.6404786 -4.9121046 -5.1636453 -5.224432 -5.0502338 -4.7180448 -4.57858 -4.8343744 -5.305336 -5.7437658 -5.9923081 -5.9535222 -5.6784034 -5.3513441][-4.489202 -4.5841 -4.8189411 -5.0784407 -5.2298603 -5.2198825 -5.0937753 -5.0353632 -5.1461439 -5.3395061 -5.4949174 -5.5253382 -5.3863754 -5.1242027 -4.8586383][-4.2207732 -4.3201184 -4.5026054 -4.7139049 -4.8786764 -4.951829 -4.9494648 -4.9379535 -4.9545455 -4.9759464 -4.9664612 -4.8887353 -4.73016 -4.5281591 -4.3511791]]...]
INFO - root - 2017-12-06 09:45:48.707617: step 19410, loss = 0.84, batch loss = 0.77 (13.8 examples/sec; 0.579 sec/batch; 50h:20m:49s remains)
INFO - root - 2017-12-06 09:45:54.865041: step 19420, loss = 1.04, batch loss = 0.97 (12.6 examples/sec; 0.637 sec/batch; 55h:23m:24s remains)
INFO - root - 2017-12-06 09:46:00.908290: step 19430, loss = 1.02, batch loss = 0.95 (13.3 examples/sec; 0.601 sec/batch; 52h:16m:47s remains)
INFO - root - 2017-12-06 09:46:06.934327: step 19440, loss = 0.78, batch loss = 0.71 (13.1 examples/sec; 0.612 sec/batch; 53h:12m:16s remains)
INFO - root - 2017-12-06 09:46:13.098285: step 19450, loss = 1.04, batch loss = 0.97 (13.0 examples/sec; 0.617 sec/batch; 53h:41m:25s remains)
INFO - root - 2017-12-06 09:46:19.269616: step 19460, loss = 0.80, batch loss = 0.73 (13.2 examples/sec; 0.605 sec/batch; 52h:35m:54s remains)
INFO - root - 2017-12-06 09:46:25.211228: step 19470, loss = 0.88, batch loss = 0.81 (13.4 examples/sec; 0.596 sec/batch; 51h:50m:27s remains)
INFO - root - 2017-12-06 09:46:31.269442: step 19480, loss = 0.70, batch loss = 0.63 (13.6 examples/sec; 0.586 sec/batch; 50h:57m:43s remains)
INFO - root - 2017-12-06 09:46:37.445817: step 19490, loss = 0.91, batch loss = 0.84 (12.8 examples/sec; 0.624 sec/batch; 54h:16m:12s remains)
INFO - root - 2017-12-06 09:46:43.530118: step 19500, loss = 1.11, batch loss = 1.04 (12.9 examples/sec; 0.619 sec/batch; 53h:51m:36s remains)
2017-12-06 09:46:44.139212: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1737843 -2.9868658 -2.8792415 -2.994801 -3.2324162 -3.3520262 -3.3023872 -3.3776488 -3.6472392 -3.8912945 -3.92502 -3.8015394 -3.6842737 -3.471596 -3.118926][-2.8598447 -2.8101363 -2.8351932 -3.0384307 -3.4120436 -3.690697 -3.7118349 -3.6528513 -3.6815295 -3.7519832 -3.7418978 -3.7341766 -3.7524543 -3.688364 -3.4849992][-3.0415759 -3.1726539 -3.350951 -3.5902176 -3.9431245 -4.1867304 -4.1489353 -3.8974626 -3.6768575 -3.5733664 -3.4859078 -3.4869919 -3.5181837 -3.5360379 -3.550106][-3.7396407 -3.9793093 -4.2068315 -4.3607659 -4.5022855 -4.5125837 -4.2990541 -3.8714809 -3.4699626 -3.275444 -3.1725309 -3.1578176 -3.1569519 -3.2058005 -3.3904235][-4.574851 -4.8576617 -5.038857 -5.0245929 -4.8603516 -4.5476007 -4.1211438 -3.5762162 -3.1014338 -2.8910041 -2.8068857 -2.7839375 -2.7593827 -2.812696 -3.0885372][-5.2241735 -5.4782171 -5.5169859 -5.2589364 -4.7420454 -4.0807796 -3.4482288 -2.8732224 -2.470717 -2.3517365 -2.3398485 -2.3678997 -2.3839164 -2.4581916 -2.7587888][-5.3590384 -5.3987117 -5.1481204 -4.5843349 -3.7675126 -2.902544 -2.2481771 -1.851881 -1.7179389 -1.8416197 -2.0265491 -2.219763 -2.3606639 -2.4608855 -2.6949644][-4.881669 -4.67078 -4.1828675 -3.447526 -2.5293541 -1.6542935 -1.1234567 -0.99007845 -1.1603508 -1.5574944 -1.9756081 -2.3467238 -2.6178417 -2.7299118 -2.8714266][-4.0575862 -3.7429287 -3.2276165 -2.5501728 -1.7335644 -0.95959806 -0.55712461 -0.60839319 -0.98625517 -1.5794163 -2.1523118 -2.6261816 -2.9572029 -3.0750499 -3.1705146][-3.3540392 -3.107857 -2.7666121 -2.3422229 -1.7885287 -1.1768677 -0.85166478 -0.95054245 -1.3730085 -2.023108 -2.6489129 -3.1213603 -3.4062057 -3.4996219 -3.5686896][-3.184494 -3.10793 -3.024173 -2.9125419 -2.6707306 -2.249357 -1.9436865 -1.9373093 -2.2206171 -2.7482607 -3.2948732 -3.6735568 -3.8395491 -3.8628891 -3.8795648][-3.4681306 -3.5228024 -3.593215 -3.6512399 -3.6145043 -3.3667085 -3.0874908 -2.9691806 -3.0807838 -3.4265954 -3.8441715 -4.1129026 -4.1612654 -4.0835185 -3.9878705][-3.8932092 -3.8697307 -3.8769431 -3.9224591 -3.9891641 -3.9343293 -3.7795968 -3.669029 -3.714695 -3.9253688 -4.2006974 -4.3699627 -4.3610806 -4.2431211 -4.0636206][-4.2075291 -3.989651 -3.8446281 -3.8214095 -3.9317315 -4.0207782 -3.9937532 -3.9459362 -4.0083418 -4.1702657 -4.3448625 -4.4467049 -4.4362583 -4.3444333 -4.1538653][-4.1637812 -3.8123741 -3.5861735 -3.531178 -3.6156268 -3.714808 -3.7336171 -3.7250504 -3.829143 -4.0119333 -4.17441 -4.2870178 -4.3221827 -4.2764874 -4.1128936]]...]
INFO - root - 2017-12-06 09:46:50.314413: step 19510, loss = 0.98, batch loss = 0.91 (12.9 examples/sec; 0.619 sec/batch; 53h:51m:05s remains)
INFO - root - 2017-12-06 09:46:56.311088: step 19520, loss = 0.77, batch loss = 0.70 (15.0 examples/sec; 0.535 sec/batch; 46h:31m:00s remains)
INFO - root - 2017-12-06 09:47:02.377843: step 19530, loss = 0.79, batch loss = 0.72 (13.1 examples/sec; 0.610 sec/batch; 53h:00m:17s remains)
INFO - root - 2017-12-06 09:47:08.434713: step 19540, loss = 0.93, batch loss = 0.86 (12.9 examples/sec; 0.618 sec/batch; 53h:43m:53s remains)
INFO - root - 2017-12-06 09:47:14.494062: step 19550, loss = 0.98, batch loss = 0.91 (14.2 examples/sec; 0.562 sec/batch; 48h:50m:06s remains)
INFO - root - 2017-12-06 09:47:20.567407: step 19560, loss = 0.69, batch loss = 0.62 (13.3 examples/sec; 0.602 sec/batch; 52h:19m:45s remains)
INFO - root - 2017-12-06 09:47:26.522565: step 19570, loss = 1.04, batch loss = 0.97 (15.8 examples/sec; 0.506 sec/batch; 44h:00m:03s remains)
INFO - root - 2017-12-06 09:47:32.689495: step 19580, loss = 1.03, batch loss = 0.96 (12.7 examples/sec; 0.631 sec/batch; 54h:51m:06s remains)
INFO - root - 2017-12-06 09:47:38.797800: step 19590, loss = 0.80, batch loss = 0.73 (13.0 examples/sec; 0.618 sec/batch; 53h:40m:39s remains)
INFO - root - 2017-12-06 09:47:44.848455: step 19600, loss = 1.12, batch loss = 1.05 (13.5 examples/sec; 0.592 sec/batch; 51h:24m:57s remains)
2017-12-06 09:47:45.407710: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.684268 -2.8514044 -3.076139 -3.2678027 -3.3834987 -3.4062345 -3.3427048 -3.2374907 -3.0873861 -2.9068263 -2.709511 -2.5695992 -2.6338243 -2.9080565 -3.3353515][-2.7388349 -2.9850826 -3.2993662 -3.5579162 -3.700186 -3.7135882 -3.6128733 -3.4509199 -3.226933 -2.9678307 -2.6968188 -2.4944797 -2.5458841 -2.859673 -3.3650575][-2.9003208 -3.2070069 -3.5849566 -3.8821616 -4.0196762 -3.990932 -3.8226886 -3.5866838 -3.3005676 -3.0051107 -2.7148862 -2.4953647 -2.5552254 -2.9031835 -3.458493][-3.1638737 -3.495358 -3.8932104 -4.1841187 -4.27278 -4.1579127 -3.8822904 -3.5458763 -3.2079067 -2.9282012 -2.6982853 -2.5429544 -2.6611919 -3.0537314 -3.6342444][-3.42037 -3.7349663 -4.08911 -4.3017745 -4.2728972 -4.0130057 -3.5890195 -3.1329894 -2.7586789 -2.5461411 -2.4600244 -2.4611301 -2.7046938 -3.1761377 -3.786921][-3.6139705 -3.8655012 -4.1014051 -4.1524935 -3.9321127 -3.4771428 -2.878691 -2.2986262 -1.9000735 -1.7840824 -1.90218 -2.1416702 -2.5820422 -3.1796033 -3.8466983][-3.6496582 -3.7951274 -3.8570611 -3.6991658 -3.2596333 -2.6019919 -1.8343782 -1.1440728 -0.7307415 -0.71673465 -1.0616038 -1.5843229 -2.2725346 -3.04358 -3.8065038][-3.5400851 -3.5719044 -3.468854 -3.1333177 -2.5242252 -1.7182014 -0.83058858 -0.071487427 0.33836603 0.26080275 -0.28962231 -1.0733697 -1.9930007 -2.9301887 -3.788106][-3.4745889 -3.4193087 -3.2096186 -2.7833743 -2.1083562 -1.257664 -0.34102678 0.41512966 0.79254341 0.64858532 -0.036647797 -0.98977995 -2.0547092 -3.0799375 -3.9630527][-3.5051239 -3.4147661 -3.1772132 -2.7565002 -2.1266718 -1.348217 -0.51302552 0.15498114 0.46895885 0.29900789 -0.40554953 -1.3753455 -2.4412014 -3.4307504 -4.2423162][-3.6553257 -3.5914569 -3.4080825 -3.0809686 -2.586843 -1.9627852 -1.2843137 -0.7574172 -0.52103019 -0.68009233 -1.2976286 -2.1464388 -3.0771513 -3.9103134 -4.5546322][-3.8719461 -3.8718557 -3.7880249 -3.59581 -3.2710428 -2.8275127 -2.331708 -1.9650202 -1.8203654 -1.9669154 -2.456986 -3.11898 -3.8378429 -4.4417915 -4.8568611][-4.0160007 -4.0787039 -4.0914407 -4.0245972 -3.8524096 -3.5762053 -3.2567668 -3.0379076 -2.9788136 -3.1128244 -3.4700987 -3.9356852 -4.4272089 -4.7949066 -4.9835596][-4.0032029 -4.0992236 -4.170238 -4.1841216 -4.1170497 -3.9688811 -3.7887259 -3.6767249 -3.6704738 -3.7813015 -4.022912 -4.3263106 -4.633389 -4.8257008 -4.8642263][-3.8063016 -3.9028637 -3.987793 -4.0346346 -4.026773 -3.9667804 -3.8863592 -3.8402309 -3.8512411 -3.9222951 -4.059495 -4.2305093 -4.4001594 -4.4929 -4.4824333]]...]
INFO - root - 2017-12-06 09:47:51.409861: step 19610, loss = 0.82, batch loss = 0.75 (13.0 examples/sec; 0.614 sec/batch; 53h:23m:13s remains)
INFO - root - 2017-12-06 09:47:57.491951: step 19620, loss = 0.84, batch loss = 0.77 (13.3 examples/sec; 0.603 sec/batch; 52h:24m:42s remains)
INFO - root - 2017-12-06 09:48:03.599471: step 19630, loss = 0.77, batch loss = 0.70 (13.1 examples/sec; 0.610 sec/batch; 53h:01m:28s remains)
INFO - root - 2017-12-06 09:48:09.624194: step 19640, loss = 0.77, batch loss = 0.69 (12.8 examples/sec; 0.625 sec/batch; 54h:18m:55s remains)
INFO - root - 2017-12-06 09:48:15.759358: step 19650, loss = 0.86, batch loss = 0.79 (12.9 examples/sec; 0.619 sec/batch; 53h:49m:16s remains)
INFO - root - 2017-12-06 09:48:21.893408: step 19660, loss = 0.83, batch loss = 0.76 (12.8 examples/sec; 0.623 sec/batch; 54h:08m:25s remains)
INFO - root - 2017-12-06 09:48:27.981298: step 19670, loss = 1.06, batch loss = 0.99 (13.6 examples/sec; 0.589 sec/batch; 51h:12m:13s remains)
INFO - root - 2017-12-06 09:48:33.727348: step 19680, loss = 0.79, batch loss = 0.72 (12.9 examples/sec; 0.622 sec/batch; 54h:04m:25s remains)
INFO - root - 2017-12-06 09:48:39.755708: step 19690, loss = 1.07, batch loss = 1.00 (13.4 examples/sec; 0.598 sec/batch; 51h:55m:48s remains)
INFO - root - 2017-12-06 09:48:45.817549: step 19700, loss = 0.99, batch loss = 0.92 (13.2 examples/sec; 0.604 sec/batch; 52h:29m:33s remains)
2017-12-06 09:48:46.430375: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5067489 -3.37183 -3.4693666 -3.8837647 -4.3441672 -4.4248619 -4.3880539 -4.311543 -4.31259 -4.2253122 -4.0076776 -3.9315248 -3.7872651 -3.5241156 -3.5056851][-3.5808661 -3.41124 -3.4343 -3.6878295 -4.0029058 -4.0389996 -3.9848785 -3.8750875 -3.8259406 -3.8087957 -3.8380942 -4.0469885 -4.0790296 -3.8344324 -3.7775631][-3.6979494 -3.5319982 -3.462574 -3.519407 -3.6442759 -3.6056795 -3.4841566 -3.302629 -3.2212226 -3.3109715 -3.6011624 -4.0726051 -4.2684288 -4.0968485 -4.01751][-3.8136234 -3.7151589 -3.5748947 -3.4531002 -3.40802 -3.2735147 -3.0454376 -2.7777443 -2.6691437 -2.8655128 -3.3288631 -3.9085855 -4.2071919 -4.1854873 -4.1852317][-3.909266 -3.9051032 -3.7265878 -3.4858413 -3.3136246 -3.0758765 -2.767117 -2.4826989 -2.4038296 -2.6868129 -3.1882973 -3.7172189 -4.0320415 -4.1579075 -4.2953377][-3.9393418 -4.0231729 -3.8615127 -3.5839489 -3.3410654 -3.0271654 -2.6988783 -2.468224 -2.4475436 -2.7777216 -3.2309198 -3.6335788 -3.9112828 -4.1391373 -4.3974285][-3.8996739 -4.0034971 -3.8647072 -3.5958996 -3.3164287 -2.9470718 -2.6256843 -2.4743743 -2.5122871 -2.8762937 -3.3230124 -3.690187 -3.9660614 -4.2469831 -4.5409608][-3.8161132 -3.8501916 -3.700062 -3.4435081 -3.1594841 -2.7482708 -2.4173465 -2.3157132 -2.4169176 -2.8609805 -3.4275975 -3.9171426 -4.2349377 -4.4786882 -4.6806626][-3.7447824 -3.6667953 -3.4943461 -3.2610407 -3.002471 -2.5855565 -2.2534704 -2.191674 -2.3700316 -2.91571 -3.6412287 -4.27126 -4.5975738 -4.7049112 -4.7431374][-3.7964146 -3.6364484 -3.4512343 -3.2409539 -3.0079303 -2.641923 -2.3516183 -2.3156712 -2.5495288 -3.1473727 -3.9321964 -4.5671406 -4.807858 -4.7539263 -4.6511049][-3.9929709 -3.7966571 -3.5870166 -3.3728871 -3.1489534 -2.8547606 -2.6293244 -2.6129022 -2.8823211 -3.4581902 -4.1409678 -4.6084404 -4.6874018 -4.5192132 -4.3678927][-4.229856 -4.0670671 -3.8467543 -3.6108351 -3.3601427 -3.0993707 -2.9115336 -2.9123995 -3.1981423 -3.6955488 -4.1779218 -4.3901634 -4.3025594 -4.0978923 -3.9900579][-4.3432784 -4.2849331 -4.1239052 -3.9004502 -3.6183848 -3.3346729 -3.1275649 -3.1263018 -3.3862062 -3.7542672 -4.0069356 -4.0085096 -3.8283286 -3.6644855 -3.6578672][-4.2679853 -4.3357067 -4.3003597 -4.16274 -3.9144063 -3.6194639 -3.3593726 -3.2894511 -3.4410567 -3.6391521 -3.7101665 -3.6058962 -3.4366138 -3.3442392 -3.411931][-4.0973063 -4.2092204 -4.3019948 -4.3103719 -4.1903005 -3.9572248 -3.6708093 -3.488857 -3.4635277 -3.4618769 -3.3884463 -3.2674901 -3.1658506 -3.1334569 -3.2119236]]...]
INFO - root - 2017-12-06 09:48:52.590480: step 19710, loss = 0.81, batch loss = 0.74 (13.0 examples/sec; 0.618 sec/batch; 53h:39m:12s remains)
INFO - root - 2017-12-06 09:48:58.634224: step 19720, loss = 1.20, batch loss = 1.13 (13.1 examples/sec; 0.610 sec/batch; 52h:58m:16s remains)
INFO - root - 2017-12-06 09:49:04.757890: step 19730, loss = 0.92, batch loss = 0.85 (13.4 examples/sec; 0.597 sec/batch; 51h:49m:46s remains)
INFO - root - 2017-12-06 09:49:10.861480: step 19740, loss = 1.10, batch loss = 1.03 (13.0 examples/sec; 0.616 sec/batch; 53h:33m:17s remains)
INFO - root - 2017-12-06 09:49:16.890681: step 19750, loss = 0.99, batch loss = 0.92 (13.3 examples/sec; 0.600 sec/batch; 52h:05m:44s remains)
INFO - root - 2017-12-06 09:49:22.953379: step 19760, loss = 0.98, batch loss = 0.91 (13.1 examples/sec; 0.613 sec/batch; 53h:13m:09s remains)
INFO - root - 2017-12-06 09:49:29.069824: step 19770, loss = 1.02, batch loss = 0.95 (12.9 examples/sec; 0.618 sec/batch; 53h:39m:54s remains)
INFO - root - 2017-12-06 09:49:35.141100: step 19780, loss = 1.12, batch loss = 1.05 (13.5 examples/sec; 0.592 sec/batch; 51h:23m:40s remains)
INFO - root - 2017-12-06 09:49:41.111514: step 19790, loss = 0.81, batch loss = 0.74 (13.0 examples/sec; 0.617 sec/batch; 53h:36m:09s remains)
INFO - root - 2017-12-06 09:49:47.249978: step 19800, loss = 1.23, batch loss = 1.16 (12.9 examples/sec; 0.620 sec/batch; 53h:52m:38s remains)
2017-12-06 09:49:47.912905: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5026321 -3.6615822 -3.912636 -4.1400228 -4.1725378 -4.3193173 -4.6114106 -4.76246 -4.7919469 -4.6573062 -4.2892108 -4.0051613 -3.8973837 -3.8347566 -3.6560612][-3.4845994 -3.5763984 -3.8193271 -4.0255766 -4.0333281 -4.1653223 -4.4318738 -4.5984216 -4.6550336 -4.5223022 -4.1474361 -3.8210902 -3.6518173 -3.5349536 -3.2887859][-3.7672105 -3.7883372 -3.9797411 -4.09251 -3.9884551 -4.0750909 -4.3548183 -4.4880977 -4.4924345 -4.3502488 -3.9865613 -3.628715 -3.3426709 -3.0737958 -2.7896943][-4.1811 -4.0671668 -4.104207 -4.0567079 -3.8228192 -3.868428 -4.1887255 -4.3029814 -4.2664509 -4.1681094 -3.8845634 -3.5408688 -3.1389012 -2.6799836 -2.3489909][-4.5302277 -4.2126403 -4.01564 -3.79862 -3.4949188 -3.54275 -3.9074588 -4.02778 -3.9408019 -3.8740931 -3.7067168 -3.4391282 -3.0046198 -2.4215012 -2.0400355][-4.759397 -4.2588148 -3.8273373 -3.4727883 -3.1525829 -3.1903167 -3.5478168 -3.7021158 -3.5770128 -3.4924238 -3.4145055 -3.2502637 -2.8766375 -2.2760937 -1.8745654][-4.9181623 -4.3551731 -3.777709 -3.3154457 -2.9658928 -2.9047701 -3.1289525 -3.3155532 -3.2180958 -3.1345029 -3.1152606 -3.0359929 -2.7655628 -2.2168083 -1.8471859][-4.9585724 -4.4247613 -3.8173075 -3.3087552 -2.9499569 -2.7719615 -2.7715468 -2.95316 -2.9531498 -2.9112942 -2.9051085 -2.890173 -2.7435896 -2.2966936 -1.9911413][-4.9659138 -4.4703865 -3.8848114 -3.3748443 -3.062624 -2.8418093 -2.6171627 -2.7357006 -2.8534245 -2.888633 -2.8784301 -2.8848896 -2.8599627 -2.582284 -2.3679862][-4.9444933 -4.5299377 -4.0152793 -3.5179043 -3.2522373 -3.0319192 -2.6249256 -2.6457748 -2.8679843 -3.0037286 -3.0061657 -2.986537 -3.008697 -2.8892083 -2.7809944][-4.7961855 -4.5584621 -4.2425165 -3.8365879 -3.5863204 -3.3326793 -2.7916574 -2.6985273 -2.9767964 -3.1918693 -3.2537794 -3.2206583 -3.1678731 -3.0633874 -2.997303][-4.6269469 -4.5746446 -4.5050879 -4.2628131 -4.0347171 -3.7570894 -3.1978807 -3.0091724 -3.2887506 -3.5319376 -3.6523221 -3.6585541 -3.4984365 -3.2364764 -3.0664248][-4.5732083 -4.6194015 -4.6891012 -4.6142788 -4.4506621 -4.2118306 -3.7661266 -3.576395 -3.8476677 -4.0936322 -4.1997013 -4.1832247 -3.88168 -3.3632045 -3.0151243][-4.5852451 -4.6644177 -4.7407627 -4.7658782 -4.6913137 -4.54066 -4.2623935 -4.130456 -4.3862567 -4.6119404 -4.6149826 -4.4824395 -4.0717578 -3.3667293 -2.8857412][-4.5236468 -4.6454024 -4.6673937 -4.6882873 -4.67823 -4.6304355 -4.514811 -4.464005 -4.7128682 -4.9347658 -4.8259096 -4.5682993 -4.124877 -3.3341718 -2.7597156]]...]
INFO - root - 2017-12-06 09:49:53.903767: step 19810, loss = 0.95, batch loss = 0.88 (13.1 examples/sec; 0.612 sec/batch; 53h:09m:35s remains)
INFO - root - 2017-12-06 09:50:00.067698: step 19820, loss = 1.03, batch loss = 0.96 (12.8 examples/sec; 0.625 sec/batch; 54h:17m:28s remains)
INFO - root - 2017-12-06 09:50:06.038426: step 19830, loss = 0.94, batch loss = 0.87 (13.1 examples/sec; 0.609 sec/batch; 52h:54m:47s remains)
INFO - root - 2017-12-06 09:50:12.085431: step 19840, loss = 0.94, batch loss = 0.87 (13.9 examples/sec; 0.575 sec/batch; 49h:56m:02s remains)
INFO - root - 2017-12-06 09:50:18.157816: step 19850, loss = 1.00, batch loss = 0.93 (13.2 examples/sec; 0.606 sec/batch; 52h:38m:45s remains)
INFO - root - 2017-12-06 09:50:24.213098: step 19860, loss = 0.87, batch loss = 0.80 (13.2 examples/sec; 0.605 sec/batch; 52h:32m:14s remains)
INFO - root - 2017-12-06 09:50:30.323888: step 19870, loss = 0.90, batch loss = 0.83 (12.6 examples/sec; 0.637 sec/batch; 55h:20m:36s remains)
INFO - root - 2017-12-06 09:50:36.418563: step 19880, loss = 0.70, batch loss = 0.63 (13.6 examples/sec; 0.588 sec/batch; 51h:03m:25s remains)
INFO - root - 2017-12-06 09:50:42.458792: step 19890, loss = 1.04, batch loss = 0.97 (13.1 examples/sec; 0.609 sec/batch; 52h:53m:29s remains)
INFO - root - 2017-12-06 09:50:48.334883: step 19900, loss = 1.03, batch loss = 0.96 (13.1 examples/sec; 0.609 sec/batch; 52h:55m:04s remains)
2017-12-06 09:50:48.906983: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.31842 -2.0659654 -2.5559757 -3.5030487 -4.5181646 -5.5059075 -5.578105 -5.5392656 -5.4227052 -5.1270795 -5.2332692 -5.3619976 -5.5410633 -5.592855 -5.619432][-2.859314 -2.8313107 -3.452796 -4.2808752 -4.9869642 -5.6209836 -5.3457203 -5.0588832 -4.9214497 -4.7252946 -4.9082565 -5.0006666 -5.0183167 -4.9680929 -5.051805][-3.5650249 -3.7540669 -4.4490027 -5.1449451 -5.5342064 -5.7814035 -5.1590996 -4.576067 -4.3261366 -4.1236854 -4.1903563 -4.0757494 -3.8416791 -3.6582694 -3.7556672][-4.013247 -4.4034166 -5.1461291 -5.768281 -5.9146743 -5.7920065 -4.8887424 -4.0465188 -3.6019676 -3.2676311 -3.0661883 -2.6546798 -2.1781843 -1.9293065 -2.0723][-4.129734 -4.6473536 -5.370472 -5.8849339 -5.7773113 -5.2579393 -4.1063509 -3.0715432 -2.4750869 -2.0529952 -1.6708322 -1.0878727 -0.52236295 -0.34694433 -0.60947084][-4.052166 -4.5768862 -5.1846786 -5.5126858 -5.1507854 -4.3032522 -2.9630041 -1.8188071 -1.1709213 -0.8293128 -0.53379726 -0.063251495 0.37377071 0.34744406 -0.068597317][-3.8591776 -4.320065 -4.7912903 -4.9631014 -4.4172769 -3.335989 -1.8155022 -0.57677507 0.010473251 0.10286903 0.065824032 0.16531038 0.29829359 0.017959118 -0.52485251][-3.7900271 -4.1839075 -4.5637641 -4.6509938 -4.0032787 -2.7938838 -1.153528 0.1366725 0.58721018 0.3835535 -0.043263912 -0.36961269 -0.55647326 -1.0316589 -1.6167226][-3.7354794 -4.0610824 -4.416904 -4.5461836 -3.9807322 -2.8541603 -1.2924929 -0.11201572 0.13566589 -0.29039955 -0.95587564 -1.5375671 -1.9011562 -2.4141245 -2.9060216][-3.4447296 -3.7366776 -4.1718669 -4.499342 -4.2340922 -3.4345872 -2.1642706 -1.2303615 -1.1766326 -1.6657546 -2.2947669 -2.8451676 -3.19067 -3.6409371 -4.0010138][-3.0798869 -3.4209418 -4.0217214 -4.6299949 -4.7793264 -4.39848 -3.4604018 -2.7383971 -2.7956066 -3.2352438 -3.684042 -4.054306 -4.2884426 -4.6605468 -4.9274397][-2.9272244 -3.3553143 -4.1118326 -4.9673977 -5.46669 -5.4260678 -4.7596855 -4.1974082 -4.3082395 -4.6618118 -4.934433 -5.1137161 -5.2181935 -5.4990807 -5.7083364][-3.1235709 -3.6000109 -4.4234519 -5.3807068 -6.0407104 -6.1770945 -5.699676 -5.2677584 -5.3645325 -5.5944881 -5.728653 -5.7616539 -5.7350597 -5.8964558 -6.0594158][-3.5046496 -3.9716983 -4.7559142 -5.6500516 -6.2534027 -6.3627491 -5.9572053 -5.6056261 -5.6305714 -5.7339597 -5.7786174 -5.738976 -5.6404781 -5.7213488 -5.8720717][-3.8303127 -4.2333035 -4.867054 -5.5422115 -5.9442263 -5.9243789 -5.5447669 -5.256598 -5.2299771 -5.2699604 -5.2889323 -5.2391844 -5.1417313 -5.20469 -5.3700581]]...]
INFO - root - 2017-12-06 09:50:54.966077: step 19910, loss = 0.95, batch loss = 0.88 (13.7 examples/sec; 0.586 sec/batch; 50h:50m:28s remains)
INFO - root - 2017-12-06 09:51:01.017889: step 19920, loss = 0.84, batch loss = 0.77 (13.1 examples/sec; 0.613 sec/batch; 53h:12m:51s remains)
INFO - root - 2017-12-06 09:51:07.104814: step 19930, loss = 0.73, batch loss = 0.66 (13.3 examples/sec; 0.601 sec/batch; 52h:11m:36s remains)
INFO - root - 2017-12-06 09:51:13.136081: step 19940, loss = 1.25, batch loss = 1.18 (13.3 examples/sec; 0.602 sec/batch; 52h:18m:24s remains)
INFO - root - 2017-12-06 09:51:19.219179: step 19950, loss = 0.95, batch loss = 0.88 (13.0 examples/sec; 0.615 sec/batch; 53h:21m:02s remains)
INFO - root - 2017-12-06 09:51:25.406839: step 19960, loss = 0.97, batch loss = 0.90 (13.0 examples/sec; 0.615 sec/batch; 53h:26m:03s remains)
INFO - root - 2017-12-06 09:51:31.524892: step 19970, loss = 1.03, batch loss = 0.96 (13.0 examples/sec; 0.614 sec/batch; 53h:17m:51s remains)
INFO - root - 2017-12-06 09:51:37.514156: step 19980, loss = 0.91, batch loss = 0.84 (13.2 examples/sec; 0.608 sec/batch; 52h:46m:07s remains)
INFO - root - 2017-12-06 09:51:43.617203: step 19990, loss = 0.71, batch loss = 0.64 (13.2 examples/sec; 0.608 sec/batch; 52h:45m:25s remains)
INFO - root - 2017-12-06 09:51:49.553930: step 20000, loss = 0.84, batch loss = 0.77 (15.2 examples/sec; 0.525 sec/batch; 45h:35m:53s remains)
2017-12-06 09:51:50.151707: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1081848 -5.100944 -4.9819193 -4.8030829 -4.6649594 -4.6770687 -4.7711329 -4.8330712 -4.8420386 -4.8252811 -4.7476993 -4.5749345 -4.3540177 -4.1745033 -4.0804381][-5.3533444 -5.4067507 -5.2786403 -5.0059209 -4.7428513 -4.7016249 -4.813375 -4.940762 -5.02398 -5.1123958 -5.1501913 -5.0196514 -4.7348437 -4.441678 -4.2428904][-5.4115615 -5.5460806 -5.4264212 -5.049871 -4.62275 -4.4583387 -4.5353608 -4.7518878 -4.9849739 -5.2501917 -5.4697704 -5.4425573 -5.1366067 -4.7334137 -4.4236][-5.3480749 -5.5302296 -5.3998051 -4.8924303 -4.2468476 -3.8718758 -3.8317223 -4.1192784 -4.5854321 -5.1407809 -5.6139 -5.7467151 -5.4905705 -5.0138855 -4.5950904][-5.2157235 -5.3229094 -5.0880184 -4.4088483 -3.4960885 -2.8211713 -2.5576491 -2.8582182 -3.6292598 -4.608283 -5.4278455 -5.7762976 -5.6540775 -5.1867828 -4.6962914][-5.1539817 -5.0756969 -4.6193118 -3.7050517 -2.5041862 -1.5008523 -0.97121596 -1.212399 -2.2455511 -3.6410592 -4.8212314 -5.4325619 -5.5428228 -5.2249613 -4.7519097][-5.338582 -5.1633611 -4.5256176 -3.39318 -1.9267356 -0.6185379 0.14468145 -0.02239275 -1.1689358 -2.748498 -4.1159158 -4.9192338 -5.2694912 -5.184154 -4.8195834][-5.6172667 -5.5528822 -4.958384 -3.8224525 -2.3050652 -0.86571 0.047440052 0.0093193054 -0.98644209 -2.4003124 -3.6702306 -4.4971375 -4.9797115 -5.0991621 -4.8878069][-5.698606 -5.8251448 -5.4475737 -4.5501184 -3.2601178 -1.9649875 -1.0904443 -0.98164821 -1.6335149 -2.6444776 -3.6217096 -4.338521 -4.8243122 -5.0488086 -4.9706774][-5.5384364 -5.7960143 -5.6299124 -5.0378051 -4.0993319 -3.0963283 -2.3904324 -2.2069192 -2.5312328 -3.1405697 -3.803894 -4.3699021 -4.7902012 -5.0365305 -5.0503969][-5.3403988 -5.6630883 -5.6300783 -5.2689281 -4.6278739 -3.8824162 -3.3378563 -3.1383793 -3.271482 -3.6279478 -4.0624714 -4.4830055 -4.8097272 -5.028687 -5.0853872][-5.1299734 -5.5005007 -5.5652995 -5.3655176 -4.9491091 -4.3984509 -3.9677765 -3.7725463 -3.810195 -4.0178695 -4.2946615 -4.5780492 -4.8073139 -4.9784665 -5.0309405][-4.8157187 -5.1879516 -5.3346834 -5.2718353 -5.0347562 -4.6385412 -4.2914925 -4.10478 -4.0833945 -4.1819553 -4.338285 -4.51577 -4.669168 -4.803184 -4.8452315][-4.444459 -4.7598844 -4.9422064 -4.972908 -4.8585181 -4.5864959 -4.3165569 -4.1433158 -4.0808306 -4.0945053 -4.1628628 -4.2751646 -4.3942747 -4.5254416 -4.5861497][-4.1256328 -4.3580375 -4.5293021 -4.6073647 -4.5646625 -4.3889966 -4.1859574 -4.0212388 -3.9183209 -3.8695154 -3.8787704 -3.9489491 -4.0565319 -4.2095881 -4.3229709]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-1/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-1/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 09:51:56.982945: step 20010, loss = 0.72, batch loss = 0.65 (12.9 examples/sec; 0.619 sec/batch; 53h:43m:01s remains)
INFO - root - 2017-12-06 09:52:03.051679: step 20020, loss = 0.87, batch loss = 0.80 (13.2 examples/sec; 0.605 sec/batch; 52h:30m:02s remains)
INFO - root - 2017-12-06 09:52:09.158446: step 20030, loss = 0.97, batch loss = 0.90 (13.3 examples/sec; 0.603 sec/batch; 52h:22m:26s remains)
INFO - root - 2017-12-06 09:52:15.286967: step 20040, loss = 0.96, batch loss = 0.89 (12.4 examples/sec; 0.646 sec/batch; 56h:01m:58s remains)
INFO - root - 2017-12-06 09:52:21.369562: step 20050, loss = 1.02, batch loss = 0.95 (13.7 examples/sec; 0.585 sec/batch; 50h:46m:54s remains)
INFO - root - 2017-12-06 09:52:27.492433: step 20060, loss = 0.72, batch loss = 0.65 (13.0 examples/sec; 0.613 sec/batch; 53h:14m:07s remains)
INFO - root - 2017-12-06 09:52:33.621258: step 20070, loss = 1.10, batch loss = 1.03 (12.8 examples/sec; 0.626 sec/batch; 54h:17m:23s remains)
INFO - root - 2017-12-06 09:52:39.702490: step 20080, loss = 0.98, batch loss = 0.91 (13.7 examples/sec; 0.582 sec/batch; 50h:31m:36s remains)
INFO - root - 2017-12-06 09:52:45.847940: step 20090, loss = 0.92, batch loss = 0.85 (13.1 examples/sec; 0.609 sec/batch; 52h:51m:57s remains)
INFO - root - 2017-12-06 09:52:51.911816: step 20100, loss = 0.92, batch loss = 0.85 (13.1 examples/sec; 0.613 sec/batch; 53h:10m:02s remains)
2017-12-06 09:52:52.483436: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6862392 -4.5508924 -4.5973253 -4.6418896 -4.7921224 -4.8454723 -4.5452905 -4.3834825 -4.7366886 -5.2068977 -5.4771633 -5.4481721 -5.0706644 -4.5091095 -4.0142479][-4.9537096 -4.8103008 -4.9070435 -4.922965 -5.0615921 -5.1775165 -4.857697 -4.6668897 -5.1011629 -5.5985041 -5.8460679 -5.8676615 -5.4690218 -4.7663813 -4.1112452][-5.0989046 -4.9436927 -5.0186806 -4.9637871 -5.0537562 -5.2615061 -5.0616674 -4.9097037 -5.3857555 -5.8230295 -5.9728951 -6.0481019 -5.6751127 -4.884366 -4.1245747][-5.3299232 -5.1840134 -5.121572 -4.8728504 -4.8126016 -5.0637674 -5.1161795 -5.177825 -5.7476044 -6.1042356 -6.1203389 -6.18129 -5.751133 -4.8482094 -4.0134144][-5.6787171 -5.5814257 -5.3076324 -4.7296686 -4.354877 -4.5169544 -4.8261228 -5.2072282 -5.9269066 -6.250576 -6.2127857 -6.2623863 -5.7210646 -4.6936746 -3.8130479][-5.9949865 -6.0227356 -5.5646849 -4.5556436 -3.6651492 -3.5283113 -3.9284856 -4.5573044 -5.4074697 -5.7823725 -5.8790359 -6.0734825 -5.5102062 -4.4311757 -3.5666924][-5.8001742 -6.04501 -5.6065044 -4.3670139 -3.0444064 -2.5629182 -2.8844295 -3.5460508 -4.3666463 -4.7486892 -5.0618486 -5.5397468 -5.1416969 -4.1408181 -3.3552299][-4.9327087 -5.5271006 -5.3964615 -4.3331962 -2.9010084 -2.1835158 -2.3321474 -2.8129268 -3.4365263 -3.7408133 -4.2297382 -5.0120735 -4.8831725 -4.0116954 -3.2834098][-3.7059379 -4.5610833 -4.8697729 -4.3196192 -3.1630058 -2.4050484 -2.4086661 -2.667619 -3.1088872 -3.3776312 -3.9929504 -4.9630222 -5.0019093 -4.16961 -3.3704176][-2.9922605 -3.7846711 -4.3222861 -4.2411895 -3.4636922 -2.752912 -2.6648433 -2.8127642 -3.2482626 -3.6183066 -4.3328815 -5.3459816 -5.3935122 -4.4813952 -3.5094817][-3.2396026 -3.6872716 -4.132513 -4.1990366 -3.588644 -2.8170481 -2.6146107 -2.7708609 -3.3717031 -3.9544716 -4.7745347 -5.7577591 -5.7291245 -4.7060375 -3.5706792][-4.1628084 -4.2366037 -4.4475336 -4.4125524 -3.7532172 -2.8591833 -2.5698986 -2.8220334 -3.5864308 -4.3198214 -5.1495695 -6.0121064 -5.8855991 -4.7955613 -3.5947056][-5.1444216 -5.0466452 -5.1239758 -5.020113 -4.320406 -3.3646688 -3.038238 -3.3759744 -4.1575775 -4.8594189 -5.5522146 -6.1815205 -5.930737 -4.8297219 -3.666786][-5.6816564 -5.6504087 -5.7446971 -5.6857319 -5.0690808 -4.1651812 -3.8181491 -4.1514039 -4.8159823 -5.3645463 -5.8473043 -6.2093434 -5.8494411 -4.818274 -3.7974482][-5.6967154 -5.7613344 -5.9067235 -5.9418859 -5.5091825 -4.7903562 -4.4788265 -4.7320476 -5.2158871 -5.5800781 -5.8620644 -6.0083857 -5.6003666 -4.7207756 -3.9130607]]...]
INFO - root - 2017-12-06 09:52:58.458176: step 20110, loss = 0.83, batch loss = 0.76 (12.9 examples/sec; 0.623 sec/batch; 54h:01m:15s remains)
INFO - root - 2017-12-06 09:53:04.540777: step 20120, loss = 1.06, batch loss = 0.99 (13.3 examples/sec; 0.604 sec/batch; 52h:22m:55s remains)
INFO - root - 2017-12-06 09:53:10.478384: step 20130, loss = 0.69, batch loss = 0.61 (13.3 examples/sec; 0.602 sec/batch; 52h:15m:39s remains)
INFO - root - 2017-12-06 09:53:16.572279: step 20140, loss = 1.05, batch loss = 0.98 (12.7 examples/sec; 0.628 sec/batch; 54h:30m:12s remains)
INFO - root - 2017-12-06 09:53:22.633441: step 20150, loss = 0.82, batch loss = 0.75 (13.8 examples/sec; 0.581 sec/batch; 50h:24m:43s remains)
INFO - root - 2017-12-06 09:53:28.694085: step 20160, loss = 0.92, batch loss = 0.85 (13.1 examples/sec; 0.609 sec/batch; 52h:50m:01s remains)
INFO - root - 2017-12-06 09:53:34.676577: step 20170, loss = 0.99, batch loss = 0.92 (12.9 examples/sec; 0.622 sec/batch; 53h:56m:01s remains)
INFO - root - 2017-12-06 09:53:40.675013: step 20180, loss = 1.03, batch loss = 0.96 (13.9 examples/sec; 0.574 sec/batch; 49h:49m:13s remains)
INFO - root - 2017-12-06 09:53:46.760928: step 20190, loss = 0.90, batch loss = 0.83 (13.3 examples/sec; 0.602 sec/batch; 52h:11m:50s remains)
INFO - root - 2017-12-06 09:53:52.907488: step 20200, loss = 0.85, batch loss = 0.78 (13.2 examples/sec; 0.606 sec/batch; 52h:33m:13s remains)
2017-12-06 09:53:53.534642: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0183048 -4.3490973 -4.610837 -4.7510815 -4.665267 -4.2184339 -3.6705122 -3.1912408 -3.1124439 -3.5160975 -3.9805918 -4.3090515 -4.3169947 -3.9843609 -3.3066812][-3.6522491 -4.0511856 -4.430335 -4.662982 -4.6233783 -4.1959124 -3.684526 -3.3204708 -3.3919954 -3.8816116 -4.3326955 -4.5635748 -4.4106092 -3.9661441 -3.2854972][-3.2098069 -3.7061667 -4.2842021 -4.6507444 -4.6173797 -4.1339059 -3.6014318 -3.3554296 -3.6048145 -4.2247605 -4.7141485 -4.8816013 -4.6235008 -4.1399312 -3.5013635][-2.7430739 -3.3266537 -4.1054821 -4.5902362 -4.5206308 -3.9366174 -3.3462143 -3.1907427 -3.5748861 -4.3161345 -4.8785543 -4.9913645 -4.6488791 -4.1554317 -3.570616][-2.4041443 -3.0586796 -3.960253 -4.4788971 -4.3201227 -3.6199374 -2.9711771 -2.8624952 -3.3129234 -4.1272931 -4.8029733 -4.9348116 -4.5659208 -4.1157284 -3.5996649][-2.2003052 -2.8811612 -3.8032141 -4.2806244 -4.0022435 -3.1673539 -2.423979 -2.2884851 -2.7479014 -3.6288872 -4.4741316 -4.7320075 -4.4184318 -4.0549221 -3.6253033][-2.1222973 -2.8313084 -3.7214191 -4.107409 -3.6671133 -2.6646006 -1.7664206 -1.5068357 -1.9534962 -2.9401889 -3.9989378 -4.4613481 -4.2658114 -4.0141468 -3.7018204][-2.274636 -2.9886847 -3.8075397 -4.0823426 -3.4769804 -2.3096688 -1.2679269 -0.86533761 -1.29797 -2.3908024 -3.6305594 -4.3098154 -4.2744322 -4.1491475 -3.9589505][-2.5633283 -3.2104955 -3.9178133 -4.1215987 -3.4661245 -2.2803555 -1.2123718 -0.74292064 -1.1817555 -2.3275671 -3.6448057 -4.4623814 -4.5685992 -4.5557165 -4.4509778][-3.028621 -3.5416532 -4.0949383 -4.2550745 -3.6602464 -2.5709827 -1.5717952 -1.113929 -1.5917914 -2.7440586 -4.0437207 -4.8783431 -5.0206375 -5.0363669 -4.9486647][-3.7558494 -4.108098 -4.4680343 -4.5594459 -4.0439615 -3.1008148 -2.1976979 -1.7651491 -2.246547 -3.3387308 -4.5259409 -5.2657785 -5.3290067 -5.26723 -5.14544][-4.4786167 -4.6594963 -4.8187556 -4.8233876 -4.4150515 -3.6939259 -2.9704871 -2.6122653 -3.0349407 -3.963233 -4.9295688 -5.4734716 -5.3632641 -5.1276035 -4.9117174][-4.8205647 -4.8741341 -4.9322047 -4.9437447 -4.7376094 -4.3250465 -3.8537161 -3.5784187 -3.8449066 -4.4654474 -5.0734072 -5.3114481 -4.9897442 -4.5852652 -4.3050532][-4.8216286 -4.7842803 -4.8147669 -4.9043455 -4.9290924 -4.8073249 -4.5465827 -4.2999024 -4.33987 -4.5952196 -4.8235774 -4.7681522 -4.320128 -3.8690281 -3.6366405][-4.5394955 -4.4258332 -4.4467249 -4.6104426 -4.8155875 -4.8980694 -4.7631078 -4.5056825 -4.3619542 -4.3671875 -4.3725314 -4.1726327 -3.704412 -3.2754178 -3.1100068]]...]
INFO - root - 2017-12-06 09:53:59.755742: step 20210, loss = 1.07, batch loss = 1.00 (12.6 examples/sec; 0.637 sec/batch; 55h:15m:28s remains)
INFO - root - 2017-12-06 09:54:05.705829: step 20220, loss = 1.04, batch loss = 0.97 (12.8 examples/sec; 0.624 sec/batch; 54h:08m:10s remains)
INFO - root - 2017-12-06 09:54:11.793225: step 20230, loss = 0.70, batch loss = 0.63 (12.9 examples/sec; 0.619 sec/batch; 53h:42m:17s remains)
INFO - root - 2017-12-06 09:54:17.920159: step 20240, loss = 1.05, batch loss = 0.98 (13.3 examples/sec; 0.599 sec/batch; 51h:59m:34s remains)
INFO - root - 2017-12-06 09:54:24.020207: step 20250, loss = 0.76, batch loss = 0.69 (12.8 examples/sec; 0.626 sec/batch; 54h:19m:59s remains)
INFO - root - 2017-12-06 09:54:30.138220: step 20260, loss = 0.90, batch loss = 0.83 (12.3 examples/sec; 0.650 sec/batch; 56h:20m:16s remains)
INFO - root - 2017-12-06 09:54:36.197566: step 20270, loss = 0.95, batch loss = 0.88 (13.4 examples/sec; 0.599 sec/batch; 51h:55m:12s remains)
INFO - root - 2017-12-06 09:54:42.142802: step 20280, loss = 0.89, batch loss = 0.82 (13.8 examples/sec; 0.581 sec/batch; 50h:24m:33s remains)
INFO - root - 2017-12-06 09:54:48.241683: step 20290, loss = 0.84, batch loss = 0.77 (13.4 examples/sec; 0.599 sec/batch; 51h:57m:29s remains)
INFO - root - 2017-12-06 09:54:54.348121: step 20300, loss = 1.22, batch loss = 1.15 (12.9 examples/sec; 0.618 sec/batch; 53h:35m:29s remains)
2017-12-06 09:54:54.901173: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3250022 -4.1234608 -3.998879 -4.0832353 -4.3113031 -4.5087695 -4.5780387 -4.6223531 -4.7023973 -4.7187219 -4.3706427 -3.759402 -3.0933919 -2.6143992 -2.6707587][-4.3303661 -4.0862117 -3.8814816 -3.8688693 -4.0677638 -4.3237915 -4.4642377 -4.5791159 -4.7971463 -4.9858804 -4.73057 -4.0783663 -3.3001404 -2.6740818 -2.6009908][-4.3002605 -4.0606461 -3.8262343 -3.6956472 -3.8081036 -4.0931673 -4.3200788 -4.5315351 -4.89728 -5.2660766 -5.1456518 -4.5099587 -3.6856787 -2.9560139 -2.7410257][-4.2637558 -4.0588937 -3.8112705 -3.521394 -3.4434814 -3.6563177 -3.9333155 -4.2753735 -4.8448734 -5.4544625 -5.5655909 -5.0721312 -4.3205438 -3.5618651 -3.2117314][-4.3228221 -4.1485119 -3.8349683 -3.3135357 -2.9234233 -2.9085727 -3.122663 -3.5582821 -4.3102031 -5.1441855 -5.5443277 -5.3509827 -4.8796749 -4.2751927 -3.8795624][-4.5157948 -4.3631639 -3.9856448 -3.2610238 -2.5356348 -2.1513386 -2.1155593 -2.5093918 -3.3611286 -4.3398166 -4.9897819 -5.1628666 -5.1088085 -4.8355722 -4.5356488][-4.6447158 -4.52305 -4.1568604 -3.3672633 -2.4068236 -1.5918243 -1.1090879 -1.262336 -2.1303341 -3.2600851 -4.1755524 -4.727519 -5.0726194 -5.1346226 -4.9948182][-4.6584587 -4.64957 -4.4737735 -3.8738313 -2.9487944 -1.8818746 -0.93621826 -0.61933374 -1.2257042 -2.321856 -3.3850219 -4.2296677 -4.8695316 -5.1914749 -5.2178807][-4.7064719 -4.8566008 -4.9568949 -4.7075315 -4.0955954 -3.1746898 -2.1211822 -1.4288776 -1.5119708 -2.202635 -3.0756941 -3.9185667 -4.6458659 -5.1241484 -5.3211727][-4.6537924 -4.8415575 -5.0511031 -5.031486 -4.7349167 -4.2208571 -3.5121179 -2.8385508 -2.5727053 -2.8142138 -3.302506 -3.8836532 -4.4767618 -4.9954138 -5.3473263][-4.4461846 -4.5340271 -4.6149426 -4.5802116 -4.4309592 -4.2992363 -4.1202984 -3.7979448 -3.523562 -3.5470681 -3.747149 -4.0667439 -4.4573879 -4.8834939 -5.255054][-4.3028116 -4.2594461 -4.0973377 -3.8664186 -3.6517348 -3.7131104 -3.9807203 -4.1295443 -4.0579247 -4.0307727 -4.0910845 -4.2742605 -4.540586 -4.8324 -5.1047277][-4.16737 -4.0596266 -3.7337508 -3.3169312 -2.9132621 -2.9151068 -3.3423491 -3.8590388 -4.1075163 -4.1913218 -4.2355661 -4.3809981 -4.5927119 -4.7496452 -4.8911057][-4.0430098 -3.9913318 -3.6873662 -3.2447138 -2.7215624 -2.529367 -2.776063 -3.3257818 -3.7917166 -4.0725894 -4.2229104 -4.4131665 -4.6365929 -4.7008944 -4.7100692][-4.0045071 -4.0503535 -3.9030271 -3.6110988 -3.1473656 -2.8271646 -2.7842679 -3.0720153 -3.4980793 -3.8559258 -4.0931263 -4.3586559 -4.6507659 -4.7172093 -4.6631727]]...]
INFO - root - 2017-12-06 09:55:00.997453: step 20310, loss = 0.85, batch loss = 0.78 (13.0 examples/sec; 0.615 sec/batch; 53h:17m:42s remains)
INFO - root - 2017-12-06 09:55:07.026346: step 20320, loss = 0.90, batch loss = 0.83 (12.7 examples/sec; 0.631 sec/batch; 54h:44m:18s remains)
INFO - root - 2017-12-06 09:55:13.037579: step 20330, loss = 0.80, batch loss = 0.73 (13.3 examples/sec; 0.604 sec/batch; 52h:21m:17s remains)
INFO - root - 2017-12-06 09:55:19.163356: step 20340, loss = 0.80, batch loss = 0.73 (13.0 examples/sec; 0.615 sec/batch; 53h:17m:46s remains)
INFO - root - 2017-12-06 09:55:25.290090: step 20350, loss = 0.75, batch loss = 0.68 (12.8 examples/sec; 0.625 sec/batch; 54h:12m:56s remains)
INFO - root - 2017-12-06 09:55:31.346508: step 20360, loss = 0.68, batch loss = 0.61 (13.2 examples/sec; 0.607 sec/batch; 52h:36m:56s remains)
INFO - root - 2017-12-06 09:55:37.496630: step 20370, loss = 1.07, batch loss = 1.00 (12.9 examples/sec; 0.621 sec/batch; 53h:50m:26s remains)
INFO - root - 2017-12-06 09:55:43.580975: step 20380, loss = 0.89, batch loss = 0.82 (13.0 examples/sec; 0.616 sec/batch; 53h:26m:34s remains)
INFO - root - 2017-12-06 09:55:49.743673: step 20390, loss = 0.92, batch loss = 0.85 (13.5 examples/sec; 0.591 sec/batch; 51h:14m:21s remains)
INFO - root - 2017-12-06 09:55:55.804636: step 20400, loss = 0.97, batch loss = 0.90 (13.0 examples/sec; 0.616 sec/batch; 53h:22m:30s remains)
2017-12-06 09:55:56.365743: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9777036 -2.9695454 -2.9981372 -2.8820665 -2.6852386 -2.5290518 -2.4714234 -2.4746697 -2.5696774 -2.7239614 -2.7519093 -2.79722 -2.9297664 -3.2549968 -3.7027607][-2.1574993 -2.0894396 -2.1540678 -2.1165643 -2.0137179 -2.0354612 -2.1548386 -2.2161574 -2.3037066 -2.3913586 -2.3132176 -2.2611454 -2.3222957 -2.6674569 -3.2752109][-1.5681052 -1.4509964 -1.5678654 -1.665468 -1.7048435 -1.9107928 -2.1797595 -2.2385485 -2.2356224 -2.1925094 -1.9913065 -1.8594577 -1.8732395 -2.1828113 -2.8227119][-1.5794504 -1.4116495 -1.5327923 -1.7392399 -1.9012737 -2.1731017 -2.4027371 -2.2959285 -2.0777762 -1.8751028 -1.6635289 -1.6345627 -1.7702942 -2.1007442 -2.6986141][-2.0981514 -1.8697898 -1.9430003 -2.2027259 -2.443341 -2.6684554 -2.6889243 -2.286943 -1.7702439 -1.3984749 -1.278286 -1.5233321 -1.9509053 -2.3983729 -2.9404166][-2.9051461 -2.6405153 -2.6588409 -2.9164588 -3.1661105 -3.2688375 -3.0055809 -2.2839026 -1.478261 -0.96134043 -0.96681786 -1.5257771 -2.2953117 -2.9205692 -3.4395411][-3.8352282 -3.560415 -3.5436907 -3.7643857 -3.9696126 -3.939755 -3.4562182 -2.5324845 -1.5578096 -0.95993352 -1.0426805 -1.7856443 -2.7631373 -3.5116742 -4.0089412][-4.7481589 -4.5138907 -4.4918871 -4.6529412 -4.7738261 -4.6345987 -4.0421729 -3.0699756 -2.0759737 -1.481447 -1.582597 -2.3200512 -3.2875743 -4.0337353 -4.4759569][-5.264204 -5.1284037 -5.1447744 -5.2751822 -5.3482003 -5.1806087 -4.6274652 -3.7896123 -2.9512618 -2.4658909 -2.5418491 -3.1040759 -3.8503284 -4.4250178 -4.7307243][-5.1650667 -5.1631446 -5.2566762 -5.4006128 -5.48456 -5.3779354 -4.9961324 -4.4348927 -3.8710623 -3.5407016 -3.5527124 -3.8534939 -4.2676821 -4.5683951 -4.691][-4.6332426 -4.7658405 -4.941112 -5.1090236 -5.217875 -5.1919045 -5.00131 -4.7173786 -4.4241514 -4.2458835 -4.2128248 -4.2999763 -4.426228 -4.4805012 -4.454545][-4.0082107 -4.2251954 -4.4493923 -4.6225734 -4.7385459 -4.7689443 -4.715116 -4.6126075 -4.4941626 -4.4200444 -4.3833356 -4.3756452 -4.3553214 -4.2812228 -4.1896811][-3.6085238 -3.8417306 -4.0646129 -4.2107105 -4.2965817 -4.3313732 -4.3200974 -4.26617 -4.1953893 -4.1546364 -4.137682 -4.1334581 -4.1014109 -4.0257049 -3.953063][-3.4823194 -3.6921139 -3.8927231 -4.012856 -4.064939 -4.0714049 -4.0231686 -3.9138937 -3.7860684 -3.7062275 -3.6930137 -3.731122 -3.7668221 -3.7648609 -3.7598643][-3.604629 -3.7610898 -3.9188511 -4.0104113 -4.0314274 -3.9929368 -3.864975 -3.6451216 -3.4009054 -3.236629 -3.2073627 -3.294086 -3.4254456 -3.5253532 -3.6056876]]...]
INFO - root - 2017-12-06 09:56:02.471303: step 20410, loss = 1.12, batch loss = 1.05 (13.2 examples/sec; 0.608 sec/batch; 52h:43m:31s remains)
INFO - root - 2017-12-06 09:56:08.516020: step 20420, loss = 0.92, batch loss = 0.85 (13.2 examples/sec; 0.608 sec/batch; 52h:41m:26s remains)
INFO - root - 2017-12-06 09:56:14.352474: step 20430, loss = 1.11, batch loss = 1.04 (13.6 examples/sec; 0.590 sec/batch; 51h:07m:03s remains)
INFO - root - 2017-12-06 09:56:20.476285: step 20440, loss = 0.88, batch loss = 0.81 (13.1 examples/sec; 0.611 sec/batch; 52h:59m:30s remains)
INFO - root - 2017-12-06 09:56:26.470052: step 20450, loss = 0.77, batch loss = 0.70 (12.8 examples/sec; 0.625 sec/batch; 54h:10m:00s remains)
INFO - root - 2017-12-06 09:56:32.518347: step 20460, loss = 0.81, batch loss = 0.74 (13.7 examples/sec; 0.584 sec/batch; 50h:35m:20s remains)
INFO - root - 2017-12-06 09:56:38.570370: step 20470, loss = 0.81, batch loss = 0.74 (12.7 examples/sec; 0.629 sec/batch; 54h:29m:04s remains)
INFO - root - 2017-12-06 09:56:44.700656: step 20480, loss = 0.82, batch loss = 0.75 (12.9 examples/sec; 0.619 sec/batch; 53h:39m:24s remains)
INFO - root - 2017-12-06 09:56:50.731187: step 20490, loss = 1.03, batch loss = 0.96 (13.1 examples/sec; 0.612 sec/batch; 53h:01m:15s remains)
INFO - root - 2017-12-06 09:56:56.891684: step 20500, loss = 0.85, batch loss = 0.78 (13.2 examples/sec; 0.606 sec/batch; 52h:31m:19s remains)
2017-12-06 09:56:57.445316: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3383403 -4.7885494 -5.1714511 -5.6697674 -6.2179136 -6.5293989 -6.3552952 -5.7815509 -5.3168888 -5.292923 -5.2299666 -5.2444053 -5.1544452 -4.898016 -4.8084764][-3.9748516 -4.5876293 -5.1789203 -5.717062 -6.1330385 -6.3709373 -6.2184219 -5.8367429 -5.5228381 -5.5535727 -5.4459829 -5.2122922 -4.822917 -4.4185691 -4.3220887][-3.4896641 -4.0903239 -4.7827764 -5.319046 -5.6133871 -5.7482858 -5.5770741 -5.4159756 -5.4086294 -5.6226749 -5.5887141 -5.1920891 -4.5965486 -4.1074533 -4.0487466][-3.142159 -3.5192096 -4.078053 -4.5151997 -4.6576285 -4.6140037 -4.2977791 -4.2470655 -4.5807271 -5.0719547 -5.2139511 -4.8045959 -4.1510849 -3.6591568 -3.6666331][-2.9488931 -3.0032196 -3.2567043 -3.4896016 -3.3909726 -3.0436673 -2.4256194 -2.3332934 -3.0019803 -3.8687072 -4.2965655 -4.0376768 -3.4485981 -2.9795756 -3.0233335][-2.971633 -2.7312765 -2.6672163 -2.63657 -2.1789422 -1.3577647 -0.22407055 0.14956284 -0.77638769 -2.1380363 -3.0689006 -3.2160306 -2.8380835 -2.3638372 -2.3355787][-3.2759581 -2.8556867 -2.5685225 -2.2540658 -1.4083648 -0.089474678 1.5631413 2.3048382 1.2263017 -0.61059308 -2.0684662 -2.6378064 -2.4106748 -1.823911 -1.6078494][-3.7793083 -3.3506093 -2.9669762 -2.4058585 -1.289515 0.23537683 2.0126958 2.929522 1.893621 -0.091827393 -1.7518988 -2.4319406 -2.1435506 -1.4149206 -1.0199907][-4.10449 -3.8683324 -3.5867944 -2.9707208 -1.8990471 -0.59660149 0.825789 1.6518435 0.94071245 -0.70797229 -2.17455 -2.6987262 -2.3203814 -1.6402161 -1.2105374][-3.9200783 -4.011384 -3.9828446 -3.5508769 -2.8315189 -2.0080607 -1.0894148 -0.43600702 -0.79325342 -1.8893576 -2.9943681 -3.3423676 -3.0299282 -2.6133952 -2.259387][-3.51101 -3.8535686 -4.0447826 -3.9511011 -3.7667868 -3.473093 -2.9747758 -2.4905953 -2.5819039 -3.1473234 -3.8421361 -4.0341167 -3.8628476 -3.7255683 -3.468776][-3.1463203 -3.5063348 -3.7837169 -4.0337629 -4.3799539 -4.5454936 -4.35237 -4.023592 -3.9607444 -4.1358805 -4.4841447 -4.6045637 -4.5975447 -4.6242623 -4.4334621][-2.8398352 -3.0570302 -3.3171334 -3.7727389 -4.4476938 -4.8739753 -4.8525333 -4.6554255 -4.5605226 -4.5707378 -4.7548075 -4.9364381 -5.1040196 -5.1862082 -5.030087][-2.6649897 -2.7543364 -2.9765868 -3.4088778 -4.063252 -4.452528 -4.458456 -4.3911166 -4.3997927 -4.489768 -4.6991382 -4.9782028 -5.2235618 -5.2795367 -5.1922479][-2.8244987 -2.8177428 -2.9413075 -3.1479337 -3.5233264 -3.6756222 -3.5716319 -3.5695341 -3.7458787 -4.0351844 -4.3522897 -4.6908741 -4.9372158 -4.9897614 -5.0405374]]...]
INFO - root - 2017-12-06 09:57:03.497729: step 20510, loss = 1.17, batch loss = 1.10 (13.3 examples/sec; 0.601 sec/batch; 52h:05m:03s remains)
INFO - root - 2017-12-06 09:57:09.532771: step 20520, loss = 0.63, batch loss = 0.56 (13.2 examples/sec; 0.605 sec/batch; 52h:23m:56s remains)
INFO - root - 2017-12-06 09:57:15.566901: step 20530, loss = 0.85, batch loss = 0.78 (12.9 examples/sec; 0.619 sec/batch; 53h:36m:44s remains)
INFO - root - 2017-12-06 09:57:21.492956: step 20540, loss = 0.91, batch loss = 0.84 (13.3 examples/sec; 0.601 sec/batch; 52h:02m:59s remains)
INFO - root - 2017-12-06 09:57:27.619841: step 20550, loss = 0.80, batch loss = 0.73 (13.3 examples/sec; 0.601 sec/batch; 52h:06m:51s remains)
INFO - root - 2017-12-06 09:57:33.693546: step 20560, loss = 0.96, batch loss = 0.89 (12.9 examples/sec; 0.619 sec/batch; 53h:37m:21s remains)
INFO - root - 2017-12-06 09:57:39.779758: step 20570, loss = 0.77, batch loss = 0.70 (13.9 examples/sec; 0.576 sec/batch; 49h:53m:34s remains)
INFO - root - 2017-12-06 09:57:45.737965: step 20580, loss = 1.14, batch loss = 1.07 (13.0 examples/sec; 0.615 sec/batch; 53h:15m:43s remains)
INFO - root - 2017-12-06 09:57:51.813636: step 20590, loss = 1.11, batch loss = 1.04 (13.7 examples/sec; 0.583 sec/batch; 50h:33m:01s remains)
INFO - root - 2017-12-06 09:57:57.870840: step 20600, loss = 1.17, batch loss = 1.10 (13.3 examples/sec; 0.602 sec/batch; 52h:08m:05s remains)
2017-12-06 09:57:58.456129: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2428908 -4.2476125 -4.4279785 -4.7807012 -5.0284638 -5.0066304 -4.7559423 -4.4139209 -4.2616587 -4.388329 -4.56304 -4.5123143 -4.1692338 -3.7899761 -3.693258][-4.407145 -4.401907 -4.6929855 -5.2314582 -5.6536646 -5.656992 -5.2308846 -4.5876508 -4.1747928 -4.2186351 -4.4640689 -4.477407 -4.12656 -3.71865 -3.6026249][-3.6359432 -3.5705881 -4.0591869 -4.9315519 -5.6732569 -5.8255334 -5.330596 -4.5215411 -4.0060215 -4.113688 -4.5421324 -4.6687388 -4.2743077 -3.7392824 -3.4697227][-2.2468178 -2.0625391 -2.7356825 -3.9399366 -4.995965 -5.2978873 -4.7577271 -3.8802483 -3.4355564 -3.7891593 -4.5391617 -4.8767347 -4.4891529 -3.808888 -3.2980518][-0.84199667 -0.46651912 -1.2053683 -2.6152806 -3.8426905 -4.1711287 -3.51797 -2.5781438 -2.29222 -3.0010791 -4.1563716 -4.8042707 -4.5544662 -3.8370075 -3.1193643][0.031649113 0.53265715 -0.20294666 -1.6434617 -2.8140197 -2.9574933 -2.0293708 -0.926069 -0.76513839 -1.8195872 -3.3649836 -4.3606038 -4.3667006 -3.7652876 -2.995729][-0.011117458 0.48118162 -0.17972374 -1.4227698 -2.2772458 -2.0499787 -0.79117489 0.4637928 0.51058483 -0.79368305 -2.591224 -3.8286681 -4.0725365 -3.6615472 -3.0009294][-1.0463407 -0.64259934 -1.105361 -1.9355295 -2.3159719 -1.7236116 -0.32728672 0.8451333 0.7109704 -0.68268824 -2.4450159 -3.6441386 -3.9421253 -3.6780519 -3.2010987][-2.4294763 -2.1063075 -2.3334475 -2.7437928 -2.7309685 -1.9941857 -0.79964304 0.020953178 -0.29517889 -1.539115 -2.9447169 -3.8297117 -4.0109839 -3.8312883 -3.5305004][-3.3516672 -3.1093066 -3.2097907 -3.3573017 -3.1634698 -2.5338039 -1.7554581 -1.3608849 -1.7869647 -2.7657819 -3.7092187 -4.2035165 -4.2128959 -4.05023 -3.8483915][-3.6373954 -3.4904089 -3.5896897 -3.6640995 -3.4715874 -3.0285048 -2.6127808 -2.5319145 -2.9770415 -3.699302 -4.2952852 -4.5423322 -4.4415359 -4.2467971 -4.064692][-3.6488104 -3.5888293 -3.7304389 -3.8166552 -3.6721058 -3.3363347 -3.0850642 -3.1401699 -3.5408621 -4.083818 -4.5221591 -4.7242317 -4.6285181 -4.4150252 -4.2325158][-3.6605659 -3.6545305 -3.8331795 -3.9646654 -3.8716722 -3.5614288 -3.308006 -3.3474591 -3.6572366 -4.0725374 -4.4671755 -4.7473044 -4.7649961 -4.619206 -4.4862971][-3.61822 -3.6090953 -3.8261993 -4.0509319 -4.0619845 -3.8128884 -3.545387 -3.5062695 -3.6759646 -3.9447377 -4.2961416 -4.6663785 -4.8556738 -4.8668437 -4.8320112][-3.5307622 -3.4713781 -3.7021625 -4.01648 -4.157269 -4.016952 -3.7738287 -3.6504455 -3.6678472 -3.7804246 -4.0410995 -4.4485545 -4.80616 -4.9985495 -5.0567822]]...]
INFO - root - 2017-12-06 09:58:04.543377: step 20610, loss = 1.05, batch loss = 0.98 (13.4 examples/sec; 0.597 sec/batch; 51h:45m:36s remains)
INFO - root - 2017-12-06 09:58:10.534346: step 20620, loss = 0.91, batch loss = 0.84 (13.4 examples/sec; 0.596 sec/batch; 51h:39m:54s remains)
INFO - root - 2017-12-06 09:58:16.694864: step 20630, loss = 0.95, batch loss = 0.88 (12.8 examples/sec; 0.626 sec/batch; 54h:11m:16s remains)
INFO - root - 2017-12-06 09:58:22.812861: step 20640, loss = 0.84, batch loss = 0.77 (13.9 examples/sec; 0.576 sec/batch; 49h:51m:44s remains)
INFO - root - 2017-12-06 09:58:28.828531: step 20650, loss = 0.94, batch loss = 0.87 (12.7 examples/sec; 0.627 sec/batch; 54h:21m:22s remains)
INFO - root - 2017-12-06 09:58:34.916337: step 20660, loss = 0.84, batch loss = 0.77 (13.4 examples/sec; 0.597 sec/batch; 51h:43m:20s remains)
INFO - root - 2017-12-06 09:58:41.032529: step 20670, loss = 0.87, batch loss = 0.80 (13.1 examples/sec; 0.612 sec/batch; 53h:02m:16s remains)
INFO - root - 2017-12-06 09:58:47.151763: step 20680, loss = 1.05, batch loss = 0.98 (12.8 examples/sec; 0.626 sec/batch; 54h:15m:45s remains)
INFO - root - 2017-12-06 09:58:53.286962: step 20690, loss = 0.99, batch loss = 0.92 (13.5 examples/sec; 0.591 sec/batch; 51h:13m:51s remains)
INFO - root - 2017-12-06 09:58:59.414441: step 20700, loss = 0.89, batch loss = 0.82 (12.9 examples/sec; 0.618 sec/batch; 53h:33m:24s remains)
2017-12-06 09:59:00.056705: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3631659 -3.2376537 -3.22478 -3.3202956 -3.4518259 -3.5928454 -3.7958813 -4.0468287 -4.2810316 -4.4329882 -4.4447556 -4.3573241 -4.2792907 -4.2898035 -4.4157834][-3.9079201 -3.7549367 -3.6796927 -3.7351577 -3.8215604 -3.9371502 -4.1599197 -4.4597025 -4.7797427 -4.9957976 -5.0191765 -4.9114809 -4.777813 -4.6721039 -4.6240768][-4.3410368 -4.089963 -3.8912542 -3.8556886 -3.8606529 -3.9150424 -4.1601491 -4.5290232 -4.9696584 -5.2995648 -5.370193 -5.2303648 -4.9993629 -4.7532964 -4.5525265][-4.4532313 -4.0822511 -3.7403667 -3.5828137 -3.4803765 -3.4349835 -3.6703072 -4.1248426 -4.742094 -5.2603583 -5.4300141 -5.2727728 -4.9554577 -4.6084137 -4.3235097][-4.2923145 -3.8128469 -3.3120575 -2.9811716 -2.7290583 -2.5140908 -2.6555874 -3.2082579 -4.0512767 -4.8140497 -5.1598749 -5.0568662 -4.7415986 -4.40566 -4.1343703][-4.0178833 -3.5208285 -2.9370093 -2.449064 -2.0115905 -1.5562024 -1.4873667 -2.0687702 -3.0996585 -4.0555792 -4.6124969 -4.6540251 -4.4503837 -4.2429671 -4.0678606][-3.8567181 -3.5082386 -3.0060992 -2.4523935 -1.8382626 -1.1095145 -0.71968532 -1.1813357 -2.2419202 -3.2700481 -4.0230789 -4.2618814 -4.2167673 -4.1918874 -4.159308][-3.9656963 -3.8884985 -3.6133928 -3.1259089 -2.4167316 -1.4773548 -0.75376487 -0.95688152 -1.8692458 -2.8463807 -3.7230611 -4.1451826 -4.2478032 -4.3695683 -4.4333086][-4.3044071 -4.4851685 -4.4737849 -4.1437974 -3.4622502 -2.4883473 -1.6130209 -1.5714226 -2.244904 -3.0626507 -3.9158697 -4.4214616 -4.6084762 -4.7703876 -4.8086753][-4.6888542 -5.0364208 -5.2340779 -5.0922146 -4.545332 -3.7113094 -2.9141438 -2.7528155 -3.2103403 -3.8294392 -4.5409579 -5.0055118 -5.1647062 -5.2150245 -5.09642][-4.8927517 -5.3299885 -5.6650085 -5.6939635 -5.3431487 -4.7504497 -4.1671872 -3.9958968 -4.2875104 -4.7296114 -5.2582722 -5.5869269 -5.6081171 -5.4597092 -5.1656971][-4.809536 -5.2557988 -5.6412916 -5.7842736 -5.6454988 -5.325541 -4.9848008 -4.8603082 -5.02287 -5.3103495 -5.6697145 -5.8647637 -5.7634277 -5.4757361 -5.1018796][-4.521946 -4.89328 -5.2520924 -5.43083 -5.4344883 -5.3065114 -5.1482739 -5.0830879 -5.1614189 -5.3318386 -5.5739269 -5.7206173 -5.6325397 -5.3775363 -5.0683022][-4.2142105 -4.4460073 -4.7236304 -4.8790169 -4.9436474 -4.9126735 -4.8621988 -4.8538475 -4.8803587 -4.9559207 -5.1193371 -5.2831683 -5.3019423 -5.1744046 -4.9880195][-4.0209222 -4.0966406 -4.2770181 -4.3874178 -4.4611974 -4.4703994 -4.469676 -4.4877486 -4.4752564 -4.4763632 -4.5732331 -4.7362185 -4.8332224 -4.8141642 -4.7337236]]...]
INFO - root - 2017-12-06 09:59:06.175213: step 20710, loss = 0.99, batch loss = 0.92 (12.8 examples/sec; 0.627 sec/batch; 54h:15m:45s remains)
INFO - root - 2017-12-06 09:59:12.215460: step 20720, loss = 0.74, batch loss = 0.67 (13.4 examples/sec; 0.595 sec/batch; 51h:31m:55s remains)
INFO - root - 2017-12-06 09:59:18.139454: step 20730, loss = 0.90, batch loss = 0.83 (13.1 examples/sec; 0.609 sec/batch; 52h:46m:40s remains)
INFO - root - 2017-12-06 09:59:24.264691: step 20740, loss = 1.04, batch loss = 0.97 (13.3 examples/sec; 0.603 sec/batch; 52h:10m:42s remains)
INFO - root - 2017-12-06 09:59:30.211621: step 20750, loss = 1.05, batch loss = 0.98 (13.2 examples/sec; 0.607 sec/batch; 52h:32m:37s remains)
INFO - root - 2017-12-06 09:59:36.261062: step 20760, loss = 0.76, batch loss = 0.69 (13.2 examples/sec; 0.606 sec/batch; 52h:30m:25s remains)
INFO - root - 2017-12-06 09:59:42.289708: step 20770, loss = 0.88, batch loss = 0.81 (13.5 examples/sec; 0.594 sec/batch; 51h:26m:27s remains)
INFO - root - 2017-12-06 09:59:48.407877: step 20780, loss = 1.01, batch loss = 0.94 (13.1 examples/sec; 0.611 sec/batch; 52h:54m:32s remains)
INFO - root - 2017-12-06 09:59:54.609122: step 20790, loss = 1.01, batch loss = 0.94 (12.8 examples/sec; 0.623 sec/batch; 53h:59m:02s remains)
INFO - root - 2017-12-06 10:00:00.717601: step 20800, loss = 0.97, batch loss = 0.90 (13.5 examples/sec; 0.593 sec/batch; 51h:21m:33s remains)
2017-12-06 10:00:01.302292: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1613059 -4.2834311 -4.3405437 -4.3424788 -4.3201671 -4.2965493 -4.29241 -4.3386612 -4.4308543 -4.5172553 -4.5292315 -4.4536653 -4.3349962 -4.2103367 -4.1217232][-4.4660425 -4.6232767 -4.7075434 -4.7338452 -4.6971564 -4.6108742 -4.5272431 -4.537118 -4.6679688 -4.8210635 -4.8691483 -4.7806082 -4.6196275 -4.4347205 -4.2759948][-4.4987788 -4.6752267 -4.7760053 -4.8290939 -4.7676158 -4.5812736 -4.3712115 -4.320365 -4.519238 -4.8039794 -4.9629488 -4.9347534 -4.8062444 -4.6238484 -4.4173541][-4.0408654 -4.2208881 -4.3391552 -4.4274039 -4.3320947 -4.0117769 -3.6282887 -3.4809029 -3.754137 -4.2188759 -4.5709162 -4.6965222 -4.6934829 -4.6006794 -4.3931971][-3.1729667 -3.3395963 -3.4824839 -3.6184547 -3.4938302 -3.0290937 -2.4497256 -2.1801643 -2.5017719 -3.1360178 -3.7110677 -4.0399351 -4.211349 -4.2605686 -4.0940623][-2.3262205 -2.4607701 -2.6264918 -2.8212266 -2.6933932 -2.1134095 -1.3559983 -0.95787072 -1.2676463 -1.981622 -2.7136438 -3.2090898 -3.5237145 -3.6986487 -3.5840681][-2.0679936 -2.1519675 -2.3109915 -2.5381429 -2.4195833 -1.7712772 -0.8824482 -0.36579561 -0.59240627 -1.2597938 -2.0260477 -2.6005297 -2.990406 -3.2346587 -3.1583645][-2.551631 -2.59762 -2.7251716 -2.9436662 -2.842149 -2.1966503 -1.2633128 -0.67020774 -0.76186275 -1.2667668 -1.9384172 -2.4970045 -2.8940434 -3.1432996 -3.0834637][-3.3939295 -3.4440596 -3.5436287 -3.7290375 -3.6631973 -3.1156015 -2.267488 -1.6805804 -1.644805 -1.9414549 -2.4333086 -2.8870349 -3.2195666 -3.4122095 -3.348207][-4.0861659 -4.1668425 -4.2553329 -4.4120746 -4.4112225 -4.0431166 -3.4017105 -2.9187489 -2.8081827 -2.9183893 -3.1948929 -3.4774227 -3.6854446 -3.7833474 -3.7051554][-4.354259 -4.4639916 -4.5589824 -4.7053871 -4.779716 -4.6130853 -4.2286363 -3.9050283 -3.7787387 -3.7572408 -3.8357153 -3.9364748 -4.0095692 -4.0177774 -3.9296143][-4.2670164 -4.3818364 -4.4783368 -4.6120772 -4.7302551 -4.7137017 -4.5460949 -4.3752823 -4.27531 -4.197103 -4.1570206 -4.133986 -4.112956 -4.0680881 -3.9854152][-4.0490966 -4.1439672 -4.2250829 -4.3326273 -4.45347 -4.5109053 -4.478704 -4.4195261 -4.3626318 -4.2845187 -4.2070651 -4.13603 -4.077867 -4.02165 -3.9613907][-3.9024477 -3.9644966 -4.0168195 -4.0854683 -4.176753 -4.2448025 -4.2689748 -4.2691836 -4.2523026 -4.2043819 -4.1404819 -4.0738115 -4.01844 -3.9748 -3.9382966][-3.8975735 -3.9319751 -3.9591825 -3.9948351 -4.0525188 -4.1019306 -4.1291027 -4.140862 -4.1387353 -4.1150045 -4.0753508 -4.0308127 -3.9912705 -3.960042 -3.9331141]]...]
INFO - root - 2017-12-06 10:00:07.446030: step 20810, loss = 0.93, batch loss = 0.86 (13.3 examples/sec; 0.602 sec/batch; 52h:09m:38s remains)
INFO - root - 2017-12-06 10:00:13.468511: step 20820, loss = 0.72, batch loss = 0.65 (13.4 examples/sec; 0.595 sec/batch; 51h:32m:20s remains)
INFO - root - 2017-12-06 10:00:19.579157: step 20830, loss = 0.78, batch loss = 0.71 (13.1 examples/sec; 0.611 sec/batch; 52h:54m:46s remains)
INFO - root - 2017-12-06 10:00:25.627309: step 20840, loss = 0.84, batch loss = 0.77 (12.9 examples/sec; 0.619 sec/batch; 53h:37m:42s remains)
INFO - root - 2017-12-06 10:00:31.825616: step 20850, loss = 0.85, batch loss = 0.78 (13.3 examples/sec; 0.604 sec/batch; 52h:15m:06s remains)
INFO - root - 2017-12-06 10:00:37.758285: step 20860, loss = 1.04, batch loss = 0.97 (13.7 examples/sec; 0.585 sec/batch; 50h:37m:07s remains)
INFO - root - 2017-12-06 10:00:43.859444: step 20870, loss = 0.63, batch loss = 0.56 (13.5 examples/sec; 0.595 sec/batch; 51h:27m:48s remains)
INFO - root - 2017-12-06 10:00:49.835349: step 20880, loss = 0.77, batch loss = 0.70 (12.8 examples/sec; 0.624 sec/batch; 54h:01m:31s remains)
INFO - root - 2017-12-06 10:00:55.918831: step 20890, loss = 0.86, batch loss = 0.79 (12.9 examples/sec; 0.622 sec/batch; 53h:48m:44s remains)
INFO - root - 2017-12-06 10:01:02.062646: step 20900, loss = 0.85, batch loss = 0.78 (12.9 examples/sec; 0.622 sec/batch; 53h:49m:40s remains)
2017-12-06 10:01:02.678885: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9832234 -4.0860205 -4.2105837 -4.30894 -4.3530879 -4.3264461 -4.2139945 -4.0311537 -3.8230944 -3.6872504 -3.6645081 -3.7248316 -3.8196235 -3.9027972 -3.9552603][-4.004426 -4.1096125 -4.2408957 -4.3642011 -4.4491677 -4.4668193 -4.3755875 -4.1713805 -3.9092855 -3.7102618 -3.6473036 -3.7033577 -3.8261259 -3.9434 -4.0111837][-4.0178971 -4.1312666 -4.2763643 -4.4348569 -4.5686851 -4.6282234 -4.54073 -4.2881 -3.9540207 -3.6871047 -3.5901821 -3.6544433 -3.8142159 -3.9691114 -4.0512323][-4.0286837 -4.1597595 -4.3302207 -4.5319595 -4.708137 -4.7791262 -4.6478367 -4.3037286 -3.8817432 -3.5547137 -3.4425669 -3.5385137 -3.7540443 -3.9606988 -4.0655422][-4.0434251 -4.1987424 -4.4016433 -4.642674 -4.8341875 -4.8615193 -4.6247635 -4.1542473 -3.6498356 -3.2888918 -3.1927543 -3.3505254 -3.6462355 -3.9231455 -4.0652056][-4.0722327 -4.2544022 -4.4841986 -4.7356334 -4.8871346 -4.8042426 -4.4130564 -3.8125494 -3.2559094 -2.9024839 -2.8663564 -3.1237988 -3.5254388 -3.8859429 -4.07879][-4.1264176 -4.3322535 -4.5669956 -4.7807183 -4.8297262 -4.5863643 -4.0266886 -3.3271644 -2.7638149 -2.4669991 -2.5388002 -2.9287014 -3.4501302 -3.8920693 -4.1356173][-4.2007918 -4.4161444 -4.625895 -4.763124 -4.6811738 -4.27286 -3.5713813 -2.8198843 -2.2950907 -2.1024406 -2.3148229 -2.843575 -3.4684248 -3.9673841 -4.238924][-4.2764683 -4.4857755 -4.6551313 -4.7141128 -4.5265656 -3.9958854 -3.2012033 -2.4374418 -1.9772553 -1.9160848 -2.2723978 -2.9080396 -3.5832756 -4.089026 -4.3458967][-4.3195581 -4.5131087 -4.6485319 -4.6590295 -4.4239659 -3.8402934 -3.0142705 -2.2705941 -1.8887522 -1.9671504 -2.4429679 -3.1263797 -3.775032 -4.2131767 -4.3969769][-4.3105979 -4.489233 -4.615303 -4.6263819 -4.4131789 -3.8534563 -3.0669789 -2.3868558 -2.0951786 -2.2902179 -2.8236375 -3.4760203 -4.0160675 -4.3164015 -4.377212][-4.2587552 -4.4317079 -4.5754251 -4.6314635 -4.4954782 -4.0239053 -3.3432679 -2.7690778 -2.5664871 -2.8196671 -3.322067 -3.8649573 -4.2397933 -4.3633962 -4.2776408][-4.1913495 -4.3661151 -4.5397453 -4.6574006 -4.6200719 -4.2773981 -3.7493343 -3.3017983 -3.1593375 -3.3897471 -3.7804108 -4.1629252 -4.360343 -4.3180304 -4.1067467][-4.1288056 -4.3027539 -4.4997997 -4.66758 -4.7174063 -4.5105171 -4.1449971 -3.817528 -3.6949935 -3.8354261 -4.0691352 -4.2852259 -4.3370385 -4.1839232 -3.9099908][-4.0812635 -4.2399988 -4.4394383 -4.6307292 -4.7379518 -4.6456752 -4.4185963 -4.1833472 -4.0471668 -4.0619497 -4.1330256 -4.2071753 -4.1667809 -3.9768627 -3.7194943]]...]
INFO - root - 2017-12-06 10:01:08.777438: step 20910, loss = 0.78, batch loss = 0.71 (12.9 examples/sec; 0.622 sec/batch; 53h:52m:41s remains)
INFO - root - 2017-12-06 10:01:14.829207: step 20920, loss = 0.97, batch loss = 0.90 (13.4 examples/sec; 0.596 sec/batch; 51h:37m:28s remains)
INFO - root - 2017-12-06 10:01:20.874454: step 20930, loss = 1.00, batch loss = 0.93 (13.4 examples/sec; 0.597 sec/batch; 51h:40m:25s remains)
INFO - root - 2017-12-06 10:01:26.987025: step 20940, loss = 1.02, batch loss = 0.95 (13.1 examples/sec; 0.612 sec/batch; 52h:56m:29s remains)
INFO - root - 2017-12-06 10:01:33.133160: step 20950, loss = 0.95, batch loss = 0.88 (13.2 examples/sec; 0.608 sec/batch; 52h:37m:56s remains)
INFO - root - 2017-12-06 10:01:39.206926: step 20960, loss = 1.31, batch loss = 1.24 (13.3 examples/sec; 0.603 sec/batch; 52h:08m:53s remains)
INFO - root - 2017-12-06 10:01:45.228971: step 20970, loss = 0.90, batch loss = 0.83 (12.9 examples/sec; 0.622 sec/batch; 53h:47m:24s remains)
INFO - root - 2017-12-06 10:01:51.310730: step 20980, loss = 1.18, batch loss = 1.11 (13.3 examples/sec; 0.603 sec/batch; 52h:11m:33s remains)
INFO - root - 2017-12-06 10:01:57.396120: step 20990, loss = 1.08, batch loss = 1.01 (12.9 examples/sec; 0.619 sec/batch; 53h:31m:12s remains)
INFO - root - 2017-12-06 10:02:03.453174: step 21000, loss = 1.09, batch loss = 1.02 (13.4 examples/sec; 0.596 sec/batch; 51h:35m:10s remains)
2017-12-06 10:02:04.122654: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7043638 -4.6042485 -4.6380653 -4.7416272 -4.8385634 -4.850174 -4.7509861 -4.5778117 -4.4185262 -4.4018688 -4.53352 -4.70489 -4.8053722 -4.8013 -4.7749839][-5.0525465 -5.0553794 -5.1292253 -5.17711 -5.1450181 -4.9617643 -4.6247792 -4.2488666 -4.0174708 -4.0768805 -4.3594828 -4.665307 -4.8571382 -4.8787875 -4.8390713][-5.1994104 -5.3526521 -5.5302835 -5.5778055 -5.4412541 -5.0713849 -4.4928865 -3.8971114 -3.5841668 -3.7219307 -4.1596985 -4.6305413 -4.9479022 -5.0114212 -4.9419789][-4.928093 -5.2178378 -5.5310864 -5.6488309 -5.4798183 -4.9948888 -4.2336841 -3.4549103 -3.0804539 -3.3306324 -3.9569941 -4.5994315 -5.0153379 -5.0982318 -4.9884057][-4.284348 -4.6401248 -5.0556393 -5.2524557 -5.0848575 -4.5341125 -3.6278825 -2.6865461 -2.2645671 -2.687593 -3.5769541 -4.4296336 -4.950119 -5.0617843 -4.9218578][-3.4897168 -3.8174858 -4.2521315 -4.4837537 -4.3131027 -3.7142916 -2.6726341 -1.543174 -1.0239012 -1.6239021 -2.8348622 -3.9613097 -4.6475711 -4.8642569 -4.7627664][-2.8281934 -3.0546145 -3.4349103 -3.6620948 -3.4975338 -2.8826351 -1.719404 -0.35525608 0.37282515 -0.28229713 -1.7292552 -3.0740027 -3.9337878 -4.3255525 -4.357697][-2.5426035 -2.6652157 -2.9562511 -3.1693678 -3.0579178 -2.509129 -1.3251717 0.20902634 1.2006874 0.684495 -0.73403788 -2.0663888 -2.9745948 -3.4998219 -3.6762674][-2.6953242 -2.7871542 -3.0193472 -3.24761 -3.2609797 -2.8999238 -1.8901138 -0.42230082 0.69115973 0.48541784 -0.59248495 -1.6196659 -2.3429992 -2.805697 -2.9942722][-3.0898817 -3.2292082 -3.4439206 -3.7142215 -3.8889384 -3.7677832 -3.0721736 -1.9266257 -0.93291759 -0.86391973 -1.4871924 -2.0721946 -2.4632246 -2.6843982 -2.7220201][-3.443424 -3.6468091 -3.8490367 -4.1335516 -4.4067907 -4.4309621 -4.0104027 -3.261493 -2.5673242 -2.4525282 -2.7936227 -3.08526 -3.2327518 -3.2297907 -3.0877485][-3.691277 -3.8584394 -3.9551725 -4.1546626 -4.396956 -4.41873 -4.13395 -3.6790519 -3.3057113 -3.3279853 -3.6279025 -3.9047594 -4.0665226 -4.0419908 -3.85269][-3.885649 -3.8601079 -3.7002151 -3.7012057 -3.8196418 -3.7673118 -3.5231986 -3.2213364 -3.0501568 -3.2350109 -3.6531019 -4.1307421 -4.5056581 -4.6352792 -4.5313616][-4.0780649 -3.8478248 -3.4226549 -3.2165194 -3.2000408 -3.071919 -2.8111057 -2.5351434 -2.4081492 -2.6826923 -3.249392 -3.9669087 -4.5711284 -4.8693666 -4.877584][-4.4443765 -4.1403847 -3.6128194 -3.3304558 -3.2576079 -3.0793781 -2.766634 -2.4345133 -2.2206366 -2.4341516 -3.0149159 -3.8243961 -4.5248919 -4.8971806 -4.9569697]]...]
INFO - root - 2017-12-06 10:02:10.194178: step 21010, loss = 0.82, batch loss = 0.75 (13.5 examples/sec; 0.591 sec/batch; 51h:06m:00s remains)
INFO - root - 2017-12-06 10:02:16.244606: step 21020, loss = 1.09, batch loss = 1.02 (13.2 examples/sec; 0.605 sec/batch; 52h:18m:38s remains)
INFO - root - 2017-12-06 10:02:22.265284: step 21030, loss = 0.66, batch loss = 0.59 (13.2 examples/sec; 0.606 sec/batch; 52h:28m:11s remains)
INFO - root - 2017-12-06 10:02:28.338288: step 21040, loss = 0.98, batch loss = 0.91 (13.2 examples/sec; 0.606 sec/batch; 52h:27m:50s remains)
INFO - root - 2017-12-06 10:02:34.338809: step 21050, loss = 0.81, batch loss = 0.74 (13.8 examples/sec; 0.582 sec/batch; 50h:19m:08s remains)
INFO - root - 2017-12-06 10:02:40.427846: step 21060, loss = 1.06, batch loss = 0.99 (13.0 examples/sec; 0.615 sec/batch; 53h:10m:49s remains)
INFO - root - 2017-12-06 10:02:46.301472: step 21070, loss = 1.14, batch loss = 1.07 (13.7 examples/sec; 0.585 sec/batch; 50h:37m:12s remains)
INFO - root - 2017-12-06 10:02:52.415025: step 21080, loss = 1.02, batch loss = 0.95 (13.2 examples/sec; 0.608 sec/batch; 52h:36m:53s remains)
INFO - root - 2017-12-06 10:02:58.487015: step 21090, loss = 1.21, batch loss = 1.14 (13.2 examples/sec; 0.606 sec/batch; 52h:24m:48s remains)
INFO - root - 2017-12-06 10:03:04.507714: step 21100, loss = 0.95, batch loss = 0.88 (13.3 examples/sec; 0.602 sec/batch; 52h:03m:16s remains)
2017-12-06 10:03:05.033674: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8103547 -4.7208843 -4.7118607 -4.770052 -4.8407636 -4.8659797 -4.8200645 -4.7334905 -4.6906853 -4.7759528 -4.9450502 -5.0960279 -5.2087469 -5.3247581 -5.4191723][-5.3626132 -5.2401552 -5.2479291 -5.3413324 -5.3935676 -5.3374214 -5.1745653 -4.969346 -4.8675504 -5.01346 -5.3175621 -5.6077852 -5.8633451 -6.0942655 -6.2097597][-5.9333696 -5.7941909 -5.8080883 -5.9108539 -5.8780355 -5.6400743 -5.2603149 -4.8446631 -4.6260877 -4.8417826 -5.332962 -5.8269968 -6.297821 -6.6799359 -6.797441][-6.2374635 -6.1093607 -6.1147442 -6.1898875 -6.0084581 -5.5088654 -4.8305311 -4.1194792 -3.7231007 -4.00364 -4.7162323 -5.456933 -6.1814427 -6.7459555 -6.895422][-6.100019 -6.021235 -6.035511 -6.0802588 -5.732378 -4.933322 -3.8980424 -2.8296628 -2.2206712 -2.5843987 -3.55423 -4.557466 -5.5235434 -6.2809114 -6.5311317][-5.6128287 -5.6656208 -5.7432337 -5.752799 -5.1937785 -4.0217175 -2.5355396 -1.0460985 -0.22397184 -0.72747135 -2.0038402 -3.2950759 -4.4774413 -5.4281774 -5.8793144][-5.0554447 -5.3347406 -5.5267196 -5.4685736 -4.64355 -3.0533233 -1.08728 0.77520514 1.70084 0.99687624 -0.55455756 -2.0528996 -3.3314745 -4.3985057 -5.1120467][-4.8486915 -5.3474231 -5.6385174 -5.4895444 -4.4495072 -2.5780232 -0.33976221 1.6264749 2.4375129 1.5432062 -0.11361933 -1.6055443 -2.7756829 -3.7837629 -4.6567812][-5.1004024 -5.7226415 -6.0678644 -5.8613362 -4.7908139 -2.9500949 -0.79973125 0.93106651 1.4431405 0.46416092 -1.0606418 -2.3176904 -3.1903751 -3.9545503 -4.7842183][-5.5151944 -6.1289248 -6.481698 -6.2981005 -5.4271979 -3.945704 -2.2038805 -0.90916085 -0.70388651 -1.6283009 -2.8381784 -3.7213597 -4.2193456 -4.6602292 -5.2955742][-5.5834427 -6.0827451 -6.4176135 -6.3472033 -5.8370914 -4.872776 -3.6373243 -2.7474041 -2.7303524 -3.5149477 -4.4034066 -4.9558868 -5.1501307 -5.30506 -5.693717][-5.0349522 -5.3743262 -5.6692705 -5.7390065 -5.5944071 -5.0975242 -4.295639 -3.7061651 -3.7608008 -4.3738689 -5.0058527 -5.3278213 -5.30618 -5.2451463 -5.4158154][-4.0966597 -4.2730932 -4.4961114 -4.661622 -4.7766767 -4.6182628 -4.1672192 -3.8337419 -3.9299915 -4.3831263 -4.7852311 -4.8895793 -4.6751 -4.4348292 -4.4363604][-3.0782661 -3.1270819 -3.2659295 -3.462811 -3.7112861 -3.7803421 -3.6190989 -3.5020931 -3.6165409 -3.907449 -4.0680013 -3.9311836 -3.5548792 -3.2197249 -3.1588426][-2.506381 -2.4913993 -2.5663996 -2.7464602 -3.0173817 -3.1900563 -3.2037878 -3.2009974 -3.288136 -3.4212372 -3.3695683 -3.0472627 -2.5943956 -2.254473 -2.1978333]]...]
INFO - root - 2017-12-06 10:03:11.069179: step 21110, loss = 0.94, batch loss = 0.87 (13.0 examples/sec; 0.615 sec/batch; 53h:12m:37s remains)
INFO - root - 2017-12-06 10:03:17.151591: step 21120, loss = 1.28, batch loss = 1.21 (12.9 examples/sec; 0.618 sec/batch; 53h:28m:10s remains)
INFO - root - 2017-12-06 10:03:23.215953: step 21130, loss = 0.99, batch loss = 0.92 (13.3 examples/sec; 0.601 sec/batch; 52h:00m:27s remains)
INFO - root - 2017-12-06 10:03:29.328021: step 21140, loss = 0.84, batch loss = 0.77 (12.9 examples/sec; 0.621 sec/batch; 53h:41m:11s remains)
INFO - root - 2017-12-06 10:03:35.549671: step 21150, loss = 1.23, batch loss = 1.16 (12.6 examples/sec; 0.636 sec/batch; 54h:59m:46s remains)
INFO - root - 2017-12-06 10:03:41.566146: step 21160, loss = 0.99, batch loss = 0.92 (12.7 examples/sec; 0.630 sec/batch; 54h:28m:06s remains)
INFO - root - 2017-12-06 10:03:47.613764: step 21170, loss = 0.78, batch loss = 0.71 (13.2 examples/sec; 0.606 sec/batch; 52h:23m:33s remains)
INFO - root - 2017-12-06 10:03:53.440922: step 21180, loss = 1.00, batch loss = 0.93 (13.1 examples/sec; 0.612 sec/batch; 52h:55m:17s remains)
INFO - root - 2017-12-06 10:03:59.504168: step 21190, loss = 0.91, batch loss = 0.84 (13.2 examples/sec; 0.605 sec/batch; 52h:19m:13s remains)
INFO - root - 2017-12-06 10:04:05.592884: step 21200, loss = 0.86, batch loss = 0.78 (12.8 examples/sec; 0.626 sec/batch; 54h:08m:53s remains)
2017-12-06 10:04:06.141593: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8574734 -4.0205679 -4.0860505 -3.9837093 -3.6510077 -3.2398913 -2.9979396 -3.0486684 -3.3367586 -3.5819929 -3.6614168 -3.888257 -4.3314571 -4.9193797 -5.0925455][-3.7850826 -3.8645656 -3.8500223 -3.7108469 -3.3919759 -3.0461435 -2.7821631 -2.6409342 -2.681627 -2.741096 -2.8009062 -3.2112775 -3.9041102 -4.6588826 -4.8788257][-3.8546257 -3.9066615 -3.8223624 -3.6233518 -3.2595043 -2.8909454 -2.5450022 -2.2004461 -2.0181112 -1.9491816 -2.0482454 -2.63879 -3.5138292 -4.3502254 -4.5693922][-4.0278153 -4.1075015 -3.9526088 -3.6374087 -3.1491241 -2.6651204 -2.2167523 -1.7965062 -1.5869904 -1.59653 -1.8361998 -2.5124421 -3.366919 -4.1040196 -4.2593532][-4.2235389 -4.363039 -4.137588 -3.6529088 -2.959235 -2.2727695 -1.719218 -1.3487499 -1.3364089 -1.667289 -2.1909223 -2.90198 -3.57489 -4.0433173 -4.0500679][-4.3937449 -4.6027184 -4.3238325 -3.6561291 -2.6916018 -1.7042847 -0.9560442 -0.61563754 -0.89590025 -1.7103198 -2.6723621 -3.5240781 -4.0421185 -4.1915588 -3.9782131][-4.5326405 -4.826129 -4.5702629 -3.8116183 -2.6131539 -1.2862089 -0.20872402 0.26355457 -0.19962406 -1.4491389 -2.8650231 -3.972569 -4.4935131 -4.4569321 -4.0861979][-4.6097803 -5.0043626 -4.89429 -4.2528992 -3.0661855 -1.6175177 -0.30193758 0.40445614 0.024593353 -1.3234293 -2.9272275 -4.1656551 -4.6840792 -4.5452394 -4.1278791][-4.5668054 -5.0524426 -5.161252 -4.8177605 -3.9285722 -2.6911905 -1.4039173 -0.55627728 -0.69464684 -1.8175213 -3.2796657 -4.3955507 -4.7748718 -4.5127168 -4.0885677][-4.4221826 -4.9598818 -5.2708082 -5.2601914 -4.7998266 -3.976675 -2.9324505 -2.0825973 -1.9647732 -2.7047853 -3.8385315 -4.7216663 -4.97824 -4.6834645 -4.3105407][-4.2255545 -4.7508736 -5.1752124 -5.3977704 -5.2959538 -4.87685 -4.1483135 -3.4014807 -3.0917082 -3.4347668 -4.1709595 -4.7763753 -4.9466772 -4.7147727 -4.4606972][-4.0315528 -4.4607983 -4.8569255 -5.1214423 -5.1438742 -4.9188147 -4.3977776 -3.7593105 -3.3749359 -3.4796219 -3.9533668 -4.39738 -4.5835996 -4.52945 -4.4862852][-3.9153328 -4.2108335 -4.4563169 -4.5778756 -4.4990969 -4.2256403 -3.7261853 -3.1139021 -2.715585 -2.7362351 -3.1213255 -3.5632236 -3.858273 -4.0391493 -4.2473569][-3.9057822 -4.1005177 -4.176178 -4.0958576 -3.8161778 -3.3649116 -2.7491632 -2.064554 -1.6451988 -1.6494658 -2.0435514 -2.5578485 -2.9561951 -3.2959256 -3.6428185][-3.9796069 -4.1519151 -4.1462722 -3.9472833 -3.5331132 -2.9269254 -2.1646492 -1.3707321 -0.91562343 -0.93685055 -1.3865151 -1.9869006 -2.4607077 -2.8940685 -3.287478]]...]
INFO - root - 2017-12-06 10:04:12.149268: step 21210, loss = 0.86, batch loss = 0.79 (13.8 examples/sec; 0.580 sec/batch; 50h:06m:37s remains)
INFO - root - 2017-12-06 10:04:18.134097: step 21220, loss = 0.68, batch loss = 0.61 (13.6 examples/sec; 0.587 sec/batch; 50h:46m:23s remains)
INFO - root - 2017-12-06 10:04:24.226780: step 21230, loss = 1.04, batch loss = 0.97 (13.6 examples/sec; 0.588 sec/batch; 50h:51m:46s remains)
INFO - root - 2017-12-06 10:04:30.239031: step 21240, loss = 0.95, batch loss = 0.88 (12.8 examples/sec; 0.625 sec/batch; 54h:03m:51s remains)
INFO - root - 2017-12-06 10:04:36.330583: step 21250, loss = 0.85, batch loss = 0.78 (13.2 examples/sec; 0.606 sec/batch; 52h:24m:33s remains)
INFO - root - 2017-12-06 10:04:42.387466: step 21260, loss = 0.86, batch loss = 0.79 (13.4 examples/sec; 0.596 sec/batch; 51h:33m:15s remains)
INFO - root - 2017-12-06 10:04:48.400105: step 21270, loss = 1.18, batch loss = 1.11 (13.4 examples/sec; 0.595 sec/batch; 51h:26m:31s remains)
INFO - root - 2017-12-06 10:04:54.525223: step 21280, loss = 1.17, batch loss = 1.10 (12.8 examples/sec; 0.626 sec/batch; 54h:09m:00s remains)
INFO - root - 2017-12-06 10:05:00.506122: step 21290, loss = 1.05, batch loss = 0.98 (13.0 examples/sec; 0.614 sec/batch; 53h:02m:27s remains)
INFO - root - 2017-12-06 10:05:06.522082: step 21300, loss = 0.89, batch loss = 0.82 (13.2 examples/sec; 0.604 sec/batch; 52h:14m:06s remains)
2017-12-06 10:05:07.102840: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.194036 -4.4171281 -4.9943027 -5.3522096 -5.0523453 -4.5652108 -4.3224816 -4.1004725 -3.9797463 -4.184967 -4.7061696 -5.1905508 -5.0111618 -4.3869247 -4.0104947][-4.0205507 -4.3834996 -4.9708872 -5.3040004 -4.9679422 -4.4326377 -4.0587206 -3.7454665 -3.6981177 -3.9487443 -4.4636283 -4.986743 -4.8554578 -4.2948518 -3.9697046][-3.6132505 -4.1433864 -4.666554 -4.8345828 -4.3188949 -3.6046057 -3.0390987 -2.6896255 -2.8230648 -3.2923539 -3.9914808 -4.6525655 -4.6475887 -4.1982307 -3.9324656][-3.5893781 -4.1715612 -4.5048466 -4.39416 -3.6525071 -2.7219291 -1.9208493 -1.4852405 -1.7434361 -2.4626539 -3.4252381 -4.2920241 -4.4721956 -4.15453 -3.9273908][-4.1957207 -4.6340108 -4.693697 -4.2974677 -3.3935041 -2.3971343 -1.4956586 -0.95331621 -1.1884589 -2.0079682 -3.1260223 -4.1286359 -4.4483395 -4.2089934 -3.9663393][-5.0336 -5.1533875 -4.9625578 -4.4180822 -3.5371261 -2.7353344 -1.9982059 -1.4338479 -1.5405266 -2.2426355 -3.2933521 -4.2333493 -4.5550566 -4.3158388 -4.0199256][-5.2732434 -4.9707952 -4.5930467 -4.0741162 -3.4248691 -3.0352802 -2.6651249 -2.2314162 -2.235971 -2.7749677 -3.6621413 -4.4255524 -4.6539536 -4.3811116 -4.0499668][-4.7510514 -4.1002688 -3.6236892 -3.2394857 -2.9238315 -2.9425132 -2.9138658 -2.6428323 -2.5925727 -3.052351 -3.8619223 -4.5036683 -4.6517849 -4.372447 -4.05091][-3.9206192 -3.1313968 -2.6964672 -2.5452163 -2.5609729 -2.8375278 -2.9702492 -2.7567134 -2.6302748 -3.0483294 -3.8554382 -4.4668627 -4.5878129 -4.3333287 -4.0426946][-3.3145771 -2.6405904 -2.3771076 -2.5056536 -2.7741823 -3.1294715 -3.2789605 -3.0502229 -2.8158956 -3.107868 -3.835006 -4.4115691 -4.5185452 -4.2882495 -4.0252504][-3.2506239 -2.8228283 -2.7759817 -3.1086955 -3.5061483 -3.87927 -4.0262847 -3.7925525 -3.4466648 -3.5142846 -4.0241189 -4.4763966 -4.5210085 -4.2657762 -4.0023828][-3.7288451 -3.5417037 -3.6158435 -3.9534323 -4.3490477 -4.7226477 -4.8723855 -4.650588 -4.2446117 -4.1159673 -4.3792362 -4.66562 -4.6139855 -4.2836576 -3.9910064][-4.342217 -4.3598771 -4.4781156 -4.7120543 -4.9983063 -5.3133821 -5.4390125 -5.1898317 -4.7467184 -4.53158 -4.64626 -4.8153892 -4.6959395 -4.3071418 -3.9972973][-4.8187227 -4.9749842 -5.0938234 -5.1762133 -5.2644768 -5.4342442 -5.5025063 -5.2140665 -4.772594 -4.5808659 -4.6691947 -4.8113084 -4.6908956 -4.2992864 -4.0073662][-4.9252543 -5.1016927 -5.1577425 -5.0521045 -4.8953724 -4.9168134 -5.0294175 -4.8373404 -4.5098367 -4.4339647 -4.5688338 -4.730495 -4.6351 -4.2684731 -4.0057969]]...]
INFO - root - 2017-12-06 10:05:13.148950: step 21310, loss = 0.69, batch loss = 0.62 (13.5 examples/sec; 0.591 sec/batch; 51h:05m:29s remains)
INFO - root - 2017-12-06 10:05:19.193020: step 21320, loss = 1.02, batch loss = 0.95 (13.4 examples/sec; 0.596 sec/batch; 51h:29m:33s remains)
INFO - root - 2017-12-06 10:05:25.065962: step 21330, loss = 0.77, batch loss = 0.70 (12.9 examples/sec; 0.619 sec/batch; 53h:31m:18s remains)
INFO - root - 2017-12-06 10:05:31.121971: step 21340, loss = 1.02, batch loss = 0.95 (12.8 examples/sec; 0.624 sec/batch; 53h:56m:44s remains)
INFO - root - 2017-12-06 10:05:37.220820: step 21350, loss = 1.00, batch loss = 0.93 (12.6 examples/sec; 0.635 sec/batch; 54h:54m:16s remains)
INFO - root - 2017-12-06 10:05:43.290091: step 21360, loss = 0.75, batch loss = 0.68 (13.4 examples/sec; 0.598 sec/batch; 51h:41m:04s remains)
INFO - root - 2017-12-06 10:05:49.340337: step 21370, loss = 0.91, batch loss = 0.84 (13.4 examples/sec; 0.598 sec/batch; 51h:39m:21s remains)
INFO - root - 2017-12-06 10:05:55.424237: step 21380, loss = 0.83, batch loss = 0.76 (13.0 examples/sec; 0.615 sec/batch; 53h:07m:36s remains)
INFO - root - 2017-12-06 10:06:01.413832: step 21390, loss = 1.08, batch loss = 1.01 (17.1 examples/sec; 0.467 sec/batch; 40h:21m:13s remains)
INFO - root - 2017-12-06 10:06:07.454383: step 21400, loss = 1.32, batch loss = 1.25 (13.3 examples/sec; 0.599 sec/batch; 51h:47m:20s remains)
2017-12-06 10:06:07.965262: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3841295 -3.4698787 -3.6113744 -3.755482 -4.0049911 -4.2963071 -4.4747872 -4.5143013 -4.4227209 -4.1221189 -3.7549484 -3.3487487 -3.1082401 -3.2461634 -3.66504][-3.4873061 -3.4903975 -3.505477 -3.5793664 -3.850111 -4.2661819 -4.5750451 -4.6441059 -4.5146017 -4.2332768 -3.9687183 -3.674628 -3.445709 -3.5055649 -3.8127296][-3.5057878 -3.4368069 -3.2906311 -3.1961858 -3.3898745 -3.899092 -4.3959012 -4.6021328 -4.5205746 -4.3085322 -4.1975865 -4.0775986 -3.907176 -3.8768601 -4.005991][-3.4561853 -3.3643808 -3.0864854 -2.7746415 -2.7629056 -3.196991 -3.798126 -4.1980267 -4.3084407 -4.2541757 -4.2926564 -4.320003 -4.2136717 -4.1225948 -4.0865927][-3.4747481 -3.4184608 -3.0724738 -2.5559859 -2.2376008 -2.3807845 -2.8898096 -3.4555526 -3.8700109 -4.0894284 -4.2821689 -4.3959956 -4.3292723 -4.214807 -4.1000805][-3.6094766 -3.6370025 -3.2881358 -2.5920362 -1.9105511 -1.6091363 -1.8205338 -2.4584346 -3.238956 -3.8502023 -4.2642255 -4.4333048 -4.3437748 -4.1874604 -4.0583096][-3.7965064 -3.8840952 -3.5564392 -2.7632952 -1.8344049 -1.1408219 -0.97009873 -1.530097 -2.5640192 -3.5168393 -4.1565208 -4.3745022 -4.2162576 -3.9765429 -3.8523769][-3.9969614 -4.0886436 -3.7653465 -2.9823208 -2.0605316 -1.2412503 -0.80791926 -1.1725233 -2.1818295 -3.2231522 -3.9460397 -4.17941 -3.9792817 -3.6961012 -3.591598][-4.1370535 -4.2443833 -3.9443316 -3.2552032 -2.5335352 -1.8563325 -1.3481259 -1.4577672 -2.2157779 -3.0854564 -3.7072902 -3.9096684 -3.7476408 -3.5315337 -3.4783301][-4.2000542 -4.3487897 -4.1053019 -3.5462914 -3.0901728 -2.6907978 -2.2605147 -2.1718276 -2.5771589 -3.131207 -3.5326509 -3.6349447 -3.5138788 -3.3769312 -3.3692532][-4.0492826 -4.2297444 -4.0475588 -3.5861869 -3.3755453 -3.3358169 -3.1523767 -3.0256548 -3.144187 -3.3369155 -3.442857 -3.3899329 -3.2557025 -3.1597469 -3.1478498][-3.8054702 -3.9534941 -3.7929492 -3.3905482 -3.3410368 -3.5982614 -3.733952 -3.7375164 -3.7291346 -3.6607385 -3.5166953 -3.3086808 -3.1200476 -3.0153027 -2.9617202][-3.6010125 -3.6909592 -3.5234821 -3.1489005 -3.1488442 -3.5487823 -3.9292779 -4.1398478 -4.1835265 -4.0344572 -3.7691441 -3.4714134 -3.2491615 -3.1646991 -3.1122444][-3.4053249 -3.4884844 -3.3660569 -3.040915 -3.0142817 -3.37106 -3.7966886 -4.1227913 -4.2820363 -4.2092104 -3.9929893 -3.7029366 -3.4854145 -3.4482276 -3.4717093][-3.2379894 -3.3682249 -3.3625097 -3.1554971 -3.0836716 -3.2670093 -3.5444074 -3.8225145 -4.0304885 -4.0837975 -4.0073853 -3.7944808 -3.569849 -3.5262218 -3.6115091]]...]
INFO - root - 2017-12-06 10:06:14.038961: step 21410, loss = 0.87, batch loss = 0.80 (12.9 examples/sec; 0.620 sec/batch; 53h:37m:04s remains)
INFO - root - 2017-12-06 10:06:20.062798: step 21420, loss = 1.05, batch loss = 0.98 (13.0 examples/sec; 0.617 sec/batch; 53h:17m:48s remains)
INFO - root - 2017-12-06 10:06:26.095154: step 21430, loss = 0.87, batch loss = 0.80 (13.3 examples/sec; 0.599 sec/batch; 51h:47m:07s remains)
INFO - root - 2017-12-06 10:06:32.223764: step 21440, loss = 0.98, batch loss = 0.91 (12.7 examples/sec; 0.628 sec/batch; 54h:18m:05s remains)
INFO - root - 2017-12-06 10:06:38.252303: step 21450, loss = 0.90, batch loss = 0.83 (13.5 examples/sec; 0.594 sec/batch; 51h:21m:16s remains)
INFO - root - 2017-12-06 10:06:44.275258: step 21460, loss = 0.80, batch loss = 0.73 (13.4 examples/sec; 0.598 sec/batch; 51h:41m:01s remains)
INFO - root - 2017-12-06 10:06:50.319125: step 21470, loss = 0.96, batch loss = 0.89 (13.6 examples/sec; 0.588 sec/batch; 50h:48m:44s remains)
INFO - root - 2017-12-06 10:06:56.239138: step 21480, loss = 0.90, batch loss = 0.83 (15.4 examples/sec; 0.518 sec/batch; 44h:45m:42s remains)
INFO - root - 2017-12-06 10:07:02.299201: step 21490, loss = 0.82, batch loss = 0.75 (13.4 examples/sec; 0.596 sec/batch; 51h:27m:16s remains)
INFO - root - 2017-12-06 10:07:08.224793: step 21500, loss = 1.05, batch loss = 0.98 (13.2 examples/sec; 0.604 sec/batch; 52h:10m:25s remains)
2017-12-06 10:07:08.863462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9229937 -3.6013663 -4.524219 -5.1639519 -5.29018 -5.004415 -4.5056543 -3.9482152 -3.5807173 -3.6316047 -4.2719421 -4.9308872 -5.0031052 -4.3534565 -3.5062768][-3.1278338 -3.9444633 -4.8194027 -5.3198833 -5.3218384 -4.9221344 -4.365911 -3.8001692 -3.5190005 -3.7194538 -4.3662086 -4.8594584 -4.8364496 -4.2401056 -3.5153956][-3.3561065 -4.1398559 -4.7763586 -4.9730797 -4.7879548 -4.294167 -3.7329786 -3.2829971 -3.1610241 -3.4577835 -4.0223937 -4.3958392 -4.4292703 -4.0285978 -3.5050159][-3.3451009 -3.9330223 -4.2271457 -4.1138778 -3.8360488 -3.3919928 -2.9285984 -2.6487176 -2.6636868 -2.9518251 -3.4098091 -3.7805007 -3.9803269 -3.8067734 -3.4564817][-2.9641647 -3.336992 -3.3547368 -3.0508683 -2.7705305 -2.4443069 -2.0940955 -1.9695263 -2.1109993 -2.3898184 -2.8376968 -3.3309569 -3.7066011 -3.6847908 -3.4020052][-2.5381522 -2.7911043 -2.6928635 -2.313024 -1.9758537 -1.6368043 -1.2718801 -1.2131019 -1.483217 -1.8521392 -2.4329286 -3.1331534 -3.6423264 -3.6757255 -3.3695567][-2.5435445 -2.7737617 -2.6948633 -2.290415 -1.7583172 -1.1698582 -0.57051682 -0.42832661 -0.8359046 -1.4538162 -2.3303947 -3.2903075 -3.871805 -3.8521259 -3.4417481][-3.1592591 -3.375355 -3.3527613 -2.9680977 -2.2618616 -1.3617468 -0.43279958 -0.087267876 -0.54072285 -1.3997848 -2.535975 -3.6428823 -4.1987085 -4.0613785 -3.5340862][-4.2708807 -4.4621968 -4.5184412 -4.2543025 -3.5613718 -2.4849415 -1.2664685 -0.61900425 -0.86982059 -1.6757481 -2.7691789 -3.8085542 -4.2815809 -4.0692058 -3.4965878][-5.2067337 -5.3502264 -5.4817019 -5.4068723 -4.8736691 -3.7664933 -2.3980334 -1.5527411 -1.5606325 -2.1436839 -3.0281565 -3.9067183 -4.3030157 -4.0432796 -3.4308038][-5.6012468 -5.721642 -5.9380617 -6.032692 -5.6386027 -4.5747094 -3.2426717 -2.443047 -2.3721428 -2.7689412 -3.4499879 -4.1858072 -4.5335617 -4.2094879 -3.4974327][-5.6910458 -5.8340931 -6.1508451 -6.3560886 -6.0281897 -5.0251184 -3.858273 -3.2637076 -3.2155714 -3.4602633 -3.97763 -4.6079421 -4.9295 -4.5367646 -3.6960776][-5.30305 -5.4910212 -5.8944683 -6.1464581 -5.8637524 -4.993885 -4.1042943 -3.8135347 -3.8953567 -4.1029735 -4.5607543 -5.1417608 -5.4071355 -4.9178343 -3.9472184][-4.2721252 -4.486989 -4.9161854 -5.16696 -4.9733224 -4.3719711 -3.8800955 -3.9417124 -4.2184892 -4.4969044 -4.9855943 -5.5594292 -5.7513623 -5.1766486 -4.1382246][-3.3600283 -3.634156 -4.0703373 -4.2877579 -4.1598649 -3.7797182 -3.5408273 -3.7612405 -4.12283 -4.4529629 -4.9864264 -5.57943 -5.7503858 -5.1884279 -4.1966639]]...]
INFO - root - 2017-12-06 10:07:14.994954: step 21510, loss = 0.93, batch loss = 0.86 (12.9 examples/sec; 0.618 sec/batch; 53h:25m:23s remains)
INFO - root - 2017-12-06 10:07:21.085593: step 21520, loss = 0.97, batch loss = 0.90 (13.2 examples/sec; 0.606 sec/batch; 52h:21m:41s remains)
INFO - root - 2017-12-06 10:07:27.227306: step 21530, loss = 0.97, batch loss = 0.90 (13.2 examples/sec; 0.606 sec/batch; 52h:19m:11s remains)
INFO - root - 2017-12-06 10:07:33.315385: step 21540, loss = 0.84, batch loss = 0.77 (13.2 examples/sec; 0.608 sec/batch; 52h:31m:10s remains)
INFO - root - 2017-12-06 10:07:39.399138: step 21550, loss = 0.94, batch loss = 0.87 (13.3 examples/sec; 0.601 sec/batch; 51h:54m:13s remains)
INFO - root - 2017-12-06 10:07:45.507265: step 21560, loss = 1.14, batch loss = 1.07 (13.5 examples/sec; 0.593 sec/batch; 51h:15m:09s remains)
INFO - root - 2017-12-06 10:07:51.578091: step 21570, loss = 1.14, batch loss = 1.07 (13.2 examples/sec; 0.608 sec/batch; 52h:30m:26s remains)
INFO - root - 2017-12-06 10:07:57.645358: step 21580, loss = 0.96, batch loss = 0.89 (13.6 examples/sec; 0.590 sec/batch; 50h:55m:42s remains)
INFO - root - 2017-12-06 10:08:03.662249: step 21590, loss = 0.81, batch loss = 0.74 (13.7 examples/sec; 0.582 sec/batch; 50h:17m:00s remains)
INFO - root - 2017-12-06 10:08:09.701590: step 21600, loss = 0.87, batch loss = 0.80 (13.6 examples/sec; 0.588 sec/batch; 50h:44m:27s remains)
2017-12-06 10:08:10.319543: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1450825 -4.1694665 -4.2376957 -4.356205 -4.48437 -4.5843744 -4.6212764 -4.6549482 -4.7220135 -4.7621059 -4.721602 -4.5952358 -4.4402852 -4.3144875 -4.2181034][-4.2025266 -4.2657747 -4.3722391 -4.542212 -4.7084885 -4.831491 -4.8881283 -4.946187 -5.0286007 -5.0580812 -4.9833179 -4.8242464 -4.6647973 -4.5376873 -4.4251719][-4.2731214 -4.3643742 -4.4951687 -4.7140908 -4.9166102 -5.0332189 -5.0454597 -5.0268664 -5.0247979 -5.0086317 -4.9468427 -4.8335004 -4.7575788 -4.7007675 -4.6322656][-4.3281522 -4.4215026 -4.5519967 -4.7996941 -5.0116735 -5.0306053 -4.8391633 -4.5759273 -4.3678627 -4.3211656 -4.4060531 -4.4787817 -4.59518 -4.6941 -4.7410049][-4.3540683 -4.4325609 -4.5492568 -4.8021502 -4.98159 -4.7888184 -4.2100911 -3.5322745 -3.0192571 -2.9813206 -3.365638 -3.7613187 -4.1580982 -4.4727035 -4.6857972][-4.3609815 -4.4396644 -4.5534472 -4.7752619 -4.8412285 -4.3083143 -3.2132137 -2.0532551 -1.2486286 -1.2946115 -2.0727913 -2.8693748 -3.5925422 -4.13347 -4.5107222][-4.3653917 -4.4642777 -4.574367 -4.6911783 -4.5507941 -3.6354637 -2.0596457 -0.526937 0.4356699 0.21296549 -0.94785333 -2.1076779 -3.1079285 -3.8129988 -4.304287][-4.3681164 -4.4956069 -4.5882039 -4.5643854 -4.2341008 -3.1221347 -1.3724976 0.23230839 1.1691184 0.78614855 -0.55354929 -1.8659801 -2.9429469 -3.6567059 -4.166503][-4.3500037 -4.5132236 -4.5942073 -4.4755459 -4.0791535 -3.0673804 -1.556175 -0.246912 0.47924757 0.07033205 -1.1392922 -2.3084817 -3.2046304 -3.7533829 -4.1781993][-4.3041196 -4.4938941 -4.5773425 -4.4399438 -4.0974107 -3.3680038 -2.3386068 -1.5358465 -1.1395838 -1.5123429 -2.3909118 -3.2197559 -3.7927785 -4.0809069 -4.3539467][-4.2304378 -4.4205737 -4.5048475 -4.4058752 -4.1856103 -3.7699564 -3.2244389 -2.9026299 -2.8285418 -3.1672506 -3.7179525 -4.1921358 -4.4639821 -4.5214019 -4.6342077][-4.1523771 -4.3109827 -4.3800297 -4.3378124 -4.2306972 -4.0340252 -3.7969911 -3.7646589 -3.90949 -4.2390122 -4.5881848 -4.8352036 -4.9401717 -4.8879437 -4.9038262][-4.0971775 -4.2061057 -4.2472425 -4.2427125 -4.2166958 -4.1401825 -4.0495009 -4.1142545 -4.3286786 -4.62063 -4.8592076 -5.0077243 -5.0775375 -5.0424209 -5.0526457][-4.0806684 -4.1411762 -4.1531148 -4.1611776 -4.1870513 -4.1837721 -4.1651945 -4.234447 -4.4191775 -4.6383815 -4.7881193 -4.8882375 -4.9732857 -5.0083475 -5.0675125][-4.1283636 -4.1599684 -4.1551943 -4.1627431 -4.1989083 -4.2222257 -4.2336631 -4.2816367 -4.4060106 -4.5401797 -4.6077409 -4.6726575 -4.78275 -4.8754692 -4.9794106]]...]
INFO - root - 2017-12-06 10:08:16.289912: step 21610, loss = 0.92, batch loss = 0.85 (13.2 examples/sec; 0.604 sec/batch; 52h:09m:13s remains)
INFO - root - 2017-12-06 10:08:22.353492: step 21620, loss = 1.11, batch loss = 1.04 (13.3 examples/sec; 0.600 sec/batch; 51h:48m:53s remains)
INFO - root - 2017-12-06 10:08:28.335196: step 21630, loss = 0.98, batch loss = 0.91 (14.7 examples/sec; 0.543 sec/batch; 46h:54m:14s remains)
INFO - root - 2017-12-06 10:08:34.450795: step 21640, loss = 0.72, batch loss = 0.65 (13.0 examples/sec; 0.614 sec/batch; 52h:58m:46s remains)
INFO - root - 2017-12-06 10:08:40.521536: step 21650, loss = 0.66, batch loss = 0.59 (12.9 examples/sec; 0.620 sec/batch; 53h:31m:11s remains)
INFO - root - 2017-12-06 10:08:46.583268: step 21660, loss = 0.94, batch loss = 0.87 (13.1 examples/sec; 0.611 sec/batch; 52h:47m:07s remains)
INFO - root - 2017-12-06 10:08:52.744176: step 21670, loss = 1.04, batch loss = 0.97 (12.6 examples/sec; 0.633 sec/batch; 54h:40m:55s remains)
INFO - root - 2017-12-06 10:08:58.910157: step 21680, loss = 0.93, batch loss = 0.86 (12.9 examples/sec; 0.620 sec/batch; 53h:33m:09s remains)
INFO - root - 2017-12-06 10:09:04.986066: step 21690, loss = 1.09, batch loss = 1.02 (12.8 examples/sec; 0.626 sec/batch; 54h:02m:47s remains)
INFO - root - 2017-12-06 10:09:11.141963: step 21700, loss = 1.28, batch loss = 1.21 (13.1 examples/sec; 0.612 sec/batch; 52h:51m:51s remains)
2017-12-06 10:09:11.716680: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0084839 -4.8505 -4.5206194 -4.1827884 -4.0907435 -4.1733675 -4.2024937 -4.3479238 -4.571198 -4.6986418 -4.7297711 -4.7057676 -4.5614948 -4.1086235 -3.7344341][-4.9747815 -4.8170023 -4.4961119 -4.2012653 -4.1459846 -4.2551174 -4.3044991 -4.4884534 -4.7880569 -4.9720368 -5.0298662 -4.9539275 -4.6718712 -4.02569 -3.3969133][-4.8777065 -4.712852 -4.413672 -4.1581144 -4.0828733 -4.1365829 -4.1352615 -4.3162107 -4.6856813 -4.9675145 -5.1244178 -5.089601 -4.7731128 -4.0371733 -3.2174029][-4.7489982 -4.5566783 -4.2668667 -4.004662 -3.8030839 -3.6643424 -3.5122991 -3.6559162 -4.1151943 -4.5497103 -4.8901787 -5.0281787 -4.8113704 -4.1273227 -3.2604647][-4.5951209 -4.3272948 -4.0082321 -3.6949365 -3.2855632 -2.8412657 -2.439666 -2.5136502 -3.0935245 -3.7052777 -4.2523451 -4.6649213 -4.6692767 -4.1608181 -3.3649592][-4.4108338 -4.0210938 -3.6309814 -3.2519159 -2.6499825 -1.8928745 -1.1844127 -1.1711473 -1.9065797 -2.6781242 -3.371237 -4.0290656 -4.2651391 -4.0150223 -3.3684292][-4.2158408 -3.6893077 -3.2185302 -2.816709 -2.1756866 -1.2715468 -0.32379484 -0.19890738 -1.0350339 -1.8683422 -2.5645072 -3.3226781 -3.6904812 -3.6859317 -3.2011604][-4.0654225 -3.4433818 -2.9330301 -2.5932646 -2.11481 -1.323375 -0.36670589 -0.17674255 -0.95415163 -1.6598229 -2.1814749 -2.8684735 -3.2410212 -3.4007494 -3.0719743][-4.0526414 -3.4225817 -2.9570489 -2.7526078 -2.53122 -2.0050349 -1.238241 -1.0723474 -1.6805792 -2.1144617 -2.3430891 -2.8574228 -3.1736133 -3.3986714 -3.1799798][-4.2347631 -3.6948314 -3.3451376 -3.2818503 -3.2652016 -2.9848342 -2.4661484 -2.3648953 -2.7725916 -2.9245269 -2.8700831 -3.2006295 -3.455883 -3.6606236 -3.4976439][-4.5601678 -4.1908803 -3.9928122 -4.0289373 -4.103385 -3.9682107 -3.6539536 -3.6027455 -3.822715 -3.7483296 -3.4829624 -3.6444106 -3.8363085 -3.9895544 -3.886189][-4.8846569 -4.70554 -4.6481633 -4.72858 -4.8056612 -4.7231874 -4.5269809 -4.4804058 -4.5507722 -4.3534036 -4.0052137 -4.0532808 -4.1881142 -4.297533 -4.2732892][-5.0685835 -5.0283818 -5.0593233 -5.1436934 -5.1898546 -5.1186481 -4.9886804 -4.9429812 -4.9311647 -4.7221732 -4.4232688 -4.4311609 -4.5369754 -4.6289377 -4.6683917][-5.0922747 -5.114687 -5.1724811 -5.2361064 -5.2535625 -5.19485 -5.1142616 -5.0861473 -5.0639081 -4.920753 -4.7404122 -4.7565885 -4.8404412 -4.9074683 -4.96165][-5.0356283 -5.0649309 -5.1129246 -5.1539955 -5.1601038 -5.1236458 -5.0827641 -5.0741553 -5.0641432 -4.9918346 -4.907361 -4.9272604 -4.9857488 -5.0293522 -5.0690475]]...]
INFO - root - 2017-12-06 10:09:17.663992: step 21710, loss = 0.91, batch loss = 0.84 (16.8 examples/sec; 0.476 sec/batch; 41h:07m:56s remains)
INFO - root - 2017-12-06 10:09:23.758300: step 21720, loss = 1.01, batch loss = 0.94 (13.1 examples/sec; 0.610 sec/batch; 52h:39m:00s remains)
INFO - root - 2017-12-06 10:09:29.885135: step 21730, loss = 0.94, batch loss = 0.86 (12.8 examples/sec; 0.623 sec/batch; 53h:49m:20s remains)
INFO - root - 2017-12-06 10:09:36.020666: step 21740, loss = 0.99, batch loss = 0.92 (13.0 examples/sec; 0.616 sec/batch; 53h:11m:54s remains)
INFO - root - 2017-12-06 10:09:42.106716: step 21750, loss = 1.00, batch loss = 0.93 (12.9 examples/sec; 0.618 sec/batch; 53h:20m:49s remains)
INFO - root - 2017-12-06 10:09:48.107356: step 21760, loss = 0.89, batch loss = 0.82 (13.6 examples/sec; 0.588 sec/batch; 50h:43m:14s remains)
INFO - root - 2017-12-06 10:09:54.185439: step 21770, loss = 0.90, batch loss = 0.83 (13.7 examples/sec; 0.583 sec/batch; 50h:21m:30s remains)
INFO - root - 2017-12-06 10:10:00.255625: step 21780, loss = 0.79, batch loss = 0.72 (13.3 examples/sec; 0.604 sec/batch; 52h:06m:14s remains)
INFO - root - 2017-12-06 10:10:06.200115: step 21790, loss = 1.18, batch loss = 1.11 (13.2 examples/sec; 0.606 sec/batch; 52h:19m:56s remains)
INFO - root - 2017-12-06 10:10:12.257214: step 21800, loss = 0.85, batch loss = 0.78 (13.3 examples/sec; 0.602 sec/batch; 51h:56m:27s remains)
2017-12-06 10:10:12.793561: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3378358 -4.4270363 -4.4290929 -4.2744179 -3.8017235 -3.3530161 -3.2105465 -3.5673976 -3.6709394 -3.4297233 -3.5341454 -3.6870279 -3.7452087 -3.6294222 -3.8131995][-4.0873828 -4.2669878 -4.3426323 -4.2312951 -3.8040049 -3.4151406 -3.231945 -3.5216393 -3.6037495 -3.3506594 -3.4182673 -3.5454366 -3.7568917 -3.8181076 -4.0149474][-3.6115396 -3.8736992 -4.0602279 -4.021924 -3.7068572 -3.4950204 -3.3962057 -3.6388032 -3.6935787 -3.4496748 -3.4644461 -3.5400031 -3.8192914 -4.0196252 -4.2272396][-3.2519388 -3.5934939 -3.8979492 -3.891526 -3.6379631 -3.5856402 -3.6047974 -3.8293972 -3.8904192 -3.713567 -3.7222242 -3.7649622 -4.0125475 -4.2171769 -4.3555989][-3.1588125 -3.5502257 -3.9572635 -3.9560134 -3.6836779 -3.6619177 -3.7246203 -3.9155931 -4.0055809 -3.9155617 -3.9470832 -4.0076065 -4.2109327 -4.3531675 -4.3620758][-3.3146627 -3.6580825 -4.0700684 -4.031847 -3.6924355 -3.5934589 -3.6291747 -3.8034539 -3.9547362 -3.9586098 -4.0097556 -4.08764 -4.2624226 -4.3514547 -4.2518215][-3.60886 -3.7498224 -3.9927645 -3.8420486 -3.4185972 -3.2136874 -3.2203493 -3.4333377 -3.7036767 -3.8503025 -3.9333203 -3.9994752 -4.1338482 -4.1836433 -4.0421019][-3.9409978 -3.8148453 -3.759819 -3.4272213 -2.9403152 -2.6965411 -2.7367835 -3.0168583 -3.3963051 -3.6888022 -3.8211687 -3.8519208 -3.9190519 -3.9376755 -3.8254967][-4.2018685 -3.840601 -3.4191828 -2.8394833 -2.3451431 -2.2415793 -2.4600594 -2.826009 -3.2640285 -3.6466918 -3.8043873 -3.7774072 -3.7555566 -3.7521 -3.7115512][-4.348875 -3.867058 -3.1608462 -2.3689871 -1.936445 -2.0972078 -2.5602789 -3.0059438 -3.4492233 -3.8256397 -3.9482048 -3.8497534 -3.7522793 -3.73886 -3.7641737][-4.3132725 -3.8825121 -3.1185784 -2.2837834 -1.9649246 -2.3350179 -2.9378538 -3.4001958 -3.8043571 -4.0988445 -4.1417694 -3.9997187 -3.8790898 -3.8659208 -3.9182091][-4.0081568 -3.7628379 -3.1873274 -2.5126534 -2.3346336 -2.7780652 -3.3773456 -3.8003354 -4.1347766 -4.3172212 -4.2875061 -4.1739984 -4.1015596 -4.0905218 -4.1092257][-3.537384 -3.4868157 -3.2367392 -2.8599577 -2.8352208 -3.2607579 -3.7771301 -4.1164126 -4.3548288 -4.4214878 -4.344141 -4.2988954 -4.3058662 -4.2906704 -4.24662][-3.0979364 -3.1445568 -3.1912713 -3.1309125 -3.248858 -3.6468506 -4.0859828 -4.3373117 -4.4530568 -4.4005947 -4.277329 -4.2793207 -4.3569803 -4.3616729 -4.2969875][-2.8449841 -2.8577812 -3.0599654 -3.2463455 -3.5043573 -3.9058514 -4.3034334 -4.5000544 -4.5139236 -4.3462667 -4.14571 -4.1200237 -4.2272882 -4.2845612 -4.2572513]]...]
INFO - root - 2017-12-06 10:10:18.883885: step 21810, loss = 0.86, batch loss = 0.79 (13.3 examples/sec; 0.603 sec/batch; 52h:04m:16s remains)
INFO - root - 2017-12-06 10:10:24.735750: step 21820, loss = 0.95, batch loss = 0.88 (13.0 examples/sec; 0.616 sec/batch; 53h:08m:09s remains)
INFO - root - 2017-12-06 10:10:30.870640: step 21830, loss = 0.84, batch loss = 0.77 (13.6 examples/sec; 0.590 sec/batch; 50h:53m:14s remains)
INFO - root - 2017-12-06 10:10:36.929288: step 21840, loss = 1.07, batch loss = 1.00 (12.9 examples/sec; 0.620 sec/batch; 53h:29m:52s remains)
INFO - root - 2017-12-06 10:10:42.963413: step 21850, loss = 0.95, batch loss = 0.88 (12.7 examples/sec; 0.628 sec/batch; 54h:10m:39s remains)
INFO - root - 2017-12-06 10:10:49.018145: step 21860, loss = 0.86, batch loss = 0.79 (13.4 examples/sec; 0.596 sec/batch; 51h:23m:35s remains)
INFO - root - 2017-12-06 10:10:55.168518: step 21870, loss = 0.75, batch loss = 0.68 (13.1 examples/sec; 0.609 sec/batch; 52h:31m:26s remains)
INFO - root - 2017-12-06 10:11:01.251300: step 21880, loss = 1.11, batch loss = 1.04 (13.0 examples/sec; 0.615 sec/batch; 53h:05m:56s remains)
INFO - root - 2017-12-06 10:11:07.262117: step 21890, loss = 0.91, batch loss = 0.84 (12.9 examples/sec; 0.622 sec/batch; 53h:41m:45s remains)
INFO - root - 2017-12-06 10:11:13.284204: step 21900, loss = 0.96, batch loss = 0.89 (13.2 examples/sec; 0.608 sec/batch; 52h:28m:35s remains)
2017-12-06 10:11:13.968422: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9104924 -5.0599494 -4.9321709 -5.0267954 -5.117712 -5.0955758 -4.8393435 -4.4815125 -4.5243969 -4.6520972 -4.793005 -4.8959374 -4.9739423 -5.1042328 -5.1121345][-4.7643151 -4.9683108 -4.8220024 -4.8977232 -5.0663977 -5.0235596 -4.6157274 -4.1244659 -4.1933393 -4.3933544 -4.5746422 -4.647933 -4.6854486 -4.8495812 -4.8923683][-4.4543524 -4.6201992 -4.4044809 -4.4529095 -4.6852984 -4.6164742 -4.1101041 -3.5781806 -3.7688477 -4.0406752 -4.1729975 -4.1887345 -4.2212267 -4.460094 -4.5835342][-4.2161279 -4.302402 -3.9833491 -3.9831481 -4.175149 -3.9839094 -3.3810463 -2.9022708 -3.3085442 -3.6763241 -3.7347324 -3.750335 -3.8134332 -4.0471678 -4.1960311][-4.1556597 -4.1426663 -3.6611459 -3.4779642 -3.3484492 -2.7956624 -1.9939661 -1.6337721 -2.3980913 -2.9851308 -3.0838957 -3.2137346 -3.3459058 -3.517529 -3.6914134][-4.29393 -4.1270905 -3.3698554 -2.7921908 -2.0658119 -1.0460415 -0.11876965 -0.058046818 -1.3604429 -2.2903664 -2.5200214 -2.761054 -2.916559 -3.0300093 -3.3158095][-4.4848852 -4.1536694 -3.1020675 -2.0494387 -0.74231339 0.54763937 1.3389363 0.96697187 -0.83415985 -2.0319202 -2.3652165 -2.6314211 -2.7425325 -2.8138096 -3.234725][-4.7814336 -4.4114542 -3.2204821 -1.8762691 -0.31810284 0.89506721 1.3877015 0.75908422 -1.1625721 -2.3113587 -2.6044173 -2.7950325 -2.8246498 -2.8953323 -3.4050527][-5.1072388 -4.8630333 -3.8153813 -2.5398476 -1.174866 -0.33954191 -0.10680389 -0.67466593 -2.2834969 -3.0882792 -3.2277281 -3.3325853 -3.2904878 -3.3525219 -3.8211956][-5.191123 -5.1216621 -4.4042454 -3.4588184 -2.5115614 -2.0792694 -1.9474413 -2.2990139 -3.4401813 -3.8268113 -3.7877302 -3.8349583 -3.7875831 -3.870491 -4.2419267][-5.1083145 -5.1482511 -4.7515097 -4.1885853 -3.6582778 -3.5015249 -3.3469765 -3.44631 -4.1176677 -4.1268888 -3.909965 -3.9174762 -3.9398541 -4.1024346 -4.41496][-4.9954271 -5.0267091 -4.794517 -4.4995208 -4.2843771 -4.2808166 -4.1189036 -4.0829663 -4.4160666 -4.1989732 -3.8651843 -3.822928 -3.8906736 -4.1131983 -4.3896513][-4.9435935 -4.9006896 -4.6980085 -4.5294218 -4.4873843 -4.5495086 -4.4337325 -4.3800578 -4.5087008 -4.2071743 -3.8361249 -3.7339373 -3.7997625 -4.0308518 -4.2777171][-4.9803095 -4.9195976 -4.7459311 -4.6329284 -4.649652 -4.71976 -4.6587176 -4.6030774 -4.5772991 -4.2626605 -3.9129162 -3.7766421 -3.8230593 -4.0207582 -4.2337923][-4.8659558 -4.8423114 -4.7365985 -4.6706147 -4.6919951 -4.7527819 -4.7511649 -4.727819 -4.6611452 -4.4326892 -4.1891742 -4.0763431 -4.0967908 -4.2242641 -4.3783083]]...]
INFO - root - 2017-12-06 10:11:20.032967: step 21910, loss = 0.72, batch loss = 0.65 (12.8 examples/sec; 0.624 sec/batch; 53h:51m:42s remains)
INFO - root - 2017-12-06 10:11:26.164508: step 21920, loss = 1.02, batch loss = 0.95 (13.1 examples/sec; 0.610 sec/batch; 52h:39m:47s remains)
INFO - root - 2017-12-06 10:11:32.069422: step 21930, loss = 0.87, batch loss = 0.80 (12.9 examples/sec; 0.618 sec/batch; 53h:19m:55s remains)
INFO - root - 2017-12-06 10:11:37.954931: step 21940, loss = 0.95, batch loss = 0.88 (13.6 examples/sec; 0.589 sec/batch; 50h:50m:55s remains)
INFO - root - 2017-12-06 10:11:44.025594: step 21950, loss = 0.80, batch loss = 0.73 (13.3 examples/sec; 0.602 sec/batch; 51h:58m:08s remains)
INFO - root - 2017-12-06 10:11:50.092456: step 21960, loss = 1.05, batch loss = 0.98 (13.0 examples/sec; 0.615 sec/batch; 53h:04m:01s remains)
INFO - root - 2017-12-06 10:11:56.263222: step 21970, loss = 1.13, batch loss = 1.06 (12.6 examples/sec; 0.635 sec/batch; 54h:44m:29s remains)
INFO - root - 2017-12-06 10:12:02.299322: step 21980, loss = 1.09, batch loss = 1.02 (13.5 examples/sec; 0.594 sec/batch; 51h:14m:17s remains)
INFO - root - 2017-12-06 10:12:08.374724: step 21990, loss = 0.97, batch loss = 0.90 (13.0 examples/sec; 0.615 sec/batch; 53h:03m:33s remains)
INFO - root - 2017-12-06 10:12:14.372352: step 22000, loss = 0.85, batch loss = 0.78 (12.9 examples/sec; 0.621 sec/batch; 53h:32m:58s remains)
2017-12-06 10:12:14.968021: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4107628 -4.89234 -5.301857 -5.2946587 -5.0791936 -4.7511392 -4.1794233 -3.9361024 -4.4393539 -5.1113477 -5.5784979 -5.6944547 -5.225481 -4.3005347 -3.514184][-5.3270192 -5.715023 -6.04939 -5.9884329 -5.6689692 -5.2177238 -4.5916786 -4.3962693 -4.9289732 -5.55573 -5.9257236 -5.8845663 -5.2947378 -4.2989745 -3.4444218][-5.7858024 -6.0995755 -6.2884312 -6.0332851 -5.4178638 -4.6806221 -3.9851489 -3.9496171 -4.7307997 -5.5538177 -5.9756656 -5.7955842 -5.0965848 -4.0454936 -3.1108861][-5.6893682 -5.9919744 -6.0397511 -5.5309591 -4.5277553 -3.4106512 -2.5984774 -2.7202868 -3.8536587 -5.0711784 -5.72818 -5.5156503 -4.7512937 -3.6654563 -2.6361563][-5.1856732 -5.5850062 -5.6014457 -4.9320941 -3.6176949 -2.1147621 -1.0874107 -1.2437775 -2.6474674 -4.2663369 -5.25268 -5.1521444 -4.4344544 -3.4011371 -2.3474538][-4.5183206 -5.0698786 -5.144722 -4.4233279 -2.9419119 -1.174298 0.065708637 -0.023416996 -1.5062571 -3.3141961 -4.5350037 -4.629447 -4.0927 -3.2583582 -2.3171854][-4.0225482 -4.7050714 -4.8625259 -4.1434345 -2.607815 -0.72664714 0.65601921 0.67456627 -0.71994114 -2.5019064 -3.8038204 -4.1055417 -3.8358665 -3.2914853 -2.5441017][-3.790853 -4.5669694 -4.850028 -4.2512097 -2.8209238 -1.0021369 0.43484879 0.60909939 -0.54275036 -2.1272511 -3.3868256 -3.8895087 -3.9317226 -3.7054358 -3.1534352][-3.608016 -4.4019556 -4.8030071 -4.4084363 -3.2166753 -1.6333346 -0.28497219 -0.015206337 -0.88438892 -2.1764042 -3.2914882 -3.9338462 -4.2471795 -4.2791376 -3.8820415][-3.5398159 -4.2216043 -4.6234446 -4.3808231 -3.4241691 -2.13665 -0.99318862 -0.73854136 -1.3986592 -2.4044449 -3.318964 -3.9883087 -4.4264536 -4.5599818 -4.2205396][-3.5258596 -4.0063233 -4.3395619 -4.2295327 -3.5325882 -2.577728 -1.6994715 -1.5258408 -2.0351067 -2.787509 -3.5021889 -4.1181 -4.5579743 -4.6642938 -4.3052044][-3.4219742 -3.6627007 -3.9067423 -3.9485836 -3.5838594 -3.0157118 -2.4569876 -2.4068184 -2.7956209 -3.3342195 -3.9122119 -4.4951811 -4.9397144 -5.033123 -4.6803727][-3.3185601 -3.3365951 -3.457289 -3.5865107 -3.5062547 -3.2798786 -3.03577 -3.1312084 -3.4546437 -3.8440163 -4.330533 -4.8819795 -5.3264585 -5.4388351 -5.1720543][-3.3173428 -3.1773865 -3.1690178 -3.2998295 -3.38911 -3.3934162 -3.3986917 -3.6293044 -3.9422617 -4.2521033 -4.6658473 -5.1490121 -5.5218267 -5.6065116 -5.4255733][-3.4354472 -3.2323866 -3.1600773 -3.2639012 -3.4307289 -3.5852005 -3.7833767 -4.1182504 -4.4414477 -4.7080188 -5.0474739 -5.4377689 -5.696743 -5.7242088 -5.60407]]...]
INFO - root - 2017-12-06 10:12:21.051911: step 22010, loss = 0.82, batch loss = 0.75 (13.4 examples/sec; 0.597 sec/batch; 51h:27m:16s remains)
INFO - root - 2017-12-06 10:12:27.118059: step 22020, loss = 0.90, batch loss = 0.83 (13.3 examples/sec; 0.603 sec/batch; 51h:59m:57s remains)
INFO - root - 2017-12-06 10:12:33.227774: step 22030, loss = 0.75, batch loss = 0.68 (13.6 examples/sec; 0.590 sec/batch; 50h:54m:29s remains)
INFO - root - 2017-12-06 10:12:39.082249: step 22040, loss = 0.85, batch loss = 0.78 (13.0 examples/sec; 0.617 sec/batch; 53h:14m:20s remains)
INFO - root - 2017-12-06 10:12:45.058380: step 22050, loss = 1.04, batch loss = 0.97 (13.6 examples/sec; 0.586 sec/batch; 50h:32m:35s remains)
INFO - root - 2017-12-06 10:12:51.170926: step 22060, loss = 0.83, batch loss = 0.76 (13.2 examples/sec; 0.606 sec/batch; 52h:16m:32s remains)
INFO - root - 2017-12-06 10:12:57.301290: step 22070, loss = 0.90, batch loss = 0.83 (13.4 examples/sec; 0.596 sec/batch; 51h:21m:04s remains)
INFO - root - 2017-12-06 10:13:03.372812: step 22080, loss = 0.85, batch loss = 0.78 (13.4 examples/sec; 0.599 sec/batch; 51h:37m:01s remains)
INFO - root - 2017-12-06 10:13:09.345044: step 22090, loss = 0.99, batch loss = 0.92 (13.3 examples/sec; 0.603 sec/batch; 51h:58m:15s remains)
INFO - root - 2017-12-06 10:13:15.337727: step 22100, loss = 1.13, batch loss = 1.06 (13.2 examples/sec; 0.606 sec/batch; 52h:13m:37s remains)
2017-12-06 10:13:15.921523: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.498457 -5.234786 -6.0521593 -6.6528912 -6.9135318 -6.9451361 -6.8735147 -6.5735378 -6.2013941 -6.267168 -6.4416661 -6.3844786 -6.1647015 -5.9799242 -5.8779545][-4.8548865 -5.7540126 -6.6240811 -7.1769247 -7.3447742 -7.2513227 -7.09249 -6.6994162 -6.231853 -6.4132519 -6.7505989 -6.7575073 -6.5451794 -6.366879 -6.2356081][-5.1090307 -6.1047359 -6.9139147 -7.2565813 -7.1916256 -6.8937531 -6.6011662 -6.1242094 -5.6160803 -6.0111141 -6.6169157 -6.7251425 -6.5529547 -6.4513912 -6.3128314][-5.1611242 -6.1002579 -6.6965113 -6.6785955 -6.2775993 -5.75404 -5.3783655 -4.9359369 -4.5045052 -5.1791739 -6.128263 -6.3236365 -6.1293902 -6.04643 -5.7873936][-4.9760647 -5.6928325 -5.944592 -5.4750934 -4.7090287 -3.9967484 -3.6183372 -3.3446569 -3.0206475 -3.8874567 -5.14407 -5.4155965 -5.2338676 -5.170331 -4.7065954][-4.6251669 -5.0807896 -4.9857225 -4.1048231 -3.0525913 -2.2141171 -1.849319 -1.7691908 -1.5258803 -2.4492233 -3.9292371 -4.2865062 -4.2007413 -4.1798058 -3.5214922][-4.2415628 -4.5284863 -4.2301183 -3.0964234 -1.8921754 -0.96703386 -0.58993125 -0.68777657 -0.51089692 -1.3568201 -2.9256375 -3.29211 -3.2459512 -3.1635427 -2.3150563][-3.9245791 -4.2119436 -3.9303865 -2.7878928 -1.6579616 -0.77301574 -0.36862469 -0.55084491 -0.3458457 -0.94394851 -2.4639626 -2.8155406 -2.7497163 -2.4744763 -1.4578073][-3.7362449 -4.162395 -4.09107 -3.168231 -2.319556 -1.6700807 -1.2914274 -1.4202614 -1.0343156 -1.2339303 -2.6005731 -2.9891572 -2.9299088 -2.4652536 -1.4153855][-3.7485261 -4.3792181 -4.6374917 -4.0533991 -3.5392041 -3.2163186 -2.9331625 -2.9606152 -2.3320186 -2.0808196 -3.1710129 -3.5908623 -3.5524106 -2.9784503 -2.0343053][-3.8751662 -4.66131 -5.2404747 -5.0254908 -4.7989092 -4.7548971 -4.6075335 -4.5479469 -3.7406611 -3.1145928 -3.8213632 -4.1612153 -4.1064587 -3.5471959 -2.793679][-3.9673185 -4.7911105 -5.5854554 -5.6829791 -5.6620803 -5.7832518 -5.7165623 -5.5372496 -4.5959749 -3.7553425 -4.092783 -4.2933021 -4.2254157 -3.8039682 -3.28119][-3.962779 -4.7383628 -5.6346064 -5.9926472 -6.1138268 -6.2602291 -6.1448555 -5.7766757 -4.7157 -3.7945189 -3.8854928 -3.9885898 -3.9699442 -3.7354136 -3.4091048][-3.8495934 -4.5223594 -5.4038739 -5.9134007 -6.1002908 -6.1496844 -5.8892198 -5.3336072 -4.2281342 -3.3708363 -3.3606331 -3.3935056 -3.4105287 -3.3051496 -3.0923524][-3.6933413 -4.2261629 -4.9937143 -5.5247154 -5.6921406 -5.5766478 -5.1450572 -4.5007339 -3.5145576 -2.8818469 -2.9073529 -2.88896 -2.8886631 -2.8263416 -2.6666024]]...]
INFO - root - 2017-12-06 10:13:21.971511: step 22110, loss = 0.90, batch loss = 0.83 (13.0 examples/sec; 0.613 sec/batch; 52h:52m:22s remains)
INFO - root - 2017-12-06 10:13:28.003236: step 22120, loss = 0.89, batch loss = 0.82 (12.9 examples/sec; 0.620 sec/batch; 53h:29m:24s remains)
INFO - root - 2017-12-06 10:13:34.084527: step 22130, loss = 1.06, batch loss = 0.99 (13.8 examples/sec; 0.578 sec/batch; 49h:52m:15s remains)
INFO - root - 2017-12-06 10:13:39.935072: step 22140, loss = 0.82, batch loss = 0.75 (12.7 examples/sec; 0.630 sec/batch; 54h:18m:38s remains)
INFO - root - 2017-12-06 10:13:45.973982: step 22150, loss = 0.83, batch loss = 0.76 (12.7 examples/sec; 0.628 sec/batch; 54h:06m:45s remains)
INFO - root - 2017-12-06 10:13:51.984598: step 22160, loss = 0.95, batch loss = 0.88 (13.5 examples/sec; 0.592 sec/batch; 51h:02m:03s remains)
INFO - root - 2017-12-06 10:13:58.107910: step 22170, loss = 0.90, batch loss = 0.83 (13.1 examples/sec; 0.611 sec/batch; 52h:40m:47s remains)
INFO - root - 2017-12-06 10:14:04.216546: step 22180, loss = 1.00, batch loss = 0.93 (13.3 examples/sec; 0.600 sec/batch; 51h:43m:22s remains)
INFO - root - 2017-12-06 10:14:10.374970: step 22190, loss = 0.97, batch loss = 0.90 (13.0 examples/sec; 0.615 sec/batch; 53h:01m:25s remains)
INFO - root - 2017-12-06 10:14:16.550333: step 22200, loss = 0.92, batch loss = 0.85 (12.8 examples/sec; 0.624 sec/batch; 53h:44m:48s remains)
2017-12-06 10:14:17.164871: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4412231 -4.5473204 -4.5423017 -4.5746207 -4.4148207 -4.039609 -3.5468807 -3.2019863 -3.1038871 -3.1543722 -3.3646955 -3.6698132 -3.9510231 -4.1452103 -4.1711307][-4.5668473 -4.8235712 -4.7732716 -4.6634746 -4.4074268 -4.0132933 -3.51732 -3.1970453 -3.2519875 -3.5087 -3.8513334 -4.1648717 -4.3795285 -4.5456319 -4.635778][-4.749001 -5.2152257 -5.1605387 -4.8529892 -4.4288778 -3.9956045 -3.5279264 -3.2417378 -3.3994212 -3.8148293 -4.2337008 -4.4899974 -4.5470548 -4.5755763 -4.6381292][-4.8638859 -5.4738073 -5.4469676 -5.0055122 -4.4110813 -3.8872473 -3.4193449 -3.1121063 -3.2944579 -3.8364513 -4.3276672 -4.5340552 -4.436408 -4.2700472 -4.2190151][-4.6155548 -5.2362881 -5.2509418 -4.8033481 -4.1322966 -3.5028448 -3.0154722 -2.6982081 -2.9305761 -3.6407113 -4.2231464 -4.3964243 -4.2028933 -3.861537 -3.6389332][-4.0086637 -4.5411787 -4.5584688 -4.1571836 -3.4902229 -2.7950759 -2.320864 -2.095094 -2.5008907 -3.4576464 -4.1415386 -4.2704263 -4.0215878 -3.539737 -3.0777116][-3.0473189 -3.4897375 -3.5360093 -3.2180531 -2.6114125 -1.9460998 -1.5467083 -1.484077 -2.0954056 -3.2595804 -4.0684185 -4.2097445 -3.9974985 -3.4751568 -2.7913489][-2.3003352 -2.711587 -2.8490057 -2.6309114 -2.1432137 -1.6078587 -1.3091691 -1.3738322 -2.0914457 -3.3026001 -4.1709414 -4.3325768 -4.2075853 -3.7878654 -3.0352726][-2.3512394 -2.7263622 -3.04049 -3.0752809 -2.856986 -2.4454374 -2.089355 -2.0199316 -2.5279117 -3.5390978 -4.3555665 -4.547554 -4.5434937 -4.3336225 -3.7108881][-2.7283583 -2.9941797 -3.4811082 -3.8408422 -3.9564071 -3.6568375 -3.1851957 -2.8693652 -3.0040088 -3.6431766 -4.307354 -4.5577712 -4.7029753 -4.7204261 -4.3141966][-3.0140967 -3.1020095 -3.598074 -4.1385584 -4.4885268 -4.3470187 -3.9396064 -3.5318408 -3.3504243 -3.5638568 -3.9852808 -4.2885003 -4.57847 -4.7649736 -4.5566921][-3.5595305 -3.4462981 -3.7269483 -4.1607251 -4.5244212 -4.4977117 -4.2268476 -3.8292751 -3.4539864 -3.3347321 -3.5238121 -3.8580828 -4.2359314 -4.4970865 -4.4202614][-4.3228807 -4.074791 -4.0766892 -4.2669888 -4.51058 -4.5315495 -4.3762665 -4.0625134 -3.6753936 -3.4144907 -3.4285738 -3.6680422 -3.988471 -4.21675 -4.2042947][-4.84927 -4.6022549 -4.4607005 -4.4811406 -4.5922532 -4.6001086 -4.5308571 -4.3611951 -4.10439 -3.8665776 -3.7775457 -3.8467083 -4.001492 -4.1363277 -4.167913][-4.9601092 -4.7681441 -4.5956664 -4.5223527 -4.5131626 -4.4859567 -4.4861174 -4.4858675 -4.4145665 -4.2868137 -4.1842208 -4.1374035 -4.1513705 -4.2147369 -4.2704124]]...]
INFO - root - 2017-12-06 10:14:23.245408: step 22210, loss = 0.94, batch loss = 0.87 (13.8 examples/sec; 0.582 sec/batch; 50h:08m:23s remains)
INFO - root - 2017-12-06 10:14:29.390832: step 22220, loss = 0.86, batch loss = 0.79 (12.6 examples/sec; 0.632 sec/batch; 54h:30m:31s remains)
INFO - root - 2017-12-06 10:14:35.487864: step 22230, loss = 1.11, batch loss = 1.04 (13.1 examples/sec; 0.609 sec/batch; 52h:29m:54s remains)
INFO - root - 2017-12-06 10:14:41.393588: step 22240, loss = 0.78, batch loss = 0.71 (13.4 examples/sec; 0.596 sec/batch; 51h:22m:44s remains)
INFO - root - 2017-12-06 10:14:47.237589: step 22250, loss = 0.93, batch loss = 0.86 (13.7 examples/sec; 0.586 sec/batch; 50h:29m:29s remains)
INFO - root - 2017-12-06 10:14:53.279528: step 22260, loss = 0.74, batch loss = 0.67 (13.2 examples/sec; 0.606 sec/batch; 52h:10m:53s remains)
INFO - root - 2017-12-06 10:14:59.425118: step 22270, loss = 0.81, batch loss = 0.74 (12.8 examples/sec; 0.624 sec/batch; 53h:47m:02s remains)
INFO - root - 2017-12-06 10:15:05.551089: step 22280, loss = 0.86, batch loss = 0.79 (12.6 examples/sec; 0.633 sec/batch; 54h:31m:55s remains)
INFO - root - 2017-12-06 10:15:11.667424: step 22290, loss = 0.88, batch loss = 0.81 (13.7 examples/sec; 0.585 sec/batch; 50h:25m:01s remains)
INFO - root - 2017-12-06 10:15:17.759880: step 22300, loss = 0.87, batch loss = 0.80 (13.0 examples/sec; 0.617 sec/batch; 53h:09m:30s remains)
2017-12-06 10:15:18.402306: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.1164465 -6.488677 -6.4315534 -6.2496557 -5.8556828 -5.191896 -4.4774837 -4.2232833 -4.7759094 -5.2627444 -5.4661317 -5.5430551 -5.445528 -5.2061896 -4.6032233][-6.3900442 -6.8730245 -6.92622 -6.7814445 -6.3159547 -5.5603557 -4.71598 -4.331089 -4.9645844 -5.6289992 -5.86896 -5.8892403 -5.7569923 -5.4666357 -4.6314874][-6.4626207 -6.9456825 -7.0911837 -7.0025063 -6.4533539 -5.6199307 -4.7158375 -4.2377758 -4.8880515 -5.7065096 -6.0707989 -6.129674 -6.0162725 -5.6811872 -4.7376881][-6.1390386 -6.43792 -6.5210829 -6.3948197 -5.6875987 -4.6932125 -3.7372534 -3.2804403 -4.0127563 -5.0089273 -5.5996561 -5.8850307 -5.9651933 -5.7262549 -4.9456415][-5.5930395 -5.5555491 -5.3531013 -4.9669695 -3.9557674 -2.6512589 -1.5750437 -1.2506702 -2.2173264 -3.436058 -4.2800937 -4.9104037 -5.3413997 -5.3362365 -4.9421797][-5.2370996 -4.8994513 -4.3676753 -3.6009023 -2.217397 -0.54217911 0.72612906 0.92506886 -0.30326176 -1.6982317 -2.6665387 -3.5717564 -4.3419518 -4.5684671 -4.5798779][-5.3810449 -4.970046 -4.3297124 -3.3245516 -1.6820011 0.24306536 1.7003641 1.9075303 0.58286142 -0.84130049 -1.7079973 -2.6191068 -3.5044007 -3.7912612 -4.0133038][-5.8825116 -5.6438994 -5.1992259 -4.2791505 -2.6831951 -0.84557533 0.59840012 0.92592669 -0.17731667 -1.3414214 -1.8461084 -2.4539835 -3.1874745 -3.3703146 -3.5843623][-6.3601737 -6.3666549 -6.2529316 -5.6680961 -4.3910918 -2.9097853 -1.6956115 -1.2399757 -1.8736351 -2.5485711 -2.626318 -2.874279 -3.3771834 -3.4636075 -3.6068189][-6.6059952 -6.8262224 -6.9790897 -6.7462692 -5.8652887 -4.7899742 -3.8762324 -3.3800194 -3.5444088 -3.7232761 -3.5123754 -3.5387814 -3.895231 -3.9897172 -4.101572][-6.5519423 -6.9237065 -7.2186847 -7.2217155 -6.681612 -5.9309664 -5.2776341 -4.8353534 -4.7024288 -4.544497 -4.2097449 -4.1492352 -4.4178309 -4.5527492 -4.6751866][-6.1393652 -6.5736113 -6.8938465 -6.9882832 -6.6876955 -6.1967134 -5.7614622 -5.4481406 -5.2410436 -4.9814949 -4.6767387 -4.6193237 -4.8037333 -4.9234829 -5.0320187][-5.492588 -5.8767867 -6.1254921 -6.20522 -6.0533814 -5.7743759 -5.5341434 -5.400497 -5.3092766 -5.1453142 -4.96843 -4.9488153 -5.0293984 -5.0393767 -5.0376334][-4.879446 -5.1221933 -5.2393746 -5.2382216 -5.14115 -5.00662 -4.9154325 -4.9371309 -5.0061407 -5.0091515 -4.9809732 -5.0132613 -5.0301127 -4.9498758 -4.8343639][-4.5052323 -4.6129413 -4.6233807 -4.5547819 -4.4628153 -4.3890419 -4.3606305 -4.43051 -4.5574012 -4.6443357 -4.6965356 -4.7568865 -4.771347 -4.7042441 -4.588438]]...]
INFO - root - 2017-12-06 10:15:24.493632: step 22310, loss = 0.85, batch loss = 0.78 (13.0 examples/sec; 0.617 sec/batch; 53h:08m:15s remains)
INFO - root - 2017-12-06 10:15:30.552807: step 22320, loss = 0.81, batch loss = 0.74 (13.3 examples/sec; 0.601 sec/batch; 51h:45m:38s remains)
INFO - root - 2017-12-06 10:15:36.637314: step 22330, loss = 1.15, batch loss = 1.08 (13.3 examples/sec; 0.602 sec/batch; 51h:53m:57s remains)
INFO - root - 2017-12-06 10:15:42.714672: step 22340, loss = 1.27, batch loss = 1.20 (13.3 examples/sec; 0.602 sec/batch; 51h:52m:01s remains)
INFO - root - 2017-12-06 10:15:48.727468: step 22350, loss = 0.92, batch loss = 0.85 (14.1 examples/sec; 0.569 sec/batch; 48h:59m:56s remains)
INFO - root - 2017-12-06 10:15:54.615158: step 22360, loss = 0.96, batch loss = 0.89 (13.4 examples/sec; 0.597 sec/batch; 51h:23m:22s remains)
INFO - root - 2017-12-06 10:16:00.685613: step 22370, loss = 0.90, batch loss = 0.83 (13.2 examples/sec; 0.605 sec/batch; 52h:05m:05s remains)
INFO - root - 2017-12-06 10:16:06.682465: step 22380, loss = 0.86, batch loss = 0.78 (14.2 examples/sec; 0.563 sec/batch; 48h:28m:22s remains)
INFO - root - 2017-12-06 10:16:12.549583: step 22390, loss = 0.92, batch loss = 0.85 (13.3 examples/sec; 0.601 sec/batch; 51h:47m:04s remains)
INFO - root - 2017-12-06 10:16:18.654495: step 22400, loss = 0.85, batch loss = 0.78 (14.0 examples/sec; 0.573 sec/batch; 49h:22m:10s remains)
2017-12-06 10:16:19.285135: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0323691 -4.5362749 -5.0068412 -5.3284388 -5.4260511 -5.2199616 -4.9606042 -4.8716931 -5.0385375 -5.4141221 -5.7060971 -5.7838135 -5.7015457 -5.5774784 -5.5166869][-4.0499582 -4.5924435 -5.1083665 -5.485467 -5.6214375 -5.3632889 -4.9981685 -4.8088312 -4.9509306 -5.401988 -5.7731857 -5.8879924 -5.76575 -5.5966468 -5.5530128][-4.0697088 -4.6025095 -5.1051941 -5.4918141 -5.640028 -5.3191118 -4.8157239 -4.4604349 -4.4746256 -4.87873 -5.2231116 -5.3213024 -5.157012 -4.9906869 -5.0536413][-4.139328 -4.6402221 -5.0997014 -5.4662175 -5.6186442 -5.2596016 -4.6569676 -4.1513405 -3.9563322 -4.1555047 -4.3192344 -4.3029389 -4.038765 -3.8661463 -4.0616016][-4.321207 -4.7867074 -5.1545448 -5.4381828 -5.537303 -5.1134777 -4.4163785 -3.8059371 -3.4500473 -3.4223032 -3.3856089 -3.2145889 -2.8032923 -2.5973701 -2.8870115][-4.5638189 -4.9450259 -5.1347256 -5.2117739 -5.1394224 -4.5668297 -3.7299862 -3.071054 -2.7115107 -2.6237764 -2.5522184 -2.3296971 -1.8261433 -1.601907 -1.9321911][-4.7047043 -4.8905587 -4.7651424 -4.4965425 -4.1695085 -3.454221 -2.5203738 -1.9182463 -1.7337389 -1.7901387 -1.8502009 -1.6938014 -1.2213759 -1.0583038 -1.4367423][-4.7393093 -4.6776743 -4.1773205 -3.5085115 -2.9299693 -2.1691878 -1.2693527 -0.84499288 -0.95038962 -1.2559068 -1.5032027 -1.4513521 -1.1001465 -1.0522904 -1.4774523][-4.8124743 -4.630023 -3.9114838 -2.9855576 -2.2565825 -1.5076149 -0.67781663 -0.43204093 -0.77595377 -1.3019421 -1.7020142 -1.713196 -1.4984188 -1.5667782 -1.9949019][-4.9627652 -4.90211 -4.2762861 -3.3901606 -2.6769929 -1.8988507 -0.96453643 -0.6541183 -0.97248769 -1.5509818 -2.0646584 -2.1929548 -2.2069848 -2.4494519 -2.9070587][-5.0215268 -5.203526 -4.8852549 -4.2870803 -3.7467442 -2.9435916 -1.8007884 -1.2171841 -1.2869284 -1.7351234 -2.2840848 -2.5729084 -2.86726 -3.3107038 -3.8256772][-4.9294486 -5.2997403 -5.2996178 -5.0619955 -4.7888565 -4.0821323 -2.8646007 -2.0588357 -1.8871698 -2.1610789 -2.6435165 -3.0035827 -3.4190028 -3.860527 -4.276082][-4.7537112 -5.2124987 -5.4552693 -5.5326538 -5.5287828 -5.0339622 -3.96969 -3.1601605 -2.9057531 -3.066762 -3.4145441 -3.6989059 -3.9953811 -4.1966629 -4.3520026][-4.5253778 -4.9850111 -5.3713202 -5.6585412 -5.8386865 -5.5629826 -4.7695408 -4.1216416 -3.9226818 -4.0511966 -4.2724304 -4.4302311 -4.558219 -4.5239096 -4.4511633][-4.2441349 -4.6299577 -5.054162 -5.4293575 -5.67257 -5.5184011 -4.9547129 -4.4817448 -4.3608203 -4.4768472 -4.6053228 -4.6580405 -4.675693 -4.5669651 -4.4569416]]...]
INFO - root - 2017-12-06 10:16:25.413779: step 22410, loss = 0.80, batch loss = 0.73 (12.9 examples/sec; 0.619 sec/batch; 53h:21m:22s remains)
INFO - root - 2017-12-06 10:16:31.424610: step 22420, loss = 0.88, batch loss = 0.81 (13.3 examples/sec; 0.602 sec/batch; 51h:52m:00s remains)
INFO - root - 2017-12-06 10:16:37.523285: step 22430, loss = 0.84, batch loss = 0.77 (13.0 examples/sec; 0.615 sec/batch; 52h:57m:11s remains)
INFO - root - 2017-12-06 10:16:43.599534: step 22440, loss = 0.82, batch loss = 0.75 (12.8 examples/sec; 0.626 sec/batch; 53h:53m:08s remains)
INFO - root - 2017-12-06 10:16:49.703849: step 22450, loss = 0.67, batch loss = 0.60 (13.4 examples/sec; 0.595 sec/batch; 51h:15m:57s remains)
INFO - root - 2017-12-06 10:16:55.698510: step 22460, loss = 0.99, batch loss = 0.92 (17.9 examples/sec; 0.446 sec/batch; 38h:26m:36s remains)
INFO - root - 2017-12-06 10:17:01.710320: step 22470, loss = 1.02, batch loss = 0.95 (13.1 examples/sec; 0.612 sec/batch; 52h:40m:44s remains)
INFO - root - 2017-12-06 10:17:07.819765: step 22480, loss = 1.14, batch loss = 1.07 (13.1 examples/sec; 0.613 sec/batch; 52h:45m:50s remains)
INFO - root - 2017-12-06 10:17:13.914703: step 22490, loss = 0.92, batch loss = 0.85 (13.1 examples/sec; 0.611 sec/batch; 52h:36m:04s remains)
INFO - root - 2017-12-06 10:17:19.992403: step 22500, loss = 0.79, batch loss = 0.72 (13.4 examples/sec; 0.599 sec/batch; 51h:34m:21s remains)
2017-12-06 10:17:20.612520: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6236649 -4.582715 -4.5397525 -4.4938374 -4.452405 -4.4148908 -4.37714 -4.3408451 -4.3127952 -4.2819366 -4.239954 -4.2008314 -4.1829028 -4.1964583 -4.22165][-4.8375549 -4.8167758 -4.7768722 -4.7111974 -4.6418176 -4.5814509 -4.5350146 -4.4992776 -4.4686475 -4.4281974 -4.3655443 -4.2959762 -4.2518115 -4.2535515 -4.27412][-5.0082173 -5.0528121 -5.0560184 -4.9914484 -4.89477 -4.8022194 -4.7374721 -4.6991282 -4.6699629 -4.6353078 -4.582653 -4.5191679 -4.4706483 -4.4591975 -4.464098][-5.0107994 -5.1827459 -5.2851872 -5.247602 -5.1229849 -4.9856935 -4.8789744 -4.8206725 -4.8004327 -4.8002238 -4.8027148 -4.7969346 -4.7847614 -4.7713022 -4.7529607][-4.7017641 -5.0787888 -5.3531265 -5.3732066 -5.2032728 -4.9753175 -4.7688971 -4.6485333 -4.6566048 -4.7657895 -4.9081516 -5.0376787 -5.1298823 -5.1571326 -5.1232843][-4.0087519 -4.6509347 -5.1455951 -5.2450237 -4.9837737 -4.5884094 -4.2142177 -3.9926429 -4.051384 -4.3569045 -4.7426419 -5.0999293 -5.3717432 -5.4810748 -5.4278069][-3.0903974 -3.9749448 -4.6763678 -4.849535 -4.4512205 -3.8054628 -3.19573 -2.820796 -2.9282217 -3.4881172 -4.1912804 -4.8471637 -5.350688 -5.5628157 -5.4685559][-2.31594 -3.35464 -4.1814003 -4.3880506 -3.8394451 -2.911643 -2.0206108 -1.4339807 -1.5352492 -2.3222337 -3.32507 -4.263165 -4.9651213 -5.2485981 -5.0746236][-2.0428529 -3.086859 -3.8747525 -4.0279536 -3.3762898 -2.2495658 -1.1097879 -0.29152346 -0.32555771 -1.2505934 -2.4431815 -3.5425608 -4.3194742 -4.5883484 -4.2840118][-2.4794006 -3.37823 -3.9505644 -3.9461048 -3.28369 -2.1607053 -0.95955873 -0.027452469 0.051555634 -0.81802368 -1.9892035 -3.0587552 -3.7394693 -3.8924894 -3.439959][-3.4143729 -4.1058321 -4.4157391 -4.2310352 -3.6221185 -2.6703987 -1.6319904 -0.815305 -0.70397282 -1.3610942 -2.2793672 -3.107177 -3.5071325 -3.4297128 -2.8525696][-4.385407 -4.8614569 -4.9804511 -4.7019076 -4.1875043 -3.4730456 -2.7287059 -2.1684754 -2.1020398 -2.5197096 -3.1010866 -3.6063759 -3.6899581 -3.3751094 -2.7478614][-4.9090452 -5.2051668 -5.263967 -5.0484476 -4.69022 -4.2199864 -3.7803183 -3.4882362 -3.4761631 -3.6886334 -3.9764366 -4.2339835 -4.1429906 -3.7260742 -3.1440504][-4.9629965 -5.124877 -5.1999693 -5.1370206 -4.9497581 -4.6696005 -4.4261456 -4.2760987 -4.2565088 -4.3246069 -4.4290271 -4.5562253 -4.4785252 -4.1740184 -3.7650268][-4.8051529 -4.8877358 -4.9673467 -5.0081639 -4.9281507 -4.7523627 -4.5972657 -4.483458 -4.4111886 -4.3895555 -4.4204588 -4.5264487 -4.5871353 -4.5413995 -4.4081345]]...]
INFO - root - 2017-12-06 10:17:26.681911: step 22510, loss = 0.89, batch loss = 0.81 (13.3 examples/sec; 0.600 sec/batch; 51h:40m:32s remains)
INFO - root - 2017-12-06 10:17:32.722849: step 22520, loss = 1.15, batch loss = 1.08 (13.0 examples/sec; 0.618 sec/batch; 53h:11m:28s remains)
INFO - root - 2017-12-06 10:17:38.832602: step 22530, loss = 0.70, batch loss = 0.63 (13.2 examples/sec; 0.607 sec/batch; 52h:18m:06s remains)
INFO - root - 2017-12-06 10:17:44.836567: step 22540, loss = 1.05, batch loss = 0.98 (13.2 examples/sec; 0.606 sec/batch; 52h:11m:34s remains)
INFO - root - 2017-12-06 10:17:50.855051: step 22550, loss = 1.03, batch loss = 0.96 (13.7 examples/sec; 0.582 sec/batch; 50h:09m:00s remains)
INFO - root - 2017-12-06 10:17:56.941727: step 22560, loss = 0.86, batch loss = 0.79 (13.3 examples/sec; 0.602 sec/batch; 51h:48m:19s remains)
INFO - root - 2017-12-06 10:18:02.809338: step 22570, loss = 0.87, batch loss = 0.80 (12.8 examples/sec; 0.627 sec/batch; 53h:56m:14s remains)
INFO - root - 2017-12-06 10:18:08.873848: step 22580, loss = 1.03, batch loss = 0.96 (12.9 examples/sec; 0.621 sec/batch; 53h:25m:09s remains)
INFO - root - 2017-12-06 10:18:14.982713: step 22590, loss = 0.98, batch loss = 0.91 (13.7 examples/sec; 0.584 sec/batch; 50h:16m:20s remains)
INFO - root - 2017-12-06 10:18:21.042915: step 22600, loss = 0.77, batch loss = 0.70 (13.0 examples/sec; 0.613 sec/batch; 52h:46m:34s remains)
2017-12-06 10:18:21.678287: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2002678 -4.2996325 -4.1024804 -3.6651878 -3.0049615 -2.168349 -1.4305716 -1.2931659 -1.8017287 -2.6053972 -3.4044447 -4.0163431 -4.3632431 -4.4124565 -4.2952614][-4.3441095 -4.462728 -4.428998 -4.3046184 -4.0322347 -3.6034048 -3.1451087 -2.9534931 -3.1499372 -3.5762897 -4.0004606 -4.303493 -4.478775 -4.4889255 -4.3888745][-4.1224332 -4.3119588 -4.5123796 -4.7289619 -4.8290086 -4.7463436 -4.5248322 -4.3452978 -4.3880649 -4.5938816 -4.7422686 -4.7928166 -4.8121719 -4.742137 -4.5847611][-3.6719561 -3.9327843 -4.3394456 -4.7990866 -5.118247 -5.1808329 -4.9956174 -4.7647409 -4.7809849 -5.0095267 -5.1516609 -5.1825056 -5.1912074 -5.0863681 -4.8377156][-3.3106856 -3.5867169 -4.0570378 -4.5433745 -4.8307748 -4.7959204 -4.4466367 -4.0524888 -4.060822 -4.4663377 -4.8648057 -5.1574836 -5.3641768 -5.3465834 -5.0571518][-3.2018328 -3.4556348 -3.8421032 -4.1333642 -4.1298747 -3.7717905 -3.1140571 -2.484118 -2.4721789 -3.1103234 -3.8768716 -4.5734863 -5.1169953 -5.326632 -5.1268735][-3.3298309 -3.5809834 -3.85596 -3.8830032 -3.4922669 -2.7158084 -1.6932209 -0.77452135 -0.67269754 -1.4680169 -2.5535164 -3.6193931 -4.4951711 -4.9946027 -5.0085864][-3.6232624 -3.9159667 -4.1471291 -4.0265551 -3.4130571 -2.4055262 -1.1754398 -0.0080528259 0.32872629 -0.37424564 -1.548466 -2.7965388 -3.8488874 -4.5472522 -4.7702656][-3.9794805 -4.3326836 -4.6027169 -4.5156145 -3.9596844 -3.0507464 -1.9219725 -0.75310326 -0.18816805 -0.52952337 -1.4568567 -2.5778625 -3.5396109 -4.219162 -4.5222178][-4.2480755 -4.6399465 -4.9963226 -5.0758038 -4.7846007 -4.1877403 -3.3554025 -2.3989737 -1.7606966 -1.7369683 -2.2470248 -3.0020685 -3.6619782 -4.1317477 -4.3709173][-4.28693 -4.682879 -5.1465673 -5.4559069 -5.47869 -5.1937127 -4.6430531 -3.9767835 -3.4665685 -3.2958026 -3.4638457 -3.8201218 -4.1403427 -4.3385315 -4.4209118][-4.1246796 -4.4920139 -5.0078425 -5.4618969 -5.7079849 -5.6325188 -5.2444625 -4.8064337 -4.5441875 -4.4959974 -4.5622106 -4.6742792 -4.7652369 -4.7497725 -4.6539607][-3.8910396 -4.1834626 -4.6243176 -5.0207686 -5.3132524 -5.3096209 -4.9461722 -4.5903773 -4.5488634 -4.7788706 -5.0049219 -5.1547728 -5.2410278 -5.1637859 -4.9435887][-3.7037745 -3.8883579 -4.1519737 -4.3099022 -4.4402437 -4.350069 -3.8811941 -3.4526739 -3.525661 -4.054606 -4.6039848 -5.0353975 -5.3464174 -5.3975019 -5.1719418][-3.6255729 -3.6931219 -3.7756615 -3.6741145 -3.5412662 -3.2159362 -2.5046895 -1.8574739 -1.9152279 -2.6778221 -3.5693769 -4.3670855 -4.9970031 -5.3035922 -5.2094984]]...]
INFO - root - 2017-12-06 10:18:27.682432: step 22610, loss = 1.04, batch loss = 0.97 (13.5 examples/sec; 0.594 sec/batch; 51h:09m:22s remains)
INFO - root - 2017-12-06 10:18:33.804929: step 22620, loss = 1.09, batch loss = 1.02 (13.1 examples/sec; 0.612 sec/batch; 52h:38m:30s remains)
INFO - root - 2017-12-06 10:18:39.862695: step 22630, loss = 0.87, batch loss = 0.80 (13.8 examples/sec; 0.578 sec/batch; 49h:44m:26s remains)
INFO - root - 2017-12-06 10:18:45.939174: step 22640, loss = 0.92, batch loss = 0.85 (13.3 examples/sec; 0.602 sec/batch; 51h:48m:33s remains)
INFO - root - 2017-12-06 10:18:52.010774: step 22650, loss = 0.86, batch loss = 0.79 (13.4 examples/sec; 0.598 sec/batch; 51h:30m:18s remains)
INFO - root - 2017-12-06 10:18:58.063634: step 22660, loss = 0.88, batch loss = 0.81 (13.5 examples/sec; 0.594 sec/batch; 51h:05m:13s remains)
INFO - root - 2017-12-06 10:19:04.112895: step 22670, loss = 0.92, batch loss = 0.85 (13.7 examples/sec; 0.585 sec/batch; 50h:19m:37s remains)
INFO - root - 2017-12-06 10:19:10.085851: step 22680, loss = 0.94, batch loss = 0.87 (12.8 examples/sec; 0.625 sec/batch; 53h:46m:40s remains)
INFO - root - 2017-12-06 10:19:16.049202: step 22690, loss = 0.81, batch loss = 0.74 (13.4 examples/sec; 0.598 sec/batch; 51h:30m:12s remains)
INFO - root - 2017-12-06 10:19:22.091668: step 22700, loss = 0.83, batch loss = 0.76 (13.5 examples/sec; 0.592 sec/batch; 50h:54m:11s remains)
2017-12-06 10:19:22.711944: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.167449 -4.1082697 -4.1228824 -4.1533794 -4.1750336 -4.1213593 -3.8794813 -3.5172343 -3.2867928 -3.2712076 -3.4683857 -3.9769986 -4.4796276 -4.597363 -4.4333668][-4.2346134 -4.2566481 -4.352406 -4.391902 -4.3616629 -4.2099276 -3.8495898 -3.3960991 -3.1709821 -3.2456145 -3.5658708 -4.2234492 -4.8677511 -5.0584674 -4.8251286][-4.3847237 -4.5028472 -4.6722312 -4.6739855 -4.527812 -4.2106915 -3.6776614 -3.1505585 -3.027724 -3.3155317 -3.8006659 -4.545331 -5.2321053 -5.4477382 -5.1546483][-4.5463152 -4.71128 -4.9018044 -4.8297424 -4.551527 -4.052619 -3.2811503 -2.6288958 -2.6604829 -3.2638683 -3.9392109 -4.6895084 -5.3079486 -5.5013103 -5.2063475][-4.6645756 -4.8437696 -5.0200367 -4.8665075 -4.46906 -3.7971003 -2.7430229 -1.8362267 -1.9460983 -2.9177716 -3.8304312 -4.5509162 -5.0231361 -5.1716204 -4.948339][-4.7370892 -4.9489093 -5.1080894 -4.8801603 -4.3789358 -3.5669847 -2.2557809 -0.9768343 -0.973295 -2.2419209 -3.4443169 -4.1645346 -4.4592113 -4.5410924 -4.44365][-4.7502894 -5.02232 -5.1873379 -4.9174795 -4.3330436 -3.4502037 -2.0064862 -0.39925766 -0.12461567 -1.5381124 -3.0308294 -3.8145516 -3.9496963 -3.9309356 -3.9329932][-4.6980987 -5.0185456 -5.2128592 -4.9547129 -4.3605103 -3.5405092 -2.2421961 -0.66644382 -0.21147776 -1.5147324 -3.0656295 -3.8121541 -3.7539358 -3.5733438 -3.5945191][-4.6118321 -4.9404278 -5.1676297 -4.971509 -4.46536 -3.8525572 -2.9564323 -1.7754979 -1.3362811 -2.2634103 -3.483772 -3.9901311 -3.7176247 -3.3679683 -3.3542991][-4.5325084 -4.8214669 -5.0457106 -4.9164934 -4.5317931 -4.1356974 -3.6627297 -2.9627156 -2.6046953 -3.0692906 -3.7811193 -4.0056324 -3.6331062 -3.2230766 -3.1673717][-4.487679 -4.7147183 -4.892272 -4.8096933 -4.5368767 -4.286562 -4.0765457 -3.7278056 -3.4730921 -3.6128731 -3.9411864 -3.9987495 -3.6874194 -3.3507991 -3.2909148][-4.465414 -4.643342 -4.775002 -4.7461748 -4.5875955 -4.4495516 -4.3838758 -4.2491946 -4.091351 -4.0969143 -4.2299314 -4.2163825 -4.0328345 -3.838748 -3.8170033][-4.4454136 -4.5997734 -4.7254872 -4.7678962 -4.73318 -4.70126 -4.718214 -4.6999564 -4.6250138 -4.612071 -4.6545091 -4.5806689 -4.4533777 -4.3661942 -4.3996749][-4.4174638 -4.5501242 -4.6770811 -4.7681537 -4.8253994 -4.8711843 -4.9345036 -4.9758668 -4.9578171 -4.9507976 -4.9382877 -4.8157654 -4.6896076 -4.6535048 -4.7226715][-4.3725986 -4.4671526 -4.5680227 -4.6633682 -4.7523155 -4.8246703 -4.895196 -4.9495091 -4.9528933 -4.94791 -4.926415 -4.8284264 -4.7384677 -4.7311916 -4.7886958]]...]
INFO - root - 2017-12-06 10:19:28.729614: step 22710, loss = 0.78, batch loss = 0.71 (13.5 examples/sec; 0.592 sec/batch; 50h:57m:45s remains)
INFO - root - 2017-12-06 10:19:34.763647: step 22720, loss = 0.83, batch loss = 0.76 (13.9 examples/sec; 0.577 sec/batch; 49h:40m:42s remains)
INFO - root - 2017-12-06 10:19:40.863771: step 22730, loss = 0.86, batch loss = 0.79 (13.4 examples/sec; 0.598 sec/batch; 51h:26m:01s remains)
INFO - root - 2017-12-06 10:19:46.873586: step 22740, loss = 1.30, batch loss = 1.23 (13.1 examples/sec; 0.609 sec/batch; 52h:26m:24s remains)
INFO - root - 2017-12-06 10:19:52.926189: step 22750, loss = 0.86, batch loss = 0.79 (13.9 examples/sec; 0.576 sec/batch; 49h:33m:12s remains)
INFO - root - 2017-12-06 10:19:58.926850: step 22760, loss = 0.71, batch loss = 0.64 (13.4 examples/sec; 0.595 sec/batch; 51h:10m:51s remains)
INFO - root - 2017-12-06 10:20:04.889939: step 22770, loss = 1.15, batch loss = 1.08 (13.7 examples/sec; 0.585 sec/batch; 50h:21m:10s remains)
INFO - root - 2017-12-06 10:20:10.965082: step 22780, loss = 1.00, batch loss = 0.93 (13.6 examples/sec; 0.588 sec/batch; 50h:36m:02s remains)
INFO - root - 2017-12-06 10:20:16.828783: step 22790, loss = 0.81, batch loss = 0.74 (13.6 examples/sec; 0.589 sec/batch; 50h:38m:45s remains)
INFO - root - 2017-12-06 10:20:22.930773: step 22800, loss = 1.02, batch loss = 0.95 (13.5 examples/sec; 0.591 sec/batch; 50h:51m:16s remains)
2017-12-06 10:20:23.533129: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9734213 -4.2235284 -4.5210052 -4.6715655 -4.5477891 -4.274703 -4.1093826 -4.2329659 -4.4892344 -4.6527619 -4.5530925 -4.3286548 -4.1982861 -4.1926317 -4.2944551][-4.5173507 -4.6948767 -4.94569 -5.098793 -4.9183197 -4.5400715 -4.299572 -4.4340444 -4.8141074 -5.0615597 -4.9630251 -4.6848736 -4.5269961 -4.5364265 -4.6245351][-5.0837588 -5.2190213 -5.3729448 -5.4226222 -5.1127906 -4.6192665 -4.3493 -4.5547295 -5.1189861 -5.4890432 -5.38681 -5.0528474 -4.871654 -4.8612318 -4.9095449][-5.408741 -5.471806 -5.4730139 -5.3486485 -4.8452091 -4.213706 -3.9389906 -4.2981381 -5.1667314 -5.7663865 -5.7043514 -5.3339825 -5.12169 -5.0378847 -5.0078573][-5.3758707 -5.2565489 -5.0584083 -4.737967 -4.0166454 -3.256186 -2.9971857 -3.5612206 -4.8108664 -5.7397141 -5.8098183 -5.4675403 -5.224791 -5.0296688 -4.8928108][-5.0968342 -4.7190542 -4.3410773 -3.8600855 -2.9494338 -2.0986893 -1.843195 -2.580431 -4.182013 -5.4489231 -5.6992517 -5.4158983 -5.146162 -4.8501167 -4.6251431][-4.7178316 -4.1247606 -3.6502993 -3.0966392 -2.0988595 -1.237118 -0.95451617 -1.7600539 -3.5701227 -5.0817065 -5.4704738 -5.1939321 -4.8854122 -4.5399332 -4.2762237][-4.43636 -3.7213101 -3.2220583 -2.6679826 -1.7158146 -0.92928195 -0.60536242 -1.3723185 -3.213474 -4.8164024 -5.2236595 -4.87437 -4.52969 -4.2027187 -3.9534328][-4.3499928 -3.6137269 -3.130209 -2.6236198 -1.8281009 -1.1698849 -0.805609 -1.4444749 -3.1297483 -4.6345787 -4.9610109 -4.5423107 -4.217979 -3.9779937 -3.7837238][-4.4298062 -3.7300847 -3.2768817 -2.8340211 -2.2320096 -1.7246926 -1.3761687 -1.8625317 -3.2225862 -4.4481759 -4.6380677 -4.2140512 -3.9834034 -3.8614473 -3.7494955][-4.5508928 -3.9217143 -3.5029476 -3.1058037 -2.6601319 -2.3083844 -2.0764909 -2.4498138 -3.4186075 -4.2972779 -4.3688035 -4.0027595 -3.8619206 -3.8029833 -3.7442758][-4.6219192 -4.1058598 -3.726104 -3.356827 -3.0236163 -2.8435988 -2.8338063 -3.1774158 -3.7883909 -4.3095946 -4.267529 -3.969166 -3.8622568 -3.7851295 -3.7489362][-4.6204286 -4.2503076 -3.9319572 -3.5933614 -3.3478136 -3.3425875 -3.5940306 -3.9831233 -4.3558135 -4.5686469 -4.4144855 -4.1683745 -4.0387273 -3.8867769 -3.8504472][-4.5982137 -4.3860292 -4.154315 -3.8759282 -3.7186284 -3.8766885 -4.3459363 -4.8143115 -5.0570703 -5.0259113 -4.7760563 -4.5815845 -4.4112682 -4.1826024 -4.1239986][-4.5162477 -4.4248405 -4.2766337 -4.1033888 -4.0659504 -4.3789997 -4.9867897 -5.4996476 -5.6541004 -5.4476175 -5.1484246 -5.0426 -4.8785014 -4.6231036 -4.540132]]...]
INFO - root - 2017-12-06 10:20:29.639796: step 22810, loss = 0.91, batch loss = 0.84 (12.7 examples/sec; 0.629 sec/batch; 54h:04m:24s remains)
INFO - root - 2017-12-06 10:20:35.600991: step 22820, loss = 0.97, batch loss = 0.90 (12.9 examples/sec; 0.621 sec/batch; 53h:25m:26s remains)
INFO - root - 2017-12-06 10:20:41.677985: step 22830, loss = 1.16, batch loss = 1.09 (13.9 examples/sec; 0.574 sec/batch; 49h:21m:28s remains)
INFO - root - 2017-12-06 10:20:47.583864: step 22840, loss = 1.06, batch loss = 0.99 (13.4 examples/sec; 0.596 sec/batch; 51h:18m:08s remains)
INFO - root - 2017-12-06 10:20:53.631056: step 22850, loss = 0.71, batch loss = 0.64 (13.3 examples/sec; 0.601 sec/batch; 51h:42m:47s remains)
INFO - root - 2017-12-06 10:20:59.694595: step 22860, loss = 1.43, batch loss = 1.36 (13.4 examples/sec; 0.599 sec/batch; 51h:29m:30s remains)
INFO - root - 2017-12-06 10:21:05.697026: step 22870, loss = 0.73, batch loss = 0.66 (13.5 examples/sec; 0.593 sec/batch; 50h:58m:30s remains)
INFO - root - 2017-12-06 10:21:11.847439: step 22880, loss = 1.04, batch loss = 0.97 (13.5 examples/sec; 0.593 sec/batch; 50h:59m:46s remains)
INFO - root - 2017-12-06 10:21:17.849622: step 22890, loss = 1.35, batch loss = 1.28 (17.0 examples/sec; 0.470 sec/batch; 40h:26m:03s remains)
INFO - root - 2017-12-06 10:21:23.934075: step 22900, loss = 0.76, batch loss = 0.69 (13.1 examples/sec; 0.610 sec/batch; 52h:29m:46s remains)
2017-12-06 10:21:24.514210: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.757946 -4.95035 -5.1026821 -5.17891 -5.1854172 -5.165092 -5.1623192 -5.1592278 -5.1447082 -5.1391273 -5.1258183 -5.0882416 -5.0161767 -4.8844681 -4.7227798][-5.2921004 -5.46411 -5.5628195 -5.5676403 -5.512392 -5.4822469 -5.5209913 -5.5526185 -5.5382676 -5.5094357 -5.4638772 -5.4281669 -5.4032373 -5.3043594 -5.1380959][-5.5954089 -5.7305536 -5.7744808 -5.7030172 -5.5476365 -5.4522157 -5.499897 -5.5833063 -5.6340561 -5.6405287 -5.5664396 -5.5242877 -5.5801697 -5.5711994 -5.4536943][-5.4585218 -5.5773363 -5.5887156 -5.4175234 -5.0782342 -4.8247819 -4.8284407 -4.9905663 -5.2436481 -5.4308672 -5.3631711 -5.283843 -5.4029174 -5.4875684 -5.4245315][-4.936923 -5.0795021 -5.0904703 -4.795527 -4.20064 -3.7052085 -3.5827255 -3.7913461 -4.3450575 -4.8602571 -4.8759551 -4.7629404 -4.8944321 -5.00721 -4.9415][-4.2072992 -4.4784279 -4.6148372 -4.2545252 -3.3762999 -2.4991703 -2.0058122 -2.0671692 -2.9137998 -3.8870528 -4.1595974 -4.1351886 -4.2764106 -4.335463 -4.1918373][-3.5547831 -4.0208359 -4.3919606 -4.0533237 -2.9430976 -1.6124244 -0.55196786 -0.30323219 -1.3830805 -2.86651 -3.5692258 -3.801842 -3.9795294 -3.9132326 -3.6251335][-3.206841 -3.763742 -4.2864447 -3.9998507 -2.8232565 -1.2278841 0.25963163 0.77335548 -0.39871454 -2.2156506 -3.3398333 -3.8955903 -4.1229615 -3.9127436 -3.481585][-3.1693215 -3.6804214 -4.220397 -4.0442691 -3.0936933 -1.683203 -0.23841095 0.32471895 -0.68892956 -2.4302897 -3.7126405 -4.4566488 -4.675055 -4.3172932 -3.7756257][-3.6485894 -4.0353684 -4.4771795 -4.4060078 -3.7904515 -2.7968175 -1.7101891 -1.2788596 -2.0434091 -3.442389 -4.6092014 -5.3426766 -5.484941 -5.0052109 -4.3887734][-4.2685695 -4.490911 -4.7615304 -4.723259 -4.3765388 -3.8078208 -3.1742454 -2.9614291 -3.5003545 -4.4579058 -5.2998085 -5.8513117 -5.8974638 -5.3980255 -4.8075123][-4.4623818 -4.5393991 -4.652863 -4.6038494 -4.4212594 -4.1745963 -3.9220951 -3.8986018 -4.2553382 -4.8036337 -5.2664671 -5.5815825 -5.5660906 -5.1431589 -4.6607347][-4.2606549 -4.2609982 -4.2754445 -4.2031312 -4.0892706 -4.0084739 -3.9503794 -3.9954937 -4.204699 -4.4623923 -4.6481462 -4.7847981 -4.7483187 -4.4426184 -4.0992317][-3.80843 -3.7651947 -3.7084668 -3.6044726 -3.5052457 -3.4655235 -3.4451618 -3.4729843 -3.5877213 -3.7008822 -3.7696488 -3.8372087 -3.8164096 -3.6295927 -3.4298713][-3.2583542 -3.1808989 -3.0826511 -2.9800739 -2.914124 -2.8891444 -2.8629656 -2.8615656 -2.9203048 -2.9741743 -3.0218639 -3.0876546 -3.0987189 -3.0143051 -2.9396646]]...]
INFO - root - 2017-12-06 10:21:30.555546: step 22910, loss = 0.82, batch loss = 0.75 (12.9 examples/sec; 0.619 sec/batch; 53h:12m:01s remains)
INFO - root - 2017-12-06 10:21:36.603746: step 22920, loss = 0.82, batch loss = 0.75 (12.8 examples/sec; 0.626 sec/batch; 53h:47m:44s remains)
INFO - root - 2017-12-06 10:21:42.686306: step 22930, loss = 0.84, batch loss = 0.77 (12.9 examples/sec; 0.618 sec/batch; 53h:08m:17s remains)
INFO - root - 2017-12-06 10:21:48.698934: step 22940, loss = 0.98, batch loss = 0.91 (13.0 examples/sec; 0.616 sec/batch; 52h:58m:27s remains)
INFO - root - 2017-12-06 10:21:54.812262: step 22950, loss = 0.90, batch loss = 0.83 (12.9 examples/sec; 0.621 sec/batch; 53h:21m:16s remains)
INFO - root - 2017-12-06 10:22:00.845050: step 22960, loss = 0.97, batch loss = 0.90 (13.5 examples/sec; 0.592 sec/batch; 50h:52m:21s remains)
INFO - root - 2017-12-06 10:22:06.861957: step 22970, loss = 0.77, batch loss = 0.70 (13.3 examples/sec; 0.603 sec/batch; 51h:50m:26s remains)
INFO - root - 2017-12-06 10:22:12.998328: step 22980, loss = 0.68, batch loss = 0.61 (13.2 examples/sec; 0.605 sec/batch; 52h:00m:59s remains)
INFO - root - 2017-12-06 10:22:18.873737: step 22990, loss = 0.96, batch loss = 0.89 (13.4 examples/sec; 0.597 sec/batch; 51h:17m:12s remains)
INFO - root - 2017-12-06 10:22:24.828168: step 23000, loss = 0.92, batch loss = 0.85 (12.7 examples/sec; 0.629 sec/batch; 54h:02m:53s remains)
2017-12-06 10:22:25.542985: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4592738 -5.0032763 -4.9706125 -4.2849483 -3.2130618 -2.5592039 -2.6830595 -3.1121633 -3.2335963 -3.1466885 -3.4025836 -3.6641302 -3.673825 -3.8067155 -4.0836811][-4.6639352 -5.2174015 -5.2663679 -4.7621722 -3.8421721 -3.0746174 -2.9155889 -3.2418656 -3.5247352 -3.67034 -4.0630021 -4.3786588 -4.3694005 -4.3491931 -4.3054519][-4.5760818 -5.1662979 -5.3345151 -5.0213923 -4.2231832 -3.3507314 -2.9213779 -3.0819361 -3.4255857 -3.7369194 -4.2344103 -4.6244621 -4.68529 -4.6182232 -4.33786][-4.5088854 -5.0949497 -5.3113885 -5.0785818 -4.2679248 -3.2468896 -2.5908036 -2.6068904 -2.9687815 -3.3958678 -3.9570594 -4.3777013 -4.5045214 -4.471663 -4.1263804][-4.7418141 -5.2472987 -5.4041061 -5.1539536 -4.2624578 -3.1142673 -2.3319719 -2.2898321 -2.683213 -3.1696544 -3.7578633 -4.2044106 -4.4171653 -4.4920073 -4.2258487][-5.2830868 -5.67655 -5.7413664 -5.4371881 -4.4493008 -3.205987 -2.4292068 -2.4633579 -2.9367232 -3.4263573 -3.9275317 -4.3267984 -4.6224728 -4.8817019 -4.8165593][-5.9142823 -6.1931448 -6.1769466 -5.7753367 -4.5925364 -3.2158616 -2.5017524 -2.7262836 -3.3697562 -3.8762383 -4.2490067 -4.5651422 -4.9301882 -5.3828473 -5.5371785][-6.1415558 -6.3095369 -6.2328262 -5.72872 -4.3228006 -2.7870061 -2.1533732 -2.6350091 -3.53292 -4.1265535 -4.4148922 -4.6345286 -5.0191388 -5.5805025 -5.8543711][-5.9044924 -5.9173069 -5.7312984 -5.1524043 -3.6486492 -2.0688252 -1.5328035 -2.2408826 -3.37317 -4.0858774 -4.3894181 -4.597394 -5.0028954 -5.5612288 -5.774188][-5.192462 -5.0775356 -4.823216 -4.2982097 -2.9621091 -1.5456762 -1.0916684 -1.8682122 -3.08354 -3.8817701 -4.2957311 -4.6186919 -5.0767403 -5.5445395 -5.55225][-4.4420328 -4.2661052 -4.0515389 -3.7681234 -2.8574042 -1.7934911 -1.4000032 -2.0193815 -3.1010852 -3.8998623 -4.4077926 -4.8108573 -5.2175236 -5.473875 -5.2508297][-4.0880675 -3.8359356 -3.6976285 -3.7729428 -3.4491754 -2.8801446 -2.558578 -2.8706152 -3.6398563 -4.2719364 -4.673923 -4.9273243 -5.0766492 -5.0477314 -4.6971579][-3.8880398 -3.562463 -3.4695463 -3.8982947 -4.182425 -4.1394415 -3.9281909 -3.9110775 -4.2571716 -4.5566278 -4.6937008 -4.6903973 -4.5570922 -4.2982497 -3.9362676][-3.65055 -3.2349007 -3.1052954 -3.7561667 -4.5020189 -4.8767681 -4.8120565 -4.60102 -4.6088085 -4.613976 -4.5483847 -4.3902082 -4.0711212 -3.6705508 -3.3562803][-3.2334919 -2.7262065 -2.5653639 -3.3631029 -4.4118915 -5.0240645 -5.06192 -4.7882648 -4.6355929 -4.4920659 -4.3545794 -4.1624541 -3.7478187 -3.268795 -3.0022058]]...]
INFO - root - 2017-12-06 10:22:31.616821: step 23010, loss = 0.96, batch loss = 0.89 (13.0 examples/sec; 0.617 sec/batch; 53h:04m:31s remains)
INFO - root - 2017-12-06 10:22:37.702026: step 23020, loss = 1.00, batch loss = 0.93 (13.9 examples/sec; 0.574 sec/batch; 49h:21m:09s remains)
INFO - root - 2017-12-06 10:22:43.720568: step 23030, loss = 1.16, batch loss = 1.09 (13.3 examples/sec; 0.600 sec/batch; 51h:36m:31s remains)
INFO - root - 2017-12-06 10:22:49.759903: step 23040, loss = 0.78, batch loss = 0.71 (13.3 examples/sec; 0.600 sec/batch; 51h:32m:56s remains)
INFO - root - 2017-12-06 10:22:55.839060: step 23050, loss = 0.82, batch loss = 0.75 (13.2 examples/sec; 0.608 sec/batch; 52h:15m:39s remains)
INFO - root - 2017-12-06 10:23:01.854328: step 23060, loss = 0.94, batch loss = 0.87 (13.2 examples/sec; 0.605 sec/batch; 52h:02m:07s remains)
INFO - root - 2017-12-06 10:23:07.895798: step 23070, loss = 1.06, batch loss = 0.99 (13.8 examples/sec; 0.580 sec/batch; 49h:51m:41s remains)
INFO - root - 2017-12-06 10:23:14.000873: step 23080, loss = 0.97, batch loss = 0.90 (12.9 examples/sec; 0.619 sec/batch; 53h:09m:53s remains)
INFO - root - 2017-12-06 10:23:20.139148: step 23090, loss = 0.81, batch loss = 0.74 (13.3 examples/sec; 0.604 sec/batch; 51h:53m:09s remains)
INFO - root - 2017-12-06 10:23:26.203244: step 23100, loss = 0.98, batch loss = 0.91 (13.6 examples/sec; 0.590 sec/batch; 50h:43m:43s remains)
2017-12-06 10:23:26.812843: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.5078392 -6.2564278 -5.7728968 -5.3569174 -5.0650024 -5.0255065 -5.072391 -5.1255493 -5.3311439 -5.4983888 -5.6369467 -5.7060866 -5.6397166 -5.6331511 -5.611649][-6.5780344 -6.4510984 -6.0127459 -5.576508 -5.1886215 -5.0904207 -5.0992908 -5.0564132 -5.159915 -5.26837 -5.40271 -5.5200143 -5.4759741 -5.4471679 -5.3510127][-6.3749266 -6.3542747 -5.9597769 -5.522378 -5.0295043 -4.8251548 -4.8004713 -4.7225218 -4.7846618 -4.8598728 -4.9605613 -5.0888143 -5.0543966 -5.0209813 -4.8711047][-6.229816 -6.206687 -5.7033167 -5.1417775 -4.52177 -4.2479591 -4.2376776 -4.1428194 -4.1433325 -4.1598759 -4.212172 -4.329689 -4.3219252 -4.3373632 -4.1916623][-6.108089 -5.9487662 -5.2402043 -4.5176735 -3.8465765 -3.5884616 -3.534595 -3.2487116 -3.0300584 -2.9747329 -3.0790892 -3.3249779 -3.4884486 -3.6571052 -3.5849895][-5.8796544 -5.6285496 -4.8260508 -4.0356631 -3.3582745 -3.0308294 -2.7101636 -2.0276403 -1.5379622 -1.5767262 -1.9693465 -2.537672 -2.9834461 -3.3281524 -3.3603535][-5.6805196 -5.5385251 -4.8187079 -4.0314274 -3.2654061 -2.689507 -1.9300926 -0.870569 -0.27577162 -0.6179924 -1.4200408 -2.3381515 -3.0380359 -3.5145164 -3.6903629][-5.50544 -5.5610991 -4.9574804 -4.1703334 -3.3237274 -2.5814805 -1.6076188 -0.52778983 -0.11580467 -0.751276 -1.7831321 -2.8280647 -3.5891519 -4.0822263 -4.3913884][-5.2265658 -5.3376045 -4.7713528 -4.0056324 -3.2423666 -2.7139335 -2.0660045 -1.4426851 -1.3338273 -1.9085584 -2.7782407 -3.6435637 -4.2723517 -4.6570787 -4.9979148][-4.9751925 -4.9366865 -4.3338881 -3.6423979 -3.1165013 -3.0194349 -2.9246519 -2.7963991 -2.8059449 -3.0916533 -3.6181493 -4.211843 -4.6549149 -4.8816051 -5.1473126][-4.7981009 -4.6108117 -4.0251412 -3.4531124 -3.156951 -3.3654857 -3.5922272 -3.6828666 -3.6348128 -3.6896982 -3.9710176 -4.3603973 -4.6494675 -4.7370067 -4.9118619][-4.69817 -4.5349741 -4.0421743 -3.5780697 -3.4107337 -3.6559978 -3.8614264 -3.9653454 -3.9140227 -3.9501579 -4.1540022 -4.3994708 -4.6103964 -4.6417913 -4.7475314][-4.5953755 -4.6317887 -4.3052878 -3.900568 -3.7169328 -3.8040071 -3.8440173 -3.9313948 -4.0257397 -4.1670074 -4.3336334 -4.4457612 -4.6390266 -4.7005105 -4.7509742][-4.5269237 -4.7718372 -4.5521359 -4.09799 -3.7977762 -3.729398 -3.6831036 -3.7886491 -4.050631 -4.3108954 -4.461935 -4.4985633 -4.7291846 -4.9105988 -4.9535828][-4.7690611 -5.0931354 -4.8099489 -4.1719971 -3.7438345 -3.6380274 -3.6148138 -3.7042353 -3.9958034 -4.2870016 -4.43303 -4.4790969 -4.81648 -5.1507416 -5.238236]]...]
INFO - root - 2017-12-06 10:23:32.652720: step 23110, loss = 1.08, batch loss = 1.01 (12.9 examples/sec; 0.621 sec/batch; 53h:20m:41s remains)
INFO - root - 2017-12-06 10:23:38.675816: step 23120, loss = 0.97, batch loss = 0.90 (13.5 examples/sec; 0.591 sec/batch; 50h:45m:11s remains)
INFO - root - 2017-12-06 10:23:44.765712: step 23130, loss = 0.74, batch loss = 0.67 (13.2 examples/sec; 0.606 sec/batch; 52h:06m:57s remains)
INFO - root - 2017-12-06 10:23:50.765072: step 23140, loss = 0.82, batch loss = 0.75 (13.2 examples/sec; 0.606 sec/batch; 52h:03m:30s remains)
INFO - root - 2017-12-06 10:23:56.840980: step 23150, loss = 1.01, batch loss = 0.94 (13.4 examples/sec; 0.595 sec/batch; 51h:06m:54s remains)
INFO - root - 2017-12-06 10:24:02.767402: step 23160, loss = 0.92, batch loss = 0.85 (13.4 examples/sec; 0.598 sec/batch; 51h:22m:34s remains)
INFO - root - 2017-12-06 10:24:08.837428: step 23170, loss = 0.85, batch loss = 0.78 (12.9 examples/sec; 0.620 sec/batch; 53h:15m:11s remains)
INFO - root - 2017-12-06 10:24:14.907742: step 23180, loss = 1.04, batch loss = 0.97 (13.0 examples/sec; 0.616 sec/batch; 52h:54m:34s remains)
INFO - root - 2017-12-06 10:24:21.011415: step 23190, loss = 0.78, batch loss = 0.71 (12.7 examples/sec; 0.629 sec/batch; 54h:03m:12s remains)
INFO - root - 2017-12-06 10:24:27.127256: step 23200, loss = 0.95, batch loss = 0.88 (12.5 examples/sec; 0.640 sec/batch; 54h:57m:52s remains)
2017-12-06 10:24:27.775361: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6186872 -4.6004958 -4.5944428 -4.61425 -4.6793571 -4.769722 -4.8570566 -4.9140592 -4.9164205 -4.8598228 -4.7594128 -4.6400518 -4.534152 -4.461791 -4.4406705][-4.717783 -4.718791 -4.7424831 -4.7970114 -4.8909559 -4.9983535 -5.0977669 -5.1705456 -5.198998 -5.1662273 -5.0693626 -4.9224281 -4.7579937 -4.6116557 -4.52526][-4.8294144 -4.8616261 -4.9113045 -4.9791584 -5.0655365 -5.145195 -5.2142038 -5.2788987 -5.3475852 -5.3901067 -5.3613749 -5.2426405 -5.0491819 -4.8250241 -4.6426077][-4.8933682 -4.9433231 -4.9795656 -5.0078788 -5.0321584 -5.0267363 -5.0036297 -5.0020037 -5.083034 -5.2268815 -5.3426447 -5.3520794 -5.2213836 -4.9836006 -4.7275271][-4.8278112 -4.8636475 -4.843565 -4.7961717 -4.7272062 -4.5886903 -4.4034228 -4.2545891 -4.2829781 -4.5093141 -4.8171668 -5.0605445 -5.1236963 -4.9910693 -4.7364788][-4.6165681 -4.6171565 -4.5303073 -4.4014111 -4.2058611 -3.8598032 -3.4269834 -3.0600135 -3.0020537 -3.3095169 -3.8376975 -4.3739119 -4.7252173 -4.8029675 -4.6401467][-4.3217258 -4.2939029 -4.1611161 -3.9610064 -3.60812 -2.9943271 -2.2742019 -1.699532 -1.6089821 -2.0216424 -2.75344 -3.5622361 -4.1934743 -4.5069718 -4.4837604][-4.0642381 -4.02947 -3.885231 -3.6429191 -3.154695 -2.3110936 -1.3761587 -0.69339633 -0.65020037 -1.1582429 -1.999079 -2.9609013 -3.7645779 -4.2541804 -4.3615856][-3.9415002 -3.9369223 -3.8259177 -3.5889046 -3.0508189 -2.125581 -1.1401012 -0.45858407 -0.4565115 -0.97890377 -1.7947862 -2.7605457 -3.5965142 -4.1602921 -4.3457112][-3.9656024 -4.03445 -4.0112381 -3.8485548 -3.3883023 -2.5640087 -1.690444 -1.0829256 -1.0587702 -1.4785352 -2.1427484 -2.9685693 -3.7085385 -4.2299056 -4.419271][-4.0807519 -4.2388134 -4.3316054 -4.2893281 -3.9873054 -3.3679838 -2.6763613 -2.16503 -2.074677 -2.3281395 -2.7805324 -3.391747 -3.9600737 -4.3595834 -4.4943662][-4.1719079 -4.4022403 -4.5941286 -4.66222 -4.5152745 -4.1026268 -3.5895932 -3.169632 -3.0156765 -3.1175585 -3.3837621 -3.7938108 -4.188271 -4.4479475 -4.5121431][-4.1933179 -4.4605455 -4.704627 -4.8295536 -4.7644887 -4.4771771 -4.0770364 -3.7235365 -3.5423188 -3.563822 -3.727731 -4.0132327 -4.2935348 -4.4576697 -4.478097][-4.1623192 -4.4365292 -4.6893 -4.8142638 -4.7481279 -4.4804115 -4.1032338 -3.7735121 -3.597446 -3.6175404 -3.7724528 -4.0304852 -4.2767482 -4.412456 -4.4328556][-4.0578179 -4.3282003 -4.5782213 -4.68923 -4.5939746 -4.2952614 -3.8895614 -3.5538239 -3.3999414 -3.4668043 -3.6770644 -3.9731128 -4.2390556 -4.3901062 -4.4364324]]...]
INFO - root - 2017-12-06 10:24:33.796824: step 23210, loss = 0.83, batch loss = 0.76 (13.2 examples/sec; 0.608 sec/batch; 52h:14m:48s remains)
INFO - root - 2017-12-06 10:24:39.890599: step 23220, loss = 1.11, batch loss = 1.04 (13.1 examples/sec; 0.609 sec/batch; 52h:18m:52s remains)
INFO - root - 2017-12-06 10:24:45.945095: step 23230, loss = 1.09, batch loss = 1.02 (13.2 examples/sec; 0.607 sec/batch; 52h:08m:27s remains)
INFO - root - 2017-12-06 10:24:52.003536: step 23240, loss = 1.00, batch loss = 0.93 (12.6 examples/sec; 0.637 sec/batch; 54h:44m:21s remains)
INFO - root - 2017-12-06 10:24:58.139275: step 23250, loss = 0.81, batch loss = 0.74 (13.3 examples/sec; 0.602 sec/batch; 51h:42m:11s remains)
INFO - root - 2017-12-06 10:25:04.191932: step 23260, loss = 0.84, batch loss = 0.77 (13.8 examples/sec; 0.580 sec/batch; 49h:50m:20s remains)
INFO - root - 2017-12-06 10:25:10.292788: step 23270, loss = 0.78, batch loss = 0.71 (13.2 examples/sec; 0.606 sec/batch; 52h:01m:42s remains)
INFO - root - 2017-12-06 10:25:16.440661: step 23280, loss = 0.96, batch loss = 0.89 (13.2 examples/sec; 0.607 sec/batch; 52h:06m:25s remains)
INFO - root - 2017-12-06 10:25:22.459829: step 23290, loss = 0.75, batch loss = 0.68 (13.1 examples/sec; 0.610 sec/batch; 52h:23m:36s remains)
INFO - root - 2017-12-06 10:25:28.552624: step 23300, loss = 0.97, batch loss = 0.90 (13.3 examples/sec; 0.601 sec/batch; 51h:34m:59s remains)
2017-12-06 10:25:29.164867: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3785057 -4.5103354 -4.6157994 -4.4592896 -4.0617604 -4.06473 -4.4790778 -4.733861 -4.8588071 -4.8720489 -4.5574455 -4.2437172 -4.1070585 -4.013144 -4.1384711][-4.2972875 -4.2591019 -4.3795233 -4.3830643 -4.11469 -4.1407466 -4.534071 -4.7827845 -4.8428683 -4.7216415 -4.3010054 -3.9472895 -3.8463986 -3.8706474 -4.1213179][-4.2809663 -4.2277026 -4.4019175 -4.5020514 -4.2831388 -4.2399936 -4.5071058 -4.6795282 -4.6530261 -4.4344325 -3.9926035 -3.6902118 -3.68548 -3.8433261 -4.1856785][-4.4017067 -4.4551635 -4.6985817 -4.8422585 -4.5879483 -4.3339243 -4.3250465 -4.3639641 -4.2788873 -4.0397892 -3.6840339 -3.5296345 -3.6726446 -3.9473298 -4.3191772][-4.6170363 -4.733036 -4.9263692 -4.9757652 -4.5927715 -4.06066 -3.7539909 -3.767118 -3.8034635 -3.7103927 -3.5390892 -3.5586433 -3.8159857 -4.1406045 -4.4773917][-4.7765336 -4.8122892 -4.7768512 -4.577435 -4.0109358 -3.2310524 -2.7354279 -2.9218559 -3.300446 -3.5018239 -3.557106 -3.713347 -4.0200047 -4.3276243 -4.6006794][-4.7833252 -4.6358914 -4.3104978 -3.832727 -3.0529761 -2.0327685 -1.4112949 -1.8750811 -2.698946 -3.2435746 -3.5399215 -3.8006377 -4.1498342 -4.4463997 -4.682806][-4.7131839 -4.4209142 -3.9132607 -3.2571058 -2.2866385 -0.99898267 -0.19431925 -0.83970237 -1.9948816 -2.8132124 -3.347343 -3.7674067 -4.24403 -4.5931034 -4.8179731][-4.7580824 -4.4401355 -3.9278774 -3.2692142 -2.2215841 -0.77227974 0.14687681 -0.51289105 -1.7237241 -2.6108394 -3.268117 -3.8358111 -4.4639268 -4.8890572 -5.0953765][-4.9978547 -4.7978158 -4.4193664 -3.9103436 -2.9836993 -1.6326349 -0.78402948 -1.3164411 -2.3027089 -2.9875247 -3.5074666 -4.0551877 -4.7404857 -5.1936293 -5.3432107][-5.195931 -5.1541634 -4.9417381 -4.6302781 -3.9364054 -2.8499603 -2.1957946 -2.5441527 -3.1943207 -3.5545654 -3.7824326 -4.2101145 -4.9153981 -5.39978 -5.4912696][-5.2247562 -5.2620664 -5.16385 -4.9892945 -4.497808 -3.696559 -3.2579479 -3.4582779 -3.8256154 -3.9103649 -3.8854551 -4.2168078 -4.9344921 -5.4067321 -5.4042993][-4.9827394 -5.0251808 -4.9799376 -4.8646417 -4.5055175 -3.973011 -3.7695274 -3.9364378 -4.136518 -4.0212507 -3.8057785 -4.0371113 -4.7063131 -5.1026096 -5.0004478][-4.5657897 -4.5634103 -4.5165286 -4.4032764 -4.120121 -3.8144264 -3.8122883 -4.018518 -4.1234374 -3.8594873 -3.4975505 -3.6336884 -4.247622 -4.6084647 -4.5200706][-4.3020015 -4.2417755 -4.1500564 -4.0031018 -3.770709 -3.6199918 -3.7150872 -3.9251916 -3.9535646 -3.6029484 -3.1614718 -3.2359936 -3.8255346 -4.2105579 -4.2313409]]...]
INFO - root - 2017-12-06 10:25:35.219571: step 23310, loss = 0.70, batch loss = 0.63 (13.3 examples/sec; 0.603 sec/batch; 51h:47m:03s remains)
INFO - root - 2017-12-06 10:25:41.064604: step 23320, loss = 0.93, batch loss = 0.86 (13.3 examples/sec; 0.603 sec/batch; 51h:48m:38s remains)
INFO - root - 2017-12-06 10:25:47.098540: step 23330, loss = 0.83, batch loss = 0.76 (12.9 examples/sec; 0.621 sec/batch; 53h:20m:27s remains)
INFO - root - 2017-12-06 10:25:53.154974: step 23340, loss = 0.82, batch loss = 0.75 (13.1 examples/sec; 0.613 sec/batch; 52h:37m:53s remains)
INFO - root - 2017-12-06 10:25:59.269191: step 23350, loss = 1.06, batch loss = 0.99 (13.5 examples/sec; 0.591 sec/batch; 50h:44m:11s remains)
INFO - root - 2017-12-06 10:26:05.300455: step 23360, loss = 0.80, batch loss = 0.73 (13.6 examples/sec; 0.589 sec/batch; 50h:32m:48s remains)
INFO - root - 2017-12-06 10:26:11.333923: step 23370, loss = 0.89, batch loss = 0.82 (13.2 examples/sec; 0.608 sec/batch; 52h:10m:45s remains)
INFO - root - 2017-12-06 10:26:17.463094: step 23380, loss = 1.00, batch loss = 0.93 (12.6 examples/sec; 0.635 sec/batch; 54h:32m:22s remains)
INFO - root - 2017-12-06 10:26:23.501606: step 23390, loss = 1.03, batch loss = 0.96 (13.7 examples/sec; 0.582 sec/batch; 49h:59m:39s remains)
INFO - root - 2017-12-06 10:26:29.575434: step 23400, loss = 1.06, batch loss = 0.99 (13.0 examples/sec; 0.616 sec/batch; 52h:51m:36s remains)
2017-12-06 10:26:30.166910: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5163555 -5.2128997 -5.7582359 -6.148818 -6.1047225 -5.6591787 -5.16812 -4.9310875 -5.0431623 -5.2329855 -5.4393506 -5.5319538 -5.346962 -5.0073762 -4.7415175][-4.4913068 -4.9941735 -5.4212317 -5.7580647 -5.6164465 -5.0394635 -4.4286571 -4.1347408 -4.2648854 -4.510931 -4.7772646 -4.93726 -4.8339548 -4.602747 -4.4480343][-4.165205 -4.6124644 -4.9678392 -5.1991286 -4.9190207 -4.2068744 -3.504452 -3.1587148 -3.2407892 -3.4758446 -3.7891316 -4.0532718 -4.1628866 -4.1799359 -4.2434959][-4.1368484 -4.6180139 -4.8940887 -4.9216561 -4.4049339 -3.4983475 -2.6641216 -2.2409956 -2.221384 -2.4208398 -2.8363147 -3.2701864 -3.6564577 -3.972497 -4.276372][-4.4672632 -4.9340973 -5.0869846 -4.8840961 -4.1417232 -3.0494978 -2.062104 -1.5439878 -1.4481292 -1.6757264 -2.2453923 -2.8732252 -3.4636467 -3.971241 -4.4034333][-4.789278 -5.2006688 -5.2502174 -4.8878222 -4.0132647 -2.7987285 -1.6951611 -1.1316004 -1.0601549 -1.402277 -2.1016297 -2.8437662 -3.4852567 -4.000494 -4.4159393][-5.0087438 -5.3165751 -5.2928748 -4.8589468 -3.9857614 -2.8039434 -1.7252312 -1.1846149 -1.1692264 -1.5770359 -2.2752688 -3.0131164 -3.6016319 -4.0194292 -4.3428535][-5.0490327 -5.1638403 -5.0726981 -4.6625915 -3.9221339 -2.9472539 -2.0476217 -1.5948577 -1.6159863 -1.9875135 -2.5853634 -3.2686462 -3.7624221 -4.03876 -4.2460237][-4.9147387 -4.8164434 -4.65411 -4.3149362 -3.7655318 -3.0524292 -2.3750525 -2.0156052 -2.0309668 -2.2976909 -2.7788188 -3.412148 -3.8344307 -4.0077343 -4.1469445][-4.7108569 -4.4032054 -4.1265979 -3.8253007 -3.4710355 -3.027359 -2.5958276 -2.3583622 -2.3522806 -2.4978633 -2.8763533 -3.4518862 -3.8081632 -3.9311578 -4.0644712][-4.5605464 -4.0486116 -3.6237485 -3.3224134 -3.1454086 -2.9458477 -2.7466791 -2.6471813 -2.6315477 -2.6981792 -3.0246131 -3.553704 -3.8555434 -3.94609 -4.0780997][-4.442627 -3.8790452 -3.4465039 -3.2188904 -3.1951818 -3.12962 -3.0457258 -3.0273581 -3.0273802 -3.0708392 -3.3743587 -3.8426478 -4.0759568 -4.1094594 -4.1943774][-4.3937244 -3.9898908 -3.7528374 -3.7099116 -3.8196342 -3.8211458 -3.7733736 -3.7824421 -3.7782605 -3.7656355 -3.9496388 -4.2334995 -4.3123865 -4.239265 -4.2500954][-4.59785 -4.3864331 -4.3435659 -4.4598475 -4.6606226 -4.7142615 -4.6917558 -4.7040367 -4.6783133 -4.5947542 -4.5950732 -4.6097775 -4.4905849 -4.3179512 -4.3089604][-4.922987 -4.8269138 -4.8725204 -5.0727434 -5.3246226 -5.4160223 -5.4026136 -5.3904748 -5.3441057 -5.222641 -5.0643568 -4.8345885 -4.5572057 -4.3384976 -4.3774309]]...]
INFO - root - 2017-12-06 10:26:36.255393: step 23410, loss = 0.69, batch loss = 0.62 (13.8 examples/sec; 0.580 sec/batch; 49h:47m:39s remains)
INFO - root - 2017-12-06 10:26:42.345178: step 23420, loss = 0.99, batch loss = 0.92 (13.0 examples/sec; 0.616 sec/batch; 52h:50m:52s remains)
INFO - root - 2017-12-06 10:26:48.313498: step 23430, loss = 0.83, batch loss = 0.76 (13.3 examples/sec; 0.601 sec/batch; 51h:34m:54s remains)
INFO - root - 2017-12-06 10:26:54.256067: step 23440, loss = 0.98, batch loss = 0.91 (14.0 examples/sec; 0.571 sec/batch; 49h:03m:16s remains)
INFO - root - 2017-12-06 10:27:00.371336: step 23450, loss = 1.11, batch loss = 1.04 (13.1 examples/sec; 0.609 sec/batch; 52h:16m:36s remains)
INFO - root - 2017-12-06 10:27:06.457303: step 23460, loss = 0.99, batch loss = 0.92 (12.9 examples/sec; 0.620 sec/batch; 53h:13m:54s remains)
INFO - root - 2017-12-06 10:27:12.589187: step 23470, loss = 0.94, batch loss = 0.87 (13.0 examples/sec; 0.617 sec/batch; 52h:59m:15s remains)
INFO - root - 2017-12-06 10:27:18.693296: step 23480, loss = 0.99, batch loss = 0.91 (13.4 examples/sec; 0.597 sec/batch; 51h:14m:02s remains)
INFO - root - 2017-12-06 10:27:24.768375: step 23490, loss = 0.86, batch loss = 0.79 (12.8 examples/sec; 0.627 sec/batch; 53h:47m:32s remains)
INFO - root - 2017-12-06 10:27:30.796450: step 23500, loss = 0.87, batch loss = 0.80 (13.1 examples/sec; 0.611 sec/batch; 52h:24m:06s remains)
2017-12-06 10:27:31.352461: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7033827 -4.1701756 -4.918766 -5.4939961 -5.6280222 -5.5313063 -5.1321778 -4.6596813 -4.32766 -4.112606 -4.1553311 -4.3600259 -4.6060195 -4.7702541 -4.8200579][-3.3565879 -3.9955211 -4.9690189 -5.6720619 -5.8024015 -5.6797681 -5.1734667 -4.6581907 -4.3333182 -4.1262922 -4.2684121 -4.5854383 -4.8339529 -4.9102678 -4.8369188][-3.0460129 -3.7755837 -4.8847675 -5.6208868 -5.6867647 -5.4873939 -4.8743124 -4.4299679 -4.2282948 -4.075016 -4.302547 -4.7084222 -4.9721074 -4.9880095 -4.7645411][-2.9062412 -3.6209683 -4.7348051 -5.3562889 -5.2455015 -4.8444743 -4.0543327 -3.7439721 -3.7883739 -3.7871833 -4.1424356 -4.6433692 -4.9747028 -5.0226622 -4.6831374][-3.0254073 -3.6077087 -4.5742612 -4.9587755 -4.5850143 -3.857147 -2.8224678 -2.6859913 -3.1452918 -3.4680781 -3.9923813 -4.5256238 -4.9079614 -5.0294123 -4.6467524][-3.3711548 -3.7760603 -4.472446 -4.5733371 -3.9482079 -2.8515012 -1.5136302 -1.4941125 -2.4223852 -3.1885655 -3.9259517 -4.464622 -4.8647327 -5.034615 -4.6424966][-3.710192 -4.038434 -4.4873075 -4.3484168 -3.5207176 -2.0769417 -0.41454124 -0.37634945 -1.6913221 -2.9271393 -3.9419754 -4.5162368 -4.909277 -5.0701613 -4.6583452][-3.8798378 -4.2722554 -4.5585546 -4.2421517 -3.3137078 -1.771383 -0.018454075 0.10282469 -1.339905 -2.8346984 -4.0289288 -4.63337 -4.9893994 -5.0941195 -4.6541004][-3.9281862 -4.4221931 -4.5879498 -4.1443152 -3.2524104 -1.9961317 -0.59779859 -0.49219537 -1.7005997 -3.0474653 -4.1428123 -4.6902657 -4.9951572 -5.0771208 -4.6469865][-3.8146939 -4.3640161 -4.4923997 -4.0339627 -3.3083115 -2.5189538 -1.660969 -1.644727 -2.4779789 -3.4117794 -4.2104044 -4.661284 -4.959856 -5.0794353 -4.7062426][-3.5138059 -4.0627432 -4.2750888 -3.9488282 -3.4609275 -3.0672402 -2.6423759 -2.741641 -3.3091807 -3.8363767 -4.2988091 -4.618988 -4.8984513 -5.0586824 -4.7539291][-3.3098669 -3.8409169 -4.176002 -4.0603762 -3.8239851 -3.6678126 -3.4413683 -3.5481606 -3.9144921 -4.1432652 -4.3787031 -4.5981855 -4.8248382 -4.9711137 -4.70102][-3.3895936 -3.887409 -4.2931161 -4.3688707 -4.3767705 -4.3550858 -4.1189036 -4.0739202 -4.2378459 -4.2899604 -4.4423327 -4.6194396 -4.7744122 -4.8487735 -4.5607944][-3.6908741 -4.1325474 -4.5218544 -4.6829915 -4.8030448 -4.8135157 -4.5209656 -4.3292103 -4.3359342 -4.3194227 -4.4700823 -4.6365466 -4.7224579 -4.7102804 -4.3954229][-3.9978833 -4.376009 -4.6926231 -4.8079343 -4.8944659 -4.8953571 -4.6187663 -4.4039359 -4.35448 -4.3398275 -4.4913816 -4.6211491 -4.623558 -4.5337725 -4.2307444]]...]
INFO - root - 2017-12-06 10:27:37.492712: step 23510, loss = 1.07, batch loss = 1.00 (13.0 examples/sec; 0.617 sec/batch; 52h:59m:12s remains)
INFO - root - 2017-12-06 10:27:43.604227: step 23520, loss = 0.93, batch loss = 0.86 (12.3 examples/sec; 0.648 sec/batch; 55h:39m:10s remains)
INFO - root - 2017-12-06 10:27:49.603165: step 23530, loss = 0.93, batch loss = 0.86 (15.7 examples/sec; 0.509 sec/batch; 43h:40m:45s remains)
INFO - root - 2017-12-06 10:27:55.699566: step 23540, loss = 1.16, batch loss = 1.09 (13.1 examples/sec; 0.609 sec/batch; 52h:17m:51s remains)
INFO - root - 2017-12-06 10:28:01.827611: step 23550, loss = 0.79, batch loss = 0.72 (12.8 examples/sec; 0.626 sec/batch; 53h:41m:15s remains)
INFO - root - 2017-12-06 10:28:07.893486: step 23560, loss = 0.81, batch loss = 0.74 (13.3 examples/sec; 0.602 sec/batch; 51h:41m:47s remains)
INFO - root - 2017-12-06 10:28:13.987231: step 23570, loss = 1.04, batch loss = 0.97 (13.0 examples/sec; 0.614 sec/batch; 52h:41m:22s remains)
INFO - root - 2017-12-06 10:28:19.987155: step 23580, loss = 1.18, batch loss = 1.11 (12.5 examples/sec; 0.638 sec/batch; 54h:42m:58s remains)
INFO - root - 2017-12-06 10:28:25.949165: step 23590, loss = 1.10, batch loss = 1.03 (13.3 examples/sec; 0.603 sec/batch; 51h:43m:53s remains)
INFO - root - 2017-12-06 10:28:32.080852: step 23600, loss = 0.79, batch loss = 0.72 (12.9 examples/sec; 0.620 sec/batch; 53h:14m:03s remains)
2017-12-06 10:28:32.638443: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3809109 -4.2689571 -4.2565646 -4.324841 -4.4204016 -4.5152845 -4.6404352 -4.711473 -4.6383357 -4.4717827 -4.4099016 -4.429152 -4.5223827 -4.6861668 -4.7349458][-4.3440289 -4.1202378 -4.0726786 -4.1361637 -4.24421 -4.3199205 -4.4223328 -4.4474959 -4.325788 -4.1792455 -4.23302 -4.4328265 -4.7203159 -5.0314794 -5.1174283][-4.1949577 -3.8218381 -3.70089 -3.7165308 -3.7950776 -3.8076553 -3.8484347 -3.8327725 -3.7248693 -3.7116051 -3.9771826 -4.4202995 -4.9309139 -5.3751907 -5.5014324][-3.9807651 -3.4449992 -3.1950965 -3.0871034 -3.0485463 -2.9364057 -2.8859544 -2.8896585 -2.9294696 -3.1886892 -3.7424612 -4.4436636 -5.1585073 -5.7015934 -5.8658996][-3.8837039 -3.2138062 -2.8337386 -2.5709841 -2.3436716 -2.0253975 -1.7902036 -1.7530043 -1.9661655 -2.546813 -3.4062719 -4.3448009 -5.263175 -5.9030008 -6.1156869][-4.003726 -3.2728019 -2.8006845 -2.3794062 -1.9143767 -1.3307498 -0.83192158 -0.66210961 -0.99276876 -1.8265927 -2.9105787 -4.0192776 -5.1052885 -5.8433304 -6.1236968][-4.2815437 -3.5907083 -3.0872109 -2.5207524 -1.8158729 -0.96108961 -0.21143675 0.099311352 -0.29794931 -1.2696741 -2.4353051 -3.5959976 -4.7637753 -5.566824 -5.916481][-4.6046085 -4.0164561 -3.537287 -2.9105582 -2.0868249 -1.1381931 -0.33093071 0.025868893 -0.35687065 -1.2780118 -2.3229628 -3.3798957 -4.506382 -5.3078451 -5.6853762][-4.9291835 -4.4373331 -4.0217633 -3.432169 -2.6488543 -1.818387 -1.1761837 -0.87285781 -1.1601956 -1.8860569 -2.6750426 -3.5319238 -4.5332193 -5.2485156 -5.5846715][-5.2703638 -4.8473916 -4.4817891 -3.9338667 -3.1997757 -2.5095096 -2.0896814 -1.8930981 -2.1095402 -2.677279 -3.265995 -3.9802437 -4.8758693 -5.4680128 -5.7030125][-5.5865541 -5.2024517 -4.8432493 -4.2897019 -3.5633981 -2.9648771 -2.7366691 -2.6610465 -2.8555799 -3.3423438 -3.8162522 -4.450809 -5.26927 -5.740345 -5.8763204][-5.7447367 -5.3722892 -5.0291266 -4.511025 -3.8367257 -3.3304353 -3.2291093 -3.2339511 -3.4207826 -3.8698056 -4.2694769 -4.8264346 -5.5466542 -5.8844967 -5.9298258][-5.7204895 -5.332953 -5.0300665 -4.6182151 -4.0808277 -3.6906343 -3.6393042 -3.6464095 -3.8037293 -4.21639 -4.5604029 -5.0407839 -5.6468258 -5.8472543 -5.8093429][-5.5558505 -5.1440716 -4.869977 -4.5519114 -4.1618123 -3.8893998 -3.8649764 -3.8541045 -3.9827244 -4.3520083 -4.6368423 -5.0168729 -5.4911466 -5.5740757 -5.4777589][-5.1660118 -4.7601504 -4.5036817 -4.2505288 -3.9734647 -3.8058839 -3.8258758 -3.8281462 -3.9429946 -4.2624588 -4.4928751 -4.7615347 -5.0961213 -5.0982304 -4.9768362]]...]
INFO - root - 2017-12-06 10:28:38.746261: step 23610, loss = 0.86, batch loss = 0.79 (13.6 examples/sec; 0.590 sec/batch; 50h:36m:01s remains)
INFO - root - 2017-12-06 10:28:44.868892: step 23620, loss = 0.76, batch loss = 0.69 (12.8 examples/sec; 0.623 sec/batch; 53h:29m:31s remains)
INFO - root - 2017-12-06 10:28:50.982546: step 23630, loss = 0.92, batch loss = 0.84 (13.3 examples/sec; 0.601 sec/batch; 51h:31m:47s remains)
INFO - root - 2017-12-06 10:28:56.823609: step 23640, loss = 0.91, batch loss = 0.84 (13.3 examples/sec; 0.601 sec/batch; 51h:34m:38s remains)
INFO - root - 2017-12-06 10:29:02.867187: step 23650, loss = 1.10, batch loss = 1.03 (13.1 examples/sec; 0.611 sec/batch; 52h:24m:24s remains)
INFO - root - 2017-12-06 10:29:08.947302: step 23660, loss = 0.93, batch loss = 0.86 (13.7 examples/sec; 0.585 sec/batch; 50h:09m:04s remains)
INFO - root - 2017-12-06 10:29:15.083656: step 23670, loss = 1.13, batch loss = 1.06 (12.3 examples/sec; 0.652 sec/batch; 55h:54m:15s remains)
INFO - root - 2017-12-06 10:29:21.172961: step 23680, loss = 0.78, batch loss = 0.71 (12.9 examples/sec; 0.619 sec/batch; 53h:04m:57s remains)
INFO - root - 2017-12-06 10:29:27.338486: step 23690, loss = 0.82, batch loss = 0.74 (13.4 examples/sec; 0.598 sec/batch; 51h:18m:02s remains)
INFO - root - 2017-12-06 10:29:33.406125: step 23700, loss = 1.05, batch loss = 0.98 (13.1 examples/sec; 0.612 sec/batch; 52h:32m:01s remains)
2017-12-06 10:29:33.958072: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.729912 -3.1411469 -2.383297 -1.7424519 -1.6168928 -1.996985 -2.6189718 -3.3952794 -4.086165 -4.3606539 -4.2532606 -4.0101018 -3.6343286 -3.2141552 -3.0598428][-3.9955356 -3.4568563 -2.7755818 -2.1825182 -2.0219767 -2.3370984 -2.9441652 -3.6916916 -4.3430982 -4.6343584 -4.5895696 -4.4280677 -4.1878242 -3.9554272 -3.9106004][-4.272872 -3.8306589 -3.2323704 -2.6799936 -2.435164 -2.613565 -3.1471612 -3.8227925 -4.4253321 -4.771204 -4.846652 -4.7945957 -4.67323 -4.6164103 -4.7095909][-4.5526867 -4.288167 -3.8142459 -3.291564 -2.9004245 -2.8408527 -3.1656451 -3.6590762 -4.143384 -4.5273376 -4.74272 -4.8250537 -4.8214931 -4.9236307 -5.1551108][-4.6615553 -4.5904288 -4.2902064 -3.8545969 -3.3653321 -3.0371103 -3.0376697 -3.2302601 -3.5193233 -3.8988996 -4.2623863 -4.5092039 -4.6260228 -4.865747 -5.2354183][-4.423099 -4.4402623 -4.2559738 -3.8874884 -3.3237853 -2.7478449 -2.4152536 -2.315429 -2.4524984 -2.8790553 -3.4342852 -3.8750474 -4.1162314 -4.4768004 -4.9842553][-3.9227819 -3.9917204 -3.8757467 -3.5293188 -2.8573551 -1.9871771 -1.3193395 -0.98904967 -1.0544977 -1.6121907 -2.4248567 -3.0603194 -3.3961992 -3.8234017 -4.4246244][-3.2978053 -3.5057874 -3.5266509 -3.2323968 -2.4335556 -1.2142556 -0.17595148 0.39207792 0.3913312 -0.31209087 -1.3717964 -2.1423953 -2.4906964 -2.9035897 -3.5194271][-2.69508 -3.1054301 -3.3459449 -3.2164805 -2.4348497 -1.0217795 0.24886608 0.98848724 1.0772882 0.34739733 -0.7503252 -1.4552064 -1.6897111 -1.9965618 -2.5206895][-2.391696 -2.9370129 -3.3663063 -3.4733961 -2.9077826 -1.5944693 -0.39203453 0.32223034 0.46181726 -0.13851309 -0.96772194 -1.3364108 -1.3254185 -1.4526439 -1.8431144][-2.3831179 -2.9374485 -3.4400673 -3.7434998 -3.4713547 -2.4715791 -1.5934031 -1.0880203 -0.98437762 -1.4316077 -1.8836246 -1.816638 -1.5126824 -1.4435501 -1.7195361][-2.5136027 -3.0092587 -3.5044355 -3.9277623 -3.9125919 -3.2597971 -2.7519751 -2.4872794 -2.4556901 -2.7881672 -2.9177117 -2.5043364 -1.9928231 -1.7935407 -2.0200388][-2.6221232 -3.0576358 -3.5460474 -4.0371871 -4.1886778 -3.7776954 -3.5024886 -3.3850579 -3.4079964 -3.6615918 -3.6025236 -3.0476685 -2.5006788 -2.2744818 -2.4735348][-2.6648283 -3.059396 -3.5588355 -4.096509 -4.3527241 -4.0829663 -3.9011707 -3.84555 -3.9052179 -4.11432 -3.9912126 -3.4451802 -2.9607382 -2.7462077 -2.8940206][-2.649004 -3.0117121 -3.5230598 -4.0835919 -4.4149203 -4.2538776 -4.1134062 -4.0851984 -4.16805 -4.34412 -4.2120142 -3.7384155 -3.3372881 -3.1237829 -3.1924524]]...]
INFO - root - 2017-12-06 10:29:40.043728: step 23710, loss = 0.88, batch loss = 0.81 (13.5 examples/sec; 0.593 sec/batch; 50h:50m:45s remains)
INFO - root - 2017-12-06 10:29:46.073465: step 23720, loss = 0.84, batch loss = 0.77 (12.8 examples/sec; 0.626 sec/batch; 53h:40m:29s remains)
INFO - root - 2017-12-06 10:29:52.053664: step 23730, loss = 0.86, batch loss = 0.78 (13.4 examples/sec; 0.598 sec/batch; 51h:16m:51s remains)
INFO - root - 2017-12-06 10:29:57.988986: step 23740, loss = 0.89, batch loss = 0.82 (13.4 examples/sec; 0.596 sec/batch; 51h:05m:12s remains)
INFO - root - 2017-12-06 10:30:03.908466: step 23750, loss = 0.97, batch loss = 0.90 (12.9 examples/sec; 0.620 sec/batch; 53h:09m:16s remains)
INFO - root - 2017-12-06 10:30:09.923228: step 23760, loss = 0.89, batch loss = 0.82 (13.2 examples/sec; 0.607 sec/batch; 52h:03m:17s remains)
INFO - root - 2017-12-06 10:30:15.999939: step 23770, loss = 0.93, batch loss = 0.86 (13.4 examples/sec; 0.597 sec/batch; 51h:12m:35s remains)
INFO - root - 2017-12-06 10:30:22.158176: step 23780, loss = 1.19, batch loss = 1.12 (13.1 examples/sec; 0.609 sec/batch; 52h:12m:06s remains)
INFO - root - 2017-12-06 10:30:28.204126: step 23790, loss = 0.85, batch loss = 0.78 (14.2 examples/sec; 0.564 sec/batch; 48h:19m:26s remains)
INFO - root - 2017-12-06 10:30:34.256378: step 23800, loss = 0.98, batch loss = 0.91 (13.6 examples/sec; 0.589 sec/batch; 50h:32m:04s remains)
2017-12-06 10:30:34.796204: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5038271 -4.6436906 -4.799509 -4.9257574 -4.9873047 -4.9898944 -4.957252 -4.7352366 -4.4064817 -4.0457559 -3.6946504 -3.4297509 -3.3820775 -3.5193014 -3.8682523][-4.8185387 -4.8752551 -4.9787064 -5.0469322 -5.0036163 -4.9280977 -4.9287233 -4.7988453 -4.5426464 -4.2627058 -3.9705179 -3.7461035 -3.7313452 -3.8107302 -4.0679727][-5.0684295 -5.0853229 -5.1197424 -5.0659275 -4.8710537 -4.661818 -4.6047955 -4.5496578 -4.4577193 -4.41822 -4.3242764 -4.2233367 -4.3039322 -4.3720112 -4.4899664][-5.2476535 -5.3186336 -5.319438 -5.115449 -4.7444019 -4.3569689 -4.1376343 -4.0807695 -4.18562 -4.4759483 -4.6577816 -4.69957 -4.8746767 -4.9685397 -4.9592304][-5.0950141 -5.3250051 -5.347178 -4.9668255 -4.401453 -3.8402538 -3.4332743 -3.3389072 -3.6513093 -4.2840562 -4.7386351 -4.8772087 -5.0563316 -5.1506238 -5.066658][-4.3821468 -4.8332624 -4.9349275 -4.4029179 -3.6642725 -2.970665 -2.4375863 -2.3330259 -2.8427386 -3.77808 -4.4842043 -4.6979051 -4.8125081 -4.885272 -4.8147664][-3.3223073 -3.9947228 -4.218195 -3.6408591 -2.8466983 -2.1058378 -1.5238333 -1.4292879 -2.0609953 -3.1467445 -3.9907649 -4.2685432 -4.34482 -4.4248743 -4.4437032][-2.5104976 -3.3396716 -3.657768 -3.1418638 -2.4110847 -1.6978929 -1.1241145 -1.0241964 -1.6485877 -2.6915565 -3.5025702 -3.7944722 -3.8567643 -3.9708123 -4.1012359][-2.3676131 -3.2348442 -3.5775278 -3.1613503 -2.5471401 -1.8823044 -1.3316433 -1.2160494 -1.7607303 -2.6418142 -3.2799735 -3.503485 -3.5430536 -3.6892533 -3.8939495][-2.8245187 -3.6057591 -3.8944254 -3.5669847 -3.06112 -2.4350457 -1.9029999 -1.7682064 -2.2198226 -2.9129691 -3.3354125 -3.4438004 -3.453867 -3.6107681 -3.8224034][-3.6103425 -4.21691 -4.3957672 -4.1205029 -3.7127185 -3.1646023 -2.6904116 -2.5679562 -2.954149 -3.5020328 -3.7612929 -3.7752724 -3.7687278 -3.9011214 -4.0364041][-4.5591717 -4.9797187 -5.0271883 -4.7579155 -4.4283204 -4.0150852 -3.6722062 -3.6198225 -3.9809532 -4.42549 -4.5614161 -4.4894333 -4.4535284 -4.5136237 -4.5033751][-5.3490286 -5.6509514 -5.6258268 -5.3766408 -5.1305275 -4.890379 -4.7256751 -4.7753396 -5.1205416 -5.4517117 -5.4664826 -5.3080177 -5.2358356 -5.2211037 -5.06647][-5.594676 -5.8915682 -5.9101043 -5.7518497 -5.614399 -5.5323715 -5.5163918 -5.6396494 -5.937119 -6.1369777 -6.0431972 -5.8398328 -5.7598767 -5.712853 -5.4769707][-5.2363272 -5.5991397 -5.7299261 -5.6986094 -5.653439 -5.6584091 -5.7146177 -5.86212 -6.0923605 -6.1872764 -6.054265 -5.8881145 -5.8333273 -5.7953024 -5.5586109]]...]
INFO - root - 2017-12-06 10:30:40.860144: step 23810, loss = 0.85, batch loss = 0.78 (13.5 examples/sec; 0.590 sec/batch; 50h:37m:34s remains)
INFO - root - 2017-12-06 10:30:46.883283: step 23820, loss = 1.01, batch loss = 0.94 (13.3 examples/sec; 0.599 sec/batch; 51h:23m:47s remains)
INFO - root - 2017-12-06 10:30:52.990858: step 23830, loss = 1.01, batch loss = 0.94 (13.5 examples/sec; 0.592 sec/batch; 50h:44m:48s remains)
INFO - root - 2017-12-06 10:30:59.110799: step 23840, loss = 1.01, batch loss = 0.94 (12.9 examples/sec; 0.621 sec/batch; 53h:12m:07s remains)
INFO - root - 2017-12-06 10:31:05.217212: step 23850, loss = 0.91, batch loss = 0.84 (13.1 examples/sec; 0.609 sec/batch; 52h:15m:14s remains)
INFO - root - 2017-12-06 10:31:11.188269: step 23860, loss = 0.83, batch loss = 0.76 (13.3 examples/sec; 0.600 sec/batch; 51h:25m:24s remains)
INFO - root - 2017-12-06 10:31:17.345702: step 23870, loss = 0.89, batch loss = 0.82 (12.7 examples/sec; 0.632 sec/batch; 54h:09m:41s remains)
INFO - root - 2017-12-06 10:31:23.460328: step 23880, loss = 0.79, batch loss = 0.72 (13.2 examples/sec; 0.608 sec/batch; 52h:06m:44s remains)
INFO - root - 2017-12-06 10:31:29.496285: step 23890, loss = 0.90, batch loss = 0.83 (14.5 examples/sec; 0.550 sec/batch; 47h:10m:22s remains)
INFO - root - 2017-12-06 10:31:35.437082: step 23900, loss = 1.19, batch loss = 1.12 (14.1 examples/sec; 0.569 sec/batch; 48h:45m:13s remains)
2017-12-06 10:31:36.002024: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1933174 -4.25246 -4.2184148 -4.2847342 -4.4567108 -4.5302396 -4.5138063 -4.4220405 -4.342196 -4.4526706 -4.6435719 -4.7507663 -4.77847 -4.6830478 -4.470108][-3.9616187 -3.9771564 -3.8697145 -3.9710546 -4.2784305 -4.3901243 -4.3085637 -4.040688 -3.8279088 -4.0347195 -4.4106326 -4.7257457 -4.9463248 -4.9201555 -4.6890807][-3.7808495 -3.7722 -3.6394224 -3.8019052 -4.2363281 -4.3620987 -4.1814332 -3.6305246 -3.1892853 -3.5076413 -4.1151013 -4.6846294 -5.1525578 -5.2335887 -4.982584][-3.6628897 -3.6725826 -3.5734067 -3.7529743 -4.193593 -4.2707911 -4.0113273 -3.214642 -2.5556784 -3.0153487 -3.8975496 -4.7424121 -5.4669538 -5.6624217 -5.3814416][-3.728018 -3.6933594 -3.6025248 -3.728971 -4.0635142 -4.0395861 -3.7076435 -2.7454367 -1.8934162 -2.4674535 -3.6309144 -4.731041 -5.6867671 -6.0054417 -5.7334604][-3.9715142 -3.8465176 -3.7770615 -3.8486676 -3.9931464 -3.7246943 -3.1261425 -1.9108789 -0.80983663 -1.4844663 -2.9722149 -4.3220925 -5.528399 -6.0556397 -5.886086][-4.3363829 -4.1770463 -4.215097 -4.2981791 -4.1865349 -3.4810681 -2.3756711 -0.77760959 0.61462116 -0.19538689 -2.0845256 -3.7148249 -5.1897721 -5.9454336 -5.8827987][-4.643187 -4.5429139 -4.7699103 -5.007381 -4.7585163 -3.699157 -2.1799798 -0.36233139 1.1591992 0.30028772 -1.7713914 -3.4942374 -5.09141 -5.967042 -5.9212346][-4.6668992 -4.6324768 -5.0488396 -5.5768061 -5.4727554 -4.4487815 -2.9389372 -1.3222566 0.026051044 -0.68820262 -2.5292602 -4.0103054 -5.4472809 -6.2335835 -6.0954504][-4.49828 -4.5079813 -5.0124726 -5.7674022 -5.8784833 -5.0972123 -3.8440909 -2.622829 -1.565383 -2.0458555 -3.5064957 -4.6868563 -5.8907309 -6.5283184 -6.2828259][-4.3118734 -4.3920736 -4.8883028 -5.678669 -5.8822145 -5.2969685 -4.2945704 -3.4007478 -2.5706306 -2.8104787 -3.9403098 -4.9209919 -5.9498339 -6.4941225 -6.2218447][-4.041914 -4.1966696 -4.6404214 -5.3472729 -5.5850382 -5.1653605 -4.39514 -3.745373 -3.0723944 -3.1077886 -3.9452624 -4.7595153 -5.6294074 -6.0971966 -5.8439903][-3.7231512 -3.8976543 -4.2631884 -4.8264961 -5.056263 -4.7799072 -4.2199016 -3.7356174 -3.193635 -3.1212721 -3.7056255 -4.3558569 -5.0803585 -5.4858265 -5.294292][-3.5997059 -3.7321317 -3.9793434 -4.3282733 -4.4676857 -4.2577248 -3.8313315 -3.4378104 -3.0343041 -2.9999938 -3.44593 -3.9689789 -4.5754318 -4.9293194 -4.8160634][-3.8383331 -3.8838043 -3.9792278 -4.0947609 -4.0893278 -3.8897831 -3.5447803 -3.1837862 -2.8581591 -2.8882141 -3.2831285 -3.7355912 -4.2566762 -4.567956 -4.5071564]]...]
INFO - root - 2017-12-06 10:31:42.039296: step 23910, loss = 0.82, batch loss = 0.75 (13.7 examples/sec; 0.582 sec/batch; 49h:54m:43s remains)
INFO - root - 2017-12-06 10:31:48.131511: step 23920, loss = 0.80, batch loss = 0.73 (13.6 examples/sec; 0.590 sec/batch; 50h:33m:39s remains)
INFO - root - 2017-12-06 10:31:54.225611: step 23930, loss = 0.88, batch loss = 0.81 (12.5 examples/sec; 0.638 sec/batch; 54h:41m:57s remains)
INFO - root - 2017-12-06 10:32:00.283023: step 23940, loss = 0.83, batch loss = 0.76 (13.0 examples/sec; 0.617 sec/batch; 52h:55m:02s remains)
INFO - root - 2017-12-06 10:32:06.383315: step 23950, loss = 0.95, batch loss = 0.88 (12.6 examples/sec; 0.633 sec/batch; 54h:17m:08s remains)
INFO - root - 2017-12-06 10:32:12.404734: step 23960, loss = 0.93, batch loss = 0.86 (14.4 examples/sec; 0.556 sec/batch; 47h:39m:15s remains)
INFO - root - 2017-12-06 10:32:18.631799: step 23970, loss = 0.94, batch loss = 0.87 (12.7 examples/sec; 0.630 sec/batch; 53h:58m:16s remains)
INFO - root - 2017-12-06 10:32:24.763079: step 23980, loss = 0.90, batch loss = 0.83 (13.1 examples/sec; 0.612 sec/batch; 52h:29m:27s remains)
INFO - root - 2017-12-06 10:32:30.868604: step 23990, loss = 1.06, batch loss = 0.99 (13.2 examples/sec; 0.604 sec/batch; 51h:47m:05s remains)
INFO - root - 2017-12-06 10:32:36.935504: step 24000, loss = 0.98, batch loss = 0.91 (13.0 examples/sec; 0.615 sec/batch; 52h:41m:49s remains)
2017-12-06 10:32:37.719501: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9611454 -5.0022631 -5.06802 -5.1595626 -5.3548112 -5.4757566 -5.4168715 -5.1228213 -4.7553086 -4.7486768 -5.1348453 -5.5553827 -5.8667107 -5.9048052 -5.6389375][-4.8251538 -4.8139391 -4.797811 -4.8190508 -4.9784088 -5.0640316 -4.9843674 -4.7552919 -4.4641542 -4.4844289 -4.8700705 -5.2535777 -5.5070305 -5.4895453 -5.315752][-4.7445865 -4.6056809 -4.4853067 -4.4269691 -4.5703068 -4.6555061 -4.58325 -4.4462204 -4.2966814 -4.4529305 -4.868084 -5.13875 -5.2191987 -5.0727806 -4.9672866][-4.636353 -4.3574338 -4.1854415 -4.1269822 -4.289845 -4.3743315 -4.2701263 -4.1821194 -4.1548271 -4.4915133 -5.0044117 -5.2153897 -5.1517549 -4.8752494 -4.7650466][-4.4446898 -4.1057391 -3.966486 -3.9808068 -4.1526909 -4.1571589 -3.8790061 -3.6996708 -3.7369266 -4.2448936 -4.918561 -5.1879015 -5.11137 -4.7863874 -4.6125531][-4.1182985 -3.8342032 -3.7524757 -3.8314695 -3.9508467 -3.7740829 -3.1764295 -2.7801218 -2.8829598 -3.6113029 -4.5167308 -4.9270668 -4.9429455 -4.6768365 -4.4589605][-3.7778578 -3.5900917 -3.510766 -3.5263338 -3.4627621 -3.0177603 -2.0751424 -1.4749084 -1.7338161 -2.7701445 -3.9565394 -4.5243411 -4.616518 -4.4430676 -4.2464113][-3.6169381 -3.4931381 -3.3987067 -3.3135285 -3.0246222 -2.314785 -1.152317 -0.46381617 -0.88956404 -2.1333454 -3.4918211 -4.15963 -4.2843432 -4.197506 -4.0396852][-3.7238135 -3.6139319 -3.4935122 -3.3324976 -2.9398465 -2.1908431 -1.1269393 -0.58042312 -1.0622902 -2.2110784 -3.4588227 -4.0795617 -4.2089977 -4.2041883 -4.0668435][-3.9148042 -3.7947576 -3.6148412 -3.4145627 -3.0928788 -2.5933905 -1.9031794 -1.6139939 -2.0307693 -2.8651178 -3.7953186 -4.2454219 -4.3487105 -4.399229 -4.2342463][-4.05988 -4.0231495 -3.831939 -3.6049566 -3.4166498 -3.2190518 -2.8928194 -2.8066893 -3.1366429 -3.6731334 -4.2743397 -4.5119114 -4.5273356 -4.5578341 -4.3360281][-4.193584 -4.3483572 -4.2834921 -4.1079187 -4.0249233 -3.983099 -3.8266704 -3.8193278 -4.0631413 -4.3881121 -4.7459378 -4.8001175 -4.6718955 -4.6145453 -4.3765941][-4.2558136 -4.5998955 -4.7138729 -4.651341 -4.6311369 -4.6066136 -4.4775915 -4.4772987 -4.655282 -4.8463163 -5.0297852 -4.962656 -4.7382479 -4.6020613 -4.3960404][-4.1677561 -4.5972362 -4.8100057 -4.8213549 -4.805912 -4.7399096 -4.599896 -4.6002955 -4.7501192 -4.8821125 -4.9654827 -4.8604355 -4.6525884 -4.5346165 -4.4049883][-4.0931649 -4.48248 -4.6712966 -4.6544042 -4.5642376 -4.4423723 -4.3219004 -4.3592706 -4.5270276 -4.6727872 -4.74351 -4.6765561 -4.5613151 -4.5214038 -4.4670358]]...]
INFO - root - 2017-12-06 10:32:43.762237: step 24010, loss = 0.70, batch loss = 0.63 (13.1 examples/sec; 0.611 sec/batch; 52h:19m:48s remains)
INFO - root - 2017-12-06 10:32:49.830140: step 24020, loss = 0.83, batch loss = 0.76 (12.9 examples/sec; 0.619 sec/batch; 53h:04m:29s remains)
INFO - root - 2017-12-06 10:32:55.937813: step 24030, loss = 1.05, batch loss = 0.98 (13.2 examples/sec; 0.608 sec/batch; 52h:03m:50s remains)
INFO - root - 2017-12-06 10:33:01.906989: step 24040, loss = 0.91, batch loss = 0.84 (16.1 examples/sec; 0.497 sec/batch; 42h:36m:26s remains)
INFO - root - 2017-12-06 10:33:08.068650: step 24050, loss = 0.91, batch loss = 0.84 (13.3 examples/sec; 0.602 sec/batch; 51h:34m:51s remains)
