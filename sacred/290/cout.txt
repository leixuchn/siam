INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "290"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - preproces -- None
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
inputs Tensor("batch:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
net Tensor("siamese_fc/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
inputs Tensor("batch:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_1/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
net Tensor("siamese_fc_1/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
inputs Tensor("batch_1:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_2/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
net Tensor("siamese_fc_2/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("siamese_fc_2/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_2/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
inputs Tensor("batch_1:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_3/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
net Tensor("siamese_fc_3/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_3/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_3/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
Tensor("detection_1/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-16 03:33:09.615627: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 03:33:09.615663: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 03:33:09.615670: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 03:33:09.615674: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 03:33:09.615678: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 03:33:09.964772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-16 03:33:09.964809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-16 03:33:09.964815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-16 03:33:09.964827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-16 03:33:13.094109: step 0, loss = 2.28, batch loss = 2.23 (3.5 examples/sec; 2.314 sec/batch; 213h:43m:29s remains)
2017-12-16 03:33:13.774092: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289474 -4.4289289 -4.428905 -4.4288821 -4.4288769 -4.4288969 -4.4289312 -4.428966 -4.4289923 -4.4290047 -4.42901 -4.4290032 -4.4289932 -4.428978 -4.4289646][-4.4289875 -4.4289765 -4.4289613 -4.428947 -4.428946 -4.4289608 -4.4289818 -4.4290037 -4.4290195 -4.4290252 -4.4290252 -4.4290204 -4.4290137 -4.4290018 -4.428988][-4.4290047 -4.4290018 -4.4289961 -4.428987 -4.4289856 -4.4289932 -4.4290051 -4.4290171 -4.4290247 -4.4290247 -4.4290209 -4.4290147 -4.429009 -4.4289975 -4.4289813][-4.4289837 -4.4289861 -4.4289851 -4.4289756 -4.4289665 -4.4289641 -4.4289703 -4.4289804 -4.4289875 -4.4289908 -4.4289913 -4.428987 -4.4289818 -4.428967 -4.42894][-4.428926 -4.4289331 -4.4289346 -4.4289184 -4.428896 -4.4288793 -4.4288816 -4.4289002 -4.4289212 -4.4289393 -4.4289508 -4.4289527 -4.428946 -4.4289231 -4.4288793][-4.4288621 -4.42887 -4.4288697 -4.4288454 -4.4288044 -4.4287667 -4.428772 -4.4288168 -4.4288659 -4.4289045 -4.4289293 -4.4289403 -4.428937 -4.4289131 -4.4288664][-4.4288411 -4.4288554 -4.4288592 -4.4288363 -4.4287934 -4.4287505 -4.42876 -4.4288149 -4.4288735 -4.4289193 -4.4289489 -4.4289656 -4.4289694 -4.4289546 -4.4289222][-4.4288697 -4.4288917 -4.4289026 -4.4288921 -4.4288673 -4.42884 -4.4288492 -4.4288869 -4.4289284 -4.4289627 -4.4289837 -4.428997 -4.4290013 -4.4289942 -4.4289765][-4.428915 -4.4289331 -4.4289384 -4.4289293 -4.4289141 -4.4289 -4.428905 -4.4289255 -4.4289541 -4.42898 -4.4289923 -4.4289989 -4.4290037 -4.4290032 -4.428998][-4.42895 -4.4289575 -4.4289503 -4.4289322 -4.4289145 -4.4289002 -4.4288969 -4.4289074 -4.4289341 -4.4289613 -4.4289713 -4.4289761 -4.4289818 -4.4289856 -4.4289885][-4.42896 -4.4289584 -4.4289441 -4.4289174 -4.4288936 -4.4288721 -4.4288564 -4.4288588 -4.4288883 -4.4289193 -4.4289351 -4.4289479 -4.4289594 -4.428967 -4.4289727][-4.428946 -4.4289384 -4.42892 -4.4288917 -4.4288654 -4.4288406 -4.4288149 -4.4288087 -4.4288378 -4.4288731 -4.4289012 -4.4289274 -4.4289484 -4.4289618 -4.4289684][-4.4289346 -4.4289231 -4.4289064 -4.428885 -4.4288664 -4.4288492 -4.4288239 -4.428813 -4.4288344 -4.428865 -4.4288969 -4.4289289 -4.4289551 -4.4289713 -4.4289751][-4.4289317 -4.428925 -4.4289179 -4.4289079 -4.4289012 -4.4288926 -4.4288769 -4.4288707 -4.4288883 -4.4289131 -4.4289417 -4.4289675 -4.4289851 -4.4289923 -4.4289851][-4.4289365 -4.4289393 -4.428946 -4.4289465 -4.4289455 -4.4289417 -4.4289355 -4.4289374 -4.4289541 -4.4289742 -4.4289956 -4.4290118 -4.429019 -4.4290147 -4.4289947]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 03:33:17.079271: step 10, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 24h:19m:15s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-fixqian-val-noclip-lr0.0001
INFO - root - 2017-12-16 03:33:19.779537: step 20, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:31m:46s remains)
INFO - root - 2017-12-16 03:33:22.465032: step 30, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:26m:19s remains)
INFO - root - 2017-12-16 03:33:25.140415: step 40, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:50m:57s remains)
INFO - root - 2017-12-16 03:33:27.803104: step 50, loss = 2.28, batch loss = 2.22 (30.3 examples/sec; 0.264 sec/batch; 24h:20m:42s remains)
INFO - root - 2017-12-16 03:33:30.461414: step 60, loss = 2.28, batch loss = 2.22 (30.4 examples/sec; 0.264 sec/batch; 24h:20m:20s remains)
INFO - root - 2017-12-16 03:33:33.181831: step 70, loss = 2.28, batch loss = 2.22 (30.4 examples/sec; 0.263 sec/batch; 24h:19m:20s remains)
INFO - root - 2017-12-16 03:33:35.846034: step 80, loss = 2.27, batch loss = 2.21 (29.4 examples/sec; 0.272 sec/batch; 25h:05m:29s remains)
INFO - root - 2017-12-16 03:33:38.536001: step 90, loss = 2.26, batch loss = 2.20 (30.0 examples/sec; 0.267 sec/batch; 24h:38m:33s remains)
INFO - root - 2017-12-16 03:33:41.237464: step 100, loss = 2.25, batch loss = 2.19 (29.7 examples/sec; 0.269 sec/batch; 24h:50m:16s remains)
2017-12-16 03:33:41.682044: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3732109 -4.3710589 -4.3708696 -4.3700256 -4.3690758 -4.3686924 -4.3683782 -4.3680367 -4.3675032 -4.3666945 -4.365077 -4.362462 -4.3592682 -4.35656 -4.3546472][-4.3719997 -4.3701005 -4.3698273 -4.3685946 -4.3670945 -4.3661761 -4.3655624 -4.3650904 -4.3645091 -4.3637347 -4.3622465 -4.3597937 -4.3569856 -4.3546996 -4.3532968][-4.3728108 -4.3713794 -4.3710756 -4.3695936 -4.367734 -4.36637 -4.3654485 -4.3648067 -4.3641214 -4.3633947 -4.3620615 -4.3598843 -4.3573213 -4.3554363 -4.3545823][-4.3715186 -4.3703279 -4.3700204 -4.3686085 -4.3669205 -4.3657279 -4.3650613 -4.3646464 -4.3641181 -4.3635597 -4.362361 -4.3603473 -4.3578997 -4.3562694 -4.3558226][-4.3682685 -4.3671007 -4.3668747 -4.3658595 -4.3648362 -4.3643017 -4.3642492 -4.3642526 -4.3639669 -4.3635359 -4.3623252 -4.3602686 -4.3577714 -4.3561659 -4.3558412][-4.3640261 -4.3630543 -4.3632145 -4.362896 -4.3627663 -4.3630328 -4.3635855 -4.3639183 -4.3637319 -4.3632579 -4.3618517 -4.3595405 -4.3568029 -4.3550529 -4.3547583][-4.3602834 -4.3595324 -4.3601041 -4.3604693 -4.3611207 -4.3620796 -4.3631163 -4.3637371 -4.3636718 -4.3632531 -4.3618369 -4.35943 -4.35657 -4.3547745 -4.3546305][-4.3582959 -4.357481 -4.358182 -4.3588319 -4.3598175 -4.3611608 -4.3625612 -4.3635335 -4.3638248 -4.3637757 -4.3627615 -4.3606696 -4.3580079 -4.3563676 -4.3564377][-4.3577046 -4.3567896 -4.3574848 -4.3581991 -4.3592539 -4.360723 -4.3623276 -4.363596 -4.3642459 -4.3645582 -4.3640704 -4.362577 -4.3604493 -4.3591919 -4.3595386][-4.3586588 -4.3575716 -4.3581371 -4.3588085 -4.3598118 -4.3612394 -4.3628826 -4.3643026 -4.3651004 -4.3654952 -4.3653421 -4.3644509 -4.3629923 -4.3622069 -4.3627529][-4.3610959 -4.35976 -4.3600364 -4.3605108 -4.3613009 -4.3625417 -4.3640985 -4.3655853 -4.3664813 -4.3669429 -4.3670874 -4.3667717 -4.3659878 -4.3656392 -4.3661885][-4.3637323 -4.3621607 -4.3621216 -4.3623409 -4.3628716 -4.3638506 -4.3652177 -4.3666906 -4.3677473 -4.3684292 -4.368937 -4.3691459 -4.3689737 -4.3690047 -4.3694477][-4.3657665 -4.3640757 -4.3638716 -4.3639846 -4.3644109 -4.3651876 -4.3663054 -4.3676357 -4.3687425 -4.36955 -4.3702517 -4.3707452 -4.3709497 -4.3711958 -4.3714886][-4.3670139 -4.365416 -4.3653183 -4.3656135 -4.3662148 -4.3669939 -4.3679543 -4.3690891 -4.3700786 -4.3708072 -4.3714762 -4.3720174 -4.3723607 -4.3726082 -4.3726931][-4.368299 -4.366909 -4.3670216 -4.3675346 -4.3683224 -4.3691568 -4.3700371 -4.370995 -4.3718476 -4.3724875 -4.3730974 -4.3736076 -4.3739429 -4.37405 -4.3738914]]...]
INFO - root - 2017-12-16 03:33:44.349863: step 110, loss = 2.21, batch loss = 2.15 (29.8 examples/sec; 0.269 sec/batch; 24h:49m:02s remains)
INFO - root - 2017-12-16 03:33:47.023395: step 120, loss = 2.17, batch loss = 2.11 (30.4 examples/sec; 0.264 sec/batch; 24h:19m:55s remains)
INFO - root - 2017-12-16 03:33:49.722137: step 130, loss = 2.04, batch loss = 1.98 (29.4 examples/sec; 0.272 sec/batch; 25h:06m:24s remains)
INFO - root - 2017-12-16 03:33:52.408484: step 140, loss = 1.90, batch loss = 1.85 (30.5 examples/sec; 0.262 sec/batch; 24h:13m:23s remains)
INFO - root - 2017-12-16 03:33:55.123009: step 150, loss = 1.74, batch loss = 1.68 (30.4 examples/sec; 0.263 sec/batch; 24h:17m:15s remains)
INFO - root - 2017-12-16 03:33:57.841330: step 160, loss = 1.00, batch loss = 0.94 (29.8 examples/sec; 0.268 sec/batch; 24h:46m:50s remains)
INFO - root - 2017-12-16 03:34:00.529797: step 170, loss = 0.73, batch loss = 0.67 (30.5 examples/sec; 0.262 sec/batch; 24h:13m:28s remains)
INFO - root - 2017-12-16 03:34:03.210558: step 180, loss = 0.80, batch loss = 0.74 (29.7 examples/sec; 0.269 sec/batch; 24h:52m:18s remains)
INFO - root - 2017-12-16 03:34:05.920317: step 190, loss = 0.79, batch loss = 0.73 (29.8 examples/sec; 0.269 sec/batch; 24h:47m:06s remains)
INFO - root - 2017-12-16 03:34:08.647786: step 200, loss = 0.61, batch loss = 0.55 (30.3 examples/sec; 0.264 sec/batch; 24h:22m:09s remains)
2017-12-16 03:34:09.077466: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5223567 -1.446655 -1.4457526 -1.3953981 -1.2942176 -1.135582 -0.94344044 -0.79382014 -0.7283318 -0.73807693 -0.81250024 -1.0007179 -1.3169551 -1.7047682 -2.0962598][-1.2767994 -1.153307 -1.1325555 -1.0913279 -1.0199108 -0.90360761 -0.77380681 -0.67200017 -0.62823415 -0.64151978 -0.71751094 -0.914906 -1.2392828 -1.638737 -2.0400555][-1.1688473 -1.0096726 -0.97902608 -0.95551372 -0.92573595 -0.86660385 -0.79372811 -0.73145127 -0.696476 -0.69946456 -0.7621634 -0.9534986 -1.2726407 -1.6680245 -2.0668342][-1.0436153 -0.86425257 -0.82774782 -0.81208658 -0.80446076 -0.77948165 -0.74494553 -0.71313643 -0.69095945 -0.69619942 -0.75984836 -0.95539379 -1.2772689 -1.6748641 -2.0771248][-0.93001127 -0.74164534 -0.69913864 -0.6820488 -0.6832757 -0.67803407 -0.66949773 -0.66042662 -0.65357113 -0.66925907 -0.74033809 -0.94049287 -1.265281 -1.6673319 -2.0757093][-0.90283608 -0.71585464 -0.66909862 -0.64520431 -0.6436789 -0.64012742 -0.63344336 -0.62330055 -0.61556268 -0.63360071 -0.70823312 -0.91122127 -1.2409539 -1.6516125 -2.0688879][-0.99488783 -0.8235538 -0.774796 -0.73540354 -0.71333361 -0.68760681 -0.65496564 -0.62111807 -0.599072 -0.61303616 -0.686512 -0.88994622 -1.2239215 -1.6412623 -2.0645692][-1.1087999 -0.96778345 -0.92615652 -0.86771274 -0.812655 -0.75396371 -0.68957448 -0.63402367 -0.60426927 -0.62417364 -0.70228696 -0.90564537 -1.2362063 -1.6497641 -2.0700529][-1.1499372 -1.0473664 -1.0298786 -0.96402073 -0.8772018 -0.7849412 -0.69552326 -0.63099623 -0.60674334 -0.6474576 -0.74539232 -0.95680785 -1.2815914 -1.6823938 -2.0899746][-1.1157708 -1.0471826 -1.0615232 -1.0059509 -0.90442061 -0.79108381 -0.68813848 -0.62303877 -0.60594893 -0.66401911 -0.7818644 -1.0042126 -1.3260958 -1.7150254 -2.1100852][-1.0224783 -0.97386456 -1.0161986 -0.98506522 -0.89640355 -0.79004121 -0.69334745 -0.63445187 -0.61924291 -0.67814064 -0.79757094 -1.019074 -1.3363032 -1.7190719 -2.1104221][-0.90785027 -0.86574006 -0.92576981 -0.91923475 -0.85605884 -0.7756989 -0.70206857 -0.65719795 -0.64391446 -0.69599438 -0.80531383 -1.0150275 -1.3230286 -1.7020965 -2.0954263][-0.83600259 -0.78993869 -0.85116291 -0.85328937 -0.80315542 -0.74245286 -0.6911974 -0.6612668 -0.65188885 -0.70015407 -0.80429363 -1.0066493 -1.3091722 -1.6876428 -2.0843737][-0.85144687 -0.7891345 -0.83218884 -0.82854366 -0.77923155 -0.72431278 -0.68245864 -0.65929627 -0.6514895 -0.69435215 -0.79484487 -0.99572968 -1.2997205 -1.6818016 -2.0822742][-0.94730234 -0.86039424 -0.87389851 -0.86154175 -0.817544 -0.77038407 -0.73147345 -0.70563412 -0.69258332 -0.71988988 -0.80379343 -0.99496651 -1.297729 -1.682611 -2.0845256]]...]
INFO - root - 2017-12-16 03:34:11.790497: step 210, loss = 0.55, batch loss = 0.50 (28.7 examples/sec; 0.279 sec/batch; 25h:43m:32s remains)
INFO - root - 2017-12-16 03:34:14.488225: step 220, loss = 0.76, batch loss = 0.70 (29.6 examples/sec; 0.271 sec/batch; 24h:59m:01s remains)
INFO - root - 2017-12-16 03:34:17.222581: step 230, loss = 0.66, batch loss = 0.60 (29.4 examples/sec; 0.272 sec/batch; 25h:05m:32s remains)
INFO - root - 2017-12-16 03:34:19.937160: step 240, loss = 0.58, batch loss = 0.52 (29.3 examples/sec; 0.273 sec/batch; 25h:13m:14s remains)
INFO - root - 2017-12-16 03:34:22.669144: step 250, loss = 0.62, batch loss = 0.57 (29.3 examples/sec; 0.273 sec/batch; 25h:11m:26s remains)
INFO - root - 2017-12-16 03:34:25.438644: step 260, loss = 0.55, batch loss = 0.49 (29.4 examples/sec; 0.272 sec/batch; 25h:05m:36s remains)
INFO - root - 2017-12-16 03:34:28.186938: step 270, loss = 0.65, batch loss = 0.59 (28.9 examples/sec; 0.276 sec/batch; 25h:30m:45s remains)
INFO - root - 2017-12-16 03:34:30.956317: step 280, loss = 0.55, batch loss = 0.49 (28.7 examples/sec; 0.278 sec/batch; 25h:41m:38s remains)
INFO - root - 2017-12-16 03:34:33.668990: step 290, loss = 0.63, batch loss = 0.57 (29.4 examples/sec; 0.272 sec/batch; 25h:05m:28s remains)
INFO - root - 2017-12-16 03:34:36.385324: step 300, loss = 0.62, batch loss = 0.57 (29.5 examples/sec; 0.271 sec/batch; 25h:00m:52s remains)
2017-12-16 03:34:36.839500: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0782659 -1.1471906 -1.247381 -1.2934763 -1.2900612 -1.2227027 -1.0734978 -0.82613063 -0.52865744 -0.24066734 -0.011330605 0.094263077 0.062067032 -0.087611675 -0.27077341][-0.75703 -0.81216359 -0.93063641 -0.98321509 -0.97827244 -0.91197133 -0.76495385 -0.51866961 -0.22213125 0.066218853 0.29294682 0.4055481 0.37947512 0.20931482 -0.01484251][-0.54097748 -0.56518054 -0.67954659 -0.7334199 -0.74251103 -0.70460892 -0.59901619 -0.40765 -0.1783762 0.041280746 0.21653843 0.3075676 0.28390551 0.12609959 -0.09078455][-0.27996254 -0.27035189 -0.38288307 -0.43623853 -0.45815015 -0.45101833 -0.39486885 -0.27322674 -0.1328311 -0.010385036 0.0810833 0.12059021 0.085572243 -0.052058697 -0.23725462][0.019587994 0.070249081 -0.038937092 -0.087759972 -0.11478233 -0.12799549 -0.11197376 -0.058933258 -0.012839317 0.0020184517 -0.0077571869 -0.043200493 -0.11364698 -0.24396896 -0.39882565][0.3562212 0.45331 0.34864807 0.30255747 0.27644539 0.26019526 0.26059341 0.26909637 0.24064112 0.16345692 0.062330723 -0.04843092 -0.16759539 -0.30585814 -0.44134808][0.72071838 0.84204435 0.73908281 0.69414234 0.66962481 0.65786171 0.65628862 0.64342451 0.56993437 0.43148851 0.26379728 0.086878777 -0.08597517 -0.24921989 -0.38333797][1.0053396 1.1502547 1.0449939 0.99514914 0.96556568 0.95108461 0.94400454 0.92090845 0.8326025 0.67377186 0.47813082 0.25921965 0.041139126 -0.15440655 -0.30607939][1.1382499 1.3095841 1.2061768 1.141295 1.0915155 1.056695 1.0301113 0.99504757 0.90865517 0.7616024 0.576283 0.35604382 0.12687969 -0.077874184 -0.23597431][1.1087928 1.3024001 1.2077451 1.1294885 1.0606799 1.0055604 0.95813084 0.91109991 0.83183575 0.709157 0.55632305 0.36760807 0.16667747 -0.010306835 -0.14164782][0.97500229 1.1925364 1.1173582 1.0365357 0.95557976 0.88067293 0.80547523 0.735064 0.65330648 0.54859447 0.42962837 0.28840828 0.14472723 0.027361393 -0.048513889][0.74025679 0.982007 0.9385972 0.8732233 0.79274225 0.70020819 0.5944376 0.49301147 0.39898968 0.30078173 0.20500183 0.10668039 0.024000168 -0.023116112 -0.032280922][0.39721346 0.6446619 0.62907171 0.5850029 0.51730347 0.42273045 0.30604267 0.19205141 0.094324112 0.0027608871 -0.076408863 -0.14044189 -0.17170715 -0.15631533 -0.10235739][-0.001001358 0.22927666 0.2284956 0.20860863 0.16946936 0.10526371 0.018481255 -0.069730759 -0.14464951 -0.21466303 -0.27139473 -0.30490112 -0.29615927 -0.23381615 -0.12879848][-0.36257076 -0.15437031 -0.14665079 -0.13997793 -0.13758993 -0.1477046 -0.17365408 -0.20437813 -0.22742128 -0.25284624 -0.27456951 -0.28245449 -0.25266838 -0.17088747 -0.046649456]]...]
INFO - root - 2017-12-16 03:34:39.559660: step 310, loss = 0.60, batch loss = 0.54 (29.6 examples/sec; 0.270 sec/batch; 24h:53m:53s remains)
INFO - root - 2017-12-16 03:34:42.311972: step 320, loss = 0.50, batch loss = 0.44 (28.6 examples/sec; 0.280 sec/batch; 25h:51m:18s remains)
INFO - root - 2017-12-16 03:34:45.065827: step 330, loss = 0.66, batch loss = 0.60 (29.5 examples/sec; 0.271 sec/batch; 25h:00m:13s remains)
INFO - root - 2017-12-16 03:34:47.801479: step 340, loss = 0.59, batch loss = 0.54 (29.0 examples/sec; 0.276 sec/batch; 25h:26m:27s remains)
INFO - root - 2017-12-16 03:34:50.516790: step 350, loss = 0.59, batch loss = 0.53 (30.1 examples/sec; 0.266 sec/batch; 24h:31m:23s remains)
INFO - root - 2017-12-16 03:34:53.218048: step 360, loss = 0.64, batch loss = 0.59 (29.5 examples/sec; 0.271 sec/batch; 25h:02m:34s remains)
INFO - root - 2017-12-16 03:34:55.942425: step 370, loss = 0.63, batch loss = 0.57 (30.2 examples/sec; 0.265 sec/batch; 24h:24m:32s remains)
INFO - root - 2017-12-16 03:34:58.667776: step 380, loss = 0.65, batch loss = 0.60 (29.3 examples/sec; 0.273 sec/batch; 25h:13m:39s remains)
INFO - root - 2017-12-16 03:35:01.429216: step 390, loss = 0.62, batch loss = 0.56 (28.8 examples/sec; 0.277 sec/batch; 25h:35m:15s remains)
INFO - root - 2017-12-16 03:35:04.176004: step 400, loss = 0.50, batch loss = 0.44 (28.7 examples/sec; 0.278 sec/batch; 25h:40m:50s remains)
2017-12-16 03:35:04.633788: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7096329 -2.6238337 -2.5345807 -2.3850415 -2.1726456 -1.9100077 -1.6226394 -1.3602917 -1.1823099 -1.1400526 -1.2505081 -1.4859314 -1.7973366 -2.1221771 -2.3991182][-2.6127353 -2.485333 -2.3408558 -2.1202483 -1.8238449 -1.4723494 -1.109436 -0.81323361 -0.64532137 -0.6459 -0.82105589 -1.134654 -1.5231194 -1.9112353 -2.2345052][-2.591639 -2.4212468 -2.2161083 -1.9235237 -1.5462458 -1.1078084 -0.67095923 -0.34800386 -0.20261192 -0.25839138 -0.50802064 -0.90717435 -1.3753412 -1.8216047 -2.1814523][-2.5549674 -2.3357043 -2.0722148 -1.7185159 -1.2778294 -0.77386308 -0.28267622 0.0552783 0.17401743 0.068097591 -0.24629402 -0.720238 -1.2581136 -1.7532763 -2.1411788][-2.5051637 -2.2343731 -1.920336 -1.5195072 -1.0359497 -0.49517202 0.018129826 0.35015821 0.43816853 0.29013157 -0.072752953 -0.60066915 -1.1869562 -1.7141654 -2.1180313][-2.440484 -2.1280618 -1.7741745 -1.3368022 -0.82110858 -0.25925016 0.25618649 0.56918287 0.62518072 0.44066 0.04120779 -0.52498078 -1.1439407 -1.6910183 -2.1034265][-2.374094 -2.0285308 -1.6507547 -1.1913624 -0.65426636 -0.084840775 0.41577625 0.69762182 0.71813917 0.50267649 0.079826832 -0.50322413 -1.1315987 -1.6808109 -2.0904162][-2.3023832 -1.9426663 -1.5622971 -1.1057177 -0.57218862 -0.019800186 0.44924593 0.69402695 0.68632412 0.4583168 0.039332867 -0.52928305 -1.1383903 -1.6706622 -2.068512][-2.217041 -1.8537967 -1.4874463 -1.0562663 -0.55139661 -0.039665222 0.38595963 0.59547853 0.57281494 0.35265398 -0.038862705 -0.57059622 -1.1431971 -1.6482165 -2.0331051][-2.0941525 -1.7384751 -1.4030867 -1.0193844 -0.56898189 -0.11770153 0.26180315 0.45033264 0.43383741 0.24138069 -0.10665941 -0.59257746 -1.1280999 -1.610266 -1.9890339][-1.9189796 -1.584784 -1.2942834 -0.9704504 -0.58636689 -0.19938803 0.14009047 0.3213892 0.32329893 0.16565943 -0.13899994 -0.58540368 -1.0952132 -1.5665798 -1.9479][-1.711834 -1.4058883 -1.1597381 -0.88823414 -0.5661037 -0.24175501 0.053641319 0.22101593 0.23286009 0.10210609 -0.16681385 -0.579206 -1.0675695 -1.5338111 -1.9215407][-1.4746091 -1.1820071 -0.95708275 -0.71018076 -0.4309001 -0.16265821 0.075731277 0.20307398 0.19455814 0.061094761 -0.19507313 -0.5860734 -1.0572209 -1.5201261 -1.9149437][-1.1871145 -0.88583684 -0.66265011 -0.42189503 -0.17213392 0.045642376 0.21593332 0.27939081 0.21927309 0.049471855 -0.22535086 -0.61524391 -1.0757978 -1.5347714 -1.932194][-0.84965992 -0.54381323 -0.3426075 -0.13321781 0.071924686 0.23586464 0.34206533 0.34764338 0.24010706 0.031767368 -0.2708106 -0.66785574 -1.1203735 -1.573175 -1.9669061]]...]
INFO - root - 2017-12-16 03:35:07.435988: step 410, loss = 0.56, batch loss = 0.50 (28.9 examples/sec; 0.277 sec/batch; 25h:31m:27s remains)
