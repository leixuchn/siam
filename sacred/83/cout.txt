INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "83"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-use_nesterov:False-clip10
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-06 05:05:35.138717: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 05:05:35.138833: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 05:05:35.138840: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 05:05:35.138844: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 05:05:35.138847: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-06 05:05:40.516665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 8.75GiB
2017-12-06 05:05:40.516701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-06 05:05:40.516708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-06 05:05:40.516716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-06 05:05:57.546413: step 0, loss = 2.03, batch loss = 1.97 (0.7 examples/sec; 10.868 sec/batch; 1003h:45m:15s remains)
2017-12-06 05:05:59.349648: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2216063 -4.2259173 -4.2035389 -4.171814 -4.1532145 -4.1563492 -4.17503 -4.1984944 -4.222455 -4.2425957 -4.2540703 -4.2623892 -4.26082 -4.2378531 -4.197535][-4.2285004 -4.227253 -4.2045088 -4.177001 -4.1626406 -4.1653562 -4.1792927 -4.1997085 -4.2180786 -4.2272363 -4.2266464 -4.2295156 -4.229248 -4.2108941 -4.1751809][-4.2391672 -4.2345271 -4.2143912 -4.194663 -4.1854739 -4.1844811 -4.1881018 -4.1971779 -4.203999 -4.2013917 -4.1928878 -4.1949606 -4.2001376 -4.1897964 -4.1668425][-4.2474227 -4.2424374 -4.2316861 -4.2235842 -4.2201667 -4.2139649 -4.2046661 -4.2005606 -4.1977239 -4.1934 -4.1893973 -4.1980963 -4.2088022 -4.2039328 -4.1913595][-4.251442 -4.2517104 -4.2534494 -4.2506037 -4.2433009 -4.2248297 -4.2039018 -4.1963205 -4.1951151 -4.1997223 -4.2078233 -4.22495 -4.2366738 -4.229351 -4.2205443][-4.246448 -4.2543116 -4.263258 -4.2588716 -4.2408876 -4.2050953 -4.1739764 -4.1725445 -4.1845345 -4.2030997 -4.2211289 -4.2402744 -4.2469897 -4.2360859 -4.2322717][-4.2249727 -4.2371879 -4.2479672 -4.2384982 -4.2044611 -4.147944 -4.1098928 -4.1243134 -4.1593566 -4.1926694 -4.21702 -4.2341752 -4.2366848 -4.222599 -4.22212][-4.1789103 -4.1893868 -4.1925364 -4.1727581 -4.1257195 -4.0590963 -4.0282397 -4.068861 -4.1285586 -4.1682906 -4.18835 -4.1979284 -4.19716 -4.1804647 -4.1804252][-4.1531658 -4.1538596 -4.1456461 -4.1209774 -4.0802627 -4.0270476 -4.0123796 -4.0629072 -4.1216516 -4.1485753 -4.1536689 -4.1541014 -4.1509705 -4.1328688 -4.1291461][-4.176321 -4.1691532 -4.1568217 -4.1403122 -4.1195545 -4.0897884 -4.0825539 -4.1172943 -4.1536508 -4.1632452 -4.1600428 -4.1585197 -4.153893 -4.13505 -4.1278181][-4.2090611 -4.2038636 -4.1933846 -4.1845055 -4.1763105 -4.1623597 -4.1603026 -4.1830192 -4.1994729 -4.1998525 -4.1968455 -4.1949515 -4.186296 -4.167706 -4.1579809][-4.220387 -4.2176523 -4.2115645 -4.2075076 -4.2074943 -4.2043242 -4.2099347 -4.2277293 -4.235548 -4.2344356 -4.2318478 -4.2269964 -4.2152972 -4.1992226 -4.1864004][-4.2131071 -4.2109938 -4.2089996 -4.2120118 -4.2192869 -4.2229261 -4.2315474 -4.24558 -4.2485538 -4.2456589 -4.2427058 -4.239718 -4.2313075 -4.2189574 -4.205626][-4.2033572 -4.202631 -4.2053003 -4.2161322 -4.2310863 -4.2395859 -4.2454815 -4.2519193 -4.2507653 -4.245985 -4.2424541 -4.2416768 -4.239655 -4.2336054 -4.223536][-4.1970305 -4.1999807 -4.2083688 -4.225039 -4.2424908 -4.2507734 -4.2542472 -4.25653 -4.2528734 -4.2468276 -4.2432437 -4.2446947 -4.2477379 -4.247602 -4.2419367]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-use_nesterov:False-clip10/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-use_nesterov:False-clip10/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 05:06:06.785595: step 10, loss = 2.06, batch loss = 2.00 (18.1 examples/sec; 0.443 sec/batch; 40h:54m:54s remains)
INFO - root - 2017-12-06 05:06:11.404366: step 20, loss = 2.05, batch loss = 1.99 (17.0 examples/sec; 0.470 sec/batch; 43h:23m:07s remains)
INFO - root - 2017-12-06 05:06:15.937930: step 30, loss = 2.06, batch loss = 2.00 (15.8 examples/sec; 0.507 sec/batch; 46h:47m:29s remains)
INFO - root - 2017-12-06 05:06:20.550138: step 40, loss = 2.09, batch loss = 2.03 (16.8 examples/sec; 0.476 sec/batch; 43h:57m:02s remains)
INFO - root - 2017-12-06 05:06:25.123769: step 50, loss = 2.09, batch loss = 2.03 (18.3 examples/sec; 0.436 sec/batch; 40h:16m:08s remains)
INFO - root - 2017-12-06 05:06:29.701746: step 60, loss = 2.10, batch loss = 2.04 (18.5 examples/sec; 0.432 sec/batch; 39h:52m:15s remains)
INFO - root - 2017-12-06 05:06:34.448822: step 70, loss = 2.07, batch loss = 2.01 (17.0 examples/sec; 0.471 sec/batch; 43h:31m:17s remains)
INFO - root - 2017-12-06 05:06:38.946575: step 80, loss = 2.06, batch loss = 2.00 (16.7 examples/sec; 0.479 sec/batch; 44h:14m:25s remains)
INFO - root - 2017-12-06 05:06:43.329496: step 90, loss = 2.09, batch loss = 2.03 (17.5 examples/sec; 0.457 sec/batch; 42h:11m:14s remains)
INFO - root - 2017-12-06 05:06:47.808709: step 100, loss = 2.06, batch loss = 2.00 (18.6 examples/sec; 0.431 sec/batch; 39h:47m:56s remains)
2017-12-06 05:06:48.338048: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2304459 -4.2270436 -4.2268143 -4.2282381 -4.2173796 -4.1946354 -4.183476 -4.1818151 -4.1850786 -4.1905675 -4.2157221 -4.2462692 -4.255084 -4.2500925 -4.2409286][-4.2483287 -4.2458897 -4.2442904 -4.2394862 -4.2182856 -4.1916738 -4.1825304 -4.1839204 -4.18933 -4.1972418 -4.2228665 -4.2558961 -4.2664719 -4.259407 -4.254231][-4.2632122 -4.2582259 -4.2499547 -4.2362185 -4.2071943 -4.1848354 -4.1848803 -4.1928544 -4.2007351 -4.2063231 -4.2254534 -4.2520709 -4.2585573 -4.2527227 -4.2542257][-4.2661486 -4.256793 -4.2380142 -4.2131944 -4.1767306 -4.1596179 -4.1721797 -4.1903515 -4.203836 -4.2108502 -4.2253036 -4.2439981 -4.2468085 -4.2455192 -4.25704][-4.2470703 -4.2292995 -4.1982241 -4.1613712 -4.1169758 -4.1002274 -4.1239243 -4.1588316 -4.1849613 -4.1998224 -4.2184386 -4.2359271 -4.2385855 -4.2394972 -4.2584186][-4.2128568 -4.1907439 -4.1514654 -4.1116037 -4.0686021 -4.0490713 -4.0765877 -4.12818 -4.1676846 -4.1906214 -4.2142696 -4.2288618 -4.2280641 -4.22964 -4.25207][-4.1867623 -4.16516 -4.1238623 -4.0874567 -4.0550313 -4.0354261 -4.0613136 -4.1204548 -4.1659169 -4.1931562 -4.215981 -4.22513 -4.2182069 -4.2166295 -4.2386155][-4.1865864 -4.1705985 -4.1341124 -4.1053758 -4.0853305 -4.0711393 -4.09204 -4.1443028 -4.1842914 -4.2071795 -4.2251635 -4.2293324 -4.2182236 -4.21227 -4.2305088][-4.2240839 -4.2154946 -4.1894288 -4.1707664 -4.1610985 -4.1517725 -4.1643705 -4.1994882 -4.2242661 -4.2342062 -4.2435932 -4.2434149 -4.2278256 -4.216383 -4.228878][-4.2711611 -4.2702227 -4.2551761 -4.245831 -4.2395382 -4.2284813 -4.2303147 -4.2453794 -4.2536631 -4.2559371 -4.2623854 -4.2598305 -4.241786 -4.2248316 -4.2274652][-4.299057 -4.3043418 -4.2984886 -4.2961535 -4.2925954 -4.279789 -4.2742491 -4.2762442 -4.2750793 -4.2747269 -4.280509 -4.2763591 -4.2553606 -4.2301903 -4.2185359][-4.3004417 -4.3090305 -4.30823 -4.3107672 -4.3111334 -4.2990365 -4.2908955 -4.2883391 -4.284904 -4.2839022 -4.2876329 -4.2815676 -4.2584572 -4.2288051 -4.2065911][-4.2915955 -4.2992973 -4.2989359 -4.3042336 -4.30895 -4.3006539 -4.29245 -4.2883248 -4.2851162 -4.2856565 -4.2873483 -4.2776637 -4.2530656 -4.2268023 -4.2033696][-4.2985382 -4.3036003 -4.3041644 -4.3094196 -4.3145523 -4.3104835 -4.3043065 -4.3004608 -4.2980032 -4.2978854 -4.2942009 -4.276659 -4.2500844 -4.2281609 -4.2048841][-4.3095503 -4.312994 -4.314136 -4.317544 -4.3209348 -4.3199372 -4.3175869 -4.3154573 -4.31352 -4.3120141 -4.3033161 -4.27992 -4.252727 -4.2332931 -4.2117839]]...]
INFO - root - 2017-12-06 05:06:52.787256: step 110, loss = 2.08, batch loss = 2.02 (17.6 examples/sec; 0.454 sec/batch; 41h:57m:45s remains)
INFO - root - 2017-12-06 05:06:57.231828: step 120, loss = 2.07, batch loss = 2.01 (17.7 examples/sec; 0.452 sec/batch; 41h:45m:12s remains)
INFO - root - 2017-12-06 05:07:01.816386: step 130, loss = 2.06, batch loss = 2.01 (18.7 examples/sec; 0.427 sec/batch; 39h:27m:44s remains)
INFO - root - 2017-12-06 05:07:06.334646: step 140, loss = 2.06, batch loss = 2.00 (17.7 examples/sec; 0.451 sec/batch; 41h:36m:56s remains)
INFO - root - 2017-12-06 05:07:10.818514: step 150, loss = 2.09, batch loss = 2.03 (17.5 examples/sec; 0.456 sec/batch; 42h:05m:36s remains)
INFO - root - 2017-12-06 05:07:15.415948: step 160, loss = 2.06, batch loss = 2.00 (18.1 examples/sec; 0.443 sec/batch; 40h:52m:14s remains)
INFO - root - 2017-12-06 05:07:19.808402: step 170, loss = 2.08, batch loss = 2.02 (18.1 examples/sec; 0.443 sec/batch; 40h:53m:00s remains)
INFO - root - 2017-12-06 05:07:24.066717: step 180, loss = 2.06, batch loss = 2.00 (17.9 examples/sec; 0.448 sec/batch; 41h:21m:50s remains)
INFO - root - 2017-12-06 05:07:28.451369: step 190, loss = 2.08, batch loss = 2.02 (17.8 examples/sec; 0.449 sec/batch; 41h:25m:20s remains)
INFO - root - 2017-12-06 05:07:32.926119: step 200, loss = 2.06, batch loss = 2.00 (17.7 examples/sec; 0.452 sec/batch; 41h:42m:34s remains)
2017-12-06 05:07:35.911476: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3249688 -4.3203816 -4.3171196 -4.3171964 -4.3204341 -4.3253417 -4.33034 -4.3347044 -4.3388205 -4.3427668 -4.3468556 -4.350493 -4.3518524 -4.3515129 -4.3495474][-4.3120608 -4.3045893 -4.2994051 -4.3006225 -4.3072662 -4.3139377 -4.3185143 -4.3216114 -4.3242207 -4.3289609 -4.3349743 -4.3422413 -4.3475304 -4.3492303 -4.3471527][-4.2914805 -4.2802029 -4.2744117 -4.2806153 -4.2917242 -4.2975111 -4.2969337 -4.2947841 -4.293891 -4.2982235 -4.3093176 -4.3241787 -4.3353066 -4.339169 -4.3373346][-4.2713089 -4.2567859 -4.2532725 -4.2641821 -4.2750158 -4.2740645 -4.2628155 -4.2509637 -4.2439551 -4.251276 -4.2732053 -4.2985506 -4.3165894 -4.323977 -4.3244047][-4.2603769 -4.24486 -4.2418089 -4.2511272 -4.254344 -4.2394476 -4.2129626 -4.1886129 -4.1792107 -4.1952362 -4.2330317 -4.267312 -4.2912707 -4.3038969 -4.3083992][-4.2506437 -4.2336278 -4.2268863 -4.2263169 -4.2152877 -4.1829948 -4.1407843 -4.1083493 -4.1102171 -4.1474915 -4.2013435 -4.241641 -4.2680221 -4.2856665 -4.2951989][-4.2221189 -4.1970458 -4.1780052 -4.1620879 -4.132236 -4.0777783 -4.0165057 -3.9863131 -4.0203629 -4.0945358 -4.1659923 -4.2125654 -4.24433 -4.2705765 -4.2872372][-4.183239 -4.1441865 -4.1075292 -4.0700955 -4.0147376 -3.9368873 -3.86042 -3.8453586 -3.9294322 -4.0450878 -4.133841 -4.1866612 -4.2267523 -4.261764 -4.2835793][-4.1461329 -4.09856 -4.0510988 -4.0008802 -3.9378486 -3.8605151 -3.7921658 -3.7991114 -3.9085612 -4.036675 -4.1278009 -4.1851668 -4.2306657 -4.2679429 -4.2889376][-4.1115255 -4.0655174 -4.0245228 -3.9880211 -3.948432 -3.9043808 -3.8738256 -3.8989356 -3.9859173 -4.0843897 -4.1579318 -4.2083292 -4.2502022 -4.2829781 -4.3001184][-4.1039786 -4.0689163 -4.0470653 -4.0364857 -4.0302587 -4.0207124 -4.0139823 -4.0378046 -4.0910449 -4.1514912 -4.1998277 -4.237412 -4.2715774 -4.2978296 -4.3117671][-4.1290274 -4.1099095 -4.1088295 -4.1172891 -4.1305127 -4.1378884 -4.1363773 -4.1497517 -4.1771607 -4.2099409 -4.2390771 -4.2646189 -4.2927446 -4.3150406 -4.3261571][-4.1800952 -4.1744995 -4.1839962 -4.1961021 -4.2103014 -4.2186322 -4.2169275 -4.2235165 -4.2371235 -4.2551603 -4.2738008 -4.2907915 -4.3136992 -4.332655 -4.3410497][-4.2343669 -4.2364435 -4.2456689 -4.25305 -4.2589221 -4.2609148 -4.2564864 -4.2576733 -4.2638931 -4.2757239 -4.2905455 -4.306097 -4.3262186 -4.3423414 -4.3491273][-4.2762971 -4.2789335 -4.2838311 -4.2865906 -4.2870164 -4.2854333 -4.2818828 -4.2812824 -4.2833853 -4.2896347 -4.3009839 -4.3156276 -4.3328772 -4.3451967 -4.3502865]]...]
INFO - root - 2017-12-06 05:07:40.515874: step 210, loss = 2.05, batch loss = 1.99 (17.8 examples/sec; 0.448 sec/batch; 41h:22m:11s remains)
INFO - root - 2017-12-06 05:07:44.949597: step 220, loss = 2.08, batch loss = 2.02 (18.3 examples/sec; 0.437 sec/batch; 40h:17m:44s remains)
INFO - root - 2017-12-06 05:07:49.558614: step 230, loss = 2.06, batch loss = 2.00 (17.3 examples/sec; 0.464 sec/batch; 42h:48m:03s remains)
INFO - root - 2017-12-06 05:07:54.073352: step 240, loss = 2.05, batch loss = 1.99 (17.9 examples/sec; 0.448 sec/batch; 41h:19m:47s remains)
INFO - root - 2017-12-06 05:07:58.665235: step 250, loss = 2.06, batch loss = 2.00 (17.6 examples/sec; 0.455 sec/batch; 41h:58m:57s remains)
INFO - root - 2017-12-06 05:08:03.092908: step 260, loss = 2.06, batch loss = 2.00 (17.9 examples/sec; 0.448 sec/batch; 41h:20m:42s remains)
INFO - root - 2017-12-06 05:08:07.430356: step 270, loss = 2.09, batch loss = 2.03 (17.7 examples/sec; 0.451 sec/batch; 41h:36m:58s remains)
INFO - root - 2017-12-06 05:08:11.867253: step 280, loss = 2.05, batch loss = 1.99 (17.7 examples/sec; 0.452 sec/batch; 41h:42m:41s remains)
INFO - root - 2017-12-06 05:08:16.367271: step 290, loss = 2.08, batch loss = 2.02 (17.9 examples/sec; 0.448 sec/batch; 41h:18m:02s remains)
INFO - root - 2017-12-06 05:08:20.848829: step 300, loss = 2.08, batch loss = 2.02 (17.3 examples/sec; 0.463 sec/batch; 42h:41m:40s remains)
2017-12-06 05:08:21.692205: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0791512 -4.0563784 -4.0564637 -4.0798469 -4.1233516 -4.1740127 -4.2102375 -4.2152104 -4.1878772 -4.1476007 -4.1189508 -4.1191463 -4.1288114 -4.124342 -4.1237359][-4.1552572 -4.1334887 -4.1147327 -4.1079679 -4.1264582 -4.1697865 -4.2125621 -4.226233 -4.20925 -4.1837778 -4.1688766 -4.1720643 -4.1749067 -4.1600695 -4.1445274][-4.2185884 -4.2028327 -4.1737318 -4.1422772 -4.1314988 -4.1586914 -4.20148 -4.223073 -4.2202086 -4.2135663 -4.2105622 -4.2103224 -4.2030144 -4.1754789 -4.1432905][-4.2474709 -4.2356071 -4.2023067 -4.1553454 -4.1213174 -4.1319256 -4.1682587 -4.1924419 -4.2004585 -4.20895 -4.2137294 -4.2080932 -4.1904769 -4.1557083 -4.1160526][-4.2325191 -4.225244 -4.1972408 -4.1482692 -4.1029072 -4.0979161 -4.1222115 -4.1453838 -4.1639609 -4.181859 -4.1915927 -4.1793118 -4.149929 -4.1081805 -4.0695286][-4.1972671 -4.1944437 -4.1704926 -4.1304379 -4.0901818 -4.0753388 -4.0839434 -4.1010714 -4.1234884 -4.1389279 -4.1440034 -4.1254683 -4.0895481 -4.046699 -4.0155725][-4.1733809 -4.1732297 -4.1515603 -4.1190443 -4.0911336 -4.0765033 -4.0738778 -4.0844455 -4.1072311 -4.1129808 -4.1066885 -4.0814075 -4.0386362 -3.9911742 -3.9659722][-4.1706529 -4.1696196 -4.1506233 -4.1256428 -4.1088939 -4.0994644 -4.0918088 -4.0949011 -4.1132269 -4.1115704 -4.0966306 -4.0665989 -4.0208926 -3.9691606 -3.9483569][-4.1783566 -4.1814551 -4.1738439 -4.1620317 -4.153121 -4.1461625 -4.13661 -4.130991 -4.1388197 -4.1303926 -4.1131873 -4.0861597 -4.0451736 -3.9994364 -3.986551][-4.1777067 -4.1949496 -4.2058043 -4.2090926 -4.2089219 -4.2032032 -4.1901689 -4.1775045 -4.1745296 -4.1628761 -4.1489429 -4.130259 -4.1004019 -4.0641165 -4.0540857][-4.1565375 -4.1926918 -4.2199645 -4.2330856 -4.2400184 -4.2375565 -4.2229066 -4.2051148 -4.1927438 -4.1776929 -4.1681209 -4.1656709 -4.1542234 -4.1355004 -4.1316133][-4.1291623 -4.17803 -4.2187161 -4.2407627 -4.2559333 -4.2601867 -4.2475681 -4.2290897 -4.2104621 -4.1888952 -4.1765828 -4.1858668 -4.1900158 -4.1844997 -4.1858668][-4.1117477 -4.16146 -4.2046504 -4.2302313 -4.2512579 -4.2591553 -4.25067 -4.2388582 -4.2187204 -4.1920285 -4.1780248 -4.1929617 -4.2058206 -4.206831 -4.2111626][-4.1205578 -4.1549773 -4.188529 -4.2132359 -4.2348261 -4.2445064 -4.2423415 -4.2375603 -4.2200217 -4.192275 -4.17759 -4.1926503 -4.2072282 -4.2091537 -4.2134271][-4.1488924 -4.1599054 -4.1737137 -4.1885061 -4.2067657 -4.2190747 -4.2244949 -4.2274351 -4.2147465 -4.1902709 -4.1766868 -4.1872411 -4.19556 -4.193749 -4.1927652]]...]
INFO - root - 2017-12-06 05:08:26.189140: step 310, loss = 2.07, batch loss = 2.01 (17.5 examples/sec; 0.458 sec/batch; 42h:17m:27s remains)
INFO - root - 2017-12-06 05:08:30.706371: step 320, loss = 2.05, batch loss = 1.99 (18.0 examples/sec; 0.446 sec/batch; 41h:06m:32s remains)
INFO - root - 2017-12-06 05:08:35.094056: step 330, loss = 2.07, batch loss = 2.02 (18.1 examples/sec; 0.443 sec/batch; 40h:50m:07s remains)
INFO - root - 2017-12-06 05:08:39.505173: step 340, loss = 2.07, batch loss = 2.01 (18.0 examples/sec; 0.445 sec/batch; 41h:01m:33s remains)
INFO - root - 2017-12-06 05:08:44.046955: step 350, loss = 2.07, batch loss = 2.01 (18.3 examples/sec; 0.438 sec/batch; 40h:22m:40s remains)
INFO - root - 2017-12-06 05:08:48.310154: step 360, loss = 2.09, batch loss = 2.03 (18.3 examples/sec; 0.437 sec/batch; 40h:17m:36s remains)
INFO - root - 2017-12-06 05:08:52.731022: step 370, loss = 2.08, batch loss = 2.02 (17.1 examples/sec; 0.468 sec/batch; 43h:08m:59s remains)
INFO - root - 2017-12-06 05:08:57.305886: step 380, loss = 2.07, batch loss = 2.01 (17.8 examples/sec; 0.449 sec/batch; 41h:27m:31s remains)
INFO - root - 2017-12-06 05:09:01.764791: step 390, loss = 2.05, batch loss = 1.99 (18.5 examples/sec; 0.434 sec/batch; 39h:59m:31s remains)
INFO - root - 2017-12-06 05:09:06.189866: step 400, loss = 2.06, batch loss = 2.00 (18.6 examples/sec; 0.430 sec/batch; 39h:37m:19s remains)
2017-12-06 05:09:06.696949: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2696486 -4.258337 -4.2611222 -4.2689834 -4.2889562 -4.2991858 -4.298337 -4.2907996 -4.2829361 -4.2822833 -4.2806616 -4.2735376 -4.2729478 -4.2764235 -4.2730494][-4.2692595 -4.2575083 -4.2588224 -4.269104 -4.2923789 -4.30224 -4.2977781 -4.2866583 -4.2724962 -4.27064 -4.2667685 -4.2564011 -4.249948 -4.2464166 -4.2419453][-4.2630963 -4.2478123 -4.2453508 -4.2568679 -4.2822332 -4.2915535 -4.2816734 -4.2680483 -4.25169 -4.2515836 -4.2505116 -4.2411637 -4.230763 -4.2201815 -4.2137213][-4.2361569 -4.2092381 -4.1991787 -4.2116628 -4.2390847 -4.2476678 -4.2362971 -4.2245927 -4.213068 -4.2189164 -4.22504 -4.2228508 -4.2146864 -4.2010922 -4.1925497][-4.2132969 -4.1735258 -4.1481891 -4.1525779 -4.1756754 -4.1824107 -4.1722426 -4.1663752 -4.16518 -4.1798186 -4.1948318 -4.20216 -4.199163 -4.1892481 -4.1808538][-4.2149968 -4.1723413 -4.13272 -4.1195946 -4.1240015 -4.1177483 -4.1026525 -4.10155 -4.114007 -4.1420727 -4.1690664 -4.1840343 -4.1870594 -4.1844158 -4.1783752][-4.2337527 -4.2012429 -4.1611605 -4.1397805 -4.133153 -4.1157832 -4.0808945 -4.0622315 -4.07373 -4.1072946 -4.1424875 -4.1662412 -4.1767445 -4.1830831 -4.1836443][-4.2389455 -4.2155108 -4.1836209 -4.1677856 -4.1716752 -4.1659174 -4.12578 -4.0891609 -4.085166 -4.1038065 -4.1308517 -4.1541128 -4.1674833 -4.1799407 -4.1889887][-4.2147851 -4.1956215 -4.1729035 -4.168262 -4.1903658 -4.2083378 -4.18292 -4.1508923 -4.1358438 -4.13427 -4.1429253 -4.1566663 -4.1650109 -4.1772251 -4.192739][-4.18098 -4.1607933 -4.1436076 -4.1505966 -4.1882615 -4.2229505 -4.2138929 -4.1970344 -4.1865582 -4.17888 -4.1763506 -4.1790867 -4.1795583 -4.1874127 -4.2032747][-4.1681137 -4.1446848 -4.1258359 -4.1346064 -4.1780505 -4.2195597 -4.2228303 -4.2184558 -4.2173834 -4.2156515 -4.216085 -4.2155504 -4.2121148 -4.2136631 -4.2227578][-4.19067 -4.1624646 -4.1367869 -4.1396804 -4.1781983 -4.2179608 -4.227766 -4.2305813 -4.2345805 -4.2397037 -4.2463741 -4.2484708 -4.2459593 -4.2443118 -4.2466545][-4.23433 -4.2096038 -4.1806607 -4.1725168 -4.1960368 -4.2223892 -4.2311769 -4.2377429 -4.2459178 -4.2544436 -4.2648325 -4.2718015 -4.2731619 -4.2715211 -4.2701435][-4.2748313 -4.2565651 -4.2316618 -4.2189765 -4.2313337 -4.2469778 -4.2514162 -4.2560625 -4.2640676 -4.2720642 -4.2816172 -4.2892613 -4.2918682 -4.2907014 -4.28936][-4.2997437 -4.2885451 -4.2716637 -4.261519 -4.2669148 -4.2749195 -4.2754602 -4.2774382 -4.28241 -4.2880492 -4.2943425 -4.2987847 -4.3013229 -4.3010969 -4.3012595]]...]
INFO - root - 2017-12-06 05:09:11.140329: step 410, loss = 2.08, batch loss = 2.02 (17.5 examples/sec; 0.457 sec/batch; 42h:09m:00s remains)
INFO - root - 2017-12-06 05:09:15.669430: step 420, loss = 2.06, batch loss = 2.00 (17.7 examples/sec; 0.452 sec/batch; 41h:42m:15s remains)
INFO - root - 2017-12-06 05:09:20.160474: step 430, loss = 2.07, batch loss = 2.01 (18.3 examples/sec; 0.438 sec/batch; 40h:21m:32s remains)
INFO - root - 2017-12-06 05:09:24.736845: step 440, loss = 2.07, batch loss = 2.01 (18.3 examples/sec; 0.437 sec/batch; 40h:20m:13s remains)
INFO - root - 2017-12-06 05:09:29.128022: step 450, loss = 2.08, batch loss = 2.02 (17.9 examples/sec; 0.446 sec/batch; 41h:09m:14s remains)
INFO - root - 2017-12-06 05:09:33.359842: step 460, loss = 2.08, batch loss = 2.02 (17.3 examples/sec; 0.463 sec/batch; 42h:43m:57s remains)
INFO - root - 2017-12-06 05:09:37.883595: step 470, loss = 2.06, batch loss = 2.01 (17.5 examples/sec; 0.456 sec/batch; 42h:02m:34s remains)
INFO - root - 2017-12-06 05:09:42.428655: step 480, loss = 2.04, batch loss = 1.99 (17.5 examples/sec; 0.457 sec/batch; 42h:06m:25s remains)
INFO - root - 2017-12-06 05:09:46.879953: step 490, loss = 2.08, batch loss = 2.02 (18.0 examples/sec; 0.445 sec/batch; 41h:03m:11s remains)
INFO - root - 2017-12-06 05:09:51.357218: step 500, loss = 2.07, batch loss = 2.01 (17.1 examples/sec; 0.468 sec/batch; 43h:09m:25s remains)
2017-12-06 05:09:51.831949: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2219076 -4.2540436 -4.2466755 -4.2328815 -4.2117519 -4.179502 -4.1458578 -4.1283283 -4.1317954 -4.1477666 -4.1641426 -4.1702194 -4.1651678 -4.164742 -4.1606445][-4.1905046 -4.2237525 -4.2168379 -4.2066541 -4.1905665 -4.15941 -4.1290755 -4.1156569 -4.1197314 -4.1345568 -4.153686 -4.1678772 -4.1706591 -4.1736674 -4.1685886][-4.149044 -4.1822205 -4.1806607 -4.1781235 -4.1675282 -4.136445 -4.1106067 -4.1029124 -4.1131444 -4.1310225 -4.1516914 -4.1727009 -4.1856208 -4.1891069 -4.1826043][-4.1108894 -4.1460533 -4.15413 -4.1622934 -4.1546221 -4.120717 -4.0963154 -4.0922213 -4.1062679 -4.1248045 -4.1467285 -4.1737518 -4.1966453 -4.2003927 -4.192946][-4.0869493 -4.1231713 -4.14108 -4.1573687 -4.1489768 -4.1088524 -4.0819607 -4.0776711 -4.0944781 -4.11624 -4.1431909 -4.1741734 -4.2023411 -4.2043581 -4.1945863][-4.076488 -4.1130943 -4.1379085 -4.1570225 -4.14049 -4.088594 -4.056622 -4.0492668 -4.0665865 -4.09581 -4.1335721 -4.1691551 -4.1998787 -4.2012796 -4.1887918][-4.0626712 -4.09909 -4.1326017 -4.1557522 -4.1336246 -4.0729823 -4.03034 -4.0119839 -4.0206747 -4.0567713 -4.1066203 -4.149188 -4.1839523 -4.1865745 -4.1752567][-4.03946 -4.0752797 -4.1191878 -4.1522107 -4.1337786 -4.0753555 -4.0256472 -3.989152 -3.97751 -4.0104008 -4.0670462 -4.1154218 -4.1536732 -4.159338 -4.152576][-4.021956 -4.0567489 -4.1097994 -4.15355 -4.1452661 -4.0975752 -4.0485182 -3.9985385 -3.9609594 -3.9794919 -4.0370717 -4.087461 -4.1213531 -4.125011 -4.1193676][-4.0124664 -4.0452185 -4.1043897 -4.1560445 -4.1592126 -4.123714 -4.0792713 -4.0214872 -3.9615006 -3.96398 -4.0184593 -4.0696611 -4.0980163 -4.0958743 -4.0879354][-4.00927 -4.0342221 -4.0957747 -4.1560116 -4.1727123 -4.1490436 -4.1066489 -4.0434475 -3.9682517 -3.9560649 -4.0072861 -4.0624528 -4.0876012 -4.0813308 -4.0723677][-4.0121527 -4.0277257 -4.0895014 -4.1563497 -4.1860819 -4.1738362 -4.1346083 -4.0706878 -3.9874122 -3.9610248 -4.0073133 -4.0667934 -4.09004 -4.0790491 -4.0671687][-4.0177016 -4.02506 -4.0857253 -4.1564441 -4.195693 -4.1945205 -4.1613536 -4.1034966 -4.0212603 -3.9838605 -4.0189714 -4.0755849 -4.09684 -4.0781469 -4.0589876][-4.0112514 -4.0157371 -4.0773139 -4.1497169 -4.1963482 -4.2048798 -4.1818576 -4.136292 -4.0662141 -4.0254412 -4.044827 -4.0877786 -4.1008949 -4.0724955 -4.0429859][-3.991576 -3.9939399 -4.0578756 -4.1330991 -4.1875572 -4.207973 -4.1999307 -4.1710215 -4.12116 -4.0857363 -4.0880542 -4.1085482 -4.1059027 -4.0673227 -4.0291]]...]
INFO - root - 2017-12-06 05:09:56.352088: step 510, loss = 2.08, batch loss = 2.02 (16.6 examples/sec; 0.482 sec/batch; 44h:26m:28s remains)
INFO - root - 2017-12-06 05:10:00.901750: step 520, loss = 2.07, batch loss = 2.01 (18.3 examples/sec; 0.438 sec/batch; 40h:22m:22s remains)
INFO - root - 2017-12-06 05:10:05.345843: step 530, loss = 2.04, batch loss = 1.99 (17.2 examples/sec; 0.465 sec/batch; 42h:52m:38s remains)
INFO - root - 2017-12-06 05:10:09.898064: step 540, loss = 2.06, batch loss = 2.00 (15.8 examples/sec; 0.507 sec/batch; 46h:45m:59s remains)
INFO - root - 2017-12-06 05:10:14.031659: step 550, loss = 2.06, batch loss = 2.00 (18.4 examples/sec; 0.435 sec/batch; 40h:04m:42s remains)
INFO - root - 2017-12-06 05:10:18.499535: step 560, loss = 2.06, batch loss = 2.01 (17.5 examples/sec; 0.457 sec/batch; 42h:07m:47s remains)
INFO - root - 2017-12-06 05:10:23.028323: step 570, loss = 2.06, batch loss = 2.00 (17.7 examples/sec; 0.451 sec/batch; 41h:35m:10s remains)
INFO - root - 2017-12-06 05:10:27.436379: step 580, loss = 2.07, batch loss = 2.02 (17.8 examples/sec; 0.450 sec/batch; 41h:30m:28s remains)
INFO - root - 2017-12-06 05:10:31.909370: step 590, loss = 2.08, batch loss = 2.02 (17.2 examples/sec; 0.464 sec/batch; 42h:48m:56s remains)
INFO - root - 2017-12-06 05:10:36.344279: step 600, loss = 2.05, batch loss = 1.99 (17.6 examples/sec; 0.455 sec/batch; 41h:56m:33s remains)
2017-12-06 05:10:36.823667: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2344422 -4.214107 -4.2060843 -4.2049165 -4.2039576 -4.2026768 -4.2027612 -4.2187347 -4.2502089 -4.2752962 -4.2884154 -4.2857265 -4.2585783 -4.2242804 -4.2160459][-4.2296467 -4.2168603 -4.2137103 -4.2128582 -4.2089438 -4.2043076 -4.205946 -4.2228408 -4.2552843 -4.2773972 -4.2866974 -4.282959 -4.2622418 -4.2401824 -4.2415237][-4.2115407 -4.1997194 -4.1984067 -4.2020311 -4.2068486 -4.2090349 -4.2161469 -4.2342844 -4.2598286 -4.2721276 -4.2728634 -4.2658038 -4.2492552 -4.2368221 -4.2481055][-4.1878886 -4.1755652 -4.1764817 -4.1837368 -4.1939983 -4.195869 -4.1960878 -4.2048821 -4.2205472 -4.2266774 -4.2281032 -4.2272248 -4.219902 -4.216291 -4.2330503][-4.1608672 -4.1520395 -4.1563106 -4.1651688 -4.1712422 -4.1602221 -4.1423492 -4.1413989 -4.1551094 -4.170764 -4.1853824 -4.1951971 -4.192287 -4.1879687 -4.1991887][-4.1353493 -4.1288495 -4.1349177 -4.1388245 -4.12963 -4.0978432 -4.0646858 -4.0645566 -4.0921106 -4.1314864 -4.1610489 -4.1717849 -4.1615868 -4.1454597 -4.1445532][-4.136858 -4.1322336 -4.13365 -4.118784 -4.0824819 -4.0250769 -3.9774473 -3.9833477 -4.0325689 -4.09205 -4.1267195 -4.1280971 -4.1051803 -4.0786343 -4.0763903][-4.1554179 -4.1524067 -4.1489477 -4.1238375 -4.0722127 -4.0005665 -3.9472315 -3.9570317 -4.0096979 -4.0634995 -4.0840592 -4.0700989 -4.0399952 -4.0173111 -4.0297103][-4.1810942 -4.1770539 -4.1721067 -4.1531363 -4.1089392 -4.0456505 -4.0014372 -4.0068254 -4.0397072 -4.0705342 -4.07091 -4.0433645 -4.0144148 -4.0050712 -4.0348144][-4.2073727 -4.20694 -4.2073283 -4.1973019 -4.1679277 -4.1229291 -4.0900893 -4.0896592 -4.1044421 -4.1150608 -4.1034942 -4.0750871 -4.051167 -4.0505061 -4.0829964][-4.2275133 -4.2318726 -4.2345762 -4.2307615 -4.2166281 -4.1928678 -4.175138 -4.1764584 -4.1817126 -4.1827383 -4.1695395 -4.144537 -4.123517 -4.1225939 -4.145936][-4.2356672 -4.2404685 -4.23839 -4.2358093 -4.2314715 -4.2249465 -4.2251821 -4.2344151 -4.2405353 -4.2406716 -4.2300906 -4.2087903 -4.1911373 -4.1901417 -4.2044005][-4.2340465 -4.2371116 -4.2295089 -4.225184 -4.2267365 -4.2332768 -4.2480006 -4.2635245 -4.2727375 -4.2736683 -4.2664618 -4.2516394 -4.2394695 -4.2402296 -4.2490029][-4.2349477 -4.2371178 -4.2259564 -4.2178068 -4.2209105 -4.23365 -4.2534676 -4.2693157 -4.2794809 -4.2816949 -4.2762856 -4.2647018 -4.2539434 -4.2540717 -4.2599955][-4.2522874 -4.2562733 -4.2403922 -4.226088 -4.2254863 -4.2365637 -4.2518978 -4.2630892 -4.2687359 -4.2678275 -4.2619095 -4.25169 -4.2432418 -4.2459569 -4.2526469]]...]
INFO - root - 2017-12-06 05:10:41.345855: step 610, loss = 2.09, batch loss = 2.03 (17.7 examples/sec; 0.451 sec/batch; 41h:36m:57s remains)
INFO - root - 2017-12-06 05:10:45.933815: step 620, loss = 2.05, batch loss = 1.99 (18.3 examples/sec; 0.438 sec/batch; 40h:23m:04s remains)
INFO - root - 2017-12-06 05:10:50.416109: step 630, loss = 2.07, batch loss = 2.01 (17.9 examples/sec; 0.446 sec/batch; 41h:09m:08s remains)
INFO - root - 2017-12-06 05:10:54.962102: step 640, loss = 2.08, batch loss = 2.02 (16.8 examples/sec; 0.477 sec/batch; 43h:59m:11s remains)
INFO - root - 2017-12-06 05:10:59.220023: step 650, loss = 2.08, batch loss = 2.02 (17.6 examples/sec; 0.454 sec/batch; 41h:52m:32s remains)
INFO - root - 2017-12-06 05:11:03.704219: step 660, loss = 2.06, batch loss = 2.00 (17.6 examples/sec; 0.454 sec/batch; 41h:48m:23s remains)
INFO - root - 2017-12-06 05:11:08.167146: step 670, loss = 2.11, batch loss = 2.05 (17.8 examples/sec; 0.450 sec/batch; 41h:26m:31s remains)
INFO - root - 2017-12-06 05:11:12.663018: step 680, loss = 2.08, batch loss = 2.02 (17.2 examples/sec; 0.466 sec/batch; 42h:56m:58s remains)
INFO - root - 2017-12-06 05:11:17.059569: step 690, loss = 2.07, batch loss = 2.01 (18.7 examples/sec; 0.428 sec/batch; 39h:24m:31s remains)
INFO - root - 2017-12-06 05:11:21.395731: step 700, loss = 2.06, batch loss = 2.00 (18.3 examples/sec; 0.436 sec/batch; 40h:12m:27s remains)
2017-12-06 05:11:21.873011: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2020721 -4.19852 -4.1977673 -4.1980414 -4.1989522 -4.2010212 -4.2024908 -4.2029004 -4.2033572 -4.2049685 -4.2067065 -4.2067046 -4.2021232 -4.1907749 -4.1768661][-4.2056217 -4.1996646 -4.1954784 -4.1916041 -4.1884193 -4.1870017 -4.1847286 -4.1814919 -4.1791949 -4.1799979 -4.1824408 -4.1828756 -4.1767473 -4.1631866 -4.1474204][-4.2304392 -4.2230906 -4.2171617 -4.210124 -4.2030683 -4.1978221 -4.1915669 -4.1838632 -4.1775417 -4.1752839 -4.17585 -4.1752353 -4.1678386 -4.1557941 -4.1417694][-4.2572064 -4.2496376 -4.2434611 -4.2359562 -4.228344 -4.222784 -4.2147417 -4.2043295 -4.1949015 -4.1900287 -4.1879897 -4.1856 -4.178896 -4.1703033 -4.1595798][-4.254056 -4.2469683 -4.2410622 -4.23661 -4.2335072 -4.2320676 -4.22649 -4.2165112 -4.2079024 -4.2048683 -4.2040529 -4.2020388 -4.1962318 -4.188694 -4.1780376][-4.2275686 -4.2176085 -4.208343 -4.2063394 -4.2101135 -4.2147741 -4.2134495 -4.2067275 -4.2021909 -4.2070141 -4.2139144 -4.2177181 -4.2152257 -4.2079568 -4.1950665][-4.1952806 -4.1777453 -4.1632118 -4.1613884 -4.1703167 -4.1783895 -4.1765561 -4.1690459 -4.1694951 -4.1854553 -4.2058544 -4.2213755 -4.2274976 -4.2234721 -4.2099314][-4.1701965 -4.1464276 -4.1288495 -4.1268563 -4.135685 -4.1397514 -4.1288848 -4.1135912 -4.1163421 -4.1425815 -4.176538 -4.2049594 -4.222538 -4.2266989 -4.217504][-4.15754 -4.1365662 -4.12205 -4.1216259 -4.1276369 -4.1226726 -4.100307 -4.07485 -4.0741558 -4.1019549 -4.140388 -4.1748786 -4.2001109 -4.2121482 -4.2100515][-4.1578832 -4.1475372 -4.141788 -4.1450758 -4.1496506 -4.1414642 -4.1184497 -4.0924497 -4.0882006 -4.1076913 -4.1382403 -4.1679416 -4.1902027 -4.2024813 -4.2020297][-4.1736717 -4.1713533 -4.1706629 -4.1741042 -4.1764064 -4.1683235 -4.150589 -4.1326733 -4.1319737 -4.1464171 -4.1680369 -4.1888962 -4.2027111 -4.207283 -4.2017994][-4.2046628 -4.2033296 -4.2007022 -4.1976752 -4.1925826 -4.181139 -4.1664977 -4.1562581 -4.1611056 -4.1755791 -4.1941681 -4.2117763 -4.22126 -4.2201996 -4.2089767][-4.2339592 -4.2298532 -4.223989 -4.2164731 -4.2066855 -4.1929383 -4.1783838 -4.1702027 -4.1761603 -4.1896977 -4.2054062 -4.2197003 -4.227757 -4.2254071 -4.2130995][-4.2479544 -4.2420797 -4.2349591 -4.2277756 -4.2205772 -4.2104783 -4.1975846 -4.1886292 -4.1906605 -4.19869 -4.2088618 -4.2187562 -4.2249022 -4.2228441 -4.2123733][-4.2420173 -4.2359047 -4.2292628 -4.2246313 -4.2212977 -4.2163196 -4.2094722 -4.2033725 -4.2029214 -4.20576 -4.209487 -4.214633 -4.2182121 -4.2149205 -4.2057862]]...]
INFO - root - 2017-12-06 05:11:26.312359: step 710, loss = 2.08, batch loss = 2.02 (17.9 examples/sec; 0.448 sec/batch; 41h:17m:35s remains)
INFO - root - 2017-12-06 05:11:30.746554: step 720, loss = 2.05, batch loss = 2.00 (18.6 examples/sec; 0.430 sec/batch; 39h:37m:17s remains)
INFO - root - 2017-12-06 05:11:35.155897: step 730, loss = 2.08, batch loss = 2.02 (17.5 examples/sec; 0.457 sec/batch; 42h:08m:29s remains)
INFO - root - 2017-12-06 05:11:39.534987: step 740, loss = 2.08, batch loss = 2.02 (27.6 examples/sec; 0.290 sec/batch; 26h:42m:50s remains)
INFO - root - 2017-12-06 05:11:43.739115: step 750, loss = 2.07, batch loss = 2.01 (18.2 examples/sec; 0.439 sec/batch; 40h:29m:15s remains)
INFO - root - 2017-12-06 05:11:48.289553: step 760, loss = 2.09, batch loss = 2.03 (17.2 examples/sec; 0.466 sec/batch; 42h:57m:49s remains)
INFO - root - 2017-12-06 05:11:52.798414: step 770, loss = 2.06, batch loss = 2.00 (18.2 examples/sec; 0.439 sec/batch; 40h:28m:55s remains)
INFO - root - 2017-12-06 05:11:57.279347: step 780, loss = 2.07, batch loss = 2.02 (16.7 examples/sec; 0.480 sec/batch; 44h:11m:30s remains)
INFO - root - 2017-12-06 05:12:01.889235: step 790, loss = 2.06, batch loss = 2.00 (18.2 examples/sec; 0.440 sec/batch; 40h:31m:46s remains)
INFO - root - 2017-12-06 05:12:06.442205: step 800, loss = 2.09, batch loss = 2.03 (18.0 examples/sec; 0.445 sec/batch; 41h:01m:33s remains)
2017-12-06 05:12:06.912114: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2299557 -4.2461214 -4.2609339 -4.2675328 -4.2594266 -4.2416759 -4.2281 -4.2232232 -4.2216959 -4.2202797 -4.2168655 -4.2162542 -4.2213383 -4.2428942 -4.2759342][-4.2195468 -4.2306271 -4.2448878 -4.255693 -4.2575316 -4.248579 -4.2390141 -4.2310772 -4.2236109 -4.2161527 -4.2114558 -4.2134748 -4.2210937 -4.2440085 -4.2769332][-4.2057815 -4.2128339 -4.2271061 -4.2405648 -4.2476792 -4.2456913 -4.2423482 -4.2354259 -4.2235041 -4.211165 -4.2045112 -4.206089 -4.2141623 -4.2373233 -4.2711563][-4.185226 -4.191411 -4.2044983 -4.2167149 -4.2220092 -4.2217288 -4.2223015 -4.218914 -4.20593 -4.1928854 -4.1865544 -4.1868677 -4.1959643 -4.2215962 -4.2589579][-4.1709709 -4.1744747 -4.1810312 -4.1847181 -4.18245 -4.1801229 -4.1816835 -4.183619 -4.1754494 -4.1668339 -4.1633906 -4.1624336 -4.1715813 -4.2000008 -4.2432766][-4.1541204 -4.1457052 -4.13698 -4.1287255 -4.1196222 -4.1173491 -4.1207776 -4.127727 -4.1240058 -4.1209168 -4.1241312 -4.12612 -4.1355143 -4.167419 -4.2196941][-4.1247292 -4.1042757 -4.0821447 -4.0633178 -4.0509806 -4.05178 -4.0577073 -4.0688829 -4.0688171 -4.0710239 -4.0784006 -4.0828791 -4.0958705 -4.1318865 -4.19306][-4.1102376 -4.088891 -4.0648727 -4.0429149 -4.0279422 -4.0292144 -4.0343614 -4.0456233 -4.0477228 -4.0520444 -4.0576396 -4.0620203 -4.0769544 -4.1130419 -4.1769414][-4.114265 -4.1014419 -4.0865903 -4.0707631 -4.0570087 -4.0561595 -4.0579424 -4.0636826 -4.0610366 -4.0626349 -4.0652285 -4.0699253 -4.0837922 -4.1158614 -4.1763554][-4.1209497 -4.1160221 -4.1115193 -4.10489 -4.0969872 -4.0948381 -4.0927744 -4.0933132 -4.0857339 -4.0820379 -4.0822849 -4.0851049 -4.0946035 -4.12252 -4.1791291][-4.14175 -4.143744 -4.1501403 -4.1534977 -4.1520519 -4.1492853 -4.1431947 -4.1378469 -4.1250892 -4.1165137 -4.112639 -4.1100512 -4.1114259 -4.1324024 -4.1839886][-4.1759844 -4.1798596 -4.1890922 -4.1964369 -4.1983767 -4.1965418 -4.1895871 -4.1816683 -4.1674557 -4.1560493 -4.1464834 -4.1365027 -4.130528 -4.1462507 -4.1933832][-4.2112708 -4.21365 -4.2195487 -4.2246914 -4.2258482 -4.2241931 -4.2182837 -4.2105222 -4.1990972 -4.1892738 -4.1789107 -4.16559 -4.1563678 -4.1696243 -4.2118983][-4.2478871 -4.2497816 -4.2538075 -4.2576675 -4.259037 -4.2575388 -4.2531829 -4.2476788 -4.2398849 -4.2319384 -4.2232838 -4.2096691 -4.1986804 -4.207828 -4.240829][-4.2878594 -4.2882204 -4.2897134 -4.2919636 -4.2936392 -4.2937794 -4.2919354 -4.2894087 -4.2845311 -4.2786508 -4.2727685 -4.26136 -4.2502217 -4.2553282 -4.2784905]]...]
INFO - root - 2017-12-06 05:12:11.450538: step 810, loss = 2.06, batch loss = 2.00 (18.0 examples/sec; 0.445 sec/batch; 40h:57m:57s remains)
INFO - root - 2017-12-06 05:12:15.870761: step 820, loss = 2.03, batch loss = 1.97 (17.7 examples/sec; 0.453 sec/batch; 41h:45m:03s remains)
INFO - root - 2017-12-06 05:12:20.232309: step 830, loss = 2.08, batch loss = 2.02 (18.6 examples/sec; 0.430 sec/batch; 39h:36m:49s remains)
INFO - root - 2017-12-06 05:12:24.552851: step 840, loss = 2.05, batch loss = 2.00 (17.3 examples/sec; 0.462 sec/batch; 42h:35m:56s remains)
INFO - root - 2017-12-06 05:12:29.133285: step 850, loss = 2.08, batch loss = 2.02 (17.4 examples/sec; 0.461 sec/batch; 42h:27m:04s remains)
INFO - root - 2017-12-06 05:12:33.567911: step 860, loss = 2.08, batch loss = 2.02 (17.9 examples/sec; 0.447 sec/batch; 41h:09m:02s remains)
INFO - root - 2017-12-06 05:12:37.944680: step 870, loss = 2.06, batch loss = 2.00 (18.2 examples/sec; 0.439 sec/batch; 40h:28m:17s remains)
INFO - root - 2017-12-06 05:12:42.448967: step 880, loss = 2.08, batch loss = 2.03 (17.6 examples/sec; 0.454 sec/batch; 41h:47m:54s remains)
INFO - root - 2017-12-06 05:12:46.923770: step 890, loss = 2.09, batch loss = 2.03 (17.7 examples/sec; 0.452 sec/batch; 41h:38m:00s remains)
INFO - root - 2017-12-06 05:12:51.480531: step 900, loss = 2.04, batch loss = 1.99 (18.0 examples/sec; 0.444 sec/batch; 40h:56m:28s remains)
2017-12-06 05:12:52.424022: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1116419 -4.1097136 -4.1089363 -4.0948157 -4.0818048 -4.0928884 -4.1133132 -4.124104 -4.1340466 -4.1354694 -4.1265593 -4.13442 -4.1229324 -4.10802 -4.1151109][-4.0948849 -4.1186852 -4.1311088 -4.1149297 -4.0918713 -4.0860424 -4.103127 -4.1216807 -4.1390028 -4.1404676 -4.12491 -4.1235514 -4.1088119 -4.0951118 -4.1112204][-4.0723858 -4.1287231 -4.156518 -4.1415277 -4.1130872 -4.0983596 -4.1067505 -4.11843 -4.1381707 -4.14487 -4.1283884 -4.1225429 -4.1033907 -4.0822015 -4.0986848][-4.0636468 -4.1398959 -4.1788864 -4.1669374 -4.1291924 -4.0972195 -4.0851822 -4.085083 -4.1133146 -4.1344004 -4.1311779 -4.1341462 -4.1204882 -4.0967822 -4.1039143][-4.1106873 -4.1744933 -4.2068548 -4.18819 -4.1325 -4.081677 -4.0422192 -4.0295496 -4.0696716 -4.1130586 -4.1270943 -4.1398306 -4.1394997 -4.1175146 -4.1119113][-4.1732607 -4.208076 -4.2169166 -4.1862636 -4.112875 -4.0435081 -3.9808955 -3.9597473 -4.0163875 -4.08415 -4.123075 -4.1429739 -4.1473045 -4.1212506 -4.1102362][-4.2063613 -4.2114711 -4.1974382 -4.1467752 -4.0585313 -3.971719 -3.8938384 -3.8826265 -3.9688256 -4.0633492 -4.1240258 -4.1409621 -4.1327958 -4.1008177 -4.0910783][-4.2194195 -4.2032886 -4.1661654 -4.0934916 -3.9920502 -3.8987455 -3.8256156 -3.8375435 -3.9572189 -4.0711269 -4.14089 -4.153131 -4.1359091 -4.0950346 -4.0779252][-4.2273903 -4.2065678 -4.1630454 -4.0912342 -4.0027032 -3.9337225 -3.8904123 -3.9153104 -4.0258412 -4.1223345 -4.1778193 -4.1823573 -4.1584425 -4.1075583 -4.0826054][-4.2451386 -4.2376857 -4.2051625 -4.151341 -4.088336 -4.0477791 -4.0286307 -4.045155 -4.1167989 -4.175921 -4.201756 -4.2035761 -4.188004 -4.1420584 -4.1160288][-4.249136 -4.2581172 -4.2452435 -4.2121387 -4.1733279 -4.1508327 -4.14262 -4.1479483 -4.1808591 -4.20789 -4.2110772 -4.2087746 -4.2053022 -4.1715937 -4.1465921][-4.2336144 -4.2523818 -4.2548103 -4.2399511 -4.2192874 -4.2087083 -4.2034135 -4.2008309 -4.21211 -4.2228332 -4.2139564 -4.2024374 -4.2033296 -4.1747336 -4.1471558][-4.2188106 -4.2367206 -4.248456 -4.2457476 -4.2348433 -4.2290659 -4.2274971 -4.2251453 -4.2324414 -4.2387304 -4.2300906 -4.2141948 -4.2131758 -4.1836705 -4.1485548][-4.2195272 -4.2273097 -4.2360525 -4.2395411 -4.2361059 -4.2341933 -4.2354116 -4.2335043 -4.2380586 -4.2465682 -4.2473722 -4.2414904 -4.2406754 -4.206986 -4.168016][-4.2298574 -4.2305989 -4.2350373 -4.2403769 -4.241693 -4.243278 -4.2482829 -4.2463088 -4.2476678 -4.2528005 -4.2532296 -4.25078 -4.2511115 -4.2235937 -4.1925726]]...]
INFO - root - 2017-12-06 05:12:57.051263: step 910, loss = 2.08, batch loss = 2.03 (17.4 examples/sec; 0.460 sec/batch; 42h:22m:32s remains)
INFO - root - 2017-12-06 05:13:01.566092: step 920, loss = 2.10, batch loss = 2.04 (18.2 examples/sec; 0.440 sec/batch; 40h:32m:30s remains)
INFO - root - 2017-12-06 05:13:05.999862: step 930, loss = 2.07, batch loss = 2.02 (19.3 examples/sec; 0.414 sec/batch; 38h:06m:30s remains)
INFO - root - 2017-12-06 05:13:10.370022: step 940, loss = 2.06, batch loss = 2.00 (18.5 examples/sec; 0.432 sec/batch; 39h:49m:14s remains)
INFO - root - 2017-12-06 05:13:14.885819: step 950, loss = 2.10, batch loss = 2.04 (17.7 examples/sec; 0.451 sec/batch; 41h:30m:53s remains)
INFO - root - 2017-12-06 05:13:19.417949: step 960, loss = 2.04, batch loss = 1.99 (17.3 examples/sec; 0.462 sec/batch; 42h:34m:28s remains)
INFO - root - 2017-12-06 05:13:23.920664: step 970, loss = 2.04, batch loss = 1.98 (17.5 examples/sec; 0.458 sec/batch; 42h:11m:18s remains)
INFO - root - 2017-12-06 05:13:28.514966: step 980, loss = 2.07, batch loss = 2.01 (17.6 examples/sec; 0.454 sec/batch; 41h:50m:00s remains)
INFO - root - 2017-12-06 05:13:33.070868: step 990, loss = 2.04, batch loss = 1.98 (16.8 examples/sec; 0.477 sec/batch; 43h:53m:46s remains)
INFO - root - 2017-12-06 05:13:37.494077: step 1000, loss = 2.03, batch loss = 1.97 (17.8 examples/sec; 0.450 sec/batch; 41h:24m:13s remains)
2017-12-06 05:13:38.097278: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1154351 -4.1233535 -4.1449661 -4.170073 -4.177341 -4.171164 -4.1509948 -4.1404729 -4.1471195 -4.156589 -4.1733718 -4.1917186 -4.2190986 -4.2483521 -4.2779255][-4.1226468 -4.1308551 -4.1483231 -4.1692567 -4.1746545 -4.1657758 -4.1423535 -4.1349106 -4.1506052 -4.168344 -4.1905684 -4.2107034 -4.2357926 -4.25994 -4.2854061][-4.11978 -4.1311007 -4.1440539 -4.159565 -4.1618881 -4.1488342 -4.1229534 -4.1176181 -4.1391206 -4.1644673 -4.191721 -4.213532 -4.2358809 -4.2585239 -4.2838807][-4.1118164 -4.1190133 -4.1259975 -4.1373653 -4.1374831 -4.1207905 -4.0919242 -4.0895729 -4.1172137 -4.1485381 -4.1784983 -4.2039013 -4.229321 -4.2544527 -4.2821069][-4.1198516 -4.1170416 -4.113966 -4.1141148 -4.1041055 -4.0768676 -4.0414996 -4.0447173 -4.086132 -4.1296558 -4.1646433 -4.1957722 -4.2261267 -4.2536483 -4.2840462][-4.1390343 -4.1260056 -4.1139903 -4.1006889 -4.07304 -4.02463 -3.9724932 -3.9766061 -4.0391455 -4.1040449 -4.1500382 -4.1887732 -4.22436 -4.2547126 -4.2865415][-4.1517715 -4.13166 -4.1139708 -4.0898371 -4.0475426 -3.9816012 -3.9132407 -3.9200842 -4.0033765 -4.0880594 -4.1457739 -4.1902595 -4.2272868 -4.2596855 -4.2914357][-4.1598263 -4.1375847 -4.1190457 -4.0934129 -4.0530052 -3.9907522 -3.9243159 -3.9329281 -4.0156765 -4.1005125 -4.1579862 -4.2011609 -4.2353783 -4.2673016 -4.2975364][-4.1665525 -4.1455026 -4.13022 -4.113461 -4.09057 -4.0532432 -4.0091553 -4.0144205 -4.071104 -4.1321926 -4.1755071 -4.2109365 -4.2409282 -4.2717056 -4.3018608][-4.1642365 -4.1448083 -4.1344109 -4.128551 -4.1238408 -4.1088758 -4.0815687 -4.0788193 -4.1096258 -4.1479821 -4.1777396 -4.2061677 -4.2347889 -4.2670374 -4.3003988][-4.1606612 -4.142549 -4.136054 -4.1371312 -4.1443682 -4.1461906 -4.130547 -4.1223168 -4.1368308 -4.16058 -4.1811137 -4.2055316 -4.2351389 -4.268362 -4.301723][-4.158309 -4.1426463 -4.1391754 -4.1453247 -4.1608772 -4.1726069 -4.1635976 -4.1540923 -4.1613479 -4.1770892 -4.1928849 -4.2166476 -4.2473097 -4.279068 -4.308259][-4.1719837 -4.1589823 -4.1581435 -4.1661277 -4.1815786 -4.1932988 -4.1847882 -4.1751089 -4.180418 -4.193924 -4.2099037 -4.2363014 -4.2675557 -4.2958207 -4.3194327][-4.2052264 -4.1966772 -4.1970654 -4.2029481 -4.2128825 -4.2191749 -4.2092938 -4.2012076 -4.2074094 -4.2225962 -4.2409368 -4.2657957 -4.292891 -4.315021 -4.332087][-4.2412076 -4.2360215 -4.2362046 -4.2397208 -4.2449908 -4.2472892 -4.2390265 -4.2334347 -4.2404122 -4.2543135 -4.2714367 -4.2913022 -4.3116274 -4.3274775 -4.33956]]...]
INFO - root - 2017-12-06 05:13:42.562376: step 1010, loss = 2.06, batch loss = 2.00 (18.4 examples/sec; 0.435 sec/batch; 40h:04m:40s remains)
INFO - root - 2017-12-06 05:13:46.944177: step 1020, loss = 2.05, batch loss = 1.99 (18.0 examples/sec; 0.445 sec/batch; 40h:58m:26s remains)
INFO - root - 2017-12-06 05:13:50.155593: step 1030, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.225 sec/batch; 20h:42m:59s remains)
INFO - root - 2017-12-06 05:13:54.577723: step 1040, loss = 2.08, batch loss = 2.02 (17.2 examples/sec; 0.464 sec/batch; 42h:42m:07s remains)
INFO - root - 2017-12-06 05:13:59.156630: step 1050, loss = 2.09, batch loss = 2.04 (18.4 examples/sec; 0.436 sec/batch; 40h:07m:29s remains)
INFO - root - 2017-12-06 05:14:03.681690: step 1060, loss = 2.07, batch loss = 2.01 (18.7 examples/sec; 0.427 sec/batch; 39h:18m:00s remains)
INFO - root - 2017-12-06 05:14:08.155908: step 1070, loss = 2.06, batch loss = 2.00 (18.5 examples/sec; 0.433 sec/batch; 39h:52m:43s remains)
INFO - root - 2017-12-06 05:14:12.539989: step 1080, loss = 2.06, batch loss = 2.00 (17.9 examples/sec; 0.446 sec/batch; 41h:02m:01s remains)
INFO - root - 2017-12-06 05:14:17.057258: step 1090, loss = 2.07, batch loss = 2.01 (17.7 examples/sec; 0.452 sec/batch; 41h:37m:40s remains)
INFO - root - 2017-12-06 05:14:21.522064: step 1100, loss = 2.05, batch loss = 2.00 (17.5 examples/sec; 0.458 sec/batch; 42h:08m:32s remains)
2017-12-06 05:14:22.093243: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2064729 -4.1779451 -4.1319842 -4.09615 -4.0858274 -4.0903425 -4.1214371 -4.1733637 -4.2117848 -4.2320638 -4.2450342 -4.2542934 -4.2663746 -4.2803783 -4.2898278][-4.2051773 -4.1754279 -4.1253004 -4.066154 -4.0200663 -3.9979274 -4.0364742 -4.1165948 -4.1764269 -4.2095361 -4.228878 -4.2426686 -4.2583828 -4.2762523 -4.29071][-4.2155948 -4.1899729 -4.140801 -4.0745349 -4.00911 -3.9650962 -3.9953814 -4.0781856 -4.1433964 -4.1826825 -4.2080679 -4.2265391 -4.2486148 -4.270865 -4.2883253][-4.2294569 -4.2110715 -4.1726637 -4.1215572 -4.0632749 -4.0131 -4.0212259 -4.0780134 -4.1321983 -4.1690331 -4.1952882 -4.2163415 -4.2429209 -4.269208 -4.2887821][-4.22154 -4.2075315 -4.1746283 -4.1353388 -4.0905032 -4.044919 -4.0384879 -4.0653143 -4.1043277 -4.1437149 -4.1756444 -4.2060351 -4.2356358 -4.2648196 -4.2869434][-4.1934628 -4.1825261 -4.1518979 -4.1158867 -4.0781784 -4.0367608 -4.0163159 -4.0148511 -4.0375204 -4.0833659 -4.131629 -4.1751595 -4.2114305 -4.2482657 -4.2777462][-4.1690741 -4.1579938 -4.1335869 -4.105814 -4.0741014 -4.0315642 -3.9965954 -3.9702191 -3.970547 -4.0186453 -4.0810738 -4.1356955 -4.1796093 -4.2231688 -4.2623796][-4.1823177 -4.1671681 -4.1437249 -4.1215906 -4.0970745 -4.0583448 -4.0218825 -3.9880998 -3.9724891 -4.0081582 -4.06595 -4.11932 -4.1659088 -4.2112575 -4.2557039][-4.2162013 -4.2006545 -4.1767054 -4.1546416 -4.1309381 -4.0989795 -4.071517 -4.0454173 -4.0244493 -4.0394821 -4.0794687 -4.1254792 -4.1707234 -4.217833 -4.2638712][-4.2305093 -4.2212024 -4.2033086 -4.1808167 -4.1543512 -4.1211224 -4.0943451 -4.0743041 -4.0572858 -4.0610418 -4.0900626 -4.1344032 -4.1786141 -4.2252154 -4.2713618][-4.2173257 -4.217452 -4.2110596 -4.1959586 -4.1723642 -4.1391754 -4.1069436 -4.0849204 -4.0681019 -4.0656757 -4.0881329 -4.1319242 -4.1772847 -4.2245846 -4.2704716][-4.207386 -4.2148676 -4.2192869 -4.2159066 -4.2010555 -4.172637 -4.1388378 -4.1115236 -4.0888653 -4.0770574 -4.089962 -4.1301689 -4.1746645 -4.2205324 -4.2638588][-4.2181268 -4.2283926 -4.2369208 -4.239861 -4.2344503 -4.21628 -4.1891637 -4.160759 -4.1328983 -4.1109943 -4.1119628 -4.1421809 -4.180088 -4.2212443 -4.2600746][-4.2334471 -4.2404962 -4.247004 -4.2518063 -4.2552438 -4.2490773 -4.2346334 -4.2105966 -4.1785336 -4.1489463 -4.13844 -4.1576514 -4.188066 -4.222497 -4.2560077][-4.2381458 -4.2390909 -4.241519 -4.2460928 -4.2516627 -4.2517328 -4.246747 -4.2268577 -4.19555 -4.1632795 -4.1489797 -4.162744 -4.1888771 -4.2185183 -4.2465811]]...]
INFO - root - 2017-12-06 05:14:26.648473: step 1110, loss = 2.05, batch loss = 1.99 (18.3 examples/sec; 0.438 sec/batch; 40h:18m:21s remains)
INFO - root - 2017-12-06 05:14:31.056611: step 1120, loss = 2.08, batch loss = 2.02 (18.1 examples/sec; 0.443 sec/batch; 40h:45m:01s remains)
INFO - root - 2017-12-06 05:14:35.211335: step 1130, loss = 2.07, batch loss = 2.01 (17.4 examples/sec; 0.461 sec/batch; 42h:23m:34s remains)
INFO - root - 2017-12-06 05:14:39.798684: step 1140, loss = 2.07, batch loss = 2.01 (17.5 examples/sec; 0.458 sec/batch; 42h:10m:25s remains)
INFO - root - 2017-12-06 05:14:44.245400: step 1150, loss = 2.07, batch loss = 2.01 (18.1 examples/sec; 0.442 sec/batch; 40h:41m:26s remains)
INFO - root - 2017-12-06 05:14:48.658701: step 1160, loss = 2.07, batch loss = 2.02 (18.6 examples/sec; 0.431 sec/batch; 39h:39m:58s remains)
INFO - root - 2017-12-06 05:14:53.252956: step 1170, loss = 2.07, batch loss = 2.01 (18.6 examples/sec; 0.430 sec/batch; 39h:32m:53s remains)
INFO - root - 2017-12-06 05:14:57.705289: step 1180, loss = 2.05, batch loss = 2.00 (18.1 examples/sec; 0.442 sec/batch; 40h:43m:18s remains)
INFO - root - 2017-12-06 05:15:02.202779: step 1190, loss = 2.06, batch loss = 2.00 (17.7 examples/sec; 0.452 sec/batch; 41h:38m:09s remains)
INFO - root - 2017-12-06 05:15:06.673225: step 1200, loss = 2.05, batch loss = 1.99 (17.8 examples/sec; 0.450 sec/batch; 41h:25m:35s remains)
2017-12-06 05:15:07.161895: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2521 -4.2607074 -4.2777586 -4.2885504 -4.285862 -4.2833614 -4.286499 -4.296699 -4.3021984 -4.2992587 -4.2893076 -4.2789936 -4.2698674 -4.2626615 -4.2613645][-4.2233572 -4.2285833 -4.2577009 -4.28169 -4.2858825 -4.2883406 -4.2946358 -4.3105245 -4.3208694 -4.3188262 -4.3012476 -4.2807264 -4.2641673 -4.2510495 -4.2513351][-4.1724548 -4.1800609 -4.2235646 -4.2606988 -4.2693915 -4.2734265 -4.284771 -4.3078012 -4.323308 -4.3256173 -4.3081508 -4.2838397 -4.2599754 -4.2386127 -4.2329493][-4.1413708 -4.1575456 -4.2087026 -4.247448 -4.2480922 -4.24082 -4.2487354 -4.2770591 -4.298789 -4.3078375 -4.3017612 -4.2877536 -4.2669258 -4.239521 -4.2261963][-4.1520796 -4.1777349 -4.2218313 -4.2442284 -4.2244358 -4.18835 -4.1797013 -4.2107186 -4.2439 -4.268889 -4.2826161 -4.2875528 -4.2784257 -4.2524533 -4.2330303][-4.1621943 -4.1911244 -4.2202268 -4.2179708 -4.170876 -4.097311 -4.0505009 -4.0870237 -4.1558819 -4.2131076 -4.2519555 -4.2778068 -4.2850142 -4.2687297 -4.2471404][-4.1722088 -4.1951733 -4.2072864 -4.1812472 -4.1063504 -3.9858315 -3.8780115 -3.9211702 -4.0479107 -4.1526542 -4.2192616 -4.2637687 -4.2833133 -4.2777238 -4.25878][-4.1842079 -4.1988354 -4.1989112 -4.1597385 -4.0743942 -3.946173 -3.813966 -3.8531444 -4.0065913 -4.1326714 -4.2127061 -4.2622256 -4.2873225 -4.28797 -4.2728887][-4.1931233 -4.203526 -4.2018723 -4.1703243 -4.11025 -4.0273981 -3.9452524 -3.9654343 -4.0716586 -4.1656122 -4.2289977 -4.2699513 -4.2936945 -4.3004956 -4.2916613][-4.2150345 -4.2221746 -4.2247782 -4.2088485 -4.1744719 -4.1360631 -4.1025362 -4.1111493 -4.1644645 -4.2161922 -4.2495842 -4.2721286 -4.2902236 -4.3017387 -4.3038864][-4.2482858 -4.2520609 -4.2553339 -4.24917 -4.229857 -4.2174845 -4.2126875 -4.2207823 -4.2414274 -4.261519 -4.2696233 -4.2740068 -4.279767 -4.2904053 -4.3024154][-4.2823958 -4.279232 -4.2784429 -4.2744808 -4.2653923 -4.2684464 -4.2773271 -4.2826848 -4.2817392 -4.2795792 -4.2745395 -4.2693071 -4.267561 -4.2739325 -4.2877393][-4.306325 -4.2998915 -4.2963729 -4.2892327 -4.2817917 -4.2883382 -4.2973166 -4.2932258 -4.2774754 -4.2639818 -4.2559581 -4.2520132 -4.2531204 -4.258009 -4.2667389][-4.3057761 -4.2990112 -4.2964482 -4.2879882 -4.2804666 -4.28626 -4.2924428 -4.2852693 -4.2646985 -4.2478132 -4.2386274 -4.2350559 -4.2407951 -4.252934 -4.26236][-4.2923126 -4.288271 -4.2868867 -4.2821832 -4.2800889 -4.2845683 -4.2860727 -4.2792783 -4.2612762 -4.2467642 -4.2398577 -4.2348289 -4.2440386 -4.2647805 -4.2778134]]...]
INFO - root - 2017-12-06 05:15:11.596766: step 1210, loss = 2.07, batch loss = 2.01 (19.0 examples/sec; 0.421 sec/batch; 38h:43m:53s remains)
INFO - root - 2017-12-06 05:15:16.161305: step 1220, loss = 2.06, batch loss = 2.00 (17.7 examples/sec; 0.453 sec/batch; 41h:41m:09s remains)
INFO - root - 2017-12-06 05:15:20.259127: step 1230, loss = 2.06, batch loss = 2.01 (17.6 examples/sec; 0.454 sec/batch; 41h:48m:02s remains)
INFO - root - 2017-12-06 05:15:24.702756: step 1240, loss = 2.08, batch loss = 2.02 (17.0 examples/sec; 0.471 sec/batch; 43h:19m:26s remains)
INFO - root - 2017-12-06 05:15:29.112304: step 1250, loss = 2.10, batch loss = 2.04 (18.9 examples/sec; 0.423 sec/batch; 38h:55m:56s remains)
INFO - root - 2017-12-06 05:15:33.568361: step 1260, loss = 2.05, batch loss = 1.99 (18.0 examples/sec; 0.444 sec/batch; 40h:50m:30s remains)
INFO - root - 2017-12-06 05:15:38.052987: step 1270, loss = 2.07, batch loss = 2.01 (18.3 examples/sec; 0.436 sec/batch; 40h:08m:37s remains)
INFO - root - 2017-12-06 05:15:42.436636: step 1280, loss = 2.05, batch loss = 2.00 (18.9 examples/sec; 0.422 sec/batch; 38h:50m:43s remains)
INFO - root - 2017-12-06 05:15:46.904120: step 1290, loss = 2.06, batch loss = 2.00 (18.4 examples/sec; 0.435 sec/batch; 40h:00m:23s remains)
INFO - root - 2017-12-06 05:15:51.315689: step 1300, loss = 2.07, batch loss = 2.01 (17.9 examples/sec; 0.446 sec/batch; 41h:01m:14s remains)
2017-12-06 05:15:51.815596: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.25855 -4.2768712 -4.2632442 -4.23385 -4.1989694 -4.1653004 -4.1513886 -4.162508 -4.1936784 -4.2151713 -4.2210617 -4.2181196 -4.2163482 -4.2212477 -4.2389297][-4.2614365 -4.2733245 -4.2560129 -4.2187428 -4.168797 -4.1197634 -4.096674 -4.1049776 -4.1408319 -4.1740804 -4.1931448 -4.1973081 -4.1971326 -4.1985259 -4.2090845][-4.2295413 -4.2340426 -4.2165027 -4.180501 -4.1262903 -4.0728235 -4.0459356 -4.0560136 -4.0945392 -4.1349168 -4.1613808 -4.1659584 -4.1632318 -4.1610565 -4.1652169][-4.1852837 -4.1838837 -4.167767 -4.1385036 -4.0894742 -4.0379443 -4.0072618 -4.0166454 -4.0589561 -4.1034784 -4.1319118 -4.1382365 -4.1331139 -4.1277609 -4.1308446][-4.1454177 -4.140975 -4.1308947 -4.1081877 -4.0659571 -4.0103049 -3.9630785 -3.9619231 -4.015873 -4.0740142 -4.1125374 -4.1273146 -4.126277 -4.1215568 -4.1243982][-4.1261358 -4.1172252 -4.1110978 -4.090261 -4.0467882 -3.9798777 -3.9104688 -3.8953438 -3.96883 -4.0533433 -4.1116219 -4.1413059 -4.1474972 -4.14456 -4.1470542][-4.1282206 -4.1171541 -4.1117096 -4.0897832 -4.0433989 -3.9726818 -3.893847 -3.8705113 -3.9536817 -4.0524416 -4.125401 -4.1651292 -4.1781034 -4.1819754 -4.1882963][-4.1293845 -4.1179752 -4.1132197 -4.0960793 -4.0589409 -4.0056677 -3.9505332 -3.9368677 -4.001833 -4.0848536 -4.152257 -4.189857 -4.2038155 -4.2152042 -4.2266116][-4.1216383 -4.1096559 -4.1065803 -4.1032314 -4.0831075 -4.0541434 -4.0267158 -4.0211844 -4.0606108 -4.1160312 -4.1723542 -4.2069163 -4.2223163 -4.2371297 -4.2484889][-4.1193929 -4.1119804 -4.1142793 -4.1231833 -4.1138616 -4.0962086 -4.081214 -4.0770068 -4.0954843 -4.1292825 -4.1768422 -4.2093349 -4.2256155 -4.2385864 -4.2446513][-4.134923 -4.1355824 -4.1445246 -4.1550136 -4.147264 -4.1335068 -4.120616 -4.1113987 -4.1188316 -4.1397495 -4.1781936 -4.2056694 -4.2191978 -4.2261 -4.2200451][-4.1623635 -4.1681223 -4.1789713 -4.182991 -4.1748123 -4.1654043 -4.1540122 -4.1419625 -4.143858 -4.1562338 -4.1799011 -4.1992168 -4.2090416 -4.2125812 -4.1963744][-4.1828532 -4.1907845 -4.2058134 -4.2118917 -4.2058024 -4.1982937 -4.1888437 -4.1767282 -4.1712122 -4.17053 -4.1793036 -4.18684 -4.1921868 -4.1966805 -4.1836753][-4.1966233 -4.2006507 -4.22073 -4.2345147 -4.2342572 -4.2284737 -4.2218542 -4.212791 -4.19846 -4.1838903 -4.177083 -4.171268 -4.1701479 -4.1791759 -4.1813912][-4.2099066 -4.2107244 -4.2309203 -4.2504878 -4.2578011 -4.2545466 -4.2493777 -4.2410445 -4.2253723 -4.2044563 -4.1877427 -4.1728573 -4.1659245 -4.1755862 -4.1859741]]...]
INFO - root - 2017-12-06 05:15:56.239018: step 1310, loss = 2.08, batch loss = 2.02 (18.0 examples/sec; 0.444 sec/batch; 40h:50m:16s remains)
INFO - root - 2017-12-06 05:16:00.492006: step 1320, loss = 2.05, batch loss = 1.99 (18.1 examples/sec; 0.442 sec/batch; 40h:37m:43s remains)
INFO - root - 2017-12-06 05:16:04.862699: step 1330, loss = 2.08, batch loss = 2.03 (18.3 examples/sec; 0.437 sec/batch; 40h:10m:04s remains)
INFO - root - 2017-12-06 05:16:09.373502: step 1340, loss = 2.07, batch loss = 2.02 (17.3 examples/sec; 0.462 sec/batch; 42h:27m:45s remains)
INFO - root - 2017-12-06 05:16:13.791539: step 1350, loss = 2.05, batch loss = 2.00 (18.9 examples/sec; 0.423 sec/batch; 38h:52m:16s remains)
INFO - root - 2017-12-06 05:16:18.153877: step 1360, loss = 2.05, batch loss = 1.99 (17.4 examples/sec; 0.461 sec/batch; 42h:22m:03s remains)
INFO - root - 2017-12-06 05:16:22.672024: step 1370, loss = 2.06, batch loss = 2.00 (17.6 examples/sec; 0.456 sec/batch; 41h:53m:50s remains)
INFO - root - 2017-12-06 05:16:27.059811: step 1380, loss = 2.05, batch loss = 1.99 (18.2 examples/sec; 0.440 sec/batch; 40h:26m:56s remains)
INFO - root - 2017-12-06 05:16:31.554779: step 1390, loss = 2.05, batch loss = 1.99 (18.1 examples/sec; 0.442 sec/batch; 40h:38m:28s remains)
INFO - root - 2017-12-06 05:16:36.013041: step 1400, loss = 2.06, batch loss = 2.00 (17.8 examples/sec; 0.448 sec/batch; 41h:13m:48s remains)
2017-12-06 05:16:36.475247: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2702146 -4.2626734 -4.2568569 -4.2550254 -4.2481947 -4.2408786 -4.2401652 -4.2459383 -4.2564178 -4.2674384 -4.2786016 -4.2937322 -4.3096461 -4.316431 -4.3208504][-4.2426291 -4.2347174 -4.2297649 -4.2340522 -4.228663 -4.2186694 -4.2166195 -4.2207146 -4.2331667 -4.2496691 -4.2619262 -4.2780757 -4.2988129 -4.3093476 -4.3164029][-4.2014184 -4.1902676 -4.1800985 -4.18843 -4.1867094 -4.1773219 -4.1758032 -4.1776123 -4.1905093 -4.2130327 -4.2291546 -4.2492943 -4.2772303 -4.2956147 -4.3095927][-4.1534872 -4.1361961 -4.120882 -4.13293 -4.1390653 -4.1373053 -4.1378064 -4.1376395 -4.1505537 -4.18143 -4.2067327 -4.2317681 -4.2627974 -4.2846408 -4.3030243][-4.1134367 -4.0861044 -4.0671892 -4.0871444 -4.1041827 -4.1079407 -4.1050434 -4.0977182 -4.109581 -4.1515527 -4.1904373 -4.2233772 -4.2563558 -4.2801085 -4.3002548][-4.0932217 -4.0651894 -4.0464106 -4.0716958 -4.095715 -4.0986214 -4.0860796 -4.0640411 -4.0711613 -4.1219506 -4.1706719 -4.20975 -4.2452383 -4.2735949 -4.2970624][-4.09877 -4.0800271 -4.0618992 -4.0835667 -4.10449 -4.0997095 -4.0760264 -4.0422893 -4.0471153 -4.1018224 -4.1526618 -4.1949635 -4.2340455 -4.2666821 -4.2938766][-4.1250944 -4.117074 -4.1031365 -4.114634 -4.1275568 -4.1157503 -4.0842977 -4.0484781 -4.0577917 -4.1123223 -4.1586595 -4.1961436 -4.2343464 -4.2659879 -4.2946424][-4.1616378 -4.1620765 -4.1563253 -4.1615982 -4.164485 -4.1462688 -4.1123104 -4.0825605 -4.0978308 -4.1496806 -4.187109 -4.2156925 -4.2467155 -4.2722163 -4.2985983][-4.1949487 -4.1926041 -4.1910887 -4.1916914 -4.1831064 -4.1600351 -4.1308379 -4.1127958 -4.1325421 -4.1788774 -4.2084341 -4.2305679 -4.2549267 -4.2759023 -4.2995377][-4.2180028 -4.2061086 -4.1979046 -4.1926994 -4.1782446 -4.1521373 -4.1291928 -4.1204786 -4.1429052 -4.1832628 -4.2111278 -4.2324362 -4.2548265 -4.2767539 -4.2996798][-4.2393842 -4.2218971 -4.2068005 -4.1953611 -4.171206 -4.1426263 -4.11901 -4.1088719 -4.1288118 -4.16773 -4.1972337 -4.2204971 -4.246398 -4.2727995 -4.2972875][-4.2485766 -4.2353559 -4.2194252 -4.2028995 -4.1734891 -4.1403403 -4.1066513 -4.0848475 -4.0951943 -4.1335616 -4.1670885 -4.1941643 -4.2265625 -4.2590489 -4.2881551][-4.2511225 -4.2415457 -4.2286038 -4.2119827 -4.1862335 -4.1522341 -4.1066589 -4.0700164 -4.0676713 -4.106348 -4.1449943 -4.1771088 -4.2141991 -4.2506914 -4.2822537][-4.2537756 -4.2484126 -4.2411084 -4.2278571 -4.2074637 -4.1745095 -4.1221733 -4.0740924 -4.0644879 -4.1046686 -4.149653 -4.1841369 -4.2196 -4.2547855 -4.283721]]...]
INFO - root - 2017-12-06 05:16:40.993261: step 1410, loss = 2.08, batch loss = 2.02 (17.9 examples/sec; 0.447 sec/batch; 41h:05m:29s remains)
INFO - root - 2017-12-06 05:16:45.318015: step 1420, loss = 2.06, batch loss = 2.00 (17.0 examples/sec; 0.471 sec/batch; 43h:16m:51s remains)
INFO - root - 2017-12-06 05:16:49.764852: step 1430, loss = 2.09, batch loss = 2.03 (18.0 examples/sec; 0.445 sec/batch; 40h:56m:59s remains)
INFO - root - 2017-12-06 05:16:54.188301: step 1440, loss = 2.07, batch loss = 2.02 (17.6 examples/sec; 0.454 sec/batch; 41h:42m:22s remains)
INFO - root - 2017-12-06 05:16:58.614064: step 1450, loss = 2.04, batch loss = 1.99 (18.2 examples/sec; 0.439 sec/batch; 40h:21m:50s remains)
INFO - root - 2017-12-06 05:17:03.083880: step 1460, loss = 2.09, batch loss = 2.04 (18.5 examples/sec; 0.433 sec/batch; 39h:46m:18s remains)
INFO - root - 2017-12-06 05:17:07.543847: step 1470, loss = 2.09, batch loss = 2.03 (18.2 examples/sec; 0.439 sec/batch; 40h:23m:32s remains)
INFO - root - 2017-12-06 05:17:12.002213: step 1480, loss = 2.08, batch loss = 2.02 (17.4 examples/sec; 0.459 sec/batch; 42h:11m:29s remains)
INFO - root - 2017-12-06 05:17:16.551854: step 1490, loss = 2.08, batch loss = 2.02 (17.9 examples/sec; 0.447 sec/batch; 41h:06m:37s remains)
INFO - root - 2017-12-06 05:17:21.003527: step 1500, loss = 2.07, batch loss = 2.02 (17.1 examples/sec; 0.468 sec/batch; 43h:00m:50s remains)
2017-12-06 05:17:21.496707: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2252173 -4.2233739 -4.2311988 -4.2381387 -4.226727 -4.2205977 -4.224772 -4.2376122 -4.2462544 -4.2326307 -4.2025967 -4.1994276 -4.2275753 -4.2474389 -4.2595234][-4.156436 -4.1581807 -4.1832528 -4.2051907 -4.1932087 -4.1814585 -4.1855721 -4.2051411 -4.2104125 -4.1933656 -4.16262 -4.1615696 -4.1979523 -4.2266417 -4.2440982][-4.1040344 -4.1108537 -4.1514411 -4.1857138 -4.1760106 -4.1553736 -4.1520572 -4.1707034 -4.1746192 -4.1631842 -4.1384058 -4.1387997 -4.1791573 -4.2133441 -4.2361951][-4.0900154 -4.0976405 -4.1420832 -4.1759439 -4.1661468 -4.1386251 -4.1286106 -4.1395307 -4.1437654 -4.1426449 -4.1292696 -4.1355515 -4.17607 -4.2101374 -4.2353988][-4.1016178 -4.108604 -4.15266 -4.1812387 -4.164547 -4.1312647 -4.1213183 -4.1323781 -4.1386409 -4.1416121 -4.1358361 -4.1477346 -4.1856136 -4.2144165 -4.2380185][-4.1312857 -4.1361384 -4.1743112 -4.1920938 -4.1647325 -4.1244826 -4.113348 -4.1279807 -4.1410666 -4.14479 -4.1409693 -4.1540046 -4.1899238 -4.2176437 -4.2418661][-4.1548352 -4.1581163 -4.1831627 -4.1885281 -4.1526108 -4.1033745 -4.0875063 -4.1063848 -4.1284771 -4.1364274 -4.129581 -4.1362762 -4.169342 -4.20214 -4.2335677][-4.1567593 -4.1581211 -4.1687942 -4.162868 -4.1249204 -4.070745 -4.0440245 -4.0588098 -4.0902267 -4.1049337 -4.0900216 -4.0874157 -4.1218581 -4.16799 -4.2113647][-4.1460018 -4.1463203 -4.147594 -4.1373587 -4.1026754 -4.0452929 -4.0039158 -4.0074511 -4.0435123 -4.062007 -4.0379429 -4.0232725 -4.0575628 -4.1192689 -4.1766696][-4.1412106 -4.1394529 -4.1365867 -4.1278858 -4.1012239 -4.049521 -4.004293 -3.9987013 -4.0291424 -4.0480657 -4.0237455 -4.0046234 -4.0326529 -4.0925813 -4.151093][-4.1354594 -4.13563 -4.1334243 -4.1299396 -4.1168466 -4.0847821 -4.0469246 -4.0306888 -4.0462208 -4.0601177 -4.0411072 -4.0232906 -4.0430408 -4.0921164 -4.1425018][-4.1391268 -4.1453719 -4.145782 -4.1458263 -4.143362 -4.1292105 -4.1026211 -4.0813389 -4.0815477 -4.0889273 -4.0766187 -4.0642343 -4.0786309 -4.1155891 -4.1572814][-4.1567979 -4.1681976 -4.1735291 -4.1756015 -4.177516 -4.1732674 -4.1558647 -4.1368442 -4.12704 -4.1278133 -4.1205983 -4.1146126 -4.1237249 -4.1492743 -4.1854606][-4.1866736 -4.2008467 -4.2118464 -4.216342 -4.2189493 -4.2188382 -4.2100382 -4.1989412 -4.1887231 -4.1845088 -4.1779718 -4.1743236 -4.1782289 -4.1924524 -4.2208424][-4.2295752 -4.2399569 -4.2498441 -4.2565293 -4.259789 -4.261024 -4.2574892 -4.2544889 -4.2511215 -4.2465649 -4.2403541 -4.2382421 -4.24106 -4.2486434 -4.2663703]]...]
INFO - root - 2017-12-06 05:17:25.699737: step 1510, loss = 2.07, batch loss = 2.01 (25.8 examples/sec; 0.310 sec/batch; 28h:30m:43s remains)
INFO - root - 2017-12-06 05:17:30.076466: step 1520, loss = 2.06, batch loss = 2.01 (18.4 examples/sec; 0.435 sec/batch; 39h:57m:23s remains)
INFO - root - 2017-12-06 05:17:34.527289: step 1530, loss = 2.06, batch loss = 2.01 (18.2 examples/sec; 0.439 sec/batch; 40h:21m:45s remains)
INFO - root - 2017-12-06 05:17:39.107002: step 1540, loss = 2.07, batch loss = 2.01 (18.4 examples/sec; 0.435 sec/batch; 39h:59m:57s remains)
INFO - root - 2017-12-06 05:17:43.605589: step 1550, loss = 2.05, batch loss = 1.99 (17.5 examples/sec; 0.457 sec/batch; 42h:00m:09s remains)
INFO - root - 2017-12-06 05:17:48.111557: step 1560, loss = 2.06, batch loss = 2.00 (17.7 examples/sec; 0.453 sec/batch; 41h:37m:07s remains)
INFO - root - 2017-12-06 05:17:52.579264: step 1570, loss = 2.06, batch loss = 2.00 (17.9 examples/sec; 0.448 sec/batch; 41h:10m:20s remains)
INFO - root - 2017-12-06 05:17:57.110109: step 1580, loss = 2.05, batch loss = 1.99 (18.6 examples/sec; 0.430 sec/batch; 39h:31m:20s remains)
INFO - root - 2017-12-06 05:18:01.537823: step 1590, loss = 2.08, batch loss = 2.03 (17.2 examples/sec; 0.465 sec/batch; 42h:44m:58s remains)
INFO - root - 2017-12-06 05:18:06.024655: step 1600, loss = 2.06, batch loss = 2.00 (17.7 examples/sec; 0.452 sec/batch; 41h:30m:38s remains)
2017-12-06 05:18:06.498621: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1957436 -4.1967506 -4.197443 -4.1907897 -4.1803083 -4.1639557 -4.1452703 -4.1411257 -4.1548672 -4.1621113 -4.1628609 -4.1725116 -4.1798806 -4.1731882 -4.1662211][-4.1819611 -4.1819391 -4.1799173 -4.1662183 -4.1516814 -4.1361456 -4.1136703 -4.1055179 -4.1225262 -4.1357126 -4.1396213 -4.15154 -4.1553822 -4.1356406 -4.1153326][-4.1899776 -4.1920485 -4.1906824 -4.1727676 -4.1558528 -4.1411457 -4.1201849 -4.1105161 -4.1257596 -4.1378284 -4.1431231 -4.1607585 -4.1668615 -4.1392865 -4.1045923][-4.174037 -4.1812267 -4.1851406 -4.1716232 -4.1576004 -4.1458168 -4.1267838 -4.11551 -4.1209955 -4.1272163 -4.1389813 -4.1646461 -4.1753 -4.1488576 -4.1130757][-4.1323447 -4.1497159 -4.1642613 -4.1603842 -4.1539316 -4.1471128 -4.1347814 -4.1227784 -4.1144576 -4.1147766 -4.1334224 -4.1636772 -4.1711578 -4.1467071 -4.114656][-4.0822554 -4.1030097 -4.1202707 -4.1191449 -4.1185021 -4.1243844 -4.1247296 -4.1164227 -4.1036711 -4.1011662 -4.1248488 -4.1525354 -4.1518 -4.1254277 -4.0974054][-4.0557189 -4.0726004 -4.0823011 -4.0737481 -4.0700922 -4.0749035 -4.0677629 -4.0504665 -4.039875 -4.0507445 -4.0894585 -4.1237392 -4.1226544 -4.0968151 -4.0788918][-4.0730596 -4.0815916 -4.0769691 -4.058322 -4.0434866 -4.032661 -3.9977252 -3.9465623 -3.9288971 -3.9681344 -4.0401587 -4.092411 -4.1050444 -4.0918994 -4.0895214][-4.1213374 -4.1220374 -4.1041751 -4.0800152 -4.0678372 -4.055593 -4.0090847 -3.9435 -3.9196577 -3.9702342 -4.04819 -4.1008086 -4.119081 -4.1203904 -4.1284552][-4.1548214 -4.1558275 -4.1393127 -4.1233563 -4.1275396 -4.1310568 -4.1023788 -4.0546775 -4.0385838 -4.0742669 -4.1242709 -4.1545506 -4.1664009 -4.17016 -4.1731482][-4.1930323 -4.1992812 -4.1909423 -4.1808262 -4.1881928 -4.1941628 -4.1760621 -4.1495485 -4.1483541 -4.1747141 -4.2024269 -4.21703 -4.2274075 -4.2341075 -4.2328973][-4.2409606 -4.2508488 -4.2476325 -4.241075 -4.2466888 -4.2474213 -4.2325416 -4.2171955 -4.224606 -4.2502861 -4.2676444 -4.2711883 -4.2750959 -4.2805915 -4.2790008][-4.2686987 -4.2766976 -4.2786574 -4.2773414 -4.2835932 -4.2858434 -4.2750034 -4.2646923 -4.2713561 -4.2904463 -4.3021502 -4.301506 -4.3006516 -4.3003936 -4.296937][-4.2774806 -4.2824979 -4.2894554 -4.2939844 -4.2999334 -4.3008766 -4.2951922 -4.289885 -4.2898059 -4.2947373 -4.299057 -4.2998457 -4.2974658 -4.2936292 -4.2884736][-4.2685151 -4.2716117 -4.2814608 -4.2887993 -4.2919736 -4.290875 -4.2892847 -4.2876105 -4.2836213 -4.2768931 -4.2710776 -4.2699156 -4.2717443 -4.2706804 -4.2666197]]...]
INFO - root - 2017-12-06 05:18:10.664897: step 1610, loss = 2.07, batch loss = 2.01 (17.2 examples/sec; 0.465 sec/batch; 42h:42m:12s remains)
INFO - root - 2017-12-06 05:18:15.098513: step 1620, loss = 2.04, batch loss = 1.98 (17.6 examples/sec; 0.455 sec/batch; 41h:50m:00s remains)
INFO - root - 2017-12-06 05:18:19.482075: step 1630, loss = 2.08, batch loss = 2.02 (18.7 examples/sec; 0.428 sec/batch; 39h:21m:33s remains)
INFO - root - 2017-12-06 05:18:23.809305: step 1640, loss = 2.06, batch loss = 2.00 (18.7 examples/sec; 0.429 sec/batch; 39h:25m:23s remains)
INFO - root - 2017-12-06 05:18:28.230492: step 1650, loss = 2.07, batch loss = 2.01 (17.7 examples/sec; 0.451 sec/batch; 41h:29m:28s remains)
INFO - root - 2017-12-06 05:18:32.746853: step 1660, loss = 2.07, batch loss = 2.01 (17.2 examples/sec; 0.464 sec/batch; 42h:41m:14s remains)
INFO - root - 2017-12-06 05:18:37.159332: step 1670, loss = 2.06, batch loss = 2.00 (18.7 examples/sec; 0.428 sec/batch; 39h:17m:57s remains)
INFO - root - 2017-12-06 05:18:41.771432: step 1680, loss = 2.06, batch loss = 2.01 (17.8 examples/sec; 0.448 sec/batch; 41h:11m:21s remains)
INFO - root - 2017-12-06 05:18:46.237626: step 1690, loss = 2.07, batch loss = 2.01 (17.0 examples/sec; 0.471 sec/batch; 43h:19m:03s remains)
INFO - root - 2017-12-06 05:18:50.617035: step 1700, loss = 2.08, batch loss = 2.02 (17.9 examples/sec; 0.447 sec/batch; 41h:03m:42s remains)
2017-12-06 05:18:51.095620: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1010437 -4.0605364 -4.096581 -4.175549 -4.25011 -4.2998109 -4.3116417 -4.3009992 -4.294075 -4.2909088 -4.2975616 -4.310792 -4.3180633 -4.3020811 -4.2768965][-4.0584335 -4.0097 -4.0567288 -4.1569366 -4.2422447 -4.2869697 -4.2920394 -4.2780366 -4.2721972 -4.2749758 -4.2908864 -4.3143554 -4.323545 -4.3016596 -4.2725296][-4.0723162 -4.0269752 -4.0751586 -4.1754813 -4.2528272 -4.2821531 -4.2712932 -4.249547 -4.2427511 -4.2519531 -4.2808332 -4.3194356 -4.329555 -4.3021469 -4.2675014][-4.1313248 -4.1117949 -4.1561975 -4.2302437 -4.2791896 -4.2787514 -4.2387757 -4.2035084 -4.2001033 -4.224791 -4.2707419 -4.32063 -4.3292685 -4.2996325 -4.2644391][-4.1994133 -4.2042308 -4.2344866 -4.2701015 -4.2775774 -4.2381411 -4.1612816 -4.1076522 -4.1183882 -4.1754766 -4.2461753 -4.3050756 -4.3141017 -4.2867107 -4.2565804][-4.2446508 -4.2565207 -4.2663794 -4.2664447 -4.2312288 -4.1395836 -4.0107594 -3.9379458 -3.982039 -4.0900831 -4.1949635 -4.2653608 -4.2796969 -4.2597504 -4.2370882][-4.2699428 -4.2801161 -4.2755885 -4.2482443 -4.1739879 -4.0343518 -3.8587325 -3.7804694 -3.8693242 -4.0212917 -4.1494441 -4.2247868 -4.2444944 -4.2299681 -4.2123446][-4.2910929 -4.2964172 -4.2814703 -4.2405138 -4.1561074 -4.0170922 -3.86193 -3.8168705 -3.913022 -4.0513644 -4.1613874 -4.2212143 -4.2330289 -4.2153239 -4.2002115][-4.3062453 -4.3022795 -4.2806611 -4.2404361 -4.1744137 -4.0788856 -3.9883659 -3.9797313 -4.0531445 -4.1490526 -4.2213979 -4.2519536 -4.2446613 -4.2184997 -4.20523][-4.30904 -4.3048663 -4.2849236 -4.2534952 -4.2090611 -4.1517549 -4.1074743 -4.1169977 -4.1739902 -4.2406378 -4.2845588 -4.28928 -4.2650776 -4.2342482 -4.2227659][-4.3100758 -4.3076119 -4.2932606 -4.2740965 -4.2490726 -4.2154417 -4.1907287 -4.2022524 -4.2467046 -4.2932425 -4.317637 -4.3106031 -4.2870507 -4.2607021 -4.2485847][-4.3146429 -4.3118973 -4.3023496 -4.2922482 -4.2797637 -4.2587032 -4.2425561 -4.2503586 -4.281117 -4.3100657 -4.3218093 -4.3139067 -4.2988281 -4.2812214 -4.268899][-4.3185153 -4.3129516 -4.3049555 -4.3007259 -4.297164 -4.289484 -4.2834725 -4.2881079 -4.3017893 -4.3111262 -4.3121839 -4.3051844 -4.2983279 -4.2892733 -4.2797813][-4.3236313 -4.3177772 -4.3117623 -4.3106713 -4.3126893 -4.3148785 -4.3168936 -4.3201003 -4.3221736 -4.3181229 -4.3125887 -4.3072023 -4.3043895 -4.2999635 -4.2937245][-4.325727 -4.3233256 -4.3198838 -4.3185544 -4.3203287 -4.3243012 -4.3284087 -4.33057 -4.3280082 -4.3221793 -4.318418 -4.3167434 -4.3178029 -4.3172188 -4.3137808]]...]
INFO - root - 2017-12-06 05:18:55.312810: step 1710, loss = 2.08, batch loss = 2.02 (17.5 examples/sec; 0.456 sec/batch; 41h:56m:19s remains)
INFO - root - 2017-12-06 05:18:59.808153: step 1720, loss = 2.06, batch loss = 2.00 (17.3 examples/sec; 0.463 sec/batch; 42h:31m:19s remains)
INFO - root - 2017-12-06 05:19:04.288853: step 1730, loss = 2.06, batch loss = 2.00 (17.8 examples/sec; 0.450 sec/batch; 41h:21m:16s remains)
INFO - root - 2017-12-06 05:19:08.875082: step 1740, loss = 2.07, batch loss = 2.01 (17.9 examples/sec; 0.447 sec/batch; 41h:03m:54s remains)
INFO - root - 2017-12-06 05:19:13.266027: step 1750, loss = 2.08, batch loss = 2.02 (18.2 examples/sec; 0.439 sec/batch; 40h:18m:44s remains)
INFO - root - 2017-12-06 05:19:17.705893: step 1760, loss = 2.09, batch loss = 2.03 (17.7 examples/sec; 0.451 sec/batch; 41h:26m:18s remains)
INFO - root - 2017-12-06 05:19:22.191165: step 1770, loss = 2.07, batch loss = 2.01 (18.2 examples/sec; 0.440 sec/batch; 40h:24m:04s remains)
INFO - root - 2017-12-06 05:19:26.627959: step 1780, loss = 2.08, batch loss = 2.02 (17.7 examples/sec; 0.453 sec/batch; 41h:37m:55s remains)
INFO - root - 2017-12-06 05:19:31.095314: step 1790, loss = 2.05, batch loss = 1.99 (18.3 examples/sec; 0.438 sec/batch; 40h:15m:46s remains)
INFO - root - 2017-12-06 05:19:35.510572: step 1800, loss = 2.06, batch loss = 2.00 (22.2 examples/sec; 0.360 sec/batch; 33h:04m:39s remains)
2017-12-06 05:19:35.930303: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2420125 -4.2262688 -4.2182221 -4.207592 -4.1931291 -4.197679 -4.216753 -4.2352428 -4.2483573 -4.2584805 -4.2663627 -4.2680917 -4.2697854 -4.2650542 -4.2543788][-4.2042041 -4.1815629 -4.1714869 -4.1644044 -4.1601934 -4.1733818 -4.1949062 -4.2123318 -4.2275872 -4.2380862 -4.2412553 -4.2378087 -4.2359076 -4.2296991 -4.2199192][-4.1663914 -4.1342154 -4.122942 -4.1169481 -4.1229386 -4.1475196 -4.1746159 -4.1949697 -4.2123413 -4.221024 -4.2195592 -4.2125621 -4.2097993 -4.2050295 -4.1990194][-4.1427903 -4.1032963 -4.0862904 -4.0761743 -4.083838 -4.1139793 -4.1461897 -4.1735597 -4.194664 -4.1999321 -4.192925 -4.1831088 -4.1812515 -4.1840396 -4.1888876][-4.1587963 -4.1273093 -4.1111121 -4.0884476 -4.0800438 -4.0977578 -4.12982 -4.1672988 -4.1951694 -4.1966586 -4.181076 -4.1678953 -4.16397 -4.1705608 -4.183918][-4.1912041 -4.1722236 -4.1597013 -4.127028 -4.0978947 -4.0885868 -4.1110439 -4.1576166 -4.1991076 -4.2072287 -4.1880517 -4.1688423 -4.1561532 -4.1508484 -4.1569481][-4.1960626 -4.194294 -4.1940336 -4.1599264 -4.1093793 -4.0672741 -4.0698891 -4.1155581 -4.1734061 -4.2024417 -4.1864843 -4.1655707 -4.1409903 -4.1100254 -4.0956006][-4.1801672 -4.1985135 -4.2097969 -4.1814318 -4.1176276 -4.0456667 -4.015893 -4.0463495 -4.1187811 -4.1747174 -4.1744328 -4.1484933 -4.1108527 -4.0489149 -4.0068507][-4.1550293 -4.1856689 -4.2049594 -4.1874514 -4.1180549 -4.0140123 -3.9417419 -3.9549611 -4.0425529 -4.1282711 -4.1469474 -4.1215377 -4.0779285 -3.9904566 -3.9218531][-4.1178217 -4.1504679 -4.172811 -4.1631246 -4.0992484 -3.9816656 -3.8684182 -3.8656261 -3.9725573 -4.0833807 -4.123713 -4.1071682 -4.0667081 -3.982465 -3.9106588][-4.09241 -4.1216254 -4.1463704 -4.1452718 -4.0988231 -4.0056772 -3.8900445 -3.8623743 -3.9545839 -4.06761 -4.1169386 -4.1165371 -4.1004014 -4.04945 -3.9974937][-4.0992956 -4.1180458 -4.1457033 -4.1572356 -4.1406837 -4.1000619 -4.0351982 -3.9975059 -4.0352139 -4.1076636 -4.1487832 -4.1588478 -4.165503 -4.1439934 -4.1071968][-4.1431766 -4.1458597 -4.1684113 -4.1857109 -4.1870232 -4.1844735 -4.167263 -4.1415515 -4.1471696 -4.1771417 -4.1990561 -4.207068 -4.2169161 -4.207 -4.1819825][-4.2110906 -4.2013378 -4.2090306 -4.2192554 -4.2219672 -4.2314067 -4.236835 -4.229075 -4.2301412 -4.2392731 -4.248807 -4.2501006 -4.2564344 -4.2564235 -4.23995][-4.2766576 -4.263803 -4.2620392 -4.2652707 -4.2658687 -4.2760754 -4.2857208 -4.2876949 -4.2895994 -4.2926221 -4.299993 -4.2992959 -4.3005848 -4.3017697 -4.2915511]]...]
INFO - root - 2017-12-06 05:19:40.343339: step 1810, loss = 2.04, batch loss = 1.98 (18.0 examples/sec; 0.444 sec/batch; 40h:49m:13s remains)
INFO - root - 2017-12-06 05:19:44.868896: step 1820, loss = 2.07, batch loss = 2.01 (18.1 examples/sec; 0.443 sec/batch; 40h:40m:00s remains)
INFO - root - 2017-12-06 05:19:49.354825: step 1830, loss = 2.04, batch loss = 1.98 (18.3 examples/sec; 0.438 sec/batch; 40h:14m:33s remains)
INFO - root - 2017-12-06 05:19:53.901624: step 1840, loss = 2.05, batch loss = 1.99 (18.4 examples/sec; 0.436 sec/batch; 40h:02m:26s remains)
INFO - root - 2017-12-06 05:19:58.358476: step 1850, loss = 2.06, batch loss = 2.00 (19.1 examples/sec; 0.418 sec/batch; 38h:23m:58s remains)
INFO - root - 2017-12-06 05:20:02.907844: step 1860, loss = 2.03, batch loss = 1.97 (18.1 examples/sec; 0.443 sec/batch; 40h:40m:49s remains)
INFO - root - 2017-12-06 05:20:07.338869: step 1870, loss = 2.08, batch loss = 2.02 (17.3 examples/sec; 0.462 sec/batch; 42h:27m:01s remains)
INFO - root - 2017-12-06 05:20:11.921631: step 1880, loss = 2.07, batch loss = 2.01 (17.6 examples/sec; 0.454 sec/batch; 41h:44m:12s remains)
INFO - root - 2017-12-06 05:20:16.354997: step 1890, loss = 2.06, batch loss = 2.00 (18.4 examples/sec; 0.434 sec/batch; 39h:49m:29s remains)
INFO - root - 2017-12-06 05:20:20.526086: step 1900, loss = 2.06, batch loss = 2.00 (17.9 examples/sec; 0.447 sec/batch; 41h:01m:21s remains)
2017-12-06 05:20:21.057273: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1788549 -4.1436977 -4.1591806 -4.1884646 -4.205863 -4.2203827 -4.2341161 -4.2599654 -4.2870679 -4.296864 -4.2734904 -4.2488046 -4.2339058 -4.234406 -4.2379937][-4.15112 -4.0986171 -4.0965605 -4.1234207 -4.1505189 -4.1735444 -4.1880245 -4.216351 -4.2472672 -4.2640982 -4.2509408 -4.2394485 -4.2309012 -4.2289114 -4.22394][-4.1370468 -4.0775123 -4.05113 -4.0563059 -4.0770841 -4.1074028 -4.1348124 -4.173399 -4.2135396 -4.236412 -4.2357221 -4.2373176 -4.2346087 -4.2323318 -4.2189789][-4.1384139 -4.0833349 -4.03909 -4.0153775 -4.016057 -4.0404458 -4.0768018 -4.1330614 -4.1910582 -4.224133 -4.2306366 -4.2352986 -4.2307343 -4.2227569 -4.203413][-4.14347 -4.0958672 -4.0471787 -4.0000572 -3.9781644 -3.982688 -4.016674 -4.0782471 -4.1483011 -4.193521 -4.2016587 -4.2037282 -4.1985126 -4.1896958 -4.1690855][-4.1442213 -4.1037617 -4.062922 -4.0073061 -3.9707589 -3.953505 -3.9717207 -4.019012 -4.0830503 -4.1309686 -4.1381745 -4.1473522 -4.1567121 -4.1651669 -4.1544929][-4.1404614 -4.1089039 -4.0792484 -4.0352159 -3.9967904 -3.9652431 -3.9567695 -3.9795272 -4.0314379 -4.0752282 -4.08371 -4.1008835 -4.1281471 -4.1527257 -4.1533966][-4.1262774 -4.1012731 -4.0797052 -4.0513668 -4.0212421 -3.9854343 -3.961148 -3.9616842 -3.9960454 -4.0340953 -4.0471578 -4.072742 -4.1089134 -4.1417189 -4.1517262][-4.1128907 -4.0918989 -4.0734916 -4.053566 -4.03393 -4.0018959 -3.9716721 -3.9609492 -3.981384 -4.0122976 -4.02917 -4.0625196 -4.1013374 -4.13431 -4.1486182][-4.1156421 -4.0992889 -4.0833483 -4.0684361 -4.0555916 -4.0338893 -4.0120158 -3.9994223 -4.0065789 -4.020587 -4.0264397 -4.0571833 -4.0994291 -4.1349611 -4.1496563][-4.1718974 -4.1574249 -4.1407428 -4.1270943 -4.1171765 -4.1053543 -4.0930824 -4.0833611 -4.0808945 -4.0775189 -4.0670791 -4.0853491 -4.1196594 -4.1523695 -4.1640897][-4.2498884 -4.2414637 -4.2292752 -4.2186108 -4.2111936 -4.2042828 -4.197196 -4.1917124 -4.1879325 -4.1797156 -4.1598053 -4.15906 -4.1752272 -4.1954255 -4.2010813][-4.3134632 -4.3107142 -4.3032236 -4.2973309 -4.2948575 -4.293056 -4.2909064 -4.2883282 -4.2853875 -4.2783351 -4.2602892 -4.249917 -4.2494726 -4.2539377 -4.2517061][-4.3482571 -4.3492785 -4.3453245 -4.3428555 -4.3429961 -4.3439226 -4.3453608 -4.3461132 -4.3454971 -4.3411131 -4.32773 -4.3151188 -4.3072052 -4.3039112 -4.2983403][-4.3489408 -4.3527408 -4.3513932 -4.3508954 -4.3520136 -4.3544269 -4.3579922 -4.3607206 -4.361073 -4.3586679 -4.3519125 -4.3447404 -4.3386931 -4.333425 -4.3267584]]...]
INFO - root - 2017-12-06 05:20:25.482540: step 1910, loss = 2.05, batch loss = 2.00 (19.0 examples/sec; 0.420 sec/batch; 38h:35m:52s remains)
INFO - root - 2017-12-06 05:20:29.993529: step 1920, loss = 2.07, batch loss = 2.01 (18.7 examples/sec; 0.428 sec/batch; 39h:19m:54s remains)
INFO - root - 2017-12-06 05:20:34.473672: step 1930, loss = 2.07, batch loss = 2.01 (17.6 examples/sec; 0.454 sec/batch; 41h:41m:11s remains)
INFO - root - 2017-12-06 05:20:38.993987: step 1940, loss = 2.06, batch loss = 2.00 (17.1 examples/sec; 0.467 sec/batch; 42h:50m:49s remains)
INFO - root - 2017-12-06 05:20:43.488195: step 1950, loss = 2.08, batch loss = 2.03 (17.5 examples/sec; 0.456 sec/batch; 41h:51m:52s remains)
INFO - root - 2017-12-06 05:20:47.991204: step 1960, loss = 2.06, batch loss = 2.01 (18.0 examples/sec; 0.445 sec/batch; 40h:49m:29s remains)
INFO - root - 2017-12-06 05:20:52.339287: step 1970, loss = 2.09, batch loss = 2.03 (19.2 examples/sec; 0.417 sec/batch; 38h:19m:43s remains)
INFO - root - 2017-12-06 05:20:56.750890: step 1980, loss = 2.06, batch loss = 2.00 (17.9 examples/sec; 0.448 sec/batch; 41h:05m:11s remains)
INFO - root - 2017-12-06 05:21:01.319322: step 1990, loss = 2.08, batch loss = 2.02 (17.9 examples/sec; 0.447 sec/batch; 41h:05m:00s remains)
INFO - root - 2017-12-06 05:21:05.593036: step 2000, loss = 2.06, batch loss = 2.00 (17.3 examples/sec; 0.463 sec/batch; 42h:27m:43s remains)
2017-12-06 05:21:06.415993: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2366838 -4.2345934 -4.2421 -4.245111 -4.2417345 -4.2455707 -4.2515774 -4.2454257 -4.2401414 -4.2384605 -4.2390337 -4.2443643 -4.2471805 -4.2468657 -4.2552862][-4.2339768 -4.2308116 -4.2385559 -4.2415252 -4.2327385 -4.2317419 -4.2395406 -4.2352424 -4.2261438 -4.2170048 -4.21045 -4.2099276 -4.2089934 -4.2090497 -4.22273][-4.2350307 -4.2332759 -4.2407894 -4.2402387 -4.2221203 -4.2131662 -4.2209382 -4.2248678 -4.21955 -4.2071943 -4.1931195 -4.181149 -4.1680465 -4.1609836 -4.1736512][-4.2219181 -4.2221327 -4.228209 -4.2245288 -4.2008581 -4.1844749 -4.19087 -4.2068977 -4.2133193 -4.203701 -4.1832237 -4.1620507 -4.1373334 -4.1226788 -4.133122][-4.190062 -4.187943 -4.191535 -4.1873684 -4.1613379 -4.1349783 -4.1340413 -4.1621361 -4.1879129 -4.19128 -4.1756897 -4.1552563 -4.1274328 -4.1088228 -4.1196389][-4.1729617 -4.16005 -4.1567879 -4.1520176 -4.1198668 -4.0709271 -4.0459628 -4.0812197 -4.14061 -4.1764917 -4.1784363 -4.1668568 -4.1415153 -4.1245012 -4.135478][-4.183929 -4.1542082 -4.1385312 -4.1284347 -4.0863204 -4.007638 -3.9449835 -3.9790993 -4.0737734 -4.1471233 -4.1741667 -4.1762786 -4.1584725 -4.1477842 -4.1606836][-4.1906228 -4.150703 -4.1252565 -4.1145573 -4.0740314 -3.9892066 -3.9104471 -3.9296198 -4.0303369 -4.1183281 -4.16057 -4.1713233 -4.1567621 -4.1487675 -4.1654959][-4.1991673 -4.1625071 -4.142118 -4.143662 -4.1247149 -4.0678544 -4.0039749 -3.9985256 -4.0565343 -4.1194654 -4.1576781 -4.167366 -4.1527944 -4.1434994 -4.1622152][-4.1948843 -4.1777077 -4.17384 -4.1887064 -4.1903524 -4.1600924 -4.1155272 -4.093493 -4.11077 -4.142487 -4.1697593 -4.1777887 -4.1665845 -4.1542068 -4.1693797][-4.17519 -4.1757655 -4.1863694 -4.2114196 -4.2258286 -4.2130651 -4.1845632 -4.1603422 -4.1590142 -4.17362 -4.1934519 -4.202322 -4.1968417 -4.18251 -4.1879573][-4.1631231 -4.1698909 -4.1849403 -4.2116771 -4.2318053 -4.230516 -4.2137842 -4.1960869 -4.19288 -4.19975 -4.214469 -4.2238326 -4.2221365 -4.2091594 -4.2087312][-4.1718755 -4.1752367 -4.1850228 -4.2056904 -4.2286696 -4.2372603 -4.2292337 -4.2181048 -4.2172885 -4.220633 -4.2293696 -4.2374763 -4.2385273 -4.2325487 -4.2311788][-4.1891003 -4.185719 -4.1865158 -4.1977334 -4.2174006 -4.23094 -4.2300849 -4.2238917 -4.2237864 -4.2260485 -4.230835 -4.2377977 -4.2412643 -4.2417707 -4.2440038][-4.2137713 -4.2073298 -4.2016187 -4.20443 -4.2183213 -4.2317138 -4.2350445 -4.2338815 -4.2344007 -4.2359776 -4.2397366 -4.2478147 -4.252737 -4.2536421 -4.255641]]...]
INFO - root - 2017-12-06 05:21:10.904398: step 2010, loss = 2.07, batch loss = 2.01 (17.1 examples/sec; 0.467 sec/batch; 42h:51m:05s remains)
INFO - root - 2017-12-06 05:21:15.354334: step 2020, loss = 2.08, batch loss = 2.02 (17.8 examples/sec; 0.449 sec/batch; 41h:11m:06s remains)
INFO - root - 2017-12-06 05:21:19.876830: step 2030, loss = 2.06, batch loss = 2.00 (18.4 examples/sec; 0.434 sec/batch; 39h:49m:21s remains)
INFO - root - 2017-12-06 05:21:24.430749: step 2040, loss = 2.06, batch loss = 2.00 (18.2 examples/sec; 0.439 sec/batch; 40h:15m:28s remains)
INFO - root - 2017-12-06 05:21:28.778841: step 2050, loss = 2.08, batch loss = 2.02 (18.9 examples/sec; 0.424 sec/batch; 38h:54m:27s remains)
INFO - root - 2017-12-06 05:21:33.326329: step 2060, loss = 2.05, batch loss = 1.99 (18.3 examples/sec; 0.437 sec/batch; 40h:04m:13s remains)
INFO - root - 2017-12-06 05:21:39.150832: step 2070, loss = 2.08, batch loss = 2.03 (18.3 examples/sec; 0.437 sec/batch; 40h:08m:31s remains)
INFO - root - 2017-12-06 05:21:45.019899: step 2080, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 61h:49m:04s remains)
INFO - root - 2017-12-06 05:21:51.347580: step 2090, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 59h:49m:45s remains)
INFO - root - 2017-12-06 05:21:57.882130: step 2100, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.664 sec/batch; 60h:54m:43s remains)
2017-12-06 05:22:02.244273: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.13846 -4.1351438 -4.1446447 -4.1703339 -4.1965146 -4.203023 -4.2096143 -4.2239451 -4.240108 -4.2609334 -4.2900748 -4.3114023 -4.3277035 -4.335638 -4.3271737][-4.1528244 -4.1527414 -4.1665807 -4.1878753 -4.206563 -4.2123609 -4.221209 -4.2341986 -4.2563457 -4.2851138 -4.3181634 -4.3368688 -4.34735 -4.3501115 -4.3380966][-4.1723561 -4.1619644 -4.1725006 -4.1906209 -4.2040687 -4.2068477 -4.2127581 -4.2250462 -4.2472348 -4.2805982 -4.3168082 -4.3357539 -4.3487539 -4.351624 -4.3378258][-4.1922774 -4.1708384 -4.1764174 -4.1967669 -4.2050281 -4.1968102 -4.1902266 -4.1935577 -4.2125311 -4.2532749 -4.2969332 -4.3188076 -4.3361607 -4.3403497 -4.3289642][-4.2062011 -4.1855149 -4.1871114 -4.201849 -4.1994042 -4.164144 -4.1296096 -4.1242013 -4.1479154 -4.2014875 -4.2583337 -4.292264 -4.3187346 -4.3284612 -4.3216729][-4.22327 -4.2109971 -4.2101636 -4.2130041 -4.1939893 -4.1304073 -4.0553255 -4.0245538 -4.0558944 -4.13112 -4.2077436 -4.2621832 -4.3002114 -4.3197155 -4.3201046][-4.2380419 -4.2401571 -4.2428012 -4.2360768 -4.2055078 -4.1292834 -4.0276957 -3.9627712 -3.9851251 -4.0759687 -4.1674919 -4.23766 -4.2869773 -4.3145156 -4.3197227][-4.2442851 -4.2596116 -4.2646475 -4.2544732 -4.2207603 -4.1497149 -4.052937 -3.9772794 -3.9832616 -4.0638571 -4.1507921 -4.2220235 -4.2785382 -4.3070717 -4.3095207][-4.2359591 -4.2596335 -4.2655835 -4.2541652 -4.2236362 -4.1646757 -4.083674 -4.0163569 -4.0168796 -4.0806222 -4.1533022 -4.2177458 -4.2737932 -4.3012056 -4.29828][-4.2211347 -4.2469373 -4.2535176 -4.2446032 -4.220284 -4.1736856 -4.1040678 -4.0473638 -4.0500364 -4.1017928 -4.1608119 -4.2170711 -4.2696857 -4.2961688 -4.2898903][-4.2238865 -4.24096 -4.2424407 -4.236517 -4.2195139 -4.1842527 -4.1264668 -4.0810213 -4.0893641 -4.1367888 -4.1850071 -4.2288933 -4.2662673 -4.2858467 -4.2830091][-4.2497878 -4.2583213 -4.2493496 -4.2390876 -4.2236347 -4.1979089 -4.1576147 -4.1270571 -4.138494 -4.1795936 -4.2157135 -4.2439485 -4.2596674 -4.2651749 -4.26574][-4.2807331 -4.2885013 -4.2767835 -4.2613974 -4.2432771 -4.2233839 -4.1967769 -4.1790872 -4.190959 -4.2262464 -4.2510195 -4.2594013 -4.2433791 -4.2220807 -4.2208943][-4.2971811 -4.3061533 -4.2990003 -4.2854562 -4.2717671 -4.259254 -4.2430439 -4.2334895 -4.2441826 -4.269722 -4.2808733 -4.2724686 -4.2311563 -4.1805997 -4.1668744][-4.2916212 -4.3001356 -4.2985778 -4.2953653 -4.2905674 -4.287044 -4.2814593 -4.2810779 -4.2915215 -4.3078165 -4.3081341 -4.285439 -4.2317152 -4.1677189 -4.14083]]...]
INFO - root - 2017-12-06 05:22:08.956330: step 2110, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 61h:36m:32s remains)
INFO - root - 2017-12-06 05:22:15.486887: step 2120, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 61h:02m:28s remains)
INFO - root - 2017-12-06 05:22:22.121613: step 2130, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.646 sec/batch; 59h:18m:54s remains)
INFO - root - 2017-12-06 05:22:28.793717: step 2140, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 62h:06m:59s remains)
INFO - root - 2017-12-06 05:22:35.568101: step 2150, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 60h:26m:30s remains)
INFO - root - 2017-12-06 05:22:42.204356: step 2160, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 58h:59m:36s remains)
INFO - root - 2017-12-06 05:22:48.695198: step 2170, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 62h:57m:10s remains)
INFO - root - 2017-12-06 05:22:55.202046: step 2180, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 63h:11m:29s remains)
INFO - root - 2017-12-06 05:23:01.754565: step 2190, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 58h:52m:27s remains)
INFO - root - 2017-12-06 05:23:08.445777: step 2200, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 63h:59m:12s remains)
2017-12-06 05:23:12.979245: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2549496 -4.2728744 -4.2892375 -4.2836022 -4.2564387 -4.2156892 -4.1591325 -4.1122 -4.1048174 -4.1252031 -4.1409726 -4.1671324 -4.2046175 -4.2385359 -4.258904][-4.2419305 -4.2630396 -4.2845531 -4.28275 -4.2524977 -4.2088947 -4.1523 -4.1086121 -4.1067429 -4.1325 -4.1565933 -4.188983 -4.2285504 -4.2671871 -4.2864051][-4.2325087 -4.2612629 -4.28536 -4.2808232 -4.2395926 -4.1851768 -4.1219749 -4.0818429 -4.0918465 -4.1288037 -4.1673231 -4.2070489 -4.2509427 -4.2949948 -4.3128519][-4.2333312 -4.2704439 -4.2952991 -4.2819967 -4.22802 -4.1548238 -4.0737147 -4.0290923 -4.050467 -4.1091428 -4.171669 -4.2252164 -4.2755485 -4.3193297 -4.3332171][-4.2399626 -4.2837925 -4.3067703 -4.2822647 -4.2136765 -4.119205 -4.0203371 -3.9676566 -4.0008855 -4.0856547 -4.1757154 -4.2474322 -4.3039322 -4.3399935 -4.3459392][-4.2516351 -4.2992077 -4.3174143 -4.2810874 -4.1980371 -4.0825481 -3.9663067 -3.9043496 -3.948051 -4.0624595 -4.17708 -4.2656379 -4.3238773 -4.3488336 -4.34504][-4.2681947 -4.3145862 -4.3265653 -4.2822595 -4.1907926 -4.0553083 -3.9142213 -3.8350291 -3.8834872 -4.0232468 -4.1602736 -4.2637639 -4.3258634 -4.3453817 -4.3347058][-4.2885785 -4.3255405 -4.3305078 -4.2839994 -4.1938648 -4.0499449 -3.8875985 -3.784935 -3.8242378 -3.9731321 -4.1251249 -4.2452989 -4.3163414 -4.3369203 -4.3201032][-4.3127947 -4.337234 -4.334177 -4.2877741 -4.2065072 -4.0714278 -3.907423 -3.7897372 -3.8113625 -3.9474146 -4.0976849 -4.2259579 -4.3045068 -4.3296046 -4.3113766][-4.333919 -4.3489928 -4.3408494 -4.2975321 -4.2284012 -4.1134782 -3.966028 -3.8501015 -3.8547659 -3.964643 -4.0979857 -4.2169771 -4.2919774 -4.3188176 -4.3049097][-4.3473248 -4.3585076 -4.3498092 -4.3151736 -4.2616358 -4.1740513 -4.0530367 -3.9498608 -3.9376249 -4.0124378 -4.1191497 -4.2193527 -4.2836471 -4.3102241 -4.3011956][-4.3492622 -4.3565741 -4.3491039 -4.3233285 -4.2856011 -4.2255297 -4.1362529 -4.0548925 -4.0335245 -4.076705 -4.1532049 -4.22989 -4.2804775 -4.3043485 -4.2976408][-4.3406296 -4.34291 -4.3359237 -4.3182025 -4.2940631 -4.2547116 -4.1932864 -4.13869 -4.1229734 -4.1473494 -4.1988683 -4.2514825 -4.2878137 -4.3057017 -4.2991571][-4.3285885 -4.3247113 -4.3161678 -4.3042278 -4.2883229 -4.2615938 -4.2213631 -4.1909738 -4.1874633 -4.207994 -4.2444868 -4.2774296 -4.2999234 -4.3102779 -4.30206][-4.3175626 -4.3079152 -4.297688 -4.2890029 -4.2787471 -4.2600656 -4.2337713 -4.2156477 -4.2158957 -4.2338252 -4.261013 -4.283 -4.29721 -4.3054714 -4.3010483]]...]
INFO - root - 2017-12-06 05:23:19.582965: step 2210, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 62h:23m:55s remains)
INFO - root - 2017-12-06 05:23:26.147148: step 2220, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 60h:06m:48s remains)
INFO - root - 2017-12-06 05:23:32.812076: step 2230, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 60h:06m:27s remains)
INFO - root - 2017-12-06 05:23:39.515189: step 2240, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 60h:23m:55s remains)
INFO - root - 2017-12-06 05:23:46.098371: step 2250, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 61h:09m:09s remains)
INFO - root - 2017-12-06 05:23:51.858750: step 2260, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 60h:38m:02s remains)
INFO - root - 2017-12-06 05:23:58.282981: step 2270, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 59h:11m:01s remains)
INFO - root - 2017-12-06 05:24:04.834607: step 2280, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 60h:47m:39s remains)
INFO - root - 2017-12-06 05:24:11.595758: step 2290, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 62h:44m:56s remains)
INFO - root - 2017-12-06 05:24:18.257564: step 2300, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 60h:32m:57s remains)
2017-12-06 05:24:18.932099: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2656689 -4.2611566 -4.2447081 -4.2177629 -4.1974678 -4.1811433 -4.1632738 -4.1534171 -4.1493154 -4.13458 -4.1006722 -4.0777473 -4.0858855 -4.1263781 -4.1733851][-4.2559729 -4.245656 -4.2265706 -4.196104 -4.170754 -4.149559 -4.1226563 -4.1007628 -4.0842876 -4.0734468 -4.0666003 -4.0739641 -4.1014748 -4.1489372 -4.1988907][-4.2423277 -4.2137656 -4.1858749 -4.1556363 -4.132483 -4.1108952 -4.0824895 -4.04914 -4.0170374 -4.0186677 -4.052053 -4.0936165 -4.1371489 -4.1901994 -4.238327][-4.2205243 -4.1796541 -4.1462264 -4.1246934 -4.1095543 -4.0923462 -4.0651293 -4.0227084 -3.9891541 -4.0064092 -4.0578084 -4.1115317 -4.1647692 -4.220665 -4.2644014][-4.2187457 -4.177577 -4.1404624 -4.1152086 -4.0956664 -4.078969 -4.0524769 -4.0220518 -4.015728 -4.0434694 -4.0900788 -4.1414809 -4.1923208 -4.2396216 -4.2746477][-4.2257276 -4.193995 -4.152863 -4.11142 -4.0736141 -4.0327473 -3.9909387 -3.9843435 -4.029295 -4.0827055 -4.1339474 -4.1848702 -4.2280388 -4.2646937 -4.2861753][-4.2166977 -4.195848 -4.1538143 -4.0930209 -4.0183368 -3.9339845 -3.8644757 -3.8945029 -4.00585 -4.0959177 -4.1607881 -4.2195115 -4.2608275 -4.2853031 -4.2955365][-4.1906042 -4.1734376 -4.1332541 -4.0608249 -3.9641831 -3.8539162 -3.7814405 -3.8531003 -4.0008674 -4.1048527 -4.1763482 -4.2350926 -4.2749791 -4.2935972 -4.2990284][-4.1433315 -4.129673 -4.1054716 -4.0573626 -3.9922819 -3.9272208 -3.9037209 -3.9710588 -4.0790405 -4.1560459 -4.2092361 -4.252574 -4.2808876 -4.2933598 -4.299994][-4.0940809 -4.1010633 -4.1050868 -4.0927854 -4.071856 -4.0555081 -4.0628195 -4.1080136 -4.1739454 -4.2221189 -4.255188 -4.2774458 -4.2880206 -4.2959137 -4.3045921][-4.09471 -4.1182203 -4.1350732 -4.1352911 -4.1320786 -4.1360989 -4.1541824 -4.1873274 -4.2303772 -4.2635756 -4.2890825 -4.2992506 -4.2997994 -4.3044953 -4.3139215][-4.1338906 -4.1534166 -4.163764 -4.1662579 -4.171484 -4.1807346 -4.1980834 -4.2233691 -4.2528458 -4.2774839 -4.3009176 -4.3105726 -4.3128419 -4.3178062 -4.3251791][-4.1730995 -4.1879034 -4.1974473 -4.1993184 -4.2018881 -4.206986 -4.2185044 -4.2381978 -4.2596755 -4.2796607 -4.3036637 -4.3167777 -4.3226662 -4.3285055 -4.3329272][-4.1961727 -4.2077451 -4.2138634 -4.2176046 -4.2211375 -4.2251081 -4.2320437 -4.2482338 -4.267405 -4.2869153 -4.3103943 -4.3252726 -4.3307843 -4.3338981 -4.3355346][-4.2118421 -4.2244525 -4.2265472 -4.2259312 -4.2328949 -4.2441926 -4.253973 -4.2713165 -4.2901917 -4.3065987 -4.3245282 -4.3340535 -4.3356967 -4.3366137 -4.336813]]...]
INFO - root - 2017-12-06 05:24:25.668945: step 2310, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 60h:38m:15s remains)
INFO - root - 2017-12-06 05:24:32.525522: step 2320, loss = 2.05, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 63h:14m:35s remains)
INFO - root - 2017-12-06 05:24:39.296521: step 2330, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 62h:33m:37s remains)
INFO - root - 2017-12-06 05:24:46.308497: step 2340, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 61h:17m:00s remains)
INFO - root - 2017-12-06 05:24:52.912767: step 2350, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 59h:37m:46s remains)
INFO - root - 2017-12-06 05:24:58.568661: step 2360, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 61h:28m:28s remains)
INFO - root - 2017-12-06 05:25:05.365250: step 2370, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 60h:59m:46s remains)
INFO - root - 2017-12-06 05:25:12.192471: step 2380, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 61h:13m:34s remains)
INFO - root - 2017-12-06 05:25:18.789855: step 2390, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 60h:59m:12s remains)
INFO - root - 2017-12-06 05:25:25.661024: step 2400, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 64h:58m:31s remains)
2017-12-06 05:25:29.147428: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0990357 -4.1262088 -4.163826 -4.1902089 -4.2025962 -4.2109933 -4.2148256 -4.2178855 -4.2213621 -4.2217369 -4.2210531 -4.2231493 -4.2249236 -4.2325668 -4.2453513][-4.1018991 -4.1235967 -4.1611891 -4.191936 -4.2129073 -4.2297931 -4.2345362 -4.23276 -4.2321544 -4.2274618 -4.2254848 -4.2330122 -4.243422 -4.2518377 -4.2601213][-4.1244774 -4.1426744 -4.1796803 -4.2098041 -4.230062 -4.2434874 -4.2430472 -4.2383618 -4.2354283 -4.228704 -4.2288322 -4.2375727 -4.2455711 -4.2487588 -4.2495041][-4.15182 -4.1717725 -4.2042651 -4.2246885 -4.2339568 -4.231636 -4.2186646 -4.2087789 -4.2074513 -4.2094927 -4.224813 -4.2385769 -4.2408085 -4.2350955 -4.2218838][-4.16098 -4.1838961 -4.2079544 -4.2150569 -4.2052264 -4.177875 -4.1465483 -4.1321268 -4.1434512 -4.1674032 -4.2057123 -4.23462 -4.2399311 -4.2284641 -4.201591][-4.1460934 -4.1689711 -4.1862607 -4.1783447 -4.1396027 -4.0787959 -4.0241547 -4.0136 -4.05367 -4.1133561 -4.1783481 -4.2243323 -4.2399807 -4.2307734 -4.1985621][-4.1288981 -4.1471958 -4.1556492 -4.1301317 -4.0595202 -3.9669063 -3.8932683 -3.8965611 -3.9693551 -4.0647693 -4.1534448 -4.2143145 -4.2422595 -4.23882 -4.2093782][-4.1329765 -4.1477365 -4.1514912 -4.1177912 -4.0354509 -3.9384937 -3.8708079 -3.8842583 -3.9580946 -4.0534544 -4.1430297 -4.2084746 -4.244288 -4.2472124 -4.2206044][-4.1568871 -4.1678352 -4.1722436 -4.1463985 -4.0840855 -4.0206175 -3.9823983 -3.9911642 -4.032218 -4.0911183 -4.1545143 -4.2074842 -4.2430825 -4.2516165 -4.2284403][-4.187345 -4.18896 -4.1904926 -4.1788974 -4.1487885 -4.1241984 -4.1135144 -4.114625 -4.1229529 -4.1425548 -4.1749673 -4.2100973 -4.2381697 -4.2494488 -4.2337861][-4.2096019 -4.2011328 -4.1994452 -4.1994767 -4.1955333 -4.1977572 -4.2009883 -4.1958404 -4.1835427 -4.1749139 -4.1830077 -4.2025514 -4.2214189 -4.2330937 -4.2297239][-4.2201843 -4.2110181 -4.2106256 -4.2175088 -4.2261462 -4.2377863 -4.2395887 -4.2252188 -4.199276 -4.1732016 -4.1650505 -4.1730003 -4.1848288 -4.1968956 -4.2074037][-4.2159376 -4.2131891 -4.21903 -4.2300472 -4.2402706 -4.2499442 -4.2441182 -4.2178631 -4.1819363 -4.1448154 -4.1218581 -4.1202383 -4.1281242 -4.1457033 -4.1703749][-4.1894193 -4.1963353 -4.2099214 -4.2221403 -4.22957 -4.2343068 -4.2227392 -4.1908 -4.1526303 -4.1148491 -4.0875673 -4.0818539 -4.0899525 -4.1145911 -4.1497655][-4.1506529 -4.1637897 -4.1821976 -4.1928735 -4.1934857 -4.1908565 -4.1760955 -4.1479559 -4.1230626 -4.1041889 -4.0890307 -4.0864725 -4.0970278 -4.128973 -4.1689792]]...]
INFO - root - 2017-12-06 05:25:35.859314: step 2410, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 62h:27m:28s remains)
INFO - root - 2017-12-06 05:25:42.549913: step 2420, loss = 2.08, batch loss = 2.03 (12.5 examples/sec; 0.638 sec/batch; 58h:30m:46s remains)
INFO - root - 2017-12-06 05:25:49.191549: step 2430, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.685 sec/batch; 62h:48m:08s remains)
INFO - root - 2017-12-06 05:25:55.891857: step 2440, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.662 sec/batch; 60h:42m:05s remains)
INFO - root - 2017-12-06 05:26:02.200074: step 2450, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.679 sec/batch; 62h:16m:21s remains)
INFO - root - 2017-12-06 05:26:08.831880: step 2460, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 60h:52m:50s remains)
INFO - root - 2017-12-06 05:26:15.379300: step 2470, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 62h:00m:31s remains)
INFO - root - 2017-12-06 05:26:22.020952: step 2480, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.653 sec/batch; 59h:50m:50s remains)
INFO - root - 2017-12-06 05:26:28.655364: step 2490, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.691 sec/batch; 63h:21m:38s remains)
INFO - root - 2017-12-06 05:26:35.473892: step 2500, loss = 2.04, batch loss = 1.99 (11.8 examples/sec; 0.675 sec/batch; 61h:53m:19s remains)
2017-12-06 05:26:36.151419: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3557587 -4.3530474 -4.3498483 -4.3513312 -4.354063 -4.3548217 -4.355844 -4.3574705 -4.3589416 -4.3600321 -4.3597894 -4.3603911 -4.3602095 -4.3601947 -4.3611403][-4.3445258 -4.3363318 -4.3292875 -4.331759 -4.3387413 -4.3435807 -4.3475342 -4.352519 -4.3568029 -4.3575954 -4.3558416 -4.3561254 -4.3567715 -4.3584914 -4.3604212][-4.3259478 -4.3103576 -4.2985897 -4.3008351 -4.3101692 -4.3146458 -4.3173943 -4.32485 -4.3313413 -4.3318024 -4.3298359 -4.3332973 -4.3406258 -4.3483167 -4.3540387][-4.3084054 -4.2877865 -4.2712984 -4.2707109 -4.276577 -4.2754846 -4.2717314 -4.2814727 -4.2921238 -4.2919006 -4.2873268 -4.29409 -4.3094792 -4.3274155 -4.3405075][-4.2892466 -4.2651095 -4.2414103 -4.2316794 -4.2288342 -4.219306 -4.2064939 -4.2159858 -4.2308064 -4.23128 -4.2251654 -4.2362604 -4.2632079 -4.2951264 -4.318541][-4.2626467 -4.2342815 -4.1994405 -4.173573 -4.155365 -4.1323223 -4.1066771 -4.1150217 -4.1362185 -4.1429682 -4.1424518 -4.1663704 -4.2107067 -4.2592697 -4.2940688][-4.2292619 -4.1907725 -4.1423426 -4.0995121 -4.0649533 -4.025454 -3.9857168 -3.9964604 -4.0313716 -4.0493455 -4.0628686 -4.1055312 -4.1674895 -4.2273531 -4.2716975][-4.19805 -4.1497674 -4.0903063 -4.040936 -4.0078168 -3.9684696 -3.924696 -3.935487 -3.9777424 -4.0010581 -4.02158 -4.0716639 -4.1382236 -4.2000928 -4.251102][-4.1686182 -4.1157479 -4.0537953 -4.0131769 -4.0090151 -4.0027914 -3.977407 -3.9793203 -4.0064306 -4.0206432 -4.0330758 -4.0715661 -4.1291356 -4.1884279 -4.2407212][-4.1571755 -4.1098118 -4.0570221 -4.0338936 -4.060143 -4.0860949 -4.0828886 -4.0762916 -4.0792961 -4.0785909 -4.08024 -4.102457 -4.1451654 -4.1976471 -4.2463741][-4.1623549 -4.1246061 -4.0841227 -4.0767145 -4.1185784 -4.1583738 -4.1687083 -4.1619091 -4.1531749 -4.1466365 -4.1437893 -4.1572294 -4.1871419 -4.2275953 -4.2665615][-4.1653013 -4.1338291 -4.1038804 -4.1103954 -4.1579423 -4.1991696 -4.2155743 -4.2153769 -4.2109885 -4.2081752 -4.2075825 -4.2194295 -4.2407346 -4.2673869 -4.29303][-4.1871181 -4.1635642 -4.1462212 -4.1602368 -4.2010555 -4.2332921 -4.2454224 -4.2468219 -4.2477074 -4.2489529 -4.249712 -4.2591982 -4.2764854 -4.295826 -4.312295][-4.2225623 -4.210639 -4.2034006 -4.2152491 -4.2408137 -4.2594948 -4.2663145 -4.2659149 -4.266747 -4.267921 -4.2680159 -4.2744145 -4.2887588 -4.3054605 -4.3202963][-4.2562127 -4.2511687 -4.2473645 -4.2528124 -4.2663813 -4.2782454 -4.2841496 -4.2847853 -4.2859764 -4.2869039 -4.2873878 -4.2923717 -4.3035593 -4.3179593 -4.3311009]]...]
INFO - root - 2017-12-06 05:26:42.924482: step 2510, loss = 2.03, batch loss = 1.98 (11.9 examples/sec; 0.674 sec/batch; 61h:49m:21s remains)
INFO - root - 2017-12-06 05:26:49.694404: step 2520, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.693 sec/batch; 63h:31m:39s remains)
INFO - root - 2017-12-06 05:26:56.416701: step 2530, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 60h:55m:35s remains)
INFO - root - 2017-12-06 05:27:03.176391: step 2540, loss = 2.04, batch loss = 1.99 (12.3 examples/sec; 0.649 sec/batch; 59h:30m:27s remains)
INFO - root - 2017-12-06 05:27:06.380468: step 2550, loss = 2.06, batch loss = 2.00 (35.2 examples/sec; 0.228 sec/batch; 20h:51m:29s remains)
INFO - root - 2017-12-06 05:27:12.879614: step 2560, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 61h:45m:18s remains)
INFO - root - 2017-12-06 05:27:19.646879: step 2570, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 65h:04m:40s remains)
INFO - root - 2017-12-06 05:27:26.307915: step 2580, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 59h:17m:45s remains)
INFO - root - 2017-12-06 05:27:32.979729: step 2590, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 61h:38m:44s remains)
INFO - root - 2017-12-06 05:27:39.801905: step 2600, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.661 sec/batch; 60h:36m:20s remains)
2017-12-06 05:27:40.532052: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3294945 -4.3340564 -4.3402677 -4.3442311 -4.3451538 -4.3454547 -4.3448324 -4.3433027 -4.3414836 -4.3396339 -4.3376908 -4.3353839 -4.3320107 -4.329854 -4.3291712][-4.2749124 -4.2817736 -4.2963371 -4.3089614 -4.3164935 -4.32007 -4.3204551 -4.3190317 -4.3180161 -4.3172803 -4.3164344 -4.3134422 -4.3061776 -4.29755 -4.2918468][-4.2001605 -4.2065015 -4.2267795 -4.2473197 -4.2634678 -4.2721248 -4.2743053 -4.2744884 -4.2766738 -4.2803683 -4.2845178 -4.28427 -4.2754374 -4.2610817 -4.2510815][-4.1150064 -4.1183372 -4.1429005 -4.1725516 -4.1988411 -4.2120438 -4.2148294 -4.2177114 -4.2259774 -4.2359643 -4.2456269 -4.2493634 -4.2423058 -4.2268806 -4.2152853][-4.0488615 -4.0407872 -4.0614362 -4.0957747 -4.1279035 -4.1406169 -4.1403952 -4.1453495 -4.1630564 -4.1837959 -4.1995864 -4.206419 -4.2036071 -4.1922255 -4.1835518][-4.0288506 -4.0038128 -4.0095258 -4.0346971 -4.058064 -4.0572619 -4.0449142 -4.0498996 -4.0805893 -4.1176667 -4.1438565 -4.1572647 -4.1613951 -4.1588035 -4.1594081][-4.072525 -4.0399575 -4.0307016 -4.038394 -4.0429573 -4.020225 -3.9859896 -3.9792404 -4.0126266 -4.0587029 -4.0924544 -4.112565 -4.1232643 -4.1296315 -4.1405144][-4.1494226 -4.1280322 -4.1217675 -4.1243353 -4.11965 -4.0867872 -4.04169 -4.0209742 -4.0412431 -4.0764813 -4.1016831 -4.1151118 -4.1206613 -4.1249218 -4.1376791][-4.2095089 -4.2039027 -4.207727 -4.2151093 -4.2152004 -4.1913962 -4.1564679 -4.1371188 -4.1469269 -4.1676478 -4.1797581 -4.1813526 -4.174109 -4.1663151 -4.1699829][-4.2356591 -4.2398949 -4.2523322 -4.267725 -4.2780519 -4.2695589 -4.2511554 -4.2399945 -4.246315 -4.2582235 -4.2629623 -4.2580538 -4.24089 -4.2213316 -4.2142563][-4.2223067 -4.22838 -4.2449327 -4.2665248 -4.2857871 -4.2917027 -4.2878146 -4.2845941 -4.2893248 -4.296711 -4.2998271 -4.2933879 -4.2691545 -4.2392025 -4.2237835][-4.1725335 -4.176249 -4.1964121 -4.2265983 -4.2547531 -4.2706304 -4.2752185 -4.2770004 -4.2803068 -4.2841616 -4.2861114 -4.2773385 -4.2451248 -4.2052917 -4.1866059][-4.1006227 -4.09237 -4.1089864 -4.1408825 -4.1736221 -4.1951141 -4.2048483 -4.2116313 -4.2185397 -4.2250648 -4.2292681 -4.217289 -4.1761322 -4.1268477 -4.1089368][-4.06028 -4.0413785 -4.051024 -4.0770454 -4.1061397 -4.1250916 -4.1313934 -4.1363783 -4.1430926 -4.1511059 -4.1572433 -4.1457257 -4.1044226 -4.0555334 -4.0428967][-4.104187 -4.0850692 -4.0902562 -4.1093593 -4.1313396 -4.1439228 -4.1449089 -4.1454844 -4.148396 -4.1533465 -4.1573238 -4.1471639 -4.11553 -4.0806456 -4.0754251]]...]
INFO - root - 2017-12-06 05:27:47.277436: step 2610, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.717 sec/batch; 65h:41m:42s remains)
INFO - root - 2017-12-06 05:27:54.030287: step 2620, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.700 sec/batch; 64h:10m:58s remains)
INFO - root - 2017-12-06 05:28:00.628864: step 2630, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.665 sec/batch; 60h:57m:21s remains)
INFO - root - 2017-12-06 05:28:07.188710: step 2640, loss = 2.08, batch loss = 2.03 (11.9 examples/sec; 0.670 sec/batch; 61h:22m:46s remains)
INFO - root - 2017-12-06 05:28:12.270185: step 2650, loss = 2.07, batch loss = 2.01 (18.6 examples/sec; 0.429 sec/batch; 39h:20m:33s remains)
INFO - root - 2017-12-06 05:28:18.890635: step 2660, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 59h:01m:58s remains)
INFO - root - 2017-12-06 05:28:25.581688: step 2670, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.670 sec/batch; 61h:22m:36s remains)
INFO - root - 2017-12-06 05:28:32.269562: step 2680, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 63h:49m:46s remains)
INFO - root - 2017-12-06 05:28:38.913066: step 2690, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 60h:13m:14s remains)
INFO - root - 2017-12-06 05:28:45.671994: step 2700, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.730 sec/batch; 66h:54m:15s remains)
2017-12-06 05:28:46.270149: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2083368 -4.2290406 -4.2617159 -4.29079 -4.3048282 -4.3066587 -4.3069849 -4.3089533 -4.3091083 -4.3061047 -4.3022261 -4.2980247 -4.296298 -4.2953043 -4.2951746][-4.2234993 -4.2405534 -4.26592 -4.2880454 -4.2984042 -4.2974133 -4.2972493 -4.2979708 -4.2979903 -4.2958932 -4.2934847 -4.290977 -4.289712 -4.2885475 -4.2879987][-4.2653871 -4.2736244 -4.28394 -4.2910008 -4.2908597 -4.2818041 -4.2756338 -4.2747788 -4.2774434 -4.2799315 -4.2831812 -4.2857852 -4.2889147 -4.2908387 -4.2903881][-4.2833548 -4.285058 -4.2830677 -4.278471 -4.2698035 -4.2549415 -4.2447758 -4.24571 -4.2542548 -4.2633381 -4.2726455 -4.280221 -4.2876925 -4.2921624 -4.289824][-4.2621307 -4.2602692 -4.2538023 -4.2426405 -4.2305856 -4.2156506 -4.20499 -4.2098408 -4.2248807 -4.2398095 -4.2523713 -4.2608819 -4.270803 -4.2755914 -4.2681279][-4.216754 -4.2143116 -4.2052937 -4.19021 -4.1762915 -4.1627 -4.1514463 -4.1570406 -4.1786923 -4.203793 -4.2267914 -4.242733 -4.258729 -4.2669497 -4.2559052][-4.1657553 -4.1609592 -4.143631 -4.1242247 -4.1082458 -4.0933275 -4.0794344 -4.0820913 -4.1092458 -4.1501226 -4.1872525 -4.2125854 -4.2322917 -4.2410374 -4.2281389][-4.1102214 -4.0979648 -4.0700226 -4.0453963 -4.028481 -4.011404 -3.990422 -3.9831283 -4.0080438 -4.0613689 -4.1129441 -4.1468267 -4.1711159 -4.1832848 -4.1722488][-4.0716705 -4.0545073 -4.0274911 -4.0100594 -4.0000806 -3.9830015 -3.9554896 -3.933162 -3.9441917 -3.998183 -4.0569296 -4.0963264 -4.12259 -4.133925 -4.1248178][-4.1128559 -4.0976319 -4.0830717 -4.0817089 -4.0841045 -4.078238 -4.0620127 -4.0457892 -4.051302 -4.0876403 -4.1284842 -4.1540241 -4.1653857 -4.1664367 -4.1532288][-4.1869664 -4.1736522 -4.1626892 -4.1641712 -4.1691408 -4.1692185 -4.1651511 -4.1609321 -4.1659851 -4.1878281 -4.2115469 -4.2245007 -4.2246208 -4.2202096 -4.2088804][-4.2396216 -4.2293029 -4.21657 -4.2101207 -4.2074866 -4.2077112 -4.2110634 -4.2123852 -4.2137647 -4.2266388 -4.2387276 -4.2383013 -4.2306671 -4.2268939 -4.2240047][-4.2434745 -4.2367911 -4.2203689 -4.2031507 -4.1914988 -4.1915264 -4.1988835 -4.1999969 -4.1967874 -4.2037344 -4.2094793 -4.2021637 -4.1934543 -4.1955767 -4.201829][-4.2265759 -4.2164044 -4.19412 -4.1683135 -4.1520443 -4.1541619 -4.1673956 -4.1700182 -4.1641107 -4.1671734 -4.1705322 -4.1667681 -4.1660366 -4.1772013 -4.1950545][-4.2165256 -4.2017727 -4.1760788 -4.1505036 -4.1386671 -4.1447582 -4.1608415 -4.1645193 -4.1611972 -4.1660657 -4.1725044 -4.1756358 -4.1812296 -4.1971149 -4.216949]]...]
INFO - root - 2017-12-06 05:28:52.927032: step 2710, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.658 sec/batch; 60h:17m:45s remains)
INFO - root - 2017-12-06 05:28:59.592587: step 2720, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 63h:13m:35s remains)
INFO - root - 2017-12-06 05:29:06.254651: step 2730, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.650 sec/batch; 59h:33m:43s remains)
INFO - root - 2017-12-06 05:29:12.873245: step 2740, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 63h:38m:01s remains)
INFO - root - 2017-12-06 05:29:19.238292: step 2750, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 62h:52m:39s remains)
INFO - root - 2017-12-06 05:29:26.064195: step 2760, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 62h:06m:25s remains)
INFO - root - 2017-12-06 05:29:32.727871: step 2770, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 62h:52m:19s remains)
INFO - root - 2017-12-06 05:29:39.450666: step 2780, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.701 sec/batch; 64h:14m:30s remains)
INFO - root - 2017-12-06 05:29:46.188178: step 2790, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 60h:36m:04s remains)
INFO - root - 2017-12-06 05:29:52.818637: step 2800, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 62h:45m:40s remains)
2017-12-06 05:29:53.572055: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2448854 -4.2361426 -4.226284 -4.2173781 -4.2030048 -4.1974092 -4.1849761 -4.1621284 -4.1534967 -4.1580067 -4.1712108 -4.186296 -4.1917014 -4.1931605 -4.2098708][-4.2497511 -4.2358646 -4.2208018 -4.2134409 -4.2028575 -4.1941619 -4.1798139 -4.1591196 -4.1527371 -4.1641498 -4.180521 -4.1930513 -4.1938334 -4.1969962 -4.2157459][-4.2259512 -4.2074866 -4.1944437 -4.194294 -4.1921687 -4.1829367 -4.1691914 -4.1541681 -4.1557765 -4.1725869 -4.1872287 -4.1947556 -4.1944027 -4.199688 -4.2185922][-4.1878605 -4.1706438 -4.1659441 -4.17351 -4.1789551 -4.1710963 -4.1574678 -4.1493287 -4.1628528 -4.1891165 -4.2071142 -4.2115264 -4.2077832 -4.2081704 -4.2179775][-4.1563091 -4.145741 -4.1459737 -4.1537809 -4.1620793 -4.1558547 -4.1376948 -4.127923 -4.1425176 -4.1759787 -4.20478 -4.2160044 -4.217607 -4.2193031 -4.2207246][-4.1304688 -4.1308222 -4.1327009 -4.1334295 -4.1393476 -4.1312213 -4.1057692 -4.0864372 -4.0955057 -4.1357 -4.1731024 -4.1900167 -4.1981692 -4.206749 -4.210691][-4.097887 -4.10131 -4.1064668 -4.1060519 -4.1090703 -4.0975885 -4.05682 -4.0115962 -4.0095811 -4.0579762 -4.1054964 -4.1322689 -4.1537786 -4.1793962 -4.1948361][-4.0805054 -4.0829163 -4.0936475 -4.099308 -4.1023097 -4.0858955 -4.0339208 -3.9641781 -3.9452119 -3.9935381 -4.0472589 -4.0824904 -4.1135731 -4.1507058 -4.1776838][-4.0856791 -4.0935812 -4.11052 -4.124876 -4.1346731 -4.1283669 -4.0949392 -4.0408373 -4.0201983 -4.0505743 -4.0887175 -4.1173077 -4.1415086 -4.16974 -4.1936603][-4.1154451 -4.1289263 -4.149322 -4.1706295 -4.1835656 -4.1858048 -4.1757212 -4.1503344 -4.1367249 -4.14581 -4.16595 -4.1826172 -4.1957169 -4.2138491 -4.2288136][-4.14885 -4.1596785 -4.1795874 -4.200695 -4.2087145 -4.2029061 -4.1955047 -4.1844172 -4.1796241 -4.1882381 -4.2063036 -4.2214217 -4.2339435 -4.2533188 -4.2695394][-4.1636 -4.1610017 -4.172873 -4.1936016 -4.2011418 -4.1908822 -4.1789684 -4.164885 -4.15915 -4.1702275 -4.1934032 -4.2127862 -4.2316904 -4.2560315 -4.274425][-4.1512766 -4.1328907 -4.1368437 -4.1646452 -4.1826277 -4.1749477 -4.1629305 -4.1453743 -4.1356745 -4.144362 -4.1675081 -4.1868753 -4.2042193 -4.2225504 -4.2343378][-4.1374454 -4.1169357 -4.1238384 -4.1601663 -4.1846452 -4.1789742 -4.1669426 -4.1482544 -4.1343288 -4.137217 -4.1547127 -4.1708326 -4.1837726 -4.194778 -4.206399][-4.1285763 -4.1218791 -4.1348219 -4.1683016 -4.1915379 -4.1882005 -4.1780906 -4.1598368 -4.1386609 -4.1317487 -4.1419997 -4.15367 -4.1634288 -4.1694088 -4.1809916]]...]
INFO - root - 2017-12-06 05:30:00.256234: step 2810, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 59h:15m:36s remains)
INFO - root - 2017-12-06 05:30:06.907335: step 2820, loss = 2.04, batch loss = 1.99 (12.3 examples/sec; 0.652 sec/batch; 59h:44m:43s remains)
INFO - root - 2017-12-06 05:30:13.704201: step 2830, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.700 sec/batch; 64h:05m:49s remains)
INFO - root - 2017-12-06 05:30:20.357836: step 2840, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.641 sec/batch; 58h:44m:21s remains)
INFO - root - 2017-12-06 05:30:26.759652: step 2850, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.653 sec/batch; 59h:48m:12s remains)
INFO - root - 2017-12-06 05:30:33.417837: step 2860, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 60h:14m:31s remains)
INFO - root - 2017-12-06 05:30:40.016430: step 2870, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 59h:51m:05s remains)
INFO - root - 2017-12-06 05:30:46.765654: step 2880, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 61h:56m:06s remains)
INFO - root - 2017-12-06 05:30:53.480604: step 2890, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.648 sec/batch; 59h:19m:26s remains)
INFO - root - 2017-12-06 05:31:00.087667: step 2900, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.641 sec/batch; 58h:40m:23s remains)
2017-12-06 05:31:02.678086: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2946706 -4.3055377 -4.3113375 -4.3128514 -4.3057895 -4.2844505 -4.2495537 -4.2218776 -4.205338 -4.207787 -4.2308636 -4.2602015 -4.2782259 -4.2766857 -4.2620349][-4.2941685 -4.2986922 -4.2948332 -4.289546 -4.2848005 -4.2723989 -4.2473674 -4.2250366 -4.2110581 -4.2165179 -4.2453771 -4.2813506 -4.3043561 -4.3061557 -4.290915][-4.2888236 -4.28715 -4.271935 -4.2605124 -4.2635288 -4.2692232 -4.2594156 -4.2428575 -4.2249746 -4.2256942 -4.2539062 -4.2903156 -4.3134913 -4.3180318 -4.3045087][-4.2878036 -4.2803893 -4.2580218 -4.2446165 -4.2558126 -4.276567 -4.2813334 -4.2715969 -4.2500014 -4.2430124 -4.2634587 -4.2912793 -4.3070168 -4.3086782 -4.2960782][-4.2911625 -4.2819381 -4.2592335 -4.2459426 -4.2590532 -4.2826476 -4.2932472 -4.2889829 -4.2683992 -4.2560678 -4.2661967 -4.2824917 -4.2883158 -4.2827263 -4.2692518][-4.2913313 -4.2871404 -4.270617 -4.2608852 -4.2712951 -4.2899079 -4.3001833 -4.2988772 -4.2817354 -4.2663412 -4.2670527 -4.2716494 -4.2677464 -4.2543063 -4.2378945][-4.2888064 -4.2926049 -4.2847838 -4.2814188 -4.2896738 -4.3007188 -4.3080754 -4.3086724 -4.29603 -4.2823782 -4.2791462 -4.2774782 -4.26705 -4.2466431 -4.2242203][-4.2797112 -4.2921314 -4.2933288 -4.2938585 -4.2968831 -4.29979 -4.3042545 -4.3059683 -4.2995729 -4.293014 -4.29413 -4.2955446 -4.2859015 -4.2643209 -4.239459][-4.2626262 -4.2872038 -4.2980332 -4.297699 -4.290257 -4.2831788 -4.2837439 -4.2859888 -4.283443 -4.2816916 -4.2875385 -4.293952 -4.2883978 -4.2734027 -4.2549472][-4.2391758 -4.2783446 -4.2984772 -4.2951384 -4.2758212 -4.2591214 -4.2572231 -4.259872 -4.258749 -4.2587237 -4.2643204 -4.2703247 -4.2679977 -4.2623348 -4.2566829][-4.2162313 -4.2675681 -4.29445 -4.28796 -4.2597961 -4.2368217 -4.2357082 -4.238966 -4.2355976 -4.2311049 -4.2308216 -4.2320352 -4.2311444 -4.2322745 -4.2379379][-4.2166157 -4.2720242 -4.2984934 -4.290082 -4.2584577 -4.2323012 -4.2317324 -4.2348738 -4.2272654 -4.2153916 -4.2043037 -4.1960211 -4.189415 -4.1918697 -4.2052546][-4.2426748 -4.2885447 -4.305696 -4.2946949 -4.2656016 -4.2420683 -4.2427135 -4.244874 -4.236568 -4.2209482 -4.200635 -4.1801138 -4.1619749 -4.1617537 -4.17741][-4.27098 -4.2981472 -4.3027148 -4.2897267 -4.2692747 -4.2549529 -4.2569985 -4.2596669 -4.2544646 -4.242722 -4.2226377 -4.1946144 -4.1689916 -4.1649551 -4.1756482][-4.2791634 -4.2895045 -4.2864022 -4.2782393 -4.2708988 -4.2656713 -4.2670808 -4.2699394 -4.2677279 -4.2612023 -4.2454357 -4.2175245 -4.1934195 -4.1897936 -4.1931157]]...]
INFO - root - 2017-12-06 05:31:09.316608: step 2910, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 60h:32m:57s remains)
INFO - root - 2017-12-06 05:31:16.085432: step 2920, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 60h:25m:02s remains)
INFO - root - 2017-12-06 05:31:22.671553: step 2930, loss = 2.11, batch loss = 2.05 (11.9 examples/sec; 0.671 sec/batch; 61h:27m:02s remains)
INFO - root - 2017-12-06 05:31:26.589500: step 2940, loss = 2.06, batch loss = 2.00 (35.8 examples/sec; 0.224 sec/batch; 20h:27m:36s remains)
INFO - root - 2017-12-06 05:31:28.837745: step 2950, loss = 2.10, batch loss = 2.04 (36.4 examples/sec; 0.220 sec/batch; 20h:08m:19s remains)
INFO - root - 2017-12-06 05:31:32.958341: step 2960, loss = 2.10, batch loss = 2.04 (12.3 examples/sec; 0.649 sec/batch; 59h:26m:14s remains)
INFO - root - 2017-12-06 05:31:39.593991: step 2970, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 60h:20m:47s remains)
INFO - root - 2017-12-06 05:31:46.182179: step 2980, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 60h:26m:04s remains)
INFO - root - 2017-12-06 05:31:52.887661: step 2990, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 64h:17m:17s remains)
INFO - root - 2017-12-06 05:31:59.438975: step 3000, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 62h:04m:01s remains)
2017-12-06 05:32:00.049164: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1654472 -4.17467 -4.20777 -4.2466574 -4.2725816 -4.2680154 -4.2296939 -4.18817 -4.1711497 -4.1752768 -4.18683 -4.1992621 -4.2209554 -4.2279038 -4.2168651][-4.1620026 -4.1737661 -4.2093649 -4.2481265 -4.2726951 -4.2631326 -4.2161241 -4.1674647 -4.1486492 -4.1569204 -4.1729727 -4.1867847 -4.2102766 -4.2175593 -4.2082977][-4.147181 -4.1591663 -4.1913137 -4.230906 -4.2592058 -4.2522168 -4.2073393 -4.1599388 -4.1436663 -4.1580586 -4.1764374 -4.1862907 -4.2051835 -4.2107353 -4.2024617][-4.1289134 -4.1421161 -4.1673594 -4.2058926 -4.2390642 -4.2395105 -4.2073426 -4.1767254 -4.1752772 -4.1991549 -4.218493 -4.2257967 -4.2392349 -4.2372489 -4.2278013][-4.120791 -4.1322994 -4.1421132 -4.1661229 -4.193397 -4.1966538 -4.1782169 -4.1641741 -4.177907 -4.2165585 -4.24681 -4.2598453 -4.2700438 -4.2663684 -4.261797][-4.1115246 -4.1187544 -4.1095514 -4.1133609 -4.1327648 -4.1374197 -4.1172028 -4.0984507 -4.1120467 -4.1653008 -4.2115307 -4.2344089 -4.2460632 -4.2472749 -4.2553263][-4.0996227 -4.099421 -4.0667777 -4.0387688 -4.042459 -4.0434823 -4.0073051 -3.9590054 -3.9631369 -4.044384 -4.1223426 -4.1625023 -4.1791048 -4.18703 -4.2089763][-4.1145248 -4.0977955 -4.0368996 -3.9713726 -3.9479396 -3.9342415 -3.8731189 -3.7757988 -3.7649224 -3.8948066 -4.0194526 -4.078229 -4.0935154 -4.1053634 -4.1340575][-4.165669 -4.1444325 -4.0797586 -4.0030532 -3.962383 -3.9402597 -3.87496 -3.7662613 -3.7501006 -3.8878744 -4.0195265 -4.0710077 -4.0704627 -4.0702739 -4.0898218][-4.2290792 -4.2134275 -4.1679206 -4.1128869 -4.0795312 -4.063735 -4.0229888 -3.9559615 -3.9477415 -4.0354295 -4.1226406 -4.1490984 -4.1289606 -4.1115913 -4.1163239][-4.2762766 -4.26734 -4.2425032 -4.2102003 -4.1888242 -4.1768374 -4.1533618 -4.1181922 -4.1095724 -4.1512461 -4.1930385 -4.1958661 -4.1685939 -4.1461277 -4.1380119][-4.295053 -4.2891932 -4.2765684 -4.2616062 -4.2500839 -4.2428427 -4.2291584 -4.2067227 -4.1933374 -4.2035055 -4.2093182 -4.1886435 -4.1527534 -4.1241908 -4.0974913][-4.3059359 -4.2997561 -4.2912278 -4.2835803 -4.2785411 -4.2729979 -4.2612062 -4.2403121 -4.2242055 -4.2199 -4.2047153 -4.1629615 -4.11477 -4.0793858 -4.0406389][-4.316824 -4.3105164 -4.3030014 -4.2973228 -4.2934508 -4.2901912 -4.2802768 -4.2624526 -4.2450223 -4.2322817 -4.2053628 -4.15451 -4.1032214 -4.066916 -4.0293131][-4.3269687 -4.3215747 -4.3154111 -4.3107209 -4.3076286 -4.3056 -4.2993212 -4.2869682 -4.2708855 -4.2530861 -4.2262139 -4.1856441 -4.1445684 -4.1125789 -4.0811024]]...]
INFO - root - 2017-12-06 05:32:06.795958: step 3010, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 61h:21m:56s remains)
INFO - root - 2017-12-06 05:32:13.454680: step 3020, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 60h:35m:57s remains)
INFO - root - 2017-12-06 05:32:20.123138: step 3030, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 62h:54m:19s remains)
INFO - root - 2017-12-06 05:32:26.813145: step 3040, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 62h:32m:01s remains)
INFO - root - 2017-12-06 05:32:33.481942: step 3050, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.663 sec/batch; 60h:41m:39s remains)
INFO - root - 2017-12-06 05:32:39.721311: step 3060, loss = 2.08, batch loss = 2.03 (12.0 examples/sec; 0.665 sec/batch; 60h:53m:07s remains)
INFO - root - 2017-12-06 05:32:46.479816: step 3070, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 61h:40m:03s remains)
INFO - root - 2017-12-06 05:32:53.259713: step 3080, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 58h:41m:42s remains)
INFO - root - 2017-12-06 05:32:59.960484: step 3090, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.690 sec/batch; 63h:07m:23s remains)
INFO - root - 2017-12-06 05:33:06.621277: step 3100, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 60h:45m:13s remains)
2017-12-06 05:33:07.276598: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3051786 -4.3018556 -4.3019261 -4.2996035 -4.297502 -4.2967882 -4.2980614 -4.2976537 -4.2948103 -4.2937303 -4.2922835 -4.290637 -4.29057 -4.2922974 -4.2956533][-4.3337941 -4.330852 -4.3302484 -4.326427 -4.3218093 -4.3172836 -4.3143597 -4.3075156 -4.2982368 -4.2925363 -4.286685 -4.28218 -4.2761564 -4.2731123 -4.2745652][-4.350842 -4.3487043 -4.3452549 -4.336822 -4.3277388 -4.3161721 -4.3065696 -4.2942448 -4.2819362 -4.2724285 -4.2620091 -4.2561607 -4.2424097 -4.2323928 -4.230803][-4.3511748 -4.34869 -4.3395286 -4.3228121 -4.3059916 -4.2848954 -4.2686296 -4.2568555 -4.2516155 -4.2435746 -4.2281785 -4.2192044 -4.19881 -4.1797085 -4.1702113][-4.3267426 -4.3230319 -4.308342 -4.2847877 -4.2594762 -4.225842 -4.1994314 -4.1925116 -4.1996636 -4.1929789 -4.16978 -4.1567626 -4.1324587 -4.1058741 -4.0877514][-4.287436 -4.2812061 -4.2585263 -4.2247887 -4.1851459 -4.1295795 -4.083993 -4.0867414 -4.113771 -4.1147122 -4.0894823 -4.0796714 -4.068882 -4.0504589 -4.0282626][-4.2533255 -4.2371812 -4.1960263 -4.1429663 -4.077466 -3.9883652 -3.9169729 -3.9402688 -3.9996378 -4.0179653 -4.0038409 -4.0122972 -4.0344253 -4.0446677 -4.0348287][-4.2404356 -4.20938 -4.1454339 -4.0685329 -3.9754515 -3.8529723 -3.754781 -3.8071094 -3.9086041 -3.9542372 -3.9669428 -4.0053592 -4.0622373 -4.1046176 -4.1175151][-4.2555146 -4.2155986 -4.1404648 -4.0496459 -3.948863 -3.8268061 -3.7324641 -3.7959824 -3.90815 -3.9668822 -3.9972558 -4.0522428 -4.1250834 -4.1831145 -4.2113814][-4.278748 -4.2456069 -4.1796546 -4.0986671 -4.0158091 -3.928776 -3.8695683 -3.9132955 -3.9914086 -4.0370197 -4.0653772 -4.1157136 -4.1829753 -4.237782 -4.269938][-4.3092275 -4.2892127 -4.2428322 -4.1848826 -4.1261086 -4.071847 -4.0403271 -4.0617638 -4.1042075 -4.1322746 -4.1520548 -4.1865106 -4.2346468 -4.2778397 -4.3074846][-4.3307681 -4.3225741 -4.2951522 -4.2592039 -4.2227855 -4.1926675 -4.1776295 -4.187078 -4.2093139 -4.2244673 -4.2342124 -4.2512259 -4.27661 -4.3038673 -4.326766][-4.3383913 -4.3379059 -4.3246622 -4.3055911 -4.2883787 -4.2753105 -4.2704115 -4.2769728 -4.2891536 -4.2967477 -4.2979922 -4.3002744 -4.3081331 -4.3229408 -4.3418097][-4.3348041 -4.3383694 -4.3336563 -4.3258924 -4.32032 -4.3180509 -4.3197975 -4.3268814 -4.3347831 -4.3386488 -4.335392 -4.3302927 -4.3297524 -4.3383985 -4.3555732][-4.3262782 -4.3321238 -4.3335195 -4.3323627 -4.3320203 -4.3355875 -4.3407331 -4.3475661 -4.3529758 -4.3548245 -4.3512878 -4.3456774 -4.3429327 -4.3477054 -4.3615689]]...]
INFO - root - 2017-12-06 05:33:13.920167: step 3110, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.664 sec/batch; 60h:42m:38s remains)
INFO - root - 2017-12-06 05:33:20.699636: step 3120, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 61h:58m:42s remains)
INFO - root - 2017-12-06 05:33:27.364211: step 3130, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 61h:08m:21s remains)
INFO - root - 2017-12-06 05:33:33.914279: step 3140, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 61h:06m:40s remains)
INFO - root - 2017-12-06 05:33:39.610928: step 3150, loss = 2.03, batch loss = 1.97 (34.9 examples/sec; 0.229 sec/batch; 20h:59m:24s remains)
INFO - root - 2017-12-06 05:33:44.613837: step 3160, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.677 sec/batch; 61h:54m:32s remains)
INFO - root - 2017-12-06 05:33:51.315083: step 3170, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 60h:02m:09s remains)
INFO - root - 2017-12-06 05:33:57.892467: step 3180, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.642 sec/batch; 58h:45m:34s remains)
INFO - root - 2017-12-06 05:34:04.517414: step 3190, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 60h:03m:53s remains)
INFO - root - 2017-12-06 05:34:11.334975: step 3200, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 60h:47m:31s remains)
2017-12-06 05:34:12.082019: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3079138 -4.2960386 -4.2908616 -4.2940531 -4.3009572 -4.300396 -4.2957349 -4.2944484 -4.2917814 -4.2890482 -4.2855539 -4.2829366 -4.2890248 -4.2989368 -4.3085523][-4.2839413 -4.2632713 -4.253304 -4.2567964 -4.2698336 -4.2739816 -4.2713695 -4.2715559 -4.2698178 -4.2693186 -4.2680697 -4.2662244 -4.2744436 -4.2842307 -4.291573][-4.2586145 -4.2292657 -4.2127295 -4.2145267 -4.2320924 -4.2394524 -4.2365203 -4.2377095 -4.238822 -4.2431674 -4.2497673 -4.2550116 -4.2661467 -4.27588 -4.2792726][-4.2290716 -4.1935196 -4.1727781 -4.1724195 -4.1914363 -4.1940117 -4.18384 -4.1846828 -4.1911216 -4.2033138 -4.2206926 -4.236587 -4.2538972 -4.26676 -4.2701178][-4.1983004 -4.1574874 -4.1328726 -4.1249652 -4.1355824 -4.1288323 -4.1106739 -4.1144743 -4.1278219 -4.14779 -4.1736646 -4.2037792 -4.2329683 -4.2532787 -4.2602592][-4.16804 -4.1182013 -4.0814519 -4.0584254 -4.0547357 -4.0430193 -4.0282326 -4.0370584 -4.0492949 -4.0680785 -4.0968728 -4.1450233 -4.1967468 -4.2332659 -4.25102][-4.1333909 -4.072598 -4.0189915 -3.9788237 -3.962502 -3.9484107 -3.9381006 -3.9395998 -3.9393227 -3.9468555 -3.9805114 -4.0575643 -4.1416621 -4.2029381 -4.2369585][-4.0815129 -4.0116448 -3.9439843 -3.891077 -3.8698974 -3.8548763 -3.8395095 -3.8284378 -3.8161302 -3.814276 -3.8555675 -3.9641728 -4.0806551 -4.1672912 -4.2191558][-4.0294275 -3.9620814 -3.8978024 -3.8486402 -3.8264639 -3.8067925 -3.7833688 -3.7670536 -3.7557774 -3.7541697 -3.8032727 -3.9263196 -4.0558505 -4.1532841 -4.2153497][-4.0382547 -3.9896135 -3.9431248 -3.9030747 -3.8792577 -3.8565617 -3.8380976 -3.8329608 -3.83198 -3.8419697 -3.8911748 -3.992672 -4.0999966 -4.181849 -4.2359161][-4.0967979 -4.0646081 -4.0376096 -4.013979 -3.995806 -3.9792137 -3.9694862 -3.9730263 -3.9767306 -3.9908354 -4.032083 -4.1025739 -4.17697 -4.2339225 -4.2721252][-4.1729441 -4.1574435 -4.147984 -4.1376 -4.12639 -4.1198535 -4.1212935 -4.1311264 -4.1351781 -4.1466336 -4.1756778 -4.2193503 -4.2624483 -4.2924547 -4.3127236][-4.2398362 -4.236433 -4.2357121 -4.2329412 -4.22701 -4.225244 -4.229208 -4.2401323 -4.2450566 -4.2540936 -4.2723064 -4.2981524 -4.3188424 -4.3294787 -4.3368449][-4.2831969 -4.2845354 -4.286294 -4.2857218 -4.2827964 -4.2837014 -4.2862639 -4.2947879 -4.3006592 -4.3067241 -4.3176503 -4.3323607 -4.3410635 -4.3437815 -4.3452415][-4.3102846 -4.3130541 -4.3152442 -4.3153443 -4.3139224 -4.3147411 -4.3155675 -4.3198609 -4.3236303 -4.3270154 -4.3328424 -4.3410077 -4.3454638 -4.3464661 -4.3470635]]...]
INFO - root - 2017-12-06 05:34:18.947974: step 3210, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 62h:36m:53s remains)
INFO - root - 2017-12-06 05:34:25.562648: step 3220, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 59h:32m:12s remains)
INFO - root - 2017-12-06 05:34:32.321393: step 3230, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 64h:03m:35s remains)
INFO - root - 2017-12-06 05:34:39.072018: step 3240, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 61h:23m:45s remains)
INFO - root - 2017-12-06 05:34:45.488648: step 3250, loss = 2.08, batch loss = 2.02 (24.1 examples/sec; 0.332 sec/batch; 30h:24m:34s remains)
INFO - root - 2017-12-06 05:34:52.016513: step 3260, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 62h:37m:57s remains)
INFO - root - 2017-12-06 05:34:58.900625: step 3270, loss = 2.08, batch loss = 2.03 (11.4 examples/sec; 0.703 sec/batch; 64h:15m:06s remains)
INFO - root - 2017-12-06 05:35:05.572802: step 3280, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.686 sec/batch; 62h:44m:26s remains)
INFO - root - 2017-12-06 05:35:12.449296: step 3290, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.678 sec/batch; 61h:58m:31s remains)
INFO - root - 2017-12-06 05:35:19.161580: step 3300, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 63h:55m:19s remains)
2017-12-06 05:35:23.753757: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2138839 -4.2227674 -4.2511668 -4.2818727 -4.3049755 -4.3116488 -4.3044891 -4.2795825 -4.2505217 -4.2276974 -4.208951 -4.2059503 -4.22713 -4.2516351 -4.2604938][-4.2348313 -4.2486873 -4.2756577 -4.3036427 -4.3258896 -4.3370981 -4.3383751 -4.3247142 -4.3041067 -4.2834806 -4.2637539 -4.2546563 -4.2611961 -4.2676048 -4.2638268][-4.2332573 -4.2546635 -4.2840762 -4.3101492 -4.3310513 -4.3443351 -4.3514771 -4.3489394 -4.340704 -4.3306103 -4.3197627 -4.3105268 -4.3021803 -4.2867875 -4.2647533][-4.2291579 -4.2533765 -4.2806711 -4.3016429 -4.3165903 -4.3250294 -4.3291268 -4.3310332 -4.3343396 -4.3383293 -4.3412714 -4.34071 -4.3295145 -4.303184 -4.26757][-4.2490335 -4.2637577 -4.2763672 -4.2817345 -4.2807484 -4.2764335 -4.2719359 -4.2746906 -4.287991 -4.3076243 -4.3286333 -4.3430834 -4.3393135 -4.316041 -4.2796993][-4.2759814 -4.275383 -4.2665935 -4.2474704 -4.22083 -4.1935744 -4.1731963 -4.1741567 -4.1983476 -4.2372651 -4.2809782 -4.3178344 -4.3310256 -4.321085 -4.29477][-4.2913671 -4.2807946 -4.2534332 -4.2078094 -4.1509285 -4.092679 -4.04921 -4.0491624 -4.0917125 -4.1548223 -4.2202525 -4.2771668 -4.3065557 -4.30823 -4.2912407][-4.2987528 -4.2877865 -4.2495251 -4.1822543 -4.0981007 -4.00843 -3.9366646 -3.9332366 -3.9958804 -4.0846162 -4.1686306 -4.2392006 -4.2796564 -4.2890058 -4.2792692][-4.3078346 -4.3017778 -4.2626414 -4.1872263 -4.0905232 -3.9828496 -3.8919082 -3.8844626 -3.9576411 -4.0579348 -4.1494517 -4.2224293 -4.2654033 -4.2798119 -4.2799015][-4.3234639 -4.3204465 -4.2851119 -4.2148681 -4.1281381 -4.0351968 -3.9604859 -3.95823 -4.0199661 -4.1035676 -4.1801734 -4.2402587 -4.2783422 -4.2946687 -4.3008738][-4.341836 -4.3365889 -4.3041925 -4.247364 -4.1849194 -4.1255951 -4.0856819 -4.092227 -4.1318989 -4.1831207 -4.2320914 -4.2734518 -4.3047767 -4.32289 -4.3312755][-4.3430176 -4.33226 -4.3037987 -4.2644796 -4.2294755 -4.2049279 -4.1976142 -4.2102556 -4.2294588 -4.249239 -4.2690115 -4.2916236 -4.3171272 -4.3381367 -4.348824][-4.33006 -4.3151903 -4.2921267 -4.2711296 -4.2608581 -4.2632246 -4.2765393 -4.291379 -4.295435 -4.2902994 -4.283988 -4.2872672 -4.3054619 -4.32823 -4.3423915][-4.3150811 -4.2976489 -4.2780132 -4.2704535 -4.2773919 -4.2952018 -4.3178086 -4.3322778 -4.3287845 -4.30815 -4.282022 -4.2694168 -4.2796612 -4.303174 -4.32225][-4.30155 -4.2879972 -4.2737613 -4.2742128 -4.2883439 -4.309649 -4.3310146 -4.3423014 -4.3369403 -4.3133168 -4.2804623 -4.2593346 -4.2626085 -4.2852178 -4.3095865]]...]
INFO - root - 2017-12-06 05:35:30.431060: step 3310, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 60h:37m:07s remains)
INFO - root - 2017-12-06 05:35:37.077278: step 3320, loss = 2.10, batch loss = 2.04 (12.5 examples/sec; 0.640 sec/batch; 58h:33m:48s remains)
INFO - root - 2017-12-06 05:35:43.856977: step 3330, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 61h:22m:11s remains)
INFO - root - 2017-12-06 05:35:49.250217: step 3340, loss = 2.06, batch loss = 2.00 (33.5 examples/sec; 0.239 sec/batch; 21h:51m:34s remains)
INFO - root - 2017-12-06 05:35:51.490641: step 3350, loss = 2.09, batch loss = 2.03 (35.1 examples/sec; 0.228 sec/batch; 20h:51m:05s remains)
INFO - root - 2017-12-06 05:35:57.941840: step 3360, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 60h:46m:02s remains)
INFO - root - 2017-12-06 05:36:04.540543: step 3370, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.673 sec/batch; 61h:30m:51s remains)
INFO - root - 2017-12-06 05:36:11.368214: step 3380, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.658 sec/batch; 60h:09m:34s remains)
INFO - root - 2017-12-06 05:36:17.920978: step 3390, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 59h:50m:40s remains)
INFO - root - 2017-12-06 05:36:24.549839: step 3400, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 61h:50m:28s remains)
2017-12-06 05:36:25.189152: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2381754 -4.2264066 -4.2556276 -4.2849746 -4.3016262 -4.3104792 -4.3077168 -4.2928987 -4.27921 -4.2754731 -4.2744508 -4.2747989 -4.2725587 -4.2579718 -4.220695][-4.2297816 -4.2098956 -4.2362227 -4.2660546 -4.2820621 -4.2877879 -4.2801161 -4.2606349 -4.2451029 -4.2403603 -4.2391939 -4.2446108 -4.2435393 -4.2232928 -4.1697636][-4.2107983 -4.1756749 -4.1966434 -4.2287 -4.24726 -4.2497005 -4.235486 -4.20859 -4.1915593 -4.1874976 -4.1909528 -4.2103276 -4.2165318 -4.19554 -4.1377277][-4.1833358 -4.1291757 -4.1420889 -4.1838098 -4.2111826 -4.2075071 -4.1800318 -4.1403127 -4.1192703 -4.1204319 -4.1383972 -4.1772189 -4.1967759 -4.1826339 -4.1332297][-4.1421862 -4.0677261 -4.0773721 -4.1340475 -4.1728749 -4.1639204 -4.1169152 -4.0539718 -4.023819 -4.0393305 -4.0849185 -4.1449714 -4.17852 -4.1760831 -4.1469283][-4.1091566 -4.0309868 -4.04567 -4.10548 -4.1383128 -4.1153307 -4.0424843 -3.9459949 -3.9038754 -3.9504361 -4.03465 -4.1123261 -4.1537423 -4.1640615 -4.1587367][-4.1115532 -4.0571218 -4.0802054 -4.1216278 -4.1254115 -4.0734468 -3.971844 -3.8429871 -3.797519 -3.8845286 -4.0025463 -4.0880728 -4.1322932 -4.1492257 -4.1581345][-4.130878 -4.1059823 -4.1337938 -4.1492906 -4.1215787 -4.0491772 -3.9327559 -3.7937295 -3.7602129 -3.8751657 -4.0034285 -4.0848417 -4.1292777 -4.1466269 -4.1536746][-4.1522322 -4.144228 -4.1668124 -4.1673646 -4.1305051 -4.0584598 -3.9546657 -3.8498671 -3.8454115 -3.9483306 -4.0519824 -4.1146851 -4.1533852 -4.1701283 -4.16724][-4.1857052 -4.1814771 -4.1937089 -4.1922426 -4.16464 -4.1071987 -4.0336628 -3.9803298 -3.9950445 -4.0628614 -4.1287813 -4.1692219 -4.1980934 -4.2078242 -4.1939192][-4.2402425 -4.2364287 -4.2380648 -4.2321644 -4.2147961 -4.1771054 -4.1338668 -4.1139784 -4.1300812 -4.1661086 -4.2060542 -4.232543 -4.247242 -4.2435484 -4.2228894][-4.2921267 -4.2884846 -4.2811947 -4.2666383 -4.2536016 -4.2322559 -4.2105379 -4.204608 -4.2148528 -4.2365007 -4.2665672 -4.2882829 -4.2932467 -4.2789559 -4.2549691][-4.32427 -4.3201537 -4.3096848 -4.2929788 -4.2802606 -4.26838 -4.2580428 -4.2598529 -4.2680326 -4.2853589 -4.3097844 -4.32662 -4.3240652 -4.3054819 -4.2829342][-4.3252425 -4.3212352 -4.31406 -4.3044567 -4.2961173 -4.2879219 -4.2829528 -4.2896385 -4.3008342 -4.3140864 -4.3279839 -4.3350253 -4.327579 -4.3094072 -4.2924523][-4.3130751 -4.3084626 -4.3046131 -4.3035932 -4.3031096 -4.2999477 -4.297605 -4.3048186 -4.31628 -4.3229928 -4.3258662 -4.3248606 -4.3167858 -4.3038507 -4.2940617]]...]
INFO - root - 2017-12-06 05:36:31.893479: step 3410, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.701 sec/batch; 64h:06m:24s remains)
INFO - root - 2017-12-06 05:36:38.549435: step 3420, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 60h:39m:44s remains)
INFO - root - 2017-12-06 05:36:45.301322: step 3430, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 63h:08m:30s remains)
INFO - root - 2017-12-06 05:36:52.071017: step 3440, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 60h:58m:13s remains)
INFO - root - 2017-12-06 05:36:57.314973: step 3450, loss = 2.08, batch loss = 2.02 (32.5 examples/sec; 0.246 sec/batch; 22h:29m:54s remains)
INFO - root - 2017-12-06 05:37:04.141943: step 3460, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.679 sec/batch; 62h:02m:22s remains)
INFO - root - 2017-12-06 05:37:10.788195: step 3470, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 59h:48m:45s remains)
INFO - root - 2017-12-06 05:37:17.293279: step 3480, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.621 sec/batch; 56h:47m:02s remains)
INFO - root - 2017-12-06 05:37:24.145666: step 3490, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 62h:22m:59s remains)
INFO - root - 2017-12-06 05:37:30.767347: step 3500, loss = 2.03, batch loss = 1.97 (11.9 examples/sec; 0.675 sec/batch; 61h:41m:34s remains)
2017-12-06 05:37:31.380812: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2153406 -4.2267857 -4.2388744 -4.2514539 -4.2563682 -4.2546492 -4.2508755 -4.2475057 -4.2483687 -4.2551832 -4.25 -4.2357359 -4.2211061 -4.1970525 -4.1769981][-4.2042279 -4.2083468 -4.21941 -4.2366104 -4.2394967 -4.2324123 -4.2244177 -4.2200646 -4.2254333 -4.2381997 -4.2331524 -4.2127714 -4.193006 -4.163116 -4.140954][-4.1635175 -4.1695366 -4.1849365 -4.208127 -4.2122393 -4.2000213 -4.1879129 -4.1829605 -4.1928568 -4.2121739 -4.21041 -4.1893597 -4.1672235 -4.1381736 -4.1188855][-4.1242118 -4.1332226 -4.150135 -4.1764836 -4.1819272 -4.1614294 -4.1432252 -4.1419892 -4.1622224 -4.1888289 -4.1900673 -4.168498 -4.1400032 -4.1070948 -4.0918903][-4.1165533 -4.1253386 -4.1422424 -4.1656117 -4.1723084 -4.1476026 -4.1295204 -4.136466 -4.1601057 -4.1882286 -4.1902189 -4.1658626 -4.1286426 -4.0907164 -4.0763507][-4.130105 -4.128593 -4.1337972 -4.1491394 -4.1551132 -4.1335897 -4.1214809 -4.1397328 -4.1670771 -4.1960692 -4.1983004 -4.1734977 -4.1344738 -4.093792 -4.0763965][-4.1293283 -4.1162944 -4.1067281 -4.1122909 -4.1130047 -4.0933766 -4.0902648 -4.1205425 -4.1562834 -4.1891007 -4.1983819 -4.1814656 -4.1420751 -4.0919347 -4.065289][-4.1086059 -4.0914145 -4.076416 -4.0773835 -4.0688419 -4.045578 -4.0476265 -4.0832386 -4.1228685 -4.15574 -4.1722269 -4.1638384 -4.1222076 -4.0643921 -4.0315213][-4.1040916 -4.0902147 -4.078021 -4.0792255 -4.0633912 -4.034863 -4.0308681 -4.0567064 -4.0855551 -4.1077409 -4.1202984 -4.1144133 -4.0792627 -4.0310593 -4.0048847][-4.1072893 -4.1028428 -4.1064754 -4.1216636 -4.1123896 -4.0912991 -4.0847731 -4.094317 -4.1030936 -4.107439 -4.1124892 -4.1072636 -4.0838261 -4.0539989 -4.03977][-4.1256862 -4.1355391 -4.157373 -4.1855264 -4.1864815 -4.1766949 -4.1741705 -4.1778369 -4.1766009 -4.1718726 -4.16879 -4.1629548 -4.1464581 -4.128509 -4.121798][-4.1538906 -4.1687922 -4.1953964 -4.228282 -4.2377272 -4.2384667 -4.2418 -4.2457619 -4.2439947 -4.2387791 -4.2343 -4.2290554 -4.2177281 -4.2054248 -4.1999335][-4.1730967 -4.1840477 -4.2084231 -4.242116 -4.2603931 -4.2698073 -4.2779417 -4.285059 -4.2893553 -4.287127 -4.2831221 -4.2788434 -4.2684169 -4.2563572 -4.2498922][-4.2005572 -4.2076063 -4.2267213 -4.25533 -4.2739387 -4.2845454 -4.2901163 -4.2960525 -4.3021774 -4.3023758 -4.2999492 -4.296176 -4.2894177 -4.2795587 -4.2722759][-4.2104769 -4.2129531 -4.2286887 -4.2555652 -4.2748075 -4.2852211 -4.2873373 -4.2889166 -4.29307 -4.2972307 -4.3002434 -4.3014021 -4.2998528 -4.2940311 -4.287365]]...]
INFO - root - 2017-12-06 05:37:38.117376: step 3510, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 62h:19m:41s remains)
INFO - root - 2017-12-06 05:37:44.809593: step 3520, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 61h:11m:19s remains)
INFO - root - 2017-12-06 05:37:51.534844: step 3530, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 63h:14m:39s remains)
INFO - root - 2017-12-06 05:37:58.196577: step 3540, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 64h:00m:16s remains)
INFO - root - 2017-12-06 05:38:04.550899: step 3550, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 60h:30m:24s remains)
INFO - root - 2017-12-06 05:38:11.182675: step 3560, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 60h:37m:18s remains)
INFO - root - 2017-12-06 05:38:17.884233: step 3570, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 60h:28m:11s remains)
INFO - root - 2017-12-06 05:38:24.683881: step 3580, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.691 sec/batch; 63h:08m:52s remains)
INFO - root - 2017-12-06 05:38:31.150218: step 3590, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.634 sec/batch; 57h:54m:27s remains)
INFO - root - 2017-12-06 05:38:37.769968: step 3600, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 62h:58m:36s remains)
2017-12-06 05:38:38.458311: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2651472 -4.2520151 -4.2320104 -4.1946993 -4.1547232 -4.1371469 -4.1424217 -4.1610384 -4.1795845 -4.2050114 -4.2210841 -4.213779 -4.1838589 -4.1461506 -4.1226158][-4.2497478 -4.2420907 -4.2321143 -4.2003489 -4.1553841 -4.126914 -4.1276336 -4.1509891 -4.1728883 -4.2003736 -4.2205472 -4.2155461 -4.186605 -4.14912 -4.1197991][-4.2319646 -4.236742 -4.2433958 -4.2269058 -4.1834931 -4.1442161 -4.1367521 -4.1599488 -4.184988 -4.2130384 -4.2352633 -4.2345376 -4.212431 -4.1811562 -4.1513314][-4.2122416 -4.2286263 -4.2457771 -4.2453022 -4.2093534 -4.1635041 -4.1500416 -4.1721549 -4.2020221 -4.2304773 -4.253943 -4.2570443 -4.244545 -4.22117 -4.1940627][-4.1906929 -4.2129979 -4.2327194 -4.2407594 -4.2150459 -4.1689305 -4.1512651 -4.1725554 -4.2051964 -4.2342052 -4.259243 -4.2700024 -4.2673869 -4.2506394 -4.22756][-4.16307 -4.1892514 -4.2080636 -4.2176704 -4.2004724 -4.1603656 -4.1417093 -4.1625957 -4.193718 -4.2201591 -4.2457042 -4.2653217 -4.2707853 -4.2589431 -4.2397494][-4.1335421 -4.1564264 -4.1745749 -4.1859493 -4.174819 -4.1458006 -4.1317763 -4.1527724 -4.1784196 -4.1965985 -4.2151046 -4.2380676 -4.2508764 -4.2447224 -4.2309856][-4.1180458 -4.1291785 -4.1477475 -4.1653142 -4.1611981 -4.1419311 -4.1336207 -4.1509027 -4.1655722 -4.1688871 -4.1754313 -4.1972418 -4.2147427 -4.2170382 -4.2131][-4.1321607 -4.1294079 -4.1429133 -4.1647873 -4.16859 -4.1575594 -4.1549606 -4.1669416 -4.1670561 -4.1560483 -4.1506877 -4.1671653 -4.1852956 -4.1954393 -4.2014928][-4.1695728 -4.1585717 -4.1635666 -4.1814547 -4.1883307 -4.1821475 -4.1843567 -4.1912827 -4.1800461 -4.161263 -4.1530795 -4.1643643 -4.1762071 -4.18675 -4.1990175][-4.2040648 -4.1919703 -4.1900167 -4.1989365 -4.2019038 -4.1981688 -4.1993208 -4.1993117 -4.1837606 -4.1662812 -4.1652441 -4.177012 -4.1837492 -4.1901717 -4.2018113][-4.2221713 -4.2115011 -4.2061992 -4.2076564 -4.2059765 -4.2012148 -4.2000222 -4.193984 -4.1786728 -4.1670494 -4.1738119 -4.18805 -4.19361 -4.1962981 -4.2039289][-4.229557 -4.2209392 -4.2159204 -4.2167144 -4.2150683 -4.2086272 -4.2031136 -4.1920447 -4.1764054 -4.1677442 -4.177659 -4.1935334 -4.2005057 -4.2026215 -4.2097125][-4.240448 -4.23555 -4.2331963 -4.2356253 -4.2351193 -4.2288852 -4.2192211 -4.2052627 -4.1893582 -4.1819954 -4.1913476 -4.2059484 -4.2131057 -4.216445 -4.2229404][-4.2528782 -4.2513542 -4.2513971 -4.2561903 -4.2591538 -4.2556815 -4.2469077 -4.2343221 -4.2206 -4.213294 -4.2186155 -4.2294836 -4.2349548 -4.2354755 -4.2386675]]...]
INFO - root - 2017-12-06 05:38:45.138287: step 3610, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 60h:19m:52s remains)
INFO - root - 2017-12-06 05:38:51.877813: step 3620, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.669 sec/batch; 61h:09m:36s remains)
INFO - root - 2017-12-06 05:38:58.653829: step 3630, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.696 sec/batch; 63h:36m:59s remains)
INFO - root - 2017-12-06 05:39:05.390998: step 3640, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.645 sec/batch; 58h:54m:56s remains)
INFO - root - 2017-12-06 05:39:11.776190: step 3650, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.681 sec/batch; 62h:12m:48s remains)
INFO - root - 2017-12-06 05:39:18.483599: step 3660, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 59h:23m:41s remains)
INFO - root - 2017-12-06 05:39:25.147998: step 3670, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.664 sec/batch; 60h:38m:35s remains)
INFO - root - 2017-12-06 05:39:31.831765: step 3680, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 61h:54m:36s remains)
INFO - root - 2017-12-06 05:39:38.608459: step 3690, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 60h:06m:12s remains)
INFO - root - 2017-12-06 05:39:45.252132: step 3700, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 62h:37m:47s remains)
2017-12-06 05:39:45.882689: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2417822 -4.2446537 -4.228549 -4.1992316 -4.1766644 -4.1759882 -4.1844978 -4.1764054 -4.1572328 -4.1402793 -4.1397681 -4.1577282 -4.1730757 -4.1867518 -4.1998625][-4.2141457 -4.2182431 -4.206522 -4.1798468 -4.1613684 -4.1704888 -4.18807 -4.1783876 -4.1468687 -4.1160436 -4.1105947 -4.134994 -4.163106 -4.1859031 -4.2017269][-4.1747169 -4.1795897 -4.1716518 -4.1527829 -4.1446056 -4.1638021 -4.1876745 -4.1803975 -4.1458478 -4.1151543 -4.1170506 -4.1503663 -4.1849513 -4.2049608 -4.2067785][-4.1403284 -4.1405263 -4.1342521 -4.1259437 -4.1297874 -4.1562319 -4.1809306 -4.1760426 -4.1503983 -4.1338077 -4.1488466 -4.1846108 -4.2109923 -4.2155895 -4.1984224][-4.1210842 -4.1104574 -4.1013503 -4.0966663 -4.1036224 -4.1255322 -4.1382284 -4.1330938 -4.126 -4.1317797 -4.1596208 -4.19126 -4.2092261 -4.2026634 -4.17247][-4.0986457 -4.07721 -4.0596943 -4.04665 -4.0431466 -4.0474839 -4.0412993 -4.0370655 -4.0549021 -4.0853162 -4.1237283 -4.1546288 -4.1692042 -4.1614528 -4.1339321][-4.0439181 -4.0076671 -3.9725921 -3.9428046 -3.9268332 -3.9161367 -3.8954887 -3.8962064 -3.944262 -4.0065079 -4.0602446 -4.0951672 -4.1079888 -4.1027741 -4.0885472][-3.9839873 -3.9244144 -3.861078 -3.8088565 -3.7846437 -3.767349 -3.7417698 -3.756887 -3.835927 -3.9244175 -3.9907691 -4.0312934 -4.0452986 -4.0479817 -4.0528045][-4.022141 -3.967684 -3.910255 -3.8669596 -3.854214 -3.8509316 -3.8460555 -3.8658502 -3.9270303 -3.9917233 -4.0355353 -4.058835 -4.0598917 -4.0558414 -4.0608335][-4.1196141 -4.0858488 -4.052824 -4.03038 -4.0273933 -4.0341544 -4.0424662 -4.0531111 -4.0790811 -4.1076541 -4.1222405 -4.1261969 -4.118083 -4.1060843 -4.1036453][-4.2068553 -4.1911125 -4.1757836 -4.1645718 -4.1620889 -4.1671391 -4.1738772 -4.1755958 -4.1810064 -4.1888456 -4.1902437 -4.1876278 -4.1813693 -4.1711459 -4.1660986][-4.263979 -4.2544 -4.2436662 -4.234046 -4.22896 -4.229732 -4.2320895 -4.229939 -4.229166 -4.23218 -4.2327528 -4.2319274 -4.2307158 -4.2278528 -4.2291112][-4.2937779 -4.2855425 -4.2761621 -4.2671208 -4.2609024 -4.2592359 -4.2595978 -4.2581429 -4.2574353 -4.2585478 -4.2601051 -4.2631826 -4.2672157 -4.2712512 -4.2762961][-4.3049259 -4.2975659 -4.2907763 -4.2846117 -4.2804046 -4.2786927 -4.2784824 -4.2779613 -4.2764792 -4.275178 -4.27602 -4.2800589 -4.2860689 -4.2917361 -4.2959309][-4.3103461 -4.3041668 -4.2996368 -4.29596 -4.293 -4.2925386 -4.2939925 -4.2952423 -4.2932839 -4.2890229 -4.2863717 -4.2863374 -4.2894659 -4.2929888 -4.2954144]]...]
INFO - root - 2017-12-06 05:39:52.483331: step 3710, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 57h:57m:01s remains)
INFO - root - 2017-12-06 05:39:59.244721: step 3720, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 61h:13m:00s remains)
INFO - root - 2017-12-06 05:40:05.997469: step 3730, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.681 sec/batch; 62h:11m:39s remains)
INFO - root - 2017-12-06 05:40:12.391829: step 3740, loss = 2.08, batch loss = 2.02 (13.6 examples/sec; 0.589 sec/batch; 53h:49m:24s remains)
INFO - root - 2017-12-06 05:40:19.169978: step 3750, loss = 2.08, batch loss = 2.03 (10.9 examples/sec; 0.733 sec/batch; 66h:54m:49s remains)
INFO - root - 2017-12-06 05:40:27.782986: step 3760, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 80h:52m:11s remains)
INFO - root - 2017-12-06 05:40:36.764293: step 3770, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 80h:06m:49s remains)
INFO - root - 2017-12-06 05:40:45.668958: step 3780, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.879 sec/batch; 80h:15m:22s remains)
INFO - root - 2017-12-06 05:40:54.393790: step 3790, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 78h:34m:36s remains)
INFO - root - 2017-12-06 05:41:03.428598: step 3800, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 83h:32m:14s remains)
2017-12-06 05:41:04.239049: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1933117 -4.1343093 -4.0815744 -4.0688829 -4.1216021 -4.1861429 -4.2167869 -4.2070251 -4.1852489 -4.1689129 -4.1731882 -4.1857991 -4.1860204 -4.1778078 -4.1874185][-4.1966968 -4.1458864 -4.1053524 -4.1069288 -4.1610069 -4.2104063 -4.2265425 -4.2128677 -4.1906552 -4.1715794 -4.1795535 -4.2081742 -4.2296252 -4.2392955 -4.2473927][-4.2255793 -4.1904688 -4.1675858 -4.1769724 -4.2142158 -4.2354388 -4.2348895 -4.2130775 -4.1845164 -4.1645684 -4.1766119 -4.212657 -4.2462835 -4.2711873 -4.2816978][-4.2743564 -4.2534127 -4.2391 -4.2443242 -4.2619586 -4.2614121 -4.2417879 -4.1993217 -4.1566467 -4.140213 -4.1696777 -4.2162986 -4.2526245 -4.2811246 -4.2932615][-4.3142753 -4.3058586 -4.2959485 -4.2956238 -4.2928219 -4.2714667 -4.2232809 -4.14451 -4.0766406 -4.0766134 -4.1414819 -4.2054906 -4.2479448 -4.2757378 -4.2895522][-4.3251987 -4.3279657 -4.323123 -4.3157396 -4.2957559 -4.2558694 -4.1716285 -4.04066 -3.9432874 -3.9822574 -4.0897341 -4.1714759 -4.2246394 -4.2634764 -4.2861643][-4.3272929 -4.3357296 -4.3302083 -4.3098669 -4.2707014 -4.20701 -4.0766444 -3.8855307 -3.7735333 -3.8799257 -4.0339446 -4.1345263 -4.2019944 -4.2515469 -4.2832403][-4.328527 -4.3387156 -4.32945 -4.298255 -4.2392755 -4.1444044 -3.97248 -3.7387116 -3.6527877 -3.824683 -4.0045042 -4.1164169 -4.1945233 -4.2489181 -4.2815943][-4.3256922 -4.3361597 -4.3227863 -4.2813573 -4.2108622 -4.1037955 -3.9354837 -3.7348611 -3.7110605 -3.8757279 -4.031127 -4.1313806 -4.2046409 -4.2561707 -4.2834253][-4.3153968 -4.3216877 -4.305233 -4.2597384 -4.1881685 -4.0964866 -3.9698856 -3.8523512 -3.8715827 -3.9899247 -4.0972662 -4.1747093 -4.2343521 -4.273622 -4.2880497][-4.3090906 -4.3113112 -4.2969942 -4.2570043 -4.1964617 -4.1301804 -4.04823 -3.9953716 -4.0301819 -4.1066055 -4.1763825 -4.2331653 -4.2754841 -4.2966571 -4.2980781][-4.3117127 -4.3138518 -4.3026619 -4.2719789 -4.230844 -4.1913738 -4.1455803 -4.127985 -4.1559639 -4.2008781 -4.2457528 -4.2849207 -4.3100424 -4.3156962 -4.309669][-4.3162589 -4.3189726 -4.3115225 -4.2950115 -4.2768855 -4.2609477 -4.240572 -4.2372112 -4.2519703 -4.2746925 -4.300591 -4.3215137 -4.331378 -4.3260555 -4.3170834][-4.3197427 -4.3225441 -4.3202987 -4.3179908 -4.317986 -4.3164735 -4.3086166 -4.3055897 -4.3084159 -4.31581 -4.3277168 -4.3355713 -4.3355231 -4.3259168 -4.3185062][-4.3197761 -4.3231087 -4.3270588 -4.3329558 -4.3403516 -4.3433843 -4.3383331 -4.331583 -4.3263483 -4.3256717 -4.3287258 -4.3293014 -4.3251009 -4.3183584 -4.3147435]]...]
INFO - root - 2017-12-06 05:41:13.161756: step 3810, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 79h:32m:45s remains)
INFO - root - 2017-12-06 05:41:22.078488: step 3820, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 82h:01m:15s remains)
INFO - root - 2017-12-06 05:41:31.024774: step 3830, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.880 sec/batch; 80h:22m:54s remains)
INFO - root - 2017-12-06 05:41:39.839882: step 3840, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.927 sec/batch; 84h:35m:15s remains)
INFO - root - 2017-12-06 05:41:48.653205: step 3850, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 79h:45m:28s remains)
INFO - root - 2017-12-06 05:41:57.599478: step 3860, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 84h:53m:46s remains)
INFO - root - 2017-12-06 05:42:06.566972: step 3870, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 80h:51m:49s remains)
INFO - root - 2017-12-06 05:42:15.652676: step 3880, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.894 sec/batch; 81h:36m:00s remains)
INFO - root - 2017-12-06 05:42:24.809893: step 3890, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 84h:16m:30s remains)
INFO - root - 2017-12-06 05:42:33.781211: step 3900, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.904 sec/batch; 82h:29m:42s remains)
2017-12-06 05:42:35.142706: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2523327 -4.1921158 -4.14091 -4.108717 -4.1085377 -4.1374855 -4.1782532 -4.1981683 -4.1969218 -4.1892018 -4.1814237 -4.1733375 -4.164063 -4.1545396 -4.154295][-4.2508669 -4.1810064 -4.1140289 -4.0698161 -4.0753713 -4.119904 -4.1674414 -4.1883912 -4.1885638 -4.1777215 -4.1694965 -4.1704965 -4.1688161 -4.1591439 -4.1563334][-4.2493391 -4.1719136 -4.0889091 -4.033782 -4.0505352 -4.1141281 -4.169713 -4.1897106 -4.1895075 -4.1747169 -4.1638651 -4.1667857 -4.1663294 -4.1552949 -4.1506896][-4.2522016 -4.1706581 -4.075316 -4.0138288 -4.0429087 -4.1241927 -4.1827683 -4.1984677 -4.1910658 -4.1716332 -4.1585217 -4.1596084 -4.1603413 -4.1556869 -4.1562529][-4.2598267 -4.1759567 -4.0736756 -4.0035796 -4.0340128 -4.1253676 -4.1883259 -4.2032609 -4.1912103 -4.1704154 -4.1557255 -4.1535559 -4.158309 -4.1659079 -4.1752348][-4.2761331 -4.1968765 -4.0949039 -4.0158563 -4.0342064 -4.1234245 -4.1866522 -4.1981764 -4.1882005 -4.1721 -4.1565871 -4.1505089 -4.1578536 -4.1734815 -4.18776][-4.2858291 -4.2157936 -4.1221313 -4.0415726 -4.0474553 -4.1253762 -4.1833086 -4.1929574 -4.186451 -4.1774549 -4.1581492 -4.14703 -4.1527209 -4.1699095 -4.18553][-4.2880654 -4.2263546 -4.1442838 -4.0715866 -4.0733476 -4.1401229 -4.1956286 -4.207201 -4.1998425 -4.1884356 -4.1621308 -4.1412029 -4.1425648 -4.1596 -4.1793571][-4.2938519 -4.239964 -4.1699739 -4.1115036 -4.1108971 -4.1642141 -4.2155571 -4.2282014 -4.2186933 -4.2075896 -4.1856046 -4.1639333 -4.1551652 -4.1617484 -4.1735358][-4.2945266 -4.2448826 -4.1851764 -4.1390047 -4.1393461 -4.1851859 -4.23341 -4.2450724 -4.2336206 -4.2247076 -4.2095203 -4.1910725 -4.1748862 -4.1686296 -4.174542][-4.287828 -4.2401342 -4.1880159 -4.1534281 -4.1591196 -4.1993904 -4.2482338 -4.2636709 -4.2541633 -4.2475004 -4.236084 -4.2176352 -4.197094 -4.1848221 -4.1867418][-4.2831125 -4.2395806 -4.1981583 -4.1744752 -4.1820354 -4.2141647 -4.2584534 -4.2762594 -4.2692475 -4.2636814 -4.2563004 -4.2399988 -4.2179575 -4.2038913 -4.2039137][-4.2770925 -4.2399487 -4.2086968 -4.1945529 -4.2028131 -4.2249589 -4.2597489 -4.2752924 -4.2705116 -4.2665448 -4.2627053 -4.2493653 -4.22837 -4.2165108 -4.2207088][-4.2706285 -4.2419682 -4.2205868 -4.2125406 -4.21993 -4.235393 -4.2623558 -4.2739787 -4.2680259 -4.2633176 -4.2617269 -4.2546115 -4.2400732 -4.2333345 -4.2410808][-4.2752686 -4.2549033 -4.2418437 -4.2378845 -4.2449493 -4.2561069 -4.2751417 -4.282248 -4.2756133 -4.269402 -4.2656517 -4.2608032 -4.2557106 -4.2560186 -4.2644076]]...]
INFO - root - 2017-12-06 05:42:43.936563: step 3910, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 77h:45m:29s remains)
INFO - root - 2017-12-06 05:42:53.081450: step 3920, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 86h:59m:39s remains)
INFO - root - 2017-12-06 05:43:01.575008: step 3930, loss = 2.10, batch loss = 2.04 (11.4 examples/sec; 0.702 sec/batch; 64h:05m:12s remains)
INFO - root - 2017-12-06 05:43:09.480311: step 3940, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 82h:16m:47s remains)
INFO - root - 2017-12-06 05:43:17.952876: step 3950, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.689 sec/batch; 62h:52m:00s remains)
INFO - root - 2017-12-06 05:43:26.794528: step 3960, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.905 sec/batch; 82h:34m:13s remains)
INFO - root - 2017-12-06 05:43:35.701492: step 3970, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 80h:08m:09s remains)
INFO - root - 2017-12-06 05:43:44.659925: step 3980, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.855 sec/batch; 77h:59m:41s remains)
INFO - root - 2017-12-06 05:43:53.666451: step 3990, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 81h:56m:49s remains)
INFO - root - 2017-12-06 05:44:02.768537: step 4000, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 82h:07m:08s remains)
2017-12-06 05:44:03.760982: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.209219 -4.1979356 -4.2141557 -4.23266 -4.2377186 -4.2384057 -4.2428036 -4.2510839 -4.2614522 -4.2760162 -4.2898693 -4.2917857 -4.2818336 -4.2704649 -4.2661872][-4.2012391 -4.1871843 -4.20165 -4.2209253 -4.2273273 -4.226387 -4.2306643 -4.2395325 -4.2528629 -4.2689462 -4.2781258 -4.2775712 -4.2665348 -4.2534113 -4.2485409][-4.1999354 -4.1812882 -4.1863265 -4.1990819 -4.2037268 -4.200027 -4.1991343 -4.2087507 -4.2319922 -4.2567148 -4.2690639 -4.2714663 -4.2627625 -4.248271 -4.2370825][-4.1906023 -4.162281 -4.1567545 -4.16479 -4.17032 -4.1693573 -4.166028 -4.1746416 -4.2042418 -4.239512 -4.2582054 -4.2641006 -4.2588878 -4.2452879 -4.2277374][-4.181572 -4.1450019 -4.1340046 -4.1412787 -4.1497455 -4.15144 -4.1452479 -4.1495552 -4.1808958 -4.2211814 -4.2445145 -4.252285 -4.2481027 -4.2340193 -4.2149019][-4.1903868 -4.1513762 -4.1378756 -4.142447 -4.1460533 -4.140521 -4.1230965 -4.1160293 -4.1438041 -4.1898427 -4.2216611 -4.2364922 -4.2368731 -4.2269831 -4.2115936][-4.2061081 -4.1780257 -4.1666608 -4.1674905 -4.1594939 -4.1337686 -4.0911908 -4.0600686 -4.0798378 -4.1400967 -4.1939869 -4.227139 -4.2383609 -4.2334466 -4.2217493][-4.2166882 -4.2007179 -4.1961856 -4.1945219 -4.1789274 -4.1386175 -4.0745316 -4.0160174 -4.0214686 -4.0982194 -4.1779785 -4.2315025 -4.2539959 -4.2538176 -4.2419496][-4.2265534 -4.2189188 -4.2179756 -4.2123775 -4.1947837 -4.1593714 -4.0981979 -4.0329237 -4.0236859 -4.0942688 -4.1778145 -4.2369127 -4.2631474 -4.266727 -4.2553058][-4.2318807 -4.2307763 -4.2324347 -4.2259068 -4.2129216 -4.1900787 -4.1450257 -4.0920749 -4.0738273 -4.116231 -4.1809731 -4.2331481 -4.2606192 -4.2682509 -4.2612214][-4.2416511 -4.241394 -4.2459493 -4.2437744 -4.2392583 -4.2273517 -4.1956453 -4.156446 -4.1324487 -4.1498117 -4.1933379 -4.2349305 -4.26174 -4.272974 -4.270124][-4.2580209 -4.25629 -4.2608876 -4.2634211 -4.2666116 -4.2627025 -4.2410474 -4.2128916 -4.1883655 -4.1923189 -4.2204075 -4.2474103 -4.2662778 -4.2780447 -4.2770972][-4.2712 -4.2676229 -4.2693586 -4.2702284 -4.2731433 -4.2716718 -4.2622161 -4.2468271 -4.229074 -4.2300982 -4.2493372 -4.2629995 -4.2710361 -4.277792 -4.2753553][-4.2681851 -4.2623315 -4.26139 -4.256762 -4.2525959 -4.2498388 -4.2514787 -4.2521839 -4.2443552 -4.2464828 -4.2604871 -4.2690187 -4.2720513 -4.2732935 -4.2685523][-4.2562976 -4.2475543 -4.2451048 -4.2347026 -4.2219963 -4.2151885 -4.2222872 -4.2342753 -4.2339249 -4.23648 -4.2490506 -4.2609954 -4.2678165 -4.2710881 -4.2679305]]...]
INFO - root - 2017-12-06 05:44:12.823275: step 4010, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 84h:45m:21s remains)
INFO - root - 2017-12-06 05:44:21.750893: step 4020, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 80h:26m:41s remains)
INFO - root - 2017-12-06 05:44:30.701043: step 4030, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 77h:48m:34s remains)
INFO - root - 2017-12-06 05:44:39.854900: step 4040, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.934 sec/batch; 85h:15m:04s remains)
INFO - root - 2017-12-06 05:44:48.755723: step 4050, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 82h:06m:02s remains)
INFO - root - 2017-12-06 05:44:57.801777: step 4060, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.926 sec/batch; 84h:28m:22s remains)
INFO - root - 2017-12-06 05:45:06.826546: step 4070, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 81h:44m:21s remains)
INFO - root - 2017-12-06 05:45:16.040946: step 4080, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 80h:53m:46s remains)
INFO - root - 2017-12-06 05:45:24.953016: step 4090, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 80h:47m:48s remains)
INFO - root - 2017-12-06 05:45:33.952705: step 4100, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.886 sec/batch; 80h:51m:25s remains)
2017-12-06 05:45:34.755027: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3269715 -4.3325477 -4.3339863 -4.3317657 -4.3301611 -4.3318434 -4.331655 -4.3310528 -4.3351922 -4.3381743 -4.3333645 -4.3273463 -4.3233628 -4.3197126 -4.3187623][-4.3093796 -4.3102145 -4.3096385 -4.3076272 -4.3096089 -4.3147135 -4.3158364 -4.3174448 -4.3254342 -4.329154 -4.3196883 -4.3087897 -4.3000555 -4.2951403 -4.295022][-4.2791867 -4.2724319 -4.2674642 -4.2678914 -4.276217 -4.2859654 -4.2912016 -4.2971339 -4.3072419 -4.3080597 -4.2938566 -4.2790718 -4.2668209 -4.2636466 -4.266264][-4.2456222 -4.2365046 -4.2330785 -4.2387123 -4.2516022 -4.262917 -4.2671719 -4.2744884 -4.2843361 -4.2826796 -4.2684174 -4.2572632 -4.2481961 -4.2478895 -4.2524681][-4.2240777 -4.2211161 -4.22328 -4.2314272 -4.242691 -4.2457066 -4.2407451 -4.2405062 -4.2461362 -4.24411 -4.2358274 -4.236692 -4.2371197 -4.2403483 -4.2422442][-4.224472 -4.2271156 -4.2303267 -4.2349968 -4.2371931 -4.2246351 -4.1994281 -4.1839576 -4.1897655 -4.19804 -4.2055936 -4.2232466 -4.2329297 -4.23471 -4.2282925][-4.23766 -4.2357092 -4.2288847 -4.2212682 -4.2067242 -4.169703 -4.1156659 -4.0857425 -4.1096697 -4.1507869 -4.1877613 -4.2223635 -4.2346334 -4.2287307 -4.2142367][-4.2435989 -4.2316403 -4.2132297 -4.1911511 -4.1557856 -4.0897212 -4.0037956 -3.9653254 -4.0290766 -4.1176147 -4.1832008 -4.2208843 -4.2250133 -4.2110114 -4.1910195][-4.2363253 -4.2173052 -4.1945896 -4.1699543 -4.1309495 -4.0603209 -3.9749515 -3.952332 -4.0417824 -4.1444349 -4.2072668 -4.2270288 -4.2145267 -4.1890173 -4.1645813][-4.2272782 -4.2080779 -4.1885252 -4.1686125 -4.1421337 -4.0986586 -4.0556974 -4.064785 -4.1361175 -4.2026486 -4.2348852 -4.2283616 -4.1947255 -4.1619921 -4.1505694][-4.2207847 -4.2116389 -4.19949 -4.1852469 -4.1699643 -4.1494555 -4.1355338 -4.1556296 -4.2006712 -4.2300091 -4.2307153 -4.2035413 -4.1615186 -4.14219 -4.157928][-4.21875 -4.2210317 -4.2145853 -4.2049661 -4.1931763 -4.1818562 -4.1792259 -4.1977396 -4.2211676 -4.2254539 -4.2066712 -4.1687555 -4.1347265 -4.1400347 -4.1790867][-4.2093463 -4.2175026 -4.2137666 -4.2067719 -4.1960359 -4.1870489 -4.191896 -4.2100167 -4.2216296 -4.2150121 -4.1867137 -4.1488247 -4.1302605 -4.1512384 -4.1945872][-4.1727166 -4.1769161 -4.1765203 -4.1749759 -4.16955 -4.1644363 -4.173358 -4.1934404 -4.2014217 -4.1940742 -4.1698079 -4.1432467 -4.1376109 -4.15905 -4.1934094][-4.1209373 -4.1172256 -4.1181746 -4.1248975 -4.1282444 -4.1287 -4.1413903 -4.1633534 -4.1718154 -4.1708288 -4.1604962 -4.1477818 -4.1457329 -4.1583309 -4.1813126]]...]
INFO - root - 2017-12-06 05:45:43.831722: step 4110, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 83h:54m:43s remains)
INFO - root - 2017-12-06 05:45:52.903765: step 4120, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 82h:07m:31s remains)
INFO - root - 2017-12-06 05:46:01.101106: step 4130, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 82h:45m:13s remains)
INFO - root - 2017-12-06 05:46:10.038521: step 4140, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.880 sec/batch; 80h:15m:41s remains)
INFO - root - 2017-12-06 05:46:18.823068: step 4150, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 79h:30m:56s remains)
INFO - root - 2017-12-06 05:46:27.855314: step 4160, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 85h:03m:29s remains)
INFO - root - 2017-12-06 05:46:36.876054: step 4170, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.916 sec/batch; 83h:30m:39s remains)
INFO - root - 2017-12-06 05:46:45.830440: step 4180, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 80h:53m:16s remains)
INFO - root - 2017-12-06 05:46:54.708103: step 4190, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 81h:53m:00s remains)
INFO - root - 2017-12-06 05:47:03.848768: step 4200, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 81h:58m:57s remains)
2017-12-06 05:47:04.606294: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2006121 -4.2142129 -4.2296977 -4.2316141 -4.2071319 -4.1537867 -4.0759544 -3.995579 -3.9287205 -3.895195 -3.9108458 -3.9611669 -4.0147262 -4.053062 -4.0848727][-4.2110777 -4.2285423 -4.2504621 -4.2586765 -4.2388086 -4.1873827 -4.1103597 -4.02018 -3.9417126 -3.8984179 -3.906245 -3.9568665 -4.0126553 -4.0521894 -4.0824914][-4.2406836 -4.2632337 -4.2860389 -4.2938952 -4.2782431 -4.2294731 -4.1572032 -4.0631633 -3.9722869 -3.9141867 -3.9106445 -3.9567945 -4.0144048 -4.0614891 -4.0938048][-4.2550421 -4.2833796 -4.3060932 -4.3107743 -4.2944174 -4.2504373 -4.1831269 -4.0927892 -3.9994166 -3.9262519 -3.90333 -3.9390872 -4.0002732 -4.0597715 -4.0998888][-4.2597389 -4.289176 -4.307538 -4.3083224 -4.2923956 -4.2542248 -4.1910892 -4.113009 -4.0296736 -3.946425 -3.9037833 -3.9237323 -3.9788685 -4.0454469 -4.0974288][-4.2576227 -4.2854109 -4.2991352 -4.298171 -4.2833519 -4.2499838 -4.1935863 -4.1261587 -4.0519438 -3.9667003 -3.9158893 -3.9214437 -3.9614394 -4.0266042 -4.0896845][-4.2562685 -4.2817855 -4.29359 -4.2905564 -4.2777171 -4.2481456 -4.199616 -4.138557 -4.06604 -3.9841993 -3.9352012 -3.9279661 -3.950547 -4.0086927 -4.0785513][-4.2779446 -4.2984529 -4.3061571 -4.3017411 -4.2906876 -4.2637415 -4.2209258 -4.163002 -4.0927734 -4.015718 -3.96649 -3.9451737 -3.9485743 -3.9980135 -4.070097][-4.3122344 -4.3240051 -4.3250871 -4.3213124 -4.3145366 -4.2921934 -4.2576303 -4.2054005 -4.1391196 -4.0678248 -4.0175929 -3.9846716 -3.9759903 -4.0150576 -4.0811872][-4.3300748 -4.335454 -4.33203 -4.3278484 -4.3247771 -4.3109765 -4.2868524 -4.2447305 -4.1855164 -4.1238589 -4.0775352 -4.0415816 -4.029808 -4.061048 -4.1156349][-4.337224 -4.3394442 -4.3343825 -4.33231 -4.3328209 -4.3254428 -4.3067322 -4.2746754 -4.2265968 -4.1775489 -4.1380548 -4.1063495 -4.095078 -4.1178174 -4.1590285][-4.3474312 -4.3491459 -4.3450336 -4.343504 -4.3424711 -4.337431 -4.3232923 -4.3004827 -4.2650623 -4.2289028 -4.1972375 -4.1720548 -4.1626706 -4.175983 -4.2044258][-4.352385 -4.3533421 -4.3494606 -4.3451586 -4.3407259 -4.3359337 -4.3282456 -4.3136368 -4.2901511 -4.2655792 -4.2442722 -4.2268863 -4.219027 -4.2260494 -4.2469673][-4.3424897 -4.3409495 -4.3370037 -4.3307347 -4.3241177 -4.320003 -4.3151875 -4.3069277 -4.2941484 -4.2816405 -4.2713466 -4.2635818 -4.2612181 -4.2681842 -4.2850614][-4.3223858 -4.3197608 -4.3167748 -4.3123846 -4.308291 -4.3051996 -4.3023629 -4.2997866 -4.296042 -4.2935758 -4.2926702 -4.2932196 -4.296865 -4.3058372 -4.31902]]...]
INFO - root - 2017-12-06 05:47:13.446419: step 4210, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 79h:50m:35s remains)
INFO - root - 2017-12-06 05:47:22.583005: step 4220, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.925 sec/batch; 84h:22m:45s remains)
INFO - root - 2017-12-06 05:47:31.580046: step 4230, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 85h:43m:08s remains)
INFO - root - 2017-12-06 05:47:40.648936: step 4240, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 83h:24m:58s remains)
INFO - root - 2017-12-06 05:47:49.708737: step 4250, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.883 sec/batch; 80h:31m:57s remains)
INFO - root - 2017-12-06 05:47:58.742015: step 4260, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.895 sec/batch; 81h:37m:33s remains)
INFO - root - 2017-12-06 05:48:07.954246: step 4270, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 84h:31m:53s remains)
INFO - root - 2017-12-06 05:48:16.980458: step 4280, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 83h:15m:08s remains)
INFO - root - 2017-12-06 05:48:25.715002: step 4290, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 80h:24m:03s remains)
INFO - root - 2017-12-06 05:48:34.828695: step 4300, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 85h:11m:21s remains)
2017-12-06 05:48:35.610304: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1872549 -4.1915855 -4.1909409 -4.1807976 -4.1588926 -4.1313314 -4.1148729 -4.1226468 -4.1464763 -4.1697292 -4.18798 -4.1992941 -4.1991405 -4.1904426 -4.1792612][-4.1739426 -4.1815271 -4.1798878 -4.1617379 -4.1261067 -4.0859485 -4.0681314 -4.0844979 -4.1182213 -4.1492829 -4.1750841 -4.1940575 -4.1971579 -4.1866941 -4.1717806][-4.1663566 -4.173707 -4.1669483 -4.1370354 -4.08566 -4.0344896 -4.0168653 -4.0431213 -4.0878687 -4.1290321 -4.1638527 -4.1876097 -4.1914325 -4.1784987 -4.158113][-4.1663661 -4.1694131 -4.1543517 -4.1126976 -4.047843 -3.9891927 -3.9712973 -4.0082211 -4.0673046 -4.1200471 -4.1632085 -4.18871 -4.19059 -4.171773 -4.1405687][-4.17422 -4.1669979 -4.1408134 -4.0880747 -4.0127687 -3.9503634 -3.9339206 -3.9846408 -4.0611162 -4.1251307 -4.1741443 -4.2000055 -4.1969581 -4.1659803 -4.1181803][-4.1843262 -4.1658325 -4.1291213 -4.0664721 -3.9828138 -3.916477 -3.9010921 -3.9664502 -4.0586276 -4.1323314 -4.1858187 -4.2118683 -4.2038636 -4.1606221 -4.0963392][-4.18655 -4.1610079 -4.1196589 -4.0523591 -3.964381 -3.8924212 -3.8771496 -3.9547796 -4.0591097 -4.1403952 -4.1970549 -4.2246189 -4.2151589 -4.1653948 -4.0906739][-4.17777 -4.149097 -4.1128945 -4.056006 -3.9779315 -3.9068251 -3.8915703 -3.9702041 -4.0753288 -4.1569762 -4.2115493 -4.2397208 -4.2326355 -4.1853709 -4.1097889][-4.1623225 -4.1326947 -4.108006 -4.072226 -4.0188522 -3.9629812 -3.9545386 -4.0239673 -4.1153736 -4.1850386 -4.2289209 -4.2526164 -4.2473378 -4.2080865 -4.1410031][-4.1462965 -4.1202478 -4.1100888 -4.0986676 -4.072793 -4.0336175 -4.02913 -4.0855207 -4.1579018 -4.2116246 -4.2416267 -4.25851 -4.2545743 -4.2236505 -4.1658216][-4.1303992 -4.1145329 -4.1217442 -4.1332316 -4.12994 -4.1048031 -4.1013474 -4.1421866 -4.1956234 -4.2347217 -4.2521477 -4.261518 -4.25711 -4.2292638 -4.1741123][-4.115941 -4.1134658 -4.1367693 -4.1638546 -4.1747084 -4.161921 -4.1596522 -4.1865449 -4.2254796 -4.2561913 -4.2681842 -4.2713585 -4.2620392 -4.2298894 -4.1714106][-4.1056414 -4.1144643 -4.1465087 -4.1800923 -4.1960645 -4.1923666 -4.1925797 -4.2102585 -4.2398376 -4.2661371 -4.2778063 -4.2782021 -4.264678 -4.2296782 -4.1732049][-4.0979896 -4.1174169 -4.1520033 -4.1837 -4.1969557 -4.1974239 -4.1964774 -4.2045121 -4.22698 -4.251492 -4.2661953 -4.2684875 -4.2544203 -4.2217369 -4.1760569][-4.0978661 -4.1202555 -4.1482906 -4.16995 -4.1747656 -4.1764593 -4.1744909 -4.1774859 -4.1967874 -4.2202172 -4.2365541 -4.2415667 -4.2311459 -4.20693 -4.1781497]]...]
INFO - root - 2017-12-06 05:48:44.634387: step 4310, loss = 2.10, batch loss = 2.04 (9.0 examples/sec; 0.886 sec/batch; 80h:43m:37s remains)
INFO - root - 2017-12-06 05:48:53.011697: step 4320, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 85h:36m:05s remains)
INFO - root - 2017-12-06 05:49:01.796243: step 4330, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 0.795 sec/batch; 72h:25m:54s remains)
INFO - root - 2017-12-06 05:49:10.778257: step 4340, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 82h:31m:42s remains)
INFO - root - 2017-12-06 05:49:19.976485: step 4350, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 85h:02m:26s remains)
INFO - root - 2017-12-06 05:49:29.011785: step 4360, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.909 sec/batch; 82h:51m:47s remains)
INFO - root - 2017-12-06 05:49:37.991516: step 4370, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 82h:52m:11s remains)
INFO - root - 2017-12-06 05:49:47.040579: step 4380, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 80h:39m:03s remains)
INFO - root - 2017-12-06 05:49:55.998910: step 4390, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.886 sec/batch; 80h:42m:49s remains)
INFO - root - 2017-12-06 05:50:04.886353: step 4400, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 79h:19m:16s remains)
2017-12-06 05:50:05.664224: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2429209 -4.2346268 -4.2339673 -4.2376904 -4.2402835 -4.2416697 -4.2394819 -4.2384372 -4.2402253 -4.2432652 -4.2472334 -4.245523 -4.2318263 -4.21503 -4.1926708][-4.2331471 -4.2283649 -4.232317 -4.2383819 -4.237637 -4.2327375 -4.2254076 -4.222784 -4.2294421 -4.2383609 -4.241508 -4.2390127 -4.2229505 -4.2033834 -4.1774492][-4.2272415 -4.225976 -4.2330985 -4.23888 -4.2338562 -4.2228446 -4.2112041 -4.2056127 -4.2159128 -4.2312565 -4.2353916 -4.2362428 -4.2229772 -4.2027555 -4.1764765][-4.2229152 -4.2251468 -4.2336297 -4.2352514 -4.2245903 -4.2050815 -4.1855536 -4.1741352 -4.1841207 -4.2068305 -4.217905 -4.2271976 -4.2231364 -4.2084084 -4.1870179][-4.2094684 -4.2162991 -4.2246575 -4.2218742 -4.2078862 -4.1809959 -4.1512318 -4.1305633 -4.1375923 -4.167901 -4.1906056 -4.2083359 -4.2151008 -4.212317 -4.2039304][-4.1929092 -4.2077107 -4.2143846 -4.206852 -4.189507 -4.1566334 -4.1140714 -4.0804472 -4.0834427 -4.1247158 -4.1622186 -4.1873074 -4.2031455 -4.2134314 -4.2178378][-4.1816525 -4.1976666 -4.1997719 -4.1869264 -4.167 -4.1300435 -4.07674 -4.0327759 -4.0345016 -4.0882778 -4.1404572 -4.1741905 -4.1954432 -4.2132936 -4.2233443][-4.1748562 -4.1848035 -4.179203 -4.1643486 -4.1459155 -4.10745 -4.046072 -3.9944074 -3.9977641 -4.0628996 -4.1298943 -4.1711183 -4.1906266 -4.2097588 -4.2218161][-4.1653161 -4.1688962 -4.1629581 -4.1521835 -4.139061 -4.102284 -4.0341349 -3.9697237 -3.9666905 -4.0393381 -4.1185741 -4.1691809 -4.1884847 -4.2053261 -4.2159925][-4.151988 -4.1571069 -4.1586351 -4.1598296 -4.1583138 -4.1294847 -4.0613685 -3.9830298 -3.9614911 -4.0254979 -4.1039867 -4.1601086 -4.1837254 -4.200069 -4.209044][-4.1354351 -4.148407 -4.1616311 -4.1735826 -4.1820469 -4.1607971 -4.0958695 -4.0104232 -3.973573 -4.0245328 -4.0986595 -4.1538167 -4.17737 -4.1919956 -4.1986513][-4.1280928 -4.1419077 -4.1573372 -4.1730189 -4.1838832 -4.1665106 -4.10254 -4.0161018 -3.9773245 -4.0243058 -4.0973234 -4.1492829 -4.1692839 -4.1793332 -4.1854734][-4.121016 -4.1265559 -4.1369848 -4.1522856 -4.1646733 -4.1518431 -4.0909004 -4.0074639 -3.970084 -4.0137939 -4.0849509 -4.1344047 -4.1532254 -4.1621118 -4.17219][-4.111784 -4.1035552 -4.1087933 -4.1263118 -4.1464138 -4.1439028 -4.0960383 -4.0230837 -3.9864316 -4.0198975 -4.0817232 -4.12703 -4.1495194 -4.1645632 -4.1768165][-4.1151524 -4.0898643 -4.0853887 -4.1039033 -4.1291952 -4.1413035 -4.1148052 -4.0604753 -4.0251584 -4.04351 -4.092978 -4.1352682 -4.1637921 -4.18624 -4.1965704]]...]
INFO - root - 2017-12-06 05:50:14.684870: step 4410, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 82h:17m:32s remains)
INFO - root - 2017-12-06 05:50:23.116042: step 4420, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 62h:03m:23s remains)
INFO - root - 2017-12-06 05:50:31.734379: step 4430, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 83h:40m:08s remains)
INFO - root - 2017-12-06 05:50:40.782243: step 4440, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 80h:45m:39s remains)
INFO - root - 2017-12-06 05:50:49.757781: step 4450, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 80h:26m:55s remains)
INFO - root - 2017-12-06 05:50:58.732855: step 4460, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.896 sec/batch; 81h:37m:56s remains)
INFO - root - 2017-12-06 05:51:07.683718: step 4470, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 78h:16m:05s remains)
INFO - root - 2017-12-06 05:51:16.732344: step 4480, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.894 sec/batch; 81h:26m:32s remains)
INFO - root - 2017-12-06 05:51:25.702766: step 4490, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 83h:21m:23s remains)
INFO - root - 2017-12-06 05:51:34.580852: step 4500, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.916 sec/batch; 83h:27m:06s remains)
2017-12-06 05:51:35.393728: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3225856 -4.3179607 -4.3151736 -4.3093734 -4.2981095 -4.2871947 -4.2740641 -4.2653942 -4.2713728 -4.2749147 -4.2760625 -4.2851124 -4.2976079 -4.3070312 -4.3078213][-4.3277597 -4.3226295 -4.3176465 -4.3060441 -4.286201 -4.2658319 -4.239028 -4.2234235 -4.2345939 -4.2424121 -4.24819 -4.2681241 -4.2885571 -4.300602 -4.3022366][-4.3312335 -4.3226695 -4.3120313 -4.2905931 -4.2595258 -4.2253232 -4.1802859 -4.1606078 -4.1877961 -4.21331 -4.228879 -4.2576575 -4.2866111 -4.3028145 -4.3053064][-4.3275275 -4.3180094 -4.3064156 -4.2769241 -4.232388 -4.1799145 -4.11468 -4.0979204 -4.1512523 -4.1984897 -4.2213025 -4.2527027 -4.2844982 -4.30571 -4.3090949][-4.316483 -4.3044271 -4.2882318 -4.2467427 -4.1867161 -4.1136575 -4.0300875 -4.0232153 -4.1063533 -4.1729941 -4.2035389 -4.2398243 -4.27581 -4.30299 -4.3089104][-4.3021016 -4.2883067 -4.2662797 -4.2113514 -4.1346922 -4.0304093 -3.922061 -3.9359941 -4.0531821 -4.1401024 -4.1837668 -4.22955 -4.2721725 -4.3022952 -4.3076315][-4.2919164 -4.2808285 -4.2550254 -4.1917329 -4.1019387 -3.9569845 -3.807708 -3.847194 -4.0047178 -4.1125388 -4.1689653 -4.222888 -4.2726254 -4.3063917 -4.3125997][-4.2896242 -4.2877464 -4.2647281 -4.2056208 -4.1178679 -3.9621696 -3.7911363 -3.8371608 -4.0080943 -4.1199083 -4.1771584 -4.2323589 -4.2829127 -4.3158479 -4.3222537][-4.288167 -4.2971706 -4.2779937 -4.23119 -4.1626067 -4.0422616 -3.9084463 -3.9378252 -4.0693526 -4.1582913 -4.2048106 -4.2541142 -4.2956338 -4.3203211 -4.3242278][-4.2808137 -4.2880344 -4.2755752 -4.244401 -4.1971874 -4.1188602 -4.0395312 -4.0625458 -4.1474757 -4.205061 -4.2353258 -4.2709432 -4.2994933 -4.31498 -4.3171844][-4.2682791 -4.2671919 -4.2582932 -4.2390518 -4.2069154 -4.157105 -4.1154766 -4.1391039 -4.1993976 -4.241262 -4.2607479 -4.2781367 -4.2934532 -4.3003154 -4.3014627][-4.2505593 -4.2470078 -4.2414703 -4.2291675 -4.2058511 -4.1753249 -4.1532736 -4.1755552 -4.2246342 -4.2602806 -4.2722812 -4.2730889 -4.2747583 -4.2744503 -4.2758822][-4.2352662 -4.2360568 -4.2341561 -4.2243438 -4.2087641 -4.1939106 -4.1868443 -4.2030568 -4.2341695 -4.256454 -4.2632852 -4.254971 -4.2514834 -4.2472281 -4.2468648][-4.2276154 -4.2337289 -4.2350469 -4.2265935 -4.2171831 -4.2117295 -4.2144184 -4.2260008 -4.2415786 -4.2501388 -4.252923 -4.2411876 -4.2316051 -4.2255173 -4.2222643][-4.2378697 -4.2501693 -4.2515054 -4.2419877 -4.23429 -4.2307062 -4.2332845 -4.2386646 -4.2440977 -4.2431746 -4.2416 -4.2297411 -4.218915 -4.2092209 -4.2028122]]...]
INFO - root - 2017-12-06 05:51:44.449830: step 4510, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 79h:57m:35s remains)
INFO - root - 2017-12-06 05:51:53.316920: step 4520, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 0.794 sec/batch; 72h:17m:56s remains)
INFO - root - 2017-12-06 05:52:02.490283: step 4530, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.953 sec/batch; 86h:49m:23s remains)
INFO - root - 2017-12-06 05:52:11.519425: step 4540, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.882 sec/batch; 80h:22m:01s remains)
INFO - root - 2017-12-06 05:52:20.358849: step 4550, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 81h:55m:56s remains)
INFO - root - 2017-12-06 05:52:29.433361: step 4560, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.877 sec/batch; 79h:53m:17s remains)
INFO - root - 2017-12-06 05:52:38.288394: step 4570, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 80h:00m:17s remains)
INFO - root - 2017-12-06 05:52:47.343321: step 4580, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 81h:44m:56s remains)
INFO - root - 2017-12-06 05:52:56.432248: step 4590, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.884 sec/batch; 80h:31m:34s remains)
INFO - root - 2017-12-06 05:53:05.301895: step 4600, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.870 sec/batch; 79h:16m:23s remains)
2017-12-06 05:53:06.097172: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3253441 -4.3223104 -4.3192477 -4.3152885 -4.3121004 -4.3108873 -4.3117375 -4.3139977 -4.3167272 -4.3159266 -4.3121781 -4.311101 -4.3141208 -4.319571 -4.3218827][-4.2935495 -4.2885833 -4.2840567 -4.278142 -4.2717957 -4.2678642 -4.269527 -4.2742796 -4.2789693 -4.2780681 -4.2715816 -4.2692609 -4.2734361 -4.2794385 -4.28324][-4.2707887 -4.2678909 -4.2630143 -4.255053 -4.2445068 -4.2359772 -4.2388487 -4.2474627 -4.2541733 -4.2516937 -4.2415948 -4.2362227 -4.239078 -4.2432375 -4.244381][-4.2517152 -4.2514143 -4.2460022 -4.2345285 -4.2200522 -4.20994 -4.2159915 -4.2248306 -4.2295384 -4.2266483 -4.2141285 -4.2065916 -4.2097578 -4.2134967 -4.2123446][-4.2244911 -4.2257423 -4.2222571 -4.2104454 -4.1955371 -4.1860828 -4.193644 -4.1964955 -4.1901484 -4.1855669 -4.1796741 -4.1793165 -4.1879406 -4.1942315 -4.1912346][-4.187243 -4.1887884 -4.1898537 -4.1799517 -4.1662278 -4.157064 -4.1600909 -4.1476278 -4.1191158 -4.1091132 -4.119597 -4.1410637 -4.1639123 -4.1780682 -4.1742473][-4.1408291 -4.1387496 -4.1441216 -4.136785 -4.1227202 -4.1120968 -4.1061316 -4.0739231 -4.0155635 -3.9928875 -4.0317287 -4.0871205 -4.1269546 -4.1506877 -4.1505861][-4.1029363 -4.0954475 -4.1015739 -4.0967298 -4.0807366 -4.0655432 -4.0478945 -3.9983974 -3.9147904 -3.8801191 -3.9531467 -4.0406952 -4.0906215 -4.1143 -4.1141319][-4.1053834 -4.0959072 -4.1015692 -4.0980463 -4.082644 -4.0662985 -4.0426669 -3.9919655 -3.9198875 -3.8956776 -3.9719279 -4.0567403 -4.0969028 -4.1069937 -4.0974264][-4.142818 -4.1374578 -4.1462131 -4.1443224 -4.1323786 -4.1165643 -4.0913382 -4.0511594 -4.0092754 -4.0029759 -4.0582957 -4.1172323 -4.1385694 -4.1296811 -4.1056075][-4.1554813 -4.1565833 -4.1724672 -4.1769071 -4.1702151 -4.1576428 -4.1353803 -4.1071029 -4.0812407 -4.0808363 -4.1153469 -4.1538973 -4.16464 -4.1475291 -4.1164985][-4.1331153 -4.1403909 -4.1636038 -4.1771126 -4.1771903 -4.1688933 -4.1479931 -4.1227016 -4.10327 -4.1070833 -4.1353259 -4.1650972 -4.1746473 -4.1611471 -4.1345029][-4.1211729 -4.1259503 -4.1504245 -4.1691251 -4.1774039 -4.1719646 -4.1466546 -4.1175928 -4.1022053 -4.1122274 -4.1412344 -4.1695452 -4.1825619 -4.17545 -4.1565571][-4.1342235 -4.1338897 -4.1533356 -4.1673794 -4.1761618 -4.1669865 -4.1382666 -4.1085615 -4.0993743 -4.1153607 -4.1459956 -4.1747212 -4.186964 -4.1847072 -4.1729054][-4.1475515 -4.1470318 -4.1583042 -4.1621389 -4.161531 -4.1463895 -4.1142926 -4.0870628 -4.0893841 -4.1136785 -4.1465588 -4.1748838 -4.1850786 -4.1801329 -4.1680617]]...]
INFO - root - 2017-12-06 05:53:15.028960: step 4610, loss = 2.03, batch loss = 1.98 (9.1 examples/sec; 0.881 sec/batch; 80h:15m:45s remains)
INFO - root - 2017-12-06 05:53:23.668500: step 4620, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 84h:04m:40s remains)
INFO - root - 2017-12-06 05:53:32.507019: step 4630, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 80h:46m:32s remains)
INFO - root - 2017-12-06 05:53:41.583198: step 4640, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 84h:45m:52s remains)
INFO - root - 2017-12-06 05:53:50.626054: step 4650, loss = 2.03, batch loss = 1.98 (8.9 examples/sec; 0.896 sec/batch; 81h:36m:31s remains)
INFO - root - 2017-12-06 05:53:59.646318: step 4660, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.878 sec/batch; 79h:56m:32s remains)
INFO - root - 2017-12-06 05:54:08.652907: step 4670, loss = 2.03, batch loss = 1.98 (8.8 examples/sec; 0.911 sec/batch; 82h:58m:51s remains)
INFO - root - 2017-12-06 05:54:17.555244: step 4680, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 78h:15m:11s remains)
INFO - root - 2017-12-06 05:54:26.518816: step 4690, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.883 sec/batch; 80h:21m:46s remains)
INFO - root - 2017-12-06 05:54:35.385681: step 4700, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 0.779 sec/batch; 70h:58m:15s remains)
2017-12-06 05:54:36.150221: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2489457 -4.2578793 -4.2749829 -4.2855392 -4.2781496 -4.2634859 -4.2587123 -4.2607555 -4.2657909 -4.2742176 -4.2696495 -4.2408924 -4.2165213 -4.224484 -4.2447643][-4.2333245 -4.2523007 -4.2797217 -4.2967091 -4.281992 -4.2590246 -4.2501397 -4.2504873 -4.2553215 -4.267252 -4.2673812 -4.2432346 -4.2202592 -4.2296643 -4.2544112][-4.1990466 -4.2328606 -4.276237 -4.3055587 -4.2890153 -4.2552238 -4.2407961 -4.2448196 -4.2545433 -4.26467 -4.2597127 -4.2365732 -4.2194781 -4.2300253 -4.2563806][-4.1733971 -4.2154922 -4.2706265 -4.3064785 -4.2855453 -4.2352662 -4.2076745 -4.2199149 -4.2425094 -4.2572775 -4.2491159 -4.2302947 -4.2222037 -4.2360649 -4.2629352][-4.1733761 -4.2163782 -4.2713556 -4.2970166 -4.2577615 -4.1784058 -4.1334658 -4.1631675 -4.2109256 -4.2380733 -4.2319603 -4.2149458 -4.212152 -4.2347312 -4.2692232][-4.1894536 -4.2277932 -4.2690797 -4.2716322 -4.2003379 -4.0811434 -4.024353 -4.0811563 -4.15635 -4.1938739 -4.1877289 -4.16964 -4.1734118 -4.2075796 -4.2550097][-4.2105336 -4.2401905 -4.264637 -4.2410016 -4.1385527 -3.9926348 -3.9307153 -4.0098634 -4.1088023 -4.1502533 -4.1354342 -4.1098881 -4.1213217 -4.1699362 -4.232655][-4.2368402 -4.2606187 -4.2719517 -4.2377338 -4.1305494 -3.9888239 -3.9319279 -4.0104675 -4.1094632 -4.1448579 -4.1209736 -4.0884795 -4.1052194 -4.1625204 -4.2322006][-4.2741714 -4.29198 -4.2940984 -4.2585115 -4.1646967 -4.0439744 -3.9947844 -4.0597239 -4.145175 -4.1751113 -4.1482463 -4.1108904 -4.1260071 -4.1840153 -4.252243][-4.3032155 -4.3154516 -4.3118291 -4.2802305 -4.2065253 -4.1082268 -4.0626378 -4.1131783 -4.1854053 -4.2108793 -4.1837091 -4.1425633 -4.1536088 -4.210803 -4.2751169][-4.3103991 -4.3194251 -4.3164778 -4.292963 -4.2394342 -4.1682415 -4.1298862 -4.1651158 -4.2242174 -4.2449131 -4.2201576 -4.1788888 -4.1815743 -4.2327194 -4.2912211][-4.3055391 -4.3124251 -4.313406 -4.30008 -4.2645922 -4.2192974 -4.1935425 -4.2166071 -4.2584529 -4.2753711 -4.255425 -4.2166533 -4.2120585 -4.2548909 -4.3050079][-4.3020482 -4.3088179 -4.3131185 -4.3068395 -4.2841239 -4.2588458 -4.2450018 -4.2601919 -4.2873855 -4.2982883 -4.2825727 -4.2523508 -4.2479606 -4.2810841 -4.3201408][-4.3106976 -4.3180184 -4.3237739 -4.3212948 -4.3059282 -4.2903171 -4.2830338 -4.2948442 -4.3115525 -4.3159542 -4.3028669 -4.2845049 -4.2839041 -4.3067203 -4.3329825][-4.320868 -4.3290873 -4.3357053 -4.3369617 -4.3274145 -4.3153968 -4.3097625 -4.3168821 -4.3270774 -4.3278031 -4.3172235 -4.3064957 -4.3072572 -4.3211508 -4.3378329]]...]
INFO - root - 2017-12-06 05:54:44.878171: step 4710, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 0.770 sec/batch; 70h:08m:54s remains)
INFO - root - 2017-12-06 05:54:53.733576: step 4720, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.901 sec/batch; 82h:00m:52s remains)
INFO - root - 2017-12-06 05:55:02.687194: step 4730, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 83h:07m:28s remains)
INFO - root - 2017-12-06 05:55:11.720017: step 4740, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 81h:07m:28s remains)
INFO - root - 2017-12-06 05:55:20.590644: step 4750, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.839 sec/batch; 76h:22m:07s remains)
INFO - root - 2017-12-06 05:55:29.451013: step 4760, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 83h:04m:00s remains)
INFO - root - 2017-12-06 05:55:38.508308: step 4770, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.889 sec/batch; 80h:54m:16s remains)
INFO - root - 2017-12-06 05:55:47.410660: step 4780, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 79h:44m:05s remains)
INFO - root - 2017-12-06 05:55:56.431355: step 4790, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 81h:08m:27s remains)
INFO - root - 2017-12-06 05:56:05.390831: step 4800, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.908 sec/batch; 82h:40m:59s remains)
2017-12-06 05:56:06.193078: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2442451 -4.2410913 -4.2396779 -4.2393494 -4.2443438 -4.250072 -4.2567339 -4.2704992 -4.2842517 -4.294044 -4.2934756 -4.2800484 -4.2688808 -4.27134 -4.2844658][-4.1989317 -4.1976261 -4.1986918 -4.1990919 -4.2029223 -4.2091837 -4.2171326 -4.2349629 -4.2555285 -4.274838 -4.2860818 -4.2844486 -4.2781763 -4.27918 -4.2900186][-4.16251 -4.1643386 -4.16634 -4.1650562 -4.1651 -4.1683836 -4.1760535 -4.1949854 -4.2199082 -4.24594 -4.2653441 -4.274065 -4.2761192 -4.2802768 -4.2913132][-4.1415858 -4.149466 -4.1529217 -4.149188 -4.1432419 -4.1406016 -4.1436691 -4.1589527 -4.1834741 -4.2129912 -4.2373533 -4.2531848 -4.2636943 -4.2734604 -4.2871122][-4.1283722 -4.1392136 -4.1403751 -4.1339855 -4.1201854 -4.1101689 -4.1072083 -4.1204257 -4.1465888 -4.1786895 -4.2054806 -4.2240362 -4.2393107 -4.2531118 -4.2686052][-4.1306763 -4.1344576 -4.12518 -4.1053119 -4.0766292 -4.053194 -4.038403 -4.0511203 -4.0922141 -4.1385527 -4.17138 -4.1922774 -4.2084455 -4.2220106 -4.2359595][-4.1533995 -4.146513 -4.1216311 -4.0792489 -4.0263028 -3.979176 -3.9390965 -3.9494178 -4.0166006 -4.0899611 -4.137094 -4.1617742 -4.1759834 -4.1871138 -4.1991038][-4.1924481 -4.1804976 -4.1415167 -4.07889 -4.0036588 -3.9291105 -3.8591359 -3.8558841 -3.9416597 -4.0400524 -4.1031723 -4.1336374 -4.1482124 -4.158679 -4.1707363][-4.230195 -4.2197795 -4.1765957 -4.1070886 -4.024179 -3.94031 -3.8604898 -3.8439071 -3.9161766 -4.0098705 -4.0742435 -4.1069355 -4.1238174 -4.1377912 -4.154573][-4.2535615 -4.2511978 -4.21665 -4.157588 -4.0840878 -4.0103965 -3.9429665 -3.9201732 -3.960578 -4.0234923 -4.0703177 -4.0951715 -4.1102166 -4.1259823 -4.1460824][-4.2590513 -4.268115 -4.2504978 -4.2098236 -4.1558838 -4.1012025 -4.0540991 -4.0326152 -4.0483871 -4.0796533 -4.1052904 -4.1187224 -4.1253662 -4.1333046 -4.1445627][-4.25411 -4.272233 -4.270586 -4.246799 -4.2111826 -4.1759424 -4.147223 -4.1341124 -4.139276 -4.1530762 -4.1646013 -4.1683183 -4.1673021 -4.16357 -4.1580062][-4.2396731 -4.2601209 -4.2679486 -4.2567034 -4.2372522 -4.2181921 -4.20666 -4.20528 -4.2102623 -4.2173724 -4.2207408 -4.2185068 -4.2141914 -4.2065191 -4.1908317][-4.2177262 -4.2317948 -4.2426548 -4.2410545 -4.2343483 -4.2285042 -4.2308655 -4.2383552 -4.2465377 -4.2531209 -4.2536588 -4.2489514 -4.2482743 -4.2458882 -4.2339454][-4.1957049 -4.2017741 -4.2108564 -4.2139068 -4.2134538 -4.2160654 -4.2284179 -4.2423773 -4.254355 -4.2634072 -4.2659659 -4.2639875 -4.2680192 -4.2715435 -4.2670832]]...]
INFO - root - 2017-12-06 05:56:14.902170: step 4810, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 78h:08m:17s remains)
INFO - root - 2017-12-06 05:56:23.731154: step 4820, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 78h:32m:14s remains)
INFO - root - 2017-12-06 05:56:32.725910: step 4830, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 84h:40m:22s remains)
INFO - root - 2017-12-06 05:56:41.615630: step 4840, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 78h:56m:22s remains)
INFO - root - 2017-12-06 05:56:50.405854: step 4850, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 82h:35m:43s remains)
INFO - root - 2017-12-06 05:56:59.404672: step 4860, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 81h:15m:49s remains)
INFO - root - 2017-12-06 05:57:08.467303: step 4870, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 82h:59m:27s remains)
INFO - root - 2017-12-06 05:57:17.448360: step 4880, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.886 sec/batch; 80h:38m:30s remains)
INFO - root - 2017-12-06 05:57:26.582164: step 4890, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.903 sec/batch; 82h:10m:18s remains)
INFO - root - 2017-12-06 05:57:35.417828: step 4900, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 79h:31m:52s remains)
2017-12-06 05:57:36.142911: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1329393 -4.1798596 -4.2227616 -4.2367997 -4.2150087 -4.1850162 -4.1762204 -4.1842551 -4.2043819 -4.237596 -4.26948 -4.2924671 -4.304563 -4.2928925 -4.26775][-4.1367455 -4.1878266 -4.2333674 -4.2491345 -4.2281475 -4.1948562 -4.1794038 -4.1846409 -4.2113857 -4.2502279 -4.2822075 -4.2988925 -4.3059244 -4.2950115 -4.2661552][-4.1260509 -4.1780534 -4.2246804 -4.2426691 -4.2225766 -4.1858807 -4.1616569 -4.1653581 -4.2003837 -4.2483158 -4.2832189 -4.2969422 -4.3008661 -4.2928195 -4.264545][-4.106812 -4.1551113 -4.2007 -4.2212973 -4.2018733 -4.1593595 -4.1223569 -4.119761 -4.1647253 -4.2261615 -4.2689118 -4.2861772 -4.2916012 -4.2832551 -4.2508969][-4.0970368 -4.1378675 -4.1801176 -4.2019119 -4.1837029 -4.134161 -4.0797715 -4.0603285 -4.1125393 -4.1900492 -4.2440929 -4.2689424 -4.2755852 -4.25755 -4.2158008][-4.11626 -4.1481781 -4.1817861 -4.1995492 -4.1857843 -4.134728 -4.0677791 -4.0266461 -4.0779042 -4.1649981 -4.2289824 -4.2611022 -4.2659283 -4.2377553 -4.1897125][-4.1547041 -4.1754856 -4.1945763 -4.20169 -4.189641 -4.1455154 -4.0871444 -4.0482845 -4.09337 -4.1705494 -4.2326975 -4.266479 -4.2689614 -4.2397771 -4.191288][-4.1876431 -4.1953583 -4.196363 -4.1884532 -4.17165 -4.1381693 -4.1034184 -4.0891976 -4.1306477 -4.1935115 -4.2465038 -4.2771344 -4.2801719 -4.2565041 -4.2148175][-4.19949 -4.1947613 -4.1827908 -4.1618295 -4.1396327 -4.1180654 -4.1103106 -4.1218925 -4.1627135 -4.2151623 -4.2582855 -4.2833042 -4.2863536 -4.270298 -4.2406816][-4.1869841 -4.1758718 -4.1601415 -4.1375675 -4.1167245 -4.10771 -4.120388 -4.148941 -4.1882648 -4.2311912 -4.2632642 -4.280921 -4.2825241 -4.2712393 -4.25424][-4.1844034 -4.1716084 -4.1564751 -4.1363134 -4.1188812 -4.1181493 -4.1416397 -4.176249 -4.2140951 -4.2491279 -4.2694468 -4.2790966 -4.2771263 -4.269515 -4.2642446][-4.1991315 -4.1855507 -4.1705503 -4.1523814 -4.1372671 -4.1407681 -4.168736 -4.2033362 -4.2368026 -4.2656908 -4.2793059 -4.2802372 -4.2756314 -4.2701635 -4.2712398][-4.2211094 -4.2098508 -4.197475 -4.1831255 -4.1706595 -4.1748142 -4.2000718 -4.2300863 -4.2553287 -4.2781286 -4.2870803 -4.2828422 -4.276566 -4.2720714 -4.2760887][-4.2415648 -4.2364149 -4.2292337 -4.2218227 -4.215107 -4.2162733 -4.2330184 -4.2529564 -4.2681141 -4.2820926 -4.2861562 -4.2799091 -4.2741914 -4.2711768 -4.2766352][-4.2543969 -4.254817 -4.2540894 -4.2548571 -4.2538624 -4.2521067 -4.2579012 -4.2651248 -4.2703023 -4.2754526 -4.274611 -4.2672343 -4.2610369 -4.2588544 -4.264967]]...]
INFO - root - 2017-12-06 05:57:45.005172: step 4910, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 79h:55m:52s remains)
INFO - root - 2017-12-06 05:57:53.926871: step 4920, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 82h:39m:55s remains)
INFO - root - 2017-12-06 05:58:02.773507: step 4930, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 80h:35m:53s remains)
INFO - root - 2017-12-06 05:58:11.743878: step 4940, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 84h:21m:11s remains)
INFO - root - 2017-12-06 05:58:20.846648: step 4950, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.895 sec/batch; 81h:24m:53s remains)
INFO - root - 2017-12-06 05:58:29.708397: step 4960, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 81h:40m:15s remains)
INFO - root - 2017-12-06 05:58:38.591294: step 4970, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 81h:39m:15s remains)
INFO - root - 2017-12-06 05:58:47.518395: step 4980, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.930 sec/batch; 84h:37m:55s remains)
INFO - root - 2017-12-06 05:58:56.323380: step 4990, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 80h:10m:27s remains)
INFO - root - 2017-12-06 05:59:05.191860: step 5000, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 85h:15m:11s remains)
2017-12-06 05:59:05.965351: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9046855 -3.923857 -3.9605038 -4.0183711 -4.0853305 -4.1407571 -4.1747055 -4.1760545 -4.1566391 -4.1300144 -4.1040206 -4.0846424 -4.0772228 -4.0740051 -4.0669117][-3.9139414 -3.9661808 -4.0243654 -4.0857687 -4.1390338 -4.1721029 -4.1810369 -4.1586776 -4.1177926 -4.08335 -4.060739 -4.0506735 -4.0520735 -4.0568547 -4.0562844][-4.0231709 -4.0814362 -4.1334538 -4.1801863 -4.2142897 -4.227767 -4.2186728 -4.1830192 -4.1340179 -4.0991373 -4.0798397 -4.076097 -4.0776939 -4.0782027 -4.0722504][-4.1519489 -4.2009788 -4.2437134 -4.2768764 -4.2921829 -4.2846727 -4.2555146 -4.2089458 -4.1608672 -4.1310048 -4.11368 -4.1101937 -4.1069546 -4.1006494 -4.0919733][-4.2582068 -4.2975717 -4.3289561 -4.3415265 -4.3281507 -4.291141 -4.2394056 -4.1839271 -4.1432209 -4.12414 -4.1137395 -4.11394 -4.1106596 -4.1107512 -4.1186476][-4.3247008 -4.3483624 -4.3539219 -4.329772 -4.2784405 -4.2116852 -4.136199 -4.0723305 -4.0487795 -4.0520077 -4.0612783 -4.0777907 -4.0924416 -4.1187053 -4.1577625][-4.3353472 -4.3321586 -4.3018689 -4.2357264 -4.1462393 -4.0533233 -3.9662976 -3.9130836 -3.9326954 -3.9883289 -4.0416136 -4.0876617 -4.123601 -4.1659913 -4.216754][-4.297184 -4.2699623 -4.2160316 -4.1302795 -4.0307622 -3.9482541 -3.8928244 -3.8855889 -3.9553759 -4.051218 -4.125761 -4.1720786 -4.1981382 -4.2250872 -4.2569][-4.256968 -4.2286444 -4.1848793 -4.1257734 -4.0654368 -4.0257597 -4.0092573 -4.0261803 -4.0918183 -4.167604 -4.2183619 -4.2382917 -4.2412748 -4.2451763 -4.2557273][-4.2479305 -4.2381992 -4.2217832 -4.1990924 -4.1750422 -4.1613317 -4.1605639 -4.1772442 -4.2141771 -4.2513061 -4.2732792 -4.2740221 -4.2605581 -4.2458873 -4.2389197][-4.238503 -4.2423239 -4.2496352 -4.253932 -4.2527804 -4.2490625 -4.249207 -4.2592869 -4.2712908 -4.2810006 -4.2865167 -4.2759295 -4.2521443 -4.2271123 -4.2099481][-4.2083669 -4.2247686 -4.2530017 -4.278482 -4.2897844 -4.2861271 -4.2796454 -4.2789812 -4.2685862 -4.2549229 -4.2468424 -4.2284017 -4.2059689 -4.1880627 -4.1731868][-4.1810369 -4.2053766 -4.2429857 -4.2729306 -4.282599 -4.2676873 -4.2474155 -4.2341065 -4.2101364 -4.1868105 -4.1781015 -4.1644368 -4.1519628 -4.1527672 -4.1513][-4.1858006 -4.2035837 -4.2307825 -4.2477436 -4.2456207 -4.2196784 -4.1938396 -4.1797323 -4.1605558 -4.1462846 -4.1494966 -4.1435714 -4.1383095 -4.1479344 -4.1516929][-4.2204008 -4.2287025 -4.2432013 -4.2506275 -4.2445049 -4.2218752 -4.2009807 -4.1920195 -4.1817784 -4.1747856 -4.178793 -4.1741977 -4.1715393 -4.1802883 -4.1826825]]...]
INFO - root - 2017-12-06 05:59:14.926617: step 5010, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 82h:30m:11s remains)
INFO - root - 2017-12-06 05:59:23.995830: step 5020, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 80h:57m:03s remains)
INFO - root - 2017-12-06 05:59:32.979408: step 5030, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 83h:17m:24s remains)
INFO - root - 2017-12-06 05:59:42.047527: step 5040, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.890 sec/batch; 80h:56m:34s remains)
INFO - root - 2017-12-06 05:59:50.987194: step 5050, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 83h:26m:59s remains)
INFO - root - 2017-12-06 05:59:59.847426: step 5060, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.862 sec/batch; 78h:23m:18s remains)
INFO - root - 2017-12-06 06:00:08.763595: step 5070, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 83h:05m:40s remains)
INFO - root - 2017-12-06 06:00:17.797783: step 5080, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 80h:46m:44s remains)
INFO - root - 2017-12-06 06:00:26.728255: step 5090, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 81h:32m:47s remains)
INFO - root - 2017-12-06 06:00:35.632409: step 5100, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 78h:36m:31s remains)
2017-12-06 06:00:36.508658: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2619319 -4.2663012 -4.2726984 -4.2751374 -4.2633948 -4.2467284 -4.2285271 -4.2113447 -4.1899633 -4.1707897 -4.1552205 -4.1508837 -4.1559081 -4.1699324 -4.1938457][-4.2641239 -4.2671957 -4.2724133 -4.2729626 -4.2644072 -4.2546725 -4.2468452 -4.2279592 -4.1960635 -4.1623397 -4.1382151 -4.1303744 -4.1408806 -4.1664209 -4.1973581][-4.2561297 -4.26281 -4.2670841 -4.2643065 -4.2599149 -4.2576013 -4.2532363 -4.2313504 -4.1939268 -4.1523786 -4.1273351 -4.1205988 -4.1391258 -4.1730533 -4.2081947][-4.2400088 -4.2507286 -4.2613282 -4.2618089 -4.2611313 -4.2605405 -4.2492394 -4.2212496 -4.1821275 -4.1452069 -4.1273232 -4.1252384 -4.1468525 -4.1846347 -4.2207627][-4.2361274 -4.2454677 -4.2563491 -4.2590861 -4.2629409 -4.2616634 -4.241097 -4.2046151 -4.1685953 -4.14064 -4.1314983 -4.1338 -4.1539955 -4.1889977 -4.2206526][-4.2266364 -4.2352767 -4.24401 -4.251123 -4.2582374 -4.2477164 -4.209764 -4.1616416 -4.1290374 -4.1179733 -4.1235037 -4.1366873 -4.1576853 -4.1856818 -4.2105384][-4.2062712 -4.2252212 -4.23425 -4.2388716 -4.2293534 -4.1974506 -4.1385837 -4.0758796 -4.0473304 -4.0623593 -4.0939674 -4.1258364 -4.1547837 -4.1771431 -4.1939349][-4.1965995 -4.2298713 -4.2384529 -4.228581 -4.1936231 -4.1323419 -4.0454569 -3.9598496 -3.9378688 -3.9880769 -4.054739 -4.1106334 -4.1480761 -4.1648636 -4.1722922][-4.2039714 -4.2412148 -4.2461286 -4.2208204 -4.1649127 -4.077549 -3.9693966 -3.8758183 -3.8737116 -3.9563491 -4.0444365 -4.1101108 -4.1483464 -4.1601586 -4.1573286][-4.2190056 -4.24645 -4.2427955 -4.2085586 -4.1487651 -4.0593648 -3.965939 -3.908649 -3.9367962 -4.0171313 -4.0868807 -4.1370339 -4.1669436 -4.1746726 -4.1657176][-4.2367887 -4.2511282 -4.2377663 -4.1962709 -4.1411424 -4.07559 -4.0250225 -4.0150108 -4.056396 -4.1132884 -4.1536455 -4.1825027 -4.2016354 -4.2059183 -4.1974068][-4.264904 -4.2637753 -4.2342448 -4.1833072 -4.1317029 -4.0925059 -4.0768075 -4.0966396 -4.1435566 -4.1871367 -4.2140269 -4.232481 -4.2453694 -4.247963 -4.2439961][-4.2908349 -4.2757449 -4.230237 -4.1656284 -4.1057987 -4.0813169 -4.090271 -4.1296811 -4.1797729 -4.217145 -4.2404466 -4.2561293 -4.268075 -4.2743115 -4.2782359][-4.291585 -4.2676539 -4.2078729 -4.1279764 -4.0649509 -4.0544944 -4.0861378 -4.1393895 -4.1892405 -4.2216754 -4.2431054 -4.2578716 -4.2710457 -4.2815928 -4.291821][-4.2777348 -4.244535 -4.1696854 -4.0797153 -4.0204678 -4.0258756 -4.0758004 -4.1397672 -4.1875091 -4.215642 -4.2350254 -4.2508831 -4.2673812 -4.2812433 -4.295105]]...]
INFO - root - 2017-12-06 06:00:45.473557: step 5110, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 81h:49m:00s remains)
INFO - root - 2017-12-06 06:00:54.438675: step 5120, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 83h:43m:12s remains)
INFO - root - 2017-12-06 06:01:03.457451: step 5130, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.906 sec/batch; 82h:22m:15s remains)
INFO - root - 2017-12-06 06:01:12.523376: step 5140, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 82h:46m:52s remains)
INFO - root - 2017-12-06 06:01:21.421170: step 5150, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 83h:26m:54s remains)
INFO - root - 2017-12-06 06:01:30.230836: step 5160, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.879 sec/batch; 79h:57m:42s remains)
INFO - root - 2017-12-06 06:01:39.376344: step 5170, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.978 sec/batch; 88h:54m:10s remains)
INFO - root - 2017-12-06 06:01:48.367390: step 5180, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 80h:13m:04s remains)
INFO - root - 2017-12-06 06:01:57.305934: step 5190, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 82h:19m:44s remains)
INFO - root - 2017-12-06 06:02:06.223756: step 5200, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 79h:50m:29s remains)
2017-12-06 06:02:07.033103: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2975907 -4.302947 -4.3033233 -4.2986054 -4.2911644 -4.28491 -4.2793889 -4.2785268 -4.2766294 -4.2775426 -4.2821145 -4.28623 -4.2893815 -4.2947369 -4.2979779][-4.2980242 -4.3064766 -4.30725 -4.2977786 -4.283185 -4.2705212 -4.2608213 -4.258842 -4.2546935 -4.2525296 -4.2548923 -4.2596869 -4.2664919 -4.2767448 -4.2863297][-4.2919259 -4.3028436 -4.3021021 -4.2854366 -4.2592196 -4.2348766 -4.2150407 -4.2057056 -4.1960526 -4.1915393 -4.1955156 -4.2057028 -4.2207232 -4.2418766 -4.2647552][-4.2819328 -4.288178 -4.2793155 -4.2517533 -4.2102156 -4.168602 -4.1325107 -4.11197 -4.0975084 -4.094789 -4.1073451 -4.1305828 -4.160059 -4.1985726 -4.2407489][-4.2678542 -4.2603617 -4.2366624 -4.1936164 -4.135951 -4.0779839 -4.0255589 -3.9962161 -3.984097 -3.9913287 -4.0195961 -4.0609212 -4.1083088 -4.1651378 -4.2245083][-4.2490134 -4.2251978 -4.18275 -4.1244216 -4.0573258 -3.989856 -3.9261913 -3.8946309 -3.8965602 -3.924233 -3.9709935 -4.0291128 -4.0912948 -4.1598763 -4.2266765][-4.2351336 -4.1987777 -4.1446414 -4.0794907 -4.0124321 -3.9469335 -3.8859608 -3.862365 -3.8786185 -3.9216192 -3.9791949 -4.0473256 -4.1158514 -4.1846347 -4.2448039][-4.2363381 -4.2002983 -4.1496983 -4.0936089 -4.0389576 -3.989141 -3.9483471 -3.9383748 -3.956821 -3.9962568 -4.0485873 -4.1086268 -4.1681786 -4.2252173 -4.2696443][-4.2487159 -4.2256041 -4.1931162 -4.1583786 -4.1220393 -4.0911884 -4.0719419 -4.0715504 -4.0831051 -4.1052752 -4.1373758 -4.17639 -4.2198887 -4.2612915 -4.2894888][-4.2617035 -4.25686 -4.245647 -4.2315817 -4.2132492 -4.1989741 -4.1920886 -4.1923265 -4.1916876 -4.19437 -4.2041092 -4.2212982 -4.2494144 -4.2792921 -4.2970943][-4.2682714 -4.2759628 -4.2797384 -4.2801905 -4.2762208 -4.2734222 -4.2698755 -4.261188 -4.2451525 -4.2323017 -4.2269998 -4.2318859 -4.25141 -4.2761664 -4.2915435][-4.2688155 -4.281477 -4.2930942 -4.3017254 -4.3050785 -4.3071442 -4.3018394 -4.2848639 -4.2583156 -4.2351193 -4.220964 -4.2186937 -4.2336111 -4.2576094 -4.276619][-4.2643967 -4.280179 -4.2942214 -4.3050585 -4.3101616 -4.311687 -4.3010921 -4.2752514 -4.2422776 -4.2152462 -4.1979027 -4.1896091 -4.2008944 -4.2277226 -4.2569642][-4.2507715 -4.2680044 -4.2840486 -4.2975745 -4.30355 -4.3008976 -4.2799606 -4.2418761 -4.2030993 -4.1750526 -4.1579723 -4.145339 -4.1565351 -4.1908536 -4.2348862][-4.2297907 -4.24612 -4.2633877 -4.278111 -4.2816038 -4.271493 -4.2413278 -4.1913195 -4.148479 -4.1221318 -4.1085739 -4.0967665 -4.1110721 -4.1543069 -4.2123833]]...]
INFO - root - 2017-12-06 06:02:15.938397: step 5210, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.934 sec/batch; 84h:55m:44s remains)
INFO - root - 2017-12-06 06:02:24.999401: step 5220, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.914 sec/batch; 83h:08m:07s remains)
INFO - root - 2017-12-06 06:02:33.913679: step 5230, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 81h:56m:59s remains)
INFO - root - 2017-12-06 06:02:42.902980: step 5240, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 82h:46m:35s remains)
INFO - root - 2017-12-06 06:02:51.902293: step 5250, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 80h:15m:43s remains)
INFO - root - 2017-12-06 06:03:00.743493: step 5260, loss = 2.09, batch loss = 2.04 (8.5 examples/sec; 0.939 sec/batch; 85h:19m:08s remains)
INFO - root - 2017-12-06 06:03:09.682166: step 5270, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 80h:33m:47s remains)
INFO - root - 2017-12-06 06:03:18.561073: step 5280, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 81h:38m:46s remains)
INFO - root - 2017-12-06 06:03:27.318241: step 5290, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 0.795 sec/batch; 72h:16m:04s remains)
INFO - root - 2017-12-06 06:03:36.172702: step 5300, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 80h:53m:01s remains)
2017-12-06 06:03:37.012201: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3136473 -4.3169322 -4.32032 -4.3195791 -4.3085146 -4.2985663 -4.2893171 -4.2822123 -4.2814231 -4.279902 -4.2810178 -4.2901421 -4.3020554 -4.3131905 -4.3224993][-4.2934833 -4.2971988 -4.3007693 -4.2981968 -4.2811069 -4.2651587 -4.2527337 -4.2445779 -4.2451653 -4.2418041 -4.2397094 -4.250246 -4.2669024 -4.2830586 -4.2952662][-4.2531548 -4.2581429 -4.2626686 -4.2586474 -4.2367563 -4.2141862 -4.1989675 -4.1946735 -4.2023735 -4.1985478 -4.1929655 -4.2033539 -4.2218552 -4.2408328 -4.2530718][-4.1969404 -4.2030382 -4.2106943 -4.207036 -4.1790652 -4.1480556 -4.1282363 -4.1317348 -4.15142 -4.1520329 -4.1457586 -4.156189 -4.1753664 -4.1952653 -4.2051816][-4.1443114 -4.1497846 -4.1602583 -4.159 -4.1259904 -4.083333 -4.0561972 -4.0657606 -4.0977058 -4.1080675 -4.1057754 -4.1170125 -4.1384773 -4.1593542 -4.1664157][-4.1089387 -4.1141577 -4.124486 -4.1222229 -4.0818181 -4.0257273 -3.9890189 -4.002584 -4.0459495 -4.0728135 -4.0816669 -4.0954075 -4.1177535 -4.1373034 -4.1421752][-4.098196 -4.10873 -4.1207385 -4.1142426 -4.0618587 -3.9861622 -3.9359543 -3.9496388 -4.0035648 -4.0492363 -4.0727925 -4.09236 -4.1145382 -4.1298876 -4.1329651][-4.11324 -4.1314187 -4.1455493 -4.13496 -4.0742154 -3.9869957 -3.9301395 -3.9437265 -4.0012918 -4.0557942 -4.0896416 -4.1110935 -4.1308451 -4.1430426 -4.1470947][-4.1233397 -4.1467333 -4.1624341 -4.1522784 -4.0978312 -4.0205483 -3.9753938 -3.9932618 -4.0414152 -4.0846176 -4.1117463 -4.1267648 -4.1406479 -4.1533589 -4.1651382][-4.1196475 -4.1462026 -4.1631818 -4.1574478 -4.1169996 -4.0587292 -4.0276427 -4.0443177 -4.0778193 -4.1043148 -4.1192322 -4.1267238 -4.1371641 -4.1525712 -4.1709032][-4.1158681 -4.1422439 -4.1618056 -4.1619816 -4.1351604 -4.0948067 -4.0730495 -4.0843573 -4.1048174 -4.1179872 -4.1257858 -4.1317444 -4.1407995 -4.1589756 -4.1802287][-4.1127181 -4.1358604 -4.1555729 -4.1616445 -4.1469536 -4.1217613 -4.1097035 -4.1204476 -4.1331658 -4.1372948 -4.1404629 -4.1462913 -4.156136 -4.1743884 -4.1929564][-4.1135087 -4.1312857 -4.1490183 -4.1590858 -4.1526518 -4.1370864 -4.1328492 -4.1460919 -4.1565313 -4.1558304 -4.1559191 -4.1610432 -4.1713905 -4.1887512 -4.2026515][-4.1205845 -4.1327353 -4.1456828 -4.15641 -4.1558003 -4.1474929 -4.1492596 -4.1650519 -4.1747918 -4.1731396 -4.1712341 -4.1752996 -4.1848292 -4.200151 -4.2114115][-4.1295013 -4.1394267 -4.1487746 -4.1600976 -4.1642685 -4.1626391 -4.1675978 -4.1815348 -4.1891766 -4.1876497 -4.1868114 -4.1922169 -4.2005486 -4.21331 -4.2234135]]...]
INFO - root - 2017-12-06 06:03:46.106156: step 5310, loss = 2.11, batch loss = 2.05 (9.2 examples/sec; 0.870 sec/batch; 79h:03m:55s remains)
INFO - root - 2017-12-06 06:03:55.108035: step 5320, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 83h:46m:06s remains)
INFO - root - 2017-12-06 06:04:04.203174: step 5330, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 82h:14m:37s remains)
INFO - root - 2017-12-06 06:04:13.158929: step 5340, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.942 sec/batch; 85h:37m:31s remains)
INFO - root - 2017-12-06 06:04:22.173621: step 5350, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 84h:29m:53s remains)
INFO - root - 2017-12-06 06:04:31.214564: step 5360, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.890 sec/batch; 80h:54m:32s remains)
INFO - root - 2017-12-06 06:04:40.139225: step 5370, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 85h:43m:23s remains)
INFO - root - 2017-12-06 06:04:49.099672: step 5380, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 82h:21m:02s remains)
INFO - root - 2017-12-06 06:04:57.872588: step 5390, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 81h:36m:02s remains)
INFO - root - 2017-12-06 06:05:06.887232: step 5400, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 82h:23m:57s remains)
2017-12-06 06:05:07.676184: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3131614 -4.3173523 -4.3212209 -4.3196397 -4.3145957 -4.3135977 -4.3149824 -4.3137808 -4.3115168 -4.3037868 -4.291748 -4.2804174 -4.2729578 -4.2723494 -4.2794857][-4.3050971 -4.3136005 -4.3240194 -4.3236656 -4.3165908 -4.315711 -4.3187418 -4.3186131 -4.3167214 -4.3061662 -4.2886372 -4.2708545 -4.2578306 -4.2539148 -4.2599444][-4.2861147 -4.2940197 -4.3064933 -4.3050222 -4.2947879 -4.2919526 -4.2951851 -4.2993288 -4.3029461 -4.2969809 -4.2771993 -4.2559018 -4.242362 -4.2393246 -4.2449079][-4.2673244 -4.2725468 -4.2809944 -4.2745051 -4.2571115 -4.2447453 -4.2405162 -4.2495761 -4.2674117 -4.2746863 -4.2640667 -4.2469969 -4.2377777 -4.2370291 -4.2426224][-4.2507448 -4.2470737 -4.2443156 -4.2281227 -4.201025 -4.1735525 -4.1519117 -4.1615334 -4.2017365 -4.2320056 -4.2394719 -4.2328048 -4.2313004 -4.2365193 -4.245821][-4.2402558 -4.2301841 -4.2178221 -4.1927 -4.1526403 -4.1024017 -4.0484352 -4.0426245 -4.1030684 -4.1619887 -4.1972661 -4.2119813 -4.2199922 -4.2328477 -4.2501612][-4.2330294 -4.2196412 -4.1991534 -4.1608396 -4.0990496 -4.0215464 -3.9311526 -3.8969245 -3.9646668 -4.0521126 -4.1195812 -4.1682792 -4.1981864 -4.2237086 -4.2481012][-4.2317581 -4.219821 -4.201755 -4.159966 -4.0881939 -4.0005784 -3.9013546 -3.8471475 -3.9035144 -3.9952846 -4.0761218 -4.1438622 -4.1862206 -4.2132459 -4.2358694][-4.2495918 -4.2500381 -4.2474685 -4.2240644 -4.1718588 -4.1077371 -4.0348077 -3.9809561 -3.9986815 -4.0495687 -4.1039705 -4.1534138 -4.1831918 -4.1983185 -4.2135096][-4.259356 -4.26606 -4.2729182 -4.2645292 -4.2401934 -4.2092371 -4.1645503 -4.1164351 -4.0994205 -4.1074924 -4.1269765 -4.1516681 -4.1644359 -4.1696482 -4.1844797][-4.2500086 -4.2577081 -4.2680173 -4.2680969 -4.2619209 -4.2559071 -4.23454 -4.2009945 -4.1740475 -4.1577725 -4.151062 -4.1544018 -4.1540828 -4.155221 -4.170908][-4.2379174 -4.2441692 -4.256804 -4.2596493 -4.2608113 -4.2687149 -4.2647905 -4.2468791 -4.2251306 -4.2020164 -4.1844864 -4.1790538 -4.1744595 -4.1771493 -4.1946392][-4.243269 -4.2465725 -4.2553291 -4.2561831 -4.2589016 -4.2734957 -4.2830005 -4.2785859 -4.2684469 -4.2510061 -4.2326331 -4.224402 -4.2184167 -4.2206068 -4.2357349][-4.2556763 -4.2561269 -4.2611074 -4.2626691 -4.2675223 -4.2827106 -4.2942233 -4.2948961 -4.2957745 -4.2896714 -4.2762494 -4.2655568 -4.257503 -4.2556677 -4.2641726][-4.2636113 -4.2611537 -4.2648292 -4.2682281 -4.2720118 -4.2816744 -4.2880459 -4.2875156 -4.2915659 -4.293241 -4.2865553 -4.2763128 -4.2687368 -4.2690988 -4.2775841]]...]
INFO - root - 2017-12-06 06:05:16.834181: step 5410, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 81h:47m:14s remains)
INFO - root - 2017-12-06 06:05:25.759146: step 5420, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 82h:19m:58s remains)
INFO - root - 2017-12-06 06:05:34.869312: step 5430, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 80h:11m:22s remains)
INFO - root - 2017-12-06 06:05:43.720914: step 5440, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 79h:59m:39s remains)
INFO - root - 2017-12-06 06:05:52.787883: step 5450, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 84h:51m:16s remains)
INFO - root - 2017-12-06 06:06:01.738909: step 5460, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.904 sec/batch; 82h:06m:59s remains)
INFO - root - 2017-12-06 06:06:10.680608: step 5470, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.905 sec/batch; 82h:12m:23s remains)
INFO - root - 2017-12-06 06:06:19.824208: step 5480, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.932 sec/batch; 84h:37m:10s remains)
INFO - root - 2017-12-06 06:06:28.753174: step 5490, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 84h:19m:30s remains)
INFO - root - 2017-12-06 06:06:37.859664: step 5500, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 82h:28m:17s remains)
2017-12-06 06:06:38.602066: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.198132 -4.1970148 -4.1965361 -4.1869593 -4.18703 -4.1932588 -4.200314 -4.209703 -4.2167563 -4.2236104 -4.2225151 -4.2197275 -4.2323809 -4.2310834 -4.2251587][-4.1802826 -4.1834612 -4.184063 -4.1767683 -4.1791439 -4.1901445 -4.195828 -4.201664 -4.2009525 -4.205246 -4.204803 -4.2011962 -4.21591 -4.2209945 -4.2205482][-4.160068 -4.1637077 -4.1640172 -4.1614509 -4.167592 -4.1817579 -4.1917415 -4.2022929 -4.2040215 -4.2046695 -4.1986604 -4.1878443 -4.2000403 -4.2119718 -4.2196245][-4.1493354 -4.1499777 -4.1459346 -4.1460705 -4.1539588 -4.1651387 -4.1787181 -4.1906676 -4.1953425 -4.1948247 -4.1862664 -4.1670966 -4.1713028 -4.1895995 -4.2093973][-4.1320963 -4.1283326 -4.1260152 -4.1253281 -4.134532 -4.1419439 -4.1476879 -4.1563125 -4.169569 -4.1756091 -4.1709485 -4.1447244 -4.1374764 -4.1528535 -4.1781492][-4.1179771 -4.1009026 -4.0877752 -4.0838213 -4.0900211 -4.0788283 -4.0549555 -4.0581388 -4.0994759 -4.1323905 -4.1444297 -4.1200962 -4.1041837 -4.1152482 -4.13614][-4.1038957 -4.0800605 -4.0505867 -4.0313764 -4.0180531 -3.9641092 -3.8783391 -3.8598132 -3.9573848 -4.0506434 -4.0971866 -4.0880365 -4.0808396 -4.0975723 -4.1190958][-4.0843153 -4.0610924 -4.0272875 -3.9942136 -3.9670639 -3.8851995 -3.7430284 -3.6788042 -3.821528 -3.9673018 -4.0406446 -4.0483208 -4.0612316 -4.0973263 -4.123261][-4.0860891 -4.0709686 -4.056108 -4.0343108 -4.0225215 -3.9646645 -3.8513432 -3.782954 -3.878561 -3.9993165 -4.0552673 -4.0687308 -4.0907688 -4.1303368 -4.15578][-4.0897903 -4.0836997 -4.09102 -4.09358 -4.1027651 -4.082016 -4.0244727 -3.9829905 -4.0268083 -4.0917163 -4.1195755 -4.1307106 -4.1495571 -4.1817107 -4.1991925][-4.1017108 -4.0996771 -4.1208878 -4.144084 -4.1648145 -4.170475 -4.1509972 -4.1257253 -4.1352224 -4.1577086 -4.1604629 -4.1590447 -4.1723757 -4.2023988 -4.2188616][-4.1294994 -4.13323 -4.1628017 -4.1906023 -4.213336 -4.2246838 -4.2204447 -4.2025771 -4.1935768 -4.1926346 -4.17326 -4.1469121 -4.1462092 -4.1806912 -4.2064419][-4.1507621 -4.1539512 -4.1835346 -4.2032847 -4.2189059 -4.2299194 -4.2312632 -4.2208762 -4.2054176 -4.1891732 -4.1569195 -4.1125054 -4.1041384 -4.1472406 -4.1861272][-4.1604776 -4.164536 -4.1916461 -4.2064633 -4.2176223 -4.2303848 -4.2359228 -4.2304959 -4.2181845 -4.2014427 -4.174448 -4.1415949 -4.1410666 -4.1866889 -4.2195778][-4.1836481 -4.18343 -4.1984444 -4.2077727 -4.2147837 -4.2253809 -4.2332616 -4.2332835 -4.2278261 -4.2196913 -4.2074471 -4.1960149 -4.2044406 -4.2401161 -4.2623749]]...]
INFO - root - 2017-12-06 06:06:47.559227: step 5510, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.889 sec/batch; 80h:43m:05s remains)
INFO - root - 2017-12-06 06:06:56.634387: step 5520, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 80h:07m:25s remains)
INFO - root - 2017-12-06 06:07:05.650998: step 5530, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.896 sec/batch; 81h:23m:30s remains)
INFO - root - 2017-12-06 06:07:14.676059: step 5540, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 82h:10m:31s remains)
INFO - root - 2017-12-06 06:07:23.748594: step 5550, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 83h:33m:38s remains)
INFO - root - 2017-12-06 06:07:32.624077: step 5560, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 76h:12m:31s remains)
INFO - root - 2017-12-06 06:07:41.403196: step 5570, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 78h:01m:38s remains)
INFO - root - 2017-12-06 06:07:50.268892: step 5580, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.891 sec/batch; 80h:56m:13s remains)
INFO - root - 2017-12-06 06:07:59.094690: step 5590, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.894 sec/batch; 81h:10m:16s remains)
INFO - root - 2017-12-06 06:08:07.992297: step 5600, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 81h:29m:59s remains)
2017-12-06 06:08:08.824685: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2317295 -4.2311077 -4.2327337 -4.2424455 -4.2543731 -4.2609649 -4.2611938 -4.2565441 -4.2467589 -4.2320304 -4.2222056 -4.2071323 -4.2018247 -4.2202396 -4.2367783][-4.2814908 -4.2739334 -4.2676506 -4.2711582 -4.2792335 -4.2820125 -4.2749228 -4.2650175 -4.2481847 -4.2253022 -4.2074656 -4.1946917 -4.1903634 -4.2101226 -4.2331071][-4.2961907 -4.28552 -4.2759142 -4.2744761 -4.2794614 -4.2805681 -4.270484 -4.2555041 -4.2325273 -4.2017469 -4.175889 -4.1658363 -4.1674166 -4.1881166 -4.2156463][-4.2968426 -4.2835941 -4.272459 -4.2616572 -4.2560282 -4.2538896 -4.24347 -4.2273989 -4.2090321 -4.1841536 -4.1636906 -4.1601272 -4.1660466 -4.1794548 -4.1950226][-4.2725039 -4.2577891 -4.2500877 -4.2361646 -4.2185082 -4.20464 -4.1840405 -4.1720624 -4.17684 -4.17927 -4.1785092 -4.1819835 -4.1899004 -4.1947336 -4.1910625][-4.1957579 -4.1757 -4.1790681 -4.173749 -4.1465983 -4.1076708 -4.0582714 -4.0432982 -4.0795484 -4.1200361 -4.1482205 -4.1723318 -4.1886239 -4.1960797 -4.1933961][-4.0729561 -4.0372863 -4.0485368 -4.0528431 -4.0173016 -3.9510367 -3.8617327 -3.832058 -3.9098692 -3.9972425 -4.0575461 -4.1033039 -4.1344318 -4.1584654 -4.1763558][-4.0027328 -3.9571805 -3.9621551 -3.9632406 -3.9203205 -3.8392506 -3.7176194 -3.6631482 -3.7698307 -3.8892472 -3.9713225 -4.0323267 -4.0720243 -4.1089568 -4.1501269][-4.0716186 -4.0337133 -4.0346556 -4.0373392 -4.0087128 -3.9587827 -3.8769124 -3.8356977 -3.8992093 -3.9804926 -4.0413284 -4.084712 -4.1079822 -4.1294851 -4.1599846][-4.1732464 -4.1524682 -4.1504602 -4.1511703 -4.1368165 -4.1135631 -4.0742121 -4.0505896 -4.0783091 -4.1173964 -4.1456857 -4.161869 -4.1659303 -4.1722908 -4.1829472][-4.2268376 -4.2224278 -4.2198329 -4.2156296 -4.2040148 -4.1903477 -4.1717873 -4.1587925 -4.1712804 -4.1840687 -4.1878376 -4.1865425 -4.1836543 -4.1855493 -4.1876626][-4.2025127 -4.2115045 -4.2164516 -4.2174897 -4.2134233 -4.2082829 -4.2016587 -4.1961422 -4.2005944 -4.1962748 -4.1846075 -4.169477 -4.1667709 -4.1763635 -4.1825185][-4.1632147 -4.1768904 -4.1874318 -4.192287 -4.1926203 -4.1921945 -4.1908851 -4.1898346 -4.1898685 -4.1771855 -4.1575341 -4.1352611 -4.1341367 -4.154664 -4.1710196][-4.1871881 -4.1943569 -4.2004747 -4.203805 -4.2052708 -4.2055025 -4.2049203 -4.2049694 -4.2038274 -4.1914539 -4.1745939 -4.1545696 -4.1549172 -4.1796627 -4.1998267][-4.264492 -4.26617 -4.2662892 -4.26734 -4.2679834 -4.2677655 -4.2673016 -4.2674193 -4.2663574 -4.2591853 -4.25052 -4.2380838 -4.2344923 -4.2508125 -4.2670255]]...]
INFO - root - 2017-12-06 06:08:17.861288: step 5610, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.890 sec/batch; 80h:46m:12s remains)
INFO - root - 2017-12-06 06:08:26.776328: step 5620, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.936 sec/batch; 84h:57m:31s remains)
INFO - root - 2017-12-06 06:08:35.891536: step 5630, loss = 2.10, batch loss = 2.05 (8.8 examples/sec; 0.910 sec/batch; 82h:35m:36s remains)
INFO - root - 2017-12-06 06:08:44.930560: step 5640, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 83h:52m:21s remains)
INFO - root - 2017-12-06 06:08:53.926204: step 5650, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 80h:59m:53s remains)
INFO - root - 2017-12-06 06:09:02.941599: step 5660, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 84h:17m:06s remains)
INFO - root - 2017-12-06 06:09:11.847325: step 5670, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.895 sec/batch; 81h:16m:09s remains)
INFO - root - 2017-12-06 06:09:20.855921: step 5680, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 83h:02m:07s remains)
INFO - root - 2017-12-06 06:09:29.776715: step 5690, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 80h:24m:40s remains)
INFO - root - 2017-12-06 06:09:38.728828: step 5700, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 83h:23m:52s remains)
2017-12-06 06:09:39.521397: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2429824 -4.2213435 -4.1911235 -4.1645555 -4.1485887 -4.1439047 -4.1353817 -4.1235275 -4.1154037 -4.1134725 -4.1095166 -4.1076832 -4.1020136 -4.0861235 -4.0714593][-4.2410083 -4.2138686 -4.1747961 -4.135921 -4.1085753 -4.0979972 -4.0891328 -4.0773149 -4.0635924 -4.0547013 -4.0481906 -4.0492806 -4.0442543 -4.0257945 -4.0110817][-4.2433743 -4.2127194 -4.1679077 -4.119792 -4.085237 -4.0718088 -4.0674324 -4.0573473 -4.0416026 -4.0281453 -4.0207653 -4.0204697 -4.0132861 -3.9965019 -3.9871585][-4.2461796 -4.2163105 -4.1729054 -4.1272731 -4.0952744 -4.0840616 -4.0858827 -4.0805135 -4.0667434 -4.0548778 -4.0505781 -4.0464263 -4.0338755 -4.016602 -4.00781][-4.2487226 -4.2257318 -4.193202 -4.1626606 -4.1431494 -4.1392 -4.1469221 -4.1469188 -4.1372442 -4.1294088 -4.1251321 -4.1158166 -4.0996952 -4.08115 -4.0702419][-4.2497969 -4.2345238 -4.2137508 -4.1959958 -4.1871166 -4.1884985 -4.2002449 -4.206285 -4.2049007 -4.2010279 -4.1952314 -4.1853671 -4.1714354 -4.15606 -4.145977][-4.2476692 -4.2363925 -4.2205558 -4.2060647 -4.198802 -4.19974 -4.212471 -4.2228808 -4.2281017 -4.2290072 -4.226263 -4.2234445 -4.218545 -4.2117043 -4.2071176][-4.2434945 -4.2311988 -4.2129054 -4.1945705 -4.1841283 -4.1828294 -4.1953011 -4.206346 -4.2144232 -4.2174077 -4.2187376 -4.2261143 -4.2330828 -4.2374849 -4.2409048][-4.2390137 -4.2221079 -4.1980352 -4.1735282 -4.1584425 -4.1554818 -4.1668496 -4.174509 -4.1803908 -4.1836042 -4.189301 -4.2058535 -4.2241836 -4.2387609 -4.2493081][-4.234272 -4.2136116 -4.1855087 -4.1573248 -4.139472 -4.1371088 -4.1466765 -4.1491981 -4.1503172 -4.1543751 -4.16383 -4.1856976 -4.2107735 -4.2316136 -4.2461352][-4.2308865 -4.2098818 -4.1833277 -4.1552606 -4.1353316 -4.1324568 -4.1412659 -4.1414609 -4.1409197 -4.1462431 -4.1572361 -4.1797047 -4.2066078 -4.2295885 -4.2449379][-4.2301207 -4.2135458 -4.1922941 -4.1673341 -4.1472793 -4.1439905 -4.1531863 -4.1561775 -4.1568751 -4.161963 -4.1713381 -4.1909704 -4.2158337 -4.2364149 -4.2492013][-4.2357821 -4.2239504 -4.2072763 -4.1866198 -4.1698508 -4.1678638 -4.1787415 -4.1871147 -4.1900463 -4.1941986 -4.2013731 -4.2161522 -4.2351084 -4.2497659 -4.2573934][-4.2459874 -4.2375784 -4.2239685 -4.2075605 -4.1957612 -4.1961207 -4.20925 -4.2219315 -4.2271953 -4.2313137 -4.2359133 -4.2452278 -4.256783 -4.2647619 -4.2668271][-4.2563519 -4.250268 -4.2391863 -4.2269855 -4.2201872 -4.2232485 -4.2373381 -4.2512813 -4.2571735 -4.2600508 -4.2624121 -4.2681055 -4.2743926 -4.2768579 -4.2747803]]...]
INFO - root - 2017-12-06 06:09:48.470751: step 5710, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.884 sec/batch; 80h:14m:38s remains)
INFO - root - 2017-12-06 06:09:57.387160: step 5720, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 83h:01m:20s remains)
INFO - root - 2017-12-06 06:10:06.374015: step 5730, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 83h:41m:57s remains)
INFO - root - 2017-12-06 06:10:15.537211: step 5740, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 80h:53m:00s remains)
INFO - root - 2017-12-06 06:10:24.565584: step 5750, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 75h:42m:28s remains)
INFO - root - 2017-12-06 06:10:33.444266: step 5760, loss = 2.11, batch loss = 2.05 (9.3 examples/sec; 0.864 sec/batch; 78h:23m:19s remains)
INFO - root - 2017-12-06 06:10:42.343568: step 5770, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 79h:41m:36s remains)
INFO - root - 2017-12-06 06:10:51.405074: step 5780, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.931 sec/batch; 84h:29m:20s remains)
INFO - root - 2017-12-06 06:11:00.397379: step 5790, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 77h:56m:44s remains)
INFO - root - 2017-12-06 06:11:09.351464: step 5800, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 81h:25m:09s remains)
2017-12-06 06:11:10.163044: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1443081 -4.1590638 -4.1755581 -4.1944952 -4.2207522 -4.2411237 -4.2518315 -4.247942 -4.2298951 -4.1972256 -4.1617079 -4.1524849 -4.165164 -4.1801338 -4.18861][-4.1302857 -4.1687078 -4.2021589 -4.2220674 -4.2375617 -4.2440615 -4.2429757 -4.2333894 -4.2152553 -4.1934896 -4.1697216 -4.1614342 -4.1742949 -4.1894746 -4.1966891][-4.1345358 -4.1801691 -4.2175646 -4.227694 -4.222837 -4.2120161 -4.2087913 -4.2012043 -4.1907344 -4.1832519 -4.1737847 -4.1709046 -4.1830349 -4.1932607 -4.1942444][-4.1354227 -4.1701703 -4.1916065 -4.1785121 -4.1479592 -4.1274934 -4.1324439 -4.1376104 -4.1392989 -4.1465073 -4.1566782 -4.1632686 -4.1716528 -4.1691265 -4.1601691][-4.1091566 -4.1242456 -4.1229429 -4.0798922 -4.0197659 -3.9903774 -4.0116482 -4.0420022 -4.0587492 -4.0810347 -4.1157064 -4.1428065 -4.1522779 -4.1378789 -4.1140556][-4.104671 -4.0911684 -4.0610557 -3.9836864 -3.8835397 -3.824358 -3.8560262 -3.9267268 -3.9733598 -4.0113978 -4.074708 -4.1294742 -4.1514511 -4.1408243 -4.1114678][-4.1086645 -4.0690875 -4.0147166 -3.9230714 -3.8009665 -3.7024615 -3.7166982 -3.8203945 -3.9010429 -3.9519196 -4.03437 -4.1114187 -4.1484718 -4.1506448 -4.1296506][-4.1360269 -4.0912166 -4.0349994 -3.9655058 -3.8775504 -3.7911608 -3.7732637 -3.8541512 -3.9260788 -3.9673114 -4.0363226 -4.1049819 -4.1406846 -4.1522455 -4.1482086][-4.1775951 -4.1451125 -4.1041427 -4.0619612 -4.0129104 -3.9621563 -3.9390435 -3.9821455 -4.0323586 -4.062078 -4.104917 -4.14538 -4.1643405 -4.1699157 -4.1679783][-4.2075977 -4.1884289 -4.1658173 -4.1423798 -4.1210113 -4.1025066 -4.0928049 -4.1172056 -4.1516676 -4.1718125 -4.1961432 -4.2163048 -4.222672 -4.2152057 -4.2005997][-4.2372417 -4.2244625 -4.212636 -4.2007723 -4.1928997 -4.1922364 -4.191102 -4.20476 -4.2276082 -4.24294 -4.2588949 -4.2721472 -4.2762594 -4.2624283 -4.2390957][-4.2637782 -4.250567 -4.2439814 -4.2410617 -4.2389612 -4.2450538 -4.24819 -4.2534819 -4.2646132 -4.2742381 -4.2870107 -4.295651 -4.3015132 -4.2929354 -4.2733016][-4.2796206 -4.270483 -4.2668042 -4.2689891 -4.2714334 -4.2781653 -4.2826257 -4.2856259 -4.2895823 -4.2921438 -4.2989187 -4.3043013 -4.3095522 -4.3065753 -4.2963195][-4.2844067 -4.2797709 -4.2782092 -4.2811689 -4.2858343 -4.2926936 -4.299705 -4.3032255 -4.3025503 -4.3014088 -4.303421 -4.3042626 -4.3064694 -4.3066993 -4.3028975][-4.2881975 -4.2846231 -4.2832794 -4.2838545 -4.2859535 -4.2894106 -4.2946444 -4.298347 -4.2987595 -4.2980351 -4.2983003 -4.29923 -4.3002758 -4.3003793 -4.2985539]]...]
INFO - root - 2017-12-06 06:11:19.227787: step 5810, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 84h:00m:07s remains)
INFO - root - 2017-12-06 06:11:28.272013: step 5820, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.924 sec/batch; 83h:52m:37s remains)
INFO - root - 2017-12-06 06:11:37.447819: step 5830, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 82h:22m:15s remains)
INFO - root - 2017-12-06 06:11:46.407288: step 5840, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.909 sec/batch; 82h:28m:26s remains)
INFO - root - 2017-12-06 06:11:55.210273: step 5850, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 83h:17m:29s remains)
INFO - root - 2017-12-06 06:12:04.196646: step 5860, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.947 sec/batch; 85h:53m:13s remains)
INFO - root - 2017-12-06 06:12:13.316767: step 5870, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 82h:13m:29s remains)
INFO - root - 2017-12-06 06:12:22.377294: step 5880, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 80h:49m:29s remains)
INFO - root - 2017-12-06 06:12:31.436487: step 5890, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 83h:59m:34s remains)
INFO - root - 2017-12-06 06:12:40.554914: step 5900, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 80h:59m:50s remains)
2017-12-06 06:12:41.328900: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2362733 -4.2452602 -4.2664261 -4.2810664 -4.2877884 -4.2844477 -4.2703691 -4.257442 -4.2565227 -4.26445 -4.269124 -4.2671938 -4.2632866 -4.2546487 -4.2409205][-4.2140012 -4.2217054 -4.244297 -4.2595968 -4.269979 -4.268528 -4.2541971 -4.239501 -4.2385612 -4.2454929 -4.2534866 -4.2505403 -4.2442808 -4.2309012 -4.2129607][-4.2013206 -4.2098446 -4.2341027 -4.2461882 -4.2536163 -4.2491393 -4.2293277 -4.2155704 -4.215744 -4.2242212 -4.2353597 -4.2346873 -4.2281728 -4.2097883 -4.18273][-4.190166 -4.2002869 -4.2253275 -4.2372613 -4.2399364 -4.2297091 -4.2018523 -4.1898017 -4.1965737 -4.209693 -4.2202249 -4.2181578 -4.2127929 -4.1918159 -4.1551847][-4.172955 -4.1847887 -4.2094612 -4.2182031 -4.2121816 -4.19279 -4.1569662 -4.1420302 -4.1573391 -4.1781378 -4.192657 -4.1907086 -4.1925139 -4.1728668 -4.1309638][-4.1625004 -4.1765356 -4.1972804 -4.1998558 -4.1816754 -4.1445503 -4.0934119 -4.0702405 -4.0972018 -4.1321874 -4.1571145 -4.1627684 -4.1780481 -4.1665826 -4.1263623][-4.1679645 -4.1826396 -4.1938524 -4.1844029 -4.147985 -4.0881763 -4.0122628 -3.9829085 -4.0241246 -4.0793581 -4.1258588 -4.150631 -4.1766233 -4.1752481 -4.1420536][-4.1851029 -4.1988659 -4.1973138 -4.1703525 -4.107944 -4.0191727 -3.9184287 -3.8865466 -3.9445336 -4.0200162 -4.0881243 -4.1361084 -4.1731253 -4.1783328 -4.1522965][-4.20009 -4.2173958 -4.2157407 -4.1870461 -4.123167 -4.0217419 -3.9082286 -3.8713758 -3.9269192 -4.002625 -4.0737958 -4.1353207 -4.1785383 -4.1791592 -4.1534882][-4.1868973 -4.2105765 -4.2209258 -4.212049 -4.1740923 -4.0915136 -3.9923334 -3.9521468 -3.9871912 -4.0371161 -4.0833921 -4.1414561 -4.1820426 -4.1707311 -4.1373887][-4.1715956 -4.1940703 -4.217433 -4.2314639 -4.2203007 -4.1696091 -4.0977092 -4.054812 -4.0675788 -4.0919161 -4.1202307 -4.16805 -4.201056 -4.1833353 -4.1404061][-4.1656713 -4.184073 -4.2150979 -4.2462358 -4.2595291 -4.2425718 -4.1985626 -4.1596661 -4.1546063 -4.16301 -4.1789894 -4.2109175 -4.2317505 -4.2098584 -4.1596737][-4.1851707 -4.2012148 -4.23348 -4.2679048 -4.2909994 -4.2944512 -4.27134 -4.2403808 -4.2319293 -4.2368994 -4.2435694 -4.2592621 -4.2696218 -4.246634 -4.1963377][-4.2259412 -4.2466044 -4.2769318 -4.3013577 -4.3160739 -4.320684 -4.3073158 -4.2875271 -4.28508 -4.2956586 -4.2996874 -4.3055682 -4.3068933 -4.2823024 -4.2371411][-4.2736874 -4.2987695 -4.3255334 -4.3378549 -4.33871 -4.3354983 -4.3233061 -4.3108497 -4.3134551 -4.3285952 -4.3349533 -4.3366961 -4.3309574 -4.3085036 -4.2727461]]...]
INFO - root - 2017-12-06 06:12:50.402609: step 5910, loss = 2.04, batch loss = 1.99 (8.4 examples/sec; 0.950 sec/batch; 86h:10m:16s remains)
INFO - root - 2017-12-06 06:12:59.386702: step 5920, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 80h:19m:38s remains)
INFO - root - 2017-12-06 06:13:08.423976: step 5930, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.936 sec/batch; 84h:56m:37s remains)
INFO - root - 2017-12-06 06:13:17.360474: step 5940, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.827 sec/batch; 74h:59m:09s remains)
INFO - root - 2017-12-06 06:13:26.100002: step 5950, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 81h:16m:06s remains)
INFO - root - 2017-12-06 06:13:35.247705: step 5960, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.961 sec/batch; 87h:09m:12s remains)
INFO - root - 2017-12-06 06:13:44.155525: step 5970, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 81h:35m:53s remains)
INFO - root - 2017-12-06 06:13:53.232772: step 5980, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 82h:57m:38s remains)
INFO - root - 2017-12-06 06:14:02.283959: step 5990, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 81h:17m:32s remains)
INFO - root - 2017-12-06 06:14:11.380298: step 6000, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 82h:29m:48s remains)
2017-12-06 06:14:12.180185: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1464362 -4.1514287 -4.1769581 -4.2082825 -4.2277093 -4.2333174 -4.2395015 -4.2516966 -4.263783 -4.2670207 -4.2635403 -4.2452154 -4.2165694 -4.1800871 -4.1503963][-4.1194019 -4.1229239 -4.1503119 -4.1904421 -4.2187753 -4.2301893 -4.2404404 -4.24988 -4.2582164 -4.2612944 -4.2550511 -4.2335081 -4.1970148 -4.1537433 -4.1267943][-4.1254115 -4.1307955 -4.1518879 -4.1855049 -4.20878 -4.2193675 -4.2293482 -4.2365866 -4.2416883 -4.2484703 -4.2452545 -4.2251148 -4.1848259 -4.1398115 -4.1155334][-4.1488376 -4.1511354 -4.1613555 -4.1817908 -4.1929073 -4.1946735 -4.2057281 -4.2196655 -4.227973 -4.2403908 -4.2414241 -4.2230396 -4.1851315 -4.1438537 -4.1236987][-4.1829343 -4.1827817 -4.1806035 -4.1859736 -4.1846967 -4.1723943 -4.1791115 -4.1977363 -4.2088947 -4.2258844 -4.2333174 -4.2246881 -4.1941495 -4.1558461 -4.1356106][-4.2206421 -4.2222643 -4.2104611 -4.2004948 -4.1821136 -4.1544795 -4.1521044 -4.1688604 -4.1761665 -4.1895 -4.2039824 -4.2147336 -4.2058153 -4.1803408 -4.1592903][-4.2370272 -4.2372885 -4.2220049 -4.2080779 -4.1811562 -4.1436362 -4.1345448 -4.1475945 -4.1422334 -4.1335893 -4.142868 -4.1742616 -4.195199 -4.1945796 -4.1869693][-4.2184024 -4.22034 -4.2152977 -4.2165852 -4.2011952 -4.1656537 -4.1457086 -4.1382709 -4.1081367 -4.0677061 -4.0614839 -4.1081328 -4.159874 -4.1911087 -4.2080917][-4.1743541 -4.1871243 -4.2015214 -4.2287283 -4.2341061 -4.2066531 -4.1737695 -4.1398592 -4.0740643 -3.9947593 -3.9718094 -4.0339255 -4.113255 -4.1736331 -4.2142987][-4.1232948 -4.1561494 -4.1969848 -4.2495346 -4.2724972 -4.2510014 -4.2103758 -4.15765 -4.0631208 -3.9574516 -3.9203753 -3.9854522 -4.077229 -4.1530523 -4.207262][-4.0857577 -4.1420159 -4.2050009 -4.273356 -4.3102827 -4.2954216 -4.2503514 -4.1838665 -4.07776 -3.966815 -3.9260244 -3.9847682 -4.0738306 -4.1457915 -4.1980772][-4.05422 -4.1320362 -4.2072873 -4.2782674 -4.3223643 -4.3145742 -4.2686687 -4.2003827 -4.1063862 -4.0125594 -3.9758332 -4.021091 -4.0942769 -4.1537485 -4.1967821][-4.0185227 -4.106812 -4.1765542 -4.243391 -4.2928119 -4.2950191 -4.2564316 -4.1994343 -4.1318913 -4.068059 -4.0411787 -4.0695333 -4.1181345 -4.1596632 -4.193531][-3.9938753 -4.0701451 -4.1224303 -4.1855593 -4.2432451 -4.2583313 -4.2336044 -4.1910644 -4.146656 -4.1116762 -4.0982327 -4.1108303 -4.1322851 -4.1553764 -4.1806664][-4.0002666 -4.0556779 -4.0916443 -4.14604 -4.2037625 -4.2245111 -4.2113552 -4.184711 -4.1601157 -4.1458182 -4.1430664 -4.1442823 -4.1479535 -4.158278 -4.1755724]]...]
INFO - root - 2017-12-06 06:14:21.265113: step 6010, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.940 sec/batch; 85h:12m:18s remains)
INFO - root - 2017-12-06 06:14:30.333048: step 6020, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 80h:36m:07s remains)
INFO - root - 2017-12-06 06:14:39.331256: step 6030, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 83h:34m:42s remains)
INFO - root - 2017-12-06 06:14:47.982102: step 6040, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 78h:31m:31s remains)
INFO - root - 2017-12-06 06:14:56.734816: step 6050, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 80h:26m:35s remains)
INFO - root - 2017-12-06 06:15:05.800196: step 6060, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.925 sec/batch; 83h:53m:45s remains)
INFO - root - 2017-12-06 06:15:14.873567: step 6070, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 82h:24m:11s remains)
INFO - root - 2017-12-06 06:15:23.763290: step 6080, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 80h:47m:37s remains)
INFO - root - 2017-12-06 06:15:32.771235: step 6090, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.897 sec/batch; 81h:17m:14s remains)
INFO - root - 2017-12-06 06:15:41.752543: step 6100, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 79h:09m:39s remains)
2017-12-06 06:15:42.555952: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3163438 -4.3226023 -4.3310761 -4.3350353 -4.3296914 -4.3185935 -4.3113203 -4.3092051 -4.3107667 -4.3187861 -4.32832 -4.3326507 -4.3319068 -4.3273635 -4.3201318][-4.3078332 -4.3117504 -4.3191228 -4.3222084 -4.3130169 -4.296371 -4.2844086 -4.280508 -4.2811565 -4.2912173 -4.3086171 -4.3192992 -4.3183546 -4.3110037 -4.3043618][-4.3030334 -4.3037438 -4.3079605 -4.3085051 -4.2957249 -4.2721286 -4.2542658 -4.252882 -4.2562747 -4.2653646 -4.2865062 -4.3030334 -4.3016891 -4.291431 -4.2869463][-4.302321 -4.2995176 -4.2979445 -4.2945294 -4.2781725 -4.2474346 -4.2229548 -4.2223554 -4.2289395 -4.2371783 -4.2590752 -4.2828493 -4.2841811 -4.2738485 -4.270381][-4.2982063 -4.2906933 -4.2840066 -4.27834 -4.2626534 -4.2302861 -4.1996856 -4.1952758 -4.2045107 -4.2145534 -4.2298918 -4.2521696 -4.2524543 -4.24065 -4.240335][-4.2925997 -4.2811713 -4.271523 -4.2667761 -4.2539048 -4.2206187 -4.1856256 -4.1800046 -4.1962867 -4.2122498 -4.2200789 -4.2325106 -4.2238264 -4.2050915 -4.2036009][-4.2901282 -4.2755427 -4.2625308 -4.2547927 -4.2391524 -4.2048526 -4.1690211 -4.16634 -4.1885471 -4.2120571 -4.2253962 -4.2335587 -4.2161188 -4.1869316 -4.1797915][-4.2894878 -4.2713938 -4.2545834 -4.2410398 -4.2197485 -4.1816626 -4.1453733 -4.1437907 -4.1654167 -4.1957836 -4.2237864 -4.2365 -4.2179394 -4.1887918 -4.1770759][-4.2929578 -4.2756314 -4.2581449 -4.2381744 -4.2088943 -4.1674938 -4.1298981 -4.1220722 -4.1381016 -4.1758142 -4.2180252 -4.2359481 -4.2250891 -4.2072368 -4.1978197][-4.2994509 -4.2891636 -4.2740927 -4.2510076 -4.2145953 -4.1685 -4.1298661 -4.1171312 -4.129745 -4.167851 -4.2118378 -4.2336416 -4.2325354 -4.2249961 -4.2183928][-4.3002725 -4.2951241 -4.2860842 -4.2638984 -4.2267408 -4.1798387 -4.1387873 -4.1224837 -4.1357832 -4.1701069 -4.2088027 -4.2313023 -4.2364435 -4.2334285 -4.2269573][-4.2962952 -4.2907786 -4.2843442 -4.2638721 -4.2320161 -4.194447 -4.1586604 -4.1458449 -4.1610246 -4.1914034 -4.2223625 -4.242425 -4.2497215 -4.2485714 -4.2439427][-4.2956276 -4.2888875 -4.2846026 -4.2696295 -4.2498951 -4.2280993 -4.2040405 -4.1949534 -4.2066545 -4.2282753 -4.2482548 -4.2607174 -4.2646527 -4.2641544 -4.2618165][-4.2959428 -4.28981 -4.2902946 -4.2839785 -4.2765307 -4.2683554 -4.2570987 -4.2522335 -4.2593646 -4.2707057 -4.2791882 -4.2833409 -4.2839584 -4.2841353 -4.2845836][-4.2972751 -4.2910881 -4.2925525 -4.2908182 -4.2899632 -4.2903357 -4.2887039 -4.2882719 -4.2920575 -4.2953749 -4.2968469 -4.29719 -4.2966375 -4.2964621 -4.2966719]]...]
INFO - root - 2017-12-06 06:15:51.511205: step 6110, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 79h:58m:45s remains)
INFO - root - 2017-12-06 06:16:00.569903: step 6120, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.914 sec/batch; 82h:53m:21s remains)
INFO - root - 2017-12-06 06:16:09.509390: step 6130, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 83h:55m:58s remains)
INFO - root - 2017-12-06 06:16:18.260730: step 6140, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 79h:35m:17s remains)
INFO - root - 2017-12-06 06:16:27.262973: step 6150, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 83h:40m:09s remains)
INFO - root - 2017-12-06 06:16:36.128616: step 6160, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 77h:39m:38s remains)
INFO - root - 2017-12-06 06:16:44.867524: step 6170, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.846 sec/batch; 76h:40m:57s remains)
INFO - root - 2017-12-06 06:16:53.681793: step 6180, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.872 sec/batch; 79h:04m:53s remains)
INFO - root - 2017-12-06 06:17:02.613777: step 6190, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.904 sec/batch; 81h:54m:36s remains)
INFO - root - 2017-12-06 06:17:11.524925: step 6200, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 77h:02m:20s remains)
2017-12-06 06:17:12.274099: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2118125 -4.1962981 -4.1895242 -4.196867 -4.2007766 -4.196651 -4.1793094 -4.1465645 -4.1078663 -4.0989289 -4.1319523 -4.1643114 -4.183949 -4.2054448 -4.24479][-4.2117958 -4.2007256 -4.2026577 -4.215816 -4.2172241 -4.1998792 -4.1684628 -4.1275496 -4.09535 -4.0927267 -4.1307592 -4.1673851 -4.1980319 -4.231668 -4.2758622][-4.224061 -4.2195787 -4.2270656 -4.2392926 -4.2348676 -4.2059035 -4.1627464 -4.1164923 -4.0878386 -4.0848436 -4.1226439 -4.1699033 -4.2138133 -4.2572131 -4.3003922][-4.2373962 -4.24168 -4.2539854 -4.262434 -4.2510734 -4.2108855 -4.153861 -4.0976853 -4.0717859 -4.0758886 -4.1200356 -4.179697 -4.2319241 -4.2765603 -4.3120575][-4.2535882 -4.26354 -4.2778311 -4.2809238 -4.2577887 -4.204124 -4.1300387 -4.0597577 -4.0365634 -4.0602183 -4.1219006 -4.1958985 -4.2562585 -4.2972951 -4.3197589][-4.2754612 -4.2871943 -4.2977004 -4.2900376 -4.2524853 -4.1843829 -4.08875 -4.0022035 -3.9891427 -4.0466027 -4.1310034 -4.2163129 -4.2809081 -4.3127975 -4.3168392][-4.2944727 -4.3029003 -4.3056078 -4.2862215 -4.2329059 -4.1428394 -4.0261936 -3.9375269 -3.9560564 -4.0562677 -4.158473 -4.2423911 -4.2991915 -4.318717 -4.30238][-4.2874322 -4.290802 -4.2879405 -4.2593393 -4.1892719 -4.0799608 -3.9588563 -3.8956678 -3.9593554 -4.0824151 -4.183897 -4.2572289 -4.3013654 -4.3089037 -4.2773066][-4.2580504 -4.2636232 -4.2608624 -4.2239108 -4.140264 -4.0283456 -3.9337616 -3.9239056 -4.01389 -4.12501 -4.207428 -4.2666955 -4.2964697 -4.2899761 -4.2483835][-4.2246494 -4.23878 -4.2368979 -4.1928334 -4.103385 -4.0054865 -3.9543529 -3.9890532 -4.0804949 -4.1703076 -4.2357049 -4.2798843 -4.2911263 -4.2691197 -4.220942][-4.1942935 -4.2146277 -4.2097263 -4.1596713 -4.0734272 -4.00232 -3.9916344 -4.05041 -4.1321349 -4.2024555 -4.2559915 -4.2892566 -4.286365 -4.2537794 -4.2007027][-4.1709375 -4.189754 -4.177731 -4.1255794 -4.0549264 -4.0149174 -4.034318 -4.1026278 -4.1745195 -4.2243786 -4.2672176 -4.2920303 -4.2793703 -4.2383013 -4.1824293][-4.1679158 -4.1792016 -4.15838 -4.1103754 -4.0629306 -4.0475397 -4.0776248 -4.1428413 -4.2045751 -4.2454762 -4.2821169 -4.2951074 -4.2720761 -4.2225461 -4.1614056][-4.1813774 -4.1838784 -4.15841 -4.1182609 -4.0916972 -4.0896058 -4.1188087 -4.1735673 -4.2259078 -4.2640429 -4.2906895 -4.2925768 -4.263422 -4.2075691 -4.1409707][-4.1984658 -4.1991129 -4.1747642 -4.1436834 -4.1303635 -4.1334367 -4.1587424 -4.2007694 -4.2404957 -4.27022 -4.2845182 -4.278985 -4.2467837 -4.1892085 -4.1232829]]...]
INFO - root - 2017-12-06 06:17:21.205369: step 6210, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 82h:25m:00s remains)
INFO - root - 2017-12-06 06:17:30.229022: step 6220, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 82h:35m:51s remains)
INFO - root - 2017-12-06 06:17:39.038760: step 6230, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 80h:19m:05s remains)
INFO - root - 2017-12-06 06:17:47.953764: step 6240, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.889 sec/batch; 80h:35m:44s remains)
INFO - root - 2017-12-06 06:17:57.018202: step 6250, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 82h:12m:05s remains)
INFO - root - 2017-12-06 06:18:06.003810: step 6260, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.925 sec/batch; 83h:51m:41s remains)
INFO - root - 2017-12-06 06:18:15.140950: step 6270, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.894 sec/batch; 81h:02m:40s remains)
INFO - root - 2017-12-06 06:18:22.365520: step 6280, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.705 sec/batch; 63h:53m:45s remains)
INFO - root - 2017-12-06 06:18:29.257391: step 6290, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 60h:45m:15s remains)
INFO - root - 2017-12-06 06:18:36.088152: step 6300, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.664 sec/batch; 60h:10m:09s remains)
2017-12-06 06:18:36.730067: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1632075 -4.1809177 -4.1891551 -4.1914644 -4.1982632 -4.2146945 -4.2399 -4.2543716 -4.2450776 -4.224267 -4.2081885 -4.2076259 -4.2057714 -4.1947818 -4.1832857][-4.19835 -4.2247066 -4.2382979 -4.2412028 -4.2431622 -4.2535381 -4.2706013 -4.2733893 -4.2579641 -4.2335782 -4.2134252 -4.2077885 -4.2020197 -4.1912155 -4.1841536][-4.2499294 -4.2777166 -4.2919636 -4.290669 -4.2831182 -4.2797103 -4.2789159 -4.2652035 -4.244936 -4.2254133 -4.2128396 -4.206275 -4.1947565 -4.1822724 -4.1788397][-4.2674527 -4.2957644 -4.3104954 -4.3050385 -4.2886376 -4.2671227 -4.2420707 -4.2089767 -4.1896267 -4.1848774 -4.1868639 -4.1831012 -4.1677012 -4.1565809 -4.15693][-4.2416062 -4.2665448 -4.2791882 -4.2725511 -4.25169 -4.2104859 -4.1545858 -4.102335 -4.093307 -4.1129684 -4.1344209 -4.1362066 -4.122478 -4.1175237 -4.1251025][-4.1932235 -4.2123833 -4.2242427 -4.2232451 -4.1998706 -4.1335492 -4.0370426 -3.9632733 -3.9762185 -4.0315528 -4.0719547 -4.0820465 -4.077239 -4.0831723 -4.1005945][-4.1495571 -4.159308 -4.1688375 -4.1704874 -4.1424675 -4.0496445 -3.9079797 -3.8092375 -3.8561878 -3.9607937 -4.0212584 -4.0384817 -4.0455031 -4.0624933 -4.0878997][-4.121623 -4.1206603 -4.1221108 -4.1196141 -4.087409 -3.9843128 -3.8197834 -3.7088342 -3.7914197 -3.9348443 -4.0090785 -4.0254536 -4.0340309 -4.0558958 -4.0835481][-4.1172657 -4.1100154 -4.1097045 -4.1064386 -4.0811777 -3.9963915 -3.8595197 -3.7751622 -3.8505497 -3.9757662 -4.0397263 -4.0477009 -4.0508041 -4.0703955 -4.0964322][-4.1338315 -4.1287317 -4.1327105 -4.1347318 -4.1208563 -4.0669193 -3.9799054 -3.9306262 -3.972219 -4.047667 -4.0871224 -4.0920033 -4.0926685 -4.1065884 -4.1274753][-4.16552 -4.165946 -4.172904 -4.1756921 -4.168973 -4.13928 -4.090961 -4.0645585 -4.0781903 -4.1132803 -4.1364436 -4.1461062 -4.1514678 -4.1608171 -4.1744819][-4.2096848 -4.2145057 -4.2197719 -4.2191634 -4.2142286 -4.1980352 -4.1748581 -4.1617169 -4.1606159 -4.1716647 -4.1831875 -4.1949873 -4.2038441 -4.20981 -4.2152][-4.2411733 -4.2483954 -4.2515297 -4.2484417 -4.2445168 -4.2384415 -4.231348 -4.2248363 -4.2152886 -4.2116261 -4.2151489 -4.2267742 -4.2348833 -4.2342253 -4.229465][-4.2674155 -4.2749643 -4.2761641 -4.271801 -4.2675867 -4.2687058 -4.2699413 -4.2659841 -4.2534103 -4.2447672 -4.2438712 -4.251173 -4.2522798 -4.2408824 -4.2242966][-4.2945213 -4.3024707 -4.3019347 -4.2955184 -4.2885981 -4.2912397 -4.2955813 -4.293241 -4.2814226 -4.2717767 -4.2677469 -4.2691412 -4.2604589 -4.2378964 -4.2110076]]...]
INFO - root - 2017-12-06 06:18:43.555648: step 6310, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.689 sec/batch; 62h:26m:56s remains)
INFO - root - 2017-12-06 06:18:50.362218: step 6320, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 57h:31m:05s remains)
INFO - root - 2017-12-06 06:18:57.253120: step 6330, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.714 sec/batch; 64h:41m:03s remains)
INFO - root - 2017-12-06 06:19:04.035043: step 6340, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.679 sec/batch; 61h:30m:06s remains)
INFO - root - 2017-12-06 06:19:10.602419: step 6350, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 59h:19m:00s remains)
INFO - root - 2017-12-06 06:19:17.512567: step 6360, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 64h:19m:49s remains)
INFO - root - 2017-12-06 06:19:24.407395: step 6370, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 62h:52m:03s remains)
INFO - root - 2017-12-06 06:19:31.152277: step 6380, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 60h:20m:47s remains)
INFO - root - 2017-12-06 06:19:37.941169: step 6390, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.705 sec/batch; 63h:52m:16s remains)
INFO - root - 2017-12-06 06:19:44.912580: step 6400, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.673 sec/batch; 60h:55m:09s remains)
2017-12-06 06:19:45.598972: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2251134 -4.2248993 -4.2223992 -4.2103033 -4.1967273 -4.1888933 -4.1837492 -4.1780419 -4.1724467 -4.1742291 -4.18857 -4.201663 -4.1977777 -4.1791706 -4.1600494][-4.2296271 -4.2202797 -4.209909 -4.1929216 -4.1740794 -4.1606503 -4.1553736 -4.1542373 -4.1551223 -4.1616592 -4.1770587 -4.19193 -4.1915431 -4.17466 -4.1584311][-4.2259383 -4.2167416 -4.2062573 -4.1847091 -4.1545038 -4.1260772 -4.1142478 -4.1201267 -4.1341591 -4.148695 -4.1619587 -4.1707006 -4.1720147 -4.1596003 -4.1478133][-4.2217221 -4.2139931 -4.2043953 -4.1796274 -4.1376858 -4.0901537 -4.0713258 -4.0866575 -4.1158624 -4.1395545 -4.1467543 -4.1474705 -4.1494465 -4.1428676 -4.1347628][-4.2107434 -4.2067766 -4.1985726 -4.1730494 -4.1230612 -4.0637417 -4.0392318 -4.0618639 -4.1028562 -4.1324883 -4.134872 -4.1291976 -4.1294603 -4.1315579 -4.1324425][-4.1892056 -4.1847544 -4.1779575 -4.1559863 -4.1060872 -4.0449224 -4.0174236 -4.0428023 -4.0913453 -4.1271038 -4.1352859 -4.1324306 -4.1352744 -4.1478028 -4.1578178][-4.1597857 -4.1488237 -4.1452575 -4.1302385 -4.0885391 -4.0356097 -4.0072565 -4.03191 -4.084444 -4.1263838 -4.1423826 -4.1520844 -4.1637635 -4.1837435 -4.1941304][-4.1455493 -4.1272311 -4.1221437 -4.1170025 -4.0918188 -4.0543332 -4.0316429 -4.0533824 -4.0983191 -4.1320481 -4.1474714 -4.1677465 -4.1898131 -4.2146215 -4.2248087][-4.1514673 -4.1255808 -4.1200571 -4.1266537 -4.1214218 -4.1030159 -4.089457 -4.09919 -4.1256537 -4.1425676 -4.1529694 -4.1795068 -4.2088518 -4.2345791 -4.2395163][-4.1666746 -4.1342988 -4.1264095 -4.1409225 -4.1537724 -4.1544566 -4.1494102 -4.1498971 -4.1591964 -4.1636996 -4.1715245 -4.1955662 -4.2187614 -4.2352939 -4.2315474][-4.1819663 -4.1490359 -4.1373091 -4.1532254 -4.1787772 -4.1954212 -4.1998534 -4.199369 -4.2012258 -4.2000632 -4.2017531 -4.2102222 -4.2150431 -4.2188673 -4.211277][-4.2000618 -4.173604 -4.159059 -4.1685719 -4.1938715 -4.2153883 -4.225225 -4.2268972 -4.2292347 -4.2257981 -4.2179971 -4.2103672 -4.2018385 -4.2007785 -4.1982436][-4.22026 -4.1973925 -4.1808114 -4.1818118 -4.2007494 -4.2219439 -4.2356811 -4.2414455 -4.2468443 -4.24374 -4.2296104 -4.2121344 -4.1977444 -4.1965237 -4.1990013][-4.2445259 -4.2245507 -4.2065234 -4.2011747 -4.211493 -4.2280703 -4.2428522 -4.2508192 -4.2578807 -4.2530575 -4.2361846 -4.2187362 -4.2086549 -4.2107453 -4.2130947][-4.2647295 -4.24911 -4.2321463 -4.2234449 -4.2252135 -4.2323203 -4.2419353 -4.2490983 -4.257257 -4.253778 -4.2390933 -4.228004 -4.224761 -4.2299604 -4.2339878]]...]
INFO - root - 2017-12-06 06:19:52.281825: step 6410, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.619 sec/batch; 56h:04m:37s remains)
INFO - root - 2017-12-06 06:19:59.202649: step 6420, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.705 sec/batch; 63h:50m:47s remains)
INFO - root - 2017-12-06 06:20:05.866441: step 6430, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.679 sec/batch; 61h:27m:27s remains)
INFO - root - 2017-12-06 06:20:12.821640: step 6440, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 60h:45m:38s remains)
INFO - root - 2017-12-06 06:20:19.656462: step 6450, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.691 sec/batch; 62h:34m:06s remains)
INFO - root - 2017-12-06 06:20:26.506994: step 6460, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 63h:03m:36s remains)
INFO - root - 2017-12-06 06:20:33.270377: step 6470, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.693 sec/batch; 62h:45m:19s remains)
INFO - root - 2017-12-06 06:20:40.342869: step 6480, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.698 sec/batch; 63h:14m:53s remains)
INFO - root - 2017-12-06 06:20:47.088085: step 6490, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 58h:35m:37s remains)
INFO - root - 2017-12-06 06:20:53.869758: step 6500, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 61h:51m:29s remains)
2017-12-06 06:20:54.516372: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3369937 -4.3598032 -4.3733916 -4.3730607 -4.355545 -4.3172159 -4.2677307 -4.2279458 -4.1944175 -4.1578979 -4.1294146 -4.1320868 -4.16473 -4.2158995 -4.2621613][-4.3404722 -4.3700762 -4.3837028 -4.3781519 -4.3517208 -4.3014922 -4.2441835 -4.1983671 -4.1606688 -4.1331725 -4.1173425 -4.1273065 -4.1569171 -4.2089605 -4.2585258][-4.3415027 -4.3725014 -4.3813381 -4.3688307 -4.3305054 -4.27071 -4.2095857 -4.1592116 -4.1236095 -4.1056991 -4.099545 -4.1144037 -4.1415091 -4.1969166 -4.2526665][-4.3443937 -4.3733182 -4.3783469 -4.35961 -4.3116527 -4.2436452 -4.1761446 -4.1265135 -4.0914574 -4.0762944 -4.0739608 -4.0871143 -4.118413 -4.1837211 -4.246685][-4.348896 -4.3777013 -4.3787436 -4.3514233 -4.2912579 -4.2154431 -4.1452494 -4.0972481 -4.0578432 -4.0398583 -4.0423126 -4.0581431 -4.105093 -4.1846781 -4.24951][-4.3586092 -4.3880029 -4.3825188 -4.3432879 -4.2655506 -4.1793246 -4.1077256 -4.0498509 -4.0065207 -3.9997635 -4.0152249 -4.0454721 -4.11312 -4.2028871 -4.2663894][-4.3676953 -4.393682 -4.3799105 -4.3290381 -4.2351904 -4.1320467 -4.0430765 -3.9700317 -3.9425969 -3.9634292 -3.9999878 -4.0514035 -4.1371093 -4.2283297 -4.2874103][-4.3710184 -4.3852663 -4.3624949 -4.3018551 -4.1954331 -4.0652986 -3.9425974 -3.8657527 -3.8792839 -3.9396439 -3.99995 -4.0757942 -4.172019 -4.252738 -4.3004694][-4.3700132 -4.3706646 -4.3369765 -4.2643375 -4.1464391 -3.993026 -3.8449397 -3.7794116 -3.8322139 -3.9207389 -4.0012016 -4.0992594 -4.2010388 -4.2680721 -4.2990746][-4.3663073 -4.350596 -4.30445 -4.220644 -4.1022239 -3.9502139 -3.8092668 -3.7587874 -3.8275099 -3.9236615 -4.0088611 -4.114502 -4.2136831 -4.2718239 -4.2911758][-4.3580542 -4.32941 -4.2734933 -4.1855092 -4.0838614 -3.9678257 -3.8701272 -3.8416638 -3.8975122 -3.9745545 -4.0507679 -4.1460209 -4.2286553 -4.273632 -4.284615][-4.3440604 -4.3115745 -4.2547059 -4.1772628 -4.1036963 -4.0328379 -3.9784231 -3.9689894 -4.0080523 -4.0604377 -4.1155748 -4.1861191 -4.2456312 -4.2742782 -4.2787318][-4.3309193 -4.3011436 -4.2504559 -4.1877904 -4.1349759 -4.0924559 -4.065835 -4.072084 -4.1036057 -4.1387658 -4.1758456 -4.2253213 -4.2644072 -4.2804804 -4.2790108][-4.32558 -4.3041229 -4.2677116 -4.2197771 -4.1795483 -4.14859 -4.1363959 -4.1491361 -4.1727619 -4.1962824 -4.2239256 -4.2594638 -4.2823615 -4.2906175 -4.28826][-4.3324823 -4.3204732 -4.2999163 -4.2676368 -4.2394438 -4.2188072 -4.2118855 -4.218986 -4.2334924 -4.2496581 -4.2671728 -4.2900763 -4.3025527 -4.3056374 -4.3031988]]...]
INFO - root - 2017-12-06 06:21:01.294403: step 6510, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.654 sec/batch; 59h:13m:36s remains)
INFO - root - 2017-12-06 06:21:08.002945: step 6520, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.693 sec/batch; 62h:46m:19s remains)
INFO - root - 2017-12-06 06:21:14.929678: step 6530, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 60h:43m:14s remains)
INFO - root - 2017-12-06 06:21:21.772999: step 6540, loss = 2.03, batch loss = 1.98 (11.9 examples/sec; 0.674 sec/batch; 61h:01m:02s remains)
INFO - root - 2017-12-06 06:21:28.566815: step 6550, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 63h:03m:14s remains)
INFO - root - 2017-12-06 06:21:35.287173: step 6560, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 59h:57m:47s remains)
INFO - root - 2017-12-06 06:21:42.055732: step 6570, loss = 2.04, batch loss = 1.99 (11.5 examples/sec; 0.697 sec/batch; 63h:04m:45s remains)
INFO - root - 2017-12-06 06:21:48.835477: step 6580, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 59h:51m:31s remains)
INFO - root - 2017-12-06 06:21:55.779548: step 6590, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 59h:45m:28s remains)
INFO - root - 2017-12-06 06:22:02.522230: step 6600, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 58h:28m:08s remains)
2017-12-06 06:22:03.180461: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3256803 -4.3059793 -4.2672715 -4.2244983 -4.1908836 -4.1662574 -4.1437006 -4.1209931 -4.0968571 -4.0827131 -4.0922775 -4.1073308 -4.1160069 -4.1434236 -4.1576796][-4.3308949 -4.3134661 -4.274272 -4.2309117 -4.195941 -4.1672478 -4.1488261 -4.132782 -4.1076417 -4.0897536 -4.0961919 -4.1180587 -4.1375747 -4.1681309 -4.182991][-4.3337197 -4.3168383 -4.2807136 -4.2399907 -4.20698 -4.1791239 -4.1581154 -4.1441288 -4.1250658 -4.1103311 -4.1120787 -4.1317124 -4.1587 -4.183394 -4.1915541][-4.3338909 -4.3154268 -4.2779322 -4.2352619 -4.2010016 -4.1764884 -4.1541848 -4.1425147 -4.134613 -4.129168 -4.12656 -4.1387877 -4.1589222 -4.1801538 -4.1862173][-4.3316321 -4.3103194 -4.2674322 -4.2214317 -4.1878366 -4.1665626 -4.1394525 -4.1207962 -4.1200366 -4.1257176 -4.12469 -4.1287007 -4.1378756 -4.1605425 -4.1760359][-4.3220162 -4.2959533 -4.2455621 -4.1970782 -4.1661539 -4.1426492 -4.1029835 -4.0734015 -4.0813279 -4.1009068 -4.1124759 -4.1181588 -4.1223645 -4.1443143 -4.1685405][-4.3086052 -4.27563 -4.2164316 -4.1614962 -4.124536 -4.0909243 -4.0340438 -3.9967551 -4.0188055 -4.05851 -4.0856714 -4.097599 -4.1007252 -4.1238861 -4.153337][-4.2986331 -4.2602415 -4.1943359 -4.1298032 -4.0825319 -4.0381866 -3.9711838 -3.928062 -3.9561005 -4.0093613 -4.0475307 -4.06599 -4.0685406 -4.0872436 -4.1169224][-4.2917209 -4.2534175 -4.1944284 -4.1370106 -4.0914936 -4.0471525 -3.9770722 -3.9247062 -3.9449344 -3.9970539 -4.0418391 -4.0671272 -4.0672731 -4.0729761 -4.091784][-4.2888918 -4.2547235 -4.2073116 -4.1636515 -4.1310182 -4.0970426 -4.0353203 -3.9804192 -3.9914894 -4.036231 -4.0771685 -4.0978432 -4.0873461 -4.0753965 -4.0764713][-4.2885842 -4.256834 -4.2163887 -4.18122 -4.1584239 -4.137114 -4.0931458 -4.0520334 -4.0569658 -4.0907378 -4.1216512 -4.13102 -4.1096249 -4.0841894 -4.0749435][-4.2869606 -4.2565932 -4.2217159 -4.1930232 -4.1790113 -4.1682606 -4.1453586 -4.1232204 -4.1253572 -4.1397481 -4.1528254 -4.1507382 -4.1229734 -4.0899143 -4.0775814][-4.290863 -4.2613487 -4.2258358 -4.1975284 -4.1890793 -4.1860065 -4.1746373 -4.1660995 -4.1704874 -4.174118 -4.1729417 -4.1627264 -4.1355114 -4.1033998 -4.0905356][-4.3035908 -4.2770739 -4.242341 -4.2152271 -4.2090697 -4.2085629 -4.202939 -4.1983609 -4.2034936 -4.20346 -4.1970196 -4.18677 -4.1675563 -4.1399651 -4.1221542][-4.3197393 -4.298872 -4.2703047 -4.2492318 -4.2493062 -4.2542253 -4.2530489 -4.247962 -4.24936 -4.2457409 -4.2376413 -4.2284532 -4.2157173 -4.19511 -4.1741939]]...]
INFO - root - 2017-12-06 06:22:09.930221: step 6610, loss = 2.08, batch loss = 2.02 (13.9 examples/sec; 0.574 sec/batch; 51h:57m:26s remains)
INFO - root - 2017-12-06 06:22:16.675068: step 6620, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.680 sec/batch; 61h:30m:43s remains)
INFO - root - 2017-12-06 06:22:23.608045: step 6630, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.705 sec/batch; 63h:48m:18s remains)
INFO - root - 2017-12-06 06:22:30.302258: step 6640, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.681 sec/batch; 61h:36m:48s remains)
INFO - root - 2017-12-06 06:22:37.104254: step 6650, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 63h:23m:47s remains)
INFO - root - 2017-12-06 06:22:43.880720: step 6660, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 62h:12m:47s remains)
INFO - root - 2017-12-06 06:22:50.705035: step 6670, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 62h:15m:21s remains)
INFO - root - 2017-12-06 06:22:57.528310: step 6680, loss = 2.08, batch loss = 2.03 (11.4 examples/sec; 0.703 sec/batch; 63h:39m:04s remains)
INFO - root - 2017-12-06 06:23:04.339171: step 6690, loss = 2.08, batch loss = 2.03 (11.8 examples/sec; 0.676 sec/batch; 61h:09m:50s remains)
INFO - root - 2017-12-06 06:23:11.285300: step 6700, loss = 2.09, batch loss = 2.04 (11.3 examples/sec; 0.711 sec/batch; 64h:20m:35s remains)
2017-12-06 06:23:11.932963: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.128129 -4.1492662 -4.1711888 -4.1737418 -4.1409326 -4.0897393 -4.0395031 -3.999516 -4.0094695 -4.0682616 -4.1267252 -4.1730204 -4.2061543 -4.2254133 -4.2297988][-4.1300287 -4.1459789 -4.1677175 -4.1829557 -4.1633134 -4.1177444 -4.0570593 -4.0041304 -3.9993198 -4.0402408 -4.0855446 -4.1270266 -4.1636896 -4.1883693 -4.2036386][-4.1356697 -4.1571856 -4.1819539 -4.2010713 -4.1873116 -4.1468587 -4.0886655 -4.0335526 -4.0104127 -4.0261531 -4.0537853 -4.0815558 -4.1125674 -4.1411924 -4.168447][-4.1271019 -4.1617832 -4.1977081 -4.2217231 -4.2090626 -4.1711154 -4.1219459 -4.0705628 -4.0433187 -4.0469666 -4.056674 -4.068038 -4.0909734 -4.1164331 -4.1449594][-4.1003466 -4.1389871 -4.1841445 -4.2166963 -4.20361 -4.1646748 -4.1160069 -4.0678444 -4.0522437 -4.0664272 -4.0808177 -4.0915532 -4.1094356 -4.1239581 -4.1408038][-4.0628872 -4.0949249 -4.1493421 -4.1873198 -4.1753469 -4.1329451 -4.0718808 -4.0129738 -4.0028639 -4.0401726 -4.0824003 -4.1142235 -4.1389804 -4.147141 -4.1561241][-4.031827 -4.05747 -4.1119919 -4.1508 -4.1427021 -4.0973897 -4.0233374 -3.9506264 -3.937304 -3.9900064 -4.0603261 -4.1204634 -4.1576786 -4.1681056 -4.1774306][-4.0100737 -4.0313244 -4.0739217 -4.1047239 -4.0985694 -4.0549464 -3.9860339 -3.9236925 -3.9151325 -3.9645464 -4.0361109 -4.1032686 -4.1454072 -4.1582918 -4.1673822][-3.9989805 -4.0157561 -4.0449362 -4.0659704 -4.0629687 -4.0290833 -3.9790821 -3.9363649 -3.9342358 -3.9732757 -4.0300126 -4.0876765 -4.1234293 -4.1290836 -4.1342163][-4.003572 -4.026083 -4.0525064 -4.0677133 -4.0641251 -4.03998 -4.0117793 -3.987922 -3.9901857 -4.0188937 -4.0601659 -4.104425 -4.1265645 -4.1207538 -4.1178327][-4.0137424 -4.0483036 -4.0801988 -4.0967655 -4.0911217 -4.0707865 -4.0541015 -4.0422115 -4.0503483 -4.0757918 -4.1110592 -4.1465726 -4.1613436 -4.1477 -4.1358676][-4.0530643 -4.0898552 -4.1241446 -4.140686 -4.1351709 -4.1181426 -4.1074877 -4.1042643 -4.11758 -4.1414361 -4.1737175 -4.2045231 -4.21394 -4.1973453 -4.1788654][-4.130878 -4.1586781 -4.1845055 -4.1972218 -4.1906223 -4.1781487 -4.1719174 -4.1715608 -4.1827803 -4.2032208 -4.232141 -4.2572489 -4.2642994 -4.2509937 -4.2302723][-4.2077579 -4.2234507 -4.2388129 -4.2490821 -4.24776 -4.2415304 -4.2361522 -4.2330151 -4.2391262 -4.2549906 -4.2783604 -4.2980342 -4.303771 -4.2952824 -4.2783217][-4.2626719 -4.2720661 -4.2826438 -4.2919579 -4.2937994 -4.2889404 -4.2823563 -4.2779169 -4.2803917 -4.2900457 -4.3068719 -4.32198 -4.3275657 -4.3236485 -4.3128829]]...]
INFO - root - 2017-12-06 06:23:18.583443: step 6710, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 61h:22m:59s remains)
INFO - root - 2017-12-06 06:23:25.451025: step 6720, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 62h:10m:46s remains)
INFO - root - 2017-12-06 06:23:32.198712: step 6730, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 61h:46m:50s remains)
INFO - root - 2017-12-06 06:23:38.873975: step 6740, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.675 sec/batch; 61h:04m:35s remains)
INFO - root - 2017-12-06 06:23:45.645502: step 6750, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 63h:18m:10s remains)
INFO - root - 2017-12-06 06:23:52.471652: step 6760, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 60h:31m:47s remains)
INFO - root - 2017-12-06 06:23:59.394430: step 6770, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.715 sec/batch; 64h:41m:33s remains)
INFO - root - 2017-12-06 06:24:06.207834: step 6780, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 61h:53m:19s remains)
INFO - root - 2017-12-06 06:24:13.019182: step 6790, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 60h:22m:02s remains)
INFO - root - 2017-12-06 06:24:19.693245: step 6800, loss = 2.07, batch loss = 2.01 (14.5 examples/sec; 0.553 sec/batch; 50h:01m:09s remains)
2017-12-06 06:24:20.410483: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2776504 -4.2495165 -4.2196107 -4.1995692 -4.2030983 -4.2298217 -4.250268 -4.2562656 -4.2507896 -4.2470937 -4.2462516 -4.2301741 -4.2210088 -4.2111239 -4.1860857][-4.2712483 -4.2374625 -4.1990724 -4.1702952 -4.1678014 -4.189786 -4.2084355 -4.2159085 -4.2123728 -4.2152042 -4.2167277 -4.2026954 -4.1948318 -4.1875033 -4.1559448][-4.2707605 -4.2334414 -4.1910462 -4.1561918 -4.1441469 -4.1552329 -4.1646614 -4.168014 -4.1645918 -4.1747413 -4.1832533 -4.1745262 -4.1635947 -4.1518264 -4.11258][-4.2722592 -4.2343259 -4.1901345 -4.1512461 -4.1261129 -4.1238527 -4.1274705 -4.1289105 -4.1248274 -4.1403346 -4.155633 -4.151525 -4.138999 -4.1222582 -4.0819087][-4.2759314 -4.2395096 -4.1970434 -4.1586704 -4.1277027 -4.1163435 -4.1186275 -4.1212626 -4.1178751 -4.1375909 -4.1544991 -4.1543736 -4.14714 -4.1274347 -4.090179][-4.2725887 -4.2353306 -4.1928611 -4.1555967 -4.1275 -4.1171942 -4.1215911 -4.1233158 -4.1247234 -4.1490126 -4.1676197 -4.1762614 -4.1765161 -4.1603956 -4.1306968][-4.2603207 -4.2178707 -4.1694822 -4.1321259 -4.1111951 -4.1027355 -4.1063676 -4.1068792 -4.1106296 -4.1358776 -4.1587324 -4.1740894 -4.1810346 -4.17517 -4.1575189][-4.2404652 -4.1896672 -4.1341081 -4.0967269 -4.0811648 -4.071209 -4.0646706 -4.0568762 -4.0528193 -4.0760045 -4.1074662 -4.1299477 -4.145628 -4.151547 -4.150527][-4.217804 -4.1547785 -4.0900936 -4.0452733 -4.0289149 -4.0193267 -4.0084844 -3.9924095 -3.9811349 -4.0078435 -4.0506525 -4.0852227 -4.1134753 -4.1292777 -4.1402435][-4.2063551 -4.137229 -4.0674748 -4.0204592 -4.0069313 -4.0041227 -3.9997425 -3.98624 -3.9756823 -4.0024762 -4.0456934 -4.082664 -4.1163559 -4.1405454 -4.1580715][-4.2190437 -4.1589255 -4.0971241 -4.057384 -4.0467558 -4.0517273 -4.0568504 -4.0531173 -4.0470896 -4.0678296 -4.1021447 -4.132988 -4.1642709 -4.1883492 -4.2049842][-4.2545972 -4.2126613 -4.1709609 -4.146596 -4.1436772 -4.1523118 -4.1590672 -4.1613016 -4.157043 -4.1683583 -4.1897984 -4.2098422 -4.2303519 -4.2472978 -4.2575369][-4.294621 -4.2694173 -4.2447305 -4.2321134 -4.2339892 -4.2413597 -4.24546 -4.2479978 -4.2437654 -4.2468152 -4.2583179 -4.2692175 -4.2799869 -4.2895689 -4.2951784][-4.3200355 -4.3050852 -4.2877774 -4.2788377 -4.2804322 -4.2870879 -4.2915988 -4.2933459 -4.2893696 -4.2880268 -4.2929883 -4.2994356 -4.3065758 -4.3134222 -4.3175368][-4.3277626 -4.3187275 -4.3081017 -4.303185 -4.3053589 -4.3103166 -4.3136077 -4.3139629 -4.310638 -4.3083048 -4.3096089 -4.3133006 -4.3188658 -4.3244839 -4.3286047]]...]
INFO - root - 2017-12-06 06:24:27.105785: step 6810, loss = 2.08, batch loss = 2.03 (12.1 examples/sec; 0.662 sec/batch; 59h:54m:36s remains)
INFO - root - 2017-12-06 06:24:33.837593: step 6820, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 61h:39m:52s remains)
INFO - root - 2017-12-06 06:24:40.694526: step 6830, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 59h:51m:43s remains)
INFO - root - 2017-12-06 06:24:47.386654: step 6840, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 65h:11m:20s remains)
INFO - root - 2017-12-06 06:24:54.243032: step 6850, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 61h:26m:09s remains)
INFO - root - 2017-12-06 06:25:01.070877: step 6860, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 63h:24m:26s remains)
INFO - root - 2017-12-06 06:25:07.839027: step 6870, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 61h:52m:24s remains)
INFO - root - 2017-12-06 06:25:14.592263: step 6880, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 60h:36m:11s remains)
INFO - root - 2017-12-06 06:25:21.494794: step 6890, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.677 sec/batch; 61h:12m:59s remains)
INFO - root - 2017-12-06 06:25:28.099971: step 6900, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 59h:48m:08s remains)
2017-12-06 06:25:28.828114: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2619638 -4.239778 -4.2215548 -4.2069926 -4.1931195 -4.1829381 -4.1786504 -4.1890812 -4.2130413 -4.2298584 -4.2453938 -4.2535214 -4.242959 -4.214468 -4.1831369][-4.2503538 -4.223031 -4.202507 -4.18981 -4.1751323 -4.1569042 -4.1452818 -4.1603756 -4.1969004 -4.2180181 -4.232954 -4.2411642 -4.2306118 -4.19289 -4.1563439][-4.2477207 -4.2105212 -4.1843128 -4.1724372 -4.1605563 -4.1388531 -4.1210632 -4.1386538 -4.1862969 -4.2134523 -4.2299476 -4.237494 -4.2287555 -4.1830363 -4.1321344][-4.2445474 -4.2015738 -4.1735148 -4.1622114 -4.1517005 -4.1259151 -4.0983505 -4.1163025 -4.1729851 -4.207562 -4.2267132 -4.2348509 -4.2289982 -4.1834011 -4.1253009][-4.2434225 -4.2021742 -4.1725812 -4.1543484 -4.13653 -4.0993032 -4.0503635 -4.0617161 -4.1365585 -4.1872745 -4.2109327 -4.2239304 -4.2215934 -4.1829147 -4.1242447][-4.2450294 -4.2061391 -4.1680946 -4.1297479 -4.0919766 -4.0259256 -3.9321041 -3.9352119 -4.0547566 -4.1473885 -4.1862578 -4.2074513 -4.2147346 -4.18957 -4.1390357][-4.2533369 -4.2159576 -4.172451 -4.1129847 -4.0383148 -3.9124789 -3.7481852 -3.752496 -3.9413145 -4.08814 -4.1608377 -4.206151 -4.2323279 -4.2264624 -4.1892123][-4.26057 -4.2278371 -4.1912041 -4.1354265 -4.0429244 -3.8766379 -3.6754959 -3.6915009 -3.9051139 -4.0656424 -4.155273 -4.2197571 -4.2560706 -4.2559896 -4.2268476][-4.2618642 -4.2311254 -4.2056828 -4.1721373 -4.0994139 -3.9753404 -3.852097 -3.8719563 -4.0073857 -4.1148272 -4.1796169 -4.2266769 -4.2566671 -4.2544565 -4.2281213][-4.2649875 -4.2346096 -4.2132063 -4.1878595 -4.1347289 -4.0601144 -4.0068851 -4.0278807 -4.1066971 -4.168582 -4.2045846 -4.2270041 -4.2375817 -4.2278218 -4.2032118][-4.2773581 -4.25179 -4.232779 -4.2066 -4.1576738 -4.0997105 -4.0719032 -4.0996776 -4.156754 -4.1937695 -4.2157869 -4.2290869 -4.221271 -4.2049088 -4.1805439][-4.2893043 -4.2714357 -4.2545953 -4.231041 -4.19147 -4.1437216 -4.1193638 -4.1464872 -4.1913309 -4.2127376 -4.2307825 -4.2381611 -4.2249184 -4.2065434 -4.1796536][-4.2981877 -4.2882962 -4.2753315 -4.2553453 -4.227592 -4.1909237 -4.1672487 -4.1962266 -4.2340693 -4.2446685 -4.2542686 -4.2543988 -4.2380214 -4.2209496 -4.1945639][-4.3018527 -4.2967577 -4.2882729 -4.272686 -4.2541943 -4.2256069 -4.2045403 -4.2376442 -4.2750497 -4.2806473 -4.281971 -4.2775664 -4.2669148 -4.2545462 -4.23058][-4.3076982 -4.3076482 -4.3057895 -4.2949066 -4.2803597 -4.2601862 -4.2458057 -4.2706361 -4.2991314 -4.3046942 -4.3103986 -4.3120856 -4.3078465 -4.3015866 -4.2831249]]...]
INFO - root - 2017-12-06 06:25:35.776153: step 6910, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 62h:49m:45s remains)
INFO - root - 2017-12-06 06:25:42.625127: step 6920, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 61h:50m:57s remains)
INFO - root - 2017-12-06 06:25:49.266220: step 6930, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 62h:22m:16s remains)
INFO - root - 2017-12-06 06:25:56.058708: step 6940, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 62h:15m:33s remains)
INFO - root - 2017-12-06 06:26:02.939578: step 6950, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 60h:25m:51s remains)
INFO - root - 2017-12-06 06:26:09.726451: step 6960, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.664 sec/batch; 60h:03m:48s remains)
INFO - root - 2017-12-06 06:26:16.493400: step 6970, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 61h:41m:46s remains)
INFO - root - 2017-12-06 06:26:23.354228: step 6980, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 62h:07m:17s remains)
INFO - root - 2017-12-06 06:26:29.911437: step 6990, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.682 sec/batch; 61h:42m:15s remains)
INFO - root - 2017-12-06 06:26:36.657685: step 7000, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 61h:18m:40s remains)
2017-12-06 06:26:37.345935: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1192422 -4.11514 -4.0941653 -4.0783782 -4.0722136 -4.0847621 -4.1055069 -4.1159019 -4.1134658 -4.1010118 -4.0802121 -4.0741587 -4.086308 -4.0898952 -4.0806069][-4.1161895 -4.104702 -4.0886292 -4.0714087 -4.0588455 -4.0694332 -4.0939574 -4.1072049 -4.1079159 -4.104847 -4.0906682 -4.0818114 -4.0789576 -4.0703144 -4.0561147][-4.1067781 -4.1054816 -4.0990968 -4.0824203 -4.0640025 -4.0640888 -4.0760317 -4.0804386 -4.0802937 -4.090373 -4.0905747 -4.0795226 -4.0554066 -4.0410185 -4.0376182][-4.0996132 -4.1104608 -4.1104169 -4.0958943 -4.0784273 -4.0700397 -4.0604229 -4.0430527 -4.0447288 -4.0757575 -4.099545 -4.0918989 -4.0656509 -4.0578909 -4.0631113][-4.0905809 -4.1085563 -4.1108918 -4.1018481 -4.0919886 -4.079505 -4.0409184 -3.9913259 -3.997262 -4.0579386 -4.1096497 -4.114862 -4.1017561 -4.1002874 -4.1063051][-4.0635467 -4.0903215 -4.1034837 -4.1018529 -4.0911875 -4.0630059 -3.9883642 -3.9085009 -3.9365609 -4.0408654 -4.1162844 -4.1357241 -4.133276 -4.1338367 -4.1336994][-4.0168166 -4.0503931 -4.0764842 -4.0824242 -4.0642128 -4.0177674 -3.9248495 -3.8466713 -3.9118862 -4.0410256 -4.118546 -4.143549 -4.1502395 -4.1537046 -4.1472325][-3.983403 -4.0290232 -4.0648785 -4.0749578 -4.056797 -4.0099926 -3.9323025 -3.8935874 -3.9657853 -4.0634203 -4.1130404 -4.1267056 -4.1331539 -4.136641 -4.1354289][-3.993053 -4.0406656 -4.0720882 -4.0785918 -4.0671878 -4.0389748 -3.9920719 -3.9878578 -4.0423493 -4.0976071 -4.1190186 -4.1223612 -4.1266603 -4.1230688 -4.1182661][-4.0288076 -4.0695143 -4.0902872 -4.0887957 -4.0857263 -4.0763011 -4.0519452 -4.0593657 -4.0963616 -4.1246538 -4.1309309 -4.1341567 -4.13686 -4.1254611 -4.1060781][-4.0638037 -4.1052885 -4.1213913 -4.1146584 -4.1122966 -4.1054235 -4.0834036 -4.0869689 -4.1078906 -4.1255226 -4.1301618 -4.1398721 -4.14652 -4.1313353 -4.102098][-4.0963626 -4.1335545 -4.145227 -4.1335845 -4.1319475 -4.1231756 -4.0948977 -4.0911293 -4.1106415 -4.1275482 -4.1324363 -4.1460271 -4.1533842 -4.1369696 -4.1098108][-4.104455 -4.1354966 -4.1378641 -4.1281996 -4.1344919 -4.1300964 -4.1052308 -4.1048932 -4.1283059 -4.1466913 -4.1487579 -4.1508808 -4.1496382 -4.1341081 -4.1170464][-4.1001959 -4.1223283 -4.12027 -4.11981 -4.1343303 -4.1354423 -4.1154475 -4.1142712 -4.1282635 -4.1377149 -4.1385455 -4.1365495 -4.1275349 -4.1130481 -4.1034794][-4.0916328 -4.1050096 -4.1096492 -4.12235 -4.1378851 -4.1435404 -4.1252861 -4.1077385 -4.1000628 -4.1035357 -4.1122322 -4.1192713 -4.1131759 -4.0948062 -4.0804858]]...]
INFO - root - 2017-12-06 06:26:44.179417: step 7010, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 59h:57m:23s remains)
INFO - root - 2017-12-06 06:26:51.046701: step 7020, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 62h:48m:08s remains)
INFO - root - 2017-12-06 06:26:57.770033: step 7030, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 63h:55m:52s remains)
INFO - root - 2017-12-06 06:27:04.631890: step 7040, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 58h:14m:16s remains)
INFO - root - 2017-12-06 06:27:11.386571: step 7050, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.669 sec/batch; 60h:31m:19s remains)
INFO - root - 2017-12-06 06:27:18.205678: step 7060, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 62h:07m:00s remains)
INFO - root - 2017-12-06 06:27:25.087166: step 7070, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 64h:25m:01s remains)
INFO - root - 2017-12-06 06:27:31.875193: step 7080, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 61h:11m:56s remains)
INFO - root - 2017-12-06 06:27:38.508015: step 7090, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 62h:27m:29s remains)
INFO - root - 2017-12-06 06:27:45.324719: step 7100, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.655 sec/batch; 59h:14m:37s remains)
2017-12-06 06:27:45.969883: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1986942 -4.1764879 -4.1458941 -4.1176882 -4.1129646 -4.1229796 -4.134244 -4.1288447 -4.1181054 -4.1276107 -4.1528549 -4.16567 -4.1542382 -4.1427822 -4.1553354][-4.1966405 -4.1793118 -4.1517754 -4.119534 -4.1064558 -4.1208534 -4.1458478 -4.1479325 -4.1330996 -4.1352754 -4.15198 -4.1588912 -4.1447339 -4.1303358 -4.1438193][-4.1918926 -4.1811604 -4.1576233 -4.1237659 -4.1055174 -4.1254482 -4.1655273 -4.18158 -4.1768236 -4.173883 -4.176147 -4.1710758 -4.1512909 -4.1359363 -4.1467209][-4.1776719 -4.1770358 -4.1576743 -4.1259623 -4.1042566 -4.121418 -4.1644988 -4.1917624 -4.2030711 -4.20897 -4.2070394 -4.1902575 -4.1645312 -4.148571 -4.1538539][-4.1646538 -4.1726089 -4.161417 -4.136342 -4.1108456 -4.11019 -4.1336708 -4.1564274 -4.183785 -4.2128649 -4.2220931 -4.2021675 -4.1730595 -4.15439 -4.1550879][-4.169651 -4.1812491 -4.1767764 -4.1546888 -4.1210003 -4.0925593 -4.0721812 -4.0695791 -4.1082315 -4.1695719 -4.2030458 -4.193634 -4.1692753 -4.154582 -4.1574578][-4.1953421 -4.20545 -4.2038283 -4.1781797 -4.13206 -4.07334 -4.0003486 -3.9551222 -3.9941185 -4.0896149 -4.1550636 -4.1656065 -4.1526566 -4.1527982 -4.1688848][-4.2284217 -4.232841 -4.2300849 -4.2045236 -4.151135 -4.0718246 -3.9628859 -3.879108 -3.9037654 -4.0138364 -4.1027956 -4.1336551 -4.1400709 -4.1586595 -4.1882367][-4.244987 -4.2446141 -4.2407432 -4.2256403 -4.1859641 -4.1179771 -4.0220432 -3.9428604 -3.9457698 -4.0148592 -4.0789247 -4.1075683 -4.125452 -4.1590586 -4.2015581][-4.2227621 -4.2246418 -4.2266049 -4.2273054 -4.2134452 -4.1738887 -4.1161137 -4.06675 -4.0521488 -4.0680656 -4.0807858 -4.0851097 -4.0997519 -4.1360497 -4.1832275][-4.1751328 -4.1796165 -4.18937 -4.2037563 -4.2109475 -4.1964579 -4.1713157 -4.1517935 -4.1418114 -4.1331062 -4.1137505 -4.0926571 -4.0900793 -4.1100054 -4.1479468][-4.1292367 -4.1351614 -4.1513615 -4.171679 -4.1878839 -4.1887813 -4.1859212 -4.1905737 -4.19817 -4.1906095 -4.1637454 -4.1301603 -4.1086326 -4.1108189 -4.1340737][-4.1050649 -4.1162987 -4.1421037 -4.1608982 -4.1683364 -4.170733 -4.175384 -4.1924834 -4.2146316 -4.2188792 -4.2008028 -4.1725116 -4.1487484 -4.1435061 -4.1560006][-4.1219954 -4.1399183 -4.17282 -4.1859922 -4.17891 -4.1710534 -4.168107 -4.1808038 -4.2057352 -4.214952 -4.208014 -4.19199 -4.1788497 -4.17454 -4.1790781][-4.1582732 -4.1725922 -4.2035623 -4.2168708 -4.2075 -4.1927495 -4.18215 -4.1808157 -4.1908894 -4.1930671 -4.1901174 -4.1840873 -4.1832323 -4.185142 -4.1877971]]...]
INFO - root - 2017-12-06 06:27:52.854754: step 7110, loss = 2.11, batch loss = 2.05 (11.6 examples/sec; 0.692 sec/batch; 62h:31m:38s remains)
INFO - root - 2017-12-06 06:27:59.756366: step 7120, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.695 sec/batch; 62h:51m:13s remains)
INFO - root - 2017-12-06 06:28:06.321420: step 7130, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 60h:42m:15s remains)
INFO - root - 2017-12-06 06:28:13.157963: step 7140, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.717 sec/batch; 64h:45m:24s remains)
INFO - root - 2017-12-06 06:28:19.933103: step 7150, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.668 sec/batch; 60h:23m:17s remains)
INFO - root - 2017-12-06 06:28:26.670862: step 7160, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 60h:44m:19s remains)
INFO - root - 2017-12-06 06:28:33.457647: step 7170, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.667 sec/batch; 60h:16m:27s remains)
INFO - root - 2017-12-06 06:28:40.220229: step 7180, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 59h:33m:04s remains)
INFO - root - 2017-12-06 06:28:47.153841: step 7190, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 61h:50m:34s remains)
INFO - root - 2017-12-06 06:28:54.019213: step 7200, loss = 2.04, batch loss = 1.99 (11.6 examples/sec; 0.691 sec/batch; 62h:25m:34s remains)
2017-12-06 06:28:54.628648: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2370386 -4.2486529 -4.2681227 -4.2807341 -4.2818375 -4.2863455 -4.2870927 -4.2778172 -4.2705116 -4.2666979 -4.2563019 -4.2419224 -4.2392073 -4.2524371 -4.2671938][-4.27862 -4.2858396 -4.2945843 -4.3016272 -4.3028255 -4.3060107 -4.3036013 -4.2871861 -4.2727032 -4.2660222 -4.25713 -4.2434406 -4.2382994 -4.2491922 -4.2606969][-4.3028231 -4.3119855 -4.313168 -4.3125563 -4.3095536 -4.3069205 -4.2988348 -4.27374 -4.2528176 -4.2495222 -4.2509766 -4.24448 -4.2379861 -4.2410622 -4.242023][-4.2981634 -4.3141003 -4.318059 -4.3122826 -4.3011622 -4.2896314 -4.2688975 -4.2338138 -4.21232 -4.2227187 -4.2457733 -4.2584825 -4.2592545 -4.2569113 -4.24898][-4.2748685 -4.2968497 -4.3019409 -4.2867637 -4.2619963 -4.2354488 -4.1966119 -4.1477771 -4.127183 -4.1597881 -4.2146111 -4.25754 -4.2781849 -4.2848053 -4.2757425][-4.2531319 -4.2750688 -4.2798038 -4.25327 -4.2060885 -4.1549225 -4.0878983 -4.0109692 -3.9809363 -4.0399871 -4.1377616 -4.2174797 -4.2701988 -4.3010707 -4.3043704][-4.2486939 -4.2711434 -4.2729578 -4.2335191 -4.1637459 -4.0834136 -3.9794054 -3.8549519 -3.7967052 -3.8875136 -4.0367308 -4.1518722 -4.2354136 -4.2941065 -4.315433][-4.2498279 -4.2822704 -4.2892556 -4.246069 -4.1623478 -4.0670772 -3.9438446 -3.7861409 -3.6949205 -3.7923589 -3.9685631 -4.1026163 -4.200603 -4.276752 -4.3075786][-4.2382522 -4.2827992 -4.3016467 -4.2720432 -4.2015133 -4.1182475 -4.0142264 -3.8789701 -3.7916868 -3.8508904 -3.9891396 -4.101613 -4.1856332 -4.2557321 -4.2853928][-4.23198 -4.2839689 -4.3126612 -4.3025775 -4.25487 -4.1934037 -4.1212687 -4.0294728 -3.9644785 -3.9853303 -4.0695276 -4.1436892 -4.1978178 -4.2435479 -4.2586246][-4.2532659 -4.295229 -4.3189473 -4.3167014 -4.2849751 -4.2412906 -4.1972175 -4.1469021 -4.1070719 -4.1125989 -4.1594253 -4.2015948 -4.2323408 -4.2536674 -4.2502384][-4.2860451 -4.3114276 -4.3253069 -4.3218503 -4.2954421 -4.2610922 -4.2374396 -4.217895 -4.2017155 -4.2083826 -4.2354603 -4.2590275 -4.2713776 -4.2727866 -4.25752][-4.2994285 -4.31757 -4.3229694 -4.3174195 -4.3003283 -4.2785282 -4.2666216 -4.2654028 -4.26425 -4.269793 -4.2838697 -4.2961755 -4.29719 -4.2874079 -4.2680364][-4.303822 -4.3191848 -4.3192315 -4.311717 -4.3015175 -4.2911592 -4.2859945 -4.2884488 -4.2905836 -4.292654 -4.2993917 -4.3084717 -4.31028 -4.3038797 -4.2887549][-4.3075151 -4.3172064 -4.3135185 -4.30575 -4.2986417 -4.29226 -4.2891836 -4.2907939 -4.2924786 -4.2935438 -4.2997293 -4.30657 -4.3089156 -4.3067169 -4.2967663]]...]
INFO - root - 2017-12-06 06:29:01.436343: step 7210, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 61h:27m:54s remains)
INFO - root - 2017-12-06 06:29:08.276471: step 7220, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.683 sec/batch; 61h:40m:25s remains)
INFO - root - 2017-12-06 06:29:15.013268: step 7230, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 63h:55m:17s remains)
INFO - root - 2017-12-06 06:29:21.893957: step 7240, loss = 2.03, batch loss = 1.97 (11.9 examples/sec; 0.673 sec/batch; 60h:48m:08s remains)
INFO - root - 2017-12-06 06:29:28.747078: step 7250, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.679 sec/batch; 61h:21m:16s remains)
INFO - root - 2017-12-06 06:29:35.507982: step 7260, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 60h:10m:51s remains)
INFO - root - 2017-12-06 06:29:42.220406: step 7270, loss = 2.06, batch loss = 2.00 (13.4 examples/sec; 0.599 sec/batch; 54h:06m:37s remains)
INFO - root - 2017-12-06 06:29:49.059892: step 7280, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.711 sec/batch; 64h:11m:21s remains)
INFO - root - 2017-12-06 06:29:55.850282: step 7290, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 59h:42m:27s remains)
INFO - root - 2017-12-06 06:30:02.576552: step 7300, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.709 sec/batch; 64h:02m:08s remains)
2017-12-06 06:30:03.311556: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2650232 -4.2611794 -4.2686858 -4.2794232 -4.2859573 -4.2828722 -4.2771397 -4.2777219 -4.2812591 -4.2768154 -4.2657375 -4.2591753 -4.2631693 -4.2735076 -4.2891078][-4.20988 -4.1999717 -4.210392 -4.2267923 -4.2368178 -4.2346964 -4.2267561 -4.2254643 -4.2282038 -4.2226 -4.2089634 -4.206665 -4.2166777 -4.2338018 -4.2532015][-4.1744361 -4.1624861 -4.17587 -4.1979933 -4.20856 -4.2063441 -4.19957 -4.1989245 -4.2003756 -4.1940608 -4.182621 -4.1873279 -4.2026334 -4.2211132 -4.2389278][-4.168263 -4.1618829 -4.1766729 -4.1969028 -4.1998672 -4.1909614 -4.182137 -4.1830935 -4.1904969 -4.1931086 -4.1920304 -4.2038593 -4.2164607 -4.2266388 -4.2361374][-4.1764245 -4.1773691 -4.1932311 -4.2016454 -4.1912289 -4.1679468 -4.1414118 -4.1356297 -4.1591706 -4.1881528 -4.2044435 -4.2176695 -4.222002 -4.2207856 -4.2196727][-4.1903224 -4.1967635 -4.2145844 -4.2134118 -4.187058 -4.138813 -4.0722084 -4.0405369 -4.089159 -4.1647177 -4.2030888 -4.2193785 -4.2212291 -4.2143183 -4.2078652][-4.2046409 -4.2116141 -4.226253 -4.2189403 -4.1804633 -4.1107984 -3.9998357 -3.926929 -4.0040431 -4.1278744 -4.1906648 -4.2116966 -4.2209325 -4.2207355 -4.2116747][-4.2145448 -4.2193904 -4.2291012 -4.2251143 -4.193007 -4.1262774 -4.0148945 -3.9408057 -4.0097575 -4.1302567 -4.1964288 -4.2161713 -4.2280226 -4.234283 -4.2241564][-4.2231088 -4.2319975 -4.2429323 -4.2455349 -4.2298975 -4.1853294 -4.1152577 -4.0710263 -4.1090865 -4.1827579 -4.2230611 -4.2287254 -4.2330751 -4.236969 -4.2261348][-4.2210951 -4.2317452 -4.2462792 -4.2551794 -4.253581 -4.2352624 -4.2016139 -4.1821656 -4.2034774 -4.23948 -4.2520461 -4.2452703 -4.238399 -4.2325754 -4.2181897][-4.2183208 -4.2259822 -4.2407789 -4.2527966 -4.2564697 -4.2505207 -4.2398629 -4.2345848 -4.2485132 -4.2683058 -4.2735753 -4.2691388 -4.2603979 -4.2496877 -4.2283912][-4.235486 -4.2422681 -4.2564039 -4.2652788 -4.2642808 -4.2595978 -4.2571034 -4.2586021 -4.2690911 -4.28699 -4.2974105 -4.3056188 -4.3037386 -4.2930489 -4.2641683][-4.2650256 -4.2745433 -4.2866583 -4.2906246 -4.2854385 -4.2791123 -4.2747097 -4.2761192 -4.2880964 -4.3067412 -4.3200474 -4.3328218 -4.3349066 -4.32567 -4.2965975][-4.28737 -4.2959814 -4.3046794 -4.3061857 -4.3009424 -4.2954121 -4.290513 -4.2930331 -4.3051767 -4.3210649 -4.3331137 -4.3422852 -4.3445086 -4.3354526 -4.3098249][-4.30515 -4.3101525 -4.3126025 -4.3128157 -4.3118153 -4.3109837 -4.306036 -4.3067555 -4.3185258 -4.3326688 -4.3389964 -4.3408628 -4.3415704 -4.3367138 -4.3172874]]...]
INFO - root - 2017-12-06 06:30:10.122912: step 7310, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.673 sec/batch; 60h:44m:58s remains)
INFO - root - 2017-12-06 06:30:16.785183: step 7320, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 62h:40m:08s remains)
INFO - root - 2017-12-06 06:30:23.663272: step 7330, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.693 sec/batch; 62h:37m:47s remains)
INFO - root - 2017-12-06 06:30:30.291353: step 7340, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.665 sec/batch; 60h:04m:23s remains)
INFO - root - 2017-12-06 06:30:37.102396: step 7350, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 64h:32m:54s remains)
INFO - root - 2017-12-06 06:30:43.855276: step 7360, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.681 sec/batch; 61h:27m:47s remains)
INFO - root - 2017-12-06 06:30:50.447446: step 7370, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 61h:08m:45s remains)
INFO - root - 2017-12-06 06:30:57.222841: step 7380, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 64h:31m:19s remains)
INFO - root - 2017-12-06 06:31:04.056704: step 7390, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 61h:32m:24s remains)
INFO - root - 2017-12-06 06:31:10.945264: step 7400, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 60h:50m:53s remains)
2017-12-06 06:31:11.593221: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2846689 -4.3106093 -4.3283472 -4.3172789 -4.2863021 -4.2444196 -4.1956649 -4.1430521 -4.0910892 -4.0882154 -4.1339245 -4.1787558 -4.2127986 -4.2318091 -4.2464242][-4.2981181 -4.3196597 -4.3274 -4.307498 -4.2763443 -4.2383504 -4.1975274 -4.1467657 -4.0957985 -4.0930481 -4.1347861 -4.1751194 -4.2092252 -4.2362294 -4.2563448][-4.2946029 -4.311203 -4.3107076 -4.2904387 -4.2708426 -4.2533965 -4.2289934 -4.1824851 -4.1313138 -4.1183248 -4.1483278 -4.1817169 -4.2134676 -4.2466044 -4.2726135][-4.299685 -4.3092532 -4.3029747 -4.285584 -4.2718287 -4.2673478 -4.2509 -4.2013783 -4.1427402 -4.1219926 -4.1480689 -4.1821928 -4.2162604 -4.2536774 -4.2840886][-4.3208294 -4.3244519 -4.3147759 -4.2950711 -4.2727118 -4.2555413 -4.2215295 -4.1521077 -4.0888824 -4.0785885 -4.1198788 -4.1661043 -4.2098641 -4.2532053 -4.289609][-4.3396387 -4.3387074 -4.3285608 -4.3045454 -4.2652712 -4.2157731 -4.1431317 -4.0335717 -3.9588804 -3.9869289 -4.0717006 -4.1401129 -4.1980805 -4.250792 -4.2915106][-4.3425183 -4.3423381 -4.33648 -4.3099241 -4.2549882 -4.169311 -4.043468 -3.8772018 -3.7819142 -3.8693833 -4.0115671 -4.1107693 -4.1850543 -4.2496333 -4.2940936][-4.3352442 -4.335556 -4.3315306 -4.304184 -4.2413735 -4.135932 -3.9808621 -3.7853279 -3.6713507 -3.7884245 -3.9620233 -4.085568 -4.1753259 -4.2486386 -4.2967906][-4.3275819 -4.3260765 -4.3207297 -4.2949576 -4.2356043 -4.14122 -4.0145922 -3.8617373 -3.7664709 -3.8379784 -3.9751627 -4.09012 -4.180573 -4.2546716 -4.3013988][-4.3225737 -4.3193684 -4.3116574 -4.2895527 -4.2440853 -4.1772428 -4.09832 -4.0058317 -3.9395835 -3.9632292 -4.0422716 -4.1263437 -4.2049518 -4.2720857 -4.3123131][-4.3230386 -4.3199182 -4.3118243 -4.2925563 -4.2584615 -4.2143135 -4.1706824 -4.1220665 -4.0801706 -4.0851073 -4.1282344 -4.1847606 -4.2463331 -4.298666 -4.3282185][-4.3272033 -4.326232 -4.3193831 -4.3015504 -4.2747436 -4.24738 -4.2243524 -4.1966152 -4.1706219 -4.1734314 -4.2011161 -4.2416062 -4.2870917 -4.3229589 -4.339982][-4.328866 -4.3310604 -4.3276534 -4.315568 -4.29618 -4.2762494 -4.2587132 -4.2366486 -4.2184572 -4.2200465 -4.2422619 -4.2770414 -4.3114753 -4.333724 -4.3412018][-4.3184724 -4.3248391 -4.32652 -4.3220568 -4.3095441 -4.2930851 -4.27632 -4.2573953 -4.2436934 -4.24603 -4.2650614 -4.293797 -4.3188944 -4.331656 -4.3337235][-4.3030481 -4.314559 -4.3211532 -4.3217583 -4.31423 -4.3012285 -4.2868271 -4.2703748 -4.2606053 -4.2641459 -4.2790532 -4.2992172 -4.3153009 -4.3218102 -4.3221259]]...]
INFO - root - 2017-12-06 06:31:18.144988: step 7410, loss = 2.07, batch loss = 2.02 (13.7 examples/sec; 0.583 sec/batch; 52h:38m:08s remains)
INFO - root - 2017-12-06 06:31:24.827501: step 7420, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 61h:03m:38s remains)
INFO - root - 2017-12-06 06:31:31.610335: step 7430, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.701 sec/batch; 63h:17m:28s remains)
INFO - root - 2017-12-06 06:31:38.381775: step 7440, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 62h:14m:01s remains)
INFO - root - 2017-12-06 06:31:45.142369: step 7450, loss = 2.08, batch loss = 2.03 (11.9 examples/sec; 0.674 sec/batch; 60h:53m:07s remains)
INFO - root - 2017-12-06 06:31:51.846367: step 7460, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 64h:04m:26s remains)
INFO - root - 2017-12-06 06:31:58.630256: step 7470, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 60h:49m:59s remains)
INFO - root - 2017-12-06 06:32:05.253749: step 7480, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 61h:05m:11s remains)
INFO - root - 2017-12-06 06:32:12.043419: step 7490, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 60h:40m:24s remains)
INFO - root - 2017-12-06 06:32:18.900885: step 7500, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.668 sec/batch; 60h:16m:34s remains)
2017-12-06 06:32:19.515512: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3511391 -4.3458657 -4.3414392 -4.3371911 -4.3327284 -4.3178878 -4.2960095 -4.2829537 -4.2892628 -4.2976418 -4.2989206 -4.2951531 -4.2889004 -4.2772632 -4.262898][-4.3496518 -4.3418407 -4.3345451 -4.3269596 -4.311234 -4.28088 -4.2427831 -4.22305 -4.2369323 -4.2600355 -4.2738123 -4.2756152 -4.2677326 -4.2486725 -4.2234225][-4.3428292 -4.3355417 -4.3302541 -4.3199763 -4.2898664 -4.24012 -4.1806588 -4.1492348 -4.1730032 -4.219502 -4.2514634 -4.2638707 -4.2590652 -4.2336507 -4.1928282][-4.3340216 -4.3296828 -4.3277307 -4.3141503 -4.2688212 -4.1964011 -4.1116729 -4.0639658 -4.1013274 -4.1784916 -4.2318249 -4.2549095 -4.2504759 -4.2174168 -4.1665339][-4.3238297 -4.3223572 -4.3209729 -4.299427 -4.2401695 -4.1441526 -4.0317574 -3.9638917 -4.0105915 -4.1228085 -4.2057457 -4.24461 -4.2424631 -4.2040949 -4.1479888][-4.3150129 -4.312984 -4.3055325 -4.2739983 -4.2049246 -4.0917406 -3.9517488 -3.8523591 -3.9007897 -4.0529461 -4.1707382 -4.2281151 -4.2335005 -4.1975689 -4.1453028][-4.3059783 -4.2970343 -4.280407 -4.2423596 -4.1722488 -4.0612717 -3.9067638 -3.7707343 -3.8069234 -3.9885149 -4.1318178 -4.1984229 -4.2066574 -4.1721306 -4.1336646][-4.2848234 -4.2685742 -4.2487264 -4.2107882 -4.1590528 -4.0840383 -3.9683239 -3.8540578 -3.8658748 -4.0059104 -4.122716 -4.174222 -4.1749806 -4.1361775 -4.1091919][-4.2610378 -4.240344 -4.2219028 -4.192699 -4.1690359 -4.1376138 -4.0779037 -4.0120173 -4.0103116 -4.0835228 -4.1489029 -4.1779251 -4.1710644 -4.1282334 -4.1019592][-4.23515 -4.2146344 -4.2009263 -4.1817303 -4.178216 -4.1733665 -4.1425385 -4.1017928 -4.095479 -4.1363335 -4.1732631 -4.1865492 -4.1741915 -4.1369061 -4.1120753][-4.2112422 -4.1919966 -4.1834731 -4.1748371 -4.1825747 -4.1925774 -4.1861019 -4.1645317 -4.1553097 -4.176146 -4.196692 -4.2037439 -4.1949539 -4.1661692 -4.1461725][-4.2008195 -4.1826634 -4.179203 -4.1759405 -4.1855865 -4.2042451 -4.2160425 -4.2094679 -4.20229 -4.2119026 -4.223402 -4.2298708 -4.2275848 -4.2129579 -4.2000141][-4.2121024 -4.1980658 -4.2005653 -4.2015777 -4.2105861 -4.2307158 -4.248734 -4.2488136 -4.2458148 -4.2508268 -4.2582526 -4.2646294 -4.2670546 -4.264111 -4.2583065][-4.2449512 -4.2375884 -4.2448206 -4.24959 -4.256176 -4.2715306 -4.2875395 -4.2912216 -4.2907724 -4.2951269 -4.2991323 -4.3056078 -4.3094778 -4.3114557 -4.3099313][-4.2877183 -4.2837763 -4.2900667 -4.2929549 -4.2956009 -4.3045278 -4.3158669 -4.3218236 -4.324471 -4.3272686 -4.3294153 -4.3338013 -4.3366489 -4.33736 -4.3357091]]...]
INFO - root - 2017-12-06 06:32:26.245612: step 7510, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 61h:39m:33s remains)
INFO - root - 2017-12-06 06:32:33.056241: step 7520, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.673 sec/batch; 60h:43m:19s remains)
INFO - root - 2017-12-06 06:32:39.948638: step 7530, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 61h:45m:36s remains)
INFO - root - 2017-12-06 06:32:46.871818: step 7540, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 60h:01m:08s remains)
INFO - root - 2017-12-06 06:32:53.603996: step 7550, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 60h:51m:32s remains)
INFO - root - 2017-12-06 06:33:00.285062: step 7560, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 60h:33m:10s remains)
INFO - root - 2017-12-06 06:33:07.041413: step 7570, loss = 2.10, batch loss = 2.04 (12.0 examples/sec; 0.667 sec/batch; 60h:11m:11s remains)
INFO - root - 2017-12-06 06:33:13.923690: step 7580, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.711 sec/batch; 64h:07m:48s remains)
INFO - root - 2017-12-06 06:33:20.776971: step 7590, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.693 sec/batch; 62h:30m:02s remains)
INFO - root - 2017-12-06 06:33:27.495226: step 7600, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 58h:51m:06s remains)
2017-12-06 06:33:28.206833: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2180705 -4.2461271 -4.2496371 -4.2374582 -4.2415886 -4.2545094 -4.2462282 -4.2190943 -4.2032619 -4.2186317 -4.2334232 -4.2452984 -4.2623582 -4.27268 -4.2676105][-4.2406693 -4.2757335 -4.2810316 -4.2667775 -4.2651238 -4.2697763 -4.2544575 -4.2208271 -4.2027712 -4.2177606 -4.2310648 -4.2429452 -4.2593584 -4.2717185 -4.2686763][-4.2642269 -4.2986741 -4.2995648 -4.2738552 -4.2578621 -4.25101 -4.2308292 -4.1964178 -4.1800895 -4.1986289 -4.2172461 -4.2340174 -4.2547956 -4.2705579 -4.2716084][-4.2807822 -4.3116827 -4.3065696 -4.2724319 -4.2446465 -4.2223864 -4.191977 -4.1530933 -4.137794 -4.1638122 -4.1913638 -4.2160125 -4.2457957 -4.26754 -4.2764654][-4.2816 -4.3029842 -4.2905474 -4.2478843 -4.2065053 -4.1634321 -4.1102433 -4.0619583 -4.0515079 -4.0920825 -4.1378055 -4.1754322 -4.2157645 -4.2500277 -4.2717147][-4.2601366 -4.2680116 -4.2445631 -4.1922374 -4.1344547 -4.0674462 -3.9910169 -3.9394774 -3.9431806 -4.0047207 -4.0739627 -4.1246524 -4.1759892 -4.2271662 -4.2606359][-4.22017 -4.2127662 -4.180542 -4.1235032 -4.0566936 -3.9816029 -3.9027274 -3.8606727 -3.8869078 -3.96524 -4.0464535 -4.1020341 -4.1556568 -4.2126484 -4.2515869][-4.1877069 -4.1691442 -4.133863 -4.07911 -4.02035 -3.9676306 -3.918952 -3.8994608 -3.9410603 -4.0207114 -4.0915313 -4.1328745 -4.1725211 -4.2207885 -4.25699][-4.1833038 -4.1608768 -4.1257706 -4.0771785 -4.0369821 -4.0169859 -4.0008144 -3.9934394 -4.0295506 -4.0934787 -4.1493673 -4.1792245 -4.2066402 -4.2418156 -4.2710629][-4.2016587 -4.1801448 -4.1460333 -4.1055269 -4.0829368 -4.0852046 -4.0857697 -4.0815868 -4.1035309 -4.1452236 -4.1867056 -4.2114954 -4.2341828 -4.2621784 -4.2851033][-4.2254672 -4.2077475 -4.1812325 -4.1553349 -4.1469321 -4.1569118 -4.1626697 -4.1591454 -4.16791 -4.1926785 -4.22191 -4.2427778 -4.2611647 -4.2819014 -4.2987337][-4.2598133 -4.2463355 -4.2283421 -4.2154131 -4.2139964 -4.2228742 -4.2278533 -4.2235141 -4.2238097 -4.2372584 -4.2570033 -4.2742038 -4.2892714 -4.3034968 -4.3130465][-4.3005462 -4.291122 -4.2789707 -4.2711945 -4.271152 -4.2765174 -4.2794604 -4.2761045 -4.2734942 -4.2809362 -4.29384 -4.3060384 -4.3162894 -4.3233566 -4.3261442][-4.3242459 -4.3195081 -4.3134165 -4.3093333 -4.3085284 -4.3106003 -4.312799 -4.3111024 -4.3086205 -4.3127408 -4.3200812 -4.3264446 -4.3306904 -4.3322439 -4.3316941][-4.32328 -4.3228106 -4.3219414 -4.32146 -4.321548 -4.3222752 -4.3238263 -4.3237514 -4.3225594 -4.3246803 -4.3286948 -4.3319578 -4.3342166 -4.3342013 -4.3326321]]...]
INFO - root - 2017-12-06 06:33:34.911095: step 7610, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.691 sec/batch; 62h:19m:51s remains)
INFO - root - 2017-12-06 06:33:41.579154: step 7620, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.635 sec/batch; 57h:19m:55s remains)
INFO - root - 2017-12-06 06:33:48.237654: step 7630, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 60h:50m:44s remains)
INFO - root - 2017-12-06 06:33:55.012340: step 7640, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 61h:14m:54s remains)
INFO - root - 2017-12-06 06:34:01.758331: step 7650, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 59h:31m:50s remains)
INFO - root - 2017-12-06 06:34:08.511290: step 7660, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.679 sec/batch; 61h:15m:50s remains)
INFO - root - 2017-12-06 06:34:15.353778: step 7670, loss = 2.04, batch loss = 1.99 (11.6 examples/sec; 0.689 sec/batch; 62h:09m:34s remains)
INFO - root - 2017-12-06 06:34:22.148775: step 7680, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 60h:46m:06s remains)
INFO - root - 2017-12-06 06:34:28.867481: step 7690, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 59h:09m:04s remains)
INFO - root - 2017-12-06 06:34:35.595231: step 7700, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.675 sec/batch; 60h:52m:36s remains)
2017-12-06 06:34:36.200694: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2303243 -4.246438 -4.2579446 -4.2503567 -4.2337127 -4.2182775 -4.2216258 -4.24795 -4.2804255 -4.3037047 -4.3219285 -4.3324685 -4.326478 -4.304369 -4.2815104][-4.175 -4.2042093 -4.2241282 -4.2155085 -4.199893 -4.1847749 -4.1891675 -4.2190871 -4.2548223 -4.2772279 -4.2890964 -4.2948055 -4.286943 -4.2629528 -4.2368989][-4.111865 -4.152317 -4.1829047 -4.1749067 -4.1630678 -4.1545453 -4.1617756 -4.1907716 -4.2232437 -4.2413292 -4.2480392 -4.2513556 -4.2477107 -4.2286148 -4.2076092][-4.0831189 -4.1286597 -4.1635494 -4.1582489 -4.1504645 -4.1469369 -4.1529813 -4.171988 -4.19185 -4.2038121 -4.207768 -4.2124686 -4.2167244 -4.2097063 -4.2025857][-4.1334505 -4.16893 -4.1922779 -4.18607 -4.1771984 -4.1670442 -4.1551805 -4.155766 -4.1657653 -4.1758685 -4.1794796 -4.184844 -4.1960182 -4.2007504 -4.2062511][-4.2074637 -4.2207284 -4.21952 -4.2019515 -4.1833982 -4.15584 -4.1211734 -4.1056366 -4.1133962 -4.1249347 -4.1312947 -4.1395535 -4.1566958 -4.1691074 -4.1899657][-4.2398086 -4.2368803 -4.2146153 -4.1784353 -4.1418104 -4.0908551 -4.0395608 -4.0181274 -4.0311565 -4.04946 -4.0598974 -4.0784311 -4.1053605 -4.1289639 -4.165441][-4.2452917 -4.2385111 -4.2029495 -4.1489477 -4.0912485 -4.0203657 -3.9659991 -3.9512563 -3.9745848 -4.0033975 -4.0226173 -4.0450683 -4.0750957 -4.1107421 -4.1602845][-4.2448006 -4.2405396 -4.2030988 -4.1436186 -4.0760808 -4.0049829 -3.9599004 -3.952559 -3.9766915 -4.0155787 -4.0484757 -4.0719914 -4.0992689 -4.14061 -4.1953039][-4.2429333 -4.2437148 -4.2128992 -4.1663909 -4.1145372 -4.0626106 -4.033689 -4.0330162 -4.0542903 -4.0988665 -4.139009 -4.1623535 -4.1857553 -4.2216406 -4.2608747][-4.23635 -4.241303 -4.2199631 -4.194283 -4.1716418 -4.1498785 -4.1413012 -4.1504846 -4.1730828 -4.211791 -4.2418151 -4.25743 -4.2713161 -4.2908788 -4.3102365][-4.2178345 -4.2256479 -4.2207804 -4.2220297 -4.23146 -4.2375908 -4.2447429 -4.257865 -4.2775731 -4.3013015 -4.312048 -4.3149409 -4.31773 -4.3271394 -4.3362622][-4.177278 -4.1904435 -4.2069478 -4.2352614 -4.2700024 -4.2939887 -4.307189 -4.3142171 -4.3252778 -4.3367538 -4.3380585 -4.3365045 -4.3371716 -4.3429437 -4.3487568][-4.1245923 -4.1429977 -4.1764908 -4.2256346 -4.2762418 -4.3073864 -4.3193336 -4.321332 -4.3269377 -4.3345451 -4.335979 -4.3363748 -4.3390183 -4.3437171 -4.3473496][-4.0880957 -4.1075239 -4.1457224 -4.2018371 -4.257453 -4.2915821 -4.3051276 -4.31071 -4.3164449 -4.3232551 -4.3263388 -4.3284183 -4.3310223 -4.3333416 -4.3349805]]...]
INFO - root - 2017-12-06 06:34:42.975705: step 7710, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 61h:39m:19s remains)
INFO - root - 2017-12-06 06:34:49.760715: step 7720, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 62h:03m:05s remains)
INFO - root - 2017-12-06 06:34:56.453706: step 7730, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 60h:49m:25s remains)
INFO - root - 2017-12-06 06:35:03.201066: step 7740, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 58h:22m:51s remains)
INFO - root - 2017-12-06 06:35:09.917532: step 7750, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 62h:15m:14s remains)
INFO - root - 2017-12-06 06:35:16.730583: step 7760, loss = 2.10, batch loss = 2.04 (11.6 examples/sec; 0.689 sec/batch; 62h:09m:18s remains)
INFO - root - 2017-12-06 06:35:23.629137: step 7770, loss = 2.05, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 60h:17m:03s remains)
INFO - root - 2017-12-06 06:35:30.345421: step 7780, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 61h:39m:26s remains)
INFO - root - 2017-12-06 06:35:37.234373: step 7790, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 62h:52m:28s remains)
INFO - root - 2017-12-06 06:35:43.826161: step 7800, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 63h:14m:38s remains)
2017-12-06 06:35:44.486558: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2569914 -4.2436051 -4.2366118 -4.2273531 -4.2170191 -4.2173724 -4.2222266 -4.2223949 -4.2194333 -4.2164493 -4.2201529 -4.2335157 -4.2529626 -4.2699142 -4.277338][-4.2191582 -4.2057705 -4.2074871 -4.2058139 -4.2017174 -4.2077351 -4.214879 -4.2138443 -4.2098627 -4.2050729 -4.2057452 -4.214807 -4.2348318 -4.2549534 -4.2635303][-4.1822615 -4.1685252 -4.1829672 -4.1963539 -4.2010217 -4.2019529 -4.2007957 -4.1954713 -4.1960268 -4.1973076 -4.1999421 -4.2077351 -4.2239914 -4.2431235 -4.2540488][-4.1652017 -4.1532655 -4.1711679 -4.1875072 -4.1894822 -4.1776505 -4.1597023 -4.1489754 -4.1633554 -4.1800895 -4.1925483 -4.2029333 -4.2144451 -4.2315583 -4.2458906][-4.1773729 -4.1713367 -4.184104 -4.1925855 -4.1859822 -4.1582179 -4.1232405 -4.1112475 -4.1458287 -4.1783686 -4.2008047 -4.21468 -4.2242241 -4.2350125 -4.2472258][-4.1900849 -4.1873088 -4.1943164 -4.1928306 -4.1738429 -4.130074 -4.0741181 -4.051558 -4.1083179 -4.1667075 -4.2007232 -4.2230477 -4.2335033 -4.2401919 -4.2535887][-4.189445 -4.1894941 -4.1912189 -4.17628 -4.1350651 -4.0628691 -3.9717097 -3.9228227 -4.00337 -4.1035132 -4.1635156 -4.2010865 -4.2177224 -4.2261305 -4.2437692][-4.1939726 -4.1960855 -4.1890869 -4.1544552 -4.0840025 -3.9755573 -3.83526 -3.745338 -3.8554425 -4.0030761 -4.0919309 -4.144948 -4.17059 -4.184444 -4.2027254][-4.2110324 -4.2178445 -4.2074766 -4.1642842 -4.0878725 -3.981163 -3.8459868 -3.7561307 -3.8571756 -4.0049644 -4.0954237 -4.1445971 -4.1672392 -4.1770573 -4.184124][-4.2386513 -4.25238 -4.2442904 -4.2101126 -4.155283 -4.0846572 -4.003108 -3.9464035 -4.0006037 -4.1033764 -4.1726561 -4.2066269 -4.2203016 -4.2260251 -4.2257013][-4.2521253 -4.2722254 -4.2719817 -4.2527714 -4.2194748 -4.1756792 -4.1272869 -4.08829 -4.111877 -4.1757817 -4.226727 -4.251524 -4.2616453 -4.2682686 -4.2691326][-4.2638702 -4.2873931 -4.2933207 -4.282392 -4.2591076 -4.2275786 -4.193882 -4.1671395 -4.1760535 -4.2157621 -4.2509465 -4.2681384 -4.2756343 -4.2811508 -4.28344][-4.2749696 -4.3022566 -4.3134885 -4.3036628 -4.2831993 -4.2610111 -4.2427025 -4.2307835 -4.2370181 -4.2577982 -4.276041 -4.283906 -4.2852874 -4.2867789 -4.2857633][-4.2815728 -4.3058004 -4.320322 -4.3144422 -4.2985897 -4.284647 -4.2772045 -4.2763357 -4.2804737 -4.2871108 -4.2933292 -4.2951069 -4.2947011 -4.2952194 -4.2929506][-4.2825975 -4.3016167 -4.3141956 -4.3115211 -4.2998519 -4.2910576 -4.291328 -4.2962918 -4.30039 -4.3009458 -4.3015718 -4.3024359 -4.3033757 -4.3050137 -4.3047519]]...]
INFO - root - 2017-12-06 06:35:51.173494: step 7810, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.668 sec/batch; 60h:14m:01s remains)
INFO - root - 2017-12-06 06:35:57.955574: step 7820, loss = 2.04, batch loss = 1.99 (11.6 examples/sec; 0.692 sec/batch; 62h:25m:54s remains)
INFO - root - 2017-12-06 06:36:04.792987: step 7830, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 64h:24m:29s remains)
INFO - root - 2017-12-06 06:36:11.314182: step 7840, loss = 2.03, batch loss = 1.97 (13.2 examples/sec; 0.605 sec/batch; 54h:35m:59s remains)
INFO - root - 2017-12-06 06:36:18.058495: step 7850, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 62h:57m:19s remains)
INFO - root - 2017-12-06 06:36:24.894698: step 7860, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.679 sec/batch; 61h:12m:44s remains)
INFO - root - 2017-12-06 06:36:31.766179: step 7870, loss = 2.05, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 62h:33m:34s remains)
INFO - root - 2017-12-06 06:36:38.497232: step 7880, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 61h:08m:25s remains)
INFO - root - 2017-12-06 06:36:45.293840: step 7890, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 60h:45m:47s remains)
INFO - root - 2017-12-06 06:36:51.999770: step 7900, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 62h:39m:51s remains)
2017-12-06 06:36:52.787290: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2478862 -4.2283344 -4.2200456 -4.2207127 -4.2277179 -4.2324934 -4.2361689 -4.2362094 -4.2350435 -4.2412977 -4.2571483 -4.2715845 -4.2811284 -4.2802424 -4.2753987][-4.2073951 -4.1793518 -4.171577 -4.1793838 -4.1902552 -4.1939378 -4.1954565 -4.1953907 -4.1958756 -4.2041407 -4.2250824 -4.244473 -4.2568831 -4.2549973 -4.2453752][-4.1832595 -4.1521754 -4.1500545 -4.1666369 -4.1795516 -4.1780105 -4.1731186 -4.1697397 -4.170372 -4.181057 -4.2053952 -4.2280936 -4.2420306 -4.2391181 -4.2255287][-4.1756158 -4.1496048 -4.1549025 -4.1758533 -4.1885443 -4.1811862 -4.1678362 -4.1560888 -4.1514382 -4.1612277 -4.1865807 -4.2128325 -4.229136 -4.2271748 -4.2130237][-4.1776319 -4.1609764 -4.1686063 -4.1867223 -4.1955175 -4.184742 -4.1656175 -4.1482863 -4.1373625 -4.1402988 -4.1629663 -4.1911907 -4.2101 -4.2114329 -4.2030182][-4.1883965 -4.18041 -4.1863952 -4.1953764 -4.1977944 -4.1837454 -4.1643033 -4.1452603 -4.1305013 -4.1261044 -4.1432371 -4.1696992 -4.1899643 -4.1977553 -4.2008157][-4.1970115 -4.19297 -4.1965361 -4.1967287 -4.1922188 -4.1778746 -4.1603813 -4.1419492 -4.1253681 -4.1160722 -4.1277442 -4.1495247 -4.169878 -4.1837745 -4.1975055][-4.195703 -4.1920562 -4.1936531 -4.189611 -4.1834111 -4.1729264 -4.1606622 -4.1481667 -4.134891 -4.1235037 -4.1294332 -4.1442175 -4.1615443 -4.1760006 -4.1928287][-4.1802626 -4.1734376 -4.1755838 -4.1734972 -4.1713667 -4.1683717 -4.1648717 -4.161026 -4.1512852 -4.1361561 -4.1388674 -4.1471596 -4.1585245 -4.1672306 -4.1815038][-4.160903 -4.152513 -4.1594486 -4.1642246 -4.1710725 -4.1753449 -4.1763239 -4.1763225 -4.1671844 -4.1470432 -4.1446738 -4.1468344 -4.1479707 -4.1453242 -4.1560187][-4.1465125 -4.1381354 -4.1520853 -4.1643071 -4.1785607 -4.1850586 -4.1870646 -4.1882706 -4.1795979 -4.1574073 -4.1485248 -4.1460638 -4.1365452 -4.1200438 -4.1240258][-4.1397266 -4.1299572 -4.1471992 -4.1648874 -4.1819172 -4.1888909 -4.1894822 -4.1895752 -4.1808643 -4.1596141 -4.1473784 -4.1417727 -4.1292048 -4.1067796 -4.1067557][-4.1354494 -4.1255264 -4.1434631 -4.1647296 -4.1844988 -4.1939359 -4.1940103 -4.1910281 -4.1780109 -4.1589928 -4.1462717 -4.1396456 -4.1315513 -4.115284 -4.11689][-4.1442518 -4.1356 -4.1511345 -4.1713843 -4.1916127 -4.2026753 -4.2025228 -4.1944246 -4.1784973 -4.1637297 -4.1554513 -4.1523871 -4.151866 -4.1464081 -4.1503506][-4.1695747 -4.1646185 -4.1763453 -4.1912847 -4.2068539 -4.2172446 -4.2179804 -4.2070808 -4.1917791 -4.1798906 -4.176065 -4.1770344 -4.1784811 -4.1773009 -4.1816421]]...]
INFO - root - 2017-12-06 06:36:59.470300: step 7910, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 57h:12m:23s remains)
INFO - root - 2017-12-06 06:37:06.220893: step 7920, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.675 sec/batch; 60h:49m:31s remains)
INFO - root - 2017-12-06 06:37:12.845980: step 7930, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 61h:01m:12s remains)
INFO - root - 2017-12-06 06:37:19.538485: step 7940, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 59h:13m:08s remains)
INFO - root - 2017-12-06 06:37:26.261880: step 7950, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 60h:47m:25s remains)
INFO - root - 2017-12-06 06:37:33.007993: step 7960, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.658 sec/batch; 59h:18m:39s remains)
INFO - root - 2017-12-06 06:37:39.663736: step 7970, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 59h:00m:42s remains)
INFO - root - 2017-12-06 06:37:46.432453: step 7980, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.675 sec/batch; 60h:50m:35s remains)
INFO - root - 2017-12-06 06:37:53.173209: step 7990, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 60h:33m:05s remains)
INFO - root - 2017-12-06 06:37:59.836909: step 8000, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.676 sec/batch; 60h:56m:15s remains)
2017-12-06 06:38:00.520090: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2215371 -4.2196193 -4.2213826 -4.2217417 -4.214931 -4.2109346 -4.2203407 -4.2331729 -4.2438965 -4.25412 -4.2664037 -4.2734442 -4.272747 -4.2681422 -4.2617841][-4.210012 -4.2031779 -4.2041345 -4.2025156 -4.1931124 -4.19108 -4.20384 -4.2205596 -4.2338452 -4.2472839 -4.2648125 -4.2764649 -4.2757263 -4.2670665 -4.2564211][-4.1953049 -4.1832747 -4.1809068 -4.1745276 -4.1596127 -4.1529493 -4.1621485 -4.1812854 -4.2052321 -4.2315712 -4.259521 -4.277122 -4.2780623 -4.2679825 -4.2558689][-4.1796503 -4.1621203 -4.1540403 -4.1406741 -4.1183553 -4.1035113 -4.1045561 -4.1239195 -4.15951 -4.2027435 -4.2417941 -4.2661052 -4.2731509 -4.2683063 -4.2603669][-4.1706905 -4.1524811 -4.1398683 -4.1210303 -4.0922813 -4.0658617 -4.0519762 -4.066587 -4.1113658 -4.1702514 -4.2172 -4.2479444 -4.2631836 -4.2675433 -4.2660489][-4.1668515 -4.1502175 -4.1331263 -4.1117535 -4.0815649 -4.0448394 -4.013545 -4.0178642 -4.0697079 -4.14067 -4.1916695 -4.2255635 -4.2463984 -4.2570238 -4.2594061][-4.1635556 -4.1450214 -4.1208906 -4.0937495 -4.0575218 -4.0063868 -3.9538171 -3.944926 -4.0054336 -4.0912228 -4.1518669 -4.1928506 -4.2234454 -4.2429762 -4.250989][-4.1627083 -4.1404061 -4.1072416 -4.0695972 -4.0256238 -3.9596024 -3.8834796 -3.8618076 -3.9355946 -4.0336108 -4.1026316 -4.1533566 -4.1990175 -4.2328563 -4.2521191][-4.1629648 -4.1401553 -4.1048546 -4.0635986 -4.0225363 -3.9615092 -3.8875239 -3.8622828 -3.9260101 -4.0080819 -4.0688272 -4.1202 -4.1718855 -4.2136168 -4.2445364][-4.1620536 -4.1409883 -4.1095266 -4.0732627 -4.0450492 -4.00926 -3.9634778 -3.9458482 -3.9807944 -4.0284834 -4.06729 -4.1055169 -4.1472015 -4.18583 -4.22099][-4.1563578 -4.1326222 -4.1024275 -4.0725327 -4.0582128 -4.0488482 -4.0306234 -4.0220623 -4.0370097 -4.061079 -4.0861549 -4.1103029 -4.137712 -4.1701593 -4.2048593][-4.1514158 -4.1232414 -4.0896759 -4.0616145 -4.0580964 -4.069941 -4.0752869 -4.07582 -4.0831861 -4.09978 -4.1194487 -4.1332827 -4.1501756 -4.1775331 -4.2051282][-4.1547866 -4.1238322 -4.0874639 -4.0626812 -4.0704632 -4.0966663 -4.1126294 -4.1142774 -4.1142883 -4.1281891 -4.145978 -4.1546421 -4.1634035 -4.18051 -4.1976395][-4.1760068 -4.1451879 -4.1113334 -4.0932817 -4.1095104 -4.1395259 -4.1516476 -4.1466088 -4.13722 -4.1444316 -4.1591153 -4.1659532 -4.1702981 -4.1794238 -4.186646][-4.2109656 -4.1831827 -4.1549335 -4.1440945 -4.1628976 -4.1855321 -4.1854172 -4.1699653 -4.1494985 -4.1497703 -4.1651506 -4.1762905 -4.1857314 -4.1939979 -4.1905613]]...]
INFO - root - 2017-12-06 06:38:07.374838: step 8010, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 61h:37m:32s remains)
INFO - root - 2017-12-06 06:38:14.177060: step 8020, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.658 sec/batch; 59h:19m:19s remains)
INFO - root - 2017-12-06 06:38:20.666010: step 8030, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.678 sec/batch; 61h:07m:01s remains)
INFO - root - 2017-12-06 06:38:27.511057: step 8040, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 61h:15m:49s remains)
INFO - root - 2017-12-06 06:38:34.348655: step 8050, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.711 sec/batch; 64h:04m:09s remains)
INFO - root - 2017-12-06 06:38:41.084015: step 8060, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.664 sec/batch; 59h:49m:51s remains)
INFO - root - 2017-12-06 06:38:47.785547: step 8070, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 61h:46m:12s remains)
INFO - root - 2017-12-06 06:38:54.530063: step 8080, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 60h:42m:43s remains)
INFO - root - 2017-12-06 06:39:01.175752: step 8090, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 59h:57m:53s remains)
INFO - root - 2017-12-06 06:39:07.929518: step 8100, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 61h:08m:37s remains)
2017-12-06 06:39:08.610739: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1877542 -4.1475358 -4.1375742 -4.1542315 -4.1745863 -4.1778889 -4.16264 -4.1558142 -4.1521139 -4.144568 -4.1504068 -4.1641808 -4.1856942 -4.2130566 -4.2371807][-4.179523 -4.137846 -4.1224217 -4.1326213 -4.1491451 -4.14794 -4.125659 -4.1191616 -4.1257825 -4.1323919 -4.1495986 -4.1648393 -4.1806145 -4.2051187 -4.2308822][-4.1767344 -4.1385431 -4.1193552 -4.1198707 -4.1300988 -4.12449 -4.099617 -4.0969834 -4.1130815 -4.130897 -4.1585608 -4.1754751 -4.1828876 -4.2006578 -4.22529][-4.1787972 -4.1504521 -4.13042 -4.1193614 -4.1191711 -4.1052461 -4.0789924 -4.08204 -4.1052551 -4.1291294 -4.164577 -4.1851606 -4.1872778 -4.1984854 -4.2209935][-4.1796494 -4.1629977 -4.1446013 -4.125001 -4.1100531 -4.081172 -4.049181 -4.0583196 -4.0929604 -4.1260214 -4.1683984 -4.1927786 -4.192802 -4.1987066 -4.2193809][-4.1716652 -4.1613975 -4.1476622 -4.1262631 -4.0982747 -4.0491619 -3.9998362 -4.0096283 -4.0623813 -4.1133523 -4.1649818 -4.19385 -4.1951747 -4.201468 -4.2216229][-4.1670895 -4.1566739 -4.1430125 -4.12111 -4.0836334 -4.011445 -3.9356 -3.9403527 -4.0148664 -4.0898848 -4.1538854 -4.1880059 -4.1929383 -4.2050085 -4.2280369][-4.1733589 -4.15187 -4.1304789 -4.1061978 -4.0653505 -3.98087 -3.8849707 -3.8828716 -3.9758286 -4.0703454 -4.1415462 -4.1788945 -4.1890869 -4.2083297 -4.2365928][-4.1877928 -4.1509466 -4.1186953 -4.0940824 -4.0627704 -3.9900529 -3.9052517 -3.9028313 -3.98864 -4.0785122 -4.142767 -4.1778502 -4.1928463 -4.2180696 -4.249208][-4.2073188 -4.1621995 -4.1255088 -4.1049323 -4.0905938 -4.0459657 -3.99486 -4.0014453 -4.0628648 -4.1260586 -4.1713905 -4.199132 -4.2141843 -4.2386322 -4.2655864][-4.2182808 -4.174458 -4.1416807 -4.1300592 -4.134089 -4.1169081 -4.0953817 -4.1103339 -4.149498 -4.1835465 -4.2068567 -4.2239885 -4.2354527 -4.2555017 -4.2756534][-4.2144632 -4.1765237 -4.1508336 -4.1490579 -4.1681356 -4.1735458 -4.1703157 -4.1853275 -4.2065063 -4.2205739 -4.2289233 -4.2380919 -4.2458563 -4.2622771 -4.277667][-4.2052169 -4.1759949 -4.1586146 -4.1629348 -4.1895885 -4.2082725 -4.2135434 -4.222774 -4.2295246 -4.2309256 -4.2313385 -4.23492 -4.2406836 -4.2559724 -4.2702513][-4.2059097 -4.1873708 -4.1781616 -4.1846008 -4.2083545 -4.2283525 -4.2357483 -4.2392278 -4.23732 -4.2309222 -4.2272897 -4.2276778 -4.2335367 -4.2487159 -4.2611756][-4.2212033 -4.2140183 -4.2118998 -4.2162256 -4.2316265 -4.2475371 -4.2533584 -4.2519107 -4.2443914 -4.2321854 -4.2252684 -4.2244196 -4.2302361 -4.2431374 -4.2507253]]...]
INFO - root - 2017-12-06 06:39:15.318731: step 8110, loss = 2.03, batch loss = 1.97 (11.8 examples/sec; 0.678 sec/batch; 61h:04m:31s remains)
INFO - root - 2017-12-06 06:39:21.946270: step 8120, loss = 2.05, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 64h:19m:56s remains)
INFO - root - 2017-12-06 06:39:28.752421: step 8130, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 58h:33m:00s remains)
INFO - root - 2017-12-06 06:39:35.413307: step 8140, loss = 2.01, batch loss = 1.96 (11.9 examples/sec; 0.674 sec/batch; 60h:43m:22s remains)
INFO - root - 2017-12-06 06:39:42.178096: step 8150, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.694 sec/batch; 62h:29m:05s remains)
INFO - root - 2017-12-06 06:39:49.019471: step 8160, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 63h:22m:17s remains)
INFO - root - 2017-12-06 06:39:55.734311: step 8170, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.667 sec/batch; 60h:05m:31s remains)
INFO - root - 2017-12-06 06:40:02.537708: step 8180, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 61h:16m:12s remains)
INFO - root - 2017-12-06 06:40:09.089472: step 8190, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.705 sec/batch; 63h:30m:13s remains)
INFO - root - 2017-12-06 06:40:15.852288: step 8200, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 60h:51m:21s remains)
2017-12-06 06:40:16.472939: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.267818 -4.26252 -4.26396 -4.2673798 -4.2716341 -4.2704506 -4.26421 -4.2658715 -4.2744145 -4.2821717 -4.2831812 -4.2811117 -4.2757773 -4.2738867 -4.2742686][-4.2508254 -4.2384706 -4.2423267 -4.2515955 -4.2592068 -4.2552824 -4.2438617 -4.2385626 -4.2413058 -4.2490282 -4.2499561 -4.2445045 -4.241199 -4.2460117 -4.2506323][-4.2386708 -4.2223706 -4.2227073 -4.230021 -4.2353039 -4.2250762 -4.2053361 -4.1895919 -4.187439 -4.2005997 -4.20804 -4.2033448 -4.2023878 -4.2090092 -4.215394][-4.2358255 -4.2232084 -4.2196345 -4.2196312 -4.2178802 -4.203928 -4.1803603 -4.1565738 -4.1555514 -4.1778545 -4.1891289 -4.1825662 -4.1812735 -4.1839848 -4.185936][-4.2291474 -4.218997 -4.2130985 -4.2074327 -4.1976957 -4.1792855 -4.1528354 -4.1288743 -4.1287937 -4.1570649 -4.1737127 -4.1710587 -4.1737442 -4.174448 -4.1707211][-4.2124324 -4.1954618 -4.1832104 -4.1761007 -4.164947 -4.1400313 -4.103384 -4.0772033 -4.0854015 -4.1153736 -4.1347361 -4.1487994 -4.1650629 -4.1690702 -4.165123][-4.177474 -4.1483116 -4.1370173 -4.13622 -4.1283207 -4.0876417 -4.0217161 -3.983021 -4.0047903 -4.0490932 -4.0835075 -4.1192107 -4.15501 -4.1688604 -4.1659527][-4.1342864 -4.0996661 -4.095964 -4.1133628 -4.1145415 -4.0683813 -3.9898133 -3.943857 -3.9673531 -4.02623 -4.07707 -4.1270022 -4.1683021 -4.1837807 -4.1791749][-4.1101437 -4.0765095 -4.0851774 -4.1189961 -4.1357832 -4.1031737 -4.0422683 -3.9997611 -4.0077882 -4.0619974 -4.1166415 -4.1659074 -4.1970363 -4.2038617 -4.1938281][-4.1241255 -4.0958686 -4.1134696 -4.1560407 -4.1806231 -4.1610832 -4.1165662 -4.0759611 -4.0700936 -4.1074963 -4.1525526 -4.1934838 -4.2179084 -4.2212849 -4.2103891][-4.1780047 -4.1559567 -4.1748271 -4.2127624 -4.232882 -4.2173676 -4.178947 -4.1413617 -4.1311407 -4.1528754 -4.1828966 -4.2137203 -4.2356825 -4.2445135 -4.2378664][-4.234539 -4.2164497 -4.2302909 -4.2595043 -4.2736278 -4.2619891 -4.2310462 -4.2060628 -4.2032695 -4.2158666 -4.2274661 -4.2405295 -4.2534471 -4.2620072 -4.2604651][-4.2595143 -4.2448254 -4.2593093 -4.279994 -4.2890306 -4.279233 -4.25657 -4.2448268 -4.2520537 -4.2600656 -4.260808 -4.2611604 -4.2632275 -4.2669053 -4.2690248][-4.2699413 -4.264514 -4.2817178 -4.2968364 -4.3006544 -4.28883 -4.2721066 -4.26788 -4.2756853 -4.2793646 -4.2768612 -4.2731223 -4.2737274 -4.2780881 -4.2837982][-4.2911181 -4.2931566 -4.3067694 -4.3147593 -4.3117533 -4.2985611 -4.2852316 -4.2838368 -4.292635 -4.2991552 -4.2980971 -4.2941027 -4.2976685 -4.3035269 -4.3070168]]...]
INFO - root - 2017-12-06 06:40:23.166469: step 8210, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 59h:39m:14s remains)
INFO - root - 2017-12-06 06:40:29.769347: step 8220, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 60h:37m:53s remains)
INFO - root - 2017-12-06 06:40:36.506038: step 8230, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 58h:39m:29s remains)
INFO - root - 2017-12-06 06:40:43.188548: step 8240, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 60h:13m:34s remains)
INFO - root - 2017-12-06 06:40:50.007524: step 8250, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 61h:25m:21s remains)
INFO - root - 2017-12-06 06:40:56.825975: step 8260, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 60h:34m:06s remains)
INFO - root - 2017-12-06 06:41:03.437622: step 8270, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 60h:04m:22s remains)
INFO - root - 2017-12-06 06:41:10.187191: step 8280, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 58h:40m:37s remains)
INFO - root - 2017-12-06 06:41:16.837700: step 8290, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 62h:12m:44s remains)
INFO - root - 2017-12-06 06:41:23.615192: step 8300, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 62h:32m:09s remains)
2017-12-06 06:41:24.279575: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1837196 -4.1678424 -4.174334 -4.1759486 -4.1691222 -4.173914 -4.1948762 -4.2175503 -4.2278233 -4.2316961 -4.2299247 -4.2220674 -4.2127495 -4.2080507 -4.2198424][-4.1824427 -4.1641703 -4.1729627 -4.1723886 -4.16029 -4.1595769 -4.1792173 -4.2047057 -4.2168794 -4.220191 -4.2182107 -4.2056947 -4.1849527 -4.1677122 -4.1815562][-4.1762013 -4.1534276 -4.1599469 -4.159162 -4.1474071 -4.14464 -4.1622334 -4.1889796 -4.2078009 -4.2168489 -4.2191949 -4.204987 -4.1689587 -4.1355515 -4.1489697][-4.1731644 -4.139946 -4.1351342 -4.1289825 -4.1163688 -4.11357 -4.128408 -4.1566682 -4.1810083 -4.1973815 -4.2049165 -4.1924596 -4.1470709 -4.1041722 -4.1149826][-4.1757417 -4.1294265 -4.1033516 -4.0846605 -4.0670142 -4.0586257 -4.0661211 -4.0888414 -4.1176348 -4.14823 -4.1673274 -4.1617651 -4.1170917 -4.0757594 -4.0911469][-4.1758456 -4.1188068 -4.0734429 -4.0402336 -4.0156746 -3.997458 -3.991137 -4.0025911 -4.0350494 -4.0865326 -4.1241975 -4.1252875 -4.0879755 -4.0612159 -4.086328][-4.1786342 -4.1201653 -4.0656676 -4.0252471 -3.9963098 -3.9701164 -3.9478962 -3.9439719 -3.9774849 -4.0457182 -4.10063 -4.1096087 -4.0846267 -4.074542 -4.1092262][-4.18841 -4.1423798 -4.0964994 -4.0607767 -4.0361357 -4.010838 -3.977802 -3.9569397 -3.9784732 -4.0464559 -4.1011996 -4.1085505 -4.0894012 -4.0868697 -4.1267042][-4.1998281 -4.1718903 -4.1448259 -4.1222811 -4.1069665 -4.0854063 -4.0470619 -4.0125608 -4.0145378 -4.0705228 -4.1120648 -4.1117377 -4.0931234 -4.08784 -4.1236877][-4.2093482 -4.1915488 -4.1790838 -4.16675 -4.1583138 -4.1394582 -4.1017556 -4.0629406 -4.0488672 -4.0841727 -4.1113944 -4.1045561 -4.0859623 -4.0733433 -4.0997381][-4.2209868 -4.2039218 -4.1974745 -4.1890626 -4.1808648 -4.1603146 -4.122643 -4.0857944 -4.0696926 -4.0897012 -4.1059523 -4.0989819 -4.0840654 -4.0622444 -4.0791445][-4.2214389 -4.2044458 -4.2041655 -4.2017965 -4.1929235 -4.1691852 -4.1328583 -4.10354 -4.0945654 -4.1050057 -4.111846 -4.1028023 -4.089591 -4.0644608 -4.077343][-4.2126207 -4.1950645 -4.1994848 -4.2074938 -4.2046828 -4.1785083 -4.1398563 -4.118463 -4.1194305 -4.1280766 -4.126895 -4.1168175 -4.1110849 -4.0869384 -4.0957422][-4.2084031 -4.1854935 -4.18821 -4.2026854 -4.2059207 -4.1816692 -4.1459813 -4.1307964 -4.14263 -4.1569819 -4.1565995 -4.1469512 -4.1427917 -4.12411 -4.1290894][-4.2066407 -4.1797204 -4.1784024 -4.1933632 -4.1999216 -4.1831584 -4.1516013 -4.1420407 -4.1615391 -4.1831622 -4.1873837 -4.1765871 -4.1647463 -4.1400971 -4.1414967]]...]
INFO - root - 2017-12-06 06:41:30.777669: step 8310, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 59h:25m:50s remains)
INFO - root - 2017-12-06 06:41:37.670367: step 8320, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 64h:12m:27s remains)
INFO - root - 2017-12-06 06:41:44.366893: step 8330, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 62h:32m:45s remains)
INFO - root - 2017-12-06 06:41:51.198453: step 8340, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.693 sec/batch; 62h:25m:05s remains)
INFO - root - 2017-12-06 06:41:58.149160: step 8350, loss = 2.10, batch loss = 2.05 (11.8 examples/sec; 0.679 sec/batch; 61h:06m:08s remains)
INFO - root - 2017-12-06 06:42:04.941329: step 8360, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 60h:21m:10s remains)
INFO - root - 2017-12-06 06:42:11.779739: step 8370, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 61h:32m:00s remains)
INFO - root - 2017-12-06 06:42:18.448854: step 8380, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 0.526 sec/batch; 47h:20m:43s remains)
INFO - root - 2017-12-06 06:42:25.201916: step 8390, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 60h:43m:07s remains)
INFO - root - 2017-12-06 06:42:32.041272: step 8400, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 61h:35m:29s remains)
2017-12-06 06:42:32.756542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.357728 -4.3686275 -4.3707414 -4.3418379 -4.2759142 -4.1919494 -4.1267447 -4.1039023 -4.1295629 -4.1799803 -4.2320271 -4.2695894 -4.2896981 -4.2912679 -4.2851038][-4.3671231 -4.3806744 -4.3882351 -4.3690538 -4.3180146 -4.2440052 -4.1749454 -4.1330066 -4.1313367 -4.1584511 -4.2015395 -4.2439728 -4.2768126 -4.2911844 -4.2931657][-4.3772073 -4.3922186 -4.4005189 -4.3852057 -4.342288 -4.2740397 -4.1991792 -4.1415157 -4.1163745 -4.1243372 -4.1650381 -4.2158694 -4.2634187 -4.2950606 -4.3100529][-4.3830953 -4.3975372 -4.4043584 -4.3914189 -4.3516312 -4.2858882 -4.2062697 -4.1350942 -4.0899515 -4.0829778 -4.1257157 -4.184155 -4.2463045 -4.29618 -4.3270884][-4.3853703 -4.3983912 -4.4032893 -4.3917084 -4.3522639 -4.2865357 -4.2030144 -4.1185966 -4.0506816 -4.0277467 -4.0746546 -4.1426311 -4.2172666 -4.2823172 -4.3282084][-4.3862658 -4.396934 -4.3964324 -4.37893 -4.3341222 -4.2625847 -4.1723838 -4.0703974 -3.9778657 -3.9440954 -4.0054345 -4.0905948 -4.1789675 -4.2570953 -4.3147073][-4.3844309 -4.3923631 -4.3849454 -4.3558722 -4.298377 -4.2167044 -4.11767 -3.999505 -3.8831463 -3.8476026 -3.9330015 -4.038249 -4.1388683 -4.2269778 -4.294795][-4.3803535 -4.3838296 -4.36804 -4.3260317 -4.2573333 -4.171216 -4.073987 -3.9577417 -3.8423114 -3.8165522 -3.9096746 -4.0173397 -4.117 -4.2060943 -4.2780318][-4.3748918 -4.3731356 -4.3511209 -4.3022537 -4.230721 -4.1506219 -4.069315 -3.9770207 -3.8915281 -3.8843484 -3.9601831 -4.0466061 -4.1305227 -4.2101941 -4.2762651][-4.3699913 -4.3633375 -4.339396 -4.29092 -4.2244129 -4.1559887 -4.0958295 -4.0342035 -3.9845779 -3.99294 -4.046802 -4.1079082 -4.173213 -4.2388234 -4.2915211][-4.3668222 -4.3587966 -4.3376932 -4.2961173 -4.24176 -4.1876488 -4.1464987 -4.1133943 -4.0935631 -4.1078815 -4.1415019 -4.1828794 -4.2309489 -4.2801175 -4.3149128][-4.365684 -4.360321 -4.3465753 -4.3175449 -4.2789412 -4.2403469 -4.2146358 -4.2017021 -4.1992779 -4.212019 -4.2311363 -4.2566218 -4.2867846 -4.3177123 -4.3334956][-4.364121 -4.3624058 -4.3569269 -4.3414483 -4.3185024 -4.2948484 -4.2818923 -4.2795172 -4.2841277 -4.2938871 -4.3050594 -4.3193641 -4.3344469 -4.3467336 -4.345048][-4.3614011 -4.3617992 -4.3616071 -4.3567362 -4.3475971 -4.3375735 -4.3322763 -4.3337455 -4.3391643 -4.3452287 -4.3519015 -4.358633 -4.361444 -4.3586164 -4.3440318][-4.35818 -4.3593478 -4.3607183 -4.3607926 -4.3597145 -4.3579807 -4.3576827 -4.3599916 -4.3630342 -4.3661861 -4.36906 -4.3690157 -4.3618846 -4.3489137 -4.3268638]]...]
INFO - root - 2017-12-06 06:42:39.510429: step 8410, loss = 2.05, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 61h:51m:26s remains)
INFO - root - 2017-12-06 06:42:46.227855: step 8420, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.662 sec/batch; 59h:37m:16s remains)
INFO - root - 2017-12-06 06:42:53.056534: step 8430, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 61h:05m:59s remains)
INFO - root - 2017-12-06 06:42:59.843729: step 8440, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 58h:50m:38s remains)
INFO - root - 2017-12-06 06:43:06.684556: step 8450, loss = 2.03, batch loss = 1.97 (11.7 examples/sec; 0.682 sec/batch; 61h:24m:24s remains)
INFO - root - 2017-12-06 06:43:13.453183: step 8460, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 62h:43m:28s remains)
INFO - root - 2017-12-06 06:43:20.381017: step 8470, loss = 2.04, batch loss = 1.99 (11.3 examples/sec; 0.711 sec/batch; 63h:58m:30s remains)
INFO - root - 2017-12-06 06:43:27.116376: step 8480, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 59h:59m:55s remains)
INFO - root - 2017-12-06 06:43:33.950011: step 8490, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 61h:09m:47s remains)
INFO - root - 2017-12-06 06:43:40.613626: step 8500, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 63h:03m:01s remains)
2017-12-06 06:43:41.249434: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3341417 -4.3339329 -4.3318362 -4.3314209 -4.3322859 -4.3343849 -4.3364115 -4.3373466 -4.3373356 -4.3387814 -4.3406219 -4.3412213 -4.340755 -4.3391938 -4.3356557][-4.3397427 -4.3412209 -4.3387136 -4.3370113 -4.3367276 -4.3374658 -4.3378468 -4.3372078 -4.336081 -4.3365526 -4.3376913 -4.3381958 -4.3382254 -4.3380456 -4.3356957][-4.3314691 -4.3332195 -4.3309107 -4.329381 -4.3294258 -4.3302207 -4.3302402 -4.3293557 -4.3284211 -4.3284688 -4.3291283 -4.3302536 -4.3315196 -4.3326335 -4.3307023][-4.3214545 -4.3212214 -4.3164191 -4.3136282 -4.3137083 -4.3147478 -4.3148642 -4.3144269 -4.3146873 -4.3159571 -4.3181281 -4.3215561 -4.325573 -4.3287086 -4.3276544][-4.3080997 -4.3029413 -4.2919469 -4.2844677 -4.282023 -4.2814569 -4.2796025 -4.2780848 -4.2789612 -4.282753 -4.2893338 -4.2976189 -4.3056273 -4.3112817 -4.3105836][-4.2879114 -4.2756329 -4.2562828 -4.2405605 -4.2321568 -4.2275605 -4.2211223 -4.2158394 -4.2155128 -4.2225871 -4.2372108 -4.2534695 -4.2664385 -4.273973 -4.273366][-4.2606645 -4.2435451 -4.2159228 -4.1894736 -4.171032 -4.157402 -4.1412315 -4.1271839 -4.1232467 -4.1344552 -4.1603427 -4.1870494 -4.20669 -4.2170296 -4.2175455][-4.24914 -4.2341738 -4.2023454 -4.1655507 -4.1354189 -4.1091442 -4.077971 -4.0501866 -4.0372405 -4.0470214 -4.0780544 -4.1104965 -4.1333261 -4.1451254 -4.1469417][-4.2260919 -4.2204804 -4.1951704 -4.1629648 -4.137291 -4.1162062 -4.0886097 -4.0625453 -4.0466666 -4.0500083 -4.0729 -4.0982747 -4.1148911 -4.1225333 -4.1229992][-4.2068577 -4.2074409 -4.1904311 -4.1693616 -4.1568227 -4.1509161 -4.1412563 -4.1328521 -4.127233 -4.1305413 -4.1450186 -4.1614819 -4.1713333 -4.1759963 -4.1753712][-4.2342291 -4.23786 -4.2281885 -4.2160959 -4.2104764 -4.2104011 -4.2111206 -4.2141986 -4.2170596 -4.2213697 -4.2282505 -4.2359629 -4.2398024 -4.2414627 -4.24085][-4.2760119 -4.2808709 -4.2768583 -4.270978 -4.268755 -4.2693458 -4.2727232 -4.2790694 -4.2853432 -4.2902241 -4.2931662 -4.2957139 -4.2973537 -4.2975736 -4.2971563][-4.3121834 -4.3155079 -4.3139071 -4.3115649 -4.310163 -4.3091712 -4.3106742 -4.315022 -4.3199458 -4.323709 -4.32495 -4.325788 -4.326807 -4.3266354 -4.3264036][-4.3293633 -4.3304915 -4.3284783 -4.326396 -4.3245416 -4.3224707 -4.3217711 -4.3233547 -4.3265309 -4.328598 -4.3287911 -4.3292093 -4.3301387 -4.3298335 -4.3295469][-4.3292441 -4.3289514 -4.3261585 -4.324676 -4.3231397 -4.3204827 -4.3176804 -4.316442 -4.3169351 -4.3168273 -4.3156462 -4.3148079 -4.3143754 -4.31381 -4.3144035]]...]
INFO - root - 2017-12-06 06:43:47.990608: step 8510, loss = 2.10, batch loss = 2.04 (11.7 examples/sec; 0.684 sec/batch; 61h:35m:13s remains)
INFO - root - 2017-12-06 06:43:54.733464: step 8520, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 59h:29m:34s remains)
INFO - root - 2017-12-06 06:44:01.454705: step 8530, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 61h:03m:16s remains)
INFO - root - 2017-12-06 06:44:08.381434: step 8540, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 62h:01m:09s remains)
INFO - root - 2017-12-06 06:44:15.184181: step 8550, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 63h:12m:35s remains)
INFO - root - 2017-12-06 06:44:21.861351: step 8560, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 61h:07m:04s remains)
INFO - root - 2017-12-06 06:44:28.787482: step 8570, loss = 2.05, batch loss = 2.00 (10.8 examples/sec; 0.743 sec/batch; 66h:51m:55s remains)
INFO - root - 2017-12-06 06:44:35.614355: step 8580, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 61h:32m:40s remains)
INFO - root - 2017-12-06 06:44:42.408664: step 8590, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.679 sec/batch; 61h:06m:22s remains)
INFO - root - 2017-12-06 06:44:49.003900: step 8600, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.694 sec/batch; 62h:25m:19s remains)
2017-12-06 06:44:49.716242: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2725406 -4.2682528 -4.2660451 -4.2643523 -4.2623758 -4.2595835 -4.2568979 -4.2552824 -4.255168 -4.2563076 -4.2576671 -4.2576919 -4.2552261 -4.2492442 -4.2367892][-4.2859468 -4.2840428 -4.2826076 -4.2815065 -4.2789135 -4.2735047 -4.2672181 -4.2626929 -4.2613316 -4.2631512 -4.2658477 -4.2668219 -4.2632408 -4.2537365 -4.2361622][-4.2939558 -4.2926307 -4.2936468 -4.2965908 -4.2973242 -4.291431 -4.2813873 -4.2720089 -4.2688746 -4.2720728 -4.2777262 -4.2815285 -4.2774673 -4.2647634 -4.2428017][-4.2914052 -4.2894359 -4.2947068 -4.303719 -4.3091941 -4.3022265 -4.2878928 -4.2759638 -4.275423 -4.2832885 -4.29146 -4.2965107 -4.2911096 -4.2765751 -4.2528439][-4.2837982 -4.277523 -4.283134 -4.2951679 -4.3004193 -4.2887554 -4.2695909 -4.25736 -4.2670593 -4.2851267 -4.2962785 -4.3010311 -4.2933326 -4.2759538 -4.2532697][-4.2601814 -4.250433 -4.2539458 -4.2644053 -4.2628942 -4.2396445 -4.2067552 -4.1897 -4.211092 -4.2460356 -4.267354 -4.2759533 -4.2706652 -4.25318 -4.235188][-4.2190013 -4.2106996 -4.2193437 -4.2336235 -4.228704 -4.1898074 -4.1304922 -4.0885978 -4.1141686 -4.1711154 -4.2086439 -4.2282476 -4.2308593 -4.2174911 -4.2049074][-4.1798172 -4.17049 -4.1796947 -4.1978536 -4.1898456 -4.1341949 -4.0415092 -3.962759 -3.9853191 -4.0617695 -4.1161757 -4.1478715 -4.1614428 -4.1571178 -4.1543183][-4.1589384 -4.1449022 -4.1469479 -4.162683 -4.1512213 -4.0873284 -3.979557 -3.8840129 -3.9061131 -3.9891963 -4.0542607 -4.0951018 -4.1137595 -4.11418 -4.1159678][-4.139317 -4.1253614 -4.1268387 -4.1472521 -4.1477342 -4.1048336 -4.0317483 -3.9694095 -3.9851089 -4.040864 -4.090085 -4.1244187 -4.1367569 -4.1324978 -4.12839][-4.1204371 -4.1121688 -4.1151571 -4.1377907 -4.1469288 -4.1263256 -4.0936766 -4.0648246 -4.0742373 -4.1037493 -4.1368322 -4.1655006 -4.1751575 -4.1664824 -4.1550231][-4.1050529 -4.1019859 -4.1034541 -4.1214151 -4.1344075 -4.1285233 -4.1217818 -4.1162682 -4.1224937 -4.1413994 -4.1719904 -4.2009115 -4.2095294 -4.1988645 -4.1821547][-4.1124821 -4.1117163 -4.1138515 -4.1279054 -4.1414127 -4.1451449 -4.151979 -4.1570339 -4.158586 -4.1710038 -4.1987829 -4.2245216 -4.2317181 -4.222712 -4.2078805][-4.1552105 -4.1525707 -4.1582618 -4.168643 -4.1793146 -4.1843061 -4.1912742 -4.196672 -4.195909 -4.2030663 -4.2224107 -4.2394142 -4.2446728 -4.2406197 -4.2334442][-4.2094045 -4.2061477 -4.2123413 -4.2207327 -4.2287273 -4.2303271 -4.2329063 -4.2333 -4.2293406 -4.2305055 -4.2398534 -4.2496729 -4.2539978 -4.252707 -4.2484865]]...]
INFO - root - 2017-12-06 06:44:56.563878: step 8610, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.687 sec/batch; 61h:46m:03s remains)
INFO - root - 2017-12-06 06:45:03.297301: step 8620, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 60h:42m:08s remains)
INFO - root - 2017-12-06 06:45:10.187074: step 8630, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.703 sec/batch; 63h:17m:12s remains)
INFO - root - 2017-12-06 06:45:17.034067: step 8640, loss = 2.05, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 60h:58m:01s remains)
INFO - root - 2017-12-06 06:45:23.754812: step 8650, loss = 2.03, batch loss = 1.98 (11.7 examples/sec; 0.684 sec/batch; 61h:31m:08s remains)
INFO - root - 2017-12-06 06:45:30.635023: step 8660, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 62h:14m:28s remains)
INFO - root - 2017-12-06 06:45:37.423083: step 8670, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 60h:31m:49s remains)
INFO - root - 2017-12-06 06:45:44.053686: step 8680, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.659 sec/batch; 59h:17m:16s remains)
INFO - root - 2017-12-06 06:45:50.596996: step 8690, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.653 sec/batch; 58h:44m:11s remains)
INFO - root - 2017-12-06 06:45:57.536166: step 8700, loss = 2.03, batch loss = 1.97 (11.6 examples/sec; 0.690 sec/batch; 62h:01m:04s remains)
2017-12-06 06:45:58.179656: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3016429 -4.2951937 -4.2920012 -4.2927961 -4.3012977 -4.3008842 -4.2954855 -4.284112 -4.2697015 -4.2596374 -4.256639 -4.2584553 -4.2610335 -4.2697768 -4.2836685][-4.3007274 -4.2916045 -4.2830138 -4.2801108 -4.29373 -4.2964783 -4.2913079 -4.2778091 -4.2647576 -4.2617607 -4.2600117 -4.2588439 -4.2590156 -4.264267 -4.2744946][-4.2993836 -4.2870731 -4.2725768 -4.2626061 -4.2741265 -4.2759047 -4.26606 -4.2511897 -4.2464414 -4.2561502 -4.2566419 -4.25795 -4.2595267 -4.2661538 -4.2768912][-4.2862191 -4.2691712 -4.2492151 -4.2386594 -4.2437363 -4.2365909 -4.2143254 -4.2007532 -4.21005 -4.2344494 -4.2444305 -4.2522335 -4.2574005 -4.2693515 -4.2820778][-4.2707834 -4.2465467 -4.2188811 -4.2017918 -4.1861267 -4.156805 -4.1106672 -4.0947251 -4.1248436 -4.1714954 -4.198648 -4.2214532 -4.2366958 -4.2565055 -4.2728992][-4.2553682 -4.2261624 -4.187386 -4.151782 -4.1052113 -4.0394416 -3.9631631 -3.9574637 -4.0217643 -4.0918236 -4.1345696 -4.1772842 -4.2057176 -4.236546 -4.2583046][-4.237885 -4.2051692 -4.15694 -4.1024175 -4.024116 -3.9096277 -3.8030989 -3.8189645 -3.9301915 -4.0197792 -4.0788016 -4.1348085 -4.1757679 -4.2180424 -4.2481141][-4.230854 -4.2006397 -4.1510844 -4.0838895 -3.9823823 -3.8291976 -3.701292 -3.7477074 -3.8979397 -3.9961045 -4.0619373 -4.119051 -4.1645422 -4.2116828 -4.2465448][-4.2323151 -4.2080631 -4.1608291 -4.0934229 -3.99992 -3.8571239 -3.750114 -3.8095472 -3.9543507 -4.039 -4.0952473 -4.1403427 -4.1766062 -4.2212138 -4.2573137][-4.2528429 -4.2343054 -4.1895461 -4.1340141 -4.0702257 -3.9790123 -3.9170752 -3.9680581 -4.0715232 -4.1303067 -4.170671 -4.1985431 -4.2192039 -4.2540822 -4.27993][-4.2810507 -4.2710667 -4.2373509 -4.19899 -4.16143 -4.1143837 -4.0909467 -4.1294975 -4.1937995 -4.2288952 -4.2535791 -4.2681341 -4.2758536 -4.2973185 -4.3093114][-4.3054934 -4.3030429 -4.283844 -4.264946 -4.2487698 -4.2256751 -4.2196026 -4.2446527 -4.2828617 -4.3031549 -4.3132286 -4.3176465 -4.3200321 -4.3330078 -4.3360248][-4.3221459 -4.3228517 -4.3155775 -4.3131766 -4.3161173 -4.3065672 -4.3033714 -4.3157916 -4.336462 -4.3452468 -4.3433175 -4.3397107 -4.3395767 -4.3476524 -4.3466115][-4.3240385 -4.3215709 -4.3191705 -4.3235908 -4.3366566 -4.3381104 -4.3360362 -4.3374715 -4.3445449 -4.3466129 -4.3410649 -4.3365016 -4.3351631 -4.3409486 -4.3404446][-4.3133445 -4.309998 -4.3101854 -4.3172994 -4.3321233 -4.3375068 -4.3341546 -4.3311825 -4.3316731 -4.331018 -4.3260365 -4.3215404 -4.3212218 -4.3264604 -4.3283029]]...]
INFO - root - 2017-12-06 06:46:05.012826: step 8710, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 61h:41m:41s remains)
INFO - root - 2017-12-06 06:46:11.792496: step 8720, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.680 sec/batch; 61h:08m:36s remains)
INFO - root - 2017-12-06 06:46:18.680408: step 8730, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.681 sec/batch; 61h:12m:40s remains)
INFO - root - 2017-12-06 06:46:25.490637: step 8740, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.683 sec/batch; 61h:25m:14s remains)
INFO - root - 2017-12-06 06:46:32.369126: step 8750, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 62h:37m:28s remains)
INFO - root - 2017-12-06 06:46:39.281116: step 8760, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 61h:43m:42s remains)
INFO - root - 2017-12-06 06:46:45.850267: step 8770, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 60h:45m:03s remains)
INFO - root - 2017-12-06 06:46:52.623992: step 8780, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.699 sec/batch; 62h:52m:13s remains)
INFO - root - 2017-12-06 06:46:59.404765: step 8790, loss = 2.03, batch loss = 1.97 (11.9 examples/sec; 0.673 sec/batch; 60h:30m:45s remains)
INFO - root - 2017-12-06 06:47:06.262916: step 8800, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 63h:47m:55s remains)
2017-12-06 06:47:06.929884: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3307176 -4.3322105 -4.3359361 -4.3409896 -4.3433867 -4.342473 -4.3394456 -4.3361912 -4.3327384 -4.3317842 -4.3336005 -4.3349552 -4.3361969 -4.3384805 -4.3412991][-4.3084044 -4.3055434 -4.3091025 -4.3155622 -4.3207731 -4.3218007 -4.3216295 -4.3197274 -4.3151474 -4.3130889 -4.3159084 -4.3173509 -4.3182378 -4.32191 -4.32689][-4.2874212 -4.2730265 -4.273901 -4.2789006 -4.2833128 -4.2855964 -4.2907543 -4.2950044 -4.2948184 -4.2962809 -4.3010817 -4.2989368 -4.2964864 -4.2998748 -4.3057289][-4.2464595 -4.2225137 -4.22075 -4.2249155 -4.2267427 -4.2292562 -4.2332468 -4.242383 -4.2509141 -4.2599187 -4.2682753 -4.2647142 -4.2574553 -4.2587018 -4.263835][-4.1857052 -4.155817 -4.1528268 -4.1543736 -4.1525044 -4.1521931 -4.15294 -4.1616373 -4.17881 -4.1982074 -4.2117605 -4.209754 -4.2006774 -4.1979747 -4.20028][-4.1352296 -4.1067352 -4.10044 -4.0930452 -4.0839658 -4.0760031 -4.0653033 -4.0616255 -4.0813084 -4.1107626 -4.135129 -4.1442161 -4.1437 -4.1425567 -4.1434493][-4.1108346 -4.0945005 -4.0872154 -4.0660539 -4.04198 -4.0199475 -3.9910147 -3.9687107 -3.9818881 -4.0163355 -4.0562282 -4.0918279 -4.1137171 -4.12156 -4.1232572][-4.1228776 -4.12085 -4.109498 -4.0771914 -4.0405054 -4.0090523 -3.9755564 -3.9475508 -3.9506626 -3.9730124 -4.0154295 -4.0745831 -4.1167164 -4.1324611 -4.1309395][-4.1567659 -4.1688643 -4.1610303 -4.1298552 -4.091692 -4.0591826 -4.031857 -4.0118518 -4.0096631 -4.012732 -4.0417628 -4.1047397 -4.1512451 -4.1669788 -4.1615629][-4.1828952 -4.2080288 -4.2150536 -4.1965971 -4.1683478 -4.13906 -4.1111946 -4.0951734 -4.09631 -4.0932875 -4.10649 -4.1491165 -4.1847029 -4.1967583 -4.1885114][-4.1871009 -4.2204685 -4.24426 -4.2439156 -4.2271862 -4.2067766 -4.1859512 -4.1726127 -4.1720076 -4.1693945 -4.1757183 -4.1949892 -4.2086987 -4.2076292 -4.1914983][-4.1759276 -4.2066407 -4.2388067 -4.2526865 -4.2482662 -4.2390704 -4.230351 -4.2224808 -4.218791 -4.2183352 -4.2234888 -4.2282944 -4.2239614 -4.2089872 -4.1866407][-4.1427536 -4.1667538 -4.2025161 -4.2266846 -4.2365541 -4.237988 -4.2403016 -4.2380137 -4.2332 -4.2303996 -4.2316103 -4.2319794 -4.2270474 -4.2099891 -4.1837344][-4.1130819 -4.1253943 -4.1570039 -4.1819916 -4.1984591 -4.2113848 -4.2227368 -4.2285752 -4.22913 -4.2271457 -4.2213354 -4.2143097 -4.210494 -4.1968546 -4.1725674][-4.1104431 -4.1143346 -4.1380339 -4.1584535 -4.1749072 -4.191721 -4.20819 -4.21823 -4.2238646 -4.2263165 -4.218091 -4.2044334 -4.1968975 -4.1853509 -4.1667089]]...]
INFO - root - 2017-12-06 06:47:13.838934: step 8810, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 63h:29m:41s remains)
INFO - root - 2017-12-06 06:47:20.646102: step 8820, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 59h:36m:58s remains)
INFO - root - 2017-12-06 06:47:27.447888: step 8830, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 62h:01m:02s remains)
INFO - root - 2017-12-06 06:47:34.296960: step 8840, loss = 2.08, batch loss = 2.03 (11.9 examples/sec; 0.675 sec/batch; 60h:41m:43s remains)
INFO - root - 2017-12-06 06:47:41.076819: step 8850, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.672 sec/batch; 60h:22m:12s remains)
INFO - root - 2017-12-06 06:47:47.834117: step 8860, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 61h:28m:28s remains)
INFO - root - 2017-12-06 06:47:54.474042: step 8870, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 59h:24m:33s remains)
INFO - root - 2017-12-06 06:48:01.078696: step 8880, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 60h:44m:47s remains)
INFO - root - 2017-12-06 06:48:07.901440: step 8890, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.699 sec/batch; 62h:51m:32s remains)
INFO - root - 2017-12-06 06:48:14.744455: step 8900, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 60h:36m:34s remains)
2017-12-06 06:48:15.397244: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.122067 -4.13757 -4.16051 -4.1931062 -4.2127514 -4.2226195 -4.224865 -4.2151966 -4.2077708 -4.2116914 -4.2107081 -4.1830006 -4.1357284 -4.0954485 -4.0800962][-4.0502834 -4.0928559 -4.1414371 -4.1895075 -4.2172332 -4.22449 -4.2188454 -4.2037272 -4.1916709 -4.1880412 -4.1800637 -4.1402388 -4.0796471 -4.0347171 -4.0179563][-3.9912276 -4.0652041 -4.1401491 -4.1943297 -4.215435 -4.207448 -4.1843309 -4.1572752 -4.1391516 -4.1324 -4.1277485 -4.0975533 -4.0492997 -4.0216417 -4.0183544][-4.040143 -4.1214218 -4.1899786 -4.2214289 -4.2198348 -4.1835437 -4.1274261 -4.0794458 -4.0589414 -4.0627251 -4.0763974 -4.074471 -4.0616374 -4.0598598 -4.0709076][-4.1369133 -4.1972294 -4.2349191 -4.231463 -4.2002387 -4.1331692 -4.0407481 -3.9741287 -3.9769998 -4.0157924 -4.0584736 -4.089241 -4.108459 -4.1189294 -4.1283307][-4.1957297 -4.2270222 -4.2304454 -4.1932735 -4.1310105 -4.0372152 -3.9095914 -3.8287146 -3.8822711 -3.9749951 -4.0513964 -4.1101871 -4.1491327 -4.1614814 -4.1550031][-4.221632 -4.2271752 -4.2008224 -4.1339931 -4.0437622 -3.9340963 -3.7951446 -3.7216039 -3.8292089 -3.9614542 -4.0524273 -4.1211786 -4.1654186 -4.170351 -4.1459694][-4.2275872 -4.2133703 -4.1730785 -4.0978556 -4.0093365 -3.9217992 -3.8316345 -3.8018136 -3.9042912 -4.0206103 -4.0903807 -4.1440091 -4.1809759 -4.1755104 -4.1437993][-4.2134356 -4.1923227 -4.1502967 -4.0855041 -4.0181952 -3.9644432 -3.9292903 -3.9302094 -4.0028281 -4.0863152 -4.1318736 -4.1667643 -4.19213 -4.1831374 -4.1565862][-4.2020588 -4.175292 -4.1328225 -4.077662 -4.0299363 -4.0048871 -4.0049 -4.019084 -4.06815 -4.1241665 -4.1577272 -4.1801267 -4.1948028 -4.1886287 -4.1722665][-4.2077279 -4.1816473 -4.1484609 -4.1143732 -4.08618 -4.076849 -4.0902119 -4.1067171 -4.1341949 -4.1666794 -4.1896133 -4.2029386 -4.2079844 -4.2054625 -4.1976604][-4.2324195 -4.2133737 -4.1947351 -4.1839228 -4.1689968 -4.1636128 -4.1772189 -4.1886396 -4.1979971 -4.2146063 -4.231214 -4.2379036 -4.239264 -4.2407765 -4.2376647][-4.2681379 -4.2535558 -4.2429891 -4.2436666 -4.2386141 -4.2367415 -4.2484126 -4.2590718 -4.2609358 -4.2673645 -4.27841 -4.2812772 -4.2812333 -4.2826872 -4.2804718][-4.2953992 -4.2875772 -4.2831392 -4.2869782 -4.2860975 -4.2857466 -4.2942963 -4.3022161 -4.3016868 -4.3023067 -4.3082089 -4.3085527 -4.3063364 -4.3066363 -4.3052173][-4.3053746 -4.30367 -4.3036866 -4.3071127 -4.3065648 -4.3056173 -4.3087859 -4.3116903 -4.3087907 -4.3059912 -4.3079467 -4.3082566 -4.306519 -4.3070602 -4.3072486]]...]
INFO - root - 2017-12-06 06:48:22.168108: step 8910, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 60h:32m:32s remains)
INFO - root - 2017-12-06 06:48:28.868754: step 8920, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 59h:29m:09s remains)
INFO - root - 2017-12-06 06:48:35.729539: step 8930, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.693 sec/batch; 62h:18m:59s remains)
INFO - root - 2017-12-06 06:48:42.614442: step 8940, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 61h:07m:57s remains)
INFO - root - 2017-12-06 06:48:49.453198: step 8950, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 61h:04m:12s remains)
INFO - root - 2017-12-06 06:48:56.348814: step 8960, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 59h:58m:39s remains)
INFO - root - 2017-12-06 06:49:02.910046: step 8970, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.622 sec/batch; 55h:54m:51s remains)
INFO - root - 2017-12-06 06:49:09.638143: step 8980, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 61h:48m:42s remains)
INFO - root - 2017-12-06 06:49:16.535291: step 8990, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 61h:36m:01s remains)
INFO - root - 2017-12-06 06:49:23.249916: step 9000, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 58h:21m:47s remains)
2017-12-06 06:49:23.940027: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3082881 -4.3022652 -4.2958746 -4.292202 -4.2927513 -4.2946858 -4.2898521 -4.2814531 -4.2660503 -4.2543836 -4.2538042 -4.2705646 -4.2919269 -4.3041835 -4.3107767][-4.26944 -4.2644691 -4.2573934 -4.2512794 -4.2473674 -4.2455182 -4.2375436 -4.2274427 -4.2125258 -4.2009463 -4.1938543 -4.2061982 -4.2304792 -4.25008 -4.2651429][-4.2198915 -4.2146854 -4.2024589 -4.1876073 -4.1755543 -4.1694436 -4.1607933 -4.151845 -4.1421142 -4.1379824 -4.1262212 -4.1267695 -4.1508255 -4.1795874 -4.205905][-4.1659646 -4.1587968 -4.1338854 -4.0926266 -4.054718 -4.0365138 -4.0307112 -4.0384312 -4.0517011 -4.0716157 -4.0652232 -4.0586376 -4.0799856 -4.1132212 -4.1512775][-4.1328063 -4.1208448 -4.0806208 -3.9996271 -3.9114816 -3.8509266 -3.8327668 -3.8669786 -3.92869 -3.9986162 -4.0165825 -4.0107403 -4.0268917 -4.0628023 -4.1121869][-4.1294413 -4.1124153 -4.0584688 -3.9492166 -3.81555 -3.6902239 -3.6237915 -3.6691158 -3.7914047 -3.9203842 -3.9787819 -3.9870067 -4.0014768 -4.0410032 -4.0955839][-4.1292477 -4.1161251 -4.0651412 -3.9608274 -3.8175194 -3.6565969 -3.5377467 -3.5583396 -3.7043183 -3.864203 -3.95405 -3.9841983 -4.0101957 -4.0570908 -4.1110554][-4.1102457 -4.1125727 -4.0833235 -4.0095 -3.901216 -3.7716122 -3.6608224 -3.6527619 -3.7571588 -3.8860343 -3.9677579 -4.0047264 -4.0374441 -4.0890017 -4.1417556][-4.0877829 -4.1114841 -4.11499 -4.0809321 -4.0171742 -3.9313705 -3.852175 -3.8271155 -3.8821688 -3.9636691 -4.0175447 -4.0448256 -4.0747867 -4.1235876 -4.1728916][-4.0735092 -4.1186519 -4.1478753 -4.1452823 -4.1091628 -4.0512457 -3.9929416 -3.9587545 -3.9831376 -4.0346365 -4.0701113 -4.0902824 -4.1159163 -4.157361 -4.1995244][-4.0701971 -4.1281185 -4.1744618 -4.1876616 -4.1691346 -4.1340709 -4.0911674 -4.0579028 -4.0664282 -4.1045928 -4.1337471 -4.1500225 -4.1705947 -4.2038107 -4.2337155][-4.1160784 -4.1721368 -4.2107596 -4.220099 -4.2099872 -4.1916094 -4.1653061 -4.1379428 -4.1399941 -4.1692848 -4.1939049 -4.2113996 -4.2290406 -4.2507868 -4.2664914][-4.188004 -4.2340689 -4.253407 -4.2493572 -4.2396517 -4.2339964 -4.2195463 -4.2031593 -4.2040725 -4.2244968 -4.2420278 -4.2533584 -4.2666507 -4.2787266 -4.2883916][-4.2294149 -4.265882 -4.2736983 -4.2620783 -4.2516446 -4.2520967 -4.2464752 -4.2396088 -4.2410436 -4.2520685 -4.2645783 -4.2719793 -4.2815166 -4.2899265 -4.2988577][-4.249774 -4.2712803 -4.2746291 -4.2670236 -4.2621751 -4.2655134 -4.26536 -4.2632065 -4.2656593 -4.2744007 -4.2855163 -4.2910066 -4.2957792 -4.299871 -4.3060884]]...]
INFO - root - 2017-12-06 06:49:30.648975: step 9010, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 61h:44m:12s remains)
INFO - root - 2017-12-06 06:49:37.422016: step 9020, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 58h:35m:22s remains)
INFO - root - 2017-12-06 06:49:44.162309: step 9030, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 61h:28m:27s remains)
INFO - root - 2017-12-06 06:49:51.106431: step 9040, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.694 sec/batch; 62h:23m:54s remains)
INFO - root - 2017-12-06 06:49:58.051617: step 9050, loss = 2.05, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 62h:03m:13s remains)
INFO - root - 2017-12-06 06:50:04.778621: step 9060, loss = 2.10, batch loss = 2.04 (11.2 examples/sec; 0.716 sec/batch; 64h:18m:03s remains)
INFO - root - 2017-12-06 06:50:11.400286: step 9070, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 63h:16m:09s remains)
INFO - root - 2017-12-06 06:50:18.209786: step 9080, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 62h:01m:16s remains)
INFO - root - 2017-12-06 06:50:25.042353: step 9090, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 58h:09m:40s remains)
INFO - root - 2017-12-06 06:50:31.923506: step 9100, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 61h:13m:58s remains)
2017-12-06 06:50:32.584037: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3255296 -4.3100853 -4.2795219 -4.2351756 -4.201797 -4.1930962 -4.2002392 -4.2001157 -4.1853638 -4.1885853 -4.1990657 -4.210669 -4.2281566 -4.2501144 -4.260829][-4.326139 -4.3075747 -4.2745829 -4.2291465 -4.1959586 -4.1893244 -4.2018037 -4.2061272 -4.1972322 -4.2043476 -4.2092929 -4.210722 -4.2192488 -4.2390547 -4.2515054][-4.3305249 -4.3105412 -4.2772746 -4.2300377 -4.1910605 -4.1784344 -4.1917186 -4.1993847 -4.2028737 -4.2220106 -4.2249765 -4.2162023 -4.2115927 -4.2212586 -4.230988][-4.33727 -4.3181114 -4.283813 -4.2325463 -4.1840262 -4.1562796 -4.1590233 -4.162468 -4.1750784 -4.2116671 -4.2257476 -4.2181883 -4.2057676 -4.205584 -4.2090693][-4.3437867 -4.3265686 -4.29225 -4.2365513 -4.1781735 -4.1271558 -4.1055236 -4.0973272 -4.1145473 -4.1744752 -4.2136049 -4.2208362 -4.2114406 -4.2025433 -4.1978889][-4.3479385 -4.3346915 -4.3049293 -4.24864 -4.1819797 -4.1094174 -4.0552855 -4.016151 -4.0234852 -4.1090832 -4.1837425 -4.2175708 -4.22394 -4.21479 -4.2033644][-4.3482938 -4.3391843 -4.3152742 -4.2642217 -4.1980629 -4.1134639 -4.0311403 -3.9464245 -3.9193301 -4.0201807 -4.1317248 -4.1990628 -4.2296424 -4.2337966 -4.2250881][-4.3491616 -4.3412461 -4.3223062 -4.2785044 -4.2211013 -4.1442575 -4.058959 -3.9611976 -3.9076126 -3.9827857 -4.0926957 -4.1722894 -4.2213168 -4.2428265 -4.2448044][-4.3516188 -4.345067 -4.3283119 -4.290648 -4.2446871 -4.190032 -4.128 -4.0548038 -4.0059824 -4.0329518 -4.0957665 -4.1558466 -4.2051611 -4.2371931 -4.2486153][-4.3537679 -4.3483725 -4.3339276 -4.3003316 -4.2596693 -4.2212257 -4.1839242 -4.1404462 -4.1058517 -4.1068945 -4.1297908 -4.1616645 -4.1979465 -4.2298522 -4.2466822][-4.3535151 -4.3477468 -4.3324065 -4.2991815 -4.2571464 -4.2269006 -4.2085938 -4.1875453 -4.1672025 -4.1645126 -4.1722264 -4.184607 -4.2050271 -4.2267413 -4.2382984][-4.35063 -4.3440671 -4.3264613 -4.2898507 -4.2441711 -4.21592 -4.2043061 -4.1907854 -4.1792283 -4.1837239 -4.1984134 -4.2109733 -4.2198973 -4.2267637 -4.2254386][-4.3479085 -4.3398824 -4.3205314 -4.2802382 -4.2332263 -4.2035079 -4.1881256 -4.1774673 -4.1737747 -4.1893048 -4.2119117 -4.2289338 -4.2349386 -4.2298884 -4.2173333][-4.3466005 -4.3380952 -4.3184543 -4.2786164 -4.2370782 -4.2101822 -4.1936245 -4.1852193 -4.1894031 -4.2110991 -4.2309589 -4.2414188 -4.2434077 -4.2328448 -4.2136836][-4.345346 -4.3371596 -4.319098 -4.2849436 -4.2539697 -4.2352357 -4.2200379 -4.2102671 -4.2155704 -4.2380557 -4.2532911 -4.2574568 -4.2571735 -4.2453089 -4.2272696]]...]
INFO - root - 2017-12-06 06:50:39.353565: step 9110, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 61h:25m:57s remains)
INFO - root - 2017-12-06 06:50:46.214126: step 9120, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.663 sec/batch; 59h:35m:32s remains)
INFO - root - 2017-12-06 06:50:53.086889: step 9130, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 60h:29m:13s remains)
INFO - root - 2017-12-06 06:50:59.983565: step 9140, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 62h:50m:19s remains)
INFO - root - 2017-12-06 06:51:06.802215: step 9150, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.667 sec/batch; 59h:55m:18s remains)
INFO - root - 2017-12-06 06:51:13.460947: step 9160, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.698 sec/batch; 62h:40m:01s remains)
INFO - root - 2017-12-06 06:51:20.175545: step 9170, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 58h:07m:27s remains)
INFO - root - 2017-12-06 06:51:26.983782: step 9180, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 61h:06m:04s remains)
INFO - root - 2017-12-06 06:51:33.816200: step 9190, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 61h:16m:35s remains)
INFO - root - 2017-12-06 06:51:40.585480: step 9200, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 58h:19m:46s remains)
2017-12-06 06:51:41.219257: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3310733 -4.32068 -4.3051434 -4.2857113 -4.263782 -4.2524457 -4.2511711 -4.2591033 -4.2675567 -4.2711091 -4.2790914 -4.289186 -4.2936554 -4.2978449 -4.2992225][-4.3296967 -4.3152142 -4.2861285 -4.2489648 -4.2138057 -4.1940675 -4.191433 -4.2074909 -4.2245655 -4.2363935 -4.250948 -4.2642574 -4.2692637 -4.2733941 -4.27201][-4.3291035 -4.3091421 -4.2628808 -4.2025223 -4.1521974 -4.1202846 -4.1091828 -4.1358476 -4.1702156 -4.198781 -4.2214408 -4.2354479 -4.2423134 -4.2496343 -4.2461123][-4.3292017 -4.303369 -4.2442269 -4.1680593 -4.1087708 -4.0593934 -4.0310688 -4.065578 -4.1166897 -4.1625004 -4.1942987 -4.2083488 -4.21627 -4.2235332 -4.2180767][-4.3326578 -4.3047371 -4.2367363 -4.1530828 -4.0821404 -4.0055494 -3.9453161 -3.9854932 -4.0628061 -4.1242452 -4.1657839 -4.1797881 -4.1845317 -4.1878 -4.1840558][-4.3336725 -4.3069086 -4.2362037 -4.1466012 -4.0624428 -3.9554925 -3.8537023 -3.8889527 -3.9942031 -4.0723462 -4.1274209 -4.1471643 -4.1530557 -4.1536565 -4.1534562][-4.3311973 -4.3027434 -4.2270455 -4.12935 -4.0260277 -3.8882742 -3.7403533 -3.7505646 -3.888344 -4.0056067 -4.0837069 -4.1158438 -4.1250334 -4.1294656 -4.1364632][-4.3246489 -4.2951188 -4.2172723 -4.1128407 -3.9906979 -3.8306179 -3.6571152 -3.6429353 -3.8105354 -3.967546 -4.0586586 -4.0969806 -4.1086512 -4.1151457 -4.130393][-4.31721 -4.2846193 -4.2042851 -4.1003947 -3.9766212 -3.8288932 -3.6969373 -3.7068925 -3.8583903 -3.9952664 -4.0707188 -4.1019917 -4.10711 -4.1109662 -4.1292963][-4.3090968 -4.2789693 -4.2062554 -4.116549 -4.0167685 -3.9047625 -3.8289208 -3.8600807 -3.968637 -4.0562992 -4.1032524 -4.1217465 -4.1224194 -4.1200905 -4.136961][-4.3046227 -4.2803841 -4.2191081 -4.1512914 -4.0797782 -3.9936841 -3.9475651 -3.9853292 -4.0553079 -4.1021075 -4.1283464 -4.1431141 -4.1490121 -4.148046 -4.1615763][-4.304625 -4.2870531 -4.2428241 -4.192719 -4.1367412 -4.0732403 -4.0482321 -4.0873346 -4.12855 -4.1465034 -4.1560044 -4.1684408 -4.1838317 -4.1895175 -4.1990581][-4.3097239 -4.2973018 -4.2633471 -4.2228322 -4.1811953 -4.1428595 -4.1362162 -4.169724 -4.1912556 -4.1919665 -4.1868248 -4.1960897 -4.2178326 -4.2292261 -4.2346292][-4.3225536 -4.3132229 -4.2889395 -4.2610445 -4.2371221 -4.2198944 -4.2229581 -4.2431774 -4.2510805 -4.2466307 -4.2378626 -4.2426543 -4.2617006 -4.2719088 -4.2730331][-4.3353157 -4.3297982 -4.3149033 -4.2969117 -4.2810698 -4.2748384 -4.2849684 -4.3015242 -4.30613 -4.302186 -4.2941546 -4.2931724 -4.3044477 -4.3101606 -4.305521]]...]
INFO - root - 2017-12-06 06:51:48.048826: step 9210, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.676 sec/batch; 60h:44m:50s remains)
INFO - root - 2017-12-06 06:51:54.751773: step 9220, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 58h:26m:31s remains)
INFO - root - 2017-12-06 06:52:01.656108: step 9230, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 62h:03m:13s remains)
INFO - root - 2017-12-06 06:52:08.503956: step 9240, loss = 2.09, batch loss = 2.04 (11.9 examples/sec; 0.671 sec/batch; 60h:17m:06s remains)
INFO - root - 2017-12-06 06:52:15.229194: step 9250, loss = 2.05, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 59h:53m:39s remains)
INFO - root - 2017-12-06 06:52:21.821168: step 9260, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 58h:53m:26s remains)
INFO - root - 2017-12-06 06:52:28.596424: step 9270, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.668 sec/batch; 59h:56m:10s remains)
INFO - root - 2017-12-06 06:52:35.488958: step 9280, loss = 2.10, batch loss = 2.05 (11.8 examples/sec; 0.676 sec/batch; 60h:41m:43s remains)
INFO - root - 2017-12-06 06:52:42.189801: step 9290, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.687 sec/batch; 61h:38m:05s remains)
INFO - root - 2017-12-06 06:52:48.950865: step 9300, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.652 sec/batch; 58h:34m:16s remains)
2017-12-06 06:52:49.584819: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2565289 -4.2522335 -4.2440329 -4.2270432 -4.203599 -4.18807 -4.1708422 -4.1524682 -4.142231 -4.132062 -4.1205473 -4.1298432 -4.1641469 -4.1938949 -4.2074394][-4.2347155 -4.2387357 -4.2388844 -4.2270517 -4.2064004 -4.1912136 -4.1764393 -4.1645594 -4.1664968 -4.1659122 -4.151607 -4.1496968 -4.1695256 -4.1888828 -4.2006831][-4.2071533 -4.2113376 -4.2167382 -4.2147756 -4.1996546 -4.1866169 -4.1754522 -4.1727762 -4.1867628 -4.1959105 -4.1836681 -4.1730108 -4.1763449 -4.1833925 -4.1899571][-4.2039232 -4.2011232 -4.1986361 -4.1962142 -4.1816635 -4.1639776 -4.1545687 -4.1609 -4.1873136 -4.20963 -4.2071977 -4.1955061 -4.1858096 -4.175621 -4.1729908][-4.2112503 -4.2036614 -4.1926007 -4.1820531 -4.1636424 -4.13534 -4.113379 -4.1121192 -4.1436286 -4.1816516 -4.1958485 -4.1934958 -4.1774368 -4.1536179 -4.1458664][-4.2113328 -4.2016835 -4.1860428 -4.1686206 -4.1446576 -4.1052027 -4.0635533 -4.0418849 -4.0750265 -4.1356859 -4.1764665 -4.1871409 -4.1682944 -4.1345892 -4.1226339][-4.1868072 -4.17285 -4.1593676 -4.1422091 -4.1142197 -4.0685229 -4.0051117 -3.9516037 -3.979759 -4.06437 -4.1366148 -4.16672 -4.1566553 -4.1257262 -4.114543][-4.1630545 -4.1443896 -4.1328783 -4.1181259 -4.0901861 -4.0477829 -3.9795768 -3.9049888 -3.9181778 -4.0080204 -4.0908217 -4.132 -4.1313806 -4.1095061 -4.1035495][-4.1449847 -4.1229434 -4.1126041 -4.1040678 -4.08501 -4.0601196 -4.0197282 -3.96801 -3.9690802 -4.0275126 -4.084341 -4.1106353 -4.107687 -4.09019 -4.0870514][-4.1349459 -4.109942 -4.0944319 -4.08415 -4.0717773 -4.0633926 -4.0525761 -4.0325065 -4.0350642 -4.0620456 -4.0852861 -4.088378 -4.075788 -4.0610132 -4.0581732][-4.1580176 -4.1339679 -4.1122131 -4.0965934 -4.0890718 -4.0920057 -4.0979862 -4.0990744 -4.108067 -4.1180935 -4.1161032 -4.1008878 -4.08164 -4.0693955 -4.0669808][-4.1837039 -4.1635804 -4.1431079 -4.1276922 -4.1242862 -4.1340704 -4.1485729 -4.1592975 -4.1690454 -4.17103 -4.1585584 -4.1363759 -4.1161866 -4.1068583 -4.10461][-4.2059321 -4.1906404 -4.1747074 -4.1635733 -4.1638379 -4.1763439 -4.1947117 -4.2100177 -4.2200284 -4.2199836 -4.2083244 -4.1911659 -4.17658 -4.1691151 -4.16566][-4.2286172 -4.2202988 -4.2112617 -4.205646 -4.2082372 -4.21866 -4.2340078 -4.2481461 -4.256145 -4.25574 -4.248219 -4.2389603 -4.2312174 -4.2275825 -4.2258906][-4.2578239 -4.25759 -4.256568 -4.2544641 -4.2557235 -4.2615142 -4.2716312 -4.2810278 -4.2859073 -4.2852416 -4.2818446 -4.2782989 -4.275301 -4.2740903 -4.2732639]]...]
INFO - root - 2017-12-06 06:52:56.418792: step 9310, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 60h:03m:50s remains)
INFO - root - 2017-12-06 06:53:03.184442: step 9320, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 61h:53m:21s remains)
INFO - root - 2017-12-06 06:53:10.030070: step 9330, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 60h:31m:17s remains)
INFO - root - 2017-12-06 06:53:16.899715: step 9340, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.671 sec/batch; 60h:13m:46s remains)
INFO - root - 2017-12-06 06:53:23.352485: step 9350, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 59h:31m:26s remains)
INFO - root - 2017-12-06 06:53:30.264538: step 9360, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.707 sec/batch; 63h:27m:20s remains)
INFO - root - 2017-12-06 06:53:37.070976: step 9370, loss = 2.10, batch loss = 2.04 (11.6 examples/sec; 0.691 sec/batch; 61h:59m:12s remains)
INFO - root - 2017-12-06 06:53:43.732094: step 9380, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 59h:12m:01s remains)
INFO - root - 2017-12-06 06:53:50.650913: step 9390, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.695 sec/batch; 62h:20m:15s remains)
INFO - root - 2017-12-06 06:53:57.365635: step 9400, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 60h:12m:58s remains)
2017-12-06 06:53:58.062458: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1823654 -4.1830974 -4.1854138 -4.1958508 -4.2069821 -4.21902 -4.2303753 -4.2344074 -4.2088242 -4.1460981 -4.1077528 -4.1226163 -4.1514144 -4.1782074 -4.2008538][-4.179173 -4.1883988 -4.2087078 -4.2271028 -4.2285042 -4.22932 -4.2398934 -4.2532349 -4.2327967 -4.1638441 -4.1174564 -4.1250176 -4.1540518 -4.1779242 -4.1978378][-4.186635 -4.1920524 -4.2185802 -4.237289 -4.2310081 -4.225317 -4.2352786 -4.2522516 -4.2344551 -4.1628385 -4.1095972 -4.11574 -4.1487465 -4.1741939 -4.1929216][-4.207377 -4.2025828 -4.2203145 -4.2290473 -4.220048 -4.2179637 -4.2286215 -4.2409811 -4.2243776 -4.1563463 -4.1020794 -4.1158843 -4.1520858 -4.1759763 -4.192256][-4.2331414 -4.2214065 -4.21633 -4.2065234 -4.1970038 -4.2027736 -4.2135668 -4.2170572 -4.1997905 -4.1385365 -4.0903168 -4.1163635 -4.1556191 -4.1769009 -4.1901484][-4.2383533 -4.2166219 -4.186111 -4.1482091 -4.1270633 -4.138195 -4.1485195 -4.1421041 -4.1264071 -4.0758824 -4.0411525 -4.0827312 -4.1251454 -4.14812 -4.1667123][-4.2051187 -4.1711578 -4.1128631 -4.0367365 -3.9931905 -4.0013552 -4.0149927 -3.9972196 -3.981163 -3.9444709 -3.9320698 -3.9971817 -4.0583782 -4.0954237 -4.1312361][-4.1530862 -4.1150079 -4.0445704 -3.9501555 -3.8965549 -3.9074624 -3.9269605 -3.9018984 -3.8820152 -3.8663683 -3.8740876 -3.9464746 -4.016572 -4.0625196 -4.111299][-4.0987797 -4.0667005 -4.0025578 -3.9131362 -3.862848 -3.8802729 -3.9097044 -3.8837659 -3.8721611 -3.8910642 -3.9183352 -3.9820967 -4.0434566 -4.0851274 -4.131278][-4.0843267 -4.0681219 -4.0223789 -3.9528234 -3.9101543 -3.92857 -3.955915 -3.9254222 -3.9187562 -3.9582767 -4.0030813 -4.0628715 -4.1193776 -4.1571302 -4.1931086][-4.1139317 -4.1096249 -4.0854731 -4.0434642 -4.0158758 -4.0239987 -4.0374775 -4.0094209 -4.006124 -4.0464115 -4.0941081 -4.1477485 -4.1969957 -4.2299857 -4.2562065][-4.1615453 -4.1679344 -4.1642489 -4.1502686 -4.137969 -4.1377687 -4.1414433 -4.1195927 -4.1133862 -4.13715 -4.172833 -4.2151408 -4.2518549 -4.2770777 -4.2927017][-4.2164822 -4.2246971 -4.2290039 -4.2306356 -4.2287655 -4.2277627 -4.2283134 -4.2150636 -4.2056236 -4.2045369 -4.2174931 -4.2467093 -4.2722011 -4.2900128 -4.2995539][-4.2623725 -4.2632937 -4.2668271 -4.2705364 -4.2712522 -4.2702913 -4.2705836 -4.2625942 -4.2496943 -4.2283959 -4.2222567 -4.2424788 -4.2620955 -4.2787967 -4.2902365][-4.2842627 -4.2804017 -4.2793841 -4.2809157 -4.2828884 -4.2839532 -4.28421 -4.2771239 -4.2592897 -4.2224565 -4.2003155 -4.2168732 -4.2373261 -4.2595005 -4.2773538]]...]
INFO - root - 2017-12-06 06:54:04.725999: step 9410, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 61h:53m:47s remains)
INFO - root - 2017-12-06 06:54:11.576295: step 9420, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 63h:04m:12s remains)
INFO - root - 2017-12-06 06:54:18.292721: step 9430, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 58h:46m:01s remains)
INFO - root - 2017-12-06 06:54:25.181387: step 9440, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 61h:56m:39s remains)
INFO - root - 2017-12-06 06:54:31.875496: step 9450, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 62h:15m:07s remains)
INFO - root - 2017-12-06 06:54:38.647104: step 9460, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 62h:57m:14s remains)
INFO - root - 2017-12-06 06:54:45.385036: step 9470, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.659 sec/batch; 59h:09m:54s remains)
INFO - root - 2017-12-06 06:54:52.284096: step 9480, loss = 2.08, batch loss = 2.03 (11.4 examples/sec; 0.701 sec/batch; 62h:52m:32s remains)
INFO - root - 2017-12-06 06:54:59.209479: step 9490, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 60h:36m:50s remains)
INFO - root - 2017-12-06 06:55:05.966259: step 9500, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 62h:17m:45s remains)
2017-12-06 06:55:06.562257: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1784787 -4.1723213 -4.1851134 -4.1939096 -4.1965933 -4.1910577 -4.1679969 -4.1366611 -4.1201367 -4.1174278 -4.1201267 -4.1301861 -4.1481109 -4.1778965 -4.2007351][-4.1941285 -4.1890459 -4.2041979 -4.2165885 -4.2251587 -4.2246175 -4.200047 -4.1648049 -4.1393185 -4.1277714 -4.1224213 -4.1243992 -4.1382537 -4.1682763 -4.1947551][-4.209516 -4.2091374 -4.2266483 -4.2420239 -4.2541003 -4.256403 -4.233418 -4.1996326 -4.1736794 -4.1601238 -4.1507716 -4.1482763 -4.1583853 -4.182148 -4.2070312][-4.2182555 -4.222177 -4.2399573 -4.2575111 -4.2706141 -4.2729712 -4.2530837 -4.2237258 -4.2059493 -4.1984959 -4.189569 -4.1832771 -4.1857281 -4.1985884 -4.2175903][-4.2206511 -4.2305813 -4.247189 -4.2632728 -4.2682338 -4.2630343 -4.2419419 -4.2161617 -4.2110081 -4.2154016 -4.2087379 -4.1992445 -4.1933517 -4.1948423 -4.2065229][-4.2136507 -4.2302737 -4.2437129 -4.2528367 -4.2485976 -4.2320442 -4.203187 -4.1780195 -4.1852169 -4.2056947 -4.2064939 -4.1974783 -4.1859927 -4.1778316 -4.1826377][-4.2018232 -4.2211795 -4.2321591 -4.2331581 -4.2201777 -4.1907334 -4.1477466 -4.11986 -4.1376476 -4.1740651 -4.186583 -4.185122 -4.1730704 -4.1609135 -4.1610732][-4.1919732 -4.2095509 -4.2189574 -4.216145 -4.1956816 -4.1558628 -4.0993876 -4.0645967 -4.0879188 -4.1354055 -4.1597662 -4.1666994 -4.1589713 -4.1470776 -4.1422772][-4.1853261 -4.2007909 -4.2120662 -4.2105489 -4.1910844 -4.1484833 -4.0871348 -4.0502548 -4.0700779 -4.1149044 -4.1426139 -4.1539063 -4.1505303 -4.1406322 -4.1326852][-4.173943 -4.1893754 -4.2034245 -4.2057047 -4.1930132 -4.1556449 -4.1004872 -4.0704226 -4.0837779 -4.1148949 -4.1373844 -4.1501923 -4.1533861 -4.1484394 -4.1425881][-4.1604548 -4.1747222 -4.1886744 -4.1925488 -4.1857176 -4.1577892 -4.1154976 -4.0988545 -4.1105995 -4.1270771 -4.140677 -4.1544461 -4.1652074 -4.1663814 -4.1638927][-4.152451 -4.1663895 -4.1815381 -4.18785 -4.1850319 -4.1626973 -4.1306934 -4.1232743 -4.1325521 -4.1398067 -4.1473303 -4.1627417 -4.179678 -4.18553 -4.1844611][-4.1484323 -4.1658621 -4.1858697 -4.1972928 -4.1959133 -4.1740766 -4.1490235 -4.1451979 -4.152709 -4.1559672 -4.1606145 -4.1732144 -4.1909075 -4.1980133 -4.1969085][-4.14376 -4.1645174 -4.1893907 -4.2075758 -4.2095084 -4.1916556 -4.1752572 -4.1738167 -4.1789503 -4.1784678 -4.178226 -4.1831732 -4.1932759 -4.1961541 -4.1942434][-4.1400738 -4.1582956 -4.1842227 -4.2070317 -4.2160296 -4.2067027 -4.1992207 -4.1986232 -4.1982965 -4.1918483 -4.1853838 -4.1833696 -4.1846681 -4.1815715 -4.1785045]]...]
INFO - root - 2017-12-06 06:55:13.380314: step 9510, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.672 sec/batch; 60h:15m:42s remains)
INFO - root - 2017-12-06 06:55:20.115552: step 9520, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 59h:44m:00s remains)
INFO - root - 2017-12-06 06:55:26.788942: step 9530, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 59h:48m:34s remains)
INFO - root - 2017-12-06 06:55:33.492067: step 9540, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.686 sec/batch; 61h:30m:20s remains)
INFO - root - 2017-12-06 06:55:40.260680: step 9550, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 61h:12m:01s remains)
INFO - root - 2017-12-06 06:55:47.018855: step 9560, loss = 2.10, batch loss = 2.04 (12.0 examples/sec; 0.666 sec/batch; 59h:45m:32s remains)
INFO - root - 2017-12-06 06:55:53.782643: step 9570, loss = 2.03, batch loss = 1.97 (11.1 examples/sec; 0.722 sec/batch; 64h:44m:07s remains)
INFO - root - 2017-12-06 06:56:00.608774: step 9580, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.698 sec/batch; 62h:35m:51s remains)
INFO - root - 2017-12-06 06:56:07.394176: step 9590, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.690 sec/batch; 61h:54m:39s remains)
INFO - root - 2017-12-06 06:56:14.208099: step 9600, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.678 sec/batch; 60h:46m:49s remains)
2017-12-06 06:56:14.827374: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2997451 -4.299654 -4.3005114 -4.2922215 -4.2747068 -4.2440734 -4.2115669 -4.1895595 -4.176445 -4.1734667 -4.1607661 -4.1467333 -4.1242471 -4.0791135 -4.0430045][-4.2800164 -4.2803421 -4.2867103 -4.2798262 -4.2627316 -4.2296653 -4.1950345 -4.1757221 -4.1709561 -4.1706996 -4.1638794 -4.1564131 -4.1413541 -4.0973458 -4.0579915][-4.2557278 -4.2599707 -4.27382 -4.2695255 -4.2538815 -4.2222676 -4.195931 -4.1873164 -4.1921043 -4.1948671 -4.1926537 -4.1938968 -4.1912479 -4.1600256 -4.127861][-4.2136974 -4.2230792 -4.2462182 -4.2493992 -4.2390065 -4.2124252 -4.20074 -4.2042189 -4.2188945 -4.2260919 -4.2271295 -4.2326269 -4.239377 -4.223701 -4.2020464][-4.1538 -4.1659231 -4.1994791 -4.2162266 -4.2130361 -4.1929197 -4.191093 -4.1977973 -4.2197676 -4.2344561 -4.2436571 -4.2539544 -4.2679162 -4.26705 -4.2566071][-4.0982666 -4.1126089 -4.1523795 -4.1802206 -4.1834049 -4.1689048 -4.1701035 -4.175859 -4.2039089 -4.2275872 -4.2452774 -4.2623062 -4.2835226 -4.292841 -4.2911024][-4.0898933 -4.0986309 -4.1251 -4.1469083 -4.1513019 -4.1408587 -4.1374917 -4.1385894 -4.1688056 -4.2020783 -4.2264028 -4.2526765 -4.2832952 -4.2986732 -4.3043628][-4.1147614 -4.1152377 -4.1170092 -4.1185923 -4.111712 -4.0952959 -4.0836573 -4.0795546 -4.1099563 -4.1522522 -4.1854753 -4.2246389 -4.26821 -4.290185 -4.3047395][-4.1390839 -4.1323662 -4.111608 -4.0873985 -4.0543957 -4.0207977 -3.99642 -3.9854243 -4.022387 -4.0844755 -4.1330113 -4.18496 -4.2399793 -4.2692308 -4.2912946][-4.162919 -4.1482892 -4.1138411 -4.0737696 -4.0221272 -3.9887774 -3.9701955 -3.9603851 -3.999191 -4.0705242 -4.12567 -4.1722794 -4.2188516 -4.2476521 -4.2724152][-4.1835332 -4.1649332 -4.1262684 -4.0880337 -4.046916 -4.0318532 -4.0288863 -4.0279174 -4.0574522 -4.1163878 -4.1650753 -4.1973381 -4.2225847 -4.2416339 -4.2618027][-4.1928806 -4.1753359 -4.1451669 -4.1179638 -4.0989647 -4.103559 -4.1090746 -4.1158824 -4.1357222 -4.1764436 -4.2131224 -4.2328005 -4.2428784 -4.2503777 -4.2612][-4.2062244 -4.1889634 -4.1732235 -4.1637993 -4.1647835 -4.1814356 -4.1913738 -4.1990128 -4.2101316 -4.2357054 -4.2591095 -4.2691298 -4.2711296 -4.2693067 -4.2701535][-4.23889 -4.2243209 -4.2245178 -4.2304769 -4.2390957 -4.2575397 -4.2691808 -4.2754936 -4.2796736 -4.2918229 -4.3016005 -4.3028913 -4.2983384 -4.2874618 -4.2768154][-4.27888 -4.2686262 -4.2785826 -4.2906952 -4.3006692 -4.31421 -4.3199005 -4.3185043 -4.3167648 -4.3198838 -4.3222661 -4.3208594 -4.3149657 -4.2986641 -4.2789054]]...]
INFO - root - 2017-12-06 06:56:21.490065: step 9610, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 59h:19m:48s remains)
INFO - root - 2017-12-06 06:56:28.248837: step 9620, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 62h:13m:43s remains)
INFO - root - 2017-12-06 06:56:34.839627: step 9630, loss = 2.06, batch loss = 2.00 (13.2 examples/sec; 0.608 sec/batch; 54h:31m:55s remains)
INFO - root - 2017-12-06 06:56:41.609325: step 9640, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 58h:38m:15s remains)
INFO - root - 2017-12-06 06:56:48.361868: step 9650, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 61h:17m:10s remains)
INFO - root - 2017-12-06 06:56:55.148641: step 9660, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 59h:55m:13s remains)
INFO - root - 2017-12-06 06:57:01.958476: step 9670, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 62h:32m:46s remains)
INFO - root - 2017-12-06 06:57:08.806884: step 9680, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 60h:17m:40s remains)
INFO - root - 2017-12-06 06:57:15.690436: step 9690, loss = 2.11, batch loss = 2.05 (11.9 examples/sec; 0.674 sec/batch; 60h:24m:12s remains)
INFO - root - 2017-12-06 06:57:22.481986: step 9700, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 60h:59m:52s remains)
2017-12-06 06:57:23.108263: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2751961 -4.2810864 -4.2812552 -4.2837186 -4.2879667 -4.2908683 -4.2976804 -4.2863011 -4.2770772 -4.2787337 -4.2876358 -4.2976747 -4.3093882 -4.3125968 -4.3076][-4.2538185 -4.2665491 -4.2635522 -4.2638993 -4.2641459 -4.2628918 -4.2669168 -4.2471943 -4.236979 -4.2481332 -4.2667489 -4.285069 -4.3044343 -4.30879 -4.3027735][-4.2324066 -4.2534919 -4.2474389 -4.2426076 -4.2360086 -4.2274823 -4.2221913 -4.1866541 -4.1756063 -4.2014656 -4.2333956 -4.2631359 -4.2903848 -4.2970347 -4.291997][-4.2200975 -4.2458649 -4.2377033 -4.2239156 -4.2053308 -4.1842804 -4.1617942 -4.1012654 -4.0900331 -4.1392436 -4.1872411 -4.2318764 -4.2675462 -4.2806096 -4.280406][-4.2196612 -4.2479954 -4.2349734 -4.2092762 -4.178122 -4.146997 -4.0981865 -3.9997835 -3.9873147 -4.0657535 -4.1334667 -4.1939778 -4.2408495 -4.2647715 -4.2717795][-4.2279668 -4.2491198 -4.2338119 -4.2013893 -4.161294 -4.1187687 -4.0353365 -3.8889234 -3.8783889 -3.9974449 -4.0918274 -4.1634459 -4.219738 -4.2515779 -4.2653017][-4.2366152 -4.2510176 -4.2417521 -4.211905 -4.1613522 -4.0916266 -3.9534523 -3.7458627 -3.7492132 -3.9325004 -4.0636249 -4.1471095 -4.210959 -4.2470207 -4.2642808][-4.2268624 -4.2359939 -4.238553 -4.2213888 -4.1673656 -4.0729775 -3.8890128 -3.6369321 -3.6627746 -3.8996415 -4.0573096 -4.1474304 -4.2113543 -4.2493157 -4.2670388][-4.2114096 -4.2155104 -4.2263975 -4.2241049 -4.1778646 -4.0837784 -3.912082 -3.7061875 -3.7481983 -3.9567475 -4.0925946 -4.1674085 -4.2213683 -4.2570806 -4.2733011][-4.1977148 -4.1971822 -4.21502 -4.2256217 -4.1942849 -4.1131349 -3.972162 -3.8255191 -3.872895 -4.0340052 -4.1385403 -4.1956835 -4.2390137 -4.2686262 -4.2802267][-4.1900878 -4.1833143 -4.205688 -4.227447 -4.2091441 -4.1442876 -4.0376487 -3.9347434 -3.9772532 -4.096283 -4.1764641 -4.222188 -4.2582703 -4.2803993 -4.2867107][-4.1905928 -4.1809487 -4.2101221 -4.2400932 -4.2354994 -4.1975331 -4.1269689 -4.0567503 -4.0873289 -4.1666079 -4.2222362 -4.2546387 -4.2816234 -4.2953453 -4.2953897][-4.2061062 -4.1939487 -4.2239351 -4.25753 -4.265347 -4.2522874 -4.2122078 -4.1662164 -4.1835618 -4.2310214 -4.267396 -4.2893443 -4.3047385 -4.3084784 -4.3031907][-4.2174096 -4.2094169 -4.2385416 -4.2689672 -4.2822304 -4.2837481 -4.26381 -4.233933 -4.2412958 -4.2703185 -4.29506 -4.305985 -4.3120036 -4.3099966 -4.3030381][-4.2424216 -4.2399788 -4.2658896 -4.2910862 -4.3059978 -4.3122931 -4.3002553 -4.2795353 -4.2787948 -4.296804 -4.31032 -4.309866 -4.3084297 -4.3034611 -4.2968817]]...]
INFO - root - 2017-12-06 06:57:29.888969: step 9710, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.671 sec/batch; 60h:11m:43s remains)
INFO - root - 2017-12-06 06:57:36.764464: step 9720, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 59h:40m:12s remains)
INFO - root - 2017-12-06 06:57:43.398117: step 9730, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.678 sec/batch; 60h:44m:44s remains)
INFO - root - 2017-12-06 06:57:50.135085: step 9740, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 61h:24m:49s remains)
INFO - root - 2017-12-06 06:57:56.902757: step 9750, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 63h:30m:20s remains)
INFO - root - 2017-12-06 06:58:03.650123: step 9760, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.636 sec/batch; 56h:59m:32s remains)
INFO - root - 2017-12-06 06:58:10.461675: step 9770, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.681 sec/batch; 61h:01m:41s remains)
INFO - root - 2017-12-06 06:58:17.315243: step 9780, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.680 sec/batch; 60h:56m:13s remains)
INFO - root - 2017-12-06 06:58:24.114744: step 9790, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.677 sec/batch; 60h:43m:02s remains)
INFO - root - 2017-12-06 06:58:30.958600: step 9800, loss = 2.10, batch loss = 2.04 (11.8 examples/sec; 0.676 sec/batch; 60h:34m:11s remains)
2017-12-06 06:58:31.611364: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.295877 -4.2904625 -4.2856936 -4.2818594 -4.2812896 -4.2843986 -4.2903919 -4.2978187 -4.3053904 -4.3100667 -4.3057928 -4.2954292 -4.2881622 -4.2752666 -4.2595387][-4.2818413 -4.2714252 -4.2615113 -4.2537422 -4.2523251 -4.2582273 -4.2701712 -4.2849336 -4.3001933 -4.3105884 -4.3089881 -4.3004355 -4.2920694 -4.273119 -4.2510505][-4.2773542 -4.2642713 -4.2495103 -4.2358756 -4.2314653 -4.2391968 -4.2560453 -4.2770624 -4.2992487 -4.3140545 -4.3155842 -4.3097038 -4.3013515 -4.2803011 -4.2552218][-4.2713656 -4.2574921 -4.23761 -4.2158012 -4.2049341 -4.2109962 -4.228776 -4.25239 -4.2789612 -4.2987556 -4.3054914 -4.3041539 -4.2983837 -4.2800646 -4.2558703][-4.245204 -4.2284636 -4.202733 -4.1712236 -4.1505995 -4.1517258 -4.1672411 -4.1894093 -4.215416 -4.2375135 -4.2529492 -4.2628736 -4.2680531 -4.2595968 -4.2432933][-4.2098269 -4.1917324 -4.1613426 -4.1214752 -4.090642 -4.0824733 -4.0898147 -4.1042433 -4.1236572 -4.1460285 -4.1721783 -4.1968827 -4.2155428 -4.2182837 -4.2134118][-4.208313 -4.1946197 -4.1676183 -4.1286755 -4.0933371 -4.0732813 -4.0621819 -4.05762 -4.0611105 -4.077939 -4.1119 -4.1469831 -4.173172 -4.1824393 -4.1879258][-4.24671 -4.2391167 -4.2229748 -4.1975584 -4.172905 -4.1551323 -4.1357112 -4.116982 -4.1046171 -4.1063704 -4.1284165 -4.1571383 -4.1790018 -4.1834474 -4.1878338][-4.2730403 -4.2713771 -4.2661619 -4.2557921 -4.2446828 -4.2357707 -4.2193146 -4.19845 -4.178865 -4.164959 -4.1691823 -4.1842422 -4.197536 -4.1941152 -4.1931267][-4.2726283 -4.2744541 -4.2745972 -4.2722373 -4.2688093 -4.2674527 -4.2589784 -4.2438779 -4.2265062 -4.2080479 -4.2023835 -4.2060852 -4.21088 -4.2005138 -4.19507][-4.2712765 -4.2719278 -4.2714167 -4.2686467 -4.2653518 -4.2653527 -4.2626462 -4.254704 -4.2444263 -4.2323937 -4.2285118 -4.2282686 -4.2263246 -4.2115049 -4.2030263][-4.2841997 -4.2804523 -4.2760181 -4.2689366 -4.2605743 -4.2561049 -4.254981 -4.2519445 -4.2485971 -4.2438164 -4.2447572 -4.2436028 -4.23596 -4.2179365 -4.2094359][-4.3044038 -4.2968855 -4.2878876 -4.2758884 -4.2605004 -4.2482796 -4.2433133 -4.2411547 -4.2416444 -4.2415586 -4.2445979 -4.2432127 -4.2334208 -4.2156672 -4.2097759][-4.3281775 -4.3207827 -4.3103209 -4.2953649 -4.274313 -4.25475 -4.2431064 -4.2377186 -4.2372756 -4.2386103 -4.2426281 -4.2410588 -4.23035 -4.2121806 -4.2070179][-4.34376 -4.3400993 -4.3334279 -4.3203487 -4.2998776 -4.2773876 -4.2595053 -4.247251 -4.2415752 -4.2406583 -4.2440653 -4.242496 -4.2332673 -4.2163529 -4.2120605]]...]
INFO - root - 2017-12-06 06:58:38.381576: step 9810, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 61h:39m:08s remains)
INFO - root - 2017-12-06 06:58:45.055491: step 9820, loss = 2.03, batch loss = 1.97 (12.1 examples/sec; 0.659 sec/batch; 59h:01m:46s remains)
INFO - root - 2017-12-06 06:58:51.763195: step 9830, loss = 2.04, batch loss = 1.99 (11.6 examples/sec; 0.690 sec/batch; 61h:48m:43s remains)
INFO - root - 2017-12-06 06:58:58.570473: step 9840, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.688 sec/batch; 61h:38m:24s remains)
INFO - root - 2017-12-06 06:59:05.354079: step 9850, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.691 sec/batch; 61h:55m:43s remains)
INFO - root - 2017-12-06 06:59:12.057621: step 9860, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.663 sec/batch; 59h:22m:59s remains)
INFO - root - 2017-12-06 06:59:18.900558: step 9870, loss = 2.10, batch loss = 2.04 (11.6 examples/sec; 0.688 sec/batch; 61h:37m:47s remains)
INFO - root - 2017-12-06 06:59:25.692002: step 9880, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 60h:51m:17s remains)
INFO - root - 2017-12-06 06:59:32.581955: step 9890, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.694 sec/batch; 62h:10m:46s remains)
INFO - root - 2017-12-06 06:59:39.330038: step 9900, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 58h:50m:17s remains)
2017-12-06 06:59:39.905034: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2660584 -4.2732735 -4.2798882 -4.2842636 -4.2857466 -4.2831817 -4.2784166 -4.2729006 -4.2710357 -4.2713947 -4.2636757 -4.24908 -4.2312059 -4.2134652 -4.1993446][-4.2614231 -4.2678819 -4.2701411 -4.2686663 -4.2657285 -4.2592421 -4.2499771 -4.2432632 -4.2422142 -4.2433238 -4.2381344 -4.2258239 -4.2045722 -4.1779356 -4.1591392][-4.2535481 -4.2567844 -4.2528653 -4.2469974 -4.2380261 -4.2259374 -4.2100992 -4.2014265 -4.2041044 -4.2103734 -4.2118282 -4.2046866 -4.1805849 -4.1461325 -4.1211653][-4.2449245 -4.2419109 -4.2324295 -4.2218008 -4.2096939 -4.1937823 -4.1714797 -4.162189 -4.1732802 -4.1920161 -4.2006669 -4.1982646 -4.1735544 -4.1378541 -4.1022639][-4.250525 -4.2381272 -4.2172766 -4.1975813 -4.1818433 -4.1626353 -4.1375122 -4.1277962 -4.1476479 -4.1765656 -4.1914935 -4.1980515 -4.1812124 -4.1454229 -4.0969381][-4.2523518 -4.2355132 -4.2078519 -4.1808162 -4.1561112 -4.1333294 -4.1086564 -4.1017642 -4.1267891 -4.1566238 -4.174274 -4.191669 -4.18364 -4.1515746 -4.0964203][-4.2374172 -4.218122 -4.1874008 -4.1525235 -4.1180305 -4.0908623 -4.0728774 -4.078599 -4.1072249 -4.1330791 -4.1540523 -4.1805377 -4.1784539 -4.1526327 -4.1066155][-4.2052932 -4.1872978 -4.1573381 -4.1165533 -4.0740218 -4.0418634 -4.0384054 -4.0607114 -4.0958128 -4.1213746 -4.1428351 -4.1682615 -4.1646481 -4.1463671 -4.1185646][-4.1715679 -4.1506038 -4.1167326 -4.0721679 -4.0286121 -4.0016317 -4.0131764 -4.0458527 -4.0850139 -4.1156526 -4.1356459 -4.1516485 -4.1473746 -4.1355529 -4.1154184][-4.1553607 -4.1237392 -4.0853462 -4.0467 -4.0127816 -3.997458 -4.0195203 -4.0472198 -4.0813308 -4.111237 -4.1301594 -4.1432519 -4.1422958 -4.1291361 -4.107089][-4.150486 -4.1067948 -4.060709 -4.0291047 -4.0135908 -4.017755 -4.0438337 -4.0664315 -4.0905333 -4.1113105 -4.128932 -4.1458125 -4.1455774 -4.129353 -4.1021972][-4.1451397 -4.0989695 -4.0528355 -4.0292611 -4.0306387 -4.051446 -4.0796719 -4.094862 -4.1020823 -4.1128497 -4.1357727 -4.1578526 -4.1588526 -4.1404057 -4.1096258][-4.1464481 -4.1056728 -4.0688009 -4.0557432 -4.0663943 -4.0936117 -4.121594 -4.13134 -4.1353288 -4.14536 -4.1670365 -4.18418 -4.1810226 -4.1585994 -4.1245108][-4.1666322 -4.1362357 -4.1143088 -4.1113415 -4.1266174 -4.1492167 -4.169805 -4.1796937 -4.1903882 -4.200191 -4.2130737 -4.21865 -4.2140055 -4.1946554 -4.166194][-4.2098627 -4.1912355 -4.18128 -4.1823788 -4.1971989 -4.2132158 -4.2281733 -4.23814 -4.2485685 -4.256011 -4.2581449 -4.256278 -4.2531018 -4.2416263 -4.2227168]]...]
INFO - root - 2017-12-06 06:59:46.501532: step 9910, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 60h:52m:28s remains)
INFO - root - 2017-12-06 06:59:53.226853: step 9920, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 62h:54m:11s remains)
INFO - root - 2017-12-06 06:59:59.843961: step 9930, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 60h:55m:05s remains)
INFO - root - 2017-12-06 07:00:06.708849: step 9940, loss = 2.05, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 59h:34m:50s remains)
INFO - root - 2017-12-06 07:00:13.541397: step 9950, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 60h:04m:58s remains)
INFO - root - 2017-12-06 07:00:20.447020: step 9960, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 62h:42m:07s remains)
INFO - root - 2017-12-06 07:00:27.203187: step 9970, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.675 sec/batch; 60h:25m:58s remains)
INFO - root - 2017-12-06 07:00:33.975326: step 9980, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.681 sec/batch; 60h:59m:06s remains)
INFO - root - 2017-12-06 07:00:40.813435: step 9990, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.709 sec/batch; 63h:28m:18s remains)
INFO - root - 2017-12-06 07:00:47.695213: step 10000, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.714 sec/batch; 63h:59m:04s remains)
2017-12-06 07:00:48.384320: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2110567 -4.2170038 -4.2265964 -4.2380576 -4.2438545 -4.2421021 -4.2345505 -4.2191381 -4.2037563 -4.1983609 -4.1968317 -4.1939063 -4.2063308 -4.228332 -4.2496252][-4.2164621 -4.2290463 -4.2461967 -4.2632127 -4.272481 -4.2720637 -4.2632866 -4.2481461 -4.234365 -4.2309079 -4.234468 -4.2350578 -4.2409177 -4.2506447 -4.2636838][-4.2252455 -4.2426381 -4.2654948 -4.2842426 -4.2940378 -4.2922511 -4.2821465 -4.2676997 -4.2555141 -4.2550316 -4.2621918 -4.2657862 -4.2684717 -4.2727914 -4.2833495][-4.2397652 -4.2598495 -4.2823253 -4.3004766 -4.3048148 -4.2968225 -4.2820621 -4.266325 -4.2535691 -4.255383 -4.2676945 -4.2747755 -4.2789516 -4.2822013 -4.2901635][-4.2418175 -4.2580032 -4.2788672 -4.2942195 -4.2901554 -4.2719607 -4.2483354 -4.223918 -4.2084913 -4.2181115 -4.2428827 -4.2603941 -4.270998 -4.2783189 -4.2876468][-4.2274814 -4.2390494 -4.2542648 -4.2588124 -4.2408195 -4.20745 -4.1691914 -4.127286 -4.1124506 -4.1430788 -4.1892629 -4.2255173 -4.247973 -4.2636576 -4.2786522][-4.2124743 -4.2134442 -4.2101655 -4.1921377 -4.15854 -4.1113772 -4.0529547 -3.9870391 -3.9770145 -4.0466294 -4.1241622 -4.1806569 -4.2150979 -4.2406631 -4.2651353][-4.2067447 -4.1931477 -4.1636562 -4.1179328 -4.0646429 -3.9974847 -3.9085658 -3.8029387 -3.8101864 -3.9388041 -4.0481353 -4.1164627 -4.1580267 -4.1971936 -4.2383657][-4.2149134 -4.1978045 -4.1547651 -4.0927935 -4.0255423 -3.9425268 -3.838614 -3.731849 -3.7593744 -3.8963051 -3.9950986 -4.0514932 -4.0892177 -4.1363873 -4.1936388][-4.2223682 -4.2212076 -4.197814 -4.1551495 -4.1042686 -4.039474 -3.9706714 -3.9129736 -3.9352663 -4.00657 -4.0438471 -4.056417 -4.0700741 -4.1140828 -4.1740947][-4.2129107 -4.2305512 -4.2352686 -4.2229023 -4.1985726 -4.1655536 -4.1282191 -4.0982795 -4.1030254 -4.123785 -4.1213274 -4.1104355 -4.1146851 -4.1516018 -4.2018657][-4.1845875 -4.2201228 -4.2458854 -4.2533932 -4.2456727 -4.2289128 -4.2070255 -4.186729 -4.1787806 -4.1754041 -4.1610503 -4.1558595 -4.1733761 -4.2155361 -4.2584014][-4.1671824 -4.2143321 -4.2468624 -4.2580237 -4.2526031 -4.235846 -4.2130356 -4.1985788 -4.1895385 -4.1795993 -4.165524 -4.1720004 -4.2010813 -4.2471871 -4.2895379][-4.1778083 -4.2261534 -4.2529173 -4.2553873 -4.2425437 -4.2173028 -4.1882548 -4.1736579 -4.166213 -4.1585927 -4.1515851 -4.1664433 -4.1970735 -4.242466 -4.2837868][-4.1949358 -4.23597 -4.2544928 -4.2466516 -4.2266693 -4.1959338 -4.1664276 -4.1527948 -4.1507511 -4.1486826 -4.143651 -4.1565127 -4.1843185 -4.2261481 -4.2647352]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-use_nesterov:False-clip10/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-use_nesterov:False-clip10/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 07:00:55.511882: step 10010, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.658 sec/batch; 58h:54m:39s remains)
INFO - root - 2017-12-06 07:01:02.297076: step 10020, loss = 2.08, batch loss = 2.03 (12.1 examples/sec; 0.659 sec/batch; 59h:03m:01s remains)
INFO - root - 2017-12-06 07:01:08.905178: step 10030, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 61h:56m:08s remains)
INFO - root - 2017-12-06 07:01:15.697635: step 10040, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 62h:23m:39s remains)
INFO - root - 2017-12-06 07:01:22.447074: step 10050, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 58h:45m:51s remains)
INFO - root - 2017-12-06 07:01:29.304600: step 10060, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.678 sec/batch; 60h:45m:13s remains)
INFO - root - 2017-12-06 07:01:35.923716: step 10070, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.681 sec/batch; 61h:01m:40s remains)
INFO - root - 2017-12-06 07:01:42.790589: step 10080, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 60h:31m:49s remains)
INFO - root - 2017-12-06 07:01:49.533066: step 10090, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.686 sec/batch; 61h:23m:35s remains)
INFO - root - 2017-12-06 07:01:56.166851: step 10100, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 57h:08m:51s remains)
2017-12-06 07:01:56.865583: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2135487 -4.2126446 -4.2292023 -4.2646713 -4.2963157 -4.3193078 -4.3307328 -4.3278008 -4.3163795 -4.2963266 -4.2768307 -4.2719541 -4.2838192 -4.3016877 -4.3110967][-4.2053881 -4.1959848 -4.2028809 -4.2301197 -4.261198 -4.2880087 -4.307508 -4.3117671 -4.3047833 -4.284893 -4.2629795 -4.2545662 -4.2634306 -4.2800183 -4.2904544][-4.205472 -4.1874342 -4.1809978 -4.1926742 -4.2155676 -4.2409234 -4.2645292 -4.2754316 -4.2767677 -4.2654405 -4.2495742 -4.2400947 -4.2425661 -4.25149 -4.2610879][-4.20714 -4.1808934 -4.1610274 -4.157095 -4.1657848 -4.1791263 -4.1967864 -4.2078571 -4.2195797 -4.2255383 -4.2264786 -4.2244616 -4.2235346 -4.22641 -4.2351189][-4.2054172 -4.1700592 -4.1376495 -4.1213555 -4.1137342 -4.1064816 -4.1051927 -4.109107 -4.130374 -4.1590805 -4.1842642 -4.199657 -4.2079959 -4.2155371 -4.2268934][-4.205966 -4.1600313 -4.1151309 -4.0865817 -4.0610857 -4.0276852 -3.9964483 -3.9827507 -4.0153103 -4.07292 -4.1284871 -4.1712718 -4.2030678 -4.2249355 -4.2402058][-4.21237 -4.1614423 -4.108798 -4.0693665 -4.0261812 -3.9622319 -3.8901243 -3.84759 -3.8899345 -3.9804339 -4.0699296 -4.1417079 -4.198895 -4.23694 -4.2557187][-4.2188239 -4.1710525 -4.122138 -4.0806189 -4.0280938 -3.9429295 -3.8370216 -3.7636428 -3.8054347 -3.9158638 -4.0287952 -4.1221938 -4.196033 -4.2431235 -4.2607512][-4.2113419 -4.1736169 -4.1403289 -4.1119795 -4.0704279 -4.0003295 -3.9097655 -3.840333 -3.8563697 -3.936645 -4.0310612 -4.1173635 -4.1907578 -4.2364521 -4.2504725][-4.1847253 -4.1594167 -4.1459866 -4.1403003 -4.1240163 -4.0885363 -4.03709 -3.9896653 -3.9840517 -4.0238967 -4.0810938 -4.1387272 -4.1909347 -4.2208323 -4.2246208][-4.1496592 -4.1329637 -4.1366754 -4.155509 -4.1659546 -4.161387 -4.1412883 -4.1147966 -4.1025605 -4.1179695 -4.1441388 -4.1678033 -4.1888609 -4.1952076 -4.1836343][-4.1287475 -4.1163831 -4.1296248 -4.1670179 -4.1960478 -4.2086654 -4.2080631 -4.1973677 -4.187746 -4.1902933 -4.1965842 -4.1952443 -4.1904421 -4.1763234 -4.147553][-4.1383948 -4.1337619 -4.1539187 -4.1995978 -4.2330079 -4.2461529 -4.24643 -4.2381926 -4.2281651 -4.22369 -4.2178397 -4.202023 -4.18341 -4.1600752 -4.12692][-4.1799321 -4.1829615 -4.2056489 -4.2486711 -4.27574 -4.27887 -4.2658138 -4.2453575 -4.2250352 -4.208971 -4.1927495 -4.172894 -4.1535835 -4.1345062 -4.11343][-4.2345767 -4.2393303 -4.2603827 -4.294066 -4.3101625 -4.3034835 -4.28191 -4.2530541 -4.2203636 -4.1865807 -4.1565218 -4.1342273 -4.118331 -4.1094284 -4.1079936]]...]
INFO - root - 2017-12-06 07:02:03.755953: step 10110, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 61h:46m:54s remains)
INFO - root - 2017-12-06 07:02:10.530747: step 10120, loss = 2.06, batch loss = 2.00 (14.4 examples/sec; 0.554 sec/batch; 49h:37m:01s remains)
INFO - root - 2017-12-06 07:02:17.296110: step 10130, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 59h:11m:42s remains)
INFO - root - 2017-12-06 07:02:23.968232: step 10140, loss = 2.05, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 59h:36m:35s remains)
INFO - root - 2017-12-06 07:02:30.785241: step 10150, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 59h:02m:28s remains)
INFO - root - 2017-12-06 07:02:37.632849: step 10160, loss = 2.05, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 60h:46m:37s remains)
INFO - root - 2017-12-06 07:02:44.398657: step 10170, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 61h:55m:22s remains)
INFO - root - 2017-12-06 07:02:51.362643: step 10180, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 60h:19m:24s remains)
INFO - root - 2017-12-06 07:02:58.173200: step 10190, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 59h:15m:55s remains)
INFO - root - 2017-12-06 07:03:04.655791: step 10200, loss = 2.08, batch loss = 2.03 (12.3 examples/sec; 0.652 sec/batch; 58h:24m:29s remains)
2017-12-06 07:03:05.386948: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2782459 -4.279387 -4.2829385 -4.2888179 -4.2982917 -4.3054633 -4.3106976 -4.3144813 -4.317523 -4.3208361 -4.3283606 -4.3360929 -4.3390636 -4.3359671 -4.3290682][-4.3000417 -4.3047552 -4.3108845 -4.3156052 -4.3199978 -4.3206005 -4.3179641 -4.3130846 -4.3107386 -4.3126259 -4.32 -4.3280759 -4.33282 -4.3302612 -4.323956][-4.312778 -4.3176003 -4.3229008 -4.3262277 -4.3248425 -4.3164563 -4.3046713 -4.2934 -4.2878766 -4.2910848 -4.2999034 -4.3093381 -4.3169289 -4.3163772 -4.3105245][-4.3125 -4.3188496 -4.3237991 -4.3254952 -4.317812 -4.2984529 -4.2743487 -4.2542443 -4.2481289 -4.2572203 -4.270895 -4.2831459 -4.2922812 -4.2912846 -4.2853179][-4.3022022 -4.3118811 -4.3162847 -4.3138461 -4.29713 -4.2648754 -4.2244549 -4.1930017 -4.1900153 -4.2115269 -4.2355084 -4.2520652 -4.260767 -4.2562046 -4.2499976][-4.2890482 -4.300539 -4.2995386 -4.2869906 -4.257874 -4.2092357 -4.1496658 -4.1103334 -4.1194673 -4.162292 -4.2039957 -4.2284389 -4.23548 -4.2260118 -4.2165871][-4.2753687 -4.2839246 -4.2738395 -4.246479 -4.2003441 -4.1325293 -4.0534067 -4.01333 -4.0455518 -4.1178751 -4.1820426 -4.216033 -4.2222724 -4.2073226 -4.192595][-4.26182 -4.2638521 -4.2429867 -4.2009978 -4.1408863 -4.0611453 -3.9738429 -3.9469376 -4.0091572 -4.1055965 -4.182241 -4.2177744 -4.217906 -4.1977592 -4.17868][-4.2589846 -4.2532787 -4.2243381 -4.1747937 -4.1143007 -4.0454335 -3.9779007 -3.9744928 -4.0475426 -4.1406074 -4.206953 -4.23013 -4.2184405 -4.1943483 -4.177289][-4.27495 -4.2640262 -4.2352448 -4.1914883 -4.1451712 -4.1036096 -4.0688567 -4.0796995 -4.1356063 -4.1990695 -4.2397528 -4.2453046 -4.2222571 -4.1999431 -4.1895533][-4.3003149 -4.2885933 -4.2676177 -4.2365918 -4.2060733 -4.1858096 -4.1731215 -4.1846118 -4.2177229 -4.2514982 -4.2702265 -4.2650647 -4.2416687 -4.2261066 -4.2197161][-4.315496 -4.3076611 -4.2967529 -4.2786465 -4.2611861 -4.2526 -4.2493663 -4.2558084 -4.2725849 -4.2898297 -4.2979369 -4.2924781 -4.2785697 -4.2698293 -4.2604747][-4.3167624 -4.3135209 -4.3085771 -4.2980375 -4.28838 -4.28416 -4.2834544 -4.2858372 -4.2963228 -4.308495 -4.3167057 -4.3174334 -4.3143721 -4.3117576 -4.2988386][-4.3116145 -4.309701 -4.3041592 -4.2937303 -4.2866149 -4.2848229 -4.2866297 -4.2891812 -4.2963967 -4.3060765 -4.3169913 -4.32391 -4.3271828 -4.3289695 -4.3194423][-4.2956057 -4.2893538 -4.2764626 -4.2581234 -4.2454867 -4.2432985 -4.2482986 -4.2553797 -4.265089 -4.2770729 -4.2944984 -4.3078127 -4.3140464 -4.3185 -4.3156152]]...]
INFO - root - 2017-12-06 07:03:12.289050: step 10210, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 57h:38m:59s remains)
INFO - root - 2017-12-06 07:03:18.780652: step 10220, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 58h:57m:41s remains)
INFO - root - 2017-12-06 07:03:25.512624: step 10230, loss = 2.12, batch loss = 2.06 (12.0 examples/sec; 0.666 sec/batch; 59h:37m:21s remains)
INFO - root - 2017-12-06 07:03:32.348851: step 10240, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 60h:32m:53s remains)
INFO - root - 2017-12-06 07:03:39.242485: step 10250, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.711 sec/batch; 63h:37m:59s remains)
INFO - root - 2017-12-06 07:03:46.016654: step 10260, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 61h:17m:12s remains)
INFO - root - 2017-12-06 07:03:52.851527: step 10270, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 61h:37m:30s remains)
INFO - root - 2017-12-06 07:03:59.561710: step 10280, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.687 sec/batch; 61h:26m:55s remains)
INFO - root - 2017-12-06 07:04:06.068814: step 10290, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 57h:56m:01s remains)
INFO - root - 2017-12-06 07:04:13.029446: step 10300, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 61h:10m:48s remains)
2017-12-06 07:04:13.643243: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2715917 -4.2757478 -4.2794065 -4.2842994 -4.288238 -4.2894397 -4.2905273 -4.2900019 -4.287991 -4.2800217 -4.265707 -4.2536039 -4.2483163 -4.2540164 -4.268868][-4.2645826 -4.2663469 -4.2679768 -4.2723961 -4.2786689 -4.2812123 -4.2813196 -4.2806296 -4.2787995 -4.2718315 -4.2589068 -4.2505016 -4.2504392 -4.2591763 -4.273984][-4.2598419 -4.2572703 -4.2581534 -4.2628641 -4.2698684 -4.2704215 -4.265779 -4.2635884 -4.263381 -4.2619104 -4.2547107 -4.2498741 -4.2527113 -4.2624073 -4.2757545][-4.2496037 -4.2422776 -4.2416916 -4.2488651 -4.255518 -4.2517176 -4.2414441 -4.2355933 -4.2407026 -4.2500739 -4.2515082 -4.2501 -4.2500663 -4.2552848 -4.2662911][-4.2191062 -4.2096691 -4.2116008 -4.2223969 -4.22977 -4.2203593 -4.2000904 -4.18888 -4.20248 -4.2292595 -4.2453313 -4.2497292 -4.244144 -4.2403822 -4.2466073][-4.16615 -4.1592984 -4.1718969 -4.1877995 -4.1914935 -4.1667376 -4.1243863 -4.1003275 -4.1266112 -4.1802983 -4.2211785 -4.2370815 -4.2277355 -4.2159314 -4.2210226][-4.10672 -4.1090903 -4.1405835 -4.1694937 -4.1691875 -4.1222792 -4.0459013 -3.9933488 -4.025135 -4.1091156 -4.1789203 -4.2107487 -4.206255 -4.1948938 -4.2043824][-4.0612793 -4.0724354 -4.1249123 -4.17207 -4.1782808 -4.1240077 -4.0298924 -3.9519489 -3.9705656 -4.0589843 -4.1420364 -4.1867023 -4.1935434 -4.1962075 -4.215271][-4.0534177 -4.0689068 -4.1336293 -4.196681 -4.2168841 -4.1761436 -4.0996242 -4.0334888 -4.0347538 -4.0920343 -4.1555905 -4.1974125 -4.2124424 -4.2289343 -4.2530522][-4.0824237 -4.0999022 -4.1601791 -4.2216959 -4.2484469 -4.223721 -4.1723518 -4.1307006 -4.1280279 -4.15655 -4.1915679 -4.221952 -4.2383142 -4.2604842 -4.2862329][-4.1219339 -4.1424036 -4.1956482 -4.2500653 -4.2783279 -4.26435 -4.2306824 -4.2057095 -4.2025681 -4.2078919 -4.2124839 -4.225028 -4.24022 -4.2653127 -4.293982][-4.1425943 -4.1658297 -4.2140622 -4.2659726 -4.2952914 -4.2902803 -4.2725468 -4.2612786 -4.2565088 -4.2448907 -4.2281556 -4.2258148 -4.2369022 -4.2592788 -4.2855344][-4.15323 -4.1704936 -4.2145448 -4.2680864 -4.3051448 -4.3093629 -4.3022285 -4.3001575 -4.29648 -4.2791677 -4.2575703 -4.2467909 -4.2498174 -4.2631192 -4.2823029][-4.1562514 -4.1651974 -4.2044463 -4.2594018 -4.3038058 -4.317071 -4.3173418 -4.3180637 -4.3131452 -4.2972512 -4.2777963 -4.2648544 -4.2611489 -4.2664509 -4.2813954][-4.161088 -4.1642604 -4.1994195 -4.2518783 -4.2956505 -4.3150492 -4.3211765 -4.3212013 -4.311707 -4.2934527 -4.27522 -4.2633061 -4.2575316 -4.2603498 -4.2744617]]...]
INFO - root - 2017-12-06 07:04:20.359067: step 10310, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 60h:37m:39s remains)
INFO - root - 2017-12-06 07:04:27.014050: step 10320, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 61h:00m:27s remains)
INFO - root - 2017-12-06 07:04:33.859995: step 10330, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.694 sec/batch; 62h:05m:41s remains)
INFO - root - 2017-12-06 07:04:40.738034: step 10340, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 60h:28m:16s remains)
INFO - root - 2017-12-06 07:04:47.483320: step 10350, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.685 sec/batch; 61h:20m:29s remains)
INFO - root - 2017-12-06 07:04:54.298257: step 10360, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 60h:10m:43s remains)
INFO - root - 2017-12-06 07:05:01.038701: step 10370, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.691 sec/batch; 61h:50m:55s remains)
INFO - root - 2017-12-06 07:05:07.914656: step 10380, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 63h:33m:24s remains)
INFO - root - 2017-12-06 07:05:14.655437: step 10390, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 61h:49m:46s remains)
INFO - root - 2017-12-06 07:05:21.479789: step 10400, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 61h:06m:52s remains)
2017-12-06 07:05:22.177227: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3491406 -4.3498497 -4.3463254 -4.3342195 -4.3114986 -4.285162 -4.268724 -4.2707191 -4.2915168 -4.3165383 -4.3332462 -4.3412838 -4.3459554 -4.3480597 -4.3482323][-4.3506541 -4.3517995 -4.3464589 -4.3300724 -4.3017449 -4.2710991 -4.2534137 -4.2590604 -4.2854872 -4.3143687 -4.3338089 -4.3439488 -4.3482871 -4.3489809 -4.3485484][-4.3501749 -4.3512344 -4.34309 -4.3209381 -4.2860336 -4.2511277 -4.2335815 -4.2433276 -4.2748208 -4.3073792 -4.3294196 -4.3423462 -4.3474073 -4.3476763 -4.3471489][-4.3473139 -4.34829 -4.3363814 -4.3071442 -4.2639132 -4.2245884 -4.2085586 -4.2231259 -4.259583 -4.2967978 -4.3225346 -4.3389506 -4.3465347 -4.34758 -4.347465][-4.3338132 -4.3330889 -4.3138323 -4.2723274 -4.2165546 -4.1734219 -4.162663 -4.1860232 -4.2299433 -4.2754884 -4.3093572 -4.3316422 -4.3442521 -4.3483682 -4.3487225][-4.3038063 -4.2969456 -4.2636685 -4.203598 -4.1365027 -4.0951338 -4.0937896 -4.126071 -4.1758103 -4.2300954 -4.2764988 -4.3102722 -4.3316503 -4.3419356 -4.3460689][-4.2592249 -4.2403264 -4.1902122 -4.1124706 -4.0409746 -4.010253 -4.0200953 -4.0565162 -4.1047335 -4.1620412 -4.2233171 -4.2748365 -4.3096914 -4.3290009 -4.3389368][-4.2191615 -4.1878963 -4.1235328 -4.0365458 -3.9695125 -3.9508617 -3.9686518 -4.0032077 -4.04193 -4.0944166 -4.1665936 -4.232821 -4.2818041 -4.3135271 -4.3311644][-4.1974664 -4.1603751 -4.0946178 -4.0137873 -3.9597342 -3.9493968 -3.9664865 -3.9915051 -4.0149322 -4.0573797 -4.1330633 -4.2055378 -4.2627907 -4.3041487 -4.3286948][-4.1896119 -4.15173 -4.0937023 -4.0284209 -3.9904721 -3.9871976 -4.0014114 -4.0152159 -4.0245662 -4.0587492 -4.1305251 -4.201643 -4.2608857 -4.3055973 -4.332778][-4.1969204 -4.1621475 -4.1172528 -4.0701885 -4.0466185 -4.0479078 -4.0570693 -4.060153 -4.0600638 -4.0874743 -4.1476049 -4.2089796 -4.2648969 -4.3090138 -4.3360844][-4.2185497 -4.1923738 -4.1606655 -4.1313729 -4.1209188 -4.1281438 -4.1365166 -4.1337719 -4.1281891 -4.1474032 -4.1894531 -4.2344861 -4.278646 -4.3152423 -4.3377528][-4.244349 -4.2290835 -4.2119236 -4.1990166 -4.2000337 -4.211165 -4.21786 -4.2126312 -4.2054954 -4.2150283 -4.2397666 -4.26995 -4.3016763 -4.32679 -4.3400116][-4.270731 -4.261375 -4.2529025 -4.2502956 -4.2582383 -4.2706804 -4.2775822 -4.2751994 -4.2692952 -4.2714992 -4.2832026 -4.3024158 -4.3229713 -4.3369632 -4.34097][-4.29701 -4.29097 -4.286592 -4.2872915 -4.2940392 -4.3027363 -4.3080139 -4.3070736 -4.3027129 -4.3017659 -4.3068705 -4.3184962 -4.3310714 -4.33771 -4.3362818]]...]
INFO - root - 2017-12-06 07:05:28.757758: step 10410, loss = 2.05, batch loss = 2.00 (13.3 examples/sec; 0.604 sec/batch; 54h:00m:31s remains)
INFO - root - 2017-12-06 07:05:35.607009: step 10420, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 61h:04m:15s remains)
INFO - root - 2017-12-06 07:05:42.398659: step 10430, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 61h:27m:31s remains)
INFO - root - 2017-12-06 07:05:49.139606: step 10440, loss = 2.08, batch loss = 2.03 (12.3 examples/sec; 0.650 sec/batch; 58h:07m:26s remains)
INFO - root - 2017-12-06 07:05:55.964070: step 10450, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.707 sec/batch; 63h:13m:40s remains)
INFO - root - 2017-12-06 07:06:02.716397: step 10460, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.657 sec/batch; 58h:44m:51s remains)
INFO - root - 2017-12-06 07:06:09.516764: step 10470, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 59h:12m:24s remains)
INFO - root - 2017-12-06 07:06:16.214137: step 10480, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 60h:01m:55s remains)
INFO - root - 2017-12-06 07:06:22.945441: step 10490, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 60h:13m:51s remains)
INFO - root - 2017-12-06 07:06:29.655048: step 10500, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 59h:54m:03s remains)
2017-12-06 07:06:30.297698: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0532169 -4.008976 -4.0165033 -4.0695691 -4.1181135 -4.1323681 -4.1137943 -4.1022921 -4.1199021 -4.1548786 -4.1960573 -4.2271819 -4.241888 -4.2437763 -4.2465739][-4.0542407 -4.0030346 -4.0125818 -4.0699072 -4.1247921 -4.1501136 -4.1424875 -4.130312 -4.1413 -4.1757817 -4.2128072 -4.23554 -4.2440233 -4.2440553 -4.2489905][-4.0704384 -4.0235977 -4.0334115 -4.0863838 -4.1336164 -4.1556048 -4.1523747 -4.145402 -4.1529865 -4.1819873 -4.2148495 -4.2341027 -4.2393804 -4.2397127 -4.2485738][-4.1035519 -4.061377 -4.063488 -4.0993805 -4.127737 -4.1332569 -4.125948 -4.1220489 -4.1344719 -4.1631961 -4.1987276 -4.2213335 -4.230073 -4.2347045 -4.2470531][-4.1326013 -4.1047258 -4.1061053 -4.1224713 -4.1281643 -4.1127234 -4.0951219 -4.092402 -4.1127896 -4.1407261 -4.1766553 -4.2051773 -4.2215352 -4.2304826 -4.2448993][-4.1425447 -4.1363959 -4.1447783 -4.1471391 -4.1319828 -4.1005015 -4.0767245 -4.0809236 -4.106709 -4.1285505 -4.1577768 -4.187829 -4.2087736 -4.222856 -4.2404809][-4.1503644 -4.1574683 -4.1675735 -4.1590037 -4.1327929 -4.10057 -4.0801387 -4.0922718 -4.1168222 -4.1289277 -4.1471529 -4.1732655 -4.1957521 -4.212184 -4.2334442][-4.1752119 -4.1886463 -4.1924109 -4.1705074 -4.1376657 -4.1129613 -4.103991 -4.1184912 -4.1347494 -4.140461 -4.1504383 -4.1708159 -4.1884103 -4.2036247 -4.227026][-4.2022719 -4.2136517 -4.2118921 -4.1857009 -4.1495113 -4.1334758 -4.1351352 -4.1512527 -4.1628318 -4.1629624 -4.1684146 -4.1772928 -4.1833415 -4.1934247 -4.218123][-4.2116556 -4.2114081 -4.2059612 -4.18507 -4.1584535 -4.1494975 -4.1600418 -4.1768937 -4.1835523 -4.17857 -4.1766877 -4.1760631 -4.1744103 -4.1805048 -4.2052774][-4.1859221 -4.1787162 -4.1743412 -4.1664762 -4.1595597 -4.1625547 -4.1775489 -4.1939297 -4.1969786 -4.18216 -4.1697173 -4.1621876 -4.1581392 -4.1655579 -4.192472][-4.15403 -4.1399293 -4.1308265 -4.132699 -4.1466718 -4.1635361 -4.18015 -4.1957679 -4.198925 -4.1795869 -4.1592064 -4.1462097 -4.142952 -4.1556311 -4.1852131][-4.13739 -4.1126828 -4.0892744 -4.0933208 -4.1254778 -4.1548834 -4.1737423 -4.1879854 -4.193511 -4.1757326 -4.1519127 -4.1364765 -4.1364522 -4.1530724 -4.1830506][-4.1363664 -4.1098084 -4.0795293 -4.0796928 -4.1117983 -4.1395979 -4.1541376 -4.1687641 -4.1839333 -4.1776729 -4.1611071 -4.1497183 -4.1496172 -4.1617541 -4.18568][-4.1405253 -4.1207051 -4.0971889 -4.0946646 -4.1133075 -4.1269255 -4.133368 -4.154119 -4.1796088 -4.1827168 -4.1780577 -4.1741891 -4.1717763 -4.173821 -4.1898341]]...]
INFO - root - 2017-12-06 07:06:37.009911: step 10510, loss = 2.10, batch loss = 2.04 (11.4 examples/sec; 0.703 sec/batch; 62h:51m:57s remains)
INFO - root - 2017-12-06 07:06:43.786824: step 10520, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.673 sec/batch; 60h:10m:32s remains)
INFO - root - 2017-12-06 07:06:50.412652: step 10530, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.665 sec/batch; 59h:26m:29s remains)
INFO - root - 2017-12-06 07:06:57.252324: step 10540, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.675 sec/batch; 60h:19m:42s remains)
INFO - root - 2017-12-06 07:07:04.093009: step 10550, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 62h:20m:15s remains)
INFO - root - 2017-12-06 07:07:10.757295: step 10560, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 59h:42m:41s remains)
INFO - root - 2017-12-06 07:07:17.573910: step 10570, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.675 sec/batch; 60h:22m:25s remains)
INFO - root - 2017-12-06 07:07:24.222343: step 10580, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.669 sec/batch; 59h:47m:45s remains)
INFO - root - 2017-12-06 07:07:30.893784: step 10590, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 60h:13m:41s remains)
INFO - root - 2017-12-06 07:07:37.698073: step 10600, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.688 sec/batch; 61h:29m:25s remains)
2017-12-06 07:07:38.473937: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1636658 -4.0779533 -3.9964821 -3.9868979 -4.0617228 -4.1288967 -4.1567187 -4.1605158 -4.1569567 -4.1599607 -4.1755719 -4.1840754 -4.1861768 -4.1904564 -4.1877165][-4.1578183 -4.0806861 -4.0129194 -4.0075178 -4.0651546 -4.1100917 -4.1222429 -4.1218138 -4.1210523 -4.1335273 -4.1595464 -4.1787796 -4.1894755 -4.2029653 -4.2136259][-4.16372 -4.1040754 -4.0582771 -4.0577025 -4.0927696 -4.1137075 -4.1098676 -4.1065321 -4.1141791 -4.1403713 -4.1746387 -4.1978383 -4.20992 -4.2195158 -4.2293582][-4.1899276 -4.149157 -4.1225429 -4.1239295 -4.1405563 -4.1446881 -4.134438 -4.1323228 -4.14218 -4.1699409 -4.2013383 -4.2230906 -4.23132 -4.2340741 -4.2392321][-4.2198572 -4.1878829 -4.1701536 -4.1704559 -4.1758261 -4.1764441 -4.1761951 -4.1895638 -4.2103324 -4.2277551 -4.2358246 -4.2371268 -4.234148 -4.23226 -4.2376842][-4.2424583 -4.2096839 -4.1882782 -4.1795688 -4.1736712 -4.1709466 -4.1795125 -4.2094221 -4.2466111 -4.2645864 -4.2530231 -4.2316127 -4.2174969 -4.213192 -4.2261143][-4.2444849 -4.2007093 -4.1658659 -4.1416698 -4.121398 -4.109273 -4.1142025 -4.1530266 -4.2037497 -4.227406 -4.2152667 -4.1940966 -4.1869283 -4.1969419 -4.2223206][-4.2228274 -4.1675973 -4.1191492 -4.0768871 -4.0419312 -4.0193095 -4.0179186 -4.0568795 -4.1130214 -4.1401205 -4.1378078 -4.1383748 -4.1584744 -4.1934743 -4.23376][-4.1917186 -4.1272659 -4.07724 -4.03151 -3.9919386 -3.9627388 -3.9504094 -3.9791749 -4.0371203 -4.062439 -4.068356 -4.0943241 -4.1471119 -4.2033391 -4.2494016][-4.1720991 -4.1025639 -4.0553503 -4.0161309 -3.9842415 -3.9600956 -3.9430807 -3.9608095 -4.0088806 -4.0224142 -4.0342603 -4.090435 -4.1711578 -4.2326479 -4.2655234][-4.1748543 -4.1082621 -4.0686116 -4.042202 -4.0281425 -4.0225744 -4.0180268 -4.0249438 -4.0475454 -4.0458126 -4.0575438 -4.1256294 -4.2129607 -4.2671828 -4.2763839][-4.1939616 -4.1416788 -4.1145906 -4.1033983 -4.1037116 -4.1126704 -4.1170921 -4.1146846 -4.125854 -4.1274171 -4.1411834 -4.1917334 -4.2546449 -4.2848077 -4.2694769][-4.2110786 -4.1728864 -4.1592913 -4.162396 -4.17396 -4.1843348 -4.184628 -4.1763959 -4.186759 -4.2000651 -4.2195606 -4.2525725 -4.2827148 -4.2848582 -4.250473][-4.22467 -4.1942244 -4.1890917 -4.2024703 -4.2215271 -4.232513 -4.2288861 -4.2189832 -4.2278285 -4.2447567 -4.2662635 -4.287096 -4.2949376 -4.2804079 -4.2392278][-4.2314558 -4.204915 -4.2032485 -4.2212138 -4.2426715 -4.2539473 -4.2517982 -4.2459679 -4.2554107 -4.2718215 -4.2873664 -4.3002234 -4.2990389 -4.2793779 -4.2399907]]...]
INFO - root - 2017-12-06 07:07:45.136746: step 10610, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 59h:29m:27s remains)
INFO - root - 2017-12-06 07:07:52.025428: step 10620, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 64h:37m:20s remains)
INFO - root - 2017-12-06 07:07:58.737869: step 10630, loss = 2.08, batch loss = 2.03 (12.4 examples/sec; 0.647 sec/batch; 57h:49m:12s remains)
INFO - root - 2017-12-06 07:08:05.538579: step 10640, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.700 sec/batch; 62h:35m:41s remains)
INFO - root - 2017-12-06 07:08:12.379783: step 10650, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 60h:50m:13s remains)
INFO - root - 2017-12-06 07:08:19.138579: step 10660, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 61h:17m:03s remains)
INFO - root - 2017-12-06 07:08:25.648022: step 10670, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.657 sec/batch; 58h:41m:56s remains)
INFO - root - 2017-12-06 07:08:32.332320: step 10680, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 59h:37m:42s remains)
INFO - root - 2017-12-06 07:08:39.153793: step 10690, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 60h:28m:03s remains)
INFO - root - 2017-12-06 07:08:46.043474: step 10700, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 61h:19m:42s remains)
2017-12-06 07:08:46.656034: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1821966 -4.1971993 -4.220439 -4.2436056 -4.2631512 -4.2756648 -4.2846279 -4.2903442 -4.2895789 -4.2835526 -4.275723 -4.2685909 -4.2642694 -4.2606192 -4.2579875][-4.1958613 -4.211328 -4.23349 -4.2555809 -4.2731304 -4.2834377 -4.2901707 -4.2943606 -4.2930188 -4.2868881 -4.278336 -4.2704039 -4.2655735 -4.2620926 -4.2606325][-4.2231646 -4.237124 -4.2543674 -4.2706432 -4.2821174 -4.2873969 -4.2907104 -4.2928891 -4.2905941 -4.2840958 -4.2750297 -4.2670622 -4.2630429 -4.2617173 -4.2625995][-4.2484164 -4.2580166 -4.2680135 -4.2761412 -4.2798219 -4.2804365 -4.2809134 -4.2816348 -4.2796597 -4.2745271 -4.2667384 -4.2599263 -4.2572765 -4.2581835 -4.2610483][-4.2660513 -4.2688775 -4.2696805 -4.26779 -4.2633748 -4.2595439 -4.2579451 -4.2587528 -4.2596135 -4.2592688 -4.2562995 -4.2529411 -4.2526155 -4.2554026 -4.2598729][-4.2780051 -4.2733393 -4.2653961 -4.2543249 -4.2425652 -4.23466 -4.2319369 -4.2343788 -4.2400489 -4.2461858 -4.2498884 -4.2516651 -4.2545013 -4.25905 -4.2643042][-4.2831306 -4.2742653 -4.2617345 -4.2463226 -4.2316074 -4.2221327 -4.2196736 -4.2245026 -4.2344761 -4.245234 -4.253232 -4.2580075 -4.2623081 -4.2672191 -4.272646][-4.2857704 -4.2773318 -4.2650342 -4.2500782 -4.2363491 -4.22807 -4.226902 -4.2329874 -4.2431321 -4.2530613 -4.2601967 -4.2643051 -4.2681766 -4.2724142 -4.2773552][-4.287384 -4.2806849 -4.2704506 -4.2581029 -4.2472606 -4.2416873 -4.2422581 -4.2479973 -4.25518 -4.2609196 -4.2645297 -4.2661676 -4.2681751 -4.2708836 -4.2746725][-4.2877717 -4.2827525 -4.2752705 -4.2663393 -4.2586465 -4.2553363 -4.2568545 -4.26096 -4.2645969 -4.2665582 -4.2671494 -4.2668257 -4.2670593 -4.2681127 -4.2702694][-4.2897811 -4.2861905 -4.2811522 -4.2750654 -4.2699113 -4.2679653 -4.2693653 -4.2718186 -4.273169 -4.2732806 -4.272644 -4.2715626 -4.2708559 -4.2709408 -4.2720828][-4.2928686 -4.2903275 -4.2871566 -4.2832723 -4.2799187 -4.2786722 -4.2796063 -4.2810321 -4.2814226 -4.2810097 -4.2802205 -4.2792072 -4.2782421 -4.2778916 -4.2786584][-4.2949238 -4.2928166 -4.2909164 -4.2887845 -4.2868328 -4.28601 -4.2866983 -4.2876453 -4.287806 -4.2875276 -4.2871552 -4.2865653 -4.28574 -4.2852798 -4.285769][-4.29447 -4.2925353 -4.291111 -4.2898836 -4.2887139 -4.2882781 -4.2888765 -4.2896791 -4.289938 -4.2899523 -4.2901926 -4.290113 -4.289587 -4.2890387 -4.2889791][-4.2934189 -4.2917371 -4.2906103 -4.2897787 -4.2889209 -4.2884235 -4.2885985 -4.2888994 -4.288691 -4.2882276 -4.2882748 -4.2881765 -4.2876039 -4.2868881 -4.2865739]]...]
INFO - root - 2017-12-06 07:08:53.408446: step 10710, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.634 sec/batch; 56h:37m:50s remains)
INFO - root - 2017-12-06 07:09:00.136347: step 10720, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 62h:33m:47s remains)
INFO - root - 2017-12-06 07:09:06.989096: step 10730, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.717 sec/batch; 64h:07m:36s remains)
INFO - root - 2017-12-06 07:09:13.694234: step 10740, loss = 2.09, batch loss = 2.04 (12.2 examples/sec; 0.655 sec/batch; 58h:31m:36s remains)
INFO - root - 2017-12-06 07:09:20.424602: step 10750, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 63h:55m:21s remains)
INFO - root - 2017-12-06 07:09:27.189801: step 10760, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 57h:33m:30s remains)
INFO - root - 2017-12-06 07:09:33.949542: step 10770, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.670 sec/batch; 59h:50m:53s remains)
INFO - root - 2017-12-06 07:09:40.709567: step 10780, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 61h:24m:16s remains)
INFO - root - 2017-12-06 07:09:47.485769: step 10790, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.669 sec/batch; 59h:44m:55s remains)
INFO - root - 2017-12-06 07:09:54.005745: step 10800, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 61h:34m:20s remains)
2017-12-06 07:09:54.712727: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.245995 -4.2559114 -4.2598705 -4.2518654 -4.2408528 -4.2471862 -4.2590322 -4.2509952 -4.2294927 -4.212203 -4.197247 -4.1928926 -4.1995053 -4.20186 -4.2070403][-4.2125196 -4.2270236 -4.23736 -4.2347364 -4.2246938 -4.2289495 -4.2379117 -4.2318363 -4.2150288 -4.2026906 -4.1902785 -4.1916447 -4.2076855 -4.2161837 -4.218441][-4.190464 -4.2089968 -4.2216749 -4.2219467 -4.2127547 -4.2140012 -4.2223277 -4.2191062 -4.2049437 -4.1937194 -4.1820631 -4.182282 -4.2018456 -4.2170835 -4.219892][-4.1915069 -4.2139091 -4.2277522 -4.2285533 -4.2185154 -4.2165685 -4.2247429 -4.22321 -4.2077594 -4.1922293 -4.1729674 -4.1644397 -4.1793451 -4.1969261 -4.2053161][-4.2091827 -4.2319145 -4.2450986 -4.2447567 -4.2339077 -4.2294679 -4.2364869 -4.2357092 -4.218255 -4.1973505 -4.1734848 -4.15625 -4.160387 -4.1721263 -4.1809735][-4.224184 -4.2393155 -4.2445784 -4.2400708 -4.2301035 -4.2240524 -4.2269697 -4.2253013 -4.20942 -4.1931734 -4.177196 -4.1600223 -4.1490154 -4.1454868 -4.1532612][-4.2299037 -4.2329025 -4.2301321 -4.2209873 -4.2075562 -4.1943393 -4.1857381 -4.17309 -4.1538219 -4.1521482 -4.1595135 -4.15608 -4.1420569 -4.1323013 -4.1397934][-4.223299 -4.2179132 -4.2103853 -4.1943908 -4.1710553 -4.146112 -4.123167 -4.0961227 -4.0696683 -4.0825024 -4.1151047 -4.1293068 -4.1219435 -4.1195846 -4.1379633][-4.2128558 -4.206162 -4.19831 -4.1788878 -4.1528845 -4.1281633 -4.1061072 -4.0768309 -4.0507188 -4.0699124 -4.1108437 -4.1303639 -4.1258125 -4.1276822 -4.1578636][-4.213964 -4.2141204 -4.2110572 -4.1967735 -4.181509 -4.1732731 -4.1648765 -4.1436987 -4.1254745 -4.1392808 -4.1639214 -4.1662621 -4.1528058 -4.1518846 -4.1845636][-4.223825 -4.2288346 -4.2304397 -4.2215433 -4.2138925 -4.2160993 -4.22033 -4.2101884 -4.2010441 -4.2101588 -4.2173486 -4.2007751 -4.1758518 -4.1674747 -4.1962638][-4.2450023 -4.2515292 -4.253932 -4.24549 -4.2371235 -4.2376289 -4.2418313 -4.235486 -4.2320647 -4.2443223 -4.2463579 -4.2237263 -4.1949463 -4.1780128 -4.1956005][-4.2752862 -4.2800407 -4.2812276 -4.2717123 -4.2585621 -4.248086 -4.2363758 -4.2257161 -4.2314544 -4.2521982 -4.254137 -4.2315059 -4.2040009 -4.1844769 -4.1914592][-4.2900729 -4.2950025 -4.2955008 -4.2855034 -4.2683706 -4.2473445 -4.21822 -4.1999569 -4.2134461 -4.2471528 -4.2578735 -4.2404313 -4.2161632 -4.2001905 -4.2002249][-4.2889385 -4.2908831 -4.2893434 -4.2809706 -4.265976 -4.2419806 -4.205349 -4.1838427 -4.2046051 -4.2453175 -4.2616768 -4.2486038 -4.2281895 -4.2189503 -4.2168403]]...]
INFO - root - 2017-12-06 07:10:01.387899: step 10810, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.686 sec/batch; 61h:16m:41s remains)
INFO - root - 2017-12-06 07:10:08.139272: step 10820, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 61h:22m:25s remains)
INFO - root - 2017-12-06 07:10:14.947818: step 10830, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.675 sec/batch; 60h:17m:52s remains)
INFO - root - 2017-12-06 07:10:21.742200: step 10840, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 60h:15m:52s remains)
INFO - root - 2017-12-06 07:10:28.395172: step 10850, loss = 2.10, batch loss = 2.04 (12.1 examples/sec; 0.659 sec/batch; 58h:52m:49s remains)
INFO - root - 2017-12-06 07:10:35.122875: step 10860, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.694 sec/batch; 62h:00m:12s remains)
INFO - root - 2017-12-06 07:10:41.944685: step 10870, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 61h:41m:08s remains)
INFO - root - 2017-12-06 07:10:48.730418: step 10880, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 61h:56m:01s remains)
INFO - root - 2017-12-06 07:10:55.552269: step 10890, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.697 sec/batch; 62h:13m:59s remains)
INFO - root - 2017-12-06 07:11:02.144217: step 10900, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.675 sec/batch; 60h:17m:49s remains)
2017-12-06 07:11:02.791062: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2410183 -4.2137203 -4.2027278 -4.2302361 -4.2718415 -4.3056378 -4.3263679 -4.3372507 -4.3363733 -4.3106041 -4.2684407 -4.2234969 -4.1792989 -4.1540346 -4.1599665][-4.2091093 -4.1749191 -4.1683321 -4.2008348 -4.2394185 -4.2648425 -4.285378 -4.3066721 -4.3184433 -4.3025951 -4.2673221 -4.2261639 -4.1842465 -4.1617794 -4.1711049][-4.1720629 -4.13698 -4.1370497 -4.1715508 -4.205049 -4.2213368 -4.2390862 -4.264607 -4.2858005 -4.278141 -4.2538986 -4.2229548 -4.1892071 -4.172843 -4.1854177][-4.1433854 -4.1171112 -4.126967 -4.1634045 -4.1887779 -4.1986132 -4.2166505 -4.2415409 -4.2563057 -4.2462697 -4.2291441 -4.210742 -4.1894484 -4.1828995 -4.1996179][-4.1110396 -4.0989614 -4.1168089 -4.151731 -4.1660466 -4.16912 -4.1888413 -4.210187 -4.215065 -4.2000995 -4.1935024 -4.1925955 -4.1872473 -4.1893177 -4.2095084][-4.1022444 -4.1026645 -4.1231146 -4.1500182 -4.1523581 -4.1536531 -4.1676979 -4.1752024 -4.1604486 -4.1348495 -4.1348419 -4.1512361 -4.1618471 -4.1722326 -4.1970606][-4.1243324 -4.1291075 -4.1436772 -4.1597562 -4.1581473 -4.1593332 -4.1619205 -4.1507645 -4.1121807 -4.0675287 -4.0690436 -4.1034069 -4.1288452 -4.1437125 -4.1709628][-4.1500087 -4.1577177 -4.1698928 -4.1821561 -4.1826324 -4.1804495 -4.1703978 -4.1408787 -4.0805774 -4.0177917 -4.0198197 -4.07247 -4.1127844 -4.1302891 -4.1590052][-4.1616354 -4.1763992 -4.1916356 -4.2064052 -4.2119484 -4.2075491 -4.1890121 -4.1526241 -4.0876703 -4.0198374 -4.0243559 -4.0864825 -4.1292229 -4.14197 -4.1686912][-4.1758003 -4.1946363 -4.2120137 -4.223062 -4.2285328 -4.2237029 -4.2085028 -4.178884 -4.1268063 -4.0717821 -4.0809188 -4.1338792 -4.1633439 -4.1665182 -4.1879439][-4.1881065 -4.2079315 -4.22437 -4.2304893 -4.2341018 -4.2321234 -4.2265968 -4.205512 -4.1685328 -4.1342931 -4.1483 -4.1894488 -4.2031574 -4.1952152 -4.2079339][-4.1981816 -4.2203155 -4.2355547 -4.2404413 -4.2431297 -4.24127 -4.2372169 -4.2171669 -4.186748 -4.16718 -4.1894622 -4.226934 -4.2276449 -4.2051435 -4.2077951][-4.2038836 -4.2278013 -4.24403 -4.2502155 -4.2552357 -4.250052 -4.2383451 -4.21606 -4.1902676 -4.1783113 -4.2034583 -4.2346725 -4.2212615 -4.1802883 -4.1736584][-4.1992526 -4.2243018 -4.243639 -4.2529035 -4.2576947 -4.2485261 -4.2295589 -4.2057362 -4.1834364 -4.17795 -4.2047572 -4.2276692 -4.2015066 -4.1454453 -4.1322069][-4.2043233 -4.2337523 -4.2542219 -4.2607784 -4.2607512 -4.2485781 -4.2302914 -4.2061529 -4.1848307 -4.183301 -4.2082362 -4.22018 -4.1831737 -4.1196194 -4.1048141]]...]
INFO - root - 2017-12-06 07:11:09.646081: step 10910, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 62h:04m:40s remains)
INFO - root - 2017-12-06 07:11:16.390545: step 10920, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 60h:59m:39s remains)
INFO - root - 2017-12-06 07:11:23.238379: step 10930, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 60h:56m:38s remains)
INFO - root - 2017-12-06 07:11:30.010364: step 10940, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 60h:42m:59s remains)
INFO - root - 2017-12-06 07:11:36.857017: step 10950, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 61h:20m:57s remains)
INFO - root - 2017-12-06 07:11:43.544183: step 10960, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 63h:03m:39s remains)
INFO - root - 2017-12-06 07:11:50.371522: step 10970, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.697 sec/batch; 62h:14m:13s remains)
INFO - root - 2017-12-06 07:11:57.202234: step 10980, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 61h:10m:55s remains)
INFO - root - 2017-12-06 07:12:04.093145: step 10990, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 62h:54m:20s remains)
INFO - root - 2017-12-06 07:12:10.788215: step 11000, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 61h:21m:01s remains)
2017-12-06 07:12:11.480389: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3137956 -4.3087254 -4.3085513 -4.3096981 -4.3145332 -4.324234 -4.3332424 -4.3395371 -4.342669 -4.3381333 -4.3306293 -4.3253155 -4.3239441 -4.3258152 -4.3287749][-4.2704692 -4.2648234 -4.2713089 -4.2809663 -4.2931876 -4.3058472 -4.3143716 -4.3190336 -4.3218732 -4.32057 -4.3174872 -4.3144341 -4.3112035 -4.308115 -4.3073788][-4.2226658 -4.2101927 -4.2195492 -4.2398849 -4.260653 -4.2745571 -4.2806334 -4.2825928 -4.2829576 -4.2885265 -4.295588 -4.2966738 -4.2906771 -4.2800407 -4.2703748][-4.1813464 -4.1690655 -4.1804032 -4.20781 -4.2351766 -4.2503986 -4.2495627 -4.2429271 -4.2405262 -4.2496481 -4.2676115 -4.27306 -4.2650642 -4.2484274 -4.230648][-4.1494112 -4.1359344 -4.147748 -4.1742606 -4.1978784 -4.2087326 -4.2009115 -4.1920695 -4.1956911 -4.2112679 -4.2397976 -4.2517929 -4.2441278 -4.2212892 -4.1985073][-4.1243358 -4.1112351 -4.1253405 -4.1482286 -4.1606259 -4.1542296 -4.1331162 -4.126565 -4.14622 -4.1743789 -4.2092571 -4.22853 -4.2229915 -4.1962843 -4.1695371][-4.1154518 -4.109551 -4.12808 -4.1466169 -4.1414495 -4.1115007 -4.0717773 -4.0719013 -4.1093607 -4.1499696 -4.1841393 -4.2053003 -4.2016506 -4.1738691 -4.1451545][-4.1310954 -4.1369462 -4.1606259 -4.1687336 -4.1488566 -4.1040249 -4.0557003 -4.0598845 -4.106112 -4.1477861 -4.17437 -4.19546 -4.1934729 -4.1667361 -4.1373935][-4.1667442 -4.1708107 -4.18929 -4.1891875 -4.169384 -4.1305094 -4.093544 -4.0976725 -4.1303825 -4.1585603 -4.1708665 -4.1894808 -4.1929293 -4.1727681 -4.15234][-4.2144742 -4.2111549 -4.2190886 -4.213335 -4.2015176 -4.1805444 -4.1552258 -4.1502614 -4.1638927 -4.1774168 -4.1774197 -4.1903968 -4.2011461 -4.1940727 -4.1852326][-4.262115 -4.258214 -4.258626 -4.2464943 -4.2319942 -4.2206526 -4.2034349 -4.1919813 -4.1946659 -4.2032623 -4.1982417 -4.2005396 -4.2136941 -4.21795 -4.2197423][-4.2916355 -4.2883906 -4.2860451 -4.2724762 -4.2541428 -4.2430654 -4.2296062 -4.2161374 -4.2128925 -4.2197065 -4.2124395 -4.2047806 -4.2173958 -4.2345848 -4.2485943][-4.3055472 -4.3025517 -4.2997494 -4.2891278 -4.2703438 -4.2535982 -4.2356772 -4.2143173 -4.2052517 -4.2118626 -4.2110581 -4.208992 -4.2258077 -4.2483482 -4.2653904][-4.3090372 -4.3068166 -4.3054562 -4.2991672 -4.2834549 -4.2603178 -4.2331343 -4.2019615 -4.187645 -4.1973233 -4.2060833 -4.2171421 -4.2384262 -4.2590446 -4.2702932][-4.3017049 -4.3032651 -4.3051643 -4.3019824 -4.2890062 -4.2638879 -4.2334461 -4.2016211 -4.1886463 -4.1995077 -4.2134953 -4.2286854 -4.2466536 -4.258925 -4.25869]]...]
INFO - root - 2017-12-06 07:12:18.249778: step 11010, loss = 2.07, batch loss = 2.02 (12.6 examples/sec; 0.635 sec/batch; 56h:42m:19s remains)
INFO - root - 2017-12-06 07:12:24.887690: step 11020, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.678 sec/batch; 60h:30m:49s remains)
INFO - root - 2017-12-06 07:12:31.817167: step 11030, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 59h:54m:59s remains)
INFO - root - 2017-12-06 07:12:38.688595: step 11040, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 61h:26m:48s remains)
INFO - root - 2017-12-06 07:12:45.375405: step 11050, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 64h:07m:12s remains)
INFO - root - 2017-12-06 07:12:52.116501: step 11060, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 59h:06m:18s remains)
INFO - root - 2017-12-06 07:12:58.995949: step 11070, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 61h:32m:47s remains)
INFO - root - 2017-12-06 07:13:05.610645: step 11080, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 57h:14m:12s remains)
INFO - root - 2017-12-06 07:13:12.406228: step 11090, loss = 2.10, batch loss = 2.04 (11.5 examples/sec; 0.693 sec/batch; 61h:53m:04s remains)
INFO - root - 2017-12-06 07:13:19.194491: step 11100, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 56h:51m:16s remains)
2017-12-06 07:13:19.826839: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2468567 -4.2529836 -4.2589903 -4.2625141 -4.2604184 -4.2503147 -4.2361856 -4.2193165 -4.2057829 -4.1979494 -4.2008247 -4.2099562 -4.2130804 -4.2117267 -4.2188129][-4.2583632 -4.2688155 -4.2793112 -4.2857418 -4.2792048 -4.2586393 -4.2307987 -4.2040205 -4.18799 -4.1851463 -4.1947093 -4.2124782 -4.2245784 -4.2295966 -4.2356105][-4.262424 -4.2714028 -4.2801743 -4.281208 -4.2644553 -4.2322593 -4.1907554 -4.1590362 -4.1497474 -4.1595626 -4.1804514 -4.2076941 -4.2311158 -4.2424483 -4.2465534][-4.26153 -4.2680945 -4.2714067 -4.2611132 -4.2285824 -4.180892 -4.1219864 -4.0842934 -4.084569 -4.1129546 -4.1503944 -4.1919913 -4.2278981 -4.2451248 -4.245275][-4.2602143 -4.2682104 -4.2659864 -4.2405071 -4.1871128 -4.1173887 -4.0335007 -3.9794505 -3.9833629 -4.0301237 -4.0888844 -4.1495819 -4.2003913 -4.2274981 -4.2302151][-4.2497811 -4.2609777 -4.2551403 -4.215673 -4.1420431 -4.0519695 -3.9498985 -3.8812187 -3.8871138 -3.9478903 -4.0250149 -4.0994596 -4.1604986 -4.1947927 -4.2036691][-4.2463408 -4.2647486 -4.2582765 -4.208395 -4.1238928 -4.0286269 -3.9259939 -3.8519263 -3.8486328 -3.9074123 -3.9892609 -4.0684519 -4.1363854 -4.1722455 -4.1776667][-4.2648864 -4.2861872 -4.277916 -4.222198 -4.1428413 -4.0649481 -3.985992 -3.9255111 -3.9122696 -3.946583 -4.0084853 -4.0768294 -4.1373067 -4.159234 -4.1457572][-4.2864914 -4.2998772 -4.2857857 -4.2336092 -4.1718764 -4.1200624 -4.0779128 -4.0466418 -4.0348024 -4.043221 -4.0732365 -4.114707 -4.1533575 -4.1570387 -4.1275096][-4.2891521 -4.2956138 -4.2780714 -4.2365928 -4.1925125 -4.1576681 -4.1429253 -4.1418829 -4.143229 -4.1432929 -4.153429 -4.1723533 -4.1838045 -4.1698427 -4.1317892][-4.2687397 -4.2729144 -4.2570839 -4.2270169 -4.1969523 -4.1704979 -4.1706781 -4.1918859 -4.2086644 -4.2135653 -4.2170177 -4.2183833 -4.203166 -4.1752453 -4.1375771][-4.2401524 -4.2438021 -4.2325778 -4.2117782 -4.1867962 -4.166358 -4.172473 -4.1983943 -4.2215962 -4.2332153 -4.2373109 -4.2306981 -4.2052422 -4.1707082 -4.1362729][-4.2323079 -4.2328072 -4.2235408 -4.2046676 -4.1784177 -4.158792 -4.1612105 -4.1801429 -4.2072606 -4.2296343 -4.2382326 -4.2312407 -4.2047133 -4.1674271 -4.137589][-4.2510777 -4.2468872 -4.2341924 -4.2144828 -4.1875677 -4.1663141 -4.1627188 -4.1728134 -4.19799 -4.2258615 -4.23912 -4.2293482 -4.1989288 -4.1618752 -4.1410265][-4.2740531 -4.2664957 -4.2531366 -4.2337241 -4.2087803 -4.1877718 -4.1829104 -4.1913629 -4.2149124 -4.2418365 -4.2535295 -4.2351732 -4.1952562 -4.1574578 -4.1417203]]...]
INFO - root - 2017-12-06 07:13:26.687488: step 11110, loss = 2.09, batch loss = 2.04 (11.8 examples/sec; 0.677 sec/batch; 60h:24m:36s remains)
INFO - root - 2017-12-06 07:13:33.639683: step 11120, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.742 sec/batch; 66h:13m:55s remains)
INFO - root - 2017-12-06 07:13:40.373307: step 11130, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 60h:42m:13s remains)
INFO - root - 2017-12-06 07:13:46.967682: step 11140, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 62h:44m:54s remains)
INFO - root - 2017-12-06 07:13:53.830068: step 11150, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 61h:06m:12s remains)
INFO - root - 2017-12-06 07:14:00.662822: step 11160, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 62h:47m:55s remains)
INFO - root - 2017-12-06 07:14:07.459272: step 11170, loss = 2.06, batch loss = 2.01 (10.9 examples/sec; 0.734 sec/batch; 65h:30m:00s remains)
INFO - root - 2017-12-06 07:14:14.176851: step 11180, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 59h:05m:19s remains)
INFO - root - 2017-12-06 07:14:20.939595: step 11190, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 61h:21m:23s remains)
INFO - root - 2017-12-06 07:14:27.775368: step 11200, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 62h:08m:01s remains)
2017-12-06 07:14:28.387563: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.271029 -4.2830329 -4.2848234 -4.2805924 -4.282485 -4.2899227 -4.3000269 -4.3058538 -4.3023047 -4.2892461 -4.2690606 -4.2487183 -4.2398958 -4.2434325 -4.2540941][-4.251442 -4.2636566 -4.2652054 -4.2582469 -4.2560587 -4.2636223 -4.2791243 -4.2913623 -4.2911959 -4.2793884 -4.2595954 -4.2373419 -4.2270851 -4.2317796 -4.24581][-4.2423954 -4.2514882 -4.2455926 -4.2263694 -4.2154269 -4.2202425 -4.2413611 -4.2634788 -4.2710319 -4.2639441 -4.25122 -4.2373452 -4.2324 -4.2392292 -4.2542424][-4.2462611 -4.2490873 -4.2287269 -4.1897244 -4.1642284 -4.162499 -4.1880164 -4.2240858 -4.2459869 -4.2515979 -4.2509556 -4.2506075 -4.2569041 -4.2670937 -4.2786293][-4.2658844 -4.2644777 -4.2291555 -4.1675305 -4.119833 -4.0993767 -4.1141906 -4.160584 -4.2062206 -4.23484 -4.2507653 -4.2634149 -4.2796707 -4.2951584 -4.3045115][-4.2875986 -4.2840505 -4.2406106 -4.1625996 -4.0871716 -4.0324616 -4.01275 -4.0584006 -4.1376319 -4.1989741 -4.2364931 -4.2616558 -4.2852988 -4.3052139 -4.3157597][-4.3006783 -4.2991533 -4.2580109 -4.1744857 -4.0800529 -3.9880261 -3.9111843 -3.9344363 -4.0443478 -4.1452346 -4.2089248 -4.2460818 -4.2730203 -4.2960234 -4.3094807][-4.3077078 -4.3136864 -4.2845197 -4.210031 -4.1129084 -4.0063829 -3.8964207 -3.8840518 -3.990113 -4.1067019 -4.1839147 -4.2255917 -4.2479725 -4.2693477 -4.2862611][-4.3124065 -4.32895 -4.3164191 -4.2594233 -4.1788144 -4.0923042 -4.0005469 -3.973696 -4.0337334 -4.1187325 -4.176218 -4.1996651 -4.2030635 -4.2173176 -4.2454028][-4.3155127 -4.3410649 -4.3434072 -4.3044553 -4.2456594 -4.187593 -4.1264029 -4.1025147 -4.126749 -4.1669445 -4.1841702 -4.1650081 -4.1254411 -4.1213427 -4.16995][-4.3152261 -4.3458619 -4.356791 -4.32944 -4.2864771 -4.2477078 -4.2088094 -4.195529 -4.2052259 -4.2143159 -4.1941638 -4.1265821 -4.0232658 -3.9822586 -4.0574832][-4.3138175 -4.3449454 -4.3573227 -4.3354249 -4.304029 -4.2777386 -4.2539625 -4.2517009 -4.2608023 -4.2571912 -4.2167139 -4.1230073 -3.981673 -3.9016426 -3.9759123][-4.3114452 -4.3406453 -4.3526087 -4.3370085 -4.3151717 -4.2987924 -4.286768 -4.2912278 -4.3032541 -4.2978282 -4.2545266 -4.17141 -4.0502825 -3.9715848 -4.0052915][-4.3086948 -4.3354187 -4.346343 -4.3362265 -4.3234711 -4.3148804 -4.3111072 -4.3178849 -4.33088 -4.327662 -4.2906494 -4.2305093 -4.1526256 -4.1021185 -4.1071577][-4.3069053 -4.3317552 -4.3419604 -4.3343878 -4.3266335 -4.3230767 -4.3233876 -4.3314095 -4.3443341 -4.3415866 -4.3101869 -4.2680154 -4.2222328 -4.197566 -4.1955032]]...]
INFO - root - 2017-12-06 07:14:35.088598: step 11210, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.667 sec/batch; 59h:32m:15s remains)
INFO - root - 2017-12-06 07:14:41.889267: step 11220, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 61h:17m:36s remains)
INFO - root - 2017-12-06 07:14:48.676425: step 11230, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 60h:52m:17s remains)
INFO - root - 2017-12-06 07:14:55.336631: step 11240, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 61h:06m:04s remains)
INFO - root - 2017-12-06 07:15:02.232489: step 11250, loss = 2.03, batch loss = 1.97 (11.2 examples/sec; 0.713 sec/batch; 63h:37m:26s remains)
INFO - root - 2017-12-06 07:15:09.122014: step 11260, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.697 sec/batch; 62h:09m:20s remains)
INFO - root - 2017-12-06 07:15:15.927341: step 11270, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 62h:01m:53s remains)
INFO - root - 2017-12-06 07:15:22.633759: step 11280, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 57h:30m:17s remains)
INFO - root - 2017-12-06 07:15:29.173811: step 11290, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 59h:16m:35s remains)
INFO - root - 2017-12-06 07:15:36.010635: step 11300, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.669 sec/batch; 59h:41m:43s remains)
2017-12-06 07:15:36.673662: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1729746 -4.1734385 -4.16342 -4.1636305 -4.1709476 -4.1671576 -4.1660681 -4.1790547 -4.1878366 -4.1727505 -4.1353226 -4.1012096 -4.1027074 -4.124537 -4.1541982][-4.1709719 -4.1682286 -4.1562982 -4.1556726 -4.1604838 -4.153718 -4.1517048 -4.1671057 -4.17688 -4.1631532 -4.1203551 -4.0755424 -4.0787034 -4.108808 -4.1376872][-4.1438208 -4.1415787 -4.1349158 -4.135181 -4.1383681 -4.1313715 -4.1352534 -4.1532145 -4.1643786 -4.1555519 -4.1157265 -4.0684953 -4.0759258 -4.11208 -4.140882][-4.1089969 -4.1099663 -4.1134334 -4.1176319 -4.121192 -4.1180692 -4.1251206 -4.1405511 -4.1508341 -4.14921 -4.1211553 -4.0820775 -4.0949817 -4.1323237 -4.160347][-4.0900192 -4.0927052 -4.1023121 -4.1090722 -4.109931 -4.1055579 -4.1085982 -4.116189 -4.1238103 -4.1287127 -4.1191988 -4.1019487 -4.1223936 -4.15581 -4.1802268][-4.0903296 -4.0927219 -4.1044631 -4.11063 -4.1045785 -4.0892081 -4.0796938 -4.0768719 -4.0803618 -4.0907092 -4.0967689 -4.0977993 -4.1279526 -4.1638126 -4.1860795][-4.1006002 -4.1038342 -4.1155276 -4.114851 -4.0977426 -4.06763 -4.0424109 -4.0294638 -4.0321236 -4.0423322 -4.0443878 -4.0422931 -4.0838084 -4.1304226 -4.1591859][-4.1141558 -4.1208234 -4.1303883 -4.1182966 -4.0871305 -4.0446887 -4.0082078 -3.9908702 -3.9934525 -3.9978395 -3.985112 -3.9641037 -4.0095739 -4.0704513 -4.1073351][-4.1295581 -4.1373258 -4.1416116 -4.1192713 -4.0803604 -4.0359058 -3.9982026 -3.9836397 -3.9903755 -3.997834 -3.9863689 -3.9596887 -3.9955838 -4.053606 -4.0900168][-4.1340961 -4.1375823 -4.1383467 -4.1144667 -4.0819836 -4.0455027 -4.0140181 -4.00979 -4.0284905 -4.0457129 -4.0447326 -4.0241227 -4.0507159 -4.0973625 -4.12718][-4.1286216 -4.13137 -4.1344552 -4.1179304 -4.0953121 -4.0689754 -4.0451236 -4.0454559 -4.0674648 -4.0875931 -4.0921736 -4.0788546 -4.1031318 -4.1415448 -4.1669297][-4.11707 -4.1238418 -4.1295056 -4.1216807 -4.1097884 -4.09649 -4.0810494 -4.0777531 -4.0902181 -4.1009984 -4.1045012 -4.09841 -4.1228089 -4.1545076 -4.1825566][-4.1168833 -4.1247678 -4.125886 -4.1193419 -4.1189303 -4.1209393 -4.1147027 -4.1098995 -4.1113853 -4.111866 -4.1077619 -4.1011677 -4.1225743 -4.152492 -4.1823478][-4.1214542 -4.1270213 -4.1282377 -4.1235614 -4.1333489 -4.1446686 -4.1434107 -4.1377163 -4.138073 -4.1379328 -4.1262293 -4.1126904 -4.1269655 -4.15426 -4.1793318][-4.1211262 -4.1255732 -4.1317673 -4.1341615 -4.1483989 -4.1631622 -4.1619282 -4.155983 -4.1591167 -4.165638 -4.1588869 -4.14716 -4.1581783 -4.177856 -4.1899314]]...]
INFO - root - 2017-12-06 07:15:43.415014: step 11310, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.675 sec/batch; 60h:14m:38s remains)
INFO - root - 2017-12-06 07:15:50.159049: step 11320, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 59h:39m:07s remains)
INFO - root - 2017-12-06 07:15:56.821555: step 11330, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 61h:01m:50s remains)
INFO - root - 2017-12-06 07:16:03.632333: step 11340, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.687 sec/batch; 61h:14m:48s remains)
INFO - root - 2017-12-06 07:16:10.343017: step 11350, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.672 sec/batch; 59h:56m:42s remains)
INFO - root - 2017-12-06 07:16:17.133841: step 11360, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.686 sec/batch; 61h:13m:39s remains)
INFO - root - 2017-12-06 07:16:23.980570: step 11370, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 60h:25m:04s remains)
INFO - root - 2017-12-06 07:16:30.707408: step 11380, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.686 sec/batch; 61h:10m:17s remains)
INFO - root - 2017-12-06 07:16:37.355140: step 11390, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 59h:07m:09s remains)
INFO - root - 2017-12-06 07:16:44.142455: step 11400, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 60h:27m:36s remains)
2017-12-06 07:16:44.801903: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2164364 -4.2062888 -4.2046151 -4.2194786 -4.2261882 -4.2114196 -4.1766405 -4.1358614 -4.1243391 -4.1442475 -4.1779685 -4.2118053 -4.24167 -4.256134 -4.2579083][-4.270422 -4.2720127 -4.2753024 -4.2816415 -4.2833824 -4.273037 -4.244658 -4.217041 -4.2150264 -4.2249713 -4.24026 -4.2585473 -4.2706728 -4.2699814 -4.263947][-4.3142128 -4.3226714 -4.3260674 -4.319726 -4.3120279 -4.2996054 -4.2790103 -4.2631364 -4.2612329 -4.2658019 -4.2749128 -4.2829456 -4.2809834 -4.2728515 -4.2592826][-4.3462343 -4.3600621 -4.3593078 -4.3441777 -4.3201728 -4.2918744 -4.2639832 -4.2466474 -4.2417183 -4.2501669 -4.2659574 -4.2761455 -4.2731094 -4.2670169 -4.254859][-4.3699179 -4.3797626 -4.3663864 -4.3318291 -4.2835813 -4.2303267 -4.1830873 -4.1582174 -4.1638947 -4.1827784 -4.2115812 -4.2377124 -4.25167 -4.2609663 -4.2584987][-4.3782969 -4.3675628 -4.3289351 -4.2655654 -4.18464 -4.0915694 -4.00577 -3.9791725 -4.0211806 -4.0666442 -4.1154013 -4.169034 -4.2100453 -4.24522 -4.2632194][-4.3548646 -4.3161974 -4.246985 -4.1508784 -4.0321059 -3.8861518 -3.7464161 -3.7242949 -3.8331833 -3.9339623 -4.0249019 -4.1161304 -4.1868978 -4.2431412 -4.2762575][-4.3264256 -4.26503 -4.1727476 -4.0538554 -3.9197519 -3.7634993 -3.620389 -3.6335673 -3.7911305 -3.92443 -4.0319 -4.1275339 -4.2007227 -4.257091 -4.2909179][-4.3229208 -4.2652555 -4.1894026 -4.0969777 -4.0019436 -3.9020076 -3.8212092 -3.84835 -3.9530051 -4.040884 -4.1158075 -4.1832695 -4.2353487 -4.2767739 -4.3032393][-4.3512421 -4.3273034 -4.2935228 -4.2436113 -4.1887569 -4.1274233 -4.0756536 -4.0795403 -4.123539 -4.1701903 -4.2171311 -4.2571516 -4.2838321 -4.3041968 -4.3155518][-4.3564835 -4.3557458 -4.3460402 -4.3216257 -4.2882929 -4.2461753 -4.2088642 -4.2037072 -4.2307873 -4.2640629 -4.2997303 -4.3231821 -4.3319516 -4.3317738 -4.3214493][-4.334753 -4.3345428 -4.3290915 -4.3167262 -4.3015609 -4.2818413 -4.2592196 -4.254549 -4.2794714 -4.3085513 -4.3383646 -4.3546605 -4.3530421 -4.3425989 -4.319355][-4.3006344 -4.2891512 -4.2779098 -4.2663164 -4.2625203 -4.2599187 -4.2547374 -4.25531 -4.2775021 -4.3036 -4.3305478 -4.3436217 -4.3374543 -4.3230176 -4.2972593][-4.2255077 -4.1974626 -4.183126 -4.1778316 -4.1830678 -4.2022586 -4.222168 -4.2383184 -4.2608261 -4.2837515 -4.3084412 -4.3182297 -4.3079858 -4.291657 -4.26993][-4.0922346 -4.0523009 -4.0595479 -4.0876966 -4.1203365 -4.1637478 -4.2077494 -4.2396855 -4.2611561 -4.2758808 -4.2925963 -4.2990518 -4.2890167 -4.2792335 -4.2640395]]...]
INFO - root - 2017-12-06 07:16:51.580332: step 11410, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.696 sec/batch; 62h:03m:42s remains)
INFO - root - 2017-12-06 07:16:58.298925: step 11420, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 61h:36m:53s remains)
INFO - root - 2017-12-06 07:17:05.023569: step 11430, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 60h:51m:59s remains)
INFO - root - 2017-12-06 07:17:11.801645: step 11440, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 61h:35m:41s remains)
INFO - root - 2017-12-06 07:17:18.597960: step 11450, loss = 2.03, batch loss = 1.97 (12.0 examples/sec; 0.668 sec/batch; 59h:35m:00s remains)
INFO - root - 2017-12-06 07:17:25.361138: step 11460, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 60h:29m:36s remains)
INFO - root - 2017-12-06 07:17:32.183040: step 11470, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 61h:40m:27s remains)
INFO - root - 2017-12-06 07:17:38.937093: step 11480, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 61h:12m:35s remains)
INFO - root - 2017-12-06 07:17:45.694677: step 11490, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.679 sec/batch; 60h:32m:30s remains)
INFO - root - 2017-12-06 07:17:50.903053: step 11500, loss = 2.08, batch loss = 2.02 (18.3 examples/sec; 0.436 sec/batch; 38h:53m:07s remains)
2017-12-06 07:17:51.418449: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3118725 -4.3087716 -4.2995138 -4.288578 -4.274569 -4.26241 -4.2545404 -4.2545609 -4.2571306 -4.2548389 -4.2566819 -4.2623291 -4.2728233 -4.2827625 -4.289011][-4.30901 -4.3035893 -4.2918835 -4.2780638 -4.2602568 -4.2438536 -4.2331209 -4.2320046 -4.2385879 -4.2446365 -4.2584577 -4.2720428 -4.2840991 -4.2893062 -4.2903471][-4.3070216 -4.3012743 -4.2875848 -4.2689228 -4.2447734 -4.2214751 -4.2051773 -4.2024112 -4.2113872 -4.2257447 -4.2535172 -4.2786222 -4.2937083 -4.2950387 -4.2930622][-4.3075938 -4.30342 -4.28851 -4.2647104 -4.2308083 -4.1958842 -4.1728349 -4.169467 -4.1820555 -4.204185 -4.2424374 -4.2759743 -4.2930164 -4.2932453 -4.2917924][-4.303175 -4.2951531 -4.2721143 -4.2385387 -4.1913443 -4.1421518 -4.1109972 -4.1155062 -4.1410894 -4.1767917 -4.223897 -4.258296 -4.2750688 -4.2767982 -4.2759862][-4.2837882 -4.2655506 -4.2296591 -4.1825404 -4.1210728 -4.0475659 -3.9945121 -4.0091748 -4.067739 -4.1320128 -4.1896868 -4.2257504 -4.243979 -4.2490025 -4.2477384][-4.2477336 -4.2130041 -4.1597824 -4.0975342 -4.0199561 -3.9204407 -3.8318486 -3.8498173 -3.9557247 -4.0592976 -4.13385 -4.1796546 -4.2071972 -4.2187634 -4.2194095][-4.2106147 -4.161582 -4.0940228 -4.0230818 -3.942018 -3.8381789 -3.7378702 -3.7628219 -3.8959436 -4.0174508 -4.0979457 -4.1498961 -4.1874504 -4.2098961 -4.2171803][-4.1973343 -4.14584 -4.0808063 -4.01983 -3.9592383 -3.8906453 -3.8343709 -3.8633294 -3.9637814 -4.0596247 -4.1257939 -4.1708655 -4.2070165 -4.2336841 -4.2441244][-4.2169538 -4.1699333 -4.1146283 -4.0732603 -4.0408435 -4.0110312 -3.9975729 -4.0276771 -4.0899186 -4.1517296 -4.1983528 -4.2309608 -4.2587633 -4.2809663 -4.287684][-4.2519016 -4.2122507 -4.1691575 -4.1462379 -4.137084 -4.1317134 -4.1407862 -4.1656651 -4.1990962 -4.2323112 -4.2617478 -4.2846904 -4.304873 -4.3201451 -4.3223724][-4.283731 -4.2542992 -4.2239065 -4.2118235 -4.2141967 -4.220356 -4.2343941 -4.2518353 -4.2690129 -4.2844944 -4.3006864 -4.3171673 -4.3324347 -4.3420386 -4.3422732][-4.3069692 -4.2908254 -4.2734842 -4.2685852 -4.2757039 -4.2852345 -4.2978396 -4.3076305 -4.314477 -4.3185306 -4.3258615 -4.3362617 -4.3456583 -4.3510938 -4.3494387][-4.324996 -4.3192086 -4.3096232 -4.306231 -4.3117323 -4.3198147 -4.3284593 -4.3337011 -4.3355193 -4.33571 -4.33825 -4.3428178 -4.3479366 -4.3503127 -4.3488069][-4.3359513 -4.3350377 -4.3294554 -4.3260632 -4.3277731 -4.331964 -4.3369641 -4.3400903 -4.3413143 -4.3414474 -4.3420691 -4.3438888 -4.3462563 -4.3477426 -4.3476491]]...]
INFO - root - 2017-12-06 07:17:56.082611: step 11510, loss = 2.07, batch loss = 2.01 (18.2 examples/sec; 0.440 sec/batch; 39h:11m:53s remains)
INFO - root - 2017-12-06 07:18:00.434423: step 11520, loss = 2.06, batch loss = 2.01 (17.8 examples/sec; 0.448 sec/batch; 39h:58m:31s remains)
INFO - root - 2017-12-06 07:18:05.006852: step 11530, loss = 2.04, batch loss = 1.98 (17.9 examples/sec; 0.446 sec/batch; 39h:48m:05s remains)
INFO - root - 2017-12-06 07:18:09.540193: step 11540, loss = 2.06, batch loss = 2.01 (18.1 examples/sec; 0.441 sec/batch; 39h:17m:58s remains)
INFO - root - 2017-12-06 07:18:14.196373: step 11550, loss = 2.06, batch loss = 2.01 (17.1 examples/sec; 0.467 sec/batch; 41h:39m:12s remains)
INFO - root - 2017-12-06 07:18:18.680980: step 11560, loss = 2.06, batch loss = 2.00 (17.8 examples/sec; 0.449 sec/batch; 40h:03m:19s remains)
INFO - root - 2017-12-06 07:18:23.226416: step 11570, loss = 2.03, batch loss = 1.98 (17.7 examples/sec; 0.453 sec/batch; 40h:22m:05s remains)
INFO - root - 2017-12-06 07:18:27.848002: step 11580, loss = 2.08, batch loss = 2.02 (16.2 examples/sec; 0.495 sec/batch; 44h:06m:00s remains)
INFO - root - 2017-12-06 07:18:32.385733: step 11590, loss = 2.06, batch loss = 2.00 (17.6 examples/sec; 0.453 sec/batch; 40h:24m:22s remains)
INFO - root - 2017-12-06 07:18:36.940990: step 11600, loss = 2.08, batch loss = 2.02 (16.3 examples/sec; 0.490 sec/batch; 43h:38m:06s remains)
2017-12-06 07:18:37.453044: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3118362 -4.3120055 -4.3137035 -4.3177333 -4.3203835 -4.3233919 -4.3283668 -4.3311028 -4.3337703 -4.3374515 -4.3380513 -4.3368597 -4.3358746 -4.3362503 -4.3387146][-4.2792015 -4.2765503 -4.276412 -4.2778473 -4.27926 -4.2814531 -4.287199 -4.292387 -4.2961168 -4.3029342 -4.3072839 -4.3095374 -4.3122835 -4.3157706 -4.3220911][-4.2303109 -4.2220082 -4.2224221 -4.2264724 -4.2297735 -4.2324352 -4.2384219 -4.2457495 -4.2537055 -4.2624583 -4.2696018 -4.2761126 -4.2838349 -4.2925119 -4.303968][-4.1700926 -4.1543 -4.1552496 -4.16429 -4.1738715 -4.1793666 -4.1828575 -4.189517 -4.201273 -4.2128105 -4.2217436 -4.2327371 -4.2450218 -4.2558432 -4.27262][-4.1233783 -4.1031194 -4.1005335 -4.1079922 -4.1153746 -4.1153774 -4.1140022 -4.1184344 -4.1366448 -4.1566944 -4.1719785 -4.1898241 -4.2053208 -4.2200732 -4.2436008][-4.0907645 -4.0669765 -4.0575695 -4.0547457 -4.0527034 -4.04025 -4.0248089 -4.0207853 -4.0410151 -4.0766878 -4.1078353 -4.1374145 -4.163435 -4.1890535 -4.2194242][-4.0801558 -4.0489221 -4.030611 -4.0142541 -3.9932256 -3.9613318 -3.9207728 -3.8941092 -3.9167798 -3.9751062 -4.0326018 -4.0833626 -4.1292076 -4.1688223 -4.2055264][-4.1020794 -4.0698352 -4.0494637 -4.0276637 -3.9933567 -3.9427242 -3.8734963 -3.8160272 -3.8335834 -3.9053333 -3.9818673 -4.0489306 -4.1069579 -4.1562676 -4.1999521][-4.1324573 -4.1094937 -4.09898 -4.0814166 -4.0462246 -3.9919083 -3.9172225 -3.8516326 -3.8593621 -3.9204311 -3.9889798 -4.0529709 -4.1121144 -4.1651611 -4.207581][-4.1562238 -4.1415229 -4.1397538 -4.1298537 -4.1003394 -4.0530272 -3.988718 -3.933516 -3.9327 -3.9745746 -4.0283084 -4.0838952 -4.138164 -4.1865768 -4.2231469][-4.1807814 -4.168931 -4.1670742 -4.1616135 -4.1388755 -4.1014113 -4.0519624 -4.0092816 -4.0030932 -4.0301619 -4.0719218 -4.117806 -4.1615634 -4.2015715 -4.2355962][-4.20988 -4.2012453 -4.1999326 -4.1956306 -4.1779137 -4.1508412 -4.1156082 -4.0849624 -4.0772786 -4.095149 -4.12658 -4.1607609 -4.1906486 -4.2232528 -4.2551913][-4.25054 -4.2460346 -4.2470584 -4.2448287 -4.2343197 -4.2160697 -4.1899791 -4.1639986 -4.1550603 -4.16705 -4.190598 -4.2123404 -4.2307129 -4.25735 -4.2849531][-4.2928758 -4.2939205 -4.2967677 -4.2946596 -4.2875881 -4.2757878 -4.2575431 -4.2386827 -4.2324181 -4.2413754 -4.2575192 -4.2693195 -4.2796535 -4.2974324 -4.31699][-4.3228049 -4.3261628 -4.3281341 -4.3252635 -4.3199091 -4.3133497 -4.3026748 -4.2927523 -4.2912431 -4.2979383 -4.3078012 -4.31387 -4.3187361 -4.3284984 -4.3397417]]...]
INFO - root - 2017-12-06 07:18:41.729522: step 11610, loss = 2.05, batch loss = 1.99 (17.4 examples/sec; 0.460 sec/batch; 41h:00m:24s remains)
INFO - root - 2017-12-06 07:18:46.295046: step 11620, loss = 2.07, batch loss = 2.01 (17.9 examples/sec; 0.447 sec/batch; 39h:52m:39s remains)
INFO - root - 2017-12-06 07:18:50.749140: step 11630, loss = 2.06, batch loss = 2.00 (17.4 examples/sec; 0.459 sec/batch; 40h:53m:12s remains)
INFO - root - 2017-12-06 07:18:55.291859: step 11640, loss = 2.07, batch loss = 2.02 (17.7 examples/sec; 0.452 sec/batch; 40h:17m:20s remains)
INFO - root - 2017-12-06 07:18:59.869387: step 11650, loss = 2.06, batch loss = 2.01 (15.6 examples/sec; 0.514 sec/batch; 45h:49m:01s remains)
INFO - root - 2017-12-06 07:19:04.369042: step 11660, loss = 2.08, batch loss = 2.02 (18.1 examples/sec; 0.442 sec/batch; 39h:23m:50s remains)
INFO - root - 2017-12-06 07:19:08.888227: step 11670, loss = 2.10, batch loss = 2.04 (17.6 examples/sec; 0.455 sec/batch; 40h:34m:57s remains)
INFO - root - 2017-12-06 07:19:13.471755: step 11680, loss = 2.05, batch loss = 2.00 (16.3 examples/sec; 0.491 sec/batch; 43h:48m:01s remains)
INFO - root - 2017-12-06 07:19:18.022408: step 11690, loss = 2.09, batch loss = 2.03 (17.0 examples/sec; 0.470 sec/batch; 41h:54m:50s remains)
INFO - root - 2017-12-06 07:19:22.522287: step 11700, loss = 2.05, batch loss = 1.99 (18.7 examples/sec; 0.427 sec/batch; 38h:03m:07s remains)
2017-12-06 07:19:23.017587: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1382928 -4.130415 -4.1439571 -4.1650677 -4.172091 -4.1665564 -4.1415515 -4.1051731 -4.1011405 -4.124722 -4.1406984 -4.1349769 -4.1205997 -4.1185627 -4.1384425][-4.1286182 -4.1130795 -4.1222839 -4.1441455 -4.1571937 -4.1567521 -4.1374946 -4.1066432 -4.1071653 -4.1348023 -4.1599588 -4.1635466 -4.1548877 -4.1503439 -4.1601806][-4.1297812 -4.1126156 -4.1256466 -4.1532946 -4.16686 -4.1662207 -4.1485534 -4.1222467 -4.1252503 -4.1539531 -4.1850805 -4.1961946 -4.1906176 -4.1861081 -4.1894212][-4.1316848 -4.1142697 -4.1267834 -4.1564388 -4.1701717 -4.164959 -4.1408677 -4.116 -4.1260967 -4.1590886 -4.1936579 -4.2112875 -4.2150145 -4.2132511 -4.2123523][-4.1085563 -4.0882854 -4.0976195 -4.1241474 -4.1386271 -4.1262879 -4.0917678 -4.0667229 -4.0876875 -4.1306739 -4.1705589 -4.1961322 -4.2117481 -4.2202053 -4.2228651][-4.0585876 -4.0321851 -4.0396066 -4.0669484 -4.0799818 -4.0488815 -3.9951587 -3.9673178 -4.0083127 -4.0737906 -4.12829 -4.1640959 -4.1892557 -4.2085843 -4.2161913][-4.0146246 -3.9794092 -3.9813662 -4.005897 -4.0047584 -3.9450104 -3.8641009 -3.8406994 -3.9212451 -4.0202336 -4.0910192 -4.1318741 -4.1585879 -4.1789007 -4.1833191][-4.0248146 -3.9874868 -3.9869485 -4.003818 -3.9836023 -3.9004984 -3.8038392 -3.7904558 -3.9016097 -4.0151834 -4.0882387 -4.1270008 -4.1449471 -4.1538048 -4.1439743][-4.074657 -4.0461988 -4.0494537 -4.0605388 -4.0349946 -3.9614506 -3.8880794 -3.8906779 -3.9885364 -4.0772309 -4.1282721 -4.1539536 -4.1568918 -4.1497846 -4.1288433][-4.114511 -4.0998797 -4.1100578 -4.1214695 -4.1013746 -4.0487709 -4.00184 -4.0111766 -4.08215 -4.1406236 -4.1718297 -4.1879964 -4.1864448 -4.1738806 -4.153532][-4.1469913 -4.1443491 -4.1618881 -4.177506 -4.168375 -4.1358438 -4.1012955 -4.1039295 -4.145834 -4.1779661 -4.1961088 -4.2086148 -4.2080927 -4.1992054 -4.188941][-4.1875486 -4.1954885 -4.2134986 -4.2279711 -4.2240915 -4.2012081 -4.1701894 -4.1593266 -4.1723843 -4.1846323 -4.1952267 -4.2060542 -4.2100515 -4.208364 -4.2107983][-4.2470202 -4.2532306 -4.2598066 -4.2632437 -4.2514815 -4.2237616 -4.1910367 -4.1733828 -4.1717005 -4.174974 -4.1865344 -4.200058 -4.2103834 -4.2176819 -4.2245321][-4.28781 -4.2820315 -4.2705975 -4.2599764 -4.2378511 -4.2020545 -4.1678047 -4.1504035 -4.149416 -4.1573682 -4.1768513 -4.194849 -4.2065034 -4.2117014 -4.2151322][-4.2890482 -4.2700806 -4.2487512 -4.2350183 -4.2135682 -4.1794052 -4.1469345 -4.1301179 -4.1251392 -4.1344395 -4.1600142 -4.1798258 -4.1886415 -4.18981 -4.1886697]]...]
INFO - root - 2017-12-06 07:19:27.330168: step 11710, loss = 2.07, batch loss = 2.01 (18.0 examples/sec; 0.444 sec/batch; 39h:31m:38s remains)
INFO - root - 2017-12-06 07:19:31.798373: step 11720, loss = 2.08, batch loss = 2.03 (18.1 examples/sec; 0.443 sec/batch; 39h:27m:49s remains)
INFO - root - 2017-12-06 07:19:36.328799: step 11730, loss = 2.06, batch loss = 2.00 (17.7 examples/sec; 0.453 sec/batch; 40h:20m:09s remains)
INFO - root - 2017-12-06 07:19:40.840799: step 11740, loss = 2.05, batch loss = 1.99 (17.8 examples/sec; 0.450 sec/batch; 40h:03m:38s remains)
INFO - root - 2017-12-06 07:19:45.374441: step 11750, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 0.473 sec/batch; 42h:09m:34s remains)
INFO - root - 2017-12-06 07:19:49.958624: step 11760, loss = 2.05, batch loss = 1.99 (17.5 examples/sec; 0.457 sec/batch; 40h:45m:05s remains)
INFO - root - 2017-12-06 07:19:54.634742: step 11770, loss = 2.07, batch loss = 2.01 (17.2 examples/sec; 0.465 sec/batch; 41h:23m:58s remains)
INFO - root - 2017-12-06 07:19:59.104550: step 11780, loss = 2.06, batch loss = 2.00 (18.1 examples/sec; 0.443 sec/batch; 39h:28m:55s remains)
INFO - root - 2017-12-06 07:20:03.569596: step 11790, loss = 2.07, batch loss = 2.02 (17.6 examples/sec; 0.455 sec/batch; 40h:34m:42s remains)
INFO - root - 2017-12-06 07:20:07.906627: step 11800, loss = 2.08, batch loss = 2.02 (18.2 examples/sec; 0.439 sec/batch; 39h:07m:10s remains)
2017-12-06 07:20:08.596298: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2337751 -4.2291021 -4.2077551 -4.1892586 -4.1907754 -4.1991663 -4.1928997 -4.1717281 -4.1507659 -4.1435089 -4.146183 -4.144052 -4.1422634 -4.1685386 -4.2038708][-4.2195239 -4.2058563 -4.1793756 -4.1569462 -4.1570959 -4.1740112 -4.182107 -4.1754665 -4.1682615 -4.16528 -4.1683578 -4.1714983 -4.1689363 -4.1888294 -4.2138638][-4.1994104 -4.1829028 -4.1554403 -4.1322536 -4.1319933 -4.1516352 -4.1645808 -4.1691785 -4.1731119 -4.1667361 -4.1640925 -4.1664186 -4.1679015 -4.1847606 -4.2065797][-4.1799822 -4.1642981 -4.1401825 -4.1146779 -4.1146755 -4.1386552 -4.1513638 -4.1581888 -4.1693435 -4.1633854 -4.158143 -4.1568265 -4.1616573 -4.1798892 -4.2041388][-4.1658587 -4.1483817 -4.1260839 -4.107729 -4.1129518 -4.1324983 -4.1415215 -4.1458874 -4.1607318 -4.1662478 -4.1629348 -4.1600318 -4.1659918 -4.1830578 -4.2102604][-4.1500587 -4.1308036 -4.1136465 -4.1074367 -4.1133976 -4.1152377 -4.1090813 -4.10449 -4.1217294 -4.1466784 -4.15088 -4.1513228 -4.1628881 -4.182487 -4.2149444][-4.1130147 -4.091239 -4.0730824 -4.072432 -4.0750041 -4.062571 -4.0401053 -4.0168848 -4.0370479 -4.0850215 -4.1081357 -4.12014 -4.1428361 -4.1707573 -4.2102237][-4.0742149 -4.0529141 -4.0398803 -4.0357656 -4.0342569 -4.0141182 -3.9745185 -3.938519 -3.9648767 -4.0336428 -4.0793881 -4.1034503 -4.1352868 -4.1680012 -4.2119684][-4.0763197 -4.0610895 -4.0609903 -4.0553746 -4.0452914 -4.0224719 -3.9837046 -3.9515455 -3.9764256 -4.0462437 -4.0999303 -4.1258216 -4.1562586 -4.1879544 -4.2256136][-4.11406 -4.1060867 -4.120399 -4.1259165 -4.1135092 -4.0903573 -4.0607514 -4.0338755 -4.0456071 -4.095408 -4.1409087 -4.1627774 -4.1911 -4.2196479 -4.2480197][-4.1557684 -4.1497464 -4.1749654 -4.1961126 -4.1878128 -4.1667171 -4.14433 -4.121376 -4.12211 -4.1530175 -4.187746 -4.2030964 -4.22515 -4.2516851 -4.2724948][-4.18922 -4.184073 -4.2115903 -4.2419124 -4.2372136 -4.2177472 -4.1970763 -4.1745491 -4.1695571 -4.1880612 -4.211998 -4.2233853 -4.2412667 -4.2687421 -4.2838306][-4.2176208 -4.2165551 -4.2408891 -4.268415 -4.2626204 -4.2411394 -4.220438 -4.1990366 -4.1924391 -4.2066164 -4.2303491 -4.2416973 -4.2509761 -4.2739248 -4.2834921][-4.2411547 -4.2459488 -4.2638044 -4.2832146 -4.2737575 -4.2535868 -4.233335 -4.2121553 -4.20761 -4.2242675 -4.2468634 -4.2515311 -4.2500339 -4.2672243 -4.2753496][-4.2631879 -4.2698641 -4.2785983 -4.288291 -4.283205 -4.2744532 -4.2571287 -4.2350564 -4.2304368 -4.2422051 -4.254005 -4.247086 -4.2383351 -4.2530956 -4.2636418]]...]
INFO - root - 2017-12-06 07:20:13.193375: step 11810, loss = 2.07, batch loss = 2.01 (17.7 examples/sec; 0.452 sec/batch; 40h:15m:39s remains)
INFO - root - 2017-12-06 07:20:17.721256: step 11820, loss = 2.04, batch loss = 1.99 (16.6 examples/sec; 0.483 sec/batch; 42h:58m:55s remains)
INFO - root - 2017-12-06 07:20:22.265179: step 11830, loss = 2.05, batch loss = 2.00 (17.7 examples/sec; 0.451 sec/batch; 40h:10m:55s remains)
INFO - root - 2017-12-06 07:20:26.887493: step 11840, loss = 2.10, batch loss = 2.04 (16.5 examples/sec; 0.484 sec/batch; 43h:06m:28s remains)
INFO - root - 2017-12-06 07:20:31.527731: step 11850, loss = 2.08, batch loss = 2.02 (17.8 examples/sec; 0.450 sec/batch; 40h:04m:33s remains)
INFO - root - 2017-12-06 07:20:36.076238: step 11860, loss = 2.06, batch loss = 2.00 (18.7 examples/sec; 0.427 sec/batch; 38h:03m:53s remains)
INFO - root - 2017-12-06 07:20:40.677243: step 11870, loss = 2.03, batch loss = 1.97 (16.8 examples/sec; 0.475 sec/batch; 42h:18m:45s remains)
INFO - root - 2017-12-06 07:20:45.170258: step 11880, loss = 2.06, batch loss = 2.00 (17.4 examples/sec; 0.461 sec/batch; 41h:02m:07s remains)
INFO - root - 2017-12-06 07:20:49.703110: step 11890, loss = 2.06, batch loss = 2.00 (17.8 examples/sec; 0.450 sec/batch; 40h:06m:36s remains)
INFO - root - 2017-12-06 07:20:54.052726: step 11900, loss = 2.06, batch loss = 2.00 (16.4 examples/sec; 0.489 sec/batch; 43h:34m:27s remains)
2017-12-06 07:20:54.543209: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2982287 -4.2922511 -4.2834697 -4.279386 -4.2778683 -4.2706594 -4.2597251 -4.251719 -4.2302914 -4.1995521 -4.1765714 -4.1723785 -4.1894317 -4.2173781 -4.2494025][-4.3114557 -4.3097692 -4.2982073 -4.2862048 -4.2743106 -4.2578421 -4.236937 -4.226213 -4.2095227 -4.189321 -4.181603 -4.1850305 -4.2022467 -4.2334528 -4.2667518][-4.2987 -4.2993245 -4.2911859 -4.2811141 -4.2636843 -4.2372103 -4.2040596 -4.1814785 -4.1560779 -4.1397676 -4.1509848 -4.1697264 -4.1957469 -4.2336469 -4.2658319][-4.275506 -4.2751393 -4.2663326 -4.2545519 -4.2339869 -4.198997 -4.1560082 -4.1215124 -4.0915728 -4.0867991 -4.1187334 -4.1552873 -4.1886773 -4.227634 -4.2539783][-4.2566657 -4.2485819 -4.2334509 -4.2118168 -4.1817241 -4.1337032 -4.0770235 -4.0317545 -4.0104556 -4.0360112 -4.0903015 -4.1385593 -4.1753845 -4.2145624 -4.2374439][-4.2489977 -4.2359238 -4.21673 -4.1817155 -4.1331081 -4.0638237 -3.9833045 -3.9138827 -3.8941197 -3.9641733 -4.0500345 -4.1114011 -4.1577735 -4.1996689 -4.2243557][-4.243978 -4.2337484 -4.2157688 -4.1735168 -4.1100063 -4.0239739 -3.9241054 -3.8302844 -3.806071 -3.9169021 -4.027483 -4.0950675 -4.1452928 -4.1844354 -4.2138386][-4.236073 -4.231823 -4.2185426 -4.1817093 -4.1202874 -4.0424194 -3.9499707 -3.8572555 -3.8339748 -3.9513206 -4.0513482 -4.1060266 -4.1466737 -4.1780887 -4.2095685][-4.2396193 -4.2434812 -4.2373033 -4.2133007 -4.166678 -4.1099763 -4.0391774 -3.9566088 -3.9279661 -4.0202994 -4.09228 -4.1250429 -4.1533127 -4.1784425 -4.2111464][-4.255712 -4.2650061 -4.2658796 -4.2539616 -4.2219329 -4.1843004 -4.1349621 -4.0640631 -4.030654 -4.0960503 -4.1436872 -4.1596961 -4.1770191 -4.1965322 -4.2257929][-4.271605 -4.2806892 -4.2859092 -4.2818637 -4.2651086 -4.243227 -4.2088175 -4.1514525 -4.1234865 -4.175334 -4.2127957 -4.2196765 -4.22565 -4.2383766 -4.2574124][-4.2941442 -4.2963204 -4.2972388 -4.2956724 -4.2895093 -4.2777271 -4.2547774 -4.2149558 -4.1986547 -4.2435913 -4.2752743 -4.2764406 -4.2727251 -4.2777972 -4.284761][-4.3197794 -4.3136649 -4.3072586 -4.3043165 -4.3039918 -4.29965 -4.2874436 -4.2613873 -4.2514839 -4.2844787 -4.3065381 -4.3036633 -4.2970996 -4.2984838 -4.2981529][-4.3297362 -4.3186569 -4.3099313 -4.3092375 -4.3140955 -4.3160114 -4.3125806 -4.3009644 -4.2977529 -4.3155761 -4.3218832 -4.3127694 -4.3050828 -4.3065763 -4.3040948][-4.3295679 -4.3220615 -4.315352 -4.315105 -4.3207912 -4.3256445 -4.3254166 -4.31917 -4.3173361 -4.3230147 -4.3149686 -4.3012562 -4.2951288 -4.3010321 -4.3017254]]...]
INFO - root - 2017-12-06 07:20:59.215306: step 11910, loss = 2.06, batch loss = 2.00 (15.7 examples/sec; 0.508 sec/batch; 45h:14m:29s remains)
INFO - root - 2017-12-06 07:21:03.681200: step 11920, loss = 2.06, batch loss = 2.00 (17.9 examples/sec; 0.448 sec/batch; 39h:53m:37s remains)
INFO - root - 2017-12-06 07:21:08.196192: step 11930, loss = 2.07, batch loss = 2.02 (18.0 examples/sec; 0.445 sec/batch; 39h:36m:51s remains)
INFO - root - 2017-12-06 07:21:12.834127: step 11940, loss = 2.05, batch loss = 2.00 (17.4 examples/sec; 0.461 sec/batch; 41h:00m:25s remains)
INFO - root - 2017-12-06 07:21:17.405998: step 11950, loss = 2.05, batch loss = 1.99 (17.9 examples/sec; 0.448 sec/batch; 39h:54m:20s remains)
INFO - root - 2017-12-06 07:21:22.051185: step 11960, loss = 2.07, batch loss = 2.02 (17.3 examples/sec; 0.462 sec/batch; 41h:10m:40s remains)
INFO - root - 2017-12-06 07:21:26.544865: step 11970, loss = 2.07, batch loss = 2.01 (17.5 examples/sec; 0.457 sec/batch; 40h:42m:40s remains)
INFO - root - 2017-12-06 07:21:30.952895: step 11980, loss = 2.08, batch loss = 2.02 (18.9 examples/sec; 0.424 sec/batch; 37h:45m:05s remains)
INFO - root - 2017-12-06 07:21:35.134433: step 11990, loss = 2.07, batch loss = 2.01 (27.7 examples/sec; 0.289 sec/batch; 25h:41m:47s remains)
INFO - root - 2017-12-06 07:21:39.675098: step 12000, loss = 2.06, batch loss = 2.00 (17.4 examples/sec; 0.460 sec/batch; 40h:59m:44s remains)
2017-12-06 07:21:40.210946: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2451105 -4.24124 -4.2349944 -4.2305236 -4.2343583 -4.2302241 -4.2171135 -4.2010117 -4.1958413 -4.2025852 -4.2148328 -4.2267747 -4.2267132 -4.2168818 -4.2147064][-4.2187572 -4.2150826 -4.2076244 -4.2070403 -4.2103686 -4.20054 -4.1760435 -4.1500797 -4.146059 -4.1621284 -4.1813068 -4.1968689 -4.1946855 -4.178844 -4.1747332][-4.1984682 -4.1916575 -4.1806269 -4.1842918 -4.18882 -4.17345 -4.1397738 -4.1091142 -4.1103659 -4.1309786 -4.1515751 -4.1702366 -4.1674623 -4.1481705 -4.1445451][-4.1942496 -4.1814227 -4.1652884 -4.1692758 -4.1745453 -4.1549621 -4.1192231 -4.0924354 -4.0994091 -4.1139493 -4.1238055 -4.1424508 -4.1438928 -4.1290636 -4.1310635][-4.2036557 -4.1825938 -4.1559629 -4.1536016 -4.1542015 -4.1331596 -4.1023784 -4.0847178 -4.0897675 -4.0866337 -4.0757923 -4.0913382 -4.1111927 -4.11651 -4.1280446][-4.21556 -4.1870751 -4.148562 -4.133956 -4.127254 -4.1097455 -4.0873847 -4.0751915 -4.0656848 -4.0355673 -3.9999321 -4.0175014 -4.0672269 -4.1040182 -4.1280093][-4.2304549 -4.2009792 -4.1584945 -4.1364365 -4.1244564 -4.1086397 -4.0868974 -4.07222 -4.0489554 -3.9928262 -3.940403 -3.9637363 -4.0369406 -4.09595 -4.1299891][-4.2396674 -4.2161913 -4.1793265 -4.1563787 -4.1419358 -4.1223679 -4.0988879 -4.0832577 -4.0573788 -3.9988472 -3.9566081 -3.9847913 -4.0510864 -4.1058884 -4.1432881][-4.2415009 -4.222074 -4.1902518 -4.1668029 -4.1506143 -4.1312962 -4.1152368 -4.1053395 -4.0860152 -4.0475864 -4.0313468 -4.05553 -4.091125 -4.1232696 -4.1549997][-4.2402205 -4.218771 -4.1860886 -4.1615386 -4.154469 -4.1453238 -4.1428356 -4.1414013 -4.13239 -4.11654 -4.1135068 -4.1231976 -4.1242495 -4.1319032 -4.1543016][-4.2434354 -4.2171445 -4.1815782 -4.1553435 -4.1580253 -4.16239 -4.1674743 -4.1770744 -4.1787934 -4.1741538 -4.1712985 -4.1645994 -4.1434488 -4.1375265 -4.1511374][-4.2665758 -4.2358913 -4.1957207 -4.1679754 -4.1727729 -4.18303 -4.1879587 -4.1987538 -4.202632 -4.1993566 -4.1943293 -4.1782708 -4.1480045 -4.13981 -4.153101][-4.2998648 -4.2703738 -4.2303672 -4.20133 -4.1988363 -4.2063508 -4.2040982 -4.2076612 -4.2086158 -4.2060342 -4.2004976 -4.1827011 -4.153028 -4.1477165 -4.162416][-4.3273535 -4.3040738 -4.2681274 -4.2404671 -4.2311144 -4.2307758 -4.2191987 -4.2127509 -4.2120805 -4.213232 -4.2145028 -4.2103696 -4.1965117 -4.1962562 -4.2047906][-4.348721 -4.3321757 -4.3005733 -4.2752595 -4.2579312 -4.2472782 -4.2311249 -4.2194662 -4.2221637 -4.2317605 -4.2485366 -4.2598758 -4.2561283 -4.2593384 -4.2610297]]...]
INFO - root - 2017-12-06 07:21:44.785174: step 12010, loss = 2.03, batch loss = 1.98 (17.7 examples/sec; 0.451 sec/batch; 40h:09m:48s remains)
INFO - root - 2017-12-06 07:21:49.405262: step 12020, loss = 2.07, batch loss = 2.01 (17.6 examples/sec; 0.453 sec/batch; 40h:21m:35s remains)
INFO - root - 2017-12-06 07:21:54.058802: step 12030, loss = 2.04, batch loss = 1.98 (17.5 examples/sec; 0.456 sec/batch; 40h:36m:38s remains)
INFO - root - 2017-12-06 07:21:58.626916: step 12040, loss = 2.04, batch loss = 1.98 (16.8 examples/sec; 0.475 sec/batch; 42h:17m:23s remains)
INFO - root - 2017-12-06 07:22:03.228264: step 12050, loss = 2.08, batch loss = 2.02 (17.1 examples/sec; 0.467 sec/batch; 41h:35m:59s remains)
INFO - root - 2017-12-06 07:22:07.799204: step 12060, loss = 2.08, batch loss = 2.02 (16.7 examples/sec; 0.479 sec/batch; 42h:40m:23s remains)
INFO - root - 2017-12-06 07:22:12.412215: step 12070, loss = 2.06, batch loss = 2.00 (17.9 examples/sec; 0.447 sec/batch; 39h:47m:03s remains)
INFO - root - 2017-12-06 07:22:16.955882: step 12080, loss = 2.09, batch loss = 2.03 (16.9 examples/sec; 0.475 sec/batch; 42h:14m:43s remains)
INFO - root - 2017-12-06 07:22:21.160422: step 12090, loss = 2.10, batch loss = 2.04 (17.8 examples/sec; 0.450 sec/batch; 40h:03m:42s remains)
INFO - root - 2017-12-06 07:22:25.764002: step 12100, loss = 2.07, batch loss = 2.01 (17.4 examples/sec; 0.460 sec/batch; 40h:57m:49s remains)
2017-12-06 07:22:26.252581: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2321091 -4.2256536 -4.220819 -4.2134104 -4.2052312 -4.2055688 -4.20272 -4.1947894 -4.1818008 -4.179328 -4.1869416 -4.1817589 -4.1816087 -4.1998062 -4.2192965][-4.20354 -4.1998382 -4.1985421 -4.1914139 -4.176373 -4.169661 -4.1617913 -4.1456933 -4.1266975 -4.1265354 -4.1419697 -4.1432881 -4.1468997 -4.1717019 -4.1963177][-4.1913991 -4.1925335 -4.196362 -4.188767 -4.164299 -4.1485229 -4.1331067 -4.106956 -4.0844536 -4.0895543 -4.1124749 -4.1212296 -4.1278687 -4.1552639 -4.1824212][-4.1941853 -4.1988626 -4.2058349 -4.1950755 -4.1645288 -4.1366682 -4.1064 -4.066174 -4.0449829 -4.0603776 -4.09373 -4.1128159 -4.1257653 -4.1533923 -4.1810708][-4.1970177 -4.2001686 -4.2046375 -4.1924314 -4.1599669 -4.121644 -4.0740356 -4.0189848 -3.9980214 -4.0284829 -4.0716538 -4.0983357 -4.1141567 -4.1402273 -4.1701584][-4.17595 -4.1761036 -4.1773076 -4.1625924 -4.1318121 -4.0903707 -4.0306587 -3.9548416 -3.9255161 -3.9758532 -4.0384359 -4.077435 -4.0956836 -4.1175356 -4.14835][-4.14085 -4.1455193 -4.1455708 -4.1311226 -4.1028585 -4.0593352 -3.9906316 -3.899406 -3.8575187 -3.9160886 -3.9872663 -4.0340095 -4.0544381 -4.0768528 -4.1136203][-4.1195407 -4.1311607 -4.1302781 -4.1196446 -4.1041675 -4.0649829 -3.998754 -3.9170115 -3.8761375 -3.9121256 -3.9630926 -4.0003519 -4.0140233 -4.0349984 -4.0785728][-4.1249361 -4.1407628 -4.1453238 -4.1432505 -4.1387763 -4.1070166 -4.0510406 -3.9860635 -3.9478307 -3.9592142 -3.9877002 -4.0098553 -4.0129271 -4.0275655 -4.0697861][-4.1531272 -4.1692753 -4.1723218 -4.17257 -4.1769052 -4.1558785 -4.114502 -4.06702 -4.0319743 -4.028368 -4.0410848 -4.0472784 -4.0413136 -4.0497451 -4.0837173][-4.1904612 -4.2039113 -4.2038803 -4.2037268 -4.2116375 -4.1963019 -4.1625443 -4.1276011 -4.0979242 -4.0873709 -4.090343 -4.0868568 -4.0807223 -4.0900307 -4.1155195][-4.2195053 -4.2271957 -4.2248154 -4.2262349 -4.2334161 -4.2228518 -4.1987071 -4.1758461 -4.1534543 -4.1426325 -4.1401639 -4.1335139 -4.1287246 -4.1344023 -4.1486745][-4.246901 -4.2486472 -4.2452106 -4.2482438 -4.2548113 -4.2496018 -4.23804 -4.2284393 -4.2158532 -4.2059712 -4.1993866 -4.1907229 -4.1840758 -4.1820827 -4.1846037][-4.2709236 -4.270925 -4.2687263 -4.2713132 -4.2759666 -4.2749243 -4.2722912 -4.2720361 -4.2686763 -4.2640104 -4.2584796 -4.2480631 -4.2381544 -4.2305665 -4.2259669][-4.2892966 -4.2889004 -4.2877741 -4.2915015 -4.2958946 -4.2970943 -4.2980046 -4.3022742 -4.3044219 -4.3033261 -4.2996058 -4.2900319 -4.2785492 -4.2692518 -4.2640195]]...]
INFO - root - 2017-12-06 07:22:30.816132: step 12110, loss = 2.07, batch loss = 2.01 (17.7 examples/sec; 0.452 sec/batch; 40h:13m:50s remains)
INFO - root - 2017-12-06 07:22:35.412796: step 12120, loss = 2.06, batch loss = 2.00 (17.4 examples/sec; 0.461 sec/batch; 41h:00m:25s remains)
INFO - root - 2017-12-06 07:22:40.104193: step 12130, loss = 2.06, batch loss = 2.00 (17.2 examples/sec; 0.465 sec/batch; 41h:24m:58s remains)
INFO - root - 2017-12-06 07:22:44.701420: step 12140, loss = 2.06, batch loss = 2.01 (16.7 examples/sec; 0.479 sec/batch; 42h:38m:37s remains)
INFO - root - 2017-12-06 07:22:49.200806: step 12150, loss = 2.08, batch loss = 2.02 (18.1 examples/sec; 0.441 sec/batch; 39h:14m:18s remains)
INFO - root - 2017-12-06 07:22:53.724115: step 12160, loss = 2.06, batch loss = 2.01 (17.1 examples/sec; 0.468 sec/batch; 41h:37m:46s remains)
INFO - root - 2017-12-06 07:22:58.293325: step 12170, loss = 2.05, batch loss = 1.99 (17.4 examples/sec; 0.460 sec/batch; 40h:58m:26s remains)
INFO - root - 2017-12-06 07:23:02.672739: step 12180, loss = 2.07, batch loss = 2.01 (17.6 examples/sec; 0.453 sec/batch; 40h:20m:51s remains)
INFO - root - 2017-12-06 07:23:07.275077: step 12190, loss = 2.04, batch loss = 1.98 (17.2 examples/sec; 0.466 sec/batch; 41h:27m:28s remains)
INFO - root - 2017-12-06 07:23:11.822835: step 12200, loss = 2.06, batch loss = 2.00 (17.6 examples/sec; 0.456 sec/batch; 40h:31m:59s remains)
2017-12-06 07:23:12.323537: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.23802 -4.2438231 -4.2620254 -4.2850103 -4.3060575 -4.3210173 -4.3310471 -4.3410425 -4.3486991 -4.3513451 -4.3511596 -4.3506722 -4.3511162 -4.3532948 -4.3556914][-4.2119961 -4.2206883 -4.2430491 -4.2751932 -4.3050528 -4.3244829 -4.3368006 -4.3463006 -4.3517804 -4.3537192 -4.3527966 -4.3508687 -4.3494 -4.3492594 -4.3495736][-4.1705 -4.1854029 -4.2160077 -4.2588344 -4.2967997 -4.3202438 -4.3338509 -4.341651 -4.3442841 -4.34504 -4.3427711 -4.3379564 -4.3335533 -4.3313746 -4.3310747][-4.1526322 -4.1732 -4.2062888 -4.2470927 -4.2813997 -4.3012753 -4.3119903 -4.3172584 -4.3178091 -4.3177223 -4.3141694 -4.3054767 -4.2981868 -4.2955685 -4.298553][-4.186378 -4.20367 -4.22358 -4.2426634 -4.2582231 -4.2674751 -4.2729955 -4.27427 -4.2737069 -4.2732468 -4.2688642 -4.2563381 -4.2480154 -4.2478423 -4.2577658][-4.23356 -4.2376714 -4.234354 -4.22466 -4.2174478 -4.2152486 -4.2164431 -4.2156062 -4.21297 -4.21084 -4.2065372 -4.19645 -4.1946373 -4.2035222 -4.2232246][-4.2640243 -4.2559958 -4.2307053 -4.1951451 -4.1687589 -4.1593342 -4.1609654 -4.1621609 -4.1588755 -4.1567721 -4.1564155 -4.1556087 -4.1664991 -4.1868405 -4.213903][-4.2676444 -4.2530446 -4.2146134 -4.1643791 -4.1288657 -4.1193719 -4.1250243 -4.1311016 -4.1307325 -4.1322751 -4.1392975 -4.1491714 -4.170373 -4.1971955 -4.2253127][-4.2563381 -4.2407613 -4.2014813 -4.1518669 -4.1194134 -4.1143923 -4.123837 -4.1333265 -4.1370177 -4.1449008 -4.1592107 -4.1757135 -4.198421 -4.2222414 -4.2448153][-4.2512078 -4.2401385 -4.2137094 -4.1805186 -4.16041 -4.16148 -4.1726279 -4.1825786 -4.1875024 -4.1956177 -4.208663 -4.2227955 -4.2390938 -4.2541318 -4.2682829][-4.2618022 -4.2566857 -4.2466021 -4.2340174 -4.2266707 -4.2313027 -4.2427273 -4.2529392 -4.256793 -4.2605028 -4.26648 -4.2729673 -4.2802687 -4.2868481 -4.2938848][-4.271924 -4.2718124 -4.2733269 -4.27574 -4.2787113 -4.2858167 -4.2950406 -4.3027492 -4.303925 -4.3015594 -4.299644 -4.3008952 -4.3042593 -4.3074102 -4.3114409][-4.27049 -4.2722359 -4.2765694 -4.2822037 -4.2884588 -4.2959046 -4.3030419 -4.3078542 -4.3063293 -4.3000507 -4.2937937 -4.2930307 -4.2968254 -4.3017273 -4.3073983][-4.2384982 -4.2410541 -4.2432489 -4.2457228 -4.2506938 -4.2579889 -4.2648325 -4.2702193 -4.2700229 -4.2645855 -4.25813 -4.2577677 -4.2638407 -4.2717428 -4.2786131][-4.1697359 -4.1742625 -4.1774869 -4.1804471 -4.1884446 -4.2006497 -4.2106833 -4.2193265 -4.2235122 -4.2204633 -4.214416 -4.2130227 -4.2180309 -4.2259421 -4.2319155]]...]
INFO - root - 2017-12-06 07:23:16.961645: step 12210, loss = 2.05, batch loss = 2.00 (17.0 examples/sec; 0.470 sec/batch; 41h:47m:40s remains)
INFO - root - 2017-12-06 07:23:21.516414: step 12220, loss = 2.06, batch loss = 2.00 (16.8 examples/sec; 0.477 sec/batch; 42h:28m:50s remains)
INFO - root - 2017-12-06 07:23:26.212626: step 12230, loss = 2.08, batch loss = 2.02 (17.7 examples/sec; 0.452 sec/batch; 40h:10m:18s remains)
INFO - root - 2017-12-06 07:23:30.727372: step 12240, loss = 2.04, batch loss = 1.98 (16.6 examples/sec; 0.481 sec/batch; 42h:46m:47s remains)
INFO - root - 2017-12-06 07:23:35.274547: step 12250, loss = 2.09, batch loss = 2.03 (18.4 examples/sec; 0.434 sec/batch; 38h:34m:54s remains)
INFO - root - 2017-12-06 07:23:39.898329: step 12260, loss = 2.06, batch loss = 2.00 (16.5 examples/sec; 0.486 sec/batch; 43h:12m:29s remains)
INFO - root - 2017-12-06 07:23:44.362381: step 12270, loss = 2.07, batch loss = 2.02 (17.7 examples/sec; 0.451 sec/batch; 40h:05m:53s remains)
INFO - root - 2017-12-06 07:23:48.626420: step 12280, loss = 2.05, batch loss = 1.99 (18.5 examples/sec; 0.432 sec/batch; 38h:27m:31s remains)
INFO - root - 2017-12-06 07:23:53.122969: step 12290, loss = 2.05, batch loss = 1.99 (17.7 examples/sec; 0.452 sec/batch; 40h:13m:24s remains)
INFO - root - 2017-12-06 07:23:57.736489: step 12300, loss = 2.05, batch loss = 2.00 (16.7 examples/sec; 0.478 sec/batch; 42h:32m:00s remains)
2017-12-06 07:23:58.298211: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3628368 -4.318152 -4.2540922 -4.1810846 -4.1245909 -4.1275744 -4.18266 -4.2388043 -4.2694378 -4.269001 -4.2493086 -4.2360244 -4.2427778 -4.2748895 -4.3145571][-4.3605375 -4.310998 -4.2459645 -4.1726165 -4.1162906 -4.1225619 -4.1750531 -4.2220383 -4.2417054 -4.2366843 -4.2232451 -4.2257576 -4.248023 -4.2826643 -4.31813][-4.3479738 -4.2952433 -4.2332196 -4.1693068 -4.1224365 -4.1296663 -4.1664591 -4.1891656 -4.1888876 -4.1764903 -4.1723461 -4.1923628 -4.2318439 -4.2752562 -4.3110495][-4.3374028 -4.2825136 -4.2240891 -4.1708336 -4.1356983 -4.1369586 -4.1463733 -4.1438365 -4.1285782 -4.1134877 -4.1214919 -4.1613088 -4.2134504 -4.262712 -4.29933][-4.3391137 -4.2934036 -4.2425265 -4.1961789 -4.1604123 -4.1408834 -4.1102509 -4.0797353 -4.0607963 -4.0600638 -4.0898743 -4.1523867 -4.2160678 -4.26372 -4.2977161][-4.355319 -4.330442 -4.2900925 -4.2412739 -4.18607 -4.1264815 -4.04272 -3.9864728 -3.9940805 -4.0323343 -4.0927329 -4.1691704 -4.2339215 -4.2768989 -4.3052149][-4.3703437 -4.3600769 -4.326612 -4.2691088 -4.1880035 -4.08598 -3.9614499 -3.8983793 -3.9503567 -4.0365562 -4.1181121 -4.1956806 -4.2531185 -4.2898 -4.311821][-4.3819084 -4.3779407 -4.3461123 -4.282496 -4.1867294 -4.064292 -3.9306409 -3.8836958 -3.9644275 -4.0722952 -4.1574926 -4.2251735 -4.272191 -4.3016319 -4.3173876][-4.3971071 -4.3971772 -4.3622189 -4.2953415 -4.1972542 -4.0818949 -3.9703982 -3.9505191 -4.0319495 -4.1328149 -4.20881 -4.2607183 -4.2911606 -4.3096113 -4.3180141][-4.4082618 -4.4082441 -4.3681936 -4.2958121 -4.2038279 -4.109726 -4.03816 -4.0459104 -4.1192918 -4.1986647 -4.2581506 -4.2923403 -4.3071284 -4.3142929 -4.3190355][-4.4078751 -4.4074593 -4.3698711 -4.3015361 -4.2189984 -4.1480966 -4.1136189 -4.1373029 -4.1955676 -4.2514968 -4.2923326 -4.3086991 -4.3112822 -4.3138409 -4.3198566][-4.4008446 -4.4005919 -4.3729472 -4.3191085 -4.2539182 -4.2047963 -4.1935925 -4.2182584 -4.2565908 -4.2899895 -4.3103747 -4.3071666 -4.3008657 -4.3009863 -4.3103909][-4.3867297 -4.3864117 -4.3670845 -4.3303313 -4.2851758 -4.2550521 -4.2556257 -4.2776794 -4.3008728 -4.3179379 -4.3187566 -4.3019972 -4.2878513 -4.2881145 -4.303534][-4.3698792 -4.37213 -4.356317 -4.3320842 -4.3057671 -4.2909455 -4.2966037 -4.3151927 -4.3275614 -4.3303618 -4.3177743 -4.293056 -4.2764797 -4.2808676 -4.3045187][-4.3569078 -4.3625932 -4.3513484 -4.3343263 -4.3176117 -4.3088508 -4.3132052 -4.3242245 -4.3276238 -4.3189335 -4.2993417 -4.274519 -4.2616696 -4.271946 -4.302618]]...]
INFO - root - 2017-12-06 07:24:02.807901: step 12310, loss = 2.08, batch loss = 2.02 (17.5 examples/sec; 0.457 sec/batch; 40h:38m:58s remains)
INFO - root - 2017-12-06 07:24:07.377214: step 12320, loss = 2.05, batch loss = 1.99 (16.7 examples/sec; 0.480 sec/batch; 42h:43m:46s remains)
INFO - root - 2017-12-06 07:24:11.928843: step 12330, loss = 2.06, batch loss = 2.00 (17.9 examples/sec; 0.447 sec/batch; 39h:43m:08s remains)
INFO - root - 2017-12-06 07:24:16.475410: step 12340, loss = 2.08, batch loss = 2.02 (18.3 examples/sec; 0.438 sec/batch; 38h:58m:58s remains)
INFO - root - 2017-12-06 07:24:21.059996: step 12350, loss = 2.07, batch loss = 2.01 (17.7 examples/sec; 0.453 sec/batch; 40h:17m:29s remains)
INFO - root - 2017-12-06 07:24:25.770251: step 12360, loss = 2.06, batch loss = 2.01 (17.6 examples/sec; 0.456 sec/batch; 40h:32m:04s remains)
INFO - root - 2017-12-06 07:24:30.096659: step 12370, loss = 2.05, batch loss = 1.99 (17.6 examples/sec; 0.455 sec/batch; 40h:28m:56s remains)
INFO - root - 2017-12-06 07:24:34.663724: step 12380, loss = 2.05, batch loss = 1.99 (17.9 examples/sec; 0.447 sec/batch; 39h:47m:30s remains)
INFO - root - 2017-12-06 07:24:39.222747: step 12390, loss = 2.09, batch loss = 2.03 (17.5 examples/sec; 0.456 sec/batch; 40h:34m:42s remains)
INFO - root - 2017-12-06 07:24:43.714875: step 12400, loss = 2.06, batch loss = 2.00 (16.7 examples/sec; 0.480 sec/batch; 42h:42m:32s remains)
2017-12-06 07:24:44.224513: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1970925 -4.1857452 -4.1877856 -4.1985989 -4.2123117 -4.2303605 -4.2439728 -4.2460642 -4.2348318 -4.2219625 -4.2125769 -4.207541 -4.2073927 -4.2138186 -4.2315068][-4.1880093 -4.1780372 -4.1801181 -4.1882534 -4.1995435 -4.2150488 -4.225647 -4.222662 -4.2046232 -4.1868439 -4.1775446 -4.1742163 -4.1754532 -4.1851034 -4.2097025][-4.2200837 -4.2122793 -4.2089591 -4.205534 -4.205471 -4.2123013 -4.2164493 -4.2107468 -4.194756 -4.1840639 -4.184999 -4.1896887 -4.1940546 -4.2026749 -4.2230058][-4.2627878 -4.2559776 -4.247859 -4.2347646 -4.22439 -4.2219577 -4.2180166 -4.2089324 -4.1967959 -4.1954546 -4.2082953 -4.2234497 -4.2332096 -4.2411013 -4.2543788][-4.2762485 -4.2648754 -4.2570105 -4.24836 -4.2395487 -4.231658 -4.216959 -4.1968217 -4.1811624 -4.1852913 -4.2062235 -4.2306914 -4.2477965 -4.260253 -4.2693744][-4.2539654 -4.2361031 -4.2307472 -4.2333775 -4.2334251 -4.2237978 -4.1938839 -4.1490021 -4.1182203 -4.1213837 -4.1480632 -4.1813211 -4.2084818 -4.2300854 -4.2402525][-4.2207026 -4.1965728 -4.1869259 -4.1941605 -4.2004666 -4.1871405 -4.1393003 -4.065866 -4.0132337 -4.0100079 -4.0394039 -4.0803561 -4.118093 -4.150527 -4.1646113][-4.21537 -4.1928482 -4.180079 -4.1818857 -4.1820531 -4.1620073 -4.1042466 -4.0167131 -3.9535811 -3.9482644 -3.9753959 -4.0064855 -4.032362 -4.0577016 -4.0679727][-4.2405057 -4.2268944 -4.21681 -4.2123647 -4.2035241 -4.1839929 -4.1381493 -4.0687532 -4.0186076 -4.01342 -4.0289507 -4.0418091 -4.0433779 -4.0401082 -4.0280519][-4.248229 -4.239759 -4.2307239 -4.2248306 -4.2177591 -4.2133665 -4.1969566 -4.1604218 -4.1313982 -4.1272235 -4.1326756 -4.13578 -4.1296091 -4.1151919 -4.0888205][-4.2211924 -4.2114854 -4.2030535 -4.1986179 -4.1962576 -4.2056127 -4.2136703 -4.2064767 -4.19705 -4.1980453 -4.1987414 -4.1957655 -4.1889529 -4.1792464 -4.1605377][-4.2020626 -4.1910071 -4.1808643 -4.1756945 -4.1754169 -4.1883783 -4.2035208 -4.2098579 -4.213346 -4.219789 -4.2180252 -4.2089944 -4.202147 -4.1950059 -4.1844115][-4.2039576 -4.1933284 -4.1856766 -4.1822405 -4.179316 -4.1851993 -4.1937609 -4.20103 -4.2078209 -4.2146177 -4.2125168 -4.2027674 -4.196054 -4.1883578 -4.1812105][-4.2061706 -4.2055717 -4.2075586 -4.2094474 -4.2059855 -4.2052665 -4.2027917 -4.2022223 -4.2042336 -4.2056322 -4.2007518 -4.1939635 -4.1896505 -4.1813354 -4.1729374][-4.1900573 -4.2043295 -4.2187991 -4.2277069 -4.2264309 -4.22404 -4.2141848 -4.2064619 -4.2008204 -4.1925707 -4.1858683 -4.1837015 -4.1842937 -4.178288 -4.1686373]]...]
INFO - root - 2017-12-06 07:24:48.786609: step 12410, loss = 2.09, batch loss = 2.03 (17.2 examples/sec; 0.465 sec/batch; 41h:20m:52s remains)
INFO - root - 2017-12-06 07:24:53.364286: step 12420, loss = 2.04, batch loss = 1.99 (17.1 examples/sec; 0.466 sec/batch; 41h:28m:35s remains)
INFO - root - 2017-12-06 07:24:57.916717: step 12430, loss = 2.04, batch loss = 1.98 (16.8 examples/sec; 0.475 sec/batch; 42h:13m:34s remains)
INFO - root - 2017-12-06 07:25:02.484355: step 12440, loss = 2.05, batch loss = 1.99 (17.8 examples/sec; 0.450 sec/batch; 40h:00m:34s remains)
INFO - root - 2017-12-06 07:25:07.136341: step 12450, loss = 2.07, batch loss = 2.01 (18.3 examples/sec; 0.437 sec/batch; 38h:53m:03s remains)
INFO - root - 2017-12-06 07:25:11.632805: step 12460, loss = 2.07, batch loss = 2.02 (17.5 examples/sec; 0.457 sec/batch; 40h:39m:36s remains)
INFO - root - 2017-12-06 07:25:15.995500: step 12470, loss = 2.06, batch loss = 2.00 (17.4 examples/sec; 0.459 sec/batch; 40h:49m:04s remains)
INFO - root - 2017-12-06 07:25:20.480369: step 12480, loss = 2.06, batch loss = 2.00 (18.4 examples/sec; 0.436 sec/batch; 38h:43m:41s remains)
INFO - root - 2017-12-06 07:25:25.003116: step 12490, loss = 2.07, batch loss = 2.02 (17.8 examples/sec; 0.450 sec/batch; 39h:58m:48s remains)
INFO - root - 2017-12-06 07:25:29.627499: step 12500, loss = 2.06, batch loss = 2.00 (16.9 examples/sec; 0.474 sec/batch; 42h:10m:11s remains)
2017-12-06 07:25:30.162138: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1227355 -4.1521616 -4.1894283 -4.2099242 -4.2162828 -4.2078018 -4.1877279 -4.1745076 -4.1891575 -4.2110281 -4.2222939 -4.2322178 -4.2486105 -4.2463803 -4.2330303][-4.1038437 -4.128736 -4.1647906 -4.190249 -4.2081532 -4.2088876 -4.1949878 -4.1868138 -4.201508 -4.2189856 -4.2220135 -4.225318 -4.2329311 -4.2272592 -4.2143273][-4.1240177 -4.1317453 -4.1538973 -4.1783991 -4.2043571 -4.2175131 -4.209466 -4.2042823 -4.2181454 -4.2350497 -4.2375035 -4.2369785 -4.2393727 -4.2300224 -4.2141843][-4.1719184 -4.1606121 -4.1647873 -4.1774774 -4.1985626 -4.2144575 -4.2076926 -4.2002907 -4.2147923 -4.2391467 -4.2465291 -4.2446332 -4.2457166 -4.2372169 -4.2183771][-4.2173877 -4.2029195 -4.1946411 -4.1935306 -4.1949472 -4.1923347 -4.1731796 -4.151247 -4.1632533 -4.2039518 -4.2320805 -4.24393 -4.248323 -4.2445321 -4.2281737][-4.2401843 -4.2261128 -4.2143841 -4.2041907 -4.1841927 -4.1467209 -4.0975976 -4.0451512 -4.0562115 -4.129096 -4.1953049 -4.2343554 -4.2555 -4.2609162 -4.2511034][-4.226263 -4.2066903 -4.1931691 -4.1794395 -4.143754 -4.0770121 -3.9887874 -3.8960323 -3.9086998 -4.0302782 -4.1429863 -4.2160177 -4.2584457 -4.2724915 -4.2665977][-4.1954689 -4.1734209 -4.16076 -4.14623 -4.1070251 -4.0363541 -3.9308267 -3.8113801 -3.8190231 -3.9641261 -4.1017809 -4.1936851 -4.2447457 -4.2599745 -4.2561917][-4.1839395 -4.1665955 -4.1577663 -4.1488 -4.1241851 -4.0736594 -3.9905639 -3.885704 -3.876385 -3.9899256 -4.1085868 -4.188096 -4.2297096 -4.2408643 -4.2389126][-4.1969309 -4.1871877 -4.1843624 -4.18505 -4.1796293 -4.1553741 -4.1084256 -4.0393324 -4.0197139 -4.08083 -4.1536179 -4.2018237 -4.2262454 -4.2259011 -4.2197027][-4.2268772 -4.2215395 -4.2198014 -4.2260904 -4.2342319 -4.2253904 -4.1999745 -4.1553812 -4.1336808 -4.1614833 -4.1974254 -4.2194977 -4.2303681 -4.219903 -4.2040248][-4.2566438 -4.2483449 -4.2424512 -4.24673 -4.2573318 -4.2529163 -4.2324829 -4.2046194 -4.1918154 -4.2093339 -4.2285962 -4.2327027 -4.2332377 -4.2140126 -4.1890249][-4.2771974 -4.2642055 -4.2528653 -4.2503166 -4.2547545 -4.2480984 -4.2271357 -4.2123189 -4.2132134 -4.2335553 -4.2471123 -4.2441688 -4.239656 -4.2179 -4.1900783][-4.2850194 -4.2700119 -4.2548461 -4.2433281 -4.2402172 -4.2350111 -4.2189684 -4.2116437 -4.220067 -4.2415876 -4.2552376 -4.2536097 -4.2481451 -4.229044 -4.2055488][-4.2912803 -4.27599 -4.2590718 -4.2419438 -4.2335558 -4.2313704 -4.2214379 -4.2165685 -4.2237253 -4.2421994 -4.255372 -4.2554035 -4.2505007 -4.2375951 -4.2226262]]...]
INFO - root - 2017-12-06 07:25:34.731783: step 12510, loss = 2.07, batch loss = 2.01 (17.6 examples/sec; 0.455 sec/batch; 40h:25m:08s remains)
INFO - root - 2017-12-06 07:25:39.273428: step 12520, loss = 2.06, batch loss = 2.00 (16.3 examples/sec; 0.492 sec/batch; 43h:42m:40s remains)
INFO - root - 2017-12-06 07:25:43.786917: step 12530, loss = 2.07, batch loss = 2.01 (17.7 examples/sec; 0.452 sec/batch; 40h:08m:38s remains)
INFO - root - 2017-12-06 07:25:48.277799: step 12540, loss = 2.08, batch loss = 2.02 (17.7 examples/sec; 0.452 sec/batch; 40h:11m:05s remains)
INFO - root - 2017-12-06 07:25:52.770592: step 12550, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 0.474 sec/batch; 42h:05m:17s remains)
INFO - root - 2017-12-06 07:25:57.151951: step 12560, loss = 2.07, batch loss = 2.01 (18.9 examples/sec; 0.422 sec/batch; 37h:31m:34s remains)
INFO - root - 2017-12-06 07:26:01.744037: step 12570, loss = 2.04, batch loss = 1.98 (17.2 examples/sec; 0.465 sec/batch; 41h:18m:17s remains)
INFO - root - 2017-12-06 07:26:06.227510: step 12580, loss = 2.09, batch loss = 2.03 (18.3 examples/sec; 0.436 sec/batch; 38h:44m:48s remains)
INFO - root - 2017-12-06 07:26:10.894966: step 12590, loss = 2.05, batch loss = 1.99 (16.4 examples/sec; 0.487 sec/batch; 43h:14m:31s remains)
INFO - root - 2017-12-06 07:26:15.469538: step 12600, loss = 2.06, batch loss = 2.00 (18.0 examples/sec; 0.445 sec/batch; 39h:34m:09s remains)
2017-12-06 07:26:15.937592: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1353145 -4.1314125 -4.1137657 -4.0862646 -4.0826483 -4.101676 -4.1203518 -4.1291294 -4.121479 -4.1054974 -4.0983353 -4.117507 -4.1485691 -4.1812015 -4.2075157][-4.122169 -4.1121936 -4.0863342 -4.0526423 -4.0450196 -4.0551505 -4.0650754 -4.0748439 -4.0775785 -4.0677814 -4.0600786 -4.0812879 -4.1212196 -4.1652985 -4.2019644][-4.1133623 -4.1013341 -4.0700417 -4.034348 -4.0226846 -4.024859 -4.0317855 -4.0497079 -4.070776 -4.0742197 -4.0669107 -4.0793324 -4.113173 -4.1586294 -4.2000937][-4.1024604 -4.0924177 -4.0602756 -4.0242567 -4.0048656 -4.0002637 -4.0084934 -4.039556 -4.0813804 -4.1009259 -4.0993919 -4.1047306 -4.1254826 -4.1614275 -4.1984553][-4.0877104 -4.0791817 -4.0493107 -4.0170765 -3.9874625 -3.9693985 -3.9680672 -4.0027494 -4.0620465 -4.10261 -4.116518 -4.1259537 -4.1409292 -4.1681771 -4.1991935][-4.0795422 -4.0776668 -4.0553422 -4.0240097 -3.9778712 -3.9362111 -3.9160819 -3.9453523 -4.0115976 -4.0702972 -4.1067824 -4.1320744 -4.1528687 -4.1795735 -4.2076144][-4.0783477 -4.0853777 -4.0724607 -4.0462027 -3.9907887 -3.929487 -3.8943789 -3.9125724 -3.9693961 -4.0315337 -4.0833921 -4.129169 -4.1659317 -4.197432 -4.2215662][-4.093545 -4.10612 -4.1013546 -4.0789652 -4.0257235 -3.9655044 -3.9293685 -3.9355206 -3.9751866 -4.026762 -4.0788546 -4.1365476 -4.1852903 -4.2185507 -4.2358952][-4.1320024 -4.14625 -4.1467738 -4.1266303 -4.082273 -4.0350003 -4.0035181 -4.00066 -4.0258722 -4.0629487 -4.1023788 -4.1559367 -4.2043724 -4.2323279 -4.2445412][-4.1741686 -4.1898432 -4.190753 -4.1722612 -4.1380415 -4.1046896 -4.0833678 -4.0791283 -4.0923114 -4.1140175 -4.1397991 -4.1802287 -4.2207465 -4.2435923 -4.2541122][-4.203752 -4.2187386 -4.2175636 -4.2014446 -4.1734428 -4.1509223 -4.1387177 -4.1357279 -4.1415858 -4.1531463 -4.1720042 -4.2065053 -4.2413788 -4.2616291 -4.2708569][-4.2223053 -4.2317595 -4.22767 -4.2155542 -4.1970143 -4.1834116 -4.1792173 -4.1780009 -4.1798797 -4.1866374 -4.2014551 -4.2328835 -4.2635722 -4.2807345 -4.2881427][-4.24959 -4.2539377 -4.2467194 -4.2373319 -4.227 -4.2214475 -4.221724 -4.2212949 -4.2213984 -4.2268691 -4.2387543 -4.2622447 -4.2851582 -4.29807 -4.3028541][-4.274219 -4.2753377 -4.2692385 -4.2640147 -4.2597027 -4.2577624 -4.2576241 -4.2572808 -4.2588148 -4.2629313 -4.2716737 -4.2879143 -4.3026567 -4.3107123 -4.3136206][-4.2807693 -4.2811694 -4.2774158 -4.2741761 -4.272131 -4.271666 -4.27167 -4.2716966 -4.2747793 -4.2798662 -4.2873368 -4.2985196 -4.3089876 -4.3161798 -4.3186455]]...]
INFO - root - 2017-12-06 07:26:20.512837: step 12610, loss = 2.09, batch loss = 2.03 (18.3 examples/sec; 0.436 sec/batch; 38h:47m:06s remains)
INFO - root - 2017-12-06 07:26:25.127440: step 12620, loss = 2.06, batch loss = 2.00 (18.2 examples/sec; 0.440 sec/batch; 39h:03m:22s remains)
INFO - root - 2017-12-06 07:26:29.642622: step 12630, loss = 2.05, batch loss = 1.99 (17.1 examples/sec; 0.467 sec/batch; 41h:29m:33s remains)
INFO - root - 2017-12-06 07:26:34.167398: step 12640, loss = 2.03, batch loss = 1.97 (18.2 examples/sec; 0.439 sec/batch; 39h:02m:03s remains)
INFO - root - 2017-12-06 07:26:38.684183: step 12650, loss = 2.04, batch loss = 1.99 (17.7 examples/sec; 0.453 sec/batch; 40h:12m:24s remains)
INFO - root - 2017-12-06 07:26:42.968088: step 12660, loss = 2.06, batch loss = 2.00 (18.5 examples/sec; 0.432 sec/batch; 38h:20m:24s remains)
INFO - root - 2017-12-06 07:26:47.589980: step 12670, loss = 2.05, batch loss = 2.00 (17.1 examples/sec; 0.469 sec/batch; 41h:40m:21s remains)
INFO - root - 2017-12-06 07:26:52.172422: step 12680, loss = 2.07, batch loss = 2.02 (17.6 examples/sec; 0.454 sec/batch; 40h:18m:45s remains)
INFO - root - 2017-12-06 07:26:56.744502: step 12690, loss = 2.07, batch loss = 2.01 (17.4 examples/sec; 0.459 sec/batch; 40h:47m:04s remains)
INFO - root - 2017-12-06 07:27:01.254134: step 12700, loss = 2.08, batch loss = 2.02 (18.1 examples/sec; 0.442 sec/batch; 39h:16m:43s remains)
2017-12-06 07:27:01.812217: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2987623 -4.3095961 -4.3135757 -4.3114514 -4.3065748 -4.3051324 -4.3095355 -4.3168416 -4.3292069 -4.3389664 -4.3433762 -4.3339128 -4.3034573 -4.2678556 -4.2405071][-4.3118095 -4.322753 -4.325685 -4.3240805 -4.3220243 -4.3233681 -4.3251524 -4.326746 -4.3318815 -4.3348913 -4.3342514 -4.3239226 -4.293272 -4.2606006 -4.2341237][-4.3080964 -4.3231597 -4.3304448 -4.3318372 -4.327333 -4.3246841 -4.3189545 -4.3132977 -4.3122692 -4.3099542 -4.305933 -4.2979307 -4.2736282 -4.2488995 -4.227602][-4.3093266 -4.3254418 -4.3340206 -4.3335476 -4.3197193 -4.3043337 -4.2866235 -4.2709475 -4.2645435 -4.2584577 -4.2537894 -4.25006 -4.2347655 -4.2217522 -4.2124906][-4.3110843 -4.3209796 -4.3208728 -4.3089671 -4.27919 -4.2461481 -4.2145548 -4.1850443 -4.1805129 -4.184761 -4.1853585 -4.1879754 -4.1839533 -4.18118 -4.1809325][-4.2987094 -4.2980285 -4.2834072 -4.2533236 -4.2084346 -4.1664324 -4.1174512 -4.0668693 -4.070828 -4.0940514 -4.1079741 -4.1211925 -4.1278167 -4.1296806 -4.1367373][-4.2614989 -4.2437067 -4.20992 -4.1621394 -4.1076469 -4.0584669 -3.9842973 -3.9061942 -3.9321444 -3.9877834 -4.0211735 -4.0492182 -4.0685649 -4.0734057 -4.0830426][-4.1993804 -4.15423 -4.0998378 -4.0446634 -3.9883482 -3.9318745 -3.8346651 -3.7367258 -3.8100882 -3.9133592 -3.9694505 -4.0125427 -4.0463 -4.0524535 -4.0492225][-4.1370559 -4.0728264 -4.0146279 -3.9630828 -3.915689 -3.876348 -3.8159604 -3.7723856 -3.8675594 -3.9694648 -4.01925 -4.0594482 -4.0954623 -4.0982785 -4.07973][-4.1029954 -4.0466886 -4.0072675 -3.9766545 -3.9529018 -3.9545536 -3.9602385 -3.9699905 -4.0338988 -4.093544 -4.122406 -4.152997 -4.1831045 -4.1814694 -4.1614609][-4.1205068 -4.0960054 -4.0884237 -4.0780058 -4.0693974 -4.08714 -4.1146951 -4.1363144 -4.1704245 -4.198864 -4.2137041 -4.2342319 -4.2545958 -4.2526503 -4.2404256][-4.1636724 -4.1684427 -4.1861625 -4.1888037 -4.1875811 -4.2048278 -4.2281227 -4.2399325 -4.2535853 -4.26556 -4.2700291 -4.2783184 -4.2900562 -4.2902012 -4.2866688][-4.183392 -4.2063241 -4.2382908 -4.2526731 -4.25674 -4.269166 -4.28358 -4.28806 -4.2902985 -4.2914095 -4.2878947 -4.2881875 -4.2919607 -4.2908311 -4.2915807][-4.1522622 -4.1867123 -4.2306828 -4.255125 -4.264514 -4.2711911 -4.2774143 -4.2765584 -4.2732377 -4.2673588 -4.258523 -4.2533503 -4.2505875 -4.24918 -4.2541304][-4.0803986 -4.1181469 -4.1655593 -4.194077 -4.2060289 -4.2097516 -4.2110257 -4.2072978 -4.201745 -4.1937284 -4.1833353 -4.1744604 -4.1686049 -4.1700191 -4.1805634]]...]
INFO - root - 2017-12-06 07:27:06.425474: step 12710, loss = 2.06, batch loss = 2.00 (17.1 examples/sec; 0.469 sec/batch; 41h:39m:24s remains)
INFO - root - 2017-12-06 07:27:10.993456: step 12720, loss = 2.07, batch loss = 2.02 (18.6 examples/sec; 0.430 sec/batch; 38h:09m:34s remains)
INFO - root - 2017-12-06 07:27:15.520259: step 12730, loss = 2.06, batch loss = 2.00 (17.0 examples/sec; 0.470 sec/batch; 41h:44m:42s remains)
INFO - root - 2017-12-06 07:27:20.070264: step 12740, loss = 2.07, batch loss = 2.01 (18.0 examples/sec; 0.445 sec/batch; 39h:30m:09s remains)
INFO - root - 2017-12-06 07:27:24.245917: step 12750, loss = 2.07, batch loss = 2.01 (28.2 examples/sec; 0.284 sec/batch; 25h:14m:00s remains)
INFO - root - 2017-12-06 07:27:28.867221: step 12760, loss = 2.09, batch loss = 2.03 (17.6 examples/sec; 0.454 sec/batch; 40h:17m:44s remains)
INFO - root - 2017-12-06 07:27:33.451060: step 12770, loss = 2.07, batch loss = 2.01 (16.5 examples/sec; 0.484 sec/batch; 43h:00m:56s remains)
INFO - root - 2017-12-06 07:27:37.969464: step 12780, loss = 2.05, batch loss = 2.00 (17.4 examples/sec; 0.461 sec/batch; 40h:55m:47s remains)
INFO - root - 2017-12-06 07:27:42.552280: step 12790, loss = 2.08, batch loss = 2.02 (17.2 examples/sec; 0.466 sec/batch; 41h:24m:04s remains)
INFO - root - 2017-12-06 07:27:47.101964: step 12800, loss = 2.08, batch loss = 2.03 (17.6 examples/sec; 0.454 sec/batch; 40h:16m:33s remains)
2017-12-06 07:27:47.595522: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2092938 -4.2171392 -4.2316132 -4.2365665 -4.2434034 -4.2516041 -4.2580442 -4.2602143 -4.2551622 -4.2430248 -4.2231345 -4.2116666 -4.2082868 -4.2042027 -4.1885018][-4.1952438 -4.2083826 -4.2265792 -4.2269335 -4.2264714 -4.2333136 -4.2405033 -4.2398276 -4.2347841 -4.2283149 -4.2158203 -4.2084813 -4.2034459 -4.1902342 -4.1614265][-4.1925344 -4.2079835 -4.2228765 -4.2163377 -4.2076583 -4.2088556 -4.2150421 -4.214335 -4.2091675 -4.2099962 -4.2108955 -4.214571 -4.212328 -4.1916122 -4.1519346][-4.2053576 -4.2183661 -4.2239957 -4.2079458 -4.1911726 -4.18593 -4.1872978 -4.1823616 -4.1783628 -4.1895137 -4.2058749 -4.221662 -4.2257304 -4.2021542 -4.1537628][-4.2256269 -4.2310009 -4.2260461 -4.2019758 -4.1787043 -4.1683397 -4.1594191 -4.1387596 -4.1264267 -4.1525745 -4.1912489 -4.2214832 -4.2351542 -4.2135763 -4.1582909][-4.2430444 -4.2385821 -4.2253265 -4.1927414 -4.1590285 -4.1383314 -4.1147308 -4.0714893 -4.0465589 -4.0915818 -4.1597056 -4.2063065 -4.231616 -4.2126307 -4.1547232][-4.2472997 -4.2365193 -4.2176404 -4.1805191 -4.1352677 -4.1001906 -4.0600405 -3.9943504 -3.9539084 -4.0170546 -4.1143723 -4.1759334 -4.2105756 -4.1934152 -4.1363626][-4.2422662 -4.2320867 -4.211916 -4.176754 -4.131671 -4.0916967 -4.0412531 -3.9597573 -3.9036608 -3.9737267 -4.0849442 -4.1515837 -4.1899972 -4.1765914 -4.1241107][-4.2459455 -4.2387314 -4.2216043 -4.1939383 -4.1572351 -4.1199875 -4.0720892 -3.9959967 -3.941628 -3.9964967 -4.0906515 -4.149497 -4.1833744 -4.1728506 -4.1285591][-4.2519507 -4.2466359 -4.2328734 -4.2129841 -4.1875215 -4.1571622 -4.1183014 -4.0592389 -4.0148621 -4.0494008 -4.1131039 -4.1581974 -4.1854029 -4.1784506 -4.1427183][-4.2528954 -4.2463837 -4.2322879 -4.2163763 -4.1977305 -4.173605 -4.1441436 -4.1031432 -4.0692143 -4.0875278 -4.1273322 -4.16125 -4.1835117 -4.180923 -4.1548691][-4.2431488 -4.2350554 -4.2184129 -4.2042279 -4.1908011 -4.1714129 -4.1500473 -4.1226821 -4.0969639 -4.1057596 -4.1308579 -4.1571918 -4.1767874 -4.1776695 -4.1601477][-4.2303171 -4.2230139 -4.20637 -4.1949267 -4.1864514 -4.1723514 -4.1578212 -4.1381841 -4.1185904 -4.1233058 -4.1413918 -4.1611271 -4.1779375 -4.1796784 -4.1678829][-4.2283635 -4.2210298 -4.207593 -4.1997533 -4.1959567 -4.1874108 -4.178864 -4.1647935 -4.1508169 -4.151988 -4.1623425 -4.1745486 -4.1876159 -4.18987 -4.1807752][-4.2359447 -4.2282372 -4.2175746 -4.2122316 -4.2111435 -4.2073617 -4.2036724 -4.1950164 -4.1866679 -4.186439 -4.1907835 -4.1957588 -4.2049341 -4.2074537 -4.2000194]]...]
INFO - root - 2017-12-06 07:27:52.243359: step 12810, loss = 2.06, batch loss = 2.00 (17.3 examples/sec; 0.463 sec/batch; 41h:07m:10s remains)
INFO - root - 2017-12-06 07:27:56.822393: step 12820, loss = 2.09, batch loss = 2.03 (17.6 examples/sec; 0.455 sec/batch; 40h:21m:36s remains)
INFO - root - 2017-12-06 07:28:01.304068: step 12830, loss = 2.07, batch loss = 2.02 (18.2 examples/sec; 0.440 sec/batch; 39h:03m:39s remains)
INFO - root - 2017-12-06 07:28:05.889388: step 12840, loss = 2.07, batch loss = 2.01 (17.7 examples/sec; 0.452 sec/batch; 40h:07m:25s remains)
INFO - root - 2017-12-06 07:28:10.123199: step 12850, loss = 2.08, batch loss = 2.02 (18.0 examples/sec; 0.446 sec/batch; 39h:33m:25s remains)
INFO - root - 2017-12-06 07:28:14.813087: step 12860, loss = 2.06, batch loss = 2.00 (17.3 examples/sec; 0.462 sec/batch; 41h:01m:59s remains)
INFO - root - 2017-12-06 07:28:19.315099: step 12870, loss = 2.09, batch loss = 2.03 (17.6 examples/sec; 0.454 sec/batch; 40h:18m:36s remains)
INFO - root - 2017-12-06 07:28:23.800429: step 12880, loss = 2.08, batch loss = 2.02 (18.4 examples/sec; 0.436 sec/batch; 38h:39m:55s remains)
INFO - root - 2017-12-06 07:28:28.493206: step 12890, loss = 2.04, batch loss = 1.98 (17.2 examples/sec; 0.465 sec/batch; 41h:15m:50s remains)
INFO - root - 2017-12-06 07:28:32.984128: step 12900, loss = 2.06, batch loss = 2.00 (17.4 examples/sec; 0.460 sec/batch; 40h:49m:58s remains)
2017-12-06 07:28:33.487665: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2594934 -4.2707529 -4.289526 -4.3008204 -4.3048868 -4.30541 -4.3089347 -4.3159389 -4.3242683 -4.32759 -4.3223991 -4.3054824 -4.278182 -4.2529249 -4.245717][-4.2313991 -4.2452011 -4.2720795 -4.2907495 -4.2957563 -4.2933321 -4.2945218 -4.2993903 -4.3093867 -4.3163462 -4.311605 -4.2881689 -4.2475419 -4.2092128 -4.1978688][-4.2012424 -4.2137675 -4.2460537 -4.2725825 -4.2800446 -4.2748766 -4.2694435 -4.2650819 -4.2673135 -4.2775779 -4.2813187 -4.2626395 -4.2181067 -4.17466 -4.1619515][-4.1768351 -4.1846428 -4.2183323 -4.248168 -4.2556639 -4.244638 -4.2252731 -4.2053108 -4.200088 -4.2219563 -4.2443304 -4.2383671 -4.2025809 -4.1651144 -4.1510644][-4.1644974 -4.1685114 -4.19962 -4.2244282 -4.2264638 -4.2042513 -4.1618624 -4.1142211 -4.10194 -4.1500063 -4.2031384 -4.2165728 -4.1959524 -4.1679282 -4.1535621][-4.1538196 -4.1544538 -4.1840158 -4.2055979 -4.202383 -4.1652355 -4.0890903 -4.0018034 -3.9817884 -4.0648594 -4.1539483 -4.1917434 -4.1878443 -4.1699405 -4.1572342][-4.1557627 -4.1531706 -4.1830044 -4.2068267 -4.2036834 -4.1569529 -4.0550613 -3.9286294 -3.8970282 -4.0052676 -4.1154819 -4.1733904 -4.1855206 -4.1760383 -4.1638837][-4.1650867 -4.1610713 -4.1927361 -4.2247782 -4.2290473 -4.1855636 -4.0838294 -3.9550571 -3.9200842 -4.0187831 -4.120398 -4.1805172 -4.1996779 -4.1970448 -4.1861558][-4.1728024 -4.1672106 -4.1954274 -4.2325335 -4.2489805 -4.2225928 -4.1467934 -4.051374 -4.0238032 -4.0871544 -4.15642 -4.2008295 -4.2192011 -4.220552 -4.2109065][-4.1792908 -4.17185 -4.1928406 -4.2240009 -4.2454729 -4.2372823 -4.1957393 -4.1397729 -4.1216726 -4.1545224 -4.1920786 -4.2204094 -4.2373095 -4.2394414 -4.2311468][-4.18495 -4.1768327 -4.1868563 -4.2033954 -4.2194309 -4.2214441 -4.2061238 -4.1796012 -4.1704736 -4.1864247 -4.2037215 -4.2175756 -4.228117 -4.2318735 -4.2290869][-4.1882839 -4.1809416 -4.1816468 -4.1884108 -4.1978679 -4.2031884 -4.2037 -4.1982412 -4.1965666 -4.1999884 -4.2016897 -4.2062931 -4.2139339 -4.2214041 -4.2232742][-4.1914096 -4.1849222 -4.1819429 -4.1904931 -4.2022834 -4.2101259 -4.2170973 -4.2199335 -4.220612 -4.2142234 -4.2023225 -4.1998119 -4.2064695 -4.2174811 -4.224453][-4.1924815 -4.1808748 -4.1726832 -4.1874752 -4.2100277 -4.2226624 -4.2301445 -4.233489 -4.2349567 -4.2254906 -4.2102633 -4.2054124 -4.2144675 -4.2287235 -4.2386823][-4.1876283 -4.1708097 -4.1523886 -4.1680827 -4.2025013 -4.2236266 -4.231297 -4.2339706 -4.2364821 -4.2319493 -4.2205009 -4.2157407 -4.224164 -4.2369657 -4.247468]]...]
INFO - root - 2017-12-06 07:28:38.079404: step 12910, loss = 2.09, batch loss = 2.03 (17.5 examples/sec; 0.457 sec/batch; 40h:35m:01s remains)
INFO - root - 2017-12-06 07:28:42.650017: step 12920, loss = 2.06, batch loss = 2.00 (19.0 examples/sec; 0.422 sec/batch; 37h:25m:56s remains)
INFO - root - 2017-12-06 07:28:47.272549: step 12930, loss = 2.07, batch loss = 2.02 (17.9 examples/sec; 0.448 sec/batch; 39h:46m:14s remains)
INFO - root - 2017-12-06 07:28:51.517389: step 12940, loss = 2.04, batch loss = 1.98 (26.5 examples/sec; 0.302 sec/batch; 26h:49m:02s remains)
INFO - root - 2017-12-06 07:28:56.008992: step 12950, loss = 2.08, batch loss = 2.02 (19.0 examples/sec; 0.422 sec/batch; 37h:27m:30s remains)
INFO - root - 2017-12-06 07:29:00.494811: step 12960, loss = 2.10, batch loss = 2.04 (17.0 examples/sec; 0.471 sec/batch; 41h:48m:47s remains)
INFO - root - 2017-12-06 07:29:04.983106: step 12970, loss = 2.06, batch loss = 2.00 (18.0 examples/sec; 0.445 sec/batch; 39h:27m:31s remains)
INFO - root - 2017-12-06 07:29:09.508361: step 12980, loss = 2.06, batch loss = 2.00 (16.7 examples/sec; 0.479 sec/batch; 42h:33m:11s remains)
INFO - root - 2017-12-06 07:29:14.080557: step 12990, loss = 2.05, batch loss = 2.00 (17.9 examples/sec; 0.446 sec/batch; 39h:35m:05s remains)
INFO - root - 2017-12-06 07:29:18.677300: step 13000, loss = 2.08, batch loss = 2.02 (17.0 examples/sec; 0.469 sec/batch; 41h:39m:56s remains)
2017-12-06 07:29:19.229695: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2997208 -4.3042083 -4.301825 -4.2961097 -4.2903924 -4.28635 -4.2829566 -4.2796493 -4.2785091 -4.2796116 -4.2815995 -4.2834015 -4.2859058 -4.2902479 -4.2956524][-4.2994032 -4.3019066 -4.2989397 -4.2930326 -4.2855887 -4.2779984 -4.2697287 -4.2611771 -4.2548914 -4.2525616 -4.2541842 -4.2583671 -4.2653704 -4.2760191 -4.2879133][-4.3016806 -4.301003 -4.296093 -4.28729 -4.273746 -4.2559443 -4.2352409 -4.2149386 -4.19932 -4.1928525 -4.1974306 -4.2099452 -4.2276511 -4.2500405 -4.2726688][-4.3007851 -4.2977877 -4.2889209 -4.273313 -4.2483339 -4.2126613 -4.170536 -4.1316872 -4.1040592 -4.0946717 -4.1066809 -4.1351085 -4.1715951 -4.2123704 -4.2501936][-4.286057 -4.28379 -4.272862 -4.2504573 -4.2116361 -4.1533403 -4.084641 -4.0249796 -3.98941 -3.9833245 -4.0093651 -4.060679 -4.1190453 -4.1777382 -4.2296367][-4.2509108 -4.2515059 -4.2444892 -4.2248154 -4.183218 -4.1149192 -4.0290756 -3.9544368 -3.916223 -3.9188523 -3.9603069 -4.0320616 -4.1071258 -4.173737 -4.2284641][-4.1977873 -4.2037215 -4.2078261 -4.20259 -4.1769657 -4.1267037 -4.0557222 -3.99028 -3.9589348 -3.9658937 -4.0079284 -4.077548 -4.1473956 -4.203692 -4.2461777][-4.1280742 -4.1393132 -4.1597028 -4.1785774 -4.1853938 -4.17274 -4.1368241 -4.0949874 -4.0734797 -4.0797315 -4.110846 -4.1611714 -4.2101226 -4.2456694 -4.2694879][-4.0730362 -4.084528 -4.1164913 -4.1564765 -4.1935835 -4.2177982 -4.21718 -4.2005291 -4.1868596 -4.1879182 -4.205339 -4.2346163 -4.2628651 -4.2803307 -4.2882481][-4.0531631 -4.0589089 -4.0914636 -4.1378508 -4.1912007 -4.2407575 -4.2674718 -4.2717915 -4.2664151 -4.2631631 -4.2687488 -4.2816324 -4.2949529 -4.3009896 -4.2985468][-4.0633349 -4.0588908 -4.0793805 -4.1178088 -4.173954 -4.2369437 -4.2827344 -4.3043394 -4.3075318 -4.3016882 -4.2964306 -4.2965636 -4.2999783 -4.3004012 -4.2937417][-4.0830674 -4.0674367 -4.0731535 -4.1003428 -4.1506181 -4.2161865 -4.2726574 -4.3036666 -4.310287 -4.2982545 -4.2809911 -4.2723641 -4.2717814 -4.2736349 -4.27128][-4.1033626 -4.0809 -4.0776739 -4.0963545 -4.1396613 -4.2025375 -4.259141 -4.2867041 -4.2835536 -4.2550926 -4.2224836 -4.2073293 -4.2095652 -4.2215586 -4.2307625][-4.132081 -4.1107683 -4.1056952 -4.1196976 -4.154963 -4.2075233 -4.2528253 -4.2649822 -4.2402282 -4.1879606 -4.1376677 -4.1177611 -4.1276445 -4.1549344 -4.17887][-4.1720729 -4.156384 -4.1556644 -4.1672626 -4.1923957 -4.2304859 -4.2591896 -4.2509837 -4.2030277 -4.128098 -4.0622673 -4.0412116 -4.0623126 -4.1065521 -4.1439052]]...]
INFO - root - 2017-12-06 07:29:23.780030: step 13010, loss = 2.05, batch loss = 1.99 (17.3 examples/sec; 0.463 sec/batch; 41h:03m:43s remains)
INFO - root - 2017-12-06 07:29:28.386556: step 13020, loss = 2.07, batch loss = 2.01 (16.6 examples/sec; 0.481 sec/batch; 42h:42m:53s remains)
INFO - root - 2017-12-06 07:29:32.840153: step 13030, loss = 2.05, batch loss = 1.99 (17.9 examples/sec; 0.448 sec/batch; 39h:45m:01s remains)
INFO - root - 2017-12-06 07:29:37.159123: step 13040, loss = 2.09, batch loss = 2.03 (17.8 examples/sec; 0.451 sec/batch; 39h:59m:13s remains)
INFO - root - 2017-12-06 07:29:41.738873: step 13050, loss = 2.09, batch loss = 2.04 (17.2 examples/sec; 0.466 sec/batch; 41h:19m:40s remains)
INFO - root - 2017-12-06 07:29:46.480303: step 13060, loss = 2.06, batch loss = 2.00 (16.9 examples/sec; 0.474 sec/batch; 42h:05m:28s remains)
INFO - root - 2017-12-06 07:29:50.949781: step 13070, loss = 2.05, batch loss = 1.99 (17.7 examples/sec; 0.452 sec/batch; 40h:05m:43s remains)
INFO - root - 2017-12-06 07:29:55.431273: step 13080, loss = 2.05, batch loss = 1.99 (17.0 examples/sec; 0.472 sec/batch; 41h:51m:53s remains)
INFO - root - 2017-12-06 07:29:59.918651: step 13090, loss = 2.06, batch loss = 2.00 (18.6 examples/sec; 0.431 sec/batch; 38h:13m:09s remains)
INFO - root - 2017-12-06 07:30:04.488070: step 13100, loss = 2.07, batch loss = 2.01 (16.1 examples/sec; 0.497 sec/batch; 44h:05m:11s remains)
2017-12-06 07:30:04.968671: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3447447 -4.3395224 -4.3323979 -4.3302054 -4.3327541 -4.3389864 -4.3454533 -4.3504376 -4.353385 -4.353992 -4.3526731 -4.3499246 -4.345787 -4.3383942 -4.3276534][-4.3224592 -4.3177233 -4.3087811 -4.30199 -4.3011513 -4.3079915 -4.3178558 -4.3269234 -4.3341694 -4.339066 -4.3412094 -4.3415442 -4.3404613 -4.3360014 -4.3281417][-4.28368 -4.2772403 -4.2663369 -4.2549944 -4.250628 -4.2552519 -4.2637696 -4.2747965 -4.2884827 -4.30045 -4.307682 -4.3135462 -4.3181186 -4.3193245 -4.3181782][-4.2369928 -4.2312541 -4.218945 -4.205864 -4.1975646 -4.1954727 -4.196548 -4.2036562 -4.2197642 -4.2378287 -4.2501864 -4.2644849 -4.278439 -4.2881269 -4.2957244][-4.1987534 -4.1957974 -4.1871662 -4.1775146 -4.1666765 -4.1545324 -4.14539 -4.1467133 -4.1640797 -4.1864462 -4.2019997 -4.2193427 -4.2378654 -4.2520156 -4.2662153][-4.17236 -4.1664352 -4.1560454 -4.1483312 -4.1364422 -4.1170082 -4.1043835 -4.1097317 -4.133163 -4.1617661 -4.1790376 -4.1945138 -4.2119365 -4.2240143 -4.2395377][-4.1655512 -4.1489682 -4.1270223 -4.1102977 -4.0954008 -4.0731139 -4.0625048 -4.0766406 -4.1094651 -4.1450415 -4.1655693 -4.1806717 -4.1951976 -4.2050619 -4.2204838][-4.1827874 -4.1572347 -4.1220427 -4.089107 -4.0627666 -4.0367718 -4.0275655 -4.0421414 -4.0802417 -4.1213207 -4.150157 -4.1703892 -4.1858959 -4.1983819 -4.21304][-4.2121749 -4.1852574 -4.1451488 -4.0979576 -4.0544543 -4.0181174 -4.0066361 -4.0156074 -4.0506945 -4.0920839 -4.1311927 -4.1626339 -4.1839986 -4.2002707 -4.2168236][-4.2437797 -4.2225628 -4.1869221 -4.1374583 -4.0870132 -4.04591 -4.0289969 -4.0299697 -4.05504 -4.0897489 -4.1288033 -4.1622443 -4.1865425 -4.2063518 -4.2265868][-4.270515 -4.2579904 -4.2336359 -4.1962276 -4.15564 -4.1230025 -4.1069493 -4.1025586 -4.1156993 -4.1380277 -4.16477 -4.1863952 -4.2020278 -4.2185993 -4.2384973][-4.2848597 -4.2759109 -4.2610512 -4.2412863 -4.2214751 -4.2089787 -4.2028646 -4.2015481 -4.2087612 -4.2220516 -4.2345085 -4.2383976 -4.2386551 -4.2446189 -4.2557459][-4.2879038 -4.279366 -4.268312 -4.2574596 -4.251533 -4.2531548 -4.2578158 -4.2638736 -4.2703695 -4.2794857 -4.285675 -4.2812891 -4.2743831 -4.2720823 -4.2741141][-4.27835 -4.2695441 -4.2611575 -4.252378 -4.248456 -4.2527494 -4.2615547 -4.2718678 -4.2816324 -4.2909708 -4.2966256 -4.293 -4.2867012 -4.2838879 -4.2840304][-4.2578669 -4.24623 -4.2381411 -4.2292228 -4.2223825 -4.2212777 -4.2275524 -4.2394147 -4.254776 -4.2693019 -4.2795339 -4.2808089 -4.2784863 -4.2791677 -4.2832589]]...]
INFO - root - 2017-12-06 07:30:09.581885: step 13110, loss = 2.06, batch loss = 2.00 (17.7 examples/sec; 0.453 sec/batch; 40h:09m:24s remains)
INFO - root - 2017-12-06 07:30:14.166655: step 13120, loss = 2.10, batch loss = 2.04 (17.6 examples/sec; 0.456 sec/batch; 40h:25m:22s remains)
INFO - root - 2017-12-06 07:30:18.616228: step 13130, loss = 2.07, batch loss = 2.02 (33.5 examples/sec; 0.239 sec/batch; 21h:11m:10s remains)
INFO - root - 2017-12-06 07:30:23.179653: step 13140, loss = 2.10, batch loss = 2.04 (17.1 examples/sec; 0.467 sec/batch; 41h:28m:14s remains)
INFO - root - 2017-12-06 07:30:27.674537: step 13150, loss = 2.10, batch loss = 2.04 (17.5 examples/sec; 0.458 sec/batch; 40h:36m:46s remains)
INFO - root - 2017-12-06 07:30:32.387668: step 13160, loss = 2.06, batch loss = 2.00 (18.2 examples/sec; 0.441 sec/batch; 39h:05m:20s remains)
INFO - root - 2017-12-06 07:30:36.940838: step 13170, loss = 2.10, batch loss = 2.04 (16.7 examples/sec; 0.480 sec/batch; 42h:32m:29s remains)
INFO - root - 2017-12-06 07:30:41.496401: step 13180, loss = 2.08, batch loss = 2.02 (18.1 examples/sec; 0.442 sec/batch; 39h:12m:53s remains)
INFO - root - 2017-12-06 07:30:45.936508: step 13190, loss = 2.04, batch loss = 1.98 (17.3 examples/sec; 0.462 sec/batch; 40h:57m:35s remains)
INFO - root - 2017-12-06 07:30:50.583748: step 13200, loss = 2.09, batch loss = 2.03 (16.1 examples/sec; 0.498 sec/batch; 44h:08m:41s remains)
2017-12-06 07:30:51.178894: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2920537 -4.2869596 -4.284307 -4.2814474 -4.2824531 -4.2849894 -4.2895579 -4.2942815 -4.2908664 -4.2842875 -4.277215 -4.2753568 -4.2769895 -4.2770119 -4.2827311][-4.282475 -4.2757273 -4.2720728 -4.2686858 -4.2716269 -4.2763157 -4.279994 -4.2829003 -4.2793803 -4.2741561 -4.267127 -4.2673368 -4.2688351 -4.2648487 -4.2698541][-4.2633629 -4.2522964 -4.242712 -4.2381721 -4.2415953 -4.2460446 -4.2499 -4.2503147 -4.2485108 -4.2521372 -4.2517705 -4.2529469 -4.2481422 -4.2398863 -4.2442269][-4.2363973 -4.2186594 -4.2014165 -4.1916533 -4.1936293 -4.1942816 -4.2020793 -4.2015758 -4.202723 -4.2182121 -4.230288 -4.2320442 -4.2179084 -4.2050056 -4.20683][-4.2149434 -4.1906657 -4.1646023 -4.1459341 -4.1414113 -4.1365504 -4.148428 -4.14937 -4.1526904 -4.1764374 -4.2016268 -4.2067537 -4.1860571 -4.1671357 -4.1649084][-4.2013478 -4.1743608 -4.1406355 -4.1101003 -4.092381 -4.0790386 -4.0937014 -4.1033907 -4.1075306 -4.129694 -4.1639485 -4.1749516 -4.1494985 -4.1293154 -4.1296473][-4.2010036 -4.1744208 -4.1348066 -4.0935254 -4.0592532 -4.0361137 -4.0515714 -4.0730023 -4.0797348 -4.0962243 -4.1310568 -4.1395464 -4.0998955 -4.0814342 -4.0992413][-4.2061515 -4.1862183 -4.150671 -4.1066523 -4.0650511 -4.0336747 -4.039742 -4.0647011 -4.0740137 -4.075789 -4.1006718 -4.1019535 -4.0424867 -4.0254292 -4.0653992][-4.2100563 -4.1994615 -4.1772509 -4.1421967 -4.110126 -4.0795226 -4.074347 -4.0920138 -4.0964003 -4.0825648 -4.0927858 -4.0784655 -4.0055966 -3.9963315 -4.0562057][-4.2059293 -4.2030649 -4.1992321 -4.1817713 -4.1650314 -4.1380663 -4.12675 -4.1358294 -4.1336894 -4.112174 -4.11044 -4.0931673 -4.0280027 -4.0294156 -4.0854907][-4.1987367 -4.1976137 -4.2012968 -4.1962738 -4.1981263 -4.1750035 -4.1549659 -4.1516089 -4.148602 -4.134079 -4.129879 -4.1171293 -4.0699687 -4.0743427 -4.1140165][-4.2033696 -4.2004924 -4.2017317 -4.1945672 -4.2045722 -4.1859579 -4.1562943 -4.1464643 -4.1495309 -4.1507564 -4.1524758 -4.1452417 -4.1109619 -4.113359 -4.137898][-4.2170711 -4.213727 -4.2131715 -4.202322 -4.2087865 -4.1917477 -4.1592851 -4.1528149 -4.1651316 -4.1764927 -4.182641 -4.176199 -4.1451669 -4.1399035 -4.1487865][-4.23571 -4.2362204 -4.2379823 -4.226016 -4.2233329 -4.2061858 -4.179276 -4.1814146 -4.1993942 -4.2120285 -4.2193775 -4.2146435 -4.1850271 -4.1689334 -4.1685162][-4.2462726 -4.2520742 -4.2597294 -4.2545328 -4.2495141 -4.2336693 -4.2175808 -4.2266221 -4.2437353 -4.2566109 -4.2641582 -4.2604418 -4.2355704 -4.2166786 -4.2133431]]...]
INFO - root - 2017-12-06 07:30:55.824649: step 13210, loss = 2.05, batch loss = 2.00 (16.5 examples/sec; 0.484 sec/batch; 42h:57m:19s remains)
INFO - root - 2017-12-06 07:31:00.461085: step 13220, loss = 2.09, batch loss = 2.03 (17.7 examples/sec; 0.452 sec/batch; 40h:05m:34s remains)
INFO - root - 2017-12-06 07:31:03.582296: step 13230, loss = 2.04, batch loss = 1.98 (35.9 examples/sec; 0.223 sec/batch; 19h:46m:33s remains)
INFO - root - 2017-12-06 07:31:05.942791: step 13240, loss = 2.06, batch loss = 2.00 (33.7 examples/sec; 0.237 sec/batch; 21h:02m:52s remains)
INFO - root - 2017-12-06 07:31:08.252110: step 13250, loss = 2.08, batch loss = 2.02 (31.4 examples/sec; 0.255 sec/batch; 22h:36m:44s remains)
INFO - root - 2017-12-06 07:31:11.282330: step 13260, loss = 2.07, batch loss = 2.01 (26.8 examples/sec; 0.298 sec/batch; 26h:27m:57s remains)
INFO - root - 2017-12-06 07:31:14.339139: step 13270, loss = 2.08, batch loss = 2.02 (23.7 examples/sec; 0.338 sec/batch; 29h:56m:17s remains)
INFO - root - 2017-12-06 07:31:17.416846: step 13280, loss = 2.07, batch loss = 2.01 (23.8 examples/sec; 0.337 sec/batch; 29h:50m:42s remains)
INFO - root - 2017-12-06 07:31:20.457530: step 13290, loss = 2.04, batch loss = 1.98 (27.0 examples/sec; 0.296 sec/batch; 26h:14m:20s remains)
INFO - root - 2017-12-06 07:31:23.561088: step 13300, loss = 2.07, batch loss = 2.01 (25.8 examples/sec; 0.311 sec/batch; 27h:31m:58s remains)
2017-12-06 07:31:24.056570: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2042294 -4.2020597 -4.2044883 -4.1954389 -4.1842318 -4.166585 -4.1293049 -4.0636272 -3.9887569 -3.9474602 -3.9590135 -4.0073733 -4.0568056 -4.0848408 -4.0657592][-4.2466311 -4.2480536 -4.2485065 -4.23518 -4.2188578 -4.19648 -4.1623282 -4.1043687 -4.0348539 -3.9969983 -4.0051217 -4.0396123 -4.0783873 -4.0977058 -4.0776639][-4.259418 -4.2611065 -4.2587819 -4.2414293 -4.2222233 -4.1994476 -4.1766725 -4.1365957 -4.0839915 -4.0541291 -4.0556211 -4.0707536 -4.0895672 -4.1013761 -4.0932269][-4.2412882 -4.244596 -4.2387886 -4.2171359 -4.1903596 -4.1696825 -4.1623821 -4.1401353 -4.1054583 -4.0853143 -4.0794067 -4.0764089 -4.0784311 -4.0840254 -4.0845966][-4.2235603 -4.2263241 -4.21988 -4.1938119 -4.1573339 -4.1343684 -4.1283879 -4.1150851 -4.0898056 -4.0721784 -4.0626941 -4.0576367 -4.0618868 -4.0719795 -4.0784788][-4.217288 -4.221283 -4.2155738 -4.1831307 -4.1342611 -4.1068468 -4.0965271 -4.0751424 -4.0520573 -4.0331831 -4.0237212 -4.031662 -4.0516191 -4.0709271 -4.0835714][-4.2020631 -4.2048364 -4.2002048 -4.1615772 -4.1057911 -4.0773373 -4.0681067 -4.0435019 -4.022048 -3.9986205 -3.9892888 -4.0203724 -4.05988 -4.0887156 -4.1040521][-4.16606 -4.1622429 -4.1535149 -4.1100211 -4.0554209 -4.036953 -4.0394087 -4.0259504 -4.0081811 -3.9796474 -3.9719172 -4.0097203 -4.049768 -4.0822439 -4.1035075][-4.1364193 -4.1242619 -4.1106639 -4.0672216 -4.020309 -4.0123158 -4.0255852 -4.0235448 -4.0095639 -3.9860051 -3.9816108 -4.0065565 -4.0315456 -4.0559611 -4.0793891][-4.12763 -4.113699 -4.0965838 -4.0560765 -4.0215383 -4.0270867 -4.0455689 -4.0512877 -4.0446153 -4.034308 -4.0307803 -4.0358295 -4.0366459 -4.0403619 -4.0557551][-4.1506729 -4.1366177 -4.11275 -4.0716844 -4.0475612 -4.0628138 -4.0832233 -4.0894608 -4.0866537 -4.089016 -4.0900416 -4.0807772 -4.0615821 -4.0443487 -4.0444613][-4.1661792 -4.1511903 -4.1217318 -4.0842543 -4.0705719 -4.0844059 -4.0936146 -4.095984 -4.0995283 -4.11533 -4.1218805 -4.1075916 -4.0797577 -4.0544424 -4.045372][-4.1784182 -4.1587977 -4.1250825 -4.0914946 -4.0756197 -4.0761776 -4.0735459 -4.0751677 -4.0901651 -4.1144133 -4.12336 -4.1126461 -4.0914278 -4.0696754 -4.0588493][-4.1965332 -4.1756516 -4.1425409 -4.1102343 -4.0851741 -4.0711932 -4.0619044 -4.0643725 -4.0869222 -4.1155624 -4.1272292 -4.1213431 -4.1088133 -4.0957165 -4.0871277][-4.19821 -4.1815796 -4.1553454 -4.124229 -4.0935106 -4.0732827 -4.0645561 -4.0700579 -4.0968418 -4.1218743 -4.1304574 -4.1292362 -4.1236973 -4.1173372 -4.1120844]]...]
INFO - root - 2017-12-06 07:31:27.019051: step 13310, loss = 2.08, batch loss = 2.02 (26.4 examples/sec; 0.303 sec/batch; 26h:51m:52s remains)
INFO - root - 2017-12-06 07:31:30.096628: step 13320, loss = 2.06, batch loss = 2.00 (24.8 examples/sec; 0.323 sec/batch; 28h:36m:33s remains)
INFO - root - 2017-12-06 07:31:33.162987: step 13330, loss = 2.08, batch loss = 2.02 (25.0 examples/sec; 0.321 sec/batch; 28h:25m:09s remains)
INFO - root - 2017-12-06 07:31:36.239957: step 13340, loss = 2.07, batch loss = 2.01 (26.6 examples/sec; 0.301 sec/batch; 26h:38m:55s remains)
INFO - root - 2017-12-06 07:31:39.196724: step 13350, loss = 2.08, batch loss = 2.02 (30.4 examples/sec; 0.263 sec/batch; 23h:19m:55s remains)
INFO - root - 2017-12-06 07:31:42.259757: step 13360, loss = 2.08, batch loss = 2.02 (25.1 examples/sec; 0.318 sec/batch; 28h:13m:20s remains)
INFO - root - 2017-12-06 07:31:45.448272: step 13370, loss = 2.08, batch loss = 2.03 (24.5 examples/sec; 0.326 sec/batch; 28h:53m:33s remains)
INFO - root - 2017-12-06 07:31:48.574388: step 13380, loss = 2.07, batch loss = 2.01 (27.5 examples/sec; 0.291 sec/batch; 25h:45m:45s remains)
INFO - root - 2017-12-06 07:31:51.601211: step 13390, loss = 2.08, batch loss = 2.03 (25.0 examples/sec; 0.320 sec/batch; 28h:19m:52s remains)
INFO - root - 2017-12-06 07:31:54.571275: step 13400, loss = 2.05, batch loss = 1.99 (27.1 examples/sec; 0.295 sec/batch; 26h:09m:07s remains)
2017-12-06 07:31:55.089240: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1186929 -4.1185169 -4.1288762 -4.1420732 -4.15172 -4.157342 -4.1594577 -4.1553621 -4.1502171 -4.1642 -4.1972828 -4.2321587 -4.2594476 -4.2747726 -4.2693124][-4.1375666 -4.1385956 -4.1522536 -4.1723876 -4.1877389 -4.1924319 -4.1913891 -4.1902404 -4.1910105 -4.2057724 -4.227725 -4.2483139 -4.2626514 -4.2675495 -4.249732][-4.1647153 -4.1660056 -4.1853065 -4.2080736 -4.2168922 -4.21035 -4.2010784 -4.1961708 -4.1927032 -4.1998663 -4.2099714 -4.2212729 -4.2319093 -4.2331152 -4.2127905][-4.1939745 -4.1954417 -4.2170467 -4.232 -4.2266297 -4.2054062 -4.1846337 -4.1684566 -4.1562848 -4.1555071 -4.1574783 -4.1667652 -4.1820769 -4.1900396 -4.1755767][-4.2059 -4.209157 -4.23013 -4.2365971 -4.2198 -4.1868558 -4.1547775 -4.1309853 -4.1146121 -4.1077666 -4.1048717 -4.1165586 -4.1396751 -4.1545572 -4.1478419][-4.1909213 -4.1952934 -4.2137241 -4.2157588 -4.1940241 -4.1555619 -4.1211262 -4.10018 -4.0839324 -4.0722475 -4.07302 -4.0957365 -4.1281247 -4.146102 -4.1457868][-4.1592016 -4.162797 -4.1782589 -4.1756911 -4.1542807 -4.124301 -4.1032329 -4.09112 -4.0716653 -4.0574617 -4.066627 -4.1000595 -4.1371884 -4.1590972 -4.1672778][-4.1322193 -4.1320877 -4.141654 -4.1389804 -4.1286054 -4.1151576 -4.1090503 -4.0984211 -4.0747714 -4.0618591 -4.0796189 -4.1168766 -4.1536503 -4.1768365 -4.1890912][-4.112586 -4.1041222 -4.1091433 -4.1150827 -4.1237135 -4.1277671 -4.12659 -4.1147785 -4.0913482 -4.0862722 -4.1121755 -4.1512938 -4.1836929 -4.2023549 -4.2143159][-4.1102338 -4.0933232 -4.0928559 -4.106214 -4.1319742 -4.1460562 -4.1486158 -4.1395559 -4.1246428 -4.1321683 -4.164392 -4.2026453 -4.2256117 -4.2351069 -4.2434025][-4.1197028 -4.0995731 -4.1013861 -4.1232634 -4.1577439 -4.1775575 -4.1842351 -4.1813049 -4.1794271 -4.1961617 -4.2266326 -4.2551155 -4.2673388 -4.2691121 -4.2727842][-4.1332989 -4.1203222 -4.1340861 -4.166707 -4.2031374 -4.2244864 -4.23277 -4.2359905 -4.240653 -4.2551646 -4.2746577 -4.2897668 -4.2936549 -4.2904854 -4.2886524][-4.1696448 -4.1683578 -4.1919184 -4.2260895 -4.2555861 -4.2707524 -4.2764945 -4.2798228 -4.2821608 -4.2865558 -4.2923493 -4.2980647 -4.2980018 -4.2932491 -4.2898674][-4.2241335 -4.2311707 -4.2530608 -4.2773981 -4.2939672 -4.2989287 -4.2982697 -4.2977443 -4.2957993 -4.2954035 -4.2963638 -4.29832 -4.2971168 -4.2930121 -4.2900805][-4.2647257 -4.2735939 -4.2870331 -4.30014 -4.3070021 -4.3050194 -4.2982931 -4.2928972 -4.2893653 -4.2893524 -4.2911625 -4.2926679 -4.2913628 -4.2869959 -4.2829838]]...]
INFO - root - 2017-12-06 07:31:58.093273: step 13410, loss = 2.08, batch loss = 2.02 (27.5 examples/sec; 0.291 sec/batch; 25h:47m:47s remains)
INFO - root - 2017-12-06 07:32:01.181242: step 13420, loss = 2.08, batch loss = 2.02 (27.3 examples/sec; 0.293 sec/batch; 26h:00m:47s remains)
INFO - root - 2017-12-06 07:32:04.298386: step 13430, loss = 2.07, batch loss = 2.01 (26.0 examples/sec; 0.307 sec/batch; 27h:15m:04s remains)
INFO - root - 2017-12-06 07:32:07.358595: step 13440, loss = 2.06, batch loss = 2.00 (27.9 examples/sec; 0.287 sec/batch; 25h:26m:15s remains)
INFO - root - 2017-12-06 07:32:10.512222: step 13450, loss = 2.05, batch loss = 2.00 (27.0 examples/sec; 0.297 sec/batch; 26h:17m:03s remains)
INFO - root - 2017-12-06 07:32:13.646995: step 13460, loss = 2.07, batch loss = 2.01 (24.2 examples/sec; 0.331 sec/batch; 29h:18m:45s remains)
INFO - root - 2017-12-06 07:32:16.659922: step 13470, loss = 2.08, batch loss = 2.02 (27.3 examples/sec; 0.293 sec/batch; 25h:58m:02s remains)
INFO - root - 2017-12-06 07:32:19.701614: step 13480, loss = 2.06, batch loss = 2.00 (28.6 examples/sec; 0.280 sec/batch; 24h:48m:18s remains)
INFO - root - 2017-12-06 07:32:22.803216: step 13490, loss = 2.07, batch loss = 2.01 (25.6 examples/sec; 0.313 sec/batch; 27h:43m:02s remains)
INFO - root - 2017-12-06 07:32:25.921549: step 13500, loss = 2.10, batch loss = 2.05 (27.9 examples/sec; 0.287 sec/batch; 25h:25m:03s remains)
2017-12-06 07:32:26.412665: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2907991 -4.2840028 -4.2759857 -4.2699413 -4.2646403 -4.2633514 -4.2584248 -4.2380967 -4.2228136 -4.2263565 -4.2391334 -4.2629642 -4.2740846 -4.2736917 -4.2659292][-4.2800927 -4.2766733 -4.2736158 -4.2723103 -4.2698212 -4.2668486 -4.2595081 -4.2343855 -4.2162323 -4.2210827 -4.2338047 -4.2506943 -4.2554274 -4.2511067 -4.2425861][-4.257545 -4.26339 -4.27284 -4.2812166 -4.2816339 -4.2755408 -4.2608905 -4.227098 -4.203826 -4.210506 -4.2241397 -4.2370229 -4.235 -4.2262449 -4.2171183][-4.2346964 -4.2525296 -4.2737994 -4.2888966 -4.2889867 -4.2740712 -4.2463074 -4.2012253 -4.1689062 -4.1746449 -4.1952634 -4.2153058 -4.2145638 -4.2089024 -4.2035403][-4.2194586 -4.2423654 -4.2643466 -4.275701 -4.271071 -4.2479305 -4.2072005 -4.1517735 -4.1114426 -4.1143589 -4.1449084 -4.1827335 -4.1983833 -4.2017145 -4.2006879][-4.2102208 -4.2306352 -4.2479191 -4.2495952 -4.2331109 -4.1989107 -4.1466231 -4.0866561 -4.0436749 -4.0413761 -4.0822592 -4.1409473 -4.1838112 -4.2046862 -4.2111225][-4.2078738 -4.2242351 -4.2392621 -4.2321157 -4.1992087 -4.1476974 -4.0827379 -4.0164347 -3.9728305 -3.9666531 -4.013473 -4.0905685 -4.1608596 -4.2027183 -4.2188973][-4.2251954 -4.2372174 -4.2465091 -4.2295909 -4.1819196 -4.1149435 -4.03499 -3.9571602 -3.9145257 -3.9063501 -3.9546967 -4.044425 -4.1342292 -4.1921182 -4.2190375][-4.25498 -4.2642097 -4.2655659 -4.2379146 -4.1797557 -4.1048737 -4.0195317 -3.9390521 -3.9022117 -3.8992636 -3.9455783 -4.0359521 -4.1303124 -4.1955976 -4.2312655][-4.2798729 -4.2929292 -4.2901607 -4.2555604 -4.1935029 -4.118999 -4.0381293 -3.9671679 -3.9463432 -3.9568071 -4.0001426 -4.0767512 -4.1568766 -4.2150555 -4.2439437][-4.2892551 -4.3030515 -4.2998266 -4.268364 -4.2124019 -4.1480289 -4.0821586 -4.0296488 -4.0226583 -4.0448532 -4.0839968 -4.137661 -4.1877131 -4.2211537 -4.2297406][-4.275908 -4.2873368 -4.2831979 -4.2598243 -4.2164011 -4.16907 -4.1254125 -4.0973129 -4.1019549 -4.1305881 -4.16786 -4.2035027 -4.2236338 -4.2229128 -4.199513][-4.2399054 -4.2499366 -4.2489734 -4.235045 -4.2065692 -4.1791906 -4.1568236 -4.151155 -4.167707 -4.2007303 -4.2329817 -4.2538118 -4.2519555 -4.22061 -4.1652575][-4.1958861 -4.2083883 -4.2090015 -4.1993 -4.1805072 -4.1664324 -4.1602387 -4.1726546 -4.2046204 -4.2423825 -4.268537 -4.2771559 -4.2580662 -4.2043371 -4.129025][-4.1664081 -4.1792641 -4.1786408 -4.1670122 -4.1475172 -4.1369762 -4.1422486 -4.1684175 -4.2110991 -4.2524862 -4.2771473 -4.2779579 -4.2469015 -4.18284 -4.1022663]]...]
INFO - root - 2017-12-06 07:32:29.438285: step 13510, loss = 2.06, batch loss = 2.00 (25.4 examples/sec; 0.315 sec/batch; 27h:55m:30s remains)
INFO - root - 2017-12-06 07:32:32.535311: step 13520, loss = 2.04, batch loss = 1.98 (26.0 examples/sec; 0.308 sec/batch; 27h:18m:16s remains)
INFO - root - 2017-12-06 07:32:35.582121: step 13530, loss = 2.06, batch loss = 2.00 (25.5 examples/sec; 0.314 sec/batch; 27h:49m:02s remains)
INFO - root - 2017-12-06 07:32:38.614467: step 13540, loss = 2.08, batch loss = 2.02 (28.1 examples/sec; 0.285 sec/batch; 25h:14m:07s remains)
INFO - root - 2017-12-06 07:32:41.659059: step 13550, loss = 2.10, batch loss = 2.05 (26.5 examples/sec; 0.302 sec/batch; 26h:45m:39s remains)
INFO - root - 2017-12-06 07:32:44.724291: step 13560, loss = 2.06, batch loss = 2.00 (25.2 examples/sec; 0.317 sec/batch; 28h:06m:50s remains)
INFO - root - 2017-12-06 07:32:47.782306: step 13570, loss = 2.03, batch loss = 1.97 (27.3 examples/sec; 0.293 sec/batch; 25h:57m:22s remains)
INFO - root - 2017-12-06 07:32:50.839706: step 13580, loss = 2.05, batch loss = 1.99 (27.2 examples/sec; 0.295 sec/batch; 26h:05m:54s remains)
INFO - root - 2017-12-06 07:32:53.948068: step 13590, loss = 2.08, batch loss = 2.02 (27.3 examples/sec; 0.294 sec/batch; 26h:00m:01s remains)
INFO - root - 2017-12-06 07:32:57.091393: step 13600, loss = 2.06, batch loss = 2.01 (25.6 examples/sec; 0.312 sec/batch; 27h:38m:49s remains)
2017-12-06 07:32:57.588162: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9420967 -4.0145426 -4.1063547 -4.1855621 -4.2412868 -4.2658806 -4.2764654 -4.2765064 -4.2505441 -4.1788349 -4.0758 -3.9776087 -3.8988385 -3.8737216 -3.9176342][-3.9474185 -4.0001011 -4.0794516 -4.1493816 -4.1978679 -4.2235751 -4.2395096 -4.25281 -4.252048 -4.2185621 -4.1524534 -4.0721703 -3.98693 -3.943692 -3.9642282][-4.0237422 -4.0593128 -4.10994 -4.1470456 -4.1638155 -4.1702895 -4.1831703 -4.2115183 -4.2384253 -4.2504425 -4.2320724 -4.1835718 -4.1104727 -4.0663681 -4.0720787][-4.114428 -4.1409483 -4.160985 -4.1574273 -4.13523 -4.1115112 -4.116303 -4.1584549 -4.2134604 -4.2626266 -4.2796273 -4.25761 -4.2054181 -4.17275 -4.1730595][-4.1962729 -4.2083549 -4.1970234 -4.1529717 -4.0945058 -4.042172 -4.0353665 -4.0871296 -4.1650066 -4.2408423 -4.2869606 -4.2917886 -4.2632618 -4.2409339 -4.2372723][-4.2586269 -4.2499409 -4.2028866 -4.118598 -4.0328331 -3.9671502 -3.9505289 -3.998476 -4.0861592 -4.1783237 -4.2478242 -4.2858105 -4.2878923 -4.2808313 -4.2750292][-4.2866325 -4.2607694 -4.1860576 -4.0749068 -3.9796205 -3.91326 -3.8871534 -3.91824 -3.9972692 -4.0916171 -4.1771579 -4.2449245 -4.2798152 -4.2914677 -4.2866549][-4.2959819 -4.2642384 -4.1865578 -4.0774031 -3.9878979 -3.9205923 -3.8859632 -3.8974881 -3.9567933 -4.0396566 -4.125289 -4.2040672 -4.2582498 -4.2873139 -4.2929659][-4.3035569 -4.279357 -4.2198381 -4.1359234 -4.0623593 -3.9952285 -3.958694 -3.9640639 -4.0024524 -4.0588093 -4.1228862 -4.1834111 -4.234797 -4.273623 -4.293776][-4.303483 -4.2952127 -4.2618179 -4.2097206 -4.1542645 -4.0953412 -4.0654192 -4.0727849 -4.0989761 -4.1253214 -4.1521268 -4.1750054 -4.2058554 -4.2447577 -4.2795086][-4.293148 -4.2981591 -4.2876959 -4.2629952 -4.2271185 -4.1838684 -4.1655388 -4.1735673 -4.1838546 -4.1808929 -4.1728096 -4.16006 -4.1659484 -4.2008533 -4.24667][-4.2698355 -4.28371 -4.2902694 -4.28787 -4.2746706 -4.25302 -4.2432284 -4.2434053 -4.2322693 -4.1987462 -4.1573439 -4.1166477 -4.1027741 -4.1377039 -4.195827][-4.2505026 -4.2670736 -4.2832079 -4.294239 -4.2955775 -4.2893686 -4.2834249 -4.2724805 -4.2370019 -4.1686568 -4.0957489 -4.0353446 -4.0133452 -4.0507822 -4.1198406][-4.2472892 -4.260385 -4.2765207 -4.2916765 -4.30038 -4.3015437 -4.2949305 -4.2712345 -4.208674 -4.1069484 -4.0125561 -3.9457698 -3.9200985 -3.9549897 -4.0271568][-4.2589421 -4.2658558 -4.2782483 -4.2922635 -4.3045278 -4.3090477 -4.2998 -4.2676697 -4.1878848 -4.067802 -3.9639692 -3.895328 -3.8639984 -3.8856637 -3.9481072]]...]
INFO - root - 2017-12-06 07:33:00.692930: step 13610, loss = 2.09, batch loss = 2.03 (26.6 examples/sec; 0.301 sec/batch; 26h:39m:10s remains)
INFO - root - 2017-12-06 07:33:03.824208: step 13620, loss = 2.06, batch loss = 2.00 (23.2 examples/sec; 0.344 sec/batch; 30h:29m:12s remains)
INFO - root - 2017-12-06 07:33:06.855127: step 13630, loss = 2.06, batch loss = 2.00 (27.5 examples/sec; 0.291 sec/batch; 25h:44m:45s remains)
INFO - root - 2017-12-06 07:33:09.911145: step 13640, loss = 2.06, batch loss = 2.00 (30.1 examples/sec; 0.266 sec/batch; 23h:31m:32s remains)
INFO - root - 2017-12-06 07:33:13.004344: step 13650, loss = 2.08, batch loss = 2.02 (26.4 examples/sec; 0.303 sec/batch; 26h:51m:56s remains)
INFO - root - 2017-12-06 07:33:15.988262: step 13660, loss = 2.06, batch loss = 2.01 (28.2 examples/sec; 0.284 sec/batch; 25h:08m:41s remains)
INFO - root - 2017-12-06 07:33:19.007588: step 13670, loss = 2.06, batch loss = 2.00 (25.3 examples/sec; 0.316 sec/batch; 28h:00m:03s remains)
INFO - root - 2017-12-06 07:33:22.142085: step 13680, loss = 2.09, batch loss = 2.03 (26.0 examples/sec; 0.307 sec/batch; 27h:11m:57s remains)
INFO - root - 2017-12-06 07:33:25.207227: step 13690, loss = 2.07, batch loss = 2.01 (24.5 examples/sec; 0.327 sec/batch; 28h:56m:21s remains)
INFO - root - 2017-12-06 07:33:28.281659: step 13700, loss = 2.09, batch loss = 2.04 (24.7 examples/sec; 0.324 sec/batch; 28h:41m:23s remains)
2017-12-06 07:33:28.722949: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1417637 -4.1412172 -4.14627 -4.16097 -4.1832938 -4.1981196 -4.2034698 -4.2085657 -4.211175 -4.2056308 -4.2068377 -4.2171474 -4.212923 -4.1917024 -4.1818895][-4.1229897 -4.1278119 -4.1379361 -4.1582179 -4.1837029 -4.1943254 -4.1971493 -4.2010121 -4.2022886 -4.1919265 -4.1850996 -4.1870847 -4.1830115 -4.1730671 -4.1722488][-4.1036396 -4.1185389 -4.1365385 -4.1620712 -4.1908803 -4.1992321 -4.1961875 -4.1947045 -4.1936111 -4.1785116 -4.1618738 -4.1496773 -4.1447997 -4.1459303 -4.1516266][-4.0899949 -4.11819 -4.1479645 -4.1786337 -4.2051654 -4.2099781 -4.1979961 -4.1854081 -4.1766982 -4.1611857 -4.1440382 -4.1274323 -4.1245914 -4.1333184 -4.145462][-4.0928307 -4.1270032 -4.1595879 -4.1902924 -4.2103982 -4.208497 -4.1906409 -4.1660275 -4.1438365 -4.1315718 -4.1203284 -4.1057539 -4.1058483 -4.1192956 -4.1380181][-4.1177521 -4.1449618 -4.1728692 -4.2021694 -4.2142563 -4.2005787 -4.1689773 -4.1248784 -4.0896516 -4.0847559 -4.0844994 -4.0774784 -4.0854468 -4.1041279 -4.128686][-4.1644955 -4.1769719 -4.1928053 -4.2127166 -4.211812 -4.1773705 -4.117456 -4.0398808 -3.9938693 -4.01396 -4.0423694 -4.0517335 -4.0714445 -4.0917935 -4.1160803][-4.1878362 -4.1840129 -4.1874571 -4.1997266 -4.1882796 -4.1346035 -4.0407424 -3.9233387 -3.8739538 -3.9416611 -4.017755 -4.0502329 -4.0691938 -4.0801063 -4.0944309][-4.1855826 -4.172833 -4.1677055 -4.1752777 -4.1584673 -4.1028008 -4.0140147 -3.9068067 -3.8769932 -3.967906 -4.0579882 -4.0928793 -4.0964642 -4.0898542 -4.0920043][-4.1895466 -4.1759644 -4.1681581 -4.1727772 -4.1591468 -4.1210384 -4.0732903 -4.0174532 -4.0091434 -4.0729523 -4.1365528 -4.1566067 -4.1469092 -4.1300864 -4.1255417][-4.1992822 -4.1940541 -4.1927347 -4.1991596 -4.1910539 -4.1719675 -4.1521211 -4.1283646 -4.1269503 -4.159493 -4.1914024 -4.1969533 -4.1849446 -4.1640186 -4.1542668][-4.2117748 -4.2126737 -4.2168074 -4.2213655 -4.215373 -4.2040052 -4.1910748 -4.1773238 -4.1772141 -4.1903682 -4.1994963 -4.2011003 -4.1955996 -4.1775432 -4.1661563][-4.2242589 -4.227809 -4.2274981 -4.2221003 -4.2124662 -4.2005982 -4.1857905 -4.1757264 -4.178864 -4.1859818 -4.1894431 -4.1987991 -4.204504 -4.1930518 -4.1809382][-4.23706 -4.2358437 -4.2266021 -4.209146 -4.1920958 -4.1773663 -4.16529 -4.1608963 -4.167274 -4.1772609 -4.1858168 -4.2043777 -4.2198153 -4.2194972 -4.2114849][-4.2378521 -4.2295694 -4.2121906 -4.1889048 -4.1708255 -4.1610231 -4.1583066 -4.1627226 -4.173418 -4.18456 -4.1943932 -4.2129583 -4.2285838 -4.2305675 -4.2258115]]...]
INFO - root - 2017-12-06 07:33:31.782349: step 13710, loss = 2.04, batch loss = 1.99 (31.0 examples/sec; 0.258 sec/batch; 22h:51m:02s remains)
INFO - root - 2017-12-06 07:33:34.881299: step 13720, loss = 2.05, batch loss = 1.99 (24.3 examples/sec; 0.330 sec/batch; 29h:11m:36s remains)
INFO - root - 2017-12-06 07:33:38.005003: step 13730, loss = 2.07, batch loss = 2.01 (25.8 examples/sec; 0.310 sec/batch; 27h:24m:24s remains)
INFO - root - 2017-12-06 07:33:41.037659: step 13740, loss = 2.06, batch loss = 2.00 (29.7 examples/sec; 0.270 sec/batch; 23h:51m:45s remains)
INFO - root - 2017-12-06 07:33:44.092740: step 13750, loss = 2.05, batch loss = 1.99 (25.5 examples/sec; 0.313 sec/batch; 27h:44m:48s remains)
INFO - root - 2017-12-06 07:33:47.154876: step 13760, loss = 2.06, batch loss = 2.00 (29.9 examples/sec; 0.268 sec/batch; 23h:43m:27s remains)
INFO - root - 2017-12-06 07:33:50.287413: step 13770, loss = 2.05, batch loss = 2.00 (24.1 examples/sec; 0.332 sec/batch; 29h:25m:42s remains)
INFO - root - 2017-12-06 07:33:53.408601: step 13780, loss = 2.06, batch loss = 2.01 (25.4 examples/sec; 0.314 sec/batch; 27h:49m:49s remains)
INFO - root - 2017-12-06 07:33:56.456082: step 13790, loss = 2.04, batch loss = 1.98 (26.7 examples/sec; 0.299 sec/batch; 26h:29m:04s remains)
INFO - root - 2017-12-06 07:33:59.233096: step 13800, loss = 2.09, batch loss = 2.03 (35.3 examples/sec; 0.227 sec/batch; 20h:03m:15s remains)
2017-12-06 07:33:59.661745: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2859106 -4.2811737 -4.2803354 -4.27458 -4.2666545 -4.2536798 -4.239758 -4.2389584 -4.2400961 -4.230844 -4.20868 -4.1966591 -4.200727 -4.2130737 -4.2319713][-4.2728562 -4.2628827 -4.259346 -4.258357 -4.2628222 -4.2554188 -4.240325 -4.2300439 -4.2216396 -4.2078919 -4.1814032 -4.1725321 -4.1845412 -4.2043238 -4.2321367][-4.2546949 -4.2452312 -4.2441716 -4.2497196 -4.2601562 -4.2553248 -4.2369637 -4.2168603 -4.1962023 -4.1707315 -4.1399612 -4.1363373 -4.1619844 -4.1952829 -4.2363791][-4.2343659 -4.2280316 -4.2306843 -4.2427578 -4.2541251 -4.2462969 -4.2251372 -4.1977892 -4.1670194 -4.1318607 -4.0997896 -4.1050935 -4.1460891 -4.193893 -4.2423291][-4.2217813 -4.2173786 -4.223053 -4.2375512 -4.2468724 -4.2342329 -4.2096028 -4.1743822 -4.1308818 -4.093 -4.0706873 -4.0913758 -4.1419382 -4.1983247 -4.248817][-4.2185993 -4.2142615 -4.2180014 -4.2296824 -4.2350378 -4.2161975 -4.1865416 -4.1454434 -4.0965853 -4.0612364 -4.0526438 -4.0878649 -4.1460724 -4.2097549 -4.2603092][-4.2040486 -4.2012191 -4.2048631 -4.2128673 -4.2141953 -4.1918492 -4.1619968 -4.1204233 -4.0736504 -4.0442114 -4.0448661 -4.0891261 -4.1574373 -4.2266593 -4.275609][-4.1742306 -4.1756082 -4.1857562 -4.187851 -4.1861715 -4.1671066 -4.1436443 -4.1087584 -4.0637026 -4.0394158 -4.0480442 -4.0969038 -4.1709938 -4.2438412 -4.2910285][-4.1496091 -4.15751 -4.1721206 -4.1658692 -4.164031 -4.1525807 -4.1400146 -4.115212 -4.0740967 -4.0529151 -4.065763 -4.1156731 -4.1883864 -4.2592344 -4.3034863][-4.1502156 -4.15898 -4.1681752 -4.1553869 -4.1544633 -4.1495395 -4.142118 -4.1233668 -4.0862117 -4.064723 -4.0756793 -4.1246929 -4.1961317 -4.2665353 -4.3103786][-4.1716056 -4.1745982 -4.1729112 -4.153923 -4.1542025 -4.153851 -4.1474805 -4.1314778 -4.0941324 -4.0681553 -4.0764723 -4.125339 -4.1989665 -4.2707448 -4.316195][-4.18586 -4.1851268 -4.1737103 -4.1498785 -4.1501074 -4.153193 -4.151 -4.1387491 -4.1060095 -4.081862 -4.0910754 -4.1405797 -4.2114143 -4.2790937 -4.3212929][-4.1828508 -4.1821613 -4.166522 -4.1414714 -4.1428776 -4.1484342 -4.1480055 -4.1407042 -4.1186433 -4.1029429 -4.1219997 -4.1731114 -4.2361932 -4.2943411 -4.327343][-4.1697292 -4.1687851 -4.1524777 -4.131072 -4.1393714 -4.1468682 -4.1475697 -4.1440983 -4.1327648 -4.1329193 -4.165256 -4.2177987 -4.2712121 -4.31409 -4.3336897][-4.1539311 -4.1486306 -4.1342616 -4.1234455 -4.142714 -4.161253 -4.1727819 -4.1756783 -4.1748772 -4.1869321 -4.2240286 -4.2701817 -4.3087511 -4.3341274 -4.3397651]]...]
INFO - root - 2017-12-06 07:34:01.909129: step 13810, loss = 2.07, batch loss = 2.01 (35.5 examples/sec; 0.225 sec/batch; 19h:55m:22s remains)
INFO - root - 2017-12-06 07:34:04.189990: step 13820, loss = 2.08, batch loss = 2.03 (35.1 examples/sec; 0.228 sec/batch; 20h:09m:55s remains)
INFO - root - 2017-12-06 07:34:06.502952: step 13830, loss = 2.06, batch loss = 2.00 (35.2 examples/sec; 0.227 sec/batch; 20h:06m:15s remains)
INFO - root - 2017-12-06 07:34:08.810001: step 13840, loss = 2.06, batch loss = 2.01 (34.5 examples/sec; 0.232 sec/batch; 20h:31m:44s remains)
INFO - root - 2017-12-06 07:34:11.096621: step 13850, loss = 2.07, batch loss = 2.01 (35.8 examples/sec; 0.224 sec/batch; 19h:47m:20s remains)
INFO - root - 2017-12-06 07:34:13.430917: step 13860, loss = 2.04, batch loss = 1.99 (33.4 examples/sec; 0.240 sec/batch; 21h:13m:01s remains)
INFO - root - 2017-12-06 07:34:15.682117: step 13870, loss = 2.08, batch loss = 2.03 (35.6 examples/sec; 0.224 sec/batch; 19h:52m:05s remains)
INFO - root - 2017-12-06 07:34:17.963040: step 13880, loss = 2.08, batch loss = 2.02 (34.7 examples/sec; 0.231 sec/batch; 20h:25m:08s remains)
INFO - root - 2017-12-06 07:34:20.288045: step 13890, loss = 2.08, batch loss = 2.02 (34.5 examples/sec; 0.232 sec/batch; 20h:30m:34s remains)
INFO - root - 2017-12-06 07:34:22.522078: step 13900, loss = 2.05, batch loss = 1.99 (34.6 examples/sec; 0.231 sec/batch; 20h:28m:16s remains)
2017-12-06 07:34:22.919417: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2638507 -4.2640934 -4.26194 -4.264905 -4.2769156 -4.2898779 -4.3031034 -4.3086333 -4.3071823 -4.3001285 -4.2901073 -4.289165 -4.2967639 -4.2999821 -4.2994905][-4.2512145 -4.2507229 -4.2480807 -4.2485914 -4.2560792 -4.2643719 -4.2736011 -4.2768846 -4.2752132 -4.2695146 -4.2660413 -4.2714372 -4.2829623 -4.2897339 -4.2905369][-4.24179 -4.23912 -4.23143 -4.2262278 -4.22898 -4.235476 -4.2450237 -4.2533236 -4.2571082 -4.25792 -4.2610192 -4.2690635 -4.2810068 -4.2870841 -4.2879272][-4.24281 -4.2343497 -4.2159181 -4.1983624 -4.19279 -4.1977034 -4.2114835 -4.2331886 -4.2506828 -4.261981 -4.2719913 -4.279459 -4.2877655 -4.291182 -4.2913723][-4.2450638 -4.2296886 -4.1968565 -4.1583238 -4.1322923 -4.1245179 -4.1361904 -4.1702991 -4.2076268 -4.23857 -4.2639313 -4.2792797 -4.2881861 -4.2908983 -4.2905636][-4.2531142 -4.2310023 -4.1833177 -4.11904 -4.0631714 -4.0271964 -4.02162 -4.054296 -4.1104069 -4.1711268 -4.2262759 -4.2616577 -4.2784543 -4.2859097 -4.286006][-4.2627211 -4.2389908 -4.1890407 -4.1135721 -4.0350366 -3.9687872 -3.932544 -3.9456592 -4.0075479 -4.0935254 -4.1783257 -4.235364 -4.2636495 -4.2771049 -4.2782292][-4.2666011 -4.2504911 -4.2113523 -4.1484203 -4.0732765 -3.9968622 -3.9368277 -3.9243073 -3.9712908 -4.0556784 -4.1468916 -4.213367 -4.2473779 -4.2627106 -4.2657256][-4.258728 -4.2548494 -4.2322073 -4.1933742 -4.145504 -4.0899291 -4.0345483 -4.0063472 -4.0245166 -4.0811539 -4.1508183 -4.2078981 -4.2385788 -4.2501121 -4.2519879][-4.239471 -4.2462797 -4.2387795 -4.2222171 -4.20482 -4.1795511 -4.1435709 -4.11736 -4.1185937 -4.14732 -4.1878614 -4.2266836 -4.249464 -4.2551212 -4.2538943][-4.2151842 -4.2242627 -4.2209077 -4.216094 -4.2178063 -4.2157478 -4.2024479 -4.1901379 -4.1913247 -4.2070546 -4.2278438 -4.2498837 -4.26333 -4.2626286 -4.2559571][-4.1978436 -4.1988549 -4.1876616 -4.1830149 -4.191792 -4.2054639 -4.2113833 -4.2135339 -4.2208405 -4.2343369 -4.2451391 -4.2557845 -4.2608514 -4.2547441 -4.244174][-4.1963367 -4.1898441 -4.1699882 -4.16135 -4.1690025 -4.1874676 -4.2020617 -4.2104979 -4.2199664 -4.23059 -4.2359676 -4.2397523 -4.2406616 -4.2323451 -4.2216773][-4.2185497 -4.21238 -4.1914325 -4.1800303 -4.1822615 -4.1951742 -4.2072663 -4.2138939 -4.2224159 -4.2308483 -4.2338018 -4.23526 -4.2348428 -4.227499 -4.2187772][-4.25652 -4.2586493 -4.2453051 -4.2358747 -4.232964 -4.2377062 -4.2422338 -4.2452412 -4.2504535 -4.2556796 -4.2575722 -4.2583852 -4.2577324 -4.25276 -4.24721]]...]
INFO - root - 2017-12-06 07:34:25.140092: step 13910, loss = 2.07, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 19h:17m:48s remains)
INFO - root - 2017-12-06 07:34:27.396800: step 13920, loss = 2.07, batch loss = 2.02 (36.2 examples/sec; 0.221 sec/batch; 19h:34m:54s remains)
INFO - root - 2017-12-06 07:34:29.708191: step 13930, loss = 2.08, batch loss = 2.02 (34.7 examples/sec; 0.230 sec/batch; 20h:23m:41s remains)
INFO - root - 2017-12-06 07:34:31.978724: step 13940, loss = 2.04, batch loss = 1.98 (35.5 examples/sec; 0.226 sec/batch; 19h:57m:36s remains)
INFO - root - 2017-12-06 07:34:34.262228: step 13950, loss = 2.04, batch loss = 1.98 (34.9 examples/sec; 0.229 sec/batch; 20h:16m:50s remains)
INFO - root - 2017-12-06 07:34:36.515941: step 13960, loss = 2.07, batch loss = 2.01 (35.5 examples/sec; 0.225 sec/batch; 19h:55m:26s remains)
INFO - root - 2017-12-06 07:34:38.782637: step 13970, loss = 2.08, batch loss = 2.02 (35.4 examples/sec; 0.226 sec/batch; 19h:59m:43s remains)
INFO - root - 2017-12-06 07:34:41.100738: step 13980, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.225 sec/batch; 19h:54m:24s remains)
INFO - root - 2017-12-06 07:34:43.419506: step 13990, loss = 2.08, batch loss = 2.02 (35.3 examples/sec; 0.227 sec/batch; 20h:02m:38s remains)
INFO - root - 2017-12-06 07:34:45.737259: step 14000, loss = 2.06, batch loss = 2.00 (34.5 examples/sec; 0.232 sec/batch; 20h:31m:14s remains)
2017-12-06 07:34:46.075939: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2537389 -4.2504339 -4.242785 -4.2367477 -4.2345839 -4.2315631 -4.2393527 -4.2580829 -4.2724824 -4.265121 -4.2344527 -4.1953554 -4.1753254 -4.1753588 -4.1856437][-4.2490911 -4.2483554 -4.2418737 -4.2343645 -4.2347507 -4.2381024 -4.2491 -4.2633872 -4.2652812 -4.2428584 -4.1967759 -4.1435161 -4.1197248 -4.124928 -4.1471696][-4.234086 -4.2343435 -4.2307053 -4.2247872 -4.2286344 -4.2375164 -4.2453437 -4.248476 -4.2358346 -4.1951427 -4.1308832 -4.0713682 -4.0563021 -4.0786538 -4.1190104][-4.2230363 -4.2257042 -4.2279153 -4.2275167 -4.2366538 -4.2467418 -4.2452588 -4.2285938 -4.1944051 -4.1321011 -4.0596862 -4.0157914 -4.0285063 -4.0705938 -4.1224174][-4.2177153 -4.2224579 -4.225297 -4.2237163 -4.2321811 -4.2374387 -4.2149782 -4.17183 -4.1151342 -4.0468717 -3.9962749 -3.9968741 -4.042942 -4.0952568 -4.1428161][-4.2158546 -4.2209997 -4.2178459 -4.2068634 -4.2045054 -4.1932063 -4.1395903 -4.0593557 -3.980144 -3.9254582 -3.9301562 -3.9930887 -4.0659428 -4.1171894 -4.1515574][-4.2089248 -4.2099676 -4.1995215 -4.1746173 -4.1510792 -4.1107183 -4.0164409 -3.8915849 -3.8081245 -3.803412 -3.8739944 -3.9719563 -4.0482845 -4.0946355 -4.1148252][-4.1995173 -4.1958 -4.1799846 -4.1415343 -4.0964918 -4.0325141 -3.9126415 -3.7738461 -3.7336035 -3.7947679 -3.892498 -3.9829931 -4.0420933 -4.0726762 -4.0776873][-4.2212243 -4.2243347 -4.2133245 -4.1741509 -4.1223459 -4.0546112 -3.9477587 -3.8501453 -3.8560414 -3.9268165 -3.998158 -4.0559363 -4.08747 -4.1027741 -4.1028957][-4.2560635 -4.2636 -4.2550812 -4.2204223 -4.1701922 -4.1153555 -4.0481887 -4.0073423 -4.03832 -4.089859 -4.1282125 -4.15687 -4.1650815 -4.1684556 -4.16656][-4.2874384 -4.2967386 -4.2921839 -4.2657094 -4.2235641 -4.1851397 -4.1526737 -4.1484365 -4.1829762 -4.2111559 -4.2251925 -4.2348042 -4.2342811 -4.234314 -4.2326937][-4.3143849 -4.3228126 -4.3213181 -4.3051949 -4.2748117 -4.2496037 -4.2363672 -4.2451425 -4.2716227 -4.2849803 -4.2854428 -4.2852359 -4.2835007 -4.2826967 -4.2803931][-4.3273549 -4.3327456 -4.3331494 -4.3260555 -4.3053689 -4.288137 -4.2824435 -4.2920632 -4.3092165 -4.3166256 -4.313241 -4.3107181 -4.30852 -4.3047795 -4.3006124][-4.3256512 -4.3259492 -4.3262835 -4.3216143 -4.307693 -4.2967005 -4.2956853 -4.3023272 -4.3108134 -4.3142157 -4.3118277 -4.3091607 -4.3052187 -4.2989407 -4.2935257][-4.31994 -4.3151855 -4.3121567 -4.30721 -4.2983232 -4.2927575 -4.2941113 -4.2975345 -4.3000383 -4.3018427 -4.3027816 -4.3035221 -4.3008633 -4.2959704 -4.2920027]]...]
INFO - root - 2017-12-06 07:34:48.376047: step 14010, loss = 2.06, batch loss = 2.00 (35.1 examples/sec; 0.228 sec/batch; 20h:09m:38s remains)
INFO - root - 2017-12-06 07:34:50.645205: step 14020, loss = 2.09, batch loss = 2.03 (36.5 examples/sec; 0.219 sec/batch; 19h:22m:52s remains)
INFO - root - 2017-12-06 07:34:52.908516: step 14030, loss = 2.07, batch loss = 2.01 (34.8 examples/sec; 0.230 sec/batch; 20h:19m:32s remains)
INFO - root - 2017-12-06 07:34:55.220405: step 14040, loss = 2.05, batch loss = 1.99 (35.8 examples/sec; 0.224 sec/batch; 19h:47m:41s remains)
INFO - root - 2017-12-06 07:34:57.483096: step 14050, loss = 2.09, batch loss = 2.03 (34.8 examples/sec; 0.230 sec/batch; 20h:20m:54s remains)
INFO - root - 2017-12-06 07:34:59.723538: step 14060, loss = 2.05, batch loss = 1.99 (35.7 examples/sec; 0.224 sec/batch; 19h:50m:21s remains)
INFO - root - 2017-12-06 07:35:01.986055: step 14070, loss = 2.07, batch loss = 2.02 (35.9 examples/sec; 0.223 sec/batch; 19h:43m:20s remains)
INFO - root - 2017-12-06 07:35:04.254138: step 14080, loss = 2.06, batch loss = 2.01 (35.3 examples/sec; 0.226 sec/batch; 20h:01m:26s remains)
INFO - root - 2017-12-06 07:35:06.556397: step 14090, loss = 2.06, batch loss = 2.00 (35.7 examples/sec; 0.224 sec/batch; 19h:50m:06s remains)
INFO - root - 2017-12-06 07:35:08.801798: step 14100, loss = 2.05, batch loss = 1.99 (35.8 examples/sec; 0.224 sec/batch; 19h:46m:18s remains)
2017-12-06 07:35:09.276132: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.284482 -4.2827659 -4.2773128 -4.2747293 -4.2728505 -4.2754374 -4.2830911 -4.2902255 -4.294878 -4.2970409 -4.2984314 -4.2998109 -4.3002787 -4.2956958 -4.2880993][-4.2718287 -4.2663779 -4.255815 -4.2494121 -4.243608 -4.2468805 -4.2596164 -4.2741065 -4.2876234 -4.2952571 -4.2985077 -4.2994037 -4.2966785 -4.28563 -4.269989][-4.2510839 -4.2408032 -4.2240868 -4.21187 -4.1987948 -4.1986856 -4.2132225 -4.235877 -4.2626696 -4.2789879 -4.285563 -4.2870283 -4.2830825 -4.2690711 -4.2504516][-4.2318258 -4.2135372 -4.1859331 -4.1631413 -4.1409893 -4.1360779 -4.150671 -4.1824026 -4.2248173 -4.2547426 -4.2678447 -4.2711129 -4.266335 -4.2520056 -4.2346563][-4.2095194 -4.1839566 -4.1466 -4.1162539 -4.0876288 -4.0746613 -4.0838728 -4.1209216 -4.17799 -4.2206931 -4.2410417 -4.2466164 -4.2444606 -4.2364392 -4.2258654][-4.1806459 -4.1503057 -4.1102481 -4.0793777 -4.0455608 -4.0166612 -4.0043669 -4.0295014 -4.0969214 -4.1616545 -4.1999683 -4.2139411 -4.2192492 -4.2207718 -4.22011][-4.1620932 -4.1306205 -4.091114 -4.0599775 -4.0109334 -3.9446034 -3.8834047 -3.8766365 -3.9523911 -4.0538158 -4.11953 -4.1463394 -4.1620045 -4.176939 -4.191092][-4.1466537 -4.1138716 -4.0741692 -4.0421586 -3.9803903 -3.8701956 -3.7372792 -3.6664495 -3.7516603 -3.9005234 -4.0038977 -4.0512609 -4.0813675 -4.1130419 -4.143434][-4.1377659 -4.1026936 -4.0625491 -4.0367222 -3.9832859 -3.8695817 -3.7174349 -3.6105947 -3.676465 -3.8243082 -3.9317651 -3.9824865 -4.0193791 -4.06418 -4.1069236][-4.1428175 -4.1061888 -4.0688357 -4.0551457 -4.0325933 -3.9666042 -3.8712673 -3.7920759 -3.8100281 -3.8914964 -3.9539518 -3.982065 -4.0137849 -4.069396 -4.1173515][-4.1463666 -4.1010776 -4.0590024 -4.0507059 -4.0619984 -4.0474381 -4.006671 -3.955333 -3.9496174 -3.9806926 -4.0048642 -4.0106015 -4.0366492 -4.09981 -4.1519017][-4.1644735 -4.1166735 -4.0742779 -4.06944 -4.096283 -4.1111889 -4.103508 -4.073782 -4.062891 -4.0704303 -4.0743203 -4.0658765 -4.0805812 -4.1381068 -4.189342][-4.1876879 -4.146235 -4.11426 -4.1164103 -4.144773 -4.1656542 -4.1695809 -4.1550074 -4.1497874 -4.1543455 -4.1569109 -4.1453733 -4.1516428 -4.1942081 -4.2314787][-4.2117572 -4.180532 -4.1652026 -4.1757278 -4.2002411 -4.2138495 -4.217133 -4.2119102 -4.2144966 -4.2251968 -4.2358627 -4.2323437 -4.2321572 -4.2515273 -4.27106][-4.2381649 -4.214716 -4.2085586 -4.222508 -4.2412868 -4.2476158 -4.24664 -4.2435894 -4.2493267 -4.2627387 -4.2765837 -4.2801313 -4.2828393 -4.2931962 -4.3025851]]...]
INFO - root - 2017-12-06 07:35:11.529372: step 14110, loss = 2.06, batch loss = 2.01 (35.3 examples/sec; 0.227 sec/batch; 20h:02m:31s remains)
INFO - root - 2017-12-06 07:35:13.788927: step 14120, loss = 2.07, batch loss = 2.02 (36.2 examples/sec; 0.221 sec/batch; 19h:32m:10s remains)
INFO - root - 2017-12-06 07:35:16.053385: step 14130, loss = 2.06, batch loss = 2.00 (35.8 examples/sec; 0.224 sec/batch; 19h:46m:29s remains)
INFO - root - 2017-12-06 07:35:18.303546: step 14140, loss = 2.08, batch loss = 2.02 (34.9 examples/sec; 0.229 sec/batch; 20h:16m:59s remains)
INFO - root - 2017-12-06 07:35:20.603756: step 14150, loss = 2.03, batch loss = 1.97 (34.6 examples/sec; 0.231 sec/batch; 20h:28m:07s remains)
INFO - root - 2017-12-06 07:35:22.866265: step 14160, loss = 2.08, batch loss = 2.02 (35.8 examples/sec; 0.223 sec/batch; 19h:45m:03s remains)
INFO - root - 2017-12-06 07:35:25.124069: step 14170, loss = 2.09, batch loss = 2.03 (34.3 examples/sec; 0.233 sec/batch; 20h:36m:16s remains)
INFO - root - 2017-12-06 07:35:27.445617: step 14180, loss = 2.05, batch loss = 1.99 (34.8 examples/sec; 0.230 sec/batch; 20h:19m:23s remains)
INFO - root - 2017-12-06 07:35:29.747410: step 14190, loss = 2.07, batch loss = 2.02 (36.2 examples/sec; 0.221 sec/batch; 19h:32m:20s remains)
INFO - root - 2017-12-06 07:35:32.068212: step 14200, loss = 2.08, batch loss = 2.02 (35.8 examples/sec; 0.223 sec/batch; 19h:45m:36s remains)
2017-12-06 07:35:32.425379: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3093243 -4.3044682 -4.3019567 -4.3024673 -4.30122 -4.2992334 -4.2996888 -4.3032446 -4.3010087 -4.3000298 -4.299994 -4.3021116 -4.3042541 -4.3051877 -4.3047781][-4.2998695 -4.2931986 -4.2906156 -4.2914095 -4.2868524 -4.2805319 -4.28106 -4.2927823 -4.2961473 -4.2974091 -4.2947187 -4.2935777 -4.2943969 -4.295589 -4.2962132][-4.2984214 -4.2928982 -4.2874346 -4.2824197 -4.2679496 -4.250536 -4.24587 -4.2664747 -4.2833095 -4.2948842 -4.2961822 -4.2950726 -4.2929707 -4.29134 -4.2919469][-4.3022528 -4.29922 -4.2878571 -4.2709751 -4.2424631 -4.211215 -4.1962314 -4.22181 -4.2538495 -4.2807651 -4.293148 -4.2993951 -4.29967 -4.2954664 -4.2936478][-4.3097267 -4.3071504 -4.2890334 -4.2587156 -4.2143736 -4.1651554 -4.1355696 -4.1662707 -4.2138119 -4.2544217 -4.2767906 -4.2941403 -4.3037467 -4.3031216 -4.3019285][-4.3191175 -4.3172669 -4.2947268 -4.2496943 -4.1854315 -4.114089 -4.0637383 -4.0980177 -4.1667352 -4.2231846 -4.2518897 -4.2760482 -4.2950873 -4.3024125 -4.3057575][-4.32646 -4.3268213 -4.3041615 -4.2535071 -4.1750054 -4.0774875 -3.993819 -4.0152206 -4.1004415 -4.1771 -4.2188406 -4.2465363 -4.2701392 -4.2853765 -4.2951589][-4.3256664 -4.3284197 -4.3091936 -4.2659564 -4.192874 -4.083632 -3.9570997 -3.936924 -4.0212874 -4.1137366 -4.17039 -4.2068019 -4.23411 -4.2563033 -4.2720213][-4.318687 -4.320014 -4.3028283 -4.2718096 -4.2210875 -4.1296945 -3.9967866 -3.935854 -3.9868679 -4.0660605 -4.1238208 -4.1631718 -4.1924157 -4.2144256 -4.232347][-4.3101778 -4.3073449 -4.290556 -4.2680326 -4.2374845 -4.1804867 -4.0823908 -4.0256753 -4.0388374 -4.0751529 -4.110775 -4.1390586 -4.1592908 -4.1747642 -4.1875319][-4.3025103 -4.2981277 -4.2825027 -4.2632623 -4.2405553 -4.2082067 -4.1517978 -4.1237226 -4.12854 -4.1321673 -4.1397619 -4.1519003 -4.1556649 -4.1510391 -4.1443763][-4.2985721 -4.2947941 -4.2844357 -4.2678838 -4.24672 -4.2222085 -4.1892815 -4.184957 -4.1927795 -4.1880126 -4.181766 -4.1863222 -4.1822262 -4.1629763 -4.1372447][-4.2951555 -4.2910357 -4.2862644 -4.27567 -4.2570357 -4.2340651 -4.2088 -4.2080865 -4.2135868 -4.2094455 -4.2043228 -4.2126832 -4.2142544 -4.1960363 -4.1680908][-4.2914867 -4.2837696 -4.2816253 -4.2763929 -4.2650819 -4.2466183 -4.2241654 -4.2188492 -4.2176991 -4.2140059 -4.2099743 -4.2202663 -4.2261815 -4.215342 -4.1988163][-4.2953811 -4.2831888 -4.2796288 -4.2776771 -4.2739019 -4.2621522 -4.2440691 -4.2372327 -4.2332497 -4.2282076 -4.2223177 -4.2309542 -4.23796 -4.2310233 -4.2213187]]...]
INFO - root - 2017-12-06 07:35:34.724324: step 14210, loss = 2.09, batch loss = 2.03 (35.4 examples/sec; 0.226 sec/batch; 19h:59m:45s remains)
INFO - root - 2017-12-06 07:35:37.006669: step 14220, loss = 2.06, batch loss = 2.00 (33.0 examples/sec; 0.242 sec/batch; 21h:25m:10s remains)
INFO - root - 2017-12-06 07:35:39.299955: step 14230, loss = 2.09, batch loss = 2.04 (34.9 examples/sec; 0.229 sec/batch; 20h:14m:39s remains)
INFO - root - 2017-12-06 07:35:41.575102: step 14240, loss = 2.06, batch loss = 2.00 (36.4 examples/sec; 0.220 sec/batch; 19h:26m:07s remains)
INFO - root - 2017-12-06 07:35:43.915373: step 14250, loss = 2.06, batch loss = 2.00 (33.4 examples/sec; 0.239 sec/batch; 21h:09m:48s remains)
INFO - root - 2017-12-06 07:35:46.173426: step 14260, loss = 2.04, batch loss = 1.99 (36.1 examples/sec; 0.222 sec/batch; 19h:36m:04s remains)
INFO - root - 2017-12-06 07:35:48.501400: step 14270, loss = 2.05, batch loss = 2.00 (33.4 examples/sec; 0.240 sec/batch; 21h:11m:35s remains)
INFO - root - 2017-12-06 07:35:50.759882: step 14280, loss = 2.05, batch loss = 1.99 (35.3 examples/sec; 0.227 sec/batch; 20h:01m:46s remains)
INFO - root - 2017-12-06 07:35:52.997357: step 14290, loss = 2.07, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 19h:16m:31s remains)
INFO - root - 2017-12-06 07:35:55.326810: step 14300, loss = 2.07, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 19h:59m:47s remains)
2017-12-06 07:35:55.732508: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3115544 -4.2962384 -4.2890291 -4.2921991 -4.2882133 -4.2807908 -4.2799139 -4.2847171 -4.284791 -4.2677665 -4.244164 -4.2256584 -4.2119441 -4.2067895 -4.2179923][-4.3164372 -4.302227 -4.2903504 -4.2843409 -4.2717485 -4.2604222 -4.2589731 -4.2632704 -4.2636948 -4.2477636 -4.2292843 -4.2194872 -4.2074981 -4.1961851 -4.204401][-4.3177438 -4.303925 -4.285687 -4.2691474 -4.248486 -4.2330227 -4.2312508 -4.2357335 -4.2377853 -4.222507 -4.2113328 -4.211936 -4.2018204 -4.1895738 -4.2009687][-4.3141451 -4.2984881 -4.2744288 -4.2500639 -4.228168 -4.2118535 -4.2039104 -4.1987214 -4.1941295 -4.1825805 -4.1859436 -4.2003169 -4.202704 -4.2014022 -4.2190814][-4.308075 -4.2898669 -4.2627835 -4.234859 -4.2124562 -4.1942887 -4.1720705 -4.1383271 -4.1141872 -4.1174278 -4.1540279 -4.195611 -4.218688 -4.2303462 -4.2464714][-4.3048515 -4.2874312 -4.2625623 -4.2345357 -4.2096467 -4.1831861 -4.132556 -4.0472283 -3.98992 -4.024189 -4.1131 -4.1892409 -4.2327991 -4.2554049 -4.269196][-4.3057914 -4.2925715 -4.27335 -4.2489409 -4.2226548 -4.181644 -4.0931745 -3.9491951 -3.8522406 -3.9242558 -4.0710969 -4.1799431 -4.2392039 -4.2697644 -4.2818017][-4.3100624 -4.3003316 -4.2862387 -4.2683191 -4.2452807 -4.1987777 -4.0991235 -3.9370644 -3.8234141 -3.9084988 -4.0714965 -4.1860995 -4.2454915 -4.2756262 -4.2815189][-4.3141108 -4.3061876 -4.2949057 -4.2797618 -4.2609434 -4.223845 -4.149467 -4.0313406 -3.9430308 -3.9982786 -4.1181936 -4.2077069 -4.2532983 -4.2753077 -4.2721839][-4.3169041 -4.306778 -4.2913537 -4.2735972 -4.2547727 -4.2291317 -4.1882553 -4.1220574 -4.0646706 -4.0918174 -4.16331 -4.2233863 -4.260601 -4.2796354 -4.2741833][-4.3156252 -4.2998772 -4.2759023 -4.2510514 -4.2316389 -4.2150145 -4.1954494 -4.1586142 -4.1216884 -4.1397152 -4.18903 -4.2343807 -4.2689047 -4.2888436 -4.2839913][-4.3133321 -4.2941484 -4.2635384 -4.2317123 -4.2111893 -4.201282 -4.1925879 -4.1666641 -4.1392684 -4.1536431 -4.1927228 -4.2315426 -4.2643466 -4.2835026 -4.2783365][-4.3108754 -4.2914681 -4.2602229 -4.2274818 -4.20689 -4.1998205 -4.1918225 -4.1670876 -4.1429615 -4.1553221 -4.1894331 -4.2233944 -4.2541742 -4.2696648 -4.261549][-4.3099661 -4.2939544 -4.2698979 -4.2440138 -4.22386 -4.2124858 -4.1991854 -4.1732831 -4.149961 -4.1594558 -4.1919403 -4.2217145 -4.2465982 -4.2569389 -4.2498975][-4.3089485 -4.2957544 -4.27964 -4.2602692 -4.2409 -4.2245626 -4.206295 -4.1784453 -4.1612711 -4.173728 -4.2063704 -4.231945 -4.2503819 -4.255424 -4.2474484]]...]
INFO - root - 2017-12-06 07:35:58.023074: step 14310, loss = 2.08, batch loss = 2.02 (35.2 examples/sec; 0.227 sec/batch; 20h:04m:54s remains)
INFO - root - 2017-12-06 07:36:00.296626: step 14320, loss = 2.06, batch loss = 2.00 (36.7 examples/sec; 0.218 sec/batch; 19h:15m:01s remains)
INFO - root - 2017-12-06 07:36:02.548871: step 14330, loss = 2.06, batch loss = 2.00 (36.1 examples/sec; 0.222 sec/batch; 19h:34m:51s remains)
INFO - root - 2017-12-06 07:36:04.800570: step 14340, loss = 2.09, batch loss = 2.03 (36.0 examples/sec; 0.222 sec/batch; 19h:38m:39s remains)
INFO - root - 2017-12-06 07:36:07.096574: step 14350, loss = 2.08, batch loss = 2.02 (34.7 examples/sec; 0.230 sec/batch; 20h:22m:12s remains)
INFO - root - 2017-12-06 07:36:09.450925: step 14360, loss = 2.08, batch loss = 2.02 (34.7 examples/sec; 0.230 sec/batch; 20h:21m:02s remains)
INFO - root - 2017-12-06 07:36:11.763589: step 14370, loss = 2.08, batch loss = 2.02 (35.7 examples/sec; 0.224 sec/batch; 19h:47m:19s remains)
INFO - root - 2017-12-06 07:36:14.029776: step 14380, loss = 2.06, batch loss = 2.01 (36.1 examples/sec; 0.222 sec/batch; 19h:35m:31s remains)
INFO - root - 2017-12-06 07:36:16.320208: step 14390, loss = 2.07, batch loss = 2.01 (35.3 examples/sec; 0.227 sec/batch; 20h:02m:21s remains)
INFO - root - 2017-12-06 07:36:18.600344: step 14400, loss = 2.08, batch loss = 2.02 (36.1 examples/sec; 0.222 sec/batch; 19h:34m:26s remains)
2017-12-06 07:36:18.968290: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3134489 -4.3021545 -4.2866287 -4.2704177 -4.2591124 -4.2413082 -4.2415967 -4.2579823 -4.2753825 -4.2846951 -4.2917848 -4.2970772 -4.2956481 -4.2911515 -4.2837005][-4.3209105 -4.306591 -4.2887788 -4.2693868 -4.2480888 -4.2200685 -4.215086 -4.2298069 -4.2473044 -4.2617574 -4.2765179 -4.288331 -4.2873597 -4.2788134 -4.2659974][-4.3232265 -4.3063903 -4.2849298 -4.26015 -4.2299852 -4.19336 -4.1824207 -4.1944785 -4.2166905 -4.2411394 -4.2645807 -4.2764463 -4.2722631 -4.2602053 -4.2411942][-4.3202796 -4.29875 -4.2742867 -4.2496538 -4.2193427 -4.1806345 -4.1620831 -4.1675391 -4.1948137 -4.2313204 -4.2594018 -4.2651024 -4.25693 -4.2406793 -4.2133837][-4.3198414 -4.2951164 -4.2698674 -4.24791 -4.2211909 -4.181829 -4.1509924 -4.1431794 -4.1756072 -4.2240438 -4.250926 -4.2470217 -4.2311711 -4.2095523 -4.1783843][-4.3190136 -4.290513 -4.2602415 -4.2337852 -4.2036672 -4.1566415 -4.100255 -4.0658865 -4.1085162 -4.1815028 -4.2192478 -4.2184529 -4.200635 -4.1770129 -4.1437907][-4.3102579 -4.2765074 -4.2356782 -4.1934752 -4.1459022 -4.0761428 -3.9780154 -3.9055424 -3.9627347 -4.0723248 -4.1342535 -4.1494622 -4.142447 -4.1276412 -4.1005478][-4.3019557 -4.2648892 -4.221333 -4.1745691 -4.1209283 -4.0443754 -3.9260716 -3.8347414 -3.8934186 -4.0122652 -4.0859346 -4.1122503 -4.1127806 -4.0981936 -4.0733347][-4.3069606 -4.2762127 -4.241993 -4.2066178 -4.1671758 -4.1132879 -4.022162 -3.9504712 -3.9888082 -4.0743036 -4.131701 -4.1519356 -4.1473584 -4.1250334 -4.0949478][-4.3149147 -4.2929173 -4.2676883 -4.2417979 -4.2146344 -4.1800056 -4.1176648 -4.0665469 -4.0862894 -4.1428227 -4.1854367 -4.2002974 -4.1918769 -4.1685195 -4.1393452][-4.3154345 -4.295301 -4.2719946 -4.2486043 -4.2275872 -4.2022872 -4.156445 -4.1200743 -4.1310806 -4.1690249 -4.1995778 -4.2139812 -4.2074509 -4.1875944 -4.1634979][-4.3134518 -4.2904525 -4.2645793 -4.2399616 -4.2231312 -4.2026944 -4.1707892 -4.1483612 -4.154562 -4.1796527 -4.2010283 -4.2128258 -4.2077603 -4.192018 -4.1755686][-4.3094354 -4.2849689 -4.25952 -4.2376547 -4.2274466 -4.2172022 -4.2029314 -4.1959906 -4.2027264 -4.217864 -4.2286544 -4.2306633 -4.220746 -4.2032676 -4.189765][-4.3035183 -4.2759509 -4.2504134 -4.2324724 -4.2266722 -4.2257495 -4.2266932 -4.2317557 -4.2391768 -4.24689 -4.2493439 -4.245594 -4.2348723 -4.2219872 -4.2136807][-4.305356 -4.2769213 -4.2494683 -4.2305055 -4.223062 -4.2244215 -4.2325568 -4.2435846 -4.2510748 -4.2545514 -4.2545619 -4.251214 -4.2451329 -4.2384481 -4.2329879]]...]
INFO - root - 2017-12-06 07:36:21.238324: step 14410, loss = 2.07, batch loss = 2.01 (34.6 examples/sec; 0.231 sec/batch; 20h:24m:02s remains)
INFO - root - 2017-12-06 07:36:23.498701: step 14420, loss = 2.08, batch loss = 2.02 (35.5 examples/sec; 0.225 sec/batch; 19h:55m:26s remains)
INFO - root - 2017-12-06 07:36:25.860784: step 14430, loss = 2.06, batch loss = 2.00 (34.3 examples/sec; 0.233 sec/batch; 20h:36m:48s remains)
INFO - root - 2017-12-06 07:36:28.136813: step 14440, loss = 2.05, batch loss = 2.00 (36.4 examples/sec; 0.220 sec/batch; 19h:24m:57s remains)
INFO - root - 2017-12-06 07:36:30.407671: step 14450, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 20h:09m:12s remains)
INFO - root - 2017-12-06 07:36:32.671559: step 14460, loss = 2.07, batch loss = 2.02 (34.1 examples/sec; 0.234 sec/batch; 20h:42m:31s remains)
INFO - root - 2017-12-06 07:36:34.960219: step 14470, loss = 2.09, batch loss = 2.03 (34.8 examples/sec; 0.230 sec/batch; 20h:18m:18s remains)
INFO - root - 2017-12-06 07:36:37.244564: step 14480, loss = 2.05, batch loss = 1.99 (34.5 examples/sec; 0.232 sec/batch; 20h:28m:53s remains)
INFO - root - 2017-12-06 07:36:39.503204: step 14490, loss = 2.08, batch loss = 2.02 (33.9 examples/sec; 0.236 sec/batch; 20h:52m:15s remains)
INFO - root - 2017-12-06 07:36:41.759817: step 14500, loss = 2.07, batch loss = 2.01 (35.3 examples/sec; 0.227 sec/batch; 20h:02m:35s remains)
2017-12-06 07:36:42.146615: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2464714 -4.249907 -4.2391596 -4.2072878 -4.1588416 -4.113749 -4.1003265 -4.1123743 -4.1414371 -4.1942482 -4.2312703 -4.2409863 -4.2318869 -4.2042956 -4.1710548][-4.2792439 -4.2741909 -4.2610936 -4.2303591 -4.1795325 -4.1229649 -4.0872059 -4.0778193 -4.0955215 -4.1466055 -4.1908011 -4.2151289 -4.2114949 -4.1858726 -4.15139][-4.2973332 -4.2858877 -4.2742214 -4.2506762 -4.20092 -4.1335945 -4.0729856 -4.0420752 -4.0514832 -4.0996351 -4.1520777 -4.1929111 -4.2015338 -4.18142 -4.14698][-4.29545 -4.2808409 -4.2699742 -4.2557063 -4.2097917 -4.1312237 -4.0485258 -4.0070233 -4.0212684 -4.068296 -4.1253414 -4.1784825 -4.2015834 -4.1950331 -4.1701241][-4.2841349 -4.2687569 -4.2578139 -4.250196 -4.2099433 -4.1228075 -4.0190763 -3.9807191 -4.0127468 -4.0582662 -4.1067138 -4.1612329 -4.1953773 -4.2025304 -4.1969709][-4.2634006 -4.2495241 -4.2361073 -4.2275991 -4.1880226 -4.0915189 -3.9749126 -3.9558227 -4.0237856 -4.0736508 -4.0992002 -4.1392217 -4.1707144 -4.1809812 -4.1962576][-4.2340455 -4.2261038 -4.2122564 -4.1998959 -4.157125 -4.0519094 -3.9289181 -3.9321651 -4.0388551 -4.098711 -4.1025333 -4.1144676 -4.1333032 -4.1403284 -4.1684017][-4.2118545 -4.2123022 -4.2000442 -4.181808 -4.1394582 -4.0290418 -3.8982449 -3.9113741 -4.0390267 -4.1098208 -4.1005993 -4.0899625 -4.09451 -4.0966649 -4.1324406][-4.21411 -4.2221222 -4.2150297 -4.194252 -4.151433 -4.0448174 -3.912276 -3.9147463 -4.032649 -4.1049995 -4.0918584 -4.0701547 -4.0659142 -4.067122 -4.1096306][-4.2307396 -4.244338 -4.2428184 -4.2222548 -4.1806526 -4.0861197 -3.9642224 -3.9432428 -4.0292997 -4.0946083 -4.08379 -4.0597429 -4.05461 -4.0617728 -4.1084366][-4.2442784 -4.2568173 -4.257184 -4.2395358 -4.2027931 -4.124043 -4.017231 -3.9794335 -4.0333786 -4.0888929 -4.0831823 -4.0659394 -4.0609646 -4.0725131 -4.1226692][-4.256855 -4.2596321 -4.2549343 -4.2391682 -4.2097359 -4.1496911 -4.0628972 -4.021615 -4.04937 -4.0903859 -4.0922427 -4.0843558 -4.0802751 -4.0912676 -4.1406279][-4.2744069 -4.26217 -4.2460093 -4.2280154 -4.2052059 -4.1646519 -4.1052003 -4.0701647 -4.0764771 -4.096416 -4.100739 -4.1015105 -4.10329 -4.115458 -4.162][-4.2930455 -4.2712226 -4.2432208 -4.2205725 -4.2027082 -4.1802459 -4.1469541 -4.1207614 -4.1105957 -4.1089382 -4.105175 -4.1091094 -4.1173248 -4.134995 -4.1794457][-4.3112884 -4.2879844 -4.253634 -4.2250843 -4.2100034 -4.2002578 -4.1837578 -4.1630554 -4.1387148 -4.11465 -4.0968647 -4.09918 -4.1154313 -4.1427817 -4.1885729]]...]
INFO - root - 2017-12-06 07:36:44.409729: step 14510, loss = 2.07, batch loss = 2.01 (34.5 examples/sec; 0.232 sec/batch; 20h:28m:26s remains)
INFO - root - 2017-12-06 07:36:46.713037: step 14520, loss = 2.06, batch loss = 2.00 (35.9 examples/sec; 0.223 sec/batch; 19h:40m:10s remains)
INFO - root - 2017-12-06 07:36:48.992118: step 14530, loss = 2.06, batch loss = 2.01 (35.8 examples/sec; 0.223 sec/batch; 19h:43m:36s remains)
INFO - root - 2017-12-06 07:36:51.229107: step 14540, loss = 2.06, batch loss = 2.00 (35.2 examples/sec; 0.227 sec/batch; 20h:04m:57s remains)
INFO - root - 2017-12-06 07:36:53.487889: step 14550, loss = 2.06, batch loss = 2.00 (35.5 examples/sec; 0.225 sec/batch; 19h:52m:54s remains)
INFO - root - 2017-12-06 07:36:55.777624: step 14560, loss = 2.07, batch loss = 2.01 (35.2 examples/sec; 0.227 sec/batch; 20h:03m:38s remains)
INFO - root - 2017-12-06 07:36:58.045621: step 14570, loss = 2.06, batch loss = 2.00 (35.3 examples/sec; 0.227 sec/batch; 20h:00m:59s remains)
INFO - root - 2017-12-06 07:37:00.320973: step 14580, loss = 2.08, batch loss = 2.02 (33.4 examples/sec; 0.239 sec/batch; 21h:07m:34s remains)
INFO - root - 2017-12-06 07:37:02.611998: step 14590, loss = 2.07, batch loss = 2.01 (36.1 examples/sec; 0.222 sec/batch; 19h:35m:43s remains)
INFO - root - 2017-12-06 07:37:04.907772: step 14600, loss = 2.07, batch loss = 2.01 (35.5 examples/sec; 0.225 sec/batch; 19h:54m:01s remains)
2017-12-06 07:37:05.289211: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3440232 -4.3381639 -4.3335266 -4.3293657 -4.3259711 -4.322804 -4.3179507 -4.3122268 -4.3082385 -4.3044448 -4.29252 -4.2759013 -4.2524595 -4.2303824 -4.2225275][-4.31063 -4.301496 -4.2933264 -4.2849107 -4.2782965 -4.2727528 -4.2650509 -4.2574081 -4.2548051 -4.2562194 -4.2493248 -4.2391381 -4.2216077 -4.2064648 -4.205461][-4.2619662 -4.2512956 -4.240778 -4.2284307 -4.2185488 -4.2106457 -4.2008476 -4.191956 -4.1918521 -4.201633 -4.2064581 -4.2071228 -4.1976838 -4.1898255 -4.196568][-4.211669 -4.2008314 -4.1904521 -4.1781859 -4.1667109 -4.155745 -4.14026 -4.1271896 -4.1277862 -4.1441951 -4.1601753 -4.1701894 -4.1683207 -4.1709151 -4.1871362][-4.1593614 -4.1517391 -4.1439848 -4.130815 -4.112247 -4.0891705 -4.0562983 -4.033103 -4.0372882 -4.064187 -4.0944986 -4.1151381 -4.1234074 -4.140635 -4.1689286][-4.1002178 -4.09212 -4.0796037 -4.0590806 -4.0270491 -3.9844759 -3.924073 -3.8884318 -3.9077365 -3.958329 -4.0127592 -4.0538888 -4.0800257 -4.1134295 -4.1508889][-4.0406075 -4.0268016 -4.0031018 -3.9689457 -3.9211571 -3.8547838 -3.7653923 -3.7247615 -3.7750306 -3.8588307 -3.9428957 -4.0088878 -4.0552082 -4.102016 -4.1458015][-4.0088329 -3.9909654 -3.9595733 -3.912576 -3.8537655 -3.776803 -3.6820004 -3.6528091 -3.7272658 -3.8309126 -3.9316201 -4.0114589 -4.0682311 -4.1177964 -4.1602759][-4.0398059 -4.0229044 -3.9913869 -3.9434309 -3.8950667 -3.8437426 -3.7861679 -3.7677703 -3.8212252 -3.9050434 -3.992182 -4.0601954 -4.1065736 -4.1423612 -4.172471][-4.094234 -4.0817666 -4.0571342 -4.0218544 -3.9933996 -3.9721026 -3.946682 -3.9309413 -3.9580097 -4.012002 -4.0741568 -4.1196389 -4.1447897 -4.1606836 -4.1760077][-4.1293082 -4.1245604 -4.1120253 -4.09307 -4.0813856 -4.0791855 -4.0727119 -4.062027 -4.0733752 -4.1038866 -4.1415548 -4.1621962 -4.1640143 -4.1630535 -4.1698132][-4.138917 -4.1477976 -4.15189 -4.1513429 -4.1528783 -4.1616764 -4.1641097 -4.1583576 -4.1614637 -4.1744976 -4.1901383 -4.1889853 -4.1705947 -4.1581311 -4.1620154][-4.1496243 -4.170166 -4.1875038 -4.1984973 -4.2071395 -4.2173786 -4.2222834 -4.2204204 -4.2204089 -4.2252169 -4.2271476 -4.2142382 -4.1847687 -4.1656637 -4.1696734][-4.1923566 -4.2133422 -4.2327871 -4.2461839 -4.2556353 -4.2641044 -4.2703114 -4.2708855 -4.2698665 -4.26989 -4.2656016 -4.2497826 -4.2195363 -4.20105 -4.2079921][-4.2575884 -4.2717547 -4.2850704 -4.296505 -4.30442 -4.3104062 -4.31567 -4.3165016 -4.3141923 -4.3116994 -4.3063307 -4.2934761 -4.2711821 -4.2591019 -4.2684412]]...]
INFO - root - 2017-12-06 07:37:07.595509: step 14610, loss = 2.05, batch loss = 1.99 (33.8 examples/sec; 0.237 sec/batch; 20h:54m:48s remains)
INFO - root - 2017-12-06 07:37:09.925050: step 14620, loss = 2.05, batch loss = 1.99 (35.2 examples/sec; 0.227 sec/batch; 20h:03m:53s remains)
INFO - root - 2017-12-06 07:37:12.201339: step 14630, loss = 2.06, batch loss = 2.00 (35.9 examples/sec; 0.223 sec/batch; 19h:39m:54s remains)
INFO - root - 2017-12-06 07:37:14.449619: step 14640, loss = 2.08, batch loss = 2.03 (35.8 examples/sec; 0.223 sec/batch; 19h:43m:08s remains)
INFO - root - 2017-12-06 07:37:16.727869: step 14650, loss = 2.07, batch loss = 2.01 (35.2 examples/sec; 0.227 sec/batch; 20h:04m:50s remains)
INFO - root - 2017-12-06 07:37:19.001521: step 14660, loss = 2.09, batch loss = 2.03 (35.3 examples/sec; 0.227 sec/batch; 20h:02m:03s remains)
INFO - root - 2017-12-06 07:37:21.300588: step 14670, loss = 2.07, batch loss = 2.01 (33.2 examples/sec; 0.241 sec/batch; 21h:15m:53s remains)
INFO - root - 2017-12-06 07:37:23.559457: step 14680, loss = 2.05, batch loss = 1.99 (34.9 examples/sec; 0.229 sec/batch; 20h:12m:33s remains)
INFO - root - 2017-12-06 07:37:25.848728: step 14690, loss = 2.05, batch loss = 1.99 (35.6 examples/sec; 0.224 sec/batch; 19h:48m:50s remains)
INFO - root - 2017-12-06 07:37:28.120789: step 14700, loss = 2.07, batch loss = 2.01 (35.8 examples/sec; 0.223 sec/batch; 19h:42m:06s remains)
2017-12-06 07:37:28.487256: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2896037 -4.2925134 -4.3027453 -4.3067966 -4.3033814 -4.3002248 -4.3013506 -4.3035226 -4.3014021 -4.3009567 -4.3018475 -4.30436 -4.307416 -4.3102627 -4.3117981][-4.2992382 -4.3005295 -4.3094831 -4.3138084 -4.312459 -4.3103361 -4.3121619 -4.3138747 -4.3113494 -4.3090987 -4.309104 -4.3102145 -4.3108726 -4.3117776 -4.3120661][-4.2779765 -4.2731256 -4.2786593 -4.2852039 -4.2882938 -4.2874503 -4.2865863 -4.2849174 -4.281004 -4.2773829 -4.2767591 -4.2758288 -4.2734547 -4.2729311 -4.2729793][-4.2253084 -4.2117405 -4.2129207 -4.2216024 -4.2293406 -4.2283926 -4.2255621 -4.2215877 -4.2172785 -4.2130919 -4.2101464 -4.2063932 -4.2020278 -4.1989346 -4.1972632][-4.1505275 -4.1245313 -4.1182532 -4.1266632 -4.1384382 -4.1440611 -4.1446638 -4.1402373 -4.1325979 -4.1247687 -4.1159215 -4.1051512 -4.0939307 -4.0859365 -4.0809965][-4.07791 -4.0363765 -4.0189009 -4.0243645 -4.0369482 -4.044282 -4.0453925 -4.0391541 -4.0303035 -4.0218868 -4.0121641 -3.9982388 -3.9839878 -3.9737449 -3.9662862][-4.0425057 -3.9943414 -3.97054 -3.9728382 -3.9813476 -3.9809887 -3.9734132 -3.9641776 -3.9596334 -3.9593351 -3.9558332 -3.946125 -3.9368837 -3.93115 -3.9263334][-4.0966377 -4.0579777 -4.035183 -4.0310659 -4.026022 -4.0097728 -3.98883 -3.9767916 -3.9845221 -4.0052671 -4.0228438 -4.0294089 -4.0317421 -4.0339 -4.0322385][-4.1552176 -4.1265306 -4.1075487 -4.0995913 -4.0857973 -4.0597887 -4.0292625 -4.0131068 -4.0285134 -4.0637083 -4.0972676 -4.1174445 -4.1289368 -4.1347032 -4.1323929][-4.1796846 -4.1545339 -4.1381669 -4.1333733 -4.1222172 -4.1003928 -4.0738392 -4.0594826 -4.0731792 -4.10654 -4.1417403 -4.1645927 -4.1772146 -4.1807175 -4.1750832][-4.1796455 -4.1513348 -4.1333652 -4.1314893 -4.1293039 -4.1206665 -4.1074042 -4.0979638 -4.1042233 -4.1241879 -4.1490817 -4.1660604 -4.1744332 -4.1740947 -4.1667404][-4.1847682 -4.1536455 -4.1306367 -4.125401 -4.1297836 -4.1347284 -4.1365333 -4.1336107 -4.131412 -4.1377473 -4.1502914 -4.1592808 -4.1624942 -4.1581297 -4.1493392][-4.2012858 -4.16818 -4.1391439 -4.1276278 -4.1336966 -4.1453271 -4.155128 -4.1551151 -4.146174 -4.1403966 -4.1413746 -4.1433697 -4.1456208 -4.1424818 -4.1366863][-4.2246456 -4.1897264 -4.1581774 -4.1435108 -4.1463737 -4.1567292 -4.1674252 -4.1672592 -4.1560335 -4.1449866 -4.138999 -4.1343288 -4.1336784 -4.1314731 -4.1293025][-4.2443476 -4.2087641 -4.17934 -4.1668873 -4.1675878 -4.1731606 -4.1808786 -4.179697 -4.1712751 -4.16205 -4.1536064 -4.1452579 -4.1423559 -4.1403241 -4.1420016]]...]
INFO - root - 2017-12-06 07:37:30.772007: step 14710, loss = 2.08, batch loss = 2.02 (35.1 examples/sec; 0.228 sec/batch; 20h:07m:08s remains)
INFO - root - 2017-12-06 07:37:33.046107: step 14720, loss = 2.05, batch loss = 1.99 (34.7 examples/sec; 0.230 sec/batch; 20h:19m:19s remains)
INFO - root - 2017-12-06 07:37:35.310653: step 14730, loss = 2.10, batch loss = 2.04 (35.5 examples/sec; 0.225 sec/batch; 19h:53m:45s remains)
INFO - root - 2017-12-06 07:37:37.580283: step 14740, loss = 2.04, batch loss = 1.98 (34.9 examples/sec; 0.229 sec/batch; 20h:12m:23s remains)
INFO - root - 2017-12-06 07:37:39.860949: step 14750, loss = 2.07, batch loss = 2.01 (36.2 examples/sec; 0.221 sec/batch; 19h:29m:41s remains)
INFO - root - 2017-12-06 07:37:42.145816: step 14760, loss = 2.05, batch loss = 1.99 (34.8 examples/sec; 0.230 sec/batch; 20h:18m:56s remains)
INFO - root - 2017-12-06 07:37:44.398252: step 14770, loss = 2.05, batch loss = 1.99 (35.0 examples/sec; 0.229 sec/batch; 20h:11m:20s remains)
INFO - root - 2017-12-06 07:37:46.672996: step 14780, loss = 2.05, batch loss = 2.00 (36.7 examples/sec; 0.218 sec/batch; 19h:14m:11s remains)
INFO - root - 2017-12-06 07:37:48.926451: step 14790, loss = 2.09, batch loss = 2.03 (34.1 examples/sec; 0.235 sec/batch; 20h:42m:10s remains)
INFO - root - 2017-12-06 07:37:51.195313: step 14800, loss = 2.08, batch loss = 2.02 (34.1 examples/sec; 0.235 sec/batch; 20h:43m:46s remains)
2017-12-06 07:37:51.530351: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1290364 -4.1951303 -4.241425 -4.26336 -4.2716622 -4.2797503 -4.2828903 -4.2801046 -4.2774978 -4.2759323 -4.2759542 -4.2825804 -4.2899575 -4.294148 -4.2976432][-4.137876 -4.2035365 -4.2485585 -4.2677035 -4.2739077 -4.2805419 -4.2853341 -4.2880116 -4.294208 -4.3029327 -4.3111959 -4.3191342 -4.3251777 -4.3282876 -4.3283639][-4.1627488 -4.2195573 -4.25477 -4.2666111 -4.2686796 -4.2730341 -4.2794948 -4.28816 -4.3011575 -4.3155789 -4.326972 -4.3350654 -4.3393755 -4.3399329 -4.3355002][-4.1927953 -4.2342925 -4.2559514 -4.2547975 -4.2458363 -4.2433767 -4.2508855 -4.2641497 -4.2801018 -4.2969513 -4.3116379 -4.3257813 -4.3358765 -4.3393426 -4.3340006][-4.2048712 -4.2262807 -4.2268233 -4.2076392 -4.1842327 -4.172595 -4.1800365 -4.1951308 -4.2131548 -4.2352414 -4.2597394 -4.2891541 -4.3125954 -4.3243532 -4.3231144][-4.1845593 -4.1869178 -4.1655889 -4.1285677 -4.0921488 -4.0671506 -4.0626211 -4.066855 -4.0875344 -4.1261678 -4.1754632 -4.229248 -4.2701893 -4.2955222 -4.3068409][-4.1447725 -4.1369467 -4.1050711 -4.0604372 -4.0178194 -3.974704 -3.9384298 -3.91138 -3.9277387 -3.991966 -4.0769639 -4.1574321 -4.2148366 -4.2569489 -4.2872224][-4.1254354 -4.1125727 -4.0762033 -4.031126 -3.9855776 -3.9214041 -3.840049 -3.7611675 -3.7676446 -3.8593953 -3.9787221 -4.0800223 -4.1472273 -4.2049117 -4.2561922][-4.1470051 -4.1396141 -4.109592 -4.0688362 -4.021625 -3.9442658 -3.8407347 -3.7344913 -3.7287362 -3.8180177 -3.9313478 -4.0266 -4.0901823 -4.1547079 -4.2170491][-4.1819148 -4.1849842 -4.1714773 -4.1441298 -4.1030736 -4.0300441 -3.9456606 -3.8684139 -3.8575344 -3.9005404 -3.9607325 -4.0142922 -4.0541019 -4.112689 -4.1772451][-4.2371469 -4.24066 -4.23059 -4.2080388 -4.1673012 -4.1034284 -4.0451841 -4.003818 -3.9974184 -4.0074205 -4.0227656 -4.0328374 -4.0425472 -4.0832629 -4.1395574][-4.281744 -4.2839012 -4.2700047 -4.2434473 -4.2001419 -4.1488905 -4.1163783 -4.1038809 -4.1098313 -4.1133933 -4.1121593 -4.1009989 -4.083766 -4.0904093 -4.1194911][-4.30566 -4.3069925 -4.2979894 -4.2780743 -4.2403345 -4.2026186 -4.184691 -4.1857285 -4.1988883 -4.2054992 -4.2038684 -4.1911635 -4.1660371 -4.1509194 -4.1498365][-4.3176236 -4.3212252 -4.3198009 -4.3096805 -4.281476 -4.2562914 -4.2456455 -4.2480555 -4.2622108 -4.2744803 -4.2808723 -4.2755589 -4.2546425 -4.2325921 -4.2143273][-4.3141394 -4.3205843 -4.3246479 -4.3219171 -4.3017926 -4.2809396 -4.266789 -4.2642579 -4.2765789 -4.2934856 -4.3092918 -4.3153963 -4.3064919 -4.2906337 -4.2713895]]...]
INFO - root - 2017-12-06 07:37:53.829765: step 14810, loss = 2.07, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 19h:36m:00s remains)
INFO - root - 2017-12-06 07:37:56.098963: step 14820, loss = 2.05, batch loss = 1.99 (34.5 examples/sec; 0.232 sec/batch; 20h:28m:49s remains)
INFO - root - 2017-12-06 07:37:58.372841: step 14830, loss = 2.09, batch loss = 2.03 (36.2 examples/sec; 0.221 sec/batch; 19h:31m:05s remains)
INFO - root - 2017-12-06 07:38:00.709708: step 14840, loss = 2.05, batch loss = 2.00 (36.1 examples/sec; 0.221 sec/batch; 19h:31m:57s remains)
INFO - root - 2017-12-06 07:38:02.955336: step 14850, loss = 2.05, batch loss = 1.99 (35.5 examples/sec; 0.225 sec/batch; 19h:51m:48s remains)
INFO - root - 2017-12-06 07:38:05.240292: step 14860, loss = 2.08, batch loss = 2.02 (34.2 examples/sec; 0.234 sec/batch; 20h:37m:57s remains)
INFO - root - 2017-12-06 07:38:07.549408: step 14870, loss = 2.06, batch loss = 2.00 (35.3 examples/sec; 0.227 sec/batch; 19h:59m:43s remains)
INFO - root - 2017-12-06 07:38:09.860609: step 14880, loss = 2.04, batch loss = 1.99 (36.5 examples/sec; 0.219 sec/batch; 19h:21m:42s remains)
INFO - root - 2017-12-06 07:38:12.101129: step 14890, loss = 2.06, batch loss = 2.01 (35.5 examples/sec; 0.225 sec/batch; 19h:52m:48s remains)
INFO - root - 2017-12-06 07:38:14.368957: step 14900, loss = 2.06, batch loss = 2.01 (35.2 examples/sec; 0.227 sec/batch; 20h:02m:42s remains)
2017-12-06 07:38:14.716114: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1930833 -4.1659884 -4.1383333 -4.1321635 -4.1519608 -4.1762571 -4.1793551 -4.1545086 -4.1272583 -4.1178045 -4.12505 -4.1377621 -4.1336226 -4.1132522 -4.1026635][-4.19167 -4.1537991 -4.1164927 -4.1149049 -4.1394873 -4.1630597 -4.1612329 -4.1226892 -4.0870204 -4.0883594 -4.107595 -4.1256452 -4.1136551 -4.0794754 -4.0672073][-4.18533 -4.1521759 -4.1230335 -4.1236362 -4.1393828 -4.1493063 -4.1401539 -4.1054416 -4.0767145 -4.0825176 -4.1023707 -4.1114006 -4.0938692 -4.0638838 -4.0622296][-4.1727629 -4.1539359 -4.1425285 -4.1467652 -4.1537943 -4.1548643 -4.1495333 -4.1337905 -4.1236162 -4.1269979 -4.1341052 -4.1331482 -4.1196055 -4.1097007 -4.1155181][-4.1308341 -4.1314821 -4.1388288 -4.1444097 -4.1443758 -4.1403461 -4.1493788 -4.1631985 -4.1699753 -4.1707439 -4.1669359 -4.1628933 -4.1625638 -4.171566 -4.1770306][-4.0857177 -4.0943956 -4.1039338 -4.0992603 -4.092536 -4.0941811 -4.1133213 -4.1392322 -4.1545706 -4.1589646 -4.1630068 -4.1674991 -4.1777353 -4.1897478 -4.1909847][-4.0696 -4.0611935 -4.0498862 -4.0245409 -4.0145583 -4.0130663 -4.0172343 -4.034018 -4.0464725 -4.0638876 -4.0951076 -4.1200771 -4.14039 -4.1499023 -4.1475573][-4.0919356 -4.0600176 -4.0204449 -3.9844835 -3.9722798 -3.9556308 -3.931886 -3.9256408 -3.9259367 -3.9527147 -4.0123219 -4.05722 -4.0725722 -4.0591087 -4.0479188][-4.1139503 -4.079124 -4.0446811 -4.0301042 -4.0279341 -4.0020528 -3.9634106 -3.945435 -3.9404495 -3.9665337 -4.0170021 -4.0446043 -4.0336475 -3.9921255 -3.9767368][-4.1227636 -4.0988073 -4.0828471 -4.0931911 -4.10232 -4.0814409 -4.0491867 -4.0340314 -4.0376058 -4.0672245 -4.1008806 -4.10641 -4.0774403 -4.036078 -4.0313158][-4.1397209 -4.12375 -4.1187596 -4.1326852 -4.1416764 -4.1212926 -4.0968566 -4.0901804 -4.1036978 -4.1311812 -4.1514893 -4.1495366 -4.1241207 -4.1032009 -4.1132221][-4.1612606 -4.1492138 -4.1426344 -4.14562 -4.1424651 -4.1222858 -4.1075954 -4.108665 -4.12397 -4.1415606 -4.1482606 -4.14516 -4.1332808 -4.1306686 -4.1406155][-4.1769338 -4.1704469 -4.1622119 -4.1558657 -4.1442013 -4.127665 -4.1249223 -4.1304183 -4.1399293 -4.1421809 -4.1375089 -4.1313133 -4.1248097 -4.1249876 -4.1221347][-4.1768646 -4.1679125 -4.1516132 -4.1404152 -4.138093 -4.1360655 -4.1434908 -4.1530228 -4.157268 -4.1543512 -4.1506667 -4.1512051 -4.1458988 -4.14048 -4.1188745][-4.1506262 -4.1321521 -4.1070628 -4.0965295 -4.1024356 -4.1122046 -4.1275229 -4.1463089 -4.1563559 -4.1608124 -4.168354 -4.1773877 -4.1767011 -4.1697783 -4.1426563]]...]
INFO - root - 2017-12-06 07:38:16.997195: step 14910, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 20h:07m:27s remains)
INFO - root - 2017-12-06 07:38:19.281485: step 14920, loss = 2.04, batch loss = 1.98 (34.9 examples/sec; 0.229 sec/batch; 20h:12m:07s remains)
INFO - root - 2017-12-06 07:38:21.593601: step 14930, loss = 2.08, batch loss = 2.02 (35.2 examples/sec; 0.227 sec/batch; 20h:02m:38s remains)
INFO - root - 2017-12-06 07:38:23.908638: step 14940, loss = 2.05, batch loss = 1.99 (32.4 examples/sec; 0.247 sec/batch; 21h:48m:05s remains)
INFO - root - 2017-12-06 07:38:26.210829: step 14950, loss = 2.05, batch loss = 1.99 (35.5 examples/sec; 0.225 sec/batch; 19h:51m:11s remains)
INFO - root - 2017-12-06 07:38:28.493806: step 14960, loss = 2.08, batch loss = 2.02 (35.1 examples/sec; 0.228 sec/batch; 20h:07m:02s remains)
INFO - root - 2017-12-06 07:38:30.806278: step 14970, loss = 2.05, batch loss = 1.99 (33.2 examples/sec; 0.241 sec/batch; 21h:14m:16s remains)
INFO - root - 2017-12-06 07:38:33.063448: step 14980, loss = 2.08, batch loss = 2.02 (35.8 examples/sec; 0.224 sec/batch; 19h:43m:41s remains)
INFO - root - 2017-12-06 07:38:35.368502: step 14990, loss = 2.07, batch loss = 2.01 (34.8 examples/sec; 0.230 sec/batch; 20h:15m:03s remains)
INFO - root - 2017-12-06 07:38:37.650342: step 15000, loss = 2.09, batch loss = 2.03 (33.7 examples/sec; 0.237 sec/batch; 20h:54m:39s remains)
2017-12-06 07:38:38.023229: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3290172 -4.3301144 -4.3278828 -4.3204741 -4.30484 -4.2822056 -4.26073 -4.2373042 -4.2203403 -4.2131948 -4.2193069 -4.2350874 -4.2569013 -4.2815442 -4.3056135][-4.3303161 -4.332334 -4.332583 -4.3232265 -4.2978883 -4.2664604 -4.2362313 -4.2069097 -4.1912966 -4.1914315 -4.2040787 -4.2180948 -4.2353992 -4.258368 -4.2889433][-4.3328948 -4.3362861 -4.3380618 -4.3231792 -4.2852683 -4.2429028 -4.2080584 -4.1833339 -4.1752973 -4.1854053 -4.2046719 -4.2175879 -4.2273254 -4.2437854 -4.2738113][-4.3359828 -4.3407941 -4.3392453 -4.3129897 -4.2597761 -4.2028651 -4.1660433 -4.159523 -4.1702065 -4.1870694 -4.2071481 -4.2198119 -4.2264404 -4.2362795 -4.262466][-4.3322043 -4.3319259 -4.3186641 -4.2785945 -4.2114921 -4.1350317 -4.0890317 -4.1063533 -4.1476192 -4.177393 -4.1979547 -4.2075505 -4.2157149 -4.2254505 -4.2469664][-4.3063107 -4.2857852 -4.2497663 -4.1911855 -4.1054621 -4.0082006 -3.948086 -3.9945374 -4.0795465 -4.1378675 -4.1669745 -4.1771617 -4.19119 -4.206614 -4.2265449][-4.2416916 -4.1901908 -4.125483 -4.0429654 -3.9410486 -3.8309102 -3.7600884 -3.8352911 -3.9680095 -4.0639071 -4.1096225 -4.128819 -4.1528306 -4.1746464 -4.1970768][-4.1551418 -4.0763226 -3.9891913 -3.8932858 -3.7970724 -3.7059498 -3.6527934 -3.7471323 -3.8949161 -4.0027313 -4.0579243 -4.0883183 -4.1218863 -4.1479878 -4.1760054][-4.0958838 -4.01089 -3.9295907 -3.8531079 -3.8023744 -3.7685952 -3.7597637 -3.8265572 -3.9248142 -4.0072722 -4.0579557 -4.0952463 -4.13325 -4.1609006 -4.1916337][-4.1026359 -4.0313697 -3.9780807 -3.9397402 -3.9348271 -3.9466414 -3.9559858 -3.9807169 -4.0218229 -4.0705833 -4.1099243 -4.1498928 -4.186337 -4.2110891 -4.2375207][-4.1589136 -4.1112881 -4.088872 -4.084404 -4.1015129 -4.1248016 -4.1338139 -4.13611 -4.1423521 -4.1649413 -4.195303 -4.2302356 -4.2566113 -4.2729855 -4.2917056][-4.2237263 -4.1980853 -4.1958685 -4.2086334 -4.228694 -4.243361 -4.2472696 -4.2438745 -4.2385349 -4.2479792 -4.2715468 -4.2984061 -4.3155074 -4.3252234 -4.3349648][-4.2727475 -4.262691 -4.2679272 -4.2825937 -4.2967954 -4.3053331 -4.306582 -4.3026633 -4.2953262 -4.2963691 -4.312356 -4.3324146 -4.3460526 -4.353467 -4.35613][-4.304594 -4.3024154 -4.3079991 -4.3181124 -4.3261323 -4.3300056 -4.3300376 -4.3258271 -4.3194056 -4.3172722 -4.3256783 -4.3404136 -4.3532581 -4.3600321 -4.3591213][-4.3242264 -4.3233991 -4.325973 -4.331172 -4.3357787 -4.3377752 -4.3369536 -4.3336525 -4.3295984 -4.3280821 -4.3319268 -4.3402128 -4.34862 -4.35292 -4.3516526]]...]
INFO - root - 2017-12-06 07:38:40.315561: step 15010, loss = 2.08, batch loss = 2.02 (35.9 examples/sec; 0.223 sec/batch; 19h:38m:54s remains)
INFO - root - 2017-12-06 07:38:42.593095: step 15020, loss = 2.06, batch loss = 2.00 (34.0 examples/sec; 0.235 sec/batch; 20h:44m:27s remains)
INFO - root - 2017-12-06 07:38:44.885366: step 15030, loss = 2.06, batch loss = 2.00 (35.3 examples/sec; 0.227 sec/batch; 20h:00m:48s remains)
INFO - root - 2017-12-06 07:38:47.191499: step 15040, loss = 2.07, batch loss = 2.02 (34.2 examples/sec; 0.234 sec/batch; 20h:38m:00s remains)
INFO - root - 2017-12-06 07:38:49.477511: step 15050, loss = 2.05, batch loss = 1.99 (35.5 examples/sec; 0.225 sec/batch; 19h:51m:20s remains)
INFO - root - 2017-12-06 07:38:51.728671: step 15060, loss = 2.05, batch loss = 1.99 (36.1 examples/sec; 0.222 sec/batch; 19h:32m:10s remains)
INFO - root - 2017-12-06 07:38:53.985063: step 15070, loss = 2.11, batch loss = 2.05 (35.0 examples/sec; 0.229 sec/batch; 20h:10m:13s remains)
INFO - root - 2017-12-06 07:38:56.252010: step 15080, loss = 2.07, batch loss = 2.01 (35.0 examples/sec; 0.229 sec/batch; 20h:10m:24s remains)
INFO - root - 2017-12-06 07:38:58.548590: step 15090, loss = 2.08, batch loss = 2.03 (35.0 examples/sec; 0.228 sec/batch; 20h:07m:40s remains)
INFO - root - 2017-12-06 07:39:00.882934: step 15100, loss = 2.06, batch loss = 2.00 (35.1 examples/sec; 0.228 sec/batch; 20h:04m:27s remains)
2017-12-06 07:39:01.273285: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3140531 -4.2958732 -4.2795544 -4.2663221 -4.2565155 -4.2466221 -4.2351527 -4.2312779 -4.2444558 -4.2622013 -4.2705021 -4.2712522 -4.271162 -4.2732682 -4.2738566][-4.3008642 -4.2760706 -4.2594409 -4.24279 -4.2264061 -4.2085633 -4.193059 -4.1873908 -4.206243 -4.2321396 -4.2453403 -4.2441263 -4.2379479 -4.2381744 -4.2387438][-4.2923183 -4.2650175 -4.2508631 -4.2352777 -4.2102737 -4.1784544 -4.1594486 -4.1587892 -4.1851778 -4.2190986 -4.2377911 -4.2350216 -4.2227507 -4.2197142 -4.2210283][-4.2855597 -4.2578025 -4.2446818 -4.2286463 -4.1964822 -4.148387 -4.1205626 -4.1354384 -4.1774044 -4.217782 -4.2399349 -4.2377882 -4.2211404 -4.2149954 -4.2160354][-4.2698278 -4.2437453 -4.229413 -4.210988 -4.1732864 -4.10563 -4.0568786 -4.0848093 -4.1499615 -4.2020874 -4.2319183 -4.2357111 -4.2226229 -4.2147303 -4.2138844][-4.2469497 -4.2222395 -4.2086663 -4.1870875 -4.1435089 -4.0587969 -3.9858737 -4.0162272 -4.1060615 -4.1757236 -4.2136979 -4.2246485 -4.2190886 -4.2151208 -4.2138438][-4.22256 -4.2014432 -4.1852617 -4.1550169 -4.101552 -4.0023222 -3.909935 -3.9322917 -4.0387077 -4.128243 -4.1781373 -4.199369 -4.2059793 -4.2113109 -4.2129917][-4.2049685 -4.1901112 -4.1740856 -4.1355228 -4.0715961 -3.9675372 -3.872962 -3.8881776 -3.9894001 -4.0807557 -4.136384 -4.1677737 -4.1905079 -4.205328 -4.210114][-4.2014203 -4.193244 -4.1825051 -4.1489282 -4.0952663 -4.0104818 -3.9375222 -3.9458692 -4.0148621 -4.0776978 -4.121799 -4.1547084 -4.1838493 -4.2011695 -4.2077918][-4.2108235 -4.2060938 -4.1991673 -4.1753416 -4.1398826 -4.0890708 -4.0496922 -4.0574026 -4.0918784 -4.1224627 -4.1486492 -4.1714535 -4.19453 -4.2072353 -4.2121696][-4.226326 -4.2261124 -4.2229304 -4.2089734 -4.1835361 -4.1511822 -4.1337619 -4.145062 -4.16546 -4.1828728 -4.1968207 -4.2077022 -4.2208624 -4.2269073 -4.2296529][-4.246748 -4.2527847 -4.255054 -4.2524056 -4.2352576 -4.2089577 -4.1980472 -4.2111969 -4.2263756 -4.2385011 -4.2458897 -4.2503104 -4.2554655 -4.256803 -4.2574768][-4.26575 -4.2769732 -4.2815266 -4.283721 -4.2704825 -4.2477355 -4.2407966 -4.257134 -4.269268 -4.2777877 -4.2835951 -4.2858138 -4.2881293 -4.2877841 -4.2857275][-4.2796388 -4.2945027 -4.3020549 -4.3052907 -4.2967725 -4.2801895 -4.2782993 -4.2925868 -4.3005419 -4.3048663 -4.3100233 -4.3128538 -4.3142562 -4.3134346 -4.3093109][-4.2878375 -4.2996936 -4.3078589 -4.3122072 -4.3108697 -4.3070536 -4.3109484 -4.3209558 -4.3252306 -4.3277907 -4.3298573 -4.3306184 -4.3287516 -4.3265052 -4.3225021]]...]
INFO - root - 2017-12-06 07:39:03.567391: step 15110, loss = 2.06, batch loss = 2.00 (34.2 examples/sec; 0.234 sec/batch; 20h:35m:58s remains)
INFO - root - 2017-12-06 07:39:05.897157: step 15120, loss = 2.09, batch loss = 2.03 (36.2 examples/sec; 0.221 sec/batch; 19h:30m:10s remains)
INFO - root - 2017-12-06 07:39:08.179956: step 15130, loss = 2.08, batch loss = 2.02 (35.4 examples/sec; 0.226 sec/batch; 19h:54m:51s remains)
INFO - root - 2017-12-06 07:39:10.430334: step 15140, loss = 2.08, batch loss = 2.02 (36.1 examples/sec; 0.222 sec/batch; 19h:32m:49s remains)
INFO - root - 2017-12-06 07:39:12.717825: step 15150, loss = 2.06, batch loss = 2.00 (34.7 examples/sec; 0.231 sec/batch; 20h:21m:06s remains)
INFO - root - 2017-12-06 07:39:15.034573: step 15160, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 20h:05m:31s remains)
INFO - root - 2017-12-06 07:39:17.318138: step 15170, loss = 2.07, batch loss = 2.01 (35.6 examples/sec; 0.225 sec/batch; 19h:49m:11s remains)
INFO - root - 2017-12-06 07:39:19.611660: step 15180, loss = 2.05, batch loss = 1.99 (35.6 examples/sec; 0.225 sec/batch; 19h:49m:49s remains)
INFO - root - 2017-12-06 07:39:21.888147: step 15190, loss = 2.07, batch loss = 2.01 (35.3 examples/sec; 0.226 sec/batch; 19h:57m:09s remains)
INFO - root - 2017-12-06 07:39:24.220126: step 15200, loss = 2.04, batch loss = 1.98 (31.6 examples/sec; 0.253 sec/batch; 22h:18m:00s remains)
2017-12-06 07:39:24.581402: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2211337 -4.2341809 -4.2377377 -4.2282648 -4.218039 -4.2198262 -4.2264171 -4.237668 -4.2280612 -4.2174025 -4.2165575 -4.2268286 -4.2551289 -4.2896519 -4.3056407][-4.2391734 -4.2546263 -4.2596169 -4.2492495 -4.2373219 -4.2312145 -4.2327013 -4.2416477 -4.238039 -4.2333069 -4.2390313 -4.2540689 -4.2739329 -4.2949224 -4.3041286][-4.2453012 -4.2651105 -4.27472 -4.2673407 -4.2540011 -4.2408357 -4.2358885 -4.2384548 -4.2341061 -4.2301483 -4.2398381 -4.2573643 -4.2728386 -4.2886481 -4.2986159][-4.2341819 -4.2561617 -4.272872 -4.2717934 -4.2583241 -4.2395515 -4.2269273 -4.2237759 -4.2181916 -4.2134986 -4.2252097 -4.242732 -4.2535534 -4.2699442 -4.2862496][-4.2110648 -4.2363529 -4.2638087 -4.27284 -4.2626958 -4.2398973 -4.2190528 -4.2063265 -4.1954012 -4.1866345 -4.1955104 -4.2097664 -4.2183452 -4.2403631 -4.2676568][-4.1884246 -4.2122912 -4.2444892 -4.2614803 -4.2556629 -4.2349052 -4.211041 -4.1886349 -4.168025 -4.1469197 -4.1453967 -4.1530857 -4.1700654 -4.2100525 -4.2523551][-4.1630516 -4.1832142 -4.2178555 -4.2418094 -4.2429004 -4.2244883 -4.196753 -4.1619473 -4.1273322 -4.0932188 -4.0796547 -4.0881414 -4.1286225 -4.194994 -4.2517042][-4.1392579 -4.1545343 -4.1886134 -4.2187796 -4.228714 -4.2161226 -4.1854897 -4.1393733 -4.0898376 -4.0422869 -4.0214334 -4.0434155 -4.1145821 -4.2021003 -4.2642074][-4.1182623 -4.1267962 -4.1578116 -4.1900649 -4.2057776 -4.2008243 -4.1693964 -4.1149197 -4.0497608 -3.9934325 -3.9803617 -4.0306635 -4.1248293 -4.2173014 -4.2755976][-4.1049314 -4.1037602 -4.1333041 -4.1662016 -4.1846123 -4.1804142 -4.1465368 -4.0881391 -4.0203404 -3.9750273 -3.9865088 -4.0580082 -4.1528397 -4.2342772 -4.2820277][-4.1128139 -4.1049194 -4.1360145 -4.1689105 -4.1831183 -4.1732836 -4.1354532 -4.0831738 -4.0338783 -4.0202646 -4.053267 -4.1222067 -4.1971679 -4.256321 -4.2908278][-4.137763 -4.1324196 -4.1650963 -4.1909823 -4.1954594 -4.1779079 -4.1393504 -4.1005082 -4.0817194 -4.0994864 -4.140131 -4.1920857 -4.2412572 -4.2812734 -4.3047342][-4.1850085 -4.1864009 -4.2131357 -4.2274847 -4.2211218 -4.2006812 -4.1698136 -4.1485248 -4.1501517 -4.1807814 -4.21986 -4.2534628 -4.2828784 -4.3090043 -4.3225789][-4.2447319 -4.2457361 -4.2603788 -4.2629552 -4.2541232 -4.2410641 -4.2241106 -4.2187696 -4.2271895 -4.2532873 -4.2824974 -4.2996569 -4.3163815 -4.3318229 -4.3366742][-4.2902493 -4.2897868 -4.29402 -4.2903357 -4.2816405 -4.27579 -4.2705312 -4.272388 -4.2826324 -4.302938 -4.3220267 -4.3309751 -4.3389587 -4.3457255 -4.3444276]]...]
INFO - root - 2017-12-06 07:39:26.847630: step 15210, loss = 2.06, batch loss = 2.00 (35.9 examples/sec; 0.223 sec/batch; 19h:36m:50s remains)
INFO - root - 2017-12-06 07:39:29.086730: step 15220, loss = 2.06, batch loss = 2.00 (37.7 examples/sec; 0.212 sec/batch; 18h:41m:24s remains)
INFO - root - 2017-12-06 07:39:31.342179: step 15230, loss = 2.06, batch loss = 2.00 (34.9 examples/sec; 0.229 sec/batch; 20h:10m:44s remains)
INFO - root - 2017-12-06 07:39:33.634438: step 15240, loss = 2.06, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 20h:05m:08s remains)
INFO - root - 2017-12-06 07:39:35.905612: step 15250, loss = 2.07, batch loss = 2.01 (36.4 examples/sec; 0.220 sec/batch; 19h:21m:16s remains)
INFO - root - 2017-12-06 07:39:38.161000: step 15260, loss = 2.07, batch loss = 2.01 (35.3 examples/sec; 0.227 sec/batch; 19h:58m:58s remains)
INFO - root - 2017-12-06 07:39:40.456381: step 15270, loss = 2.08, batch loss = 2.02 (35.9 examples/sec; 0.223 sec/batch; 19h:37m:05s remains)
INFO - root - 2017-12-06 07:39:42.722043: step 15280, loss = 2.06, batch loss = 2.00 (35.8 examples/sec; 0.224 sec/batch; 19h:42m:55s remains)
INFO - root - 2017-12-06 07:39:44.995468: step 15290, loss = 2.09, batch loss = 2.03 (34.6 examples/sec; 0.231 sec/batch; 20h:21m:43s remains)
INFO - root - 2017-12-06 07:39:47.230497: step 15300, loss = 2.07, batch loss = 2.01 (36.8 examples/sec; 0.217 sec/batch; 19h:08m:18s remains)
2017-12-06 07:39:47.587918: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1912332 -4.1994042 -4.2257872 -4.2481518 -4.2575693 -4.2547159 -4.2246637 -4.1868124 -4.1588216 -4.1486168 -4.1372261 -4.1087084 -4.0979118 -4.1397758 -4.1995726][-4.185884 -4.2040243 -4.2299809 -4.2407103 -4.2407804 -4.2326488 -4.1981983 -4.1610847 -4.1390762 -4.1314096 -4.1214948 -4.0986805 -4.092845 -4.136898 -4.2023149][-4.1747308 -4.192862 -4.208765 -4.2122583 -4.2174325 -4.2150159 -4.1836724 -4.1518044 -4.1408191 -4.1383991 -4.1255717 -4.0968108 -4.0821762 -4.1154509 -4.1763463][-4.1403871 -4.1591635 -4.1652613 -4.16967 -4.1872268 -4.1923656 -4.1611629 -4.1320395 -4.13315 -4.144774 -4.1385674 -4.1060281 -4.0848556 -4.1089139 -4.1671448][-4.098762 -4.1205816 -4.121469 -4.1250634 -4.1451511 -4.1496964 -4.1163759 -4.0861993 -4.0962772 -4.1267881 -4.1370773 -4.117177 -4.1012993 -4.1192822 -4.172977][-4.0757422 -4.1008668 -4.1000504 -4.0979285 -4.1119933 -4.1093917 -4.0688887 -4.0314522 -4.044836 -4.0939174 -4.1286697 -4.1332793 -4.1274862 -4.1370454 -4.1775613][-4.0738058 -4.0985909 -4.0961728 -4.081552 -4.0892429 -4.0839763 -4.0431108 -4.0019126 -4.0144176 -4.0701423 -4.1230855 -4.1502514 -4.1554894 -4.1504459 -4.1715117][-4.08303 -4.0980692 -4.0872312 -4.0586128 -4.0614133 -4.05775 -4.02108 -3.9846961 -3.9973979 -4.0522723 -4.1138868 -4.1571155 -4.1706161 -4.1526713 -4.1574821][-4.0965943 -4.1032991 -4.0836825 -4.0488825 -4.0462394 -4.0457687 -4.0156412 -3.994163 -4.0119915 -4.0638785 -4.1228428 -4.1646309 -4.1723313 -4.1473546 -4.1431251][-4.1366677 -4.1378202 -4.1162033 -4.0812883 -4.0759711 -4.0759311 -4.0582991 -4.0520382 -4.0698633 -4.1081929 -4.1516066 -4.178369 -4.1710773 -4.1388369 -4.1295791][-4.1825433 -4.1843524 -4.1672544 -4.1403122 -4.1380844 -4.1409655 -4.1321731 -4.1323943 -4.1480908 -4.1706409 -4.1937823 -4.2035065 -4.1794496 -4.1407452 -4.1292357][-4.2054362 -4.2114215 -4.2046704 -4.1917071 -4.1948748 -4.202 -4.198297 -4.1991215 -4.2112675 -4.2234907 -4.2319527 -4.2311926 -4.2019176 -4.1679854 -4.16051][-4.2133994 -4.2223325 -4.2227864 -4.2205725 -4.2274766 -4.23596 -4.2373271 -4.241796 -4.2565727 -4.2677455 -4.2708006 -4.2650957 -4.2402349 -4.2163868 -4.2130604][-4.2108083 -4.2217865 -4.2299576 -4.2377729 -4.2492237 -4.2605376 -4.2679305 -4.2759838 -4.2903066 -4.3005147 -4.3003774 -4.29081 -4.270802 -4.2559381 -4.2554159][-4.2062397 -4.2185092 -4.2303586 -4.2419972 -4.2543736 -4.2671924 -4.2809205 -4.2933159 -4.3062272 -4.3143797 -4.3119655 -4.3006134 -4.285871 -4.2777791 -4.2777457]]...]
INFO - root - 2017-12-06 07:39:49.902399: step 15310, loss = 2.08, batch loss = 2.02 (36.1 examples/sec; 0.222 sec/batch; 19h:32m:09s remains)
INFO - root - 2017-12-06 07:39:52.186373: step 15320, loss = 2.09, batch loss = 2.03 (35.1 examples/sec; 0.228 sec/batch; 20h:06m:27s remains)
INFO - root - 2017-12-06 07:39:54.456602: step 15330, loss = 2.03, batch loss = 1.97 (34.3 examples/sec; 0.233 sec/batch; 20h:32m:10s remains)
INFO - root - 2017-12-06 07:39:56.713365: step 15340, loss = 2.06, batch loss = 2.00 (35.9 examples/sec; 0.223 sec/batch; 19h:36m:24s remains)
INFO - root - 2017-12-06 07:39:58.966514: step 15350, loss = 2.09, batch loss = 2.03 (34.5 examples/sec; 0.232 sec/batch; 20h:26m:12s remains)
INFO - root - 2017-12-06 07:40:01.237058: step 15360, loss = 2.04, batch loss = 1.98 (34.7 examples/sec; 0.230 sec/batch; 20h:17m:45s remains)
INFO - root - 2017-12-06 07:40:03.546747: step 15370, loss = 2.08, batch loss = 2.02 (35.7 examples/sec; 0.224 sec/batch; 19h:45m:39s remains)
INFO - root - 2017-12-06 07:40:05.848563: step 15380, loss = 2.05, batch loss = 1.99 (35.4 examples/sec; 0.226 sec/batch; 19h:53m:05s remains)
INFO - root - 2017-12-06 07:40:08.168942: step 15390, loss = 2.09, batch loss = 2.03 (33.3 examples/sec; 0.240 sec/batch; 21h:10m:07s remains)
INFO - root - 2017-12-06 07:40:10.478324: step 15400, loss = 2.07, batch loss = 2.02 (35.5 examples/sec; 0.226 sec/batch; 19h:52m:25s remains)
2017-12-06 07:40:10.890022: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2952795 -4.2959547 -4.2953629 -4.2943244 -4.2902055 -4.2840176 -4.2782073 -4.2746243 -4.27419 -4.2765393 -4.2802715 -4.2845621 -4.2886505 -4.2911544 -4.2920609][-4.2788019 -4.2800364 -4.2802582 -4.278831 -4.2718315 -4.2613578 -4.2516 -4.2460713 -4.2453909 -4.2491841 -4.255712 -4.2627335 -4.2699475 -4.2750616 -4.2780161][-4.2521577 -4.2545419 -4.2557735 -4.2532992 -4.241456 -4.2233868 -4.2060642 -4.1970596 -4.1972761 -4.2056723 -4.2190938 -4.2317119 -4.2430067 -4.2513227 -4.2568469][-4.2111835 -4.2139764 -4.2155447 -4.2107158 -4.1926279 -4.1642218 -4.13775 -4.1268229 -4.1327448 -4.1504755 -4.1730614 -4.1913271 -4.205543 -4.2158132 -4.2233524][-4.17499 -4.1745334 -4.1727362 -4.1626015 -4.1355224 -4.0947418 -4.0597 -4.0521398 -4.072401 -4.1039591 -4.1345663 -4.1565161 -4.1723986 -4.1826587 -4.1901631][-4.1453671 -4.1372795 -4.1273403 -4.1082115 -4.0691662 -4.0152097 -3.9772358 -3.9842377 -4.0257516 -4.071898 -4.1073723 -4.1305184 -4.1455045 -4.1524448 -4.1579323][-4.1228523 -4.1037827 -4.0844951 -4.0584054 -4.0105572 -3.9477286 -3.915941 -3.9444029 -4.0052185 -4.0618153 -4.0996785 -4.1204453 -4.1324463 -4.1350212 -4.1387324][-4.1161671 -4.093019 -4.0748706 -4.0539117 -4.0133367 -3.9602423 -3.9436421 -3.9813361 -4.0418315 -4.0943542 -4.1259322 -4.1379218 -4.1439605 -4.141026 -4.1408749][-4.1283937 -4.1101089 -4.1023169 -4.095799 -4.0778503 -4.0508533 -4.0498104 -4.0780382 -4.1168108 -4.149683 -4.1641874 -4.1611147 -4.1585927 -4.1505795 -4.1454654][-4.14926 -4.1399913 -4.1446452 -4.1491771 -4.1467285 -4.1369271 -4.1431656 -4.1622281 -4.1808023 -4.1925921 -4.19162 -4.1773219 -4.1669106 -4.154511 -4.1468463][-4.1719928 -4.1733704 -4.1881413 -4.1967731 -4.1979961 -4.1943727 -4.2018824 -4.2144394 -4.2211037 -4.2216444 -4.213635 -4.1958375 -4.1803012 -4.1679735 -4.1632357][-4.19068 -4.1994376 -4.2183647 -4.2276115 -4.2290621 -4.2280488 -4.2365432 -4.2465162 -4.2501035 -4.2451982 -4.234128 -4.2190905 -4.2036376 -4.1923409 -4.1887755][-4.199059 -4.2077665 -4.2237825 -4.2335916 -4.2366934 -4.238596 -4.2488708 -4.2573528 -4.2610555 -4.25304 -4.2415886 -4.2335052 -4.2228184 -4.2121234 -4.2072639][-4.2004828 -4.201405 -4.2111764 -4.2235126 -4.2323637 -4.2389507 -4.2509785 -4.2581267 -4.2593317 -4.2471981 -4.236928 -4.2356462 -4.2318716 -4.2235074 -4.2195263][-4.1935644 -4.1865406 -4.1902304 -4.2047977 -4.2202091 -4.2328987 -4.2466974 -4.2531195 -4.2522507 -4.2394361 -4.2326617 -4.2378545 -4.2419124 -4.2368217 -4.2331753]]...]
INFO - root - 2017-12-06 07:40:13.193134: step 15410, loss = 2.05, batch loss = 1.99 (35.7 examples/sec; 0.224 sec/batch; 19h:44m:54s remains)
INFO - root - 2017-12-06 07:40:15.464894: step 15420, loss = 2.04, batch loss = 1.99 (35.7 examples/sec; 0.224 sec/batch; 19h:43m:53s remains)
INFO - root - 2017-12-06 07:40:17.747153: step 15430, loss = 2.09, batch loss = 2.03 (34.3 examples/sec; 0.233 sec/batch; 20h:32m:43s remains)
INFO - root - 2017-12-06 07:40:19.977528: step 15440, loss = 2.08, batch loss = 2.02 (35.6 examples/sec; 0.224 sec/batch; 19h:45m:51s remains)
INFO - root - 2017-12-06 07:40:22.261983: step 15450, loss = 2.04, batch loss = 1.99 (36.5 examples/sec; 0.219 sec/batch; 19h:17m:02s remains)
INFO - root - 2017-12-06 07:40:24.536149: step 15460, loss = 2.07, batch loss = 2.01 (34.5 examples/sec; 0.232 sec/batch; 20h:24m:11s remains)
INFO - root - 2017-12-06 07:40:26.789303: step 15470, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 20h:04m:27s remains)
INFO - root - 2017-12-06 07:40:29.134112: step 15480, loss = 2.07, batch loss = 2.01 (33.1 examples/sec; 0.242 sec/batch; 21h:18m:19s remains)
INFO - root - 2017-12-06 07:40:31.441603: step 15490, loss = 2.09, batch loss = 2.03 (35.9 examples/sec; 0.223 sec/batch; 19h:38m:49s remains)
INFO - root - 2017-12-06 07:40:33.721179: step 15500, loss = 2.07, batch loss = 2.01 (34.9 examples/sec; 0.229 sec/batch; 20h:10m:09s remains)
2017-12-06 07:40:34.067594: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2036424 -4.1980462 -4.192461 -4.1863623 -4.1782703 -4.1702571 -4.1848569 -4.2032113 -4.2175851 -4.2344284 -4.2441626 -4.240418 -4.2355046 -4.2336283 -4.2226176][-4.1524515 -4.1459451 -4.145339 -4.1579232 -4.1638656 -4.1688619 -4.1939178 -4.2124019 -4.2148461 -4.2269187 -4.2405195 -4.2345839 -4.22229 -4.2153597 -4.2046332][-4.1194158 -4.1153297 -4.1342125 -4.1730318 -4.1957617 -4.2149272 -4.2403846 -4.254384 -4.24339 -4.2415652 -4.2532964 -4.2475352 -4.2308903 -4.2183328 -4.2045097][-4.1189671 -4.1260071 -4.1679354 -4.2185397 -4.2460961 -4.2676382 -4.2861667 -4.290318 -4.2679853 -4.2535119 -4.2614403 -4.2582655 -4.2413893 -4.22752 -4.2092609][-4.1722136 -4.1906672 -4.2355013 -4.2726226 -4.2885036 -4.2955432 -4.2942276 -4.2805037 -4.252615 -4.2343287 -4.2376723 -4.2380848 -4.2273307 -4.218061 -4.1994152][-4.2461147 -4.2631607 -4.2909923 -4.3027148 -4.2949605 -4.2720132 -4.236413 -4.2076049 -4.1966939 -4.188725 -4.1945968 -4.2033319 -4.1971149 -4.1940918 -4.18958][-4.2883244 -4.2917633 -4.3010883 -4.2903633 -4.2588916 -4.2030544 -4.1224461 -4.0806527 -4.0980854 -4.1052556 -4.1212749 -4.1542139 -4.1677732 -4.1808071 -4.2073307][-4.2996483 -4.2879243 -4.2756114 -4.2448845 -4.191957 -4.0994568 -3.9626112 -3.9064479 -3.968575 -4.0106192 -4.0490527 -4.1108308 -4.1526022 -4.1930089 -4.2439361][-4.3052711 -4.2819481 -4.2435112 -4.1873064 -4.1171532 -4.0109253 -3.8569472 -3.80315 -3.9046302 -3.9816787 -4.04239 -4.1201539 -4.176981 -4.2248878 -4.2761116][-4.3093362 -4.2798991 -4.2274833 -4.1588659 -4.0917478 -4.0147843 -3.9140663 -3.8929434 -3.9819915 -4.0559545 -4.1181951 -4.1901 -4.24318 -4.2810116 -4.3138156][-4.3102193 -4.2768483 -4.2305627 -4.1744885 -4.1254063 -4.0824738 -4.0419707 -4.0557981 -4.1228828 -4.1780233 -4.2238503 -4.2748065 -4.309124 -4.3271117 -4.3404255][-4.3007994 -4.2591925 -4.2173195 -4.1806693 -4.1488996 -4.1310916 -4.1347661 -4.1751671 -4.2240877 -4.255753 -4.2803884 -4.3113236 -4.3278232 -4.3330927 -4.3389158][-4.2712078 -4.2163644 -4.1683121 -4.1409883 -4.1281152 -4.1355767 -4.1715894 -4.2286782 -4.2689295 -4.2854371 -4.2935143 -4.3089767 -4.3169684 -4.3153644 -4.3163462][-4.2208076 -4.1545448 -4.0943131 -4.0704641 -4.0837865 -4.121273 -4.1774459 -4.2390189 -4.2734632 -4.2786036 -4.2764688 -4.2791967 -4.2779379 -4.2723842 -4.271327][-4.1727467 -4.1097708 -4.0551734 -4.0430317 -4.079133 -4.1319928 -4.1857319 -4.23171 -4.2511878 -4.2452312 -4.2338796 -4.2259007 -4.2153711 -4.2121854 -4.2149076]]...]
INFO - root - 2017-12-06 07:40:36.311760: step 15510, loss = 2.05, batch loss = 1.99 (36.1 examples/sec; 0.222 sec/batch; 19h:30m:42s remains)
INFO - root - 2017-12-06 07:40:38.585653: step 15520, loss = 2.06, batch loss = 2.00 (34.7 examples/sec; 0.231 sec/batch; 20h:19m:14s remains)
INFO - root - 2017-12-06 07:40:40.842534: step 15530, loss = 2.07, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 19h:35m:35s remains)
INFO - root - 2017-12-06 07:40:43.132041: step 15540, loss = 2.08, batch loss = 2.03 (35.4 examples/sec; 0.226 sec/batch; 19h:54m:31s remains)
INFO - root - 2017-12-06 07:40:45.422334: step 15550, loss = 2.06, batch loss = 2.01 (34.6 examples/sec; 0.231 sec/batch; 20h:21m:52s remains)
INFO - root - 2017-12-06 07:40:47.715758: step 15560, loss = 2.08, batch loss = 2.03 (35.3 examples/sec; 0.226 sec/batch; 19h:56m:00s remains)
INFO - root - 2017-12-06 07:40:50.000986: step 15570, loss = 2.07, batch loss = 2.01 (34.2 examples/sec; 0.234 sec/batch; 20h:34m:06s remains)
INFO - root - 2017-12-06 07:40:52.291652: step 15580, loss = 2.03, batch loss = 1.97 (34.6 examples/sec; 0.232 sec/batch; 20h:22m:54s remains)
INFO - root - 2017-12-06 07:40:54.599903: step 15590, loss = 2.09, batch loss = 2.04 (34.4 examples/sec; 0.233 sec/batch; 20h:29m:45s remains)
INFO - root - 2017-12-06 07:40:56.866914: step 15600, loss = 2.05, batch loss = 1.99 (36.4 examples/sec; 0.220 sec/batch; 19h:20m:47s remains)
2017-12-06 07:40:57.230619: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2524023 -4.2512231 -4.2610307 -4.2640467 -4.2671571 -4.2665682 -4.2610617 -4.2452259 -4.2263422 -4.2228131 -4.2186012 -4.2084322 -4.1902905 -4.1796718 -4.1821451][-4.2556291 -4.2594094 -4.2762671 -4.284162 -4.2881069 -4.2785912 -4.2580791 -4.2263956 -4.1946211 -4.1848845 -4.1852379 -4.1846404 -4.1782346 -4.16831 -4.16822][-4.2353358 -4.2498112 -4.2795777 -4.2989688 -4.3032541 -4.2799935 -4.2419677 -4.1930261 -4.152421 -4.1431313 -4.151525 -4.1618905 -4.1687565 -4.1645517 -4.1585665][-4.1807351 -4.2097135 -4.2546606 -4.2854424 -4.28989 -4.2579732 -4.2094383 -4.1538882 -4.1159735 -4.1084261 -4.1157451 -4.1253238 -4.1375656 -4.1380496 -4.1251106][-4.1004195 -4.1482983 -4.2039285 -4.2355552 -4.2302885 -4.1868672 -4.1334071 -4.0849814 -4.0626264 -4.066906 -4.07551 -4.0805016 -4.0915027 -4.0930557 -4.0779138][-4.043119 -4.0972781 -4.1484485 -4.1655922 -4.1397538 -4.0730376 -4.0073071 -3.9661155 -3.9632781 -3.9818025 -3.9983752 -4.0139713 -4.038363 -4.0551105 -4.0609274][-4.0402837 -4.0824747 -4.1113219 -4.1027408 -4.05009 -3.9598229 -3.8743527 -3.83244 -3.8479345 -3.8934669 -3.9408541 -3.9913101 -4.0430951 -4.0777287 -4.1028118][-4.0847635 -4.1001968 -4.1034307 -4.0756059 -4.00902 -3.914506 -3.8294525 -3.7971497 -3.831054 -3.9033585 -3.9796598 -4.0557 -4.12112 -4.1569071 -4.1783385][-4.1530814 -4.1517754 -4.1437006 -4.1096187 -4.048738 -3.9746411 -3.9117231 -3.8974462 -3.9428158 -4.0235882 -4.0993485 -4.1649923 -4.2146454 -4.2324872 -4.2360458][-4.2138495 -4.2079129 -4.20163 -4.1734929 -4.1271677 -4.0788851 -4.0424943 -4.0429778 -4.0870805 -4.1569057 -4.2120638 -4.2486167 -4.2732911 -4.27237 -4.2571774][-4.2561541 -4.255475 -4.2562551 -4.2393508 -4.2100549 -4.1830263 -4.1622453 -4.1623783 -4.193078 -4.2407651 -4.2712626 -4.2837706 -4.2825937 -4.2661095 -4.2424722][-4.2706041 -4.2768836 -4.2826042 -4.27513 -4.2592735 -4.2467747 -4.2340565 -4.2296395 -4.2441912 -4.2656374 -4.2723718 -4.2652245 -4.2431459 -4.2150631 -4.1814246][-4.2678795 -4.2766342 -4.2845936 -4.2842879 -4.27541 -4.2646427 -4.2503715 -4.24173 -4.2411089 -4.2368255 -4.2185779 -4.188293 -4.1496606 -4.1125474 -4.0766129][-4.2797775 -4.2862945 -4.2927089 -4.2934761 -4.284668 -4.269165 -4.2477736 -4.233007 -4.2204981 -4.1948442 -4.1536961 -4.1030498 -4.0586362 -4.0261412 -3.9969389][-4.3079615 -4.3111067 -4.3140092 -4.3129449 -4.3030772 -4.2835569 -4.2540784 -4.2324572 -4.2146988 -4.180459 -4.1302195 -4.0820007 -4.0488648 -4.0233355 -4.0016851]]...]
INFO - root - 2017-12-06 07:40:59.483188: step 15610, loss = 2.09, batch loss = 2.03 (35.4 examples/sec; 0.226 sec/batch; 19h:52m:08s remains)
INFO - root - 2017-12-06 07:41:01.744185: step 15620, loss = 2.07, batch loss = 2.01 (36.2 examples/sec; 0.221 sec/batch; 19h:25m:58s remains)
INFO - root - 2017-12-06 07:41:04.047200: step 15630, loss = 2.07, batch loss = 2.01 (35.5 examples/sec; 0.225 sec/batch; 19h:49m:57s remains)
INFO - root - 2017-12-06 07:41:06.356173: step 15640, loss = 2.08, batch loss = 2.02 (35.4 examples/sec; 0.226 sec/batch; 19h:54m:50s remains)
INFO - root - 2017-12-06 07:41:08.616094: step 15650, loss = 2.09, batch loss = 2.03 (35.7 examples/sec; 0.224 sec/batch; 19h:44m:04s remains)
INFO - root - 2017-12-06 07:41:10.878523: step 15660, loss = 2.08, batch loss = 2.02 (34.7 examples/sec; 0.230 sec/batch; 20h:16m:51s remains)
INFO - root - 2017-12-06 07:41:13.133871: step 15670, loss = 2.08, batch loss = 2.02 (36.0 examples/sec; 0.223 sec/batch; 19h:34m:57s remains)
INFO - root - 2017-12-06 07:41:15.441000: step 15680, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.225 sec/batch; 19h:46m:36s remains)
INFO - root - 2017-12-06 07:41:17.700769: step 15690, loss = 2.07, batch loss = 2.02 (35.3 examples/sec; 0.226 sec/batch; 19h:55m:36s remains)
INFO - root - 2017-12-06 07:41:20.009664: step 15700, loss = 2.06, batch loss = 2.00 (34.6 examples/sec; 0.231 sec/batch; 20h:20m:15s remains)
2017-12-06 07:41:20.398683: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.344089 -4.3430061 -4.3422351 -4.340066 -4.3368621 -4.3285985 -4.317873 -4.3091335 -4.3044534 -4.3022113 -4.3044791 -4.3086162 -4.312326 -4.31925 -4.327455][-4.3423934 -4.3423176 -4.3407483 -4.3342137 -4.3262858 -4.3106713 -4.2912283 -4.2785559 -4.2733359 -4.2705951 -4.2726398 -4.277986 -4.2849412 -4.2956529 -4.30738][-4.3383079 -4.3369031 -4.3307242 -4.3164 -4.298389 -4.27514 -4.2518005 -4.2390189 -4.2383933 -4.2350264 -4.2333121 -4.2373481 -4.2489071 -4.265749 -4.2798648][-4.3273931 -4.3175993 -4.3021441 -4.2784252 -4.2476645 -4.2172971 -4.1931019 -4.1841617 -4.1951766 -4.1960678 -4.1926026 -4.1969538 -4.2117267 -4.231977 -4.2487936][-4.314095 -4.292172 -4.2657986 -4.2334566 -4.1888556 -4.1458073 -4.1076064 -4.100687 -4.1366534 -4.1590319 -4.1661539 -4.1715436 -4.1859751 -4.2044811 -4.2202463][-4.296464 -4.2665162 -4.2315388 -4.1886005 -4.1231465 -4.0527387 -3.979924 -3.9699426 -4.0399184 -4.0956945 -4.1259084 -4.1389489 -4.157176 -4.1751528 -4.1921039][-4.2713456 -4.2333984 -4.1893129 -4.13205 -4.0382032 -3.9210725 -3.792419 -3.7783782 -3.8928025 -3.9903982 -4.0535593 -4.0863228 -4.1156678 -4.1402054 -4.1627212][-4.2339025 -4.1880808 -4.1394506 -4.076417 -3.9689081 -3.8166184 -3.6370072 -3.6165056 -3.7681243 -3.8973293 -3.9836512 -4.0367379 -4.0807033 -4.1143303 -4.14238][-4.1964054 -4.1463189 -4.1009312 -4.0512357 -3.9706104 -3.8510418 -3.7024455 -3.6764264 -3.7908051 -3.9003053 -3.9752352 -4.0292873 -4.0767288 -4.1165304 -4.147934][-4.166388 -4.1180415 -4.084959 -4.0610676 -4.026979 -3.9651308 -3.880203 -3.863143 -3.9243636 -3.9876673 -4.0332875 -4.071888 -4.1102185 -4.1474442 -4.1770072][-4.1493664 -4.1046829 -4.0828032 -4.084043 -4.0864525 -4.0664763 -4.0280466 -4.0214911 -4.049777 -4.0820885 -4.1052523 -4.1271534 -4.1585007 -4.1917953 -4.2197766][-4.1592512 -4.123704 -4.1133747 -4.1290488 -4.1462092 -4.1460433 -4.136519 -4.1371932 -4.149168 -4.166996 -4.1819324 -4.1952529 -4.2171607 -4.2436962 -4.268683][-4.2038822 -4.1826344 -4.1820068 -4.1988826 -4.2156343 -4.21894 -4.2175407 -4.22106 -4.2258148 -4.2363935 -4.2490773 -4.2595615 -4.2757096 -4.2950354 -4.3125134][-4.2587686 -4.2470131 -4.2497811 -4.2643433 -4.2773948 -4.2815366 -4.2822819 -4.2856684 -4.28639 -4.2914042 -4.2996626 -4.30872 -4.3206592 -4.333519 -4.343801][-4.3006425 -4.2948279 -4.2991176 -4.3097034 -4.31774 -4.3209119 -4.3217649 -4.3242769 -4.3239093 -4.3256817 -4.3303781 -4.3363991 -4.343266 -4.3507075 -4.3559656]]...]
INFO - root - 2017-12-06 07:41:22.716284: step 15710, loss = 2.05, batch loss = 1.99 (35.9 examples/sec; 0.223 sec/batch; 19h:36m:08s remains)
INFO - root - 2017-12-06 07:41:24.956394: step 15720, loss = 2.08, batch loss = 2.03 (36.7 examples/sec; 0.218 sec/batch; 19h:11m:30s remains)
INFO - root - 2017-12-06 07:41:27.225806: step 15730, loss = 2.08, batch loss = 2.02 (34.9 examples/sec; 0.229 sec/batch; 20h:11m:28s remains)
INFO - root - 2017-12-06 07:41:29.479031: step 15740, loss = 2.05, batch loss = 1.99 (34.8 examples/sec; 0.230 sec/batch; 20h:12m:22s remains)
INFO - root - 2017-12-06 07:41:31.791649: step 15750, loss = 2.06, batch loss = 2.00 (34.3 examples/sec; 0.233 sec/batch; 20h:29m:40s remains)
INFO - root - 2017-12-06 07:41:34.105963: step 15760, loss = 2.06, batch loss = 2.00 (36.4 examples/sec; 0.220 sec/batch; 19h:21m:03s remains)
INFO - root - 2017-12-06 07:41:36.346049: step 15770, loss = 2.04, batch loss = 1.99 (37.3 examples/sec; 0.215 sec/batch; 18h:53m:42s remains)
INFO - root - 2017-12-06 07:41:38.627957: step 15780, loss = 2.07, batch loss = 2.02 (35.6 examples/sec; 0.225 sec/batch; 19h:47m:07s remains)
INFO - root - 2017-12-06 07:41:40.944419: step 15790, loss = 2.08, batch loss = 2.02 (35.1 examples/sec; 0.228 sec/batch; 20h:03m:01s remains)
INFO - root - 2017-12-06 07:41:43.208012: step 15800, loss = 2.08, batch loss = 2.03 (36.0 examples/sec; 0.222 sec/batch; 19h:31m:46s remains)
2017-12-06 07:41:43.570921: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2263932 -4.2204475 -4.21407 -4.1962056 -4.1823173 -4.1815062 -4.2030668 -4.2229385 -4.211019 -4.1966214 -4.1925697 -4.1987543 -4.2111559 -4.2215595 -4.2260275][-4.21501 -4.202395 -4.1937566 -4.1786251 -4.1669397 -4.1613045 -4.1750946 -4.1921582 -4.1793623 -4.15694 -4.1417694 -4.14511 -4.1662426 -4.1885662 -4.2010579][-4.2154064 -4.1943989 -4.1806297 -4.1664696 -4.16041 -4.1563935 -4.1606951 -4.16674 -4.1517496 -4.1242785 -4.0989332 -4.0967922 -4.1264257 -4.1612167 -4.1818132][-4.2190771 -4.1895685 -4.1710997 -4.1584339 -4.1569071 -4.1551285 -4.1553555 -4.1551747 -4.1385555 -4.1087284 -4.0747161 -4.06609 -4.0987654 -4.1420269 -4.1661134][-4.20784 -4.1712375 -4.1492786 -4.1352196 -4.1294374 -4.1272321 -4.1319976 -4.1362729 -4.1201525 -4.0934858 -4.0596371 -4.04908 -4.0856676 -4.1333585 -4.1594553][-4.1793723 -4.128788 -4.0931931 -4.0703311 -4.0503345 -4.038456 -4.04673 -4.0666 -4.0646276 -4.0505266 -4.0221176 -4.015419 -4.0602841 -4.1108141 -4.1388049][-4.1248727 -4.0605192 -4.0081773 -3.9694076 -3.9304464 -3.8977914 -3.8940775 -3.9235671 -3.9317074 -3.9284818 -3.9099565 -3.9155686 -3.9832146 -4.0555096 -4.0979972][-4.0707989 -4.0022988 -3.9467242 -3.8963382 -3.8422608 -3.7809711 -3.7436597 -3.7655997 -3.776969 -3.7887008 -3.7920542 -3.8166008 -3.9040828 -3.9987898 -4.0662656][-4.06942 -4.01256 -3.9685163 -3.9264793 -3.8830333 -3.8180833 -3.765686 -3.7767527 -3.7777417 -3.7790623 -3.7844341 -3.8101647 -3.8974936 -3.9991078 -4.0798821][-4.1251426 -4.090055 -4.0663352 -4.0453949 -4.0255723 -3.9782717 -3.9335122 -3.944242 -3.9472065 -3.9422588 -3.9427283 -3.9558902 -4.0160961 -4.0889506 -4.1543169][-4.2029457 -4.1850405 -4.1768394 -4.17254 -4.1708665 -4.1451874 -4.1157742 -4.1250424 -4.1286616 -4.1265192 -4.1280909 -4.13708 -4.1721153 -4.2108088 -4.2446671][-4.2591624 -4.2521925 -4.2521195 -4.2544217 -4.2599053 -4.2471085 -4.231092 -4.239399 -4.2454677 -4.2489362 -4.2510996 -4.2563496 -4.2739439 -4.2877936 -4.3001447][-4.2972751 -4.2964845 -4.2990975 -4.301548 -4.3045278 -4.2974963 -4.2891026 -4.2942958 -4.29922 -4.3037896 -4.3061852 -4.31153 -4.3190742 -4.3215876 -4.3238721][-4.3199987 -4.3209248 -4.3233719 -4.3241715 -4.3241472 -4.3190055 -4.3137608 -4.3152461 -4.3184371 -4.3215976 -4.3242049 -4.3288236 -4.3330393 -4.3343639 -4.3342][-4.3327713 -4.3318853 -4.3312688 -4.3308454 -4.3301325 -4.3286467 -4.3269262 -4.3272414 -4.3285046 -4.3296962 -4.3309255 -4.3327088 -4.3361454 -4.3381796 -4.3397765]]...]
INFO - root - 2017-12-06 07:41:45.836063: step 15810, loss = 2.07, batch loss = 2.01 (34.6 examples/sec; 0.231 sec/batch; 20h:21m:32s remains)
INFO - root - 2017-12-06 07:41:48.069238: step 15820, loss = 2.07, batch loss = 2.01 (35.8 examples/sec; 0.223 sec/batch; 19h:37m:50s remains)
INFO - root - 2017-12-06 07:41:50.302763: step 15830, loss = 2.05, batch loss = 2.00 (35.8 examples/sec; 0.224 sec/batch; 19h:39m:55s remains)
INFO - root - 2017-12-06 07:41:52.556340: step 15840, loss = 2.07, batch loss = 2.01 (35.8 examples/sec; 0.223 sec/batch; 19h:37m:47s remains)
INFO - root - 2017-12-06 07:41:54.821983: step 15850, loss = 2.04, batch loss = 1.99 (35.0 examples/sec; 0.229 sec/batch; 20h:06m:52s remains)
INFO - root - 2017-12-06 07:41:57.158395: step 15860, loss = 2.05, batch loss = 1.99 (35.6 examples/sec; 0.225 sec/batch; 19h:45m:40s remains)
INFO - root - 2017-12-06 07:41:59.415050: step 15870, loss = 2.08, batch loss = 2.02 (34.5 examples/sec; 0.232 sec/batch; 20h:24m:04s remains)
INFO - root - 2017-12-06 07:42:01.674259: step 15880, loss = 2.08, batch loss = 2.02 (34.2 examples/sec; 0.234 sec/batch; 20h:34m:00s remains)
INFO - root - 2017-12-06 07:42:03.945999: step 15890, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 20h:02m:11s remains)
INFO - root - 2017-12-06 07:42:06.208958: step 15900, loss = 2.08, batch loss = 2.02 (35.4 examples/sec; 0.226 sec/batch; 19h:52m:48s remains)
2017-12-06 07:42:06.587193: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2654748 -4.2644439 -4.2598968 -4.2526259 -4.2368979 -4.2210555 -4.2126937 -4.2157555 -4.2027187 -4.1999016 -4.2112732 -4.2153096 -4.1967549 -4.1698985 -4.1466184][-4.295073 -4.3051105 -4.3066692 -4.2996655 -4.2864184 -4.2720342 -4.2599831 -4.2564588 -4.2429614 -4.2414088 -4.2552719 -4.2660265 -4.2509289 -4.2230892 -4.1934133][-4.3108172 -4.3253708 -4.3289719 -4.3203335 -4.307332 -4.2946353 -4.278739 -4.2663617 -4.2525606 -4.25481 -4.2718415 -4.2865105 -4.279614 -4.2569742 -4.2280259][-4.3153338 -4.3322158 -4.3388085 -4.3345847 -4.3209562 -4.3026543 -4.2775526 -4.2478876 -4.22562 -4.2272792 -4.2451358 -4.2633715 -4.2609639 -4.2424383 -4.2206278][-4.3116407 -4.3293424 -4.3340425 -4.324666 -4.3031039 -4.2728238 -4.2347589 -4.1873794 -4.1564565 -4.1623139 -4.1909919 -4.2119441 -4.2045159 -4.1841764 -4.16884][-4.3059278 -4.3164678 -4.3122525 -4.29221 -4.2566729 -4.2128696 -4.16404 -4.0981321 -4.0512295 -4.060101 -4.1022224 -4.127512 -4.1244407 -4.1157207 -4.1146874][-4.293838 -4.2884107 -4.266819 -4.2316136 -4.1805706 -4.125484 -4.070488 -3.994777 -3.9394794 -3.9537671 -4.01619 -4.057898 -4.0716681 -4.083385 -4.0937705][-4.2623129 -4.239697 -4.2022681 -4.153687 -4.0920706 -4.0326152 -3.9834034 -3.9252608 -3.8870757 -3.9143937 -3.9895525 -4.0405469 -4.059329 -4.0723453 -4.0802951][-4.2230844 -4.1919732 -4.1493373 -4.10015 -4.0455866 -3.9939451 -3.9624324 -3.9423447 -3.9410751 -3.9741888 -4.0373459 -4.0787563 -4.0866451 -4.0862951 -4.0837483][-4.2070565 -4.1726761 -4.1318083 -4.0936704 -4.06015 -4.0269647 -4.0166574 -4.0251307 -4.0467854 -4.0765619 -4.1183548 -4.1437263 -4.1402359 -4.1276636 -4.1151953][-4.2394342 -4.2142258 -4.1830559 -4.1568308 -4.1388874 -4.11958 -4.1204128 -4.1352124 -4.1557121 -4.1752524 -4.1987453 -4.2113123 -4.2040238 -4.1890569 -4.17581][-4.2897258 -4.27784 -4.26313 -4.2509055 -4.243988 -4.2352419 -4.2374425 -4.2432466 -4.2530437 -4.2629905 -4.272625 -4.2767434 -4.2702217 -4.2588296 -4.2481437][-4.3228407 -4.3182611 -4.3137608 -4.3110995 -4.3111973 -4.3098011 -4.3117809 -4.3116012 -4.3134561 -4.3174963 -4.3190832 -4.3156815 -4.309258 -4.3012533 -4.2939591][-4.3382487 -4.3355565 -4.3332868 -4.3323383 -4.333209 -4.3326626 -4.3327069 -4.3321662 -4.3325138 -4.3327994 -4.3307137 -4.3260765 -4.3214316 -4.3179502 -4.3154516][-4.3410406 -4.3396459 -4.3374753 -4.3353829 -4.3344717 -4.3340635 -4.3341131 -4.333889 -4.3327737 -4.3308072 -4.3287287 -4.3271289 -4.3258977 -4.3254786 -4.3255763]]...]
INFO - root - 2017-12-06 07:42:08.874711: step 15910, loss = 2.10, batch loss = 2.04 (33.9 examples/sec; 0.236 sec/batch; 20h:45m:30s remains)
INFO - root - 2017-12-06 07:42:11.121944: step 15920, loss = 2.06, batch loss = 2.00 (35.8 examples/sec; 0.224 sec/batch; 19h:39m:55s remains)
INFO - root - 2017-12-06 07:42:13.380669: step 15930, loss = 2.06, batch loss = 2.01 (34.4 examples/sec; 0.232 sec/batch; 20h:25m:34s remains)
INFO - root - 2017-12-06 07:42:15.613954: step 15940, loss = 2.08, batch loss = 2.03 (33.9 examples/sec; 0.236 sec/batch; 20h:44m:34s remains)
INFO - root - 2017-12-06 07:42:17.865801: step 15950, loss = 2.05, batch loss = 1.99 (36.9 examples/sec; 0.217 sec/batch; 19h:02m:19s remains)
INFO - root - 2017-12-06 07:42:20.126204: step 15960, loss = 2.08, batch loss = 2.02 (34.9 examples/sec; 0.229 sec/batch; 20h:08m:29s remains)
INFO - root - 2017-12-06 07:42:22.412944: step 15970, loss = 2.05, batch loss = 2.00 (34.7 examples/sec; 0.231 sec/batch; 20h:17m:59s remains)
INFO - root - 2017-12-06 07:42:24.657600: step 15980, loss = 2.06, batch loss = 2.00 (36.6 examples/sec; 0.219 sec/batch; 19h:13m:58s remains)
INFO - root - 2017-12-06 07:42:26.928935: step 15990, loss = 2.08, batch loss = 2.02 (35.2 examples/sec; 0.227 sec/batch; 19h:59m:39s remains)
INFO - root - 2017-12-06 07:42:29.208991: step 16000, loss = 2.06, batch loss = 2.00 (34.8 examples/sec; 0.230 sec/batch; 20h:14m:22s remains)
2017-12-06 07:42:29.639538: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2107024 -4.1925 -4.1873212 -4.1953373 -4.2187891 -4.2342124 -4.2351384 -4.2361474 -4.2438149 -4.2578669 -4.2741604 -4.2570672 -4.2319131 -4.2216105 -4.2222962][-4.1856513 -4.1672559 -4.1661997 -4.1825557 -4.2166319 -4.2377539 -4.2393889 -4.241292 -4.25181 -4.2625694 -4.2753911 -4.258379 -4.2309718 -4.218832 -4.2258911][-4.1808553 -4.1696997 -4.1727605 -4.1954737 -4.2323041 -4.2499623 -4.2489395 -4.2459369 -4.2523332 -4.2605591 -4.2645059 -4.2438269 -4.2127619 -4.1932878 -4.2001605][-4.208818 -4.2036877 -4.2108617 -4.2380137 -4.2736444 -4.2801394 -4.2651086 -4.2413282 -4.2275691 -4.227387 -4.229362 -4.2169037 -4.1923823 -4.1678076 -4.1636877][-4.2234378 -4.2205043 -4.2349091 -4.2685094 -4.2967415 -4.2849154 -4.2452235 -4.1907606 -4.1485429 -4.1400743 -4.1555829 -4.1668715 -4.1614037 -4.1412468 -4.1223826][-4.2072711 -4.2014866 -4.2189941 -4.2580686 -4.2779393 -4.2466664 -4.1767011 -4.084497 -4.0075736 -3.9959004 -4.0443277 -4.0910058 -4.1081467 -4.1001158 -4.0783796][-4.1532273 -4.1320486 -4.1468191 -4.1935749 -4.2153306 -4.1744947 -4.081408 -3.9592073 -3.8537345 -3.8478136 -3.9391077 -4.0218639 -4.0575714 -4.0566969 -4.0356903][-4.056623 -4.0138597 -4.02804 -4.0915656 -4.13397 -4.1031828 -4.0112362 -3.8892119 -3.7876048 -3.7988925 -3.9095664 -4.002769 -4.0436087 -4.04249 -4.0203757][-3.9691083 -3.9081256 -3.9184167 -3.9971976 -4.0688219 -4.0699224 -4.0117173 -3.9289575 -3.8668017 -3.8934522 -3.9792583 -4.0458045 -4.0765419 -4.0729017 -4.0523453][-3.9790952 -3.9229114 -3.927489 -3.9929974 -4.0677156 -4.0951438 -4.0736923 -4.0302272 -4.0026345 -4.0305638 -4.0879788 -4.1316614 -4.1523151 -4.1456933 -4.1258588][-4.0673165 -4.0357609 -4.0370235 -4.0717554 -4.11613 -4.1411538 -4.1384821 -4.1206121 -4.1165438 -4.1476321 -4.1914635 -4.220654 -4.233305 -4.2208309 -4.1980672][-4.1462831 -4.1338654 -4.132349 -4.1410975 -4.1501565 -4.1599193 -4.1665835 -4.1657624 -4.1776981 -4.2124424 -4.2480688 -4.2674856 -4.2733932 -4.2596436 -4.238575][-4.1915979 -4.1820426 -4.1778989 -4.1729 -4.16291 -4.1629376 -4.1756744 -4.1852293 -4.2022758 -4.232193 -4.2581162 -4.2700768 -4.2725663 -4.2631273 -4.2485142][-4.20548 -4.1944547 -4.1903276 -4.1842027 -4.1760015 -4.1803646 -4.197474 -4.2097244 -4.2207394 -4.2358203 -4.2470922 -4.2507963 -4.2503543 -4.2444439 -4.2334909][-4.2054572 -4.1939111 -4.1913304 -4.192821 -4.1977658 -4.2123013 -4.2281632 -4.2348442 -4.2376451 -4.2380185 -4.2341714 -4.2297573 -4.2282467 -4.222827 -4.2112708]]...]
INFO - root - 2017-12-06 07:42:31.930736: step 16010, loss = 2.06, batch loss = 2.00 (34.3 examples/sec; 0.233 sec/batch; 20h:28m:31s remains)
INFO - root - 2017-12-06 07:42:34.217166: step 16020, loss = 2.06, batch loss = 2.00 (35.0 examples/sec; 0.229 sec/batch; 20h:06m:36s remains)
INFO - root - 2017-12-06 07:42:36.497445: step 16030, loss = 2.06, batch loss = 2.00 (34.7 examples/sec; 0.230 sec/batch; 20h:15m:09s remains)
INFO - root - 2017-12-06 07:42:38.772824: step 16040, loss = 2.07, batch loss = 2.01 (35.7 examples/sec; 0.224 sec/batch; 19h:43m:25s remains)
INFO - root - 2017-12-06 07:42:41.011799: step 16050, loss = 2.07, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 19h:36m:43s remains)
INFO - root - 2017-12-06 07:42:43.278546: step 16060, loss = 2.08, batch loss = 2.02 (35.3 examples/sec; 0.227 sec/batch; 19h:54m:36s remains)
INFO - root - 2017-12-06 07:42:45.554513: step 16070, loss = 2.06, batch loss = 2.01 (34.2 examples/sec; 0.234 sec/batch; 20h:35m:23s remains)
INFO - root - 2017-12-06 07:42:47.853429: step 16080, loss = 2.07, batch loss = 2.01 (35.2 examples/sec; 0.227 sec/batch; 19h:58m:43s remains)
INFO - root - 2017-12-06 07:42:50.206729: step 16090, loss = 2.09, batch loss = 2.03 (34.1 examples/sec; 0.235 sec/batch; 20h:37m:29s remains)
INFO - root - 2017-12-06 07:42:52.508634: step 16100, loss = 2.04, batch loss = 1.98 (35.0 examples/sec; 0.229 sec/batch; 20h:05m:05s remains)
2017-12-06 07:42:52.915312: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2667136 -4.2664471 -4.2602825 -4.2534347 -4.2462554 -4.2470732 -4.2586746 -4.2704906 -4.2755194 -4.2745819 -4.2741151 -4.2784576 -4.2759757 -4.266572 -4.2592154][-4.2322221 -4.2358 -4.2297082 -4.2209153 -4.2101297 -4.2114024 -4.2267356 -4.2409239 -4.248282 -4.2476115 -4.2462983 -4.2538977 -4.2558784 -4.2472539 -4.2369137][-4.1908641 -4.1966224 -4.1904473 -4.1792421 -4.1671009 -4.1719017 -4.193398 -4.2095551 -4.2128677 -4.206511 -4.2010722 -4.2131739 -4.2249861 -4.22565 -4.21869][-4.14256 -4.1479974 -4.1474943 -4.140099 -4.130928 -4.1399441 -4.1636782 -4.1774311 -4.172359 -4.1571827 -4.1487656 -4.1658888 -4.1900687 -4.2023511 -4.20441][-4.095953 -4.1006703 -4.10863 -4.1103625 -4.1075168 -4.1130819 -4.1213331 -4.1170621 -4.1066375 -4.1009665 -4.1025271 -4.1254487 -4.1553936 -4.1741519 -4.1875792][-4.0588584 -4.0631905 -4.0783334 -4.0900393 -4.09308 -4.0838275 -4.0552382 -4.0109835 -4.0013046 -4.0401053 -4.0804935 -4.1154137 -4.1414266 -4.1577711 -4.1719885][-4.0582747 -4.0676231 -4.086041 -4.0982924 -4.0971513 -4.0668163 -3.9881206 -3.8894398 -3.8827276 -3.9863932 -4.0759978 -4.1230192 -4.1464105 -4.1588683 -4.16726][-4.0985589 -4.1155362 -4.1296244 -4.1260777 -4.10431 -4.0600019 -3.9554994 -3.8396115 -3.8517275 -3.9878254 -4.0916791 -4.141541 -4.1672955 -4.1754079 -4.1714606][-4.1488438 -4.1708465 -4.1802163 -4.1551204 -4.1068192 -4.0584421 -3.9833498 -3.9222734 -3.9589529 -4.0612369 -4.1289916 -4.159915 -4.1830068 -4.1867065 -4.1757455][-4.1896963 -4.2071486 -4.2065344 -4.1619983 -4.0884809 -4.0419049 -4.0206146 -4.0277305 -4.0771356 -4.1391144 -4.1698217 -4.1770411 -4.1913486 -4.194808 -4.1889811][-4.2200236 -4.22673 -4.2158394 -4.1626596 -4.0809278 -4.0404568 -4.0557833 -4.1011219 -4.150609 -4.1843 -4.1900635 -4.1823783 -4.1907115 -4.200058 -4.2071896][-4.23769 -4.2345581 -4.2238493 -4.1845655 -4.122026 -4.0966177 -4.1196556 -4.16214 -4.195179 -4.2033319 -4.1913147 -4.1742644 -4.1788468 -4.1969752 -4.2198849][-4.2388916 -4.2331963 -4.2310772 -4.2175288 -4.190618 -4.1828704 -4.198452 -4.2220383 -4.2346425 -4.225563 -4.2034264 -4.1835918 -4.1862521 -4.2078443 -4.2363124][-4.2358871 -4.2305241 -4.2371345 -4.2453794 -4.2455869 -4.2485433 -4.2567091 -4.2656884 -4.2675657 -4.2558632 -4.2362304 -4.2182446 -4.218194 -4.2351494 -4.2578125][-4.2365918 -4.2278275 -4.2350278 -4.25339 -4.2681351 -4.2788167 -4.2871127 -4.2927618 -4.2929616 -4.2851896 -4.2721729 -4.26019 -4.2581162 -4.2673907 -4.2816553]]...]
INFO - root - 2017-12-06 07:42:55.176035: step 16110, loss = 2.06, batch loss = 2.00 (35.1 examples/sec; 0.228 sec/batch; 20h:03m:05s remains)
INFO - root - 2017-12-06 07:42:57.415172: step 16120, loss = 2.05, batch loss = 1.99 (35.2 examples/sec; 0.227 sec/batch; 19h:59m:03s remains)
INFO - root - 2017-12-06 07:42:59.683347: step 16130, loss = 2.09, batch loss = 2.03 (36.1 examples/sec; 0.222 sec/batch; 19h:29m:20s remains)
INFO - root - 2017-12-06 07:43:01.941685: step 16140, loss = 2.10, batch loss = 2.04 (35.2 examples/sec; 0.227 sec/batch; 19h:56m:53s remains)
INFO - root - 2017-12-06 07:43:04.194484: step 16150, loss = 2.08, batch loss = 2.02 (35.1 examples/sec; 0.228 sec/batch; 20h:00m:37s remains)
INFO - root - 2017-12-06 07:43:06.445702: step 16160, loss = 2.05, batch loss = 1.99 (35.8 examples/sec; 0.224 sec/batch; 19h:39m:35s remains)
INFO - root - 2017-12-06 07:43:08.705217: step 16170, loss = 2.07, batch loss = 2.01 (33.0 examples/sec; 0.242 sec/batch; 21h:17m:44s remains)
INFO - root - 2017-12-06 07:43:10.991605: step 16180, loss = 2.05, batch loss = 1.99 (33.8 examples/sec; 0.237 sec/batch; 20h:49m:32s remains)
INFO - root - 2017-12-06 07:43:13.237122: step 16190, loss = 2.09, batch loss = 2.03 (35.3 examples/sec; 0.227 sec/batch; 19h:56m:08s remains)
INFO - root - 2017-12-06 07:43:15.483767: step 16200, loss = 2.06, batch loss = 2.00 (35.1 examples/sec; 0.228 sec/batch; 20h:02m:38s remains)
2017-12-06 07:43:15.829876: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.093802 -4.1031389 -4.1294227 -4.1610613 -4.1751676 -4.1678162 -4.1435976 -4.1125846 -4.0881052 -4.0669103 -4.057303 -4.0711393 -4.1035118 -4.1311736 -4.1423869][-4.0890975 -4.0924778 -4.1115913 -4.1431265 -4.1599145 -4.1568551 -4.1353807 -4.1093407 -4.091598 -4.0704165 -4.0640755 -4.0790477 -4.1047754 -4.1313591 -4.1461697][-4.0898633 -4.0849266 -4.0937152 -4.1207571 -4.1379352 -4.141026 -4.12335 -4.1035962 -4.0953722 -4.084929 -4.0870013 -4.0980496 -4.1086988 -4.1307373 -4.1487231][-4.0989356 -4.0911045 -4.0935283 -4.1131268 -4.12693 -4.1294303 -4.112628 -4.093111 -4.0891767 -4.0886636 -4.0994706 -4.1036558 -4.10116 -4.11576 -4.1357431][-4.0996847 -4.100071 -4.1061292 -4.1243749 -4.1336904 -4.122364 -4.0916057 -4.0635767 -4.0609803 -4.0695076 -4.0853524 -4.0878468 -4.0787926 -4.0854726 -4.10647][-4.092772 -4.1066713 -4.1225858 -4.1434822 -4.1431932 -4.1040339 -4.045547 -4.0084915 -4.01831 -4.0437388 -4.0690188 -4.0803814 -4.0739193 -4.0726371 -4.0829129][-4.0647287 -4.0888591 -4.1143327 -4.1357942 -4.1237321 -4.0584292 -3.9777675 -3.9404798 -3.9741197 -4.0250869 -4.0622764 -4.0791378 -4.0734463 -4.0617952 -4.0581245][-4.0335464 -4.0621018 -4.0915341 -4.1081638 -4.080688 -3.9961958 -3.9036753 -3.8772466 -3.9345691 -4.0088811 -4.0604939 -4.0796084 -4.0703387 -4.0538735 -4.0446658][-4.0429225 -4.0676851 -4.0900693 -4.0912528 -4.0488715 -3.9625182 -3.8814659 -3.8724346 -3.9381647 -4.0198731 -4.0774822 -4.0949807 -4.085588 -4.0713592 -4.0623431][-4.0717845 -4.0894675 -4.1021953 -4.0948505 -4.0521021 -3.9825728 -3.9308558 -3.9361362 -3.9853692 -4.0515037 -4.1001568 -4.113277 -4.1087408 -4.1000781 -4.0906138][-4.11166 -4.1179295 -4.1215944 -4.1120415 -4.0800166 -4.0341415 -4.0085564 -4.0187116 -4.0507112 -4.0992603 -4.1352239 -4.142345 -4.1394315 -4.1312513 -4.1221709][-4.145925 -4.14081 -4.1360378 -4.1262774 -4.1092033 -4.0900431 -4.085557 -4.0995631 -4.1206493 -4.1543775 -4.1789379 -4.1818848 -4.1795282 -4.1714621 -4.1638036][-4.1747189 -4.1660891 -4.161274 -4.1568551 -4.1526513 -4.1508236 -4.1565623 -4.1703382 -4.1832623 -4.2019377 -4.2140093 -4.2122383 -4.2108517 -4.2070594 -4.20266][-4.2116113 -4.2025819 -4.201498 -4.1999779 -4.19862 -4.2016044 -4.210835 -4.2254782 -4.2360525 -4.2464094 -4.251668 -4.2503138 -4.2500539 -4.2497525 -4.2477608][-4.2455983 -4.2370291 -4.2388062 -4.2401218 -4.2404985 -4.2447495 -4.2542405 -4.26713 -4.2751122 -4.2800918 -4.2829514 -4.2847128 -4.28701 -4.2888641 -4.2880111]]...]
INFO - root - 2017-12-06 07:43:18.097955: step 16210, loss = 2.06, batch loss = 2.01 (35.7 examples/sec; 0.224 sec/batch; 19h:41m:23s remains)
INFO - root - 2017-12-06 07:43:20.336488: step 16220, loss = 2.06, batch loss = 2.01 (35.8 examples/sec; 0.223 sec/batch; 19h:37m:52s remains)
INFO - root - 2017-12-06 07:43:22.613470: step 16230, loss = 2.06, batch loss = 2.00 (35.4 examples/sec; 0.226 sec/batch; 19h:51m:59s remains)
INFO - root - 2017-12-06 07:43:24.874255: step 16240, loss = 2.05, batch loss = 1.99 (35.5 examples/sec; 0.226 sec/batch; 19h:49m:26s remains)
INFO - root - 2017-12-06 07:43:27.160026: step 16250, loss = 2.06, batch loss = 2.01 (34.2 examples/sec; 0.234 sec/batch; 20h:33m:50s remains)
INFO - root - 2017-12-06 07:43:29.424275: step 16260, loss = 2.05, batch loss = 1.99 (35.6 examples/sec; 0.225 sec/batch; 19h:44m:03s remains)
INFO - root - 2017-12-06 07:43:31.649048: step 16270, loss = 2.06, batch loss = 2.00 (36.7 examples/sec; 0.218 sec/batch; 19h:10m:10s remains)
INFO - root - 2017-12-06 07:43:33.927489: step 16280, loss = 2.04, batch loss = 1.98 (35.2 examples/sec; 0.227 sec/batch; 19h:58m:59s remains)
INFO - root - 2017-12-06 07:43:36.231979: step 16290, loss = 2.06, batch loss = 2.00 (36.5 examples/sec; 0.219 sec/batch; 19h:16m:39s remains)
INFO - root - 2017-12-06 07:43:38.545937: step 16300, loss = 2.07, batch loss = 2.01 (33.5 examples/sec; 0.239 sec/batch; 20h:59m:39s remains)
2017-12-06 07:43:38.999245: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3159871 -4.31015 -4.3050532 -4.2988386 -4.2945461 -4.2940416 -4.2985935 -4.3081779 -4.3155971 -4.3147655 -4.3039236 -4.2874413 -4.2761126 -4.277534 -4.2906089][-4.2956371 -4.288084 -4.2816658 -4.2733765 -4.2644348 -4.2581005 -4.2602067 -4.2745438 -4.2882552 -4.2903118 -4.2754707 -4.2494035 -4.2317734 -4.2367253 -4.2601137][-4.2664781 -4.2619863 -4.2599859 -4.2514343 -4.2342992 -4.2160234 -4.2123895 -4.2318053 -4.2547922 -4.2623835 -4.2441263 -4.2068081 -4.1791725 -4.1833115 -4.2146182][-4.2447548 -4.2489548 -4.2563686 -4.248116 -4.2177949 -4.1800284 -4.1673827 -4.1929903 -4.2288189 -4.2455158 -4.2288027 -4.1832075 -4.1399689 -4.1355224 -4.1694255][-4.2323961 -4.2425756 -4.2561107 -4.2460823 -4.2007189 -4.138947 -4.11173 -4.1448393 -4.2009048 -4.2341256 -4.2290926 -4.1873722 -4.1326976 -4.1123552 -4.1405826][-4.2204962 -4.2354765 -4.2512374 -4.2365975 -4.1751795 -4.0865736 -4.0377359 -4.0769596 -4.1597352 -4.2182317 -4.2339854 -4.2087011 -4.1541967 -4.1189647 -4.134582][-4.1992726 -4.216671 -4.2317929 -4.2113161 -4.1340828 -4.0202689 -3.9483113 -3.9915855 -4.1019764 -4.1914058 -4.2330942 -4.227375 -4.1768918 -4.1297336 -4.1296859][-4.1650662 -4.1833835 -4.1993017 -4.1765547 -4.0923028 -3.9656568 -3.8775876 -3.9171467 -4.0424652 -4.156579 -4.2257504 -4.2393126 -4.1974187 -4.1433043 -4.1253815][-4.1343484 -4.1566072 -4.18002 -4.1692281 -4.0992985 -3.9860275 -3.8971374 -3.91451 -4.0219283 -4.1367984 -4.2206211 -4.2483993 -4.2172327 -4.1595788 -4.1238284][-4.1220345 -4.1458936 -4.1758428 -4.184721 -4.1461997 -4.0678883 -3.9958365 -3.9886444 -4.0543122 -4.1451221 -4.2212224 -4.249269 -4.2223082 -4.1633391 -4.1188068][-4.1348395 -4.1576781 -4.1888747 -4.2104392 -4.199162 -4.1573009 -4.1073256 -4.0866876 -4.1166396 -4.1763439 -4.2343264 -4.2575173 -4.2344065 -4.1813769 -4.1395159][-4.1743817 -4.1936884 -4.2242575 -4.251163 -4.2574234 -4.24291 -4.2128043 -4.19066 -4.1994047 -4.2321935 -4.2702765 -4.2883463 -4.270175 -4.2255745 -4.190104][-4.231904 -4.2469063 -4.2737188 -4.2995453 -4.3124633 -4.3122163 -4.2970533 -4.2792163 -4.2776847 -4.2919865 -4.3145895 -4.3262372 -4.3127542 -4.2771368 -4.2484941][-4.2828984 -4.2931204 -4.3126659 -4.3313723 -4.3405681 -4.3439493 -4.3394237 -4.3297372 -4.3255534 -4.328825 -4.3402486 -4.34676 -4.3362293 -4.3098178 -4.2877941][-4.3075733 -4.3138404 -4.3246651 -4.3338294 -4.3371572 -4.3402867 -4.3423462 -4.3400578 -4.3373365 -4.3362875 -4.3401504 -4.3420444 -4.3338852 -4.3167553 -4.3027391]]...]
INFO - root - 2017-12-06 07:43:41.311458: step 16310, loss = 2.08, batch loss = 2.02 (35.6 examples/sec; 0.225 sec/batch; 19h:45m:14s remains)
INFO - root - 2017-12-06 07:43:43.553411: step 16320, loss = 2.05, batch loss = 1.99 (34.7 examples/sec; 0.231 sec/batch; 20h:15m:18s remains)
INFO - root - 2017-12-06 07:43:45.837749: step 16330, loss = 2.05, batch loss = 1.99 (35.5 examples/sec; 0.225 sec/batch; 19h:46m:36s remains)
INFO - root - 2017-12-06 07:43:48.092216: step 16340, loss = 2.07, batch loss = 2.01 (35.8 examples/sec; 0.224 sec/batch; 19h:38m:25s remains)
INFO - root - 2017-12-06 07:43:50.323693: step 16350, loss = 2.07, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 19h:30m:03s remains)
INFO - root - 2017-12-06 07:43:52.581468: step 16360, loss = 2.11, batch loss = 2.05 (35.8 examples/sec; 0.224 sec/batch; 19h:38m:41s remains)
INFO - root - 2017-12-06 07:43:54.851741: step 16370, loss = 2.07, batch loss = 2.02 (33.8 examples/sec; 0.237 sec/batch; 20h:48m:17s remains)
INFO - root - 2017-12-06 07:43:57.143244: step 16380, loss = 2.05, batch loss = 1.99 (34.8 examples/sec; 0.230 sec/batch; 20h:10m:37s remains)
INFO - root - 2017-12-06 07:43:59.443443: step 16390, loss = 2.06, batch loss = 2.00 (35.4 examples/sec; 0.226 sec/batch; 19h:51m:20s remains)
INFO - root - 2017-12-06 07:44:01.707682: step 16400, loss = 2.07, batch loss = 2.01 (36.2 examples/sec; 0.221 sec/batch; 19h:23m:13s remains)
2017-12-06 07:44:02.090582: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2644739 -4.2662821 -4.2692442 -4.2711706 -4.2719951 -4.27221 -4.2716193 -4.2699933 -4.2676926 -4.2652645 -4.2630043 -4.2612443 -4.2602739 -4.25988 -4.259778][-4.2518578 -4.2564592 -4.2610073 -4.2635856 -4.263854 -4.2628222 -4.2608271 -4.25801 -4.2548943 -4.2514763 -4.2483048 -4.2457976 -4.2440524 -4.2426486 -4.24106][-4.253334 -4.2590055 -4.263135 -4.2643223 -4.2628579 -4.2600703 -4.2566047 -4.2530584 -4.2503581 -4.247478 -4.2447834 -4.2421765 -4.2400022 -4.2379856 -4.235178][-4.2609763 -4.2642665 -4.2661004 -4.2652612 -4.2616363 -4.2574115 -4.2532434 -4.2493811 -4.2478395 -4.2469025 -4.2458611 -4.24446 -4.2430844 -4.2415919 -4.2389321][-4.2603545 -4.2606945 -4.2596178 -4.2562637 -4.2493162 -4.2412105 -4.2344074 -4.229444 -4.2281513 -4.2283998 -4.2287149 -4.2285304 -4.2284188 -4.2285271 -4.2280879][-4.2528749 -4.2508874 -4.2457647 -4.2368474 -4.2226033 -4.2074833 -4.1955304 -4.1874576 -4.1842723 -4.1838689 -4.1842594 -4.1844926 -4.1851869 -4.1866832 -4.1885996][-4.242516 -4.2378578 -4.2268944 -4.210453 -4.1879277 -4.1654396 -4.1478157 -4.1360607 -4.1296673 -4.1265478 -4.125535 -4.1255412 -4.1268044 -4.1297717 -4.13413][-4.2211528 -4.2141066 -4.2002368 -4.1804147 -4.1542664 -4.1283112 -4.1080618 -4.0933018 -4.0844431 -4.0798435 -4.0789719 -4.0803971 -4.0831451 -4.0878143 -4.0943108][-4.2078886 -4.201293 -4.1889629 -4.1730666 -4.15271 -4.1314712 -4.1141009 -4.1009445 -4.093226 -4.089746 -4.0898118 -4.0921421 -4.0956459 -4.10046 -4.1068659][-4.2239146 -4.2220864 -4.2161403 -4.2070165 -4.194459 -4.1794348 -4.166223 -4.156291 -4.1504383 -4.1474457 -4.1472726 -4.1493235 -4.1527596 -4.1565223 -4.1604347][-4.2529612 -4.25493 -4.254005 -4.2506466 -4.2439771 -4.2342296 -4.2251873 -4.2182164 -4.2142019 -4.2118115 -4.2109513 -4.2115049 -4.2131867 -4.2148957 -4.2160339][-4.2591405 -4.2627239 -4.264679 -4.2654834 -4.2634544 -4.2580829 -4.2527347 -4.2486334 -4.2461452 -4.2440157 -4.2420635 -4.2407212 -4.2404866 -4.2411952 -4.242219][-4.2400727 -4.2432208 -4.2469988 -4.2505069 -4.25209 -4.2509604 -4.2495818 -4.2487726 -4.2487249 -4.2481956 -4.2467642 -4.2452836 -4.2451706 -4.2463064 -4.2478852][-4.2249265 -4.2261477 -4.229382 -4.2335453 -4.2375231 -4.2405043 -4.2438736 -4.2477045 -4.2515078 -4.2536411 -4.253571 -4.2527661 -4.2531967 -4.25438 -4.2550745][-4.2289772 -4.227746 -4.2283444 -4.2308335 -4.2345886 -4.2390513 -4.2455211 -4.2529383 -4.2592993 -4.2631183 -4.2642775 -4.2643309 -4.2649236 -4.2656107 -4.2653556]]...]
INFO - root - 2017-12-06 07:44:04.377953: step 16410, loss = 2.07, batch loss = 2.02 (34.9 examples/sec; 0.229 sec/batch; 20h:07m:21s remains)
INFO - root - 2017-12-06 07:44:06.667920: step 16420, loss = 2.09, batch loss = 2.03 (35.0 examples/sec; 0.229 sec/batch; 20h:05m:01s remains)
INFO - root - 2017-12-06 07:44:08.899975: step 16430, loss = 2.05, batch loss = 1.99 (33.5 examples/sec; 0.239 sec/batch; 20h:58m:43s remains)
INFO - root - 2017-12-06 07:44:11.141167: step 16440, loss = 2.08, batch loss = 2.02 (35.2 examples/sec; 0.227 sec/batch; 19h:57m:56s remains)
INFO - root - 2017-12-06 07:44:13.378605: step 16450, loss = 2.07, batch loss = 2.02 (35.2 examples/sec; 0.227 sec/batch; 19h:55m:37s remains)
INFO - root - 2017-12-06 07:44:15.606885: step 16460, loss = 2.08, batch loss = 2.02 (36.0 examples/sec; 0.222 sec/batch; 19h:29m:00s remains)
INFO - root - 2017-12-06 07:44:17.881268: step 16470, loss = 2.05, batch loss = 1.99 (35.5 examples/sec; 0.225 sec/batch; 19h:47m:04s remains)
INFO - root - 2017-12-06 07:44:20.162573: step 16480, loss = 2.07, batch loss = 2.02 (35.3 examples/sec; 0.226 sec/batch; 19h:52m:00s remains)
INFO - root - 2017-12-06 07:44:22.453252: step 16490, loss = 2.05, batch loss = 1.99 (35.3 examples/sec; 0.227 sec/batch; 19h:53m:52s remains)
INFO - root - 2017-12-06 07:44:24.754189: step 16500, loss = 2.09, batch loss = 2.03 (33.9 examples/sec; 0.236 sec/batch; 20h:43m:20s remains)
2017-12-06 07:44:25.174854: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3145623 -4.3058844 -4.2980781 -4.2906237 -4.2833405 -4.2829828 -4.2854328 -4.2792449 -4.27762 -4.2803717 -4.2771931 -4.2694373 -4.2612586 -4.266458 -4.2723036][-4.2953029 -4.2823186 -4.2694297 -4.2585044 -4.2479949 -4.2509828 -4.25864 -4.2532744 -4.2488303 -4.2507753 -4.2484565 -4.2432261 -4.2372689 -4.249496 -4.2559013][-4.2552829 -4.2336259 -4.2144785 -4.2028232 -4.1977243 -4.2081733 -4.2219028 -4.2189336 -4.2095666 -4.2058992 -4.2070718 -4.2094531 -4.206594 -4.2228532 -4.2309227][-4.2129107 -4.1853242 -4.164619 -4.1538157 -4.1567206 -4.1720395 -4.1897349 -4.18617 -4.1703682 -4.1564355 -4.1557951 -4.1647663 -4.1672039 -4.1852283 -4.19155][-4.1934838 -4.1656208 -4.1451793 -4.1336222 -4.1364317 -4.1499133 -4.1639509 -4.1562362 -4.1287341 -4.1026397 -4.0987792 -4.112309 -4.123364 -4.1422672 -4.1426115][-4.1855688 -4.1645732 -4.1499815 -4.1351442 -4.1297283 -4.1292439 -4.1260624 -4.1056237 -4.0670476 -4.040606 -4.0453358 -4.0665059 -4.0862765 -4.1079621 -4.1051326][-4.1904807 -4.1783357 -4.1680908 -4.1472263 -4.1235676 -4.0955691 -4.06613 -4.0263543 -3.9798422 -3.9653962 -3.99761 -4.0349073 -4.0664163 -4.0967779 -4.0998125][-4.2019844 -4.1960254 -4.18954 -4.158906 -4.1080947 -4.0462036 -3.9949691 -3.9494789 -3.9117084 -3.9157665 -3.9716992 -4.0251412 -4.0671 -4.1086078 -4.1229868][-4.196394 -4.1866655 -4.1790128 -4.148706 -4.0808072 -3.9928598 -3.9375026 -3.9146168 -3.9095087 -3.9283092 -3.9820766 -4.035439 -4.0856934 -4.1381416 -4.1653886][-4.1757379 -4.158134 -4.1453829 -4.1228547 -4.0661654 -3.9887195 -3.9526114 -3.9626355 -3.9875948 -4.0103941 -4.048121 -4.0891037 -4.1354504 -4.1861548 -4.2184272][-4.1816783 -4.1528177 -4.1319952 -4.1218128 -4.0956435 -4.0458422 -4.0238457 -4.0488567 -4.08254 -4.100975 -4.1242518 -4.1534705 -4.1899009 -4.2300177 -4.2579088][-4.2190776 -4.18665 -4.1641688 -4.1652331 -4.1633816 -4.1388764 -4.1242871 -4.1403933 -4.1658039 -4.1820488 -4.1983256 -4.2164373 -4.2387552 -4.2637253 -4.2825084][-4.2606525 -4.231019 -4.2102442 -4.2204604 -4.2371306 -4.2352424 -4.2274604 -4.23222 -4.2480173 -4.2609415 -4.2690272 -4.2750568 -4.2828608 -4.2938967 -4.3033137][-4.2975016 -4.2764816 -4.259994 -4.27224 -4.2940521 -4.3034806 -4.3022094 -4.3031893 -4.3135014 -4.3216724 -4.3247833 -4.3240523 -4.3232527 -4.3259726 -4.3293281][-4.3247142 -4.3113046 -4.30195 -4.3117561 -4.3284931 -4.3372726 -4.3367639 -4.3353996 -4.3418541 -4.3501635 -4.3552132 -4.3530269 -4.3476648 -4.3462391 -4.345953]]...]
INFO - root - 2017-12-06 07:44:27.448356: step 16510, loss = 2.07, batch loss = 2.01 (35.8 examples/sec; 0.224 sec/batch; 19h:37m:47s remains)
INFO - root - 2017-12-06 07:44:29.693836: step 16520, loss = 2.09, batch loss = 2.04 (34.0 examples/sec; 0.235 sec/batch; 20h:37m:35s remains)
INFO - root - 2017-12-06 07:44:31.927062: step 16530, loss = 2.08, batch loss = 2.02 (35.3 examples/sec; 0.226 sec/batch; 19h:52m:20s remains)
INFO - root - 2017-12-06 07:44:34.170369: step 16540, loss = 2.08, batch loss = 2.02 (35.5 examples/sec; 0.225 sec/batch; 19h:46m:32s remains)
INFO - root - 2017-12-06 07:44:36.431793: step 16550, loss = 2.05, batch loss = 1.99 (35.2 examples/sec; 0.227 sec/batch; 19h:57m:14s remains)
INFO - root - 2017-12-06 07:44:38.692210: step 16560, loss = 2.07, batch loss = 2.01 (34.1 examples/sec; 0.235 sec/batch; 20h:37m:01s remains)
INFO - root - 2017-12-06 07:44:40.970606: step 16570, loss = 2.06, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 19h:29m:11s remains)
INFO - root - 2017-12-06 07:44:43.207505: step 16580, loss = 2.07, batch loss = 2.02 (35.3 examples/sec; 0.227 sec/batch; 19h:53m:52s remains)
INFO - root - 2017-12-06 07:44:45.470764: step 16590, loss = 2.07, batch loss = 2.01 (33.3 examples/sec; 0.240 sec/batch; 21h:03m:57s remains)
INFO - root - 2017-12-06 07:44:47.717993: step 16600, loss = 2.05, batch loss = 1.99 (35.9 examples/sec; 0.223 sec/batch; 19h:32m:34s remains)
2017-12-06 07:44:48.153416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2729964 -4.2648005 -4.2568693 -4.2523189 -4.2599273 -4.2707386 -4.2745042 -4.2799668 -4.2900028 -4.3029842 -4.3189259 -4.3329463 -4.3383107 -4.3351064 -4.3290038][-4.2897882 -4.2830353 -4.27498 -4.26563 -4.2640543 -4.2656159 -4.26236 -4.2627578 -4.2711706 -4.2872019 -4.307045 -4.3240528 -4.3332672 -4.3349733 -4.3318324][-4.3025088 -4.2935972 -4.2810769 -4.2645235 -4.25188 -4.241116 -4.2290735 -4.225039 -4.2341957 -4.2560353 -4.2816987 -4.3027267 -4.3170075 -4.3245354 -4.3269043][-4.314949 -4.3035378 -4.2842894 -4.2581191 -4.2342529 -4.2118931 -4.1920624 -4.18419 -4.1933694 -4.2191105 -4.2493429 -4.2725677 -4.2904396 -4.3030558 -4.3109679][-4.3272672 -4.314887 -4.2887864 -4.2512212 -4.2126427 -4.1746655 -4.1440077 -4.1345091 -4.1489673 -4.1805358 -4.213006 -4.2352672 -4.253263 -4.2695308 -4.2835951][-4.3313684 -4.3185058 -4.2875419 -4.239707 -4.1875682 -4.1351738 -4.0924816 -4.0783491 -4.1007848 -4.1420927 -4.1776576 -4.1986952 -4.2152781 -4.2330532 -4.2520862][-4.3233657 -4.3122811 -4.2798223 -4.227653 -4.1675992 -4.1062961 -4.0562258 -4.0390096 -4.0672145 -4.1164379 -4.1563635 -4.1791682 -4.1966553 -4.2152319 -4.2361107][-4.3044348 -4.2982321 -4.2713394 -4.2249641 -4.1678386 -4.1072931 -4.0586362 -4.0407043 -4.0697374 -4.1226325 -4.1641641 -4.1870608 -4.2040405 -4.221643 -4.2397742][-4.2784624 -4.27854 -4.2617378 -4.2287331 -4.1869164 -4.140389 -4.1020308 -4.0828819 -4.1009626 -4.1434493 -4.1787267 -4.1994004 -4.2177448 -4.2356286 -4.2510381][-4.2517114 -4.2562804 -4.2484217 -4.229846 -4.2056556 -4.1741681 -4.1466575 -4.1279907 -4.134398 -4.1596222 -4.1834731 -4.2024784 -4.2224526 -4.2427144 -4.2582207][-4.2255931 -4.2319179 -4.229063 -4.2204885 -4.210741 -4.1929669 -4.1730466 -4.1538138 -4.150084 -4.1566868 -4.166667 -4.1832676 -4.2071924 -4.2322016 -4.2502623][-4.2134776 -4.2212739 -4.2212138 -4.2176938 -4.214292 -4.2030072 -4.1851139 -4.1619225 -4.1484294 -4.1407514 -4.1406989 -4.1541038 -4.1806703 -4.2121315 -4.2347374][-4.2208743 -4.2324858 -4.233593 -4.2290692 -4.224164 -4.2122059 -4.1883903 -4.1571159 -4.133234 -4.1167531 -4.1143022 -4.1283703 -4.1591 -4.1974187 -4.2253003][-4.22995 -4.2451615 -4.2486358 -4.2451835 -4.2403708 -4.2287579 -4.2040129 -4.1708884 -4.1417656 -4.1204433 -4.116076 -4.1278772 -4.15545 -4.1938744 -4.2226276][-4.2293744 -4.2427039 -4.2458243 -4.2431331 -4.2412825 -4.2368383 -4.2235627 -4.2039213 -4.1829181 -4.1657095 -4.1608009 -4.1644812 -4.1783838 -4.2041507 -4.2251797]]...]
INFO - root - 2017-12-06 07:44:50.499952: step 16610, loss = 2.06, batch loss = 2.00 (34.0 examples/sec; 0.235 sec/batch; 20h:37m:40s remains)
INFO - root - 2017-12-06 07:44:52.698690: step 16620, loss = 2.07, batch loss = 2.01 (36.2 examples/sec; 0.221 sec/batch; 19h:22m:13s remains)
INFO - root - 2017-12-06 07:44:54.985003: step 16630, loss = 2.08, batch loss = 2.02 (34.4 examples/sec; 0.232 sec/batch; 20h:23m:20s remains)
INFO - root - 2017-12-06 07:44:57.276812: step 16640, loss = 2.06, batch loss = 2.01 (33.2 examples/sec; 0.241 sec/batch; 21h:10m:20s remains)
INFO - root - 2017-12-06 07:44:59.538643: step 16650, loss = 2.08, batch loss = 2.02 (33.2 examples/sec; 0.241 sec/batch; 21h:09m:27s remains)
INFO - root - 2017-12-06 07:45:01.837682: step 16660, loss = 2.05, batch loss = 2.00 (35.0 examples/sec; 0.228 sec/batch; 20h:01m:56s remains)
INFO - root - 2017-12-06 07:45:04.095936: step 16670, loss = 2.06, batch loss = 2.00 (36.2 examples/sec; 0.221 sec/batch; 19h:22m:27s remains)
INFO - root - 2017-12-06 07:45:06.369738: step 16680, loss = 2.05, batch loss = 2.00 (35.7 examples/sec; 0.224 sec/batch; 19h:39m:15s remains)
INFO - root - 2017-12-06 07:45:08.632629: step 16690, loss = 2.06, batch loss = 2.00 (34.9 examples/sec; 0.229 sec/batch; 20h:05m:26s remains)
INFO - root - 2017-12-06 07:45:10.954260: step 16700, loss = 2.06, batch loss = 2.00 (36.0 examples/sec; 0.222 sec/batch; 19h:31m:05s remains)
2017-12-06 07:45:11.360068: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3422289 -4.3382792 -4.3391008 -4.34074 -4.3408642 -4.3400631 -4.3400793 -4.3410268 -4.3399224 -4.3357515 -4.3288774 -4.3196683 -4.3056669 -4.2865796 -4.2712584][-4.3398733 -4.3318534 -4.3292112 -4.3289213 -4.3286748 -4.32776 -4.3274293 -4.3287039 -4.3279386 -4.3240285 -4.3183975 -4.312016 -4.3033066 -4.2880497 -4.271934][-4.3422303 -4.3289685 -4.3205285 -4.3163948 -4.3150492 -4.3129339 -4.313314 -4.3152156 -4.3129678 -4.307364 -4.3024812 -4.2999086 -4.2993174 -4.2903972 -4.2756915][-4.3424 -4.3205881 -4.3021731 -4.29048 -4.2821188 -4.2734523 -4.271255 -4.2722316 -4.2680144 -4.2625027 -4.2601333 -4.263617 -4.2746272 -4.2748427 -4.2646456][-4.3296275 -4.2958708 -4.2642837 -4.2397742 -4.2152267 -4.1891994 -4.1711025 -4.1659255 -4.1629553 -4.1634803 -4.1678224 -4.1840224 -4.2129068 -4.2277365 -4.2267056][-4.2915893 -4.244482 -4.2036476 -4.1676779 -4.1216497 -4.0627246 -4.0050244 -3.9844742 -4.0004139 -4.0259147 -4.0501494 -4.0851607 -4.1311808 -4.1613297 -4.1717143][-4.2478213 -4.1973872 -4.1592336 -4.1204963 -4.0569081 -3.9622703 -3.854212 -3.8236651 -3.8835254 -3.9502068 -3.9990535 -4.0450935 -4.0904417 -4.1205111 -4.1322751][-4.2312245 -4.1896834 -4.1633234 -4.1377974 -4.084518 -3.9913108 -3.8779883 -3.8530903 -3.9285221 -3.9972363 -4.038 -4.0694017 -4.091373 -4.101079 -4.1035738][-4.2302089 -4.196414 -4.1796041 -4.1710925 -4.1451583 -4.081254 -3.9999034 -3.9867554 -4.0409756 -4.0814939 -4.1010823 -4.1105962 -4.1086154 -4.0992723 -4.0946989][-4.2279792 -4.1956491 -4.1845098 -4.18767 -4.1828132 -4.1460152 -4.0975156 -4.0929232 -4.1246185 -4.1466122 -4.1556206 -4.1585479 -4.1507425 -4.1383491 -4.1350341][-4.2363858 -4.2076406 -4.1996164 -4.2061071 -4.2079592 -4.1865878 -4.159 -4.1576447 -4.1753607 -4.1880956 -4.1937995 -4.1983805 -4.197093 -4.1920643 -4.1968708][-4.2609396 -4.2391825 -4.232491 -4.2383013 -4.2425609 -4.2347832 -4.2247524 -4.2280674 -4.2404628 -4.2491159 -4.2527905 -4.255023 -4.2545295 -4.2520738 -4.2567439][-4.2965946 -4.2823286 -4.2782531 -4.2855225 -4.2927017 -4.2946692 -4.2955141 -4.3012433 -4.3117404 -4.3173409 -4.3171 -4.3148208 -4.3120689 -4.3087254 -4.3102627][-4.3296256 -4.3207226 -4.3191209 -4.3264637 -4.3343668 -4.339345 -4.3422875 -4.3473163 -4.3562427 -4.3614674 -4.3622804 -4.3610287 -4.3594217 -4.3568649 -4.3564215][-4.3507471 -4.3475447 -4.3494673 -4.3559709 -4.3618288 -4.3636889 -4.3633924 -4.364944 -4.3696036 -4.3732467 -4.3756094 -4.3770251 -4.3768773 -4.3764682 -4.3765354]]...]
INFO - root - 2017-12-06 07:45:13.622595: step 16710, loss = 2.08, batch loss = 2.02 (34.8 examples/sec; 0.230 sec/batch; 20h:09m:02s remains)
INFO - root - 2017-12-06 07:45:15.922149: step 16720, loss = 2.06, batch loss = 2.00 (34.0 examples/sec; 0.235 sec/batch; 20h:37m:55s remains)
INFO - root - 2017-12-06 07:45:18.174246: step 16730, loss = 2.07, batch loss = 2.01 (35.2 examples/sec; 0.227 sec/batch; 19h:54m:38s remains)
INFO - root - 2017-12-06 07:45:20.432588: step 16740, loss = 2.08, batch loss = 2.02 (31.9 examples/sec; 0.251 sec/batch; 21h:59m:26s remains)
INFO - root - 2017-12-06 07:45:22.693773: step 16750, loss = 2.04, batch loss = 1.99 (36.1 examples/sec; 0.222 sec/batch; 19h:27m:01s remains)
INFO - root - 2017-12-06 07:45:24.984931: step 16760, loss = 2.08, batch loss = 2.02 (35.3 examples/sec; 0.226 sec/batch; 19h:51m:10s remains)
INFO - root - 2017-12-06 07:45:27.249399: step 16770, loss = 2.09, batch loss = 2.04 (35.2 examples/sec; 0.228 sec/batch; 19h:57m:36s remains)
INFO - root - 2017-12-06 07:45:29.504765: step 16780, loss = 2.08, batch loss = 2.02 (37.2 examples/sec; 0.215 sec/batch; 18h:52m:10s remains)
INFO - root - 2017-12-06 07:45:31.753655: step 16790, loss = 2.10, batch loss = 2.05 (35.8 examples/sec; 0.223 sec/batch; 19h:34m:30s remains)
INFO - root - 2017-12-06 07:45:34.036448: step 16800, loss = 2.04, batch loss = 1.98 (34.3 examples/sec; 0.233 sec/batch; 20h:27m:58s remains)
2017-12-06 07:45:34.445322: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2794018 -4.2889733 -4.3004322 -4.3064818 -4.3023911 -4.2846031 -4.2659626 -4.2531695 -4.2460914 -4.2489262 -4.2640572 -4.2878814 -4.3122139 -4.3328829 -4.3473816][-4.2805543 -4.2955446 -4.310163 -4.3167248 -4.3101587 -4.2910304 -4.2751689 -4.26624 -4.2571225 -4.2549071 -4.2640119 -4.2830615 -4.3051972 -4.323905 -4.3366246][-4.2842536 -4.2994852 -4.3134303 -4.3176575 -4.3099103 -4.2935958 -4.2822385 -4.2745233 -4.2634134 -4.2612267 -4.2678866 -4.283648 -4.3028045 -4.3189182 -4.3290386][-4.2619843 -4.271615 -4.28281 -4.2821336 -4.2725987 -4.2602015 -4.2534447 -4.2469993 -4.23888 -4.2417641 -4.2509222 -4.2674017 -4.2889371 -4.3061776 -4.3168149][-4.2090316 -4.2093863 -4.217721 -4.2155008 -4.2029533 -4.1901908 -4.1856451 -4.1816792 -4.1828113 -4.2005954 -4.222652 -4.2475867 -4.2753277 -4.2952461 -4.3063946][-4.143764 -4.1380277 -4.1447773 -4.1404409 -4.1199675 -4.0987267 -4.0848093 -4.077796 -4.092412 -4.1346178 -4.1799068 -4.2235942 -4.2647066 -4.2902169 -4.3019648][-4.0786524 -4.0635543 -4.06333 -4.0515423 -4.0178561 -3.9814911 -3.9600973 -3.9598355 -3.9954979 -4.0647478 -4.13585 -4.1995387 -4.2544003 -4.2871685 -4.3012066][-4.0195451 -3.9985371 -3.9946733 -3.981138 -3.9408643 -3.9006705 -3.8867249 -3.9031439 -3.9580169 -4.0394111 -4.1155815 -4.1834931 -4.24241 -4.2797794 -4.2979226][-4.0064282 -3.9916325 -3.9908972 -3.9827161 -3.9484763 -3.9146881 -3.9072747 -3.9274762 -3.9810138 -4.0507774 -4.1132007 -4.16868 -4.2223444 -4.2635136 -4.2892241][-4.0304794 -4.0237088 -4.0247512 -4.021317 -3.9989514 -3.9752395 -3.9683828 -3.976243 -4.0070381 -4.0538568 -4.0989804 -4.1438818 -4.193738 -4.2412658 -4.2776971][-4.0592732 -4.0515995 -4.0519276 -4.0556741 -4.0503488 -4.0414023 -4.0361519 -4.0296235 -4.0312328 -4.0548191 -4.0878453 -4.1290779 -4.1810894 -4.2349434 -4.2771525][-4.0922327 -4.0832334 -4.0842237 -4.0950246 -4.1017761 -4.1029668 -4.1020021 -4.0886183 -4.0744104 -4.0845203 -4.1133037 -4.1551085 -4.2069798 -4.2565274 -4.2917786][-4.1332555 -4.1242089 -4.1296406 -4.14628 -4.1610222 -4.16954 -4.1736913 -4.16596 -4.154562 -4.1623197 -4.1867433 -4.2213268 -4.2611895 -4.2939758 -4.31399][-4.201324 -4.197897 -4.2056627 -4.2218933 -4.2374105 -4.2466316 -4.2509236 -4.246891 -4.239706 -4.2461729 -4.2627783 -4.285213 -4.3095584 -4.3266187 -4.3329992][-4.2689018 -4.26931 -4.2753911 -4.2853851 -4.2960114 -4.3029361 -4.3063726 -4.3041806 -4.2996039 -4.3040051 -4.3134027 -4.3240118 -4.3349819 -4.3416438 -4.343019]]...]
INFO - root - 2017-12-06 07:45:36.766551: step 16810, loss = 2.07, batch loss = 2.01 (35.8 examples/sec; 0.223 sec/batch; 19h:35m:12s remains)
INFO - root - 2017-12-06 07:45:39.051617: step 16820, loss = 2.06, batch loss = 2.00 (35.5 examples/sec; 0.225 sec/batch; 19h:45m:45s remains)
INFO - root - 2017-12-06 07:45:41.303401: step 16830, loss = 2.05, batch loss = 2.00 (35.1 examples/sec; 0.228 sec/batch; 19h:58m:15s remains)
INFO - root - 2017-12-06 07:45:43.547515: step 16840, loss = 2.09, batch loss = 2.03 (36.3 examples/sec; 0.221 sec/batch; 19h:20m:38s remains)
INFO - root - 2017-12-06 07:45:45.806080: step 16850, loss = 2.05, batch loss = 1.99 (35.7 examples/sec; 0.224 sec/batch; 19h:38m:44s remains)
INFO - root - 2017-12-06 07:45:48.066646: step 16860, loss = 2.06, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 19h:58m:17s remains)
INFO - root - 2017-12-06 07:45:50.316718: step 16870, loss = 2.05, batch loss = 1.99 (33.9 examples/sec; 0.236 sec/batch; 20h:40m:54s remains)
INFO - root - 2017-12-06 07:45:52.594866: step 16880, loss = 2.09, batch loss = 2.03 (36.6 examples/sec; 0.218 sec/batch; 19h:08m:24s remains)
INFO - root - 2017-12-06 07:45:54.846594: step 16890, loss = 2.07, batch loss = 2.01 (35.2 examples/sec; 0.227 sec/batch; 19h:56m:28s remains)
INFO - root - 2017-12-06 07:45:57.117230: step 16900, loss = 2.06, batch loss = 2.01 (35.8 examples/sec; 0.224 sec/batch; 19h:35m:59s remains)
2017-12-06 07:45:57.570095: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1304092 -4.1317387 -4.154058 -4.1805325 -4.190721 -4.1797662 -4.1625075 -4.1433043 -4.1225958 -4.0907621 -4.0645747 -4.0730538 -4.1088934 -4.14863 -4.1700735][-4.1235957 -4.1315193 -4.1577497 -4.182735 -4.1885533 -4.1743674 -4.15801 -4.1394205 -4.1105175 -4.0629797 -4.0332227 -4.0487738 -4.1004057 -4.1487684 -4.1670723][-4.1132636 -4.1312027 -4.1566687 -4.1739855 -4.180253 -4.1722488 -4.1568127 -4.1428733 -4.1149874 -4.0623384 -4.030395 -4.0463834 -4.1051531 -4.1575284 -4.1720557][-4.1055045 -4.1339393 -4.1582794 -4.16853 -4.1732874 -4.1734872 -4.1611543 -4.1583223 -4.1468649 -4.1062269 -4.0711522 -4.0767035 -4.1237111 -4.1694517 -4.1873083][-4.098033 -4.1289272 -4.1531663 -4.1619177 -4.1669774 -4.1677241 -4.1493726 -4.1510954 -4.1658325 -4.1535606 -4.1284132 -4.1231518 -4.1458411 -4.1760426 -4.1918635][-4.0755486 -4.1033049 -4.1307564 -4.1434674 -4.1419754 -4.1228881 -4.0766497 -4.0670428 -4.1128888 -4.14651 -4.145535 -4.14061 -4.153461 -4.1671281 -4.1675334][-4.0646696 -4.0867815 -4.1182618 -4.1317215 -4.1122336 -4.0506072 -3.9336061 -3.8725836 -3.9580059 -4.0671139 -4.115375 -4.1263471 -4.1352324 -4.1345596 -4.118928][-4.0756803 -4.0888662 -4.1170712 -4.1216526 -4.0785656 -3.9808049 -3.8005686 -3.6611388 -3.7687349 -3.9584208 -4.0640769 -4.0980954 -4.1061678 -4.1011009 -4.0827618][-4.1211424 -4.1286035 -4.1559243 -4.1574059 -4.112864 -4.0300374 -3.8820267 -3.7456272 -3.7967472 -3.956064 -4.0655193 -4.1054606 -4.1147914 -4.1122813 -4.0928588][-4.168715 -4.1811838 -4.2138777 -4.2184386 -4.185164 -4.1348238 -4.0472827 -3.9639072 -3.9777379 -4.063592 -4.1270633 -4.1460228 -4.1441512 -4.140327 -4.1158853][-4.2002177 -4.2156954 -4.2487221 -4.2603168 -4.242178 -4.2141418 -4.16421 -4.1245108 -4.1340919 -4.1767216 -4.1964006 -4.1851363 -4.16512 -4.1590257 -4.1381578][-4.2074289 -4.2250333 -4.2521873 -4.2617769 -4.2525859 -4.2382026 -4.2106071 -4.1960449 -4.2090883 -4.2306852 -4.2286196 -4.1998067 -4.1754327 -4.173676 -4.164618][-4.2092662 -4.2249904 -4.2405877 -4.2370191 -4.2206855 -4.2076297 -4.1925278 -4.1904578 -4.2093372 -4.2249365 -4.2192631 -4.1937895 -4.1821041 -4.1938982 -4.1953459][-4.2293873 -4.2348337 -4.232132 -4.2120566 -4.1848984 -4.167068 -4.1540184 -4.1534443 -4.1694226 -4.1834927 -4.1852984 -4.17274 -4.175559 -4.1998858 -4.2103133][-4.271183 -4.265841 -4.2474837 -4.2141109 -4.1803803 -4.1609879 -4.1467705 -4.1386428 -4.1405873 -4.1447525 -4.1477113 -4.1428452 -4.1506624 -4.1730428 -4.1867352]]...]
INFO - root - 2017-12-06 07:45:59.810278: step 16910, loss = 2.05, batch loss = 1.99 (36.6 examples/sec; 0.219 sec/batch; 19h:11m:11s remains)
INFO - root - 2017-12-06 07:46:02.027803: step 16920, loss = 2.07, batch loss = 2.01 (36.2 examples/sec; 0.221 sec/batch; 19h:22m:22s remains)
INFO - root - 2017-12-06 07:46:04.254549: step 16930, loss = 2.06, batch loss = 2.00 (35.2 examples/sec; 0.228 sec/batch; 19h:56m:36s remains)
INFO - root - 2017-12-06 07:46:06.550475: step 16940, loss = 2.07, batch loss = 2.01 (36.6 examples/sec; 0.218 sec/batch; 19h:08m:16s remains)
INFO - root - 2017-12-06 07:46:08.817442: step 16950, loss = 2.07, batch loss = 2.01 (35.0 examples/sec; 0.228 sec/batch; 20h:00m:24s remains)
INFO - root - 2017-12-06 07:46:11.117507: step 16960, loss = 2.05, batch loss = 1.99 (36.5 examples/sec; 0.219 sec/batch; 19h:11m:45s remains)
INFO - root - 2017-12-06 07:46:13.378256: step 16970, loss = 2.04, batch loss = 1.99 (35.6 examples/sec; 0.225 sec/batch; 19h:42m:16s remains)
INFO - root - 2017-12-06 07:46:15.640306: step 16980, loss = 2.05, batch loss = 1.99 (34.8 examples/sec; 0.230 sec/batch; 20h:08m:52s remains)
INFO - root - 2017-12-06 07:46:17.867799: step 16990, loss = 2.07, batch loss = 2.01 (34.7 examples/sec; 0.231 sec/batch; 20h:13m:08s remains)
INFO - root - 2017-12-06 07:46:20.121676: step 17000, loss = 2.06, batch loss = 2.00 (36.1 examples/sec; 0.222 sec/batch; 19h:25m:11s remains)
2017-12-06 07:46:20.471492: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.30065 -4.2866926 -4.2674322 -4.2422986 -4.2112265 -4.1816092 -4.1708694 -4.18303 -4.2020822 -4.2144523 -4.2143054 -4.2195706 -4.2378254 -4.2499151 -4.2483296][-4.2957349 -4.275404 -4.2457161 -4.2032313 -4.1504884 -4.1004696 -4.0824265 -4.1007791 -4.1260633 -4.14601 -4.1548266 -4.1775637 -4.2141032 -4.2383323 -4.23867][-4.2882853 -4.2627344 -4.219717 -4.1575594 -4.07711 -3.9988663 -3.9751744 -4.0063133 -4.0434828 -4.0712643 -4.0952115 -4.141211 -4.1977592 -4.2375765 -4.2429118][-4.2843819 -4.258275 -4.2046623 -4.1221023 -4.0140109 -3.9122369 -3.8807535 -3.9262009 -3.986325 -4.0281839 -4.0678844 -4.1290321 -4.1944137 -4.2445936 -4.2569466][-4.2868681 -4.2618337 -4.2043748 -4.1133308 -3.9971442 -3.8879161 -3.8439064 -3.8863323 -3.9717402 -4.0378008 -4.0901265 -4.1487956 -4.20687 -4.2517738 -4.2651329][-4.2950072 -4.2735634 -4.2186451 -4.1328592 -4.0220242 -3.9083898 -3.8296151 -3.8414714 -3.9522724 -4.0543752 -4.1204414 -4.1721611 -4.2110457 -4.2327 -4.2359772][-4.3047752 -4.2861891 -4.2357011 -4.1537051 -4.0423584 -3.9049945 -3.7690711 -3.7460659 -3.8978415 -4.0453506 -4.1234083 -4.1628122 -4.1737552 -4.1630125 -4.1553116][-4.3082066 -4.2912884 -4.2454648 -4.16561 -4.0510736 -3.8968682 -3.733532 -3.7012048 -3.8744743 -4.0348659 -4.1037254 -4.1166739 -4.0956922 -4.0551672 -4.0433068][-4.3091941 -4.2929716 -4.2533145 -4.183157 -4.081285 -3.9520905 -3.8267448 -3.8196194 -3.9588006 -4.0725551 -4.1004786 -4.0708809 -4.0151381 -3.9620829 -3.9654644][-4.3101797 -4.2956314 -4.2627525 -4.2113338 -4.1377435 -4.0522318 -3.9823329 -3.9964218 -4.0856428 -4.1447949 -4.138165 -4.0777688 -4.0087652 -3.9704573 -3.9927065][-4.3057985 -4.2929707 -4.2696328 -4.2403364 -4.1991067 -4.1507483 -4.1191659 -4.1393371 -4.1906586 -4.2134266 -4.1934023 -4.1328449 -4.076757 -4.0594206 -4.0900288][-4.3002424 -4.2867355 -4.2671084 -4.2522554 -4.2375593 -4.2200651 -4.2156825 -4.2378597 -4.2663083 -4.2705336 -4.2463918 -4.2004962 -4.1669116 -4.1678452 -4.1962819][-4.3033447 -4.2899895 -4.2711086 -4.26401 -4.2662058 -4.2697206 -4.2800546 -4.300077 -4.3170447 -4.3165851 -4.2958279 -4.2645836 -4.2465792 -4.2521005 -4.2726336][-4.3131866 -4.3021069 -4.2882056 -4.2870398 -4.2971268 -4.3084297 -4.3189492 -4.3324294 -4.3419623 -4.3386612 -4.3215351 -4.3009458 -4.2909613 -4.2962747 -4.3105793][-4.3193603 -4.312397 -4.3047194 -4.3067923 -4.3182354 -4.3302813 -4.3387403 -4.3464394 -4.3503232 -4.3452678 -4.3318787 -4.3184395 -4.3120961 -4.3159313 -4.32469]]...]
INFO - root - 2017-12-06 07:46:22.728587: step 17010, loss = 2.09, batch loss = 2.03 (36.1 examples/sec; 0.222 sec/batch; 19h:26m:07s remains)
INFO - root - 2017-12-06 07:46:24.989115: step 17020, loss = 2.08, batch loss = 2.02 (34.4 examples/sec; 0.232 sec/batch; 20h:22m:15s remains)
INFO - root - 2017-12-06 07:46:27.252891: step 17030, loss = 2.06, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 19h:47m:32s remains)
INFO - root - 2017-12-06 07:46:29.500331: step 17040, loss = 2.06, batch loss = 2.00 (35.7 examples/sec; 0.224 sec/batch; 19h:39m:41s remains)
INFO - root - 2017-12-06 07:46:31.722513: step 17050, loss = 2.06, batch loss = 2.00 (35.8 examples/sec; 0.224 sec/batch; 19h:35m:33s remains)
INFO - root - 2017-12-06 07:46:33.961462: step 17060, loss = 2.06, batch loss = 2.00 (35.2 examples/sec; 0.227 sec/batch; 19h:55m:36s remains)
INFO - root - 2017-12-06 07:46:36.251722: step 17070, loss = 2.06, batch loss = 2.00 (35.0 examples/sec; 0.228 sec/batch; 20h:00m:06s remains)
INFO - root - 2017-12-06 07:46:38.491230: step 17080, loss = 2.09, batch loss = 2.03 (35.4 examples/sec; 0.226 sec/batch; 19h:47m:22s remains)
INFO - root - 2017-12-06 07:46:40.756709: step 17090, loss = 2.07, batch loss = 2.01 (35.0 examples/sec; 0.228 sec/batch; 20h:01m:02s remains)
INFO - root - 2017-12-06 07:46:43.002806: step 17100, loss = 2.04, batch loss = 1.99 (33.8 examples/sec; 0.237 sec/batch; 20h:44m:53s remains)
2017-12-06 07:46:43.446491: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2462206 -4.2483239 -4.2533193 -4.2591391 -4.2658563 -4.2723522 -4.2765403 -4.2836 -4.2907729 -4.2907248 -4.2839088 -4.2779646 -4.2771759 -4.28209 -4.286757][-4.23705 -4.236279 -4.2340608 -4.2337213 -4.2361383 -4.2428169 -4.2485709 -4.2604461 -4.2767668 -4.2848473 -4.2827992 -4.2790785 -4.277173 -4.2822623 -4.2876225][-4.2144666 -4.2122817 -4.20395 -4.1898875 -4.1769967 -4.1736932 -4.1763148 -4.1956968 -4.229003 -4.251585 -4.2599277 -4.2653718 -4.2643909 -4.2672682 -4.2706251][-4.196568 -4.199913 -4.1899967 -4.1633759 -4.1279807 -4.0956697 -4.072278 -4.0915365 -4.1457715 -4.1914024 -4.2200851 -4.2402053 -4.2444725 -4.2434254 -4.2385368][-4.1930451 -4.2034578 -4.1936274 -4.1591678 -4.100028 -4.0271311 -3.9622989 -3.9663007 -4.0417171 -4.1152115 -4.1663547 -4.1987104 -4.2023392 -4.194346 -4.1801963][-4.1879106 -4.2021642 -4.1931033 -4.1541548 -4.0734925 -3.9607852 -3.8462672 -3.8269849 -3.9282107 -4.0388956 -4.1112003 -4.1498537 -4.150331 -4.1353388 -4.1209702][-4.1778078 -4.1913533 -4.1824651 -4.1420245 -4.0622339 -3.9370046 -3.7881606 -3.7334781 -3.8471661 -3.99526 -4.0886908 -4.1306691 -4.1260686 -4.1058087 -4.0946822][-4.1716623 -4.1850257 -4.1816511 -4.1448641 -4.0802832 -3.9782975 -3.8374348 -3.751677 -3.8482926 -4.004076 -4.1008263 -4.1405034 -4.1344843 -4.1068764 -4.0937762][-4.1802082 -4.1955571 -4.1972675 -4.1684747 -4.1270881 -4.064343 -3.9686389 -3.8879056 -3.9414041 -4.0627232 -4.1422434 -4.1621895 -4.1456218 -4.1089978 -4.0888767][-4.1998816 -4.2107325 -4.2134743 -4.2005081 -4.1840267 -4.1586175 -4.1151476 -4.0625129 -4.0756841 -4.1427536 -4.1898327 -4.1878676 -4.15745 -4.1115694 -4.0828576][-4.2085438 -4.2166533 -4.2182937 -4.2130566 -4.2115617 -4.2084684 -4.1945171 -4.162221 -4.1660476 -4.2055769 -4.2291951 -4.2143955 -4.1750264 -4.1228461 -4.0881844][-4.200038 -4.2078218 -4.2088194 -4.20637 -4.2127733 -4.2230649 -4.2288313 -4.2141342 -4.2167497 -4.2433562 -4.2579522 -4.2463522 -4.2125912 -4.1668205 -4.1335306][-4.1921554 -4.2009783 -4.2090254 -4.2155805 -4.2281337 -4.2446156 -4.2579036 -4.2526026 -4.2492056 -4.2626357 -4.2741461 -4.2707806 -4.25049 -4.2213082 -4.2006016][-4.1955228 -4.2050705 -4.2250056 -4.2433443 -4.2543616 -4.26691 -4.2762766 -4.2711945 -4.2637315 -4.2727013 -4.2852 -4.2836542 -4.2686934 -4.2492633 -4.2396359][-4.2178459 -4.2260442 -4.2483215 -4.2692394 -4.2773051 -4.2835894 -4.2905059 -4.2845612 -4.2759266 -4.2826285 -4.2910242 -4.2859316 -4.2702956 -4.2602568 -4.2598672]]...]
INFO - root - 2017-12-06 07:46:45.689270: step 17110, loss = 2.05, batch loss = 1.99 (33.6 examples/sec; 0.238 sec/batch; 20h:52m:53s remains)
INFO - root - 2017-12-06 07:46:47.952757: step 17120, loss = 2.08, batch loss = 2.02 (35.3 examples/sec; 0.226 sec/batch; 19h:49m:56s remains)
INFO - root - 2017-12-06 07:46:50.200355: step 17130, loss = 2.07, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 19h:26m:42s remains)
INFO - root - 2017-12-06 07:46:52.449615: step 17140, loss = 2.08, batch loss = 2.03 (34.8 examples/sec; 0.230 sec/batch; 20h:06m:38s remains)
INFO - root - 2017-12-06 07:46:54.707176: step 17150, loss = 2.07, batch loss = 2.02 (35.7 examples/sec; 0.224 sec/batch; 19h:39m:19s remains)
INFO - root - 2017-12-06 07:46:57.060159: step 17160, loss = 2.08, batch loss = 2.02 (25.9 examples/sec; 0.309 sec/batch; 27h:03m:42s remains)
INFO - root - 2017-12-06 07:46:59.282557: step 17170, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 19h:06m:59s remains)
INFO - root - 2017-12-06 07:47:01.538366: step 17180, loss = 2.04, batch loss = 1.98 (35.9 examples/sec; 0.223 sec/batch; 19h:31m:56s remains)
INFO - root - 2017-12-06 07:47:03.788201: step 17190, loss = 2.06, batch loss = 2.00 (35.5 examples/sec; 0.225 sec/batch; 19h:43m:25s remains)
INFO - root - 2017-12-06 07:47:06.048012: step 17200, loss = 2.07, batch loss = 2.02 (36.5 examples/sec; 0.219 sec/batch; 19h:11m:54s remains)
2017-12-06 07:47:06.495254: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2775493 -4.2705951 -4.2640686 -4.2632976 -4.2677383 -4.2699671 -4.2654667 -4.2574615 -4.2528658 -4.2530241 -4.2582135 -4.2645783 -4.2654662 -4.2571092 -4.2421427][-4.2818422 -4.27733 -4.270525 -4.2652016 -4.2649937 -4.2654619 -4.2614121 -4.2542062 -4.2508354 -4.2529769 -4.2585516 -4.2643828 -4.2653756 -4.2583785 -4.2463794][-4.2682176 -4.267592 -4.2615256 -4.2525859 -4.2470455 -4.2446356 -4.2428422 -4.2419767 -4.2446628 -4.2508407 -4.258 -4.2618542 -4.2591376 -4.2503057 -4.2417493][-4.226831 -4.2270532 -4.2189956 -4.2059007 -4.1950417 -4.1918645 -4.1955066 -4.2041612 -4.2159214 -4.2292066 -4.2414761 -4.2484665 -4.2460814 -4.2373872 -4.231163][-4.1791544 -4.1835084 -4.174449 -4.1576924 -4.1418371 -4.1348224 -4.137044 -4.1452274 -4.1573091 -4.1730833 -4.18995 -4.2043309 -4.2096763 -4.209969 -4.2127066][-4.15369 -4.1703572 -4.1642532 -4.1460867 -4.1282954 -4.1164269 -4.1094112 -4.1048141 -4.10604 -4.1167455 -4.1342912 -4.1518664 -4.1627626 -4.1733379 -4.1888919][-4.1602721 -4.1854258 -4.1848121 -4.1707234 -4.1523452 -4.1376491 -4.1223369 -4.103466 -4.0914812 -4.0970745 -4.1118064 -4.1266627 -4.1386676 -4.1561584 -4.1810017][-4.185432 -4.2131329 -4.2201862 -4.2134809 -4.1983881 -4.1851735 -4.1670518 -4.1414995 -4.1185384 -4.1161041 -4.1251078 -4.1347065 -4.1469941 -4.1667004 -4.1942363][-4.2289114 -4.2493963 -4.2559509 -4.2522612 -4.243999 -4.2363982 -4.2225132 -4.2000861 -4.1755872 -4.165432 -4.1638632 -4.16668 -4.177268 -4.1946826 -4.2180996][-4.2798176 -4.2904434 -4.2905655 -4.2860551 -4.282083 -4.2771626 -4.268786 -4.2547994 -4.2371979 -4.2251725 -4.2157464 -4.2118216 -4.2170324 -4.2279854 -4.2444062][-4.3207545 -4.3238792 -4.3173733 -4.3114533 -4.310894 -4.3100863 -4.3079872 -4.3025522 -4.2922163 -4.2808914 -4.2698212 -4.2619052 -4.2586131 -4.2609277 -4.2691207][-4.3330379 -4.3343859 -4.3255548 -4.3186951 -4.3191624 -4.3219934 -4.3262982 -4.3276472 -4.3241792 -4.3169503 -4.3097024 -4.3031607 -4.2947531 -4.2890196 -4.2889709][-4.3181887 -4.3232317 -4.3171687 -4.3112059 -4.3115091 -4.3157821 -4.3236566 -4.3285794 -4.3280435 -4.3232145 -4.3201127 -4.3172021 -4.3095226 -4.3016877 -4.2977204][-4.2940049 -4.3049603 -4.3043394 -4.3008132 -4.30076 -4.3047953 -4.3149176 -4.3211288 -4.3202562 -4.3137612 -4.3105736 -4.3093605 -4.3043308 -4.2970104 -4.2927508][-4.2756724 -4.2912707 -4.2964511 -4.2952633 -4.294024 -4.2968926 -4.3078818 -4.3153419 -4.3148146 -4.3057647 -4.2992506 -4.2979822 -4.2957573 -4.2890983 -4.2851081]]...]
INFO - root - 2017-12-06 07:47:08.724402: step 17210, loss = 2.04, batch loss = 1.98 (34.9 examples/sec; 0.229 sec/batch; 20h:04m:11s remains)
INFO - root - 2017-12-06 07:47:10.966597: step 17220, loss = 2.07, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 19h:47m:44s remains)
INFO - root - 2017-12-06 07:47:13.212731: step 17230, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 19h:57m:48s remains)
INFO - root - 2017-12-06 07:47:15.443295: step 17240, loss = 2.04, batch loss = 1.99 (36.1 examples/sec; 0.222 sec/batch; 19h:25m:02s remains)
INFO - root - 2017-12-06 07:47:17.674489: step 17250, loss = 2.09, batch loss = 2.03 (36.2 examples/sec; 0.221 sec/batch; 19h:21m:50s remains)
INFO - root - 2017-12-06 07:47:19.970883: step 17260, loss = 2.06, batch loss = 2.00 (36.0 examples/sec; 0.222 sec/batch; 19h:26m:20s remains)
INFO - root - 2017-12-06 07:47:22.238915: step 17270, loss = 2.07, batch loss = 2.01 (35.5 examples/sec; 0.226 sec/batch; 19h:45m:00s remains)
INFO - root - 2017-12-06 07:47:24.536918: step 17280, loss = 2.06, batch loss = 2.00 (35.1 examples/sec; 0.228 sec/batch; 19h:56m:42s remains)
INFO - root - 2017-12-06 07:47:26.799112: step 17290, loss = 2.08, batch loss = 2.02 (34.2 examples/sec; 0.234 sec/batch; 20h:29m:17s remains)
INFO - root - 2017-12-06 07:47:29.120152: step 17300, loss = 2.05, batch loss = 2.00 (34.9 examples/sec; 0.229 sec/batch; 20h:03m:29s remains)
2017-12-06 07:47:29.496257: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3304825 -4.3254924 -4.3175473 -4.3109336 -4.30728 -4.3054824 -4.3035851 -4.3031969 -4.3054004 -4.3135972 -4.325294 -4.3379493 -4.3488564 -4.3572931 -4.358036][-4.3185554 -4.3107109 -4.3021159 -4.296195 -4.2951527 -4.2965703 -4.2948093 -4.2911029 -4.2889786 -4.2959189 -4.3079119 -4.3217068 -4.3347149 -4.3468285 -4.3488097][-4.3101864 -4.2995377 -4.2909989 -4.2834892 -4.2807384 -4.2798228 -4.2723341 -4.2599287 -4.2518606 -4.2619348 -4.2763224 -4.2918167 -4.3102455 -4.328074 -4.3330007][-4.2979441 -4.2815747 -4.2668653 -4.2500191 -4.2394509 -4.232625 -4.2185068 -4.1954303 -4.1801834 -4.200366 -4.2250457 -4.2466879 -4.2742333 -4.3002324 -4.3111095][-4.2720952 -4.247889 -4.2180552 -4.1832623 -4.1546388 -4.1339455 -4.1155334 -4.0870829 -4.0718541 -4.1118441 -4.1551247 -4.1883588 -4.226841 -4.2654285 -4.283916][-4.2339373 -4.20084 -4.1525559 -4.0923328 -4.0336843 -3.9861703 -3.9616661 -3.9357679 -3.9310122 -4.0037513 -4.0771322 -4.1283789 -4.17599 -4.2257648 -4.253046][-4.193953 -4.1482234 -4.0832028 -3.999665 -3.908721 -3.8276229 -3.7917454 -3.7711082 -3.7839913 -3.8954489 -4.0061178 -4.0818996 -4.1386037 -4.1961417 -4.2318192][-4.1836114 -4.1310759 -4.0611138 -3.9732761 -3.8748231 -3.7851527 -3.7472868 -3.7298124 -3.749702 -3.8716929 -3.9977944 -4.0851421 -4.1446385 -4.2008767 -4.2352018][-4.2099557 -4.1699677 -4.1174521 -4.0522156 -3.9829154 -3.9182756 -3.8881187 -3.8616142 -3.8647249 -3.954668 -4.0587492 -4.1360416 -4.1886067 -4.2343669 -4.2616162][-4.2511005 -4.2281442 -4.1992621 -4.1637621 -4.1293135 -4.0931354 -4.0702372 -4.0399747 -4.0284395 -4.0793896 -4.147841 -4.2046313 -4.2455969 -4.2804017 -4.2990775][-4.28744 -4.277339 -4.2636995 -4.24813 -4.2383161 -4.2221045 -4.2109351 -4.1896653 -4.1772275 -4.2014241 -4.2375774 -4.2701921 -4.2973723 -4.3227296 -4.3335133][-4.3026476 -4.2949519 -4.2907124 -4.2871752 -4.2915335 -4.2896314 -4.2894573 -4.2813454 -4.2750425 -4.2845993 -4.2990437 -4.3138132 -4.3303137 -4.3473063 -4.3531361][-4.3071761 -4.2998614 -4.3008728 -4.3035822 -4.3107519 -4.3151522 -4.3202014 -4.31836 -4.3163724 -4.3197794 -4.3267837 -4.335247 -4.3465004 -4.3582625 -4.3628287][-4.3159232 -4.3108778 -4.3140383 -4.3206129 -4.3277407 -4.3316832 -4.335701 -4.3349576 -4.3335943 -4.3337979 -4.3372912 -4.3430138 -4.3516188 -4.3606725 -4.3653135][-4.3340578 -4.33219 -4.3356376 -4.3422241 -4.3479075 -4.350297 -4.3520851 -4.3514433 -4.3497138 -4.34866 -4.3498363 -4.3531141 -4.358253 -4.3637371 -4.3670135]]...]
INFO - root - 2017-12-06 07:47:31.722196: step 17310, loss = 2.08, batch loss = 2.02 (36.5 examples/sec; 0.219 sec/batch; 19h:10m:43s remains)
INFO - root - 2017-12-06 07:47:33.941637: step 17320, loss = 2.05, batch loss = 2.00 (34.9 examples/sec; 0.229 sec/batch; 20h:03m:19s remains)
INFO - root - 2017-12-06 07:47:36.199344: step 17330, loss = 2.03, batch loss = 1.97 (33.9 examples/sec; 0.236 sec/batch; 20h:38m:07s remains)
INFO - root - 2017-12-06 07:47:38.480893: step 17340, loss = 2.04, batch loss = 1.98 (36.1 examples/sec; 0.222 sec/batch; 19h:25m:09s remains)
INFO - root - 2017-12-06 07:47:40.762482: step 17350, loss = 2.06, batch loss = 2.00 (35.5 examples/sec; 0.225 sec/batch; 19h:42m:31s remains)
INFO - root - 2017-12-06 07:47:43.032002: step 17360, loss = 2.08, batch loss = 2.02 (34.8 examples/sec; 0.230 sec/batch; 20h:07m:26s remains)
INFO - root - 2017-12-06 07:47:45.325829: step 17370, loss = 2.04, batch loss = 1.98 (34.1 examples/sec; 0.234 sec/batch; 20h:31m:17s remains)
INFO - root - 2017-12-06 07:47:47.621094: step 17380, loss = 2.07, batch loss = 2.01 (36.4 examples/sec; 0.220 sec/batch; 19h:14m:06s remains)
INFO - root - 2017-12-06 07:47:49.872997: step 17390, loss = 2.07, batch loss = 2.01 (36.2 examples/sec; 0.221 sec/batch; 19h:21m:22s remains)
INFO - root - 2017-12-06 07:47:52.135141: step 17400, loss = 2.08, batch loss = 2.03 (35.4 examples/sec; 0.226 sec/batch; 19h:46m:59s remains)
2017-12-06 07:47:52.524488: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.27896 -4.22586 -4.1713982 -4.1365128 -4.1378255 -4.1757512 -4.2170949 -4.240489 -4.2525454 -4.2590103 -4.2496057 -4.2309546 -4.2181721 -4.2027025 -4.1880784][-4.2707372 -4.20591 -4.1377783 -4.0893192 -4.0867 -4.1336064 -4.1864619 -4.2188921 -4.2421088 -4.2628112 -4.2624865 -4.2441611 -4.2262845 -4.2022562 -4.1784873][-4.2594237 -4.1861353 -4.1092815 -4.0476317 -4.0295396 -4.07328 -4.1342115 -4.1826391 -4.2173285 -4.2452955 -4.2483468 -4.2312827 -4.2104387 -4.1790018 -4.1481209][-4.2464604 -4.1666679 -4.0852308 -4.0194798 -3.9884288 -4.0207062 -4.0877328 -4.1535087 -4.1989617 -4.2310147 -4.2363491 -4.2161055 -4.186625 -4.1467695 -4.1123981][-4.2334151 -4.1477475 -4.0644207 -3.9972258 -3.9571278 -3.9801753 -4.0540709 -4.1291018 -4.1777496 -4.2087584 -4.2147021 -4.1924334 -4.1566505 -4.1165786 -4.0850344][-4.2210565 -4.1301179 -4.0418172 -3.9693291 -3.9178166 -3.9322481 -4.0124454 -4.0928011 -4.1369953 -4.1657858 -4.1706834 -4.1434736 -4.1106811 -4.0849752 -4.0664463][-4.2079062 -4.1085882 -4.0134435 -3.9311864 -3.8642621 -3.8633268 -3.9445581 -4.0235724 -4.0591693 -4.0833673 -4.0870638 -4.0604887 -4.03624 -4.0337873 -4.0411797][-4.1925793 -4.0834732 -3.978097 -3.8837674 -3.7957692 -3.7759602 -3.8517096 -3.9249253 -3.9505882 -3.9718008 -3.9766483 -3.9565392 -3.9499352 -3.9763117 -4.0117068][-4.183866 -4.0686913 -3.9544971 -3.852767 -3.7552867 -3.7231793 -3.7900162 -3.8563938 -3.8789549 -3.901757 -3.9179668 -3.9152911 -3.9262736 -3.9667549 -4.0105209][-4.1930895 -4.0833869 -3.9724448 -3.8757658 -3.7889807 -3.7611825 -3.8153923 -3.8649931 -3.8853345 -3.9107585 -3.938282 -3.9517274 -3.9714699 -4.0057235 -4.0324492][-4.2227235 -4.1319613 -4.0355158 -3.9526229 -3.8857794 -3.8661356 -3.905488 -3.9413369 -3.9578757 -3.9759183 -3.9989657 -4.016398 -4.0324545 -4.050488 -4.0551081][-4.261178 -4.1960444 -4.1233745 -4.0633416 -4.0167933 -4.0016217 -4.0249071 -4.0483665 -4.0618229 -4.0716271 -4.087256 -4.1014347 -4.1087174 -4.1122575 -4.1024294][-4.2921066 -4.2518148 -4.2042427 -4.1663938 -4.1382418 -4.1280494 -4.1391973 -4.151711 -4.1630058 -4.1717672 -4.183414 -4.1930642 -4.1938076 -4.1887 -4.1736445][-4.3112016 -4.2880068 -4.2581182 -4.2350364 -4.2183566 -4.2118444 -4.2154546 -4.2215252 -4.2308393 -4.2385268 -4.2479959 -4.2555 -4.25531 -4.2493343 -4.2365656][-4.3203688 -4.3070741 -4.2879786 -4.2723551 -4.2632427 -4.25915 -4.2590308 -4.2610588 -4.2662082 -4.2723737 -4.2801867 -4.2870708 -4.2875614 -4.28181 -4.2729092]]...]
INFO - root - 2017-12-06 07:47:54.831804: step 17410, loss = 2.05, batch loss = 1.99 (35.1 examples/sec; 0.228 sec/batch; 19h:58m:16s remains)
INFO - root - 2017-12-06 07:47:57.139625: step 17420, loss = 2.07, batch loss = 2.01 (36.6 examples/sec; 0.219 sec/batch; 19h:08m:51s remains)
INFO - root - 2017-12-06 07:47:59.419655: step 17430, loss = 2.07, batch loss = 2.01 (34.9 examples/sec; 0.229 sec/batch; 20h:04m:49s remains)
INFO - root - 2017-12-06 07:48:01.690038: step 17440, loss = 2.08, batch loss = 2.02 (35.4 examples/sec; 0.226 sec/batch; 19h:48m:07s remains)
INFO - root - 2017-12-06 07:48:04.039368: step 17450, loss = 2.06, batch loss = 2.00 (34.1 examples/sec; 0.235 sec/batch; 20h:31m:31s remains)
INFO - root - 2017-12-06 07:48:06.322752: step 17460, loss = 2.05, batch loss = 1.99 (36.3 examples/sec; 0.221 sec/batch; 19h:17m:57s remains)
INFO - root - 2017-12-06 07:48:08.549964: step 17470, loss = 2.04, batch loss = 1.98 (35.7 examples/sec; 0.224 sec/batch; 19h:36m:40s remains)
INFO - root - 2017-12-06 07:48:10.851408: step 17480, loss = 2.07, batch loss = 2.01 (35.3 examples/sec; 0.227 sec/batch; 19h:50m:31s remains)
INFO - root - 2017-12-06 07:48:13.145704: step 17490, loss = 2.08, batch loss = 2.02 (34.8 examples/sec; 0.230 sec/batch; 20h:07m:20s remains)
INFO - root - 2017-12-06 07:48:15.439554: step 17500, loss = 2.07, batch loss = 2.01 (34.9 examples/sec; 0.229 sec/batch; 20h:03m:12s remains)
2017-12-06 07:48:15.844347: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2286158 -4.2189651 -4.2148733 -4.2199178 -4.2307706 -4.2382908 -4.2434707 -4.2554336 -4.2695012 -4.2799373 -4.2882247 -4.2940392 -4.2952127 -4.2944207 -4.293015][-4.1786189 -4.1685867 -4.1659427 -4.1737347 -4.18869 -4.19804 -4.20036 -4.2100654 -4.228282 -4.2469397 -4.26258 -4.2714524 -4.2723589 -4.2716417 -4.2695007][-4.1316819 -4.1251817 -4.1276975 -4.137373 -4.1523957 -4.1589475 -4.1538506 -4.1553264 -4.1738744 -4.2049046 -4.2335348 -4.2502494 -4.25168 -4.2494822 -4.2445221][-4.0896153 -4.0962548 -4.1079073 -4.1172915 -4.1286397 -4.1272068 -4.1019464 -4.0825443 -4.0992956 -4.1509819 -4.2009993 -4.2310143 -4.23207 -4.2234373 -4.2098947][-4.040679 -4.0689263 -4.0920749 -4.1023417 -4.1084046 -4.0928683 -4.0358939 -3.9764659 -3.9913547 -4.0760803 -4.1535726 -4.1962404 -4.197247 -4.1787472 -4.1469917][-3.9868724 -4.0397887 -4.0750275 -4.0872231 -4.0885043 -4.052609 -3.9519498 -3.8338342 -3.8501747 -3.9826858 -4.0916939 -4.1460323 -4.1494484 -4.1240435 -4.0680265][-3.9701231 -4.0309114 -4.0679541 -4.0792751 -4.0757051 -4.0230732 -3.8845584 -3.7113323 -3.7243741 -3.8977065 -4.02925 -4.0897226 -4.0967503 -4.0720611 -3.9975791][-4.0206585 -4.066319 -4.093749 -4.1025777 -4.1060877 -4.0627928 -3.9352458 -3.7717476 -3.7621422 -3.9081364 -4.020503 -4.0741539 -4.0875425 -4.0748992 -4.0065451][-4.0960183 -4.120471 -4.1349206 -4.1443233 -4.1613269 -4.1409111 -4.0582023 -3.94528 -3.9190142 -4.0028148 -4.0744729 -4.11132 -4.1303086 -4.132503 -4.0890288][-4.1537604 -4.16081 -4.1620722 -4.1687489 -4.1932106 -4.1934009 -4.1509814 -4.0857396 -4.0572658 -4.0916586 -4.1279607 -4.1504893 -4.1706562 -4.182446 -4.160367][-4.183845 -4.1785812 -4.1710091 -4.1745248 -4.2018223 -4.2124681 -4.1944046 -4.1612558 -4.1391997 -4.1471214 -4.1619759 -4.1731482 -4.1900034 -4.2057972 -4.1978779][-4.2128062 -4.2002878 -4.1890578 -4.1922436 -4.2159915 -4.2286978 -4.2217669 -4.2056146 -4.1930556 -4.1919932 -4.1969771 -4.201941 -4.2130818 -4.2264824 -4.2242069][-4.2584581 -4.2455206 -4.2365384 -4.239367 -4.2540617 -4.2628922 -4.2611256 -4.2560444 -4.2517815 -4.2486768 -4.2474184 -4.2472715 -4.2529345 -4.2621317 -4.2616582][-4.2996697 -4.2918558 -4.2871265 -4.2895122 -4.2966824 -4.30044 -4.2995882 -4.2983685 -4.2984982 -4.2964826 -4.2938108 -4.2909236 -4.2924418 -4.2966495 -4.2959518][-4.3181939 -4.3135452 -4.3098974 -4.3106003 -4.3139853 -4.3158431 -4.3155813 -4.3160868 -4.3171654 -4.3162913 -4.3139181 -4.3118286 -4.3122668 -4.3135943 -4.3129058]]...]
INFO - root - 2017-12-06 07:48:18.083975: step 17510, loss = 2.06, batch loss = 2.00 (36.1 examples/sec; 0.222 sec/batch; 19h:23m:59s remains)
INFO - root - 2017-12-06 07:48:20.355516: step 17520, loss = 2.05, batch loss = 1.99 (34.0 examples/sec; 0.235 sec/batch; 20h:33m:56s remains)
INFO - root - 2017-12-06 07:48:22.591362: step 17530, loss = 2.06, batch loss = 2.01 (36.1 examples/sec; 0.222 sec/batch; 19h:23m:46s remains)
INFO - root - 2017-12-06 07:48:24.853465: step 17540, loss = 2.05, batch loss = 1.99 (35.2 examples/sec; 0.227 sec/batch; 19h:53m:38s remains)
INFO - root - 2017-12-06 07:48:27.129815: step 17550, loss = 2.05, batch loss = 2.00 (34.9 examples/sec; 0.229 sec/batch; 20h:04m:26s remains)
INFO - root - 2017-12-06 07:48:29.409323: step 17560, loss = 2.06, batch loss = 2.00 (35.3 examples/sec; 0.227 sec/batch; 19h:48m:58s remains)
INFO - root - 2017-12-06 07:48:31.708328: step 17570, loss = 2.09, batch loss = 2.03 (34.5 examples/sec; 0.232 sec/batch; 20h:18m:21s remains)
INFO - root - 2017-12-06 07:48:33.989733: step 17580, loss = 2.07, batch loss = 2.01 (34.0 examples/sec; 0.235 sec/batch; 20h:33m:54s remains)
INFO - root - 2017-12-06 07:48:36.246200: step 17590, loss = 2.06, batch loss = 2.00 (35.1 examples/sec; 0.228 sec/batch; 19h:57m:06s remains)
INFO - root - 2017-12-06 07:48:38.465432: step 17600, loss = 2.09, batch loss = 2.03 (35.3 examples/sec; 0.226 sec/batch; 19h:48m:44s remains)
2017-12-06 07:48:38.880748: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2200336 -4.2254496 -4.2378497 -4.246664 -4.2530522 -4.256319 -4.2563276 -4.2561426 -4.2587495 -4.2617393 -4.2632108 -4.2634468 -4.26368 -4.2632494 -4.2619019][-4.2028093 -4.2125182 -4.2308106 -4.2450275 -4.25361 -4.2558789 -4.2550859 -4.252995 -4.2559204 -4.2598157 -4.2617435 -4.2620735 -4.2596426 -4.2569 -4.2553706][-4.20921 -4.2211771 -4.2398648 -4.2541852 -4.2612557 -4.2620282 -4.2613883 -4.2592115 -4.2620583 -4.2664857 -4.2681804 -4.266901 -4.2622776 -4.2559357 -4.2524853][-4.2388573 -4.2486844 -4.2597418 -4.264926 -4.26354 -4.2576365 -4.2516932 -4.2465091 -4.2468987 -4.25533 -4.26279 -4.2649288 -4.2613053 -4.2530179 -4.2478037][-4.2784681 -4.2814112 -4.281795 -4.2766428 -4.2673416 -4.257792 -4.2456293 -4.2297359 -4.2202249 -4.2295361 -4.2435427 -4.2523685 -4.2492585 -4.2400746 -4.2357578][-4.2838964 -4.2751021 -4.2633867 -4.2473135 -4.2294736 -4.2142124 -4.1917949 -4.1567507 -4.1321192 -4.1466537 -4.17535 -4.1993141 -4.2017627 -4.1939344 -4.191206][-4.246664 -4.2204843 -4.1917844 -4.1634297 -4.134459 -4.1032443 -4.0606642 -3.9968531 -3.9566412 -3.9881537 -4.0496459 -4.1046829 -4.1179557 -4.1120024 -4.1085205][-4.1761885 -4.1373158 -4.0958991 -4.0574756 -4.0119295 -3.9524837 -3.8824787 -3.7906566 -3.7368073 -3.791961 -3.8865764 -3.9707499 -4.0014944 -4.0059562 -4.0131192][-4.1238208 -4.0830727 -4.0416 -4.0073709 -3.9658656 -3.905936 -3.8424962 -3.765501 -3.7212734 -3.7705412 -3.8509948 -3.9231048 -3.9558742 -3.9675002 -3.985755][-4.1214304 -4.0925341 -4.0665755 -4.0497165 -4.0300784 -3.9977803 -3.9690409 -3.9320374 -3.9078817 -3.9319084 -3.9732826 -4.0110393 -4.029768 -4.0352807 -4.0505137][-4.1607037 -4.1479416 -4.1384511 -4.1363959 -4.1341658 -4.1238647 -4.1172767 -4.104434 -4.0934033 -4.1023917 -4.1210928 -4.137897 -4.1455255 -4.1469088 -4.1570888][-4.2185426 -4.2162542 -4.2162304 -4.2217259 -4.2273545 -4.2288041 -4.23238 -4.2296534 -4.2238832 -4.2254891 -4.2327952 -4.2410283 -4.2440004 -4.2447171 -4.2488885][-4.263061 -4.2624373 -4.2633243 -4.2682886 -4.2740488 -4.2772903 -4.2814164 -4.281498 -4.2789793 -4.2785897 -4.2806511 -4.284708 -4.2857242 -4.2847281 -4.2832723][-4.2847357 -4.2846785 -4.2861342 -4.2901726 -4.2938671 -4.2954783 -4.2972145 -4.2965732 -4.29453 -4.294023 -4.294538 -4.2956638 -4.2949371 -4.2927361 -4.2907333][-4.2850475 -4.2855763 -4.2859321 -4.2872539 -4.2881346 -4.2878137 -4.2878542 -4.2875037 -4.2871013 -4.2877469 -4.2888112 -4.2896829 -4.2892342 -4.2883906 -4.2877526]]...]
INFO - root - 2017-12-06 07:48:41.155491: step 17610, loss = 2.08, batch loss = 2.02 (35.0 examples/sec; 0.229 sec/batch; 20h:00m:01s remains)
INFO - root - 2017-12-06 07:48:43.433076: step 17620, loss = 2.07, batch loss = 2.02 (32.4 examples/sec; 0.247 sec/batch; 21h:35m:17s remains)
INFO - root - 2017-12-06 07:48:45.687597: step 17630, loss = 2.09, batch loss = 2.03 (35.9 examples/sec; 0.223 sec/batch; 19h:28m:38s remains)
INFO - root - 2017-12-06 07:48:47.971950: step 17640, loss = 2.06, batch loss = 2.01 (35.8 examples/sec; 0.224 sec/batch; 19h:33m:52s remains)
INFO - root - 2017-12-06 07:48:50.284189: step 17650, loss = 2.06, batch loss = 2.00 (31.9 examples/sec; 0.251 sec/batch; 21h:57m:47s remains)
INFO - root - 2017-12-06 07:48:52.532282: step 17660, loss = 2.06, batch loss = 2.00 (35.9 examples/sec; 0.223 sec/batch; 19h:29m:32s remains)
INFO - root - 2017-12-06 07:48:54.797483: step 17670, loss = 2.07, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 19h:30m:02s remains)
INFO - root - 2017-12-06 07:48:57.067379: step 17680, loss = 2.06, batch loss = 2.00 (36.5 examples/sec; 0.219 sec/batch; 19h:09m:18s remains)
INFO - root - 2017-12-06 07:48:59.351356: step 17690, loss = 2.05, batch loss = 1.99 (36.1 examples/sec; 0.221 sec/batch; 19h:21m:53s remains)
INFO - root - 2017-12-06 07:49:01.601965: step 17700, loss = 2.08, batch loss = 2.03 (35.4 examples/sec; 0.226 sec/batch; 19h:46m:23s remains)
2017-12-06 07:49:01.930669: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2515225 -4.2459712 -4.2284427 -4.2137136 -4.2139149 -4.2225266 -4.2378964 -4.2509904 -4.2430496 -4.2294278 -4.223052 -4.2352929 -4.2544436 -4.2626772 -4.2488461][-4.231257 -4.2217116 -4.2039075 -4.1866894 -4.1904006 -4.2000895 -4.2128272 -4.2196555 -4.208673 -4.2039924 -4.2113695 -4.2338305 -4.2566719 -4.2670279 -4.2554092][-4.2007985 -4.1885643 -4.1717839 -4.1564841 -4.1628418 -4.1755171 -4.1883388 -4.1914849 -4.1851716 -4.1918697 -4.2133617 -4.243906 -4.2670159 -4.2723126 -4.2576175][-4.1737208 -4.1636853 -4.1469936 -4.1307921 -4.1363516 -4.1482887 -4.1588464 -4.1616192 -4.16606 -4.1863232 -4.2164154 -4.249887 -4.2708321 -4.2708731 -4.2525277][-4.1775851 -4.1694312 -4.1486235 -4.1294227 -4.1339145 -4.1460943 -4.1537895 -4.15845 -4.1707034 -4.1959848 -4.2232542 -4.2490997 -4.2651958 -4.2659421 -4.2485285][-4.1942558 -4.1802263 -4.1561146 -4.132412 -4.1339121 -4.1498737 -4.1608009 -4.1701636 -4.1859879 -4.2058845 -4.2146149 -4.2282095 -4.2481589 -4.2603006 -4.24835][-4.1864047 -4.168365 -4.14581 -4.1205859 -4.1171579 -4.1318483 -4.1487608 -4.16321 -4.1786613 -4.1924782 -4.1914363 -4.2030458 -4.2337637 -4.250452 -4.2402983][-4.1606112 -4.1521373 -4.1397247 -4.1172671 -4.1095815 -4.1212482 -4.1410284 -4.1601534 -4.1750689 -4.1861682 -4.1864982 -4.2020326 -4.2299924 -4.2400851 -4.2277188][-4.1435475 -4.1438332 -4.1441741 -4.1300755 -4.1227136 -4.1303244 -4.1475339 -4.1668463 -4.1785641 -4.1856117 -4.1904287 -4.2082925 -4.2299175 -4.2330518 -4.2198157][-4.1418085 -4.1434431 -4.1494384 -4.1450148 -4.1421223 -4.1443181 -4.1533227 -4.169692 -4.1797481 -4.1830454 -4.1849089 -4.1994553 -4.2181492 -4.224246 -4.2170587][-4.1616745 -4.1610265 -4.1670589 -4.1654754 -4.1617455 -4.1594634 -4.1631532 -4.1760755 -4.1883688 -4.190855 -4.1881785 -4.19271 -4.2059693 -4.2139978 -4.2114911][-4.1784821 -4.1834846 -4.1848626 -4.1811004 -4.17633 -4.1717033 -4.1744041 -4.1862183 -4.200521 -4.2018833 -4.1913643 -4.1824384 -4.1813331 -4.1826763 -4.1791048][-4.1737795 -4.1873779 -4.1896358 -4.1907415 -4.1888552 -4.1837711 -4.1827331 -4.1885223 -4.195868 -4.1920147 -4.1769743 -4.1598845 -4.1460705 -4.1390271 -4.1352572][-4.1575723 -4.17979 -4.1832666 -4.1864223 -4.1934853 -4.1965346 -4.1942887 -4.187746 -4.1825094 -4.1703162 -4.1545205 -4.1388812 -4.1235714 -4.112308 -4.108079][-4.15442 -4.1727419 -4.1707959 -4.1691537 -4.1794295 -4.1905475 -4.189836 -4.1762743 -4.162641 -4.1535487 -4.1447263 -4.1360292 -4.1266403 -4.1175175 -4.1121216]]...]
INFO - root - 2017-12-06 07:49:04.209391: step 17710, loss = 2.08, batch loss = 2.03 (35.9 examples/sec; 0.223 sec/batch; 19h:29m:34s remains)
INFO - root - 2017-12-06 07:49:06.467704: step 17720, loss = 2.08, batch loss = 2.02 (35.8 examples/sec; 0.224 sec/batch; 19h:33m:18s remains)
INFO - root - 2017-12-06 07:49:08.733707: step 17730, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 19h:04m:10s remains)
INFO - root - 2017-12-06 07:49:11.032065: step 17740, loss = 2.06, batch loss = 2.00 (35.8 examples/sec; 0.223 sec/batch; 19h:31m:28s remains)
INFO - root - 2017-12-06 07:49:13.270900: step 17750, loss = 2.08, batch loss = 2.03 (35.0 examples/sec; 0.229 sec/batch; 19h:59m:08s remains)
INFO - root - 2017-12-06 07:49:15.547811: step 17760, loss = 2.06, batch loss = 2.00 (33.0 examples/sec; 0.242 sec/batch; 21h:12m:01s remains)
INFO - root - 2017-12-06 07:49:17.800719: step 17770, loss = 2.05, batch loss = 2.00 (36.0 examples/sec; 0.222 sec/batch; 19h:25m:22s remains)
INFO - root - 2017-12-06 07:49:20.015761: step 17780, loss = 2.05, batch loss = 1.99 (36.8 examples/sec; 0.217 sec/batch; 18h:58m:57s remains)
INFO - root - 2017-12-06 07:49:22.305051: step 17790, loss = 2.08, batch loss = 2.02 (36.5 examples/sec; 0.219 sec/batch; 19h:09m:19s remains)
INFO - root - 2017-12-06 07:49:24.542571: step 17800, loss = 2.04, batch loss = 1.98 (35.2 examples/sec; 0.227 sec/batch; 19h:52m:23s remains)
2017-12-06 07:49:24.906995: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2376475 -4.2371774 -4.2378221 -4.2392378 -4.2388878 -4.2217426 -4.2078714 -4.219676 -4.2460628 -4.2476392 -4.2312765 -4.2273016 -4.2344809 -4.236156 -4.2221031][-4.2451363 -4.2439904 -4.2473888 -4.2492008 -4.2418418 -4.2112327 -4.1858811 -4.19473 -4.2178741 -4.2221622 -4.2157769 -4.2240534 -4.2415557 -4.2483377 -4.236217][-4.2447143 -4.2453456 -4.2467766 -4.2447786 -4.2302418 -4.1899209 -4.1568613 -4.1535249 -4.164649 -4.1744456 -4.1889005 -4.2173414 -4.250001 -4.2672949 -4.2605925][-4.2428093 -4.245584 -4.2380404 -4.22807 -4.20793 -4.1649761 -4.1263256 -4.1063213 -4.1077728 -4.1302352 -4.1655593 -4.2112017 -4.2565184 -4.2844877 -4.2837782][-4.2397823 -4.2484255 -4.2393351 -4.2218976 -4.1915126 -4.1385746 -4.0843859 -4.0414014 -4.0387311 -4.0825653 -4.1401138 -4.1991706 -4.2561007 -4.2946267 -4.2988381][-4.21731 -4.2366352 -4.2344074 -4.2133188 -4.1721854 -4.1009669 -4.0144467 -3.9363914 -3.9282446 -4.0019503 -4.089057 -4.1650143 -4.2354078 -4.2837243 -4.297945][-4.1585903 -4.1912746 -4.1979342 -4.1778388 -4.1311383 -4.0429406 -3.9180326 -3.7931678 -3.7808728 -3.8935869 -4.0153122 -4.1125083 -4.1970062 -4.2556696 -4.2822051][-4.0723643 -4.1232848 -4.1463881 -4.1365042 -4.0988383 -4.0120616 -3.8774374 -3.7332299 -3.7121253 -3.8305511 -3.9603903 -4.068604 -4.1616216 -4.2283206 -4.2641926][-4.0464163 -4.1021843 -4.1373391 -4.1411128 -4.1231551 -4.0627928 -3.9628808 -3.858279 -3.8339632 -3.903281 -3.991931 -4.0801606 -4.16482 -4.2271304 -4.2600832][-4.0864754 -4.1366057 -4.1701016 -4.1755052 -4.168808 -4.1312523 -4.0698519 -4.010891 -3.99205 -4.0192394 -4.06463 -4.1191754 -4.1846905 -4.2374597 -4.2651315][-4.1348929 -4.1758561 -4.2032056 -4.2090821 -4.2069736 -4.1829786 -4.1425891 -4.1102176 -4.0939293 -4.0944963 -4.1102362 -4.1396823 -4.1898417 -4.2352352 -4.2617869][-4.1776886 -4.2095113 -4.2316871 -4.2373695 -4.23752 -4.2221222 -4.1917644 -4.1691618 -4.15165 -4.1351385 -4.1310048 -4.143805 -4.1795406 -4.2163453 -4.2395997][-4.1985965 -4.220726 -4.238112 -4.2466869 -4.2496877 -4.2414379 -4.2223344 -4.2093768 -4.1917825 -4.1684165 -4.1561584 -4.1583939 -4.1798644 -4.2033882 -4.2192802][-4.1802149 -4.2077942 -4.230741 -4.2488432 -4.2611995 -4.2623034 -4.253314 -4.2440314 -4.2251196 -4.2005062 -4.1879425 -4.1871505 -4.197927 -4.2056503 -4.2092781][-4.1534133 -4.1848359 -4.2177052 -4.2476163 -4.2699232 -4.2772741 -4.2716103 -4.2601786 -4.2363615 -4.2061357 -4.1925464 -4.1971664 -4.2053537 -4.2045727 -4.2010107]]...]
INFO - root - 2017-12-06 07:49:27.229472: step 17810, loss = 2.07, batch loss = 2.01 (35.8 examples/sec; 0.224 sec/batch; 19h:32m:27s remains)
INFO - root - 2017-12-06 07:49:29.484278: step 17820, loss = 2.05, batch loss = 2.00 (34.6 examples/sec; 0.231 sec/batch; 20h:11m:05s remains)
INFO - root - 2017-12-06 07:49:31.786937: step 17830, loss = 2.06, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 19h:24m:24s remains)
INFO - root - 2017-12-06 07:49:34.092856: step 17840, loss = 2.07, batch loss = 2.01 (34.2 examples/sec; 0.234 sec/batch; 20h:26m:34s remains)
INFO - root - 2017-12-06 07:49:36.360737: step 17850, loss = 2.07, batch loss = 2.01 (35.0 examples/sec; 0.229 sec/batch; 19h:58m:48s remains)
INFO - root - 2017-12-06 07:49:38.646902: step 17860, loss = 2.07, batch loss = 2.02 (35.8 examples/sec; 0.223 sec/batch; 19h:31m:12s remains)
INFO - root - 2017-12-06 07:49:40.936862: step 17870, loss = 2.07, batch loss = 2.01 (32.4 examples/sec; 0.247 sec/batch; 21h:33m:31s remains)
INFO - root - 2017-12-06 07:49:43.198406: step 17880, loss = 2.08, batch loss = 2.02 (34.0 examples/sec; 0.235 sec/batch; 20h:33m:26s remains)
INFO - root - 2017-12-06 07:49:45.447332: step 17890, loss = 2.06, batch loss = 2.01 (36.1 examples/sec; 0.222 sec/batch; 19h:21m:38s remains)
INFO - root - 2017-12-06 07:49:47.730959: step 17900, loss = 2.07, batch loss = 2.01 (34.7 examples/sec; 0.230 sec/batch; 20h:08m:03s remains)
2017-12-06 07:49:48.093139: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2048087 -4.1992083 -4.2000194 -4.2054625 -4.2150164 -4.2156372 -4.2008433 -4.1790528 -4.158361 -4.162437 -4.2003822 -4.2534981 -4.3005657 -4.3351684 -4.3559623][-4.2401419 -4.2467184 -4.2607856 -4.2756419 -4.2863922 -4.282897 -4.2607665 -4.2350674 -4.2121983 -4.2087703 -4.2331481 -4.2717743 -4.3101583 -4.3381896 -4.3563633][-4.24974 -4.2656307 -4.2886415 -4.3103061 -4.3243513 -4.3215556 -4.2992897 -4.2689838 -4.2394505 -4.2262807 -4.2367759 -4.2652764 -4.3026223 -4.333467 -4.3544936][-4.2310305 -4.254837 -4.2860413 -4.3110781 -4.3281403 -4.3279371 -4.3057771 -4.2710657 -4.234642 -4.2156382 -4.2201719 -4.2472124 -4.2901788 -4.3302383 -4.3576427][-4.2155285 -4.2404184 -4.2702513 -4.2919216 -4.307303 -4.3045464 -4.2788997 -4.2380075 -4.1935611 -4.1711545 -4.1821222 -4.2213607 -4.2765865 -4.3257303 -4.358686][-4.2434716 -4.2640839 -4.2818651 -4.2899728 -4.293242 -4.2767057 -4.2369189 -4.1834235 -4.1257219 -4.097105 -4.1186404 -4.1786737 -4.2502704 -4.3090153 -4.3484259][-4.2905827 -4.3087859 -4.3145595 -4.3076844 -4.2921109 -4.2525716 -4.1919832 -4.1204028 -4.0474186 -4.0108962 -4.0396256 -4.1202707 -4.2084842 -4.280293 -4.32874][-4.3103838 -4.3310242 -4.337719 -4.3295045 -4.3052669 -4.24918 -4.1676416 -4.0758018 -3.9910407 -3.9497247 -3.9821441 -4.0722117 -4.1684437 -4.2492328 -4.3073106][-4.2862964 -4.3120832 -4.327714 -4.3286338 -4.3074937 -4.2474771 -4.1558671 -4.0548897 -3.9686155 -3.926897 -3.9605603 -4.0505795 -4.14666 -4.232182 -4.2965689][-4.2417259 -4.2713327 -4.2978921 -4.3108239 -4.2991705 -4.2453694 -4.1600914 -4.06381 -3.9873614 -3.9481006 -3.9770317 -4.0598617 -4.15026 -4.2353053 -4.3002129][-4.2134404 -4.2468686 -4.2826576 -4.3057723 -4.3022776 -4.2595172 -4.1886206 -4.1069527 -4.042932 -4.0058522 -4.0238266 -4.0905566 -4.1687627 -4.2477412 -4.3098307][-4.2031279 -4.2394733 -4.2835488 -4.3152051 -4.3200374 -4.2919912 -4.23935 -4.1757612 -4.1221743 -4.0874057 -4.0943756 -4.1408625 -4.2025938 -4.2700496 -4.3236084][-4.1941643 -4.2297912 -4.2815781 -4.3243346 -4.3427839 -4.3334165 -4.3025684 -4.2596383 -4.2195945 -4.1908569 -4.1900373 -4.2177525 -4.2587657 -4.3067231 -4.3444123][-4.178483 -4.2086411 -4.264873 -4.3184662 -4.3514004 -4.3610759 -4.3528953 -4.3324256 -4.3097157 -4.2911119 -4.2870097 -4.298831 -4.318758 -4.3449903 -4.3656712][-4.15702 -4.1774507 -4.233016 -4.2944336 -4.3381448 -4.3610697 -4.3686929 -4.3660936 -4.3599195 -4.3535352 -4.35159 -4.3537183 -4.3583407 -4.3664327 -4.37358]]...]
INFO - root - 2017-12-06 07:49:50.370568: step 17910, loss = 2.08, batch loss = 2.02 (35.9 examples/sec; 0.223 sec/batch; 19h:28m:46s remains)
INFO - root - 2017-12-06 07:49:52.596353: step 17920, loss = 2.10, batch loss = 2.04 (36.6 examples/sec; 0.219 sec/batch; 19h:06m:05s remains)
INFO - root - 2017-12-06 07:49:54.866361: step 17930, loss = 2.05, batch loss = 1.99 (35.9 examples/sec; 0.223 sec/batch; 19h:29m:40s remains)
INFO - root - 2017-12-06 07:49:57.184039: step 17940, loss = 2.05, batch loss = 1.99 (34.3 examples/sec; 0.233 sec/batch; 20h:24m:02s remains)
INFO - root - 2017-12-06 07:49:59.507659: step 17950, loss = 2.06, batch loss = 2.00 (35.9 examples/sec; 0.223 sec/batch; 19h:26m:46s remains)
INFO - root - 2017-12-06 07:50:01.738965: step 17960, loss = 2.06, batch loss = 2.00 (37.0 examples/sec; 0.216 sec/batch; 18h:54m:21s remains)
INFO - root - 2017-12-06 07:50:04.069821: step 17970, loss = 2.07, batch loss = 2.01 (33.5 examples/sec; 0.239 sec/batch; 20h:52m:48s remains)
INFO - root - 2017-12-06 07:50:06.329539: step 17980, loss = 2.07, batch loss = 2.01 (36.6 examples/sec; 0.219 sec/batch; 19h:05m:57s remains)
INFO - root - 2017-12-06 07:50:08.574037: step 17990, loss = 2.08, batch loss = 2.02 (32.7 examples/sec; 0.245 sec/batch; 21h:24m:07s remains)
INFO - root - 2017-12-06 07:50:10.846948: step 18000, loss = 2.05, batch loss = 1.99 (35.8 examples/sec; 0.223 sec/batch; 19h:31m:18s remains)
2017-12-06 07:50:11.216965: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2726965 -4.2658248 -4.2603526 -4.2563562 -4.2500615 -4.2449307 -4.2460651 -4.2499247 -4.2516265 -4.2503586 -4.2439003 -4.2391906 -4.2493434 -4.2695446 -4.2881989][-4.2526097 -4.2456217 -4.2399497 -4.2334495 -4.2240095 -4.2160029 -4.2187486 -4.2236738 -4.2224307 -4.2189479 -4.2121811 -4.2067242 -4.2200942 -4.2467318 -4.2710333][-4.2340007 -4.219111 -4.2089219 -4.2020264 -4.1919956 -4.1805849 -4.181447 -4.1851249 -4.1807575 -4.1748962 -4.1651697 -4.1576757 -4.1733022 -4.204814 -4.2369251][-4.205976 -4.180944 -4.1659188 -4.1630015 -4.156723 -4.1426272 -4.1366792 -4.139329 -4.13633 -4.1295905 -4.1141219 -4.1027436 -4.1168127 -4.1519847 -4.1956277][-4.1783571 -4.1471863 -4.1259332 -4.1206965 -4.1123576 -4.0925922 -4.0771227 -4.0791435 -4.0814261 -4.0832682 -4.0681863 -4.0527105 -4.0685558 -4.1095881 -4.1637578][-4.1524839 -4.1225686 -4.0967016 -4.08469 -4.0704865 -4.0366282 -4.0053558 -4.0069947 -4.0253234 -4.0484319 -4.0401955 -4.0269151 -4.0494757 -4.0949802 -4.1526484][-4.1255627 -4.100637 -4.0736542 -4.0530181 -4.026741 -3.9672074 -3.9074244 -3.9180756 -3.9740837 -4.0281334 -4.0373654 -4.0346761 -4.0580835 -4.10106 -4.1546278][-4.1042843 -4.0867443 -4.0596075 -4.0321569 -3.9956505 -3.9077075 -3.8059573 -3.8295782 -3.9317968 -4.0165262 -4.0486264 -4.0538993 -4.0721846 -4.1096458 -4.1556563][-4.1101866 -4.103693 -4.0907774 -4.0644965 -4.0235949 -3.9334617 -3.8186803 -3.8420489 -3.9479489 -4.0342784 -4.0724292 -4.0758896 -4.0850887 -4.1147404 -4.1546669][-4.140975 -4.1478267 -4.1495175 -4.1327643 -4.0971732 -4.0306897 -3.9447947 -3.9521258 -4.0214429 -4.0833135 -4.1096821 -4.1042256 -4.1014915 -4.1200542 -4.1516337][-4.1678867 -4.18281 -4.1936059 -4.1866393 -4.1557765 -4.1049647 -4.043611 -4.0456209 -4.0847707 -4.12038 -4.13637 -4.1267705 -4.1164794 -4.1266537 -4.1482363][-4.1725373 -4.1980395 -4.215405 -4.2150955 -4.1882749 -4.1443572 -4.0975604 -4.0992622 -4.1249175 -4.1428752 -4.1523237 -4.1414056 -4.1286373 -4.1305428 -4.1446176][-4.1560125 -4.1888242 -4.2149777 -4.2160778 -4.1883855 -4.1483593 -4.11509 -4.1194663 -4.1399875 -4.14937 -4.1545 -4.1485534 -4.1408634 -4.142952 -4.1512694][-4.1416149 -4.1711731 -4.1990094 -4.1978865 -4.1687684 -4.1360755 -4.1160126 -4.1212158 -4.1423063 -4.15203 -4.1557922 -4.1540542 -4.1551194 -4.1606612 -4.1649346][-4.1494584 -4.162642 -4.1801887 -4.1787987 -4.1524611 -4.1274142 -4.1128144 -4.1171474 -4.1448884 -4.1663365 -4.1714911 -4.1676269 -4.1733093 -4.179256 -4.1780515]]...]
INFO - root - 2017-12-06 07:50:13.445780: step 18010, loss = 2.08, batch loss = 2.02 (36.5 examples/sec; 0.219 sec/batch; 19h:10m:04s remains)
INFO - root - 2017-12-06 07:50:15.694381: step 18020, loss = 2.08, batch loss = 2.02 (34.9 examples/sec; 0.229 sec/batch; 20h:00m:23s remains)
INFO - root - 2017-12-06 07:50:17.948758: step 18030, loss = 2.06, batch loss = 2.00 (36.2 examples/sec; 0.221 sec/batch; 19h:17m:09s remains)
INFO - root - 2017-12-06 07:50:20.189893: step 18040, loss = 2.10, batch loss = 2.04 (37.5 examples/sec; 0.214 sec/batch; 18h:39m:23s remains)
INFO - root - 2017-12-06 07:50:22.495826: step 18050, loss = 2.06, batch loss = 2.00 (35.5 examples/sec; 0.226 sec/batch; 19h:42m:19s remains)
INFO - root - 2017-12-06 07:50:24.771449: step 18060, loss = 2.10, batch loss = 2.04 (36.8 examples/sec; 0.217 sec/batch; 18h:58m:14s remains)
INFO - root - 2017-12-06 07:50:27.018082: step 18070, loss = 2.07, batch loss = 2.01 (37.1 examples/sec; 0.216 sec/batch; 18h:50m:40s remains)
INFO - root - 2017-12-06 07:50:29.285046: step 18080, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 19h:54m:24s remains)
INFO - root - 2017-12-06 07:50:31.524611: step 18090, loss = 2.07, batch loss = 2.01 (36.5 examples/sec; 0.219 sec/batch; 19h:07m:50s remains)
INFO - root - 2017-12-06 07:50:33.789288: step 18100, loss = 2.07, batch loss = 2.01 (36.6 examples/sec; 0.218 sec/batch; 19h:03m:52s remains)
2017-12-06 07:50:34.159113: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3269114 -4.3273816 -4.326303 -4.3212142 -4.3165164 -4.31547 -4.3163381 -4.3144031 -4.3093309 -4.3052311 -4.3045907 -4.3085809 -4.317009 -4.3262115 -4.3304968][-4.3198538 -4.3176284 -4.3149824 -4.3078523 -4.2993083 -4.2955108 -4.2952762 -4.2925067 -4.2842731 -4.27859 -4.2798324 -4.2879262 -4.3020477 -4.31873 -4.328186][-4.306622 -4.3021274 -4.2994442 -4.2886038 -4.27263 -4.2659817 -4.2693553 -4.2702727 -4.26126 -4.2526031 -4.2518573 -4.2617292 -4.2821803 -4.3086996 -4.3255529][-4.2862649 -4.279429 -4.2739491 -4.252903 -4.2221713 -4.2086916 -4.2158804 -4.2168026 -4.2036839 -4.1901937 -4.1900854 -4.2072349 -4.2405338 -4.2840257 -4.3122334][-4.2689953 -4.2541423 -4.2357535 -4.1974063 -4.1507993 -4.1330242 -4.1435394 -4.14258 -4.1218963 -4.1006746 -4.10574 -4.1344757 -4.1836138 -4.2476454 -4.2905197][-4.2615395 -4.2307038 -4.1880484 -4.1206293 -4.0484486 -4.023623 -4.0359211 -4.0387316 -4.0204678 -4.0076571 -4.0333204 -4.0773721 -4.1389956 -4.2146211 -4.2691646][-4.25362 -4.2109032 -4.14652 -4.0454469 -3.9354625 -3.8967307 -3.915185 -3.9374394 -3.9473882 -3.9649632 -4.0113387 -4.0664716 -4.12906 -4.1982045 -4.2524486][-4.2441592 -4.196444 -4.1236644 -4.0129218 -3.8925481 -3.85237 -3.8712993 -3.9011621 -3.9323757 -3.9733467 -4.0311065 -4.0879192 -4.141963 -4.1934881 -4.2399249][-4.2422318 -4.1987224 -4.1419034 -4.0628324 -3.9799061 -3.953155 -3.9613996 -3.9764266 -3.9984462 -4.0385122 -4.0899343 -4.1337886 -4.1712923 -4.2053337 -4.240159][-4.2531729 -4.2247715 -4.192173 -4.149807 -4.1032124 -4.0871291 -4.0881314 -4.0904794 -4.1004114 -4.1288285 -4.1647873 -4.1917434 -4.2144094 -4.234643 -4.2577219][-4.2753358 -4.2636342 -4.2473111 -4.2231274 -4.1947074 -4.1866636 -4.1883097 -4.1913257 -4.197649 -4.2152743 -4.233808 -4.245182 -4.2575827 -4.2716427 -4.2868333][-4.3014417 -4.2999096 -4.2884603 -4.2703681 -4.2504807 -4.2484207 -4.2561674 -4.262661 -4.2670603 -4.2776461 -4.2852869 -4.2875695 -4.2915311 -4.3000736 -4.3096433][-4.3129978 -4.3150854 -4.3066487 -4.2938762 -4.28333 -4.2885232 -4.3006711 -4.306994 -4.3085446 -4.3140235 -4.3154168 -4.311141 -4.3078103 -4.3097506 -4.3156877][-4.3163333 -4.3174067 -4.3131986 -4.3066921 -4.3024597 -4.3098464 -4.3222532 -4.3270779 -4.3254485 -4.3264117 -4.3244514 -4.3173895 -4.3125768 -4.3127022 -4.3165288][-4.31476 -4.3131967 -4.3082485 -4.30394 -4.3027258 -4.3079305 -4.3161879 -4.3193183 -4.3174376 -4.3170314 -4.3150153 -4.3106756 -4.30931 -4.3119164 -4.3161297]]...]
INFO - root - 2017-12-06 07:50:36.416633: step 18110, loss = 2.08, batch loss = 2.02 (33.8 examples/sec; 0.236 sec/batch; 20h:39m:09s remains)
INFO - root - 2017-12-06 07:50:38.655894: step 18120, loss = 2.07, batch loss = 2.02 (36.2 examples/sec; 0.221 sec/batch; 19h:17m:40s remains)
INFO - root - 2017-12-06 07:50:40.902774: step 18130, loss = 2.02, batch loss = 1.97 (35.0 examples/sec; 0.229 sec/batch; 19h:57m:50s remains)
INFO - root - 2017-12-06 07:50:43.184037: step 18140, loss = 2.07, batch loss = 2.01 (36.1 examples/sec; 0.222 sec/batch; 19h:21m:32s remains)
INFO - root - 2017-12-06 07:50:45.436969: step 18150, loss = 2.06, batch loss = 2.00 (34.6 examples/sec; 0.231 sec/batch; 20h:11m:02s remains)
INFO - root - 2017-12-06 07:50:47.701097: step 18160, loss = 2.04, batch loss = 1.98 (36.3 examples/sec; 0.220 sec/batch; 19h:13m:16s remains)
INFO - root - 2017-12-06 07:50:49.930665: step 18170, loss = 2.06, batch loss = 2.00 (35.9 examples/sec; 0.223 sec/batch; 19h:26m:26s remains)
INFO - root - 2017-12-06 07:50:52.182741: step 18180, loss = 2.04, batch loss = 1.98 (36.5 examples/sec; 0.219 sec/batch; 19h:07m:52s remains)
INFO - root - 2017-12-06 07:50:54.429895: step 18190, loss = 2.06, batch loss = 2.01 (36.1 examples/sec; 0.222 sec/batch; 19h:22m:19s remains)
INFO - root - 2017-12-06 07:50:56.725579: step 18200, loss = 2.09, batch loss = 2.04 (35.9 examples/sec; 0.223 sec/batch; 19h:26m:26s remains)
2017-12-06 07:50:57.164952: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1368594 -4.1302395 -4.1342812 -4.1425662 -4.1528215 -4.1515174 -4.1625309 -4.1650686 -4.1480455 -4.124846 -4.0993056 -4.0680633 -4.0493193 -4.0401344 -4.0382681][-4.1284571 -4.1330938 -4.150393 -4.1630058 -4.1707063 -4.1636934 -4.170536 -4.167625 -4.1459141 -4.1205258 -4.095777 -4.06904 -4.0480595 -4.03677 -4.0412083][-4.1236849 -4.1322055 -4.1514211 -4.161994 -4.1624079 -4.1520576 -4.15988 -4.1591849 -4.1397939 -4.1112809 -4.0842977 -4.0609078 -4.0446339 -4.0400586 -4.0517759][-4.1207995 -4.1262364 -4.1416693 -4.1441908 -4.1331043 -4.1191669 -4.1258459 -4.1267982 -4.1168489 -4.0916476 -4.0654078 -4.0455432 -4.0374045 -4.0445395 -4.06413][-4.1258192 -4.1242647 -4.1307435 -4.1197271 -4.0938344 -4.0708637 -4.073246 -4.0738921 -4.0780225 -4.0602565 -4.0376348 -4.025897 -4.0290108 -4.0465603 -4.0697188][-4.1321096 -4.123857 -4.1243448 -4.101275 -4.0586715 -4.0194154 -4.0130115 -4.0173645 -4.0381122 -4.0290594 -4.0149746 -4.0154014 -4.0239987 -4.0448551 -4.0679193][-4.1391311 -4.1269073 -4.1196327 -4.0903096 -4.0370131 -3.9774671 -3.9535992 -3.9607766 -4.0033674 -4.0112023 -4.0107794 -4.0187049 -4.0201917 -4.0348077 -4.0572529][-4.1433115 -4.1284065 -4.111928 -4.0815687 -4.0279341 -3.9569368 -3.9159448 -3.9273696 -3.9881473 -4.0099235 -4.0202293 -4.0281429 -4.019743 -4.0235558 -4.0460896][-4.1565704 -4.1398544 -4.1156673 -4.08959 -4.05029 -3.9921117 -3.9565225 -3.9717762 -4.0198526 -4.031292 -4.0392132 -4.0495467 -4.0407028 -4.0367317 -4.0611248][-4.1714058 -4.1609068 -4.1395617 -4.1244807 -4.1007214 -4.0640745 -4.04516 -4.0611248 -4.0859866 -4.0820713 -4.086071 -4.0946827 -4.084126 -4.0780387 -4.1037946][-4.1772695 -4.1771436 -4.15817 -4.1443419 -4.1337934 -4.11752 -4.1138778 -4.1295705 -4.1417251 -4.1381207 -4.1418233 -4.1460295 -4.1351643 -4.1286111 -4.1508713][-4.1712589 -4.1786423 -4.1683583 -4.1627321 -4.1642432 -4.15771 -4.1590452 -4.1718192 -4.1813803 -4.1836042 -4.1893282 -4.1917796 -4.1845183 -4.1777921 -4.1922808][-4.1840615 -4.192677 -4.1883192 -4.1846032 -4.1842642 -4.1787767 -4.1806498 -4.1898384 -4.1982236 -4.2055068 -4.215373 -4.219573 -4.2166872 -4.2137141 -4.2231221][-4.1963725 -4.2069216 -4.2070918 -4.2002335 -4.194674 -4.1867342 -4.1853266 -4.1914372 -4.2017255 -4.2145777 -4.2277274 -4.2338724 -4.2314758 -4.2296853 -4.2345009][-4.2152796 -4.2213078 -4.219821 -4.2124939 -4.2074542 -4.2034597 -4.2030053 -4.2086368 -4.2219043 -4.2367964 -4.2483087 -4.2542048 -4.2526774 -4.2497396 -4.2490234]]...]
INFO - root - 2017-12-06 07:50:59.416167: step 18210, loss = 2.06, batch loss = 2.00 (37.0 examples/sec; 0.216 sec/batch; 18h:51m:21s remains)
INFO - root - 2017-12-06 07:51:01.657615: step 18220, loss = 2.07, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 18h:53m:57s remains)
INFO - root - 2017-12-06 07:51:03.978513: step 18230, loss = 2.03, batch loss = 1.98 (34.2 examples/sec; 0.234 sec/batch; 20h:25m:00s remains)
INFO - root - 2017-12-06 07:51:06.287791: step 18240, loss = 2.05, batch loss = 1.99 (34.1 examples/sec; 0.234 sec/batch; 20h:28m:10s remains)
INFO - root - 2017-12-06 07:51:08.574449: step 18250, loss = 2.09, batch loss = 2.03 (32.3 examples/sec; 0.248 sec/batch; 21h:37m:29s remains)
INFO - root - 2017-12-06 07:51:10.851004: step 18260, loss = 2.06, batch loss = 2.00 (35.4 examples/sec; 0.226 sec/batch; 19h:44m:11s remains)
INFO - root - 2017-12-06 07:51:13.109637: step 18270, loss = 2.07, batch loss = 2.01 (34.3 examples/sec; 0.233 sec/batch; 20h:21m:12s remains)
INFO - root - 2017-12-06 07:51:15.383397: step 18280, loss = 2.05, batch loss = 1.99 (36.4 examples/sec; 0.220 sec/batch; 19h:10m:19s remains)
INFO - root - 2017-12-06 07:51:17.667275: step 18290, loss = 2.06, batch loss = 2.00 (36.0 examples/sec; 0.222 sec/batch; 19h:22m:12s remains)
INFO - root - 2017-12-06 07:51:19.933359: step 18300, loss = 2.07, batch loss = 2.01 (34.8 examples/sec; 0.230 sec/batch; 20h:04m:28s remains)
2017-12-06 07:51:20.419043: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1896248 -4.2230072 -4.2457838 -4.2655573 -4.2747149 -4.2654624 -4.2472258 -4.2364593 -4.2388029 -4.2487955 -4.2596827 -4.2715869 -4.2845831 -4.2972207 -4.3076844][-4.2198668 -4.251904 -4.264658 -4.2692676 -4.259716 -4.229804 -4.1937532 -4.1769977 -4.188097 -4.2125616 -4.2378798 -4.2607136 -4.2795653 -4.2958922 -4.3081875][-4.2371821 -4.2653189 -4.2689447 -4.2593813 -4.2295761 -4.1764207 -4.1183834 -4.092649 -4.1130161 -4.1546173 -4.198616 -4.2379346 -4.266994 -4.2908087 -4.3083577][-4.2480674 -4.2721987 -4.2672081 -4.2454019 -4.197681 -4.1212664 -4.0402455 -4.0023808 -4.0316873 -4.0911641 -4.1548858 -4.2112064 -4.2506762 -4.2811065 -4.3042288][-4.2541809 -4.2751827 -4.2634706 -4.2309513 -4.1694369 -4.0738258 -3.9701419 -3.9175482 -3.9554205 -4.0349293 -4.1184835 -4.1899557 -4.2378144 -4.2708778 -4.2962213][-4.2514262 -4.2675004 -4.2491455 -4.2066412 -4.1343484 -4.0257349 -3.9007301 -3.8293905 -3.8767865 -3.9829104 -4.0891662 -4.1746469 -4.229887 -4.2637377 -4.2890673][-4.2399573 -4.2504511 -4.2274308 -4.1775007 -4.0995622 -3.9857831 -3.8449819 -3.7503688 -3.8031359 -3.9387956 -4.067615 -4.1641874 -4.2251091 -4.2611418 -4.2843447][-4.2289782 -4.2370553 -4.2172823 -4.1691575 -4.0965962 -3.9900627 -3.8507404 -3.7448673 -3.7882895 -3.9292705 -4.0620351 -4.1611047 -4.2234921 -4.2593794 -4.2799797][-4.2278991 -4.2376003 -4.2271738 -4.1918516 -4.138504 -4.0574346 -3.9457402 -3.85567 -3.8768172 -3.9858096 -4.0925264 -4.1747489 -4.2287731 -4.2591729 -4.2724986][-4.2243133 -4.2366958 -4.2394929 -4.2251563 -4.197257 -4.1483693 -4.0749073 -4.0103183 -4.013226 -4.0784464 -4.1440158 -4.1964531 -4.2345195 -4.2578735 -4.2646027][-4.2045207 -4.2171717 -4.2315292 -4.2385039 -4.2361727 -4.216887 -4.1762648 -4.1358933 -4.1294885 -4.1613035 -4.1908607 -4.2112179 -4.2289929 -4.2439237 -4.2461195][-4.1640878 -4.1745691 -4.1955733 -4.2194438 -4.2398887 -4.2425947 -4.2252254 -4.2012525 -4.1909509 -4.1991739 -4.2018051 -4.199542 -4.2028093 -4.209383 -4.2080917][-4.1192346 -4.1287503 -4.1529007 -4.1875 -4.2243004 -4.2433205 -4.2427707 -4.2316027 -4.2223921 -4.2160134 -4.1960869 -4.1730847 -4.1640754 -4.1663084 -4.1642036][-4.1030207 -4.1146049 -4.1361041 -4.1719437 -4.2143345 -4.2412257 -4.2526259 -4.2472324 -4.2360368 -4.2194409 -4.186779 -4.1549516 -4.1449604 -4.1492434 -4.1494355][-4.1263628 -4.1397104 -4.1545496 -4.1853967 -4.2261958 -4.2545338 -4.2685356 -4.26427 -4.2522373 -4.2341647 -4.2015882 -4.1721482 -4.16916 -4.1771083 -4.1777706]]...]
INFO - root - 2017-12-06 07:51:22.653792: step 18310, loss = 2.07, batch loss = 2.01 (35.5 examples/sec; 0.226 sec/batch; 19h:41m:19s remains)
INFO - root - 2017-12-06 07:51:24.899770: step 18320, loss = 2.08, batch loss = 2.02 (35.4 examples/sec; 0.226 sec/batch; 19h:43m:50s remains)
INFO - root - 2017-12-06 07:51:27.203057: step 18330, loss = 2.07, batch loss = 2.01 (33.1 examples/sec; 0.241 sec/batch; 21h:03m:57s remains)
INFO - root - 2017-12-06 07:51:29.482183: step 18340, loss = 2.06, batch loss = 2.00 (34.5 examples/sec; 0.232 sec/batch; 20h:14m:03s remains)
INFO - root - 2017-12-06 07:51:31.752221: step 18350, loss = 2.08, batch loss = 2.02 (35.2 examples/sec; 0.227 sec/batch; 19h:49m:54s remains)
INFO - root - 2017-12-06 07:51:34.043503: step 18360, loss = 2.05, batch loss = 1.99 (34.5 examples/sec; 0.232 sec/batch; 20h:12m:31s remains)
INFO - root - 2017-12-06 07:51:36.320966: step 18370, loss = 2.08, batch loss = 2.02 (33.3 examples/sec; 0.240 sec/batch; 20h:56m:00s remains)
INFO - root - 2017-12-06 07:51:38.675174: step 18380, loss = 2.08, batch loss = 2.02 (34.1 examples/sec; 0.234 sec/batch; 20h:27m:21s remains)
INFO - root - 2017-12-06 07:51:40.983640: step 18390, loss = 2.07, batch loss = 2.01 (34.2 examples/sec; 0.234 sec/batch; 20h:22m:59s remains)
INFO - root - 2017-12-06 07:51:43.321992: step 18400, loss = 2.08, batch loss = 2.02 (34.9 examples/sec; 0.229 sec/batch; 19h:58m:56s remains)
2017-12-06 07:51:43.722275: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2171731 -4.2101145 -4.2003345 -4.1919231 -4.1902208 -4.1951842 -4.1967897 -4.1940784 -4.196403 -4.2185555 -4.2474341 -4.2632113 -4.2649994 -4.2697945 -4.277472][-4.2563477 -4.2463889 -4.2291212 -4.21575 -4.2154107 -4.2244492 -4.231194 -4.2392135 -4.2527518 -4.2726684 -4.2864919 -4.2834778 -4.270278 -4.265069 -4.2663603][-4.2759452 -4.27407 -4.2618628 -4.253274 -4.251133 -4.2532053 -4.2562928 -4.2648335 -4.2787027 -4.2874136 -4.2851472 -4.2669029 -4.2491832 -4.2420983 -4.2464652][-4.2715774 -4.2814236 -4.2783294 -4.2750783 -4.2689128 -4.258647 -4.2521186 -4.2554574 -4.2648458 -4.2632542 -4.2498674 -4.2279353 -4.21657 -4.219943 -4.2356129][-4.2512512 -4.2632551 -4.2654824 -4.2646027 -4.2560186 -4.2411184 -4.2284536 -4.2234774 -4.2219782 -4.210999 -4.19255 -4.1780763 -4.1835742 -4.2074289 -4.2368813][-4.2234716 -4.2367988 -4.2413144 -4.2404733 -4.2302437 -4.2134209 -4.1945038 -4.1812167 -4.1723485 -4.1576681 -4.1436563 -4.143157 -4.1723671 -4.2138519 -4.2458973][-4.2033949 -4.2209496 -4.2193484 -4.2075944 -4.1921854 -4.1761127 -4.1542158 -4.1338158 -4.1215 -4.1135569 -4.1159968 -4.1380525 -4.1861873 -4.2298265 -4.2541823][-4.1980429 -4.2102776 -4.1964755 -4.1709161 -4.1463323 -4.1259069 -4.0991364 -4.0748053 -4.059761 -4.065917 -4.0964818 -4.1451797 -4.2021337 -4.2422643 -4.259758][-4.1915226 -4.1923871 -4.1686831 -4.1293917 -4.0939326 -4.0688443 -4.046752 -4.0291071 -4.0167913 -4.0329065 -4.0821524 -4.147 -4.2067776 -4.2443056 -4.2616324][-4.1827245 -4.1797156 -4.1514769 -4.1040816 -4.0615044 -4.0397482 -4.031621 -4.0315108 -4.0280018 -4.0499945 -4.100193 -4.1566434 -4.2082639 -4.2432208 -4.2623177][-4.188695 -4.1874371 -4.1598 -4.1128616 -4.0755258 -4.06561 -4.0740695 -4.0847878 -4.0852308 -4.10605 -4.14403 -4.1811843 -4.2184243 -4.2487364 -4.26793][-4.1923609 -4.1940022 -4.1748819 -4.1417413 -4.1200557 -4.1278934 -4.1503263 -4.1644926 -4.1649981 -4.1758041 -4.1946564 -4.2112546 -4.2359366 -4.2631454 -4.28004][-4.2025037 -4.2036095 -4.1959696 -4.1841435 -4.1818495 -4.1995955 -4.2245731 -4.2351251 -4.232337 -4.2322464 -4.2346053 -4.2386541 -4.2553353 -4.2791491 -4.2939081][-4.2272844 -4.2281919 -4.2288065 -4.2319465 -4.2395687 -4.2571454 -4.2751331 -4.2805061 -4.272913 -4.2659249 -4.2623744 -4.26107 -4.2748184 -4.2962704 -4.3092909][-4.2700539 -4.2699847 -4.2712994 -4.27759 -4.285666 -4.2960215 -4.3035355 -4.3019614 -4.291399 -4.2838387 -4.2798815 -4.2769761 -4.2899733 -4.3105655 -4.3219838]]...]
INFO - root - 2017-12-06 07:51:45.994063: step 18410, loss = 2.04, batch loss = 1.98 (35.9 examples/sec; 0.223 sec/batch; 19h:25m:43s remains)
INFO - root - 2017-12-06 07:51:48.238265: step 18420, loss = 2.06, batch loss = 2.00 (35.4 examples/sec; 0.226 sec/batch; 19h:42m:48s remains)
INFO - root - 2017-12-06 07:51:50.549575: step 18430, loss = 2.06, batch loss = 2.01 (34.1 examples/sec; 0.235 sec/batch; 20h:29m:18s remains)
INFO - root - 2017-12-06 07:51:52.809022: step 18440, loss = 2.08, batch loss = 2.02 (35.5 examples/sec; 0.225 sec/batch; 19h:38m:13s remains)
INFO - root - 2017-12-06 07:51:55.088058: step 18450, loss = 2.05, batch loss = 2.00 (34.1 examples/sec; 0.234 sec/batch; 20h:27m:03s remains)
INFO - root - 2017-12-06 07:51:57.369926: step 18460, loss = 2.07, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 19h:27m:12s remains)
INFO - root - 2017-12-06 07:51:59.741789: step 18470, loss = 2.08, batch loss = 2.02 (34.8 examples/sec; 0.230 sec/batch; 20h:02m:09s remains)
INFO - root - 2017-12-06 07:52:02.017845: step 18480, loss = 2.09, batch loss = 2.03 (35.0 examples/sec; 0.229 sec/batch; 19h:56m:29s remains)
INFO - root - 2017-12-06 07:52:04.281496: step 18490, loss = 2.06, batch loss = 2.01 (35.8 examples/sec; 0.224 sec/batch; 19h:30m:12s remains)
INFO - root - 2017-12-06 07:52:06.576534: step 18500, loss = 2.05, batch loss = 1.99 (34.6 examples/sec; 0.231 sec/batch; 20h:09m:40s remains)
2017-12-06 07:52:06.961713: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2660871 -4.2795916 -4.2938037 -4.3066545 -4.3167329 -4.3219094 -4.3244786 -4.326705 -4.3269057 -4.3240094 -4.3181834 -4.3073559 -4.2897086 -4.2667766 -4.2491961][-4.2591968 -4.2710037 -4.2840424 -4.2975941 -4.3088217 -4.3155708 -4.3214092 -4.3296742 -4.3362651 -4.337348 -4.3335552 -4.3217492 -4.30155 -4.275269 -4.2543054][-4.248383 -4.2535362 -4.2602396 -4.2709718 -4.2822256 -4.2895947 -4.29788 -4.31259 -4.3266459 -4.3335686 -4.3338466 -4.3240914 -4.3041539 -4.2772007 -4.2547083][-4.2345276 -4.2286286 -4.2244453 -4.2284951 -4.2365713 -4.2414403 -4.2489471 -4.2680745 -4.290072 -4.3045678 -4.3112888 -4.3073192 -4.2924118 -4.2687378 -4.248168][-4.2174668 -4.1987667 -4.1818061 -4.176055 -4.1782122 -4.1773434 -4.1781983 -4.1957378 -4.2237372 -4.2467628 -4.2614918 -4.2662449 -4.2613139 -4.2465239 -4.2327905][-4.1962862 -4.166914 -4.138782 -4.12254 -4.1166396 -4.1073833 -4.0954266 -4.1018286 -4.1323667 -4.1650181 -4.1899714 -4.2064056 -4.2152123 -4.2138367 -4.2112379][-4.1711106 -4.1366925 -4.1025786 -4.0782003 -4.0630612 -4.0436964 -4.0162773 -4.0045609 -4.0336976 -4.0774994 -4.1147037 -4.1428146 -4.1658425 -4.1795673 -4.1898713][-4.1489744 -4.1155992 -4.0808 -4.0521855 -4.0290518 -4.0000529 -3.9609604 -3.9346337 -3.9598019 -4.0132713 -4.0610905 -4.0979171 -4.1306667 -4.1563091 -4.1771331][-4.137032 -4.1064572 -4.0740933 -4.0457792 -4.0207167 -3.989938 -3.9506853 -3.9222436 -3.9411259 -3.9944386 -4.0455322 -4.0859065 -4.1214037 -4.1520653 -4.1775856][-4.1425123 -4.1127625 -4.0831838 -4.0583162 -4.0384903 -4.0154309 -3.986155 -3.9648278 -3.9776747 -4.0217862 -4.0676279 -4.1041412 -4.1350503 -4.1634808 -4.1877875][-4.172606 -4.1441236 -4.11751 -4.0969038 -4.083704 -4.0702929 -4.0524058 -4.0392294 -4.0469117 -4.0777068 -4.1129408 -4.1406517 -4.1629176 -4.1844969 -4.2027364][-4.2187152 -4.1972036 -4.1763515 -4.1602526 -4.15174 -4.14488 -4.1345139 -4.1255484 -4.1274371 -4.1437197 -4.1654534 -4.1825409 -4.1954708 -4.20776 -4.2169714][-4.2616196 -4.2548027 -4.2454524 -4.2351942 -4.2285886 -4.2226372 -4.2138095 -4.2039857 -4.1990943 -4.2034025 -4.2130156 -4.2208271 -4.2254667 -4.227829 -4.2257409][-4.2775593 -4.2882862 -4.2926369 -4.2911425 -4.2879491 -4.282042 -4.2717881 -4.2593427 -4.2489409 -4.2443495 -4.2443357 -4.2449303 -4.2428861 -4.2361107 -4.2222762][-4.2526159 -4.2792716 -4.2980547 -4.3085394 -4.3133087 -4.3115926 -4.3035717 -4.2912078 -4.2775307 -4.2661018 -4.2577515 -4.2517676 -4.2435446 -4.2283835 -4.2036738]]...]
INFO - root - 2017-12-06 07:52:09.257030: step 18510, loss = 2.06, batch loss = 2.00 (36.2 examples/sec; 0.221 sec/batch; 19h:16m:00s remains)
INFO - root - 2017-12-06 07:52:11.530774: step 18520, loss = 2.06, batch loss = 2.00 (34.3 examples/sec; 0.233 sec/batch; 20h:20m:53s remains)
INFO - root - 2017-12-06 07:52:13.797806: step 18530, loss = 2.06, batch loss = 2.01 (36.5 examples/sec; 0.219 sec/batch; 19h:06m:23s remains)
INFO - root - 2017-12-06 07:52:16.121320: step 18540, loss = 2.08, batch loss = 2.02 (33.4 examples/sec; 0.239 sec/batch; 20h:52m:07s remains)
INFO - root - 2017-12-06 07:52:18.417309: step 18550, loss = 2.04, batch loss = 1.98 (35.7 examples/sec; 0.224 sec/batch; 19h:30m:58s remains)
INFO - root - 2017-12-06 07:52:20.705786: step 18560, loss = 2.04, batch loss = 1.98 (36.1 examples/sec; 0.222 sec/batch; 19h:19m:26s remains)
INFO - root - 2017-12-06 07:52:22.988627: step 18570, loss = 2.07, batch loss = 2.01 (33.5 examples/sec; 0.239 sec/batch; 20h:48m:17s remains)
INFO - root - 2017-12-06 07:52:25.339967: step 18580, loss = 2.05, batch loss = 1.99 (35.0 examples/sec; 0.229 sec/batch; 19h:56m:39s remains)
INFO - root - 2017-12-06 07:52:27.662145: step 18590, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 19h:52m:23s remains)
INFO - root - 2017-12-06 07:52:29.923661: step 18600, loss = 2.06, batch loss = 2.00 (34.4 examples/sec; 0.233 sec/batch; 20h:18m:08s remains)
2017-12-06 07:52:30.299662: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1872311 -4.1885633 -4.2019429 -4.2184463 -4.2157207 -4.1967249 -4.182394 -4.1769419 -4.1657629 -4.1515222 -4.1518574 -4.17604 -4.2072549 -4.2432423 -4.2856665][-4.1900387 -4.1852689 -4.1955514 -4.2150459 -4.2202 -4.20594 -4.1894565 -4.1750937 -4.1579151 -4.1414127 -4.1468043 -4.174921 -4.2014766 -4.2322445 -4.2726278][-4.1850553 -4.1815848 -4.195375 -4.2184067 -4.2299657 -4.2238569 -4.2078819 -4.1911221 -4.1767831 -4.1607776 -4.1597252 -4.177237 -4.1930432 -4.2154517 -4.25433][-4.175343 -4.1846905 -4.2061453 -4.2294397 -4.2384806 -4.2318759 -4.2145858 -4.1998167 -4.1960635 -4.1936011 -4.1879439 -4.1871433 -4.1862965 -4.200511 -4.2417846][-4.1786427 -4.2005067 -4.224402 -4.2436271 -4.2475281 -4.2324548 -4.2031479 -4.1723657 -4.1724291 -4.1959143 -4.2034645 -4.1978183 -4.1919665 -4.2033691 -4.2439241][-4.1931062 -4.220046 -4.2414808 -4.25868 -4.253747 -4.2218308 -4.1663351 -4.0959997 -4.0907731 -4.1534681 -4.1916723 -4.2002587 -4.2004771 -4.2151389 -4.2515121][-4.2115593 -4.2357903 -4.2537856 -4.269187 -4.2525368 -4.2007589 -4.1099882 -3.9857044 -3.975925 -4.0829444 -4.1581779 -4.1834412 -4.1931605 -4.2126846 -4.2470412][-4.2238717 -4.2447844 -4.2561607 -4.2657642 -4.2449579 -4.1830511 -4.0697064 -3.9094191 -3.9007988 -4.0340748 -4.1259608 -4.1622438 -4.1770015 -4.2017288 -4.2353625][-4.2346659 -4.2502632 -4.2527919 -4.256906 -4.24227 -4.1915035 -4.0911522 -3.9543297 -3.948519 -4.0473194 -4.1141415 -4.1445007 -4.1571865 -4.1815515 -4.2154746][-4.2356238 -4.2478952 -4.2454491 -4.2486382 -4.2468047 -4.217649 -4.1486955 -4.0605741 -4.0567675 -4.1055884 -4.13535 -4.1492095 -4.1524539 -4.1697631 -4.2008352][-4.2297897 -4.2392445 -4.2370453 -4.2439594 -4.2519741 -4.2395625 -4.1970162 -4.1392288 -4.1357059 -4.1548662 -4.1641731 -4.1709328 -4.1640525 -4.1690178 -4.1962929][-4.2358441 -4.2416878 -4.2401485 -4.2473836 -4.2588062 -4.255127 -4.2271647 -4.1888433 -4.1878686 -4.191184 -4.1925964 -4.1985149 -4.1902442 -4.1881046 -4.2133951][-4.2506452 -4.2486477 -4.2467008 -4.2542696 -4.2637339 -4.2623587 -4.2412438 -4.2176995 -4.2199173 -4.21525 -4.2156911 -4.2256932 -4.2206459 -4.2177687 -4.2385621][-4.266551 -4.2592406 -4.2564087 -4.2620749 -4.2701135 -4.2705693 -4.2517309 -4.2343922 -4.2372241 -4.232686 -4.2371335 -4.2518678 -4.2518616 -4.2489715 -4.2639713][-4.2738128 -4.26657 -4.2620058 -4.2664304 -4.272841 -4.2733727 -4.2566919 -4.24386 -4.2448907 -4.2408423 -4.2451787 -4.2594986 -4.261775 -4.2602315 -4.271019]]...]
INFO - root - 2017-12-06 07:52:32.573343: step 18610, loss = 2.08, batch loss = 2.02 (34.8 examples/sec; 0.230 sec/batch; 20h:03m:37s remains)
INFO - root - 2017-12-06 07:52:34.880179: step 18620, loss = 2.05, batch loss = 1.99 (34.1 examples/sec; 0.234 sec/batch; 20h:26m:32s remains)
INFO - root - 2017-12-06 07:52:37.133887: step 18630, loss = 2.06, batch loss = 2.00 (35.4 examples/sec; 0.226 sec/batch; 19h:42m:53s remains)
INFO - root - 2017-12-06 07:52:39.447029: step 18640, loss = 2.07, batch loss = 2.01 (35.6 examples/sec; 0.225 sec/batch; 19h:34m:42s remains)
INFO - root - 2017-12-06 07:52:41.700205: step 18650, loss = 2.08, batch loss = 2.02 (34.1 examples/sec; 0.234 sec/batch; 20h:26m:33s remains)
INFO - root - 2017-12-06 07:52:43.966435: step 18660, loss = 2.05, batch loss = 1.99 (35.4 examples/sec; 0.226 sec/batch; 19h:43m:12s remains)
INFO - root - 2017-12-06 07:52:46.219767: step 18670, loss = 2.07, batch loss = 2.01 (34.1 examples/sec; 0.234 sec/batch; 20h:25m:32s remains)
INFO - root - 2017-12-06 07:52:48.518182: step 18680, loss = 2.04, batch loss = 1.98 (35.6 examples/sec; 0.225 sec/batch; 19h:35m:37s remains)
INFO - root - 2017-12-06 07:52:50.774358: step 18690, loss = 2.06, batch loss = 2.00 (35.8 examples/sec; 0.223 sec/batch; 19h:27m:37s remains)
INFO - root - 2017-12-06 07:52:53.028844: step 18700, loss = 2.08, batch loss = 2.03 (34.8 examples/sec; 0.230 sec/batch; 20h:02m:17s remains)
2017-12-06 07:52:53.499508: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.34939 -4.3155923 -4.2724495 -4.2297068 -4.1937866 -4.1781812 -4.188673 -4.1854477 -4.1842022 -4.2018714 -4.20158 -4.1891413 -4.1839561 -4.20875 -4.2293081][-4.33001 -4.2918115 -4.2466488 -4.20162 -4.1612754 -4.1442194 -4.1604252 -4.1702595 -4.1805377 -4.2043815 -4.2163744 -4.2167492 -4.2118511 -4.2250786 -4.2370038][-4.3049269 -4.2656546 -4.2201819 -4.172904 -4.1342587 -4.1251059 -4.14682 -4.1646705 -4.17737 -4.2005253 -4.2195563 -4.2266626 -4.2212195 -4.2238216 -4.2276478][-4.293159 -4.2620287 -4.2289076 -4.193152 -4.1669731 -4.1670575 -4.186017 -4.1915579 -4.1941342 -4.2094707 -4.2223377 -4.2283792 -4.2251606 -4.225934 -4.2295423][-4.2931204 -4.2778916 -4.2620058 -4.2443509 -4.2332025 -4.23985 -4.2469292 -4.2330608 -4.2235031 -4.2289157 -4.230876 -4.2314339 -4.2320356 -4.2340536 -4.24173][-4.285779 -4.2804561 -4.2735386 -4.2639375 -4.2614493 -4.2716484 -4.2655888 -4.2373013 -4.2223554 -4.228828 -4.2288485 -4.2271533 -4.2248836 -4.228415 -4.2414746][-4.2732482 -4.2703905 -4.2633348 -4.2526383 -4.2450743 -4.2434545 -4.2179031 -4.1675916 -4.1516223 -4.1787062 -4.1955709 -4.2043223 -4.2068524 -4.2137132 -4.2288084][-4.2585096 -4.2543454 -4.245975 -4.2318788 -4.2123737 -4.1866117 -4.1321416 -4.0540094 -4.0396104 -4.102572 -4.1541443 -4.18772 -4.2001624 -4.2086406 -4.2220597][-4.245234 -4.2417607 -4.2343121 -4.2196465 -4.1920671 -4.1507182 -4.0721979 -3.9755394 -3.9614635 -4.0489798 -4.1266918 -4.1819568 -4.2080369 -4.2171288 -4.2235026][-4.2424397 -4.2397451 -4.2350707 -4.2247577 -4.203238 -4.1679921 -4.0986333 -4.0186048 -4.0062718 -4.0786114 -4.1458397 -4.19667 -4.2229662 -4.2282667 -4.2226033][-4.2547784 -4.25242 -4.2502704 -4.2455721 -4.23454 -4.2140121 -4.1723518 -4.121881 -4.1102562 -4.1524129 -4.19554 -4.2297053 -4.2482433 -4.2461939 -4.2309437][-4.2741289 -4.2684255 -4.2641096 -4.2619276 -4.2579412 -4.2480974 -4.2258081 -4.1951361 -4.1846786 -4.2066813 -4.2306719 -4.2515779 -4.2652478 -4.264112 -4.2538724][-4.2759218 -4.2659941 -4.2596097 -4.2603521 -4.26089 -4.2588158 -4.24632 -4.2253804 -4.2178411 -4.2318406 -4.2448444 -4.2579522 -4.2701106 -4.2759018 -4.27763][-4.2606778 -4.2485232 -4.2400784 -4.2396593 -4.2416506 -4.2436872 -4.239768 -4.2310557 -4.23016 -4.2408009 -4.2504721 -4.2610312 -4.2746873 -4.2866158 -4.2945523][-4.2576 -4.2463164 -4.2365656 -4.2312841 -4.2313261 -4.2351685 -4.2379475 -4.2412515 -4.2465529 -4.2557068 -4.2641516 -4.2744989 -4.2893286 -4.3025336 -4.3106647]]...]
INFO - root - 2017-12-06 07:52:55.803769: step 18710, loss = 2.09, batch loss = 2.04 (34.0 examples/sec; 0.235 sec/batch; 20h:29m:45s remains)
INFO - root - 2017-12-06 07:52:58.080548: step 18720, loss = 2.07, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 19h:41m:39s remains)
INFO - root - 2017-12-06 07:53:00.352335: step 18730, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.224 sec/batch; 19h:34m:01s remains)
INFO - root - 2017-12-06 07:53:02.678032: step 18740, loss = 2.04, batch loss = 1.98 (33.5 examples/sec; 0.239 sec/batch; 20h:47m:40s remains)
INFO - root - 2017-12-06 07:53:04.973935: step 18750, loss = 2.08, batch loss = 2.02 (35.6 examples/sec; 0.225 sec/batch; 19h:36m:04s remains)
INFO - root - 2017-12-06 07:53:07.251180: step 18760, loss = 2.07, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 19h:26m:30s remains)
INFO - root - 2017-12-06 07:53:09.521337: step 18770, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 19h:53m:08s remains)
INFO - root - 2017-12-06 07:53:11.749397: step 18780, loss = 2.06, batch loss = 2.00 (36.4 examples/sec; 0.220 sec/batch; 19h:08m:31s remains)
INFO - root - 2017-12-06 07:53:13.998528: step 18790, loss = 2.08, batch loss = 2.02 (36.1 examples/sec; 0.222 sec/batch; 19h:18m:30s remains)
INFO - root - 2017-12-06 07:53:16.226984: step 18800, loss = 2.07, batch loss = 2.01 (36.5 examples/sec; 0.219 sec/batch; 19h:05m:51s remains)
2017-12-06 07:53:16.604525: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2712727 -4.2948685 -4.3013458 -4.2899437 -4.2677474 -4.243978 -4.222261 -4.1978035 -4.1699338 -4.1350641 -4.1088843 -4.0976629 -4.1026025 -4.1348133 -4.1741314][-4.2545366 -4.2709661 -4.2730231 -4.2626534 -4.2457242 -4.2255344 -4.2025008 -4.1749811 -4.1510749 -4.1296391 -4.1159968 -4.1068282 -4.1032639 -4.1194439 -4.1467276][-4.2298036 -4.2342157 -4.2298789 -4.2201219 -4.2065639 -4.1894779 -4.1649389 -4.13707 -4.1239271 -4.1222005 -4.1230588 -4.119853 -4.10906 -4.1068606 -4.1194153][-4.2103324 -4.2026129 -4.1906524 -4.1756024 -4.156992 -4.1346674 -4.1037807 -4.0787954 -4.0816154 -4.1017895 -4.1190753 -4.1257191 -4.1159439 -4.1072469 -4.1103973][-4.2001152 -4.1842589 -4.163064 -4.1351852 -4.1036286 -4.0692244 -4.0275722 -4.00377 -4.023798 -4.06563 -4.0997367 -4.1203117 -4.1215415 -4.121716 -4.1287007][-4.2119112 -4.1943374 -4.1655917 -4.1203046 -4.0679579 -4.0110426 -3.9498148 -3.9146755 -3.9521775 -4.0158486 -4.0667458 -4.10222 -4.1177588 -4.1291952 -4.1423855][-4.2350945 -4.2198911 -4.1858282 -4.1233754 -4.0426993 -3.95346 -3.8569927 -3.7958996 -3.8562188 -3.9535439 -4.0289855 -4.08524 -4.1188211 -4.1432257 -4.1635265][-4.2408433 -4.2289753 -4.1939292 -4.1271696 -4.0341797 -3.9278545 -3.8093233 -3.7271538 -3.8036797 -3.9265916 -4.019424 -4.0897765 -4.1388965 -4.1760969 -4.2021103][-4.2459469 -4.2331591 -4.1991272 -4.1425428 -4.0669122 -3.9860542 -3.8983474 -3.8402786 -3.8909533 -3.9812155 -4.0520849 -4.1081538 -4.1510839 -4.1902809 -4.2166739][-4.2502537 -4.2373576 -4.203908 -4.16184 -4.115334 -4.0739384 -4.032938 -4.0028524 -4.0235324 -4.0657225 -4.0980053 -4.1245475 -4.1488609 -4.1812086 -4.2036119][-4.2362266 -4.2266474 -4.2003236 -4.1745338 -4.1528692 -4.137485 -4.12346 -4.1103406 -4.1141434 -4.1228 -4.1277313 -4.1341839 -4.1470261 -4.1789947 -4.198287][-4.2305675 -4.2288184 -4.2141781 -4.200223 -4.1940169 -4.1926885 -4.188746 -4.1787577 -4.167542 -4.1538019 -4.1420188 -4.1390414 -4.1479583 -4.1785154 -4.1943555][-4.2469854 -4.2506104 -4.2432442 -4.2337141 -4.2320037 -4.2325273 -4.22705 -4.214499 -4.195869 -4.1713295 -4.1526217 -4.1503491 -4.1629448 -4.1907482 -4.2035637][-4.2615309 -4.2676229 -4.2642808 -4.2584949 -4.2586465 -4.2558231 -4.2444434 -4.2255759 -4.2020054 -4.179595 -4.1670918 -4.1770911 -4.2041221 -4.2326541 -4.24292][-4.2776203 -4.2830439 -4.28381 -4.2815394 -4.2819357 -4.2772436 -4.2627506 -4.2421279 -4.2214842 -4.2083344 -4.2057452 -4.2240138 -4.2557678 -4.2803812 -4.2872806]]...]
INFO - root - 2017-12-06 07:53:18.895149: step 18810, loss = 2.05, batch loss = 1.99 (35.3 examples/sec; 0.227 sec/batch; 19h:45m:36s remains)
INFO - root - 2017-12-06 07:53:21.151973: step 18820, loss = 2.08, batch loss = 2.02 (34.2 examples/sec; 0.234 sec/batch; 20h:22m:49s remains)
INFO - root - 2017-12-06 07:53:23.411131: step 18830, loss = 2.06, batch loss = 2.00 (35.0 examples/sec; 0.228 sec/batch; 19h:53m:18s remains)
INFO - root - 2017-12-06 07:53:25.698319: step 18840, loss = 2.05, batch loss = 1.99 (36.1 examples/sec; 0.222 sec/batch; 19h:18m:20s remains)
INFO - root - 2017-12-06 07:53:27.930333: step 18850, loss = 2.05, batch loss = 2.00 (34.9 examples/sec; 0.229 sec/batch; 19h:57m:26s remains)
INFO - root - 2017-12-06 07:53:30.187667: step 18860, loss = 2.06, batch loss = 2.00 (34.4 examples/sec; 0.232 sec/batch; 20h:15m:05s remains)
INFO - root - 2017-12-06 07:53:32.474785: step 18870, loss = 2.08, batch loss = 2.02 (34.5 examples/sec; 0.232 sec/batch; 20h:12m:25s remains)
INFO - root - 2017-12-06 07:53:34.751373: step 18880, loss = 2.06, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 19h:25m:46s remains)
INFO - root - 2017-12-06 07:53:37.014064: step 18890, loss = 2.05, batch loss = 1.99 (34.7 examples/sec; 0.231 sec/batch; 20h:04m:52s remains)
INFO - root - 2017-12-06 07:53:39.289045: step 18900, loss = 2.07, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 19h:21m:32s remains)
2017-12-06 07:53:39.698460: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2767029 -4.2662416 -4.2620196 -4.2676115 -4.2711773 -4.2632627 -4.2539825 -4.2406807 -4.2245846 -4.201653 -4.1973915 -4.2114162 -4.2209706 -4.2191415 -4.2102962][-4.2840729 -4.2756395 -4.27313 -4.2789388 -4.2753634 -4.2509651 -4.2272577 -4.2092814 -4.1941695 -4.1768341 -4.1799917 -4.1978955 -4.2159896 -4.217957 -4.2141318][-4.2831283 -4.2784934 -4.2818351 -4.2877994 -4.2723618 -4.2295847 -4.18763 -4.1645479 -4.1575203 -4.1532145 -4.164114 -4.1871915 -4.210341 -4.2139988 -4.2137504][-4.2815137 -4.2774472 -4.2797832 -4.2781334 -4.2462568 -4.1774807 -4.108088 -4.0798187 -4.0910926 -4.1139088 -4.1340303 -4.1631618 -4.1952567 -4.208818 -4.2114773][-4.2773786 -4.2654629 -4.2559204 -4.235352 -4.18052 -4.0739727 -3.9636993 -3.9365947 -3.9943485 -4.058435 -4.0999656 -4.1390076 -4.1822443 -4.2091093 -4.2112288][-4.2712622 -4.2427425 -4.2066669 -4.1516027 -4.0606675 -3.9038827 -3.7477782 -3.7471547 -3.8918741 -4.0209103 -4.0927243 -4.1393466 -4.187808 -4.2207012 -4.2129188][-4.2551174 -4.2006111 -4.1321611 -4.0485964 -3.9288697 -3.7433798 -3.5613706 -3.608263 -3.8486669 -4.0255027 -4.1108818 -4.1576638 -4.199625 -4.2177768 -4.1843653][-4.232688 -4.15873 -4.0737739 -3.9947786 -3.8994763 -3.7567949 -3.6299825 -3.7112553 -3.9454145 -4.0989342 -4.1600056 -4.1862 -4.2019281 -4.1813078 -4.104353][-4.2034988 -4.1220584 -4.0469909 -4.0126982 -3.9777217 -3.9058185 -3.8530083 -3.9297016 -4.0859561 -4.1804633 -4.2074695 -4.1996455 -4.1802111 -4.1250706 -4.015903][-4.1684813 -4.0862122 -4.0318022 -4.045784 -4.0605822 -4.035984 -4.0239463 -4.0850344 -4.1719704 -4.2202182 -4.2171612 -4.1820312 -4.1377163 -4.063839 -3.947742][-4.1282678 -4.047935 -4.0108018 -4.0553131 -4.1015606 -4.10555 -4.1098161 -4.1548557 -4.1996951 -4.2182245 -4.1969285 -4.1550646 -4.1094785 -4.0401583 -3.9391959][-4.1163111 -4.0507069 -4.0253744 -4.0718837 -4.1220345 -4.1387758 -4.14955 -4.1795917 -4.2027526 -4.2070112 -4.1865692 -4.1566105 -4.1266961 -4.0812912 -4.0050731][-4.1536489 -4.1092505 -4.0887313 -4.1173458 -4.15085 -4.1683011 -4.1801271 -4.1996717 -4.2154694 -4.2182527 -4.2062006 -4.189671 -4.1766872 -4.1501646 -4.1058311][-4.2134385 -4.18954 -4.17499 -4.1847744 -4.1991925 -4.2125978 -4.2247157 -4.2421241 -4.2571321 -4.2591915 -4.2482529 -4.2341604 -4.2242675 -4.2115774 -4.1941733][-4.26186 -4.252984 -4.2434921 -4.2412133 -4.2413783 -4.2473068 -4.260035 -4.2751365 -4.2855592 -4.2851596 -4.2749066 -4.2612467 -4.2544107 -4.2540059 -4.2544684]]...]
INFO - root - 2017-12-06 07:53:42.013821: step 18910, loss = 2.08, batch loss = 2.02 (32.1 examples/sec; 0.249 sec/batch; 21h:43m:35s remains)
INFO - root - 2017-12-06 07:53:44.288406: step 18920, loss = 2.07, batch loss = 2.01 (33.0 examples/sec; 0.242 sec/batch; 21h:05m:17s remains)
INFO - root - 2017-12-06 07:53:46.580135: step 18930, loss = 2.09, batch loss = 2.03 (35.5 examples/sec; 0.226 sec/batch; 19h:39m:18s remains)
INFO - root - 2017-12-06 07:53:48.833363: step 18940, loss = 2.07, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 19h:23m:06s remains)
INFO - root - 2017-12-06 07:53:51.111929: step 18950, loss = 2.03, batch loss = 1.97 (34.4 examples/sec; 0.233 sec/batch; 20h:15m:56s remains)
INFO - root - 2017-12-06 07:53:53.457113: step 18960, loss = 2.08, batch loss = 2.02 (33.0 examples/sec; 0.243 sec/batch; 21h:08m:14s remains)
INFO - root - 2017-12-06 07:53:55.747554: step 18970, loss = 2.07, batch loss = 2.01 (35.8 examples/sec; 0.223 sec/batch; 19h:27m:31s remains)
INFO - root - 2017-12-06 07:53:58.002050: step 18980, loss = 2.07, batch loss = 2.01 (36.6 examples/sec; 0.219 sec/batch; 19h:02m:07s remains)
INFO - root - 2017-12-06 07:54:00.297915: step 18990, loss = 2.07, batch loss = 2.01 (34.7 examples/sec; 0.231 sec/batch; 20h:05m:09s remains)
INFO - root - 2017-12-06 07:54:02.555444: step 19000, loss = 2.06, batch loss = 2.00 (36.7 examples/sec; 0.218 sec/batch; 18h:57m:32s remains)
2017-12-06 07:54:02.994403: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2822332 -4.2920318 -4.2967014 -4.2974792 -4.2907906 -4.2706795 -4.2415109 -4.2131619 -4.195426 -4.1983128 -4.218503 -4.2484474 -4.2782421 -4.3007269 -4.31643][-4.2607374 -4.2784114 -4.2886314 -4.2935305 -4.2876487 -4.2649751 -4.233839 -4.200201 -4.1762819 -4.1748257 -4.1942677 -4.229032 -4.2656293 -4.2923245 -4.31045][-4.25206 -4.2752056 -4.2876067 -4.2952924 -4.2918282 -4.2714887 -4.240407 -4.2020431 -4.1714525 -4.162662 -4.1788325 -4.2185106 -4.2626657 -4.2931824 -4.3116069][-4.2469788 -4.2744412 -4.2870317 -4.2946453 -4.2903671 -4.2709789 -4.2376604 -4.1941657 -4.1578674 -4.1432023 -4.1583571 -4.2069268 -4.2619572 -4.2977252 -4.3167157][-4.2409663 -4.2670159 -4.276341 -4.2784085 -4.268177 -4.245409 -4.2072248 -4.1589708 -4.1197357 -4.1081991 -4.1316872 -4.1932306 -4.2595096 -4.3009381 -4.3213878][-4.225318 -4.2481074 -4.2531557 -4.2454948 -4.2252049 -4.1952753 -4.1493807 -4.0945649 -4.0589371 -4.0615444 -4.099452 -4.172658 -4.2501688 -4.2991052 -4.323019][-4.2061481 -4.2260838 -4.2239165 -4.2054219 -4.1725125 -4.1303954 -4.0721135 -4.0117273 -3.9878721 -4.0117407 -4.0646591 -4.1465163 -4.2333865 -4.2920647 -4.3219886][-4.188848 -4.20967 -4.2039604 -4.1756177 -4.1303678 -4.0739884 -4.00211 -3.9390869 -3.9332337 -3.9779141 -4.0416965 -4.1280751 -4.2205372 -4.2860823 -4.3210673][-4.1880841 -4.2127323 -4.2056837 -4.1702313 -4.11568 -4.0553579 -3.9843307 -3.9286804 -3.937803 -3.9904933 -4.0535541 -4.1335607 -4.2216797 -4.2851 -4.3213754][-4.2057705 -4.2325735 -4.2253923 -4.1908259 -4.1411643 -4.0941873 -4.043355 -4.0051641 -4.0140095 -4.0527263 -4.0989847 -4.1601152 -4.2335157 -4.2893853 -4.3231874][-4.2209659 -4.2489381 -4.2493896 -4.226006 -4.1885195 -4.158433 -4.1269965 -4.0984726 -4.0958614 -4.1132064 -4.1421638 -4.1871443 -4.2445049 -4.2907753 -4.32146][-4.2293043 -4.2555251 -4.2630324 -4.2549243 -4.2324162 -4.2149305 -4.1936221 -4.1640673 -4.1464224 -4.1470208 -4.1666489 -4.20343 -4.2496915 -4.2886944 -4.3159852][-4.2182717 -4.2457967 -4.2611632 -4.2641706 -4.2529888 -4.2450261 -4.2293162 -4.1979904 -4.1710453 -4.1620045 -4.1780128 -4.2107224 -4.2513247 -4.286272 -4.3108335][-4.1961741 -4.2239776 -4.2462068 -4.2568212 -4.2532744 -4.2518697 -4.2423067 -4.2121916 -4.1825771 -4.1680207 -4.1795435 -4.2088842 -4.2451282 -4.279428 -4.3049617][-4.1850271 -4.2091689 -4.2300014 -4.2429495 -4.2479386 -4.2529049 -4.2455053 -4.2152977 -4.1830807 -4.1649904 -4.1728134 -4.1991839 -4.2344742 -4.271584 -4.29952]]...]
INFO - root - 2017-12-06 07:54:05.249386: step 19010, loss = 2.05, batch loss = 1.99 (36.1 examples/sec; 0.221 sec/batch; 19h:16m:51s remains)
INFO - root - 2017-12-06 07:54:07.595009: step 19020, loss = 2.06, batch loss = 2.00 (34.8 examples/sec; 0.230 sec/batch; 20h:02m:16s remains)
