INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "189"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.1-lastlr0.5-clip10000-initconv1-4-baias-relu
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/Relu:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/Relu:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
aiaiaiaiaiaiaiiaaiia [<tf.Variable 'siamese_fc/conv5/def/offset1/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/offset2/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b1/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b2/biases:0' shape=(128,) dtype=float32_ref>]
2017-12-10 07:11:13.849588: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:11:13.849630: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:11:13.849636: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:11:13.849642: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:11:13.849646: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:11:14.199463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-10 07:11:14.199505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-10 07:11:14.199512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-10 07:11:14.199521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-10 07:11:17.232932: step 0, loss = 0.75, batch loss = 0.69 (3.5 examples/sec; 2.269 sec/batch; 209h:34m:30s remains)
2017-12-10 07:11:17.608255: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00021529308 0.0002284061 0.00024078091 0.00025526466 0.00027190446 0.00028683542 0.00030089091 0.000318223 0.00033124338 0.00033196789 0.0003189292 0.00029650412 0.00026787541 0.00023775479 0.00021150643][0.00023207927 0.0002499143 0.00026725585 0.00029012244 0.0003185402 0.00034665022 0.00037431906 0.00040071786 0.0004164486 0.00041157778 0.00038934743 0.00035228953 0.00030821952 0.00026582248 0.00022990517][0.00024585534 0.00026572443 0.00028801017 0.00032127107 0.00036760865 0.00041567854 0.0004583082 0.00049361633 0.00051239575 0.00050094526 0.00046498029 0.00040984148 0.00034816653 0.00029242094 0.00024757328][0.00025970614 0.00028007125 0.00030579636 0.00035104839 0.00041358828 0.00047689682 0.00053220289 0.0005827432 0.0006077186 0.0005888645 0.00053838204 0.00046505337 0.00038540154 0.00031671356 0.00026363129][0.00027349722 0.00029292289 0.00032010389 0.00037585109 0.00045293791 0.00053177262 0.00060562225 0.00068087247 0.0007131357 0.00068180374 0.00061270711 0.00051860715 0.00042019624 0.00033765458 0.0002757028][0.00028957939 0.00030929613 0.00033997989 0.00040368739 0.0004912357 0.0005837176 0.00067895034 0.00078283338 0.00081673486 0.00076126889 0.00066559436 0.00055287569 0.00044003013 0.00034943924 0.00028162554][0.00031129739 0.00033445656 0.00036966638 0.00043707251 0.00052567018 0.00062697654 0.00074920221 0.0008818584 0.00090131967 0.00080391043 0.00068244321 0.000560701 0.00044476098 0.000351303 0.00028229502][0.00033438532 0.00036324744 0.00040237533 0.00046840272 0.00055216823 0.000656352 0.00078938564 0.0009220338 0.00089806475 0.00076619483 0.00064062385 0.00053072092 0.00042749089 0.00034147379 0.000277856][0.00035370997 0.00038624142 0.0004255833 0.00048273147 0.00055071525 0.0006355528 0.00073593535 0.00081706763 0.00076043716 0.00064941135 0.0005565146 0.000476722 0.00039427544 0.00032362025 0.00026949588][0.00036135741 0.00039429392 0.00042950152 0.00047522588 0.00052289892 0.00057519967 0.00062918669 0.00066261378 0.00060904748 0.00053842884 0.000482046 0.00042946715 0.00036658818 0.00030879723 0.00026206917][0.00035796472 0.00039149361 0.00042372817 0.0004591769 0.00048967265 0.00051562645 0.00053770357 0.00054480828 0.0005014803 0.0004582777 0.00042522734 0.00039084852 0.00034203893 0.00029517279 0.00025491806][0.00034648523 0.00037856071 0.00040739457 0.00043182317 0.00044890493 0.00045845658 0.0004639203 0.00045922256 0.00042537291 0.00039903153 0.00038132755 0.00035661514 0.00031798708 0.00028034457 0.00024677906][0.00033139114 0.00035487019 0.00037448274 0.00038601685 0.00039264481 0.00039537181 0.00039748917 0.00039276897 0.0003725971 0.00036213786 0.0003541128 0.00033452176 0.00030080604 0.00026868706 0.00024030835][0.00031449107 0.00032851516 0.00033832085 0.00034123642 0.00034481005 0.00035028058 0.00035675144 0.00036058319 0.0003567441 0.00035829633 0.00035479825 0.00033431442 0.00029753044 0.00026452876 0.00023844888][0.00030299716 0.00031266818 0.0003178051 0.00031962903 0.00032544963 0.00033465592 0.00034739595 0.0003615187 0.00036844387 0.00037510393 0.00037129497 0.00034614955 0.00030396381 0.00026659062 0.00024023968]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.1-lastlr0.5-clip10000-initconv1-4-baias-relu/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.1-lastlr0.5-clip10000-initconv1-4-baias-relu/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-10 07:11:20.629322: step 10, loss = 215.03, batch loss = 4.59 (30.3 examples/sec; 0.264 sec/batch; 24h:22m:16s remains)
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>]
INFO - root - 2017-12-10 07:11:23.228484: step 20, loss = 715.25, batch loss = 1.95 (31.1 examples/sec; 0.258 sec/batch; 23h:47m:17s remains)
INFO - root - 2017-12-10 07:11:25.779208: step 30, loss = 1259.24, batch loss = 0.70 (32.2 examples/sec; 0.248 sec/batch; 22h:54m:41s remains)
INFO - root - 2017-12-10 07:11:28.322180: step 40, loss = 1762.00, batch loss = 0.70 (31.5 examples/sec; 0.254 sec/batch; 23h:26m:58s remains)
INFO - root - 2017-12-10 07:11:30.872533: step 50, loss = 2207.59, batch loss = 0.67 (32.5 examples/sec; 0.246 sec/batch; 22h:44m:26s remains)
INFO - root - 2017-12-10 07:11:33.393262: step 60, loss = 2615.71, batch loss = 12.08 (31.6 examples/sec; 0.253 sec/batch; 23h:22m:27s remains)
INFO - root - 2017-12-10 07:11:35.891125: step 70, loss = 3473.60, batch loss = 0.71 (31.5 examples/sec; 0.254 sec/batch; 23h:27m:14s remains)
INFO - root - 2017-12-10 07:11:38.411192: step 80, loss = 4207.83, batch loss = 0.70 (31.4 examples/sec; 0.255 sec/batch; 23h:31m:06s remains)
INFO - root - 2017-12-10 07:11:40.923820: step 90, loss = 4712.86, batch loss = 0.70 (31.7 examples/sec; 0.252 sec/batch; 23h:16m:06s remains)
INFO - root - 2017-12-10 07:11:43.449370: step 100, loss = 5233.27, batch loss = 0.70 (31.1 examples/sec; 0.258 sec/batch; 23h:47m:19s remains)
2017-12-10 07:11:43.765740: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.20626345 -0.20626344 -0.20626344 -0.20626342 -0.20626342 -0.20626341 -0.20626339 -0.20626339 -0.20626339 -0.20626341 -0.20626342 -0.20626342 -0.20626344 -0.20626344 -0.20626344][-0.20626345 -0.20626345 -0.20626344 -0.20626344 -0.20626344 -0.20626342 -0.20626341 -0.20626341 -0.20626342 -0.20626342 -0.20626342 -0.20626342 -0.20626342 -0.20626344 -0.20626344][-0.20626345 -0.20626345 -0.20626344 -0.20626344 -0.20626344 -0.20626344 -0.20626344 -0.20626342 -0.20626342 -0.20626342 -0.20626342 -0.20626342 -0.20626342 -0.20626344 -0.20626344][-0.20626345 -0.20626345 -0.20626345 -0.20626344 -0.20626344 -0.20626344 -0.20626344 -0.20626344 -0.20626342 -0.20626342 -0.20626342 -0.20626344 -0.20626344 -0.20626344 -0.20626344][-0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626344 -0.20626344 -0.20626344 -0.20626344 -0.20626344 -0.20626344 -0.20626344 -0.20626344 -0.20626344 -0.20626344 -0.20626344][-0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626344 -0.20626344 -0.20626344 -0.20626344 -0.20626344 -0.20626344][-0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626344][-0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345][-0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345][-0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345][-0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345][-0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345][-0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345][-0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345][-0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345 -0.20626345]]...]
INFO - root - 2017-12-10 07:11:46.254653: step 110, loss = 5735.36, batch loss = 0.70 (30.5 examples/sec; 0.262 sec/batch; 24h:13m:24s remains)
INFO - root - 2017-12-10 07:11:48.742426: step 120, loss = 6085.77, batch loss = 0.70 (31.6 examples/sec; 0.253 sec/batch; 23h:20m:41s remains)
INFO - root - 2017-12-10 07:11:51.258059: step 130, loss = 6650.60, batch loss = 0.69 (31.3 examples/sec; 0.255 sec/batch; 23h:34m:11s remains)
INFO - root - 2017-12-10 07:11:53.771505: step 140, loss = 7296.08, batch loss = 0.69 (31.1 examples/sec; 0.257 sec/batch; 23h:45m:23s remains)
INFO - root - 2017-12-10 07:11:56.264385: step 150, loss = 7799.43, batch loss = 0.69 (32.6 examples/sec; 0.246 sec/batch; 22h:41m:02s remains)
INFO - root - 2017-12-10 07:11:58.776048: step 160, loss = 8218.65, batch loss = 0.69 (31.5 examples/sec; 0.254 sec/batch; 23h:27m:54s remains)
INFO - root - 2017-12-10 07:12:01.268755: step 170, loss = 8491.63, batch loss = 0.69 (32.6 examples/sec; 0.245 sec/batch; 22h:38m:21s remains)
INFO - root - 2017-12-10 07:12:03.771734: step 180, loss = 8678.96, batch loss = 0.69 (30.9 examples/sec; 0.259 sec/batch; 23h:56m:13s remains)
INFO - root - 2017-12-10 07:12:06.268676: step 190, loss = 8901.27, batch loss = 0.69 (32.4 examples/sec; 0.247 sec/batch; 22h:47m:46s remains)
INFO - root - 2017-12-10 07:12:08.848711: step 200, loss = 9082.92, batch loss = 0.69 (30.7 examples/sec; 0.261 sec/batch; 24h:05m:22s remains)
2017-12-10 07:12:09.179410: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367][-0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367][-0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367][-0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367][-0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367][-0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367][-0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367][-0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367][-0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367][-0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367][-0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367][-0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367][-0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367][-0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367][-0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367 -0.019282367]]...]
INFO - root - 2017-12-10 07:12:11.676305: step 210, loss = 9179.79, batch loss = 0.69 (32.7 examples/sec; 0.244 sec/batch; 22h:33m:04s remains)
INFO - root - 2017-12-10 07:12:14.151157: step 220, loss = 9219.68, batch loss = 0.69 (30.9 examples/sec; 0.259 sec/batch; 23h:53m:53s remains)
INFO - root - 2017-12-10 07:12:16.651727: step 230, loss = 9318.40, batch loss = 0.69 (32.3 examples/sec; 0.247 sec/batch; 22h:49m:48s remains)
INFO - root - 2017-12-10 07:12:19.155822: step 240, loss = 9521.01, batch loss = 0.69 (32.7 examples/sec; 0.245 sec/batch; 22h:34m:44s remains)
INFO - root - 2017-12-10 07:12:21.667987: step 250, loss = 9737.01, batch loss = 0.69 (31.3 examples/sec; 0.255 sec/batch; 23h:33m:50s remains)
INFO - root - 2017-12-10 07:12:24.171502: step 260, loss = 9897.82, batch loss = 0.69 (32.6 examples/sec; 0.246 sec/batch; 22h:40m:41s remains)
INFO - root - 2017-12-10 07:12:26.659440: step 270, loss = 10136.70, batch loss = 0.69 (32.8 examples/sec; 0.244 sec/batch; 22h:30m:43s remains)
INFO - root - 2017-12-10 07:12:29.157562: step 280, loss = 10400.44, batch loss = 0.69 (31.9 examples/sec; 0.250 sec/batch; 23h:06m:35s remains)
INFO - root - 2017-12-10 07:12:31.701315: step 290, loss = 10703.21, batch loss = 0.69 (29.9 examples/sec; 0.268 sec/batch; 24h:43m:21s remains)
INFO - root - 2017-12-10 07:12:34.220967: step 300, loss = 10884.57, batch loss = 0.69 (31.9 examples/sec; 0.251 sec/batch; 23h:10m:14s remains)
2017-12-10 07:12:34.526143: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0015927685 -0.0015927685 -0.0015927682 -0.0015927681 -0.0015927681 -0.0015927681 -0.0015927681 -0.0015927682 -0.0015927656 -0.001592766 -0.0015927665 -0.0015927656 -0.0015927653 -0.0015927642 -0.0015927664][-0.001592768 -0.0015927679 -0.0015927667 -0.0015927667 -0.0015927671 -0.0015927671 -0.0015927671 -0.0015927665 -0.0015927636 -0.0015927654 -0.001592766 -0.0015927652 -0.0015927649 -0.001592764 -0.0015927649][-0.001592768 -0.001592768 -0.0015927679 -0.0015927673 -0.0015927671 -0.0015927672 -0.0015927673 -0.0015927673 -0.0015927653 -0.001592766 -0.0015927658 -0.0015927649 -0.0015927645 -0.0015927636 -0.0015927654][-0.0015927678 -0.0015927678 -0.0015927674 -0.0015927671 -0.0015927673 -0.0015927673 -0.0015927673 -0.0015927673 -0.0015927654 -0.0015927631 -0.0015927658 -0.0015927649 -0.0015927646 -0.0015927635 -0.0015927646][-0.0015927661 -0.0015927661 -0.0015927659 -0.0015927657 -0.0015927656 -0.0015927637 -0.0015927666 -0.0015927667 -0.0015927674 -0.001592768 -0.0015927679 -0.001592767 -0.0015927667 -0.0015927666 -0.0015927666][-0.0015927661 -0.001592766 -0.0015927659 -0.0015927656 -0.0015927651 -0.0015927651 -0.0015927667 -0.0015927673 -0.0015927674 -0.0015927679 -0.0015927679 -0.001592767 -0.0015927656 -0.0015927664 -0.0015927667][-0.0015927661 -0.001592766 -0.0015927659 -0.0015927654 -0.0015927646 -0.0015927654 -0.0015927673 -0.0015927673 -0.0015927675 -0.001592768 -0.0015927671 -0.0015927678 -0.0015927672 -0.0015927679 -0.0015927679][-0.0015927666 -0.0015927665 -0.0015927665 -0.0015927666 -0.0015927666 -0.0015927667 -0.0015927684 -0.0015927684 -0.0015927684 -0.0015927677 -0.0015927675 -0.0015927671 -0.0015927677 -0.0015927678 -0.0015927666][-0.0015927656 -0.0015927654 -0.0015927653 -0.0015927637 -0.0015927654 -0.0015927668 -0.0015927684 -0.0015927684 -0.0015927685 -0.001592768 -0.0015927679 -0.001592768 -0.001592768 -0.0015927681 -0.0015927681][-0.0015927656 -0.0015927643 -0.0015927654 -0.0015927654 -0.0015927665 -0.0015927668 -0.0015927684 -0.0015927677 -0.001592766 -0.0015927656 -0.0015927657 -0.0015927657 -0.0015927652 -0.0015927653 -0.0015927681][-0.0015927671 -0.001592767 -0.0015927659 -0.0015927673 -0.0015927681 -0.0015927685 -0.0015927685 -0.0015927678 -0.0015927661 -0.0015927657 -0.0015927657 -0.0015927657 -0.0015927657 -0.0015927665 -0.0015927681][-0.0015927645 -0.0015927644 -0.0015927642 -0.001592763 -0.0015927649 -0.0015927666 -0.0015927677 -0.0015927671 -0.0015927644 -0.0015927656 -0.0015927657 -0.0015927657 -0.0015927657 -0.0015927665 -0.0015927681][-0.0015927643 -0.0015927644 -0.0015927644 -0.0015927643 -0.001592766 -0.0015927671 -0.0015927678 -0.0015927672 -0.0015927656 -0.0015927658 -0.0015927653 -0.0015927658 -0.0015927658 -0.0015927665 -0.0015927681][-0.0015927639 -0.0015927639 -0.0015927644 -0.0015927643 -0.001592766 -0.0015927673 -0.0015927678 -0.0015927672 -0.0015927656 -0.0015927659 -0.0015927657 -0.0015927658 -0.0015927658 -0.0015927664 -0.0015927672][-0.0015927647 -0.0015927647 -0.0015927646 -0.0015927646 -0.0015927596 -0.0015927588 -0.0015927616 -0.001592761 -0.0015927594 -0.0015927549 -0.0015927657 -0.0015927657 -0.0015927658 -0.0015927664 -0.0015927681]]...]
INFO - root - 2017-12-10 07:12:37.062343: step 310, loss = 10962.42, batch loss = 0.69 (28.6 examples/sec; 0.280 sec/batch; 25h:48m:37s remains)
INFO - root - 2017-12-10 07:12:39.608216: step 320, loss = 11023.21, batch loss = 0.69 (31.9 examples/sec; 0.251 sec/batch; 23h:06m:52s remains)
INFO - root - 2017-12-10 07:12:42.162842: step 330, loss = 11087.48, batch loss = 0.69 (31.1 examples/sec; 0.257 sec/batch; 23h:43m:38s remains)
INFO - root - 2017-12-10 07:12:44.718979: step 340, loss = 11121.83, batch loss = 0.69 (30.8 examples/sec; 0.260 sec/batch; 23h:58m:57s remains)
INFO - root - 2017-12-10 07:12:47.275572: step 350, loss = 11145.97, batch loss = 0.69 (31.1 examples/sec; 0.257 sec/batch; 23h:43m:24s remains)
INFO - root - 2017-12-10 07:12:49.863063: step 360, loss = 11169.29, batch loss = 0.69 (31.3 examples/sec; 0.256 sec/batch; 23h:37m:03s remains)
INFO - root - 2017-12-10 07:12:52.420833: step 370, loss = 11199.26, batch loss = 0.69 (31.6 examples/sec; 0.253 sec/batch; 23h:21m:16s remains)
INFO - root - 2017-12-10 07:12:54.985885: step 380, loss = 11225.82, batch loss = 0.70 (31.8 examples/sec; 0.252 sec/batch; 23h:14m:26s remains)
INFO - root - 2017-12-10 07:12:57.547139: step 390, loss = 11268.55, batch loss = 0.68 (31.8 examples/sec; 0.252 sec/batch; 23h:12m:12s remains)
INFO - root - 2017-12-10 07:13:00.126433: step 400, loss = 11387.49, batch loss = 0.69 (30.8 examples/sec; 0.260 sec/batch; 23h:58m:44s remains)
2017-12-10 07:13:00.424094: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952][-0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952][-0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952][-0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952][-0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952][-0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952][-0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952][-0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952][-0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952][-0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952][-0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952][-0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952][-0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952][-0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952][-0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952 -0.021264952]]...]
INFO - root - 2017-12-10 07:13:02.981234: step 410, loss = 11490.67, batch loss = 0.69 (31.8 examples/sec; 0.252 sec/batch; 23h:13m:17s remains)
INFO - root - 2017-12-10 07:13:05.537738: step 420, loss = 11533.81, batch loss = 0.69 (31.3 examples/sec; 0.255 sec/batch; 23h:32m:24s remains)
INFO - root - 2017-12-10 07:13:08.137551: step 430, loss = 11560.62, batch loss = 0.69 (29.6 examples/sec; 0.270 sec/batch; 24h:53m:28s remains)
INFO - root - 2017-12-10 07:13:10.717477: step 440, loss = 11594.40, batch loss = 0.69 (31.3 examples/sec; 0.256 sec/batch; 23h:36m:26s remains)
INFO - root - 2017-12-10 07:13:13.349405: step 450, loss = 11641.73, batch loss = 0.69 (31.5 examples/sec; 0.254 sec/batch; 23h:27m:20s remains)
INFO - root - 2017-12-10 07:13:15.937443: step 460, loss = 11664.49, batch loss = 0.69 (30.7 examples/sec; 0.261 sec/batch; 24h:02m:26s remains)
INFO - root - 2017-12-10 07:13:18.541763: step 470, loss = 11675.17, batch loss = 0.69 (31.3 examples/sec; 0.256 sec/batch; 23h:34m:19s remains)
INFO - root - 2017-12-10 07:13:21.141096: step 480, loss = 11716.91, batch loss = 0.70 (30.8 examples/sec; 0.259 sec/batch; 23h:55m:01s remains)
INFO - root - 2017-12-10 07:13:23.716782: step 490, loss = 11806.36, batch loss = 0.69 (30.7 examples/sec; 0.261 sec/batch; 24h:01m:37s remains)
INFO - root - 2017-12-10 07:13:26.327218: step 500, loss = 11869.22, batch loss = 0.70 (30.7 examples/sec; 0.260 sec/batch; 24h:00m:56s remains)
2017-12-10 07:13:26.729592: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0081817182 -0.0081817089 -0.00818171 -0.00818171 -0.0081817126 -0.0081817107 -0.0081817126 -0.0081817033 -0.0081817079 -0.0081817117 -0.0081817154 -0.0081817107 -0.007997849 -0.0078620575 -0.0079972968][-0.0081817154 -0.0081817079 -0.00818171 -0.0081817089 -0.0081817154 -0.0081817172 -0.0081817135 -0.008181707 -0.0081817107 -0.0081817182 -0.0081817154 -0.0081817172 -0.0081793144 -0.00020163786 0.025332151][-0.0081817172 -0.0081817117 -0.00818171 -0.0081817154 -0.0081817126 -0.0081817079 -0.0081817126 -0.00818171 -0.0081817135 -0.0081817182 -0.0081817182 -0.0081817135 -0.0081817089 -0.0072161751 0.0092549138][-0.0081817219 -0.0081817117 -0.0081817275 -0.0081817051 -0.0081817107 -0.0081817079 -0.0081817172 -0.0081817126 -0.0081817089 -0.0081817145 -0.0081817182 -0.0081817145 -0.0081817284 -0.007846483 0.027369713][-0.0081817275 -0.0081817266 -0.0081817266 -0.0081817219 -0.0081817219 -0.00818172 -0.0081817247 -0.0081817247 -0.0081817247 -0.0081817256 -0.0081817228 -0.0081817238 -0.00818174 0.0046534557 0.034845665][-0.0081817359 -0.0081817508 -0.0081817275 -0.0081817452 -0.0081817219 -0.0081817517 -0.0081817228 -0.0081817228 -0.0081817033 -0.0081817564 -0.0081817582 -0.0081817685 -0.0081807347 -0.0064528985 0.031040074][0.0040757572 0.0065928288 0.0066180406 -0.0054649473 -0.0023643859 -0.0019970983 -0.0054633464 -0.0081817675 -0.0053373738 -0.0039879861 -0.0067375209 -0.0076415706 -0.0077236728 -0.006869664 0.043152995][-0.0081713721 -0.004284434 -0.0069174753 0.0001269877 -0.0060625374 0.017830735 0.011340488 0.027868191 0.008308772 0.031974152 0.0059303977 -0.00727591 0.0013427176 0.025931938 0.077364281][0.04558523 0.03323824 0.020372305 0.1037279 0.011992643 0.12171971 0.01667084 0.057746 0.080571927 0.084003024 0.062575676 0.08597526 0.11124352 0.069576994 0.073018193][0.058443643 0.03599178 0.037442341 0.044221707 -0.0049155182 0.027508257 -0.006168589 0.036531918 0.016626835 0.15126979 0.16021176 0.22639108 0.15230171 0.19198574 0.048716187][0.15611658 0.10694961 0.26382163 0.26265085 0.006718073 0.20957726 -0.0070613124 0.14664602 -0.0078233555 0.20536917 -0.0062952791 0.2155599 0.22035742 0.22473048 0.23884855][0.2537286 0.2893309 0.26058188 0.12413257 0.066729017 -0.0054859603 -0.0067907758 -0.0066683604 -0.0066808588 -0.0068363291 -0.0076375911 -0.0074437032 -0.0074177193 0.1223216 0.096422449][-0.0071190791 -0.0079129254 -0.0075269095 -0.007930656 -0.0071880007 -0.0067704581 -0.0039230427 -0.0038379109 -0.00373657 -0.0056468658 -0.0042876303 -0.0068865446 -0.005840796 -0.0031513809 -0.0011543124][-0.0012974404 -0.0061831945 -0.0026775612 -0.0070010652 -0.0042096665 -0.0061478657 -0.0019502118 -0.0056324666 -0.00063537341 -0.00099900085 -0.00090416381 -0.00064121746 -0.0003347313 0.00086190458 0.00098388642][-0.0044453302 -0.0063382019 -0.008166221 -0.0081737 -0.008168336 -0.00817011 -0.0081753153 -0.0080909654 -0.0080743767 -0.0071591819 -0.0043126903 -0.0044554449 -0.0030159224 -0.0051938621 -0.008181802]]...]
INFO - root - 2017-12-10 07:13:29.352847: step 510, loss = 11904.03, batch loss = 0.69 (31.6 examples/sec; 0.253 sec/batch; 23h:21m:45s remains)
INFO - root - 2017-12-10 07:13:31.976541: step 520, loss = 12025.30, batch loss = 0.69 (30.3 examples/sec; 0.264 sec/batch; 24h:21m:40s remains)
INFO - root - 2017-12-10 07:13:34.551319: step 530, loss = 12173.53, batch loss = 0.69 (30.5 examples/sec; 0.263 sec/batch; 24h:12m:40s remains)
INFO - root - 2017-12-10 07:13:37.195986: step 540, loss = 12277.55, batch loss = 0.69 (29.4 examples/sec; 0.273 sec/batch; 25h:07m:51s remains)
INFO - root - 2017-12-10 07:13:39.814199: step 550, loss = 12330.93, batch loss = 0.69 (31.3 examples/sec; 0.256 sec/batch; 23h:35m:41s remains)
INFO - root - 2017-12-10 07:13:42.447378: step 560, loss = 12355.80, batch loss = 0.70 (30.4 examples/sec; 0.263 sec/batch; 24h:13m:47s remains)
INFO - root - 2017-12-10 07:13:45.060532: step 570, loss = 12371.33, batch loss = 0.68 (30.6 examples/sec; 0.262 sec/batch; 24h:07m:06s remains)
INFO - root - 2017-12-10 07:13:47.661020: step 580, loss = 12412.38, batch loss = 8.49 (31.5 examples/sec; 0.254 sec/batch; 23h:25m:30s remains)
INFO - root - 2017-12-10 07:13:50.279064: step 590, loss = 12560.52, batch loss = 0.70 (30.2 examples/sec; 0.265 sec/batch; 24h:25m:52s remains)
INFO - root - 2017-12-10 07:13:52.910534: step 600, loss = 12706.79, batch loss = 0.69 (30.4 examples/sec; 0.263 sec/batch; 24h:13m:24s remains)
2017-12-10 07:13:53.278024: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.038680166 0.059219278 0.076616056 0.0963275 0.095468983 0.088972934 0.083113857 0.068755463 0.053047881 0.033348113 0.012896709 -0.0066689849 -0.025755716 -0.031934679 -0.033632986][-0.033750951 -0.033777814 -0.033816814 -0.033836905 -0.033845168 -0.033826873 -0.033792842 -0.027583977 -0.025860805 -0.025827469 -0.025793876 -0.025772762 -0.025762232 -0.031941198 -0.033632986][-0.03369952 -0.033727009 -0.033756364 -0.033777129 -0.033785574 -0.033777568 -0.033753321 -0.033743721 -0.033721779 -0.033698454 -0.033675216 -0.033654056 -0.03364348 -0.033632927 -0.033633132][-0.033664197 -0.033682987 -0.033704169 -0.03373117 -0.033749159 -0.033751547 -0.033747915 -0.033747017 -0.033743825 -0.033730496 -0.033717539 -0.033706959 -0.03369638 -0.033686914 -0.033676539][-0.033650212 -0.033659142 -0.033671066 -0.032271646 -0.032192148 -0.032210268 -0.032226041 -0.032245588 -0.032262232 -0.033675328 -0.033770438 -0.033759858 -0.033749282 -0.033739813 -0.033718862][-0.033608038 -0.033611156 -0.033613462 -0.032224588 -0.032155633 -0.0321843 -0.032101605 -0.032126978 -0.03215345 -0.033577122 -0.033672236 -0.033661656 -0.033760086 -0.033740018 -0.033708282][-0.032988887 -0.032991678 -0.033579916 -0.032212198 -0.032144684 -0.032174796 -0.032083951 -0.0321116 -0.03214334 -0.033581533 -0.033679422 -0.033668842 -0.033767272 -0.033747204 -0.033715468][-0.033020861 -0.033010282 -0.033589408 -0.032202125 -0.032114819 -0.032157134 -0.032079864 -0.03212218 -0.0321645 -0.033613272 -0.033721738 -0.033700578 -0.033788424 -0.03375778 -0.033715464][-0.033084337 -0.033063177 -0.033631723 -0.03223386 -0.032135341 -0.0321565 -0.032058075 -0.032110967 -0.032163866 -0.033612639 -0.033721738 -0.033711158 -0.033809587 -0.033768363 -0.033715468][-0.032266013 -0.033126652 -0.0336952 -0.03227618 -0.0321565 -0.0321565 -0.032047495 -0.032079231 -0.032121547 -0.0335809 -0.033700578 -0.033700578 -0.033799004 -0.033767268 -0.033714373][-0.031747051 -0.032618269 -0.033197396 -0.03319541 -0.033163041 -0.033141881 -0.033572994 -0.033583574 -0.03360473 -0.03362589 -0.033637103 -0.033637103 -0.033735532 -0.033703793 -0.033661477][-0.03177879 -0.032660589 -0.033250291 -0.033258885 -0.033215936 -0.03319478 -0.0337349 -0.033724319 -0.033724315 -0.033724315 -0.033735529 -0.033724949 -0.033703789 -0.033682629 -0.033650894][-0.032387663 -0.033269458 -0.033269458 -0.033248298 -0.033205349 -0.033184193 -0.033734888 -0.033734892 -0.033745468 -0.033734892 -0.033735525 -0.033724949 -0.033703789 -0.033672053 -0.033629738][-0.032345343 -0.033237725 -0.0332483 -0.033237725 -0.033205353 -0.033173617 -0.033713736 -0.03370316 -0.033703156 -0.03369258 -0.033693213 -0.033693217 -0.033682637 -0.03367205 -0.033650883][-0.032281876 -0.03318483 -0.03320599 -0.03320599 -0.03318483 -0.033163674 -0.033714373 -0.033693213 -0.033682633 -0.033672053 -0.033672053 -0.033672053 -0.033661474 -0.033661466 -0.033650879]]...]
INFO - root - 2017-12-10 07:13:55.878916: step 610, loss = 12776.85, batch loss = 0.69 (30.9 examples/sec; 0.259 sec/batch; 23h:52m:35s remains)
INFO - root - 2017-12-10 07:13:58.477029: step 620, loss = 12807.89, batch loss = 0.69 (31.4 examples/sec; 0.255 sec/batch; 23h:28m:41s remains)
INFO - root - 2017-12-10 07:14:01.071891: step 630, loss = 12827.53, batch loss = 0.69 (31.0 examples/sec; 0.258 sec/batch; 23h:48m:54s remains)
INFO - root - 2017-12-10 07:14:03.626646: step 640, loss = 12841.02, batch loss = 1.22 (31.4 examples/sec; 0.255 sec/batch; 23h:31m:25s remains)
INFO - root - 2017-12-10 07:14:06.214197: step 650, loss = 12850.50, batch loss = 0.69 (30.6 examples/sec; 0.261 sec/batch; 24h:05m:51s remains)
INFO - root - 2017-12-10 07:14:08.816105: step 660, loss = 12867.19, batch loss = 0.72 (31.7 examples/sec; 0.253 sec/batch; 23h:17m:17s remains)
INFO - root - 2017-12-10 07:14:11.410276: step 670, loss = 12904.53, batch loss = 1.35 (30.7 examples/sec; 0.260 sec/batch; 23h:59m:15s remains)
INFO - root - 2017-12-10 07:14:14.043280: step 680, loss = 12964.89, batch loss = 0.69 (30.1 examples/sec; 0.265 sec/batch; 24h:28m:14s remains)
INFO - root - 2017-12-10 07:14:16.656764: step 690, loss = 13006.95, batch loss = 0.69 (30.9 examples/sec; 0.259 sec/batch; 23h:50m:07s remains)
INFO - root - 2017-12-10 07:14:19.300947: step 700, loss = 13044.28, batch loss = 0.69 (28.6 examples/sec; 0.279 sec/batch; 25h:45m:25s remains)
2017-12-10 07:14:19.664443: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314][-0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314][-0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314][-0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314][-0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314][-0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314][-0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314][-0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314][-0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314][-0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314][-0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314][-0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314][-0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314][-0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314][-0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314 -0.030781314]]...]
INFO - root - 2017-12-10 07:14:22.309874: step 710, loss = 13063.05, batch loss = 0.69 (29.2 examples/sec; 0.274 sec/batch; 25h:15m:13s remains)
INFO - root - 2017-12-10 07:14:24.942431: step 720, loss = 13087.33, batch loss = 0.69 (29.9 examples/sec; 0.267 sec/batch; 24h:38m:59s remains)
INFO - root - 2017-12-10 07:14:27.570259: step 730, loss = 13153.98, batch loss = 0.69 (30.4 examples/sec; 0.263 sec/batch; 24h:16m:06s remains)
INFO - root - 2017-12-10 07:14:30.242484: step 740, loss = 13217.14, batch loss = 0.69 (30.6 examples/sec; 0.262 sec/batch; 24h:07m:16s remains)
INFO - root - 2017-12-10 07:14:32.875541: step 750, loss = 13297.83, batch loss = 0.69 (30.8 examples/sec; 0.260 sec/batch; 23h:57m:59s remains)
INFO - root - 2017-12-10 07:14:35.482934: step 760, loss = 13335.53, batch loss = 0.69 (30.6 examples/sec; 0.262 sec/batch; 24h:07m:38s remains)
INFO - root - 2017-12-10 07:14:38.117212: step 770, loss = 13367.89, batch loss = 0.69 (29.5 examples/sec; 0.271 sec/batch; 24h:58m:20s remains)
INFO - root - 2017-12-10 07:14:40.767554: step 780, loss = 13388.01, batch loss = 0.69 (30.4 examples/sec; 0.263 sec/batch; 24h:15m:09s remains)
INFO - root - 2017-12-10 07:14:43.409674: step 790, loss = 13405.25, batch loss = 0.69 (30.3 examples/sec; 0.264 sec/batch; 24h:19m:32s remains)
INFO - root - 2017-12-10 07:14:46.088667: step 800, loss = 13515.62, batch loss = 0.69 (30.9 examples/sec; 0.259 sec/batch; 23h:53m:31s remains)
2017-12-10 07:14:46.401282: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836][-0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836][-0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836][-0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836][-0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836][-0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836][-0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836][-0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836][-0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836][-0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836][-0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836][-0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836][-0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836][-0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836][-0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836 -0.0033352836]]...]
INFO - root - 2017-12-10 07:14:49.027200: step 810, loss = 14026.03, batch loss = 1.31 (28.7 examples/sec; 0.279 sec/batch; 25h:42m:10s remains)
INFO - root - 2017-12-10 07:14:51.652200: step 820, loss = 14577.24, batch loss = 0.69 (31.3 examples/sec; 0.256 sec/batch; 23h:33m:07s remains)
INFO - root - 2017-12-10 07:14:54.279160: step 830, loss = 15096.18, batch loss = 0.69 (31.2 examples/sec; 0.257 sec/batch; 23h:38m:28s remains)
INFO - root - 2017-12-10 07:14:56.902876: step 840, loss = 15803.26, batch loss = 0.69 (31.3 examples/sec; 0.255 sec/batch; 23h:31m:40s remains)
INFO - root - 2017-12-10 07:14:59.513020: step 850, loss = 16556.52, batch loss = 0.69 (30.1 examples/sec; 0.266 sec/batch; 24h:27m:58s remains)
INFO - root - 2017-12-10 07:15:02.126896: step 860, loss = 17191.12, batch loss = 0.69 (30.4 examples/sec; 0.263 sec/batch; 24h:13m:47s remains)
INFO - root - 2017-12-10 07:15:04.765552: step 870, loss = 18312.00, batch loss = 0.69 (28.6 examples/sec; 0.279 sec/batch; 25h:43m:39s remains)
INFO - root - 2017-12-10 07:15:07.411853: step 880, loss = 19631.44, batch loss = 0.72 (30.2 examples/sec; 0.265 sec/batch; 24h:21m:55s remains)
INFO - root - 2017-12-10 07:15:10.036449: step 890, loss = 20666.07, batch loss = 0.69 (29.8 examples/sec; 0.268 sec/batch; 24h:41m:17s remains)
INFO - root - 2017-12-10 07:15:12.641977: step 900, loss = 21129.33, batch loss = 0.69 (30.2 examples/sec; 0.265 sec/batch; 24h:24m:13s remains)
2017-12-10 07:15:13.032903: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972][-0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972][-0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972][-0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972][-0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972][-0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972][-0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972][-0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972][-0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972][-0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972][-0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972][-0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972][-0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972][-0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972][-0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972 -0.015801972]]...]
INFO - root - 2017-12-10 07:15:15.736545: step 910, loss = 21409.08, batch loss = 0.69 (29.0 examples/sec; 0.275 sec/batch; 25h:21m:57s remains)
INFO - root - 2017-12-10 07:15:18.364263: step 920, loss = 21681.45, batch loss = 0.69 (30.9 examples/sec; 0.259 sec/batch; 23h:52m:29s remains)
INFO - root - 2017-12-10 07:15:20.991200: step 930, loss = 21968.57, batch loss = 0.69 (30.4 examples/sec; 0.264 sec/batch; 24h:16m:22s remains)
INFO - root - 2017-12-10 07:15:23.610802: step 940, loss = 22319.80, batch loss = 0.69 (30.2 examples/sec; 0.265 sec/batch; 24h:25m:39s remains)
INFO - root - 2017-12-10 07:15:26.184237: step 950, loss = 22601.93, batch loss = 0.71 (30.9 examples/sec; 0.259 sec/batch; 23h:49m:43s remains)
INFO - root - 2017-12-10 07:15:28.836019: step 960, loss = 22893.89, batch loss = 16.58 (29.8 examples/sec; 0.269 sec/batch; 24h:45m:04s remains)
INFO - root - 2017-12-10 07:15:31.439549: step 970, loss = 23704.80, batch loss = 0.69 (31.9 examples/sec; 0.251 sec/batch; 23h:04m:15s remains)
INFO - root - 2017-12-10 07:15:33.978778: step 980, loss = 24473.26, batch loss = 0.69 (31.7 examples/sec; 0.253 sec/batch; 23h:15m:55s remains)
INFO - root - 2017-12-10 07:15:36.585834: step 990, loss = 24846.23, batch loss = 0.69 (31.9 examples/sec; 0.251 sec/batch; 23h:04m:27s remains)
INFO - root - 2017-12-10 07:15:39.197105: step 1000, loss = 25048.71, batch loss = 0.69 (31.9 examples/sec; 0.251 sec/batch; 23h:05m:28s remains)
2017-12-10 07:15:39.600699: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464][-0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464][-0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464][-0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464][-0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464][-0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464][-0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464][-0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464][-0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464][-0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464][-0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464][-0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464][-0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464][-0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464][-0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464 -0.022413464]]...]
INFO - root - 2017-12-10 07:15:42.155802: step 1010, loss = 25220.50, batch loss = 0.69 (29.7 examples/sec; 0.270 sec/batch; 24h:49m:29s remains)
INFO - root - 2017-12-10 07:15:44.707762: step 1020, loss = 25349.56, batch loss = 0.69 (31.6 examples/sec; 0.253 sec/batch; 23h:16m:50s remains)
INFO - root - 2017-12-10 07:15:47.260970: step 1030, loss = 25483.53, batch loss = 0.69 (31.9 examples/sec; 0.251 sec/batch; 23h:07m:28s remains)
INFO - root - 2017-12-10 07:15:49.788971: step 1040, loss = 25565.53, batch loss = 0.69 (30.5 examples/sec; 0.262 sec/batch; 24h:07m:59s remains)
INFO - root - 2017-12-10 07:15:52.321093: step 1050, loss = 25603.31, batch loss = 0.69 (32.2 examples/sec; 0.249 sec/batch; 22h:52m:56s remains)
INFO - root - 2017-12-10 07:15:54.861905: step 1060, loss = 25649.10, batch loss = 0.69 (31.2 examples/sec; 0.256 sec/batch; 23h:36m:54s remains)
INFO - root - 2017-12-10 07:15:57.452126: step 1070, loss = 25716.73, batch loss = 0.69 (29.1 examples/sec; 0.275 sec/batch; 25h:19m:41s remains)
INFO - root - 2017-12-10 07:15:59.991091: step 1080, loss = 25818.52, batch loss = 0.69 (31.4 examples/sec; 0.255 sec/batch; 23h:27m:21s remains)
INFO - root - 2017-12-10 07:16:02.576616: step 1090, loss = 25875.80, batch loss = 0.69 (31.3 examples/sec; 0.256 sec/batch; 23h:31m:23s remains)
INFO - root - 2017-12-10 07:16:05.178138: step 1100, loss = 26137.28, batch loss = 0.69 (31.1 examples/sec; 0.258 sec/batch; 23h:42m:43s remains)
2017-12-10 07:16:05.554173: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093][-0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093][-0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093][-0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093][-0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093][-0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093][-0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093][-0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093][-0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093][-0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093][-0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093][-0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093][-0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093][-0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093][-0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093 -0.0020514093]]...]
INFO - root - 2017-12-10 07:16:08.121631: step 1110, loss = 26481.92, batch loss = 0.69 (30.6 examples/sec; 0.261 sec/batch; 24h:03m:21s remains)
INFO - root - 2017-12-10 07:16:10.613707: step 1120, loss = 26644.79, batch loss = 0.69 (31.7 examples/sec; 0.253 sec/batch; 23h:14m:39s remains)
INFO - root - 2017-12-10 07:16:13.102174: step 1130, loss = 26746.74, batch loss = 0.69 (32.6 examples/sec; 0.245 sec/batch; 22h:35m:50s remains)
INFO - root - 2017-12-10 07:16:15.593571: step 1140, loss = 27024.90, batch loss = 0.69 (31.0 examples/sec; 0.258 sec/batch; 23h:45m:05s remains)
INFO - root - 2017-12-10 07:16:18.066919: step 1150, loss = 27360.35, batch loss = 0.69 (32.0 examples/sec; 0.250 sec/batch; 22h:58m:37s remains)
INFO - root - 2017-12-10 07:16:20.583488: step 1160, loss = 27882.30, batch loss = 0.69 (31.1 examples/sec; 0.257 sec/batch; 23h:41m:12s remains)
INFO - root - 2017-12-10 07:16:23.136969: step 1170, loss = 28657.65, batch loss = 0.69 (32.2 examples/sec; 0.248 sec/batch; 22h:49m:52s remains)
INFO - root - 2017-12-10 07:16:25.649059: step 1180, loss = 29210.37, batch loss = 0.69 (32.0 examples/sec; 0.250 sec/batch; 23h:00m:20s remains)
INFO - root - 2017-12-10 07:16:28.149466: step 1190, loss = 29637.95, batch loss = 0.69 (32.8 examples/sec; 0.244 sec/batch; 22h:24m:45s remains)
INFO - root - 2017-12-10 07:16:30.618330: step 1200, loss = 29952.68, batch loss = 0.69 (31.9 examples/sec; 0.251 sec/batch; 23h:05m:36s remains)
2017-12-10 07:16:30.893875: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879][-0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879][-0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879][-0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879][-0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879][-0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879][-0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879][-0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879][-0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879][-0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879][-0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879][-0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879][-0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879][-0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879][-0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879 -0.0031710879]]...]
INFO - root - 2017-12-10 07:16:33.370079: step 1210, loss = 30243.75, batch loss = 0.69 (32.1 examples/sec; 0.249 sec/batch; 22h:54m:13s remains)
INFO - root - 2017-12-10 07:16:35.817438: step 1220, loss = 30699.96, batch loss = 0.69 (32.3 examples/sec; 0.248 sec/batch; 22h:48m:17s remains)
INFO - root - 2017-12-10 07:16:38.332891: step 1230, loss = 31094.81, batch loss = 0.69 (32.3 examples/sec; 0.248 sec/batch; 22h:49m:23s remains)
INFO - root - 2017-12-10 07:16:40.824898: step 1240, loss = 31404.98, batch loss = 0.69 (31.4 examples/sec; 0.254 sec/batch; 23h:24m:27s remains)
INFO - root - 2017-12-10 07:16:43.305206: step 1250, loss = 31826.54, batch loss = 0.69 (32.8 examples/sec; 0.244 sec/batch; 22h:28m:09s remains)
INFO - root - 2017-12-10 07:16:45.836041: step 1260, loss = 32197.89, batch loss = 0.69 (31.1 examples/sec; 0.257 sec/batch; 23h:40m:43s remains)
INFO - root - 2017-12-10 07:16:48.374058: step 1270, loss = 32474.04, batch loss = 0.69 (32.2 examples/sec; 0.248 sec/batch; 22h:51m:45s remains)
INFO - root - 2017-12-10 07:16:50.898631: step 1280, loss = 32712.67, batch loss = 0.69 (32.2 examples/sec; 0.249 sec/batch; 22h:53m:08s remains)
INFO - root - 2017-12-10 07:16:53.433498: step 1290, loss = 32860.62, batch loss = 0.69 (32.4 examples/sec; 0.247 sec/batch; 22h:42m:47s remains)
INFO - root - 2017-12-10 07:16:55.965761: step 1300, loss = 32925.19, batch loss = 0.69 (30.9 examples/sec; 0.259 sec/batch; 23h:49m:42s remains)
2017-12-10 07:16:56.254322: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893][-0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893][-0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893][-0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893][-0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893][-0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893][-0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893][-0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893][-0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893][-0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893][-0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893][-0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893][-0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893][-0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893][-0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893 -0.00061644893]]...]
INFO - root - 2017-12-10 07:16:58.798218: step 1310, loss = 33001.77, batch loss = 0.69 (31.7 examples/sec; 0.252 sec/batch; 23h:12m:14s remains)
INFO - root - 2017-12-10 07:17:01.326763: step 1320, loss = 33056.43, batch loss = 0.69 (32.0 examples/sec; 0.250 sec/batch; 23h:00m:16s remains)
INFO - root - 2017-12-10 07:17:03.873306: step 1330, loss = 33485.03, batch loss = 2.92 (32.2 examples/sec; 0.249 sec/batch; 22h:51m:37s remains)
INFO - root - 2017-12-10 07:17:06.414234: step 1340, loss = 34508.84, batch loss = 0.72 (31.7 examples/sec; 0.253 sec/batch; 23h:15m:03s remains)
INFO - root - 2017-12-10 07:17:09.020904: step 1350, loss = 35450.09, batch loss = 0.69 (31.5 examples/sec; 0.254 sec/batch; 23h:20m:53s remains)
INFO - root - 2017-12-10 07:17:11.567929: step 1360, loss = 36040.24, batch loss = 0.69 (31.3 examples/sec; 0.255 sec/batch; 23h:28m:34s remains)
INFO - root - 2017-12-10 07:17:14.119478: step 1370, loss = 36370.95, batch loss = 0.69 (31.1 examples/sec; 0.257 sec/batch; 23h:37m:57s remains)
INFO - root - 2017-12-10 07:17:16.666766: step 1380, loss = 36746.78, batch loss = 0.69 (31.0 examples/sec; 0.258 sec/batch; 23h:43m:20s remains)
INFO - root - 2017-12-10 07:17:19.228190: step 1390, loss = 37389.97, batch loss = 0.69 (30.7 examples/sec; 0.260 sec/batch; 23h:57m:24s remains)
INFO - root - 2017-12-10 07:17:21.800564: step 1400, loss = 37975.76, batch loss = 0.69 (30.1 examples/sec; 0.266 sec/batch; 24h:28m:02s remains)
2017-12-10 07:17:22.112382: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0061282828 -0.0062015066 -0.0065571559 -0.0067724492 -0.0067359139 -0.0071934895 -0.0066655595 -0.0058759488 -0.0051539922 -0.0050159604 -0.0048780749 -0.0052591553 -0.0052256966 -0.0050113071 -0.0061682612][-0.0065122615 -0.0063461666 -0.0062266467 -0.006357633 -0.0064809732 -0.0066697225 -0.0074774181 -0.0072719743 -0.006719186 -0.006300143 -0.0059215659 -0.0057588746 -0.0058066631 -0.0059115863 -0.0063029095][-0.0080522588 -0.0080091162 -0.0077493861 -0.00743009 -0.0070694825 -0.0070161503 -0.0071031456 -0.0071700467 -0.0069838422 -0.0074929381 -0.0074925278 -0.0072993836 -0.0072076642 -0.0071466812 -0.0072795111][-0.0081512043 -0.0081512043 -0.0081512015 -0.0081359446 -0.0081053665 -0.008093968 -0.0080029117 -0.0078226021 -0.0076610669 -0.0077923476 -0.0078224912 -0.0074534509 -0.00807144 -0.00806982 -0.0081037469][-0.0081512043 -0.0081512043 -0.0081512043 -0.0081512043 -0.0081512043 -0.0081512043 -0.0081512043 -0.0081512043 -0.007882325 -0.00801415 -0.0081512043 -0.0081508188 -0.0079953838 -0.00807149 -0.0081511559][-0.0081512043 -0.0081512043 -0.0081512043 -0.0080231288 -0.008114825 -0.0081512043 -0.0081512043 -0.0080776187 -0.007998107 -0.0081507433 -0.0081506493 -0.0077009406 -0.0081055649 -0.0081507741 -0.0081503764][-0.0081512043 -0.0081512043 -0.0081512043 -0.0081512043 -0.0081512043 -0.0081233028 -0.0081512043 -0.0081503447 -0.0081497692 -0.0081495736 -0.0081496425 -0.0081492895 -0.00814884 -0.0081477212 -0.0081468252][-0.0081512043 -0.0081512043 -0.0081512043 -0.0081512043 -0.0081512043 -0.0081512043 -0.0081512043 -0.0081510544 -0.0081507033 -0.0081494711 -0.0081488127 -0.0081478553 -0.00814733 -0.0081465021 -0.0081447167][-0.0081494544 -0.0081496425 -0.0081496388 -0.0081332084 -0.0081500616 -0.0081503885 -0.00815045 -0.0081504537 -0.008149487 -0.0081490576 -0.0081488267 -0.00814941 -0.0081478646 -0.0081423884 -0.0081382962][-0.0081501883 -0.0081495456 -0.0081494637 -0.0079914732 -0.0080675827 -0.0081497887 -0.0081495792 -0.008149052 -0.0081480434 -0.0081481636 -0.008147941 -0.0081485221 -0.00814847 -0.0081479615 -0.0081461091][-0.00814997 -0.008149961 -0.0081502749 -0.0081106117 -0.0081504509 -0.0081500011 -0.0079089 -0.0081481561 -0.0081462394 -0.0081451973 -0.0081459219 -0.0081460485 -0.0081456648 -0.008142774 -0.008139384][-0.008150219 -0.0081501938 -0.0081503727 -0.008150517 -0.0081504192 -0.0081504621 -0.0081506148 -0.0081502823 -0.0081495484 -0.008148632 -0.0081459442 -0.0081439344 -0.0081430627 -0.0081399791 -0.008134353][-0.0081495158 -0.0081497151 -0.0081496639 -0.0081503848 -0.00815091 -0.0081511447 -0.00815114 -0.0081505952 -0.0081484048 -0.0081476159 -0.008147411 -0.008148361 -0.0081481924 -0.0081409933 -0.0081325974][-0.0081488937 -0.0081482753 -0.0081473319 -0.0081470879 -0.0081469286 -0.0081490427 -0.0081504164 -0.0081503652 -0.0081493836 -0.0081488984 -0.0081484169 -0.0081468765 -0.0081466818 -0.00814378 -0.0081438608][-0.0081499657 -0.00814924 -0.0081496825 -0.0081497151 -0.00814893 -0.0081496937 -0.0081469938 -0.0081476038 -0.0081494981 -0.0081478544 -0.0081479121 -0.0081469035 -0.0081469929 -0.0081451442 -0.00814085]]...]
INFO - root - 2017-12-10 07:17:24.663209: step 1410, loss = 38382.93, batch loss = 0.69 (31.1 examples/sec; 0.257 sec/batch; 23h:38m:36s remains)
INFO - root - 2017-12-10 07:17:27.258344: step 1420, loss = 38676.43, batch loss = 0.69 (31.3 examples/sec; 0.256 sec/batch; 23h:31m:47s remains)
INFO - root - 2017-12-10 07:17:29.812142: step 1430, loss = 39010.17, batch loss = 0.80 (31.4 examples/sec; 0.255 sec/batch; 23h:25m:44s remains)
INFO - root - 2017-12-10 07:17:32.351808: step 1440, loss = 39777.83, batch loss = 1.86 (31.6 examples/sec; 0.253 sec/batch; 23h:18m:40s remains)
INFO - root - 2017-12-10 07:17:34.890067: step 1450, loss = 41373.63, batch loss = 7.31 (31.3 examples/sec; 0.256 sec/batch; 23h:30m:29s remains)
INFO - root - 2017-12-10 07:17:37.446840: step 1460, loss = 42561.15, batch loss = 0.92 (31.5 examples/sec; 0.254 sec/batch; 23h:21m:18s remains)
INFO - root - 2017-12-10 07:17:40.027352: step 1470, loss = 43216.81, batch loss = 0.69 (30.3 examples/sec; 0.264 sec/batch; 24h:14m:50s remains)
INFO - root - 2017-12-10 07:17:42.582214: step 1480, loss = 43891.81, batch loss = 0.69 (31.1 examples/sec; 0.257 sec/batch; 23h:38m:13s remains)
INFO - root - 2017-12-10 07:17:45.159588: step 1490, loss = 44448.20, batch loss = 0.69 (30.3 examples/sec; 0.264 sec/batch; 24h:18m:05s remains)
INFO - root - 2017-12-10 07:17:47.722558: step 1500, loss = 44994.28, batch loss = 0.72 (31.1 examples/sec; 0.257 sec/batch; 23h:39m:03s remains)
2017-12-10 07:17:48.001310: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.066633321 -0.05770687 -0.023299001 0.22321746 0.55386853 0.90818477 1.2921425 1.3533945 1.3575842 1.3829148 1.1659667 1.1571771 1.1711729 1.2087364 1.2115406][-0.068858422 0.36799845 0.14135242 0.32152832 0.71672833 0.9885456 1.2691326 1.4452481 1.8636507 2.0072014 1.5537881 1.4846802 1.5164703 1.6185334 1.6738399][0.05403056 -0.012063984 0.023416787 0.45102513 0.67305845 0.82523996 0.96546394 1.1130065 1.6151284 2.0610535 1.7002999 1.6183695 1.8108915 1.9834586 2.0770385][0.17354026 -0.068847559 -0.031792089 0.57286948 0.79550153 0.89918453 1.2915537 1.0765382 0.971541 1.4305991 1.201262 1.2935103 1.5657911 1.763131 1.8623807][0.076700114 -0.044246942 -0.04350698 0.044254132 0.36693391 0.99645883 0.9991712 0.805598 0.96766847 1.2336516 1.0553966 0.99432391 1.1454675 1.2202972 1.442899][-0.031646229 -0.068855219 -0.010511875 0.12667617 0.26327032 0.20602447 0.34791082 0.1659795 0.18806711 0.38014257 0.51766294 0.5059889 0.47387791 0.57659489 0.65283489][-0.068862066 -0.06663207 -0.064952828 -0.031862479 0.052527539 0.20776176 0.17857435 0.061283238 0.02554816 0.12874418 0.0873278 0.15415812 0.0950687 0.14814255 0.18535423][-0.068862177 -0.068862207 -0.065961838 -0.0545024 -0.035067879 -0.023249801 -0.028325584 -0.0059845448 -0.023651656 -0.0035265982 -0.022009619 0.0022187978 -0.035444517 -0.012319203 -0.011486698][-0.068862192 -0.068862222 -0.068461083 -0.068226047 -0.0650727 -0.059018012 -0.050452907 -0.052719474 -0.050325122 -0.0094865784 -0.0017128512 0.0041405335 -0.040527172 -0.046345666 -0.047012117][-0.060255669 -0.067968443 -0.06810578 -0.068862244 -0.068817966 -0.0687618 -0.068862133 -0.0658915 -0.065213837 -0.051169991 -0.007020697 -0.0057924092 -0.063428871 -0.056234814 -0.067133248][-0.046217885 -0.062866837 -0.068361394 -0.061589491 -0.06886223 -0.068862237 -0.068630196 -0.068335295 -0.068142869 -0.049015865 -0.033904936 -0.042709447 -0.062523581 -0.065841958 -0.063649833][-0.032446072 -0.057269767 -0.052166164 -0.054739069 -0.06886223 -0.068862207 -0.068862222 -0.068862207 -0.067779072 -0.068374589 -0.055841759 -0.060612947 -0.068190932 -0.068794236 -0.068660058][-0.058163732 -0.061378472 -0.068138838 -0.059105251 -0.061126083 -0.068861969 -0.068862014 -0.059732318 -0.057298075 -0.06744916 -0.065843813 -0.065677978 -0.068688348 -0.068858914 -0.068862081][-0.062426876 -0.06692902 -0.063528582 -0.04844432 -0.054217171 -0.068860345 -0.053046748 -0.034018919 -0.0494614 -0.068852425 -0.065300912 -0.06550476 -0.068851337 -0.035014864 -0.068862222][-0.068862237 -0.068862259 -0.068810426 -0.068745255 -0.059200957 -0.068862006 -0.068861872 -0.060036324 -0.056610379 -0.068862185 -0.068849914 -0.066633329 -0.00057281554 -0.0046897009 -0.068862207]]...]
INFO - root - 2017-12-10 07:17:50.547217: step 1510, loss = 45900.12, batch loss = 6.19 (31.1 examples/sec; 0.257 sec/batch; 23h:37m:02s remains)
INFO - root - 2017-12-10 07:17:53.096533: step 1520, loss = 47033.26, batch loss = 0.69 (31.6 examples/sec; 0.253 sec/batch; 23h:15m:51s remains)
INFO - root - 2017-12-10 07:17:55.622669: step 1530, loss = 48183.54, batch loss = 0.69 (31.2 examples/sec; 0.257 sec/batch; 23h:35m:37s remains)
INFO - root - 2017-12-10 07:17:58.153854: step 1540, loss = 48840.34, batch loss = 0.69 (31.9 examples/sec; 0.251 sec/batch; 23h:05m:20s remains)
INFO - root - 2017-12-10 07:18:00.705896: step 1550, loss = 49356.97, batch loss = 0.69 (31.6 examples/sec; 0.253 sec/batch; 23h:16m:54s remains)
INFO - root - 2017-12-10 07:18:03.251527: step 1560, loss = 49759.82, batch loss = 0.69 (32.4 examples/sec; 0.247 sec/batch; 22h:43m:07s remains)
INFO - root - 2017-12-10 07:18:05.813378: step 1570, loss = 50049.62, batch loss = 0.76 (31.7 examples/sec; 0.252 sec/batch; 23h:12m:27s remains)
INFO - root - 2017-12-10 07:18:08.421573: step 1580, loss = 50555.04, batch loss = 0.69 (31.6 examples/sec; 0.253 sec/batch; 23h:14m:38s remains)
INFO - root - 2017-12-10 07:18:10.990109: step 1590, loss = 51262.04, batch loss = 0.69 (31.6 examples/sec; 0.253 sec/batch; 23h:15m:54s remains)
INFO - root - 2017-12-10 07:18:13.538590: step 1600, loss = 51933.77, batch loss = 0.69 (30.9 examples/sec; 0.259 sec/batch; 23h:49m:26s remains)
2017-12-10 07:18:13.842810: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.027525922 -0.027879037 -0.027665582 -0.026596097 -0.025674203 -0.027809592 -0.027105087 -0.027879676 -0.027425079 -0.027845554 -0.027534993 -0.027823703 -0.026298098 -0.027877213 -0.027879676][-0.027879676 -0.027781278 -0.027879441 -0.027879678 -0.027879678 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879678 -0.027879678 -0.027879676][-0.027879676 -0.027879678 -0.027872555 -0.027871914 -0.027879676 -0.027879678 -0.027879678 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879676][-0.027879676 -0.027879672 -0.027879678 -0.027509598 -0.027354728 -0.027815897 -0.027789887 -0.027879678 -0.02787968 -0.027879672 -0.027879676 0.01968918 0.10003463 0.058031574 -0.027879676][-0.027879674 -0.024248356 -0.027874194 -0.027764136 -0.027817842 -0.027879674 -0.027437268 -0.0272541 -0.027879676 -0.027763234 -0.02753309 -0.027879676 -0.027879678 0.02710101 0.048695751][-0.027879676 -0.027879676 -0.027879676 -0.027879678 -0.02567295 -0.027879676 -0.027879678 -0.027879678 -0.027879678 -0.027879676 -0.027879676 -0.027559167 -0.027879676 -0.027879676 -0.027879676][-0.027879674 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027872575 -0.027858283 -0.027879678 -0.027879678 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.02787598 -0.027840367][-0.027879674 -0.027879674 -0.027879676 -0.027879674 -0.027879674 -0.027879672 -0.027879674 -0.027777236 -0.027783401 -0.027879676 -0.027876237 -0.027879678 -0.027878698 -0.027857089 -0.027879676][-0.02264262 -0.020584594 -0.027879676 -0.027876845 -0.027879676 -0.027879676 -0.027877416 -0.027879676 -0.027879676 -0.027842024 -0.027787114 -0.027879495 -0.027879676 -0.027879674 -0.0277884][-0.027879674 -0.027879678 -0.010181716 -0.0032348093 -0.027879678 -0.027879676 -0.027879676 -0.027879676 -0.027879678 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879676][-0.027781514 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879678 -0.027879678 -0.027879678 -0.027879676][-0.027879678 -0.027879676 -0.027879676 -0.027298307 -0.020217488 0.003347436 0.0023195744 -0.027879674 -0.027879674 -0.027879676 -0.027879676 -0.027879678 -0.027879676 -0.027879676 -0.027879678][-0.027879678 -0.027879471 -0.027845852 -0.027757835 -0.027879676 -0.027879676 -0.027879674 -0.027879676 -0.027879674 -0.027879674 -0.027879676 -0.027879676 -0.027879676 -0.027879678 -0.027879678][-0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879678 -0.027879678 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879678 -0.027879678 -0.027879676][-0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879678 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879676 -0.027879676]]...]
INFO - root - 2017-12-10 07:18:16.411289: step 1610, loss = 52617.98, batch loss = 0.69 (31.4 examples/sec; 0.255 sec/batch; 23h:27m:05s remains)
INFO - root - 2017-12-10 07:18:18.963533: step 1620, loss = 53176.14, batch loss = 0.69 (31.2 examples/sec; 0.257 sec/batch; 23h:34m:42s remains)
INFO - root - 2017-12-10 07:18:21.517124: step 1630, loss = 53890.84, batch loss = 0.69 (31.2 examples/sec; 0.257 sec/batch; 23h:35m:31s remains)
INFO - root - 2017-12-10 07:18:24.098579: step 1640, loss = 55013.23, batch loss = 0.69 (31.7 examples/sec; 0.253 sec/batch; 23h:13m:39s remains)
INFO - root - 2017-12-10 07:18:26.647513: step 1650, loss = 56228.96, batch loss = 0.69 (31.3 examples/sec; 0.255 sec/batch; 23h:28m:27s remains)
INFO - root - 2017-12-10 07:18:29.211256: step 1660, loss = 57476.20, batch loss = 0.69 (30.8 examples/sec; 0.260 sec/batch; 23h:52m:18s remains)
INFO - root - 2017-12-10 07:18:31.808551: step 1670, loss = 58028.00, batch loss = 0.69 (29.2 examples/sec; 0.274 sec/batch; 25h:12m:30s remains)
INFO - root - 2017-12-10 07:18:34.387386: step 1680, loss = 58482.76, batch loss = 0.69 (31.4 examples/sec; 0.255 sec/batch; 23h:26m:58s remains)
INFO - root - 2017-12-10 07:18:36.965269: step 1690, loss = 58903.71, batch loss = 0.69 (31.1 examples/sec; 0.258 sec/batch; 23h:39m:44s remains)
INFO - root - 2017-12-10 07:18:39.553024: step 1700, loss = 59298.69, batch loss = 0.69 (30.8 examples/sec; 0.260 sec/batch; 23h:52m:05s remains)
2017-12-10 07:18:39.852952: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262][-0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262][-0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262][-0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262][-0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262][-0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262][-0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262][-0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262][-0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262][-0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262][-0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262][-0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262][-0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262][-0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262][-0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262 -0.013638262]]...]
INFO - root - 2017-12-10 07:18:42.429261: step 1710, loss = 60133.73, batch loss = 0.69 (31.6 examples/sec; 0.254 sec/batch; 23h:17m:45s remains)
