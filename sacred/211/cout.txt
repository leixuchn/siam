INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "211"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.1-nosplit-clip50-shortcut
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
111111111 Tensor("siamese_fc/conv5/concat:0", shape=(8, 6, 6, 256), dtype=float32) Tensor("siamese_fc/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/db1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 6, 6), dtype=float32)
sdiufhasudf Tensor("siamese_fc/conv5/b1/BiasAdd:0", shape=(8, 6, 6, 128), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-11 09:48:37.014017: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 09:48:37.014057: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 09:48:37.014063: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 09:48:37.014068: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 09:48:37.014072: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 09:48:37.807392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 8.74GiB
2017-12-11 09:48:37.807429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-11 09:48:37.807435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-11 09:48:37.807448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
111111111 Tensor("siamese_fc_1/conv5/concat:0", shape=(8, 20, 20, 256), dtype=float32) Tensor("siamese_fc_1/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/db1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 20, 20), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/b1/BiasAdd:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - 2017-12-11 09:48:44.215529: step 0, loss = 0.24, batch loss = 0.15 (1.7 examples/sec; 4.705 sec/batch; 434h:31m:06s remains)
2017-12-11 09:48:44.844277: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9154069 -4.1958389 -4.7051563 -5.35047 -5.8162818 -5.9356384 -5.70854 -5.3669853 -5.1930447 -5.325139 -5.6696668 -6.021338 -6.160953 -5.8559127 -5.2430091][-4.2797122 -4.7001419 -5.4515371 -6.3243814 -6.8061028 -6.765707 -6.38379 -6.0464563 -5.9922514 -6.2804828 -6.7482815 -7.1460752 -7.2511559 -6.803771 -5.9681416][-4.5382357 -5.0930667 -6.0589304 -7.0130129 -7.219595 -6.695435 -5.9091506 -5.4378376 -5.4433661 -5.9408989 -6.7132425 -7.4051037 -7.764534 -7.4445014 -6.5403795][-4.5795937 -5.2126775 -6.2844028 -7.1132469 -6.8053761 -5.6094828 -4.280807 -3.5103157 -3.4193187 -4.1199417 -5.3819532 -6.6403522 -7.5431662 -7.6063242 -6.8001494][-4.3025579 -4.950686 -6.0456023 -6.6670175 -5.8297639 -4.0198832 -2.1336775 -0.90411687 -0.58433533 -1.5147762 -3.4195158 -5.3598614 -6.8991766 -7.4541664 -6.8495107][-3.7322049 -4.3790879 -5.4721475 -5.8828545 -4.5921688 -2.2417943 0.31461906 2.277781 2.9891138 1.7627707 -0.92851472 -3.625046 -5.8451433 -6.960495 -6.6643739][-3.0776677 -3.7111545 -4.7317238 -4.8910923 -3.2618442 -0.51647139 2.6872072 5.4561071 6.5559921 4.9550219 1.4817138 -1.9021273 -4.6624403 -6.2296662 -6.2454486][-2.5450077 -3.1664391 -4.06573 -4.0386219 -2.3567171 0.36068535 3.7260714 6.814786 7.9669905 6.0127125 2.1200628 -1.5062804 -4.3109007 -5.8988733 -5.9712577][-2.4384356 -3.0313048 -3.8355968 -3.840699 -2.4904792 -0.26453924 2.6394529 5.367631 6.2275743 4.2719374 0.630064 -2.6179261 -4.9121056 -6.0779977 -5.9552321][-2.786201 -3.2704844 -3.980839 -4.1801982 -3.3474455 -1.7796261 0.43035889 2.5226974 3.0917821 1.5079384 -1.4484017 -3.9897301 -5.5815964 -6.2328167 -5.915544][-3.3595157 -3.6908708 -4.2979836 -4.7073636 -4.3332968 -3.2727613 -1.6536188 -0.16954327 0.17383766 -0.952369 -3.1477423 -5.0059133 -6.0118084 -6.2462397 -5.7915177][-3.8673687 -4.0640788 -4.6261368 -5.2269177 -5.1757789 -4.4321856 -3.2649217 -2.280468 -2.0837359 -2.7882602 -4.2904944 -5.6065631 -6.2276783 -6.215663 -5.7000895][-3.9050555 -4.08023 -4.6919618 -5.4435062 -5.5307765 -4.8880749 -3.9430351 -3.2145021 -3.0499864 -3.3963237 -4.3845339 -5.4188008 -5.9748025 -6.0014114 -5.5730629][-3.6768913 -3.8993013 -4.5860138 -5.3921165 -5.4400496 -4.6969595 -3.7367859 -3.016531 -2.7785487 -2.8496456 -3.5419202 -4.5527015 -5.3064728 -5.5972166 -5.366847][-3.2433872 -3.47622 -4.2061367 -5.0359049 -5.0100279 -4.0933347 -2.9833975 -2.1764386 -1.9046743 -1.8774393 -2.5206819 -3.6864097 -4.7127142 -5.2620578 -5.1852093]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.1-nosplit-clip50-shortcut/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.1-nosplit-clip50-shortcut/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-11 09:48:49.988384: step 10, loss = 0.26, batch loss = 0.18 (17.0 examples/sec; 0.469 sec/batch; 43h:21m:09s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.1-nosplit-clip50-shortcut
INFO - root - 2017-12-11 09:48:54.404774: step 20, loss = 0.25, batch loss = 0.17 (17.9 examples/sec; 0.448 sec/batch; 41h:19m:51s remains)
INFO - root - 2017-12-11 09:48:58.790608: step 30, loss = 0.28, batch loss = 0.20 (18.0 examples/sec; 0.445 sec/batch; 41h:07m:03s remains)
INFO - root - 2017-12-11 09:49:02.989915: step 40, loss = 0.39, batch loss = 0.31 (17.2 examples/sec; 0.465 sec/batch; 42h:59m:19s remains)
INFO - root - 2017-12-11 09:49:07.487294: step 50, loss = 0.28, batch loss = 0.20 (17.5 examples/sec; 0.458 sec/batch; 42h:19m:29s remains)
INFO - root - 2017-12-11 09:49:11.975001: step 60, loss = 0.46, batch loss = 0.38 (18.3 examples/sec; 0.438 sec/batch; 40h:24m:28s remains)
INFO - root - 2017-12-11 09:49:16.410154: step 70, loss = 0.22, batch loss = 0.14 (18.2 examples/sec; 0.438 sec/batch; 40h:28m:53s remains)
INFO - root - 2017-12-11 09:49:20.885716: step 80, loss = 0.28, batch loss = 0.20 (17.8 examples/sec; 0.449 sec/batch; 41h:26m:44s remains)
INFO - root - 2017-12-11 09:49:25.397874: step 90, loss = 0.30, batch loss = 0.22 (18.0 examples/sec; 0.445 sec/batch; 41h:06m:44s remains)
INFO - root - 2017-12-11 09:49:29.803355: step 100, loss = 0.23, batch loss = 0.15 (18.2 examples/sec; 0.440 sec/batch; 40h:40m:21s remains)
2017-12-11 09:49:30.321313: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2119446 -4.4463215 -3.8458683 -3.7732034 -4.1593633 -5.0007596 -5.8452258 -6.5112476 -6.6823587 -5.950634 -4.2436957 -1.8367727 0.57068968 2.4644771 3.0001621][-5.9668446 -5.2085776 -4.5932541 -4.3832936 -4.6245747 -5.4036818 -6.1315651 -6.5173388 -6.2835402 -5.1990852 -3.3252611 -0.934402 1.4641318 3.4306293 4.0409966][-6.7062554 -6.1212931 -5.535162 -5.0866337 -4.9536572 -5.330267 -5.6547313 -5.6677589 -5.2167811 -4.21996 -2.7452459 -0.89266229 1.1980343 3.0949516 3.8011007][-7.3687267 -6.9604011 -6.3652458 -5.5817461 -4.830513 -4.4075069 -4.0504956 -3.6750357 -3.2600517 -2.7760575 -2.1868072 -1.2967165 0.12850571 1.5887799 2.2028308][-7.9065619 -7.5877924 -6.9235697 -5.7606263 -4.2279711 -2.7490518 -1.5445437 -0.76066756 -0.53452611 -0.89519954 -1.5860691 -1.9299254 -1.3955131 -0.63377857 -0.28390884][-8.1406689 -7.7670851 -7.0056577 -5.5326471 -3.2885292 -0.79529905 1.2496109 2.5096083 2.5991592 1.3308206 -0.73288631 -2.3077688 -2.6876049 -2.7453141 -2.8238263][-7.7613444 -7.2776079 -6.4999733 -4.960041 -2.38389 0.7082963 3.3616648 5.0821686 5.1521873 3.1969395 0.089321136 -2.3877301 -3.5276878 -4.3755813 -4.93888][-6.5753512 -6.1812534 -5.6595 -4.4446077 -2.1151764 0.99026251 3.9395723 6.06001 6.2390232 3.9294338 0.29927826 -2.5995541 -4.2111063 -5.5654478 -6.3958049][-4.8676558 -4.7900996 -4.7619286 -4.1313725 -2.4793324 0.10915947 2.8872061 5.0400705 5.3125057 3.1041651 -0.26664686 -2.866425 -4.4118223 -5.8003654 -6.6259079][-2.2075479 -2.67142 -3.3650684 -3.5272515 -2.9175963 -1.3753963 0.61501408 2.2264748 2.4122376 0.63327837 -1.8129308 -3.443012 -4.3453112 -5.3050537 -5.9129248][0.81482697 -0.2613039 -1.7771082 -2.7729697 -3.2211695 -2.7923617 -1.7831135 -0.98456717 -1.0623527 -2.3771122 -3.7578068 -4.2316232 -4.2537842 -4.5724859 -4.8359108][3.1606464 1.6200047 -0.5903461 -2.3086185 -3.6040978 -4.0728593 -3.8872573 -3.7574759 -4.0301433 -4.9018793 -5.3830667 -4.9028878 -4.1437831 -3.8548224 -3.7409892][4.3422918 2.7102332 0.17760515 -1.9759889 -3.7609634 -4.7330155 -4.9897251 -5.1524243 -5.4442205 -5.96829 -5.9103937 -4.9474678 -3.806771 -3.2057123 -2.8748107][3.8616543 2.4636045 0.1237402 -1.9659224 -3.7220111 -4.6923113 -4.9521656 -5.0372076 -5.1667037 -5.4169226 -5.1861749 -4.2482615 -3.1975241 -2.6461754 -2.3503795][2.1474257 1.1564555 -0.61706543 -2.2932382 -3.7108939 -4.4061718 -4.4417372 -4.2683825 -4.1452389 -4.1746893 -3.9897072 -3.3729167 -2.6853905 -2.3703885 -2.2382317]]...]
INFO - root - 2017-12-11 09:49:34.767019: step 110, loss = 0.37, batch loss = 0.28 (17.8 examples/sec; 0.448 sec/batch; 41h:24m:36s remains)
INFO - root - 2017-12-11 09:49:39.260858: step 120, loss = 0.32, batch loss = 0.24 (18.0 examples/sec; 0.445 sec/batch; 41h:04m:53s remains)
INFO - root - 2017-12-11 09:49:43.762489: step 130, loss = 0.30, batch loss = 0.21 (17.7 examples/sec; 0.451 sec/batch; 41h:39m:14s remains)
INFO - root - 2017-12-11 09:49:48.185432: step 140, loss = 0.32, batch loss = 0.24 (18.2 examples/sec; 0.440 sec/batch; 40h:36m:40s remains)
INFO - root - 2017-12-11 09:49:52.665333: step 150, loss = 0.37, batch loss = 0.28 (18.1 examples/sec; 0.441 sec/batch; 40h:44m:20s remains)
INFO - root - 2017-12-11 09:49:56.863092: step 160, loss = 0.28, batch loss = 0.19 (18.5 examples/sec; 0.433 sec/batch; 39h:57m:00s remains)
INFO - root - 2017-12-11 09:50:01.308404: step 170, loss = 0.34, batch loss = 0.25 (18.1 examples/sec; 0.442 sec/batch; 40h:50m:41s remains)
INFO - root - 2017-12-11 09:50:05.776909: step 180, loss = 0.28, batch loss = 0.20 (18.0 examples/sec; 0.444 sec/batch; 41h:00m:00s remains)
INFO - root - 2017-12-11 09:50:10.237738: step 190, loss = 0.31, batch loss = 0.23 (17.8 examples/sec; 0.451 sec/batch; 41h:35m:45s remains)
INFO - root - 2017-12-11 09:50:14.733836: step 200, loss = 0.29, batch loss = 0.21 (17.6 examples/sec; 0.456 sec/batch; 42h:03m:25s remains)
2017-12-11 09:50:15.202102: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.9540195 -6.752821 -6.2609644 -5.8635931 -5.2165666 -4.6009426 -4.392622 -4.0725513 -3.8545756 -3.6436749 -3.428278 -3.4542475 -3.5519247 -3.7172656 -4.1080847][-5.8964896 -5.71733 -5.3457642 -4.9755182 -4.2055941 -3.4743538 -3.1751926 -2.858335 -2.852345 -2.876121 -2.6907692 -2.5977955 -2.6519117 -2.811101 -3.2058053][-4.2799072 -4.0365362 -3.8058138 -3.496053 -2.6620617 -1.8616242 -1.477757 -1.3053322 -1.7557137 -2.3030643 -2.3630872 -2.1475594 -1.9414444 -1.7849813 -1.9083776][-3.2720976 -2.9618263 -2.8631105 -2.6240673 -1.738096 -0.71091461 0.045280933 0.27709436 -0.60529232 -1.7745891 -2.2587333 -2.0815043 -1.6644835 -1.2333367 -1.140044][-2.5042477 -1.9539247 -1.7646825 -1.4886253 -0.5676291 0.75367546 2.0727043 2.6179409 1.459105 -0.31037998 -1.3275959 -1.3675694 -0.93355441 -0.46830082 -0.39341831][-1.6875584 -0.796283 -0.38211536 -0.020093918 0.9787879 2.5987635 4.4139223 5.2578793 3.8424129 1.5168204 -0.043306828 -0.4365859 -0.1731329 0.17576075 0.15872335][-1.0698736 0.034205914 0.51385832 0.80185223 1.8482227 3.676506 5.752492 6.7676582 5.2251539 2.6072869 0.71949482 0.024952888 0.15082693 0.45095158 0.31207895][-1.6727111 -0.58330917 -0.0856576 0.152143 1.2108817 2.9431543 4.7282381 5.5495825 4.1584568 1.7629881 -0.06959486 -0.95976448 -0.90818858 -0.45124578 -0.44095802][-3.1246018 -2.1361296 -1.5982585 -1.3375638 -0.37091494 0.99877214 2.211369 2.7298288 1.6694317 -0.13354874 -1.4921222 -2.3223362 -2.3080292 -1.7262559 -1.5272629][-4.2542639 -3.2377615 -2.5999894 -2.3109794 -1.5826092 -0.73683524 -0.074170589 0.24224567 -0.49674797 -1.7097673 -2.5249662 -3.1112795 -3.0095291 -2.340179 -2.0222692][-5.3666472 -4.3977222 -3.7147961 -3.3396733 -2.7500205 -2.35142 -2.0808434 -1.8540833 -2.3620408 -3.1532826 -3.6142302 -3.9550107 -3.7118163 -2.9733024 -2.5261269][-5.9952755 -5.1133804 -4.4970074 -4.1176448 -3.6547787 -3.61025 -3.5760951 -3.3424511 -3.6613045 -4.1389704 -4.3588057 -4.5017929 -4.1853023 -3.5578251 -3.2273989][-6.4387026 -5.756381 -5.2816606 -4.9717627 -4.6682868 -4.9264674 -5.0688453 -4.8304143 -5.0306587 -5.3237329 -5.4178405 -5.4508157 -5.119247 -4.5641584 -4.2584863][-6.7430024 -6.3600235 -6.0595546 -5.8060617 -5.5623803 -5.8984771 -6.1097746 -5.9088016 -6.0190063 -6.1679149 -6.1768332 -6.1792822 -5.9049749 -5.4209771 -5.088501][-6.3500314 -6.181673 -5.9855995 -5.7461486 -5.5089331 -5.7544479 -5.9240441 -5.7983851 -5.859756 -5.9067249 -5.856123 -5.8523512 -5.6967397 -5.358572 -5.0592413]]...]
INFO - root - 2017-12-11 09:50:19.652833: step 210, loss = 0.27, batch loss = 0.19 (18.2 examples/sec; 0.439 sec/batch; 40h:32m:37s remains)
INFO - root - 2017-12-11 09:50:24.103351: step 220, loss = 0.29, batch loss = 0.21 (17.3 examples/sec; 0.462 sec/batch; 42h:40m:59s remains)
INFO - root - 2017-12-11 09:50:28.562472: step 230, loss = 0.30, batch loss = 0.21 (18.1 examples/sec; 0.443 sec/batch; 40h:52m:49s remains)
INFO - root - 2017-12-11 09:50:33.025419: step 240, loss = 0.28, batch loss = 0.19 (17.5 examples/sec; 0.457 sec/batch; 42h:09m:40s remains)
INFO - root - 2017-12-11 09:50:37.501176: step 250, loss = 0.30, batch loss = 0.22 (18.2 examples/sec; 0.441 sec/batch; 40h:39m:57s remains)
INFO - root - 2017-12-11 09:50:42.061105: step 260, loss = 0.34, batch loss = 0.26 (17.9 examples/sec; 0.448 sec/batch; 41h:20m:29s remains)
INFO - root - 2017-12-11 09:50:46.496685: step 270, loss = 0.35, batch loss = 0.27 (17.7 examples/sec; 0.451 sec/batch; 41h:36m:04s remains)
INFO - root - 2017-12-11 09:50:50.639652: step 280, loss = 0.27, batch loss = 0.18 (18.1 examples/sec; 0.442 sec/batch; 40h:45m:01s remains)
INFO - root - 2017-12-11 09:50:55.112112: step 290, loss = 0.28, batch loss = 0.19 (18.5 examples/sec; 0.433 sec/batch; 39h:55m:36s remains)
INFO - root - 2017-12-11 09:50:59.535731: step 300, loss = 0.42, batch loss = 0.34 (17.8 examples/sec; 0.449 sec/batch; 41h:26m:16s remains)
2017-12-11 09:51:00.010286: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0571942 -3.179873 -3.466773 -3.7555416 -3.7854261 -3.2775247 -2.422466 -1.8608773 -1.7401836 -1.718874 -1.8974211 -2.3904357 -3.5643311 -4.3044615 -3.9433062][-3.78628 -3.8573184 -3.9729576 -3.9621108 -3.5951657 -2.6779861 -1.4789352 -0.60198712 -0.25010967 -0.16165257 -0.50814033 -1.3767586 -2.9005532 -3.8850584 -3.694479][-4.3920841 -4.5292554 -4.6209192 -4.4952908 -3.977514 -2.9970829 -1.8421321 -0.92646408 -0.36607456 -0.005546093 -0.16052055 -1.0278904 -2.52109 -3.4714 -3.3736467][-4.7039375 -4.7983103 -4.8281994 -4.6241889 -4.0872836 -3.3252707 -2.6424913 -2.1390009 -1.6635661 -1.1287336 -0.9645822 -1.51387 -2.5862651 -3.209013 -3.090523][-4.3020058 -4.1740947 -4.0420365 -3.6753294 -3.0241194 -2.3396358 -2.10931 -2.2173929 -2.1884761 -1.8374889 -1.5977054 -1.89645 -2.5440731 -2.83454 -2.6988509][-3.4271538 -3.0907667 -2.8122196 -2.2559631 -1.3512356 -0.39551926 -0.22064734 -0.75550318 -1.221698 -1.2008464 -1.0276353 -1.2388182 -1.6723578 -1.7943504 -1.7114038][-2.3855846 -1.8930728 -1.4646485 -0.71903968 0.46725559 1.8116965 2.1912589 1.4836736 0.66234159 0.37242746 0.45765686 0.28107214 -0.022031784 -0.021823883 0.010868073][-1.5422282 -0.96910024 -0.44283223 0.42663574 1.8658533 3.6184969 4.3319798 3.6789036 2.6840792 2.1870894 2.2338104 2.1481414 2.0190721 2.1580777 2.1424613][-1.2030313 -0.68418145 -0.24567223 0.54135132 2.0233231 3.9215393 4.7933559 4.2365589 3.2335138 2.7266817 2.8951306 3.0533338 3.2313175 3.6198683 3.685873][-1.5157254 -1.1571074 -0.94647932 -0.44118094 0.762238 2.4192295 3.2059383 2.7670102 1.9660001 1.6580892 2.0476422 2.50909 3.037652 3.7324171 3.991621][-2.5249331 -2.417397 -2.4373972 -2.2210712 -1.3893821 -0.12837219 0.48944712 0.19926023 -0.3129611 -0.37656212 0.15382719 0.78363132 1.5343747 2.4284835 2.8543081][-3.6369255 -3.7783113 -3.9888694 -4.0028963 -3.5248847 -2.6805708 -2.2727327 -2.470613 -2.7406032 -2.6318288 -2.1284811 -1.5726326 -0.86369944 0.013191223 0.51197433][-4.1690097 -4.3817668 -4.652101 -4.7899594 -4.5996795 -4.158999 -3.9922922 -4.1714177 -4.3277321 -4.2024469 -3.8702121 -3.5345659 -3.0518045 -2.3778908 -1.9027035][-3.9391561 -4.0970621 -4.2953134 -4.4218612 -4.3901424 -4.2449937 -4.250721 -4.4236217 -4.5682859 -4.533123 -4.3839927 -4.22935 -3.9839213 -3.605901 -3.2745392][-3.432394 -3.5340314 -3.6601377 -3.7540696 -3.8005633 -3.8137062 -3.8771396 -4.0026402 -4.13692 -4.1973987 -4.1891966 -4.1350317 -4.0266161 -3.8558998 -3.6639669]]...]
INFO - root - 2017-12-11 09:51:04.416455: step 310, loss = 0.26, batch loss = 0.17 (18.5 examples/sec; 0.432 sec/batch; 39h:52m:05s remains)
INFO - root - 2017-12-11 09:51:08.932819: step 320, loss = 0.26, batch loss = 0.18 (17.8 examples/sec; 0.448 sec/batch; 41h:21m:39s remains)
INFO - root - 2017-12-11 09:51:13.370441: step 330, loss = 0.29, batch loss = 0.21 (18.0 examples/sec; 0.443 sec/batch; 40h:54m:03s remains)
INFO - root - 2017-12-11 09:51:17.762715: step 340, loss = 0.25, batch loss = 0.16 (18.7 examples/sec; 0.429 sec/batch; 39h:33m:40s remains)
INFO - root - 2017-12-11 09:51:22.251180: step 350, loss = 0.31, batch loss = 0.23 (18.4 examples/sec; 0.434 sec/batch; 40h:04m:11s remains)
INFO - root - 2017-12-11 09:51:26.670445: step 360, loss = 0.39, batch loss = 0.31 (18.3 examples/sec; 0.438 sec/batch; 40h:23m:00s remains)
INFO - root - 2017-12-11 09:51:31.109948: step 370, loss = 0.35, batch loss = 0.27 (18.4 examples/sec; 0.435 sec/batch; 40h:05m:13s remains)
INFO - root - 2017-12-11 09:51:35.525021: step 380, loss = 0.31, batch loss = 0.23 (18.4 examples/sec; 0.436 sec/batch; 40h:11m:38s remains)
INFO - root - 2017-12-11 09:51:39.940112: step 390, loss = 0.29, batch loss = 0.21 (18.2 examples/sec; 0.440 sec/batch; 40h:33m:24s remains)
INFO - root - 2017-12-11 09:51:44.262262: step 400, loss = 0.23, batch loss = 0.15 (21.2 examples/sec; 0.377 sec/batch; 34h:45m:33s remains)
2017-12-11 09:51:44.682465: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5809331 -4.8132305 -5.0161366 -5.4016061 -5.9263368 -6.0048194 -5.5906029 -5.3590627 -5.4607491 -5.9387803 -6.3841248 -6.399889 -6.5473232 -7.1145363 -7.6947279][-5.0352979 -5.2721257 -5.4584537 -5.8466315 -6.3862438 -6.396637 -5.8094053 -5.4030008 -5.4405274 -5.9629874 -6.355237 -6.2181778 -6.1691408 -6.6384449 -7.3076897][-5.4143057 -5.5190096 -5.4714155 -5.6167374 -5.898572 -5.618578 -4.7950115 -4.2807603 -4.447309 -5.2196088 -5.7305045 -5.6114841 -5.4533978 -5.7990155 -6.4821434][-5.2120585 -5.1778221 -4.864718 -4.7450042 -4.7021871 -4.03381 -2.916862 -2.2778685 -2.6130834 -3.6537652 -4.3162651 -4.2965264 -4.1555181 -4.51479 -5.3074007][-4.24913 -3.9277003 -3.3057425 -3.0071216 -2.8254111 -2.0679867 -0.93737769 -0.29324245 -0.70755434 -1.7923717 -2.4064853 -2.3708673 -2.2560515 -2.7615004 -3.8167577][-3.6913364 -3.1014163 -2.1563969 -1.5676043 -1.1151145 -0.22131395 0.89462519 1.5275135 1.1008329 0.023869991 -0.5820539 -0.58073211 -0.524364 -1.2160318 -2.5561776][-3.3150992 -2.6979036 -1.728883 -1.0876484 -0.52933359 0.45162964 1.6624637 2.4789581 2.2438707 1.2501664 0.53229713 0.3381052 0.24103355 -0.57373357 -2.0598035][-2.3948698 -1.8610666 -1.1962523 -1.0183899 -0.98015976 -0.3961997 0.72508717 1.8458815 2.1063185 1.4775238 0.76764822 0.35169697 0.0072627068 -0.91610861 -2.4134016][-1.0836709 -0.41056585 0.005865097 -0.41470289 -1.2262824 -1.4143958 -0.73942566 0.43167305 1.1293325 0.96337414 0.40998888 -0.14038277 -0.74352312 -1.7944481 -3.2653365][0.14208984 1.12005 1.4265094 0.47805834 -1.2061622 -2.2709987 -2.2474446 -1.2960865 -0.36746025 -0.14831161 -0.48474288 -1.0688949 -1.8344846 -2.9447179 -4.3136573][0.66559219 1.9564586 2.32379 1.1237082 -1.1276417 -2.893702 -3.5026851 -2.882988 -1.9146228 -1.4913642 -1.7167821 -2.3580217 -3.2192638 -4.2928944 -5.4472904][0.12822485 1.6330333 2.2320905 1.1858864 -1.1087117 -3.1708667 -4.2297592 -3.9877222 -3.2140431 -2.8453679 -3.1234682 -3.8545508 -4.7458267 -5.6906772 -6.53109][-1.2730818 0.068431377 0.74869537 0.032624245 -1.8236175 -3.6292465 -4.7276063 -4.7347584 -4.240191 -4.0314569 -4.3662562 -5.0940413 -5.9284797 -6.7239203 -7.2864313][-3.107471 -2.0423567 -1.3499382 -1.6883748 -2.9077537 -4.1625061 -5.0031056 -5.0897608 -4.84836 -4.8102207 -5.1601119 -5.8093228 -6.521903 -7.1499085 -7.4907417][-4.5806141 -3.7194328 -3.0393577 -3.102576 -3.7991142 -4.5737534 -5.1289711 -5.2198863 -5.1443529 -5.1992736 -5.4865613 -5.9720068 -6.4924278 -6.9210858 -7.0882039]]...]
INFO - root - 2017-12-11 09:51:49.180761: step 410, loss = 0.35, batch loss = 0.27 (17.7 examples/sec; 0.453 sec/batch; 41h:44m:35s remains)
INFO - root - 2017-12-11 09:51:53.640172: step 420, loss = 0.33, batch loss = 0.25 (18.1 examples/sec; 0.442 sec/batch; 40h:45m:22s remains)
INFO - root - 2017-12-11 09:51:58.159869: step 430, loss = 0.32, batch loss = 0.23 (17.7 examples/sec; 0.452 sec/batch; 41h:41m:42s remains)
INFO - root - 2017-12-11 09:52:02.594740: step 440, loss = 0.26, batch loss = 0.18 (18.5 examples/sec; 0.432 sec/batch; 39h:51m:44s remains)
INFO - root - 2017-12-11 09:52:07.100547: step 450, loss = 0.32, batch loss = 0.24 (18.0 examples/sec; 0.445 sec/batch; 41h:01m:22s remains)
INFO - root - 2017-12-11 09:52:11.539105: step 460, loss = 0.28, batch loss = 0.20 (17.5 examples/sec; 0.458 sec/batch; 42h:16m:02s remains)
INFO - root - 2017-12-11 09:52:16.000919: step 470, loss = 0.24, batch loss = 0.15 (18.1 examples/sec; 0.443 sec/batch; 40h:49m:37s remains)
INFO - root - 2017-12-11 09:52:20.462547: step 480, loss = 0.25, batch loss = 0.16 (17.4 examples/sec; 0.461 sec/batch; 42h:30m:57s remains)
INFO - root - 2017-12-11 09:52:24.859978: step 490, loss = 0.26, batch loss = 0.18 (18.1 examples/sec; 0.442 sec/batch; 40h:45m:41s remains)
INFO - root - 2017-12-11 09:52:29.287562: step 500, loss = 0.30, batch loss = 0.21 (17.3 examples/sec; 0.462 sec/batch; 42h:36m:24s remains)
2017-12-11 09:52:29.732501: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8770547 -3.9413953 -3.7950456 -3.9825096 -3.9685559 -3.6465058 -3.0639329 -2.3661642 -1.8019116 -1.9133573 -2.1668406 -2.1602972 -2.4622436 -2.7946949 -2.7683413][-2.7885709 -3.1972051 -3.3759723 -3.7555833 -3.7860355 -3.3896036 -2.6863203 -1.956533 -1.4389288 -1.4945633 -1.7244084 -1.9714103 -2.5615997 -3.1524532 -3.3295097][-1.6241469 -2.3496084 -2.8816781 -3.5290403 -3.5995376 -3.0715127 -2.2741654 -1.6029828 -1.2222934 -1.3401506 -1.6037357 -2.0329597 -2.7733691 -3.4520583 -3.7558239][-0.78907251 -1.7136214 -2.5017266 -3.3274262 -3.3324058 -2.5717382 -1.6461818 -1.0604236 -0.86860704 -1.1554544 -1.5911732 -2.2166541 -3.0122442 -3.6377423 -3.9588764][-0.11931944 -1.2036605 -2.2192986 -3.1345353 -2.951313 -1.8302131 -0.63552213 -0.018113613 0.036664963 -0.47928381 -1.2406092 -2.1552942 -2.9981284 -3.5553098 -3.8851855][0.43827629 -0.68896627 -1.8804781 -2.9132304 -2.6203487 -1.1666155 0.3830514 1.2623401 1.4388709 0.85940933 -0.2076292 -1.445071 -2.4188955 -3.0430703 -3.5127144][0.79937458 -0.14255285 -1.3455808 -2.51298 -2.3194189 -0.76080823 1.0318241 2.2631073 2.7835145 2.3769102 1.1431336 -0.34551477 -1.4695182 -2.2278268 -2.8833606][0.8470726 0.23854828 -0.83766389 -2.0830207 -2.1261706 -0.75433326 0.98731041 2.4235845 3.3285189 3.2940965 2.1423621 0.63397741 -0.46701384 -1.2153733 -1.9056296][0.88011885 0.518126 -0.51919508 -1.8649354 -2.2137196 -1.2479439 0.18349266 1.5583048 2.672256 3.0130334 2.1350493 0.86703634 0.073774338 -0.370327 -0.80914855][1.055294 0.89440441 -0.21444464 -1.7729485 -2.4632745 -1.9722536 -0.90997434 0.3228569 1.5073743 2.0661182 1.4020429 0.34776258 -0.16659307 -0.28749943 -0.34105587][1.020864 1.1778374 0.11850691 -1.5743747 -2.553369 -2.5263696 -1.8622899 -0.85332894 0.26305056 0.88954496 0.35070419 -0.59897041 -0.99591088 -0.90760159 -0.56605411][0.83138561 1.3759913 0.50571728 -1.2049973 -2.4170399 -2.8516185 -2.6270134 -1.9135535 -0.98410606 -0.36437798 -0.751745 -1.6082487 -1.9779208 -1.7351739 -1.0141375][0.40906191 1.2471743 0.5795536 -1.0933444 -2.4960141 -3.2895408 -3.3838046 -2.908005 -2.1810668 -1.5855067 -1.8094056 -2.5230377 -2.8698153 -2.5108314 -1.4518538][-0.059446812 0.83520412 0.27586126 -1.3274107 -2.8478367 -3.8752525 -4.1530294 -3.8462019 -3.2951708 -2.7461035 -2.8130381 -3.3193891 -3.5920594 -3.1592512 -1.8811424][-0.40231037 0.37335062 -0.12554502 -1.5633349 -3.048413 -4.1600308 -4.5520859 -4.3961582 -4.0146289 -3.5598173 -3.5561774 -3.9135315 -4.1658411 -3.8017421 -2.5091226]]...]
INFO - root - 2017-12-11 09:52:34.173564: step 510, loss = 0.36, batch loss = 0.27 (18.2 examples/sec; 0.441 sec/batch; 40h:37m:32s remains)
INFO - root - 2017-12-11 09:52:38.678565: step 520, loss = 0.28, batch loss = 0.20 (18.8 examples/sec; 0.425 sec/batch; 39h:13m:35s remains)
INFO - root - 2017-12-11 09:52:42.263544: step 530, loss = 0.27, batch loss = 0.19 (17.4 examples/sec; 0.461 sec/batch; 42h:31m:02s remains)
INFO - root - 2017-12-11 09:52:46.735940: step 540, loss = 0.33, batch loss = 0.25 (18.2 examples/sec; 0.439 sec/batch; 40h:26m:05s remains)
INFO - root - 2017-12-11 09:52:51.216049: step 550, loss = 0.29, batch loss = 0.20 (17.6 examples/sec; 0.455 sec/batch; 41h:54m:36s remains)
INFO - root - 2017-12-11 09:52:55.733678: step 560, loss = 0.29, batch loss = 0.21 (17.9 examples/sec; 0.446 sec/batch; 41h:08m:03s remains)
INFO - root - 2017-12-11 09:53:00.240566: step 570, loss = 0.29, batch loss = 0.21 (17.8 examples/sec; 0.451 sec/batch; 41h:33m:03s remains)
INFO - root - 2017-12-11 09:53:04.695095: step 580, loss = 0.26, batch loss = 0.17 (17.7 examples/sec; 0.453 sec/batch; 41h:46m:11s remains)
INFO - root - 2017-12-11 09:53:09.118843: step 590, loss = 0.32, batch loss = 0.23 (17.4 examples/sec; 0.460 sec/batch; 42h:24m:57s remains)
INFO - root - 2017-12-11 09:53:13.616333: step 600, loss = 0.27, batch loss = 0.19 (17.4 examples/sec; 0.460 sec/batch; 42h:23m:17s remains)
2017-12-11 09:53:14.066107: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.9701161 -5.959137 -5.9687023 -6.0373073 -6.1564331 -6.2761445 -6.3749242 -6.45378 -6.4646387 -6.4162097 -6.3605127 -6.3185163 -6.2899818 -6.2554512 -6.2177091][-7.1983843 -7.1083431 -7.0011735 -6.9973536 -7.1185646 -7.2660446 -7.3687105 -7.4037752 -7.2912731 -7.1009369 -6.9769907 -6.9412918 -6.9587193 -6.9790039 -7.0079][-7.4100809 -7.2099943 -6.9900913 -6.942009 -7.1157694 -7.3357315 -7.4447637 -7.3937688 -7.1061125 -6.802269 -6.6964769 -6.741909 -6.8389373 -6.9134731 -7.0014534][-6.1060233 -5.6968093 -5.308012 -5.1704187 -5.3539963 -5.6060419 -5.68163 -5.5418367 -5.1760168 -4.9909735 -5.173985 -5.4469185 -5.6823568 -5.8519831 -6.0171394][-4.2162185 -3.6524532 -3.1253119 -2.8230281 -2.8316283 -2.91288 -2.7943249 -2.4923673 -2.1138341 -2.22495 -2.8519621 -3.3877206 -3.7984774 -4.1594114 -4.4733887][-2.0624192 -1.432364 -0.83380151 -0.3318224 -0.00056123734 0.27405739 0.7198801 1.2128606 1.4741192 0.92241669 -0.22579908 -1.0204637 -1.6063714 -2.2015975 -2.6901226][-0.43266368 0.25464153 0.91883183 1.6592627 2.4490833 3.259367 4.2539349 5.1007833 5.2327118 4.250535 2.6797156 1.6939988 0.97074938 0.21420193 -0.37052536][-0.05197525 0.47423315 1.0089226 1.7939677 2.8825431 4.1500282 5.7205067 6.9962835 7.0964384 5.9414263 4.2979946 3.3597775 2.6823335 1.9464669 1.3627572][-1.0957861 -0.85079479 -0.551615 0.061136723 1.082633 2.3825893 4.0956469 5.4622316 5.5808277 4.6902266 3.5184569 3.0399895 2.6858582 2.1294656 1.564785][-2.296788 -2.1665301 -1.9327054 -1.4905035 -0.7603786 0.23870707 1.671123 2.7574072 2.7927957 2.2407298 1.5989599 1.5582676 1.466835 1.0344815 0.45704937][-2.9399958 -2.8402205 -2.5432444 -2.1640213 -1.6725113 -0.99096823 -0.0259943 0.55946684 0.3621788 -0.047936916 -0.43894267 -0.34439993 -0.35892105 -0.7718122 -1.41592][-3.7365549 -3.6173768 -3.201957 -2.7906621 -2.4380181 -2.0097382 -1.5031409 -1.3617339 -1.6985228 -1.9782033 -2.1931744 -2.0651996 -2.0459216 -2.4422975 -3.1386542][-3.999109 -3.7304215 -3.2033014 -2.8208749 -2.6707911 -2.5945296 -2.5928717 -2.845511 -3.2683125 -3.4767807 -3.6163759 -3.5224001 -3.4708109 -3.7592025 -4.335494][-4.27249 -3.9505053 -3.4727142 -3.217577 -3.2451262 -3.4093978 -3.6763749 -4.0420671 -4.3764648 -4.4943094 -4.6060181 -4.5894289 -4.549758 -4.7337294 -5.1288438][-4.7193384 -4.4709177 -4.1461058 -4.0209713 -4.0939965 -4.2560077 -4.447844 -4.5937905 -4.6197314 -4.5292726 -4.5513139 -4.5905304 -4.6285291 -4.8231363 -5.1525936]]...]
INFO - root - 2017-12-11 09:53:18.520438: step 610, loss = 0.28, batch loss = 0.19 (18.4 examples/sec; 0.436 sec/batch; 40h:11m:05s remains)
INFO - root - 2017-12-11 09:53:22.936624: step 620, loss = 0.24, batch loss = 0.16 (18.8 examples/sec; 0.425 sec/batch; 39h:11m:43s remains)
INFO - root - 2017-12-11 09:53:27.373907: step 630, loss = 0.25, batch loss = 0.17 (17.9 examples/sec; 0.447 sec/batch; 41h:09m:58s remains)
INFO - root - 2017-12-11 09:53:31.814328: step 640, loss = 0.32, batch loss = 0.24 (18.1 examples/sec; 0.442 sec/batch; 40h:42m:57s remains)
INFO - root - 2017-12-11 09:53:35.954414: step 650, loss = 0.32, batch loss = 0.24 (18.5 examples/sec; 0.433 sec/batch; 39h:55m:13s remains)
INFO - root - 2017-12-11 09:53:40.405183: step 660, loss = 0.28, batch loss = 0.20 (18.4 examples/sec; 0.434 sec/batch; 39h:58m:31s remains)
INFO - root - 2017-12-11 09:53:44.894437: step 670, loss = 0.32, batch loss = 0.24 (18.0 examples/sec; 0.445 sec/batch; 41h:00m:33s remains)
INFO - root - 2017-12-11 09:53:49.325576: step 680, loss = 0.32, batch loss = 0.24 (18.0 examples/sec; 0.444 sec/batch; 40h:57m:18s remains)
INFO - root - 2017-12-11 09:53:53.832805: step 690, loss = 0.29, batch loss = 0.21 (17.3 examples/sec; 0.462 sec/batch; 42h:33m:07s remains)
INFO - root - 2017-12-11 09:53:58.290353: step 700, loss = 0.33, batch loss = 0.25 (16.3 examples/sec; 0.491 sec/batch; 45h:15m:33s remains)
2017-12-11 09:53:58.761776: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5044842 -3.5616398 -3.5162063 -3.4324362 -3.5277503 -3.9172082 -4.4116988 -4.811831 -4.9093537 -4.621758 -4.0054932 -3.3140688 -2.8285375 -2.6013966 -2.523598][-3.7922819 -3.802021 -3.6525817 -3.4232049 -3.3224335 -3.5581515 -4.0064111 -4.5113063 -4.9165096 -5.05517 -4.7888031 -4.2439413 -3.6806986 -3.2274191 -2.920609][-3.8965533 -3.7550836 -3.4648166 -3.2083454 -3.0544176 -3.1557922 -3.3942182 -3.7264853 -4.2319236 -4.8042068 -5.0635438 -4.9274149 -4.5190349 -3.9608064 -3.4609702][-3.8600125 -3.4486761 -2.9554024 -2.7605462 -2.7083197 -2.7668881 -2.8099964 -2.8871803 -3.3188086 -4.1896763 -4.9247074 -5.2714906 -5.1729379 -4.6575465 -4.0564532][-3.6803579 -2.9638538 -2.1959736 -1.9292331 -1.7639916 -1.5196218 -1.2521527 -1.0604808 -1.4423542 -2.6683192 -3.9608052 -4.9487872 -5.3761125 -5.1310244 -4.5707994][-3.5263987 -2.5963869 -1.53547 -1.0060716 -0.43157744 0.39919186 1.0427575 1.4212499 0.98731565 -0.61147332 -2.4776549 -4.1274834 -5.1261697 -5.272718 -4.8481269][-3.6441889 -2.7080011 -1.425808 -0.52211261 0.63085222 2.1757479 3.2188897 3.746954 3.2443895 1.3501449 -1.0064454 -3.2118008 -4.6757407 -5.2014871 -4.9136157][-3.9485936 -3.278461 -2.0779796 -1.0441644 0.45064592 2.5109119 3.9517632 4.7607241 4.4384604 2.6173162 0.095837116 -2.3798261 -4.09902 -4.9260688 -4.7800412][-4.1699686 -3.9757748 -3.2392216 -2.4894938 -1.0681682 1.1863556 3.0126863 4.250001 4.3866425 3.0288849 0.7182703 -1.7184212 -3.4936047 -4.5094314 -4.5019732][-4.1260777 -4.4206486 -4.3042417 -4.0685592 -3.0146995 -0.90593433 1.1494021 2.7997293 3.4623322 2.7621689 0.91187286 -1.2780032 -2.9912348 -4.088306 -4.1806707][-3.8106439 -4.3763404 -4.7270956 -4.9603167 -4.4033551 -2.7645643 -0.83978057 0.9499402 1.9629951 1.8592119 0.56780529 -1.220305 -2.7457895 -3.7903507 -3.9221902][-3.3452673 -3.9525521 -4.53933 -5.0911489 -5.0247178 -4.0284438 -2.5141115 -0.82806325 0.33669996 0.6402998 -0.14439821 -1.4938385 -2.7586803 -3.6578898 -3.7799106][-2.8272276 -3.310992 -3.941855 -4.6723776 -5.0427923 -4.7436471 -3.8149891 -2.40862 -1.1935844 -0.57551885 -0.86075258 -1.7286115 -2.6922359 -3.4342084 -3.5945311][-2.386951 -2.6753008 -3.1880755 -3.8937402 -4.4944458 -4.741065 -4.4236674 -3.4351406 -2.3149507 -1.5215132 -1.3846071 -1.7951927 -2.4632092 -3.0844874 -3.3219101][-2.181015 -2.2739165 -2.5369573 -2.9929857 -3.535291 -4.0530872 -4.25234 -3.7753341 -2.9314761 -2.1267405 -1.7005961 -1.7339053 -2.1390266 -2.6707664 -2.9842267]]...]
INFO - root - 2017-12-11 09:54:03.223969: step 710, loss = 0.34, batch loss = 0.26 (18.1 examples/sec; 0.443 sec/batch; 40h:47m:03s remains)
INFO - root - 2017-12-11 09:54:07.705071: step 720, loss = 0.27, batch loss = 0.19 (17.7 examples/sec; 0.452 sec/batch; 41h:40m:32s remains)
INFO - root - 2017-12-11 09:54:12.217459: step 730, loss = 0.31, batch loss = 0.23 (17.6 examples/sec; 0.454 sec/batch; 41h:48m:42s remains)
INFO - root - 2017-12-11 09:54:16.682485: step 740, loss = 0.37, batch loss = 0.28 (18.2 examples/sec; 0.440 sec/batch; 40h:30m:31s remains)
INFO - root - 2017-12-11 09:54:21.059617: step 750, loss = 0.32, batch loss = 0.24 (18.1 examples/sec; 0.441 sec/batch; 40h:40m:08s remains)
INFO - root - 2017-12-11 09:54:25.511034: step 760, loss = 0.26, batch loss = 0.18 (17.7 examples/sec; 0.453 sec/batch; 41h:42m:12s remains)
INFO - root - 2017-12-11 09:54:29.920757: step 770, loss = 0.27, batch loss = 0.19 (18.6 examples/sec; 0.430 sec/batch; 39h:37m:48s remains)
INFO - root - 2017-12-11 09:54:34.079120: step 780, loss = 0.37, batch loss = 0.29 (17.5 examples/sec; 0.457 sec/batch; 42h:08m:02s remains)
INFO - root - 2017-12-11 09:54:38.482071: step 790, loss = 0.39, batch loss = 0.31 (18.3 examples/sec; 0.436 sec/batch; 40h:10m:54s remains)
INFO - root - 2017-12-11 09:54:43.048147: step 800, loss = 0.26, batch loss = 0.18 (17.6 examples/sec; 0.455 sec/batch; 41h:57m:30s remains)
2017-12-11 09:54:43.511231: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4083147 -4.0054712 -3.7394633 -3.4601123 -2.6668735 -1.4044518 -0.30884504 0.034864902 -0.1622386 -0.5491209 -1.1043775 -1.8714976 -2.7567415 -3.252146 -3.0458727][-4.9354286 -4.5791597 -4.245513 -3.9256239 -3.1644239 -1.9412155 -0.81935811 -0.30457067 -0.30655575 -0.52458811 -0.93003249 -1.4983425 -2.1044104 -2.3709369 -1.9752085][-4.7353315 -4.2923169 -3.8040261 -3.4101026 -2.7339835 -1.673568 -0.68421745 -0.20131636 -0.26620293 -0.63841176 -1.1741946 -1.7421737 -2.2576208 -2.4691658 -2.0029402][-3.82028 -3.3163466 -2.6666551 -2.1645873 -1.5461283 -0.64250255 0.20598221 0.52940989 0.12707329 -0.71130657 -1.5312364 -2.133321 -2.6006289 -2.7642426 -2.2355282][-2.8461835 -2.2583647 -1.376199 -0.608485 0.20835018 1.25179 2.2227645 2.4789824 1.663137 0.18665981 -1.1187003 -1.9456739 -2.5070148 -2.6671298 -2.1121247][-2.1650004 -1.4957967 -0.3848896 0.68516684 1.7555866 2.9836349 4.1337852 4.3838396 3.2214732 1.1354346 -0.70716476 -1.8988392 -2.7035871 -2.8872862 -2.2293489][-1.9168885 -1.3475246 -0.17734003 1.1284404 2.481041 3.9528027 5.3798304 5.818141 4.6387482 2.3009338 0.20933867 -1.1218688 -1.9909613 -2.0533454 -1.18957][-2.0748551 -1.6201611 -0.53140354 0.78251934 2.169549 3.642724 5.1293383 5.7097225 4.7365026 2.559217 0.5973115 -0.60083747 -1.3338706 -1.1661525 -0.060779572][-2.5503521 -2.2041523 -1.3116906 -0.22468185 0.89079094 2.0398693 3.2321658 3.7483149 3.0646739 1.3658047 -0.17891979 -1.0696521 -1.546653 -1.1344762 0.16217756][-3.3881202 -3.199791 -2.5740576 -1.7222414 -0.78983212 0.19503307 1.2323275 1.8297234 1.5636454 0.42649031 -0.7126081 -1.3763289 -1.6703725 -1.1457171 0.19447851][-4.8346796 -4.7609468 -4.3998985 -3.8006039 -3.0737772 -2.2724166 -1.4196992 -0.8006072 -0.74601579 -1.4181488 -2.2496159 -2.799221 -2.9945149 -2.4339006 -1.1121302][-6.1637797 -6.1744161 -6.048913 -5.7139888 -5.2283316 -4.6535664 -4.0377288 -3.52922 -3.3711543 -3.7675748 -4.3767133 -4.8299522 -4.9421725 -4.3705297 -3.129925][-6.5416107 -6.574955 -6.6009345 -6.4865685 -6.2137718 -5.8255157 -5.41211 -5.0604968 -4.930305 -5.1896324 -5.6378126 -6.0258579 -6.1337848 -5.6869969 -4.7085886][-6.067183 -6.0831852 -6.1636562 -6.1771736 -6.0854368 -5.9092331 -5.7268 -5.5677519 -5.5170245 -5.6962943 -5.990623 -6.2796226 -6.4226589 -6.2284365 -5.679265][-5.1333375 -5.1502013 -5.2254615 -5.25799 -5.2351642 -5.1770334 -5.1322756 -5.08963 -5.0877247 -5.2044549 -5.3773227 -5.5754042 -5.7440195 -5.770606 -5.5964837]]...]
INFO - root - 2017-12-11 09:54:47.977237: step 810, loss = 0.30, batch loss = 0.22 (18.0 examples/sec; 0.444 sec/batch; 40h:54m:55s remains)
INFO - root - 2017-12-11 09:54:52.466564: step 820, loss = 0.26, batch loss = 0.17 (17.9 examples/sec; 0.447 sec/batch; 41h:12m:22s remains)
INFO - root - 2017-12-11 09:54:56.980653: step 830, loss = 0.36, batch loss = 0.28 (16.9 examples/sec; 0.473 sec/batch; 43h:36m:50s remains)
INFO - root - 2017-12-11 09:55:01.463649: step 840, loss = 0.25, batch loss = 0.16 (17.9 examples/sec; 0.448 sec/batch; 41h:14m:56s remains)
INFO - root - 2017-12-11 09:55:05.924592: step 850, loss = 0.22, batch loss = 0.14 (18.1 examples/sec; 0.442 sec/batch; 40h:43m:05s remains)
INFO - root - 2017-12-11 09:55:10.399372: step 860, loss = 0.36, batch loss = 0.28 (17.8 examples/sec; 0.450 sec/batch; 41h:25m:34s remains)
INFO - root - 2017-12-11 09:55:14.860520: step 870, loss = 0.30, batch loss = 0.22 (17.5 examples/sec; 0.458 sec/batch; 42h:09m:03s remains)
INFO - root - 2017-12-11 09:55:19.298263: step 880, loss = 0.33, batch loss = 0.25 (18.2 examples/sec; 0.440 sec/batch; 40h:30m:16s remains)
INFO - root - 2017-12-11 09:55:23.728074: step 890, loss = 0.33, batch loss = 0.25 (18.1 examples/sec; 0.442 sec/batch; 40h:42m:40s remains)
INFO - root - 2017-12-11 09:55:27.885277: step 900, loss = 0.25, batch loss = 0.17 (18.1 examples/sec; 0.442 sec/batch; 40h:44m:24s remains)
2017-12-11 09:55:28.323046: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.25673 -4.364418 -4.5070138 -4.609756 -4.656713 -4.6485591 -4.5869741 -4.5744886 -4.598392 -4.5875382 -4.5209455 -4.400589 -4.2349963 -3.9923787 -3.6976297][-4.6885567 -4.8834939 -5.0522861 -5.1514893 -5.1872454 -5.1537447 -5.076117 -5.1281958 -5.3049951 -5.4428835 -5.430459 -5.3001318 -5.0706468 -4.6744227 -4.175674][-5.0669117 -5.2842455 -5.355649 -5.3538923 -5.3278933 -5.2279663 -5.1754951 -5.4082603 -5.9032459 -6.3600287 -6.4977703 -6.4210625 -6.1641226 -5.587585 -4.8166027][-5.2026553 -5.3408689 -5.1925888 -4.9957213 -4.8363152 -4.5492797 -4.4461451 -4.8611236 -5.73715 -6.6326351 -7.0511489 -7.1763282 -7.0646296 -6.447628 -5.4825606][-5.0645952 -5.0055437 -4.5025411 -3.9535129 -3.4872868 -2.7918055 -2.436167 -2.9248319 -4.1691093 -5.5743113 -6.4130335 -6.9274406 -7.18799 -6.793704 -5.8746715][-4.9215279 -4.6095428 -3.6764975 -2.6335716 -1.6704271 -0.36810064 0.44931936 0.00023508072 -1.5592697 -3.4506052 -4.7816324 -5.8023758 -6.5575352 -6.5603838 -5.9028473][-4.9039121 -4.3683906 -3.0643287 -1.5166502 0.027987003 2.0487032 3.5275803 3.2459559 1.4420938 -0.85197973 -2.642848 -4.1841168 -5.4415369 -5.8581328 -5.5332975][-5.1629357 -4.6401095 -3.2678316 -1.4403772 0.55361843 3.212388 5.4725943 5.6618681 3.8716192 1.3344417 -0.85307956 -2.8429511 -4.5180855 -5.24758 -5.13993][-5.6051092 -5.3495722 -4.2583804 -2.5206606 -0.45139003 2.405004 5.1669712 6.0317516 4.7268028 2.3218923 -0.041918755 -2.29865 -4.2480121 -5.1730461 -5.1391745][-5.80497 -5.876708 -5.2498364 -3.9428444 -2.2355876 0.22407627 2.8559012 4.1598749 3.6103897 1.7719727 -0.37526941 -2.5548635 -4.4937778 -5.4565277 -5.4087863][-5.6773338 -6.0155926 -5.8827324 -5.2139668 -4.169539 -2.4805927 -0.49522305 0.75269079 0.76002312 -0.33002377 -1.8837962 -3.6097486 -5.2093234 -5.9861064 -5.8160086][-5.36103 -5.8120422 -5.990315 -5.8647633 -5.4917631 -4.61169 -3.4224155 -2.6040473 -2.4882708 -3.0365357 -3.9049983 -4.9594364 -5.9939489 -6.4216547 -6.0765786][-5.0033264 -5.426939 -5.6805573 -5.8103476 -5.8540878 -5.5781918 -5.0534949 -4.7041974 -4.7080841 -5.01019 -5.4216528 -5.9061389 -6.371666 -6.4337492 -5.9764795][-4.6668696 -5.0157628 -5.2473946 -5.4426513 -5.6467943 -5.6949973 -5.56006 -5.4684858 -5.5102005 -5.6633053 -5.85772 -6.0654716 -6.2064028 -6.0601239 -5.5992174][-4.2703128 -4.5254164 -4.7134309 -4.8987646 -5.1120234 -5.2685204 -5.3079705 -5.3183231 -5.3462543 -5.4057159 -5.4981194 -5.5838532 -5.5876346 -5.3937564 -5.0150318]]...]
INFO - root - 2017-12-11 09:55:32.737193: step 910, loss = 0.37, batch loss = 0.28 (18.0 examples/sec; 0.445 sec/batch; 41h:01m:59s remains)
INFO - root - 2017-12-11 09:55:37.156277: step 920, loss = 0.49, batch loss = 0.41 (18.0 examples/sec; 0.446 sec/batch; 41h:02m:42s remains)
INFO - root - 2017-12-11 09:55:41.588083: step 930, loss = 0.27, batch loss = 0.18 (17.3 examples/sec; 0.462 sec/batch; 42h:31m:26s remains)
INFO - root - 2017-12-11 09:55:46.116681: step 940, loss = 0.24, batch loss = 0.16 (17.8 examples/sec; 0.450 sec/batch; 41h:28m:49s remains)
INFO - root - 2017-12-11 09:55:50.613063: step 950, loss = 0.28, batch loss = 0.19 (17.5 examples/sec; 0.458 sec/batch; 42h:10m:58s remains)
INFO - root - 2017-12-11 09:55:55.045827: step 960, loss = 0.32, batch loss = 0.23 (18.5 examples/sec; 0.433 sec/batch; 39h:50m:38s remains)
INFO - root - 2017-12-11 09:55:59.494550: step 970, loss = 0.26, batch loss = 0.18 (17.9 examples/sec; 0.448 sec/batch; 41h:16m:03s remains)
INFO - root - 2017-12-11 09:56:03.938554: step 980, loss = 0.26, batch loss = 0.18 (18.5 examples/sec; 0.432 sec/batch; 39h:44m:20s remains)
INFO - root - 2017-12-11 09:56:08.405285: step 990, loss = 0.29, batch loss = 0.20 (18.4 examples/sec; 0.434 sec/batch; 39h:56m:40s remains)
INFO - root - 2017-12-11 09:56:12.880131: step 1000, loss = 0.30, batch loss = 0.22 (17.4 examples/sec; 0.460 sec/batch; 42h:19m:02s remains)
2017-12-11 09:56:13.355338: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.08419 -3.5724766 -4.1762924 -4.7145219 -4.4662609 -3.4889939 -3.3619523 -4.0172176 -5.071414 -6.260818 -7.5587626 -8.3114824 -7.6499572 -6.2381768 -4.8707771][-1.9222186 -2.6482644 -3.4565811 -4.2328248 -4.1294971 -3.0332961 -2.699615 -3.2921395 -4.4806504 -5.8128548 -7.2017217 -8.0714941 -7.5771794 -6.3562055 -5.0874443][-1.1749659 -2.1734636 -3.1422191 -4.0266147 -3.8981009 -2.5408762 -1.7807014 -2.1112931 -3.3326721 -4.8177056 -6.3597107 -7.4142623 -7.1907768 -6.2765307 -5.17879][-0.90819883 -1.9640453 -2.8467226 -3.5006297 -2.9871149 -1.1470096 0.17620516 0.095727444 -1.3377028 -3.2380707 -5.1382489 -6.4775667 -6.5926723 -6.0302892 -5.1509018][-1.396955 -2.1933339 -2.6987307 -2.7783194 -1.5607624 0.95564938 2.9588361 3.1501875 1.3663092 -1.1914215 -3.6626441 -5.4291039 -6.0002623 -5.8425045 -5.1869159][-2.4125464 -2.7533872 -2.6228762 -1.8246663 0.29118919 3.5202513 6.1443892 6.5216532 4.290205 0.95593691 -2.1945286 -4.4918709 -5.5815105 -5.8171349 -5.3454194][-3.5329478 -3.4464018 -2.695148 -1.1118991 1.6663361 5.2893944 8.2386646 8.7049484 6.1918268 2.3865314 -1.2093573 -3.8997674 -5.3606067 -5.8451338 -5.4926491][-4.4784603 -4.1747513 -3.1882324 -1.4081001 1.3497009 4.7664785 7.6160355 8.1569881 5.8320074 2.2718406 -1.1575735 -3.8207705 -5.3321309 -5.8538938 -5.5431056][-4.5227122 -4.1973677 -3.3980813 -2.09806 -0.10281563 2.4680471 4.7602949 5.2891884 3.4922276 0.72318554 -2.0294662 -4.2559071 -5.49597 -5.8580718 -5.5129108][-3.8532374 -3.6163626 -3.1920156 -2.6645184 -1.7300677 -0.26433229 1.2547302 1.6934276 0.48136806 -1.3853054 -3.334547 -4.9678383 -5.7817688 -5.8861709 -5.4618244][-3.1774919 -3.0731325 -2.9953179 -3.1405609 -3.1127794 -2.6190853 -1.8230309 -1.5115616 -2.2631609 -3.414825 -4.7112951 -5.8267374 -6.2407303 -6.08512 -5.5373917][-2.6837606 -2.6786313 -2.8094327 -3.3603344 -3.9013553 -4.0760078 -3.8395989 -3.7443876 -4.3504753 -5.1729307 -6.1032591 -6.853344 -6.9459419 -6.5329571 -5.7877569][-2.8813214 -2.9964948 -3.2407241 -3.9079432 -4.609889 -5.0375228 -5.0914288 -5.2173142 -5.9029355 -6.6960292 -7.4803839 -7.9629278 -7.7551503 -7.0651159 -6.0890427][-3.9190507 -4.1081028 -4.3511891 -4.8973913 -5.4458537 -5.79431 -5.8942537 -6.1445732 -6.9025784 -7.718298 -8.4262581 -8.70739 -8.2567348 -7.3440709 -6.2095041][-5.0075493 -5.1988206 -5.3828635 -5.7152276 -5.9946446 -6.1162896 -6.1047754 -6.3215575 -7.0052485 -7.7674055 -8.4106407 -8.5930328 -8.068119 -7.1132817 -6.000493]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.1-nosplit-clip50-shortcut
INFO - root - 2017-12-11 09:56:17.835185: step 1010, loss = 0.29, batch loss = 0.21 (18.6 examples/sec; 0.431 sec/batch; 39h:39m:58s remains)
INFO - root - 2017-12-11 09:56:22.085484: step 1020, loss = 0.24, batch loss = 0.15 (17.2 examples/sec; 0.465 sec/batch; 42h:50m:30s remains)
INFO - root - 2017-12-11 09:56:26.518798: step 1030, loss = 0.32, batch loss = 0.24 (17.6 examples/sec; 0.455 sec/batch; 41h:56m:09s remains)
INFO - root - 2017-12-11 09:56:30.972907: step 1040, loss = 0.33, batch loss = 0.24 (18.3 examples/sec; 0.438 sec/batch; 40h:20m:27s remains)
